- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:34:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:34:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2401.11734] Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2401.11734] 深度学习时代的结直肠息肉分割：全面调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.11734](https://ar5iv.labs.arxiv.org/html/2401.11734)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.11734](https://ar5iv.labs.arxiv.org/html/2401.11734)
- en: Colorectal Polyp Segmentation in the
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结直肠息肉分割在
- en: 'Deep Learning Era: A Comprehensive Survey'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习时代：全面调查
- en: Zhenyu Wu, Fengmao Lv, Chenglizhao Chen, Aimin Hao, Shuo Li Zhenyu Wu and Fengmao
    Lv are with the Southwest Jiaotong University. Chenglizhao Chen is with China
    University of Petroleum (Qingdao). Aimin Hao is with Beihang University Shuo Li
    is with Case Western Reserve University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 吴振宇、吕凤茂、陈成利、郝爱民、李硕。吴振宇和吕凤茂来自西南交通大学。陈成利来自中国石油大学（青岛）。郝爱民来自北京航空航天大学，李硕来自凯斯西储大学。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Colorectal polyp segmentation (CPS), an essential problem in medical image analysis,
    has garnered growing research attention. Recently, the deep learning-based model
    completely overwhelmed traditional methods in the field of CPS, and more and more
    deep CPS methods have emerged, bringing the CPS into the deep learning era. To
    help the researchers quickly grasp the main techniques, datasets, evaluation metrics,
    challenges, and trending of deep CPS, this paper presents a systematic and comprehensive
    review of deep-learning-based CPS methods from 2014 to 2023, a total of 115 technical
    papers. In particular, we first provide a comprehensive review of the current
    deep CPS with a novel taxonomy, including network architectures, level of supervision,
    and learning paradigm. More specifically, network architectures include eight
    subcategories, the level of supervision comprises six subcategories, and the learning
    paradigm encompasses 12 subcategories, totaling 26 subcategories. Then, we provided
    a comprehensive analysis the characteristics of each dataset, including the number
    of datasets, annotation types, image resolution, polyp size, contrast values,
    and polyp location. Following that, we summarized CPS’s commonly used evaluation
    metrics and conducted a detailed analysis of 40 deep SOTA models, including out-of-distribution
    generalization and attribute-based performance analysis. Finally, we discussed
    deep learning-based CPS methods’ main challenges and opportunities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 结直肠息肉分割（CPS）是医学图像分析中的一个重要问题，已经引起了越来越多的研究关注。最近，基于深度学习的模型完全超越了传统方法，在CPS领域涌现出越来越多的深度CPS方法，将CPS带入了深度学习时代。为了帮助研究人员快速掌握深度CPS的主要技术、数据集、评估指标、挑战和趋势，本文系统而全面地回顾了2014年至2023年间基于深度学习的CPS方法，共115篇技术论文。特别地，我们首先提供了一个全面的当前深度CPS的综述，并给出了一个新的分类法，包括网络架构、监督级别和学习范式。更具体地，网络架构包括八个子类别，监督级别包括六个子类别，学习范式涵盖12个子类别，总共26个子类别。接着，我们提供了对每个数据集特征的全面分析，包括数据集数量、注释类型、图像分辨率、息肉大小、对比值和息肉位置。随后，我们总结了CPS常用的评估指标，并对40个深度SOTA模型进行了详细分析，包括分布外泛化和基于属性的性能分析。最后，我们讨论了基于深度学习的CPS方法面临的主要挑战和机遇。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引术语：
- en: Colorectal Polyp Segmentation, Deep Learning, Colorectal Cancer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 结直肠息肉分割，深度学习，结直肠癌。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Colorectal cancer (CRC) has become the second leading cause of cancer-related
    death worldwide, causing millions of incidence cases and deaths every year. Research
    shows that nearly 95% of colorectal cancers originate from polyps, undergoing
    a progression from normal mucosa to adenomatous polyps, then precancerous polyps,
    adenocarcinoma, and finally transforming into cancer [[1](#bib.bib1)], as shown
    in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey"). The survival rate of CRC patients
    exceeds 90% in the initial stage but drastically drops to less than 5% in the
    last stage [[2](#bib.bib2)]. Thus, the survival rate can be improved by early
    diagnosis and treatment of precancerous polyps via colonoscopy screening, such
    as colonoscopy and capsule endoscopy. Although colonoscopy provides information
    on the appearance and localization of polyps, it demands expensive labor resources
    and exhibits a high miss rate due to the following challenges: 1) Diagnostic accuracy
    heavily relies on the doctor’s experience.2) Polyps exhibit significant variations
    in color, size, and shape. 3) The boundaries between polyps and surrounding tissues
    are unclear. For instance, sessile polyps, also known as flat polyps, is no significant
    elevation compared to the surrounding normal tissue, making them inconspicuous
    and challenging to detect. Therefore, it is essential to develop an automated
    and accurate colorectal polyp segmentation (CPS) system, minimizing the occurrence
    of misdiagnoses to the greatest extent.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 结直肠癌（CRC）已成为全球第二大癌症相关死亡原因，每年导致数百万例发病和死亡。研究显示，近95%的结直肠癌源自息肉，经历从正常黏膜到腺瘤性息肉、再到癌前息肉、腺癌，最终转变为癌症
    [[1](#bib.bib1)]，如图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ 深度学习时代的结直肠息肉分割：一项综合性综述")所示。结直肠癌患者在初期的生存率超过90%，但在末期则急剧下降至不到5%
    [[2](#bib.bib2)]。因此，通过结肠镜筛查如结肠镜和胶囊内镜，对癌前息肉进行早期诊断和治疗可以提高生存率。虽然结肠镜提供了息肉的外观和定位信息，但由于以下挑战，仍然需要昂贵的劳动资源且漏诊率较高：1)
    诊断准确性严重依赖医生的经验。2) 息肉在颜色、大小和形状上存在显著变化。3) 息肉与周围组织的边界不清楚。例如，粘附性息肉，即平坦息肉，相对于周围正常组织没有明显的隆起，使其不显眼且难以检测。因此，开发一个自动化且准确的结直肠息肉分割（CPS）系统，以最大限度地减少误诊的发生，是至关重要的。
- en: Traditional polyp segmentation methods primarily focus on learning low-level
    features, such as texture, shape, or color distribution, failing to deal with
    complex scenarios. With advanced deep learning, plenty of CNN/Transformer-based
    methods have been proposed for polyp segmentation. In recent years, UNet [[3](#bib.bib3)]
    based deep learning methods such as UNet++ [[4](#bib.bib4)], ResUNet++ [[5](#bib.bib5)],
    and PraNet [[6](#bib.bib6)] have dominated the field. Recently, transformer-based
    models [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)] have also been proposed
    for polyp segmentation and achieve state-of-the-art (SOTA) performance. Despite
    significant progress made by these deep learning models, it still lacks a comprehensive
    overview for the polyp segmentation task to date.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的息肉分割方法主要关注低级特征的学习，例如纹理、形状或颜色分布，无法处理复杂的场景。借助先进的深度学习技术，已经提出了大量基于CNN/Transformer的方法用于息肉分割。近年来，基于UNet
    [[3](#bib.bib3)] 的深度学习方法如UNet++ [[4](#bib.bib4)]、ResUNet++ [[5](#bib.bib5)] 和PraNet
    [[6](#bib.bib6)] 在这一领域占据了主导地位。最近，基于transformer的模型 [[7](#bib.bib7)、[8](#bib.bib8)、[9](#bib.bib9)]
    也被提出用于息肉分割，并取得了最先进的（SOTA）性能。尽管这些深度学习模型取得了显著进展，但至今仍缺乏对息肉分割任务的全面概述。
- en: '![Refer to caption](img/e5ecac1034023529afde2b716b2dd1db.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5ecac1034023529afde2b716b2dd1db.png)'
- en: 'Figure 1: Colorectal cancers originate from polyps, undergoing a progression
    from normal mucosa to adenomatous polyps, then pre-cancerous polyps, adenocarcinoma,
    and finally transforming into cancer. The survival rate of CRC patients exceeds
    90% in the first stage but drastically drops to less than 5% in the last stage.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：结直肠癌源自息肉，经历从正常黏膜到腺瘤性息肉、再到癌前息肉、腺癌，最终转变为癌症的过程。结直肠癌患者在初期的生存率超过90%，但在末期则急剧下降至不到5%。
- en: '![Refer to caption](img/33e66fdf3b19afb13885b1e7bac20a4a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/33e66fdf3b19afb13885b1e7bac20a4a.png)'
- en: 'Figure 2: A brief history of CPS. The first seminal work [[10](#bib.bib10)]
    on CPS can be traced back to 2003\. The first deep learning-based CPS method [[3](#bib.bib3)]
    emerged in 2015\. The methods mentioned above serve as milestones, which are typically
    highly cited. See Section [1.1.1](#S1.SS1.SSS1 "1.1.1 A brife history of polyp
    segmentation ‣ 1.1 History and Scope ‣ 1 Introduction ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey") for more details.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：息肉分割的简要历史。第一个开创性工作[[10](#bib.bib10)] 可以追溯到 2003 年。第一个基于深度学习的息肉分割方法[[3](#bib.bib3)]
    出现于 2015 年。上述方法作为里程碑，通常被引用频繁。更多细节请参见第 [1.1.1](#S1.SS1.SSS1 "1.1.1 结肠息肉分割的简要历史
    ‣ 1.1 历史和范围 ‣ 1 引言 ‣ 深度学习时代的结直肠息肉分割：综合调查") 节。
- en: This paper presents a systematic and comprehensive review of deep-learning-based
    CPS methods. We first provide a comprehensive review of the current deep CPS with
    a novel taxonomy, including network architectures, level of supervision, and their
    learning paradigm. Then, we also provide a comprehensive analysis of the characteristics
    of each dataset, including the number of datasets, annotation types, image resolution,
    polyp size, contrast values, and polyp location. We conducted a comprehensive
    summary and analysis of 40 deep SOTA models, building an open and standardized
    evaluation benchmarking. Finally, we discussed the main challenges and opportunities
    of deep learning-based CPS methods. We hope that this survey can assist researchers
    in quickly grasping the development history of polyp segmentation and attracting
    more researchers to join the field.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文系统而全面地回顾了基于深度学习的息肉分割方法。我们首先提供了对当前深度息肉分割的全面回顾，并提出了一种新的分类法，包括网络架构、监督程度及其学习范式。然后，我们还对每个数据集的特征进行了全面分析，包括数据集数量、注释类型、图像分辨率、息肉大小、对比度值和息肉位置。我们对
    40 个深度 SOTA 模型进行了全面总结和分析，建立了一个开放和标准化的评估基准。最后，我们讨论了基于深度学习的息肉分割方法的主要挑战和机遇。我们希望这项调查能够帮助研究人员快速了解息肉分割的发展历史，并吸引更多研究人员加入该领域。
- en: 1.1 History and Scope
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 历史和范围
- en: 1.1.1 A brife history of polyp segmentation
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.1 结肠息肉分割的简要历史
- en: 'As shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey"), we briefly outline the history
    of colorectal polyp segmentation. To our best knowledge, the first work of CPS
    can be traced back to the seminal works of [[10](#bib.bib10)] published in 2003,
    which employs the canny edge detector to segment images of polyp candidates for
    computed tomography (CT) colonography. Subsequently, Yao et al. [[11](#bib.bib11)]
    proposed an automatic method to segment colonic polyps in CT colonography, which
    is based on a combination of knowledge-guided intensity adjustment and fuzzy c-mean
    clustering. Gross et al. [[12](#bib.bib12)] presented the first automatic polyp
    segmentation algorithm for colonoscopic narrow-band images. Hwang et al. [[13](#bib.bib13)]
    presented an unsupervised method for the detection of polyps in wireless capsule
    endoscopy videos, which adopts watershed segmentation with a novel initial marker
    selection method based on Gabor texture features and K-means clustering. These
    non-deep learning-based polyp segmentation methods [[14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)] typically depend on manually extracted low-level features and
    traditional segmentation algorithms. However, manually extracting low-level features
    for polyp segmentation is inadequate for handling complex scenarios.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 深度学习时代的结直肠息肉分割：综合调查") 所示，我们简要概述了结肠息肉分割的历史。据我们所知，结肠息肉分割的首次研究可以追溯到
    2003 年发表的[[10](#bib.bib10)]的开创性工作，该工作采用 Canny 边缘检测器对计算机断层扫描 (CT) 结肠造影中的息肉候选图像进行分割。随后，Yao
    等人[[11](#bib.bib11)] 提出了一个自动分割 CT 结肠造影中结肠息肉的方法，该方法基于知识引导的强度调整和模糊 C 均值聚类的结合。Gross
    等人[[12](#bib.bib12)] 提出了第一个用于结肠镜窄带图像的自动息肉分割算法。Hwang 等人[[13](#bib.bib13)] 提出了一个用于无线胶囊内镜视频中息肉检测的无监督方法，该方法采用了基于
    Gabor 纹理特征和 K-means 聚类的新颖初始标记选择方法的水域分割。这些非深度学习基础的息肉分割方法[[14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)] 通常依赖于手动提取的低级特征和传统的分割算法。然而，手动提取低级特征进行息肉分割不足以应对复杂的场景。
- en: With deep learning technology completely overwhelms traditional methods in the
    field of computer vision, more and more deep CPS methods [[17](#bib.bib17), [4](#bib.bib4),
    [2](#bib.bib2)] have emerged since 2015, bringing the polyp segmentation task
    into the deep learning era. Ronneberger et al. [[3](#bib.bib3)] proposed a UNet
    shape network for biomedical image segmentation and won the ISBI cell tracking
    challenge 2015 by a large margin. Later, Fang et al. [[18](#bib.bib18)] proposed
    a selective feature aggregation network with the area and boundary constraints
    for CPS. Fan et al. [[6](#bib.bib6)] propose a parallel reverse attention network
    for accurate polyp segmentation in colonoscopy images. Zhang et al. [[7](#bib.bib7)]
    propose a transformer-based parallel-in-branch architecture, which combines Transformers
    and CNNs in a parallel style. Recently, Ling et al. [[9](#bib.bib9)] proposed
    a Gaussian-Probabilistic guided semantic fusion method that progressively fuses
    the probability information of polyp positions with the decoder supervised by
    binary masks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习技术在计算机视觉领域完全压倒传统方法，自2015年以来，越来越多的深度CPS方法[[17](#bib.bib17)、[4](#bib.bib4)、[2](#bib.bib2)]出现，将息肉分割任务带入了深度学习时代。Ronneberger等人[[3](#bib.bib3)]提出了一种用于生物医学图像分割的UNet形状网络，并在2015年ISBI细胞跟踪挑战赛中大幅领先获胜。后来，Fang等人[[18](#bib.bib18)]提出了一种带有面积和边界约束的选择性特征聚合网络用于CPS。Fan等人[[6](#bib.bib6)]提出了一种并行反向注意力网络，用于在结肠镜图像中精确分割息肉。Zhang等人[[7](#bib.bib7)]提出了一种基于变压器的并行分支架构，将变压器和CNN以并行方式结合在一起。最近，Ling等人[[9](#bib.bib9)]提出了一种高斯-概率引导的语义融合方法，该方法逐步融合息肉位置的概率信息，并由二进制掩膜监督的解码器进行指导。
- en: 1.1.2 Scope of this survey.
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.2 本调查的范围。
- en: Clinical colorectal polyp screening can be divided at the task level into three
    classical tasks, i.e., polyp classification, polyp detection, and polyp segmentation.
    This work focuses on the polyp segmentation task and aims to systematically summarize
    deep polyp segmentation methods, commonly used polyp segmentation datasets, evaluation
    metrics, and model performance. Although the development of the deep polyp segmentation
    model is only eight years, it has spawned hundreds of papers, making it impractical
    to review all of them. In this survey, we mainly focus on these influential papers
    published in mainstream journals and conferences of medical image analysis from
    2019 to 2023\. Besides, for completeness and better readability, some early relevant
    works ranging from 2014 to 2018 have also been mentioned briefly. Due to space
    limitations and the extent of our knowledge, we apologize to those authors whose
    works could not be included in this paper.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 临床结直肠息肉筛查在任务层面上可以分为三个经典任务，即息肉分类、息肉检测和息肉分割。本文主要关注息肉分割任务，旨在系统总结深度息肉分割方法、常用的息肉分割数据集、评价指标和模型性能。虽然深度息肉分割模型的发展仅有八年，但已经产生了数百篇论文，因此全面回顾这些论文是不切实际的。在这项调查中，我们主要关注2019年至2023年在医学图像分析主流期刊和会议上发表的具有影响力的论文。此外，为了完整性和更好的可读性，我们还简要提及了2014年至2018年间的一些早期相关工作。由于篇幅限制和我们的知识范围，我们对未能包含在本文中的作者表示歉意。
- en: 1.2 Related Surveys
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 相关调查
- en: 'Table [I](#S1.T1 "TABLE I ‣ 1.2 Related Surveys ‣ 1 Introduction ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey") lists existing
    surveys related to this work. Among them, Prasath et al. [[19](#bib.bib19)] reviewed
    “capsule endoscopy” colorectal polyp detection and segmentation methods based
    on handcrafted features and discussed the existing challenges. Later, Taha et
    al. [[20](#bib.bib20)] reviewed endoscopy colorectal polyp detection models, which
    divide these models into shape, texture, and fusion features. Sanchez-Peralta
    et al. [[21](#bib.bib21)] mainly focuses on the deep learning-based colorectal
    polyp detection, localization, and segmentation approaches. Besides, they also
    summarize six widely used polyp detection datasets for training or benchmarking
    and 19 commonly used metrics for model evaluation. Finally, a more recently published
    survey [[22](#bib.bib22)] summarized the commonly adopted network architectures,
    introduced the benchmark datasets and evaluation metrics, and discussed the current
    challenges. Although existing reviews provide insights from different perspectives,
    they also have some shortcomings: 1) Insufficient in the number of literature.
    In the above-mentioned surveys, the most comprehensive survey [[19](#bib.bib19)]
    contains only 37 papers. 2) Limited in time span. For example, surveys [[19](#bib.bib19)]
    and [[20](#bib.bib20)] focuse on polyp segmentation methods before 2016 while
    [[21](#bib.bib21)] review polyp segmentation methods from 2015 to 2018\. [[22](#bib.bib22)]
    reviews 20 papers, but 18 out of 20 were published in 2022\. 3) Lacking comprehensive
    evaluation and analysis for existing models and datasets.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [I](#S1.T1 "表 I ‣ 1.2 相关调查 ‣ 1 引言 ‣ 深度学习时代的结直肠息肉分割：综合调查")列出了与此工作相关的现有调查。其中，Prasath
    等人 [[19](#bib.bib19)] 回顾了基于手工特征的“胶囊内镜”结直肠息肉检测和分割方法，并讨论了现有挑战。后来，Taha 等人 [[20](#bib.bib20)]
    回顾了内镜下的结直肠息肉检测模型，并将这些模型分为形状、纹理和融合特征。Sanchez-Peralta 等人 [[21](#bib.bib21)] 主要关注基于深度学习的结直肠息肉检测、定位和分割方法。此外，他们还总结了六个广泛使用的息肉检测数据集用于训练或基准测试和19个常用的模型评估指标。最后，一篇更近期的综述
    [[22](#bib.bib22)] 总结了常用的网络架构，介绍了基准数据集和评估指标，并讨论了当前的挑战。尽管现有的综述从不同角度提供了见解，但它们也存在一些不足之处：1)
    文献数量不足。在上述综述中，最全面的综述 [[19](#bib.bib19)] 仅包含37篇论文。2) 时间跨度有限。例如，综述 [[19](#bib.bib19)]
    和 [[20](#bib.bib20)] 关注2016年之前的息肉分割方法，而 [[21](#bib.bib21)] 综述了2015年至2018年的息肉分割方法。[[22](#bib.bib22)]
    综述了20篇论文，但其中18篇发表于2022年。3) 对现有模型和数据集的综合评估和分析不足。
- en: '| Title | Year | Description |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 年份 | 描述 |'
- en: '| Polyp detection and segmentation from video capsule endoscopy: A review [[19](#bib.bib19)]
    | 2016 | This paper reviews traditional based capsule endoscopy polyps detection
    and segmentation methods before 2016 with only 37 papers. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 从视频胶囊内镜中检测和分割息肉：综述 [[19](#bib.bib19)] | 2016 | 本文回顾了2016年之前基于传统方法的胶囊内镜息肉检测和分割方法，共有37篇论文。
    |'
- en: '| Automatic polyp detection in endoscopy videos: A survey [[20](#bib.bib20)]
    | 2017 | This paper reviews handcrafted features based polyps detection methods
    before 2016 with only 28 papers. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 内镜视频中自动息肉检测：综述 [[20](#bib.bib20)] | 2017 | 本文回顾了2016年之前基于手工特征的息肉检测方法，共有28篇论文。
    |'
- en: '| Deep learning to find colorectal polyps in colonoscopy:A systematic literature
    review [[21](#bib.bib21)] | 2020 | This paper reviews deep learning-based polyps
    detection, localization, and segmentation methods from 2015 to 2018 with only
    35 papers. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 使用深度学习在内镜检查中发现结直肠息肉：系统文献综述 [[21](#bib.bib21)] | 2020 | 本文回顾了2015年至2018年基于深度学习的息肉检测、定位和分割方法，共有35篇论文。
    |'
- en: '| Detection of colorectal polyps from colonoscopy using machine learning: A
    survey on modern techniques [[22](#bib.bib22)] | 2023 | This paper reviews deep
    learning-based polyps detection with only 20 papers, of which 18 out of 20 were
    published in 2022. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 通过内镜检查使用机器学习检测结直肠息肉：现代技术的调查 [[22](#bib.bib22)] | 2023 | 本文回顾了基于深度学习的息肉检测，共有20篇论文，其中18篇发表于2022年。
    |'
- en: 'TABLE I: Summary of previous surveys on CPS. For each survey, the title, publication
    year, and a summary of the content are provided. More discussion can be found
    in Section [1.2](#S1.SS2 "1.2 Related Surveys ‣ 1 Introduction ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 之前关于 CPS 的调查总结。每个调查都提供了标题、出版年份和内容总结。更多讨论可以在第[1.2](#S1.SS2 "1.2 相关调查 ‣
    1 引言 ‣ 深度学习时代的结直肠息肉分割：综合调查")节中找到。'
- en: 'Recently, Mei et al. [[23](#bib.bib23)] also conducted a polyp segmentation
    survey and released their paper on the Arxiv platform. Compared to this contemporary
    work and previous surveys, our work has several advantages: 1) A well-organized
    taxonomy of deep CPS models. As shown in Fig. [3](#S1.F3 "Figure 3 ‣ 1.3 Our Contributions
    ‣ 1 Introduction ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey"), our survey categorized polyp segmentation methods into three main groups:
    network architectures, level of supervision, and learning paradigm. More specifically,
    network architectures include eight subcategories, the level of supervision comprises
    six subcategories, and the learning paradigm encompasses 12 subcategories, totaling
    26 subcategories. In contrast, Mei’s survey divided existing polyp segmentation
    methods into six categories. 2) A systematic study of current deep CPS methods.
    As shown in Table [II](#S2.T2 "TABLE II ‣ 2.1.2 CNN-based Method ‣ 2.1 Network
    Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey") and Table [III](#S2.T3 "TABLE III
    ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey)
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"),
    our survey contains 115 papers from 2014 to 2023, while Mei’s survey only covered
    45 papers from 2019 to 2023\. The number of papers in our survey is nearly three
    times that of theirs. Besides we also provided a summary of the architecture,
    key technologies, and training datasets utilized by the models proposed in each
    paper, aiming to enhance readers’ understanding. 3) A comprehensive analysis of
    current CPS datasets. As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Image-level
    Datasets ‣ 3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey"), Fig. [5](#S3.F5 "Figure 5 ‣ 3.1 Image-level
    Datasets ‣ 3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey") and Table [IV](#S2.T4 "TABLE IV ‣
    2.3.4 Domain Adapation ‣ 2.3 Learning Paradigm ‣ 2 Methodology (Survey) ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"), we provide
    a comprehensive analysis of the characteristics of each dataset, including the
    number of datasets, annotations types, image resolution, polyp size, contrast
    values, number of polyps and polyp location. 4) An extensive performance comparison
    and analysis of deep CPS approaches. As shown in Table [V](#S4.T5 "TABLE V ‣ 4
    Evaluation Metrics ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A
    Comprehensive Survey"), our survey comprehensively summarized and analyzed 40
    models from 2015 to 2023, while Mei’s survey included only 21 models. In addition,
    as shown in Table [VI](#S4.T6 "TABLE VI ‣ 4 Evaluation Metrics ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey") and Table [VIII](#S5.T8
    "TABLE VIII ‣ 5.3 Attribute-based Performance Analysis ‣ 5 Performance Benchmarking
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"),
    We also assessed the model’s generalization performance on the out-of-distribution
    dataset (i.e., PolypGen [[24](#bib.bib24)]) and its attribute-based performance
    on SUN-SEG[[2](#bib.bib2)] datasets for better uncovering the nuanced strengths
    and weaknesses of deep CPS models. 5) A deeper look into the challenges of CPS
    and providing some promising directions. We extensively summarized the challenges
    encountered in polyp segmentation, including the deep model’s interpretability,
    generalization ability, robustness to adversarial attacks, data privacy, domain
    shift, etc. We also highlighted several promising directions for CPS, such as
    combining CPS with unsupervised anomaly localization, large visual models and
    large language models.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，梅等人[[23](#bib.bib23)]也进行了一项息肉分割调查，并在 Arxiv 平台上发布了他们的论文。与这项现代工作和以前的调查相比，我们的工作具有几个优势：1)
    一个组织良好的深度 CPS 模型分类系统。如图 [3](#S1.F3 "Figure 3 ‣ 1.3 Our Contributions ‣ 1 Introduction
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")所示，我们的调查将息肉分割方法分为三大类：网络架构、监督级别和学习范式。更具体地说，网络架构包括八个子类别，监督级别包含六个子类别，而学习范式涵盖
    12 个子类别，总计 26 个子类别。相比之下，梅的调查将现有的息肉分割方法分为六类。2) 对当前深度 CPS 方法的系统研究。如表 [II](#S2.T2
    "TABLE II ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology
    (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey") 和表 [III](#S2.T3 "TABLE III ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")所示，我们的调查包含了 2014 年至 2023 年的 115 篇论文，而梅的调查仅覆盖了 2019
    年至 2023 年的 45 篇论文。我们调查的论文数量几乎是他们的三倍。此外，我们还提供了每篇论文中提出的模型的架构、关键技术和训练数据集的总结，旨在提高读者的理解。3)
    对当前 CPS 数据集的全面分析。如图 [6](#S3.F6 "Figure 6 ‣ 3.1 Image-level Datasets ‣ 3 Polyp
    Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning Era:
    A Comprehensive Survey")、图 [5](#S3.F5 "Figure 5 ‣ 3.1 Image-level Datasets ‣ 3
    Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey") 和表 [IV](#S2.T4 "TABLE IV ‣ 2.3.4 Domain Adapation
    ‣ 2.3 Learning Paradigm ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")所示，我们提供了每个数据集特征的全面分析，包括数据集数量、注释类型、图像分辨率、息肉大小、对比度值、息肉数量和息肉位置。4)
    深度 CPS 方法的广泛性能比较和分析。如表 [V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")所示，我们的调查全面总结和分析了
    2015 年至 2023 年的 40 个模型，而梅的调查仅包括 21 个模型。此外，如表 [VI](#S4.T6 "TABLE VI ‣ 4 Evaluation
    Metrics ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey") 和表 [VIII](#S5.T8 "TABLE VIII ‣ 5.3 Attribute-based Performance Analysis
    ‣ 5 Performance Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")所示，我们还评估了模型在分布外数据集（即 PolypGen [[24](#bib.bib24)]）上的泛化性能以及在
    SUN-SEG[[2](#bib.bib2)] 数据集上的基于属性的性能，以更好地揭示深度 CPS 模型的细微优缺点。5) 对 CPS 挑战的深入探讨和提供一些有前景的方向。我们广泛总结了在息肉分割中遇到的挑战，包括深度模型的可解释性、泛化能力、对抗攻击的鲁棒性、数据隐私、领域转移等。我们还强调了
    CPS 的几个有前景的方向，例如将 CPS 与无监督异常定位、大型视觉模型和大型语言模型相结合。'
- en: 1.3 Our Contributions
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 我们的贡献
- en: 'In summary, this survey systematically studies deep polyp segmentation methods,
    datasets, and metrics from 2014 to 2023, including 115 technical papers, 14 polyp
    segmentation datasets, and 12 commonly used evaluation metrics. We aim to provide
    an intuitive understanding and high-level insights into polyp segmentation so
    that researchers can choose the proper directions to explore. The contributions
    of this work are summarized as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本调查系统地研究了2014年至2023年间的深度息肉分割方法、数据集和评价指标，包括115篇技术论文、14个息肉分割数据集和12个常用评价指标。我们旨在提供对息肉分割的直观理解和高级见解，以便研究人员能够选择合适的方向进行探索。本工作的贡献总结如下：
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A systematic study of deep CPS methods from 2014 to 2023, a total of 115 technical
    papers. We propose a novel taxonomy of polyp segmentation approaches and categorize
    these approaches into three main categories: network architectures, level of supervision,
    and learning paradigm, which can be further divided into 26 subcategories. The
    introduced taxonomy was designed to help researchers gain a deeper insight into
    the essential characteristics of deep CPS models.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对2014年至2023年的深度CPS方法的系统研究，共115篇技术论文。我们提出了一种新的息肉分割方法分类法，并将这些方法分为三大类：网络架构、监督级别和学习范式，这些类别可以进一步细分为26个子类别。引入的分类法旨在帮助研究人员深入了解深度CPS模型的核心特征。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A comprehensive analysis on current CPS datasets. We thoroughly analyze each
    CPS dataset, including the number of datasets, annotation types, image resolution,
    polyp size, contrast values, number of polyps, and polyp location. Furthermore,
    we analyzed the challenges and difficulties in the existing CPS dataset.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对当前CPS数据集的综合分析。我们彻底分析了每个CPS数据集，包括数据集数量、注释类型、图像分辨率、息肉大小、对比值、息肉数量和息肉位置。此外，我们还分析了现有CPS数据集中的挑战和困难。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An extensive performance comparison and analysis of deep CPS models. We conducted
    a comprehensive summary and analysis of 40 models from 2015 to 2023 for building
    an open and standardized evaluation benchmarking. Additionally, we evaluated the
    model’s generalization performance on the multi-center dataset (PolypGen) and
    its attribute-based performance on SUN-SEG datasets to better understand the strengths
    and weaknesses of deep CPS models.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度CPS模型的广泛性能比较和分析。我们对2015年至2023年的40个模型进行了全面总结和分析，以建立一个开放和标准化的评估基准。此外，我们评估了模型在多中心数据集（PolypGen）上的泛化性能以及在SUN-SEG数据集上的属性性能，以更好地了解深度CPS模型的优缺点。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An in-depth delving into the challenges of deep CPS models and providing promising
    directions. We summarized the challenges of current deep CPS models, including
    deep model’s interpretability, generalization ability, robustness to adversarial
    attacks, data privacy, domain shift, etc. We also present several promising directions
    for CPS, such as combining CPS with unsupervised anomaly localization, large visual
    models, and large language models.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对深度CPS模型挑战的深入探讨并提供有前景的方向。我们总结了当前深度CPS模型的挑战，包括深度模型的可解释性、泛化能力、对对抗攻击的鲁棒性、数据隐私、领域偏移等。同时，我们还提出了几个有前景的CPS方向，如将CPS与无监督异常定位、大型视觉模型和大型语言模型结合。
- en: 'The rest of this survey is organized as follows: Section [2](#S2 "2 Methodology
    (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey") presents a novel taxonomy and reviews some representative state-of-the-art
    deep learning-based polyp segmentation models for each category. Section [3](#S3
    "3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey") overviews some of the most popular polyp segmentation
    datasets and their characteristics. Section [4](#S4 "4 Evaluation Metrics ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey") lists popular
    metrics for evaluating deep CPS models. Section [5](#S5 "5 Performance Benchmarking
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")
    benchmarks 40 deep CPS models and provides in-depth analyses. Section [6](#S6
    "6 Challenges and Future Directions ‣ Colorectal Polyp Segmentation in the Deep
    Learning Era: A Comprehensive Survey") discusses the main challenges and opportunities
    of deep learning-based CPS methods. Section [7](#S7 "7 Conlusion ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey") is the conclusion.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的其余部分组织如下：[第2节](#S2 "2 方法论（调查） ‣ 深度学习时代的结直肠息肉分割：全面调查")介绍了一种新的分类法，并回顾了每个类别中一些具有代表性的最先进的基于深度学习的息肉分割模型。[第3节](#S3
    "3 息肉分割数据集 ‣ 深度学习时代的结直肠息肉分割：全面调查")概述了一些最流行的息肉分割数据集及其特征。[第4节](#S4 "4 评估指标 ‣ 深度学习时代的结直肠息肉分割：全面调查")列出了用于评估深度CPS模型的流行指标。[第5节](#S5
    "5 性能基准 ‣ 深度学习时代的结直肠息肉分割：全面调查")对40个深度CPS模型进行了基准测试，并提供了深入的分析。[第6节](#S6 "6 挑战与未来方向
    ‣ 深度学习时代的结直肠息肉分割：全面调查")讨论了基于深度学习的CPS方法的主要挑战和机会。[第7节](#S7 "7 结论 ‣ 深度学习时代的结直肠息肉分割：全面调查")是结论部分。
- en: '![Refer to caption](img/fe9042f1e5b4af05a9b176f9cf0fc427.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fe9042f1e5b4af05a9b176f9cf0fc427.png)'
- en: 'Figure 3: An overview of our proposed taxonomy. We categorize existing polyp
    segmentation approaches into three branches from different perspectives: network
    architectures, level of supervision, and learning paradigm. Specifically, network
    architectures include eight subcategories, the level of supervision comprises
    six subcategories, and the learning paradigm encompasses 12 subcategories, totaling
    26 subcategories.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：我们提出的分类法概述。我们从不同的角度将现有的息肉分割方法分为三个分支：网络架构、监督级别和学习范式。具体而言，网络架构包括八个子类别，监督级别包括六个子类别，而学习范式则包含12个子类别，共计26个子类别。
- en: 2 Methodology (Survey)
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论（调查）
- en: 'This section provides a comprehensive overview of prominent deep CPS methods
    from novel taxonomy, including network architectures (Sec. [2.1](#S2.SS1 "2.1
    Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")), level of supervision (Sec.
    [2.2](#S2.SS2 "2.2 Level of Supervision ‣ 2 Methodology (Survey) ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"))and learning
    paradigm (Sec. [2.3](#S2.SS3 "2.3 Learning Paradigm ‣ 2 Methodology (Survey) ‣
    Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")),
    as shown in Fig. [3](#S1.F3 "Figure 3 ‣ 1.3 Our Contributions ‣ 1 Introduction
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey").
    Table [II](#S2.T2 "TABLE II ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey") and Table [III](#S2.T3 "TABLE III ‣ 2.1.2 CNN-based
    Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey") summarizes the
    recent proposed deep CPS models and some representative traditional CPS methods.
    Due to limited space, we only selected a few representative methods for each category.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '本节提供了从新的分类法出发，对突出深度CPS方法的全面概述，包括网络架构（参见 [2.1](#S2.SS1 "2.1 Network Architectures
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")），监督级别（参见 [2.2](#S2.SS2 "2.2 Level of Supervision
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")）和学习范式（参见 [2.3](#S2.SS3 "2.3 Learning Paradigm ‣
    2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era:
    A Comprehensive Survey")），如图 [3](#S1.F3 "Figure 3 ‣ 1.3 Our Contributions ‣ 1
    Introduction ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey")所示。表 [II](#S2.T2 "TABLE II ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey") 和表 [III](#S2.T3 "TABLE III ‣ 2.1.2 CNN-based Method
    ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey") 总结了最近提出的深度CPS模型和一些具有代表性的传统CPS方法。由于篇幅有限，我们仅选择了每个类别中的一些代表性方法。'
- en: 2.1 Network Architectures
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 网络架构
- en: 'Based on the adopted backbone architectures, we further classify these deep
    CPS models into four categories: multilayer perceptron (MLP) based models (Sec.
    [2.1.1](#S2.SS1.SSS1 "2.1.1 MLP-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology
    (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey")), convolutional neural network (CNN) based models (Sec. [2.1.2](#S2.SS1.SSS2
    "2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣
    Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")),
    transformer-based models (Sec. [2.1.3](#S2.SS1.SSS3 "2.1.3 Transformer-based Method
    ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")), and hybrid-network based
    models (Sec. [2.1.4](#S2.SS1.SSS4 "2.1.4 Hybrid-network Based Method ‣ 2.1 Network
    Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey")).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '根据采用的骨干网络架构，我们进一步将这些深度CPS模型分为四类：基于多层感知机（MLP）的模型（参见 [2.1.1](#S2.SS1.SSS1 "2.1.1
    MLP-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")），基于卷积神经网络（CNN）的模型（参见
    [2.1.2](#S2.SS1.SSS2 "2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology
    (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey")），基于变换器（transformer）的模型（参见 [2.1.3](#S2.SS1.SSS3 "2.1.3 Transformer-based
    Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey")），以及基于混合网络的模型（参见
    [2.1.4](#S2.SS1.SSS4 "2.1.4 Hybrid-network Based Method ‣ 2.1 Network Architectures
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")）。'
- en: 2.1.1 MLP-based Method
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 基于MLP的方法
- en: 'As shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.2 CNN-based Method ‣ 2.1 Network
    Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey")(a), MLP is a type of artificial neural
    network architecture composed of multiple layers of nodes, including an input
    layer, one or more hidden layers, and an output layer. Shi et al. [[25](#bib.bib25)]
    proposed a novel work to investigate the MLP-based architecture in polyp segmentation,
    which uses CycleMLP [[26](#bib.bib26)] as the encoder to overcome the fixed input
    scale issue. Yuan et al. [[17](#bib.bib17)] introduce an MLP-based sparse autoencoder
    to learn high-level superpixel characterization from the hand-crafted features.
    Although MLP-based CPS methods exhibit superior performance compared to non-deep
    learning methods, MLP-based models are limited in leveraging the image’s spatial
    information. Moreover, these methods are time-consuming, as they require processing
    in a multi-stage manner.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[4](#S2.F4 "图 4 ‣ 2.1.2 基于CNN的方法 ‣ 2.1 网络架构 ‣ 2 方法论（综述） ‣ 深度学习时代的结直肠息肉分割：全面综述")(a)所示，MLP是一种由多个节点层组成的人工神经网络架构，包括输入层、一个或多个隐藏层和输出层。Shi等人[[25](#bib.bib25)]
    提出了一个新颖的工作来研究基于MLP的息肉分割架构，该架构使用CycleMLP[[26](#bib.bib26)]作为编码器，以克服固定输入规模的问题。Yuan等人[[17](#bib.bib17)]
    介绍了一种基于MLP的稀疏自编码器，用于从手工特征中学习高层次的超像素特征。尽管基于MLP的CPS方法相比于非深度学习方法表现优越，但基于MLP的模型在利用图像的空间信息方面存在局限。此外，这些方法耗时较长，因为它们需要以多阶段的方式进行处理。
- en: 2.1.2 CNN-based Method
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 基于CNN的方法
- en: To overcome the limitations of MLP-based methods, recent CPS approaches [[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)] adopted the CNN architecture,
    which enables polyp representation and segmentation in a end-to-end manner. The
    CNN-based methods have become dominant in the CPS field and can be further divided
    into FCN-based networks, UNet-based networks, multi-branch networks, and deep
    supervision networks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服基于MLP的方法的局限性，近期的CPS方法[[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35)] 采用了CNN架构，这使得息肉的表示和分割能够以端到端的方式进行。基于CNN的方法已在CPS领域占据主导地位，并且可以进一步分为FCN-based网络、UNet-based网络、多分支网络和深度监督网络。
- en: 'FCN-based Network. Long et al. [[36](#bib.bib36)] introduced Fully Convolutional
    Networks (FCN) for semantic segmentation, As shown in Fig. [4](#S2.F4 "Figure
    4 ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey)
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")(b).
    Since then, FCN has become the foundational framework for image segmentation,
    and subsequent CPS methods [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41)] largely evolving from it. Yin et al. [[42](#bib.bib42)]
    proposed two parallel attention-based modules, which can be incorporated into
    any encoder-decoder architecture. Xu et al. [[43](#bib.bib43)] proposed a temporal
    correlation network for video polyp segmentation, in which the temporal correlation
    is unprecedentedly modeled based on the relationship between the original video
    and the captured frames to be adaptable for video polyp segmentation. Feng et
    al. [[44](#bib.bib44)] proposed a novel stair-shape network for real-time polyp
    segmentation in colonoscopy images. Akbari et al. [[45](#bib.bib45)] proposed
    a polyp segmentation method based on fully the convolutional neural network. Wichakam
    et al. [[46](#bib.bib46)] proposed a compressed fully convolutional network by
    modifying the FCN-8s network to detect and segment polyp in real-time. Wu et al.
    [[47](#bib.bib47)] proposed a novel ConvNet to accurately segment polyps from
    colonoscopy videos in a bottom-up/top-down manner. Dong et al. [[48](#bib.bib48)]
    present asymmetric attention upsampling, which utilizes the information of low-level
    feature maps to rescale the high-level feature maps smartly through spatial pooling
    and attention mechanisms.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 FCN 的网络。Long 等人 [[36](#bib.bib36)] 介绍了用于语义分割的全卷积网络（FCN），如图 [4](#S2.F4 "图
    4 ‣ 2.1.2 CNN 基于方法 ‣ 2.1 网络架构 ‣ 2 方法论（调查） ‣ 深度学习时代的结直肠息肉分割：综合调查")(b) 所示。此后，FCN
    成为图像分割的基础框架，随后的 CPS 方法 [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41)] 大多从中演变而来。Yin 等人 [[42](#bib.bib42)] 提出了两个并行的基于注意力的模块，这些模块可以集成到任何编码器-解码器架构中。Xu
    等人 [[43](#bib.bib43)] 提出了一个用于视频息肉分割的时间相关网络，其中时间相关性前所未有地基于原始视频和捕获帧之间的关系进行建模，以适应视频息肉分割。Feng
    等人 [[44](#bib.bib44)] 提出了一个新颖的阶梯形网络，用于实时结肠镜图像中的息肉分割。Akbari 等人 [[45](#bib.bib45)]
    提出了一个基于全卷积神经网络的息肉分割方法。Wichakam 等人 [[46](#bib.bib46)] 通过修改 FCN-8s 网络提出了一个压缩的全卷积网络，用于实时检测和分割息肉。Wu
    等人 [[47](#bib.bib47)] 提出了一个新颖的 ConvNet，以自下而上/自上而下的方式准确分割结肠镜视频中的息肉。Dong 等人 [[48](#bib.bib48)]
    提出了不对称注意力上采样，通过空间池化和注意力机制智能地利用低级特征图的信息来重新缩放高级特征图。
- en: 'UNet-based Network. Ronneberger et al. [[3](#bib.bib3)] proposed the UNet architecture
    designed for biomedical image segmentation tasks. As shown in Fig. [4](#S2.F4
    "Figure 4 ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology
    (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey")(c), it comprises a contracting path to capture context and a symmetric
    expanding path, making it suitable for CPS tasks. For instance, Zhang et al. [[49](#bib.bib49)]
    designed a typical UNet structure, followed by a bottom-up feature extraction
    and top-down feature fusion strategy to obtain more comprehensive and semantically
    rich feature representations. Zhao et al. [[50](#bib.bib50)] a ResUnet based framework
    to fuse the proximity frame information at different layers and capture contextual
    information. Srivastava et al. [[51](#bib.bib51)] propose a novel framework for
    medical image segmentation that consists of an encoder block, a shape stream block,
    and a decoder block. Lin et al. [[52](#bib.bib52)] proposed a UNet-based network
    to effectively suppress noises in feature maps and simultaneously improve the
    ability of feature expression at different levels. Song et al. [[53](#bib.bib53)]
    designed a new neural network structure based on the currently popular encoding-decoding
    network architecture.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于UNet的网络。Ronneberger等人[[3](#bib.bib3)] 提出了为生物医学图像分割任务设计的UNet架构。如图 [4](#S2.F4
    "图 4 ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey)
    ‣ 深度学习时代的结直肠息肉分割：综合调查")(c)所示，它包括一个收缩路径用于捕捉上下文和一个对称的扩展路径，使其适用于CPS任务。例如，Zhang等人[[49](#bib.bib49)]
    设计了一个典型的UNet结构，随后采用了自下而上的特征提取和自上而下的特征融合策略，以获得更全面和语义丰富的特征表示。Zhao等人[[50](#bib.bib50)]
    提出了一个基于ResUnet的框架，用于融合不同层次的邻近帧信息并捕捉上下文信息。Srivastava等人[[51](#bib.bib51)] 提出了一个新颖的医学图像分割框架，包括一个编码器块、一个形状流块和一个解码器块。Lin等人[[52](#bib.bib52)]
    提出了一个基于UNet的网络，有效地抑制了特征图中的噪声，同时提高了不同层次上特征表达的能力。Song等人[[53](#bib.bib53)] 设计了一种基于当前流行的编码-解码网络架构的新型神经网络结构。
- en: '![Refer to caption](img/1726a84b56203874e6b8e39bf8923b78.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1726a84b56203874e6b8e39bf8923b78.png)'
- en: 'Figure 4: Classification of deep CPS models according to the network architectures.
    (a) MLP-based methods, (b) FCN-based models, (c) UNet-based approaches, (d) Deep
    supervision network, (e) Transformer-based network, and (f) Multi-branch networks,
    and (g) Hybrid-network.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：根据网络架构对深度CPS模型进行分类。(a) 基于MLP的方法，(b) 基于FCN的模型，(c) 基于UNet的方法，(d) 深度监督网络，(e)
    基于Transformer的网络，(f) 多分支网络，(g) 混合网络。
- en: 'Multi-branch Network. As shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.2 CNN-based
    Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey")(f), multi-branch-based
    methods typically consist of multiple encoders/decoders to extract multi-scale
    features for polyp segmentation explicitly. Li et al. [[54](#bib.bib54)] proposed
    a multi-branch framework, consisting of a segmentation branch and a propagation
    branch, respectively. Huy et al. [[55](#bib.bib55)] proposed an adversarial contrastive
    Fourier method, learning contrastive loss with supervision from the content encoder
    and the style encoder. Ji et al. [[2](#bib.bib2)] design a simple but efficient
    multistream network (named PNS+) for video polyp segmentation, which consists
    of a global encoder and a local encoder. The global and local encoders extract
    long- and short-term spatial-temporal representations from the first anchor frame
    and multiple successive frames. Qiu et al. [[56](#bib.bib56)] design a two-branch
    network; one branch is used to generate a boundary distribution map, and another
    is used to predict the polyp segmentation maps. Chen et al. [[57](#bib.bib57)]
    proposed a novel two-branched network for polyp segmentation of single-modality
    endoscopic images, which consists of an image-to-image translation branch and
    an image segmentation branch. Tomar et al. [[58](#bib.bib58)] propose a novel
    deep learning architecture called dual decoder attention network for automatic
    polyp segmentation.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '多分支网络。如图 [4](#S2.F4 "Figure 4 ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")(f) 所示，多分支方法通常由多个编码器/解码器组成，用于明确地提取多尺度特征以进行息肉分割。Li
    等人 [[54](#bib.bib54)] 提出了一个多分支框架，包括一个分割分支和一个传播分支。Huy 等人 [[55](#bib.bib55)] 提出了一个对抗性对比傅里叶方法，通过内容编码器和风格编码器的监督学习对比损失。Ji
    等人 [[2](#bib.bib2)] 设计了一个简单但高效的多流网络（名为 PNS+），用于视频息肉分割，该网络包括一个全局编码器和一个局部编码器。全局和局部编码器从第一个锚点帧和多个连续帧中提取长短期时空表示。Qiu
    等人 [[56](#bib.bib56)] 设计了一个双分支网络；一个分支用于生成边界分布图，另一个分支用于预测息肉分割图。Chen 等人 [[57](#bib.bib57)]
    提出了一个新颖的双分支网络，用于单模态内窥镜图像的息肉分割，该网络包括一个图像到图像的翻译分支和一个图像分割分支。Tomar 等人 [[58](#bib.bib58)]
    提出了一个名为双解码器注意力网络的深度学习新架构，用于自动息肉分割。'
- en: 'Deep Supervision Network. As shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.2 CNN-based
    Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey")(d), the deep supervision
    learning strategy [[59](#bib.bib59), [4](#bib.bib4), [60](#bib.bib60), [61](#bib.bib61)]
    is proposed to utilize the groundtruth to supervise at each layer of the network,
    facilitating rapid convergence of the model. Patel et al. [[62](#bib.bib62)] proposed
    a novel fuzzy network to focus more on the difficult pixels, where deep supervision
    is applied at the end of the output of each fuzzy attention module and partial
    decoder. Du et al. [[63](#bib.bib63)] proposed an integration context-based reverse-contour
    guidance network with multi-level deep supervision. Nguyen et al. [[64](#bib.bib64)]
    proposed an encoder-decoder-based architecture and each layer was supervised by
    the resized ground truth. Shen et al. [[65](#bib.bib65)] proposed a novel hard
    region enhancement network for polyp segmentation supervised multi-level groundtruth.
    [[7](#bib.bib7)] uses deep supervision to improve the gradient flow by additionally
    supervising the transformer branch and the first fusion branch. Kim et al. [[66](#bib.bib66)]
    propose uncertainty augmented context attention network using extra supervision
    in each layer. Zhang et al. [[67](#bib.bib67)] propose the adaptive context selection
    network supervised by the down-sampled groundtruth. Fan et al. [[6](#bib.bib6)]
    propose a parallel reverse attention network for the polyp segmentation task,
    adopting deep supervision for the last three side outputs.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 深度监督网络。如图 [4](#S2.F4 "图 4 ‣ 2.1.2 基于 CNN 的方法 ‣ 2.1 网络架构 ‣ 2 方法论（调查） ‣ 深度学习时代的结直肠息肉分割：全面调查")
    (d) 所示，提出了一种深度监督学习策略 [[59](#bib.bib59), [4](#bib.bib4), [60](#bib.bib60), [61](#bib.bib61)]，旨在利用真实标签在网络的每一层进行监督，促进模型的快速收敛。Patel
    等人 [[62](#bib.bib62)] 提出了一个新型模糊网络，专注于难处理的像素，其中深度监督应用于每个模糊注意模块和部分解码器的输出末端。Du 等人
    [[63](#bib.bib63)] 提出了一个基于集成上下文的反向轮廓引导网络，并进行了多层次的深度监督。Nguyen 等人 [[64](#bib.bib64)]
    提出了一个基于编码器-解码器的架构，每一层都由调整后的真实标签进行监督。Shen 等人 [[65](#bib.bib65)] 提出了一个新型的硬区域增强网络，用于息肉分割，进行多层次真实标签监督。[[7](#bib.bib7)]
    使用深度监督通过额外监督变压器分支和第一个融合分支来改进梯度流。Kim 等人 [[66](#bib.bib66)] 提出了一个不确定性增强上下文注意网络，在每一层中使用额外的监督。Zhang
    等人 [[67](#bib.bib67)] 提出了一个由下采样真实标签监督的自适应上下文选择网络。Fan 等人 [[6](#bib.bib6)] 提出了一个用于息肉分割任务的并行反向注意网络，采用深度监督用于最后三个侧输出。
- en: '|   Year | Methods | Publication | Architecture | Backbone | Supervision |
    Learning Paradigm | Training Dataset | Number | Available |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发表会议 | 架构 | 主干网络 | 监督方式 | 学习范式 | 训练数据集 | 数量 | 可用性 |'
- en: '| 2023 | PETNet [[9](#bib.bib9)] | MICCAI | Transformer | PVTv2-B2 [[68](#bib.bib68)]
    | Fully-Sup. | Gaussian-Probabilistic, Ensemble | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | PETNet [[9](#bib.bib9)] | MICCAI | Transformer | PVTv2-B2 [[68](#bib.bib68)]
    | 全监督 | 高斯概率，集成 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] |
    548+900 | 否 |'
- en: '| RPFA [[71](#bib.bib71)] | MICCAI | Transformer+UNet | PVT [[72](#bib.bib72)]
    | Fully-Sup. | Feature Propagation/Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| RPFA [[71](#bib.bib71)] | MICCAI | Transformer+UNet | PVT [[72](#bib.bib72)]
    | 全监督 | 特征传播/融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] |
    548+900 | 否 |'
- en: '| S2ME [[73](#bib.bib73)] | MICCAI | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | Scribble-Sup. | Ensemble Learning, Mutual Teaching | SUN-SEG[[2](#bib.bib2)]
    | 6677 | [Open](https://github.com/lofrienger/S2ME) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| S2ME [[73](#bib.bib73)] | MICCAI | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | 画笔监督 | 集成学习，互教 | SUN-SEG [[2](#bib.bib2)] | 6677 | [Open](https://github.com/lofrienger/S2ME)
    |'
- en: '| WeakPolpy [[75](#bib.bib75)] | MICCAI | Transformer/FCN | Res2Net50[[76](#bib.bib76)]/PVTv2-B2
    [[68](#bib.bib68)] | Box-Sup. | Transformation, Scale Consistency | SUN-SEG[[2](#bib.bib2)]/POLYP-SEG[[75](#bib.bib75)]
    | 19,544/15,916 | [Open](https://github.com/weijun88/WeakPolyp) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| WeakPolpy [[75](#bib.bib75)] | MICCAI | Transformer/FCN | Res2Net50 [[76](#bib.bib76)]/PVTv2-B2
    [[68](#bib.bib68)] | Box-Sup. | 转换，尺度一致性 | SUN-SEG [[2](#bib.bib2)]/POLYP-SEG
    [[75](#bib.bib75)] | 19,544/15,916 | [Open](https://github.com/weijun88/WeakPolyp)
    |'
- en: '| FSFM [[41](#bib.bib41)] | ISBI | CNN+FPN | VAN [[77](#bib.bib77)] | Fully-Sup.
    | Feature Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | No |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| FSFM [[41](#bib.bib41)] | ISBI | CNN+FPN | VAN [[77](#bib.bib77)] | 全监督 |
    特征融合，注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 548+900
    | 否 |'
- en: '| EPSG [[78](#bib.bib78)] | ISBI | CNN+UNet | ResNet50 [[74](#bib.bib74)] |
    Fully-Sup. | Image Transformation, Variation | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 490+800 | No |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| EPSG [[78](#bib.bib78)] | ISBI | CNN+UNet | ResNet50 [[74](#bib.bib74)] |
    完全监督 | 图像变换, 变异 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] |
    490+800 | 无 |'
- en: '| RealSeg [[79](#bib.bib79)] | ISBI | FPN | VAN [[77](#bib.bib77)] | Fully-Sup.
    | Real-time, Feature Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | No |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| RealSeg [[79](#bib.bib79)] | ISBI | FPN | VAN [[77](#bib.bib77)] | 完全监督 |
    实时, 特征融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 548+900
    | 无 |'
- en: '| PSTNet [[80](#bib.bib80)] | ISBI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Domain Adaptation, Translation | PICCOLO [[81](#bib.bib81)] | 3,546
    | No |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| PSTNet [[80](#bib.bib80)] | ISBI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 完全监督 | 领域适应, 翻译 | PICCOLO [[81](#bib.bib81)] | 3,546 | 无 |'
- en: '| EMTSNet [[82](#bib.bib82)] | JBHI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Multi-Task, Activation Map | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| EMTSNet [[82](#bib.bib82)] | JBHI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 完全监督 | 多任务, 激活图 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | 无 |'
- en: '| GAN-PSNet [[83](#bib.bib83)] | JBHI | GAN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Attention, Generative Adversarial | Kvasir-SEG [[70](#bib.bib70)]
    | 800 | No |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| GAN-PSNet [[83](#bib.bib83)] | JBHI | GAN+UNet | Res2Net50[[76](#bib.bib76)]
    | 完全监督 | 注意力, 生成对抗 | Kvasir-SEG [[70](#bib.bib70)] | 800 | 无 |'
- en: '| FEGNet [[60](#bib.bib60)] | JBHI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Feedback Mechanism, Boundary | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+800 | No |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| FEGNet [[60](#bib.bib60)] | JBHI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 完全监督 | 反馈机制, 边界 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+800 | 无 |'
- en: '| PMSACL [[84](#bib.bib84)] | MIA | CNN+UNet | ResNet18[[74](#bib.bib74)] |
    Self/Un-Sup. | Data augmentation, Anomaly | HyperKvasir [[85](#bib.bib85)] | 2100
    | No |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| PMSACL [[84](#bib.bib84)] | MIA | CNN+UNet | ResNet18[[74](#bib.bib74)] |
    自监督/无监督 | 数据增强, 异常 | HyperKvasir [[85](#bib.bib85)] | 2100 | 无 |'
- en: '| XBFormer [[86](#bib.bib86)] | TMI | Transformer | PVT [[72](#bib.bib72)]
    | Fully-Sup. | Cross-scale, Boundary Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+800 | [Open](https://github.com/jcwang123/xboundformer)
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| XBFormer [[86](#bib.bib86)] | TMI | Transformer | PVT [[72](#bib.bib72)]
    | 完全监督 | 跨尺度, 边界融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+800 | [开放](https://github.com/jcwang123/xboundformer) |'
- en: '| ColnNet [[87](#bib.bib87)] | TMI | CNN+UNet | DenseNet121 [[88](#bib.bib88)]
    | Fully-Sup. | Relationship of Feature, Boundary | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+800 | No |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| ColnNet [[87](#bib.bib87)] | TMI | CNN+UNet | DenseNet121 [[88](#bib.bib88)]
    | 完全监督 | 特征关系, 边界 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+800 | 无 |'
- en: '| RPANet [[89](#bib.bib89)] | IPMI | CNN+UNet | ResNet101 [[74](#bib.bib74)]
    | Fully-Sup. | Domain Adaptation, Self-Supervision | Private Dataset [[89](#bib.bib89)]
    | 5,175 | No |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| RPANet [[89](#bib.bib89)] | IPMI | CNN+UNet | ResNet101 [[74](#bib.bib74)]
    | 完全监督 | 领域适应, 自监督 | 私有数据集 [[89](#bib.bib89)] | 5,175 | 无 |'
- en: '| TransNetR [[90](#bib.bib90)] | MIDL | Transformer+CNN | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Out-of-distribution, Generalization | Kvasir-SEG [[70](#bib.bib70)]
    | 900 | [Open](https://github.com/DebeshJha/TransNetR) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| TransNetR [[90](#bib.bib90)] | MIDL | Transformer+CNN | ResNet50 [[74](#bib.bib74)]
    | 完全监督 | Out-of-distribution, Generalization | Kvasir-SEG [[70](#bib.bib70)] |
    900 | [开放](https://github.com/DebeshJha/TransNetR) |'
- en: '| CFANet [[91](#bib.bib91)] | PR | CNN+UNet | Res2Net50[[76](#bib.bib76)] |
    Fully-Sup. | Boundary-aware, Feature Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | [Open](https://github.com/taozh2017/CFANet) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| CFANet [[91](#bib.bib91)] | PR | CNN+UNet | Res2Net50[[76](#bib.bib76)] |
    完全监督 | 边界感知, 特征融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | [开放](https://github.com/taozh2017/CFANet) |'
- en: '| GSAL [[92](#bib.bib92)] | PR | CNN+GAN | VGG16 [[93](#bib.bib93)] | Fully-Sup.
    | Adversarial Learning | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | [Open](https://github.com/DLWK/GSAL) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GSAL [[92](#bib.bib92)] | PR | CNN+GAN | VGG16 [[93](#bib.bib93)] | 完全监督
    | 对抗学习 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 548+900 |
    [开放](https://github.com/DLWK/GSAL) |'
- en: '| Polyp-Mixer [[25](#bib.bib25)] | TCSVT | MLP | CycleMLP-B1 [[26](#bib.bib26)]
    | Fully-Sup. | Context-Aware, MLP | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | No |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Polyp-Mixer [[25](#bib.bib25)] | TCSVT | MLP | CycleMLP-B1 [[26](#bib.bib26)]
    | 完全监督 | 上下文感知, MLP | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | 无 |'
- en: '| PTMFNet [[94](#bib.bib94)] | ICIP | Transformer | PVTv2 [[68](#bib.bib68)]
    | Fully-Sup. | Pyramid Transformer, Multibranch | - | - | No |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| PTMFNet [[94](#bib.bib94)] | ICIP | Transformer | PVTv2 [[68](#bib.bib68)]
    | 完全监督 | Pyramid Transformer, Multibranch | - | - | 无 |'
- en: '| PLCUT-Seg [[95](#bib.bib95)] | IJCNN | CNN | HarDNet68 [[96](#bib.bib96)]
    | Self/Semi-Sup. | Synthetic Data, Self/Semi-Supervised | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| PLCUT-Seg [[95](#bib.bib95)] | IJCNN | CNN | HarDNet68 [[96](#bib.bib96)]
    | 自监督/半监督 | 合成数据, 自/半监督 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | 无 |'
- en: '| PolypSeg+ [[97](#bib.bib97)] | TCYB | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Context-aware, Feature Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| PolypSeg+ [[97](#bib.bib97)] | TCYB | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | 完全监督 | 上下文感知, 特征融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | 无 |'
- en: '|  | CASCADE [[98](#bib.bib98)] | WACV | Transformer+UNet | PVTv2 [[68](#bib.bib68)]
    | Fully-Sup. | Cascaded Attention, Feature Fusion | [Synapse](https://www.synapse.org/#!Synapse:syn3193805/wiki/217789)+ACDC[[99](#bib.bib99)]
    + ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 2212+1930 +548+900
    | [Open](https://github.com/SLDGroup/CASCADE) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | CASCADE [[98](#bib.bib98)] | WACV | Transformer+UNet | PVTv2 [[68](#bib.bib68)]
    | 完全监督 | 级联注意力, 特征融合 | [Synapse](https://www.synapse.org/#!Synapse:syn3193805/wiki/217789)+ACDC[[99](#bib.bib99)]
    + ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 2212+1930 +548+900
    | [Open](https://github.com/SLDGroup/CASCADE) |'
- en: '|  | PolypPVT [[100](#bib.bib100)] | CAAI AIR | Transformer+UNet | PVT[[72](#bib.bib72)]
    | Fully-Sup. | Cascaded Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [Open](https://github.com/DengPingFan/Polyp-PVT) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | PolypPVT [[100](#bib.bib100)] | CAAI AIR | Transformer+UNet | PVT[[72](#bib.bib72)]
    | 完全监督 | 级联融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 550+900
    | [Open](https://github.com/DengPingFan/Polyp-PVT) |'
- en: '|  | PEFNet [[101](#bib.bib101)] | MMM | CNN+UNet | EfficientNetV2-L [[102](#bib.bib102)]
    | Fully-Sup. | Positional Embedding | Kvasir-SEG [[70](#bib.bib70)] | 600 | [Open](https://github.com/huyquoctrinh/PEFNet)
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | PEFNet [[101](#bib.bib101)] | MMM | CNN+UNet | EfficientNetV2-L [[102](#bib.bib102)]
    | 完全监督 | 位置嵌入 | Kvasir-SEG [[70](#bib.bib70)] | 600 | [Open](https://github.com/huyquoctrinh/PEFNet)
    |'
- en: '| 2022 | TRFR-Net [[27](#bib.bib27)] | MICCAI | CNN+GAN | ResNet34 [[74](#bib.bib74)]
    | Fully-Sup. | Domain Adaptation, Adversarial Learning | Kvasir-SEG [[70](#bib.bib70)]+ETIS-Larib
    [[103](#bib.bib103)] + CVC-ColonDB [[104](#bib.bib104)] | 700+137+210 | No |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | TRFR-Net [[27](#bib.bib27)] | MICCAI | CNN+GAN | ResNet34 [[74](#bib.bib74)]
    | 完全监督 | 域适应, 对抗学习 | Kvasir-SEG [[70](#bib.bib70)]+ETIS-Larib [[103](#bib.bib103)]
    + CVC-ColonDB [[104](#bib.bib104)] | 700+137+210 | 无 |'
- en: '| TGANet [[28](#bib.bib28)] | MICCAI | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Attention, Multi-scale | Kvasir-SEG [[70](#bib.bib70)] | 880 |
    [Open](https://github.com/nikhilroxtomar/tganet) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| TGANet [[28](#bib.bib28)] | MICCAI | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | 完全监督 | 注意力, 多尺度 | Kvasir-SEG [[70](#bib.bib70)] | 880 | [Open](https://github.com/nikhilroxtomar/tganet)
    |'
- en: '| LDNet [[49](#bib.bib49)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Dynamci Kernel, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+800 | [Open](https://github.com/ReaFly/LDNet) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| LDNet [[49](#bib.bib49)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 完全监督 | 动态内核, 注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+800 | [Open](https://github.com/ReaFly/LDNet) |'
- en: '| SSFormer [[8](#bib.bib8)] | MICCAI | Transformer+UNet | PVTv2 [[68](#bib.bib68)]
    | Fully-Sup. | Local,Global Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | [Open](https://github.com/Qiming-Huang/ssformer) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| SSFormer [[8](#bib.bib8)] | MICCAI | Transformer+UNet | PVTv2 [[68](#bib.bib68)]
    | 完全监督 | 局部,全局融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] |
    548+900 | [Open](https://github.com/Qiming-Huang/ssformer) |'
- en: '| BoxPolyp [[29](#bib.bib29)] | MICCAI | Transformer/CNN | PVTv2 [[68](#bib.bib68)]/Res2Net50[[76](#bib.bib76)]
    | Box-Sup. | Fusion, Consistency Loss | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | [Open](https://github.com/weijun88/BoxPolyp) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| BoxPolyp [[29](#bib.bib29)] | MICCAI | Transformer/CNN | PVTv2 [[68](#bib.bib68)]/Res2Net50[[76](#bib.bib76)]
    | 盒子监督 | 融合, 一致性损失 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | [Open](https://github.com/weijun88/BoxPolyp) |'
- en: '| PPFormer [[105](#bib.bib105)] | MICCAI | Transformer+CNN | CvT[[106](#bib.bib106)]+VGG16[[93](#bib.bib93)]
    | Fully-Sup. | Self-attention, Local2Global Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | No |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| PPFormer [[105](#bib.bib105)] | MICCAI | Transformer+CNN | CvT[[106](#bib.bib106)]+VGG16[[93](#bib.bib93)]
    | 完全监督 | 自注意力, Local2Global 融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+900 | 无 |'
- en: '| SSTAN [[50](#bib.bib50)] | MICCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Semi-Sup. | Context Attention | LDPolypVideo [[107](#bib.bib107)] | 693+14704
    | No |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| SSTAN [[50](#bib.bib50)] | MICCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | 半监督 | 上下文注意力 | LDPolypVideo [[107](#bib.bib107)] | 693+14704 | 无 |'
- en: '| NIP [[30](#bib.bib30)] | MedIA | CNN+UNet | ResNet101[[74](#bib.bib74)] |
    Fully-Sup. | Data Augmentation, Resampling | EndoScene [[108](#bib.bib108)] |
    547 | No |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| NIP [[30](#bib.bib30)] | MedIA | CNN+UNet | ResNet101[[74](#bib.bib74)] |
    完全监督 | 数据增强, 重新采样 | EndoScene [[108](#bib.bib108)] | 547 | 无 |'
- en: '| FSM [[31](#bib.bib31)] | MedIA | CNN+UNet | ResNet101[[74](#bib.bib74)] |
    Fully-Sup. | Domain Adaptation, Resampling | EndoScene [[108](#bib.bib108)] |
    547 | [Open](https://github.com/CityU-AIM-Group/SFDA-FSM) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| FSM [[31](#bib.bib31)] | MedIA | CNN+UNet | ResNet101[[74](#bib.bib74)] |
    全监督 | 域适应, 重新采样 | EndoScene [[108](#bib.bib108)] | 547 | [开源](https://github.com/CityU-AIM-Group/SFDA-FSM)
    |'
- en: '| MSRF-Net [[51](#bib.bib51)] | JBHI | CNN+UNet | SENet [[109](#bib.bib109)]
    | Fully-Sup. | Multi-scale Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 490+800 | [Open](https://github.com/NoviceMAn-prog/MSRF-Net) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| MSRF-Net [[51](#bib.bib51)] | JBHI | CNN+UNet | SENet [[109](#bib.bib109)]
    | 全监督 | 多尺度融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 490+800
    | [开源](https://github.com/NoviceMAn-prog/MSRF-Net) |'
- en: '| BCNet [[32](#bib.bib32)] | JBHI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Cross-Feature Fusion, Boundary | Kvasir-SEG [[70](#bib.bib70)]
    | 800 | [Open](https://github.com/NoviceMAn-prog/MSRF-Net) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| BCNet [[32](#bib.bib32)] | JBHI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 全监督 | 交叉特征融合, 边界 | Kvasir-SEG [[70](#bib.bib70)] | 800 | [开源](https://github.com/NoviceMAn-prog/MSRF-Net)
    |'
- en: '| PNS+ [[2](#bib.bib2)] | MIR | CNN+UNet | Res2Net50[[76](#bib.bib76)] | Fully-Sup.
    | Feature Aggregation, Attention | SUN-SEG [[2](#bib.bib2)] | 19,544 | [Open](https://github.com/GewelsJI/VPS)
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| PNS+ [[2](#bib.bib2)] | MIR | CNN+UNet | Res2Net50[[76](#bib.bib76)] | 全监督
    | 特征聚合, 注意力 | SUN-SEG [[2](#bib.bib2)] | 19,544 | [开源](https://github.com/GewelsJI/VPS)
    |'
- en: '| FCBFormer [[110](#bib.bib110)] | MIUA | Transformer+CNN | PVTv2-B3 [[68](#bib.bib68)]+ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Two-stream Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 490+800 | [Open](https://github.com/ESandML/FCBFormer) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| FCBFormer [[110](#bib.bib110)] | MIUA | Transformer+CNN | PVTv2-B3 [[68](#bib.bib68)]+ResNet34[[74](#bib.bib74)]
    | 全监督 | 双流融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 490+800
    | [开源](https://github.com/ESandML/FCBFormer) |'
- en: '| BDGNet [[56](#bib.bib56)] | SPIE MI | CNN+UNet | EfficientNet-B5 [[111](#bib.bib111)]
    | Fully-Sup. | Multi-Scale Fusion, Boundary | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/zihuanqiu/BDG-Net) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| BDGNet [[56](#bib.bib56)] | SPIE MI | CNN+UNet | EfficientNet-B5 [[111](#bib.bib111)]
    | 全监督 | 多尺度融合, 边界 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [开源](https://github.com/zihuanqiu/BDG-Net) |'
- en: '| TCNet [[43](#bib.bib43)] | BIBM | FCN | Res2Net50[[76](#bib.bib76)] | Fully-Sup.
    | Temporal Correlation Modeling | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+800 | No |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| TCNet [[43](#bib.bib43)] | BIBM | FCN | Res2Net50[[76](#bib.bib76)] | 全监督
    | 时序相关建模 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 550+800
    | 否 |'
- en: '| TransMixer [[61](#bib.bib61)] | BIBM | Transformer+CNN | PVTv2 [[68](#bib.bib68)]+SENet
    [[109](#bib.bib109)] | Fully-Sup. | Interaction Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| TransMixer [[61](#bib.bib61)] | BIBM | Transformer+CNN | PVTv2 [[68](#bib.bib68)]+SENet
    [[109](#bib.bib109)] | 全监督 | 互动融合, 注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | 否 |'
- en: '| ICBNet [[112](#bib.bib112)] | BIBM | Transformer+UNet | PVT [[72](#bib.bib72)]
    | Fully-Sup. | Iterative Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ICBNet [[112](#bib.bib112)] | BIBM | Transformer+UNet | PVT [[72](#bib.bib72)]
    | 全监督 | 迭代融合, 注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | 否 |'
- en: '| CLDNet [[113](#bib.bib113)] | BIBM | Transformer+UNet | PVTv2 [[68](#bib.bib68)]
    | Fully-Sup. | Multi-Scale Fusion, Boundary | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 490+800 | No |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| CLDNet [[113](#bib.bib113)] | BIBM | Transformer+UNet | PVTv2 [[68](#bib.bib68)]
    | 全监督 | 多尺度融合, 边界 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 490+800 | 否 |'
- en: '| TASNet [[57](#bib.bib57)] | BIBM | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Color Reversal, Two-Branched Network | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| TASNet [[57](#bib.bib57)] | BIBM | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 全监督 | 色彩反转, 双分支网络 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | 否 |'
- en: '| FuzzyNet [[62](#bib.bib62)] | NeurIPS | CNN/Transformer | Res2Net50[[76](#bib.bib76)]/PVT
    [[72](#bib.bib72)] | Fully-Sup. | Fuzzy Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/krushi1992/FuzzyNet)
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| FuzzyNet [[62](#bib.bib62)] | NeurIPS | CNN/Transformer | Res2Net50[[76](#bib.bib76)]/PVT
    [[72](#bib.bib72)] | 全监督 | 模糊注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [开源](https://github.com/krushi1992/FuzzyNet) |'
- en: '| SwinPA-Net [[114](#bib.bib114)] | TNNLS | Transformer+UNet | Swin-B [[115](#bib.bib115)]
    | Fully-Sup. | Pyramid Attention, Multiscale Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| SwinPA-Net [[114](#bib.bib114)] | TNNLS | Transformer+UNet | Swin-B [[115](#bib.bib115)]
    | 全监督 | 金字塔注意力, 多尺度融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | 否 |'
- en: '| BSCA-Net [[52](#bib.bib52)] | PR | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Multipath, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | No |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| BSCA-Net [[52](#bib.bib52)] | PR | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 完全监督 | 多路径，注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] |
    550+900 | 否 |'
- en: '| HSNet [[116](#bib.bib116)] | CBM | CNN+Transformer | PVTv2 [[68](#bib.bib68)]+
    Res2Net50[[76](#bib.bib76)] | Fully-Sup. | Cross Attention, Semantic Complementary
    | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 548+900 | [Open](https://github.com/baiboat/HSNet)
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| HSNet [[116](#bib.bib116)] | CBM | CNN+Transformer | PVTv2 [[68](#bib.bib68)]+
    Res2Net50[[76](#bib.bib76)] | 完全监督 | 跨注意力，语义互补 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+900 | [开放](https://github.com/baiboat/HSNet) |'
- en: '| MSRAformer [[59](#bib.bib59)] | CBM | Transformer+UNet | Swin-B [[115](#bib.bib115)]
    | Fully-Sup. | Channel/Reverse Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 490+800 | [Open](https://github.com/ChengLong1222/MSRAformer-main)
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| MSRAformer [[59](#bib.bib59)] | CBM | Transformer+UNet | Swin-B [[115](#bib.bib115)]
    | 完全监督 | 通道/反向注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 490+800 | [开放](https://github.com/ChengLong1222/MSRAformer-main) |'
- en: '| AMNet [[53](#bib.bib53)] | CBM | CNN+UNet | Res2Net50[[76](#bib.bib76)] |
    Fully-Sup. | Multiscale Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 500+800 | No |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| AMNet [[53](#bib.bib53)] | CBM | CNN+UNet | Res2Net50[[76](#bib.bib76)] |
    完全监督 | 多尺度融合，注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] |
    500+800 | 否 |'
- en: '| DBMF [[117](#bib.bib117)] | CBM | Transformer+CNN | Swin-B [[115](#bib.bib115)]+EfficientNet[[111](#bib.bib111)]
    | Fully-Sup. | Multiscale Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| DBMF [[117](#bib.bib117)] | CBM | Transformer+CNN | Swin-B [[115](#bib.bib115)]+EfficientNet[[111](#bib.bib111)]
    | 完全监督 | 多尺度融合，注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | 否 |'
- en: '| ICGNet [[63](#bib.bib63)] | IJCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Local-Global Fusion, Boundary Supervision | EndoScene [[108](#bib.bib108)]+Kvasir-SEG
    [[70](#bib.bib70)] | 547+600 | No |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ICGNet [[63](#bib.bib63)] | IJCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | 完全监督 | 局部-全局融合，边界监督 | EndoScene [[108](#bib.bib108)]+Kvasir-SEG [[70](#bib.bib70)]
    | 547+600 | 否 |'
- en: '| TCCNet [[54](#bib.bib54)] | IJCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Semi-Sup. | Context-Free Loss, Reverse Attention | ClinicDB [[69](#bib.bib69)]+CVC-ColonDB
    [[104](#bib.bib104)] | 367+180 | [Open](https://github.com/wener-yung/TCCNet)
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| TCCNet [[54](#bib.bib54)] | IJCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 半监督 | 无上下文损失，反向注意力 | ClinicDB [[69](#bib.bib69)]+CVC-ColonDB [[104](#bib.bib104)]
    | 367+180 | [开放](https://github.com/wener-yung/TCCNet) |'
- en: '| CoFo [[55](#bib.bib55)] | ISBI | CNN+UNet | ResNet18[[74](#bib.bib74)] |
    Fully-Sup. | Domain Adapation, Adversarial Learning | EndoScene [[108](#bib.bib108)]+Kvasir-SEG
    [[70](#bib.bib70)] | 547+600 | [Open](https://github.com/tadeephuy/CoFo) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| CoFo [[55](#bib.bib55)] | ISBI | CNN+UNet | ResNet18[[74](#bib.bib74)] |
    完全监督 | 领域适应，对抗学习 | EndoScene [[108](#bib.bib108)]+Kvasir-SEG [[70](#bib.bib70)]
    | 547+600 | [开放](https://github.com/tadeephuy/CoFo) |'
- en: '| DCRNet [[42](#bib.bib42)] | ISBI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Contextual-Relation Learning | EndoScene [[108](#bib.bib108)]+Kvasir-SEG
    [[70](#bib.bib70)] + Piccolo[[118](#bib.bib118)] | 547+600 +2203 | [Open](https://github.com/tadeephuy/CoFo)
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| DCRNet [[42](#bib.bib42)] | ISBI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | 完全监督 | 上下文关系学习 | EndoScene [[108](#bib.bib108)]+Kvasir-SEG [[70](#bib.bib70)]
    + Piccolo[[118](#bib.bib118)] | 547+600 +2203 | [开放](https://github.com/tadeephuy/CoFo)
    |'
- en: '| 2021 | MSNet [[33](#bib.bib33)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Multiscale Fusion, Loss Network | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/Xiaoqi-Zhao-DLUT/MSNet-M2SNet)
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | MSNet [[33](#bib.bib33)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 完全监督 | 多尺度融合，损失网络 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [开放](https://github.com/Xiaoqi-Zhao-DLUT/MSNet-M2SNet) |'
- en: '| CCBANet [[64](#bib.bib64)] | MICCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Cascading Context, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/ntcongvn/CCBANet) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| CCBANet [[64](#bib.bib64)] | MICCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | 完全监督 | 级联上下文，注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [开放](https://github.com/ntcongvn/CCBANet) |'
- en: '| HRNet [[65](#bib.bib65)] | MICCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Feature Aggregation, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | No |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| HRNet [[65](#bib.bib65)] | MICCAI | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | 完全监督 | 特征聚合，注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | 否 |'
- en: '| LODNet [[119](#bib.bib119)] | MICCAI | CNN | ResNet50[[74](#bib.bib74)] |
    Fully-Sup. | Oriented, Sensitive Loss | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/midsdsy/LOD-Net) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LODNet [[119](#bib.bib119)] | MICCAI | CNN | ResNet50[[74](#bib.bib74)] |
    完全监督 | 定向，敏感损失 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 550+900
    | [Open](https://github.com/midsdsy/LOD-Net) |'
- en: '| SANet [[34](#bib.bib34)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Color Exchange, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/weijun88/SANet) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| SANet [[34](#bib.bib34)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 完全监督 | 颜色交换，注意力机制 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [Open](https://github.com/weijun88/SANet) |'
- en: '| Transfuse [[7](#bib.bib7)] | MICCAI | Transformer+CNN | Res2Net34[[76](#bib.bib76)]+DeiT-S[[120](#bib.bib120)]
    | Fully-Sup. | Feature Fusion | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [Open](https://github.com/Rayicer/TransFuse) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Transfuse [[7](#bib.bib7)] | MICCAI | Transformer+CNN | Res2Net34[[76](#bib.bib76)]+DeiT-S[[120](#bib.bib120)]
    | 完全监督 | 特征融合 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 550+900
    | [Open](https://github.com/Rayicer/TransFuse) |'
- en: '| CCD [[121](#bib.bib121)] | MICCAI | CNN+UNet | ResNet18[[74](#bib.bib74)]
    | Self/Un-Sup. | Contrastive Learning, Anomaly Localisation | HyperKvasir [[85](#bib.bib85)]
    | 2100 | [Open](https://github.com/tianyu0207/CCD) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| CCD [[121](#bib.bib121)] | MICCAI | CNN+UNet | ResNet18[[74](#bib.bib74)]
    | 自监督/无监督 | 对比学习，异常定位 | HyperKvasir [[85](#bib.bib85)] | 2100 | [Open](https://github.com/tianyu0207/CCD)
    |'
- en: '| Polyformer [[122](#bib.bib122)] | MICCAI | Transformer+UNet | Segtran[[123](#bib.bib123)]
    | Fully-Sup. | Few-Shot Learning, Domain Adaptation | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/askerlee/segtran) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Polyformer [[122](#bib.bib122)] | MICCAI | Transformer+UNet | Segtran[[123](#bib.bib123)]
    | 完全监督 | 少样本学习，领域适应 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [Open](https://github.com/askerlee/segtran) |'
- en: '| PNet [[124](#bib.bib124)] | MICCAI | CNN | Res2Net50[[76](#bib.bib76)] |
    Fully-Sup. | Feature Aggregation, Graph Convolution | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/DengPingFan/PraNet) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| PNet [[124](#bib.bib124)] | MICCAI | CNN | Res2Net50[[76](#bib.bib76)] |
    完全监督 | 特征聚合，图卷积 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] |
    550+900 | [Open](https://github.com/DengPingFan/PraNet) |'
- en: '| HieraSeg [[125](#bib.bib125)] | MIA | CNN+FCN | Deeplabv3+[[126](#bib.bib126)]
    | Fully-Sup. | Hierarchical, Dynamic Weighting | EndoScene[[108](#bib.bib108)]
    | 547 | [Open](https://github.com/CityU-AIM-Group/DW-HieraSeg) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| HieraSeg [[125](#bib.bib125)] | MIA | CNN+FCN | Deeplabv3+[[126](#bib.bib126)]
    | 完全监督 | 层次化，动态加权 | EndoScene[[108](#bib.bib108)] | 547 | [Open](https://github.com/CityU-AIM-Group/DW-HieraSeg)
    |'
- en: '| ThresholdNet [[127](#bib.bib127)] | TMI | CNN+FCN | Deeplabv3+[[126](#bib.bib126)]
    | Fully-Sup. | Mixup, Threshold Loss | EndoScene[[108](#bib.bib108)] | 547 | [Open](https://github.com/Guo-Xiaoqing/ThresholdNetv)
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| ThresholdNet [[127](#bib.bib127)] | TMI | CNN+FCN | Deeplabv3+[[126](#bib.bib126)]
    | 完全监督 | Mixup，阈值损失 | EndoScene[[108](#bib.bib108)] | 547 | [Open](https://github.com/Guo-Xiaoqing/ThresholdNetv)
    |'
- en: '| FANet [[35](#bib.bib35)] | TNNLS | CNN+UNet | N/A | Fully-Sup. | Iterative
    Refining, Feedback Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [Open](https://github.com/nikhilroxtomar/FANet) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| FANet [[35](#bib.bib35)] | TNNLS | CNN+UNet | 无 | 完全监督 | 迭代优化，反馈注意力 | ClinicDB
    [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/nikhilroxtomar/FANet)
    |'
- en: '| MPA-DA [[128](#bib.bib128)] | JBHI | CNN+UNet | ResNet101[[74](#bib.bib74)]
    | Fully-Sup. | Domain Adaptation, Self-training | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 548+1000 | [Open](https://github.com/CityU-AIM-Group/MPA-DA)
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| MPA-DA [[128](#bib.bib128)] | JBHI | CNN+UNet | ResNet101[[74](#bib.bib74)]
    | 完全监督 | 领域适应，自训练 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 548+1000 | [Open](https://github.com/CityU-AIM-Group/MPA-DA) |'
- en: '| ResUNet++[[129](#bib.bib129)] | JBHI | CNN+UNet | ResUNet[[130](#bib.bib130)]
    | Fully-Sup. | Test Time Augmentation | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/DebeshJha/ResUNetPlusPlus-with-CRF-and-TTA)
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ResUNet++[[129](#bib.bib129)] | JBHI | CNN+UNet | ResUNet[[130](#bib.bib130)]
    | 完全监督 | 测试时间增强 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] |
    550+900 | [Open](https://github.com/DebeshJha/ResUNetPlusPlus-with-CRF-and-TTA)
    |'
- en: '| SCRNet [[47](#bib.bib47)] | AAAI | CNN+UNet | FPN[[131](#bib.bib131)] | Fully-Sup.
    | Semantic Calibration/Refinement | Kvasir-SEG [[70](#bib.bib70)] | 700 | No |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| SCRNet [[47](#bib.bib47)] | AAAI | CNN+UNet | FPN[[131](#bib.bib131)] | 完全监督
    | 语义校准/细化 | Kvasir-SEG [[70](#bib.bib70)] | 700 | 否 |'
- en: '| CAFD [[132](#bib.bib132)] | ICCV | CNN+UNet | ResNet50[[74](#bib.bib74)]
    | Semi-Sup. | Adversarial Learning, Collaborative Learning | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 306+500 | No |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| CAFD [[132](#bib.bib132)] | ICCV | CNN+UNet | ResNet50[[74](#bib.bib74)]
    | 半监督 | 对抗学习，协作学习 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 306+500 | 否 |'
- en: '| UACANet [[66](#bib.bib66)] | ACM MM | CNN+UNet | Res2Net[[76](#bib.bib76)]
    | Fully-Sup. | Uncertainty, Self-attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/plemeri/UACANet) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| UACANet [[66](#bib.bib66)] | ACM MM | CNN+UNet | Res2Net[[76](#bib.bib76)]
    | 完全监督 | 不确定性，自注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [开放](https://github.com/plemeri/UACANet) |'
- en: '| Segtran[[123](#bib.bib123)] | IJCAI | Transformer+CNN | ResNet101[[74](#bib.bib74)]
    | Fully-Sup. | Squeezed Attention, Positional Encoding | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 489+800 | [Open](https://github.com/askerlee/segtran) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Segtran[[123](#bib.bib123)] | IJCAI | Transformer+CNN | ResNet101[[74](#bib.bib74)]
    | 完全监督 | 压缩注意力，位置编码 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 489+800 | [开放](https://github.com/askerlee/segtran) |'
- en: '| DenseUNet [[133](#bib.bib133)] | Sensors | CNN+UNet | DenseNet[[88](#bib.bib88)]
    | Fully-Sup. | Dense Connections, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 490+800 | No |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| DenseUNet [[133](#bib.bib133)] | Sensors | CNN+UNet | DenseNet[[88](#bib.bib88)]
    | 完全监督 | 密集连接，注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 490+800 | 否 |'
- en: '| EUNet [[134](#bib.bib134)] | CVR | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Semantic Enhancement, Global Context | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/rucv/Enhanced-U-Net)
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| EUNet [[134](#bib.bib134)] | CVR | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | 完全监督 | 语义增强，全球上下文 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [开放](https://github.com/rucv/Enhanced-U-Net) |'
- en: '| DDANet [[58](#bib.bib58)] | ICPR | CNN+UNet | ResUNet[[130](#bib.bib130)]
    | Fully-Sup. | Dual Decoder | Kvasir-SEG [[70](#bib.bib70)] | 880 | [Open](https://github.com/nikhilroxtomar/DDANet)
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| DDANet [[58](#bib.bib58)] | ICPR | CNN+UNet | ResUNet[[130](#bib.bib130)]
    | 完全监督 | 双解码器 | Kvasir-SEG [[70](#bib.bib70)] | 880 | [开放](https://github.com/nikhilroxtomar/DDANet)
    |'
- en: '| FUNet [[135](#bib.bib135)] | CBM | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | Fully-Sup. | Dual Attention, Gate | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | No |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| FUNet [[135](#bib.bib135)] | CBM | CNN+UNet | ResNet34[[74](#bib.bib74)]
    | 完全监督 | 双重注意力，门控 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | 否 |'
- en: '| T-UNet [[136](#bib.bib136)] | Healthcare | CNN+UNet | ResNet50[[74](#bib.bib74)]
    | Fully-Sup. | Dilated Convolution, Feature Fusion | ClinicDB [[69](#bib.bib69)]
    | 489 | No |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| T-UNet [[136](#bib.bib136)] | Healthcare | CNN+UNet | ResNet50[[74](#bib.bib74)]
    | 完全监督 | 膨胀卷积，特征融合 | ClinicDB [[69](#bib.bib69)] | 489 | 否 |'
- en: '| DNets[[137](#bib.bib137)] | ISBI | CNN+FCN | FPN[[131](#bib.bib131)]+Deeplabv3+[[126](#bib.bib126)]
    | Fully-Sup. | Ensemble Learning | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 489+800 | [Open](https://github.com/vlbthambawita/divergent-nets) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| DNets[[137](#bib.bib137)] | ISBI | CNN+FCN | FPN[[131](#bib.bib131)]+Deeplabv3+[[126](#bib.bib126)]
    | 完全监督 | 集成学习 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 489+800
    | [开放](https://github.com/vlbthambawita/divergent-nets) |'
- en: '|  | AAU [[48](#bib.bib48)] | ISBI | CNN+FCN | ResNet34[[74](#bib.bib74)] |
    Fully-Sup. | Asymmetric Attention, Upsampling | Kvasir-SEG [[70](#bib.bib70)]
    | 800 | No |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | AAU [[48](#bib.bib48)] | ISBI | CNN+FCN | ResNet34[[74](#bib.bib74)] |
    完全监督 | 非对称注意力，上采样 | Kvasir-SEG [[70](#bib.bib70)] | 800 | 否 |'
- en: '|  | BI-GCN [[138](#bib.bib138)] | BMVC | GCN | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Feature Aggregation, Graph Convolution | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 550+900 | [Open](https://github.com/smallmax00/BI-GConv)
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | BI-GCN [[138](#bib.bib138)] | BMVC | GCN | Res2Net50[[76](#bib.bib76)]
    | 完全监督 | 特征聚合，图卷积 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 550+900 | [开放](https://github.com/smallmax00/BI-GConv) |'
- en: '|   |  |  |  |  |  |  |  |  |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |'
- en: 'TABLE II: Summary of popular deep CPS methods from 2021 to 2023, including
    network architecture, backbone, level of supervision, learning paradigm, training
    dataset and the source code. Section [2](#S2 "2 Methodology (Survey) ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey") gives more
    detailed descriptions.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 2021年至2023年流行的深度CPS方法的总结，包括网络架构、骨干网络、监督水平、学习范式、训练数据集和源代码。第[2](#S2 "2
    Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era:
    A Comprehensive Survey")节提供了更详细的描述。'
- en: '|   Year | Methods | Publication | Architecture | Backbone | Supervision |
    Learning Paradigm | Training Dataset | Number | Available |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|   年份 | 方法 | 发表 | 架构 | 骨干网络 | 监督 | 学习范式 | 训练数据集 | 数量 | 可用性 |'
- en: '| 2020 | ACSNet [[67](#bib.bib67)] | MICCAI | CNN+UNet | ResNet34 [[74](#bib.bib74)]
    | Fully-Sup. | Adaptive Selection, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 547+600 | [Open](https://github.com/ReaFly/ACSNet) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | ACSNet [[67](#bib.bib67)] | MICCAI | CNN+UNet | ResNet34 [[74](#bib.bib74)]
    | 完全监督 | 自适应选择，注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 547+600 | [开放](https://github.com/ReaFly/ACSNet) |'
- en: '| MI²GAN [[139](#bib.bib139)] | MICCAI | GAN | CycleGAN [[140](#bib.bib140)]
    | Fully-Sup. | Domain Adaptation, Adversarial Learning | ClinicDB [[69](#bib.bib69)]
    | 490 | No |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| MI²GAN [[139](#bib.bib139)] | MICCAI | GAN | CycleGAN [[140](#bib.bib140)]
    | 完全监督 | 域适应，对抗学习 | ClinicDB [[69](#bib.bib69)] | 490 | 否 |'
- en: '| PraNet [[6](#bib.bib6)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | Fully-Sup. | Feature Aggregating, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 547+800 | [Open](https://github.com/DengPingFan/PraNet) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| PraNet [[6](#bib.bib6)] | MICCAI | CNN+UNet | Res2Net50[[76](#bib.bib76)]
    | 完全监督 | 特征聚合，注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 547+800 | [开放](https://github.com/DengPingFan/PraNet) |'
- en: '| Polypseg [[141](#bib.bib141)] | MICCAI | CNN+UNet | UNet [[3](#bib.bib3)]
    | Fully-Sup. | Context-aware, Semantic | Kvasir-SEG [[70](#bib.bib70)] | 600 |
    No |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Polypseg [[141](#bib.bib141)] | MICCAI | CNN+UNet | UNet [[3](#bib.bib3)]
    | 完全监督 | 上下文感知，语义 | Kvasir-SEG [[70](#bib.bib70)] | 600 | 否 |'
- en: '| ABCNet [[142](#bib.bib142)] | Sensors | CNN+FCN | ResNeXt50 [[143](#bib.bib143)]
    | Fully-Sup. | Boundary Loss-aware, Two-stream | Kvasir-SEG [[70](#bib.bib70)]+EndoScene[[108](#bib.bib108)]
    | 800+547 | No |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| ABCNet [[142](#bib.bib142)] | Sensors | CNN+FCN | ResNeXt50 [[143](#bib.bib143)]
    | 完全监督 | 边界损失感知，两流 | Kvasir-SEG [[70](#bib.bib70)]+EndoScene[[108](#bib.bib108)]
    | 800+547 | 否 |'
- en: '| UINet [[144](#bib.bib144)] | MIA | CNN | Res2Net50[[76](#bib.bib76)] | Fully-Sup.
    | Uncertainty, Interpretability | EndoScene[[108](#bib.bib108)] | 547 | No |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| UINet [[144](#bib.bib144)] | MIA | CNN | Res2Net50[[76](#bib.bib76)] | 完全监督
    | 不确定性，可解释性 | EndoScene[[108](#bib.bib108)] | 547 | 否 |'
- en: '| MCNet [[145](#bib.bib145)] | JBHI | CNN+UNet | VGG16[[93](#bib.bib93)] |
    Fully-Sup. | Global Semantic, Local Detail | ClinicDB [[69](#bib.bib69)]+[EndoVC](https://endovissub-abnormal.grand-challenge.org/)
    | 412+465 | No |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| MCNet [[145](#bib.bib145)] | JBHI | CNN+UNet | VGG16[[93](#bib.bib93)] |
    完全监督 | 全局语义，本地细节 | ClinicDB [[69](#bib.bib69)]+[EndoVC](https://endovissub-abnormal.grand-challenge.org/)
    | 412+465 | 否 |'
- en: '| SSN [[44](#bib.bib44)] | ISBI | CNN+FCN | ResNet [[74](#bib.bib74)] | Fully-Sup.
    | Multi-scale Fusion, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 547+800 | No |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| SSN [[44](#bib.bib44)] | ISBI | CNN+FCN | ResNet [[74](#bib.bib74)] | 完全监督
    | 多尺度融合，注意力 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 547+800
    | 否 |'
- en: '| RCIS [[146](#bib.bib146)] | ICARM | CNN+FCN | ResNet [[74](#bib.bib74)] |
    Fully-Sup. | Knowledge Distillation | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 547+800 | No |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| RCIS [[146](#bib.bib146)] | ICARM | CNN+FCN | ResNet [[74](#bib.bib74)] |
    完全监督 | 知识蒸馏 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)] | 547+800
    | 否 |'
- en: '| Double-UNet [[147](#bib.bib147)] | CBMS | CNN+UNet | VGG19[[93](#bib.bib93)]
    | Fully-Sup. | Multi-branch, Feature Fusion | ClinicDB [[69](#bib.bib69)]+ETIS-Larib
    [[103](#bib.bib103)] | 490+157 | No |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Double-UNet [[147](#bib.bib147)] | CBMS | CNN+UNet | VGG19[[93](#bib.bib93)]
    | 完全监督 | 多分支，特征融合 | ClinicDB [[69](#bib.bib69)]+ETIS-Larib [[103](#bib.bib103)]
    | 490+157 | 否 |'
- en: '| 2019 | SFANet [[18](#bib.bib18)] | MICCAI | CNN+UNet | ResNet34 [[74](#bib.bib74)]
    | Fully-Sup. | Selective Aggregation, Boundary Loss | EndoScene[[108](#bib.bib108)]
    | 730 | No |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | SFANet [[18](#bib.bib18)] | MICCAI | CNN+UNet | ResNet34 [[74](#bib.bib74)]
    | 完全监督 | 选择性聚合，边界损失 | EndoScene[[108](#bib.bib108)] | 730 | 否 |'
- en: '| TDE [[148](#bib.bib148)] | CBMS | CNN+UNet | ResNet34 [[74](#bib.bib74)]
    | Fully-Sup. | Data Augmentation | ClinicDB [[69](#bib.bib69)] | 612 | No |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| TDE [[148](#bib.bib148)] | CBMS | CNN+UNet | ResNet34 [[74](#bib.bib74)]
    | 完全监督 | 数据增强 | ClinicDB [[69](#bib.bib69)] | 612 | 否 |'
- en: '| PsiNet [[149](#bib.bib149)] | EMBC | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Boundary, Multi-task Learning | EndoScene[[108](#bib.bib108)] |
    638 | [Open](https://github.com/Bala93/Multi-task-deep-network) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| PsiNet [[149](#bib.bib149)] | EMBC | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | 完全监督 | 边界，多任务学习 | EndoScene[[108](#bib.bib108)] | 638 | [开放](https://github.com/Bala93/Multi-task-deep-network)
    |'
- en: '| PSGAN [[150](#bib.bib150)] | EMBC | GAN | ResNet50 [[74](#bib.bib74)] | Fully-Sup.
    | Generative Adversarial Network | ClinicDB [[69](#bib.bib69)] | 488 | No |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| PSGAN [[150](#bib.bib150)] | EMBC | GAN | ResNet50 [[74](#bib.bib74)] | 完全监督
    | 生成对抗网络 | ClinicDB [[69](#bib.bib69)] | 488 | 否 |'
- en: '| CPSUNet[[151](#bib.bib151)] | ICMLA | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Dilated Convolution, Morphological | ClinicDB [[69](#bib.bib69)]+
    GIANA[[152](#bib.bib152)] | 300+56 | No |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| CPSUNet[[151](#bib.bib151)] | ICMLA | CNN+UNet | ResNet50 [[74](#bib.bib74)]
    | 完全监督 | 膨胀卷积，形态学 | ClinicDB [[69](#bib.bib69)]+ GIANA[[152](#bib.bib152)] | 300+56
    | 否 |'
- en: '| ResUNet++ [[5](#bib.bib5)] | ISM | CNN+UNet | ResUNet [[130](#bib.bib130)]
    | Fully-Sup. | Squeeze-Excitation, Attention | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG
    [[70](#bib.bib70)] | 547+800 | [Open](https://github.com/DebeshJha/ResUNetPlusPlus-with-CRF-and-TTA)
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| ResUNet++ [[5](#bib.bib5)] | ISM | CNN+UNet | ResUNet [[130](#bib.bib130)]
    | 完全监督 | 压缩激励, 注意力机制 | ClinicDB [[69](#bib.bib69)]+Kvasir-SEG [[70](#bib.bib70)]
    | 547+800 | [开放](https://github.com/DebeshJha/ResUNetPlusPlus-with-CRF-and-TTA)
    |'
- en: '| PDS [[153](#bib.bib153)] | ISMICT | CNN | Maks R-CNN [[154](#bib.bib154)]
    | Fully-Sup. | Feature Extractor, Ensemble | ClinicDB [[69](#bib.bib69)]+ ETIS-Larib[[103](#bib.bib103)]
    | 547+157 | No |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| PDS [[153](#bib.bib153)] | ISMICT | CNN | Maks R-CNN [[154](#bib.bib154)]
    | 完全监督 | 特征提取, 集成 | ClinicDB [[69](#bib.bib69)]+ ETIS-Larib[[103](#bib.bib103)]
    | 547+157 | 否 |'
- en: '| GIANA [[155](#bib.bib155)] | IJCCV | CNN+FCN | ResNet50 [[74](#bib.bib74)]
    | Fully-Sup. | Dilated Convolution, Squeeze-Excitation | ClinicDB [[69](#bib.bib69)]+
    GIANA[[152](#bib.bib152)] | 300+56 | No |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| GIANA [[155](#bib.bib155)] | IJCCV | CNN+FCN | ResNet50 [[74](#bib.bib74)]
    | 完全监督 | 膨胀卷积, 压缩激励 | ClinicDB [[69](#bib.bib69)]+ GIANA[[152](#bib.bib152)] |
    300+56 | 否 |'
- en: '| CCS [[156](#bib.bib156)] | EMBC | CNN | LinkNet [[157](#bib.bib157)] | Fully-Sup.
    | Color Space Transformations | CVC-ColonDB [[104](#bib.bib104)] | 284 | No |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| CCS [[156](#bib.bib156)] | EMBC | CNN | LinkNet [[157](#bib.bib157)] | 完全监督
    | 颜色空间变换 | CVC-ColonDB [[104](#bib.bib104)] | 284 | 否 |'
- en: '| APS [[158](#bib.bib158)] | Med Phys | CNN+UNet | VGG16 [[93](#bib.bib93)]
    | Fully-Sup. | Data Augmentation, Ensemble Learning | ClinicDB [[69](#bib.bib69)]+
    ETIS-Larib[[103](#bib.bib103)] | 547+157 | No |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| APS [[158](#bib.bib158)] | Med Phys | CNN+UNet | VGG16 [[93](#bib.bib93)]
    | 完全监督 | 数据增强, 集成学习 | ClinicDB [[69](#bib.bib69)]+ ETIS-Larib[[103](#bib.bib103)]
    | 547+157 | 否 |'
- en: '| 2014-2018 | PSFCN [[45](#bib.bib45)] | EMBC | CNN+FCN | FCN-8S [[36](#bib.bib36)]
    | Fully-Sup. | Patch Selection, Data Augmentation | CVC-ColonDB [[104](#bib.bib104)]
    | 200 | No |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2014-2018 | PSFCN [[45](#bib.bib45)] | EMBC | CNN+FCN | FCN-8S [[36](#bib.bib36)]
    | 完全监督 | 裁剪选择, 数据增强 | CVC-ColonDB [[104](#bib.bib104)] | 200 | 否 |'
- en: '| ACPS [[159](#bib.bib159)] | CBM | N/A | N/A | Un-Sup. | Image Preprocessing,
    Edge Detection | N/A | N/A | No |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| ACPS [[159](#bib.bib159)] | CBM | 不适用 | 不适用 | 无监督 | 图像预处理, 边缘检测 | 不适用 | 不适用
    | 否 |'
- en: '| UMI [[160](#bib.bib160)] | MLSP | CNN+FCN | SegNet [[161](#bib.bib161)] |
    Fully-Sup. | Uncertainty Modeling, Interpretability | EndoScene[[108](#bib.bib108)]
    | 547 | No |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| UMI [[160](#bib.bib160)] | MLSP | CNN+FCN | SegNet [[161](#bib.bib161)] |
    完全监督 | 不确定性建模, 可解释性 | EndoScene[[108](#bib.bib108)] | 547 | 否 |'
- en: '| UNet++ [[4](#bib.bib4)] | DLMIA | CNN+UNet | UNet [[3](#bib.bib3)] | Fully-Sup.
    | Skip Pathways, Deep Supervision | ASU-Mayo [[104](#bib.bib104)] | 7,379 | [Open](https://github.com/MrGiovanni/UNetPlusPlus)
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| UNet++ [[4](#bib.bib4)] | DLMIA | CNN+UNet | UNet [[3](#bib.bib3)] | 完全监督
    | 跳跃路径, 深度监督 | ASU-Mayo [[104](#bib.bib104)] | 7,379 | [开放](https://github.com/MrGiovanni/UNetPlusPlus)
    |'
- en: '| RTPS [[46](#bib.bib46)] | MMM | CNN+FCN | FCN-8S [[36](#bib.bib36)] | Fully-Sup.
    | Real-time, Compression | EndoScene[[108](#bib.bib108)] | 547 | No |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| RTPS [[46](#bib.bib46)] | MMM | CNN+FCN | FCN-8S [[36](#bib.bib36)] | 完全监督
    | 实时, 压缩 | EndoScene[[108](#bib.bib108)] | 547 | 否 |'
- en: '| MED [[37](#bib.bib37)] | AIKE | CNN+FCN | Deeplabv3 [[126](#bib.bib126)]
    | Fully-Sup. | Database Augmentation, Multimodal | ClinicDB [[69](#bib.bib69)]
    | 547 | No |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| MED [[37](#bib.bib37)] | AIKE | CNN+FCN | Deeplabv3 [[126](#bib.bib126)]
    | 完全监督 | 数据库增强, 多模态 | ClinicDB [[69](#bib.bib69)] | 547 | 否 |'
- en: '| FCNet [[40](#bib.bib40)] | SPIE MI | CNN+FCN | VGG16 [[93](#bib.bib93)] |
    Fully-Sup. | Fully Convolution Networks | ClinicDB [[69](#bib.bib69)] | 612 |
    No |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| FCNet [[40](#bib.bib40)] | SPIE MI | CNN+FCN | VGG16 [[93](#bib.bib93)] |
    完全监督 | 完全卷积网络 | ClinicDB [[69](#bib.bib69)] | 612 | 否 |'
- en: '| CPFCNet [[39](#bib.bib39)] | BMEI | CNN+FCN | FCN[[36](#bib.bib36)] | Fully-Sup.
    | Fully Convolution Networks | ClinicDB [[69](#bib.bib69)] | 428 | No |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| CPFCNet [[39](#bib.bib39)] | BMEI | CNN+FCN | FCN[[36](#bib.bib36)] | 完全监督
    | 完全卷积网络 | ClinicDB [[69](#bib.bib69)] | 428 | 否 |'
- en: '| APSNet [[38](#bib.bib38)] | MIUA | CNN+FCN | FCN-8s [[36](#bib.bib36)] |
    Fully-Sup. | Region Proposals, Textons | CVC-ColonDB [[104](#bib.bib104)] | 200
    | No |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| APSNet [[38](#bib.bib38)] | MIUA | CNN+FCN | FCN-8s [[36](#bib.bib36)] |
    完全监督 | 区域提议, 纹理 | CVC-ColonDB [[104](#bib.bib104)] | 200 | 否 |'
- en: '| SuperSeg [[162](#bib.bib162)] | SPMB | N/A | N/A | Un-Sup. | Superpixel Segmentation,
    SVM | N/A | N/A | No |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| SuperSeg [[162](#bib.bib162)] | SPMB | 不适用 | 不适用 | 无监督 | 超像素分割, 支持向量机 | 不适用
    | 不适用 | 否 |'
- en: '| APD [[17](#bib.bib17)] | JBHI | MLP | N/A | Un-Sup. | Superpixel, Autoencoder,
    Saliency | N/A | N/A | No |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| APD [[17](#bib.bib17)] | JBHI | MLP | 不适用 | 无监督 | 超像素, 自编码器, 显著性 | 不适用 |
    不适用 | 否 |'
- en: '| WM-DOVA [[69](#bib.bib69)] | CMIG | N/A | N/A | Un-Sup. | Boundary Constraints,
    Saliency | N/A | N/A | No |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| WM-DOVA [[69](#bib.bib69)] | CMIG | 不适用 | 不适用 | 无监督 | 边界约束, 显著性 | 不适用 | 不适用
    | 否 |'
- en: '| PAD-WCE [[163](#bib.bib163)] | ROBIO | N/A | N/A | Un-Sup. | K-means Clustering,
    Contour | N/A | N/A | No |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| PAD-WCE [[163](#bib.bib163)] | ROBIO | 不适用 | 不适用 | 无监督 | K-means 聚类, 轮廓 |
    不适用 | 不适用 | 否 |'
- en: '| MSA-DOVA [[16](#bib.bib16)] | TRMI | N/A | N/A | Un-Sup. | Valley Fnformation,
    Energy Maps | N/A | N/A | No |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| MSA-DOVA [[16](#bib.bib16)] | TRMI | N/A | N/A | 无监督 | Valley Fnformation,
    Energy Maps | N/A | N/A | 否 |'
- en: '|   |  |  |  |  |  |  |  |  |  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |'
- en: 'TABLE III: Summary of popular deep CPS methods from 2014 to 2020\. For the
    completeness, some early relevant works ranging from 2014 to 2018 have also been
    stated briefly.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：2014年至2020年流行深度 CPS 方法的总结。为确保完整性，也简要列出了2014年至2018年的一些早期相关工作。
- en: 2.1.3 Transformer-based Method
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 基于 Transformer 的方法
- en: The transformer architecture was first introduced by [[164](#bib.bib164)] published
    in 2017\. Compared to CNN, transformer architecture can capture long-range dependencies
    in sequences, making it suitable for understanding global context. Recently, Ling
    et al. [[9](#bib.bib9)] proposed a novel Gaussian-probabilistic guided semantic
    fusion method for polyp segmentation. Wang et al. [[8](#bib.bib8)] introduce a
    pyramid transformer architecture for the polyp segmentation task to increase the
    generalization ability of the neural network. Xiao et al. [[112](#bib.bib112)]
    proposed a transformer-based network for robust and accurate polyp segmentation
    by mimicking the preliminary-to-refined working paradigm of doctors. Chen et al.
    [[113](#bib.bib113)] proposes a novel complement local transformer-based network
    architecture for medical small object segmentation, which can complement local
    detailed information when up-sampling global features. Du et al. [[114](#bib.bib114)]
    introduce a novel method called the swin pyramid aggregation network, which introduces
    two designed modules into the network with a swin transformer as the backbone
    to learn more powerful and robust features. Wu et al. [[59](#bib.bib59)] presents
    a multiscale spatial reverse attention network that adopts the Swin Transformer
    encoder with a pyramid structure to extract the features of four different stages.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构首次由[[164](#bib.bib164)]在2017年提出。与 CNN 相比，Transformer 架构能够捕捉序列中的长程依赖，使其适合理解全局上下文。最近，Ling
    等人[[9](#bib.bib9)]提出了一种新颖的高斯概率引导语义融合方法用于息肉分割。Wang 等人[[8](#bib.bib8)]介绍了一种用于息肉分割任务的金字塔
    Transformer 架构，以提高神经网络的泛化能力。Xiao 等人[[112](#bib.bib112)]提出了一种基于 Transformer 的网络，通过模仿医生从初步到精细的工作范式，实现了稳健且准确的息肉分割。Chen
    等人[[113](#bib.bib113)]提出了一种新颖的局部 Transformer 网络架构用于医学小物体分割，该方法在上采样全局特征时能够补充局部详细信息。Du
    等人[[114](#bib.bib114)]介绍了一种新方法，称为 swin pyramid aggregation network，该方法在网络中引入了两个设计模块，并以
    swin transformer 作为骨干网，以学习更强大和更稳健的特征。Wu 等人[[59](#bib.bib59)]展示了一种多尺度空间反向注意力网络，采用了具有金字塔结构的
    Swin Transformer 编码器，以提取四个不同阶段的特征。
- en: 2.1.4 Hybrid-network Based Method
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 混合网络基方法
- en: 'Very recently, as shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.2 CNN-based Method
    ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")(g), the researcher attempted
    to combine CNN with transformer for polyp segmentation, aiming at separately capturing
    local and long-term features from CNN and transformer, respectively. Jha et al.
    [[90](#bib.bib90)] proposed a transformer-based residual network for colon polyp
    segmentation and evaluated its diagnostic performance. Cai et al. [[105](#bib.bib105)]
    proposed a hybrid framework for the polyp segmentation task, in which the encoder
    consists of a deep Transformer branch and a shallow CNN branch to extract rich
    features. Sanderson et al. [[110](#bib.bib110)] proposed a new architecture for
    polyp segmentation in colonoscopy images, which combines FCNs and transformers
    to achieve state-of-the-art results. Huang et al. [[61](#bib.bib61)] presented
    TransMixer, a hybrid interaction fusion architecture of the Transformer branch
    and the CNN branch, which can enhance the local details of global representations
    and the global context awareness of local features. Zhang et al. [[116](#bib.bib116)]
    introduced a hybrid semantic network by combining CNN and Transformer, aiming
    at separately capturing local and long-term features in the polyp. Liu et al.
    [[117](#bib.bib117)] proposed a dual branch multiscale feature fusion network
    for Polyp Segmentation, which uses CNN and Transformer in parallel to extract
    multiscale local information and global contextual information, respectively.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，如图 [4](#S2.F4 "Figure 4 ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")(g) 所示，研究人员尝试将CNN与变压器结合用于息肉分割，旨在分别从CNN和变压器中捕捉局部和长期特征。Jha
    等人 [[90](#bib.bib90)] 提出了一个基于变压器的残差网络用于结肠息肉分割，并评估了其诊断性能。Cai 等人 [[105](#bib.bib105)]
    提出了一个用于息肉分割任务的混合框架，其中编码器由深层变压器分支和浅层CNN分支组成，以提取丰富的特征。Sanderson 等人 [[110](#bib.bib110)]
    提出了一个新的结肠镜图像息肉分割架构，结合了FCNs和变压器以实现最先进的结果。Huang 等人 [[61](#bib.bib61)] 提出了TransMixer，一种变压器分支和CNN分支的混合交互融合架构，能够增强全局表示的局部细节和局部特征的全局上下文感知。Zhang
    等人 [[116](#bib.bib116)] 介绍了一种通过结合CNN和变压器的混合语义网络，旨在分别捕捉息肉中的局部和长期特征。Liu 等人 [[117](#bib.bib117)]
    提出了一个双分支多尺度特征融合网络用于息肉分割，该网络并行使用CNN和变压器，分别提取多尺度的局部信息和全局上下文信息。'
- en: 2.2 Level of Supervision
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 监督级别
- en: 'According to the level of supervision, existing deep CPS methods can be categorized
    into fully-supervised (Sec. [2.2.1](#S2.SS2.SSS1 "2.2.1 Fully-Supervised Methods
    ‣ 2.2 Level of Supervision ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")), semi-supervised (Sec. [2.2.2](#S2.SS2.SSS2
    "2.2.2 Semi-Supervised Methods ‣ 2.2 Level of Supervision ‣ 2 Methodology (Survey)
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")),
    weakly-supervised (Sec. [2.2.3](#S2.SS2.SSS3 "2.2.3 Weakly-Supervised Methods
    ‣ 2.2 Level of Supervision ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")), and self-/un-supervised approaches
    (Sec. [2.2.4](#S2.SS2.SSS4 "2.2.4 Self-/Un-Supervised Methods ‣ 2.2 Level of Supervision
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '根据监督级别，现有的深度CPS方法可以分为完全监督（第 [2.2.1](#S2.SS2.SSS1 "2.2.1 Fully-Supervised Methods
    ‣ 2.2 Level of Supervision ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey") 节）、半监督（第 [2.2.2](#S2.SS2.SSS2
    "2.2.2 Semi-Supervised Methods ‣ 2.2 Level of Supervision ‣ 2 Methodology (Survey)
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")
    节）、弱监督（第 [2.2.3](#S2.SS2.SSS3 "2.2.3 Weakly-Supervised Methods ‣ 2.2 Level of
    Supervision ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep
    Learning Era: A Comprehensive Survey") 节）和自监督/无监督方法（第 [2.2.4](#S2.SS2.SSS4 "2.2.4
    Self-/Un-Supervised Methods ‣ 2.2 Level of Supervision ‣ 2 Methodology (Survey)
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")
    节）。'
- en: 2.2.1 Fully-Supervised Methods
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 完全监督方法
- en: In a fully supervised learning paradigm, a deep model trained on a dataset with
    pixel-level annotations, which heavily relies on the extensive manual annotation
    to ensure remarkable performance, and most current deep CPS methods [[27](#bib.bib27),
    [28](#bib.bib28), [49](#bib.bib49), [8](#bib.bib8), [105](#bib.bib105), [30](#bib.bib30),
    [31](#bib.bib31), [51](#bib.bib51), [110](#bib.bib110), [56](#bib.bib56), [61](#bib.bib61),
    [33](#bib.bib33)] are based on fully supervised learning. Yin et al. [[42](#bib.bib42)]
    proposed a duplex contextual relation network to simultaneously capture the contextual
    relations across images and within individual images, with 3350 pixel-wise annotations.
    Zhang et al. [[49](#bib.bib49)] designed a lesion-aware dynamic network for the
    polyp segmentation task with 1348 pixel-wise labels. Wang et al. [[8](#bib.bib8)]
    uses a pyramid Transformer encoder to improve the generalization ability of models.
    Cai et al. [[105](#bib.bib105)] propose a PPFormer for accurate polyp segmentation,
    which takes advantage of the Transformer’s long-range modeling ability and CNN’s
    local feature extraction. Yue et al. [[32](#bib.bib32)] proposed a novel boundary
    constraint network where the polyp segmentation task is completed in a fully supervised
    manner. Ji et al. [[2](#bib.bib2)] design a multistream network for video polyp
    segmentation, with 19,554 SUN-SEG data for training.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全监督学习范式中，深度模型在具有像素级标注的数据集上进行训练，这种方法严重依赖广泛的人工标注以确保显著的性能，大多数当前的深度CPS方法[[27](#bib.bib27),
    [28](#bib.bib28), [49](#bib.bib49), [8](#bib.bib8), [105](#bib.bib105), [30](#bib.bib30),
    [31](#bib.bib31), [51](#bib.bib51), [110](#bib.bib110), [56](#bib.bib56), [61](#bib.bib61),
    [33](#bib.bib33)] 基于完全监督学习。Yin等人[[42](#bib.bib42)]提出了一种双重上下文关系网络，旨在同时捕捉图像间和图像内的上下文关系，使用了3350个像素级标注。Zhang等人[[49](#bib.bib49)]设计了一种病灶感知动态网络，用于息肉分割任务，使用了1348个像素级标签。Wang等人[[8](#bib.bib8)]使用金字塔Transformer编码器来提高模型的泛化能力。Cai等人[[105](#bib.bib105)]提出了一种名为PPFormer的精确息肉分割方法，该方法利用了Transformer的长距离建模能力和CNN的局部特征提取能力。Yue等人[[32](#bib.bib32)]提出了一种新颖的边界约束网络，其中息肉分割任务以完全监督的方式完成。Ji等人[[2](#bib.bib2)]设计了一种多流网络用于视频息肉分割，使用了19,554个SUN-SEG数据进行训练。
- en: 2.2.2 Semi-Supervised Methods
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 半监督方法
- en: Although a fully-supervised model can achieve high performance, it heavily relies
    on pixel-wise labeled large-scale datasets. Therefore, more and more researchers
    are focusing on semi-supervised learning [[54](#bib.bib54)], which can achieve
    good performance even with a limited amount of labeled and unlabeled data. For
    instance, Li et al. proposed a semi-supervised learning paradigm for video polyp
    segmentation using only 547 pixel-level labeled images, containing a co-training
    scheme to supervise the predictions of unlabeled images. Zhao et al. [[50](#bib.bib50)]
    proposed an accurate and novel network for semi-supervised polyp video segmentation
    task, which exploits the spatial and temporal information from the proximity frames
    in endoscope videos Wu et al. [[132](#bib.bib132)] propose a novel semi-supervised
    polyp segmentation method called collaborative and adversarial learning of focused
    and dispersive representations learning model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管完全监督模型能够实现高性能，但它严重依赖于像素级标注的大规模数据集。因此，越来越多的研究人员开始关注半监督学习[[54](#bib.bib54)]，即使在标注和未标注数据有限的情况下也能取得良好性能。例如，Li等人提出了一种半监督学习范式用于视频息肉分割，仅使用547张像素级标注的图像，并包含了一个共同训练方案以监督未标注图像的预测。Zhao等人[[50](#bib.bib50)]提出了一种准确且新颖的网络用于半监督息肉视频分割任务，该网络利用了内窥镜视频中相邻帧的空间和时间信息。Wu等人[[132](#bib.bib132)]提出了一种名为协作与对抗学习的聚焦与分散表示学习模型的新型半监督息肉分割方法。
- en: 2.2.3 Weakly-Supervised Methods
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 弱监督方法
- en: On the other hand, to alleviate the fully-supervised methods relying on pixel-wise
    annotation, some researchers have adopted semi-supervised learning methods, including
    point supervision, semantic label supervision, scribble supervision, and bounding-box
    supervision. Wang et al. [[73](#bib.bib73)] proposed a framework of spatial-spectral
    mutual teaching and ensemble learning for scribble-supervised polyp segmentation.
    Wei et al. [[75](#bib.bib75)] introduced a model completely based on bounding
    box annotations, reducing the labeling cost and achieving comparable performance
    to full supervision. Wei et al. [[29](#bib.bib29)] proposed a model to make full
    use of both accurate mask and extra 40,266 frames box annotation from LDPolypVideo
    [[107](#bib.bib107)].
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，为了缓解完全监督方法对逐像素注释的依赖，一些研究者采用了半监督学习方法，包括点监督、语义标签监督、涂鸦监督和边界框监督。王等人 [[73](#bib.bib73)]
    提出了一个空间-光谱互教和集成学习的框架用于涂鸦监督的息肉分割。魏等人 [[75](#bib.bib75)] 引入了一种完全基于边界框注释的模型，降低了标注成本，并实现了与完全监督相当的性能。魏等人
    [[29](#bib.bib29)] 提出了一个模型，充分利用准确的掩膜和来自LDPolypVideo [[107](#bib.bib107)] 的额外40,266帧边界框注释。
- en: 2.2.4 Self-/Un-Supervised Methods
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 自监督/无监督方法
- en: Furthermore, to thoroughly address the dependence on annotated datasets, some
    researchers have adopted self-supervised methods to pre-train models and transfer
    them to downstream tasks. Wang et al. [[89](#bib.bib89)] proposed to use pseudo
    labels predicted from the source pre-trained model to perform contrastive learning
    in a supervised way. Yang et al. [[128](#bib.bib128)] propose a progressive self-training
    module, which selects reliable pseudo labels through a novel uncertainty-guided
    self-training loss to obtain accurate prototypes in the target domain. Tian et
    al. [[121](#bib.bib121)] propose a novel self-supervised representation learning
    method for polyp segmentation, which learns fine-grained feature representations
    by simultaneously predicting the distribution of augmented data and image contexts
    using contrastive learning with pretext constraints. It is worth noting that traditional
    polyp segmentation methods [[159](#bib.bib159), [17](#bib.bib17), [162](#bib.bib162),
    [69](#bib.bib69), [163](#bib.bib163)] are typically implemented in an unsupervised
    manner. For instance, Jia et al. [[163](#bib.bib163)] proposed a feasible method
    using K-means clustering and localizing region-based active contour segmentation
    for CPS.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了彻底解决对带注释数据集的依赖，一些研究者采用了自监督方法来预训练模型，并将其迁移到下游任务中。王等人 [[89](#bib.bib89)] 提出了使用源预训练模型预测的伪标签以监督方式进行对比学习。杨等人
    [[128](#bib.bib128)] 提出了一个渐进自训练模块，通过新颖的不确定性引导自训练损失选择可靠的伪标签，从而在目标领域中获得准确的原型。田等人
    [[121](#bib.bib121)] 提出了一种新颖的自监督表示学习方法用于息肉分割，通过使用带有前置约束的对比学习，同时预测增强数据和图像上下文的分布来学习细粒度特征表示。值得注意的是，传统的息肉分割方法
    [[159](#bib.bib159), [17](#bib.bib17), [162](#bib.bib162), [69](#bib.bib69), [163](#bib.bib163)]
    通常以无监督的方式实现。例如，贾等人 [[163](#bib.bib163)] 提出了使用K-means聚类和基于区域的活动轮廓分割进行CPS的可行方法。
- en: 2.3 Learning Paradigm
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 学习范式
- en: From the perspective of the learning paradigm, deep CPS models can be divided
    into 12 subcategories, such as attention mechanism, multi-scale feature fusion,
    boundary awareness, domain adaptation, multi-task learning, adversarial learning,
    data augmentation, designing loss function, multi-stage refinement, ensemble learning,
    interpretability, and knowledge distillation.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从学习范式的角度来看，深度CPS模型可以分为12个子类别，如注意力机制、多尺度特征融合、边界意识、领域适应、多任务学习、对抗学习、数据增强、设计损失函数、多阶段优化、集成学习、可解释性和知识蒸馏。
- en: 2.3.1 Attention Mechanism
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 注意力机制
- en: The attention mechanism was first proposed in [[165](#bib.bib165)] of machine
    translation, which allows the model to focus on relevant parts of the input when
    making predictions. Subsequently, it was introduced to the polyp segmentation
    task. For instance, Cai et al. [[105](#bib.bib105)] present the PP-guided self-attention
    to enhance the model’s perception of polyp boundary. Zhao et al. [[50](#bib.bib50)]
    propose a novel spatial-temporal attention network composed of temporal local
    context attention module and proximity frame time-space attention module. Ji et
    al. [[2](#bib.bib2)] propose a normalized self-attention block, which is motivated
    by the fact that dynamically updating the receptive field is important for self-attention-based
    networks. Huang et al. [[61](#bib.bib61)] proposed a hierarchical attention module
    to encourage the collection of polyp semantic information from high-level features
    to gradually guide the recovery of polyp spatial information in low-level features.
    Patel et al. [[62](#bib.bib62)] designed a novel attention module to focus more
    on the difficult pixels that usually lie near the boundary region. Du et al. [[114](#bib.bib114)]
    proposed a local pyramid attention module to aggregate attention cues in different
    scales and guide the network to enhance semantic features and emphasize the target
    area.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制最初在 [[165](#bib.bib165)] 的机器翻译中提出，它允许模型在进行预测时关注输入的相关部分。随后，它被引入到息肉分割任务中。例如，Cai
    等人 [[105](#bib.bib105)] 提出了 PP 引导的自注意力，以增强模型对息肉边界的感知。Zhao 等人 [[50](#bib.bib50)]
    提出了一个新颖的时空注意力网络，由时间局部上下文注意力模块和接近帧时空注意力模块组成。Ji 等人 [[2](#bib.bib2)] 提出了一个归一化自注意力块，这一块的动机是动态更新感受野对于基于自注意力的网络至关重要。Huang
    等人 [[61](#bib.bib61)] 提出了一个层次化注意力模块，以鼓励从高层特征中收集息肉语义信息，并逐步引导低层特征中息肉空间信息的恢复。Patel
    等人 [[62](#bib.bib62)] 设计了一个新颖的注意力模块，更加关注通常位于边界区域附近的困难像素。Du 等人 [[114](#bib.bib114)]
    提出了一个局部金字塔注意力模块，以聚合不同尺度的注意力线索，引导网络增强语义特征并强调目标区域。
- en: 2.3.2 Multiscale Feature Fusion
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 多尺度特征融合
- en: The goal of multi-scale feature fusion is to improve the model’s ability to
    deal with varying sizes of objects, leading to more robust and accurate segmentation.
    Tomar et al. [[28](#bib.bib28)] propose a multi-scale feature aggregation to capture
    features learned by different decoder blocks. Wang et al. [[8](#bib.bib8)] propose
    a novel multi-stage feature aggregation decoder, which consists of the local emphasis
    module and the stepwise feature aggregation module. Srivastava et al. [[51](#bib.bib51)]
    proposed a novel dual-scale dense fusion block that performs dual-scale feature
    exchange and a sub-network that exchanges multi-scale features. Yue et al. [[32](#bib.bib32)]
    propose a novel cross-layer feature integration strategy that consists of an attention-driven
    cross-layer feature interaction module and a global feature integration module.
    Ji et al. [[2](#bib.bib2)] propose a novel global-to-local learning paradigm,
    which realizes both long-term and short-term spatial-temporal propagation at an
    arbitrary temporal distance. Qiu et al. [[56](#bib.bib56)] design a boundary distribution
    guided decoder to fuse multi-scale features to improve polyp segmentation on different
    sizes. Huang et al. [[61](#bib.bib61)] present the interaction fusion module to
    bridge the semantic gap between the Transformer branch and the CNN branch, thereby
    fully capturing the global contextual information and local detailed information
    of polyps.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度特征融合的目标是提升模型处理不同尺寸物体的能力，从而实现更强健和准确的分割。Tomar 等人 [[28](#bib.bib28)] 提出了多尺度特征聚合，以捕捉由不同解码器块学习到的特征。Wang
    等人 [[8](#bib.bib8)] 提出了一个新颖的多阶段特征聚合解码器，该解码器由局部强调模块和逐步特征聚合模块组成。Srivastava 等人 [[51](#bib.bib51)]
    提出了一个新颖的双尺度密集融合块，执行双尺度特征交换，并且包含一个交换多尺度特征的子网络。Yue 等人 [[32](#bib.bib32)] 提出了一个新颖的跨层特征融合策略，包括一个基于注意力的跨层特征交互模块和一个全局特征融合模块。Ji
    等人 [[2](#bib.bib2)] 提出了一个新颖的全局到局部学习范式，实现了在任意时间距离上的长期和短期空间-时间传播。Qiu 等人 [[56](#bib.bib56)]
    设计了一个边界分布引导的解码器，以融合多尺度特征，改进不同尺寸的息肉分割。Huang 等人 [[61](#bib.bib61)] 提出了交互融合模块，以弥合
    Transformer 分支和 CNN 分支之间的语义差距，从而充分捕捉息肉的全局上下文信息和局部详细信息。
- en: 2.3.3 Boundary-aware
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 边界感知
- en: The boundary-aware-based method is designed to address the issue of fuzzy segmentation
    boundaries in existing polyp segmentation methods. For example, Jin et al. [[60](#bib.bib60)]
    introduces an edge extractor to obtain the edge information for auxiliary supervision
    to use low-level features better. Yue et al. [[32](#bib.bib32)] propose a novel
    deep network (termed BCNet) by focusing on cross-layer feature integration and
    boundary extraction in consideration of the challenging characteristics of polyps.
    With the assistance of high-level location features and boundary constraints,
    BCNet explores the polyp and non-polyp information of the shallow layer collaboratively
    and yields a better segmentation performance. Qiu et al. [[56](#bib.bib56)] design
    a boundary distribution guided network for accurate polyp segmentation, which
    consists of a boundary distribution generate module and a boundary distribution
    guided decoder. Xiao et al. [[112](#bib.bib112)] proposed an iterative feedback
    prediction module to consider contextual and boundary-aware information. Chen
    et al. [[113](#bib.bib113)] proposes a local edge feature extraction block to
    progressively generate a sequence of high-resolution local edge features. Du et
    al. [[63](#bib.bib63)] proposed the reverse-contour guidance module to receive
    contour detail and reverse information to highlight the boundary.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 基于边界感知的方法旨在解决现有息肉分割方法中的模糊分割边界问题。例如，Jin 等人 [[60](#bib.bib60)] 引入了一种边缘提取器，以获取边缘信息用于辅助监督，从而更好地利用低级特征。Yue
    等人 [[32](#bib.bib32)] 提出了一种新颖的深度网络（称为 BCNet），通过关注跨层特征集成和边界提取来考虑息肉的挑战特征。在高层位置信息和边界约束的帮助下，BCNet
    共同探索浅层的息肉和非息肉信息，并取得更好的分割性能。Qiu 等人 [[56](#bib.bib56)] 设计了一种边界分布引导网络，用于准确的息肉分割，该网络包括一个边界分布生成模块和一个边界分布引导解码器。Xiao
    等人 [[112](#bib.bib112)] 提出了一个迭代反馈预测模块，以考虑上下文和边界感知信息。Chen 等人 [[113](#bib.bib113)]
    提出了一个局部边缘特征提取块，以逐步生成一系列高分辨率局部边缘特征。Du 等人 [[63](#bib.bib63)] 提出了反向轮廓引导模块，以接收轮廓细节和反向信息，从而突出边界。
- en: 2.3.4 Domain Adapation
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 领域适应
- en: Domain adaptation addresses the model’s performance degradation when there is
    a distributional shift between the training and testing data. Wang et al. [[89](#bib.bib89)]
    a practical problem of source-free domain adaptation, which eliminates the reliance
    on annotated source data. Xiong et al. [[80](#bib.bib80)] propose an image-to-image
    translation network for domain adaptation. Shen et al. [[27](#bib.bib27)] propose
    a task-relevant feature replenishment-based network to tackle existing problems
    in unsupervised domain adaptation. Huy et al. [[55](#bib.bib55)] an unsupervised
    domain adaptation that transfers the style between the domains using Fourier transform
    and adversarial training. Yang et al. [[31](#bib.bib31)] devise a novel source-free
    domain adaptation framework with Fourier style mining, where only a well-trained
    source segmentation model is available to adapt to the target domain. Yang et
    al. [[128](#bib.bib128)] propose a mutual-prototype adaptation network to eliminate
    domain shifts in multi-center and multidevices colonoscopy images. Li et al. [[122](#bib.bib122)]
    propose a polymorphic transformer, which can be incorporated into any DNN backbones
    for few-shot domain adaptation.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应解决了训练数据和测试数据之间存在分布偏移时模型性能下降的问题。Wang 等人 [[89](#bib.bib89)] 研究了源无关领域适应的实际问题，该方法消除了对标注源数据的依赖。Xiong
    等人 [[80](#bib.bib80)] 提出了一种用于领域适应的图像到图像翻译网络。Shen 等人 [[27](#bib.bib27)] 提出了基于任务相关特征补充的网络，以解决无监督领域适应中的现有问题。Huy
    等人 [[55](#bib.bib55)] 提出了一种无监督领域适应方法，通过傅里叶变换和对抗训练在领域之间转移风格。Yang 等人 [[31](#bib.bib31)]
    设计了一种新颖的源无关领域适应框架，结合傅里叶风格挖掘，其中仅有一个经过良好训练的源分割模型可用于适应目标领域。Yang 等人 [[128](#bib.bib128)]
    提出了一个互原型适应网络，以消除多中心和多设备结肠镜图像中的领域偏移。Li 等人 [[122](#bib.bib122)] 提出了一个多态变压器，可集成到任何
    DNN 主干网络中，用于少量样本领域适应。
- en: '|   | Datasets | Year | Publication | Number | Annotation Type | Resolution
    | Polpy Size ($\%$) | Contrast Values | #Obj. | Availability |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|   | 数据集 | 年份 | 发表刊物 | 数量 | 标注类型 | 分辨率 | 多态大小 ($\%$) | 对比度值 | #对象 | 可用性 |'
- en: '| Image-level | ETIS-Larib [[103](#bib.bib103)] | 2014 | IJCARS | 196 | Pixel-wise
    | 1225$\times$966 | 0.10 $\sim$ 29.1 | 0.25 $\sim$ 0.87 | 1 $\sim$ 3 | [Open](https://drive.google.com/drive/folders/10QXjxBJqCf7PAXqbDvoceWmZ-qF07tFi)
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 图像级别 | ETIS-Larib [[103](#bib.bib103)] | 2014 | IJCARS | 196 | 像素级 | 1225$\times$966
    | 0.10 $\sim$ 29.1 | 0.25 $\sim$ 0.87 | 1 $\sim$ 3 | [打开](https://drive.google.com/drive/folders/10QXjxBJqCf7PAXqbDvoceWmZ-qF07tFi)
    |'
- en: '| CVC-ClinicDB [[69](#bib.bib69)] | 2015 | CMIG | 612 | Pixel-wise | 384$\times$288
    | 0.33 $\sim$ 48.9 | 0.42 $\sim$ 0.96 | 1 $\sim$ 3 | [Open](https://polyp.grand-challenge.org/CVCClinicDB/)
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| CVC-ClinicDB [[69](#bib.bib69)] | 2015 | CMIG | 612 | 像素级 | 384$\times$288
    | 0.33 $\sim$ 48.9 | 0.42 $\sim$ 0.96 | 1 $\sim$ 3 | [打开](https://polyp.grand-challenge.org/CVCClinicDB/)
    |'
- en: '| CVC-ColonDB [[104](#bib.bib104)] | 2015 | TMI | 300 | Pixel-wise | 574$\times$500
    | 0.29 $\sim$ 63.1 | 0.28 $\sim$ 0.93 | 1 | [Open](https://drive.google.com/drive/folders/1-gZUo1dgsdcWxSdXV9OAPmtGEbwZMfDY)
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| CVC-ColonDB [[104](#bib.bib104)] | 2015 | TMI | 300 | 像素级 | 574$\times$500
    | 0.29 $\sim$ 63.1 | 0.28 $\sim$ 0.93 | 1 | [打开](https://drive.google.com/drive/folders/1-gZUo1dgsdcWxSdXV9OAPmtGEbwZMfDY)
    |'
- en: '| EndoScene [[108](#bib.bib108)] | 2017 | JHE | 912 | Pixel-wise | 384$\times$288
    to 574$\times$500 | 0.29 $\sim$ 63.2 | 0.27 $\sim$ 0.96 | 1 $\sim$ 3 | [Open](http://pages.cvc.uab.es/CVC-Colon/)
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| EndoScene [[108](#bib.bib108)] | 2017 | JHE | 912 | 像素级 | 384$\times$288
    到 574$\times$500 | 0.29 $\sim$ 63.2 | 0.27 $\sim$ 0.96 | 1 $\sim$ 3 | [打开](http://pages.cvc.uab.es/CVC-Colon/)
    |'
- en: '| Kvasir-SEG [[70](#bib.bib70)] | 2020 | MMM | 1,000 | Pixel-wise | 332$\times$487
    to 1920$\times$1072 | 0.51 $\sim$ 81.4 | 0.35 $\sim$ 0.87 | 1 $\sim$ 3 | [Open](https://datasets.simula.no/kvasir-seg/)
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Kvasir-SEG [[70](#bib.bib70)] | 2020 | MMM | 1,000 | 像素级 | 332$\times$487
    到 1920$\times$1072 | 0.51 $\sim$ 81.4 | 0.35 $\sim$ 0.87 | 1 $\sim$ 3 | [打开](https://datasets.simula.no/kvasir-seg/)
    |'
- en: '| HyperKvasir [[85](#bib.bib85)] | 2020 | SD | 110,079 | Pixel-wise, Bounding-box
    | 332$\times$487 to 1920$\times$1072 | 0.57 $\sim$ 76.6 | 0.35 $\sim$ 0.88 | 1
    $\sim$ 10 | [Open](https://osf.io/mh9sj/) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| HyperKvasir [[85](#bib.bib85)] | 2020 | SD | 110,079 | 像素级, 边界框 | 332$\times$487
    到 1920$\times$1072 | 0.57 $\sim$ 76.6 | 0.35 $\sim$ 0.88 | 1 $\sim$ 10 | [打开](https://osf.io/mh9sj/)
    |'
- en: '| Piccolo [[118](#bib.bib118)] | 2020 | AS | 3,433 | Pixel-wise | 854$\times$480
    to 1920$\times$1080 | 0.0005 $\sim$ 65.5 | 0.40 $\sim$ 0.72 | 1 $\sim$ 5 | [Request](https://www.biobancovasco.bioef.eus/en/Sample-and-data-catalog/Databases/PD178-PICCOLO-EN.html)
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Piccolo [[118](#bib.bib118)] | 2020 | AS | 3,433 | 像素级 | 854$\times$480 到
    1920$\times$1080 | 0.0005 $\sim$ 65.5 | 0.40 $\sim$ 0.72 | 1 $\sim$ 5 | [请求](https://www.biobancovasco.bioef.eus/en/Sample-and-data-catalog/Databases/PD178-PICCOLO-EN.html)
    |'
- en: '| Kvasir-sessile [[129](#bib.bib129)] | 2021 | JBHI | 196 | Pixel-wise | 401$\times$415
    to 1348$\times$1070 | 0.54 $\sim$ 58.4 | 0.39 $\sim$ 0.86 | 1 $\sim$ 3 | [Open](https://datasets.simula.no/kvasir-seg/)
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Kvasir-sessile [[129](#bib.bib129)] | 2021 | JBHI | 196 | 像素级 | 401$\times$415
    到 1348$\times$1070 | 0.54 $\sim$ 58.4 | 0.39 $\sim$ 0.86 | 1 $\sim$ 3 | [打开](https://datasets.simula.no/kvasir-seg/)
    |'
- en: '| BKAI-IGH [[166](#bib.bib166)] | 2021 | AVC | 1,200 | Pixel-wise | 1280$\times$959
    | 0.14 $\sim$ 19.4 | 0.29 $\sim$ 0.88 | 1 $\sim$ 18 | [Open](https://www.kaggle.com/competitions/bkai-igh-neopolyp/data)
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| BKAI-IGH [[166](#bib.bib166)] | 2021 | AVC | 1,200 | 像素级 | 1280$\times$959
    | 0.14 $\sim$ 19.4 | 0.29 $\sim$ 0.88 | 1 $\sim$ 18 | [打开](https://www.kaggle.com/competitions/bkai-igh-neopolyp/data)
    |'
- en: '| PolypGen [[24](#bib.bib24)] | 2023 | SD | 8,037 | Pixel-wise, Bounding-box
    | 384$\times$288 to 1920$\times$1080 | 0.001 $\sim$ 74.1 | 0.29 $\sim$1.0 | 1
    $\sim$ 17 | [Open](https://www.kaggle.com/competitions/bkai-igh-neopolyp/data)
    |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| PolypGen [[24](#bib.bib24)] | 2023 | SD | 8,037 | 像素级, 边界框 | 384$\times$288
    到 1920$\times$1080 | 0.001 $\sim$ 74.1 | 0.29 $\sim$1.0 | 1 $\sim$ 17 | [打开](https://www.kaggle.com/competitions/bkai-igh-neopolyp/data)
    |'
- en: '|   Video-level | ASU-Mayo [[104](#bib.bib104)] | 2016 | TMI | 36,458 | Pixel-wise
    | 688$\times$550 | - | - | 1 | [Request](https://polyp.grand-challenge.org/AsuMayo/)
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|   视频级 | ASU-Mayo [[104](#bib.bib104)] | 2016 | TMI | 36,458 | 像素级 | 688$\times$550
    | - | - | 1 | [请求](https://polyp.grand-challenge.org/AsuMayo/) |'
- en: '| LDPolyVideo [[107](#bib.bib107)] | 2021 | MICCAI | 901,626 | Bounding-box
    | 560$\times$480 | - | 0.10 $\sim$ 0.99 | 1 | [Open](https://github.com/dashishi/LDPolypVideo-Benchmark)
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| LDPolyVideo [[107](#bib.bib107)] | 2021 | MICCAI | 901,626 | 边界框 | 560$\times$480
    | - | 0.10 $\sim$ 0.99 | 1 | [打开](https://github.com/dashishi/LDPolypVideo-Benchmark)
    |'
- en: '| ToPV [[167](#bib.bib167)] | 2022 | ISBI | 11,953 | Video-level | 384$\times$352
    | - | 0.08 $\sim$ 1.02 | 1 | [Request](http://www.pami.sjtu.edu.cn/En/Show/74/163)
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| ToPV [[167](#bib.bib167)] | 2022 | ISBI | 11,953 | 视频级 | 384$\times$352 |
    - | 0.08 $\sim$ 1.02 | 1 | [请求](http://www.pami.sjtu.edu.cn/En/Show/74/163) |'
- en: '| SUN-SEG [[2](#bib.bib2)] | 2022 | MIR | 158,690 | Diverse-Annotations | 1158$\times$1008
    to 1240$\times$1080 | 0.06 $\sim$ 30.0 | 0.20 $\sim$ 0.89 | 1 | [Request](https://github.com/GewelsJI/VPS)
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| SUN-SEG [[2](#bib.bib2)] | 2022 | MIR | 158,690 | 多样化注释 | 1158$\times$1008
    到 1240$\times$1080 | 0.06 $\sim$ 30.0 | 0.20 $\sim$ 0.89 | 1 | [请求](https://github.com/GewelsJI/VPS)
    |'
- en: '|   |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |  |'
- en: 'TABLE IV: Statistics of popular polyp segmentation datasets, including the
    number of images, annotation type, image resolution, ratio of the object size
    and contrast value in mages. Please refer to Section. [3](#S3 "3 Polyp Segmentation
    Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey") for more detailed descriptions.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV：流行的息肉分割数据集的统计信息，包括图像数量、注释类型、图像分辨率、物体大小比例和图像中的对比度值。有关更详细的描述，请参见第 [3](#S3
    "3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey") 节。'
- en: 2.3.5 Multi-task Learning
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.5 多任务学习
- en: Multi-task learning enhances the model’s learning capabilities by jointly training
    on multiple tasks, exploiting complementary information between different tasks
    to achieve better performance. Wang et al. [[82](#bib.bib82)] propose an efficient
    multi-task synergetic network for concurrent polyp segmentation and classification.
    Tomar et al. [[28](#bib.bib28)] propose a text-guided attention mechanism for
    polyp segmentation using a simple byte-pair encoding. Yue et al. [[32](#bib.bib32)]
    proposed a multi-task learning strategy that simultaneously supervises the boundary
    prediction and polyp prediction with the boundary mask and polyp mask. Xiao et
    al. [[112](#bib.bib112)] presents an iterative context-boundary feedback network
    to iterative feedback the preliminary predictions of both segmentation and boundary
    into different encoder levels. Murugesan et al. [[149](#bib.bib149)] proposed
    a novel multi-task network and mask prediction is the primary task, while contour
    detection and distance map estimation are auxiliary tasks.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习通过对多个任务进行联合训练来增强模型的学习能力，利用不同任务之间的互补信息以实现更好的性能。Wang 等人 [[82](#bib.bib82)]
    提出了一个高效的多任务协同网络，用于同时进行息肉分割和分类。Tomar 等人 [[28](#bib.bib28)] 提出了一个基于文本指导的注意力机制，利用简单的字节对编码进行息肉分割。Yue
    等人 [[32](#bib.bib32)] 提出了一个多任务学习策略，同时使用边界掩码和息肉掩码来监督边界预测和息肉预测。Xiao 等人 [[112](#bib.bib112)]
    提出了一个迭代上下文-边界反馈网络，将分割和边界的初步预测迭代反馈到不同的编码器层级。Murugesan 等人 [[149](#bib.bib149)] 提出了一个新颖的多任务网络，其中掩码预测是主要任务，而轮廓检测和距离图估计是辅助任务。
- en: 2.3.6 Adversarial Learning
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.6 对抗学习
- en: Adversarial learning in image segmentation can enhance models’ robustness and
    generalization capability, making them more suitable for challenging scenarios
    in the real world. Mazumdar et al. [[83](#bib.bib83)] a novel generative adversarial
    network (GAN) based approach for polyp segmentation, integrating a standard generator
    and an attention-based discriminator. Shen et al. [[27](#bib.bib27)] propose polyp-aware
    adversarial learning is employed to bridge the domain gap by aligning features
    in output space. Chen et al. [[57](#bib.bib57)] designed a random color reversal
    synthesis module under the GAN framework, which synthesizes images in which some
    regions are highlighted randomly while the center area of the polyps is always
    highlighted. Yang et al. [[128](#bib.bib128)] propose a novel foreground consistency
    loss with adversarial learning to optimize the reconstruction module. Wu et al.
    [[132](#bib.bib132)] proposed auxiliary adversarial learning to improve the quality
    of segmentation predictions from unlabeled images in the semi-supervised training
    stage. Xie et al. [[139](#bib.bib139)] propose a novel MI²GAN to maintain the
    contents of the medical images during image-to-image domain adaptation. Poorneshwaran
    et al. [[150](#bib.bib150)] propose a conditional generative convolutional framework
    for the task of polyp segmentation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割中的对抗学习可以增强模型的鲁棒性和泛化能力，使其更适合实际世界中的挑战性场景。Mazumdar 等人 [[83](#bib.bib83)] 提出了基于生成对抗网络（GAN）的新方法，用于息肉分割，结合了标准生成器和基于注意力的判别器。Shen
    等人 [[27](#bib.bib27)] 提出了通过对抗学习来缩小领域差距的息肉感知方法，通过对齐输出空间中的特征。Chen 等人 [[57](#bib.bib57)]
    在 GAN 框架下设计了一个随机颜色反转合成模块，合成图像中某些区域被随机突出显示，同时息肉的中心区域始终被突出显示。Yang 等人 [[128](#bib.bib128)]
    提出了具有对抗学习的新前景一致性损失，以优化重建模块。Wu 等人 [[132](#bib.bib132)] 提出了辅助对抗学习，以提高半监督训练阶段未标记图像的分割预测质量。Xie
    等人 [[139](#bib.bib139)] 提出了新颖的 MI²GAN，以在图像到图像领域适应过程中保持医学图像的内容。Poorneshwaran 等人
    [[150](#bib.bib150)] 提出了一个用于息肉分割任务的条件生成卷积框架。
- en: 2.3.7 Data Augmentation
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.7 数据增强
- en: Data augmentation enhances the model’s generalization and prevents overfitting
    by applying transformations to the training data, such as rotation, scaling, flipping,
    etc. Moreu et al. [[95](#bib.bib95)] an end-to-end model for integrating synthetic
    and real data under different levels of supervision. Haithami et al. [[78](#bib.bib78)]
    propose a deep learning framework to train an image transformation model with
    a segmentation model jointly. Guo et al. [[30](#bib.bib30)] propose a novel meta-learning
    mixup data augmentation method and a confidence-aware resampling strategy for
    polyp segmentation. Chen et al. [[57](#bib.bib57)] presented a random color reversal
    synthesis module to synthesize polyp images as data augmentation to enhance model
    performance. Wei et al. [[34](#bib.bib34)] propose the color exchange operation
    to decouple the contents and colors, which reduces the overfitting issue. Guo
    et al. [[127](#bib.bib127)] propose a novel ThresholdNet with a confidence-guided
    manifold mixup data augmentation method to alleviate the limited training dataset
    and the class imbalance problems. Thomaz et al. [[148](#bib.bib148)] proposes
    a novel method to increase the quantity and variability of training images from
    a publicly available colonoscopy dataset.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强通过对训练数据应用变换（如旋转、缩放、翻转等）来提高模型的泛化能力并防止过拟合。Moreu 等人 [[95](#bib.bib95)] 提出了一个用于在不同监督级别下整合合成数据和真实数据的端到端模型。Haithami
    等人 [[78](#bib.bib78)] 提出了一个深度学习框架，用于联合训练图像变换模型和分割模型。Guo 等人 [[30](#bib.bib30)]
    提出了一个新颖的元学习混合数据增强方法和一个基于置信度的重新采样策略用于息肉分割。Chen 等人 [[57](#bib.bib57)] 提出了一个随机颜色反转合成模块，通过合成息肉图像作为数据增强来提高模型性能。Wei
    等人 [[34](#bib.bib34)] 提出了颜色交换操作以解耦内容和颜色，从而减少过拟合问题。Guo 等人 [[127](#bib.bib127)]
    提出了一个新颖的ThresholdNet，并采用了基于置信度引导的流形混合数据增强方法，以缓解有限训练数据集和类别不平衡问题。Thomaz 等人 [[148](#bib.bib148)]
    提出了一个新方法来增加来自公开结肠镜检查数据集的训练图像的数量和变异性。
- en: 2.3.8 Designing Loss Function
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.8 设计损失函数
- en: In the context of deep CPS, the binary cross-entropy (BCE) is the commonly used
    loss function. In practice, the BCE loss always fails to learn accurate object
    boundaries due to the high similarity between polyps and the background tissue.
    Thus, researchers are dedicated to designing various effective loss functions
    to address the shortcomings of BCE. Wei et al. [[75](#bib.bib75)] propose the
    scale consistency loss to improve the robustness of the model against the variability
    of the predictions. Li et al. [[54](#bib.bib54)] proposed a context-free loss
    to mitigate the impact of varying contexts within successive frames. Wei et al.
    [[29](#bib.bib29)] propose the image consistency loss, which mines supervisory
    information from the relationship between images. Zhao et al. [[33](#bib.bib33)]
    build a general training-free loss network to implement the structure supervision
    in the feature levels, which provides an essential supplement to the loss design
    based on the prediction itself. Shen et al. [[65](#bib.bib65)] design an edge
    and structure consistency aware deep loss to improve the segmentation performance
    further. Guo et al. [[127](#bib.bib127)] devised a mixup feature map consistency
    loss and a mixup confidence map consistency loss to exploit the consistent constraints
    in the training of the augmented mixup data. Fang et al. [[142](#bib.bib142)]
    proposed a boundary-sensitive loss designed to leverage the area-boundary constraints.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度CPS的背景下，二进制交叉熵（BCE）是常用的损失函数。实际上，由于息肉与背景组织之间的高相似性，BCE损失通常无法准确学习对象边界。因此，研究人员致力于设计各种有效的损失函数来解决BCE的不足之处。Wei
    等人 [[75](#bib.bib75)] 提出了尺度一致性损失，以提高模型对预测变异性的鲁棒性。Li 等人 [[54](#bib.bib54)] 提出了一个无上下文损失，以减轻连续帧中变化的上下文的影响。Wei
    等人 [[29](#bib.bib29)] 提出了图像一致性损失，通过图像之间的关系挖掘监督信息。Zhao 等人 [[33](#bib.bib33)] 构建了一个通用的无训练损失网络，以在特征层级实现结构监督，这为基于预测本身的损失设计提供了重要补充。Shen
    等人 [[65](#bib.bib65)] 设计了一种边缘和结构一致性感知的深度损失，以进一步提高分割性能。Guo 等人 [[127](#bib.bib127)]
    设计了一个混合特征图一致性损失和一个混合置信度图一致性损失，以利用训练增强混合数据中的一致性约束。Fang 等人 [[142](#bib.bib142)]
    提出了一个边界敏感损失，旨在利用区域边界约束。
- en: 2.3.9 Others Techniques
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.9 其他技术
- en: In addition to the mainstream techniques mentioned above, there are some less
    commonly used techniques, such as multi-stage refinement, ensemble learning, interpretability,
    and knowledge distillation. Xiao et al. [[112](#bib.bib112)] propose a multi-stage
    refinement model iteratively via feedbacking contextual and boundary-aware detail
    from the preliminary segmentation and boundary predictions. Guo et al. [[158](#bib.bib158)]
    proposed an ensemble model for automated polyp segmentation by ensembling the
    UNet, SegNet [[161](#bib.bib161)], and PSPNet [[168](#bib.bib168)]. Wickstrom
    et al. [[144](#bib.bib144)] develop and evaluate recent advances in uncertainty
    estimation and model interpretability for CPS. Huang et al. [[146](#bib.bib146)]
    proposed the first knowledge distillation to compress a colonoscopy image segmentation
    model to achieve real-time segmentation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述主流技术，还有一些较少使用的技术，如多阶段细化、集成学习、可解释性和知识蒸馏。Xiao 等人 [[112](#bib.bib112)] 提出了一个多阶段细化模型，通过反馈来自初步分割和边界预测的上下文和边界感知细节进行迭代。Guo
    等人 [[158](#bib.bib158)] 提出了一个集成模型，通过集成 UNet、SegNet [[161](#bib.bib161)] 和 PSPNet
    [[168](#bib.bib168)] 实现自动化息肉分割。Wickstrom 等人 [[144](#bib.bib144)] 开发并评估了CPS中不确定性估计和模型可解释性的最新进展。Huang
    等人 [[146](#bib.bib146)] 提出了首个知识蒸馏方法，用于压缩结肠镜图像分割模型，以实现实时分割。
- en: 3 Polyp Segmentation Datasets
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 息肉分割数据集
- en: 'With the rapid development of CPS, many datasets have been introduced. Table
    [IV](#S2.T4 "TABLE IV ‣ 2.3.4 Domain Adapation ‣ 2.3 Learning Paradigm ‣ 2 Methodology
    (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey") provides a systematic summary of 14 commonly used CPS datasets, which
    can be divided into image-level (Sec. [3.1](#S3.SS1 "3.1 Image-level Datasets
    ‣ 3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")) and video-level datasets (Sec. [3.2](#S3.SS2 "3.2
    Video-level Datasets ‣ 3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")). Besides, as shown in Fig.
    [5](#S3.F5 "Figure 5 ‣ 3.1 Image-level Datasets ‣ 3 Polyp Segmentation Datasets
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")
    and Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Image-level Datasets ‣ 3 Polyp Segmentation
    Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey"), we also statistics the polyp size, color contrast, and polyp location
    distribution, providing a better understanding of the characteristics of existing
    polyp datasets.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '随着CPS的快速发展，已经引入了许多数据集。表格 [IV](#S2.T4 "TABLE IV ‣ 2.3.4 Domain Adapation ‣ 2.3
    Learning Paradigm ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in
    the Deep Learning Era: A Comprehensive Survey") 提供了14个常用CPS数据集的系统总结，这些数据集可以分为图像级（第
    [3.1节](#S3.SS1 "3.1 Image-level Datasets ‣ 3 Polyp Segmentation Datasets ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")）和视频级数据集（第
    [3.2节](#S3.SS2 "3.2 Video-level Datasets ‣ 3 Polyp Segmentation Datasets ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")）。此外，如图 [5](#S3.F5
    "Figure 5 ‣ 3.1 Image-level Datasets ‣ 3 Polyp Segmentation Datasets ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey") 和图 [6](#S3.F6
    "Figure 6 ‣ 3.1 Image-level Datasets ‣ 3 Polyp Segmentation Datasets ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey") 所示，我们还统计了息肉的大小、颜色对比度以及息肉位置的分布，以提供对现有息肉数据集特征的更好理解。'
- en: 3.1 Image-level Datasets
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 图像级数据集
- en: ETIS-Larib [[103](#bib.bib103)] dataset was introduced in 2014, which contains
    196 polyp images and their corresponding pixel-wise groundtruth, with a resolution
    of 1255$\times$966 pixels. The distribution of image contrast of ETIS-Larib ranges
    from 0.25 to 0.87, with the majority of images having contrast values between
    0.45 and 0.7\. The proportion of polyp sizes relative to the entire image varies
    from 0.1% to 29%, with most images having polyp sizes falling between 0.1% to
    10%. The distribution of polyp locations in the images is scattered.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ETIS-Larib [[103](#bib.bib103)] 数据集于2014年推出，包含196张息肉图像及其对应的像素级真值，分辨率为1255$\times$966
    像素。ETIS-Larib的图像对比度分布范围为0.25到0.87，大多数图像的对比度值在0.45到0.7之间。相对于整个图像，息肉的大小比例从0.1%到29%不等，大多数图像中的息肉大小在0.1%到10%之间。图像中息肉位置的分布较为分散。
- en: '![Refer to caption](img/87ab63e086bff78f084959ae5cffc836.png)![Refer to caption](img/850c0bc43908462c9921482f8b645e5c.png)![Refer
    to caption](img/702ba63e8b4ec99835250a0863bc7f76.png)![Refer to caption](img/d8463f68def0ae8b22dfedf52da6c8c6.png)![Refer
    to caption](img/04157d8d6c2f10672248757d3c14be2f.png)![Refer to caption](img/2c570f7be509f9270289681dfb1e1e45.png)![Refer
    to caption](img/0bfa66ebccb19ca930840219ac0a14ed.png)![Refer to caption](img/0a88c9342b84fa53b88f8e7e47a389a4.png)![Refer
    to caption](img/764375076495f3bb064559af6bfb000d.png)![Refer to caption](img/7f1d52315c45082b37dcdf88b57d103d.png)![Refer
    to caption](img/55f8cdd045c1ccea844e576c31c83fdb.png)![Refer to caption](img/0fce34f5532328309968d14f93b9960e.png)![Refer
    to caption](img/eebc86a04dd31ac712a071f1180cd086.png)![Refer to caption](img/27f843f7ef466bc0e236eb84923b0f5c.png)![Refer
    to caption](img/39f01b79a4012bb7ee5d78c7b1f23ec5.png)![Refer to caption](img/4ba542f7da0fe041b63d1aa0acc7064f.png)![Refer
    to caption](img/be1ff6b13277fc7c91d303efd9379258.png)![Refer to caption](img/ba0ab400db93ade7b7217aede5272366.png)![Refer
    to caption](img/f445425c930f6d949c19f01a0a281428.png)![Refer to caption](img/ff559b5489e7bb5433ea8cbf81ba9a3b.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/87ab63e086bff78f084959ae5cffc836.png)![参见标题](img/850c0bc43908462c9921482f8b645e5c.png)![参见标题](img/702ba63e8b4ec99835250a0863bc7f76.png)![参见标题](img/d8463f68def0ae8b22dfedf52da6c8c6.png)![参见标题](img/04157d8d6c2f10672248757d3c14be2f.png)![参见标题](img/2c570f7be509f9270289681dfb1e1e45.png)![参见标题](img/0bfa66ebccb19ca930840219ac0a14ed.png)![参见标题](img/0a88c9342b84fa53b88f8e7e47a389a4.png)![参见标题](img/764375076495f3bb064559af6bfb000d.png)![参见标题](img/7f1d52315c45082b37dcdf88b57d103d.png)![参见标题](img/55f8cdd045c1ccea844e576c31c83fdb.png)![参见标题](img/0fce34f5532328309968d14f93b9960e.png)![参见标题](img/eebc86a04dd31ac712a071f1180cd086.png)![参见标题](img/27f843f7ef466bc0e236eb84923b0f5c.png)![参见标题](img/39f01b79a4012bb7ee5d78c7b1f23ec5.png)![参见标题](img/4ba542f7da0fe041b63d1aa0acc7064f.png)![参见标题](img/be1ff6b13277fc7c91d303efd9379258.png)![参见标题](img/ba0ab400db93ade7b7217aede5272366.png)![参见标题](img/f445425c930f6d949c19f01a0a281428.png)![参见标题](img/ff559b5489e7bb5433ea8cbf81ba9a3b.png)'
- en: 'Figure 5: Statistics of polyp size (first two rows) and contrast value (last
    two rows) of current CPS datasets. From the perspective of polyp size, the BKAI-IGH
    is the most challenging dataset because it has the smallest average polyp size,
    while from the perspective of contrast, the SUN-SEG is the most challenging one
    because its contrast value is only 0.51.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：当前CPS数据集的息肉大小（前两行）和对比度值（最后两行）统计。从息肉大小的角度来看，BKAI-IGH是最具挑战性的数据集，因为其平均息肉大小最小，而从对比度的角度来看，SUN-SEG是最具挑战性的数据集，因为其对比度值仅为0.51。
- en: CVC-ClinicDB [[69](#bib.bib69)] was released in 2015, containing 612 polyp images
    and the corresponding pixel-wise annotations, with a resolution of 384$\times$288
    pixels. The distribution of image contrast of CVC-ClinicDB ranges from 0.42 to
    0.96, with most images having contrast values between 0.53 and 0.9, and the average
    contrast is 0.73\. The proportion of polyp sizes relative to the entire image
    varies from 0.33% to 48.9%, with the majority of images having polyp sizes falling
    between 0.1% to 10% and the mean sizes being 9.3%. The distribution of polyp locations
    in the images is relatively close to the center of the images.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: CVC-ClinicDB [[69](#bib.bib69)] 于2015年发布，包含612张息肉图像及其对应的像素级标注，分辨率为384$\times$288像素。CVC-ClinicDB图像对比度的分布范围为0.42至0.96，大多数图像的对比度值在0.53到0.9之间，平均对比度为0.73。息肉相对于整个图像的大小比例从0.33%到48.9%不等，大多数图像的息肉大小在0.1%到10%之间，平均大小为9.3%。图像中息肉位置的分布相对接近图像的中心。
- en: CVC-ColonDB [[104](#bib.bib104)] was released in 2015, including 300 polyp images
    and their corresponding pixel-level labels, with a resolution of 574$\times$500
    pixels. The distribution of image contrast of CVC-ColonDB ranges from 0.28 to
    0.93, with most images having contrast values between 0.4 and 0.75, and the average
    contrast is 0.59\. The proportion of polyp sizes relative to the entire image
    varies from 0.29% to 63.1%, with the majority of images having polyp sizes falling
    between 0.3% to 10% and the mean sizes being 7.4%.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: CVC-ColonDB [[104](#bib.bib104)] 于2015年发布，包括300张息肉图像及其对应的像素级标签，分辨率为574$\times$500像素。CVC-ColonDB图像对比度的分布范围为0.28至0.93，大多数图像的对比度值在0.4到0.75之间，平均对比度为0.59。息肉相对于整个图像的大小比例从0.29%到63.1%不等，大多数图像的息肉大小在0.3%到10%之间，平均大小为7.4%。
- en: EndoScene [[108](#bib.bib108)] combines CVC-ColonDB and CVC-ClinicDB into a
    new dataset (EndoScene) composed of 912 images obtained from 44 video sequences
    acquired from 36 patients, with different resolutions ranging from 384$\times$288
    to 574$\times$500 pixels. The distribution of image contrast of EndoScene ranges
    from 0.27 to 0.96, with the majority of images having contrast values between
    0.5 and 0.8, and the average contrast is 0.67\. The proportion of polyp sizes
    relative to the entire image varies from 0.29% to 63.2%, with most images having
    polyp sizes falling between 0.3% to 15% and the mean sizes being 8.6%.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: EndoScene [[108](#bib.bib108)] 将 CVC-ColonDB 和 CVC-ClinicDB 合并为一个新数据集（EndoScene），包含
    912 张图像，来源于 36 名患者的 44 个视频序列，分辨率从 384$\times$288 到 574$\times$500 像素不等。EndoScene
    图像的对比度范围从 0.27 到 0.96，大多数图像的对比度值在 0.5 和 0.8 之间，平均对比度为 0.67。息肉在整张图像中的占比范围从 0.29%
    到 63.2%，大多数图像中的息肉大小介于 0.3% 到 15% 之间，平均大小为 8.6%。
- en: Kvasir-SEG [[70](#bib.bib70)] contains 1000 polyp images and their corresponding
    groundtruth, with different resolutions ranging from 332$\times$487 to 1920$\times$1072
    pixels. The contrast of Kvasir-SEG ranges from 0.35 to 0.87, with most images
    having contrast values between 0.5 and 0.7, and the average contrast is 0.59\.
    The polyp sizes vary from 0.51% to 81.4%, with the majority of images falling
    between 0.5% to 20%, and the mean size being 15.6%. The distribution of polyp
    locations in the images is close to the center of the images.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Kvasir-SEG [[70](#bib.bib70)] 包含 1000 张息肉图像及其相应的真实标注，分辨率从 332$\times$487 到 1920$\times$1072
    像素不等。Kvasir-SEG 的对比度范围从 0.35 到 0.87，大多数图像的对比度值介于 0.5 和 0.7 之间，平均对比度为 0.59。息肉的大小从
    0.51% 到 81.4% 不等，大多数图像中的息肉大小在 0.5% 到 20% 之间，平均大小为 15.6%。图像中息肉的位置分布接近图像中心。
- en: HyperKvasir [[85](#bib.bib85)] was collected during real gastro- and colonoscopy
    examinations at a hospital in Norway and partly labeled by experienced gastrointestinal
    endoscopists. The HyperKvasir contains 110,079 images but only with 348 pixel-wise
    annotations. The contrast of HyperKvasir ranges from 0.35 to 0.88, and the average
    contrast is 0.59\. The polyp sizes vary from 0.57% to 76.6%, with the majority
    of images falling between 0.5% to 16%, and the mean size is 15.1%.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: HyperKvasir [[85](#bib.bib85)] 是在挪威医院进行实际胃肠镜和结肠镜检查过程中收集的，部分由经验丰富的胃肠内镜医生标注。HyperKvasir
    包含 110,079 张图像，但只有 348 张图像具有像素级标注。HyperKvasir 的对比度范围从 0.35 到 0.88，平均对比度为 0.59。息肉的大小从
    0.57% 到 76.6% 不等，大多数图像中的息肉大小在 0.5% 到 16% 之间，平均大小为 15.1%。
- en: Piccolo [[118](#bib.bib118)] contains 3433 manually annotated images (2131 White-Light
    images and 1302 Narrow-Band images), which is 2203 images for the training set,
    897 images for the validation set, and 333 images for the test set, with a resolution
    of 854$\times$480 to 1920$\times$1080 pixels. Its polyps size ranges from 0.0005%
    to 65.5%, its contrast varies from 0.40 to 0.72, and each image contains 1$\sim$5
    polyps.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Piccolo [[118](#bib.bib118)] 包含 3433 张手工标注的图像（2131 张白光图像和 1302 张窄带图像），其中 2203
    张图像用于训练集，897 张图像用于验证集，333 张图像用于测试集，分辨率从 854$\times$480 到 1920$\times$1080 像素不等。其息肉大小范围从
    0.0005% 到 65.5%，对比度范围从 0.40 到 0.72，每张图像包含 1$\sim$5 个息肉。
- en: Kvasir-sessile [[129](#bib.bib129)] selected from the Kvasir-SEG [[70](#bib.bib70)],
    contains 196 flat or sessile polyps and the corresponding pixel-wise annotations,
    with a resolution of 401$\times$415 to 1348$\times$1070 pixels. Its polyps size
    ranges from 0.54% to 58.4%, its contrast varies from 0.39 to 0.86, and each image
    contains 1$\sim$3 polyps.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Kvasir-sessile [[129](#bib.bib129)] 从 Kvasir-SEG [[70](#bib.bib70)] 中选取，包含 196
    个平坦或 sessile 息肉及其对应的像素级标注，分辨率从 401$\times$415 到 1348$\times$1070 像素不等。其息肉大小范围从
    0.54% 到 58.4%，对比度范围从 0.39 到 0.86，每张图像包含 1$\sim$3 个息肉。
- en: BKAI-IGH [[166](#bib.bib166)] contains 1200 images, with a resolution of 1280$\times$959
    pixels. The training set consists of 1000 images with instance-level pixel-wise
    annotations, and the test set consists of 200 images without annotations. All
    polyps are also classified into neoplastic or non-neoplastic classes denoted by
    red and green colors. Its polyps size ranges from 0.14% to 19.4%, its contrast
    varies from 0.29 to 0.88, and each image contains 1$\sim$18 polyps.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: BKAI-IGH [[166](#bib.bib166)] 包含 1200 张图像，分辨率为 1280$\times$959 像素。训练集包含 1000
    张具有实例级像素标注的图像，测试集包含 200 张没有标注的图像。所有息肉也被分类为新生物性或非新生物性类别，以红色和绿色标记。其息肉大小范围从 0.14%
    到 19.4%，对比度范围从 0.29 到 0.88，每张图像包含 1$\sim$18 个息肉。
- en: PolypGen [[24](#bib.bib24)] is composed of a total of 8037 frames, including
    both single and sequence frames, consisting of 3762 positive sample frames collected
    from six centers and 4275 negative sample frames collected from four different
    hospitals. Its polyps size ranges from 0.001% to 74.1%, its contrast varies from
    0.29 to 1.0, and each image contains 1$\sim$17 polyps.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: PolypGen [[24](#bib.bib24)] 由8037帧组成，包括单帧和序列帧，包含来自六个中心的3762个正样本帧和来自四家不同医院的4275个负样本帧。息肉的大小范围从0.001%到74.1%，对比度从0.29到1.0，每幅图像包含1$\sim$17个息肉。
- en: '![Refer to caption](img/e9eb802033ecdc05bb4c32993625e4ef.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9eb802033ecdc05bb4c32993625e4ef.png)'
- en: 'Figure 6: Polyps location distributions of some popular CPS datasets. From
    the perspective of location distribution, the HyperKvasir and Kvasir-SEG datasets
    are relatively easier. In contrast, the CVC-ColonDB and ETIS-Larib datasets are
    more challenging because their polyp location distribution is more dispersed.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：一些流行的CPS数据集中息肉位置分布。从位置分布的角度来看，HyperKvasir和Kvasir-SEG数据集相对较容易。相比之下，CVC-ColonDB和ETIS-Larib数据集则更具挑战性，因为它们的息肉位置分布更为分散。
- en: 3.2 Video-level Datasets
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 视频级数据集
- en: ASU-Mayo [[104](#bib.bib104)] is composed of a total of 36,458 frames, which
    is the first and a constantly growing set of short and long colonoscopy videos,
    collected and de-identified at the Department of Gastroenterology at Mayo Clinic
    in Arizona. Each frame has a groundtruth or binary mask indicating the polyp region.
    However, the problem is that it is not publicly available, and as of our submission,
    the authors have not responded to our requests for the dataset.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ASU-Mayo [[104](#bib.bib104)] 由36,458帧组成，是第一个且不断增长的短期和长期结肠镜视频数据集，收集并去标识化于亚利桑那州梅奥诊所的胃肠病学部门。每帧都有一个标注或二值掩模，指示息肉区域。然而，问题在于它尚未公开，并且截至我们提交时，作者尚未回应我们对数据集的请求。
- en: ToPV [[167](#bib.bib167)] contains 360 short videos collected during real colonoscopy
    examination. Two hundred four videos contain one polyp labeled by experienced
    endoscopic physicians and cropped to a size of 384×352\. The maximum duration
    of video clips in our dataset is 58.4 seconds, and the minimal duration is 5.88
    seconds, with a median duration of 15.76 seconds and an average duration of 16.78
    seconds. All videos are under white-light endoscopy observation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ToPV [[167](#bib.bib167)] 包含360个在实际结肠镜检查过程中收集的短视频。204个视频包含一个由经验丰富的内窥镜医生标记的息肉，并裁剪到384×352\的大小。我们数据集中视频剪辑的最大时长为58.4秒，最小时长为5.88秒，中位时长为15.76秒，平均时长为16.78秒。所有视频均在白光内窥镜观察下拍摄。
- en: LDPolyVideo [[107](#bib.bib107)] consists of 160 videos with 40,266 frames and
    the corresponding bounding box annotations, with a resolution of 560$\times$480\.
    33,884 frames contain at least one polyp, and in total, 200 labeled polyps. Besides
    we also provide 103 videos, including 861,400 frames without full annotations.
    Each video has a label indicating whether it contains polyps.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: LDPolyVideo [[107](#bib.bib107)] 包含160个视频，40,266帧以及相应的边界框注释，分辨率为560$\times$480\。33,884帧包含至少一个息肉，总共标记了200个息肉。此外，我们还提供了103个视频，包括861,400帧，没有完整注释。每个视频都有一个标签，指示其是否包含息肉。
- en: SUN-SEG [[2](#bib.bib2)] contains positive cases with 49,136 polyp frames and
    negative cases with 109,554 non-polyp frames. The SUN-SEG dataset contains diversified
    annotations, including pixel-wise object mask, boundary, scribble, and polygon
    annotations. The SUN-SEG dataset consists of 19,544 frames for training and 29,592
    frames for testing. The SUN-SEG is the most well-annotated and high-quality dataset
    for polyp segmentation.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: SUN-SEG [[2](#bib.bib2)] 包含49,136个息肉帧的正例和109,554个非息肉帧的负例。SUN-SEG数据集包含多样化的注释，包括像素级目标掩模、边界、涂鸦和多边形注释。SUN-SEG数据集包括19,544帧用于训练，29,592帧用于测试。SUN-SEG是最具注释和高质量的息肉分割数据集。
- en: 3.3 Analysis and Discussion
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 分析与讨论
- en: 'As shown in Table [IV](#S2.T4 "TABLE IV ‣ 2.3.4 Domain Adapation ‣ 2.3 Learning
    Paradigm ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep
    Learning Era: A Comprehensive Survey"), HyperKvasir is the largest dataset for
    image polyp segmentation in terms of quantity but only with 348 pixel-wise annotations,
    while LDPolyVideo is the largest dataset for video polyp segmentation with only
    bounding box annotations. From the perspective of quality and quantity of annotated
    data, SUN-SEG is currently the best dataset, with 49,136 well-annotated images,
    including pixel-wise object mask, boundary, scribble, and polygon annotations.
    As demonstrated in Fig. [5](#S3.F5 "Figure 5 ‣ 3.1 Image-level Datasets ‣ 3 Polyp
    Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning Era:
    A Comprehensive Survey"), the BKAI-IGH is the most challenging dataset in terms
    of polyp size because it has the smallest average polyp size, while from the perspective
    of contrast, the SUN-SEG is more challenging one because its contrast value is
    only 0.51\. As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1 Image-level Datasets ‣
    3 Polyp Segmentation Datasets ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey"), from the perspective of location distribution,
    the HyperKvasir and Kvasir-SEG datasets are relatively easier, while the CVC-ColonDB
    and ETIS-Larib datasets are more challenging because their polyp location distribution
    is more dispersed. Besides, polyps exhibit diverse variations in color, size,
    and quantity, making it very challenging to accurately segment them, as shown
    in Fig. [7](#S4.F7 "Figure 7 ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey").'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [IV](#S2.T4 "TABLE IV ‣ 2.3.4 Domain Adapation ‣ 2.3 Learning Paradigm ‣
    2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning Era:
    A Comprehensive Survey") 所示，HyperKvasir 是数量上最大的图像息肉分割数据集，但只有 348 个像素级标注，而 LDPolyVideo
    是最大的视频息肉分割数据集，但仅有边界框标注。从标注数据的质量和数量的角度来看，SUN-SEG 目前是最好的数据集，包含 49,136 张标注良好的图像，包括像素级对象掩码、边界、涂鸦和多边形标注。如图
    [5](#S3.F5 "Figure 5 ‣ 3.1 Image-level Datasets ‣ 3 Polyp Segmentation Datasets
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")
    所示，BKAI-IGH 是最具挑战性的数据集，因为它具有最小的平均息肉大小，而从对比度的角度来看，SUN-SEG 更具挑战性，因为其对比度值仅为 0.51。如图
    [6](#S3.F6 "Figure 6 ‣ 3.1 Image-level Datasets ‣ 3 Polyp Segmentation Datasets
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")
    所示，从位置分布的角度来看，HyperKvasir 和 Kvasir-SEG 数据集相对较易，而 CVC-ColonDB 和 ETIS-Larib 数据集则更具挑战性，因为它们的息肉位置分布更分散。此外，息肉在颜色、大小和数量上的多样变化使得准确分割变得非常具有挑战性，如图
    [7](#S4.F7 "Figure 7 ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation in
    the Deep Learning Era: A Comprehensive Survey") 所示。'
- en: 4 Evaluation Metrics
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估指标
- en: This section reviews commonly used CPS evaluation metrics, including Dice, IoU,
    Precision, Recall, Specificity, PR curves, F-measure [[169](#bib.bib169)], MAE
    [[170](#bib.bib170)], Weighted F-measure [[171](#bib.bib171)], S-measure [[172](#bib.bib172)],
    E-measure [[173](#bib.bib173)] and FPS.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了常用的 CPS 评估指标，包括 Dice、IoU、精度、召回率、特异性、PR 曲线、F-measure [[169](#bib.bib169)]、MAE
    [[170](#bib.bib170)]、加权 F-measure [[171](#bib.bib171)]、S-measure [[172](#bib.bib172)]、E-measure
    [[173](#bib.bib173)] 和 FPS。
- en: Dice coefficient, also known as the F1 score, is a measure of the overlap between
    two sets, with a range of 0 to 1\. A value of 1 indicates a perfect overlap, while
    0 indicates no overlap.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Dice 系数，也称为 F1 分数，是衡量两个集合重叠程度的指标，范围从 0 到 1。值为 1 表示完全重叠，而 0 表示没有重叠。
- en: '|  | $Dice=\frac{2TP}{2TP+FP+FN}$ |  | (1) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $Dice=\frac{2TP}{2TP+FP+FN}$ |  | (1) |'
- en: IoU (Intersection over Union) measures the overlap between two sets but is expressed
    as a ratio of the size of the intersection to the size of the union of the sets.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: IoU（交并比）衡量两个集合的重叠情况，但其表示为交集大小与集合并集大小的比率。
- en: '|  | $IoU=\frac{TP}{TP+FP+FN}$ |  | (2) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $IoU=\frac{TP}{TP+FP+FN}$ |  | (2) |'
- en: '![Refer to caption](img/e3ad44a121131a71fda8382c4944f6fa.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e3ad44a121131a71fda8382c4944f6fa.png)'
- en: 'Figure 7: Illustration of polyp diverse variations in color, size, and quantity,
    making it very challenging to accurately segment polyps.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：展示了息肉在颜色、大小和数量上的多样变化，使得准确分割息肉变得非常具有挑战性。
- en: Precision is a measure of the positive predictive value of a classifier or the
    proportion of true positive predictions among all positive predictions.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 精度是分类器的正预测值的度量，或所有正预测中真正正预测的比例。
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (3) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (3) |'
- en: Recall, also known as sensitivity or true positive rate, is an evaluation metric
    used in binary and multiclass classification to measure the ability of a model
    to identify positive instances correctly. It quantifies the proportion of actual
    positive instances that the model correctly predicted.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率，也称为敏感性或真正率，是一种评估指标，用于二分类和多分类任务中，衡量模型正确识别正实例的能力。它量化了模型正确预测的实际正实例的比例。
- en: '|  | $Recall=\frac{TP}{TP+FN}$ |  | (4) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | $Recall=\frac{TP}{TP+FN}$ |  | (4) |'
- en: Accuracy is the overall correct classification rate or the proportion of correct
    predictions made by the classifier out of all predictions made.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是总体正确分类率或分类器所做的所有预测中正确预测的比例。
- en: '|  | $Acc=\frac{TP+TN}{TP+TN+FP+FN}$ |  | (5) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $Acc=\frac{TP+TN}{TP+TN+FP+FN}$ |  | (5) |'
- en: '|    Method | Publication | CVC-Clinic. [[69](#bib.bib69)] | Kvasir-SEG [[70](#bib.bib70)]
    | CVC-ColonDB [[104](#bib.bib104)] | ETIS-Larib [[103](#bib.bib103)] | EndoScene
    [[108](#bib.bib108)] |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|    方法 | 发表刊物 | CVC-Clinic. [[69](#bib.bib69)] | Kvasir-SEG [[70](#bib.bib70)]
    | CVC-ColonDB [[104](#bib.bib104)] | ETIS-Larib [[103](#bib.bib103)] | EndoScene
    [[108](#bib.bib108)] |'
- en: '| mDice | mIoU | mDice | mIoU | mDice | mIoU | mDice | mIoU | mDice | mIoU
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| mDice | mIoU | mDice | mIoU | mDice | mIoU | mDice | mIoU | mDice | mIoU
    |'
- en: '| RPFA [[71](#bib.bib71)] | MICCAI 2023 | 0.931 | 0.885 | 0.929 | 0.880 | 0.837
    | 0.759 | 0.822 | 0.746 | 0.905 | 0.839 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| RPFA [[71](#bib.bib71)] | MICCAI 2023 | 0.931 | 0.885 | 0.929 | 0.880 | 0.837
    | 0.759 | 0.822 | 0.746 | 0.905 | 0.839 |'
- en: '| XBFormer [[86](#bib.bib86)] | TMI 2023 | 0.923 | 0.875 | 0.926 | 0.871 |
    0.808 | 0.724 | 0.738 | 0.650 | 0.868 | 0.791 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| XBFormer [[86](#bib.bib86)] | TMI 2023 | 0.923 | 0.875 | 0.926 | 0.871 |
    0.808 | 0.724 | 0.738 | 0.650 | 0.868 | 0.791 |'
- en: '| ColnNet [[87](#bib.bib87)] | TMI 2023 | 0.930 | 0.887 | 0.926 | 0.872 | 0.797
    | 0.729 | 0.759 | 0.690 | 0.909 | 0.863 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| ColnNet [[87](#bib.bib87)] | TMI 2023 | 0.930 | 0.887 | 0.926 | 0.872 | 0.797
    | 0.729 | 0.759 | 0.690 | 0.909 | 0.863 |'
- en: '| FSFM [[41](#bib.bib41)] | ISBI 2023 | 0.934 | 0.884 | 0.913 | 0.861 | 0.786
    | 0.709 | 0.778 | 0.702 | 0.910 | 0.846 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| FSFM [[41](#bib.bib41)] | ISBI 2023 | 0.934 | 0.884 | 0.913 | 0.861 | 0.786
    | 0.709 | 0.778 | 0.702 | 0.910 | 0.846 |'
- en: '| CASCADE [[98](#bib.bib98)] | WACV 2023 | 0.943 | 0.899 | 0.926 | 0.878 |
    0.825 | 0.745 | 0.801 | 0.728 | 0.905 | 0.838 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| CASCADE [[98](#bib.bib98)] | WACV 2023 | 0.943 | 0.899 | 0.926 | 0.878 |
    0.825 | 0.745 | 0.801 | 0.728 | 0.905 | 0.838 |'
- en: '| PolypPVT [[100](#bib.bib100)] | CAAI AIR 2023 | 0.937 | 0.889 | 0.917 | 0.864
    | 0.808 | 0.727 | 0.787 | 0.706 | 0.900 | 0.833 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| PolypPVT [[100](#bib.bib100)] | CAAI AIR 2023 | 0.937 | 0.889 | 0.917 | 0.864
    | 0.808 | 0.727 | 0.787 | 0.706 | 0.900 | 0.833 |'
- en: '| RealSeg [[79](#bib.bib79)] | ISBI 2023 | 0.923 | 0.873 | 0.913 | 0.863 |
    0.785 | 0.710 | 0.777 | 0.698 | 0.909 | 0.844 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| RealSeg [[79](#bib.bib79)] | ISBI 2023 | 0.923 | 0.873 | 0.913 | 0.863 |
    0.785 | 0.710 | 0.777 | 0.698 | 0.909 | 0.844 |'
- en: '| Polyp-Mixer [[25](#bib.bib25)] | TCSVT 2023 | 0.908 | 0.856 | 0.916 | 0.864
    | 0.791 | 0.706 | 0.759 | 0.676 | - | - |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Polyp-Mixer [[25](#bib.bib25)] | TCSVT 2023 | 0.908 | 0.856 | 0.916 | 0.864
    | 0.791 | 0.706 | 0.759 | 0.676 | - | - |'
- en: '| CFANet [[91](#bib.bib91)] | PR 2023 | 0.933 | 0.883 | 0.915 | 0.861 | 0.743
    | 0.665 | 0.732 | 0.655 | 0.893 | 0.827 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| CFANet [[91](#bib.bib91)] | PR 2023 | 0.933 | 0.883 | 0.915 | 0.861 | 0.743
    | 0.665 | 0.732 | 0.655 | 0.893 | 0.827 |'
- en: '| M2SNet  [[174](#bib.bib174)] | Arxiv 2023 | 0.922 | 0.880 | 0.912 | 0.861
    | 0.758 | 0.685 | 0.749 | 0.678 | 0.869 | 0.807 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| M2SNet  [[174](#bib.bib174)] | Arxiv 2023 | 0.922 | 0.880 | 0.912 | 0.861
    | 0.758 | 0.685 | 0.749 | 0.678 | 0.869 | 0.807 |'
- en: '| EMTSNet [[82](#bib.bib82)] | JBHI 2023 | 0.935 | 0.885 | 0.919 | 0.869 |
    0.788 | 0.708 | 0.780 | 0.702 | 0.900 | 0.833 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| EMTSNet [[82](#bib.bib82)] | JBHI 2023 | 0.935 | 0.885 | 0.919 | 0.869 |
    0.788 | 0.708 | 0.780 | 0.702 | 0.900 | 0.833 |'
- en: '| PPFormer [[105](#bib.bib105)] | MICCAI 2022 | 0.946 | 0.902 | 0.930 | 0.879
    | 0.823 | 0.756 | 0.791 | 0.706 | 0.919 | 0.857 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| PPFormer [[105](#bib.bib105)] | MICCAI 2022 | 0.946 | 0.902 | 0.930 | 0.879
    | 0.823 | 0.756 | 0.791 | 0.706 | 0.919 | 0.857 |'
- en: '| SSFormer-L [[8](#bib.bib8)] | MICCAI 2022 | 0.906 | 0.855 | 0.917 | 0.864
    | 0.802 | 0.721 | 0.796 | 0.720 | 0.895 | 0.827 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| SSFormer-L [[8](#bib.bib8)] | MICCAI 2022 | 0.906 | 0.855 | 0.917 | 0.864
    | 0.802 | 0.721 | 0.796 | 0.720 | 0.895 | 0.827 |'
- en: '| SSFormer-S [[8](#bib.bib8)] | MICCAI 2022 | 0.916 | 0.873 | 0.925 | 0.878
    | 0.772 | 0.697 | 0.767 | 0.698 | 0.887 | 0.821 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| SSFormer-S [[8](#bib.bib8)] | MICCAI 2022 | 0.916 | 0.873 | 0.925 | 0.878
    | 0.772 | 0.697 | 0.767 | 0.698 | 0.887 | 0.821 |'
- en: '| LDNet [[49](#bib.bib49)] | MICCAI 2022 | 0.923 | 0.872 | 0.912 | 0.855 |
    0.794 | 0.715 | 0.778 | 0.707 | 0.893 | 0.826 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| LDNet [[49](#bib.bib49)] | MICCAI 2022 | 0.923 | 0.872 | 0.912 | 0.855 |
    0.794 | 0.715 | 0.778 | 0.707 | 0.893 | 0.826 |'
- en: '| TGANet [[28](#bib.bib28)] | MICCAI 2022 | 0.863 | 0.805 | 0.886 | 0.822 |
    0.695 | 0.609 | 0.574 | 0.488 | 0.822 | 0.733 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| TGANet [[28](#bib.bib28)] | MICCAI 2022 | 0.863 | 0.805 | 0.886 | 0.822 |
    0.695 | 0.609 | 0.574 | 0.488 | 0.822 | 0.733 |'
- en: '| TransMixer [[61](#bib.bib61)] | BIBM 2022 | 0.945 | 0.900 | 0.923 | 0.876
    | 0.823 | 0.745 | 0.795 | 0.719 | 0.910 | 0.844 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| TransMixer [[61](#bib.bib61)] | BIBM 2022 | 0.945 | 0.900 | 0.923 | 0.876
    | 0.823 | 0.745 | 0.795 | 0.719 | 0.910 | 0.844 |'
- en: '| ICBNet [[112](#bib.bib112)] | BIBM 2022 | 0.938 | 0.892 | 0.928 | 0.883 |
    0.812 | 0.738 | 0.800 | 0.727 | 0.898 | 0.833 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| ICBNet [[112](#bib.bib112)] | BIBM 2022 | 0.938 | 0.892 | 0.928 | 0.883 |
    0.812 | 0.738 | 0.800 | 0.727 | 0.898 | 0.833 |'
- en: '| TASNet [[57](#bib.bib57)] | BIBM 2022 | 0.930 | 0.884 | 0.913 | 0.863 | 0.799
    | 0.719 | 0.797 | 0.720 | 0.894 | 0.825 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| TASNet [[57](#bib.bib57)] | BIBM 2022 | 0.930 | 0.884 | 0.913 | 0.863 | 0.799
    | 0.719 | 0.797 | 0.720 | 0.894 | 0.825 |'
- en: '| DCRNet [[42](#bib.bib42)] | ISBI 2022 | 0.896 | 0.844 | 0.886 | 0.825 | 0.704
    | 0.631 | 0.556 | 0.496 | 0.856 | 0.788 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| DCRNet [[42](#bib.bib42)] | ISBI 2022 | 0.896 | 0.844 | 0.886 | 0.825 | 0.704
    | 0.631 | 0.556 | 0.496 | 0.856 | 0.788 |'
- en: '| BDGNet [[56](#bib.bib56)] | SPIE MI 2022 | 0.905 | 0.857 | 0.915 | 0.863
    | 0.797 | 0.723 | 0.752 | 0.681 | 0.899 | 0.831 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| BDGNet [[56](#bib.bib56)] | SPIE MI 2022 | 0.905 | 0.857 | 0.915 | 0.863
    | 0.797 | 0.723 | 0.752 | 0.681 | 0.899 | 0.831 |'
- en: '| Conv-MLP [[175](#bib.bib175)] | VisCom 2022 | 0.924 | 0.870 | 0.920 | 0.869
    | 0.793 | 0.717 | 0.753 | 0.676 | 0.893 | 0.822 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Conv-MLP [[175](#bib.bib175)] | VisCom 2022 | 0.924 | 0.870 | 0.920 | 0.869
    | 0.793 | 0.717 | 0.753 | 0.676 | 0.893 | 0.822 |'
- en: '| GLFRNet [[176](#bib.bib176)] | TMI 2022 | 0.941 | 0.895 | 0.894 | 0.837 |
    0.729 | 0.659 | 0.674 | 0.595 | 0.898 | 0.827 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| GLFRNet [[176](#bib.bib176)] | TMI 2022 | 0.941 | 0.895 | 0.894 | 0.837 |
    0.729 | 0.659 | 0.674 | 0.595 | 0.898 | 0.827 |'
- en: '| FANet [[35](#bib.bib35)] | TNNLS 2022 | 0.823 | 0.756 | 0.852 | 0.791 | 0.558
    | 0.486 | 0.415 | 0.361 | 0.668 | 0.600 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| FANet [[35](#bib.bib35)] | TNNLS 2022 | 0.823 | 0.756 | 0.852 | 0.791 | 0.558
    | 0.486 | 0.415 | 0.361 | 0.668 | 0.600 |'
- en: '| FNet-Res2Net [[62](#bib.bib62)] | NeurIPS 2022 | 0.919 | 0.867 | 0.889 |
    0.830 | 0.739 | 0.662 | 0.731 | 0.658 | 0.894 | 0.825 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| FNet-Res2Net [[62](#bib.bib62)] | NeurIPS 2022 | 0.919 | 0.867 | 0.889 |
    0.830 | 0.739 | 0.662 | 0.731 | 0.658 | 0.894 | 0.825 |'
- en: '| FNet-PVT [[62](#bib.bib62)] | NeurIPS 2022 | 0.937 | 0.889 | 0.913 | 0.864
    | 0.811 | 0.728 | 0.791 | 0.702 | 0.891 | 0.818 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| FNet-PVT [[62](#bib.bib62)] | NeurIPS 2022 | 0.937 | 0.889 | 0.913 | 0.864
    | 0.811 | 0.728 | 0.791 | 0.702 | 0.891 | 0.818 |'
- en: '| CaraNet[[177](#bib.bib177)] | SPIE MI 2022 | 0.921 | 0.876 | 0.913 | 0.859
    | 0.775 | 0.700 | 0.740 | 0.660 | 0.902 | 0.836 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| CaraNet[[177](#bib.bib177)] | SPIE MI 2022 | 0.921 | 0.876 | 0.913 | 0.859
    | 0.775 | 0.700 | 0.740 | 0.660 | 0.902 | 0.836 |'
- en: '| Transfuse [[7](#bib.bib7)] | MICCAI 2021 | 0.908 | 0.857 | 0.915 | 0.860
    | 0.790 | 0.710 | 0.748 | 0.657 | 0.893 | 0.825 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Transfuse [[7](#bib.bib7)] | MICCAI 2021 | 0.908 | 0.857 | 0.915 | 0.860
    | 0.790 | 0.710 | 0.748 | 0.657 | 0.893 | 0.825 |'
- en: '| MSNet [[33](#bib.bib33)] | MICCAI 2021 | 0.915 | 0.866 | 0.902 | 0.847 |
    0.747 | 0.668 | 0.720 | 0.650 | 0.862 | 0.796 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| MSNet [[33](#bib.bib33)] | MICCAI 2021 | 0.915 | 0.866 | 0.902 | 0.847 |
    0.747 | 0.668 | 0.720 | 0.650 | 0.862 | 0.796 |'
- en: '| SANet [[34](#bib.bib34)] | MICCAI 2021 | 0.916 | 0.859 | 0.904 | 0.847 |
    0.752 | 0.669 | 0.750 | 0.654 | 0.888 | 0.815 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| SANet [[34](#bib.bib34)] | MICCAI 2021 | 0.916 | 0.859 | 0.904 | 0.847 |
    0.752 | 0.669 | 0.750 | 0.654 | 0.888 | 0.815 |'
- en: '| UACANet-L [[66](#bib.bib66)] | ACM MM 2021 | 0.926 | 0.880 | 0.912 | 0.859
    | 0.751 | 0.678 | 0.766 | 0.689 | 0.909 | 0.844 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| UACANet-L [[66](#bib.bib66)] | ACM MM 2021 | 0.926 | 0.880 | 0.912 | 0.859
    | 0.751 | 0.678 | 0.766 | 0.689 | 0.909 | 0.844 |'
- en: '| UACANet-S [[66](#bib.bib66)] | ACM MM 2021 | 0.916 | 0.870 | 0.905 | 0.852
    | 0.783 | 0.704 | 0.694 | 0.615 | 0.902 | 0.837 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| UACANet-S [[66](#bib.bib66)] | ACM MM 2021 | 0.916 | 0.870 | 0.905 | 0.852
    | 0.783 | 0.704 | 0.694 | 0.615 | 0.902 | 0.837 |'
- en: '| EUNet [[134](#bib.bib134)] | CVR 2021 | 0.902 | 0.846 | 0.908 | 0.854 | 0.756
    | 0.681 | 0.687 | 0.609 | 0.837 | 0.765 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| EUNet [[134](#bib.bib134)] | CVR 2021 | 0.902 | 0.846 | 0.908 | 0.854 | 0.756
    | 0.681 | 0.687 | 0.609 | 0.837 | 0.765 |'
- en: '| EMSNet [[178](#bib.bib178)] | EMBC 2021 | 0.923 | 0.874 | 0.897 | 0.842 |
    0.715 | 0.642 | 0.682 | 0.611 | 0.900 | 0.834 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| EMSNet [[178](#bib.bib178)] | EMBC 2021 | 0.923 | 0.874 | 0.897 | 0.842 |
    0.715 | 0.642 | 0.682 | 0.611 | 0.900 | 0.834 |'
- en: '| MSEG[[179](#bib.bib179)] | Arxiv 2021 | 0.909 | 0.864 | 0.897 | 0.839 | 0.735
    | 0.666 | 0.700 | 0.630 | 0.874 | 0.804 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| MSEG[[179](#bib.bib179)] | Arxiv 2021 | 0.909 | 0.864 | 0.897 | 0.839 | 0.735
    | 0.666 | 0.700 | 0.630 | 0.874 | 0.804 |'
- en: '| PraNet [[6](#bib.bib6)] | MICCAI 2020 | 0.899 | 0.849 | 0.898 | 0.840 | 0.709
    | 0.640 | 0.628 | 0.567 | 0.871 | 0.797 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| PraNet [[6](#bib.bib6)] | MICCAI 2020 | 0.899 | 0.849 | 0.898 | 0.840 | 0.709
    | 0.640 | 0.628 | 0.567 | 0.871 | 0.797 |'
- en: '| ACSNet [[67](#bib.bib67)] | MICCAI 2020 | 0.882 | 0.826 | 0.898 | 0.838 |
    0.716 | 0.649 | 0.578 | 0.509 | 0.863 | 0.787 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| ACSNet [[67](#bib.bib67)] | MICCAI 2020 | 0.882 | 0.826 | 0.898 | 0.838 |
    0.716 | 0.649 | 0.578 | 0.509 | 0.863 | 0.787 |'
- en: '| SFANet [[18](#bib.bib18)] | MICCAI 2019 | 0.700 | 0.607 | 0.723 | 0.611 |
    0.469 | 0.347 | 0.297 | 0.217 | 0.467 | 0.329 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| SFANet [[18](#bib.bib18)] | MICCAI 2019 | 0.700 | 0.607 | 0.723 | 0.611 |
    0.469 | 0.347 | 0.297 | 0.217 | 0.467 | 0.329 |'
- en: '| ResUNet++ [[5](#bib.bib5)] | ISM 2019 | 0.846 | 0.786 | 0.807 | 0.727 | 0.588
    | 0.497 | 0.337 | 0.275 | 0.687 | 0.598 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| ResUNet++ [[5](#bib.bib5)] | ISM 2019 | 0.846 | 0.786 | 0.807 | 0.727 | 0.588
    | 0.497 | 0.337 | 0.275 | 0.687 | 0.598 |'
- en: '| UNet [[3](#bib.bib3)] | MICCAI 2015 | 0.823 | 0.755 | 0.818 | 0.746 | 0.512
    | 0.444 | 0.398 | 0.335 | 0.710 | 0.627 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| UNet [[3](#bib.bib3)] | MICCAI 2015 | 0.823 | 0.755 | 0.818 | 0.746 | 0.512
    | 0.444 | 0.398 | 0.335 | 0.710 | 0.627 |'
- en: '|   |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |  |  |'
- en: 'TABLE V: Benchmarking performance of 40 state-of-the-art deep CPS models on
    five commonly used datasets in terms of mDice and mIoU. The top 2 methods are
    marked in Red and Blue, respectively.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：对 40 个最先进的深度 CPS 模型在五个常用数据集上的 mDice 和 mIoU 基准性能进行评估。前 2 名方法分别用红色和蓝色标记。
- en: Specificity, also known as True Negative Rate, is an evaluation metric used
    in binary and multiclass classification to measure the ability of a model to identify
    negative instances correctly.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 特异性，也称为真正负率，是一种评估指标，用于二分类和多分类任务中，衡量模型正确识别负实例的能力。
- en: '|  | $Specificity=\frac{TN}{TN+FP}$ |  | (6) |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  | $Specificity=\frac{TN}{TN+FP}$ |  | (6) |'
- en: PR curves provides a visual representation of the performance of a classification
    model across various levels of precision and recall by plotting precision against
    recall at different classification thresholds.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: PR 曲线通过在不同分类阈值下绘制精度与召回率，提供了分类模型在各种精度和召回水平下的性能的可视化表示。
- en: F-measure [[169](#bib.bib169)] is a harmonic mean of average precision and average
    recall. We compute the F-measure as
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: F-measure [[169](#bib.bib169)] 是平均精度和平均召回率的调和平均数。我们计算 F-measure 的公式如下：
- en: '|  | $F_{\beta}=\frac{(1+\beta^{2})\times{\rm Precision}\times{\rm Recall}}{\beta^{2}\times{\rm
    Precision+Recall}}$ |  | (7) |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{\beta}=\frac{(1+\beta^{2})\times{\rm Precision}\times{\rm Recall}}{\beta^{2}\times{\rm
    Precision+Recall}}$ |  | (7) |'
- en: where we set $\beta^{2}$ to be 0.3 to weigh precision more than recall.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们将 $\beta^{2}$ 设置为 0.3，以便将精度的权重高于召回率。
- en: MAE (Mean Absolute Error) [[170](#bib.bib170)] is calculated as the average
    pixel-wise absolute difference between the binary ${GT}$ and the saliency map
    ${S}$.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: MAE (均方误差) [[170](#bib.bib170)] 计算为二值 ${GT}$ 与显著图 ${S}$ 之间的平均像素绝对差值。
- en: '|  | $MAE=\frac{1}{W\times H}\sum\limits_{x=1}^{W}\sum\limits_{y=1}^{H}\Big{&#124;}{S}(x,y)-{GT}(x,y)\Big{&#124;}$
    |  | (8) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  | $MAE=\frac{1}{W\times H}\sum\limits_{x=1}^{W}\sum\limits_{y=1}^{H}\Big{|}{S}(x,y)-{GT}(x,y)\Big{|}$
    |  | (8) |'
- en: where $W$ and $H$ are width and height of the saliency map $S$, respectively.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W$ 和 $H$ 分别是显著图 $S$ 的宽度和高度。
- en: 'Weighted F-measure. Weighted F-measure [[171](#bib.bib171)] define weighted
    precision, which is a measure of exactness, and weighted recall, which is a measure
    of completeness:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 加权 F-measure。加权 F-measure [[171](#bib.bib171)] 定义加权精度，这是精确度的衡量指标，以及加权召回率，这是完整性的衡量指标：
- en: '|  | $F_{\beta}^{w}=\frac{(1+\beta^{2})\times{\rm Precision}^{w}\times{\rm
    Recall}^{w}}{\beta^{2}\times{\rm Precision}^{w}+{\rm Recall}^{w}}$ |  | (9) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{\beta}^{w}=\frac{(1+\beta^{2})\times{\rm Precision}^{w}\times{\rm
    Recall}^{w}}{\beta^{2}\times{\rm Precision}^{w}+{\rm Recall}^{w}}$ |  | (9) |'
- en: 'S-measure  [[172](#bib.bib172)] simultaneously evaluates region-aware $S_{r}$
    and object-aware $S_{o}$ structural similarity between the saliency map and ground
    truth. It can be written as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: S-measure [[172](#bib.bib172)] 同时评估显著图和真实值之间的区域感知 $S_{r}$ 和目标感知 $S_{o}$ 结构相似性。可以写成如下形式：
- en: '|  | $S_{m}=\alpha\times S_{o}+(1-\alpha)\times S_{r}$ |  | (10) |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{m}=\alpha\times S_{o}+(1-\alpha)\times S_{r}$ |  | (10) |'
- en: where $\alpha$ is set to 0.5.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 设置为 0.5。
- en: E-measure [[173](#bib.bib173)] combines local pixel values with the image-level
    mean value to jointly evaluate the similarity between the prediction and the ground
    truth.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: E-measure [[173](#bib.bib173)] 将局部像素值与图像级别的均值结合起来，共同评估预测与真实值之间的相似性。
- en: FPS (Frame Per Second) is commonly used to assess the efficiency of model inference.
    FPS is a crucial indicator for evaluating a model’s inference speed and real-time
    performance.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: FPS (每秒帧数) 通常用于评估模型推理的效率。FPS 是评估模型推理速度和实时性能的关键指标。
- en: '|    Method | PolypGen-C1 | PolypGen-C2 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|    方法 | PolypGen-C1 | PolypGen-C2 |'
- en: '| mIoU | mDice | Rec. | Prec. | mIoU | mDice | Rec. | Prec. |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| mIoU | mDice | 召回率 | 精度 | mIoU | mDice | 召回率 | 精度 |'
- en: '| UNet [[3](#bib.bib3)] | .577 | .647 | .678 | .846 | .570 | .634 | .735 |
    .737 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| UNet [[3](#bib.bib3)] | .577 | .647 | .678 | .846 | .570 | .634 | .735 |
    .737 |'
- en: '| UNet++ [[180](#bib.bib180)] | .586 | .661 | .695 | .825 | .561 | .624 | .719
    | .763 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| UNet++ [[180](#bib.bib180)] | .586 | .661 | .695 | .825 | .561 | .624 | .719
    | .763 |'
- en: '| ResUNet++ [[5](#bib.bib5)] | .420 | .524 | .639 | .579 | .278 | .343 | .500
    | .420 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| ResUNet++ [[5](#bib.bib5)] | .420 | .524 | .639 | .579 | .278 | .343 | .500
    | .420 |'
- en: '| MSEG [[179](#bib.bib179)] | .626 | .712 | .780 | .793 | .567 | .631 | .727
    | .715 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| MSEG [[179](#bib.bib179)] | .626 | .712 | .780 | .793 | .567 | .631 | .727
    | .715 |'
- en: '| LDNet [[49](#bib.bib49)] | .639 | .719 | .755 | .848 | .609 | .689 | .854
    | .687 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| LDNet [[49](#bib.bib49)] | .639 | .719 | .755 | .848 | .609 | .689 | .854
    | .687 |'
- en: '| TGANet [[28](#bib.bib28)] | .448 | .539 | .642 | .691 | .378 | .458 | .637
    | .524 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| TGANet [[28](#bib.bib28)] | .448 | .539 | .642 | .691 | .378 | .458 | .637
    | .524 |'
- en: '| XBFormer [[86](#bib.bib86)] | .654 | .720 | .744 | .878 | .661 | .723 | .807
    | .810 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| XBFormer [[86](#bib.bib86)] | .654 | .720 | .744 | .878 | .661 | .723 | .807
    | .810 |'
- en: '|   | PolypGen-C3 | PolypGen-C4 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|   | PolypGen-C3 | PolypGen-C4 |'
- en: '| UNet [[3](#bib.bib3)] | .677 | .748 | .764 | .879 | .370 | .415 | .655 |
    .598 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| UNet [[3](#bib.bib3)] | .677 | .748 | .764 | .879 | .370 | .415 | .655 |
    .598 |'
- en: '| UNet++ [[180](#bib.bib180)] | .653 | .725 | .753 | .857 | .381 | .420 | .634
    | .610 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| UNet++ [[180](#bib.bib180)] | .653 | .725 | .753 | .857 | .381 | .420 | .634
    | .610 |'
- en: '| ResUNet++ [[5](#bib.bib5)] | .410 | .511 | .646 | .548 | .169 | .227 | .634
    | .282 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| ResUNet++ [[5](#bib.bib5)] | .410 | .511 | .646 | .548 | .169 | .227 | .634
    | .282 |'
- en: '| MSEG [[179](#bib.bib179)] | .662 | .744 | .795 | .818 | .352 | .394 | .676
    | .553 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| MSEG [[179](#bib.bib179)] | .662 | .744 | .795 | .818 | .352 | .394 | .676
    | .553 |'
- en: '| LDNet [[49](#bib.bib49)] | .707 | .787 | .795 | .889 | .427 | .483 | .737
    | .630 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| LDNet [[49](#bib.bib49)] | .707 | .787 | .795 | .889 | .427 | .483 | .737
    | .630 |'
- en: '| TGANet [[28](#bib.bib28)] | .465 | .553 | .626 | .687 | .226 | .276 | .665
    | .355 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| TGANet [[28](#bib.bib28)] | .465 | .553 | .626 | .687 | .226 | .276 | .665
    | .355 |'
- en: '| XBFormer [[86](#bib.bib86)] | .722 | .787 | .790 | .913 | .460 | .504 | .687
    | .714 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| XBFormer [[86](#bib.bib86)] | .722 | .787 | .790 | .913 | .460 | .504 | .687
    | .714 |'
- en: '|   | PolypGen-C5 | PolypGen-C6 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|   | PolypGen-C5 | PolypGen-C6 |'
- en: '| UNet [[3](#bib.bib3)] | .296 | .361 | .458 | .550 | .538 | .613 | .705 |
    .751 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| UNet [[3](#bib.bib3)] | .296 | .361 | .458 | .550 | .538 | .613 | .705 |
    .751 |'
- en: '| UNet++ [[180](#bib.bib180)] | .314 | .377 | .447 | .603 | .536 | .616 | .734
    | .723 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| UNet++ [[180](#bib.bib180)] | .314 | .377 | .447 | .603 | .536 | .616 | .734
    | .723 |'
- en: '| ResUNet++ [[5](#bib.bib5)] | .204 | .275 | .464 | .303 | .282 | .368 | .622
    | .353 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| ResUNet++ [[5](#bib.bib5)] | .204 | .275 | .464 | .303 | .282 | .368 | .622
    | .353 |'
- en: '| MSEG [[179](#bib.bib179)] | .309 | .377 | .459 | .525 | .555 | .634 | .720
    | .772 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| MSEG [[179](#bib.bib179)] | .309 | .377 | .459 | .525 | .555 | .634 | .720
    | .772 |'
- en: '| LDNet [[49](#bib.bib49)] | .326 | .403 | .494 | .562 | .604 | .675 | .770
    | .767 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| LDNet [[49](#bib.bib49)] | .326 | .403 | .494 | .562 | .604 | .675 | .770
    | .767 |'
- en: '| TGANet [[28](#bib.bib28)] | .253 | .329 | .465 | .419 | .374 | .454 | .602
    | .505 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| TGANet [[28](#bib.bib28)] | .253 | .329 | .465 | .419 | .374 | .454 | .602
    | .505 |'
- en: '| XBFormer [[86](#bib.bib86)] | .360 | .421 | .451 | .777 | .634 | .692 | .678
    | .943 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| XBFormer [[86](#bib.bib86)] | .360 | .421 | .451 | .777 | .634 | .692 | .678
    | .943 |'
- en: '|   |  |  |  |  |  |  |  |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |'
- en: 'TABLE VI: Results of the models trained on Kvasir-SEG and tested on multi-centre
    colonoscopy dataset PolypGen.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：在 Kvasir-SEG 数据集上训练并在多中心结肠镜检查数据集 PolypGen 上测试的模型结果
- en: 5 Performance Benchmarking
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 性能基准测试
- en: 'This section presents empirical analyses to illustrate essential challenges
    and progress in the CPS field. Firstly, we evaluate the current state-of-the-art
    (SOTA) polyp segmentation models and report their performance on five commonly
    used benchmark datasets (Sec. [5.1](#S5.SS1 "5.1 SOTA Performance Comparisons
    ‣ 5 Performance Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")). Secondly, we assess SOTA polyp segmentation models’
    generalizability on out-of-distribution datasets belonging to different medical
    centers (Sec. [5.2](#S5.SS2 "5.2 Out-of-Distribution Generalization ‣ 5 Performance
    Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey")). Subsequently, we undertake an attribute-based study to understand better
    the inherent strengths and weaknesses in current models (Sec. [5.3](#S5.SS3 "5.3
    Attribute-based Performance Analysis ‣ 5 Performance Benchmarking ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '本节展示了实证分析，以说明 CPS 领域中的主要挑战和进展。首先，我们评估当前最先进的 (SOTA) 息肉分割模型，并报告其在五个常用基准数据集上的性能（参见
    [5.1](#S5.SS1 "5.1 SOTA Performance Comparisons ‣ 5 Performance Benchmarking ‣
    Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")）。其次，我们评估
    SOTA 息肉分割模型在不同医疗中心的分布外数据集上的泛化能力（参见 [5.2](#S5.SS2 "5.2 Out-of-Distribution Generalization
    ‣ 5 Performance Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")）。随后，我们进行基于属性的研究，以更好地理解当前模型的固有优缺点（参见 [5.3](#S5.SS3
    "5.3 Attribute-based Performance Analysis ‣ 5 Performance Benchmarking ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey")）。'
- en: 5.1 SOTA Performance Comparisons
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 SOTA 性能比较
- en: 'Table [V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey") illustrates the performance
    of 40 cutting-edge deep CPS models across five widely used benchmark datasets,
    and the methodologies proposed over the last three years are emphasized. These
    SOTA models were measured by two commonly adopted metrics, i.e., mDice and mIoU.
    To ensure a fair comparison, we follow the same setting as PraNet [[6](#bib.bib6)],
    which includes 900 and 548 images from ClinicDB and Kvasir-Seg datasets as the
    train set, and the remaining 64 and 100 images are used as the test set. For clarity,
    we highlight the best and second performances using red and blue colors. We aspire
    that our performance benchmarking will contribute to establishing an open and
    standardized evaluation system in the CPS community.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '表[V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")展示了40个前沿深度CPS模型在五个广泛使用的基准数据集上的表现，并强调了过去三年提出的方法。这些SOTA模型通过两个常用指标，即mDice和mIoU进行测量。为了确保公平比较，我们采用了与PraNet
    [[6](#bib.bib6)]相同的设置，其中包含来自ClinicDB和Kvasir-Seg数据集的900和548张图像作为训练集，剩余的64张和100张图像用于测试集。为了清晰起见，我们使用红色和蓝色标出最佳和第二佳性能。我们希望我们的性能基准测试能有助于建立CPS社区中的开放和标准化评估系统。'
- en: 'As shown in Table [V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp
    Segmentation in the Deep Learning Era: A Comprehensive Survey"), RPFA [[71](#bib.bib71)]
    and PPFormer [[105](#bib.bib105)] achieved the Top 2 performance on CVC-ClinicDB,
    Kvasir-SEG, CVC-ColonDB, ETIS-Larib and EndoScene datasets. In the CVC-ColonDB
    and On ETIS-Larib datasets, RPFA demonstrated its superior performance, outperforming
    PPFormer by 1.4% and 3.1% in terms of mDice, respectively. Conversely, for the
    CVC-ClinicDB and EndoScene datasets, PPFormer achieves the best performance by
    surpassing the RPFA model by 1.5% and 1.4%. As anticipated, the overall learning
    capabilities of these deep learning-based models continue to improvement over
    time. For instance, the UNet [[3](#bib.bib3)] segmentation model, introduced in
    2015, initially achieved only 82.3% in terms of mDice on the CVC-ClinicDB dataset.
    Presently, the recently proposed PPFormer achieves an impressive 94.6%, signaling
    an annual improvement in segmentation accuracy of nearly 1.4%.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey")所示，RPFA [[71](#bib.bib71)]和PPFormer
    [[105](#bib.bib105)]在CVC-ClinicDB、Kvasir-SEG、CVC-ColonDB、ETIS-Larib和EndoScene数据集上均取得了前两名的表现。在CVC-ColonDB和ETIS-Larib数据集上，RPFA展示了其优越的性能，在mDice指标上分别超越了PPFormer
    1.4%和3.1%。相反，对于CVC-ClinicDB和EndoScene数据集，PPFormer凭借超越RPFA模型1.5%和1.4%的表现，达到了最佳性能。正如预期，这些基于深度学习的模型的整体学习能力随着时间的推移持续改进。例如，2015年引入的UNet
    [[3](#bib.bib3)]分割模型在CVC-ClinicDB数据集上的mDice指标初始值仅为82.3%。目前，最近提出的PPFormer达到了令人印象深刻的94.6%，显示出分割准确度年均提升近1.4%。'
- en: 5.2 Out-of-Distribution Generalization
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 分布外泛化
- en: 'Generalization measures how effectively a well-trained model can be applied
    to out-of-distribution data. A model with strong generalization can make accurate
    predictions on unseen data, which is essential for deploying deep learning-based
    poppy segmentation models in real-world clinical scenarios. To assess the SOTA
    model’s generalization ability, we first employ three datasets not encountered
    during training: ETIS, CVC-ColonDB, and EndoScene. These datasets comprise a combined
    total of 196, 300, and 912 images, respectively. Unsurprisingly, as shown in Table
    [V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation in
    the Deep Learning Era: A Comprehensive Survey"), the model exhibits considerably
    lower performance on ETIS, ColonDB, and EndoScene than on the training datasets
    (CVC-ClinicDB and Kvasir-SEG). For instance, the performance of RPFA [[71](#bib.bib71)]
    on the CVC-ColonDB dataset is approximately 10% lower than on the CVC-ClinicDB
    dataset.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '泛化度衡量了一个经过良好训练的模型在分布外数据上的应用效果。具有强泛化能力的模型可以对未见过的数据做出准确预测，这对于将基于深度学习的息肉分割模型部署到现实世界的临床场景中至关重要。为了评估SOTA模型的泛化能力，我们首先使用了训练过程中未遇到的三个数据集：ETIS、CVC-ColonDB和EndoScene。这些数据集的图像总数分别为196、300和912。正如表[V](#S4.T5
    "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey")所示，该模型在ETIS、ColonDB和EndoScene上的表现明显低于在训练数据集（CVC-ClinicDB和Kvasir-SEG）上的表现。例如，RPFA
    [[71](#bib.bib71)]在CVC-ColonDB数据集上的表现比在CVC-ClinicDB数据集上的表现低约10%。'
- en: 'Additionally, we explore the generalization ability of these SOTA models on
    the PolypGen dataset [[24](#bib.bib24)], collected from six different centers
    representing diverse populations. Consequently, validating these SOTA models on
    PolypGen enhances the comprehensiveness of the study and brings it closer to real-world
    scenarios. As shown in Table [VI](#S4.T6 "TABLE VI ‣ 4 Evaluation Metrics ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"), compared
    to the ETIS, CVC-ColonDB, and EndoScene dataset, these SOTA models exhibit a more
    pronounced performance degradation on the PolypGen dataset. In particular, XBFormer
    [[86](#bib.bib86)] achieves 87.5% in terms of mIoU on CVC-ColinicDB while only
    36% on PolypGen-C5\. This performance drop is more substantial compared to ETIS,
    CVC-ColonDB, and EndoScene datasets. Therefore, we can easily conclude that when
    the distribution of the test dataset is inconsistent with the training data set,
    the model performance will decrease significantly, and the degree of model performance
    degradation is proportional to the degree of inconsistency.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们探讨了这些SOTA模型在PolypGen数据集 [[24](#bib.bib24)] 上的泛化能力，该数据集来自六个不同的中心，代表了多样化的人群。因此，在PolypGen上验证这些SOTA模型增强了研究的全面性，并使其更接近实际场景。如表
    [VI](#S4.T6 "TABLE VI ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation in
    the Deep Learning Era: A Comprehensive Survey")所示，与ETIS、CVC-ColonDB和EndoScene数据集相比，这些SOTA模型在PolypGen数据集上的性能降级更加明显。特别是，XBFormer
    [[86](#bib.bib86)] 在CVC-ColonDB上的mIoU为87.5%，而在PolypGen-C5上的mIoU仅为36%。这种性能下降相较于ETIS、CVC-ColonDB和EndoScene数据集更为显著。因此，我们可以很容易地得出结论，当测试数据集的分布与训练数据集不一致时，模型性能将显著下降，且模型性能下降的程度与不一致的程度成正比。
    |'
- en: '| Attr. | Description |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 描述 |'
- en: '| SI | Surgical Instruments. The endoscopic surgical procedures involve the
    positioning of instruments, such as snares, forceps, knives, and electrodes. |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| SI | 外科手术工具。内窥镜手术程序涉及工具的定位，如套索、镊子、刀具和电极。 |'
- en: '| IB | Indefinable Boundaries. The foreground and background areas around the
    object have similar color. |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| IB | 不可定义的边界。物体周围的前景和背景区域颜色相似。 |'
- en: '| HO | Heterogeneous Object. Object regions have distinct colors. |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| HO | 异质物体。物体区域有不同的颜色。 |'
- en: '| GH | Ghosting. Object has anomaly RGB-colored boundary due to fast moving
    or insufficient refresh rate. |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| GH | 虚影。由于快速移动或刷新率不足，物体有异常的RGB颜色边界。 |'
- en: '| FM | Fast-motion. The average per-frame object motion in a clip, computed
    as the Euclidean distance of polyp centroids between consecutive frames, is larger
    than 20 pixels |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| FM | 快速运动。剪辑中每帧的平均物体运动，以连续帧之间息肉质心的欧几里得距离计算，大于 20 像素。 |'
- en: '| SO | Small Object. The average ratio between the object size and the image
    area in a clip is smaller than 0.05. |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| SO | 小物体。剪辑中物体大小与图像面积的平均比率小于 0.05。 |'
- en: '| LO | Large Object. The average ratio between the object size and the image
    area in a clip is larger than 0.15. |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| LO | 大物体。剪辑中物体大小与图像面积的平均比率大于 0.15。 |'
- en: '| OC | Occlusion. Polyp object becomes partially or fully occluded. |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| OC | 遮挡。息肉物体部分或完全被遮挡。 |'
- en: '| OV | Out-of-view. Polyp object is partially clipped by the image boundaries.
    |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| OV | 超出视野。息肉物体被图像边界部分剪裁。 |'
- en: '| SV | Scale-variation. The average area ratio among any pair of bounding boxes
    enclosing the target object in a clip is smaller than 0.5. |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| SV | 尺度变化。剪辑中包围目标物体的任何一对边界框的平均面积比小于 0.5。 |'
- en: 'TABLE VII: Visual attributes and descriptions of SUN-SEG dataset.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：SUN-SEG 数据集的视觉属性及描述。 |
- en: 5.3 Attribute-based Performance Analysis
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 基于属性的性能分析 |
- en: '|    Method | SUN-SEG Easy | SUN-SEG Hard |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|    方法 | SUN-SEG 简单 | SUN-SEG 困难 |'
- en: '| SI | IB | HO | GH | FM | SO | LO | OC | OV | SV | SI | IB | HO | GH | FM
    | SO | LO | OC | OV | SV |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| SI | IB | HO | GH | FM | SO | LO | OC | OV | SV | SI | IB | HO | GH | FM
    | SO | LO | OC | OV | SV |'
- en: '| UNet [[3](#bib.bib3)] | .675 | .548 | .768 | .715 | .633 | .593 | .648 |
    .670 | .643 | .620 | .618 | .619 | .663 | .676 | .713 | .689 | .633 | .658 | .659
    | .658 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| UNet [[3](#bib.bib3)] | .675 | .548 | .768 | .715 | .633 | .593 | .648 |
    .670 | .643 | .620 | .618 | .619 | .663 | .676 | .713 | .689 | .633 | .658 | .659
    | .658 |'
- en: '| UNet++ [[180](#bib.bib180)] | .701 | .542 | .782 | .739 | .647 | .591 | .678
    | .683 | .665 | .617 | .654 | .604 | .665 | .696 | .714 | .681 | .660 | .676 |
    .677 | .678 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| UNet++ [[180](#bib.bib180)] | .701 | .542 | .782 | .739 | .647 | .591 | .678
    | .683 | .665 | .617 | .654 | .604 | .665 | .696 | .714 | .681 | .660 | .676 |
    .677 | .678 |'
- en: '| COSNet [[181](#bib.bib181)] | .663 | .531 | .786 | .684 | .610 | .549 | .637
    | .648 | .613 | .617 | .641 | .593 | .727 | .668 | .690 | .637 | .694 | .707 |
    .666 | .625 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| COSNet [[181](#bib.bib181)] | .663 | .531 | .786 | .684 | .610 | .549 | .637
    | .648 | .613 | .617 | .641 | .593 | .727 | .668 | .690 | .637 | .694 | .707 |
    .666 | .625 |'
- en: '| ACSNet [[67](#bib.bib67)] | .789 | .612 | .896 | .820 | .704 | .663 | .787
    | .770 | .759 | .705 | .770 | .681 | .828 | .795 | .817 | .738 | .810 | .828 |
    .806 | .759 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| ACSNet [[67](#bib.bib67)] | .789 | .612 | .896 | .820 | .704 | .663 | .787
    | .770 | .759 | .705 | .770 | .681 | .828 | .795 | .817 | .738 | .810 | .828 |
    .806 | .759 |'
- en: '| PraNet [[6](#bib.bib6)] | .745 | .585 | .821 | .772 | .673 | .611 | .722
    | .722 | .703 | .653 | .673 | .635 | .725 | .720 | .755 | .691 | .666 | .714 |
    .708 | .703 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| PraNet [[6](#bib.bib6)] | .745 | .585 | .821 | .772 | .673 | .611 | .722
    | .722 | .703 | .653 | .673 | .635 | .725 | .720 | .755 | .691 | .666 | .714 |
    .708 | .703 |'
- en: '| SANet [[34](#bib.bib34)] | .724 | .582 | .854 | .760 | .676 | .615 | .703
    | .701 | .711 | .680 | .658 | .565 | .738 | .709 | .760 | .692 | .733 | .729 |
    .727 | .693 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| SANet [[34](#bib.bib34)] | .724 | .582 | .854 | .760 | .676 | .615 | .703
    | .701 | .711 | .680 | .658 | .565 | .738 | .709 | .760 | .692 | .733 | .729 |
    .727 | .693 |'
- en: '| MAT [[182](#bib.bib182)] | .772 | .664 | .873 | .789 | .706 | .691 | .755
    | .738 | .746 | .715 | .772 | .701 | .801 | .776 | .782 | .780 | .791 | .795 |
    .789 | .750 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| MAT [[182](#bib.bib182)] | .772 | .664 | .873 | .789 | .706 | .691 | .755
    | .738 | .746 | .715 | .772 | .701 | .801 | .776 | .782 | .780 | .791 | .795 |
    .789 | .750 |'
- en: '| PCSA [[183](#bib.bib183)] | .676 | .563 | .759 | .708 | .628 | .610 | .634
    | .662 | .656 | .616 | .656 | .591 | .692 | .683 | .706 | .671 | .612 | .677 |
    .665 | .663 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| PCSA [[183](#bib.bib183)] | .676 | .563 | .759 | .708 | .628 | .610 | .634
    | .662 | .656 | .616 | .656 | .591 | .692 | .683 | .706 | .671 | .612 | .677 |
    .665 | .663 |'
- en: '| AMD [[184](#bib.bib184)] | .476 | .461 | .471 | .481 | .484 | .466 | .447
    | .467 | .442 | .498 | .471 | .468 | .447 | .473 | .468 | .469 | .453 | .487 |
    .462 | .481 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| AMD [[184](#bib.bib184)] | .476 | .461 | .471 | .481 | .484 | .466 | .447
    | .467 | .442 | .498 | .471 | .468 | .447 | .473 | .468 | .469 | .453 | .487 |
    .462 | .481 |'
- en: '| DCF [[185](#bib.bib185)] | .465 | .485 | .479 | .505 | .541 | .495 | .362
    | .484 | .492 | .495 | .441 | .508 | .422 | .498 | .587 | .556 | .351 | .470 |
    .494 | .540 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| DCF [[185](#bib.bib185)] | .465 | .485 | .479 | .505 | .541 | .495 | .362
    | .484 | .492 | .495 | .441 | .508 | .422 | .498 | .587 | .556 | .351 | .470 |
    .494 | .540 |'
- en: '| FSNet [[186](#bib.bib186)] | .719 | .603 | .810 | .752 | .694 | .632 | .686
    | .711 | .691 | .665 | .662 | .648 | .743 | .713 | .774 | .723 | .701 | .728 |
    .728 | .694 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| FSNet [[186](#bib.bib186)] | .719 | .603 | .810 | .752 | .694 | .632 | .686
    | .711 | .691 | .665 | .662 | .648 | .743 | .713 | .774 | .723 | .701 | .728 |
    .728 | .694 |'
- en: '| PNSNet [[124](#bib.bib124)] | .789 | .592 | .871 | .820 | .723 | .619 | .768
    | .749 | .751 | .705 | .746 | .631 | .803 | .780 | .778 | .743 | .805 | .790 |
    .794 | .758 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| PNSNet [[124](#bib.bib124)] | .789 | .592 | .871 | .820 | .723 | .619 | .768
    | .749 | .751 | .705 | .746 | .631 | .803 | .780 | .778 | .743 | .805 | .790 |
    .794 | .758 |'
- en: '| HBNet [[187](#bib.bib187)] | .809 | .625 | .899 | .835 | .728 | .667 | .820
    | .783 | .778 | .719 | .768 | .662 | .865 | .784 | .797 | .737 | .853 | .827 |
    .808 | .765 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| HBNet [[187](#bib.bib187)] | .809 | .625 | .899 | .835 | .728 | .667 | .820
    | .783 | .778 | .719 | .768 | .662 | .865 | .784 | .797 | .737 | .853 | .827 |
    .808 | .765 |'
- en: '| PNS+ [[2](#bib.bib2)] | .819 | .667 | .883 | .844 | .738 | .690 | .796 |
    .782 | .798 | .734 | .770 | .703 | .817 | .801 | .823 | .793 | .792 | .808 | .807
    | .795 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| PNS+ [[2](#bib.bib2)] | .819 | .667 | .883 | .844 | .738 | .690 | .796 |
    .782 | .798 | .734 | .770 | .703 | .817 | .801 | .823 | .793 | .792 | .808 | .807
    | .795 |'
- en: '|   |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: 'TABLE VIII: Visual attributes-based performance comparisons on SUN-SEG-Easy/Hard
    using structure measure.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VIII: 基于视觉属性的 SUN-SEG-Easy/Hard 上结构测量的性能比较。'
- en: '![Refer to caption](img/04403e957f92d306faf322ccef75ddd9.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/04403e957f92d306faf322ccef75ddd9.png)'
- en: 'Figure 8: Qualitative visualization of the selected representative deep CPS
    models on SUN-SEG dataset.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 在 SUN-SEG 数据集上选择的代表性深度 CPS 模型的定性可视化。'
- en: 'While the community has observed significant progress in deep CPS models, there
    remains a lack of clarity regarding the specific conditions under which these
    models perform well. Given the multitude of factors influencing CPS algorithm
    performance, including scene category and occlusion, it becomes imperative to
    assess their effectiveness across diverse scenarios. This evaluation is essential
    for uncovering deep CPS models’ nuanced strengths and weaknesses, identifying
    unresolved challenges, and pinpointing prospective research avenues to develop
    more robust algorithms. This paper employs the recently proposed SUN-SEG dataset
    [[2](#bib.bib2)], containing 158 690 frames selected from the SUNdatabase [[188](#bib.bib188)].
    Ji et al. [[2](#bib.bib2)] divided the SUN-SEG dataset into SUN-SEG-Easy with
    119 clips (17, 070 frames) and SUN-SEG-Hard with 54 clips (12, 522 frames) according
    to difficulty levels in each pathological category, and further provide ten visual
    attributes according to the visual characteristics of images, as detailed in Table
    [VII](#S5.T7 "TABLE VII ‣ 5.2 Out-of-Distribution Generalization ‣ 5 Performance
    Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey").'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管社区在深度 CPS 模型方面观察到显著进展，但仍不清楚这些模型在特定条件下的表现。考虑到影响 CPS 算法性能的因素众多，包括场景类别和遮挡，评估它们在不同场景下的效果显得尤为重要。这一评估对于揭示深度
    CPS 模型的细微优缺点、识别未解决的挑战以及找出未来的研究方向至关重要。本文使用了最近提出的 SUN-SEG 数据集 [[2](#bib.bib2)]，该数据集包含从
    SUNdatabase [[188](#bib.bib188)] 中选取的 158 690 帧。Ji 等人 [[2](#bib.bib2)] 根据每个病理类别的难度将
    SUN-SEG 数据集分为 SUN-SEG-Easy（119 个片段，17,070 帧）和 SUN-SEG-Hard（54 个片段，12,522 帧），并根据图像的视觉特征提供了十个视觉属性，详细信息见表
    [VII](#S5.T7 "TABLE VII ‣ 5.2 Out-of-Distribution Generalization ‣ 5 Performance
    Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey")。'
- en: 'In Table [VIII](#S5.T8 "TABLE VIII ‣ 5.3 Attribute-based Performance Analysis
    ‣ 5 Performance Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey"), we report the performance of specific attribute-based
    subsets on the SUN-SEG dataset. For clarity, we highlight the best and second
    performances using red and blue colors. Besides, as shown in Fig. [8](#S5.F8 "Figure
    8 ‣ 5.3 Attribute-based Performance Analysis ‣ 5 Performance Benchmarking ‣ Colorectal
    Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"), we also
    present visual results on these attribute-based scenarios. Below are some important
    observations drawn from these results.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [VIII](#S5.T8 "TABLE VIII ‣ 5.3 Attribute-based Performance Analysis ‣ 5
    Performance Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey") 中，我们报告了 SUN-SEG 数据集中基于特定属性的子集的性能。为清晰起见，我们用红色和蓝色标出最佳和第二佳表现。此外，如图
    [8](#S5.F8 "Figure 8 ‣ 5.3 Attribute-based Performance Analysis ‣ 5 Performance
    Benchmarking ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive
    Survey") 所示，我们还展示了这些属性场景下的视觉结果。以下是从这些结果中得出的一些重要观察。'
- en: 1) Not winner takes all. Among the compared methods, PNS+ [[2](#bib.bib2)] stands
    out as the best from an overall perspective; however, it falls short of achieving
    optimal performance in every attribute. Similarly, while the ACSNet [[67](#bib.bib67)]
    does not secure a spot in the top three rankings, it excels in achieving the best
    performance for specific attributes.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 胜者并非全得。在比较的方法中，从整体角度来看，PNS+ [[2](#bib.bib2)] 脱颖而出为最佳；然而，它在每一个属性上都未能实现最佳性能。同样，虽然
    ACSNet [[67](#bib.bib67)] 未能跻身前三名，但在某些特定属性上表现最佳。
- en: 2) Easy and hard scenes. Unexpectedly, methods tested on the SUN-SEG easy dataset
    achieved better performance than those tested on the SUN-SEG hard dataset regarding
    SI, HO, and GH attributes. Typically, the model’s performance on the SUN-SEG easy
    dataset should surpass that on the SUN-SEG hard dataset, indicating potential
    errors in the SUN-SEG dataset partitioning.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 简单和困难场景。出乎意料的是，在 SUN-SEG 易数据集上测试的方法在 SI、HO 和 GH 属性上的表现优于在 SUN-SEG 难数据集上测试的方法。通常，模型在
    SUN-SEG 易数据集上的表现应优于 SUN-SEG 难数据集，表明 SUN-SEG 数据集分区可能存在潜在错误。
- en: 3) Easy and hard attribution. For both SUN-SEG easy and SUN-SEG hard datasets,
    attribute HO is identified as the least challenging, with HBNet [[187](#bib.bib187)]
    achieving a performance score of 0.899\. Conversely, attribute IB is considered
    the most challenging, and the PNS+’s [[2](#bib.bib2)] peak performance on attribute
    IB only reaches 0.667.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 简单与困难的属性。对于SUN-SEG简单数据集和SUN-SEG困难数据集，属性HO被识别为最不具挑战性的，HBNet [[187](#bib.bib187)]
    在属性HO上的表现得分为0.899。相反，属性IB被认为是最具挑战性的，PNS+ [[2](#bib.bib2)] 在属性IB上的最佳表现仅达到0.667。
- en: 6 Challenges and Future Directions
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 挑战与未来方向
- en: Undoubtedly, deep learning has substantially advanced polyp segmentation; however,
    there are numerous challenges that need to be addressed. In the following sections,
    we will explore promising research directions that we believe will help in further
    advancing polyp segmentation.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，深度学习在息肉分割方面取得了显著进展；然而，仍然存在许多需要解决的挑战。在接下来的章节中，我们将探讨我们认为有助于进一步推进息肉分割的有前景的研究方向。
- en: 6.1 Interpretable Deep CPS Model
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 可解释的深度CPS模型
- en: The developments of deep neural networks have revolutionized the fields of artificial
    intelligence, and have achieved promising performance in polyp segmentation tasks.
    Nevertheless, the majority of deep models are designed without prioritizing interpretability,
    making them as black-box systems. The absence of a clear understanding of the
    underlying mechanisms behind predictions raises concerns about the trustworthiness
    of these deep models. This lack of transparency impedes their application in real-world
    clinical scenarios. For instance, why does the model identify the image as a polyp?
    What are the criteria for this decision? To ensure the secure and trustworthy
    deployment of deep models, it becomes imperative to deliver not only accurate
    predictions but also human-intelligible explanations, particularly for users in
    medical diagnosis. These factors mentioned above underscore the necessity for
    the development of novel techniques to interpret deep neural networks.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的发展彻底改变了人工智能领域，并在息肉分割任务中取得了令人满意的表现。然而，大多数深度模型的设计并未优先考虑可解释性，使其成为黑箱系统。对预测背后的机制缺乏清晰的理解引发了对这些深度模型可靠性的担忧。这种不透明性阻碍了它们在现实临床场景中的应用。例如，为什么模型将图像识别为息肉？做出这一决策的标准是什么？为了确保深度模型的安全可靠部署，除了提供准确的预测外，还必须提供易于理解的解释，特别是对医疗诊断用户而言。上述因素突显了开发新技术以解释深度神经网络的必要性。
- en: 6.2 Federated Learning for Data Privacy
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 数据隐私的联邦学习
- en: Current deep learning-based models are data-hungry, which means that the greater
    the volume of training data, the higher the model’s performance. However, the
    reality is that medical data is often scattered across different hospitals, safeguarded
    by privacy restrictions. For example, data from different hospitals are isolated
    and become ”data islands.” Given the constraints in size and distribution within
    each data island, a single hospital may struggle to train a high-performance model.
    In an ideal scenario, hospitals can benefit more if we can collaboratively train
    a deep learning model on the union of their data. However, the challenge is that
    we cannot straightforwardly share data among hospitals due to data privacy and
    diverse policies. The recently emerged federated learning [[189](#bib.bib189),
    [190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192)]can handle these challenges,
    which can collaboratively train machine learning models without collecting their
    local data, making it particularly suitable for scenarios where data cannot be
    easily centralized due to privacy concerns.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 当前基于深度学习的模型对数据的需求很高，这意味着训练数据的量越大，模型的表现越好。然而，现实情况是，医疗数据通常分散在不同的医院，并受到隐私限制的保护。例如，来自不同医院的数据被隔离开来，成为“数据孤岛”。鉴于每个数据孤岛内的规模和分布限制，单一医院可能难以训练出高性能的模型。在理想情况下，如果我们能够在数据的联合上协作训练深度学习模型，医院将能获得更多的收益。然而，挑战在于，由于数据隐私和多样化的政策，我们不能直接在医院之间共享数据。最近出现的联邦学习
    [[189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192)]
    可以处理这些挑战，它可以在不收集本地数据的情况下协作训练机器学习模型，使其特别适用于因隐私问题无法轻易集中数据的场景。
- en: 6.3 Domain Adaptation for Domain Shift
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 领域适应以应对领域转移
- en: 'The existing deep learning-based CPS model typically assumes that the training
    dataset (source/reference domain) and the test dataset (target domain) share the
    same data distribution. Unfortunately, this assumption is overly restrictive and
    may not be true in real-world scenarios. Owing to various factors such as illumination
    and image quality, a distribution shift commonly occurs between the training and
    testing datasets that can degrade the performance drastically. As demonstrated
    in Table [V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey") and Table [VI](#S4.T6 "TABLE
    VI ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey"), when the distribution of the test dataset is inconsistent
    with the training data set, these SOTA model’s performance decreased significantly.
    Thus, handling domain shift is crucial to effectively applying deep learning methods
    to medical image analysis. As a promising solution to tackle the distribution
    shift among medical image datasets, domain adaptation [[193](#bib.bib193), [194](#bib.bib194),
    [195](#bib.bib195)] has attracted increasing attention in many tasks, aiming to
    minimize the distribution gap among different but related domains.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '现有基于深度学习的 CPS 模型通常假设训练数据集（源/参考域）和测试数据集（目标域）共享相同的数据分布。不幸的是，这一假设过于苛刻，现实场景中可能并不成立。由于光照和图像质量等多种因素，训练数据集和测试数据集之间通常会发生分布偏移，这可能会大幅度降低性能。如表格
    [V](#S4.T5 "TABLE V ‣ 4 Evaluation Metrics ‣ Colorectal Polyp Segmentation in
    the Deep Learning Era: A Comprehensive Survey") 和表格 [VI](#S4.T6 "TABLE VI ‣ 4
    Evaluation Metrics ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A
    Comprehensive Survey") 所示，当测试数据集的分布与训练数据集不一致时，这些 SOTA 模型的性能显著下降。因此，处理领域偏移对于有效应用深度学习方法于医学图像分析至关重要。作为应对医学图像数据集间分布偏移的一个有前途的解决方案，领域适应
    [[193](#bib.bib193), [194](#bib.bib194), [195](#bib.bib195)] 在许多任务中引起了越来越多的关注，旨在最小化不同但相关领域之间的分布差距。'
- en: 6.4 Defending Against Adversarial Attack
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 防御对抗攻击
- en: Recent research revealed that deep models are susceptible to adversarial attacks,
    which may be invisible to the human eye but can lead the model to misclassify
    the output. Szegedy et al. [[196](#bib.bib196)] was the first to demonstrate that
    high-performing deep neural networks can also fall prey to adversarial attacks.
    Su et al. [[197](#bib.bib197)] claimed successful fooling of three different network
    models on the tested images by changing only one pixel per image. Additionally,
    they observed that the average confidence of the networks in assigning incorrect
    labels was 97.47%. Such an adversarial attack poses a significant challenge to
    deploying deep CPS models in real-world clinical applications. In particular,
    the model’s accurate and reliable diagnoses are extremely important in healthcare
    because misdiagnosis may cause severe consequences, including patient mortality.
    Hence, it is extremely urgent to improve the robustness of the deep CPS model
    against various adversaries.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 近期研究揭示了深度模型容易受到对抗攻击，这些攻击可能对人眼不可见，但会导致模型错误分类。Szegedy 等人 [[196](#bib.bib196)]
    首次证明了高性能的深度神经网络也会受到对抗攻击。Su 等人 [[197](#bib.bib197)] 通过仅改变每张图像中的一个像素，成功欺骗了三种不同的网络模型。他们还观察到，网络在分配错误标签时的平均置信度为
    97.47%。这种对抗攻击对将深度 CPS 模型应用于现实临床中的挑战非常大。特别是，模型的准确和可靠的诊断在医疗保健中极为重要，因为误诊可能导致严重后果，包括患者死亡。因此，迫切需要提高深度
    CPS 模型对各种对抗攻击的鲁棒性。
- en: 6.5 Weakly/Un-supervised Learning
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 弱监督/无监督学习
- en: 'As shown in Table [II](#S2.T2 "TABLE II ‣ 2.1.2 CNN-based Method ‣ 2.1 Network
    Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the
    Deep Learning Era: A Comprehensive Survey") and Table [III](#S2.T3 "TABLE III
    ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey)
    ‣ Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey"),
    existing deep CPS models are commonly trained in a fully-supervised manner, relying
    on numerous meticulously annotated pixel-level groundtruths. However, constructing
    such a well-annotated pixel-level dataset is resource-intensive and time-consuming.
    Recently, there have been some approaches to training a CPS model with weak supervision,
    such as bounding-box [[75](#bib.bib75)] and scribble level [[73](#bib.bib73)]
    annotations, but performance disparity remains compared to a fully-supervised
    model. Another noteworthy direction is self-supervised learning, which has gained
    considerable attention across various tasks. Misra et al. [[198](#bib.bib198)]
    have proved that a self-supervised model can capture intricate image details,
    facilitating the training of segmentation models. With the multitude of algorithmic
    breakthroughs witnessed in recent years, we anticipate a surge of innovation in
    this promising direction.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格 [II](#S2.T2 "TABLE II ‣ 2.1.2 CNN-based Method ‣ 2.1 Network Architectures
    ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation in the Deep Learning
    Era: A Comprehensive Survey") 和表格 [III](#S2.T3 "TABLE III ‣ 2.1.2 CNN-based Method
    ‣ 2.1 Network Architectures ‣ 2 Methodology (Survey) ‣ Colorectal Polyp Segmentation
    in the Deep Learning Era: A Comprehensive Survey") 所示，现有的深度CPS模型通常以完全监督的方式进行训练，依赖于大量精心标注的像素级真值。然而，构建这样一个标注良好的像素级数据集是资源密集型且耗时的。最近，一些方法尝试使用弱监督来训练CPS模型，例如边界框[[75](#bib.bib75)]和涂鸦级[[73](#bib.bib73)]标注，但与完全监督模型相比，性能差距仍然存在。另一个值得注意的方向是自监督学习，这在各种任务中获得了相当大的关注。Misra等人[[198](#bib.bib198)]证明了自监督模型能够捕捉复杂的图像细节，促进了分割模型的训练。鉴于近年来算法的突破，我们预计这一有前途的方向会涌现出更多创新。'
- en: 6.6 Lightweight Model for Real-world Application
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 适用于现实世界应用的轻量级模型
- en: Advanced deep CPS models are intricately designed to enhance learning capacity
    and the model’s performance. However, there is a growing requirement for more
    innovative and lightweight architectures to meet the demands of mobile and embedded
    devices. Dollar et al. [[199](#bib.bib199)] demonstrated that simply scaling the
    model capacity significantly incurs performance degradation in terms of accuracy
    and generalization. To facilitate the practical implementation of deep CPS models
    in clinical settings, we recommend developing a lightweight model to maintain
    a good balance between performance and efficiency. Another recommended direction
    is employing model compression [[200](#bib.bib200)] or knowledge distillation
    [[201](#bib.bib201)] to condense these heavy and high-performance models, which
    can ensure minimal performance degradation and obtain a lightweight model.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 先进的深度CPS模型经过精心设计，以增强学习能力和模型性能。然而，针对移动设备和嵌入式设备的需求，日益需要更加创新和轻量级的架构。Dollar等人[[199](#bib.bib199)]展示了单纯扩大模型容量会显著导致准确性和泛化能力的性能下降。为了促进深度CPS模型在临床环境中的实际应用，我们建议开发一种轻量级模型，以保持性能和效率之间的良好平衡。另一个推荐方向是采用模型压缩[[200](#bib.bib200)]或知识蒸馏[[201](#bib.bib201)]来精简这些庞大且高性能的模型，以确保最小的性能下降并获得轻量级模型。
- en: 6.7 Connecting CPS with Anomaly Detection
  id: totrans-420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 将CPS与异常检测相结合
- en: Anomaly detection [[202](#bib.bib202)], also known as outlier detection, aims
    to identify patterns or instances that deviate significantly from the norm or
    expected behavior within a dataset. The goal is to identify data points that are
    considered rare, unusual, or suspicious compared to the majority of the data.
    In the polyp dataset, a significant portion of images does not contain polyps,
    with only a subset containing polyps. Thus, polyps in an image can be considered
    typical instances of anomaly detection, which means that we can segment polyps
    by using unsupervised anomaly localization techniques without large-scale pixel-wise
    annotations. Recently, some researchers have trained the polyp segmentation model
    with the help of unsupervised anomaly detection, i.e., CCD [[121](#bib.bib121)]
    and PMSACL [[84](#bib.bib84)]. We believe there is still much room for improvement
    in this direction, and the prospects for unsupervised polyp localization are promising.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测[[202](#bib.bib202)]，也称为离群点检测，旨在识别数据集中显著偏离正常或预期行为的模式或实例。目标是识别那些相对于大多数数据被认为是稀有、不寻常或可疑的数据点。在息肉数据集中，大部分图像不包含息肉，只有一小部分图像包含息肉。因此，图像中的息肉可以被视为异常检测的典型实例，这意味着我们可以通过使用无监督的异常定位技术对息肉进行分割，而无需大规模的像素级注释。最近，一些研究者利用无监督异常检测，即
    CCD [[121](#bib.bib121)] 和 PMSACL [[84](#bib.bib84)] 来训练息肉分割模型。我们相信在这方面仍有很大的改进空间，无监督息肉定位的前景非常有希望。
- en: 6.8 Combining CPS with Large Segmentation Models
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8 将 CPS 与大型分割模型相结合
- en: Very recently, the segmentation anything model (SAM) [[203](#bib.bib203)] has
    gained massive attention due to its impressive performance in many segmentation
    tasks, which has been trained on the largest segmentation dataset with more than
    1 billion image-mask pairs, surpassing existing segmentation datasets by a factor
    of 400\. Finetuning the SAM for downstream tasks, such as camouflaged object detection
    [[204](#bib.bib204)], skin cancer segmentation [[205](#bib.bib205)] and image
    style transfer [[206](#bib.bib206)], has become a hot research area. Zhou et al.
    [[207](#bib.bib207)] show that directly applying SAM to the polyp segmentation
    cannot achieve satisfactory performance for these unseen medical images. Thus,
    one promising direction is to finetune the SAM model using training polyp datasets.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，分割万物模型（SAM）[[203](#bib.bib203)]由于在许多分割任务中的出色表现而受到广泛关注，该模型在超过10亿对图像-掩膜的最大分割数据集上进行训练，超越现有分割数据集400倍。对
    SAM 进行微调以适应下游任务，如伪装物体检测 [[204](#bib.bib204)]、皮肤癌分割 [[205](#bib.bib205)] 和图像风格迁移
    [[206](#bib.bib206)] 已成为一个热门研究领域。Zhou 等人 [[207](#bib.bib207)] 表明，直接将 SAM 应用于息肉分割无法获得令人满意的结果。因此，一个有前途的方向是利用训练息肉数据集对
    SAM 模型进行微调。
- en: 6.9 Combining CPS with Large Language Models
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9 将 CPS 与大型语言模型相结合
- en: Combining large language models (LLMs) and computer vision has recently become
    a hot research area, driving significant advancements in the many tasks [[208](#bib.bib208),
    [209](#bib.bib209)]. The large language models, initially created to understand
    human language, are gradually expanding to encompass visual tasks and begin to
    combine text data with visual data. This fusion of LLMs and CV leads to an era
    where AI systems can see the world, understand it, and communicate with it just
    like humans do. In this way, the deep model detects lesions and explains them
    clearly, helping doctors understand and trust the prediction results. On the other
    hand, combining LLMs and polyp segmentation could lead to better diagnosis and
    treatment by joining visual information with a wide range of medical knowledge.
    In summary, this is a promising direction and has the potential to break the current
    learning paradigm.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，将大型语言模型（LLMs）与计算机视觉相结合已成为一个热门研究领域，推动了许多任务的重大进展[[208](#bib.bib208)、[209](#bib.bib209)]。最初创建用于理解人类语言的大型语言模型，正逐渐扩展到视觉任务，并开始将文本数据与视觉数据结合。这种
    LLM 和 CV 的融合引领了一个 AI 系统能够像人类一样观察世界、理解世界并与之沟通的时代。这样，深度模型可以检测病变并清楚地解释它们，帮助医生理解和信任预测结果。另一方面，将
    LLM 和息肉分割相结合可能通过将视觉信息与广泛的医学知识结合，改善诊断和治疗。总之，这是一个有前景的方向，具有打破当前学习范式的潜力。
- en: 7 Conlusion
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This paper presents a comprehensive review of deep learning-based CPS models.
    Firstly, we provide innovative taxonomy for categorizing deep CPS models from
    perspectives of network architecture, levels of supervision, and learning paradigms.
    Subsequently, we delve into contemporary literature on popular CPS datasets and
    evaluation metrics and conduct a thorough performance benchmarking of major CPS
    methods. In particular, we reveal the strengths and weaknesses of CPS datasets
    by comparing the number of datasets available, types of annotations, image resolutions,
    polyp sizes, contrast values, and polyp location. Furthermore, we evaluate the
    model’s generalization performance on out-of-distribution datasets and its attribute-based
    performance on SUN-SEG datasets, providing a nuanced understanding of the strengths
    and weaknesses of deep CPS models. Finally, we look deeper into the challenges
    of current deep learning-based CPS models, providing insightful discussions and
    several potentially promising directions. We hope our survey will help researchers
    gain a deeper understanding of the developmental history of CPS and inspire new
    works to advance this field.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了基于深度学习的CPS模型的全面综述。首先，我们从网络架构、监督层级和学习范式的角度，提供了对深度CPS模型的创新分类。随后，我们深入探讨了流行CPS数据集和评估指标的最新文献，并对主要CPS方法进行了彻底的性能基准测试。特别地，我们通过比较数据集数量、注释类型、图像分辨率、息肉大小、对比度值和息肉位置，揭示了CPS数据集的优缺点。此外，我们评估了模型在分布外数据集上的泛化性能及其在SUN-SEG数据集上的属性性能，提供了对深度CPS模型优缺点的细致理解。最后，我们深入探讨了当前基于深度学习的CPS模型的挑战，提供了有见地的讨论和若干潜在有前景的方向。我们希望我们的综述能帮助研究人员更深入地了解CPS的发展历程，并激发新研究以推动该领域的进步。
- en: References
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Bernal, J. Sánchez, and F. Vilarino, “Towards automatic polyp detection
    with a polyp appearance model,” *Pattern Recognition (PR)*, vol. 45, no. 9, pp.
    3166–3182, 2012.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Bernal, J. Sánchez, 和 F. Vilarino，“基于息肉外观模型的自动息肉检测”，*模式识别（PR）*，第45卷，第9期，第3166–3182页，2012年。'
- en: '[2] G.-P. Ji, G. Xiao, Y.-C. Chou, D.-P. Fan, K. Zhao, G. Chen, and L. Van Gool,
    “Video polyp segmentation: A deep learning perspective,” *Machine Intelligence
    Research*, vol. 19, no. 6, pp. 531–549, 2022.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] G.-P. Ji, G. Xiao, Y.-C. Chou, D.-P. Fan, K. Zhao, G. Chen, 和 L. Van Gool，“视频息肉分割：深度学习视角”，*机器智能研究*，第19卷，第6期，第531–549页，2022年。'
- en: '[3] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2015,
    pp. 234–241.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] O. Ronneberger, P. Fischer, 和 T. Brox，“U-net：用于生物医学图像分割的卷积网络”，发表于*国际医学图像计算与计算机辅助干预会议（MICCAI）*，2015年，第234–241页。'
- en: '[4] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: A
    nested u-net architecture for medical image segmentation,” in *Deep Learning in
    Medical Image Analysis and Multimodal Learning for Clinical Decision Support,Workshop
    (DLMIA ML-CDS)*, 2018, pp. 3–11.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, 和 J. Liang，“Unet++：用于医学图像分割的嵌套U-net架构”，发表于*医学图像分析中的深度学习和临床决策支持的多模态学习，研讨会（DLMIA
    ML-CDS）*，2018年，第3–11页。'
- en: '[5] D. Jha, P. H. Smedsrud, M. A. Riegler, D. Johansen, T. De Lange, P. Halvorsen,
    and H. D. Johansen, “Resunet++: An advanced architecture for medical image segmentation,”
    in *Proceedings of the IEEE International Symposium on Multimedia (ISM)*, 2019,
    pp. 225–2255.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. Jha, P. H. Smedsrud, M. A. Riegler, D. Johansen, T. De Lange, P. Halvorsen,
    和 H. D. Johansen，“Resunet++：用于医学图像分割的先进架构”，发表于*IEEE国际多媒体研讨会（ISM）*，2019年，第225–2255页。'
- en: '[6] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao, “Pranet:
    Parallel reverse attention network for polyp segmentation,” in *Proceedings of
    the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2020, pp. 263–273.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, 和 L. Shao，“Pranet：用于息肉分割的并行逆注意力网络”，发表于*国际医学图像计算与计算机辅助干预会议（MICCAI）*，2020年，第263–273页。'
- en: '[7] Y. Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and cnns for
    medical image segmentation,” in *Proceedings of the International Conference on
    Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2021, pp.
    14–24.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Zhang, H. Liu, 和 Q. Hu，“Transfuse：融合变压器和CNN用于医学图像分割”，发表于*国际医学图像计算与计算机辅助干预会议（MICCAI）*，2021年，第14–24页。'
- en: '[8] J. Wang, Q. Huang, F. Tang, J. Meng, J. Su, and S. Song, “Stepwise feature
    fusion: Local guides global,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2022,
    pp. 110–120.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Wang, Q. Huang, F. Tang, J. Meng, J. Su, 和 S. Song，“逐步特征融合：局部引导全局”，在*国际医学图像计算与计算机辅助干预会议
    (MICCAI)*，2022 年，第 110–120 页。'
- en: '[9] T. Ling, C. Wu, H. Yu, T. Cai, D. Wang, Y. Zhou, M. Chen, and K. Ding,
    “Probabilistic modeling ensemble vision transformer improves complex polyp segmentation,”
    in *Proceedings of the International Conference on Medical Image Computing and
    Computer-Assisted Intervention (MICCAI)*, 2023, pp. 572–581.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] T. Ling, C. Wu, H. Yu, T. Cai, D. Wang, Y. Zhou, M. Chen, 和 K. Ding，“概率建模集成视觉变换器提高复杂息肉分割”，在*国际医学图像计算与计算机辅助干预会议
    (MICCAI)*，2023 年，第 572–581 页。'
- en: '[10] A. K. Jerebko, S. Teerlink, M. Franaszek, and R. M. Summers, “Polyp segmentation
    method for ct colonography computer-aided detection,” in *SPIE Medical imaging:
    Physiology and Function: Methods, Systems, and Applications*, vol. 5031, 2003,
    pp. 359–369.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. K. Jerebko, S. Teerlink, M. Franaszek, 和 R. M. Summers，“CT 结肠成像计算机辅助检测的息肉分割方法”，在*SPIE
    医学成像：生理学和功能：方法、系统和应用*，第 5031 卷，2003 年，第 359–369 页。'
- en: '[11] J. Yao, M. Miller, M. Franaszek, and R. M. Summers, “Colonic polyp segmentation
    in ct colonography-based on fuzzy clustering and deformable models,” *IEEE Transactions
    on Medical Imaging (TMI)*, vol. 23, no. 11, pp. 1344–1352, 2004.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Yao, M. Miller, M. Franaszek, 和 R. M. Summers，“基于模糊聚类和可变形模型的 CT 结肠成像中的结肠息肉分割”，*IEEE
    医学影像学报 (TMI)*，第 23 卷，第 11 期，第 1344–1352 页，2004 年。'
- en: '[12] S. Gross, M. Kennel, T. Stehle, J. Wulff, J. Tischendorf, C. Trautwein,
    and T. Aach, “Polyp segmentation in nbi colonoscopy,” in *Bildverarbeitung für
    die Medizin 2009: Algorithmen—Systeme—Anwendungen*, 2009, pp. 252–256.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. Gross, M. Kennel, T. Stehle, J. Wulff, J. Tischendorf, C. Trautwein,
    和 T. Aach，“在 NBI 结肠镜检查中的息肉分割”，在*医学图像处理 2009：算法—系统—应用*，2009 年，第 252–256 页。'
- en: '[13] S. Hwang and M. E. Celebi, “Polyp detection in wireless capsule endoscopy
    videos based on image segmentation and geometric feature,” in *Proceedings of
    the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2010, pp. 678–681.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Hwang 和 M. E. Celebi，“基于图像分割和几何特征的无线胶囊内窥镜视频中的息肉检测”，在*IEEE 国际声学、语音与信号处理会议
    (ICASSP)*，2010 年，第 678–681 页。'
- en: '[14] L. Lu, A. Barbu, M. Wolf, J. Liang, M. Salganicoff, and D. Comaniciu,
    “Accurate polyp segmentation for 3d ct colongraphy using multi-staged probabilistic
    binary learning and compositional model,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2008, pp. 1–8.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] L. Lu, A. Barbu, M. Wolf, J. Liang, M. Salganicoff, 和 D. Comaniciu，“利用多阶段概率二进制学习和组合模型进行
    3D CT 结肠成像的精确息肉分割”，在*IEEE 计算机视觉与模式识别会议 (CVPR)*，2008 年，第 1–8 页。'
- en: '[15] M. Ganz, X. Yang, and G. Slabaugh, “Automatic segmentation of polyps in
    colonoscopic narrow-band imaging data,” *IEEE Transactions on Biomedical Engineering
    (TBE)*, vol. 59, no. 8, pp. 2144–2151, 2012.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Ganz, X. Yang, 和 G. Slabaugh，“结肠镜窄带成像数据中息肉的自动分割”，*IEEE 生物医学工程学报 (TBE)*，第
    59 卷，第 8 期，第 2144–2151 页，2012 年。'
- en: '[16] J. Bernal, J. M. Núñez, F. J. Sánchez, and F. Vilariño, “Polyp segmentation
    method in colonoscopy videos by means of msa-dova energy maps calculation,” in
    *Clinical Image-Based Procedures, Translational Research in Medical Imaging: Third
    International Workshop (CLIP)*, 2014, pp. 41–49.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Bernal, J. M. Núñez, F. J. Sánchez, 和 F. Vilariño，“通过 MSA-DOVA 能量图计算进行结肠镜视频中的息肉分割方法”，在*临床图像基础程序，医学成像转化研究：第三届国际研讨会
    (CLIP)*，2014 年，第 41–49 页。'
- en: '[17] Y. Yuan, D. Li, and M. Q.-H. Meng, “Automatic polyp detection via a novel
    unified bottom-up and top-down saliency approach,” *IEEE Journal of Biomedical
    and Health Informatics (JBHI)*, vol. 22, no. 4, pp. 1250–1260, 2017.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Yuan, D. Li, 和 M. Q.-H. Meng，“通过新型统一的自下而上和自上而下的显著性方法进行自动息肉检测”，*IEEE
    生物医学与健康信息学杂志 (JBHI)*，第 22 卷，第 4 期，第 1250–1260 页，2017 年。'
- en: '[18] Y. Fang, C. Chen, Y. Yuan, and K.-y. Tong, “Selective feature aggregation
    network with area-boundary constraints for polyp segmentation,” in *Proceedings
    of the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2019, pp. 302–310.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. Fang, C. Chen, Y. Yuan, 和 K.-y. Tong，“具有区域-边界约束的选择性特征聚合网络用于息肉分割”，在*国际医学图像计算与计算机辅助干预会议
    (MICCAI)*，2019 年，第 302–310 页。'
- en: '[19] V. S. Prasath, “Polyp detection and segmentation from video capsule endoscopy:
    A review,” *Journal of Imaging*, vol. 3, no. 1, p. 1, 2016.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] V. S. Prasath，“从视频胶囊内窥镜中检测和分割息肉：综述”，*成像杂志*，第 3 卷，第 1 期，第 1 页，2016 年。'
- en: '[20] B. Taha, N. Werghi, and J. Dias, “Automatic polyp detection in endoscopy
    videos: A survey,” in *Proceedings of the International Conference on Biomedical
    Engineering (BioMed)*, 2017, pp. 233–240.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] B. Taha, N. Werghi, 和 J. Dias, “内镜视频中的自动息肉检测：综述，” 收录于 *国际生物医学工程会议 (BioMed)*
    论文集, 2017年, 页码233–240。'
- en: '[21] L. F. Sanchez-Peralta, L. Bote-Curiel, A. Picon, F. M. Sanchez-Margallo,
    and J. B. Pagador, “Deep learning to find colorectal polyps in colonoscopy: A
    systematic literature review,” *Artificial Intelligence in Medicine*, vol. 108,
    p. 101923, 2020.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] L. F. Sanchez-Peralta, L. Bote-Curiel, A. Picon, F. M. Sanchez-Margallo,
    和 J. B. Pagador, “使用深度学习检测结肠镜中的结直肠息肉：系统文献综述，” *医学中的人工智能*, 第108卷, 页码101923, 2020年。'
- en: '[22] K. ELKarazle, V. Raman, P. Then, and C. Chua, “Detection of colorectal
    polyps from colonoscopy using machine learning: A survey on modern techniques,”
    *Sensors*, vol. 23, no. 3, p. 1225, 2023.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] K. ELKarazle, V. Raman, P. Then, 和 C. Chua, “利用机器学习从结肠镜检查中检测结直肠息肉：现代技术综述，”
    *传感器*, 第23卷，第3期, 页码1225, 2023年。'
- en: '[23] J. Mei, T. Zhou, K. Huang, Y. Zhang, Y. Zhou, Y. Wu, and H. Fu, “A survey
    on deep learning for polyp segmentation: Techniques, challenges and future trends,”
    *arXiv preprint arXiv:2304.07583*, 2023.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Mei, T. Zhou, K. Huang, Y. Zhang, Y. Zhou, Y. Wu, 和 H. Fu, “基于深度学习的息肉分割综述：技术、挑战及未来趋势，”
    *arXiv预印本 arXiv:2304.07583*, 2023年。'
- en: '[24] S. Ali, D. Jha, N. Ghatwary, S. Realdon, R. Cannizzaro, O. E. Salem, D. Lamarque,
    C. Daul, M. A. Riegler, K. V. Anonsen *et al.*, “A multi-centre polyp detection
    and segmentation dataset for generalisability assessment,” *Scientific Data*,
    vol. 10, no. 1, p. 75, 2023.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Ali, D. Jha, N. Ghatwary, S. Realdon, R. Cannizzaro, O. E. Salem, D. Lamarque,
    C. Daul, M. A. Riegler, K. V. Anonsen *等*, “一个多中心的息肉检测与分割数据集，用于普适性评估，” *科学数据*,
    第10卷，第1期, 页码75, 2023年。'
- en: '[25] J.-H. Shi, Q. Zhang, Y.-H. Tang, and Z.-Q. Zhang, “Polyp-mixer: An efficient
    context-aware mlp-based paradigm for polyp segmentation,” *IEEE Transactions on
    Circuits and Systems for Video Technology (TCSVT)*, vol. 33, no. 1, pp. 30–42,
    2022.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J.-H. Shi, Q. Zhang, Y.-H. Tang, 和 Z.-Q. Zhang, “Polyp-mixer：一种高效的上下文感知的基于MLP的息肉分割范式，”
    *IEEE视频技术电路与系统汇刊 (TCSVT)*, 第33卷，第1期, 页码30–42, 2022年。'
- en: '[26] S. Chen, E. Xie, C. GE, and P. Luo, “Cyclemlp: A mlp-like architecture
    for dense prediction,” in *Proceedings of the International Conference on Learning
    Representation (ICLR)*, 2022.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Chen, E. Xie, C. GE, 和 P. Luo, “Cyclemlp：一种类似MLP的密集预测架构，” 收录于 *国际学习表征会议
    (ICLR)* 论文集, 2022年。'
- en: '[27] Y. Shen, Y. Lu, X. Jia, F. Bai, and M. Q.-H. Meng, “Task-relevant feature
    replenishment for cross-centre polyp segmentation,” in *Proceedings of the International
    Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*,
    2022, pp. 599–608.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Shen, Y. Lu, X. Jia, F. Bai, 和 M. Q.-H. Meng, “跨中心息肉分割的任务相关特征补充，” 收录于
    *国际医学图像计算与计算机辅助干预会议 (MICCAI)* 论文集, 2022年, 页码599–608。'
- en: '[28] N. K. Tomar, D. Jha, U. Bagci, and S. Ali, “Tganet: Text-guided attention
    for improved polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2022,
    pp. 151–160.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] N. K. Tomar, D. Jha, U. Bagci, 和 S. Ali, “Tganet：改进息肉分割的文本引导注意机制，” 收录于
    *国际医学图像计算与计算机辅助干预会议 (MICCAI)* 论文集, 2022年, 页码151–160。'
- en: '[29] J. Wei, Y. Hu, G. Li, S. Cui, S. Kevin Zhou, and Z. Li, “Boxpolyp: Boost
    generalized polyp segmentation using extra coarse bounding box annotations,” in
    *Proceedings of the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2022, pp. 67–77.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Wei, Y. Hu, G. Li, S. Cui, S. Kevin Zhou, 和 Z. Li, “Boxpolyp：通过额外的粗略边界框注释提升通用息肉分割能力，”
    收录于 *国际医学图像计算与计算机辅助干预会议 (MICCAI)* 论文集, 2022年, 页码67–77。'
- en: '[30] X. Guo, Z. Chen, J. Liu, and Y. Yuan, “Non-equivalent images and pixels:
    Confidence-aware resampling with meta-learning mixup for polyp segmentation,”
    *Medical Image Analysis (MedIA)*, vol. 78, p. 102394, 2022.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] X. Guo, Z. Chen, J. Liu, 和 Y. Yuan, “非等效图像和像素：基于自信度感知的重采样与元学习混合用于息肉分割，”
    *医学图像分析 (MedIA)*, 第78卷, 页码102394, 2022年。'
- en: '[31] C. Yang, X. Guo, Z. Chen, and Y. Yuan, “Source free domain adaptation
    for medical image segmentation with fourier style mining,” *Medical Image Analysis
    (MedIA)*, vol. 79, p. 102457, 2022.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] C. Yang, X. Guo, Z. Chen, 和 Y. Yuan, “用于医学图像分割的源无关领域适应与傅里叶风格挖掘，” *医学图像分析
    (MedIA)*, 第79卷, 页码102457, 2022年。'
- en: '[32] G. Yue, W. Han, B. Jiang, T. Zhou, R. Cong, and T. Wang, “Boundary constraint
    network with cross layer feature integration for polyp segmentation,” *IEEE Journal
    of Biomedical and Health Informatics (JBHI)*, vol. 26, no. 8, pp. 4090–4099, 2022.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] G. Yue, W. Han, B. Jiang, T. Zhou, R. Cong, 和 T. Wang， “边界约束网络与跨层特征集成用于息肉分割，”
    *IEEE生物医学与健康信息学杂志（JBHI）*，第26卷，第8期，第4090–4099页，2022年。'
- en: '[33] X. Zhao, L. Zhang, and H. Lu, “Automatic polyp segmentation via multi-scale
    subtraction network,” in *Proceedings of the International Conference on Medical
    Image Computing and Computer Assisted Intervention (MICCAI)*, 2021, pp. 120–130.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] X. Zhao, L. Zhang, 和 H. Lu， “通过多尺度减法网络进行自动息肉分割，” 见于 *国际医学图像计算与计算机辅助干预会议（MICCAI）*，2021年，第120–130页。'
- en: '[34] J. Wei, Y. Hu, R. Zhang, Z. Li, S. K. Zhou, and S. Cui, “Shallow attention
    network for polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI)*, 2021,
    pp. 699–708.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. Wei, Y. Hu, R. Zhang, Z. Li, S. K. Zhou, 和 S. Cui， “用于息肉分割的浅层注意力网络，”
    见于 *国际医学图像计算与计算机辅助干预会议（MICCAI）*，2021年，第699–708页。'
- en: '[35] N. K. Tomar, D. Jha, M. A. Riegler, H. D. Johansen, D. Johansen, J. Rittscher,
    P. Halvorsen, and S. Ali, “Fanet: A feedback attention network for improved biomedical
    image segmentation,” *IEEE Transactions on Neural Networks and Learning Systems
    (TNNLS)*, vol. 34, no. 11, pp. 9375–9388, 2022.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] N. K. Tomar, D. Jha, M. A. Riegler, H. D. Johansen, D. Johansen, J. Rittscher,
    P. Halvorsen, 和 S. Ali， “Fanet：一种用于改进生物医学图像分割的反馈注意力网络，” *IEEE神经网络与学习系统汇刊（TNNLS）*，第34卷，第11期，第9375–9388页，2022年。'
- en: '[36] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2015, pp. 3431–3440.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Long, E. Shelhamer, 和 T. Darrell， “用于语义分割的全卷积网络，” 见于 *IEEE计算机视觉与模式识别会议（CVPR）*，2015年，第3431–3440页。'
- en: '[37] Q. Nguyen and S.-W. Lee, “Colorectal segmentation using multiple encoder-decoder
    network in colonoscopy images,” in *Proceedings of the IEEE International Conference
    on Artificial Intelligence and Knowledge Engineering (AIKE)*, 2018, pp. 208–211.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Q. Nguyen 和 S.-W. Lee， “在结肠镜图像中使用多重编码解码网络进行结直肠分割，” 见于 *IEEE国际人工智能与知识工程会议（AIKE）*，2018年，第208–211页。'
- en: '[38] L. Zhang, S. Dolwani, and X. Ye, “Automated polyp segmentation in colonoscopy
    frames using fully convolutional neural network and textons,” in *Proceedings
    of the Medical Image Understanding and Analysis (MIUA)*, 2017, pp. 707–717.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] L. Zhang, S. Dolwani, 和 X. Ye， “使用全卷积神经网络和纹理进行结肠镜帧中的自动息肉分割，” 见于 *医学图像理解与分析（MIUA）*，2017年，第707–717页。'
- en: '[39] Q. Li, G. Yang, Z. Chen, B. Huang, L. Chen, D. Xu, X. Zhou, S. Zhong,
    H. Zhang, and T. Wang, “Colorectal polyp segmentation using a fully convolutional
    neural network,” in *Proceedings of the IEEE International Congress on Image and
    Signal Processing, Biomedical Engineering and Informatics (CISP-BMEI)*, 2017,
    pp. 1–5.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Q. Li, G. Yang, Z. Chen, B. Huang, L. Chen, D. Xu, X. Zhou, S. Zhong,
    H. Zhang, 和 T. Wang， “利用全卷积神经网络进行结直肠息肉分割，” 见于 *IEEE国际图像和信号处理、生物医学工程与信息学会议（CISP-BMEI）*，2017年，第1–5页。'
- en: '[40] P. Brandao, E. Mazomenos, G. Ciuti, R. Caliò, F. Bianchi, A. Menciassi,
    P. Dario, A. Koulaouzidis, A. Arezzo, and D. Stoyanov, “Fully convolutional neural
    networks for polyp segmentation in colonoscopy,” in *SPIE Medical Imaging: Computer-Aided
    Diagnosis*, vol. 10134, 2017, pp. 101–107.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] P. Brandao, E. Mazomenos, G. Ciuti, R. Caliò, F. Bianchi, A. Menciassi,
    P. Dario, A. Koulaouzidis, A. Arezzo, 和 D. Stoyanov， “用于结肠镜检查中息肉分割的全卷积神经网络，” 见于
    *SPIE医学成像：计算机辅助诊断*，第10134卷，2017年，第101–107页。'
- en: '[41] Y. Su, Q. Xie, J. Ye, J. He, and J. Cheng, “An accurate polyp segmentation
    framework via feature secondary fusion,” in *Proceedings of the International
    Symposium on Biomedical Imaging (ISBI)*, 2023, pp. 1–5.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Y. Su, Q. Xie, J. Ye, J. He, 和 J. Cheng， “通过特征二次融合的精确息肉分割框架，” 见于 *国际生物医学成像研讨会（ISBI）*，2023年，第1–5页。'
- en: '[42] Z. Yin, K. Liang, Z. Ma, and J. Guo, “Duplex contextual relation network
    for polyp segmentation,” in *Proceedings of the IEEE International Symposium on
    Biomedical Imaging (ISBI)*, 2022, pp. 1–5.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Z. Yin, K. Liang, Z. Ma, 和 J. Guo， “用于息肉分割的双重上下文关系网络，” 见于 *IEEE国际生物医学成像研讨会（ISBI）*，2022年，第1–5页。'
- en: '[43] Z. Xu, D. Qiu, S. Lin, X. Zhang, S. Shi, S. Zhu, F. Zhang, and X. Wan,
    “Temporal correlation network for video polyp segmentation,” in *Proceedings of
    the IEEE International Conference on Bioinformatics and Biomedicine (BIBM)*, 2022,
    pp. 1317–1322.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Z. Xu, D. Qiu, S. Lin, X. Zhang, S. Shi, S. Zhu, F. Zhang, 和 X. Wan, “用于视频息肉分割的时间相关网络，”
    收录于 *IEEE国际生物信息学与生物医学会议（BIBM）论文集*，2022年，第1317–1322页。'
- en: '[44] R. Feng, B. Lei, W. Wang, T. Chen, J. Chen, D. Z. Chen, and J. Wu, “Ssn:
    A stair-shape network for real-time polyp segmentation in colonoscopy images,”
    in *Proceedings of the IEEE 17th International Symposium on Biomedical Imaging
    (ISBI)*, 2020, pp. 225–229.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Feng, B. Lei, W. Wang, T. Chen, J. Chen, D. Z. Chen, 和 J. Wu, “Ssn:
    一种用于实时息肉分割的阶梯形网络，” 收录于 *IEEE第17届国际生物医学影像研讨会（ISBI）论文集*，2020年，第225–229页。'
- en: '[45] M. Akbari, M. Mohrekesh, E. Nasr-Esfahani, S. R. Soroushmehr, N. Karimi,
    S. Samavi, and K. Najarian, “Polyp segmentation in colonoscopy images using fully
    convolutional network,” in *Proceedings of the IEEE Engineering in Medicine and
    Biology Society (EMBC)*, 2018, pp. 69–72.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M. Akbari, M. Mohrekesh, E. Nasr-Esfahani, S. R. Soroushmehr, N. Karimi,
    S. Samavi, 和 K. Najarian, “使用全卷积网络进行结肠镜图像中的息肉分割，” 收录于 *IEEE医学与生物学工程学会会议（EMBC）论文集*，2018年，第69–72页。'
- en: '[46] I. Wichakam, T. Panboonyuen, C. Udomcharoenchaikit, and P. Vateekul, “Real-time
    polyps segmentation for colonoscopy video frames using compressed fully convolutional
    network,” in *Proceedings of the International Conference on Multimedia Modeling
    (MMM)*, 2018, pp. 393–404.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] I. Wichakam, T. Panboonyuen, C. Udomcharoenchaikit, 和 P. Vateekul, “利用压缩全卷积网络进行实时息肉分割，”
    收录于 *国际多媒体建模会议（MMM）论文集*，2018年，第393–404页。'
- en: '[47] H. Wu, J. Zhong, W. Wang, Z. Wen, and J. Qin, “Precise yet efficient semantic
    calibration and refinement in convnets for real-time polyp segmentation from colonoscopy
    videos,” in *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*,
    vol. 35, no. 4, 2021, pp. 2916–2924.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] H. Wu, J. Zhong, W. Wang, Z. Wen, 和 J. Qin, “精确而高效的语义校准与修正用于实时息肉分割的卷积网络，”
    收录于 *AAAI人工智能会议（AAAI）论文集*，第35卷，第4期，2021年，第2916–2924页。'
- en: '[48] C. Dong, Q. Zhao, K. Chen, and X. Huang, “Asymmetric attention upsampling:
    Rethinking upsampling for biological image segmentation,” in *Proceedings of the
    IEEE 18th International Symposium on Biomedical Imaging (ISBI)*, 2021, pp. 645–649.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] C. Dong, Q. Zhao, K. Chen, 和 X. Huang, “非对称注意上采样：重新思考生物图像分割的上采样方法，” 收录于
    *IEEE第18届国际生物医学影像研讨会（ISBI）论文集*，2021年，第645–649页。'
- en: '[49] R. Zhang, P. Lai, X. Wan, D.-J. Fan, F. Gao, X.-J. Wu, and G. Li, “Lesion-aware
    dynamic kernel for polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2022,
    pp. 99–109.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] R. Zhang, P. Lai, X. Wan, D.-J. Fan, F. Gao, X.-J. Wu, 和 G. Li, “用于息肉分割的病变感知动态核，”
    收录于 *国际医学图像计算与计算机辅助干预会议（MICCAI）论文集*，2022年，第99–109页。'
- en: '[50] X. Zhao, Z. Wu, S. Tan, D.-J. Fan, Z. Li, X. Wan, and G. Li, “Semi-supervised
    spatial temporal attention network for video polyp segmentation,” in *Proceedings
    of the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2022, pp. 456–466.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] X. Zhao, Z. Wu, S. Tan, D.-J. Fan, Z. Li, X. Wan, 和 G. Li, “用于视频息肉分割的半监督时空注意网络，”
    收录于 *国际医学图像计算与计算机辅助干预会议（MICCAI）论文集*，2022年，第456–466页。'
- en: '[51] A. Srivastava, D. Jha, S. Chanda, U. Pal, H. D. Johansen, D. Johansen,
    M. A. Riegler, S. Ali, and P. Halvorsen, “Msrf-net: a multi-scale residual fusion
    network for biomedical image segmentation,” *IEEE Journal of Biomedical and Health
    Informatics (JBHI)*, vol. 26, no. 5, pp. 2252–2263, 2021.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A. Srivastava, D. Jha, S. Chanda, U. Pal, H. D. Johansen, D. Johansen,
    M. A. Riegler, S. Ali, 和 P. Halvorsen, “Msrf-net: 一种用于生物医学图像分割的多尺度残差融合网络，” *IEEE生物医学与健康信息学期刊（JBHI）*，第26卷，第5期，第2252–2263页，2021年。'
- en: '[52] Y. Lin, J. Wu, G. Xiao, J. Guo, G. Chen, and J. Ma, “Bsca-net: Bit slicing
    context attention network for polyp segmentation,” *Pattern Recognition (PR)*,
    vol. 132, p. 108917, 2022.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Lin, J. Wu, G. Xiao, J. Guo, G. Chen, 和 J. Ma, “Bsca-net: 位切片上下文注意网络用于息肉分割，”
    *模式识别（PR）*，第132卷，第108917页，2022年。'
- en: '[53] P. Song, J. Li, and H. Fan, “Attention based multi-scale parallel network
    for polyp segmentation,” *Computers in Biology and Medicine (CBM)*, vol. 146,
    p. 105476, 2022.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] P. Song, J. Li, 和 H. Fan, “基于注意力的多尺度并行网络用于息肉分割，” *生物医学与医学计算机（CBM）*，第146卷，第105476页，2022年。'
- en: '[54] X. Li, J. Xu, Y. Zhang, R. Feng, R.-W. Zhao, T. Zhang, X. Lu, and S. Gao,
    “Tccnet: Temporally consistent context-free network for semi-supervised video
    polyp segmentation,” in *Proceedings of the International Joint Conference on
    Artificial Intelligence (IJCAI)*, 2022, pp. 1109–1115.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] X. Li, J. Xu, Y. Zhang, R. Feng, R.-W. Zhao, T. Zhang, X. Lu, 和 S. Gao,
    “Tccnet：时间一致的无上下文网络用于半监督视频息肉分割”，发表于 *国际人工智能联合会议（IJCAI）*，2022年，第1109–1115页。'
- en: '[55] T. D. Huy, H. C. Huyen, C. D. Nguyen, S. T. Duong, T. Bui, and S. Q. Truong,
    “Adversarial contrastive fourier domain adaptation for polyp segmentation,” in
    *Proceedings of the IEEE International Symposium on Biomedical Imaging (ISBI)*,
    2022, pp. 1–5.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] T. D. Huy, H. C. Huyen, C. D. Nguyen, S. T. Duong, T. Bui, 和 S. Q. Truong,
    “对抗性对比傅里叶域适应在息肉分割中的应用”，发表于 *IEEE 国际生物医学成像研讨会（ISBI）*，2022年，第1–5页。'
- en: '[56] Z. Qiu, Z. Wang, M. Zhang, Z. Xu, J. Fan, and L. Xu, “Bdg-net: boundary
    distribution guided network for accurate polyp segmentation,” in *SPIE Medical
    Imaging: Image Processing*, vol. 12032, 2022, pp. 792–799.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Z. Qiu, Z. Wang, M. Zhang, Z. Xu, J. Fan, 和 L. Xu, “Bdg-net：边界分布引导网络用于精确息肉分割”，发表于
    *SPIE 医学成像：图像处理*，第12032卷，2022年，第792–799页。'
- en: '[57] M. Chen, X. Li, J. Xu, R. Yuan, Y. Zhang, R. Feng, T. Zhang, and S. Gao,
    “Single-modality endoscopic polyp segmentation via random color reversal synthesis
    and two-branched learning,” in *Proceedings of the IEEE International Conference
    on Bioinformatics and Biomedicine (BIBM)*, 2022, pp. 1501–1504.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] M. Chen, X. Li, J. Xu, R. Yuan, Y. Zhang, R. Feng, T. Zhang, 和 S. Gao,
    “单模态内窥镜息肉分割通过随机颜色反转合成和双分支学习”，发表于 *IEEE 国际生物信息学与生物医学会议（BIBM）*，2022年，第1501–1504页。'
- en: '[58] N. K. Tomar, D. Jha, S. Ali, H. D. Johansen, D. Johansen, M. A. Riegler,
    and P. Halvorsen, “Ddanet: Dual decoder attention network for automatic polyp
    segmentation,” in *Proceedings of the International Conference on Pattern Recognition
    Workshops (ICPRW)*, 2021, pp. 307–314.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] N. K. Tomar, D. Jha, S. Ali, H. D. Johansen, D. Johansen, M. A. Riegler,
    和 P. Halvorsen, “Ddanet：双解码器注意力网络用于自动息肉分割”，发表于 *国际模式识别大会工作坊（ICPRW）*，2021年，第307–314页。'
- en: '[59] C. Wu, C. Long, S. Li, J. Yang, F. Jiang, and R. Zhou, “Msraformer: Multiscale
    spatial reverse attention network for polyp segmentation,” *Computers in Biology
    and Medicine (CBM)*, vol. 151, p. 106274, 2022.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] C. Wu, C. Long, S. Li, J. Yang, F. Jiang, 和 R. Zhou, “Msraformer：多尺度空间反向注意力网络用于息肉分割”，*计算机生物医学（CBM）*，第151卷，第106274页，2022年。'
- en: '[60] Q. Jin, H. Hou, G. Zhang, and Z. Li, “Fegnet: A feedback enhancement gate
    network for automatic polyp segmentation,” *IEEE Journal of Biomedical and Health
    Informatics (JBHI)*, vol. 27, no. 7, pp. 3420–3430, 2023.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Q. Jin, H. Hou, G. Zhang, 和 Z. Li, “Fegnet：一种反馈增强门控网络用于自动息肉分割”，*IEEE 生物医学与健康信息学期刊（JBHI）*，第27卷，第7期，第3420–3430页，2023年。'
- en: '[61] Y. Huang, D. Tan, Y. Zhang, X. Li, and K. Hu, “Transmixer: A hybrid transformer
    and cnn architecture for polyp segmentation,” in *Proceedings of the IEEE International
    Conference on Bioinformatics and Biomedicine (BIBM)*, 2022, pp. 1558–1561.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Huang, D. Tan, Y. Zhang, X. Li, 和 K. Hu, “Transmixer：一种混合的变压器和卷积神经网络架构用于息肉分割”，发表于
    *IEEE 国际生物信息学与生物医学会议（BIBM）*，2022年，第1558–1561页。'
- en: '[62] K. B. Patel, F. Li, and G. Wang, “Fuzzynet: A fuzzy attention module for
    polyp segmentation,” in *Proceedings of the Conference on Neural Information Processing
    Systems Workshop (NeurIPS)*, 2022.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] K. B. Patel, F. Li, 和 G. Wang, “Fuzzynet：用于息肉分割的模糊注意力模块”，发表于 *神经信息处理系统研讨会（NeurIPS）*，2022年。'
- en: '[63] X. Du, X. Xu, and K. Ma, “Icgnet: Integration context-based reverse-contour
    guidance network for polyp segmentation,” in *Proceedings of the International
    Joint Conferences on Artificial Intelligence (IJCAI)*, 2022, pp. 877–883.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] X. Du, X. Xu, 和 K. Ma, “Icgnet：基于集成上下文的反向轮廓指导网络用于息肉分割”，发表于 *国际人工智能联合会议（IJCAI）*，2022年，第877–883页。'
- en: '[64] T.-C. Nguyen, T.-P. Nguyen, G.-H. Diep, A.-H. Tran-Dinh, T. V. Nguyen,
    and M.-T. Tran, “Ccbanet: cascading context and balancing attention for polyp
    segmentation,” in *Proceedings of the International Conference on Medical Image
    Computing and Computer Assisted Intervention (MICCAI)*, 2021, pp. 633–643.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] T.-C. Nguyen, T.-P. Nguyen, G.-H. Diep, A.-H. Tran-Dinh, T. V. Nguyen,
    和 M.-T. Tran, “Ccbanet：级联上下文和均衡注意力用于息肉分割”，发表于 *国际医学图像计算与计算机辅助干预会议（MICCAI）*，2021年，第633–643页。'
- en: '[65] Y. Shen, X. Jia, and M. Q.-H. Meng, “Hrenet: A hard region enhancement
    network for polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer Assisted Intervention (MICCAI)*, 2021,
    pp. 559–568.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Y. Shen, X. Jia, 和 M. Q.-H. Meng, “Hrenet: 一种用于息肉分割的硬区域增强网络，” 见于 *国际医学图像计算与计算机辅助干预会议（MICCAI）*，2021，第559–568页。'
- en: '[66] T. Kim, H. Lee, and D. Kim, “Uacanet: Uncertainty augmented context attention
    for polyp segmentation,” in *Proceedings of the 29th ACM International Conference
    on Multimedia (ACM MM)*, 2021, pp. 2167–2175.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] T. Kim, H. Lee, 和 D. Kim, “Uacanet: 用于息肉分割的上下文注意力的不确定性增强，” 见于 *第29届ACM国际多媒体会议（ACM
    MM）*，2021，第2167–2175页。'
- en: '[67] R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, and Y. Yu, “Adaptive context
    selection for polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2020,
    pp. 253–262.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, 和 Y. Yu, “用于息肉分割的自适应上下文选择，” 见于
    *国际医学图像计算与计算机辅助干预会议（MICCAI）*，2020，第253–262页。'
- en: '[68] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and
    L. Shao, “Pvt v2: Improved baselines with pyramid vision transformer,” *Computational
    Visual Media (CVM)*, vol. 8, no. 3, pp. 415–424, 2022.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, 和
    L. Shao, “Pvt v2: 基于金字塔视觉变换器的改进基线，” *计算视觉媒体（CVM）*，第8卷，第3期，第415–424页，2022。'
- en: '[69] J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, D. Gil, C. Rodríguez,
    and F. Vilariño, “Wm-dova maps for accurate polyp highlighting in colonoscopy:
    Validation vs. saliency maps from physicians,” *Computerized Medical Imaging and
    Graphics*, vol. 43, pp. 99–111, 2015.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, D. Gil, C. Rodríguez,
    和 F. Vilariño, “用于准确标记结肠镜检查中息肉的Wm-dova图: 与医生的显著性图验证，” *计算医学成像与图形*，第43卷，第99–111页，2015。'
- en: '[70] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange, D. Johansen,
    and H. D. Johansen, “Kvasir-seg: A segmented polyp dataset,” in *Proceedings of
    the International Conference on Multimedia Modeling (MMM)*, 2020, pp. 451–462.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange, D. Johansen,
    和 H. D. Johansen, “Kvasir-seg: 一个分割的息肉数据集，” 见于 *国际多媒体建模会议（MMM）*，2020，第451–462页。'
- en: '[71] Y. Su, Y. Shen, J. Ye, J. He, and J. Cheng, “Revisiting feature propagation
    and aggregation in polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2023,
    pp. 632–641.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Y. Su, Y. Shen, J. Ye, J. He, 和 J. Cheng, “重新审视息肉分割中的特征传播与聚合，” 见于 *国际医学图像计算与计算机辅助干预会议（MICCAI）*，2023，第632–641页。'
- en: '[72] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and
    L. Shao, “Pyramid vision transformer: A versatile backbone for dense prediction
    without convolutions,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision (ICCV)*, 2021, pp. 568–578.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, 和
    L. Shao, “金字塔视觉变换器: 一种无需卷积的多功能骨干网络用于密集预测，” 见于 *IEEE/CVF国际计算机视觉大会（ICCV）*，2021，第568–578页。'
- en: '[73] A. Wang, M. Xu, Y. Zhang, M. Islam, and H. Ren, “S2me: Spatial-spectral
    mutual teaching and ensemble learning for scribble-supervised polyp segmentation,”
    2023.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] A. Wang, M. Xu, Y. Zhang, M. Islam, 和 H. Ren, “S2me: 空间-光谱互教与集成学习用于涂鸦监督的息肉分割，”
    2023。'
- en: '[74] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, 2016, pp. 770–778.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] K. He, X. Zhang, S. Ren, 和 J. Sun, “用于图像识别的深度残差学习，” 见于 *IEEE计算机视觉与模式识别会议（CVPR）*，2016，第770–778页。'
- en: '[75] J. Wei, Y. Hu, S. Cui, S. K. Zhou, and Z. Li, “Weakpolyp: You only look
    bounding box for polyp segmentation,” in *Proceedings of the International Conference
    on Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 2023,
    pp. 757–766.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] J. Wei, Y. Hu, S. Cui, S. K. Zhou, 和 Z. Li, “Weakpolyp: 仅通过边界框进行息肉分割，”
    见于 *国际医学图像计算与计算机辅助干预会议（MICCAI）*，2023，第757–766页。'
- en: '[76] S.-H. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and P. Torr,
    “Res2net: A new multi-scale backbone architecture,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence (TPAMI)*, vol. 43, no. 2, pp. 652–662, 2019.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S.-H. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, 和 P. Torr, “Res2net:
    一种新的多尺度骨干架构，” *IEEE模式分析与机器智能汇刊（TPAMI）*，第43卷，第2期，第652–662页，2019。'
- en: '[77] M.-H. Guo, C.-Z. Lu, Z.-N. Liu, M.-M. Cheng, and S.-M. Hu, “Visual attention
    network,” *Computational Visual Media (CVM)*, vol. 9, no. 4, pp. 733–752, 2023.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] M.-H. Guo, C.-Z. Lu, Z.-N. Liu, M.-M. Cheng, 和 S.-M. Hu, “视觉注意力网络，” *计算视觉媒体
    (CVM)*, vol. 9, no. 4, pp. 733–752, 2023.'
- en: '[78] M. Haithami, A. Ahmed, I. Y. Liao, and H. Jalab, “Enhancing polyp segmentation
    generalizability by minimizing images’ total variation,” in *Proceedings of the
    International Symposium on Biomedical Imaging (ISBI)*, 2023, pp. 1–5.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Haithami, A. Ahmed, I. Y. Liao, 和 H. Jalab, “通过最小化图像总变差来提升息肉分割的泛化能力，”
    收录于 *国际生物医学成像研讨会 (ISBI)*, 2023, pp. 1–5.'
- en: '[79] Y. Su, C. Deng, Z. Deng, J. Ye, J. He, and J. Cheng, “Go to the right:
    A real-time and accurate polyp segmentation model for practical use,” in *Proceedings
    of the International Symposium on Biomedical Imaging (ISBI)*, 2023, pp. 1–5.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Y. Su, C. Deng, Z. Deng, J. Ye, J. He, 和 J. Cheng, “向右走：一个实时且准确的息肉分割模型用于实际应用，”
    收录于 *国际生物医学成像研讨会 (ISBI)*, 2023, pp. 1–5.'
- en: '[80] X. Xiong, S. Li, and G. Li, “Unpaired image-to-image translation based
    domain adaptation for polyp segmentation,” in *Proceedings of the International
    Symposium on Biomedical Imaging (ISBI)*, 2023, pp. 1–5.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] X. Xiong, S. Li, 和 G. Li, “基于未配对图像到图像转换的领域适应用于息肉分割，” 收录于 *国际生物医学成像研讨会
    (ISBI)*, 2023, pp. 1–5.'
- en: '[81] L. F. Snchez-Peralta, J. B. Pagador, A. Picón, F. Caldern, N. Andraka,
    R. Bilbao, B. Glover, C. L. Saratxaga, and F. M. Snchez-Margallo, “Piccolo white-light
    and narrow-band imaging colonoscopic dataset: A performance comparative of models
    and datasets,” *Applied Sciences*, vol. 10, no. 23, 2020.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] L. F. Snchez-Peralta, J. B. Pagador, A. Picón, F. Caldern, N. Andraka,
    R. Bilbao, B. Glover, C. L. Saratxaga, 和 F. M. Snchez-Margallo, “Piccolo 白光和窄带成像结肠镜数据集：模型和数据集的性能比较，”
    *应用科学*, vol. 10, no. 23, 2020.'
- en: '[82] M. Wang, X. An, Z. Pei, N. Li, L. Zhang, G. Liu, and D. Ming, “An efficient
    multi-task synergetic network for polyp segmentation and classification,” *IEEE
    Journal of Biomedical and Health Informatics (JBHI)*, 2023.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] M. Wang, X. An, Z. Pei, N. Li, L. Zhang, G. Liu, 和 D. Ming, “一种高效的多任务协同网络用于息肉分割和分类，”
    *IEEE 生物医学与健康信息学期刊 (JBHI)*, 2023.'
- en: '[83] H. Mazumdar, C. Chakraborty, M. Sathvik, P. Jayakumar, and A. Kaushik,
    “Optimizing pix2pix gan with attention mechanisms for ai-driven polyp segmentation
    in iomt-enabled smart healthcare,” *IEEE Journal of Biomedical and Health Informatics
    (JBHI)*, pp. 1–8, 2023.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] H. Mazumdar, C. Chakraborty, M. Sathvik, P. Jayakumar, 和 A. Kaushik, “通过注意机制优化
    pix2pix gan 用于 AI 驱动的息肉分割在 IOMT 支持的智能医疗中，” *IEEE 生物医学与健康信息学期刊 (JBHI)*, pp. 1–8,
    2023.'
- en: '[84] Y. Tian, F. Liu, G. Pang, Y. Chen, Y. Liu, J. W. Verjans, R. Singh, and
    G. Carneiro, “Self-supervised pseudo multi-class pre-training for unsupervised
    anomaly detection and segmentation in medical images,” *Medical Image Analysis
    (MedIA)*, vol. 90, p. 102930, 2023.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. Tian, F. Liu, G. Pang, Y. Chen, Y. Liu, J. W. Verjans, R. Singh, 和
    G. Carneiro, “自监督伪多类预训练用于医学图像中的无监督异常检测和分割，” *医学图像分析 (MedIA)*, vol. 90, p. 102930,
    2023.'
- en: '[85] H. Borgli, V. Thambawita, P. H. Smedsrud, S. Hicks, D. Jha, S. L. Eskeland,
    K. R. Randel, K. Pogorelov, M. Lux, D. T. D. Nguyen, D. Johansen, C. Griwodz,
    H. K. Stensland, E. Garcia-Ceja, P. T. Schmidt, H. L. Hammer, M. A. Riegler, P. Halvorsen,
    and T. de Lange, “HyperKvasir, a comprehensive multi-class image and video dataset
    for gastrointestinal endoscopy,” *Scientific Data*, vol. 7, no. 1, p. 283, 2020.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] H. Borgli, V. Thambawita, P. H. Smedsrud, S. Hicks, D. Jha, S. L. Eskeland,
    K. R. Randel, K. Pogorelov, M. Lux, D. T. D. Nguyen, D. Johansen, C. Griwodz,
    H. K. Stensland, E. Garcia-Ceja, P. T. Schmidt, H. L. Hammer, M. A. Riegler, P.
    Halvorsen, 和 T. de Lange, “HyperKvasir，一个综合的多类图像和视频数据集用于胃肠内窥镜检查，” *科学数据*, vol.
    7, no. 1, p. 283, 2020.'
- en: '[86] J. Wang, F. Chen, Y. Ma, L. Wang, Z. Fei, J. Shuai, X. Tang, Q. Zhou,
    and J. Qin, “Xbound-former: Toward cross-scale boundary modeling in transformers,”
    *IEEE Transactions on Medical Imaging (TMI)*, vol. 42, no. 6, pp. 1735–1745, 2023.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Wang, F. Chen, Y. Ma, L. Wang, Z. Fei, J. Shuai, X. Tang, Q. Zhou,
    和 J. Qin, “Xbound-former：在变压器中实现跨尺度边界建模，” *IEEE 医学成像学报 (TMI)*, vol. 42, no. 6,
    pp. 1735–1745, 2023.'
- en: '[87] S. Jain, R. Atale, A. Gupta, U. Mishra, A. Seal, A. Ojha, J. Kuncewicz,
    and O. Krejcar, “Coinnet: A convolution-involution network with a novel statistical
    attention for automatic polyp segmentation,” *IEEE Transactions on Medical Imaging
    (TMI)*, vol. 42, no. 12, pp. 3987–4000, 2023.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] S. Jain, R. Atale, A. Gupta, U. Mishra, A. Seal, A. Ojha, J. Kuncewicz,
    和 O. Krejcar, “Coinnet：一种带有新型统计注意力的卷积-反卷积网络用于自动息肉分割，” *IEEE 医学成像学报 (TMI)*, vol.
    42, no. 12, pp. 3987–4000, 2023.'
- en: '[88] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2017, pp. 4700–4708.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] G. Huang, Z. Liu, L. Van Der Maaten, 和 K. Q. Weinberger, “密集连接卷积网络，” 收录于
    *IEEE 计算机视觉与模式识别会议 (CVPR)*，2017 年，第 4700–4708 页。'
- en: '[89] J. Wang and C. Chen, “Unsupervised adaptation of polyp segmentation models
    via coarse-to-fine self-supervision,” in *Proceedings of the International Conference
    on Information Processing in Medical Imaging (IPMI)*, 2023, pp. 250–262.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] J. Wang 和 C. Chen, “通过粗到细自监督的无监督息肉分割模型适应，” 收录于 *医学影像信息处理国际会议 (IPMI)*，2023
    年，第 250–262 页。'
- en: '[90] D. Jha, N. K. Tomar, V. Sharma, and U. Bagci, “Transnetr: Transformer-based
    residual network for polyp segmentation with multi-center out-of-distribution
    testing,” in *Medical Imaging with Deep Learning (MIDL)*, 2023.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] D. Jha, N. K. Tomar, V. Sharma, 和 U. Bagci, “Transnetr: 基于变换器的残差网络用于多中心分布外测试的息肉分割，”
    收录于 *深度学习医学影像 (MIDL)*，2023 年。'
- en: '[91] T. Zhou, Y. Zhou, K. He, C. Gong, J. Yang, H. Fu, and D. Shen, “Cross-level
    feature aggregation network for polyp segmentation,” *Pattern Recognition (PR)*,
    vol. 140, p. 109555, 2023.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] T. Zhou, Y. Zhou, K. He, C. Gong, J. Yang, H. Fu, 和 D. Shen, “用于息肉分割的跨层特征聚合网络，”
    *Pattern Recognition (PR)*，第 140 卷，第 109555 页，2023 年。'
- en: '[92] K. Wang, X. Zhang, Y. Lu, W. Zhang, S. Huang, and D. Yang, “Gsal: Geometric
    structure adversarial learning for robust medical image segmentation,” *Pattern
    Recognition (PR)*, vol. 140, p. 109596, 2023.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] K. Wang, X. Zhang, Y. Lu, W. Zhang, S. Huang, 和 D. Yang, “Gsal: 用于鲁棒医学图像分割的几何结构对抗学习，”
    *Pattern Recognition (PR)*，第 140 卷，第 109596 页，2023 年。'
- en: '[93] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *Proceedings of the International Conference on Learning
    Representations (ICLR)*, 2015.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络，” 收录于 *国际学习表征会议 (ICLR)*，2015
    年。'
- en: '[94] A. Wang, M. Wu, H. Qi, H. Shi, J. Chen, Y. Chen, and X. Luo, “Pyramid
    transformer driven multibranch fusion for polyp segmentation in colonoscopic video
    images,” in *Proceedings of the IEEE International Conference on Image Processing
    (ICIP)*, 2023, pp. 2350–2354.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] A. Wang, M. Wu, H. Qi, H. Shi, J. Chen, Y. Chen, 和 X. Luo, “基于金字塔变换器驱动的多分支融合用于结肠镜视频图像中的息肉分割，”
    收录于 *IEEE 国际图像处理会议 (ICIP)*，2023 年，第 2350–2354 页。'
- en: '[95] E. Moreu, E. Arazo, K. McGuinness, and N. E. O’Connor, “Self-supervised
    and semi-supervised polyp segmentation using synthetic data,” in *Proceedings
    of the IEEE International Joint Conference on Neural Networks (IJCNN)*, 2023,
    pp. 1–9.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] E. Moreu, E. Arazo, K. McGuinness, 和 N. E. O’Connor, “利用合成数据进行自监督和半监督的息肉分割，”
    收录于 *IEEE 国际神经网络联合会议 (IJCNN)*，2023 年，第 1–9 页。'
- en: '[96] P. Chao, C.-Y. Kao, Y.-S. Ruan, C.-H. Huang, and Y.-L. Lin, “Hardnet:
    A low memory traffic network,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision (ICCV)*, 2019, pp. 3552–3561.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] P. Chao, C.-Y. Kao, Y.-S. Ruan, C.-H. Huang, 和 Y.-L. Lin, “Hardnet: 低内存流量网络，”
    收录于 *IEEE/CVF 国际计算机视觉会议 (ICCV)*，2019 年，第 3552–3561 页。'
- en: '[97] H. Wu, Z. Zhao, J. Zhong, W. Wang, Z. Wen, and J. Qin, “Polypseg+: A lightweight
    context-aware network for real-time polyp segmentation,” *IEEE Transactions on
    Cybernetics*, vol. 53, no. 4, pp. 2610–2621, 2022.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] H. Wu, Z. Zhao, J. Zhong, W. Wang, Z. Wen, 和 J. Qin, “Polypseg+: 一种轻量级的上下文感知网络用于实时息肉分割，”
    *IEEE Transactions on Cybernetics*，第 53 卷，第 4 期，第 2610–2621 页，2022 年。'
- en: '[98] M. M. Rahman and R. Marculescu, “Medical image segmentation via cascaded
    attention decoding,” in *Proceedings of the IEEE/CVF Winter Conference on Applications
    of Computer Vision (WACV)*, 2023, pp. 6222–6231.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. M. Rahman 和 R. Marculescu, “通过级联注意解码进行医学图像分割，” 收录于 *IEEE/CVF 冬季计算机视觉应用会议
    (WACV)*，2023 年，第 6222–6231 页。'
- en: '[99] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng,
    I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester *et al.*, “Deep learning techniques
    for automatic mri cardiac multi-structures segmentation and diagnosis: is the
    problem solved?” *IEEE Transactions on Medical Imaging (TMI)*, vol. 37, no. 11,
    pp. 2514–2525, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng,
    I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester *等*，“自动 MRI 心脏多结构分割与诊断的深度学习技术：问题解决了吗？”
    *IEEE Transactions on Medical Imaging (TMI)*，第 37 卷，第 11 期，第 2514–2525 页，2018
    年。'
- en: '[100] D. Bo, W. Wenhai, F. Deng-Ping, L. Jinpeng, F. Huazhu, and S. Ling, “Polyp-pvt:
    Polyp segmentation with pyramid vision transformers,” 2023.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] D. Bo, W. Wenhai, F. Deng-Ping, L. Jinpeng, F. Huazhu, 和 S. Ling, “Polyp-pvt:
    使用金字塔视觉变换器进行息肉分割，” 2023 年。'
- en: '[101] T.-H. Nguyen-Mau, Q.-H. Trinh, N.-T. Bui, P.-T. V. Thi, M.-V. Nguyen,
    X.-N. Cao, M.-T. Tran, and H.-D. Nguyen, “Pefnet: Positional embedding feature
    for polyp segmentation,” in *Proceedings of the International Conference on Multimedia
    Modeling (MMM)*, 2023, pp. 240–251.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] T.-H. Nguyen-Mau, Q.-H. Trinh, N.-T. Bui, P.-T. V. Thi, M.-V. Nguyen,
    X.-N. Cao, M.-T. Tran, 和 H.-D. Nguyen, “Pefnet：用于息肉分割的位置信息嵌入特征，” 收录于*国际多媒体建模会议（MMM）论文集*，2023年，第240–251页。'
- en: '[102] M. Tan and Q. Le, “Efficientnetv2: Smaller models and faster training,”
    in *Proceedings of the International Conference on Machine Learning (ICML)*, 2021,
    pp. 10 096–10 106.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] M. Tan 和 Q. Le, “Efficientnetv2：更小的模型和更快的训练，” 收录于*国际机器学习会议（ICML）论文集*，2021年，第10,096–10,106页。'
- en: '[103] J. Silva, A. Histace, O. Romain, X. Dray, and B. Granado, “Toward embedded
    detection of polyps in wce images for early diagnosis of colorectal cancer,” *International
    Journal of Computer Assisted Radiology and Surgery*, vol. 9, pp. 283–293, 2014.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. Silva, A. Histace, O. Romain, X. Dray, 和 B. Granado, “朝向在WCE图像中嵌入式息肉检测以早期诊断结直肠癌，”
    *计算机辅助放射学与手术国际期刊*，卷9，第283–293页，2014年。'
- en: '[104] N. Tajbakhsh, S. R. Gurudu, and J. Liang, “Automated polyp detection
    in colonoscopy videos using shape and context information,” *IEEE Transactions
    on Medical Imaging (TMI)*, vol. 35, no. 2, pp. 630–644, 2015.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] N. Tajbakhsh, S. R. Gurudu, 和 J. Liang, “使用形状和上下文信息的结肠镜视频自动息肉检测，” *IEEE医学成像期刊（TMI）*，卷35，第2期，第630–644页，2015年。'
- en: '[105] L. Cai, M. Wu, L. Chen, W. Bai, M. Yang, S. Lyu, and Q. Zhao, “Using
    guided self-attention with local information for polyp segmentation,” in *Proceedings
    of the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2022, pp. 629–638.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] L. Cai, M. Wu, L. Chen, W. Bai, M. Yang, S. Lyu, 和 Q. Zhao, “使用带有局部信息的引导自注意力进行息肉分割，”
    收录于*国际医学图像计算与计算机辅助干预会议（MICCAI）论文集*，2022年，第629–638页。'
- en: '[106] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, “Cvt:
    Introducing convolutions to vision transformers,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision (ICCV)*, 2021, pp. 22–31.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, 和 L. Zhang, “Cvt：将卷积引入视觉变换器，”
    收录于*IEEE/CVF国际计算机视觉会议（ICCV）论文集*，2021年，第22–31页。'
- en: '[107] Y. Ma, X. Chen, K. Cheng, Y. Li, and B. Sun, “Ldpolypvideo benchmark:
    a large-scale colonoscopy video dataset of diverse polyps,” in *Proceedings of
    the International Conference on Medical Image Computing and Computer Assisted
    Intervention (MICCAI)*, 2021, pp. 387–396.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Y. Ma, X. Chen, K. Cheng, Y. Li, 和 B. Sun, “Ldpolypvideo基准：大规模多样化息肉的结肠镜视频数据集，”
    收录于*国际医学图像计算与计算机辅助干预会议（MICCAI）论文集*，2021年，第387–396页。'
- en: '[108] D. Vázquez, J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, A. M. López,
    A. Romero, M. Drozdzal, A. Courville *et al.*, “A benchmark for endoluminal scene
    segmentation of colonoscopy images,” *Journal of Healthcare Engineering*, vol.
    2017, 2017.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] D. Vázquez, J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, A. M. López,
    A. Romero, M. Drozdzal, A. Courville *等*，“结肠镜图像内腔场景分割基准测试，” *医疗工程期刊*，卷2017，2017年。'
- en: '[109] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018,
    pp. 7132–7141.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] J. Hu, L. Shen, 和 G. Sun, “挤压与激励网络，” 收录于*IEEE计算机视觉与模式识别会议（CVPR）论文集*，2018年，第7132–7141页。'
- en: '[110] E. Sanderson and B. J. Matuszewski, “Fcn-transformer feature fusion for
    polyp segmentation,” in *Proceedings of the Conference on Medical Image Understanding
    and Analysis (MIUA)*, 2022, pp. 892–907.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] E. Sanderson 和 B. J. Matuszewski, “Fcn-transformer特征融合用于息肉分割，” 收录于*医疗图像理解与分析会议（MIUA）论文集*，2022年，第892–907页。'
- en: '[111] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolutional
    neural networks,” in *Proceedings of the International Conference on Machine Learning
    (ICML)*, 2019, pp. 6105–6114.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] M. Tan 和 Q. Le, “Efficientnet：重新思考卷积神经网络的模型缩放，” 收录于*国际机器学习会议（ICML）论文集*，2019年，第6105–6114页。'
- en: '[112] Y. Xiao, Z. Chen, L. Wan, L. Yu, and L. Zhu, “Icbnet: Iterative context-boundary
    feedback network for polyp segmentation,” in *Proceedings of the IEEE International
    Conference on Bioinformatics and Biomedicine (BIBM)*, 2022, pp. 1297–1304.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Y. Xiao, Z. Chen, L. Wan, L. Yu, 和 L. Zhu, “Icbnet：用于息肉分割的迭代上下文边界反馈网络，”
    收录于*IEEE国际生物信息学与生物医学会议（BIBM）论文集*，2022年，第1297–1304页。'
- en: '[113] R. Chen, X. Wang, B. Jin, J. Tu, F. Zhu, and Y. Li, “Cld-net: Complement
    local detail for medical small-object segmentation,” in *Proceedings of the IEEE
    International Conference on Bioinformatics and Biomedicine (BIBM)*, 2022, pp.
    942–947.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] R. Chen, X. Wang, B. Jin, J. Tu, F. Zhu 和 Y. Li，“Cld-net：补充局部细节用于医学小物体分割”，发表于
    *IEEE生物信息学与生物医学国际会议（BIBM）论文集*，2022年，第942–947页。'
- en: '[114] H. Du, J. Wang, M. Liu, Y. Wang, and E. Meijering, “Swinpa-net: Swin
    transformer-based multiscale feature pyramid aggregation network for medical image
    segmentation,” *IEEE Transactions on Neural Networks and Learning Systems (TNNLS)*,
    2022.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] H. Du, J. Wang, M. Liu, Y. Wang 和 E. Meijering，“Swinpa-net：基于Swin变换器的多尺度特征金字塔聚合网络用于医学图像分割”，*IEEE神经网络与学习系统汇刊（TNNLS）*，2022年。'
- en: '[115] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
    “Swin transformer: Hierarchical vision transformer using shifted windows,” in
    *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*,
    2021, pp. 10 012–10 022.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin 和 B. Guo，“Swin变换器：使用移位窗口的层次视觉变换器”，发表于
    *IEEE/CVF 国际计算机视觉大会（ICCV）论文集*，2021年，第10 012–10 022页。'
- en: '[116] W. Zhang, C. Fu, Y. Zheng, F. Zhang, Y. Zhao, and C.-W. Sham, “Hsnet:
    A hybrid semantic network for polyp segmentation,” *Computers in Biology and Medicine
    (CBM)*, vol. 150, p. 106173, 2022.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] W. Zhang, C. Fu, Y. Zheng, F. Zhang, Y. Zhao 和 C.-W. Sham，“Hsnet：用于息肉分割的混合语义网络”，*生物医学计算机（CBM）*，第150卷，第106173页，2022年。'
- en: '[117] F. Liu, Z. Hua, J. Li, and L. Fan, “Dbmf: Dual branch multiscale feature
    fusion network for polyp segmentation,” *Computers in Biology and Medicine (CBM)*,
    vol. 151, p. 106304, 2022.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] F. Liu, Z. Hua, J. Li 和 L. Fan，“Dbmf：用于息肉分割的双分支多尺度特征融合网络”，*生物医学计算机（CBM）*，第151卷，第106304页，2022年。'
- en: '[118] L. F. Sánchez-Peralta, J. B. Pagador, A. Picón, Á. J. Calderón, F. Polo,
    N. Andraka, R. Bilbao, B. Glover, C. L. Saratxaga, and F. M. Sánchez-Margallo,
    “Piccolo white-light and narrow-band imaging colonoscopic dataset: A performance
    comparative of models and datasets,” *Applied Sciences*, vol. 10, no. 23, p. 8501,
    2020.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] L. F. Sánchez-Peralta, J. B. Pagador, A. Picón, Á. J. Calderón, F. Polo,
    N. Andraka, R. Bilbao, B. Glover, C. L. Saratxaga 和 F. M. Sánchez-Margallo，“Piccolo白光和窄带成像结肠镜数据集：模型和数据集的性能比较”，*应用科学*，第10卷，第23期，第8501页，2020年。'
- en: '[119] M. Cheng, Z. Kong, G. Song, Y. Tian, Y. Liang, and J. Chen, “Learnable
    oriented-derivative network for polyp segmentation,” in *Proceedings of the International
    Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)*.   Springer,
    2021, pp. 720–730.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] M. Cheng, Z. Kong, G. Song, Y. Tian, Y. Liang 和 J. Chen，“可学习的方向导数网络用于息肉分割”，发表于
    *国际医学图像计算与计算机辅助手术会议（MICCAI）论文集*。 Springer，2021年，第720–730页。'
- en: '[120] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,
    “Training data-efficient image transformers & distillation through attention,”
    in *Proceedings of the International Conference on Machine Learning (ICML)*, 2021,
    pp. 10 347–10 357.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles 和 H. Jégou，“通过注意力训练数据高效的图像变换器与蒸馏”，发表于
    *国际机器学习大会（ICML）论文集*，2021年，第10 347–10 357页。'
- en: '[121] Y. Tian, G. Pang, F. Liu, Y. Chen, S. H. Shin, J. W. Verjans, R. Singh,
    and G. Carneiro, “Constrained contrastive distribution learning for unsupervised
    anomaly detection and localisation in medical images,” in *Proceedings of the
    International Conference on Medical Image Computing and Computer Assisted Intervention
    (MICCAI)*, 2021, pp. 128–140.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Y. Tian, G. Pang, F. Liu, Y. Chen, S. H. Shin, J. W. Verjans, R. Singh
    和 G. Carneiro，“用于医学图像的无监督异常检测与定位的约束对比分布学习”，发表于 *国际医学图像计算与计算机辅助手术会议（MICCAI）论文集*，2021年，第128–140页。'
- en: '[122] S. Li, X. Sui, J. Fu, H. Fu, X. Luo, Y. Feng, X. Xu, Y. Liu, D. S. Ting,
    and R. S. M. Goh, “Few-shot domain adaptation with polymorphic transformers,”
    in *Proceedings of the International Conference on Medical Image Computing and
    Computer Assisted Intervention (MICCAI)*, 2021, pp. 330–340.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] S. Li, X. Sui, J. Fu, H. Fu, X. Luo, Y. Feng, X. Xu, Y. Liu, D. S. Ting
    和 R. S. M. Goh，“具有多态变换器的少样本领域适应”，发表于 *国际医学图像计算与计算机辅助手术会议（MICCAI）论文集*，2021年，第330–340页。'
- en: '[123] S. Li, X. Sui, X. Luo, X. Xu, Y. Liu, and R. Goh, “Medical image segmentation
    using squeeze-and-expansion transformers,” *Proceedings of the International Joint
    Conference on Artificial Intelligence (IJCAI)*, vol. 576, p. 576.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] S. Li, X. Sui, X. Luo, X. Xu, Y. Liu 和 R. Goh，“使用挤压与扩张变换器进行医学图像分割”，*国际人工智能联合会议（IJCAI）论文集*，第576卷，第576页。'
- en: '[124] G.-P. Ji, Y.-C. Chou, D.-P. Fan, G. Chen, H. Fu, D. Jha, and L. Shao,
    “Progressively normalized self-attention network for video polyp segmentation,”
    in *Proceedings of the International Conference on Medical Image Computing and
    Computer-Assisted Intervention (MICCAI)*, 2021, pp. 142–152.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] G.-P. Ji, Y.-C. Chou, D.-P. Fan, G. Chen, H. Fu, D. Jha, 和 L. Shao，“渐进归一化自注意力网络用于视频息肉分割”，见于
    *医学图像计算与计算机辅助干预国际会议（MICCAI）论文集*，2021，pp. 142–152。'
- en: '[125] X. Guo, C. Yang, and Y. Yuan, “Dynamic-weighting hierarchical segmentation
    network for medical images,” *Medical Image Analysis (MedIA)*, vol. 73, p. 102196,
    2021.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] X. Guo, C. Yang, 和 Y. Yuan，“用于医学图像的动态加权层次分割网络”，*医学图像分析（MedIA）*，第73卷，p.
    102196，2021。'
- en: '[126] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous
    convolution for semantic image segmentation,” *arXiv preprint arXiv:1706.05587*,
    2017.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] L.-C. Chen, G. Papandreou, F. Schroff, 和 H. Adam，“重新思考用于语义图像分割的膨胀卷积”，*arXiv
    预印本 arXiv:1706.05587*，2017。'
- en: '[127] X. Guo, C. Yang, Y. Liu, and Y. Yuan, “Learn to threshold: Thresholdnet
    with confidence-guided manifold mixup for polyp segmentation,” *IEEE Transactions
    on Medical Imaging (TMI)*, vol. 40, no. 4, pp. 1134–1146, 2021.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] X. Guo, C. Yang, Y. Liu, 和 Y. Yuan，“学习阈值：用于息肉分割的信心引导流形混合的 Thresholdnet”，*IEEE
    医学成像学报（TMI）*，第40卷，第4期，pp. 1134–1146，2021。'
- en: '[128] C. Yang, X. Guo, M. Zhu, B. Ibragimov, and Y. Yuan, “Mutual-prototype
    adaptation for cross-domain polyp segmentation,” *IEEE Journal of Biomedical and
    Health Informatics (JBHI)*, vol. 25, no. 10, pp. 3886–3897, 2021.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] C. Yang, X. Guo, M. Zhu, B. Ibragimov, 和 Y. Yuan，“跨域息肉分割的互原型适应”，*IEEE
    生物医学与健康信息学杂志（JBHI）*，第25卷，第10期，pp. 3886–3897，2021。'
- en: '[129] D. Jha, P. H. Smedsrud, D. Johansen, T. de Lange, H. D. Johansen, P. Halvorsen,
    and M. A. Riegler, “A comprehensive study on colorectal polyp segmentation with
    resunet++, conditional random field and test-time augmentation,” *IEEE Journal
    of Biomedical and Health Informatics (JBHI)*, vol. 25, no. 6, pp. 2029–2040, 2021.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] D. Jha, P. H. Smedsrud, D. Johansen, T. de Lange, H. D. Johansen, P.
    Halvorsen, 和 M. A. Riegler，“关于结直肠息肉分割的全面研究：使用 resunet++、条件随机场和测试时间增强”，*IEEE 生物医学与健康信息学杂志（JBHI）*，第25卷，第6期，pp.
    2029–2040，2021。'
- en: '[130] Z. Zhang, Q. Liu, and Y. Wang, “Road extraction by deep residual u-net,”
    *IEEE Transactions on Geoscience and Remote Sensing (TGRS)*, vol. 15, no. 5, pp.
    749–753, 2018.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Z. Zhang, Q. Liu, 和 Y. Wang，“通过深度残差 u-net 进行道路提取”，*IEEE 地球科学与遥感学报（TGRS）*，第15卷，第5期，pp.
    749–753，2018。'
- en: '[131] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2017, pp. 2117–2125.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, 和 S. Belongie，“用于物体检测的特征金字塔网络”，见于
    *IEEE 计算机视觉与模式识别会议（CVPR）论文集*，2017，pp. 2117–2125。'
- en: '[132] H. Wu, G. Chen, Z. Wen, and J. Qin, “Collaborative and adversarial learning
    of focused and dispersive representations for semi-supervised polyp segmentation,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*,
    2021, pp. 3489–3498.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] H. Wu, G. Chen, Z. Wen, 和 J. Qin，“面向半监督息肉分割的聚焦与分散表示的协作与对抗学习”，见于 *IEEE/CVF
    国际计算机视觉会议（ICCV）论文集*，2021，pp. 3489–3498。'
- en: '[133] S. Safarov and T. K. Whangbo, “A-denseunet: Adaptive densely connected
    unet for polyp segmentation in colonoscopy images with atrous convolution,” *Sensors*,
    vol. 21, no. 4, p. 1441, 2021.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] S. Safarov 和 T. K. Whangbo，“A-denseunet：一种自适应密集连接的 Unet，用于内窥镜图像中息肉分割，采用膨胀卷积”，*传感器*，第21卷，第4期，p.
    1441，2021。'
- en: '[134] K. Patel, A. M. Bur, and G. Wang, “Enhanced u-net: A feature enhancement
    network for polyp segmentation,” in *Proceedings of the IEEE 18th Conference on
    Robots and Vision (CRV)*, 2021, pp. 181–188.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] K. Patel, A. M. Bur, 和 G. Wang，“增强的 u-net：一种用于息肉分割的特征增强网络”，见于 *IEEE 第18届机器人与视觉会议（CRV）论文集*，2021，pp.
    181–188。'
- en: '[135] M. Yeung, E. Sala, C.-B. Schönlieb, and L. Rundo, “Focus u-net: A novel
    dual attention-gated cnn for polyp segmentation during colonoscopy,” *Computers
    in Biology and Medicine (CBM)*, vol. 137, p. 104815, 2021.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] M. Yeung, E. Sala, C.-B. Schönlieb, 和 L. Rundo，“Focus u-net：一种新型的双重注意力门控
    CNN，用于内窥镜检查中的息肉分割”，*生物医学与医学计算机（CBM）*，第137卷，p. 104815，2021。'
- en: '[136] S.-T. Tran, C.-H. Cheng, T.-T. Nguyen, M.-H. Le, and D.-G. Liu, “Tmd-unet:
    Triple-unet with multi-scale input features and dense skip connection for medical
    image segmentation,” in *Healthcare*, vol. 9, no. 1, 2021, p. 54.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] S.-T. Tran, C.-H. Cheng, T.-T. Nguyen, M.-H. Le, 和 D.-G. Liu，“Tmd-unet：具有多尺度输入特征和密集跳跃连接的三重
    Unet，用于医学图像分割”，见于 *医疗保健*，第9卷，第1期，2021，p. 54。'
- en: '[137] V. Thambawita, S. A. Hicks, P. Halvorsen, and M. A. Riegler, “Divergentnets:
    Medical image segmentation by network ensemble,” in *Proceedings of the IEEE International
    Symposium on Biomedical Imaging Workshops (ISBIW)*, 2021.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] V. Thambawita, S. A. Hicks, P. Halvorsen, 和 M. A. Riegler, “Divergentnets:
    通过网络集成进行医学图像分割，” 发表在 *IEEE国际生物医学成像研讨会（ISBIW）论文集*，2021年。'
- en: '[138] Y. Meng, H. Zhang, D. Gao, Y. Zhao, X. Yang, X. Qian, X. Huang, Y. Zheng,
    A. Remark, and U. London, “Bi-gcn: Boundary-aware input-dependent graph convolution
    network for biomedical image segmentation,” *Proceedings of the British Machine
    Vision Conference (BMVC)*, 2021.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Y. Meng, H. Zhang, D. Gao, Y. Zhao, X. Yang, X. Qian, X. Huang, Y. Zheng,
    A. Remark, 和 U. London, “Bi-gcn: 边界感知的输入依赖图卷积网络用于生物医学图像分割，” 发表在 *英国机器视觉会议（BMVC）论文集*，2021年。'
- en: '[139] X. Xie, J. Chen, Y. Li, L. Shen, K. Ma, and Y. Zheng, “Mi2gan: Generative
    adversarial network for medical image domain adaptation using mutual information
    constraint,” in *Proceedings of the International Conference on Medical Image
    Computing and Computer-Assisted Intervention (MICCAI)*, 2020, pp. 516–525.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] X. Xie, J. Chen, Y. Li, L. Shen, K. Ma, 和 Y. Zheng, “Mi2gan: 使用互信息约束的生成对抗网络用于医学图像领域适应，”
    发表在 *国际医学图像计算与计算机辅助干预会议（MICCAI）论文集*，2020年，第516–525页。'
- en: '[140] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *Proceedings of the
    IEEE International Conference on Computer Vision (ICCV)*, 2017, pp. 2223–2232.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] J.-Y. Zhu, T. Park, P. Isola, 和 A. A. Efros, “使用循环一致对抗网络的无配对图像到图像翻译，”
    发表在 *IEEE国际计算机视觉会议（ICCV）论文集*，2017年，第2223–2232页。'
- en: '[141] J. Zhong, W. Wang, H. Wu, Z. Wen, and J. Qin, “Polypseg: An efficient
    context-aware network for polyp segmentation from colonoscopy videos,” in *Proceedings
    of the International Conference on Medical Image Computing and Computer-Assisted
    Intervention (MICCAI)*, 2020, pp. 285–294.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] J. Zhong, W. Wang, H. Wu, Z. Wen, 和 J. Qin, “Polypseg: 一种高效的上下文感知网络用于结肠镜视频中的息肉分割，”
    发表在 *国际医学图像计算与计算机辅助干预会议（MICCAI）论文集*，2020年，第285–294页。'
- en: '[142] Y. Fang, D. Zhu, J. Yao, Y. Yuan, and K.-Y. Tong, “Abc-net: Area-boundary
    constraint network with dynamical feature selection for colorectal polyp segmentation,”
    *IEEE Sensors Journal*, vol. 21, no. 10, pp. 11 799–11 809, 2020.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Y. Fang, D. Zhu, J. Yao, Y. Yuan, 和 K.-Y. Tong, “Abc-net: 一种用于结直肠息肉分割的面积-边界约束网络，具备动态特征选择，”
    *IEEE传感器期刊*，第21卷，第10期，第11,799–11,809页，2020年。'
- en: '[143] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2017, pp. 1492–1500.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] S. Xie, R. Girshick, P. Dollár, Z. Tu, 和 K. He, “用于深度神经网络的聚合残差变换，” 发表在
    *IEEE计算机视觉与模式识别会议（CVPR）论文集*，2017年，第1492–1500页。'
- en: '[144] K. Wickstrøm, M. Kampffmeyer, and R. Jenssen, “Uncertainty and interpretability
    in convolutional neural networks for semantic segmentation of colorectal polyps,”
    *Medical Image Analysis (MedIA)*, vol. 60, p. 101619, 2020.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] K. Wickstrøm, M. Kampffmeyer, 和 R. Jenssen, “卷积神经网络在结直肠息肉语义分割中的不确定性和可解释性，”
    *医学图像分析（MedIA）*，第60卷，第101619页，2020年。'
- en: '[145] S. Wang, Y. Cong, H. Zhu, X. Chen, L. Qu, H. Fan, Q. Zhang, and M. Liu,
    “Multi-scale context-guided deep network for automated lesion segmentation with
    endoscopy images of gastrointestinal tract,” *IEEE Journal of Biomedical and Health
    Informatics (JBHI)*, vol. 25, no. 2, pp. 514–525, 2020.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] S. Wang, Y. Cong, H. Zhu, X. Chen, L. Qu, H. Fan, Q. Zhang, 和 M. Liu,
    “基于多尺度上下文引导的深度网络用于胃肠道内镜图像的自动病灶分割，” *IEEE生物医学与健康信息学期刊（JBHI）*，第25卷，第2期，第514–525页，2020年。'
- en: '[146] Z. Huang, Z. Wang, J. Chen, Z. Zhu, and J. Li, “Real-time colonoscopy
    image segmentation based on ensemble knowledge distillation,” in *Proceedings
    of the IEEE International Conference on Advanced Robotics and Mechatronics (ICARM)*,
    2020, pp. 454–459.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Z. Huang, Z. Wang, J. Chen, Z. Zhu, 和 J. Li, “基于集成知识蒸馏的实时结肠镜图像分割，” 发表在
    *IEEE国际先进机器人与机电一体化会议（ICARM）论文集*，2020年，第454–459页。'
- en: '[147] D. Jha, M. A. Riegler, D. Johansen, P. Halvorsen, and H. D. Johansen,
    “Doubleu-net: A deep convolutional neural network for medical image segmentation,”
    in *Proceedings of the IEEE International Symposium on Computer-based Medical
    Systems (CBMS)*, 2020, pp. 558–564.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] D. Jha, M. A. Riegler, D. Johansen, P. Halvorsen, 和 H. D. Johansen, “Doubleu-net:
    一种用于医学图像分割的深度卷积神经网络，” 发表在 *IEEE国际计算机医学系统研讨会（CBMS）会议论文集*，2020年，第558–564页。'
- en: '[148] V. de Almeida Thomaz, C. A. Sierra-Franco, and A. B. Raposo, “Training
    data enhancements for robust polyp segmentation in colonoscopy images,” in *Proceedings
    of the IEEE International Symposium on Computer-Based Medical Systems (CBMS)*,
    2019, pp. 192–197.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] V. de Almeida Thomaz, C. A. Sierra-Franco, 和 A. B. Raposo，“针对结肠镜图像中鲁棒的息肉分割的训练数据增强”，收录于*IEEE国际计算机医学系统研讨会
    (CBMS)*，2019，页码192–197。'
- en: '[149] B. Murugesan, K. Sarveswaran, S. M. Shankaranarayana, K. Ram, J. Joseph,
    and M. Sivaprakasam, “Psi-net: Shape and boundary aware joint multi-task deep
    network for medical image segmentation,” in *Proceedings of the IEEE Engineering
    in Medicine and Biology Society (EMBC)*, 2019, pp. 7223–7226.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] B. Murugesan, K. Sarveswaran, S. M. Shankaranarayana, K. Ram, J. Joseph,
    和 M. Sivaprakasam，“Psi-net: 形状和边界感知的联合多任务深度网络用于医学图像分割”，收录于*IEEE医学与生物工程学会会议录 (EMBC)*，2019，页码7223–7226。'
- en: '[150] J. Poorneshwaran, S. S. Kumar, K. Ram, J. Joseph, and M. Sivaprakasam,
    “Polyp segmentation using generative adversarial network,” in *Proceedings of
    the IEEE Engineering in Medicine and Biology Society (EMBC)*, 2019, pp. 7201–7204.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] J. Poorneshwaran, S. S. Kumar, K. Ram, J. Joseph, 和 M. Sivaprakasam，“使用生成对抗网络进行息肉分割”，收录于*IEEE医学与生物工程学会会议录
    (EMBC)*，2019，页码7201–7204。'
- en: '[151] X. Sun, P. Zhang, D. Wang, Y. Cao, and B. Liu, “Colorectal polyp segmentation
    by u-net with dilation convolution,” in *Proceedings of the IEEE International
    Conference on Machine Learning and Applications (ICMLA)*, 2019, pp. 851–858.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] X. Sun, P. Zhang, D. Wang, Y. Cao, 和 B. Liu，“通过带扩张卷积的u-net进行结直肠息肉分割”，收录于*IEEE国际机器学习与应用会议录
    (ICMLA)*，2019，页码851–858。'
- en: '[152] J. Bernal, N. Tajkbaksh, F. J. Sanchez, B. J. Matuszewski, H. Chen, L. Yu,
    Q. Angermann, O. Romain, B. Rustad, I. Balasingham *et al.*, “Comparative validation
    of polyp detection methods in video colonoscopy: results from the miccai 2015
    endoscopic vision challenge,” *IEEE Transactions on Medical Imaging*, vol. 36,
    no. 6, pp. 1231–1249, 2017.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] J. Bernal, N. Tajkbaksh, F. J. Sanchez, B. J. Matuszewski, H. Chen, L.
    Yu, Q. Angermann, O. Romain, B. Rustad, I. Balasingham *等*，“视频结肠镜检查中息肉检测方法的比较验证：来自miccai
    2015内窥镜视觉挑战的结果”，*IEEE医学成像汇刊*，第36卷，第6期，页码1231–1249，2017。'
- en: '[153] H. A. Qadir, Y. Shin, J. Solhusvik, J. Bergsland, L. Aabakken, and I. Balasingham,
    “Polyp detection and segmentation using mask r-cnn: Does a deeper feature extractor
    cnn always perform better?” in *Proceedings of the IEEE International Symposium
    on Medical Information and Communication Technology (ISMICT)*, 2019, pp. 1–6.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] H. A. Qadir, Y. Shin, J. Solhusvik, J. Bergsland, L. Aabakken, 和 I. Balasingham，“使用mask
    r-cnn进行息肉检测和分割：更深的特征提取器cnn是否总是表现更好？” 收录于*IEEE国际医学信息与通信技术研讨会 (ISMICT)*，2019，页码1–6。'
- en: '[154] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *Proceedings
    of the IEEE International Conference on Computer Vision (ICCV)*, 2017, pp. 2961–2969.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] K. He, G. Gkioxari, P. Dollár, 和 R. Girshick，“Mask r-cnn”，收录于*IEEE国际计算机视觉大会
    (ICCV)*，2017，页码2961–2969。'
- en: '[155] Y. B. Guo and B. Matuszewski, “Giana polyp segmentation with fully convolutional
    dilation neural networks,” in *Proceedings of the International Joint Conference
    on Computer Vision, Imaging and Computer Graphics Theory and Applications*, 2019,
    pp. 632–641.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Y. B. Guo 和 B. Matuszewski，“Giana息肉分割与全卷积扩张神经网络”，收录于*国际联合计算机视觉、成像和计算机图形理论与应用大会会议录*，2019，页码632–641。'
- en: '[156] M. Bagheri, M. Mohrekesh, M. Tehrani, K. Najarian, N. Karimi, S. Samavi,
    and S. R. Soroushmehr, “Deep neural network based polyp segmentation in colonoscopy
    images using a combination of color spaces,” in *Proceedings of the IEEE Engineering
    in Medicine and Biology Society (EMBC)*, 2019, pp. 6742–6745.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] M. Bagheri, M. Mohrekesh, M. Tehrani, K. Najarian, N. Karimi, S. Samavi,
    和 S. R. Soroushmehr，“基于深度神经网络的结肠镜图像息肉分割，使用色彩空间组合”，收录于*IEEE医学与生物工程学会会议录 (EMBC)*，2019，页码6742–6745。'
- en: '[157] A. Chaurasia and E. Culurciello, “Linknet: Exploiting encoder representations
    for efficient semantic segmentation,” in *Proceedings of the IEEE Visual Communications
    and Image Processing (VCIP)*, 2017, pp. 1–4.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] A. Chaurasia 和 E. Culurciello，“Linknet: 利用编码器表示进行高效的语义分割”，收录于*IEEE视觉通信与图像处理会议录
    (VCIP)*，2017，页码1–4。'
- en: '[158] X. Guo, N. Zhang, J. Guo, H. Zhang, Y. Hao, and J. Hang, “Automated polyp
    segmentation for colonoscopy images: A method based on convolutional neural networks
    and ensemble learning,” *Medical Physics*, vol. 46, no. 12, pp. 5666–5676, 2019.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] X. Guo, N. Zhang, J. Guo, H. Zhang, Y. Hao, 和 J. Hang，“用于结肠镜图像的自动息肉分割：基于卷积神经网络和集成学习的方法”，*医学物理学*，第46卷，第12期，页码5666–5676，2019。'
- en: '[159] A. Sánchez-González, B. García-Zapirain, D. Sierra-Sosa, and A. Elmaghraby,
    “Automatized colon polyp segmentation via contour region analysis,” *Computers
    in Biology and Medicine (CBM)*, vol. 100, pp. 152–164, 2018.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] A. Sánchez-González, B. García-Zapirain, D. Sierra-Sosa, 和 A. Elmaghraby，“通过轮廓区域分析自动化结肠息肉分割”，*生物医学与医学中的计算机（CBM）*，第100卷，第152–164页，2018年。'
- en: '[160] K. Wickstrøm, M. Kampffmeyer, and R. Jenssen, “Uncertainty modeling and
    interpretability in convolutional neural networks for polyp segmentation,” in
    *Proceedings of the IEEE International Workshop on Machine Learning for Signal
    Processing (MLSP)*, 2018, pp. 1–6.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] K. Wickstrøm, M. Kampffmeyer, 和 R. Jenssen，“卷积神经网络中息肉分割的置信度建模和可解释性”，见于*IEEE信号处理机器学习国际研讨会（MLSP）*，2018年，第1–6页。'
- en: '[161] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence (TPAMI)*, vol. 39, no. 12, pp. 2481–2495, 2017.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] V. Badrinarayanan, A. Kendall, 和 R. Cipolla，“Segnet：一种用于图像分割的深度卷积编码解码器架构”，*IEEE模式分析与机器智能汇刊（TPAMI）*，第39卷，第12期，第2481–2495页，2017年。'
- en: '[162] O. H. Maghsoudi, “Superpixel based segmentation and classification of
    polyps in wireless capsule endoscopy,” in *Proceedings of the IEEE Signal Processing
    in Medicine and Biology Symposium (SPMB)*, 2017, pp. 1–4.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] O. H. Maghsoudi，“基于超像素的无线胶囊内镜中息肉的分割和分类”，见于*IEEE医学和生物学信号处理研讨会（SPMB）*，2017年，第1–4页。'
- en: '[163] Y. Jia, “Polyps auto-detection in wireless capsule endoscopy images using
    improved method based on image segmentation,” in *Proceedings of the IEEE International
    Conference on Robotics and Biomimetics (ROBIO)*, 2015, pp. 1631–1636.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Y. Jia，“基于图像分割的改进方法在无线胶囊内镜图像中的息肉自动检测”，见于*IEEE国际机器人与仿生学会议（ROBIO）*，2015年，第1631–1636页。'
- en: '[164] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Proceedings of the
    Advances in Neural Information Processing Systems (NeurIPS)*, vol. 30, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“Attention is all you need”，见于*神经信息处理系统进展会议（NeurIPS）*，第30卷，2017年。'
- en: '[165] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” *arXiv preprint arXiv:1409.0473*, 2014.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] D. Bahdanau, K. Cho, 和 Y. Bengio，“通过联合学习对齐和翻译的神经机器翻译”，*arXiv预印本 arXiv:1409.0473*，2014年。'
- en: '[166] P. Ngoc Lan, N. S. An, D. V. Hang, D. V. Long, T. Q. Trung, N. T. Thuy,
    and D. V. Sang, “Neounet: Towards accurate colon polyp segmentation and neoplasm
    detection,” in *Advances in Visual Computing: 16th International Symposium (ISVC)*,
    2021, pp. 15–28.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] P. Ngoc Lan, N. S. An, D. V. Hang, D. V. Long, T. Q. Trung, N. T. Thuy,
    和 D. V. Sang，“Neounet：致力于准确的结肠息肉分割和新生物检测”，见于*视觉计算进展：第16届国际研讨会（ISVC）*，2021年，第15–28页。'
- en: '[167] L. Yao, F. He, X. Wang, L. Zhou, H. Peng, and X. Huang, “Scheme and dataset
    for evaluating computer-aided polyp detection system in colonoscopy,” in *Proceedings
    of the IEEE International Symposium on Biomedical Imaging (ISBI)*, 2022, pp. 1–5.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] L. Yao, F. He, X. Wang, L. Zhou, H. Peng, 和 X. Huang，“用于评估计算机辅助肠镜息肉检测系统的方案和数据集”，见于*IEEE国际生物医学成像研讨会（ISBI）*，2022年，第1–5页。'
- en: '[168] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2017, pp. 2881–2890.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] H. Zhao, J. Shi, X. Qi, X. Wang, 和 J. Jia，“金字塔场景解析网络”，见于*IEEE计算机视觉与模式识别会议（CVPR）*，2017年，第2881–2890页。'
- en: '[169] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk, “Frequency-tuned
    salient region detection,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2009, pp. 1597–1604.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] R. Achanta, S. Hemami, F. Estrada, 和 S. Susstrunk，“频率调谐显著区域检测”，见于*IEEE计算机视觉与模式识别会议（CVPR）*，2009年，第1597–1604页。'
- en: '[170] F. Perazzi, P. Krähenbühl, Y. Pritch, and A. Hornung, “Saliency filters:
    Contrast based filtering for salient region detection,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2012, pp.
    733–740.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] F. Perazzi, P. Krähenbühl, Y. Pritch, 和 A. Hornung，“显著性滤波器：基于对比度的显著区域检测”，见于*IEEE计算机视觉与模式识别会议（CVPR）*，2012年，第733–740页。'
- en: '[171] M. Ran, Z.-M. Lihi, and T. Ayellet, “How to evaluate foreground maps?”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2014, pp. 248–255.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] M. Ran, Z.-M. Lihi, 和 T. Ayellet，“如何评估前景图？”见于*IEEE计算机视觉与模式识别会议（CVPR）*，2014年，第248–255页。'
- en: '[172] D.-P. Fan, M.-M. Cheng, Y. Liu, T. Li, and A. Borji, “Structure-measure:
    A new way to evaluate foreground maps.” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2017, pp. 4548–4557.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] D.-P. Fan, M.-M. Cheng, Y. Liu, T. Li, 和 A. Borji, “结构测量：一种评估前景图的新方法。”
    见于*IEEE计算机视觉与模式识别会议（CVPR）论文集*，2017年，页4548–4557。'
- en: '[173] D.-P. Fan, C. Gong, Y. Cao, B. Ren, M.-M. Cheng, and A. Borji, “Enhanced-alignment
    measure for binary foreground map evaluation,” in *Proceedings of the 27th International
    Joint Conference on Artificial Intelligence (IJCAI)*, 2018, pp. 698–704.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] D.-P. Fan, C. Gong, Y. Cao, B. Ren, M.-M. Cheng, 和 A. Borji, “用于二值前景图评估的增强对齐度量，”
    见于*第27届国际人工智能联合大会（IJCAI）论文集*，2018年，页698–704。'
- en: '[174] X. Zhao, H. Jia, Y. Pang, L. Lv, F. Tian, L. Zhang, W. Sun, and H. Lu,
    “M2snet: Multi-scale in multi-scale subtraction network for medical image segmentation,”
    *arXiv preprint arXiv:2303.10894*, 2023.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] X. Zhao, H. Jia, Y. Pang, L. Lv, F. Tian, L. Zhang, W. Sun, 和 H. Lu,
    “M2snet：多尺度减法网络用于医学图像分割，” *arXiv预印本 arXiv:2303.10894*，2023年。'
- en: '[175] Y. Jin, Y. Hu, Z. Jiang, and Q. Zheng, “Polyp segmentation with convolutional
    mlp,” *The Visual Computer*, vol. 39, no. 10, pp. 4819–4837, 2023.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Y. Jin, Y. Hu, Z. Jiang, 和 Q. Zheng, “使用卷积MLP的息肉分割，” *视觉计算*，第39卷，第10期，页4819–4837，2023年。'
- en: '[176] J. Song, X. Chen, Q. Zhu, F. Shi, D. Xiang, Z. Chen, Y. Fan, L. Pan,
    and W. Zhu, “Global and local feature reconstruction for medical image segmentation,”
    *IEEE Transactions on Medical Imaging (TMI)*, vol. 41, no. 9, pp. 2273–2284, 2022.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] J. Song, X. Chen, Q. Zhu, F. Shi, D. Xiang, Z. Chen, Y. Fan, L. Pan,
    和 W. Zhu, “医学图像分割的全局和局部特征重建，” *IEEE医学影像学汇刊（TMI）*，第41卷，第9期，页2273–2284，2022年。'
- en: '[177] A. Lou, S. Guan, H. Ko, and M. H. Loew, “Caranet: Context axial reverse
    attention network for segmentation of small medical objects,” in *SPIE Medical
    Imaging: Image Processing*, vol. 12032, 2022, pp. 81–92.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] A. Lou, S. Guan, H. Ko, 和 M. H. Loew, “Caranet：上下文轴向逆注意力网络用于小型医学物体分割，”
    见于*SPIE医学成像：图像处理*，第12032卷，2022年，页81–92。'
- en: '[178] M. Wang, X. An, Y. Li, N. Li, W. Hang, and G. Liu, “Ems-net: Enhanced
    multi-scale network for polyp segmentation,” in *Proceedings of the IEEE Engineering
    in Medicine and Biology Society (EMBC)*, 2021, pp. 2936–2939.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] M. Wang, X. An, Y. Li, N. Li, W. Hang, 和 G. Liu, “Ems-net：增强的多尺度网络用于息肉分割，”
    见于*IEEE医学与生物工程学会（EMBC）会议论文集*，2021年，页2936–2939。'
- en: '[179] C.-H. Huang, H.-Y. Wu, and Y.-L. Lin, “Hardnet-mseg: A simple encoder-decoder
    polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps,”
    *arXiv e-prints*, pp. arXiv–2101, 2021.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] C.-H. Huang, H.-Y. Wu, 和 Y.-L. Lin, “Hardnet-mseg：一种简单的编码器-解码器息肉分割神经网络，达到了超过0.9的平均Dice和86
    fps，” *arXiv电子预印本*，页arXiv–2101，2021年。'
- en: '[180] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: Redesigning
    skip connections to exploit multiscale features in image segmentation,” *IEEE
    Transactions on Medical Imaging (TMI)*, vol. 39, no. 6, pp. 1856–1867, 2019.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, 和 J. Liang, “Unet++：重新设计跳跃连接以利用图像分割中的多尺度特征，”
    *IEEE医学影像学汇刊（TMI）*，第39卷，第6期，页1856–1867，2019年。'
- en: '[181] X. Lu, W. Wang, C. Ma, J. Shen, L. Shao, and F. Porikli, “See more, know
    more: Unsupervised video object segmentation with co-attention siamese networks,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2019, pp. 3623–3632.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] X. Lu, W. Wang, C. Ma, J. Shen, L. Shao, 和 F. Porikli, “看得更多，知道更多：使用协注意力孪生网络进行无监督视频物体分割，”
    见于*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*，2019年，页3623–3632。'
- en: '[182] T. Zhou, J. Li, S. Wang, R. Tao, and J. Shen, “Matnet: Motion-attentive
    transition network for zero-shot video object segmentation,” *IEEE Transactions
    on Image Processing (TIP)*, vol. 29, pp. 8326–8338, 2020.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] T. Zhou, J. Li, S. Wang, R. Tao, 和 J. Shen, “Matnet：运动注意过渡网络用于零样本视频物体分割，”
    *IEEE图像处理汇刊（TIP）*，第29卷，页8326–8338，2020年。'
- en: '[183] Y. Gu, L. Wang, Z. Wang, Y. Liu, M.-M. Cheng, and S.-P. Lu, “Pyramid
    constrained self-attention network for fast video salient object detection,” in
    *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, vol. 34,
    no. 07, 2020, pp. 10 869–10 876.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Y. Gu, L. Wang, Z. Wang, Y. Liu, M.-M. Cheng, 和 S.-P. Lu, “用于快速视频显著物体检测的金字塔约束自注意力网络，”
    见于*AAAI人工智能会议论文集（AAAI）*，第34卷，第07期，2020年，页10 869–10 876。'
- en: '[184] R. Liu, Z. Wu, S. Yu, and S. Lin, “The emergence of objectness: Learning
    zero-shot segmentation from videos,” *Proceedings of the Advances in Neural Information
    Processing Systems (NeurIPS)*, vol. 34, pp. 13 137–13 152, 2021.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] R. Liu, Z. Wu, S. Yu, 和 S. Lin, “对象性的出现：从视频中学习零样本分割，” *神经信息处理系统进展（NeurIPS）论文集*，第34卷，页13 137–13 152，2021年。'
- en: '[185] M. Zhang, J. Liu, Y. Wang, Y. Piao, S. Yao, W. Ji, J. Li, H. Lu, and
    Z. Luo, “Dynamic context-sensitive filtering network for video salient object
    detection,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision (ICCV)*, 2021, pp. 1553–1563.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] M. Zhang, J. Liu, Y. Wang, Y. Piao, S. Yao, W. Ji, J. Li, H. Lu 和 Z. Luo，"用于视频显著目标检测的动态上下文敏感过滤网络"，收录于
    *IEEE/CVF 国际计算机视觉大会 (ICCV)* 论文集，2021 年，第 1553–1563 页。'
- en: '[186] G.-P. Ji, K. Fu, Z. Wu, D.-P. Fan, J. Shen, and L. Shao, “Full-duplex
    strategy for video object segmentation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*, 2021, pp. 4922–4933.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] G.-P. Ji, K. Fu, Z. Wu, D.-P. Fan, J. Shen 和 L. Shao，"视频对象分割的全双工策略"，收录于
    *IEEE/CVF 国际计算机视觉大会 (ICCV)* 论文集，2021 年，第 4922–4933 页。'
- en: '[187] J. G.-B. Puyal, K. K. Bhatia, P. Brandao, O. F. Ahmad, D. Toth, R. Kader,
    L. Lovat, P. Mountney, and D. Stoyanov, “Endoscopic polyp segmentation using a
    hybrid 2d/3d cnn,” in *Proceedings of the International Conference on Medical
    Image Computing and Computer Assisted Intervention (MICCAI)*, 2020, pp. 295–305.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] J. G.-B. Puyal, K. K. Bhatia, P. Brandao, O. F. Ahmad, D. Toth, R. Kader,
    L. Lovat, P. Mountney 和 D. Stoyanov，"使用混合 2D/3D CNN 的内窥镜息肉分割"，收录于 *国际医学图像计算与计算机辅助干预大会
    (MICCAI)* 论文集，2020 年，第 295–305 页。'
- en: '[188] M. Misawa, S.-e. Kudo, Y. Mori, K. Hotta, K. Ohtsuka, T. Matsuda, S. Saito,
    T. Kudo, T. Baba, F. Ishida *et al.*, “Development of a computer-aided detection
    system for colonoscopy and a publicly accessible large colonoscopy video database
    (with video),” *Gastrointestinal Endoscopy*, vol. 93, no. 4, pp. 960–967, 2021.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] M. Misawa, S.-e. Kudo, Y. Mori, K. Hotta, K. Ohtsuka, T. Matsuda, S. Saito,
    T. Kudo, T. Baba, F. Ishida *等*，"用于结肠镜检查的计算机辅助检测系统的开发及公开的大型结肠镜视频数据库（含视频）"，*胃肠内窥镜*，第
    93 卷，第 4 期，第 960–967 页，2021 年。'
- en: '[189] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
    K. Bonawitz, Z. Charles, G. Cormode, R. Cummings *et al.*, “Advances and open
    problems in federated learning,” *Foundations and Trends® in Machine Learning
    (FTML)*, vol. 14, no. 1–2, pp. 1–210, 2021.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
    K. Bonawitz, Z. Charles, G. Cormode, R. Cummings *等*，"联邦学习中的进展与开放问题"，*机器学习基础与趋势
    (FTML)*，第 14 卷，第 1–2 期，第 1–210 页，2021 年。'
- en: '[190] S. Hong and J. Chae, “Communication-efficient randomized algorithm for
    multi-kernel online federated learning,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence (TPAMI)*, vol. 44, no. 12, pp. 9872–9886, 2021.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] S. Hong 和 J. Chae，"用于多核在线联邦学习的通信效率随机算法"，*IEEE 计算机学会模式分析与机器智能汇刊 (TPAMI)*，第
    44 卷，第 12 期，第 9872–9886 页，2021 年。'
- en: '[191] S. Zhou and G. Y. Li, “Federated learning via inexact admm,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence (TPAMI)*, vol. 45, no. 8, pp. 9699–9708,
    2023.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] S. Zhou 和 G. Y. Li，"通过不精确的 ADMM 进行联邦学习"，*IEEE 计算机学会模式分析与机器智能汇刊 (TPAMI)*，第
    45 卷，第 8 期，第 9699–9708 页，2023 年。'
- en: '[192] L. Liu, X. Jiang, F. Zheng, H. Chen, G.-J. Qi, H. Huang, and L. Shao,
    “A bayesian federated learning framework with online laplace approximation,” *IEEE
    Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, pp. 1–16,
    2023.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] L. Liu, X. Jiang, F. Zheng, H. Chen, G.-J. Qi, H. Huang 和 L. Shao，"具有在线拉普拉斯近似的贝叶斯联邦学习框架"，*IEEE
    计算机学会模式分析与机器智能汇刊 (TPAMI)*，第 1–16 页，2023 年。'
- en: '[193] W. M. Kouw and M. Loog, “A review of domain adaptation without target
    labels,” *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*,
    vol. 43, no. 3, pp. 766–785, 2019.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] W. M. Kouw 和 M. Loog，"无目标标签的领域适应综述"，*IEEE 计算机学会模式分析与机器智能汇刊 (TPAMI)*，第
    43 卷，第 3 期，第 766–785 页，2019 年。'
- en: '[194] J. Dong, Y. Cong, G. Sun, Z. Fang, and Z. Ding, “Where and how to transfer:
    knowledge aggregation-induced transferability perception for unsupervised domain
    adaptation,” *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*,
    2021.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] J. Dong, Y. Cong, G. Sun, Z. Fang 和 Z. Ding，"知识聚合诱导的迁移感知：无监督领域适应的转移位置与方式"，*IEEE
    计算机学会模式分析与机器智能汇刊 (TPAMI)*，2021 年。'
- en: '[195] P. Oza, V. A. Sindagi, V. V. Sharmini, and V. M. Patel, “Unsupervised
    domain adaptation of object detectors: A survey,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence (TPAMI)*, 2023.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] P. Oza, V. A. Sindagi, V. V. Sharmini 和 V. M. Patel，"对象检测器的无监督领域适应：综述"，*IEEE
    计算机学会模式分析与机器智能汇刊 (TPAMI)*，2023 年。'
- en: '[196] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” in *Proceedings of
    the International Conference on Learning Representations (ICLR)*, 2014.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow
    和 R. Fergus，"神经网络的有趣属性"，收录于 *国际学习表征大会 (ICLR)* 论文集，2014 年。'
- en: '[197] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep
    neural networks,” *IEEE Transactions on Evolutionary Computation (TEC)*, vol. 23,
    no. 5, pp. 828–841, 2019.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] J. Su, D. V. Vargas 和 K. Sakurai，“一种用于欺骗深度神经网络的单像素攻击，” *IEEE演化计算学报（TEC）*，第23卷，第5期，第828–841页，2019年。'
- en: '[198] I. Misra and L. v. d. Maaten, “Self-supervised learning of pretext-invariant
    representations,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2020, pp. 6707–6717.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] I. Misra 和 L. v. d. Maaten，“自监督学习预文本不变表示，” 载于 *IEEE/CVF计算机视觉与模式识别大会（CVPR）论文集*，2020年，第6707–6717页。'
- en: '[199] P. Dollar, M. Singh, and R. Girshick, “Fast and accurate model scaling,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2021, pp. 924–932.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] P. Dollar, M. Singh 和 R. Girshick，“快速准确的模型缩放，” 载于 *IEEE/CVF计算机视觉与模式识别大会（CVPR）论文集*，2021年6月，第924–932页。'
- en: '[200] T. Choudhary, V. Mishra, A. Goswami, and J. Sarangapani, “A comprehensive
    survey on model compression and acceleration,” *Artificial Intelligence Review*,
    vol. 53, pp. 5113–5155, 2020.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] T. Choudhary, V. Mishra, A. Goswami 和 J. Sarangapani，“关于模型压缩与加速的综合调查，”
    *人工智能评论*，第53卷，第5113–5155页，2020年。'
- en: '[201] T. Li, J. Li, Z. Liu, and C. Zhang, “Few sample knowledge distillation
    for efficient network compression,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2020, pp. 14 639–14 647.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] T. Li, J. Li, Z. Liu 和 C. Zhang，“少样本知识蒸馏用于高效网络压缩，” 载于 *IEEE/CVF计算机视觉与模式识别大会（CVPR）论文集*，2020年，第14 639–14 647页。'
- en: '[202] X. Xia, X. Pan, N. Li, X. He, L. Ma, X. Zhang, and N. Ding, “Gan-based
    anomaly detection: A review,” *Neurocomputing*, vol. 493, pp. 497–535, 2022.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] X. Xia, X. Pan, N. Li, X. He, L. Ma, X. Zhang 和 N. Ding，“基于GAN的异常检测：综述，”
    *神经计算*，第493卷，第497–535页，2022年。'
- en: '[203] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo *et al.*, “Segment anything,” *arXiv preprint
    arXiv:2304.02643*, 2023.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T.
    Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo *等*，“分割任意物体，” *arXiv 预印本 arXiv:2304.02643*，2023年。'
- en: '[204] L. Tang, H. Xiao, and B. Li, “Can sam segment anything? when sam meets
    camouflaged object detection,” *arXiv preprint arXiv:2304.04709*, 2023.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] L. Tang, H. Xiao 和 B. Li，“sam 能分割任何东西吗？当 sam 遇到伪装物体检测时，” *arXiv 预印本 arXiv:2304.04709*，2023年。'
- en: '[205] M. Hu, Y. Li, and X. Yang, “Skinsam: Empowering skin cancer segmentation
    with segment anything model,” *arXiv preprint arXiv:2304.13973*, 2023.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] M. Hu, Y. Li 和 X. Yang，“Skinsam：利用分割任意模型增强皮肤癌分割，” *arXiv 预印本 arXiv:2304.13973*，2023年。'
- en: '[206] T. Yu, R. Feng, R. Feng, J. Liu, X. Jin, W. Zeng, and Z. Chen, “Inpaint
    anything: Segment anything meets image inpainting,” *arXiv preprint arXiv:2304.06790*,
    2023.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] T. Yu, R. Feng, R. Feng, J. Liu, X. Jin, W. Zeng 和 Z. Chen，“修补任何物体：分割任何物体与图像修补相遇，”
    *arXiv 预印本 arXiv:2304.06790*，2023年。'
- en: '[207] T. Zhou, Y. Zhang, Y. Zhou, Y. Wu, and C. Gong, “Can sam segment polyps?”
    *arXiv preprint arXiv:2304.07583*, 2023.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] T. Zhou, Y. Zhang, Y. Zhou, Y. Wu 和 C. Gong，“sam 能分割息肉吗？” *arXiv 预印本
    arXiv:2304.07583*，2023年。'
- en: '[208] S. Menon and C. Vondrick, “Visual classification via description from
    large language models,” in *Proceedings of the International Conference on Learning
    Representations (ICLR)*, 2022.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] S. Menon 和 C. Vondrick，“通过大型语言模型的描述进行视觉分类，” 载于 *国际学习表征会议（ICLR）论文集*，2022年。'
- en: '[209] S. Parisot, Y. Yang, and S. McDonagh, “Learning to name classes for vision
    and language models,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2023, pp. 23 477–23 486.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] S. Parisot, Y. Yang 和 S. McDonagh，“为视觉和语言模型学习命名类别，” 载于 *IEEE/CVF计算机视觉与模式识别大会（CVPR）论文集*，2023年6月，第23 477–23 486页。'
