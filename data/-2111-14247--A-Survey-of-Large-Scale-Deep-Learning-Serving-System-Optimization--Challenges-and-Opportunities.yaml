- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:49:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:49:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2111.14247] A Survey of Large-Scale Deep Learning Serving System Optimization:
    Challenges and Opportunities'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2111.14247] 大规模深度学习服务系统优化调查：挑战与机遇'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2111.14247](https://ar5iv.labs.arxiv.org/html/2111.14247)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2111.14247](https://ar5iv.labs.arxiv.org/html/2111.14247)
- en: 'A Survey of Large-Scale Deep Learning Serving System Optimization: Challenges
    and Opportunities'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模深度学习服务系统优化调查：挑战与机遇
- en: Fuxun Yu^†, Di Wang^‡, Longfei Shangguan^‡, Minjia Zhang^‡, Xulong Tang^∗, Chenchen
    Liu^§, Xiang Chen^† ^†George Mason University, ^†Microsoft, ^†University of Pittsburgh,
    ^§University of Maryland, Baltimore County
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Fuxun Yu^†, Di Wang^‡, Longfei Shangguan^‡, Minjia Zhang^‡, Xulong Tang^∗, Chenchen
    Liu^§, Xiang Chen^† ^†乔治·梅森大学, ^†微软, ^†匹兹堡大学, ^§马里兰大学巴尔的摩分校
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep Learning (DL) models have achieved superior performance in many application
    domains, including vision, language, medical, commercial ads, entertainment, etc.
    With the fast development, both DL applications and the underlying serving hardware
    have demonstrated strong scaling trends, i.e., Model Scaling and Compute Scaling,
    for example, the recent pre-trained model with hundreds of billions of parameters
    with $\sim$TB level memory consumption, as well as the newest GPU accelerators
    providing hundreds of TFLOPS. With both scaling trends, new problems and challenges
    emerge in DL inference serving systems, which gradually trends towards Large-scale
    Deep learning Serving system (LDS). This survey aims to summarize and categorize
    the emerging challenges and optimization opportunities for large-scale deep learning
    serving systems. By providing a novel taxonomy, summarizing the computing paradigms,
    and elaborating the recent technique advances, we hope that this survey could
    shed lights on new optimization perspectives and motivate novel works in large-scale
    deep learning system optimization.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）模型在许多应用领域中取得了卓越的性能，包括视觉、语言、医疗、商业广告、娱乐等。随着快速发展，DL 应用和底层服务硬件都表现出强劲的扩展趋势，即模型扩展和计算扩展。例如，最近的预训练模型具有数百亿参数，内存消耗达到
    $\sim$TB 级别，以及最新的 GPU 加速器提供数百 TFLOPS。伴随这些扩展趋势，DL 推理服务系统中出现了新问题和挑战，逐渐趋向于大规模深度学习服务系统（LDS）。本次调查旨在总结和分类大规模深度学习服务系统的新兴挑战和优化机会。通过提供一种新颖的分类法，总结计算范式，并阐述最近的技术进展，我们希望这项调查能够为大规模深度学习系统优化提供新的视角，并激发创新工作。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Computing Methodologies, Artificial Intelligence, Hardware Description Languages
    and Compilation, Computer Systems Organization, Parallel Architectures.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 计算方法、人工智能、硬件描述语言与编译、计算机系统组织、并行架构。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Deep Learning (DL) models, such as CNNs [[16](#bib.bib16), [37](#bib.bib37),
    [46](#bib.bib46)], transformers [[11](#bib.bib11), [8](#bib.bib8), [3](#bib.bib3),
    [30](#bib.bib30)] and recommendation models [[32](#bib.bib32), [42](#bib.bib42)]
    have achieved superior performance in many cognitive tasks like vision, speech
    and language applications, which poses potentials in numerous areas like medical
    image analysis [[39](#bib.bib39)], photo styling [[35](#bib.bib35)], machine translation [[41](#bib.bib41)],
    product recommendation [[32](#bib.bib32)], customized advertising [[14](#bib.bib14)],
    game playing [[22](#bib.bib22)], *etc.* Such widespread DL applications bring
    great market values and lead to significant DL serving traffics. For example,
    FB has 1.82 billions of daily active users [[12](#bib.bib12)]. The number of advertising
    recommendation queries can reach 10M queries per second. The huge growth in consumer-generated
    data and use of DL services have also propelled the demand for AI-centric data
    centers (such as Amazon AWS [[28](#bib.bib28)] and Microsoft Azure [[7](#bib.bib7)])
    and the increased adoption of powerful DL accelerators like GPUs. According to
    the report [[36](#bib.bib36)], GPU has accounted for the major share of 85% with
    2983M USD in the global data center accelerator market in 2018\. And this product
    segment is poised to reach 29819M USD by 2025 [[36](#bib.bib36)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）模型，如CNNs [[16](#bib.bib16), [37](#bib.bib37), [46](#bib.bib46)]、transformers
    [[11](#bib.bib11), [8](#bib.bib8), [3](#bib.bib3), [30](#bib.bib30)] 和推荐模型 [[32](#bib.bib32),
    [42](#bib.bib42)] 在视觉、语音和语言应用等认知任务中表现出色，这为医疗图像分析 [[39](#bib.bib39)]、照片风格化 [[35](#bib.bib35)]、机器翻译
    [[41](#bib.bib41)]、产品推荐 [[32](#bib.bib32)]、定制广告 [[14](#bib.bib14)] 和游戏 [[22](#bib.bib22)]
    等众多领域带来了潜力。如此广泛的DL应用带来了巨大的市场价值，并导致了显著的DL服务流量。例如，FB每天有18.2亿活跃用户 [[12](#bib.bib12)]。广告推荐查询的数量可以达到每秒1000万次。消费者生成的数据大幅增长和DL服务的使用也推动了对AI中心数据中心（如Amazon
    AWS [[28](#bib.bib28)] 和 Microsoft Azure [[7](#bib.bib7)])的需求增加，以及对强大DL加速器如GPU的广泛采用。根据报告
    [[36](#bib.bib36)]，2018年GPU在全球数据中心加速器市场中占据了85%的主要份额，金额为29.83亿美元。预计到2025年，该产品细分市场将达到298.19亿美元
    [[36](#bib.bib36)]。
- en: 'With the ever increasing market demands, DL applications and the underlying
    serving hardware have demonstrate strong scaling trends in terms of Computing
    Scaling (*e.g.*, increased computing parallelism, memory and storage to serve
    larger models) and Model Scaling (*e.g.*, higher structure complexity, computing
    workload, parameter size for better accuracy), which greatly complicates the serving
    system management and optimization. On the one hand, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey of Large-Scale Deep Learning Serving System
    Optimization: Challenges and Opportunities") (a), with the Computing Scaling trend,
    GPU with massive computing parallelism has become one of the major types of DL
    computing accelerators in recent data centers and maintains continuously exponential
    performance scaling. Recent GPUs such as NVIDIA Tesla V100 offer 130 Tera floating
    point operations per second (TFLOPS), and 900 GB/s memory bandwidth, and these
    numbers further increase to 312 TFLOPS and 1.6TB/s memory bandwidth, which can
    serve tens of DL models such as ResNet50 [[16](#bib.bib16)] simultaneously and
    provide higher efficiency (Perf/Watt). On the other hand, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey of Large-Scale Deep Learning Serving System
    Optimization: Challenges and Opportunities") (b), Model Scaling has been show
    to be one of the most important factor in achieving better accuracy, the effectiveness
    of which is consistently shown in practice by industrial ultra-large models in
    all domains, such as vision model BiT [[23](#bib.bib23)], NLP model BERT [[8](#bib.bib8)],
    GPT3 [[3](#bib.bib3)] and deep learning recommendation model DLRM [[32](#bib.bib32)].
    For example, recent ultra-large model MT-NLG [[30](#bib.bib30)] has achieved 530
    billions of parameters. Industrial-level commercial DLRMs [[32](#bib.bib32)] have
    reached $\sim$TB model size, which significantly surpass single-machine memory
    capability and require multiple devices for collaborative computing.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着市场需求的不断增加，深度学习应用和底层服务硬件在计算规模化（*例如*，增加计算并行性、内存和存储以支持更大模型）和模型规模化（*例如*，更高的结构复杂性、计算负载、参数规模以提高准确性）方面展现出强烈的扩展趋势，这极大地增加了服务系统管理和优化的复杂性。一方面，如图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 大规模深度学习服务系统优化调查：挑战与机遇")（a）所示，随着计算规模化趋势的出现，具备大规模计算并行性的GPU已成为近期数据中心主要的深度学习计算加速器之一，并保持着持续的指数级性能扩展。最近的GPU，例如NVIDIA
    Tesla V100，提供130万亿次浮点运算每秒（TFLOPS）和900 GB/s的内存带宽，这些数字进一步增加到312 TFLOPS和1.6TB/s的内存带宽，可以同时服务于如ResNet50 [[16](#bib.bib16)]等数十个深度学习模型，并提供更高的效率（Perf/Watt）。另一方面，如图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 大规模深度学习服务系统优化调查：挑战与机遇")（b）所示，模型规模化被证明是实现更高准确性的一个重要因素，这在各个领域的工业超大模型中得到了实践验证，例如视觉模型BiT [[23](#bib.bib23)]、NLP模型BERT [[8](#bib.bib8)]、GPT3 [[3](#bib.bib3)]和深度学习推荐模型DLRM [[32](#bib.bib32)]。例如，最近的超大模型MT-NLG [[30](#bib.bib30)]已达到5300亿个参数。工业级商业DLRMs [[32](#bib.bib32)]的模型大小已达到$\sim$TB，这显著超出了单机内存能力，需要多个设备进行协同计算。
- en: '![Refer to caption](img/74bc2ce7e62a55f27773df38c8749790.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/74bc2ce7e62a55f27773df38c8749790.png)'
- en: 'Figure 1: Deep Learning Model Scaling and Computing Scaling [[45](#bib.bib45)].
    Such exponential scaling trends on both sides leads to Large-scale DL Systems.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：深度学习模型规模化和计算规模化 [[45](#bib.bib45)]。这种在两侧的指数级扩展趋势导致了大规模深度学习系统的出现。
- en: 'Within such contexts, we observe the current DL system community still lacks
    enough awareness and attention to such Large-scale Deep Learning Systems (LDS),
    overlooking the emerged challenges and opportunities: Traditional DL system optimization
    usually focuses on the single-model single-machine inference setting (i.e., one-to-one
    mapping). However, LDS with bigger DL models and more powerful hardwares enable
    more flexible inference computing, bringing the many-instance to one-device, one-instance
    to many-device, and even many-instance to many-device mapping into practice. For
    example, computing scaling (such as GPUs, TPUs) motivates many research works
    into multi-model inference on one single device, *e.g.*, splitting one GPU into
    multiple containerized vGPUs or Multi-Instance GPUs (MIG) for better hardware
    utilization, higher serving throughput and cost efficiency. Considering practical
    cost management (*e.g.*, total cost of ownership, TCO), data centers serving massive
    inference queries also tend to migrate to multi-tenant inference serving, *e.g.*,
    co-locating multiple inference queries on the same device, which incurs new optimization
    objectives (*e.g.*, total served queries per second, QPS) and constraints (*e.g.*,
    service-level agreement, SLA) from traditional single-tenant inference. Similarly,
    the model scaling also poses requests for new one-to-many inference scenarios.
    The recent ultra-large model (*e.g.*, DLRMs) incurs huge memory cost ($\sim$TB
    without quantization) during inference, which requires new collaborative computing
    paradigms such as heterogeneous computing or distributed inference. Such collaboratively
    serving involves remote process calls (RPCs) and low-bandwidth communications,
    which brings dramatically different bottlenecks from traditional single-device
    inference. With all above scenarios involved, modern data centers face more sophisticated
    many-to-many scenarios and require dedicated inference query scheduling, such
    as service router and compute device management, for better serving performance
    like latency, throughput and cost, *etc.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，我们观察到当前的深度学习系统社区对大规模深度学习系统（LDS）仍然缺乏足够的认识和关注，忽视了出现的挑战和机会：传统的深度学习系统优化通常关注于单模型单机器推理设置（即一对一映射）。然而，具有更大深度学习模型和更强硬件的LDS使得推理计算更加灵活，将多实例到单设备、单实例到多设备，甚至多实例到多设备映射付诸实践。例如，计算扩展（如GPU、TPU）激励了许多研究工作将多个模型推理集中在一个单独的设备上，*例如*，将一个GPU分割成多个容器化的vGPUs或多实例GPU（MIG），以提高硬件利用率、服务吞吐量和成本效率。考虑到实际的成本管理（*例如*，总拥有成本，TCO），数据中心在处理大量推理查询时也倾向于迁移到多租户推理服务上，*例如*，将多个推理查询放在同一设备上，这引发了新的优化目标（*例如*，每秒服务查询量，QPS）和约束（*例如*，服务级别协议，SLA），不同于传统的单租户推理。同样，模型扩展也提出了对新的一对多推理场景的需求。近期的超大规模模型（*例如*，DLRMs）在推理过程中引发了巨大的内存成本（$\sim$TB未量化），这需要新的协作计算范式，如异构计算或分布式推理。这种协作服务涉及远程过程调用（RPCs）和低带宽通信，这与传统的单设备推理带来了截然不同的瓶颈。考虑到以上所有场景，现代数据中心面临更复杂的多对多场景，并需要专门的推理查询调度，如服务路由器和计算设备管理，以提高服务性能，如延迟、吞吐量和成本，*等等*。
- en: In this work, we propose a a novel computing paradigm taxonomy for ongoing LDS
    works, summarize the new optimization objectives, elaborate new technical design
    perspectives, and provide the insights for future LDS optimization.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种新颖的计算范式分类法用于当前LDS（大规模深度学习系统）工作，总结了新的优化目标，详细阐述了新的技术设计视角，并为未来的LDS优化提供了见解。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-to-Multi Computing Paradigm Characterized by the relations between DNN
    Instances (I) and Compute Devices (D), emerging LDS computing paradigms can be
    taxonomized into three new categories beyond Single Instance Single Device (SISD),
    that is, Multi Instance Single Device (MISD), Single Instance Multi Device (SIMD)
    and Multi Instance Multi Devices (MIMD), as shown in Figure [2](#S2.F2 "Figure
    2 ‣ 2 Large-Scale DL Serving System: A Novel Taxonomy ‣ A Survey of Large-Scale
    Deep Learning Serving System Optimization: Challenges and Opportunities").'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '以DNN实例（I）和计算设备（D）之间的关系为特征的多对多计算范式，出现的LDS计算范式可以被分类为超越单实例单设备（SISD）的三种新类别，即多实例单设备（MISD）、单实例多设备（SIMD）和多实例多设备（MIMD），如图[2](#S2.F2
    "Figure 2 ‣ 2 Large-Scale DL Serving System: A Novel Taxonomy ‣ A Survey of Large-Scale
    Deep Learning Serving System Optimization: Challenges and Opportunities")所示。'
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inference Serving Oriented Objectives Different from SISD that focuses on single-model
    performance, LDS works have different optimization goals, including inference
    latency, serving throughput, costs, scalability, quality of service, *etc.* For
    example, multi-tenant inference (MISD) targets at improving the serving throughput
    and power efficiency, while super-scale model inference serving aims to enhance
    hardware scalability with low costs.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推断服务导向的目标与关注单模型性能的SISD不同，LDS工作的优化目标包括推断延迟、服务吞吐量、成本、可扩展性、服务质量等。例如，多租户推断（MISD）旨在提高服务吞吐量和功率效率，而超大规模模型推断服务则旨在以低成本增强硬件可扩展性。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At-Scale Design and Technologies Due to the scale of inference serving, LDS
    works have also demonstrated many optimization challenges and opportunities within
    algorithm innovation, runtime scheduling and resource management. For example,
    multi-tenant inference optimization seeks for fine-grained hardware resource partitioning
    and job scheduling, *e.g.*, spatial/temporal sharing to provide QoS assurance.
    Distributed inference with slow communication bottlenecks requires dedicated model-hardware
    co-optimization, *e.g.*, efficient model sharding and balanced collaboration,
    *etc.*
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 按规模设计和技术 由于推断服务的规模，LDS工作还展示了许多优化挑战和机会，包括算法创新、运行时调度和资源管理。例如，多租户推断优化寻求细粒度的硬件资源分区和作业调度，例如，空间/时间共享以提供QoS保证。分布式推断中存在的通信瓶颈需要专门的模型-硬件共同优化，例如，高效的模型分片和均衡的协作等。
- en: 'By summarizing the existing works, we aim to provide a comprehensive survey
    on emerging challenges, opportunities, and innovations, and thus motivates new
    innovations in LDS operation and optimization. The rest of the survey is organized
    as follows: Section §[2](#S2 "2 Large-Scale DL Serving System: A Novel Taxonomy
    ‣ A Survey of Large-Scale Deep Learning Serving System Optimization: Challenges
    and Opportunities") presents the research preliminaries including our taxonomy
    for LDS and indicate the scope of this survey. Section §[3](#S3 "3 Computing Scaling:
    Trending to Multi-Instance Single-Device (MISD) ‣ A Survey of Large-Scale Deep
    Learning Serving System Optimization: Challenges and Opportunities") summarizes
    the challenges and recent works in multi-instance single-device (MISD) optimization;
    Section §[4](#S4 "4 Model Scaling: Trending to Single Instance Multi Device (SIMD)
    ‣ A Survey of Large-Scale Deep Learning Serving System Optimization: Challenges
    and Opportunities") summarizes the research works in single-instance multi-device
    (SIMD) optimization; Section §[5](#S5 "5 Conclusion ‣ A Survey of Large-Scale
    Deep Learning Serving System Optimization: Challenges and Opportunities") concludes
    this work.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过总结现有的研究，我们旨在提供关于新兴挑战、机会和创新的全面调研，从而激发对LDS操作和优化的新创新。本调研的其余部分组织如下：第§[2](#S2 "2
    大规模深度学习服务系统：一种新颖的分类法 ‣ 大规模深度学习服务系统优化：挑战与机会的调研")节介绍了研究的基础知识，包括我们的LDS分类法，并指出了本调研的范围。第§[3](#S3
    "3 计算扩展：趋向于多实例单设备 (MISD) ‣ 大规模深度学习服务系统优化：挑战与机会的调研")节总结了多实例单设备（MISD）优化中的挑战和近期工作；第§[4](#S4
    "4 模型扩展：趋向于单实例多设备 (SIMD) ‣ 大规模深度学习服务系统优化：挑战与机会的调研")节总结了单实例多设备（SIMD）优化中的研究工作；第§[5](#S5
    "5 结论 ‣ 大规模深度学习服务系统优化：挑战与机会的调研")节对这项工作进行了总结。
- en: '2 Large-Scale DL Serving System:'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 大规模深度学习服务系统：
- en: A Novel Taxonomy
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一种新颖的分类法
- en: '![Refer to caption](img/0a40d256889eb9b078c4715e49623457.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0a40d256889eb9b078c4715e49623457.png)'
- en: 'Figure 2: A Taxonomy of Deep Learning System from the relationship between
    DNN Instance (I) and Compute Device (D): Single-Instance Single-Device, SISD,
    Single-Instance Multi-Device, SIMD, Multi-Instance Single-Device, MISD and Multi-Instance
    Multi-Device, MIMD.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：深度学习系统的分类法，从DNN实例（I）与计算设备（D）之间的关系来看：单实例单设备（SISD）、单实例多设备（SIMD）、多实例单设备（MISD）和多实例多设备（MIMD）。
- en: 'Taxonomy Overview We first give an high-level overview of LDS optimization
    taxonomy. Specifically, we use a taxonomy shown in Figure [2](#S2.F2 "Figure 2
    ‣ 2 Large-Scale DL Serving System: A Novel Taxonomy ‣ A Survey of Large-Scale
    Deep Learning Serving System Optimization: Challenges and Opportunities") to demonstrate
    such differences between traditional DL system optimizations and new emerging
    challenges in LDS. We use Instance (I) to denote one DNN model and Device (D)
    to denote the underlying serving compute hardware. Traditional DL system optimizations,
    though incorporating a thorough full stack (*e.g.*, with model-, graph-, runtime-
    and kernel-optimization levels), usually comes within the Single Instance Single
    Device (SISD) assumption. Therefore, most existing works only constitute the top-left
    quarter in the full-spectrum of DLS, neglecting the Single Instance Multiple Devices
    (SIMD), Multiple Instances Single Device (MISD), and even Multiple Instances Multiple
    Devices (MIMD) optimizations.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 分类概述 我们首先提供一个 LDS 优化分类的高层次概述。具体来说，我们使用图 [2](#S2.F2 "图 2 ‣ 2 大规模深度学习服务系统：一种新颖的分类
    ‣ 大规模深度学习服务系统优化：挑战与机遇") 中所示的分类来展示传统深度学习系统优化与 LDS 中新兴挑战之间的差异。我们用实例（I）来表示一个 DNN
    模型，用设备（D）来表示底层服务计算硬件。传统的深度学习系统优化，尽管涵盖了完整的堆栈（*例如*，模型级、图级、运行时级和内核级优化），通常在单实例单设备（SISD）假设下进行。因此，大多数现有工作只涵盖了
    DLS 全谱中的左上四分之一，忽视了单实例多个设备（SIMD）、多个实例单设备（MISD），甚至多个实例多个设备（MIMD）优化。
- en: $\bullet$ Single Instance Single Device (SISD) SISD optimization improves one
    model’s end-to-end performance (such as latency) on the targeted hardware device,
    such as CPU, GPU, FPGA, *etc.* Conventional SISD optimizations have studied the
    full stacks thoroughly, including the algorithm-level NN design [[38](#bib.bib38),
    [55](#bib.bib55), [17](#bib.bib17)], pruning and NAS works [[47](#bib.bib47),
    [25](#bib.bib25), [24](#bib.bib24)], as well as the compiler-level optimization
    works [[4](#bib.bib4), [21](#bib.bib21), [13](#bib.bib13)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 单实例单设备（SISD）SISD 优化提高了单个模型在目标硬件设备（如 CPU、GPU、FPGA 等）上的端到端性能（例如延迟）。传统的
    SISD 优化已彻底研究了整个堆栈，包括算法级的神经网络设计 [[38](#bib.bib38)、[55](#bib.bib55)、[17](#bib.bib17)]、剪枝和
    NAS 工作 [[47](#bib.bib47)、[25](#bib.bib25)、[24](#bib.bib24)]，以及编译器级的优化工作 [[4](#bib.bib4)、[21](#bib.bib21)、[13](#bib.bib13)]。
- en: Previous DL optimization is dominant by SISD works. For example, the popular
    DL compiler frameworks (*e.g.*, TVM [[4](#bib.bib4)], Ansor [[57](#bib.bib57)],
    TASO [[21](#bib.bib21)], *etc.*) tune the low-level computing primitives to yield
    high-performance kernels for the given underlying hardware. However, with the
    fast evolving of DL models and AI hardwares, more applications and potentials
    come up in terms of MISD and SIMD optimization domains. Although demonstrating
    optimal performance for SISD serving, these techniques are usually ill-suited
    for new scenarios multiple device co-serving one model (SIMD) or multi-model co-running
    on one device (MISD), which have distinct LDS-incurred computing challenges and
    opportunities.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的深度学习优化主要以 SISD 工作为主。例如，流行的深度学习编译框架（*例如*，TVM [[4](#bib.bib4)]、Ansor [[57](#bib.bib57)]、TASO
    [[21](#bib.bib21)]、*等*）通过调整低级计算原语来为给定的底层硬件生成高性能的内核。然而，随着深度学习模型和人工智能硬件的快速发展，在 MISD
    和 SIMD 优化领域出现了更多应用和潜力。尽管这些技术在 SISD 服务中展示了最佳性能，但通常不适合新的场景，比如多个设备共同服务一个模型（SIMD）或多个模型在一个设备上共同运行（MISD），这些情况面临不同的
    LDS 计算挑战和机遇。
- en: $\bullet$ Multi Instance Single Device (MISD) MISD optimization targets at improving
    the serving throughput (such as serving query per second, QPS) by co-locating
    multiple DNN instances on one high-performance hardware (i.e., multi-tenant inference).
    The MISD optimization is raised mainly due to the compute scaling, i.e., the tremendous
    computing performance of recent GPUs (like V100/A100s with $\sim$130TFLOPs/s)
    overwhelms the general DNNs’ inference requirement (*e.g.*, ResNet50 with $\sim$4GFLOPS).
    Therefore, due to the practical cost consideration (*e.g.*, cost of hardware,
    power consumption), data center-level serving also tends to adopt such a multi-tenant
    serving paradigm to effectively reduce the total cost of ownership (TCO).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 多实例单设备（MISD）MISD优化的目标是通过将多个DNN实例部署在一个高性能硬件上（即多租户推理）来提高服务吞吐量（例如每秒服务查询数，QPS）。MISD优化的提出主要是由于计算规模的增加，即最近的GPU（如V100/A100s，具有$\sim$130TFLOPs/s）的巨大计算性能超出了普通DNN的推理需求（*例如*，ResNet50具有$\sim$4GFLOPS）。因此，出于实际成本考虑（*例如*，硬件成本，功耗），数据中心级别的服务也趋向于采用这种多租户服务范式，以有效降低总拥有成本（TCO）。
- en: $\bullet$ Single Instance Multi Device (SIMD) SIMD optimization, to the contrary
    of MISD, is mainly raised by the model scaling trend. The ultra-scale model size,
    especially in language [[8](#bib.bib8), [3](#bib.bib3)] and recommendation [[32](#bib.bib32)]
    domains, has shown to be an important factor in improving the model accuracy.
    As a result, recent state-of-the-art industrial models have achieved the volume
    of hundreds of billions parameters [[30](#bib.bib30)] and the model inference
    can take $\sim$TB space in host memory [[32](#bib.bib32)], which is nearly impossible
    to be served on a single host. In such scenario, distributed inference by multiple
    devices becomes the only solution for such ultra-large model inference [[27](#bib.bib27)].
    Nevertheless, as slow inter-device communication channels are involved (*e.g.*,
    remote process calls, RPCs), distributed inference requires both algorithm and
    scheduling innovations to achieve inference efficiency. For example, we need to
    consider both efficient model sharding in the algorithm side such as model/pipeline
    parallelism [[44](#bib.bib44), [18](#bib.bib18)]¹¹1Although such model/pipeline
    parallelism is mostly used for large model training, recent ultra-large model
    inference also requires such model sharding techniques so as to run on several
    devices. and the inter-node co-scheduling in the scheduling side.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 单实例多设备（SIMD）与MISD相反，SIMD优化主要是由于模型规模趋势的提出。超大规模模型，特别是在语言 [[8](#bib.bib8)，[3](#bib.bib3)]和推荐 [[32](#bib.bib32)]领域，已被证明是提高模型准确度的重要因素。因此，最近最先进的工业模型已经达到了数百亿参数的规模 [[30](#bib.bib30)]，模型推理可能需要$\sim$TB的主机内存 [[32](#bib.bib32)]，这几乎无法在单个主机上完成。在这种情况下，通过多个设备进行分布式推理成为了超大规模模型推理的唯一解决方案 [[27](#bib.bib27)]。然而，由于涉及到较慢的设备间通信通道（*例如*，远程过程调用，RPCs），分布式推理需要算法和调度的创新来实现推理效率。例如，我们需要在算法方面考虑高效的模型切分，例如模型/流水线并行 [[44](#bib.bib44)，[18](#bib.bib18)]¹¹尽管这种模型/流水线并行主要用于大型模型训练，但最近超大规模模型推理也需要这样的模型切分技术，以便在多个设备上运行。以及调度方面的节点间协同调度。
- en: $\bullet$ Multi Instance Multi Device (MIMD) Towards even complexer scenarios,
    Multi-Instance Multi-Device (MIMD) optimizations consider how to route various
    model inference requests to different devices (*e.g.*, service routers) and manage
    compute device utilization. Such optimization mainly lie in data center-level
    management for optimal infrastructure utilization and cost. Currently, public
    available works [[49](#bib.bib49), [19](#bib.bib19), [52](#bib.bib52)] mainly
    target at optimizing training jobs consuming more resources (*e.g.*, 4/8-GPU machines,
    taking hours to days). There are still limited public works targeting at inference
    MIMD optimizations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 多实例多设备（MIMD）在更复杂的场景下，多实例多设备（MIMD）优化考虑了如何将各种模型推理请求路由到不同的设备（*例如*，服务路由器）并管理计算设备的利用率。这种优化主要涉及数据中心级别的管理，以实现最佳的基础设施利用和成本控制。目前，公开的工作 [[49](#bib.bib49)，[19](#bib.bib19)，[52](#bib.bib52)]主要针对优化消耗更多资源的训练任务（*例如*，4/8-GPU机器，耗时数小时到数天）。针对推理MIMD优化的公开工作仍然有限。
- en: '![Refer to caption](img/6a8a7ba7732bc76c3e4cbaf55163ec21.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6a8a7ba7732bc76c3e4cbaf55163ec21.png)'
- en: 'Figure 3: Multi-tenant inference latency and throughput comparison. (a) Although
    co-locating multiple DNNs degrades the latency by 5%-10%, the overall throughput
    can be improved 25%+ on average [[6](#bib.bib6)]. (b) Among 250 model co-location
    combination experiments, *e.g.*, up to 90-percent of bi-model execution shows
    less than 17% latency degradation [[5](#bib.bib5)].'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：多租户推断延迟和吞吐量比较。（a）尽管共置多个 DNN 会使延迟降低 5%-10%，但整体吞吐量可以平均提高 25%+ [[6](#bib.bib6)]。
    （b）在 250 个模型共置组合实验中，*例如*，最多 90% 的双模型执行显示延迟降低不到 17% [[5](#bib.bib5)]。
- en: '3 Computing Scaling: Trending to'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 计算扩展：趋势向
- en: Multi-Instance Single-Device (MISD)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 多实例单设备（MISD）
- en: 3.1 Overview
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概述
- en: With the compute scaling, the capacity of recent hardwares has achieved exponential
    growing for deep learning. Especially for GPUs, recent GPUs have demonstrated
    overwhelming capacities (*e.g.*, $\sim$130 TFLOPs/s for A100) compared to common
    model inference workload (*e.g.*, ResNet50 with $\sim$4 GFLOPs). As executing
    single DNN instance on such hardwares can incur severe resource under-utilization,
    multi-instance single-device (MISD) computing paradigm, or multi-tenant inference,
    co-locates multiple DNN instances onto the same hardware device to improve the
    hardware utilization as well as the serving throughput.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算扩展，近年来硬件在深度学习中的容量实现了指数级增长。特别是对于 GPU，最近的 GPU 显示了超强的能力（*例如*，A100 的 $\sim$130
    TFLOPs/s），与常见的模型推断工作负载（*例如*，ResNet50 的 $\sim$4 GFLOPs）相比。由于在此类硬件上执行单个 DNN 实例可能导致严重的资源利用不足，多实例单设备（MISD）计算范式或多租户推断，将多个
    DNN 实例共置到同一硬件设备上，以提高硬件利用率和服务吞吐量。
- en: 'Enhancing Serving Throughput One of the major goal of MISD is to achiever higher
    serving throughput, which is usually measured by the served queries per second
    (QPS). By improving the resource utilization (*e.g.*, computing units, memory
    bandwidth), MISD could increase the serving throughput on the same hardware. Figure [3](#S2.F3
    "Figure 3 ‣ 2 Large-Scale DL Serving System: A Novel Taxonomy ‣ A Survey of Large-Scale
    Deep Learning Serving System Optimization: Challenges and Opportunities") (left)
    shows an example of co-running two DNN models (GoogLeNet and ResNet) on the same
    hardware. Although the latency of each model degrades by 5%-10%, the overall throughput
    is improved by 25%+ on average.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '提高服务吞吐量 MISD 的主要目标之一是实现更高的服务吞吐量，这通常通过每秒处理的查询数（QPS）来衡量。通过提高资源利用率（*例如*，计算单元、内存带宽），MISD
    可以在相同硬件上提高服务吞吐量。图 [3](#S2.F3 "Figure 3 ‣ 2 Large-Scale DL Serving System: A Novel
    Taxonomy ‣ A Survey of Large-Scale Deep Learning Serving System Optimization:
    Challenges and Opportunities")（左侧）展示了在相同硬件上共运行两个 DNN 模型（GoogLeNet 和 ResNet）的示例。尽管每个模型的延迟降低了
    5%-10%，但整体吞吐量平均提高了 25%+。'
- en: '![Refer to caption](img/4e7bca86a67828399bec537ed1faf80d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4e7bca86a67828399bec537ed1faf80d.png)'
- en: 'Figure 4: Throughput and Power Comparison of CPUs/GPUs [[2](#bib.bib2)].'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：CPU/ GPU 吞吐量和功耗比较 [[2](#bib.bib2)]。
- en: 'Reducing Power and Infrastructural Costs By enhancing the serving throughput,
    another benefit of MISD is it could lower both the cost of infrastructures, as
    well as reduce the average power needed for processing each query. Figure [4](#S3.F4
    "Figure 4 ‣ 3.1 Overview ‣ 3 Computing Scaling: Trending to Multi-Instance Single-Device
    (MISD) ‣ A Survey of Large-Scale Deep Learning Serving System Optimization: Challenges
    and Opportunities") compares the power and serving throughput between CPU-based
    and GPU-based serving. Although the RTX2080Ti has 3$\times$ power consumption
    compared to Intel Xeon416 CPU (250W vs. 85W), the GPU serving throughput can reach
    at most 100$\times$, *e.g.*, for MobileNetV2 and NasNet. This translates to $\sim$30$\times$
    average power reduction for processing each query, thus greatly reducing the power
    and related infrastructural costs.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '降低功耗和基础设施成本 通过提高服务吞吐量，MISD 的另一个好处是它可以降低基础设施成本，并减少处理每个查询所需的平均功率。图 [4](#S3.F4
    "Figure 4 ‣ 3.1 Overview ‣ 3 Computing Scaling: Trending to Multi-Instance Single-Device
    (MISD) ‣ A Survey of Large-Scale Deep Learning Serving System Optimization: Challenges
    and Opportunities") 比较了基于 CPU 和基于 GPU 的服务之间的功耗和服务吞吐量。尽管 RTX2080Ti 的功耗是 Intel Xeon416
    CPU 的 3$\times$（250W 对 85W），但 GPU 服务的吞吐量最多可以达到 100$\times$，*例如*，对于 MobileNetV2
    和 NasNet。这转化为处理每个查询的平均功耗减少约 $\sim$30$\times$，从而大大降低了功耗和相关基础设施成本。'
- en: 'Latency Performance Degradation However, co-locating more DNN instances would
    have worse latency performance since the average available resources are less
    for each DNN instance. Therefore, DL inference service providers and consumers
    will usually set certain latency constraints (SLA), which requires queries to
    be served within given latency (for example, less than 100ms for ads display for
    the best user experience). Thus, multi-tenant inference with certain latency degradation
    in SLA range is also considered acceptable. Figure [3](#S2.F3 "Figure 3 ‣ 2 Large-Scale
    DL Serving System: A Novel Taxonomy ‣ A Survey of Large-Scale Deep Learning Serving
    System Optimization: Challenges and Opportunities") (right) shows an example of
    the latency degradation analysis among 250 pairs of model co-running combinations,
    the average latency degradation is only 17% for up to 90% of the combinations,
    demonstrating the great potential of GPU utilization enhancement.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '延迟性能退化 但是，更多的DNN实例共置会导致更差的延迟性能，因为每个DNN实例可用的平均资源更少。因此，DL推理服务提供者和消费者通常会设置一定的延迟约束（SLA），要求查询在给定的延迟内完成（例如，为了获得最佳用户体验，广告展示的延迟应小于100毫秒）。因此，SLA范围内的多租户推理具有一定的延迟退化也被认为是可接受的。图 [3](#S2.F3
    "Figure 3 ‣ 2 Large-Scale DL Serving System: A Novel Taxonomy ‣ A Survey of Large-Scale
    Deep Learning Serving System Optimization: Challenges and Opportunities")（右）显示了250对模型共运行组合的延迟退化分析示例，对于高达90%的组合，平均延迟退化仅为17%，显示了GPU利用提升的巨大潜力。'
- en: 'TABLE I: Recent Works on Multi-Tenant Inference Optimization (JCT: job completion
    time, SLA: service-level agreement).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：多租户推理优化的近期工作（JCT：作业完成时间，SLA：服务水平协议）。
- en: '| Ref. | Hardware | Problem | Perspective | Algorithm/Strategy | Improvement/Achievement
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 硬件 | 问题 | 视角 | 算法/策略 | 改进/成就 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| [[53](#bib.bib53)] | GPU |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| [[53](#bib.bib53)] | GPU |'
- en: '&#124; $\bullet$ Resource under-utilization &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 资源未充分利用 &#124;'
- en: '&#124; $\bullet$ Contention &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 争用 &#124;'
- en: '| SW Scheduling |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| SW调度 |'
- en: '&#124; $\bullet$ Operator-level scheduling &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 操作员级别的调度 &#124;'
- en: '&#124; $\bullet$ ML-based scheduling auto-search &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 基于ML的调度自动搜索 &#124;'
- en: '| $\bullet$ Reduced inference makespan |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 减少推理完成时间 |'
- en: '| [[29](#bib.bib29)] | GPU | $\bullet$ Inter-job interference | SW Scheduling
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] | GPU | $\bullet$ 作业间干扰 | SW调度 |'
- en: '&#124; $\bullet$ Query-level online scheduling &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 查询级别的在线调度 &#124;'
- en: '&#124; $\bullet$ ML-based interference predictor &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 基于ML的干扰预测器 &#124;'
- en: '| $\bullet$ Reduced latency |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 延迟减少 |'
- en: '| [[51](#bib.bib51)] | GPU | $\bullet$ Client-side waiting time | SW Scheduling
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| [[51](#bib.bib51)] | GPU | $\bullet$ 客户端等待时间 | SW调度 |'
- en: '&#124; $\bullet$ Query-level online scheduling &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 查询级别的在线调度 &#124;'
- en: '&#124; $\bullet$ Heuristic-based preemption &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 基于启发式的抢占 &#124;'
- en: '&#124; $\bullet$ Concurrent and batching &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 并发和批处理 &#124;'
- en: '| $\bullet$ Reduced latency |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 延迟减少 |'
- en: '| [[6](#bib.bib6)] | NPU | $\bullet$ Priority-based serving | SW Scheduling
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [[6](#bib.bib6)] | NPU | $\bullet$ 基于优先级的服务 | SW调度 |'
- en: '&#124; $\bullet$ Query-level online scheduling &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 查询级别的在线调度 &#124;'
- en: '&#124; $\bullet$ Heuristic preemption &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 启发式抢占 &#124;'
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\bullet$ Reduced high-priority job JCT &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 减少高优先级作业JCT &#124;'
- en: '&#124; $\bullet$ Maintaining low-priority SLA &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 维护低优先级SLA &#124;'
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [[9](#bib.bib9)] | GPU |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| [[9](#bib.bib9)] | GPU |'
- en: '&#124; $\bullet$ Resource under-utilization &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 资源未充分利用 &#124;'
- en: '&#124; $\bullet$ contention &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 争用 &#124;'
- en: '| HW Resource Managing |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| HW资源管理 |'
- en: '&#124; $\bullet$ Managed resource provisioning &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 管理的资源提供 &#124;'
- en: '&#124; $\bullet$ Adaptive batching &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 自适应批处理 &#124;'
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\bullet$ Enhanced serving throughput &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 增强的服务吞吐量 &#124;'
- en: '&#124; $\bullet$ Maintaining SLA &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 维护SLA &#124;'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [[5](#bib.bib5)] | GPU |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [[5](#bib.bib5)] | GPU |'
- en: '&#124; $\bullet$ Resource under-utilization &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 资源未充分利用 &#124;'
- en: '&#124; $\bullet$ Contention &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 争用 &#124;'
- en: '| HW Resource Managing |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| HW资源管理 |'
- en: '&#124; $\bullet$ Managed resource provisioning &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 管理的资源提供 &#124;'
- en: '&#124; $\bullet$ Adaptive batching &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 自适应批处理 &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\bullet$ Enhanced serving throughput &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 增强的服务吞吐量 &#124;'
- en: '&#124; $\bullet$ Maintaining SLA &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 维护SLA &#124;'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [[15](#bib.bib15)] |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| [[15](#bib.bib15)] |'
- en: '&#124; Systolic &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 脉冲型 &#124;'
- en: '&#124; Arrays &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数组 &#124;'
- en: '| $\bullet$ Resource under-utilization | Architecture ReConfig | $\bullet$
    Hardware resource fission |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $\bullet$ 资源利用不足 | 架构重配置 | $\bullet$ 硬件资源分裂 |'
- en: '&#124; $\bullet$ Enhanced serving throughput &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 提高服务吞吐量 &#124;'
- en: '&#124; $\bullet$ Reduced energy cost &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\bullet$ 降低能源成本 &#124;'
- en: '|'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 3.2 Challenges in MISD
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 MISD中的挑战
- en: There are many challenges that hinder the performance improvement of MISD. We
    summarize the major ones, such as inter-tenant interference and serving workload
    dynamics.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多挑战阻碍了MISD的性能提升。我们总结了主要的问题，如租户间干扰和服务工作负载动态。
- en: 3.2.1 Inter-Tenant Interference
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 租户间干扰
- en: In MISD, co-locating multiple DL jobs on the same hardware allows these models
    to share the compute resource, while brings the problem of inter-tenant interference.
    As DNN models contain many types of operators such as convolution, batch-norm,
    skip-connection, *etc.*, different types of operators can be either compute intensive
    (*e.g.*, convolution) or memory intensive (*e.g.*, skip-connection). Therefore,
    co-running multiple models on the GPU can incur resource contention in computing
    or memory bandwidth due to the poor operator concurrency management (*e.g.*, co-running
    the same type of operators at the same time). As a result, the inference speed
    of both models can be slowed down. Such cases can happen frequently with more
    increased number of tenants and thus degrade the overall serving throughput.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在MISD中，将多个深度学习作业共置于相同的硬件上允许这些模型共享计算资源，但也带来了租户间干扰的问题。由于DNN模型包含许多类型的操作符，如卷积、批量归一化、跳过连接，*等等*，不同类型的操作符可能是计算密集型的（*例如*，卷积）或内存密集型的（*例如*，跳过连接）。因此，在GPU上同时运行多个模型可能会因操作符并发管理不善（*例如*，同时运行相同类型的操作符）而引发计算或内存带宽的资源争用。结果，两个模型的推理速度可能都会减慢。随着租户数量的增加，这种情况可能会频繁发生，从而降低整体服务吞吐量。
- en: 3.2.2 Serving Workload Dynamics
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 服务工作负载动态
- en: The inter-tenant interference could be more difficult to handle when the serving
    workload is not static but dynamic. For example, in the cloud environment, due
    to the fact that DL jobs’ arrival is usually non-predictable, perfectly interleaving
    compute-intensive and memory-intensive DNN queries is usually not feasible. In
    such case, multi-tenant inference and potential interference behavior is hard
    to predict as it has a tremendous combination space due to various of models,
    different number of co-running jobs, *etc.* With such unpredictability, it is
    very challenging to maintain no inter-tenant interference and achieve ideal serving
    throughput.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务工作负载不是静态而是动态时，租户间干扰可能更难处理。例如，在云环境中，由于深度学习作业的到达通常不可预测，完美地交错计算密集型和内存密集型的DNN查询通常是不切实际的。在这种情况下，多租户推理和潜在的干扰行为很难预测，因为由于各种模型、不同数量的并发作业，*等等*，它们的组合空间非常庞大。面对这种不可预测性，保持无租户间干扰并实现理想的服务吞吐量是非常具有挑战性的。
- en: 3.2.3 Job Priority and Completion Time
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 作业优先级和完成时间
- en: There are several other problems in MISD computing. For example, one important
    issue that needs to be considered is the job priority, *e.g.*, certain jobs need
    to be finished with ensured performance or with higher priority. In such case,
    preemption is a common scheduling strategy to ensure high-priority jobs to finish
    in a short time. Or we can also allocate dedicated resource to high-priority jobs.
    Another issue is that instead of considering server-side serving throughput, certain
    serving systems also need to consider client-side user experiences measured by
    average job completion time (JCT). Minimizing JCT is also contradictory to maximizing
    throughput, *e.g.*, concurrently running multiple jobs can cause longer average
    JCT but increase serving throughput.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在MISD计算中还存在其他一些问题。例如，一个需要考虑的重要问题是作业优先级，*例如*，某些作业需要确保性能完成或具有更高的优先级。在这种情况下，抢占是一种常见的调度策略，以确保高优先级作业在短时间内完成。或者，我们还可以为高优先级作业分配专用资源。另一个问题是，某些服务系统除了需要考虑服务器端的服务吞吐量，还需要考虑客户端用户体验，这可以通过平均作业完成时间（JCT）来衡量。最小化JCT也与最大化吞吐量相矛盾，*例如*，同时运行多个作业可能会导致更长的平均JCT，但却提高了服务吞吐量。
- en: 3.3 Optimization Techniques
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 优化技术
- en: 'We summarize recent works in multi-tenant inference optimization into two major
    categories: temporal workload scheduling and spatial resource management. An overview
    of these works are shown in Table [I](#S3.T1 "TABLE I ‣ 3.1 Overview ‣ 3 Computing
    Scaling: Trending to Multi-Instance Single-Device (MISD) ‣ A Survey of Large-Scale
    Deep Learning Serving System Optimization: Challenges and Opportunities").'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将近期在多租户推理优化方面的研究总结为两个主要类别：时间工作负载调度和空间资源管理。这些工作的概述见表[I](#S3.T1 "TABLE I ‣ 3.1
    概述 ‣ 3 计算扩展：趋向于多实例单设备（MISD） ‣ 大规模深度学习服务系统优化的调查：挑战与机遇")。
- en: '![Refer to caption](img/94af91957f380bf0d1f5156108b096f0.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/94af91957f380bf0d1f5156108b096f0.png)'
- en: 'Figure 5: Multi-Tenant Inference with Temporal and Spatial Scheduling.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 多租户推理中的时间和空间调度。'
- en: 3.3.1 Software-Level Workload Scheduling
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 软件级工作负载调度
- en: 'As we mentioned before, one of the major challenges in multi-tenant inference
    is to avoid job interference. Therefore, many research works [[53](#bib.bib53),
    [29](#bib.bib29), [51](#bib.bib51), [6](#bib.bib6)] conduct workload scheduling
    among different DL jobs temporally to reduce such interfering, as shown in Figure [5](#S3.F5
    "Figure 5 ‣ 3.3 Optimization Techniques ‣ 3 Computing Scaling: Trending to Multi-Instance
    Single-Device (MISD) ‣ A Survey of Large-Scale Deep Learning Serving System Optimization:
    Challenges and Opportunities"). Such solutions aims to avoid interference by scheduling
    different DL model workloads along the time dimension, thus we categorize such
    works as software-level scheduling solutions.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，多租户推理中的主要挑战之一是避免任务干扰。因此，许多研究工作[[53](#bib.bib53)、[29](#bib.bib29)、[51](#bib.bib51)、[6](#bib.bib6)]对不同深度学习任务进行时间上的工作负载调度，以减少这种干扰，如图[5](#S3.F5
    "图 5 ‣ 3.3 优化技术 ‣ 3 计算扩展：趋向于多实例单设备（MISD） ‣ 大规模深度学习服务系统优化的调查：挑战与机遇")所示。这些解决方案旨在通过沿时间维度调度不同的深度学习模型工作负载来避免干扰，因此我们将这类工作归类为软件级调度解决方案。
- en: In practice, such workload scheduling granularity could be fine-grained, *e.g.*,
    scheduling the DNN operators, or coarse-grained, *e.g.*, scheduling the entire
    inference query.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这种工作负载调度粒度可以是细粒度的，例如调度深度神经网络（DNN）操作符，或者是粗粒度的，例如调度整个推理查询。
- en: 'Operator Scheduling To achieve fine-grained scheduling, works [[53](#bib.bib53),
    [10](#bib.bib10)] regard the DL model operators (*e.g.*, conv, batch-norm, identity)
    as the minimum scheduling units. They first abstract multiple DNN’s computation
    graph with many operators into an intermediate representation (IR). To explore
    the huge scheduling space, they design a ML-based auto-search method by defining
    three main factors: scheduling search space, profiling-guided latency cost model,
    and the machine learning-based search algorithm to find the best scheduling for
    a balanced GPU utilization without interference and reduced latency. Such fine-grained
    operator scheduling could achieve better performance, but usually face scalability
    issues when the number of workloads increases to very large, such as hundreds
    of DNN queries.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符调度 为实现细粒度调度，工作[[53](#bib.bib53)、[10](#bib.bib10)]将深度学习模型操作符（例如，卷积、批量归一化、身份）视为最小调度单元。他们首先将多个DNN的计算图与许多操作符抽象为中间表示（IR）。为了探索巨大的调度空间，他们通过定义三个主要因素设计了基于机器学习的自动搜索方法：调度搜索空间、基于分析的延迟成本模型，以及基于机器学习的搜索算法，以找到最佳调度，实现平衡的GPU利用率而不干扰且延迟减少。这种细粒度的操作符调度可以实现更好的性能，但通常在工作负载数量增加到非常大（如数百个DNN查询）时面临可扩展性问题。
- en: Service Router Therefore, other works like [[29](#bib.bib29), [51](#bib.bib51),
    [6](#bib.bib6)] use a more coarse scheduling granularity, *e.g.*, regarding each
    query as the minimum scheduling units, which thus reduces the scheduling complexity.
    For example, one of the query-level workload scheduling example is the service
    router in large-scale data center, such as Microsoft Deep Learning Inference Service
    (DLIS) system [[43](#bib.bib43)]. To achieve high utilization while avoid resource
    contention, service router needs to understand different models’ requirements
    and place one or multiple queries intelligently onto hardware.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 服务路由器 因此，其他工作如[[29](#bib.bib29)、[51](#bib.bib51)、[6](#bib.bib6)]采用了更粗粒度的调度，例如，将每个查询视为最小调度单元，从而减少了调度复杂性。例如，一个查询级工作负载调度的例子是大规模数据中心中的服务路由器，如微软深度学习推理服务（DLIS）系统[[43](#bib.bib43)]。为了实现高利用率同时避免资源争用，服务路由器需要了解不同模型的要求，并智能地将一个或多个查询分配到硬件上。
- en: Predictive Scheduling Within above scheduling techniques, one challenging factor
    is the serving queries dynamics, for example, unknown number of incoming queries,
    RNN/LSTMs with varied input lengths. Different from static workloads that we can
    get the full workload information for scheduling, such dynamic workloads require
    us to consider the potential incoming workloads. To handle such dynamics, PREMA [[6](#bib.bib6)]
    proposed a predictive multi-task DNN scheduling algorithm that combines off-line
    profiling latency records and an online token-based job scheduling algorithm.
    Meanwhile, it also enables adaptive job preemption to achieve priority control.
    But as each DNN can have many operators (*e.g.*, layers) that have fluctuated
    resource consumption, such coarse-grained scheduling may still suffer occasionally
    resource under-utilization/contention and sub-optimal performance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 预测调度 在上述调度技术中，一个具有挑战性的因素是服务查询的动态性，例如，未知数量的传入查询、具有不同输入长度的RNN/LSTM。与静态工作负载不同，我们可以获得完整的工作负载信息进行调度，这种动态工作负载要求我们考虑潜在的传入工作负载。为应对这种动态性，PREMA
    [[6](#bib.bib6)]提出了一种预测性多任务DNN调度算法，该算法结合了离线分析延迟记录和基于令牌的在线作业调度算法。同时，它还启用了自适应作业抢占以实现优先级控制。但是，由于每个DNN可能有许多操作符（*例如*，层）具有波动的资源消耗，这种粗粒度的调度仍然可能偶尔遭遇资源利用不足/争用和次优性能。
- en: 3.3.2 Hardware-Level Resource Management
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 硬件级资源管理
- en: Besides temporal scheduling, another optimization perspective to solve the inter-tenant
    inference is to conduct fine-grained resource managing [[9](#bib.bib9), [15](#bib.bib15)],
    such as spatial partitioning and isolation of resources for different jobs, which
    we consider as a spatial perspective. As such partitioning could isolate different
    job’s used resource (*e.g.*, stream multiprocessors (SMs), memory bandwidths),
    such solution can help avoid the job interference in the hardware level.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了时间调度，解决租户间推理问题的另一个优化视角是进行细粒度的资源管理[[9](#bib.bib9), [15](#bib.bib15)]，例如空间分区和资源隔离，这被视为空间视角。由于这种分区可以隔离不同作业使用的资源（*例如*，流式多处理器（SMs）、内存带宽），这种解决方案可以帮助避免在硬件层面的作业干扰。
- en: Resource Partitioning Previously, achieving fine-grained resource partitioning
    is non-achievable until recently NVIDIA GPUs release a series of resource sharing
    and partitioning support like multi-streams (MST), multi-process services (MPS [[34](#bib.bib34)])
    and multi-instance GPU (MIG [[33](#bib.bib33)])²²2The major differences between
    three tools are that MST only enables resource sharing but doesn’t avoid SM nor
    memory bandwidth interference. MPS enables SM partitioning by allocating certain
    SMs to given jobs (*e.g.*, for 5 DNNs, allocating 20% SMs for each one), thus
    avoiding SM interference but doesn’t address memory interference. MIG achieves
    both SM and memory bandwidth partitioning, which is recently supported on NVIDIA
    A100 GPUs only [[33](#bib.bib33)].. Leveraging such support, [[9](#bib.bib9)]
    uses MPS to conduct adaptive SM partitioning for different DNN jobs. By doing
    so, the SM resource contention could be avoided among co-located DL workloads.
    Similarly, [[15](#bib.bib15)] utilizes special accelerator (systolic arrays) and
    implements architecture support for hardware fission, which achieves fine-grained
    resource managing.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 资源分区 以前，实现细粒度的资源分区是不可能的，直到最近NVIDIA GPUs发布了一系列资源共享和分区支持功能，如多流（MST）、多进程服务（MPS
    [[34](#bib.bib34)]) 和多实例GPU（MIG [[33](#bib.bib33)]）。这三种工具的主要区别在于，MST仅启用资源共享，但不能避免SM或内存带宽干扰。MPS通过将特定SM分配给给定作业（*例如*，对5个DNN，将20%的SM分配给每个）实现SM分区，从而避免SM干扰，但不解决内存干扰问题。MIG实现了SM和内存带宽的分区，目前仅在NVIDIA
    A100 GPUs上得到支持[[33](#bib.bib33)]。利用这种支持，[[9](#bib.bib9)]使用MPS为不同DNN作业进行自适应SM分区，从而避免了共同定位的DL工作负载之间的SM资源争用。类似地，[[15](#bib.bib15)]利用特殊加速器（系统数组）并实现了硬件裂变的架构支持，实现了细粒度的资源管理。
- en: Hardware Re-Configuration However, such spatial resource partitioning solutions
    also have a intrinsic limitation that is the inflexible re-configuration when
    facing dynamic workloads. For both GPUs and other accelerators, changing the resource
    partitioning configurations requires certain amount of time (*e.g.*, tens of seconds
    or more), which can be much larger than DL workloads’ processing time (usually
    served in ms). Therefore, re-configuring the resource partitioning frequently
    is usually not practical and thus limits such solutions’ performance when facing
    dynamic workloads. [[9](#bib.bib9)] tries to reduce the stall caused by reconfiguration
    time of MPS by utilizing a standby/shadow process. However, the minimum time for
    switching one partitioning configuration to another one still cost several seconds,
    which is non-negligible in online serving scenarios.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件重新配置 然而，这种空间资源分区解决方案也有一个固有的限制，即在面对动态工作负载时的不可灵活的重新配置。对于 GPU 和其他加速器，改变资源分区配置需要一定的时间（*例如*，几十秒或更多），这可能远大于
    DL 工作负载的处理时间（通常为毫秒级）。因此，频繁重新配置资源分区通常是不切实际的，从而限制了这种解决方案在面对动态工作负载时的性能。[[9](#bib.bib9)]
    通过利用备用/影子进程来尝试减少由 MPS 重新配置时间造成的停滞。然而，从一个分区配置切换到另一个配置的最短时间仍需几秒钟，这在在线服务场景中是不可忽视的。
- en: 3.4 Future Directions
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 未来方向
- en: 3.4.1 Software-Hardware Co-Scheduling
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 软件-硬件共调度
- en: The software and hardware scheduling could be complementary to each other to
    provide both high job scheduling flexibility and strict resource isolation. Recently,
    there are certain works that adopt a temporal-spatial combined perspective to
    achieve better serving performance. [[5](#bib.bib5)] adopts MPS to conduct resource
    partitioning and split one GPU to multiple gpulets and then implements a heuristic-based
    greedy task scheduler to find the appropriate mapping relationship between the
    current DNN queries and gpulets. Nevertheless, the serving workload dynamics in
    spatial-temporal scheduling is more complex and needs to be re-thinked and designed.
    For example, beyond previous temporal-only scheduling that only decides when to
    start each job, having multiple hardware partitions also requires to decide which
    one to place on. The current greedy task scheduler in [[5](#bib.bib5)] treats
    the current workload as static without considering potentially dynamic incoming
    job requests. Thus, a simple sub-optimal case can be that it can allocate a current
    small workload to a large gpulet, while leaving no gpulet for a larger incoming
    DNN job to use, resulting in potential resource under-utilization.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 软件和硬件调度可以相互补充，以提供高作业调度灵活性和严格的资源隔离。最近，一些工作采用了时空结合的视角以实现更好的服务性能。[[5](#bib.bib5)]
    采用 MPS 进行资源分区，将一个 GPU 切分为多个 gpulet，然后实施基于启发式的贪婪任务调度器，寻找当前 DNN 查询与 gpulet 之间的适当映射关系。然而，时空调度中的服务工作负载动态更为复杂，需要重新思考和设计。例如，除了之前的仅时间调度只决定何时启动每个作业外，多个硬件分区还需要决定将作业放置在哪个分区。[[5](#bib.bib5)]
    中的当前贪婪任务调度器将当前工作负载视为静态的，没有考虑潜在的动态到来的作业请求。因此，一个简单的次优情况可能是将当前小工作负载分配到一个大 gpulet
    上，而没有为更大的即将到来的 DNN 作业保留 gpulet，从而导致潜在的资源利用不足。
- en: 3.4.2 ML-based Prediction and Online Learning
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 基于 ML 的预测与在线学习
- en: To avoid such problem, we can potentially use a ML-based predictive model (*e.g.*,
    reinforcement learning, LSTM, *etc.*) to predict the dynamic workload and then
    guide the workload-to-partition mapping. The ML-based model can be initially trained
    offline by historical serving records. During the online serving process, active
    and lifelong learning, i.e., using the latency/throughput as feedback to consistently
    improve the predictive accuracy, can also be potentially utilized in such case
    to improve the workload prediction and scheduling effectiveness.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种问题，我们可以潜在地使用基于 ML 的预测模型（*例如*，强化学习、LSTM、*等*）来预测动态工作负载，然后指导工作负载到分区的映射。基于
    ML 的模型可以通过历史服务记录进行离线初始训练。在在线服务过程中，可以利用主动和终身学习，即使用延迟/吞吐量作为反馈来持续提高预测准确性，以改进工作负载预测和调度效果。
- en: Another way of leveraging ML-based prediction is to conduct modeling to predict
    the potential latency performance under different multi-model and hardware combinations
    so that the scheduler can make better decision regarding the latency SLA constraints.
    For example, the work [[29](#bib.bib29)] built a ML model to predict the latency
    of multi-model inference cases on different machines. However, the effectiveness
    of such solution also highly depends on the modeling accuracy, scalability and
    generality, which can be hard to achieve all in practice.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 利用基于机器学习的预测的另一种方法是进行建模，以预测在不同的多模型和硬件组合下的潜在延迟性能，从而使调度器能够更好地决策关于延迟SLA约束。例如，工作[[29](#bib.bib29)]建立了一个机器学习模型，用于预测不同机器上的多模型推断案例的延迟。然而，这种解决方案的有效性也高度依赖于建模的准确性、可扩展性和通用性，这在实践中很难同时实现。
- en: '4 Model Scaling: Trending to'
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 模型扩展：趋势分析
- en: Single Instance Multi Device (SIMD)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 单实例多设备（SIMD）
- en: 4.1 Overview
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 概述
- en: Different from MISD, the trend of SIMD is mainly brought by the tremendous model
    scaling [[3](#bib.bib3), [30](#bib.bib30)] which makes one model hard to execute
    on one single machine. For example, industrial recommendation models [[32](#bib.bib32)]
    requires $\sim$TB of memory for inference. In such cases, scaling up single machine
    to support such memory capacity can incur huge infrastructural cost in large-scale
    data centers. Therefore, one more cost-efficient way is to scale out, which is
    to using multiple distributed compute devices to conduct collaboratively inference.
    However, unlike distributed training with long time duration and thus loose time
    constraints, inference serving have strict latency constraints. Thus, the communication
    cost in such collaboratively serving can make the SLAs more challenging to achieve.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与MISD不同，SIMD的趋势主要受到模型规模极大扩展的影响[[3](#bib.bib3), [30](#bib.bib30)]，这使得一个模型难以在单台机器上执行。例如，工业推荐模型[[32](#bib.bib32)]在推断时需要$\sim$TB的内存。在这种情况下，将单台机器扩展以支持如此大的内存容量可能会在大规模数据中心引发巨大的基础设施成本。因此，另一种更具成本效益的方法是扩展，这就是使用多个分布式计算设备协同进行推断。然而，与具有较长时间持续性的分布式训练不同，推断服务具有严格的延迟约束。因此，在这种协同服务中，通信成本可能使得服务水平协议（SLA）更难实现。
- en: 'Enhancing Capacity Scaling The first priority of SIMD is to achieve capacity
    scaling of infrastructures so as to enable super-scale model inference serving.
    To do so, there are two major ways: Scale up vs. Scale out. Scale up (or vertical
    scaling) means upgrading or adding more resources to an system to reach a desired
    state of performance, *e.g.*, more compute, memory, storage and network bandwidth,
    *etc.* By contrast, scale out (or horizontal scaling) uses more distributed low-end
    machines to collaboratively serve one large DL model. SIMD such as distributed
    inference [[27](#bib.bib27), [56](#bib.bib56), [59](#bib.bib59), [54](#bib.bib54)]
    mainly uses the second scale-out solution to maintain the capacity scaling for
    larger DL model serving. Recently, heterogeneous inference [[26](#bib.bib26),
    [50](#bib.bib50), [48](#bib.bib48)] comes up, which combines heterogeneous resources
    such as hybriding CPU/GPU, hybriding cache/memory/SSD, etc. Such solutions could
    be considered as a scale-up solution.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 提升容量扩展 SIMD的首要任务是实现基础设施的容量扩展，从而支持超大规模模型的推断服务。为此，有两种主要方式：垂直扩展与水平扩展。垂直扩展（或称为纵向扩展）意味着升级或增加系统的资源，以达到期望的性能状态，例如，更多的计算、内存、存储和网络带宽等。相比之下，水平扩展（或称为横向扩展）则使用更多分布式的低端机器协同服务一个大型深度学习模型。SIMD如分布式推断[[27](#bib.bib27),
    [56](#bib.bib56), [59](#bib.bib59), [54](#bib.bib54)]主要使用第二种水平扩展解决方案来维持更大深度学习模型的服务容量。最近，异构推断[[26](#bib.bib26),
    [50](#bib.bib50), [48](#bib.bib48)]出现，它结合了异构资源，如混合CPU/GPU、混合缓存/内存/SSD等。这些解决方案可以被视为垂直扩展解决方案。
- en: Reducing Costs For both scaling up and scaling out solutions, the major optimization
    targets of above solutions (i.e., SIMD) is to reduce the total cost of ownership
    (TCO). For example, scaling up by increasing single machine capacity such as upgrading
    CPU generations, increasing memory capacity and frequencies can be expensive within
    large-scale data center-level infrastructures. Therefore, the heterogeneous inference
    mainly relies on existing hardwares and combines them to enhance the compute capacity,
    thus reducing the capacity scaling costs. By contrast, scaling out utilizes multiple
    current generation of compute devices to serve next-generation larger models,
    thus also reducing the cost of setting up new dedicated hardware. For both methods,
    the shared downside is that it brings complex distributed computing and communication
    management, and even potential performance degradation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 降低成本 对于横向扩展和纵向扩展的解决方案，上述解决方案（即SIMD）的主要优化目标是降低总拥有成本（TCO）。例如，通过提高单台机器的容量，如升级CPU代数、增加内存容量和频率，在大规模数据中心级基础设施中可能会很昂贵。因此，异构推理主要依赖现有硬件并将其组合以增强计算能力，从而降低容量扩展成本。相对而言，纵向扩展利用多台当前一代计算设备来服务下一代更大模型，从而也降低了设置新专用硬件的成本。两种方法的共同缺点是它带来了复杂的分布式计算和通信管理，甚至可能导致性能下降。
- en: '![Refer to caption](img/4f78b5f0b12d38c55fdca8bd5985b9c5.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4f78b5f0b12d38c55fdca8bd5985b9c5.png)'
- en: 'Figure 6: In three parallelisms, model parallelism is the common way to achieve
    distributed inference while the other two are only suitable for distributed training.
    For example, data parallelism is not suitable for inference with small batch sizes.
    Pipeline parallelism cannot leverage multi-node computing parallelism within a
    single inference request.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在三种并行性中，模型并行性是实现分布式推理的常见方式，而另外两种仅适用于分布式训练。例如，数据并行性不适用于小批量推理。流水线并行性无法在单个推理请求中利用多节点计算并行性。
- en: 4.2 Challenges in SIMD
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 SIMD中的挑战
- en: 4.2.1 Computing Parallelism
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 计算并行性
- en: 'Although SIMD with distributed devices can achieve the model’s required overall
    capacity, distributing one entire model to collaboratively compute on multi-nodes
    is still very complex. There are many model partitioning algorithms that have
    been explored in distributed training algorithms such as data, model, and pipeline
    parallelism [[44](#bib.bib44), [31](#bib.bib31), [20](#bib.bib20)], *etc.* However,
    not all of them are suitable for inference scenarios. As shown in Figure [6](#S4.F6
    "Figure 6 ‣ 4.1 Overview ‣ 4 Model Scaling: Trending to Single Instance Multi
    Device (SIMD) ‣ A Survey of Large-Scale Deep Learning Serving System Optimization:
    Challenges and Opportunities"), traditional data parallelism by splitting the
    batch sizes is usually not applicable since the inference serving usually comes
    with small batch sizes. Pipeline parallelism [[44](#bib.bib44), [31](#bib.bib31),
    [18](#bib.bib18)] by allocating different layers into different machines is also
    not suitable as it cannot achieve multi-node computing parallelism within one
    single inference request. As a result, model parallelism [[20](#bib.bib20)] is
    the most common way to achieve SIMD or distributed inference, as we will introduce
    later.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管带有分布式设备的SIMD可以实现模型所需的总体容量，但将整个模型分配到多个节点上进行协作计算仍然非常复杂。已经在分布式训练算法中探索了许多模型分区算法，如数据并行性、模型并行性和流水线并行性[[44](#bib.bib44),
    [31](#bib.bib31), [20](#bib.bib20)]，*等等*。然而，并非所有这些算法都适用于推理场景。如图[6](#S4.F6 "图6
    ‣ 4.1 概述 ‣ 4 模型扩展：趋向单实例多设备（SIMD） ‣ 大规模深度学习服务系统优化调查：挑战与机遇")所示，传统的数据并行性通过拆分批量大小通常不适用，因为推理服务通常具有小批量大小。流水线并行性[[44](#bib.bib44),
    [31](#bib.bib31), [18](#bib.bib18)]通过将不同层分配到不同机器上也不适用，因为它不能在单个推理请求中实现多节点计算并行性。因此，模型并行性[[20](#bib.bib20)]是实现SIMD或分布式推理的最常见方法，后续我们将进行介绍。
- en: 4.2.2 Communication Bottlenecks
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 通信瓶颈
- en: For model parallelism on multi-distributed machines, achieving linear or even
    sub-linear speedup is usually very hard. The reason is that, the major factor
    that influences the computing efficiency (such as latency) in model parallelism
    is the communication bottlenecks caused by the intermediate data transfer between
    different model shards (distributed nodes) [[18](#bib.bib18), [40](#bib.bib40)].
    Depending on the data transmission speed, too much data transfer will incur significant
    latency overhead, *e.g.*, with Ethernet connections in distributed machines. To
    avoid that, an efficient model sharding algorithm is needed to ensure the inference
    serving performance in MISD. However, the intermediate data communication mechanisms
    vary a lot in different model architectures, such as CNNs, Transformers, and recommendation
    DLRMs, *etc.*, and thus it requires many analysis efforts and design innovations
    to find the optimal model sharding algorithm.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在多分布式机器上进行模型并行，实现线性甚至是次线性加速通常是非常困难的。原因在于，影响模型并行计算效率的主要因素（如延迟）是不同模型碎片（分布式节点）之间的中间数据传输所造成的通信瓶颈
    [[18](#bib.bib18), [40](#bib.bib40)]。根据数据传输速度，过多的数据传输会产生显著的延迟开销，*例如*，在分布式机器中使用以太网连接。为了避免这种情况，需要一个高效的模型分片算法来确保
    MISD 中的推理服务性能。然而，不同模型结构中的中间数据通信机制差异很大，比如 CNN、Transformers 和推荐 DLRMs，*等*，因此需要许多分析工作和设计创新来找到最佳的模型分片算法。
- en: '![Refer to caption](img/146df4dd388dd37044b7ec9949f5fc77.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/146df4dd388dd37044b7ec9949f5fc77.png)'
- en: 'Figure 7: Distributed Inference could use RPCs to leverage multiple distributed
    hosts to collaboratively execute one large recommendation model [[27](#bib.bib27)].'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：分布式推理可以使用 RPC 来利用多个分布式主机协作执行一个大型推荐模型 [[27](#bib.bib27)]。
- en: 4.3 Optimization Techniques
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 优化技术
- en: 'We mainly introduce two aspects of works that conduct SIMD: Scaling out by
    distributed inference and Scaling up by heterogeneous inference.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要介绍进行 SIMD 的两个方面的工作：通过分布式推理进行扩展和通过异构推理进行扩展。
- en: '4.3.1 Scaling Out: Distributed Inference'
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 扩展：分布式推理
- en: Recommendation Model Distributed Inference For example, recent industry-level
    deep learning recommendation model (DLRM) [[32](#bib.bib32)] has reached $\sim$TB-level
    model size and such recommendation request takes over 79% of Facebook’s data center’s
    workload, [[27](#bib.bib27)] proposed the first work that applies the distributed
    inference technique into the large DLRM inference serving. Different from conventional
    MLPs and CNNs, DLRMs are mainly composed of feature embedding tables and fully-connected
    layers, where feature embedding tables are the major part of model weights (up
    to 80% to 95%). Such feature embedding part of DLRMs is memory-intensive, but
    have very light computation workload as it only needs to access certain columns
    of the embedding tables and conducts summation [[27](#bib.bib27)].
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐模型分布式推理 例如，最近行业级的深度学习推荐模型（DLRM）[[32](#bib.bib32)] 已经达到了 $\sim$TB 级别的模型大小，并且这种推荐请求占据了
    Facebook 数据中心工作负载的 79%以上，[[27](#bib.bib27)]提出了将分布式推理技术应用到大型 DLRM 推理服务的初始工作。不同于传统的
    MLP 和 CNN，DLRM 主要由特征嵌入表和全连接层组成，其中特征嵌入表是模型权重的主要部分（高达 80% 到 95%）。这种 DLRM 的特征嵌入部分对内存要求很高，但计算工作量很轻，因为它只需要访问嵌入表的某些列并进行求和
    [[27](#bib.bib27)]。
- en: Therefore, distributed inference is proposed to partition large embedding tables
    into different compute nodes and thus overcome the inference serving memory limitation.
    When the table embeddings in remote nodes are required, central model could invoke
    remote process calls (RPCs) to directly get the computed results from remote nodes
    and thus fulfill the distributed inference serving.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，提出了分布式推理来将大型嵌入表分区到不同的计算节点，并因此克服了推理服务的内存限制。当远程节点中需要表嵌入时，中央模型可以调用远程过程调用（RPCs）直接从远程节点获取计算结果并实现分布式推理服务。
- en: CNN Distributed Inference Compared to DLRMs, CNN model parallelism is more widely
    studied (such as channel parallelism/spatial parallelism) [[59](#bib.bib59)] and
    many works have also propose the CNN models’ distributed inference [[54](#bib.bib54),
    [56](#bib.bib56)]. For example, CoEdge [[54](#bib.bib54)] is one example of cooperative
    CNN inference with adaptive workload partitioning over heterogeneous distributed
    devices. Similarly, DeepThings [[56](#bib.bib56)] also proposes a framework for
    adaptively distributed execution of CNN-based inference on resource constrained
    IoT edge devices. It designs a scalable Fused Tile Partitioning (FTP) to minimize
    memory footprint while exposing parallelism to collaboratively compute of major
    convolutional layers. It further realizes a distributed work stealing approach
    to enable dynamic workload distribution and balancing at inference runtime.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: CNN分布式推断 与DLRM相比，CNN模型并行性（如通道并行/空间并行）[[59](#bib.bib59)] 已经被广泛研究，许多研究也提出了CNN模型的分布式推断[[54](#bib.bib54),
    [56](#bib.bib56)]。例如，CoEdge [[54](#bib.bib54)] 是一种在异构分布式设备上具有自适应工作负载分配的合作CNN推断示例。类似地，DeepThings
    [[56](#bib.bib56)] 也提出了一个在资源受限的IoT边缘设备上自适应分布执行CNN推断的框架。它设计了一种可扩展的融合平铺分区（FTP），以最小化内存占用，同时暴露并行性以协作计算主要的卷积层。它进一步实现了一种分布式工作窃取方法，以在推断运行时实现动态工作负载分配和平衡。
- en: '4.3.2 Scaling Up: Heterogeneous Inference'
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 扩展：异构推断
- en: Heterogeneous Memory Besides using distributed machines, another way to address
    super-scale model’s large memory capacity requirement is to leverage heterogeneous
    memory, *e.g.*, by combining different levels of memory such as cache, main memory
    and even SSDs. The major challenge of such heterogeneous memory is to address
    the slow access speed of SSD (the access bandwidth can be usually 100x slower
    than memory). Targeting at this bottleneck, [[26](#bib.bib26), [50](#bib.bib50),
    [48](#bib.bib48)] have proposed similar heterogeneous memory design that leverages
    storage-level SSDs to store partial of the embedding table weights of the DLRM.
    As the embedding table access patterns of DLRMs are usually sparse and have certain
    spatial/temporal locality, a proper embedding table placement strategy between
    memory/SSD with dedicated caching strategy could thus greatly enhance the heterogeneous
    memory access efficiency, thus reaching on-pair performance with pure memory based
    performance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 异构内存 除了使用分布式机器外，解决超大规模模型的大内存需求的另一种方法是利用异构内存，*例如*，通过结合不同层次的内存，如缓存、主内存甚至SSD。此类异构内存的主要挑战是解决SSD的访问速度慢的问题（访问带宽通常比内存慢100倍）。针对这一瓶颈，[[26](#bib.bib26),
    [50](#bib.bib50), [48](#bib.bib48)] 提出了类似的异构内存设计，利用存储级SSD来存储DLRM的部分嵌入表权重。由于DLRM的嵌入表访问模式通常是稀疏的，并具有一定的空间/时间局部性，因此在内存/SSD之间采用适当的嵌入表放置策略以及专门的缓存策略，可以显著提高异构内存的访问效率，从而达到与纯内存性能相当的水平。
- en: Heterogeneous Computing Along with heterogeneous memory, heterogeneous computing
    is a similar concept by hybriding the computing capacity for different processors,
    which is mostly commonly used for large model inference in SoCs with multiple
    types of computing processors such as CPU, GPU and other accelerators. For example,
    Synergy [[58](#bib.bib58)] proposed a HW/SW framework for high throughput CNNs
    on embedded heterogeneous SoC. It leverages all the available on-chip resources,
    which includes the dual-core ARM processor along with the FPGA and the accelerator
    to collaboratively compute for the CNN inference. Similar to distributed inference
    which partition one model onto multiple machines, heterogeneous computing also
    requires dedicated model sharding algorithms and co-designs the model sharding
    with the heterogeneous computing devices, and also considers the inter-processor
    communication, *etc.*
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 异构计算 与异构内存类似，异构计算是通过将不同处理器的计算能力混合来实现的概念，通常用于多个类型的计算处理器（如CPU、GPU和其他加速器）的SoC中的大型模型推断。例如，Synergy
    [[58](#bib.bib58)] 提出了一个用于嵌入式异构SoC的高吞吐量CNN的HW/SW框架。它利用所有可用的片上资源，包括双核ARM处理器、FPGA和加速器，共同计算CNN推断。类似于将一个模型分配到多个机器上的分布式推断，异构计算也需要专门的模型分片算法，并将模型分片与异构计算设备共同设计，还要考虑处理器间的通信，*等*。
- en: 5 Conclusion
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this survey, we propose a a novel computing paradigm taxonomy to characterize
    ongoing large-scale deep learning system (LDS) optimization works. We summarize
    the new optimization objectives, elaborate new technical design perspectives,
    and provide the insights for future LDS optimization. By summarizing the existing
    works, we hope that this survey could provide a comprehensive summary on emerging
    challenges, opportunities, and innovations, and thus motivates new innovations
    in LDS operation and optimization.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调查中，我们提出了一种新的计算范式分类法，以描述正在进行的大规模深度学习系统 (LDS) 优化工作。我们总结了新的优化目标，详细阐述了新的技术设计视角，并提供了对未来
    LDS 优化的见解。通过总结现有工作，我们希望这项调查能够提供对新兴挑战、机会和创新的全面总结，从而激发 LDS 操作和优化的新创新。
- en: References
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1]'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1]'
- en: 'Almeida et al. [2019] Mario Almeida, Stefanos Laskaridis, Ilias Leontiadis,
    Stylianos I Venieris, and Nicholas D Lane. 2019. EmBench: Quantifying performance
    variations of deep neural networks across modern commodity devices. In *The 3rd
    international workshop on deep learning for mobile systems and applications*.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Almeida 等人 [2019] Mario Almeida, Stefanos Laskaridis, Ilias Leontiadis, Stylianos
    I Venieris, 和 Nicholas D Lane. 2019. EmBench: 量化现代商品设备上深度神经网络的性能变化。发表于 *第3届国际移动系统与应用深度学习研讨会*。'
- en: Brown et al. [2020] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020. Language models are few-shot learners. *arXiv preprint
    arXiv:2005.14165* (2020).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等人. 2020. 语言模型是少量样本学习者。*arXiv 预印本 arXiv:2005.14165* (2020)。
- en: 'Chen et al. [2018] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng,
    Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al.
    2018. $\{$TVM$\}$: An automated end-to-end optimizing compiler for deep learning.
    In *13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation
    ($\{$OSDI$\}$ 18)*. 578–594.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2018] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie
    Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze 等人. 2018. $\{$TVM$\}$:
    一个用于深度学习的自动化端到端优化编译器。发表于 *第13届 $\{$USENIX$\}$ 操作系统设计与实现研讨会 ($\{$OSDI$\}$ 18)*。578–594。'
- en: Choi et al. [2021] Seungbeom Choi, Sunho Lee, Yeonjae Kim, Jongse Park, Youngjin
    Kwon, and Jaehyuk Huh. 2021. Multi-model Machine Learning Inference Serving with
    GPU Spatial Partitioning. *arXiv preprint arXiv:2109.01611* (2021).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等人 [2021] Seungbeom Choi, Sunho Lee, Yeonjae Kim, Jongse Park, Youngjin
    Kwon, 和 Jaehyuk Huh. 2021. 多模型机器学习推理服务与 GPU 空间分区。*arXiv 预印本 arXiv:2109.01611*
    (2021)。
- en: 'Choi and Rhu [2020] Yujeong Choi and Minsoo Rhu. 2020. Prema: A predictive
    multi-task scheduling algorithm for preemptible neural processing units. In *2020
    IEEE International Symposium on High Performance Computer Architecture (HPCA)*.
    IEEE, 220–233.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choi 和 Rhu [2020] Yujeong Choi 和 Minsoo Rhu. 2020. Prema: 一种预测的多任务调度算法，用于可抢占神经处理单元。发表于
    *2020 IEEE 国际高性能计算架构研讨会 (HPCA)*。IEEE, 220–233。'
- en: 'Copeland et al. [2015] Marshall Copeland, Julian Soh, Anthony Puca, Mike Manning,
    and David Gollob. 2015. Microsoft Azure. *New York, NY, USA:: Apress* (2015).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Copeland 等人 [2015] Marshall Copeland, Julian Soh, Anthony Puca, Mike Manning,
    和 David Gollob. 2015. Microsoft Azure. *纽约, NY, USA:: Apress* (2015)。'
- en: 'Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等人 [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2018. Bert: 用于语言理解的深度双向变换器的预训练。*arXiv 预印本 arXiv:1810.04805* (2018)。'
- en: 'Dhakal et al. [2020] Aditya Dhakal, Sameer G Kulkarni, and KK Ramakrishnan.
    2020. Gslice: controlled spatial sharing of gpus for a scalable inference platform.
    In *Proceedings of the 11th ACM Symposium on Cloud Computing*. 492–506.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dhakal 等人 [2020] Aditya Dhakal, Sameer G Kulkarni, 和 KK Ramakrishnan. 2020.
    Gslice: 受控的 GPU 空间共享用于可扩展推理平台。发表于 *第11届 ACM 云计算研讨会会议论文集*。492–506。'
- en: 'Ding et al. [2020] Yaoyao Ding, Ligeng Zhu, Zhihao Jia, Gennady Pekhimenko,
    and Song Han. 2020. IOS: Inter-Operator Scheduler for CNN Acceleration. *arXiv
    preprint arXiv:2011.01302* (2020).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding 等人 [2020] Yaoyao Ding, Ligeng Zhu, Zhihao Jia, Gennady Pekhimenko, 和 Song
    Han. 2020. IOS: 用于 CNN 加速的操作符调度器。*arXiv 预印本 arXiv:2011.01302* (2020)。'
- en: 'Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words:
    Transformers for image recognition at scale. *arXiv:2010.11929* (2020).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, 等. 2020. 一张图像值 16x16 个词: 大规模图像识别中的 Transformers。*arXiv:2010.11929*
    (2020)。'
- en: Facebook [2021] Facebook. 2021. Facebook for Business. https://www.facebook.com/iq/insights-to-go/1820m-facebook-daily-active-users-were-1820m-on-average-for-september.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook [2021] Facebook. 2021. Facebook for Business. https://www.facebook.com/iq/insights-to-go/1820m-facebook-daily-active-users-were-1820m-on-average-for-september。
- en: 'Fegade et al. [2020] Pratik Fegade, Tianqi Chen, Phillip B Gibbons, and Todd C
    Mowry. 2020. Cortex: A Compiler for Recursive Deep Learning Models. *arXiv preprint
    arXiv:2011.01383* (2020).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fegade et al. [2020] Pratik Fegade, Tianqi Chen, Phillip B Gibbons, 和 Todd
    C Mowry. 2020. Cortex: 递归深度学习模型的编译器。*arXiv 预印本 arXiv:2011.01383* (2020)。'
- en: Gharibshah et al. [2020] Zhabiz Gharibshah, Xingquan Zhu, Arthur Hainline, and
    Michael Conway. 2020. Deep learning for user interest and response prediction
    in online display advertising. *Data Science and Engineering* 5, 1 (2020), 12–26.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gharibshah et al. [2020] Zhabiz Gharibshah, Xingquan Zhu, Arthur Hainline, 和
    Michael Conway. 2020. 在线展示广告中用户兴趣和响应预测的深度学习。*数据科学与工程* 5, 1 (2020), 12–26。
- en: 'Ghodrati et al. [2020] Soroush Ghodrati, Byung Hoon Ahn, Joon Kyung Kim, Sean
    Kinzer, Brahmendra Reddy Yatham, Navateja Alla, Hardik Sharma, Mohammad Alian,
    Eiman Ebrahimi, Nam Sung Kim, et al. 2020. Planaria: Dynamic architecture fission
    for spatial multi-tenant acceleration of deep neural networks. In *2020 53rd Annual
    IEEE/ACM International Symposium on Microarchitecture (MICRO)*. IEEE, 681–697.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ghodrati et al. [2020] Soroush Ghodrati, Byung Hoon Ahn, Joon Kyung Kim, Sean
    Kinzer, Brahmendra Reddy Yatham, Navateja Alla, Hardik Sharma, Mohammad Alian,
    Eiman Ebrahimi, Nam Sung Kim, 等. 2020. Planaria: 用于深度神经网络空间多租户加速的动态架构分裂。见于 *2020
    第 53 届 IEEE/ACM 国际微体系结构研讨会 (MICRO)*。IEEE, 681–697。'
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 2016.
    深度残差学习用于图像识别。见于 *IEEE 计算机视觉与模式识别会议论文集*。770–778。
- en: 'Howard et al. [2017] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
    Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:
    Efficient convolutional neural networks for mobile vision applications. *arXiv
    preprint arXiv:1704.04861* (2017).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Howard et al. [2017] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
    Weijun Wang, Tobias Weyand, Marco Andreetto, 和 Hartwig Adam. 2017. Mobilenets:
    高效的卷积神经网络用于移动视觉应用。*arXiv 预印本 arXiv:1704.04861* (2017)。'
- en: 'Huang et al. [2019] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,
    Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al.
    2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism.
    *Advances in neural information processing systems* 32 (2019), 103–112.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. [2019] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,
    Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, 等.
    2019. Gpipe: 使用管道并行性高效训练巨型神经网络。*神经信息处理系统进展* 32 (2019), 103–112。'
- en: Jeon et al. [2019] Myeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee,
    Junjie Qian, Wencong Xiao, and Fan Yang. 2019. Analysis of large-scale multi-tenant
    $\{$GPU$\}$ clusters for $\{$DNN$\}$ training workloads. In *2019 $\{$USENIX$\}$
    Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 19)*. 947–960.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeon et al. [2019] Myeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee,
    Junjie Qian, Wencong Xiao, 和 Fan Yang. 2019. 大规模多租户 $\{$GPU$\}$ 集群的分析，用于 $\{$DNN$\}$
    训练工作负载。见于 *2019 $\{$USENIX$\}$ 年度技术会议 ($\{$USENIX$\}$$\{$ATC$\}$ 19)*。947–960。
- en: Jia et al. [2018] Zhihao Jia, Sina Lin, Charles R Qi, and Alex Aiken. 2018.
    Exploring hidden dimensions in parallelizing convolutional neural networks. *arXiv
    preprint arXiv:1802.04924* (2018).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia et al. [2018] Zhihao Jia, Sina Lin, Charles R Qi, 和 Alex Aiken. 2018. 探索卷积神经网络并行化中的隐含维度。*arXiv
    预印本 arXiv:1802.04924* (2018)。
- en: 'Jia et al. [2019] Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei
    Zaharia, and Alex Aiken. 2019. TASO: optimizing deep learning computation with
    automatic generation of graph substitutions. In *Proceedings of the 27th ACM Symposium
    on Operating Systems Principles*. 47–62.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jia et al. [2019] Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei
    Zaharia, 和 Alex Aiken. 2019. TASO: 通过自动生成图替代优化深度学习计算。见于 *第 27 届 ACM 操作系统原理研讨会论文集*。47–62。'
- en: Justesen et al. [2019] Niels Justesen, Philip Bontrager, Julian Togelius, and
    Sebastian Risi. 2019. Deep learning for video game playing. *IEEE Transactions
    on Games* 12, 1 (2019), 1–20.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Justesen 等人 [2019] 尼尔斯·贾斯滕、菲利普·邦特拉格、朱利安·托戈利乌斯、塞巴斯蒂安·里西。2019。深度学习在视频游戏中的应用。*IEEE游戏期刊*
    12, 1 (2019)，1–20。
- en: 'Kolesnikov et al. [2020] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
    Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. 2020. Big transfer
    (bit): General visual representation learning. In *Computer Vision–ECCV 2020:
    16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V
    16*. Springer, 491–507.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolesnikov 等人 [2020] 亚历山大·科列斯尼科夫、卢卡斯·贝耶、夏华·翟、琼·普伊克塞弗、杰西卡·杨、西尔万·热利、尼尔·霍尔斯比。2020。大转移（BIT）：通用视觉表示学习。在*计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第V部分 16*。Springer，491–507。
- en: 'Li et al. [2019] Xin Li, Yiming Zhou, Zheng Pan, and Jiashi Feng. 2019. Partial
    order pruning: for best speed/accuracy trade-off in neural architecture search.
    In *Proceedings of the IEEE Conference on computer vision and pattern recognition*.
    9145–9153.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2019] 李鑫、周一鸣、潘正、冯佳施。2019。部分顺序剪枝：神经网络架构搜索中的最佳速度/准确性权衡。在*IEEE计算机视觉与模式识别会议论文集*。9145–9153。
- en: 'Lin et al. [2020] Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang
    Zhang, Yonghong Tian, and Ling Shao. 2020. HRank: Filter Pruning using High-Rank
    Feature Map. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*. 1529–1538.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等人 [2020] 林名宝、季荣荣、王艳、张艺辰、张宝昌、田永宏、邵凌。2020。HRank：基于高秩特征图的滤波剪枝。在*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*。1529–1538。
- en: 'Liu et al. [2021] Jiawen Liu, Dong Li, Roberto Gioiosa, and Jiajia Li. 2021.
    Athena: high-performance sparse tensor contraction sequence on heterogeneous memory.
    In *Proceedings of the ACM International Conference on Supercomputing*. 190–202.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2021] 刘家文、李东、罗伯托·焦约萨、李佳佳。2021。Athena：异构内存上的高性能稀疏张量收缩序列。在*ACM国际超级计算会议论文集*。190–202。
- en: Lui et al. [2021] Michael Lui, Yavuz Yetim, Özgür Özkan, Zhuoran Zhao, Shin-Yeh
    Tsai, Carole-Jean Wu, and Mark Hempstead. 2021. Understanding capacity-driven
    scale-out neural recommendation inference. In *2021 IEEE International Symposium
    on Performance Analysis of Systems and Software (ISPASS)*. IEEE, 162–171.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lui 等人 [2021] 迈克尔·刘、雅武兹·耶蒂姆、厄尔古尔·厄兹坎、赵卓然、蔡新叶、卡罗尔-简·吴、马克·亨普斯特德。2021。理解基于容量驱动的规模扩展神经推荐推理。在*2021
    IEEE国际系统与软件性能分析研讨会（ISPASS）*。IEEE，162–171。
- en: 'Madhuri and Sowjanya [2016] T Madhuri and P Sowjanya. 2016. Microsoft Azure
    v/s Amazon AWS cloud services: A comparative study. *International Journal of
    Innovative Research in Science, Engineering and Technology* 5, 3 (2016), 3904–3907.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madhuri 和 Sowjanya [2016] T Madhuri 和 P Sowjanya。2016。微软Azure与亚马逊AWS云服务：比较研究。*国际创新研究期刊：科学、工程与技术*
    5, 3 (2016)，3904–3907。
- en: Mendoza et al. [2021] Daniel Mendoza, Francisco Romero, Qian Li, Neeraja J Yadwadkar,
    and Christos Kozyrakis. 2021. Interference-Aware Scheduling for Inference Serving.
    In *Proceedings of the 1st Workshop on Machine Learning and Systems*. 80–88.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mendoza 等人 [2021] 丹尼尔·门多萨、弗朗西斯科·罗梅罗、李倩、内拉贾·J·亚德瓦德卡尔、克里斯托斯·科齐拉基斯。2021。干扰感知调度用于推理服务。在*第1届机器学习与系统研讨会论文集*。80–88。
- en: Microsoft [2020] Nvidia Microsoft. 2020. Using DeepSpeed and Megatron to Train
    Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language
    Model. https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软 [2020] Nvidia 微软。2020。使用DeepSpeed和Megatron训练Megatron-Turing NLG 530B，世界上最大最强的生成语言模型。https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/。
- en: 'Narayanan et al. [2019] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek
    Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia.
    2019. PipeDream: generalized pipeline parallelism for DNN training. In *Proceedings
    of the 27th ACM Symposium on Operating Systems Principles*. 1–15.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayanan 等人 [2019] 深克·纳拉亚南、亚伦·哈尔拉普、阿马尔·帕尼莎耶、维维克·谢沙德里、尼基尔·R·德万努尔、格雷戈里·R·甘格、菲利普·B·吉本斯、马泰·扎哈里亚。2019。PipeDream：用于DNN训练的广义管道并行。在*第27届ACM操作系统原理研讨会论文集*。1–15。
- en: Naumov et al. [2019] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi,
    Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean
    Wu, Alisson G Azzolini, et al. 2019. Deep learning recommendation model for personalization
    and recommendation systems. *arXiv preprint arXiv:1906.00091* (2019).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naumov等人 [2019] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu
    Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean
    Wu, Alisson G Azzolini 等人. 2019. 深度学习推荐模型用于个性化和推荐系统。*arXiv预印本 arXiv:1906.00091*
    (2019)。
- en: NVIDIA [2020a] NVIDIA. 2020a. NVIDIA Multi Instance GPU (MIG). https://docs.nvidia.com/datacenter/tesla/mig-user-guide/.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA [2020a] NVIDIA. 2020a. NVIDIA 多实例 GPU (MIG)。https://docs.nvidia.com/datacenter/tesla/mig-user-guide/。
- en: NVIDIA [2020b] NVIDIA. 2020b. NVIDIA Multi Process Service (MPS). https://docs.nvidia.com/deploy/pdf/CUDA-Multi-Process-Service-Overview.pdf.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA [2020b] NVIDIA. 2020b. NVIDIA 多进程服务 (MPS)。https://docs.nvidia.com/deploy/pdf/CUDA-Multi-Process-Service-Overview.pdf。
- en: Park et al. [2019] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.
    2019. Semantic image synthesis with spatially-adaptive normalization. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2337–2346.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人 [2019] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, 和 Jun-Yan Zhu. 2019.
    带有空间自适应归一化的语义图像合成。见于*IEEE/CVF计算机视觉与模式识别会议论文集*。2337–2346。
- en: Reports [2021] Market Reports. 2021. Global Data Center Accelerator Market Size,
    Status and Forecast 2020-2025. https://www.mynewsdesk.com/brandessence/pressreleases/data-center-accelerator-market-size-2021-cagr-38-dot-7-percent-3112488.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reports [2021] 市场报告. 2021. 全球数据中心加速器市场规模、状态及预测 2020-2025。https://www.mynewsdesk.com/brandessence/pressreleases/data-center-accelerator-market-size-2021-cagr-38-dot-7-percent-3112488。
- en: 'Sandler et al. [2018a] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    and Liang-Chieh Chen. 2018a. Mobilenetv2: Inverted residuals and linear bottlenecks.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    4510–4520.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sandler等人 [2018a] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    和 Liang-Chieh Chen. 2018a. Mobilenetv2: 倒置残差和线性瓶颈。见于*IEEE计算机视觉与模式识别会议论文集*。4510–4520。'
- en: 'Sandler et al. [2018b] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    and Liang-Chieh Chen. 2018b. Mobilenetv2: Inverted residuals and linear bottlenecks.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    4510–4520.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sandler等人 [2018b] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    和 Liang-Chieh Chen. 2018b. Mobilenetv2: 倒置残差和线性瓶颈。见于*IEEE计算机视觉与模式识别会议论文集*。4510–4520。'
- en: Shen et al. [2017] Dinggang Shen, Guorong Wu, and Heung-Il Suk. 2017. Deep learning
    in medical image analysis. *Annual review of biomedical engineering* 19 (2017),
    221–248.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen等人 [2017] Dinggang Shen, Guorong Wu, 和 Heung-Il Suk. 2017. 医学图像分析中的深度学习。*生物医学工程年度回顾*
    19 (2017), 221–248。
- en: 'Shoeybi et al. [2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick
    LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion
    parameter language models using model parallelism. *arXiv preprint arXiv:1909.08053*
    (2019).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shoeybi等人 [2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,
    Jared Casper, 和 Bryan Catanzaro. 2019. Megatron-lm: 使用模型并行训练多亿参数语言模型。*arXiv预印本
    arXiv:1909.08053* (2019)。'
- en: 'Singh et al. [2017] Shashi Pal Singh, Ajai Kumar, Hemant Darbari, Lenali Singh,
    Anshika Rastogi, and Shikha Jain. 2017. Machine translation using deep learning:
    An overview. In *2017 international conference on computer, communications and
    electronics (comptelix)*. IEEE, 162–167.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh等人 [2017] Shashi Pal Singh, Ajai Kumar, Hemant Darbari, Lenali Singh, Anshika
    Rastogi, 和 Shikha Jain. 2017. 使用深度学习的机器翻译：概述。见于*2017年国际计算机、通信与电子会议 (comptelix)*。IEEE,
    162–167。
- en: 'Singhal et al. [2017] Ayush Singhal, Pradeep Sinha, and Rakesh Pant. 2017.
    Use of deep learning in modern recommendation system: A summary of recent works.
    *arXiv preprint arXiv:1712.07525* (2017).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal等人 [2017] Ayush Singhal, Pradeep Sinha, 和 Rakesh Pant. 2017. 现代推荐系统中的深度学习应用：近期研究综述。*arXiv预印本
    arXiv:1712.07525* (2017)。
- en: Soifer et al. [2019] Jonathan Soifer, Jason Li, Mingqin Li, Jeffrey Zhu, Yingnan
    Li, Yuxiong He, Elton Zheng, Adi Oltean, Maya Mosyak, Chris Barnes, et al. 2019.
    Deep learning inference service at microsoft. In *2019 $\{$USENIX$\}$ Conference
    on Operational Machine Learning (OpML 19)*. 15–17.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soifer等人 [2019] Jonathan Soifer, Jason Li, Mingqin Li, Jeffrey Zhu, Yingnan
    Li, Yuxiong He, Elton Zheng, Adi Oltean, Maya Mosyak, Chris Barnes, 等人. 2019.
    微软的深度学习推理服务。见于*2019年USENIX运维机器学习会议 (OpML 19)*。15–17。
- en: 'Song et al. [2017] Linghao Song, Xuehai Qian, Hai Li, and Yiran Chen. 2017.
    Pipelayer: A pipelined reram-based accelerator for deep learning. In *2017 IEEE
    International Symposium on High Performance Computer Architecture (HPCA)*. IEEE,
    541–552.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等 [2017] Linghao Song, Xuehai Qian, Hai Li, 和 Yiran Chen. 2017. Pipelayer：一种基于管道的
    RERAM 加速器用于深度学习。发表于 *2017 IEEE 国际高性能计算架构研讨会 (HPCA)*。IEEE, 541–552。
- en: Sun et al. [2019] Yifan Sun, Nicolas Bohm Agostini, Shi Dong, and David Kaeli.
    2019. Summarizing CPU and GPU design trends with product data. *arXiv preprint
    arXiv:1911.11313* (2019).
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 [2019] Yifan Sun, Nicolas Bohm Agostini, Shi Dong, 和 David Kaeli. 2019.
    用产品数据总结 CPU 和 GPU 设计趋势。*arXiv preprint arXiv:1911.11313* (2019)。
- en: 'Tan et al. [2019] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark
    Sandler, Andrew Howard, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architecture
    search for mobile. In *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition*. 2820–2828.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等 [2019] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler,
    Andrew Howard, 和 Quoc V Le. 2019. Mnasnet：面向移动平台的神经架构搜索。发表于 *IEEE 计算机视觉与模式识别会议论文集*。2820–2828。
- en: 'Tan and Le [2019] Mingxing Tan and Quoc V Le. 2019. Efficientnet: Rethinking
    model scaling for convolutional neural networks. *arXiv preprint arXiv:1905.11946*
    (2019).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 和 Le [2019] Mingxing Tan 和 Quoc V Le. 2019. Efficientnet：重新思考卷积神经网络的模型缩放。*arXiv
    preprint arXiv:1905.11946* (2019)。
- en: 'Wan et al. [2021] Hu Wan, Xuan Sun, Yufei Cui, Chia-Lin Yang, Tei-Wei Kuo,
    and Chun Jason Xue. 2021. FlashEmbedding: storing embedding tables in SSD for
    large-scale recommender systems. In *Proceedings of the 12th ACM SIGOPS Asia-Pacific
    Workshop on Systems*. 9–16.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等 [2021] Hu Wan, Xuan Sun, Yufei Cui, Chia-Lin Yang, Tei-Wei Kuo, 和 Chun
    Jason Xue. 2021. FlashEmbedding：将嵌入表存储在 SSD 中以支持大规模推荐系统。发表于 *第12届 ACM SIGOPS 亚太系统研讨会论文集*。9–16。
- en: Wesolowski et al. [2021] Lukasz Wesolowski, Bilge Acun, Valentin Andrei, Adnan
    Aziz, Gisle Dankel, Christopher Gregg, Xiaoqiao Meng, Cyril Meurillon, Denis Sheahan,
    Lei Tian, et al. 2021. Datacenter-Scale Analysis and Optimization of GPU Machine
    Learning Workloads. *IEEE Micro* 41, 5 (2021), 101–112.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wesolowski 等 [2021] Lukasz Wesolowski, Bilge Acun, Valentin Andrei, Adnan Aziz,
    Gisle Dankel, Christopher Gregg, Xiaoqiao Meng, Cyril Meurillon, Denis Sheahan,
    Lei Tian 等 2021. 数据中心规模的 GPU 机器学习工作负载分析与优化。*IEEE Micro* 41, 5 (2021), 101–112。
- en: 'Wilkening et al. [2021] Mark Wilkening, Udit Gupta, Samuel Hsia, Caroline Trippel,
    Carole-Jean Wu, David Brooks, and Gu-Yeon Wei. 2021. RecSSD: near data processing
    for solid state drive based recommendation inference. In *Proceedings of the 26th
    ACM International Conference on Architectural Support for Programming Languages
    and Operating Systems*. 717–729.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilkening 等 [2021] Mark Wilkening, Udit Gupta, Samuel Hsia, Caroline Trippel,
    Carole-Jean Wu, David Brooks, 和 Gu-Yeon Wei. 2021. RecSSD：基于固态硬盘的推荐推理近数据处理。发表于
    *第26届 ACM 国际编程语言和操作系统架构支持会议论文集*。717–729。
- en: 'Wu et al. [2020] Xiaorui Wu, Hong Xu, and Yi Wang. 2020. Irina: Accelerating
    DNN Inference with Efficient Online Scheduling. In *4th Asia-Pacific Workshop
    on Networking*. 36–43.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 [2020] Xiaorui Wu, Hong Xu, 和 Yi Wang. 2020. Irina：通过高效的在线调度加速 DNN 推理。发表于
    *第4届亚太网络研讨会*。36–43。
- en: 'Xiao et al. [2020] Wencong Xiao, Shiru Ren, Yong Li, Yang Zhang, Pengyang Hou,
    Zhi Li, Yihui Feng, Wei Lin, and Yangqing Jia. 2020. AntMan: Dynamic Scaling on
    $\{$GPU$\}$ Clusters for Deep Learning. In *14th $\{$USENIX$\}$ Symposium on Operating
    Systems Design and Implementation ($\{$OSDI$\}$ 20)*. 533–548.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等 [2020] Wencong Xiao, Shiru Ren, Yong Li, Yang Zhang, Pengyang Hou, Zhi
    Li, Yihui Feng, Wei Lin, 和 Yangqing Jia. 2020. AntMan：用于深度学习的 $\{$GPU$\}$ 集群动态缩放。发表于
    *第14届 $\{$USENIX$\}$ 操作系统设计与实现研讨会 ($\{$OSDI$\}$ 20)*。533–548。
- en: Yu and et al. [2021] Fuxun Yu and et al. 2021. Automated Runtime-Aware Scheduling
    for Multi-Tenant DNN Inference on GPU. In *Proceedings of the 40th IEEE International
    Conference on Computer Aided Design (ICCAD)*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 和等 [2021] Fuxun Yu 等 2021. 针对 GPU 上多租户 DNN 推理的自动化运行时感知调度。发表于 *第40届 IEEE 国际计算机辅助设计会议
    (ICCAD) 论文集*。
- en: 'Zeng et al. [2020] Liekang Zeng, Xu Chen, Zhi Zhou, Lei Yang, and Junshan Zhang.
    2020. Coedge: Cooperative dnn inference with adaptive workload partitioning over
    heterogeneous edge devices. *IEEE/ACM Transactions on Networking* 29, 2 (2020),
    595–608.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等 [2020] Liekang Zeng, Xu Chen, Zhi Zhou, Lei Yang, 和 Junshan Zhang. 2020.
    Coedge：在异构边缘设备上进行自适应负载分配的合作 DNN 推理。*IEEE/ACM Transactions on Networking* 29, 2
    (2020), 595–608。
- en: 'Zhang et al. [2018] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
    2018. Shufflenet: An extremely efficient convolutional neural network for mobile
    devices. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*. 6848–6856.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2018] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin 和 Jian Sun。2018年。《Shufflenet:
    一种极其高效的移动设备卷积神经网络》。在*IEEE计算机视觉与模式识别会议论文集*中，6848–6856页。'
- en: 'Zhao et al. [2018] Zhuoran Zhao, Kamyar Mirzazad Barijough, and Andreas Gerstlauer.
    2018. Deepthings: Distributed adaptive deep learning inference on resource-constrained
    iot edge clusters. *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems* 37, 11 (2018), 2348–2359.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. [2018] Zhuoran Zhao, Kamyar Mirzazad Barijough 和 Andreas Gerstlauer。2018年。《Deepthings:
    在资源受限的物联网边缘集群上进行分布式自适应深度学习推理》。*IEEE集成电路和系统计算机辅助设计汇刊* 37卷，11期（2018年），2348–2359页。'
- en: 'Zheng et al. [2020] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao
    Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, et al. 2020.
    Ansor: Generating high-performance tensor programs for deep learning. In *14th
    $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$
    20)*. 863–879.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. [2020] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody
    Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen 等人。2020年。《Ansor:
    生成高性能张量程序用于深度学习》。在*第14届$\{$USENIX$\}$操作系统设计与实现研讨会（$\{$OSDI$\}$ 20）*上，863–879页。'
- en: 'Zhong et al. [2019] Guanwen Zhong, Akshat Dubey, Cheng Tan, and Tulika Mitra.
    2019. Synergy: An hw/sw framework for high throughput cnns on embedded heterogeneous
    soc. *ACM Transactions on Embedded Computing Systems (TECS)* 18, 2 (2019), 1–23.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong et al. [2019] Guanwen Zhong, Akshat Dubey, Cheng Tan 和 Tulika Mitra。2019年。《Synergy:
    一种用于嵌入式异构系统上高吞吐量卷积神经网络的硬件/软件框架》。*ACM嵌入式计算系统汇刊（TECS）* 18卷，2期（2019年），1–23页。'
- en: Zhou et al. [2019] Li Zhou, Hao Wen, Radu Teodorescu, and David HC Du. 2019.
    Distributing deep neural networks with containerized partitions at the edge. In
    *2nd $\{$USENIX$\}$ Workshop on Hot Topics in Edge Computing (HotEdge 19)*.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2019] Li Zhou, Hao Wen, Radu Teodorescu 和 David HC Du。2019年。《在边缘计算中使用容器化分区分布深度神经网络》。在*第2届$\{$USENIX$\}$边缘计算热点话题研讨会（HotEdge
    19）*上。
