- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:31:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:31:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2406.00146] A survey of deep learning audio generation methods'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2406.00146] 深度学习音频生成方法的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.00146](https://ar5iv.labs.arxiv.org/html/2406.00146)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.00146](https://ar5iv.labs.arxiv.org/html/2406.00146)
- en: A survey of deep learning audio generation methods
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习音频生成方法的调查
- en: Matej Božić1, Marko Horvat2 Department of Applied Computing
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Matej Božić1, Marko Horvat2 应用计算系
- en: University of Zagreb, Faculty of Electrical Engineering and Computing, Zagreb,
    Croatia
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 萨格勒布大学，电气工程与计算机学院，克罗地亚萨格勒布
- en: 1matej.bozic@fer.hr 2marko.horvat3@fer.hr
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 1matej.bozic@fer.hr 2marko.horvat3@fer.hr
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This article presents a review of typical techniques used in three distinct
    aspects of deep learning model development for audio generation. In the first
    part of the article, we provide an explanation of audio representations, beginning
    with the fundamental audio waveform. We then progress to the frequency domain,
    with an emphasis on the attributes of human hearing, and finally introduce a relatively
    recent development. The main part of the article focuses on explaining basic and
    extended deep learning architecture variants, along with their practical applications
    in the field of audio generation. The following architectures are addressed: 1)
    Autoencoders 2) Generative adversarial networks 3) Normalizing flows 4) Transformer
    networks 5) Diffusion models. Lastly, we will examine four distinct evaluation
    metrics that are commonly employed in audio generation. This article aims to offer
    novice readers and beginners in the field a comprehensive understanding of the
    current state of the art in audio generation methods as well as relevant studies
    that can be explored for future research.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文回顾了音频生成深度学习模型开发中的三种不同方面的典型技术。文章的第一部分解释了音频表示，从基础的音频波形开始。接着我们进展到频域，重点介绍了人类听觉的特性，并最终引入了相对较新的发展。文章的主要部分集中在解释基本和扩展的深度学习架构变体及其在音频生成领域的实际应用。讨论的架构包括：1）自编码器
    2）生成对抗网络 3）归一化流 4）变换器网络 5）扩散模型。最后，我们将考察四种在音频生成中常用的评价指标。本文旨在为新手读者和该领域的初学者提供对音频生成方法当前最前沿状态的全面了解，以及可以探索的相关研究以进行未来的研究。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep learning, Audio representations, Audio generation, Generative models, Sound
    synthesis
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，音频表示，音频生成，生成模型，声音合成
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: The trend towards deep learning in Computer Vision (CV) and Natural Language
    Processing (NLP) has also reached the field of audio generation [[1](#bib.bibx1)].
    Deep learning has allowed us to move away from the complexity of hand-crafted
    features towards simple representations by letting the depth of the model create
    more complex mappings. We define audio generation as any method whose outputs
    are audio and cannot be derived solely from the inputs. Even though tasks such
    as text-to-speech involve translation from the text domain to the speech domain,
    there are many unknowns, such as the speaker’s voice. This means that the models
    have to invent or generate information for the translation to work. There are
    many applications for audio generation. We can create human-sounding voice assistants,
    generate ambient sounds for games or movies based on the current visual input,
    create various music samples to help music producers with ideas or composition,
    and much more. The structure of the presented survey on deep learning audio generation
    methods is illustrated in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A survey
    of deep learning audio generation methods").
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉（CV）和自然语言处理（NLP）领域对深度学习的趋势也已扩展到音频生成领域[[1](#bib.bibx1)]。深度学习使我们能够从手工制作的复杂特征转向简单的表示，通过让模型的深度创建更复杂的映射。我们将音频生成定义为任何其输出为音频且不能仅从输入中推导出的的方法。即使像文本到语音这样的任务涉及从文本领域到语音领域的转换，但仍有许多未知因素，例如说话者的声音。这意味着模型必须发明或生成信息以使翻译工作。音频生成有许多应用。我们可以创建听起来像人的语音助手，基于当前的视觉输入生成游戏或电影的环境声音，创建各种音乐样本以帮助音乐制作人获得灵感或进行创作，等等。图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A survey of deep learning audio generation methods")展示了对深度学习音频生成方法的调查的结构。
- en: This article will mainly focus on deep learning methods, as the field seems
    to be developing in this direction. Nevertheless, section [III](#S3 "III Background
    ‣ A survey of deep learning audio generation methods") will examine the development
    of audio generation methods over the years, starting around the 1970s. We consider
    this section important because, just as deep learning methods have re-emerged,
    there may be a time when audio generation methods that are now obsolete become
    state-of-the-art again. The goal is to take a broad but shallow look at the field
    of audio generation. Some areas, such as text-to-speech, will be more heavily
    represented as they have received more attention, but an attempt has been made
    to include many different subfields. This article does not attempt to present
    all possible methods but only introduces the reader to some of the popular methods
    in the field. Each listing of works on a topic is sorted so that the most recent
    articles are at the end.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将主要关注深度学习方法，因为该领域似乎正朝着这个方向发展。然而，第[III](#S3 "III 背景 ‣ 深度学习音频生成方法综述")节将回顾音频生成方法多年来的发展，从大约1970年代开始。我们认为这一节很重要，因为正如深度学习方法重新出现一样，可能会有一天现在已经过时的音频生成方法会再次成为最前沿技术。我们的目标是对音频生成领域进行广泛但浅显的审视。某些领域，如文本到语音，将会有更重的表现，因为它们受到了更多关注，但我们尝试包括了许多不同的子领域。本文并不尝试介绍所有可能的方法，而是向读者介绍该领域一些流行的方法。每个主题下的工作列表都是按排序的，以便最新的文章在最后。
- en: 'The article is structured as follows: section [II](#S2 "II Related work ‣ A
    survey of deep learning audio generation methods") presents previous work dealing
    with deep learning in audio, section [III](#S3 "III Background ‣ A survey of deep
    learning audio generation methods") gives a brief overview of previous audio generation
    methods in text-to-speech and music generation, section [IV](#S4 "IV Audio features
    ‣ A survey of deep learning audio generation methods") deals with the two most
    prominent features and a recent advancement, section [V](#S5 "V Architectures
    ‣ A survey of deep learning audio generation methods") discusses five deep learning
    architectures and some of their popular extensions, and finally, section [VI](#S6
    "VI Evaluation metrics ‣ A survey of deep learning audio generation methods")
    looks at measuring the performance of generation models, some of which are specific
    to audio generation, while others are more generally applicable.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的结构如下：第[II](#S2 "II 相关工作 ‣ 深度学习音频生成方法综述")节介绍了处理音频中的深度学习的前期工作，第[III](#S3 "III
    背景 ‣ 深度学习音频生成方法综述")节简要概述了文本到语音和音乐生成中的前期音频生成方法，第[IV](#S4 "IV 音频特征 ‣ 深度学习音频生成方法综述")节讨论了两个最显著的特征和一个最近的进展，第[V](#S5
    "V 架构 ‣ 深度学习音频生成方法综述")节讨论了五种深度学习架构及其一些流行的扩展，最后，第[VI](#S6 "VI 评估指标 ‣ 深度学习音频生成方法综述")节着眼于评估生成模型的性能，其中一些是特定于音频生成的，而其他则更具通用性。
- en: '{forest}'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: for tree= forked edges, grow’=0, draw, rounded corners , [Survey, rotate=90
    [[IV](#S4 "IV Audio features ‣ A survey of deep learning audio generation methods").
    Audio features, for tree=fill=red!45, section [Raw waveform, fill=red!30, subsection]
    [Mel-spectrogram, fill=red!30, subsection] [Neural codecs, fill=red!30, subsection]
    ] [[V](#S5 "V Architectures ‣ A survey of deep learning audio generation methods").
    Architectures, for tree=fill=brown!45, section [Auto-encoder, fill=brown!30, subsection]
    [Generative adversarial networks, fill=brown!30, subsection] [Normalizing flows,
    fill=brown!30, subsection] [Transformer networks, fill=brown!30, subsection] [Diffusion
    models, fill=brown!30, subsection] ] [[VI](#S6 "VI Evaluation metrics ‣ A survey
    of deep learning audio generation methods"). Evaluation metrics, for tree=fill=orange!45,
    section [Human evaluation, fill=orange!30, subsection] [Inception score, fill=orange!30,
    subsection] [Fréchet distance, fill=orange!30, subsection] [Kullback-Leibler divergence,
    fill=orange!30, subsection] ] ]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树=分叉边，grow’=0，绘制，圆角，[调查，旋转=90 [[IV](#S4 "IV 音频特征 ‣ 深度学习音频生成方法的调查")。音频特征，对于树=填充=red!45，部分
    [原始波形，填充=red!30，小节] [梅尔谱图，填充=red!30，小节] [神经编解码器，填充=red!30，小节] ] [[V](#S5 "V 架构
    ‣ 深度学习音频生成方法的调查")。架构，对于树=填充=brown!45，部分 [自编码器，填充=brown!30，小节] [生成对抗网络，填充=brown!30，小节]
    [归一化流，填充=brown!30，小节] [变换器网络，填充=brown!30，小节] [扩散模型，填充=brown!30，小节] ] [[VI](#S6
    "VI 评估指标 ‣ 深度学习音频生成方法的调查")。评估指标，对于树=填充=orange!45，部分 [人工评估，填充=orange!30，小节] [起始评分，填充=orange!30，小节]
    [Fréchet 距离，填充=orange!30，小节] [Kullback-Leibler 散度，填充=orange!30，小节] ] ]
- en: 'Figure 1: The main sections of the survey.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：调查的主要部分。
- en: II Related work
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: In this section, we will mention some of the works that are good sources for
    further research in the field of audio generation. Some of them investigate only
    a specific model architecture or sub-area, while others, like this work, show
    a broader view.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提到一些在音频生成领域进行进一步研究的良好来源。其中一些只研究特定的模型架构或子领域，而其他像这项工作则展示了更广泛的视角。
- en: In [[2](#bib.bibx2)], deep learning discriminative and generative architectures
    are discussed, along with their applications in speech and music synthesis. The
    article covers discriminative neural networks such as Multi-Layer Perceptron (MLP),
    Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN), as well
    as generative neural networks like Variational Autoencoders (VAE) and Deep Belief
    Networks (DBN). They also describe generative adversarial networks (GAN), their
    flaws, and enhancement strategies (with Wasserstein GAN as a standout). The study
    mainly focuses on speech generation and doesn’t focus much on different hybrid
    models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[2](#bib.bibx2)] 中，讨论了深度学习的判别性和生成性架构及其在语音和音乐合成中的应用。文章涵盖了判别性神经网络，如多层感知器（MLP）、卷积神经网络（CNN）和递归神经网络（RNN），以及生成性神经网络，如变分自编码器（VAE）和深度置信网络（DBN）。他们还描述了生成对抗网络（GAN），其缺陷以及增强策略（其中
    Wasserstein GAN 为突出）。该研究主要集中在语音生成上，对不同的混合模型关注较少。
- en: In contrast, [[3](#bib.bibx3)] emphasizes other areas of modeling, including
    feature representations, loss functions, data, and evaluation methods. It also
    investigates a variety of additional application fields, including enhancement
    as well as those outside of audio generation, such as source separation, audio
    classification, and tagging. They describe various audio aspects that are not
    covered here, such as the mel frequency cepstral coefficients (MFCC) and the constant-Q
    spectrogram. They do not cover as many architectures, but they do provide domain-specific
    datasets and evaluation methods.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，[[3](#bib.bibx3)] 强调了建模的其他领域，包括特征表示、损失函数、数据和评估方法。它还调查了各种额外的应用领域，包括增强以及音频生成之外的领域，如源分离、音频分类和标记。他们描述了一些在此未涵盖的音频方面，如梅尔频率倒谱系数（MFCC）和常数Q谱图。他们覆盖的架构不如其他文献多，但提供了特定领域的数据集和评估方法。
- en: 'Unlike previous works, [[4](#bib.bibx4)] attempts to comprehensively examine
    a specific field of audio generation. This study considers five dimensions of
    music generation: objective, representation, architecture, challenge, and strategy.
    It looks at a variety of representations, both domain-specific and more general.
    Explains the fundamentals of music theory, including notes, rhythm, and chords.
    Introduces various previously established architectures such as MLP, VAE, RNN,
    CNN, and GAN, as well as some new ones like the Restricted Boltzmann Machine (RBM).
    Finally, it discusses the many challenges of music generation and ways for overcoming
    them. The work is quite extensive; however, some sections may benefit from a more
    detailed explanation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的研究不同，[[4](#bib.bibx4)] 试图全面考察音频生成的特定领域。本研究考虑了音乐生成的五个维度：目标、表示、架构、挑战和策略。它考察了各种表示方法，包括领域特定的和更一般的表示。解释了音乐理论的基本原理，包括音符、节奏和和弦。介绍了各种已建立的架构，如MLP、VAE、RNN、CNN和GAN，以及一些新兴的架构，如限制玻尔兹曼机（RBM）。最后，它讨论了音乐生成的许多挑战及其解决方法。这项工作相当广泛，但某些部分可能需要更详细的解释。
- en: '[[5](#bib.bibx5)] is another work that explores the subject of music generation
    and includes music translation. It discusses data representation, generative neural
    networks, and two popular DNN-based synthesizers. It discusses the issue of long-term
    dependence and how conditioning might alleviate it. Explains the autoregressive
    (AR) and normalized flow (NF) models, as well as VAE and GAN.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[[5](#bib.bibx5)] 是另一项探索音乐生成主题的工作，包括音乐翻译。它讨论了数据表示、生成神经网络和两个流行的DNN基础合成器。讨论了长期依赖性问题以及如何通过条件化来缓解它。解释了自回归（AR）和标准化流（NF）模型，以及VAE和GAN。'
- en: '[[1](#bib.bibx1)] provides an overview of deep learning techniques for audio.
    It distinguishes architectures from meta-architectures. The architectures include
    MLP, CNN, Temporal Convolutional Networks (TCN), and RNN, while the meta-architectures
    are Auto-Encoders (AE), VAE, GAN, Encoder/Decoder, Attention Mechanism, and Transformers.
    Divides audio representations into three categories: time-frequency, waveform,
    and knowledge-driven. Time-frequency representations include the Short-Time Fourier
    Transform (STFT), MFCC, Log-Mel-Spectrogram (LMS), and Constant-Q-Transform (CQT).
    The article concludes with a list of applications for audio deep learning algorithms,
    including music content description, environmental sound description, and content
    processing. It also briefly discusses semi-supervised and self-supervised learning.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1](#bib.bibx1)] 提供了音频深度学习技术的概述。它区分了架构和元架构。架构包括MLP、CNN、时序卷积网络（TCN）和RNN，而元架构包括自编码器（AE）、VAE、GAN、编码器/解码器、注意力机制和变换器。将音频表示分为三类：时间频率、波形和知识驱动。时间频率表示包括短时傅里叶变换（STFT）、MFCC、对数梅尔谱图（LMS）和常数Q变换（CQT）。文章最后列出了音频深度学习算法的应用，包括音乐内容描述、环境声音描述和内容处理。还简要讨论了半监督学习和自监督学习。'
- en: '[[6](#bib.bibx6)] provides a comprehensive overview of TTS methods, including
    history. It explains the basic components of TTS systems, such as text analysis,
    acoustic models, and vocoders, and includes a list of models in each area. Finally,
    it discusses advanced methods for implementing TTS systems in certain use situations,
    such as Fast TTS, Low-Resource TTS, and Robust TTS.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6](#bib.bibx6)] 提供了TTS方法的全面概述，包括历史。它解释了TTS系统的基本组成部分，如文本分析、声学模型和声码器，并包括每个领域的模型列表。最后，它讨论了在特定使用场景中实现TTS系统的高级方法，如快速TTS、低资源TTS和鲁棒TTS。'
- en: '[[7](#bib.bibx7)] discusses TTS, music generation, audiovisual multi-modal
    processing, and datasets. This effort differs from earlier ones in that it organizes
    relevant articles by category rather than explaining subjects in depth.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7](#bib.bibx7)] 讨论了TTS、音乐生成、视听多模态处理和数据集。这项工作不同于早期的研究，因为它按类别组织相关文献，而不是深入解释主题。'
- en: '[[8](#bib.bibx8)] is the closest work to this one. It follows a similar structure,
    starting with input representations including raw waveforms, spectrograms, acoustic
    characteristics, embeddings, and symbolic representations, followed by conditioning
    representations used to guide audio synthesis. Includes audio synthesis techniques
    such as AR, NF, GAN, and VAE. The article concludes with the following evaluation
    methods: perceptual evaluation, number of statistically different bins, inception
    score, distance-based measurements, spectral convergence, and log likelihood.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8](#bib.bibx8)] 是与本文最为接近的工作。它遵循类似的结构，从包括原始波形、声谱图、声学特征、嵌入和符号表示的输入表示开始，接着是用于引导音频合成的条件表示。包括了音频合成技术，如AR、NF、GAN和VAE。文章最后总结了以下评估方法：感知评估、统计学上不同的箱数、生成评分、基于距离的测量、频谱收敛和对数似然。'
- en: '[[9](#bib.bibx9)] provides an overview of transformer architectures used in
    the field of speech processing. The article provides a description of the transformer,
    a list of popular transformers for speech, and a literature review on its applications.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9](#bib.bibx9)] 提供了用于语音处理领域的变换器架构的概述。文章描述了变换器、流行的语音变换器列表以及其应用的文献综述。'
- en: '[[10](#bib.bibx10)] surveys TTS and speech enhancement, with a focus on diffusion
    models. Although the emphasis is on diffusion models, they also discuss the stages
    of TTS, pioneering work, and specialized models for distinct speech enhancement
    tasks.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10](#bib.bibx10)] 调查了TTS和语音增强，重点关注扩散模型。尽管重点是扩散模型，但它们也讨论了TTS的阶段、开创性工作以及用于不同语音增强任务的专门模型。'
- en: '[[11](#bib.bibx11)] conducted a comprehensive survey of deep learning techniques
    in speech processing. It begins with speech features and traditional speech processing
    models. It addresses the following deep learning architectures: RNN, CNN, Transformer,
    Conformer, Sequence-to-Sequence models (Seq2seq), Reinforcement learning, Graph
    neural networks (GNN), and diffusion probabilistic networks. Explains supervised,
    unsupervised, semi-supervised, and self-directed speech representation learning.
    Finally, it discusses a variety of speech processing tasks, including neural speech
    synthesis, speech-to-speech translation, speech enhancement, audio super resolution,
    as well transfer learning techniques.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[[11](#bib.bibx11)] 对语音处理中的深度学习技术进行了全面调查。它从语音特征和传统语音处理模型开始，涵盖了以下深度学习架构：RNN、CNN、Transformer、Conformer、Sequence-to-Sequence模型（Seq2seq）、强化学习、图神经网络（GNN）和扩散概率网络。解释了监督、无监督、半监督和自指导的语音表示学习。最后，讨论了包括神经语音合成、语音到语音翻译、语音增强、音频超分辨率以及迁移学习技术在内的各种语音处理任务。'
- en: III Background
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 背景
- en: 'The main purpose of this section is to show how audio generation has developed
    over the years up to this point. Since audio generation is a broad field that
    encompasses many different areas, such as text-to-speech synthesis, voice conversion,
    speech enhancement,… [[12](#bib.bibx12)], we will only focus on two different
    areas of audio generation: text-to-speech synthesis and music generation. There
    is no particular reason for this choice, except that they are among the more popular
    ones. The trend we want to show is how domain-specific knowledge is shifting towards
    general-purpose methods and how feature engineering is turning into feature recognition.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的主要目的是展示音频生成在这些年来的发展。由于音频生成是一个包含许多不同领域的广泛领域，如文本到语音合成、语音转换、语音增强等[[12](#bib.bibx12)]，我们将仅关注音频生成的两个不同领域：文本到语音合成和音乐生成。之所以选择这两个领域，主要是因为它们较为流行。我们想展示的趋势是领域特定知识如何转向通用方法，以及特征工程如何转变为特征识别。
- en: III-A Text-to-Speech
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 文本到语音
- en: Text-to-speech (TTS) is a task with numerous applications, ranging from phone
    assistants to GPS navigators. The desire to construct a machine that can communicate
    with a human has historically fueled growth in this subject. Conventional speech
    synthesis technologies include rule-based concatenative speech synthesis (CSS)
    and statistical parametric speech synthesis (SPSS) [[13](#bib.bibx13)]. CSS and
    SPSS, which employ speech data, may be considered corpus-based speech synthesis
    approaches [[14](#bib.bibx14)].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到语音（TTS）是一个具有众多应用的任务，从电话助手到GPS导航仪。构建能够与人类沟通的机器的愿望历史上推动了这一领域的发展。传统的语音合成技术包括基于规则的连接语音合成（CSS）和统计参数语音合成（SPSS）[[13](#bib.bibx13)]。CSS和SPSS使用语音数据，可能被视为基于语料库的语音合成方法[[14](#bib.bibx14)]。
- en: Until the late 1980s, the field was dominated by rule-based systems [[15](#bib.bibx15)].
    They were heavily reliant on domain expertise such as phonological theory, necessitating
    the collaboration of many experts to develop a comprehensive rule set that would
    generate speech parameters. There are numerous works like [[16](#bib.bibx16),
    [17](#bib.bibx17), [18](#bib.bibx18), [19](#bib.bibx19)].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 到 1980 年代末，该领域由基于规则的系统主导[[15](#bib.bibx15)]。这些系统严重依赖于领域专业知识，如语音理论，需要许多专家的合作以开发一个全面的规则集来生成语音参数。类似的工作有很多，如[[16](#bib.bibx16)，[17](#bib.bibx17)，[18](#bib.bibx18)，[19](#bib.bibx19)]。
- en: 'CSS methods try to achieve the naturalness and intelligibility of speech by
    combining chunks of recorded speech. They can be divided into two categories:
    fixed inventory and unit-selection approaches [[13](#bib.bibx13)]. Fixed inventory
    uses only one instance of each concatenative unit, which goes through signal processing
    before being combined into a spoken word. An example of this might be [[20](#bib.bibx20)],
    which uses the diphone method of segment assembly. On the other hand, unit-selection
    employs a large number of concatenative units, which can result in a better match
    between adjacent units, potentially boosting speech quality. There are two fundamental
    concepts: target cost and concatenation cost. The target cost determines how well
    an element from a database of units fits the desired unit, whereas the concatenation
    cost indicates how well a pair of selected units combine. The goal is to minimize
    both costs for the entire sequence of units; this is commonly done using a Viterbi
    search [[13](#bib.bibx13)]. Although it is always possible to minimize costs,
    the resulting speech may still contain errors. This can arise owing to a lack
    of units to choose from, an issue that can be mitigated by increasing the database
    size. It sounds straightforward; however, doing so increases unit creation costs
    and search times due to the increased number of possible concatenations. All of
    this requires CSS techniques to pick between speech quality and synthesis speed.
    Works in this domain include [[21](#bib.bibx21)], where they employ non-uniform
    synthesis units, and [[22](#bib.bibx22)], which treats units of a unit-selection
    database as states in a state transition network.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: CSS 方法试图通过结合录制语音的片段来实现语音的自然性和可理解性。它们可以分为两类：固定库存和单元选择方法[[13](#bib.bibx13)]。固定库存仅使用每个连接单元的一个实例，该实例经过信号处理后与其他单元合成成一个发音词。一个例子可能是[[20](#bib.bibx20)]，它使用双音素方法进行片段组装。另一方面，单元选择方法使用大量的连接单元，这可能会改善相邻单元之间的匹配，从而提升语音质量。基本概念有两个：目标成本和连接成本。目标成本确定数据库中一个元素与所需单元的匹配程度，而连接成本则指示一对选择的单元的组合效果。目标是最小化整个单元序列的这两种成本；这通常通过
    Viterbi 搜索[[13](#bib.bibx13)]来实现。尽管总是可以最小化成本，但生成的语音可能仍包含错误。这可能由于缺少可选择的单元而引起，这个问题可以通过增加数据库大小来缓解。这听起来很简单；然而，这样做会增加单元创建成本和搜索时间，因为可能的连接数量增加了。所有这些都需要
    CSS 技术在语音质量和合成速度之间做出选择。该领域的工作包括[[21](#bib.bibx21)]，其中采用了非均匀合成单元，以及[[22](#bib.bibx22)]，它将单元选择数据库的单元视为状态转换网络中的状态。
- en: SPSS models speech parameters using statistical methods depending on the desired
    phoneme sequence. This differs from CSS techniques in that we are not maintaining
    natural, unaltered speech but rather teaching the model how to recreate it. In
    a typical SPSS system, this is done by first extracting parametric representations
    of speech and then modeling them using generative models, commonly by applying
    the maximum likelihood criterion [[23](#bib.bibx23)]. The primary advantage of
    SPSS over CSS is its ability to generalize to unknown data [[13](#bib.bibx13)].
    This enables us to adjust the model to generate different voice characteristics
    [[15](#bib.bibx15)]. It also requires orders of magnitude less memory because
    we use model parameters instead of a speech database. Although there are other
    SPSS techniques, the majority of research has centered on hidden Markov models
    (HMM) [[15](#bib.bibx15)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: SPSS 使用统计方法建模语音参数，依据所需的音素序列。这与 CSS 技术不同，因为我们不是保持自然的、未修改的语音，而是教模型如何重现它。在典型的 SPSS
    系统中，这通常通过首先提取语音的参数表示，然后使用生成模型进行建模，常常通过应用最大似然准则[[23](#bib.bibx23)]来完成。SPSS 相对于
    CSS 的主要优势在于其对未知数据的泛化能力[[13](#bib.bibx13)]。这使我们能够调整模型以生成不同的声音特征[[15](#bib.bibx15)]。此外，由于我们使用模型参数而不是语音数据库，因此所需的内存量减少了几个数量级。尽管还有其他
    SPSS 技术，但大多数研究集中在隐马尔可夫模型（HMM）上[[15](#bib.bibx15)]。
- en: Some HMM works include [[14](#bib.bibx14)], which considers not only the output
    probability of static and dynamic feature vectors but also the global variance
    (GV). [[24](#bib.bibx24)] directly models speech waveforms with a trajectory HMM.
    [[25](#bib.bibx25), [26](#bib.bibx26)] use decision-tree-based context clustering
    to represent spectrum, pitch, and HMM state duration simultaneously. Commonly
    used contexts include the current phoneme, preceding and succeeding phonemes,
    the position of the current syllable within the current word or phrase, etc. [[27](#bib.bibx27)].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 HMM 研究包括 [[14](#bib.bibx14)]，它不仅考虑了静态和动态特征向量的输出概率，还考虑了全局方差（GV）。[[24](#bib.bibx24)]
    直接使用轨迹 HMM 对语音波形进行建模。[[25](#bib.bibx25), [26](#bib.bibx26)] 使用基于决策树的上下文聚类来同时表示频谱、音高和
    HMM 状态持续时间。常用的上下文包括当前音素、前后音素、当前音节在当前单词或短语中的位置等。[[27](#bib.bibx27)]。
- en: The notion that the human speech system has a layered structure in its transformation
    of the linguistic level to the waveform level has stimulated the adoption of deep
    neural network speech synthesis [[28](#bib.bibx28)]. [[29](#bib.bibx29)] employs
    an artificial neural network alongside a rule-based method to model speech parameters.
    [[30](#bib.bibx30)] employs limited Boltzmann machines and deep belief networks
    to predict speech parameters for each HMM state. Some other methods worth noting
    are multi-layer perceptron [[31](#bib.bibx31), [32](#bib.bibx32), [28](#bib.bibx28),
    [33](#bib.bibx33), [34](#bib.bibx34)], time-delay neural network [[35](#bib.bibx35),
    [36](#bib.bibx36)], long short-term memory [[37](#bib.bibx37), [38](#bib.bibx38),
    [39](#bib.bibx39), [40](#bib.bibx40)], gated recurrent unit [[41](#bib.bibx41)],
    attention-based recurrent network [[42](#bib.bibx42)], and mixture density network
    [[43](#bib.bibx43), [41](#bib.bibx41)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 人类语音系统在将语言层级转换为波形层级时具有分层结构的概念刺激了深度神经网络语音合成的采用 [[28](#bib.bibx28)]。[[29](#bib.bibx29)]
    使用人工神经网络和基于规则的方法来建模语音参数。[[30](#bib.bibx30)] 使用限制玻尔兹曼机和深度置信网络来预测每个 HMM 状态的语音参数。其他值得注意的方法包括多层感知器
    [[31](#bib.bibx31), [32](#bib.bibx32), [28](#bib.bibx28), [33](#bib.bibx33), [34](#bib.bibx34)]，时间延迟神经网络
    [[35](#bib.bibx35), [36](#bib.bibx36)]，长短期记忆 [[37](#bib.bibx37), [38](#bib.bibx38),
    [39](#bib.bibx39), [40](#bib.bibx40)]，门控递归单元 [[41](#bib.bibx41)]，基于注意力的递归网络 [[42](#bib.bibx42)]，和混合密度网络
    [[43](#bib.bibx43), [41](#bib.bibx41)]。
- en: 'The TTS system consists of four major components: the first converts text to
    a linguistic representation, the second determines the duration of each speech
    segment, the third converts the linguistic and timing representations into speech
    parameters, and the fourth is the vocoder, which generates the speech waveform
    based on the speech parameters [[35](#bib.bibx35)]. The majority of the works
    we presented focused on converting the linguistic representation into speech parameters,
    but there are also models focusing on, for example, grapheme-to-phoneme conversion
    [[44](#bib.bibx44), [45](#bib.bibx45)] to allow TTS without knowledge of linguistic
    features. Examples of vocoders include MLSA [[46](#bib.bibx46)], STRAIGHT [[47](#bib.bibx47)],
    and Vocaine [[48](#bib.bibx48)]. Finally, there have also been attempts to construct
    a fully end-to-end system, which means integrating text analysis and acoustic
    modeling into a single model [[42](#bib.bibx42)].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: TTS 系统由四个主要组件组成：第一个将文本转换为语言表示，第二个确定每个语音段的持续时间，第三个将语言和时序表示转换为语音参数，第四个是声码器，根据语音参数生成语音波形
    [[35](#bib.bibx35)]。我们展示的大多数工作集中于将语言表示转换为语音参数，但也有模型专注于，例如，图形到音素转换 [[44](#bib.bibx44),
    [45](#bib.bibx45)]，以便在没有语言特征知识的情况下进行 TTS。声码器的例子包括 MLSA [[46](#bib.bibx46)]，STRAIGHT
    [[47](#bib.bibx47)]，和 Vocaine [[48](#bib.bibx48)]。最后，也有尝试构建完全端到端系统，这意味着将文本分析和声学建模集成到一个模型中
    [[42](#bib.bibx42)]。
- en: III-B Music generation
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 音乐生成
- en: 'Music has been a part of human life long before the invention of the electronic
    computer, and people have developed many guidelines for how beautifully sounded
    music should be made. For this reason alone, the discipline of music generation
    has placed a heavy emphasis on rule-based systems that use music theory to create
    logical rules. Unlike text, musical vocabulary is rather tiny, consisting of at
    most several hundred discrete note symbols [[49](#bib.bibx49)]. Music creation
    is classified into six categories: grammars, knowledge-based, markov chains, artificial
    neural networks, evolutionary methods, and self-similarity [[50](#bib.bibx50)].
    Specific methods include discrete nonlinear maps [[51](#bib.bibx51), [52](#bib.bibx52)],
    rule-based [[53](#bib.bibx53), [54](#bib.bibx54)], genetic algorithm [[55](#bib.bibx55),
    [56](#bib.bibx56), [57](#bib.bibx57), [58](#bib.bibx58)], recurrent neural network
    [[59](#bib.bibx59), [57](#bib.bibx57)], long short-term memory [[60](#bib.bibx60),
    [61](#bib.bibx61), [62](#bib.bibx62), [63](#bib.bibx63)], markov chain [[52](#bib.bibx52),
    [64](#bib.bibx64)], context-free grammars [[64](#bib.bibx64), [58](#bib.bibx58)],
    context-sensitive grammars [[65](#bib.bibx65), [66](#bib.bibx66)], cellular automaton
    [[67](#bib.bibx67)], random fields [[49](#bib.bibx49)], L-systems [[68](#bib.bibx68)],
    knowledge base [[69](#bib.bibx69)], and restricted Boltzmann machines [[70](#bib.bibx70)].
    Unlike language, music employs a significantly smaller number of acoustic features.
    These include MIDI representation [[56](#bib.bibx56), [62](#bib.bibx62)], encoded
    sheet music [[57](#bib.bibx57)], binary vector of an octave [[49](#bib.bibx49)],
    and piano roll [[70](#bib.bibx70), [62](#bib.bibx62)].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐在人类生活中存在的时间要远早于电子计算机的发明，人们也制定了许多有关如何制作优美音乐的准则。仅仅因为这个原因，音乐生成的学科在很大程度上强调使用音乐理论来创建逻辑规则的基于规则的系统。与文本不同，音乐词汇相当有限，最多包含几百个离散的音符符号[[49](#bib.bibx49)]。音乐创作被分为六个类别：语法、基于知识、马尔可夫链、人工神经网络、进化方法和自相似性[[50](#bib.bibx50)]。具体的方法包括离散非线性映射[[51](#bib.bibx51),
    [52](#bib.bibx52)]、基于规则[[53](#bib.bibx53), [54](#bib.bibx54)]、遗传算法[[55](#bib.bibx55),
    [56](#bib.bibx56), [57](#bib.bibx57), [58](#bib.bibx58)]、递归神经网络[[59](#bib.bibx59),
    [57](#bib.bibx57)]、长短期记忆[[60](#bib.bibx60), [61](#bib.bibx61), [62](#bib.bibx62),
    [63](#bib.bibx63)]、马尔可夫链[[52](#bib.bibx52), [64](#bib.bibx64)]、无上下文语法[[64](#bib.bibx64),
    [58](#bib.bibx58)]、有上下文语法[[65](#bib.bibx65), [66](#bib.bibx66)]、细胞自动机[[67](#bib.bibx67)]、随机场[[49](#bib.bibx49)]、L系统[[68](#bib.bibx68)]、知识库[[69](#bib.bibx69)]和限制玻尔兹曼机[[70](#bib.bibx70)]。与语言不同，音乐采用的声学特征数量明显较少。这些特征包括MIDI表示[[56](#bib.bibx56),
    [62](#bib.bibx62)]、编码的乐谱[[57](#bib.bibx57)]、八度的二进制向量[[49](#bib.bibx49)]和钢琴卷轴[[70](#bib.bibx70),
    [62](#bib.bibx62)]。
- en: IV Audio features
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 音频特征
- en: Even though there have been numerous audio features used throughout the history
    of audio generation. Here we will describe the two most popular features, the
    raw waveform and the log-mel spectrogram, but also mention features that have
    recently gained traction. Keep in mind that there are too many features to describe
    them all, especially if we take into account the many hand-crafted features that
    were created before the rise of deep learning methods.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管历史上使用了大量的音频特征，我们将在这里描述两个最受欢迎的特征：原始波形和对数梅尔频谱图，并提及一些最近获得关注的特征。请记住，有太多特征无法全部描述，特别是考虑到在深度学习方法兴起之前，许多手工制作的特征。
- en: IV-A Raw waveform
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 原始波形
- en: The term "raw audio" typically refers to a waveform recorded using pulse code
    modulation (PCM) [[8](#bib.bibx8)]. In PCM, a continuous waveform is sampled at
    uniform intervals, known as the sampling frequency. According to the sampling
    principle, if a signal is sampled at regular intervals at a rate slightly higher
    than twice the highest signal frequency, then it will contain all of the original
    signal information [[71](#bib.bibx71)]. The average sample frequency for audio
    applications is 44.1 kHz [[8](#bib.bibx8)], hence we cannot hold frequencies equal
    to or greater than 22.05 kHz. Computers cannot store real numbers with absolute precision;
    thus, each sample value is approximated by assigning it an element from a set
    of finite values, a technique known as quantization [[8](#bib.bibx8)]. The most
    common quantization levels are kept in 8 bits (256 levels), 16 bits (65536 levels),
    and 24 bits (16.8 million levels) [[8](#bib.bibx8)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: “原始音频”这个术语通常指的是使用脉冲编码调制（PCM）[[8](#bib.bibx8)]录制的波形。在PCM中，连续波形以均匀的间隔进行采样，这些间隔被称为采样频率。根据采样原理，如果信号以略高于最高信号频率两倍的频率进行均匀采样，则它将包含所有原始信号的信息[[71](#bib.bibx71)]。音频应用的平均采样频率为44.1
    kHz[[8](#bib.bibx8)]，因此我们不能保持等于或高于22.05 kHz的频率。计算机无法以绝对精度存储实数；因此，每个样本值通过从有限值集合中分配一个元素来近似，这种技术被称为量化[[8](#bib.bibx8)]。最常见的量化级别是8位（256级）、16位（65536级）和24位（1680万级）[[8](#bib.bibx8)]。
- en: The advantage of using raw audio waveforms is that they can be easily transformed
    into actual sound. In certain tasks, the disadvantages appear to outweigh the
    benefits, as raw waveforms are still not universally used. The issue is that raw
    audio synthesis at higher bit rates becomes problematic due to the sheer amount
    of states involved [[72](#bib.bibx72)]. For example, 24-bit audio signals have
    more than 16 million states. High sampling rates create exceptionally long sequences,
    making raw audio synthesis more challenging [[73](#bib.bibx73)]. $\mu$-law is
    frequently employed in speech generative models like WaveNet [[74](#bib.bibx74)]
    to compress integer values and sequence length. The method can quantize each timestep
    to 256 values and reconstruct high-quality audio [[73](#bib.bibx73)]. According
    to [[75](#bib.bibx75)], increased bit depth representation can lead to models
    learning undesirable aspects, such as the calm background of the surroundings.
    It should be emphasized that this issue was only observed in older publications
    and is not discussed in current ones.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原始音频波形的优点在于它们可以很容易地转换成实际的声音。在某些任务中，缺点似乎大于优点，因为原始波形仍未被广泛使用。问题在于，原始音频合成在更高位深率下变得问题重重，这主要是由于涉及的状态数量巨大[[72](#bib.bibx72)]。例如，24位音频信号有超过1600万个状态。高采样率产生异常长的序列，使得原始音频合成更加具有挑战性[[73](#bib.bibx73)]。$\mu$-law经常在像WaveNet这样的语音生成模型中被使用[[74](#bib.bibx74)]，以压缩整数值和序列长度。这种方法可以将每个时间步量化为256个值，并重建高质量的音频[[73](#bib.bibx73)]。根据[[75](#bib.bibx75)]，增加位深表示可能导致模型学习到一些不希望出现的方面，例如周围环境的宁静背景。需要强调的是，这个问题仅在较早的出版物中观察到，目前的出版物中并未讨论这个问题。
- en: The most common models that use raw waveforms as their representation of choice
    are text-to-speech models called vocoders. In section [III](#S3 "III Background
    ‣ A survey of deep learning audio generation methods"), we mentioned vocoders,
    which are used to translate mid-term representations, such as mel-spectrograms,
    to raw audio waveforms. Examples include WaveNet [[74](#bib.bibx74)], SampleRNN
    [[76](#bib.bibx76)], and Deep Voice 3 [[77](#bib.bibx77)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原始波形作为其表示方式的最常见模型是称为声码器的文本到语音模型。在[III](#S3 "III Background ‣ A survey of deep
    learning audio generation methods")节中，我们提到了一些声码器，它们用于将中期表示（如Mel-谱图）转换为原始音频波形。示例包括WaveNet
    [[74](#bib.bibx74)]、SampleRNN [[76](#bib.bibx76)]和Deep Voice 3 [[77](#bib.bibx77)]。
- en: IV-B Mel-spectrogram
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B Mel-谱图
- en: Before we can talk about mel-spectrograms, we must first understand the Short-Time
    Fourier Transform (STFT). To represent audio frequencies, we use a Discrete-Fourier
    Transform (DFT), which transforms the original waveform into a sum of weighted
    complex exponentials [[78](#bib.bibx78)]. The problem emerges when we attempt
    to analyze complex audio signals; because the content of most audio signals changes
    over time, we can’t use DFT to figure out how frequencies change. Instead, we
    use STFT to apply DFT to overlapping sections of the audio waveform [[8](#bib.bibx8)].
    Most techniques that use the STFT to represent audio consider just its amplitude
    [[1](#bib.bibx1)], which results in a lossy representation. By removing the phase
    of the STFT, we can arrange it in a time/frequency visual, creating a spectrogram.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论mel-谱图之前，我们必须先了解短时傅里叶变换（STFT）。为了表示音频频率，我们使用离散傅里叶变换（DFT），它将原始波形转换为加权复指数的和[[78](#bib.bibx78)]。当我们试图分析复杂的音频信号时，问题就出现了；由于大多数音频信号的内容随时间变化，我们无法使用DFT来确定频率如何变化。相反，我们使用STFT对音频波形的重叠部分应用DFT[[8](#bib.bibx8)]。大多数使用STFT表示音频的技术仅考虑其振幅[[1](#bib.bibx1)]，这导致了信息丢失的表示。通过去除STFT的相位，我们可以将其排列成时间/频率图，从而创建谱图。
- en: A mel-spectrogram compresses the STFT in the frequency axis by projecting it
    onto a scale known as the mel-scale [[79](#bib.bibx79)]. The mel-scale divides
    the frequency range into a set of mel-frequency bands, with higher frequencies
    having lower resolution and lower frequencies having higher resolution [[11](#bib.bibx11)].
    The scale was inspired by the non-linear frequency perception of human hearing
    [[11](#bib.bibx11)]. Applying the logarithm to the amplitude results in the log-mel-spectrogram
    [[1](#bib.bibx1)]. Finally, using the discrete cosine transform yields the mel
    frequency cepstral coefficients (MFCC) [[3](#bib.bibx3)]. MFCC is a popular representation
    in speech applications [[13](#bib.bibx13)], but it was shown to be unnecessary
    with deep learning models [[3](#bib.bibx3), [1](#bib.bibx1)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: mel-谱图通过将短时傅里叶变换（STFT）压缩到一个称为mel-尺度的范围来实现频率轴的压缩[[79](#bib.bibx79)]。mel-尺度将频率范围划分为一组mel-频带，高频部分具有较低的分辨率，而低频部分具有较高的分辨率[[11](#bib.bibx11)]。这一尺度的灵感来源于人类听觉的非线性频率感知[[11](#bib.bibx11)]。对振幅应用对数后会得到对数mel-谱图[[1](#bib.bibx1)]。最后，使用离散余弦变换可以得到mel频率倒谱系数（MFCC）[[3](#bib.bibx3)]。MFCC是语音应用中一种常用的表示方式[[13](#bib.bibx13)]，但在深度学习模型中已经显示其不再必要[[3](#bib.bibx3),
    [1](#bib.bibx1)]。
- en: While representations such as the STFT and raw waveform are invertible, the
    spectrogram is not, so we must use some approach to approximate the missing values.
    The algorithms used for these were already mentioned in the previous section.
    In addition to neural-based vocoders, other algorithms include Griffin-Lim [[80](#bib.bibx80)],
    gradient-based inversion [[81](#bib.bibx81)], single-pass spectrogram inversion
    (SPSI) [[82](#bib.bibx82)], and phase gradient heap integration (PGHI) [[83](#bib.bibx83)].
    Mel-spectrograms have been frequently utilized as intermediate features in text-to-speech
    pipelines [[7](#bib.bibx7)]. Tacotron 1/2 [[84](#bib.bibx84), [85](#bib.bibx85)]
    and FastSpeech 1/2 [[86](#bib.bibx86), [87](#bib.bibx87)] are examples of such
    models. To better illustrate the compression of the mel-spectrogram, take, for
    example, a 5-minute video sampled at 44.1 kHz. With 16-bit depth, our raw waveform
    will take up ${\approx}25\text{MB}$, while the mel-spectrogram with a common configuration¹¹1Based
    on a small sample of articles observed using mel-spectrograms of 80 bins, 256
    hop size, 1024 window size, and 1024 points of Fourier transform takes up ${\approx}8\text{MB}$
    at the same bit depth. In a \citeyearchoi_ComparisonAudioSignal_2018 article,
    it was proven that mel-spectrogram is preferable over STFT because it achieves
    the same performance while having a more compact representation [[88](#bib.bibx88)].
    Given the field’s progress, it should be emphasized that only recurrent and convolutional
    models were examined in the article. Another advantage of the mel-spectrogram,
    and spectrograms in general, is that they can be displayed as images. Because
    it ignores phase information, it can be shown with one dimension being frequency
    and the other being time. This is useful since images have been widely employed
    in computer vision tasks, allowing us to borrow models for usage in the audio
    domain. There is a concern as spectrograms aren’t the same as images due to the
    different meaning of the axis. This does not appear to have a substantial effect
    since many works implement mel-spectrograms in their convolutional models [[1](#bib.bibx1)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然如 STFT 和原始波形这样的表示是可逆的，但频谱图却不是，因此我们必须使用某种方法来近似缺失的值。用于这些的算法在上一节中已有提及。除了基于神经网络的声码器，其他算法包括
    Griffin-Lim [[80](#bib.bibx80)]、基于梯度的反演 [[81](#bib.bibx81)]、单次频谱图反演 (SPSI) [[82](#bib.bibx82)]
    和相位梯度堆叠积分 (PGHI) [[83](#bib.bibx83)]。梅尔频谱图经常被用作文本到语音流程中的中间特征 [[7](#bib.bibx7)]。Tacotron
    1/2 [[84](#bib.bibx84), [85](#bib.bibx85)] 和 FastSpeech 1/2 [[86](#bib.bibx86),
    [87](#bib.bibx87)] 是这种模型的例子。为了更好地说明梅尔频谱图的压缩，以一个采样率为 44.1 kHz 的 5 分钟视频为例。16 位深度的情况下，我们的原始波形将占用
    ${\approx}25\text{MB}$，而使用常见配置¹¹1基于观察到的 80 个频带、256 跳跃大小、1024 窗口大小和 1024 点傅里叶变换的梅尔频谱图在相同位深度下占用
    ${\approx}8\text{MB}$。在 \citeyearchoi_ComparisonAudioSignal_2018 的文章中，已证明梅尔频谱图比
    STFT 更优，因为它在具有更紧凑表示的同时实现了相同的性能 [[88](#bib.bibx88)]。鉴于该领域的进展，必须强调的是，文章中仅检查了递归和卷积模型。梅尔频谱图的另一个优势，以及频谱图的一般优势在于它们可以作为图像展示。由于忽略了相位信息，它可以用一个维度表示频率，另一个维度表示时间。这是有用的，因为图像已广泛应用于计算机视觉任务，使我们可以借用模型在音频领域中使用。需要注意的是，由于轴的意义不同，频谱图并不完全等同于图像。这似乎没有产生实质性影响，因为许多工作在其卷积模型中实现了梅尔频谱图
    [[1](#bib.bibx1)]。
- en: IV-C Neural codecs
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 神经编码器
- en: 'An audio codec is a signal processing technique that compresses an audio signal
    into discrete codes before using those codes to reconstruct the audio signal,
    which is not always possible with complete accuracy. A typical audio codec system
    consists of three components: an encoder, a quantizer, and a decoder. The function
    of each component is explained in section [V-A](#S5.SS1 "V-A Auto-encoders ‣ V
    Architectures ‣ A survey of deep learning audio generation methods"). The goal
    of an audio codec is to use as little information as possible to store or transmit
    an audio signal while ensuring that the decoded audio quality is not significantly
    reduced by eliminating redundant or irrelevant information from the audio signal.
    Traditionally, this is accomplished by changing the signal and trading off the
    quality of specific signal components that are less likely to influence the quality
    [[89](#bib.bibx89)]. Audio codecs have been utilized for a wide range of applications,
    including mobile and internet communication. There are numerous types of audio
    codecs; some are utilized in real-time applications like streaming, while others
    may be used for audio production. Whereas in streaming, latency is a larger concern,
    which means sacrificing quality for speed, in production, we want to retain as
    much detail as possible while maintaining a compact representation.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 音频编解码器是一种信号处理技术，它将音频信号压缩成离散代码，然后使用这些代码重建音频信号，这并不总是能够完全准确地完成。一个典型的音频编解码器系统由三个组件组成：编码器、量化器和解码器。每个组件的功能在[章节
    V-A](#S5.SS1 "V-A Auto-encoders ‣ V Architectures ‣ A survey of deep learning
    audio generation methods")中有解释。音频编解码器的目标是使用尽可能少的信息来存储或传输音频信号，同时确保解码后的音频质量不会因去除音频信号中的冗余或无关信息而显著降低。传统上，这通过改变信号并在某些信号组件的质量上做出权衡来实现，这些组件不太可能影响质量[[89](#bib.bibx89)]。音频编解码器已被广泛应用于包括移动和互联网通信在内的各种应用中。音频编解码器有很多种类型；一些用于实时应用，如流媒体，而另一些则用于音频制作。在流媒体中，延迟是一个更大的问题，这意味着需要牺牲质量以换取速度；而在制作中，我们希望在保持紧凑表示的同时保留尽可能多的细节。
- en: 'Audio codecs can be separated into two categories: waveform codecs and parametric
    codecs. Waveform codecs make little to no assumptions about the nature of audio,
    allowing them to work with any audio signal. This universality makes them well-suited
    for creating high-quality audio at low compression, but they tend to produce artifacts
    when operating at high compression [[90](#bib.bibx90)]. Furthermore, because they
    do not operate well in high compression, they tend to increase storage and transmission
    costs. In contrast to waveform codecs, parametric codecs make assumptions about
    the source audio being encoded and introduce strong priors in the form of a parametric
    model that characterizes the audio synthesis process. The goal is not to achieve
    a faithful reconstruction on a sample-by-sample basis but rather to generate audio
    that is perceptually comparable to the original [[90](#bib.bibx90)]. Parametric
    codecs offer great compression but suffer from low decoded audio quality and noise
    susceptibility [[91](#bib.bibx91)].'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 音频编解码器可以分为两类：波形编解码器和参数编解码器。波形编解码器对音频的本质几乎没有假设，因此它们可以处理任何音频信号。这种通用性使它们非常适合在低压缩下创建高质量音频，但它们在高压缩时往往会产生*伪影*[[90](#bib.bibx90)]。此外，由于它们在高压缩下表现不佳，因此往往会增加存储和传输成本。与波形编解码器不同，参数编解码器对被编码的源音频做出假设，并通过一个表征音频合成过程的参数模型引入强假设。目标不是在逐样本基础上实现忠实重建，而是生成在感知上与原始音频可比的音频[[90](#bib.bibx90)]。参数编解码器提供了很好的压缩，但在解码音频质量和噪声敏感性方面存在问题[[91](#bib.bibx91)]。
- en: 'On the way to the neural codec, we first encountered hybrid codecs, which substituted
    some parametric codec modules with neural networks. This type of codec improves
    performance by leveraging neural networks’ adaptability. Following that came vocoder-based
    approaches, which could leverage previously introduced neural vocoders to reconstruct
    audio signals by conditioning them on parametric coder codes or quantized acoustic
    features. However, their performance and compression were still dependent on the
    handcrafted features received at the input [[92](#bib.bibx92)]. The observation
    that separating models into modules prevents them from functioning effectively
    has inspired end-to-end auto-encoders (E2E AE) that accept raw waveforms as input
    and output. A standard E2E AE is made up of four basic components: an encoder,
    a projector, a quantizer, and a decoder [[92](#bib.bibx92)]. The basic use case
    is to take the raw waveform and use the encoder to construct a representation
    with reduced temporal resolution, which is then projected into a multidimensional
    space by the projector component. To make the representations suitable for transmission
    and storage, we further quantize the projections into codes. These codes make
    up a lookup table, which is used at the other end by the decoder to transform
    the quantized representations back to a raw waveform. [[93](#bib.bibx93)] defines
    the neural codec as a kind of neural network model that converts audio waveform
    into compact representations with a codec encoder and reconstructs audio waveform
    from these representations with a codec decoder. The core idea is to use the audio
    codec to compress the speech or sound into a set of discrete tokens, and then
    the generation model is used to generate these tokens [[94](#bib.bibx94)]. They
    have been shown to allow for cross-modal tasks [[95](#bib.bibx95)].'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经编解码的过程中，我们首先遇到了混合编解码器，它们用神经网络替代了一些参数化编解码模块。这种类型的编解码器通过利用神经网络的适应性来提高性能。随后出现了基于声码器的方法，这些方法可以利用之前介绍的神经声码器，通过将其条件化于参数化编码器代码或量化声学特征来重建音频信号。然而，它们的性能和压缩仍然依赖于输入的手工特征
    [[92](#bib.bibx92)]。将模型分解为模块而导致其无法有效工作的观察，激发了接受原始波形作为输入和输出的端到端自动编码器（E2E AE）的出现。一个标准的
    E2E AE 由四个基本组件组成：编码器、投影器、量化器和解码器 [[92](#bib.bibx92)]。基本的用例是获取原始波形并使用编码器构建一个具有降低时间分辨率的表示，然后通过投影器组件将其投影到多维空间中。为了使表示适合传输和存储，我们进一步将投影量化为代码。这些代码构成了一个查找表，解码器在另一端使用这个查找表将量化的表示转换回原始波形。
    [[93](#bib.bibx93)] 定义了神经编解码器为一种将音频波形转换为紧凑表示的神经网络模型，使用编解码器编码器对其进行编码，并用编解码器解码器从这些表示中重建音频波形。核心思想是使用音频编解码器将语音或声音压缩为一组离散的标记，然后生成模型用于生成这些标记
    [[94](#bib.bibx94)]。这些方法已被证明可以支持跨模态任务 [[95](#bib.bibx95)]。
- en: Neural codec methods include SoundStream [[90](#bib.bibx90)], EnCodec [[89](#bib.bibx89)],
    HiFi-Codec [[94](#bib.bibx94)], AudioDec [[92](#bib.bibx92)], and APCodec [[91](#bib.bibx91)].
    All the said methods use residual vector quantization (RVQ), while HiFi-Codec
    also introduced an extension called group-RVQ. VQ methods will be talked about
    in section [V-A](#S5.SS1 "V-A Auto-encoders ‣ V Architectures ‣ A survey of deep
    learning audio generation methods"). SoundStream [[90](#bib.bibx90)] is used by
    AudioLM, [[96](#bib.bibx96)], MusicLM [[97](#bib.bibx97)], SingSong [[98](#bib.bibx98)]
    and SoundStorm [[99](#bib.bibx99)], while EnCodec [[89](#bib.bibx89)] is used
    by VALL-E [[73](#bib.bibx73)], VALL-E X [[100](#bib.bibx100)], Speech-X [[101](#bib.bibx101)],
    and VioLA [[95](#bib.bibx95)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 神经编解码方法包括 SoundStream [[90](#bib.bibx90)]、EnCodec [[89](#bib.bibx89)]、HiFi-Codec
    [[94](#bib.bibx94)]、AudioDec [[92](#bib.bibx92)] 和 APCodec [[91](#bib.bibx91)]。所有这些方法都使用残差向量量化（RVQ），而
    HiFi-Codec 还引入了一种叫做 group-RVQ 的扩展。VQ 方法将在 [V-A](#S5.SS1 "V-A Auto-encoders ‣ V
    Architectures ‣ A survey of deep learning audio generation methods") 节中讨论。SoundStream
    [[90](#bib.bibx90)] 被 AudioLM [[96](#bib.bibx96)]、MusicLM [[97](#bib.bibx97)]、SingSong
    [[98](#bib.bibx98)] 和 SoundStorm [[99](#bib.bibx99)] 使用，而 EnCodec [[89](#bib.bibx89)]
    被 VALL-E [[73](#bib.bibx73)]、VALL-E X [[100](#bib.bibx100)]、Speech-X [[101](#bib.bibx101)]
    和 VioLA [[95](#bib.bibx95)] 使用。
- en: Finally, despite the fact that neural codec approaches are relatively new, they
    have not been without criticism. [[93](#bib.bibx93)] noted that although RVQ can
    achieve acceptable reconstruction quality and low bitrate, they are meant for
    compression and transmission; therefore, they may not be suited as intermediate
    representations for audio production jobs. This is because the sequence of discrete
    tokens created by RVQ can be very long, approximately $N$ times longer when $N$
    residual quantifiers are utilized. Because language models cannot handle extremely
    long sequences, we will encounter inaccurate predictions of discrete tokens, resulting
    in word skipping, word repetition, or speech collapse issues while attempting
    to reconstruct the speech waveform from these tokens.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管神经编码器方法相对较新，但它们并非没有受到批评。[[93](#bib.bibx93)]指出，尽管RVQ可以实现可接受的重建质量和低比特率，但它们主要用于压缩和传输，因此可能不适合作为音频制作工作的中间表示。这是因为RVQ创建的离散符号序列可能非常长，当使用$N$个残差量化器时，长度大约是$N$倍。由于语言模型无法处理极长的序列，我们将遇到离散符号的不准确预测，从而在尝试从这些符号重建语音波形时出现词汇跳跃、词汇重复或语音崩溃问题。
- en: V Architectures
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 架构
- en: As the models become more advanced, they start utilizing many different architectures
    in unison, making it impossible to categorize them efficiently. Therefore, each
    subsection will contain models that fit into many subsections but have been divided
    up in the way the author thought made the most sense. Unlike the audio features,
    there are many different architectures. Here we will mention the architectures
    that have been most commonly used in the field of audio generation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型变得越来越先进，它们开始联合使用多种不同的架构，使得对其进行有效分类变得不可能。因此，每个子章节将包含适用于多个子章节的模型，但按照作者认为最合理的方式进行划分。与音频特征不同，这里涉及许多不同的架构。我们将在这里提到在音频生成领域中最常用的架构。
- en: V-A Auto-encoders
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 自编码器
- en: The majority of this section was taken from works by [[102](#bib.bibx102), [103](#bib.bibx103)].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本节大部分内容取自[[102](#bib.bibx102), [103](#bib.bibx103)]的研究成果。
- en: 'The auto-encoder’s objective is to duplicate the input into the output. It
    consists of two parts: an encoder and a decoder. The intersection of the two components
    depicts a code that attempts to represent both the input and, by extension, the
    output. The encoder receives input data and changes it into a code that the decoder
    then uses to approximate the original input. If we allowed arbitrary values in
    the encoder and decoder, we would obtain no meaningful code because it would simply
    simulate an identity function. To obtain meaningful code, we constrain both the
    encoder and decoder, preventing them from just passing data through. We can accomplish
    this by, for example, restricting the dimensionality of the values in the model.
    The auto-encoder has the advantage of not requiring labeled data because it merely
    seeks to reconstruct the input, allowing for unsupervised learning. [[104](#bib.bibx104)]
    demonstrates a basic use case for a simple auto-encoder, feature extraction. While
    VITS [[105](#bib.bibx105)] connects two text-to-speech modules using VAE, enabling
    end-to-end learning in an adversarial setting. Figure [2a](#S5.F2.sf1 "In Figure
    2 ‣ V-B Generative adversarial networks ‣ V Architectures ‣ A survey of deep learning
    audio generation methods") depicts a simple auto-encoder setup using generic encoder
    and decoder components.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的目标是将输入复制到输出中。它由两部分组成：编码器和解码器。两部分的交集描绘了一个试图表示输入以及由此延伸的输出的编码。编码器接收输入数据，并将其转换为编码，解码器则使用该编码来近似原始输入。如果我们允许编码器和解码器中使用任意值，我们将无法得到有意义的编码，因为这只是模拟了一个身份函数。为了获得有意义的编码，我们限制了编码器和解码器，防止它们只是简单地传递数据。例如，我们可以通过限制模型中值的维度来实现这一点。自编码器的优势在于它不需要标记数据，因为它仅仅寻求重建输入，从而允许无监督学习。[[104](#bib.bibx104)]演示了一个简单自编码器的基本应用案例，即特征提取。而VITS[[105](#bib.bibx105)]则通过使用VAE连接两个文本到语音模块，实现了在对抗设置中的端到端学习。图[2a](#S5.F2.sf1
    "在图2中 ‣ V-B 生成对抗网络 ‣ V 架构 ‣ 深度学习音频生成方法的调查")展示了一个使用通用编码器和解码器组件的简单自编码器设置。
- en: As this article focuses on generation, we will now introduce one of the most
    popular forms of the auto-encoder, the Variational Auto-Encoder (VAE) [[1](#bib.bibx1)].
    VAE has been proposed to enable us to employ auto-encoders as generative models
    [[8](#bib.bibx8)]. The VAE components can be considered as a combination of two
    separately parameterized models, the recognition model and the generative model.
    The VAE’s success was mostly due to the choice of the Kullback-Leibler (KL) divergence
    as the loss function [[8](#bib.bibx8)]. KL will also be described in section [VI](#S6
    "VI Evaluation metrics ‣ A survey of deep learning audio generation methods").
    Unlike the auto-encoder, the VAE learns the parameters of a probability distribution
    rather than a compressed representation of the data [[5](#bib.bibx5)]. Modeling
    the probability distribution allows us to sample from the learned data distribution.
    The Gaussian distribution is typically used for its generality [[4](#bib.bibx4)].
    MusicVAE [[106](#bib.bibx106)] uses a hierarchical decoder in a recurrent VAE
    to avoid posterior collapse. BUTTER [[107](#bib.bibx107)] creates a unified multi-model
    representation learning model using VAEs. Music FaderNets [[108](#bib.bibx108)]
    introduces a Guassian Mixture VAE. RAVE [[109](#bib.bibx109)] employs multi-stage
    training, initially with representation learning and then with adversarial fine-tuning.
    Tango [[110](#bib.bibx110)] and Make-An-Audio 2 [[111](#bib.bibx111)] generate
    mel-spectrograms by using VAE in a diffusion model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本文聚焦于生成，我们将介绍一种最流行的自编码器形式——变分自编码器（VAE）[[1](#bib.bibx1)]。VAE的提出使我们能够将自编码器用作生成模型[[8](#bib.bibx8)]。VAE的组成部分可以视为两个独立参数化模型的结合：识别模型和生成模型。VAE的成功主要归功于选择了Kullback-Leibler（KL）散度作为损失函数[[8](#bib.bibx8)]。KL散度将在第[VI](#S6
    "VI Evaluation metrics ‣ A survey of deep learning audio generation methods")节中描述。与自编码器不同，VAE学习的是概率分布的参数，而不是数据的压缩表示[[5](#bib.bibx5)]。对概率分布的建模使我们能够从学习到的数据分布中进行采样。高斯分布因其通用性而被广泛使用[[4](#bib.bibx4)]。MusicVAE
    [[106](#bib.bibx106)]在递归VAE中使用了分层解码器，以避免后验崩溃。BUTTER [[107](#bib.bibx107)]使用VAE创建了一个统一的多模型表示学习模型。Music
    FaderNets [[108](#bib.bibx108)]引入了高斯混合VAE。RAVE [[109](#bib.bibx109)]采用了多阶段训练，最初进行表示学习，然后进行对抗性微调。Tango
    [[110](#bib.bibx110)]和Make-An-Audio 2 [[111](#bib.bibx111)]通过在扩散模型中使用VAE生成梅尔频谱图。
- en: Vector-Quantized VAE (VQ-VAE) is an extension of VAE that places the latent
    representation in a discrete latent space. VQ-VAE changes the auto-encoder structure
    by introducing a new component called the codebook. The most significant change
    happens between the encoder and decoder, where the encoder’s output is used in
    a nearest neighbor lookup utilizing the codebook. In other words, the continuous
    value received from the encoder is quantized and mapped onto a discrete latent
    space that will be received by the decoder. VQ-VAE replaces the KL divergence
    loss with negative log likelihood, codebook, and commitment losses. One possible
    issue with the VQ-VAE is codebook collapse. This occurs when the model stops using
    a piece of the codebook, indicating that it is no longer at full capacity. It
    can result in decreased likelihoods and inadequate reconstruction [[75](#bib.bibx75)].
    [[75](#bib.bibx75)] proposes the argmax auto-encoder as an alternative to VQ-VAE
    for music generation. MelGAN [[112](#bib.bibx112)], VQVAE [[113](#bib.bibx113)],
    Jukebox [[114](#bib.bibx114)] with Hierarchical VQ-VAE, DiscreTalk [[115](#bib.bibx115)],
    FIGARO [[116](#bib.bibx116)], Diffsound [[117](#bib.bibx117)], and Im2Wav [[118](#bib.bibx118)]
    use VQ-VAE to compress the input to a lower-dimensional space. While Dance2Music-GAN
    [[119](#bib.bibx119)], SpeechT5 [[120](#bib.bibx120)], VQTTS [[121](#bib.bibx121)],
    and DelightfulTTS 2 [[122](#bib.bibx122)] only use the vector quantization.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 向量量化VAE（VQ-VAE）是VAE的一种扩展，它将潜在表示置于离散的潜在空间中。VQ-VAE通过引入一个称为代码本的新组件来改变自编码器结构。最显著的变化发生在编码器和解码器之间，在那里编码器的输出被用在利用代码本的最近邻查找中。换句话说，从编码器接收到的连续值被量化并映射到一个离散的潜在空间，解码器将接收到这个空间。VQ-VAE用负对数似然、代码本和承诺损失替换了KL散度损失。VQ-VAE的一个可能问题是代码本崩溃。这发生在模型停止使用代码本的一部分时，表明代码本不再满负荷使用。这可能导致似然性下降和重建不足[[75](#bib.bibx75)]。[[75](#bib.bibx75)]提出了argmax自编码器作为VQ-VAE在音乐生成中的替代方案。MelGAN[[112](#bib.bibx112)]、VQVAE[[113](#bib.bibx113)]、Jukebox[[114](#bib.bibx114)]（带有层次化VQ-VAE）、DiscreTalk[[115](#bib.bibx115)]、FIGARO[[116](#bib.bibx116)]、Diffsound[[117](#bib.bibx117)]和Im2Wav[[118](#bib.bibx118)]使用VQ-VAE将输入压缩到低维空间。而Dance2Music-GAN[[119](#bib.bibx119)]、SpeechT5[[120](#bib.bibx120)]、VQTTS[[121](#bib.bibx121)]和DelightfulTTS
    2[[122](#bib.bibx122)]仅使用向量量化。
- en: Residual Vector Quantization (RVQ) improves VAE by computing the residual after
    quantization and further quantizing it using a second codebook, a third, and so
    on [[89](#bib.bibx89)]. In other words, RVQ cascades $N$ layers of VQ where unquantized
    input vector is passed through a first VQ and quantization residuals are computed,
    then those residuals are iteratively quantized by an additional sequence of $N-1$
    vector quantizers [[90](#bib.bibx90)]. Section [IV-C](#S4.SS3 "IV-C Neural codecs
    ‣ IV Audio features ‣ A survey of deep learning audio generation methods") provides
    a list of models that employ RVQ.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 残差向量量化（RVQ）通过在量化后计算残差并使用第二个代码本、第三个代码本等进一步进行量化，从而改进了VAE[[89](#bib.bibx89)]。换句话说，RVQ级联了$N$层VQ，其中未量化的输入向量通过第一层VQ，计算量化残差，然后这些残差被额外的$N-1$个向量量化器迭代量化[[90](#bib.bibx90)]。第[IV-C](#S4.SS3
    "IV-C Neural codecs ‣ IV Audio features ‣ A survey of deep learning audio generation
    methods")节列出了使用RVQ的模型。
- en: V-B Generative adversarial networks
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 生成对抗网络
- en: 'Another prominent generating architecture is generative adversarial networks
    (GAN). It is made up of two models that serve different purposes: the generator
    and the discriminator. The generator’s function is to convert a random input vector
    to a data sample. The random vector is usually smaller because the generator mimics
    the decoder part of the auto-encoder [[1](#bib.bibx1)]. Unlike VAE, which imposes a
    distribution to generate realistic data, GAN utilizes a second network called
    the discriminator [[1](#bib.bibx1)]. It takes the generator’s output or a sample
    from the dataset and attempts to classify it as either real or fake. The generator
    is penalized based on the discriminator’s ability to tell the difference between
    real and fake. The opposite is also true: if the discriminator is unable to distinguish
    between the generator and the actual data points, it is penalized as well. In
    other words, the two neural networks face off in a two-player minimax game. According
    to [[123](#bib.bibx123)], the ideal outcome for network training is for the discriminator
    to be 50% certain whether the input is real or bogus. In practice, we train the
    generator through the discriminator by reducing the probability that the sample
    is fake, while the discriminator does the opposite for fake data and the same
    for real data. Figure [2b](#S5.F2.sf2 "In Figure 2 ‣ V-B Generative adversarial
    networks ‣ V Architectures ‣ A survey of deep learning audio generation methods")
    illustrates the generator taking a random vector input and the discriminator attempting
    to distinguish between real and fake samples.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种显著的生成架构是生成对抗网络（GAN）。它由两个具有不同功能的模型组成：生成器和判别器。生成器的功能是将一个随机输入向量转换为一个数据样本。随机向量通常较小，因为生成器模拟了自编码器的解码器部分[[1](#bib.bibx1)]。与施加分布以生成逼真数据的变分自编码器（VAE）不同，GAN
    使用了一个称为判别器的第二个网络[[1](#bib.bibx1)]。判别器接收生成器的输出或数据集中的样本，并尝试将其分类为真实或虚假。生成器根据判别器区分真实与虚假的能力受到惩罚。反之亦然：如果判别器无法区分生成器生成的样本和实际数据点，它也会受到惩罚。换句话说，这两个神经网络在一个二人博弈的最小化游戏中对抗。根据[[123](#bib.bibx123)]，网络训练的理想结果是判别器对输入是否真实或虚假有50%的确定性。在实践中，我们通过减少样本是虚假的概率来训练生成器，而判别器则对虚假数据和真实数据进行相反的处理。图[2b](#S5.F2.sf2
    "在图2 ‣ V-B 生成对抗网络 ‣ V 架构 ‣ 深度学习音频生成方法综述")展示了生成器接收一个随机向量输入，并且判别器尝试区分真实和虚假样本。
- en: '![Refer to caption](img/0b4fd019599e3eafbc0236c2e03b1d9a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0b4fd019599e3eafbc0236c2e03b1d9a.png)'
- en: (a)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/7056c8bac75f0dd3eed0f57c7fe3fe58.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7056c8bac75f0dd3eed0f57c7fe3fe58.png)'
- en: (b)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 2: Two deep learning architectures that appear to have little in common
    until we look closer. The generator mimics the auto-encoder’s decoder, whereas
    the discriminator resembles the encoder.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：两个深度学习架构，看起来似乎没有什么共同之处，直到我们仔细观察。生成器模拟了自编码器的解码器，而判别器则类似于编码器。
- en: This basic setup allows us to generate samples resembling those in the dataset,
    but it doesn’t let us condition the generation. In other words, the random vector
    used in the generator does not match the semantic features of the data [[5](#bib.bibx5)].
    Many datasets contain additional information about each sample, such as the type
    of object in an image. It would be beneficial if we could use the additional information to
    condition the generator and generate from a subset of the learned outputs. Conditional
    GAN (cGAN) induces additional structure by including additional information into
    the generator and discriminator inputs. The generator adds the additional information
    to the random vector, whereas the discriminator adds it to the data to discriminate.
    Some of the works that utilize cGAN are MidiNet [[124](#bib.bibx124)], [[125](#bib.bibx125)],
    [[126](#bib.bibx126)], [[127](#bib.bibx127)], and V2RA-GAN [[128](#bib.bibx128)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基本设置使我们能够生成类似于数据集中样本的样本，但它不允许我们对生成进行条件控制。换句话说，生成器中使用的随机向量与数据的语义特征不匹配[[5](#bib.bibx5)]。许多数据集包含有关每个样本的附加信息，例如图像中的物体类型。如果我们能利用这些附加信息来控制生成器，从已学习输出的子集生成样本，将会非常有益。条件生成对抗网络（cGAN）通过将附加信息包含到生成器和判别器输入中来引入额外的结构。生成器将附加信息添加到随机向量中，而判别器则将其添加到数据中进行区分。一些利用cGAN的工作包括MidiNet[[124](#bib.bibx124)]、[[125](#bib.bibx125)]、[[126](#bib.bibx126)]、[[127](#bib.bibx127)]和V2RA-GAN[[128](#bib.bibx128)]。
- en: Common issues with GAN include mode collapse, unstable training, and a lack
    of an evaluation metric [[2](#bib.bibx2)]. Mode collapse occurs when the generator
    focuses exclusively on a few outputs that can trick the discriminator into thinking
    they are real. Even if the generator meets the discriminator requirements, we
    cannot use it to produce more than a few examples. This might happen because the
    discriminator is unable to force the generator to be diverse [[123](#bib.bibx123)].
    The Wasserstein GAN (WGAN) is a well-known variant for addressing this problem.
    WGAN shifts the discriminator’s job from distinguishing between real and forged
    data to computing the Wasserstein distance, commonly known as the Earth Mover’s
    distance. In addition, a modification to aid WGAN convergence has been proposed;
    it uses a gradient penalty rather than weight clipping and is known as WGAN-GP.
    WGAN was used by MuseGAN [[129](#bib.bibx129)], WaveGAN [[130](#bib.bibx130)],
    TiFGAN [[131](#bib.bibx131)], and Catch-A-Waveform [[132](#bib.bibx132)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 的常见问题包括模式崩溃、不稳定的训练以及缺乏评估指标 [[2](#bib.bibx2)]。模式崩溃发生在生成器专注于少数几种输出，这些输出能够欺骗判别器，让其认为这些输出是真实的。即使生成器满足了判别器的要求，我们也无法使用它生成超过少数几个样本。这可能是因为判别器无法强制生成器保持多样性
    [[123](#bib.bibx123)]。Wasserstein GAN (WGAN) 是一种著名的变体，用于解决这一问题。WGAN 将判别器的任务从区分真实数据和伪造数据转变为计算
    Wasserstein 距离，通常称为地球搬运工距离。此外，还提出了一种修改方案以帮助 WGAN 收敛；该方案使用梯度惩罚而不是权重裁剪，被称为 WGAN-GP。WGAN
    被 MuseGAN [[129](#bib.bibx129)]、WaveGAN [[130](#bib.bibx130)]、TiFGAN [[131](#bib.bibx131)]
    和 Catch-A-Waveform [[132](#bib.bibx132)] 等使用。
- en: Another popular modification to the GAN architecture is the use of deep convolutional
    networks known as deep convolutional GANs (DCGAN). Unlike WGAN, DCGAN only requires
    a change to the model architecture, rather than the entire training procedure,
    for both the generator and discriminator. It aims to provide a stable learning
    environment in an unsupervised setting by applying a set of architectural constraints
    [[123](#bib.bibx123)]. DGAN is used in works such as MidiNet [[124](#bib.bibx124)],
    WaveGAN [[130](#bib.bibx130)], and TiFGAN [[131](#bib.bibx131)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的 GAN 架构修改是使用深度卷积网络，即深度卷积 GAN (DCGAN)。与 WGAN 不同，DCGAN 只需要改变模型架构，而不是整个训练过程，包括生成器和判别器。它旨在通过应用一组架构约束，在无监督环境中提供稳定的学习环境
    [[123](#bib.bibx123)]。DGAN 被用于如 MidiNet [[124](#bib.bibx124)]、WaveGAN [[130](#bib.bibx130)]
    和 TiFGAN [[131](#bib.bibx131)] 等工作中。
- en: Furthermore, it is worth noting a simple GAN extension designed to address the
    issue of vanishing gradients while simultaneously improving training stability.
    Least squares GAN (LSGAN) improves the quality of generated samples by altering
    the discriminator’s loss function. Unlike the regular GAN, LSGAN penalizes correctly
    classified samples much more, pulling them toward the decision boundary, which
    allows LSGAN to generate samples that are closer to the real data [[133](#bib.bibx133)].
    Papers using LSGAN include SEGAN [[134](#bib.bibx134)], [[135](#bib.bibx135)],
    HiFi-GAN [[136](#bib.bibx136)], Parallel WaveGAN [[137](#bib.bibx137)], Fre-GAN
    [[138](#bib.bibx138)], VITS [[105](#bib.bibx105)] and V2RA-GAN [[128](#bib.bibx128)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，值得注意的是一个简单的 GAN 扩展，旨在解决梯度消失问题，同时提高训练稳定性。最小二乘 GAN (LSGAN) 通过改变判别器的损失函数来提高生成样本的质量。与常规
    GAN 不同，LSGAN 对正确分类的样本施加了更大的惩罚，将它们拉向决策边界，这使得 LSGAN 能生成更接近真实数据的样本 [[133](#bib.bibx133)]。使用
    LSGAN 的论文包括 SEGAN [[134](#bib.bibx134)]、[[135](#bib.bibx135)]、HiFi-GAN [[136](#bib.bibx136)]、Parallel
    WaveGAN [[137](#bib.bibx137)]、Fre-GAN [[138](#bib.bibx138)]、VITS [[105](#bib.bibx105)]
    和 V2RA-GAN [[128](#bib.bibx128)]。
- en: There are many more modifications to GAN we haven’t mentioned, like the Cycle
    GAN [[139](#bib.bibx139)] or the Boundary-Equilibrium GAN [[140](#bib.bibx140)],
    as we tried to showcase the most prevalent modifications in the field of audio
    generation. Works like MelGAN [[112](#bib.bibx112)], GAAE [[141](#bib.bibx141)],
    GGAN [[142](#bib.bibx142)], SEANet [[143](#bib.bibx143)], EATS [[144](#bib.bibx144)],
    Dance2Music-GAN [[119](#bib.bibx119)] and Musika [[145](#bib.bibx145)] use yet
    another type of loss called the hinge loss.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的 GAN 修改方法我们没有提到，如 Cycle GAN [[139](#bib.bibx139)] 或 Boundary-Equilibrium
    GAN [[140](#bib.bibx140)]，我们试图展示音频生成领域最常见的修改方法。像 MelGAN [[112](#bib.bibx112)]、GAAE
    [[141](#bib.bibx141)]、GGAN [[142](#bib.bibx142)]、SEANet [[143](#bib.bibx143)]、EATS
    [[144](#bib.bibx144)]、Dance2Music-GAN [[119](#bib.bibx119)] 和 Musika [[145](#bib.bibx145)]
    使用了另一种损失类型，称为铰链损失。
- en: Finally, we’d like to mention works that were challenging to categorize. They
    are GANSynth [[146](#bib.bibx146)], GAN-TTS [[147](#bib.bibx147)], RegNet [[148](#bib.bibx148)],
    Audeo [[149](#bib.bibx149)], Multi-Band MelGAN [[150](#bib.bibx150)], Multi-Singer
    [[151](#bib.bibx151)] and DelightfulTTS 2 [[122](#bib.bibx122)].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想提到一些难以分类的工作。它们包括GANSynth [[146](#bib.bibx146)]、GAN-TTS [[147](#bib.bibx147)]、RegNet
    [[148](#bib.bibx148)]、Audeo [[149](#bib.bibx149)]、Multi-Band MelGAN [[150](#bib.bibx150)]、Multi-Singer
    [[151](#bib.bibx151)] 和 DelightfulTTS 2 [[122](#bib.bibx122)]。
- en: V-C Normalizing flows
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 标准化流
- en: Although VAE and GAN were frequently utilized in audio generation, neither allowed
    for an exact evaluation of the probability density of new points [[152](#bib.bibx152)].
    Normalizing Flows (NF) are a family of generative models with tractable distributions
    that enable exact density evaluation and sampling. The network is made up of two
    "flows" that move in opposite directions. One flow starts with a base density,
    which we call noise, and progresses to a more complex density. The opposing flow
    reverses the direction, transforming the complex density back into the base density.
    The movement from base to complex is known as the generating direction, whereas
    the reverse is known as the normalizing direction. The term normalizing flow refers
    to the notion that the normalizing direction makes a complex distribution more
    regular, or normal. The normal distribution is typically used for base density,
    which is another reason for the name. Similar to how we layer transformations in
    a deep neural network, we compose several simple functions to generate complex
    behavior. These functions cannot be chosen arbitrarily because the flow must be
    in both directions; hence, the functions chosen must be invertible. Using a characteristic
    of the invertible function composition, we can create a likelihood-based estimation
    of the parameters that we can apply to train the model. In this setup, data generation
    is simple; utilizing the generative flow, we can input a sample from the base distribution
    and generate the required complex distribution sample. It has been formally proven
    that if you can build an arbitrarily complex transformation, you will be able
    to generate any distribution under reasonable assumptions [[152](#bib.bibx152)].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管VAE和GAN在音频生成中被频繁使用，但它们都无法对新点的概率密度进行精确评估 [[152](#bib.bibx152)]。标准化流（NF）是一类具有可处理分布的生成模型，这些分布可以进行精确的密度评估和采样。网络由两个“流”组成，分别朝着相反的方向移动。一个流从基础密度开始，我们称之为噪声，然后逐渐发展为更复杂的密度。相反的流则反转方向，将复杂密度转换回基础密度。从基础到复杂的过程称为生成方向，而相反的过程称为标准化方向。标准化流一词指的是标准化方向使复杂分布更为规律或标准化的概念。标准分布通常用于基础密度，这也是名称的另一个原因。类似于我们在深度神经网络中叠加转换，我们通过组合多个简单函数来生成复杂行为。这些函数不能随意选择，因为流必须双向进行；因此，所选择的函数必须是可逆的。利用可逆函数组合的特性，我们可以创建基于似然的参数估计，并应用于训练模型。在这种设置下，数据生成很简单；通过使用生成流，我们可以输入来自基础分布的样本，并生成所需的复杂分布样本。已经正式证明，如果你能构建任意复杂的转换，你将能够在合理假设下生成任何分布
    [[152](#bib.bibx152)]。
- en: Depending on the nature of the function employed in the flow, there can be significant
    performance differences that impact training or inference time. Inverse Autoregressive
    Flows (IAF) are a type of NF model with a specialized function that allows for
    efficient synthesis. The transform is based on an autoregressive network, which
    means that the current output is only determined by the current and previous input
    values. The advantage of this transformation is that the generative flow may be
    computed in parallel, allowing efficient use of resources such as the GPU. Although
    IAF networks can be run in parallel during inference, training with maximum likelihood
    estimation requires sequential processing. To allow for parallel training, a probability
    density distillation strategy is used [[153](#bib.bibx153), [154](#bib.bibx154)].
    In this method, we try to transfer knowledge from an already-trained teacher to
    a student model. Parallel WaveNet [[153](#bib.bibx153)] and ClariNet [[154](#bib.bibx154)] are
    two IAF models that employ this method. On the other hand, WaveGlow [[155](#bib.bibx155)],
    FloWaveNet [[156](#bib.bibx156)], and Glow-TTS [[157](#bib.bibx157)] all utilize
    an affine coupling layer. Because this layer allows for both parallel training
    and inference, they can avoid the problems associated with the former. Other efforts
    that are worth mentioning are WaveNODE [[158](#bib.bibx158)], which uses continuous
    normalizing flow, and WaveFlow [[159](#bib.bibx159)], which uses a dilated 2-D
    convolutional architecture.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所使用函数的性质，可能会有显著的性能差异，这会影响训练或推断时间。逆自回归流（IAF）是一种具有专门化函数的NF模型，允许高效合成。该变换基于自回归网络，这意味着当前输出仅由当前和前一个输入值决定。这种变换的优势在于生成流可以并行计算，从而有效利用如GPU等资源。虽然IAF网络在推断时可以并行运行，但最大似然估计训练需要顺序处理。为了实现并行训练，采用了概率密度蒸馏策略[[153](#bib.bibx153),
    [154](#bib.bibx154)]。在这种方法中，我们尝试将已训练好的教师模型的知识转移给学生模型。Parallel WaveNet [[153](#bib.bibx153)]
    和 ClariNet [[154](#bib.bibx154)] 是使用这种方法的两个IAF模型。另一方面，WaveGlow [[155](#bib.bibx155)]、FloWaveNet
    [[156](#bib.bibx156)] 和 Glow-TTS [[157](#bib.bibx157)] 都利用了仿射耦合层。因为这一层允许并行训练和推断，所以它们可以避免前述问题。其他值得提及的努力包括使用连续归一化流的WaveNODE
    [[158](#bib.bibx158)] 和使用扩张2-D卷积架构的WaveFlow [[159](#bib.bibx159)]。
- en: At the end of this section, we’d like to address a problem that can arise when
    using flow-based networks with audio data. Because audio is digitally stored in
    a discrete representation, training a continuous density model on discrete data
    might lead to poor model performance. [[160](#bib.bibx160)] presented audio dequantization
    methods that can be deployed in flow-based networks and improved audio generating
    quality.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节末尾，我们想讨论一个可能在使用基于流的网络处理音频数据时出现的问题。由于音频以离散表示形式数字存储，在离散数据上训练连续密度模型可能导致模型性能不佳。[[160](#bib.bibx160)]
    提出了可以在基于流的网络中应用的音频去量化方法，并提高了音频生成质量。
- en: V-D Transformer networks
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D Transformer 网络
- en: The majority of the material discussed in this section comes from [[161](#bib.bibx161)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的大部分材料来自[[161](#bib.bibx161)]。
- en: 'Before we can discuss transformers, we need to talk about attention. Attention
    has three components: a query, keys, and values. We have a database of (key, value)
    pairs, and our goal is to locate all values that closely match their key with
    the query. In the transformer, we improve on this concept by introducing a new
    type of attention known as self-attention. Self-attention includes three new functions
    that accept input and have learnable parameters. The functions change the input
    into one of the three previously mentioned components: query, key, and value.
    The prefix self refers to the fact that we utilize the same input for both the
    database and the query. If we were to translate a sentence, we would expect the
    translation of the nth word to be determined not only by itself but also by the
    other words in the sentence. For this example, the query would be the nth word
    and the database would be the sentence itself; if the parameters were learned
    correctly, we would expect to see relevant values for translation as the output
    of self-attention. To boost the model’s capacity to capture both short- and long-term
    dependencies, we can concatenate numerous self-attention modules, each with its
    own set of parameters, resulting in multi-head self-attention. Furthermore, if
    we want to prevent the model from attending to future entries, we can use masked
    multi-head self-attention, which employs a mask to specify which future entries
    we wish to ignore.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论变压器模型之前，我们需要谈谈注意力机制。注意力机制有三个组成部分：查询（query）、键（key）和值（value）。我们有一个（键，值）对的数据库，我们的目标是定位所有与查询的键紧密匹配的值。在变压器模型中，我们通过引入一种新的注意力机制——自注意力（self-attention）来改进这一概念。自注意力包括三个新的函数，这些函数接受输入并具有可学习的参数。这些函数将输入转换为之前提到的三种组件之一：查询、键和值。前缀“自”指的是我们对数据库和查询使用相同的输入。如果我们要翻译一个句子，我们会期望第n个词的翻译不仅仅由它自己决定，还由句子中的其他词决定。在这个例子中，查询就是第n个词，数据库就是整个句子；如果参数学习得当，我们会期望看到相关的翻译值作为自注意力的输出。为了提升模型捕捉短期和长期依赖的能力，我们可以连接多个自注意力模块，每个模块都有自己的一组参数，从而实现多头自注意力（multi-head
    self-attention）。此外，如果我们想防止模型关注未来的条目，我们可以使用掩码多头自注意力（masked multi-head self-attention），它使用掩码来指定我们希望忽略哪些未来的条目。
- en: The second essential element of the transformer is positional encoding. Instead
    of processing a sequence one at a time, the self-attention in the transformer
    provides parallel computing. The consequence is that the sequence’s order is not
    preserved. The prevailing method for preserving order information is to feed the
    model with additional input associated with each token. These additional inputs
    are known as positional encodings. Position encoding can be absolute or relative,
    and it can be predefined or learned during training.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的第二个关键元素是位置编码（positional encoding）。变压器中的自注意力提供了并行计算，而不是一次处理一个序列。这导致序列的顺序信息没有被保留。保留顺序信息的普遍方法是为每个标记提供额外的输入，这些额外的输入称为位置编码。位置编码可以是绝对的或相对的，也可以是预定义的或在训练过程中学习到的。
- en: At last, the transformer, like an auto-encoder, has both an encoder and a decoder.
    Both the encoder and decoder are made up of a stack of identical layers, each
    with two sublayers. The first sublayer is multi-head self-attention, whereas the
    second is a feed-forward network. In addition, each identical layer contains a
    residual connection surrounding both sublayers that follows layer normalization.
    Unlike the encoder, the decoder employs both encoder-decoder attention and masked
    multi-head self-attention at the input. The encoder-decoder attention is a normal multi-head
    attention with queries from the decoder and (key, value) pairs from the encoder.
    Before the input is fed into the network, positional embedding is applied.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，变压器模型类似于自编码器，具有编码器（encoder）和解码器（decoder）。编码器和解码器都由一系列相同的层组成，每层有两个子层。第一个子层是多头自注意力，而第二个子层是前馈网络（feed-forward
    network）。此外，每个相同的层包含一个残差连接（residual connection），包围两个子层，并在层归一化（layer normalization）之后进行。与编码器不同，解码器在输入时使用编码器-解码器注意力（encoder-decoder
    attention）和掩码多头自注意力。编码器-解码器注意力是正常的多头注意力，其查询来自解码器，而（键，值）对来自编码器。在输入网络之前，应用位置嵌入（positional
    embedding）。
- en: Transformers were primarily intended for natural language processing but were
    then used for images with the vision transformer and lately for audio signals
    [[162](#bib.bibx162)]. They have revolutionized modern deep learning by providing
    the ability to model long-term sequences [[72](#bib.bibx72)]. On the other hand,
    transformers are generally referred to as data-hungry since they require a large
    amount of training data [[162](#bib.bibx162)]. The attention mechanism’s quadratic
    complexity makes it difficult to process long sequences [[72](#bib.bibx72)]. To
    use transformers with audio, we would convert signals into visual spectrograms
    and divide them into "patches" that are then treated as separate input tokens,
    analogous to text [[162](#bib.bibx162)]. There are many works that use the transformer
    architecture, including Music Transformer [[163](#bib.bibx163)], FastSpeech [[86](#bib.bibx86)],
    Wave2Midi2Wave [[164](#bib.bibx164)], [[165](#bib.bibx165)], RobuTrans [[166](#bib.bibx166)],
    Jukebox [[114](#bib.bibx114)], AlignTTS [[167](#bib.bibx167)], Multi-Track Music
    Machine [[168](#bib.bibx168)], JDI-T [[169](#bib.bibx169)], AdaSpeech [[170](#bib.bibx170)],
    FastPitch [[171](#bib.bibx171)], [[72](#bib.bibx72)], Controllable Music Transformer
    [[172](#bib.bibx172)], [[173](#bib.bibx173)], SpeechT5 [[120](#bib.bibx120)],
    CPS [[174](#bib.bibx174)], FastSpeech 2 [[87](#bib.bibx87)], FIGARO [[116](#bib.bibx116)],
    HAT [[175](#bib.bibx175)], ELMG [[176](#bib.bibx176)], AudioLM [[96](#bib.bibx96)],
    VALL-E [[73](#bib.bibx73)], MusicLM [[97](#bib.bibx97)], SingSong [[98](#bib.bibx98)],
    SPEAR-TTS [[177](#bib.bibx177)], AudioGen [[178](#bib.bibx178)], VALL-E X [[100](#bib.bibx100)],
    dGSLM [[179](#bib.bibx179)], VioLA [[95](#bib.bibx95)], MuseCoco [[180](#bib.bibx180)],
    Im2Wav [[118](#bib.bibx118)], AudioPaLM [[181](#bib.bibx181)], VampNet [[182](#bib.bibx182)],
    LM-VC [[183](#bib.bibx183)], UniAudio [[184](#bib.bibx184)], and MusicGen [[185](#bib.bibx185)].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 最初是为了自然语言处理而设计的，但后来它们被用于图像处理，如视觉变换器（vision transformer），最近又被用于音频信号
    [[162](#bib.bibx162)]。它们通过提供建模长期序列的能力，彻底改变了现代深度学习 [[72](#bib.bibx72)]。另一方面，transformers
    通常被称为数据饥饿型，因为它们需要大量的训练数据 [[162](#bib.bibx162)]。注意力机制的平方复杂性使得处理长序列变得困难 [[72](#bib.bibx72)]。要将
    transformers 应用于音频，我们会将信号转换成视觉谱图，并将其分割成“补丁”，然后将这些补丁作为独立的输入令牌，类似于文本 [[162](#bib.bibx162)]。有许多工作使用了
    transformer 架构，包括 Music Transformer [[163](#bib.bibx163)]、FastSpeech [[86](#bib.bibx86)]、Wave2Midi2Wave
    [[164](#bib.bibx164)]、[[165](#bib.bibx165)]、RobuTrans [[166](#bib.bibx166)]、Jukebox
    [[114](#bib.bibx114)]、AlignTTS [[167](#bib.bibx167)]、Multi-Track Music Machine
    [[168](#bib.bibx168)]、JDI-T [[169](#bib.bibx169)]、AdaSpeech [[170](#bib.bibx170)]、FastPitch
    [[171](#bib.bibx171)]、[[72](#bib.bibx72)]、Controllable Music Transformer [[172](#bib.bibx172)]、[[173](#bib.bibx173)]、SpeechT5
    [[120](#bib.bibx120)]、CPS [[174](#bib.bibx174)]、FastSpeech 2 [[87](#bib.bibx87)]、FIGARO
    [[116](#bib.bibx116)]、HAT [[175](#bib.bibx175)]、ELMG [[176](#bib.bibx176)]、AudioLM
    [[96](#bib.bibx96)]、VALL-E [[73](#bib.bibx73)]、MusicLM [[97](#bib.bibx97)]、SingSong
    [[98](#bib.bibx98)]、SPEAR-TTS [[177](#bib.bibx177)]、AudioGen [[178](#bib.bibx178)]、VALL-E
    X [[100](#bib.bibx100)]、dGSLM [[179](#bib.bibx179)]、VioLA [[95](#bib.bibx95)]、MuseCoco
    [[180](#bib.bibx180)]、Im2Wav [[118](#bib.bibx118)]、AudioPaLM [[181](#bib.bibx181)]、VampNet
    [[182](#bib.bibx182)]、LM-VC [[183](#bib.bibx183)]、UniAudio [[184](#bib.bibx184)]
    和 MusicGen [[185](#bib.bibx185)]。
- en: LakhNES [[186](#bib.bibx186)] and REMI [[187](#bib.bibx187)] use Transformer-XL,
    an extension of the Transformer that can, in theory, encode arbitrary long contexts
    into fixed-length representations. This is accomplished by providing a recurrence
    mechanism [[188](#bib.bibx188)], wherein the preceding segment is cached for later
    usage as an expanded context for the subsequent segment. Furthermore, to support
    the recurrence mechanism, it introduces an expanded positional encoding scheme known
    as relative positional encoding, which keeps positional information coherent when
    reusing states. In addition to Transformer-XL, [[189](#bib.bibx189)] and [[190](#bib.bibx190)]
    presented Perceiver AR and Museformer as alternatives to tackle problems that
    require extended contexts.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: LakhNES [[186](#bib.bibx186)] 和 REMI [[187](#bib.bibx187)] 使用 Transformer-XL，这是
    Transformer 的一个扩展，理论上可以将任意长的上下文编码为固定长度的表示。这是通过提供一种递归机制 [[188](#bib.bibx188)] 实现的，其中前一个片段会被缓存以便后续用作扩展上下文。此外，为了支持递归机制，它引入了一种称为相对位置编码的扩展位置编码方案，这种方案在重用状态时保持位置一致。除了
    Transformer-XL，[[189](#bib.bibx189)] 和 [[190](#bib.bibx190)] 还提出了 Perceiver AR
    和 Museformer 作为解决需要扩展上下文的问题的替代方案。
- en: Finally, another extension to the transformer has been successful for various
    speech tasks [[191](#bib.bibx191)]. Convolution-augmented Transformer (Conformer)
    extends the Transformer by incorporating convolution and self-attention between
    two feed-forward modules; this cascade of modules is a single Conformer block.
    It integrates a relative positional encoding scheme, a method adopted from the
    described Transformer-XL to improve generalization for diverse input lengths [[192](#bib.bibx192)].
    Papers utilizing the conformer are SpeechNet [[193](#bib.bibx193)], $\text{A}^{3}\text{T}$
    [[191](#bib.bibx191)], VQTTS [[121](#bib.bibx121)], [[194](#bib.bibx194)], and
    SoundStorm [[99](#bib.bibx99)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一种对变压器的扩展在各种语音任务中取得了成功 [[191](#bib.bibx191)]。卷积增强变压器（Conformer）通过在两个前馈模块之间引入卷积和自注意力来扩展变压器；这一级联模块就是一个
    Conformer 块。它集成了一种相对位置编码方案，这种方法借鉴了 Transformer-XL，以提高对不同输入长度的泛化能力 [[192](#bib.bibx192)]。利用
    Conformer 的论文包括 SpeechNet [[193](#bib.bibx193)]、$\text{A}^{3}\text{T}$ [[191](#bib.bibx191)]、VQTTS
    [[121](#bib.bibx121)]、[[194](#bib.bibx194)] 和 SoundStorm [[99](#bib.bibx99)]。
- en: V-E Diffusion models
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 扩散模型
- en: 'Diffusion models are generative models inspired by non-equilibrium thermodynamics
    [[11](#bib.bibx11)]. Diffusion models, like normalizing flows, consist of two
    processes: forward and reverse. The forward process converts the data to a conventional
    Gaussian distribution by constructing a Markov chain of diffusion steps with a
    predetermined noise schedule. The reverse method gradually reconstructs data samples
    from the noise using an interference noise schedule. Unlike other architectures
    that change the distribution of data, such as variational auto-encoders and normalizing
    flows, diffusion models maintain the dimensionality of the latent variables fixed.
    Because the dimensionality of the latent variables must be fixed during the iterative
    generation process, which can result in slow inference speed in high-dimensional
    spaces [[195](#bib.bibx195)]. A potential solution is to utilize a more compressed
    representation, such as a mel-spectrogram, instead of a short-time Fourier transform.
    Papers employing diffusion models include WaveGrad [[196](#bib.bibx196)], DiffWave
    [[197](#bib.bibx197)], Diff-TTS [[198](#bib.bibx198)], Grad-TTS [[199](#bib.bibx199)],
    DiffuSE [[200](#bib.bibx200)], FastDiff [[201](#bib.bibx201)], CDiffuSE [[202](#bib.bibx202)],
    Guided-TTS [[203](#bib.bibx203)], Guided-TTS 2 [[204](#bib.bibx204)], DiffSinger
    [[205](#bib.bibx205)], UNIVERSE [[206](#bib.bibx206)], Diffsound [[117](#bib.bibx117)],
    Noise2Music [[207](#bib.bibx207)], DiffAVA [[208](#bib.bibx208)], MeLoDy [[209](#bib.bibx209)],
    Tango [[110](#bib.bibx110)], SRTNet [[210](#bib.bibx210)], MusicLDM [[211](#bib.bibx211)],
    JEN-1 [[212](#bib.bibx212)], AudioLDM [[195](#bib.bibx195)], [[213](#bib.bibx213)],
    ERNIE-Music [[214](#bib.bibx214)], and Re-AudioLDM [[215](#bib.bibx215)].'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型是受非平衡热力学 [[11](#bib.bibx11)] 启发的生成模型。扩散模型类似于标准化流，由两个过程组成：前向过程和反向过程。前向过程通过构建具有预定噪声时间表的扩散步骤的马尔可夫链，将数据转换为常规高斯分布。反向方法则利用干扰噪声时间表逐步重建数据样本。与其他改变数据分布的架构（如变分自编码器和标准化流）不同，扩散模型保持潜在变量的维度固定。由于在迭代生成过程中潜在变量的维度必须固定，这可能导致高维空间中的推断速度较慢
    [[195](#bib.bibx195)]。一种潜在的解决方案是使用更压缩的表示方式，例如梅尔频谱图，而不是短时傅里叶变换。采用扩散模型的论文包括 WaveGrad
    [[196](#bib.bibx196)]、DiffWave [[197](#bib.bibx197)]、Diff-TTS [[198](#bib.bibx198)]、Grad-TTS
    [[199](#bib.bibx199)]、DiffuSE [[200](#bib.bibx200)]、FastDiff [[201](#bib.bibx201)]、CDiffuSE
    [[202](#bib.bibx202)]、Guided-TTS [[203](#bib.bibx203)]、Guided-TTS 2 [[204](#bib.bibx204)]、DiffSinger
    [[205](#bib.bibx205)]、UNIVERSE [[206](#bib.bibx206)]、Diffsound [[117](#bib.bibx117)]、Noise2Music
    [[207](#bib.bibx207)]、DiffAVA [[208](#bib.bibx208)]、MeLoDy [[209](#bib.bibx209)]、Tango
    [[110](#bib.bibx110)]、SRTNet [[210](#bib.bibx210)]、MusicLDM [[211](#bib.bibx211)]、JEN-1
    [[212](#bib.bibx212)]、AudioLDM [[195](#bib.bibx195)]、[[213](#bib.bibx213)]、ERNIE-Music
    [[214](#bib.bibx214)] 和 Re-AudioLDM [[215](#bib.bibx215)]。
- en: Transformer and diffusion models were the most popular designs discussed in
    this article. As a result, in the final half of this chapter, we will list some
    works that use both the transformer and diffusion models. These works include
    [[216](#bib.bibx216)], Make-An-Audio 2 [[111](#bib.bibx111)], NaturalSpeech 2
    [[93](#bib.bibx93)], Grad-StyleSpeech [[217](#bib.bibx217)], Make-An-Audio [[218](#bib.bibx218)],
    AudioLDM 2 [[219](#bib.bibx219)], and Moûsai [[220](#bib.bibx220)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了变压器和扩散模型这两种最受欢迎的设计。因此，在本章的最后一部分，我们将列出一些同时使用变压器和扩散模型的工作。这些工作包括 [[216](#bib.bibx216)]、Make-An-Audio
    2 [[111](#bib.bibx111)]、NaturalSpeech 2 [[93](#bib.bibx93)]、Grad-StyleSpeech [[217](#bib.bibx217)]、Make-An-Audio
    [[218](#bib.bibx218)]、AudioLDM 2 [[219](#bib.bibx219)] 和 Moûsai [[220](#bib.bibx220)]。
- en: VI Evaluation metrics
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 评估指标
- en: 'Evaluations could be considered the most important piece of the puzzle. By
    introducing evaluations, we are able to quantify progress. We can compare, improve,
    and optimize our models, all thanks to evaluation metrics. We will not mention
    domain-specific evaluation metrics such as the character error rate used in text-to-speech
    or the perceptual evaluation of speech quality used in speech enhancement. There
    are many more widely used metrics that we will not mention in the following sections.
    Some of them are: Nearest neighbor comparisons, Number of statistically-different
    bins, Kernel Inception Distance, and CLIP score.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 评估可以被认为是最重要的环节。通过引入评估，我们能够量化进展。我们可以比较、改进和优化我们的模型，这都得益于评估指标。我们不会提及特定领域的评估指标，例如在文本转语音中使用的字符错误率或在语音增强中使用的感知语音质量评估。接下来的部分我们也不会提到许多其他广泛使用的指标。其中一些包括：最近邻比较、统计上不同的箱数、核生成距离和CLIP分数。
- en: VI-A Human evaluation
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 人工评估
- en: As humans, we are constantly exposed to various sorts of sounds, which provides
    us with a wealth of expertise when attempting to distinguish between real and
    manufactured audio. The audio-generating method we are seeking to construct is
    intended to trick people into thinking the sound is a recording rather than synthesis.
    As a result, who better to judge the success of such systems than the ones we’re
    attempting to fool? Human evaluation is the gold standard for assessing audio
    quality. The human ear is particularly sensitive to irregularities, which are
    disruptive for the listener [[146](#bib.bibx146)]. Intuitively, it is simple to
    label an audio sample as good or bad, real or fake, but it is much more challenging to
    document a procedure derived from our thinking that may be used to evaluate future
    audio samples. The human assessment is often carried out with a defined number
    of listeners who listen and rate the audio on a 1-5 Likert scale. This type of
    test is termed the Mean-Opinion Score (MOS) [[221](#bib.bibx221)]. While MOS is
    used to evaluate naturalness, similarity MOS is used to assess how similar the
    generated and real samples are [[222](#bib.bibx222)]. Another metric, known as
    the comparative MOS, can be used to compare two systems by subtracting their MOS
    values. We may also calculate it by providing listeners with two audio samples
    generated by different models and immediately judging which one is better [[165](#bib.bibx165)].
    [[153](#bib.bibx153)] discovered that preference scores from a paired comparison
    test, frequently referred to as the A/B test, were more reliable than the MOS
    score. Many alternative human evaluation metrics have been proposed for music;
    domain-specific metrics include melody, groove, consonance, coherence, and integrity.
    The biggest disadvantage of human evaluation is that the findings cannot be replicated exactly.
    This means that the concrete numbers in the evaluation are unimportant, and only
    the link between them is crucial. This stems from the inherent subjectivity of
    human evaluation as well as biases or predispositions for specific auditory features.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们不断接触各种声音，这为我们在区分真实与合成音频时提供了丰富的专业知识。我们正在寻求构建的音频生成方法旨在让人们误以为声音是录音而不是合成的。因此，谁更适合评判这些系统的成功呢？当然是那些我们试图欺骗的人。人工评估是评估音频质量的黄金标准。人耳对不规则的声音特别敏感，这会打扰听者[[146](#bib.bibx146)]。直观上，将音频样本标记为好或坏、真实或虚假是简单的，但记录一个来源于我们思维的过程以评估未来音频样本则要复杂得多。人工评估通常由一组确定数量的听众进行，他们会用1-5的李克特量表来听取和评分音频。这种测试被称为平均意见分数（MOS）[[221](#bib.bibx221)]。虽然MOS用于评估自然性，但相似性MOS用于评估生成样本与真实样本的相似程度[[222](#bib.bibx222)]。另一种指标，称为比较MOS，可以通过减去两个系统的MOS值来比较这两个系统。我们也可以通过向听众提供两个由不同模型生成的音频样本并立即判断哪个更好来计算[[165](#bib.bibx165)]。[[153](#bib.bibx153)]发现，配对比较测试中的偏好分数（通常称为A/B测试）比MOS分数更可靠。音乐领域还提出了许多替代的人工评估指标；特定领域的指标包括旋律、节奏、一致性、连贯性和完整性。人工评估的最大缺点是结果不能被完全复现。这意味着评估中的具体数字并不重要，唯一重要的是它们之间的关系。这源于人工评估的固有主观性以及对特定听觉特征的偏见或倾向。
- en: VI-B Inception score
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 生成分数
- en: The Inception Score (IS) is a perceptual metric that correlates with human evaluation.
    The Inception score is calculated by applying a pretrained Inception classifier
    to the generative model’s output. The IS is defined as the mean Kullback-Leibler
    divergence between the conditional output class label distribution and the labels’
    marginal distribution. It evaluates the diversity and quality of the audio outputs
    and prefers generated samples that the model can confidently classify. With $N$
    samples, the measure ranges from $1$ to $N$. The IS is maximized when the classifier
    is confident in every classification of the generated sample and each label is
    predicted equally often [[130](#bib.bibx130)]. Normally, the Inception classifier
    is trained using the ImageNet dataset, which may not be compatible with audio
    spectrograms. This will cause the classifier to be unable to separate the data
    into meaningful categories, resulting in a low IS score. An extension to the IS
    called Modified Inception Score (mIS) measures the within-class diversity of samples
    in addition to the IS which favors sharp and clear samples [[197](#bib.bibx197)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 分数（IS）是一种感知度量，与人工评估相关。Inception 分数通过将预训练的 Inception 分类器应用于生成模型的输出计算得出。IS
    定义为条件输出类别标签分布与标签边际分布之间的均值 Kullback-Leibler 散度。它评估音频输出的多样性和质量，并更喜欢模型可以自信分类的生成样本。对于
    $N$ 个样本，度量范围从 $1$ 到 $N$。当分类器对每个生成样本的分类都非常自信，并且每个标签的预测频率相同，IS 就会达到最大 [[130](#bib.bibx130)]。通常，Inception
    分类器是在 ImageNet 数据集上训练的，这可能与音频频谱图不兼容。这将导致分类器无法将数据分成有意义的类别，从而导致 IS 分数较低。对 IS 的扩展称为
    Modified Inception Score（mIS），除了 IS 之外，还测量样本的类内多样性，偏向于清晰锐利的样本 [[197](#bib.bibx197)]。
- en: VI-C Fréchet distance
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C Fréchet 距离
- en: The inception score is based solely on the generated samples, not taking into
    consideration the real samples [[141](#bib.bibx141)]. The Fréchet Inception Distance
    (FID) score addresses this issue by comparing the generated samples with the real ones.
    The FID calculates the Fréchet distance between two distributions for both generated
    and real samples using distribution parameters taken from the intermediate layer
    of the pretrained Inception Network [[141](#bib.bibx141)]. The lower the FID score,
    the higher the perceived generation quality. It is frequently used to assess the
    fidelity of generated samples in the image generation domain [[117](#bib.bibx117)].
    This metric was found to correlate with perceptual quality and diversity on synthetic
    distributions [[146](#bib.bibx146)]. The Inception Network is trained on the ImageNet
    dataset, which is purpose-built for images, but this does not ensure it will function
    for spectrograms. It may be unable to classify the spectrograms into any meaningful
    categories, resulting in unsatisfactory results [[142](#bib.bibx142)].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 分数仅基于生成样本，而不考虑真实样本 [[141](#bib.bibx141)]。Fréchet Inception 距离（FID）分数通过比较生成样本和真实样本来解决这一问题。FID
    计算了生成样本和真实样本之间的 Fréchet 距离，使用的是从预训练 Inception 网络的中间层提取的分布参数 [[141](#bib.bibx141)]。FID
    分数越低，感知生成质量越高。它常用于评估图像生成领域中生成样本的保真度 [[117](#bib.bibx117)]。这个指标被发现与合成分布的感知质量和多样性相关
    [[146](#bib.bibx146)]。Inception 网络是在 ImageNet 数据集上训练的，该数据集专门为图像构建，但这并不能确保它对频谱图有效。它可能无法将频谱图分类为任何有意义的类别，从而导致不满意的结果
    [[142](#bib.bibx142)]。
- en: Fréchet Audio Distance (FAD) is a perceptual metric adapted from the FID for
    the audio domain. Unlike reference-based metrics, FAD measures the distance between
    the generated audio distribution and the real audio distribution using a pretrained
    audio classifier that does not use reference audio samples. The VGGish model [[223](#bib.bibx223)]
    is used to extract the characteristics of both generated and real audio [[219](#bib.bibx219)].
    As with the FID, the lower the score, the better the audio fidelity. According
    to [[178](#bib.bibx178)], the FAD correlates well with human perceptions of audio
    quality. The FAD was found to be robust against noise, computationally efficient,
    consistent with human judgments, and sensitive to intra-class mode dropping [[79](#bib.bibx79)].
    Although FAD may indicate good audio quality, it does not necessarily indicate
    that the sample is desired or relevant [[97](#bib.bibx97)]. For instance, in text-to-speech
    applications, low-FAD audio may be generated that does not match the input text.
    According to [[147](#bib.bibx147)], the FAD measure is not appropriate for evaluating
    text-to-speech models since it was created for music. While according to [[214](#bib.bibx214)],
    calculating the similarity between real and generated samples does not account
    for sample quality. Another similar metric called the Fréchet DeepSpeech Distance
    (FDSD) also uses the Fréchet distance on audio features extracted by a speech
    recognition model [[8](#bib.bibx8)]. [[144](#bib.bibx144)] found the FDSD to be
    unreliable in their use case.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Fréchet音频距离（FAD）是一种感知度量，源自于音频领域的FID。与基于参考的度量不同，FAD使用预训练的音频分类器来测量生成的音频分布与真实音频分布之间的距离，该分类器不使用参考音频样本。VGGish模型
    [[223](#bib.bibx223)] 用于提取生成音频和真实音频的特征[[219](#bib.bibx219)]。与FID类似，分数越低，音频保真度越高。根据[[178](#bib.bibx178)]，FAD与人类对音频质量的感知有很好的相关性。研究发现，FAD对噪声具有鲁棒性，计算效率高，与人类判断一致，对类别内模式丢失敏感[[79](#bib.bibx79)]。尽管FAD可能表明音频质量良好，但它不一定表明样本是理想或相关的[[97](#bib.bibx97)]。例如，在文本到语音应用中，可能会生成低FAD的音频，但与输入文本不匹配。根据[[147](#bib.bibx147)]，FAD度量不适用于评估文本到语音模型，因为它是为音乐创建的。虽然根据[[214](#bib.bibx214)]，计算真实样本与生成样本之间的相似性并未考虑样本质量。另一种类似的度量叫做Fréchet
    DeepSpeech距离（FDSD），也使用Fréchet距离来处理由语音识别模型提取的音频特征[[8](#bib.bibx8)]。[[144](#bib.bibx144)]发现FDSD在他们的用例中不可靠。
- en: The last Fréchet metric that is important to discuss is the Fréchet Distance
    (FD). Unlike the FAD, which extracts features using the VGGish [[223](#bib.bibx223)]
    model, FD employs the PANN [[224](#bib.bibx224)] model. The model change enables
    the FD to use different audio representations as input. PANN [[224](#bib.bibx224)]
    uses mel-spectrogram as input, whereas VGGish [[223](#bib.bibx223)] uses raw waveform.
    FD evaluates audio quality using an audio embedding model to measure the similarity
    between the embedding space of generations and that of targets [[211](#bib.bibx211)].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个重要的Fréchet度量是Fréchet距离（FD）。与使用VGGish [[223](#bib.bibx223)]模型提取特征的FAD不同，FD采用PANN
    [[224](#bib.bibx224)]模型。模型的变化使得FD可以使用不同的音频表示作为输入。PANN [[224](#bib.bibx224)]使用mel-spectrogram作为输入，而VGGish
    [[223](#bib.bibx223)]使用原始波形。FD使用音频嵌入模型来评估音频质量，以测量生成空间和目标空间之间的相似性[[211](#bib.bibx211)]。
- en: VI-D Kullback-Leibler divergence
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D Kullback-Leibler散度
- en: Kullback-Leibler (KL) divergence is a reference-dependent metric that computes
    the divergence between the generated and reference audio distributions. It uses
    a pretrained classifier to obtain the probabilities of generated and reference
    samples and then calculates the KL divergence between the distributions. The probabilities
    are computed over the class predictions of the pretrained classifier. A low KL
    divergence score may indicate that a generated audio sample shares concepts with
    the given reference [[212](#bib.bibx212)]. In music, this could indicate that
    the created audio has similar acoustic characteristics [[97](#bib.bibx97)]. While
    the FAD measure is more related to human perception [[110](#bib.bibx110)], the
    KL measure reflects more on the broader audio concepts occurring in the sample
    [[178](#bib.bibx178)].
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Kullback-Leibler (KL) 散度是一种依赖参考的度量，用于计算生成音频分布与参考音频分布之间的差异。它使用预训练分类器来获取生成样本和参考样本的概率，然后计算分布之间的KL散度。这些概率是基于预训练分类器的类别预测计算的。较低的KL散度分数可能表明生成的音频样本与给定参考样本共享概念[[212](#bib.bibx212)]。在音乐中，这可能表明生成的音频具有类似的声学特征[[97](#bib.bibx97)]。虽然FAD度量与人类感知的相关性更大[[110](#bib.bibx110)]，但KL度量更反映样本中出现的更广泛的音频概念[[178](#bib.bibx178)]。
- en: VII Conclusion
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: The development of deep learning methods has significantly changed the field
    of audio generation. In this work, we have presented three important parts of
    building a deep learning model for the task of audio generation. For audio representation,
    we have presented two long-standing champions and a third up-and-comer. We explained
    five architectures and listed work that implements them in the field of audio
    generation. Finally, we presented the four most common evaluations in the works
    we examined. While the first three architectures mentioned above seem to have
    lost importance in recent years, the transformer and diffusion models seem to
    have taken their place. This could be due to the popularization of large language
    models such as ChatGPT or, in the case of the diffusion models, diffusion-based
    text-to-image generators.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法的发展显著改变了音频生成领域。在这项工作中，我们介绍了构建深度学习模型用于音频生成的三个重要部分。关于音频表示，我们介绍了两个长期以来的佼佼者和一个新的竞争者。我们解释了五种架构，并列出了在音频生成领域实现这些架构的工作。最后，我们展示了我们检查过的作品中最常见的四种评估方法。尽管前三种架构在近年来似乎失去了重要性，但transformer和扩散模型似乎已取而代之。这可能是由于大型语言模型如ChatGPT的普及，或者在扩散模型的情况下，基于扩散的文本到图像生成器的出现。
- en: With the ever-increasing computing power and availability of large databases,
    it looks like the age of deep learning has only just begun. Just as deep learning
    has allowed us to move from domain-dependent features and methods to a more universal
    solution, more recent work has attempted to move from a single task or purpose
    to a multi-task or even multi-modality model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算能力的不断提升和大型数据库的普及，深度学习的时代似乎才刚刚开始。正如深度学习使我们能够从依赖特定领域的特征和方法转向更通用的解决方案，最近的工作也尝试从单一任务或目的转向多任务甚至多模态模型。
- en: References
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Geoffroy Peeters and Gaël Richard “Deep Learning for Audio and Music” In
    *Multi-Faceted Deep Learning* Cham: Springer International Publishing, 2021, pp.
    231–266 DOI: [10.1007/978-3-030-74478-6_10](https://dx.doi.org/10.1007/978-3-030-74478-6_10)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Geoffroy Peeters 和 Gaël Richard 的《音频与音乐的深度学习》，发表在*Multi-Faceted Deep Learning*
    Cham: Springer International Publishing, 2021, 第231–266页 DOI: [10.1007/978-3-030-74478-6_10](https://dx.doi.org/10.1007/978-3-030-74478-6_10)'
- en: '[2] Yuanjun Zhao, Xianjun Xia and Roberto Togneri “Applications of Deep Learning
    to Audio Generation” In *IEEE Circuits and Systems Magazine* 19.4, 2019, pp. 19–38
    DOI: [10.1109/MCAS.2019.2945210](https://dx.doi.org/10.1109/MCAS.2019.2945210)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yuanjun Zhao, Xianjun Xia 和 Roberto Togneri 的《深度学习在音频生成中的应用》，发表在*IEEE Circuits
    and Systems Magazine* 19.4, 2019, 第19–38页 DOI: [10.1109/MCAS.2019.2945210](https://dx.doi.org/10.1109/MCAS.2019.2945210)'
- en: '[3] Hendrik Purwins et al. “Deep Learning for Audio Signal Processing” In *IEEE
    Journal of Selected Topics in Signal Processing* 13.2, 2019, pp. 206–219 DOI:
    [10.1109/JSTSP.2019.2908700](https://dx.doi.org/10.1109/JSTSP.2019.2908700)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Hendrik Purwins 等的《音频信号处理中的深度学习》，发表在*IEEE Journal of Selected Topics in
    Signal Processing* 13.2, 2019, 第206–219页 DOI: [10.1109/JSTSP.2019.2908700](https://dx.doi.org/10.1109/JSTSP.2019.2908700)'
- en: '[4] Jean-Pierre Briot, Gaëtan Hadjeres and François-David Pachet “Deep Learning
    Techniques for Music Generation – A Survey”, 2019 DOI: [10.48550/arXiv.1709.01620](https://dx.doi.org/10.48550/arXiv.1709.01620)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Jean-Pierre Briot, Gaëtan Hadjeres 和 François-David Pachet 的《音乐生成的深度学习技术——综述》，2019
    DOI: [10.48550/arXiv.1709.01620](https://dx.doi.org/10.48550/arXiv.1709.01620)'
- en: '[5] M. Huzaifah and L. Wyse “Deep Generative Models for Musical Audio Synthesis”,
    2020 arXiv: [http://arxiv.org/abs/2006.06426](http://arxiv.org/abs/2006.06426)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Huzaifah 和 L. Wyse “用于音乐音频合成的深度生成模型”，2020 arXiv: [http://arxiv.org/abs/2006.06426](http://arxiv.org/abs/2006.06426)'
- en: '[6] Xu Tan, Tao Qin, Frank Soong and Tie-Yan Liu “A Survey on Neural Speech
    Synthesis”, 2021 DOI: [10.48550/arXiv.2106.15561](https://dx.doi.org/10.48550/arXiv.2106.15561)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Xu Tan, Tao Qin, Frank Soong 和 Tie-Yan Liu “神经语音合成综述”，2021 DOI: [10.48550/arXiv.2106.15561](https://dx.doi.org/10.48550/arXiv.2106.15561)'
- en: '[7] Zhaofeng Shi “A Survey on Audio Synthesis and Audio-Visual Multimodal Processing”,
    2021 DOI: [10.48550/arXiv.2108.00443](https://dx.doi.org/10.48550/arXiv.2108.00443)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Zhaofeng Shi “音频合成与音视频多模态处理综述”，2021 DOI: [10.48550/arXiv.2108.00443](https://dx.doi.org/10.48550/arXiv.2108.00443)'
- en: '[8] Anastasia Natsiou and Seán O’Leary “Audio Representations for Deep Learning
    in Sound Synthesis: A Review” In *2021 IEEE/ACS 18th International Conference
    on Computer Systems and Applications (AICCSA)*, 2021, pp. 1–8 DOI: [10.1109/AICCSA53542.2021.9686838](https://dx.doi.org/10.1109/AICCSA53542.2021.9686838)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Anastasia Natsiou 和 Seán O’Leary “声音合成中深度学习的音频表示：一项综述” 载于 *2021 IEEE/ACS
    第18届国际计算机系统与应用大会（AICCSA）*，2021, 第1–8页 DOI: [10.1109/AICCSA53542.2021.9686838](https://dx.doi.org/10.1109/AICCSA53542.2021.9686838)'
- en: '[9] Siddique Latif et al. “Transformers in Speech Processing: A Survey”, 2023
    DOI: [10.48550/arXiv.2303.11607](https://dx.doi.org/10.48550/arXiv.2303.11607)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Siddique Latif 等. “语音处理中的变压器：一项综述”，2023 DOI: [10.48550/arXiv.2303.11607](https://dx.doi.org/10.48550/arXiv.2303.11607)'
- en: '[10] Chenshuang Zhang et al. “A Survey on Audio Diffusion Models: Text To Speech
    Synthesis and Enhancement in Generative AI”, 2023 DOI: [10.48550/arXiv.2303.13336](https://dx.doi.org/10.48550/arXiv.2303.13336)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Chenshuang Zhang 等. “音频扩散模型综述：文本到语音合成与生成AI中的增强”，2023 DOI: [10.48550/arXiv.2303.13336](https://dx.doi.org/10.48550/arXiv.2303.13336)'
- en: '[11] Ambuj Mehrish et al. “A Review of Deep Learning Techniques for Speech
    Processing” In *Information Fusion* 99, 2023, pp. 101869 DOI: [10.1016/j.inffus.2023.101869](https://dx.doi.org/10.1016/j.inffus.2023.101869)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Ambuj Mehrish 等. “深度学习技术在语音处理中的综述” 载于 *信息融合* 99, 2023, 第101869页 DOI: [10.1016/j.inffus.2023.101869](https://dx.doi.org/10.1016/j.inffus.2023.101869)'
- en: '[12] Zhen-Hua Ling et al. “Deep Learning for Acoustic Modeling in Parametric
    Speech Generation: A Systematic Review of Existing Techniques and Future Trends”
    In *IEEE Signal Processing Magazine* 32.3, 2015, pp. 35–52 DOI: [10.1109/MSP.2014.2359987](https://dx.doi.org/10.1109/MSP.2014.2359987)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Zhen-Hua Ling 等. “用于参数化语音生成的声学建模深度学习：现有技术的系统回顾及未来趋势” 载于 *IEEE信号处理杂志* 32.3,
    2015, 第35–52页 DOI: [10.1109/MSP.2014.2359987](https://dx.doi.org/10.1109/MSP.2014.2359987)'
- en: '[13] “Springer Handbook of Speech Processing”, Springer Handbooks Berlin, Heidelberg:
    Springer Berlin Heidelberg, 2008 DOI: [10.1007/978-3-540-49127-9](https://dx.doi.org/10.1007/978-3-540-49127-9)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] “Springer语音处理手册”，Springer手册 柏林，海德堡：Springer Berlin Heidelberg, 2008 DOI:
    [10.1007/978-3-540-49127-9](https://dx.doi.org/10.1007/978-3-540-49127-9)'
- en: '[14] Tomoki Toda and Keiichi Tokuda “A Speech Parameter Generation Algorithm
    Considering Global Variance for HMM-Based Speech Synthesis” In *IEICE TRANSACTIONS
    on Information and Systems* E90-D.5 The Institute of Electronics, Information
    and Communication Engineers, 2007, pp. 816–824'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Tomoki Toda 和 Keiichi Tokuda “考虑全局方差的HMM语音合成参数生成算法” 载于 *IEICE 信息与系统汇刊*
    E90-D.5 电子信息通信学会, 2007, 第816–824页'
- en: '[15] Paul Taylor “Text-to-Speech Synthesis” Cambridge: Cambridge University
    Press, 2009 DOI: [10.1017/CBO9780511816338](https://dx.doi.org/10.1017/CBO9780511816338)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Paul Taylor “文本到语音合成” 剑桥：剑桥大学出版社, 2009 DOI: [10.1017/CBO9780511816338](https://dx.doi.org/10.1017/CBO9780511816338)'
- en: '[16] A.. Liberman et al. “Minimal Rules for Synthesizing Speech” In *The Journal
    of the Acoustical Society of America* 31.11, 1959, pp. 1490–1499 DOI: [10.1121/1.1907654](https://dx.doi.org/10.1121/1.1907654)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] A.. Liberman 等. “合成语音的最小规则” 载于 *美国声学学会期刊* 31.11, 1959, 第1490–1499页 DOI:
    [10.1121/1.1907654](https://dx.doi.org/10.1121/1.1907654)'
- en: '[17] J.. Holmes, Ignatius G. Mattingly and J.. Shearme “Speech Synthesis by
    Rule” In *Language and Speech* 7.3 SAGE Publications Ltd, 1964, pp. 127–143 DOI:
    [10.1177/002383096400700301](https://dx.doi.org/10.1177/002383096400700301)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J.. Holmes, Ignatius G. Mattingly 和 J.. Shearme “按规则合成语音” 载于 *语言与语音* 7.3
    SAGE Publications Ltd, 1964, 第127–143页 DOI: [10.1177/002383096400700301](https://dx.doi.org/10.1177/002383096400700301)'
- en: '[18] C. Coker, N. Umeda and C. Browman “Automatic Synthesis from Ordinary English
    Text” In *IEEE Transactions on Audio and Electroacoustics* 21.3, 1973, pp. 293–298
    DOI: [10.1109/TAU.1973.1162458](https://dx.doi.org/10.1109/TAU.1973.1162458)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] C. Coker, N. Umeda 和 C. Browman “从普通英语文本自动合成” 发表在 *IEEE Transactions on
    Audio and Electroacoustics* 21.3, 1973, 页码 293–298 DOI: [10.1109/TAU.1973.1162458](https://dx.doi.org/10.1109/TAU.1973.1162458)'
- en: '[19] D. Klatt “Structure of a Phonological Rule Component for a Synthesis-by-Rule
    Program” In *IEEE Transactions on Acoustics, Speech, and Signal Processing* 24.5,
    1976, pp. 391–398 DOI: [10.1109/TASSP.1976.1162847](https://dx.doi.org/10.1109/TASSP.1976.1162847)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] D. Klatt “用于规则合成程序的音位规则组件结构” 发表在 *IEEE Transactions on Acoustics, Speech,
    and Signal Processing* 24.5, 1976, 页码 391–398 DOI: [10.1109/TASSP.1976.1162847](https://dx.doi.org/10.1109/TASSP.1976.1162847)'
- en: '[20] N. Dixon and H. Maxey “Terminal Analog Synthesis of Continuous Speech
    Using the Diphone Method of Segment Assembly” In *IEEE Transactions on Audio and
    Electroacoustics* 16.1, 1968, pp. 40–50 DOI: [10.1109/TAU.1968.1161948](https://dx.doi.org/10.1109/TAU.1968.1161948)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] N. Dixon 和 H. Maxey “使用双音节方法进行连续语音的终端模拟合成” 发表在 *IEEE Transactions on Audio
    and Electroacoustics* 16.1, 1968, 页码 40–50 DOI: [10.1109/TAU.1968.1161948](https://dx.doi.org/10.1109/TAU.1968.1161948)'
- en: '[21] Y. Sagisaka “Speech Synthesis by Rule Using an Optimal Selection of Non-Uniform
    Synthesis Units” IEEE Computer Society, 1988, pp. 679\bibrangessep680\bibrangessep681\bibrangessep682–679\bibrangessep680\bibrangessep681\bibrangessep682
    DOI: [10.1109/ICASSP.1988.196677](https://dx.doi.org/10.1109/ICASSP.1988.196677)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Sagisaka “通过规则进行语音合成，使用非均匀合成单元的最优选择” IEEE计算机学会, 1988, 页码 679\bibrangessep680\bibrangessep681\bibrangessep682–679\bibrangessep680\bibrangessep681\bibrangessep682
    DOI: [10.1109/ICASSP.1988.196677](https://dx.doi.org/10.1109/ICASSP.1988.196677)'
- en: '[22] A.J. Hunt and A.W. Black “Unit Selection in a Concatenative Speech Synthesis
    System Using a Large Speech Database” In *1996 IEEE International Conference on
    Acoustics, Speech, and Signal Processing Conference Proceedings* 1, 1996, pp.
    373–376 vol. 1 DOI: [10.1109/ICASSP.1996.541110](https://dx.doi.org/10.1109/ICASSP.1996.541110)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A.J. Hunt 和 A.W. Black “在连接语音合成系统中使用大型语音数据库进行单元选择” 发表在 *1996 IEEE International
    Conference on Acoustics, Speech, and Signal Processing Conference Proceedings*
    1, 1996, 页码 373–376 vol. 1 DOI: [10.1109/ICASSP.1996.541110](https://dx.doi.org/10.1109/ICASSP.1996.541110)'
- en: '[23] Heiga Zen, Keiichi Tokuda and Alan W. Black “Statistical Parametric Speech
    Synthesis” In *Speech Communication* 51.11, 2009, pp. 1039–1064 DOI: [10.1016/j.specom.2009.04.004](https://dx.doi.org/10.1016/j.specom.2009.04.004)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Heiga Zen, Keiichi Tokuda 和 Alan W. Black “统计参数语音合成” 发表在 *Speech Communication*
    51.11, 2009, 页码 1039–1064 DOI: [10.1016/j.specom.2009.04.004](https://dx.doi.org/10.1016/j.specom.2009.04.004)'
- en: '[24] Kazuhiro Nakamura, Kei Hashimoto, Yoshihiko Nankaku and Keiichi Tokuda
    “Integration of Spectral Feature Extraction and Modeling for HMM-Based Speech
    Synthesis” In *IEICE Transactions on Information and Systems* E97.D.6, 2014, pp.
    1438–1448 DOI: [10.1587/transinf.E97.D.1438](https://dx.doi.org/10.1587/transinf.E97.D.1438)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Kazuhiro Nakamura, Kei Hashimoto, Yoshihiko Nankaku 和 Keiichi Tokuda “HMM-based
    语音合成中的光谱特征提取和建模的整合” 发表在 *IEICE Transactions on Information and Systems* E97.D.6,
    2014, 页码 1438–1448 DOI: [10.1587/transinf.E97.D.1438](https://dx.doi.org/10.1587/transinf.E97.D.1438)'
- en: '[25] Takayoshi Yoshimura et al. “Simultaneous Modeling of Spectrum, Pitch and
    Duration in HMM-based Speech Synthesis” In *6th European Conference on Speech
    Communication and Technology (Eurospeech 1999)* ISCA, 1999, pp. 2347–2350 DOI:
    [10.21437/Eurospeech.1999-513](https://dx.doi.org/10.21437/Eurospeech.1999-513)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Takayoshi Yoshimura 等 “HMM-based 语音合成中谱、音高和时长的同时建模” 发表在 *6th European
    Conference on Speech Communication and Technology (Eurospeech 1999)* ISCA, 1999,
    页码 2347–2350 DOI: [10.21437/Eurospeech.1999-513](https://dx.doi.org/10.21437/Eurospeech.1999-513)'
- en: '[26] Keiichi Tokuda, Heiga Zen and Alan W. Black “An HMM-based Speech Synthesis
    System Applied to English” In *Proc. IEEE Workshop on Speech Synthesis, 2002*,
    2002 URL: [https://cir.nii.ac.jp/crid/1572543025105746048](https://cir.nii.ac.jp/crid/1572543025105746048)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Keiichi Tokuda, Heiga Zen 和 Alan W. Black “基于HMM的英语语音合成系统” 发表在 *Proc.
    IEEE Workshop on Speech Synthesis, 2002*, 2002 URL: [https://cir.nii.ac.jp/crid/1572543025105746048](https://cir.nii.ac.jp/crid/1572543025105746048)'
- en: '[27] Keiichi Tokuda et al. “Speech Synthesis Based on Hidden Markov Models”
    In *Proceedings of the IEEE* 101.5, 2013, pp. 1234–1252 DOI: [10.1109/JPROC.2013.2251852](https://dx.doi.org/10.1109/JPROC.2013.2251852)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Keiichi Tokuda 等 “基于隐马尔可夫模型的语音合成” 发表在 *Proceedings of the IEEE* 101.5,
    2013, 页码 1234–1252 DOI: [10.1109/JPROC.2013.2251852](https://dx.doi.org/10.1109/JPROC.2013.2251852)'
- en: '[28] Heiga Zen, Andrew Senior and Mike Schuster “Statistical Parametric Speech
    Synthesis Using Deep Neural Networks” In *2013 IEEE International Conference on
    Acoustics, Speech and Signal Processing*, 2013, pp. 7962–7966 DOI: [10.1109/ICASSP.2013.6639215](https://dx.doi.org/10.1109/ICASSP.2013.6639215)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Heiga Zen、Andrew Senior 和 Mike Schuster “使用深度神经网络的统计参数语音合成” 发表在 *2013
    IEEE国际声学、语音与信号处理会议*，2013年，第7962–7966页 DOI: [10.1109/ICASSP.2013.6639215](https://dx.doi.org/10.1109/ICASSP.2013.6639215)'
- en: '[29] J. Burniston and K.M. Curtis “A Hybrid Neural Network/Rule Based Architecture
    for Diphone Speech Synthesis” In *Proceedings of ICSIPNN ’94\. International Conference
    on Speech, Image Processing and Neural Networks*, 1994, pp. 323–326 vol.1 DOI:
    [10.1109/SIPNN.1994.344901](https://dx.doi.org/10.1109/SIPNN.1994.344901)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Burniston 和 K.M. Curtis “一种混合神经网络/规则基础的双音语音合成架构” 发表在 *ICSIPNN ’94国际会议上的语音、图像处理和神经网络*，1994年，第323–326页
    第1卷 DOI: [10.1109/SIPNN.1994.344901](https://dx.doi.org/10.1109/SIPNN.1994.344901)'
- en: '[30] Zhen-Hua Ling, Li Deng and Dong Yu “Modeling Spectral Envelopes Using
    Restricted Boltzmann Machines and Deep Belief Networks for Statistical Parametric
    Speech Synthesis” In *IEEE Transactions on Audio, Speech, and Language Processing*
    21.10, 2013, pp. 2129–2139 DOI: [10.1109/TASL.2013.2269291](https://dx.doi.org/10.1109/TASL.2013.2269291)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Zhen-Hua Ling、Li Deng 和 Dong Yu “使用限制玻尔兹曼机和深度信念网络建模谱包络，用于统计参数语音合成” 发表在
    *IEEE音频、语音与语言处理汇刊* 21.10，2013年，第2129–2139页 DOI: [10.1109/TASL.2013.2269291](https://dx.doi.org/10.1109/TASL.2013.2269291)'
- en: '[31] T. Weijters and J. Thole “Speech Synthesis with Artificial Neural Networks”
    In *IEEE International Conference on Neural Networks*, 1993, pp. 1764–1769 vol.3
    DOI: [10.1109/ICNN.1993.298824](https://dx.doi.org/10.1109/ICNN.1993.298824)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] T. Weijters 和 J. Thole “使用人工神经网络的语音合成” 发表在 *IEEE国际神经网络会议*，1993年，第1764–1769页
    第3卷 DOI: [10.1109/ICNN.1993.298824](https://dx.doi.org/10.1109/ICNN.1993.298824)'
- en: '[32] Heng Lu, Simon King and Oliver Watts “Combining a Vector Space Representation
    of Linguistic Context with a Deep Neural Network for Text-To-Speech Synthesis”
    In *8th ISCA Speech Synthesis Workshop*, 2013, pp. 261–265 URL: [https://www.research.ed.ac.uk/en/publications/combining-a-vector-space-representation-of-linguistic-context-wit](https://www.research.ed.ac.uk/en/publications/combining-a-vector-space-representation-of-linguistic-context-wit)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Heng Lu、Simon King 和 Oliver Watts “将语言上下文的向量空间表示与深度神经网络结合用于文本到语音合成” 发表在
    *第八届ISCA语音合成研讨会*，2013年，第261–265页 URL: [https://www.research.ed.ac.uk/en/publications/combining-a-vector-space-representation-of-linguistic-context-wit](https://www.research.ed.ac.uk/en/publications/combining-a-vector-space-representation-of-linguistic-context-wit)'
- en: '[33] Yuchen Fan, Yao Qian, Frank K. Soong and Lei He “Multi-Speaker Modeling
    and Speaker Adaptation for DNN-based TTS Synthesis” In *2015 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2015, pp. 4475–4479
    DOI: [10.1109/ICASSP.2015.7178817](https://dx.doi.org/10.1109/ICASSP.2015.7178817)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Yuchen Fan、Yao Qian、Frank K. Soong 和 Lei He “基于DNN的TTS合成的多说话人建模与说话人适应”
    发表在 *2015 IEEE国际声学、语音与信号处理会议 (ICASSP)*，2015年，第4475–4479页 DOI: [10.1109/ICASSP.2015.7178817](https://dx.doi.org/10.1109/ICASSP.2015.7178817)'
- en: '[34] Keiichi Tokuday and Heiga Zen “Directly Modeling Speech Waveforms by Neural
    Networks for Statistical Parametric Speech Synthesis” In *2015 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2015, pp. 4215–4219
    DOI: [10.1109/ICASSP.2015.7178765](https://dx.doi.org/10.1109/ICASSP.2015.7178765)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Keiichi Tokuday 和 Heiga Zen “通过神经网络直接建模语音波形用于统计参数语音合成” 发表在 *2015 IEEE国际声学、语音与信号处理会议
    (ICASSP)*，2015年，第4215–4219页 DOI: [10.1109/ICASSP.2015.7178765](https://dx.doi.org/10.1109/ICASSP.2015.7178765)'
- en: '[35] Orhan Karaali, Gerald Corrigan and Ira Gerson “Speech Synthesis with Neural
    Networks”, 1998 arXiv: [http://arxiv.org/abs/cs/9811031](http://arxiv.org/abs/cs/9811031)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Orhan Karaali、Gerald Corrigan 和 Ira Gerson “使用神经网络的语音合成”，1998年 arXiv:
    [http://arxiv.org/abs/cs/9811031](http://arxiv.org/abs/cs/9811031)'
- en: '[36] Orhan Karaali, Gerald Corrigan, Ira Gerson and Noel Massey “Text-To-Speech
    Conversion with Neural Networks: A Recurrent TDNN Approach”, 1998 DOI: [10.48550/arXiv.cs/9811032](https://dx.doi.org/10.48550/arXiv.cs/9811032)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Orhan Karaali、Gerald Corrigan、Ira Gerson 和 Noel Massey “使用神经网络的文本到语音转换：一种递归TDNN方法”，1998年
    DOI: [10.48550/arXiv.cs/9811032](https://dx.doi.org/10.48550/arXiv.cs/9811032)'
- en: '[37] Yuchen Fan, Yao Qian, Feng-Long Xie and Frank K. Soong “TTS Synthesis
    with Bidirectional LSTM Based Recurrent Neural Networks” In *Interspeech 2014*
    ISCA, 2014, pp. 1964–1968 DOI: [10.21437/Interspeech.2014-443](https://dx.doi.org/10.21437/Interspeech.2014-443)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Yuchen Fan、Yao Qian、Feng-Long Xie 和 Frank K. Soong “基于双向LSTM的递归神经网络的TTS合成”
    发表在 *Interspeech 2014* ISCA，2014年，第1964–1968页 DOI: [10.21437/Interspeech.2014-443](https://dx.doi.org/10.21437/Interspeech.2014-443)'
- en: '[38] Heiga Zen and Haşim Sak “Unidirectional Long Short-Term Memory Recurrent
    Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis” In
    *2015 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, 2015, pp. 4470–4474 DOI: [10.1109/ICASSP.2015.7178816](https://dx.doi.org/10.1109/ICASSP.2015.7178816)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Heiga Zen 和 Haşim Sak “具有递归输出层的单向长短期记忆递归神经网络用于低延迟语音合成” 载于*2015 IEEE国际声学、语音和信号处理会议
    (ICASSP)*，2015年，第4470–4474页 DOI: [10.1109/ICASSP.2015.7178816](https://dx.doi.org/10.1109/ICASSP.2015.7178816)'
- en: '[39] Bo Li and Heiga Zen “Multi-Language Multi-Speaker Acoustic Modeling for
    LSTM-RNN Based Statistical Parametric Speech Synthesis” In *Interspeech 2016*
    ISCA, 2016, pp. 2468–2472 DOI: [10.21437/Interspeech.2016-172](https://dx.doi.org/10.21437/Interspeech.2016-172)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Bo Li 和 Heiga Zen “基于LSTM-RNN的多语言多说话人声学建模” 载于*Interspeech 2016* ISCA，2016年，第2468–2472页
    DOI: [10.21437/Interspeech.2016-172](https://dx.doi.org/10.21437/Interspeech.2016-172)'
- en: '[40] Keiichi Tokuda and Heiga Zen “Directly Modeling Voiced and Unvoiced Components
    in Speech Waveforms by Neural Networks” In *2016 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2016, pp. 5640–5644 DOI:
    [10.1109/ICASSP.2016.7472757](https://dx.doi.org/10.1109/ICASSP.2016.7472757)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Keiichi Tokuda 和 Heiga Zen “通过神经网络直接建模语音波形中的有声和无声成分” 载于*2016 IEEE国际声学、语音和信号处理会议
    (ICASSP)*，2016年，第5640–5644页 DOI: [10.1109/ICASSP.2016.7472757](https://dx.doi.org/10.1109/ICASSP.2016.7472757)'
- en: '[41] Wenfu Wang, Shuang Xu and Bo Xu “Gating Recurrent Mixture Density Networks
    for Acoustic Modeling in Statistical Parametric Speech Synthesis” In *2016 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2016, pp. 5520–5524 DOI: [10.1109/ICASSP.2016.7472733](https://dx.doi.org/10.1109/ICASSP.2016.7472733)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Wenfu Wang, Shuang Xu 和 Bo Xu “用于声学建模的门控递归混合密度网络” 载于*2016 IEEE国际声学、语音和信号处理会议
    (ICASSP)*，2016年，第5520–5524页 DOI: [10.1109/ICASSP.2016.7472733](https://dx.doi.org/10.1109/ICASSP.2016.7472733)'
- en: '[42] Wenfu Wang, Shuang Xu and Bo Xu “First Step Towards End-to-End Parametric
    TTS Synthesis: Generating Spectral Parameters with Neural Attention” In *Interspeech
    2016* ISCA, 2016, pp. 2243–2247 DOI: [10.21437/Interspeech.2016-134](https://dx.doi.org/10.21437/Interspeech.2016-134)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Wenfu Wang, Shuang Xu 和 Bo Xu “迈向端到端参数TTS合成的第一步：使用神经注意力生成谱参数” 载于*Interspeech
    2016* ISCA，2016年，第2243–2247页 DOI: [10.21437/Interspeech.2016-134](https://dx.doi.org/10.21437/Interspeech.2016-134)'
- en: '[43] Heiga Zen and Andrew Senior “Deep Mixture Density Networks for Acoustic
    Modeling in Statistical Parametric Speech Synthesis” In *2014 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2014, pp. 3844–3848
    DOI: [10.1109/ICASSP.2014.6854321](https://dx.doi.org/10.1109/ICASSP.2014.6854321)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Heiga Zen 和 Andrew Senior “用于声学建模的深度混合密度网络” 载于*2014 IEEE国际声学、语音和信号处理会议
    (ICASSP)*，2014年，第3844–3848页 DOI: [10.1109/ICASSP.2014.6854321](https://dx.doi.org/10.1109/ICASSP.2014.6854321)'
- en: '[44] Kanishka Rao, Fuchun Peng, Haşim Sak and Françoise Beaufays “Grapheme-to-Phoneme
    Conversion Using Long Short-Term Memory Recurrent Neural Networks” In *2015 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2015, pp. 4225–4229 DOI: [10.1109/ICASSP.2015.7178767](https://dx.doi.org/10.1109/ICASSP.2015.7178767)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Kanishka Rao, Fuchun Peng, Haşim Sak 和 Françoise Beaufays “使用长短期记忆递归神经网络的字形到音素转换”
    载于*2015 IEEE国际声学、语音和信号处理会议 (ICASSP)*，2015年，第4225–4229页 DOI: [10.1109/ICASSP.2015.7178767](https://dx.doi.org/10.1109/ICASSP.2015.7178767)'
- en: '[45] Kaisheng Yao and Geoffrey Zweig “Sequence-to-Sequence Neural Net Models
    for Grapheme-to-Phoneme Conversion”, 2015 DOI: [10.48550/arXiv.1506.00196](https://dx.doi.org/10.48550/arXiv.1506.00196)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Kaisheng Yao 和 Geoffrey Zweig “用于字形到音素转换的序列到序列神经网络模型”，2015年 DOI: [10.48550/arXiv.1506.00196](https://dx.doi.org/10.48550/arXiv.1506.00196)'
- en: '[46] S. Imai “Cepstral Analysis Synthesis on the Mel Frequency Scale” In *ICASSP
    ’83\. IEEE International Conference on Acoustics, Speech, and Signal Processing*
    8, 1983, pp. 93–96 DOI: [10.1109/ICASSP.1983.1172250](https://dx.doi.org/10.1109/ICASSP.1983.1172250)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] S. Imai “梅尔频率尺度上的倒谱分析合成” 载于*ICASSP ’83\. IEEE国际声学、语音和信号处理会议* 8，1983年，第93–96页
    DOI: [10.1109/ICASSP.1983.1172250](https://dx.doi.org/10.1109/ICASSP.1983.1172250)'
- en: '[47] Hideki Kawahara, Ikuyo Masuda-Katsuse and Alain Cheveigné “Restructuring
    Speech Representations Using a Pitch-Adaptive Time–Frequency Smoothing and an
    Instantaneous-Frequency-Based F0 Extraction: Possible Role of a Repetitive Structure
    in Sounds1” In *Speech Communication* 27.3, 1999, pp. 187–207 DOI: [10.1016/S0167-6393(98)00085-5](https://dx.doi.org/10.1016/S0167-6393(98)00085-5)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Hideki Kawahara, Ikuyo Masuda-Katsuse 和 Alain Cheveigné “使用音高自适应时频平滑和基于瞬时频率的F0提取重构语音表示：声音中的重复结构可能的作用”
    见 *Speech Communication* 27.3, 1999, 页 187–207 DOI: [10.1016/S0167-6393(98)00085-5](https://dx.doi.org/10.1016/S0167-6393(98)00085-5)'
- en: '[48] Yannis Agiomyrgiannakis “Vocaine the Vocoder and Applications in Speech
    Synthesis” In *2015 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, 2015, pp. 4230–4234 DOI: [10.1109/ICASSP.2015.7178768](https://dx.doi.org/10.1109/ICASSP.2015.7178768)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Yannis Agiomyrgiannakis “Vocaine 语音编码器及其在语音合成中的应用” 见 *2015 IEEE国际声学、语音和信号处理会议
    (ICASSP)*, 2015, 页 4230–4234 DOI: [10.1109/ICASSP.2015.7178768](https://dx.doi.org/10.1109/ICASSP.2015.7178768)'
- en: '[49] Victor Lavrenko and Jeremy Pickens “Polyphonic Music Modeling with Random
    Fields” In *Proceedings of the Eleventh ACM International Conference on Multimedia*,
    MULTIMEDIA ’03 New York, NY, USA: Association for Computing Machinery, 2003, pp.
    120–129 DOI: [10.1145/957013.957041](https://dx.doi.org/10.1145/957013.957041)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Victor Lavrenko 和 Jeremy Pickens “多声部音乐建模与随机场” 见 *第十一届ACM国际多媒体会议论文集*,
    MULTIMEDIA ’03 纽约, NY, USA: 计算机协会, 2003, 页 120–129 DOI: [10.1145/957013.957041](https://dx.doi.org/10.1145/957013.957041)'
- en: '[50] J.. Fernandez and F. Vico “AI Methods in Algorithmic Composition: A Comprehensive
    Survey” In *Journal of Artificial Intelligence Research* 48, 2013, pp. 513–582
    DOI: [10.1613/jair.3908](https://dx.doi.org/10.1613/jair.3908)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J.. Fernandez 和 F. Vico “算法作曲中的AI方法：全面调查” 见 *Journal of Artificial Intelligence
    Research* 48, 2013, 页 513–582 DOI: [10.1613/jair.3908](https://dx.doi.org/10.1613/jair.3908)'
- en: '[51] Jeff Pressing “Nonlinear Maps as Generators of Musical Design” In *Computer
    Music Journal* 12.2 The MIT Press, 1988, pp. 35–46 DOI: [10.2307/3679940](https://dx.doi.org/10.2307/3679940)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Jeff Pressing “非线性映射作为音乐设计的生成器” 见 *Computer Music Journal* 12.2 MIT出版社,
    1988, 页 35–46 DOI: [10.2307/3679940](https://dx.doi.org/10.2307/3679940)'
- en: '[52] Charles Dodge and Thomas A. Jerse “Computer Music: Synthesis, Composition,
    and Performance” London: Macmillan, 1985'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Charles Dodge 和 Thomas A. Jerse “计算机音乐：合成、作曲与表演” 伦敦：麦克米伦出版社, 1985'
- en: '[53] Francesco Giomi and Marco Ligabue “Computational Generation and Study
    of Jazz Music” In *Interface* 20.1 Routledge, 1991, pp. 47–64 DOI: [10.1080/09298219108570576](https://dx.doi.org/10.1080/09298219108570576)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Francesco Giomi 和 Marco Ligabue “计算生成和研究爵士音乐” 见 *Interface* 20.1 Routledge,
    1991, 页 47–64 DOI: [10.1080/09298219108570576](https://dx.doi.org/10.1080/09298219108570576)'
- en: '[54] Charles Ames and Michael Domino “Cybernetic Composer: An Overview” In
    *Understanding Music with AI: Perspectives on Music Cognition* Cambridge, MA,
    USA: MIT Press, 1992, pp. 186–205'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Charles Ames 和 Michael Domino “控制论作曲家：概述” 见 *Understanding Music with
    AI: Perspectives on Music Cognition* 剑桥, MA, USA: MIT出版社, 1992, 页 186–205'
- en: '[55] John Biles “GenJam: A Genetic Algorithm for Generating Jazz Solos” In
    *ICMC* 94 Ann Arbor, MI, 1994, pp. 131–137'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] John Biles “GenJam: 一种生成爵士独奏的遗传算法” 见 *ICMC* 94 Ann Arbor, MI, 1994, 页
    131–137'
- en: '[56] Artemis Moroni, Jônatas Manzolli, Fernando Von Zuben and Ricardo Gudwin
    “Vox Populi: An Interactive Evolutionary System for Algorithmic Music Composition”
    In *Leonardo Music Journal* 10, 2000, pp. 49–54 DOI: [10.1162/096112100570602](https://dx.doi.org/10.1162/096112100570602)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Artemis Moroni, Jônatas Manzolli, Fernando Von Zuben 和 Ricardo Gudwin
    “Vox Populi：用于算法音乐作曲的交互式进化系统” 见 *Leonardo Music Journal* 10, 2000, 页 49–54 DOI:
    [10.1162/096112100570602](https://dx.doi.org/10.1162/096112100570602)'
- en: '[57] C.-C.J. Chen and R. Miikkulainen “Creating Melodies with Evolving Recurrent
    Neural Networks” In *IJCNN’01\. International Joint Conference on Neural Networks.
    Proceedings (Cat. No.01CH37222)* 3, 2001, pp. 2241–2246 vol.3 DOI: [10.1109/IJCNN.2001.938515](https://dx.doi.org/10.1109/IJCNN.2001.938515)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] C.-C.J. Chen 和 R. Miikkulainen “用进化递归神经网络创作旋律” 见 *IJCNN’01\. 国际联合神经网络会议.
    会议录 (Cat. No.01CH37222)* 3, 2001, 页 2241–2246 vol.3 DOI: [10.1109/IJCNN.2001.938515](https://dx.doi.org/10.1109/IJCNN.2001.938515)'
- en: '[58] Alfonso Ortega Puente, Rafael Sánchez Alfonso and Manuel Alfonseca Moreno
    “Automatic Composition of Music by Means of Grammatical Evolution” In *Proceedings
    of the 2002 Conference on APL: Array Processing Languages: Lore, Problems, and
    Applications*, APL ’02 New York, NY, USA: Association for Computing Machinery,
    2002, pp. 148–155 DOI: [10.1145/602231.602249](https://dx.doi.org/10.1145/602231.602249)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Alfonso Ortega Puente, Rafael Sánchez Alfonso 和 Manuel Alfonseca Moreno
    “通过语法进化自动作曲” 载于 *2002年APL大会论文集：数组处理语言：传说、问题与应用*，APL ’02 纽约, NY, 美国：计算机协会, 2002,
    第148–155页 DOI: [10.1145/602231.602249](https://dx.doi.org/10.1145/602231.602249)'
- en: '[59] MICHAEL C. MOZER “Neural Network Music Composition by Prediction: Exploring
    the Benefits of Psychoacoustic Constraints and Multi-scale Processing” In *Connection
    Science* 6.2-3 Taylor & Francis, 1994, pp. 247–280 DOI: [10.1080/09540099408915726](https://dx.doi.org/10.1080/09540099408915726)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] MICHAEL C. MOZER “通过预测的神经网络音乐创作：探索心理声学约束和多尺度处理的好处” 载于 *连接科学* 6.2-3 泰勒与弗朗西斯,
    1994, 第247–280页 DOI: [10.1080/09540099408915726](https://dx.doi.org/10.1080/09540099408915726)'
- en: '[60] Douglas Eck and Juergen Schmidhuber “A First Look at Music Composition
    Using Lstm Recurrent Neural Networks” In *Istituto Dalle Molle Di Studi Sull Intelligenza
    Artificiale* 103.4, 2002, pp. 48–56'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Douglas Eck 和 Juergen Schmidhuber “使用LSTM递归神经网络进行音乐创作的初步研究” 载于 *Dalle
    Molle人工智能研究所* 103.4, 2002, 第48–56页'
- en: '[61] Hang Chu, Raquel Urtasun and Sanja Fidler “Song From PI: A Musically Plausible
    Network for Pop Music Generation”, 2016 DOI: [10.48550/arXiv.1611.03477](https://dx.doi.org/10.48550/arXiv.1611.03477)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Hang Chu, Raquel Urtasun 和 Sanja Fidler “PI的歌曲：一个音乐上合理的流行音乐生成网络”，2016
    DOI: [10.48550/arXiv.1611.03477](https://dx.doi.org/10.48550/arXiv.1611.03477)'
- en: '[62] Allen Huang and Raymond Wu “Deep Learning for Music”, 2016 DOI: [10.48550/arXiv.1606.04930](https://dx.doi.org/10.48550/arXiv.1606.04930)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Allen Huang 和 Raymond Wu “音乐的深度学习”，2016 DOI: [10.48550/arXiv.1606.04930](https://dx.doi.org/10.48550/arXiv.1606.04930)'
- en: '[63] Olof Mogren “C-RNN-GAN: Continuous Recurrent Neural Networks with Adversarial
    Training”, 2016 DOI: [10.48550/arXiv.1611.09904](https://dx.doi.org/10.48550/arXiv.1611.09904)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Olof Mogren “C-RNN-GAN：带有对抗训练的连续递归神经网络”，2016 DOI: [10.48550/arXiv.1611.09904](https://dx.doi.org/10.48550/arXiv.1611.09904)'
- en: '[64] Kevin Jones “Compositional Applications of Stochastic Processes” In *Computer
    Music Journal* 5.2 The MIT Press, 1981, pp. 45–61 DOI: [10.2307/3679879](https://dx.doi.org/10.2307/3679879)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Kevin Jones “随机过程的作曲应用” 载于 *计算机音乐期刊* 5.2 麻省理工学院出版社, 1981, 第45–61页 DOI:
    [10.2307/3679879](https://dx.doi.org/10.2307/3679879)'
- en: '[65] Kohonen “A Self-Learning Musical Grammar, or ’Associative Memory of the
    Second Kind”’ In *International 1989 Joint Conference on Neural Networks*, 1989,
    pp. 1–5 vol.1 DOI: [10.1109/IJCNN.1989.118552](https://dx.doi.org/10.1109/IJCNN.1989.118552)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Kohonen “一种自学习的音乐语法，或第二类‘联想记忆’” 载于 *1989年国际联合神经网络会议*，1989, 第1–5页 第1卷 DOI:
    [10.1109/IJCNN.1989.118552](https://dx.doi.org/10.1109/IJCNN.1989.118552)'
- en: '[66] Teuvo Kohonen, Pauli Laine, Kalev Tiits and Kari Torkkola “A Nonheuristic
    Automatic Composing Method” In *Music and Connectionism* The MIT Press, 1991,
    pp. 229–242 DOI: [10.7551/mitpress/4804.003.0020](https://dx.doi.org/10.7551/mitpress/4804.003.0020)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Teuvo Kohonen, Pauli Laine, Kalev Tiits 和 Kari Torkkola “一种非启发式自动作曲方法”
    载于 *音乐与连接主义* 麻省理工学院出版社, 1991, 第229–242页 DOI: [10.7551/mitpress/4804.003.0020](https://dx.doi.org/10.7551/mitpress/4804.003.0020)'
- en: '[67] Eduardo Reck Miranda “Granular Synthesis of Sounds by Means of a Cellular
    Automaton” In *Leonardo* 28.4 The MIT Press, 1995, pp. 297–300 DOI: [10.2307/1576193](https://dx.doi.org/10.2307/1576193)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Eduardo Reck Miranda “通过细胞自动机进行声音的粒状合成” 载于 *Leonardo* 28.4 麻省理工学院出版社,
    1995, 第297–300页 DOI: [10.2307/1576193](https://dx.doi.org/10.2307/1576193)'
- en: '[68] Peter Worth and Susan Stepney “Growing Music: Musical Interpretations
    of L-Systems” In *Applications of Evolutionary Computing* Berlin, Heidelberg:
    Springer, 2005, pp. 545–550 DOI: [10.1007/978-3-540-32003-6_56](https://dx.doi.org/10.1007/978-3-540-32003-6_56)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Peter Worth 和 Susan Stepney “成长的音乐：L-系统的音乐诠释” 载于 *进化计算的应用* 柏林, 海德堡：施普林格,
    2005, 第545–550页 DOI: [10.1007/978-3-540-32003-6_56](https://dx.doi.org/10.1007/978-3-540-32003-6_56)'
- en: '[69] Michael Chan, John Potter and Emery Schubert “Improving Algorithmic Music
    Composition with Machine Learning” In *9th International Conference on Music Perception
    and Cognition* Citeseer, 2006, pp. 1848–1854'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Michael Chan, John Potter 和 Emery Schubert “利用机器学习改进算法音乐创作” 载于 *第九届国际音乐感知与认知会议*
    Citeseer, 2006, 第1848–1854页'
- en: '[70] Nicolas Boulanger-Lewandowski, Yoshua Bengio and Pascal Vincent “Modeling
    Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic
    Music Generation and Transcription”, 2012 DOI: [10.48550/arXiv.1206.6392](https://dx.doi.org/10.48550/arXiv.1206.6392)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Nicolas Boulanger-Lewandowski, Yoshua Bengio 和 Pascal Vincent “高维序列中的时间依赖建模:
    应用于多音音乐生成和转录”，2012 DOI: [10.48550/arXiv.1206.6392](https://dx.doi.org/10.48550/arXiv.1206.6392)'
- en: '[71] H.. Black and J.. Edson “Pulse Code Modulation” In *Transactions of the
    American Institute of Electrical Engineers* 66.1, 1947, pp. 895–899 DOI: [10.1109/T-AIEE.1947.5059525](https://dx.doi.org/10.1109/T-AIEE.1947.5059525)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] H.. Black 和 J.. Edson “脉冲编码调制” 见于 *美国电气工程师学会会刊* 66.1, 1947, 页码 895–899
    DOI: [10.1109/T-AIEE.1947.5059525](https://dx.doi.org/10.1109/T-AIEE.1947.5059525)'
- en: '[72] Prateek Verma and Chris Chafe “A Generative Model for Raw Audio Using
    Transformer Architectures” In *2021 24th International Conference on Digital Audio
    Effects (DAFx)*, 2021, pp. 230–237 DOI: [10.23919/DAFx51585.2021.9768298](https://dx.doi.org/10.23919/DAFx51585.2021.9768298)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Prateek Verma 和 Chris Chafe “一种基于 Transformer 架构的原始音频生成模型” 见于 *2021年第24届数字音频效果国际会议
    (DAFx)*, 2021, 页码 230–237 DOI: [10.23919/DAFx51585.2021.9768298](https://dx.doi.org/10.23919/DAFx51585.2021.9768298)'
- en: '[73] Chengyi Wang et al. “Neural Codec Language Models Are Zero-Shot Text to
    Speech Synthesizers”, 2023 DOI: [10.48550/arXiv.2301.02111](https://dx.doi.org/10.48550/arXiv.2301.02111)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Chengyi Wang 等人. “神经编解码语言模型是零样本文本到语音合成器”，2023 DOI: [10.48550/arXiv.2301.02111](https://dx.doi.org/10.48550/arXiv.2301.02111)'
- en: '[74] Aaron Oord et al. “WaveNet: A Generative Model for Raw Audio”, 2016 DOI:
    [10.48550/arXiv.1609.03499](https://dx.doi.org/10.48550/arXiv.1609.03499)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Aaron Oord 等人. “WaveNet: 一种用于原始音频的生成模型”，2016 DOI: [10.48550/arXiv.1609.03499](https://dx.doi.org/10.48550/arXiv.1609.03499)'
- en: '[75] Sander Dieleman, Aaron Oord and Karen Simonyan “The Challenge of Realistic
    Music Generation: Modelling Raw Audio at Scale” In *Advances in Neural Information
    Processing Systems* 31 Curran Associates, Inc., 2018 URL: [https://proceedings.neurips.cc/paper/2018/hash/3e441eec3456b703a4fe741005f3981f-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/3e441eec3456b703a4fe741005f3981f-Abstract.html)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Sander Dieleman, Aaron Oord 和 Karen Simonyan “逼真的音乐生成挑战: 大规模建模原始音频” 见于
    *神经信息处理系统进展* 31 Curran Associates, Inc., 2018 URL: [https://proceedings.neurips.cc/paper/2018/hash/3e441eec3456b703a4fe741005f3981f-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/3e441eec3456b703a4fe741005f3981f-Abstract.html)'
- en: '[76] Soroush Mehri et al. “SampleRNN: An Unconditional End-to-End Neural Audio
    Generation Model”, 2017 DOI: [10.48550/arXiv.1612.07837](https://dx.doi.org/10.48550/arXiv.1612.07837)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Soroush Mehri 等人. “SampleRNN: 一种无条件的端到端神经音频生成模型”，2017 DOI: [10.48550/arXiv.1612.07837](https://dx.doi.org/10.48550/arXiv.1612.07837)'
- en: '[77] Wei Ping et al. “Deep Voice 3: Scaling Text-to-Speech with Convolutional
    Sequence Learning”, 2018 DOI: [10.48550/arXiv.1710.07654](https://dx.doi.org/10.48550/arXiv.1710.07654)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Wei Ping 等人. “深度语音 3: 通过卷积序列学习扩展文本到语音”，2018 DOI: [10.48550/arXiv.1710.07654](https://dx.doi.org/10.48550/arXiv.1710.07654)'
- en: '[78] Paolo Prandoni and Martin Vetterli “Signal Processing for Communications”
    EPFL Press, 2008 GOOGLEBOOKS: tKUYSrBM4gYC'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Paolo Prandoni 和 Martin Vetterli “通信中的信号处理” EPFL Press, 2008 GOOGLEBOOKS:
    tKUYSrBM4gYC'
- en: '[79] Javier Nistal, Stefan Lattner and Gaël Richard “Comparing Representations
    for Audio Synthesis Using Generative Adversarial Networks” In *2020 28th European
    Signal Processing Conference (EUSIPCO)*, 2021, pp. 161–165 DOI: [10.23919/Eusipco47968.2020.9287799](https://dx.doi.org/10.23919/Eusipco47968.2020.9287799)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Javier Nistal, Stefan Lattner 和 Gaël Richard “使用生成对抗网络比较音频合成的表示” 见于 *2020年第28届欧洲信号处理会议
    (EUSIPCO)*, 2021, 页码 161–165 DOI: [10.23919/Eusipco47968.2020.9287799](https://dx.doi.org/10.23919/Eusipco47968.2020.9287799)'
- en: '[80] D. Griffin and Jae Lim “Signal Estimation from Modified Short-Time Fourier
    Transform” In *IEEE Transactions on Acoustics, Speech, and Signal Processing*
    32.2, 1984, pp. 236–243 DOI: [10.1109/TASSP.1984.1164317](https://dx.doi.org/10.1109/TASSP.1984.1164317)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] D. Griffin 和 Jae Lim “从修改的短时傅里叶变换中估计信号” 见于 *IEEE 声学、语音与信号处理学报* 32.2, 1984,
    页码 236–243 DOI: [10.1109/TASSP.1984.1164317](https://dx.doi.org/10.1109/TASSP.1984.1164317)'
- en: '[81] Rémi Decorsière, Peter L. Søndergaard, Ewen N. MacDonald and Torsten Dau
    “Inversion of Auditory Spectrograms, Traditional Spectrograms, and Other Envelope
    Representations” In *IEEE/ACM Transactions on Audio, Speech, and Language Processing*
    23.1, 2015, pp. 46–56 DOI: [10.1109/TASLP.2014.2367821](https://dx.doi.org/10.1109/TASLP.2014.2367821)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Rémi Decorsière, Peter L. Søndergaard, Ewen N. MacDonald 和 Torsten Dau
    “听觉声谱图、传统声谱图及其他包络表示的逆变换” 见于 *IEEE/ACM 音频、语音与语言处理学报* 23.1, 2015, 页码 46–56 DOI:
    [10.1109/TASLP.2014.2367821](https://dx.doi.org/10.1109/TASLP.2014.2367821)'
- en: '[82] Gerald T. Beauregard, Mithila Harish and Lonce Wyse “Single Pass Spectrogram
    Inversion” In *2015 IEEE International Conference on Digital Signal Processing
    (DSP)*, 2015, pp. 427–431 DOI: [10.1109/ICDSP.2015.7251907](https://dx.doi.org/10.1109/ICDSP.2015.7251907)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Gerald T. Beauregard, Mithila Harish 和 Lonce Wyse “单次通过谱图反演” 见 *2015 IEEE国际数字信号处理会议
    (DSP)*, 2015, 第427–431页 DOI: [10.1109/ICDSP.2015.7251907](https://dx.doi.org/10.1109/ICDSP.2015.7251907)'
- en: '[83] Zdeněk Průša, Peter Balazs and Peter Lempel Søndergaard “A Noniterative
    Method for Reconstruction of Phase From STFT Magnitude” In *IEEE/ACM Transactions
    on Audio, Speech, and Language Processing* 25.5, 2017, pp. 1154–1164 DOI: [10.1109/TASLP.2017.2678166](https://dx.doi.org/10.1109/TASLP.2017.2678166)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Zdeněk Průša, Peter Balazs 和 Peter Lempel Søndergaard “一种非迭代方法用于从STFT幅度重建相位”
    见 *IEEE/ACM音频、语音和语言处理汇刊* 25.5, 2017, 第1154–1164页 DOI: [10.1109/TASLP.2017.2678166](https://dx.doi.org/10.1109/TASLP.2017.2678166)'
- en: '[84] Yuxuan Wang et al. “Tacotron: Towards End-to-End Speech Synthesis”, 2017
    DOI: [10.48550/arXiv.1703.10135](https://dx.doi.org/10.48550/arXiv.1703.10135)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] 王宇轩等。“Tacotron：迈向端到端语音合成”，2017 DOI: [10.48550/arXiv.1703.10135](https://dx.doi.org/10.48550/arXiv.1703.10135)'
- en: '[85] Jonathan Shen et al. “Natural TTS Synthesis by Conditioning Wavenet on
    MEL Spectrogram Predictions” In *2018 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2018, pp. 4779–4783 DOI: [10.1109/ICASSP.2018.8461368](https://dx.doi.org/10.1109/ICASSP.2018.8461368)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Jonathan Shen 等。“通过将Wavenet条件化于MEL谱图预测的自然TTS合成” 见 *2018 IEEE国际声学、语音和信号处理会议
    (ICASSP)*, 2018, 第4779–4783页 DOI: [10.1109/ICASSP.2018.8461368](https://dx.doi.org/10.1109/ICASSP.2018.8461368)'
- en: '[86] Yi Ren et al. “FastSpeech: Fast, Robust and Controllable Text to Speech”
    In *Advances in Neural Information Processing Systems* 32 Curran Associates, Inc.,
    2019 URL: [https://proceedings.neurips.cc/paper_files/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] 任怡等。“FastSpeech：快速、鲁棒且可控的文本转语音” 见 *Neural Information Processing Systems进展*
    32 Curran Associates, Inc., 2019 网址: [https://proceedings.neurips.cc/paper_files/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html)'
- en: '[87] Yi Ren et al. “FastSpeech 2: Fast and High-Quality End-to-End Text to
    Speech”, 2022 DOI: [10.48550/arXiv.2006.04558](https://dx.doi.org/10.48550/arXiv.2006.04558)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] 任怡等。“FastSpeech 2：快速且高质量的端到端文本转语音”，2022 DOI: [10.48550/arXiv.2006.04558](https://dx.doi.org/10.48550/arXiv.2006.04558)'
- en: '[88] Keunwoo Choi, György Fazekas, Mark Sandler and Kyunghyun Cho “A Comparison
    of Audio Signal Preprocessing Methods for Deep Neural Networks on Music Tagging”
    In *2018 26th European Signal Processing Conference (EUSIPCO)*, 2018, pp. 1870–1874
    DOI: [10.23919/EUSIPCO.2018.8553106](https://dx.doi.org/10.23919/EUSIPCO.2018.8553106)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Keunwoo Choi, György Fazekas, Mark Sandler 和 Kyunghyun Cho “用于音乐标签的深度神经网络的音频信号预处理方法比较”
    见 *2018年第26届欧洲信号处理会议 (EUSIPCO)*, 2018, 第1870–1874页 DOI: [10.23919/EUSIPCO.2018.8553106](https://dx.doi.org/10.23919/EUSIPCO.2018.8553106)'
- en: '[89] Alexandre Défossez, Jade Copet, Gabriel Synnaeve and Yossi Adi “High Fidelity
    Neural Audio Compression”, 2022 DOI: [10.48550/arXiv.2210.13438](https://dx.doi.org/10.48550/arXiv.2210.13438)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Alexandre Défossez, Jade Copet, Gabriel Synnaeve 和 Yossi Adi “高保真神经音频压缩”，2022
    DOI: [10.48550/arXiv.2210.13438](https://dx.doi.org/10.48550/arXiv.2210.13438)'
- en: '[90] Neil Zeghidour et al. “SoundStream: An End-to-End Neural Audio Codec”
    In *IEEE/ACM Transactions on Audio, Speech, and Language Processing* 30, 2022,
    pp. 495–507 DOI: [10.1109/TASLP.2021.3129994](https://dx.doi.org/10.1109/TASLP.2021.3129994)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Neil Zeghidour 等。“SoundStream：一种端到端神经音频编解码器” 见 *IEEE/ACM音频、语音和语言处理汇刊*
    30, 2022, 第495–507页 DOI: [10.1109/TASLP.2021.3129994](https://dx.doi.org/10.1109/TASLP.2021.3129994)'
- en: '[91] Yang Ai et al. “APCodec: A Neural Audio Codec with Parallel Amplitude
    and Phase Spectrum Encoding and Decoding”, 2024 arXiv: [http://arxiv.org/abs/2402.10533](http://arxiv.org/abs/2402.10533)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] 杨艾等。“APCodec：一种具有并行幅度和相位谱编码与解码的神经音频编解码器”，2024 arXiv: [http://arxiv.org/abs/2402.10533](http://arxiv.org/abs/2402.10533)'
- en: '[92] Yi-Chiao Wu, Israel D. Gebru, Dejan Marković and Alexander Richard “AudioDec:
    An Open-source Streaming High-fidelity Neural Audio Codec” In *ICASSP 2023 - 2023
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2023, pp. 1–5 DOI: [10.1109/ICASSP49357.2023.10096509](https://dx.doi.org/10.1109/ICASSP49357.2023.10096509)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Yi-Chiao Wu, Israel D. Gebru, Dejan Marković 和 Alexander Richard “AudioDec：一个开源流媒体高保真神经音频编解码器”
    见 *ICASSP 2023 - 2023 IEEE国际声学、语音和信号处理会议 (ICASSP)*, 2023, 第1–5页 DOI: [10.1109/ICASSP49357.2023.10096509](https://dx.doi.org/10.1109/ICASSP49357.2023.10096509)'
- en: '[93] Kai Shen et al. “NaturalSpeech 2: Latent Diffusion Models Are Natural
    and Zero-Shot Speech and Singing Synthesizers”, 2023 DOI: [10.48550/arXiv.2304.09116](https://dx.doi.org/10.48550/arXiv.2304.09116)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Kai Shen 等人. “NaturalSpeech 2: 潜在扩散模型作为自然和零样本的语音及歌唱合成器”，2023年 DOI: [10.48550/arXiv.2304.09116](https://dx.doi.org/10.48550/arXiv.2304.09116)'
- en: '[94] Dongchao Yang et al. “HiFi-Codec: Group-residual Vector Quantization for
    High Fidelity Audio Codec”, 2023 DOI: [10.48550/arXiv.2305.02765](https://dx.doi.org/10.48550/arXiv.2305.02765)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Dongchao Yang 等人. “HiFi-Codec: 用于高保真音频编解码的组残差向量量化”，2023年 DOI: [10.48550/arXiv.2305.02765](https://dx.doi.org/10.48550/arXiv.2305.02765)'
- en: '[95] Tianrui Wang et al. “VioLA: Unified Codec Language Models for Speech Recognition,
    Synthesis, and Translation”, 2023 DOI: [10.48550/arXiv.2305.16107](https://dx.doi.org/10.48550/arXiv.2305.16107)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Tianrui Wang 等人. “VioLA: 统一编解码语言模型用于语音识别、合成和翻译”，2023年 DOI: [10.48550/arXiv.2305.16107](https://dx.doi.org/10.48550/arXiv.2305.16107)'
- en: '[96] Zalán Borsos et al. “AudioLM: A Language Modeling Approach to Audio Generation”
    In *IEEE/ACM Transactions on Audio, Speech, and Language Processing* 31, 2023,
    pp. 2523–2533 DOI: [10.1109/TASLP.2023.3288409](https://dx.doi.org/10.1109/TASLP.2023.3288409)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Zalán Borsos 等人. “AudioLM: 一种用于音频生成的语言建模方法” 发表在 *IEEE/ACM Transactions
    on Audio, Speech, and Language Processing* 31, 2023年, 页码 2523–2533 DOI: [10.1109/TASLP.2023.3288409](https://dx.doi.org/10.1109/TASLP.2023.3288409)'
- en: '[97] Andrea Agostinelli et al. “MusicLM: Generating Music From Text”, 2023
    DOI: [10.48550/arXiv.2301.11325](https://dx.doi.org/10.48550/arXiv.2301.11325)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Andrea Agostinelli 等人. “MusicLM: 从文本生成音乐”，2023年 DOI: [10.48550/arXiv.2301.11325](https://dx.doi.org/10.48550/arXiv.2301.11325)'
- en: '[98] Chris Donahue et al. “SingSong: Generating Musical Accompaniments from
    Singing”, 2023 DOI: [10.48550/arXiv.2301.12662](https://dx.doi.org/10.48550/arXiv.2301.12662)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Chris Donahue 等人. “SingSong: 从唱歌生成音乐伴奏”，2023年 DOI: [10.48550/arXiv.2301.12662](https://dx.doi.org/10.48550/arXiv.2301.12662)'
- en: '[99] Zalán Borsos et al. “SoundStorm: Efficient Parallel Audio Generation”,
    2023 DOI: [10.48550/arXiv.2305.09636](https://dx.doi.org/10.48550/arXiv.2305.09636)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Zalán Borsos 等人. “SoundStorm: 高效的并行音频生成”，2023年 DOI: [10.48550/arXiv.2305.09636](https://dx.doi.org/10.48550/arXiv.2305.09636)'
- en: '[100] Ziqiang Zhang et al. “Speak Foreign Languages with Your Own Voice: Cross-Lingual
    Neural Codec Language Modeling”, 2023 DOI: [10.48550/arXiv.2303.03926](https://dx.doi.org/10.48550/arXiv.2303.03926)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Ziqiang Zhang 等人. “用你自己的声音说外语：跨语言神经编解码器语言建模”，2023年 DOI: [10.48550/arXiv.2303.03926](https://dx.doi.org/10.48550/arXiv.2303.03926)'
- en: '[101] Xiaofei Wang et al. “SpeechX: Neural Codec Language Model as a Versatile
    Speech Transformer”, 2023 DOI: [10.48550/arXiv.2308.06873](https://dx.doi.org/10.48550/arXiv.2308.06873)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Xiaofei Wang 等人. “SpeechX: 作为多功能语音变换器的神经编解码器语言模型”，2023年 DOI: [10.48550/arXiv.2308.06873](https://dx.doi.org/10.48550/arXiv.2308.06873)'
- en: '[102] Ian Goodfellow, Yoshua Bengio and Aaron Courville “Deep Learning” Cambridge,
    Mass: The MIT Press, 2016'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Ian Goodfellow, Yoshua Bengio 和 Aaron Courville “深度学习” 剑桥, 马萨诸塞州: MIT出版社,
    2016年'
- en: '[103] Diederik P. Kingma and Max Welling “An Introduction to Variational Autoencoders”
    In *Foundations and Trends® in Machine Learning* 12.4 Now Publishers, Inc., 2019,
    pp. 307–392 DOI: [10.1561/2200000056](https://dx.doi.org/10.1561/2200000056)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Diederik P. Kingma 和 Max Welling “变分自编码器简介” 发表在 *Foundations and Trends®
    in Machine Learning* 12.4 Now Publishers, Inc., 2019年, 页码 307–392 DOI: [10.1561/2200000056](https://dx.doi.org/10.1561/2200000056)'
- en: '[104] Hu-Cheng Lee, Chih-Yu Lin, Pin-Chun Hsu and Winston H. Hsu “Audio Feature
    Generation for Missing Modality Problem in Video Action Recognition” In *ICASSP
    2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, 2019, pp. 3956–3960 DOI: [10.1109/ICASSP.2019.8682513](https://dx.doi.org/10.1109/ICASSP.2019.8682513)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Hu-Cheng Lee, Chih-Yu Lin, Pin-Chun Hsu 和 Winston H. Hsu “视频动作识别中的缺失模态问题的音频特征生成”
    发表在 *ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and
    Signal Processing (ICASSP)*, 2019年, 页码 3956–3960 DOI: [10.1109/ICASSP.2019.8682513](https://dx.doi.org/10.1109/ICASSP.2019.8682513)'
- en: '[105] Jaehyeon Kim, Jungil Kong and Juhee Son “Conditional Variational Autoencoder
    with Adversarial Learning for End-to-End Text-to-Speech” In *Proceedings of the
    38th International Conference on Machine Learning* PMLR, 2021, pp. 5530–5540 URL:
    [https://proceedings.mlr.press/v139/kim21f.html](https://proceedings.mlr.press/v139/kim21f.html)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Jaehyeon Kim, Jungil Kong 和 Juhee Son “用于端到端文本转语音的条件变分自编码器与对抗学习” 发表在
    *Proceedings of the 38th International Conference on Machine Learning* PMLR, 2021年,
    页码 5530–5540 URL: [https://proceedings.mlr.press/v139/kim21f.html](https://proceedings.mlr.press/v139/kim21f.html)'
- en: '[106] Adam Roberts et al. “A Hierarchical Latent Vector Model for Learning
    Long-Term Structure in Music” In *Proceedings of the 35th International Conference
    on Machine Learning* PMLR, 2018, pp. 4364–4373 URL: [https://proceedings.mlr.press/v80/roberts18a.html](https://proceedings.mlr.press/v80/roberts18a.html)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] 亚当·罗伯茨等“用于学习音乐长期结构的层次化潜在向量模型”，载于*第35届国际机器学习会议* PMLR，2018年，第4364–4373页，网址：[https://proceedings.mlr.press/v80/roberts18a.html](https://proceedings.mlr.press/v80/roberts18a.html)'
- en: '[107] Yixiao Zhang, Ziyu Wang, Dingsu Wang and Gus Xia “BUTTER: A Representation
    Learning Framework for Bi-directional Music-Sentence Retrieval and Generation”
    In *Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA)* Online:
    Association for Computational Linguistics, 2020, pp. 54–58 URL: [https://aclanthology.org/2020.nlp4musa-1.11](https://aclanthology.org/2020.nlp4musa-1.11)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] 张一晓、王子瑜、王鼎肃和夏谷“BUTTER：用于双向音乐-句子检索与生成的表示学习框架”，载于*第1届音乐与音频自然语言处理研讨会（NLP4MusA）*，在线：计算语言学协会，2020年，第54–58页，网址：[https://aclanthology.org/2020.nlp4musa-1.11](https://aclanthology.org/2020.nlp4musa-1.11)'
- en: '[108] Hao Hao Tan and Dorien Herremans “Music FaderNets: Controllable Music
    Generation Based On High-Level Features via Low-Level Feature Modelling”, 2020
    DOI: [10.48550/arXiv.2007.15474](https://dx.doi.org/10.48550/arXiv.2007.15474)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] 谭浩浩和多里恩·赫雷曼斯“Music FaderNets：基于高层特征的可控音乐生成通过低层特征建模”，2020年，DOI：[10.48550/arXiv.2007.15474](https://dx.doi.org/10.48550/arXiv.2007.15474)'
- en: '[109] Antoine Caillon and Philippe Esling “RAVE: A Variational Autoencoder
    for Fast and High-Quality Neural Audio Synthesis”, 2021 DOI: [10.48550/arXiv.2111.05011](https://dx.doi.org/10.48550/arXiv.2111.05011)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] 安托万·凯永和菲利普·埃斯林“RAVE：用于快速高质量神经音频合成的变分自编码器”，2021年，DOI：[10.48550/arXiv.2111.05011](https://dx.doi.org/10.48550/arXiv.2111.05011)'
- en: '[110] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish and Soujanya Poria
    “Text-to-Audio Generation Using Instruction-Tuned LLM and Latent Diffusion Model”,
    2023 DOI: [10.48550/arXiv.2304.13731](https://dx.doi.org/10.48550/arXiv.2304.13731)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] 迪潘维·戈沙尔、纳沃尼尔·马朱姆德、安布吉·梅里什和苏贾尼亚·波利亚“使用指令调优的LLM和潜在扩散模型生成文本到音频”，2023年，DOI：[10.48550/arXiv.2304.13731](https://dx.doi.org/10.48550/arXiv.2304.13731)'
- en: '[111] Jiawei Huang et al. “Make-An-Audio 2: Temporal-Enhanced Text-to-Audio
    Generation”, 2023 DOI: [10.48550/arXiv.2305.18474](https://dx.doi.org/10.48550/arXiv.2305.18474)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] 黄家伟等“Make-An-Audio 2：时间增强文本到音频生成”，2023年，DOI：[10.48550/arXiv.2305.18474](https://dx.doi.org/10.48550/arXiv.2305.18474)'
- en: '[112] Kundan Kumar et al. “MelGAN: Generative Adversarial Networks for Conditional
    Waveform Synthesis” In *Advances in Neural Information Processing Systems* 32
    Curran Associates, Inc., 2019 URL: [https://proceedings.neurips.cc/paper/2019/hash/6804c9bca0a615bdb9374d00a9fcba59-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/6804c9bca0a615bdb9374d00a9fcba59-Abstract.html)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] 昆丹·库马尔等“MelGAN：用于条件波形合成的生成对抗网络”，载于*神经信息处理系统进展* 32 Curran Associates,
    Inc., 2019年，网址：[https://proceedings.neurips.cc/paper/2019/hash/6804c9bca0a615bdb9374d00a9fcba59-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/6804c9bca0a615bdb9374d00a9fcba59-Abstract.html)'
- en: '[113] Andros Tjandra et al. “VQVAE Unsupervised Unit Discovery and Multi-scale
    Code2Spec Inverter for Zerospeech Challenge 2019”, 2019 DOI: [10.48550/arXiv.1905.11449](https://dx.doi.org/10.48550/arXiv.1905.11449)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] 安德罗斯·姜等“VQVAE 无监督单元发现及多尺度 Code2Spec 反演器，用于 Zerospeech 挑战 2019”，2019年，DOI：[10.48550/arXiv.1905.11449](https://dx.doi.org/10.48550/arXiv.1905.11449)'
- en: '[114] Prafulla Dhariwal et al. “Jukebox: A Generative Model for Music”, 2020
    DOI: [10.48550/arXiv.2005.00341](https://dx.doi.org/10.48550/arXiv.2005.00341)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] 普拉弗拉·达里瓦尔等“Jukebox：音乐生成模型”，2020年，DOI：[10.48550/arXiv.2005.00341](https://dx.doi.org/10.48550/arXiv.2005.00341)'
- en: '[115] Tomoki Hayashi and Shinji Watanabe “DiscreTalk: Text-to-Speech as a Machine
    Translation Problem”, 2020 DOI: [10.48550/arXiv.2005.05525](https://dx.doi.org/10.48550/arXiv.2005.05525)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] 林基·林和渡边信治“DiscreTalk：将文本到语音作为机器翻译问题”，2020年，DOI：[10.48550/arXiv.2005.05525](https://dx.doi.org/10.48550/arXiv.2005.05525)'
- en: '[116] Dimitri Rütte, Luca Biggio, Yannic Kilcher and Thomas Hofmann “FIGARO:
    Controllable Music Generation Using Learned and Expert Features”, 2022 URL: [https://openreview.net/forum?id=NyR8OZFHw6i](https://openreview.net/forum?id=NyR8OZFHw6i)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] 德米特里·鲁特、卢卡·比吉奥、扬尼克·基尔彻和托马斯·霍夫曼“FIGARO：使用学习和专家特征的可控音乐生成”，2022年，网址：[https://openreview.net/forum?id=NyR8OZFHw6i](https://openreview.net/forum?id=NyR8OZFHw6i)'
- en: '[117] Dongchao Yang et al. “Diffsound: Discrete Diffusion Model for Text-to-Sound
    Generation” In *IEEE/ACM Transactions on Audio, Speech, and Language Processing*
    31, 2023, pp. 1720–1733 DOI: [10.1109/TASLP.2023.3268730](https://dx.doi.org/10.1109/TASLP.2023.3268730)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Dongchao Yang 等人。“Diffsound: 离散扩散模型用于文本到声音生成” 发表在 *IEEE/ACM 音频、语音与语言处理汇刊*
    31，2023，第1720–1733页 DOI: [10.1109/TASLP.2023.3268730](https://dx.doi.org/10.1109/TASLP.2023.3268730)'
- en: '[118] Roy Sheffer and Yossi Adi “I Hear Your True Colors: Image Guided Audio
    Generation” In *ICASSP 2023 - 2023 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2023, pp. 1–5 DOI: [10.1109/ICASSP49357.2023.10096023](https://dx.doi.org/10.1109/ICASSP49357.2023.10096023)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Roy Sheffer 和 Yossi Adi “I Hear Your True Colors: 图像引导的音频生成” 发表在 *ICASSP
    2023 - 2023 IEEE 国际声学、语音与信号处理会议（ICASSP）*，2023，第1–5页 DOI: [10.1109/ICASSP49357.2023.10096023](https://dx.doi.org/10.1109/ICASSP49357.2023.10096023)'
- en: '[119] Ye Zhu et al. “Quantized GAN for Complex Music Generation from Dance
    Videos” In *Computer Vision – ECCV 2022* Cham: Springer Nature Switzerland, 2022,
    pp. 182–199 DOI: [10.1007/978-3-031-19836-6_11](https://dx.doi.org/10.1007/978-3-031-19836-6_11)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Ye Zhu 等人。“量化 GAN 用于从舞蹈视频生成复杂音乐” 发表在 *计算机视觉 – ECCV 2022* Cham: Springer
    Nature Switzerland，2022，第182–199页 DOI: [10.1007/978-3-031-19836-6_11](https://dx.doi.org/10.1007/978-3-031-19836-6_11)'
- en: '[120] Junyi Ao et al. “SpeechT5: Unified-Modal Encoder-Decoder Pre-Training
    for Spoken Language Processing”, 2022 DOI: [10.48550/arXiv.2110.07205](https://dx.doi.org/10.48550/arXiv.2110.07205)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Junyi Ao 等人。“SpeechT5: 统一模态编码器-解码器预训练用于口语语言处理”，2022 DOI: [10.48550/arXiv.2110.07205](https://dx.doi.org/10.48550/arXiv.2110.07205)'
- en: '[121] Chenpeng Du, Yiwei Guo, Xie Chen and Kai Yu “VQTTS: High-Fidelity Text-to-Speech
    Synthesis with Self-Supervised VQ Acoustic Feature”, 2022 DOI: [10.48550/arXiv.2204.00768](https://dx.doi.org/10.48550/arXiv.2204.00768)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Chenpeng Du、Yiwei Guo、Xie Chen 和 Kai Yu “VQTTS: 高保真文本到语音合成与自监督 VQ 声学特征”，2022
    DOI: [10.48550/arXiv.2204.00768](https://dx.doi.org/10.48550/arXiv.2204.00768)'
- en: '[122] Yanqing Liu et al. “DelightfulTTS 2: End-to-End Speech Synthesis with
    Adversarial Vector-Quantized Auto-Encoders”, 2022 DOI: [10.48550/arXiv.2207.04646](https://dx.doi.org/10.48550/arXiv.2207.04646)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Yanqing Liu 等人。“DelightfulTTS 2: 端到端语音合成与对抗性向量量化自编码器”，2022 DOI: [10.48550/arXiv.2207.04646](https://dx.doi.org/10.48550/arXiv.2207.04646)'
- en: '[123] Norberto Torres-Reyes and Shahram Latifi “Audio Enhancement and Synthesis
    Using Generative Adversarial Networks: A Survey” In *International Journal of
    Computer Applications* 182.35, 2019, pp. 27–31 DOI: [10.5120/ijca2019918334](https://dx.doi.org/10.5120/ijca2019918334)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Norberto Torres-Reyes 和 Shahram Latifi “使用生成对抗网络的音频增强与合成：综述” 发表在 *国际计算机应用期刊*
    182.35，2019，第27–31页 DOI: [10.5120/ijca2019918334](https://dx.doi.org/10.5120/ijca2019918334)'
- en: '[124] Li-Chia Yang, Szu-Yu Chou and Yi-Hsuan Yang “MidiNet: A Convolutional
    Generative Adversarial Network for Symbolic-domain Music Generation”, 2017 DOI:
    [10.48550/arXiv.1703.10847](https://dx.doi.org/10.48550/arXiv.1703.10847)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Li-Chia Yang、Szu-Yu Chou 和 Yi-Hsuan Yang “MidiNet: 一种用于符号领域音乐生成的卷积生成对抗网络”，2017
    DOI: [10.48550/arXiv.1703.10847](https://dx.doi.org/10.48550/arXiv.1703.10847)'
- en: '[125] Daniel Michelsanti and Zheng-Hua Tan “Conditional Generative Adversarial
    Networks for Speech Enhancement and Noise-Robust Speaker Verification” In *Interspeech
    2017*, 2017, pp. 2008–2012 DOI: [10.21437/Interspeech.2017-1620](https://dx.doi.org/10.21437/Interspeech.2017-1620)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Daniel Michelsanti 和 Zheng-Hua Tan “用于语音增强和抗噪声说话人验证的条件生成对抗网络” 发表在 *Interspeech
    2017*，2017，第2008–2012页 DOI: [10.21437/Interspeech.2017-1620](https://dx.doi.org/10.21437/Interspeech.2017-1620)'
- en: '[126] Lele Chen, Sudhanshu Srivastava, Zhiyao Duan and Chenliang Xu “Deep Cross-Modal
    Audio-Visual Generation” In *Proceedings of the on Thematic Workshops of ACM Multimedia
    2017*, Thematic Workshops ’17 New York, NY, USA: Association for Computing Machinery,
    2017, pp. 349–357 DOI: [10.1145/3126686.3126723](https://dx.doi.org/10.1145/3126686.3126723)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Lele Chen、Sudhanshu Srivastava、Zhiyao Duan 和 Chenliang Xu “深度跨模态音频-视觉生成”
    发表在 *ACM Multimedia 2017 年专题研讨会论文集*，专题研讨会 ’17 纽约，NY，美国：计算机学会，2017，第349–357页 DOI:
    [10.1145/3126686.3126723](https://dx.doi.org/10.1145/3126686.3126723)'
- en: '[127] Paarth Neekhara et al. “Expediting TTS Synthesis with Adversarial Vocoding”,
    2019 DOI: [10.48550/arXiv.1904.07944](https://dx.doi.org/10.48550/arXiv.1904.07944)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Paarth Neekhara 等人。“通过对抗性声码加快 TTS 合成”，2019 DOI: [10.48550/arXiv.1904.07944](https://dx.doi.org/10.48550/arXiv.1904.07944)'
- en: '[128] Shiguang Liu, Sijia Li and Haonan Cheng “Towards an End-to-End Visual-to-Raw-Audio
    Generation With GAN” In *IEEE Transactions on Circuits and Systems for Video Technology*
    32.3, 2022, pp. 1299–1312 DOI: [10.1109/TCSVT.2021.3079897](https://dx.doi.org/10.1109/TCSVT.2021.3079897)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Shiguang Liu、Sijia Li 和 Haonan Cheng “面向端到端视觉到原始音频生成的GAN” 见 *IEEE视频技术电路与系统汇刊*
    32.3，2022年，第1299–1312页 DOI: [10.1109/TCSVT.2021.3079897](https://dx.doi.org/10.1109/TCSVT.2021.3079897)'
- en: '[129] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang and Yi-Hsuan Yang “MuseGAN:
    Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation
    and Accompaniment” In *Proceedings of the AAAI Conference on Artificial Intelligence*
    32.1, 2018 DOI: [10.1609/aaai.v32i1.11312](https://dx.doi.org/10.1609/aaai.v32i1.11312)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Hao-Wen Dong、Wen-Yi Hsiao、Li-Chia Yang 和 Yi-Hsuan Yang “MuseGAN: 多轨序列生成对抗网络用于符号音乐生成与伴奏”
    见 *AAAI人工智能会议论文集* 32.1，2018 DOI: [10.1609/aaai.v32i1.11312](https://dx.doi.org/10.1609/aaai.v32i1.11312)'
- en: '[130] Chris Donahue, Julian McAuley and Miller Puckette “Adversarial Audio
    Synthesis”, 2019 DOI: [10.48550/arXiv.1802.04208](https://dx.doi.org/10.48550/arXiv.1802.04208)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Chris Donahue、Julian McAuley 和 Miller Puckette “对抗音频合成”，2019 DOI: [10.48550/arXiv.1802.04208](https://dx.doi.org/10.48550/arXiv.1802.04208)'
- en: '[131] Andrés Marafioti, Nathanaël Perraudin, Nicki Holighaus and Piotr Majdak
    “Adversarial Generation of Time-Frequency Features with Application in Audio Synthesis”
    In *Proceedings of the 36th International Conference on Machine Learning* PMLR,
    2019, pp. 4352–4362 URL: [https://proceedings.mlr.press/v97/marafioti19a.html](https://proceedings.mlr.press/v97/marafioti19a.html)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Andrés Marafioti、Nathanaël Perraudin、Nicki Holighaus 和 Piotr Majdak “时频特征的对抗生成及其在音频合成中的应用”
    见 *第36届国际机器学习大会论文集* PMLR，2019年，第4352–4362页 URL: [https://proceedings.mlr.press/v97/marafioti19a.html](https://proceedings.mlr.press/v97/marafioti19a.html)'
- en: '[132] Gal Greshler, Tamar Shaham and Tomer Michaeli “Catch-A-Waveform: Learning
    to Generate Audio from a Single Short Example” In *Advances in Neural Information
    Processing Systems* 34 Curran Associates, Inc., 2021, pp. 20916–20928 URL: [https://proceedings.neurips.cc/paper/2021/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Gal Greshler、Tamar Shaham 和 Tomer Michaeli “Catch-A-Waveform: 从单个短示例学习生成音频”
    见 *神经信息处理系统进展* 34 Curran Associates, Inc.，2021年，第20916–20928页 URL: [https://proceedings.neurips.cc/paper/2021/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html)'
- en: '[133] Xudong Mao et al. “Least Squares Generative Adversarial Networks”, 2017
    DOI: [10.48550/arXiv.1611.04076](https://dx.doi.org/10.48550/arXiv.1611.04076)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Xudong Mao 等 “最小二乘生成对抗网络”，2017 DOI: [10.48550/arXiv.1611.04076](https://dx.doi.org/10.48550/arXiv.1611.04076)'
- en: '[134] Santiago Pascual, Antonio Bonafonte and Joan Serrà “SEGAN: Speech Enhancement
    Generative Adversarial Network”, 2017 DOI: [10.48550/arXiv.1703.09452](https://dx.doi.org/10.48550/arXiv.1703.09452)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Santiago Pascual、Antonio Bonafonte 和 Joan Serrà “SEGAN: Speech Enhancement
    Generative Adversarial Network”，2017 DOI: [10.48550/arXiv.1703.09452](https://dx.doi.org/10.48550/arXiv.1703.09452)'
- en: '[135] Ryuichi Yamamoto, Eunwoo Song and Jae-Min Kim “Probability Density Distillation
    with Generative Adversarial Networks for High-Quality Parallel Waveform Generation”,
    2019 DOI: [10.48550/arXiv.1904.04472](https://dx.doi.org/10.48550/arXiv.1904.04472)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Ryuichi Yamamoto、Eunwoo Song 和 Jae-Min Kim “基于生成对抗网络的高质量并行波形生成的概率密度蒸馏”，2019
    DOI: [10.48550/arXiv.1904.04472](https://dx.doi.org/10.48550/arXiv.1904.04472)'
- en: '[136] Jungil Kong, Jaehyeon Kim and Jaekyoung Bae “HiFi-GAN: Generative Adversarial
    Networks for Efficient and High Fidelity Speech Synthesis” In *Advances in Neural
    Information Processing Systems* 33 Curran Associates, Inc., 2020, pp. 17022–17033
    URL: [https://proceedings.neurips.cc/paper_files/paper/2020/hash/c5d736809766d46260d816d8dbc9eb44-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2020/hash/c5d736809766d46260d816d8dbc9eb44-Abstract.html)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Jungil Kong、Jaehyeon Kim 和 Jaekyoung Bae “HiFi-GAN: 用于高效高保真语音合成的生成对抗网络”
    见 *神经信息处理系统进展* 33 Curran Associates, Inc.，2020年，第17022–17033页 URL: [https://proceedings.neurips.cc/paper_files/paper/2020/hash/c5d736809766d46260d816d8dbc9eb44-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2020/hash/c5d736809766d46260d816d8dbc9eb44-Abstract.html)'
- en: '[137] Ryuichi Yamamoto, Eunwoo Song and Jae-Min Kim “Parallel Wavegan: A Fast
    Waveform Generation Model Based on Generative Adversarial Networks with Multi-Resolution
    Spectrogram” In *ICASSP 2020 - 2020 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2020, pp. 6199–6203 DOI: [10.1109/ICASSP40776.2020.9053795](https://dx.doi.org/10.1109/ICASSP40776.2020.9053795)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Ryuichi Yamamoto, Eunwoo Song 和 Jae-Min Kim “Parallel Wavegan: 基于多分辨率谱图的快速波形生成模型”
    发表在 *ICASSP 2020 - 2020 IEEE 国际声学、语音与信号处理会议 (ICASSP)*，2020，第 6199–6203 页 DOI:
    [10.1109/ICASSP40776.2020.9053795](https://dx.doi.org/10.1109/ICASSP40776.2020.9053795)'
- en: '[138] Ji-Hoon Kim, Sang-Hoon Lee, Ji-Hyun Lee and Seong-Whan Lee “Fre-GAN:
    Adversarial Frequency-consistent Audio Synthesis”, 2021 DOI: [10.48550/arXiv.2106.02297](https://dx.doi.org/10.48550/arXiv.2106.02297)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Ji-Hoon Kim, Sang-Hoon Lee, Ji-Hyun Lee 和 Seong-Whan Lee “Fre-GAN: 对抗频率一致音频合成”，2021
    DOI: [10.48550/arXiv.2106.02297](https://dx.doi.org/10.48550/arXiv.2106.02297)'
- en: '[139] Wangli Hao, Zhaoxiang Zhang and He Guan “CMCGAN: A Uniform Framework
    for Cross-Modal Visual-Audio Mutual Generation” In *Proceedings of the AAAI Conference
    on Artificial Intelligence* 32.1, 2018 DOI: [10.1609/aaai.v32i1.12329](https://dx.doi.org/10.1609/aaai.v32i1.12329)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Wangli Hao, Zhaoxiang Zhang 和 He Guan “CMCGAN: 跨模态视觉-音频互生成的统一框架” 发表在
    *AAAI 人工智能会议论文集* 32.1, 2018 DOI: [10.1609/aaai.v32i1.12329](https://dx.doi.org/10.1609/aaai.v32i1.12329)'
- en: '[140] Jen-Yu Liu, Yu-Hua Chen, Yin-Cheng Yeh and Yi-Hsuan Yang “Unconditional
    Audio Generation with Generative Adversarial Networks and Cycle Regularization”
    In *Interspeech 2020*, 2020, pp. 1997–2001 DOI: [10.21437/Interspeech.2020-1137](https://dx.doi.org/10.21437/Interspeech.2020-1137)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Jen-Yu Liu, Yu-Hua Chen, Yin-Cheng Yeh 和 Yi-Hsuan Yang “使用生成对抗网络和循环正则化的无条件音频生成”
    发表在 *Interspeech 2020*，2020，第 1997–2001 页 DOI: [10.21437/Interspeech.2020-1137](https://dx.doi.org/10.21437/Interspeech.2020-1137)'
- en: '[141] Kazi Nazmul Haque, Rajib Rana and Björn W. Schuller “High-Fidelity Audio
    Generation and Representation Learning With Guided Adversarial Autoencoder” In
    *IEEE Access* 8, 2020, pp. 223509–223528 DOI: [10.1109/ACCESS.2020.3040797](https://dx.doi.org/10.1109/ACCESS.2020.3040797)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Kazi Nazmul Haque, Rajib Rana 和 Björn W. Schuller “使用引导对抗自编码器的高保真音频生成和表示学习”
    发表在 *IEEE Access* 8, 2020, 第 223509–223528 页 DOI: [10.1109/ACCESS.2020.3040797](https://dx.doi.org/10.1109/ACCESS.2020.3040797)'
- en: '[142] Kazi Nazmul Haque et al. “Guided Generative Adversarial Neural Network
    for Representation Learning and Audio Generation Using Fewer Labelled Audio Data”
    In *IEEE/ACM Transactions on Audio, Speech, and Language Processing* 29, 2021,
    pp. 2575–2590 DOI: [10.1109/TASLP.2021.3098764](https://dx.doi.org/10.1109/TASLP.2021.3098764)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Kazi Nazmul Haque 等人 “针对表示学习和音频生成的引导生成对抗神经网络，使用更少的标记音频数据” 发表在 *IEEE/ACM
    音频、语音与语言处理汇刊* 29, 2021, 第 2575–2590 页 DOI: [10.1109/TASLP.2021.3098764](https://dx.doi.org/10.1109/TASLP.2021.3098764)'
- en: '[143] Marco Tagliasacchi, Yunpeng Li, Karolis Misiunas and Dominik Roblek “SEANet:
    A Multi-modal Speech Enhancement Network”, 2020 DOI: [10.48550/arXiv.2009.02095](https://dx.doi.org/10.48550/arXiv.2009.02095)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Marco Tagliasacchi, Yunpeng Li, Karolis Misiunas 和 Dominik Roblek “SEANet:
    多模态语音增强网络”，2020 DOI: [10.48550/arXiv.2009.02095](https://dx.doi.org/10.48550/arXiv.2009.02095)'
- en: '[144] Jeff Donahue et al. “End-to-End Adversarial Text-to-Speech”, 2021 DOI:
    [10.48550/arXiv.2006.03575](https://dx.doi.org/10.48550/arXiv.2006.03575)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Jeff Donahue 等人 “端到端对抗文本到语音”，2021 DOI: [10.48550/arXiv.2006.03575](https://dx.doi.org/10.48550/arXiv.2006.03575)'
- en: '[145] Marco Pasini and Jan Schlüter “Musika! Fast Infinite Waveform Music Generation”,
    2022 DOI: [10.48550/arXiv.2208.08706](https://dx.doi.org/10.48550/arXiv.2208.08706)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Marco Pasini 和 Jan Schlüter “Musika! 快速无限波形音乐生成”，2022 DOI: [10.48550/arXiv.2208.08706](https://dx.doi.org/10.48550/arXiv.2208.08706)'
- en: '[146] Jesse Engel et al. “GANSynth: Adversarial Neural Audio Synthesis”, 2019
    DOI: [10.48550/arXiv.1902.08710](https://dx.doi.org/10.48550/arXiv.1902.08710)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Jesse Engel 等人 “GANSynth: 对抗神经音频合成”，2019 DOI: [10.48550/arXiv.1902.08710](https://dx.doi.org/10.48550/arXiv.1902.08710)'
- en: '[147] Mikołaj Bińkowski et al. “High Fidelity Speech Synthesis with Adversarial
    Networks”, 2019 DOI: [10.48550/arXiv.1909.11646](https://dx.doi.org/10.48550/arXiv.1909.11646)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Mikołaj Bińkowski 等人 “高保真语音合成与对抗网络”，2019 DOI: [10.48550/arXiv.1909.11646](https://dx.doi.org/10.48550/arXiv.1909.11646)'
- en: '[148] Peihao Chen et al. “Generating Visually Aligned Sound From Videos” In
    *IEEE Transactions on Image Processing* 29, 2020, pp. 8292–8302 DOI: [10.1109/TIP.2020.3009820](https://dx.doi.org/10.1109/TIP.2020.3009820)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Peihao Chen 等人 “从视频生成视觉对齐的声音” 发表在 *IEEE 图像处理汇刊* 29, 2020, 第 8292–8302
    页 DOI: [10.1109/TIP.2020.3009820](https://dx.doi.org/10.1109/TIP.2020.3009820)'
- en: '[149] Kun Su, Xiulong Liu and Eli Shlizerman “Audeo: Audio Generation for a
    Silent Performance Video” In *Advances in Neural Information Processing Systems*
    33 Curran Associates, Inc., 2020, pp. 3325–3337 URL: [https://proceedings.neurips.cc/paper/2020/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Kun Su、Xiulong Liu 和 Eli Shlizerman “Audeo: 为无声表演视频生成音频” 发表在 *神经信息处理系统进展*
    33 Curran Associates, Inc., 2020年，页码 3325–3337 URL: [https://proceedings.neurips.cc/paper/2020/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html)'
- en: '[150] Geng Yang et al. “Multi-Band Melgan: Faster Waveform Generation For High-Quality
    Text-To-Speech” In *2021 IEEE Spoken Language Technology Workshop (SLT)*, 2021,
    pp. 492–498 DOI: [10.1109/SLT48900.2021.9383551](https://dx.doi.org/10.1109/SLT48900.2021.9383551)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Geng Yang 等 “Multi-Band Melgan: 高质量文本到语音的更快波形生成” 发表在 *2021 IEEE 口语语言技术研讨会（SLT）*，2021年，页码
    492–498 DOI: [10.1109/SLT48900.2021.9383551](https://dx.doi.org/10.1109/SLT48900.2021.9383551)'
- en: '[151] Rongjie Huang et al. “Multi-Singer: Fast Multi-Singer Singing Voice Vocoder
    With A Large-Scale Corpus” In *Proceedings of the 29th ACM International Conference
    on Multimedia*, MM ’21 New York, NY, USA: Association for Computing Machinery,
    2021, pp. 3945–3954 DOI: [10.1145/3474085.3475437](https://dx.doi.org/10.1145/3474085.3475437)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Rongjie Huang 等 “Multi-Singer: 快速多歌手歌声合成器与大规模语料库” 发表在 *第29届ACM国际多媒体会议论文集*，MM
    ’21 纽约，NY，USA: 计算机协会，2021年，页码 3945–3954 DOI: [10.1145/3474085.3475437](https://dx.doi.org/10.1145/3474085.3475437)'
- en: '[152] Ivan Kobyzev, Simon J.. Prince and Marcus A. Brubaker “Normalizing Flows:
    An Introduction and Review of Current Methods” In *IEEE Transactions on Pattern
    Analysis and Machine Intelligence* 43.11, 2021, pp. 3964–3979 DOI: [10.1109/TPAMI.2020.2992934](https://dx.doi.org/10.1109/TPAMI.2020.2992934)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Ivan Kobyzev、Simon J.. Prince 和 Marcus A. Brubaker “正则化流：介绍与当前方法综述” 发表在
    *IEEE 模式分析与机器智能学报* 43.11，2021年，页码 3964–3979 DOI: [10.1109/TPAMI.2020.2992934](https://dx.doi.org/10.1109/TPAMI.2020.2992934)'
- en: '[153] Aaron Oord et al. “Parallel WaveNet: Fast High-Fidelity Speech Synthesis”
    In *Proceedings of the 35th International Conference on Machine Learning* PMLR,
    2018, pp. 3918–3926 URL: [https://proceedings.mlr.press/v80/oord18a.html](https://proceedings.mlr.press/v80/oord18a.html)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Aaron Oord 等 “Parallel WaveNet: 快速高保真语音合成” 发表在 *第35届国际机器学习大会论文集* PMLR，2018年，页码
    3918–3926 URL: [https://proceedings.mlr.press/v80/oord18a.html](https://proceedings.mlr.press/v80/oord18a.html)'
- en: '[154] Wei Ping, Kainan Peng and Jitong Chen “ClariNet: Parallel Wave Generation
    in End-to-End Text-to-Speech”, 2019 DOI: [10.48550/arXiv.1807.07281](https://dx.doi.org/10.48550/arXiv.1807.07281)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Wei Ping、Kainan Peng 和 Jitong Chen “ClariNet: 端到端文本到语音中的并行波形生成”，2019年
    DOI: [10.48550/arXiv.1807.07281](https://dx.doi.org/10.48550/arXiv.1807.07281)'
- en: '[155] Ryan Prenger, Rafael Valle and Bryan Catanzaro “Waveglow: A Flow-based
    Generative Network for Speech Synthesis” In *ICASSP 2019 - 2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2019, pp. 3617–3621
    DOI: [10.1109/ICASSP.2019.8683143](https://dx.doi.org/10.1109/ICASSP.2019.8683143)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Ryan Prenger、Rafael Valle 和 Bryan Catanzaro “Waveglow: 基于流的生成网络用于语音合成”
    发表在 *ICASSP 2019 - 2019 IEEE 国际声学、语音和信号处理会议（ICASSP）*，2019年，页码 3617–3621 DOI: [10.1109/ICASSP.2019.8683143](https://dx.doi.org/10.1109/ICASSP.2019.8683143)'
- en: '[156] Sungwon Kim et al. “FloWaveNet : A Generative Flow for Raw Audio”, 2019
    DOI: [10.48550/arXiv.1811.02155](https://dx.doi.org/10.48550/arXiv.1811.02155)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Sungwon Kim 等 “FloWaveNet: 一种用于原始音频的生成流”，2019年 DOI: [10.48550/arXiv.1811.02155](https://dx.doi.org/10.48550/arXiv.1811.02155)'
- en: '[157] Jaehyeon Kim, Sungwon Kim, Jungil Kong and Sungroh Yoon “Glow-TTS: A
    Generative Flow for Text-to-Speech via Monotonic Alignment Search” In *Advances
    in Neural Information Processing Systems* 33 Curran Associates, Inc., 2020, pp.
    8067–8077 URL: [https://proceedings.neurips.cc/paper/2020/hash/5c3b99e8f92532e5ad1556e53ceea00c-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/5c3b99e8f92532e5ad1556e53ceea00c-Abstract.html)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Jaehyeon Kim、Sungwon Kim、Jungil Kong 和 Sungroh Yoon “Glow-TTS: 一种用于文本到语音的生成流通过单调对齐搜索”
    发表在 *神经信息处理系统进展* 33 Curran Associates, Inc., 2020年，页码 8067–8077 URL: [https://proceedings.neurips.cc/paper/2020/hash/5c3b99e8f92532e5ad1556e53ceea00c-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/5c3b99e8f92532e5ad1556e53ceea00c-Abstract.html)'
- en: '[158] Hyeongju Kim et al. “WaveNODE: A Continuous Normalizing Flow for Speech
    Synthesis”, 2020 DOI: [10.48550/arXiv.2006.04598](https://dx.doi.org/10.48550/arXiv.2006.04598)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Hyeongju Kim 等 “WaveNODE: 一种用于语音合成的连续正则化流”，2020年 DOI: [10.48550/arXiv.2006.04598](https://dx.doi.org/10.48550/arXiv.2006.04598)'
- en: '[159] Wei Ping, Kainan Peng, Kexin Zhao and Zhao Song “WaveFlow: A Compact
    Flow-based Model for Raw Audio” In *Proceedings of the 37th International Conference
    on Machine Learning* PMLR, 2020, pp. 7706–7716 URL: [https://proceedings.mlr.press/v119/ping20a.html](https://proceedings.mlr.press/v119/ping20a.html)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Wei Ping、Kainan Peng、Kexin Zhao 和 Zhao Song “WaveFlow：一种紧凑的基于流的原始音频模型”
    在 *第37届国际机器学习大会论文集* PMLR，2020年，第7706–7716页 URL: [https://proceedings.mlr.press/v119/ping20a.html](https://proceedings.mlr.press/v119/ping20a.html)'
- en: '[160] Hyun-Wook Yoon, Sang-Hoon Lee, Hyeong-Rae Noh and Seong-Whan Lee “Audio
    Dequantization for High Fidelity Audio Generation in Flow-based Neural Vocoder”,
    2020 DOI: [10.48550/arXiv.2008.06867](https://dx.doi.org/10.48550/arXiv.2008.06867)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Hyun-Wook Yoon、Sang-Hoon Lee、Hyeong-Rae Noh 和 Seong-Whan Lee “基于流的神经声码器中的高保真音频生成的音频去量化”，2020年
    DOI: [10.48550/arXiv.2008.06867](https://dx.doi.org/10.48550/arXiv.2008.06867)'
- en: '[161] Aston Zhang, Zachary C. Lipton, Mu Li and Alexander J. Smola “Dive into
    Deep Learning”, 2023 DOI: [10.48550/arXiv.2106.11342](https://dx.doi.org/10.48550/arXiv.2106.11342)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Aston Zhang、Zachary C. Lipton、Mu Li 和 Alexander J. Smola “深入深度学习”，2023年
    DOI: [10.48550/arXiv.2106.11342](https://dx.doi.org/10.48550/arXiv.2106.11342)'
- en: '[162] Khalid Zaman, Melike Sah, Cem Direkoglu and Masashi Unoki “A Survey of
    Audio Classification Using Deep Learning” In *IEEE Access* 11, 2023, pp. 106620–106649
    DOI: [10.1109/ACCESS.2023.3318015](https://dx.doi.org/10.1109/ACCESS.2023.3318015)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Khalid Zaman、Melike Sah、Cem Direkoglu 和 Masashi Unoki “基于深度学习的音频分类调查”
    在 *IEEE Access* 11，2023年，第106620–106649页 DOI: [10.1109/ACCESS.2023.3318015](https://dx.doi.org/10.1109/ACCESS.2023.3318015)'
- en: '[163] Cheng-Zhi Anna Huang et al. “Music Transformer”, 2018 DOI: [10.48550/arXiv.1809.04281](https://dx.doi.org/10.48550/arXiv.1809.04281)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Cheng-Zhi Anna Huang 等，“音乐 Transformer”，2018年 DOI: [10.48550/arXiv.1809.04281](https://dx.doi.org/10.48550/arXiv.1809.04281)'
- en: '[164] Curtis Hawthorne et al. “Enabling Factorized Piano Music Modeling and
    Generation with the MAESTRO Dataset”, 2019 DOI: [10.48550/arXiv.1810.12247](https://dx.doi.org/10.48550/arXiv.1810.12247)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Curtis Hawthorne 等，“利用 MAESTRO 数据集实现分解的钢琴音乐建模与生成”，2019年 DOI: [10.48550/arXiv.1810.12247](https://dx.doi.org/10.48550/arXiv.1810.12247)'
- en: '[165] Naihan Li et al. “Neural Speech Synthesis with Transformer Network” In
    *Proceedings of the AAAI Conference on Artificial Intelligence* 33.01, 2019, pp.
    6706–6713 DOI: [10.1609/aaai.v33i01.33016706](https://dx.doi.org/10.1609/aaai.v33i01.33016706)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Naihan Li 等，“基于 Transformer 网络的神经语音合成” 在 *AAAI 人工智能会议论文集* 33.01，2019年，第6706–6713页
    DOI: [10.1609/aaai.v33i01.33016706](https://dx.doi.org/10.1609/aaai.v33i01.33016706)'
- en: '[166] Naihan Li et al. “RobuTrans: A Robust Transformer-Based Text-to-Speech
    Model” In *Proceedings of the AAAI Conference on Artificial Intelligence* 34.05,
    2020, pp. 8228–8235 DOI: [10.1609/aaai.v34i05.6337](https://dx.doi.org/10.1609/aaai.v34i05.6337)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Naihan Li 等，“RobuTrans：一个稳健的基于 Transformer 的文本到语音模型” 在 *AAAI 人工智能会议论文集*
    34.05，2020年，第8228–8235页 DOI: [10.1609/aaai.v34i05.6337](https://dx.doi.org/10.1609/aaai.v34i05.6337)'
- en: '[167] Zhen Zeng et al. “Aligntts: Efficient Feed-Forward Text-to-Speech System
    Without Explicit Alignment” In *ICASSP 2020 - 2020 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2020, pp. 6714–6718 DOI:
    [10.1109/ICASSP40776.2020.9054119](https://dx.doi.org/10.1109/ICASSP40776.2020.9054119)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Zhen Zeng 等，“Aligntts：高效的前馈文本到语音系统，无需显式对齐” 在 *ICASSP 2020 - 2020 IEEE国际声学、语音和信号处理会议（ICASSP）*，2020年，第6714–6718页
    DOI: [10.1109/ICASSP40776.2020.9054119](https://dx.doi.org/10.1109/ICASSP40776.2020.9054119)'
- en: '[168] Jeff Ens and Philippe Pasquier “MMM : Exploring Conditional Multi-Track
    Music Generation with the Transformer”, 2020 DOI: [10.48550/arXiv.2008.06048](https://dx.doi.org/10.48550/arXiv.2008.06048)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Jeff Ens 和 Philippe Pasquier “MMM：使用 Transformer 探索条件多轨音乐生成”，2020年 DOI:
    [10.48550/arXiv.2008.06048](https://dx.doi.org/10.48550/arXiv.2008.06048)'
- en: '[169] Dan Lim et al. “JDI-T: Jointly Trained Duration Informed Transformer
    for Text-To-Speech without Explicit Alignment”, 2020 DOI: [10.48550/arXiv.2005.07799](https://dx.doi.org/10.48550/arXiv.2005.07799)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Dan Lim 等，“JDI-T：联合训练的持续时间信息 Transformer，无需显式对齐的文本到语音”，2020年 DOI: [10.48550/arXiv.2005.07799](https://dx.doi.org/10.48550/arXiv.2005.07799)'
- en: '[170] Mingjian Chen et al. “AdaSpeech: Adaptive Text to Speech for Custom Voice”,
    2021 DOI: [10.48550/arXiv.2103.00993](https://dx.doi.org/10.48550/arXiv.2103.00993)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Mingjian Chen 等，“AdaSpeech：用于自定义声音的自适应文本到语音”，2021年 DOI: [10.48550/arXiv.2103.00993](https://dx.doi.org/10.48550/arXiv.2103.00993)'
- en: '[171] Adrian Łańcucki “Fastpitch: Parallel Text-to-Speech with Pitch Prediction”
    In *ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and
    Signal Processing (ICASSP)*, 2021, pp. 6588–6592 DOI: [10.1109/ICASSP39728.2021.9413889](https://dx.doi.org/10.1109/ICASSP39728.2021.9413889)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Adrian Łańcucki “Fastpitch：具有音高预测的并行文本到语音” 见 *ICASSP 2021 - 2021 IEEE国际声学、语音与信号处理会议（ICASSP）*，2021年，第6588–6592页
    DOI: [10.1109/ICASSP39728.2021.9413889](https://dx.doi.org/10.1109/ICASSP39728.2021.9413889)'
- en: '[172] Shangzhe Di et al. “Video Background Music Generation with Controllable
    Music Transformer” In *Proceedings of the 29th ACM International Conference on
    Multimedia*, MM ’21 New York, NY, USA: Association for Computing Machinery, 2021,
    pp. 2037–2045 DOI: [10.1145/3474085.3475195](https://dx.doi.org/10.1145/3474085.3475195)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Shangzhe Di 等人。“具有可控音乐变换器的视频背景音乐生成” 见 *第29届ACM国际多媒体会议论文集*，MM ’21 纽约，NY，美国：计算机协会，2021年，第2037–2045页
    DOI: [10.1145/3474085.3475195](https://dx.doi.org/10.1145/3474085.3475195)'
- en: '[173] Ann Lee et al. “Direct Speech-to-Speech Translation with Discrete Units”,
    2022 DOI: [10.48550/arXiv.2107.05604](https://dx.doi.org/10.48550/arXiv.2107.05604)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Ann Lee 等人。“基于离散单元的直接语音到语音翻译”，2022 DOI: [10.48550/arXiv.2107.05604](https://dx.doi.org/10.48550/arXiv.2107.05604)'
- en: '[174] Weipeng Wang et al. “CPS: Full-Song and Style-Conditioned Music Generation
    with Linear Transformer” In *2022 IEEE International Conference on Multimedia
    and Expo Workshops (ICMEW)*, 2022, pp. 1–6 DOI: [10.1109/ICMEW56448.2022.9859286](https://dx.doi.org/10.1109/ICMEW56448.2022.9859286)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Weipeng Wang 等人。“CPS：全歌曲和风格条件音乐生成与线性变换器” 见 *2022 IEEE国际多媒体与博览会工作坊（ICMEW）*，2022年，第1–6页
    DOI: [10.1109/ICMEW56448.2022.9859286](https://dx.doi.org/10.1109/ICMEW56448.2022.9859286)'
- en: '[175] Xueyao Zhang et al. “Structure-Enhanced Pop Music Generation via Harmony-Aware
    Learning” In *Proceedings of the 30th ACM International Conference on Multimedia*,
    MM ’22 New York, NY, USA: Association for Computing Machinery, 2022, pp. 1204–1213
    DOI: [10.1145/3503161.3548084](https://dx.doi.org/10.1145/3503161.3548084)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Xueyao Zhang 等人。“通过和声感知学习增强结构的流行音乐生成” 见 *第30届ACM国际多媒体会议论文集*，MM ’22 纽约，NY，美国：计算机协会，2022年，第1204–1213页
    DOI: [10.1145/3503161.3548084](https://dx.doi.org/10.1145/3503161.3548084)'
- en: '[176] Chunhui Bao and Qianru Sun “Generating Music With Emotions” In *IEEE
    Transactions on Multimedia* 25, 2023, pp. 3602–3614 DOI: [10.1109/TMM.2022.3163543](https://dx.doi.org/10.1109/TMM.2022.3163543)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Chunhui Bao 和 Qianru Sun “情感音乐生成” 见 *IEEE多媒体期刊* 25，2023年，第3602–3614页
    DOI: [10.1109/TMM.2022.3163543](https://dx.doi.org/10.1109/TMM.2022.3163543)'
- en: '[177] Eugene Kharitonov et al. “Speak, Read and Prompt: High-Fidelity Text-to-Speech
    with Minimal Supervision”, 2023 DOI: [10.48550/arXiv.2302.03540](https://dx.doi.org/10.48550/arXiv.2302.03540)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Eugene Kharitonov 等人。“说话、阅读和提示：高保真度的文本到语音转换，最少的监督”，2023 DOI: [10.48550/arXiv.2302.03540](https://dx.doi.org/10.48550/arXiv.2302.03540)'
- en: '[178] Felix Kreuk et al. “AudioGen: Textually Guided Audio Generation”, 2023
    DOI: [10.48550/arXiv.2209.15352](https://dx.doi.org/10.48550/arXiv.2209.15352)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Felix Kreuk 等人。“AudioGen：文本引导的音频生成”，2023 DOI: [10.48550/arXiv.2209.15352](https://dx.doi.org/10.48550/arXiv.2209.15352)'
- en: '[179] Tu Anh Nguyen et al. “Generative Spoken Dialogue Language Modeling” In
    *Transactions of the Association for Computational Linguistics* 11, 2023, pp.
    250–266 DOI: [10.1162/tacl_a_00545](https://dx.doi.org/10.1162/tacl_a_00545)'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Tu Anh Nguyen 等人。“生成性口语对话语言建模” 见 *计算语言学协会会刊* 11，2023年，第250–266页 DOI:
    [10.1162/tacl_a_00545](https://dx.doi.org/10.1162/tacl_a_00545)'
- en: '[180] Peiling Lu et al. “MuseCoco: Generating Symbolic Music from Text”, 2023
    DOI: [10.48550/arXiv.2306.00110](https://dx.doi.org/10.48550/arXiv.2306.00110)'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Peiling Lu 等人。“MuseCoco：从文本生成符号音乐”，2023 DOI: [10.48550/arXiv.2306.00110](https://dx.doi.org/10.48550/arXiv.2306.00110)'
- en: '[181] Paul K. Rubenstein et al. “AudioPaLM: A Large Language Model That Can
    Speak and Listen”, 2023 DOI: [10.48550/arXiv.2306.12925](https://dx.doi.org/10.48550/arXiv.2306.12925)'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Paul K. Rubenstein 等人。“AudioPaLM：一个可以说话和听的语言模型”，2023 DOI: [10.48550/arXiv.2306.12925](https://dx.doi.org/10.48550/arXiv.2306.12925)'
- en: '[182] Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar and Bryan Pardo “VampNet:
    Music Generation via Masked Acoustic Token Modeling”, 2023 DOI: [10.48550/arXiv.2307.04686](https://dx.doi.org/10.48550/arXiv.2307.04686)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar 和 Bryan Pardo “VampNet：通过掩蔽声学标记建模生成音乐”，2023
    DOI: [10.48550/arXiv.2307.04686](https://dx.doi.org/10.48550/arXiv.2307.04686)'
- en: '[183] Zhichao Wang et al. “LM-VC: Zero-shot Voice Conversion via Speech Generation
    Based on Language Models”, 2023 DOI: [10.48550/arXiv.2306.10521](https://dx.doi.org/10.48550/arXiv.2306.10521)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Zhichao Wang 等人。“LM-VC：基于语言模型的零样本语音转换”，2023 DOI: [10.48550/arXiv.2306.10521](https://dx.doi.org/10.48550/arXiv.2306.10521)'
- en: '[184] Dongchao Yang et al. “UniAudio: An Audio Foundation Model Toward Universal
    Audio Generation”, 2023 DOI: [10.48550/arXiv.2310.00704](https://dx.doi.org/10.48550/arXiv.2310.00704)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Dongchao Yang 等人。“UniAudio: 通用音频生成的音频基础模型”，2023 DOI: [10.48550/arXiv.2310.00704](https://dx.doi.org/10.48550/arXiv.2310.00704)'
- en: '[185] Jade Copet et al. “Simple and Controllable Music Generation” In *Advances
    in Neural Information Processing Systems* 36, 2023, pp. 47704–47720 URL: [https://proceedings.neurips.cc/paper_files/paper/2023/hash/94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2023/hash/94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Jade Copet 等人。“简单且可控的音乐生成” 收录于 *神经信息处理系统进展* 36，2023，第47704–47720页 URL:
    [https://proceedings.neurips.cc/paper_files/paper/2023/hash/94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2023/hash/94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html)'
- en: '[186] Chris Donahue et al. “LakhNES: Improving Multi-Instrumental Music Generation
    with Cross-Domain Pre-Training”, 2019 DOI: [10.48550/arXiv.1907.04868](https://dx.doi.org/10.48550/arXiv.1907.04868)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Chris Donahue 等人。“LakhNES: 通过跨领域预训练提升多乐器音乐生成”，2019 DOI: [10.48550/arXiv.1907.04868](https://dx.doi.org/10.48550/arXiv.1907.04868)'
- en: '[187] Yu-Siang Huang and Yi-Hsuan Yang “Pop Music Transformer: Beat-based Modeling
    and Generation of Expressive Pop Piano Compositions” In *Proceedings of the 28th
    ACM International Conference on Multimedia*, MM ’20 New York, NY, USA: Association
    for Computing Machinery, 2020, pp. 1180–1188 DOI: [10.1145/3394171.3413671](https://dx.doi.org/10.1145/3394171.3413671)'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Yu-Siang Huang 和 Yi-Hsuan Yang “流行音乐Transformer: 基于节拍的表达性流行钢琴作曲建模与生成”
    收录于 *第28届ACM国际多媒体会议*，MM ’20 纽约, NY, USA: 计算机协会，2020，第1180–1188页 DOI: [10.1145/3394171.3413671](https://dx.doi.org/10.1145/3394171.3413671)'
- en: '[188] Zihang Dai et al. “Transformer-XL: Attentive Language Models Beyond a
    Fixed-Length Context”, 2019 arXiv: [http://arxiv.org/abs/1901.02860](http://arxiv.org/abs/1901.02860)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Zihang Dai 等人。“Transformer-XL: 超越固定长度上下文的注意力语言模型”，2019 arXiv: [http://arxiv.org/abs/1901.02860](http://arxiv.org/abs/1901.02860)'
- en: '[189] Curtis Hawthorne et al. “General-Purpose, Long-Context Autoregressive
    Modeling with Perceiver AR” In *Proceedings of the 39th International Conference
    on Machine Learning* PMLR, 2022, pp. 8535–8558 URL: [https://proceedings.mlr.press/v162/hawthorne22a.html](https://proceedings.mlr.press/v162/hawthorne22a.html)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Curtis Hawthorne 等人。“通用长上下文自回归建模与Perceiver AR” 收录于 *第39届国际机器学习大会* PMLR，2022，第8535–8558页
    URL: [https://proceedings.mlr.press/v162/hawthorne22a.html](https://proceedings.mlr.press/v162/hawthorne22a.html)'
- en: '[190] Botao Yu et al. “Museformer: Transformer with Fine- and Coarse-Grained
    Attention for Music Generation” In *Advances in Neural Information Processing
    Systems* 35, 2022, pp. 1376–1388 URL: [https://proceedings.neurips.cc/paper_files/paper/2022/hash/092c2d45005ea2db40fc24c470663416-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2022/hash/092c2d45005ea2db40fc24c470663416-Abstract-Conference.html)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Botao Yu 等人。“Museformer: 用于音乐生成的细粒度和粗粒度注意力的Transformer” 收录于 *神经信息处理系统进展*
    35，2022，第1376–1388页 URL: [https://proceedings.neurips.cc/paper_files/paper/2022/hash/092c2d45005ea2db40fc24c470663416-Abstract-Conference.html](https://proceedings.neurips.cc/paper_files/paper/2022/hash/092c2d45005ea2db40fc24c470663416-Abstract-Conference.html)'
- en: '[191] He Bai et al. “A$^3$T: Alignment-Aware Acoustic and Text Pretraining
    for Speech Synthesis and Editing” In *Proceedings of the 39th International Conference
    on Machine Learning* PMLR, 2022, pp. 1399–1411 URL: [https://proceedings.mlr.press/v162/bai22d.html](https://proceedings.mlr.press/v162/bai22d.html)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] He Bai 等人。“A$^3$T: 面向语音合成和编辑的对齐感知声学和文本预训练” 收录于 *第39届国际机器学习大会* PMLR，2022，第1399–1411页
    URL: [https://proceedings.mlr.press/v162/bai22d.html](https://proceedings.mlr.press/v162/bai22d.html)'
- en: '[192] Anmol Gulati et al. “Conformer: Convolution-augmented Transformer for
    Speech Recognition”, 2020 arXiv: [http://arxiv.org/abs/2005.08100](http://arxiv.org/abs/2005.08100)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Anmol Gulati 等人。“Conformer: 适用于语音识别的卷积增强Transformer”，2020 arXiv: [http://arxiv.org/abs/2005.08100](http://arxiv.org/abs/2005.08100)'
- en: '[193] Yi-Chen Chen et al. “SpeechNet: A Universal Modularized Model for Speech
    Processing Tasks”, 2021 DOI: [10.48550/arXiv.2105.03070](https://dx.doi.org/10.48550/arXiv.2105.03070)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Yi-Chen Chen 等人。“SpeechNet: 一种通用模块化语音处理模型”，2021 DOI: [10.48550/arXiv.2105.03070](https://dx.doi.org/10.48550/arXiv.2105.03070)'
- en: '[194] Sravya Popuri et al. “Enhanced Direct Speech-to-Speech Translation Using
    Self-supervised Pre-training and Data Augmentation”, 2022 DOI: [10.48550/arXiv.2204.02967](https://dx.doi.org/10.48550/arXiv.2204.02967)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Sravya Popuri 等人。“通过自监督预训练和数据增强提升直接语音到语音翻译”，2022 DOI: [10.48550/arXiv.2204.02967](https://dx.doi.org/10.48550/arXiv.2204.02967)'
- en: '[195] Haohe Liu et al. “AudioLDM: Text-to-Audio Generation with Latent Diffusion
    Models”, 2023 DOI: [10.48550/arXiv.2301.12503](https://dx.doi.org/10.48550/arXiv.2301.12503)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Haohe Liu 等 “AudioLDM: 基于潜在扩散模型的文本到音频生成”, 2023 DOI: [10.48550/arXiv.2301.12503](https://dx.doi.org/10.48550/arXiv.2301.12503)'
- en: '[196] Nanxin Chen et al. “WaveGrad: Estimating Gradients for Waveform Generation”,
    2020 DOI: [10.48550/arXiv.2009.00713](https://dx.doi.org/10.48550/arXiv.2009.00713)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Nanxin Chen 等 “WaveGrad: 用于波形生成的梯度估计”, 2020 DOI: [10.48550/arXiv.2009.00713](https://dx.doi.org/10.48550/arXiv.2009.00713)'
- en: '[197] Zhifeng Kong et al. “DiffWave: A Versatile Diffusion Model for Audio
    Synthesis”, 2021 DOI: [10.48550/arXiv.2009.09761](https://dx.doi.org/10.48550/arXiv.2009.09761)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Zhifeng Kong 等 “DiffWave: 一种多功能音频合成扩散模型”, 2021 DOI: [10.48550/arXiv.2009.09761](https://dx.doi.org/10.48550/arXiv.2009.09761)'
- en: '[198] Myeonghun Jeong et al. “Diff-TTS: A Denoising Diffusion Model for Text-to-Speech”,
    2021 DOI: [10.48550/arXiv.2104.01409](https://dx.doi.org/10.48550/arXiv.2104.01409)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] Myeonghun Jeong 等 “Diff-TTS: 一种去噪扩散模型用于文本到语音”, 2021 DOI: [10.48550/arXiv.2104.01409](https://dx.doi.org/10.48550/arXiv.2104.01409)'
- en: '[199] Vadim Popov et al. “Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech”
    In *Proceedings of the 38th International Conference on Machine Learning* PMLR,
    2021, pp. 8599–8608 URL: [https://proceedings.mlr.press/v139/popov21a.html](https://proceedings.mlr.press/v139/popov21a.html)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Vadim Popov 等 “Grad-TTS: 一种用于文本到语音的扩散概率模型” 发表在 *第38届国际机器学习会议论文集* PMLR,
    2021, 页码 8599–8608 URL: [https://proceedings.mlr.press/v139/popov21a.html](https://proceedings.mlr.press/v139/popov21a.html)'
- en: '[200] Yen-Ju Lu, Yu Tsao and Shinji Watanabe “A Study on Speech Enhancement
    Based on Diffusion Probabilistic Model” In *2021 Asia-Pacific Signal and Information
    Processing Association Annual Summit and Conference (APSIPA ASC)*, 2021, pp. 659–666
    URL: [https://ieeexplore.ieee.org/abstract/document/9689602](https://ieeexplore.ieee.org/abstract/document/9689602)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Yen-Ju Lu, Yu Tsao 和 Shinji Watanabe “基于扩散概率模型的语音增强研究” 发表在 *2021年亚太信号与信息处理协会年会及会议（APSIPA
    ASC）*，2021年，页码 659–666 URL: [https://ieeexplore.ieee.org/abstract/document/9689602](https://ieeexplore.ieee.org/abstract/document/9689602)'
- en: '[201] Rongjie Huang et al. “FastDiff: A Fast Conditional Diffusion Model for
    High-Quality Speech Synthesis”, 2022 DOI: [10.48550/arXiv.2204.09934](https://dx.doi.org/10.48550/arXiv.2204.09934)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Rongjie Huang 等 “FastDiff: 一种快速的条件扩散模型用于高质量语音合成”, 2022 DOI: [10.48550/arXiv.2204.09934](https://dx.doi.org/10.48550/arXiv.2204.09934)'
- en: '[202] Yen-Ju Lu et al. “Conditional Diffusion Probabilistic Model for Speech
    Enhancement” In *ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2022, pp. 7402–7406 DOI: [10.1109/ICASSP43922.2022.9746901](https://dx.doi.org/10.1109/ICASSP43922.2022.9746901)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Yen-Ju Lu 等 “用于语音增强的条件扩散概率模型” 发表在 *ICASSP 2022 - 2022 IEEE国际声学、语音与信号处理会议（ICASSP）*，2022年，页码
    7402–7406 DOI: [10.1109/ICASSP43922.2022.9746901](https://dx.doi.org/10.1109/ICASSP43922.2022.9746901)'
- en: '[203] Heeseung Kim, Sungwon Kim and Sungroh Yoon “Guided-TTS: A Diffusion Model
    for Text-to-Speech via Classifier Guidance” In *Proceedings of the 39th International
    Conference on Machine Learning* PMLR, 2022, pp. 11119–11133 URL: [https://proceedings.mlr.press/v162/kim22d.html](https://proceedings.mlr.press/v162/kim22d.html)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Heeseung Kim, Sungwon Kim 和 Sungroh Yoon “Guided-TTS: 通过分类器引导的文本到语音扩散模型”
    发表在 *第39届国际机器学习会议论文集* PMLR, 2022, 页码 11119–11133 URL: [https://proceedings.mlr.press/v162/kim22d.html](https://proceedings.mlr.press/v162/kim22d.html)'
- en: '[204] Sungwon Kim, Heeseung Kim and Sungroh Yoon “Guided-TTS 2: A Diffusion
    Model for High-quality Adaptive Text-to-Speech with Untranscribed Data”, 2022
    DOI: [10.48550/arXiv.2205.15370](https://dx.doi.org/10.48550/arXiv.2205.15370)'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Sungwon Kim, Heeseung Kim 和 Sungroh Yoon “Guided-TTS 2: 一种高质量自适应文本到语音扩散模型，支持无转录数据”,
    2022 DOI: [10.48550/arXiv.2205.15370](https://dx.doi.org/10.48550/arXiv.2205.15370)'
- en: '[205] Jinglin Liu et al. “DiffSinger: Singing Voice Synthesis via Shallow Diffusion
    Mechanism” In *Proceedings of the AAAI Conference on Artificial Intelligence*
    36.10, 2022, pp. 11020–11028 DOI: [10.1609/aaai.v36i10.21350](https://dx.doi.org/10.1609/aaai.v36i10.21350)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Jinglin Liu 等 “DiffSinger: 通过浅层扩散机制的唱歌声音合成” 发表在 *AAAI人工智能会议论文集* 36.10,
    2022, 页码 11020–11028 DOI: [10.1609/aaai.v36i10.21350](https://dx.doi.org/10.1609/aaai.v36i10.21350)'
- en: '[206] Joan Serrà et al. “Universal Speech Enhancement with Score-based Diffusion”,
    2022 DOI: [10.48550/arXiv.2206.03065](https://dx.doi.org/10.48550/arXiv.2206.03065)'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] Joan Serrà 等 “基于评分的扩散模型的通用语音增强”, 2022 DOI: [10.48550/arXiv.2206.03065](https://dx.doi.org/10.48550/arXiv.2206.03065)'
- en: '[207] Qingqing Huang et al. “Noise2Music: Text-conditioned Music Generation
    with Diffusion Models”, 2023 DOI: [10.48550/arXiv.2302.03917](https://dx.doi.org/10.48550/arXiv.2302.03917)'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] 黄青青等. “Noise2Music: 基于文本条件的音乐生成与扩散模型”, 2023 DOI: [10.48550/arXiv.2302.03917](https://dx.doi.org/10.48550/arXiv.2302.03917)'
- en: '[208] Shentong Mo, Jing Shi and Yapeng Tian “DiffAVA: Personalized Text-to-Audio
    Generation with Visual Alignment”, 2023 DOI: [10.48550/arXiv.2305.12903](https://dx.doi.org/10.48550/arXiv.2305.12903)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] 申通莫, 石静 和 田亚鹏 “DiffAVA: 基于视觉对齐的个性化文本到音频生成”, 2023 DOI: [10.48550/arXiv.2305.12903](https://dx.doi.org/10.48550/arXiv.2305.12903)'
- en: '[209] Max W.. Lam et al. “Efficient Neural Music Generation”, 2023 DOI: [10.48550/arXiv.2305.15719](https://dx.doi.org/10.48550/arXiv.2305.15719)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Max W. Lam等. “高效的神经音乐生成”, 2023 DOI: [10.48550/arXiv.2305.15719](https://dx.doi.org/10.48550/arXiv.2305.15719)'
- en: '[210] Zhibin Qiu et al. “SRTNET: Time Domain Speech Enhancement via Stochastic
    Refinement” In *ICASSP 2023 - 2023 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2023, pp. 1–5 DOI: [10.1109/ICASSP49357.2023.10095850](https://dx.doi.org/10.1109/ICASSP49357.2023.10095850)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] 邱志斌等. “SRTNET: 通过随机细化的时域语音增强” 载于 *ICASSP 2023 - 2023 IEEE国际声学、语音和信号处理会议
    (ICASSP)*, 2023, 页1–5 DOI: [10.1109/ICASSP49357.2023.10095850](https://dx.doi.org/10.1109/ICASSP49357.2023.10095850)'
- en: '[211] Ke Chen et al. “MusicLDM: Enhancing Novelty in Text-to-Music Generation
    Using Beat-Synchronous Mixup Strategies”, 2023 DOI: [10.48550/arXiv.2308.01546](https://dx.doi.org/10.48550/arXiv.2308.01546)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] 陈科等. “MusicLDM: 使用节拍同步混合策略提升文本到音乐生成的创新性”, 2023 DOI: [10.48550/arXiv.2308.01546](https://dx.doi.org/10.48550/arXiv.2308.01546)'
- en: '[212] Peike Li et al. “JEN-1: Text-Guided Universal Music Generation with Omnidirectional
    Diffusion Models”, 2023 DOI: [10.48550/arXiv.2308.04729](https://dx.doi.org/10.48550/arXiv.2308.04729)'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] 李佩克等. “JEN-1: 使用全方位扩散模型的文本引导通用音乐生成”, 2023 DOI: [10.48550/arXiv.2308.04729](https://dx.doi.org/10.48550/arXiv.2308.04729)'
- en: '[213] Yatong Bai et al. “Accelerating Diffusion-Based Text-to-Audio Generation
    with Consistency Distillation”, 2023 DOI: [10.48550/arXiv.2309.10740](https://dx.doi.org/10.48550/arXiv.2309.10740)'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] 白亚彤等. “通过一致性蒸馏加速基于扩散的文本到音频生成”, 2023 DOI: [10.48550/arXiv.2309.10740](https://dx.doi.org/10.48550/arXiv.2309.10740)'
- en: '[214] Pengfei Zhu et al. “ERNIE-Music: Text-to-Waveform Music Generation with
    Diffusion Models”, 2023 DOI: [10.48550/arXiv.2302.04456](https://dx.doi.org/10.48550/arXiv.2302.04456)'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] 朱鹏飞等. “ERNIE-Music: 使用扩散模型的文本到波形音乐生成”, 2023 DOI: [10.48550/arXiv.2302.04456](https://dx.doi.org/10.48550/arXiv.2302.04456)'
- en: '[215] Yi Yuan et al. “Retrieval-Augmented Text-to-Audio Generation”, 2024 DOI:
    [10.48550/arXiv.2309.08051](https://dx.doi.org/10.48550/arXiv.2309.08051)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] 袁怡等. “检索增强的文本到音频生成”, 2024 DOI: [10.48550/arXiv.2309.08051](https://dx.doi.org/10.48550/arXiv.2309.08051)'
- en: '[216] Curtis Hawthorne et al. “Multi-Instrument Music Synthesis with Spectrogram
    Diffusion”, 2022 DOI: [10.48550/arXiv.2206.05408](https://dx.doi.org/10.48550/arXiv.2206.05408)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] Curtis Hawthorne等. “使用频谱图扩散的多乐器音乐合成”, 2022 DOI: [10.48550/arXiv.2206.05408](https://dx.doi.org/10.48550/arXiv.2206.05408)'
- en: '[217] Minki Kang, Dongchan Min and Sung Ju Hwang “Grad-StyleSpeech: Any-Speaker
    Adaptive Text-to-Speech Synthesis with Diffusion Models” In *ICASSP 2023 - 2023
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2023, pp. 1–5 DOI: [10.1109/ICASSP49357.2023.10095515](https://dx.doi.org/10.1109/ICASSP49357.2023.10095515)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Minki Kang, Dongchan Min 和 Sung Ju Hwang “Grad-StyleSpeech: 任何说话者自适应的文本到语音合成与扩散模型”
    载于 *ICASSP 2023 - 2023 IEEE国际声学、语音和信号处理会议 (ICASSP)*, 2023, 页1–5 DOI: [10.1109/ICASSP49357.2023.10095515](https://dx.doi.org/10.1109/ICASSP49357.2023.10095515)'
- en: '[218] Rongjie Huang et al. “Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced
    Diffusion Models” In *Proceedings of the 40th International Conference on Machine
    Learning* PMLR, 2023, pp. 13916–13932 URL: [https://proceedings.mlr.press/v202/huang23i.html](https://proceedings.mlr.press/v202/huang23i.html)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] 黄荣杰等. “Make-An-Audio: 使用提示增强的扩散模型生成文本到音频” 载于 *第40届国际机器学习会议论文集* PMLR,
    2023, 页13916–13932 网址: [https://proceedings.mlr.press/v202/huang23i.html](https://proceedings.mlr.press/v202/huang23i.html)'
- en: '[219] Haohe Liu et al. “AudioLDM 2: Learning Holistic Audio Generation with
    Self-supervised Pretraining”, 2023 DOI: [10.48550/arXiv.2308.05734](https://dx.doi.org/10.48550/arXiv.2308.05734)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] 刘浩禾等. “AudioLDM 2: 通过自监督预训练学习全面音频生成”, 2023 DOI: [10.48550/arXiv.2308.05734](https://dx.doi.org/10.48550/arXiv.2308.05734)'
- en: '[220] Flavio Schneider, Ojasv Kamal, Zhijing Jin and Bernhard Schölkopf “Mo\^usai:
    Text-to-Music Generation with Long-Context Latent Diffusion”, 2023 DOI: [10.48550/arXiv.2301.11757](https://dx.doi.org/10.48550/arXiv.2301.11757)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Flavio Schneider, Ojasv Kamal, Zhijing Jin 和 Bernhard Schölkopf “Mo\^usai:
    长上下文潜在扩散的文本到音乐生成”，2023 DOI: [10.48550/arXiv.2301.11757](https://dx.doi.org/10.48550/arXiv.2301.11757)'
- en: '[221] Soumi Maiti, Yifan Peng, Takaaki Saeki and Shinji Watanabe “Speechlmscore:
    Evaluating Speech Generation Using Speech Language Model” In *ICASSP 2023 - 2023
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2023, pp. 1–5 DOI: [10.1109/ICASSP49357.2023.10095710](https://dx.doi.org/10.1109/ICASSP49357.2023.10095710)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] Soumi Maiti, Yifan Peng, Takaaki Saeki 和 Shinji Watanabe “Speechlmscore:
    使用语音语言模型评估语音生成” 在 *ICASSP 2023 - 2023 IEEE 国际声学、语音与信号处理会议 (ICASSP)*，2023，第 1–5
    页 DOI: [10.1109/ICASSP49357.2023.10095710](https://dx.doi.org/10.1109/ICASSP49357.2023.10095710)'
- en: '[222] Sercan Arik et al. “Neural Voice Cloning with a Few Samples” In *Advances
    in Neural Information Processing Systems* 31 Curran Associates, Inc., 2018 URL:
    [https://proceedings.neurips.cc/paper_files/paper/2018/hash/4559912e7a94a9c32b09d894f2bc3c82-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2018/hash/4559912e7a94a9c32b09d894f2bc3c82-Abstract.html)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Sercan Arik 等人 “少量样本的神经语音克隆” 在 *神经信息处理系统进展* 第 31 卷 Curran Associates,
    Inc., 2018 网址: [https://proceedings.neurips.cc/paper_files/paper/2018/hash/4559912e7a94a9c32b09d894f2bc3c82-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2018/hash/4559912e7a94a9c32b09d894f2bc3c82-Abstract.html)'
- en: '[223] Shawn Hershey et al. “CNN Architectures for Large-Scale Audio Classification”
    In *2017 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, 2017, pp. 131–135 DOI: [10.1109/ICASSP.2017.7952132](https://dx.doi.org/10.1109/ICASSP.2017.7952132)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Shawn Hershey 等人 “大型音频分类的 CNN 架构” 在 *2017 IEEE 国际声学、语音与信号处理会议 (ICASSP)*，2017，第
    131–135 页 DOI: [10.1109/ICASSP.2017.7952132](https://dx.doi.org/10.1109/ICASSP.2017.7952132)'
- en: '[224] Qiuqiang Kong et al. “PANNs: Large-Scale Pretrained Audio Neural Networks
    for Audio Pattern Recognition” In *IEEE/ACM Transactions on Audio, Speech, and
    Language Processing* 28, 2020, pp. 2880–2894 DOI: [10.1109/TASLP.2020.3030497](https://dx.doi.org/10.1109/TASLP.2020.3030497)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Qiuqiang Kong 等人 “PANNs: 大规模预训练音频神经网络用于音频模式识别” 在 *IEEE/ACM 音频、语音和语言处理汇刊*
    第 28 卷，2020，第 2880–2894 页 DOI: [10.1109/TASLP.2020.3030497](https://dx.doi.org/10.1109/TASLP.2020.3030497)'
