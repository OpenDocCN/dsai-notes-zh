- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:39:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:39:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.13854] The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.13854] 分布式图神经网络系统的演变及其在图处理和深度学习中的起源：一项调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.13854](https://ar5iv.labs.arxiv.org/html/2305.13854)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.13854](https://ar5iv.labs.arxiv.org/html/2305.13854)
- en: 'The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式图神经网络系统的演变及其在图处理和深度学习中的起源：一项调查
- en: Jana Vatter [jana.vatter@tum.de](mailto:jana.vatter@tum.de) [0000-0002-5900-5709](https://orcid.org/0000-0002-5900-5709
    "ORCID identifier") Technical University of MunichDepartment of Computer ScienceMunichGermany
    ,  Ruben Mayer [ruben.mayer@uni-bayreuth.de](mailto:ruben.mayer@uni-bayreuth.de)
    [0000-0001-9870-7466](https://orcid.org/0000-0001-9870-7466 "ORCID identifier")
    University of BayreuthFaculty of Mathematics, Physics & Computer ScienceBayreuthGermany
     and  Hans-Arno Jacobsen [jacobsen@eecg.toronto.edu](mailto:jacobsen@eecg.toronto.edu)
    [0000-0003-0813-0101](https://orcid.org/0000-0003-0813-0101 "ORCID identifier")
    University of TorontoDepartment of Electrical & Computer EngineeringTorontoCanada
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Jana Vatter [jana.vatter@tum.de](mailto:jana.vatter@tum.de) [0000-0002-5900-5709](https://orcid.org/0000-0002-5900-5709
    "ORCID identifier") 慕尼黑工业大学计算机科学系慕尼黑德国，Ruben Mayer [ruben.mayer@uni-bayreuth.de](mailto:ruben.mayer@uni-bayreuth.de)
    [0000-0001-9870-7466](https://orcid.org/0000-0001-9870-7466 "ORCID identifier")
    拜罗伊特大学数学、物理与计算机科学学院拜罗伊特德国，以及 Hans-Arno Jacobsen [jacobsen@eecg.toronto.edu](mailto:jacobsen@eecg.toronto.edu)
    [0000-0003-0813-0101](https://orcid.org/0000-0003-0813-0101 "ORCID identifier")
    多伦多大学电气与计算机工程系多伦多加拿大
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Graph Neural Networks (GNNs) are an emerging research field. This specialized
    Deep Neural Network (DNN) architecture is capable of processing graph structured
    data and bridges the gap between graph processing and Deep Learning (DL). As graphs
    are everywhere, GNNs can be applied to various domains including recommendation
    systems, computer vision, natural language processing, biology and chemistry.
    With the rapid growing size of real world graphs, the need for efficient and scalable
    GNN training solutions has come. Consequently, many works proposing GNN systems
    have emerged throughout the past few years. However, there is an acute lack of
    overview, categorization and comparison of such systems. We aim to fill this gap
    by summarizing and categorizing important methods and techniques for large-scale
    GNN solutions. In addition, we establish connections between GNN systems, graph
    processing systems and DL systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）是一个新兴的研究领域。这种专门的深度神经网络（DNN）架构能够处理图结构数据，并弥合图处理与深度学习（DL）之间的差距。由于图的广泛存在，GNNs
    可以应用于各种领域，包括推荐系统、计算机视觉、自然语言处理、生物学和化学。随着现实世界图的规模快速增长，对高效和可扩展的GNN训练解决方案的需求也随之而来。因此，过去几年中出现了许多提出GNN系统的工作。然而，针对这些系统的概述、分类和比较严重不足。我们旨在通过总结和分类大规模GNN解决方案的重要方法和技术来填补这一空白。此外，我们还建立了GNN系统、图处理系统和DL系统之间的联系。
- en: 'Graph Neural Networks, Deep Learning Systems, Graph Processing Systems^†^†ccs:
    General and reference Surveys and overviews^†^†ccs: Computing methodologies Machine
    learning^†^†ccs: Mathematics of computing Graph algorithms^†^†ccs: Computing methodologies Distributed
    computing methodologies^†^†ccs: Computer systems organization Neural networks{NoHyper}^†^†© Owner
    2023\. This is the author’s version of the work. It is posted here for your personal
    use. Not for redistribution. The definite version was published in ACM Computing
    Surveys, https://doi.org/10.1145/3597428'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '图神经网络、深度学习系统、图处理系统^†^†ccs: 一般和参考调查及概述^†^†ccs: 计算方法 机器学习^†^†ccs: 计算数学 图算法^†^†ccs:
    计算方法 分布式计算方法^†^†ccs: 计算机系统组织 神经网络{NoHyper}^†^†© 版权所有 2023\. 这是作者版本的工作。仅供个人使用。禁止再分发。最终版本已发表在《ACM计算调查》中，https://doi.org/10.1145/3597428'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Deep Learning (DL) on graph structured data is a promising and rising field.
    As graphs are all around us (Miller, [1995](#bib.bib133); Gupta et al., [2013](#bib.bib65);
    Balaban, [1985](#bib.bib7)), they can be used in numerous DL applications to model
    and analyze complex problems. Due to the differing properties of the input data,
    common DL architectures such as Convolutional Neural Networks (CNNs) (LeCun et al.,
    [1995](#bib.bib105)) or Recurrent Neural Networks (RNNs) (Rumelhart et al., [1986](#bib.bib151);
    Hochreiter and Schmidhuber, [1997](#bib.bib76)) can not easily be used for DL
    on graphs. Therefore, a new type of Deep Neural Network (DNN) architecture has
    been developed in the late 2000s, the so-called Graph Neural Network (GNN) (Gori
    et al., [2005](#bib.bib61); Scarselli et al., [2008](#bib.bib156)). This architecture
    bridges the gap between graph processing and DL by combining message passing with
    neural network (NN) operations. The field of applications of GNNs ranges from
    recommendation systems (Fan et al., [2019](#bib.bib38); Ying et al., [2018](#bib.bib201)),
    computer vision (Gao et al., [2020a](#bib.bib49); Sarlin et al., [2020](#bib.bib155))
    and natural language processing (Yao et al., [2019](#bib.bib200); LeClair et al.,
    [2020](#bib.bib104)) to biology and medicine (Gaudelet et al., [2021](#bib.bib53);
    Gao et al., [2020b](#bib.bib50)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）在图结构数据上的应用是一个有前景且快速发展的领域。由于图结构数据无处不在（Miller, [1995](#bib.bib133); Gupta
    et al., [2013](#bib.bib65); Balaban, [1985](#bib.bib7)），它们可以在众多DL应用中用于建模和分析复杂问题。由于输入数据的特性不同，常见的DL架构，如卷积神经网络（CNNs）（LeCun
    et al., [1995](#bib.bib105)）或递归神经网络（RNNs）（Rumelhart et al., [1986](#bib.bib151);
    Hochreiter and Schmidhuber, [1997](#bib.bib76)），不能直接用于图上的DL。因此，在2000年代末，开发了一种新的深度神经网络（DNN）架构，即所谓的图神经网络（GNN）（Gori
    et al., [2005](#bib.bib61); Scarselli et al., [2008](#bib.bib156)）。这种架构通过将消息传递与神经网络（NN）操作相结合，弥合了图处理和DL之间的差距。GNN的应用领域包括推荐系统（Fan
    et al., [2019](#bib.bib38); Ying et al., [2018](#bib.bib201)）、计算机视觉（Gao et al.,
    [2020a](#bib.bib49); Sarlin et al., [2020](#bib.bib155)）以及自然语言处理（Yao et al., [2019](#bib.bib200);
    LeClair et al., [2020](#bib.bib104)），甚至涵盖了生物学和医学（Gaudelet et al., [2021](#bib.bib53);
    Gao et al., [2020b](#bib.bib50)）。
- en: 'With the ever-growing size of real world graphs (Hu et al., [2021](#bib.bib77))
    and deeper GNN models (Liu et al., [2020](#bib.bib113); Li et al., [2020](#bib.bib107),
    [2021b](#bib.bib106)), the need for efficient GNN training solutions has emerged
    that aim to process large-scale graphs in a fast and efficient manner. With large
    data sets containing millions of nodes and billions of edges (Hu et al., [2021](#bib.bib77)),
    a high level of parallelization is demanded along with the opportunity to run
    the computations on distributed architectures. As a result, current research increasingly
    deals with developing large-scale GNN systems (Zheng et al., [2020](#bib.bib214);
    Wang et al., [2021a](#bib.bib184); Jia et al., [2020a](#bib.bib88)). Multiple
    issues arise when designing such a system: First, systems originally designed
    for DL or for graph processing cannot be directly used for GNN training as the
    former do not support graph processing operations and the latter do not support
    DL operations. A vertex in a graph usually only is connected to few other vertices
    leading to many zero values in the graph adjacency matrix. In contrast, DL operations
    incorporate high-dimensional features leading to dense matrices. Consequently,
    both sparse and dense matrix operations need to be supported. Thus, specialized
    frameworks are being developed incorporating and optimizing both types of tasks.
    Second, redundant computations and repeated data access might occur during a training
    iteration. For instance, if nodes share the same neighbor, the neighbors’ activation
    will be computed multiple times in most cases since the computation of the nodes
    is regarded independently of one another (Chen et al., [2022](#bib.bib19); Jia
    et al., [2020b](#bib.bib89)). Thus, the same activation is calculated redundantly.
    Therefore, the need for appropriate memory management and inter-process communication
    is enforced. Third, the interdependence of training samples is challenging and
    leads to increased communication between the machines. One has to decide how to
    partition and distribute the graph among the machines and which strategy to choose
    for synchronizing intermediate results.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着实际图形的规模不断增长（Hu et al., [2021](#bib.bib77)）以及更深层次的图神经网络（GNN）模型（Liu et al.,
    [2020](#bib.bib113); Li et al., [2020](#bib.bib107), [2021b](#bib.bib106)），对高效GNN训练解决方案的需求应运而生，这些解决方案旨在以快速而高效的方式处理大规模图形。由于大数据集包含数百万个节点和数十亿条边（Hu
    et al., [2021](#bib.bib77)），因此需要高度的并行化，并有机会在分布式架构上运行计算。因此，目前的研究越来越多地集中于开发大规模GNN系统（Zheng
    et al., [2020](#bib.bib214); Wang et al., [2021a](#bib.bib184); Jia et al., [2020a](#bib.bib88)）。在设计这样的系统时出现了多个问题：首先，最初为深度学习（DL）或图处理设计的系统不能直接用于GNN训练，因为前者不支持图处理操作，后者不支持DL操作。图中的一个顶点通常只与少数其他顶点连接，导致图邻接矩阵中有许多零值。相对而言，DL操作涉及高维特征，导致密集矩阵。因此，需要支持稀疏矩阵和密集矩阵操作。因此，正在开发专门的框架来整合和优化这两种任务。其次，在训练迭代过程中可能会发生冗余计算和重复数据访问。例如，如果节点共享相同的邻居，则邻居的激活在大多数情况下会被多次计算，因为节点的计算被视为彼此独立（Chen
    et al., [2022](#bib.bib19); Jia et al., [2020b](#bib.bib89)）。因此，相同的激活被重复计算。因此，需要适当的内存管理和进程间通信。第三，训练样本的相互依赖性是具有挑战性的，并导致机器之间的通信增加。必须决定如何在机器之间划分和分发图形，以及选择哪种策略来同步中间结果。
- en: While there are many works proposing GNN systems that aim to solve the above
    issues and propose optimization methods, there is an acute lack of overview, categorization,
    and comparison. There are numerous surveys that classify either scalable graph
    processing or DL systems, but rarely any articles coping with systems for GNNs.
    Hence, we aim to fill this gap by giving an overview and categorization of large-scale
    GNN training techniques.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多工作提出了旨在解决上述问题并提出优化方法的GNN系统，但缺乏全面的概述、分类和比较。虽然有许多调查对可扩展的图处理或DL系统进行了分类，但很少有文章涉及GNN系统。因此，我们的目标是通过提供大规模GNN训练技术的概述和分类来填补这一空白。
- en: 1.1\. Related Surveys
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 相关调查
- en: On the one hand, there are surveys on graph processing systems. For instance,
    Batafari et al. (Batarfi et al., [2015](#bib.bib8)) provide an overview of large-scale
    graph processing systems and present five popular platforms in detail. In addition,
    they investigate the performance using selected algorithms and graph datasets.
    Heidari et al. (Heidari et al., [2018](#bib.bib71)) and Coimbra et al. (Coimbra
    et al., [2021](#bib.bib28)) discuss and categorize graph processing systems in
    regard to concepts such as partitioning, communication and dynamism. While the
    above papers give a general overview of systems irrespective of the hardware set
    up, Shi et al. (Shi et al., [2018](#bib.bib164)) focus on graph processing on
    GPUs. They distinguish between graph processing systems on a single GPU and others
    with a multi GPU setting. Another specialized work is done by Kalavri et al. (Kalavri
    et al., [2017](#bib.bib91)) where programming abstractions for large-scale graph
    processing systems are investigated and evaluated. Gui et al. (Gui et al., [2019](#bib.bib63))
    concentrate on preprocessing, parallel graph computing and runtime scheduling
    from the hardware side. Xu et al. (Xu et al., [2014](#bib.bib195)) further explore
    hardware acceleration for graph processing and mainly investigate GPUs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，有关图处理系统的调查研究。例如，Batafari 等人（Batarfi et al., [2015](#bib.bib8)）提供了对大规模图处理系统的概述，并详细介绍了五个流行的平台。此外，他们还使用选择的算法和图数据集研究了性能。Heidari
    等人（Heidari et al., [2018](#bib.bib71)）和Coimbra 等人（Coimbra et al., [2021](#bib.bib28)）讨论并分类了图处理系统，涉及如分区、通信和动态性等概念。虽然上述论文提供了对系统的一般概述，而不考虑硬件设置，但Shi
    等人（Shi et al., [2018](#bib.bib164)）专注于在GPU上的图处理。他们区分了单GPU和多GPU设置下的图处理系统。另一个专门的工作是Kalavri
    等人（Kalavri et al., [2017](#bib.bib91)）的研究，涉及大规模图处理系统的编程抽象，并进行了评估。Gui 等人（Gui et
    al., [2019](#bib.bib63)）则专注于从硬件角度进行预处理、并行图计算和运行时调度。Xu 等人（Xu et al., [2014](#bib.bib195)）进一步探索了图处理的硬件加速，并主要研究了GPU。
- en: On the other hand, an overview of systems for DNN training is provided in numerous
    works. Chahal et al. (Chahal et al., [2020](#bib.bib17)) give an overview of distributed
    training methods and algorithms. The authors also dive into frameworks for distributed
    DNN training. In their survey, Ben-Nun et al. (Ben-Nun and Hoefler, [2019](#bib.bib9))
    give insights into parallelization strategies for DL. They approach the problem
    from a theoretical angle, model different types of concurrency and explore distributed
    system architectures. The survey by Zhang et al. (Zhang et al., [2018](#bib.bib212))
    introduces distributed acceleration techniques from the algorithm, distributed
    system and applications side. In addition, the price and cost of acceleration
    as well as challenges and limitations are presented. Mayer and Jacobsen (Mayer
    and Jacobsen, [2020](#bib.bib125)) investigate challenges, techniques and tools
    for distributed DL. In their paper, a selection of 11 open-source DL frameworks
    are analyzed and compared. The key aspects of Ouyang et al. (Ouyang et al., [2021](#bib.bib135))
    and Tang et al. (Tang et al., [2020](#bib.bib171)) are communication optimization
    strategies when performing distributed DL. The former highlight algorithm optimizations
    as well as network-level optimizations for large-scale DNN training, the latter
    present communication synchronization methods, compression techniques, systems
    architectures, and different types of parallelization. Other research focusing
    on communication optimizations for large-scale distributed DL is done by Shi et
    al. (Shi et al., [2020](#bib.bib163)). Instead of examining the scaling problem
    from a qualitative perspective, the authors pursue a quantitative approach. Therefore,
    they conduct experiments with selected communication optimization techniques on
    a GPU cluster.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，许多研究提供了有关 DNN 训练系统的概述。Chahal 等人（Chahal et al., [2020](#bib.bib17)）概述了分布式训练方法和算法。作者还深入探讨了分布式
    DNN 训练的框架。在他们的调查中，Ben-Nun 等人（Ben-Nun and Hoefler, [2019](#bib.bib9)）提供了对 DL 并行化策略的见解。他们从理论角度出发，建模不同类型的并发，并探索分布式系统架构。Zhang
    等人（Zhang et al., [2018](#bib.bib212)）的调查介绍了从算法、分布式系统和应用方面的分布式加速技术。此外，还介绍了加速的价格和成本以及面临的挑战和限制。Mayer
    和 Jacobsen（Mayer and Jacobsen, [2020](#bib.bib125)）研究了分布式 DL 的挑战、技术和工具。在他们的论文中，对
    11 个开源 DL 框架进行了分析和比较。Ouyang 等人（Ouyang et al., [2021](#bib.bib135)）和 Tang 等人（Tang
    et al., [2020](#bib.bib171)）关注的关键方面是进行分布式 DL 时的通信优化策略。前者强调了大规模 DNN 训练的算法优化以及网络层级优化，后者则介绍了通信同步方法、压缩技术、系统架构和不同类型的并行化。Shi
    等人（Shi et al., [2020](#bib.bib163)）进行的其他研究专注于大规模分布式 DL 的通信优化。他们没有从定性角度检查扩展问题，而是采用了定量方法。因此，他们在
    GPU 集群上进行了一些选定通信优化技术的实验。
- en: To our knowledge, there are many surveys on general aspects of GNNs, like methods,
    architectures and applications (Zhou et al., [2020](#bib.bib216); Zhang et al.,
    [2020b](#bib.bib210); Hamilton et al., [2017b](#bib.bib68); Chami et al., [2020](#bib.bib18);
    Wu et al., [2020](#bib.bib190)), but only few study optimizations for GNN training
    (Abadal et al., [2021](#bib.bib2); Besta and Hoefler, [2022](#bib.bib10); Serafini
    and Guan, [2021](#bib.bib160)). Abadal et al. (Abadal et al., [2021](#bib.bib2))
    provide an overview of algorithms and accelerators for GNN training. They focus
    on methods from the software side as well as hardware specific optimizations,
    but do not draw parallels to methods introduced by graph processing systems, which
    is one of our core contributions. Serafini et al. (Serafini and Guan, [2021](#bib.bib160))
    also explore scalable GNN training. However, they specialize on the comparison
    of whole-graph and sample-based training and do not investigate other techniques
    for large-scale GNN training. Besta and Hoefler (Besta and Hoefler, [2022](#bib.bib10))
    provide an in-depth analysis of accelerator techniques for GNN training. Their
    main contribution lies in the field of parallelism techniques whereas we give
    insight into the overall GNN training pipeline, corresponding optimizations and,
    most importantly, their origin. In contrast to all of the above, our focus lies
    on large-scale systems for GNNs and the corresponding optimization techniques
    within each step of the training process. Additionally, we provide an overview
    of the two background technologies, systems for graph processing and systems for
    DNN training, and draw the connection to GNN systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，关于GNN的一般方面，如方法、架构和应用（Zhou et al., [2020](#bib.bib216); Zhang et al., [2020b](#bib.bib210);
    Hamilton et al., [2017b](#bib.bib68); Chami et al., [2020](#bib.bib18); Wu et
    al., [2020](#bib.bib190)）有很多调查，但仅有少数研究了GNN训练的优化（Abadal et al., [2021](#bib.bib2);
    Besta and Hoefler, [2022](#bib.bib10); Serafini and Guan, [2021](#bib.bib160)）。Abadal
    et al. (Abadal et al., [2021](#bib.bib2))提供了GNN训练算法和加速器的概述。他们专注于软件方面的方法以及硬件特定的优化，但没有将其与图处理系统引入的方法相比较，而这正是我们的核心贡献之一。Serafini
    et al. (Serafini and Guan, [2021](#bib.bib160))也探索了可扩展的GNN训练。然而，他们专注于整体图和基于样本的训练比较，并未研究大规模GNN训练的其他技术。Besta
    and Hoefler (Besta and Hoefler, [2022](#bib.bib10))提供了对GNN训练加速器技术的深入分析。他们的主要贡献在于并行技术领域，而我们则提供了对整体GNN训练管道、相应优化以及最重要的，它们的来源的洞见。与上述所有不同的是，我们的重点放在大规模GNN系统以及训练过程每一步中的相应优化技术上。此外，我们提供了两个背景技术的概述，即图处理系统和DNN训练系统，并将其与GNN系统联系起来。
- en: 1.2\. Our Contributions
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 我们的贡献
- en: This survey examines distributed systems for scalable GNN training. We differentiate
    ourselves from other surveys by not only presenting methods for GNN systems, but
    also by giving insights into the two crucial background topics, systems for graph
    processing and systems for DNN training. It is important to know about the basic
    ideas behind those topics to get a better understanding of how and why these techniques
    are used in large-scale GNN training. We establish connections between the fields
    and show that most techniques used for large-scale GNN training are inspired by
    methods either from graph processing or DNN training. This way, we bring together
    the graph processing, DL and systems community that are all contributing to distributed
    GNN training. In addition, we categorize and compare the methods regarding questions
    of partitioning, sampling, inter-process communication, level of parallelism and
    scheduling. This should make it easier for researchers and practitioners to asses
    the usability of the presented systems and methods to their own application scenario.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查研究了用于可扩展GNN训练的分布式系统。我们与其他调查的区别在于不仅呈现了GNN系统的方法，还对两个关键背景主题，即图处理系统和DNN训练系统，提供了洞见。了解这些主题背后的基本思想对更好地理解这些技术在大规模GNN训练中的应用及原因非常重要。我们建立了各领域之间的联系，并展示了大多数用于大规模GNN训练的技术是如何受到图处理或DNN训练方法的启发的。通过这种方式，我们将图处理、深度学习和系统社区聚集在一起，这些社区都为分布式GNN训练做出了贡献。此外，我们对方法进行了分类和比较，涉及分区、采样、进程间通信、并行度和调度等问题。这将使研究人员和实践者更容易评估所呈现系统和方法在自己应用场景中的适用性。
- en: 1.3\. Structure of the Survey
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 调查结构
- en: 'We structure this survey as follows: In Section [2](#S2 "2\. Foundations ‣
    The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey"), we provide fundamentals of
    distributed systems for graph processing and for DL. We discuss the main aspects
    for achieving parallelism, scalability and efficiency. In Section [3](#S3 "3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey"),
    we relate the insights from graph processing systems and DL systems to facilitate
    the comprehension of methods used by systems for GNN training. Finally, open challenges,
    limitations and research trends are presented in Section [4](#S4 "4\. Discussion
    and Outlook ‣ The Evolution of Distributed Systems for Graph Neural Networks and
    their Origin in Graph Processing and Deep Learning: A Survey").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本调查报告结构安排如下：在第[2](#S2 "2\. 基础 ‣ 图神经网络的分布式系统演变及其在图处理和深度学习中的起源：调查")节中，我们提供了图处理和深度学习（DL）的分布式系统的基础知识。我们讨论了实现并行性、可扩展性和效率的主要方面。在第[3](#S3
    "3\. 图神经网络的系统 ‣ 图神经网络的分布式系统演变及其在图处理和深度学习中的起源：调查")节中，我们将图处理系统和深度学习系统的见解联系起来，以帮助理解用于GNN训练的系统方法。最后，在第[4](#S4
    "4\. 讨论与展望 ‣ 图神经网络的分布式系统演变及其在图处理和深度学习中的起源：调查")节中，我们介绍了开放挑战、局限性和研究趋势。
- en: 2\. Foundations
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 基础
- en: This section describes relevant topics building the basis of distributed GNN
    training. We start by giving a short introduction to general graph processing
    followed by presenting selected graph processing systems. Furthermore, we explore
    the basic steps of neural network training, and how the training process can be
    realized in a distributed fashion.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了构建分布式GNN训练基础的相关主题。我们首先简要介绍一般图处理，然后介绍一些选定的图处理系统。此外，我们探讨了神经网络训练的基本步骤，以及如何以分布式方式实现训练过程。
- en: 2.1\. Graph Processing
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 图处理
- en: A graph $G$ is a non-linear data structure, meaning the data elements are not
    sequentially or linearly ordered. It can be formally described as $G=(V,E)$ where
    $V$ denotes the set of vertices and $E$ the set of edges. A vertex $v$, also known
    as node, represents an object and the edge $e$ describes a relation between two
    vertices. For example, a vertex can represent a person and an edge models the
    relationship between two persons. There are many different types of graphs, e.g.,
    the graph can contain cycles (cyclic graph) or no cycles (acyclic graph), the
    edges can be directed or undirected and a weight can be assigned to each edge
    signifying its importance (weighted graph). Depending on the density of edges
    within the graph, it can be sparse, dense or fully connected.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 $G$ 是一种非线性数据结构，这意味着数据元素不是按顺序或线性顺序排列的。它可以正式描述为 $G=(V,E)$，其中 $V$ 表示顶点集合，$E$
    表示边集合。一个顶点 $v$，也称为节点，表示一个对象，而边 $e$ 描述两个顶点之间的关系。例如，一个顶点可以表示一个人，而一条边表示两个个人之间的关系。图有许多不同的类型，例如，图可以包含循环（循环图）或不包含循环（非循环图），边可以是有向的或无向的，并且可以给每条边分配一个权重，表示其重要性（加权图）。根据图中边的密度，图可以是稀疏的、密集的或完全连接的。
- en: Graphs occur in numerous domains including social networks (Tang and Liu, [2010](#bib.bib170);
    Fan, [2012](#bib.bib36); Gupta et al., [2013](#bib.bib65)), route optimization
    and transportation (Sobota et al., [2008](#bib.bib166); Czerwionka et al., [2011](#bib.bib29)),
    natural language processing (Miller, [1995](#bib.bib133); Manning et al., [2014](#bib.bib121)),
    recommendation systems (Huang et al., [2002](#bib.bib83); Silva et al., [2010](#bib.bib165);
    Guo et al., [2020](#bib.bib64)) and the natural sciences (Balaban, [1985](#bib.bib7);
    Ralaivola et al., [2005](#bib.bib143); Mason and Verwoerd, [2007](#bib.bib123)).
    For instance, a social network graph models the relation between users, graphs
    in natural language processing represent the relations between words or sentences,
    and in the natural sciences, graphs help to model the constitution of molecules.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图在许多领域中出现，包括社交网络（Tang 和 Liu，[2010](#bib.bib170)；Fan，[2012](#bib.bib36)；Gupta
    等，[2013](#bib.bib65)），路径优化和交通运输（Sobota 等，[2008](#bib.bib166)；Czerwionka 等，[2011](#bib.bib29)），自然语言处理（Miller，[1995](#bib.bib133)；Manning
    等，[2014](#bib.bib121)），推荐系统（Huang 等，[2002](#bib.bib83)；Silva 等，[2010](#bib.bib165)；Guo
    等，[2020](#bib.bib64)）以及自然科学（Balaban，[1985](#bib.bib7)；Ralaivola 等，[2005](#bib.bib143)；Mason
    和 Verwoerd，[2007](#bib.bib123)）。例如，社交网络图建模用户之间的关系，自然语言处理中的图表示单词或句子之间的关系，而在自然科学中，图有助于建模分子结构。
- en: Graph processing algorithms explore properties of the vertices, relations between
    them or characteristics of a subgraph or the whole graph. Depending on the use
    case, different types of graph problems need to be solved by those such as traversals,
    component identification, community detection, centrality calculation, pattern
    matching and graph anonymization (Dominguez-Sal et al., [2010](#bib.bib34); Coimbra
    et al., [2021](#bib.bib28)). Algorithms like depth-first search (Tarjan, [1972](#bib.bib172)),
    breadth-first search (Bundy and Wallen, [1984](#bib.bib15)), Dijkstra (Dijkstra
    et al., [1959](#bib.bib32)), Girvan-Newman algorithm (Girvan and Newman, [2002](#bib.bib58)),
    and PageRank (Page et al., [1999](#bib.bib138)) are among the most famous to tackle
    the presented problems.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图处理算法探索顶点的属性、它们之间的关系或子图或整个图的特征。根据用例，不同类型的图问题需要解决，例如遍历、组件识别、社区检测、中心性计算、模式匹配和图匿名化（Dominguez-Sal等，[2010](#bib.bib34)；Coimbra等，[2021](#bib.bib28)）。像深度优先搜索（Tarjan，[1972](#bib.bib172)）、广度优先搜索（Bundy和Wallen，[1984](#bib.bib15)）、Dijkstra（Dijkstra等，[1959](#bib.bib32)）、Girvan-Newman算法（Girvan和Newman，[2002](#bib.bib58)）和PageRank（Page等，[1999](#bib.bib138)）等算法是解决这些问题的最著名算法之一。
- en: 2.2\. Distributed Graph Processing
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 分布式图处理
- en: To process large graphs and to accelerate computation, graph processing is parallelized
    and distributed across machines. One of the most important methods to process
    data in parallel is the MapReduce computing model (Dean and Ghemawat, [2008](#bib.bib31)).
    MapReduce incorporates two user-defined primitive functions map and reduce. The
    map function takes a key-value pair as input and processes it in regard to the
    specified functionality. All intermediate results are grouped and passed to the
    reduce function which merges them to obtain a smaller set of values. A master
    node is responsible for assigning the map and reduce functions to the worker nodes.
    Each map worker takes a shard of the input pairs, applies the given function and
    stores the intermediate results locally. The reduce workers load the intermediate
    files, apply the reduce function and write the results to output files. With the
    help of the MapReduce computing model, large amounts of data can be handled; however,
    it reaches its limits when employing it for graph processing. A reason for MapReduce
    being unsuitable for graph processing is that the vertices within a graph are
    not independent. When performing a computing step of a graph algorithm, knowledge
    about multiple vertices is needed, leading to time- and resource-consuming data
    accesses. While graph processing algorithms may perform multiple iterations, the
    MapReduce model is optimized for sequential algorithms (McCune et al., [2015](#bib.bib130)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理大型图形并加速计算，图处理被并行化并分布在多台机器上。处理数据的最重要方法之一是MapReduce计算模型（Dean和Ghemawat，[2008](#bib.bib31)）。MapReduce包含两个用户定义的原始函数map和reduce。map函数以键值对作为输入，并根据指定的功能处理它。所有中间结果被分组并传递给reduce函数，后者将其合并以获得一个较小的值集。一个主节点负责将map和reduce函数分配给工作节点。每个map工作节点处理输入对的一个分片，应用给定的函数并在本地存储中间结果。reduce工作节点加载中间文件，应用reduce函数并将结果写入输出文件。借助MapReduce计算模型，大量数据可以被处理；然而，当用于图处理时，它达到了极限。MapReduce不适用于图处理的一个原因是图中的顶点不是独立的。在执行图算法的计算步骤时，需要了解多个顶点，这导致时间和资源消耗大的数据访问。尽管图处理算法可能执行多次迭代，但MapReduce模型是针对顺序算法优化的（McCune等，[2015](#bib.bib130)）。
- en: 'Therefore, the Pregel system (Malewicz et al., [2010](#bib.bib120)) was developed
    which specializes on parallel graph processing. It supports iterative computations
    more efficiently than MapReduce and keeps the data set in memory to facilitate
    repeated accesses. Further, a vertex-centric programming model is provided allowing
    the user to express graph algorithms more easily. Thus, the user thinks of how
    the algorithm is modeled from the perspective of a vertex rather than dataflows
    and transformation operators. During the process, messages are passed between
    the vertices in a Bulk Synchronous Parallel (BSP) (Valiant, [1990](#bib.bib176))
    manner. Hence, the messages are transmitted synchronously after all machines have
    finished their computation step. The iterative model works as follows: in each
    iteration, called superstep $S$, a user-defined function is executed in parallel
    for each vertex $v$. Aside from computing the vertex value, the function can pass
    messages to other vertices along outgoing edges during an iteration. In superstep
    $S$, it can send a message which will be received in superstep $S+1$ while messages
    sent to $v$ in superstep $S-1$ can be read. Similar to MapReduce, Pregel uses
    a reduction mechanism called ”aggregator” to combine resulting values and make
    them available in the next superstep $S+1$. The introduced synchronous superstep
    model can be used for various graph algorithms and is the basis for subsequent
    systems (Salihoglu and Widom, [2013](#bib.bib153); Khayyat et al., [2013](#bib.bib95);
    Bu et al., [2014](#bib.bib14)).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Pregel系统（Malewicz et al., [2010](#bib.bib120)）应运而生，专注于并行图处理。它比MapReduce更高效地支持迭代计算，并将数据集保存在内存中，以便于重复访问。此外，提供了一个以顶点为中心的编程模型，使用户更容易表达图算法。因此，用户从顶点的角度思考算法的建模，而不是数据流和转换操作符。在此过程中，消息以批同步并行（BSP）（Valiant,
    [1990](#bib.bib176)）的方式在顶点之间传递。因此，消息在所有机器完成计算步骤后同步传输。迭代模型的工作方式如下：在每次迭代中，称为超级步骤$S$，用户定义的函数在每个顶点$v$上并行执行。除了计算顶点值外，该函数还可以在迭代过程中沿着外发边向其他顶点传递消息。在超级步骤$S$中，它可以发送一个消息，该消息将在超级步骤$S+1$中被接收，同时在超级步骤$S-1$中发送给$v$的消息可以被读取。类似于MapReduce，Pregel使用称为“聚合器”的归约机制来合并结果值，并使其在下一个超级步骤$S+1$中可用。引入的同步超级步骤模型可以用于各种图算法，并成为后续系统（Salihoglu和Widom，[2013](#bib.bib153)；Khayyat
    et al., [2013](#bib.bib95)；Bu et al., [2014](#bib.bib14)）的基础。
- en: The GAS (gather-apply-scatter) model used in PowerGraph (Gonzalez et al., [2012](#bib.bib59))
    follows the four steps gather, sum, apply and scatter. The gather and sum operations
    resemble the map and reduce scheme aiming at assembling information about the
    vertices’ neighborhood. This phase is invoked on the edges adjacent to the vertex
    and executed locally on each machine. The results are sent to the master which
    runs the apply function with the aggregated information. It sends the resulting
    updates to all machines which execute the scatter-phase.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: PowerGraph（Gonzalez et al., [2012](#bib.bib59)）中使用的GAS（gather-apply-scatter）模型遵循四个步骤：收集、求和、应用和散布。收集和求和操作类似于映射和归约方案，旨在汇总顶点邻域的信息。此阶段在与顶点相邻的边上调用，并在每台机器上本地执行。结果被发送到主控机器，主控机器使用聚合的信息运行应用函数。它将结果更新发送到所有机器，这些机器执行散布阶段。
- en: 'Following the two programming models, a general distributed iterative vertex-centric
    graph processing scheme can be described: Before starting the iterative computation,
    the graph is split into partitions which are distributed across the machines.
    After that, the iterative process starts. Each vertex aggregates messages and
    applies a given function to compute a new state. Subsequently, the vertex state
    is updated. Then, the vertex passes messages to adjacent vertices containing the
    updated information and the aggregation phase starts over.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这两种编程模型，可以描述一种通用的分布式迭代顶点中心图处理方案：在开始迭代计算之前，图被分割成分区并分布在各台机器上。随后，迭代过程开始。每个顶点聚合消息并应用给定函数来计算新的状态。之后，顶点状态被更新。然后，顶点将包含更新信息的消息传递给相邻的顶点，聚合阶段重新开始。
- en: The vertex-centric paradigm lets the user intuitively define computations on
    graphs, but it reaches its limitations when processing real-world graphs with
    skewed degree distributions, large diameters and high densities (Yan et al., [2014](#bib.bib197)).
    Therefore, block-centric models have been developed (Yan et al., [2014](#bib.bib197);
    Fan et al., [2017](#bib.bib39); Tian et al., [2013](#bib.bib174)). Instead of
    sending messages from a vertex to its neighbors, blocks comprising of multiple
    vertices send messages to other blocks. Inside the blocks, information moves freely
    (Tian et al., [2013](#bib.bib174)) resulting in reduced communication cost and
    improved performance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 顶点中心范式允许用户直观地定义图上的计算，但在处理具有偏斜度分布、大直径和高密度的现实世界图时达到了其极限（Yan et al., [2014](#bib.bib197)）。因此，已经开发了基于块的模型（Yan
    et al., [2014](#bib.bib197); Fan et al., [2017](#bib.bib39); Tian et al., [2013](#bib.bib174)）。与其将消息从一个顶点发送到其邻居，不如由包含多个顶点的块向其他块发送消息。在块内部，信息自由流动（Tian
    et al., [2013](#bib.bib174)），从而降低了通信成本并提高了性能。
- en: 'Based on this general iterative model, we present and categorize selected distributed
    graph processing systems. We start by discussing partitioning strategies and how
    to store the resulting subgraphs. We distinguish between the synchronization method
    used in the message propagation phase and show how the messages can be transmitted.
    Table [1](#S2.T1 "Table 1 ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey") summarizes the most important
    properties of the selected systems and categories.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '基于这一通用迭代模型，我们展示并分类了选定的分布式图处理系统。我们首先讨论分区策略以及如何存储生成的子图。我们区分了消息传播阶段使用的同步方法，并展示了消息如何被传输。[表
    1](#S2.T1 "Table 1 ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣ The
    Evolution of Distributed Systems for Graph Neural Networks and their Origin in
    Graph Processing and Deep Learning: A Survey")总结了选定系统和类别的最重要属性。'
- en: Table 1. Categorization of different distributed graph processing systems
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 不同分布式图处理系统的分类
- en: '|  |  | Partitioning | Execution Mode | Message Propagation |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 分区 | 执行模式 | 消息传播 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Year | System |  | edge-cut | vertex-cut | synchronous | asynchronous | push
    | pull |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 系统 |  | 边切割 | 顶点切割 | 同步 | 异步 | 推送 | 拉取 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2010 | Pregel | (Malewicz et al., [2010](#bib.bib120)) | ✓ |  | ✓ |  | ✓
    |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 2010 | Pregel | (Malewicz et al., [2010](#bib.bib120)) | ✓ |  | ✓ |  | ✓
    |  |'
- en: '| 2012 | Apache Giraph | (FOUNDATION, [2012](#bib.bib45)) | ✓ |  | ✓ |  | ✓
    |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | Apache Giraph | (FOUNDATION, [2012](#bib.bib45)) | ✓ |  | ✓ |  | ✓
    |  |'
- en: '| 2012 | GraphLab | (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117))
    | ✓ |  |  | ✓ |  | ✓ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | GraphLab | (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117))
    | ✓ |  |  | ✓ |  | ✓ |'
- en: '| 2012 | Distributed GraphLab | (Low et al., [2012](#bib.bib116)) | ✓ |  |  |
    ✓ |  | ✓ |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | Distributed GraphLab | (Low et al., [2012](#bib.bib116)) | ✓ |  |  |
    ✓ |  | ✓ |'
- en: '| 2012 | PowerGraph | (Gonzalez et al., [2012](#bib.bib59)) |  | ✓ | ✓ | ✓
    |  | ✓ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | PowerGraph | (Gonzalez et al., [2012](#bib.bib59)) |  | ✓ | ✓ | ✓
    |  | ✓ |'
- en: '| 2013 | GPS | (Salihoglu and Widom, [2013](#bib.bib153)) | ✓ |  | ✓ |  | ✓
    |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | GPS | (Salihoglu and Widom, [2013](#bib.bib153)) | ✓ |  | ✓ |  | ✓
    |  |'
- en: '| 2013 | GRACE | (Wang et al., [2013](#bib.bib178)) | ✓ |  |  | ✓ | ✓ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | GRACE | (Wang et al., [2013](#bib.bib178)) | ✓ |  |  | ✓ | ✓ |  |'
- en: '| 2015 | PowerLyra | (Chen et al., [2015b](#bib.bib22)) | ✓ | ✓ | ✓ | ✓ |  |
    ✓ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | PowerLyra | (Chen et al., [2015b](#bib.bib22)) | ✓ | ✓ | ✓ | ✓ |  |
    ✓ |'
- en: 2.2.1\. Sampling
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 采样
- en: Graph sampling is a preprocessing step aiming to make the graph sparser by removing
    vertices or edges to reduce processing time and memory consumption. A key challenge
    here is to ensure that important graph structures are preserved. Iyer et al. (Iyer
    et al., [2018b](#bib.bib85)) calculate a probability signifying whether an edge
    between vertex $a$ and vertex $b$ should be kept in the graph or not. The probability
    value depends on the average degree of the graph, the out-degree of vertex $a$,
    the in-degree of vertex $b$ and the level of sparsification $s$ which can be chosen
    by the user. In this manner, less memory is needed while preserving the most important
    structures within the graph. The fast, approximate ASAP engine (Iyer et al., [2018a](#bib.bib84))
    for graph pattern mining consists of two phases, namely the sampling and closing
    phase. In the sampling phase, the graph is treated as a stream of edges. The edges
    are either randomly selected or depending on the previously streamed ones. Then,
    the closing phase awaits certain edges to complete patterns. This technique helps
    to preserve certain structures within the graph while excluding certain edges.
    An application-aware approach that drops certain messages is proposed by Schramm
    et al. (Schramm et al., [2022](#bib.bib158)). In every superstep, a given percentage
    of messages is identified as least important and dropped on-the-fly. The calculation
    of the importance value is dependent on the desired application and the deployed
    graph algorithm. Therefore, this method can be used in a plethora of applications.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图采样是一个预处理步骤，旨在通过移除顶点或边来使图更稀疏，以减少处理时间和内存消耗。一个关键挑战是确保重要的图结构得以保留。Iyer 等（Iyer et
    al., [2018b](#bib.bib85)）计算了一个概率，用以表明顶点 $a$ 和顶点 $b$ 之间的边是否应该保留在图中。该概率值取决于图的平均度、顶点
    $a$ 的出度、顶点 $b$ 的入度以及用户可以选择的稀疏化水平 $s$。通过这种方式，在保留图中最重要结构的同时，所需的内存较少。用于图模式挖掘的快速近似
    ASAP 引擎（Iyer et al., [2018a](#bib.bib84)）包括两个阶段，即采样阶段和闭合阶段。在采样阶段，图被视为边的流。边要么是随机选择的，要么是基于先前流过的边进行选择。然后，闭合阶段等待特定的边来完成模式。这种技术有助于在排除某些边的同时保留图中的某些结构。Schramm
    等（Schramm et al., [2022](#bib.bib158)）提出了一种感知应用的策略，该策略丢弃某些消息。在每个超级步骤中，一定百分比的消息被标识为最不重要的，并且在飞行过程中被丢弃。重要性值的计算依赖于所需的应用程序和部署的图算法。因此，这种方法可以用于各种应用程序。
- en: 2.2.2\. Partitioning
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. 分区
- en: There exist numerous approaches on graph partitioning algorithms that the systems
    adopt and refine. Those algorithms either follow the edge-cut (Karypis and Kumar,
    [1998a](#bib.bib93); Stanton and Kliot, [2012](#bib.bib167); Tsourakakis et al.,
    [2014](#bib.bib175)), vertex-cut (Schlag et al., [2019](#bib.bib157); Hanai et al.,
    [2019](#bib.bib69); Petroni et al., [2015](#bib.bib141); Mayer et al., [2018](#bib.bib124);
    Mayer and Jacobsen, [2021](#bib.bib126); Mayer et al., [2022](#bib.bib128)) or
    hybrid-cut model (Fan et al., [2022](#bib.bib40); Chen et al., [2015b](#bib.bib22)).
    Further, we distinguish between offline and online partitioners. While offline
    partitioners (Karypis, [1997](#bib.bib92); Hendrickson and Leland, [1993](#bib.bib72))
    determine the partitions according to the whole graph, online partitioners (Tsourakakis
    et al., [2014](#bib.bib175); Petroni et al., [2015](#bib.bib141); Mayer et al.,
    [2018](#bib.bib124)) stream vertices or edges and assign the vertex or edge to
    a partition on-the-fly. An interesting approach is HEP (Mayer and Jacobsen, [2021](#bib.bib126))
    where offline and online components are combined into a hybrid scheme.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多图分区算法的方法，系统采用并加以改进。这些算法要么遵循边切割（Karypis and Kumar, [1998a](#bib.bib93); Stanton
    and Kliot, [2012](#bib.bib167); Tsourakakis et al., [2014](#bib.bib175)）、顶点切割（Schlag
    et al., [2019](#bib.bib157); Hanai et al., [2019](#bib.bib69); Petroni et al.,
    [2015](#bib.bib141); Mayer et al., [2018](#bib.bib124); Mayer and Jacobsen, [2021](#bib.bib126);
    Mayer et al., [2022](#bib.bib128)）或混合切割模型（Fan et al., [2022](#bib.bib40); Chen
    et al., [2015b](#bib.bib22)）。此外，我们区分离线分区器和在线分区器。离线分区器（Karypis, [1997](#bib.bib92);
    Hendrickson and Leland, [1993](#bib.bib72)）根据整个图来确定分区，而在线分区器（Tsourakakis et al.,
    [2014](#bib.bib175); Petroni et al., [2015](#bib.bib141); Mayer et al., [2018](#bib.bib124)）流式处理顶点或边，并在飞行过程中将顶点或边分配到分区中。一种有趣的方法是
    HEP（Mayer and Jacobsen, [2021](#bib.bib126)），其中离线和在线组件被结合成一个混合方案。
- en: To asses the quality of the partitions, metrics like replication factor, communication
    cost and workload balancing are used. The replication factor indicates the ratio
    between the number of replicas and the total number of vertices. Based on that,
    the communication cost can be determined. Whenever an edge is cut, communication
    between the partitions is needed. When using an edge-cut method for example, the
    communication increases proportional to the number of edges that are cut. The
    so-called workload balancing aims to partition the graph in a way such that during
    the computation phase of the graph processing algorithm the load is balanced among
    the workers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估分区的质量，使用了如复制因子、通信成本和负载平衡等指标。复制因子表示复制的数量与顶点总数之间的比率。基于此，可以确定通信成本。每当切割一条边时，分区之间就需要进行通信。例如，使用边切割方法时，通信量与切割的边的数量成正比。所谓的负载平衡旨在以这样的方式对图进行分区，即在图处理算法的计算阶段，负载在工作者之间平衡。
- en: There exist various cost functions that help to form partitions. Depending on
    the goal and the application, the cost function needs to be defined differently.
    In the Linear Deterministic Greedy (LDG) (Stanton and Kliot, [2012](#bib.bib167))
    method, a vertex is allocated to the partition where it has most edges. This is
    combined with a penalty function indicating the capacity of a partition. Consequently,
    communication costs are minimized and balanced workloads across partitions is
    ensured. FENNEL (Tsourakakis et al., [2014](#bib.bib175)) unifies two heuristics,
    a vertex is either assigned to the cluster that shares the largest number of neighbors
    or the one with the smallest number of non-neighbors. This results in a minimal
    number of edges that are cut and consequently minimal communication costs. The
    High Degree (are) Replicated First (HDRF) (Petroni et al., [2015](#bib.bib141))
    method handles graphs with skewed distribution where the degree of the nodes highly
    varies. The goal is to balance the load evenly by cutting and replicating high-degree
    vertices. 2PS (Mayer et al., [2020](#bib.bib127)) gathers clustering information
    in a preprocessing phase which is then used in the scoring mechanism. Instead
    of defining one particular cost function, Fan et al. (Fan et al., [2022](#bib.bib40))
    use an application-driven approach. A cost model based on a given application
    algorithm is minimized in order to partition the graph. This results in an adaptive
    partitiong strategy suitable for numerous use cases.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 存在各种成本函数来帮助形成分区。根据目标和应用，成本函数的定义需要不同。在线性确定性贪婪（LDG）（Stanton 和 Kliot, [2012](#bib.bib167)）方法中，一个顶点被分配到拥有最多边的分区。这与一个表示分区容量的惩罚函数相结合。因此，通信成本被最小化，同时确保了分区之间的负载平衡。FENNEL（Tsourakakis
    et al., [2014](#bib.bib175)）将两种启发式方法统一为：一个顶点要么分配给共享最多邻居的簇，要么分配给邻居最少的簇。这会导致切割的边的数量最小，从而使通信成本最小。高度优先复制（HDRF）（Petroni
    et al., [2015](#bib.bib141)）方法处理具有偏斜分布的图，其中节点的度数差异很大。目标是通过切割和复制高度顶点来均匀平衡负载。2PS（Mayer
    et al., [2020](#bib.bib127)）在预处理阶段收集聚类信息，然后用于评分机制。Fan et al.（Fan et al., [2022](#bib.bib40)）采用了应用驱动的方法，而不是定义一个特定的成本函数。基于给定应用算法的成本模型被最小化，以便对图进行分区。这导致了一种适用于多种用例的自适应分区策略。
- en: '![Refer to caption](img/b59722995ade3a63e6c1bf3d402cf951.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b59722995ade3a63e6c1bf3d402cf951.png)'
- en: (a) edge-cut
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: （a）边切割
- en: '![Refer to caption](img/9171e27aa2ed74e51312b00ef33cf115.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9171e27aa2ed74e51312b00ef33cf115.png)'
- en: (b) vertex-cut
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: （b）顶点切割
- en: Figure 1. Edge-cut and vertex-cut partitioning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 边切割和顶点切割分区。
- en: 'Pregel (Malewicz et al., [2010](#bib.bib120)) uses one of the simplest methods
    to initially partition the graph. Here, the partitioning is based on the vertex
    ID using a hash function $hash(ID)\,mod\,N$ where $N$ is the number of partitions.
    The partitions are then evenly distributed across all workers. The underlying
    partitioning principle is called edge-cut as the edges are cut and replicated
    while the vertices are assigned to the different machines (Fig. [1(a)](#S2.F1.sf1
    "In Figure 1 ‣ 2.2.2\. Partitioning ‣ 2.2\. Distributed Graph Processing ‣ 2\.
    Foundations ‣ The Evolution of Distributed Systems for Graph Neural Networks and
    their Origin in Graph Processing and Deep Learning: A Survey")). Other systems
    relying on edge-cut partitioning are Apache Giraph (FOUNDATION, [2012](#bib.bib45)),
    GraphLab (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117)), Distributed
    GraphLab (Low et al., [2012](#bib.bib116)), Graph Processing System (GPS) (Salihoglu
    and Widom, [2013](#bib.bib153)) and GRACE (Wang et al., [2013](#bib.bib178)).
    Compared to other partitioning strategies, the presented edge-cut methods induce
    less computation cost and less overheads. Conversely, the following methods aim
    to reduce the cut size at the cost of higher runtimes and memory overheads. However,
    the obtained partitions are of improved quality.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Pregel (Malewicz 等人，[2010](#bib.bib120)) 使用了最简单的方法之一来初始分区图。在这里，分区是基于顶点 ID 使用哈希函数
    $hash(ID)\,mod\,N$，其中 $N$ 是分区的数量。然后将分区均匀分配给所有工作节点。底层分区原则称为边切分，因为边被切分和复制，而顶点被分配到不同的机器上（见图
    [1(a)](#S2.F1.sf1 "在图 1 ‣ 2.2.2\. 分区 ‣ 2.2\. 分布式图处理 ‣ 2\. 基础 ‣ 图神经网络的分布式系统的演变及其在图处理和深度学习中的起源：一项调查")）。其他依赖于边切分分区的系统包括
    Apache Giraph (FOUNDATION，[2012](#bib.bib45))、GraphLab (Low，[2013](#bib.bib115)；Low
    等人，[2014](#bib.bib117))、分布式 GraphLab (Low 等人，[2012](#bib.bib116))、图处理系统 (GPS)
    (Salihoglu 和 Widom，[2013](#bib.bib153)) 以及 GRACE (Wang 等人，[2013](#bib.bib178))。与其他分区策略相比，提出的边切分方法引起的计算成本和开销较少。相反，以下方法旨在减少切分大小，但代价是更高的运行时间和内存开销。然而，得到的分区质量有所提高。
- en: 'With edge-cut partitioning, the number of vertices is balanced across partitions,
    however, the number of edges per partition might highly vary. To obtain more equal
    partitions and to improve distributed processing of natural graphs, Gonzalez et
    al. propose balanced vertex-cut partitioning (Fig. [1(b)](#S2.F1.sf2 "In Figure
    1 ‣ 2.2.2\. Partitioning ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")) in their PowerGraph (Gonzalez
    et al., [2012](#bib.bib59)) model. Natural graphs typically are imbalanced and
    thus, difficult to partition. Some vertices have a high degree of outgoing edges
    while other vertices are of low degree. Hence, the computation costs per partition
    vary a lot. For that reason, the system employs balanced vertex-cut partitioning.
    The novel partitioning strategy improves the processing of natural graphs by equally
    distributing the edges between the machines whereas adjacent vertices are mirrored
    on the machines. One vertex among all replicas is randomly assigned as the master
    to compute and update the vertex state. All mirrors keep local copies of the vertex
    state.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用边切分分区时，顶点在各分区之间的数量是平衡的，但每个分区的边的数量可能差异很大。为了获得更均衡的分区并改善自然图的分布式处理，Gonzalez 等人提出了平衡顶点切分分区（见图
    [1(b)](#S2.F1.sf2 "在图 1 ‣ 2.2.2\. 分区 ‣ 2.2\. 分布式图处理 ‣ 2\. 基础 ‣ 图神经网络的分布式系统的演变及其在图处理和深度学习中的起源：一项调查")）在他们的
    PowerGraph (Gonzalez 等人，[2012](#bib.bib59)) 模型中。自然图通常是不平衡的，因此难以分区。一些顶点有很高的出边度，而其他顶点的度数较低。因此，每个分区的计算成本差异很大。为此，该系统采用了平衡顶点切分分区。这种新颖的分区策略通过在机器之间均匀分配边来改善自然图的处理，同时相邻的顶点在机器上被镜像。所有副本中有一个顶点被随机指定为主节点来计算和更新顶点状态。所有镜像保持顶点状态的本地副本。
- en: A dynamic repartitioning strategy to reduce communication is introduced in GPS
    (Salihoglu and Widom, [2013](#bib.bib153)). First, the system partitions the input
    graph with a standard partitioning technique. During the execution of the graph
    processing algorithm, communication is observed to determine which vertices to
    reassign to another worker and when to do so. Messages that are passed over the
    network are significantly reduced because repeated vertex accesses are performed
    by workers that already loaded the vertex in a previous iteration. Therefore,
    processing time is sped up, the workload is balanced and scaling to larger graphs
    is improved.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: GPS（Salihoglu 和 Widom，[2013](#bib.bib153)）引入了一种动态重新分区策略以减少通信。首先，系统使用标准分区技术对输入图进行分区。在图处理算法执行期间，观察通信情况以确定哪些顶点需要重新分配给其他工作者以及何时重新分配。由于重复的顶点访问由已经在先前迭代中加载该顶点的工作者执行，因此通过网络传递的消息显著减少。这加快了处理时间，平衡了工作负载，并改善了对更大图的扩展。
- en: PowerLyra (Chen et al., [2015b](#bib.bib22)) further optimizes the processing
    of natural graphs by introducing a hybrid-cut partitioning algorithm which combines
    edge- and vertex-cut. Furthermore, it handles high-degree vertices and low-degree
    vertices separately to minimize the replication of edges and vertices. The proposed
    balanced p-way hybrid-cut algorithm exploits a form of edge-cut if a vertex has
    a low number of outgoing edges and vertex-cut if a vertex is of high degree.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PowerLyra（Chen 等，[2015b](#bib.bib22)）通过引入一种混合切割分区算法进一步优化了自然图的处理，该算法结合了边切割和顶点切割。此外，它分别处理高度顶点和低度顶点，以最小化边和顶点的复制。提出的平衡
    p-way 混合切割算法利用边切割的形式，如果顶点的出边数较少，则使用顶点切割；如果顶点的度数较高，则使用顶点切割。
- en: GridGraph (Zhu et al., [2015](#bib.bib217)) introduces a grid representation
    for graphs to speed up partitioning. First, the vertices are partitioned into
    $P$ chunks where each chunk contains connected vertices. The edges are sorted
    into the resulting $P\times P$ grid according to their source and destination
    vertices. In this method, the list of edges does not need to be ordered leading
    to small preprocessing times. Further, the grid representation can facilitate
    the execution of the following graph processing steps.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GridGraph（Zhu 等，[2015](#bib.bib217)）引入了一种图的网格表示法以加速分区。首先，将顶点分成 $P$ 个块，每个块包含连接的顶点。边按照其源顶点和目标顶点被排序到结果的
    $P\times P$ 网格中。在这种方法中，边的列表无需排序，从而减少了预处理时间。此外，网格表示法可以促进以下图处理步骤的执行。
- en: As there are numerous partitioning strategies with different characteristics
    and objectives, it is challenging to choose the optimal one for a given application.
    Therefore, some experimental studies investigate the performance and resource
    usage of different strategies (Gill et al., [2018](#bib.bib56); Pacaci and Özsu,
    [2019](#bib.bib137); Abbas et al., [2018](#bib.bib4)) while the EASE system (Merkel
    et al., [pear](#bib.bib132)) provides a quantitative prediction and automatic
    selection.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在众多具有不同特性和目标的分区策略，选择适合特定应用的最佳策略具有挑战性。因此，一些实验研究调查了不同策略的性能和资源使用情况（Gill 等，[2018](#bib.bib56)；Pacaci
    和 Özsu，[2019](#bib.bib137)；Abbas 等，[2018](#bib.bib4)），而 EASE 系统（Merkel 等，[pear](#bib.bib132)）提供了定量预测和自动选择。
- en: 2.2.3\. Inter-Process Communication
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. 进程间通信
- en: Once the graph has been partitioned, it has to be decided how the worker processes
    synchronize their data, e.g., by sending messages or by accessing shared memory.
    In the former case, the systems store the subgraphs locally on the assigned machines
    and exchange synchronization messages. GraphLab (Low, [2013](#bib.bib115); Low
    et al., [2014](#bib.bib117)) supports the latter case and uses a shared-memory
    abstraction. A data graph accessible for all workers stores the program state
    as well as the corresponding data structures. Oriented on GraphLab, PowerGraph
    (Gonzalez et al., [2012](#bib.bib59)) also performs computation following a shared-memory
    view.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图被分区，需要决定工作进程如何同步它们的数据，例如，通过发送消息或访问共享内存。在前一种情况下，系统将子图本地存储在分配的机器上并交换同步消息。GraphLab（Low，[2013](#bib.bib115)；Low
    等，[2014](#bib.bib117)）支持后一种情况，并使用共享内存抽象。一个所有工作者都可以访问的数据图存储程序状态以及相应的数据结构。以 GraphLab
    为基础，PowerGraph（Gonzalez 等，[2012](#bib.bib59)）也按照共享内存视图进行计算。
- en: 2.2.4\. State Synchronization and Scheduling
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4\. 状态同步与调度
- en: When iteratively executing graph processing algorithms, the computation steps
    can be performed following a synchronous, asynchronous or hybrid scheme. The underlying
    basic principle Pregel follows is the BSP model. Every node goes through an iteration
    by aggregating and combining the desired features and subsequently updating the
    state. When the node has finished the computation step, it waits for all other
    nodes to finish before continuing with the next iteration. This assures that every
    node shares the same parameters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在迭代执行图处理算法时，计算步骤可以按照同步、异步或混合方案进行。Pregel 所遵循的基本原则是 BSP 模型。每个节点通过聚合和组合所需的特征来进行一次迭代，并随后更新状态。当节点完成计算步骤后，它会等待所有其他节点完成，然后再继续进行下一次迭代。这确保了每个节点共享相同的参数。
- en: While the synchronous mode works well in a lot of cases, it can be inefficient
    in other cases. An example is the label propagation algorithm for community detection
    (Raghavan et al., [2007](#bib.bib142)). Here, each vertex is assigned an initial
    label. In each iteration, the vertex adopts the label the maximum of neighbors
    has. After some propagation rounds through the network, dense communities consent
    to the same label. If the synchronous mode is chosen and the graph is bipartite,
    meaning each vertex of one subgraph connects to each vertex in another subgraph,
    the labels might oscillate and change after each iteration. This makes it impossible
    for the algorithm to terminate, as the labels therefore need to be stable. To
    solve this issue, asynchronous execution is used. Asynchronous processing means
    that vertices can already read state updates of the current iteration in addition
    to the updates of the previous iterations. In GNN training, the asynchronous mode
    has the ability to prioritize specific state updates which results in faster convergence
    of the overall computation. Another important benefit is the avoidance of long
    idle times through stragglers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管同步模式在许多情况下表现良好，但在其他情况下可能效率较低。一个例子是用于社区检测的标签传播算法（Raghavan 等，[2007](#bib.bib142)）。在这里，每个顶点被分配一个初始标签。在每次迭代中，顶点采用邻居中最大标签的标签。经过几轮传播后，密集的社区会达成相同的标签。如果选择同步模式，并且图是二分图，意味着一个子图中的每个顶点都连接到另一个子图中的每个顶点，那么标签可能会在每次迭代后发生振荡和变化。这使得算法无法终止，因为标签需要稳定。为了解决这个问题，使用了异步执行。异步处理意味着顶点可以在读取当前迭代的状态更新的同时，也读取前几次迭代的更新。在
    GNN 训练中，异步模式可以优先考虑特定的状态更新，从而使整体计算的收敛速度更快。另一个重要的好处是通过避免由于落后节点造成的长时间空闲。
- en: 'GraphLab (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117)) uses an
    asynchronous execution method. Three steps are performed independently by each
    worker: fold, merge and apply. In the fold step, the data across all vertices
    is gathered. If provided, a merge operation is performed. Otherwise, the apply
    function directly completes the computation and stores the data in shared memory.
    This process is done without regarding the phase of computation the other workers
    are currently passing. GRACE (Wang et al., [2013](#bib.bib178)) allows for high
    performance execution by adapting the BSP model to permit asynchronous processing.
    Hence, idle times are minimized, but it is possible that the workers perform their
    iteration with stale data. As both synchronization methods have their benefits
    and drawbacks, PowerGraph (Gonzalez et al., [2012](#bib.bib59)) allows the user
    to either choose a synchronous or asynchronous mode. The synchronous mode is performed
    analogously to the synchronous execution in Pregel, the asynchronous mode resembles
    the GraphLab computation model.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: GraphLab（Low，[2013](#bib.bib115)；Low 等，[2014](#bib.bib117)）使用异步执行方法。每个工作节点独立执行三个步骤：折叠、合并和应用。在折叠步骤中，所有顶点的数据会被收集。如果提供了合并操作，就会进行合并操作。否则，应用函数会直接完成计算并将数据存储在共享内存中。这个过程不会考虑其他工作节点当前处于计算的哪个阶段。GRACE（Wang
    等，[2013](#bib.bib178)）通过调整 BSP 模型来允许异步处理，从而实现高性能执行。因此，空闲时间被最小化，但可能会导致工作节点在处理过时数据时进行迭代。由于同步方法各有优缺点，PowerGraph（Gonzalez
    等，[2012](#bib.bib59)）允许用户选择同步或异步模式。同步模式的执行方式类似于 Pregel 的同步执行，而异步模式则类似于 GraphLab
    的计算模型。
- en: Usually, choosing a mode manually requires the user to deeply understand the
    graph processing system and often does not lead to the optimal performance. Hysync
    (Xie et al., [2015](#bib.bib191)) removes this issue by automatically switching
    between the synchronous and asynchronous mode according to a set of heuristics.
    The heuristics aim to predict the performance of the current mode and determine
    the step at which a switch to the other mode can be beneficial. In the Adaptive
    Asynchronous Parallel (AAP) model (Fan et al., [2020](#bib.bib37)), each worker
    decides on its own when to start the next computation step depending on two parameters.
    The first parameter is the relative progress of a worker compared to the other
    ones. The second parameter indicates data staleness. Consequently, stragglers
    are avoided while also reducing stale computations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，手动选择模式需要用户深入了解图处理系统，并且往往无法实现**最佳**性能。Hysync (Xie et al., [2015](#bib.bib191))
    通过根据一组启发式方法自动在同步模式和异步模式之间切换来解决这个问题。启发式方法旨在预测当前模式的性能，并确定何时切换到另一种模式会更有利。在自适应异步并行（AAP）模型中
    (Fan et al., [2020](#bib.bib37))，每个工作节点根据两个参数自行决定何时开始下一计算步骤。第一个参数是该工作节点与其他节点相比的相对进展。第二个参数指示数据的过时程度。因此，可以避免滞后节点，同时减少过时的计算。
- en: PowerLyra (Chen et al., [2015b](#bib.bib22)) goes a step further and not only
    provides both synchronization modes, but also distinguishes between high- and
    low-degree vertices to determine how they are processed. The former are processed
    based on the GAS model (i.e. gather, apply, scatter) (Gonzalez et al., [2012](#bib.bib59)).
    The master vertex activates the mirrors to execute the gather function and the
    results are sent back to the master. After having received all messages, the master
    runs the apply function. A combined message with the updated data and the activation
    for the scatter function is sent from master to mirror. In contrast to the original
    GAS model, PowerLyra combines the apply and scatter messages from master to mirror
    vertices to minimize communication. The system handles the low-degree vertices
    similar to GraphLab. The master vertex performs the gather and apply phase locally.
    Hereafter, activation and update messages are combined and sent to the mirrored
    vertices. Each mirror then performs the scatter phase. Because of the adapted
    scheme, replication of edges is eliminated and in each iteration, only one message
    needs to be sent by a mirror.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: PowerLyra (Chen et al., [2015b](#bib.bib22)) 更进一步，不仅提供了两种同步模式，还区分了高维度和低维度顶点以确定它们的处理方式。前者基于GAS模型（即收集、应用、分发）(Gonzalez
    et al., [2012](#bib.bib59)) 进行处理。主节点激活镜像节点以执行收集函数，然后将结果发送回主节点。在接收到所有消息后，主节点运行应用函数。一个包含更新数据和分发函数激活的组合消息从主节点发送到镜像节点。与原始GAS模型不同，PowerLyra结合了从主节点到镜像节点的应用和分发消息，以最小化通信。系统处理低度顶点的方式类似于GraphLab。主节点在本地执行收集和应用阶段。之后，激活和更新消息被组合并发送到镜像节点。每个镜像节点然后执行分发阶段。由于调整后的方案，边的复制被消除，在每次迭代中，每个镜像节点只需要发送一个消息。
- en: Opposed to the synchronous mode, using an asynchronous execution implies the
    need of a scheduling scheme which can influence the convergence of the overall
    computation. Besides, a high level of concurrent execution can be achieved as
    scheduling helps to determine the order of the tasks and assigns the tasks to
    the machines. GraphLab (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117))
    provides the so-called set scheduler. Based on the dependencies between the tasks,
    an execution plan is established and an overall speed-up of the computations can
    be observed. In their GRACE (Wang et al., [2013](#bib.bib178)) system, Wang et
    al. incorporate a customizable scheduling policy. The system loosens the restrictions
    of the BSP model and lets the user prioritize specific vertices for faster convergence.
    One can choose an individual set of vertices and also the desired processing order
    of those. Then, the system calculates a scheduling priority for each vertex to
    determine the overall execution order.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与同步模式相对，使用异步执行意味着需要一个调度方案，这可能会影响整体计算的收敛性。此外，调度可以帮助确定任务的顺序并将任务分配到机器上，从而实现高水平的并发执行。GraphLab（Low,
    [2013](#bib.bib115); Low et al., [2014](#bib.bib117)）提供了所谓的集合调度器。根据任务之间的依赖关系，建立执行计划，并可以观察到计算的总体加速。在他们的GRACE（Wang
    et al., [2013](#bib.bib178)）系统中，Wang等人引入了可定制的调度策略。该系统放宽了BSP模型的限制，并允许用户优先考虑特定的顶点以加快收敛速度。用户可以选择一组单独的顶点以及这些顶点的期望处理顺序。然后，系统计算每个顶点的调度优先级，以确定整体执行顺序。
- en: 2.2.5\. Message Propagation
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5. 消息传播
- en: 'Regardless of the execution mode and scheduling technique, messages need to
    be transmitted between the vertices containing the updated states. There are two
    main methods to transfer messages: push and pull. While Pregel-based systems (Malewicz
    et al., [2010](#bib.bib120); FOUNDATION, [2012](#bib.bib45); Salihoglu and Widom,
    [2013](#bib.bib153); Wang et al., [2013](#bib.bib178)) use the push-operation,
    systems supporting the GAS model (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117),
    [2012](#bib.bib116); Gonzalez et al., [2012](#bib.bib59); Chen et al., [2015b](#bib.bib22))
    rely on pulling the messages. After each iteration, Pregel-based systems synchronously
    propagate the update messages. All vertices simultaneously push their messages,
    meaning each vertex directly sends a message to all its adjacent vertices containing
    the updated state. GraphLab (Low, [2013](#bib.bib115); Low et al., [2014](#bib.bib117)),
    on the other hand, stores the data graph with associated features in shared memory.
    Like this, all workers are able to access the data any time. At the beginning
    of an iteration, the worker pulls the current data graph via the gather operation
    to perform calculations on the most recent features. At the end of an iteration,
    the worker updates the data graph with the newly computed state.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 无论执行模式和调度技术如何，消息都需要在包含更新状态的顶点之间传递。消息传递的主要方法有两种：推送和拉取。虽然基于Pregel的系统（Malewicz
    et al., [2010](#bib.bib120); FOUNDATION, [2012](#bib.bib45); Salihoglu and Widom,
    [2013](#bib.bib153); Wang et al., [2013](#bib.bib178)）使用推送操作，但支持GAS模型的系统（Low,
    [2013](#bib.bib115); Low et al., [2014](#bib.bib117), [2012](#bib.bib116); Gonzalez
    et al., [2012](#bib.bib59); Chen et al., [2015b](#bib.bib22)）依赖于拉取消息。在每次迭代后，基于Pregel的系统同步传播更新消息。所有顶点同时推送消息，这意味着每个顶点直接将消息发送到所有包含更新状态的相邻顶点。另一方面，GraphLab（Low,
    [2013](#bib.bib115); Low et al., [2014](#bib.bib117)）将带有相关特征的数据图存储在共享内存中。这样，所有工作者随时可以访问数据。在迭代开始时，工作者通过聚集操作拉取当前数据图，以在最新特征上执行计算。在迭代结束时，工作者使用新计算的状态更新数据图。
- en: 2.3\. Distributed Neural Network Training
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 分布式神经网络训练
- en: A neural network consists of a number of connected nodes, called neurons, organized
    in one or multiple layers. Each neuron takes an input and processes it in regard
    to given weights and an update function. When having computed the new value, an
    activation function (Sharma et al., [2017](#bib.bib161); Ramachandran et al.,
    [2017](#bib.bib144)) decides how important the output value is. The values are
    passed through the network until the last set of neurons is reached, called forward
    pass. After that, a loss function is computed in regard to the calculated and
    expected output. In order to minimize the loss, the weights need to be adapted.
    The backpropagation (Rumelhart et al., [1995](#bib.bib150)) algorithm is used
    to adjust weights in a backward pass through the network. A widely used technique
    to do so is stochastic gradient descent (SGD) (Robbins and Monro, [1951](#bib.bib146);
    Ruder, [2016](#bib.bib149)). It computes the partial gradients by considering
    the calculated loss. With the help of those gradients, the weights are adjusted.
    This is called backward pass. The whole process, forward and backward pass, is
    iteratively repeated until convergence or a maximum number of iterations is reached.
    The final neural network weights can be used to make predictions on unseen data
    (Widrow and Lehr, [1990](#bib.bib187); Lawrence, [1993](#bib.bib103); Goodfellow
    et al., [2016](#bib.bib60)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由多个互联的节点组成，称为神经元，排列在一个或多个层中。每个神经元接收输入并根据给定的权重和更新函数处理这些输入。在计算出新值后，激活函数（Sharma
    等，[2017](#bib.bib161)；Ramachandran 等，[2017](#bib.bib144)）决定输出值的重要性。这些值在网络中传递，直到达到最后一组神经元，称为前向传播。之后，计算损失函数以对比计算出的输出和期望输出。为了最小化损失，需要调整权重。反向传播（Rumelhart
    等，[1995](#bib.bib150)）算法用于在网络中反向调整权重。一种广泛使用的技术是随机梯度下降（SGD）（Robbins 和 Monro，[1951](#bib.bib146)；Ruder，[2016](#bib.bib149)）。它通过考虑计算的损失来计算部分梯度。借助这些梯度，调整权重，这称为反向传播。整个过程，包括前向和反向传播，会迭代进行，直到收敛或达到最大迭代次数。最终的神经网络权重可用于对未见过的数据进行预测（Widrow
    和 Lehr，[1990](#bib.bib187)；Lawrence，[1993](#bib.bib103)；Goodfellow 等，[2016](#bib.bib60)）。
- en: 'With the growing amount of training data and the increase of model size, distributed
    neural network training has become necessary. In the following, we discuss different
    techniques to scale neural network training with regard to parallelism, execution
    mode and coordination. Table [2](#S2.T2 "Table 2 ‣ 2.3\. Distributed Neural Network
    Training ‣ 2\. Foundations ‣ The Evolution of Distributed Systems for Graph Neural
    Networks and their Origin in Graph Processing and Deep Learning: A Survey") gives
    an overview of the described systems and techniques.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '随着训练数据量的增加和模型规模的扩大，分布式神经网络训练变得越来越必要。接下来，我们讨论了在并行性、执行模式和协调方面扩展神经网络训练的不同技术。表[2](#S2.T2
    "Table 2 ‣ 2.3\. Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")概述了所描述的系统和技术。'
- en: Table 2. Categorization of systems for distributed neural network training
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. 分布式神经网络训练系统的分类
- en: '|  |  | Parallelism | Synchronization Mode | Coordination |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 并行性 | 同步模式 | 协调 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Year | System |  | data | model | synchronous | asynchronous | centralized
    | decentralized |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 系统 |  | 数据 | 模型 | 同步 | 异步 | 集中式 | 去中心化 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2012 | DistBelief | (Dean et al., [2012](#bib.bib30)) | ✓ |  |  | ✓ | ✓ |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | DistBelief | (Dean 等，[2012](#bib.bib30)) | ✓ |  |  | ✓ | ✓ |  |'
- en: '| 2014 | Project Adam | (Chilimbi et al., [2014](#bib.bib26)) |  | ✓ |  | ✓
    | ✓ |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | Project Adam | (Chilimbi 等，[2014](#bib.bib26)) |  | ✓ |  | ✓ | ✓ |  |'
- en: '| 2015 | MALT | (Li et al., [2015a](#bib.bib108)) | ✓ |  |  | ✓ |  | ✓ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | MALT | (Li 等，[2015a](#bib.bib108)) | ✓ |  |  | ✓ |  | ✓ |'
- en: '| 2016 | Tensorflow | (Abadi et al., [2016](#bib.bib3)) | ✓ | ✓ | ✓ | ✓ | ✓
    |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Tensorflow | (Abadi 等，[2016](#bib.bib3)) | ✓ | ✓ | ✓ | ✓ | ✓ |  |'
- en: '| 2016 | Ako | (Watcharapichat et al., [2016](#bib.bib185)) | ✓ |  |  | ✓ |  |
    ✓ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Ako | (Watcharapichat 等，[2016](#bib.bib185)) | ✓ |  |  | ✓ |  | ✓
    |'
- en: '| 2019 | CROSSBOW | (Koliousis et al., [2019](#bib.bib100)) | ✓ |  | ✓ |  |  |
    ✓ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | CROSSBOW | (Koliousis 等，[2019](#bib.bib100)) | ✓ |  | ✓ |  |  | ✓
    |'
- en: '| 2019 | PyTorch | (Paszke et al., [2019](#bib.bib139)) | ✓ | ✓ | ✓ | ✓ | ✓
    | ✓ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | PyTorch | (Paszke 等，[2019](#bib.bib139)) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: 2.3.1\. Parallelism
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1\. 并行性
- en: Data parallelism allows for parallel training on multiple processors. Therefore,
    the data is divided into a fixed number of subsets. Each worker is assigned a
    subset which it processes on a local replica of the model. After that, the resulting
    model parameters are exchanged with the other workers. The next iteration is performed
    with the updated parameter configuration. This process is repeated until convergence.
    Systems using data parallelism are for instance MALT (Li et al., [2015a](#bib.bib108))
    and Ako (Watcharapichat et al., [2016](#bib.bib185)). If the model itself is too
    big to fit on a single machine (Krizhevsky et al., [2012](#bib.bib101); Brown
    et al., [2020](#bib.bib13)), it is split up. In model parallelism, the computation
    nodes process the whole data set on their partition of the model. After having
    finished the computation, the intermediate output of the forward pass is passed
    to the machines responsible for computing the subsequent layer. Here, scheduling
    is important to efficiently coordinate the training process. The most intuitive
    way to partition the model is layer-wise, meaning each layer is assigned to one
    node. However, this sometimes does not benefit parallelism as the worker controlling
    the current layer needs to wait for the worker handling the prior layer to finish
    before being able to run their computation. A system exploiting model parallelism
    is Project Adam (Chilimbi et al., [2014](#bib.bib26)) where each machine is assigned
    a certain part of the model. Another possibility to distribute the model more
    intelligently is according to its specific architecture. Suitable could be architectures
    like the two-fold Siamese network (He et al., [2018](#bib.bib70)) where some of
    the components can be easily run in parallel.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行允许在多个处理器上进行并行训练。因此，数据被划分为固定数量的子集。每个工作节点被分配一个子集，并在模型的本地副本上进行处理。之后，将得到的模型参数与其他工作节点交换。下一次迭代使用更新后的参数配置进行。这一过程重复进行，直到收敛。例如，使用数据并行的系统有MALT（Li
    et al., [2015a](#bib.bib108)）和Ako（Watcharapichat et al., [2016](#bib.bib185)）。如果模型本身太大，无法放在单台机器上（Krizhevsky
    et al., [2012](#bib.bib101); Brown et al., [2020](#bib.bib13)），则会将其拆分。在模型并行中，计算节点处理模型的各自分区上的整个数据集。计算完成后，前向传递的中间输出被传递给负责计算后续层的机器。在这里，调度对于高效协调训练过程非常重要。将模型分层划分是一种最直观的方式，即每层分配给一个节点。然而，这有时并不会有利于并行性，因为控制当前层的工作节点需要等待处理前一层的工作节点完成后才能进行计算。一个利用模型并行的系统是Project
    Adam（Chilimbi et al., [2014](#bib.bib26)），其中每台机器被分配模型的某个部分。另一种更智能地分配模型的方法是根据其特定架构进行分配。例如，适合的架构可以是双胞胎Siamese网络（He
    et al., [2018](#bib.bib70)），其中一些组件可以轻松地并行运行。
- en: The two techniques are combined, e.g., by PipeDream (Narayanan et al., [2019](#bib.bib134))
    and GPipe (Huang et al., [2019](#bib.bib82)), into the so-called hybrid or pipeline
    parallelism. Here, the data as well as the model are shared among the workers.
    In contrast to PipeDream, the GPipe algorithm further splits the input mini-batches
    into chunks to maximize the number of concurrent calculations within an iteration.
    In general, the use of hybrid parallelism can significantly improve training speed
    in comparison to data and model parallelism.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术被结合在一起，例如，PipeDream（Narayanan et al., [2019](#bib.bib134)）和GPipe（Huang
    et al., [2019](#bib.bib82)），形成了所谓的混合或管道并行。在这里，数据以及模型在工作节点之间共享。与PipeDream相比，GPipe算法进一步将输入的小批量数据分割成块，以最大化每次迭代中的并发计算数量。总体来说，与数据和模型并行相比，使用混合并行可以显著提高训练速度。
- en: 2.3.2\. Synchronization Mode
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. 同步模式
- en: When training neural networks in a distributed fashion, it is important that
    the machines regularly exchange parameter updates or intermediate results to ensure
    convergence. This can either follow a synchronous, asynchronous or hybrid mode
    (Dean et al., [2012](#bib.bib30); Chen et al., [2016](#bib.bib21); Jin et al.,
    [2016](#bib.bib90)). When using a synchronous mode, the updates are sent simultaneously
    to the other nodes as soon as all machines have finished their computation. Then,
    the nodes continue with the computation of the next iteration. In this manner,
    every machine is always aware of the current parameters. A downside of this approach
    is that a single straggler decreases the speed of the whole training process (Cipar
    et al., [2013](#bib.bib27)). A system following the synchronous execution mode
    is CROSSBOW (Koliousis et al., [2019](#bib.bib100)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式方式训练神经网络时，重要的是机器要定期交换参数更新或中间结果，以确保收敛。这可以采用同步、异步或混合模式（Dean et al., [2012](#bib.bib30);
    Chen et al., [2016](#bib.bib21); Jin et al., [2016](#bib.bib90)）。使用同步模式时，更新会在所有机器完成计算后同时发送到其他节点。然后，节点继续进行下一次迭代的计算。这样，每台机器总是能了解当前的参数。该方法的一个缺点是，如果有一个节点处理缓慢，会降低整个训练过程的速度（Cipar
    et al., [2013](#bib.bib27)）。遵循同步执行模式的系统有CROSSBOW（Koliousis et al., [2019](#bib.bib100)）。
- en: Models using asynchronous execution (Dean et al., [2012](#bib.bib30); Chilimbi
    et al., [2014](#bib.bib26); Li et al., [2015a](#bib.bib108); Watcharapichat et al.,
    [2016](#bib.bib185)) eliminate the problem of stragglers by not waiting for all
    workers to finish. Instead, the updates are sent as soon as they are available.
    Thus, the training speed is increased and the resources are efficiently used.
    A drawback of this method is that the worker nodes might not always be up to date,
    therefore computing their updates on stale parameters. The computation of gradients
    on outdated parameters can lead to a slower or no convergence at all which is
    called stale gradients problem (Dutta et al., [2018](#bib.bib35)). An attempt
    to merge the synchronous and asynchronous execution scheme was made by Ho et al.
    (Ho et al., [2013](#bib.bib73)). Their model is based on synchronous execution,
    but incorporates a staleness threshold determining how many time steps two workers
    may be apart until the faster worker needs to wait for the slower worker to finish
    its current computation. In contrast to synchronous execution, this decreases
    the impact of stragglers while also limiting the staleness of the parameters to
    ensure up-to-date computation. Popular frameworks like TensorFlow (Abadi et al.,
    [2016](#bib.bib3)) and PyTorch (Paszke et al., [2019](#bib.bib139)) leave the
    choice of synchronization level to the user. Here, both methods are supported.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用异步执行的模型（Dean et al., [2012](#bib.bib30); Chilimbi et al., [2014](#bib.bib26);
    Li et al., [2015a](#bib.bib108); Watcharapichat et al., [2016](#bib.bib185)）通过不等待所有工作节点完成来消除延迟节点的问题。相反，更新会在它们可用时立即发送。因此，训练速度会提高，资源使用也更加高效。这种方法的一个缺点是，工作节点可能不会始终保持最新，从而在过时的参数上计算更新。基于过时参数计算梯度可能导致较慢甚至完全不收敛，这称为过时梯度问题（Dutta
    et al., [2018](#bib.bib35)）。Ho et al.（Ho et al., [2013](#bib.bib73)）尝试将同步和异步执行方案结合起来。他们的模型基于同步执行，但加入了一个陈旧阈值，以确定两个工作节点可以相隔多少时间步骤，直到较快的工作节点需要等待较慢的工作节点完成当前计算。与同步执行相比，这减少了延迟节点的影响，同时也限制了参数的陈旧性，以确保计算的最新性。像
    TensorFlow（Abadi et al., [2016](#bib.bib3)）和 PyTorch（Paszke et al., [2019](#bib.bib139)）这样的流行框架允许用户选择同步级别。这两种方法都得到了支持。
- en: 2.3.3\. Coordination
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3\. 协调
- en: Besides determining the synchronization mode, it needs to be decided how the
    parameters are stored and the updates are coordinated. Common ways to do so are
    in a centralized or decentralized manner. The centralized method operates with
    a global parameter server which stores, aggregates and distributes the relevant
    updates. Consequently, all machines share the same set of parameters. However,
    the use of a parameter server introduces a single-point of failure as it is part
    of all update requests. DistBelief (Dean et al., [2012](#bib.bib30)) and Project
    Adam (Chilimbi et al., [2014](#bib.bib26)) are systems operating in a centralized
    fashion. Decentralized systems (Li et al., [2015a](#bib.bib108); Watcharapichat
    et al., [2016](#bib.bib185); Koliousis et al., [2019](#bib.bib100)) eliminate
    the need for a parameter server by passing the update messages directly from machine
    to machine by using a collective communication primitive such as an all-reduce
    operation (Sanders et al., [2019](#bib.bib154)). Here, each machine exchanges
    update information with its peers and combines the received parameters with its
    own. As a result, each machine holds the latest set of parameters. An advantage
    of this approach is that the computation of updates is balanced among all machines
    (Koliousis et al., [2019](#bib.bib100)). As both strategies have their benefits
    and drawbacks, PyTorch (Paszke et al., [2019](#bib.bib139)) leaves the choice
    of coordination to the user.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了确定同步模式外，还需要决定如何存储参数和协调更新。常见的方法有集中式或分散式。集中式方法使用一个全球参数服务器，该服务器存储、聚合并分发相关更新。因此，所有机器共享相同的参数集。然而，使用参数服务器引入了单点故障，因为它是所有更新请求的一部分。DistBelief
    (Dean et al., [2012](#bib.bib30)) 和 Project Adam (Chilimbi et al., [2014](#bib.bib26))
    是以集中式方式运作的系统。分散式系统 (Li et al., [2015a](#bib.bib108); Watcharapichat et al., [2016](#bib.bib185);
    Koliousis et al., [2019](#bib.bib100)) 通过使用如全减操作 (Sanders et al., [2019](#bib.bib154))
    这样的集体通信原语，直接从机器到机器传递更新信息，从而消除了对参数服务器的需求。在这里，每台机器与其同伴交换更新信息，并将接收到的参数与自身的参数结合。结果是，每台机器都保持最新的参数集。这种方法的一个优点是更新的计算在所有机器之间是平衡的
    (Koliousis et al., [2019](#bib.bib100))。由于这两种策略各有优缺点，PyTorch (Paszke et al., [2019](#bib.bib139))
    将协调的选择留给用户。
- en: To conclude, there are several ways to perform distributed DNN training. Depending
    on the architecture and the data, one might choose between data, model or hybrid
    parallelism, synchronous or asynchronous updates and a centralized or decentralized
    system. In all methods, parameters need to be exchanged between the machines.
    Therefore, communication is a bottleneck that needs to be addressed when training
    neural networks in a multi-machine setting. In addition, it is important that
    the resources are fully utilized without long idle times (Zhang et al., [2020a](#bib.bib209)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，进行分布式深度神经网络（DNN）训练有几种方法。根据架构和数据，可以选择数据并行、模型并行或混合并行、同步更新或异步更新，以及集中式或分散式系统。在所有方法中，参数需要在机器之间交换。因此，通信是训练神经网络时需要解决的瓶颈。此外，重要的是确保资源得到充分利用，而不会出现长时间的空闲（Zhang
    et al., [2020a](#bib.bib209)）。
- en: 3\. Systems for Graph Neural Networks
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 图神经网络的系统
- en: 3.1\. Graph Neural Network Basics
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 图神经网络基础
- en: The term graph neural network (GNN) initially emerged in a work by Gori et al.
    (Gori et al., [2005](#bib.bib61)) and was further investigated by Scarselli et
    al. (Scarselli et al., [2008](#bib.bib156)). It refers to neural network architectures
    that do not take multiple independent or sequenced data samples as input like
    CNNs (LeCun et al., [1995](#bib.bib105)) or RNNs (Sherstinsky, [2020](#bib.bib162)),
    but rather graphs. In contrast to images or text, graphs do not follow a specific
    structure and are not sequentially ordered. A graph $G$ can be formally denoted
    as $G=(V,E)$. It consists of a set of vertices $V$ and a set of edges $E$. A vertex
    $v$ represents an object and is also known as node. An edge $e$ describes the
    relation between two vertices. As graphs are unstructured, it is necessary to
    employ a new type of neural network, the GNN. They combine DNN operations like
    matrix multiplication and convolution with methods known from graph processing
    like iterative message propagation (Jia et al., [2020a](#bib.bib88); Wang et al.,
    [2021c](#bib.bib179)). Due to uniting DNN operations and message passing, GNNs
    are sometimes also denoted as Message Passing Neural Networks (Gilmer et al.,
    [2017](#bib.bib57); Riba et al., [2018](#bib.bib145); Zhang et al., [2020d](#bib.bib206);
    Hamilton, [2020](#bib.bib66)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNN）这一术语最初出现在Gori等人的研究中（Gori et al., [2005](#bib.bib61)），并由Scarselli等人进一步研究（Scarselli
    et al., [2008](#bib.bib156)）。它指的是一种不以多个独立或按序排列的数据样本作为输入的神经网络架构，如CNN（LeCun et al.,
    [1995](#bib.bib105)）或RNN（Sherstinsky, [2020](#bib.bib162)），而是图。与图像或文本不同，图没有特定结构，也不是按序排列的。一个图$G$可以形式上表示为$G=(V,E)$。它由一组顶点$V$和一组边$E$组成。一个顶点$v$代表一个对象，也称为节点。一条边$e$描述了两个顶点之间的关系。由于图是非结构化的，需要使用一种新的神经网络类型，即GNN。它们结合了矩阵乘法和卷积等DNN操作，以及从图处理方法中获得的迭代消息传递（Jia
    et al., [2020a](#bib.bib88); Wang et al., [2021c](#bib.bib179)）。由于结合了DNN操作和消息传递，GNN有时也被称为消息传递神经网络（Gilmer
    et al., [2017](#bib.bib57); Riba et al., [2018](#bib.bib145); Zhang et al., [2020d](#bib.bib206);
    Hamilton, [2020](#bib.bib66)）。
- en: To begin with, each vertex of an input graph is initially represented by a feature
    vector, called activation. This initial activation only incorporates information
    about the vertex itself but not about the context within the graph. At each layer,
    a set of DNN operations and message passing steps are performed vertex-wise to
    update the activation values. In the first step of an iteration, each vertex aggregates
    the activations of its adjacent vertices by exchanging messages according to an
    aggregation function
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，输入图的每个顶点最初由一个特征向量表示，称为激活值。这个初始激活值只包含关于顶点本身的信息，而不涉及图中的上下文。在每一层，进行一组DNN操作和消息传递步骤，以顶点为单位更新激活值。在迭代的第一步中，每个顶点通过根据聚合函数交换消息来汇总其邻接顶点的激活值。
- en: '| (1) |  | $a_{v}^{(k)}=AGGREGATE^{(k)}(\{h_{u}^{(k-1)}&#124;u\in N(v)\})$
    |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $a_{v}^{(k)}=AGGREGATE^{(k)}(\{h_{u}^{(k-1)}&#124;u\in N(v)\})$
    |  |'
- en: where $a_{v}^{(k)}$ denotes the aggregated values of vertex $v$ at the $k$-th
    layer. The term $h_{u}^{(k-1)}|u\in N(v)$ describes the activations of the neighboring
    vertices at the previous layer with $N(v)$ denoting the neighbors of $v$ in the
    given graph. Next, the gathered information is combined and the current value
    of the vertex is updated with an update function. The update function $h_{v}{(k)}$
    can include standard DNN operations like matrix multiplication and is defined
    by
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$a_{v}^{(k)}$表示第$k$层顶点$v$的聚合值。术语$h_{u}^{(k-1)}|u\in N(v)$描述了在前一层中邻接顶点的激活值，$N(v)$表示给定图中顶点$v$的邻居。接下来，将收集到的信息进行组合，并通过更新函数更新顶点的当前值。更新函数$h_{v}{(k)}$可以包括标准的DNN操作，如矩阵乘法，定义如下
- en: '| (2) |  | $h_{v}^{(k)}=UPDATE^{(k)}(a_{v}^{(k)},h_{v}^{(k-1)})$ |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $h_{v}^{(k)}=UPDATE^{(k)}(a_{v}^{(k)},h_{v}^{(k-1)})$ |  |'
- en: where $h_{v}^{(k)}$ is the new activation of vertex $v$ at the $k$-th layer.
    To obtain the updated activation, the aggregated activations $a_{v}^{(k)}$ are
    combined with the vertices’ activation of the previous layer $h_{v}^{(k-1)}$.
    A special case occurs if $k=1$, then the initial activations $h_{v}{(0)}$ or $h_{u}{(0)}|u\in
    N(V)$ are needed, respectively (Jia et al., [2020a](#bib.bib88); Hamilton, [2020](#bib.bib66)).
    The new values now serve as starting point for the next layer where the activations
    are aggregated and combined again. This process is repeated iteratively. Consequently,
    more and more vertices are explored. After $k$ layers, the $k$-hop neighborhood
    of a vertex is captured. When the final layer has been passed, the final representation
    of a vertex includes information about the vertex itself as well as about the
    vertices within the graph context.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{v}^{(k)}$ 是第 $k$ 层中顶点 $v$ 的新激活值。为了获得更新后的激活值，聚合的激活值 $a_{v}^{(k)}$ 与前一层的顶点激活值
    $h_{v}^{(k-1)}$ 结合。如果 $k=1$，则需要初始激活值 $h_{v}{(0)}$ 或 $h_{u}{(0)}|u\in N(V)$（Jia
    等，[2020a](#bib.bib88)；Hamilton，[2020](#bib.bib66)）。新的值现在作为下一层的起始点，在这一层中，激活值被重新聚合和组合。这个过程会迭代进行。因此，探索的顶点越来越多。在经过
    $k$ 层后，一个顶点的 $k$ 跳邻域被捕获。当最后一层通过后，顶点的最终表示包括了有关顶点自身以及图中其他顶点的信息。
- en: '![Refer to caption](img/1eaf5e3f926665c3221cd2f0b13c390d.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1eaf5e3f926665c3221cd2f0b13c390d.png)'
- en: Figure 2. Schematic of the GNN training process
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. GNN 训练过程示意图
- en: 'In Figure [2](#S3.F2 "Figure 2 ‣ 3.1\. Graph Neural Network Basics ‣ 3\. Systems
    for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural
    Networks and their Origin in Graph Processing and Deep Learning: A Survey"), an
    overview of a complete forward pass is given. It consists of the above described
    steps: (1) fetch the initial weights, (2) pass and aggregate messages from neighboring
    nodes, (3) perform DNN operations and (4) update the weights according to a given
    function. The steps (2) to (4) are repeatedly executed. After $n$ iterations,
    a final model configuration (5) is obtained.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [2](#S3.F2 "图 2 ‣ 3.1\. 图神经网络基础 ‣ 3\. 图神经网络系统 ‣ 图神经网络分布式系统的演变及其在图处理和深度学习中的起源：综述")
    中，给出了完整的前向传播的概述。它包括上述步骤：（1）获取初始权重，（2）传递和聚合来自邻近节点的消息，（3）执行 DNN 操作，（4）根据给定的函数更新权重。步骤（2）到（4）会被重复执行。在
    $n$ 次迭代后，得到最终的模型配置（5）。
- en: 'Analogous to the general neural network training described in Section [2.3](#S2.SS3
    "2.3\. Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution of
    Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey"), a loss function is computed relative to the output
    of the forward pass. Backpropagation is applied during the backward pass in order
    to adapt the weights of the network (Xu et al., [2018a](#bib.bib193)). After multiple
    forward and backward passes, the model can make vertex- and edge-level predictions.
    In order to make assertions about the whole graph, a pooling layer needs to be
    added which aggregates and combines all states and labels contained in the output
    graph based on a specified pooling operation (Zhou et al., [2020](#bib.bib216)).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于第 [2.3](#S2.SS3 "2.3\. 分布式神经网络训练 ‣ 2\. 基础 ‣ 图神经网络分布式系统的演变及其在图处理和深度学习中的起源：综述")
    节中描述的一般神经网络训练，将计算相对于前向传播输出的损失函数。在反向传播过程中应用反向传播算法，以调整网络的权重（Xu 等，[2018a](#bib.bib193)）。经过多次前向和反向传播后，模型可以进行顶点和边级别的预测。为了对整个图进行断言，需要添加一个池化层，该层根据指定的池化操作聚合并组合输出图中的所有状态和标签（Zhou
    等，[2020](#bib.bib216)）。
- en: There exist several types of GNNs (Wu et al., [2020](#bib.bib190)), among the
    most famous ones are Gated Graph Neural Networks (GG-NN) (Li et al., [2015b](#bib.bib111)),
    Graph Convolution Networks (GCN) (Kipf and Welling, [2016a](#bib.bib98)), GraphSAGE
    (Hamilton et al., [2017b](#bib.bib68)), Graph Attention Networks (GAT) (Veličković
    et al., [2017](#bib.bib177)) and Graph Auto-Encoders (GAE) (Kipf and Welling,
    [2016b](#bib.bib99)). Architectural differences involve the message propagation
    process, sampling method, pooling operation as well as the composition of the
    layers. For instance, the GraphSAGE model uses a max-pooling strategy while GCNs
    use mean-pooling instead. GATs include masked self-attention in the pooling process
    and GG-NNs capture spatial and temporal changes throughout time by using gated
    recurrent units as update module. In general, if the relations between objects
    are essential to make predictions on the data, GNNs can be used and are often
    preferable over common DNN architectures (LeCun et al., [1995](#bib.bib105); Hochreiter
    and Schmidhuber, [1997](#bib.bib76)). In contrast to DNNs which read in the data
    object by object or in an ordered sequence, GNNs naturally capture the relations
    within a graph and are also able to predict the relations between data points.
    This cannot be easily done with other DNN models. Thus, GNNs benefit the processing
    of and prediction on graph data. Sometimes it can be advantageous to combine GNNs
    with other DNN models, for instance, when handling temporal graphs where nodes
    and edges are updated sequentially (Kumar et al., [2019](#bib.bib102); Rossi et al.,
    [2020](#bib.bib147); Zhang et al., [2021b](#bib.bib208)). Here, a GNN is combined
    with a Recurrent Neural Network. However, if there are no significant connections
    between the individual data points, there usually is no need to choose a GNN over
    another DNN model to perform DL.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 存在几种类型的 GNN（Wu et al., [2020](#bib.bib190)），其中最著名的包括 Gated Graph Neural Networks
    (GG-NN)（Li et al., [2015b](#bib.bib111)）、Graph Convolution Networks (GCN)（Kipf
    和 Welling, [2016a](#bib.bib98)）、GraphSAGE（Hamilton et al., [2017b](#bib.bib68)）、Graph
    Attention Networks (GAT)（Veličković et al., [2017](#bib.bib177)）和 Graph Auto-Encoders
    (GAE)（Kipf 和 Welling, [2016b](#bib.bib99)）。它们的架构差异涉及消息传播过程、采样方法、池化操作以及层的组成。例如，GraphSAGE
    模型使用最大池化策略，而 GCN 使用平均池化。GAT 在池化过程中包含了掩码自注意力，GG-NN 通过使用门控递归单元作为更新模块来捕捉时间上的空间和时间变化。一般而言，如果对象之间的关系对数据预测至关重要，则可以使用
    GNN，并且通常优于常见的 DNN 架构（LeCun et al., [1995](#bib.bib105); Hochreiter 和 Schmidhuber,
    [1997](#bib.bib76)）。与逐个读取数据对象或有序序列的 DNN 不同，GNN 自然捕捉图中的关系，并且能够预测数据点之间的关系。这是其他 DNN
    模型难以做到的。因此，GNN 有利于图数据的处理和预测。有时，将 GNN 与其他 DNN 模型结合起来可能是有利的，例如，在处理节点和边顺序更新的时间图时（Kumar
    et al., [2019](#bib.bib102); Rossi et al., [2020](#bib.bib147); Zhang et al.,
    [2021b](#bib.bib208)）。在这种情况下，GNN 与递归神经网络结合。然而，如果个别数据点之间没有显著的连接，则通常没有必要选择 GNN 而不是其他
    DNN 模型来执行深度学习。
- en: 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2. 分布式图神经网络训练方法的分类
- en: Training GNNs on large graphs is a challenging task which requires high communication,
    large memory capacity and high bandwidth (Md et al., [2021](#bib.bib131)). In
    contrast to general distributed DNN training, all data points within a graph are
    connected and not independent of each other. Due to this dependency, one can not
    simply split the data to process the batches in parallel. Furthermore, memory-intensive
    edge-centric operations in addition to arithmetic-intensive vertex-centric operations
    need to be regarded when optimizing GNNs (Kiningham et al., [2020b](#bib.bib97)).
    Thus, large-scale graph processing methods or efficient DNN training operations
    can not directly be used for GNN training. More specialized techniques adjusted
    to GNN characteristics are needed to overcome the described challenges and to
    speed up training and inference.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在大图上训练 GNN 是一项具有挑战性的任务，需要高通信、大内存容量和高带宽（Md et al., [2021](#bib.bib131)）。与一般的分布式
    DNN 训练不同，图中的所有数据点都是连接的，而不是相互独立的。由于这种依赖性，无法简单地拆分数据以并行处理批次。此外，在优化 GNN 时，需要考虑内存密集型的边缘中心操作以及算术密集型的顶点中心操作（Kiningham
    et al., [2020b](#bib.bib97)）。因此，大规模图处理方法或高效的 DNN 训练操作不能直接用于 GNN 训练。需要更多调整为 GNN
    特征的专门技术来克服这些挑战，并加快训练和推理速度。
- en: '![Refer to caption](img/5d7a58ba629aca972d356afbe7dac7f6.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5d7a58ba629aca972d356afbe7dac7f6.png)'
- en: Figure 3. General set up of a GNN system
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. GNN 系统的一般设置
- en: 'A general set up of GNN systems is shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.2\.
    Categorization of Methods for Distributed Graph Neural Network Training ‣ 3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey").
    Similar to the first step in distributed graph processing, the graph can initially
    be partitioned and distributed across the workers (①: Section [3.2.1](#S3.SS2.SSS1
    "3.2.1\. Partitioning ‣ 3.2\. Categorization of Methods for Distributed Graph
    Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")). Other types of parallelism (Section [3.2.5](#S3.SS2.SSS5
    "3.2.5\. Parallelism ‣ 3.2\. Categorization of Methods for Distributed Graph Neural
    Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey")) are also possible, however data parallelism is the most
    common choice. After that, questions of how to store the subgraphs and whether
    to cache any data need to be addressed (②: Section [3.2.4](#S3.SS2.SSS4 "3.2.4\.
    Inter-Process Communication ‣ 3.2\. Categorization of Methods for Distributed
    Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")). This can either be done locally on the machines
    or globally on a dedicated graph store. Depending on the training mode, a sampling
    step is performed (③: Section [3.2.2](#S3.SS2.SSS2 "3.2.2\. Sampling ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey")). Here, only
    a selection of vertices instead of the whole partition is used to train the model.
    The main training begins with a random initial representation of the vertices.
    Then, messages are pulled to or pushed from adjacent vertices (④: Section [3.2.6](#S3.SS2.SSS6
    "3.2.6\. Message Propagation ‣ 3.2\. Categorization of Methods for Distributed
    Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")), DNN operations are applied and the updated vertex
    states are shared with the other vertices before the next iteration starts. The
    whole process can be executed in a synchronous or asynchronous fashion (⑤: Section
    [3.2.7](#S3.SS2.SSS7 "3.2.7\. Synchronization Mode ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")) and various scheduling techniques
    may be applied (Section [3.2.8](#S3.SS2.SSS8 "3.2.8\. Scheduling ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey")). Instead of
    a decentral all-reduce operation to share the parameters, a centralized parameter
    server might be used (⑥: Section [3.2.9](#S3.SS2.SSS9 "3.2.9\. Coordination ‣
    3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey")). After having finished all iterations, predictions can be made based
    on the final set of parameters. Most systems additionally provide a programming
    abstraction adapted to the individual optimizations (Section [3.2.3](#S3.SS2.SSS3
    "3.2.3\. Programming Abstractions ‣ 3.2\. Categorization of Methods for Distributed
    Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")). In the following, we will present and categorize
    systems for GNNs based on the described steps for setting up such a system.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#S3.F3 "Figure 3 ‣ 3.2\. Categorization of Methods for Distributed Graph
    Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey") 展示了 GNN 系统的一般设置。与分布式图处理的第一步类似，图可以最初被划分并分布到工作节点上
    (①: Section [3.2.1](#S3.SS2.SSS1 "3.2.1\. Partitioning ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey"))。其他类型的并行性 (Section
    [3.2.5](#S3.SS2.SSS5 "3.2.5\. Parallelism ‣ 3.2\. Categorization of Methods for
    Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")) 也是可能的，但数据并行性是最常见的选择。之后，需要解决如何存储子图和是否缓存任何数据的问题
    (②: Section [3.2.4](#S3.SS2.SSS4 "3.2.4\. Inter-Process Communication ‣ 3.2\.
    Categorization of Methods for Distributed Graph Neural Network Training ‣ 3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey"))。这可以在本地机器上完成，也可以在专用图存储中完成。根据训练模式，执行一个采样步骤
    (③: Section [3.2.2](#S3.SS2.SSS2 "3.2.2\. Sampling ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey"))。在这里，只选择顶点的一个子集而不是整个分区来训练模型。主要训练从顶点的随机初始表示开始。然后，将消息从相邻顶点拉取或推送
    (④: Section [3.2.6](#S3.SS2.SSS6 "3.2.6\. Message Propagation ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey"))，应用 DNN 操作，并在下一次迭代开始前将更新后的顶点状态共享给其他顶点。整个过程可以以同步或异步方式执行
    (⑤: Section [3.2.7](#S3.SS2.SSS7 "3.2.7\. Synchronization Mode ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey"))，并可以应用各种调度技术
    (Section [3.2.8](#S3.SS2.SSS8 "3.2.8\. Scheduling ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey"))。而不是使用去中心化的全归约操作来共享参数，可以使用集中式参数服务器
    (⑥: Section [3.2.9](#S3.SS2.SSS9 "3.2.9\. Coordination ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey"))。在完成所有迭代后，可以根据最终的参数集进行预测。大多数系统还提供了适应于个体优化的编程抽象
    (Section [3.2.3](#S3.SS2.SSS3 "3.2.3\. Programming Abstractions ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey"))。接下来，我们将基于上述设置步骤介绍和分类
    GNN 系统。'
- en: 3.2.1\. Partitioning
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 分区
- en: 'Graph processing systems rely on partitioning the input graph and distributing
    it across machines before starting the main computation. GNN systems adopt the
    idea of partitioning because the input graphs are unlikely to fit in a single
    machine’s memory. Some systems use traditional edge-cut or vertex-cut methods
    (Zheng et al., [2020](#bib.bib214); Md et al., [2021](#bib.bib131)) whereas others
    combine those with features like a cost model (Jia et al., [2020a](#bib.bib88)),
    feasibility score (Lin et al., [2020](#bib.bib112)) or dataflow partitioning (Kiningham
    et al., [2020a](#bib.bib96)). Table [3](#S3.T3 "Table 3 ‣ 3.2.1\. Partitioning
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") summarizes the different partitioning methods.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '图处理系统依赖于对输入图进行分区，并将其分布到各个机器上，然后再开始主要计算。GNN 系统采用分区的想法，因为输入图不太可能完全适配单台机器的内存。一些系统使用传统的边切割或顶点切割方法（Zheng
    et al., [2020](#bib.bib214); Md et al., [2021](#bib.bib131)），而其他系统则将这些方法与如成本模型（Jia
    et al., [2020a](#bib.bib88)）、可行性评分（Lin et al., [2020](#bib.bib112)）或数据流分区（Kiningham
    et al., [2020a](#bib.bib96)）等特性结合使用。表 [3](#S3.T3 "Table 3 ‣ 3.2.1\. Partitioning
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") 总结了不同的分区方法。'
- en: Table 3. Categorization of partitioning strategies
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3. 分区策略的分类
- en: '|  |  | Cut type | Static vs. Dynamic | Offline vs. Online |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 切割类型 | 静态 vs. 动态 | 离线 vs. 在线 |  |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Year | System |  | Edge | Vertex | Hybrid | Static | Dynamic | Offline |
    Online | Balancing Objective |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 系统 |  | 边缘 | 顶点 | 混合 | 静态 | 动态 | 离线 | 在线 | 平衡目标 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) |  |  | ✓ | ✓ |  | ✓ |  |
    2D partitioning with equally-sized disjoint vertex chunks |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) |  |  | ✓ | ✓ |  | ✓ |  |
    等大小不相交的顶点块的 2D 分区 |'
- en: '| 2019 | GReTA | (Kiningham et al., [2020a](#bib.bib96)) |  |  | ✓ | ✓ |  |
    ✓ |  | 2D dataflow partitioning |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | GReTA | (Kiningham et al., [2020a](#bib.bib96)) |  |  | ✓ | ✓ |  |
    ✓ |  | 2D 数据流分区 |'
- en: '| 2020 | ROC | (Jia et al., [2020a](#bib.bib88)) | ✓ |  |  |  | ✓ | ✓ |  |
    runtime of a partition |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | ROC | (Jia et al., [2020a](#bib.bib88)) | ✓ |  |  |  | ✓ | ✓ |  |
    分区的运行时 |'
- en: '| 2020 | AGL | (Zhang et al., [2020c](#bib.bib205)) | ✓ |  |  | ✓ |  | ✓ |  |
    neighborhood size |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | AGL | (Zhang et al., [2020c](#bib.bib205)) | ✓ |  |  | ✓ |  | ✓ |  |
    邻域大小 |'
- en: '| 2020 | PaGraph | (Lin et al., [2020](#bib.bib112)) | ✓ |  |  |  | ✓ |  |
    ✓ | feasibility score, node degree, computation expense |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | PaGraph | (Lin et al., [2020](#bib.bib112)) | ✓ |  |  |  | ✓ |  |
    ✓ | 可行性评分、节点度、计算开销 |'
- en: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) | ✓ |  |  | ✓ |  | ✓
    |  | minimum edge-cut |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) | ✓ |  |  | ✓ |  | ✓
    |  | 最小边切割 |'
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | ✓ |  |  | ✓ |  | ✓ |  |
    random hash |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | ✓ |  |  | ✓ |  | ✓ |  |
    随机哈希 |'
- en: '| 2021 | GNNAdvisor | (Wang et al., [2021a](#bib.bib184)) | ✓ |  |  | ✓ |  |
    ✓ |  | 2D workload partitioning |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | GNNAdvisor | (Wang et al., [2021a](#bib.bib184)) | ✓ |  |  | ✓ |  |
    ✓ |  | 2D 工作负载分区 |'
- en: '| 2021 | DistGNN | (Md et al., [2021](#bib.bib131)) |  | ✓ |  | ✓ |  |  | ✓
    | edges per partition |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | DistGNN | (Md et al., [2021](#bib.bib131)) |  | ✓ |  | ✓ |  |  | ✓
    | 每个分区的边数 |'
- en: '| 2021 | DeepGalois | (Hoang et al., [2021](#bib.bib74)) | ✓ |  |  |  |  |  |
    ✓ | user-defined policy |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | DeepGalois | (Hoang et al., [2021](#bib.bib74)) | ✓ |  |  |  |  |  |
    ✓ | 用户定义策略 |'
- en: '| 2021 | ZIPPER | (Zhang et al., [2021a](#bib.bib211)) |  | ✓ |  | ✓ |  | ✓
    |  | 2D partitioning with equally-sized disjoint vertex chunks |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | ZIPPER | (Zhang et al., [2021a](#bib.bib211)) |  | ✓ |  | ✓ |  | ✓
    |  | 等大小不相交的顶点块的 2D 分区 |'
- en: In DistDGL (Zheng et al., [2020](#bib.bib214)), the input graph is partitioned
    using the METIS (Karypis, [1997](#bib.bib92); Karypis and Kumar, [1998a](#bib.bib93))
    edge-cut algorithm. In addition, optimizations like multi-constraint partitioning
    (Karypis and Kumar, [1998b](#bib.bib94)) and a refinement phase are used to improve
    load balancing. DistGNN (Md et al., [2021](#bib.bib131)) generates partitions
    with a minimum vertex-cut algorithm and the tool Libra (Xie et al., [2014](#bib.bib192)).
    Here, edges belong to one specific partition while vertices can correspond to
    multiple partitions, hence they need to be replicated. The so-called replication
    factor measures the number of replicas. A lower replication factor induces less
    communication across partitions. Moreover, Libra generates balanced partitions
    by equally distributing edges among subgraphs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在DistDGL（郑等，[2020](#bib.bib214)）中，输入图使用METIS（Karypis，[1997](#bib.bib92)；Karypis和Kumar，[1998a](#bib.bib93)）边切割算法进行划分。此外，还使用了多约束划分（Karypis和Kumar，[1998b](#bib.bib94)）和优化阶段来改善负载均衡。DistGNN（Md等，[2021](#bib.bib131)）使用最小顶点切割算法和工具Libra（Xie等，[2014](#bib.bib192)）生成划分。在这里，边属于一个特定的划分，而顶点可以对应多个划分，因此需要进行复制。所谓的复制因子衡量了副本的数量。较低的复制因子会减少划分间的通信。此外，Libra通过在子图之间均匀分配边来生成平衡的划分。
- en: NeuGraph (Ma et al., [2019](#bib.bib118)) first preprocesses the input graph
    with the METIS edge-cut algorithm and then applies a grid-based partitioning scheme
    which combines edge- and vertex-cut. It is similar to the method used in GridGraph
    (Zhu et al., [2015](#bib.bib217)) and assigns each vertex and its corresponding
    features to one of $P$ vertex chunks. Then, the adjacency matrix is tiled into
    $P\times P$ chunks containing the corresponding edges. This partitioning strategy
    benefits the edge-wise processing, because only the source and destination vertex
    data need to be loaded. Unlike NeuGraph, GReTA (Kiningham et al., [2020a](#bib.bib96))
    does not partition the graph itself, but the dataflow into blocks. The dataflow,
    also called nodeflow, is a graph structure representing the propagation of feature
    vectors throughout the forward pass of the GNN model. The vertices each represent
    a mathematical unit of computation while the edges represent the inputs and outputs
    of the units. At first, GReTA partitions the vertices of the dataflow graph into
    $n$- and $m$-sized chunks. Then, blocks of size $n\times m$ are formed out of
    the adjacency matrix containing the relevant edges. Hence, only a part of the
    grid representation instead of the whole graph needs to be loaded for performing
    a computation step. Additionally, an entire column can be processed in the aggregation
    phase. Inspired by NeuGraph and GReTA, ZIPPER (Zhang et al., [2021a](#bib.bib211))
    also uses a grid-based partitioning technique where the graph adjacency matrix
    is tiled into multiple rectangular blocks. It is distinguished between source
    and destination vertices throughout the partitioning process to ensure that each
    block is only associated with one source and one destination partition. Thus,
    each edge is uniquely identified. As described by the former systems, this partitioning
    strategy is applied to reduce the memory footprint and communication.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: NeuGraph（Ma等，[2019](#bib.bib118)）首先使用METIS边切割算法对输入图进行预处理，然后应用基于网格的划分方案，该方案结合了边切割和顶点切割。它类似于GridGraph（Zhu等，[2015](#bib.bib217)）中使用的方法，并将每个顶点及其相应的特征分配到$P$个顶点块中的一个。接着，将邻接矩阵切分为$P\times
    P$块，每块包含相应的边。这种划分策略有利于边的处理，因为只需要加载源和目标顶点的数据。与NeuGraph不同，GReTA（Kiningham等，[2020a](#bib.bib96)）不对图本身进行划分，而是对数据流进行块划分。数据流，也称为节点流，是一种图结构，表示特征向量在GNN模型的前向传播中的传播。顶点表示计算的数学单元，而边表示单元的输入和输出。首先，GReTA将数据流图的顶点划分为$n$和$m$大小的块。然后，从包含相关边的邻接矩阵中形成大小为$n\times
    m$的块。因此，在执行计算步骤时，只需要加载网格表示的一部分，而不是整个图。此外，在聚合阶段可以处理整个列。受NeuGraph和GReTA的启发，ZIPPER（Zhang等，[2021a](#bib.bib211)）也使用了基于网格的划分技术，将图的邻接矩阵切分成多个矩形块。在划分过程中区分源顶点和目标顶点，以确保每个块仅与一个源和一个目标划分相关。因此，每条边都是唯一标识的。如前述系统所描述，这种划分策略旨在减少内存占用和通信。
- en: Usually, the graph is partitioned at the beginning of the computation and these
    partitions are used until the end of the overall process. ROC (Jia et al., [2020a](#bib.bib88)),
    however, repartitions the graph before each iteration using an online regression
    model. A cost model predicts the execution time of various operations on a given
    graph partition based on parameters like the number of vertices and edges and
    the number of memory accesses. The cost model is updated and minimized at the
    end of each iteration with the actual runtimes needed for the subgraph. Then,
    the graph is repartitioned based on the updated costs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，图在计算开始时被划分，并且这些分区会一直使用到整个过程结束。然而，ROC（Jia 等人，[2020a](#bib.bib88)）在每次迭代前会使用在线回归模型重新划分图。一个成本模型根据顶点和边的数量以及内存访问次数等参数预测给定图分区上各种操作的执行时间。成本模型在每次迭代结束时根据子图的实际运行时间进行更新和最小化。然后，根据更新后的成本重新划分图。
- en: Zhang et al. provide AGL (Ant Graph ML system) (Zhang et al., [2020c](#bib.bib205))
    to employ large GNNs for industrial use cases. The GraphFlat module is responsible
    for dividing the input graph into $k$-hop neighborhoods. It uses a pipeline inspired
    by message passing to produce the desired neighborhoods. In a MapReduce-style,
    self-information about a vertex is generated, propagated along outgoing edges
    and aggregated. This is done until $k$ iterations are reached. Now, the nodes
    contain information determining the partitions regarding their $k$-hop neighbors.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人提供了 AGL（Ant Graph ML 系统）（Zhang 等人，[2020c](#bib.bib205)），用于工业应用中的大型 GNNs。GraphFlat
    模块负责将输入图划分为 $k$-跳邻域。它采用一种受消息传递启发的流水线来生成所需的邻域。在 MapReduce 风格中，关于一个顶点的自信息被生成、沿着出边传播并汇总。这个过程会持续进行，直到达到
    $k$ 次迭代。现在，节点包含了关于其 $k$-跳邻域的分区信息。
- en: 'PaGraph (Lin et al., [2020](#bib.bib112)) designs a GNN-aware partitioning
    algorithm which distributes the vertices among partitions depending on a score.
    In each iteration of the algorithm, a vertex is scanned and a vector is calculated
    where the $i$-th position determines the feasibility for assigning the vertex
    to partition $i$. The score incorporates features of the vertices so far assigned
    to partition $i$, the in-neighbor set of the vertex and the expected number of
    vertices in the partition. The current vertex is allocated to the partition with
    the highest feasibility score and it is proceeded with the subsequent vertex.
    Balanced partitions and an evenly distributed computing effort are ensured. GNNAdvisor
    (Wang et al., [2021a](#bib.bib184)) relies on neighbor partitioning where only
    the adjacent vertices of a target vertex belong to the given partition. A reason
    to choose neighbor partitioning over edge- or vertex-cut partitioning is the mitigation
    of highly varying partition sizes. Further, the probability for tiny partitions
    is lower which reduces the managing costs. In contrast to the above presented
    systems, DeepGalois (Hoang et al., [2021](#bib.bib74)) allows for customized partitioning
    by incorporating the Customizable Streaming Partitioner (CuSP) (Hoang et al.,
    [2019](#bib.bib75)) framework. A simple API lets the user determine the specific
    partitioning policy, supported are edge-cut, vertex-cut and hybrid-cut. Hence,
    the user can tailor the partitioning strategy to the specific GNN architecture.
    Unlike systems exploiting a compute-intensive customized partitioning strategy
    (Kiningham et al., [2020a](#bib.bib96); Jia et al., [2020a](#bib.bib88); Lin et al.,
    [2020](#bib.bib112)), P³ (Gandhi and Iyer, [2021](#bib.bib48)) relies on a simple
    random hash partitioner. This ensures simple, fast and efficient partitioning
    with only minimal overhead. Here, the initial graph partitioning helps to balance
    the workload, but the main optimizations to scale to large graphs are done in
    the consecutive steps of the system, namely Sampling (Section [3.2.2](#S3.SS2.SSS2
    "3.2.2\. Sampling ‣ 3.2\. Categorization of Methods for Distributed Graph Neural
    Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey")), Inter-Process Communication (Section [3.2.4](#S3.SS2.SSS4
    "3.2.4\. Inter-Process Communication ‣ 3.2\. Categorization of Methods for Distributed
    Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")), choice of Parallelism (Section [3.2.5](#S3.SS2.SSS5
    "3.2.5\. Parallelism ‣ 3.2\. Categorization of Methods for Distributed Graph Neural
    Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey")), Synchronization Mode (Section [3.2.7](#S3.SS2.SSS7 "3.2.7\.
    Synchronization Mode ‣ 3.2\. Categorization of Methods for Distributed Graph Neural
    Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey")), Scheduling (Section [3.2.8](#S3.SS2.SSS8 "3.2.8\. Scheduling
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey")) and Coordination (Section [3.2.9](#S3.SS2.SSS9 "3.2.9\. Coordination
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey")).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 'PaGraph (Lin 等，[2020](#bib.bib112)) 设计了一种 GNN-aware 分区算法，该算法根据分数将顶点分配到各个分区。在算法的每次迭代中，会扫描一个顶点并计算一个向量，其中第
    $i$ 个位置决定了将顶点分配到分区 $i$ 的可行性。分数综合了迄今为止分配到分区 $i$ 的顶点特征、顶点的邻居集合以及分区中预期的顶点数量。当前顶点被分配到具有最高可行性分数的分区，并继续处理下一个顶点。确保分区平衡和计算工作负载的均匀分配。GNNAdvisor
    (Wang 等，[2021a](#bib.bib184)) 依赖于邻居分区，其中只有目标顶点的相邻顶点属于给定的分区。选择邻居分区而非边切分区或顶点切分区的原因是减少高度变化的分区大小。此外，微小分区的概率较低，这减少了管理成本。与上述系统不同，DeepGalois
    (Hoang 等，[2021](#bib.bib74)) 通过结合 Customizable Streaming Partitioner (CuSP) (Hoang
    等，[2019](#bib.bib75)) 框架允许定制分区。一个简单的 API 允许用户确定特定的分区策略，支持边切、顶点切和混合切。因此，用户可以根据具体的
    GNN 架构量身定制分区策略。与那些利用计算密集型定制分区策略的系统（Kiningham 等，[2020a](#bib.bib96)；Jia 等，[2020a](#bib.bib88)；Lin
    等，[2020](#bib.bib112)）不同，P³ (Gandhi 和 Iyer，[2021](#bib.bib48)) 依赖于简单的随机哈希分区器。这确保了简单、快速且高效的分区，仅有最小的开销。在这里，初始图分区有助于平衡工作负载，但主要的优化是在系统的后续步骤中完成的，即采样（第
    [3.2.2](#S3.SS2.SSS2 "3.2.2\. Sampling ‣ 3.2\. Categorization of Methods for Distributed
    Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey") 节），进程间通信（第 [3.2.4](#S3.SS2.SSS4 "3.2.4\. Inter-Process
    Communication ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network
    Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") 节），并行性选择（第 [3.2.5](#S3.SS2.SSS5 "3.2.5\. Parallelism ‣ 3.2\.
    Categorization of Methods for Distributed Graph Neural Network Training ‣ 3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey")
    节），同步模式（第 [3.2.7](#S3.SS2.SSS7 "3.2.7\. Synchronization Mode ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey") 节），调度（第 [3.2.8](#S3.SS2.SSS8
    "3.2.8\. Scheduling ‣ 3.2\. Categorization of Methods for Distributed Graph Neural
    Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") 节）和协调（第 [3.2.9](#S3.SS2.SSS9 "3.2.9\. Coordination ‣ 3.2\.
    Categorization of Methods for Distributed Graph Neural Network Training ‣ 3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey")
    节）。'
- en: 3.2.2\. Sampling
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2. 采样
- en: The underlying idea of sampling originates in graph processing. However, the
    idea of sampling slightly differs in connection with GNN training. Different to
    DNN training, the samples within a graph are not independent. When performing
    mini-batch training, one can not randomly pick out a number of vertices without
    regarding the relation to other ones. Thus, training samples of a mini-batch need
    to include the k-hop neighborhood of a vertex. However, without sampling, these
    neighborhoods are likely to ”explode” as the neighborhood size quickly grows with
    each hop. For that reason, numerous strategies such as vertex-wise (Hamilton et al.,
    [2017a](#bib.bib67)), layer-wise (Zou et al., [2019](#bib.bib219)) or subgraph-based
    (Chiang et al., [2019](#bib.bib25)) sampling are introduced. The underlying idea
    of all these methods is to restrict the number of k-hop neighbors to be explored
    to prevent the described neighborhood explosion issue (Zheng et al., [2020](#bib.bib214)).
    While sampling is an established technique working well for many tasks, the choice
    of the specific strategy depends on the desired downstream ML task, the graph
    structure and the objective of the sampling method (Rozemberczki et al., [2020](#bib.bib148)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 采样的基本思想起源于图处理。然而，采样的思想在 GNN 训练中略有不同。与 DNN 训练不同，图中的样本不是独立的。在执行小批量训练时，不能随意挑选顶点而不考虑与其他顶点的关系。因此，小批量的训练样本需要包括一个顶点的
    k-hop 邻域。然而，如果没有采样，这些邻域可能会“爆炸”，因为邻域大小随着每个跳跃迅速增长。因此，引入了许多策略，如顶点级（Hamilton et al.,
    [2017a](#bib.bib67)）、层级（Zou et al., [2019](#bib.bib219)）或子图级（Chiang et al., [2019](#bib.bib25)）采样。这些方法的基本思想是限制需要探索的
    k-hop 邻居数量，以防止描述的邻域爆炸问题（Zheng et al., [2020](#bib.bib214)）。虽然采样是一种已经建立且适用于许多任务的技术，但具体策略的选择取决于期望的下游
    ML 任务、图结构和采样方法的目标（Rozemberczki et al., [2020](#bib.bib148)）。
- en: An early algorithm for efficient sampling in GNN training is GraphSAGE (SAmple
    and aggreGatE) (Hamilton et al., [2017a](#bib.bib67)). The model trains mini-batches
    and restricts the neighborhood size per layer. The number of vertices to be sampled
    is fixed and the vertices are randomly chosen. GraphSAGE is able to work on larger
    graphs compared to the general GCN architecture. However, by picking the vertices
    at random, neighborhood information might be lost leading to a decrease in accuracy.
    Therefore, PinSage (Ying et al., [2018](#bib.bib201)) adopts importance-based
    neighborhood sampling. The new technique incorporates random walks to compute
    a score for each vertex and select a fixed number of neighbors accordingly. Hence,
    memory usage can be controlled and adjusted if needed, while yielding higher accuracy
    than random sampling.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 高效采样的早期算法之一是 GraphSAGE（SAmple and aggreGatE）（Hamilton et al., [2017a](#bib.bib67)）。该模型训练小批量数据，并限制每层的邻域大小。要采样的顶点数量是固定的，顶点是随机选择的。与通用的
    GCN 架构相比，GraphSAGE 能够处理更大的图。然而，通过随机选择顶点，可能会丢失邻域信息，导致准确度下降。因此，PinSage（Ying et al.,
    [2018](#bib.bib201)）采用基于重要性的邻域采样。这种新技术结合了随机游走来为每个顶点计算分数，并相应地选择固定数量的邻居。因此，内存使用可以控制和调整，同时比随机采样获得更高的准确度。
- en: FastGCN (Chen et al., [2018](#bib.bib20)) further explores the idea of sampling
    based on a calculated score. The authors introduce an importance based layer-wise
    sampling mechanism where an importance score and a fixed neighborhood size determine
    which vertices to select. The score mainly depends on the degree of each vertex
    and is calculated for each layer to restrict the corresponding number of vertices.
    Thus, the neighborhood explosion problem can be avoided and large graphs can be
    handled. However, as the vertex-wise importance is calculated independently per
    layer, the selected neighborhoods for two subsequent layers may differ which might
    lead to slow convergence. This issue is faced by LADIES (Zou et al., [2019](#bib.bib219))
    which exploits importance sampling in a layer-dependent way. Depending on the
    sampled vertices in the previous layer, the neighboring vertices in the current
    layer are selected and a bipartite graph between the two layers is constructed.
    After that, the sampling probability is calculated and a fixed number of vertices
    is sampled. This procedure is repeated for each layer to sample the needed vertices.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: FastGCN（Chen et al., [2018](#bib.bib20)）进一步探索了基于计算得分的采样思想。作者引入了一种基于重要性的层次采样机制，其中重要性得分和固定的邻域大小决定了选择哪些顶点。得分主要取决于每个顶点的度数，并为每一层计算，以限制相应的顶点数量。因此，可以避免邻域爆炸问题，并处理大型图。然而，由于每层独立计算顶点的重要性，相邻两层的选择邻域可能不同，这可能导致收敛速度较慢。这个问题由LADIES（Zou
    et al., [2019](#bib.bib219)）解决，LADIES以层依赖的方式利用重要性采样。根据前一层采样的顶点，选择当前层的邻近顶点，并在两层之间构建一个二分图。之后，计算采样概率并采样固定数量的顶点。该过程对每一层重复，以采样所需的顶点。
- en: ClusterGCN (Chiang et al., [2019](#bib.bib25)) allows for subgraph-based sampling.
    Contrary to general mini-batch GCN training where the vertices are randomly chosen,
    a graph clustering algorithm is used to form the mini-batches. The clustering
    strategy aims at minimizing the number of links between vertices in the batch
    or between multiple batches. As a consequence, ClusterGCN is faster and uses less
    memory compared to previous methods. Zeng et al. propose GraphSAINT (Zeng et al.,
    [2019](#bib.bib203)) which also supports subgraph-based sampling. In contrast
    to former sampling-based systems first building a GCN and then sampling the input
    graph, GraphSAINT starts with sampling subgraphs and builds a GCN for each subgraph
    after that. By building a complete GCN for each sample, extensions like skip connections
    as proposed by JK-Net (Xu et al., [2018b](#bib.bib194)) are applicable without
    needing to adapt the sampling process. JK-Net requires the samples of the current
    layer to be a subset of the previous layers’ samples which is naturally met by
    using GraphSAINT. Further, GraphSAINT ensures a minimized neighborhood size while
    maintaining a high accuracy.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterGCN（Chiang et al., [2019](#bib.bib25)）允许基于子图的采样。与一般的迷你批次GCN训练中随机选择顶点不同，ClusterGCN使用图聚类算法来形成迷你批次。聚类策略旨在最小化批次内顶点之间或多个批次之间的链接数量。因此，与之前的方法相比，ClusterGCN速度更快且内存使用更少。Zeng
    et al. 提出了GraphSAINT（Zeng et al., [2019](#bib.bib203)），它也支持基于子图的采样。与以往先构建GCN然后对输入图进行采样的系统不同，GraphSAINT首先对子图进行采样，然后为每个子图构建GCN。通过为每个样本构建完整的GCN，像JK-Net（Xu
    et al., [2018b](#bib.bib194)）所提出的跳跃连接等扩展可以适用，而无需调整采样过程。JK-Net要求当前层的样本是前一层样本的子集，而使用GraphSAINT自然满足这一要求。此外，GraphSAINT在保持高准确度的同时，确保了最小化的邻域大小。
- en: 'AliGraph (Yang, [2019](#bib.bib198)) incorporates sampling by providing three
    steps: traverse, neighborhood and negative. Traverse draws a set of vertices and
    edges from the subgraphs and neighborhood is responsible for building a vertex
    context which may be one- or multi-hop neighbor vertices. The last step, negative,
    speeds up convergence of the training by setting up negative samples. Here, negative
    sampling refers to including exemplary vertices in the training process that are
    not part of the given sample. For instance, given a Graph $G$ where vertex $A$
    is connected to vertex $B$, but there is no edge between vertex $A$ and vertex
    $C$. Negative sampling would mean to not only incorporate edge $(A,B)$ in the
    training process as positive example, but also edge $(A,C)$ with an explicit negative
    tag indicating there is no connection between those vertices.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: AliGraph（Yang，[2019](#bib.bib198)）通过提供三个步骤来进行采样：遍历、邻域和负采样。遍历从子图中绘制一组顶点和边，邻域负责构建一个顶点上下文，该上下文可能是单跳或多跳的邻居顶点。最后一步负采样通过设置负样本来加速训练的收敛。这里，负采样是指在训练过程中包含不在给定样本中的示例顶点。例如，给定一个图$G$，其中顶点$A$与顶点$B$相连，但顶点$A$和顶点$C$之间没有边。负采样意味着不仅将边$(A,B)$作为正例加入训练过程中，还将边$(A,C)$作为具有明确的负标记加入。
- en: Inspired by ClusterGCN (Chiang et al., [2019](#bib.bib25)), GraphTheta (Li et al.,
    [2021a](#bib.bib109)) uses a clustering algorithm to form the graph samples. Within
    one subgraph, the algorithm detects and builds communities maximizing intra-community
    and minimizing inter-community edges. Due to forming communities before the main
    sampling step, the sampled vertices tend to overlap not as much as with (random)
    neighbor sampling leading to fewer repeated vertex accesses. Despite the advantage
    of less redundant calculations, graphs with weak community structures are not
    supported and the batch size can be imbalanced due to varying community sizes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: GraphTheta（Li等，[2021a](#bib.bib109)）受ClusterGCN（Chiang等，[2019](#bib.bib25)）的启发，使用聚类算法形成图样本。在一个子图内，该算法检测并构建最大化社区内部边和最小化社区间边的社区。由于在主要采样步骤之前形成社区，采样的顶点重叠程度不如（随机）邻居采样多，从而减少了重复的顶点访问。尽管减少了冗余计算的优点，但无法支持具有较弱社区结构的图，并且由于社区大小不同，批量大小可能不平衡。
- en: The AGL system (Zhang et al., [2020c](#bib.bib205)) provides a variety of sampling
    methods that can be chosen from, for instance, uniform sampling and weighted sampling.
    This ensures that the user can select the best strategy for each application.
    Wang et al. also pursue the idea of providing several sampling techniques in DGL
    (Wang et al., [2019](#bib.bib180)). A set of methods is provided, like the well-known
    neighbor sampling (Hamilton et al., [2017a](#bib.bib67)) and cluster sampling
    (Chiang et al., [2019](#bib.bib25)). The DistDGL system (Zheng et al., [2020](#bib.bib214))
    is based on DGL and supports several sampling techniques, but implements the sampling
    step in a distributed way. The sampling request originates from the trainer process
    and is sent to the machine responsible for the target set of vertices. After having
    received the request, the sampling worker calls the sampling operators of DGL
    and performs sampling on the local partition. It sends the results back to the
    trainer process which generates a mini-batch by assembling all acquired results.
    Like AGL, DGL and DistDGL, P³ (Gandhi and Iyer, [2021](#bib.bib48)) does not provide
    a particular sampling strategy. However, it adopts the sampling method given by
    the GNN architecture. If no specific method is included, P³ proceeds without a
    sampling phase. This ensures that a variety of GNN architectures are supported
    by the system.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: AGL系统（Zhang等，[2020c](#bib.bib205)）提供了多种可供选择的采样方法，例如均匀采样和加权采样。这确保了用户可以为每个应用选择最佳策略。Wang等人在DGL中也追求提供几种采样技术的想法（Wang等，[2019](#bib.bib180)）。提供了一系列方法，例如众所周知的邻居采样（Hamilton等，[2017a](#bib.bib67)）和聚类采样（Chiang等，[2019](#bib.bib25)）。DistDGL系统（Zheng等，[2020](#bib.bib214)）基于DGL，支持多种采样技术，但以分布式方式实现采样步骤。采样请求源自训练器进程，并发送给负责目标顶点集的计算机。在接收到请求后，采样工作器调用DGL的采样运算符，并在本地分区上执行采样。它将结果发送回训练器进程，后者通过组装所有获取的结果生成一个小批量。与AGL、DGL和DistDGL一样，P³（Gandhi和Iyer，[2021](#bib.bib48)）不提供特定的采样策略。然而，它采用了GNN架构给出的采样方法。如果没有包含特定方法，P³将在没有采样阶段的情况下继续进行。这确保系统支持多种GNN架构。
- en: 'Although sampling-based methods may decrease training time of GNNs, there remain
    issues like lack of consistency (Hu et al., [2020a](#bib.bib78)) and limited applicability
    to GNN architectures with many-hop or global context layers. For that reason,
    NeuGraph (Ma et al., [2019](#bib.bib118)) and ROC (Jia et al., [2020a](#bib.bib88))
    omit the sampling phase and rely on full-batch training. A short overview of the
    different sampling strategies can be found in Table [4](#S3.T4 "Table 4 ‣ 3.2.2\.
    Sampling ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network
    Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey").'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于采样的方法可能减少GNN的训练时间，但仍存在诸如一致性缺失（Hu et al., [2020a](#bib.bib78)）以及对具有多跳或全局上下文层的GNN架构适用性有限的问题。因此，NeuGraph（Ma
    et al., [2019](#bib.bib118)）和ROC（Jia et al., [2020a](#bib.bib88)）省略了采样阶段，依赖于全批次训练。不同采样策略的简要概述见表[4](#S3.T4
    "表4 ‣ 3.2.2. 采样 ‣ 3.2. 方法分类 ‣ 3. 系统 ‣ 图神经网络分布式系统的发展及其在图处理和深度学习中的起源：综述")。
- en: Table 4. Categorization of sampling strategies
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表4. 采样策略分类
- en: '|  |  | Method | Coordination |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 方法 | 协调 |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Year | System |  | Community | User Defined | Full Batch | Centralized |
    Distributed | Main Sampling Concepts |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 系统 |  | 社区 | 用户定义 | 全批次 | 集中式 | 分布式 | 主要采样概念 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) |  | ✓ |  | ✓ |  | choose
    method based on application |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) |  | ✓ |  | ✓ |  | 根据应用选择方法
    |'
- en: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) |  |  | ✓ | ✓ |  | no
    sampling |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) |  |  | ✓ | ✓ |  | 无采样
    |'
- en: '| 2019 | Aligraph | (Yang, [2019](#bib.bib198)) |  | ✓ |  | ✓ |  | three steps:
    traverse, neighborhood, negative |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Aligraph | (Yang, [2019](#bib.bib198)) |  | ✓ |  | ✓ |  | 三个步骤：遍历、邻域、负样本
    |'
- en: '| 2020 | ROC | (Jia et al., [2020a](#bib.bib88)) |  |  | ✓ | ✓ |  | no sampling
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | ROC | (Jia et al., [2020a](#bib.bib88)) |  |  | ✓ | ✓ |  | 无采样 |'
- en: '| 2020 | AGL | (Zhang et al., [2020c](#bib.bib205)) |  | ✓ |  | ✓ |  | choose
    method based on application |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | AGL | (Zhang et al., [2020c](#bib.bib205)) |  | ✓ |  | ✓ |  | 根据应用选择方法
    |'
- en: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) |  | ✓ |  |  | ✓ | sampling
    worker responsible for local partition |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) |  | ✓ |  |  | ✓ | 负责本地分区的采样工作者
    |'
- en: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) | ✓ |  |  | ✓ |  |
    sample from clusters, minimize overlapping vertices |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) | ✓ |  |  | ✓ |  |
    从集群中采样，最小化重叠顶点 |'
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) |  | ✓ |  | ✓ |  | adapt
    to given GNN architecture and application |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) |  | ✓ |  | ✓ |  | 适应于给定的GNN架构和应用
    |'
- en: 3.2.3\. Programming Abstractions
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3. 编程抽象
- en: To facilitate the implementation of the desired GNN architecture and to support
    custom optimizations, the proposed systems come with programming abstractions
    including user-defined functions. There are programming models based on message
    passing (Wang et al., [2019](#bib.bib180); Fey and Lenssen, [2019](#bib.bib43);
    Hu et al., [2020b](#bib.bib79)) while other abstractions are using a dataflow
    paradigm (Ma et al., [2019](#bib.bib118); Kiningham et al., [2020a](#bib.bib96);
    Li et al., [2021a](#bib.bib109)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便实现所需的GNN架构并支持自定义优化，提议的系统提供了包括用户定义函数在内的编程抽象。有基于消息传递的编程模型（Wang et al., [2019](#bib.bib180);
    Fey and Lenssen, [2019](#bib.bib43); Hu et al., [2020b](#bib.bib79)），而其他抽象则使用数据流范式（Ma
    et al., [2019](#bib.bib118); Kiningham et al., [2020a](#bib.bib96); Li et al.,
    [2021a](#bib.bib109)）。
- en: A PyTorch (Paszke et al., [2019](#bib.bib139)) extension tailored to GNN training
    is PyTorch Geometric (PyG) (Fey and Lenssen, [2019](#bib.bib43)). The library
    comes with a message passing base class where the user only needs to define the
    construction of messages, the update function as well as the aggregation scheme.
    Message propagation is handled automatically. Numerous GNN architectures can be
    implemented, for instance, GCN (Kipf and Welling, [2016a](#bib.bib98)), SGC (Wu
    et al., [2019](#bib.bib188)), GraphSAGE (Hamilton et al., [2017a](#bib.bib67)),
    GAT (Veličković et al., [2017](#bib.bib177)), and GIN (Xu et al., [2018a](#bib.bib193)).
    The Deep Graph Library (DGL) (Wang et al., [2019](#bib.bib180)) also lets the
    user define the desired GNN model as a set of message passing primitives covering
    forward and backward pass. The central abstraction is the graph data structure
    DGLGraph. (Pre-)defined functions such as neighbor sampling directly operate on
    a DGLGraph and return a subgraph object. Therefore, manually slicing tensors and
    manipulating graph data is made obsolete in contrast to frameworks such as PyG.
    Another message passing-based programming interface is introduced by FeatGraph
    (Hu et al., [2020b](#bib.bib79)). In addition to customizing the GNN model, the
    user is able to determine the parallelization strategy for the vertex- and edge-wise
    feature dimension computation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 针对GNN训练量身定制的PyTorch扩展是PyTorch Geometric（PyG）（Fey和Lenssen，[2019](#bib.bib43)）。该库提供了一个消息传递基础类，用户只需定义消息的构造、更新函数以及聚合方案。消息传播是自动处理的。可以实现众多GNN架构，例如GCN（Kipf和Welling，[2016a](#bib.bib98)）、SGC（Wu等，[2019](#bib.bib188)）、GraphSAGE（Hamilton等，[2017a](#bib.bib67)）、GAT（Veličković等，[2017](#bib.bib177)）和GIN（Xu等，[2018a](#bib.bib193)）。深度图书馆（DGL）（Wang等，[2019](#bib.bib180)）也允许用户将所需的GNN模型定义为一组覆盖前向和反向传递的消息传递原语。核心抽象是图数据结构DGLGraph。（预）定义的函数如邻居采样直接作用于DGLGraph并返回一个子图对象。因此，相比于像PyG这样的框架，手动切片张量和操作图数据变得不再必要。另一种基于消息传递的编程接口由FeatGraph（Hu等，[2020b](#bib.bib79)）引入。除了自定义GNN模型外，用户还能够确定顶点和边的特征维度计算的并行化策略。
- en: 'For applying the optimization techniques proposed by P³ (Gandhi and Iyer, [2021](#bib.bib48)),
    the system provides a message passing API that developers can use to implement
    GNN models that automatically include the optimizations. The P-TAGS API consists
    of six functions: partition, scatter, gather, transform, sync and apply. All functions
    are user-defined and target a different step in the GNN training process. An independent
    partition function is provided where the developer can implement an individual
    partitioning algorithm to reduce communication. While scatter is a message passing
    function defined on each edge, gather assembles the messages vertex-wise with
    a commutative and associative function. The transform function applies the given
    NN operations on each vertex to compute the partial activations. Then, a neighborhood
    representation is computed with NN operations such as convolution. Those representations
    are collected over the network with sync and a composite apply function is used
    to update the vertices’ states.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用P³（Gandhi和Iyer，[2021](#bib.bib48)）提出的优化技术，系统提供了一个消息传递API，开发者可以使用它来实现自动包含优化的GNN模型。P-TAGS
    API包含六个函数：partition、scatter、gather、transform、sync和apply。所有函数都是用户定义的，并且针对GNN训练过程中的不同步骤。提供了一个独立的partition函数，开发者可以在其中实现个别的分区算法以减少通信。scatter是一个定义在每条边上的消息传递函数，而gather则使用一个交换律和结合律函数按顶点收集消息。transform函数在每个顶点上应用给定的NN操作以计算部分激活。接着，使用卷积等NN操作计算邻域表示。这些表示通过sync在网络中收集，并使用复合的apply函数更新顶点的状态。
- en: 'Oriented on the GAS model (Gonzalez et al., [2012](#bib.bib59)), NeuGraph (Ma
    et al., [2019](#bib.bib118)) introduces the SAGA-NN (scatter-applyedge-gather-applyvertex
    with Neural Networks) programming scheme. The vertex-centric model expresses one
    GNN forward pass as four steps: scatter, applyedge, gather, and applyvertex. scatter
    and gather are predefined and responsible for data propagation while the other
    two, applyedge and applyvertex, are defined by the user and are expressed as dataflow
    with tensor-based operations. In the first phase, namely scatter, information
    about the vertices is passed onto adjacent edges where the values are aggregated
    and subsequently combined to form a single edge value in applyedge. The gather
    step propagates the updated values to the vertices where they are assembled. The
    vertex state is updated in the last step, applyvertex. Within each step, the computation
    is parallelized. The abstraction combines graph processing and NN training by
    uniting the dataflow model with a vertex-centric view. In general, SAGA-NN follows
    the common iterative GNN computation model which makes it applicable to various
    architectures (Kipf and Welling, [2016a](#bib.bib98); Sukhbaatar et al., [2016](#bib.bib168);
    Li et al., [2015b](#bib.bib111)).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GAS模型（Gonzalez等，[2012](#bib.bib59)），NeuGraph（Ma等，[2019](#bib.bib118)）引入了SAGA-NN（scatter-applyedge-gather-applyvertex
    with Neural Networks）编程方案。该顶点中心模型将一个GNN前向传播表示为四个步骤：scatter、applyedge、gather和applyvertex。scatter和gather是预定义的，负责数据传播，而其他两个步骤，applyedge和applyvertex，由用户定义，并以基于张量的操作的数据流形式表示。在第一个阶段，即scatter，关于顶点的信息被传递到相邻的边上，这些值被聚合并随后在applyedge中组合成一个单一的边值。gather步骤将更新后的值传播到顶点处，在那里进行汇总。最后一步applyvertex中更新顶点状态。在每一步中，计算是并行化的。该抽象通过将数据流模型与顶点中心视图相结合，融合了图处理和NN训练。一般而言，SAGA-NN遵循常见的迭代GNN计算模型，这使其适用于各种架构（Kipf和Welling，[2016a](#bib.bib98)；Sukhbaatar等，[2016](#bib.bib168)；Li等，[2015b](#bib.bib111)）。
- en: 'To allow for efficient execution of GNN training on an accelerator, GReTA (Kiningham
    et al., [2020a](#bib.bib96)) introduces four stateless user-defined functions:
    gather, reduce, transform and activate. gather loads and aggregates edge and vertex
    data. The reduce operation merges the data into one single value. The current
    and previous reduced vertex state are combined in transform. Finally, activate
    updates the vertices with new values. A GNN layer is expressed as one or multiple
    GReTA programs making it expressive enough for a diverse set of GNN models (Kipf
    and Welling, [2016a](#bib.bib98); Bresson and Laurent, [2017](#bib.bib12); Hamilton
    et al., [2017a](#bib.bib67); Xu et al., [2018a](#bib.bib193)). NN-TGAR proposed
    by GraphTheta (Li et al., [2021a](#bib.bib109)) provides user-friendly programming
    and enables training on a cluster of machines. Moreover, it unites graph processing
    and graph learning frameworks. In contrast to the GAS model and GReTA, GraphTheta
    lets the user implement the GNN model in a vertex- and edge-centric view. While
    some functions are applied to each vertex, other functions are applied to each
    edge. The first step of the abstraction, the NN-T operation, transforms values
    vertex-wise and generates corresponding messages. After that, NN-G (gather) is
    applied to each edge to update the edge values and transmit the message to the
    destination vertex. The sum operation in turn operates on each vertex and combines
    the received messages with methods like averaging or concatenation (NN-A). Then,
    the result is applied to the vertices and the parameters are updated with reduce.
    NN-T, NN-G and NN-A are implemented as neural networks, making it easy to perform
    the forward pass and subsequent gradient computation. In addition, an encoding
    layer is decomposed into subsequent independent stages allowing for general applicability.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在加速器上高效执行 GNN 训练，GReTA（Kiningham 等，[2020a](#bib.bib96)）引入了四个无状态的用户定义函数：gather、reduce、transform
    和 activate。gather 用于加载和聚合边缘及顶点数据。reduce 操作将数据合并为一个单一的值。当前和先前的减少顶点状态在 transform
    中结合。最后，activate 用新值更新顶点。一个 GNN 层被表示为一个或多个 GReTA 程序，使其足够表达多样的 GNN 模型（Kipf 和 Welling，[2016a](#bib.bib98)；Bresson
    和 Laurent，[2017](#bib.bib12)；Hamilton 等，[2017a](#bib.bib67)；Xu 等，[2018a](#bib.bib193)）。GraphTheta
    提出的 NN-TGAR（Li 等，[2021a](#bib.bib109)）提供了用户友好的编程并支持在集群上训练。此外，它将图处理和图学习框架结合起来。与
    GAS 模型和 GReTA 相比，GraphTheta 允许用户以顶点和边缘中心视角实现 GNN 模型。虽然一些函数应用于每个顶点，其他函数则应用于每个边缘。抽象的第一步，NN-T
    操作，按顶点转换值并生成相应的消息。之后，NN-G（gather）应用于每条边更新边缘值并将消息传递给目标顶点。随后，sum 操作在每个顶点上执行，并用如平均或连接（NN-A）等方法将接收到的消息合并。然后，结果应用于顶点，并用
    reduce 更新参数。NN-T、NN-G 和 NN-A 被实现为神经网络，使得前向传播和随后的梯度计算变得容易。此外，编码层被分解为后续的独立阶段，从而具有广泛的适用性。
- en: FlexGraph (Wang et al., [2021c](#bib.bib179)) introduces the programming model
    NAU (neighborselection, aggregation and update). In contrast to models based on
    the GAS abstraction (Gonzalez et al., [2012](#bib.bib59); Ma et al., [2019](#bib.bib118)),
    NAU comprises neighborselection which builds Hierarchical Dependency Graphs (HDGs)
    including chosen neighbors to capture the dependencies among vertices. After that,
    the neighborhood features are aggregated and a neighborhood representation is
    computed in the aggregation step. In the update phase, a new representation is
    calculated consisting of old features and the new neighborhood representation.
    Moreover, one single message comprises of multiple features and messages are assembled
    to reduce traffic. As opposed to programming models like SAGA-NN (Ma et al., [2019](#bib.bib118))
    and its variants, NAU is not limited to 1-hop neighbors during computation. Additional
    to flat aggregation operations, hierarchical aggregation can be used to support
    various GNN architectures. Therefore, NAU also supports GNN models with indirect
    neighbors and hierarchical aggregations, for instance, PinSage (Ying et al., [2018](#bib.bib201)),
    MAGNN (Fu et al., [2020](#bib.bib47)), P-GNN (You et al., [2019](#bib.bib202)),
    JK-Net (Xu et al., [2018b](#bib.bib194)), while SAGA-NN merely supports architectures
    where direct neighbors and flat aggregations are regarded (Kipf and Welling, [2016a](#bib.bib98);
    Xu et al., [2018a](#bib.bib193); Marcheggiani and Titov, [2017](#bib.bib122)).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: FlexGraph (Wang 等, [2021c](#bib.bib179)) 引入了编程模型 NAU（邻居选择、聚合和更新）。与基于 GAS 抽象的模型
    (Gonzalez 等, [2012](#bib.bib59); Ma 等, [2019](#bib.bib118)) 相比，NAU 包含邻居选择，这构建了包含选择邻居的层次依赖图
    (HDGs)，以捕获顶点之间的依赖关系。之后，邻域特征被聚合，在聚合步骤中计算邻域表示。在更新阶段，计算包含旧特征和新邻域表示的新表示。此外，一个消息包含多个特征，消息被组装以减少流量。与像
    SAGA-NN (Ma 等, [2019](#bib.bib118)) 及其变体的编程模型相比，NAU 在计算过程中不局限于 1-hop 邻居。除了扁平聚合操作外，层次聚合还可以用于支持各种
    GNN 架构。因此，NAU 也支持具有间接邻居和层次聚合的 GNN 模型，例如 PinSage (Ying 等, [2018](#bib.bib201))、MAGNN
    (Fu 等, [2020](#bib.bib47))、P-GNN (You 等, [2019](#bib.bib202))、JK-Net (Xu 等, [2018b](#bib.bib194))，而
    SAGA-NN 仅支持将直接邻居和扁平聚合视为架构 (Kipf 和 Welling, [2016a](#bib.bib98); Xu 等, [2018a](#bib.bib193);
    Marcheggiani 和 Titov, [2017](#bib.bib122))。
- en: Table 5. Categorization of programming abstractions
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表5. 编程抽象的分类
- en: '| Year | System |  | Expressiveness | Optimizations | Algorithms |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 系统 |  | 表达能力 | 优化 | 算法 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | message passing abstraction
    | operates on DGLGraph | GCN, SGC, GraphSAGE, GAT, GIN |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | DGL | (Wang 等, [2019](#bib.bib180)) | 消息传递抽象 | 在 DGLGraph 上操作 | GCN,
    SGC, GraphSAGE, GAT, GIN |'
- en: '| 2019 | PyG | (Fey and Lenssen, [2019](#bib.bib43)) | message passing abstraction
    | optimized sparse softmax kernels | GCN, SGC, GraphSAGE, GAT, GIN, ARMA, APPNP
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | PyG | (Fey 和 Lenssen, [2019](#bib.bib43)) | 消息传递抽象 | 优化的稀疏 softmax
    核心 | GCN, SGC, GraphSAGE, GAT, GIN, ARMA, APPNP |'
- en: '| 2019 | GReTA | (Kiningham et al., [2020a](#bib.bib96)) | dataflow abstraction
    | one or multiple GReTA programs per GNN layer | GCN, G-GCN, GraphSAGE, GIN |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | GReTA | (Kiningham 等, [2020a](#bib.bib96)) | 数据流抽象 | 每个 GNN 层一个或多个
    GReTA 程序 | GCN, G-GCN, GraphSAGE, GIN |'
- en: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) | dataflow model with
    vertex-centric view, direct neighbors and flat aggregations | tensor-based operations
    | CommNet, GCN, GG-NN |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | NeuGraph | (Ma 等, [2019](#bib.bib118)) | 带顶点中心视图的数据流模型、直接邻居和扁平聚合 |
    张量操作 | CommNet, GCN, GG-NN |'
- en: '| 2020 | FeatGraph | (Hu et al., [2020b](#bib.bib79)) | message passing abstraction
    | custom parallelization strategy | GCN, GraphSAGE, GAT |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | FeatGraph | (Hu 等, [2020b](#bib.bib79)) | 消息传递抽象 | 自定义并行策略 | GCN,
    GraphSAGE, GAT |'
- en: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) | vertex- and edge-centric
    abstraction | independent steps are implemented as neural networks | GCN, FastGCN,
    VR-GCN |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | GraphTheta | (Li 等, [2021a](#bib.bib109)) | 顶点和边缘中心的抽象 | 独立步骤实现为神经网络
    | GCN, FastGCN, VR-GCN |'
- en: '| 2021 | FlexGraph | (Wang et al., [2021c](#bib.bib179)) | indirect neighbors
    and hierarchical aggregations | hierarchical dependency graphs | PinSage, MAGNN,
    P-GNN, JK-Net |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | FlexGraph | (Wang 等, [2021c](#bib.bib179)) | 间接邻居和层次聚合 | 层次依赖图 | PinSage,
    MAGNN, P-GNN, JK-Net |'
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | vertex- and edge-centric
    abstraction | user-defined partition function | S-GCN, GCN, GraphSAGE, GAT |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | P³ | (Gandhi 和 Iyer, [2021](#bib.bib48)) | 顶点和边缘中心的抽象 | 用户定义的分区函数
    | S-GCN, GCN, GraphSAGE, GAT |'
- en: '| 2021 | Seastar | (Wu et al., [2021](#bib.bib189)) | vertex-centric abstraction
    | improved usability | GCN, GAT, APPNP, R-GCN |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | Seastar | (Wu et al., [2021](#bib.bib189)) | 顶点中心抽象 | 改善可用性 | GCN,
    GAT, APPNP, R-GCN |'
- en: 'Inspired by Pregel (Malewicz et al., [2010](#bib.bib120)), the underlying programming
    model of Seastar (Wu et al., [2021](#bib.bib189)) is realized in a vertex-centric
    fashion. From the viewpoint of a vertex, the user defines functions to implement
    the GNN architecture. Seastar then executes the given operations on each vertex.
    This improves usability compared to message passing systems (Fey and Lenssen,
    [2019](#bib.bib43); Wang et al., [2019](#bib.bib180); Ma et al., [2019](#bib.bib118))
    and dataflow programming systems (Ma et al., [2019](#bib.bib118); Yang, [2019](#bib.bib198);
    Alibaba, [2020](#bib.bib5)). With the proposed abstraction, it is possible to
    implement GNN models more easily and the implementation can be adjusted faster.
    More details on the various programming abstractions are shown in Table [5](#S3.T5
    "Table 5 ‣ 3.2.3\. Programming Abstractions ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey").'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '受Pregel（Malewicz et al., [2010](#bib.bib120)）的启发，Seastar（Wu et al., [2021](#bib.bib189)）的底层编程模型以顶点中心的方式实现。从顶点的角度来看，用户定义函数来实现GNN架构。Seastar随后在每个顶点上执行给定的操作。这比消息传递系统（Fey
    and Lenssen, [2019](#bib.bib43); Wang et al., [2019](#bib.bib180); Ma et al.,
    [2019](#bib.bib118)）和数据流编程系统（Ma et al., [2019](#bib.bib118); Yang, [2019](#bib.bib198);
    Alibaba, [2020](#bib.bib5)）提高了可用性。通过提出的抽象，可以更轻松地实现GNN模型，并且实现可以更快地调整。关于各种编程抽象的更多细节显示在表[5](#S3.T5
    "Table 5 ‣ 3.2.3\. Programming Abstractions ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")中。'
- en: 3.2.4\. Inter-Process Communication
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 进程间通信
- en: 'Before starting the GNN training process, how to store the partitions, samples
    and corresponding features and whether to cache any vertex data needs to be determined.
    DGL (Wang et al., [2019](#bib.bib180)) is built on top of DNN frameworks like
    TensorFlow (Abadi et al., [2016](#bib.bib3)) or PyTorch (Paszke et al., [2019](#bib.bib139))
    and leaves the memory management those frameworks instead of developing its own
    storing and caching strategy. Even though those frameworks are able to store datasets
    used for DNN training efficiently, more specialized techniques are beneficial
    for GNN training. Here, samples might contain overlapping neighborhoods, some
    vertices are repeatedly accessed, and it is crucial to preserve the connections
    within the graph. Therefore, the following systems employ more sophisticated methods
    which are summarized in Table [6](#S3.T6 "Table 6 ‣ 3.2.4\. Inter-Process Communication
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey").'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '在开始GNN训练过程之前，需要确定如何存储分区、样本及其相应特征，并且是否需要缓存任何顶点数据。DGL（Wang et al., [2019](#bib.bib180)）构建在TensorFlow（Abadi
    et al., [2016](#bib.bib3)）或PyTorch（Paszke et al., [2019](#bib.bib139)）等DNN框架之上，并将内存管理留给这些框架，而不是开发自己的存储和缓存策略。尽管这些框架能够高效地存储用于DNN训练的数据集，但对于GNN训练，更多的专门技术是有益的。在这里，样本可能包含重叠的邻域，一些顶点被重复访问，因此保持图中的连接至关重要。因此，以下系统采用了更复杂的方法，这些方法在表[6](#S3.T6
    "Table 6 ‣ 3.2.4\. Inter-Process Communication ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")中进行了总结。'
- en: Table 6. Categorization of inter-process communication methods
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 表6. 进程间通信方法的分类
- en: '|  |  | Storage | Caching |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 存储 | 缓存 |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Year | System | Centralized | Distributed | Data | Objective | Optimizations
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 系统 | 集中式 | 分布式 | 数据 | 目标 | 优化 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | ✓ | ✓ | - | - | leaves
    memory management and caching to base framework |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | ✓ | ✓ | - | - | 将内存管理和缓存留给基础框架
    |'
- en: '| 2019 | Aligraph | (Yang, [2019](#bib.bib198)) |  | ✓ | neighbors of selected
    vertices | based on importance value | - |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Aligraph | (Yang, [2019](#bib.bib198)) |  | ✓ | 选择顶点的邻居 | 基于重要性值 |
    - |'
- en: '| 2020 | ROC | (Jia et al., [2020a](#bib.bib88)) |  | ✓ | intermediate tensors
    on GPU | minimize cost model based on the graph, GNN model and GPU device | -
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | ROC | (Jia et al., [2020a](#bib.bib88)) |  | ✓ | GPU上的中间张量 | 基于图、GNN模型和GPU设备的最小化成本模型
    | - |'
- en: '| 2020 | PaGraph | (Lin et al., [2020](#bib.bib112)) | ✓ |  | frequently accessed
    feature vectors | minimize computation and communication | - |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | PaGraph | (Lin et al., [2020](#bib.bib112)) | ✓ |  | 频繁访问的特征向量 | 最小化计算和通信
    | - |'
- en: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) |  | ✓ | - | - | KVStore,
    co-location of data and computation |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) |  | ✓ | - | - | KVStore，数据和计算的共定位
    |'
- en: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) |  | ✓ | - | - | task-oriented
    layout |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) |  | ✓ | - | - | 任务导向布局
    |'
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | ✓ |  | graph and/or features
    | user-defined | store partial activations |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | ✓ |  | 图和/或特征 | 用户定义
    | 存储部分激活 |'
- en: '| 2021 | GNNAdvisor | (Wang et al., [2021a](#bib.bib184)) |  | ✓ | - | - |
    vertex reordering |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | GNNAdvisor | (Wang et al., [2021a](#bib.bib184)) |  | ✓ | - | - |
    顶点重排序 |'
- en: AliGraph (Yang, [2019](#bib.bib198)) proposes to cache neighbors of important
    vertices. An importance value for each vertex with respect to the number of k-hop
    in- and out-neighbors is calculated. The out-neighbors of vertices with an importance
    value exceeding a user-defined threshold are locally cached. In this way, frequently
    needed data becomes easily accessible and communication cost is reduced. ROC (Jia
    et al., [2020a](#bib.bib88)) optimizes runtime performance by caching intermediate
    tensors on GPUs while maintaining the remaining data in the host memory. By caching
    these tensors, the data transfers between CPU and GPU are decreased. A cost model
    is built with respect to a given input graph, a GNN model and a GPU device. This
    cost model is minimized with a dynamic programming algorithm to find the globally
    optimal caching strategy. As the data copy operation from CPU to GPU is a major
    bottleneck in distributed GNN training, PaGraph (Lin et al., [2020](#bib.bib112))
    uses a computation-aware caching mechanism to minimize data copy. Vertex features
    as well as structural information about the graph are stored in a Graph Store
    Server on the CPU. This shared memory is globally accessible. Additionally, a
    cache on each GPU keeps frequently accessed feature vectors. Before deciding which
    vertices to cache, Lin et al. analyze the characteristics of the training process.
    GNNs including a sampling technique randomly shuffle the samples in each epoch
    making it impossible to predict at runtime which vertices belong to which mini-batch.
    As a consequence, it is not possible to forecast which vertices are accessed at
    the next training iteration. However, the out-degree of a vertex indicates how
    likely it is to be sampled in the whole epoch. With a higher out-degree, it is
    an in-vertex for a higher number of neighbors. Thus, those vertices are chosen
    more often in other samples, yield higher computation costs and should be easily
    accessible. The caching policy of PaGraph is oriented on those findings. It pre-sorts
    the vertices by out-degree and fills up the cache following that order. This leads
    to a high cache hit ratio and reduced data transfer.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: AliGraph (Yang, [2019](#bib.bib198)) 提出了缓存重要顶点的邻居的方案。为每个顶点计算一个重要性值，考虑到k跳的入邻居和出邻居的数量。超过用户定义阈值的顶点的出邻居会被本地缓存。这样，频繁需要的数据变得易于访问，同时减少了通信成本。ROC
    (Jia et al., [2020a](#bib.bib88)) 通过在GPU上缓存中间张量来优化运行时性能，同时将其余数据保留在主机内存中。通过缓存这些张量，减少了CPU和GPU之间的数据传输。针对给定的输入图、GNN模型和GPU设备，建立了一个成本模型。这个成本模型通过动态规划算法进行最小化，以找到全局最优的缓存策略。由于CPU到GPU的数据复制操作是分布式GNN训练中的主要瓶颈，PaGraph
    (Lin et al., [2020](#bib.bib112)) 使用了一个计算感知的缓存机制来最小化数据复制。顶点特征以及关于图的结构信息被存储在CPU上的图存储服务器中。这个共享内存是全局可访问的。此外，每个GPU上的缓存保存频繁访问的特征向量。在决定缓存哪些顶点之前，Lin等人分析了训练过程的特性。包括采样技术的GNN会在每个周期随机打乱样本，使得在运行时无法预测哪些顶点属于哪个小批量。因此，无法预测哪些顶点会在下一次训练迭代中被访问。然而，顶点的出度表示它在整个周期中被采样的可能性。出度越高，意味着它是更多邻居的入顶点。因此，这些顶点在其他样本中被选择的频率更高，计算成本也更高，应该易于访问。PaGraph的缓存策略依据这些发现进行优化。它按照出度对顶点进行预排序，并按照该顺序填充缓存。这导致了高缓存命中率和减少的数据传输。
- en: A distributed file system is used by AGL (Zhang et al., [2020c](#bib.bib205))
    to store the neighborhoods. During computation, one or a batch of them is loaded
    instead of the whole graph. This highly decreases communication between graph
    stores and workers. AGL can be run on a single machine or a CPU cluster. DistDGL
    (Zheng et al., [2020](#bib.bib214)) also works on multiple CPUs. Thus, the graph
    structure, corresponding features and embeddings are stored on multiple machines.
    The so-called distributed key-value store (KVStore) is globally accessible for
    all trainer processes. DistDGL co-locates data and computation, meaning the distribution
    of vertices and edges among the KVStore servers on the machines resemble the obtained
    graph partitions in the partitioning step. Consequently, trainer processes can
    directly access the data and communication is reduced. Euler (Alibaba, [2020](#bib.bib5))
    also exploits a distributed storage architecture. The graph engine layer is responsible
    for loading and dividing the graph into subgraphs and distributing them among
    the machines.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: AGL（张等，[2020c](#bib.bib205)）使用分布式文件系统来存储邻域。在计算过程中，会加载一个或一批邻域，而不是整个图。这大大减少了图存储和工作线程之间的通信。AGL可以在单台机器或CPU集群上运行。DistDGL（郑等，[2020](#bib.bib214)）也可以在多个CPU上运行。因此，图结构、相应特征和嵌入被存储在多个机器上。所谓的分布式键值存储（KVStore）对所有训练过程全局可访问。DistDGL将数据和计算共同定位，这意味着顶点和边在KVStore服务器上的分布类似于分区步骤中获得的图分区。因此，训练过程可以直接访问数据，减少了通信。Euler（阿里巴巴，[2020](#bib.bib5)）也利用了分布式存储架构。图引擎层负责加载并将图划分为子图，然后在机器之间分配这些子图。
- en: A more specialized memory optimization is introduced by GNNAdvisor (Wang et al.,
    [2021a](#bib.bib184)). The underlying idea is to couple vertices and the computing
    units where they are processed more tightly. Therefore, graph reordering is performed
    as by RabbitOrder (Arai et al., [2016](#bib.bib6)) and the GO-PQ algorithm (Wei
    et al., [2016](#bib.bib186)). Neighbor groups being close to each other are assigned
    consecutive vertex IDs increasing the possibility for them to be scheduled closely
    on the same machine. As two adjacent neighborhoods often share common neighbors,
    the L1 cache is more efficiently used and data locality is exploited. ZIPPER (Zhang
    et al., [2021a](#bib.bib211)) also includes a vertex reordering technique. Here,
    a heuristic degree sorting strategy is used to group the out-edges of the source
    vertices. As a consequence, vertex data is more efficiently reused and redundancies
    are minimized.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: GNNAdvisor（王等，[2021a](#bib.bib184)）引入了一种更为专业的内存优化方法。其基本思想是将顶点与处理它们的计算单元更紧密地耦合。因此，图的重新排序由RabbitOrder（新井等，[2016](#bib.bib6)）和GO-PQ算法（魏等，[2016](#bib.bib186)）执行。彼此接近的邻居组被分配连续的顶点ID，增加了它们在同一台机器上紧密调度的可能性。由于两个相邻的邻域通常共享公共邻居，因此L1缓存的使用效率更高，数据局部性得以利用。ZIPPER（张等，[2021a](#bib.bib211)）也包括一个顶点重新排序技术。这里使用启发式度排序策略来分组源顶点的出边。因此，顶点数据得到了更有效的重用，冗余得到了最小化。
- en: GraphTheta (Li et al., [2021a](#bib.bib109)) also stores subgraphs in a distributed
    fashion. To achieve low-latency access and reduced memory overhead, the proposed
    parallel tensor storage utilizes a task-oriented layout. The memory needed for
    each task, e.g., forward pass, backward pass or aggregation phase, is grouped
    process-wise. The task-specific memory includes raw data and tensors which are
    further sliced into frames for more efficient access. For each frame, memory is
    allocated and deallocated immediately after usage throughout the whole computation
    process to reduce the memory utilization.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: GraphTheta（李等，[2021a](#bib.bib109)）也以分布式方式存储子图。为了实现低延迟访问和减少内存开销，提出的并行张量存储利用了面向任务的布局。每个任务所需的内存，如前向传递、反向传递或聚合阶段，被按处理方式分组。任务特定的内存包括原始数据和张量，这些张量进一步切片为帧，以便更高效地访问。对于每个帧，内存在整个计算过程中使用后立即分配和释放，以减少内存使用。
- en: Gandhi et al. (Gandhi and Iyer, [2021](#bib.bib48)) extend the KVStore introduced
    in DistDGL (Zheng et al., [2020](#bib.bib214)). In addition to vertex and edge
    data, P³ also stores partial activations in the KVStore. The extended KVStore
    coordinates data movement across machines. As soon as the machines are synchronized,
    the accumulated activation is moved to the device memory and shared with the trainer
    process. Additionally, P³ allows the user to define a caching strategy. A simple
    method tested by the authors is to store the input on a minimum number of machines
    and replicate partitions on so far unused machines.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Gandhi 等人（Gandhi and Iyer, [2021](#bib.bib48)）扩展了在 DistDGL（Zheng et al., [2020](#bib.bib214)）中引入的
    KVStore。除了顶点和边数据，P³ 还在 KVStore 中存储部分激活。扩展的 KVStore 协调了跨机器的数据移动。一旦机器同步，累计的激活被移动到设备内存并与训练进程共享。此外，P³
    允许用户定义缓存策略。作者测试的一个简单方法是将输入存储在最少数量的机器上，并在尚未使用的机器上复制分区。
- en: 3.2.5\. Parallelism
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5\. 并行性
- en: In distributed DNN training, the appropriate type of parallelism can scale computations
    to large sets of data. The proposed methods, namely data parallelism, model parallelism
    and hybrid parallelism, have proven to work well. For that reason, researchers
    apply methods taken from DNN training to GNN training.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式 DNN 训练中，合适的并行类型可以将计算扩展到大规模的数据集。所提出的方法，即数据并行、模型并行和混合并行，已被证明效果良好。因此，研究人员将从
    DNN 训练中获得的方法应用于 GNN 训练。
- en: The most common method is data parallelism. Analog to parallelism in DNN training,
    the graph, is split into subgraphs. The model is replicated on the machines while
    each machine handles its own subgraph. Systems like NeuGraph (Ma et al., [2019](#bib.bib118)),
    PaGraph (Lin et al., [2020](#bib.bib112)) and DistGNN (Md et al., [2021](#bib.bib131))
    rely on data parallelism. Two different implementations of data parallelism, namely
    edge and vertex parallelism, are introduced by DGL (Wang et al., [2019](#bib.bib180)).
    Two types of matrix multiplications are distinguished to determine whether to
    process the data edge- or vertex-parallel. Vertex-parallel computation is used
    for generalized sparse-dense matrix multiplication. In this case, the entire adjacency
    list of a vertex is managed by one thread. The edge-parallel strategy is used
    for generalized sampled dense-dense matrix multiplication where one edge is managed
    by one thread. Here, the workload is balanced implicitly while the workload extent
    of vertex-parallel processing depends on the vertex degree.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的方法是数据并行。与深度神经网络（DNN）训练中的并行性类似，图被拆分成子图。模型在机器上进行复制，每台机器处理自己的子图。像 NeuGraph（Ma
    et al., [2019](#bib.bib118)）、PaGraph（Lin et al., [2020](#bib.bib112)）和 DistGNN（Md
    et al., [2021](#bib.bib131)）这样的系统依赖于数据并行。DGL（Wang et al., [2019](#bib.bib180)）介绍了数据并行的两种不同实现方式，即边并行和顶点并行。为了确定处理数据是边并行还是顶点并行，区分了两种类型的矩阵乘法。顶点并行计算用于广义的稀疏-稠密矩阵乘法。在这种情况下，一个线程管理一个顶点的整个邻接列表。边并行策略用于广义的采样稠密-稠密矩阵乘法，其中一个线程管理一条边。在这里，工作负载是隐式平衡的，而顶点并行处理的工作负载程度取决于顶点的度数。
- en: A hybrid between data and model parallelism is employed in P³ by Gandhi et al.
    (Gandhi and Iyer, [2021](#bib.bib48)) to tackle issues like ineffectiveness of
    partitioning and GPU underutilization. First, the model is partitioned and distributed
    among the machines. After having computed the partial activations for Layer 1,
    the machines apply a reduce function to aggregate those activations. Then, P³
    switches to data parallelism to finish the forward pass. The backward pass is
    very similar, until Layer 1, data parallelism is exploited and the error gradient
    is exchanged among the machines. Then, P³ switches back to model-parallel execution
    to perform the remaining steps of the backward pass locally.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Gandhi 等人（Gandhi and Iyer, [2021](#bib.bib48)）在 P³ 中采用了数据和模型并行的混合方式，以解决如分区无效和
    GPU 未充分利用等问题。首先，将模型进行分区并在机器间分发。计算了 Layer 1 的部分激活后，机器应用 reduce 函数来汇总这些激活。然后，P³
    切换到数据并行以完成前向传递。反向传递非常相似，直到 Layer 1，利用数据并行并在机器间交换误差梯度。然后，P³ 切换回模型并行执行以在本地执行反向传递的其余步骤。
- en: Table 7. Categorization of types of parallelism
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7. 并行类型的分类
- en: '|  |  | Type of Parallelism |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 并行类型 |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Year | System | Data | Model | Hybrid | Main Concepts |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 系统 | 数据 | 模型 | 混合 | 主要概念 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | ✓ |  |  | Edge and vertex
    parallelism |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | ✓ |  |  | 边和顶点并行 |'
- en: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) | ✓ |  |  | Mini-batch
    training |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) | ✓ |  |  | 小批量训练 |'
- en: '| 2020 | PaGraph | (Lin et al., [2020](#bib.bib112)) | ✓ |  |  | Mini-batch
    training |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | PaGraph | (Lin et al., [2020](#bib.bib112)) | ✓ |  |  | 小批量训练'
- en: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) |  |  | ✓ | Data and
    operations are split |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) |  |  | ✓ | 数据和操作被分割
    |'
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) |  |  | ✓ | Push-pull parallelism
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) |  |  | ✓ | 推送-拉取并行 |'
- en: '| 2021 | DistGNN | (Md et al., [2021](#bib.bib131)) | ✓ |  |  | Mini-batch
    training |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | DistGNN | (Md et al., [2021](#bib.bib131)) | ✓ |  |  | 小批量训练 |'
- en: '| 2021 | ZIPPER | (Zhang et al., [2021a](#bib.bib211)) |  |  | ✓ | Tile- and
    operator-level parallelism |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | ZIPPER | (Zhang et al., [2021a](#bib.bib211)) |  |  | ✓ | 瓦片和操作级别并行
    |'
- en: GraphTheta (Li et al., [2021a](#bib.bib109)) also deploys a form of hybrid parallelism
    to overcome the scalability issue when handling graphs with highly skewed vertex
    degree distribution. When processing a full iteration with a high-degree vertex,
    a worker could run out of memory. For that reason, GraphTheta not only splits
    up and distributes the input graph among workers, but also the operations forming
    forward and backward pass. This ensures an efficient training phase with natural
    graphs.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: GraphTheta (Li et al., [2021a](#bib.bib109)) 也采用了一种混合并行形式，以克服处理具有高度偏斜顶点度分布的图时的可扩展性问题。在处理具有高度顶点的完整迭代时，工作节点可能会耗尽内存。因此，GraphTheta
    不仅将输入图拆分并分配给工作节点，还将形成前向和后向传播的操作进行拆分。这确保了在处理自然图时高效的训练阶段。
- en: 'Data and model parallelism are exploited by ZIPPER (Zhang et al., [2021a](#bib.bib211)).
    Subgraphs are formed by applying grid-based partitioning on the adjacency matrix.
    The partitions are processed in parallel resulting in data parallelism. Further,
    model parallelism is achieved by separating and overlapping the operations forming
    forward and backward pass. Now, the different operations can be executed concurrently
    on selected partitions to speed up computation and use the memory more efficiently.
    Table [7](#S3.T7 "Table 7 ‣ 3.2.5\. Parallelism ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey") summarizes the types of parallelism.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '数据和模型并行性由 ZIPPER (Zhang et al., [2021a](#bib.bib211)) 进行利用。通过对邻接矩阵应用基于网格的划分，形成子图。这些分区被并行处理，从而实现数据并行性。此外，通过分离和重叠形成前向和后向传播的操作来实现模型并行性。现在，不同的操作可以在选定的分区上并发执行，从而加快计算速度并更有效地使用内存。表
    [7](#S3.T7 "Table 7 ‣ 3.2.5\. Parallelism ‣ 3.2\. Categorization of Methods for
    Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey") 总结了并行性的类型。'
- en: 3.2.6\. Message Propagation
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.6\. 消息传播
- en: Similar to the exchange of data in graph processing system, machines in GNN
    systems may synchronize by pushing or pulling the relevant values. NeuGraph (Ma
    et al., [2019](#bib.bib118)) and GReTA (Kiningham et al., [2020a](#bib.bib96))
    propagate messages by pushing them directly to adjacent vertices. Messages containing
    graph features and other required information are sent across the network. The
    pull-based approach is utilized by systems like DGL (Wang et al., [2019](#bib.bib180)),
    Dorylus (Thorpe et al., [2021](#bib.bib173)) and GNNAutoScale (Fey et al., [2021](#bib.bib44)).
    The associated features as well as the k-hop neighborhood are pulled from memory
    to construct the computation graph and perform a training step. However, the moving
    of features across the network may lead to high communication. For that reason,
    no features are transferred across the network by the P³ system (Gandhi and Iyer,
    [2021](#bib.bib48)), except for partial activations and error gradients. Moreover,
    the system proposes a push-pull parallelism which switches between pushing and
    pulling during the training phase. First, P³ pulls the desired neighborhood of
    a vertex to build a computation graph which is pushed to all the machines to start
    the training phase. After having computed the partial activations at Layer 1,
    the machines pull them from all other machines. Then, the computation of the forward
    pass is performed until the last layer and the backward pass starts. At Layer
    1, the error gradients are pushed back to all machines and the backward pass ends.
    The authors chose to switch between the push and pull method to decrease the messages
    transferred over the network.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于图处理系统中的数据交换，GNN 系统中的机器可以通过推送或拉取相关值来同步。NeuGraph (Ma et al., [2019](#bib.bib118))
    和 GReTA (Kiningham et al., [2020a](#bib.bib96)) 通过直接推送消息到相邻顶点来传播消息。包含图特征和其他所需信息的消息通过网络发送。拉取式方法被
    DGL (Wang et al., [2019](#bib.bib180))、Dorylus (Thorpe et al., [2021](#bib.bib173))
    和 GNNAutoScale (Fey et al., [2021](#bib.bib44)) 等系统使用。相关特征以及 k-hop 邻域从内存中拉取，以构建计算图并执行训练步骤。然而，特征在网络中的传输可能会导致高通信开销。因此，P³
    系统 (Gandhi and Iyer, [2021](#bib.bib48)) 不会在网络中传输特征，除非是部分激活和误差梯度。此外，该系统提出了一种推拉并行方法，在训练阶段在推送和拉取之间切换。首先，P³
    拉取一个顶点的所需邻域来构建计算图，该图被推送到所有机器以开始训练阶段。在第 1 层计算了部分激活后，机器从所有其他机器中拉取这些激活。然后，进行前向传播计算直到最后一层，之后开始反向传播。在第
    1 层，误差梯度被推送回所有机器，反向传播结束。作者选择在推送和拉取方法之间切换，以减少网络上传输的消息。
- en: 3.2.7\. Synchronization Mode
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.7\. 同步模式
- en: When performing tasks in parallel, it is important to determine the execution
    mode. It needs to be decided whether to follow a synchronous, asynchronous or
    hybrid scheme. As seen in distributed graph processing and distributed NN training,
    it depends on the specific architecture which mode is best. Therefore, AliGraph
    (Yang, [2019](#bib.bib198)) does not enforce a particular synchronization method,
    but chooses the updating mode based on the provided training algorithm. If the
    implemented algorithm uses synchronous updates, the system will adopt the synchronous
    scheme, whereas the asynchronous mode will be chosen if the given training algorithm
    is based on asynchronous execution. Besides AliGraph, GraphTheta (Li et al., [2021a](#bib.bib109))
    is also not fixed to a specific mode.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行执行任务时，确定执行模式是非常重要的。需要决定是采用同步、异步还是混合方案。如在分布式图处理和分布式神经网络训练中所见，最佳模式取决于具体的架构。因此，AliGraph
    (Yang, [2019](#bib.bib198)) 并不强制使用特定的同步方法，而是根据提供的训练算法选择更新模式。如果实现的算法使用同步更新，系统将采用同步方案，而如果给定的训练算法基于异步执行，则会选择异步模式。除了
    AliGraph，GraphTheta (Li et al., [2021a](#bib.bib109)) 也没有固定到特定模式。
- en: In DistGNN (Md et al., [2021](#bib.bib131)), three update algorithms with varying
    communication intensity during the aggregation phase are implemented and compared.
    The update algorithms regard target vertices and their replicas that emerged during
    vertex-cut partitioning. The first algorithm does not allow any communication
    between split-vertices in local partitions and their cloned vertices. Hence, there
    is no need for synchronization. The second algorithm supports communication of
    local partitions with their replicas, the vertices send partial aggregates to
    their replicas. Only if all vertices have finished communication, the vertices
    move to the next step of the training phase. A delayed update mechanism is exploited
    in the third algorithm. It is an asynchronous execution mode where the vertices
    send partial aggregates in the current epoch and receive them in a consecutive
    one. In this way, remote communication and local computation are overlapped. Communication
    is further avoided by only regarding selected split-vertices during each epoch.
    The overall results show that the zero-communication strategy is the fastest while
    maintaining only slight fluctuations in accuracy, followed by the asynchronous
    delayed update algorithm.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DistGNN (Md et al., [2021](#bib.bib131)) 中，实现并比较了三种在聚合阶段具有不同通信强度的更新算法。这些更新算法涉及在顶点切割分区过程中出现的目标顶点及其副本。第一个算法不允许本地分区中的分裂顶点与其克隆顶点之间的任何通信。因此，不需要同步。第二个算法支持本地分区与其副本之间的通信，顶点将部分聚合值发送到其副本。只有当所有顶点都完成通信后，顶点才会进入训练阶段的下一步。第三个算法利用了一种延迟更新机制。这是一种异步执行模式，其中顶点在当前纪元中发送部分聚合值，并在随后的纪元中接收这些值。通过这种方式，远程通信和本地计算被重叠。通过仅在每个纪元中考虑选定的分裂顶点，进一步避免了通信。总体结果表明，零通信策略是最快的，同时保持了仅有的轻微准确度波动，其次是异步延迟更新算法。
- en: Dorylus (Thorpe et al., [2021](#bib.bib173)) compares three execution mode variants,
    a synchronous version and two asynchronous ones which differ in the choice of
    the staleness threshold. Synchronization is performed at the gather operation,
    meaning if the neighbors of a vertex have not finished to scatter their updated
    values, the vertex cannot start computing the next layer. For the asynchronous
    versions, the staleness threshold $s$ determines which stale values of neighbors
    are allowed to be used by a vertex. For one experiment, the authors chose a value
    of $s=0$. In this case, stale values of neighbors might be used if the neighbor
    is in the same epoch. A staleness value of $s=1$ was chosen for another experiment
    allowing for two successive epochs. By employing asynchronous updates with $s=0$,
    the per-epoch time could be sped up by around $1.234\times$. A staleness value
    that is too high induces slow convergence. Even though the time needed for one
    epoch decreases with $s=0$ compared to synchronous execution, the number of epochs
    to obtain the same accuracy rises.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Dorylus (Thorpe et al., [2021](#bib.bib173)) 比较了三种执行模式变体，一种是同步版本，另外两种是异步版本，它们在陈旧性阈值的选择上有所不同。同步操作发生在收集操作时，这意味着如果一个顶点的邻居尚未完成散播其更新值，该顶点不能开始计算下一层。对于异步版本，陈旧性阈值
    $s$ 决定了一个顶点允许使用哪些邻居的陈旧值。在一个实验中，作者选择了 $s=0$ 的值。在这种情况下，如果邻居在相同的纪元中，则可以使用邻居的陈旧值。另一个实验选择了
    $s=1$ 的陈旧性值，允许两个连续的纪元。通过采用 $s=0$ 的异步更新，每纪元的时间可以加快约 $1.234\times$。过高的陈旧性值会导致收敛速度缓慢。尽管与同步执行相比，使用
    $s=0$ 时每纪元所需的时间减少，但要获得相同的准确度所需的纪元数量会增加。
- en: The execution with P³ (Gandhi and Iyer, [2021](#bib.bib48)) needs to be highly
    coordinated as all machines switch concurrently from data to model parallelism.
    Additionally, during the data-parallel phase, global gradient synchronization
    is performed. Therefore, P³ follows a synchronous execution mode.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 P³ (Gandhi 和 Iyer, [2021](#bib.bib48)) 时需要高度协调，因为所有机器同时从数据并行切换到模型并行。此外，在数据并行阶段，执行了全局梯度同步。因此，P³
    遵循同步执行模式。
- en: 3.2.8\. Scheduling
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.8\. 调度
- en: 'To distribute the workload evenly across all workers, tasks and data need to
    be assigned in an intelligent way. Therefore, scheduling methods are applied which
    help to increase workload balance and minimize idle times. In the following, we
    describe important scheduling techniques. An overview and categorization can be
    found in Table [8](#S3.T8 "Table 8 ‣ 3.2.8\. Scheduling ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey").'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '为了在所有工作者中均匀分配负载，需要以智能的方式分配任务和数据。因此，应用调度方法来提高负载平衡和最小化空闲时间。接下来，我们将描述重要的调度技术。概述和分类可以在表格
    [8](#S3.T8 "Table 8 ‣ 3.2.8\. Scheduling ‣ 3.2\. Categorization of Methods for
    Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey") 中找到。'
- en: Table 8. Categorization of scheduling strategies
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8. 调度策略的分类
- en: '|  | System | Static vs. Dynamic |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | 系统 | 静态 vs. 动态 |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Year | System | Static | Dynamic | Main Concepts |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 系统 | 静态 | 动态 | 主要概念 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) |  | ✓ | Selective scheduling
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | NeuGraph | (Ma et al., [2019](#bib.bib118)) |  | ✓ | 选择性调度 |'
- en: '| 2020 | AGL | (Zhang et al., [2020c](#bib.bib205)) | ✓ |  | Parallel preprocessing
    and model computation stage |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | AGL | (Zhang et al., [2020c](#bib.bib205)) | ✓ |  | 并行预处理和模型计算阶段 |'
- en: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) |  | ✓ | Work stealing
    technique |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | GraphTheta | (Li et al., [2021a](#bib.bib109)) |  | ✓ | 工作窃取技术 |'
- en: '| 2021 | FlexGraph | (Wang et al., [2021c](#bib.bib179)) | ✓ |  | Computation
    cost based |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | FlexGraph | (Wang et al., [2021c](#bib.bib179)) | ✓ |  | 基于计算成本 |'
- en: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | ✓ |  | Inspired by PipeDream
    (Narayanan et al., [2019](#bib.bib134)), based on dependencies within computation
    graph |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | P³ | (Gandhi and Iyer, [2021](#bib.bib48)) | ✓ |  | 灵感来自 PipeDream
    (Narayanan et al., [2019](#bib.bib134))，基于计算图中的依赖关系 |'
- en: '| 2021 | Dorylus | (Thorpe et al., [2021](#bib.bib173)) |  | ✓ | Divide tasks
    based on data and computation type |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | Dorylus | (Thorpe et al., [2021](#bib.bib173)) |  | ✓ | 基于数据和计算类型划分任务
    |'
- en: '| 2021 | ZIPPER | (Zhang et al., [2021a](#bib.bib211)) | ✓ |  | Co-locate operations
    of different subgraphs |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | ZIPPER | (Zhang et al., [2021a](#bib.bib211)) | ✓ |  | 协同定位不同子图的操作
    |'
- en: 'The AGL (Zhang et al., [2020c](#bib.bib205)) pipeline divides the training
    procedure into two main stages: a preprocessing stage where the data is loaded
    and a model computation stage. Instead of performing the two stages sequentially,
    AGL schedules the stages in parallel. The time needed for the preprocessing stage
    is smaller compared to the model computation stage. Thus, the training time almost
    equals the time needed for model computation after some iterations.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: AGL (Zhang et al., [2020c](#bib.bib205)) 流水线将训练过程分为两个主要阶段：一个是数据加载的预处理阶段，另一个是模型计算阶段。AGL
    不是依次执行这两个阶段，而是将这些阶段并行调度。与模型计算阶段相比，预处理阶段所需的时间较少。因此，在经过几轮迭代后，训练时间几乎等于模型计算所需的时间。
- en: Selective scheduling in NeuGraph (Ma et al., [2019](#bib.bib118)) chooses the
    most important vertices for computing the edge values based on costs for data
    copy and transfer. Thus, only a subset of vertex data is transmitted to the GPU
    and unnecessary vertices are not regarded. Further, the system makes use of pipeline
    scheduling to find the best execution configuration. To hide the transfer latency,
    transfer of data chunks between host and device memory and computation is overlapped.
    An initial scheduling plan is iteratively refined during the training process.
    The initial random order is gradually adjusted by swapping pairs of chunks while
    monitoring computation and transfer time to ensure an optimal final schedule.
    GraphTheta (Li et al., [2021a](#bib.bib109)) adopts a work-stealing scheduling
    technique (Blumofe and Leiserson, [1999](#bib.bib11)). Tasks are assigned to all
    machines which then start computing. As soon as a machine has finished its tasks,
    it ”steals” tasks that are queued for other machines and processes them. Benefits
    of this method are improved load balance and efficiency due to reduced idle times.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: NeuGraph (Ma et al., [2019](#bib.bib118)) 的选择性调度根据数据复制和传输的成本选择最重要的顶点来计算边值。因此，只有部分顶点数据会传输到
    GPU，未考虑不必要的顶点。此外，系统利用管道调度来寻找最佳执行配置。为了隐藏传输延迟，主机和设备内存之间的数据块传输与计算重叠。初步调度计划在训练过程中被反复优化。初始的随机顺序通过交换数据块对并监控计算和传输时间来逐步调整，以确保最终的最佳调度。GraphTheta
    (Li et al., [2021a](#bib.bib109)) 采用了工作窃取调度技术 (Blumofe and Leiserson, [1999](#bib.bib11))。任务被分配给所有机器，然后开始计算。一旦某台机器完成了它的任务，它会“窃取”队列中其他机器的任务并处理它们。这种方法的好处在于由于减少了空闲时间，提高了负载平衡和效率。
- en: FlexGraph (Wang et al., [2021c](#bib.bib179)) deploys workload balancing using
    a cost function to reduce communication. In place of metrics like vertex weight
    or edge weight, the proposed cost function is based upon the GNN training cost
    per partition. To predict the computation cost of a vertex, features like the
    number of neighbors as well as the size of each neighborhood are taken into account.
    The predicted costs of all vertices are summed to estimate the final computation
    cost of the partition. An online workload balancing strategy uses the estimations
    to construct a fixed number of balancing plans where certain vertices should be
    moved from overloaded to other partitions. Finally, the system chooses the plan
    cutting the least number of edges. For an even more efficient computational process,
    FlexGraph uses a pipeline processing strategy overlapping computation and communication.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: FlexGraph (Wang et al., [2021c](#bib.bib179)) 使用成本函数进行工作负载平衡，以减少通信。提出的成本函数基于每个分区的
    GNN 训练成本，而非像顶点权重或边权重这样的指标。为了预测顶点的计算成本，会考虑邻居的数量以及每个邻居的大小。所有顶点的预测成本会被汇总，以估算分区的最终计算成本。一种在线工作负载平衡策略利用这些估算来构建固定数量的平衡计划，其中某些顶点应从过载分区移动到其他分区。最后，系统选择削减边缘数量最少的计划。为了更高效的计算过程，FlexGraph
    使用了管道处理策略来重叠计算和通信。
- en: Inspired by PipeDream (Narayanan et al., [2019](#bib.bib134)), P³ exploits a
    simple pipelining mechanism. As soon as a computation phase of a mini-batch is
    dependent on another phase, communication starts. This communication is overlapped
    with the computation of other mini-batches to avoid stalls. Due to the pipeline
    delay, weight staleness occurs. Consequently, a weight update function regarding
    weights from the previous forward and backward pass is applied. Dorylus (Thorpe
    et al., [2021](#bib.bib173)) decomposes forward and backward pass into fine-grained
    tasks. The tasks are categorized based on data type and computation type. Depending
    on the type, the tasks are processed differently and can be performed concurrently.
    Hereby, communication latency is avoided. Furthermore, the tasks are pooled and
    whenever a worker is ready, it takes the one that is scheduled next and executes
    it. ZIPPER (Zhang et al., [2021a](#bib.bib211)) co-locates operations of different
    subgraphs with a pipelining strategy. As different operations target different
    resources, the overall performance increases due to more efficiently utilized
    resources.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 PipeDream (Narayanan 等，[2019](#bib.bib134)) 的启发，P³ 利用了一种简单的流水线机制。一旦一个迷你批次的计算阶段依赖于另一个阶段，通信就会开始。这种通信与其他迷你批次的计算重叠，以避免停顿。由于流水线延迟，权重过时现象发生。因此，应用一个关于前向和反向传递的权重更新函数。Dorylus
    (Thorpe 等，[2021](#bib.bib173)) 将前向和反向传递分解为细粒度任务。这些任务根据数据类型和计算类型进行分类。根据类型，任务处理方式不同，可以并发执行。这样，通信延迟得到避免。此外，任务被汇总，当一个工作者准备好时，它会取下一个计划的任务并执行。ZIPPER
    (Zhang 等，[2021a](#bib.bib211)) 采用流水线策略将不同子图的操作共同定位。由于不同操作针对不同资源，整体性能提高，因为资源得到更有效的利用。
- en: 3.2.9\. Coordination
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.9\. 协调
- en: 'Section [2.3.3](#S2.SS3.SSS3 "2.3.3\. Coordination ‣ 2.3\. Distributed Neural
    Network Training ‣ 2\. Foundations ‣ The Evolution of Distributed Systems for
    Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") showed that it is possible to perform the training phase in a centralized,
    decentralized or hybrid manner. DistDGL (Zheng et al., [2020](#bib.bib214)) leaves
    the choice whether to operate central or decentral to the underlying framework.
    For example, if DistDGL is built on top of PyTorch (Paszke et al., [2019](#bib.bib139)),
    an all-reduce primitive is executed to collect and distribute information. However,
    if the backend framework is TensorFlow (Abadi et al., [2016](#bib.bib3)), DistDGL
    supports a parameter server implementation.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[2.3.3](#S2.SS3.SSS3 "2.3.3\. 协调 ‣ 2.3\. 分布式神经网络训练 ‣ 2\. 基础 ‣ 图神经网络的分布式系统演变及其在图处理和深度学习中的起源：综述")节显示，训练阶段可以在集中式、去中心化或混合方式下进行。DistDGL
    (Zheng 等，[2020](#bib.bib214)) 将是否操作于集中或去中心化留给底层框架。例如，如果 DistDGL 构建在 PyTorch (Paszke
    等，[2019](#bib.bib139)) 上，将执行一个全规约原语来收集和分发信息。然而，如果后端框架是 TensorFlow (Abadi 等，[2016](#bib.bib3))，DistDGL
    支持参数服务器实现。'
- en: 'AGL (Zhang et al., [2020c](#bib.bib205)) operates central and makes use of
    a parameter server as introduced in Section [2.3.3](#S2.SS3.SSS3 "2.3.3\. Coordination
    ‣ 2.3\. Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey"). The parameter server stores all required data and
    features. Each machine accesses it to fetch the assigned graph partition and exchanges
    updates without extra communication from machine to machine. GraphTheta (Li et al.,
    [2021a](#bib.bib109)) also supports computation in a central fashion. Whereas
    systems like AGL store current model parameters, the parameter server in GraphTheta
    keeps multiple version of parameters. In this manner, machines can fetch the required
    parameter version at any time helping to concurrently execute tasks with the appropriate
    parameters. DistGNN (Md et al., [2021](#bib.bib131)) shares updates in a decentralized
    way with an all-reduce operation and direct communication from machine to machine.
    Consequently, the need for a parameter server is eliminated. Another decentral
    system is PaGraph (Lin et al., [2020](#bib.bib112)). Here, trainer processes directly
    interact to exchange locally computed gradients.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: AGL (Zhang et al., [2020c](#bib.bib205)) 以**中心化**方式运作，并使用了在[2.3.3](#S2.SS3.SSS3
    "2.3.3\. 协调 ‣ 2.3\. 分布式神经网络训练 ‣ 2\. 基础 ‣ 图神经网络的分布式系统演变及其在图处理和深度学习中的起源：综述")节中介绍的参数服务器。参数服务器存储所有必需的数据和特征。每台机器访问它以获取分配的图分区，并在机器之间交换更新，无需额外的机器间通信。GraphTheta
    (Li et al., [2021a](#bib.bib109)) 也支持以中心化方式进行计算。虽然像AGL这样的系统存储当前模型参数，但GraphTheta中的参数服务器保持多个版本的参数。这样，机器可以随时获取所需的参数版本，有助于并发执行具有适当参数的任务。DistGNN
    (Md et al., [2021](#bib.bib131)) 通过全规约操作和机器间直接通信以去中心化方式共享更新。因此，消除了对参数服务器的需求。另一个去中心化系统是PaGraph
    (Lin et al., [2020](#bib.bib112))。在这里，训练进程直接互动以交换本地计算的梯度。
- en: 3.2.10\. Datasets and Benchmarks
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.10\. 数据集和基准测试
- en: Table 9. Overview of graph datasets
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9. 图数据集概览
- en: '|  |  |  |  | Task Type |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 任务类型 |  |'
- en: '| Name |  | #Vertices | #Edges | Vertex | Edge | Graph | Systems |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 名称 |  | #顶点 | #边 | 顶点 | 边 | 图 | 系统 |'
- en: '| CiteSeer | (Giles et al., [1998](#bib.bib55)) | 3,327 | 4,732 | ✓ | ✓ |  |
    PyG, GraphTheta, GNNAdvisor, GNNAutoScale |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| CiteSeer | (Giles et al., [1998](#bib.bib55)) | 3,327 | 4,732 | ✓ | ✓ |  |
    PyG, GraphTheta, GNNAdvisor, GNNAutoScale |'
- en: '| CORA | (McCallum et al., [2000](#bib.bib129)) | 2,708 | 5,429 | ✓ | ✓ |  |
    PyG, AGL, GraphTheta, GNNAdvisor, GNNAutoScale |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| CORA | (McCallum et al., [2000](#bib.bib129)) | 2,708 | 5,429 | ✓ | ✓ |  |
    PyG, AGL, GraphTheta, GNNAdvisor, GNNAutoScale |'
- en: '| PubMed | (Sen et al., [2008](#bib.bib159)) | 19,717 | 44,338 | ✓ | ✓ |  |
    PyG, NeuGraph, ROC, GraphTheta, GNNAdvisor, GNNAutoScale |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| PubMed | (Sen et al., [2008](#bib.bib159)) | 19,717 | 44,338 | ✓ | ✓ |  |
    PyG, NeuGraph, ROC, GraphTheta, GNNAdvisor, GNNAutoScale |'
- en: '| PPI | (Zitnik and Leskovec, [2017](#bib.bib218); Hamilton et al., [2017a](#bib.bib67))
    | 2,373 | 61,318 | ✓ |  |  | PyG, ROC, AGL, GNNAdvisor, GNNAutoScale |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| PPI | (Zitnik and Leskovec, [2017](#bib.bib218); Hamilton et al., [2017a](#bib.bib67))
    | 2,373 | 61,318 | ✓ |  |  | PyG, ROC, AGL, GNNAdvisor, GNNAutoScale |'
- en: '| Reddit | (Hamilton et al., [2017a](#bib.bib67)) | 232,965 | 114,848,857 |
    ✓ |  | ✓ | DGL, GReTA, NeuGraph, ROC, PaGraph, GraphTheta, FlexGraph, Dorylus,
    DistGNN, DeepGalois |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Reddit | (Hamilton et al., [2017a](#bib.bib67)) | 232,965 | 114,848,857 |
    ✓ |  | ✓ | DGL, GReTA, NeuGraph, ROC, PaGraph, GraphTheta, FlexGraph, Dorylus,
    DistGNN, DeepGalois |'
- en: '| LiveJournal | (Yang and Leskovec, [2015](#bib.bib199)) | 4,847,571 | 68,993,773
    |  | ✓ |  | GReTA, PaGraph, ZIPPER |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| LiveJournal | (Yang and Leskovec, [2015](#bib.bib199)) | 4,847,571 | 68,993,773
    |  | ✓ |  | GReTA, PaGraph, ZIPPER |'
- en: '| OGBL-ppa | (Hu et al., [2020a](#bib.bib78)) | 576,289 | 30,326,273 |  | ✓
    |  | DGL |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| OGBL-ppa | (Hu et al., [2020a](#bib.bib78)) | 576,289 | 30,326,273 |  | ✓
    |  | DGL |'
- en: '| OGBL-citation2 | (Hu et al., [2020a](#bib.bib78)) | 2,927,963 | 30,561,187
    |  | ✓ |  | DGL |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| OGBL-citation2 | (Hu et al., [2020a](#bib.bib78)) | 2,927,963 | 30,561,187
    |  | ✓ |  | DGL |'
- en: '| OGBN-arxiv | (Hu et al., [2020a](#bib.bib78)) | 169,343 | 1,166,243 | ✓ |  |  |
    DGL, GNNAutoScale |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| OGBN-arxiv | (Hu et al., [2020a](#bib.bib78)) | 169,343 | 1,166,243 | ✓ |  |  |
    DGL, GNNAutoScale |'
- en: '| OGBN-proteins | (Hu et al., [2020a](#bib.bib78)) | 132,534 | 39,561,252 |
    ✓ |  |  | DGL, GNNAdvisor |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| OGBN-proteins | (Hu et al., [2020a](#bib.bib78)) | 132,534 | 39,561,252 |
    ✓ |  |  | DGL, GNNAdvisor |'
- en: '| OGBN-products | (Hu et al., [2020a](#bib.bib78)) | 2,449,029 | 61,859,140
    | ✓ |  |  | DGL, DistDGL, P3, GNNAutoScale, DistGNN, DeepGalois |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| OGBN-products | (Hu et al., [2020a](#bib.bib78)) | 2,449,029 | 61,859,140
    | ✓ |  |  | DGL, DistDGL, P3, GNNAutoScale, DistGNN, DeepGalois |'
- en: '| OGBN-papers100M | (Hu et al., [2020a](#bib.bib78)) | 111,059,956 | 1,615,685,872
    | ✓ |  |  | DistDGL, P3, DistGNN |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| OGBN-papers100M | (Hu et al., [2020a](#bib.bib78)) | 111,059,956 | 1,615,685,872
    | ✓ |  |  | DistDGL, P3, DistGNN |'
- en: '| MAG240M | (Hu et al., [2021](#bib.bib77)) | 244,160,499 | 1,728,364,232 |
    ✓ |  |  | - |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| MAG240M | (Hu et al., [2021](#bib.bib77)) | 244,160,499 | 1,728,364,232 |
    ✓ |  |  | - |'
- en: '| WikiKG90Mv2 | (Hu et al., [2021](#bib.bib77)) | 91,230,610 | 601,062,811
    |  | ✓ |  | - |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| WikiKG90Mv2 | (Hu et al., [2021](#bib.bib77)) | 91,230,610 | 601,062,811
    |  | ✓ |  | - |'
- en: '| PCQM4Mv2 | (Hu et al., [2021](#bib.bib77)) | 52,970,652 | 54,546,813 |  |  |
    ✓ | - |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| PCQM4Mv2 | (Hu et al., [2021](#bib.bib77)) | 52,970,652 | 54,546,813 |  |  |
    ✓ | - |'
- en: 'This section gives an overview of commonly used datasets in the literature
    to provide a summary of applications and use cases of GNN systems. We highlight
    selected publicly available graph datasets and show their characteristics (see
    Table [9](#S3.T9 "Table 9 ‣ 3.2.10\. Datasets and Benchmarks ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey")). There are
    some early citation graph datasets also used to evaluate graph processing systems
    called CiteSeer (Giles et al., [1998](#bib.bib55)), CORA (McCallum et al., [2000](#bib.bib129))
    and PubMed (Sen et al., [2008](#bib.bib159)). Here, the vertices represent documents
    and the edges represent the citations between them. The size is rather small with
    approximately 3,000 vertices and 5,000 edges in CiteSeer and CORA and around 19,700
    vertices and 44,300 edges in PubMed. Predictions about the vertices and edges
    can be made, however, no graph-level tasks are currently included. Those sets
    are included in systems like PyG (Fey and Lenssen, [2019](#bib.bib43)), GraphTheta
    (Li et al., [2021a](#bib.bib109)), GNNAdvisor (Wang et al., [2021a](#bib.bib184))
    and GNNAutoScale (Fey et al., [2021](#bib.bib44)). The Protein-Protein-Interaction
    (PPI) dataset (Zitnik and Leskovec, [2017](#bib.bib218); Hamilton et al., [2017a](#bib.bib67))
    models the role of proteins in different types of human tissue. It contains 20
    graphs, each with an average number of 2,373 vertices and is applicable to vertex-level
    tasks and is included in systems like PyG (Fey and Lenssen, [2019](#bib.bib43)),
    ROC (Jia et al., [2020a](#bib.bib88)) and AGL (Zhang et al., [2020c](#bib.bib205)).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '本节概述了文献中常用的数据集，以提供 GNN 系统应用和用例的总结。我们突出了选定的公开图数据集，并展示了它们的特点（参见表 [9](#S3.T9 "Table
    9 ‣ 3.2.10\. Datasets and Benchmarks ‣ 3.2\. Categorization of Methods for Distributed
    Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")）。还有一些早期的引文图数据集也被用于评估图处理系统，如 CiteSeer (Giles et al.,
    [1998](#bib.bib55))、CORA (McCallum et al., [2000](#bib.bib129)) 和 PubMed (Sen
    et al., [2008](#bib.bib159))。在这些数据集中，顶点代表文档，边代表它们之间的引文。数据集的规模较小，CiteSeer 和 CORA
    大约有 3,000 个顶点和 5,000 条边，而 PubMed 则约有 19,700 个顶点和 44,300 条边。可以对顶点和边进行预测，但目前不包括图级任务。这些数据集被包含在
    PyG (Fey and Lenssen, [2019](#bib.bib43))、GraphTheta (Li et al., [2021a](#bib.bib109))、GNNAdvisor
    (Wang et al., [2021a](#bib.bib184)) 和 GNNAutoScale (Fey et al., [2021](#bib.bib44))
    等系统中。Protein-Protein-Interaction (PPI) 数据集 (Zitnik and Leskovec, [2017](#bib.bib218);
    Hamilton et al., [2017a](#bib.bib67)) 模拟了蛋白质在不同类型的人体组织中的作用。它包含 20 个图，每个图的平均顶点数为
    2,373，适用于顶点级任务，并被包含在 PyG (Fey and Lenssen, [2019](#bib.bib43))、ROC (Jia et al.,
    [2020a](#bib.bib88)) 和 AGL (Zhang et al., [2020c](#bib.bib205)) 等系统中。'
- en: Nowadays, a common strategy to acquire graph structured data is to crawl social
    networks and use community information as basis for the resulting graph. About
    233,000 Reddit posts from different communities are included in the Reddit (Hamilton
    et al., [2017a](#bib.bib67)) dataset and the LiveJournal (Yang and Leskovec, [2015](#bib.bib199))
    set represents around 4.8 million users and their connections. Especially the
    Reddit dataset is used by numerous systems (Wang et al., [2019](#bib.bib180);
    Kiningham et al., [2020a](#bib.bib96); Ma et al., [2019](#bib.bib118); Jia et al.,
    [2020a](#bib.bib88); Lin et al., [2020](#bib.bib112); Li et al., [2021a](#bib.bib109);
    Md et al., [2021](#bib.bib131)) to measure performance. In contrast to the PPI
    set, vertex- and graph-level tasks may be carried out on the Reddit set.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，获取图结构数据的一个常见策略是爬取社交网络，并以社区信息作为生成图的基础。Reddit (Hamilton et al., [2017a](#bib.bib67))
    数据集中包含了来自不同社区的大约 233,000 条 Reddit 帖子，而 LiveJournal (Yang and Leskovec, [2015](#bib.bib199))
    数据集则代表了约 480 万用户及其连接。尤其是 Reddit 数据集被众多系统（Wang et al., [2019](#bib.bib180); Kiningham
    et al., [2020a](#bib.bib96); Ma et al., [2019](#bib.bib118); Jia et al., [2020a](#bib.bib88);
    Lin et al., [2020](#bib.bib112); Li et al., [2021a](#bib.bib109); Md et al., [2021](#bib.bib131)）用于性能测评。与
    PPI 数据集相比，可以在 Reddit 数据集上进行顶点和图级任务。
- en: The Open Graph Benchmark (OGB) (Hu et al., [2020a](#bib.bib78)) comprises a
    collection of datasets of varying size, origin and task types. It is differentiated
    between small, medium and large sets. The small ones consist of up to 170,000
    vertices (OGBN-arxiv), the medium of up to 2.9 million vertices (OGBL-citation2)
    and the large ones of up to 111 million vertices (OGBN-papers100M). Recently,
    even larger sets have been added in the course of a large-scale challenge (Hu
    et al., [2021](#bib.bib77)) to represent real world data. The largest set, namely
    MAG240M, includes an academic graph representing papers, paper subjects, authors
    and institutions. To store the approximately 244 million vertices and 1 billion
    edges, more than 200 GB are needed. The WikiKG90Mv2 knowledge graph consists of
    91 million vertices and around 601 million edges resulting in a file size of up
    to 160 GB. The third set, PCQM4Mv2, is around 8 GB large and contains over 3,700
    graphs with a total vertex number of around 53 million and 54 million edges. OGB
    also provides a unified evaluation and benchmarking suite. In this manner, researchers
    are able to run, test and compare the performance of their model to the existing
    state-of-the-art.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 开放图基准（OGB）（Hu et al., [2020a](#bib.bib78)）包含了一系列大小、来源和任务类型各异的数据集。它区分了小型、中型和大型数据集。小型数据集最多包含170,000个顶点（OGBN-arxiv），中型数据集最多包含2.9百万个顶点（OGBL-citation2），大型数据集最多包含1.11亿个顶点（OGBN-papers100M）。最近，在大规模挑战（Hu
    et al., [2021](#bib.bib77)）过程中，还加入了更大的数据集，以代表现实世界数据。最大的数据集，即MAG240M，包括一个学术图，表示论文、论文主题、作者和机构。为了存储大约2.44亿个顶点和10亿条边，需要超过200
    GB的空间。WikiKG90Mv2知识图包含9100万个顶点和约6.01亿条边，文件大小可达160 GB。第三个数据集PCQM4Mv2大约为8 GB，包含超过3700个图，总顶点数约为5300万，边数为5400万。OGB还提供了统一的评估和基准测试套件。通过这种方式，研究人员能够运行、测试并将其模型的性能与现有的最先进技术进行比较。
- en: To date, there is no established standard dataset used by all systems for evaluation.
    Thus, comparison of their performance is difficult. One could categorize the datasets
    depending on the sizes and task types as done for the OGB datasets to compare
    the systems. However, the characteristics of the graphs might differ. For example,
    some graphs with a similar number of vertices may have varying numbers of edges
    resulting in more sparse or more dense graphs. A special case are fully connected
    graphs, where each vertex is connected to each other vertex. Another issue is
    the continuously growing size of real world graphs. If a graph might be suitable
    for representing real world data right now, it could not be suitable anymore in
    a couple of years and a new or updated dataset is needed, making comparison of
    system performance extremely difficult.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，还没有一个统一的标准数据集被所有系统用于评估。因此，比较它们的性能是困难的。可以像对OGB数据集进行分类那样，根据数据集的大小和任务类型来分类数据集，以比较系统。然而，图的特征可能有所不同。例如，某些具有类似顶点数量的图可能具有不同数量的边，从而导致图的稀疏或密集程度不同。特殊情况是完全连接图，其中每个顶点都与其他顶点连接。另一个问题是现实世界图的持续增长。如果一个图现在适合表示现实世界数据，几年后可能就不再适用，需要新的或更新的数据集，使得系统性能的比较变得极其困难。
- en: Table 10. Overview of availability and compatibility of publicly available systems
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表10. 公开可用系统的可用性和兼容性概述
- en: '|  |  | Programming language | Hardware |  |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 编程语言 | 硬件 |  |  |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Year | System | Python | C/C++ | CPU | GPU | Compatibility |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 系统 | Python | C/C++ | CPU | GPU | 兼容性 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | ✓ | ✓ | ✓ | ✓ | PyTorch,
    Tensorflow, MXNet |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | DGL | (Wang et al., [2019](#bib.bib180)) | ✓ | ✓ | ✓ | ✓ | PyTorch,
    Tensorflow, MXNet |  |'
- en: '| 2019 | PyG | (Fey and Lenssen, [2019](#bib.bib43)) | ✓ |  | ✓ | ✓ | PyTorch
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | PyG | (Fey and Lenssen, [2019](#bib.bib43)) | ✓ |  | ✓ | ✓ | PyTorch
    |  |'
- en: '| 2019 | AliGraph | (Yang, [2019](#bib.bib198)) | ✓ | ✓ | ✓ | ✓ | PyTorch,
    Tensorflow |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | AliGraph | (Yang, [2019](#bib.bib198)) | ✓ | ✓ | ✓ | ✓ | PyTorch,
    Tensorflow |  |'
- en: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) | ✓ |  | ✓ |  | DGL
    |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | DistDGL | (Zheng et al., [2020](#bib.bib214)) | ✓ |  | ✓ |  | DGL
    |  |'
- en: '| 2021 | Dorylus | (Thorpe et al., [2021](#bib.bib173)) | ✓ | ✓ | serverless
    | - |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | Dorylus | (Thorpe et al., [2021](#bib.bib173)) | ✓ | ✓ | 无服务器 | -
    |  |'
- en: '| 2021 | GNNAdvisor | (Wang et al., [2021a](#bib.bib184)) | ✓ | ✓ | ✓ | ✓ |
    PyG, DGL, Gunrock |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | GNNAdvisor | (Wang et al., [2021a](#bib.bib184)) | ✓ | ✓ | ✓ | ✓ |
    PyG, DGL, Gunrock |  |'
- en: '| 2021 | GNNAutoScale | (Fey et al., [2021](#bib.bib44)) | ✓ | ✓ | ✓ | ✓ |
    PyG |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | GNNAutoScale | (Fey et al., [2021](#bib.bib44)) | ✓ | ✓ | ✓ | ✓ |
    PyG |  |'
- en: 3.2.11\. Availability and Compatibility
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.11\. 可用性和兼容性
- en: 'To better asses which system to choose, it is important to know which ones
    are freely accessible and which frameworks are compatible (see Table [10](#S3.T10
    "Table 10 ‣ 3.2.10\. Datasets and Benchmarks ‣ 3.2\. Categorization of Methods
    for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph Neural Networks
    ‣ The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")). We found seven systems to
    be publicly available, namely PyG (Fey and Lenssen, [2019](#bib.bib43)), DGL (Wang
    et al., [2019](#bib.bib180)), AliGraph (Yang, [2019](#bib.bib198)), DistDGL (Zheng
    et al., [2020](#bib.bib214)), GNNAdvisor (Wang et al., [2021a](#bib.bib184)),
    GNNAutoScale (Fey et al., [2021](#bib.bib44)) and Dorylus (Thorpe et al., [2021](#bib.bib173)).
    PyG is a library built upon PyTorch (Paszke et al., [2019](#bib.bib139)) and is
    fully implemented in Python. It comes with multi-GPU support to achieve scalability.
    Also, there is a GitHub community¹¹1[https://github.com/pyg-team/pytorch_geometric](https://github.com/pyg-team/pytorch_geometric)
    with over 240 contributors. In contrast to PyG, DGL is framework agnostic and
    GNN models can be built with PyTorch, Tensorflow (Abadi et al., [2016](#bib.bib3))
    or Apache MXNet (Chen et al., [2015a](#bib.bib23)). DGL includes CPU and GPU support
    and is implemented in Python and C++. The GitHub community²²2[https://github.com/dmlc/dgl](https://github.com/dmlc/dgl)
    includes almost 200 contributors. DistDGL is integrated in DGL as a module³³3[https://docs.dgl.ai/en/0.6.x/api/python/dgl.distributed.html](https://docs.dgl.ai/en/0.6.x/api/python/dgl.distributed.html).
    The used programming language is Python and it runs on a cluster of CPUs. AliGraph⁴⁴4[https://github.com/alibaba/graph-learn](https://github.com/alibaba/graph-learn)
    is compatible with PyTorch and Tensorflow and uses Python and C++. GNNAdvisor⁵⁵5[https://github.com/YukeWang96/OSDI21_AE](https://github.com/YukeWang96/OSDI21_AE)
    is also implemented with Python and C++. It can either be built upon DGL, PyG
    or Gunrock (Wang et al., [2016](#bib.bib182)) as underlying framework and includes
    CPU and GPU support. GNNAutoScale, also referred to as PyGAS, is implemented in
    PyTorch and uses PyG. The code can be downloaded on GitHub⁶⁶6[https://github.com/rusty1s/pyg_autoscale](https://github.com/rusty1s/pyg_autoscale).
    Dorylus combines data servers with serverless computing. The main logic is written
    in Python and C++, the code is available on GitHub⁷⁷7[https://github.com/uclasystem/dorylus](https://github.com/uclasystem/dorylus).
    Besides the target use case, it is important to be aware of the framework support
    as well as utilized programming languages when deciding on a system. Some systems
    support various underlying frameworks, for instance DGL, AliGraph or GNNAdvisor.
    Other systems are bound to a specific set up, for example GNNAutoScale is built
    upon PyG, and PyG is implemented on top of PyTorch. Ultimately, the user needs
    to decide which one suits the application and the personal preferences the best.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地评估选择哪个系统，了解哪些系统是免费访问的以及哪些框架是兼容的非常重要（参见表[10](#S3.T10 "表 10 ‣ 3.2.10\. 数据集和基准
    ‣ 3.2\. 分布式图神经网络训练方法的分类 ‣ 3\. 图神经网络的系统 ‣ 分布式图神经网络系统的演变及其在图处理和深度学习中的起源：一项调查")）。我们发现七个系统是公开可用的，即
    PyG（Fey 和 Lenssen，[2019](#bib.bib43)），DGL（Wang 等，[2019](#bib.bib180)），AliGraph（Yang，[2019](#bib.bib198)），DistDGL（Zheng
    等，[2020](#bib.bib214)），GNNAdvisor（Wang 等，[2021a](#bib.bib184)），GNNAutoScale（Fey
    等，[2021](#bib.bib44)）和 Dorylus（Thorpe 等，[2021](#bib.bib173)）。PyG 是一个建立在 PyTorch（Paszke
    等，[2019](#bib.bib139)）上的库，完全用 Python 实现。它支持多 GPU 来实现可扩展性。此外，还有一个 GitHub 社区¹¹1[https://github.com/pyg-team/pytorch_geometric](https://github.com/pyg-team/pytorch_geometric)，拥有超过
    240 名贡献者。与 PyG 相比，DGL 是框架无关的，可以用 PyTorch、Tensorflow（Abadi 等，[2016](#bib.bib3)）或
    Apache MXNet（Chen 等，[2015a](#bib.bib23)）构建 GNN 模型。DGL 包括 CPU 和 GPU 支持，采用 Python
    和 C++ 实现。GitHub 社区²²2[https://github.com/dmlc/dgl](https://github.com/dmlc/dgl)
    包括近 200 名贡献者。DistDGL 集成在 DGL 中作为一个模块³³3[https://docs.dgl.ai/en/0.6.x/api/python/dgl.distributed.html](https://docs.dgl.ai/en/0.6.x/api/python/dgl.distributed.html)。使用的编程语言是
    Python，运行在 CPU 集群上。AliGraph⁴⁴4[https://github.com/alibaba/graph-learn](https://github.com/alibaba/graph-learn)
    兼容 PyTorch 和 Tensorflow，使用 Python 和 C++。GNNAdvisor⁵⁵5[https://github.com/YukeWang96/OSDI21_AE](https://github.com/YukeWang96/OSDI21_AE)
    也用 Python 和 C++ 实现。它可以基于 DGL、PyG 或 Gunrock（Wang 等，[2016](#bib.bib182)）构建，支持 CPU
    和 GPU。GNNAutoScale，也称为 PyGAS，使用 PyTorch 实现，基于 PyG。代码可以在 GitHub 上下载⁶⁶6[https://github.com/rusty1s/pyg_autoscale](https://github.com/rusty1s/pyg_autoscale)。Dorylus
    将数据服务器与无服务器计算结合在一起。主要逻辑用 Python 和 C++ 编写，代码可以在 GitHub 上获取⁷⁷7[https://github.com/uclasystem/dorylus](https://github.com/uclasystem/dorylus)。除了目标使用案例外，了解框架支持以及所使用的编程语言在决定选择系统时也很重要。有些系统支持各种底层框架，例如
    DGL、AliGraph 或 GNNAdvisor。其他系统则依赖于特定的设置，例如 GNNAutoScale 基于 PyG，而 PyG 构建在 PyTorch
    之上。最终，用户需要决定哪个系统最适合应用程序和个人偏好。
- en: 3.2.12\. Performance assessment
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.12\. 性能评估
- en: After reviewing the optimizations, availabilities and compatibilities of a plethora
    of systems, it would be interesting to understand the performance behavior of
    each approach. As a full fledged benchmarking effort is beyond the scope of this
    survey, we now present a selection of performance results available from the literature.
    DistDGL (Zheng et al., [2020](#bib.bib214)) gains an overall speedup of $2.2\times$
    over Euler (Alibaba, [2020](#bib.bib5)) and is more than $5\times$ faster in the
    data copy phase. GNNAdvisor (Wang et al., [2021a](#bib.bib184)) outperforms DGL
    (Wang et al., [2019](#bib.bib180)) by a factor of $3$ and NeuGraph (Ma et al.,
    [2019](#bib.bib118)) by a factor of up to $4$ when measuring training time. When
    evaluating ROC (Jia et al., [2020a](#bib.bib88)), the system achieves to perform
    up to $4\times$ higher as measured in number of epochs per second in the given
    experiments than NeuGraph. ROC, in turn, is outperformed by P³ (Gandhi and Iyer,
    [2021](#bib.bib48)) which completes epochs up to $2\times$ faster. Although some
    general points can be made above about how well the systems perform, the used
    datasets and hardware setups differ across the reported systems. This makes it
    difficult to draw explicit conclusions from the reported quantifications across
    different papers. Consequently, the need for a comprehensive performance comparison
    of GNN systems arises, a worthy endeavor for future work.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在审查了大量系统的优化、可用性和兼容性之后，了解每种方法的性能行为将是有趣的。由于全面的基准测试超出了本调查的范围，我们现在展示了一些文献中可用的性能结果。DistDGL（Zheng
    et al., [2020](#bib.bib214)）相比于Euler（Alibaba, [2020](#bib.bib5)）整体加速了 $2.2\times$，在数据复制阶段则快了超过
    $5\times$。GNNAdvisor（Wang et al., [2021a](#bib.bib184)）在训练时间上超越了DGL（Wang et al.,
    [2019](#bib.bib180)） $3$ 倍，超越了NeuGraph（Ma et al., [2019](#bib.bib118)）最多 $4$ 倍。在评估ROC（Jia
    et al., [2020a](#bib.bib88)）时，该系统在实验中每秒训练轮数的表现高出NeuGraph最多 $4\times$。而ROC则被P³（Gandhi
    和 Iyer, [2021](#bib.bib48)）超越，P³的每轮训练速度快了最多 $2\times$。虽然可以对系统性能做出一些一般性的评论，但所用的数据集和硬件设置在报告的系统中各不相同。这使得从不同论文报告的量化数据中得出明确结论变得困难。因此，GNN系统的综合性能比较成为一种必要的工作，这是未来工作的一个值得追求的目标。
- en: Table 11. Connections across systems for graph processing, DL and GNNs
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11. 图处理、深度学习和GNN系统之间的连接
- en: '|  | Graph Processing Systems | DL Systems | GNN Systems |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | 图处理系统 | 深度学习系统 | 图神经网络系统 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Partitioning | Section [2.2.2](#S2.SS2.SSS2 "2.2.2\. Partitioning ‣ 2.2\.
    Distributed Graph Processing ‣ 2\. Foundations ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") | - | Section [3.2.1](#S3.SS2.SSS1 "3.2.1\. Partitioning
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 划分 | 部分 [2.2.2](#S2.SS2.SSS2 "2.2.2\. 划分 ‣ 2.2\. 分布式图处理 ‣ 2\. 基础 ‣ 图神经网络的分布式系统演变及其在图处理和深度学习中的起源：调查")
    | - | 部分 [3.2.1](#S3.SS2.SSS1 "3.2.1\. 划分 ‣ 3.2\. 分布式图神经网络训练方法的分类 ‣ 3\. 图神经网络系统
    ‣ 图神经网络的分布式系统演变及其在图处理和深度学习中的起源：调查") |'
- en: '| Sampling | Section [2.2.1](#S2.SS2.SSS1 "2.2.1\. Sampling ‣ 2.2\. Distributed
    Graph Processing ‣ 2\. Foundations ‣ The Evolution of Distributed Systems for
    Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") | - | Section [3.2.2](#S3.SS2.SSS2 "3.2.2\. Sampling ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey") |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 采样 | 部分 [2.2.1](#S2.SS2.SSS1 "2.2.1\. 采样 ‣ 2.2\. 分布式图处理 ‣ 2\. 基础 ‣ 图神经网络的分布式系统演变及其在图处理和深度学习中的起源：调查")
    | - | 部分 [3.2.2](#S3.SS2.SSS2 "3.2.2\. 采样 ‣ 3.2\. 分布式图神经网络训练方法的分类 ‣ 3\. 图神经网络系统
    ‣ 图神经网络的分布式系统演变及其在图处理和深度学习中的起源：调查") |'
- en: '| Programming Abstraction | Section [2.2](#S2.SS2 "2.2\. Distributed Graph
    Processing ‣ 2\. Foundations ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey")
    | - | Section [3.2.3](#S3.SS2.SSS3 "3.2.3\. Programming Abstractions ‣ 3.2\. Categorization
    of Methods for Distributed Graph Neural Network Training ‣ 3\. Systems for Graph
    Neural Networks ‣ The Evolution of Distributed Systems for Graph Neural Networks
    and their Origin in Graph Processing and Deep Learning: A Survey") |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 编程抽象 | 章节 [2.2](#S2.SS2 "2.2\. 分布式图处理 ‣ 2\. 基础知识 ‣ 分布式图神经网络系统的演变及其在图处理和深度学习中的起源:
    一项调查") | - | 章节 [3.2.3](#S3.SS2.SSS3 "3.2.3\. 编程抽象 ‣ 3.2\. 分布式图神经网络训练方法的分类 ‣ 3\.
    图神经网络系统 ‣ 分布式图神经网络系统的演变及其在图处理和深度学习中的起源: 一项调查") |'
- en: '| Inter-Process Communication | Section [2.2.3](#S2.SS2.SSS3 "2.2.3\. Inter-Process
    Communication ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey") | - | Section [3.2.4](#S3.SS2.SSS4 "3.2.4\. Inter-Process
    Communication ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network
    Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 进程间通信 | 章节 [2.2.3](#S2.SS2.SSS3 "2.2.3\. 进程间通信 ‣ 2.2\. 分布式图处理 ‣ 2\. 基础知识
    ‣ 分布式图神经网络系统的演变及其在图处理和深度学习中的起源: 一项调查") | - | 章节 [3.2.4](#S3.SS2.SSS4 "3.2.4\.
    进程间通信 ‣ 3.2\. 分布式图神经网络训练方法的分类 ‣ 3\. 图神经网络系统 ‣ 分布式图神经网络系统的演变及其在图处理和深度学习中的起源: 一项调查")
    |'
- en: '| Parallelism | - | Section [2.3.1](#S2.SS3.SSS1 "2.3.1\. Parallelism ‣ 2.3\.
    Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") | Section [3.2.5](#S3.SS2.SSS5 "3.2.5\. Parallelism ‣ 3.2\.
    Categorization of Methods for Distributed Graph Neural Network Training ‣ 3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey")
    |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 并行性 | - | 章节 [2.3.1](#S2.SS3.SSS1 "2.3.1\. 并行性 ‣ 2.3\. 分布式神经网络训练 ‣ 2\. 基础知识
    ‣ 分布式图神经网络系统的演变及其在图处理和深度学习中的起源: 一项调查") | 章节 [3.2.5](#S3.SS2.SSS5 "3.2.5\. 并行性
    ‣ 3.2\. 分布式图神经网络训练方法的分类 ‣ 3\. 图神经网络系统 ‣ 分布式图神经网络系统的演变及其在图处理和深度学习中的起源: 一项调查") |'
- en: '| Message Propagation | Section [2.2.5](#S2.SS2.SSS5 "2.2.5\. Message Propagation
    ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") | - | Section [3.2.6](#S3.SS2.SSS6 "3.2.6\. Message Propagation
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 信息传播 | 章节 [2.2.5](#S2.SS2.SSS5 "2.2.5\. 信息传播 ‣ 2.2\. 分布式图处理 ‣ 2\. 基础知识 ‣
    分布式图神经网络系统的演变及其在图处理和深度学习中的起源: 一项调查") | - | 章节 [3.2.6](#S3.SS2.SSS6 "3.2.6\. 信息传播
    ‣ 3.2\. 分布式图神经网络训练方法的分类 ‣ 3\. 图神经网络系统 ‣ 分布式图神经网络系统的演变及其在图处理和深度学习中的起源: 一项调查") |'
- en: '| Synchronization Mode | Section [2.2.4](#S2.SS2.SSS4 "2.2.4\. State Synchronization
    and Scheduling ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey") | Section [2.3.2](#S2.SS3.SSS2 "2.3.2\. Synchronization
    Mode ‣ 2.3\. Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey") | Section [3.2.7](#S3.SS2.SSS7 "3.2.7\. Synchronization
    Mode ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 同步模式 | 部分 [2.2.4](#S2.SS2.SSS4 "2.2.4\. 状态同步与调度 ‣ 2.2\. 分布式图处理 ‣ 2\. 基础 ‣
    图神经网络分布式系统的发展及其在图处理和深度学习中的起源：综述") | 部分 [2.3.2](#S2.SS3.SSS2 "2.3.2\. 同步模式 ‣ 2.3\.
    分布式神经网络训练 ‣ 2\. 基础 ‣ 图神经网络分布式系统的发展及其在图处理和深度学习中的起源：综述") | 部分 [3.2.7](#S3.SS2.SSS7
    "3.2.7\. 同步模式 ‣ 3.2\. 分布式图神经网络训练方法分类 ‣ 3\. 图神经网络系统 ‣ 图神经网络分布式系统的发展及其在图处理和深度学习中的起源：综述")
    |'
- en: '| Scheduling | Section [2.2.4](#S2.SS2.SSS4 "2.2.4\. State Synchronization
    and Scheduling ‣ 2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey") | - | Section [3.2.8](#S3.SS2.SSS8 "3.2.8\. Scheduling
    ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network Training
    ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems
    for Graph Neural Networks and their Origin in Graph Processing and Deep Learning:
    A Survey") |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 调度 | 部分 [2.2.4](#S2.SS2.SSS4 "2.2.4\. 状态同步与调度 ‣ 2.2\. 分布式图处理 ‣ 2\. 基础 ‣ 图神经网络分布式系统的发展及其在图处理和深度学习中的起源：综述")
    | - | 部分 [3.2.8](#S3.SS2.SSS8 "3.2.8\. 调度 ‣ 3.2\. 分布式图神经网络训练方法分类 ‣ 3\. 图神经网络系统
    ‣ 图神经网络分布式系统的发展及其在图处理和深度学习中的起源：综述") |'
- en: '| Coordination | - | Section [2.3.3](#S2.SS3.SSS3 "2.3.3\. Coordination ‣ 2.3\.
    Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") | Section [3.2.9](#S3.SS2.SSS9 "3.2.9\. Coordination ‣ 3.2\.
    Categorization of Methods for Distributed Graph Neural Network Training ‣ 3\.
    Systems for Graph Neural Networks ‣ The Evolution of Distributed Systems for Graph
    Neural Networks and their Origin in Graph Processing and Deep Learning: A Survey")
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 协调 | - | 部分 [2.3.3](#S2.SS3.SSS3 "2.3.3\. 协调 ‣ 2.3\. 分布式神经网络训练 ‣ 2\. 基础 ‣
    图神经网络分布式系统的发展及其在图处理和深度学习中的起源：综述") | 部分 [3.2.9](#S3.SS2.SSS9 "3.2.9\. 协调 ‣ 3.2\.
    分布式图神经网络训练方法分类 ‣ 3\. 图神经网络系统 ‣ 图神经网络分布式系统的发展及其在图处理和深度学习中的起源：综述") |'
- en: 3.3\. Connections across graph processing, deep learning and GNN systems
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 图处理、深度学习和 GNN 系统之间的联系
- en: 'We have introduced methods used to distribute and optimize graph processing
    (Section [2.2](#S2.SS2 "2.2\. Distributed Graph Processing ‣ 2\. Foundations ‣
    The Evolution of Distributed Systems for Graph Neural Networks and their Origin
    in Graph Processing and Deep Learning: A Survey")) and DL (Section [2.3](#S2.SS3
    "2.3\. Distributed Neural Network Training ‣ 2\. Foundations ‣ The Evolution of
    Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey")) and techniques commonly used by systems for GNNs
    in Section [3.2](#S3.SS2 "3.2\. Categorization of Methods for Distributed Graph
    Neural Network Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution
    of Distributed Systems for Graph Neural Networks and their Origin in Graph Processing
    and Deep Learning: A Survey"). Table [11](#S3.T11 "Table 11 ‣ 3.2.12\. Performance
    assessment ‣ 3.2\. Categorization of Methods for Distributed Graph Neural Network
    Training ‣ 3\. Systems for Graph Neural Networks ‣ The Evolution of Distributed
    Systems for Graph Neural Networks and their Origin in Graph Processing and Deep
    Learning: A Survey") shows how the introduced methods and the systems are connected.
    Systems for GNNs and systems for graph processing share a wide range of principles,
    such as partitioning, message propagation and scheduling. DL systems share ideas
    of parallelism, synchronization and coordination with GNN systems. Sampling occurs
    in GNN training and in graph processing. However, sampling in GNN training is
    not exactly the same as sampling in graph processing. In GNN training, sampling
    is used to exclude certain vertices during a training epoch and to create mini-batches,
    while sampling in graph processing means to sparsify the whole input graph before
    starting the computation.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了用于分布和优化图处理的方法（第 [2.2](#S2.SS2 "2.2\. 分布式图处理 ‣ 2\. 基础 ‣ 分布式图神经网络系统的发展及其在图处理和深度学习中的起源：综述")
    节）和深度学习（第 [2.3](#S2.SS3 "2.3\. 分布式神经网络训练 ‣ 2\. 基础 ‣ 分布式图神经网络系统的发展及其在图处理和深度学习中的起源：综述")
    节），以及系统在图神经网络中的常用技术（第 [3.2](#S3.SS2 "3.2\. 分布式图神经网络训练方法的分类 ‣ 3\. 图神经网络系统 ‣ 分布式图神经网络系统的发展及其在图处理和深度学习中的起源：综述")
    节）。表 [11](#S3.T11 "表 11 ‣ 3.2.12\. 性能评估 ‣ 3.2\. 分布式图神经网络训练方法的分类 ‣ 3\. 图神经网络系统
    ‣ 分布式图神经网络系统的发展及其在图处理和深度学习中的起源：综述") 展示了所介绍的方法和系统之间的联系。图神经网络系统和图处理系统共享广泛的原则，例如分区、消息传播和调度。深度学习系统与图神经网络系统共享并行性、同步和协调的理念。在图神经网络训练和图处理过程中都会发生采样。然而，图神经网络训练中的采样与图处理中的采样并不完全相同。在图神经网络训练中，采样用于在训练时期排除某些顶点并创建小批量，而图处理中的采样则意味着在开始计算之前对整个输入图进行稀疏化。
- en: 4\. Discussion and Outlook
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 讨论与展望
- en: We have seen that designing systems for GNN training is a challenging task.
    The recently proposed systems face issues like workload imbalance, redundant vertex
    accesses, communication overhead or changing the parallelism. In the following,
    we discuss current topics that are increasingly investigated as well as open challenges
    that still arise when developing GNN systems.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，设计图神经网络训练系统是一个具有挑战性的任务。最近提出的系统面临着工作负载不平衡、冗余顶点访问、通信开销或并行性变化等问题。接下来，我们讨论了当前越来越多的研究主题以及在开发图神经网络系统时仍然存在的开放挑战。
- en: 4.1\. Current Research Trends
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 当前研究趋势
- en: An increasing amount of research dealing with the co-design of software and
    hardware to accelerate GNN training (Zhang et al., [2020e](#bib.bib204); Chen
    et al., [2021](#bib.bib24); Kiningham et al., [2020b](#bib.bib97), [a](#bib.bib96)).
    Here, not only software and algorithms are optimized, but also hardware modules
    are developed to better address the characteristics of GNNs. Another interesting
    approach examines the acceleration of quantized GNN architectures (Wang et al.,
    [2022](#bib.bib183)). Quantized GNNs (Tailor et al., [2020](#bib.bib169); Feng
    et al., [2020](#bib.bib41); Ding et al., [2021](#bib.bib33); Saad and Beferull-Lozano,
    [2021](#bib.bib152)) have increasingly emerged in the past years and incorporate
    compressed weights and node embeddings to reduce the memory footprint and computation.
    In addition, they are robust and the loss of accuracy is marginal. Systems adapted
    to quantized GNNs can further accelerate training and inference times. Beyond
    quantized GNNs, acceleration techniques for dynamic graphs have been proposed
    (Guan et al., [2022](#bib.bib62); Wang et al., [2021b](#bib.bib181)). Dynamic
    graphs change over time, edges might be added or deleted and vertex features evolve.
    Some architectures exist that are able to process dynamic graphs, for example,
    SDG (Fu and He, [2021](#bib.bib46)), Dynamic-GRCNN (Peng et al., [2020](#bib.bib140)),
    and DyGNN (Ma et al., [2020](#bib.bib119)). Dynamic graphs and architectures for
    processing them pose new research challenges that are currently being solved.
    Ongoing research also aims at optimizing inference for GNNs (Zhang et al., [2022](#bib.bib207);
    Gao et al., [2022](#bib.bib51)), as real-time inference might be crucial for applications
    such as self-driving cars (Zhou et al., [2021](#bib.bib215)). Methods range from
    combining multi-layer perceptrons (Jain et al., [1996](#bib.bib86)) with GNNs
    (Zhang et al., [2022](#bib.bib207)), introducing a novel propagation scheme which
    is adapted per node (Gao et al., [2022](#bib.bib51)) and pruning channels of the
    feature matrices which leads to pruned weight matrices (Zhou et al., [2021](#bib.bib215)).
    Moreover, work focusing on improving the data loading and preprocessing step is
    proposed (Liu et al., [2021](#bib.bib114); Jangda et al., [2021](#bib.bib87)).
    When measuring the time of the different training steps, it can be noted that
    a prominent proportion of the overall training time is consumed by reading in
    and preprocessing the graphs. By investigating and improving data I/O and preprocessing,
    the training process can be made more efficient in future work. Lately, neural
    architecture search (NAS) and automated methods for GNNs has caught the attention
    of many experts (Gao et al., [2020c](#bib.bib52); Cai et al., [2021](#bib.bib16);
    Li and King, [2020](#bib.bib110); Huan et al., [2021](#bib.bib80); Zhao et al.,
    [2020](#bib.bib213)). Going a step further, mechanisms like NAS can inspire researchers
    to also include automated methods assessing which optimization technique fits
    best for the given architecture, data and system. For instance, it could be automatically
    made a recommendation which partitioning strategy is most suitable depending on
    the requirements of the user. But not only single analyses could be made, also
    an overall estimation about the combination and composition of optimizations.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 处理软件和硬件协同设计以加速GNN训练的研究越来越多（Zhang et al., [2020e](#bib.bib204); Chen et al.,
    [2021](#bib.bib24); Kiningham et al., [2020b](#bib.bib97), [a](#bib.bib96)）。在这里，不仅优化了软件和算法，还开发了硬件模块，以更好地应对GNN的特性。另一种有趣的方法是研究量化GNN架构的加速（Wang
    et al., [2022](#bib.bib183)）。量化GNN（Tailor et al., [2020](#bib.bib169); Feng et
    al., [2020](#bib.bib41); Ding et al., [2021](#bib.bib33); Saad and Beferull-Lozano,
    [2021](#bib.bib152)）近年来逐渐出现，包含压缩权重和节点嵌入，以减少内存占用和计算。此外，它们具有鲁棒性，准确度损失很小。适配量化GNN的系统可以进一步加速训练和推断时间。除了量化GNN外，还提出了针对动态图的加速技术（Guan
    et al., [2022](#bib.bib62); Wang et al., [2021b](#bib.bib181)）。动态图随时间变化，边缘可能会被添加或删除，顶点特征也在演变。一些架构能够处理动态图，例如SDG（Fu
    and He, [2021](#bib.bib46)），Dynamic-GRCNN（Peng et al., [2020](#bib.bib140)）和DyGNN（Ma
    et al., [2020](#bib.bib119)）。动态图及其处理架构提出了新的研究挑战，目前正在解决中。正在进行的研究还旨在优化GNN的推断（Zhang
    et al., [2022](#bib.bib207); Gao et al., [2022](#bib.bib51)），因为实时推断对于自驾车等应用可能至关重要（Zhou
    et al., [2021](#bib.bib215)）。方法包括将多层感知器（Jain et al., [1996](#bib.bib86)）与GNN结合（Zhang
    et al., [2022](#bib.bib207)），引入针对每个节点的创新传播方案（Gao et al., [2022](#bib.bib51)），以及剪枝特征矩阵的通道，从而得到剪枝的权重矩阵（Zhou
    et al., [2021](#bib.bib215)）。此外，还提出了改进数据加载和预处理步骤的工作（Liu et al., [2021](#bib.bib114);
    Jangda et al., [2021](#bib.bib87)）。在测量不同训练步骤的时间时，可以注意到整体训练时间中相当大的一部分用于读取和预处理图。通过调查和改进数据I/O和预处理，未来的工作可以使训练过程更加高效。最近，神经架构搜索（NAS）和针对GNN的自动化方法引起了许多专家的关注（Gao
    et al., [2020c](#bib.bib52); Cai et al., [2021](#bib.bib16); Li and King, [2020](#bib.bib110);
    Huan et al., [2021](#bib.bib80); Zhao et al., [2020](#bib.bib213)）。进一步说，像NAS这样的机制可以激励研究人员也包括自动化方法，以评估哪种优化技术最适合给定的架构、数据和系统。例如，可以自动推荐哪种分区策略最适合用户的需求。但不仅可以进行单一分析，还可以对优化的组合和构成进行整体估计。
- en: 4.2\. Open Challenges
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 开放挑战
- en: As GNNs and systems for GNNs are young research fields, there still are open
    challenges to be solved. Most systems are evaluated on static graphs and standard
    GNN models like GCN, GAT and GraphSAGE, but there is no information on how the
    systems would perform on more specialized models and types of data. There are,
    for instance, GNNs for hypergraphs (Feng et al., [2019](#bib.bib42); Yadati et al.,
    [2019](#bib.bib196)) and multigraphs (Ouyang et al., [2019](#bib.bib136); Geng
    et al., [2019](#bib.bib54)). As they come with different characteristics than
    general GNNs, it is likely that optimization techniques need to be adapted to
    those types of GNNs. Beyond that, deeper GNN architectures with numerous layers
    have recently been proposed (Li et al., [2020](#bib.bib107), [2021b](#bib.bib106);
    Liu et al., [2020](#bib.bib113)) which could pose new challenges for developing
    GNN systems. Another topic is the size of the feature vectors. When evaluating
    the systems, fixed feature vector lengths are used, but there is no consistent
    size within the presented systems. Now, the question is whether the systems perform
    equally well with variable feature vector sizes as the size of the feature vector
    might have a great effect on storage, data loading and computation time (Huang
    et al., [2021](#bib.bib81)). Moreover, the presented systems are all either based
    upon an existing graph processing system or a DL framework. Then, the missing
    operations needed for GNN training are added and optimized. Although this is an
    effective approach to design a GNN system, the characteristics of GNNs are not
    fully exploited. A GNN training iteration alternately incorporates dense computation
    of NN operations with regular memory access and a sparse aggregation phase with
    irregular memory accesses (Wang et al., [2021a](#bib.bib184)). Furthermore, the
    first layers of a GNN generally are the most compute intensive ones. Hence, it
    could be beneficial to adapt the system to handle the computations of the first
    layers differently compared to the subsequent ones. P³ is the first system going
    this direction, but it is difficult to find the optimal spot when to change from
    model to data parallelism and vice versa. In addition, finding a good way to partition
    and distribute the model among the machines is challenging. In general, such a
    method is not favorable if the underlying assumption that the activations are
    significantly smaller than the features is not met.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '由于GNN及其系统仍是新兴研究领域，仍然存在待解决的挑战。大多数系统在静态图和标准GNN模型（如GCN、GAT和GraphSAGE）上进行评估，但尚无关于这些系统在更专门化的模型和数据类型上的表现信息。例如，存在用于超图的GNN（Feng
    et al., [2019](#bib.bib42); Yadati et al., [2019](#bib.bib196)）和多图的GNN（Ouyang
    et al., [2019](#bib.bib136); Geng et al., [2019](#bib.bib54)）。由于它们具有与通用GNN不同的特性，因此优化技术可能需要调整以适应这些类型的GNN。此外，最近提出了具有众多层的更深GNN架构（Li
    et al., [2020](#bib.bib107), [2021b](#bib.bib106); Liu et al., [2020](#bib.bib113)），这可能会对开发GNN系统带来新的挑战。另一个话题是特征向量的大小。在评估系统时，使用了固定的特征向量长度，但所展示的系统中没有一致的大小。目前的问题是，系统在变量特征向量大小下的表现是否相同，因为特征向量的大小可能对存储、数据加载和计算时间有很大影响（Huang
    et al., [2021](#bib.bib81)）。此外，所展示的系统要么基于现有的图处理系统，要么基于DL框架。然后，添加并优化GNN训练所需的缺失操作。尽管这种方法在设计GNN系统时是有效的，但GNN的特性没有得到充分利用。GNN训练迭代交替进行密集的NN操作计算与常规内存访问，以及稀疏的聚合阶段与不规则内存访问（Wang
    et al., [2021a](#bib.bib184)）。此外，GNN的第一层通常是计算最密集的层。因此，将系统适应处理第一层计算与后续层不同可能是有益的。P³是首个朝着这个方向发展的系统，但找到从模型并行到数据并行的最佳转换时机以及反之亦然非常困难。此外，找到一种有效的方法来划分和分配模型到各台机器上也具有挑战性。一般来说，如果激活量显著小于特征量，则这种方法并不理想。 '
- en: 5\. Conclusions
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: GNNs are increasingly used in various fields of application. With the steadily
    growing size of real-world graphs, the need for large-scale GNN systems arises.
    To better comprehend the main concepts of GNN systems, we first provided an overview
    of two fundamental topics, systems for graph processing and systems for DNN training.
    We established connections between the used methods and showed that many ideas
    of GNN systems are inspired by the two related fields. In the main part of this
    survey, we discussed, categorized and compared concepts for distributed GNN training.
    This included partitioning strategies, sampling methods, different types of parallelism
    as well as efficient scheduling and caching. We further investigated datasets
    and benchmarks for evaluation as well as availability and compatibility of the
    systems. Although the current systems are able to scale to large graphs, there
    still are unresolved issues. For instance, the support for specialized GNN architectures
    like dynamic GNNs or those using hierarchical aggregation are not fully explored.
    Further, it should be investigated how to handle variable feature vector sizes
    as the systems only provide insights into a fixed length. When looking into the
    future, we see a trend in investigating system support for quantized GNNs as they
    are robust and only have a small memory footprint. Another interesting type of
    GNN architecture that could be increasingly supported are dynamic GNNs. The software-hardware
    co-design of systems is receiving growing attention and systems even more adapted
    to the characteristics of GNNs can further improve the performance. Lastly, a
    closer look at the data loading and preprocessing phase can help to minimize the
    overall training time.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs在各种应用领域中的使用越来越多。随着现实世界图的规模不断扩大，对大规模GNN系统的需求也随之增加。为了更好地理解GNN系统的主要概念，我们首先概述了两个基本主题，即图处理系统和DNN训练系统。我们建立了所用方法之间的联系，并展示了GNN系统的许多思想受到这两个相关领域的启发。在本调查的主要部分，我们讨论、分类并比较了分布式GNN训练的概念。这包括分区策略、采样方法、不同类型的并行性以及高效的调度和缓存。我们进一步调查了评估的数据集和基准，以及系统的可用性和兼容性。尽管当前的系统能够扩展到大规模图，但仍存在未解决的问题。例如，对专用GNN架构如动态GNN或使用层次聚合的GNN的支持尚未完全探讨。此外，还应研究如何处理变量特征向量大小，因为系统仅提供固定长度的见解。展望未来，我们看到一个趋势，即研究系统对量化GNN的支持，因为它们具有鲁棒性且内存占用小。另一个可能会得到越来越多支持的有趣GNN架构是动态GNN。系统的软硬件协同设计正受到越来越多的关注，更适应GNN特征的系统可以进一步提高性能。最后，仔细查看数据加载和预处理阶段可以帮助最小化整体训练时间。
- en: References
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abadal et al. (2021) Sergi Abadal, Akshay Jain, Robert Guirado, Jorge López-Alonso,
    and Eduard Alarcón. 2021. Computing graph neural networks: A survey from algorithms
    to accelerators. *ACM Comput. Surveys (CSUR)* 54, 9 (2021), 1–38.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abadal et al. (2021) Sergi Abadal, Akshay Jain, Robert Guirado, Jorge López-Alonso,
    和 Eduard Alarcón。2021。计算图神经网络：从算法到加速器的调查。*ACM计算机调查（CSUR）* 54, 9 (2021)，1–38。
- en: 'Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
    Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, et al. 2016. $\{$TensorFlow$\}$: A System for $\{$Large-Scale$\}$ Machine
    Learning. In *12th USENIX symposium on operating systems design and implementation
    (OSDI 16)*. 265–283.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy
    Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, 等。2016。$\{$TensorFlow$\}$：一个用于$\{$大规模$\}$机器学习的系统。发表于*第12届USENIX操作系统设计与实现研讨会（OSDI
    16）*。265–283。
- en: 'Abbas et al. (2018) Zainab Abbas, Vasiliki Kalavri, Paris Carbone, and Vladimir
    Vlassov. 2018. Streaming graph partitioning: an experimental study. *Proceedings
    of the VLDB Endowment* 11, 11 (2018), 1590–1603.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbas et al. (2018) Zainab Abbas, Vasiliki Kalavri, Paris Carbone, 和 Vladimir
    Vlassov。2018。流式图分区：一项实验研究。*VLDB年鉴会议录* 11, 11 (2018)，1590–1603。
- en: Alibaba (2020) Alibaba. 2020. Euler. [https://github.com/alibaba/euler](https://github.com/alibaba/euler).
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alibaba (2020) Alibaba。2020。Euler。 [https://github.com/alibaba/euler](https://github.com/alibaba/euler)。
- en: 'Arai et al. (2016) Junya Arai, Hiroaki Shiokawa, Takeshi Yamamuro, Makoto Onizuka,
    and Sotetsu Iwamura. 2016. Rabbit order: Just-in-time parallel reordering for
    fast graph analysis. In *2016 IEEE IPDPS*. IEEE, 22–31.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arai et al. (2016) Junya Arai, Hiroaki Shiokawa, Takeshi Yamamuro, Makoto Onizuka,
    和 Sotetsu Iwamura。2016。兔子排序：用于快速图分析的及时并行重排序。发表于*2016 IEEE IPDPS*。IEEE，22–31。
- en: Balaban (1985) Alexandru T Balaban. 1985. Applications of graph theory in chemistry.
    *J.of chemical information and computer sciences* 25, 3 (1985), 334–343.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balaban (1985) Alexandru T Balaban. 1985. 图论在化学中的应用。*化学信息与计算科学期刊* 25, 3 (1985),
    334–343。
- en: 'Batarfi et al. (2015) Omar Batarfi, Radwa El Shawi, Ayman G Fayoumi, Reza Nouri,
    Ahmed Barnawi, Sherif Sakr, et al. 2015. Large scale graph processing systems:
    survey and an experimental evaluation. *Cluster Computing* 18, 3 (2015), 1189–1213.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Batarfi 等 (2015) Omar Batarfi, Radwa El Shawi, Ayman G Fayoumi, Reza Nouri,
    Ahmed Barnawi, Sherif Sakr 等. 2015. 大规模图处理系统：调查与实验评估。*集群计算* 18, 3 (2015), 1189–1213。
- en: 'Ben-Nun and Hoefler (2019) Tal Ben-Nun and Torsten Hoefler. 2019. Demystifying
    parallel and distributed deep learning: An in-depth concurrency analysis. *ACM
    Comput. Surveys (CSUR)* 52, 4 (2019), 1–43.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben-Nun 和 Hoefler (2019) Tal Ben-Nun 和 Torsten Hoefler. 2019. 揭示并行和分布式深度学习的奥秘：深入的并发分析。*ACM
    计算机调查 (CSUR)* 52, 4 (2019), 1–43。
- en: 'Besta and Hoefler (2022) Maciej Besta and Torsten Hoefler. 2022. Parallel and
    Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. *arXiv preprint
    arXiv:2205.09702* (2022).'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta 和 Hoefler (2022) Maciej Besta 和 Torsten Hoefler. 2022. 并行和分布式图神经网络：深入的并发分析。*arXiv
    预印本 arXiv:2205.09702* (2022)。
- en: Blumofe and Leiserson (1999) Robert D Blumofe and Charles E Leiserson. 1999.
    Scheduling multithreaded computations by work stealing. *J. of the ACM (JACM)*
    46, 5 (1999), 720–748.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blumofe 和 Leiserson (1999) Robert D Blumofe 和 Charles E Leiserson. 1999. 通过工作窃取调度多线程计算。*ACM
    期刊 (JACM)* 46, 5 (1999), 720–748。
- en: Bresson and Laurent (2017) Xavier Bresson and Thomas Laurent. 2017. Residual
    gated graph convnets. *arXiv preprint arXiv:1711.07553* (2017).
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bresson 和 Laurent (2017) Xavier Bresson 和 Thomas Laurent. 2017. 残差门控图卷积网络。*arXiv
    预印本 arXiv:1711.07553* (2017)。
- en: Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020. Language models are few-shot learners. *arXiv preprint
    arXiv:2005.14165* (2020).
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等. 2020. 语言模型是少样本学习者。*arXiv 预印本 arXiv:2005.14165* (2020)。
- en: 'Bu et al. (2014) Yingyi Bu, Vinayak Borkar, Jianfeng Jia, Michael J Carey,
    and Tyson Condie. 2014. Pregelix: Big (ger) graph analytics on a dataflow engine.
    *arXiv preprint arXiv:1407.0455* (2014).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bu 等 (2014) Yingyi Bu, Vinayak Borkar, Jianfeng Jia, Michael J Carey, 和 Tyson
    Condie. 2014. Pregelix: 在数据流引擎上的大规模图分析。*arXiv 预印本 arXiv:1407.0455* (2014)。'
- en: Bundy and Wallen (1984) Alan Bundy and Lincoln Wallen. 1984. Breadth-first search.
    In *Catalogue of AI tools*. Springer, 13–13.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bundy 和 Wallen (1984) Alan Bundy 和 Lincoln Wallen. 1984. 广度优先搜索。收录于 *人工智能工具目录*。Springer,
    13–13。
- en: Cai et al. (2021) Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun
    Zha, Li Su, and Qingming Huang. 2021. Rethinking graph neural network search from
    message-passing. *arXiv e-prints* (2021), arXiv–2103.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等 (2021) Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha,
    Li Su, 和 Qingming Huang. 2021. 从消息传递重新思考图神经网络搜索。*arXiv 电子预印本* (2021), arXiv–2103。
- en: Chahal et al. (2020) Karanbir Singh Chahal, Manraj Singh Grover, Kuntal Dey,
    and Rajiv Ratn Shah. 2020. A hitchhiker’s guide on distributed training of deep
    neural networks. *J. Parallel and Distrib. Comput.* 137 (2020), 65–76.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chahal 等 (2020) Karanbir Singh Chahal, Manraj Singh Grover, Kuntal Dey, 和 Rajiv
    Ratn Shah. 2020. 分布式深度神经网络训练指南。*并行与分布式计算期刊* 137 (2020), 65–76。
- en: 'Chami et al. (2020) Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher
    Ré, and Kevin Murphy. 2020. Machine learning on graphs: A model and comprehensive
    taxonomy. *arXiv preprint arXiv:2005.03675* (2020).'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chami 等 (2020) Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Ré,
    和 Kevin Murphy. 2020. 图上的机器学习：模型和全面分类法。*arXiv 预印本 arXiv:2005.03675* (2020)。
- en: 'Chen et al. (2022) Cen Chen, Kenli Li, Yangfan Li, and Xiaofeng Zou. 2022.
    ReGNN: A Redundancy-Eliminated Graph Neural Networks Accelerator. In *2022 IEEE
    Int’l Symposium on High-Performance Computer Architecture (HPCA)*. IEEE, 429–443.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2022) Cen Chen, Kenli Li, Yangfan Li, 和 Xiaofeng Zou. 2022. ReGNN:
    一种去冗余的图神经网络加速器。收录于 *2022 IEEE 国际高性能计算架构研讨会 (HPCA)*。IEEE, 429–443。'
- en: 'Chen et al. (2018) Jie Chen, Tengfei Ma, and Cao Xiao. 2018. Fastgcn: fast
    learning with graph convolutional networks via importance sampling. *arXiv preprint
    arXiv:1801.10247* (2018).'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2018) Jie Chen, Tengfei Ma, 和 Cao Xiao. 2018. Fastgcn: 通过重要性采样加快图卷积网络的学习。*arXiv
    预印本 arXiv:1801.10247* (2018)。'
- en: Chen et al. (2016) Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and
    Rafal Jozefowicz. 2016. Revisiting distributed synchronous SGD. *arXiv preprint
    arXiv:1604.00981* (2016).
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2016) Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, 和 Rafal Jozefowicz.
    2016. 重访分布式同步 SGD。*arXiv 预印本 arXiv:1604.00981* (2016)。
- en: 'Chen et al. (2015b) Rong Chen, Jiaxin Shi, Yanzhe Chen, and Haibo Chen. 2015b.
    PowerLyra: Differentiated Graph Computation and Partitioning on Skewed Graphs.
    In *Proc. of the Tenth European Conf. on Computer Systems* *(EuroSys ’15)*. Association
    for Comput. Machinery, Article 1, 15 pages.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2015b）Rong Chen, Jiaxin Shi, Yanzhe Chen, 和 Haibo Chen。2015b年。PowerLyra：在倾斜图上进行差异化图计算和划分。在
    *第十届欧洲计算机系统会议* *(EuroSys ’15)* 中。计算机协会，文章1，15页。
- en: 'Chen et al. (2015a) Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie
    Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015a. Mxnet: A flexible
    and efficient machine learning library for heterogeneous distributed systems.
    *arXiv preprint arXiv:1512.01274* (2015).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2015a）Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang,
    Tianjun Xiao, Bing Xu, Chiyuan Zhang, 和 Zheng Zhang。2015a年。Mxnet：一种用于异构分布式系统的灵活高效的机器学习库。*arXiv
    预印本 arXiv:1512.01274*（2015）。
- en: 'Chen et al. (2021) Xiaobing Chen, Yuke Wang, Xinfeng Xie, Xing Hu, Abanti Basak,
    Ling Liang, Mingyu Yan, Lei Deng, Yufei Ding, Zidong Du, et al. 2021. Rubik: A
    hierarchical architecture for efficient graph neural network training. *IEEE Trans.
    on Computer-Aided Design of Integrated Circuits and Systems* (2021).'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021）Xiaobing Chen, Yuke Wang, Xinfeng Xie, Xing Hu, Abanti Basak, Ling
    Liang, Mingyu Yan, Lei Deng, Yufei Ding, Zidong Du 等。2021年。Rubik：一种高效图神经网络训练的层次结构。*IEEE
    计算机辅助设计集成电路与系统学报* （2021）。
- en: 'Chiang et al. (2019) Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio,
    and Cho-Jui Hsieh. 2019. Cluster-gcn: An efficient algorithm for training deep
    and large graph convolutional networks. In *Proc. of the 25th ACM SIGKDD Int’l
    Conf. on Knowledge Discovery & Data Mining*. 257–266.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 等（2019）Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, 和 Cho-Jui
    Hsieh。2019年。Cluster-gcn：一种用于训练深度和大规模图卷积网络的高效算法。在 *第25届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*
    中。257–266。
- en: 'Chilimbi et al. (2014) Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and
    Karthik Kalyanaraman. 2014. Project adam: Building an efficient and scalable deep
    learning training system. In *11th $\{$USENIX$\}$ Symposium on Operating Systems
    Design and Implementation ($\{$OSDI$\}$ 14)*. 571–582.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chilimbi 等（2014）Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, 和 Karthik
    Kalyanaraman。2014年。Project adam：构建一个高效且可扩展的深度学习训练系统。在 *第11届 USENIX 操作系统设计与实现研讨会（OSDI
    14）* 中。571–582。
- en: Cipar et al. (2013) James Cipar, Qirong Ho, Jin Kyu Kim, Seunghak Lee, Gregory R
    Ganger, Garth Gibson, Kimberly Keeton, and Eric Xing. 2013. Solving the straggler
    problem with bounded staleness. In *14th Workshop on Hot Topics in Operating Systems
    (HotOS $\{$XIV$\}$)*.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cipar 等（2013）James Cipar, Qirong Ho, Jin Kyu Kim, Seunghak Lee, Gregory R Ganger,
    Garth Gibson, Kimberly Keeton, 和 Eric Xing。2013年。解决拖延问题的有界陈旧性。在 *第14届操作系统热点问题研讨会（HotOS
    XIV）* 中。
- en: Coimbra et al. (2021) Miguel E Coimbra, Alexandre P Francisco, and Luís Veiga.
    2021. An analysis of the graph processing landscape. *J. of big Data* 8, 1 (2021),
    1–41.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coimbra 等（2021）Miguel E Coimbra, Alexandre P Francisco, 和 Luís Veiga。2021年。图处理领域的分析。*大数据杂志*
    8, 1 (2021), 1–41。
- en: Czerwionka et al. (2011) Paul Czerwionka, Miao Wang, and Fabian Wiesel. 2011.
    Optimized route network graph as map reference for autonomous cars operating on
    german autobahn. In *The 5th Int’l Conf. on Autom., Robotics and Appl.* IEEE,
    78–83.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czerwionka 等（2011）Paul Czerwionka, Miao Wang, 和 Fabian Wiesel。2011年。优化的路网图作为在德国高速公路上运行的自动驾驶汽车的地图参考。在
    *第5届国际自动化、机器人与应用会议* 中。IEEE，78–83。
- en: Dean et al. (2012) Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu
    Devin, Mark Mao, Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al.
    2012. Large scale distributed deep networks. *Advances in neural information processing
    systems* 25 (2012), 1223–1231.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dean 等（2012）Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin,
    Mark Mao, Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang 等。2012年。大规模分布式深度网络。*神经信息处理系统进展*
    25（2012），1223–1231。
- en: 'Dean and Ghemawat (2008) Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce:
    simplified data processing on large clusters. *Commun. ACM* 51, 1 (2008), 107–113.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dean 和 Ghemawat（2008）Jeffrey Dean 和 Sanjay Ghemawat。2008年。MapReduce：简化大规模集群数据处理。*ACM
    通讯* 51, 1 (2008), 107–113。
- en: Dijkstra et al. (1959) Edsger W Dijkstra et al. 1959. A note on two problems
    in connexion with graphs. *Numer. math.* 1, 1 (1959), 269–271.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dijkstra 等（1959）Edsger W Dijkstra 等。1959年。关于图相关的两个问题的备注。*Numer. math.* 1, 1
    (1959), 269–271。
- en: 'Ding et al. (2021) Mucong Ding, Kezhi Kong, Jingling Li, Chen Zhu, John Dickerson,
    Furong Huang, and Tom Goldstein. 2021. VQ-GNN: A Universal Framework to Scale
    up Graph Neural Networks using Vector Quantization. *Advances in Neural Information
    Processing Systems* 34 (2021).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding 等人 (2021) Mucong Ding, Kezhi Kong, Jingling Li, Chen Zhu, John Dickerson,
    Furong Huang, 和 Tom Goldstein. 2021. VQ-GNN: 使用向量量化扩展图神经网络的通用框架。*神经信息处理系统进展* 34
    (2021)。'
- en: Dominguez-Sal et al. (2010) David Dominguez-Sal, Norbert Martinez-Bazan, Victor
    Muntes-Mulero, Pere Baleta, and Josep Lluis Larriba-Pey. 2010. A discussion on
    the design of graph database benchmarks. In *Technology Conf. on Performance Evaluation
    and Benchmarking*. Springer, 25–40.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dominguez-Sal 等人 (2010) David Dominguez-Sal, Norbert Martinez-Bazan, Victor
    Muntes-Mulero, Pere Baleta, 和 Josep Lluis Larriba-Pey. 2010. 关于图数据库基准测试设计的讨论。在
    *性能评估与基准测试技术会议*。Springer, 25–40。
- en: 'Dutta et al. (2018) Sanghamitra Dutta, Gauri Joshi, Soumyadip Ghosh, Parijat
    Dube, and Priya Nagpurkar. 2018. Slow and stale gradients can win the race: Error-runtime
    trade-offs in distributed SGD. In *Int’l Conf. on AI and Statistics*. PMLR, 803–812.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dutta 等人 (2018) Sanghamitra Dutta, Gauri Joshi, Soumyadip Ghosh, Parijat Dube,
    和 Priya Nagpurkar. 2018. 慢而陈旧的梯度可以赢得比赛：分布式 SGD 的错误-运行时权衡。在 *人工智能与统计国际会议*。PMLR,
    803–812。
- en: Fan (2012) Wenfei Fan. 2012. Graph pattern matching revised for social network
    analysis. In *Proc. of the 15th Int’l Conf. on Database Theory*. 8–21.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan (2012) Wenfei Fan. 2012. 针对社交网络分析修订的图模式匹配。在 *第15届国际数据库理论会议论文集*。8–21。
- en: Fan et al. (2020) Wenfei Fan, Ping Lu, Wenyuan Yu, Jingbo Xu, Qiang Yin, Xiaojian
    Luo, Jingren Zhou, and Ruochun Jin. 2020. Adaptive asynchronous parallelization
    of graph algorithms. *ACM Trans. on Database Systems (TODS)* 45, 2 (2020), 1–45.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人 (2020) Wenfei Fan, Ping Lu, Wenyuan Yu, Jingbo Xu, Qiang Yin, Xiaojian
    Luo, Jingren Zhou, 和 Ruochun Jin. 2020. 图算法的自适应异步并行化。*ACM 数据库系统学报 (TODS)* 45,
    2 (2020), 1–45。
- en: Fan et al. (2019) Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang,
    and Dawei Yin. 2019. Graph neural networks for social recommendation. In *The
    world wide web conf.* 417–426.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人 (2019) Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang,
    和 Dawei Yin. 2019. 社会推荐的图神经网络。在 *全球信息网会议* 417–426。
- en: 'Fan et al. (2017) Wenfei Fan, Jingbo Xu, Yinghui Wu, Wenyuan Yu, and Jiaxin
    Jiang. 2017. GRAPE: Parallelizing sequential graph computations. *Proc. of the
    VLDB Endowment* 10, 12 (2017), 1889–1892.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan 等人 (2017) Wenfei Fan, Jingbo Xu, Yinghui Wu, Wenyuan Yu, 和 Jiaxin Jiang.
    2017. GRAPE: 并行化顺序图计算。*VLDB 基金会会议论文集* 10, 12 (2017), 1889–1892。'
- en: Fan et al. (2022) Wenfei Fan, Ruiqi Xu, Qiang Yin, Wenyuan Yu, and Jingren Zhou.
    2022. Application-driven graph partitioning. *The VLDB Journal* (2022), 1–24.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人 (2022) Wenfei Fan, Ruiqi Xu, Qiang Yin, Wenyuan Yu, 和 Jingren Zhou. 2022.
    应用驱动的图划分。*VLDB 期刊* (2022), 1–24。
- en: 'Feng et al. (2020) Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, and
    Yufei Ding. 2020. Sgquant: Squeezing the last bit on graph neural networks with
    specialized quantization. In *2020 IEEE 32nd Int’l Conf. on Tools with AI (ICTAI)*.
    IEEE, 1044–1052.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng 等人 (2020) Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, 和 Yufei
    Ding. 2020. Sgquant: 使用专门量化技术在图神经网络上挤压最后一位。在 *2020 IEEE 第32届国际人工智能工具会议 (ICTAI)*。IEEE,
    1044–1052。'
- en: Feng et al. (2019) Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue
    Gao. 2019. Hypergraph neural networks. In *Proc. of the AAAI Conf. on AI*, Vol. 33\.
    3558–3565.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人 (2019) Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, 和 Yue Gao.
    2019. 超图神经网络。在 *AAAI 人工智能会议论文集*，第33卷。3558–3565。
- en: Fey and Lenssen (2019) Matthias Fey and Jan Eric Lenssen. 2019. Fast graph representation
    learning with PyTorch Geometric. *arXiv preprint arXiv:1903.02428* (2019).
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fey 和 Lenssen (2019) Matthias Fey 和 Jan Eric Lenssen. 2019. 使用 PyTorch Geometric
    的快速图表示学习。*arXiv 预印本 arXiv:1903.02428* (2019)。
- en: 'Fey et al. (2021) Matthias Fey, Jan E Lenssen, Frank Weichert, and Jure Leskovec.
    2021. Gnnautoscale: Scalable and expressive graph neural networks via historical
    embeddings. In *Int’l Conf. on ML*. PMLR, 3294–3304.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fey 等人 (2021) Matthias Fey, Jan E Lenssen, Frank Weichert, 和 Jure Leskovec.
    2021. Gnnautoscale: 通过历史嵌入实现可扩展和表达力强的图神经网络。在 *国际机器学习会议*。PMLR, 3294–3304。'
- en: FOUNDATION (2012) APACHE SOFTWARE FOUNDATION. 2012. Giraph. [https://giraph.apache.org/](https://giraph.apache.org/).
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FOUNDATION (2012) APACHE 软件基金会. 2012. Giraph. [https://giraph.apache.org/](https://giraph.apache.org/)。
- en: 'Fu and He (2021) Dongqi Fu and Jingrui He. 2021. SDG: A Simplified and Dynamic
    Graph Neural Network. In *Proc. of the 44th Int’l ACM SIGIR Conf. on Research
    and Development in Information Retrieval*. 2273–2277.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 和 He (2021) Dongqi Fu 和 Jingrui He. 2021. SDG: 简化且动态的图神经网络。在 *第44届国际 ACM
    SIGIR 信息检索研究与开发会议论文集*。2273–2277。'
- en: 'Fu et al. (2020) Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. 2020.
    Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding.
    In *Proc. of The Web Conf. 2020*. 2331–2341.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu et al. (2020) Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. 2020.
    MAGNN: 用于异构图嵌入的元路径聚合图神经网络。发表于*2020年网络会议论文集*。2331–2341。'
- en: 'Gandhi and Iyer (2021) Swapnil Gandhi and Anand Padmanabha Iyer. 2021. P3:
    Distributed deep graph learning at scale. In *15th $\{$USENIX$\}$ Symposium on
    Operating Systems Design and Implementation ($\{$OSDI$\}$ 21)*. 551–568.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gandhi and Iyer (2021) Swapnil Gandhi and Anand Padmanabha Iyer. 2021. P3:
    大规模分布式深度图学习。发表于*第15届 $\{$USENIX$\}$ 操作系统设计与实现研讨会 ($\{$OSDI$\}$ 21)*。551–568。'
- en: Gao et al. (2020a) Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, and Xilin
    Chen. 2020a. Multi-modal graph neural network for joint reasoning on vision and
    scene text. In *Proc. of the IEEE/CVF Conf. on CV and Pattern Recognition*. 12746–12756.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2020a) Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, and Xilin
    Chen. 2020a. 用于视觉和场景文本联合推理的多模态图神经网络。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*。12746–12756。
- en: 'Gao et al. (2020b) Jianliang Gao, Tengfei Lyu, Fan Xiong, Jianxin Wang, Weimao
    Ke, and Zhao Li. 2020b. Mgnn: A multimodal graph neural network for predicting
    the survival of cancer patients. In *Proc. of the 43rd Int’l ACM SIGIR Conf. on
    Research and Development in Information Retrieval*. 1697–1700.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. (2020b) Jianliang Gao, Tengfei Lyu, Fan Xiong, Jianxin Wang, Weimao
    Ke, and Zhao Li. 2020b. MGNN: 一种多模态图神经网络用于预测癌症患者的生存。发表于*第43届国际ACM SIGIR信息检索研究与开发会议论文集*。1697–1700。'
- en: Gao et al. (2022) Xinyi Gao, Wentao Zhang, Yingxia Shao, Quoc Viet Hung Nguyen,
    Bin Cui, and Hongzhi Yin. 2022. Efficient Graph Neural Network Inference at Large
    Scale. *arXiv preprint arXiv:2211.00495* (2022).
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2022) Xinyi Gao, Wentao Zhang, Yingxia Shao, Quoc Viet Hung Nguyen,
    Bin Cui, and Hongzhi Yin. 2022. 高效的大规模图神经网络推断。*arXiv 预印本 arXiv:2211.00495* (2022)。
- en: Gao et al. (2020c) Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu.
    2020c. Graph Neural Architecture Search.. In *IJCAI*, Vol. 20. 1403–1409.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2020c) Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu.
    2020c. 图神经网络架构搜索。发表于*IJCAI*，第20卷。1403–1409。
- en: Gaudelet et al. (2021) Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman,
    Cristian Regep, Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts,
    Jian Tang, et al. 2021. Utilizing graph machine learning within drug discovery
    and development. *Briefings in bioinformatics* 22, 6 (2021), bbab159.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaudelet et al. (2021) Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman,
    Cristian Regep, Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts,
    Jian Tang, et al. 2021. 在药物发现和开发中利用图机器学习。*生物信息学简报* 22, 6 (2021), bbab159。
- en: Geng et al. (2019) Xu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, Qiang Yang,
    Jieping Ye, and Yan Liu. 2019. Spatiotemporal multi-graph convolution network
    for ride-hailing demand forecasting. In *Proc. of the AAAI conf. on AI*, Vol. 33\.
    3656–3663.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geng et al. (2019) Xu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, Qiang Yang,
    Jieping Ye, and Yan Liu. 2019. 用于叫车需求预测的时空多图卷积网络。发表于*AAAI人工智能会议论文集*，第33卷。3656–3663。
- en: 'Giles et al. (1998) C Lee Giles, Kurt D Bollacker, and Steve Lawrence. 1998.
    CiteSeer: An automatic citation indexing system. In *Proc. of the third ACM conf.
    on Digital libraries*. 89–98.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Giles et al. (1998) C Lee Giles, Kurt D Bollacker, and Steve Lawrence. 1998.
    CiteSeer: 一种自动化的引用索引系统。发表于*第三届ACM数字图书馆会议论文集*。89–98。'
- en: Gill et al. (2018) Gurbinder Gill, Roshan Dathathri, Loc Hoang, and Keshav Pingali.
    2018. A study of partitioning policies for graph analytics on large-scale distributed
    platforms. *Proceedings of the VLDB Endowment* 12, 4 (2018), 321–334.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gill et al. (2018) Gurbinder Gill, Roshan Dathathri, Loc Hoang, and Keshav Pingali.
    2018. 对大规模分布式平台上图分析分区策略的研究。*VLDB 结束会会议录* 12, 4 (2018), 321–334。
- en: Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol
    Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry.
    In *Int’l conf. on ML*. PMLR, 1263–1272.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol
    Vinyals, and George E Dahl. 2017. 用于量子化学的神经消息传递。发表于*国际机器学习会议*。PMLR，1263–1272。
- en: Girvan and Newman (2002) Michelle Girvan and Mark EJ Newman. 2002. Community
    structure in social and biological networks. *Proc. of the national academy of
    sciences* 99, 12 (2002), 7821–7826.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girvan and Newman (2002) Michelle Girvan and Mark EJ Newman. 2002. 社会和生物网络中的社区结构。*国家科学院院刊*
    99, 12 (2002), 7821–7826。
- en: 'Gonzalez et al. (2012) Joseph E Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson,
    and Carlos Guestrin. 2012. Powergraph: Distributed graph-parallel computation
    on natural graphs. In *10th $\{$USENIX$\}$ Symposium on Operating Systems Design
    and Implementation ($\{$OSDI$\}$ 12)*. 17–30.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gonzalez 等（2012）**Joseph E Gonzalez**、**Yucheng Low**、**Haijie Gu**、**Danny
    Bickson** 和 **Carlos Guestrin**。2012。Powergraph：自然图上的分布式图并行计算。见 *第10届USENIX操作系统设计与实现研讨会（OSDI
    12）*。17–30。
- en: Goodfellow et al. (2016) Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
    2016. Deep feedforward networks. *Deep learning* (2016), 164–223.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2016）**Ian Goodfellow**、**Yoshua Bengio** 和 **Aaron Courville**。2016。深度前馈网络。*深度学习*（2016），164–223。
- en: Gori et al. (2005) Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005.
    A new model for learning in graph domains. In *Proceedings. 2005 IEEE Int’l Joint
    Conf. on Neural Networks, 2005.*, Vol. 2\. IEEE, 729–734.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gori 等（2005）**Marco Gori**、**Gabriele Monfardini** 和 **Franco Scarselli**。2005。图域学习的新模型。见
    *2005 IEEE 国际联合神经网络会议论文集，2005*，第2卷。IEEE，729–734。
- en: 'Guan et al. (2022) Mingyu Guan, Anand Padmanabha Iyer, and Taesoo Kim. 2022.
    DynaGraph: dynamic graph neural networks at scale. In *Proc. of the 5th ACM SIGMOD
    Joint Int’l Workshop on Graph Data Management Experiences & Systems (GRADES) and
    Network Data Analytics (NDA)*. 1–10.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guan 等（2022）**Mingyu Guan**、**Anand Padmanabha Iyer** 和 **Taesoo Kim**。2022。DynaGraph：大规模动态图神经网络。见
    *第5届 ACM SIGMOD 联合国际图数据管理经验与系统研讨会（GRADES）和网络数据分析（NDA）*。1–10。
- en: 'Gui et al. (2019) Chuang-Yi Gui, Long Zheng, Bingsheng He, Cheng Liu, Xin-Yu
    Chen, Xiao-Fei Liao, and Hai Jin. 2019. A survey on graph processing accelerators:
    Challenges and opportunities. *J. of Comp. Science and Tech.* 34, 2 (2019), 339–371.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gui 等（2019）**Chuang-Yi Gui**、**Long Zheng**、**Bingsheng He**、**Cheng Liu**、**Xin-Yu
    Chen**、**Xiao-Fei Liao** 和 **Hai Jin**。2019。图处理加速器综述：挑战与机遇。*计算机科学与技术杂志* 34，第2期（2019），339–371。
- en: Guo et al. (2020) Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie,
    Hui Xiong, and Qing He. 2020. A survey on knowledge graph-based recommender systems.
    *IEEE Trans. on Knowledge and Data Engineering* (2020).
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2020）**Qingyu Guo**、**Fuzhen Zhuang**、**Chuan Qin**、**Hengshu Zhu**、**Xing
    Xie**、**Hui Xiong** 和 **Qing He**。2020。基于知识图谱的推荐系统综述。*IEEE 知识与数据工程学报*（2020）。
- en: 'Gupta et al. (2013) Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong
    Wang, and Reza Zadeh. 2013. Wtf: The who to follow service at twitter. In *Proc.
    of the 22nd int’l conf. on World Wide Web*. 505–514.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等（2013）**Pankaj Gupta**、**Ashish Goel**、**Jimmy Lin**、**Aneesh Sharma**、**Dong
    Wang** 和 **Reza Zadeh**。2013。Wtf：Twitter上的关注服务。见 *第22届国际万维网会议论文集*。505–514。
- en: Hamilton (2020) William L Hamilton. 2020. Graph representation learning. *Synthesis
    Lectures on AI and ML* 14, 3 (2020), 1–159.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton（2020）**William L Hamilton**。2020。图表示学习。*合成讲座：人工智能与机器学习* 14，第3期（2020），1–159。
- en: Hamilton et al. (2017a) William L Hamilton, Rex Ying, and Jure Leskovec. 2017a.
    Inductive representation learning on large graphs. In *Proc. of the 31st Int’l
    Conf. on Neural Information Processing Systems*. 1025–1035.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton 等（2017a）**William L Hamilton**、**Rex Ying** 和 **Jure Leskovec**。2017a。在大型图上的归纳表示学习。见
    *第31届国际神经信息处理系统大会论文集*。1025–1035。
- en: 'Hamilton et al. (2017b) William L Hamilton, Rex Ying, and Jure Leskovec. 2017b.
    Representation learning on graphs: Methods and applications. *arXiv preprint arXiv:1709.05584*
    (2017).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton 等（2017b）**William L Hamilton**、**Rex Ying** 和 **Jure Leskovec**。2017b。图上的表示学习：方法和应用。*arXiv
    预印本 arXiv:1709.05584*（2017）。
- en: Hanai et al. (2019) Masatoshi Hanai, Toyotaro Suzumura, Wen Jun Tan, Elvis Liu,
    Georgios Theodoropoulos, and Wentong Cai. 2019. Distributed edge partitioning
    for trillion-edge graphs. *arXiv preprint arXiv:1908.05855* (2019).
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanai 等（2019）**Masatoshi Hanai**、**Toyotaro Suzumura**、**Wen Jun Tan**、**Elvis
    Liu**、**Georgios Theodoropoulos** 和 **Wentong Cai**。2019。分布式边缘划分用于万亿边图。*arXiv
    预印本 arXiv:1908.05855*（2019）。
- en: He et al. (2018) Anfeng He, Chong Luo, Xinmei Tian, and Wenjun Zeng. 2018. A
    twofold siamese network for real-time object tracking. In *Proc. of the IEEE Conf.
    on Computer Vision and Pattern Recognition*. 4834–4843.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2018）**Anfeng He**、**Chong Luo**、**Xinmei Tian** 和 **Wenjun Zeng**。2018。用于实时目标跟踪的双重孪生网络。见
    *IEEE计算机视觉与模式识别会议论文集*。4834–4843。
- en: 'Heidari et al. (2018) Safiollah Heidari, Yogesh Simmhan, Rodrigo N Calheiros,
    and Rajkumar Buyya. 2018. Scalable graph processing frameworks: A taxonomy and
    open challenges. *ACM Computing Surveys (CSUR)* 51, 3 (2018), 1–53.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heidari 等（2018）**Safiollah Heidari**、**Yogesh Simmhan**、**Rodrigo N Calheiros**
    和 **Rajkumar Buyya**。2018。可扩展图处理框架：分类与开放挑战。*ACM 计算调查（CSUR）* 51，第3期（2018），1–53。
- en: Hendrickson and Leland (1993) Bruce Hendrickson and Robert Leland. 1993. *The
    chaco users guide. version 1.0*. Technical Report. Sandia National Labs., Albuquerque,
    NM (United States).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrickson 和 Leland（1993）**Bruce Hendrickson** 和 **Robert Leland**。1993。*The
    chaco users guide. version 1.0*。技术报告。**Sandia National Labs.**，**Albuquerque,
    NM（美国）**。
- en: Ho et al. (2013) Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu
    Kim, Phillip B Gibbons, Garth A Gibson, Greg Ganger, and Eric P Xing. 2013. More
    effective distributed ml via a stale synchronous parallel parameter server. In
    *Advances in neural information processing systems*. 1223–1231.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho et al. (2013) Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu
    Kim, Phillip B Gibbons, Garth A Gibson, Greg Ganger, 和 Eric P Xing. 2013. 通过过时的同步并行参数服务器实现更有效的分布式机器学习。见于
    *神经信息处理系统进展*。1223–1231。
- en: Hoang et al. (2021) Loc Hoang, Xuhao Chen, Hochan Lee, Roshan Dathathri, Gurbinder
    Gill, and Keshav Pingali. 2021. Efficient distribution for deep learning on large
    graphs. *update* 1050 (2021), 1.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoang et al. (2021) Loc Hoang, Xuhao Chen, Hochan Lee, Roshan Dathathri, Gurbinder
    Gill, 和 Keshav Pingali. 2021. 大型图上的深度学习高效分布。*update* 1050 (2021)，1。
- en: 'Hoang et al. (2019) Loc Hoang, Roshan Dathathri, Gurbinder Gill, and Keshav
    Pingali. 2019. Cusp: A customizable streaming edge partitioner for distributed
    graph analytics. In *2019 IEEE Int’l Parallel and Distrib. Proc. Symp. (IPDPS)*.
    IEEE, 439–450.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoang et al. (2019) Loc Hoang, Roshan Dathathri, Gurbinder Gill, 和 Keshav Pingali.
    2019. Cusp：用于分布式图分析的可定制流处理边分区器。见于 *2019 IEEE 国际并行和分布式处理研讨会 (IPDPS)*。IEEE，439–450。
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long short-term memory. *Neural computation* 9, 8 (1997), 1735–1780.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter and Schmidhuber (1997) Sepp Hochreiter 和 Jürgen Schmidhuber. 1997.
    长短期记忆。*神经计算* 9, 8 (1997)，1735–1780。
- en: 'Hu et al. (2021) Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong,
    and Jure Leskovec. 2021. Ogb-lsc: A large-scale challenge for machine learning
    on graphs. *arXiv preprint arXiv:2103.09430* (2021).'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2021) Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong,
    和 Jure Leskovec. 2021. Ogb-lsc：图上机器学习的大规模挑战。*arXiv 预印本 arXiv:2103.09430* (2021)。
- en: 'Hu et al. (2020a) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020a. Open graph benchmark:
    Datasets for machine learning on graphs. *Advances in neural information processing
    systems* 33 (2020), 22118–22133.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2020a) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, 和 Jure Leskovec. 2020a. 开放图基准：用于图的机器学习数据集。*神经信息处理系统进展*
    33 (2020)，22118–22133。
- en: 'Hu et al. (2020b) Yuwei Hu, Zihao Ye, Minjie Wang, Jiali Yu, Da Zheng, Mu Li,
    Zheng Zhang, Zhiru Zhang, and Yida Wang. 2020b. Featgraph: A flexible and efficient
    backend for graph neural network systems. In *SC20: Int’l Conf. for High Performance
    Comput., Networking, Storage and Analysis*. IEEE, 1–13.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2020b) Yuwei Hu, Zihao Ye, Minjie Wang, Jiali Yu, Da Zheng, Mu Li,
    Zheng Zhang, Zhiru Zhang, 和 Yida Wang. 2020b. Featgraph：图神经网络系统的灵活高效后端。见于 *SC20：高性能计算、网络、存储和分析国际会议*。IEEE，1–13。
- en: Huan et al. (2021) ZHAO Huan, YAO Quanming, and TU Weiwei. 2021. Search to aggregate
    neighborhood for graph neural network. In *2021 IEEE 37th Int’l Conf. on Data
    Engineering (ICDE)*. IEEE, 552–563.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huan et al. (2021) ZHAO Huan, YAO Quanming, 和 TU Weiwei. 2021. 搜索以聚合邻域用于图神经网络。见于
    *2021 IEEE 第37届国际数据工程会议 (ICDE)*。IEEE，552–563。
- en: Huang et al. (2021) Kezhao Huang, Jidong Zhai, Zhen Zheng, Youngmin Yi, and
    Xipeng Shen. 2021. Understanding and bridging the gaps in current GNN performance
    optimizations. In *Proc. of the 26th ACM SIGPLAN Symposium on PPoPP*. 119–132.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2021) Kezhao Huang, Jidong Zhai, Zhen Zheng, Youngmin Yi, 和 Xipeng
    Shen. 2021. 理解和弥合当前GNN性能优化中的差距。见于 *第26届ACM SIGPLAN PPoPP研讨会论文集*。119–132。
- en: 'Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,
    Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al.
    2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism.
    *Advances in neural information processing systems* 32 (2019), 103–112.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,
    Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, 等.
    2019. Gpipe：使用流水线并行ism高效训练大型神经网络。*神经信息处理系统进展* 32 (2019)，103–112。
- en: Huang et al. (2002) Zan Huang, Wingyan Chung, Thian-Huat Ong, and Hsinchun Chen.
    2002. A graph-based recommender system for digital library. In *Proc. of the 2nd
    ACM/IEEE-CS joint conf. on Digital libraries*. 65–73.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2002) Zan Huang, Wingyan Chung, Thian-Huat Ong, 和 Hsinchun Chen.
    2002. 基于图的数字图书馆推荐系统。见于 *第2届ACM/IEEE-CS联合数字图书馆会议论文集*。65–73。
- en: 'Iyer et al. (2018a) Anand Padmanabha Iyer, Zaoxing Liu, Xin Jin, Shivaram Venkataraman,
    Vladimir Braverman, and Ion Stoica. 2018a. $\{$ASAP$\}$: Fast, approximate graph
    pattern mining at scale. In *13th USENIX Symposium on Operating Systems Design
    and Implementation (OSDI 18)*. 745–761.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyer et al. (2018a) Anand Padmanabha Iyer, Zaoxing Liu, Xin Jin, Shivaram Venkataraman,
    Vladimir Braverman, 和 Ion Stoica. 2018a. $\{$ASAP$\}$：大规模快速近似图模式挖掘。见于 *第13届USENIX操作系统设计与实现研讨会
    (OSDI 18)*。745–761。
- en: 'Iyer et al. (2018b) Anand Padmanabha Iyer, Aurojit Panda, Shivaram Venkataraman,
    Mosharaf Chowdhury, Aditya Akella, Scott Shenker, and Ion Stoica. 2018b. Bridging
    the GAP: towards approximate graph analytics. In *Proc. of the 1st ACM SIGMOD
    Joint Int’l Workshop on Graph Data Management Experiences & Systems (GRADES) and
    Network Data Analytics (NDA)*. 1–5.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyer et al. (2018b) 阿南德·帕德马纳巴·艾耶尔、奥罗吉特·潘达、希瓦拉姆·文卡塔拉曼、莫沙拉夫·乔杜里、阿迪提亚·阿凯拉、斯科特·申克和艾昂·斯托伊卡。2018b年。《弥合差距：迈向近似图分析》。在*第1届ACM
    SIGMOD联合国际研讨会，图数据管理经验与系统（GRADES）和网络数据分析（NDA）*上。1–5。
- en: 'Jain et al. (1996) Anil K Jain, Jianchang Mao, and K Moidin Mohiuddin. 1996.
    Artificial neural networks: A tutorial. *Computer* 29, 3 (1996), 31–44.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain et al. (1996) 阿尼尔·K·贾因、简昌·毛和K·莫伊丁·穆希丁。1996年。《人工神经网络：教程》。*计算机* 29, 3（1996年），31–44。
- en: Jangda et al. (2021) Abhinav Jangda, Sandeep Polisetty, Arjun Guha, and Marco
    Serafini. 2021. Accelerating graph sampling for graph machine learning using GPUs.
    In *Proc. of the Sixteenth European Conf. on Computer Systems*. 311–326.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jangda et al. (2021) 阿宾纳夫·江达、桑迪普·波利塞提、阿尔君·古哈和马尔科·塞拉菲尼。2021年。《利用GPU加速图采样以进行图机器学习》。在*第十六届欧洲计算机系统会议*上。311–326。
- en: Jia et al. (2020a) Zhihao Jia, Sina Lin, Mingyu Gao, Matei Zaharia, and Alex
    Aiken. 2020a. Improving the accuracy, scalability, and performance of graph neural
    networks with roc. *Proc. of ML and Systems* 2 (2020), 187–198.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia et al. (2020a) 直昊·贾、斯纳·林、明宇·高、马泰·扎哈里亚和亚历克斯·艾肯。2020a年。《通过ROC提高图神经网络的准确性、可扩展性和性能》。*机器学习与系统会议论文集*
    2（2020年），187–198。
- en: Jia et al. (2020b) Zhihao Jia, Sina Lin, Rex Ying, Jiaxuan You, Jure Leskovec,
    and Alex Aiken. 2020b. Redundancy-free computation for graph neural networks.
    In *Proc. of the 26th ACM SIGKDD Int’l Conf. on Knowledge Discovery & Data Mining*.
    997–1005.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia et al. (2020b) 直昊·贾、斯纳·林、雷克斯·英、贾轩·尤、尤尔·莱斯科维奇和亚历克斯·艾肯。2020b年。《无冗余计算的图神经网络》。在*第26届ACM
    SIGKDD国际知识发现与数据挖掘会议*上。997–1005。
- en: Jin et al. (2016) Peter H Jin, Qiaochu Yuan, Forrest Iandola, and Kurt Keutzer.
    2016. How to scale distributed deep learning? *arXiv preprint arXiv:1611.04581*
    (2016).
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. (2016) 彼得·H·金、丘晓楚、福雷斯特·伊安多拉和库尔特·凯特泽。2016年。《如何扩展分布式深度学习？》*arXiv预印本
    arXiv:1611.04581*（2016年）。
- en: Kalavri et al. (2017) Vasiliki Kalavri, Vladimir Vlassov, and Seif Haridi. 2017.
    High-level programming abstractions for distributed graph processing. *IEEE Trans.
    on Knowledge and Data Engineering* 30, 2 (2017), 305–324.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalavri et al. (2017) 瓦西利基·卡拉夫里、弗拉基米尔·弗拉索夫和赛义夫·哈里迪。2017年。《分布式图处理的高级编程抽象》。*IEEE知识与数据工程汇刊*
    30, 2（2017年），305–324。
- en: 'Karypis (1997) George Karypis. 1997. METIS: Unstructured graph partitioning
    and sparse matrix ordering system. *Tech. report* (1997).'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karypis (1997) 乔治·卡里皮斯。1997年。《METIS：非结构化图划分和稀疏矩阵排序系统》。*技术报告*（1997年）。
- en: Karypis and Kumar (1998a) George Karypis and Vipin Kumar. 1998a. A fast and
    high quality multilevel scheme for partitioning irregular graphs. *SIAM J. on
    scientific Comput.* 20, 1 (1998), 359–392.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karypis and Kumar (1998a) 乔治·卡里皮斯和维平·库马尔。1998a年。《一种快速且高质量的多级方案用于不规则图的划分》。*SIAM
    J. on scientific Comput.* 20, 1（1998年），359–392。
- en: 'Karypis and Kumar (1998b) George Karypis and Vipin Kumar. 1998b. Multilevel
    algorithms for multi-constraint graph partitioning. In *SC’98: Proc. of the 1998
    ACM/IEEE Conf. on Supercomputing*. IEEE, 28–28.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karypis and Kumar (1998b) 乔治·卡里皮斯和维平·库马尔。1998b年。《多级算法用于多约束图划分》。在*SC’98：1998年ACM/IEEE超级计算会议论文集*。IEEE，28–28。
- en: 'Khayyat et al. (2013) Zuhair Khayyat, Karim Awara, Amani Alonazi, Hani Jamjoom,
    Dan Williams, and Panos Kalnis. 2013. Mizan: a system for dynamic load balancing
    in large-scale graph processing. In *Proc. of the 8th ACM European conf. on computer
    systems*. 169–182.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khayyat et al. (2013) 苏哈伊尔·哈耶特、卡里姆·阿瓦拉、阿曼·阿洛纳齐、哈尼·贾姆贾姆、丹·威廉姆斯和帕诺斯·卡尔尼斯。2013年。《Mizan：大规模图处理中的动态负载均衡系统》。在*第八届ACM欧洲计算机系统会议*上。169–182。
- en: 'Kiningham et al. (2020a) Kevin Kiningham, Philip Levis, and Christopher Ré.
    2020a. GReTA: Hardware Optimized Graph Processing for GNNs. In *Proc. of the Workshop
    on Resource-Constrained ML (ReCoML 2020)*.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiningham et al. (2020a) 凯文·基宁汉、菲利普·莱维斯和克里斯托弗·瑞。2020a年。《GReTA：为GNNs优化的硬件图处理》。在*资源受限机器学习研讨会（ReCoML
    2020）*上。
- en: 'Kiningham et al. (2020b) Kevin Kiningham, Christopher Re, and Philip Levis.
    2020b. GRIP: a graph neural network accelerator architecture. *arXiv preprint
    arXiv:2007.13828* (2020).'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiningham et al. (2020b) 凯文·基宁汉、克里斯托弗·瑞和菲利普·莱维斯。2020b年。《GRIP：图神经网络加速器架构》。*arXiv预印本
    arXiv:2007.13828*（2020年）。
- en: Kipf and Welling (2016a) Thomas N Kipf and Max Welling. 2016a. Semi-supervised
    classification with graph convolutional networks. *arXiv preprint arXiv:1609.02907*
    (2016).
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf and Welling (2016a) 托马斯·N·基普夫和马克斯·韦林。2016a年。《基于图卷积网络的半监督分类》。*arXiv预印本 arXiv:1609.02907*（2016年）。
- en: Kipf and Welling (2016b) Thomas N Kipf and Max Welling. 2016b. Variational graph
    auto-encoders. *arXiv preprint arXiv:1611.07308* (2016).
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf and Welling (2016b) Thomas N Kipf and Max Welling. 2016b. 变分图自编码器。*arXiv预印本
    arXiv:1611.07308* (2016)。
- en: 'Koliousis et al. (2019) Alexandros Koliousis, Pijika Watcharapichat, Matthias
    Weidlich, Luo Mai, Paolo Costa, and Peter Pietzuch. 2019. CROSSBOW: scaling deep
    learning with small batch sizes on multi-gpu servers. *arXiv preprint arXiv:1901.02244*
    (2019).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Koliousis et al. (2019) Alexandros Koliousis, Pijika Watcharapichat, Matthias
    Weidlich, Luo Mai, Paolo Costa, and Peter Pietzuch. 2019. CROSSBOW: 在多GPU服务器上使用小批量大小进行深度学习的扩展。*arXiv预印本
    arXiv:1901.02244* (2019)。'
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. *Advances
    in neural information processing systems* 25 (2012), 1097–1105.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. 使用深度卷积神经网络进行Imagenet分类。*神经信息处理系统进展* 25 (2012), 1097–1105。
- en: Kumar et al. (2019) Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting
    dynamic embedding trajectory in temporal interaction networks. In *Proc. of the
    25th ACM SIGKDD int’l conf. on knowledge discovery & data mining*. 1269–1278.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar et al. (2019) Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. 预测时间交互网络中的动态嵌入轨迹。发表于*第25届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*。1269–1278。
- en: Lawrence (1993) Jeannette Lawrence. 1993. *Introduction to neural networks*.
    California Scientific Software.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lawrence (1993) Jeannette Lawrence. 1993. *神经网络介绍*。加州科学软件。
- en: LeClair et al. (2020) Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin
    McMillan. 2020. Improved code summarization via a graph neural network. In *Proc.
    of the 28th int’l conf. on program comprehension*. 184–195.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeClair et al. (2020) Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin
    McMillan. 2020. 通过图神经网络改进代码总结。发表于*第28届国际程序理解会议论文集*。184–195。
- en: LeCun et al. (1995) Yann LeCun, Yoshua Bengio, et al. 1995. Convolutional networks
    for images, speech, and time series. *The handbook of brain theory and neural
    networks* 3361, 10 (1995), 1995.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1995) Yann LeCun, Yoshua Bengio, et al. 1995. 用于图像、语音和时间序列的卷积网络。*脑理论与神经网络手册*
    3361, 10 (1995), 1995。
- en: Li et al. (2021b) Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun.
    2021b. Training graph neural networks with 1000 layers. In *Int’l conf. on ML*.
    PMLR, 6437–6449.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021b) Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun.
    2021b. 训练具有1000层的图神经网络。发表于*国际机器学习会议*。PMLR, 6437–6449。
- en: 'Li et al. (2020) Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem.
    2020. Deepergcn: All you need to train deeper gcns. *arXiv preprint arXiv:2006.07739*
    (2020).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2020) Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem.
    2020. Deepergcn: 训练更深层次的GCNs所需的一切。*arXiv预印本 arXiv:2006.07739* (2020)。'
- en: 'Li et al. (2015a) Hao Li, Asim Kadav, Erik Kruus, and Cristian Ungureanu. 2015a.
    Malt: distributed data-parallelism for existing ml applications. In *Proc. of
    the Tenth European Conf. on Computer Systems*. 1–16.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2015a) Hao Li, Asim Kadav, Erik Kruus, and Cristian Ungureanu. 2015a.
    Malt: 现有机器学习应用的分布式数据并行性。发表于*第十届欧洲计算机系统会议论文集*。1–16。'
- en: 'Li et al. (2021a) Houyi Li, Yongchao Liu, Yongyong Li, Bin Huang, Peng Zhang,
    Guowei Zhang, Xintan Zeng, Kefeng Deng, Wenguang Chen, and Changhua He. 2021a.
    GraphTheta: A Distributed Graph Neural Network Learning System With Flexible Training
    Strategy. *arXiv preprint arXiv:2104.10569* (2021).'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2021a) Houyi Li, Yongchao Liu, Yongyong Li, Bin Huang, Peng Zhang,
    Guowei Zhang, Xintan Zeng, Kefeng Deng, Wenguang Chen, and Changhua He. 2021a.
    GraphTheta: 一种具有灵活训练策略的分布式图神经网络学习系统。*arXiv预印本 arXiv:2104.10569* (2021)。'
- en: 'Li and King (2020) Yaoman Li and Irwin King. 2020. Autograph: Automated graph
    neural network. In *Int’l Conf. on Neural Information Processing*. Springer, 189–201.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li and King (2020) Yaoman Li and Irwin King. 2020. Autograph: 自动化图神经网络。发表于*国际神经信息处理会议*。Springer,
    189–201。'
- en: Li et al. (2015b) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
    2015b. Gated graph sequence neural networks. *arXiv preprint arXiv:1511.05493*
    (2015).
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2015b) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
    2015b. 门控图序列神经网络。*arXiv预印本 arXiv:1511.05493* (2015)。
- en: 'Lin et al. (2020) Zhiqi Lin, Cheng Li, Youshan Miao, Yunxin Liu, and Yinlong
    Xu. 2020. Pagraph: Scaling gnn training on large graphs via computation-aware
    caching. In *Proc. of the 11th ACM Symposium on Cloud Comput.* 401–415.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2020) Zhiqi Lin, Cheng Li, Youshan Miao, Yunxin Liu, and Yinlong
    Xu. 2020. Pagraph: 通过计算感知缓存扩展大规模图神经网络训练。发表于*第11届ACM云计算研讨会论文集*。401–415。'
- en: Liu et al. (2020) Meng Liu, Hongyang Gao, and Shuiwang Ji. 2020. Towards deeper
    graph neural networks. In *Proc. of the 26th ACM SIGKDD int’l conf. on knowledge
    discovery & data mining*. 338–348.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020) Meng Liu, Hongyang Gao, and Shuiwang Ji. 2020. 迈向更深层次的图神经网络。发表于*第26届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*。338–348。
- en: 'Liu et al. (2021) Tianfeng Liu, Yangrui Chen, Dan Li, Chuan Wu, Yibo Zhu, Jun
    He, Yanghua Peng, Hongzheng Chen, Hongzhi Chen, and Chuanxiong Guo. 2021. BGL:
    GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing. *arXiv
    preprint arXiv:2112.08541* (2021).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2021) Tianfeng Liu, Yangrui Chen, Dan Li, Chuan Wu, Yibo Zhu, Jun He,
    Yanghua Peng, Hongzheng Chen, Hongzhi Chen 和 Chuanxiong Guo. 2021. BGL：通过优化图数据
    I/O 和预处理提高 GPU 高效 GNN 训练。*arXiv 预印本 arXiv:2112.08541* (2021)。
- en: 'Low (2013) Yucheng Low. 2013. Graphlab: A distributed abstraction for large
    scale machine learning. *Univ. of California* (2013).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Low (2013) Yucheng Low. 2013. Graphlab：一种用于大规模机器学习的分布式抽象。*加州大学* (2013)。
- en: 'Low et al. (2012) Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson,
    Carlos Guestrin, and Joseph M Hellerstein. 2012. Distributed graphlab: A framework
    for machine learning in the cloud. *arXiv preprint arXiv:1204.6078* (2012).'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Low 等 (2012) Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos
    Guestrin 和 Joseph M Hellerstein. 2012. 分布式 Graphlab：云中的机器学习框架。*arXiv 预印本 arXiv:1204.6078*
    (2012)。
- en: 'Low et al. (2014) Yucheng Low, Joseph E Gonzalez, Aapo Kyrola, Danny Bickson,
    Carlos E Guestrin, and Joseph Hellerstein. 2014. Graphlab: A new framework for
    parallel machine learning. *arXiv preprint arXiv:1408.2041* (2014).'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Low 等 (2014) Yucheng Low, Joseph E Gonzalez, Aapo Kyrola, Danny Bickson, Carlos
    E Guestrin 和 Joseph Hellerstein. 2014. Graphlab：一种用于并行机器学习的新框架。*arXiv 预印本 arXiv:1408.2041*
    (2014)。
- en: 'Ma et al. (2019) Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu,
    Lidong Zhou, and Yafei Dai. 2019. Neugraph: parallel deep neural network computation
    on large graphs. In *2019 $\{$USENIX$\}$ Ann. Tech. Conf. ($\{$USENIX$\}$$\{$ATC$\}$
    19)*. 443–458.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等 (2019) Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong
    Zhou 和 Yafei Dai. 2019. Neugraph：在大规模图上进行并行深度神经网络计算。见于*2019 $\{$USENIX$\}$ 年度技术会议
    ($\{$USENIX$\}$$\{$ATC$\}$ 19)*。443–458。
- en: Ma et al. (2020) Yao Ma, Ziyi Guo, Zhaocun Ren, Jiliang Tang, and Dawei Yin.
    2020. Streaming graph neural networks. In *Proc. of the 43rd Int’l ACM SIGIR Conf.
    on Research and Development in Information Retrieval*. 719–728.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等 (2020) Yao Ma, Ziyi Guo, Zhaocun Ren, Jiliang Tang 和 Dawei Yin. 2020. 流式图神经网络。见于*第43届国际
    ACM SIGIR 信息检索研究与发展会议论文集*。719–728。
- en: 'Malewicz et al. (2010) Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C
    Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. 2010. Pregel: a system
    for large-scale graph processing. In *Proc. of the 2010 ACM SIGMOD Int’l Conf.
    on Management of data*. 135–146.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malewicz 等 (2010) Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C
    Dehnert, Ilan Horn, Naty Leiser 和 Grzegorz Czajkowski. 2010. Pregel：一个大规模图处理系统。见于*2010
    年 ACM SIGMOD 数据管理国际会议论文集*。135–146。
- en: 'Manning et al. (2014) Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose
    Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural
    language processing toolkit. In *Proc. of 52nd ann. meeting of the association
    for computational linguistics: system demonstrations*. 55–60.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manning 等 (2014) Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose
    Finkel, Steven Bethard 和 David McClosky. 2014. 斯坦福 CoreNLP 自然语言处理工具包。见于*第52届计算语言学协会年会：系统演示*。55–60。
- en: Marcheggiani and Titov (2017) Diego Marcheggiani and Ivan Titov. 2017. Encoding
    sentences with graph convolutional networks for semantic role labeling. *arXiv
    preprint arXiv:1703.04826* (2017).
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcheggiani 和 Titov (2017) Diego Marcheggiani 和 Ivan Titov. 2017. 使用图卷积网络对句子进行编码以进行语义角色标注。*arXiv
    预印本 arXiv:1703.04826* (2017)。
- en: Mason and Verwoerd (2007) Oliver Mason and Mark Verwoerd. 2007. Graph theory
    and networks in biology. *IET systems biology* 1, 2 (2007), 89–119.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mason 和 Verwoerd (2007) Oliver Mason 和 Mark Verwoerd. 2007. 生物学中的图论和网络。*IET
    系统生物学* 1, 2 (2007), 89–119。
- en: 'Mayer et al. (2018) Christian Mayer, Ruben Mayer, Muhammad Adnan Tariq, Heiko
    Geppert, Larissa Laich, Lukas Rieger, and Kurt Rothermel. 2018. Adwise: Adaptive
    window-based streaming edge partitioning for high-speed graph processing. In *2018
    IEEE 38th Int’l Conf. on Distrib. Comput. Systems (ICDCS)*. IEEE, 685–695.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mayer 等 (2018) Christian Mayer, Ruben Mayer, Muhammad Adnan Tariq, Heiko Geppert,
    Larissa Laich, Lukas Rieger 和 Kurt Rothermel. 2018. Adwise：用于高速图处理的自适应基于窗口的流式边缘划分。见于*2018
    IEEE 第38届国际分布式计算系统会议 (ICDCS)*。IEEE，685–695。
- en: 'Mayer and Jacobsen (2020) Ruben Mayer and Hans-Arno Jacobsen. 2020. Scalable
    deep learning on distributed infrastructures: Challenges, techniques, and tools.
    *ACM Comput. Surveys (CSUR)* 53, 1 (2020), 1–37.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mayer 和 Jacobsen (2020) Ruben Mayer 和 Hans-Arno Jacobsen. 2020. 在分布式基础设施上的可扩展深度学习：挑战、技术和工具。*ACM
    计算机调查 (CSUR)* 53, 1 (2020), 1–37。
- en: 'Mayer and Jacobsen (2021) Ruben Mayer and Hans-Arno Jacobsen. 2021. Hybrid
    Edge Partitioner: Partitioning Large Power-Law Graphs under Memory Constraints.
    In *Proc. of the 2021 Int’l Conf. on Management of Data*. 1289–1302.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mayer和Jacobsen（2021）Ruben Mayer和Hans-Arno Jacobsen。2021年。混合边分区器：在内存限制下分区大规模幂律图。在*Proc.
    of the 2021 Int’l Conf. on Management of Data*。1289–1302。
- en: 'Mayer et al. (2020) Ruben Mayer, Kamil Orujzade, and Hans-Arno Jacobsen. 2020.
    2ps: High-quality edge partitioning with two-phase streaming. *arXiv preprint
    arXiv:2001.07086* (2020).'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mayer等人（2020）Ruben Mayer、Kamil Orujzade和Hans-Arno Jacobsen。2020年。2ps：具有两阶段流处理的高质量边分区。*arXiv
    preprint arXiv:2001.07086*（2020）。
- en: Mayer et al. (2022) Ruben Mayer, Kamil Orujzade, and Hans-Arno Jacobsen. 2022.
    Out-of-core edge partitioning at linear run-time. In *2022 IEEE 38th Int’l Conf.
    on Data Engineering (ICDE)*. IEEE, 2629–2642.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mayer等人（2022）Ruben Mayer、Kamil Orujzade和Hans-Arno Jacobsen。2022年。在线边分区的线性运行时间。在*2022
    IEEE 38th Int’l Conf. on Data Engineering (ICDE)*。IEEE，2629–2642。
- en: McCallum et al. (2000) Andrew Kachites McCallum, Kamal Nigam, Jason Rennie,
    and Kristie Seymore. 2000. Automating the construction of internet portals with
    machine learning. *Information Retrieval* 3, 2 (2000), 127–163.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCallum等人（2000）Andrew Kachites McCallum、Kamal Nigam、Jason Rennie和Kristie Seymore。2000年。利用机器学习自动构建互联网门户。*Information
    Retrieval* 3, 2（2000），127–163。
- en: 'McCune et al. (2015) Robert Ryan McCune, Tim Weninger, and Greg Madey. 2015.
    Thinking like a vertex: a survey of vertex-centric frameworks for large-scale
    distributed graph processing. *ACM Computing Surveys (CSUR)* 48, 2 (2015), 1–39.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCune等人（2015）Robert Ryan McCune、Tim Weninger和Greg Madey。2015年。像顶点一样思考：大规模分布式图处理的顶点中心框架综述。*ACM
    Computing Surveys (CSUR)* 48, 2（2015），1–39。
- en: 'Md et al. (2021) Vasimuddin Md, Sanchit Misra, Guixiang Ma, Ramanarayan Mohanty,
    Evangelos Georganas, Alexander Heinecke, Dhiraj Kalamkar, Nesreen K Ahmed, and
    Sasikanth Avancha. 2021. DistGNN: Scalable Distributed Training for Large-Scale
    Graph Neural Networks. *arXiv preprint arXiv:2104.06700* (2021).'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Md等人（2021）Vasimuddin Md、Sanchit Misra、Guixiang Ma、Ramanarayan Mohanty、Evangelos
    Georganas、Alexander Heinecke、Dhiraj Kalamkar、Nesreen K Ahmed和Sasikanth Avancha。2021年。DistGNN：用于大规模图神经网络的可扩展分布式训练。*arXiv
    preprint arXiv:2104.06700*（2021）。
- en: Merkel et al. (pear) Nikolai Merkel, Ruben Mayer, Tawkir Ahmed Fakir, and Hans-Arno
    Jacobsen. To Appear. Partitioner Selection with EASE to Optimize Distributed Graph
    Processing. *Proceedings of the 2023 IEEE 39th Int’l Conf. on Data Engineering
    (ICDE’23)* (To Appear), 15.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merkel等人（待发表）Nikolai Merkel、Ruben Mayer、Tawkir Ahmed Fakir和Hans-Arno Jacobsen。待发表。使用EASE进行分区选择以优化分布式图处理。*Proceedings
    of the 2023 IEEE 39th Int’l Conf. on Data Engineering (ICDE’23)*（待发表），15。
- en: 'Miller (1995) George A Miller. 1995. WordNet: a lexical database for English.
    *Commun. ACM* 38, 11 (1995), 39–41.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller（1995）George A Miller。1995年。WordNet：英语词汇数据库。*Commun. ACM* 38, 11（1995），39–41。
- en: 'Narayanan et al. (2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek
    Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia.
    2019. PipeDream: generalized pipeline parallelism for DNN training. In *Proc.
    of the 27th ACM Symposium on Operating Systems Principles*. 1–15.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayanan等人（2019）Deepak Narayanan、Aaron Harlap、Amar Phanishayee、Vivek Seshadri、Nikhil
    R Devanur、Gregory R Ganger、Phillip B Gibbons和Matei Zaharia。2019年。PipeDream：用于DNN训练的广义流水线并行。载于*Proc.
    of the 27th ACM Symposium on Operating Systems Principles*。1–15。
- en: 'Ouyang et al. (2021) Shuo Ouyang, Dezun Dong, Yemao Xu, and Liquan Xiao. 2021.
    Communication optimization strategies for distributed deep neural network training:
    A survey. *J. Parallel and Distrib. Comput.* 149 (2021), 52–65.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2021）Shuo Ouyang、Dezun Dong、Yemao Xu和Liquan Xiao。2021年。分布式深度神经网络训练的通信优化策略：综述。*J.
    Parallel and Distrib. Comput.* 149 (2021), 52–65。
- en: Ouyang et al. (2019) Yi Ouyang, Bin Guo, Xing Tang, Xiuqiang He, Jian Xiong,
    and Zhiwen Yu. 2019. Learning cross-domain representation with multi-graph neural
    network. *arXiv preprint arXiv:1905.10095* (2019).
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2019）Yi Ouyang、Bin Guo、Xing Tang、Xiuqiang He、Jian Xiong和Zhiwen Yu。2019年。通过多图神经网络学习跨领域表示。*arXiv
    preprint arXiv:1905.10095*（2019）。
- en: Pacaci and Özsu (2019) Anil Pacaci and M Tamer Özsu. 2019. Experimental analysis
    of streaming algorithms for graph partitioning. In *Proceedings of the 2019 International
    Conference on Management of Data*. 1375–1392.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pacaci和Özsu（2019）Anil Pacaci和M Tamer Özsu。2019年。图分区流算法的实验分析。在*Proceedings of
    the 2019 International Conference on Management of Data*。1375–1392。
- en: 'Page et al. (1999) Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.
    1999. *The PageRank citation ranking: Bringing order to the web.* Technical Report.
    Stanford InfoLab.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Page等人（1999）Lawrence Page、Sergey Brin、Rajeev Motwani和Terry Winograd。1999年。*The
    PageRank citation ranking: Bringing order to the web.* 技术报告。斯坦福InfoLab。'
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning
    library. *Advances in neural information processing systems* 32 (2019).'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等人 (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga 等人. 2019. PyTorch：一种命令式风格的高性能深度学习库。*神经信息处理系统进展* 32 (2019)。
- en: Peng et al. (2020) Hao Peng, Hongfei Wang, Bowen Du, Md Zakirul Alam Bhuiyan,
    Hongyuan Ma, Jianwei Liu, Lihong Wang, Zeyu Yang, Linfeng Du, Senzhang Wang, et al.
    2020. Spatial temporal incidence dynamic graph neural networks for traffic flow
    forecasting. *Information Sciences* 521 (2020), 277–290.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人 (2020) Hao Peng, Hongfei Wang, Bowen Du, Md Zakirul Alam Bhuiyan, Hongyuan
    Ma, Jianwei Liu, Lihong Wang, Zeyu Yang, Linfeng Du, Senzhang Wang 等人. 2020. 用于交通流预测的空间时间发生动态图神经网络。*信息科学*
    521 (2020), 277–290。
- en: 'Petroni et al. (2015) Fabio Petroni, Leonardo Querzoni, Khuzaima Daudjee, Shahin
    Kamali, and Giorgio Iacoboni. 2015. Hdrf: Stream-based partitioning for power-law
    graphs. In *Proc. of the 24th ACM int’l on conf. on information and knowledge
    management*. 243–252.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petroni 等人 (2015) Fabio Petroni, Leonardo Querzoni, Khuzaima Daudjee, Shahin
    Kamali 和 Giorgio Iacoboni. 2015. HDRF：基于流的幂律图分区。载于 *第24届ACM国际信息与知识管理会议论文集*。243–252。
- en: Raghavan et al. (2007) Usha Nandini Raghavan, Réka Albert, and Soundar Kumara.
    2007. Near linear time algorithm to detect community structures in large-scale
    networks. *Physical review E* 76, 3 (2007), 036106.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raghavan 等人 (2007) Usha Nandini Raghavan, Réka Albert 和 Soundar Kumara. 2007.
    检测大规模网络中的社区结构的接近线性时间算法。*物理评论E* 76, 3 (2007), 036106。
- en: Ralaivola et al. (2005) Liva Ralaivola, Sanjay J Swamidass, Hiroto Saigo, and
    Pierre Baldi. 2005. Graph kernels for chemical informatics. *Neural networks*
    18, 8 (2005), 1093–1110.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ralaivola 等人 (2005) Liva Ralaivola, Sanjay J Swamidass, Hiroto Saigo 和 Pierre
    Baldi. 2005. 化学信息学的图核。*神经网络* 18, 8 (2005), 1093–1110。
- en: Ramachandran et al. (2017) Prajit Ramachandran, Barret Zoph, and Quoc V Le.
    2017. Searching for activation functions. *arXiv preprint arXiv:1710.05941* (2017).
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandran 等人 (2017) Prajit Ramachandran, Barret Zoph 和 Quoc V Le. 2017. 寻找激活函数。*arXiv
    预印本 arXiv:1710.05941* (2017)。
- en: Riba et al. (2018) Pau Riba, Andreas Fischer, Josep Lladós, and Alicia Fornés.
    2018. Learning graph distances with message passing neural networks. In *2018
    24th Int’l Conf. on Pattern Recognition (ICPR)*. IEEE, 2239–2244.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riba 等人 (2018) Pau Riba, Andreas Fischer, Josep Lladós 和 Alicia Fornés. 2018.
    使用消息传递神经网络学习图距离。载于 *2018年第24届国际模式识别会议 (ICPR)*。IEEE, 2239–2244。
- en: Robbins and Monro (1951) Herbert Robbins and Sutton Monro. 1951. A stochastic
    approximation method. *The annals of math. statistics* (1951), 400–407.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robbins 和 Monro (1951) Herbert Robbins 和 Sutton Monro. 1951. 一种随机近似方法。*数学统计年鉴*
    (1951), 400–407。
- en: Rossi et al. (2020) Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide
    Eynard, Federico Monti, and Michael Bronstein. 2020. Temporal graph networks for
    deep learning on dynamic graphs. *arXiv preprint arXiv:2006.10637* (2020).
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rossi 等人 (2020) Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard,
    Federico Monti 和 Michael Bronstein. 2020. 动态图上的深度学习的时间图网络。*arXiv 预印本 arXiv:2006.10637*
    (2020)。
- en: 'Rozemberczki et al. (2020) Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar.
    2020. Little ball of fur: a python library for graph sampling. In *Proc. of the
    29th ACM Int’l Conf. on Information & Knowledge Management*. 3133–3140.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rozemberczki 等人 (2020) Benedek Rozemberczki, Oliver Kiss 和 Rik Sarkar. 2020.
    小毛球：用于图采样的 Python 库。载于 *第29届ACM国际信息与知识管理会议论文集*。3133–3140。
- en: Ruder (2016) Sebastian Ruder. 2016. An overview of gradient descent optimization
    algorithms. *arXiv preprint arXiv:1609.04747* (2016).
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder (2016) Sebastian Ruder. 2016. 梯度下降优化算法概述。*arXiv 预印本 arXiv:1609.04747*
    (2016)。
- en: 'Rumelhart et al. (1995) David E Rumelhart, Richard Durbin, Richard Golden,
    and Yves Chauvin. 1995. Backpropagation: The basic theory. *Backpropagation: Theory,
    architectures and applications* (1995), 1–34.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart 等人 (1995) David E Rumelhart, Richard Durbin, Richard Golden 和 Yves
    Chauvin. 1995. 反向传播：基本理论。*反向传播：理论、架构与应用* (1995), 1–34。
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning representations by back-propagating errors. *nature* 323, 6088
    (1986), 533–536.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart 等人 (1986) David E Rumelhart, Geoffrey E Hinton 和 Ronald J Williams.
    1986. 通过反向传播错误学习表示。*自然* 323, 6088 (1986), 533–536。
- en: Saad and Beferull-Lozano (2021) Leila Ben Saad and Baltasar Beferull-Lozano.
    2021. Quantization in Graph Convolutional Neural Networks. In *2021 29th European
    Signal Processing Conf. (EUSIPCO)*. IEEE, 1855–1859.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saad 和 Beferull-Lozano (2021) Leila Ben Saad 和 Baltasar Beferull-Lozano. 2021.
    图卷积神经网络中的量化。载于 *2021年第29届欧洲信号处理会议 (EUSIPCO)*。IEEE, 1855–1859。
- en: 'Salihoglu and Widom (2013) Semih Salihoglu and Jennifer Widom. 2013. Gps: A
    graph processing system. In *Proc. of the 25th int’l conf. on scientific and statistical
    database management*. 1–12.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salihoglu 和 Widom（2013）Semih Salihoglu 和 Jennifer Widom。2013。Gps：图处理系统。在 *第25届国际科学与统计数据库管理会议*
    上。1–12。
- en: Sanders et al. (2019) Peter Sanders, Kurt Mehlhorn, Martin Dietzfelbinger, and
    Roman Dementiev. 2019. *Sequential and Parallel Algorithms and Data Structures*.
    Springer, 403–404.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanders 等人（2019）Peter Sanders、Kurt Mehlhorn、Martin Dietzfelbinger 和 Roman Dementiev。2019。*顺序和并行算法与数据结构*。Springer，403–404。
- en: 'Sarlin et al. (2020) Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
    and Andrew Rabinovich. 2020. Superglue: Learning feature matching with graph neural
    networks. In *Proc. of the IEEE/CVF conf. on CV and pattern recognition*. 4938–4947.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarlin 等人（2020）Paul-Edouard Sarlin、Daniel DeTone、Tomasz Malisiewicz 和 Andrew
    Rabinovich。2020。Superglue：利用图神经网络学习特征匹配。在 *IEEE/CVF 计算机视觉与模式识别会议* 上。4938–4947。
- en: Scarselli et al. (2008) Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
    Hagenbuchner, and Gabriele Monfardini. 2008. The graph neural network model. *IEEE
    trans. on neural networks* 20, 1 (2008), 61–80.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scarselli 等人（2008）Franco Scarselli、Marco Gori、Ah Chung Tsoi、Markus Hagenbuchner
    和 Gabriele Monfardini。2008。图神经网络模型。*IEEE 神经网络交易* 20, 1 (2008)，61–80。
- en: Schlag et al. (2019) Sebastian Schlag, Christian Schulz, Daniel Seemaier, and
    Darren Strash. 2019. Scalable edge partitioning. In *2019 Proc. of the Twenty-First
    Workshop on Algorithm Engineering and Experiments (ALENEX)*. SIAM, 211–225.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schlag 等人（2019）Sebastian Schlag、Christian Schulz、Daniel Seemaier 和 Darren Strash。2019。可扩展的边划分。在
    *2019年第21届算法工程与实验研讨会（ALENEX）* 上。SIAM，211–225。
- en: Schramm et al. (2022) Michael Schramm, Sukanya Bhowmik, and Kurt Rothermel.
    2022. Flexible application-aware approximation for modern distributed graph processing
    frameworks. In *Proc. of the 5th ACM SIGMOD Joint Int’l Workshop on Graph Data
    Management Experiences & Systems (GRADES) and Network Data Analytics (NDA)*. 1–10.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schramm 等人（2022）Michael Schramm、Sukanya Bhowmik 和 Kurt Rothermel。2022。现代分布式图处理框架的灵活应用感知近似。在
    *第5届 ACM SIGMOD 联合国际图数据管理经验与系统（GRADES）和网络数据分析（NDA）研讨会* 上。1–10。
- en: Sen et al. (2008) Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor,
    Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classification in network
    data. *AI magazine* 29, 3 (2008), 93–93.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sen 等人（2008）Prithviraj Sen、Galileo Namata、Mustafa Bilgic、Lise Getoor、Brian Galligher
    和 Tina Eliassi-Rad。2008。网络数据中的集体分类。*AI 杂志* 29, 3 (2008)，93–93。
- en: 'Serafini and Guan (2021) Marco Serafini and Hui Guan. 2021. Scalable Graph
    Neural Network Training: The Case for Sampling. *ACM SIGOPS Operating Systems
    Review* 55, 1 (2021), 68–76.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serafini 和 Guan（2021）Marco Serafini 和 Hui Guan。2021。可扩展图神经网络训练：采样的案例。*ACM SIGOPS
    操作系统评论* 55, 1 (2021)，68–76。
- en: Sharma et al. (2017) Sagar Sharma, Simone Sharma, and Anidhya Athaiya. 2017.
    Activation functions in neural networks. *towards data science* 6, 12 (2017),
    310–316.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人（2017）Sagar Sharma、Simone Sharma 和 Anidhya Athaiya。2017。神经网络中的激活函数。*数据科学前沿*
    6, 12 (2017)，310–316。
- en: 'Sherstinsky (2020) Alex Sherstinsky. 2020. Fundamentals of recurrent neural
    network (RNN) and long short-term memory (LSTM) network. *Physica D: Nonlinear
    Phenomena* 404 (2020), 132306.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sherstinsky（2020）Alex Sherstinsky。2020。递归神经网络（RNN）和长短期记忆（LSTM）网络的基础。*Physica
    D: 非线性现象* 404 (2020)，132306。'
- en: Shi et al. (2020) Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Chengjian Liu, Wei
    Wang, and Bo Li. 2020. A quantitative survey of communication optimizations in
    distributed deep learning. *IEEE Network* 35, 3 (2020), 230–237.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2020）Shaohuai Shi、Zhenheng Tang、Xiaowen Chu、Chengjian Liu、Wei Wang 和
    Bo Li。2020。分布式深度学习中的通信优化定量调查。*IEEE 网络* 35, 3 (2020)，230–237。
- en: 'Shi et al. (2018) Xuanhua Shi, Zhigao Zheng, Yongluan Zhou, Hai Jin, Ligang
    He, Bo Liu, and Qiang-Sheng Hua. 2018. Graph processing on GPUs: A survey. *ACM
    Computing Surveys (CSUR)* 50, 6 (2018), 1–35.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2018）Xuanhua Shi、Zhigao Zheng、Yongluan Zhou、Hai Jin、Ligang He、Bo Liu
    和 Qiang-Sheng Hua。2018。GPU 上的图处理：综述。*ACM 计算调查（CSUR）* 50, 6 (2018)，1–35。
- en: Silva et al. (2010) Nitai B Silva, Ren Tsang, George DC Cavalcanti, and Jyh
    Tsang. 2010. A graph-based friend recommendation system using genetic algorithm.
    In *IEEE congress on evolutionary computation*. IEEE, 1–7.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silva 等人（2010）Nitai B Silva、Ren Tsang、George DC Cavalcanti 和 Jyh Tsang。2010。使用遗传算法的图基朋友推荐系统。发表于
    *IEEE 进化计算大会*。IEEE，1–7。
- en: Sobota et al. (2008) B Sobota, Cs Szabó, and J Perhac. 2008. Using path-finding
    algorithms of graph theory for route-searching in geographical information systems.
    In *2008 6th Int’l Symposium on Intelligent Systems and Informatics*. IEEE, 1–6.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sobota 等 (2008) B Sobota, Cs Szabó 和 J Perhac. 2008. 使用图论的路径查找算法进行地理信息系统的路线搜索。见
    *2008年第六届国际智能系统与信息学研讨会*。IEEE, 1–6。
- en: Stanton and Kliot (2012) Isabelle Stanton and Gabriel Kliot. 2012. Streaming
    graph partitioning for large distributed graphs. In *Proc. of the 18th ACM SIGKDD
    int’l conf. on Knowledge discovery and data mining*. 1222–1230.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stanton 和 Kliot (2012) Isabelle Stanton 和 Gabriel Kliot. 2012. 大型分布式图的流式图划分。见
    *第18届 ACM SIGKDD 国际知识发现与数据挖掘会议*。1222–1230。
- en: Sukhbaatar et al. (2016) Sainbayar Sukhbaatar, Rob Fergus, et al. 2016. Learning
    multiagent communication with backpropagation. *Advances in neural information
    processing systems* 29 (2016).
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sukhbaatar 等 (2016) Sainbayar Sukhbaatar, Rob Fergus 等. 2016. 通过反向传播学习多智能体通信。*神经信息处理系统进展*
    29 (2016)。
- en: 'Tailor et al. (2020) Shyam A Tailor, Javier Fernandez-Marques, and Nicholas D
    Lane. 2020. Degree-quant: Quantization-aware training for graph neural networks.
    *arXiv preprint arXiv:2008.05000* (2020).'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tailor 等 (2020) Shyam A Tailor, Javier Fernandez-Marques 和 Nicholas D Lane.
    2020. Degree-quant：针对图神经网络的量化感知训练。*arXiv 预印本 arXiv:2008.05000* (2020)。
- en: Tang and Liu (2010) Lei Tang and Huan Liu. 2010. Graph mining applications to
    social network analysis. In *Managing and Mining Graph Data*. Springer, 487–513.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 和 Liu (2010) Lei Tang 和 Huan Liu. 2010. 图挖掘在社交网络分析中的应用。见 *管理与挖掘图数据*。Springer,
    487–513。
- en: 'Tang et al. (2020) Zhenheng Tang, Shaohuai Shi, Xiaowen Chu, Wei Wang, and
    Bo Li. 2020. Communication-efficient distributed deep learning: A comprehensive
    survey. *arXiv preprint arXiv:2003.06307* (2020).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等 (2020) Zhenheng Tang, Shaohuai Shi, Xiaowen Chu, Wei Wang 和 Bo Li. 2020.
    通信高效的分布式深度学习：全面调查。*arXiv 预印本 arXiv:2003.06307* (2020)。
- en: Tarjan (1972) Robert Tarjan. 1972. Depth-first search and linear graph algorithms.
    *SIAM J. on Computing* 1, 2 (1972), 146–160.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarjan (1972) Robert Tarjan. 1972. 深度优先搜索和线性图算法。*SIAM 计算杂志* 1, 2 (1972), 146–160。
- en: 'Thorpe et al. (2021) John Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng,
    Guanzhou Hu, Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim,
    et al. 2021. Dorylus: Affordable, Scalable, and Accurate $\{$GNN$\}$ Training
    with Distributed $\{$CPU$\}$ Servers and Serverless Threads. In *15th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 21)*. 495–514.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thorpe 等 (2021) John Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng, Guanzhou
    Hu, Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim 等. 2021.
    Dorylus：具有分布式 $\{$CPU$\}$ 服务器和无服务器线程的负担得起、可扩展且准确的 $\{$GNN$\}$ 训练。见 *第15届 USENIX
    操作系统设计与实现研讨会 (OSDI 21)*。495–514。
- en: Tian et al. (2013) Yuanyuan Tian, Andrey Balmin, Severin Andreas Corsten, Shirish
    Tatikonda, and John McPherson. 2013. From” think like a vertex” to” think like
    a graph”. *Proc. of the VLDB Endowment* 7, 3 (2013), 193–204.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等 (2013) Yuanyuan Tian, Andrey Balmin, Severin Andreas Corsten, Shirish
    Tatikonda 和 John McPherson. 2013. 从“像顶点一样思考”到“像图一样思考”。*VLDB 基金会会议录* 7, 3 (2013),
    193–204。
- en: 'Tsourakakis et al. (2014) Charalampos Tsourakakis, Christos Gkantsidis, Bozidar
    Radunovic, and Milan Vojnovic. 2014. Fennel: Streaming graph partitioning for
    massive scale graphs. In *Proc. of the 7th ACM int’l conf. on Web search and data
    mining*. 333–342.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsourakakis 等 (2014) Charalampos Tsourakakis, Christos Gkantsidis, Bozidar Radunovic
    和 Milan Vojnovic. 2014. Fennel：用于大规模图的流式图划分。见 *第七届 ACM 国际网络搜索与数据挖掘会议*。333–342。
- en: Valiant (1990) Leslie G Valiant. 1990. A bridging model for parallel computation.
    *Commun. ACM* 33, 8 (1990), 103–111.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valiant (1990) Leslie G Valiant. 1990. 用于并行计算的桥接模型。*ACM 通讯* 33, 8 (1990), 103–111。
- en: Veličković et al. (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks.
    *arXiv preprint arXiv:1710.10903* (2017).
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veličković 等 (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana
    Romero, Pietro Lio 和 Yoshua Bengio. 2017. 图注意力网络。*arXiv 预印本 arXiv:1710.10903*
    (2017)。
- en: Wang et al. (2013) Guozhang Wang, Wenlei Xie, Alan J Demers, and Johannes Gehrke.
    2013. Asynchronous Large-Scale Graph Processing Made Easy.. In *CIDR*, Vol. 13.
    3–6.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2013) Guozhang Wang, Wenlei Xie, Alan J Demers 和 Johannes Gehrke. 2013.
    异步大规模图处理变得简单。见 *CIDR*, Vol. 13. 3–6。
- en: 'Wang et al. (2021c) Lei Wang, Qiang Yin, Chao Tian, Jianbang Yang, Rong Chen,
    Wenyuan Yu, Zihang Yao, and Jingren Zhou. 2021c. FlexGraph: a flexible and efficient
    distributed framework for GNN training. In *Proc. of the Sixteenth EuroSys*. 67–82.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2021c) Lei Wang, Qiang Yin, Chao Tian, Jianbang Yang, Rong Chen, Wenyuan
    Yu, Zihang Yao 和 Jingren Zhou. 2021c. FlexGraph：一个灵活高效的分布式 GNN 训练框架。见 *第十六届 EuroSys
    会议录*。67–82。
- en: 'Wang et al. (2019) Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao
    Ye, Mufei Li, Jinjing Zhou, Qi Huang, Chao Ma, et al. 2019. Deep Graph Library:
    Towards Efficient and Scalable Deep Learning on Graphs. (2019).'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2019）Minjie Wang、Lingfan Yu、Da Zheng、Quan Gan、Yu Gai、Zihao Ye、Mufei Li、Jinjing
    Zhou、Qi Huang、Chao Ma 等。2019。深度图书馆：迈向高效且可扩展的图深度学习。（2019）。
- en: 'Wang et al. (2021b) Xuhong Wang, Ding Lyu, Mengjian Li, Yang Xia, Qi Yang,
    Xinwen Wang, Xinguang Wang, Ping Cui, Yupu Yang, Bowen Sun, et al. 2021b. APAN:
    Asynchronous propagation attention network for real-time temporal graph embedding.
    In *Proc. of the 2021 Int’l Conf. on Management of Data*. 2628–2638.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021b）Xuhong Wang、Ding Lyu、Mengjian Li、Yang Xia、Qi Yang、Xinwen Wang、Xinguang
    Wang、Ping Cui、Yupu Yang、Bowen Sun 等。2021b。APAN：用于实时时间图嵌入的异步传播注意力网络。发表于 *2021 年国际数据管理大会*。2628–2638。
- en: 'Wang et al. (2016) Yangzihao Wang, Andrew Davidson, Yuechao Pan, Yuduo Wu,
    Andy Riffel, and John D Owens. 2016. Gunrock: A high-performance graph processing
    library on the GPU. In *Proc. of the 21st ACM SIGPLAN symposium on principles
    and practice of parallel programming*. 1–12.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2016）Yangzihao Wang、Andrew Davidson、Yuechao Pan、Yuduo Wu、Andy Riffel
    和 John D Owens。2016。Gunrock：一种高性能的 GPU 图处理库。发表于 *第21届 ACM SIGPLAN 并行编程原理与实践研讨会*。1–12。
- en: 'Wang et al. (2022) Yuke Wang, Boyuan Feng, and Yufei Ding. 2022. QGTC: accelerating
    quantized graph neural networks via GPU tensor core. In *Proc. of the 27th ACM
    SIGPLAN Symposium on Principles and Practice of Parallel Programming*. 107–119.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2022）Yuke Wang、Boyuan Feng 和 Yufei Ding。2022。QGTC：通过 GPU 张量核心加速量化图神经网络。发表于
    *第27届 ACM SIGPLAN 并行编程原理与实践研讨会*。107–119。
- en: 'Wang et al. (2021a) Yuke Wang, Boyuan Feng, Gushu Li, Shuangchen Li, Lei Deng,
    Yuan Xie, and Yufei Ding. 2021a. $\{$GNNAdvisor$\}$: An Adaptive and Efficient
    Runtime System for $\{$GNN$\}$ Acceleration on $\{$GPUs$\}$. In *15th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 21)*. 515–531.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021a）Yuke Wang、Boyuan Feng、Gushu Li、Shuangchen Li、Lei Deng、Yuan Xie
    和 Yufei Ding。2021a。$\{$GNNAdvisor$\}$：一个用于 $\{$GNN$\}$ 加速的自适应和高效运行时系统。发表于 *第15届
    USENIX 操作系统设计与实现研讨会（OSDI 21）*。515–531。
- en: 'Watcharapichat et al. (2016) Pijika Watcharapichat, Victoria Lopez Morales,
    Raul Castro Fernandez, and Peter Pietzuch. 2016. Ako: Decentralised deep learning
    with partial gradient exchange. In *Proc. of the Seventh ACM Symposium on Cloud
    Comput.* 84–97.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watcharapichat 等（2016）Pijika Watcharapichat、Victoria Lopez Morales、Raul Castro
    Fernandez 和 Peter Pietzuch。2016。Ako：具有部分梯度交换的去中心化深度学习。发表于 *第七届 ACM 云计算研讨会*。84–97。
- en: Wei et al. (2016) Hao Wei, Jeffrey Xu Yu, Can Lu, and Xuemin Lin. 2016. Speedup
    graph processing by graph ordering. In *Proc. of the 2016 Int’l Conf. on Management
    of Data*. 1813–1828.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2016）Hao Wei、Jeffrey Xu Yu、Can Lu 和 Xuemin Lin。2016。通过图排序加速图处理。发表于 *2016
    年国际数据管理大会*。1813–1828。
- en: 'Widrow and Lehr (1990) Bernard Widrow and Michael A Lehr. 1990. 30 years of
    adaptive neural networks: perceptron, madaline, and backpropagation. *Proc. of
    the IEEE* 78, 9 (1990), 1415–1442.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Widrow 和 Lehr（1990）Bernard Widrow 和 Michael A Lehr。1990。30 年的自适应神经网络：感知器、Madaline
    和反向传播。*IEEE 期刊* 78, 9（1990），1415–1442。
- en: Wu et al. (2019) Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao
    Yu, and Kilian Weinberger. 2019. Simplifying graph convolutional networks. In
    *Int’l conf. on ML*. PMLR, 6861–6871.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2019）Felix Wu、Amauri Souza、Tianyi Zhang、Christopher Fifty、Tao Yu 和 Kilian
    Weinberger。2019。简化图卷积网络。发表于 *国际机器学习大会*。PMLR, 6861–6871。
- en: 'Wu et al. (2021) Yidi Wu, Kaihao Ma, Zhenkun Cai, Tatiana Jin, Boyang Li, Chenguang
    Zheng, James Cheng, and Fan Yu. 2021. Seastar: vertex-centric programming for
    graph neural networks. In *Proc. of the 16th Europ. Conf. on Comput. Systems*.
    359–375.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2021）Yidi Wu、Kaihao Ma、Zhenkun Cai、Tatiana Jin、Boyang Li、Chenguang Zheng、James
    Cheng 和 Fan Yu。2021。Seastar：面向图神经网络的顶点中心编程。发表于 *第16届欧洲计算系统大会*。359–375。
- en: Wu et al. (2020) Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi
    Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks.
    *IEEE trans. on neural networks and learning systems* 32, 1 (2020), 4–24.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2020）Zonghan Wu、Shirui Pan、Fengwen Chen、Guodong Long、Chengqi Zhang 和 S
    Yu Philip。2020。关于图神经网络的全面调查。*IEEE 神经网络与学习系统期刊* 32, 1（2020），4–24。
- en: 'Xie et al. (2015) Chenning Xie, Rong Chen, Haibing Guan, Binyu Zang, and Haibo
    Chen. 2015. Sync or async: Time to fuse for distributed graph-parallel computation.
    *ACM SIGPLAN Notices* 50, 8 (2015), 194–204.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2015）Chenning Xie、Rong Chen、Haibing Guan、Binyu Zang 和 Haibo Chen。2015。同步还是异步：分布式图并行计算的融合时机。*ACM
    SIGPLAN 通告* 50, 8（2015），194–204。
- en: 'Xie et al. (2014) Cong Xie, Ling Yan, Wu-Jun Li, and Zhihua Zhang. 2014. Distributed
    Power-law Graph Computing: Theoretical and Empirical Analysis.. In *Nips*, Vol. 27\.
    1673–1681.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等 (2014) Cong Xie, Ling Yan, Wu-Jun Li, 和 Zhihua Zhang. 2014. 分布式幂律图计算：理论与实证分析。在
    *Nips*，第 27 卷。1673–1681。
- en: Xu et al. (2018a) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
    2018a. How powerful are graph neural networks? *arXiv preprint arXiv:1810.00826*
    (2018).
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2018a) Keyulu Xu, Weihua Hu, Jure Leskovec, 和 Stefanie Jegelka. 2018a.
    图神经网络的能力有多强？*arXiv 预印本 arXiv:1810.00826* (2018)。
- en: Xu et al. (2018b) Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
    Kawarabayashi, and Stefanie Jegelka. 2018b. Representation learning on graphs
    with jumping knowledge networks. In *Int’l Conf. on ML*. PMLR, 5453–5462.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2018b) Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
    Kawarabayashi, 和 Stefanie Jegelka. 2018b. 基于跳跃知识网络的图表示学习。在 *国际机器学习会议*。PMLR, 5453–5462。
- en: 'Xu et al. (2014) Qiumin Xu, Hyeran Jeon, and Murali Annavaram. 2014. Graph
    processing on GPUs: Where are the bottlenecks?. In *2014 IEEE Int’l Symposium
    on Workload Characterization (IISWC)*. IEEE, 140–149.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2014) Qiumin Xu, Hyeran Jeon, 和 Murali Annavaram. 2014. GPU 上的图处理：瓶颈在哪里？在
    *2014 IEEE 国际工作负载特征研讨会 (IISWC)*。IEEE, 140–149。
- en: 'Yadati et al. (2019) Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram
    Nitin, Anand Louis, and Partha Talukdar. 2019. Hypergcn: A new method for training
    graph convolutional networks on hypergraphs. *Advances in neural information processing
    systems* 32 (2019).'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yadati 等 (2019) Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin,
    Anand Louis, 和 Partha Talukdar. 2019. Hypergcn：一种在超图上训练图卷积网络的新方法。*神经信息处理系统进展*
    32 (2019)。
- en: 'Yan et al. (2014) Da Yan, James Cheng, Yi Lu, and Wilfred Ng. 2014. Blogel:
    A block-centric framework for distributed computation on real-world graphs. *Proc.
    of the VLDB Endowment* 7, 14 (2014), 1981–1992.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等 (2014) Da Yan, James Cheng, Yi Lu, 和 Wilfred Ng. 2014. Blogel：一种基于区块的分布式计算框架，适用于真实世界图。*VLDB
    赠款记录* 7, 14 (2014), 1981–1992。
- en: 'Yang (2019) Hongxia Yang. 2019. Aligraph: A comprehensive graph neural network
    platform. In *Proc. of the 25th ACM SIGKDD int’l conf. on knowledge discovery
    & data mining*. 3165–3166.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang (2019) Hongxia Yang. 2019. Aligraph：一个全面的图神经网络平台。在 *第 25 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*。3165–3166。
- en: Yang and Leskovec (2015) Jaewon Yang and Jure Leskovec. 2015. Defining and evaluating
    network communities based on ground-truth. *Knowledge and Information Systems*
    42, 1 (2015), 181–213.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 和 Leskovec (2015) Jaewon Yang 和 Jure Leskovec. 2015. 基于真实数据定义和评估网络社区。*知识与信息系统*
    42, 1 (2015), 181–213。
- en: Yao et al. (2019) Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Graph convolutional
    networks for text classification. In *Proc. of the AAAI conf. on AI*, Vol. 33\.
    7370–7377.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等 (2019) Liang Yao, Chengsheng Mao, 和 Yuan Luo. 2019. 用于文本分类的图卷积网络。在 *AAAI
    人工智能会议论文集*，第 33 卷。7370–7377。
- en: Ying et al. (2018) Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L
    Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
    recommender systems. In *Proc. of the 24th ACM SIGKDD Int’l Conf. on Knowledge
    Discovery & Data Mining*. 974–983.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ying 等 (2018) Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William
    L Hamilton, 和 Jure Leskovec. 2018. 用于网页规模推荐系统的图卷积神经网络。在 *第 24 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*。974–983。
- en: You et al. (2019) Jiaxuan You, Rex Ying, and Jure Leskovec. 2019. Position-aware
    graph neural networks. In *Int’l Conf. on ML*. PMLR, 7134–7143.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等 (2019) Jiaxuan You, Rex Ying, 和 Jure Leskovec. 2019. 位置感知图神经网络。在 *国际机器学习会议*。PMLR,
    7134–7143。
- en: 'Zeng et al. (2019) Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal
    Kannan, and Viktor Prasanna. 2019. Graphsaint: Graph sampling based inductive
    learning method. *arXiv preprint arXiv:1907.04931* (2019).'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等 (2019) Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan,
    和 Viktor Prasanna. 2019. Graphsaint：基于图采样的归纳学习方法。*arXiv 预印本 arXiv:1907.04931*
    (2019)。
- en: Zhang et al. (2020e) Bingyi Zhang, Hanqing Zeng, and Viktor Prasanna. 2020e.
    Hardware acceleration of large scale gcn inference. In *2020 IEEE 31st Int’l Conf.
    on Application-specific Systems, Architectures and Processors (ASAP)*. IEEE, 61–68.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020e) Bingyi Zhang, Hanqing Zeng, 和 Viktor Prasanna. 2020e. 大规模 GCN
    推理的硬件加速。在 *2020 IEEE 第 31 届国际应用系统、架构与处理器会议 (ASAP)*。IEEE, 61–68。
- en: 'Zhang et al. (2020c) Dalong Zhang, Xin Huang, Ziqi Liu, Zhiyang Hu, Xianzheng
    Song, Zhibang Ge, Zhiqiang Zhang, Lin Wang, Jun Zhou, Yang Shuang, et al. 2020c.
    Agl: a scalable system for industrial-purpose graph machine learning. *arXiv preprint
    arXiv:2003.02454* (2020).'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020c) Dalong Zhang, Xin Huang, Ziqi Liu, Zhiyang Hu, Xianzheng Song,
    Zhibang Ge, Zhiqiang Zhang, Lin Wang, Jun Zhou, Yang Shuang, 等. 2020c. Agl：一个用于工业目的图机器学习的可扩展系统。*arXiv
    预印本 arXiv:2003.02454* (2020)。
- en: 'Zhang et al. (2020d) Fuyang Zhang, Nelson Nauata, and Yasutaka Furukawa. 2020d.
    Conv-mpn: Convolutional message passing neural network for structured outdoor
    architecture reconstruction. In *Proc. of the IEEE/CVF Conf. on Computer Vision
    and Pattern Recognition*. 2798–2807.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020d) Fuyang Zhang, Nelson Nauata, 和 Yasutaka Furukawa. 2020d.
    Conv-mpn：用于结构化户外建筑重建的卷积消息传递神经网络。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*。2798–2807.
- en: 'Zhang et al. (2022) Shichang Zhang, Yozen Liu, Yizhou Sun, and Neil Shah. 2022.
    Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation. In
    *Int’l Conf. on Learning Representations*. [https://openreview.net/forum?id=4p6_5HBWPCw](https://openreview.net/forum?id=4p6_5HBWPCw)'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) Shichang Zhang, Yozen Liu, Yizhou Sun, 和 Neil Shah. 2022.
    图无关神经网络：通过蒸馏教旧的MLPs新技巧。发表于*国际学习表征会议*。[https://openreview.net/forum?id=4p6_5HBWPCw](https://openreview.net/forum?id=4p6_5HBWPCw)
- en: 'Zhang et al. (2021b) Yao Zhang, Yun Xiong, Dongsheng Li, Caihua Shan, Kan Ren,
    and Yangyong Zhu. 2021b. CoPE: Modeling Continuous Propagation and Evolution on
    Interaction Graph. In *Proc. of the 30th ACM Int’l Conf. on Information & Knowledge
    Management*. 2627–2636.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021b) Yao Zhang, Yun Xiong, Dongsheng Li, Caihua Shan, Kan Ren,
    和 Yangyong Zhu. 2021b. CoPE：在交互图上建模连续传播与演变。发表于*第30届ACM国际信息与知识管理会议论文集*。2627–2636.
- en: Zhang et al. (2020a) Zhen Zhang, Chaokun Chang, Haibin Lin, Yida Wang, Raman
    Arora, and Xin Jin. 2020a. Is network the bottleneck of distributed training?.
    In *Proc. of the Workshop on Network Meets AI & ML*. 8–13.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020a) Zhen Zhang, Chaokun Chang, Haibin Lin, Yida Wang, Raman
    Arora, 和 Xin Jin. 2020a. 网络是否是分布式训练的瓶颈？发表于*网络与AI & ML研讨会论文集*。8–13.
- en: 'Zhang et al. (2020b) Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2020b. Deep learning
    on graphs: A survey. *IEEE Trans. on Knowledge and Data Engineering* (2020).'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020b) Ziwei Zhang, Peng Cui, 和 Wenwu Zhu. 2020b. 图上的深度学习：一项调查。*IEEE知识与数据工程汇刊*
    (2020).
- en: 'Zhang et al. (2021a) Zhihui Zhang, Jingwen Leng, Shuwen Lu, Youshan Miao, Yijia
    Diao, Minyi Guo, Chao Li, and Yuhao Zhu. 2021a. ZIPPER: Exploiting Tile-and Operator-level
    Parallelism for General and Scalable Graph Neural Network Acceleration. *arXiv
    preprint arXiv:2107.08709* (2021).'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021a) Zhihui Zhang, Jingwen Leng, Shuwen Lu, Youshan Miao, Yijia
    Diao, Minyi Guo, Chao Li, 和 Yuhao Zhu. 2021a. ZIPPER：利用块级和操作级并行性加速通用可扩展的图神经网络。*arXiv预印本
    arXiv:2107.08709* (2021).
- en: Zhang et al. (2018) Zhaoning Zhang, Lujia Yin, Yuxing Peng, and Dongsheng Li.
    2018. A quick survey on large scale distributed deep learning systems. In *2018
    IEEE 24th Int’l Conf. on Parallel and Distrib. Systems (ICPADS)*. IEEE, 1052–1056.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018) Zhaoning Zhang, Lujia Yin, Yuxing Peng, 和 Dongsheng Li.
    2018. 大规模分布式深度学习系统的快速调查。发表于*2018 IEEE第24届国际并行与分布式系统会议（ICPADS）*。IEEE, 1052–1056.
- en: Zhao et al. (2020) Yiren Zhao, Duo Wang, Daniel Bates, Robert Mullins, Mateja
    Jamnik, and Pietro Lio. 2020. Learned low precision graph neural networks. *arXiv
    preprint arXiv:2009.09232* (2020).
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2020) Yiren Zhao, Duo Wang, Daniel Bates, Robert Mullins, Mateja
    Jamnik, 和 Pietro Lio. 2020. 学习的低精度图神经网络。*arXiv预印本 arXiv:2009.09232* (2020).
- en: 'Zheng et al. (2020) Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su,
    Xiang Song, Quan Gan, Zheng Zhang, and George Karypis. 2020. Distdgl: distributed
    graph neural network training for billion-scale graphs. In *2020 IEEE/ACM 10th
    Workshop on Irregular Applications: Architectures and Algorithms (IA3)*. IEEE,
    36–44.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2020) Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su,
    Xiang Song, Quan Gan, Zheng Zhang, 和 George Karypis. 2020. Distdgl：用于十亿规模图的分布式图神经网络训练。发表于*2020
    IEEE/ACM第10届不规则应用：架构与算法研讨会（IA3）*。IEEE, 36–44.
- en: Zhou et al. (2021) Hongkuan Zhou, Ajitesh Srivastava, Hanqing Zeng, Rajgopal
    Kannan, and Viktor Prasanna. 2021. Accelerating Large Scale Real-Time GNN Inference
    Using Channel Pruning. *Proc. VLDB Endow.* 14, 9 (oct 2021), 1597–1605.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2021) Hongkuan Zhou, Ajitesh Srivastava, Hanqing Zeng, Rajgopal
    Kannan, 和 Viktor Prasanna. 2021. 使用通道剪枝加速大规模实时GNN推断。*VLDB期刊* 14, 9 (2021年10月),
    1597–1605.
- en: 'Zhou et al. (2020) Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng
    Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural
    networks: A review of methods and applications. *AI Open* 1 (2020), 57–81.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2020) Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng
    Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, 和 Maosong Sun. 2020. 图神经网络：方法与应用综述。*AI
    Open* 1 (2020), 57–81.
- en: Zhu et al. (2015) Xiaowei Zhu, Wentao Han, and Wenguang Chen. 2015. $\{$GridGraph$\}$:$\{$Large-Scale$\}$
    Graph Processing on a Single Machine Using 2-Level Hierarchical Partitioning.
    In *2015 USENIX Ann. Tech. Conf. (USENIX ATC 15)*. 375–386.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2015) Xiaowei Zhu, Wentao Han, 和 Wenguang Chen. 2015. $\{$GridGraph$\}$：在单台机器上使用2级层次划分进行大规模图处理。发表于*2015年USENIX年会技术会议（USENIX
    ATC 15）*。375–386.
- en: Zitnik and Leskovec (2017) Marinka Zitnik and Jure Leskovec. 2017. Predicting
    multicellular function through multi-layer tissue networks. *Bioinformatics* 33,
    14 (2017), i190–i198.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zitnik 和 Leskovec（2017）Marinka Zitnik 和 Jure Leskovec。2017。通过多层组织网络预测多细胞功能。*Bioinformatics*
    33, 14（2017），i190–i198。
- en: Zou et al. (2019) Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and
    Quanquan Gu. 2019. Layer-dependent importance sampling for training deep and large
    graph convolutional networks. *arXiv preprint arXiv:1911.07323* (2019).
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人（2019）Difan Zou、Ziniu Hu、Yewen Wang、Song Jiang、Yizhou Sun 和 Quanquan Gu。2019。用于训练深度和大规模图卷积网络的层依赖重要性采样。*arXiv
    预印本 arXiv:1911.07323*（2019）。
