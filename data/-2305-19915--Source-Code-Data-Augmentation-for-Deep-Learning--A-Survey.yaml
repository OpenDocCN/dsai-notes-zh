- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:39:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:39:13
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.19915] Source Code Data Augmentation for Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.19915] 深度学习的源代码数据增强：一项调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.19915](https://ar5iv.labs.arxiv.org/html/2305.19915)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.19915](https://ar5iv.labs.arxiv.org/html/2305.19915)
- en: 'Source Code Data Augmentation for Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的源代码数据增强：一项调查
- en: Terry Yue Zhuo^(1,2†), Zhou Yang³, Zhensu Sun³,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Terry Yue Zhuo^(1,2†)，Zhou Yang³，Zhensu Sun³，
- en: Yufei Wang⁴, Li Li⁵, Xiaoning Du¹, Zhenchang Xing^(2,6), David Lo³
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Yufei Wang⁴，Li Li⁵，Xiaoning Du¹，Zhenchang Xing^(2,6)，David Lo³
- en: ¹ Monash University ² CSIRO’s Data61 ³ Singapore Management University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 莫纳什大学 ² CSIRO的数据61 ³ 新加坡管理大学
- en: ⁴ Huawei Noah’s Ark Lab ⁵ Beihang University ⁶ Australian National University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 华为诺亚方舟实验室 ⁵ 北京航空航天大学 ⁶ 澳大利亚国立大学
- en: terry.zhuo@monash.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: terry.zhuo@monash.edu
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The increasingly popular adoption of deep learning models in many critical source
    code tasks motivates the development of data augmentation (DA) techniques to enhance
    training data and improve various capabilities (e.g., robustness and generalizability)
    of these models. Although a series of DA methods have been proposed and tailored
    for source code models, there lacks a comprehensive survey and examination to
    understand their effectiveness and implications. This paper fills this gap by
    conducting a comprehensive and integrative survey of data augmentation for source
    code, wherein we systematically compile and encapsulate existing literature to
    provide a comprehensive overview of the field. We start with an introduction of
    data augmentation in source code and then provide a discussion on major representative
    approaches. Next, we highlight the general strategies and techniques to optimize
    the DA quality. Subsequently, we underscore techniques useful in real-world source
    code scenarios and downstream tasks. Finally, we outline the prevailing challenges
    and potential opportunities for future research. In essence, we aim to demystify
    the corpus of existing literature on source code DA for deep learning, and foster
    further exploration in this sphere. Complementing this, we present a continually
    updated GitHub repository that hosts a list of update-to-date papers on DA for
    source code modeling, accessible at [https://github.com/terryyz/DataAug4Code](https://github.com/terryyz/DataAug4Code).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型在许多关键源代码任务中的日益普及，激发了数据增强（DA）技术的发展，以提升训练数据并改善这些模型的各种能力（例如，鲁棒性和泛化能力）。尽管已经提出了一系列专门针对源代码模型的数据增强方法，但缺乏对其有效性和影响的全面调查和检验。本文通过对源代码数据增强进行全面和综合的调查来填补这一空白，我们系统地编纂并总结现有文献，提供该领域的全面概述。我们从源代码中的数据增强介绍开始，然后讨论主要的代表性方法。接着，我们突出优化数据增强质量的总体策略和技术。随后，我们强调在实际源代码场景和下游任务中有用的技术。最后，我们概述了当前的挑战和未来研究的潜在机会。实质上，我们旨在揭示现有文献在深度学习源代码数据增强中的核心内容，并促进该领域的进一步探索。补充说明的是，我们提供了一个持续更新的GitHub库，包含有关源代码建模的数据增强的最新论文列表，访问链接：[https://github.com/terryyz/DataAug4Code](https://github.com/terryyz/DataAug4Code)。
- en: ^†^†$\dagger$ Corresponding author.![Refer to caption](img/4ff3e7f759722e9a9df10968999b2d6c.png)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†$\dagger$ 通讯作者。![参见说明](img/4ff3e7f759722e9a9df10968999b2d6c.png)
- en: 'Figure 1: Yearly publications on the topic of “Source Code DA for Deep Learning”.
    Data Statistics as of November 2023.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：关于“深度学习源代码数据增强”主题的年度出版物统计。数据统计截至2023年11月。
- en: '![Refer to caption](img/6d7db8ce5ae0332e47abb47715bfe0a5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6d7db8ce5ae0332e47abb47715bfe0a5.png)'
- en: 'Figure 2: Venue Distribution of the collected publications.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：收集的出版物的场所分布。
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Data augmentation (DA) is a technique used to increase the variety of training
    examples without collecting new data. It has gained popularity in recent machine
    learning (ML) research, with methods like back-translation Sennrich et al. ([2015](#bib.bib99));
    Shiri et al. ([2022](#bib.bib105)), Mixup Zhang et al. ([2018](#bib.bib149)),
    and synthetic audio Asyrofi et al. ([2021](#bib.bib9)) being widely adopted in
    natural language processing (NLP), computer vision (CV), and speech recognition.
    These techniques have significantly improved the performance of data-centric models
    in low-resource domains. For example, Fadaee et al. ([2017](#bib.bib32)) obtain
    substantial improvements for low-resource machine translation via DA, where the
    translation system is trained with the bilingual pairs synthesized from a limited
    training corpus.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强（DA）是一种用于增加训练示例多样性的技术，而无需收集新数据。它在最近的机器学习（ML）研究中获得了广泛关注，像回译 Sennrich 等 ([2015](#bib.bib99));
    Shiri 等 ([2022](#bib.bib105))、混合方法 Zhang 等 ([2018](#bib.bib149)) 和合成音频 Asyrofi
    等 ([2021](#bib.bib9)) 等方法在自然语言处理（NLP）、计算机视觉（CV）和语音识别中被广泛采用。这些技术显著提高了低资源领域数据中心模型的性能。例如，Fadaee
    等 ([2017](#bib.bib32)) 通过 DA 实现了低资源机器翻译的显著改进，其中翻译系统使用从有限训练语料库合成的双语对进行训练。
- en: 'However, DA has not yet been fully explored in source code modeling, which
    is the intersection of ML and software engineering (SE). Source code modeling
    is an emerging area that applies ML techniques to solve various source code tasks
    such as code completion Yin and Neubig ([2017](#bib.bib145)), code summarization McBurney
    and McMillan ([2014](#bib.bib72)), and defect detection Wang et al. ([2016](#bib.bib126)),
    by training models on a vast amount of data available in open-source repositories Allamanis
    et al. ([2017](#bib.bib3)). Source code data typically has two modalities: the
    programming language (e.g., Python and Java) and the natural language (e.g., doc-strings
    and code comments), which complement each other. Such dual-modality nature of
    source code data presents unique challenges in tailoring DA for NLP to source
    code models. For example, the context of a sentence can be relatively standalone
    or derived from a few surrounding sentences in many NLP tasks Feng et al. ([2021](#bib.bib34)).
    However, in source code, the context can span across multiple functions or even
    different files, due to the widespread use of function calls, object-oriented
    programming, and modular design. Therefore, we argue that DA methods for source
    code would need to take this extended context into account, to avoid introducing
    errors or changing the original program’s behavior. In addition, source code follows
    strict syntactic rules that are specified using context-free grammar. Consequently,
    conventional NLP data augmentation methods, such as token substitution with similar
    words, may make the augmented source code fail to compile and introduce erroneous
    knowledge for training models.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据增强（DA）在源代码建模中的应用尚未完全探索，而源代码建模是机器学习（ML）与软件工程（SE）的交集。源代码建模是一个新兴领域，它运用ML技术解决各种源代码任务，例如代码完成
    Yin 和 Neubig ([2017](#bib.bib145))、代码摘要 McBurney 和 McMillan ([2014](#bib.bib72))
    和缺陷检测 Wang 等 ([2016](#bib.bib126))，通过在开源库 Allamanis 等 ([2017](#bib.bib3)) 中的大量数据上训练模型。源代码数据通常具有两种模态：编程语言（例如
    Python 和 Java）和自然语言（例如 doc-strings 和代码注释），它们互为补充。这种源代码数据的双模态特性在将 DA 定制为源代码模型的
    NLP 时带来了独特的挑战。例如，在许多 NLP 任务中，一句话的上下文可以相对独立或从周围几句话中推导而来 Feng 等 ([2021](#bib.bib34))。然而，在源代码中，由于函数调用、面向对象编程和模块化设计的广泛使用，上下文可以跨越多个函数甚至不同的文件。因此，我们认为源代码的
    DA 方法需要考虑这种扩展上下文，以避免引入错误或改变原始程序的行为。此外，源代码遵循使用上下文无关文法指定的严格语法规则。因此，传统的 NLP 数据增强方法，例如用类似单词进行的标记替换，可能会导致增强的源代码无法编译，并为训练模型引入错误知识。
- en: Despite such challenges, there has been increasing interest and demand for DA
    for source code modeling. With the growing accessibility of large, off-the-shelf,
    pre-trained source code models via learning from large-scale corpora Chen et al.
    ([2021](#bib.bib15)); Li et al. ([2023b](#bib.bib57)); Allal et al. ([2023](#bib.bib2)),
    there is a growing focus on applying these models to real-world software development.
    For instance, (Husain et al., [2019](#bib.bib50)) observed that many programming
    languages are low-resource, emphasizing the importance of DA to improve model
    performance and robustness on unseen data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管面临这些挑战，对源代码建模的DA（数据增强）的兴趣和需求仍在增加。随着通过大规模语料库学习获得的大型现成预训练源代码模型的日益可及（Chen et
    al. ([2021](#bib.bib15)); Li et al. ([2023b](#bib.bib57)); Allal et al. ([2023](#bib.bib2))），越来越多地关注将这些模型应用于实际的软件开发。例如，（Husain
    et al., [2019](#bib.bib50)）观察到许多编程语言资源稀缺，强调了DA在提高模型在未见数据上的性能和鲁棒性方面的重要性。
- en: 'This study aims to bring attention from both ML and SE communities to this
    emerging field. As depicted in Figure [2](#S0.F2 "Figure 2 ‣ Source Code Data
    Augmentation for Deep Learning: A Survey"), the relevant publications have been
    increasing in the recent five years. More precisely, we have compiled a list of
    89 core papers from the past five years, mainly from premier conferences and journals
    in both the ML and SE disciplines as shown in Figure [2](#S0.F2 "Figure 2 ‣ Source
    Code Data Augmentation for Deep Learning: A Survey") (with 62 out of 89 papers
    published in Core Rank A/A* venues¹¹1We refer to the venues listed at [http://portal.core.edu.au/conf-ranks/](http://portal.core.edu.au/conf-ranks/)
    and [http://portal.core.edu.au/jnl-ranks/](http://portal.core.edu.au/jnl-ranks/).).
    Given the escalating interest and burgeoning research in this domain, it is timely
    for our survey to (1) provide a comprehensive overview of DA for source code models,
    and (2) pinpoint key challenges and opportunities to stimulate and guide further
    exploration in this emerging field. To the best of our awareness, our paper constitutes
    the first comprehensive survey offering an in-depth examination of DA techniques
    for source code models.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '本研究旨在引起机器学习（ML）和软件工程（SE）社区对这一新兴领域的关注。如图[2](#S0.F2 "Figure 2 ‣ Source Code Data
    Augmentation for Deep Learning: A Survey")所示，相关出版物在最近五年中不断增加。更准确地说，我们编制了一份来自过去五年的89篇核心论文的清单，主要来自ML和SE学科的顶级会议和期刊，如图[2](#S0.F2
    "Figure 2 ‣ Source Code Data Augmentation for Deep Learning: A Survey")所示（其中89篇论文中有62篇发表在Core
    Rank A/A*场所¹¹1我们参考了[http://portal.core.edu.au/conf-ranks/](http://portal.core.edu.au/conf-ranks/)和[http://portal.core.edu.au/jnl-ranks/](http://portal.core.edu.au/jnl-ranks/)的场所列表）。鉴于该领域日益增长的兴趣和蓬勃发展的研究，我们的调查适时地（1）提供对源代码模型DA的全面概述，以及（2）指出关键挑战和机会，以激发和指导进一步的探索。根据我们的了解，我们的论文是首个全面调查源代码模型DA技术的研究。'
- en: 'The structure of this paper is organized as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文的结构安排如下：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [3](#S3 "3 Source Code Data Augmentation Methods for Deep Learning
    ‣ Source Code Data Augmentation for Deep Learning: A Survey") offers a thorough
    review of three categories of DA for source code modeling: rule-based ([3.1](#S3.SS1
    "3.1 Rule-based Techniques ‣ 3 Source Code Data Augmentation Methods for Deep
    Learning ‣ Source Code Data Augmentation for Deep Learning: A Survey")), model-based
    ([3.2](#S3.SS2 "3.2 Model-based Techniques ‣ 3 Source Code Data Augmentation Methods
    for Deep Learning ‣ Source Code Data Augmentation for Deep Learning: A Survey")),
    and example interpolation-based ([3.3](#S3.SS3 "3.3 Example Interpolation Techniques
    ‣ 3 Source Code Data Augmentation Methods for Deep Learning ‣ Source Code Data
    Augmentation for Deep Learning: A Survey")) techniques.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[3](#S3 "3 Source Code Data Augmentation Methods for Deep Learning ‣ Source
    Code Data Augmentation for Deep Learning: A Survey")节提供了对三类源代码建模DA的全面审查：基于规则的（[3.1](#S3.SS1
    "3.1 Rule-based Techniques ‣ 3 Source Code Data Augmentation Methods for Deep
    Learning ‣ Source Code Data Augmentation for Deep Learning: A Survey")）、基于模型的（[3.2](#S3.SS2
    "3.2 Model-based Techniques ‣ 3 Source Code Data Augmentation Methods for Deep
    Learning ‣ Source Code Data Augmentation for Deep Learning: A Survey")）和基于示例插值的（[3.3](#S3.SS3
    "3.3 Example Interpolation Techniques ‣ 3 Source Code Data Augmentation Methods
    for Deep Learning ‣ Source Code Data Augmentation for Deep Learning: A Survey")）技术。'
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [4](#S4 "4 Strategies and Techniques ‣ Source Code Data Augmentation
    for Deep Learning: A Survey") provides a summary of prevalent strategies and techniques
    designed to enhance the quality of augmented data, encompassing method stacking
    ([4.1](#S4.SS1 "4.1 Method Stacking ‣ 4 Strategies and Techniques ‣ Source Code
    Data Augmentation for Deep Learning: A Survey")) and optimization ([4.2](#S4.SS2
    "4.2 Optimization ‣ 4 Strategies and Techniques ‣ Source Code Data Augmentation
    for Deep Learning: A Survey")).'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[4](#S4 "4 Strategies and Techniques ‣ Source Code Data Augmentation for Deep
    Learning: A Survey")节提供了提高增强数据质量的常见策略和技术的总结，包括方法叠加（[4.1](#S4.SS1 "4.1 Method Stacking
    ‣ 4 Strategies and Techniques ‣ Source Code Data Augmentation for Deep Learning:
    A Survey")）和优化（[4.2](#S4.SS2 "4.2 Optimization ‣ 4 Strategies and Techniques ‣
    Source Code Data Augmentation for Deep Learning: A Survey")）。'
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [5](#S5 "5 Scenarios ‣ Source Code Data Augmentation for Deep Learning:
    A Survey") articulates various beneficial source code scenarios for DA, including
    adversarial examples for robustness ([5.1](#S5.SS1 "5.1 Adversarial Examples for
    Robustness ‣ 5 Scenarios ‣ Source Code Data Augmentation for Deep Learning: A
    Survey")), low-resource domains ([5.2](#S5.SS2 "5.2 Low-Resource Domains ‣ 5 Scenarios
    ‣ Source Code Data Augmentation for Deep Learning: A Survey")), retrieval augmentation
    ([5.3](#S5.SS3 "5.3 Retrieval Augmentation ‣ 5 Scenarios ‣ Source Code Data Augmentation
    for Deep Learning: A Survey")), and contrastive learning ([5.4](#S5.SS4 "5.4 Contrastive
    Learning ‣ 5 Scenarios ‣ Source Code Data Augmentation for Deep Learning: A Survey")).'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[5](#S5 "5 Scenarios ‣ Source Code Data Augmentation for Deep Learning: A
    Survey")节阐述了各种有益的源代码数据增强场景，包括增强鲁棒性的对抗示例（[5.1](#S5.SS1 "5.1 Adversarial Examples
    for Robustness ‣ 5 Scenarios ‣ Source Code Data Augmentation for Deep Learning:
    A Survey")）、低资源领域（[5.2](#S5.SS2 "5.2 Low-Resource Domains ‣ 5 Scenarios ‣ Source
    Code Data Augmentation for Deep Learning: A Survey")）、检索增强（[5.3](#S5.SS3 "5.3
    Retrieval Augmentation ‣ 5 Scenarios ‣ Source Code Data Augmentation for Deep
    Learning: A Survey")）和对比学习（[5.4](#S5.SS4 "5.4 Contrastive Learning ‣ 5 Scenarios
    ‣ Source Code Data Augmentation for Deep Learning: A Survey")）。'
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [6](#S6 "6 Downstream Tasks ‣ Source Code Data Augmentation for Deep
    Learning: A Survey") delineates DA methodologies for common source code tasks,
    such as code authorship attribution ([6.1](#S6.SS1 "6.1 Code Authorship Attribution
    ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")),
    clone detection ([6.2](#S6.SS2 "6.2 Clone Detection ‣ 6 Downstream Tasks ‣ Source
    Code Data Augmentation for Deep Learning: A Survey")), defect detection and repair
    ([6.3](#S6.SS3 "6.3 Defect Detection and Repair ‣ 6 Downstream Tasks ‣ Source
    Code Data Augmentation for Deep Learning: A Survey")), code summarization ([6.4](#S6.SS4
    "6.4 Code Summarization ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation for
    Deep Learning: A Survey")), code search ([6.5](#S6.SS5 "6.5 Code Search ‣ 6 Downstream
    Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")), code completion
    ([6.6](#S6.SS6 "6.6 Code Completion ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation
    for Deep Learning: A Survey")), code translation ([6.7](#S6.SS7 "6.7 Code Translation
    ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")),
    code question answering ([6.8](#S6.SS8 "6.8 Code Question Answering (CQA) ‣ 6
    Downstream Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")),
    problem classification ([6.9](#S6.SS9 "6.9 Code Classification ‣ 6 Downstream
    Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")), method name
    prediction ([6.10](#S6.SS10 "6.10 Method Name Prediction ‣ 6 Downstream Tasks
    ‣ Source Code Data Augmentation for Deep Learning: A Survey")), and type prediction
    ([6.11](#S6.SS11 "6.11 Type Prediction ‣ 6 Downstream Tasks ‣ Source Code Data
    Augmentation for Deep Learning: A Survey")).'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[6](#S6 "6 Downstream Tasks ‣ Source Code Data Augmentation for Deep Learning:
    A Survey")节描述了常见源代码任务的DA方法，如代码作者归属（[6.1](#S6.SS1 "6.1 Code Authorship Attribution
    ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")）、克隆检测（[6.2](#S6.SS2
    "6.2 Clone Detection ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation for
    Deep Learning: A Survey")）、缺陷检测和修复（[6.3](#S6.SS3 "6.3 Defect Detection and Repair
    ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")）、代码总结（[6.4](#S6.SS4
    "6.4 Code Summarization ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation for
    Deep Learning: A Survey")）、代码搜索（[6.5](#S6.SS5 "6.5 Code Search ‣ 6 Downstream
    Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")）、代码完成（[6.6](#S6.SS6
    "6.6 Code Completion ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation for
    Deep Learning: A Survey")）、代码翻译（[6.7](#S6.SS7 "6.7 Code Translation ‣ 6 Downstream
    Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")）、代码问答（[6.8](#S6.SS8
    "6.8 Code Question Answering (CQA) ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation
    for Deep Learning: A Survey")）、问题分类（[6.9](#S6.SS9 "6.9 Code Classification ‣ 6
    Downstream Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")）、方法名预测（[6.10](#S6.SS10
    "6.10 Method Name Prediction ‣ 6 Downstream Tasks ‣ Source Code Data Augmentation
    for Deep Learning: A Survey")）和类型预测（[6.11](#S6.SS11 "6.11 Type Prediction ‣ 6
    Downstream Tasks ‣ Source Code Data Augmentation for Deep Learning: A Survey")）。'
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [7](#S7 "7 Challenges and Opportunities ‣ Source Code Data Augmentation
    for Deep Learning: A Survey") expounds on the challenges and future prospects
    in the realm of DA for source code modeling.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[7](#S7 "7 Challenges and Opportunities ‣ Source Code Data Augmentation for
    Deep Learning: A Survey")节阐述了源代码建模领域中数据增强（DA）的挑战和未来前景。'
- en: Through this work, we hope to emulate prior surveys which have analyzed DA techniques
    for other data types, such as text Feng et al. ([2021](#bib.bib34)), time series Wen
    et al. ([2020](#bib.bib132)), and images Shorten and Khoshgoftaar ([2019](#bib.bib106)).
    Our intention is to pique further interest, spark curiosity, and encourage further
    research in the field of data augmentation, specifically focusing on its application
    to source code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这项工作，我们希望模仿先前的调查，这些调查分析了其他数据类型的DA技术，例如文本（Feng et al. ([2021](#bib.bib34))）、时间序列（Wen
    et al. ([2020](#bib.bib132))）和图像（Shorten and Khoshgoftaar ([2019](#bib.bib106))）。我们的意图是激发更多兴趣，引发好奇心，并鼓励在数据增强领域的进一步研究，特别是关注其在源代码中的应用。
- en: 2 Background
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 What are source code models?
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 什么是源代码模型？
- en: Source code models are trained on large-scale corpora of source code and therefore
    able to model the contextual representations of given code snippets Allamanis
    et al. ([2017](#bib.bib3)). In the early stage, researchers have attempted to
    leverage deep learning architectures like LSTM Gu et al. ([2016](#bib.bib37))
    and Seq2Seq Yin and Neubig ([2017](#bib.bib145)) to model the source code like
    plain text, and shown that these models can achieve great performance on specific
    downstream tasks of source code. With the development of pre-trained language
    models in NLP, many pre-trained source code models are proposed to enhance the
    source code representations and efficiently be scaled to any downstream tasks Feng
    et al. ([2020](#bib.bib35)); Guo et al. ([2021](#bib.bib39)); Nijkamp et al. ([2023](#bib.bib81)).
    Some of these models incorporate the inherent structure of code. For example,
    instead of taking the syntactic-level structure of source code like ASTs, Guo
    et al. ([2021](#bib.bib39)) consider program data flow in the pre-training stage,
    which is a semantic-level structure of code that encodes the relation of “where-the-value-comes-from”
    between variables. In this survey, we focus DA methods designed for all the deep-learning-based
    source code models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码模型在大规模源代码语料库上进行训练，因此能够对给定代码片段的上下文表示进行建模 Allamanis et al. ([2017](#bib.bib3))。在早期，研究人员尝试利用深度学习架构如
    LSTM Gu et al. ([2016](#bib.bib37)) 和 Seq2Seq Yin 和 Neubig ([2017](#bib.bib145))
    将源代码建模为类似普通文本的形式，并展示了这些模型在特定下游任务上的优异性能。随着 NLP 中预训练语言模型的发展，提出了许多预训练源代码模型，以增强源代码表示并有效地扩展到任何下游任务
    Feng et al. ([2020](#bib.bib35)); Guo et al. ([2021](#bib.bib39)); Nijkamp et
    al. ([2023](#bib.bib81))。这些模型中的一些结合了代码的固有结构。例如，Guo et al. ([2021](#bib.bib39))
    在预训练阶段考虑了程序数据流，这是代码的语义级结构，编码了变量之间的“值来源”关系。本文综述了针对所有基于深度学习的源代码模型设计的数据增强方法。
- en: 2.2 What is data augmentation?
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 什么是数据增强？
- en: Data augmentation (DA) techniques aim to improve the model’s performance in
    terms of various aspects (e.g., accuracy and robustness) via increasing training
    example diversity with data synthesis. Besides, DA techniques can help avoid model
    overfitting in the training stage, which maintains the generability of the model.
    In CV, DA techniques with predefined rules are commonly adopted when training
    models, such as image cropping, image flipping, and color jittering Shorten and
    Khoshgoftaar ([2019](#bib.bib106)). These techniques can be classified as rule-based
    DA. Furthermore, some attempts like Mixup have been made to create new examples
    by fusing multiple examples together, which is categorized as example interpolation
    DA. Compared to CV, DA techniques for NLP greatly rely on language models that
    can help paraphrase the given context by word replacing or sentence rewriting Feng
    et al. ([2021](#bib.bib34)). As most of these language models are pre-trained
    and can capture the semantics of inputs, they serve as reasonable frameworks to
    modify or paraphrase the plain text. We denote such DA methods as model-based
    DA.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强（DA）技术旨在通过数据合成增加训练示例的多样性，以提高模型在各种方面（如准确性和鲁棒性）的性能。此外，DA 技术可以帮助避免模型在训练阶段的过拟合，从而保持模型的泛化能力。在计算机视觉（CV）中，通常采用带有预定义规则的
    DA 技术进行模型训练，如图像裁剪、图像翻转和颜色抖动 Shorten 和 Khoshgoftaar ([2019](#bib.bib106))。这些技术可被归类为基于规则的
    DA。此外，像 Mixup 这样的尝试通过融合多个示例来创建新示例，这被归类为示例插值 DA。与计算机视觉相比，NLP 中的 DA 技术在很大程度上依赖于语言模型，这些模型可以通过单词替换或句子重写来帮助改写给定的上下文
    Feng et al. ([2021](#bib.bib34))。由于这些语言模型大多是预训练的，并能够捕捉输入的语义，因此它们作为合理的框架来修改或改写普通文本。我们将这种
    DA 方法称为基于模型的 DA。
- en: 2.3 How does data augmentation work in source code?
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 数据增强在源代码中的工作原理是什么？
- en: Compared to images and plain texts, source code is less flexible to be augmented
    due to the nature of strict programming syntactic rules. Hence, we observe that
    most DA approaches in source code must follow the predetermined transformation
    rules in order to preserve the functionality and syntax of the original code snippets.
    To enable the complex processing of the given source code, a common approach is
    to use a parser to build a concrete syntax tree from the code, which represents
    the program grammar in a tree-like form. The concrete syntax tree will be further
    transformed into an abstract syntax tree (AST) to simplify the representation
    but maintain the key information such as identifiers, if-else statements, and
    loop conditions. The parsed information is utilized as the basis of the rule-based
    DA approaches for identifier replacement and statement rewrite Quiring et al.
    ([2019](#bib.bib91)). From a software engineering perspective, these DA approaches
    can emulate more diverse code representation in real-world scenarios and thus
    make source code models more robust by training with the augmented data Yefet
    et al. ([2020](#bib.bib144)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像和纯文本相比，源代码由于严格的编程语法规则，其数据增强的灵活性较低。因此，我们观察到大多数源代码的数据增强方法必须遵循预定的转换规则，以保持原始代码片段的功能性和语法。为了使给定源代码的复杂处理成为可能，常见的方法是使用解析器从代码中构建具体语法树，该树以树状形式表示程序语法。具体语法树将进一步转化为抽象语法树（AST），以简化表示，但保留诸如标识符、if-else语句和循环条件等关键信息。解析后的信息作为基于规则的数据增强方法中标识符替换和语句重写的基础。Quiring等人（[2019](#bib.bib91)）。从软件工程的角度来看，这些数据增强方法可以模拟更为多样的代码表示方式，从而通过用增强的数据进行训练使源代码模型更加健壮。Yefet等人（[2020](#bib.bib144)）。
- en: 3 Source Code Data Augmentation Methods for Deep Learning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习的源代码数据增强方法
- en: 'This section categorizes the mainstream DA techniques specifically designed
    for source code models into three parts: rule-based, model-based, and example-interpolation
    techniques. We explain studies of different branches as follows.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将专门为源代码模型设计的主流数据增强（DA）技术分为三部分：基于规则的、基于模型的和基于示例插值的技术。我们将分别解释不同分支的研究。
- en: 3.1 Rule-based Techniques
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于规则的技术
- en: 'A large number of DA methods utilize predetermined rules to transform the programs
    without breaking syntax rules and semantics. Specifically, these rules mainly
    implicitly leverage ASTs to transform the code snippets. The transformations can
    include operations such as replacing variable names, renaming method names, and
    inserting dead code. Besides the basic program syntax, some code transformations
    consider deeper structural information, such as control-flow graph (CGF) and use-define
    chains (UDG) Quiring et al. ([2019](#bib.bib91)). Additionally, a small part of
    rule-based DA techniques focuses on augmenting the natural language context in
    the code snippets, including doc-strings and comments Bahrami et al. ([2021](#bib.bib11));
    Song et al. ([2022](#bib.bib108)); Park et al. ([2023](#bib.bib86)). We illustrate
    a rule-based DA example relying on program grammars in Figure [3](#S3.F3 "Figure
    3 ‣ 3.1 Rule-based Techniques ‣ 3 Source Code Data Augmentation Methods for Deep
    Learning ‣ Source Code Data Augmentation for Deep Learning: A Survey").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的数据增强方法利用预定规则对程序进行转换，同时不破坏语法规则和语义。具体而言，这些规则主要隐式利用AST来转换代码片段。这些转换可以包括替换变量名、重命名方法名和插入死代码等操作。除了基本的程序语法，一些代码转换还考虑了更深层次的结构信息，如控制流图（CGF）和使用-定义链（UDG）Quiring等人（[2019](#bib.bib91)）。此外，一小部分基于规则的数据增强技术侧重于增强代码片段中的自然语言上下文，包括文档字符串和注释。Bahrami等人（[2021](#bib.bib11)）；Song等人（[2022](#bib.bib108)）；Park等人（[2023](#bib.bib86)）。我们在图[3](#S3.F3
    "图 3 ‣ 3.1 基于规则的技术 ‣ 3 深度学习的源代码数据增强方法 ‣ 源代码数据增强的深度学习：综述")中展示了一个依赖于程序语法的基于规则的数据增强示例。
- en: '![Refer to caption](img/847293400c3204716856d317a1b23ec2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/847293400c3204716856d317a1b23ec2.png)'
- en: 'Figure 3: Rule-based DA to transform code snippets, Wang et al. ([2022d](#bib.bib129)).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于规则的数据增强以转换代码片段，Wang等人（[2022d](#bib.bib129)）。
- en: Zhang et al. ([2020a](#bib.bib150)) propose MHM, a method of iteratively renaming
    identifiers in the code snippets. Considered as the approach to generate examples
    for adversarial training, MHM greatly improves the robustness of source code models.
    Later, [Srikant et al.](#bib.bib110) consider program obfuscations as adversarial
    perturbations, where they rename program variables in an attempt to hide the program’s
    intent from a reader. By applying these perturbed examples to the training stage,
    the source code models become more robust to the adversarial attack. Instead of
    just renaming identifiers, BUGLAB-Aug Allamanis et al. ([2021](#bib.bib5)) contains
    more rules to augment code snippets, emphasizing both the programming language
    and natural language, such as comment deletion, comparison expression mirroring,
    and if-else branch swapping. The evaluation on BUGLAB-Aug demonstrates that DA
    methods can be exploited for self-supervised bug detection and repair. Similarly,
    Jain et al. ([2021](#bib.bib51)) use compiler transforms as data augmentation,
    called Transpiler, automatically generating a dataset of equivalent functions.
    Specifically, they define 11 compiler transforms by exploiting ASTs of the programs.
    Rule-based DA later has been widely used for source code models to capture code
    representation effectively via contrastive learning Ding et al. ([2021](#bib.bib27));
    Liu et al. ([2023b](#bib.bib67)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人 ([2020a](#bib.bib150)) 提出了 MHM，这是一种在代码片段中迭代重命名标识符的方法。MHM 被认为是生成对抗训练示例的方法，它极大地提高了源代码模型的鲁棒性。后来，[Srikant
    等人](#bib.bib110) 将程序混淆视为对抗扰动，他们重命名程序变量以试图隐藏程序的意图。通过将这些扰动示例应用于训练阶段，源代码模型变得对对抗攻击更具鲁棒性。BUGLAB-Aug
    Allamanis 等人 ([2021](#bib.bib5)) 在单纯重命名标识符的基础上，包含了更多增强代码片段的规则，强调编程语言和自然语言，如注释删除、比较表达式镜像和
    if-else 分支交换。对 BUGLAB-Aug 的评估表明，DA 方法可以用于自监督的错误检测和修复。同样，Jain 等人 ([2021](#bib.bib51))
    使用编译器转换作为数据增强，称为 Transpiler，自动生成等效函数的数据集。他们通过利用程序的 AST 定义了 11 种编译器转换规则。基于规则的 DA
    后来被广泛用于源代码模型，通过对比学习有效捕捉代码表示 Ding 等人 ([2021](#bib.bib27)); Liu 等人 ([2023b](#bib.bib67))。
- en: Brockschmidt et al. ([2019](#bib.bib13)) present a generative source code model
    by augmenting the given AST with additional edges to learn diverse code expressions.
    Instead of the direct augmentation on AST, Quiring et al. ([2019](#bib.bib91))
    propose three different augmentation schemes via the combination of AST and CFG,
    UDG and declaration-reference mapping (DRM), named as Control Transformations,
    Declaration Transformations and API Transformations. Control Transformations rewrite
    control-flow statements or modify the control flow between functions. In total,
    the family contains 5 transformations. This transformation involves passing variables
    as function arguments, updating their values, and changing the control flow of
    the caller and callee. Declaration Transformations consist of 14 transformers
    that modify, add or remove declarations in source code. Declaration Transformations
    make DA necessary to update all usages of variables which can be elegantly carried
    out using the DRM representation. API Transformations contain 9 transformations
    and exploits the fact that various APIs can be used to solve the same problem.
    Programmers are known to favor different APIs and thus tampering with API usage
    is an effective strategy for changing stylistic patterns.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Brockschmidt 等人 ([2019](#bib.bib13)) 通过在给定的 AST 上增加额外的边来提出一种生成源代码模型，以学习多样的代码表达。Quiring
    等人 ([2019](#bib.bib91)) 提出了三种不同的增强方案，通过 AST 和 CFG、UDG 以及声明-引用映射 (DRM) 的组合，分别称为控制转换、声明转换和
    API 转换。控制转换重写控制流语句或修改函数之间的控制流。总共有 5 种转换。这些转换涉及将变量作为函数参数传递、更新其值以及改变调用者和被调用者的控制流。声明转换包含
    14 种转换器，修改、添加或删除源代码中的声明。声明转换使 DA 在更新所有变量使用方面变得必要，这可以通过 DRM 表示优雅地完成。API 转换包含 9
    种转换，利用了各种 API 可以用来解决相同问题的事实。程序员通常偏好不同的 API，因此篡改 API 使用是一种有效的策略，用于改变风格模式。
- en: 'Another line of work is augmenting the natural language context in source code.
    QRA Huang et al. ([2021](#bib.bib47)) augments examples by rewriting natural language
    queries when performing code search and code question answering. It rewrites queries
    with minor rule-based modifications that share the same semantics as the original
    one. Specifically, it consists of three ways: randomly deleting a word, randomly
    switching the position of two words, and randomly copying a word. Inspired by
    this approach, Park et al. ([2023](#bib.bib86)) recently devised KeyDAC with an
    emphasis on the query keywords. KeyDAC augments on both natural language and programming
    language. For natural language query, it follows the rules in QRA but only modifies
    non-keywords. In terms of programming language augmentation, KeyDAC simply uses
    ASTs to rename program variables, similar to the aforementioned works.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作是增强源代码中的自然语言上下文。QRA Huang等人（[2021](#bib.bib47)）通过在执行代码搜索和代码问答时重写自然语言查询来增强示例。它通过一些规则基于的微小修改重写查询，这些修改与原始查询具有相同的语义。具体而言，它包括三种方式：随机删除一个词、随机交换两个词的位置和随机复制一个词。受到这种方法的启发，Park等人（[2023](#bib.bib86)）最近设计了以查询关键字为重点的KeyDAC。KeyDAC在自然语言和编程语言上都进行增强。对于自然语言查询，它遵循QRA中的规则，但仅修改非关键字。在编程语言增强方面，KeyDAC简单地使用ASTs来重命名程序变量，类似于前述工作。
- en: 3.2 Model-based Techniques
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于模型的技术
- en: A series of DA techniques for source code target training various models to
    augment data. Intuitively, Mi et al. ([2021](#bib.bib76)) utilize Auxiliary Classifier
    Generative Adversarial Networks (AC-GAN) Odena et al. ([2017](#bib.bib82)) to
    generate augmented programs. Similarly, Wang et al. ([2023a](#bib.bib123)) trained
    a generative adversarial network to boost code generation and code search at the
    same time. To increase the training data for code summarization, CDA-CS Song et al.
    ([2022](#bib.bib108)) uses the pre-trained BERT model Devlin et al. ([2019](#bib.bib24))
    to replace synonyms for non-keywords in code comments, which benefits the source
    code downstream tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列用于源代码的DA技术旨在训练各种模型以增强数据。从直观上看，Mi等人（[2021](#bib.bib76)）利用辅助分类生成对抗网络（AC-GAN）Odena等人（[2017](#bib.bib82)）来生成增强的程序。同样，Wang等人（[2023a](#bib.bib123)）训练了一个生成对抗网络，以同时提升代码生成和代码搜索。为了增加代码摘要的训练数据，CDA-CS
    Song等人（[2022](#bib.bib108)）使用预训练的BERT模型 Devlin等人（[2019](#bib.bib24)）来替换代码注释中的非关键字同义词，这有利于源代码下游任务。
- en: While these methods largely adapt the existing model-based DA techniques for
    general purposes, most DA approaches are specifically designed for source code
    models. Li et al. ([2022f](#bib.bib63)) introduce IRGen, a genetic-algorithm-based
    model using compiler intermediate representation (LLVM IR) to augment source code
    embeddings, where IRGen generates a piece of source code into a range of semantically
    identical but syntactically distinct IR codes for improving model’s contextual
    understanding. Several works Roziere et al. ([2021](#bib.bib98)); Ahmad et al.
    ([2023](#bib.bib1)); Silva et al. ([2023](#bib.bib107)) have investigated the
    suitability of the multilingual generative source code models for unsupervised
    programming language translation via Back-translation, in the similar scope of
    the one for NLP Sennrich et al. ([2016](#bib.bib100)). However, unlike the one
    in NLP, Back-translation here is defined as translating between two programming
    languages via the natural language as an intermediate language. Pinku et al. ([2023](#bib.bib88))
    exploit another generative source code model, Transcoder Roziere et al. ([2020](#bib.bib97)),
    to perform source-to-source translation for augmenting cross-language source code.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些方法大多将现有的基于模型的DA技术适配于一般用途，但大多数DA方法是专门为源代码模型设计的。Li等人（[2022f](#bib.bib63)）引入了IRGen，这是一种基于遗传算法的模型，使用编译器中间表示（LLVM
    IR）来增强源代码嵌入，其中IRGen将一段源代码生成一系列语义相同但语法不同的IR代码，以提高模型的上下文理解。几项工作 Roziere等人（[2021](#bib.bib98)）；Ahmad等人（[2023](#bib.bib1)）；Silva等人（[2023](#bib.bib107)）已经研究了多语言生成源代码模型在无监督编程语言翻译中的适用性，通过回译，类似于NLP中的
    Sennrich等人（[2016](#bib.bib100)）。然而，与NLP中的回译不同的是，这里的回译定义为通过自然语言作为中间语言在两种编程语言之间进行翻译。Pinku等人（[2023](#bib.bib88)）利用另一种生成源代码模型Transcoder
    Roziere等人（[2020](#bib.bib97)）进行源到源的翻译，以增强跨语言源代码。
- en: 3.3 Example Interpolation Techniques
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 示例插值技术
- en: Another category of data augmentation (DA) techniques, originated by Mixup Zhang
    et al. ([2018](#bib.bib149)), involves interpolating the inputs and labels of
    two or more actual examples. For instance, given that a binary classification
    task in CV and two images of a dog and a cat, respectively, these DA approaches
    like Mixup can blend these two image inputs and their corresponding labels based
    on a randomly selected weight. This collection of methods is also termed Mixed
    Sample Data Augmentation. Despite trials in the context of text classification
    problems, such methods are hard to be deployed in the realm of source code, as
    each code snippet is constrained by its unique program grammar and functionality.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类数据增强（DA）技术，源于 Mixup Zhang 等人 ([2018](#bib.bib149))，涉及对两个或多个实际示例的输入和标签进行插值。例如，在计算机视觉中的二分类任务中，给定一张狗的图片和一张猫的图片，这些
    DA 方法如 Mixup 可以根据随机选择的权重混合这两个图像输入及其对应的标签。这类方法也被称为混合样本数据增强。尽管在文本分类问题的背景下进行过尝试，但这种方法在源代码领域很难部署，因为每段代码都受到其独特程序语法和功能的限制。
- en: '![Refer to caption](img/8f403905a376e23b1688ec1923bdac13.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8f403905a376e23b1688ec1923bdac13.png)'
- en: 'Figure 4: MixCode, Dong et al. ([2023a](#bib.bib29)).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MixCode，Dong 等人 ([2023a](#bib.bib29))。
- en: 'In contrast to the aforementioned surface-level interpolation, the majority
    of example-interpolation DA methods are enhanced to fuse multiple real examples
    into a single input via model embeddings Feng et al. ([2021](#bib.bib34)). As
    an illustration in Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Example Interpolation Techniques
    ‣ 3 Source Code Data Augmentation Methods for Deep Learning ‣ Source Code Data
    Augmentation for Deep Learning: A Survey"), Dong et al. ([2023a](#bib.bib29))
    merge rule-based techniques for source code models with Mixup to blend the representations
    of the original code snippet and its transformation. This approach is commonly
    regarded as the linear interpolation technique deployed in NLP classification
    tasks.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述表面插值方法相比，大多数示例插值 DA 方法被增强为通过模型嵌入将多个真实示例融合为单个输入，Feng 等人 ([2021](#bib.bib34))。如图
    [4](#S3.F4 "图 4 ‣ 3.3 示例插值技术 ‣ 3 深度学习的源代码数据增强方法 ‣ 深度学习的数据增强调查") 所示，Dong 等人 ([2023a](#bib.bib29))
    将基于规则的源代码模型技术与 Mixup 结合，混合原始代码片段及其变换的表示。这种方法通常被认为是应用于 NLP 分类任务的线性插值技术。
- en: Li et al. ([2022a](#bib.bib55)) introduce two novel interpolation techniques
    for source code models, namely Binary Interpolation and Linear Extrapolation.
    Binary Interpolation serves as a data augmentation strategy, which interchangeably
    swaps features between samples using elements acquired from a Bernoulli distribution.
    On the other hand, Linear Extrapolation is another data augmentation approach
    that generates new data points beyond the existing feature space by extending
    current features in accordance with a uniform distribution.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 ([2022a](#bib.bib55)) 引入了两种新颖的源代码模型插值技术，即二元插值和线性外推。二元插值作为一种数据增强策略，通过使用从伯努利分布中获得的元素在样本之间交换特征。另一方面，线性外推是一种数据增强方法，通过根据均匀分布扩展当前特征，生成超出现有特征空间的新数据点。
- en: 4 Strategies and Techniques
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 策略与技术
- en: In real-world applications, the design and efficacy of DA techniques for source
    code models are influenced by a variety of factors, such as computing cost, example
    diversity, and models’ robustness. This section highlights these factors, offering
    insights and techniques for devising and optimizing suitable DA methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，源代码模型的 DA 技术的设计和效果受到多种因素的影响，如计算成本、示例多样性和模型的鲁棒性。本节重点介绍这些因素，提供了制定和优化适当
    DA 方法的见解和技术。
- en: 4.1 Method Stacking
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 方法堆叠
- en: 'As discussed in Section [3](#S3 "3 Source Code Data Augmentation Methods for
    Deep Learning ‣ Source Code Data Augmentation for Deep Learning: A Survey"), numerous
    DA strategies are proposed concurrently in a single work, aiming to enhance the
    models’ performance. [Add one sentence to define method stacking] Typically, the
    combination entails two types: same-type DA or a mixture of different DA methods.
    The former is typically applied in rule-based DA techniques, stemming from the
    realization that a single code transformation cannot fully represent the diverse
    code style and implementation found in the real world.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '如第 [3](#S3 "3 Source Code Data Augmentation Methods for Deep Learning ‣ Source
    Code Data Augmentation for Deep Learning: A Survey") 节讨论的那样，许多 DA 策略在单一工作中同时提出，旨在提高模型的性能。[添加一句话以定义方法堆叠]
    通常，这种组合包括两种类型：同类型 DA 或不同 DA 方法的混合。前者通常应用于基于规则的 DA 技术，因为单一的代码变换无法完全代表现实世界中的多样代码风格和实现。'
- en: 'Several works Shi et al. ([2023](#bib.bib102)); Huang et al. ([2021](#bib.bib47))
    demonstrate that merging multiple types of DA techniques can enhance the performance
    of source code models. Mi et al. ([2021](#bib.bib76)) combined rule-based code
    transformation schemes with model-based DA using AC-GAN to create an augmented
    corpus for model training. Instead of augmenting on programming language, CDA-CS Song
    et al. ([2022](#bib.bib108)) encompasses two kinds of DA techniques: rule-based
    non-keyword extraction and model-based non-keyword replacement. Empirical evidence
    from Chen and Lampouras ([2023](#bib.bib16)) shows that combining Back-translation
    and variable renaming can result in improved code completion performance.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究 Shi 等人 ([2023](#bib.bib102)); Huang 等人 ([2021](#bib.bib47)) 证明了合并多种 DA
    技术可以提高源代码模型的性能。**Mi** 等人 ([2021](#bib.bib76)) 将基于规则的代码变换方案与使用 AC-GAN 的基于模型的 DA
    结合起来，创建了一个用于模型训练的增强语料库。与编程语言增强不同，**CDA-CS** 宋等人 ([2022](#bib.bib108)) 包括两种 DA
    技术：基于规则的非关键字提取和基于模型的非关键字替换。陈和 Lampouras ([2023](#bib.bib16)) 的实证证据表明，结合回译和变量重命名可以改善代码完成性能。
- en: 4.2 Optimization
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 优化
- en: 'In certain scenarios such as enhancing robustness and minimizing computational
    cost, optimally selecting specific augmented example candidates is crucial. We
    denote such goal-oriented candidate selections in DA as optimization. Subsequently,
    we introduce three types of strategies: probabilistic, model-based, and rule-based
    selection. Probabilistic selection is defined as the optimization via sampling
    from a probability distribution, while model-based selection is guided by the
    model to select the most proper examples. In terms of rule-based selection, it
    is an optimization strategy where specific predetermined rules or heuristics are
    used to select the most suitable examples.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景下，如提升鲁棒性和最小化计算成本，优化选择特定的增强示例候选是至关重要的。我们将这种以目标为导向的候选选择称为优化。随后，我们介绍了三种策略：概率、基于模型和基于规则的选择。概率选择被定义为通过从概率分布中采样进行优化，而基于模型的选择则由模型指导，选择最合适的示例。在基于规则的选择方面，它是一种优化策略，使用特定的预定规则或启发式方法来选择最合适的示例。
- en: 4.2.1 Probabilistic Selection
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 概率选择
- en: We introduce three representative probabilistic selection strategies, MHM, QMDP,
    and BUGLAB-Aug. MHM Zhang et al. ([2020a](#bib.bib150)) adopts the Metropolis-Hastings
    probabilistic sampling method, which is a Markov Chain Monte Carlo technique,
    to choose adversarial examples via identifier replacement. Similarly, QMDP Tian
    et al. ([2021](#bib.bib116)) uses a Q-learning approach to strategically select
    and execute rule-based structural transformations on the source code, thereby
    guiding the generation of adversarial examples. In BUGLAB-Aug, Allamanis et al.
    ([2021](#bib.bib5)) model the probability of applying a specific rewrite rule
    at a location in a code snippet similar to the pointer net [Merity et al.](#bib.bib73)
    .
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了三种代表性的概率选择策略：MHM、QMDP 和 BUGLAB-Aug。**MHM** 张等人 ([2020a](#bib.bib150)) 采用了
    Metropolis-Hastings 概率采样方法，这是一种马尔可夫链蒙特卡罗技术，通过标识符替换选择对抗样本。类似地，**QMDP** 田等人 ([2021](#bib.bib116))
    使用 Q 学习方法来战略性地选择并执行基于规则的结构变换，从而指导对抗样本的生成。在 **BUGLAB-Aug** 中，Allamanis 等人 ([2021](#bib.bib5))
    模拟在代码片段中的特定位置应用特定重写规则的概率，类似于指针网络 [Merity et al.](#bib.bib73)。
- en: 4.2.2 Model-based Selection
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 基于模型的选择
- en: Several DA techniques employing this strategy use the model’s gradient information
    to guide the selection of augmented examples. An emblematic approach is the DAMP
    method Yefet et al. ([2020](#bib.bib144)), which optimizes based on the model
    loss to select and generate adversarial examples via variable renaming. Another
    variant, SPACE Li et al. ([2022c](#bib.bib60)), performs selection and perturbation
    of code identifiers’ embeddings via gradient ascent, targeting to maximize the
    model’s performance impact while upholding semantic and grammatical correctness
    of the programming language. A more complex technique, ALERT Yang et al. ([2022b](#bib.bib141)),
    uses a genetic algorithm in its gradient-based selection strategy. It evolves
    a population of candidate solutions iteratively, guided by a fitness function
    that calculates the model’s confidence difference, aiming to identify the most
    potent adversarial examples.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '-   一些采用该策略的 DA 技术使用模型的梯度信息来指导增强示例的选择。一个典型的方法是 DAMP 方法 Yefet 等人 ([2020](#bib.bib144))，该方法基于模型损失进行优化，通过变量重命名选择和生成对抗示例。另一个变体，SPACE
    Li 等人 ([2022c](#bib.bib60))，通过梯度上升执行代码标识符嵌入的选择和扰动，旨在最大化模型的性能影响，同时保持编程语言的语义和语法正确性。更复杂的技术
    ALERT Yang 等人 ([2022b](#bib.bib141)) 使用遗传算法进行基于梯度的选择策略。它通过一个适应度函数计算模型的置信度差异，迭代进化候选解决方案的种群，旨在识别最有效的对抗示例。'
- en: '| DA Method | Category | PL | NL | Optimization | Preprocess | Parsing | Level
    | TA | LA |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| DA 方法 | 类别 | PL | NL | 优化 | 预处理 | 解析 | 级别 | TA | LA |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| ComputeEdge Brockschmidt et al. ([2019](#bib.bib13)) | Rule | ✓ | ✗ | — |
    — | AST | AST | ✓ | ✓ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| ComputeEdge Brockschmidt 等人 ([2019](#bib.bib13)) | 规则 | ✓ | ✗ | — | — | AST
    | AST | ✓ | ✓ |'
- en: '| RefineRepresentation Bielik and Vechev ([2020](#bib.bib12)) | Rule | ✓ |
    ✗ | Model | — | AST | AST | ✓ | ✓ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 精细表示 Bielik 和 Vechev ([2020](#bib.bib12)) | 规则 | ✓ | ✗ | 模型 | — | AST | AST
    | ✓ | ✓ |'
- en: '| Control Transformations Quiring et al. ([2019](#bib.bib91)) | Rule | ✓ |
    ✗ | Prob | — | AST+CFG+UDG | Input | ✓ | ✗ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 控制转换 Quiring 等人 ([2019](#bib.bib91)) | 规则 | ✓ | ✗ | 概率 | — | AST+CFG+UDG
    | 输入 | ✓ | ✗ |'
- en: '| Declaration Transformations Quiring et al. ([2019](#bib.bib91)) | Rule |
    ✓ | ✗ | Prob | — | AST+DRM | Input | ✓ | ✗ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 声明转换 Quiring 等人 ([2019](#bib.bib91)) | 规则 | ✓ | ✗ | 概率 | — | AST+DRM | 输入
    | ✓ | ✗ |'
- en: '| API Transformations Quiring et al. ([2019](#bib.bib91)) | Rule | ✓ | ✗ |
    Prob | — | AST+CFG+DRM | Input | ✓ | ✗ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| API 转换 Quiring 等人 ([2019](#bib.bib91)) | 规则 | ✓ | ✗ | 概率 | — | AST+CFG+DRM
    | 输入 | ✓ | ✗ |'
- en: '| DAMP Yefet et al. ([2020](#bib.bib144)) | Rule | ✓ | ✗ | Model | — | AST
    | Input | ✓ | ✓ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| DAMP Yefet 等人 ([2020](#bib.bib144)) | 规则 | ✓ | ✗ | 模型 | — | AST | 输入 | ✓
    | ✓ |'
- en: '| IBA Huang et al. ([2021](#bib.bib47)) | Rule | ✗ | ✓ | — | Tok | — | Embed
    | ✗ | ✓ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| IBA 黄等人 ([2021](#bib.bib47)) | 规则 | ✗ | ✓ | — | Tok | — | 嵌入 | ✗ | ✓ |'
- en: '| QRA Huang et al. ([2021](#bib.bib47)) | Rule | ✓ | ✗ | — | Tok | — | Input
    | ✗ | ✓ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| QRA 黄等人 ([2021](#bib.bib47)) | 规则 | ✓ | ✗ | — | Tok | — | 输入 | ✗ | ✓ |'
- en: '| MHM Zhang et al. ([2020a](#bib.bib150)) | Rule | ✗ | ✓ | Prob | — | AST |
    Input | ✓ | ✗ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| MHM Zhang 等人 ([2020a](#bib.bib150)) | 规则 | ✗ | ✓ | 概率 | — | AST | 输入 | ✓
    | ✗ |'
- en: '| Mossad Devore-McDonald and Berger ([2020](#bib.bib25)) | Rule | ✓ | ✗ | Rule
    | Tok | AST | Input | ✓ | ✓ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Mossad Devore-McDonald 和 Berger ([2020](#bib.bib25)) | 规则 | ✓ | ✗ | 规则 |
    Tok | AST | 输入 | ✓ | ✓ |'
- en: '| AugmentedCode Bahrami et al. ([2021](#bib.bib11)) | Rule | ✓ | ✗ | — | Tok
    | — | Input | ✗ | ✓ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 增强代码 Bahrami 等人 ([2021](#bib.bib11)) | 规则 | ✓ | ✗ | — | Tok | — | 输入 | ✗
    | ✓ |'
- en: '| QMDP Tian et al. ([2021](#bib.bib116)) | Rule | ✓ | ✗ | Prob | Tok | AST
    | Input | ✓ | ✗ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| QMDP Tian 等人 ([2021](#bib.bib116)) | 规则 | ✓ | ✗ | 概率 | Tok | AST | 输入 | ✓
    | ✗ |'
- en: '| Transpiler Jain et al. ([2021](#bib.bib51)) | Rule | ✓ | ✗ | Prob | — | AST
    | Input | ✓ | ✗ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 转译器 Jain 等人 ([2021](#bib.bib51)) | 规则 | ✓ | ✗ | 概率 | — | AST | 输入 | ✓ | ✗
    |'
- en: '| BUGLAB-Aug Allamanis et al. ([2021](#bib.bib5)) | Rule | ✓ | ✗ | Prob | Tok
    | AST | Input | ✗ | ✓ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| BUGLAB-Aug Allamanis 等人 ([2021](#bib.bib5)) | 规则 | ✓ | ✗ | 概率 | Tok | AST
    | 输入 | ✗ | ✓ |'
- en: '| SPAT Yu et al. ([2022b](#bib.bib148)) | Rule | ✓ | ✗ | Model | — | AST |
    Input | ✓ | ✗ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| SPAT Yu 等人 ([2022b](#bib.bib148)) | 规则 | ✓ | ✗ | 模型 | — | AST | 输入 | ✓ |
    ✗ |'
- en: '| RoPGen Li et al. ([2022d](#bib.bib61)) | Rule | ✓ | ✗ | Model | — | AST |
    Input | ✓ | ✗ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| RoPGen Li 等人 ([2022d](#bib.bib61)) | 规则 | ✓ | ✗ | 模型 | — | AST | 输入 | ✓ |
    ✗ |'
- en: '| ACCENT Zhou et al. ([2022](#bib.bib156)) | Rule | ✓ | ✗ | Rule | — | AST
    | Input | ✓ | ✓ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ACCENT Zhou 等人 ([2022](#bib.bib156)) | 规则 | ✓ | ✗ | 规则 | — | AST | 输入 | ✓
    | ✓ |'
- en: '| SPACE Li et al. ([2022c](#bib.bib60)) | Rule | ✓ | ✗ | Model | Tok | AST
    | Embed | ✓ | ✓ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| SPACE Li 等人 ([2022c](#bib.bib60)) | 规则 | ✓ | ✗ | 模型 | Tok | AST | 嵌入 | ✓
    | ✓ |'
- en: '| ALERT Yang et al. ([2022b](#bib.bib141)) | Rule | ✓ | ✗ | Model | Tok | AST
    | Input | ✓ | ✓ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ALERT Yang 等人 ([2022b](#bib.bib141)) | 规则 | ✓ | ✗ | 模型 | Tok | AST | 输入 |
    ✓ | ✓ |'
- en: '| IRGen Li et al. ([2022f](#bib.bib63)) | Rule | ✓ | ✗ | Rule | — | AST+IR
    | IR | ✓ | ✓ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| IRGen Li 等 ([2022f](#bib.bib63)) | 规则 | ✓ | ✗ | 规则 | — | 抽象语法树 + IR | IR
    | ✓ | ✓ |'
- en: '| Binary Interpolation Li et al. ([2022a](#bib.bib55)) | EI | ✓ | ✓ | — | —
    | — | Embeb | ✓ | ✓ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 二进制插值 Li 等 ([2022a](#bib.bib55)) | 示例插值 (EI) | ✓ | ✓ | — | — | — | 嵌入 | ✓
    | ✓ |'
- en: '| Linear Extrapolation Li et al. ([2022a](#bib.bib55)) | EI | ✓ | ✓ | — | —
    | — | Embeb | ✓ | ✓ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 线性外推 Li 等 ([2022a](#bib.bib55)) | 示例插值 (EI) | ✓ | ✓ | — | — | — | 嵌入 | ✓
    | ✓ |'
- en: '| Gaussian Scaling Li et al. ([2022a](#bib.bib55)) | Rule | ✓ | ✓ | Model |
    — | — | Embeb | ✓ | ✓ |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 高斯缩放 Li 等 ([2022a](#bib.bib55)) | 规则 | ✓ | ✓ | 模型 | — | — | 嵌入 | ✓ | ✓ |'
- en: '| CodeTransformator Zubkov et al. ([2022](#bib.bib160)) | Rule | ✓ | ✗ | Rule
    | — | AST | Input | ✓ | ✗ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| CodeTransformator Zubkov 等 ([2022](#bib.bib160)) | 规则 | ✓ | ✗ | 规则 | — |
    抽象语法树 (AST) | 输入 | ✓ | ✗ |'
- en: '| RADAR Yang et al. ([2022a](#bib.bib138)) | Rule | ✓ | ✗ | Rule | — | AST
    | Input | ✓ | ✗ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| RADAR Yang 等 ([2022a](#bib.bib138)) | 规则 | ✓ | ✗ | 规则 | — | 抽象语法树 (AST) |
    输入 | ✓ | ✗ |'
- en: '| AC-GAN Mi et al. ([2021](#bib.bib76)) | Model | ✓ | ✗ | — | — | — | Input
    | ✓ | ✓ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| AC-GAN Mi 等 ([2021](#bib.bib76)) | 模型 | ✓ | ✗ | — | — | — | 输入 | ✓ | ✓ |'
- en: '| CDA-CS Song et al. ([2022](#bib.bib108)) | Model | ✗ | ✓ | Model | KWE |
    — | Input | ✗ | ✓ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| CDA-CS Song 等 ([2022](#bib.bib108)) | 模型 | ✗ | ✓ | 模型 | 关键词提取 (KWE) | — |
    输入 | ✗ | ✓ |'
- en: '| srcML-embed Li et al. ([2022e](#bib.bib62)) | Rule | ✓ | ✗ | — | — | AST
    | Embed | ✓ | ✗ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| srcML-embed Li 等 ([2022e](#bib.bib62)) | 规则 | ✓ | ✗ | — | — | 抽象语法树 (AST)
    | 嵌入 | ✓ | ✗ |'
- en: '| MultIPA Orvalho et al. ([2022](#bib.bib84)) | Rule | ✓ | ✗ | — | — | AST
    | Input | ✓ | ✗ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| MultIPA Orvalho 等 ([2022](#bib.bib84)) | 规则 | ✓ | ✗ | — | — | 抽象语法树 (AST)
    | 输入 | ✓ | ✗ |'
- en: '| ProgramTransformer Rabin and Alipour ([2022](#bib.bib92)) | Rule | ✓ | ✗
    | — | — | AST | Input | ✓ | ✗ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ProgramTransformer Rabin 和 Alipour ([2022](#bib.bib92)) | 规则 | ✓ | ✗ | —
    | — | 抽象语法树 (AST) | 输入 | ✓ | ✗ |'
- en: '| Back-translation Ahmad et al. ([2023](#bib.bib1)) | Model | ✓ | ✗ | — | Tok
    | — | Input | ✗ | ✓ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 反向翻译 Ahmad 等 ([2023](#bib.bib1)) | 模型 | ✓ | ✗ | — | 词汇 (Tok) | — | 输入 | ✗
    | ✓ |'
- en: '| MixCode Dong et al. ([2023a](#bib.bib29)) | Rule+EI | ✓ | ✓ | — | — | — |
    Embed | ✓ | ✓ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| MixCode Dong 等 ([2023a](#bib.bib29)) | 规则+EI | ✓ | ✓ | — | — | — | 嵌入 | ✓
    | ✓ |'
- en: '| NP-GD [Shen et al.](#bib.bib101) | Model | ✓ | ✗ | Model | Tok | — | Embed
    | ✓ | ✓ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| NP-GD [Shen 等](#bib.bib101) | 模型 | ✓ | ✗ | 模型 | 词汇 (Tok) | — | 嵌入 | ✓ | ✓
    |'
- en: '| ExploitGen Yang et al. ([2023a](#bib.bib137)) | Rule | ✗ | ✓ | — | — | —
    | Input | ✓ | ✗ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ExploitGen Yang 等 ([2023a](#bib.bib137)) | 规则 | ✗ | ✓ | — | — | — | 输入 |
    ✓ | ✗ |'
- en: '| SoDa Shi et al. ([2023](#bib.bib102)) | Model | ✓ | ✓ | — | — | AST | Input
    | ✓ | ✓ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| SoDa Shi 等 ([2023](#bib.bib102)) | 模型 | ✓ | ✓ | — | — | 抽象语法树 (AST) | 输入
    | ✓ | ✓ |'
- en: '| Transcompiler Pinku et al. ([2023](#bib.bib88)) | Model | ✓ | ✗ | — | — |
    — | Input | ✓ | ✗ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 转译器 Pinku 等 ([2023](#bib.bib88)) | 模型 | ✓ | ✗ | — | — | — | 输入 | ✓ | ✗ |'
- en: '| STRATA Springer et al. ([2021](#bib.bib109)) | Rule | ✓ | ✗ | Model | Tok
    | AST | Input | ✓ | ✓ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| STRATA Springer 等 ([2021](#bib.bib109)) | 规则 | ✓ | ✗ | 模型 | 词汇 (Tok) | 抽象语法树
    (AST) | 输入 | ✓ | ✓ |'
- en: '| KeyDAC Park et al. ([2023](#bib.bib86)) | Rule | ✓ | ✓ | — | KWE | AST |
    Embed | ✗ | ✓ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| KeyDAC Park 等 ([2023](#bib.bib86)) | 规则 | ✓ | ✓ | — | 关键词提取 (KWE) | 抽象语法树
    (AST) | 嵌入 | ✗ | ✓ |'
- en: '| Simplex Interpolation Zhang et al. ([2022](#bib.bib153)) | EI | ✓ | ✗ | —
    | — | AST+IR | Embed | ✗ | ✓ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 简单插值 Zhang 等 ([2022](#bib.bib153)) | 示例插值 (EI) | ✓ | ✗ | — | — | 抽象语法树 +
    IR | 嵌入 | ✗ | ✓ |'
- en: 'Table 1: Comparing a selection of DA methods by various aspects relating to
    their applicability, dependencies, and requirements. PL, NL, TA, LA, EI, Prob,
    Tok, and KWE stand for Programming Language, Natural Language, Example Interpolation,
    Probability, Tokenization, Keyword Extraction, Task-Agnostic, and Language-Agnostic.
    PL and NL determine if the DA method is applied to the programming language or
    natural language context. Preprocess denotes preprocessing required besides the
    program parsing. Parsing refers to the type of feature used by the DA method during
    program parsing. Level denotes the depth at which data is modified by the DA.
    TA and LA represent whether the DA method can be applied to different tasks or
    programming languages. As most papers do not clearly state if their DA methods
    are TA and LA, we subjectively denote the applicability.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：通过各个方面比较一些 DA 方法的适用性、依赖性和要求。PL、NL、TA、LA、EI、Prob、Tok 和 KWE 分别代表编程语言、自然语言、示例插值、概率、分词、关键词提取、任务无关和语言无关。PL
    和 NL 决定 DA 方法是否应用于编程语言或自然语言上下文。预处理表示除了程序解析外所需的预处理。解析指 DA 方法在程序解析期间使用的特征类型。级别表示
    DA 修改数据的深度。TA 和 LA 表示 DA 方法是否可以应用于不同的任务或编程语言。由于大多数论文没有明确说明其 DA 方法是否为 TA 和 LA，我们主观地标记了适用性。
- en: 4.2.3 Rule-based Selection
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 基于规则的选择
- en: Rule-based selection stands as a powerful approach, featuring predetermined
    fitness functions or rules. This method often relies on evaluation metrics for
    decision-making. For instance, IRGen Li et al. ([2022f](#bib.bib63)) utilizes
    a Genetic-Algorithm-based optimization technique with a fitness function based
    on IR similarity. On the other hand, ACCENT Zhou et al. ([2022](#bib.bib156))
    and RADAR apply evaluation metrics such as BLEU Papineni et al. ([2002](#bib.bib85))
    and CodeBLEU Ren et al. ([2020](#bib.bib96)) respectively to guide the selection
    and replacement process, aiming for maximum adversarial impact. Finally, STRATA Springer
    et al. ([2021](#bib.bib109)) employs a rule-based technique to select high-impact
    subtokens that significantly alter the model’s interpretation of the code.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的选择是一种强有力的方法，具有预定的适应度函数或规则。这种方法通常依赖于评估指标进行决策。例如，IRGen Li 等 ([2022f](#bib.bib63))
    采用基于遗传算法的优化技术，适应度函数基于IR相似性。另一方面，ACCENT Zhou 等 ([2022](#bib.bib156)) 和RADAR分别应用评估指标，如BLEU Papineni
    等 ([2002](#bib.bib85)) 和CodeBLEU Ren 等 ([2020](#bib.bib96)) 来指导选择和替换过程，旨在实现最大对抗影响。最后，STRATA Springer
    等 ([2021](#bib.bib109)) 采用基于规则的技术来选择高影响的子令牌，这些子令牌显著改变模型对代码的解释。
- en: 5 Scenarios
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 场景
- en: This section delves into several commonplace scenarios of source code scenarios,
    where DA approaches can be applied.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本节**深入探讨**了几个常见的源代码场景，其中可以应用DA方法。
- en: 5.1 Adversarial Examples for Robustness
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 针对稳健性的对抗样本
- en: Robustness presents a critical and complex dimension of software engineering,
    necessitating the creation of semantically-preserved adversarial examples to discern
    and mitigate vulnerabilities within source code models. There is a surge in designing
    more effective DA techniques for generating these examples in recent years. Several
    studies Yefet et al. ([2020](#bib.bib144)); Li et al. ([2022d](#bib.bib61)); [Srikant
    et al.](#bib.bib110) ; Li et al. ([2022c](#bib.bib60)); [Anand et al.](#bib.bib8)
    ; Henke et al. ([2022](#bib.bib44)); Tian et al. ([2023](#bib.bib117)); Yang et al.
    ([2023b](#bib.bib139)); Li et al. ([2023c](#bib.bib59)); Gu et al. ([2023](#bib.bib38));
    Gao et al. ([2023](#bib.bib36)) have utilized various DA methods for testing and
    enhancing model robustness. Wang et al. ([2023b](#bib.bib124)) have gone a step
    further to consolidate universally accepted code transformation rules to establish
    the first benchmark for source code model robustness.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 稳健性呈现出软件工程中一个关键且复杂的维度，要求创建语义保留的对抗样本，以识别和减轻源代码模型中的漏洞。近年来，设计更有效的DA技术以生成这些样本的需求激增。若干研究 Yefet
    等 ([2020](#bib.bib144)); Li 等 ([2022d](#bib.bib61)); [Srikant 等](#bib.bib110)
    ; Li 等 ([2022c](#bib.bib60)); [Anand 等](#bib.bib8) ; Henke 等 ([2022](#bib.bib44));
    Tian 等 ([2023](#bib.bib117)); Yang 等 ([2023b](#bib.bib139)); Li 等 ([2023c](#bib.bib59));
    Gu 等 ([2023](#bib.bib38)); Gao 等 ([2023](#bib.bib36)) 已利用各种DA方法测试和增强模型的稳健性。Wang
    等 ([2023b](#bib.bib124)) 更进一步，通过整合普遍接受的代码转换规则，建立了源代码模型稳健性的第一个基准。
- en: 5.2 Low-Resource Domains
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 低资源领域
- en: In the domain of software engineering, the resources of programming languages
    are severely imbalanced Orlanski et al. ([2023](#bib.bib83)). While some most
    popular programming languages like Python and Java play major roles in the open-source
    repositories, many languages like Rust Wu et al. ([2023](#bib.bib134)) are starkly
    low-resource. As source code models are trained on open-source repositories and
    forums, the programming language resource imbalance can adversely impact their
    performance on the resource-scarce programming languages. Furthermore, the application
    of DA methods within low-resource domains is a recurrent theme within the CV and
    NLP communities Shorten and Khoshgoftaar ([2019](#bib.bib106)); Feng et al. ([2021](#bib.bib34)).
    Yet, this scenario remains underexplored within the source code discipline.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程领域，编程语言的资源严重不均衡 Orlanski 等 ([2023](#bib.bib83))。尽管一些如Python和Java等流行编程语言在开源代码库中扮演重要角色，但许多语言如Rust Wu
    等 ([2023](#bib.bib134)) 则资源极其稀缺。由于源代码模型在开源代码库和论坛上进行训练，编程语言资源的不均衡可能会对它们在资源稀缺编程语言上的表现产生负面影响。此外，在低资源领域应用DA方法是CV和NLP社区中的一个反复出现的主题 Shorten
    和 Khoshgoftaar ([2019](#bib.bib106)); Feng 等 ([2021](#bib.bib34))。然而，在源代码学科中，这一场景仍然没有被充分探索。
- en: In order to increase data in the low-resource domain for representation learning,
    Li et al. ([2022f](#bib.bib63)) tend to add more training data to enhance source
    code model embeddings by unleashing the power of compiler IR. Ahmad et al. ([2023](#bib.bib1))
    propose to use source code models to perform Back-translation DA, taking into
    consideration the scenario of low-resource programming languages. Meanwhile, Chen
    and Lampouras ([2023](#bib.bib16)) underscore the fact that source code datasets
    are markedly smaller than their NLP equivalents, which often encompass millions
    of instances. As a result, they commence investigations into code completion tasks
    under this context and experiment with Back-translation and variable renaming.
    [Shen et al.](#bib.bib101) contend that the generation of bash comments is hampered
    by a dearth of training data and thus explore model-based DA methods for this
    task.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加低资源领域的数据以进行表示学习，Li et al. ([2022f](#bib.bib63)) 倾向于添加更多的训练数据，以通过释放编译器IR的力量来增强源代码模型嵌入。Ahmad
    et al. ([2023](#bib.bib1)) 提出了使用源代码模型进行回译DA，考虑到低资源编程语言的情况。同时，Chen 和 Lampouras
    ([2023](#bib.bib16)) 强调源代码数据集明显小于其NLP等效数据集，后者通常包含数百万个实例。因此，他们开始在这种背景下研究代码补全任务，并实验回译和变量重命名。[Shen
    et al.](#bib.bib101) 认为生成bash评论受到训练数据匮乏的阻碍，因此探索了用于此任务的模型基础DA方法。
- en: 5.3 Retrieval Augmentation
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 检索增强
- en: Increasing interest has been observed in the application of DA for retrieval
    augmentation within NLP Mialon et al. ([2023](#bib.bib77)) and source code Lu
    et al. ([2022](#bib.bib69)). These retrieval augmentation frameworks for source
    code models incorporate retrieval-augmented examples from the training set when
    pre-training or fine-tuning source code models. This form of augmentation enhances
    the parameter efficiency of models, as they are able to store less knowledge within
    their parameters and instead retrieve it. It is shown as a promising application
    of DA in various source code downstream tasks, such as code summarization Zhang
    et al. ([2020b](#bib.bib151)); [Liu et al.](#bib.bib66) ; Yu et al. ([2022a](#bib.bib147));
    Choi et al. ([2023](#bib.bib19)); Ye et al. ([2023](#bib.bib143)), code completion Parvez
    et al. ([2021](#bib.bib87)); Tang et al. ([2023](#bib.bib115)) and program repair Nashid
    et al. ([2023](#bib.bib80)); Jin et al. ([2023](#bib.bib53)); Wang et al. ([2023c](#bib.bib127)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP领域中，对DA用于检索增强的应用产生了越来越多的兴趣 Mialon et al. ([2023](#bib.bib77)) 和源代码 Lu et
    al. ([2022](#bib.bib69))。这些用于源代码模型的检索增强框架在预训练或微调源代码模型时，会从训练集中引入检索增强的示例。这种增强形式提高了模型的参数效率，因为它们可以在参数中存储更少的知识，而是通过检索来获取。这被展示为DA在各种源代码下游任务中的一个有前途的应用，如代码总结 Zhang
    et al. ([2020b](#bib.bib151)); [Liu et al.](#bib.bib66); Yu et al. ([2022a](#bib.bib147));
    Choi et al. ([2023](#bib.bib19)); Ye et al. ([2023](#bib.bib143)), 代码补全 Parvez
    et al. ([2021](#bib.bib87)); Tang et al. ([2023](#bib.bib115)) 和程序修复 Nashid et
    al. ([2023](#bib.bib80)); Jin et al. ([2023](#bib.bib53)); Wang et al. ([2023c](#bib.bib127)).
- en: 5.4 Contrastive Learning
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 对比学习
- en: Another source code scenario to deploy DA methods is contrastive learning, where
    it enables models to learn an embedding space in which similar samples are close
    to each other while dissimilar ones are far apart Chen et al. ([2022](#bib.bib17));
    Wang et al. ([2022c](#bib.bib128)); Zhang et al. ([2022](#bib.bib153)); Yang et al.
    ([2023c](#bib.bib140)); Ding et al. ([2023](#bib.bib28)). As the training datasets
    commonly contain limited sets of positive samples, DA methods are preferred to
    construct similar samples as the positive ones. Liu et al. ([2023b](#bib.bib67))
    make use of contrastive learning with DA to devise superior pre-training paradigms
    for source code models, while some works study the advantages of this application
    in some source code tasks like defect detection Cheng et al. ([2022](#bib.bib18)),
    clone detection Zubkov et al. ([2022](#bib.bib160)); Wang et al. ([2022a](#bib.bib121))
    and code search Shi et al. ([2022b](#bib.bib104), [2023](#bib.bib102)); Li et al.
    ([2022b](#bib.bib58)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种部署DA方法的源代码场景是对比学习，其中它使模型能够学习一个嵌入空间，使得相似样本彼此接近，而不相似的样本则远离 Chen et al. ([2022](#bib.bib17));
    Wang et al. ([2022c](#bib.bib128)); Zhang et al. ([2022](#bib.bib153)); Yang et
    al. ([2023c](#bib.bib140)); Ding et al. ([2023](#bib.bib28))。由于训练数据集通常包含有限的正样本，DA方法被优先用于构造与正样本相似的样本。Liu
    et al. ([2023b](#bib.bib67)) 利用对比学习与DA来制定优越的源代码模型预训练范式，而一些研究则探讨了这种应用在一些源代码任务中的优势，如缺陷检测 Cheng
    et al. ([2022](#bib.bib18))，克隆检测 Zubkov et al. ([2022](#bib.bib160)); Wang et
    al. ([2022a](#bib.bib121)) 和代码搜索 Shi et al. ([2022b](#bib.bib104), [2023](#bib.bib102));
    Li et al. ([2022b](#bib.bib58)).
- en: 6 Downstream Tasks
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 下游任务
- en: In this section, we discuss several DA works for common source code tasks and
    evaluation datasets.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了几种针对常见源代码任务和评估数据集的DA方法。
- en: 6.1 Code Authorship Attribution
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 代码作者归属
- en: Code authorship attribution is the process of identifying the author of a given
    code, usually achieved by source code models. Yang et al. ([2022b](#bib.bib141))
    initially investigate generating adversarial examples on the Google Code Jam (GCJ)
    dataset, which effectively fools source code models to identify the wrong author
    of a given code snippet. By training with these augmented examples, the model’s
    robustness can be further improved. Li et al. ([2022d](#bib.bib61)) propose another
    DA method called RoPGen for the adversarial attack and demonstrate its efficacy
    on GCJ. Dong et al. ([2023b](#bib.bib30)) empirically study the effectiveness
    of several existing DA approaches for NLP on several source code tasks, including
    authorship attribution on GCJ.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 代码作者归属是识别给定代码作者的过程，通常通过源代码模型实现。Yang 等人（[2022b](#bib.bib141)）最初研究了在 Google Code
    Jam (GCJ) 数据集上生成对抗样本，这些样本有效地欺骗了源代码模型，使其识别出错误的代码片段作者。通过使用这些增强样本进行训练，可以进一步提高模型的鲁棒性。Li
    等人（[2022d](#bib.bib61)）提出了另一种名为 RoPGen 的对抗攻击 DA 方法，并在 GCJ 上展示了其有效性。Dong 等人（[2023b](#bib.bib30)）实证研究了几种现有
    DA 方法在 NLP 中的有效性，涵盖了包括 GCJ 上的作者归属在内的多个源代码任务。
- en: 6.2 Clone Detection
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 克隆检测
- en: Code clone detection refers to the task of identifying if the given code snippet
    is cloned and modified from the original sample, and can be called plagiarism
    detection in some cases Zubkov et al. ([2022](#bib.bib160)); Pinku et al. ([2023](#bib.bib88));
    Hasija et al. ([2023](#bib.bib42)). This is a challenging downstream task as it
    needs the source code model to understand the source code both syntactically and
    semantically. Jain et al. ([2021](#bib.bib51)) propose correct-by-construction
    DA via compiler information to generate many variants with equivalent functionality
    of the training sample and show its effectiveness of improving the model robustness
    on BigCloneBench Svajlenko et al. ([2014](#bib.bib113)) and a self-collected JavaScript
    dataset. Jia et al. ([2023](#bib.bib52)) show that when training with adversarial
    examples via obfuscation transformation, the robustness of source code models
    can be significantly improved. Zubkov et al. ([2022](#bib.bib160)) provide the
    comparison of multiple contrastive learning frameworks, combined with rule-based
    transformations for the clone detection task. Pinku et al. ([2023](#bib.bib88))
    later use Transcompiler to translate between limited source code in Python and
    Java and increase the training data for cross-language code clone detection.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 代码克隆检测是指识别给定代码片段是否从原始样本中克隆并修改而来的任务，在某些情况下也称为抄袭检测 Zubkov 等人（[2022](#bib.bib160)）；Pinku
    等人（[2023](#bib.bib88)）；Hasija 等人（[2023](#bib.bib42)）。这是一个具有挑战性的下游任务，因为它需要源代码模型在语法和语义上都理解源代码。Jain
    等人（[2021](#bib.bib51)）通过编译器信息提出了正确构造的 DA 方法，生成了许多功能等效的训练样本变体，并展示了其在 BigCloneBench
    Svajlenko 等人（[2014](#bib.bib113)）和自收集的 JavaScript 数据集上提高模型鲁棒性的有效性。Jia 等人（[2023](#bib.bib52)）表明，当使用混淆变换的对抗样本进行训练时，源代码模型的鲁棒性可以显著提高。Zubkov
    等人（[2022](#bib.bib160)）提供了多种对比学习框架的比较，结合规则基于的变换用于克隆检测任务。Pinku 等人（[2023](#bib.bib88)）随后使用
    Transcompiler 在 Python 和 Java 之间进行有限源代码翻译，增加了跨语言代码克隆检测的训练数据。
- en: 6.3 Defect Detection and Repair
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 缺陷检测与修复
- en: Defect Detection and Repair, in other words, bug or vulnerability detection
    and fix, is to capture the bugs in given code snippets Cheng et al. ([2022](#bib.bib18));
    Dong et al. ([2023a](#bib.bib29)) and generate repaired versions Drain et al.
    ([2021](#bib.bib31)); Yasunaga and Liang ([2021](#bib.bib142)); Wang et al. ([2022b](#bib.bib122)).
    The task can be considered as the binary classification task, where the labels
    are either true or false. Allamanis et al. ([2021](#bib.bib5)) implement BUGLAB-Aug,
    a DA framework of self-supervised bug detection and repair. BUGLAB-Aug has two
    sets of code transformation rules, one is a bug-inducing rewrite and the other
    one is rewriting as DA. Their approach boosts the performance and robustness of
    source code models simultaneously. Cheng et al. ([2022](#bib.bib18)) present a
    path-sensitive code embedding technique called ContraFlow, which uses self-supervised
    contrastive learning to detect defects based on value-flow paths. ContraFlow utilizes
    DA to generate contrastive value-flow representations of three datasets (namely
    D2A Zheng et al. ([2021](#bib.bib154)), Fan Fan et al. ([2020](#bib.bib33)) and
    FFMPeg+Qemu Zhou et al. ([2019](#bib.bib155))) to learn the (dis)-similarity among
    programs. Ding et al. ([2021](#bib.bib27)) present a novel self-supervised model
    focusing on identifying (dis)similar functionalities of source code, which outperforms
    the state-of-the-art models on REVEAL Chakraborty et al. ([2022](#bib.bib14))
    and FFMPeg+Qemu Zhou et al. ([2019](#bib.bib155)). Specifically, they design code
    transformation heuristics to automatically create bugged programs and similar
    code for augmenting pre-training data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 缺陷检测和修复，换句话说，错误或漏洞检测与修复，是捕捉给定代码片段中的错误 Cheng 等人 ([2022](#bib.bib18))；董等人 ([2023a](#bib.bib29))
    和生成修复版本 Drain 等人 ([2021](#bib.bib31))；Yasunaga 和 Liang ([2021](#bib.bib142))；王等人
    ([2022b](#bib.bib122))。这个任务可以被认为是一个二分类任务，其中标签为真或假。Allamanis 等人 ([2021](#bib.bib5))
    实现了 BUGLAB-Aug，一个自监督错误检测与修复的 DA 框架。BUGLAB-Aug 有两套代码转换规则，一套是诱导错误的重写，另一套是作为 DA 的重写。他们的方法同时提升了源代码模型的性能和鲁棒性。程等人
    ([2022](#bib.bib18)) 提出了一种称为 ContraFlow 的路径敏感代码嵌入技术，该技术利用自监督对比学习基于值流路径检测缺陷。ContraFlow
    利用 DA 生成三个数据集（即 D2A 郑等人 ([2021](#bib.bib154))、范范等人 ([2020](#bib.bib33)) 和 FFMPeg+Qemu
    周等人 ([2019](#bib.bib155))) 的对比值流表示，以学习程序之间的（不）相似性。丁等人 ([2021](#bib.bib27)) 提出了一个新颖的自监督模型，专注于识别源代码的（不）相似功能，该模型在
    REVEAL Chakraborty 等人 ([2022](#bib.bib14)) 和 FFMPeg+Qemu 周等人 ([2019](#bib.bib155))
    上超越了现有的最先进模型。具体来说，他们设计了代码转换启发式方法以自动创建有错误的程序和相似代码，以增强预训练数据。
- en: 6.4 Code Summarization
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 代码摘要
- en: Code summarization is considered as a task that generates a comment for a piece
    of the source code, and is thus also named code comment generation. Zhang et al.
    ([2020c](#bib.bib152)) apply MHM to perturb training examples and mix them with
    the original ones for adversarial training, which effectively improves the robustness
    of source code models in summarizing the adversarial code snippets. Zhang et al.
    ([2020b](#bib.bib151)) develop a retrieval-augmentation framework for code summarization,
    relying on similar code-summary pairs to generate the new summary on PCSD and
    JCSD datasets Miceli-Barone and Sennrich ([2017](#bib.bib78)); Hu et al. ([2018](#bib.bib46)).
    Based on this framework, [Liu et al.](#bib.bib66) leverage Hybrid GNN to propose
    a novel retrieval-augmented code summarization method and use it during model
    training on the self-collected CCSD dataset. Zhou et al. ([2022](#bib.bib156))
    generate adversarial examples of a Python dataset Wan et al. ([2018](#bib.bib120))
    and JSCD to evaluate and enhance the source code model robustness.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 代码摘要被认为是一种为源代码生成注释的任务，因此也被称为代码注释生成。张等人 ([2020c](#bib.bib152)) 采用 MHM 来扰动训练示例，并将其与原始示例混合以进行对抗训练，这有效地提高了源代码模型在总结对抗代码片段时的鲁棒性。张等人
    ([2020b](#bib.bib151)) 开发了一种基于检索增强的代码摘要框架，依赖于相似的代码-摘要对在 PCSD 和 JCSD 数据集上生成新的摘要
    Miceli-Barone 和 Sennrich ([2017](#bib.bib78))；胡等人 ([2018](#bib.bib46))。基于该框架，[刘等人](#bib.bib66)
    利用 Hybrid GNN 提出了一个新颖的检索增强代码摘要方法，并在自收集的 CCSD 数据集上进行模型训练。周等人 ([2022](#bib.bib156))
    生成了 Python 数据集的对抗示例 Wan 等人 ([2018](#bib.bib120)) 和 JSCD 以评估和增强源代码模型的鲁棒性。
- en: 6.5 Code Search
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 代码搜索
- en: Code search, or code retrieval, is a text-code task that searches code snippets
    based on the given natural language queries. The source code models on this task
    need to map the semantics of the text to the source code Li et al. ([2022a](#bib.bib55),
    [2023a](#bib.bib56)); Huang et al. ([2023](#bib.bib49)); Ma et al. ([2023](#bib.bib71)).
    Bahrami et al. ([2021](#bib.bib11)) increase the code search queries by augmenting
    the natural language context such as doc-string, code comments and commit messages.
    Shi et al. ([2022b](#bib.bib104)) use AST-focused DA to replace the function and
    variable names of the data in CodeSearchNet Husain et al. ([2019](#bib.bib50))
    and CoSQA Huang et al. ([2021](#bib.bib47)). Shi et al. ([2023](#bib.bib102))
    introduce soft data augmentation (SoDa), without external transformation rules
    on code and text. With SoDa, the model predicts tokens based on dynamic masking
    or replacement when processing CodeSearchNet. Instead of applying rule-based DA
    techniques, Li et al. ([2022a](#bib.bib55)) manipulate the representation of the
    input data by interpolating examples of CodeSearchNet.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 代码搜索，或代码检索，是一种文本代码任务，它基于给定的自然语言查询搜索代码片段。这个任务中的源代码模型需要将文本的语义映射到源代码上 Li et al.
    ([2022a](#bib.bib55), [2023a](#bib.bib56)); Huang et al. ([2023](#bib.bib49));
    Ma et al. ([2023](#bib.bib71))。Bahrami et al. ([2021](#bib.bib11)) 通过增强自然语言上下文，如文档字符串、代码注释和提交消息，来增加代码搜索查询。Shi
    et al. ([2022b](#bib.bib104)) 使用基于 AST 的 DA 替换 CodeSearchNet Husain et al. ([2019](#bib.bib50))
    和 CoSQA Huang et al. ([2021](#bib.bib47)) 中的数据的函数和变量名称。Shi et al. ([2023](#bib.bib102))
    引入了软数据增强（SoDa），无需对代码和文本进行外部转换规则。使用 SoDa，模型在处理 CodeSearchNet 时，根据动态掩码或替换预测标记。与应用基于规则的
    DA 技术不同，Li et al. ([2022a](#bib.bib55)) 通过插值 CodeSearchNet 的示例来操控输入数据的表示。
- en: 6.6 Code Completion
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 代码补全
- en: Code completion requires source code models to generate lines of code to complete
    given programming challenges Brockschmidt et al. ([2019](#bib.bib13)); Lu et al.
    ([2022](#bib.bib69)); Wang et al. ([2022e](#bib.bib130)). [Anand et al.](#bib.bib8)
    suggest that source code models are vulnerable to adversarial examples which are
    perturbed with transformation rules. Lu et al. ([2022](#bib.bib69)) propose a
    retrieval-augmented code completion framework composed of the rule-based DA module
    to generate on PY150 Raychev et al. ([2016](#bib.bib95)) and GitHub Java Corpus
    datasets Allamanis and Sutton ([2013](#bib.bib6)). Wang et al. ([2023b](#bib.bib124))
    customize over 30 transformations specifically for code on docstrings, function
    and variable names, code syntax, and code format and benchmark generative source
    code models on HumanEval Chen et al. ([2021](#bib.bib15)) and MBPP Austin et al.
    ([2021](#bib.bib10)). Yang et al. ([2022a](#bib.bib138)) devise transformations
    on functional descriptions and signatures to attack source code models and show
    that their performances are susceptible.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 代码补全要求源代码模型生成代码行以完成给定的编程挑战 Brockschmidt et al. ([2019](#bib.bib13)); Lu et al.
    ([2022](#bib.bib69)); Wang et al. ([2022e](#bib.bib130))。[Anand et al.](#bib.bib8)
    建议源代码模型易受到对抗样本的攻击，这些样本通过转换规则进行扰动。Lu et al. ([2022](#bib.bib69)) 提出了一个基于检索增强的代码补全框架，该框架由基于规则的
    DA 模块组成，用于在 PY150 Raychev et al. ([2016](#bib.bib95)) 和 GitHub Java 语料库数据集 Allamanis
    and Sutton ([2013](#bib.bib6)) 上生成代码。Wang et al. ([2023b](#bib.bib124)) 专门为文档字符串、函数和变量名称、代码语法和代码格式定制了
    30 多种转换，并在 HumanEval Chen et al. ([2021](#bib.bib15)) 和 MBPP Austin et al. ([2021](#bib.bib10))
    上对生成源代码模型进行了基准测试。Yang et al. ([2022a](#bib.bib138)) 设计了对功能描述和签名的转换以攻击源代码模型，并展示了它们的性能易受影响。
- en: 6.7 Code Translation
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 代码翻译
- en: Similar to neural machine translation in NLP Stahlberg ([2020](#bib.bib111)),
    the task is to translate source code written in a specific programming language
    translation to another one. Ahmad et al. ([2023](#bib.bib1)) apply data augmentation
    through back-translation to enhance unsupervised code translation. They use pre-trained
    sequence-to-sequence models to translate code into natural language summaries
    and then back into code in a different programming language, thereby creating
    additional synthetic training data to improve model performance. Chen and Lampouras
    ([2023](#bib.bib16)) utilize Back-translation and variable augmentation techniques
    to yield the improvement in code translation on CodeTrans Lu et al. ([2021](#bib.bib70)).
    Recently, Xie et al. ([2023](#bib.bib135)) have proposed two novel data augmentation
    methods to generate code translation pairs with similar functionality, via model
    training and back-translation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 NLP 中的神经机器翻译 Stahlberg ([2020](#bib.bib111))，该任务是将用特定编程语言编写的源代码翻译成另一种语言。Ahmad
    等人 ([2023](#bib.bib1)) 通过回译应用数据增强，以提高无监督代码翻译的效果。他们使用预训练的序列到序列模型将代码翻译为自然语言总结，然后再翻译回另一种编程语言，从而创建额外的合成训练数据以提高模型性能。Chen
    和 Lampouras ([2023](#bib.bib16)) 利用回译和变量增强技术，在 CodeTrans Lu 等人 ([2021](#bib.bib70))
    上实现了代码翻译的改进。最近，Xie 等人 ([2023](#bib.bib135)) 提出了两种新颖的数据增强方法，通过模型训练和回译生成具有相似功能的代码翻译对。
- en: 6.8 Code Question Answering (CQA)
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8 代码问题回答 (CQA)
- en: CQA can be formulated as a task where the source code models are required to
    generate a textual answer based on given a code snippet and a question. Huang
    et al. ([2021](#bib.bib47)) incorporate two rule-base DA methods on code and text
    to create examples for contrastive learning. Li et al. ([2022c](#bib.bib60)) explore
    the efficacy of adversarial training on the continuous embedding space with rule-based
    DA on CodeQA Liu and Wan ([2021](#bib.bib64)), a free-form CQA dataset. Park et al.
    ([2023](#bib.bib86)) evaluate KeyDAC, a framework using query writing and variable
    renaming as DA, on WebQueryTest of CodeXGLUE Lu et al. ([2021](#bib.bib70)). Different
    from CodeQA, WebQueryTest is a CQA benchmark only containing Yes/No questions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: CQA 可以被表述为一个任务，其中源代码模型需要根据给定的代码片段和问题生成文本答案。Huang 等人 ([2021](#bib.bib47)) 将两种基于规则的
    DA 方法结合到代码和文本中，以创建用于对比学习的示例。Li 等人 ([2022c](#bib.bib60)) 探讨了在 CodeQA Liu 和 Wan
    ([2021](#bib.bib64)) 上使用基于规则的 DA 的对抗训练在连续嵌入空间中的有效性，该数据集是一个自由形式的 CQA 数据集。Park 等人
    ([2023](#bib.bib86)) 在 CodeXGLUE Lu 等人 ([2021](#bib.bib70)) 的 WebQueryTest 上评估了
    KeyDAC 框架，该框架使用查询写作和变量重命名作为 DA。与 CodeQA 不同，WebQueryTest 是一个仅包含 Yes/No 问题的 CQA
    基准。
- en: 6.9 Code Classification
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9 代码分类
- en: The task performs the categorization of programs regarding their functionality Zhang
    et al. ([2020a](#bib.bib150)); Dong et al. ([2023a](#bib.bib29)) or readability Mi
    et al. ([2021](#bib.bib76), [2022a](#bib.bib74), [2022b](#bib.bib75)). Wang et al.
    ([2022c](#bib.bib128)) propose a novel AST hierarchy representation for contrastive
    learning with the graph neural network. Specifically, they augment the node embeddings
    in AST paths on OJ, a dataset containing 104 classes of programs. Zhang et al.
    ([2022](#bib.bib153)) incorporate simplex interpolation, an example-interpolation
    DA approach on IR, to create intermediate embeddings on POJ-104 from CodeXGLUE Lu
    et al. ([2021](#bib.bib70)). Dong et al. ([2023a](#bib.bib29)) also explore the
    example-interpolation DA to fuse the embeddings of code snippets. They evaluate
    the method on two datasets, JAVA250 and Python800 Puri et al. ([2021](#bib.bib90)).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务对程序进行分类，依据其功能性 Zhang 等人 ([2020a](#bib.bib150))；Dong 等人 ([2023a](#bib.bib29))
    或可读性 Mi 等人 ([2021](#bib.bib76), [2022a](#bib.bib74), [2022b](#bib.bib75))。Wang
    等人 ([2022c](#bib.bib128)) 提出了用于对比学习的 AST 层次表示方法，并采用图神经网络。具体而言，他们在 OJ 数据集（包含 104
    类程序）上增强了 AST 路径中的节点嵌入。Zhang 等人 ([2022](#bib.bib153)) 将简单插值（一个在 IR 上的示例插值 DA 方法）结合进来，以从
    CodeXGLUE Lu 等人 ([2021](#bib.bib70)) 创建 POJ-104 上的中间嵌入。Dong 等人 ([2023a](#bib.bib29))
    也探索了示例插值 DA 来融合代码片段的嵌入。他们在两个数据集 JAVA250 和 Python800 上评估了该方法 Puri 等人 ([2021](#bib.bib90))。
- en: 6.10 Method Name Prediction
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.10 方法名称预测
- en: The goal of method name prediction is to predict the name of a method given
    the program. Yefet et al. ([2020](#bib.bib144)) attack and defense source code
    models by using variable-name-replaced adversarial programs on the Code2Seq dataset Alon
    et al. ([2019](#bib.bib7)). Pour et al. ([2021](#bib.bib89)) propose a search-based
    testing framework specifically for adversarial robustness. They generate adversarial
    examples of Java with ten popular refactoring operators widely used in Java. Rabin
    et al. ([2021](#bib.bib93)) and Yu et al. ([2022b](#bib.bib148)) both implement
    data augmentation frameworks and various transformation rules for processing Java
    source code on the Code2Seq dataset.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 方法名称预测的目标是给定程序预测方法的名称。Yefet et al. ([2020](#bib.bib144)) 通过在 Code2Seq 数据集 Alon
    et al. ([2019](#bib.bib7)) 上使用变量名称替换的对抗程序来攻击和防御源代码模型。Pour et al. ([2021](#bib.bib89))
    提出了一个基于搜索的测试框架，专门用于对抗鲁棒性。他们生成了使用十种流行重构操作符的 Java 对抗样本。Rabin et al. ([2021](#bib.bib93))
    和 Yu et al. ([2022b](#bib.bib148)) 都实现了数据增强框架和各种转换规则，用于在 Code2Seq 数据集上处理 Java
    源代码。
- en: 6.11 Type Prediction
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.11 类型预测
- en: Type prediction, or type interference, aims to predict parameter and function
    types in programs. Bielik and Vechev ([2020](#bib.bib12)) conduct adversarial
    attacks on source code models with examples of transformed ASTs. They instantiate
    the attack to type prediction on JavaScript and TypeScript. Jain et al. ([2021](#bib.bib51))
    apply compiler transforms to generates many variants of programs in DeepTyper Hellendoorn
    et al. ([2018](#bib.bib43)), with equivalent functionality with 11 rules. Li et al.
    ([2022e](#bib.bib62)) incorporate srcML Collard et al. ([2013](#bib.bib20)) meta-grammar
    embeddings to augment the syntactic features of examples in three datasets, DeepTyper,
    Typilus Data Allamanis et al. ([2020](#bib.bib4)) and CodeSearchNet Husain et al.
    ([2019](#bib.bib50)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 类型预测，或称类型推断，旨在预测程序中的参数和函数类型。Bielik 和 Vechev ([2020](#bib.bib12)) 通过变换后的 AST
    示例对源代码模型进行对抗攻击。他们将攻击实例化为对 JavaScript 和 TypeScript 的类型预测。Jain et al. ([2021](#bib.bib51))
    应用编译器转换生成 DeepTyper Hellendoorn et al. ([2018](#bib.bib43)) 中许多程序的变体，这些变体具有 11
    条规则的等效功能。Li et al. ([2022e](#bib.bib62)) 将 srcML Collard et al. ([2013](#bib.bib20))
    元语法嵌入应用于三种数据集中的示例，以增强语法特征，包括 DeepTyper、Typilus Data Allamanis et al. ([2020](#bib.bib4))
    和 CodeSearchNet Husain et al. ([2019](#bib.bib50))。
- en: 7 Challenges and Opportunities
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 挑战与机遇
- en: When it comes to source code, DA faces significant challenges. Nonetheless,
    it’s crucial to acknowledge that these challenges pave the way for new possibilities
    and exciting opportunities in this area of work.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在源代码方面，数据增强面临着显著的挑战。尽管如此，重要的是要认识到这些挑战为该领域的新可能性和激动人心的机会铺平了道路。
- en: Discussion on theory.
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 理论讨论。
- en: Currently, there’s a noticeable gap in the in-depth exploration and theoretical
    understanding of DA methods in source code. Most existing research on DA is centered
    around image processing and natural language fields, viewing data augmentation
    as a way of applying pre-existing knowledge about data or task invariance Dao
    et al. ([2019](#bib.bib22)); Wu et al. ([2020](#bib.bib133)); Shi et al. ([2022a](#bib.bib103)).
    When shifting to source code, much of the previous work introduces new methods
    or demonstrates how DA techniques can be effective for subsequent tasks. However,
    these studies often overlook why and how particularly from a mathematical perspective.
    With source code being discrete by nature, having a theoretical discussion becomes
    even more important. It allows us to understand DA from a broader perspective,
    not just by looking at experimental results. By exploring DA in this way, we can
    better understand its underlying principles without being solely dependent on
    experimental validation.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，在源代码中的数据增强（DA）方法的深入探索和理论理解上存在明显的差距。现有的大多数研究集中在图像处理和自然语言领域，将数据增强视为应用关于数据或任务不变性的预先知识的一种方式 Dao
    et al. ([2019](#bib.bib22)); Wu et al. ([2020](#bib.bib133)); Shi et al. ([2022a](#bib.bib103))。在转向源代码时，以往的工作往往引入了新方法或展示了数据增强技术如何对后续任务有效。然而，这些研究常常忽略了从数学角度特别是为何和如何的问题。由于源代码本质上是离散的，因此进行理论讨论变得尤为重要。它使我们能够从更广泛的角度理解数据增强，而不仅仅是通过实验结果来了解。通过这种方式探索数据增强，我们可以更好地理解其基本原理，而不是仅仅依赖实验验证。
- en: More study on pre-trained models.
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对预训练模型的进一步研究。
- en: In recent years, pre-trained source code models have been widely applied in
    source code, containing rich knowledge through self-supervision on a huge scale
    of corpora Feng et al. ([2020](#bib.bib35)); Guo et al. ([2021](#bib.bib39));
    Zhuo ([2023](#bib.bib157)). Numerous studies have been conducted utilizing pre-trained
    source code models for the purpose of DA, yet, most of these attempts are confined
    to mask token replacement Shi et al. ([2023](#bib.bib102)), direct generation
    after fine-tuning Ahmad et al. ([2023](#bib.bib1)); Pinku et al. ([2023](#bib.bib88)).
    An emergent research opportunity lies in exploring the potential of DA in the
    source code domain with the help of large language models (LLMs) trained on a
    large amount of text and source code Chen et al. ([2021](#bib.bib15)); Li et al.
    ([2023b](#bib.bib57)). LLMs have the capability of context generation based on
    prompted instructions and provided examples, making them a choice to automate
    the DA process in NLP Yoo et al. ([2021](#bib.bib146)); Wang et al. ([2021a](#bib.bib125)).
    Different from the previous usages of pre-trained models in DA, these works open
    the era of “prompt-based DA”. In contrast, the exploration of prompt-based DA
    in source code domains remains a relatively untouched research area. Another direction
    is to harness the internal knowledge encoded in pre-trained source code models.
    For example, Karmakar and Robbes ([2021](#bib.bib54)); Wan et al. ([2022](#bib.bib119))
    show that ASTs and code semantics can be induced from these models without the
    static analysis tools. As most DA methods for source code models tend to predefine
    the code transformation rules via program analysis, it is expected that the programming
    knowledge inside these pre-trained source code models can automate the rule designs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，预训练源代码模型已广泛应用于源代码中，通过对大规模语料库的自我监督，包含丰富的知识 Feng et al. ([2020](#bib.bib35));
    Guo et al. ([2021](#bib.bib39)); Zhuo ([2023](#bib.bib157))。尽管许多研究利用了预训练源代码模型进行数据增强（DA），但这些尝试大多数限于掩码令牌替换 Shi
    et al. ([2023](#bib.bib102))，在微调后的直接生成 Ahmad et al. ([2023](#bib.bib1)); Pinku
    et al. ([2023](#bib.bib88))。一个新兴的研究机会在于利用大量文本和源代码训练的大型语言模型（LLMs）探索DA在源代码领域的潜力 Chen
    et al. ([2021](#bib.bib15)); Li et al. ([2023b](#bib.bib57))。LLMs能够基于提示指令和提供的示例生成上下文，使其成为自动化NLP中DA过程的选择 Yoo
    et al. ([2021](#bib.bib146)); Wang et al. ([2021a](#bib.bib125))。与先前在DA中使用预训练模型不同，这些工作开启了“基于提示的DA”时代。相比之下，基于提示的DA在源代码领域的探索仍然是一个相对未触及的研究领域。另一个方向是利用预训练源代码模型中编码的内部知识。例如，Karmakar
    和 Robbes ([2021](#bib.bib54)); Wan et al. ([2022](#bib.bib119)) 证明了AST和代码语义可以从这些模型中引导出来，而无需静态分析工具。由于大多数源代码模型的DA方法倾向于通过程序分析预定义代码转换规则，因此预训练源代码模型中的编程知识有望自动化规则设计。
- en: Working with domain-specific data.
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 处理特定领域的数据。
- en: Our paper focuses on surveying DA techniques for common downstream tasks involving
    processing source code. However, we are aware that there are a few works on other
    task-specific data in the field of source code. For instance, API recommendation
    and API sequence generation can be considered a part of source code tasks Huang
    et al. ([2018](#bib.bib48)); Gu et al. ([2016](#bib.bib37)). DA methods covered
    by our survey can not be directly generalized to these tasks, as most of them
    only target program-level augmentation but not API-level. We observe a gap of
    DA techniques between these two different layers Treude and Robillard ([2016](#bib.bib118));
    Xu et al. ([2020](#bib.bib136)); Wang et al. ([2021b](#bib.bib131)), which provides
    opportunities for future works to explore. Additionally, the source code modeling
    has not fully justified DA for out-of-distribution generalization. Previous studies Hajipour
    et al. ([2022](#bib.bib40)); Hu et al. ([2022](#bib.bib45)) assume the domain
    as the programs with different complexity, syntax, and semantics. We argue that
    this definition is not natural enough. Similar to the subdomains in NLP, like
    biomedical and financial texts, the application subdomains of source code can
    be diverse. For example, the programs to solve data science problems can significantly
    differ from those for web design. We encourage SE and ML communities to study
    the benefits of DA when applied to various application subdomains of source code.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的论文专注于调查涉及处理源代码的常见下游任务的DA技术。然而，我们意识到在源代码领域还有一些任务特定数据的研究。例如，API推荐和API序列生成可以视为源代码任务的一部分
    Huang et al. ([2018](#bib.bib48)); Gu et al. ([2016](#bib.bib37))。我们的调查覆盖的DA方法不能直接推广到这些任务，因为大多数方法仅针对程序级增强而非API级增强。我们观察到这两个不同层次之间的DA技术存在差距
    Treude and Robillard ([2016](#bib.bib118)); Xu et al. ([2020](#bib.bib136)); Wang
    et al. ([2021b](#bib.bib131))，这为未来的研究提供了探索的机会。此外，源代码建模尚未充分证明DA在分布外泛化中的有效性。之前的研究
    Hajipour et al. ([2022](#bib.bib40)); Hu et al. ([2022](#bib.bib45)) 将领域假设为具有不同复杂性、语法和语义的程序。我们认为这一定义不够自然。类似于NLP中的子领域，如生物医学和金融文本，源代码的应用子领域可以是多样的。例如，用于解决数据科学问题的程序可能与用于网页设计的程序显著不同。我们鼓励SE和ML社区研究DA在源代码各种应用子领域中的应用效益。
- en: More exploration on project-level source code and low-resource programming languages.
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更多探索项目级源代码和低资源编程语言。
- en: The existing methods have made sufficient progress in function-level code snippets
    and common programming languages. The emphasis on code snippets at the function
    level fails to capture the intricacies and complexities of programming in real-world
    scenarios, where developers often work with multiple files and folders simultaneously.
    Therefore, we highlight the importance of exploring DA approaches on the project
    level. The DA on source code projects can be distinct from the function-level
    DA, as it may involve more information such as the interdependencies between different
    code modules, high-level architectural considerations, and the often intricate
    relationship between data structures and algorithms used across the project Mockus
    et al. ([2002](#bib.bib79)). At the same time, limited by data resources Husain
    et al. ([2019](#bib.bib50)); Orlanski et al. ([2023](#bib.bib83)), augmentation
    methods of low-resource languages are scarce, although they have more demand for
    DA. Exploration in these two directions is still limited, and they could be promising
    directions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现有方法在函数级代码片段和常见编程语言上已有充分进展。对函数级别代码片段的强调未能捕捉现实场景中编程的复杂性和精细性，在这些场景中，开发者经常同时处理多个文件和文件夹。因此，我们强调在项目级别探索DA方法的重要性。源代码项目上的DA可能与函数级DA有所不同，因为它可能涉及更多信息，例如不同代码模块之间的相互依赖关系、高层架构考虑因素，以及项目中使用的数据结构和算法之间的复杂关系
    Mockus et al. ([2002](#bib.bib79))。与此同时，由于数据资源的限制 Husain et al. ([2019](#bib.bib50));
    Orlanski et al. ([2023](#bib.bib83))，低资源语言的增强方法稀缺，尽管这些语言对DA的需求更大。这两个方向的探索仍然有限，可能是有前景的研究方向。
- en: Mitigating social bias.
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缓解社会偏见。
- en: As source code models have advanced software development, they may be used to
    develop human-centric applications such as human resources and education, where
    biased programs may result in unjustified and unethical decisions for underrepresented
    people Zhuo et al. ([2023a](#bib.bib158)). While social bias in NLP has been well
    studied and can be mitigated with DA Feng et al. ([2021](#bib.bib34)), the social
    bias in source code has not been brought to attention. For example, Zhuo et al.
    ([2023a](#bib.bib158)) and Liu et al. ([2023c](#bib.bib68)) find that LLMs of
    source code have server bias in various demographics such as gender, sexuality,
    and occupation when performing code generation based on the natural language queries.
    To make these models more responsible in source code, we urge more research on
    mitigating bias. As prior works in NLP suggested, DA may be an effective technique
    to make source code models more responsible.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 随着源代码模型推动了软件开发，它们可能会被用于开发以人为本的应用程序，如人力资源和教育，这可能导致偏见程序对少数群体做出不公正和不道德的决策 Zhuo
    et al. ([2023a](#bib.bib158))。虽然自然语言处理中的社会偏见已经得到充分研究，并且可以通过数据增强方法加以缓解 Feng et al.
    ([2021](#bib.bib34))，但源代码中的社会偏见尚未受到关注。例如，Zhuo et al. ([2023a](#bib.bib158)) 和
    Liu et al. ([2023c](#bib.bib68)) 发现源代码的LLM在根据自然语言查询进行代码生成时，在性别、性取向和职业等各种人口统计特征中存在服务器偏见。为了使这些模型在源代码中更加负责任，我们呼吁更多的研究来缓解偏见。正如以往在自然语言处理中的研究所建议的，数据增强可能是使源代码模型更负责任的有效技术。
- en: Few-shot learning.
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 少样本学习。
- en: In few-shot scenarios, models are required to achieve performance that rivals
    that of traditional machine learning models, yet the amount of training data is
    extremely limited. DA methods provide a direct solution to the problem. However,
    limited works in few-shot scenarios have adopted DA methods Nashid et al. ([2023](#bib.bib80)).
    Mainstream pre-trained source code models obtain rich semantic knowledge through
    language modeling. Such knowledge even covers, to some extent, the semantic information
    introduced by traditional paraphrasing-based DA methods. In other words, the improvement
    space that traditional DA methods bring to pre-trained source code models has
    been greatly compressed. Therefore, it is an interesting question how to provide
    models with fast generalization and problem-solving capability by generating high-quality
    augmented data in few-shot scenarios.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在少样本场景中，模型需要在训练数据极为有限的情况下实现与传统机器学习模型相媲美的性能。数据增强（DA）方法为此问题提供了直接的解决方案。然而，在少样本场景中，采用DA方法的相关工作较少 Nashid
    et al. ([2023](#bib.bib80))。主流的预训练源代码模型通过语言建模获得丰富的语义知识。这些知识甚至在某种程度上涵盖了传统基于释义的数据增强方法所引入的语义信息。换句话说，传统数据增强方法对预训练源代码模型带来的提升空间已经大大压缩。因此，如何通过在少样本场景中生成高质量的增强数据来提供模型快速的泛化和问题解决能力，是一个有趣的问题。
- en: Multimodal applications.
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多模态应用。
- en: It is important to note that the emphasis on function-level code snippets does
    not accurately represent the intricacies and complexities of real-world programming
    situations. In such scenarios, developers often work with multiple files and folders
    simultaneously.s have also been developed. Wang et al. ([2021b](#bib.bib131))
    and Liu et al. ([2023a](#bib.bib65)) explore the chart derendering with an emphasis
    on source code and corresponding APIs. Surís et al. ([2023](#bib.bib112)) propose
    a framework to generate Python programs to solve complex visual tasks including
    images and videos. Although such multimodal applications are more and more popular,
    no study has yet been conducted on applying DA methods to them. A potential challenge
    for the multimodal source code task technique is to effectively bridge between
    the embedding representations for each modality in source code models, which has
    been investigated in vision-language multimodal tasks Ray et al. ([2019](#bib.bib94));
    Tang et al. ([2020](#bib.bib114)); Hao et al. ([2023](#bib.bib41)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，强调功能级代码片段并不能准确代表现实编程环境中的复杂性。在这种情况下，开发人员通常会同时处理多个文件和文件夹。Wang et al. ([2021b](#bib.bib131))
    和 Liu et al. ([2023a](#bib.bib65)) 探讨了源代码和相关API的图表渲染。Surís et al. ([2023](#bib.bib112))
    提出了一个框架，用于生成Python程序以解决包括图像和视频在内的复杂视觉任务。尽管这种多模态应用越来越受欢迎，但尚未对将数据增强方法应用于其中进行研究。多模态源代码任务技术的一个潜在挑战是有效桥接源代码模型中每种模态的嵌入表示，这在视觉-语言多模态任务中已经有所研究 Ray
    et al. ([2019](#bib.bib94)); Tang et al. ([2020](#bib.bib114)); Hao et al. ([2023](#bib.bib41))。
- en: Lack of unification.
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缺乏统一性。
- en: The current body of literature on data augmentation (DA) for source code presents
    a challenging landscape, with the most popular methods often being portrayed in
    a supplementary manner. A handful of empirical studies have sought to compare
    DA methods for source code models de Paula Rodrigues et al. ([2023](#bib.bib23));
    Dong et al. ([2023b](#bib.bib30)). However, none of these works leverages most
    of the existing advanced DA methods for source code models. Whereas there are
    well-accepted frameworks for DA for CV (e.g. default augmentation libraries in
    PyTorch, RandAugment Cubuk et al. ([2020](#bib.bib21))) and DA for NLP (e.g. NL-Augmenter Dhole
    et al. ([2021](#bib.bib26))), a corresponding library of generalized DA techniques
    for source code models is conspicuously absent. Furthermore, as existent DA methods
    are usually evaluated with various datasets, it is hard to determine the efficacy.
    Therefore, we posit that the progression of DA research would be significantly
    facilitated by the establishment of standardized and unified benchmark tasks,
    along with datasets, for the purpose of contrasting and evaluating the effectiveness
    of different augmentation methods. This would pave the way towards a more systematic
    and comparative understanding of the benefits and limitations of these methods.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当前关于源代码的数据增强（DA）的文献呈现出一个具有挑战性的格局，最受欢迎的方法往往以补充的方式呈现。一些实证研究试图比较源代码模型的DA方法，如de
    Paula Rodrigues等人（[2023](#bib.bib23)）；Dong等人（[2023b](#bib.bib30)）。然而，这些研究没有利用大多数现有的先进DA方法。虽然在计算机视觉（CV）中已有公认的DA框架（例如PyTorch中的默认增强库，RandAugment Cubuk等人（[2020](#bib.bib21)））和自然语言处理（NLP）中的DA框架（例如NL-Augmenter Dhole等人（[2021](#bib.bib26)）），但对应于源代码模型的通用DA技术库却显著缺乏。此外，由于现有DA方法通常使用不同的数据集进行评估，确定其有效性变得困难。因此，我们认为，建立标准化和统一的基准任务及数据集，将显著促进DA研究的进展，从而对不同增强方法的效果进行对比和评估。这将为更系统和比较地理解这些方法的优缺点铺平道路。
- en: 8 Conclusion
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: Our paper comprehensively analyzes data augmentation techniques in the context
    of source code. We first explain the concept of data augmentation and its function.
    We then examine the primary data augmentation methods commonly employed in source
    code research and explore augmentation approaches for typical source code applications
    and tasks. Finally, we conclude by outlining the current challenges in the field
    and suggesting potential directions for future source code research. In presenting
    this paper, we aim to assist source code researchers in selecting appropriate
    data augmentation techniques and encourage further exploration and advancement
    in this field.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的论文全面分析了源代码背景下的数据增强技术。我们首先解释数据增强的概念及其功能。然后，我们审视了源代码研究中常用的主要数据增强方法，并探索了典型源代码应用和任务的增强方法。最后，我们总结了该领域当前的挑战，并提出了未来源代码研究的潜在方向。在撰写这篇论文时，我们旨在帮助源代码研究人员选择合适的数据增强技术，并鼓励进一步探索和推进该领域的研究。
- en: Limitations
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: While the work presents in this paper has its merits, we acknowledge the several
    limitations. Firstly, our work only surveys imperative programming languages used
    for general-purpose programming. Therefore, some DA methods for declarative languages Zhuo
    et al. ([2023b](#bib.bib159)) or minor downstream tasks like cryptography misuse
    detection de Paula Rodrigues et al. ([2023](#bib.bib23)), including SQL. Secondly,
    our focus has been primarily on function-level DA within the source code context.
    As such, there remains a need for future development in project-level DA methods.
    Nonetheless, this paper offers a valuable collection of general-purpose DA techniques
    for source code models, and we hope that it can serve as an inspiration for further
    research in this area. Thirdly, given the page limits, the descriptions presented
    in this survey are essentially brief in nature. Our approach has been to offer
    the works in meaningful structured groups rather than unstructured sequences,
    to ensure comprehensive coverage. This work can be used as an index where more
    detailed information can be found in the corresponding works. Lastly, it is worth
    noting that this survey is purely qualitative and does not include any experiments
    or empirical results. To provide more meaningful guidance, it would be helpful
    to conduct comparative experiments across different DA strategies. We leave this
    as a suggestion for future work.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本文展示的工作有其优点，但我们承认其若干局限性。首先，我们的工作仅调查了用于通用编程的命令式编程语言。因此，一些针对声明式语言的 DA 方法 Zhuo
    et al. ([2023b](#bib.bib159)) 或诸如密码学误用检测 de Paula Rodrigues et al. ([2023](#bib.bib23))
    等较小的下游任务，包括 SQL，都未包含在内。其次，我们的重点主要放在源代码上下文中的函数级 DA 上。因此，项目级 DA 方法仍需未来的发展。尽管如此，本文提供了一组有价值的通用
    DA 技术集合，希望能为该领域的进一步研究提供灵感。第三，由于页面限制，本次调查中的描述本质上较为简略。我们的方法是将工作以有意义的结构化组别而非非结构化序列的方式呈现，以确保全面覆盖。这项工作可以作为索引，详细信息可在相关工作中找到。最后，值得注意的是，这项调查纯属定性分析，并未包含任何实验或实证结果。为了提供更有意义的指导，进行不同
    DA 策略的比较实验将是有帮助的。我们将此作为未来工作的建议。
- en: References
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ahmad et al. (2023) Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and
    Kai-Wei Chang. 2023. [Summarize and generate to back-translate: Unsupervised translation
    of programming languages](https://aclanthology.org/2023.eacl-main.112). In *Proceedings
    of the 17th Conference of the European Chapter of the Association for Computational
    Linguistics*, pages 1528–1542, Dubrovnik, Croatia. Association for Computational
    Linguistics.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad et al. (2023) Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and
    Kai-Wei Chang. 2023. [总结并生成以进行回译：无监督的编程语言翻译](https://aclanthology.org/2023.eacl-main.112)。在
    *第17届欧洲计算语言学协会年会论文集* 中，第1528–1542页，杜布罗夫尼克，克罗地亚。计算语言学协会。
- en: 'Allal et al. (2023) Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    Alexander Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian
    Zi, J. Poirier, Hailey Schoelkopf, Sergey Mikhailovich Troshin, Dmitry Abulkhanov,
    Manuel Romero, Michael Franz Lappert, Francesco De Toni, Bernardo Garc’ia del
    R’io, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo
    Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor,
    Luisa Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Christopher Hughes,
    Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. 2023. Santacoder:
    don’t reach for the stars! *ArXiv*, abs/2301.03988.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Allal et al. (2023) Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    Alexander Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian
    Zi, J. Poirier, Hailey Schoelkopf, Sergey Mikhailovich Troshin, Dmitry Abulkhanov,
    Manuel Romero, Michael Franz Lappert, Francesco De Toni, Bernardo García del Río,
    Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas,
    Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luisa
    Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Christopher Hughes, Daniel
    Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. 2023. Santacoder: don''t
    reach for the stars! *ArXiv*, abs/2301.03988.'
- en: Allamanis et al. (2017) Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu,
    and Charles Sutton. 2017. A survey of machine learning for big code and naturalness.
    *ACM Computing Surveys (CSUR)*, 51:1 – 37.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis et al. (2017) Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu,
    and Charles Sutton. 2017. 关于大规模代码和自然性的机器学习调查。*ACM Computing Surveys (CSUR)*, 51:1
    – 37.
- en: 'Allamanis et al. (2020) Miltiadis Allamanis, Earl T Barr, Soline Ducousso,
    and Zheng Gao. 2020. Typilus: Neural type hints. In *Proceedings of the 41st acm
    sigplan conference on programming language design and implementation*, pages 91–105.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis等人（2020）米尔提亚迪斯·阿拉马尼斯、厄尔·T·巴尔、索林·杜库索和郑高。2020年。Typilus：神经类型提示。见于*第41届ACM
    SIGPLAN编程语言设计与实现会议*，第91–105页。
- en: Allamanis et al. (2021) Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt.
    2021. Self-supervised bug detection and repair. *Advances in Neural Information
    Processing Systems*, 34:27865–27876.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis等人（2021）米尔提亚迪斯·阿拉马尼斯、亨利·杰克逊-弗拉克斯和马克·布罗克施密特。2021年。自监督的错误检测与修复。*神经信息处理系统进展*，34:27865–27876。
- en: Allamanis and Sutton (2013) Miltiadis Allamanis and Charles Sutton. 2013. Mining
    source code repositories at massive scale using language modeling. In *2013 10th
    working conference on mining software repositories (MSR)*, pages 207–216\. IEEE.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis和Sutton（2013）米尔提亚迪斯·阿拉马尼斯和查尔斯·萨顿。2013年。使用语言建模在大规模源代码库中挖掘。见于*2013年第10届软件库挖掘工作会议（MSR）*，第207–216页。IEEE。
- en: 'Alon et al. (2019) Uri Alon, Omer Levy, and Eran Yahav. 2019. [code2seq: Generating
    sequences from structured representations of code](https://openreview.net/forum?id=H1gKYo09tX).
    In *International Conference on Learning Representations*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alon等人（2019）乌里·阿隆、奥梅尔·莱维和埃兰·雅哈夫。2019年。[code2seq：从代码的结构化表示生成序列](https://openreview.net/forum?id=H1gKYo09tX)。见于*国际学习表征会议*。
- en: (8) Mrinal Anand, Pratik Kayal, and Mayank Singh. Adversarial robustness of
    program synthesis models. In *Advances in Programming Languages and Neurosymbolic
    Systems Workshop*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) Mrinal Anand、Pratik Kayal和Mayank Singh。程序合成模型的对抗性鲁棒性。见于*编程语言与神经符号系统进展研讨会*。
- en: Asyrofi et al. (2021) Muhammad Hilmi Asyrofi, Zhou Yang, Jieke Shi, Chu Wei
    Quan, and David Lo. 2021. [Can differential testing improve automatic speech recognition
    systems?](https://doi.org/10.1109/ICSME52107.2021.00079) In *2021 IEEE International
    Conference on Software Maintenance and Evolution (ICSME)*, pages 674–678.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asyrofi等人（2021）穆罕默德·希尔米·阿西罗菲、周阳、石洁科、楚伟全和大卫·洛。2021年。[差异测试能否改善自动语音识别系统？](https://doi.org/10.1109/ICSME52107.2021.00079)
    见于*2021年IEEE国际软件维护与演化会议（ICSME）*，第674–678页。
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. Program synthesis with large language models. *arXiv preprint
    arXiv:2108.07732*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin等人（2021）雅各布·奥斯丁、奥古斯图斯·奥德纳、马克斯韦尔·奈、马滕·博斯马、亨利克·米哈勒夫斯基、大卫·多汉、艾伦·姜、凯瑞·蔡、迈克尔·特里、阮克·勒等人。2021年。使用大型语言模型进行程序合成。*arXiv预印本
    arXiv:2108.07732*。
- en: 'Bahrami et al. (2021) Mehdi Bahrami, NC Shrikanth, Yuji Mizobuchi, Lei Liu,
    Masahiro Fukuyori, Wei-Peng Chen, and Kazuki Munakata. 2021. Augmentedcode: Examining
    the effects of natural language resources in code retrieval models. *arXiv preprint
    arXiv:2110.08512*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahrami等人（2021）梅赫迪·巴赫拉米、NC·施里坎特、藤内悠二、刘磊、福代理、陈伟鹏和宗角基。2021年。Augmentedcode：检查自然语言资源在代码检索模型中的效果。*arXiv预印本
    arXiv:2110.08512*。
- en: Bielik and Vechev (2020) Pavol Bielik and Martin Vechev. 2020. Adversarial robustness
    for code. In *International Conference on Machine Learning*, pages 896–907\. PMLR.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bielik和Vechev（2020）帕沃尔·比耶利克和马丁·维赫夫。2020年。代码的对抗性鲁棒性。见于*国际机器学习会议*，第896–907页。PMLR。
- en: Brockschmidt et al. (2019) Marc Brockschmidt, Miltiadis Allamanis, Alexander L.
    Gaunt, and Oleksandr Polozov. 2019. [Generative code modeling with graphs](https://openreview.net/forum?id=Bke4KsA5FX).
    In *International Conference on Learning Representations*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brockschmidt等人（2019）马克·布罗克施密特、米尔提亚迪斯·阿拉马尼斯、亚历山大·L·冈特和奥列克桑德·波洛佐夫。2019年。[生成式代码建模与图](https://openreview.net/forum?id=Bke4KsA5FX)。见于*国际学习表征会议*。
- en: 'Chakraborty et al. (2022) Saikat Chakraborty, Rahul Krishna, Yangruibo Ding,
    and Baishakhi Ray. 2022. [Deep learning based vulnerability detection: Are we
    there yet?](https://doi.org/10.1109/TSE.2021.3087402) *IEEE Transactions on Software
    Engineering*, 48(9):3280–3296.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakraborty等人（2022）赛卡特·查克拉博提、拉胡尔·克里希纳、杨瑞博丁和贝莎基·雷。2022年。[基于深度学习的漏洞检测：我们到达了吗？](https://doi.org/10.1109/TSE.2021.3087402)
    *IEEE软件工程学报*，48(9):3280–3296。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman,
    Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry,
    Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
    Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski
    Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes,
    Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji,
    Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,
    Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
    Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech
    Zaremba. 2021. Evaluating large language models trained on code. *ArXiv*, abs/2107.03374.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde,
    Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex
    Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry,
    Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
    Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski
    Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes,
    Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji,
    Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,
    Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
    Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever 和 Wojciech
    Zaremba. 2021. 评估基于代码训练的大型语言模型。*ArXiv*，abs/2107.03374。
- en: 'Chen and Lampouras (2023) Pinzhen Chen and Gerasimos Lampouras. 2023. Exploring
    data augmentation for code generation tasks. In *Findings of the Association for
    Computational Linguistics: EACL 2023*, pages 1497–1505.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Lampouras (2023) Pinzhen Chen 和 Gerasimos Lampouras. 2023. 探索代码生成任务的数据增强。在
    *计算语言学协会会议成果：EACL 2023*，第 1497–1505 页。
- en: 'Chen et al. (2022) Qibin Chen, Jeremy Lacomis, Edward J Schwartz, Graham Neubig,
    Bogdan Vasilescu, and Claire Le Goues. 2022. Varclr: Variable semantic representation
    pre-training via contrastive learning. In *Proceedings of the 44th International
    Conference on Software Engineering*, pages 2327–2339.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2022) Qibin Chen, Jeremy Lacomis, Edward J Schwartz, Graham Neubig,
    Bogdan Vasilescu 和 Claire Le Goues. 2022. Varclr：通过对比学习进行的变量语义表示预训练。在 *第 44 届国际软件工程大会论文集*，第
    2327–2339 页。
- en: Cheng et al. (2022) Xiao Cheng, Guanqin Zhang, Haoyu Wang, and Yulei Sui. 2022.
    Path-sensitive code embedding via contrastive learning for software vulnerability
    detection. In *Proceedings of the 31st ACM SIGSOFT International Symposium on
    Software Testing and Analysis*, pages 519–531.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等 (2022) Xiao Cheng, Guanqin Zhang, Haoyu Wang 和 Yulei Sui. 2022. 通过对比学习的路径敏感代码嵌入用于软件漏洞检测。在
    *第 31 届 ACM SIGSOFT 软件测试与分析国际研讨会论文集*，第 519–531 页。
- en: 'Choi et al. (2023) YunSeok Choi, CheolWon Na, Hyojun Kim, and Jee-Hyong Lee.
    2023. Readsum: Retrieval-augmented adaptive transformer for source code summarization.
    *IEEE Access*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等 (2023) YunSeok Choi, CheolWon Na, Hyojun Kim 和 Jee-Hyong Lee. 2023. Readsum：用于源代码总结的检索增强适应性变换器。*IEEE
    Access*。
- en: 'Collard et al. (2013) Michael L Collard, Michael John Decker, and Jonathan I
    Maletic. 2013. srcml: An infrastructure for the exploration, analysis, and manipulation
    of source code: A tool demonstration. In *2013 IEEE International conference on
    software maintenance*, pages 516–519\. IEEE.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collard 等 (2013) Michael L Collard, Michael John Decker 和 Jonathan I Maletic.
    2013. srcml：一个用于探索、分析和操作源代码的基础设施：工具演示。在 *2013 IEEE 国际软件维护会议*，第 516–519 页。IEEE。
- en: 'Cubuk et al. (2020) Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
    Le. 2020. Randaugment: practical automated data augmentation with a reduced search
    space. In *Proceedings of the 34th International Conference on Neural Information
    Processing Systems*, pages 18613–18624.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cubuk 等 (2020) Ekin D Cubuk, Barret Zoph, Jonathon Shlens 和 Quoc V Le. 2020.
    Randaugment：具有简化搜索空间的实用自动化数据增强。在 *第 34 届国际神经信息处理系统大会论文集*，第 18613–18624 页。
- en: Dao et al. (2019) Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris
    De Sa, and Christopher Ré. 2019. A kernel theory of modern data augmentation.
    In *International Conference on Machine Learning*, pages 1528–1537\. PMLR.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等 (2019) Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De
    Sa 和 Christopher Ré. 2019. 现代数据增强的核理论。在 *国际机器学习大会*，第 1528–1537 页。PMLR。
- en: 'de Paula Rodrigues et al. (2023) Gustavo Eloi de Paula Rodrigues, Alexandre M
    Braga, and Ricardo Dahab. 2023. Detecting cryptography misuses with machine learning:
    Graph embeddings, transfer learning and data augmentation in source code related
    tasks. *IEEE Transactions on Reliability*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Paula Rodrigues 等 (2023) Gustavo Eloi de Paula Rodrigues, Alexandre M Braga
    和 Ricardo Dahab. 2023. 使用机器学习检测密码学误用：图嵌入、迁移学习和数据增强在源代码相关任务中的应用。*IEEE 可靠性学报*。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *ArXiv*, abs/1810.04805.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等人（2019）Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova。2019年。Bert：用于语言理解的深度双向转换器预训练。
    *ArXiv*，abs/1810.04805。
- en: 'Devore-McDonald and Berger (2020) Breanna Devore-McDonald and Emery D Berger.
    2020. Mossad: Defeating software plagiarism detection. *Proceedings of the ACM
    on Programming Languages*, 4(OOPSLA):1–28.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devore-McDonald 和 Berger（2020）Breanna Devore-McDonald 和 Emery D Berger。2020年。Mossad：击败软件抄袭检测。
    *ACM 编程语言会议论文集*，4(OOPSLA)：1–28。
- en: 'Dhole et al. (2021) Kaustubh D. Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh
    Gupta, Zhenhao Li, Saad Mahamood, Abinaya Mahendiran, Simon Mille, Ashish Srivastava,
    Samson Tan, Tongshuang Wu, Jascha Sohl-Dickstein, Jinho D. Choi, Eduard Hovy,
    Ondrej Dusek, Sebastian Ruder, Sajant Anand, Nagender Aneja, Rabin Banjade, Lisa
    Barthe, Hanna Behnke, Ian Berlot-Attwell, Connor Boyle, Caroline Brun, Marco Antonio Sobrevilla
    Cabezudo, Samuel Cahyawijaya, Emile Chapuis, Wanxiang Che, Mukund Choudhary, Christian
    Clauss, Pierre Colombo, Filip Cornell, Gautier Dagan, Mayukh Das, Tanay Dixit,
    Thomas Dopierre, Paul-Alexis Dray, Suchitra Dubey, Tatiana Ekeinhor, Marco Di
    Giovanni, Rishabh Gupta, Rishabh Gupta, Louanes Hamla, Sang Han, Fabrice Harel-Canada,
    Antoine Honore, Ishan Jindal, Przemyslaw K. Joniak, Denis Kleyko, Venelin Kovatchev,
    Kalpesh Krishna, Ashutosh Kumar, Stefan Langer, Seungjae Ryan Lee, Corey James
    Levinson, Hualou Liang, Kaizhao Liang, Zhexiong Liu, Andrey Lukyanenko, Vukosi
    Marivate, Gerard de Melo, Simon Meoni, Maxime Meyer, Afnan Mir, Nafise Sadat Moosavi,
    Niklas Muennighoff, Timothy Sum Hon Mun, Kenton Murray, Marcin Namysl, Maria Obedkova,
    Priti Oli, Nivranshu Pasricha, Jan Pfister, Richard Plant, Vinay Prabhu, Vasile
    Pais, Libo Qin, Shahab Raji, Pawan Kumar Rajpoot, Vikas Raunak, Roy Rinberg, Nicolas
    Roberts, Juan Diego Rodriguez, Claude Roux, Vasconcellos P. H. S., Ananya B. Sai,
    Robin M. Schmidt, Thomas Scialom, Tshephisho Sefara, Saqib N. Shamsi, Xudong Shen,
    Haoyue Shi, Yiwen Shi, Anna Shvets, Nick Siegel, Damien Sileo, Jamie Simon, Chandan
    Singh, Roman Sitelew, Priyank Soni, Taylor Sorensen, William Soto, Aman Srivastava,
    KV Aditya Srivatsa, Tony Sun, Mukund Varma T, A Tabassum, Fiona Anting Tan, Ryan
    Teehan, Mo Tiwari, Marie Tolkiehn, Athena Wang, Zijian Wang, Gloria Wang, Zijie J.
    Wang, Fuxuan Wei, Bryan Wilie, Genta Indra Winata, Xinyi Wu, Witold Wydmański,
    Tianbao Xie, Usama Yaseen, M. Yee, Jing Zhang, and Yue Zhang. 2021. [Nl-augmenter:
    A framework for task-sensitive natural language augmentation](http://arxiv.org/abs/2112.02721).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dhole 等人（2021）Kaustubh D. Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta,
    Zhenhao Li, Saad Mahamood, Abinaya Mahendiran, Simon Mille, Ashish Srivastava,
    Samson Tan, Tongshuang Wu, Jascha Sohl-Dickstein, Jinho D. Choi, Eduard Hovy,
    Ondrej Dusek, Sebastian Ruder, Sajant Anand, Nagender Aneja, Rabin Banjade, Lisa
    Barthe, Hanna Behnke, Ian Berlot-Attwell, Connor Boyle, Caroline Brun, Marco Antonio
    Sobrevilla Cabezudo, Samuel Cahyawijaya, Emile Chapuis, Wanxiang Che, Mukund Choudhary,
    Christian Clauss, Pierre Colombo, Filip Cornell, Gautier Dagan, Mayukh Das, Tanay
    Dixit, Thomas Dopierre, Paul-Alexis Dray, Suchitra Dubey, Tatiana Ekeinhor, Marco
    Di Giovanni, Rishabh Gupta, Rishabh Gupta, Louanes Hamla, Sang Han, Fabrice Harel-Canada,
    Antoine Honore, Ishan Jindal, Przemyslaw K. Joniak, Denis Kleyko, Venelin Kovatchev,
    Kalpesh Krishna, Ashutosh Kumar, Stefan Langer, Seungjae Ryan Lee, Corey James
    Levinson, Hualou Liang, Kaizhao Liang, Zhexiong Liu, Andrey Lukyanenko, Vukosi
    Marivate, Gerard de Melo, Simon Meoni, Maxime Meyer, Afnan Mir, Nafise Sadat Moosavi,
    Niklas Muennighoff, Timothy Sum Hon Mun, Kenton Murray, Marcin Namysl, Maria Obedkova,
    Priti Oli, Nivranshu Pasricha, Jan Pfister, Richard Plant, Vinay Prabhu, Vasile
    Pais, Libo Qin, Shahab Raji, Pawan Kumar Rajpoot, Vikas Raunak, Roy Rinberg, Nicolas
    Roberts, Juan Diego Rodriguez, Claude Roux, Vasconcellos P. H. S., Ananya B. Sai,
    Robin M. Schmidt, Thomas Scialom, Tshephisho Sefara, Saqib N. Shamsi, Xudong Shen,
    Haoyue Shi, Yiwen Shi, Anna Shvets, Nick Siegel, Damien Sileo, Jamie Simon, Chandan
    Singh, Roman Sitelew, Priyank Soni, Taylor Sorensen, William Soto, Aman Srivastava,
    KV Aditya Srivatsa, Tony Sun, Mukund Varma T, A Tabassum, Fiona Anting Tan, Ryan
    Teehan, Mo Tiwari, Marie Tolkiehn, Athena Wang, Zijian Wang, Gloria Wang, Zijie
    J. Wang, Fuxuan Wei, Bryan Wilie, Genta Indra Winata, Xinyi Wu, Witold Wydmański,
    Tianbao Xie, Usama Yaseen, M. Yee, Jing Zhang 和 Yue Zhang。2021年。[Nl-augmenter:
    一个任务敏感的自然语言增强框架](http://arxiv.org/abs/2112.02721)。'
- en: Ding et al. (2021) Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari,
    Baishakhi Ray, and Saikat Chakraborty. 2021. Towards learning (dis)-similarity
    of source code from program contrasts. In *Annual Meeting of the Association for
    Computational Linguistics*.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等人（2021）Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari,
    Baishakhi Ray 和 Saikat Chakraborty。2021年。基于程序对比的源代码（非）相似性学习。发表于 *计算语言学协会年会*。
- en: 'Ding et al. (2023) Yangruibo Ding, Saikat Chakraborty, Luca Buratti, Saurabh
    Pujar, Alessandro Morari, Gail Kaiser, and Baishakhi Ray. 2023. Concord: Clone-aware
    contrastive learning for source code.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等人（2023）Yangruibo Ding, Saikat Chakraborty, Luca Buratti, Saurabh Pujar,
    Alessandro Morari, Gail Kaiser 和 Baishakhi Ray。2023年。Concord：用于源代码的克隆感知对比学习。
- en: 'Dong et al. (2023a) Zeming Dong, Qiang Hu, Yuejun Guo, Maxime Cordy, Mike Papadakis,
    Zhenya Zhang, Yves Le Traon, and Jianjun Zhao. 2023a. Mixcode: Enhancing code
    classification by mixup-based data augmentation. In *2023 IEEE International Conference
    on Software Analysis, Evolution and Reengineering (SANER)*, pages 379–390\. IEEE.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2023a) Zeming Dong、Qiang Hu、Yuejun Guo、Maxime Cordy、Mike Papadakis、Zhenya
    Zhang、Yves Le Traon 和 Jianjun Zhao。2023a年。《Mixcode：通过基于Mixup的数据增强提升代码分类》。见于 *2023年IEEE国际软件分析、演化与重构会议（SANER）*，第379–390页。IEEE。
- en: 'Dong et al. (2023b) Zeming Dong, Qiang Hu, Yuejun Guo, Zhenya Zhang, Maxime
    Cordy, Mike Papadakis, Yves Le Traon, and Jianjun Zhao. 2023b. Boosting source
    code learning with data augmentation: An empirical study. *arXiv preprint arXiv:2303.06808*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2023b) Zeming Dong、Qiang Hu、Yuejun Guo、Zhenya Zhang、Maxime Cordy、Mike
    Papadakis、Yves Le Traon 和 Jianjun Zhao。2023b年。《通过数据增强提升源代码学习：一项实证研究》。*arXiv预印本
    arXiv:2303.06808*。
- en: 'Drain et al. (2021) Dawn Drain, Colin B Clement, Guillermo Serrato, and Neel
    Sundaresan. 2021. Deepdebug: Fixing python bugs using stack traces, backtranslation,
    and code skeletons. *arXiv preprint arXiv:2105.09352*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Drain et al. (2021) Dawn Drain、Colin B Clement、Guillermo Serrato 和 Neel Sundaresan。2021年。《Deepdebug：使用堆栈跟踪、回译和代码框架修复Python错误》。*arXiv预印本
    arXiv:2105.09352*。
- en: Fadaee et al. (2017) Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017.
    Data augmentation for low-resource neural machine translation. In *Annual Meeting
    of the Association for Computational Linguistics*, pages 567–573\. Association
    for Computational Linguistics (ACL).
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fadaee et al. (2017) Marzieh Fadaee、Arianna Bisazza 和 Christof Monz。2017年。《低资源神经机器翻译的数据增强》。见于
    *Annual Meeting of the Association for Computational Linguistics*，第567–573页。计算语言学协会（ACL）。
- en: Fan et al. (2020) Jiahao Fan, Yi Li, Shaohua Wang, and Tien N Nguyen. 2020.
    Ac/c++ code vulnerability dataset with code changes and cve summaries. In *Proceedings
    of the 17th International Conference on Mining Software Repositories*, pages 508–512.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan et al. (2020) Jiahao Fan、Yi Li、Shaohua Wang 和 Tien N Nguyen。2020年。《带有代码变更和CVE总结的AC/C++代码漏洞数据集》。见于
    *第17届国际软件仓库挖掘会议论文集*，第508–512页。
- en: 'Feng et al. (2021) Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chandar,
    Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation
    approaches for nlp. In *Findings of the Association for Computational Linguistics:
    ACL-IJCNLP 2021*, pages 968–988.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng et al. (2021) Steven Y Feng、Varun Gangal、Jason Wei、Sarath Chandar、Soroush
    Vosoughi、Teruko Mitamura 和 Eduard Hovy。2021年。《自然语言处理的数据增强方法综述》。见于 *Association
    for Computational Linguistics: ACL-IJCNLP 2021*，第968–988页。'
- en: 'Feng et al. (2020) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
    Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert:
    A pre-trained model for programming and natural languages. In *Findings of the
    Association for Computational Linguistics: EMNLP 2020*, pages 1536–1547.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng et al. (2020) Zhangyin Feng、Daya Guo、Duyu Tang、Nan Duan、Xiaocheng Feng、Ming
    Gong、Linjun Shou、Bing Qin、Ting Liu、Daxin Jiang 等。2020年。《Codebert：一种用于编程和自然语言的预训练模型》。见于
    *Association for Computational Linguistics: EMNLP 2020*，第1536–1547页。'
- en: Gao et al. (2023) Fengjuan Gao, Yu Wang, and Ke Wang. 2023. Discrete adversarial
    attack to models of code. *Proceedings of the ACM on Programming Languages*, 7(PLDI):172–195.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2023) Fengjuan Gao、Yu Wang 和 Ke Wang。2023年。《针对代码模型的离散对抗攻击》。*ACM编程语言会议论文集*，7(PLDI)：172–195。
- en: Gu et al. (2016) Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim.
    2016. Deep api learning. In *Proceedings of the 2016 24th ACM SIGSOFT international
    symposium on foundations of software engineering*, pages 631–642.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2016) Xiaodong Gu、Hongyu Zhang、Dongmei Zhang 和 Sunghun Kim。2016年。《深度API学习》。见于
    *2016年第24届ACM SIGSOFT国际软件工程基础研讨会论文集*，第631–642页。
- en: 'Gu et al. (2023) Yafeng Gu, Yiheng Shen, Xiang Chen, Shaoyu Yang, Yiling Huang,
    and Zhixiang Cao. 2023. Apicom: Automatic api completion via prompt learning and
    adversarial training-based data augmentation. In *Proceedings of the 14th Asia-Pacific
    Symposium on Internetware*, pages 259–269.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2023) Yafeng Gu、Yiheng Shen、Xiang Chen、Shaoyu Yang、Yiling Huang 和
    Zhixiang Cao。2023年。《Apicom：通过提示学习和对抗训练基于数据增强的自动API补全》。见于 *第14届亚太互联网软件研讨会论文集*，第259–269页。
- en: 'Guo et al. (2021) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie
    LIU, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
    Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming
    Zhou. 2021. [Graphcode{bert}: Pre-training code representations with data flow](https://openreview.net/forum?id=jLoC4ez43PZ).
    In *International Conference on Learning Representations*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo等（2021）Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU,
    Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
    Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, 和 Ming
    Zhou. 2021. [Graphcode{bert}: 用数据流预训练代码表示](https://openreview.net/forum?id=jLoC4ez43PZ)。在*国际学习表征会议*上。'
- en: 'Hajipour et al. (2022) Hossein Hajipour, Ning Yu, Cristian-Alexandru Staicu,
    and Mario Fritz. 2022. Simscood: Systematic analysis of out-of-distribution behavior
    of source code models. *arXiv preprint arXiv:2210.04802*.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hajipour等（2022）Hossein Hajipour, Ning Yu, Cristian-Alexandru Staicu, 和 Mario
    Fritz. 2022. Simscood: 系统性分析源代码模型的分布外行为。*arXiv预印本 arXiv:2210.04802*。'
- en: 'Hao et al. (2023) Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang, Wanqian
    Zhang, Bo Li, and Mu Li. 2023. Mixgen: A new multi-modal data augmentation. In
    *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*,
    pages 379–389.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hao等（2023）Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang, Wanqian Zhang,
    Bo Li, 和 Mu Li. 2023. Mixgen: 一种新的多模态数据增强方法。在*IEEE/CVF计算机视觉应用冬季会议*中，页码379–389。'
- en: Hasija et al. (2023) Krishnam Hasija, Shrishti Pradhan, Manasi Patwardhan, Raveendra Kumar
    Medicherla, Lovekesh Vig, and Ravindra Naik. 2023. Neuro-symbolic zero-shot code
    cloning with cross-language intermediate representation. *arXiv preprint arXiv:2304.13350*.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasija等（2023）Krishnam Hasija, Shrishti Pradhan, Manasi Patwardhan, Raveendra
    Kumar Medicherla, Lovekesh Vig, 和 Ravindra Naik. 2023. 神经符号零样本代码克隆与跨语言中间表示。*arXiv预印本
    arXiv:2304.13350*。
- en: Hellendoorn et al. (2018) Vincent J Hellendoorn, Christian Bird, Earl T Barr,
    and Miltiadis Allamanis. 2018. Deep learning type inference. In *Proceedings of
    the 2018 26th acm joint meeting on european software engineering conference and
    symposium on the foundations of software engineering*, pages 152–162.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hellendoorn等（2018）Vincent J Hellendoorn, Christian Bird, Earl T Barr, 和 Miltiadis
    Allamanis. 2018. 深度学习类型推断。在*2018年第26届ACM联合会议——欧洲软件工程会议与软件工程基础研讨会*中，页码152–162。
- en: Henke et al. (2022) Jordan Henke, Goutham Ramakrishnan, Zi Wang, Aws Albarghouth,
    Somesh Jha, and Thomas Reps. 2022. Semantic robustness of models of source code.
    In *2022 IEEE International Conference on Software Analysis, Evolution and Reengineering
    (SANER)*, pages 526–537\. IEEE.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henke等（2022）Jordan Henke, Goutham Ramakrishnan, Zi Wang, Aws Albarghouth, Somesh
    Jha, 和 Thomas Reps. 2022. 源代码模型的语义鲁棒性。在*2022年IEEE国际软件分析、演变与重构会议（SANER）*中，页码526–537。IEEE。
- en: 'Hu et al. (2022) Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Lei Ma, Mike
    Papadakis, and Yves Le Traon. 2022. Codes: A distribution shift benchmark dataset
    for source code learning. *arXiv preprint arXiv:2206.05480*.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu等（2022）Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Lei Ma, Mike Papadakis,
    和 Yves Le Traon. 2022. Codes: 一种用于源代码学习的分布转移基准数据集。*arXiv预印本 arXiv:2206.05480*。'
- en: Hu et al. (2018) Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018.
    Summarizing source code with transferred api knowledge. In *Proceedings of the
    27th International Joint Conference on Artificial Intelligence*, pages 2269–2275.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等（2018）Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, 和 Zhi Jin. 2018. 用转移的API知识总结源代码。在*第27届国际人工智能联合会议*中，页码2269–2275。
- en: 'Huang et al. (2021) Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu,
    Daxin Jiang, Ming Zhou, and Nan Duan. 2021. Cosqa: 20,000+ web queries for code
    search and question answering. In *Proceedings of the 59th Annual Meeting of the
    Association for Computational Linguistics and the 11th International Joint Conference
    on Natural Language Processing (Volume 1: Long Papers)*, pages 5690–5700.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang等（2021）Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang,
    Ming Zhou, 和 Nan Duan. 2021. Cosqa: 20,000+ 网络查询用于代码搜索和问答。在*第59届计算语言学协会年会及第11届国际自然语言处理联合会议（第1卷：长篇论文）*中，页码5690–5700。'
- en: Huang et al. (2018) Qiao Huang, Xin Xia, Zhenchang Xing, David Lo, and Xinyu
    Wang. 2018. Api method recommendation without worrying about the task-api knowledge
    gap. In *Proceedings of the 33rd ACM/IEEE International Conference on Automated
    Software Engineering*, pages 293–304.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等（2018）Qiao Huang, Xin Xia, Zhenchang Xing, David Lo, 和 Xinyu Wang. 2018.
    无需担心任务-API知识差距的API方法推荐。在*第33届ACM/IEEE国际自动化软件工程会议*中，页码293–304。
- en: Huang et al. (2023) Xiangbing Huang, Yingwei Ma, Haifang Zhou, Zhijie Jiang,
    Yuanliang Zhang, Teng Wang, and Shanshan Li. 2023. Towards better multilingual
    code search through cross-lingual contrastive learning. In *Proceedings of the
    14th Asia-Pacific Symposium on Internetware*, pages 22–32.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2023）**向兵·黄**、**英伟·马**、**海方·周**、**志杰·姜**、**元亮·张**、**腾·王** 和 **珊珊·李**。2023年。通过跨语言对比学习实现更好的多语言代码搜索。在
    *第14届亚太互联网软件研讨会论文集*，第22–32页。
- en: 'Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of
    semantic code search. *arXiv preprint arXiv:1909.09436*.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Husain 等人（2019）**哈梅尔·侯赛因**、**胡-香·吴**、**提费雷特·加齐特**、**米尔提亚迪斯·阿拉马尼斯** 和 **马克·布罗克施密特**。2019年。Codesearchnet
    挑战：评估语义代码搜索的现状。*arXiv 预印本 arXiv:1909.09436*。
- en: Jain et al. (2021) Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph
    Gonzalez, and Ion Stoica. 2021. Contrastive code representation learning. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    5954–5971.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等人（2021）**帕拉斯·贾因**、**阿贾伊·贾因**、**田俊·张**、**皮特·阿贝尔**、**约瑟夫·冈萨雷斯** 和 **伊昂·斯托伊卡**。2021年。对比代码表示学习。在
    *2021年自然语言处理经验方法会议论文集*，第5954–5971页。
- en: 'Jia et al. (2023) Jinghan Jia, Shashank Srikant, Tamara Mitrovska, Chuang Gan,
    Shiyu Chang, Sijia Liu, and Una-May O’Reilly. 2023. Clawsat: Towards both robust
    and accurate code models. In *2023 IEEE International Conference on Software Analysis,
    Evolution and Reengineering (SANER)*, pages 212–223\. IEEE.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jia 等人（2023）**景汉·贾**、**沙尚克·斯里坎特**、**塔玛拉·米特罗夫斯卡**、**蒋干**、**施宇·常**、**司佳·刘** 和
    **乌娜-梅·奥赖利**。2023年。Clawsat: 迈向既稳健又准确的代码模型。在 *2023 IEEE 国际软件分析、演变与重构会议（SANER）*，第212–223页。IEEE。'
- en: 'Jin et al. (2023) Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai
    Lu, Neel Sundaresan, and Alexey Svyatkovskiy. 2023. Inferfix: End-to-end program
    repair with llms. *arXiv preprint arXiv:2303.07263*.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin 等人（2023）**马修·金**、**赛义德·沙赫里亚尔**、**米歇尔·图法诺**、**辛·施**、**帅·陆**、**尼尔·孙达雷桑**
    和 **阿列克谢·斯维亚特科夫斯基**。2023年。Inferfix: 端到端程序修复与 llms。*arXiv 预印本 arXiv:2303.07263*。'
- en: Karmakar and Robbes (2021) Anjan Karmakar and Romain Robbes. 2021. What do pre-trained
    code models know about code? In *2021 36th IEEE/ACM International Conference on
    Automated Software Engineering (ASE)*, pages 1332–1336\. IEEE.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karmakar 和 Robbes（2021）**安然·卡尔马克** 和 **罗曼·罗贝斯**。2021年。预训练代码模型对代码了解多少？在 *2021年第36届IEEE/ACM自动化软件工程国际会议（ASE）*，第1332–1336页。IEEE。
- en: Li et al. (2022a) Haochen Li, Chunyan Miao, Cyril Leung, Yanxian Huang, Yuan
    Huang, Hongyu Zhang, and Yanlin Wang. 2022a. [Exploring representation-level augmentation
    for code search](https://aclanthology.org/2022.emnlp-main.327). In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages
    4924–4936, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022a）**郝晨·李**、**春艳·苗**、**西里尔·梁**、**艳仙·黄**、**袁·黄**、**洪宇·张** 和 **闫林·王**。2022a年。[探索用于代码搜索的表示级增强](https://aclanthology.org/2022.emnlp-main.327)。在
    *2022年自然语言处理经验方法会议论文集*，第4924–4936页，阿布扎比，阿拉伯联合酋长国。计算语言学协会。
- en: Li et al. (2023a) Haochen Li, Xin Zhou, Luu Anh Tuan, and Chunyan Miao. 2023a.
    Rethinking negative pairs in code search.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023a）**郝晨·李**、**辛·周**、**陆·安·段** 和 **春艳·苗**。2023a年。重新思考代码搜索中的负对。
- en: 'Li et al. (2023b) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene,
    Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier,
    Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin
    Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp
    Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nourhan Fahmy,
    Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim
    Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire
    Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, Alexander
    Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor,
    Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
    Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.
    2023b. Starcoder: may the source be with you! *ArXiv*, abs/2305.06161.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2023b) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene,
    Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier,
    Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin
    Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva
    Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nourhan
    Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas,
    Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding,
    Claire Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, Alexander
    Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor,
    Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
    Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra 和 Harm de Vries. 2023b.
    Starcoder: 愿源代码与你同在！*ArXiv*，abs/2305.06161。'
- en: 'Li et al. (2022b) Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang,
    Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, and Nan Duan. 2022b. Coderetriever:
    A large scale contrastive pre-training method for code search. In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages
    2898–2910.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2022b) Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang,
    Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen 和 Nan Duan. 2022b. Coderetriever:
    一种大规模对比预训练方法用于代码搜索。发表于*2022年自然语言处理实证方法会议论文集*，第2898–2910页。'
- en: Li et al. (2023c) Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei
    Zhang, and Yang Liu. 2023c. Multi-target backdoor attacks for code pre-trained
    models.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023c) Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei
    Zhang 和 Yang Liu. 2023c. 针对代码预训练模型的多目标后门攻击。
- en: Li et al. (2022c) Yiyang Li, Hongqiu Wu, and Hai Zhao. 2022c. Semantic-preserving
    adversarial code comprehension. In *Proceedings of the 29th International Conference
    on Computational Linguistics*, pages 3017–3028.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2022c) Yiyang Li, Hongqiu Wu 和 Hai Zhao. 2022c. 语义保留的对抗性代码理解。发表于*第29届国际计算语言学大会论文集*，第3017–3028页。
- en: 'Li et al. (2022d) Zhen Li, Guenevere Chen, Chen Chen, Yayi Zou, and Shouhuai
    Xu. 2022d. Ropgen: Towards robust code authorship attribution via automatic coding
    style transformation. In *Proceedings of the 44th International Conference on
    Software Engineering*, pages 1906–1918.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2022d) Zhen Li, Guenevere Chen, Chen Chen, Yayi Zou 和 Shouhuai Xu. 2022d.
    Ropgen: 通过自动编码风格转换实现鲁棒的代码作者归属。发表于*第44届国际软件工程会议论文集*，第1906–1918页。'
- en: Li et al. (2022e) Zhiming Li, Xiaofei Xie, Haoliang Li, Zhengzi Xu, Yi Li, and
    Yang Liu. 2022e. Cross-lingual transfer learning for statistical type inference.
    In *Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing
    and Analysis*, pages 239–250.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2022e) Zhiming Li, Xiaofei Xie, Haoliang Li, Zhengzi Xu, Yi Li 和 Yang
    Liu. 2022e. 统计类型推断的跨语言迁移学习。发表于*第31届ACM SIGSOFT国际软件测试与分析研讨会论文集*，第239–250页。
- en: Li et al. (2022f) Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang,
    Sen Nie, and Shi Wu. 2022f. Unleashing the power of compiler intermediate representation
    to enhance neural program embeddings. In *Proceedings of the 44th International
    Conference on Software Engineering*, pages 2253–2265.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2022f) Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang,
    Sen Nie 和 Shi Wu. 2022f. 释放编译器中间表示的力量以增强神经程序嵌入。发表于*第44届国际软件工程会议论文集*，第2253–2265页。
- en: 'Liu and Wan (2021) Chenxiao Liu and Xiaojun Wan. 2021. Codeqa: A question answering
    dataset for source code comprehension. In *Findings of the Association for Computational
    Linguistics: EMNLP 2021*, pages 2618–2632.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 和 Wan (2021) Chenxiao Liu 和 Xiaojun Wan. 2021. Codeqa: 一个用于源代码理解的问答数据集。发表于*2021年计算语言学协会会议论文集:
    EMNLP 2021*，第2618–2632页。'
- en: 'Liu et al. (2023a) Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi
    Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian Martin
    Eisenschlos. 2023a. Matcha: Enhancing visual language pretraining with math reasoning
    and chart derendering. In *Proceedings of the 61th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘等人 (2023a) 刘芳雨, 弗朗切斯科·皮奇诺, 赛林·克里歇内, 庞晨曦, 肯顿·李, 曼达尔·乔希, 雅塞敏·阿尔图恩, 奈杰尔·科利尔,
    和 朱利安·马丁·艾森施洛斯。2023a。Matcha: 通过数学推理和图表去渲染增强视觉语言预训练。在*第61届计算语言学协会年会（第1卷: 长篇论文）*。'
- en: (66) Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. Retrieval-augmented
    generation for code summarization via hybrid gnn. In *International Conference
    on Learning Representations*.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (66) 刘上清, 陈宇, 谢晓飞, 萧晶凯, 和 刘杨。通过混合 GNN 进行代码总结的检索增强生成。在*国际学习表征会议*。
- en: 'Liu et al. (2023b) Shangqing Liu, Bozhi Wu, Xiaofei Xie, Guozhu Meng, and Yang
    Liu. 2023b. Contrabert: Enhancing code pre-trained models via contrastive learning.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘等人 (2023b) 刘上清, 吴博智, 谢晓飞, 孟国柱, 和 刘杨。2023b。Contrabert: 通过对比学习增强代码预训练模型。'
- en: Liu et al. (2023c) Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang
    Zan, JianGuang Lou, PinYuChen, and TsungYiHo. 2023c. Uncovering and quantifyingsocialbiases
    incodegeneration.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 (2023c) 刘燕, 陈晓康, 高焰, 苏哲, 张风集, 臧道光, 樓剑光, 陈品瑜, 和 何宗毅。2023c。揭示并量化代码生成中的社会偏见。
- en: 'Lu et al. (2022) Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang,
    and Alexey Svyatkovskiy. 2022. Reacc: A retrieval-augmented code completion framework.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 6227–6240.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陆等人 (2022) 陆帅, 段楠, 韩浩杰, 郭大亚, 黄承元, 和 亚历克谢·斯维亚特科夫斯基。2022。Reacc: 一个检索增强的代码补全框架。在*第60届计算语言学协会年会（第1卷:
    长篇论文）*，第6227–6240页。'
- en: 'Lu et al. (2021) Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
    Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong
    Zhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan,
    Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie LIU. 2021. [CodeXGLUE:
    A machine learning benchmark dataset for code understanding and generation](https://openreview.net/forum?id=6lE4dQXaUcb).
    In *Thirty-fifth Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track (Round 1)*.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陆等人 (2021) 陆帅, 郭大亚, 任硕, 黄俊杰, 亚历克谢·斯维亚特科夫斯基, 安布罗西奥·布兰科, 柯林·克莱门特, 黛恩·德雷恩, 蒋达鑫,
    唐度宇, 李戈, 周力东, 守林军, 周龙, 米歇尔·图法诺, 龚鸣, 周明, 段楠, 尼尔·桑达雷桑, 邓少坤, 傅胜宇, 和 刘书杰。2021。[CodeXGLUE:
    代码理解和生成的机器学习基准数据集](https://openreview.net/forum?id=6lE4dQXaUcb)。在*第三十五届神经信息处理系统会议数据集和基准跟踪（第一轮）*。'
- en: 'Ma et al. (2023) Yingwei Ma, Yue Yu, Shanshan Li, Zhouyang Jia, Jun Ma, Rulin
    Xu, Wei Dong, and Xiangke Liao. 2023. Mulcs: Towards a unified deep representation
    for multilingual code search. In *2023 IEEE International Conference on Software
    Analysis, Evolution and Reengineering (SANER)*, pages 120–131\. IEEE.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '马等人 (2023) 马颖伟, 于岳, 李姗姗, 贾周阳, 马俊, 许如林, 董伟, 和 廖香科。2023。Mulcs: 面向多语言代码搜索的统一深度表示。在*2023
    IEEE国际软件分析、演变与重构会议（SANER）*，第120–131页。IEEE。'
- en: McBurney and McMillan (2014) Paul W McBurney and Collin McMillan. 2014. Automatic
    documentation generation via source code summarization of method context. In *Proceedings
    of the 22nd International Conference on Program Comprehension*, pages 279–290.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 麦伯尼和麦克米伦 (2014) 保罗·W·麦伯尼 和 柯林·麦克米伦。2014。通过方法上下文的源代码总结进行自动文档生成。在*第22届国际程序理解会议论文集*，第279–290页。
- en: (73) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. In *International Conference on Learning Representations*.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (73) 斯蒂芬·梅里蒂, 蔡明雄, 詹姆斯·布拉德伯里, 和 理查德·索彻。指针哨兵混合模型。在*国际学习表征会议*。
- en: Mi et al. (2022a) Qing Mi, Yiqun Hao, Maran Wu, and Liwei Ou. 2022a. An enhanced
    data augmentation approach to support multi-class code readability classification.
    In *International conference on software engineering and knowledge engineering*.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 米等人 (2022a) 米青, 郝一群, 吴玛然, 和 欧李伟。2022a。一种增强的数据增强方法以支持多类别代码可读性分类。在*软件工程与知识工程国际会议*。
- en: Mi et al. (2022b) Qing Mi, Luo Wang, Lisha Hu, Liwei Ou, and Yang Yu. 2022b.
    Improving multi-class code readability classification with an enhanced data augmentation
    approach (130). *International Journal of Software Engineering and Knowledge Engineering*,
    32(11n12):1709–1731.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mi 等 (2022b) Qing Mi, Luo Wang, Lisha Hu, Liwei Ou, 和 Yang Yu. 2022b. 通过增强数据扩增方法提高多类别代码可读性分类
    (130). *国际软件工程与知识工程期刊*, 32(11n12):1709–1731.
- en: Mi et al. (2021) Qing Mi, Yan Xiao, Zhi Cai, and Xibin Jia. 2021. The effectiveness
    of data augmentation in code readability classification. *Information and Software
    Technology*, 129:106378.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mi 等 (2021) Qing Mi, Yan Xiao, Zhi Cai, 和 Xibin Jia. 2021. 数据扩增在代码可读性分类中的有效性.
    *信息与软件技术*, 129:106378.
- en: 'Mialon et al. (2023) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos
    Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane
    Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented language models: a survey.
    *arXiv preprint arXiv:2302.07842*.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mialon 等 (2023) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis,
    Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
    Asli Celikyilmaz, 等. 2023. 增强语言模型：综述. *arXiv 预印本 arXiv:2302.07842*.
- en: 'Miceli-Barone and Sennrich (2017) Antonio Valerio Miceli-Barone and Rico Sennrich.
    2017. A parallel corpus of python functions and documentation strings for automated
    code documentation and code generation. In *Proceedings of the Eighth International
    Joint Conference on Natural Language Processing (Volume 2: Short Papers)*, pages
    314–319.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miceli-Barone 和 Sennrich (2017) Antonio Valerio Miceli-Barone 和 Rico Sennrich.
    2017. 用于自动化代码文档和代码生成的 Python 函数和文档字符串的平行语料库. 在 *第八届国际联合自然语言处理会议论文集 (第 2 卷：短论文)*
    中，第 314–319 页.
- en: 'Mockus et al. (2002) Audris Mockus, Roy T Fielding, and James D Herbsleb. 2002.
    Two case studies of open source software development: Apache and mozilla. *ACM
    Transactions on Software Engineering and Methodology (TOSEM)*, 11(3):309–346.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mockus 等 (2002) Audris Mockus, Roy T Fielding, 和 James D Herbsleb. 2002. 开源软件开发的两个案例研究：Apache
    和 Mozilla. *ACM 软件工程与方法学期刊 (TOSEM)*, 11(3):309–346.
- en: Nashid et al. (2023) Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-based
    prompt selection for code-related few-shot learning. In *Proceedings of the 45th
    International Conference on Software Engineering (ICSE’23)*.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nashid 等 (2023) Noor Nashid, Mifta Sintaha, 和 Ali Mesbah. 2023. 基于检索的代码相关少样本学习提示选择.
    在 *第 45 届国际软件工程会议 (ICSE’23) 论文集* 中.
- en: 'Nijkamp et al. (2023) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open
    large language model for code with multi-turn program synthesis. *ICLR*.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nijkamp 等 (2023) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang,
    Yingbo Zhou, Silvio Savarese, 和 Caiming Xiong. 2023. Codegen: 一个开放的大型代码语言模型，支持多轮程序合成.
    *ICLR*.'
- en: Odena et al. (2017) Augustus Odena, Christopher Olah, and Jonathon Shlens. 2017.
    Conditional image synthesis with auxiliary classifier gans. In *International
    conference on machine learning*, pages 2642–2651\. PMLR.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Odena 等 (2017) Augustus Odena, Christopher Olah, 和 Jonathon Shlens. 2017. 使用辅助分类器GAN的条件图像合成.
    在 *国际机器学习会议* 中，第 2642–2651 页\. PMLR.
- en: Orlanski et al. (2023) Gabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey
    Hui, Joshua Howland, Jonathan Malmaud, Jacob Austin, Rishah Singh, and Michele
    Catasta. 2023. Measuring the impact of programming language distribution. *arXiv
    preprint arXiv:2302.01973*.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Orlanski 等 (2023) Gabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey Hui,
    Joshua Howland, Jonathan Malmaud, Jacob Austin, Rishah Singh, 和 Michele Catasta.
    2023. 衡量编程语言分布的影响. *arXiv 预印本 arXiv:2302.01973*.
- en: 'Orvalho et al. (2022) Pedro Orvalho, Mikoláš Janota, and Vasco Manquinho. 2022.
    Multipas: applying program transformations to introductory programming assignments
    for data augmentation. In *Proceedings of the 30th ACM Joint European Software
    Engineering Conference and Symposium on the Foundations of Software Engineering*,
    pages 1657–1661.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Orvalho 等 (2022) Pedro Orvalho, Mikoláš Janota, 和 Vasco Manquinho. 2022. Multipas：将程序转换应用于入门编程作业以进行数据扩增.
    在 *第 30 届 ACM 欧洲软件工程联合会议暨软件工程基础研讨会论文集* 中，第 1657–1661 页.
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In
    *Annual Meeting of the Association for Computational Linguistics*.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni 等 (2002) Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing Zhu.
    2002. Bleu：一种机器翻译自动评估方法. 在 *计算语言学协会年会* 中.
- en: Park et al. (2023) Shinwoo Park, Youngwook Kim, and Yo-Sub Han. 2023. Contrastive
    learning with keyword-based data augmentation for code search and code question
    answering. In *Conference of the European Chapter of the Association for Computational
    Linguistics*.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2023) Shinwoo Park, Youngwook Kim和Yo-Sub Han. 2023. 基于关键词的数据增强的对比学习用于代码搜索和代码问答.
    发表在*欧洲计算语言学协会会议*上。
- en: 'Parvez et al. (2021) Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi
    Ray, and Kai-Wei Chang. 2021. Retrieval augmented code generation and summarization.
    In *Findings of the Association for Computational Linguistics: EMNLP 2021*, pages
    2719–2734.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Parvez et al. (2021) Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi
    Ray和Kai-Wei Chang. 2021. 检索增强的代码生成与总结. 发表在*计算语言学协会发现: EMNLP 2021*上，第2719–2734页。'
- en: Pinku et al. (2023) Subroto Nag Pinku, Debajyoti Mondal, and Chanchal K Roy.
    2023. Pathways to leverage transcompiler based data augmentation for cross-language
    clone detection. *arXiv preprint arXiv:2303.01435*.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinku et al. (2023) Subroto Nag Pinku, Debajyoti Mondal和Chanchal K Roy. 2023.
    利用跨编译器数据增强技术进行跨语言克隆检测的路径. *arXiv预印本 arXiv:2303.01435*。
- en: Pour et al. (2021) Maryam Vahdat Pour, Zhuo Li, Lei Ma, and Hadi Hemmati. 2021.
    A search-based testing framework for deep neural networks of source code embedding.
    In *2021 14th IEEE Conference on Software Testing, Verification and Validation
    (ICST)*, pages 36–46\. IEEE.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pour et al. (2021) Maryam Vahdat Pour, Zhuo Li, Lei Ma和Hadi Hemmati. 2021. 基于搜索的深度神经网络源代码嵌入测试框架.
    发表在*2021年第14届IEEE软件测试、验证与验证会议（ICST）*上，第36–46页。IEEE。
- en: 'Puri et al. (2021) Ruchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo
    Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey
    Decker, et al. 2021. Codenet: A large-scale ai for code dataset for learning a
    diversity of coding tasks. In *Annual Conference on Neural Information Processing
    Systems*.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Puri et al. (2021) Ruchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo
    Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey
    Decker等人. 2021. Codenet: 一个大规模代码AI数据集，用于学习多样的编码任务. 发表在*年度神经信息处理系统会议*上。'
- en: Quiring et al. (2019) Erwin Quiring, Alwin Maier, Konrad Rieck, et al. 2019.
    Misleading authorship attribution of source code using adversarial learning. In
    *USENIX Security Symposium*, pages 479–496.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quiring et al. (2019) Erwin Quiring, Alwin Maier, Konrad Rieck等人. 2019. 使用对抗学习误导源代码的作者归属.
    发表在*USENIX Security Symposium*上，第479–496页。
- en: 'Rabin and Alipour (2022) Md Rafiqul Islam Rabin and Mohammad Amin Alipour.
    2022. Programtransformer: A tool for generating semantically equivalent transformed
    programs. *Software Impacts*, 14:100429.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rabin and Alipour (2022) Md Rafiqul Islam Rabin和Mohammad Amin Alipour. 2022.
    Programtransformer: 一种生成语义等价转化程序的工具. *Software Impacts*, 14:100429。'
- en: Rabin et al. (2021) Md Rafiqul Islam Rabin, Nghi DQ Bui, Ke Wang, Yijun Yu,
    Lingxiao Jiang, and Mohammad Amin Alipour. 2021. On the generalizability of neural
    program models with respect to semantic-preserving program transformations. *Information
    and Software Technology*, 135:106552.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabin et al. (2021) Md Rafiqul Islam Rabin, Nghi DQ Bui, Ke Wang, Yijun Yu,
    Lingxiao Jiang和Mohammad Amin Alipour. 2021. 关于神经程序模型在语义保留程序转换中的可泛化性. *信息与软件技术*,
    135:106552。
- en: Ray et al. (2019) Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, and Giedrius
    Burachas. 2019. Sunny and dark outside?! improving answer consistency in vqa through
    entailed question generation. In *Proceedings of the 2019 Conference on Empirical
    Methods in Natural Language Processing and the 9th International Joint Conference
    on Natural Language Processing (EMNLP-IJCNLP)*, pages 5860–5865.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ray et al. (2019) Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee和Giedrius
    Burachas. 2019. 外面晴天还是阴天？！通过蕴含问题生成提高VQA的答案一致性. 发表在*2019年自然语言处理经验方法会议与第9届国际自然语言处理联合会议（EMNLP-IJCNLP）*上，第5860–5865页。
- en: Raychev et al. (2016) Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016.
    Probabilistic model for code with decision trees. *ACM SIGPLAN Notices*, 51(10):731–747.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raychev et al. (2016) Veselin Raychev, Pavol Bielik和Martin Vechev. 2016. 使用决策树的代码概率模型.
    *ACM SIGPLAN Notices*, 51(10):731–747。
- en: 'Ren et al. (2020) Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu
    Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu:
    a method for automatic evaluation of code synthesis. *arXiv preprint arXiv:2009.10297*.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren et al. (2020) Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu
    Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco和Shuai Ma. 2020. Codebleu: 一种自动评估代码合成的方法.
    *arXiv预印本 arXiv:2009.10297*。'
- en: Roziere et al. (2020) Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot,
    and Guillaume Lample. 2020. Unsupervised translation of programming languages.
    *Advances in Neural Information Processing Systems*, 33:20601–20611.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roziere 等 (2020) Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot 和 Guillaume
    Lample. 2020. 无监督编程语言翻译。*神经信息处理系统进展*，33:20601–20611。
- en: Roziere et al. (2021) Baptiste Roziere, Jie Zhang, Francois Charton, Mark Harman,
    Gabriel Synnaeve, and Guillaume Lample. 2021. Leveraging automated unit tests
    for unsupervised code translation. In *International Conference on Learning Representations*.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roziere 等 (2021) Baptiste Roziere, Jie Zhang, Francois Charton, Mark Harman,
    Gabriel Synnaeve 和 Guillaume Lample. 2021. 利用自动化单元测试进行无监督代码翻译。发表于 *国际学习表征大会*。
- en: Sennrich et al. (2015) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015.
    Improving neural machine translation models with monolingual data. *arXiv preprint
    arXiv:1511.06709*.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sennrich 等 (2015) Rico Sennrich, Barry Haddow 和 Alexandra Birch. 2015. 利用单语数据改进神经机器翻译模型。*arXiv
    预印本 arXiv:1511.06709*。
- en: 'Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
    [Improving neural machine translation models with monolingual data](https://doi.org/10.18653/v1/P16-1009).
    In *Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 86–96, Berlin, Germany. Association
    for Computational Linguistics.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sennrich 等 (2016) Rico Sennrich, Barry Haddow 和 Alexandra Birch. 2016. [利用单语数据改进神经机器翻译模型](https://doi.org/10.18653/v1/P16-1009)。发表于
    *第54届计算语言学协会年会论文集 (第1卷：长篇论文)*，第86–96页，德国柏林。计算语言学协会。
- en: (101) Yiheng Shen, Xiaolin JU, Xiang Chen, and Guang Yang. Bash comment generation
    via data augmentation and semantic-aware codebert. *Available at SSRN 4385791*.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (101) Yiheng Shen, Xiaolin JU, Xiang Chen 和 Guang Yang. 通过数据增强和语义感知的 codebert
    生成 Bash 注释。*可在 SSRN 4385791 上获得*。
- en: 'Shi et al. (2023) Ensheng Shi, Yanlin Wang, Wenchao Gu, Lun Du, Hongyu Zhang,
    Shi Han, Dongmei Zhang, and Hongbin Sun. 2023. Cocosoda: Effective contrastive
    learning for code search. In *Proceedings of the 45th International Conference
    on Software Engineering*.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等 (2023) Ensheng Shi, Yanlin Wang, Wenchao Gu, Lun Du, Hongyu Zhang, Shi
    Han, Dongmei Zhang 和 Hongbin Sun. 2023. Cocosoda：有效的对比学习用于代码搜索。发表于 *第45届国际软件工程大会论文集*。
- en: 'Shi et al. (2022a) Yiwen Shi, Taha ValizadehAslani, Jing Wang, Ping Ren, Yi Zhang,
    Meng Hu, Liang Zhao, and Hualou Liang. 2022a. Improving imbalanced learning by
    pre-finetuning with data augmentation. In *Fourth International Workshop on Learning
    with Imbalanced Domains: Theory and Applications*, pages 68–82\. PMLR.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等 (2022a) Yiwen Shi, Taha ValizadehAslani, Jing Wang, Ping Ren, Yi Zhang,
    Meng Hu, Liang Zhao 和 Hualou Liang. 2022a. 通过数据增强进行预微调以改善不平衡学习。发表于 *第四届不平衡领域学习国际研讨会：理论与应用*，第68–82页。PMLR。
- en: Shi et al. (2022b) Zejian Shi, Yun Xiong, Xiaolong Zhang, Yao Zhang, Shanshan
    Li, and Yangyong Zhu. 2022b. Cross-modal contrastive learning for code search.
    In *2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)*,
    pages 94–105\. IEEE.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等 (2022b) Zejian Shi, Yun Xiong, Xiaolong Zhang, Yao Zhang, Shanshan Li
    和 Yangyong Zhu. 2022b. 用于代码搜索的跨模态对比学习。发表于 *2022 IEEE 国际软件维护与演化会议 (ICSME)*，第94–105页。IEEE。
- en: Shiri et al. (2022) Fatemeh Shiri, Terry Yue Zhuo, Zhuang Li, Shirui Pan, Weiqing
    Wang, Reza Haffari, Yuan-Fang Li, and Van Nguyen. 2022. Paraphrasing techniques
    for maritime qa system. In *2022 25th International Conference on Information
    Fusion (FUSION)*, pages 1–8\. IEEE.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shiri 等 (2022) Fatemeh Shiri, Terry Yue Zhuo, Zhuang Li, Shirui Pan, Weiqing
    Wang, Reza Haffari, Yuan-Fang Li 和 Van Nguyen. 2022. 海事问答系统的释义技术。发表于 *2022年第25届国际信息融合大会
    (FUSION)*，第1–8页。IEEE。
- en: Shorten and Khoshgoftaar (2019) Connor Shorten and Taghi M. Khoshgoftaar. 2019.
    A survey on image data augmentation for deep learning. *Journal of Big Data*,
    6:1–48.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shorten 和 Khoshgoftaar (2019) Connor Shorten 和 Taghi M. Khoshgoftaar. 2019.
    深度学习图像数据增强的调查。*大数据杂志*，6:1–48。
- en: 'Silva et al. (2023) André Silva, João F Ferreira, He Ye, and Martin Monperrus.
    2023. Mufin: Improving neural repair models with back-translation. *arXiv preprint
    arXiv:2304.02301*.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silva 等 (2023) André Silva, João F Ferreira, He Ye 和 Martin Monperrus. 2023.
    Mufin：通过回译改进神经修复模型。*arXiv 预印本 arXiv:2304.02301*。
- en: Song et al. (2022) Zixuan Song, Xiuwei Shang, Mengxuan Li, Rong Chen, Hui Li,
    and Shikai Guo. 2022. Do not have enough data? an easy data augmentation for code
    summarization. In *2022 IEEE 13th International Symposium on Parallel Architectures,
    Algorithms and Programming (PAAP)*, pages 1–6\. IEEE.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等 (2022) Zixuan Song, Xiuwei Shang, Mengxuan Li, Rong Chen, Hui Li 和 Shikai
    Guo. 2022. 数据不足？一种简便的代码摘要数据增强方法。发表于 *2022 IEEE 第13届国际并行体系结构、算法与编程研讨会 (PAAP)*，第1–6页。IEEE。
- en: 'Springer et al. (2021) Jacob M. Springer, Bryn Marie Reinstadler, and Una-May
    O’Reilly. 2021. [Strata: Simple, gradient-free attacks for models of code](http://arxiv.org/abs/2009.13562).'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Springer et al. (2021) Jacob M. Springer, Bryn Marie Reinstadler, 和 Una-May
    O’Reilly. 2021. [Strata: 简单的、无梯度的代码模型攻击](http://arxiv.org/abs/2009.13562)。'
- en: (110) Shashank Srikant, Sijia Liu, Tamara Mitrovska, Shiyu Chang, Quanfu Fan,
    Gaoyuan Zhang, and Una-May OŔeilly. Generating adversarial computer programs using
    optimized obfuscations. In *International Conference on Learning Representations*.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (110) Shashank Srikant, Sijia Liu, Tamara Mitrovska, Shiyu Chang, Quanfu Fan,
    Gaoyuan Zhang, 和 Una-May O’Reilly. 生成对抗计算机程序的优化混淆方法。发表于*国际学习表征会议*。
- en: 'Stahlberg (2020) Felix Stahlberg. 2020. Neural machine translation: A review.
    *Journal of Artificial Intelligence Research*, 69:343–418.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stahlberg (2020) Felix Stahlberg. 2020. 神经机器翻译：综述。*人工智能研究期刊*，69:343–418。
- en: 'Surís et al. (2023) Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt:
    Visual inference via python execution for reasoning. *arXiv preprint arXiv:2303.08128*.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Surís et al. (2023) Dídac Surís, Sachit Menon, 和 Carl Vondrick. 2023. Vipergpt:
    通过Python执行进行视觉推理。*arXiv预印本 arXiv:2303.08128*。'
- en: Svajlenko et al. (2014) Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal K
    Roy, and Mohammad Mamun Mia. 2014. Towards a big data curated benchmark of inter-project
    code clones. In *2014 IEEE International Conference on Software Maintenance and
    Evolution*, pages 476–480\. IEEE.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Svajlenko et al. (2014) Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal
    K Roy, 和 Mohammad Mamun Mia. 2014. 朝着一个大数据策划的跨项目代码克隆基准迈进。发表于*2014年IEEE国际软件维护与演化会议*，第476–480页。IEEE。
- en: 'Tang et al. (2020) Ruixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, and Xiaokang
    Yang. 2020. Semantic equivalent adversarial data augmentation for visual question
    answering. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
    August 23–28, 2020, Proceedings, Part XIX 16*, pages 437–453. Springer.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al. (2020) Ruixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, 和 Xiaokang Yang.
    2020. 用于视觉问答的语义等效对抗数据增强。发表于*计算机视觉–ECCV 2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第XIX卷第16部分*，第437–453页。Springer。
- en: Tang et al. (2023) Ze Tang, Jidong Ge, Shangqing Liu, Tingwei Zhu, Tongtong
    Xu, Liguo Huang, and Bin Luo. 2023. Domain adaptive code completion via language
    models and decoupled domain databases.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al. (2023) Ze Tang, Jidong Ge, Shangqing Liu, Tingwei Zhu, Tongtong
    Xu, Liguo Huang, 和 Bin Luo. 2023. 通过语言模型和解耦域数据库进行领域自适应代码补全。
- en: Tian et al. (2021) Junfeng Tian, Chenxin Wang, Zhen Li, and Yu Wen. 2021. Generating
    adversarial examples of source code classification models via q-learning-based
    markov decision process. In *2021 IEEE 21st International Conference on Software
    Quality, Reliability and Security (QRS)*, pages 807–818\. IEEE.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian et al. (2021) Junfeng Tian, Chenxin Wang, Zhen Li, 和 Yu Wen. 2021. 通过基于Q学习的马尔可夫决策过程生成源代码分类模型的对抗样本。发表于*2021年IEEE第21届软件质量、可靠性和安全性国际会议(QRS)*，第807–818页。IEEE。
- en: Tian et al. (2023) Zhao Tian, Junjie Chen, and Zhi Jin. 2023. Adversarial attacks
    on neural models of code via code difference reduction. *arXiv preprint arXiv:2301.02412*.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian et al. (2023) Zhao Tian, Junjie Chen, 和 Zhi Jin. 2023. 通过代码差异减少对神经代码模型的对抗攻击。*arXiv预印本
    arXiv:2301.02412*。
- en: Treude and Robillard (2016) Christoph Treude and Martin P Robillard. 2016. Augmenting
    api documentation with insights from stack overflow. In *Proceedings of the 38th
    International Conference on Software Engineering*, pages 392–403.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Treude and Robillard (2016) Christoph Treude 和 Martin P Robillard. 2016. 利用Stack
    Overflow的见解增强API文档。发表于*第38届国际软件工程会议论文集*，第392–403页。
- en: Wan et al. (2022) Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and
    Hai Jin. 2022. What do they capture? a structural analysis of pre-trained language
    models for source code. In *Proceedings of the 44th International Conference on
    Software Engineering*, pages 2377–2388.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. (2022) Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, 和
    Hai Jin. 2022. 他们捕获了什么？对预训练语言模型在源代码中表现的结构分析。发表于*第44届国际软件工程会议论文集*，第2377–2388页。
- en: Wan et al. (2018) Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian
    Wu, and Philip S Yu. 2018. Improving automatic source code summarization via deep
    reinforcement learning. In *Proceedings of the 33rd ACM/IEEE international conference
    on automated software engineering*, pages 397–407.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. (2018) Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian
    Wu, 和 Philip S Yu. 2018. 通过深度强化学习改进自动源代码摘要。发表于*第33届ACM/IEEE自动化软件工程国际会议论文集*，第397–407页。
- en: Wang et al. (2022a) Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong,
    Wei Dong, and Xiangke Liao. 2022a. Bridging pre-trained models and downstream
    tasks for source code understanding. In *Proceedings of the 44th International
    Conference on Software Engineering*, pages 287–298.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2022a）Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong,
    和 Xiangke Liao。2022a。连接预训练模型与下游任务以进行源代码理解。见于*第44届国际软件工程会议录*，第287–298页。
- en: Wang et al. (2022b) Jianzong Wang, Shijing Si, Zhitao Zhu, Xiaoyang Qu, Zhenhou
    Hong, and Jing Xiao. 2022b. Leveraging causal inference for explainable automatic
    program repair. In *2022 International Joint Conference on Neural Networks (IJCNN)*,
    pages 1–6\. IEEE.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2022b）Jianzong Wang, Shijing Si, Zhitao Zhu, Xiaoyang Qu, Zhenhou Hong,
    和 Jing Xiao。2022b。利用因果推断实现可解释的自动程序修复。见于*2022年国际神经网络联合会议（IJCNN）*，第1–6页。IEEE。
- en: 'Wang et al. (2023a) Shangwen Wang, Bo Lin, Zhensu Sun, Ming Wen, Yepang Liu,
    Yan Lei, and Xiaoguang Mao. 2023a. Two birds with one stone: Boosting code generation
    and code search via a generative adversarial network. *Proceedings of the ACM
    on Programming Languages*, 7(OOPSLA2):486–515.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023a）Shangwen Wang, Bo Lin, Zhensu Sun, Ming Wen, Yepang Liu, Yan Lei,
    和 Xiaoguang Mao。2023a。事半功倍：通过生成对抗网络提升代码生成和代码搜索。*ACM编程语言会议录*，7（OOPSLA2）：486–515。
- en: 'Wang et al. (2023b) Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian
    Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia,
    Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2023b.
    Recode: Robustness evaluation of code generation models. In *Proceedings of the
    61th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023b）Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang,
    Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh
    Nallapati, Murali Krishna Ramanathan, Dan Roth, 和 Bing Xiang。2023b。Recode：代码生成模型的鲁棒性评估。见于*第61届计算语言学协会年会（第一卷：长篇论文）*。
- en: 'Wang et al. (2021a) Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and
    Michael Zeng. 2021a. Want to reduce labeling cost? gpt-3 can help. In *Findings
    of the Association for Computational Linguistics: EMNLP 2021*, pages 4195–4205.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2021a）Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, 和 Michael Zeng。2021a。想减少标注成本？gpt-3可以帮忙。见于*计算语言学协会年会：EMNLP
    2021*，第4195–4205页。
- en: Wang et al. (2016) Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning
    semantic features for defect prediction. In *Proceedings of the 38th International
    Conference on Software Engineering*, pages 297–308.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2016）Song Wang, Taiyue Liu, 和 Lin Tan。2016。自动学习语义特征用于缺陷预测。见于*第38届国际软件工程会议录*，第297–308页。
- en: 'Wang et al. (2023c) Weishi Wang, Yue Wang, Shafiq Joty, and Steven CH Hoi.
    2023c. Rap-gen: Retrieval-augmented patch generation with codet5 for automatic
    program repair.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023c）Weishi Wang, Yue Wang, Shafiq Joty, 和 Steven CH Hoi。2023c。Rap-gen：使用codet5进行检索增强的补丁生成以实现自动程序修复。
- en: 'Wang et al. (2022c) Xiao Wang, Qiong Wu, Hongyu Zhang, Chen Lyu, Xue Jiang,
    Zhuoran Zheng, Lei Lyu, and Songlin Hu. 2022c. Heloc: Hierarchical contrastive
    learning of source code representation. In *Proceedings of the 30th IEEE/ACM International
    Conference on Program Comprehension*, pages 354–365.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2022c）Xiao Wang, Qiong Wu, Hongyu Zhang, Chen Lyu, Xue Jiang, Zhuoran
    Zheng, Lei Lyu, 和 Songlin Hu。2022c。Heloc：源代码表示的分层对比学习。见于*第30届IEEE/ACM国际程序理解会议录*，第354–365页。
- en: Wang et al. (2022d) Xin Wang, Xiao Liu, Pingyi Zhou, Qixia Liu, Jin Liu, Hao
    Wu, and Xiao Cui. 2022d. Test-driven multi-task learning with functionally equivalent
    code transformation for neural code generation. *Proceedings of the 37th IEEE/ACM
    International Conference on Automated Software Engineering*.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2022d）Xin Wang, Xiao Liu, Pingyi Zhou, Qixia Liu, Jin Liu, Hao Wu, 和 Xiao
    Cui。2022d。基于测试驱动的多任务学习与功能等效代码转换用于神经代码生成。*第37届IEEE/ACM国际自动化软件工程会议录*。
- en: Wang et al. (2022e) Xin Wang, Xiao Liu, Pingyi Zhou, Qixia Liu, Jin Liu, Hao
    Wu, and Xiaohui Cui. 2022e. Test-driven multi-task learning with functionally
    equivalent code transformation for neural code generation. In *Proceedings of
    the 37th IEEE/ACM International Conference on Automated Software Engineering*,
    pages 1–6.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2022e）Xin Wang, Xiao Liu, Pingyi Zhou, Qixia Liu, Jin Liu, Hao Wu, 和 Xiaohui
    Cui。2022e。基于测试驱动的多任务学习与功能等效代码转换用于神经代码生成。见于*第37届IEEE/ACM国际自动化软件工程会议录*，第1–6页。
- en: 'Wang et al. (2021b) Zeyu Wang, Sheng Huang, Zhongxin Liu, Meng Yan, Xin Xia,
    Bei Wang, and Dan Yang. 2021b. Plot2api: recommending graphic api from plot via
    semantic parsing guided neural network. In *2021 IEEE International Conference
    on Software Analysis, Evolution and Reengineering (SANER)*, pages 458–469\. IEEE.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2021b）王泽宇、黄晟、刘中鑫、严萌、夏鑫、王贝和杨丹。2021b。《Plot2api：通过语义解析引导的神经网络从图形推荐API》。见于*2021
    IEEE国际软件分析、演化与重构大会（SANER）*，第458–469页。IEEE。
- en: 'Wen et al. (2020) Qingsong Wen, Liang Sun, Xiaomin Song, Jing Gao, Xue Wang,
    and Huan Xu. 2020. Time series data augmentation for deep learning: A survey.
    In *International Joint Conference on Artificial Intelligence*.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen等（2020）温青松、孙亮、宋小敏、高晶、王雪和徐欢。2020。《深度学习的时间序列数据增强：综述》。见于*国际人工智能联合会议*。
- en: Wu et al. (2020) Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher Ré.
    2020. On the generalization effects of linear transformations in data augmentation.
    In *International Conference on Machine Learning*, pages 10410–10420\. PMLR.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2020）吴森、张宏阳、格雷戈里·瓦利安特和克里斯托弗·瑞。2020。《数据增强中线性变换的泛化效应》。见于*国际机器学习大会*，第10410–10420页。PMLR。
- en: 'Wu et al. (2023) Xingbo Wu, Nathanaël Cheriere, Cheng Zhang, and Dushyanth
    Narayanan. 2023. Rustgen: An augmentation approach for generating compilable rust
    code with large language models.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2023）吴兴博、纳塔纳埃尔·谢里埃、张程和杜尚南亚南。2023。《Rustgen：一种生成可编译Rust代码的增强方法，基于大语言模型》。
- en: Xie et al. (2023) Yiqing Xie, Atharva Naik, Daniel Fried, and Carolyn Rose.
    2023. Data augmentation for code translation with comparable corpora and multiple
    references.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等（2023）谢亦清、纳伊克·阿塔尔瓦、丹尼尔·弗里德和卡罗琳·罗斯。2023。《利用可比语料和多个参考进行代码翻译的数据增强》。
- en: Xu et al. (2020) Frank F Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu,
    and Graham Neubig. 2020. Incorporating external knowledge through pre-training
    for natural language to code generation. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics*, pages 6045–6052.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等（2020）徐方、姜正宝、殷鹏程、博格丹·瓦西尔斯库和格雷厄姆·纽比。2020。《通过预训练结合外部知识用于自然语言到代码生成》。见于*第58届计算语言学协会年会论文集*，第6045–6052页。
- en: 'Yang et al. (2023a) Guang Yang, Yu Zhou, Xiang Chen, Xiangyu Zhang, Tingting
    Han, and Taolue Chen. 2023a. Exploitgen: Template-augmented exploit code generation
    based on codebert. *Journal of Systems and Software*, 197:111577.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2023a）杨光、周宇、陈翔、张相宇、韩婷婷和陈涛略。2023a。《Exploitgen：基于CodeBERT的模板增强利用代码生成》。*系统与软件期刊*，197:111577。
- en: Yang et al. (2022a) Guang Yang, Yu Zhou, Wenhua Yang, Tao Yue, Xiang Chen, and
    Taolue Chen. 2022a. How important are good method names in neural code generation?
    a model robustness perspective. *arXiv preprint arXiv:2211.15844*.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2022a）杨光、周宇、杨文华、岳涛、陈翔和陈涛略。2022a。《神经代码生成中好的方法名称有多重要？一种模型鲁棒性视角》。*arXiv预印本arXiv:2211.15844*。
- en: Yang et al. (2023b) Guang Yang, Yu Zhou, Xiangyu Zhang, Xiang Chen, Tingting
    Han, and Taolue Chen. 2023b. Assessing and improving syntactic adversarial robustness
    of pre-trained models for code translation. *arXiv preprint arXiv:2310.18587*.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2023b）杨光、周宇、张相宇、陈翔、韩婷婷和陈涛略。2023b。《评估和改进预训练模型在代码翻译中的句法对抗鲁棒性》。*arXiv预印本arXiv:2310.18587*。
- en: Yang et al. (2023c) Hongyu Yang, Jianma Hui, Min Hou, Shuanghong Shen, and Enhong
    Chen. 2023c. A pre-training method for enhanced code representation based on multimodal
    contrastive learning. *Journal of Software*, 35(4):0–0.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2023c）杨红宇、惠建玛、侯敏、沈霜虹和陈恩宏。2023c。《基于多模态对比学习的增强代码表示的预训练方法》。*软件期刊*，35(4)：0–0。
- en: Yang et al. (2022b) Zhou Yang, Jieke Shi, Junda He, and David Lo. 2022b. Natural
    attack for pre-trained models of code. In *Proceedings of the 44th International
    Conference on Software Engineering*, pages 1482–1493.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2022b）周杨、谢杰克、何俊达和戴维·洛。2022b。《代码预训练模型的自然攻击》。见于*第44届国际软件工程大会论文集*，第1482–1493页。
- en: 'Yasunaga and Liang (2021) Michihiro Yasunaga and Percy Liang. 2021. Break-it-fix-it:
    Unsupervised learning for program repair. In *International Conference on Machine
    Learning*, pages 11941–11952\. PMLR.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yasunaga和Liang（2021）安田道广和梁佩西。2021。《Break-it-fix-it：程序修复的无监督学习》。见于*国际机器学习大会*，第11941–11952页。PMLR。
- en: 'Ye et al. (2023) Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang, Yangkai Du,
    Peiyu Liu, Wenhai Wang, and Shouling Ji. 2023. Tram: A token-level retrieval-augmented
    mechanism for source code summarization. *arXiv preprint arXiv:2305.11074*.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye 等（2023）Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang, Yangkai Du, Peiyu
    Liu, Wenhai Wang 和 Shouling Ji. 2023. Tram: 一种用于源代码总结的 token 级检索增强机制. *arXiv 预印本
    arXiv:2305.11074*。'
- en: Yefet et al. (2020) Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial
    examples for models of code. *Proceedings of the ACM on Programming Languages*,
    4(OOPSLA):1–30.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yefet 等（2020）Noam Yefet, Uri Alon 和 Eran Yahav. 2020. 针对代码模型的对抗样本. *ACM 编程语言会议论文集*，4（OOPSLA）：1–30。
- en: 'Yin and Neubig (2017) Pengcheng Yin and Graham Neubig. 2017. A syntactic neural
    model for general-purpose code generation. In *Proceedings of the 55th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*,
    pages 440–450.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 和 Neubig（2017）Pengcheng Yin 和 Graham Neubig. 2017. 一种用于通用代码生成的语法神经模型. 见
    *第 55 届计算语言学协会年会论文集（卷 1：长篇论文）*，第 440–450 页。
- en: 'Yoo et al. (2021) Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and
    Woomyoung Park. 2021. Gpt3mix: Leveraging large-scale language models for text
    augmentation. In *Findings of the Association for Computational Linguistics: EMNLP
    2021*, pages 2225–2239.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yoo 等（2021）Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee 和 Woomyoung
    Park. 2021. Gpt3mix: 利用大规模语言模型进行文本增强. 见 *计算语言学协会会议发现：EMNLP 2021*，第 2225–2239 页。'
- en: 'Yu et al. (2022a) Chi Yu, Guang Yang, Xiang Chen, Ke Liu, and Yanlin Zhou.
    2022a. Bashexplainer: Retrieval-augmented bash code comment generation based on
    fine-tuned codebert. In *2022 IEEE International Conference on Software Maintenance
    and Evolution (ICSME)*, pages 82–93\. IEEE.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2022a）Chi Yu, Guang Yang, Xiang Chen, Ke Liu 和 Yanlin Zhou. 2022a. Bashexplainer:
    基于微调的 CodeBERT 的检索增强 bash 代码注释生成. 见 *2022 IEEE 国际软件维护与演化会议（ICSME）*，第 82–93 页。IEEE。'
- en: Yu et al. (2022b) Shiwen Yu, Ting Wang, and Ji Wang. 2022b. Data augmentation
    by program transformation. *Journal of Systems and Software*, 190:111304.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等（2022b）Shiwen Yu, Ting Wang 和 Ji Wang. 2022b. 通过程序转换进行数据增强. *系统与软件期刊*，190:111304。
- en: 'Zhang et al. (2018) Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David
    Lopez-Paz. 2018. [mixup: Beyond empirical risk minimization](https://openreview.net/forum?id=r1Ddp1-Rb).
    In *International Conference on Learning Representations*.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2018）Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin 和 David Lopez-Paz.
    2018. [mixup: 超越经验风险最小化](https://openreview.net/forum?id=r1Ddp1-Rb). 见 *国际学习表示大会*。'
- en: Zhang et al. (2020a) Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and
    Zhi Jin. 2020a. Generating adversarial examples for holding robustness of source
    code processing models. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 34, pages 1169–1176.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020a）Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu 和 Zhi Jin. 2020a.
    生成对抗样本以保持源代码处理模型的鲁棒性. 见 *AAAI 人工智能大会论文集*，第 34 卷，第 1169–1176 页。
- en: Zhang et al. (2020b) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong
    Liu. 2020b. Retrieval-based neural source code summarization. In *Proceedings
    of the ACM/IEEE 42nd International Conference on Software Engineering*, pages
    1385–1397.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020b）Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun 和 Xudong Liu. 2020b.
    基于检索的神经源代码总结. 见 *ACM/IEEE 第 42 届国际软件工程会议论文集*，第 1385–1397 页。
- en: Zhang et al. (2020c) Xiaoqing Zhang, Yu Zhou, Tingting Han, and Taolue Chen.
    2020c. Training deep code comment generation models via data augmentation. In
    *Proceedings of the 12th Asia-Pacific Symposium on Internetware*, pages 185–188.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020c）Xiaoqing Zhang, Yu Zhou, Tingting Han 和 Taolue Chen. 2020c. 通过数据增强训练深度代码注释生成模型.
    见 *第 12 届亚太互联网软件研讨会论文集*，第 185–188 页。
- en: 'Zhang et al. (2022) Yifan Zhang, Chen Huang, Yueke Zhang, Kevin Cao, Scott Thomas
    Andersen, Huajie Shao, Kevin Leach, and Yu Huang. 2022. Combo: Pre-training representations
    of binary code using contrastive learning. *arXiv preprint arXiv:2210.05102*.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2022）Yifan Zhang, Chen Huang, Yueke Zhang, Kevin Cao, Scott Thomas
    Andersen, Huajie Shao, Kevin Leach 和 Yu Huang. 2022. Combo: 使用对比学习预训练二进制代码表示.
    *arXiv 预印本 arXiv:2210.05102*。'
- en: 'Zheng et al. (2021) Yunhui Zheng, Saurabh Pujar, Burn Lewis, Luca Buratti,
    Edward Epstein, Bo Yang, Jim Laredo, Alessandro Morari, and Zhong Su. 2021. D2a:
    A dataset built for ai-based vulnerability detection methods using differential
    analysis. In *2021 IEEE/ACM 43rd International Conference on Software Engineering:
    Software Engineering in Practice (ICSE-SEIP)*, pages 111–120. IEEE.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2021) Yunhui Zheng, Saurabh Pujar, Burn Lewis, Luca Buratti,
    Edward Epstein, Bo Yang, Jim Laredo, Alessandro Morari, 和 Zhong Su. 2021. D2a:
    一个用于基于 AI 的漏洞检测方法的差分分析数据集。在 *2021 IEEE/ACM 第43届国际软件工程大会：实践中的软件工程 (ICSE-SEIP)*，页面
    111–120。IEEE。'
- en: 'Zhou et al. (2019) Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and
    Yang Liu. 2019. Devign: Effective vulnerability identification by learning comprehensive
    program semantics via graph neural networks. *Advances in neural information processing
    systems*, 32.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2019) Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, 和
    Yang Liu. 2019. Devign: 通过图神经网络学习全面程序语义的有效漏洞识别。*神经信息处理系统进展*, 32。'
- en: Zhou et al. (2022) Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue
    Chen, and Harald Gall. 2022. Adversarial robustness of deep code comment generation.
    *ACM Transactions on Software Engineering and Methodology (TOSEM)*, 31(4):1–30.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2022) Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue
    Chen, 和 Harald Gall. 2022. 深度代码注释生成的对抗性鲁棒性。*ACM 软件工程与方法学交易 (TOSEM)*, 31(4):1–30。
- en: Zhuo (2023) Terry Yue Zhuo. 2023. Large language models are state-of-the-art
    evaluators of code generation. *arXiv preprint arXiv:2304.14317*.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuo (2023) Terry Yue Zhuo. 2023. 大型语言模型是最先进的代码生成评估器。*arXiv 预印本 arXiv:2304.14317*。
- en: 'Zhuo et al. (2023a) Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang
    Xing. 2023a. Exploring ai ethics of chatgpt: A diagnostic analysis. *arXiv preprint
    arXiv:2301.12867*.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuo et al. (2023a) Terry Yue Zhuo, Yujin Huang, Chunyang Chen, 和 Zhenchang
    Xing. 2023a. 探索 ChatGPT 的 AI 伦理：一种诊断分析。*arXiv 预印本 arXiv:2301.12867*。
- en: 'Zhuo et al. (2023b) Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri,
    Weiqing Wang, Gholamreza Haffari, and Yuan-Fang Li. 2023b. On robustness of prompt-based
    semantic parsing with large pre-trained language model: An empirical study on
    codex. In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, pages 1090–1102.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuo et al. (2023b) Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing
    Wang, Gholamreza Haffari, 和 Yuan-Fang Li. 2023b. 基于大型预训练语言模型的提示式语义解析的鲁棒性：对 Codex
    的实证研究。在 *第十七届欧洲计算语言学协会年会论文集*，页面 1090–1102。
- en: Zubkov et al. (2022) Maksim Zubkov, Egor Spirin, Egor Bogomolov, and Timofey
    Bryksin. 2022. Evaluation of contrastive learning with various code representations
    for code clone detection. *arXiv preprint arXiv:2206.08726*.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zubkov et al. (2022) Maksim Zubkov, Egor Spirin, Egor Bogomolov, 和 Timofey Bryksin.
    2022. 利用各种代码表示的对比学习在代码克隆检测中的评估。*arXiv 预印本 arXiv:2206.08726*。
