- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 20:01:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:01:58'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2003.11066] A Survey of Methods for Low-Power Deep Learning and Computer Vision'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2003.11066] 《低功耗深度学习与计算机视觉方法综述》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2003.11066](https://ar5iv.labs.arxiv.org/html/2003.11066)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2003.11066](https://ar5iv.labs.arxiv.org/html/2003.11066)
- en: A Survey of Methods for Low-Power Deep Learning and Computer Vision
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《低功耗深度学习与计算机视觉方法综述》
- en: Abhinav Goel, Caleb Tung, Yung-Hsiang Lu, and George K. Thiruvathukal2 {goel39,
    tung3, yunglu}@purdue.edu, gkt@cs.luc.edu School of Electrical and Computer Engineering,
    Purdue University 2Department of Computer Science, Loyola University Chicago
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Abhinav Goel、Caleb Tung、Yung-Hsiang Lu 和 George K. Thiruvathukal2 {goel39, tung3,
    yunglu}@purdue.edu，gkt@cs.luc.edu，普渡大学电气与计算机工程学院 2芝加哥洛约拉大学计算机科学系
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep neural networks (DNNs) are successful in many computer vision tasks. However,
    the most accurate DNNs require millions of parameters and operations, making them
    energy, computation and memory intensive. This impedes the deployment of large
    DNNs in low-power devices with limited compute resources. Recent research improves
    DNN models by reducing the memory requirement, energy consumption, and number
    of operations without significantly decreasing the accuracy. This paper surveys
    the progress of low-power deep learning and computer vision, specifically in regards
    to inference, and discusses the methods for compacting and accelerating DNN models.
    The techniques can be divided into four major categories: (1) parameter quantization
    and pruning, (2) compressed convolutional filters and matrix factorization, (3)
    network architecture search, and (4) knowledge distillation. We analyze the accuracy,
    advantages, disadvantages, and potential solutions to the problems with the techniques
    in each category. We also discuss new evaluation metrics as a guideline for future
    research.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）在许多计算机视觉任务中表现成功。然而，最准确的DNNs需要数百万个参数和操作，使其在能源、计算和内存方面的需求很高。这阻碍了大型DNNs在资源有限的低功耗设备上的应用。最近的研究通过减少内存需求、能耗和操作数量来改进DNN模型，而不会显著降低准确性。本文综述了低功耗深度学习和计算机视觉的进展，特别是在推理方面，并讨论了紧凑化和加速DNN模型的方法。这些技术可以分为四大类：（1）参数量化和剪枝，（2）压缩卷积滤波器和矩阵分解，（3）网络架构搜索，以及（4）知识蒸馏。我们分析了每类技术的准确性、优缺点及潜在解决方案。同时讨论了作为未来研究指南的新评估指标。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: neural networks, computer vision, low-power
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络、计算机视觉、低功耗
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Deep Neural Networks (DNNs) are widely used in computer vision tasks like object
    detection, classification, and segmentation [[1](#bib.bibx1), [2](#bib.bibx2)].
    DNNs are designated as “Deep” because they are made of many layers with a large
    spread of connections between layers. This gives DNNs a tremendous range of variability
    that can be fine-tuned for accurate inference through training. Unfortunately,
    DNNs are also computation-heavy and energy-expensive as a result. VGG-16 [[3](#bib.bibx3)]
    needs 15 billion operations to perform image classification on a single image [[4](#bib.bibx4)].
    Similarly, YOLOv3 performs 39 billion operations to process one image [[5](#bib.bibx5)].
    These many computations require significant compute resources and lead to a high
    energy cost [[6](#bib.bibx6)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）广泛应用于对象检测、分类和分割等计算机视觉任务[[1](#bib.bibx1), [2](#bib.bibx2)]。DNNs之所以称为“深度”，是因为它们由许多层组成，并且层之间有广泛的连接。这赋予了DNNs极大的可变性，可以通过训练进行精确推理。不幸的是，DNNs也因而计算量大且耗能。VGG-16
    [[3](#bib.bibx3)] 在对单张图像进行分类时需要150亿次操作[[4](#bib.bibx4)]。类似地，YOLOv3 处理一张图像时需要进行390亿次操作[[5](#bib.bibx5)]。这些大量的计算需要显著的计算资源，并导致高能耗[[6](#bib.bibx6)]。
- en: 'This presents a problem for DNNs: how can they be meaningfully deployed on
    low-power embedded systems and mobile devices? Such machines are often constrained
    by battery power or obtain energy through low-current USB connections [[7](#bib.bibx7)].
    They also do not usually come with GPUs. Offloading computing to the cloud is
    a solution [[8](#bib.bibx8)], but many DNN applications need to be performed on
    low-power devices, e.g., computer vision deployed on drones flying in areas without
    reliable network coverage to offload computation, or in satellites where offloading
    is too expensive [[9](#bib.bibx9)].'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这给DNN带来了一个问题：如何在低功耗嵌入式系统和移动设备上进行有意义的部署？这些机器通常受到电池电量的限制，或通过低电流USB连接获取能量[[7](#bib.bibx7)]。它们通常也没有配备GPU。将计算卸载到云端是一种解决方案[[8](#bib.bibx8)]，但许多DNN应用需要在低功耗设备上进行，例如，在没有可靠网络覆盖的区域中飞行的无人机上部署的计算机视觉，或在卸载过于昂贵的卫星中[[9](#bib.bibx9)]。
- en: Some low-power computer vision techniques remove redundancies from DNNs to reduce
    the number of operations by 75% and the inference time by 50% with negligible
    loss in accuracy. To deploy DNNs on small embedded computers, more such optimizations
    are necessary. Therefore, pursuing low-power improvements in deep learning for
    efficient inference is worthwhile and is a growing area of research [[10](#bib.bibx10)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一些低功耗计算机视觉技术通过从DNN中去除冗余，将操作次数减少75%，推理时间缩短50%，同时*精度损失微乎其微*。为了在小型嵌入式计算机上部署DNN，还需要更多这样的优化。因此，追求深度学习中的低功耗改进以实现高效推理是值得的，并且是一个不断增长的研究领域[[10](#bib.bibx10)]。
- en: 'This paper surveys the literature and reports state-of-the-art solutions for
    low-power computer vision. We focus specifically on low-power DNN inference, not
    training, as the goal is to attain high throughput. The paper classifies the low-power
    inference methods into four categories:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文调查了相关文献，并报告了低功耗计算机视觉的最新解决方案。我们特别关注低功耗DNN推理，而非训练，因为目标是实现高吞吐量。本文将低功耗推理方法分为四类：
- en: '1.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Parameter Quantization and Pruning: Lowers the memory and computation costs
    by reducing the number of bits used to store the parameters of DNN models.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数量化和剪枝：通过减少存储DNN模型参数所用的位数来降低内存和计算成本。
- en: '2.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Compressed Convolutional Filters and Matrix Factorization: Decomposes large
    DNN layers into smaller layers to decrease the memory requirement and the number
    of redundant matrix operations.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 压缩卷积滤波器和矩阵分解：将大型DNN层分解为较小的层，以减少内存需求和冗余矩阵操作的数量。
- en: '3.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Network Architecture Search: Builds DNNs with different combinations of layers
    automatically to find a DNN architecture that achieves the desired performance.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网络架构搜索：自动构建不同层组合的DNN，以找到实现期望性能的DNN架构。
- en: '4.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Knowledge Distillation: Trains a compact DNN that mimics the outputs, features,
    and activations of a more computation-heavy DNN.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识蒸馏：训练一个紧凑的DNN，模仿一个计算量更大的DNN的输出、特征和激活。
- en: TABLE [I](#S1.T1 "TABLE I ‣ I Introduction ‣ A Survey of Methods for Low-Power
    Deep Learning and Computer Vision") summarizes these methods. This survey will
    focus on the above mentioned software-based low-power computer vision techniques,
    without considering low-power hardware optimizations (e.g. hardware accelerators,
    spiking DNNs). This paper uses results reported in the existing literature to
    analyze the advantages, disadvantages, and propose potential improvements to the
    four methods. We also suggest an additional set of evaluation metrics to guide
    future research.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [I](#S1.T1 "TABLE I ‣ I Introduction ‣ A Survey of Methods for Low-Power Deep
    Learning and Computer Vision")总结了这些方法。本调查将重点关注上述软件基础的低功耗计算机视觉技术，而不考虑低功耗硬件优化（例如硬件加速器、脉冲DNN）。本文利用现有文献中的结果来分析四种方法的优缺点，并提出潜在的改进建议。我们还建议增加一组评估指标，以指导未来的研究。
- en: '| Technique | Description | Advantages | Disadvantages |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 描述 | 优点 | 缺点 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Quantization and Pruning | Reduces precision/completely removes the redundant
    parameters and connections from a DNN. | Negligible accuracy loss with small model
    size. Highly efficient arithmetic operations. | Difficult to implement on CPUs
    and GPUs because of matrix sparsity. High training costs. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 量化和剪枝 | 降低精度/完全去除DNN中冗余的参数和连接。 | 小模型尺寸，*精度损失微乎其微*。高效的算术运算。 | 由于矩阵稀疏，难以在CPU和GPU上实现。高训练成本。
    |'
- en: '| Filter Compression and Matrix Factorization | Decreases the size of DNN filters
    and layers to improve efficiency. | High accuracy. Compatible with other optimization
    techniques. | Compact convolutions can be memory- inefficient. Matrix factorization
    is computationally expensive. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 滤波器压缩和矩阵分解 | 减少DNN滤波器和层的大小以提高效率。 | 高准确性。与其他优化技术兼容。 | 紧凑的卷积可能在内存上效率低。矩阵分解计算成本高。
    |'
- en: '| Network Architecture Search | Automatically finds a DNN architecture that
    meets performance and accuracy requirements on a target device. | State-of-the-art
    accuracy with low energy consumption. | Prohibitively high training costs. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 网络架构搜索 | 自动找到满足目标设备上性能和准确性要求的DNN架构。 | 具有前沿的准确性和低能耗。 | 训练成本极高。 |'
- en: '| Knowledge Distillation | Trains a small DNN with the knowledge of a larger
    DNN to reduce model size. | Low computation cost with few DNN parameters. | Strict
    assumptions on DNN structure. Only compatible with softmax outputs. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 知识蒸馏 | 利用较大DNN的知识训练较小的DNN，以减小模型规模。 | 低计算成本，参数较少。 | 对DNN结构有严格的假设，仅适用于softmax输出。
    |'
- en: 'TABLE I: Comparison of different techniques for performing low-power computer
    vision.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 不同低功耗计算机视觉技术的比较。'
- en: II Parameter Quantization and Pruning
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 参数量化与剪枝
- en: Memory accesses contribute significantly to the energy consumption of DNNs [[4](#bib.bibx4),
    [11](#bib.bibx11)]. To build low-power DNNs, recent research has looked into the
    tradeoff between accuracy and the number of memory accesses.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 内存访问对DNN的能耗贡献显著[[4](#bib.bibx4), [11](#bib.bibx11)]。为了构建低功耗的DNN，最近的研究关注了精度和内存访问次数之间的权衡。
- en: II-A Quantization of Deep Neural Networks
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 深度神经网络的量化
- en: One method to reduce the number of memory accesses is to decrease the size of
    DNN parameters. Some methods [[12](#bib.bibx12), [13](#bib.bibx13)] show that
    it is possible to have negligible accuracy loss even when the precision of the
    DNN parameters is reduced. Courbariaux et al. [[13](#bib.bibx13)] experiment with
    parameters stored in different fixed-point formats to demonstrate that reduced
    bit-widths are sufficient for training DNNs. Fig. [1](#S2.F1 "Figure 1 ‣ II-A
    Quantization of Deep Neural Networks ‣ II Parameter Quantization and Pruning ‣
    A Survey of Methods for Low-Power Deep Learning and Computer Vision") compares
    the energy consumption and test error of different DNN architectures with varying
    levels of quantization. Here, as the parameter bit-width decreases, the energy
    consumption decreases at the expense of increasing test error. Building on these
    findings, LightNN [[14](#bib.bibx14)], CompactNet [[15](#bib.bibx15)], and FLightNN [[11](#bib.bibx11)]
    find the optimal bit-width for different parameters of a DNN, given an accuracy
    constraint. Moons et al. [[16](#bib.bibx16)] also use DNNs with parameters in
    different integer formats. Binarized neural networks proposed in Courbariaux el
    al. [[17](#bib.bibx17)] and Rastegari et al. [[18](#bib.bibx18)] train DNNs with
    binary parameters and activations. In binarized neural networks each parameter
    is represented with a single bit. Because of the major reduction in precision,
    these DNNs require many layers to obtain high accuracy. In Fig. [1](#S2.F1 "Figure
    1 ‣ II-A Quantization of Deep Neural Networks ‣ II Parameter Quantization and
    Pruning ‣ A Survey of Methods for Low-Power Deep Learning and Computer Vision"),
    for a given DNN architecture, the 1-bit quantization (binarized neural networks)
    consumes the least energy and has the highest error. To improve the accuracy of
    binarized DNNs, Zhou et al. [[19](#bib.bibx19)] quantize the back propagation
    gradients for better training convergence.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 减少内存访问次数的一种方法是减少 DNN 参数的大小。一些方法[[12](#bib.bibx12), [13](#bib.bibx13)]显示，即使在减少
    DNN 参数的精度时，也可能不会显著损失准确性。Courbariaux 等人[[13](#bib.bibx13)]通过实验使用不同的定点格式存储参数，展示了减少比特宽度足以训练
    DNN。图[1](#S2.F1 "Figure 1 ‣ II-A Quantization of Deep Neural Networks ‣ II Parameter
    Quantization and Pruning ‣ A Survey of Methods for Low-Power Deep Learning and
    Computer Vision")比较了不同 DNN 架构在不同量化水平下的能量消耗和测试误差。在这里，随着参数比特宽度的减少，能量消耗降低，但测试误差增加。基于这些发现，LightNN[[14](#bib.bibx14)],
    CompactNet[[15](#bib.bibx15)], 和 FLightNN[[11](#bib.bibx11)]在给定准确性约束的情况下，找到了 DNN
    不同参数的最佳比特宽度。Moons 等人[[16](#bib.bibx16)]也使用了具有不同整数格式参数的 DNN。Courbariaux 等人[[17](#bib.bibx17)]和
    Rastegari 等人[[18](#bib.bibx18)]提出的二值化神经网络使用二进制参数和激活来训练 DNN。在二值化神经网络中，每个参数用单个位表示。由于精度大幅降低，这些
    DNN 需要很多层才能获得高准确性。在图[1](#S2.F1 "Figure 1 ‣ II-A Quantization of Deep Neural Networks
    ‣ II Parameter Quantization and Pruning ‣ A Survey of Methods for Low-Power Deep
    Learning and Computer Vision")中，对于给定的 DNN 架构，1 位量化（二值化神经网络）消耗的能量最少，但误差最大。为了提高二值化
    DNN 的准确性，Zhou 等人[[19](#bib.bibx19)]对反向传播梯度进行量化，以实现更好的训练收敛性。
- en: Often, parameter quantization is used along with compression to further reduce
    the memory requirement of DNNs. Han et al. [[4](#bib.bibx4)] first quantize the
    parameters into discrete bins. Huffman coding is then used to compress these bins
    to reduce the model size by approximately 89%, with negligible accuracy loss.
    Similarly, HashedNet [[20](#bib.bibx20)] quantizes the DNN connections into hash
    buckets such that all connections grouped to the same hash bucket share a single
    parameter. However, because these techniques have a high training cost, their
    adoption is limited.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，参数量化与压缩一起使用，以进一步减少 DNN 的内存需求。Han 等人[[4](#bib.bibx4)]首先将参数量化为离散的桶，然后使用霍夫曼编码压缩这些桶，将模型大小减少约
    89%，而准确性几乎没有损失。类似地，HashedNet[[20](#bib.bibx20)]将 DNN 连接量化为哈希桶，使所有分组到同一哈希桶的连接共享一个参数。然而，由于这些技术具有较高的训练成本，其采用受到限制。
- en: 'Advantages: When the bit-widths of the parameters decrease, the prediction
    performance of DNNs remains constant [[13](#bib.bibx13), [21](#bib.bibx21)]. This
    is because constraining parameters has a regularization effect in the training
    process. Moreover, when designing custom hardware for DNNs, quantization allows
    power-hungry multiply-accumulate operations to be replaced with shift [[15](#bib.bibx15)]
    or XNOR [[18](#bib.bibx18)] operations: leading to a reduction in circuit area
    and energy requirements. Disadvantages and Potential Improvements: DNNs employing
    quantization techniques need to be retrained multiple times, making the training
    process very expensive [[19](#bib.bibx19)]. The training cost must be reduced
    to make these techniques easier more practical. Furthermore, different layers
    in DNNs are sensitive to different features. A constant bit-width for all layers
    can lead to poor performance [[11](#bib.bibx11)]. In order to select a different
    parameter precision for each connection of the DNN (depending on its importance
    to the output), the precision value can be represented in a differentiable manner
    and be included in the training process. Thus, during training, each connection
    will learn its parameter value and the parameter precision.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：当参数的位宽降低时，DNN 的预测性能保持不变 [[13](#bib.bibx13), [21](#bib.bibx21)]。这是因为约束参数在训练过程中具有正则化效应。此外，在为
    DNN 设计定制硬件时，量化允许将功耗高的乘加操作替换为移位 [[15](#bib.bibx15)] 或 XNOR [[18](#bib.bibx18)]
    操作，从而减少电路面积和能量需求。缺点和潜在改进：采用量化技术的 DNN 需要多次重新训练，使得训练过程非常昂贵 [[19](#bib.bibx19)]。必须降低训练成本，以使这些技术更加实用。此外，DNN
    中不同层对不同特征的敏感性不同。对所有层使用恒定的位宽可能导致性能不佳 [[11](#bib.bibx11)]。为了为 DNN 的每个连接选择不同的参数精度（取决于其对输出的重要性），精度值可以以可微分的方式表示并纳入训练过程中。因此，在训练过程中，每个连接将学习其参数值和参数精度。
- en: '![Refer to caption](img/ec2c616baad9555f4c565246522c488e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec2c616baad9555f4c565246522c488e.png)'
- en: 'Figure 1: Pareto analysis of DNNs with varying levels of quantization on the
    MNIST dataset. Lower-left is better because it indicates low energy and smaller
    error. The annotations ^(1,2,3) on the data-points represent three different DNN
    architectures for each quantization technique used in the experiments [[15](#bib.bibx15)].'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：MNIST 数据集上不同量化级别的 DNNs 的帕累托分析。左下角较好，因为它表示低能耗和较小的误差。数据点上的注释 ^(1,2,3) 代表了实验中使用的每种量化技术对应的三种不同
    DNN 架构 [[15](#bib.bibx15)]。
- en: II-B Pruning Parameters and Connections
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 剪枝参数和连接
- en: Removing the unimportant parameters and connections from DNNs can reduce the
    number of memory accesses. The Hessian-weighted distortion measure can be used
    to identify the importance of parameters in a DNN [[22](#bib.bibx22)]. Some techniques [[23](#bib.bibx23),
    [24](#bib.bibx24)] use this measure to remove redundant parameters and reduce
    the DNN size. These measure-based pruning methods only operate on the fully-connected
    layers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从 DNNs 中删除不重要的参数和连接可以减少内存访问的次数。Hessian 加权失真度量可以用来识别 DNN 中参数的重要性 [[22](#bib.bibx22)]。一些技术
    [[23](#bib.bibx23), [24](#bib.bibx24)] 使用这一度量来去除冗余参数并减小 DNN 的大小。这些基于度量的剪枝方法仅在全连接层上操作。
- en: '| Method | LeNet 5 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LeNet 5 |'
- en: '&#124; LeNet &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LeNet &#124;'
- en: '&#124; 300-100 &#124;'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 300-100 &#124;'
- en: '| AlexNet | VGG-16 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| AlexNet | VGG-16 |'
- en: '&#124; Training &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '&#124; Time &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间 &#124;'
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Unoptimized | 1.000 | 1.000 | 1.000 | 1.000 | 1.000 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 未优化 | 1.000 | 1.000 | 1.000 | 1.000 | 1.000 |'
- en: '| P | 0.080 | 0.080 | 0.090 | 0.075 | 4.000 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| P | 0.080 | 0.080 | 0.090 | 0.075 | 4.000 |'
- en: '| P+Q | 0.031 | 0.030 | 0.037 | 0.032 | 6.000 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| P+Q | 0.031 | 0.030 | 0.037 | 0.032 | 6.000 |'
- en: '| P+Q+C | 0.025 | 0.025 | 0.028 | 0.020 | 6.000 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| P+Q+C | 0.025 | 0.025 | 0.028 | 0.020 | 6.000 |'
- en: 'TABLE II: Comparison of model compression rates with different DNNs [[4](#bib.bibx4)].
    Note that training time increases as models are compressed. P: Pruning, Q: Quantization,
    C: Compression.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II：不同 DNNs 的模型压缩率比较 [[4](#bib.bibx4)]。注意，随着模型压缩的增加，训练时间也会增加。P: 剪枝，Q: 量化，C:
    压缩。'
- en: To extend pruning to convolutional layers, Anwar et al. [[25](#bib.bibx25)]
    use particle filtering to locate the pruning candidates. Polyak et al. [[26](#bib.bibx26)]
    use sample input data and prune the sparsely activated connections. Han et al. [[27](#bib.bibx27)]
    use a new loss function to learn both parameters and connections in DNNs. Yu et
    al. [[28](#bib.bibx28)] use an algorithm that propagates importance scores to
    measure the importance of each parameter with respect to the final output. By
    performing pruning, quantization, and encoding, Deep Compression [[4](#bib.bibx4)]
    reduces the model size by $95\%$. Path-level pruning is also seen in tree-based
    hierarchical DNNs [[29](#bib.bibx29), [30](#bib.bibx30)]. Although these techniques
    can identify the unimportant connections, they create unwanted sparsity in DNNs.
    Sparse matrices require special data structures (unavailable in deep learning
    libraries), and are difficult to map onto modern GPUs. To overcome this issue,
    some methods [[31](#bib.bibx31), [32](#bib.bibx32)] concentrate on building pruned
    DNNs with sparsity constraints.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将剪枝扩展到卷积层，Anwar 等人 [[25](#bib.bibx25)] 使用粒子过滤器来定位剪枝候选。Polyak 等人 [[26](#bib.bibx26)]
    使用样本输入数据并剪枝稀疏激活的连接。Han 等人 [[27](#bib.bibx27)] 使用新的损失函数来学习 DNN 中的参数和连接。Yu 等人 [[28](#bib.bibx28)]
    使用一种算法来传播重要性评分，以衡量每个参数相对于最终输出的重要性。通过进行剪枝、量化和编码，Deep Compression [[4](#bib.bibx4)]
    将模型大小减少了 $95\%$。树状层次 DNN 中也可以看到路径级剪枝 [[29](#bib.bibx29), [30](#bib.bibx30)]。尽管这些技术能够识别不重要的连接，但它们会在
    DNN 中产生不必要的稀疏性。稀疏矩阵需要特殊的数据结构（在深度学习库中不可用），并且难以映射到现代 GPU 上。为了克服这个问题，一些方法 [[31](#bib.bibx31),
    [32](#bib.bibx32)] 重点在于构建具有稀疏性约束的剪枝 DNN。
- en: 'Advantages: As seen in TABLE [II](#S2.T2 "TABLE II ‣ II-B Pruning Parameters
    and Connections ‣ II Parameter Quantization and Pruning ‣ A Survey of Methods
    for Low-Power Deep Learning and Computer Vision"), pruning can be combined with
    quantization and encoding for significant performance gains. When the three techniques
    are used together, the VGG-16 model size decreases to $2\%$ of its original size.
    Furthermore, pruning reduces the complexity of DNNs, and thus reduces overfitting.
    Disadvantages and Potential Improvements: The training effort associated with
    DNN pruning is considerable because DNNs have to be pruned and trained multiple
    times. TABLE [II](#S2.T2 "TABLE II ‣ II-B Pruning Parameters and Connections ‣
    II Parameter Quantization and Pruning ‣ A Survey of Methods for Low-Power Deep
    Learning and Computer Vision") shows that the training time increases by 600%
    when using pruning and quantization together. This problem is exacerbated when
    the DNNs are pruned with sparsity constraints [[33](#bib.bibx33)]. Also, the advantages
    of pruning are noticed only when using custom hardware or special data structures
    for sparse matrices [[33](#bib.bibx33)]. Channel-level pruning is a potential
    improvement to the existing connection-level pruning techniques, because it can
    be performed without any special data structures and does not create unintended
    matrix sparsity. By developing techniques to automatically identify unimportant
    channels, it is possible to perform channel-level pruning without a significant
    training overhead.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 优势：如在 TABLE [II](#S2.T2 "TABLE II ‣ II-B Pruning Parameters and Connections
    ‣ II Parameter Quantization and Pruning ‣ A Survey of Methods for Low-Power Deep
    Learning and Computer Vision") 中所示，剪枝可以与量化和编码结合，以实现显著的性能提升。当这三种技术一起使用时，VGG-16
    模型的大小减少到原始大小的 $2\%$。此外，剪枝减少了 DNN 的复杂性，从而减少了过拟合。劣势及潜在改进：与 DNN 剪枝相关的训练工作量相当大，因为
    DNN 必须进行多次剪枝和训练。TABLE [II](#S2.T2 "TABLE II ‣ II-B Pruning Parameters and Connections
    ‣ II Parameter Quantization and Pruning ‣ A Survey of Methods for Low-Power Deep
    Learning and Computer Vision") 显示，当同时使用剪枝和量化时，训练时间增加了 600%。当 DNN 在具有稀疏性约束的情况下进行剪枝时，这一问题更加严重
    [[33](#bib.bibx33)]。此外，只有在使用自定义硬件或特殊数据结构来处理稀疏矩阵 [[33](#bib.bibx33)] 时，才能注意到剪枝的优势。通道级剪枝是对现有连接级剪枝技术的潜在改进，因为它可以在没有任何特殊数据结构的情况下进行，并且不会产生意外的矩阵稀疏性。通过开发技术来自动识别不重要的通道，可以在没有显著训练开销的情况下进行通道级剪枝。
- en: III Convolutional Filter Compression and Matrix Factorization
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 卷积滤波器压缩与矩阵分解
- en: Convolution operations contribute to the bulk of the computations in DNNs, and
    the fully connected layers contain around 89% of the parameters in DNNs like AlexNet [[32](#bib.bibx32)].
    To reduce the power consumption of DNNs, researchers have focused on reducing
    the computations in convolution layers, and the number of parameters in fully
    connected layers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作在深度神经网络（DNN）的计算中占据了主要部分，而全连接层在像 AlexNet [[32](#bib.bibx32)] 这样的 DNN 中含有大约
    89% 的参数。为了减少 DNN 的功耗，研究者们集中于减少卷积层中的计算量以及全连接层中的参数数量。
- en: III-A Convolutional Filter Compression
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 卷积滤波器压缩
- en: '| Method |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |'
- en: '&#124; ImageNet &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ImageNet &#124;'
- en: '&#124; Top-1 Acc &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Top-1 准确率 &#124;'
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Number of &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数量 &#124;'
- en: '&#124; Parameters &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 参数 &#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Number of &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数量 &#124;'
- en: '&#124; Operations &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 操作数 &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| AlexNet [[34](#bib.bibx34)] | 57.20% | 60.00 M | 727 M |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| AlexNet [[34](#bib.bibx34)] | 57.20% | 60.00 M | 727 M |'
- en: '| SqueezeNet 1.0 [[35](#bib.bibx35)] | 57.50% | 1.24 M | 837 M |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeNet 1.0 [[35](#bib.bibx35)] | 57.50% | 1.24 M | 837 M |'
- en: '| SqueezeNet 1.1 [[35](#bib.bibx35)] | 58.00% | 1.24 M | 360 M |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeNet 1.1 [[35](#bib.bibx35)] | 58.00% | 1.24 M | 360 M |'
- en: '| MobileNet v3 Large [[36](#bib.bibx36)] | 75.20% | 5.40 M | 219 M |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet v3 Large [[36](#bib.bibx36)] | 75.20% | 5.40 M | 219 M |'
- en: '| MobileNet v3 Small [[36](#bib.bibx36)] | 67.40% | 2.50 M | 56 M |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet v3 Small [[36](#bib.bibx36)] | 67.40% | 2.50 M | 56 M |'
- en: '| ShiftNet-A [[37](#bib.bibx37)] | 70.10% | 4.10 M | 1,400 M |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| ShiftNet-A [[37](#bib.bibx37)] | 70.10% | 4.10 M | 1,400 M |'
- en: '| Shift Attention Layer [[38](#bib.bibx38)] | 71.00% | 3.30 M | 538 M |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Shift Attention Layer [[38](#bib.bibx38)] | 71.00% | 3.30 M | 538 M |'
- en: 'TABLE III: Comparison of convolutional filter compression techniques (accuracy,
    number of parameters and operations).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：卷积滤波器压缩技术的比较（准确率、参数数量和操作数）。
- en: 'Smaller convolution filters have considerably fewer parameters and lower computation
    costs than larger filters. For example, a 1$\times$1 filter has only 11% parameters
    of a 3$\times$3 filter. However, removing all large convolution layers affects
    the translation invariance property of a DNN and lowers its accuracy [[39](#bib.bibx39)].
    Some studies identify and replace redundant filters with smaller filters for DNN
    acceleration. SqueezeNet [[35](#bib.bibx35)] is one such technique that uses three
    strategies to convert $3\times 3$ convolutions into $1\times 1$ convolutions to
    reduce the number of parameters. TABLE [III](#S3.T3 "TABLE III ‣ III-A Convolutional
    Filter Compression ‣ III Convolutional Filter Compression and Matrix Factorization
    ‣ A Survey of Methods for Low-Power Deep Learning and Computer Vision") compares
    the performance of different convolutional filter compression techniques: SqueezeNet
    has 98% ($1-\frac{1.24}{60}$) fewer parameters than AlexNet, at the expense of
    a greater number of operations. MobileNets [[36](#bib.bibx36)] and SqueezeNet 1.1
    reduce the number of operations along with the number of parameters [[10](#bib.bibx10)].
    MobileNets use depthwise separable convolutions along with bottleneck layers to
    decrease the computation, latency, and the number of parameters. MobileNets achieve
    high accuracy by maintaining a small feature size and only expanding to a larger
    feature space when performing the depthwise separable convolutions.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的卷积滤波器相对于较大的滤波器具有显著较少的参数和较低的计算成本。例如，1$\times$1 滤波器的参数只有 3$\times$3 滤波器的 11%。然而，去除所有大型卷积层会影响
    DNN 的平移不变性属性，并降低其准确性 [[39](#bib.bibx39)]。一些研究通过识别和用较小的滤波器替换冗余滤波器来加速 DNN。SqueezeNet [[35](#bib.bibx35)]
    就是一种利用三种策略将 $3\times 3$ 卷积转换为 $1\times 1$ 卷积以减少参数数量的技术。表 [III](#S3.T3 "TABLE III
    ‣ III-A Convolutional Filter Compression ‣ III Convolutional Filter Compression
    and Matrix Factorization ‣ A Survey of Methods for Low-Power Deep Learning and
    Computer Vision") 比较了不同卷积滤波器压缩技术的性能：SqueezeNet 的参数比 AlexNet 少 98%（$1-\frac{1.24}{60}$），但代价是操作数增多。MobileNets [[36](#bib.bibx36)]
    和 SqueezeNet 1.1 同时减少了操作数和参数数量 [[10](#bib.bibx10)]。MobileNets 通过使用深度可分卷积和瓶颈层来减少计算量、延迟和参数数量。MobileNets
    通过保持小的特征尺寸并仅在执行深度可分卷积时扩展到更大的特征空间，从而实现高准确率。
- en: 'Advantages: Bottleneck convolutional filters reduce the memory and latency
    requirements of DNNs significantly. For most computer vision tasks, these techniques
    obtain state-of-the-art accuracy. Filter compaction is orthogonal to pruning and
    quantization techniques. The three techniques can be used together to further
    reduce energy consumption. Disadvantages and Potential Improvements: It has been
    shown that the $1\times 1$ convolutions are computationally expensive in small
    DNNs, and lead to poor accuracy [[40](#bib.bibx40)]. It is also difficult to implement
    depthwise separable convolutions efficiently because their arithmetic intensity
    (ratio of number of operations to memory accesses) is too low to efficiently utilize
    the hardware [[37](#bib.bibx37)]. The arithmetic intensity of depthwise separable
    convolutions can be increased by managing memory more effectively. By optimizing
    the spatial and temporal locality of the parameters in the cache, the number of
    memory accesses can be reduced.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：瓶颈卷积滤波器显著减少了深度神经网络（DNN）的内存和延迟需求。对于大多数计算机视觉任务，这些技术能获得**最先进**的准确度。滤波器压缩与剪枝和量化技术正交，这三种技术可以结合使用，以进一步降低能耗。缺点和潜在改进：已经证明，$1\times
    1$ 卷积在小型 DNN 中计算开销较大，且导致较差的准确性 [[40](#bib.bibx40)]。实现深度可分离卷积也很困难，因为它们的算术强度（操作次数与内存访问的比率）过低，无法有效利用硬件
    [[37](#bib.bibx37)]。通过更有效地管理内存，可以增加深度可分离卷积的算术强度。通过优化缓存中参数的空间和时间局部性，可以减少内存访问次数。
- en: III-B Matrix Factorization
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 矩阵分解
- en: Tensor decompositions and matrix factorizations represent the DNN operations
    in a sum-product form for acceleration [[41](#bib.bibx41), [42](#bib.bibx42)].
    Such techniques factorize multi-dimensional tensors (in convolutional and fully-connected
    layers) into smaller matrices to eliminate redundant computation. Some factorizations
    accelerate DNNs up to 4$\times$ because they create dense parameter matrices and
    avoid the locality problem of non-structured sparse multiplications [[33](#bib.bibx33)].
    To minimize the accuracy loss, matrix factorizations are performed one layer at
    a time. After factorizing the parameters of one layer, subsequent layers are then
    factorized based on some reconstruction error. The layer-by-layer optimization
    approach makes it difficult to scale these techniques to large DNNs because the
    number of factorization hyper-parameters increases exponentially with DNN depth.
    To apply these methods in large DNNs, Wen et al. [[33](#bib.bibx33)] enforce compact
    kernel shapes and depth structures to reduce the number of factorization hyper-parameters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 张量分解和矩阵分解将 DNN 操作表示为加和乘积形式以加速 [[41](#bib.bibx41), [42](#bib.bibx42)]。这些技术将多维张量（在卷积层和全连接层中）分解为更小的矩阵，以消除冗余计算。一些分解方法使
    DNN 加速高达 4$\times$，因为它们创建了稠密的参数矩阵，避免了非结构化稀疏乘法的局部性问题 [[33](#bib.bibx33)]。为了最小化准确性损失，矩阵分解是逐层进行的。对一层的参数进行分解后，随后的层基于某些重建误差进行分解。逐层优化方法使得将这些技术扩展到大型
    DNN 时变得困难，因为分解超参数的数量随着 DNN 深度的增加而呈指数增长。为了在大型 DNN 中应用这些方法，Wen 等人 [[33](#bib.bibx33)]
    强制使用紧凑的卷积核形状和深度结构，以减少分解超参数的数量。
- en: There are multiple matrix factorization techniques. Kolda et al. [[43](#bib.bibx43)]
    show that most factorization techniques can be applied to DNNs for acceleration.
    However, some techniques do not necessarily provide the optimal tradeoff between
    accuracy and computation complexity [[43](#bib.bibx43)]. Canonical Polyadic Decomposition
    (CPD) and Batch Normalization Decomposition (BMD) are the best performing decompositions
    in terms of accuracy, while the Tucker-2 Decomposition and the Singular Value
    Decomposition (SVD) result in poor accuracy [[44](#bib.bibx44), [45](#bib.bibx45)].
    CPD compresses the DNN more than BMD, and thus accelerates the DNN to a greater
    extent. The accuracy obtained with BMD is higher than CPD. Moreover, the optimization
    problem associated with CPD is sometimes unsolvable, making the factorization
    impossible [[45](#bib.bibx45)]. On the other hand, BMD is a stable factorization
    and it always exists.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种矩阵分解技术。Kolda 等人 [[43](#bib.bibx43)] 证明，大多数分解技术可以应用于 DNN 以加速其运算。然而，一些技术不一定提供最佳的准确率和计算复杂度之间的权衡
    [[43](#bib.bibx43)]。在准确率方面，标准多项式分解 (CPD) 和批量归一化分解 (BMD) 是表现最好的分解方法，而 Tucker-2
    分解和奇异值分解 (SVD) 的准确率较差 [[44](#bib.bibx44), [45](#bib.bibx45)]。CPD 比 BMD 更加压缩 DNN，从而使
    DNN 的加速效果更大。BMD 获得的准确率高于 CPD。此外，CPD 相关的优化问题有时难以解决，使得分解变得不可能 [[45](#bib.bibx45)]。另一方面，BMD
    是一种稳定的分解方法，总是存在的。
- en: 'Advantages: Matrix factorizations are methods to reduce the computation costs
    in DNNs. The same factorizations can be used in both convolutional layers and
    fully connected layers. The performance gain when using CPD and BMD is significant,
    with small accuracy loss. Disadvantages and Potential Improvements: Because of
    the lack of theoretical understanding, it is difficult to say why some decompositions
    (e.g. CPD and BMD) obtain high accuracy, while other decompositions (e.g. Tucker-2
    Decomposition and SVD) do not. Furthermore, the computation costs associated with
    matrix factorization often offset the performance gains obtained from performing
    fewer operations. Matrix factorizations are also difficult to implement in large
    DNNs because the training time increases exponentially with increasing depth.
    The high training time is mainly attributed to the fact that the search space
    for finding the correct decomposition hyper-parameters is large. Instead of searching
    through the entire space, the hyper-parameters can be included in the training
    process and be learned to accelerate training for large DNNs.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 优势：矩阵分解方法可以降低 DNN 的计算成本。相同的分解方法可以同时用于卷积层和全连接层。使用 CPD 和 BMD 时，性能提升显著，且准确率损失较小。缺点和潜在改进：由于缺乏理论理解，很难解释为什么某些分解（例如
    CPD 和 BMD）能够获得高准确率，而其他分解（例如 Tucker-2 分解和 SVD）则无法做到这一点。此外，矩阵分解的计算成本往往抵消了减少操作所带来的性能提升。矩阵分解在大型
    DNN 中实施也很困难，因为训练时间随着深度的增加呈指数增长。高训练时间主要归因于寻找正确分解超参数的搜索空间很大。可以将超参数包含在训练过程中，并通过学习加速大型
    DNN 的训练，而不是在整个空间中搜索。
- en: IV Network Architecture Search
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 网络架构搜索
- en: There are many different DNN architectures and optimization techniques to consider
    when designing low-power computer vision applications. It is often difficult to
    manually find the best DNN for a particular task when there are many architectural
    possibilities. Network Architecture Search (NAS) is a technique that automates
    DNN architecture design for various tasks. NAS uses a Recurrent Neural Network
    (RNN) controller and uses reinforced learning to compose candidate DNN architectures.
    These candidate architectures are trained and then tested with the validation
    set. The validation accuracy is used as a reward function to then optimize the
    controller’s next candidate architecture. NASNet [[46](#bib.bibx46)] and AmoebaNet [[47](#bib.bibx47)]
    demonstrate the effectiveness of NAS to obtain state-of-the-art accuracy. To automatically
    find efficient DNNs for mobile devices, Tan et al. [[48](#bib.bibx48)] propose
    MNasNet. This technique uses a multi-objective reward function in the controller
    to find a DNN architecture that achieves the desired accuracy and latency (when
    deployed on a target mobile device) requirements. MNasNet is $2.3\times$ faster
    than NASNet with $4.8\times$ fewer parameters and $10\times$ fewer operations.
    Moreover, MNasNet is also more accurate than NASNet. Despite the remarkable results,
    most NAS algorithms are prohibitively computation-intensive, requiring to train
    thousands of candidate architectures for a single task. MNasNet requires 50,000
    GPU hours to find an efficient DNN architecture on the ImageNet dataset.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计低功耗计算机视觉应用时，需要考虑许多不同的DNN架构和优化技术。当存在多种架构选择时，手动找到适合特定任务的最佳DNN往往是困难的。网络架构搜索（NAS）是一种自动化DNN架构设计的技术。NAS使用一个递归神经网络（RNN）控制器，并通过强化学习来构建候选DNN架构。这些候选架构会被训练，然后用验证集进行测试。验证准确率作为奖励函数用于优化控制器的下一个候选架构。NASNet [[46](#bib.bibx46)]和AmoebaNet [[47](#bib.bibx47)]展示了NAS在获得最先进的准确率方面的有效性。为了自动找到适合移动设备的高效DNN，Tan等人 [[48](#bib.bibx48)]提出了MNasNet。这项技术在控制器中使用多目标奖励函数，以寻找一个在目标移动设备上满足准确率和延迟（部署时）的DNN架构。MNasNet比NASNet快$2.3\times$，参数减少$4.8\times$，操作减少$10\times$。此外，MNasNet的准确率也优于NASNet。尽管结果显著，但大多数NAS算法计算成本极高，需要为单一任务训练数千个候选架构。MNasNet需要50,000个GPU小时才能在ImageNet数据集上找到一个高效的DNN架构。
- en: To reduce the computation costs associated with NAS, some researchers propose
    to search for candidate architectures based on proxy tasks and rewards, such as
    working with a smaller dataset (e.g. CIFAR-10) or approximating the device latency
    with the number of DNN parameters. FBNet [[49](#bib.bibx49)] is one such technique
    that uses a proxy task (optimizing over a smaller dataset) to find efficient architectures
    $420\times$ faster than MNasNet. Cai et al. [[50](#bib.bibx50)] show that DNN
    architectures optimized on proxy tasks are not guaranteed to be optimal on the
    target task, especially when hardware metrics like latency are approximated with
    the number of operations or the number of parameters. They also propose Proxyless-NAS
    to overcome the limitations with proxy-based NAS solutions. Proxyless-NAS uses
    path-level pruning to reduce the number of candidate architectures and a gradient-based
    approach for handling objectives like latency to find an efficient architecture
    in approximately 300 GPU hours. A technique called Single-Path NAS [[51](#bib.bibx51)]
    reduces the architecture search time to 4 hours. This speedup comes at the cost
    of reduced accuracy.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低与NAS相关的计算成本，一些研究者提出基于代理任务和奖励来搜索候选架构，例如使用较小的数据集（如CIFAR-10）或用DNN参数的数量来近似设备延迟。FBNet [[49](#bib.bibx49)]是一种利用代理任务（在较小数据集上优化）来寻找高效架构的技术，比MNasNet快$420\times$。Cai等人 [[50](#bib.bibx50)]表明，在代理任务上优化的DNN架构并不能保证在目标任务上最优，特别是当像延迟这样的硬件指标通过操作数量或参数数量来近似时。他们还提出了Proxyless-NAS以克服基于代理的NAS解决方案的限制。Proxyless-NAS使用路径级剪枝来减少候选架构的数量，并采用基于梯度的方法处理像延迟这样的目标，以在大约300个GPU小时内找到一个高效的架构。一种叫做Single-Path
    NAS [[51](#bib.bibx51)]的技术将架构搜索时间缩短至4小时。这种加速以准确率降低为代价。
- en: 'Advantages: NAS automatically balances the trade-offs between accuracy, memory,
    and latency by searching through the space of all possible architectures without
    any human intervention. NAS achieves state-of-the-art performance in terms of
    accuracy and energy efficiency on many mobile devices. Disadvantages and Potential
    Improvements: The computational demand of NAS algorithms makes it difficult to
    search for architectures that are optimized for large datasets. To find an architecture
    that meets the performance requirements, each candidate architecture must be trained
    (to check accuracy) and run on the target device (to check latency/energy) to
    generate the reward function. The time taken to train and measure the performance
    of each candidate architecture is significant - leading to high computation costs.
    To reduce the training time, the candidate DNNs can be trained in parallel with
    different subsets of the data. The gradients obtained from the different data
    subsets can be merged to produce one trained DNN. However, such parallel training
    techniques generally result in low accuracy. Accuracy can be increased by using
    adaptive learning rates while maintaining high convergence rates.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 优势：NAS 自动在准确性、内存和延迟之间平衡，通过搜索所有可能架构的空间而无需人工干预。NAS 在许多移动设备上实现了在准确性和能效方面的最先进性能。劣势与潜在改进：NAS
    算法的计算需求使得搜索针对大型数据集优化的架构变得困难。为了找到符合性能要求的架构，每个候选架构必须经过训练（以检查准确性）和在目标设备上运行（以检查延迟/能量）以生成奖励函数。训练和测量每个候选架构性能所花费的时间是显著的——这导致了高计算成本。为了减少训练时间，可以并行训练候选
    DNN，使用数据的不同子集。从不同数据子集获得的梯度可以合并以生成一个训练好的 DNN。然而，这种并行训练技术通常会导致较低的准确性。可以通过使用自适应学习率来提高准确性，同时保持高收敛速度。
- en: V Knowledge Transfer and Distillation
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识转移与蒸馏
- en: Large DNNs are more accurate than small DNNs because the greater numbers of
    parameters allow large DNNs to learn complex functions [[52](#bib.bibx52)]. Training
    small DNNs to learn such functions is challenging with the conventional back propagation
    algorithm. However, some methods train small DNNs to learn complex functions by
    making the small DNNs mimic larger pre-trained DNNs. These techniques transfer
    the “knowledge” of a large DNN to a small DNN through a process called Knowledge
    Transfer (KT). Some early techniques utilizing KT [[53](#bib.bibx53), [54](#bib.bibx54)]
    have been widely used to perform DNN compression. Here, the small DNN is trained
    on data labeled by a large DNN in order to learn complex functions. The key idea
    behind such techniques is that the data labeled by the large DNN contains a lot
    of information that is useful for the small DNN. For example, if the large DNN
    outputs a moderately high probability to multiple categories for a single input
    image, then it might mean that those categories might share some visual features.
    By forcing the small DNN to mimic these probabilities, the small DNN learns more
    than what is available in the training data [[55](#bib.bibx55)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 大型 DNN 比小型 DNN 更准确，因为更多的参数使得大型 DNN 能够学习复杂的函数[[52](#bib.bibx52)]。使用传统的反向传播算法训练小型
    DNN 学习这些复杂函数是具有挑战性的。然而，一些方法通过使小型 DNN 模仿大型预训练 DNN 来训练小型 DNN 学习复杂函数。这些技术通过一种称为知识转移（KT）的过程，将大型
    DNN 的“知识”转移到小型 DNN 上。一些早期利用 KT 的技术[[53](#bib.bibx53), [54](#bib.bibx54)] 已广泛用于执行
    DNN 压缩。在这里，小型 DNN 在大型 DNN 标记的数据上进行训练，以学习复杂函数。这些技术的关键思想是，大型 DNN 标记的数据包含了对小型 DNN
    有用的很多信息。例如，如果大型 DNN 对单个输入图像输出多个类别的中等高概率，这可能意味着这些类别可能共享一些视觉特征。通过迫使小型 DNN 模仿这些概率，小型
    DNN 学到的内容比训练数据中提供的更多[[55](#bib.bibx55)]。
- en: Hinton et al. [[56](#bib.bibx56)] propose another class of techniques called
    Knowledge Distillation (KD), where the training process is significantly simpler
    than KT based techniques. In their work, the small DNN is trained using a student-teacher
    paradigm. The small DNN is the student, and an ensemble of specialized DNNs is
    the teacher. By training the student to mimic the output of the teacher, the authors
    show that the small DNN can perform the task of the ensemble with some loss in
    accuracy. To improve the accuracy of the small DNN, Li et al. [[57](#bib.bibx57)]
    minimize the Euclidean distance of feature vectors between the teacher and the
    student. Similarly, FitNet [[58](#bib.bibx58)] builds small and thin DNNs by making
    each layer in the student mimic a feature map in the teacher. However, the metrics
    used in Li et al. [[57](#bib.bibx57)] and FitNet [[58](#bib.bibx58)] require strict
    assumptions on the structure of the student and are not sufficient to build energy-efficient
    student DNNs. To solve this problem and improve generalizability, Peng et al. [[59](#bib.bibx59)]
    utilize the correlation between the metrics as the optimization problem during
    training.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Hinton等人[[56](#bib.bibx56)]提出了另一类技术，称为知识蒸馏（KD），其训练过程显著简单于基于KT的技术。在他们的工作中，小型DNN采用学生-教师模式进行训练。小型DNN是学生，专门的DNN集合是教师。通过训练学生模仿教师的输出，作者展示了小型DNN可以在一定的精度损失下执行集合的任务。为了提高小型DNN的准确性，Li等人[[57](#bib.bibx57)]最小化了教师和学生之间特征向量的欧几里得距离。类似地，FitNet[[58](#bib.bibx58)]通过使学生的每一层模仿教师中的特征图来构建小型和稀疏的DNN。然而，Li等人[[57](#bib.bibx57)]和FitNet[[58](#bib.bibx58)]中使用的度量需要对学生的结构做出严格假设，并不足以构建能源高效的学生DNN。为了解决这个问题并提高泛化能力，Peng等人[[59](#bib.bibx59)]在训练过程中利用度量之间的相关性作为优化问题。
- en: 'Advantages: KT- and KD-based techniques can reduce the computation costs of
    large pre-trained DNNs significantly. Prior research has shown that the concepts
    used in KD can be used outside computer vision as well [[55](#bib.bibx55)] (e.g.
    semi-supervised learning, domain adaptation, etc.) Disadvantages and Potential
    Improvements: KD often places strict assumptions on the structure and the size
    of the student and the teacher, making it difficult to generalize to all applications.
    Moreover, the current KD techniques rely heavily on the softmax output and do
    not work with different output layers. Instead of making the student just mimic
    the outputs of neurons and layers from the teacher, the student can learn the
    sequence in which the neurons are activated. This removes the requirement that
    the student and the teacher have the same DNN structure (thus improving generalizability)
    and reduces the reliance on softmax output layers.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 优势：基于KT和KD的技术可以显著减少大型预训练DNN的计算成本。先前的研究表明，KD中使用的概念也可以应用于计算机视觉之外[[55](#bib.bibx55)]（例如，半监督学习、领域适应等）。劣势和潜在改进：KD通常对学生和教师的结构和大小有严格的假设，使其难以推广到所有应用。此外，目前的KD技术过于依赖softmax输出，并且不适用于不同的输出层。学生可以学习神经元激活的顺序，而不是仅仅模仿教师的神经元和层的输出。这去除了学生和教师具有相同DNN结构的要求（从而提高了泛化能力）并减少了对softmax输出层的依赖。
- en: VI Discussion
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 讨论
- en: VI-A Guidelines for Low-Power Computer Vision
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 低功耗计算机视觉的指南
- en: There is no single technique to build efficient DNNs for computer vision. Most
    techniques are complementary and can be used together for better energy efficiency.
    We include some general guidelines to consider for low-power computer vision.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 没有单一的技术可以构建高效的计算机视觉DNN。大多数技术是互补的，可以一起使用以提高能源效率。我们包括一些用于低功耗计算机视觉的通用指南。
- en: '1.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Quantization and reduced parameter precision decrease the model size and the
    complexity of arithmetic operations significantly, but unfortunately, it is difficult
    to manually implement quantization in most machine learning libraries. NVIDIA’s
    TensorRT library provides an interface for such optimizations.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化和减少参数精度显著降低了模型的大小和算术运算的复杂性，但不幸的是，在大多数机器学习库中，手动实现量化是困难的。NVIDIA的TensorRT库提供了这样的优化接口。
- en: '2.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: When optimizing large pre-trained DNNs, pruning and model compression are effective
    options.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在优化大型预训练深度神经网络（DNNs）时，剪枝和模型压缩是有效的选项。
- en: '3.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: When training a new DNN from scratch, compressed convolutional filters and matrix
    factorizations should be used to reduce the model size and computation.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在从零开始训练新的DNN时，应使用压缩卷积滤波器和矩阵分解来减少模型的大小和计算量。
- en: '4.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: NAS finds DNNs that are optimized for individual devices with performance guarantees.
    DNNs with several branches (e.g. Proxyless-NAS, MNasNet, etc.) frequently require
    expensive kernel launches and synchronizations on GPUs and CPUs.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NAS 寻找为特定设备优化的 DNN，并提供性能保证。具有多个分支的 DNN（例如 Proxyless-NAS、MNasNet 等）通常需要在 GPU
    和 CPU 上进行昂贵的内核启动和同步。
- en: '5.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Knowledge distillation should be used with small or medium-sized datasets. This
    is because fewer assumptions on the DNN architectures of the student and the teacher
    are required, leading to higher accuracy.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识蒸馏应与小型或中型数据集一起使用。这是因为对学生和教师的 DNN 架构的假设较少，从而提高了准确性。
- en: VI-B Evaluation Metrics
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 评估指标
- en: Low-power DNNs for computer vision need to be evaluated on more aspects beyond
    just accuracy. We list some of the major metrics that should be considered.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的低功耗 DNN 需要在准确性之外的更多方面进行评估。我们列出了应该考虑的一些主要指标。
- en: '1.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The test accuracy should be evaluated on large datasets like ImageNet, CIFAR,
    COCO, etc. K-folds cross-validation is necessary if the training dataset is small.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试准确性应在大型数据集上进行评估，如 ImageNet、CIFAR、COCO 等。如果训练数据集较小，则需要进行 K 折交叉验证。
- en: '2.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The number of parameters is generally associated with the memory requirement
    of the DNN. It is important to compare both these metrics when working with quantization
    and pruning techniques.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数数量通常与 DNN 的内存需求相关。使用量化和剪枝技术时，比较这两个指标很重要。
- en: '3.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The number of operations should be evaluated to find the computation costs.
    When using low precision DNNs, the cost of each operation reduces. In this case,
    it is important to also measure the energy consumption.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应评估操作数量以确定计算成本。使用低精度 DNN 时，每个操作的成本会降低。在这种情况下，还需测量能耗。
- en: '4.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: The number of parameters and operations are not always proportional to the energy
    consumption of the DNN [[10](#bib.bibx10)]. To find the energy consumption, DNNs
    should be deployed on a device connected to a power meter.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数和操作的数量并不总是与 DNN 的能耗成正比[[10](#bib.bibx10)]。要找到能耗，应将 DNN 部署在连接到功率计的设备上。
- en: VII Summary And Conclusions
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 总结与结论
- en: 'DNNs are powerful tools for performing many computer vision tasks. However,
    because the best-performing (most accurate) DNNs are designed for situations where
    computational resources are readily available, they are difficult to deploy on
    embedded and mobile devices. There has been significant research that focuses
    on reducing the energy consumption of these DNNs with minimal accuracy loss to
    make them better suited for low-power devices. This survey paper investigates
    the research landscape for low-power computer vision and identifies four categories
    of techniques: Quantization and Pruning, Filter Compression and Matrix Factorization,
    Network Architecture Search, and Knowledge Distillation. These techniques have
    their own strengths and weaknesses, with no clear winner. Continued research on
    improving the state-of-the-art low-power techniques will make computer vision
    deployable on embedded and mobile devices in the future.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 是执行许多计算机视觉任务的强大工具。然而，由于性能最佳（最准确）的 DNN 是为计算资源充足的情况设计的，因此很难在嵌入式和移动设备上部署。已经有大量研究集中在减少这些
    DNN 的能耗，以最小的准确性损失，使其更适合低功耗设备。本文综述了低功耗计算机视觉的研究现状，并识别出四类技术：量化与剪枝、滤波器压缩与矩阵分解、网络架构搜索和知识蒸馏。这些技术各有优缺点，没有明显的胜者。对最先进的低功耗技术的持续研究将使计算机视觉能够在未来嵌入式和移动设备上部署。
- en: References
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Z. Zhao al. “Object Detection With Deep Learning: A Review” In *IEEE TNNLS*
    30.11, 2019, pp. 3212–3232'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Z. Zhao 等人. “使用深度学习进行物体检测：综述” 见 *IEEE TNNLS* 30.11, 2019, pp. 3212–3232'
- en: '[2] X. Liu al. “Recent progress in semantic image segmentation” In *arXiv:1809.10198*,
    2018'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] X. Liu 等人. “语义图像分割的近期进展” 见 *arXiv:1809.10198*, 2018'
- en: '[3] K. Simonyan al. “Very Deep Convolutional Networks for Large-Scale Image
    Recognition” In *arXiv:1409.1556*'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] K. Simonyan 等人. “用于大规模图像识别的超深卷积网络” 见 *arXiv:1409.1556*'
- en: '[4] S. Han al. “Deep Compression: Compressing Deep Neural Networks with Pruning,
    Trained Quantization and Huffman Coding” In *arXiv:1510.00149*, 2016'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Han 等人. “深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络” 见 *arXiv:1510.00149*, 2016'
- en: '[5] J. Redmon al. “You Only Look Once: Unified, Real-Time Object Detection”
    In *2016 IEEE CVPR*'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Redmon 等人. “你只看一次：统一的实时物体检测” 见 *2016 IEEE CVPR*'
- en: '[6] D. T. Nguyen al. “A High-Throughput and Power-Efficient FPGA Implementation
    of YOLO CNN for Object Detection” In *2019 IEEE Transactions on VLSI Systems*'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. T. Nguyen 等人。“YOLO CNN 对象检测的高吞吐量和节能 FPGA 实现” 发表在 *2019 IEEE VLSI 系统期刊*'
- en: '[7] A. Mohan et al. “Internet of Video Things in 2030: A World with Many Cameras”
    In *2017 IEEE ISCA*'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Mohan 等人。“2030 年的视频物联网：一个充满摄像头的世界” 发表在 *2017 IEEE ISCA*'
- en: '[8] K. Kumar al. “Cloud Computing for Mobile Users: Can Offloading Computation
    Save Energy?” In *2010 Computer*'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] K. Kumar 等人。“移动用户的云计算：卸载计算能节省能量吗？” 发表在 *2010 计算机*'
- en: '[9] S. Anup al. “Visual Positioning System for Automated Indoor/Outdoor Navigation”
    In *2017 IEEE TENCON*'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Anup 等人。“用于自动室内/室外导航的视觉定位系统” 发表在 *2017 IEEE TENCON*'
- en: '[10] S. Alyamkin et al. “Low-Power Computer Vision: Status, Challenges, and
    Opportunities” In *2019 IEEE JETCAS*'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. Alyamkin 等人。“低功耗计算机视觉：现状、挑战和机遇” 发表在 *2019 IEEE JETCAS*'
- en: '[11] R. Ding et al. “FLightNNs: Lightweight Quantized Deep Neural Networks
    for Fast and Accurate Inference” In *2019 ACM DAC*'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] R. Ding 等人。“FLightNNs：用于快速和准确推断的轻量化量化深度神经网络” 发表在 *2019 ACM DAC*'
- en: '[12] N. Wang et al. “Training Deep Neural Networks with 8-bit Floating Point
    Numbers” In *2018 NeurIPS*, pp. 7675–7684'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] N. Wang 等人。“使用 8 位浮点数训练深度神经网络” 发表在 *2018 NeurIPS*，第 7675–7684 页'
- en: '[13] M. Courbariaux al. “Training Deep Neural Networks with Low Precision Multiplications”
    In *arXiv:1412.7024*, 2015'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. Courbariaux 等人。“使用低精度乘法训练深度神经网络” 发表在 *arXiv:1412.7024*，2015'
- en: '[14] R. Ding et al. “LightNN: Filling the Gap between Conventional Deep Neural
    Networks and Binarized Networks” In *2017 GGVLSI*, pp. 35–40'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] R. Ding 等人。“LightNN：填补传统深度神经网络与二值化网络之间的差距” 发表在 *2017 GGVLSI*，第 35–40 页'
- en: '[15] A. Goel al. “CompactNet: High Accuracy Deep Neural Network Optimized for
    On-Chip Implementation” In *2018 IEEE Big Data*'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. Goel 等人。“CompactNet：为片上实现优化的高精度深度神经网络” 发表在 *2018 IEEE Big Data*'
- en: '[16] B. Moons, Koen Goetschalckx, Nick Van Berckelaer and Marian Verhelst “Minimum
    Energy Quantized Neural Networks” In *arXiv:1711.00215*, 2017'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] B. Moons, Koen Goetschalckx, Nick Van Berckelaer 和 Marian Verhelst “最小能量量化神经网络”
    发表在 *arXiv:1711.00215*，2017'
- en: '[17] M. Courbariaux et al. “Binarized Neural Networks: Training Deep Neural
    Networks with Weights and Activations Constrained to +1 or -1” In *arXiv:1602.02830
    [cs]*, 2016'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] M. Courbariaux 等人。“二值化神经网络：训练权重和激活值受限于 +1 或 -1 的深度神经网络” 发表在 *arXiv:1602.02830
    [cs]*，2016'
- en: '[18] M. Rastegari, Vicente Ordonez, Joseph Redmon and Ali Farhadi “XNOR-Net:
    ImageNet Classification Using Binary Convolutional Neural Networks” In *arXiv:1603.05279*,
    2016'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. Rastegari, Vicente Ordonez, Joseph Redmon 和 Ali Farhadi “XNOR-Net：使用二值化卷积神经网络进行
    ImageNet 分类” 发表在 *arXiv:1603.05279*，2016'
- en: '[19] S. Zhou et al. “DoReFa-Net: Training Low Bitwidth Convolutional Neural
    Networks with Low Bitwidth Gradients” In *arXiv:1606.06160*, 2018'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. Zhou 等人。“DoReFa-Net：使用低位宽梯度训练低位宽卷积神经网络” 发表在 *arXiv:1606.06160*，2018'
- en: '[20] W. Chen et al. “Compressing Neural Networks with the Hashing Trick” In
    *arXiv:1504.04788*, 2015'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] W. Chen 等人。“使用哈希技巧压缩神经网络” 发表在 *arXiv:1504.04788*，2015'
- en: '[21] P. Merolla al. “Deep Neural Networks are Robust to Weight Binarization
    and Other Non-Linear Distortions” In *arXiv:1606.01981*, 2016'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] P. Merolla 等人。“深度神经网络对权重二值化和其他非线性失真具有鲁棒性” 发表在 *arXiv:1606.01981*，2016'
- en: '[22] Y. Choi al. “Towards the Limit of Network Quantization” In *arXiv:1612.01543*,
    2017'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. Choi 等人。“网络量化的极限” 发表在 *arXiv:1612.01543*，2017'
- en: '[23] Y. LeCun al. “Optimal Brain Damage” In *1990 NeurIPS*'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. LeCun 等人。“最佳脑损伤” 发表在 *1990 NeurIPS*'
- en: '[24] B. Hassibi al. “Optimal Brain Surgeon and General Network Pruning” In
    *1993 IEEE Intl. Conference on Neural Networks*'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] B. Hassibi 等人。“最佳脑外科医生和一般网络剪枝” 发表在 *1993 IEEE 神经网络国际会议*'
- en: '[25] S. Anwar al. “Structured Pruning of Deep Convolutional Neural Networks”
    In *arXiv:1512.08571*, 2015'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Anwar 等人。“深度卷积神经网络的结构化剪枝” 发表在 *arXiv:1512.08571*，2015'
- en: '[26] A. Polyak al. “Channel-Level Acceleration of Deep Face Representations”
    In *2015 IEEE Access*, pp. 2163–2175'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Polyak 等人。“深度人脸表示的通道级加速” 发表在 *2015 IEEE Access*，第 2163–2175 页'
- en: '[27] S. Han al. “Learning both Weights and Connections for Efficient Neural
    Network” In *2015 NeurIPS*, pp. 1135–1143'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Han 等人。“学习权重和连接以实现高效神经网络” 发表在 *2015 NeurIPS*，第 1135–1143 页'
- en: '[28] R. Yu et al. “NISP: Pruning Networks Using Neuron Importance Score Propagation”
    In *2018 IEEE CVPR*'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. Yu 等人。“NISP：使用神经元重要性评分传播修剪网络” 发表在 *2018 IEEE CVPR*'
- en: '[29] V. Peluso al. “Scalable-Effort ConvNets for Multilevel Classification”
    In *2018 IEEE/ACM ICCAD*, pp. 1–8'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] V. Peluso 等人。“用于多层分类的可扩展卷积网络” 发表在 *2018 IEEE/ACM ICCAD*，第 1–8 页'
- en: '[30] J. Deng al. “Fast and Balanced: Efficient Label Tree Learning for Large
    Scale Object Recognition” In *2011 NeurIPS*'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Deng 等. “快速和平衡：大规模对象识别的高效标签树学习” 见 *2011 NeurIPS*'
- en: '[31] H. Li al. “Pruning Filters for Efficient ConvNets” In *arXiv:1608.08710
    [cs]*, 2016'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] H. Li 等. “为了高效的卷积网络修剪滤波器” 见 *arXiv:1608.08710 [cs]*, 2016'
- en: '[32] H. Zhou al. “Less Is More: Towards Compact CNNs” In *2016 ECCV*, pp. 662–677'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] H. Zhou 等. “少即是多：面向紧凑型 CNN” 见 *2016 ECCV*, 第662–677页'
- en: '[33] W. Wen et al. “Learning Structured Sparsity in Deep Neural Networks” In
    *2016 NeurIPS*, pp. 2074–2082'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] W. Wen 等. “在深度神经网络中学习结构化稀疏性” 见 *2016 NeurIPS*, 第2074–2082页'
- en: '[34] A. Krizhevsky al. “ImageNet Classification with Deep Convolutional Neural
    Networks” In *2012 NeurIPS*'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. Krizhevsky 等. “使用深度卷积神经网络进行 ImageNet 分类” 见 *2012 NeurIPS*'
- en: '[35] F. N. Iandola et al. “SqueezeNet: AlexNet-level accuracy with 50x fewer
    parameters and <0.5MB model size” In *arXiv:1602.07360*, 2016'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] F. N. Iandola 等. “SqueezeNet：AlexNet 级别的准确性，参数减少 50 倍，模型大小小于 0.5MB” 见
    *arXiv:1602.07360*, 2016'
- en: '[36] A Howard et al. “Searching for MobileNetV3” In *arXiv:1905.02244*, 2019'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] A Howard 等. “寻找 MobileNetV3” 见 *arXiv:1905.02244*, 2019'
- en: '[37] B. Wu et al. “Shift: A Zero FLOP, Zero Parameter Alternative to Spatial
    Convolutions” In *arXiv:1711.08141*, 2017'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] B. Wu 等. “Shift：一种零 FLOP、零参数的空间卷积替代方案” 见 *arXiv:1711.08141*, 2017'
- en: '[38] G. B. Hacene et al. “Attention Based Pruning for Shift Networks” In *arXiv:1905.12300*,
    2019'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] G. B. Hacene 等. “基于注意力的 Shift 网络修剪” 见 *arXiv:1905.12300*, 2019'
- en: '[39] H. Li al. “Multi-Bias Non-linear Activation in Deep Neural Networks” In
    *arXiv:1604.00676*, 2016'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] H. Li 等. “深度神经网络中的多偏差非线性激活” 见 *arXiv:1604.00676*, 2016'
- en: '[40] X. Zhang al. “ShuffleNet: An Extremely Efficient Convolutional Neural
    Network for Mobile Devices” In *arXiv:1707.01083*, 2017'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] X. Zhang 等. “ShuffleNet：一种极其高效的移动设备卷积神经网络” 见 *arXiv:1707.01083*, 2017'
- en: '[41] M. Jaderberg al. “Speeding up Convolutional Neural Networks with Low Rank
    Expansions” In *arXiv:1405.3866*, 2014'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. Jaderberg 等. “通过低秩扩展加速卷积神经网络” 见 *arXiv:1405.3866*, 2014'
- en: '[42] E. Denton al. “Exploiting Linear Structure Within Convolutional Networks
    for Efficient Evaluation” In *2014 NeurIPS*'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] E. Denton 等. “利用卷积网络中的线性结构进行高效评估” 见 *2014 NeurIPS*'
- en: '[43] T.G. Kolda al. “Tensor Decompositions and Applications” In *SIAM Review*,
    2009, pp. 455–500'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] T.G. Kolda 等. “张量分解及其应用” 见 *SIAM Review*, 2009, 第455–500页'
- en: '[44] K. Hayashi, Taiki Yamaguchi, Yohei Sugawara and Shin-ichi Maeda “Exploring
    Unexplored Tensor Network Decompositions for Convolutional Neural Networks” In
    *2019 NeurIPS*, pp. 5553–5563'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] K. Hayashi, Taiki Yamaguchi, Yohei Sugawara 和 Shin-ichi Maeda “探索卷积神经网络中的未探索张量网络分解”
    见 *2019 NeurIPS*, 第5553–5563页'
- en: '[45] C. Tai et al. “Convolutional Neural Networks with Low-Rank Regularization”
    In *arXiv:1511.06067*, 2016'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C. Tai 等. “具有低秩正则化的卷积神经网络” 见 *arXiv:1511.06067*, 2016'
- en: '[46] B. Zoph al. “Learning Transferable Architectures for Scalable Image Recognition”
    In *arXiv:1707.07012*, 2018'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] B. Zoph 等. “学习可迁移架构以实现可扩展的图像识别” 见 *arXiv:1707.07012*, 2018'
- en: '[47] E. Real al. “Regularized Evolution for Image Classifier Architecture Search”
    In *arXiv:1802.01548*, 2019'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] E. Real 等. “图像分类器架构搜索的正则化进化” 见 *arXiv:1802.01548*, 2019'
- en: '[48] M. Tan al. “MnasNet: Platform-Aware Neural Architecture Search for Mobile”
    In *arXiv:1807.11626*, 2019'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] M. Tan 等. “MnasNet：面向移动的平台感知神经网络架构搜索” 见 *arXiv:1807.11626*, 2019'
- en: '[49] B. Wu al. “FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable
    Neural Architecture Search” In *arXiv:1812.03443*, 2019'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] B. Wu 等. “FBNet：通过可微分神经网络架构搜索进行硬件感知的高效卷积网络设计” 见 *arXiv:1812.03443*, 2019'
- en: '[50] H. Cai al. “ProxylessNAS: Direct Neural Architecture Search on Target
    Task and Hardware” In *arXiv:1812.00332*, 2019'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] H. Cai 等. “ProxylessNAS：在目标任务和硬件上直接进行神经网络架构搜索” 见 *arXiv:1812.00332*, 2019'
- en: '[51] D. Stamoulis al. “Single-Path NAS: Designing Hardware-Efficient ConvNets
    in less than 4 Hours” In *arXiv:1904.02877*, 2019'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] D. Stamoulis 等. “单路径 NAS：在不到 4 小时内设计硬件高效的卷积网络” 见 *arXiv:1904.02877*, 2019'
- en: '[52] Y. Bengio al. “Representation Learning: A Review and New Perspectives”
    In *arXiv:1206.5538*, 2014'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Bengio 等. “表示学习：综述与新视角” 见 *arXiv:1206.5538*, 2014'
- en: '[53] J. Ba al. “Do Deep Nets Really Need to be Deep?” In *2014 NeurIPS*'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Ba 等. “深度网络真的需要深度吗？” 见 *2014 NeurIPS*'
- en: '[54] C. Bucilua al. “Model Compression” In *2006 ACM Intl. Conference on Knowledge
    Discovery and Data Mining*'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] C. Bucilua 等. “模型压缩” 见 *2006 ACM 国际知识发现与数据挖掘大会*'
- en: '[55] J. H.Cho al. “On the Efficacy of Knowledge Distillation” In *arXiv:1910.01348*,
    2019'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. H. Cho 等. “知识蒸馏的有效性” 见 *arXiv:1910.01348*, 2019'
- en: '[56] G. Hinton al. “Distilling the Knowledge in a Neural Network” In *arXiv:1503.02531*,
    2015'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] G. Hinton 等. “提取神经网络中的知识” 见 *arXiv:1503.02531*, 2015'
- en: '[57] Q. Li al. “Mimicking Very Efficient Network for Object Detection” In *2017
    IEEE CVPR*, pp. 7341–7349'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Q. Li 等人。“模仿非常高效的目标检测网络”发表于 *2017 IEEE CVPR*，第7341–7349页'
- en: '[58] A. Romero al. “FitNets: Hints for Thin Deep Nets” In *arXiv:1412.6550*,
    2015'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] A. Romero 等人。“FitNets：细薄深度网络的提示”发表于 *arXiv:1412.6550*，2015年'
- en: '[59] B. Peng al. “Correlation Congruence for Knowledge Distillation” In *2019
    ICCV*'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] B. Peng 等人。“用于知识蒸馏的相关性一致性”发表于 *2019 ICCV*'
