- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:57:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:57:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2101.01993] A Survey of Deep RL and IL for Autonomous Driving Policy Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2101.01993] 关于深度强化学习和深度模仿学习在自动驾驶策略学习中的调研'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2101.01993](https://ar5iv.labs.arxiv.org/html/2101.01993)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2101.01993](https://ar5iv.labs.arxiv.org/html/2101.01993)
- en: A Survey of Deep RL and IL for Autonomous Driving Policy Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于深度强化学习和深度模仿学习在自动驾驶策略学习中的调研
- en: 'Zeyu Zhu,  Huijing Zhao This work is supported in part by the National Natural
    Science Foundation of China under Grant (61973004). The authors are with the Key
    Laboratory of Machine Perception (MOE) and with the School of Electronics Engineering
    and Computer Science, Peking University, Beijing 100871, China. Correspondence:
    H.Zhao, zhaohj@pku.edu.cn.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 朱泽宇，赵惠晶 这项工作部分由中国国家自然科学基金资助（61973004）。作者隶属于北京大学电子工程与计算机科学学院及机器感知重点实验室，地址：北京市
    100871，中国。联系方式：H.Zhao, zhaohj@pku.edu.cn。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Autonomous driving (AD) agents generate driving policies based on online perception
    results, which are obtained at multiple levels of abstraction, e.g., behavior
    planning, motion planning and control. Driving policies are crucial to the realization
    of safe, efficient and harmonious driving behaviors, where AD agents still face
    substantial challenges in complex scenarios. Due to their successful application
    in fields such as robotics and video games, the use of deep reinforcement learning
    (DRL) and deep imitation learning (DIL) techniques to derive AD policies have
    witnessed vast research efforts in recent years. This paper is a comprehensive
    survey of this body of work, which is conducted at three levels: First, a taxonomy
    of the literature studies is constructed from the system perspective, among which
    five modes of integration of DRL/DIL models into an AD architecture are identified.
    Second, the formulations of DRL/DIL models for conducting specified AD tasks are
    comprehensively reviewed, where various designs on the model state and action
    spaces and the reinforcement learning rewards are covered. Finally, an in-depth
    review is conducted on how the critical issues of AD applications regarding driving
    safety, interaction with other traffic participants and uncertainty of the environment
    are addressed by the DRL/DIL models. To the best of our knowledge, this is the
    first survey to focus on AD policy learning using DRL/DIL, which is addressed
    simultaneously from the system, task-driven and problem-driven perspectives. We
    share and discuss findings, which may lead to the investigation of various topics
    in the future.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶（AD）代理基于在线感知结果生成驾驶策略，这些结果在多个抽象层次上获得，例如行为规划、运动规划和控制。驾驶策略对于实现安全、高效和和谐的驾驶行为至关重要，而AD代理在复杂场景中仍面临重大挑战。由于深度强化学习（DRL）和深度模仿学习（DIL）技术在机器人技术和视频游戏等领域的成功应用，近年来在推导AD策略方面进行了大量研究。本文对这方面的工作进行了全面的调研，分三个层次进行：首先，从系统角度构建文献研究的分类体系，识别出DRL/DIL模型在AD架构中的五种集成模式。其次，全面回顾了DRL/DIL模型用于特定AD任务的公式，其中涵盖了模型状态和动作空间以及强化学习奖励的各种设计。最后，深入回顾了DRL/DIL模型如何解决与驾驶安全、与其他交通参与者的互动以及环境不确定性相关的关键问题。据我们所知，这是首个同时从系统、任务驱动和问题驱动的角度关注AD策略学习的调研。我们分享并讨论了这些发现，这可能会引发未来的各种研究话题。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: deep reinforcement learning, deep imitation learning, autonomous driving policy
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习，深度模仿学习，自动驾驶策略
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Autonoumous driving (AD) has received extensive attention in recent decades
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)] and could be
    a promising solution for improving road safety [[5](#bib.bib5)], traffic flow
    [[6](#bib.bib6)] and fuel economy [[7](#bib.bib7)], among other factors. A typical
    architecture of an AD system is illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"), which is
    composed of perception, planning and control modules. An AD agent generates driving
    policies based on online perception results, which are obtained at multiple levels
    of abstraction, e.g., behavior planning, motion planning and control. The earliest
    autonomous vehicles can be dated back to [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)].
    One milestone was the Defense Advanced Research Projects Agency (DARPA) Grand
    Challenges [[11](#bib.bib11), [12](#bib.bib12)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶（AD）在最近几十年受到了广泛关注[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)]，并可能成为提高道路安全[[5](#bib.bib5)]、交通流量[[6](#bib.bib6)]和燃油经济性[[7](#bib.bib7)]等因素的有前途的解决方案。图
    [1](#S1.F1 "图 1 ‣ 引言 ‣ 深度强化学习和模仿学习在自动驾驶策略学习中的应用综述")中展示了一个典型的AD系统架构，该架构由感知、规划和控制模块组成。AD代理基于在线感知结果生成驾驶策略，这些结果在多个抽象层次上获得，例如行为规划、运动规划和控制。最早的自动驾驶车辆可以追溯到[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10)]。一个里程碑是国防高级研究计划局（DARPA）大挑战[[11](#bib.bib11),
    [12](#bib.bib12)]。
- en: '![Refer to caption](img/ec6dd15ea92264e4b6e86b955c0d57ce.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec6dd15ea92264e4b6e86b955c0d57ce.png)'
- en: 'Figure 1: Architecture of autonomous driving systems. A general abstraction
    that is based on [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：自动驾驶系统的架构。基于[[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]的通用抽象。
- en: 'Recent years have witnessed a huge boost in AD research, and many products
    and prototyping systems have been developed. Despite the fast development of the
    field, AD still faces substantial challenges in complex scenarios for the realization
    of safe, efficient and harmonious driving behaviors [[17](#bib.bib17), [18](#bib.bib18)].
    Reinforcement learning (RL) is a principled mathematical framework for solving
    sequential decision making problems [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)].
    Imitation learning (IL), which is closely related, refers to learning from expert
    demonstrations. However, the early methods of both were limited to relatively
    low-dimensional problems. The rise of deep learning (DL) techniques [[22](#bib.bib22),
    [23](#bib.bib23)] in recent years has provided powerful solutions to this problem
    through the appealing properties of deep neural networks (DNNs): function approximation
    and representation learning. DL techniques enable the scaling of RL/IL to previously
    intractable problems (e.g., high-dimensional state spaces), which have increased
    in popularity for complex locomotion [[24](#bib.bib24)], robotics [[25](#bib.bib25)]
    and autonomous driving [[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)]
    tasks. Unless otherwise stated, this survey focuses on Deep RL (DRL) and Deep
    IL (DIL).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，自动驾驶（AD）研究取得了巨大进展，许多产品和原型系统也得到了开发。尽管该领域发展迅速，自动驾驶在复杂场景中仍面临实现安全、高效和协调驾驶行为的重大挑战[[17](#bib.bib17),
    [18](#bib.bib18)]。强化学习（RL）是解决序列决策问题的原则性数学框架[[19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21)]。模仿学习（IL），其与之密切相关，指的是从专家示范中学习。然而，早期的方法都局限于相对低维的问题。近年来，深度学习（DL）技术[[22](#bib.bib22),
    [23](#bib.bib23)]通过深度神经网络（DNNs）的诱人特性：函数逼近和表示学习，为这一问题提供了强有力的解决方案。DL技术使得RL/IL能够扩展到以前无法处理的问题（例如，高维状态空间），这在复杂的运动[[24](#bib.bib24)]、机器人技术[[25](#bib.bib25)]和自动驾驶[[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28)]任务中越来越受欢迎。除非另有说明，本调查重点关注深度强化学习（DRL）和深度模仿学习（DIL）。
- en: 'A large variety of DRL/DIL models have been developed for learning AD policies,
    which are reviewed in this paper. Several surveys are relevant to this study.
    [[13](#bib.bib13), [15](#bib.bib15)] survey the motion planning and control methods
    of automated vehicles before the era of DL. [[29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33)] review general DRL/DIL methods
    without considering any particular applications. [[4](#bib.bib4)] addresses the
    deep learning techniques for AD with a focus on perception and control, while
    [[34](#bib.bib34)] addresses control only. [[35](#bib.bib35)] provides a taxonomy
    of AD tasks to which DRL models have been applied and highlights the key challenges.
    However, none of these studies answers the following questions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文回顾了用于学习自动驾驶（AD）策略的多种DRL/DIL模型。一些相关的综述文章如[[13](#bib.bib13)、[15](#bib.bib15)]在深度学习（DL）时代之前，回顾了自动驾驶车辆的运动规划和控制方法。[[29](#bib.bib29)、[30](#bib.bib30)、[31](#bib.bib31)、[32](#bib.bib32)、[33](#bib.bib33)]回顾了通用的DRL/DIL方法，而未考虑任何特定应用。[[4](#bib.bib4)]关注于自动驾驶的深度学习技术，侧重于感知和控制，而[[34](#bib.bib34)]仅涉及控制。[[35](#bib.bib35)]提供了DRL模型应用于自动驾驶任务的分类，并突出了关键挑战。然而，这些研究中没有回答以下问题：
- en: How can DRL/DIL models be integrated into AD systems from the perspective of
    system architecture? How can they be formulated to accomplish specified AD tasks?
    How can methods be designed that address the challenging issues of AD, such as
    safety, interaction with other traffic participants, and uncertainty of the environment?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如何从系统架构的角度将DRL/DIL模型整合到自动驾驶系统中？它们如何被制定以完成特定的自动驾驶任务？如何设计方法来解决自动驾驶中的挑战性问题，如安全性、与其他交通参与者的互动以及环境的不确定性？
- en: '![Refer to caption](img/d0241f3bfb377a3397cc0339b9791d01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d0241f3bfb377a3397cc0339b9791d01.png)'
- en: 'Figure 2: A taxonomy of the general methods of reinforcement learning (RL)
    and imitation learning (IL)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：强化学习（RL）和模仿学习（IL）一般方法的分类
- en: 'This study seeks answers to the above questions. To the best of our knowledge,
    this is the first survey to focus on AD policy learning using DRL/DIL, which is
    addressed from the system, task-driven and problem-driven perspectives. Our contributions
    are threefold:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究寻求上述问题的答案。据我们所知，这是第一篇专注于使用DRL/DIL进行自动驾驶策略学习的综述，涉及系统、任务驱动和问题驱动的视角。我们的贡献有三方面：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A taxonomy of the literature is presented from the system perspective, from
    which five modes of integration of DRL/DIL models into an AD architecture are
    identified. The studies on each mode are reviewed, and the architectures are compared.
    It is found that the vast research efforts have focused mainly on exploring the
    potential of DRL/DIL in accomplishing AD tasks, while intensive studies are needed
    on the optimization of the architectures of DRL/DIL embedded systems toward real-world
    applications.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从系统角度出发，本文提供了文献的分类，并识别了将DRL/DIL模型整合到自动驾驶架构中的五种模式。对每种模式的研究进行了回顾，并对这些架构进行了比较。研究发现，广泛的研究工作主要集中在探索DRL/DIL在完成自动驾驶任务中的潜力上，而对DRL/DIL嵌入系统架构优化的深入研究对于实际应用仍然非常必要。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The formulations of DRL/DIL models for accomplishing specified AD tasks are
    comprehensively reviewed, where various designs on the model state and action
    spaces and the reinforcement learning rewards are covered. It is found that these
    formulations rely heavily on empirical designs, which are brute-force approaches
    and lack theoretic support. Changing the designs or tuning the parameters could
    result in substantially different driving policies, which may pose large challenges
    to the AD system’s stability and robustness in real-world deployment.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于完成特定自动驾驶任务的DRL/DIL模型的制定进行了全面回顾，涵盖了模型状态和动作空间以及强化学习奖励的各种设计。发现这些制定在很大程度上依赖于经验设计，这些方法是暴力破解的方法，并缺乏理论支持。改变设计或调整参数可能会导致实质上不同的驾驶策略，这可能对自动驾驶系统在实际部署中的稳定性和鲁棒性构成较大挑战。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The critical issues of AD applications that are addressed by the DRL/DIL models
    regarding driving safety, interaction with other traffic participants and uncertainty
    of the environment are comprehensively discussed. It is found that driving safety
    has been well studied. A typical strategy is to combine with traditional methods
    to ensure a DRL/DIL agent’s functional safety; however, striking a balance between
    optimal policies and hard constraints remains non-trivial. The studies on the
    latter two issues are still highly preliminary, in which the problems have been
    addressed from divergent perspectives and the studies have not been conducted
    systematically.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合讨论了DRL/DIL模型在关于驾驶安全性、与其他交通参与者的交互和环境的不确定性方面涉及到的自动驾驶应用的关键问题。研究发现，驾驶安全性已经得到了广泛研究。一个典型的策略是与传统方法结合，以确保DRL/DIL代理的功能安全；然而，在最优策略和硬约束之间取得平衡仍然是一个非常困难的问题。对后两个问题的研究仍然处于高度初步阶段，其中问题已经从不同的角度得到解决，但研究还没有系统地进行。
- en: 'The remainder of this paper is organized as follows: Sections [II](#S2 "II
    Preliminaries of Reinforcement Learning ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning") and [III](#S3 "III Preliminaries of Imitation Learning
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning") briefly
    introduce (D)RL and (D)IL, respectively. Section [IV](#S4 "IV Architectures of
    DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning") reviews the research on DRL/DIL in AD from a system architecture
    perspective. Section [V](#S5 "V Task-Driven Methods ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning") reviews the task-driven methods and
    examines the formulations of DRL/DIL models for completing specified AD tasks.
    Section [VI](#S6 "VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning") reviews problem-driven methods, in which specified autonomous
    vehicle problems and challenges are addressed. Section [VII](#S7 "VII Discussion
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning") discusses
    the remaining challenges, and the conclusions of the survey are presented in Section
    [VIII](#S8 "VIII Conclusions ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分安排如下：[第II节](#S2)和[第III节](#S3)分别简要介绍了(D)RL和(D)IL。[第IV节](#S4)从系统体系结构的角度回顾了自动驾驶(DRL/DIL)方面的研究。[第V节](#S5)回顾了任务驱动方法，并考察了完成指定自动驾驶任务的DRL/DIL模型的构造。[第VI节](#S6)回顾了问题驱动方法，其中解决了指定的自动驾驶问题和挑战。[第VII节](#S7)讨论了剩余的挑战，并在[第VIII节](#S8)中总结了调查的结论。
- en: II Preliminaries of Reinforcement Learning
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 强化学习基础
- en: II-A Problem Formulation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 问题形式化
- en: Reinforcement learning (RL) is a principled mathematical framework that is based
    upon the paradigm of trial-and-error learning, where an agent interacts with its
    environment through a trade-off between exploitation and exploration [[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38)]. Markov decision processes (MDPs) are a mathematically
    idealized form of RL [[21](#bib.bib21)], which are represented as $(\cal S,\cal
    A,\cal P,\cal R,\gamma)$, where $\cal S$ and $\cal A$ denote the sets of states
    and actions, respectively, and $\cal P$$(s_{t+1}|s_{t},a_{t}):\cal S\times\cal
    S\times\cal A\rightarrow$ $[0,1]$ is the transition/dynamics function that maps
    state-action pairs onto a distribution of next-step states. The numerical immediate
    reward function $\cal R$$(s_{t},a_{t},s_{t+1}):\cal S\times\cal A\times\cal S\rightarrow\mathbb{R}$
    serves as a learning signal. A discount factor $\gamma\in[0,1]$ determines the
    present value of future rewards (lower values encourage more myopic behaviors).
    MDPs’ states satisfy the Markov property [[21](#bib.bib21)], namely, future states
    depend only on the immediately preceding states and actions. Partially observable
    MDPs (POMDPs) extend MDPs to problems in which access to fully observable Markov
    property states is not available. A POMDP has an observation set $\Omega$ and
    an observation function $\cal O$, where $\cal O$$(a_{t},s_{t+1},o_{t+1})=p(o_{t+1}|a_{t},s_{t+1})$
    is the probability of observing $o_{t+1}$ given that the agent has executed action
    $a_{t}$ and reached state $s_{t+1}$ [[39](#bib.bib39)]. For the theory and algorithms
    of POMDPs, we refer readers to [[40](#bib.bib40), [39](#bib.bib39)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一个基于试错学习范式的原则性数学框架，其中代理通过利用和探索之间的权衡与环境进行互动 [[36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38)]。马尔可夫决策过程（MDP）是 RL 的一种数学理想化形式 [[21](#bib.bib21)]，其表示为 $(\cal
    S,\cal A,\cal P,\cal R,\gamma)$，其中 $\cal S$ 和 $\cal A$ 分别表示状态集和动作集，$\cal P$$(s_{t+1}|s_{t},a_{t}):\cal
    S\times\cal S\times\cal A\rightarrow$ $[0,1]$ 是将状态-动作对映射到下一步状态分布的转移/动态函数。数值即时奖励函数
    $\cal R$$(s_{t},a_{t},s_{t+1}):\cal S\times\cal A\times\cal S\rightarrow\mathbb{R}$
    作为学习信号。折扣因子 $\gamma\in[0,1]$ 确定未来奖励的现值（较低值鼓励更多短视行为）。MDP 的状态满足马尔可夫性质 [[21](#bib.bib21)]，即未来状态仅依赖于立即先前的状态和动作。部分可观察
    MDP（POMDP）将 MDP 扩展到无法完全访问马尔可夫性质状态的问题。POMDP 具有观察集 $\Omega$ 和观察函数 $\cal O$，其中 $\cal
    O$$(a_{t},s_{t+1},o_{t+1})=p(o_{t+1}|a_{t},s_{t+1})$ 是在代理执行动作 $a_{t}$ 并到达状态 $s_{t+1}$
    的情况下观察到 $o_{t+1}$ 的概率 [[39](#bib.bib39)]。有关 POMDP 的理论和算法，请参见 [[40](#bib.bib40),
    [39](#bib.bib39)]。
- en: 'The MDP agent selects an action $a_{t}\in\cal A$ at each time step $t$ based
    on the current state $s_{t}$, receives a numerical reward $r_{t+1}$ and visits
    a new state $s_{t+1}$. The generated sequence $\{s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...\}$
    is called a rollout or trajectory. The expected cumulative reward in the future,
    namely, the expected discounted return $G_{t}$ after time step $t$, is defined
    as [[21](#bib.bib21)]:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: MDP 代理在每个时间步 $t$ 基于当前状态 $s_{t}$ 选择一个动作 $a_{t}\in\cal A$，接收数值奖励 $r_{t+1}$ 并访问一个新状态
    $s_{t+1}$。生成的序列 $\{s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...\}$ 被称为展开或轨迹。未来的期望累计奖励，即时间步
    $t$ 后的期望折扣回报 $G_{t}$，定义为 [[21](#bib.bib21)]：
- en: '|  | $\displaystyle G_{t}\doteq r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+...=\sum_{k=0}^{T}\gamma^{k}r_{t+k+1}$
    |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{t}\doteq r_{t+1}+\gamma r_{t+2}+\gamma^{2}r_{t+3}+...=\sum_{k=0}^{T}\gamma^{k}r_{t+k+1}$
    |  | (1) |'
- en: 'where $T$ is a finite value or $\infty$ for finite and infinite horizon problems,
    respectively. The policy $\pi(a|s)$ maps states to probabilities of selecting
    each possible action. The value function $v_{\pi}(s)$ is the expected return following
    $\pi$ from state $s$:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$T$ 是一个有限值或 $\infty$，分别对应于有限和无限期问题。策略 $\pi(a|s)$ 将状态映射到选择每个可能动作的概率。价值函数 $v_{\pi}(s)$
    是从状态 $s$ 开始，跟随策略 $\pi$ 的期望回报：
- en: '|  | $\displaystyle v_{\pi}(s)\doteq\mathbb{E}_{\pi}[G_{t}&#124;s_{t}=s]$ |  |
    (2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v_{\pi}(s)\doteq\mathbb{E}_{\pi}[G_{t}&#124;s_{t}=s]$ |  |
    (2) |'
- en: 'Similarly, the action-value function $q_{\pi}(s,a)$ is defined as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，动作-价值函数 $q_{\pi}(s,a)$ 被定义为：
- en: '|  | $q_{\pi}(s,a)\doteq\mathbb{E}_{\pi}[G_{t}&#124;s_{t}=s,a_{t}=a]$ |  |
    (3) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $q_{\pi}(s,a)\doteq\mathbb{E}_{\pi}[G_{t}&#124;s_{t}=s,a_{t}=a]$ |  |
    (3) |'
- en: 'which satisfies the recursive Bellman equation [[41](#bib.bib41)] :'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这满足递归贝尔曼方程 [[41](#bib.bib41)]：
- en: '|  | $q_{\pi}(s_{t},a_{t})=\mathbb{E}_{s_{t+1}}[r_{t+1}+\gamma q_{\pi}(s_{t+1},\pi(s_{t+1}))]$
    |  | (4) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $q_{\pi}(s_{t},a_{t})=\mathbb{E}_{s_{t+1}}[r_{t+1}+\gamma q_{\pi}(s_{t+1},\pi(s_{t+1}))]$
    |  | (4) |'
- en: The objective of RL is to identify the optimal policy that maximizes the expected
    return $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\pi}[G_{t}|s_{t}=s]$. The methods can
    be divided into three groups, as shown in Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning") (a).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是识别最大化期望回报的最优策略 $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\pi}[G_{t}|s_{t}=s]$。这些方法可以分为三类，如图
    [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning") (a) 所示。
- en: II-B Value-based Methods
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 基于价值的方法
- en: To solve a reinforcement learning problem, one can identify an optimal action-value
    function and recover the optimal policy from the learned state-action values.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决强化学习问题，可以确定一个最优的动作-价值函数，并从学习到的状态-动作值中恢复最优策略。
- en: '|  | $\displaystyle q_{\pi^{*}}(s,a)=\max_{\pi}q_{\pi}(s,a)$ |  | (5) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q_{\pi^{*}}(s,a)=\max_{\pi}q_{\pi}(s,a)$ |  | (5) |'
- en: '|  | $\displaystyle{\pi^{*}}(s)=\arg\max_{a}q_{\pi^{*}}(s,a)$ |  | (6) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\pi^{*}}(s)=\arg\max_{a}q_{\pi^{*}}(s,a)$ |  | (6) |'
- en: 'Q-learning [[42](#bib.bib42)] is one of the most popular methods, which estimates
    Q values through temporal difference (TD):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning [[42](#bib.bib42)] 是最流行的方法之一，它通过时间差（TD）来估计 Q 值：
- en: '|  | $\displaystyle q_{\pi}(s_{t},a_{t})$ | $\displaystyle\leftarrow$ | $\displaystyle
    q_{\pi}(s_{t},a_{t})+\alpha(Y-q_{\pi}(s_{t},a_{t}))$ |  | (7) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q_{\pi}(s_{t},a_{t})$ | $\displaystyle\leftarrow$ | $\displaystyle
    q_{\pi}(s_{t},a_{t})+\alpha(Y-q_{\pi}(s_{t},a_{t}))$ |  | (7) |'
- en: where $\displaystyle Y=r_{t+1}+\gamma\max\limits_{a_{t+1}\in\cal A}q_{\pi}(s_{t+1},a_{t+1})$
    is the temporal difference target and $\alpha$ is the learning rate. This can
    be regarded as a standard regression problem in which the error to be minimized
    is $Y-q_{\pi}(s_{t},a_{t})$. Q-learning is off-policy since it updates $q_{\pi}$
    based on experiences that are not necessarily generated by the derived policy,
    while SARSA [[43](#bib.bib43)] is an on-policy algorithm that uses the derived
    policy to generate experiences. Another distinction is that SARSA uses target
    $Y=r_{t+1}+\gamma q_{\pi}(s_{t+1},a_{t+1})$. In contrast to TD methods, Monte
    Carlo methods estimate the expected return through averaging the results of multiple
    rollouts, which can be applied to non-Markovian episodic environments. TD and
    Monte Carlo have been further combined in TD($\lambda$) [[21](#bib.bib21)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\displaystyle Y=r_{t+1}+\gamma\max\limits_{a_{t+1}\in\cal A}q_{\pi}(s_{t+1},a_{t+1})$
    是时间差目标，$\alpha$ 是学习率。这可以看作是一个标准的回归问题，其中需要最小化的误差是 $Y-q_{\pi}(s_{t},a_{t})$。Q-learning
    是离策略的，因为它基于不一定由推导策略生成的经验来更新 $q_{\pi}$，而 SARSA [[43](#bib.bib43)] 是一种在策略算法，利用推导策略生成经验。另一个区别是
    SARSA 使用目标 $Y=r_{t+1}+\gamma q_{\pi}(s_{t+1},a_{t+1})$。与 TD 方法不同，蒙特卡罗方法通过平均多个回合的结果来估计期望回报，这可以应用于非马尔可夫回合环境。TD
    和蒙特卡罗方法在 TD($\lambda$) [[21](#bib.bib21)] 中进一步结合。
- en: 'The early methods [[42](#bib.bib42), [43](#bib.bib43), [21](#bib.bib21)] of
    this group rely on tabular representations. A major problem is the “curse of dimensionality”
    [[44](#bib.bib44)], namely, an increase in the number of state features would
    result in exponential growth of the number of state-action pairs that must be
    stored. Recent methods use DNNs to approximate a parameterized value function
    $q(s,a;\omega)$, of which Deep Q-networks (DQNs) [[45](#bib.bib45)] are the most
    representative, which learn the values by minimizing the following loss function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该小组的早期方法 [[42](#bib.bib42), [43](#bib.bib43), [21](#bib.bib21)] 依赖于表格表示。一个主要问题是“维度诅咒”
    [[44](#bib.bib44)]，即状态特征数量的增加会导致需要存储的状态-动作对数量呈指数增长。近期的方法使用深度神经网络（DNNs）来近似一个参数化的价值函数
    $q(s,a;\omega)$，其中深度 Q 网络（DQNs） [[45](#bib.bib45)] 是最具代表性的，这些网络通过最小化以下损失函数来学习价值：
- en: '|  | $\displaystyle\displaystyle J(\omega)$ | $\displaystyle=$ | $\displaystyle\mathbb{E}_{t}[(Y-q(s_{t},a_{t};\omega))^{2}]$
    |  | (8) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\displaystyle J(\omega)$ | $\displaystyle=$ | $\displaystyle\mathbb{E}_{t}[(Y-q(s_{t},a_{t};\omega))^{2}]$
    |  | (8) |'
- en: where $Y=r_{t+1}+\gamma\max q(s_{t+1},a_{t+1};\omega^{-})$ is the target, $\omega^{-}$
    denotes the parameters of the target network, and $\omega$ can be learnt based
    on the gradients.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Y=r_{t+1}+\gamma\max q(s_{t+1},a_{t+1};\omega^{-})$ 是目标，$\omega^{-}$ 表示目标网络的参数，而
    $\omega$ 可以基于梯度进行学习。
- en: '|  | $\displaystyle\omega\leftarrow\omega-\alpha\mathbb{E}_{t}[(Y-q(s_{t},a_{t};\omega))\nabla
    q(s_{t},a_{t};\omega)]$ |  | (9) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\omega\leftarrow\omega-\alpha\mathbb{E}_{t}[(Y-q(s_{t},a_{t};\omega))\nabla
    q(s_{t},a_{t};\omega)]$ |  | (9) |'
- en: The major contributions of DQN are the introduction of the target network and
    experience replay. To avoid rapidly fluctuating Q values and stabilize the training,
    the target network is fixed for a specified number of iterations during the update
    of the primary Q-network and subsequently updated to match the primary Q-network.
    Moreover, experience replay [[46](#bib.bib46)], which maintains a memory that
    stores transitions $(s_{t},a_{t},s_{t+1},r_{t+1})$, increases the sample efficiency.
    A later study improves the uniform sample experience replay by introducing priority
    [[47](#bib.bib47)]. Continuous DQN (cDQN) [[48](#bib.bib48)] derives a continuous
    variant of DQN based on normalized advantage functions (NAFs). Double DQN (DDQN)
    [[49](#bib.bib49)] addresses the overestimation problem of DQN through the use
    of a double estimator. Dueling-DQN [[50](#bib.bib50)] introduces a dueling architecture
    in which both the value function and associated advantage function are estimated.
    QR-DQN [[51](#bib.bib51)] utilizes distributional reinforcement learning to learn
    the full value distribution rather than only the expectation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 的主要贡献在于引入了目标网络和经验回放。为避免 Q 值剧烈波动并稳定训练，目标网络在更新主 Q 网络时固定若干迭代次数，随后更新以匹配主 Q 网络。此外，经验回放
    [[46](#bib.bib46)] 通过维护存储转移 $(s_{t},a_{t},s_{t+1},r_{t+1})$ 的记忆，增加了样本效率。后来的研究通过引入优先级
    [[47](#bib.bib47)] 改进了均匀样本经验回放。连续 DQN (cDQN) [[48](#bib.bib48)] 基于归一化优势函数 (NAFs)
    推导了 DQN 的连续变体。双重 DQN (DDQN) [[49](#bib.bib49)] 通过使用双重估计器解决了 DQN 的高估问题。对抗 DQN [[50](#bib.bib50)]
    引入了对抗结构，其中价值函数和相关优势函数都被估计。QR-DQN [[51](#bib.bib51)] 利用分布式强化学习来学习完整的价值分布，而不仅仅是期望值。
- en: II-C Policy-based Methods
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 基于策略的方法
- en: 'Alternatively, one can directly search and optimize a parameterized policy
    $\pi_{\theta}$ to maximize the expected return:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，也可以直接搜索和优化一个参数化的策略 $\pi_{\theta}$ 以最大化期望回报：
- en: '|  | $\max_{\theta}J(\theta)=\max_{\theta}v_{\pi_{\theta}}(s)=\max_{\theta}\mathbb{E}_{\pi_{\theta}}[G_{t}&#124;s_{t}=s]$
    |  | (10) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta}J(\theta)=\max_{\theta}v_{\pi_{\theta}}(s)=\max_{\theta}\mathbb{E}_{\pi_{\theta}}[G_{t}&#124;s_{t}=s]$
    |  | (10) |'
- en: 'where $\theta$ denotes the policy parameters, which can be optimized based
    on the policy gradient theorem [[21](#bib.bib21)]:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta$ 表示策略参数，可以基于策略梯度定理 [[21](#bib.bib21)] 进行优化。
- en: '|  | $\displaystyle\displaystyle\nabla J(\theta)$ | $\displaystyle\propto$
    | $\displaystyle\sum_{s}\mu(s)\sum_{a}q_{\pi}(s,a)\nabla\pi_{\theta}(a&#124;s)$
    |  | (11) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\displaystyle\nabla J(\theta)$ | $\displaystyle\propto$
    | $\displaystyle\sum_{s}\mu(s)\sum_{a}q_{\pi}(s,a)\nabla\pi_{\theta}(a&#124;s)$
    |  | (11) |'
- en: '|  |  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{\pi}[\sum_{a}q_{\pi}(s_{t},a)\nabla\pi_{\theta}(a&#124;s_{t})]$
    |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{\pi}[\sum_{a}q_{\pi}(s_{t},a)\nabla\pi_{\theta}(a&#124;s_{t})]$
    |  |'
- en: '|  |  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{\pi}[G_{t}\nabla\ln\pi_{\theta}(a_{t}&#124;s_{t})]$
    |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{\pi}[G_{t}\nabla\ln\pi_{\theta}(a_{t}&#124;s_{t})]$
    |  |'
- en: 'where $\mu(s)$ denotes the state visitation frequency. REINFORCE [[52](#bib.bib52)]
    is a straightforward Monte Carlo policy-based method that selects the discounted
    return $G_{t}$ following the policy $\pi_{\theta}$ to estimate the policy gradient
    in Eqn.[11](#S2.E11 "In II-C Policy-based Methods ‣ II Preliminaries of Reinforcement
    Learning ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning").
    The parameters are updated as follows [[21](#bib.bib21)]:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu(s)$ 表示状态访问频率。REINFORCE [[52](#bib.bib52)] 是一种直接的蒙特卡洛基于策略的方法，它通过策略 $\pi_{\theta}$
    选择折扣回报 $G_{t}$ 来估计方程 [11](#S2.E11 "In II-C Policy-based Methods ‣ II Preliminaries
    of Reinforcement Learning ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning") 中的策略梯度。参数更新如下 [[21](#bib.bib21)]：
- en: '|  | $\theta\leftarrow\theta+\alpha G_{t}\nabla\ln\pi_{\theta}(a_{t}&#124;s_{t})$
    |  | (12) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta\leftarrow\theta+\alpha G_{t}\nabla\ln\pi_{\theta}(a_{t}&#124;s_{t})$
    |  | (12) |'
- en: This update intuitively increases the log probability of actions that lead to
    higher returns. Since empirical returns are used, the resulting gradients suffer
    from high variances. A common technique for reducing the variance and accelerating
    the learning is to replace $G_{t}$ in Eqn. [11](#S2.E11 "In II-C Policy-based
    Methods ‣ II Preliminaries of Reinforcement Learning ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning") and [12](#S2.E12 "In II-C Policy-based
    Methods ‣ II Preliminaries of Reinforcement Learning ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning") by $G_{t}-b(s_{t})$ [[52](#bib.bib52),
    [21](#bib.bib21)], where $b(s_{t})$ is a baseline. Alternatively, $G_{t}$ can
    be replaced by the advantage function [[53](#bib.bib53), [54](#bib.bib54)] $A_{\pi_{\theta}}(s,a)=q_{\pi_{\theta}(s,a)}-v_{\pi_{\theta}}(s)$.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更新直观地增加了导致更高回报的动作的对数概率。由于使用了经验回报，结果的梯度存在较高的方差。减少方差和加速学习的一个常见技术是用$G_{t}-b(s_{t})$
    [[52](#bib.bib52), [21](#bib.bib21)]替换公式中的$G_{t}$，其中$b(s_{t})$是基线。另一种方法是用优势函数[[53](#bib.bib53),
    [54](#bib.bib54)] $A_{\pi_{\theta}}(s,a)=q_{\pi_{\theta}(s,a)}-v_{\pi_{\theta}}(s)$替换$G_{t}$。
- en: One problem of policy-based methods is poor gradient updates may result in newly
    updated policies that deviate wildly from previous policies, which may decrease
    the policy performance. Trust region policy optimization (TRPO) [[55](#bib.bib55)]
    solves this problem through optimization of a surrogate objective function, which
    guarantees the monotonic improvement of policy performance. Each policy gradient
    update is constrained by using a quadratic approximation of the Kullback-Leibler
    (KL) divergence between policies. Proximal policy optimization (PPO) [[56](#bib.bib56)]
    improved upon TRPO through the application of an adaptive penalty on the KL divergence
    and a heuristic clipped surrogate objective function. The requirement for only
    a first-order gradient also renders PPO easier to implement than TRPO.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 策略方法的一个问题是，较差的梯度更新可能导致新更新的策略与之前的策略偏差较大，从而可能降低策略性能。信任区域策略优化（TRPO）[[55](#bib.bib55)]通过优化一个代理目标函数来解决这个问题，从而保证策略性能的单调改进。每次策略梯度更新都受到政策之间Kullback-Leibler（KL）散度的二次近似的约束。近端策略优化（PPO）[[56](#bib.bib56)]通过对KL散度应用自适应惩罚和启发式裁剪代理目标函数改进了TRPO。只需要一阶梯度的要求也使得PPO比TRPO更易于实现。
- en: II-D Actor-Critic Methods
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D Actor-Critic 方法
- en: Actor-critic methods have the advantages of both value-based and policy-based
    methods, where a neural network actor $\pi_{\theta}(a|s)$ selects actions and
    a neural network critic $q(s,a;\omega)$ or $v(s;\omega)$ estimates the values.
    The actor and critic are typically updated alternately according to Eqn. [11](#S2.E11
    "In II-C Policy-based Methods ‣ II Preliminaries of Reinforcement Learning ‣ A
    Survey of Deep RL and IL for Autonomous Driving Policy Learning") and Eqn. [8](#S2.E8
    "In II-B Value-based Methods ‣ II Preliminaries of Reinforcement Learning ‣ A
    Survey of Deep RL and IL for Autonomous Driving Policy Learning"), respectively.
    Deterministic policy gradient (DPG) [[57](#bib.bib57)] is an off-policy actor-critic
    algorithm that derives deterministic policies. Compared with stochastic policies,
    DPG only integrates over the state space and requires fewer samples in problems
    with large action spaces. Deep deterministic policy gradient (DDPG) [[24](#bib.bib24)]
    utilizes DNNs to operate on high-dimensional state spaces with experience replay
    and a separate actor-critic target network, which is similar to DQN. Exploitation
    of parallel computation is an alternative to experience replay. Asynchronous advantage
    actor-critic (A3C) [[58](#bib.bib58)] uses advantage estimates rather than discounted
    returns in the actor-critic framework and asynchronously updates policy and value
    networks on multiple parallel threads of the environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic方法结合了基于值的方法和基于策略的方法的优点，其中神经网络actor $\pi_{\theta}(a|s)$选择动作，神经网络critic
    $q(s,a;\omega)$或$v(s;\omega)$估计值。actor和critic通常根据公式[11](#S2.E11 "在II-C基于策略的方法
    ‣ II 模仿学习的基础 ‣ 自动驾驶政策学习的深度强化学习和模仿学习概述")和公式[8](#S2.E8 "在II-B基于值的方法 ‣ II 模仿学习的基础
    ‣ 自动驾驶政策学习的深度强化学习和模仿学习概述")交替更新。确定性策略梯度（DPG）[[57](#bib.bib57)]是一种导出确定性策略的离线策略actor-critic算法。与随机策略相比，DPG只在状态空间上进行积分，并且在具有大动作空间的问题中需要更少的样本。深度确定性策略梯度（DDPG）[[24](#bib.bib24)]利用DNNs在高维状态空间上进行操作，采用经验回放和一个单独的actor-critic目标网络，类似于DQN。并行计算的利用是经验回放的一个替代方案。异步优势actor-critic（A3C）[[58](#bib.bib58)]在actor-critic框架中使用优势估计而不是折扣回报，并在环境的多个并行线程上异步更新策略和值网络。
- en: The parallel independent environments stabilize the learning process and enable
    more exploration. Advantage actor critic (A2C) [[59](#bib.bib59)], which is the
    synchronous version of A3C, uses a single agent for simplicity or waits for each
    agent to finish its experience to collect multiple trajectories. Soft actor critic
    (SAC) [[60](#bib.bib60)] benefits from the addition of an entropy term to the
    reward function to encourage better exploration.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 并行独立环境稳定了学习过程，并支持更多的探索。优势actor-critic（A2C）[[59](#bib.bib59)]，即A3C的同步版本，使用单一agent以简化或等待每个agent完成其经验以收集多个轨迹。软actor-critic（SAC）[[60](#bib.bib60)]通过在奖励函数中添加熵项来促进更好的探索。
- en: III Preliminaries of Imitation Learning
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 模仿学习的基础
- en: III-A Problem Formulation
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 问题表述
- en: Imitation learning possesses a simpler form and is based on learning from demonstrations
    (LfD) [[61](#bib.bib61)]. It is attractive for AD applications, where interaction
    with the real environment could be dangerous and vast amount of human driving
    data can be easily collected [[62](#bib.bib62)]. A demonstration dataset ${\cal
    D}=\{\xi_{i}\}_{i=0..N}$ contains a set of trajectories, where each trajectory
    $\xi_{i}=\{(s_{t}^{i},a_{t}^{i})\}_{t=0..T}$ is a sequence of state-action pairs,
    and action $a_{t}^{i}\in\cal A$ is taken by expert at state $s_{t}^{i}\in S\cal$
    under the guidance of an underlying policy $\pi_{E}$ of the expert [[32](#bib.bib32)].
    Using the collected dataset $\cal D$, a common optimization-based strategy of
    imitation learning is to learn a policy $\pi^{*}:\cal S\rightarrow\cal A$ that
    mimics the expert’s policy $\pi_{E}$ by satisfying [[33](#bib.bib33)]
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习具有一种更简单的形式，并基于从演示（LfD）中学习[[61](#bib.bib61)]。它在自动驾驶应用中具有吸引力，因为与真实环境的互动可能是危险的，而且大量的人类驾驶数据可以被轻松收集[[62](#bib.bib62)]。一个演示数据集${\cal
    D}=\{\xi_{i}\}_{i=0..N}$包含一组轨迹，其中每个轨迹$\xi_{i}=\{(s_{t}^{i},a_{t}^{i})\}_{t=0..T}$是状态-动作对的序列，动作$a_{t}^{i}\in\cal
    A$由专家在状态$s_{t}^{i}\in S\cal$下采取，并依据专家的潜在策略$\pi_{E}$[[32](#bib.bib32)]。利用收集的数据集$\cal
    D$，一种常见的基于优化的模仿学习策略是学习一个策略$\pi^{*}:\cal S\rightarrow\cal A$，通过满足条件来模拟专家的策略$\pi_{E}$[[33](#bib.bib33)]。
- en: '|  | $\pi^{*}=\arg\min_{\pi}\mathbb{D}(\pi_{E},\pi)$ |  | (13) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}=\arg\min_{\pi}\mathbb{D}(\pi_{E},\pi)$ |  | (13) |'
- en: where $\mathbb{D}$ is a similarity measure between policies. The methods for
    solving the problem can be divided into three groups, as shown in Fig. [2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning") (b), which are reviewed below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{D}$ 是策略之间的相似性度量。解决该问题的方法可以分为三组，如图 [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning") (b) 所示，下面对其进行回顾。
- en: III-B Behavior Clone
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 行为克隆
- en: 'Behavior clone (BC) formulates the problem as a supervised learning process
    with the objective of matching the learned policy $\pi_{\theta}$ to the expert
    policy $\pi_{E}$:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 行为克隆（BC）将问题公式化为一个监督学习过程，目标是将学习到的策略 $\pi_{\theta}$ 匹配到专家策略 $\pi_{E}$：
- en: '|  | $\min_{\theta}\mathbb{E}&#124;&#124;\pi_{\theta}-\pi_{E}&#124;&#124;_{2}$
    |  | (14) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\mathbb{E}&#124;&#124;\pi_{\theta}-\pi_{E}&#124;&#124;_{2}$
    |  | (14) |'
- en: 'which is typically realized by minimizing the L2-loss:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常通过最小化L2损失来实现：
- en: '|  | $J(\theta)=\mathbb{E}_{(s,a)\sim\cal D}[(\pi_{\theta}(s)-a)^{2}]$ |  |
    (15) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=\mathbb{E}_{(s,a)\sim\cal D}[(\pi_{\theta}(s)-a)^{2}]$ |  |
    (15) |'
- en: Early research on imitation learning can be dated back to the ALVINN system
    [[10](#bib.bib10)], which used a 3-layer neural network to perform road following
    based on front camera images. In the most recent decade, deep imitation learning
    (DIL) has been conducted using DNNs as policy function approximators and has realized
    success in end-to-end AD systems [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)].
    BC performs well for states that are covered by the training distribution but
    generalizes poorly to new states due to compounding errors in the actions, which
    is also referred to as covariate shift [[66](#bib.bib66), [67](#bib.bib67)]. To
    overcome this problem, Ross et al. [[68](#bib.bib68)] proposed DAgger, which improves
    upon supervised learning by using a primary policy to collect training examples
    while running a reference policy simultaneously. In each iteration, states that
    are visited by the primary policy are also sent to the reference policy to output
    expert actions, and the newly generated demonstrations are aggregated into the
    training dataset. SafeDAgger [[69](#bib.bib69)] extends on DAgger by introducing
    a safe policy that learns to predict the error that is made by a primary policy
    without querying the reference policy. In addition to dataset aggregation, data
    augmentation techniques such as the addition of random noise to the expert action
    have also been commonly used in DIL [[70](#bib.bib70), [71](#bib.bib71)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的模仿学习研究可以追溯到ALVINN系统 [[10](#bib.bib10)]，该系统使用了一个3层神经网络，通过前置摄像头图像进行道路跟随。在最近的十年中，深度模仿学习（DIL）通过使用DNN作为策略函数近似器，并在端到端自动驾驶系统中取得了成功
    [[63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)]。BC在训练分布覆盖的状态下表现良好，但由于行动中的累积误差，它对新状态的泛化能力较差，这也被称为协变量偏移
    [[66](#bib.bib66), [67](#bib.bib67)]。为了克服这一问题，Ross等人 [[68](#bib.bib68)] 提出了DAgger，它通过使用主策略收集训练样本，同时运行参考策略进行改进。在每次迭代中，由主策略访问的状态也会被发送到参考策略，以输出专家行为，新生成的示例将被聚合到训练数据集中。SafeDAgger
    [[69](#bib.bib69)] 在DAgger的基础上扩展，引入了一种安全策略，该策略学习预测主策略的错误，而无需查询参考策略。除了数据集聚合，数据增强技术，如在专家行为中添加随机噪声，也在DIL中被广泛使用
    [[70](#bib.bib70), [71](#bib.bib71)]。
- en: III-C Inverse Reinforcement Learning
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 逆向强化学习
- en: 'The inverse reinforcement learning problem, which was first formulized in the
    study of Ng et al. [[72](#bib.bib72)], is to identify a reward function $r_{\theta}$
    for which the expert behavior is optimal:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 逆向强化学习问题，最早由Ng等人 [[72](#bib.bib72)] 公式化，是确定一个奖励函数 $r_{\theta}$，使得专家行为是最优的：
- en: '|  | $\max_{\theta}\mathbb{E}_{\pi_{E}}[G_{t}&#124;r_{\theta}]-\mathbb{E}_{\pi}[G_{t}&#124;r_{\theta}]$
    |  | (16) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\theta}\mathbb{E}_{\pi_{E}}[G_{t}&#124;r_{\theta}]-\mathbb{E}_{\pi}[G_{t}&#124;r_{\theta}]$
    |  | (16) |'
- en: 'Early studies utilized linear function approximation of reward functions and
    identified the optimal reward via maximum margin approaches [[73](#bib.bib73),
    [74](#bib.bib74)]. By introducing the maximum entropy principle, Ziebart et al.
    [[75](#bib.bib75)] eliminated the reward ambiguity between demonstrations and
    expert policy where multiple rewards may explain the expert behavior. The reward
    function is learned through maximizing the posterior probability of observing
    expert trajectories:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的研究利用线性函数近似奖励函数，并通过最大边际方法确定最优奖励 [[73](#bib.bib73), [74](#bib.bib74)]。通过引入最大熵原理，Ziebart
    等人 [[75](#bib.bib75)] 消除了示范和专家策略之间的奖励模糊性，其中多个奖励可能解释专家行为。奖励函数通过最大化观察专家轨迹的后验概率来学习：
- en: '|  | $J(\theta)=\mathbb{E}_{\xi_{i}\sim\cal D}[\log P(\xi_{i}&#124;r_{\theta})]$
    |  | (17) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=\mathbb{E}_{\xi_{i}\sim\cal D}[\log P(\xi_{i}|r_{\theta})]$
    |  | (17) |'
- en: where the probability of a trajectory satisfies $P(\xi_{i}|r_{\theta})\propto\exp(r_{\theta}(\xi_{i}))$.
    Several studies have extended the reward functions to nonlinear formulations through
    Gaussian processes [[76](#bib.bib76)] or boosting [[77](#bib.bib77), [78](#bib.bib78)].
    However, the above methods generally operate on low-dimensional features. The
    use of rich and expressive function approximators, in the form of neural networks,
    has been proposed to learn reward functions directly on raw high-dimensional state
    representations [[79](#bib.bib79), [80](#bib.bib80)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，轨迹的概率满足 $P(\xi_{i}|r_{\theta})\propto\exp(r_{\theta}(\xi_{i}))$。一些研究通过高斯过程
    [[76](#bib.bib76)] 或提升 [[77](#bib.bib77), [78](#bib.bib78)] 将奖励函数扩展到非线性形式。然而，上述方法通常在低维特征上操作。提出了使用丰富且表达力强的函数逼近器，以神经网络的形式直接在原始高维状态表示上学习奖励函数
    [[79](#bib.bib79), [80](#bib.bib80)]。
- en: A problem that is encountered with IRL is that to evaluate the reward function,
    a forward reinforcement learning process must be conducted to obtain the corresponding
    policy, thereby rendering IRL inefficient and computationally expensive. Many
    early approaches require solving an MDP in the inner loop of each iterative optimization
    [[72](#bib.bib72), [20](#bib.bib20), [74](#bib.bib74), [75](#bib.bib75)]. Perfect
    knowledge of the system dynamics and an efficient offline solver are needed in
    these methods, thereby limiting their applications in complex real-world scenarios
    such as robotic control. Finn et al. [[80](#bib.bib80)] proposed guided cost learning
    (GCL), which handles unknown dynamics in high-dimensional complex systems and
    learns complex neural network cost functions through an efficient sample-based
    approximation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: IRL 面临的问题是，为了评估奖励函数，必须进行前向强化学习过程以获得相应的策略，这使得 IRL 效率低下且计算成本高。许多早期的方法需要在每次迭代优化的内部循环中求解
    MDP [[72](#bib.bib72), [20](#bib.bib20), [74](#bib.bib74), [75](#bib.bib75)]。这些方法需要对系统动态有完美的了解和一个高效的离线求解器，从而限制了它们在复杂现实场景中的应用，如机器人控制。Finn
    等人 [[80](#bib.bib80)] 提出了指导成本学习（GCL），该方法处理高维复杂系统中的未知动态，并通过高效的基于样本的近似学习复杂的神经网络成本函数。
- en: III-D Generative Adversarial Imitation Learning
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 生成对抗模仿学习
- en: 'Generative adversarial imitation learning (GAIL) [[81](#bib.bib81)] directly
    learns a policy from expert demonstrations while requiring neither the reward
    design in RL nor the expensive RL process in the inner loop of IRL. GAIL establishes
    an analogy between imitation learning and generative adversarial networks (GANs)
    [[82](#bib.bib82)]. The generator $\pi_{\theta}$ serves as a policy for imitating
    expert behavior by matching the state-action distribution of demonstrations, while
    the discriminator $D_{\omega}\in(0,1)$ serves as a surrogate reward function for
    measuring the similarity between generated and demonstration samples. The objective
    function of GAIL is formulated in the following min-max form:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗模仿学习（GAIL） [[81](#bib.bib81)] 直接从专家示范中学习策略，同时不需要在 RL 中进行奖励设计，也不需要在 IRL 的内部循环中进行昂贵的
    RL 过程。GAIL 在模仿学习和生成对抗网络（GANs） [[82](#bib.bib82)] 之间建立了类比。生成器 $\pi_{\theta}$ 作为模仿专家行为的策略，通过匹配示范的状态-动作分布，而判别器
    $D_{\omega}\in(0,1)$ 作为替代奖励函数，用于测量生成样本和示范样本之间的相似性。GAIL 的目标函数以如下的最小-最大形式进行公式化：
- en: '|  | $\min\limits_{\pi_{\theta}}\max\limits_{D_{\omega}}{\mathbb{E}}_{\pi_{\theta}}[\log
    D_{\omega}(s,a)]+{\mathbb{E}}_{\pi_{E}}[\log(1-D_{\omega}(s,a))]-\lambda H(\pi_{\theta})$
    |  | (18) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min\limits_{\pi_{\theta}}\max\limits_{D_{\omega}}{\mathbb{E}}_{\pi_{\theta}}[\log
    D_{\omega}(s,a)]+{\mathbb{E}}_{\pi_{E}}[\log(1-D_{\omega}(s,a))]-\lambda H(\pi_{\theta})$
    |  | (18) |'
- en: 'where $H(\pi)$ is a regularization entropy term. The generator and the discriminator
    are updated with the following gradients:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H(\pi)$ 是正则化熵项。生成器和判别器通过以下梯度更新：
- en: '|  | $\displaystyle\nabla_{\theta}J(\theta)={\mathbb{E}}_{\pi}[\nabla_{\theta}\log\pi_{\theta}(a&#124;s)Q(s,a)]-\lambda\nabla_{\theta}H(\pi_{\theta})$
    |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\theta}J(\theta)={\mathbb{E}}_{\pi}[\nabla_{\theta}\log\pi_{\theta}(a&#124;s)Q(s,a)]-\lambda\nabla_{\theta}H(\pi_{\theta})$
    |  |'
- en: '|  | $\displaystyle\nabla_{\omega}J(\omega)={\mathbb{E}}_{\pi}[\nabla_{\omega}\log
    D_{\omega}(s,a)]+{\mathbb{E}}_{\pi_{E}}[\nabla_{\omega}\log(1-D_{\omega}(s,a))]$
    |  | (19) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\omega}J(\omega)={\mathbb{E}}_{\pi}[\nabla_{\omega}\log
    D_{\omega}(s,a)]+{\mathbb{E}}_{\pi_{E}}[\nabla_{\omega}\log(1-D_{\omega}(s,a))]$
    |  | (19) |'
- en: Finn et al. [[83](#bib.bib83)] mathematically proved the connection among GANs,
    IRL and energy-based models. Fu et al.[[84](#bib.bib84)] proposed adversarial
    inverse reinforcement learning (AIRL) based on an adversarial reward learning
    formulation, which can recover reward functions that are robust to dynamics changes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Finn等人[[83](#bib.bib83)]在数学上证明了GANs、IRL和基于能量的模型之间的联系。Fu等人[[84](#bib.bib84)]提出了基于对抗性奖励学习公式的对抗性逆强化学习（AIRL），可以恢复对动态变化具有鲁棒性的奖励函数。
- en: IV Architectures of DRL/DIL Integrated AD Systems
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV DRL/DIL集成AD系统架构
- en: AD systems have been studied for decades [[13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)], which are commonly composed of modular pipelines,
    as illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A Survey of Deep
    RL and IL for Autonomous Driving Policy Learning"). How can DRL/DIL models be
    integrated into an AD system and collaborate with other modules? This section
    reviews the literature from the system architecture perspective, from which five
    modes are identified, as illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"). A classification of the studies in each mode is presented in
    Table [I](#S4.T1 "TABLE I ‣ IV Architectures of DRL/DIL Integrated AD Systems
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"), along with
    the exploited DRL/DIL methods, the upstream module for perception, the targeted
    AD tasks, and the advantages and disadvantages of the architectures, among other
    information. Below, we detail each mode of the architectures, which is followed
    by a comparison of the number of studies that correspond to each mode or to the
    use of DRL or DIL methods.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: AD系统的研究已有数十年[[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]，通常由模块化管道组成，如图[1](#S1.F1
    "图1 ‣ I 引言 ‣ 深度RL和IL在自动驾驶策略学习中的综述")所示。DRL/DIL模型如何集成到AD系统中，并与其他模块协作？本节从系统架构的角度回顾文献，其中识别出五种模式，如图[3](#S4.F3
    "图3 ‣ IV DRL/DIL集成AD系统架构 ‣ 深度RL和IL在自动驾驶策略学习中的综述")所示。每种模式的研究分类见表[I](#S4.T1 "表I
    ‣ IV DRL/DIL集成AD系统架构 ‣ 深度RL和IL在自动驾驶策略学习中的综述")，同时列出了所利用的DRL/DIL方法、感知的上游模块、目标AD任务，以及架构的优势和劣势等信息。接下来，我们详细介绍每种架构模式，并比较对应每种模式或使用DRL或DIL方法的研究数量。
- en: '![Refer to caption](img/b5e6df58cd7d42d0eab57d3c807e38d9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b5e6df58cd7d42d0eab57d3c807e38d9.png)'
- en: 'Figure 3: Integration modes of DRL/DIL models into AD architectures'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：DRL/DIL模型在AD架构中的集成模式
- en: 'TABLE I: A taxonomy of the literature by the integration modes of DRL or DIL
    models into AD architectures'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：按DRL或DIL模型在AD架构中的集成模式对文献的分类
- en: '| Architecture | Advantages | Disadvantages | Studies | Methods | Perception¹
    | Tasks² | Remarks³ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 优势 | 缺点 | 研究 | 方法 | 感知¹ | 任务² | 备注³ |'
- en: '| Mode 1. DRL/DIL Integrated Control | - Features a simple structure and adapts
    to various learning methods. | - Limited to specified tasks or skills. - Bypassing
    planning modules weakens the model’s interpretability and capability. | [[10](#bib.bib10)],
    [[85](#bib.bib85)], [[63](#bib.bib63)], [[64](#bib.bib64)], [[86](#bib.bib86)],
    [[87](#bib.bib87)], [[65](#bib.bib65)], [[88](#bib.bib88)] | BC | D |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 模式1. DRL/DIL集成控制 | - 具有简单结构并适应各种学习方法。 | - 限于特定任务或技能。 - 绕过规划模块削弱了模型的可解释性和能力。
    | [[10](#bib.bib10)], [[85](#bib.bib85)], [[63](#bib.bib63)], [[64](#bib.bib64)],
    [[86](#bib.bib86)], [[87](#bib.bib87)], [[65](#bib.bib65)], [[88](#bib.bib88)]
    | BC | D |'
- en: '&#124; road/lane following &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 道路/车道跟踪 &#124;'
- en: '&#124; urban driving &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 城市驾驶 &#124;'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; safety: [[69](#bib.bib69)] &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 安全性: [[69](#bib.bib69)] &#124;'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [[69](#bib.bib69)], [[89](#bib.bib89)] | DAgger | D | road/lane following
    |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| [[69](#bib.bib69)], [[89](#bib.bib89)] | DAgger | D | 道路/车道跟踪 |  |'
- en: '|'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[90](#bib.bib90)], [[91](#bib.bib91)], &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[90](#bib.bib90)], [[91](#bib.bib91)], &#124;'
- en: '&#124; [[92](#bib.bib92)], [[93](#bib.bib93)], &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[92](#bib.bib92)], [[93](#bib.bib93)], &#124;'
- en: '&#124; [[94](#bib.bib94)], [[95](#bib.bib95)] &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[94](#bib.bib94)], [[95](#bib.bib95)] &#124;'
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; NQL &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NQL &#124;'
- en: '&#124; DQN &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQN &#124;'
- en: '| T |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| T |'
- en: '&#124; lane changing &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 车道变换 &#124;'
- en: '&#124; traffic merging &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交通合流 &#124;'
- en: '&#124; imminent events &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 紧急事件 &#124;'
- en: '&#124; intersection &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 十字路口 &#124;'
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; safety: [[90](#bib.bib90)] &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 安全性: [[90](#bib.bib90)] &#124;'
- en: '&#124; interaction: &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交互: &#124;'
- en: '&#124; [[95](#bib.bib95)] &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[95](#bib.bib95)] &#124;'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [[96](#bib.bib96)], [[97](#bib.bib97)] | PPO | T |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)], [[97](#bib.bib97)] | PPO | T |'
- en: '&#124; traffic merging &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交通合流 &#124;'
- en: '&#124; urban driving &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 城市驾驶 &#124;'
- en: '|  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| [[28](#bib.bib28)], [[98](#bib.bib98)] | DDPG | D |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)], [[98](#bib.bib98)] | DDPG | D |'
- en: '&#124; road/lane following &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 道路/车道跟踪 &#124;'
- en: '&#124; imminent events &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 紧急事件 &#124;'
- en: '|  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| [[99](#bib.bib99)], [[100](#bib.bib100)], [[101](#bib.bib101)], [[102](#bib.bib102)],
    [[103](#bib.bib103)] | DDPG | T |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| [[99](#bib.bib99)], [[100](#bib.bib100)], [[101](#bib.bib101)], [[102](#bib.bib102)],
    [[103](#bib.bib103)] | DDPG | T |'
- en: '&#124; road/lane following &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 道路/车道跟踪 &#124;'
- en: '&#124; lane changing &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 车道变换 &#124;'
- en: '&#124; overtaking &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超车 &#124;'
- en: '&#124; imminent events &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 紧急事件 &#124;'
- en: '|  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| Mode 2. Extension of Mode 1 with High-level Command | - Considers both high-level
    planning and perception - Generates distinct control behaviors according to the
    high-level decisions | - Single model may not capture sufficiently diverse behaviors
    - Learning a model for each behavior increases the training cost and the demand
    for data | [[104](#bib.bib104)] | BC | D | urban driving |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 模式 2. 模式 1 的扩展，加入高层次命令 | - 考虑高层次规划和感知 - 根据高层次决策生成不同的控制行为 | - 单一模型可能无法捕捉足够多样的行为
    - 为每种行为学习一个模型增加了训练成本和数据需求 | [[104](#bib.bib104)] | BC | D | 城市驾驶 |  |'
- en: '| [[105](#bib.bib105)], [[70](#bib.bib70)], [[106](#bib.bib106)] | CIL | D
    | urban navigation |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [[105](#bib.bib105)], [[70](#bib.bib70)], [[106](#bib.bib106)] | CIL | D
    | 城市导航 |  |'
- en: '| [[107](#bib.bib107)], [[108](#bib.bib108)] | UAIL | D | urban navigation
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [[107](#bib.bib107)], [[108](#bib.bib108)] | UAIL | D | 城市导航 |'
- en: '&#124; uncertainty: &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不确定性: &#124;'
- en: '&#124; [[107](#bib.bib107)], [[108](#bib.bib108)] &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[107](#bib.bib107)], [[108](#bib.bib108)] &#124;'
- en: '|'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [[109](#bib.bib109)] | DDPG | T | parking |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[109](#bib.bib109)] | DDPG | T | 停车 |  |'
- en: '| [[26](#bib.bib26)] | DDPG | D | urban navigation |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bib26)] | DDPG | D | 城市导航 |  |'
- en: '| [[110](#bib.bib110)] | DDQN/TD3/SAC | T | roundabout |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110)] | DDQN/TD3/SAC | T | 环形交叉路口 |  |'
- en: '| Mode 3. DRL/DIL Integrated Motion Planning | - Learn to imitate human trajectories
    - Efficient forward prediction process | - No guarantee on safety or feasibility
    | [[111](#bib.bib111)], [[71](#bib.bib71)] | BC | T | urban driving |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 模式 3. DRL/DIL 综合运动规划 | - 学习模仿人类轨迹 - 高效的前向预测过程 | - 无法保证安全性或可行性 | [[111](#bib.bib111)],
    [[71](#bib.bib71)] | BC | T | 城市驾驶 |'
- en: '&#124; safety: [[71](#bib.bib71)] &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 安全性: [[71](#bib.bib71)] &#124;'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [[112](#bib.bib112)] | DAgger | T | highway driving |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| [[112](#bib.bib112)] | DAgger | T | 高速公路驾驶 |  |'
- en: '| [[113](#bib.bib113)] | MaxEnt DIRL | D | urban driving |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bib113)] | MaxEnt DIRL | D | 城市驾驶 |  |'
- en: '| [[114](#bib.bib114)] | DDQN/DQfD | T | valet parking |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| [[114](#bib.bib114)] | DDQN/DQfD | T | 代客泊车 |'
- en: '&#124; safety: [[114](#bib.bib114)] &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 安全性: [[114](#bib.bib114)] &#124;'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [[115](#bib.bib115)] | SAC | T | traffic merge |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| [[115](#bib.bib115)] | SAC | T | 交通合流 |  |'
- en: '| Mode 4. DRL/DIL Integrated Behavior Planning | - The DNNs need only plan
    high- level behavioral actions. | - Simple and few actions limit the control precision
    and diversity of driving styles. - Complicated and too many actions increase the
    training cost. | [[116](#bib.bib116)] | AIRL | T | lane change |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 模式 4. DRL/DIL 综合行为规划 | - DNNs 只需规划高层次的行为动作。 | - 简单且少量的动作限制了控制精度和驾驶风格的多样性。
    - 复杂且过多的动作增加了训练成本。 | [[116](#bib.bib116)] | AIRL | T | 车道变换 |  |'
- en: '| [[117](#bib.bib117)] | IRL | D | lane change |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bib117)] | IRL | D | 车道变换 |  |'
- en: '| [[118](#bib.bib118)], [[119](#bib.bib119)], [[120](#bib.bib120)], [[121](#bib.bib121)],
    [[122](#bib.bib122)], [[123](#bib.bib123)], [[124](#bib.bib124)], [[125](#bib.bib125)]
    | DQN | T |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118)], [[119](#bib.bib119)], [[120](#bib.bib120)], [[121](#bib.bib121)],
    [[122](#bib.bib122)], [[123](#bib.bib123)], [[124](#bib.bib124)], [[125](#bib.bib125)]
    | DQN | T |'
- en: '&#124; lane change &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 车道变换 &#124;'
- en: '&#124; intersection &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 十字路口 &#124;'
- en: '|  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| [[126](#bib.bib126)], [[127](#bib.bib127)] | DQN | D | lane change |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| [[126](#bib.bib126)], [[127](#bib.bib127)] | DQN | D | 车道变换 |  |'
- en: '| [[128](#bib.bib128)], [[129](#bib.bib129)] | Q-Learning | T | lane changing
    |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| [[128](#bib.bib128)], [[129](#bib.bib129)] | Q-Learning | T | 变道 |  |'
- en: '| [[130](#bib.bib130)], [[131](#bib.bib131)] | DDQN | T |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| [[130](#bib.bib130)], [[131](#bib.bib131)] | DDQN | T |'
- en: '&#124; lane changing &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 变道 &#124;'
- en: '&#124; urban driving &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 城市驾驶 &#124;'
- en: '|  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| [[132](#bib.bib132)] | DQfD | T | lane changing |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)] | DQfD | T | 变道 |'
- en: '&#124; safety: [[132](#bib.bib132)] &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 安全: [[132](#bib.bib132)] &#124;'
- en: '|'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [[133](#bib.bib133)] | QR-DQN | D | highway driving |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| [[133](#bib.bib133)] | QR-DQN | D | 高速公路驾驶 |  |'
- en: '| Mode 5. DRL/DIL Integrated Hierarchical Planning and Control | - Simultaneously
    plan at various levels of abstraction. | - Hierarchical DNNs increase the training
    cost and decrease the convergence speed. | [[134](#bib.bib134)], [[135](#bib.bib135)]
    | DQN | T |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 模式 5. DRL/DIL 集成的分层规划与控制 | - 在各个抽象层次上同时进行规划。 | - 分层DNNs增加了训练成本并降低了收敛速度。 |
    [[134](#bib.bib134)], [[135](#bib.bib135)] | DQN | T |'
- en: '&#124; cruise control &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自适应巡航控制 &#124;'
- en: '&#124; lane changing &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 变道 &#124;'
- en: '|  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| [[136](#bib.bib136)] | Hierarchical Policy gradient | T | traffic light passing
    |  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| [[136](#bib.bib136)] | 分层策略梯度 | T | 通过交通信号灯 |  |'
- en: '| [[27](#bib.bib27)] | DDPG | D | lane changing |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] | DDPG | D | 变道 |  |'
- en: '| [[137](#bib.bib137)] | DDPG | T | intersection |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| [[137](#bib.bib137)] | DDPG | T | 交叉口 |  |'
- en: '| [[138](#bib.bib138)] | DDPG | T | urban driving |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138)] | DDPG | T | 城市驾驶 |  |'
- en: '1'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: 'Type of upstream perception module: (D)eep learning method/(T)raditional method'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上游感知模块的类型：（D）深度学习方法 / （T）传统方法
- en: '2'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: For detailed information about AD tasks, see Table [III](#S4.T3 "TABLE III ‣
    IV-C Mode 3\. DRL/DIL Integrated Motion Planning ‣ IV Architectures of DRL/DIL
    Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy
    Learning")
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关自动驾驶任务的详细信息，请参见表[III](#S4.T3 "TABLE III ‣ IV-C 模式 3\. DRL/DIL 集成运动规划 ‣ IV
    DRL/DIL 集成自动驾驶系统的架构 ‣ 深度强化学习和模仿学习在自动驾驶策略学习中的应用调查")
- en: '3'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3'
- en: Studies that also address safety [VI-A](#S6.SS1 "VI-A Safety-enhanced DRL/DIL
    for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning"), interaction [VI-B](#S6.SS2 "VI-B Interaction-aware
    DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning") and uncertainty [VI-C](#S6.SS3 "VI-C Uncertainty-aware
    DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning") problems are labelled.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 也涉及安全[VI-A](#S6.SS1 "VI-A 安全增强的DRL/DIL用于自动驾驶 ‣ VI 问题驱动方法 ‣ 深度强化学习和模仿学习在自动驾驶策略学习中的应用调查")、互动[VI-B](#S6.SS2
    "VI-B 互动感知的DRL/DIL用于自动驾驶 ‣ VI 问题驱动方法 ‣ 深度强化学习和模仿学习在自动驾驶策略学习中的应用调查")和不确定性[VI-C](#S6.SS3
    "VI-C 不确定性感知的DRL/DIL用于自动驾驶 ‣ VI 问题驱动方法 ‣ 深度强化学习和模仿学习在自动驾驶策略学习中的应用调查")问题的研究均已标注。
- en: IV-A Mode 1\. DRL/DIL Integrated Control
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 模式 1\. DRL/DIL 集成控制
- en: Many studies have applied DRL/DIL to control, which can be abstracted as the
    architecture of Mode 1 and is illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"). Bojarski et al. [[63](#bib.bib63)] proposed a well-known end-to-end
    self-driving control framework. They trained a nine-layer CNN by supervised learning
    to learn the steering policy without explicit manual decomposition of sequential
    modules. However, their model only adapts to lane keeping. An alternative option
    is to feed traditional perception results into the DNN control module. Tang et
    al. [[96](#bib.bib96)] proposed the use of environmental rasterization encoding,
    along with the relative distance, velocity and acceleration, as input to a two-branch
    neural network, which was trained via proximal policy optimization.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究已将DRL/DIL应用于控制，可以抽象为模式1的架构，并在图[3](#S4.F3 "图3 ‣ IV DRL/DIL 集成自动驾驶系统的架构 ‣
    深度强化学习和模仿学习在自动驾驶策略学习中的应用调查")中进行了说明。Bojarski等人[[63](#bib.bib63)]提出了一个知名的端到端自动驾驶控制框架。他们通过监督学习训练了一个九层CNN，以学习转向策略，而无需明确的手动分解序列模块。然而，他们的模型仅适用于车道保持。另一种选择是将传统的感知结果输入到DNN控制模块中。Tang等人[[96](#bib.bib96)]提出了将环境栅格编码、相对距离、速度和加速度作为输入，输入到一个双分支神经网络中，并通过近端策略优化进行训练。
- en: Although Mode 1 features a simple structure and adapts to a large variety of
    learning methods, it is limited to specified tasks; thus, it has difficulty addressing
    scenarios in which multiple driving skills that are conditioned on various situations
    are needed. Moreover, bypassing and ignoring behavior planning or motion planning
    processes may weaken the model’s interpretability and performance in complex tasks
    (e.g., urban navigation).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模式1具有简单的结构并适应多种学习方法，但它仅限于特定任务；因此，在需要多种情境条件下的驾驶技能的场景中，它难以应对。此外，绕过和忽略行为规划或运动规划过程可能会削弱模型在复杂任务（例如城市导航）中的可解释性和性能。
- en: IV-B Mode 2\. Extension of Mode 1 with High-level Command
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 模式2\. 高级命令扩展模式1
- en: As illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated
    AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"),
    Mode 2 extends Mode 1 by considering the high-level planning output. The control
    module is composed of either a general model for all behaviors [[104](#bib.bib104),
    [110](#bib.bib110)] or a series of models for distinct behaviors [[105](#bib.bib105),
    [70](#bib.bib70), [26](#bib.bib26), [106](#bib.bib106), [108](#bib.bib108)]. Chen
    et al. [[110](#bib.bib110)] projected detected environment vehicles and the routing
    onto a bird-view road map, which served as the input of a policy network. Liang
    et al. [[26](#bib.bib26)] built on conditional imitation learning (CIL) [[70](#bib.bib70)]
    and proposed the branched actor network, as illustrated in Fig. [3](#S4.F3 "Figure
    3 ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning"). These methods learn several control
    submodules for distinct behaviors. A gating control command from high-level route
    planning and behavior planning modules is responsible for the selection of the
    corresponding control submodule.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣
    A Survey of Deep RL and IL for Autonomous Driving Policy Learning")所示，模式2通过考虑高级规划输出来扩展模式1。控制模块由一个通用模型（适用于所有行为）[[104](#bib.bib104),
    [110](#bib.bib110)]或一系列针对不同行为的模型[[105](#bib.bib105), [70](#bib.bib70), [26](#bib.bib26),
    [106](#bib.bib106), [108](#bib.bib108)]组成。陈等人[[110](#bib.bib110)]将检测到的环境车辆和路线投影到鸟瞰图道路地图上，作为策略网络的输入。梁等人[[26](#bib.bib26)]在条件模仿学习（CIL）[[70](#bib.bib70)]的基础上，提出了分支型演员网络，如图[3](#S4.F3
    "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep
    RL and IL for Autonomous Driving Policy Learning")所示。这些方法为不同的行为学习了多个控制子模块。来自高级路线规划和行为规划模块的门控控制命令负责选择相应的控制子模块。
- en: Although Mode 2 mitigates the problems that are encountered with Mode 1, it
    has its own limitations. A general model may be not sufficient for capturing diverse
    behaviors. However, learning a model for each behavior increases the demand for
    training data. Moreover, Mode 2 may be not as computationally efficient as Mode
    1 since it requires high-level planning modules that are determined in advance
    to guide the control module.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模式2缓解了模式1中遇到的问题，但它自身也有局限性。通用模型可能不足以捕捉多样化的行为。然而，为每种行为学习一个模型增加了对训练数据的需求。此外，模式2可能不如模式1在计算上高效，因为它需要事先确定的高级规划模块来指导控制模块。
- en: IV-C Mode 3\. DRL/DIL Integrated Motion Planning
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 模式3\. DRL/DIL集成运动规划
- en: Mode 3 integrates DRL/DIL into the motion planning module, and its architecture
    is illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated
    AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning").
    Utilizing the planning output (e.g., routes and driving decisions) from high-level
    modules, along with the current perception output, DNNs are trained to predict
    future trajectories or paths. DIL models [[111](#bib.bib111), [112](#bib.bib112),
    [113](#bib.bib113), [71](#bib.bib71)] are the mainstream choices for implementing
    this architecture. As illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"), Sun et al. [[112](#bib.bib112)] proposed training a neural
    network that imitates long-term MPC via Sampled-DAgger, where the policy network’s
    input was from perception (obstacles, environment, and current states) and decision-making
    (driving decisions). Alternatively, Wulfmeier et al. [[113](#bib.bib113)] proposed
    projecting the LiDAR point cloud onto a grid map, which is sent to the DNN. The
    DNN is responsible for learning a cost function that guides the motion planning.
    The control part in Mode 3 typically utilizes traditional control techniques such
    as PID [[71](#bib.bib71)] or MPC [[112](#bib.bib112)].
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 模式 3 将 DRL/DIL 集成到运动规划模块中，其架构如图 [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL
    Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy
    Learning") 所示。利用来自高层模块的规划输出（如路线和驾驶决策）以及当前感知输出，DNN 被训练以预测未来轨迹或路径。DIL 模型 [[111](#bib.bib111),
    [112](#bib.bib112), [113](#bib.bib113), [71](#bib.bib71)] 是实现这一架构的主流选择。如图 [3](#S4.F3
    "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep
    RL and IL for Autonomous Driving Policy Learning") 所示，Sun 等 [[112](#bib.bib112)]
    提出了通过 Sampled-DAgger 训练一个模仿长期 MPC 的神经网络，其中策略网络的输入来自感知（障碍物、环境和当前状态）和决策（驾驶决策）。另一种方法是
    Wulfmeier 等 [[113](#bib.bib113)] 提出了将 LiDAR 点云投影到网格地图上，并将其发送到 DNN。DNN 负责学习指导运动规划的成本函数。模式
    3 中的控制部分通常利用传统控制技术，如 PID [[71](#bib.bib71)] 或 MPC [[112](#bib.bib112)]。
- en: One major disadvantage of Mode 3 is that although DNN planned trajectories can
    imitate human trajectories, their safety and feasibility cannot be guaranteed.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 模式 3 的一个主要缺点是，尽管 DNN 规划的轨迹可以模仿人类轨迹，但其安全性和可行性无法得到保证。
- en: 'TABLE II: Comparison of the literature by DRL/DIL integration modes'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 根据 DRL/DIL 集成模式的文献比较'
- en: '| Perception |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 感知 |'
- en: '&#124; Control &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 控制 &#124;'
- en: '&#124; (Modes 1&2) &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (模式 1&2) &#124;'
- en: '|'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Motion Planning &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运动规划 &#124;'
- en: '&#124; (Mode 3) &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (模式 3) &#124;'
- en: '|'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Behavior Planning &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行为规划 &#124;'
- en: '&#124; (Mode 4) &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (模式 4) &#124;'
- en: '|'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hierarchical P. & C. ¹ &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 层级规划与控制¹ &#124;'
- en: '&#124; (Mode 5) &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (模式 5) &#124;'
- en: '|'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DRL | DIL | DRL | DIL | DRL | DIL | DRL | DIL |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| DRL | DIL | DRL | DIL | DRL | DIL | DRL | DIL |'
- en: '| Traditional | 15 | 0 | 2 | 3 | 13 | 1 | 5 | 0 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 传统 | 15 | 0 | 2 | 3 | 13 | 1 | 5 | 0 |'
- en: '| DNN | 3 | 16 | 0 | 1 | 3 | 1 | 1 | 0 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| DNN | 3 | 16 | 0 | 1 | 3 | 1 | 1 | 0 |'
- en: '| Subtotal | 18 (52.9%) | 16 (47.1%) | 2 (33.3%) | 4 (66.7%) | 16 (88.9%) |
    2 (11.1%) | 6 (100%) | 0 (0%) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 小计 | 18 (52.9%) | 16 (47.1%) | 2 (33.3%) | 4 (66.7%) | 16 (88.9%) | 2 (11.1%)
    | 6 (100%) | 0 (0%) |'
- en: '| Total | 34 (53.1%) | 6 (9.4%) | 18 (28.1%) | 6 (9.4%) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 34 (53.1%) | 6 (9.4%) | 18 (28.1%) | 6 (9.4%) |'
- en: '*'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: The values in this table are the numbers and percentages of papers in Table
    [I](#S4.T1 "TABLE I ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning") that belong to the
    corresponding categories.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表中这些值是表 [I](#S4.T1 "TABLE I ‣ IV Architectures of DRL/DIL Integrated AD Systems
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning") 中属于相应类别的论文数量和百分比。
- en: '1'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: Abbreviation for “Hierarchical Planning and Control”
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “层级规划与控制”的缩写
- en: 'TABLE III: A taxonomy of the literature by scenarios and AD tasks'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 根据场景和 AD 任务对文献的分类'
- en: '| Scenario | AD Task | Description | Ref. |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | AD 任务 | 描述 | 参考 |'
- en: '| DRL Methods | DIL Methods |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| DRL 方法 | DIL 方法 |'
- en: '| Urban | Intersection |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 城市 | 交叉口 |'
- en: '&#124; Learn to drive through intersections (while interacting &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过交叉路口学习驾驶（同时与其他交通参与者交互）&#124;'
- en: '&#124; and negotiating with other traffic participants). &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以及与其他交通参与者协商）。&#124;'
- en: '|'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DQN [[119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [90](#bib.bib90)]
    &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQN [[119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [90](#bib.bib90)]
    &#124;'
- en: '&#124; cDQN [[138](#bib.bib138)] &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; cDQN [[138](#bib.bib138)] &#124;'
- en: '&#124; DDPG [[137](#bib.bib137), [138](#bib.bib138)] &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DDPG [[137](#bib.bib137), [138](#bib.bib138)] &#124;'
- en: '| — |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| — |'
- en: '| Roundabout |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 绕行 |'
- en: '&#124; Learn to drive through roundabouts (while interacting &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学会通过环形交叉口驾驶（同时进行交互）&#124;'
- en: '&#124; and negotiating with other traffic participants). &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并与其他交通参与者进行协商）。 &#124;'
- en: '|'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DDQN [[110](#bib.bib110)] &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DDQN [[110](#bib.bib110)] &#124;'
- en: '&#124; TD3 [[110](#bib.bib110)] &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TD3 [[110](#bib.bib110)] &#124;'
- en: '&#124; SAC [[110](#bib.bib110)] &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SAC [[110](#bib.bib110)] &#124;'
- en: '| Horizon GAIL[[139](#bib.bib139)] |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Horizon GAIL[[139](#bib.bib139)] |'
- en: '| Urban Navigation |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 城市导航 |'
- en: '&#124; Learn to drive in urban environments to reach specified &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学会在城市环境中驾驶以达到指定目标。&#124;'
- en: '&#124; objectives by following global routes. &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过遵循全球路线来实现目标。 &#124;'
- en: '| DDPG [[26](#bib.bib26)] |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| DDPG [[26](#bib.bib26)] |'
- en: '&#124; BC [[104](#bib.bib104)] &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BC [[104](#bib.bib104)] &#124;'
- en: '&#124; CIL [[105](#bib.bib105), [70](#bib.bib70), [106](#bib.bib106)] &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CIL [[105](#bib.bib105), [70](#bib.bib70), [106](#bib.bib106)] &#124;'
- en: '&#124; UAIL [[107](#bib.bib107), [108](#bib.bib108)] &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UAIL [[107](#bib.bib107), [108](#bib.bib108)] &#124;'
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Urban Driving | Learn to drive in urban environments without specified objectives.
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 城市驾驶 | 学会在城市环境中驾驶，没有特定的目标。 |'
- en: '&#124; DDQN [[122](#bib.bib122)] &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DDQN [[122](#bib.bib122)] &#124;'
- en: '&#124; PPO [[140](#bib.bib140)] &#124;'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PPO [[140](#bib.bib140)] &#124;'
- en: '&#124; Policy gradient [[136](#bib.bib136)] &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 策略梯度 [[136](#bib.bib136)] &#124;'
- en: '|'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; BC [[65](#bib.bib65), [111](#bib.bib111)] &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BC [[65](#bib.bib65), [111](#bib.bib111)] &#124;'
- en: '&#124; DIRL [[113](#bib.bib113)] &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DIRL [[113](#bib.bib113)] &#124;'
- en: '|'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Highway | Lane Change (LC) | Learn to decide and perform lane changes. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 高速公路 | 车道变换 (LC) | 学会决定并执行车道变换。 |'
- en: '&#124; DQN [[118](#bib.bib118), [91](#bib.bib91)] &#124;'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQN [[118](#bib.bib118), [91](#bib.bib91)] &#124;'
- en: '&#124; DDQN [[141](#bib.bib141)] &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DDQN [[141](#bib.bib141)] &#124;'
- en: '&#124; DDPG [[27](#bib.bib27)] &#124;'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DDPG [[27](#bib.bib27)] &#124;'
- en: '|'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Projection IRL [[117](#bib.bib117)] &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 投影 IRL [[117](#bib.bib117)] &#124;'
- en: '&#124; AIRL [[116](#bib.bib116)] &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AIRL [[116](#bib.bib116)] &#124;'
- en: '|'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Lane Keep (LK) | Learn to drive while maintaining the current lane. |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 车道保持 (LK) | 学会在保持当前车道的情况下驾驶。 |'
- en: '&#124; DQN [[102](#bib.bib102)] &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQN [[102](#bib.bib102)] &#124;'
- en: '&#124; DDPG [[28](#bib.bib28)] &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DDPG [[28](#bib.bib28)] &#124;'
- en: '|'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; BC [[86](#bib.bib86), [87](#bib.bib87)] &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BC [[86](#bib.bib86), [87](#bib.bib87)] &#124;'
- en: '&#124; SafeDAgger [[69](#bib.bib69)] &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SafeDAgger [[69](#bib.bib69)] &#124;'
- en: '|'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Cruise Control | Learn a policy for adaptive cruise control. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 自适应巡航控制 | 学会自适应巡航控制的策略。 |'
- en: '&#124; NQL [[92](#bib.bib92)] &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NQL [[92](#bib.bib92)] &#124;'
- en: '&#124; DQN [[134](#bib.bib134)] &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQN [[134](#bib.bib134)] &#124;'
- en: '&#124; Policy gradient [[142](#bib.bib142)] &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 策略梯度 [[142](#bib.bib142)] &#124;'
- en: '&#124; Actor-critic [[143](#bib.bib143), [144](#bib.bib144)] &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Actor-critic [[143](#bib.bib143), [144](#bib.bib144)] &#124;'
- en: '| — |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| — |'
- en: '| Traffic Merging | Learn to merge into highways. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 交通合并 | 学会合并到高速公路。 |'
- en: '&#124; DQN [[93](#bib.bib93)] &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQN [[93](#bib.bib93)] &#124;'
- en: '&#124; PPO [[96](#bib.bib96)] &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PPO [[96](#bib.bib96)] &#124;'
- en: '| — |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| — |'
- en: '| Highway Driving |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 高速公路驾驶 |'
- en: '&#124; Learn a general policy for driving on a highway, which &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习高速公路驾驶的一般策略，&#124;'
- en: '&#124; may include multiple behaviors such as LC and LK. &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可能包括多种行为，如 LC 和 LK。 &#124;'
- en: '|'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DQN [[145](#bib.bib145), [126](#bib.bib126)] &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQN [[145](#bib.bib145), [126](#bib.bib126)] &#124;'
- en: '&#124; QR-DQN [[133](#bib.bib133)] &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; QR-DQN [[133](#bib.bib133)] &#124;'
- en: '|'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GAIL [[146](#bib.bib146)] &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAIL [[146](#bib.bib146)] &#124;'
- en: '&#124; PS-GAIL [[147](#bib.bib147)] &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PS-GAIL [[147](#bib.bib147)] &#124;'
- en: '&#124; MaxEnt IRL [[148](#bib.bib148)] &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MaxEnt IRL [[148](#bib.bib148)] &#124;'
- en: '|'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Others | Road Following | Learn to simply follow one road. | — |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | 道路跟随 | 学会简单地跟随一条道路。 | — |'
- en: '&#124; BC [[10](#bib.bib10), [85](#bib.bib85), [63](#bib.bib63), [64](#bib.bib64)]
    &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BC [[10](#bib.bib10), [85](#bib.bib85), [63](#bib.bib63), [64](#bib.bib64)]
    &#124;'
- en: '&#124; DAgger [[89](#bib.bib89)] &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DAgger [[89](#bib.bib89)] &#124;'
- en: '|'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Imminent Events | Learn to avoid or mitigate imminent events such as collisions.
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 紧急事件 | 学会避免或缓解诸如碰撞等紧急事件。 |'
- en: '&#124; DQN [[94](#bib.bib94)] &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQN [[94](#bib.bib94)] &#124;'
- en: '&#124; DDPG [[98](#bib.bib98)] &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DDPG [[98](#bib.bib98)] &#124;'
- en: '| RAIL [[149](#bib.bib149)] |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| RAIL [[149](#bib.bib149)] |'
- en: IV-D Mode 4\. DRL/DIL Integrated Behavior Planning
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 模式 4\. DRL/DIL 综合行为规划
- en: Many studies focus on integrating DRL/DIL into the behavior planning module
    and deriving high-level driving policies. The corresponding architecture is presented
    in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"), where DNNs
    decide behavioral actions and the subsequent motion planning and control modules
    typically utilize traditional methods. Many studies build upon DQN and its variants
    [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [126](#bib.bib126), [122](#bib.bib122), [124](#bib.bib124), [125](#bib.bib125),
    [123](#bib.bib123)]. As illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"), Yuan et al. [[126](#bib.bib126)] decomposed the action space
    into five typical behavioral decisions on a highway. Compared with DRL, studies
    that employ DIL to learn high-level policies are limited. A recent study by Wang
    et al. [[116](#bib.bib116)] proposed the use of augmented adversarial inverse
    reinforcement learning (AIRL) to learn human-like decision-making on highways,
    where the action space consists of all possible combinations of lateral and longitudinal
    decisions.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究集中于将 DRL/DIL 集成到行为规划模块中，并推导出高级驾驶策略。相应的架构见图 [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning")，其中 DNNs 决定行为动作，而随后的运动规划和控制模块通常采用传统方法。许多研究基于 DQN 及其变体 [[118](#bib.bib118),
    [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [126](#bib.bib126),
    [122](#bib.bib122), [124](#bib.bib124), [125](#bib.bib125), [123](#bib.bib123)]。如图
    [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning") 所示，Yuan 等人 [[126](#bib.bib126)]
    将动作空间分解为高速公路上的五种典型行为决策。与 DRL 相比，利用 DIL 学习高级策略的研究较少。 Wang 等人 [[116](#bib.bib116)]
    最近提出使用增强的对抗性逆向强化学习 (AIRL) 来学习高速公路上的类人决策，其中动作空间包括所有可能的横向和纵向决策组合。
- en: In Mode 4, the design of behavioral actions is nontrivial, and one must balance
    the training cost and the diversity of driving styles. Simple and few behavioral
    actions limits the control precision and diversity of driving styles, whereas
    many and sophisticated actions increases the training cost.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在模式 4 中，行为动作的设计并不简单，必须平衡训练成本和驾驶风格的多样性。简单且少量的行为动作会限制控制精度和驾驶风格的多样性，而大量且复杂的动作则会增加训练成本。
- en: IV-E Mode 5\. DRL/DIL Integrated Hierarchical Planning and Control
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 模式 5\. DRL/DIL 集成层次规划与控制
- en: 'TABLE IV: Inputs of the DRL/DIL methods for AD tasks'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：用于自动驾驶任务的 DRL/DIL 方法的输入
- en: '| Information Source | Class | Inputs | Ref. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 信息来源 | 类别 | 输入 | 参考文献 |'
- en: '| DRL Methods | DIL Methods |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| DRL 方法 | DIL 方法 |'
- en: '| Ego Vehicle |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 自车 |'
- en: '&#124; Position Information &#124;'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 位置信息 &#124;'
- en: '| ego position | [[110](#bib.bib110), [120](#bib.bib120), [90](#bib.bib90),
    [136](#bib.bib136), [122](#bib.bib122), [93](#bib.bib93), [91](#bib.bib91)] |
    [[147](#bib.bib147), [146](#bib.bib146), [149](#bib.bib149), [111](#bib.bib111)]
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 自车位置 | [[110](#bib.bib110), [120](#bib.bib120), [90](#bib.bib90), [136](#bib.bib136),
    [122](#bib.bib122), [93](#bib.bib93), [91](#bib.bib91)] | [[147](#bib.bib147),
    [146](#bib.bib146), [149](#bib.bib149), [111](#bib.bib111)] |'
- en: '|'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Heading Information &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方向信息 &#124;'
- en: '|'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; heading angle, orientation, &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方向角、方向，&#124;'
- en: '&#124; steering, yaw, and yaw rate &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转向、偏航和偏航速率 &#124;'
- en: '| [[119](#bib.bib119), [121](#bib.bib121), [93](#bib.bib93), [91](#bib.bib91),
    [28](#bib.bib28), [138](#bib.bib138)] | [[147](#bib.bib147), [65](#bib.bib65),
    [64](#bib.bib64), [146](#bib.bib146), [149](#bib.bib149), [139](#bib.bib139)]
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| [[119](#bib.bib119), [121](#bib.bib121), [93](#bib.bib93), [91](#bib.bib91),
    [28](#bib.bib28), [138](#bib.bib138)] | [[147](#bib.bib147), [65](#bib.bib65),
    [64](#bib.bib64), [146](#bib.bib146), [149](#bib.bib149), [139](#bib.bib139)]
    |'
- en: '|'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Speed Information &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 速度信息 &#124;'
- en: '| speed/velocity and acceleration |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 速度/速度和加速度 |'
- en: '&#124; [[138](#bib.bib138), [118](#bib.bib118), [122](#bib.bib122), [91](#bib.bib91),
    [141](#bib.bib141), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [137](#bib.bib137), [90](#bib.bib90), [136](#bib.bib136), [26](#bib.bib26)] &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[138](#bib.bib138), [118](#bib.bib118), [122](#bib.bib122), [91](#bib.bib91),
    [141](#bib.bib141), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [137](#bib.bib137), [90](#bib.bib90), [136](#bib.bib136), [26](#bib.bib26)] &#124;'
- en: '&#124; [[93](#bib.bib93), [102](#bib.bib102), [28](#bib.bib28), [150](#bib.bib150),
    [94](#bib.bib94), [144](#bib.bib144), [134](#bib.bib134), [98](#bib.bib98)] &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[93](#bib.bib93), [102](#bib.bib102), [28](#bib.bib28), [150](#bib.bib150),
    [94](#bib.bib94), [144](#bib.bib144), [134](#bib.bib134), [98](#bib.bib98)] &#124;'
- en: '|'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[139](#bib.bib139), [117](#bib.bib117), [107](#bib.bib107), [108](#bib.bib108),
    [106](#bib.bib106), [105](#bib.bib105), [70](#bib.bib70), [65](#bib.bib65)] &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[139](#bib.bib139), [117](#bib.bib117), [107](#bib.bib107), [108](#bib.bib108),
    [106](#bib.bib106), [105](#bib.bib105), [70](#bib.bib70), [65](#bib.bib65)] &#124;'
- en: '&#124; [[147](#bib.bib147), [148](#bib.bib148), [146](#bib.bib146), [149](#bib.bib149),
    [89](#bib.bib89)] &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[147](#bib.bib147), [148](#bib.bib148), [146](#bib.bib146), [149](#bib.bib149),
    [89](#bib.bib89)] &#124;'
- en: '|'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Road Environment | Pixel Data | camera RGB images | [[126](#bib.bib126),
    [145](#bib.bib145), [27](#bib.bib27), [133](#bib.bib133), [26](#bib.bib26), [28](#bib.bib28)]
    |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 道路环境 | 像素数据 | 相机 RGB 图像 | [[126](#bib.bib126), [145](#bib.bib145), [27](#bib.bib27),
    [133](#bib.bib133), [26](#bib.bib26), [28](#bib.bib28)] |'
- en: '&#124; [[69](#bib.bib69), [86](#bib.bib86), [87](#bib.bib87), [107](#bib.bib107),
    [108](#bib.bib108), [106](#bib.bib106), [105](#bib.bib105), [70](#bib.bib70),
    [65](#bib.bib65), [104](#bib.bib104)] &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[69](#bib.bib69), [86](#bib.bib86), [87](#bib.bib87), [107](#bib.bib107),
    [108](#bib.bib108), [106](#bib.bib106), [105](#bib.bib105), [70](#bib.bib70),
    [65](#bib.bib65), [104](#bib.bib104)] &#124;'
- en: '&#124; [[10](#bib.bib10), [19](#bib.bib19), [85](#bib.bib85), [89](#bib.bib89),
    [63](#bib.bib63), [64](#bib.bib64)] &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[10](#bib.bib10), [19](#bib.bib19), [85](#bib.bib85), [89](#bib.bib89),
    [63](#bib.bib63), [64](#bib.bib64)] &#124;'
- en: '|'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; semantically segmented images &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语义分割图像 &#124;'
- en: '| [[98](#bib.bib98)] | — |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| [[98](#bib.bib98)] | — |'
- en: '| 2D bird’s-eye-view images | [[96](#bib.bib96)] | — |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 2D 鸟瞰图像 | [[96](#bib.bib96)] | — |'
- en: '| Point Data | LiDAR sensor readings | [[126](#bib.bib126), [145](#bib.bib145),
    [133](#bib.bib133)] | [[147](#bib.bib147), [146](#bib.bib146), [149](#bib.bib149),
    [139](#bib.bib139)] |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 点数据 | LiDAR 传感器读数 | [[126](#bib.bib126), [145](#bib.bib145), [133](#bib.bib133)]
    | [[147](#bib.bib147), [146](#bib.bib146), [149](#bib.bib149), [139](#bib.bib139)]
    |'
- en: '| 2D LiDAR grid map | — | [[113](#bib.bib113)] |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 2D LiDAR 网格地图 | — | [[113](#bib.bib113)] |'
- en: '| Object Data |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 对象数据 |'
- en: '&#124; other road users’ information: &#124;'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 其他道路使用者的信息: &#124;'
- en: '&#124; relative speed, position, &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相对速度，位置, &#124;'
- en: '&#124; and distance to ego &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与自车的距离 &#124;'
- en: '|'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[141](#bib.bib141), [118](#bib.bib118), [119](#bib.bib119), [110](#bib.bib110),
    [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [138](#bib.bib138)]
    &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[141](#bib.bib141), [118](#bib.bib118), [119](#bib.bib119), [110](#bib.bib110),
    [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [138](#bib.bib138)]
    &#124;'
- en: '&#124; [[93](#bib.bib93), [134](#bib.bib134), [142](#bib.bib142), [94](#bib.bib94),
    [92](#bib.bib92), [144](#bib.bib144), [143](#bib.bib143), [96](#bib.bib96)] &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[93](#bib.bib93), [134](#bib.bib134), [142](#bib.bib142), [94](#bib.bib94),
    [92](#bib.bib92), [144](#bib.bib144), [143](#bib.bib143), [96](#bib.bib96)] &#124;'
- en: '| [[147](#bib.bib147), [149](#bib.bib149), [116](#bib.bib116), [111](#bib.bib111)]
    |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| [[147](#bib.bib147), [149](#bib.bib149), [116](#bib.bib116), [111](#bib.bib111)]
    |'
- en: '|'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; lane/road information: &#124;'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 车道/道路信息: &#124;'
- en: '&#124; ego vehicle’s distance to &#124;'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自车与 &#124;'
- en: '&#124; lane markings, road &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 车道标记，道路 &#124;'
- en: '&#124; curvature, and lane width &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 曲率和车道宽度 &#124;'
- en: '| [[118](#bib.bib118), [134](#bib.bib134), [137](#bib.bib137), [122](#bib.bib122),
    [93](#bib.bib93), [91](#bib.bib91), [102](#bib.bib102), [141](#bib.bib141)] |
    [[147](#bib.bib147), [146](#bib.bib146), [149](#bib.bib149), [116](#bib.bib116)]
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118), [134](#bib.bib134), [137](#bib.bib137), [122](#bib.bib122),
    [93](#bib.bib93), [91](#bib.bib91), [102](#bib.bib102), [141](#bib.bib141)] |
    [[147](#bib.bib147), [146](#bib.bib146), [149](#bib.bib149), [116](#bib.bib116)]
    |'
- en: '| Task |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 任务 |'
- en: '&#124; Navigation Information &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 导航信息 &#124;'
- en: '|'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; navigational driving commands &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 导航驾驶指令 &#124;'
- en: '&#124; or planned routes &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 或计划路线 &#124;'
- en: '| [[110](#bib.bib110), [26](#bib.bib26)] | [[106](#bib.bib106), [105](#bib.bib105),
    [70](#bib.bib70), [104](#bib.bib104), [111](#bib.bib111)] |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110), [26](#bib.bib26)] | [[106](#bib.bib106), [105](#bib.bib105),
    [70](#bib.bib70), [104](#bib.bib104), [111](#bib.bib111)] |'
- en: '|'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Destination Information &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目的地信息 &#124;'
- en: '|'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; destination position, distance &#124;'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目标位置，距离 &#124;'
- en: '&#124; or angle to destination &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 或目标角度 &#124;'
- en: '| — | [[139](#bib.bib139)] |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| — | [[139](#bib.bib139)] |'
- en: '|'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Traffic Rule Information &#124;'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交通规则信息 &#124;'
- en: '|'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; traffic lights’ state, speed &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交通灯状态，速度 &#124;'
- en: '&#124; limit, and desired speed &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 限速和期望速度 &#124;'
- en: '| [[136](#bib.bib136)] | [[148](#bib.bib148), [111](#bib.bib111)] |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| [[136](#bib.bib136)] | [[148](#bib.bib148), [111](#bib.bib111)] |'
- en: '|'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Prior Knowledge &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 先前知识 &#124;'
- en: '| Road Map |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 道路地图 |'
- en: '&#124; 2D top-down road map images &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2D 自上而下的道路地图图像 &#124;'
- en: '| [[110](#bib.bib110)] | [[111](#bib.bib111)] | ![Refer to caption](img/f73b681db058067cf0b5dac103ddf3ca.png)![Refer
    to caption](img/b36837e52cf72dbe151ee1cc0565a120.png)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '| [[110](#bib.bib110)] | [[111](#bib.bib111)] | ![参考说明](img/f73b681db058067cf0b5dac103ddf3ca.png)![参考说明](img/b36837e52cf72dbe151ee1cc0565a120.png)'
- en: 'Figure 4: The percentages of the literature that uses certain data as input.
    (a) Comparison by DRL/DIL methods. (b) Comparison by scenarios.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用某些数据作为输入的文献百分比。（a）按DRL/DIL方法的比较。（b）按场景的比较。
- en: As illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated
    AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"),
    Mode 5 blurs the lines between planning and control, where a single DNN outputs
    hierarchical actions [[27](#bib.bib27)] based on the parameterized action space
    [[151](#bib.bib151)] or hierarchical DNNs output actions at multiple levels [[136](#bib.bib136),
    [134](#bib.bib134), [135](#bib.bib135)]. Rezaee et al. [[134](#bib.bib134)] proposed
    an architecture, which is illustrated in Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures
    of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"), in which BP (behavior planning) is used to make high-level
    decisions regarding transitions between discrete states and MoP (motion planning)
    generates a target trajectory according to the decisions that are made via BP.
    Qiao et al. [[137](#bib.bib137)] built upon hierarchical MDP (HMDP) and realized
    their model through an options framework. In their implementation, the hierarchical
    options were modeled as high-level decisions (SlowForward or Forward). Based on
    high-level decisions and current observations, low-level control policies were
    derived.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣
    A Survey of Deep RL and IL for Autonomous Driving Policy Learning")所示，模式5模糊了规划与控制之间的界限，其中单个DNN基于参数化的动作空间[[151](#bib.bib151)]输出层次化动作[[27](#bib.bib27)]，或层次化DNN在多个层次上输出动作[[136](#bib.bib136),
    [134](#bib.bib134), [135](#bib.bib135)]。Rezaee等人[[134](#bib.bib134)]提出了一种架构，如图[3](#S4.F3
    "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep
    RL and IL for Autonomous Driving Policy Learning")所示，其中BP（行为规划）用于做出关于离散状态之间转变的高层决策，而MoP（运动规划）根据通过BP做出的决策生成目标轨迹。Qiao等人[[137](#bib.bib137)]在层次化MDP（HMDP）的基础上进行了扩展，并通过选项框架实现了他们的模型。在他们的实现中，层次化选项被建模为高层决策（SlowForward或Forward）。基于高层决策和当前观察，推导出低层控制策略。
- en: Mode 5 simultaneously plans at multiple levels, and the low-level planning process
    considers high-level decisions. However, the use of hierarchical DNNs results
    in the increase of the training cost and potentially the decrease of the convergence
    speed since one poorly trained high-level DNN may mislead and disturb the learning
    process of low-level DNNs.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 模式5在多个层次上同时进行规划，低层规划过程考虑高层决策。然而，使用层次化DNN会增加训练成本，并可能降低收敛速度，因为一个训练不良的高层DNN可能误导并干扰低层DNN的学习过程。
- en: IV-F Statistical Comparison
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 统计比较
- en: 'A statistical comparison among modes in terms of the number of studies is presented
    in Table [II](#S4.T2 "TABLE II ‣ IV-C Mode 3\. DRL/DIL Integrated Motion Planning
    ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning"). Current studies on architectures
    are premature, unsystematic and unbalanced:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 表[II](#S4.T2 "TABLE II ‣ IV-C Mode 3\. DRL/DIL Integrated Motion Planning ‣
    IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL
    for Autonomous Driving Policy Learning")中展示了各模式在研究数量方面的统计比较。目前关于架构的研究尚处于初期阶段，缺乏系统性和均衡性：
- en: •
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Most studies focus on integrating DRL/DIL into control (Mode1&2), followed by
    behavior planning (Mode 4).
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数研究集中在将DRL/DIL集成到控制中（模式1&2），其次是行为规划（模式4）。
- en: •
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For Mode 1&2, DRL methods mainly adopt traditional method-based perception,
    while DNN-based perception is preferred in DIL methods.
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于模式1&2，DRL方法主要采用传统的方法感知，而DNN基础的感知在DIL方法中更为受青睐。
- en: •
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DRL seems to be more popular for high-level decision making (Mode 4&5), while
    DIL is chosen more frequently for low-level control (Mode 1&2).
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DRL似乎在高层决策（模式4&5）中更为流行，而DIL则在低层控制（模式1&2）中使用得更频繁。
- en: Future studies may address the imbalance problem and identify potential new
    architectures.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究可能会解决不平衡问题并识别潜在的新架构。
- en: V Task-Driven Methods
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 任务驱动方法
- en: 'DRL/DIL studies in AD can be categorized according to their application scenarios
    and targeted tasks, as listed in Table [III](#S4.T3 "TABLE III ‣ IV-C Mode 3\.
    DRL/DIL Integrated Motion Planning ‣ IV Architectures of DRL/DIL Integrated AD
    Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning").
    These task-driven studies use DRL/DIL to solve specified AD tasks, where the formulations
    of DRL/DIL can be decomposed into several key components: 1) state space and input
    design, 2) action space and output design, and 3) reinforcement learning reward
    design.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: DRL/DIL 在自动驾驶中的研究可以根据其应用场景和目标任务进行分类，如表 [III](#S4.T3 "TABLE III ‣ IV-C Mode 3\.
    DRL/DIL Integrated Motion Planning ‣ IV Architectures of DRL/DIL Integrated AD
    Systems ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning")
    所示。这些以任务为驱动的研究使用 DRL/DIL 来解决特定的自动驾驶任务，其中 DRL/DIL 的表述可以分解为几个关键组件：1) 状态空间和输入设计，2)
    动作空间和输出设计，3) 强化学习奖励设计。
- en: 'TABLE V: Actions of the DRL/DIL methods for AD tasks'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：DRL/DIL 方法在自动驾驶任务中的动作
- en: '| Action Category | Subclass | Action Outputs | Ref |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 动作类别 | 子类 | 动作输出 | 参考文献 |'
- en: '| DRL Methods | DIL Methods |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| DRL 方法 | DIL 方法 |'
- en: '| Behavior-level Actions |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 行为级动作 |'
- en: '&#124; Acceleration &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 加速度 &#124;'
- en: '&#124; Related &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相关 &#124;'
- en: '| e.g., full brake, decelerate, continue, and accelerate | [[119](#bib.bib119),
    [121](#bib.bib121), [122](#bib.bib122), [142](#bib.bib142), [145](#bib.bib145),
    [126](#bib.bib126), [133](#bib.bib133)] | — |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 例如，完全刹车、减速、继续和加速 | [[119](#bib.bib119), [121](#bib.bib121), [122](#bib.bib122),
    [142](#bib.bib142), [145](#bib.bib145), [126](#bib.bib126), [133](#bib.bib133)]
    | — |'
- en: '|'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Lane Change &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 车道变换 &#124;'
- en: '&#124; Related &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相关 &#124;'
- en: '|'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; e.g., keep, LLC, and RLC; &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 例如，保持、左车道变换和右车道变换；&#124;'
- en: '&#124; choice of lane change gaps &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 车道变换间隙选择 &#124;'
- en: '| [[122](#bib.bib122), [141](#bib.bib141), [118](#bib.bib118), [145](#bib.bib145),
    [133](#bib.bib133)] | [[116](#bib.bib116)] |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| [[122](#bib.bib122), [141](#bib.bib141), [118](#bib.bib118), [145](#bib.bib145),
    [133](#bib.bib133)] | [[116](#bib.bib116)] |'
- en: '|'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Turn &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转弯 &#124;'
- en: '&#124; Related &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相关 &#124;'
- en: '| e.g., straight, left-turn, right-turn, and stop | [[126](#bib.bib126)] |
    [[117](#bib.bib117), [65](#bib.bib65)] |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 例如，直行、左转、右转和停车 | [[126](#bib.bib126)] | [[117](#bib.bib117), [65](#bib.bib65)]
    |'
- en: '|'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Interaction &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互动 &#124;'
- en: '&#124; Related &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相关 &#124;'
- en: '| e.g., take/give way and follow a vehicle; wait/go | [[120](#bib.bib120),
    [121](#bib.bib121)] | — |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 例如，避让/让行和跟随车辆；等待/前进 | [[120](#bib.bib120), [121](#bib.bib121)] | — |'
- en: '|'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Trajectory-level &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轨迹级别 &#124;'
- en: '&#124; Actions &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 动作 &#124;'
- en: '|'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Planned &#124;'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 规划 &#124;'
- en: '&#124; Trajectory &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轨迹 &#124;'
- en: '| future path 2D points | — | [[113](#bib.bib113), [111](#bib.bib111)] |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 未来路径 2D 点 | — | [[113](#bib.bib113), [111](#bib.bib111)] |'
- en: '| Control-level Actions | Lateral | discrete steer angles | [[19](#bib.bib19)]
    | [[10](#bib.bib10)] |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 控制级动作 | 侧向 | 离散转向角度 | [[19](#bib.bib19)] | [[10](#bib.bib10)] |'
- en: '| continuous steer | — | [[87](#bib.bib87), [86](#bib.bib86), [85](#bib.bib85),
    [63](#bib.bib63), [64](#bib.bib64)] |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 连续转向 | — | [[87](#bib.bib87), [86](#bib.bib86), [85](#bib.bib85), [63](#bib.bib63),
    [64](#bib.bib64)] |'
- en: '|'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; continuous angular speed &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 连续角速度 &#124;'
- en: '&#124; or yaw acceleration &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 或偏航加速度 &#124;'
- en: '| [[91](#bib.bib91)] | [[65](#bib.bib65)] |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| [[91](#bib.bib91)] | [[65](#bib.bib65)] |'
- en: '| Longitudinal | discrete acceleration values | [[90](#bib.bib90), [94](#bib.bib94)]
    | — |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 纵向 | 离散加速度值 | [[90](#bib.bib90), [94](#bib.bib94)] | — |'
- en: '| continuous acceleration | [[92](#bib.bib92), [143](#bib.bib143), [144](#bib.bib144)]
    | — |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 连续加速度 | [[92](#bib.bib92), [143](#bib.bib143), [144](#bib.bib144)] | — |'
- en: '| continuous brake and throttle | [[150](#bib.bib150)] | — |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 连续刹车和油门 | [[150](#bib.bib150)] | — |'
- en: '| Simultaneous Lateral & Longitudinal |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 同时侧向与纵向 |'
- en: '&#124; continuous steer/turn-rate and &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 连续转向/转向率和 &#124;'
- en: '&#124; speed/acceleration/throttle &#124;'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 速度/加速度/油门 &#124;'
- en: '| [[93](#bib.bib93), [110](#bib.bib110), [96](#bib.bib96), [28](#bib.bib28)]
    | [[70](#bib.bib70), [104](#bib.bib104), [106](#bib.bib106), [89](#bib.bib89),
    [107](#bib.bib107), [146](#bib.bib146), [147](#bib.bib147)] |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93), [110](#bib.bib110), [96](#bib.bib96), [28](#bib.bib28)]
    | [[70](#bib.bib70), [104](#bib.bib104), [106](#bib.bib106), [89](#bib.bib89),
    [107](#bib.bib107), [146](#bib.bib146), [147](#bib.bib147)] |'
- en: '|'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; continuous steer, &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 连续转向，&#124;'
- en: '&#124; acceleration/throttle and brake &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 加速度/油门和刹车 &#124;'
- en: '| [[102](#bib.bib102), [26](#bib.bib26), [98](#bib.bib98)] | [[105](#bib.bib105),
    [108](#bib.bib108)] |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| [[102](#bib.bib102), [26](#bib.bib26), [98](#bib.bib98)] | [[105](#bib.bib105),
    [108](#bib.bib108)] |'
- en: '| continuous steer and binary brake decision | — | [[69](#bib.bib69)] |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 连续转向和二元刹车决策 | — | [[69](#bib.bib69)] |'
- en: '| Hierarchical Actions | Behavior & Control |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 分层动作 | 行为与控制 |'
- en: '&#124; e.g., behavior-level pass/stop, &#124;'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 例如，行为级别的通过/停止，&#124;'
- en: '&#124; control-level acceleration, and steer &#124;'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 控制级别的加速度和转向 &#124;'
- en: '| [[138](#bib.bib138), [137](#bib.bib137), [136](#bib.bib136), [27](#bib.bib27)]
    | — |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138), [137](#bib.bib137), [136](#bib.bib136), [27](#bib.bib27)]
    | — |'
- en: '| Behavior & Trajectory |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 行为 & 轨迹 |'
- en: '&#124; e.g., behavior-level maintenance, LLC, RLC &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 例如，行为级别维护，LLC，RLC &#124;'
- en: '&#124; and trajectory-level path points &#124;'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和轨迹级别的路径点 &#124;'
- en: '| [[134](#bib.bib134)] | — |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| [[134](#bib.bib134)] | — |'
- en: 'TABLE VI: Rewards of the DRL methods for AD tasks'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 自适应驾驶任务的 DRL 方法的奖励'
- en: '| Category | Subclass | Description | Ref. |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 子类 | 描述 | 参考 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Safety | Avoid Collision | Impose penalties if a collision occurs |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 安全 | 避免碰撞 | 如果发生碰撞则施加惩罚 |'
- en: '&#124; [[118](#bib.bib118), [26](#bib.bib26), [105](#bib.bib105), [119](#bib.bib119),
    [122](#bib.bib122), [116](#bib.bib116), [110](#bib.bib110), [120](#bib.bib120),
    [121](#bib.bib121), [137](#bib.bib137), [90](#bib.bib90), [138](#bib.bib138)]
    &#124;'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[118](#bib.bib118), [26](#bib.bib26), [105](#bib.bib105), [119](#bib.bib119),
    [122](#bib.bib122), [116](#bib.bib116), [110](#bib.bib110), [120](#bib.bib120),
    [121](#bib.bib121), [137](#bib.bib137), [90](#bib.bib90), [138](#bib.bib138)]
    &#124;'
- en: '&#124; [[94](#bib.bib94), [143](#bib.bib143), [149](#bib.bib149), [96](#bib.bib96),
    [69](#bib.bib69), [126](#bib.bib126), [145](#bib.bib145), [133](#bib.bib133),
    [98](#bib.bib98)] &#124;'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[94](#bib.bib94), [143](#bib.bib143), [149](#bib.bib149), [96](#bib.bib96),
    [69](#bib.bib69), [126](#bib.bib126), [145](#bib.bib145), [133](#bib.bib133),
    [98](#bib.bib98)] &#124;'
- en: '|'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Time to Collision |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 碰撞时间 |'
- en: '&#124; Impose penalties if the time to collision (TTC) is below &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果碰撞时间 (TTC) 低于 &#124;'
- en: '&#124; a safe threshold &#124;'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个安全阈值 &#124;'
- en: '| [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [122](#bib.bib122),
    [144](#bib.bib144)] |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [122](#bib.bib122),
    [144](#bib.bib144)] |'
- en: '| Distance to Other Vehicles |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 距离其他车辆 |'
- en: '&#124; Impose penalties if this distance is shorter than a safe threshold &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果距离短于安全阈值，则施加惩罚 &#124;'
- en: '| [[134](#bib.bib134), [142](#bib.bib142), [144](#bib.bib144), [93](#bib.bib93)]
    |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| [[134](#bib.bib134), [142](#bib.bib142), [144](#bib.bib144), [93](#bib.bib93)]
    |'
- en: '| Number of Lane Changes |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 车道变换数量 |'
- en: '&#124; Impose penalties if the number of lane changes is too &#124;'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果车道变换数量过多，则施加惩罚 &#124;'
- en: '&#124; large or reward a smaller number of lane changes &#124;'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 奖励较少的车道变换数量 &#124;'
- en: '| [[118](#bib.bib118), [134](#bib.bib134), [126](#bib.bib126), [145](#bib.bib145),
    [133](#bib.bib133), [141](#bib.bib141)] |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118), [134](#bib.bib134), [126](#bib.bib126), [145](#bib.bib145),
    [133](#bib.bib133), [141](#bib.bib141)] |'
- en: '| Out of Road | Impose penalties on driving out of road | [[118](#bib.bib118),
    [27](#bib.bib27), [102](#bib.bib102), [19](#bib.bib19), [96](#bib.bib96)] |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 离开道路 | 对驶离道路的行为施加惩罚 | [[118](#bib.bib118), [27](#bib.bib27), [102](#bib.bib102),
    [19](#bib.bib19), [96](#bib.bib96)] |'
- en: '| Efficiency | Speed |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 效率 | 速度 |'
- en: '&#124; Reward higher speed until the maximum speed limit is reached; &#124;'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 奖励更高的速度，直到达到最高速度限制； &#124;'
- en: '&#124; Impose penalties if the speed is lower than the minimum speed limit
    &#124;'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果速度低于最低速度限制，则施加惩罚 &#124;'
- en: '|'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[138](#bib.bib138), [118](#bib.bib118), [141](#bib.bib141), [119](#bib.bib119),
    [122](#bib.bib122), [110](#bib.bib110), [136](#bib.bib136), [26](#bib.bib26),
    [105](#bib.bib105), [140](#bib.bib140)] &#124;'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[138](#bib.bib138), [118](#bib.bib118), [141](#bib.bib141), [119](#bib.bib119),
    [122](#bib.bib122), [110](#bib.bib110), [136](#bib.bib136), [26](#bib.bib26),
    [105](#bib.bib105), [140](#bib.bib140)] &#124;'
- en: '&#124; [[133](#bib.bib133), [126](#bib.bib126), [145](#bib.bib145), [134](#bib.bib134),
    [93](#bib.bib93), [102](#bib.bib102), [28](#bib.bib28), [96](#bib.bib96), [27](#bib.bib27),
    [150](#bib.bib150)] &#124;'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[133](#bib.bib133), [126](#bib.bib126), [145](#bib.bib145), [134](#bib.bib134),
    [93](#bib.bib93), [102](#bib.bib102), [28](#bib.bib28), [96](#bib.bib96), [27](#bib.bib27),
    [150](#bib.bib150)] &#124;'
- en: '|'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Success | Reward the agent if it finished the task successfully | [[118](#bib.bib118),
    [120](#bib.bib120), [121](#bib.bib121), [137](#bib.bib137), [90](#bib.bib90),
    [143](#bib.bib143), [116](#bib.bib116), [96](#bib.bib96), [138](#bib.bib138)]
    |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| 成功 | 如果代理成功完成任务，则给予奖励 | [[118](#bib.bib118), [120](#bib.bib120), [121](#bib.bib121),
    [137](#bib.bib137), [90](#bib.bib90), [143](#bib.bib143), [116](#bib.bib116),
    [96](#bib.bib96), [138](#bib.bib138)] |'
- en: '| Number of Overtakes | Reward a higher number of overtakes for efficiency
    | [[126](#bib.bib126), [145](#bib.bib145), [27](#bib.bib27), [133](#bib.bib133)]
    |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| 超车数量 | 奖励更多的超车以提高效率 | [[126](#bib.bib126), [145](#bib.bib145), [27](#bib.bib27),
    [133](#bib.bib133)] |'
- en: '| Time |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 碰撞时间 |'
- en: '&#124; Impose a negative reward in each step to encourage the agent finish
    &#124;'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在每一步施加负奖励，以鼓励代理完成 &#124;'
- en: '&#124; the task faster or penalize the agent if the task cannot &#124;'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 更快完成任务，或者如果任务无法完成，则对代理施加惩罚&#124;'
- en: '&#124; be finished within a time threshold &#124;'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在时间阈值内完成&#124;'
- en: '| [[110](#bib.bib110), [121](#bib.bib121), [91](#bib.bib91), [137](#bib.bib137)]
    |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110), [121](#bib.bib121), [91](#bib.bib91), [137](#bib.bib137)]
    |'
- en: '| Distance to the Destination | Provide a larger reward the closer the agent
    is to the destination | [[137](#bib.bib137), [136](#bib.bib136), [105](#bib.bib105)]
    |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 到目的地的距离 | 代理离目的地越近，奖励越大 | [[137](#bib.bib137), [136](#bib.bib136), [105](#bib.bib105)]
    |'
- en: '| Comfort | Jerk |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 舒适度 | 振动 |'
- en: '&#124; Impose penalties if the longitudinal or lateral control &#124;'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果纵向或横向控制&#124;'
- en: '&#124; is too urgent &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 过于急迫&#124;'
- en: '| [[110](#bib.bib110), [120](#bib.bib120), [136](#bib.bib136), [93](#bib.bib93),
    [91](#bib.bib91), [150](#bib.bib150), [144](#bib.bib144), [149](#bib.bib149),
    [96](#bib.bib96), [138](#bib.bib138)] |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110), [120](#bib.bib120), [136](#bib.bib136), [93](#bib.bib93),
    [91](#bib.bib91), [150](#bib.bib150), [144](#bib.bib144), [149](#bib.bib149),
    [96](#bib.bib96), [138](#bib.bib138)] |'
- en: '| Traffic Rules | Lane Mark Invasion | Impose penalties if the agent invades
    the lane marks | [[26](#bib.bib26), [105](#bib.bib105), [149](#bib.bib149)] |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 交通规则 | 车道标记入侵 | 如果代理侵入车道标记，则施加惩罚 | [[26](#bib.bib26), [105](#bib.bib105),
    [149](#bib.bib149)] |'
- en: '| Distance to the Lane Centerlines |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 到车道中心线的距离 |'
- en: '&#124; Impose penalties if the agent deviates from the lane &#124;'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果代理偏离车道，则施加惩罚&#124;'
- en: '&#124; centerlines or routing baselines &#124;'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 中线或路径基线&#124;'
- en: '| [[110](#bib.bib110), [140](#bib.bib140), [27](#bib.bib27), [19](#bib.bib19),
    [96](#bib.bib96), [138](#bib.bib138)] |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110), [140](#bib.bib140), [27](#bib.bib27), [19](#bib.bib19),
    [96](#bib.bib96), [138](#bib.bib138)] |'
- en: '| Wrong Lane |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 错误车道 |'
- en: '&#124; Impose penalties if the agent is in the wrong lane, e.g., &#124;'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果代理在错误车道中，则施加惩罚，例如，&#124;'
- en: '&#124; staying in the left-turn lane if the assigned route is straight &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果指定路线是直线，却停留在左转车道中&#124;'
- en: '| [[122](#bib.bib122)] |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| [[122](#bib.bib122)] |'
- en: '| Blocking Traffic |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 阻碍交通 |'
- en: '&#124; Impose penalties if the agent blocks the future paths of other &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果代理阻挡了其他车辆的未来路径，则施加惩罚&#124;'
- en: '&#124; vehicles that have the right of way &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 具有优先通行权的车辆&#124;'
- en: '| [[137](#bib.bib137)] | ![Refer to caption](img/bac3a079625b66bd6f00830622a7d6c0.png)'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '| [[137](#bib.bib137)] | ![参见说明](img/bac3a079625b66bd6f00830622a7d6c0.png)'
- en: 'Figure 5: A taxonomy of the literature on how driving safety is addressed by
    DRL/DIL models. (a)-(c) Three main methods with typical examples in literature.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 文献中有关如何通过DRL/DIL模型解决驾驶安全问题的分类。(a)-(c) 三种主要方法及其在文献中的典型示例。'
- en: '![Refer to caption](img/1065a5f8a0878a9c4791f28c43898f1b.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1065a5f8a0878a9c4791f28c43898f1b.png)'
- en: 'Figure 6: Reinforcement learning rewards for AD tasks.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 强化学习在自动驾驶任务中的奖励机制。'
- en: V-A State Space and Input Design
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 状态空间和输入设计
- en: Table [IV](#S4.T4 "TABLE IV ‣ IV-E Mode 5\. DRL/DIL Integrated Hierarchical
    Planning and Control ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning") classifies commonly
    used inputs according to information source (ego vehicle/road environment/task/prior
    knowledge). A statistical comparison of the percentages of input classes that
    are used in DRL/DIL methods is presented in Fig. [4](#S4.F4 "Figure 4 ‣ IV-E Mode
    5\. DRL/DIL Integrated Hierarchical Planning and Control ‣ IV Architectures of
    DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"). Compared with the task and prior knowledge, the ego vehicle
    and road environment seem to be more popular information sources. In all input
    classes, the ego vehicle speed, pixel data (e.g., camera images) and object data
    (e.g., other road users’ relative speeds and positions) are the most commonly
    used. Another significant difference between DRL and DIL is that DRL models prefer
    object data while DIL models prefer pixel data. The selection of low-dimensional
    object data rather than high-dimensional pixel data as input for DRL models renders
    the problem more tractable and accelerates the training procedure. Fig. [4](#S4.F4
    "Figure 4 ‣ IV-E Mode 5\. DRL/DIL Integrated Hierarchical Planning and Control
    ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning") presents a statistical comparison
    of preferences for input classes in various scenarios. Input from the task (e.g.,
    goal positions) and prior knowledge (e.g., road maps) are mostly used in urban
    scenarios. Point data (e.g., LiDAR sensor readings) and object data are more commonly
    used in highway scenarios.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 表[IV](#S4.T4 "TABLE IV ‣ IV-E Mode 5\. DRL/DIL Integrated Hierarchical Planning
    and Control ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of
    Deep RL and IL for Autonomous Driving Policy Learning")根据信息源（自车/道路环境/任务/先验知识）对常用输入进行了分类。图[4](#S4.F4
    "Figure 4 ‣ IV-E Mode 5\. DRL/DIL Integrated Hierarchical Planning and Control
    ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning")展示了DRL/DIL方法中各输入类别的百分比统计比较。与任务和先验知识相比，自车和道路环境似乎是更常见的信息源。在所有输入类别中，自车速度、像素数据（例如，相机图像）和物体数据（例如，其他道路使用者的相对速度和位置）是最常用的。DRL与DIL的另一个显著区别在于，DRL模型更倾向于使用物体数据，而DIL模型更倾向于使用像素数据。将低维物体数据而非高维像素数据作为DRL模型的输入，使得问题更易于处理，并加快了训练过程。图[4](#S4.F4
    "Figure 4 ‣ IV-E Mode 5\. DRL/DIL Integrated Hierarchical Planning and Control
    ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning")展示了各种场景下输入类别偏好的统计比较。任务输入（例如，目标位置）和先验知识（例如，地图）在城市场景中使用较多。点数据（例如，LiDAR传感器读数）和物体数据在高速公路场景中更为常见。
- en: Aside from deciding the choice of input, the application of a dynamic input
    size is an important problem since the number of cars or pedestrians in the ego’s
    vicinity varies over time. Unfortunately, standard DRL/DIL methods rely on inputs
    of fixed size. The use of occupancy grid maps as inputs of CNNs is a practical
    solution [[121](#bib.bib121), [119](#bib.bib119)]. However, this solution imposes
    a trade-off between computational workload and expressiveness. Low-resolution
    grids decrease the computational burden at the cost of being imprecise in their
    representation of the environment, whereas for high-resolution grids, most computations
    may be redundant due to sparsity of the grid maps. Furthermore, a grid map is
    still limited by its defined size, and agents outside this region is neglected.
    Alternatively, Everett et al. [[152](#bib.bib152)] proposed to leverage LSTMs’
    ability to encode a variable number of agents’ observations. Huegle et al. [[141](#bib.bib141)]
    suggested the use of deep sets [[153](#bib.bib153)] as a flexible and permutation-invariant
    architecture to handle dynamic input. Dynamic input remains an open topic for
    future studies.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 除了决定输入选择之外，动态输入大小的应用也是一个重要问题，因为自车周围的汽车或行人数量随着时间变化。不幸的是，标准的DRL/DIL方法依赖于固定大小的输入。使用占据网格地图作为CNN输入是一种实际解决方案[[121](#bib.bib121),
    [119](#bib.bib119)]。然而，这种解决方案在计算工作负载和表达能力之间存在权衡。低分辨率网格减少了计算负担，但在环境表示上存在不准确的问题，而高分辨率网格由于网格地图的稀疏性，大部分计算可能是多余的。此外，网格地图仍然受限于其定义的大小，超出该区域的代理被忽略。另一方面，Everett等人[[152](#bib.bib152)]建议利用LSTM编码可变数量的代理观察能力。Huegle等人[[141](#bib.bib141)]建议使用深度集合[[153](#bib.bib153)]作为一种灵活且对排列不变的架构来处理动态输入。动态输入仍然是未来研究的一个开放话题。
- en: V-B Action Space and Output Design
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 动作空间和输出设计
- en: 'A self-driving DRL/DIL agent can plan at different levels of abstraction, namely,
    low-level control, high-level behavioral planning and trajectory planning, or
    even at multiple levels simultaneously. According to this, Table [V](#S5.T5 "TABLE
    V ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning") categorizes mainstream action spaces into four groups: behavior-level
    actions, trajectory-level actions, control-level actions and hierarchical actions.
    Behavior-level actions are usually designed according to specified tasks. For
    speed control, acceleration-related actions (e.g., full brake, decelerate, continue,
    and accelerate [[119](#bib.bib119)]) are commonly used. Lane change actions (e.g.,
    keep/LLC/LRC) and turn actions (e.g., turn left/right/go straight) are preferred
    in highway scenarios and urban scenarios, respectively. Trajectory-level actions
    refer to the planned or predicted trajectories/paths [[113](#bib.bib113), [111](#bib.bib111)],
    which are typically composed of future path 2D points. Control-level actions refer
    to low-level control commands (e.g., steer, acceleration, throttle, and brake),
    which are divided into three classes: lateral control, longitudinal control and
    simultaneous lateral and longitudinal control. Early studies focused mainly on
    discrete lateral [[10](#bib.bib10), [19](#bib.bib19)] or discrete longitudinal
    control [[143](#bib.bib143)], while continuous control was considered later [[91](#bib.bib91),
    [85](#bib.bib85), [63](#bib.bib63), [64](#bib.bib64)]. Continuous control is demonstrated
    in [[102](#bib.bib102)] to produce smoother trajectories than discrete control
    for lane keeping, which may make passengers feel more comfortable. Simultaneous
    lateral and longitudinal control has received wide attention, especially in urban
    scenarios [[105](#bib.bib105), [70](#bib.bib70), [106](#bib.bib106), [107](#bib.bib107),
    [108](#bib.bib108)]. Recently, hierarchical actions have attracted more attention
    [[138](#bib.bib138), [137](#bib.bib137), [136](#bib.bib136), [27](#bib.bib27),
    [134](#bib.bib134)], which provide higher robustness and interpretability.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自驾DRL/DIL代理可以在不同的抽象层次上进行规划，即低级控制、高级行为规划和轨迹规划，甚至可以同时在多个层次上进行规划。根据这一点，表格 [V](#S5.T5
    "TABLE V ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning") 将主流的动作空间分类为四组：行为级动作、轨迹级动作、控制级动作和层级动作。行为级动作通常根据指定任务设计。对于速度控制，加速度相关的动作（例如，全刹车、减速、继续和加速
    [[119](#bib.bib119)]) 常被使用。车道变换动作（例如，保持/LLC/LRC）和转弯动作（例如，左转/右转/直行）分别在高速公路场景和城市场景中受到青睐。轨迹级动作指的是计划或预测的轨迹/路径
    [[113](#bib.bib113), [111](#bib.bib111)]，通常由未来路径的二维点组成。控制级动作指的是低级控制命令（例如，转向、加速、油门和刹车），这些命令分为三类：横向控制、纵向控制和横向与纵向同时控制。早期的研究主要集中在离散的横向
    [[10](#bib.bib10), [19](#bib.bib19)] 或离散的纵向控制 [[143](#bib.bib143)]，而连续控制则在后来被考虑
    [[91](#bib.bib91), [85](#bib.bib85), [63](#bib.bib63), [64](#bib.bib64)]。连续控制在
    [[102](#bib.bib102)] 中被证明相比离散控制能够产生更平滑的车道保持轨迹，这可能使乘客感觉更舒适。横向与纵向同时控制得到了广泛关注，尤其是在城市场景中
    [[105](#bib.bib105), [70](#bib.bib70), [106](#bib.bib106), [107](#bib.bib107),
    [108](#bib.bib108)]。最近，层级动作引起了更多关注 [[138](#bib.bib138), [137](#bib.bib137), [136](#bib.bib136),
    [27](#bib.bib27), [134](#bib.bib134)]，这些动作提供了更高的鲁棒性和可解释性。
- en: V-C Reinforcement Learning Reward Design
  id: totrans-495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 强化学习奖励设计
- en: A major problem that limits RL’s real-world AD applications is the lack of underlying
    reward functions. Further, the ground truth reward, if it exists, may be multi-modal
    since human drivers change objectives according to the circumstances. To simplify
    the problem, current DRL models for AD tasks commonly formulate the reward function
    as a linear combination of factors, as presented in Fig. [6](#S5.F6 "Figure 6
    ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy
    Learning"). A large proportion of studies consider safety and efficiency. Reward
    terms that are used in the literature are listed in Table [VI](#S5.T6 "TABLE VI
    ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy
    Learning"). Collison and speed are the most common reward terms when considering
    safety and efficiency, respectively. However, empirically designed reward functions
    rely heavily on expert knowledge. It is difficult to balance rewards terms, which
    affects the trained policy performance. Recent studies on predictive reward and
    multi-reward RL may inspire future investigation. Hayashi et al. [[154](#bib.bib154)]
    proposed a predictive reward function that is based on the prediction error of
    a deep predictive network that models the transition of the surrounding environment.
    Their hypothesis is that the movement of surrounding vehicles becomes unpredictable
    when the ego vehicle performs an unnatural driving behavior. Yuan et al. [[126](#bib.bib126)]
    decomposed a single reward function into multi-reward functions to better represent
    multi-dimensional driving policies through a branched version of Q networks.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 限制 RL 在现实世界自动驾驶应用的一个主要问题是缺乏基础的奖励函数。此外，如果存在实际奖励，其可能是多模态的，因为人类驾驶员会根据情况改变目标。为简化问题，当前针对自动驾驶任务的
    DRL 模型通常将奖励函数表示为因素的线性组合，如图 [6](#S5.F6 "Figure 6 ‣ V Task-Driven Methods ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning") 所示。大量研究考虑了安全性和效率。文献中使用的奖励项列在表
    [VI](#S5.T6 "TABLE VI ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL for
    Autonomous Driving Policy Learning") 中。在考虑安全性和效率时，碰撞和速度是最常见的奖励项。然而，经验设计的奖励函数在很大程度上依赖于专家知识。平衡奖励项困难，这影响了训练后的策略性能。最近关于预测奖励和多奖励
    RL 的研究可能会激发未来的研究。Hayashi 等人[[154](#bib.bib154)] 提出了基于深度预测网络预测误差的预测奖励函数，该网络模型用于模拟周围环境的过渡。他们的假设是，当自车执行不自然的驾驶行为时，周围车辆的运动变得不可预测。Yuan
    等人[[126](#bib.bib126)] 将单一奖励函数分解为多奖励函数，以通过分支的 Q 网络更好地表示多维驾驶策略。
- en: VI Problem-driven Methods
  id: totrans-497
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 问题驱动方法
- en: AD application has special requirements on factors such as driving safety, interaction
    with other traffic participants and uncertainty of the environment. This section
    reviews the literature from the problem-driven perspective with the objectives
    of determining how these critical issues are addressed by the DRL/DIL models and
    identifying the challenges that remain.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶应用对驾驶安全、与其他交通参与者的互动以及环境的不确定性等因素有特殊要求。本节从问题驱动的角度回顾文献，旨在确定 DRL/DIL 模型如何解决这些关键问题，并识别仍存在的挑战。
- en: VI-A Safety-enhanced DRL/DIL for AD
  id: totrans-499
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 安全增强型 DRL/DIL 自动驾驶
- en: '![Refer to caption](img/674208fa6d028791ca29586a448449aa.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/674208fa6d028791ca29586a448449aa.png)'
- en: 'Figure 7: A taxonomy of the literature on interaction-aware DRL/DIL models
    for AD. (a) Qi et al. [[155](#bib.bib155)], (b) Chen et al. [[156](#bib.bib156)]
    are examples of explicit and implicit interactive environment encoding, respectively.
    (c) Hu et al. [[157](#bib.bib157)] is an example of interactive learning strategy.'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：关于互动感知 DRL/DIL 模型在自动驾驶（AD）中的文献分类。（a）Qi 等人[[155](#bib.bib155)]，（b）Chen 等人[[156](#bib.bib156)]分别是显式和隐式互动环境编码的例子。（c）Hu
    等人[[157](#bib.bib157)]是互动学习策略的一个例子。
- en: 'Although DRL/DIL can learn driving policies for complex high-dimensional problems,
    they only guarantee optimality of the learned policies in a statistical sense.
    However, in safety-critical AD systems, one failure (e.g., collision) would cause
    catastrophe. Below, we review representative methods for enhancing safety of DRL/DIL
    in the AD literature. Fig. [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning") categorizes the methods
    into three groups: (a) modified methods: methods that modify the original DRL/DIL
    algorithms, (b) combined methods: methods that combine DRL/DIL with traditional
    methods, and (c) hybrid methods: methods that integrate DRL/DIL into traditional
    methods.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DRL/DIL 可以学习复杂高维问题的驾驶策略，但它们仅在统计意义上保证所学策略的最优性。然而，在安全关键的自动驾驶系统中，一次失败（例如碰撞）可能会导致灾难。下面，我们回顾了
    DRL/DIL 在自动驾驶文献中增强安全性的代表性方法。图 [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣ A
    Survey of Deep RL and IL for Autonomous Driving Policy Learning") 将这些方法分为三组：（a）修改方法：修改原始
    DRL/DIL 算法的方法，（b）组合方法：将 DRL/DIL 与传统方法结合的方法，以及（c）混合方法：将 DRL/DIL 融入传统方法中的方法。
- en: VI-A1 Modified Methods
  id: totrans-503
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A1 修改方法
- en: As illustrated in Fig. [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning")(a), modified methods
    modify the standard DRL/DIL algorithms to enhance safety, typically by constraining
    the exploration space [[158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)].
    A safety model checker is introduced to identify the set of actions that satisfy
    the safety constraints at each state. This can be realized through several approaches,
    such as goal reachability [[90](#bib.bib90)][[161](#bib.bib161)], probabilistic
    prediction [[162](#bib.bib162)] and prior knowledge & constraints [[163](#bib.bib163)].
    Bouton et al. [[90](#bib.bib90)][[161](#bib.bib161)] use a probabilistic model
    checker, as illustrated in Fig. [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣
    A Survey of Deep RL and IL for Autonomous Driving Policy Learning")(a), to compute
    the probability of reaching the goal safely at each state-action pair. Then, safe
    actions are identified by applying a user-defined threshold on the probability.
    However, the proposed model checker requires a discretization of the state space
    and a full transition model. Alternatively, Isele et al. [[162](#bib.bib162)]
    proposed the use of probabilistic prediction to identify potentially dangerous
    actions that would cause collision, but the safety guarantee may not be sufficiently
    strong if the prediction is not accurate. Prior knowledge & constraints (e.g.,
    lane changes are disallowed if they will lead to small time gaps) are also exploited
    [[163](#bib.bib163), [125](#bib.bib125), [132](#bib.bib132)]. For DIL, Zhang et
    al. [[69](#bib.bib69)] proposed SafeDAgger, in which a safety policy is learned
    to predict the error made by a primary policy without querying the reference policy.
    If the safety policy determines that it is unsafe to let the primary policy drive,
    the reference policy will take over. One drawback is that the quality of the learned
    policy may be limited by that of the reference policy.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL
    for Autonomous Driving Policy Learning")(a) 所示，修改方法通过限制探索空间来修改标准的 DRL/DIL 算法，以提高安全性，通常是通过约束探索空间
    [[158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)]。引入了安全模型检查器，以识别每个状态下满足安全约束的行动集。这可以通过几种方法实现，例如目标可达性
    [[90](#bib.bib90)][[161](#bib.bib161)]、概率预测 [[162](#bib.bib162)] 和先验知识 & 约束 [[163](#bib.bib163)]。Bouton
    等人 [[90](#bib.bib90)][[161](#bib.bib161)] 使用了概率模型检查器，如图 [5](#S5.F5 "Figure 5 ‣
    V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy
    Learning")(a) 所示，以计算每个状态-动作对安全到达目标的概率。然后，通过对概率应用用户定义的阈值来识别安全动作。然而，所提出的模型检查器需要对状态空间进行离散化和完整的转移模型。另一种方法是
    Isele 等人 [[162](#bib.bib162)] 提出的使用概率预测来识别可能导致碰撞的危险动作，但如果预测不准确，安全保障可能不够强。还利用了先验知识
    & 约束（例如，如果车道变化会导致时间间隔过小则不允许） [[163](#bib.bib163), [125](#bib.bib125), [132](#bib.bib132)]。对于
    DIL，Zhang 等人 [[69](#bib.bib69)] 提出了 SafeDAgger，其中学习一个安全策略来预测主要策略所犯的错误，而无需查询参考策略。如果安全策略确定让主要策略驾驶是不安全的，参考策略将接管。一个缺点是所学习策略的质量可能受到参考策略的限制。
- en: VI-A2 Combined Methods
  id: totrans-505
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A2 组合方法
- en: Various studies combine standard DRL/DIL with traditional rule-based methods
    to enhance safety. In contrast to the modified methods that are discussed above,
    combined methods don’t modify the learning process of standard DRL/DIL. As presented
    in Fig. [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣ A Survey of Deep RL and
    IL for Autonomous Driving Policy Learning")(b), Chen et al. [[71](#bib.bib71)]
    proposed a framework in which DIL plans trajectories, while the rule-based tracking
    and safe set controller ensure safe control. Xiong et al. [[164](#bib.bib164)]
    proposed the linear combination of the control output from DDPG, artificial potential
    field and path tracking modules. According to Shalev-Shwartz et al. [[165](#bib.bib165)],
    hard constraints should be injected outside the learning framework. They decompose
    the double-merge problem into a composition of a learnable DRL policy and a trajectory
    planning module with non-learnable hard constraints. The learning part enables
    driving comfort, while the hard constraints guarantee safety.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 各种研究将标准 DRL/DIL 与传统的基于规则的方法结合，以提高安全性。与上述修改方法不同，结合方法不修改标准 DRL/DIL 的学习过程。如图 [5](#S5.F5
    "图 5 ‣ V 任务驱动方法 ‣ 深度 RL 和 IL 在自主驾驶政策学习中的调查")(b)所示，Chen 等人 [[71](#bib.bib71)] 提出了一个框架，其中
    DIL 规划轨迹，而基于规则的跟踪和安全集控制器确保安全控制。Xiong 等人 [[164](#bib.bib164)] 提出了 DDPG、人工势场和路径跟踪模块的控制输出的线性组合。根据
    Shalev-Shwartz 等人 [[165](#bib.bib165)] 的说法，硬约束应该被注入到学习框架之外。他们将双合并问题分解为一个可学习的 DRL
    政策和一个具有不可学习硬约束的轨迹规划模块的组合。学习部分使驾驶更加舒适，而硬约束则保证了安全。
- en: VI-A3 Hybrid Methods
  id: totrans-507
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A3 混合方法
- en: Hybrid methods integrate DRL/DIL into traditional heuristic search or POMDP
    planning methods. As presented in Fig. [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning")(c), Bernhard
    et al. [[114](#bib.bib114)] integrated experiences in the form of pretrained Q-values
    into Hybrid A^∗ as heuristics, thereby overcoming the statistical failure rate
    of DRL while still benefitting computationally from the learned policy. However,
    the experiments are limited to stationary environments. Pusse et al. [[166](#bib.bib166)]
    presented a hybrid solution that combines DRL and approximate POMDP planning for
    collision-free autonomous navigation in simulated critical traffic scenarios,
    which benefits from advantages of both methods.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法将 DRL/DIL 融入传统的启发式搜索或 POMDP 规划方法中。如图 [5](#S5.F5 "图 5 ‣ V 任务驱动方法 ‣ 深度 RL
    和 IL 在自主驾驶政策学习中的调查")(c)所示，Bernhard 等人 [[114](#bib.bib114)] 将预训练的 Q 值以启发式的形式集成到混合
    A^∗ 中，从而克服了 DRL 的统计失败率，同时仍从学习的政策中获益。然而，实验仅限于静态环境。Pusse 等人 [[166](#bib.bib166)]
    提出了一个混合解决方案，将 DRL 和近似 POMDP 规划相结合，用于模拟关键交通场景中的无碰撞自主导航，兼顾了两种方法的优点。
- en: VI-B Interaction-aware DRL/DIL for AD
  id: totrans-509
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 互动感知的 DRL/DIL 适用于 AD
- en: Interaction is one of the intrinsic characteristics of traffic environments.
    An intelligent agent should reason beforehand about the behaviors of other traffic
    participants to passively react or actively adjust its own policy to cooperate
    or compete with other agents. This section reviews the interaction modeling methods
    and two groups of interaction-aware DRL/DIL methods for AD, as presented in Fig.
    [7](#S6.F7 "Figure 7 ‣ VI-A Safety-enhanced DRL/DIL for AD ‣ VI Problem-driven
    Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning").
    One group of methods focus on interactive environment encoding, while the other
    focus on interactive learning strategies.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 互动是交通环境的内在特征之一。智能体应该预先推理其他交通参与者的行为，以被动反应或主动调整自身政策，与其他智能体合作或竞争。本节回顾了互动建模方法以及图
    [7](#S6.F7 "图 7 ‣ VI-A 安全增强的 DRL/DIL ‣ VI 问题驱动方法 ‣ 深度 RL 和 IL 在自主驾驶政策学习中的调查")所示的两组互动感知
    DRL/DIL 方法。其中一组方法侧重于交互环境编码，另一组方法则侧重于交互学习策略。
- en: VI-B1 Interaction Modeling
  id: totrans-511
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B1 互动建模
- en: 'The simplest way to model interaction between multi-agents is to use a standard
    Markov decision process (MDP), where the other traffic participants are only treated
    as part of the environment [[156](#bib.bib156), [141](#bib.bib141)]. POMDP is
    another common interaction model [[95](#bib.bib95), [155](#bib.bib155), [167](#bib.bib167)],
    where the agent has limited sensing capabilities. A Markov game (MG) is also used
    for modeling interaction scenarios. According to whether agents have the same
    importance, methods can be categorized into three groups: 1) equal importance
    [[168](#bib.bib168), [157](#bib.bib157), [169](#bib.bib169)], 2) one vs. others
    [[170](#bib.bib170)], and 3) proactive-passive pair [[171](#bib.bib171)].'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 建模多代理之间互动的最简单方法是使用标准的马尔可夫决策过程（MDP），在这种方法中，其他交通参与者仅被视为环境的一部分 [[156](#bib.bib156),
    [141](#bib.bib141)]。POMDP 是另一种常见的互动模型 [[95](#bib.bib95), [155](#bib.bib155), [167](#bib.bib167)]，其中代理具有有限的感知能力。马尔可夫博弈（MG）也用于建模互动场景。根据代理是否具有相同的重要性，方法可以分为三组：1)
    相等重要性 [[168](#bib.bib168), [157](#bib.bib157), [169](#bib.bib169)]，2) 一对多 [[170](#bib.bib170)]，以及
    3) 主动-被动对 [[171](#bib.bib171)]。
- en: VI-B2 Interactive Environment Encoding
  id: totrans-513
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B2 互动环境编码
- en: Interactive encoding of the environment is a popular research direction. As
    presented in Fig. [7](#S6.F7 "Figure 7 ‣ VI-A Safety-enhanced DRL/DIL for AD ‣
    VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"), mainstream methods can be divided into two groups. One group
    of methods explicitly model other agents and utilize active reasoning about other
    agents in the algorithm workflow. POMDP is a common choice for these methods,
    where the intentions/cooperation levels of other agents are modeled as unobservable
    states that must be inferred. Qi et al. [[155](#bib.bib155)] proposed an intent-aware
    multi-agent planning framework, as presented in Fig. [7](#S6.F7 "Figure 7 ‣ VI-A
    Safety-enhanced DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep
    RL and IL for Autonomous Driving Policy Learning")(a), which decouples intent
    prediction, high-level reasoning and low-level planning. The maintained belief
    regarding other agents’ intents (objectives) was considered in the planning process.
    Bouton et al. [[95](#bib.bib95)] proposed a similar method that maintains a belief
    regarding the cooperation levels (e.g., the willingness to yield to the ego vehicle)
    of other drivers.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 环境的互动编码是一个热门的研究方向。如图 [7](#S6.F7 "Figure 7 ‣ VI-A Safety-enhanced DRL/DIL for
    AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning") 所示，主流方法可以分为两组。一组方法显式地建模其他代理，并在算法工作流中利用对其他代理的主动推理。POMDP 是这些方法的常见选择，其中其他代理的意图/合作水平被建模为必须推断的不可观察状态。Qi
    等人 [[155](#bib.bib155)] 提出了一个意图感知的多代理规划框架，如图 [7](#S6.F7 "Figure 7 ‣ VI-A Safety-enhanced
    DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning")(a) 所示，该框架将意图预测、高层推理和低层规划解耦。规划过程中考虑了关于其他代理意图（目标）的维持信念。Bouton
    等人 [[95](#bib.bib95)] 提出了一个类似的方法，该方法维护了关于其他驾驶员合作水平（例如，是否愿意让路给自车）的信念。
- en: The other group of methods focuses on utilizing special neural network architectures
    to capture the interplay between agents by their relation or interaction representations.
    These methods are usually agnostic regarding the intentions of other agents. Jiang
    et al. [[167](#bib.bib167)] proposed graph convolutional reinforcement learning,
    in which the multi-agent environment is constructed as a graph. Agents are represented
    by nodes, and each node’s corresponding neighbors are determined by distance or
    other metrics. Then, the latent features that are produced by graph convolutional
    layers are exploited to learn cooperation. Similarly, Huegle et al. [[172](#bib.bib172)]
    built upon graph neural networks [[173](#bib.bib173)] and proposed the deep scene
    architecture for learning complex interaction-aware scene representations. Inspired
    by social pooling [[174](#bib.bib174), [175](#bib.bib175)] and attention models
    [[176](#bib.bib176), [177](#bib.bib177)], Chen et al. [[156](#bib.bib156)] proposed
    a socially attentive DRL method for interaction-aware robot navigation through
    a crowd. As illustrated in Fig. [7](#S6.F7 "Figure 7 ‣ VI-A Safety-enhanced DRL/DIL
    for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning")(b), they extracted pairwise features of interaction
    between the robot and each human and captured the interactions among humans via
    local maps. A self-attention mechanism was subsequently used to infer the relative
    importance of neighboring humans and aggregate interaction features.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组方法专注于利用特殊的神经网络架构，通过它们的关系或交互表示捕捉代理之间的相互作用。这些方法通常对其他代理的意图持中立态度。Jiang 等人 [[167](#bib.bib167)]
    提出了图卷积强化学习，其中多代理环境被构建为图。代理由节点表示，每个节点的相邻节点通过距离或其他指标确定。然后，利用图卷积层生成的潜在特征来学习合作。同样，Huegle
    等人 [[172](#bib.bib172)] 基于图神经网络 [[173](#bib.bib173)] 提出了深度场景架构，用于学习复杂的交互感知场景表示。受到社交池化
    [[174](#bib.bib174), [175](#bib.bib175)] 和注意力模型 [[176](#bib.bib176), [177](#bib.bib177)]
    的启发，Chen 等人 [[156](#bib.bib156)] 提出了一个社交注意的 DRL 方法，用于通过人群进行交互感知的机器人导航。如图 [7](#S6.F7
    "图 7 ‣ VI-A 增强安全的 DRL/DIL 方法 ‣ VI 问题驱动的方法 ‣ 自主驾驶策略学习的深度 RL 和 IL 综述")(b) 所示，他们提取了机器人与每个人之间的配对交互特征，并通过局部地图捕捉人类之间的交互。随后使用自注意机制推断邻近人类的相对重要性，并汇总交互特征。
- en: VI-B3 Interactive Learning Strategy
  id: totrans-516
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B3 互动学习策略
- en: '![Refer to caption](img/9c82e71c44827897dea4c0e7574d879d.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9c82e71c44827897dea4c0e7574d879d.png)'
- en: 'Figure 8: A taxonomy of the literature on uncertainty-aware DRL/DIL models.
    (a) Tai et al. [[108](#bib.bib108)] addresses aleatoric uncertainty, while (b)
    Henaff et al. [[178](#bib.bib178)] addresses both aleatoric and epistemic uncertainties.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：不确定性感知 DRL/DIL 模型文献的分类。（a）Tai 等人 [[108](#bib.bib108)] 处理了偶然不确定性，而（b）Henaff
    等人 [[178](#bib.bib178)] 处理了偶然不确定性和认识不确定性。
- en: Various learning strategies have been used to learn interactive policies. Curriculum
    learning has been used to learn interactive policies [[157](#bib.bib157)][[168](#bib.bib168)],
    which can decouple complex problems into simpler problems. As presented in Fig.
    [7](#S6.F7 "Figure 7 ‣ VI-A Safety-enhanced DRL/DIL for AD ‣ VI Problem-driven
    Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning")(c),
    Hu et al. [[157](#bib.bib157)] proposed an interaction-aware decision making approach
    that leverages curriculum learning. First, a decentralized critic for each agent
    is learned to generate distinct behaviors, where the agent does not react to other
    agents and only learns how to execute rational actions to complete its own task.
    Second, a centralized critic is learned to enable agents to interact with each
    other to realize joint success and maintain smooth traffic. One limitation of
    these methods is that new models must be learned if the number of agents increases.
    Based on dynamic coordination graph (DCG) [[179](#bib.bib179)], Chao et al. [[180](#bib.bib180)]
    proposed a strategic learning solution for coordinating multiple autonomous vehicles
    in highways. DCG was utilized to explicitly model the continuously changing coordination
    dependencies among vehicles. Another group of interaction-aware methods use game
    theory[[181](#bib.bib181)]. Game theory has already been applied in robotics tasks
    such as robust control [[182](#bib.bib182), [183](#bib.bib183)] and motion planning
    [[184](#bib.bib184), [185](#bib.bib185)]. Recent years have also witnessed the
    increasing application of game theory in interaction-aware AD policy learning
    [[170](#bib.bib170), [169](#bib.bib169), [171](#bib.bib171)]. Li et al. [[170](#bib.bib170)]
    proposed the combination of hierarchical reasoning game theory (i.e. “level-$k$”
    reasoning [[186](#bib.bib186)]) and reinforcement learning. Level-$k$ reasoning
    is used to model intelligent vehicles’ interactions in traffic, while RL evolves
    these interactions in a time-extended scenario. Ding et al. [[171](#bib.bib171)]
    introduced a proactive-passive game theoretical lane changing framework. The proactive
    vehicles learn to take actions to merge, while the passive vehicles learn to create
    merging space. Fisac et al. [[169](#bib.bib169)] proposed a novel game-theoretic
    real-time trajectory planning algorithm. The dynamic game is hierarchically decomposed
    into a long-horizon “strategic” game and a short-horizon “tactical” game. Furthermore,
    the long-horizon interaction game is solved to guide short-horizon planning, thereby
    implicitly extending the planning horizon and pushing the local trajectory optimization
    closer to global solutions. Apart from combining game theory and RL, solving an
    imitation learning problem under game-theoretic formalism is another approach.
    Sun et al. [[187](#bib.bib187)] proposed an interactive probabilistic prediction
    approach that was based on hierarchical inverse reinforcement learning (HIRL).
    They modeled the problem from the perspective of a two-agent game by explicitly
    considering the responses of one agent to the other. However, some of the current
    game-theoretic interaction-aware methods are limited by their two-vehicle settings
    and simulation experiments [[171](#bib.bib171), [169](#bib.bib169), [187](#bib.bib187)].
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 各种学习策略已被用来学习交互式策略。课程学习已被用来学习交互式策略[[157](#bib.bib157)][[168](#bib.bib168)]，这可以将复杂问题分解为更简单的问题。如图[7](#S6.F7
    "Figure 7 ‣ VI-A Safety-enhanced DRL/DIL for AD ‣ VI Problem-driven Methods ‣
    A Survey of Deep RL and IL for Autonomous Driving Policy Learning")(c)所示，Hu 等人[[157](#bib.bib157)]提出了一种利用课程学习的交互感知决策方法。首先，学习每个代理的去中心化评价器以生成不同的行为，在此过程中，代理不会对其他代理做出反应，只学习如何执行合理的动作来完成自己的任务。其次，学习一个中心化评价器，使得代理能够相互作用，实现联合成功并保持交通顺畅。这些方法的一个局限性是，当代理数量增加时，必须学习新的模型。基于动态协调图（DCG）[[179](#bib.bib179)]，Chao
    等人[[180](#bib.bib180)]提出了一种用于协调高速公路上多个自主车辆的战略学习解决方案。DCG 被用来显式建模车辆之间不断变化的协调依赖关系。另一组交互感知方法使用博弈论[[181](#bib.bib181)]。博弈论已经被应用于机器人任务，如鲁棒控制[[182](#bib.bib182),
    [183](#bib.bib183)]和运动规划[[184](#bib.bib184), [185](#bib.bib185)]。近年来，博弈论在交互感知自动驾驶策略学习中的应用也在增加[[170](#bib.bib170),
    [169](#bib.bib169), [171](#bib.bib171)]。Li 等人[[170](#bib.bib170)]提出了层次推理博弈论（即“level-$k$”推理[[186](#bib.bib186)])与强化学习的结合。层次-$k$
    推理用于建模智能车辆在交通中的互动，而RL在时间扩展的场景中演化这些互动。Ding 等人[[171](#bib.bib171)]介绍了一个主动-被动博弈理论车道变换框架。主动车辆学习采取行动进行合并，而被动车辆学习创造合并空间。Fisac
    等人[[169](#bib.bib169)]提出了一种新颖的博弈理论实时轨迹规划算法。动态博弈被分解为一个长时间范围的“战略”博弈和一个短时间范围的“战术”博弈。此外，解决长时间范围的互动博弈来指导短时间范围的规划，从而隐式地扩展规划范围，并将局部轨迹优化推向全球解决方案。除了结合博弈论和RL之外，在博弈论形式下解决模仿学习问题是另一种方法。Sun
    等人[[187](#bib.bib187)]提出了一种基于层次逆强化学习（HIRL）的互动概率预测方法。他们从双代理博弈的角度建模问题，明确考虑一个代理对另一个代理的响应。然而，一些当前的博弈论交互感知方法受到其双车设置和仿真实验的限制[[171](#bib.bib171),
    [169](#bib.bib169), [187](#bib.bib187)]。
- en: VI-C Uncertainty-aware DRL/DIL for AD
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 不确定性感知DRL/DIL用于自动驾驶
- en: 'Before deployment of a learned model, it is important to determine what it
    does not understand and estimate the uncertainty of the decision making output.
    As presented in Fig. [8](#S6.F8 "Figure 8 ‣ VI-B3 Interactive Learning Strategy
    ‣ VI-B Interaction-aware DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning"), this section reviews
    the uncertainty-aware DRL/DIL methods for AD from three aspects: 1) AD and deep
    learning uncertainty, 2) uncertainty estimation methods, and 3) multi-modal driving
    behavior learning.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署学习模型之前，确定模型的不理解内容并评估决策输出的不确定性是重要的。如图[8](#S6.F8 "Figure 8 ‣ VI-B3 Interactive
    Learning Strategy ‣ VI-B Interaction-aware DRL/DIL for AD ‣ VI Problem-driven
    Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning")所示，本节从三个方面回顾了用于自动驾驶的不确定性感知DRL/DIL方法：1）自动驾驶和深度学习不确定性，2）不确定性估计方法，以及3）多模态驾驶行为学习。
- en: VI-C1 Autonomous Driving and Deep Learning Uncertainty
  id: totrans-522
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C1 自动驾驶与深度学习不确定性
- en: 'Autonomous driving has inherent uncertainty, while deep learning methods have
    deep learning uncertainty, and the two can intersect. The AD uncertainty can be
    categorized as:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶具有固有的不确定性，而深度学习方法具有深度学习不确定性，两者可能交织在一起。自动驾驶的不确定性可以分类为：
- en: •
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Traffic environment uncertainty [[188](#bib.bib188), [107](#bib.bib107), [189](#bib.bib189),
    [108](#bib.bib108), [190](#bib.bib190), [178](#bib.bib178)]. Stochastic and dynamic
    interactions among agents with distinct behaviors lead to intrinsic irreducible
    randomness and uncertainty in a traffic environment.
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交通环境的不确定性 [[188](#bib.bib188), [107](#bib.bib107), [189](#bib.bib189), [108](#bib.bib108),
    [190](#bib.bib190), [178](#bib.bib178)]。代理之间具有不同行为的随机和动态交互导致了交通环境中的固有不可减少的随机性和不确定性。
- en: •
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Driving behavior uncertainty [[191](#bib.bib191)]. Human driving behavior is
    multi-modal and stochastic (e.g., a driver can either make a left lane change
    or a right lane change when he comes up behind a van that is moving at a crawl).
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 驾驶行为不确定性 [[191](#bib.bib191)]。人类驾驶行为是多模态和随机的（例如，司机在遇到缓慢行驶的货车时，可能选择左侧车道变道或右侧车道变道）。
- en: •
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Partial observability and sensor noise uncertainty [[192](#bib.bib192)]. In
    real-world scenarios, the AD agent usually has limited partial observability (e.g.,
    due to occlusion), and there is noise in the sensor observation.
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分可观察性和传感器噪声不确定性 [[192](#bib.bib192)]。在实际场景中，自动驾驶代理通常具有有限的部分可观察性（例如，由于遮挡）以及传感器观测中的噪声。
- en: Deriving from Bayesian deep learning approaches, Gal et al. [[193](#bib.bib193)]
    categorized deep learning uncertainty as aleatoric/data and epistemic/model uncertainties.
    Aleatoric uncertainty results from incomplete knowledge about the environment
    (e.g., partial observability and measurement noise), which can’t be reduced through
    access to more or even unlimited data but can be explicitly modeled. In contrast,
    epistemic uncertainty originates from an insufficient dataset and measures what
    our model doesn’t know, which can be eliminated with sufficient training data.
    We refer readers to [[194](#bib.bib194), [193](#bib.bib193)] for a deeper background
    on predictive uncertainty in deep neural networks. Although it is sometimes possible
    to use only aleatoric [[188](#bib.bib188), [189](#bib.bib189), [108](#bib.bib108)]
    or epistemic [[107](#bib.bib107), [190](#bib.bib190)] uncertainty to develop a
    reasonable model, the ideal approach would be to combine these two uncertainty
    estimates [[192](#bib.bib192), [191](#bib.bib191), [178](#bib.bib178)].
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 源自贝叶斯深度学习方法，Gal等人 [[193](#bib.bib193)] 将深度学习不确定性分为随机/数据不确定性和认知/模型不确定性。随机不确定性源于对环境的不完全知识（例如，部分可观察性和测量噪声），这种不确定性无法通过获取更多或无限的数据来减少，但可以明确建模。相比之下，认知不确定性源于数据集不足，衡量我们的模型不知道什么，这种不确定性可以通过足够的训练数据来消除。我们建议读者参考
    [[194](#bib.bib194), [193](#bib.bib193)] 以获得有关深度神经网络预测不确定性的更深入背景。尽管有时可以仅使用随机 [[188](#bib.bib188),
    [189](#bib.bib189), [108](#bib.bib108)] 或认知 [[107](#bib.bib107), [190](#bib.bib190)]
    不确定性来开发合理的模型，但理想的方法是结合这两种不确定性估计 [[192](#bib.bib192), [191](#bib.bib191), [178](#bib.bib178)]。
- en: VI-C2 Uncertainty Estimation Methods
  id: totrans-531
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C2 不确定性估计方法
- en: Aleatoric uncertainty is usually learned by using the heteroscedastic loss function
    [[194](#bib.bib194)]. The regression task and the loss are formulated as
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 随机不确定性通常通过使用异方差损失函数来学习 [[194](#bib.bib194)]。回归任务和损失函数的公式为
- en: '|  | $\displaystyle[\tilde{\mathbf{y}},\tilde{\sigma}]=\mathbf{f}^{\theta}(\mathbf{x})$
    |  | (20) |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle[\tilde{\mathbf{y}},\tilde{\sigma}]=\mathbf{f}^{\theta}(\mathbf{x})$
    |  | (20) |'
- en: '|  | $\displaystyle\mathcal{L}(\theta)=\frac{1}{2}\frac{{\parallel\mathbf{y}-\tilde{\mathbf{y}}\parallel}^{2}}{\tilde{\sigma}^{2}}+\frac{1}{2}\log\tilde{\sigma}^{2}$
    |  | (21) |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}(\theta)=\frac{1}{2}\frac{{\parallel\mathbf{y}-\tilde{\mathbf{y}}\parallel}^{2}}{\tilde{\sigma}^{2}}+\frac{1}{2}\log\tilde{\sigma}^{2}$
    |  | (21) |'
- en: where $\mathbf{x}$ denotes the input data and $\mathbf{y}$ and $\mathbf{\tilde{y}}$
    denote the regression ground truth and the prediction output, respectively. $\theta$
    denotes the model parameters, and $\tilde{\sigma}$ is another output of the model,
    which represents the standard variance of data $\mathbf{x}$ (the aleatoric uncertainty).
    The loss function can be interpreted as penalizing a large prediction error when
    the uncertainty is small and relaxing constraints on the prediction error when
    the uncertainty is large. In practice, the network predicts the log variance $\log\tilde{\sigma}^{2}$
    [[194](#bib.bib194)]. Tai et al. [[108](#bib.bib108)] proposed an end-to-end real-to-sim
    visual navigation deployment pipeline, as illustrated in Fig. [8](#S6.F8 "Figure
    8 ‣ VI-B3 Interactive Learning Strategy ‣ VI-B Interaction-aware DRL/DIL for AD
    ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning")(a). An uncertainty-aware IL policy is trained with the heteroscedastic
    loss and outputs actions, along with associated uncertainties. A similar technique
    is proposed by Lee et al. [[192](#bib.bib192)].
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{x}$ 表示输入数据，$\mathbf{y}$ 和 $\mathbf{\tilde{y}}$ 分别表示回归的真实值和预测输出。$\theta$
    表示模型参数，$\tilde{\sigma}$ 是模型的另一个输出，代表数据 $\mathbf{x}$ 的标准方差（即 aleatoric 不确定性）。损失函数可以解释为当不确定性较小时对大的预测误差进行惩罚，而当不确定性较大时放宽对预测误差的约束。在实际应用中，网络预测对数方差
    $\log\tilde{\sigma}^{2}$ [[194](#bib.bib194)]。Tai 等人 [[108](#bib.bib108)] 提出了一个端到端的真实到模拟视觉导航部署管道，如图
    [8](#S6.F8 "Figure 8 ‣ VI-B3 Interactive Learning Strategy ‣ VI-B Interaction-aware
    DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning")(a) 所示。一个不确定性感知的 IL 策略通过异方差损失进行训练，并输出动作及相关的不确定性。Lee 等人
    [[192](#bib.bib192)] 也提出了类似的技术。
- en: 'The epistemic uncertainty is usually estimated via two popular methods: Monte
    Carlo (MC)-dropout [[195](#bib.bib195), [196](#bib.bib196)] and ensembles [[197](#bib.bib197),
    [198](#bib.bib198)]. These methods are similar in the sense that both apply probabilistic
    reasoning on the network weights. The variance of the model output serves as an
    estimate of the model uncertainty. However, multiple stochastic forward passes
    through dropout sampling may be time-consuming, while ensemble methods have higher
    training and storage costs. Kahn et al. [[190](#bib.bib190)] proposed an uncertainty-aware
    RL method that utilizes MC-dropout and bootstrapping [[199](#bib.bib199)], where
    the confidence for a specified obstacle is updated iteratively. Guided by the
    uncertainty cost, the agent behaves more carefully in unfamiliar scenarios in
    the early training phase. As presented in Fig. [8](#S6.F8 "Figure 8 ‣ VI-B3 Interactive
    Learning Strategy ‣ VI-B Interaction-aware DRL/DIL for AD ‣ VI Problem-driven
    Methods ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning")(b),
    Henaff et al. [[178](#bib.bib178)] proposed training a driving policy by unrolling
    a learned dynamics model over multiple time steps while explicitly penalizing
    the original policy cost and an uncertainty cost that represents the divergence
    from the training dataset. Their method estimates both the aleatoric and epistemic
    uncertainties.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 表征不确定性通常通过两种流行方法来估计：蒙特卡罗（MC）- dropout [[195](#bib.bib195), [196](#bib.bib196)]
    和集成方法 [[197](#bib.bib197), [198](#bib.bib198)]。这两种方法的相似之处在于，它们都对网络权重进行概率推理。模型输出的方差作为模型不确定性的估计。然而，dropout
    采样的多个随机前向传递可能耗时较长，而集成方法具有更高的训练和存储成本。Kahn 等人 [[190](#bib.bib190)] 提出了一个不确定性感知的强化学习方法，该方法利用
    MC-dropout 和自助法 [[199](#bib.bib199)]，其中对特定障碍物的信心通过迭代更新。在不确定性成本的指导下，代理在早期训练阶段在不熟悉的场景中表现得更加小心。如图
    [8](#S6.F8 "Figure 8 ‣ VI-B3 Interactive Learning Strategy ‣ VI-B Interaction-aware
    DRL/DIL for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning")(b) 所示，Henaff 等人 [[178](#bib.bib178)] 提出了通过在多个时间步展开学习的动态模型来训练驾驶策略，同时明确地惩罚原始策略成本和表示与训练数据集偏差的不确定性成本。他们的方法估计了
    aleatoric 和 epistemic 不确定性。
- en: The uncertainty estimation methods that are presented above depend mainly on
    sampling. A novel uncertainty estimation method that utilizes a mixture density
    network (MDN) was proposed by Choi et al. [[191](#bib.bib191)] for learning from
    complex and noisy human demonstrations. Since an MDN outputs the parameters for
    constructing a Gaussian mixture model (GMM), the total variance of the GMM can
    be calculated analytically, and the acquisition of uncertainty requires only a
    single forward pass. Distributional reinforcement learning [[200](#bib.bib200),
    [51](#bib.bib51), [201](#bib.bib201)] offers another approach for modelling the
    uncertainty that is associated with actions. It models the RL return $R$ as a
    random variable that is subject to the probability distribution $Z(r|s,a)$ and
    the Q-value as the expected return $Q(s,a)=\mathbb{E}_{r\sim Z(r|s,a)}[r]$. In
    the AD domain, Wang et al. [[188](#bib.bib188)] applied distributional DDPG to
    an energy management strategy (EMS) problem as a case study to evaluate the effects
    of estimating the uncertainty that is associated with various actions at various
    states. Bernhard [[189](#bib.bib189)] et al. presented a two-step approach for
    risk-sensitive behavior generation that combined offline distribution reinforcement
    learning with online risk assessment, which increased safety in intersection crossing
    scenarios.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的不确定性估计方法主要依赖于采样。Choi 等人 [[191](#bib.bib191)] 提出了利用混合密度网络（MDN）的新型不确定性估计方法，用于从复杂和嘈杂的人类示例中学习。由于
    MDN 输出用于构建高斯混合模型（GMM）的参数，GMM 的总方差可以通过解析方法计算，不确定性的获取只需一次前向传播。分布式强化学习 [[200](#bib.bib200),
    [51](#bib.bib51), [201](#bib.bib201)] 提供了另一种与动作相关的不确定性建模方法。它将 RL 回报 $R$ 建模为一个随机变量，受概率分布
    $Z(r|s,a)$ 影响，将 Q 值建模为期望回报 $Q(s,a)=\mathbb{E}_{r\sim Z(r|s,a)}[r]$。在 AD 领域，Wang
    等人 [[188](#bib.bib188)] 将分布式 DDPG 应用于能源管理策略（EMS）问题，作为案例研究来评估在各种状态下估计与各种动作相关的不确定性的效果。Bernhard
    [[189](#bib.bib189)] 等人提出了一种风险敏感行为生成的两步方法，将离线分布式强化学习与在线风险评估相结合，提高了交叉路口场景中的安全性。
- en: VI-C3 Multi-Modal Driving Behavior Learning
  id: totrans-538
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C3 多模态驾驶行为学习
- en: The inherent uncertainty in driving behavior results in multi-modal demonstrations.
    Many multi-modal imitation learning methods have been proposed. InfoGAIL [[202](#bib.bib202)]
    and Burn-InfoGAIL [[203](#bib.bib203)] infer latent/modal variables by maximizing
    the mutual information between latent variables and state-action pairs. VAE-GAIL
    [[204](#bib.bib204)] introduces a variational auto-encoder for inferring modal
    variables. However, due to the lack of labels in the demonstrations, these algorithms
    tend to distinguish latent labels without considering semantic information or
    the task context. Another direction focuses on labeled data in expert demonstrations.
    CGAIL [[205](#bib.bib205)] sends the modal labels directly to the generator and
    the discriminator. ACGAIL [[206](#bib.bib206)] introduces an auxiliary classifier
    for reconstructing the modal information, where the classifier cooperates with
    the discriminator to provide the adversarial loss to the generator. Nevertheless,
    the above methods mainly leverage random sampling of latent labels from a known
    prior distribution to distinguish multiple modalities. The trained models rely
    on manually specified labels to output actions; hence, they cannot select modes
    adaptively according to environmental scenarios. Recently, Fei et al. [[207](#bib.bib207)]
    proposed Triple-GAIL, which can learn adaptive skill selection and imitation jointly
    from expert demonstrations, and generated experiences by introducing an auxiliary
    skill selector.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 驾驶行为中的固有不确定性导致了多模态展示。许多多模态模仿学习方法已经被提出。InfoGAIL [[202](#bib.bib202)] 和 Burn-InfoGAIL
    [[203](#bib.bib203)] 通过最大化潜在变量和状态-动作对之间的互信息来推断潜在/模态变量。VAE-GAIL [[204](#bib.bib204)]
    引入了变分自编码器来推断模态变量。然而，由于示例中缺乏标签，这些算法往往在不考虑语义信息或任务上下文的情况下区分潜在标签。另一个方向集中于专家示例中的标记数据。CGAIL
    [[205](#bib.bib205)] 直接将模态标签发送给生成器和判别器。ACGAIL [[206](#bib.bib206)] 引入了一个辅助分类器来重建模态信息，其中分类器与判别器合作，为生成器提供对抗损失。然而，上述方法主要利用从已知先验分布中随机采样的潜在标签来区分多个模态。训练后的模型依赖于手动指定的标签来输出动作，因此无法根据环境场景自适应地选择模式。最近，Fei
    等人 [[207](#bib.bib207)] 提出了 Triple-GAIL，它可以从专家示例中联合学习自适应技能选择和模仿，并通过引入辅助技能选择器生成经验。
- en: VII Discussion
  id: totrans-540
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 讨论
- en: Although DRL and DIL attract significant amounts of interest in AD research,
    they remain far from ready for real-world applications, and challenges are faced
    at the architecture, task and algorithm levels. However, solutions remain largely
    underexplored. In this section, we discuss these challenges along with future
    investigation. DRL and DIL also have their own technical challenges; we refer
    readers to comprehensive discussions in [[29](#bib.bib29), [31](#bib.bib31), [33](#bib.bib33)].
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度强化学习（**DRL**）和深度模仿学习（**DIL**）在自动驾驶（**AD**）研究中引起了大量关注，但它们仍远未准备好应用于实际场景，并且在架构、任务和算法层面面临挑战。然而，解决方案仍然
    largely underexplored。在本节中，我们讨论了这些挑战以及未来的研究方向。**DRL**和**DIL**也有其自身的技术挑战；我们请读者参考
    [[29](#bib.bib29), [31](#bib.bib31), [33](#bib.bib33)] 中的详细讨论。
- en: VII-A System architecture
  id: totrans-542
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 系统架构
- en: The success of modern AD systems depends on the meticulous design of architectures.
    The integration of DRL/DIL methods to collaborate with other modules and improve
    the performance of the system remains a substantial challenge. Studies have demonstrated
    various ways that DRL/DIL models could be integrated into an AD system. As illustrated
    Fig. [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"), some studies
    propose new AD architectures, e.g., Modes 1&2, where an entire pipeline from the
    input of the sensor/perception to the output of the vehicle’s actuators is covered.
    However, the traditional modules of sequential planning are missing in these new
    architectures, and driving policy is addressed at the control level only. Hence,
    these AD systems could adapt to only simple tasks, such as road following, that
    require neither the guidance of goal points nor switching of driving behaviors.
    The extension of these architectures to accomplish more complicated AD tasks remains
    a substantial challenge. Other studies utilize the traditional AD architectures,
    e.g., Modes 3&4, where DRL/DIL models are studied as substitutes for traditional
    modules to improve the performance in challenging scenarios. Mode 5 studies use
    both new and traditional architectures. Overall, the research effort until now
    has been focused more on exploring the potential of DRL/DIL in accomplishing AD
    tasks, whereas the design of the system architectures has yet to be intensively
    investigated.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 现代**AD**系统的成功依赖于精细设计的架构。将**DRL**/**DIL**方法与其他模块进行集成以提升系统性能仍然是一个重要挑战。研究表明了**DRL**/**DIL**模型如何集成到**AD**系统中的各种方法。如图
    [3](#S4.F3 "Figure 3 ‣ IV Architectures of DRL/DIL Integrated AD Systems ‣ A Survey
    of Deep RL and IL for Autonomous Driving Policy Learning") 所示，一些研究提出了新的**AD**架构，例如模式
    1&2，其中涵盖了从传感器/感知的输入到车辆执行器的输出的整个流程。然而，这些新架构中缺少传统的顺序规划模块，驾驶策略仅在控制层面得到处理。因此，这些**AD**系统只能适应简单任务，例如道路跟随，这些任务既不需要目标点的指导，也不需要切换驾驶行为。将这些架构扩展以完成更复杂的**AD**任务仍然是一个重要挑战。其他研究利用了传统的**AD**架构，例如模式
    3&4，其中**DRL**/**DIL**模型被研究作为传统模块的替代品，以提高在挑战性场景下的性能。模式 5 研究则同时使用了新架构和传统架构。总体而言，到目前为止，研究工作更多集中在探索**DRL**/**DIL**在完成**AD**任务中的潜力，而系统架构的设计仍待深入研究。
- en: VII-B Formulation of driving tasks
  id: totrans-544
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 驾驶任务的制定
- en: 'Various DRL/DIL formulations have been established for accomplishing AD tasks.
    However, these formulations rely heavily on empirical designs. As reviewed in
    Section [V](#S5 "V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning"), the state space and input data are designed case by
    case, and ad-hoc reward functions are usually adopted with hand-tuned coefficients
    that balance the costs regarding safety, efficiency, comfort, and traffic rules,
    among other factors. Such designs are very brute-force approaches, which lack
    both theoretical proof and in-depth investigations. Changing the designs or tuning
    the parameters could result in substantially different driving policies. However,
    in real-world deployment, more attention should be paid to the following questions:
    What design could realize the most optimal driving policy? Could such a design
    adapt to various scenes? How can the boundary conditions of designs be identified?
    To answer these questions, rigorous studies with comparative experiments are needed.'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 各种 DRL/DIL 公式已经建立以完成自动驾驶任务。然而，这些公式在很大程度上依赖于经验设计。如第 [V](#S5 "V Task-Driven Methods
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning") 节所述，状态空间和输入数据是逐案设计的，并且通常采用手动调整系数的临时奖励函数，以平衡安全、效率、舒适度和交通规则等因素。这些设计是非常粗糙的方法，缺乏理论证明和深入研究。更改设计或调整参数可能会导致截然不同的驾驶策略。然而，在实际部署中，应更多关注以下问题：什么设计可以实现最优驾驶策略？这种设计能否适应各种场景？如何确定设计的边界条件？为回答这些问题，需要进行严谨的研究和比较实验。
- en: VII-C Safe driving policy
  id: totrans-546
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 安全驾驶政策
- en: AD applications have high requirements on safety, and guaranteeing the safety
    of a DRL/DIL integrated AD system is of substantial importance. Compared to traditional
    rule-based methods, DNN has been widely acknowledged as having poor interpretability.
    Its “black-box” nature renders difficult the prediction of when the agent may
    fail to generate a safe policy. Deep models for real-world AD applications must
    address unseen or rarely seen scenarios, which is difficult for DL methods as
    they optimize objectives at the level of expectations over specified instances.
    To solve this problem, a general strategy is to combine traditional methods to
    ensure a DRL/DIL agent’s functional safety. As reviewed in Fig. [5](#S5.F5 "Figure
    5 ‣ V Task-Driven Methods ‣ A Survey of Deep RL and IL for Autonomous Driving
    Policy Learning"), various methods have been proposed in the literature, where
    the problems are usually formulated as compositions of learned policies with hard
    constraints [[132](#bib.bib132), [125](#bib.bib125), [163](#bib.bib163)]. However,
    balancing between the learned optimal policy and the safety guarantee by hard
    constraints is non-trivial and requires intensive investigation in the future.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶应用对安全性有很高的要求，因此确保 DRL/DIL 集成自动驾驶系统的安全性至关重要。与传统的规则基础方法相比，DNN 被广泛认为具有较差的可解释性。其“黑箱”特性使得预测代理何时可能无法生成安全策略变得困难。针对现实世界自动驾驶应用的深度模型必须处理未见过或很少见的场景，这对于深度学习方法而言很困难，因为它们在指定实例的期望水平上优化目标。为了解决这个问题，一种通用策略是结合传统方法以确保
    DRL/DIL 代理的功能安全。如图 [5](#S5.F5 "Figure 5 ‣ V Task-Driven Methods ‣ A Survey of
    Deep RL and IL for Autonomous Driving Policy Learning") 所述，文献中提出了各种方法，这些方法通常将问题表述为具有硬约束的学习策略的组合
    [[132](#bib.bib132), [125](#bib.bib125), [163](#bib.bib163)]。然而，通过硬约束在学习的最优策略和安全保障之间取得平衡并非易事，未来需要进行深入研究。
- en: VII-D Interaction with traffic participants
  id: totrans-548
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D 与交通参与者的互动
- en: 'The capability of human-like interaction is required of self-driving agents
    for sharing the roads with other traffic participants. As reviewed in Section
    [VI-B](#S6.SS2 "VI-B Interaction-aware DRL/DIL for AD ‣ VI Problem-driven Methods
    ‣ A Survey of Deep RL and IL for Autonomous Driving Policy Learning"), interaction-aware
    DRL/DIL is a rising topic, but the following problems remain: First, current studies
    attempt to solve the problem from various perspectives, and the systematic studies
    are needed. Second, few interaction-aware DIL methods are available, while interaction-aware
    DRL methods are limited to simplified scenarios that involve only a few agents.
    The combination of interaction-aware trajectory prediction methods [[208](#bib.bib208),
    [209](#bib.bib209), [210](#bib.bib210)] may be an open topic of potential value.
    Third, game theory and multi-agent reinforcement learning (MARL) [[31](#bib.bib31)]
    are highly correlated for interactive scenarios. MARL methods usually build on
    concepts of game theory (e.g., Markov games) to model the interaction process.
    Apart from DRL/DIL, methods are available for learning interactive policies through
    traditional game theory approaches, such as Nash equilibrium [[211](#bib.bib211)],
    level-$k$ reasoning [[212](#bib.bib212)] and game tree search [[213](#bib.bib213)].
    Exploiting POMDP planning to learn interactive polices is also a trend [[214](#bib.bib214),
    [215](#bib.bib215)]. These methods have satisfactory interpretability but are
    limited to simplified or coarse discretizations of the agents’ action space [[213](#bib.bib213),
    [215](#bib.bib215)]. Although the simplification reduces the computation burden,
    it tends to also lower the control precision. In the future, the combination of
    these methods and DRL/DIL may be promising.'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 自驾代理需要具有人类般的交互能力，以便与其他交通参与者共享道路。正如在第[VI-B](#S6.SS2 "VI-B Interaction-aware DRL/DIL
    for AD ‣ VI Problem-driven Methods ‣ A Survey of Deep RL and IL for Autonomous
    Driving Policy Learning")节中回顾的那样，关注交互的深度强化学习（DRL）/深度模仿学习（DIL）是一个新兴话题，但仍然存在以下问题：首先，目前的研究试图从不同角度解决这一问题，但需要系统性的研究。其次，可用的交互感知DIL方法很少，而交互感知DRL方法则仅限于涉及少量代理的简化场景。交互感知轨迹预测方法的结合[[208](#bib.bib208),
    [209](#bib.bib209), [210](#bib.bib210)]可能是一个具有潜在价值的开放话题。第三，博弈论和多智能体强化学习（MARL）[[31](#bib.bib31)]在交互场景中高度相关。MARL方法通常建立在博弈论的概念上（例如，马尔可夫博弈）来建模交互过程。除了DRL/DIL外，还有通过传统博弈论方法学习交互策略的方法，如纳什均衡[[211](#bib.bib211)]、$k$级推理[[212](#bib.bib212)]和博弈树搜索[[213](#bib.bib213)]。利用部分可观测马尔可夫决策过程（POMDP）规划来学习交互策略也是一种趋势[[214](#bib.bib214),
    [215](#bib.bib215)]。这些方法具有令人满意的可解释性，但限制在代理动作空间的简化或粗略离散化[[213](#bib.bib213), [215](#bib.bib215)]。尽管简化减少了计算负担，但也倾向于降低控制精度。未来，这些方法与DRL/DIL的结合可能会很有前景。
- en: VII-E Uncertainty of the environment
  id: totrans-550
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-E 环境的不确定性
- en: 'Decision-making under uncertainty has been studied for decades [[216](#bib.bib216),
    [217](#bib.bib217)]. Nevertheless, modeling the uncertainty in DRL/DIL formulations
    remains challenging, especially under complex uncertain traffic environments.
    Several problems have been identified in current research: First, most uncertainty-aware
    methods follow the style of deep learning predictive uncertainty [[193](#bib.bib193)]
    without a deeper investigation. Is computing the predictive uncertainty of DNNs
    sufficient for AD tasks? Second, can the computed uncertainty be effectively utilized
    to realize a better decision making policy? Various methods incorporate the uncertainty
    cost into the global cost functions [[190](#bib.bib190), [178](#bib.bib178)],
    while other methods utilize uncertainty to generate risk-sensitive behavior [[189](#bib.bib189)].
    Future efforts are needed to identify more promising applications. Third, human-driving
    behavior is uncertain, or multi-modal. However, DIL performs well for demonstrations
    from one expert rather than multiple experts [[218](#bib.bib218)]. A naive solution
    is to neglect the multi-modality and treat the demonstrations as if there is only
    one expert. The main side effect is that the model tends to learn an average policy
    rather than a multi-modal policy [[202](#bib.bib202)]. Thus, determining whether
    DRL/DIL learn effectively from noisy uncertain naturalistic driving data and generate
    multi-modal driving behavior according to various scenarios is meaningful.'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 在不确定性下的决策制定已经研究了几十年[[216](#bib.bib216), [217](#bib.bib217)]。然而，在 DRL/DIL 公式中建模不确定性仍然具有挑战性，尤其是在复杂的不确定交通环境下。目前的研究中发现了几个问题：首先，大多数关注不确定性的方法遵循深度学习预测不确定性[[193](#bib.bib193)]的风格，但没有深入探讨。计算
    DNN 的预测不确定性是否足以用于自动驾驶任务？其次，计算出的不确定性能否有效利用以实现更好的决策制定策略？各种方法将不确定性成本纳入全局成本函数[[190](#bib.bib190),
    [178](#bib.bib178)]，而其他方法利用不确定性生成风险敏感行为[[189](#bib.bib189)]。未来的努力需要识别更有前途的应用。第三，人类驾驶行为是不确定的或多模态的。然而，DIL
    在来自单一专家的演示中表现良好，而不是多个专家[[218](#bib.bib218)]。一种简单的解决方案是忽略多模态性，将演示视为只有一个专家。主要副作用是模型倾向于学习一个平均策略而不是多模态策略[[202](#bib.bib202)]。因此，确定
    DRL/DIL 是否有效地从噪声不确定的自然驾驶数据中学习，并根据各种场景生成多模态驾驶行为是有意义的。
- en: VII-F Validation and benchmarks
  id: totrans-552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-F 验证和基准
- en: Validation and benchmarks are especially important for AD, but far from sufficient
    effort has been made regarding these aspects. First, comparison between DRL/DIL
    integrated architectures and traditional architectures is usually neglected in
    the literature, which is meaningful for identifying the quantitative performance
    gains and disadvantages of introducing DRL/DIL. Second, systematic comparison
    between DRL/DIL architectures is necessary. A technical barrier of the former
    two problems is the lack of a reasonable benchmark. High-fidelity simulators such
    as CARLA [[105](#bib.bib105)] may provide a virtual platform on which various
    architectures can be deployed and evaluated. Third, exhaustive validation of trained
    policies before deployment is of vital importance. However, validation is challenging.
    Real-world testing on vehicles has high costs in terms of time, finances and human
    labor and could be dangerous. Empirical validation through simulation can reduce
    the amount of required field testing and can be used as a first step for performance
    and safety evaluation. However, verification through simulation only ensures the
    performance in a statistical sense. Even small variations between the simulators
    and the real scenario can have drastic effects on the system behavior. Future
    studies are needed to identify practical, effective, low-risk and economical validation
    methods.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 验证和基准对于自动驾驶尤其重要，但在这些方面的努力远远不够。首先，文献中通常忽略了 DRL/DIL 集成架构与传统架构之间的比较，这对于识别引入 DRL/DIL
    的定量性能提升和劣势是有意义的。其次，DRL/DIL 架构之间的系统比较是必要的。前两者问题的一个技术障碍是缺乏合理的基准。高保真度的模拟器，如 CARLA[[105](#bib.bib105)]，可能提供一个虚拟平台，允许各种架构进行部署和评估。第三，在部署前对训练政策进行详尽的验证至关重要。然而，验证具有挑战性。现实世界中的车辆测试在时间、财务和人力成本方面都很高，并且可能是危险的。通过模拟进行经验验证可以减少所需的实地测试量，并可以作为性能和安全评估的第一步。然而，模拟验证仅能确保统计意义上的性能。即使是模拟器和真实场景之间的小变化也可能对系统行为产生剧烈影响。未来的研究需要识别实际、有效、低风险且经济的验证方法。
- en: VIII Conclusions
  id: totrans-554
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 结论
- en: 'In this study, a comprehensive survey is presented that focuses on autonomous
    driving policy learning using DRL/DIL, which is addressed simultaneously from
    the system, task-driven and problem-driven perspectives. The study is conducted
    at three levels: First, a taxonomy of the literature studies is presented from
    the system perspective, from which five modes of integration of DRL/DIL models
    into an AD architecture are identified. Second, the formulations of DRL/DIL models
    for accomplishing specified AD tasks are comprehensively reviewed, where various
    designs on the model state and action spaces and the reinforcement learning rewards
    are covered. Finally, an in-depth review is presented on how the critical issues
    of AD applications regarding driving safety, interaction with other traffic participants
    and uncertainty of the environment are addressed by the DRL/DIL models. The major
    findings are listed below, from which potential topics for future investigation
    are identified.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提供了一项全面的调查，重点关注使用DRL/DIL的自主驾驶政策学习，同时从系统、任务驱动和问题驱动的角度进行探讨。研究分为三个层面：首先，从系统角度呈现了文献研究的分类，其中识别出五种将DRL/DIL模型集成到AD架构中的模式。其次，全面回顾了完成指定AD任务的DRL/DIL模型的公式化，其中涵盖了模型状态和动作空间以及强化学习奖励的各种设计。最后，深入审查了DRL/DIL模型如何解决AD应用中关于驾驶安全、与其他交通参与者的互动以及环境不确定性的关键问题。主要发现如下，其中识别出未来研究的潜在主题。
- en: •
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DRL/DIL attract significant amounts of interest in AD research. However, literature
    studies in this scope have focused more on exploring the potential of DRL/DIL
    in accomplishing AD tasks, whereas the design of the system architectures remains
    to be intensively investigated.
  id: totrans-557
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DRL/DIL在AD研究中引起了极大的兴趣。然而，在这一领域的文献研究更侧重于探索DRL/DIL在完成AD任务中的潜力，而系统架构的设计仍需深入研究。
- en: •
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Many DRL/DIL models have been formulated for accomplishing AD tasks. However,
    these formulations rely heavily on empirical designs, which are brute-force approaches
    and lack both theoretical proof and in-depth investigations. In the real-world
    deployment of such models, substantial challenges in terms of stability and robustness
    may be encountered.
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 许多DRL/DIL模型已被公式化用于完成AD任务。然而，这些公式化依赖于经验设计，这些是粗暴的方法，缺乏理论证明和深入研究。在这些模型的实际应用中，可能会遇到稳定性和鲁棒性方面的重大挑战。
- en: •
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Driving safety, which is the main issue in AD applications, has received the
    most attention in the literature. However, the studies on interaction with other
    traffic participants and the uncertainty of the environment remain highly preliminary,
    in which the problems have been addressed from divergent perspectives, and have
    not been conducted systematically.
  id: totrans-561
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 驾驶安全，这是AD应用中的主要问题，已在文献中获得了最多的关注。然而，关于与其他交通参与者的互动和环境不确定性的研究仍然处于初步阶段，问题从不同的角度进行了探讨，但尚未系统地进行研究。
- en: References
  id: totrans-562
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] C. Urmson and W. Whittaker, “Self-driving cars and the urban challenge,”
    *IEEE Intelligent Systems*, vol. 23, no. 2, pp. 66–68, 2008.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] C. Urmson 和 W. Whittaker，“无人驾驶汽车与城市挑战，” *IEEE智能系统*，第23卷，第2期，第66–68页，2008年。'
- en: '[2] S. Thrun, “Toward robotic cars,” *Communications of the ACM*, vol. 53,
    no. 4, pp. 99–106, 2010.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S. Thrun，“朝向机器人汽车，” *ACM通讯*，第53卷，第4期，第99–106页，2010年。'
- en: '[3] A. Eskandarian, *Handbook of intelligent vehicles*.   Springer, 2012, vol. 2.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Eskandarian, *智能车辆手册*。Springer，2012年，第2卷。'
- en: '[4] S. M. Grigorescu, B. Trasnea, T. T. Cocias, and G. Macesanu, “A survey
    of deep learning techniques for autonomous driving,” *J. Field Robotics*, vol. 37,
    no. 3, pp. 362–386, 2020.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. M. Grigorescu, B. Trasnea, T. T. Cocias 和 G. Macesanu，“自主驾驶深度学习技术调查，”
    *领域机器人学*，第37卷，第3期，第362–386页，2020年。'
- en: '[5] W. H. Organization *et al.*, “Global status report on road safety 2018:
    Summary,” World Health Organization, Tech. Rep., 2018.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] W. H. 组织 *等*，“2018年全球道路安全状况报告：摘要，” 世界卫生组织，技术报告，2018年。'
- en: '[6] A. Talebpour and H. S. Mahmassani, “Influence of connected and autonomous
    vehicles on traffic flow stability and throughput,” *Transportation Research Part
    C: Emerging Technologies*, vol. 71, pp. 143–163, 2016.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Talebpour 和 H. S. Mahmassani，“联网与自主车辆对交通流稳定性和通行能力的影响，” *运输研究C部分：新兴技术*，第71卷，第143–163页，2016年。'
- en: '[7] W. Payre, J. Cestac, and P. Delhomme, “Intention to use a fully automated
    car: Attitudes and a priori acceptability,” *Transportation research part F: traffic
    psychology and behaviour*, vol. 27, pp. 252–263, 2014.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] W. Payre, J. Cestac 和 P. Delhomme，“使用完全自动化车辆的意图：态度和先验接受度，”*交通研究F部分：交通心理学和行为*，第27卷，第252–263页，2014年。'
- en: '[8] E. D. Dickmanns and A. Zapp, “Autonomous high speed road vehicle guidance
    by computer vision,” *IFAC Proceedings Volumes*, vol. 20, no. 5, pp. 221–226,
    1987.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] E. D. Dickmanns 和 A. Zapp，“通过计算机视觉实现自主高速公路车辆引导，”*IFAC会议卷*，第20卷，第5期，第221–226页，1987年。'
- en: '[9] C. Thorpe, M. H. Hebert, T. Kanade, and S. A. Shafer, “Vision and navigation
    for the carnegie-mellon navlab,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 10, no. 3, pp. 362–373, 1988.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] C. Thorpe, M. H. Hebert, T. Kanade 和 S. A. Shafer，“卡内基梅隆Navlab的视觉和导航，”*IEEE模式分析与机器智能汇刊*，第10卷，第3期，第362–373页，1988年。'
- en: '[10] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural network,”
    in *Advances in Neural Information Processing Systems*, 1989, pp. 305–313.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] D. A. Pomerleau，“Alvinn：一个神经网络中的自主陆地车辆，”在*神经信息处理系统进展*，1989年，第305–313页。'
- en: '[11] S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel,
    P. Fong, J. Gale, M. Halpenny, G. Hoffmann *et al.*, “Stanley: The robot that
    won the darpa grand challenge,” *Journal of field Robotics*, vol. 23, no. 9, pp.
    661–692, 2006.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel,
    P. Fong, J. Gale, M. Halpenny, G. Hoffmann *等*，“Stanley：赢得DARPA大奖挑战赛的机器人，”*现场机器人学杂志*，第23卷，第9期，第661–692页，2006年。'
- en: '[12] M. Buehler, K. Iagnemma, and S. Singh, *The DARPA urban challenge: autonomous
    vehicles in city traffic*.   springer, 2009, vol. 56.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Buehler, K. Iagnemma 和 S. Singh，*DARPA城市挑战：城市交通中的自主车辆*。Springer，2009年，第56卷。'
- en: '[13] D. González, J. Pérez, V. Milanés, and F. Nashashibi, “A review of motion
    planning techniques for automated vehicles,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 17, no. 4, pp. 1135–1145, 2016.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] D. González, J. Pérez, V. Milanés 和 F. Nashashibi，“自动化车辆运动规划技术综述，”*IEEE智能交通系统汇刊*，第17卷，第4期，第1135–1145页，2016年。'
- en: '[14] X. Li, Z. Sun, D. Cao, Z. He, and Q. Zhu, “Real-time trajectory planning
    for autonomous urban driving: Framework, algorithms, and verifications,” *IEEE/ASME
    Transactions on Mechatronics*, vol. 21, no. 2, pp. 740–753, 2016.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] X. Li, Z. Sun, D. Cao, Z. He 和 Q. Zhu，“自主城市驾驶的实时轨迹规划：框架、算法及验证，”*IEEE/ASME机电一体化汇刊*，第21卷，第2期，第740–753页，2016年。'
- en: '[15] B. Paden, M. Čáp, S. Z. Yong, D. Yershov, and E. Frazzoli, “A survey of
    motion planning and control techniques for self-driving urban vehicles,” *IEEE
    Transactions on Intelligent Vehicles*, vol. 1, no. 1, pp. 33–55, 2016.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] B. Paden, M. Čáp, S. Z. Yong, D. Yershov 和 E. Frazzoli，“自驾城市车辆的运动规划和控制技术调查，”*IEEE智能车辆汇刊*，第1卷，第1期，第33–55页，2016年。'
- en: '[16] S. Ulbrich, A. Reschka, J. Rieken, S. Ernst, G. Bagschik, F. Dierkes,
    M. Nolte, and M. Maurer, “Towards a functional system architecture for automated
    vehicles,” *arXiv preprint arXiv:1703.08557*, 2017.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Ulbrich, A. Reschka, J. Rieken, S. Ernst, G. Bagschik, F. Dierkes,
    M. Nolte 和 M. Maurer，“迈向自动化车辆的功能系统架构，”*arXiv预印本arXiv:1703.08557*，2017年。'
- en: '[17] L. Li, K. Ota, and M. Dong, “Humanlike driving: Empirical decision-making
    system for autonomous vehicles,” *IEEE Trans. Veh. Technol.*, vol. 67, no. 8,
    pp. 6814–6823, 2018\. [Online]. Available: [https://doi.org/10.1109/TVT.2018.2822762](https://doi.org/10.1109/TVT.2018.2822762)'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] L. Li, K. Ota 和 M. Dong，“类人驾驶：自主车辆的经验决策系统，”*IEEE车辆技术汇刊*，第67卷，第8期，第6814–6823页，2018年。[在线]。可用：[https://doi.org/10.1109/TVT.2018.2822762](https://doi.org/10.1109/TVT.2018.2822762)'
- en: '[18] W. Schwarting, J. Alonso-Mora, and D. Rus, “Planning and decision-making
    for autonomous vehicles,” *Annual Review of Control, Robotics, and Autonomous
    Systems*, vol. 1, 05 2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] W. Schwarting, J. Alonso-Mora 和 D. Rus，“自主车辆的规划和决策，”*控制、机器人与自动化系统年鉴*，第1卷，2018年5月。'
- en: '[19] G. Yu and I. K. Sethi, “Road-following with continuous learning,” in *the
    Intelligent Vehicles’ 95\. Symposium*.   IEEE, 1995, pp. 412–417.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] G. Yu 和 I. K. Sethi，“连续学习的道路跟随，”在*智能车辆95周年研讨会*。IEEE，1995年，第412–417页。'
- en: '[20] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger,
    and E. Liang, “Autonomous inverted helicopter flight via reinforcement learning,”
    in *International Symposium on Experimental Robotics*, ser. Springer Tracts in
    Advanced Robotics, vol. 21.   Springer, 2004, pp. 363–372.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger
    和 E. Liang，“通过强化学习实现自主倒置直升机飞行，”在*国际实验机器人研讨会*，系列：Springer高级机器人丛书，第21卷。Springer，2004年，第363–372页。'
- en: '[21] R. S. Sutton and A. G. Barto, *Reinforcement learning: an introduction*.   MIT
    Press, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] R. S. Sutton 和 A. G. Barto，*强化学习：导论*。MIT 出版社，2018 年。'
- en: '[22] Y. LeCun, Y. Bengio, and G. E. Hinton, “Deep learning,” *Nature*, vol.
    521, no. 7553, pp. 436–444, 2015.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. LeCun, Y. Bengio 和 G. E. Hinton，“深度学习，” *自然*，卷 521，第 7553 期，第 436–444
    页，2015 年。'
- en: '[23] I. J. Goodfellow, Y. Bengio, and A. C. Courville, *Deep Learning*, ser.
    Adaptive computation and machine learning.   MIT Press, 2016.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] I. J. Goodfellow, Y. Bengio 和 A. C. Courville，*深度学习*，系列：自适应计算与机器学习。MIT
    出版社，2016 年。'
- en: '[24] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    in *International Conference on Learning Representations*, 2016.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver 和 D. Wierstra，“利用深度强化学习进行连续控制，”在 *国际学习表征会议*，2016 年。'
- en: '[25] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in *International Conference on Robotics and Automation*.   IEEE, 2017, pp. 3357–3364.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei 和 A. Farhadi，“在室内场景中使用深度强化学习进行目标驱动的视觉导航，”在
    *国际机器人与自动化会议*。IEEE，2017 年，第 3357–3364 页。'
- en: '[26] X. Liang, T. Wang, L. Yang, and E. Xing, “Cirl: Controllable imitative
    reinforcement learning for vision-based self-driving,” in *the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 584–599.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] X. Liang, T. Wang, L. Yang 和 E. Xing，“Cirl：用于基于视觉的自驾控制的可控模仿强化学习，”在 *欧洲计算机视觉会议
    (ECCV)*，2018 年，第 584–599 页。'
- en: '[27] Y. Chen, C. Dong, P. Palanisamy, P. Mudalige, K. Muelling, and J. M. Dolan,
    “Attention-based hierarchical deep reinforcement learning for lane change behaviors
    in autonomous driving,” in *IEEE Conference on Computer Vision and Pattern Recognition
    Workshops*, 2019, pp. 0–0.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Chen, C. Dong, P. Palanisamy, P. Mudalige, K. Muelling 和 J. M. Dolan，“基于注意力的层次化深度强化学习用于自动驾驶中的车道变换行为，”在
    *IEEE 计算机视觉与模式识别研讨会*，2019 年，第 0–0 页。'
- en: '[28] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam,
    A. Bewley, and A. Shah, “Learning to drive in a day,” in *International Conference
    on Robotics and Automation*.   IEEE, 2019, pp. 8248–8254.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam,
    A. Bewley 和 A. Shah，“一天内学会驾驶，”在 *国际机器人与自动化会议*。IEEE，2019 年，第 8248–8254 页。'
- en: '[29] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
    reinforcement learning: A brief survey,” *IEEE Signal Processing Magazine*, vol. 34,
    no. 6, pp. 26–38, 2017.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] K. Arulkumaran, M. P. Deisenroth, M. Brundage 和 A. A. Bharath，“深度强化学习：简要调查，”
    *IEEE 信号处理杂志*，卷 34，第 6 期，第 26–38 页，2017 年。'
- en: '[30] Y. Li, “Deep reinforcement learning: An overview,” *CoRR*, vol. abs/1701.07274,
    2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Li, “深度强化学习：概述，” *CoRR*，卷 abs/1701.07274，2017 年。'
- en: '[31] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning
    for multi-agent systems: A review of challenges, solutions and applications,”
    *CoRR*, vol. abs/1812.11794, 2018.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] T. T. Nguyen, N. D. Nguyen 和 S. Nahavandi，“多智能体系统中的深度强化学习：挑战、解决方案和应用综述，”
    *CoRR*，卷 abs/1812.11794，2018 年。'
- en: '[32] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne, “Imitation learning:
    A survey of learning methods,” *ACM Computing Surveys (CSUR)*, vol. 50, no. 2,
    pp. 1–35, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Hussein, M. M. Gaber, E. Elyan 和 C. Jayne，“模仿学习：学习方法综述，” *ACM 计算机调查
    (CSUR)*，卷 50，第 2 期，第 1–35 页，2017 年。'
- en: '[33] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and J. Peters,
    “An algorithmic perspective on imitation learning,” *Found. Trends Robotics*,
    vol. 7, no. 1-2, pp. 1–179, 2018.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel 和 J. Peters，“模仿学习的算法视角，”
    *机器人学基础趋势*，卷 7，第 1-2 期，第 1–179 页，2018 年。'
- en: '[34] S. Kuutti, R. Bowden, Y. Jin, P. Barber, and S. Fallah, “A survey of deep
    learning applications to autonomous vehicle control,” *CoRR*, vol. abs/1912.10773,
    2019.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Kuutti, R. Bowden, Y. Jin, P. Barber 和 S. Fallah，“深度学习在自动驾驶控制中的应用调查，”
    *CoRR*，卷 abs/1912.10773，2019 年。'
- en: '[35] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. K.
    Yogamani, and P. Pérez, “Deep reinforcement learning for autonomous driving: A
    survey,” *CoRR*, vol. abs/2002.00444, 2020.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. K.
    Yogamani 和 P. Pérez，“深度强化学习在自动驾驶中的应用：综述，” *CoRR*，卷 abs/2002.00444，2020 年。'
- en: '[36] S. B. Thrun, “Efficient exploration in reinforcement learning,” 1992.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. B. Thrun，“强化学习中的高效探索，”1992 年。'
- en: '[37] M. Coggan, “Exploration and exploitation in reinforcement learning,” *Research
    supervised by Prof. Doina Precup, CRA-W DMP Project at McGill University*, 2004.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] M. Coggan, “强化学习中的探索与利用，” *由 Doina Precup 教授监督的研究，麦吉尔大学 CRA-W DMP 项目*，2004
    年。'
- en: '[38] Z. Hong, T. Shann, S. Su, Y. Chang, T. Fu, and C. Lee, “Diversity-driven
    exploration strategy for deep reinforcement learning,” in *Advances in Neural
    Information Processing Systems*, 2018.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Z. Hong, T. Shann, S. Su, Y. Chang, T. Fu 和 C. Lee, “用于深度强化学习的多样性驱动探索策略，”
    在 *神经信息处理系统进展*，2018年。'
- en: '[39] G. Shani, J. Pineau, and R. Kaplow, “A survey of point-based pomdp solvers,”
    *Autonomous Agents and Multi-Agent Systems*, vol. 27, no. 1, pp. 1–51, 2013.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] G. Shani, J. Pineau 和 R. Kaplow, “基于点的 POMDP 解算器综述，” *自主代理与多代理系统*，第27卷，第1期，页码1–51，2013年。'
- en: '[40] W. S. Lovejoy, “A survey of algorithmic methods for partially observed
    markov decision processes,” *Annals of Operations Research*, vol. 28, no. 1, pp.
    47–65, 1991.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] W. S. Lovejoy, “部分观测马尔可夫决策过程的算法方法综述，” *运筹学年鉴*，第28卷，第1期，页码47–65，1991年。'
- en: '[41] R. Bellman and R. Kalaba, “On the role of dynamic programming in statistical
    communication theory,” *IRE Trans. Inf. Theory*, vol. 3, no. 3, pp. 197–203, 1957.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] R. Bellman 和 R. Kalaba, “动态规划在统计通信理论中的作用，” *IRE 信息理论汇刊*，第3卷，第3期，页码197–203，1957年。'
- en: '[42] C. J. C. H. Watkins and P. Dayan, “Technical note q-learning,” *Mach.
    Learn.*, vol. 8, pp. 279–292, 1992.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] C. J. C. H. Watkins 和 P. Dayan, “技术说明 Q 学习，” *机器学习*，第8卷，页码279–292，1992年。'
- en: '[43] G. A. Rummery and M. Niranjan, *On-line Q-learning using connectionist
    systems*.   University of Cambridge, Department of Engineering Cambridge, UK,
    1994, vol. 37.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] G. A. Rummery 和 M. Niranjan, *使用连接主义系统的在线 Q 学习*。剑桥大学工程系，英国剑桥，1994年，第37卷。'
- en: '[44] R. Bellman, “Dynamic programming,” *Science*, vol. 153, no. 3731, pp.
    34–37, 1966.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Bellman, “动态规划，” *科学*，第153卷，第3731期，页码34–37，1966年。'
- en: '[45] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski, and et al., “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, pp.
    529–533, 2015.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski 等， “通过深度强化学习实现人类水平的控制，”
    *自然*，第518卷，第7540期，页码529–533，2015年。'
- en: '[46] L.-J. Lin, “Self-improving reactive agents based on reinforcement learning,
    planning and teaching,” *Machine learning*, vol. 8, no. 3-4, pp. 293–321, 1992.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] L.-J. Lin, “基于强化学习、规划和教学的自我改进反应代理，” *机器学习*，第8卷，第3-4期，页码293–321，1992年。'
- en: '[47] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” in *International Conference on Learning Representations*, 2016.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] T. Schaul, J. Quan, I. Antonoglou 和 D. Silver, “优先经验重放，” 在 *国际学习表征会议*，2016年。'
- en: '[48] S. Gu, T. P. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep
    q-learning with model-based acceleration,” in *International Conference on Machine
    Learning*, 2016.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] S. Gu, T. P. Lillicrap, I. Sutskever 和 S. Levine, “基于模型加速的连续深度 Q 学习，”
    在 *国际机器学习会议*，2016年。'
- en: '[49] H. v. Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning,” in *the Thirtieth AAAI Conference on Artificial Intelligence*,
    2016, pp. 2094–2100.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] H. v. Hasselt, A. Guez 和 D. Silver, “双重 Q 学习的深度强化学习，” 在 *第三十届 AAAI 人工智能会议*，2016年，页码2094–2100。'
- en: '[50] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas,
    “Dueling network architectures for deep reinforcement learning,” in *International
    Conference on Machine Learning*, 2016, pp. 1995–2003.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot 和 N. Freitas, “深度强化学习的对抗网络架构，”
    在 *国际机器学习会议*，2016年，页码1995–2003。'
- en: '[51] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos, “Distributional
    reinforcement learning with quantile regression,” in *AAAI Conference on Artificial
    Intelligence*, 2018, pp. 2892–2901.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] W. Dabney, M. Rowland, M. G. Bellemare 和 R. Munos, “通过分位回归进行分布式强化学习，”
    在 *AAAI 人工智能会议*，2018年，页码2892–2901。'
- en: '[52] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Mach. Learn.*, vol. 8, pp. 229–256, 1992.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] R. J. Williams, “用于连接主义强化学习的简单统计梯度跟踪算法，” *机器学习*，第8卷，页码229–256，1992年。'
- en: '[53] L. C. Baird, “Reinforcement learning in continuous time: Advantage updating,”
    in *IEEE International Conference on Neural Networks*, vol. 4.   IEEE, 1994, pp.
    2448–2453.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] L. C. Baird, “连续时间中的强化学习：优势更新，” 在 *IEEE 国际神经网络会议*，第4卷。IEEE，1994年，页码2448–2453。'
- en: '[54] J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel, “High-dimensional
    continuous control using generalized advantage estimation,” in *International
    Conference on Learning Representations*, 2016.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] J. Schulman, P. Moritz, S. Levine, M. I. Jordan 和 P. Abbeel, “使用广义优势估计的高维连续控制，”
    在 *国际学习表征会议*，2016年。'
- en: '[55] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” in *International Conference on Machine Learning*, 2015,
    pp. 1889–1897.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. Schulman, S. Levine, P. Abbeel, M. Jordan, 和 P. Moritz, “信任区域策略优化，”
    *国际机器学习会议*，2015年，第1889–1897页。'
- en: '[56] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *CoRR*, vol. abs/1707.06347, 2017.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, 和 O. Klimov, “邻近政策优化算法，”
    *CoRR*，第abs/1707.06347卷，2017年。'
- en: '[57] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. A. Riedmiller,
    “Deterministic policy gradient algorithms,” in *International Conference on Machine
    Learning*, vol. 32, 2014, pp. 387–395.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, 和 M. A. Riedmiller,
    “确定性策略梯度算法，” *国际机器学习会议*，第32卷，2014年，第387–395页。'
- en: '[58] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International Conference on Machine Learning*, 2016, pp. 1928–1937.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
    Silver, 和 K. Kavukcuoglu, “深度强化学习的异步方法，” *国际机器学习会议*，2016年，第1928–1937页。'
- en: '[59] J. Wang, Z. Kurth-Nelson, H. Soyer, J. Z. Leibo, D. Tirumala, R. Munos,
    C. Blundell, D. Kumaran, and M. M. Botvinick, “Learning to reinforcement learn,”
    in *the 39th Annual Meeting of the Cognitive Science Society, CogSci 2017, London,
    UK, 16-29 July 2017*.   cognitivesciencesociety.org, 2017.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] J. Wang, Z. Kurth-Nelson, H. Soyer, J. Z. Leibo, D. Tirumala, R. Munos,
    C. Blundell, D. Kumaran, 和 M. M. Botvinick, “学习强化学习，” *第39届认知科学学会年会，CogSci 2017，伦敦，英国，2017年7月16日至29日*。cognitivesciencesociety.org，2017年。'
- en: '[60] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar,
    H. Zhu, A. Gupta, P. Abbeel, and S. Levine, “Soft actor-critic algorithms and
    applications,” *CoRR*, vol. abs/1812.05905, 2018.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar,
    H. Zhu, A. Gupta, P. Abbeel, 和 S. Levine, “软演员-评论家算法及其应用，” *CoRR*，第abs/1812.05905卷，2018年。'
- en: '[61] B. D. Argall, S. Chernova, M. M. Veloso, and B. Browning, “A survey of
    robot learning from demonstration,” *Robotics Auton. Syst.*, vol. 57, no. 5, pp.
    469–483, 2009.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] B. D. Argall, S. Chernova, M. M. Veloso, 和 B. Browning, “基于示范的机器人学习综述，”
    *机器人与自主系统*，第57卷，第5期，第469–483页，2009年。'
- en: '[62] H. John and C. James, “Ngsim interstate 80 freeway dataset,” US Fedeal
    Highway Administration, FHWA-HRT-06-137, Washington, DC, USA, Tech. Rep., 2006.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] H. John 和 C. James, “Ngsim州际80号高速公路数据集，” 美国联邦公路管理局，FHWA-HRT-06-137，华盛顿特区，美国，技术报告，2006年。'
- en: '[63] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang *et al.*, “End to end learning for
    self-driving cars,” *arXiv preprint arXiv:1604.07316*, 2016.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang *等*，“端到端学习自驾车，” *arXiv预印本arXiv:1604.07316*，2016年。'
- en: '[64] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner, L. Jackel,
    and U. Muller, “Explaining how a deep neural network trained with end-to-end learning
    steers a car,” *arXiv preprint arXiv:1704.07911*, 2017.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner, L. Jackel,
    和 U. Muller, “解释深度神经网络如何通过端到端学习控制汽车，” *arXiv预印本arXiv:1704.07911*，2017年。'
- en: '[65] H. Xu, Y. Gao, F. Yu, and T. Darrell, “End-to-end learning of driving
    models from large-scale video datasets,” in *IEEE conference on Computer Vision
    and Pattern Recognition*, 2017, pp. 2174–2182.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] H. Xu, Y. Gao, F. Yu, 和 T. Darrell, “从大规模视频数据集中端到端学习驾驶模型，” *IEEE计算机视觉与模式识别会议*，2017年，第2174–2182页。'
- en: '[66] D. A. Pomerleau, “Efficient training of artificial neural networks for
    autonomous navigation,” *Neural computation*, vol. 3, no. 1, pp. 88–97, 1991.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] D. A. Pomerleau, “高效训练用于自主导航的人工神经网络，” *神经计算*，第3卷，第1期，第88–97页，1991年。'
- en: '[67] S. Ross and D. Bagnell, “Efficient reductions for imitation learning,”
    in *International Conference on Artificial Intelligence and Statistics*, ser.
    JMLR Proceedings, vol. 9.   JMLR.org, 2010, pp. 661–668.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] S. Ross 和 D. Bagnell, “模仿学习的高效减少方法，” *国际人工智能与统计会议*，JMLR会议论文系列，第9卷。JMLR.org，2010年，第661–668页。'
- en: '[68] S. Ross, G. J. Gordon, and D. Bagnell, “A reduction of imitation learning
    and structured prediction to no-regret online learning,” in *International Conference
    on Artificial Intelligence and Statistics*, ser. JMLR Proceedings, vol. 15, 2011,
    pp. 627–635.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] S. Ross, G. J. Gordon, 和 D. Bagnell, “将模仿学习和结构化预测简化为无悔在线学习，” *国际人工智能与统计会议*，JMLR会议论文系列，第15卷，2011年，第627–635页。'
- en: '[69] J. Zhang and K. Cho, “Query-efficient imitation learning for end-to-end
    autonomous driving,” *arXiv preprint arXiv:1605.06450*, 2016.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] J. Zhang 和 K. Cho，“端到端自主驾驶的查询高效模仿学习”，*arXiv预印本 arXiv:1605.06450*，2016年。'
- en: '[70] F. Codevilla, M. Miiller, A. López, V. Koltun, and A. Dosovitskiy, “End-to-end
    driving via conditional imitation learning,” in *International Conference on Robotics
    and Automation*.   IEEE, 2018, pp. 1–9.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] F. Codevilla, M. Miuller, A. López, V. Koltun, 和 A. Dosovitskiy，“通过条件模仿学习进行端到端驾驶”，发表于
    *机器人与自动化国际大会*。 IEEE, 2018年，第1–9页。'
- en: '[71] J. Chen, B. Yuan, and M. Tomizuka, “Deep imitation learning for autonomous
    driving in generic urban scenarios with enhanced safety,” in *IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2019, pp. 2884–2890.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J. Chen, B. Yuan, 和 M. Tomizuka，“用于通用城市场景的深度模仿学习与增强安全性”，发表于 *IEEE/RSJ国际智能机器人与系统大会（IROS）*，2019年，第2884–2890页。'
- en: '[72] A. Y. Ng and S. J. Russell, “Algorithms for inverse reinforcement learning,”
    in *International Conference on Machine Learning*, 2000, pp. 663–670.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Y. Ng 和 S. J. Russell，“逆强化学习算法”，发表于 *国际机器学习大会*，2000年，第663–670页。'
- en: '[73] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement
    learning,” in *International Conference on Machine Learning*, 2004, p. 1.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] P. Abbeel 和 A. Y. Ng，“通过逆强化学习进行学徒学习”，发表于 *国际机器学习大会*，2004年，第1页。'
- en: '[74] N. D. Ratliff, J. A. Bagnell, and M. Zinkevich, “Maximum margin planning,”
    in *International Conference on Machine Learning*, vol. 148, 2006, pp. 729–736.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] N. D. Ratliff, J. A. Bagnell, 和 M. Zinkevich，“最大边际规划”，发表于 *国际机器学习大会*，卷148，2006年，第729–736页。'
- en: '[75] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, “Maximum entropy
    inverse reinforcement learning,” in *Aaai*, vol. 8, 2008, pp. 1433–1438.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] B. D. Ziebart, A. L. Maas, J. A. Bagnell, 和 A. K. Dey，“最大熵逆强化学习”，发表于 *Aaai*，卷8，2008年，第1433–1438页。'
- en: '[76] S. Levine, Z. Popovic, and V. Koltun, “Nonlinear inverse reinforcement
    learning with gaussian processes,” in *Advances in Neural Information Processing
    Systems*, 2011, pp. 19–27.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S. Levine, Z. Popovic, 和 V. Koltun，“具有高斯过程的非线性逆强化学习”，发表于 *神经信息处理系统进展*，2011年，第19–27页。'
- en: '[77] N. D. Ratliff, D. M. Bradley, J. A. Bagnell, and J. E. Chestnutt, “Boosting
    structured prediction for imitation learning,” in *Advances in Neural Information
    Processing Systems*, 2006, pp. 1153–1160.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] N. D. Ratliff, D. M. Bradley, J. A. Bagnell, 和 J. E. Chestnutt，“提升结构化预测以用于模仿学习”，发表于
    *神经信息处理系统进展*，2006年，第1153–1160页。'
- en: '[78] N. D. Ratliff, D. Silver, and J. A. Bagnell, “Learning to search: Functional
    gradient techniques for imitation learning,” *Auton. Robots*, vol. 27, no. 1,
    pp. 25–53, 2009.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] N. D. Ratliff, D. Silver, 和 J. A. Bagnell，“学习搜索：用于模仿学习的函数梯度技术”，*自主机器人*，卷27，第1期，第25–53页，2009年。'
- en: '[79] M. Wulfmeier, P. Ondruska, and I. Posner, “Maximum entropy deep inverse
    reinforcement learning,” *arXiv preprint arXiv:1507.04888*, 2015.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] M. Wulfmeier, P. Ondruska, 和 I. Posner，“最大熵深度逆强化学习”，*arXiv预印本 arXiv:1507.04888*，2015年。'
- en: '[80] C. Finn, S. Levine, and P. Abbeel, “Guided cost learning: Deep inverse
    optimal control via policy optimization,” in *International Conference on Machine
    Learning*, 2016, pp. 49–58.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] C. Finn, S. Levine, 和 P. Abbeel，“引导成本学习：通过策略优化的深度逆最优控制”，发表于 *国际机器学习大会*，2016年，第49–58页。'
- en: '[81] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in *Advances
    in Neural Information Processing Systems*, 2016, pp. 4565–4573.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] J. Ho 和 S. Ermon，“生成对抗模仿学习”，发表于 *神经信息处理系统进展*，2016年，第4565–4573页。'
- en: '[82] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems*, 2014, pp. 2672–2680.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络”，发表于 *神经信息处理系统进展*，2014年，第2672–2680页。'
- en: '[83] C. Finn, P. F. Christiano, P. Abbeel, and S. Levine, “A connection between
    generative adversarial networks, inverse reinforcement learning, and energy-based
    models,” *CoRR*, vol. abs/1611.03852, 2016.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] C. Finn, P. F. Christiano, P. Abbeel, 和 S. Levine，“生成对抗网络、逆强化学习和基于能量的模型之间的联系”，*CoRR*，卷abs/1611.03852，2016年。'
- en: '[84] J. Fu, K. Luo, and S. Levine, “Learning robust rewards with adversarial
    inverse reinforcement learning,” *CoRR*, vol. abs/1710.11248, 2017.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] J. Fu, K. Luo, 和 S. Levine，“通过对抗性逆强化学习学习鲁棒奖励”，*CoRR*，卷abs/1710.11248，2017年。'
- en: '[85] U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. L. Cun, “Off-road obstacle
    avoidance through end-to-end learning,” in *Advances in neural information processing
    systems*, 2006, pp. 739–746.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] U. Muller, J. Ben, E. Cosatto, B. Flepp, 和 Y. L. Cun，“通过端到端学习进行越野障碍物避让”，发表于
    *神经信息处理系统进展*，2006, 页码 739–746。'
- en: '[86] V. Rausch, A. Hansen, E. Solowjow, C. Liu, E. Kreuzer, and J. K. Hedrick,
    “Learning a deep neural net policy for end-to-end control of autonomous vehicles,”
    in *2017 American Control Conference (ACC)*.   IEEE, 2017, pp. 4914–4919.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] V. Rausch, A. Hansen, E. Solowjow, C. Liu, E. Kreuzer, 和 J. K. Hedrick，“为自主车辆的端到端控制学习深度神经网络策略”，发表于
    *2017年美国控制会议（ACC）*。IEEE, 2017, 页码 4914–4919。'
- en: '[87] H. M. Eraqi, M. N. Moustafa, and J. Honer, “End-to-end deep learning for
    steering autonomous vehicles considering temporal dependencies,” *arXiv preprint
    arXiv:1710.03804*, 2017.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. M. Eraqi, M. N. Moustafa, 和 J. Honer，“考虑时间依赖性的端到端深度学习用于自主车辆的转向”，*arXiv预印本
    arXiv:1710.03804*，2017。'
- en: '[88] D. Wang, C. Devin, Q.-Z. Cai, F. Yu, and T. Darrell, “Deep object-centric
    policies for autonomous driving,” in *International Conference on Robotics and
    Automation*.   IEEE, 2019, pp. 8853–8859.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] D. Wang, C. Devin, Q.-Z. Cai, F. Yu, 和 T. Darrell，“用于自动驾驶的深度物体中心策略”，发表于
    *国际机器人与自动化会议*。IEEE, 2019, 页码 8853–8859。'
- en: '[89] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and B. Boots,
    “Agile autonomous driving using end-to-end deep imitation learning,” *arXiv preprint
    arXiv:1709.07174*, 2017.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, 和 B. Boots，“使用端到端深度模仿学习的灵活自动驾驶”，*arXiv预印本
    arXiv:1709.07174*，2017。'
- en: '[90] M. Bouton, A. Nakhaei, K. Fujimura, and M. J. Kochenderfer, “Safe reinforcement
    learning with scene decomposition for navigating complex urban environments,”
    in *Intelligent Vehicles Symposium*.   IEEE, 2019, pp. 1469–1476.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. Bouton, A. Nakhaei, K. Fujimura, 和 M. J. Kochenderfer，“通过场景分解实现安全强化学习以导航复杂城市环境”，发表于
    *智能车辆研讨会*。IEEE, 2019, 页码 1469–1476。'
- en: '[91] P. Wang, C.-Y. Chan, and A. de La Fortelle, “A reinforcement learning
    based approach for automated lane change maneuvers,” in *Intelligent Vehicles
    Symposium*.   IEEE, 2018, pp. 1379–1384.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] P. Wang, C.-Y. Chan, 和 A. de La Fortelle，“基于强化学习的自动车道变换策略”，发表于 *智能车辆研讨会*。IEEE,
    2018, 页码 1379–1384。'
- en: '[92] X. Chen, Y. Zhai, C. Lu, J. Gong, and G. Wang, “A learning model for personalized
    adaptive cruise control,” in *Intelligent Vehicles Symposium*.   IEEE, 2017, pp.
    379–384.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] X. Chen, Y. Zhai, C. Lu, J. Gong, 和 G. Wang，“个性化自适应巡航控制学习模型”，发表于 *智能车辆研讨会*。IEEE,
    2017, 页码 379–384。'
- en: '[93] P. Wang and C.-Y. Chan, “Formulation of deep reinforcement learning architecture
    toward autonomous driving for on-ramp merge,” in *International Conference on
    Intelligent Transportation Systems*.   IEEE, 2017, pp. 1–6.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] P. Wang 和 C.-Y. Chan，“面向自动驾驶的深度强化学习架构的制定用于匝道合流”，发表于 *国际智能交通系统会议*。IEEE,
    2017, 页码 1–6。'
- en: '[94] H. Chae, C. M. Kang, B. Kim, J. Kim, C. C. Chung, and J. W. Choi, “Autonomous
    braking system via deep reinforcement learning,” in *International Conference
    on Intelligent Transportation Systems*.   IEEE, 2017, pp. 1–6.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] H. Chae, C. M. Kang, B. Kim, J. Kim, C. C. Chung, 和 J. W. Choi，“通过深度强化学习的自动制动系统”，发表于
    *国际智能交通系统会议*。IEEE, 2017, 页码 1–6。'
- en: '[95] M. Bouton, A. Nakhaei, K. Fujimura, and M. J. Kochenderfer, “Cooperation-aware
    reinforcement learning for merging in dense traffic,” in *IEEE Intelligent Transportation
    Systems Conference*.   IEEE, 2019, pp. 3441–3447.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] M. Bouton, A. Nakhaei, K. Fujimura, 和 M. J. Kochenderfer，“面向密集交通的合作意识强化学习”，发表于
    *IEEE智能交通系统会议*。IEEE, 2019, 页码 3441–3447。'
- en: '[96] Y. Tang, “Towards learning multi-agent negotiations via self-play,” in
    *IEEE International Conference on Computer Vision Workshops*, 2019, pp. 0–0.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Y. Tang，“通过自我博弈学习多智能体协商”，发表于 *IEEE国际计算机视觉研讨会*，2019, 页码 0–0。'
- en: '[97] A. Folkers, M. Rick, and C. Büskens, “Controlling an autonomous vehicle
    with deep reinforcement learning,” in *Intelligent Vehicles Symposium*.   IEEE,
    2019, pp. 2025–2031.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] A. Folkers, M. Rick, 和 C. Büskens，“通过深度强化学习控制自动驾驶车辆”，发表于 *智能车辆研讨会*。IEEE,
    2019, 页码 2025–2031。'
- en: '[98] H. Porav and P. Newman, “Imminent collision mitigation with reinforcement
    learning and vision,” in *International Conference on Intelligent Transportation
    Systems*.   IEEE, 2018, pp. 958–964.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] H. Porav 和 P. Newman，“通过强化学习和视觉减少即将发生的碰撞”，发表于 *国际智能交通系统会议*。IEEE, 2018,
    页码 958–964。'
- en: '[99] S. Wang, D. Jia, and X. Weng, “Deep reinforcement learning for autonomous
    driving,” *arXiv preprint arXiv:1811.11329*, 2018.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] S. Wang, D. Jia, 和 X. Weng，“用于自动驾驶的深度强化学习”，*arXiv预印本 arXiv:1811.11329*，2018。'
- en: '[100] P. Wang, H. Li, and C.-Y. Chan, “Continuous control for automated lane
    change behavior based on deep deterministic policy gradient algorithm,” in *Intelligent
    Vehicles Symposium*.   IEEE, 2019, pp. 1454–1460.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] P. Wang, H. Li, 和 C.-Y. Chan, “基于深度确定性策略梯度算法的自动化车道变换行为连续控制，” 见于 *智能车辆研讨会*。IEEE，2019年，第1454–1460页。'
- en: '[101] M. Kaushik, V. Prasad, K. M. Krishna, and B. Ravindran, “Overtaking maneuvers
    in simulated highway driving using deep reinforcement learning,” in *Intelligent
    Vehicles Symposium*.   IEEE, 2018, pp. 1885–1890.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] M. Kaushik, V. Prasad, K. M. Krishna, 和 B. Ravindran, “在模拟高速公路驾驶中进行超车机动的深度强化学习，”
    见于 *智能车辆研讨会*。IEEE，2018年，第1885–1890页。'
- en: '[102] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “End-to-end deep reinforcement
    learning for lane keeping assist,” *arXiv preprint arXiv:1612.04340*, 2016.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] A. E. Sallab, M. Abdou, E. Perot, 和 S. Yogamani, “用于车道保持辅助的端到端深度强化学习，”
    *arXiv预印本 arXiv:1612.04340*，2016年。'
- en: '[103] R. Vasquez and B. Farooq, “Multi-objective autonomous braking system
    using naturalistic dataset,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE,
    2019, pp. 4348–4353.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] R. Vasquez 和 B. Farooq, “基于自然驾驶数据集的多目标自主制动系统，” 见于 *IEEE智能交通系统会议*。IEEE，2019年，第4348–4353页。'
- en: '[104] S. Hecker, D. Dai, and L. Van Gool, “End-to-end learning of driving models
    with surround-view cameras and route planners,” in *the European Conference on
    Computer Vision (ECCV)*, 2018, pp. 435–453.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] S. Hecker, D. Dai, 和 L. Van Gool, “利用全景摄像头和路线规划器进行驾驶模型的端到端学习，” 见于 *欧洲计算机视觉会议（ECCV）*，2018年，第435–453页。'
- en: '[105] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla:
    An open urban driving simulator,” *arXiv preprint arXiv:1711.03938*, 2017.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, 和 V. Koltun, “Carla:
    一个开放的城市驾驶模拟器，” *arXiv预印本 arXiv:1711.03938*，2017年。'
- en: '[106] M. Abdou, H. Kamal, S. El-Tantawy, A. Abdelkhalek, O. Adel, K. Hamdy,
    and M. Abaas, “End-to-end deep conditional imitation learning for autonomous driving,”
    in *2019 31st International Conference on Microelectronics (ICM)*.   IEEE, 2019,
    pp. 346–350.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] M. Abdou, H. Kamal, S. El-Tantawy, A. Abdelkhalek, O. Adel, K. Hamdy,
    和 M. Abaas, “用于自主驾驶的端到端深度条件模仿学习，” 见于 *2019年第31届国际微电子会议（ICM）*。IEEE，2019年，第346–350页。'
- en: '[107] Y. Cui, D. Isele, S. Niekum, and K. Fujimura, “Uncertainty-aware data
    aggregation for deep imitation learning,” in *International Conference on Robotics
    and Automation*.   IEEE, 2019, pp. 761–767.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Y. Cui, D. Isele, S. Niekum, 和 K. Fujimura, “针对深度模仿学习的不确定性感知数据聚合，” 见于
    *国际机器人与自动化会议*。IEEE，2019年，第761–767页。'
- en: '[108] L. Tai, P. Yun, Y. Chen, C. Liu, H. Ye, and M. Liu, “Visual-based autonomous
    driving deployment from a stochastic and uncertainty-aware perspective,” *arXiv
    preprint arXiv:1903.00821*, 2019.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] L. Tai, P. Yun, Y. Chen, C. Liu, H. Ye, 和 M. Liu, “从随机和不确定性感知的角度进行视觉基础的自主驾驶部署，”
    *arXiv预印本 arXiv:1903.00821*，2019年。'
- en: '[109] M. Buechel and A. Knoll, “Deep reinforcement learning for predictive
    longitudinal control of automated vehicles,” in *International Conference on Intelligent
    Transportation Systems*.   IEEE, 2018, pp. 2391–2397.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] M. Buechel 和 A. Knoll, “用于自动驾驶车辆预测性纵向控制的深度强化学习，” 见于 *智能交通系统国际会议*。IEEE，2018年，第2391–2397页。'
- en: '[110] J. Chen, B. Yuan, and M. Tomizuka, “Model-free deep reinforcement learning
    for urban autonomous driving,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE,
    2019, pp. 2765–2771.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. Chen, B. Yuan, 和 M. Tomizuka, “用于城市自主驾驶的无模型深度强化学习，” 见于 *IEEE智能交通系统会议*。IEEE，2019年，第2765–2771页。'
- en: '[111] M. Bansal, A. Krizhevsky, and A. Ogale, “Chauffeurnet: Learning to drive
    by imitating the best and synthesizing the worst,” *arXiv preprint arXiv:1812.03079*,
    2018.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] M. Bansal, A. Krizhevsky, 和 A. Ogale, “Chauffeurnet: 通过模仿最优秀的驾驶者和合成最差的驾驶者来学习驾驶，”
    *arXiv预印本 arXiv:1812.03079*，2018年。'
- en: '[112] L. Sun, C. Peng, W. Zhan, and M. Tomizuka, “A fast integrated planning
    and control framework for autonomous driving via imitation learning,” in *Dynamic
    Systems and Control Conference*, vol. 51913.   American Society of Mechanical
    Engineers, 2018, p. V003T37A012.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] L. Sun, C. Peng, W. Zhan, 和 M. Tomizuka, “通过模仿学习实现的自主驾驶快速集成规划与控制框架，”
    见于 *动态系统与控制会议*，卷51913。美国机械工程师学会，2018年，第V003T37A012页。'
- en: '[113] M. Wulfmeier, D. Rao, D. Z. Wang, P. Ondruska, and I. Posner, “Large-scale
    cost function learning for path planning using deep inverse reinforcement learning,”
    *The International Journal of Robotics Research*, vol. 36, no. 10, pp. 1073–1087,
    2017.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] M. Wulfmeier, D. Rao, D. Z. Wang, P. Ondruska, 和 I. Posner, “使用深度逆强化学习进行路径规划的大规模成本函数学习，”
    *国际机器人研究期刊*，第36卷，第10期，第1073–1087页，2017年。'
- en: '[114] J. Bernhard, R. Gieselmann, K. Esterle, and A. Knol, “Experience-based
    heuristic search: Robust motion planning with deep q-learning,” in *International
    Conference on Intelligent Transportation Systems*.   IEEE, 2018, pp. 3175–3182.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] J. Bernhard, R. Gieselmann, K. Esterle, 和 A. Knol，"基于经验的启发式搜索：使用深度Q学习的鲁棒运动规划"，发表于*国际智能交通系统会议*。IEEE，2018年，第3175–3182页。'
- en: '[115] P. Hart, L. Rychly, and A. Knoll, “Lane-merging using policy-based reinforcement
    learning and post-optimization,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE,
    2019, pp. 3176–3181.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] P. Hart, L. Rychly, 和 A. Knoll，"利用基于策略的强化学习和后优化进行车道合并"，发表于*IEEE智能交通系统会议*。IEEE，2019年，第3176–3181页。'
- en: '[116] P. Wang, D. Liu, J. Chen, H. Li, and C.-Y. Chan, “Human-like decision
    making for autonomous driving via adversarial inverse reinforcement learning,”
    *arXiv*, pp. arXiv–1911, 2019.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] P. Wang, D. Liu, J. Chen, H. Li, 和 C.-Y. Chan，"通过对抗性逆强化学习实现类人决策的自动驾驶"，*arXiv*，第arXiv–1911页，2019年。'
- en: '[117] S. Sharifzadeh, I. Chiotellis, R. Triebel, and D. Cremers, “Learning
    to drive using inverse reinforcement learning and deep q-networks,” *arXiv preprint
    arXiv:1612.03653*, 2016.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. Sharifzadeh, I. Chiotellis, R. Triebel, 和 D. Cremers，"利用逆强化学习和深度Q网络进行驾驶学习"，*arXiv预印本arXiv:1612.03653*，2016年。'
- en: '[118] A. Alizadeh, M. Moghadam, Y. Bicer, N. K. Ure, U. Yavas, and C. Kurtulus,
    “Automated lane change decision making using deep reinforcement learning in dynamic
    and uncertain highway environment,” in *IEEE Intelligent Transportation Systems
    Conference*.   IEEE, 2019, pp. 1399–1404.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] A. Alizadeh, M. Moghadam, Y. Bicer, N. K. Ure, U. Yavas, 和 C. Kurtulus，"在动态和不确定的高速公路环境中使用深度强化学习进行自动车道变换决策"，发表于*IEEE智能交通系统会议*。IEEE，2019年，第1399–1404页。'
- en: '[119] N. Deshpande and A. Spalanzani, “Deep reinforcement learning based vehicle
    navigation amongst pedestrians using a grid-based state representation,” in *IEEE
    Intelligent Transportation Systems Conference*.   IEEE, 2019, pp. 2081–2086.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] N. Deshpande 和 A. Spalanzani，"基于深度强化学习的车辆在行人之间导航，使用基于网格的状态表示"，发表于*IEEE智能交通系统会议*。IEEE，2019年，第2081–2086页。'
- en: '[120] T. Tram, I. Batkovic, M. Ali, and J. Sjöberg, “Learning when to drive
    in intersections by combining reinforcement learning and model predictive control,”
    in *International Conference on Intelligent Transportation Systems*.   IEEE, 2019,
    pp. 3263–3268.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] T. Tram, I. Batkovic, M. Ali, 和 J. Sjöberg，"通过结合强化学习和模型预测控制来学习在交叉口驾驶的时机"，发表于*国际智能交通系统会议*。IEEE，2019年，第3263–3268页。'
- en: '[121] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura, “Navigating
    occluded intersections with autonomous vehicles using deep reinforcement learning,”
    in *International Conference on Robotics and Automation*.   IEEE, 2018, pp. 2034–2039.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, 和 K. Fujimura，"使用深度强化学习在遮挡交叉口导航的自动驾驶车辆"，发表于*国际机器人与自动化会议*。IEEE，2018年，第2034–2039页。'
- en: '[122] C. Li and K. Czarnecki, “Urban driving with multi-objective deep reinforcement
    learning,” in *International Conference on Autonomous Agents and MultiAgent Systems
    (AAMAS)*.   International Foundation for Autonomous Agents and Multiagent Systems,
    2019, pp. 359–367.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] C. Li 和 K. Czarnecki，"使用多目标深度强化学习的城市驾驶"，发表于*国际自主代理和多代理系统会议（AAMAS）*。国际自主代理与多代理系统基金会，2019年，第359–367页。'
- en: '[123] M. P. Ronecker and Y. Zhu, “Deep q-network based decision making for
    autonomous driving,” in *International Conference on Robotics and Automation Sciences*.   IEEE,
    2019, pp. 154–160.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] M. P. Ronecker 和 Y. Zhu，"基于深度Q网络的自动驾驶决策"，发表于*国际机器人与自动化科学会议*。IEEE，2019年，第154–160页。'
- en: '[124] P. Wolf, K. Kurzer, T. Wingert, F. Kuhnt, and J. M. Zollner, “Adaptive
    behavior generation for autonomous driving using deep reinforcement learning with
    compact semantic states,” in *Intelligent Vehicles Symposium*.   IEEE, 2018, pp.
    993–1000.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] P. Wolf, K. Kurzer, T. Wingert, F. Kuhnt, 和 J. M. Zollner，"使用紧凑语义状态的深度强化学习进行自动驾驶的自适应行为生成"，发表于*智能车辆研讨会*。IEEE，2018年，第993–1000页。'
- en: '[125] B. Mirchevska, C. Pek, M. Werling, M. Althoff, and J. Boedecker, “High-level
    decision making for safe and reasonable autonomous lane changing using reinforcement
    learning,” in *International Conference on Intelligent Transportation Systems*.   IEEE,
    2018, pp. 2156–2162.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] B. Mirchevska, C. Pek, M. Werling, M. Althoff, 和 J. Boedecker，"使用强化学习进行安全和合理的自动车道变换的高层决策"，发表于*国际智能交通系统会议*。IEEE，2018年，第2156–2162页。'
- en: '[126] W. Yuan, M. Yang, Y. He, C. Wang, and B. Wang, “Multi-reward architecture
    based reinforcement learning for highway driving policies,” in *International
    Conference on Intelligent Transportation Systems*.   IEEE, 2019, pp. 3810–3815.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] W. Yuan, M. Yang, Y. He, C. Wang, 和 B. Wang，“基于多奖励架构的强化学习用于高速公路驾驶策略，”
    见于 *国际智能交通系统会议*。IEEE, 2019, 页码 3810–3815。'
- en: '[127] J. Lee and J. W. Choi, “May i cut into your lane?: A policy network to
    learn interactive lane change behavior for autonomous driving,” in *IEEE Intelligent
    Transportation Systems Conference*.   IEEE, 2019, pp. 4342–4347.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Lee 和 J. W. Choi，“我可以切入你的车道吗？：一种学习交互车道变换行为的策略网络，” 见于 *IEEE 智能交通系统会议*。IEEE,
    2019, 页码 4342–4347。'
- en: '[128] C. You, J. Lu, D. Filev, and P. Tsiotras, “Highway traffic modeling and
    decision making for autonomous vehicle using reinforcement learning,” in *Intelligent
    Vehicles Symposium*.   IEEE, 2018, pp. 1227–1232.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] C. You, J. Lu, D. Filev, 和 P. Tsiotras，“使用强化学习进行高速公路交通建模和自主车辆决策，” 见于
    *智能车辆研讨会*。IEEE, 2018, 页码 1227–1232。'
- en: '[129] L. Wang, F. Ye, Y. Wang, J. Guo, I. Papamichail, M. Papageorgiou, S. Hu,
    and L. Zhang, “A q-learning foresighted approach to ego-efficient lane changes
    of connected and automated vehicles on freeways,” in *IEEE Intelligent Transportation
    Systems Conference*.   IEEE, 2019, pp. 1385–1392.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] L. Wang, F. Ye, Y. Wang, J. Guo, I. Papamichail, M. Papageorgiou, S.
    Hu, 和 L. Zhang，“一种面向前瞻的 Q 学习方法用于高速公路上连接和自动化车辆的自我高效车道变换，” 见于 *IEEE 智能交通系统会议*。IEEE,
    2019, 页码 1385–1392。'
- en: '[130] C.-J. Hoel, K. Wolff, and L. Laine, “Automated speed and lane change
    decision making using deep reinforcement learning,” in *International Conference
    on Intelligent Transportation Systems*.   IEEE, 2018, pp. 2148–2155.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] C.-J. Hoel, K. Wolff, 和 L. Laine，“使用深度强化学习进行自动化速度和车道变换决策，” 见于 *国际智能交通系统会议*。IEEE,
    2018, 页码 2148–2155。'
- en: '[131] Y. Zhang, P. Sun, Y. Yin, L. Lin, and X. Wang, “Human-like autonomous
    vehicle speed control by deep reinforcement learning with double q-learning,”
    in *Intelligent Vehicles Symposium*.   IEEE, 2018, pp. 1251–1256.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Y. Zhang, P. Sun, Y. Yin, L. Lin, 和 X. Wang，“通过深度强化学习和双重 Q 学习实现类人自主车辆速度控制，”
    见于 *智能车辆研讨会*。IEEE, 2018, 页码 1251–1256。'
- en: '[132] D. Liu, M. Brännstrom, A. Backhouse, and L. Svensson, “Learning faster
    to perform autonomous lane changes by constructing maneuvers from shielded semantic
    actions,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE, 2019,
    pp. 1838–1844.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] D. Liu, M. Brännstrom, A. Backhouse, 和 L. Svensson，“通过从保护语义动作中构建机动来加速自主车道变换学习，”
    见于 *IEEE 智能交通系统会议*。IEEE, 2019, 页码 1838–1844。'
- en: '[133] K. Min, H. Kim, and K. Huh, “Deep distributional reinforcement learning
    based high-level driving policy determination,” *IEEE Transactions on Intelligent
    Vehicles*, vol. 4, no. 3, pp. 416–424, 2019.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] K. Min, H. Kim, 和 K. Huh，“基于深度分布式强化学习的高层驾驶策略确定，” *IEEE 智能车辆交易*，第4卷，第3期，页码
    416–424, 2019。'
- en: '[134] K. Rezaee, P. Yadmellat, M. S. Nosrati, E. A. Abolfathi, M. Elmahgiubi,
    and J. Luo, “Multi-lane cruising using hierarchical planning and reinforcement
    learning,” in *International Conference on Intelligent Transportation Systems*.   IEEE,
    2019, pp. 1800–1806.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] K. Rezaee, P. Yadmellat, M. S. Nosrati, E. A. Abolfathi, M. Elmahgiubi,
    和 J. Luo，“使用分层规划和强化学习进行多车道巡航，” 见于 *国际智能交通系统会议*。IEEE, 2019, 页码 1800–1806。'
- en: '[135] T. Shi, P. Wang, X. Cheng, C.-Y. Chan, and D. Huang, “Driving decision
    and control for automated lane change behavior based on deep reinforcement learning,”
    in *IEEE Intelligent Transportation Systems Conference*.   IEEE, 2019, pp. 2895–2900.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] T. Shi, P. Wang, X. Cheng, C.-Y. Chan, 和 D. Huang，“基于深度强化学习的自动车道变换行为的驾驶决策与控制，”
    见于 *IEEE 智能交通系统会议*。IEEE, 2019, 页码 2895–2900。'
- en: '[136] J. Chen, Z. Wang, and M. Tomizuka, “Deep hierarchical reinforcement learning
    for autonomous driving with distinct behaviors,” in *Intelligent Vehicles Symposium*.   IEEE,
    2018, pp. 1239–1244.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] J. Chen, Z. Wang, 和 M. Tomizuka，“用于自主驾驶的深度分层强化学习与不同的行为，” 见于 *智能车辆研讨会*。IEEE,
    2018, 页码 1239–1244。'
- en: '[137] Z. Qiao, K. Muelling, J. Dolan, P. Palanisamy, and P. Mudalige, “Pomdp
    and hierarchical options mdp with continuous actions for autonomous driving at
    intersections,” in *International Conference on Intelligent Transportation Systems*.   IEEE,
    2018, pp. 2377–2382.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Z. Qiao, K. Muelling, J. Dolan, P. Palanisamy, 和 P. Mudalige，“用于交叉路口自主驾驶的Pomdp和分层选项mdp与连续动作，”
    见于 *国际智能交通系统会议*。IEEE, 2018, 页码 2377–2382。'
- en: '[138] C. Paxton, V. Raman, G. D. Hager, and M. Kobilarov, “Combining neural
    networks and tree search for task and motion planning in challenging environments,”
    in *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2017, pp. 6059–6066.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] C. Paxton, V. Raman, G. D. Hager 和 M. Kobilarov，“在挑战性环境中结合神经网络和树搜索进行任务和运动规划，”在
    *IEEE/RSJ 国际智能机器人与系统大会 (IROS)* 上。 IEEE，2017 年，页码 6059–6066。'
- en: '[139] F. Behbahani, K. Shiarlis, X. Chen, V. Kurin, S. Kasewa, C. Stirbu, J. Gomes,
    S. Paul, F. A. Oliehoek, J. Messias *et al.*, “Learning from demonstration in
    the wild,” in *International Conference on Robotics and Automation*.   IEEE, 2019,
    pp. 775–781.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] F. Behbahani, K. Shiarlis, X. Chen, V. Kurin, S. Kasewa, C. Stirbu, J.
    Gomes, S. Paul, F. A. Oliehoek, J. Messias *等*，“在野外的示范学习，”在 *国际机器人与自动化会议* 上。 IEEE，2019
    年，页码 775–781。'
- en: '[140] L. Chen, Y. Chen, X. Yao, Y. Shan, and L. Chen, “An adaptive path tracking
    controller based on reinforcement learning with urban driving application,” in
    *Intelligent Vehicles Symposium*.   IEEE, 2019, pp. 2411–2416.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] L. Chen, Y. Chen, X. Yao, Y. Shan 和 L. Chen，“基于强化学习的适应性路径跟踪控制器及其城市驾驶应用，”在
    *智能车辆研讨会* 上。 IEEE，2019 年，页码 2411–2416。'
- en: '[141] M. Huegle, G. Kalweit, B. Mirchevska, M. Werling, and J. Boedecker, “Dynamic
    input for deep reinforcement learning in autonomous driving,” in *IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2019, pp. 7566–7573.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] M. Huegle, G. Kalweit, B. Mirchevska, M. Werling 和 J. Boedecker，“用于自主驾驶的深度强化学习的动态输入，”在
    *IEEE/RSJ 国际智能机器人与系统大会 (IROS)* 上，2019 年，页码 7566–7573。'
- en: '[142] C. Desjardins and B. Chaib-Draa, “Cooperative adaptive cruise control:
    A reinforcement learning approach,” *IEEE Transactions on intelligent transportation
    systems*, vol. 12, no. 4, pp. 1248–1260, 2011.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] C. Desjardins 和 B. Chaib-Draa，“协作自适应巡航控制：一种强化学习方法，” *IEEE 智能交通系统汇刊*，第
    12 卷，第 4 期，页码 1248–1260，2011 年。'
- en: '[143] D. Zhao, B. Wang, and D. Liu, “A supervised actor–critic approach for
    adaptive cruise control,” *Soft Computing*, vol. 17, no. 11, pp. 2089–2099, 2013.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] D. Zhao, B. Wang 和 D. Liu，“用于自适应巡航控制的监督演员–评论家方法，” *软计算*，第 17 卷，第 11 期，页码
    2089–2099，2013 年。'
- en: '[144] D. Zhao, Z. Xia, and Q. Zhang, “Model-free optimal control based intelligent
    cruise control with hardware-in-the-loop demonstration [research frontier],” *IEEE
    Computational Intelligence Magazine*, vol. 12, no. 2, pp. 56–69, 2017.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] D. Zhao, Z. Xia 和 Q. Zhang，“基于模型无关最优控制的智能巡航控制与硬件在环演示 [研究前沿]，” *IEEE 计算智能杂志*，第
    12 卷，第 2 期，页码 56–69，2017 年。'
- en: '[145] K. Min, H. Kim, and K. Huh, “Deep q learning based high level driving
    policy determination,” in *Intelligent Vehicles Symposium*.   IEEE, 2018, pp.
    226–231.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] K. Min, H. Kim 和 K. Huh，“基于深度 Q 学习的高层驾驶策略确定，”在 *智能车辆研讨会* 上。 IEEE，2018
    年，页码 226–231。'
- en: '[146] A. Kuefler, J. Morton, T. Wheeler, and M. Kochenderfer, “Imitating driver
    behavior with generative adversarial networks,” in *Intelligent Vehicles Symposium*.   IEEE,
    2017, pp. 204–211.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] A. Kuefler, J. Morton, T. Wheeler 和 M. Kochenderfer，“用生成对抗网络模仿驾驶员行为，”在
    *智能车辆研讨会* 上。 IEEE，2017 年，页码 204–211。'
- en: '[147] R. P. Bhattacharyya, D. J. Phillips, B. Wulfe, J. Morton, A. Kuefler,
    and M. J. Kochenderfer, “Multi-agent imitation learning for driving simulation,”
    in *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2018, pp. 1534–1539.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] R. P. Bhattacharyya, D. J. Phillips, B. Wulfe, J. Morton, A. Kuefler
    和 M. J. Kochenderfer，“用于驾驶模拟的多智能体模仿学习，”在 *IEEE/RSJ 国际智能机器人与系统大会 (IROS)* 上。 IEEE，2018
    年，页码 1534–1539。'
- en: '[148] M. Kuderer, S. Gulati, and W. Burgard, “Learning driving styles for autonomous
    vehicles from demonstration,” in *International Conference on Robotics and Automation*.   IEEE,
    2015, pp. 2641–2646.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] M. Kuderer, S. Gulati 和 W. Burgard，“从示范中学习自主车辆的驾驶风格，”在 *国际机器人与自动化会议*
    上。 IEEE，2015 年，页码 2641–2646。'
- en: '[149] R. P. Bhattacharyya, D. J. Phillips, C. Liu, J. K. Gupta, K. Driggs-Campbell,
    and M. J. Kochenderfer, “Simulating emergent properties of human driving behavior
    using multi-agent reward augmented imitation learning,” in *International Conference
    on Robotics and Automation*.   IEEE, 2019, pp. 789–795.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] R. P. Bhattacharyya, D. J. Phillips, C. Liu, J. K. Gupta, K. Driggs-Campbell
    和 M. J. Kochenderfer，“使用多智能体奖励增强模仿学习模拟人类驾驶行为的突现特性，”在 *国际机器人与自动化会议* 上。 IEEE，2019
    年，页码 789–795。'
- en: '[150] Z. Huang, X. Xu, H. He, J. Tan, and Z. Sun, “Parameterized batch reinforcement
    learning for longitudinal control of autonomous land vehicles,” *IEEE Transactions
    on Systems, Man, and Cybernetics: Systems*, vol. 49, no. 4, pp. 730–741, 2017.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Z. Huang, X. Xu, H. He, J. Tan 和 Z. Sun，“用于自主陆地车辆纵向控制的参数化批量强化学习，” *IEEE
    系统、人类与控制论汇刊：系统*，第 49 卷，第 4 期，页码 730–741，2017 年。'
- en: '[151] M. J. Hausknecht and P. Stone, “Deep reinforcement learning in parameterized
    action space,” in *International Conference on Learning Representations*, 2016.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] M. J. Hausknecht 和 P. Stone，“参数化动作空间中的深度强化学习，” 见于 *国际学习表征会议*，2016年。'
- en: '[152] M. Everett, Y. F. Chen, and J. P. How, “Motion planning among dynamic,
    decision-making agents with deep reinforcement learning,” in *IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*.   IEEE, 2018, pp. 3052–3059.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] M. Everett, Y. F. Chen, 和 J. P. How，“在动态决策代理之间进行运动规划，利用深度强化学习，” 见于 *IEEE/RSJ
    国际智能机器人与系统会议（IROS）*。   IEEE，2018年，第3052–3059页。'
- en: '[153] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Póczos, R. Salakhutdinov, and
    A. J. Smola, “Deep sets,” in *Advances in Neural Information Processing Systems
    30*, 2017.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Póczos, R. Salakhutdinov, 和
    A. J. Smola，“深度集，” 见于 *神经信息处理系统进展 30*，2017年。'
- en: '[154] D. Hayashi, Y. Xu, T. Bando, and K. Takeda, “A predictive reward function
    for human-like driving based on a transition model of surrounding environment,”
    in *International Conference on Robotics and Automation*.   IEEE, 2019, pp. 7618–7624.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] D. Hayashi, Y. Xu, T. Bando, 和 K. Takeda，“基于周围环境转移模型的人类驾驶预测奖励函数，” 见于
    *国际机器人与自动化会议*。   IEEE，2019年，第7618–7624页。'
- en: '[155] S. Qi and S.-C. Zhu, “Intent-aware multi-agent reinforcement learning,”
    in *International Conference on Robotics and Automation*.   IEEE, 2018, pp. 7533–7540.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] S. Qi 和 S.-C. Zhu，“意图感知的多智能体强化学习，” 见于 *国际机器人与自动化会议*。   IEEE，2018年，第7533–7540页。'
- en: '[156] C. Chen, Y. Liu, S. Kreiss, and A. Alahi, “Crowd-robot interaction: Crowd-aware
    robot navigation with attention-based deep reinforcement learning,” in *International
    Conference on Robotics and Automation*.   IEEE, 2019, pp. 6015–6022.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] C. Chen, Y. Liu, S. Kreiss, 和 A. Alahi，“人群机器人交互：基于注意力的深度强化学习的拥挤感知机器人导航，”
    见于 *国际机器人与自动化会议*。   IEEE，2019年，第6015–6022页。'
- en: '[157] Y. Hu, A. Nakhaei, M. Tomizuka, and K. Fujimura, “Interaction-aware decision
    making with adaptive strategies under merging scenarios,” *arXiv preprint arXiv:1904.06025*,
    2019.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Y. Hu, A. Nakhaei, M. Tomizuka, 和 K. Fujimura，“在合并场景下具有自适应策略的交互感知决策制定，”
    *arXiv 预印本 arXiv:1904.06025*，2019年。'
- en: '[158] J. García, Fern, and o Fernández, “A comprehensive survey on safe reinforcement
    learning,” *Journal of Machine Learning Research*, vol. 16, no. 42, pp. 1437–1480,
    2015.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] J. García, Fern, 和 o Fernández，“关于安全强化学习的综合调查，” *机器学习研究期刊*，第16卷，第42期，第1437–1480页，2015年。'
- en: '[159] N. Jansen, B. Könighofer, S. Junges, and R. Bloem, “Shielded decision-making
    in mdps,” *CoRR*, vol. abs/1807.06096, 2018.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] N. Jansen, B. Könighofer, S. Junges, 和 R. Bloem，“在MDPs中的屏蔽决策制定，” *CoRR*，第abs/1807.06096卷，2018年。'
- en: '[160] N. Fulton and A. Platzer, “Safe reinforcement learning via formal methods:
    Toward safe control through proof and learning,” in *the Thirty-Second AAAI Conference
    on Artificial Intelligence*, 2018, pp. 6485–6492.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] N. Fulton 和 A. Platzer，“通过形式化方法实现安全强化学习：通过证明和学习实现安全控制，” 见于 *第三十二届 AAAI
    人工智能会议*，2018年，第6485–6492页。'
- en: '[161] M. Bouton, J. Karlsson, A. Nakhaei, K. Fujimura, M. J. Kochenderfer,
    and J. Tumova, “Reinforcement learning with probabilistic guarantees for autonomous
    driving,” *arXiv preprint arXiv:1904.07189*, 2019.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] M. Bouton, J. Karlsson, A. Nakhaei, K. Fujimura, M. J. Kochenderfer,
    和 J. Tumova，“具有概率保障的强化学习用于自动驾驶，” *arXiv 预印本 arXiv:1904.07189*，2019年。'
- en: '[162] D. Isele, A. Nakhaei, and K. Fujimura, “Safe reinforcement learning on
    autonomous vehicles,” in *IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS)*.   IEEE, 2018, pp. 1–6.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] D. Isele, A. Nakhaei, 和 K. Fujimura，“在自动驾驶车辆上的安全强化学习，” 见于 *IEEE/RSJ 国际智能机器人与系统会议（IROS）*。   IEEE，2018年，第1–6页。'
- en: '[163] M. Mukadam, A. Cosgun, A. Nakhaei, and K. Fujimura, “Tactical decision
    making for lane changing with deep reinforcement learning,” 2017.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] M. Mukadam, A. Cosgun, A. Nakhaei, 和 K. Fujimura，“基于深度强化学习的车道变换战术决策，”
    2017年。'
- en: '[164] X. Xiong, J. Wang, F. Zhang, and K. Li, “Combining deep reinforcement
    learning and safety based control for autonomous driving,” *arXiv preprint arXiv:1612.00147*,
    2016.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] X. Xiong, J. Wang, F. Zhang, 和 K. Li，“结合深度强化学习和基于安全的控制用于自动驾驶，” *arXiv
    预印本 arXiv:1612.00147*，2016年。'
- en: '[165] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement
    learning for autonomous driving,” *CoRR*, vol. abs/1610.03295, 2016.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] S. Shalev-Shwartz, S. Shammah, 和 A. Shashua，“安全的多智能体强化学习用于自动驾驶，” *CoRR*，第abs/1610.03295卷，2016年。'
- en: '[166] F. Pusse and M. Klusch, “Hybrid online pomdp planning and deep reinforcement
    learning for safer self-driving cars,” in *Intelligent Vehicles Symposium*.   IEEE,
    2019, pp. 1013–1020.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] F. Pusse 和 M. Klusch，“混合在线POMDP规划和深度强化学习用于更安全的自动驾驶汽车，”在 *智能车辆研讨会* 中。IEEE，2019年，页码1013–1020。'
- en: '[167] J. Jiang, C. Dun, T. Huang, and Z. Lu, “Graph convolutional reinforcement
    learning,” in *International Conference on Learning Representations*, 2020.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. Jiang, C. Dun, T. Huang, 和 Z. Lu，“图卷积强化学习，”在 *国际学习表征会议* 中，2020年。'
- en: '[168] A. Mohseni-Kabir, D. Isele, and K. Fujimura, “Interaction-aware multi-agent
    reinforcement learning for mobile agents with individual goals,” in *International
    Conference on Robotics and Automation*.   IEEE, 2019, pp. 3370–3376.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] A. Mohseni-Kabir, D. Isele, 和 K. Fujimura，“面向互动的多智能体强化学习，用于具有个体目标的移动代理，”在
    *国际机器人与自动化会议* 中。IEEE，2019年，页码3370–3376。'
- en: '[169] J. F. Fisac, E. Bronstein, E. Stefansson, D. Sadigh, S. S. Sastry, and
    A. D. Dragan, “Hierarchical game-theoretic planning for autonomous vehicles,”
    in *International Conference on Robotics and Automation*.   IEEE, 2019, pp. 9590–9596.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] J. F. Fisac, E. Bronstein, E. Stefansson, D. Sadigh, S. S. Sastry, 和
    A. D. Dragan，“用于自动驾驶车辆的分层博弈论规划，”在 *国际机器人与自动化会议* 中。IEEE，2019年，页码9590–9596。'
- en: '[170] N. Li, D. W. Oyler, M. Zhang, Y. Yildiz, I. Kolmanovsky, and A. R. Girard,
    “Game theoretic modeling of driver and vehicle interactions for verification and
    validation of autonomous vehicle control systems,” *IEEE Transactions on control
    systems technology*, vol. 26, no. 5, pp. 1782–1797, 2017.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] N. Li, D. W. Oyler, M. Zhang, Y. Yildiz, I. Kolmanovsky, 和 A. R. Girard，“用于验证和验证自动驾驶车辆控制系统的驾驶员和车辆交互的博弈论建模，”
    *IEEE控制系统技术学报*，第26卷，第5期，页码1782–1797，2017年。'
- en: '[171] G. Ding, S. Aghli, C. Heckman, and L. Chen, “Game-theoretic cooperative
    lane changing using data-driven models,” in *IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*.   IEEE, 2018, pp. 3640–3647.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] G. Ding, S. Aghli, C. Heckman, 和 L. Chen，“基于数据驱动模型的博弈论合作车道变更，”在 *IEEE/RSJ国际智能机器人与系统会议
    (IROS)* 中。IEEE，2018年，页码3640–3647。'
- en: '[172] M. Huegle, G. Kalweit, M. Werling, and J. Boedecker, “Dynamic interaction-aware
    scene understanding for reinforcement learning in autonomous driving,” *arXiv
    preprint arXiv:1909.13582*, 2019.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] M. Huegle, G. Kalweit, M. Werling, 和 J. Boedecker，“用于自动驾驶中强化学习的动态互动感知场景理解，”
    *arXiv预印本 arXiv:1909.13582*，2019年。'
- en: '[173] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *International Conference on Learning Representations*,
    2017.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] T. N. Kipf 和 M. Welling，“基于图卷积网络的半监督分类，”在 *国际学习表征会议* 中，2017年。'
- en: '[174] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, F. Li, and S. Savarese,
    “Social LSTM: human trajectory prediction in crowded spaces,” in *IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 961–971.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, F. Li, 和 S. Savarese，“Social
    LSTM: 拥挤空间中的人类轨迹预测，”在 *IEEE计算机视觉与模式识别会议* 中，2016年，页码961–971。'
- en: '[175] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, “Social
    GAN: socially acceptable trajectories with generative adversarial networks,” in
    *IEEE Conference on Computer Vision and Pattern Recognition*, 2018, pp. 2255–2264.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, 和 A. Alahi，“Social GAN:
    使用生成对抗网络生成社会可接受的轨迹，”在 *IEEE计算机视觉与模式识别会议* 中，2018年，页码2255–2264。'
- en: '[176] A. Vemula, K. Muelling, and J. Oh, “Social attention: Modeling attention
    in human crowds,” in *International Conference on Robotics and Automation*.   IEEE,
    2018, pp. 1–7.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] A. Vemula, K. Muelling, 和 J. Oh，“社会关注：人群中的注意力建模，”在 *国际机器人与自动化会议* 中。IEEE，2018年，页码1–7。'
- en: '[177] Y. Hoshen, “VAIN: attentional multi-agent predictive modeling,” in *Advances
    in Neural Information Processing Systems*, 2017, pp. 2701–2711.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Y. Hoshen，“VAIN: 注意力多智能体预测建模，”在 *神经信息处理系统进展* 中，2017年，页码2701–2711。'
- en: '[178] M. Henaff, A. Canziani, and Y. LeCun, “Model-predictive policy learning
    with uncertainty regularization for driving in dense traffic,” in *International
    Conference on Learning Representations*, 2019.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] M. Henaff, A. Canziani, 和 Y. LeCun，“具有不确定性正则化的模型预测策略学习用于密集交通中的驾驶，”在 *国际学习表征会议*
    中，2019年。'
- en: '[179] C. Guestrin, M. G. Lagoudakis, and R. Parr, “Coordinated reinforcement
    learning,” in *International Conference on Machine Learning*, 2002, pp. 227–234.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] C. Guestrin, M. G. Lagoudakis, 和 R. Parr，“协调强化学习，”在 *国际机器学习会议* 中，2002年，页码227–234。'
- en: '[180] C. Yu, X. Wang, X. Xu, M. Zhang, H. Ge, J. Ren, L. Sun, B. Chen, and
    G. Tan, “Distributed multiagent coordinated learning for autonomous driving in
    highways based on dynamic coordination graphs,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 21, no. 2, pp. 735–748, 2019.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] C. Yu, X. Wang, X. Xu, M. Zhang, H. Ge, J. Ren, L. Sun, B. Chen 和 G.
    Tan, “基于动态协调图的高速公路自主驾驶的分布式多智能体协调学习，” *IEEE智能交通系统汇刊*，第21卷，第2期，页码735–748，2019年。'
- en: '[181] K. Leyton-Brown and Y. Shoham, *Essentials of Game Theory: A Concise
    Multidisciplinary Introduction*, ser. Synthesis Lectures on Artificial Intelligence
    and Machine Learning.   Morgan & Claypool Publishers, 2008.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] K. Leyton-Brown 和 Y. Shoham, *博弈论要点：简明的多学科介绍*，系列：人工智能与机器学习的合成讲座。摩根与克莱普尔出版社，2008年。'
- en: '[182] G. P. Papavassilopoulos and M. G. Safonov, “Robust control design via
    game theoretic methods,” in *IEEE Conference on Decision and Control*, 1989, pp.
    382–387 vol.1.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] G. P. Papavassilopoulos 和 M. G. Safonov, “通过博弈论方法进行鲁棒控制设计，” 发表在 *IEEE决策与控制大会*，1989年，页码382–387，第1卷。'
- en: '[183] U. Branch, S. Ganebnyi, S. Kumkov, V. Patsko, and S. Pyatko, “Robust
    control in game problems with linear dynamics,” *International Journal of Mathematics,
    Game Theory and Algebra*, vol. 3, 01 2007.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] U. Branch, S. Ganebnyi, S. Kumkov, V. Patsko 和 S. Pyatko, “具有线性动态的博弈问题中的鲁棒控制，”
    *国际数学、博弈论与代数杂志*，第3卷，2007年1月。'
- en: '[184] Hong Zhang, V. Kumar, and J. Ostrowski, “Motion planning with uncertainty,”
    in *International Conference on Robotics and Automation*, vol. 1, 1998, pp. 638–643
    vol.1.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Hong Zhang, V. Kumar 和 J. Ostrowski, “具有不确定性的运动规划，” 发表在 *国际机器人与自动化大会*，第1卷，1998年，页码638–643，第1卷。'
- en: '[185] M. Zhu, M. Otte, P. Chaudhari, and E. Frazzoli, “Game theoretic controller
    synthesis for multi-robot motion planning part i: Trajectory based algorithms,”
    in *International Conference on Robotics and Automation*, 2014, pp. 1646–1651.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] M. Zhu, M. Otte, P. Chaudhari 和 E. Frazzoli, “多机器人运动规划的博弈论控制器合成第I部分：基于轨迹的算法，”
    发表在 *国际机器人与自动化大会*，2014年，页码1646–1651。'
- en: '[186] P. Wilson and D. Stahl, “On players’ models of other players: Theory
    and experimental evidence,” *Games and Economic Behavior*, vol. 10, pp. 218–254,
    07 1995.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] P. Wilson 和 D. Stahl, “关于玩家对其他玩家的模型：理论与实证证据，” *游戏与经济行为*，第10卷，页码218–254，1995年7月。'
- en: '[187] L. Sun, W. Zhan, and M. Tomizuka, “Probabilistic prediction of interactive
    driving behavior via hierarchical inverse reinforcement learning,” in *International
    Conference on Intelligent Transportation Systems*.   IEEE, 2018, pp. 2111–2117.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] L. Sun, W. Zhan 和 M. Tomizuka, “通过层次化逆强化学习对互动驾驶行为进行概率预测，” 发表在 *国际智能交通系统大会*。IEEE，2018年，页码2111–2117。'
- en: '[188] P. Wang, Y. Li, S. Shekhar, and W. F. Northrop, “Uncertainty estimation
    with distributional reinforcement learning for applications in intelligent transportation
    systems: A case study,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE,
    2019, pp. 3822–3827.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] P. Wang, Y. Li, S. Shekhar 和 W. F. Northrop, “利用分布式强化学习进行不确定性估计在智能交通系统中的应用：案例研究，”
    发表在 *IEEE智能交通系统大会*。IEEE，2019年，页码3822–3827。'
- en: '[189] J. Bernhard, S. Pollok, and A. Knoll, “Addressing inherent uncertainty:
    Risk-sensitive behavior generation for automated driving using distributional
    reinforcement learning,” in *sium*.   IEEE, 2019, pp. 2148–2155.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] J. Bernhard, S. Pollok 和 A. Knoll, “解决固有的不确定性：使用分布式强化学习生成自动驾驶的风险敏感行为，”
    发表在 *sium*。IEEE，2019年，页码2148–2155。'
- en: '[190] G. Kahn, A. Villaflor, V. Pong, P. Abbeel, and S. Levine, “Uncertainty-aware
    reinforcement learning for collision avoidance,” *arXiv preprint arXiv:1702.01182*,
    2017.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] G. Kahn, A. Villaflor, V. Pong, P. Abbeel 和 S. Levine, “用于碰撞避免的不确定性感知强化学习，”
    *arXiv 预印本 arXiv:1702.01182*，2017年。'
- en: '[191] S. Choi, K. Lee, S. Lim, and S. Oh, “Uncertainty-aware learning from
    demonstration using mixture density networks with sampling-free variance modeling,”
    in *International Conference on Robotics and Automation*.   IEEE, 2018, pp. 6915–6922.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] S. Choi, K. Lee, S. Lim 和 S. Oh, “使用无采样方差建模的混合密度网络进行的从示例学习的不确定性感知，” 发表在
    *国际机器人与自动化大会*。IEEE，2018年，页码6915–6922。'
- en: '[192] K. Lee, K. Saigol, and E. A. Theodorou, “Safe end-to-end imitation learning
    for model predictive control,” *arXiv preprint arXiv:1803.10231*, 2018.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] K. Lee, K. Saigol 和 E. A. Theodorou, “用于模型预测控制的安全端到端模仿学习，” *arXiv 预印本
    arXiv:1803.10231*，2018年。'
- en: '[193] Y. Gal, “Uncertainty in deep learning,” *University of Cambridge*, vol. 1,
    no. 3, 2016.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Y. Gal, “深度学习中的不确定性，” *剑桥大学*，第1卷，第3期，2016年。'
- en: '[194] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian deep
    learning for computer vision?” in *Advances in neural information processing systems*,
    2017, pp. 5574–5584.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] A. Kendall 和 Y. Gal，“在计算机视觉的贝叶斯深度学习中，我们需要什么不确定性？”发表于*神经信息处理系统进展*，2017年，第5574–5584页。'
- en: '[195] Y. Gal and Z. Ghahramani, “Bayesian convolutional neural networks with
    bernoulli approximate variational inference,” *CoRR*, vol. abs/1506.02158, 2015.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Y. Gal 和 Z. Ghahramani，“贝叶斯卷积神经网络与伯努利近似变分推断，”*CoRR*，第abs/1506.02158卷，2015年。'
- en: '[196] Y. Gal, R. Islam, and Z. Ghahramani, “Deep bayesian active learning with
    image data,” in *International Conference on Machine Learning*, 2017, pp. 1183–1192.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Y. Gal, R. Islam, 和 Z. Ghahramani，“带图像数据的深度贝叶斯主动学习，”发表于*国际机器学习会议*，2017年，第1183–1192页。'
- en: '[197] T. G. Dietterich, “Ensemble methods in machine learning,” in *Multiple
    Classifier Systems, First International Workshop, MCS 2000, Cagliari, Italy, June
    21-23, 2000, Proceedings*, ser. Lecture Notes in Computer Science, vol. 1857.   Springer,
    2000, pp. 1–15.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] T. G. Dietterich，“机器学习中的集成方法，”发表于*多分类器系统，第一次国际研讨会，MCS 2000，意大利卡利亚里，2000年6月21-23日，会议录*，Lecture
    Notes in Computer Science系列，第1857卷。Springer，2000年，第1–15页。'
- en: '[198] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable
    predictive uncertainty estimation using deep ensembles,” in *Advances in Neural
    Information Processing Systems 30*, 2017.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] B. Lakshminarayanan, A. Pritzel, 和 C. Blundell，“使用深度集成进行简单而可扩展的预测不确定性估计，”发表于*神经信息处理系统进展
    30*，2017年。'
- en: '[199] B. Efron and R. Tibshirani, *An Introduction to the Bootstrap*.   Springer,
    1993.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] B. Efron 和 R. Tibshirani，*引导法介绍*。Springer，1993年。'
- en: '[200] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective
    on reinforcement learning,” in *International Conference on Machine Learning*,
    2017, pp. 449–458.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] M. G. Bellemare, W. Dabney, 和 R. Munos，“从分布角度看强化学习，”发表于*国际机器学习会议*，2017年，第449–458页。'
- en: '[201] W. Dabney, G. Ostrovski, D. Silver, and R. Munos, “Implicit quantile
    networks for distributional reinforcement learning,” in *International Conference
    on Machine Learning*, 2018, pp. 1104–1113.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] W. Dabney, G. Ostrovski, D. Silver, 和 R. Munos，“用于分布式强化学习的隐式分位数网络，”发表于*国际机器学习会议*，2018年，第1104–1113页。'
- en: '[202] Y. Li, J. Song, and S. Ermon, “Infogail: Interpretable imitation learning
    from visual demonstrations,” in *Advances in Neural Information Processing Systems*,
    2017, pp. 3812–3822.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Y. Li, J. Song, 和 S. Ermon，“Infogail：从视觉演示中可解释的模仿学习，”发表于*神经信息处理系统进展*，2017年，第3812–3822页。'
- en: '[203] A. Kuefler and M. J. Kochenderfer, “Burn-in demonstrations for multi-modal
    imitation learning,” *arXiv preprint arXiv:1710.05090*, 2017.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] A. Kuefler 和 M. J. Kochenderfer，“多模态模仿学习的烧入演示，”*arXiv 预印本 arXiv:1710.05090*，2017年。'
- en: '[204] Z. Wang, J. S. Merel, S. E. Reed, N. de Freitas, G. Wayne, and N. Heess,
    “Robust imitation of diverse behaviors,” in *Advances in Neural Information Processing
    Systems*, 2017, pp. 5320–5329.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Z. Wang, J. S. Merel, S. E. Reed, N. de Freitas, G. Wayne, 和 N. Heess，“鲁棒的多样行为模仿，”发表于*神经信息处理系统进展*，2017年，第5320–5329页。'
- en: '[205] J. Merel, Y. Tassa, D. TB, S. Srinivasan, J. Lemmon, Z. Wang, G. Wayne,
    and N. Heess, “Learning human behaviors from motion capture by adversarial imitation,”
    *arXiv preprint arXiv:1707.02201*, 2017.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] J. Merel, Y. Tassa, D. TB, S. Srinivasan, J. Lemmon, Z. Wang, G. Wayne,
    和 N. Heess，“通过对抗模仿从运动捕捉中学习人类行为，”*arXiv 预印本 arXiv:1707.02201*，2017年。'
- en: '[206] J. Lin and Z. Zhang, “Acgail: Imitation learning about multiple intentions
    with auxiliary classifier gans,” in *Pacific Rim International Conference on Artificial
    Intelligence*.   Springer, 2018, pp. 321–334.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] J. Lin 和 Z. Zhang，“Acgail：关于多重意图的模仿学习，采用辅助分类器GANs，”发表于*太平洋环国际人工智能会议*。Springer，2018年，第321–334页。'
- en: '[207] C. Fei, B. Wang, Y. Zhuang, Z. Zhang, J. Hao, H. Zhang, X. Ji, and W. Liu,
    “Triple-gail: A multi-modal imitation learning framework with generative adversarial
    nets,” in *the Twenty-Ninth International Joint Conference on Artificial Intelligence
    (IJCAI)*, 2020, pp. 2929–2935.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] C. Fei, B. Wang, Y. Zhuang, Z. Zhang, J. Hao, H. Zhang, X. Ji, 和 W. Liu，“Triple-gail：一个多模态模仿学习框架，采用生成对抗网络，”发表于*第二十九届国际人工智能联合会议
    (IJCAI)*，2020年，第2929–2935页。'
- en: '[208] E. Schmerling, K. Leung, W. Vollprecht, and M. Pavone, “Multimodal probabilistic
    model-based planning for human-robot interaction,” in *International Conference
    on Robotics and Automation*.   IEEE, 2018, pp. 1–9.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] E. Schmerling, K. Leung, W. Vollprecht, 和 M. Pavone，“用于人机交互的多模态概率模型规划，”发表于*国际机器人与自动化会议*。IEEE，2018年，第1–9页。'
- en: '[209] J. Li, H. Ma, and M. Tomizuka, “Interaction-aware multi-agent tracking
    and probabilistic behavior prediction via adversarial learning,” in *International
    Conference on Robotics and Automation*.   IEEE, 2019, pp. 6658–6664.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] J. Li, H. Ma, 和 M. Tomizuka, “面向互动的多智能体跟踪与对抗学习的概率行为预测，” 发表在*国际机器人与自动化会议*。IEEE，2019年，第6658–6664页。'
- en: '[210] H. Ma, J. Li, W. Zhan, and M. Tomizuka, “Wasserstein generative learning
    with kinematic constraints for probabilistic interactive driving behavior prediction,”
    in *Intelligent Vehicles Symposium*.   IEEE, 2019, pp. 2477–2483.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] H. Ma, J. Li, W. Zhan, 和 M. Tomizuka, “带有运动约束的Wasserstein生成学习用于概率互动驾驶行为预测，”
    发表在*智能车辆研讨会*。IEEE，2019年，第2477–2483页。'
- en: '[211] A. Turnwald, D. Althoff, D. Wollherr, and M. Buss, “Understanding human
    avoidance behavior: interaction-aware decision making based on game theory,” *International
    Journal of Social Robotics*, vol. 8, no. 2, pp. 331–351, 2016.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] A. Turnwald, D. Althoff, D. Wollherr, 和 M. Buss, “理解人类回避行为：基于博弈理论的互动感知决策制定，”
    *国际社会机器人学杂志*，第8卷，第2期，第331–351页，2016年。'
- en: '[212] R. Tian, S. Li, N. Li, I. Kolmanovsky, A. Girard, and Y. Yildiz, “Adaptive
    game-theoretic decision making for autonomous vehicle control at roundabouts,”
    in *IEEE Conference on Decision and Control (CDC)*.   IEEE, 2018, pp. 321–326.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] R. Tian, S. Li, N. Li, I. Kolmanovsky, A. Girard, 和 Y. Yildiz, “环形交叉口的自主车辆控制的自适应博弈理论决策制定，”
    发表在*IEEE决策与控制会议（CDC）*。IEEE，2018年，第321–326页。'
- en: '[213] D. Isele, “Interactive decision making for autonomous vehicles in dense
    traffic,” in *IEEE Intelligent Transportation Systems Conference*.   IEEE, 2019,
    pp. 3981–3986.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] D. Isele, “密集交通中自主车辆的互动决策制定，” 发表在*IEEE智能交通系统会议*。IEEE，2019年，第3981–3986页。'
- en: '[214] H. Bai, S. Cai, N. Ye, D. Hsu, and W. S. Lee, “Intention-aware online
    pomdp planning for autonomous driving in a crowd,” in *International Conference
    on Robotics and Automation*.   IEEE, 2015, pp. 454–460.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] H. Bai, S. Cai, N. Ye, D. Hsu, 和 W. S. Lee, “面向意图的在线pomdp规划用于人群中的自主驾驶，”
    发表在*国际机器人与自动化会议*。IEEE，2015年，第454–460页。'
- en: '[215] C. Hubmann, J. Schulz, G. Xu, D. Althoff, and C. Stiller, “A belief state
    planner for interactive merge maneuvers in congested traffic,” in *International
    Conference on Intelligent Transportation Systems*.   IEEE, 2018, pp. 1617–1624.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] C. Hubmann, J. Schulz, G. Xu, D. Althoff, 和 C. Stiller, “拥堵交通中的互动合并操作的信念状态规划器，”
    发表在*国际智能交通系统会议*。IEEE，2018年，第1617–1624页。'
- en: '[216] C. A. Holloway, *Decision making under uncertainty: models and choices*.   Prentice-Hall
    Englewood Cliffs, NJ, 1979, vol. 8.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] C. A. Holloway, *不确定性下的决策：模型与选择*。Prentice-Hall Englewood Cliffs, NJ，1979年，第8卷。'
- en: '[217] M. J. Kochenderfer, *Decision making under uncertainty: theory and application*.   MIT
    press, 2015.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] M. J. Kochenderfer, *不确定性下的决策：理论与应用*。MIT出版社，2015年。'
- en: '[218] R. Camacho and D. Michie, “Behavioral cloning A correction,” *AI Mag.*,
    vol. 16, no. 2, p. 92, 1995.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] R. Camacho 和 D. Michie, “行为克隆的修正，” *AI Mag.*，第16卷，第2期，第92页，1995年。'
- en: '| ![[Uncaptioned image]](img/9de2ba3603745830a3f7a5daffbb2ce2.png) | Zeyu Zhu
    received B.S. degree in computer science from Peking University, Beijing, China,
    in 2019, where he is currently pursuing the Ph.D. degree with the Key Laboratory
    of Machine Perception (MOE), Peking University. His research interests include
    intelligent vehicles, reinforcement learning, and machine learning. |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/9de2ba3603745830a3f7a5daffbb2ce2.png) | Zeyu Zhu于2019年获得北京大学计算机科学学士学位，目前在北京大学机器感知（MOE）重点实验室攻读博士学位。他的研究兴趣包括智能车辆、强化学习和机器学习。
    |'
- en: '| ![[Uncaptioned image]](img/ef69948710785d2219365e9917766d29.png) | Huijing
    Zhao received B.S. degree in computer science from Peking University in 1991\.
    She obtained M.E. degree in 1996 and Ph.D. degree in 1999 in civil engineering
    from the University of Tokyo, Japan. From 1999 to 2007, she was a postdoctoral
    researcher and visiting associate professor at the Center for Space Information
    Science, University of Tokyo. In 2007, she joined Peking University as a tenure-track
    professor at the School of Electronics Engineering and Computer Science. She became
    an associate professor with tenure on 2013 and was promoted to full professor
    on 2020\. She has research interest in several areas in connection with intelligent
    vehicle and mobile robot, such as machine perception, behavior learning and motion
    planning, and she has special interests on the studies through real world data
    collection. |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/ef69948710785d2219365e9917766d29.png) | 赵慧晶于1991年获得北京大学计算机科学学士学位。她于1996年获得东京大学土木工程硕士学位，并于1999年获得博士学位。从1999年到2007年，她在东京大学空间信息科学中心担任博士后研究员和访问副教授。2007年，她加入北京大学，担任电子工程与计算机科学学院的终身教职教授。她于2013年晋升为终身副教授，并于2020年晋升为正教授。她的研究兴趣涉及智能车辆和移动机器人多个领域，如机器感知、行为学习和运动规划，她对通过真实世界数据收集的研究有特别兴趣。'
