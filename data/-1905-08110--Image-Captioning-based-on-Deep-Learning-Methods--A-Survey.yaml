- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:06:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 20:06:16'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1905.08110] Image Captioning based on Deep Learning Methods: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1905.08110] 基于深度学习方法的图像描述：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1905.08110](https://ar5iv.labs.arxiv.org/html/1905.08110)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1905.08110](https://ar5iv.labs.arxiv.org/html/1905.08110)
- en: 'Image Captioning based on Deep Learning Methods: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习方法的图像描述：综述
- en: Yiyu Wang¹¹1Y. Wang and J. Xu are corresponding authors    Jungang Xu^∗    Yingfei
    Sun &Ben He University of Chinese Academy of Sciences, Beijing
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yiyu Wang¹¹1Y. Wang 和 J. Xu 是通讯作者    Jungang Xu^∗    Yingfei Sun & Ben He 中国科学院大学，北京
- en: wangyiyu18@mails.ucas.ac.cn, {xujg,yfsun,benhe}@ucas.ac.cn
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: wangyiyu18@mails.ucas.ac.cn，{xujg,yfsun,benhe}@ucas.ac.cn
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Image captioning is a challenging task and attracting more and more attention
    in the field of Artificial Intelligence, and which can be applied to efficient
    image retrieval, intelligent blind guidance and human-computer interaction, etc.
    In this paper, we present a survey on advances in image captioning based on Deep
    Learning methods, including Encoder-Decoder structure, improved methods in Encoder,
    improved methods in Decoder, and other improvements. Furthermore, we discussed
    future research directions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述是一个具有挑战性的任务，并且在人工智能领域越来越受到关注，它可以应用于高效的图像检索、智能盲人导航和人机交互等。在本文中，我们对基于深度学习方法的图像描述的进展进行了综述，包括编码器-解码器结构、编码器中的改进方法、解码器中的改进方法以及其他改进。此外，我们讨论了未来的研究方向。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: There are a large number of unlabeled images in the network; it is impossible
    to label them manually. How to automatically generate natural language descriptions
    for images by computer is a challenging task in the field of artificial intelligence.
    Image captioning can be applied to efficient image retrieval, intelligent blind
    guidance and human-computer interaction, so it is also a task with practical value.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中有大量未标记的图像；人工标记这些图像是不可能的。如何通过计算机自动生成自然语言描述是人工智能领域的一个挑战性任务。图像描述可以应用于高效的图像检索、智能盲人导航和人机交互，因此它也是一个具有实际价值的任务。
- en: 'The goal of image captioning is to generate a trusted description for a given
    image. So, it is necessary to ensure the correctness of the objects, attribute
    information, semantic information, and position relationship information in the
    description. Therefore, we can decompose image captioning into two sub-tasks:
    (1) understanding the image, acquiring the relevant information correctly; (2)
    generating description based on the understanding of the image. Image captioning
    is a challenging task because it connects the two fields of Computer Vision(CV)
    and Natural Language Processing(NLP).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述的目标是为给定的图像生成一个可信的描述。因此，必须确保描述中对象、属性信息、语义信息和位置关系信息的正确性。因此，我们可以将图像描述分解为两个子任务：（1）理解图像，正确获取相关信息；（2）基于对图像的理解生成描述。图像描述是一个具有挑战性的任务，因为它连接了计算机视觉（CV）和自然语言处理（NLP）两个领域。
- en: In other words, image understanding is equivalent to feature extraction. In
    traditional methods, the bottom visual features (such as geometry, texture, colour,
    etc.) are extracted by using artificially designed feature operators, and then
    combined to form high-level global features. However, there are some drawbacks
    in these traditional methods. On one hand, the design of feature operator relies
    too much on luck and experience. On the other hand, the problem of ”semantic gap”
    leads to the inability of low-level visual features to accurately express semantic
    features. Therefore, traditional methods lack robustness and generalisation performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，图像理解相当于特征提取。在传统方法中，底层视觉特征（如几何、纹理、颜色等）是通过使用人工设计的特征操作符来提取的，然后组合成高级全局特征。然而，这些传统方法存在一些缺点。一方面，特征操作符的设计过于依赖运气和经验。另一方面，"语义差距"的问题导致低层次视觉特征无法准确表达语义特征。因此，传统方法缺乏鲁棒性和泛化性能。
- en: For a given image, the retrieval-based method selects sentence(s) from a specified
    image-description pool as the description(s) of the image; the template-based
    method detects a series of specified visual features from the image, and then
    fills them into the blank position of the given template. Images are very complex
    data. The description extracted by the retrieval-based method may not fully conform
    to the image. The image description generated by template-based method seems too
    rigid and lacks diversity.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的图像，基于检索的方法从指定的图像描述库中选择句子作为图像的描述；基于模板的方法则从图像中检测出一系列指定的视觉特征，然后将这些特征填入给定模板的空白位置。图像是非常复杂的数据。基于检索的方法提取的描述可能不完全符合图像。基于模板的方法生成的图像描述显得过于僵化，缺乏多样性。
- en: In recent years, Convolutional Neural Networks (CNN) have obtained outstanding
    effects in CV tasks, such as image classification, object detection. Recurrent
    Neural Networks (RNN) also played a significant role in NLP. In addition, inspired
    by Encoder-Decoder structure in machine translation Sutskever et al. ([2014](#bib.bib21)),
    Vinyals et al. ([2015](#bib.bib23)) uses GoogLeNet as Encoder to automatically
    extract image features, and then uses Long and Short-Term Memory network (LSTM)
    Hochreiter and Schmidhuber ([1997](#bib.bib9)) as Decoder to generate description,
    which is a pioneering work of image captioning using deep learning. Since then,
    Deep Learning methods based on Encoder-Decoder structure have become the basic
    framework of image captioning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，卷积神经网络（CNN）在计算机视觉（CV）任务中取得了卓越的效果，例如图像分类、物体检测。递归神经网络（RNN）在自然语言处理（NLP）中也发挥了重要作用。此外，受到机器翻译中编码器-解码器结构的启发，Sutskever
    等人（[2014](#bib.bib21)）和 Vinyals 等人（[2015](#bib.bib23)）使用 GoogLeNet 作为编码器自动提取图像特征，然后使用长短期记忆网络（LSTM）Hochreiter
    和 Schmidhuber（[1997](#bib.bib9)）作为解码器生成描述，这一工作开创了基于深度学习的图像描述技术。从那时起，基于编码器-解码器结构的深度学习方法已成为图像描述的基本框架。
- en: In the past few years, a large number of research works based on Deep Learning
    methods were published. Many useful improvements are proposed based on Encoder-Decoder
    structure, such as semantic attention You et al. ([2016](#bib.bib28)), visual
    sentinel Lu et al. ([2017](#bib.bib17)), and review network Yang et al. ([2016](#bib.bib26)).
    We divide them into (1) Improvements in Encoder (2) Improvements in Decoder and
    (3) Other Improvements.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，基于深度学习方法的大量研究工作被发表。许多基于编码器-解码器结构的有用改进被提出，例如语义注意力 You 等人（[2016](#bib.bib28)）、视觉哨兵
    Lu 等人（[2017](#bib.bib17)）和评审网络 Yang 等人（[2016](#bib.bib26)）。我们将这些改进分为（1）编码器的改进（2）解码器的改进和（3）其他改进。
- en: The main contributions of this paper include:(1) introduced and analyzed traditional
    methods such as Retrieval-Based and Template-Based methods; (2) provided an overview
    of Encoder-Decoder structure; (3)summarized improvements in Encoder and Decoder
    for image captioning; (4)discussed and proposed future research directions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献包括：（1）介绍并分析了传统方法，如基于检索和基于模板的方法；（2）提供了编码器-解码器结构的概述；（3）总结了图像描述中编码器和解码器的改进；（4）讨论并提出了未来的研究方向。
- en: The rest of this paper is organized as follows. Section 2 introduces the traditional
    image captioning methods. Section 3 focuses on the improvements in Encoder-Decoder.
    Section 4 and 5 introduce the existing standard Datasets and evaluation metrics.
    Section 6 discusses the future research directions. Section 7 gives the conclusions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。第2节介绍了传统的图像描述方法。第3节关注编码器-解码器的改进。第4节和第5节介绍了现有的标准数据集和评估指标。第6节讨论了未来的研究方向。第7节给出了结论。
- en: 2 Traditional Methods
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 传统方法
- en: This paper mainly focuses on deep learning methods. Hence, in this part, we
    only briefly review retrieval-based and template-based methods as traditional
    methods.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本文主要集中在深度学习方法上。因此，在这一部分，我们仅简要回顾作为传统方法的基于检索和基于模板的方法。
- en: '![Refer to caption](img/45033adeb54fc55c9b82bf134e835d3c.png)![Refer to caption](img/47ec25feb38face042b9eeaf51708e36.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/45033adeb54fc55c9b82bf134e835d3c.png)![参见说明文字](img/47ec25feb38face042b9eeaf51708e36.png)'
- en: 'Figure 1: The fundamental process of traditional methods: Reterival-Based Method
    (top). Template-Based Method (bottom).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：传统方法的基本流程：基于检索的方法（上）。基于模板的方法（下）。
- en: 2.1 Retrieval-Based Methods
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基于检索的方法
- en: For a given image, the retrieval-based image captioning methods aim to retrieve
    the matching sentence(s) from a set of given image-description pairs as the language
    description of the image, see Figure 1 (top). Therefore, the quality of this method
    depends on not only the diversity of image-description pairs but also the image-based
    retrieval algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的图像，基于检索的图像描述方法旨在从一组给定的图像-描述对中检索匹配的句子作为图像的语言描述，见图 1（顶部）。因此，这种方法的质量不仅取决于图像-描述对的多样性，还取决于基于图像的检索算法。
- en: Ordonez et al. ([2011](#bib.bib18)) firstly retrieves a series of related images
    from the image-description pairs by Gist and Tiny image descriptors, then detects
    and classifies the query images by specific objects and scenes, and reorders the
    retrieved images in turn, choosing the description of the first image to be ranked
    as the description of the query image. This method can be regarded as retrieval
    in visual space.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Ordonez 等人 ([2011](#bib.bib18)) 首先通过 Gist 和 Tiny 图像描述符从图像-描述对中检索一系列相关图像，然后通过特定对象和场景检测和分类查询图像，并依次重新排序检索到的图像，选择排名第一的图像的描述作为查询图像的描述。该方法可以视为在视觉空间中的检索。
- en: Hodosh et al. ([2015](#bib.bib10)) regards image captioning as a ranking task,
    and KCCA (A kernelized version of canonical correlation analysis) is employed
    to project images and descriptions into a common multimodal space. Then the query
    image is projected into the multimode space, and the Cosine similarity between
    the query image and the descriptions in datasets are calculated. The top rank
    is accepted as the description of the query image. However, KCCA is only suitable
    for small datasets, which can affect the performance of this method. This method
    can be regarded as retrieval in multimodal space.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Hodosh 等人 ([2015](#bib.bib10)) 将图像描述视为一个排序任务，并采用 KCCA（典型相关分析的核化版本）将图像和描述投影到一个共同的多模态空间中。然后，查询图像被投影到多模态空间中，并计算查询图像与数据集中描述之间的余弦相似度。排名靠前的描述被接受为查询图像的描述。然而，KCCA
    仅适用于小型数据集，这可能会影响该方法的性能。该方法可以视为在多模态空间中的检索。
- en: But, the shortcomings of the retrieval-based method are also explicit. The quality
    of the description generated by this method depends extensively on the given image-description
    pool. The image-description pairs are established artificially, so it is sufficient
    to ensure the fluency of the description sentence and the accuracy of the grammar;
    however, to ensure the accuracy of the description content and semantics, the
    pre-given image-description pairs need to be large sufficient to cover enough
    rich scenes. The limitation of this method may not suit the object and scene of
    new images correctly, so it also limits the generalisation performance of this
    method.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但基于检索的方法的缺点也很明显。该方法生成的描述质量在很大程度上依赖于给定的图像-描述池。图像-描述对是人工建立的，因此确保描述句子的流畅性和语法的准确性是足够的；然而，为了确保描述内容和语义的准确性，预先给定的图像-描述对需要足够大，以覆盖足够丰富的场景。这种方法的限制可能不适用于新图像的对象和场景，因此也限制了该方法的泛化性能。
- en: 2.2 Template-Based Methods
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基于模板的方法
- en: For a given image, the template-based image captioning method usually requires
    to extract some objects, attributes or semantic information from the image, and
    then uses a specified grammar rule to combine the information or fills the obtained
    data into the pre-defined blanks of the sentence template to form the image description,
    see Figure 1 (bottom).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的图像，基于模板的图像描述方法通常需要从图像中提取一些对象、属性或语义信息，然后使用指定的语法规则将信息组合起来，或将获得的数据填入预定义的句子模板的空白处以形成图像描述，见图
    1（底部）。
- en: Li et al. ([2011](#bib.bib14)) firstly uses an image recogniser to obtain visual
    information from the image, including objects, attributes of objects and spatial
    relationships between different objects. And then they encode the information
    as a triple form of $<<$adj1, obj1 $>$, prep, $<$adj2, obj2 $>>$. Furthermore,
    an approach based on web-scale n-gram is used to get the frequency counts of all
    possible n-gram sequences ($1\leq n\leq 5$). Finally, the phrases are selected
    and fused, and the best combination is accepted as the description of the query
    image by the dynamic programming algorithm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 ([2011](#bib.bib14)) 首先使用图像识别器从图像中获得视觉信息，包括对象、对象的属性和不同对象之间的空间关系。然后，他们将信息编码为
    $<<$adj1, obj1 $>$, prep, $<$adj2, obj2 $>>$ 的三元组形式。此外，使用基于网页规模 n-gram 的方法来获取所有可能的
    n-gram 序列的频率计数（$1\leq n\leq 5$）。最后，短语被选择和融合，通过动态规划算法接受最佳组合作为查询图像的描述。
- en: Kulkarni et al. ([2011](#bib.bib13)) uses an object detector to detect objects
    in the image, and then sends candidate object regions into attribute classifier
    and prepositional relation function to obtain attribute information of candidate
    objects and prepositional relation information between objects. Furthermore, a
    Conditional Random Field (CRF) is constructed to deduce the relevant information
    previously obtained for final use.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Kulkarni 等人 ([2011](#bib.bib13)) 使用了一个对象检测器来检测图像中的对象，然后将候选对象区域送入属性分类器和介词关系函数，以获得候选对象的属性信息和对象之间的介词关系信息。此外，构建了一个条件随机场（CRF）来推断先前获得的相关信息以供最终使用。
- en: Compared with retrieval-based methods, template-based methods can also generate
    grammatically correct description statements, and because this method needs to
    detect objects from the image, the generated description is more relevant to the
    image to some extent. But, the deficiencies of template-based methods are also
    apparent. On one hand, sentence templates or grammar rules need to be pre-designed
    artificially, so this method can not generate variable-length sentences, which
    limits the diversity of descriptions between different images, and descriptions
    may seem rigid and unnatural; On the other hand, the performance of the object
    detector limits the accuracy of image description, so the generated description
    may omit the details of the query image.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于检索的方法相比，基于模板的方法也可以生成语法正确的描述语句，而且由于这种方法需要从图像中检测对象，因此生成的描述在某种程度上与图像更相关。但是，基于模板的方法的缺陷也很明显。一方面，句子模板或语法规则需要人为预先设计，因此这种方法不能生成可变长度的句子，这限制了不同图像之间描述的多样性，描述可能显得僵硬和不自然；另一方面，对象检测器的性能限制了图像描述的准确性，因此生成的描述可能遗漏查询图像的细节。
- en: 3 Deep Learning Methods
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习方法
- en: In recent years, deep learning methods have made significant progress in CV
    and NLP. Inspired by machine translation Sutskever et al. ([2014](#bib.bib21)),
    the Encoder-Decoder structure is also applied to image captioning. Usually, CNN
    is used to construct an Encoder to extract and encode information from images.
    RNN is used to construct a Decoder to generate descriptions. On this basis, many
    researchers have also proposed various efficient improvement methods, but they
    have different focuses. Therefore, we divide them into multiple sub-categories
    according to the improved focus, then introduce and discuss each sub-category
    separately.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习方法在计算机视觉（CV）和自然语言处理（NLP）方面取得了显著进展。受到机器翻译 Sutskever 等人 ([2014](#bib.bib21))
    的启发，编码器-解码器结构也被应用于图像描述。通常，CNN 被用来构建编码器，以从图像中提取和编码信息。RNN 被用来构建解码器，以生成描述。在此基础上，许多研究人员还提出了各种高效的改进方法，但它们的关注点不同。因此，我们根据改进的重点将它们分为多个子类别，然后分别介绍和讨论每个子类别。
- en: 3.1 Basic Encoder-Decoder structure
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基本的编码器-解码器结构
- en: Show and Tell Vinyals et al. ([2015](#bib.bib23)) is the first work to apply
    the Encoder-Decoder structure proposed in machine translation to image captioning.
    It also serves as the basis for subsequent improvements and a baseline model for
    performance comparison between models. The model structure is shown in Figure
    2 (top).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Show and Tell Vinyals 等人 ([2015](#bib.bib23)) 是第一个将机器翻译中提出的编码器-解码器结构应用于图像描述的工作。它也成为了后续改进的基础和模型性能比较的基准模型。模型结构见图
    2（顶部）。
- en: '![Refer to caption](img/3b0e531c92ca8aa7e7794c2befcfa250.png)![Refer to caption](img/1068558985fbd3be09071a67ea9228f4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3b0e531c92ca8aa7e7794c2befcfa250.png)![参见标题](img/1068558985fbd3be09071a67ea9228f4.png)'
- en: 'Figure 2: Models Based on Encoder-Decoder structure: Show and tell (top). Show,
    attend and tell (bottom)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：基于编码器-解码器结构的模型：展示与描述（顶部）。展示、关注与描述（底部）
- en: This model first uses the CNN as the Encoder part, encodes the image into a
    fixed-length vector representation as the image feature map, and then sends the
    image feature map to the Decoder part of the RNN to decode and generate an image
    description. It can be expressed as Eq.(1)-Eq.(3). The Encoder part is a CNN,
    which corresponds to GoogLeNet (Inception V3); the Decoder part is LSTM.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型首先使用 CNN 作为编码器部分，将图像编码为固定长度的向量表示，即图像特征图，然后将图像特征图发送到 RNN 的解码器部分，以解码并生成图像描述。这可以表示为
    Eq.(1)-Eq.(3)。编码器部分是 CNN，对应于 GoogLeNet（Inception V3）；解码器部分是 LSTM。
- en: '|  | $\displaystyle x_{-1}$ | $\displaystyle={\rm Encoder}(I)$ |  | (1) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x_{-1}$ | $\displaystyle={\rm Encoder}(I)$ |  | (1) |'
- en: '|  | $\displaystyle x_{t}$ | $\displaystyle=W_{e}S_{t},$ | $\displaystyle t\in\{0,...,N-1\}$
    |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x_{t}$ | $\displaystyle=W_{e}S_{t},$ | $\displaystyle t\in\{0,...,N-1\}$
    |  | (2) |'
- en: '|  | $\displaystyle p_{t+1}$ | $\displaystyle={\rm Decoder}(x_{t}),$ | $\displaystyle
    t\in\{0,...,N-1\}$ |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{t+1}$ | $\displaystyle={\rm Decoder}(x_{t}),$ | $\displaystyle
    t\in\{0,...,N-1\}$ |  | (3) |'
- en: Suppose the vocabulary size is $D$, where $I$ represents the input image, $x_{-1}$
    is the feature map, which is only used to initialize the LSTM; $S_{t}$ is the
    one-hot vector in size $D$, representing the $t$-th word of the image description,
    and $S_{0}$ is the $<$START$>$ tag, $S_{N}$ is the $<$END$>$ tag; $W_{e}$ is the
    word embedding matrix; $p_{t+1}\in\mathbb{R}^{D}$ represents the probability vector
    generated by the $t+1$ time step, wherein the most probable one corresponds to
    the word as the time step word output.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设词汇表大小为 $D$，其中 $I$ 表示输入图像，$x_{-1}$ 是特征图，仅用于初始化 LSTM；$S_{t}$ 是大小为 $D$ 的 one-hot
    向量，表示图像描述中的第 $t$ 个词，$S_{0}$ 是 $<$START$>$ 标签，$S_{N}$ 是 $<$END$>$ 标签；$W_{e}$ 是词嵌入矩阵；$p_{t+1}\in\mathbb{R}^{D}$
    表示第 $t+1$ 时间步生成的概率向量，其中最可能的词对应于时间步词输出。
- en: Show Attend and Tell Xu et al. ([2015](#bib.bib25)) is an extension of Vinyals
    et al. ([2015](#bib.bib23)), which introduces a visual attention mechanism based
    on the Encoder-Decoder structure, which can dynamically focus on the salient regions
    of the image during the process of generating descriptions in Decoder. The model
    structure is shown in Figure 2 (bottom).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 展示、关注与描述 Xu 等人（[2015](#bib.bib25)）是 Vinyals 等人（[2015](#bib.bib23)）的扩展，引入了基于编码器-解码器结构的视觉注意机制，在解码器生成描述的过程中可以动态关注图像的显著区域。模型结构如图
    2（底部）所示。
- en: This model also uses a CNN as Encoder to extract $L$ vectors of $K$ dimensions
    from the image, each vector corresponds to a portion of the image. But unlike
    Vinyals et al. ([2015](#bib.bib23)), the model uses the underlying convolutional
    layer output instead of the final fully connected layer output as the image feature
    vector.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还使用 CNN 作为编码器，从图像中提取 $L$ 个 $K$ 维向量，每个向量对应图像的一部分。但与 Vinyals 等人（[2015](#bib.bib23)）不同，该模型使用底层卷积层的输出，而不是最终全连接层的输出作为图像特征向量。
- en: '|  | $a=\{a_{1},...,a_{L}\},a_{i}\in\mathbb{R}^{K}$ |  | (4) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $a=\{a_{1},...,a_{L}\},a_{i}\in\mathbb{R}^{K}$ |  | (4) |'
- en: In the Decoder part, Xu et al. ([2015](#bib.bib25)) also uses LSTM for description
    generation. But this model needs to use the image-based feature vector $a$ for
    each time step $t$ to generate the context vector $z_{t}=\sum_{i=1}^{L}\alpha_{ti}a_{i}$.
    This is the embodiment of the attention mechanism, $\alpha_{t}\in\mathbb{R}^{L}$
    is the attention weight vector of the $t$ time step, which satisfies $\sum_{i=1}^{L}\alpha_{ti}=1$.
    $a$ can be predicted by the simple neural network $f_{\rm att}$ and the Softmax
    activation function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码器部分，Xu 等人（[2015](#bib.bib25)）还使用 LSTM 进行描述生成。但该模型需要在每个时间步 $t$ 使用基于图像的特征向量
    $a$ 来生成上下文向量 $z_{t}=\sum_{i=1}^{L}\alpha_{ti}a_{i}$。这是注意力机制的体现，$\alpha_{t}\in\mathbb{R}^{L}$
    是第 $t$ 时间步的注意力权重向量，满足 $\sum_{i=1}^{L}\alpha_{ti}=1$。$a$ 可以通过简单的神经网络 $f_{\rm att}$
    和 Softmax 激活函数进行预测。
- en: '|  | $\alpha_{ti}\propto\exp\{f_{\rm att}(a_{i},m_{t-1})\}$ |  | (5) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{ti}\propto\exp\{f_{\rm att}(a_{i},m_{t-1})\}$ |  | (5) |'
- en: Therefore, the attention Encoder-Decoder structure can be expressed as Eq.(6)-Eq.(9).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，注意力编码器-解码器结构可以表示为 Eq.(6)-Eq.(9)。
- en: '|  | $\displaystyle a$ | $\displaystyle={\rm Encoder}(I)$ |  | (6) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle a$ | $\displaystyle={\rm Encoder}(I)$ |  | (6) |'
- en: '|  | $\displaystyle z_{t}$ | $\displaystyle=\sum_{i=1}^{L}\alpha_{ti}a_{i},$
    | $\displaystyle\alpha_{ti}\in\mathbb{R},a_{i}\in\mathbb{R}^{K}$ |  | (7) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z_{t}$ | $\displaystyle=\sum_{i=1}^{L}\alpha_{ti}a_{i},$
    | $\displaystyle\alpha_{ti}\in\mathbb{R},a_{i}\in\mathbb{R}^{K}$ |  | (7) |'
- en: '|  | $\displaystyle x_{t}$ | $\displaystyle=W_{e}S_{t},$ | $\displaystyle t\in\{0,...,N-1\}$
    |  | (8) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x_{t}$ | $\displaystyle=W_{e}S_{t},$ | $\displaystyle t\in\{0,...,N-1\}$
    |  | (8) |'
- en: '|  | $\displaystyle p_{t+1}$ | $\displaystyle={\rm Decoder}(x_{t},z_{t}),$
    | $\displaystyle t\in\{0,...,N-1\}$ |  | (9) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{t+1}$ | $\displaystyle={\rm Decoder}(x_{t},z_{t}),$
    | $\displaystyle t\in\{0,...,N-1\}$ |  | (9) |'
- en: The above Eqs is the Soft attention mechanism proposed in the paper, details
    are shown in Figure 3 (left), and another Hard attention is also proposed. However,
    most of the improved models use Soft attention easy to implement, so only the
    Soft attention mechanism is introduced here.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程是论文中提出的**Soft**注意力机制，详细内容见图3（左），此外还提出了另一种**Hard**注意力机制。然而，大多数改进的模型使用易于实现的**Soft**注意力机制，因此这里仅介绍**Soft**注意力机制。
- en: 3.2 Improvements in Encoder
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 编码器的改进
- en: You et al. ([2016](#bib.bib28)) proposes a semantic attention model, in addition
    to using CNN’s intermediate activation output as the global feature of the image
    $v$, and also using a set of attribute detectors to extract $\{A_{i}\}$ the most
    likely to appear in the image. Each attribute $A_{i}$ corresponds to an entry
    in the vocabulary, so the model encodes the image as a collection of visual features
    and semantic features. Then adaptively process $\{A_{i}\}$ to calculate the input
    of the Decoder $x_{t}$ and get the current word output $p_{t}$.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: You 等人 ([2016](#bib.bib28)) 提出了一个语义注意力模型，除了使用 CNN 的中间激活输出作为图像的全局特征 $v$，还使用一组属性检测器提取
    $\{A_{i}\}$ 中最可能出现在图像中的属性。每个属性 $A_{i}$ 对应词汇表中的一个条目，因此模型将图像编码为视觉特征和语义特征的集合。然后自适应地处理
    $\{A_{i}\}$ 以计算解码器输入 $x_{t}$ 并获得当前词输出 $p_{t}$。
- en: '|  | $\displaystyle v$ | $\displaystyle={\rm Encoder}(I)$ |  | (10) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v$ | $\displaystyle={\rm Encoder}(I)$ |  | (10) |'
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle={\rm Decoder}(h_{t-1},x_{t})$ |  |
    (11) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{t}$ | $\displaystyle={\rm Decoder}(h_{t-1},x_{t})$ |  |
    (11) |'
- en: '|  | $\displaystyle p_{t}$ | $\displaystyle=\varphi(h_{t},\{A_{i}\})$ |  |
    (12) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{t}$ | $\displaystyle=\varphi(h_{t},\{A_{i}\})$ |  |
    (12) |'
- en: '|  | $\displaystyle x_{t}$ | $\displaystyle=\phi(p_{t-1},\{A_{i}\})$ |  | (13)
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x_{t}$ | $\displaystyle=\phi(p_{t-1},\{A_{i}\})$ |  | (13)
    |'
- en: Liu et al. ([2017](#bib.bib16)) returns the image captioning problem back to
    machine translation, which first uses the object detector to represent the image
    $I$ as the sequence of detection objects $seq(I)=\{O_{1},O_{2},...,O_{T_{A}}\}$,
    where $\{O_{1},...,O_{T_{A}-1}\}$ is the image objects feature representation,
    the last item $O_{T_{A}}$ is the global feature of the image; then applies the
    Sequence 2 Sequence framework in machine translation to $seq(I)$ to generate the
    image description $S=\{S_{1},S_{2},...,S_{T_{B}}\}$, Encoder and Decoder are implemented
    using LSTM.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Liu 等人 ([2017](#bib.bib16)) 将图像描述问题回归到机器翻译中，首先使用对象检测器将图像 $I$ 表示为检测对象的序列 $seq(I)=\{O_{1},O_{2},...,O_{T_{A}}\}$，其中
    $\{O_{1},...,O_{T_{A}-1}\}$ 是图像对象特征表示，最后一个项目 $O_{T_{A}}$ 是图像的全局特征；然后将机器翻译中的序列到序列框架应用于
    $seq(I)$ 生成图像描述 $S=\{S_{1},S_{2},...,S_{T_{B}}\}$，编码器和解码器使用 LSTM 实现。
- en: '|  | $\displaystyle h_{t_{E}}={\rm Encoder}(O_{t_{E}},h_{t_{E}-1}),t_{E}=1,2,...,T_{A}$
    |  | (14) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{t_{E}}={\rm Encoder}(O_{t_{E}},h_{t_{E}-1}),t_{E}=1,2,...,T_{A}$
    |  | (14) |'
- en: '|  | $\displaystyle d_{t_{D}}={\rm Decoder}(S_{t_{D}},d_{t_{D}-1}),t_{D}=1,2,...,T_{B}$
    |  | (15) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d_{t_{D}}={\rm Decoder}(S_{t_{D}},d_{t_{D}-1}),t_{D}=1,2,...,T_{B}$
    |  | (15) |'
- en: In addition, when $S_{T}$ is generated, the model applies the attention mechanism
    to generate $d_{t-1}^{\prime}$ on the Encoder hidden layer sequence output $h=\{h_{1},h_{2},...,h_{T_{A}}\}$.
    Then cocat $d_{t-1}^{\prime}$ and $d_{t}$ use the Softmax activation function
    to generate the current $S_{t}$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当生成 $S_{T}$ 时，模型应用注意力机制生成 Encoder 隐藏层序列输出 $h=\{h_{1},h_{2},...,h_{T_{A}}\}$
    上的 $d_{t-1}^{\prime}$。然后将 $d_{t-1}^{\prime}$ 和 $d_{t}$ 连接起来，使用 **Softmax** 激活函数生成当前的
    $S_{t}$。
- en: Chen et al. ([2017](#bib.bib5)) believes that CNN’s kernels can be used as pattern
    detectors, and each channel of image feature map is activated by the corresponding
    convolution kernel. Therefore, the application of attention mechanism on the channel
    can be regarded as a process of selecting image semantic attributes. They proposed
    SCA-CNN, which applies the attention mechanism to both space and channel. However,
    unlike the previous attention mechanism, when calculating the context vector,
    they only weight the region features without summing, which can ensure that the
    feature vector and the context vector are the same sizes, so the SCA-CNN can be
    embedded in the stack multiple times.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人 ([2017](#bib.bib5)) 认为 CNN 的卷积核可以用作模式检测器，每个图像特征图的通道由相应的卷积核激活。因此，对通道应用注意力机制可以看作是选择图像语义属性的过程。他们提出了
    SCA-CNN，该方法将注意力机制应用于空间和通道。然而，与之前的注意力机制不同，在计算上下文向量时，他们只加权区域特征而不求和，这可以确保特征向量和上下文向量大小相同，因此
    SCA-CNN 可以嵌入堆叠多次。
- en: Fu et al. ([2017](#bib.bib8)) introduced advanced semantic information to improve
    image description based on attention. Firstly, the object detection generates
    a series of candidate regions, and a two-classifier is used to classify the candidate
    regions (good/bad). Finally, the first 29 regions and the image global region
    are selected as the visual feature information. The attention mechanism generates
    a context vector $z_{t}$. In addition, they use LDA to model all descriptions
    in the dataset to map the images flexibly to 80-Dimensional topic vectors (corresponding
    to the implicit 80 scene categories) and then train a multi-layer perceptron to
    predict the scene context vector $s$ to better generate image descriptions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Fu 等人 ([2017](#bib.bib8)) 引入了先进的语义信息来改进基于注意力的图像描述。首先，对象检测生成一系列候选区域，然后使用两个分类器对候选区域进行分类（好/坏）。最后，选择前
    29 个区域和图像全局区域作为视觉特征信息。注意力机制生成上下文向量 $z_{t}$。此外，他们使用 LDA 对数据集中所有描述进行建模，将图像灵活地映射到
    80 维主题向量（对应隐含的 80 个场景类别），然后训练一个多层感知机来预测场景上下文向量 $s$，以更好地生成图像描述。
- en: Yao et al. ([2018](#bib.bib27)) believes that the semantic relationship and
    spatial relationship between image objects are helpful for image description generation.
    They first use the object detection module Faster R-CNN Ren et al. ([2015](#bib.bib20))
    to detect objects in the image, and represent the image as $K$ image saliency
    area containing the object $V=\{v_{i}\}_{i=1}^{K}$; Then use a simple classification
    network to predict the semantic relationship between the objects and construct
    a semantic relationship graph $\mathcal{G}_{sem}=(V,\varepsilon_{sem})$, and construct
    a spatial relationship graph $\mathcal{G}_{spa}=(V,\varepsilon_{spa})$ by using
    the positional relationship of the object area. Then they design a [GCN]-based
    image Encoder to fuse the semantic and spatial relationships between the objects
    to obtain visual features $V^{(1)}=\{v_{i}^{(1)}\}_{i=1}^{K}$ containing more
    information.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Yao 等人 ([2018](#bib.bib27)) 认为图像对象之间的语义关系和空间关系对图像描述生成有帮助。他们首先使用对象检测模块 Faster
    R-CNN Ren 等人 ([2015](#bib.bib20)) 检测图像中的对象，并将图像表示为 $K$ 个包含对象的图像显著性区域 $V=\{v_{i}\}_{i=1}^{K}$；然后使用一个简单的分类网络预测对象之间的语义关系，并构建语义关系图
    $\mathcal{G}_{sem}=(V,\varepsilon_{sem})$，并通过使用对象区域的位置关系构建空间关系图 $\mathcal{G}_{spa}=(V,\varepsilon_{spa})$。然后，他们设计了一个基于
    [GCN] 的图像编码器，以融合对象之间的语义和空间关系，从而获得包含更多信息的视觉特征 $V^{(1)}=\{v_{i}^{(1)}\}_{i=1}^{K}$。
- en: As can be seen from the above, the original intention of improving Encoder is
    mostly to extract more useful information from images, such as adding semantic
    information on the basis of visual information, replacing the original CNN response
    activation region with the object detection module. Therefore, these methods have
    improved the image description effect, but there are also some inherent defects.
    On one hand, object detection may affect the efficiency of image description generation,
    on the other hand, it is difficult to effectively interpret the reliability of
    the semantic information of the image acquired implicitly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述内容可以看出，改进编码器的原意主要是从图像中提取更多有用的信息，例如在视觉信息的基础上添加语义信息，或用对象检测模块替代原来的 CNN 响应激活区域。因此，这些方法提高了图像描述效果，但也存在一些固有的缺陷。一方面，对象检测可能会影响图像描述生成的效率，另一方面，难以有效解释隐式获得的图像语义信息的可靠性。
- en: '![Refer to caption](img/84c437269faa531571c909aa4337d7a0.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/84c437269faa531571c909aa4337d7a0.png)'
- en: 'Figure 3: Attention Language Model Details.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：注意力语言模型细节。
- en: 3.3 Improvements in Decoder
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 解码器中的改进
- en: Lu et al. ([2017](#bib.bib17)) believes that in the process of generating image
    description, visual attention should not be added to non-visual words such as
    prepositions and quantifiers. So they introduced a visual sentinel in Decoder,
    essentially adding a gating to the LSTM for generating the sentinel vector $s_{t}$
    for each time step. In addition, they think that visual attention should be more
    relevant to the current time step hidden layer state of LSTM, so the visual attention
    mechanism is improved compared to Xu et al. ([2015](#bib.bib25)), see Figure 3
    (center). When visual attention weights $\alpha_{t}$ are generated, the weight
    value $\beta_{t}$ is calculated to determine whether to visually focus on the
    image. Therefore the context vector for each time step is calculated as follows.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Lu 等人（[2017](#bib.bib17)）认为在生成图像描述的过程中，不应将视觉注意力添加到诸如介词和量词等非视觉词汇上。因此，他们在解码器中引入了一个视觉哨兵，实际上为每个时间步生成了一个哨兵向量
    $s_{t}$ 的 LSTM 添加了门控。此外，他们认为视觉注意力应与 LSTM 当前时间步的隐藏层状态更相关，因此与 Xu 等人（[2015](#bib.bib25)）相比，视觉注意力机制得到了改进，见图3（中间）。当生成视觉注意力权重
    $\alpha_{t}$ 时，会计算权重值 $\beta_{t}$ 以确定是否在视觉上关注图像。因此，每个时间步的上下文向量计算如下。
- en: '|  | $\displaystyle\alpha_{ti}$ | $\displaystyle\propto\exp\{f_{\rm Vatt}(a_{i},m_{t})\}$
    |  | (16) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\alpha_{ti}$ | $\displaystyle\propto\exp\{f_{\rm Vatt}(a_{i},m_{t})\}$
    |  | (16) |'
- en: '|  | $\displaystyle\beta_{t}$ | $\displaystyle\propto\exp\{f_{\rm Satt}(s_{t},m_{t})\}$
    |  | (17) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\beta_{t}$ | $\displaystyle\propto\exp\{f_{\rm Satt}(s_{t},m_{t})\}$
    |  | (17) |'
- en: '|  | $\displaystyle z_{t}$ | $\displaystyle=\beta_{t}s_{t}+(1-\beta_{t})\sum_{i=1}^{L}\alpha_{ti}a_{i}$
    |  | (18) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z_{t}$ | $\displaystyle=\beta_{t}s_{t}+(1-\beta_{t})\sum_{i=1}^{L}\alpha_{ti}a_{i}$
    |  | (18) |'
- en: Anderson et al. ([2018](#bib.bib2)) combines Bottom-Up and Top-Down attention.
    Firstly, based on Faster R-CNN as Bottom-Up attention model, a variable-size image
    feature set,$V=\{v_{i}\}_{i=1}^{K}$, is obtained. Each feature is the encoding
    of a salient region of the image. Decoder used to generate language descriptions
    uses a two-tier LSTM structure, see Figure 3 (right). The first LSTM acts as Top-Down
    attention layer, which applies attention mechanism on hidden layer output and
    visual feature $V$ to calculates context vector $z_{t}$. Then it is fed into the
    second LSTM and delivers the output of the second LSTM to Softmax classifier to
    generate the current time step word prediction.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Anderson 等人（[2018](#bib.bib2)）结合了自下而上的注意力和自上而下的注意力。首先，基于 Faster R-CNN 作为自下而上注意力模型，获得了一个可变大小的图像特征集
    $V=\{v_{i}\}_{i=1}^{K}$。每个特征是图像显著区域的编码。用于生成语言描述的解码器使用了一个两层的 LSTM 结构，见图3（右侧）。第一个
    LSTM 充当自上而下的注意力层，对隐藏层输出和视觉特征 $V$ 应用注意力机制，以计算上下文向量 $z_{t}$。然后将其输入到第二个 LSTM，并将第二个
    LSTM 的输出传递给 Softmax 分类器，以生成当前时间步的词预测。
- en: Zhou et al. ([2017](#bib.bib30)) pointed out that in previous work, image features
    are only initially fed into LSTM, or on the basis of which attention mechanism
    is introduced to compute context vectors to input LSTM. Whether text context could
    be used to improve image description performance has not been solved yet, that
    is, the relationship between generated words and visual information was not involved.
    To explore this problem, they proposed a Text-Conditional attention mechanism,
    which allows attention to focus on image features related to previously generated
    words. They fused the previously generated words with global image features $I$
    to generate context vector $z_{t}$, and then input them to LSTM to generate words
    $S_{t+1}$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Zhou 等人（[2017](#bib.bib30)）指出，在以往的工作中，图像特征仅在最初被输入到 LSTM 中，或者在此基础上引入了注意力机制来计算上下文向量并输入
    LSTM。文本上下文是否可以用来提高图像描述性能尚未解决，即生成的词与视觉信息之间的关系没有涉及。为了解决这个问题，他们提出了一种文本条件注意力机制，该机制允许注意力集中在与先前生成的词相关的图像特征上。他们将先前生成的词与全局图像特征
    $I$ 融合以生成上下文向量 $z_{t}$，然后将其输入到 LSTM 中生成词 $S_{t+1}$。
- en: '|  | $\displaystyle z_{t}$ | $\displaystyle=\phi(I\odot W_{C}S_{t})$ |  | (19)
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z_{t}$ | $\displaystyle=\phi(I\odot W_{C}S_{t})$ |  | (19)
    |'
- en: '|  | $\displaystyle z_{t}$ | $\displaystyle=\phi(I\odot W_{C}\sum_{k=1}^{t}\frac{S_{k-1}}{t})$
    |  | (20) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z_{t}$ | $\displaystyle=\phi(I\odot W_{C}\sum_{k=1}^{t}\frac{S_{k-1}}{t})$
    |  | (20) |'
- en: Eq.(19) is Text-Conditional attention in the form of 1-gram, and the context
    information is limited to the previous word; Eq.(20) is an extreme form, and the
    context information takes advantage of all the previously generated words.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（19）是1-gram形式的文本条件注意力，上下文信息仅限于前一个词；方程（20）是一个极端形式，上下文信息利用了所有之前生成的词汇。
- en: In most work, RNN in one or two layers is used as a language model to generate
    descriptive words. Fang et al. ([2018](#bib.bib7)) thinks that this structure
    can deal with visual words such as nouns more easily, but it may not be able to
    learn verbs and adjectives. Therefore, they proposed a deep attention language
    model based on multi-layer LSTM, which can learn more abstract word information,
    and design three overlapping methods to generate attention context vectors.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数工作中，使用一层或两层RNN作为语言模型来生成描述性词汇。Fang等人（[2018](#bib.bib7)）认为这种结构可以更容易地处理名词等视觉词汇，但可能无法学习动词和形容词。因此，他们提出了一种基于多层LSTM的深度注意力语言模型，该模型可以学习更抽象的词汇信息，并设计了三种重叠的方法来生成注意力上下文向量。
- en: LSTM is often used as Decoder part in image captioning tasks, but LSTM is relatively
    complex and can not be performed in parallel. Aneja et al. ([2018](#bib.bib3))
    and Wang and Chan ([2018](#bib.bib24)) proposed that CNN is used as Decoder part
    to generate image description, which can achieve the same effect as LSTM and greatly
    improve the computing speed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM通常被用作图像字幕任务中的解码器部分，但LSTM相对复杂且无法并行执行。Aneja等人（[2018](#bib.bib3)）和Wang与Chan（[2018](#bib.bib24)）提出使用CNN作为解码器部分来生成图像描述，这可以达到与LSTM相同的效果，并大大提高计算速度。
- en: When using RNN (e.g. LSTM and GRU) as Decoder to generate description, Decoder’s
    input, hidden states and output are usually expressed as 1-D vectors. Dai et al.
    ([2018](#bib.bib6)) considers that 2-D feature mapping is more effective in interpretation
    and convenient for visual analysis to study the relationship between input visual
    information and output descriptive words; secondly, 2-D features can retain important
    spatial structure information. Therefore, they proposed to design Decoder on 2-D
    feature maps. Firstly, CNN is used to transform an image into multi-channel 2-D
    feature mapping. Decoder still uses GRU structure, but the state mapping transformations
    is replaced by convolution operations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用RNN（例如LSTM和GRU）作为解码器生成描述时，解码器的输入、隐藏状态和输出通常表示为1维向量。Dai等人（[2018](#bib.bib6)）认为，2维特征映射在解释上更有效，且方便进行视觉分析，以研究输入视觉信息与输出描述词之间的关系；其次，2维特征可以保留重要的空间结构信息。因此，他们提出在2维特征图上设计解码器。首先，使用CNN将图像转换为多通道2维特征映射。解码器仍然使用GRU结构，但状态映射转换被卷积操作所替代。
- en: The above works show that the improvement of Decoder mainly focuses on the richness
    of information and the correctness of the attention when generating the description.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 上述工作表明，解码器的改进主要集中在生成描述时信息的丰富性和注意力的正确性。
- en: 3.4 Other Improvements
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 其他改进
- en: On the basis of Encoder and Decoder, Yang et al. ([2016](#bib.bib26)) introduced
    a Reviewer module, which is essentially an improved LSTM unit introducing attention
    mechanism. It is used to perform multiple Reviews on the local features of Encoder
    output, and calculate a fact vector $f_{t}$ at each step as the input of attention
    module in Decoder. The author considers that the fact vector extracted by Reviewer
    module is more compact and abstract than the image feature maps obtained by Encoder.
    Therefore, the visual attention of the model is applied to the Reviewer module,
    while the Decoder module applies the attention mechanism to the fact vector.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器和解码器的基础上，Yang等人（[2016](#bib.bib26)）引入了一个Reviewer模块，这本质上是一个改进的LSTM单元，引入了注意力机制。它用于对编码器输出的局部特征进行多次回顾，并在每一步计算一个事实向量$f_{t}$，作为解码器中注意力模块的输入。作者认为，由Reviewer模块提取的事实向量比编码器获得的图像特征图更紧凑、更抽象。因此，模型的视觉注意力应用于Reviewer模块，而解码器模块则将注意力机制应用于事实向量。
- en: Two forms of Reviwer module are introduced in this paper. One is Attention Input
    Reviewer, which first applies the attention mechanism to the image region features
    $a$ and then uses the attention output as the input of LSTM unit to generate the
    fact vector $f_{t}$,
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了两种形式的Reviewer模块。一种是**Attention Input Reviewer**，该模块首先将注意力机制应用于图像区域特征$a$，然后将注意力输出作为LSTM单元的输入来生成事实向量$f_{t}$。
- en: '|  | $\displaystyle\alpha_{ti}$ | $\displaystyle\propto\exp\{f_{\rm att}(a_{i},f_{t-1})\}$
    |  | (21) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\alpha_{ti}$ | $\displaystyle\propto\exp\{f_{\rm att}(a_{i},f_{t-1})\}$
    |  | (21) |'
- en: '|  | $\displaystyle\tilde{f}_{t}$ | $\displaystyle=\sum_{i=1}^{L}\alpha_{ti}a_{i}$
    |  | (22) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{f}_{t}$ | $\displaystyle=\sum_{i=1}^{L}\alpha_{ti}a_{i}$
    |  | (22) |'
- en: '|  | $\displaystyle f_{t}$ | $\displaystyle={\rm LSTM_{R}}(\tilde{f}_{t},f_{t-1})$
    |  | (23) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{t}$ | $\displaystyle={\rm LSTM_{R}}(\tilde{f}_{t},f_{t-1})$
    |  | (23) |'
- en: Another one is Attention Output Reviewer, which also applies attention mechanism
    to image region features, but uses zero vector as input of LSTM unit, fact vector
    is calculated as the sum of LSTM output and attention output,
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个是注意力输出审阅器，它也将注意力机制应用于图像区域特征，但使用零向量作为 LSTM 单元的输入，事实向量作为 LSTM 输出和注意力输出的总和进行计算，
- en: '|  | $\displaystyle f_{t}$ | $\displaystyle={\rm LSTM_{R}}(0,f_{t-1})+W\tilde{f}_{t}$
    |  | (24) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{t}$ | $\displaystyle={\rm LSTM_{R}}(0,f_{t-1})+W\tilde{f}_{t}$
    |  | (24) |'
- en: Inspired by Yang et al. ([2016](#bib.bib26)), Jiang et al. ([2018](#bib.bib11))
    designs a Guiding Network based on a simple neural network in Encoder and Decoder
    structure. The region feature set of the image is used as input to generate a
    guidance vector $v$ containing the global information of the image. The guidance
    vector $v$ will then be fused with the original input of the Decoder to ensure
    that richer image information is input when generating image descriptions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 Yang et al. ([2016](#bib.bib26)) 的启发，Jiang et al. ([2018](#bib.bib11)) 设计了一个基于编码器和解码器结构的简单神经网络的指导网络。图像的区域特征集作为输入生成一个包含图像全局信息的指导向量
    $v$。然后，指导向量 $v$ 将与解码器的原始输入融合，以确保在生成图像描述时输入更丰富的图像信息。
- en: '|  | Flickr30K | MS COCO |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | Flickr30K | MS COCO |'
- en: '| --- | --- | --- |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Methods | B-1 | B-2 | B-3 | B-4 | MT | CD | B-1 | B-2 | B-3 | B-4 | MT |
    CD | SP |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | B-1 | B-2 | B-3 | B-4 | MT | CD | B-1 | B-2 | B-3 | B-4 | MT | CD |
    SP |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
- en: '| Vinyals et al. ([2015](#bib.bib23)) | 66.3 | 42.3 | 27.7 | 18.3 | - | - |
    66.6 | 46.1 | 32.9 | 24.6 | - | - | - |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Vinyals et al. ([2015](#bib.bib23)) | 66.3 | 42.3 | 27.7 | 18.3 | - | - |
    66.6 | 46.1 | 32.9 | 24.6 | - | - | - |'
- en: '| Xu et al. ([2015](#bib.bib25)) | 66.7 | 43.4 | 28.8 | 19.1 | 18.49 | - |
    70.7 | 49.2 | 34.4 | 24.3 | 23.9 | - | - |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. ([2015](#bib.bib25)) | 66.7 | 43.4 | 28.8 | 19.1 | 18.49 | - |
    70.7 | 49.2 | 34.4 | 24.3 | 23.9 | - | - |'
- en: '| You et al. ([2016](#bib.bib28)) | 64.7 | 46.0 | 32.4 | 23.0 | 18.9 | - |
    70.9 | 53.7 | 40.2 | 30.4 | 24.3 | - | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| You et al. ([2016](#bib.bib28)) | 64.7 | 46.0 | 32.4 | 23.0 | 18.9 | - |
    70.9 | 53.7 | 40.2 | 30.4 | 24.3 | - | - |'
- en: '| Liu et al. ([2017](#bib.bib16)) | - | - | - | - | - | - | 73.1 | 56.7 | 42.9
    | 32.3 | 25.8 | 105.8 | 18.9 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. ([2017](#bib.bib16)) | - | - | - | - | - | - | 73.1 | 56.7 | 42.9
    | 32.3 | 25.8 | 105.8 | 18.9 |'
- en: '| Chen et al. ([2017](#bib.bib5)) | 66.2 | 46.8 | 32.5 | 22.3 | 19.5 | - |
    71.9 | 54.8 | 41.1 | 31.1 | 25.0 | - | - |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. ([2017](#bib.bib5)) | 66.2 | 46.8 | 32.5 | 22.3 | 19.5 | - |
    71.9 | 54.8 | 41.1 | 31.1 | 25.0 | - | - |'
- en: '| Fu et al. ([2017](#bib.bib8)) | 64.9 | 46.2 | 32.4 | 22.4 | 19.4 | 47.2 |
    72.4 | 55.5 | 41.8 | 31.3 | 24.8 | 95.5 | - |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Fu et al. ([2017](#bib.bib8)) | 64.9 | 46.2 | 32.4 | 22.4 | 19.4 | 47.2 |
    72.4 | 55.5 | 41.8 | 31.3 | 24.8 | 95.5 | - |'
- en: '| Yao et al. ([2018](#bib.bib27)) | - | - | - | - | - | - | 77.4 | - | - |
    37.1 | 28.1 | 117.1 | 21.1 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Yao et al. ([2018](#bib.bib27)) | - | - | - | - | - | - | 77.4 | - | - |
    37.1 | 28.1 | 117.1 | 21.1 |'
- en: '| Anderson et al. ([2018](#bib.bib2)) | - | - | - | - | - | - | 77.2 | - |
    - | 36.2 | 27.0 | 113.5 | 20.3 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Anderson et al. ([2018](#bib.bib2)) | - | - | - | - | - | - | 77.2 | - |
    - | 36.2 | 27.0 | 113.5 | 20.3 |'
- en: '| Zhou et al. ([2017](#bib.bib30)) | - | - | - | - | - | - | 71.6 | 54.5 |
    40.5 | 30.1 | 24.7 | 97.0 | - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Zhou et al. ([2017](#bib.bib30)) | - | - | - | - | - | - | 71.6 | 54.5 |
    40.5 | 30.1 | 24.7 | 97.0 | - |'
- en: '| Fang et al. ([2018](#bib.bib7)) | - | - | 32.8 | 23.4 | 18.7 | 43.7 | - |
    - | 44.2 | 34.0 | 26.4 | 105.6 | - |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Fang et al. ([2018](#bib.bib7)) | - | - | 32.8 | 23.4 | 18.7 | 43.7 | - |
    - | 44.2 | 34.0 | 26.4 | 105.6 | - |'
- en: '| Aneja et al. ([2018](#bib.bib3)) | - | - | - | - | - | - | 71.1 | 53.8 |
    39.4 | 28.7 | 24.4 | 91.2 | 17.5 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Aneja et al. ([2018](#bib.bib3)) | - | - | - | - | - | - | 71.1 | 53.8 |
    39.4 | 28.7 | 24.4 | 91.2 | 17.5 |'
- en: '| Wang and Chan ([2018](#bib.bib24)) | 60.7 | 42.5 | 29.2 | 19.9 | 19.1 | 39.5
    | 68.5 | 51.1 | 36.9 | 26.7 | 23.4 | 84.4 | - |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Wang and Chan ([2018](#bib.bib24)) | 60.7 | 42.5 | 29.2 | 19.9 | 19.1 | 39.5
    | 68.5 | 51.1 | 36.9 | 26.7 | 23.4 | 84.4 | - |'
- en: '| Dai et al. ([2018](#bib.bib6)) | - | - | - | 22.0 | - | 42.7 | - | - | -
    | 31.9 | - | 99.4 | 18.7 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Dai et al. ([2018](#bib.bib6)) | - | - | - | 22.0 | - | 42.7 | - | - | -
    | 31.9 | - | 99.4 | 18.7 |'
- en: 'Table 1: Evaluation results of some models. B-n, MT, CD, SP stand for BLEU-n,
    METEOR, CIDEr and SPICE respectively.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：一些模型的评估结果。B-n、MT、CD 和 SP 分别代表 BLEU-n、METEOR、CIDEr 和 SPICE。
- en: 4 Datasets
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 个数据集
- en: 'Image captioning based on deep learning methods requires a lot of label data.
    Fortunately, many researchers and research organization have collected and tagged
    data sets. Here we mainly introduce four common data sets: Flickr 8K Hodosh et
    al. ([2015](#bib.bib10)), Flickr 30K Young et al. ([2014](#bib.bib29)), MS COCO
    Lin et al. ([2014](#bib.bib15)) and Visual GenomeKrishna et al. ([2017](#bib.bib12)).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习方法的图像描述需要大量标签数据。幸运的是，许多研究人员和研究机构已经收集并标注了数据集。这里主要介绍四个常见的数据集：Flickr 8K Hodosh
    et al. ([2015](#bib.bib10))、Flickr 30K Young et al. ([2014](#bib.bib29))、MS COCO
    Lin et al. ([2014](#bib.bib15)) 和 Visual Genome Krishna et al. ([2017](#bib.bib12))。
- en: Flickr8K Hodosh et al. ([2015](#bib.bib10)) contains a total of 8092 images,
    which were collected from Flickr.com and captions were obtained through crowdsourcing
    services provided by Amazon Mechanical Turk. Each image contains five different
    captions for reference with an average length of 11.8 words, and these descriptions
    are required to accurately describe the objects, scenes, and activities displayed
    in the image. In practical applications, 8000 images are usually selected, of
    which 6000 for train, 1000 for verification, and 1000 for test.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Flickr8K Hodosh et al. ([2015](#bib.bib10)) 包含总共8092张图像，这些图像是从Flickr.com收集的，标题通过Amazon
    Mechanical Turk提供的众包服务获得。每张图像包含五个不同的标题供参考，平均长度为11.8个单词，这些描述需要准确描述图像中显示的物体、场景和活动。在实际应用中，通常选择8000张图像，其中6000张用于训练，1000张用于验证，1000张用于测试。
- en: Flickr30K Young et al. ([2014](#bib.bib29)) is an extension to Flickr8K. It
    contains 31,783 images (including 8092 images in Flickr8K) and 158,915 descriptions.
    An annotation guide similar to Flickr8K is used to obtain image descriptions,
    control description quality, and correct description errors. Usually, 1000 images
    are selected as validation data, 1000 images as test data, and the remaining images
    are used as train data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Flickr30K Young et al. ([2014](#bib.bib29)) 是对Flickr8K的扩展。它包含31,783张图像（包括Flickr8K中的8092张图像）和158,915个描述。使用类似于Flickr8K的注释指南来获得图像描述、控制描述质量并纠正描述错误。通常，选择1000张图像作为验证数据，1000张图像作为测试数据，其余图像用作训练数据。
- en: MicroSoft COCO Lin et al. ([2014](#bib.bib15)) is a large-scale dataset that
    can be used for object detection, instance segmentation, and image captioning.
    It is also the most popular dataset in image captioning. The dataset contains
    91 object categories, a total of 328K images, 2.5 million tag instances, and each
    image contains 5 descriptions. The dataset is divided into two parts. The part
    released in 2014 includes 82,783 train data, 40,504 validation data and 40,775
    test data. However, the description of the test set is not publicly available,
    so the train set data and the validation set data are often re-divided into training/validation/test
    set in practical applications.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: MicroSoft COCO Lin et al. ([2014](#bib.bib15)) 是一个大型数据集，可用于物体检测、实例分割和图像描述。它也是图像描述中最受欢迎的数据集。数据集包含91个物体类别，共328K张图像，250万标签实例，每张图像包含5个描述。数据集分为两个部分。2014年发布的部分包括82,783张训练数据、40,504张验证数据和40,775张测试数据。然而，测试集的描述未公开，因此实际应用中通常将训练集数据和验证集数据重新划分为训练/验证/测试集。
- en: Visual Genome Krishna et al. ([2017](#bib.bib12)) contains more than 108K images.
    Each image contains an average of 35 objects with dense description annotations,
    26 attributes and 21 interactions between objects. Therefore, Visual Genome dataset
    can be used to pre-train image captioning tasks that introduce spatial and semantic
    relationships between objects.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Visual Genome Krishna et al. ([2017](#bib.bib12)) 包含超过108K张图像。每张图像平均包含35个物体，具有密集的描述注释、26个属性和21种物体之间的互动。因此，Visual
    Genome 数据集可以用于预训练图像描述任务，引入物体之间的空间和语义关系。
- en: 5 Evaluation
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: BLEUPapineni et al. ([2002](#bib.bib19)) is the most commonly used evaluation
    metric in image captioning tasks. It was originally used to measure the quality
    of machine translation. The core idea of BLEU is that ”the closer the test sentences
    are to the reference sentences, the better”. In other words, BLEU is evaluated
    by comparing the similarity of the test sentences and the reference sentences
    at the n-gram level. Therefore, this method does not consider the grammatical
    correctness, synonyms, similar expressions, and is more credible only in the case
    of shorter sentences.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU Papineni et al. ([2002](#bib.bib19)) 是图像描述任务中最常用的评估指标。它最初用于衡量机器翻译的质量。BLEU
    的核心思想是“测试句子与参考句子越接近，效果越好”。换句话说，BLEU 通过比较测试句子和参考句子在 n-gram 级别的相似性来评估。因此，该方法不考虑语法正确性、同义词、相似表达，仅在较短句子的情况下更为可靠。
- en: METEOR Banerjee and Lavie ([2005](#bib.bib4)) is also a commonly used evaluation
    metric for machine translation. Firstly, test sentences are aligned with reference
    sentences, such as word precise matching, stemmer-based matching, synonym matching
    and alignment based on WordNet, etc. Then, similarity scores between the test
    and the reference sentences are calculated based on alignment results. The calculation
    of similarity scores involves such indicators as matching word accuracy and recall
    rate. This method solves some shortcomings of BLEU and can express better relevance
    at the sentence level.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: METEOR Banerjee and Lavie ([2005](#bib.bib4)) 也是一个常用的机器翻译评估指标。首先，将测试句子与参考句子对齐，例如通过词精确匹配、基于词干提取的匹配、同义词匹配和基于
    WordNet 的对齐等。然后，根据对齐结果计算测试句子和参考句子之间的相似度分数。相似度分数的计算涉及匹配词的准确率和召回率等指标。该方法解决了 BLEU
    的一些不足，并能更好地表达句子层面的相关性。
- en: CIDEr Vedantam et al. ([2015](#bib.bib22)) is an evaluation metric aiming at
    image captioning. The authors think that the past evaluation metrics have a strong
    correlation with human, but they can not evaluate the similarity between them
    and human. So they proposed Consensus-based evaluation metric. Each sentence is
    regarded as a ”document” and expressed as a TF-IDF vector. The weight of TF-IDF
    is calculated for each n-gram, and then the cosine similarity between the test
    sentences and the reference sentences is calculated for evaluation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: CIDEr Vedantam et al. ([2015](#bib.bib22)) 是一个针对图像描述的评估指标。作者认为过去的评估指标与人类有很强的相关性，但无法评估它们与人类之间的相似性。因此，他们提出了基于共识的评估指标。每个句子被视为一个“文档”，并表示为
    TF-IDF 向量。计算每个 n-gram 的 TF-IDF 权重，然后计算测试句子和参考句子之间的余弦相似度进行评估。
- en: SPICE Anderson et al. ([2016](#bib.bib1)) is also an evaluation metric designed
    for image captioning. The metric codes the objects, attributes and relationships
    in image description into a semantic graph. This method captures the human’s judgment
    of model generation description better than the existing n-gram based evaluation
    metrics and can reflect the advantages and disadvantages of the language model
    more accurately.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: SPICE Anderson et al. ([2016](#bib.bib1)) 也是一个为图像描述设计的评估指标。该指标将图像描述中的对象、属性和关系编码成一个语义图。该方法比现有的基于
    n-gram 的评估指标更好地捕捉了人类对模型生成描述的判断，并且可以更准确地反映语言模型的优缺点。
- en: 6 Discussion & Future Research Directions
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论与未来研究方向
- en: The evaluation results of some deep learning methods are shown in Table 1, which
    shows that deep learning methods have achieved great success in image captioning
    tasks. In the previous part, we mainly discussed the improved model based on Encoder-Decoder
    structure. The emphasis of different improvements is different, but most of them
    aim to enrich the visual feature information of images, which is also a common
    original intention of them. For example, the improvement of Encoder includes extracting
    more accurate salient region features from images by object detection, enriching
    visual information of images by extracting semantic relations between salient
    objects from images, and implicitly extracting a scene vector from images to guide
    the generation of descriptions, all of which are in order to obtain richer and
    more abstract information from images or obtain additional information. Further
    improvements of Decoder include increasing the use of previously generated descriptive
    words, adding control gates to language models to ensure proper application of
    attention mechanisms, and implicitly increasing the number of layers of LSTM to
    obtain more abstract information.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一些深度学习方法的评估结果显示在表1中，表明深度学习方法在图像字幕任务中取得了巨大成功。在前面的部分中，我们主要讨论了基于编码器-解码器结构的改进模型。不同改进的重点不同，但大多数目的是丰富图像的视觉特征信息，这也是它们的共同初衷。例如，对编码器的改进包括通过目标检测从图像中提取更精确的显著区域特征，通过提取图像中显著对象之间的语义关系丰富图像信息，以及通过从图像中隐式提取场景向量来指导描述的生成，所有这些都是为了从图像中获取更丰富和更抽象的信息或获得额外的信息。解码器的进一步改进包括增加先前生成的描述性词的使用，向语言模型添加控制门以确保适当应用注意机制，并隐式增加LSTM层的数量以获取更抽象的信息。
- en: Nevertheless, image captioning is far from the human level, so there is still
    much space for improvement. On one hand, we can continue to study how to extract
    richer visual information from images or combine the extracted feature maps into
    more abstract information to enhance the context features of Decoder. Such as
    introducing semantic segmentation into Encoder part and using the latest language
    models as Decoder; on the other hand, I think we can deepen the development of
    datasets. Existing image captioning datasets only correspond images and descriptions,
    regions of interest of descriptions and how to generate descriptions are not reflected.
    If the development of datasets can be strengthened, more monitoring information
    can be introduced into the training of models, which may improve the performance
    of image captioning.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，图像字幕与人类水平还有很大差距，因此还有很大的改进空间。一方面，我们可以继续研究如何从图像中提取更丰富的视觉信息，或者将提取的特征图结合成更抽象的信息以增强解码器的上下文特征。例如，在编码器部分引入语义分割并使用最新的语言模型作为解码器；另一方面，我认为我们可以深化数据集的开发。现有的图像字幕数据集只对应图像和描述，对描述的感兴趣区域以及如何生成描述的过程没有反映。如果数据集的开发能够加强，更多的监控信息可以被引入到模型的训练中，这可能会提高图像字幕的性能。
- en: 7 Conclusion
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'In this paper, image captioning based on deep learning methods is summarized.
    Firstly, traditional template-based and retrieval-based methods are briefly introduced.
    Secondly, the deep learning methods and their improvements based on Encoder-Decoder
    structure in recent years are mainly introduced. According to the emphasis on
    improvements, these improvements are divided into three parts: Encoder Improvements,
    Decoder Improvements, and Other Improvements. Then, we introduce the commonly
    used datasets and evaluation metrics in image captioning. Although image captioning
    based on deep learning has been improved, they also have much space for improvements.
    So finally, we summarize the results of some deep learning methods and forecast
    future research directions.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本文总结了基于深度学习方法的图像字幕。首先，简要介绍了传统基于模板和检索的方法。其次，主要介绍了近年来基于编码器-解码器结构的深度学习方法及其改进。根据改进的重点，这些改进分为三部分：编码器改进、解码器改进和其他改进。然后，介绍了图像字幕中常用的数据集和评估指标。尽管基于深度学习的图像字幕已经得到改进，但它们也还有很大的改进空间。因此，最后，我们总结了一些深度学习方法的结果，并预测未来的研究方向。
- en: References
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Anderson et al. [2016] P. Anderson, B. Fernando, M. Johnson, and S. Gould.
    SPICE: semantic propositional image caption evaluation. In ECCV, pages 382–398,
    2016.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson等人[2016] P. Anderson, B. Fernando, M. Johnson和S. Gould。SPICE：语义命题图像字幕评估。在ECCV，2016年，第382-398页。
- en: Anderson et al. [2018] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
    S. Gould, and L. Zhang. Bottom-up and top-down attention for image captioning
    and visual question answering. In CVPR, pages 6077–6086, 2018.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 等人 [2018] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S.
    Gould 和 L. Zhang. 图像描述和视觉问答的自下而上和自上而下注意力。载于 CVPR，第 6077–6086 页，2018 年。
- en: Aneja et al. [2018] J. Aneja, A. Deshpande, and A. G. Schwing. Convolutional
    image captioning. In CVPR, pages 5561–5570, 2018.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aneja 等人 [2018] J. Aneja, A. Deshpande 和 A. G. Schwing. 卷积图像描述。载于 CVPR，第 5561–5570
    页，2018 年。
- en: 'Banerjee and Lavie [2005] S. Banerjee and A. Lavie. METEOR: an automatic metric
    for MT evaluation with improved correlation with human judgments. In Proceedings
    of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation,
    pages 65–72, 2005.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banerjee 和 Lavie [2005] S. Banerjee 和 A. Lavie. METEOR：一种改进了与人工评估相关性的自动化评估指标。载于机器翻译内在和外在评估度量研讨会论文集，第
    65–72 页，2005 年。
- en: 'Chen et al. [2017] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and
    T. Chua. SCA-CNN: spatial and channel-wise attention in convolutional networks
    for image captioning. In CVPR, pages 6298–6306, 2017.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2017] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu 和 T. Chua.
    SCA-CNN：用于图像描述的卷积网络中的空间和通道注意力。载于 CVPR，第 6298–6306 页，2017 年。
- en: Dai et al. [2018] B. Dai, D. Ye, and D. Lin. Rethinking the form of latent states
    in image captioning. In ECCV, pages 294–310, 2018.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人 [2018] B. Dai, D. Ye 和 D. Lin. 重新思考图像描述中的潜在状态形式。载于 ECCV，第 294–310 页，2018
    年。
- en: Fang et al. [2018] F. Fang, H. Wang, Y. Chen, and P. Tang. Looking deeper and
    transferring attention for image captioning. Multimedia Tools Appl., 77(23):31159–31175,
    2018.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等人 [2018] F. Fang, H. Wang, Y. Chen 和 P. Tang. 深度分析和转移注意力用于图像描述。Multimedia
    Tools Appl., 77(23):31159–31175, 2018 年。
- en: 'Fu et al. [2017] K. Fu, J. Jin, R. Cui, F. Sha, and C. Zhang. Aligning where
    to see and what to tell: Image captioning with region-based attention and scene-specific
    contexts. IEEE Trans. Pattern Anal. Mach. Intell., 39(12):2321–2334, 2017.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人 [2017] K. Fu, J. Jin, R. Cui, F. Sha 和 C. Zhang. 对齐观察位置和描述内容：基于区域的注意力和场景特定上下文的图像描述。IEEE
    Trans. Pattern Anal. Mach. Intell., 39(12):2321–2334, 2017 年。
- en: Hochreiter and Schmidhuber [1997] S. Hochreiter and J. Schmidhuber. Long short-term
    memory. Neural Computation, 9(8):1735–1780, 1997.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber [1997] S. Hochreiter 和 J. Schmidhuber. 长短期记忆。神经计算，9(8):1735–1780，1997
    年。
- en: 'Hodosh et al. [2015] M. Hodosh, P. Young, and J. Hockenmaier. Framing image
    description as a ranking task: Data, models and evaluation metrics (extended abstract).
    In IJCAI, pages 4188–4192, 2015.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hodosh 等人 [2015] M. Hodosh, P. Young 和 J. Hockenmaier. 将图像描述框定为排序任务：数据、模型和评估指标（扩展摘要）。载于
    IJCAI，第 4188–4192 页，2015 年。
- en: Jiang et al. [2018] W. Jiang, L. Ma, X. Chen, H. Zhang, and W. Liu. Learning
    to guide decoding for image captioning. In AAAI, pages 6959–6966, 2018.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2018] W. Jiang, L. Ma, X. Chen, H. Zhang 和 W. Liu. 学习引导图像描述的解码过程。载于
    AAAI，第 6959–6966 页，2018 年。
- en: 'Krishna et al. [2017] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
    S. Chen, Y. Kalantidis, L. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei.
    Visual genome: Connecting language and vision using crowdsourced dense image annotations.
    International Journal of Computer Vision, 123(1):32–73, 2017.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等人 [2017] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
    S. Chen, Y. Kalantidis, L. Li, D. A. Shamma, M. S. Bernstein 和 L. Fei-Fei. 视觉基因组：使用众包密集图像注释连接语言和视觉。《计算机视觉国际期刊》，123(1):32–73,
    2017 年。
- en: 'Kulkarni et al. [2011] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C.
    Berg, and T. L. Berg. Baby talk: Understanding and generating simple image descriptions.
    In CVPR, pages 1601–1608, 2011.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni 等人 [2011] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg
    和 T. L. Berg. 宝宝话：理解和生成简单的图像描述。载于 CVPR，第 1601–1608 页，2011 年。
- en: Li et al. [2011] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi. Composing
    simple image descriptions using web-scale n-grams. In CoNLL, pages 220–228, 2011.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2011] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg 和 Y. Choi. 使用网页规模 n-gram
    生成简单的图像描述。载于 CoNLL，第 220–228 页，2011 年。
- en: 'Lin et al. [2014] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan,
    P. Dollár, and C. L. Zitnick. Microsoft COCO: common objects in context. In ECCV,
    pages 740–755, 2014.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2014] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan,
    P. Dollár 和 C. L. Zitnick. Microsoft COCO：上下文中的常见对象。载于 ECCV，第 740–755 页，2014 年。
- en: 'Liu et al. [2017] C. Liu, F. Sun, C. Wang, F. Wang, and A. L. Yuille. MAT:
    A multimodal attentive translator for image captioning. In IJCAI, pages 4033–4039,
    2017.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2017] C. Liu, F. Sun, C. Wang, F. Wang 和 A. L. Yuille. MAT：一种多模态注意力翻译器用于图像描述。载于
    IJCAI，第 4033–4039 页，2017 年。
- en: 'Lu et al. [2017] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when to
    look: Adaptive attention via a visual sentinel for image captioning. In CVPR,
    pages 3242–3250, 2017.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等人 [2017] J. Lu, C. Xiong, D. Parikh, 和 R. Socher. 知道何时查看: 通过视觉哨兵实现的自适应注意力用于图像描述.
    在 CVPR, 页码 3242–3250, 2017.'
- en: 'Ordonez et al. [2011] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing
    images using 1 million captioned photographs. In NIPS, pages 1143–1151, 2011.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ordonez 等人 [2011] V. Ordonez, G. Kulkarni, 和 T. L. Berg. Im2text: 使用 100 万张标注照片描述图像.
    在 NIPS, 页码 1143–1151, 2011.'
- en: 'Papineni et al. [2002] K. Papineni, S. Roukos, T. Ward, and W. Zhu. Bleu: a
    method for automatic evaluation of machine translation. In ACL, pages 311–318,
    2002.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papineni 等人 [2002] K. Papineni, S. Roukos, T. Ward, 和 W. Zhu. Bleu: 一种自动评估机器翻译的方法.
    在 ACL, 页码 311–318, 2002.'
- en: Ren et al. [2015] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN towards
    real-time object detection with region proposal networks. In NIPS, pages 91–99,
    2015.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等人 [2015] S. Ren, K. He, R. Girshick, 和 J. Sun. Faster R-CNN: 通过区域建议网络实现实时目标检测.
    在 NIPS, 页码 91–99, 2015.'
- en: Sutskever et al. [2014] I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence
    learning with neural networks. In NIPS, pages 3104–3112, 2014.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等人 [2014] I. Sutskever, O. Vinyals, 和 Q. Le. 基于神经网络的序列到序列学习. 在 NIPS,
    页码 3104–3112, 2014.
- en: 'Vedantam et al. [2015] R. Vedantam, C. L. Zitnick, and D. Parikh. Cider: Consensus-based
    image description evaluation. In CVPR, pages 4566–4575, 2015.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vedantam 等人 [2015] R. Vedantam, C. L. Zitnick, 和 D. Parikh. Cider: 基于共识的图像描述评估.
    在 CVPR, 页码 4566–4575, 2015.'
- en: 'Vinyals et al. [2015] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show
    and tell: A neural image caption generator. In CVPR, pages 3156–3164, 2015.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vinyals 等人 [2015] O. Vinyals, A. Toshev, S. Bengio, 和 D. Erhan. 展示与讲述: 一个神经图像标题生成器.
    在 CVPR, 页码 3156–3164, 2015.'
- en: 'Wang and Chan [2018] Q. Wang and A. B. Chan. CNN+CNN: convolutional decoders
    for image captioning. CoRR, abs/1805.09019, 2018.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 和 Chan [2018] Q. Wang 和 A. B. Chan. CNN+CNN: 用于图像描述的卷积解码器. CoRR, abs/1805.09019,
    2018.'
- en: 'Xu et al. [2015] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov,
    R. S. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation
    with visual attention. In ICML, pages 2048–2057, 2015.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 [2015] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov,
    R. S. Zemel, 和 Y. Bengio. 展示、关注与讲述: 带有视觉注意力的神经图像标题生成. 在 ICML, 页码 2048–2057, 2015.'
- en: Yang et al. [2016] Z. Yang, Y. Yuan, Y. Wu, W. W. Cohen, and R. Salakhutdinov.
    Review networks for caption generation. In NIPS, pages 2361–2369, 2016.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2016] Z. Yang, Y. Yuan, Y. Wu, W. W. Cohen, 和 R. Salakhutdinov. 用于标题生成的复审网络.
    在 NIPS, 页码 2361–2369, 2016.
- en: Yao et al. [2018] T. Yao, Y. Pan, Y. Li, and T. Mei. Exploring visual relationship
    for image captioning. In ECCV, pages 711–727, 2018.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2018] T. Yao, Y. Pan, Y. Li, 和 T. Mei. 探索视觉关系用于图像描述. 在 ECCV, 页码 711–727,
    2018.
- en: You et al. [2016] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image captioning
    with semantic attention. In CVPR, pages 4651–4659, 2016.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等人 [2016] Q. You, H. Jin, Z. Wang, C. Fang, 和 J. Luo. 带有语义注意力的图像描述. 在 CVPR,
    页码 4651–4659, 2016.
- en: 'Young et al. [2014] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image
    descriptions to visual denotations: New similarity metrics for semantic inference
    over event descriptions. TACL, 2:67–78, 2014.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Young 等人 [2014] P. Young, A. Lai, M. Hodosh, 和 J. Hockenmaier. 从图像描述到视觉指称:
    针对事件描述的新相似性度量. TACL, 2:67–78, 2014.'
- en: 'Zhou et al. [2017] L. Zhou, C. Xu, P. A. Koch, and J. J. Corso. Watch what
    you just said: Image captioning with text-conditional attention. In Proceedings
    of the on Thematic Workshops of ACM Multimedia, pages 305–313, 2017.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人 [2017] L. Zhou, C. Xu, P. A. Koch, 和 J. J. Corso. 观察你刚刚说了什么: 带有文本条件注意力的图像描述.
    在 ACM Multimedia 的主题研讨会会议录, 页码 305–313, 2017.'
