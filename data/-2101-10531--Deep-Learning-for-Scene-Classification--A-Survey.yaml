- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:57:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:57:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2101.10531] Deep Learning for Scene Classification: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2101.10531] 深度学习在场景分类中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2101.10531](https://ar5iv.labs.arxiv.org/html/2101.10531)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2101.10531](https://ar5iv.labs.arxiv.org/html/2101.10531)
- en: 'Deep Learning for Scene Classification: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在场景分类中的应用：综述
- en: Delu Zeng, Minyu Liao, Mohammad Tavakolian, Yulan Guo
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Delu Zeng, Minyu Liao, Mohammad Tavakolian, Yulan Guo
- en: Bolei Zhou, Dewen Hu, Matti Pietikäinen, and Li Liu Delu Zeng (dlzeng@scut.edu.cn)
    and Minyu Liao (201820127075@mail.-scut.edu.cn) are with the South China University
    of Technology, China. Mohammad Tavakolian (mohammad.tavakolian@oulu.fi), Matti
    Pietikäinen (matti.pietikainen@oulu.fi) and Li Liu (li.liu@oulu.fi) are with the
    University of Oulu, Finland. Yulan Guo (yulan.guo@nudt.edu.cn), Dewen Hu (dwhu@nudt.edu.cn)
    and Li Liu are with the National University of Defense Technology. Bolei Zhou
    (bzhou@ie.cuhk.edu.hk) is with the Chinese University of Hong Kong, China. Li
    Liu is the corresponding author. Delu Zeng, Minlu Liao, and Li Liu have equal
    contribution to this work and are cofirst authors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Bolei Zhou, Dewen Hu, Matti Pietikäinen, 和 Li Liu Delu Zeng (dlzeng@scut.edu.cn)
    和 Minyu Liao (201820127075@mail.-scut.edu.cn) 现任职于中国华南理工大学。Mohammad Tavakolian
    (mohammad.tavakolian@oulu.fi)、Matti Pietikäinen (matti.pietikainen@oulu.fi) 和
    Li Liu (li.liu@oulu.fi) 现任职于芬兰奥卢大学。Yulan Guo (yulan.guo@nudt.edu.cn)、Dewen Hu
    (dwhu@nudt.edu.cn) 和 Li Liu 现任职于国防科技大学。Bolei Zhou (bzhou@ie.cuhk.edu.hk) 现任职于中国香港中文大学。Li
    Liu 是通讯作者。Delu Zeng、Minlu Liao 和 Li Liu 对这项工作贡献相等，是共同第一作者。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Scene classification, aiming at classifying a scene image to one of the predefined
    scene categories by comprehending the entire image, is a longstanding, fundamental
    and challenging problem in computer vision. The rise of large-scale datasets,
    which constitute the corresponding dense sampling of diverse real-world scenes,
    and the renaissance of deep learning techniques, which learn powerful feature
    representations directly from big raw data, have been bringing remarkable progress
    in the field of scene representation and classification. To help researchers master
    needed advances in this field, the goal of this paper is to provide a comprehensive
    survey of recent achievements in scene classification using deep learning. More
    than 200 major publications are included in this survey covering different aspects
    of scene classification, including challenges, benchmark datasets, taxonomy, and
    quantitative performance comparisons of the reviewed methods. In retrospect of
    what has been achieved so far, this paper is also concluded with a list of promising
    research opportunities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 场景分类，旨在通过理解整个图像将场景图像分类到预定义的场景类别之一，是计算机视觉中一个长期存在、基础且具有挑战性的问题。大规模数据集的兴起，这些数据集涵盖了各种现实世界场景的密集采样，以及深度学习技术的复兴，这些技术直接从大量原始数据中学习强大的特征表示，已经在场景表示和分类领域带来了显著进展。为了帮助研究人员掌握这一领域所需的进展，本文的目标是提供一个全面的深度学习场景分类研究成果的综述。本文综述了200多篇重要出版物，涵盖了场景分类的不同方面，包括挑战、基准数据集、分类法以及所评审方法的定量性能比较。回顾迄今为止所取得的成就，本文还总结了有前景的研究机会列表。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Scene classification, Deep learning, Convolutional neural network, Scene representation,
    Literature survey.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 场景分类、深度学习、卷积神经网络、场景表示、文献综述。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The goal of scene classification is to classify a scene image¹¹1A scene is a
    semantically coherent view of a real-world environment that contains background
    and multiple objects, organized in a spatially licensed manner [[1](#bib.bib1),
    [2](#bib.bib2)]. to one of the predefined scene categories (such as beach, kitchen,
    and bakery), based on the image’s ambient content, objects, and their layout.
    Visual scene understanding requires reasoning about the diverse and complicated
    environments that we encounter in our daily life. Recognizing visual categories
    such as objects, actions and events is no doubt the indispensable ability of a
    visual system. Moreover, recognizing the scene where the objects appear is of
    equal importance for an intelligent system to predict the context for the recognized
    objects by reasoning “What is happening? What will happen next?”. Humans are remarkably
    efficient at categorizing natural scenes [[3](#bib.bib3), [4](#bib.bib4)]. However,
    it is not an easy task for machines due to the scene’s semantic ambiguity, and
    the large intraclass variations caused by imaging conditions like variations in
    illumination, viewing angle and scale, imaging distance, *etc*. As a longstanding,
    fundamental and challenging problem in computer vision, scene classification has
    been an active area of research for several decades, and has a wide range of applications,
    such as content based image retrieval [[5](#bib.bib5), [6](#bib.bib6)], robot
    navigation [[7](#bib.bib7), [8](#bib.bib8)], intelligent video surveillance [[9](#bib.bib9),
    [10](#bib.bib10)], augmented reality [[11](#bib.bib11), [12](#bib.bib12)], and
    disaster detection applications [[13](#bib.bib13)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 场景分类的目标是将场景图像分类¹¹1场景是对现实世界环境的语义一致的视图，包含背景和多个对象，以空间上许可的方式组织 [[1](#bib.bib1),
    [2](#bib.bib2)]。根据图像的环境内容、对象及其布局，将其分到预定义的场景类别（如海滩、厨房和面包店）之一。视觉场景理解需要推理我们在日常生活中遇到的各种复杂环境。识别如对象、动作和事件等视觉类别无疑是视觉系统不可或缺的能力。此外，识别对象出现的场景对智能系统预测已识别对象的背景同样重要，通过推理“发生了什么？接下来会发生什么？”
    人类在分类自然场景方面非常高效 [[3](#bib.bib3), [4](#bib.bib4)]。然而，由于场景的语义模糊性以及成像条件（如光照变化、视角和尺度、成像距离等）造成的大类内差异，这对机器来说并不是一项简单的任务。作为计算机视觉中的一个长期、基础且具有挑战性的问题，场景分类已经成为活跃的研究领域数十年，并且具有广泛的应用，如基于内容的图像检索
    [[5](#bib.bib5), [6](#bib.bib6)]、机器人导航 [[7](#bib.bib7), [8](#bib.bib8)]、智能视频监控
    [[9](#bib.bib9), [10](#bib.bib10)]、增强现实 [[11](#bib.bib11), [12](#bib.bib12)] 和灾害检测应用
    [[13](#bib.bib13)]。
- en: '![Refer to caption](img/787c67514e1a0a1c6b7459b2240c8462.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/787c67514e1a0a1c6b7459b2240c8462.png)'
- en: 'Figure 1: A performance overview of scene classification: we can observe a
    significant improvement on two benchmark datasets since the reignition of deep
    learning. Dense-SIFT [[14](#bib.bib14)], Object Bank [[15](#bib.bib15)] and OTC
    [[16](#bib.bib16)] are handcrafted methods, while the others are deep learning
    based methods.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：场景分类的性能概述：我们可以观察到自深度学习再度兴起以来，在两个基准数据集上的显著改善。Dense-SIFT [[14](#bib.bib14)]、Object
    Bank [[15](#bib.bib15)] 和 OTC [[16](#bib.bib16)] 是手工制作的方法，而其他则是基于深度学习的方法。
- en: 'Figure 2: A taxonomy of deep learning for scene classification. With the rise
    of large-scale datasets, powerful features are learned from pre-trained CNNs,
    fine-tuned CNNs, or specific CNNs, having made remarkable progress. The major
    features include global CNN features, spatially invariant features, semantic features,
    multi-layer features, and multi-view features. Meanwhile, many methods are improved
    via effective strategies, like encoding, attention learning, and context modeling.
    As a new issue, methods using RGB-D datasets, focus on learning depth specific
    features and fusing multiple modalities.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：场景分类深度学习的分类。随着大规模数据集的兴起，从预训练的CNN、微调的CNN或特定的CNN中学习到强大的特征，取得了显著进展。主要特征包括全局CNN特征、空间不变特征、语义特征、多层特征和多视角特征。同时，许多方法通过有效的策略如编码、注意力学习和上下文建模得到了改进。作为一个新问题，使用RGB-D数据集的方法专注于学习深度特定特征并融合多种模态。
- en: 'As the core of scene classification, scene representation is the process of
    transforming a scene image into its concise descriptors (*i.e.,* features), and
    still attracts tremendous and increasing attention. The recent revival of interest
    in Artificial Neural Networks (ANNs), particularly deep learning [[17](#bib.bib17)],
    has revolutionized computer vision and been ubiquitously used in various tasks
    like object classification and detection [[51](#bib.bib51), [52](#bib.bib52)],
    semantic segmentation [[53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55)] and
    scene classification [[28](#bib.bib28), [18](#bib.bib18)]. In 2012, object classification
    with the large-scale ImageNet dataset [[56](#bib.bib56)] achieved a significant
    breakthrough in performance by a Deep Neural Network (DNN) named AlexNet [[17](#bib.bib17)],
    which is arguably what reignited the field of ANNs and triggered the recent revolution
    in computer vision. Since then, research focus on scene classification has begun
    to move away from handcrafted feature engineering to deep learning, which can
    learn powerful representations directly from data. Recent advances in deep learning
    have opened the possibility of scene classification towards the datasets *of large
    scale* and *in the wild* [[18](#bib.bib18), [57](#bib.bib57), [25](#bib.bib25)],
    and many scene representations [[28](#bib.bib28), [58](#bib.bib58), [30](#bib.bib30),
    [21](#bib.bib21)] have been proposed. As illustrated in Fig. [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Deep Learning for Scene Classification: A Survey"), deep
    learning has brought significant improvements in scene classification. Given the
    exceptionally rapid rate of progress, this article attempts to track recent advances
    and summarize their achievements to gain a clearer picture of the current panorama
    in scene classification using deep learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '作为场景分类的核心，场景表示是将场景图像转换为其简明特征（*即*，特征）的过程，并且仍然吸引着大量且不断增长的关注。近年来对人工神经网络（ANNs）特别是深度学习的兴趣复兴[[17](#bib.bib17)]，彻底改变了计算机视觉，并在目标分类和检测[[51](#bib.bib51),
    [52](#bib.bib52)]、语义分割[[53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55)]以及场景分类[[28](#bib.bib28),
    [18](#bib.bib18)]等各种任务中被广泛使用。2012年，使用大规模ImageNet数据集[[56](#bib.bib56)]的目标分类通过名为AlexNet的深度神经网络（DNN）[[17](#bib.bib17)]取得了性能的重大突破，这可以说是重新点燃了ANNs领域，并引发了计算机视觉的近期革命。自那时起，场景分类的研究重点已开始从手工特征工程转向深度学习，深度学习可以直接从数据中学习强大的表示。深度学习的最新进展使得场景分类有可能向*大规模*和*复杂环境*的数据集[[18](#bib.bib18),
    [57](#bib.bib57), [25](#bib.bib25)]迈进，并且已经提出了许多场景表示[[28](#bib.bib28), [58](#bib.bib58),
    [30](#bib.bib30), [21](#bib.bib21)]。如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Deep Learning for Scene Classification: A Survey")所示，深度学习在场景分类中带来了显著的改进。鉴于进展异常迅速，本文尝试追踪最新进展并总结其成就，以便对当前使用深度学习进行的场景分类的全景有一个更清晰的了解。'
- en: 'Recently, several surveys for scene classification have also been available,
    such as [[59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61)]. Cheng et al. [[60](#bib.bib60)]
    provided a recent comprehensive review of the recent progress for remote sensing
    image scene classification. Wei et al. [[59](#bib.bib59)] carried out an experimental
    study of 14 scene descriptors mainly in the handcrafted feature engineering way
    for scene classification. Xie et al. [[61](#bib.bib61)] reviewed scene recognition
    approaches in the past two decades, and most of discussed methods in their survey
    appeared in this handcrafted way. *As opposed to* these existing reviews [[60](#bib.bib60),
    [59](#bib.bib59), [61](#bib.bib61)], this work herein summarizes the striking
    success and dominance in indoor/outdoor scene classification using deep learning
    and its related methods, but not including other scene classification tasks, *e.g.,*
    remote sensing scene classification [[62](#bib.bib62), [63](#bib.bib63), [60](#bib.bib60)],
    acoustic scene classification [[64](#bib.bib64), [65](#bib.bib65)], place classification [[66](#bib.bib66),
    [67](#bib.bib67)], *etc*. The major contributions of this work can be summarized
    as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，也有一些关于场景分类的综述文章，如 [[59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61)]。Cheng
    等人 [[60](#bib.bib60)] 提供了关于遥感图像场景分类最新进展的综合综述。Wei 等人 [[59](#bib.bib59)] 进行了 14
    种场景描述符的实验研究，主要集中在手工特征工程方法上。Xie 等人 [[61](#bib.bib61)] 回顾了过去二十年的场景识别方法，他们的综述中讨论的大多数方法都是手工特征工程的方法。*与*这些现有的综述
    [[60](#bib.bib60), [59](#bib.bib59), [61](#bib.bib61)] 相对的是，本文总结了深度学习及其相关方法在室内/室外场景分类中的显著成功和主导地位，但不包括其他场景分类任务，如遥感场景分类
    [[62](#bib.bib62), [63](#bib.bib63), [60](#bib.bib60)]，声学场景分类 [[64](#bib.bib64),
    [65](#bib.bib65)]，地点分类 [[66](#bib.bib66), [67](#bib.bib67)]，*等*。本文的主要贡献总结如下：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: As far as we know, this paper is the first to specifically focus on deep learning
    methods for indoor/outdoor scene classification, including RGB scene classification,
    as well as RGB-D scene classification.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，本文首次专注于室内/室外场景分类的深度学习方法，包括 RGB 场景分类以及 RGB-D 场景分类。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We present a taxonomy (see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep
    Learning for Scene Classification: A Survey")), covering the most recent and advanced
    progresses of deep learning for scene representation.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们提出了一个分类法（参见图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning for Scene
    Classification: A Survey")），涵盖了深度学习在场景表示方面的最新和最先进的进展。'
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Comprehensive comparisons of existing methods on several public datasets are
    provided, meanwhile we also present brief summaries and insightful discussions.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供了对若干公共数据集上现有方法的全面比较，同时我们还提供了简要总结和深刻讨论。
- en: 'The remainder of this paper is organized as follows: Challenges and benchmark
    datasets are summarized in Section [2](#S2 "2 Background ‣ Deep Learning for Scene
    Classification: A Survey"). In section [3](#S3 "3 Deep Learning based Methods
    ‣ Deep Learning for Scene Classification: A Survey"), we present a taxonomy of
    the existing deep learning based methods. Then in section [4](#S4 "4 Performance
    Comparison ‣ Deep Learning for Scene Classification: A Survey"), we provide an
    overall discussion of their performance (Tables [II](#S4.T2 "TABLE II ‣ 4.1 Performance
    on RGB scene data ‣ 4 Performance Comparison ‣ Deep Learning for Scene Classification:
    A Survey"), [IV](#S4.T4 "TABLE IV ‣ 4.1 Performance on RGB scene data ‣ 4 Performance
    Comparison ‣ Deep Learning for Scene Classification: A Survey")). Followed by
    Section [5](#S5 "5 Conclusion and Outlook ‣ Deep Learning for Scene Classification:
    A Survey") we conclude important future research outlook.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的剩余部分组织如下：挑战和基准数据集在第 [2](#S2 "2 Background ‣ Deep Learning for Scene Classification:
    A Survey") 节中总结。在第 [3](#S3 "3 Deep Learning based Methods ‣ Deep Learning for
    Scene Classification: A Survey") 节中，我们呈现了现有深度学习方法的分类法。然后在第 [4](#S4 "4 Performance
    Comparison ‣ Deep Learning for Scene Classification: A Survey") 节中，我们对这些方法的性能进行总体讨论（表
    [II](#S4.T2 "TABLE II ‣ 4.1 Performance on RGB scene data ‣ 4 Performance Comparison
    ‣ Deep Learning for Scene Classification: A Survey")、[IV](#S4.T4 "TABLE IV ‣ 4.1
    Performance on RGB scene data ‣ 4 Performance Comparison ‣ Deep Learning for Scene
    Classification: A Survey")）。接着在第 [5](#S5 "5 Conclusion and Outlook ‣ Deep Learning
    for Scene Classification: A Survey") 节中，我们总结了未来重要的研究展望。'
- en: 2 Background
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 The Problem and Challenge
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题与挑战
- en: Scene classification can be further dissected through analyzing its strong ties
    with related vision tasks, such as object classification and texture classification.
    As typical pattern recognition problems, these tasks all consist of feature representation
    and classification. However, in contrast to object classification (images are
    object-centric) or texture classification (images include only textures), the
    scene images are more complicated, and it is essential to further explore the
    content of scene, *e.g.,* what the semantic parts (*e.g.,* objects, textures,
    background) are, in what way they are organized together, and what their semantic
    connections with each other are. Despite over several decades of development in
    scene classification (shown in the appendix due to space limit), most of methods
    still have not been capable of performing at a level sufficient for various real-world
    scenes. The inherent difficulty is due to the nature of complexity and high variance
    of scenes. Overall, significant challenges in scene classification stem from large
    intraclass variations, semantic ambiguity, and computational efficiency.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 场景分类可以通过分析其与相关视觉任务（如物体分类和纹理分类）的紧密联系来进一步剖析。作为典型的模式识别问题，这些任务都涉及特征表示和分类。然而，与物体分类（图像以物体为中心）或纹理分类（图像仅包含纹理）不同，场景图像更为复杂，因此需要进一步探索场景的内容，*例如*，语义部分（*例如*，物体、纹理、背景）是什么，它们是如何组织在一起的，以及它们之间的语义联系是什么。尽管场景分类在过去几十年中有了发展（由于空间限制，详见附录），但大多数方法仍未能在各种真实场景中达到足够的水平。内在的困难在于场景的复杂性和高变异性。总体而言，场景分类面临的重大挑战来自于大类内变异、语义模糊和计算效率。
- en: '![Refer to caption](img/e122f14e0b7e232244e0176c633b04a9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e122f14e0b7e232244e0176c633b04a9.png)'
- en: 'Figure 3: Illustrations of *large intraclass variation* and *semantic ambiguity*.
    Top (large intraclass variation): The shopping malls are quite different caused
    by lighting and overall content. Below (semantic ambiguity): General layout and
    uniformly arranged objects are similar on archive, bookstore, and library.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：*大类内变异*和*语义模糊*的示意图。顶部（大类内变异）：由于光照和整体内容的不同，购物中心的外观差异很大。底部（语义模糊）：档案馆、书店和图书馆的总体布局和均匀安排的物体相似。
- en: 'Large intraclass variation. Intraclass variation mainly originates from intrinsic
    factors of the scene itself and imaging conditions. In terms of intrinsic factors,
    each scene can have many different example images, possibly varying with large
    variations among various objects, background, or human activities. Imaging conditions
    like changes in illumination, viewpoint, scale and heavy occlusion, clutter, shading,
    blur, motion, *etc*. contribute to large intraclass variations. Further challenges
    may be added by digitization artifacts, noise corruption, poor resolution, and
    filtering distortion. For instance, three shopping malls (top row of Fig. [3](#S2.F3
    "Figure 3 ‣ 2.1 The Problem and Challenge ‣ 2 Background ‣ Deep Learning for Scene
    Classification: A Survey")) are shown with different lighting conditions, viewing
    angle, and objects.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大类内变异。类内变异主要来源于场景本身的内在因素和成像条件。在内在因素方面，每个场景可能有许多不同的示例图像，这些图像在各种物体、背景或人类活动之间可能会有很大的变化。成像条件如光照、视角、尺度以及严重的遮挡、杂乱、阴影、模糊、运动，*等等*，都会导致大类内变异。数字化伪影、噪声污染、低分辨率和过滤失真等也可能带来进一步的挑战。例如，图[3](#S2.F3
    "图 3 ‣ 2.1 问题与挑战 ‣ 2 背景 ‣ 深度学习在场景分类中的应用")顶部显示了三种具有不同光照条件、视角和物体的购物中心。
- en: 'Semantic ambiguity. Since images of different classes may share similar objects,
    textures, background, *etc.*, they look very similar in visual appearances, which
    causes ambiguity among them [[68](#bib.bib68), [43](#bib.bib43)]. The bottom row
    of Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 The Problem and Challenge ‣ 2 Background ‣
    Deep Learning for Scene Classification: A Survey") depicts strong visual correlation
    between three different indoor scenes, *i.e.,* archive, bookstore, and library.
    With the emerging of new scene categories, the problem of semantic ambiguity would
    be more serious. In addition, scene category annotation is subjective, relying
    on the experience of the annotators, therefore a scene image may belong to multiple
    semantic categories [[68](#bib.bib68), [69](#bib.bib69)].'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '语义模糊。由于不同类别的图像可能共享类似的对象、纹理、背景、*等*，它们在视觉上看起来非常相似，这导致了它们之间的模糊性[[68](#bib.bib68),
    [43](#bib.bib43)]。图[3](#S2.F3 "Figure 3 ‣ 2.1 The Problem and Challenge ‣ 2 Background
    ‣ Deep Learning for Scene Classification: A Survey")的底行展示了三种不同室内场景之间的强视觉关联，*即*档案室、书店和图书馆。随着新场景类别的出现，语义模糊问题将更加严重。此外，场景类别注释是主观的，依赖于注释者的经验，因此一个场景图像可能属于多个语义类别[[68](#bib.bib68),
    [69](#bib.bib69)]。'
- en: Computational efficiency. The prevalence of social media networks and mobile/wearable
    devices has led to increasing demands for various computer vision tasks including
    scene recognition. However, mobile/wearable devices have constrained computing
    related resources, making efficient scene recognition a pressing requirement.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 计算效率。社交媒体网络以及移动/可穿戴设备的普及导致了对各种计算机视觉任务（包括场景识别）的需求增加。然而，移动/可穿戴设备的计算相关资源受限，使得高效的场景识别成为紧迫需求。
- en: 2.2 Datasets
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 数据集
- en: 'This section reviews publicly available datasets for scene classification.
    The scene datasets (see Fig. [4](#S2.F4 "Figure 4 ‣ 2.2 Datasets ‣ 2 Background
    ‣ Deep Learning for Scene Classification: A Survey")) are broadly divided into
    two main categories based on the image type: RGB and RGB-D datasets. The datasets
    can further be divided into two categories in terms of their size. Small-size
    datasets (*e.g.,* Scene15 [[14](#bib.bib14)], MIT67 [[70](#bib.bib70)], SUN397 [[57](#bib.bib57)],
    NYUD2 [[71](#bib.bib71)], SUN RGBD [[72](#bib.bib72)]) are usually used for evaluation,
    while large-scale datasets, *e.g.,* ImageNet [[56](#bib.bib56)] and Places [[18](#bib.bib18),
    [25](#bib.bib25)], are essential for pre-training and developing deep learning
    models. Table [I](#S2.T1 "TABLE I ‣ 2.2 Datasets ‣ 2 Background ‣ Deep Learning
    for Scene Classification: A Survey") summarizes the characteristics of these datasets
    for scene classification.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '本节回顾了用于场景分类的公开数据集。这些场景数据集（见图[4](#S2.F4 "Figure 4 ‣ 2.2 Datasets ‣ 2 Background
    ‣ Deep Learning for Scene Classification: A Survey")）根据图像类型大致分为两大类：RGB 数据集和 RGB-D
    数据集。这些数据集还可以根据其规模进一步分为两类。小规模数据集（*例如*，Scene15 [[14](#bib.bib14)]、MIT67 [[70](#bib.bib70)]、SUN397
    [[57](#bib.bib57)]、NYUD2 [[71](#bib.bib71)]、SUN RGBD [[72](#bib.bib72)]) 通常用于评估，而大规模数据集（*例如*，ImageNet
    [[56](#bib.bib56)] 和 Places [[18](#bib.bib18), [25](#bib.bib25)]）对预训练和深度学习模型开发至关重要。表[I](#S2.T1
    "TABLE I ‣ 2.2 Datasets ‣ 2 Background ‣ Deep Learning for Scene Classification:
    A Survey")总结了这些数据集的场景分类特征。'
- en: '![Refer to caption](img/d4037eace28cb7798d99004d5d41427b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d4037eace28cb7798d99004d5d41427b.png)'
- en: 'Figure 4: Some example images for scene classification from benchmark datasets
    for a summary of these datasets. RGB-D images consist of RGB and a depth map.
    Moreover, Gupta et al. [[73](#bib.bib73)] proposed to convert depth image into
    three-channel feature maps, *i.e.,* Horizontal disparity, Height above the ground,
    and Angle of the surface norm (HHA). Such HHA encoding is useful for the visualization
    of depth data.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：来自基准数据集的场景分类示例图像的总结。RGB-D 图像由 RGB 和深度图组成。此外，Gupta 等人[[73](#bib.bib73)] 提出了将深度图像转换为三通道特征图，*即*水平视差、离地高度和表面法线角度（HHA）。这种
    HHA 编码对于深度数据的可视化非常有用。
- en: 'TABLE I: Popular datasets for scene classification. “#” denotes *the number
    of*.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：场景分类的流行数据集。"#" 表示 *数量*。
- en: '| Type | Dataset | #Images | #Class | Resolution | Class label |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 数据集 | 图像数量 | 类别数量 | 分辨率 | 类别标签 |'
- en: '| RGB | Scene15 [[14](#bib.bib14)] | 4,488 | 15 | $\thickapprox$ 300$\times$250
    | Indoor/outdoor scene |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| RGB | Scene15 [[14](#bib.bib14)] | 4,488 | 15 | $\thickapprox$ 300$\times$250
    | 室内/室外场景 |'
- en: '|  | MIT67 [[70](#bib.bib70)] | 15,620 | 67 | $\geq$ 200$\times$200 | Indoor
    scene |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | MIT67 [[70](#bib.bib70)] | 15,620 | 67 | $\geq$ 200$\times$200 | 室内场景
    |'
- en: '|  | SUN397 [[57](#bib.bib57)] | 108,754 | 397 | $\thickapprox$ 500$\times$300
    | Indoor/outdoor scene |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | SUN397 [[57](#bib.bib57)] | 108,754 | 397 | $\thickapprox$ 500$\times$300
    | 室内/室外场景 |'
- en: '|  | ImageNet [[17](#bib.bib17)] | 14 million+ | 21,841 | $\thickapprox$ 500$\times$400
    | Object |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | ImageNet [[17](#bib.bib17)] | 1400 万+ | 21,841 | $\thickapprox$ 500$\times$400
    | 对象 |'
- en: '|  | Places205 [[18](#bib.bib18)] | 7,076,580 | 205 | $\geq$ 200$\times$200
    | Indoor/outdoor scene |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | Places205 [[18](#bib.bib18)] | 7,076,580 | 205 | $\geq$ 200$\times$200
    | 室内/室外场景 |'
- en: '|  | Places88 [[18](#bib.bib18)] | $-$ | 88 | $\geq$ 200$\times$200 | Indoor/outdoor
    scene |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | Places88 [[18](#bib.bib18)] | $-$ | 88 | $\geq$ 200$\times$200 | 室内/室外场景
    |'
- en: '|  | Places365-S [[25](#bib.bib25)] | 1,803,460 | 365 | $\geq$ 200$\times$200
    | Indoor/outdoor scene |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | Places365-S [[25](#bib.bib25)] | 1,803,460 | 365 | $\geq$ 200$\times$200
    | 室内/室外场景 |'
- en: '|  | Places365-C [[25](#bib.bib25)] | 8 million+ | 365 | $\geq$ 200$\times$200
    | Indoor/outdoor scene |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | Places365-C [[25](#bib.bib25)] | 800 万+ | 365 | $\geq$ 200$\times$200
    | 室内/室外场景 |'
- en: '| RGB-D | NYUD2 [[71](#bib.bib71)] | 1,449 | 10 | $\thickapprox$ 640$\times$480
    | Indoor scene |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| RGB-D | NYUD2 [[71](#bib.bib71)] | 1,449 | 10 | $\thickapprox$ 640$\times$480
    | 室内场景 |'
- en: '|  | SUN RGBD [[72](#bib.bib72)] | 10,355 | 19 | $\geq$512$\times$424 | Indoor
    scene |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | SUN RGBD [[72](#bib.bib72)] | 10,355 | 19 | $\geq$512$\times$424 | 室内场景
    |'
- en: Scene15 dataset [[14](#bib.bib14)] is a small scene dataset containing 4,448
    grayscale images of 15 scene categories, *i.e.,* 5 indoor scene classes (*e.g.,*
    office, store, and kitchen) along with 10 outdoor scene classes (like suburb,
    forest, and tall building). Each class contains 210$-$410 scene images, and the
    image size is around 300$\times$250\. The dataset is divided into two splits;
    there are at least 100 images per class in the training set, and the rest are
    for testing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Scene15 数据集 [[14](#bib.bib14)] 是一个小型场景数据集，包含 4,448 张 15 个场景类别的灰度图像，即，5 个室内场景类别（例如，办公室、商店和厨房）和
    10 个室外场景类别（如郊区、森林和高楼）。每个类别包含 210 到 410 张场景图像，图像大小约为 300$\times$250。数据集被分为两个部分；训练集中每个类别至少有
    100 张图像，其余的用于测试。
- en: MIT Indoor 67 (MIT67) dataset [[70](#bib.bib70)] covers a wide range of indoor
    scenes, *e.g.,* store, public space, and leisure. MIT67 comprises 15,620 scene
    images from 67 indoor categories, where each category has about 100 images. Moreover,
    all images have a minimum resolution of 200$\times$200 pixels on the smallest
    axis. Because of the shared similarities among objects in this dataset, the classification
    of images is challenging. There are 80 and 20 images per class in the training
    and testing set, respectively.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: MIT Indoor 67 (MIT67) 数据集 [[70](#bib.bib70)] 涵盖了广泛的室内场景，例如，商店、公共空间和休闲场所。MIT67
    包含来自 67 个室内类别的 15,620 张场景图像，其中每个类别约有 100 张图像。此外，所有图像在最小轴上的分辨率至少为 200$\times$200
    像素。由于数据集中对象之间的相似性，图像分类具有挑战性。训练集中每个类别有 80 张图像，测试集中有 20 张图像。
- en: Scene UNderstanding 397 (SUN397) dataset [[57](#bib.bib57)] consists of 397
    scene categories, in which each category has more than 100 images. The dataset
    contains 108,754 images with an image size of about 500$\times$300 pixels. SUN397
    spans over 175 indoor, 220 outdoor scene classes, and two classes with mixed indoor
    and outdoor images, *e.g.,* a promenade deck with a ticket booth. There are several
    train/test split settings with 50 images per category in the testing.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Scene UNderstanding 397 (SUN397) 数据集 [[57](#bib.bib57)] 包含 397 个场景类别，其中每个类别有超过
    100 张图像。数据集包含 108,754 张图像，图像大小约为 500$\times$300 像素。SUN397 涵盖了 175 个室内场景、220 个室外场景类别，以及两个混合室内和室外图像的类别，例如，有一个带售票亭的
    promenad deck。测试时有多个训练/测试分割设置，每个类别有 50 张图像。
- en: ImageNet dataset [[56](#bib.bib56)] is one of the most famous large-scale image
    databases particularly used for visual tasks. It is organized in terms of the
    WordNet [[74](#bib.bib74)] hierarchy, each node of which is depicted by hundreds
    and thousands of images. Up to now, there are more than 14 million images and
    about 20 thousand notes in the ImageNet. Usually, a subset of ImageNet dataset
    (about 1000 categories with a total of 1.2 million images [[17](#bib.bib17)])
    is used to pre-train the CNN for scene classification.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet 数据集 [[56](#bib.bib56)] 是最著名的大规模图像数据库之一，特别用于视觉任务。它按照 WordNet [[74](#bib.bib74)]
    层级组织，其中每个节点由数百到数千张图像描绘。迄今为止，ImageNet 中有超过 1400 万张图像和大约 2 万个标签。通常，ImageNet 数据集的一个子集（约
    1000 个类别，总计 120 万张图像 [[17](#bib.bib17)]）用于对 CNN 进行场景分类的预训练。
- en: 'Places dataset [[18](#bib.bib18), [25](#bib.bib25)] is a large-scale scene
    dataset with 434 scene categories, which provides an exhaustive list of the classes
    of environments encountered in the real world. The Places dataset has inherited
    the same list of scene categories from SUN397 [[57](#bib.bib57)]. Four benchmark
    subsets of Places are shown as follows: 1) Places205 [[18](#bib.bib18)] has 2.5
    million images from scene categories. The image number per class varies from 5,000
    to 15,000\. The training set has 2,448,873 images, with 100 images per category
    for validation and 200 images per category for testing. 2) Places88 [[18](#bib.bib18)]
    contains the 88 common scene categories among the ImageNet [[56](#bib.bib56)],
    SUN397 [[57](#bib.bib57)], and Places205 datasets. Places88 includes only the
    images obtained in the second round of annotation from the Places. 3) Places365-Standard [[25](#bib.bib25)]
    has 1,803,460 training images with the image number per class varying from 3,068
    to 5,000\. The validation set has 50 images/class, while the testing set has 900
    images/class. 4) Places365-Challenge contains the same categories as the Places365-Standard,
    but its training set is significantly larger with a total of 8 million images.
    This subset was released for the Places Challenge held in conjunction with ECCV,
    as part of the ILSVRC 2016 Challenge.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Places 数据集 [[18](#bib.bib18)、[25](#bib.bib25)] 是一个大规模场景数据集，包含434个场景类别，提供了现实世界中遇到的环境类别的详尽列表。Places
    数据集继承了与SUN397 [[57](#bib.bib57)] 相同的场景类别列表。Places 数据集的四个基准子集如下所示：1) Places205
    [[18](#bib.bib18)] 包含250万张来自场景类别的图像。每类图像数量从5000到15000不等。训练集有2,448,873张图像，每个类别有100张图像用于验证，每个类别有200张图像用于测试。2)
    Places88 [[18](#bib.bib18)] 包含ImageNet [[56](#bib.bib56)]、SUN397 [[57](#bib.bib57)]
    和Places205 数据集中88个常见场景类别。Places88仅包括在第二轮标注中获得的图像。3) Places365-Standard [[25](#bib.bib25)]
    具有1,803,460张训练图像，每类图像数量从3,068到5,000不等。验证集每类50张图像，测试集每类900张图像。4) Places365-Challenge
    包含与Places365-Standard相同的类别，但其训练集显著更大，总共有800万张图像。该子集是为与ECCV联合举行的Places Challenge发布的，作为ILSVRC
    2016 Challenge的一部分。
- en: NYU-Depth V2 (NYUD2) dataset [[71](#bib.bib71)] is comprised of video sequences
    from a variety of indoor scenes as recorded by both the RGB and depth cameras.
    The dataset consists of 1,449 densely labeled pairs of aligned RGB and depth images
    from 27 indoor scene categories. It features 464 scenes taken from 3 cities and
    407,024 unlabeled frames. With the publicly available split, NYUD2 for scene classification
    offers 795 images for training while 654 images for testing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: NYU-Depth V2 (NYUD2) 数据集 [[71](#bib.bib71)] 包含来自不同室内场景的RGB和深度相机记录的视频序列。该数据集由27个室内场景类别中的1,449对对齐的RGB和深度图像组成。它包括来自3个城市的464个场景和407,024个未标记的帧。使用公开可用的拆分，NYUD2的场景分类提供795张用于训练的图像和654张用于测试的图像。
- en: SUN RGBD dataset [[72](#bib.bib72)] consists of 10,335 RGB-D images with dense
    annotations in both 2D and 3D, for both objects and rooms. The dataset is collected
    by four different sensors at a similar scale as PASCAL VOC [[75](#bib.bib75)].
    The whole dataset is densely annotated and includes 146,617 2D polygons and 58,657
    3D bounding boxes with accurate object orientations, as well as a 3D room layout
    and category for scenes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: SUN RGBD 数据集 [[72](#bib.bib72)] 包含10,335张带有2D和3D密集标注的RGB-D图像，涉及对象和房间。该数据集由四种不同的传感器在与PASCAL
    VOC [[75](#bib.bib75)] 相似的尺度下收集。整个数据集进行了密集标注，包括146,617个2D多边形和58,657个带有准确对象方向的3D边界框，以及3D房间布局和场景类别。
- en: '![Refer to caption](img/2a49c8f9b17a7226bb695b29e94589ae.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2a49c8f9b17a7226bb695b29e94589ae.png)'
- en: 'Figure 5: Generic pipeline of deep learning for scene classification. An entire
    pipeline consists of a module in each of the three stages (local feature extraction,
    feature encoding and pooling, and category prediction). The common pipelines are
    shown with arrows in different colors, including global CNN feature based pipeline
    (blue arrows), spatially invariant feature based pipeline (green arrows), and
    semantic feature based pipeline (red arrows). Although the pipeline of some methods
    (like [[31](#bib.bib31), [32](#bib.bib32)]) are unified and trained in an end-to-end
    manner, they are virtually composed of these three stages.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：场景分类的深度学习通用流程图。整个流程包括三个阶段的每个模块（局部特征提取、特征编码与池化、类别预测）。通用流程以不同颜色的箭头表示，包括基于全局CNN特征的流程（蓝色箭头）、空间不变特征的流程（绿色箭头）和语义特征的流程（红色箭头）。尽管某些方法的流程（如[[31](#bib.bib31)、[32](#bib.bib32)]）是统一的，并以端到端的方式训练，但它们实际上由这三个阶段组成。
- en: 3 Deep Learning based Methods
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于深度学习的方法
- en: 'In this section, we present a comprehensive review of deep learning methods
    for scene classification. A brief introduction of deep learning is provided in
    the appendix due to limit space. The most common deep learning architecture is
    Convolutional Neural Network (CNN) [[76](#bib.bib76)]. With CNN as feature extractor,
    Fig. [5](#S2.F5 "Figure 5 ‣ 2.2 Datasets ‣ 2 Background ‣ Deep Learning for Scene
    Classification: A Survey") shows the generic pipeline of most CNN based methods
    for scene classification. Almost without exception, given an input scene image,
    the first stage is to use CNN extractors to obtain local features. Then, the second
    process is to aggregate these features into an image-level representation via
    encoding, concatenating, or pooling. Finally, with the representation as input,
    the classification stage is to get a predicted category.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们提供了对场景分类的深度学习方法的全面回顾。由于空间限制，附录中提供了深度学习的简要介绍。最常见的深度学习架构是卷积神经网络（CNN）[[76](#bib.bib76)]。以CNN作为特征提取器，图[5](#S2.F5
    "Figure 5 ‣ 2.2 Datasets ‣ 2 Background ‣ Deep Learning for Scene Classification:
    A Survey")展示了大多数基于CNN的场景分类方法的通用流程。几乎没有例外，给定一个输入场景图像，第一阶段是使用CNN提取器获取局部特征。然后，第二个过程是通过编码、连接或池化将这些特征汇总成图像级别的表示。最后，以该表示作为输入，分类阶段则是得到预测类别。'
- en: 'The taxonomy, shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning
    for Scene Classification: A Survey"), covers different aspects of deep learning
    for scene classification. In the following investigation, we firstly study the
    main CNN frameworks for scene classification. Then, we review existing CNN based
    scene representations. Furthermore, we explore various techniques for improving
    the obtained representations. Finally, as a supplement, we investigate scene classification
    using RGB-D data.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '图[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning for Scene Classification:
    A Survey")展示的分类法涵盖了场景分类的深度学习的不同方面。在接下来的调查中，我们首先研究场景分类的主要CNN框架。然后，我们回顾现有的基于CNN的场景表示。此外，我们探索了提高获得的表示的各种技术。最后，作为补充，我们研究了使用RGB-D数据进行场景分类。'
- en: 3.1 Main CNN Framework
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 主要的CNN框架
- en: Convolutional Neural Networks (CNNs) are common deep learning models to extract
    high quality representation. At the beginning, limited by computing resources
    and labeled data, scene features are extracted from pre-trained CNNs, which is
    usually combined to BoVW pipeline [[6](#bib.bib6)]. Then, fine-tuned CNN models
    are used to keep last layers more data-specific. Alternatively, specific CNN models
    have emerged to adapt to scene attributes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）是提取高质量表示的常见深度学习模型。最初，由于计算资源和标记数据的限制，场景特征从预训练的CNN中提取，这通常与BoVW管道结合使用[[6](#bib.bib6)]。随后，细化的CNN模型用于保持最后几层更具数据特异性。或者，特定的CNN模型已经出现，以适应场景属性。
- en: 3.1.1 Pre-trained CNN Model
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 预训练CNN模型
- en: The network architecture plays a pivotal role in the performance of deep models.
    In the beginning, AlexNet [[17](#bib.bib17)] served as the mainstream CNN model
    for feature representation and classification purposes. Later, Simonyan et al. [[77](#bib.bib77)]
    developed VGGNet and showed that, for a given receptive field, using multiple
    stacked small kernels is better than using a large convolution kernel, because
    applying non-linearity on multiple feature maps yields more discriminative representations.
    On the other hand, the reduction of kernels’ receptive filed size decreases the
    number of parameters for bigger networks. Therefore, VGGNet has 3$\times$3 convolution
    kernels instead of large convolution kernels (*i.e.,* 11$\times$11, 7$\times$7,
    and 5$\times$5) in AlexNet. Motivated by the idea that only a handful of neurons
    have an effective role in feature representation, Szegedy et al. [[78](#bib.bib78)]
    proposed an Inception module to make a sparse approximation of CNNs. Deeper the
    model, the more descriptive representations. This is the advantage of hierarchical
    feature extraction using CNN. However, constantly increasing CNN’s depth could
    result in gradient vanishing. To address this issue, He et al. [[79](#bib.bib79)]
    included skip connection to the hierarchical structure of CNN and proposed Residual
    Networks (ResNets), which are easier to optimize and can gain accuracy from considerably
    increased depth.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 网络架构在深度模型的性能中扮演着关键角色。最初，AlexNet [[17](#bib.bib17)] 作为主流的CNN模型用于特征表示和分类。后来，Simonyan
    等人 [[77](#bib.bib77)] 开发了VGGNet，并展示了在给定感受野的情况下，使用多个叠加的小卷积核比使用大卷积核效果更好，因为在多个特征图上应用非线性可以获得更具区分性的表示。另一方面，卷积核感受野尺寸的减少会降低大网络的参数数量。因此，VGGNet
    使用了 3$\times$3 卷积核，而不是 AlexNet 中的大卷积核（*即*，11$\times$11，7$\times$7 和 5$\times$5）。受限于只有少量神经元在特征表示中发挥有效作用的理念，Szegedy
    等人 [[78](#bib.bib78)] 提出了Inception模块来进行CNN的稀疏近似。模型越深，描述性表示越多。这就是使用CNN进行分层特征提取的优势。然而，不断增加CNN的深度可能导致梯度消失。为了解决这个问题，He
    等人 [[79](#bib.bib79)] 在CNN的层次结构中加入了跳跃连接，并提出了残差网络（ResNets），这些网络更易于优化，并且可以从显著增加的深度中获得更高的准确性。
- en: In addition to the network architecture, the performance of CNN interwinds with
    a sufficiently large amount of training data. However, the training data are scarce
    in certain applications, which results in the under-fitting of the model during
    the training process. To overcome this issue, pre-trained models can be employed
    to effectively extract feature representations of small datasets [[80](#bib.bib80)].
    Training CNN on large-scale datasets, such as the ImageNet [[56](#bib.bib56)]
    and the Places [[18](#bib.bib18), [25](#bib.bib25)], makes them learn enriched
    visual representations. Such models can further be used as pre-trained models
    for other tasks. However, the effectiveness of the employment of pre-trained models
    largely depends on the similarity between the source and target domains. Yosinski
    et al. [[81](#bib.bib81)] documented that the transferability of pre-trained CNN
    models decreases as the similarity of the target task and original source task
    decreases. Nevertheless, pre-trained models still have better performance than
    random initialization of the models [[81](#bib.bib81)].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了网络架构，CNN的性能还与足够大的训练数据量密切相关。然而，在某些应用中，训练数据稀缺，这导致模型在训练过程中出现欠拟合。为了解决这个问题，可以使用预训练模型来有效地提取小数据集的特征表示 [[80](#bib.bib80)]。在大规模数据集上训练CNN，例如ImageNet [[56](#bib.bib56)]
    和 Places [[18](#bib.bib18), [25](#bib.bib25)]，使其能够学习丰富的视觉表示。这些模型可以进一步用作其他任务的预训练模型。然而，预训练模型的有效性在很大程度上依赖于源领域和目标领域之间的相似性。Yosinski
    等人 [[81](#bib.bib81)] 记录了预训练CNN模型的迁移能力随着目标任务和原始源任务之间的相似性减少而下降。尽管如此，预训练模型仍然比模型的随机初始化表现更好 [[81](#bib.bib81)]。
- en: 'Pre-trained CNNs, as fixed feature extractors, are divided into two categories:
    object-centric and scene-centric CNNs. Object-centric CNNs refer to the model
    pre-trained on object datasets, *e.g.,* the ImageNet [[56](#bib.bib56)], and deployed
    for scene classification. Since object images do not contain the diversity provided
    by the scene [[18](#bib.bib18)], object-centric CNNs have limited performance
    for scene classification. Hence, scene-centric CNNs, pre-trained on scene images,
    like Places [[18](#bib.bib18), [25](#bib.bib25)], are more effective to extract
    scene-related features.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的 CNN 作为固定特征提取器，分为两类：以物体为中心的和以场景为中心的 CNN。以物体为中心的 CNN 是指在物体数据集上预训练的模型，例如 ImageNet
    [[56](#bib.bib56)]，并用于场景分类。由于物体图像没有场景提供的多样性 [[18](#bib.bib18)]，以物体为中心的 CNN 在场景分类上的表现有限。因此，在场景图像上预训练的场景中心
    CNN，如 Places [[18](#bib.bib18), [25](#bib.bib25)]，在提取与场景相关的特征方面更为有效。
- en: Object-centric CNNs. Cimpoi et al. [[82](#bib.bib82)] asserted that the feature
    representations obtained from object-centric CNNs are object descriptors since
    they have likely more object descriptive properties. The scene image is represented
    as a bag of semantics [[30](#bib.bib30)], and object-centric CNNs are sensitive
    to the overall shape of objects, so many methods [[28](#bib.bib28), [83](#bib.bib83),
    [30](#bib.bib30), [82](#bib.bib82), [31](#bib.bib31)] used object-centric CNNs
    to extract local features from different regions of the scene image. Another important
    factor in the effective deployment of object-centric CNNs is the relational size
    of images in the source and target datasets. Although CNNs are generally robust
    against size and scale, the performance of object-centric CNNs is influenced by
    scaling because such models are originally pre-trained on datasets to detect and/or
    recognize objects. Therefore, the shift to describing scenes, which have multiple
    objects with different scales, would drastically affect their performance [[19](#bib.bib19)].
    For instance, if the image size of the target dataset is smaller than the source
    dataset to a certain degree, the accuracy of the model would be compromised.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以物体为中心的 CNN。Cimpoi 等人 [[82](#bib.bib82)] 认为，从以物体为中心的 CNN 获得的特征表示是物体描述符，因为它们可能具有更多的物体描述特性。场景图像被表示为语义的集合
    [[30](#bib.bib30)]，而以物体为中心的 CNN 对物体的整体形状非常敏感，因此许多方法 [[28](#bib.bib28), [83](#bib.bib83),
    [30](#bib.bib30), [82](#bib.bib82), [31](#bib.bib31)] 使用以物体为中心的 CNN 从场景图像的不同区域提取局部特征。有效部署以物体为中心的
    CNN 的另一个重要因素是源数据集和目标数据集中的图像相对大小。尽管 CNN 通常对大小和尺度具有鲁棒性，但以物体为中心的 CNN 的性能受到缩放的影响，因为这些模型最初是在数据集上预训练以检测和/或识别物体。因此，转向描述具有不同尺度的多个物体的场景将严重影响它们的性能
    [[19](#bib.bib19)]。例如，如果目标数据集的图像尺寸小于源数据集，则模型的准确性将受到影响。
- en: Scene-centric CNNs. Zhou et al. [[18](#bib.bib18), [25](#bib.bib25)] demonstrated
    the classification performance of scene-centric CNNs is better than object-centric
    CNNs since the former use the prior knowledge of the scene. Herranz et al. [[19](#bib.bib19)]
    found that Places-CNNs [[23](#bib.bib23)] achieve better performance at larger
    scales; therefore, scene-centric CNNs generally extract the representations in
    the whole range of scales. Guo et al. [[40](#bib.bib40)] noticed that the CONV
    layers of scene-centric CNNs capture more detail information of a scene, such
    as local semantic regions and fine-scale objects, which is crucial to discriminate
    the ambiguous scenes, while the feature representations obtained from the FC layers
    do not convey such perceptive quality. Zhou et al. [[84](#bib.bib84)] showed that
    scene-centric CNNs may also perform as object detectors without explicitly being
    trained on object datasets.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以场景为中心的 CNN。Zhou 等人 [[18](#bib.bib18), [25](#bib.bib25)] 证明了以场景为中心的 CNN 在分类性能上优于以物体为中心的
    CNN，因为前者利用了场景的先验知识。Herranz 等人 [[19](#bib.bib19)] 发现 Places-CNNs [[23](#bib.bib23)]
    在较大尺度下表现更好；因此，场景中心的 CNN 通常能够在整个尺度范围内提取表示。Guo 等人 [[40](#bib.bib40)] 发现场景中心的 CNN
    的 CONV 层捕获了更多场景的细节信息，如局部语义区域和细尺度物体，这对于区分模糊场景至关重要，而从 FC 层获得的特征表示则没有这种感知质量。Zhou
    等人 [[84](#bib.bib84)] 显示，场景中心的 CNN 也可以作为物体检测器，而无需在物体数据集上明确训练。
- en: 3.1.2 Fine-tuned CNN Model
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 微调的 CNN 模型
- en: 'Pre-trained CNNs, described in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Pre-trained
    CNN Model ‣ 3.1 Main CNN Framework ‣ 3 Deep Learning based Methods ‣ Deep Learning
    for Scene Classification: A Survey"), perform as feature extractor with prior
    knowledge of the training data [[6](#bib.bib6), [85](#bib.bib85)]. However, using
    only the pre-training strategy would prevent exploiting the full capability of
    the deep models in describing the target scenes adaptively. Hence, fine-tuning
    the pre-trained CNNs using the target scene dataset improves their performance
    by reducing the possible domain shift between two datasets [[85](#bib.bib85)].
    Notably, a suitable weight initialization becomes very important, because it is
    quite difficult to train a model with many adjustable parameters and non-convex
    loss functions [[86](#bib.bib86)]. Therefore, fine-tuning the pre-trained CNN
    contributes to the effective training process [[87](#bib.bib87), [30](#bib.bib30),
    [29](#bib.bib29), [34](#bib.bib34)].'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '预训练的CNN，如第[3.1.1](#S3.SS1.SSS1 "3.1.1 Pre-trained CNN Model ‣ 3.1 Main CNN
    Framework ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification:
    A Survey")节所述，作为特征提取器，具备训练数据的先验知识[[6](#bib.bib6), [85](#bib.bib85)]。然而，仅使用预训练策略会阻止深度模型在描述目标场景时充分发挥其适应性能力。因此，使用目标场景数据集对预训练CNN进行微调可以通过减少两个数据集之间可能的领域偏移来提高其性能[[85](#bib.bib85)]。值得注意的是，合适的权重初始化变得非常重要，因为训练具有许多可调参数和非凸损失函数的模型是相当困难的[[86](#bib.bib86)]。因此，微调预训练的CNN有助于有效的训练过程[[87](#bib.bib87),
    [30](#bib.bib30), [29](#bib.bib29), [34](#bib.bib34)]。'
- en: For CNNs, a common fine-tuning technique is the *freeze strategy*. In this method,
    the last FC layer of a pretrained model is replaced with a new FC layer with the
    same number of neurons as the classes in the target dataset (*i.e.,* MIT67, SUN397),
    while the previous CONV layers’ parameters are frozen, *i.e.,* they are not updated
    during fine-tuning. Then, this modified CNN is fine-tuned by training on the target
    dataset. Herein, the back-propagation is stopped after the last FC layers, which
    allows these layers to extract discriminative features from the previous learned
    layers. Through updating few parameters, training a complex model using small
    datasets would be affordable. Optionally, it is also possible to gradually unfreeze
    some layers to further enhance the learning quality as the earlier layers would
    adapt new representations from the target dataset. Alternatively, different learning
    rates could be assigned to different layers of CNN, in which the early layers
    of the model have very low learning rate and the last layers have higher learning
    rates. In this way, the early CONV layers that have more abstract representations
    are less affected, while the specialized FC layers are fine-tuned with higher
    speed.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于卷积神经网络（CNNs），一种常见的微调技术是*冻结策略*。在这种方法中，用一个新的全连接（FC）层替换预训练模型的最后一个FC层，该新的FC层具有与目标数据集中的类数（*即，*
    MIT67、SUN397）相同的神经元数量，同时之前的卷积（CONV）层的参数被冻结，*即，* 在微调过程中不会更新。然后，通过在目标数据集上进行训练来对这个修改过的CNN进行微调。在这里，反向传播在最后一个FC层之后停止，这允许这些层从之前学习的层中提取判别特征。通过更新少量参数，使用小数据集训练复杂模型是可行的。可选地，也可以逐渐解冻某些层，以进一步提升学习质量，因为早期层将从目标数据集中适应新的表示。或者，可以为CNN的不同层分配不同的学习率，其中模型的早期层具有非常低的学习率，而最后的层具有较高的学习率。这样，具有更多抽象表示的早期CONV层受到的影响较小，而专门的FC层则以更高的速度进行微调。
- en: Small-size training dataset limits the effective fine-tuning process, while
    *data augmentation* is one alternative to deal with this issue [[88](#bib.bib88),
    [20](#bib.bib20), [21](#bib.bib21), [48](#bib.bib48)]. Liu et al. [[85](#bib.bib85)]
    indicated that deep models may not benefit from fine-tuning on a small target
    dataset. In addition, fine-tuning may have negative effects since the specialized
    FC layers are changed while inadequate training data are provided for fine-tuning.
    To this end, Khan et al. [[20](#bib.bib20)] augmented the scene image dataset
    with flipped, cropped, and rotated versions to increase the size of the dataset
    and further improve the robustness of the learned representations. Liu et al. [[21](#bib.bib21)]
    proposed a method to select representative image patches of the original image.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 小规模的训练数据集限制了有效的微调过程，而*数据增强*是应对这一问题的一个替代方案[[88](#bib.bib88), [20](#bib.bib20),
    [21](#bib.bib21), [48](#bib.bib48)]。Liu等人[[85](#bib.bib85)] 指出，深度模型可能不会从小型目标数据集的微调中受益。此外，微调可能会产生负面效果，因为在微调过程中专门的全连接层被更改，而提供的训练数据不足。为此，Khan等人[[20](#bib.bib20)]
    通过翻转、裁剪和旋转版本来增强场景图像数据集，以增加数据集的大小并进一步提高学习到的表示的鲁棒性。Liu等人[[21](#bib.bib21)] 提出了选择原始图像的代表性图像补丁的方法。
- en: There exists a problem via data augmentation to fine-tune CNNs for scene classification.
    Herranz et al. [[19](#bib.bib19)] asserted that fine-tuning a CNN model have certain
    “equalizing” effect between the input patch scale and final accuracy, *i.e.,*
    to some extent, with too small patches as CNN inputs, the final classification
    accuracy is worse. This is because the small patch inputs contain insufficient
    image information, while the final labels indicate scene categories [[32](#bib.bib32),
    [89](#bib.bib89)]. Moreover, the number of cropped patches is huge, so just a
    tiny part of these patches is used to fine-tune CNN models, rendering limited
    overall improvement [[19](#bib.bib19)]. On the other hand, Herranz et al. [[19](#bib.bib19)]
    also explored the effect of fine-tuning CNNs on different scales, *i.e.,* with
    different scale patches as inputs. From the practical results, there is a moderate
    accuracy gain in the range of scale patches where the original CNNs perform poorly,
    *e.g.,* in the cases of global scales for ImageNet-CNN and local scales for Places-CNN.
    However, there is marginal or no gain in ranges where CNN have already strong
    performance. For example, since Places-CNN has the best performance in the whole
    range of scale patches, in this case, fine-tuning on target dataset leads to negligible
    performance improvement.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强存在一个问题，即通过对CNN进行微调来进行场景分类。Herranz等人[[19](#bib.bib19)] 认为，微调CNN模型在输入补丁尺度和最终准确性之间具有一定的“平衡”效果，*即，*在一定程度上，若CNN输入的补丁过小，最终的分类准确性会降低。这是因为小的补丁输入包含的图像信息不足，而最终标签则指示场景类别[[32](#bib.bib32),
    [89](#bib.bib89)]。此外，裁剪补丁的数量巨大，因此仅使用这些补丁的一小部分来微调CNN模型，导致整体提升有限[[19](#bib.bib19)]。另一方面，Herranz等人[[19](#bib.bib19)]
    还探索了在不同尺度下微调CNN的效果，*即，*使用不同尺度的补丁作为输入。从实际结果来看，在原始CNN表现不佳的尺度范围内，存在适度的准确性提升，例如，对于ImageNet-CNN的全局尺度和Places-CNN的局部尺度。然而，在CNN已经表现出强劲性能的范围内，几乎没有提升。例如，由于Places-CNN在整个尺度范围内表现最佳，因此在这种情况下，在目标数据集上进行微调不会显著提升性能。
- en: 3.1.3 Specific CNN Model
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 特定CNN模型
- en: '![Refer to caption](img/34b3370671c0014b0178cb52f370d1a8.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/34b3370671c0014b0178cb52f370d1a8.png)'
- en: 'Figure 6: Illustrations of three typical specific CNN models. (a) In GAP-CNN [[23](#bib.bib23)],
    to reduce parameters of the standard CNN, FC layers are removed, and GAP layer
    is introduced to form Class Activation Maps (CAMs). (b) In DL-CNN [[22](#bib.bib22)],
    to reduce parameters of CNN model and obtain enhanced sparse features, Dictionary
    Learning (DL) layers are proposed to replace FC layers. (c) In CFA [[24](#bib.bib24)],
    Sun et al. bypassed four directional LSTM layers on the CONV maps to capture contextual
    information.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：三种典型特定CNN模型的示意图。(a) 在GAP-CNN[[23](#bib.bib23)]中，为了减少标准CNN的参数，去除了全连接层，引入了GAP层以形成类别激活图（CAMs）。(b)
    在DL-CNN[[22](#bib.bib22)]中，为了减少CNN模型的参数并获得增强的稀疏特征，提出了字典学习（DL）层来替代全连接层。(c) 在CFA[[24](#bib.bib24)]中，Sun等人绕过了CONV图上的四个方向LSTM层以捕捉上下文信息。
- en: 'In addition to the generic CNN models, *i.e.,* pre-trained CNN models and the
    fine-tuned CNN models, another group of deep models are specifically designed
    for scene classification. These models are specifically developed to extract effective
    scene representations from the input by introducing new network architectures.
    As is shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.3 Specific CNN Model ‣ 3.1 Main
    CNN Framework ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification:
    A Survey"), we only show four typical specific models [[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [26](#bib.bib26)].'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '除了通用的CNN模型，如**预训练CNN模型**和**微调CNN模型**，另一组深度模型是专门为场景分类设计的。这些模型通过引入新的网络架构，专门开发用于从输入中提取有效的场景表示。如图[6](#S3.F6
    "Figure 6 ‣ 3.1.3 Specific CNN Model ‣ 3.1 Main CNN Framework ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey")所示，我们只展示了四种典型的具体模型[[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [26](#bib.bib26)]。'
- en: 'To capture discriminative information from regions of interest, Zhou et al. [[23](#bib.bib23)]
    replaced the FC layers in a CNN model with a Global Average Pooling (GAP) layer [[90](#bib.bib90)]
    followed by a Softmax layer, *i.e.,* GAP-CNN. As shown in Fig. [6](#S3.F6 "Figure
    6 ‣ 3.1.3 Specific CNN Model ‣ 3.1 Main CNN Framework ‣ 3 Deep Learning based
    Methods ‣ Deep Learning for Scene Classification: A Survey") (a), by a simple
    combination of the original GAP layer and the $1\times 1$ convolution operation
    to form a class activation map (CAM), GAP-CNN can focus on class-specific regions
    and perform scene classification well. Although the GAP layer has a lower number
    of parameters than the FC layer [[23](#bib.bib23), [48](#bib.bib48)], the GAP-CNN
    can obtain comparable classification accuracy.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '为了从感兴趣区域捕捉辨别信息，Zhou等人[[23](#bib.bib23)]用一个全局平均池化（GAP）层[[90](#bib.bib90)]替换了CNN模型中的全连接（FC）层，接着是一个Softmax层，即GAP-CNN。如图[6](#S3.F6
    "Figure 6 ‣ 3.1.3 Specific CNN Model ‣ 3.1 Main CNN Framework ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey") (a)所示，通过将原始GAP层与$1\times
    1$卷积操作简单组合形成类激活图（CAM），GAP-CNN可以专注于特定类别区域，并良好地执行场景分类。尽管GAP层的参数数量低于FC层[[23](#bib.bib23),
    [48](#bib.bib48)]，但GAP-CNN可以获得可比的分类准确性。'
- en: 'Hypothesizing that a certain amount of sparsity improves the discriminability
    of the feature representations [[91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)],
    Liu et al. [[22](#bib.bib22)] proposed a sparsity model named Dictionary Learning
    CNN (DL-CNN), seen in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.3 Specific CNN Model ‣ 3.1
    Main CNN Framework ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification:
    A Survey") (b). They replaced FC layers with new dictionary learning layers, which
    are composed of a finite number of recurrent units that correspond to iteration
    processes in the Approximate Message Passing [[94](#bib.bib94)]. In particular,
    these dictionary learning layers’ parameters are updated through back-propagation
    in an end-to-end manner.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '假设一定程度的稀疏性能提高特征表示的辨别能力[[91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)]，Liu等人[[22](#bib.bib22)]提出了一种名为字典学习CNN（DL-CNN）的稀疏性模型，如图[6](#S3.F6
    "Figure 6 ‣ 3.1.3 Specific CNN Model ‣ 3.1 Main CNN Framework ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey") (b)所示。他们用新的字典学习层替换了FC层，这些层由有限数量的递归单元组成，对应于近似消息传递中的迭代过程[[94](#bib.bib94)]。特别地，这些字典学习层的参数通过端到端的反向传播进行更新。'
- en: 'Since the CONV layers perform local operations on small patches of the image,
    they are not able to explicitly describe the contextual relation between different
    regions of the scene image. To address this limitation, Sun et al. [[24](#bib.bib24)]
    proposed Contextual features in Appearance (CFA) based on LSTM [[95](#bib.bib95)].
    As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.3 Specific CNN Model ‣ 3.1 Main CNN
    Framework ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification:
    A Survey") (c), CONV feature maps are regarded as the input of LSTM layers, which
    is transformed into four directed sequences in an acyclic way. Finally, LSTM layers
    are used to describe spatial contextual dependencies, and the output of four LSTM
    modules are concatenated to describe contextual relations in appearance.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '由于CONV层对图像的小块进行局部操作，它们无法明确描述场景图像中不同区域之间的上下文关系。为了解决这一限制，Sun等人[[24](#bib.bib24)]提出了基于LSTM[[95](#bib.bib95)]的外观中的上下文特征（CFA）。如图[6](#S3.F6
    "Figure 6 ‣ 3.1.3 Specific CNN Model ‣ 3.1 Main CNN Framework ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey") (c)所示，CONV特征图被视为LSTM层的输入，这些输入以无环的方式转换成四个有向序列。最终，LSTM层用于描述空间上下文依赖关系，并将四个LSTM模块的输出连接起来，以描述外观中的上下文关系。'
- en: Sequential operations of CONV and FC layers in standard CNNs retain the global
    spatial structure of the image, which shows global features are sensitive to geometrical
    variations [[28](#bib.bib28), [96](#bib.bib96)], *e.g.,* object translations and
    rotation directly affect the obtained deep features, which drastically limits
    the application of these features for scene classification. To achieve geometric
    invariance, Hayat et al. [[26](#bib.bib26)] designed a spatial unstructured layer
    via shuffling the original position of the original feature maps by swapping adjacent
    diagonal image blocks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 标准CNN中CONV和FC层的序列操作保留了图像的全局空间结构，这表明全局特征对几何变化敏感[[28](#bib.bib28), [96](#bib.bib96)]，*例如，*物体的平移和旋转直接影响获取的深度特征，这极大地限制了这些特征在场景分类中的应用。为了实现几何不变性，Hayat等人[[26](#bib.bib26)]设计了一种空间非结构化层，通过交换相邻的对角图像块来打乱原始特征图的原始位置。
- en: Back-propagation algorithm is the essence of CNN training. It is the practice
    of fine-tuning the weights of a neural net based on the error rate (*i.e.,* loss)
    obtained in the previous epoch (*i.e.,* iteration). Proper tuning of the weights
    ensures lower error rates, making the model reliable by increasing its generalization.
    Therefore, many approaches [[67](#bib.bib67), [96](#bib.bib96), [34](#bib.bib34),
    [37](#bib.bib37)] developed new layers with parameters that can be updated via
    back-propagation. The end-to-end system is trained via back-propagation in a holistic
    manner, which has been proved as a powerful training manner in various domains,
    and scene classification is no exception. Many methods [[67](#bib.bib67), [34](#bib.bib34),
    [31](#bib.bib31), [96](#bib.bib96), [37](#bib.bib37)] are training in an end-to-end
    manner. According to our investigations, theoretically, these models can learn
    more discriminative information through end-to-end optimization; however, the
    optimization results may fall into bad local optima [[97](#bib.bib97), [96](#bib.bib96)],
    so methods training in a multi-stage manner may achieve better results in some
    case.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法是CNN训练的精髓。它是根据前一轮（*即，*迭代）获得的错误率（*即，*损失）来微调神经网络权重的实践。权重的正确调整能确保较低的错误率，通过提高模型的泛化能力使其更加可靠。因此，许多方法[[67](#bib.bib67),
    [96](#bib.bib96), [34](#bib.bib34), [37](#bib.bib37)]开发了可以通过反向传播更新的具有参数的新层。端到端系统通过反向传播以整体方式进行训练，这已被证明是各种领域中强大的训练方式，场景分类也不例外。许多方法[[67](#bib.bib67),
    [34](#bib.bib34), [31](#bib.bib31), [96](#bib.bib96), [37](#bib.bib37)]以端到端的方式进行训练。根据我们的调查，从理论上讲，这些模型可以通过端到端优化学习到更多的判别信息；然而，优化结果可能会陷入不佳的局部最优[[97](#bib.bib97),
    [96](#bib.bib96)]，因此以多阶段方式进行训练的方法在某些情况下可能取得更好的结果。
- en: 3.2 CNN based Scene Representation
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于CNN的场景表示
- en: 'Scene representation, the core of scene classification, has been the focus
    of this research. Hence, many methods have been put forward for effective scene
    representations, broadly divided into five categories: global CNN features, spatially
    invariant features, semantic features, multi-layer features, and combined features,
    *i.e.,* multi-view features.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 场景表示是场景分类的核心，一直是这项研究的重点。因此，许多方法已被提出用于有效的场景表示，广泛分为五类：全局CNN特征、空间不变特征、语义特征、多层特征和组合特征，即多视角特征。
- en: '![Refer to caption](img/7077fa6ad1ab73b0500100d5fedf5876.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7077fa6ad1ab73b0500100d5fedf5876.png)'
- en: 'Figure 7: Five typical architectures to extract CNN based scene representations
    (see Section [3.2](#S3.SS2 "3.2 CNN based Scene Representation ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey")), respectively.
    Hourglass architectures are backbone networks, such as AlexNet or VGGNet. (a)
    HLSTM [[27](#bib.bib27)], a global CNN feature based method, extracts deep feature
    from the whole image. Spatial LSTM is used to model 2D characteristics among the
    spatial layout of image regions. Moreover, Zuo et al. captured cross-scale contextual
    dependencies via multiple LSTM layers. (b) SFV [[30](#bib.bib30)], a spatially
    invariant feature based method, extract local features from dense patches. The
    highlight of SFV is to add a natural parameterization to transform the semantic
    space into a natural parameter space. (c) WELDON [[34](#bib.bib34)], a semantic
    feature based method, extracts deep features from top evidence (red) and negative
    instances (yellow). In WSP scheme, Durand et al. used the max layer and min layer
    to select positive and negative instances, respectively. (d) FTOTLM [[21](#bib.bib21)],
    a typical multi-layer feature based method, extracts deep feature from each residual
    block. (e) Scale-specific network [[19](#bib.bib19)], a multi-view feature based
    architecture, used scene-centric CNN extract deep features from coarse versions,
    while object-centric CNN is used to extract features from fine patches. Two types
    of deep features complement each other.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：五种典型的架构用于提取基于 CNN 的场景表示（见第 [3.2](#S3.SS2 "3.2 CNN 基于场景表示 ‣ 3 深度学习方法 ‣ 深度学习在场景分类中的应用：综述")
    节），分别展示。Hourglass 架构是骨干网络，如 AlexNet 或 VGGNet。（a）HLSTM [[27](#bib.bib27)]，一种基于全球
    CNN 特征的方法，从整个图像中提取深层特征。空间 LSTM 用于建模图像区域空间布局中的 2D 特性。此外，Zuo 等人通过多个 LSTM 层捕获跨尺度的上下文依赖。（b）SFV [[30](#bib.bib30)]，一种空间不变特征方法，从密集的补丁中提取局部特征。SFV
    的亮点在于添加了自然参数化，将语义空间转换为自然参数空间。（c）WELDON [[34](#bib.bib34)]，一种语义特征方法，从顶级证据（红色）和负实例（黄色）中提取深层特征。在
    WSP 方案中，Durand 等人使用最大层和最小层分别选择正实例和负实例。（d）FTOTLM [[21](#bib.bib21)]，一种典型的多层特征方法，从每个残差块中提取深层特征。（e）特定尺度网络 [[19](#bib.bib19)]，一种多视图特征架构，使用以场景为中心的
    CNN 从粗略版本中提取深层特征，同时使用以对象为中心的 CNN 从细节补丁中提取特征。两种类型的深层特征相互补充。
- en: 3.2.1 Global CNN feature based Method
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 基于全球 CNN 特征的方法
- en: Global CNN feature based methods directly predict the probabilities of scene
    categories from the whole scene image. Frequently, global CNN features are extracted
    from input images via generic CNN models, pre-trained on large-scale datasets
    (*e.g.,* ImageNet [[56](#bib.bib56)] and Places [[18](#bib.bib18), [25](#bib.bib25)]),
    or then fine-tuned on target datasets (*e.g.,* SUN397 [[57](#bib.bib57)] and MIT67 [[70](#bib.bib70)]).
    Owing to the available large datasets and powerful computing resources (e.g.,
    GPUs and parallel computing clusters) [[98](#bib.bib98)], deep networks have been
    developed into deeper and more complicated, and thus global representations from
    these networks are able to achieve a more advanced performance on many applications
    including scene classification.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基于全球 CNN 特征的方法直接从整个场景图像中预测场景类别的概率。通常，全球 CNN 特征是通过在大规模数据集（*例如*，ImageNet [[56](#bib.bib56)]
    和 Places [[18](#bib.bib18), [25](#bib.bib25)]）上预训练的通用 CNN 模型从输入图像中提取的，或者在目标数据集（*例如*，SUN397 [[57](#bib.bib57)]
    和 MIT67 [[70](#bib.bib70)]）上进行微调的。由于可用的大型数据集和强大的计算资源（*例如*，GPU 和并行计算集群）[[98](#bib.bib98)]，深度网络已经发展得更加深层和复杂，因此这些网络中的全球表示能够在许多应用中，包括场景分类，达到更先进的性能。
- en: 'Except for generic CNNs, some scene-specific CNNs are designed to extract global
    features. For instance, as shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based
    Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene
    Classification: A Survey") (a), Zuo et al. [[27](#bib.bib27)] proposed Hierarchical
    LSTM (HLSTM) to describe the contextual relation. They treated CONV maps as an
    undirected graph, which is transformed into four directed acyclic graphs, and
    LSTM modules are used to capture spatial contextual dependencies in an acyclic
    way. They also explored the potential spatial dependencies among different scale
    CONV maps, so HLSTM features not only involve the relations within the same feature
    maps but also contain the contextual dependencies among different scales. In addition,
    Liu et al. [[22](#bib.bib22)] proposed DL-CNN model to extract sparse global features
    from entire scene image. Xie et al. [[99](#bib.bib99)] presented InterActive,
    a novel global CNN feature extraction algorithm which integrates high-level visual
    context with low-level neuron responses. InterActive increases the receptive field
    size of low-level neurons by allowing the supervision of the high-level neurons.
    Hayat et al. [[26](#bib.bib26)] designed a spatial unstructured layer to address
    the challenges of large-scale spatial layout deformations and scale variations.
    Along this way, Xie et al. [[100](#bib.bib100)] designed a Reversal Invariant
    Convolution (RI-Conv) layer so that they can obtain the identical representation
    for an image and its left-right reversed copy. Nevertheless, global CNN feature
    based methods have not fully exploited the underlying geometric and appearance
    variability of scene images.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '除了通用的卷积神经网络（CNN），一些特定场景的CNN也被设计用来提取全局特征。例如，如图 [7](#S3.F7 "Figure 7 ‣ 3.2 CNN
    based Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning for
    Scene Classification: A Survey") (a) 所示，Zuo 等人 [[27](#bib.bib27)] 提出了层次化LSTM（HLSTM）来描述上下文关系。他们将CONV图像视为无向图，并将其转换为四个有向无环图，LSTM模块用于以无环方式捕捉空间上下文依赖关系。他们还探索了不同尺度CONV图像之间的潜在空间依赖关系，因此HLSTM特征不仅涉及同一特征图内的关系，还包含不同尺度之间的上下文依赖关系。此外，Liu
    等人 [[22](#bib.bib22)] 提出了DL-CNN模型来从整个场景图像中提取稀疏的全局特征。Xie 等人 [[99](#bib.bib99)]
    提出了InterActive，这是一种新颖的全局CNN特征提取算法，它将高层视觉上下文与低层神经元响应结合起来。InterActive通过允许高层神经元的监督，增加了低层神经元的感受野大小。Hayat 等人 [[26](#bib.bib26)]
    设计了一个空间无结构层，以解决大规模空间布局变形和尺度变化的挑战。在这方面，Xie 等人 [[100](#bib.bib100)] 设计了一个反转不变卷积（RI-Conv）层，使他们能够获得图像及其左右翻转副本的相同表示。然而，基于全局CNN特征的方法尚未充分利用场景图像的潜在几何和外观变异性。'
- en: The performance of global CNN features is greatly affected by the content of
    the input image. CNN models can extract generic global feature representations
    once trained on a sufficiently large and rich training dataset, as opposed to
    handcrafted feature extraction methods. It is noteworthy that global representations
    obtained by scene-centric CNN models yield more enriched spatial information than
    those obtained using object-centric CNN models, arguably since global representations
    from scene-centric CNNs contain spatial correlations between objects and global
    scene properties [[18](#bib.bib18), [25](#bib.bib25), [19](#bib.bib19)]. In addition,
    Herranz et al. [[19](#bib.bib19)] showed that the performance of a scene recognition
    system depends on the entities in the scene image, *i.e.,* when the global features
    are extracted from images with chaotic background, the model’s performance is
    degraded compared to the cases that the object is isolated from the background
    or the image has a plain background. This suggests that the background may introduce
    some noise into the feature that weakens the performance. Since contour symmetry
    provides a perceptual advantage when human observers recognize complex real-world
    scenes, Rezanejad et al. [[101](#bib.bib101)] studied global CNN features from
    the full image and only contour information and showed that the performance of
    the full image as input is better, because CNN captures potential information
    from images. Nevertheless, they still concluded that contour is an auxiliary clue
    to improve recognition accuracy.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 全球CNN特征的性能受到输入图像内容的巨大影响。CNN模型在经过足够大且丰富的训练数据集训练后，可以提取通用的全球特征表示，这与手工特征提取方法不同。值得注意的是，由场景中心CNN模型获得的全球表示比使用对象中心CNN模型获得的表示具有更丰富的空间信息，这可以归因于场景中心CNN的全球表示包含了对象与全球场景属性之间的空间相关性[[18](#bib.bib18),
    [25](#bib.bib25), [19](#bib.bib19)]。此外，Herranz等人[[19](#bib.bib19)]表明，场景识别系统的性能取决于场景图像中的实体，即，当从背景混乱的图像中提取全球特征时，模型的性能会降低，相比之下，背景孤立的对象或具有简单背景的图像则性能更佳。这表明背景可能会引入一些噪声，从而削弱性能。由于轮廓对人类观察者在识别复杂现实场景时提供了感知优势，Rezanejad等人[[101](#bib.bib101)]研究了来自整个图像和仅轮廓信息的全球CNN特征，并表明使用完整图像作为输入的性能更好，因为CNN能够从图像中捕捉潜在信息。然而，他们仍然得出结论认为轮廓是提高识别准确性的辅助线索。
- en: 3.2.2 Spatially Invariant Feature based Method
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 基于空间不变特征的方法
- en: To alleviate the problems caused by sequential operations in the standard CNN,
    a body of alternatives [[28](#bib.bib28), [82](#bib.bib82), [40](#bib.bib40)]
    proposed spatially invariant feature based methods to maintain spatial robustness.
    The “spatially invariant” means that the output features are robust against the
    geometrical variations of the input image [[96](#bib.bib96)].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解标准CNN中顺序操作带来的问题，一些替代方法[[28](#bib.bib28), [82](#bib.bib82), [40](#bib.bib40)]提出了基于空间不变特征的方法，以保持空间鲁棒性。
    “空间不变”意味着输出特征对输入图像的几何变化具有鲁棒性[[96](#bib.bib96)]。
- en: 'As shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey") (b),
    spatially invariant features are usually extracted from multiple local patches.
    The visualization of such a feature extraction process is shown in Fig. [5](#S2.F5
    "Figure 5 ‣ 2.2 Datasets ‣ 2 Background ‣ Deep Learning for Scene Classification:
    A Survey") (marked in green arrows). The entire process can be decomposed into
    five basic steps: 1) Local patch extraction: a given input image is divided into
    smaller local patches, which are used as the input to a CNN model, 2) Local feature
    extraction: deep features are extracted from either the CONV or FC layers of the
    model, 3) Codebook generation: this step is to generate a codebook with multiple
    codewords based on the extracted deep features from different regions of the image.
    The codewords usually are learned in an unsupervised way (*e.g.,* using GMM),
    4) Spatially invariant feature generation: given the generated codebook, deep
    features are encoded into a spatially invariant representation, and 5) Class prediction:
    the representation input is classified into a predefined scene category.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey") (b) 所示，空间不变特征通常是从多个局部补丁中提取的。这种特征提取过程的可视化如图
    [5](#S2.F5 "Figure 5 ‣ 2.2 Datasets ‣ 2 Background ‣ Deep Learning for Scene Classification:
    A Survey")（绿色箭头标记）所示。整个过程可以分解为五个基本步骤：1) 局部补丁提取：将给定的输入图像分成较小的局部补丁，这些补丁作为 CNN 模型的输入；2)
    局部特征提取：从模型的 CONV 或 FC 层中提取深度特征；3) 码本生成：此步骤是基于从图像的不同区域提取的深度特征生成具有多个码字的码本。这些码字通常以无监督的方式（*例如，*
    使用 GMM）学习；4) 空间不变特征生成：给定生成的码本，将深度特征编码为空间不变的表示；5) 类别预测：将表示输入分类为预定义的场景类别。'
- en: As opposed to patch-based local feature extraction (each local feature is extracted
    from an original patch by independently using the CNN extractor), local features
    can also be extracted from the semantic CONV maps of a standard CNN [[29](#bib.bib29),
    [102](#bib.bib102), [103](#bib.bib103), [44](#bib.bib44)]. Specifically, since
    each cell (deep descriptor) of the feature map corresponds to one local image
    patch in the input image, each cell is regarded as a local feature. In this approach,
    the computation time is decreased, compared to independently processing of multiple
    spatial patches to obtain local features. For instance, Yoo et al. [[29](#bib.bib29)]
    replaced the FC layers with CONV layers to obtain large amount of local spatial
    features. They also used multi-scale CNN activations to achieve geometric robustness.
    Gao et al. [[102](#bib.bib102)] used a spatial pyramid to directly divide the
    activations into multi-level pyramids, which contain more discriminative spatial
    information.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于补丁的局部特征提取（每个局部特征通过独立使用 CNN 提取器从原始补丁中提取）不同，局部特征也可以从标准 CNN 的语义 CONV 图中提取[[29](#bib.bib29),
    [102](#bib.bib102), [103](#bib.bib103), [44](#bib.bib44)]。具体而言，由于特征图的每个单元（深度描述符）对应于输入图像中的一个局部图像补丁，因此每个单元被视为一个局部特征。与独立处理多个空间补丁以获得局部特征相比，这种方法可以减少计算时间。例如，Yoo
    等人[[29](#bib.bib29)] 用 CONV 层替换了 FC 层，以获得大量的局部空间特征。他们还使用了多尺度 CNN 激活来实现几何鲁棒性。Gao
    等人[[102](#bib.bib102)] 使用空间金字塔直接将激活分割成多级金字塔，这些金字塔包含更多的辨别空间信息。
- en: The feature encoding technique, which aggregates the local features, is crucial
    in relating local features with the final feature representation, and it directly
    influences the accuracy and efficiency of the scene classification algorithms [[85](#bib.bib85)].
    Improved Fisher Vector (IFV) [[104](#bib.bib104)], Vector of Locally Aggregated
    Descriptors (VLAD) [[105](#bib.bib105)], and Bag-of-Visual-Word (BoVW) [[106](#bib.bib106)]
    are among the popular and effective encoding techniques that are used in deep
    learning based methods. For instance, many methods, like FV-CNN [[82](#bib.bib82)],
    MFA-FS [[42](#bib.bib42)], and MFAFVNet [[31](#bib.bib31)], apply IFV encoding
    to obtain the image embedding as spatially invariant representations, while MOP-CNN [[28](#bib.bib28)],
    SDO [[35](#bib.bib35)], *etc* utilize VLAD to cluster local features. Noteworthily,
    the codebook selection and encoding procedures result in disjoint training of
    the model. To this end, some works proposed networks that are trained in an end-to-end
    manner, *e.g.,* NetVLAD [[67](#bib.bib67)], MFAFVNet [[31](#bib.bib31)], and VSAD [[32](#bib.bib32)].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 特征编码技术，通过聚合局部特征，在将局部特征与最终特征表示相关联方面至关重要，并且直接影响场景分类算法的准确性和效率[[85](#bib.bib85)]。改进的费舍尔向量（IFV）[[104](#bib.bib104)]、局部聚合描述符向量（VLAD）[[105](#bib.bib105)]和视觉词袋（BoVW）[[106](#bib.bib106)]是深度学习方法中常用且有效的编码技术。例如，许多方法，如FV-CNN[[82](#bib.bib82)]、MFA-FS[[42](#bib.bib42)]和MFAFVNet[[31](#bib.bib31)]，应用IFV编码以获得空间不变的图像嵌入表示，而MOP-CNN[[28](#bib.bib28)]、SDO[[35](#bib.bib35)]等则利用VLAD对局部特征进行聚类。值得注意的是，代码本选择和编码过程导致模型的离散训练。为此，一些工作提出了以端到端方式训练的网络，例如NetVLAD[[67](#bib.bib67)]、MFAFVNet[[31](#bib.bib31)]和VSAD[[32](#bib.bib32)]。
- en: Spatially invariant feature based methods are efficient to achieve geometric
    robustness. Nevertheless, the sliding windows based paradigm requires multi-resolution
    scanning with fixed aspect ratios, which is not suitable for arbitrary objects
    with variable sizes or aspect ratios in the scene image. Moreover, using dense
    patches may introduce noise into the final representation, which decreases the
    classification accuracy. Therefore, extracting semantic features from salient
    regions of the scene image can circumvent these drawbacks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于空间不变特征的方法在实现几何鲁棒性方面非常有效。然而，基于滑动窗口的范式需要多分辨率扫描且具有固定的纵横比，这不适用于场景图像中具有可变大小或纵横比的任意对象。此外，使用密集的补丁可能会将噪声引入最终表示中，从而降低分类准确性。因此，从场景图像的显著区域提取语义特征可以规避这些缺点。
- en: 3.2.3 Semantic Feature based Method
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 基于语义特征的方法
- en: Processing all patches of the input image requires computational cost while
    yields redundant information. Object detection determines whether or not any instance
    of the salient regions is presented in an image [[107](#bib.bib107)]. Inspired
    by this, object detector based approaches allow identifying salient regions of
    the scene, which provide distinctive information about the context of the image.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 处理输入图像的所有补丁需要计算成本，同时产生冗余信息。目标检测确定图像中是否存在显著区域的实例[[107](#bib.bib107)]。受此启发，基于目标检测器的方法可以识别场景中的显著区域，这些区域提供了关于图像背景的独特信息。
- en: 'Different methods have been put forward to effective saliency detection, such
    as selective search [[108](#bib.bib108)], unsupervised discovery [[109](#bib.bib109)],
    Multi-scale Combinatorial Grouping (MCG) [[110](#bib.bib110)], and object detection
    networks (*e.g.,* Fast RCNN [[51](#bib.bib51)], Faster RCNN [[52](#bib.bib52)],
    SSD [[111](#bib.bib111)], Yolo [[112](#bib.bib112), [113](#bib.bib113)]). For
    instance, since selective search combines the strengths of exhaustive search and
    segmentation, Liu et al. [[87](#bib.bib87)] used it to capture all possible semantic
    regions, and then used a pre-trained CNN to extract the feature maps of each region
    followed by a spatial pyramid to reduce map dimensions. Because the common objects
    or characteristics in different scenes lead to the commonality of different scenes,
    Cheng et al. [[35](#bib.bib35)] used a region proposal network [[52](#bib.bib52)]
    to extract the discriminative regions while discarde non-discriminative regions.
    These semantic feature based methods [[87](#bib.bib87), [35](#bib.bib35)] harvest
    many semantic regions, so encoding technology is adapted to aggregate key features,
    which pipeline is shown in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2 Datasets ‣ 2 Background
    ‣ Deep Learning for Scene Classification: A Survey") (red arrows).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '提出了不同的方法来有效地进行显著性检测，如选择性搜索[[108](#bib.bib108)]、无监督发现[[109](#bib.bib109)]、多尺度组合分组（MCG）[[110](#bib.bib110)]和目标检测网络（*例如，*
    Fast RCNN[[51](#bib.bib51)]、Faster RCNN[[52](#bib.bib52)]、SSD[[111](#bib.bib111)]、Yolo[[112](#bib.bib112),
    [113](#bib.bib113)]）。例如，由于选择性搜索结合了全面搜索和分割的优点，Liu等人[[87](#bib.bib87)]使用它来捕捉所有可能的语义区域，然后使用预训练的CNN提取每个区域的特征图，再用空间金字塔减少图的维度。由于不同场景中常见的物体或特征导致了不同场景的共性，Cheng等人[[35](#bib.bib35)]使用区域提议网络[[52](#bib.bib52)]提取具有区分性的区域，同时丢弃非区分性的区域。这些基于语义特征的方法[[87](#bib.bib87),
    [35](#bib.bib35)]获取了许多语义区域，因此采用了编码技术来聚合关键特征，其管道如图[5](#S2.F5 "Figure 5 ‣ 2.2 Datasets
    ‣ 2 Background ‣ Deep Learning for Scene Classification: A Survey")（红色箭头）所示。'
- en: 'On the other hand, some semantic feature based methods [[33](#bib.bib33), [34](#bib.bib34)]
    are based on weakly supervised learning, which directly predicts categories by
    several semantic features of the scene. For instance, Wu et al. [[33](#bib.bib33)]
    generated high-quality proposal regions by using MCG [[110](#bib.bib110)], and
    then used SVM on each scene category to prune outliers and redundant regions.
    Semantic features from different scale patches supply complementary cues, since
    the coarser scales deal with larger objects, while the finer levels provide smaller
    objects or object parts. In practice, they found two semantic features sufficient
    to represent the whole scene, comparable to multiple semantic features. Training
    a deep model only using a single salient region may result in a suboptimal performance
    due to the possible existence of outliers in the training set. Hence, multiple
    regions can be selected to train the model together [[34](#bib.bib34)]. As shown
    in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey") (c), Durand et
    al. [[34](#bib.bib34)] designed a Max layer to select the attention regions to
    enhance the discrimination. To provide a more robust strategy, they also designed
    a Min layer to capture the regions with the most negative evidence to further
    improve the model.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，一些基于语义特征的方法[[33](#bib.bib33), [34](#bib.bib34)]基于弱监督学习，直接通过场景的几个语义特征预测类别。例如，Wu等人[[33](#bib.bib33)]通过使用MCG[[110](#bib.bib110)]生成高质量的提议区域，然后在每个场景类别上使用SVM来修剪异常值和冗余区域。来自不同尺度补丁的语义特征提供了互补的线索，因为较粗的尺度处理较大的物体，而较细的层次提供较小的物体或物体部件。在实践中，他们发现两个语义特征足以代表整个场景，能够与多个语义特征相媲美。仅使用单个显著区域训练深度模型可能会导致亚优性能，因为训练集中可能存在异常值。因此，可以选择多个区域一起训练模型[[34](#bib.bib34)]。如图[7](#S3.F7
    "Figure 7 ‣ 3.2 CNN based Scene Representation ‣ 3 Deep Learning based Methods
    ‣ Deep Learning for Scene Classification: A Survey")（c）所示，Durand等人[[34](#bib.bib34)]设计了一个Max层来选择注意区域以增强区分度。为了提供更稳健的策略，他们还设计了一个Min层来捕捉具有最负面证据的区域，从而进一步改进模型。'
- en: Although better performance can be obtained via using more semantic local features,
    semantic feature based methods deeply rely on the performance of object detection.
    Weak supervision settings (*i.e.,* without the patch labels of scene images) make
    it difficult to accurately identify the scene by the key information of an image [[34](#bib.bib34)].
    Moreover, the error accumulation problem and extra computation cost also limit
    the development of semantic feature based methods [[103](#bib.bib103)].
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过使用更多的语义局部特征可以获得更好的性能，但语义特征基础的方法深受目标检测性能的影响。弱监督设置（*即*，没有场景图像的补丁标签）使得仅凭图像的关键信息难以准确识别场景[[34](#bib.bib34)]。此外，误差累积问题和额外的计算成本也限制了语义特征基础方法的发展[[103](#bib.bib103)]。
- en: 3.2.4 Multi-layer Feature based Method
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 多层特征基础方法
- en: Global feature based methods usually extract the high-layer CNN features, and
    feed them into a classifier to achieve classification task. Due to the compactness
    of such high-layer features, it is easy to miss some important slight clues [[40](#bib.bib40),
    [114](#bib.bib114)]. Features from different layers are complementary [[33](#bib.bib33),
    [115](#bib.bib115)]. Low-layer features generally capture small objects, while
    high-layer features capture big objects [[33](#bib.bib33)]. Moreover, semantic
    information of low-layer features is relatively less, but the object location
    is accurate [[115](#bib.bib115)]. To take full advantage of features from different
    layers, many methods [[39](#bib.bib39), [47](#bib.bib47), [21](#bib.bib21), [38](#bib.bib38)]
    used the high resolution features from the early layers along with the high semantic
    information of the features from the latest layers of hierarchical models (*e.g.,*
    CNNs).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 全球特征基础的方法通常提取高层CNN特征，并将其输入分类器以实现分类任务。由于这些高层特征的紧凑性，容易忽略一些重要的细微线索[[40](#bib.bib40),
    [114](#bib.bib114)]。来自不同层的特征是互补的[[33](#bib.bib33), [115](#bib.bib115)]。低层特征通常捕捉小物体，而高层特征捕捉大物体[[33](#bib.bib33)]。此外，低层特征的语义信息相对较少，但物体位置准确[[115](#bib.bib115)]。为了充分利用来自不同层的特征，许多方法[[39](#bib.bib39),
    [47](#bib.bib47), [21](#bib.bib21), [38](#bib.bib38)]使用了来自早期层的高分辨率特征以及来自最新层的层次模型（*例如*，CNNs）的高语义信息。
- en: 'As shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey") (d),
    typical multi-layer feature formation process includes: 1) Feature extraction:
    the outputs (feature maps) of certain layers are extracted as deep features, 2)
    Feature vectorization: vectorize the extracted feature maps, 3) Multi-layer feature
    combination: multiple features from different layers are combined into a single
    feature vector, and 4) Feature classification: classify the given scene image
    based on the obtained combined feature.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation ‣ 3 Deep Learning
    based Methods ‣ Deep Learning for Scene Classification: A Survey") (d)所示，典型的多层特征形成过程包括：1)
    特征提取：提取某些层的输出（特征图）作为深度特征；2) 特征向量化：将提取的特征图向量化；3) 多层特征组合：将来自不同层的多个特征组合成一个单一的特征向量；4)
    特征分类：基于获得的组合特征对给定的场景图像进行分类。'
- en: Although using all features from different layers seems to improve the final
    representation, it likely increases the chance of overfitting, and thus hurts
    performance [[37](#bib.bib37)]. Therefore, many methods [[38](#bib.bib38), [39](#bib.bib39),
    [21](#bib.bib21), [47](#bib.bib47), [37](#bib.bib37)] only extract features from
    certain layers. For instance, Xie et al. [[38](#bib.bib38)] constructed two dictionary-based
    representations, Convolution Fisher Vector (CFV), and Mid-level Local Discriminative
    Representation (MLR) to classify subsidiarily scene images. Tang et al. [[39](#bib.bib39)]
    divided GoogLeNet layers into three parts from bottom to top and extracted final
    feature maps of each part. Liu et al. [[21](#bib.bib21)] captured feature maps
    from each residual block from ResNet independently. Song et al. [[47](#bib.bib47)]
    selected discriminative combinations from different layers and different network
    branches via minimizing a weighted sum of the probability of error and the average
    correlation coefficient. Yang et al. [[37](#bib.bib37)] used greedily select to
    explore the best layer combinations.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用来自不同层的所有特征似乎可以改善最终表示，但这可能增加过拟合的风险，从而影响性能[[37](#bib.bib37)]。因此，许多方法[[38](#bib.bib38),
    [39](#bib.bib39), [21](#bib.bib21), [47](#bib.bib47), [37](#bib.bib37)]仅从特定层提取特征。例如，Xie等人[[38](#bib.bib38)]构建了两种基于字典的表示：卷积费希尔向量（CFV）和中层局部区分表示（MLR），用于对附属场景图像进行分类。Tang等人[[39](#bib.bib39)]将GoogLeNet层从底部到顶部分为三部分，并提取每部分的最终特征图。Liu等人[[21](#bib.bib21)]独立地从ResNet中的每个残差块捕获特征图。Song等人[[47](#bib.bib47)]通过最小化错误概率的加权和以及平均相关系数来选择来自不同层和不同网络分支的区分组合。Yang等人[[37](#bib.bib37)]则使用贪婪选择来探索最佳层组合。
- en: 'Feature fusion in multi-layer feature based methods is another important direction.
    Feature fusion techniques are mainly divided into two groups [[116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118)]: 1) Early fusion: extracting multi-layer
    features and merging them into a comprehensive feature for scene classification,
    and 2) Late fusion: directly learning each multi-layer feature via a supervised
    learner, which enforces the features to be directly sensitive to the category
    label, and then merging them into a final feature. Although the performance of
    late fusion is better, it is more complex and time-consuming, so early fusion
    is more popular [[38](#bib.bib38), [39](#bib.bib39), [21](#bib.bib21), [47](#bib.bib47)].
    In addition, addition and product rules are usually applied to combine multiple
    features [[39](#bib.bib39)]. Since the feature spaces in different layers are
    disparate, product rule is better than addition rule to fusing features, and empirical
    experiments on [[39](#bib.bib39)] also show this statement. Moreover, Tang et
    al. [[39](#bib.bib39)] proposed two strategies to fuse multi-layer features, *i.e.,*
    ‘fusion with score’ and ‘fusion with features’. Fusion with score technique has
    obtained a better performance over fusion with feature thanks to the end-to-end
    training.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 多层特征方法中的特征融合是另一个重要方向。特征融合技术主要分为两组[[116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118)]：1)
    早期融合：提取多层特征并将它们合并为一个综合特征用于场景分类；2) 后期融合：通过监督学习器直接学习每个多层特征，这使得特征直接对类别标签敏感，然后将它们合并为最终特征。虽然后期融合的性能更好，但它更复杂且耗时，因此早期融合更受欢迎[[38](#bib.bib38),
    [39](#bib.bib39), [21](#bib.bib21), [47](#bib.bib47)]。此外，通常使用加法和乘法规则来结合多个特征[[39](#bib.bib39)]。由于不同层中的特征空间差异，乘法规则比加法规则更适合融合特征，实际实验[[39](#bib.bib39)]也证明了这一点。此外，Tang等人[[39](#bib.bib39)]提出了两种融合多层特征的策略，即‘融合得分’和‘融合特征’。由于端到端训练，得分融合技术在性能上优于特征融合。
- en: 3.2.5 Multiple-view Feature based Method
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5 多视角特征方法
- en: Describing a complex scene just using a single and compact feature representation
    is a non-trivial task. Hence, there has been extensive effort to compute a comprehensive
    representation of a scene by integrating multiple features generated from complementary
    CNN models [[24](#bib.bib24), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [41](#bib.bib41), [32](#bib.bib32)].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用单一且紧凑的特征表示来描述复杂场景是一项非平凡的任务。因此，已经进行了广泛的努力，通过整合来自互补CNN模型的多个特征来计算场景的综合表示[[24](#bib.bib24),
    [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [41](#bib.bib41),
    [32](#bib.bib32)]。
- en: 'Features generated from *networks trained on different datasets* usually are
    complementary. As shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    (e), Herranz et al.[[19](#bib.bib19)] found the best scale response of object-centric
    CNNs and scene-centric CNNs, and they combine the knowledge in a scale-adaptive
    way via either object-centric CNNs or scene-centric CNNs. This finding is widely
    used [[121](#bib.bib121), [122](#bib.bib122)]. For instance, the authors in [[121](#bib.bib121)]
    used an object-centric CNN to carry information about object depicted in the image,
    while a scene-centric CNN was used to capture global scene information. Along
    this way, Wang et al. [[32](#bib.bib32)] designed PatchNet, a weakly supervised
    learning method, which uses image-level supervision information as the supervision
    signal for effective extraction of the patch-level features. To enhance the recognition
    performance, Scene-PatchNet and Object-PatchNet jointly used to extract features
    for each patch.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '从*在不同数据集上训练的网络*生成的特征通常是互补的。如图[7](#S3.F7 "Figure 7 ‣ 3.2 CNN based Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")（e）所示，Herranz等人[[19](#bib.bib19)]发现了物体中心CNN和场景中心CNN的最佳尺度响应，他们通过物体中心CNN或场景中心CNN以尺度自适应的方式结合这些知识。这一发现被广泛使用[[121](#bib.bib121),
    [122](#bib.bib122)]。例如，[[121](#bib.bib121)]中的作者使用物体中心CNN传递图像中物体的信息，而场景中心CNN则用于捕捉全局场景信息。沿着这条路，Wang等人[[32](#bib.bib32)]设计了PatchNet，一种弱监督学习方法，使用图像级监督信息作为有效提取补丁级特征的监督信号。为了提高识别性能，Scene-PatchNet和Object-PatchNet被联合使用来提取每个补丁的特征。'
- en: Employing *complementary CNN architectures* is essential for obtaining discriminative
    multi-view feature representations. Wang et al. [[41](#bib.bib41)] proposed a
    multi-resolution CNN (MR-CNN) architecture to capture visual content in multiple
    scale images. In their work, normal BN-Inception [[123](#bib.bib123)] is used
    to extract coarse resolution features, while deeper BN-Inception is employed to
    extract fine resolution features. Jin et al. [[124](#bib.bib124)] used global
    features and spatially invariant features to account for both the coarse layout
    of the scene and the transient objects. Sun et al. [[24](#bib.bib24)] separately
    extracted three representations, *i.e.,* object semantics representation, contextual
    information, and global appearance, from discriminative views, which are complementarity
    to each other. Specifically, the object semantic features of the scene image are
    extracted by a CNN followed by spatial fisher vectors, while the deep feature
    of a multi-direction LSTM-based model represents contextual information, and the
    FC feature represents global appearance. Li et al. [[119](#bib.bib119)] used ResNet18 [[79](#bib.bib79)]
    to generate discriminative attention maps, which is used as an explicit input
    of CNN together with the original image. Using global features extracted by ResNet18
    and attention map features extracted from the spatial feature transformer network,
    the attention map features are multiplied to the global features for adaptive
    feature refinement so that the network focuses on the most discriminative parts.
    Later, a multi-modal architecture is proposed in [[43](#bib.bib43)], composed
    of a deep branch and a semantic branch. The deep Branch extracts global CNN features,
    while semantic branch aims to extract meaningful scene objects and their relations
    from super pixels.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*互补CNN架构*对于获得判别性的多视图特征表示至关重要。Wang等人[[41](#bib.bib41)]提出了一种多分辨率CNN（MR-CNN）架构，用于捕捉多尺度图像中的视觉内容。在他们的工作中，正常的BN-Inception[[123](#bib.bib123)]用于提取粗分辨率特征，而更深的BN-Inception用于提取细分辨率特征。Jin等人[[124](#bib.bib124)]使用全局特征和空间不变特征来考虑场景的粗略布局和瞬态物体。Sun等人[[24](#bib.bib24)]从判别视图中分别提取了三种表示，即物体语义表示、上下文信息和全局外观，这些表示彼此互补。具体而言，场景图像的物体语义特征通过CNN提取，接着是空间Fisher向量，而多方向LSTM模型的深层特征表示上下文信息，FC特征表示全局外观。Li等人[[119](#bib.bib119)]使用ResNet18[[79](#bib.bib79)]生成判别性注意力图，该图与原始图像一起作为CNN的显式输入。使用ResNet18提取的全局特征和从空间特征变换网络中提取的注意力图特征，注意力图特征与全局特征相乘以进行自适应特征细化，使网络关注最具判别性的部分。随后，在[[43](#bib.bib43)]中提出了一种多模态架构，由深层分支和语义分支组成。深层分支提取全局CNN特征，而语义分支旨在从超像素中提取有意义的场景物体及其关系。
- en: 3.3 Strategies for Improving Scene Representation
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 提高场景表示的策略
- en: To obtain more discriminative representations for scene classification, a range
    of strategies has been proposed. Four major categories (*i.e.,* encoding strategy,
    attention strategy, contextual strategy, and regularization strategy) will be
    discussed below.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更具辨别力的场景分类表示，已提出了一系列策略。以下将讨论四大类策略 (*即，* 编码策略、注意力策略、上下文策略和正则化策略)。
- en: 3.3.1 Encoding strategy
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 编码策略
- en: Although the current driving force has been the incorporation of CNNs, encoding
    technology of the first generation methods have also been adapted in deep learning
    based methods. Fisher Vector (FV) coding [[125](#bib.bib125), [104](#bib.bib104)]
    is an encoding technique commonly used in scene classification. Fisher vector
    stores the mean and the covariance deviation vectors per component of the GMM
    and each element of the local features together. Thanks to the covariance deviation
    vectors, FV encoding leads to excellent results. Moreover, it is empirically proven
    that Fisher vectors are complementary to global CNN features [[30](#bib.bib30),
    [42](#bib.bib42), [46](#bib.bib46), [38](#bib.bib38), [40](#bib.bib40), [31](#bib.bib31)].
    Therefore, this survey takes FV-based approaches as the major cue and discusses
    the adapted combination of encoding technology and deep learning.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管当前的推动力是卷积神经网络（CNNs）的引入，但第一代方法的编码技术也已在基于深度学习的方法中得到应用。Fisher 向量 (FV) 编码 [[125](#bib.bib125),
    [104](#bib.bib104)] 是一种常用于场景分类的编码技术。Fisher 向量存储每个 GMM 组件的均值和协方差偏差向量，以及每个局部特征的元素。得益于协方差偏差向量，FV
    编码能取得出色的结果。此外，实证证明 Fisher 向量对全局 CNN 特征是互补的 [[30](#bib.bib30), [42](#bib.bib42),
    [46](#bib.bib46), [38](#bib.bib38), [40](#bib.bib40), [31](#bib.bib31)]。因此，本调查将
    FV 基础方法作为主要线索，并讨论编码技术与深度学习的适应性结合。
- en: '![Refer to caption](img/fd5ff5d4e394008366b26b7da35e3f32.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fd5ff5d4e394008366b26b7da35e3f32.png)'
- en: 'Figure 8: Structure comparisons of (a) basic Fisher vector [[82](#bib.bib82)]
    and its variations. BoF denotes bag of features, while BoS represents bag of semantic
    probabilities. (b) In semantic FV [[30](#bib.bib30)], natural parameterization
    is added to map multinomial distribution (*i.e.,* $\pi$) to its natural parameter
    space (*i.e.,* $\nu$). (c) In MFAFVNet [[31](#bib.bib31)], GMM is replaced with
    MFA to build codebook. (d) In VSAD [[32](#bib.bib32)], codebook is constructed
    via exploiting semantics (*i.e.,* BoS) to aggregate local features (*i.e.,* BoF).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：对比（a）基本的 Fisher 向量 [[82](#bib.bib82)] 及其变体的结构。BoF 表示特征包，而 BoS 代表语义概率包。 (b)
    在语义 FV [[30](#bib.bib30)] 中，添加了自然参数化来将多项分布 (*即，* $\pi$) 映射到其自然参数空间 (*即，* $\nu$)。
    (c) 在 MFAFVNet [[31](#bib.bib31)] 中，用 MFA 替代 GMM 以构建词典。 (d) 在 VSAD [[32](#bib.bib32)]
    中，通过利用语义 (*即，* BoS) 来构建词典，从而聚合局部特征 (*即，* BoF)。
- en: 'Generally, CONV features and FC features are regarded as Bags of Features (BoF),
    they can be readily modeled by the Gaussian Mixture Model followed by Fisher Vector
    (GMM-FV) [[30](#bib.bib30), [31](#bib.bib31)]. To avoid the computation of the
    FC layers, Cimpoi et al. [[82](#bib.bib82)] utilized GMM-FV to aggregate BoF from
    different CONV layers, respectively. Comparing their experiment results, they
    asserted that the last CONV features can more effectively represent scenes. To
    rescue the fine-grained information of early/middle layers, Guo et al. [[40](#bib.bib40)]
    proposed Fisher Convolutional Vector (FCV) to encode the feature maps from multiple
    CONV layers. Wang et al. [[46](#bib.bib46)] extracted the feature maps from RGB,
    HHA, and surface normal images, and then directly encoded these maps by FV coding.
    In addition, through the performance comparisons of GMM-FV encoding on CONV features
    and FC features, respectively, Dixit et al. [[30](#bib.bib30)] asserted that the
    FC features is more effective for scene classification. However, since the CONV
    features and FC features do not derive from semantic probability space, it is
    likely to be both less discriminant and less abstract than the truly semantic
    features  [[82](#bib.bib82), [30](#bib.bib30)]. The activations of Softmax layer
    are probability vectors, inhabiting the probability simplex, which are more abstract
    and semantic, but it is difficult to implement an effective invariant coding (*e.g.,*
    GMM-FV) [[126](#bib.bib126), [30](#bib.bib30)]. To this end, Dixit et al. [[30](#bib.bib30)]
    proposed an indirect FV implementation to aggregate these semantic probability
    features, *i.e.,* adding a step to convert semantic multinomials from probability
    space to the natural parameter space, as shown in Fig. [8](#S3.F8 "Figure 8 ‣
    3.3.1 Encoding strategy ‣ 3.3 Strategies for Improving Scene Representation ‣
    3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey") (b).
    Inspired by FV and VLAD, Wang et al. [[32](#bib.bib32)] proposed Vector of Semantically
    Aggregated Descriptors (VSAD) to encode the probability features, as shown in
    Fig. [8](#S3.F8 "Figure 8 ‣ 3.3.1 Encoding strategy ‣ 3.3 Strategies for Improving
    Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene
    Classification: A Survey") (d). Comparing the discriminant probability learned
    by the weakly-supervised method (PatchNet) with the generative probability from
    an unsupervised method (GMM), the results show that the discriminant probability
    is more expressive in aggregating local features. From the above discussion, representation
    encoding local features on probability space outperforms that on non-probability
    space.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，CONV特征和FC特征被视为特征袋（BoF），它们可以通过高斯混合模型加上Fisher向量（GMM-FV）来建模[[30](#bib.bib30),
    [31](#bib.bib31)]。为了避免计算FC层，Cimpoi等[[82](#bib.bib82)]利用GMM-FV分别聚合来自不同CONV层的BoF。通过比较他们的实验结果，他们断言最后的CONV特征能更有效地表示场景。为了挽救早期/中间层的细粒度信息，Guo等[[40](#bib.bib40)]提出了Fisher卷积向量（FCV）来编码来自多个CONV层的特征图。Wang等[[46](#bib.bib46)]提取了来自RGB、HHA和表面法线图像的特征图，然后直接通过FV编码这些图。此外，通过对CONV特征和FC特征的GMM-FV编码性能比较，Dixit等[[30](#bib.bib30)]断言FC特征在场景分类中更有效。然而，由于CONV特征和FC特征并非源于语义概率空间，它们可能比真正的语义特征更不具判别性和抽象性[[82](#bib.bib82),
    [30](#bib.bib30)]。Softmax层的激活是概率向量，位于概率单纯形中，较为抽象和语义，但实施有效的不变编码（*例如*，GMM-FV）较为困难[[126](#bib.bib126),
    [30](#bib.bib30)]。为此，Dixit等[[30](#bib.bib30)]提出了一种间接FV实现方法，用于聚合这些语义概率特征，即增加一个步骤，将语义多项式从概率空间转换到自然参数空间，如图[8](#S3.F8
    "Figure 8 ‣ 3.3.1 Encoding strategy ‣ 3.3 Strategies for Improving Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    (b)所示。受FV和VLAD的启发，Wang等[[32](#bib.bib32)]提出了语义聚合描述符向量（VSAD）来编码概率特征，如图[8](#S3.F8
    "Figure 8 ‣ 3.3.1 Encoding strategy ‣ 3.3 Strategies for Improving Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    (d)所示。比较弱监督方法（PatchNet）学习的判别概率与无监督方法（GMM）生成的概率，结果表明判别概率在聚合局部特征时更具表现力。综上所述，概率空间上的局部特征表示编码优于非概率空间。'
- en: Deep features usually are high dimensional ones. Therefore, more Gaussian kernels
    are needed to accurately model the feature space [[83](#bib.bib83)]. However,
    this would a lot of overhead to the computations and, hence, it is not efficient.
    Liu et al. [[83](#bib.bib83)] empirically proved that the discriminative power
    of FV features increases slowly as the number of Gaussian kernels increases. Therefore,
    dimensionality reduction of the features is very important, as it directly affects
    the computational efficiency. A wide range of approaches [[83](#bib.bib83), [30](#bib.bib30),
    [29](#bib.bib29), [58](#bib.bib58), [40](#bib.bib40), [42](#bib.bib42), [32](#bib.bib32)]
    used poplular dimensionality reduction techniques, Principal Component Analysis
    (PCA), for pre-processing of the local features. Moreover, Liu et al. [[83](#bib.bib83)]
    drew local features from Gaussian distribution with a nearly zero mean, which
    ensures the sparsity of the resulting FV. Wang et al. [[46](#bib.bib46)] enforced
    intercomponent sparsity of GMM-FV features via component regularization to discount
    unnecessary components.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 深层特征通常是高维的。因此，需要更多的高斯核来准确建模特征空间[[83](#bib.bib83)]。然而，这会带来大量的计算开销，因此效率不高。Liu等人[[83](#bib.bib83)]通过实验证明，FV特征的判别能力随着高斯核数量的增加而缓慢提升。因此，特征的降维非常重要，因为它直接影响计算效率。许多方法[[83](#bib.bib83),
    [30](#bib.bib30), [29](#bib.bib29), [58](#bib.bib58), [40](#bib.bib40), [42](#bib.bib42),
    [32](#bib.bib32)]使用流行的降维技术，主成分分析（PCA），对局部特征进行预处理。此外，Liu等人[[83](#bib.bib83)]从高斯分布中提取局部特征，具有接近零的均值，这确保了结果FV的稀疏性。Wang等人[[46](#bib.bib46)]通过组件正则化强制GMM-FV特征的组件间稀疏性，以减少不必要的组件。
- en: 'Due to the non-linear property of deep features and a limited ability of the
    covariance of GMM, a large number of diagonal GMM components are required to model
    deep features so that the FV has very high dimensions [[42](#bib.bib42), [31](#bib.bib31)].
    To address this issue, Dixit et al. [[42](#bib.bib42)] proposed MFA-FS, in which
    GMM is replaced by Mixtures of Factor Analysis (MFA) [[127](#bib.bib127), [128](#bib.bib128)],
    *i.e.,* a set of local linear subspaces is used to capture non-linear features.
    MFA-FS performs well but does not support end-to-end training. However, end-to-end
    training is more efficient than any disjoint training process [[31](#bib.bib31)].
    Therefore, Li et al. [[31](#bib.bib31)] proposed MFAFVnet, an improved variant
    of MFA-FS [[42](#bib.bib42)], which is conveniently embedded into the state-of-the-art
    network architectures. Fig. [8](#S3.F8 "Figure 8 ‣ 3.3.1 Encoding strategy ‣ 3.3
    Strategies for Improving Scene Representation ‣ 3 Deep Learning based Methods
    ‣ Deep Learning for Scene Classification: A Survey") (c) shows the MFA-FV layer
    of MFAFVNet, compared with the other two structures.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '由于深层特征的非线性特性和GMM协方差的有限能力，需要大量的对角GMM组件来建模深层特征，从而使FV具有非常高的维度[[42](#bib.bib42),
    [31](#bib.bib31)]。为了解决这个问题，Dixit等人[[42](#bib.bib42)]提出了MFA-FS，其中GMM被因子分析混合模型（MFA）[[127](#bib.bib127),
    [128](#bib.bib128)]取代，*即*，使用一组局部线性子空间来捕捉非线性特征。MFA-FS表现良好，但不支持端到端训练。然而，端到端训练比任何离散训练过程更高效[[31](#bib.bib31)]。因此，Li等人[[31](#bib.bib31)]提出了MFAFVnet，这是一种MFA-FS的改进变体[[42](#bib.bib42)]，它可以方便地嵌入到最先进的网络架构中。图[8](#S3.F8
    "Figure 8 ‣ 3.3.1 Encoding strategy ‣ 3.3 Strategies for Improving Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    (c)展示了MFAFVNet的MFA-FV层，与其他两个结构进行了比较。'
- en: In FV coding, local features are assumed to be independent and identically distributed
    (iid), which violates intrinsic image attributes that these patches are not iid.
    To this end, Cinbis et al. [[58](#bib.bib58)] introduced a non-iid model via treating
    the model parameters as latent variables, rendering features related locally.
    Later, Wei et al. [[129](#bib.bib129)] proposed a correlated topic vector, treated
    as an evolution oriented from Fisher kernel framework, to explore latent semantics,
    and consider semantic correlation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在FV编码中，局部特征被假设为独立同分布（iid），这违反了这些补丁不是iid的内在图像属性。为此，Cinbis等人[[58](#bib.bib58)]通过将模型参数视为潜变量引入了非iid模型，从而使特征局部相关。后来，Wei等人[[129](#bib.bib129)]提出了一个相关的主题向量，作为从Fisher核框架演变而来的模型，以探索潜在语义，并考虑语义相关性。
- en: 3.3.2 Attention strategy
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 注意力策略
- en: As opposed to semantic feature based methods (focusing on key cues generally
    from original images), attention mechanism aims to capture distinguishing cues
    from the extracted feature space [[96](#bib.bib96), [122](#bib.bib122), [44](#bib.bib44),
    [43](#bib.bib43)]. The attention maps are learned without any explicit training
    signal, rather task-related loss function alone provides the training signal for
    the attention weights. Generally, attention policy mainly includes channel attention
    and spatial attention.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于语义特征的方法（通常关注于原始图像中的关键线索）不同，注意力机制旨在从提取的特征空间中捕捉区别性线索[[96](#bib.bib96), [122](#bib.bib122),
    [44](#bib.bib44), [43](#bib.bib43)]。注意力图是在没有任何显式训练信号的情况下学习的，而仅依靠任务相关的损失函数提供注意力权重的训练信号。通常，注意力策略主要包括通道注意力和空间注意力。
- en: Channel attention policy. Channel activation maps (ChAMs) generated from attention
    policy refers to the weighted activation maps, which highlights the class-specific
    discriminative regions. For instance, class activation map [[23](#bib.bib23)]
    is a simple ChAM, widely used in many works [[130](#bib.bib130), [131](#bib.bib131)].
    Since the same semantic cue has different roles for different types of scenes
    in some cases, Li et al. [[96](#bib.bib96)] designed class-aware attentive pooling,
    including intra-modality attentive pooling and cross-aware attentive pooling,
    to learn the contributions of RGB and depth modalities, respectively. Here the
    attention strategies are also used to further fuse the learned discriminate semantic
    cues across RGB and depth modalities. Moreover, they also designed a class-agnostic
    attentive pooling to ignore some salient regions that may mislead classification.
    Inspired by the idea that specific objects are associated with a scene, Seong et
    al. [[132](#bib.bib132)] proposed correlative context gating to activate scene-specific
    object features.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通道注意力策略。通道激活图（ChAMs）是通过注意力策略生成的加权激活图，突出显示了类别特定的判别区域。例如，类激活图[[23](#bib.bib23)]是一种简单的ChAM，广泛用于许多工作中[[130](#bib.bib130),
    [131](#bib.bib131)]。由于相同的语义线索在某些情况下对不同类型的场景具有不同的作用，Li等人[[96](#bib.bib96)] 设计了类别感知的注意力池化，包括模态内注意力池化和跨模态注意力池化，以分别学习RGB和深度模态的贡献。在这里，注意力策略还用于进一步融合RGB和深度模态中的学习到的判别性语义线索。此外，他们还设计了类别无关的注意力池化，以忽略一些可能误导分类的显著区域。受到特定对象与场景相关联的思想启发，Seong等人[[132](#bib.bib132)]
    提出了相关上下文门控，以激活场景特定的对象特征。
- en: 'Channel attention maps can also be computed from different sources of information.
    With multiple salient regions on different scales as input, Xia et al. [[122](#bib.bib122)]
    designed a Weakly Supervised Attention Map (WS-AM) by proposing a gradient-weighted
    class activation mapping technique and privileging weakly supervised information.
    In another work [[43](#bib.bib43)], the input of semantic branch is a semantic
    segmentation score map, and semantic features are extracted from semantic branch
    via using three channel attention modules, shown in Fig. [9](#S3.F9 "Figure 9
    ‣ 3.3.2 Attention strategy ‣ 3.3 Strategies for Improving Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    (a). Moreover, semantic features are used to gate global CNN features via another
    attention module.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '通道注意力图也可以从不同的信息源计算得到。Xia等人[[122](#bib.bib122)] 设计了一种弱监督注意力图（WS-AM），通过提出一种梯度加权类激活映射技术，并优先考虑弱监督信息，利用不同尺度的多个显著区域作为输入。在另一项工作中[[43](#bib.bib43)]，语义分支的输入是语义分割得分图，语义特征通过使用三个通道注意力模块从语义分支中提取，如图[9](#S3.F9
    "Figure 9 ‣ 3.3.2 Attention strategy ‣ 3.3 Strategies for Improving Scene Representation
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    (a)所示。此外，语义特征还通过另一个注意力模块用于门控全局CNN特征。'
- en: '![Refer to caption](img/21c2282a78ec20d87efe33eec0f7f1c9.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/21c2282a78ec20d87efe33eec0f7f1c9.png)'
- en: 'Figure 9: Illustrations of two typical attentions. (a) In channel attention [[43](#bib.bib43)],
    the channel attention map is used to weight the input by a Hadamard product. (b)
    Spatial attention in [[44](#bib.bib44)] is used to enhance the local feature selection.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：两个典型注意力的示意图。(a) 在通道注意力[[43](#bib.bib43)]中，通道注意力图用于通过Hadamard积对输入进行加权。(b)
    空间注意力[[44](#bib.bib44)]用于增强局部特征选择。
- en: 'Spatial attention policy. Spatial attention policy infers attention maps along
    height and width of input feature maps, then the attention maps are combined to
    original maps for adaptive feature refinement. Joseph et al. [[133](#bib.bib133)]
    proposed a layer-spatial attention model, including a hard attention to select
    a CNN layer and a soft attention to achieve spatial localization within the selected
    layer. Attention maps are obtained from a Conv-LSTM architecture, where the layer
    attention uses the previous hidden states, and spatial attention uses both the
    selected layer and the previous hidden states. To enhance local feature selection,
    Xiong et al. [[103](#bib.bib103), [44](#bib.bib44)] designed a spatial attention
    module, shown in Fig. [9](#S3.F9 "Figure 9 ‣ 3.3.2 Attention strategy ‣ 3.3 Strategies
    for Improving Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning
    for Scene Classification: A Survey") (b), to generate attention masks. This attention
    masks of RGB and depth modalities are encouraged to be similar, and then learn
    the modal-consistent features.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 空间注意力策略。空间注意力策略沿输入特征图的高度和宽度推断注意力图，然后将注意力图与原始图像结合进行自适应特征细化。Joseph 等人 [[133](#bib.bib133)]
    提出了一个层级空间注意力模型，包括一个硬注意力来选择 CNN 层和一个软注意力来实现所选层内的空间定位。注意力图来自 Conv-LSTM 架构，其中层注意力使用前一个隐藏状态，空间注意力则同时使用所选层和前一个隐藏状态。为了增强局部特征选择，Xiong
    等人 [[103](#bib.bib103), [44](#bib.bib44)] 设计了一个空间注意力模块，如图 [9](#S3.F9 "图 9 ‣ 3.3.2
    注意力策略 ‣ 3.3 改进场景表示的策略 ‣ 3 深度学习方法 ‣ 深度学习在场景分类中的应用") (b)，以生成注意力掩膜。这些 RGB 和深度模态的注意力掩膜应尽可能相似，然后学习模态一致的特征。
- en: 3.3.3 Contextual strategy
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 上下文策略
- en: Contextual information (the correlations among image regions, and local features),
    and objects/scenes may provide beneficial information in disambiguating visual
    words [[134](#bib.bib134)]. However, convolution and pooling kernels are locally
    performed on image regions separately, and encoding technologies usually integrate
    multiple local features into an unstructured feature. As a result, contextual
    correlations among different regions have not been taken into account [[135](#bib.bib135)].
    To this end, contextual correlations have been further explored to focus on the
    global layout or local region coherence [[136](#bib.bib136)].
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文信息（图像区域之间的关联和局部特征），以及对象/场景可能在消除视觉词汇歧义方面提供有益的信息[[134](#bib.bib134)]。然而，卷积和池化核在图像区域上是局部执行的，编码技术通常将多个局部特征整合成一个非结构化特征。因此，不同区域之间的上下文相关性没有被考虑到[[135](#bib.bib135)]。为此，上下文相关性已被进一步探索，以关注全球布局或局部区域一致性[[136](#bib.bib136)]。
- en: 'The contextual relations can broadly be grouped into two major categories:
    1) spatial contextual relation: the correlations of neighboring regions, in which
    capturing spatial contextual relation usually encounters the problem of incomplete
    regions or noise caused by predefined grid patches, and 2) semantic contextual
    relation: the relations of salient regions. The network to extract semantic relations
    is often a two-stage framework (*i.e.,* detecting objects and classifying scenes).
    Therefore, accuracy is also influenced by object detection. Generally, there are
    three types of algorithms to capture contextual relations: 1) sequential model,
    like RNN [[137](#bib.bib137)] and LSTM [[95](#bib.bib95)], and 2) graph-related
    model, such as Markov Random Field (MRF) [[138](#bib.bib138), [139](#bib.bib139)],
    Correlated Topic Model (CTM) [[140](#bib.bib140), [129](#bib.bib129)] and graph
    convolution [[141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143)].'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文关系可以大致分为两大类：1) 空间上下文关系：相邻区域之间的关联，其中捕捉空间上下文关系通常遇到预定义网格补丁导致的不完整区域或噪声问题；2) 语义上下文关系：显著区域之间的关系。提取语义关系的网络通常是一个两阶段框架（*即，*检测对象和分类场景）。因此，准确性也受到对象检测的影响。一般来说，有三种算法来捕捉上下文关系：1)
    序列模型，如 RNN [[137](#bib.bib137)] 和 LSTM [[95](#bib.bib95)]，2) 图相关模型，如马尔可夫随机场（MRF）[[138](#bib.bib138),
    [139](#bib.bib139)]，相关主题模型（CTM）[[140](#bib.bib140), [129](#bib.bib129)] 和图卷积 [[141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143)]。
- en: '![Refer to caption](img/b7c9c60de3a83aec0a5bb7532959e1d4.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b7c9c60de3a83aec0a5bb7532959e1d4.png)'
- en: (a) Sequential model
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 序列模型
- en: '![Refer to caption](img/381360d70354bcb4287d0aa3922d426a.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/381360d70354bcb4287d0aa3922d426a.png)'
- en: (b) Graph-related model
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 图相关模型
- en: 'Figure 10: Illustrations of a sequence model and a graph model to related contextual
    information. (a) Local feature is extracted from each salient region via a CNN,
    and a bidirectional LSTM is used to model synchronously many-to-many local feature
    relations [[36](#bib.bib36)]. (b) Graph is constructed by assigning selected key
    features to graph nodes (including the center nodes, sub-center nodes and other
    nodes) [[144](#bib.bib144)].'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：序列模型和图模型的相关上下文信息插图。 (a) 通过 CNN 从每个显著区域提取局部特征，双向 LSTM 被用来同步建模许多对许多局部特征关系
    [[36](#bib.bib36)]。 (b) 通过将选定的关键特征分配给图节点（包括中心节点、子中心节点和其他节点）来构建图 [[144](#bib.bib144)]。
- en: 'Sequential Model. With the success of sequential models, such as RNN and LSTM,
    capturing the sequential information among local regions has shown promising performance
    for scene classification [[36](#bib.bib36)]. Spatial dependencies are captured
    from direct or indirect connections between each region and its surrounding neighbors.
    Zuo et al. [[27](#bib.bib27)] stacked multi-directional LSTM layers on the top
    of CONV feature maps to encode spatial contextual information in scene images.
    Furthermore, a hierarchical strategy was adopted to capture cross-scale contextual
    information. Like the work [[27](#bib.bib27)], Sun et al. [[24](#bib.bib24)] bypassed
    two sets of multi-directional LSTM layers on the CONV feature maps. In their framework,
    the outputs of all LSTM layers are concatenated to form a contextual representation.
    In addition, the works [[145](#bib.bib145), [36](#bib.bib36)] captured semantic
    contextual knowledge from variable salient regions. In [[145](#bib.bib145)], two
    types of representations, *i.e.,* COOR and SOOR, are proposed to describe object-to-object
    relations. Herein, COOR adapts the co-occurring frequency to represent the object-to-object
    relations, while SOOR is encoded with sequential model via regarding object sequences
    as sentences. Rooted in the work of Javed et al. [[146](#bib.bib146)], Laranjeira et
    al. [[36](#bib.bib36)] proposed a bidirectional LSTM to capture the contextual
    relations of regions of interest, as shown in Fig. [10](#S3.F10 "Figure 10 ‣ 3.3.3
    Contextual strategy ‣ 3.3 Strategies for Improving Scene Representation ‣ 3 Deep
    Learning based Methods ‣ Deep Learning for Scene Classification: A Survey") (a).
    Their model supports variable length sequences, because the number of object parts
    of each image are different.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '序列模型。随着序列模型，如 RNN 和 LSTM 的成功，捕捉局部区域之间的序列信息在场景分类中显示出有希望的性能 [[36](#bib.bib36)]。空间依赖性通过每个区域与其周围邻域之间的直接或间接连接来捕获。Zuo
    等人 [[27](#bib.bib27)] 在 CONV 特征图上堆叠了多方向 LSTM 层，以编码场景图像中的空间上下文信息。此外，采用了分层策略来捕获跨尺度的上下文信息。类似于
    [[27](#bib.bib27)] 的工作，Sun 等人 [[24](#bib.bib24)] 在 CONV 特征图上绕过了两组多方向 LSTM 层。在他们的框架中，所有
    LSTM 层的输出被串联形成上下文表示。此外，工作 [[145](#bib.bib145), [36](#bib.bib36)] 捕获了来自变动显著区域的语义上下文知识。在
    [[145](#bib.bib145)] 中，提出了两种表示，即 COOR 和 SOOR，以描述对象之间的关系。这里，COOR 采用共现频率来表示对象之间的关系，而
    SOOR 则通过将对象序列视为句子来使用序列模型进行编码。基于 Javed 等人 [[146](#bib.bib146)] 的工作，Laranjeira 等人
    [[36](#bib.bib36)] 提出了双向 LSTM 来捕获感兴趣区域的上下文关系，如图 [10](#S3.F10 "Figure 10 ‣ 3.3.3
    Contextual strategy ‣ 3.3 Strategies for Improving Scene Representation ‣ 3 Deep
    Learning based Methods ‣ Deep Learning for Scene Classification: A Survey") (a)
    所示。他们的模型支持可变长度的序列，因为每张图像的对象部件数量不同。'
- en: 'Graph-related Model. The sequential models often simplify the contextual relations,
    while graph-related models can explore more complicated structural layouts. Song et
    al. [[147](#bib.bib147)] proposed a joint context model that uses MRFs to combine
    multiple scales, spatial relations, and multiple features among neighboring semantic
    multinomials, showing that this method can discover consistent co-occurrence patterns
    and filter out noisy ones. Based on CTM, Wei et al. [[129](#bib.bib129)] captured
    relations among latent themes as a semantic feature, *i.e.,* corrected topic vector
    (CTV). Later, with the development of Graph Neural Network (GNN) [[142](#bib.bib142),
    [143](#bib.bib143)], graph convolution has become increasingly popular to model
    contextual information for scene classification. Yuan et al. [[144](#bib.bib144)]
    used spectral graph convolution to mine the relations among the selected local
    CNN features, as shown in Fig. [10](#S3.F10 "Figure 10 ‣ 3.3.3 Contextual strategy
    ‣ 3.3 Strategies for Improving Scene Representation ‣ 3 Deep Learning based Methods
    ‣ Deep Learning for Scene Classification: A Survey") (b). To use the complementary
    cues of multiple modalities, Yuan et al. also considered the inter-modality correlations
    of RGB and depth modalities through a cross-modal graph. Chen et al. [[45](#bib.bib45)]
    used graph convolution [[148](#bib.bib148)] to model the more complex spatial
    structural layouts via pre-defining the features of discriminative regions as
    graph nodes. However, the spatial relation overlooks the semantic meanings of
    regions. To address this issue, Chen et al. also defined a similarity subgraph
    as a complement to the spatial subgraph.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图相关模型。顺序模型通常简化了上下文关系，而图相关模型可以探索更复杂的结构布局。宋等人[[147](#bib.bib147)] 提出了一个联合上下文模型，该模型使用马尔可夫随机场（MRF）来结合多个尺度、空间关系以及邻近语义多项式之间的多个特征，显示出这种方法可以发现一致的共现模式并过滤掉噪声。基于CTM，魏等人[[129](#bib.bib129)]
    捕捉了潜在主题之间的关系作为语义特征，即纠正的主题向量（CTV）。后来，随着图神经网络（GNN）[[142](#bib.bib142), [143](#bib.bib143)]
    的发展，图卷积在场景分类中的上下文信息建模变得越来越受欢迎。袁等人[[144](#bib.bib144)] 使用谱图卷积来挖掘所选择的局部CNN特征之间的关系，如图[10](#S3.F10
    "图 10 ‣ 3.3.3 上下文策略 ‣ 3.3 改进场景表示的策略 ‣ 3 深度学习方法 ‣ 深度学习在场景分类中的应用：综述")（b）所示。为了利用多模态的互补线索，袁等人还通过跨模态图考虑了RGB和深度模态的互模态相关性。陈等人[[45](#bib.bib45)]
    使用图卷积[[148](#bib.bib148)] 通过预定义具有辨别性的区域特征作为图节点，来建模更复杂的空间结构布局。然而，空间关系忽视了区域的语义含义。为了解决这个问题，陈等人还定义了一个相似性子图作为空间子图的补充。
- en: 3.3.4 Regularization strategy
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 正则化策略
- en: The training classifier not only requires a classification loss function, but
    it may also need multi-task learning with different regularization terms to reduce
    generalization error. The regularization strategies for scene classification mainly
    include sparse regularization, structured regularization, and supervised regularization.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 训练分类器不仅需要分类损失函数，还可能需要通过不同的正则化项进行多任务学习，以减少泛化误差。场景分类的正则化策略主要包括稀疏正则化、结构化正则化和监督正则化。
- en: Sparse Regularization. Sparse regularization is a technique to reduce the complexity
    of the model to prevent overfitting and even improve generalization ability. Many
    works [[87](#bib.bib87), [149](#bib.bib149), [40](#bib.bib40), [22](#bib.bib22),
    [150](#bib.bib150)] include $\ell_{0}$, $\ell_{1}$, or $\ell_{2}$ norms to the
    base loss function for learning sparse features. For example, the sparse reconstruction
    term in [[87](#bib.bib87)] encourages the learned representations to be significantly
    informative. The loss in [[22](#bib.bib22)] combines the strength of the Mahalanobis
    and Euclidean distances to balance the accuracy and the generalization ability.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏正则化。稀疏正则化是一种减少模型复杂性以防止过拟合甚至提高泛化能力的技术。许多工作[[87](#bib.bib87), [149](#bib.bib149),
    [40](#bib.bib40), [22](#bib.bib22), [150](#bib.bib150)] 在基础损失函数中包含了$\ell_{0}$、$\ell_{1}$或$\ell_{2}$范数以学习稀疏特征。例如，[[87](#bib.bib87)]中的稀疏重建项鼓励学习到的表示具有显著的信息性。[[22](#bib.bib22)]中的损失结合了马氏距离和欧几里得距离的优势，以平衡准确性和泛化能力。
- en: Structured Regularization. Minimizing the triplet loss function minimizes the
    distance between the anchor and positive features with the same class labels while
    maximizing the distance between the anchor and negative features with one different
    class labels. In addition, according to the maximum margin theory in learning
    [[151](#bib.bib151)], hinge distance focus on the hard training samples. Hence,
    many research efforts [[149](#bib.bib149), [50](#bib.bib50), [152](#bib.bib152),
    [103](#bib.bib103), [44](#bib.bib44)] have utilized structured regularization
    of the triplet loss with hinge distance to learn robust feature representations.
    The structured regularization term is $\sum_{a,p,n}{max(d(x_{a},x_{p})-d(x_{a},x_{n})+\alpha,0)}$,
    where $x_{a},x_{p},x_{n}$ are anchor, positive, negative features, and $\alpha$
    is an adjustable parameter, while the function $d(x,y)$ denotes calculating a
    distance of $x$ and $y$. The structured regularization term promotes exemplar
    selection, while it also ignores noisy training examples that might overwhelm
    the useful discriminative patterns[[149](#bib.bib149), [103](#bib.bib103)].
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化正则化。最小化三元组损失函数可以最小化锚点和正样本之间的距离（同一类别标签），同时最大化锚点和负样本之间的距离（不同类别标签）。此外，根据学习中的最大间隔理论[[151](#bib.bib151)]，铰链距离集中于困难的训练样本。因此，许多研究努力[[149](#bib.bib149),
    [50](#bib.bib50), [152](#bib.bib152), [103](#bib.bib103), [44](#bib.bib44)]利用了三元组损失与铰链距离的结构化正则化来学习稳健的特征表示。结构化正则化项为$\sum_{a,p,n}{max(d(x_{a},x_{p})-d(x_{a},x_{n})+\alpha,0)}$，其中$x_{a},x_{p},x_{n}$分别为锚点、正样本和负样本特征，$\alpha$为可调参数，而函数$d(x,y)$表示计算$x$和$y$之间的距离。结构化正则化项促进了示例选择，同时也忽略了可能压倒有用区分模式的噪声训练样本[[149](#bib.bib149),
    [103](#bib.bib103)]。
- en: Supervised Regularization. Supervised regularization uses the label information
    for tuning the intermediate feature maps. The supervised regularization is generally
    expressed in terms of $\sum_{i}{d(y_{i},f(x_{i}))}$, where $x_{i}andy_{i}$ denote
    the middle-layer activated features and real label of the image $i$, respectively,
    and $f(x_{i})$ is a predicted label. For example, Guo et al. [[40](#bib.bib40)]
    utilized an auxiliary loss to directly propagate the label information to the
    CONV layers, and thus accurately captures the information of local objects and
    fine structures in the CONV layers. Similarly, these alternatives [[103](#bib.bib103),
    [144](#bib.bib144), [44](#bib.bib44)] used supervised regularization to learn
    modal-specific features.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 监督正则化。监督正则化使用标签信息来调整中间特征图。监督正则化通常表示为$\sum_{i}{d(y_{i},f(x_{i}))}$，其中$x_{i}$和$y_{i}$分别表示图像$i$的中间层激活特征和真实标签，而$f(x_{i})$是预测标签。例如，Guo等人[[40](#bib.bib40)]利用辅助损失直接将标签信息传播到CONV层，从而准确捕捉CONV层中局部物体和细微结构的信息。类似地，这些替代方法[[103](#bib.bib103),
    [144](#bib.bib144), [44](#bib.bib44)]使用监督正则化来学习特定模态的特征。
- en: Others. Extracting discriminative features by incorporating different regularization
    techniques has been always a mainstream topic in scene classification. For example,
    label consistent regularization [[87](#bib.bib87)] guarantees that inputs from
    different categories have discriminative responses. The shareable constraint in
    [[149](#bib.bib149)] can learn a flexible number of filters to represent common
    patterns across different categories. Clustering loss in [[124](#bib.bib124)]
    is utilized to further fine-tune confusing clusters to overcome the intra-class
    variation issues inherent. Since assigning soft labels to the samples cause a
    degree of ambiguity, which reaps high benefits when increasing the number of scene
    categories [[153](#bib.bib153)], Wang et al. [[41](#bib.bib41)] improved generalization
    ability by exploiting soft labels contained in knowledge networks as a bias term
    of the loss function. Noteworthily, optimizing proper loss function can pick up
    effective patches for image classification. In fast RCNN [[51](#bib.bib51)] and
    Faster RCNN [[52](#bib.bib52)], regression loss is used to learn effective region
    proposals. Wu et al. [[33](#bib.bib33)] adopted one-class SVMs [[154](#bib.bib154)]
    as discriminative models to get meta-objects. Inspired by MANTRA [[155](#bib.bib155)],
    the main intuition in [[34](#bib.bib34)] is to equip each possible output with
    pairs of latent variables, *i.e.,* top positive and negative patches, via optimizing
    max$+$min prediction problem.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其他。通过结合不同的正则化技术提取具有区分性的特征一直是场景分类中的主流话题。例如，标签一致性正则化[[87](#bib.bib87)]保证了来自不同类别的输入具有区分性的响应。[[149](#bib.bib149)]中的共享约束可以学习灵活数量的过滤器，以表示不同类别间的共通模式。[[124](#bib.bib124)]中的聚类损失用于进一步微调混淆的聚类，以克服固有的类内变化问题。由于将软标签分配给样本会导致一定程度的模糊性，这在增加场景类别数量时会带来很高的收益[[153](#bib.bib153)]，Wang
    等人[[41](#bib.bib41)]通过利用知识网络中的软标签作为损失函数的偏置项，提高了泛化能力。值得注意的是，优化适当的损失函数可以为图像分类选择有效的补丁。在
    fast RCNN[[51](#bib.bib51)]和 Faster RCNN[[52](#bib.bib52)]中，回归损失用于学习有效的区域提议。Wu
    等人[[33](#bib.bib33)]采用了单类 SVM[[154](#bib.bib154)]作为区分模型来获取元对象。受到 MANTRA[[155](#bib.bib155)]的启发，[[34](#bib.bib34)]中的主要直觉是通过优化
    max$+$min 预测问题，为每个可能的输出配备成对的潜在变量，即*正面*和*负面*补丁。
- en: Nearly all multi-task learning approaches using regularization aim to find a
    trade-off among conflicting requirements, *e.g.,* accuracy, generalization robustness,
    and efficiency. Researchers apply completely different supervision information
    to a variety of auxiliary tasks in an effort to facilitate the convergence of
    the major scene classification task [[40](#bib.bib40)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有使用正则化的多任务学习方法都旨在找到各冲突需求之间的折中，如*准确性*、*泛化鲁棒性*和*效率*。研究人员应用完全不同的监督信息于各种辅助任务，以促进主要场景分类任务的收敛[[40](#bib.bib40)]。
- en: 3.4 RGB-D Scene Classification
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 RGB-D 场景分类
- en: RGB modality provides the intensity of the colors and texture cues, while depth
    modality carries information regarding the distance of the scene surfaces from
    a viewpoint. The depth information is invariant to lighting and color variations,
    and contains geometrical and shape cues, which is useful for scene representation
    [[156](#bib.bib156), [157](#bib.bib157), [158](#bib.bib158)]. Moreover, HHA data [[73](#bib.bib73)],
    an encoding result of depth image, depth information presents a certain color
    modality, which somewhat is similar to the RGB image. Hence, some CNNs trained
    on RGB images can transfer their knowledge and be used on HHA data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: RGB 模态提供了颜色的强度和纹理线索，而深度模态则携带了场景表面距离观察点的信息。深度信息对光照和颜色变化不变，并包含几何和形状线索，这对场景表示非常有用[[156](#bib.bib156),
    [157](#bib.bib157), [158](#bib.bib158)]。此外，HHA 数据[[73](#bib.bib73)]，即深度图像的编码结果，深度信息展现出一定的颜色模态，这在某种程度上类似于
    RGB 图像。因此，一些在 RGB 图像上训练的 CNN 可以将其知识迁移到 HHA 数据上。
- en: 'The depth information of RGB-D image can further improve the performance of
    CNN models compared to RGB images [[89](#bib.bib89)]. For the task of RGB-D scene
    classification, except for exploring suitable RGB features, described in Section [3.2](#S3.SS2
    "3.2 CNN based Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning
    for Scene Classification: A Survey"), there exists another two main problems,
    *i.e.,* 1) how to extract depth-specific features and 2) how to properly fuse
    features of RGB and depth modalities.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '与RGB图像相比，RGB-D图像的深度信息可以进一步提升CNN模型的性能[[89](#bib.bib89)]。对于RGB-D场景分类任务，除了探索适合的RGB特征（见第[3.2节](#S3.SS2
    "3.2 CNN based Scene Representation ‣ 3 Deep Learning based Methods ‣ Deep Learning
    for Scene Classification: A Survey")），还存在另外两个主要问题，*即*：1）如何提取深度特征，2）如何合理融合RGB和深度模态的特征。'
- en: 3.4.1 Depth-specific feature learning
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 深度特征学习
- en: Depth information is usually scarce compared to RGB data. Therefore, it is non-trivial
    to train CNNs only on limited depth data to achieve depth-specific models [[89](#bib.bib89)],
    *i.e.,* depth-CNN. Hence, different training strategies are employed to train
    CNNs using limited amount of depth images.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与RGB数据相比，深度信息通常较少。因此，仅使用有限的深度数据训练CNN以实现深度特定模型[[89](#bib.bib89)]并非易事，*即*：深度CNN。因此，采用了不同的训练策略来使用有限数量的深度图像训练CNN。
- en: Fine-tuning RGB-CNNs for depth images. Due to the availability of RGB data,
    many models [[49](#bib.bib49), [50](#bib.bib50), [46](#bib.bib46)] are first pre-trained
    on large-scale RGB datasets, such as ImageNet and Places, followed by fine-tuning
    on depth data. Fine-tuning only updates the last few FC layers, while the parameters
    of the previous layers are not adjusted. Therefore, the fine-tuned model’s layers
    do not fully leverage depth data [[89](#bib.bib89)]. However, abstract representations
    of early CONV layers play a crucial role in computing deep features using different
    modalities. Weakly-supervised learning and semi-supervised learning can enforce
    explicit adaptation in the previous layers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度图像的RGB-CNN微调。由于RGB数据的可用性，许多模型[[49](#bib.bib49)、[50](#bib.bib50)、[46](#bib.bib46)]首先在大规模RGB数据集（如ImageNet和Places）上进行预训练，然后在深度数据上进行微调。微调只更新最后几层全连接层（FC层），而前面的层参数不做调整。因此，微调后的模型层未能充分利用深度数据[[89](#bib.bib89)]。然而，早期卷积层（CONV层）的抽象表示在使用不同模态计算深度特征中起着关键作用。弱监督学习和半监督学习可以在前面层中强制显式适应。
- en: Weak-supervised learning with patches of depth images. Song et al. [[89](#bib.bib89),
    [47](#bib.bib47)] proposed to learn depth features from scratch using weakly supervised
    learning. Song et al. [[89](#bib.bib89)] pointed out that the diversity and complexity
    of patterns in the depth images are significantly lower than those in the RGB
    images. Therefore, they designed a Depth-CNN (DCNN) with fewer layers for depth
    features extraction. They also trained the DCNN by three strategies of freezing,
    fine-tuning, and training from scratch to adequately capture depth information.
    Nevertheless, weakly-supervised learning is sensitive to the noise in the training
    data. As a result, the extracted features may not have good discriminative quality
    for classification.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度图像补丁的弱监督学习。Song等人[[89](#bib.bib89)、[47](#bib.bib47)]提出了从头开始使用弱监督学习来学习深度特征。Song等人[[89](#bib.bib89)]指出，深度图像中的模式多样性和复杂性显著低于RGB图像。因此，他们设计了一个层数较少的深度卷积神经网络（DCNN）来提取深度特征。他们还通过冻结、微调和从头训练三种策略来训练DCNN，以充分捕捉深度信息。然而，弱监督学习对训练数据中的噪声很敏感。因此，提取的特征可能在分类中不具备良好的区分质量。
- en: Semi-supervised learning with unlabeled images. Due to the convenient collection
    of unlabeled RGB-D data, semi-supervised learning can also be employed in the
    training of CNNs with a limited number of labeled samples compared to very large
    size of unlabeled data [[48](#bib.bib48), [158](#bib.bib158)]. Cheng et al. [[158](#bib.bib158)]
    trained a CNN using a very limited number of labeled RGB-D images while a massive
    amount of unlabeled RGB-D images via a co-training algorithm to preserve diversity.
    Subsequently, Du et al. [[48](#bib.bib48)] developed an encoder-decoder model
    to construct paired complementary-modal data of the input. In particular, the
    encoder is used as a modality-specific network to extract specific features for
    the subsequent classification task.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用未标记图像的半监督学习。由于未标记RGB-D数据的方便收集，相较于大量的未标记数据，半监督学习也可以用于有限标记样本的CNN训练 [[48](#bib.bib48),
    [158](#bib.bib158)]。Cheng 等 [[158](#bib.bib158)] 使用少量标记的RGB-D图像和大量未标记的RGB-D图像通过共训练算法来训练CNN，以保留多样性。随后，Du
    等 [[48](#bib.bib48)] 开发了一种编码器-解码器模型，以构建输入的配对互补模态数据。特别是，编码器作为模态特定网络提取特定特征用于后续的分类任务。
- en: 3.4.2 Multiple modality fusion
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 多模态融合
- en: '![Refer to caption](img/df74223bf5ed4d585286655de016832d.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/df74223bf5ed4d585286655de016832d.png)'
- en: 'Figure 11: Illustrations of multi-modal feature learning. (a) Three popular
    ways to achieve feature combination: directly concatenate features, combine weighted
    features and combine features with linear converting. (b) Three methods to achieve
    modal-consistent: minimize the pairwise distances between modalities [[50](#bib.bib50)];
    encourage the attention maps of modalities similar [[103](#bib.bib103)]; minimize
    the distances between modalities [[44](#bib.bib44), [152](#bib.bib152)]. (c) Three
    strategies to achieve modal-distinctive: learn the model structure via triplet
    loss [[50](#bib.bib50), [152](#bib.bib152)]; use label information to guide modal-specific
    learning [[103](#bib.bib103), [144](#bib.bib144)]; minimize cosine similarity
    between modalities [[44](#bib.bib44)].'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：多模态特征学习的示意图。 (a) 实现特征组合的三种流行方式：直接连接特征、加权特征组合和线性转换特征组合。 (b) 实现模态一致性的三种方法：最小化模态之间的配对距离
    [[50](#bib.bib50)]; 鼓励模态的注意力图相似 [[103](#bib.bib103)]; 最小化模态之间的距离 [[44](#bib.bib44),
    [152](#bib.bib152)]。 (c) 实现模态特异性的三种策略：通过三元组损失学习模型结构 [[50](#bib.bib50), [152](#bib.bib152)];
    使用标签信息指导模态特定学习 [[103](#bib.bib103), [144](#bib.bib144)]; 最小化模态之间的余弦相似度 [[44](#bib.bib44)]。
- en: 'Various modality fusion methods [[159](#bib.bib159), [152](#bib.bib152), [44](#bib.bib44)]
    have been put forward to combine the information of different modalities to further
    enhance the performance of the classification model. The fusion strategies are
    mainly divided into three categories, *i.e.,* feature-level modal combination,
    consistent feature based fusion, and distinctive feature based fusion. Fig. [11](#S3.F11
    "Figure 11 ‣ 3.4.2 Multiple modality fusion ‣ 3.4 RGB-D Scene Classification ‣
    3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    shows illustrations of the late three categories. Despite the existence of different
    fusion categories, some works[[46](#bib.bib46), [152](#bib.bib152), [44](#bib.bib44)]
    combine multiple fusion strategies to achieve better performance for scene classification.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '各种模态融合方法 [[159](#bib.bib159), [152](#bib.bib152), [44](#bib.bib44)] 已被提出，以结合不同模态的信息，从而进一步提高分类模型的性能。融合策略主要分为三类，即特征级模态组合、一致性特征融合和特异性特征融合。图
    [11](#S3.F11 "Figure 11 ‣ 3.4.2 Multiple modality fusion ‣ 3.4 RGB-D Scene Classification
    ‣ 3 Deep Learning based Methods ‣ Deep Learning for Scene Classification: A Survey")
    展示了后两类的示意图。尽管存在不同的融合类别，一些研究 [[46](#bib.bib46), [152](#bib.bib152), [44](#bib.bib44)]
    结合多种融合策略以实现更好的场景分类性能。'
- en: Feature-level modal combination. Song et al. [[47](#bib.bib47)] proposed a multi-modal
    combination approach to select discriminative combinations of layers from different
    source models. They concatenated RGB and depth features for not losing the correlation
    between the RGB and depth data. Reducing the redundancy of features can significantly
    improve the performance when RGB and depth features have correlations; especially,
    in the case of extracting depth features merely via RGB-CNNs [[89](#bib.bib89)].
    Because of such correlation, direct concatenation of features may result in redundancy
    of information. To avoid this issue, Du et al. [[48](#bib.bib48)] performed global
    average pooling to reduce the feature dimensions after concatenating modality-specific
    features. Wang et al. [[46](#bib.bib46)] used the modality regularization based
    on exclusive group lasso to ensure feature sparsity and co-existence, while features
    within a modality are encouraged to compete. Li et al. [[96](#bib.bib96)] used
    an attention module to discern discriminative semantic cues from intra- and cross-modalities.
    Moreover, Cheng et al. [[49](#bib.bib49)] proposed a gated fusion layer to adjust
    the RGB and depth contributions on image pixels.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 特征层级模态组合。Song 等人[[47](#bib.bib47)] 提出了多模态组合方法，以选择来自不同源模型的区分组合层。他们将 RGB 和深度特征连接起来，以保持
    RGB 和深度数据之间的相关性。减少特征的冗余可以显著提高性能，特别是在 RGB 和深度特征之间存在相关性的情况下；尤其是在通过 RGB-CNNs[[89](#bib.bib89)]
    仅提取深度特征时。由于这种相关性，直接连接特征可能导致信息冗余。为了避免这种问题，Du 等人[[48](#bib.bib48)] 在连接模态特定特征后执行全局平均池化，以减少特征维度。Wang
    等人[[46](#bib.bib46)] 基于排他组套索的模态正则化来确保特征稀疏性和共存，同时鼓励模态内的特征竞争。Li 等人[[96](#bib.bib96)]
    使用注意力模块来辨别来自模态内和跨模态的区分语义线索。此外，Cheng 等人[[49](#bib.bib49)] 提出了一个门控融合层，以调整 RGB 和深度对图像像素的贡献。
- en: Consistent-feature based modal fusion. Images may suffer from missing information
    or noise pollution so that multi-modal features are not consistent, hence it is
    essential to exploit the correlation between different modalities to exclude such
    issue. To drive feature consistency of different modalities, Zhu et al. [[50](#bib.bib50)]
    introduced an inter-modality correlation term to minimize pairwise distances of
    two modalities from the same class, while maximize pairwise distances from different
    classes. Zheng et al. [[160](#bib.bib160)] used multi-task metric learning to
    learn linear transformations of RGB and depth features, making full use of inter-modal
    relations. Li et al. [[152](#bib.bib152)] learned a correlative embedding module
    between the RGB and depth features inspired by Canonical Correlation Analysis [[161](#bib.bib161),
    [162](#bib.bib162)]. Xiong et al. [[103](#bib.bib103), [44](#bib.bib44)] proposed
    a learning approach to encourage two modal-specific networks to focus on features
    with similar spatial positions to learn more discriminative modal-consistent features.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一致特征基础的模态融合。图像可能会因信息缺失或噪声污染而导致多模态特征不一致，因此，利用不同模态之间的相关性来排除这种问题至关重要。为了驱动不同模态的特征一致性，Zhu
    等人[[50](#bib.bib50)] 引入了一个模态间相关性项，以最小化同一类别中两个模态的成对距离，同时最大化不同类别中的成对距离。Zheng 等人[[160](#bib.bib160)]
    采用了多任务度量学习来学习 RGB 和深度特征的线性变换，充分利用了模态间关系。Li 等人[[152](#bib.bib152)] 借鉴典型相关分析[[161](#bib.bib161),
    [162](#bib.bib162)] 学习了 RGB 和深度特征之间的相关嵌入模块。Xiong 等人[[103](#bib.bib103), [44](#bib.bib44)]
    提出了一种学习方法，鼓励两个特定模态的网络关注具有相似空间位置的特征，从而学习更具区分性的模态一致特征。
- en: Distinctive-feature based modal fusion. In addition to constructing multimodal
    consistent features, features can also be processed separately to increase discriminative
    capability. For instance, Li et al. [[152](#bib.bib152)] and Zhu et al. [[50](#bib.bib50)]
    adopted structured regularization in the triplet loss function, in which to encourage
    the model to learn the modal-specific features under the supervision of this regularization.
    Li et al. [[152](#bib.bib152)] designed a distinctive embedding module for each
    modality to learn distinctive features. Using labels for separate supervision
    of model-specific representation learning for each modality is also another technique
    of individual processing [[103](#bib.bib103), [144](#bib.bib144), [44](#bib.bib44)].
    Moreover, by minimizing the feature correlation, Xiong et al. [[44](#bib.bib44)]
    learned the modal distinctive features as the RGB and depth modalities have different
    characteristics.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 基于独特特征的模态融合。除了构建多模态一致特征外，还可以单独处理特征以提高区分能力。例如，Li 等 [[152](#bib.bib152)] 和 Zhu
    等 [[50](#bib.bib50)] 在三重损失函数中采用结构化正则化，鼓励模型在这种正则化的监督下学习模态特定特征。Li 等 [[152](#bib.bib152)]
    为每种模态设计了独特的嵌入模块，以学习独特特征。使用标签对每种模态的模型特定表示学习进行单独监督也是另一种单独处理的技术 [[103](#bib.bib103),
    [144](#bib.bib144), [44](#bib.bib44)]。此外，通过最小化特征相关性，Xiong 等 [[44](#bib.bib44)]
    学习了模态独特特征，因为 RGB 和深度模态具有不同的特性。
- en: 4 Performance Comparison
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 性能比较
- en: 4.1 Performance on RGB scene data
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 RGB 场景数据的性能
- en: 'TABLE II: Performance (%) summarization of some representative methods on popular
    benchmark datasets. All scores are quoted directly from the original papers. For
    each dataset, the highest three classification scores are highlighted. Some abbreviations.
    Column “Scheme”: Whole Image (WI), Dense Patches (DP), Regional Proposals (RP);
    Column “Init.”(Initialization): ImageNet (IN), Places205 (PL1), Places365 (PL2).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：一些代表性方法在流行基准数据集上的性能 (%) 总结。所有分数均直接引用自原始论文。对于每个数据集，突出显示前三个分类分数。部分缩写：列“方案”：全图像
    (WI)，密集补丁 (DP)，区域提议 (RP)；列“初始化”：ImageNet (IN)，Places205 (PL1)，Places365 (PL2)。
- en: '| Group | Method | Input information | Feature Information | Architecture |
    Results (RGB) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | 方法 | 输入信息 | 特征信息 | 结构 | 结果（RGB） |'
- en: '|  | Scale | Scheme | Data Aug. | Aggregation | Dimension | Init. | Backbone
    | Scene15 | MIT67 | SUN397 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | 比例 | 方案 | 数据增强 | 聚合 | 维度 | 初始化 | 主干 | Scene15 | MIT67 | SUN397 |'
- en: '| Global CNN features based methods | ImageNet-CNN [[17](#bib.bib17)] | Single
    | WI | ${\times}$ | pooling | 4,096 | IN | AlexNet | 84.2 | 56.8 | 42.6 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 基于全球 CNN 特征的方法 | ImageNet-CNN [[17](#bib.bib17)] | 单一 | WI | ${\times}$ |
    池化 | 4,096 | IN | AlexNet | 84.2 | 56.8 | 42.6 |'
- en: '| PL1-CNN [[18](#bib.bib18)] | Single | WI | ${\times}$ | pooling | 4,096 |
    PL1 | VGGNet | 91.6 | 79.8 | 62.0 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| PL1-CNN [[18](#bib.bib18)] | 单一 | WI | ${\times}$ | 池化 | 4,096 | PL1 | VGGNet
    | 91.6 | 79.8 | 62.0 |'
- en: '| PL2-CNN [[25](#bib.bib25)] | Single | WI | ${\times}$ | pooling | 4,096 |
    PL2 | VGGNet | 92.0 | 76.5 | 63.2 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| PL2-CNN [[25](#bib.bib25)] | 单一 | WI | ${\times}$ | 池化 | 4,096 | PL2 | VGGNet
    | 92.0 | 76.5 | 63.2 |'
- en: '| S2ICA [[26](#bib.bib26)] | Multi | DP | ${\surd}$ | pooling | 8,192 | IN
    | AlexNet | 93.1 | 71.2 | $-$ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| S2ICA [[26](#bib.bib26)] | 多重 | DP | ${\surd}$ | 池化 | 8,192 | IN | AlexNet
    | 93.1 | 71.2 | $-$ |'
- en: '|  | GAP-CNN [[23](#bib.bib23)] | Single | WI | ${\surd}$ | GAP | 4,096 | PL1
    | GoogLeNet | 88.3 | 66.6 | 51.3 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | GAP-CNN [[23](#bib.bib23)] | 单一 | WI | ${\surd}$ | GAP | 4,096 | PL1 |
    GoogLeNet | 88.3 | 66.6 | 51.3 |'
- en: '|  | InterActive [[99](#bib.bib99)] | Single | WI | ${\times}$ | pooling |
    4,096 | IN | VGG19 | $-$ | 78.7 | 63.0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | InterActive [[99](#bib.bib99)] | 单一 | WI | ${\times}$ | 池化 | 4,096 | IN
    | VGG19 | $-$ | 78.7 | 63.0 |'
- en: '|  | C-HLSTM [[27](#bib.bib27)] | Multi | WI | ${\surd}$ | LSTM | 4,096 | PL1
    | AlexNet | $-$ | 75.7 | 60.3 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | C-HLSTM [[27](#bib.bib27)] | 多重 | WI | ${\surd}$ | LSTM | 4,096 | PL1
    | AlexNet | $-$ | 75.7 | 60.3 |'
- en: '|  | DL-CNN [[22](#bib.bib22)] | Single | WI | ${\times}$ | DL | $-$ | PL1
    | VGGNet | 96.0 | 86.4 | 70.1 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | DL-CNN [[22](#bib.bib22)] | 单一 | WI | ${\times}$ | DL | $-$ | PL1 | VGGNet
    | 96.0 | 86.4 | 70.1 |'
- en: '| Spatially invariant features based methods | SCFVC [[83](#bib.bib83)] | Single
    | DP | ${\times}$ | FV | 200,000 | IN | AlexNet | $-$ | 68.2 | $-$ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 基于空间不变特征的方法 | SCFVC [[83](#bib.bib83)] | 单一 | DP | ${\times}$ | FV | 200,000
    | IN | AlexNet | $-$ | 68.2 | $-$ |'
- en: '| MOP-CNN [[28](#bib.bib28)] | Multi | DP | ${\times}$ | VLAD | 12,288 | IN
    | AlexNet | $-$ | 68.9 | 52.0 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| MOP-CNN [[28](#bib.bib28)] | 多重 | DP | ${\times}$ | VLAD | 12,288 | IN |
    AlexNet | $-$ | 68.9 | 52.0 |'
- en: '| DSP [[102](#bib.bib102)] | Multi | WI | ${\times}$ | FV | 12,288 | IN | VGGNet
    | 91.8 | 78.3 | 59.8 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| DSP [[102](#bib.bib102)] | 多重 | WI | ${\times}$ | FV | 12,288 | IN | VGGNet
    | 91.8 | 78.3 | 59.8 |'
- en: '| MPP-CNN [[29](#bib.bib29)] | Multi | RP | ${\times}$ | FV | 65,536 | IN |
    AlexNet | $-$ | 80.8 | $-$ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| MPP-CNN [[29](#bib.bib29)] | 多 | RP | ${\times}$ | FV | 65,536 | IN | AlexNet
    | $-$ | 80.8 | $-$ |'
- en: '| SFV [[30](#bib.bib30)] | Multi | DP, WI | ${\times}$ | FV | 9,216 | IN |
    AlexNet | $-$ | 72.8 | 54.4 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| SFV [[30](#bib.bib30)] | 多 | DP, WI | ${\times}$ | FV | 9,216 | IN | AlexNet
    | $-$ | 72.8 | 54.4 |'
- en: '| FV-CNN [[82](#bib.bib82)] | Multi | RP, WI | ${\times}$ | FV | 4,096 | IN
    | VGGNet | $-$ | 81.0 | $-$ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| FV-CNN [[82](#bib.bib82)] | 多 | RP, WI | ${\times}$ | FV | 4,096 | IN | VGGNet
    | $-$ | 81.0 | $-$ |'
- en: '| LatMoG [[58](#bib.bib58)] | Multi | RP | ${\times}$ | FV | $-$ | IN | AlexNet
    | $-$ | 69.1 | $-$ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| LatMoG [[58](#bib.bib58)] | 多 | RP | ${\times}$ | FV | $-$ | IN | AlexNet
    | $-$ | 69.1 | $-$ |'
- en: '| DUCA [[20](#bib.bib20)] | Single | DP | ${\surd}$ | CSMC | 4,096 | IN | AlexNet
    | 94.5 | 78.8 | $-$ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| DUCA [[20](#bib.bib20)] | 单一 | DP | ${\surd}$ | CSMC | 4,096 | IN | AlexNet
    | 94.5 | 78.8 | $-$ |'
- en: '| D3 [[163](#bib.bib163)] | Single | DP | ${\times}$ | D3, FV | 1,048,576 |
    IN | VGG16 | 92.8 | 77.1 | 61.5 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| D3 [[163](#bib.bib163)] | 单一 | DP | ${\times}$ | D3, FV | 1,048,576 | IN
    | VGG16 | 92.8 | 77.1 | 61.5 |'
- en: '| MFA-FS [[42](#bib.bib42)] | Multi | DP | ${\times}$ | MFA-FV | 5,000 | IN
    | VGGNet | $-$ | 81.4 | 63.3 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| MFA-FS [[42](#bib.bib42)] | 多 | DP | ${\times}$ | MFA-FV | 5,000 | IN | VGGNet
    | $-$ | 81.4 | 63.3 |'
- en: '| CTV [[129](#bib.bib129)] | Multi | WI | ${\times}$ | CTV | $-$ | PL1 | AlexNet
    | $-$ | 73.9 | 58.4 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| CTV [[129](#bib.bib129)] | 多 | WI | ${\times}$ | CTV | $-$ | PL1 | AlexNet
    | $-$ | 73.9 | 58.4 |'
- en: '| MFAFVNet [[31](#bib.bib31)] | Multi | DP | ${\times}$ | MFA-FV | 500,000
    | IN | VGG19 | $-$ | 82.7 | 64.6 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| MFAFVNet [[31](#bib.bib31)] | 多 | DP | ${\times}$ | MFA-FV | 500,000 | IN
    | VGG19 | $-$ | 82.7 | 64.6 |'
- en: '| EMFS [[147](#bib.bib147)] | Multi | DP | ${\times}$ | SM | 4,096 | IN, PL2
    | VGGNet | $-$ | 86.5 | 72.6 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| EMFS [[147](#bib.bib147)] | 多 | DP | ${\times}$ | SM | 4,096 | IN, PL2 |
    VGGNet | $-$ | 86.5 | 72.6 |'
- en: '| VSAD [[32](#bib.bib32)] | Multi | DP | ${\surd}$ | VSAD | 25,600 | IN, PL1
    | VGG16 | $-$ | 86.1 | 72.0 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| VSAD [[32](#bib.bib32)] | 多 | DP | ${\surd}$ | VSAD | 25,600 | IN, PL1 |
    VGG16 | $-$ | 86.1 | 72.0 |'
- en: '| LLC [[150](#bib.bib150)] | Single | DP | ${\times}$ | SSE | 3,072 | IN, PL2
    | VGG16 | $-$ | 79.6 | 57.5 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| LLC [[150](#bib.bib150)] | 单一 | DP | ${\times}$ | SSE | 3,072 | IN, PL2 |
    VGG16 | $-$ | 79.6 | 57.5 |'
- en: '| Semantic features based methods | URDL [[87](#bib.bib87)] | Multi | SS |
    ${\surd}$ | pooling | 4,096+ | IN | VGG16 | 91.2 | 71.9 | $-$ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 语义特征基础的方法 | URDL [[87](#bib.bib87)] | 多 | SS | ${\surd}$ | pooling | 4,096+
    | IN | VGG16 | 91.2 | 71.9 | $-$ |'
- en: '| MetaObject-CNN [[33](#bib.bib33)] | Multi | RP | ${\surd}$ | LSAQ | 4,096
    | PL1 | AlexNet | $-$ | 78.9 | 58.1 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| MetaObject-CNN [[33](#bib.bib33)] | 多 | RP | ${\surd}$ | LSAQ | 4,096 | PL1
    | AlexNet | $-$ | 78.9 | 58.1 |'
- en: '| SOAL [[164](#bib.bib164)] | Multi | RP | ${\times}$ | CRF | 1,024 | PL1 |
    VGGNet | $-$ | 82.5 | 75.5 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| SOAL [[164](#bib.bib164)] | 多 | RP | ${\times}$ | CRF | 1,024 | PL1 | VGGNet
    | $-$ | 82.5 | 75.5 |'
- en: '| WELDON [[34](#bib.bib34)] | Single | WI | ${\times}$ | pooling | 4,096 |
    IN | VGG16 | 94.3 | 78.0 | $-$ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| WELDON [[34](#bib.bib34)] | 单一 | WI | ${\times}$ | pooling | 4,096 | IN |
    VGG16 | 94.3 | 78.0 | $-$ |'
- en: '| Adi-Red [[165](#bib.bib165)] | Multi | DisNet | ${\times}$ | GAP | 12,288
    | IN, PL1-2 | ResNet | $-$ | $-$ | 73.6 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Adi-Red [[165](#bib.bib165)] | 多 | DisNet | ${\times}$ | GAP | 12,288 | IN,
    PL1-2 | ResNet | $-$ | $-$ | 73.6 |'
- en: '| SDO [[35](#bib.bib35)] | Multi | OMD | ${\times}$ | VLAD | 8,192 | PL1 |
    VGGNet | *95.9* | 86.8 | 73.4 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| SDO [[35](#bib.bib35)] | 多 | OMD | ${\times}$ | VLAD | 8,192 | PL1 | VGGNet
    | *95.9* | 86.8 | 73.4 |'
- en: '| M2M BiLSTM [[36](#bib.bib36)] | Single | SS | ${\times}$ | LSTM | $-$ | IN
    | ResNet | *96.3* | 88.3 | 71.8 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| M2M BiLSTM [[36](#bib.bib36)] | 单一 | SS | ${\times}$ | LSTM | $-$ | IN |
    ResNet | *96.3* | 88.3 | 71.8 |'
- en: '| LGN [[45](#bib.bib45)] | Single | WI | ${\times}$ | LGN | 8,192 | PL2 | ResNet
    | $-$ | 88.1 | 74.1 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| LGN [[45](#bib.bib45)] | 单一 | WI | ${\times}$ | LGN | 8,192 | PL2 | ResNet
    | $-$ | 88.1 | 74.1 |'
- en: '| Multilayer features based methods | Deep19-DAG [[37](#bib.bib37)] | Single
    | WI | ${\times}$ | pooling | 6,144 | IN | VGG19 | 92.9 | 77.5 | 56.2 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 多层特征基础的方法 | Deep19-DAG [[37](#bib.bib37)] | 单一 | WI | ${\times}$ | pooling
    | 6,144 | IN | VGG19 | 92.9 | 77.5 | 56.2 |'
- en: '| Hybrid CNNs [[38](#bib.bib38)] | Multi | SS | ${\times}$ | FV | 12,288+ |
    IN, PL1 | VGGNet | $-$ | 82.3 | 64.5 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 混合CNN [[38](#bib.bib38)] | 多 | SS | ${\times}$ | FV | 12,288+ | IN, PL1 |
    VGGNet | $-$ | 82.3 | 64.5 |'
- en: '| G-MS2F [[39](#bib.bib39)] | Single | WI | ${\surd}$ | pooling | 3,072 | IN,
    PL1 | GoogLeNet | 93.2 | 80.0 | 65.1 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| G-MS2F [[39](#bib.bib39)] | 单一 | WI | ${\surd}$ | pooling | 3,072 | IN, PL1
    | GoogLeNet | 93.2 | 80.0 | 65.1 |'
- en: '|  | FTOTLM [[21](#bib.bib21)] | Single | WI | ${\times}$ | GAP | 3,968 | IN,
    PL2 | ResNet | 94.0 | 74.6 | $65.5$ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | FTOTLM [[21](#bib.bib21)] | 单一 | WI | ${\times}$ | GAP | 3,968 | IN, PL2
    | ResNet | 94.0 | 74.6 | $65.5$ |'
- en: '|  | FTOTLM Aug. [[21](#bib.bib21)] | Single | WI | ${\surd}$ | GAP | 3,968
    | IN, PL2 | ResNet | *97.4^∗* | *94.1^∗* | *85.2^∗* |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | FTOTLM Aug. [[21](#bib.bib21)] | 单一 | WI | ${\surd}$ | GAP | 3,968 | IN,
    PL2 | ResNet | *97.4^∗* | *94.1^∗* | *85.2^∗* |'
- en: '| Multiview features based methods | [[166](#bib.bib166)] | Multi | WI | ${\surd}$
    | pooling | 8,192 | IN, aratio | AlexNet | 92.1 | 70.1 | 54.7 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 多视角特征基础的方法 | [[166](#bib.bib166)] | 多 | WI | ${\surd}$ | pooling | 8,192
    | IN, aratio | AlexNet | 92.1 | 70.1 | 54.7 |'
- en: '| Scale-specific CNNs [[19](#bib.bib19)] | Multi | Crops | ${\times}$ | pooling
    | 4,096 | IN, PL1 | VGGNet | 95.2 | 86.0 | 70.2 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 特定尺度 CNNs [[19](#bib.bib19)] | 多模态 | 裁剪 | ${\times}$ | pooling | 4,096 |
    IN, PL1 | VGGNet | 95.2 | 86.0 | 70.2 |'
- en: '| LS-DHM [[40](#bib.bib40)] | Single | WI, DP | ${\times}$ | FV | 40,960 |
    IN | VGGNet | $-$ | 83.8 | 67.6 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| LS-DHM [[40](#bib.bib40)] | 单一 | WI, DP | ${\times}$ | FV | 40,960 | IN |
    VGGNet | $-$ | 83.8 | 67.6 |'
- en: '| [[167](#bib.bib167)] | Multi | WI, DP | ${\times}$ | SC | 6,096 | IN, PL1
    | VGG16 | 95.7 | 87.2 | 71.1 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| [[167](#bib.bib167)] | 多模态 | WI, DP | ${\times}$ | SC | 6,096 | IN, PL1 |
    VGG16 | 95.7 | 87.2 | 71.1 |'
- en: '| MR CNN [[41](#bib.bib41)] | Multi | WI | ${\surd}$ | pooling | $-$ | Places401
    | Inception 2 | $-$ | 86.7 | 72.0 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| MR CNN [[41](#bib.bib41)] | 多模态 | WI | ${\surd}$ | pooling | $-$ | Places401
    | Inception 2 | $-$ | 86.7 | 72.0 |'
- en: '| SOSF+CFA+GAF [[24](#bib.bib24)] | Single | WI, DP | ${\surd}$ | SFV | 12,288
    | IN | VGG16 | $-$ | *89.5* | *78.9* |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SOSF+CFA+GAF [[24](#bib.bib24)] | 单一 | WI, DP | ${\surd}$ | SFV | 12,288
    | IN | VGG16 | $-$ | *89.5* | *78.9* |'
- en: '| FOSNet [[132](#bib.bib132)] | Single | WI | ${\surd}$ | GAP | 4,096 | PL2
    | ResNet | $-$ | *90.3* | *77.3* |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| FOSNet [[132](#bib.bib132)] | 单一 | WI | ${\surd}$ | GAP | 4,096 | PL2 | ResNet
    | $-$ | *90.3* | *77.3* |'
- en: '|  | ChAM [[43](#bib.bib43)] | Single | WI | ${\surd}$ | pooling | 512 | PL2
    | ResNet | $-$ | *87.1* | *74.0* |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | ChAM [[43](#bib.bib43)] | 单一 | WI | ${\surd}$ | pooling | 512 | PL2 |
    ResNet | $-$ | *87.1* | *74.0* |'
- en: 'TABLE III: Performance (%) comparison of related methods with/without concatenating
    global CNN feature on benchmark scene datasets.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：在基准场景数据集上，相关方法在是否连接全局 CNN 特征下的性能 (%) 比较。
- en: '|  |  | DSFL [[149](#bib.bib149)] | SFV [[30](#bib.bib30)] | MFA$-$FS [[42](#bib.bib42)]
    | MFAFVNet [[31](#bib.bib31)] | VSAD [[32](#bib.bib32)] | SOSF$+$CFA [[24](#bib.bib24)]
    | SDO [[35](#bib.bib35)] | LGN [[45](#bib.bib45)] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  |  | DSFL [[149](#bib.bib149)] | SFV [[30](#bib.bib30)] | MFA$-$FS [[42](#bib.bib42)]
    | MFAFVNet [[31](#bib.bib31)] | VSAD [[32](#bib.bib32)] | SOSF$+$CFA [[24](#bib.bib24)]
    | SDO [[35](#bib.bib35)] | LGN [[45](#bib.bib45)] |'
- en: '| MIT67 | Baseline | 52.2 | 72.8 | 81.4 | 82.6 | 84.9 | 84.1 | 68.1 | 85.2
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| MIT67 | 基线 | 52.2 | 72.8 | 81.4 | 82.6 | 84.9 | 84.1 | 68.1 | 85.2 |'
- en: '|  | $+$Global feature | 76.2 ($\uparrow 24$) | 79 ($\uparrow 6.2$) | 87.2
    ($\uparrow 5.8$) | 87.9 ($\uparrow 5.3$) | 85.3 ($\uparrow 0.4$) | 89.5 ($\uparrow
    5.4$) | 84 ($\uparrow 15.9$) | 85.4 ($\uparrow 0.2$) |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | $+$全局特征 | 76.2 ($\uparrow 24$) | 79 ($\uparrow 6.2$) | 87.2 ($\uparrow
    5.8$) | 87.9 ($\uparrow 5.3$) | 85.3 ($\uparrow 0.4$) | 89.5 ($\uparrow 5.4$)
    | 84 ($\uparrow 15.9$) | 85.4 ($\uparrow 0.2$) |'
- en: '| SUN397 | Baseline | $-$ | 54.4 | 63.3 | 64.6 | 71.7 | 66.5 | 54.8 | $-$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| SUN397 | 基线 | $-$ | 54.4 | 63.3 | 64.6 | 71.7 | 66.5 | 54.8 | $-$ |'
- en: '|  | $+$Global feature | $-$ | 61.7 ($\uparrow 7.3$) | 71.1 ($\uparrow 7.8$)
    | 72 ($\uparrow 7.4$) | 72.5 ($\uparrow 0.8$) | 78.9 ($\uparrow 12.4$) | 67 ($\uparrow
    12.2$) | $-$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $+$全局特征 | $-$ | 61.7 ($\uparrow 7.3$) | 71.1 ($\uparrow 7.8$) | 72 ($\uparrow
    7.4$) | 72.5 ($\uparrow 0.8$) | 78.9 ($\uparrow 12.4$) | 67 ($\uparrow 12.2$)
    | $-$ |'
- en: 'In contrast, CNN-based methods have quickly demonstrated their strengths in
    scene classification. Table [II](#S4.T2 "TABLE II ‣ 4.1 Performance on RGB scene
    data ‣ 4 Performance Comparison ‣ Deep Learning for Scene Classification: A Survey")
    compares the performance of deep models for scene classification on RGB datasets.
    To gain insight into the performance of the presented methods, we also provided
    input information, feature information, and architecture of each method. The results
    show that a simple deep model (*i.e.,* AlexNet), which is trained on ImageNet,
    achieves 84.23%, 56.79%, and 42.61% accuracy on Scene15, MIT67, and SUN397 datasets,
    respectively. This accuracy is comparable with the best non-deep learning methods.
    Starting from the generic deep models [[17](#bib.bib17), [18](#bib.bib18), [25](#bib.bib25)],
    CNN-based methods improve steadily when more effective strategies have been introduced.
    As a result, nearly all the approaches yielded an accuracy of 90% on the Scene15
    dataset. Moreover, FTOTLM [[21](#bib.bib21)] combined with a novel data augmentation
    outperforms other state-of-the-art models and achieves the best accuracy on three
    benchmark datasets so far.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '相比之下，基于 CNN 的方法在场景分类中迅速展示了它们的优势。表 [II](#S4.T2 "TABLE II ‣ 4.1 Performance on
    RGB scene data ‣ 4 Performance Comparison ‣ Deep Learning for Scene Classification:
    A Survey") 比较了深度模型在 RGB 数据集上的场景分类性能。为了深入了解所呈现方法的性能，我们还提供了每种方法的输入信息、特征信息和架构。结果表明，一个简单的深度模型（*即，*
    AlexNet），在 ImageNet 上训练后，在 Scene15、MIT67 和 SUN397 数据集上的准确率分别为 84.23%、56.79% 和
    42.61%。这一准确率与最佳的非深度学习方法相当。从通用深度模型 [[17](#bib.bib17), [18](#bib.bib18), [25](#bib.bib25)]
    开始，当引入更有效的策略时，基于 CNN 的方法逐步提升。因此，几乎所有方法在 Scene15 数据集上都达到了 90% 的准确率。此外，FTOTLM [[21](#bib.bib21)]
    结合新颖的数据增强技术，超越了其他先进模型，并在三个基准数据集上取得了最佳准确率。'
- en: Extracting global CNN features, which are computed using a pre-trained model,
    is faster than other deep feature representation techniques, but their quality
    is not good when there are large differences between the source and target datasets.
    Comparing these performances [[17](#bib.bib17), [18](#bib.bib18), [25](#bib.bib25)]
    demonstrates that the expressive power of global CNN features is improved as richer
    scene datasets appear. In GAP-CNN [[23](#bib.bib23)] and DL-CNN [[22](#bib.bib22)],
    new layers with a small number of parameters substitute for FC layers, but they
    can still achieve considerable results comparing with benchmark CNNs [[17](#bib.bib17),
    [18](#bib.bib18)].
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 提取全局CNN特征的速度比其他深度特征表示技术要快，这些特征是使用预训练模型计算的，但当源数据集和目标数据集之间存在较大差异时，它们的质量并不好。比较这些性能 [[17](#bib.bib17),
    [18](#bib.bib18), [25](#bib.bib25)] 证明了全局CNN特征的表现力随着丰富场景数据集的出现而得到提升。在 GAP-CNN [[23](#bib.bib23)]
    和 DL-CNN [[22](#bib.bib22)] 中，新的具有少量参数的层替代了全连接层，但与基准CNN [[17](#bib.bib17), [18](#bib.bib18)]
    相比，它们仍然能够取得相当好的结果。
- en: Spatially invariant feature based methods are usually time-consuming, especially
    the computational time of sampling local patches, extracting individually local
    features, and building codebook. However, these methods are robust against geometrical
    variance, and thus improve the accuracy of benchmark CNNs, like SFV [[30](#bib.bib30)]
    vs. ImageNet-CNN [[17](#bib.bib17)], and MFA-FS [[42](#bib.bib42)] vs. PL1-CNN [[18](#bib.bib18)].
    Encoding technologies generally include more complicated training procedure, so
    some architectures (*e.g.,* MFAFVNet [[31](#bib.bib31)] and VSAD [[32](#bib.bib32)])
    are designed in a unified pipeline to reduce the operation complexity.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 基于空间不变特征的方法通常耗时较长，尤其是在采样局部补丁、提取个体局部特征和构建词汇表的计算时间。然而，这些方法对几何变换具有鲁棒性，从而提高了基准CNN的准确性，如
    SFV [[30](#bib.bib30)] 与 ImageNet-CNN [[17](#bib.bib17)]，以及 MFA-FS [[42](#bib.bib42)]
    与 PL1-CNN [[18](#bib.bib18)]。编码技术通常包括更复杂的训练过程，因此一些架构（*例如*，MFAFVNet [[31](#bib.bib31)]
    和 VSAD [[32](#bib.bib32)]）被设计成统一的流程，以减少操作复杂性。
- en: Semantic feature based methods [[165](#bib.bib165), [35](#bib.bib35), [36](#bib.bib36),
    [45](#bib.bib45)] demonstrate very competitive performance, due to the discriminative
    information laying on the salient regions, compared to global CNN feature based
    and spatially invariant feature based methods. Salient regions generally are generated
    by region selection algorithms, which may cause a two-stage training procedure
    and require more time and computations [[61](#bib.bib61)]. In addition, even though
    the contextual analysis demands more computational power, methods [[36](#bib.bib36),
    [45](#bib.bib45)], exploring the contextual relations among salient regions, can
    significantly improve the classification accuracy.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语义特征的方法 [[165](#bib.bib165), [35](#bib.bib35), [36](#bib.bib36), [45](#bib.bib45)]
    展示了非常具有竞争力的性能，因为与基于全局CNN特征和空间不变特征的方法相比，它们的显著区域包含了更多的区分信息。显著区域通常由区域选择算法生成，这可能导致两阶段训练过程，并需要更多的时间和计算 [[61](#bib.bib61)]。此外，尽管上下文分析需要更多的计算能力，但一些方法 [[36](#bib.bib36),
    [45](#bib.bib45)] 通过探索显著区域之间的上下文关系，可以显著提高分类准确性。
- en: Multi-layer feature based methods employ the complementary features from different
    layers to improve performance. It is a simple way to use more feature cues, while
    it also does not require to add any other layers. However, these methods are structurally
    complicated and have high-dimensional features, which make training models difficult
    and prone to overfitting [[37](#bib.bib37)]. Nevertheless, owing to a novel data
    augmentation, FTOTLM [[21](#bib.bib21)] yields a gain of 19.5% and 19.7% on MIT67
    and SUN397, respectively, and has achieved the best results so far.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 多层特征方法利用来自不同层的互补特征来提高性能。这是一种简单的使用更多特征线索的方式，同时也不需要添加其他层。然而，这些方法结构复杂，具有高维特征，使得模型训练困难且容易过拟合 [[37](#bib.bib37)]。尽管如此，由于一种新颖的数据增强方法，FTOTLM [[21](#bib.bib21)]
    在 MIT67 和 SUN397 上分别取得了 19.5% 和 19.7% 的提升，并且迄今为止取得了最佳结果。
- en: 'Multi-view feature based methods take full advantage of features extracted
    from various CNNs to achieve high classification accuracy. For instance, Table [III](#S4.T3
    "TABLE III ‣ 4.1 Performance on RGB scene data ‣ 4 Performance Comparison ‣ Deep
    Learning for Scene Classification: A Survey") shows that combining global features
    with other baselines significantly improves their original classification accuracy,
    *e.g.,* a baseline model “SFV”[[30](#bib.bib30)] achieves 72.8% on MIT67, while
    “SFV$+$global feature” yields 79%. Moreover, there are two aspects to emphasize:
    1) Herranz et al. [[19](#bib.bib19)] empirically proved that combining too much
    invalid features is marginally helpful and significantly increases calculation
    and introduces noise into the final feature. 2) It is essential to improve the
    expression ability of each view feature, and thus enhance the entire ability of
    multi-view features.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '基于多视角特征的方法充分利用从各种CNN中提取的特征，以实现高分类准确率。例如，表[III](#S4.T3 "TABLE III ‣ 4.1 Performance
    on RGB scene data ‣ 4 Performance Comparison ‣ Deep Learning for Scene Classification:
    A Survey")显示，将全局特征与其他基线进行组合显著提高了原始分类准确率，*例如*，基线模型“SFV”[[30](#bib.bib30)]在MIT67上取得了72.8%，而“SFV$+$全局特征”则达到了79%。此外，有两个方面需要强调：1）Herranz等[[19](#bib.bib19)]实证证明，组合过多无效特征的边际效果有限，且显著增加了计算量并引入噪声。2）提高每个视角特征的表达能力至关重要，从而增强多视角特征的整体能力。'
- en: In summary, the scene classification performance can be boosted by adopting
    more sophisticated deep models [[77](#bib.bib77), [79](#bib.bib79)] and large-scale
    datasets [[18](#bib.bib18), [25](#bib.bib25)]. Meanwhile, deep learning based
    methods can obtain relatively satisfied accuracy on public datasets via combining
    multiple features [[24](#bib.bib24)], focusing on semantic regions [[35](#bib.bib35)],
    augmenting data [[21](#bib.bib21)], and training in a unified pipeline [[132](#bib.bib132)].
    In addition, many methods also improve their accuracy via adopting different strategies,
    *i.e.,* improved encoding [[32](#bib.bib32), [31](#bib.bib31)], contextual modeling [[36](#bib.bib36),
    [45](#bib.bib45)], attention policy [[23](#bib.bib23), [43](#bib.bib43)], and
    multi-task learning [[22](#bib.bib22), [40](#bib.bib40)].
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，通过采用更复杂的深度模型[[77](#bib.bib77), [79](#bib.bib79)]和大规模数据集[[18](#bib.bib18),
    [25](#bib.bib25)]，场景分类性能可以得到提升。同时，基于深度学习的方法可以通过组合多种特征[[24](#bib.bib24)]、关注语义区域[[35](#bib.bib35)]、数据增强[[21](#bib.bib21)]和在统一管道中训练[[132](#bib.bib132)]，在公共数据集上获得相对满意的准确率。此外，许多方法还通过采用不同的策略提高了准确性，*即*，改进编码[[32](#bib.bib32),
    [31](#bib.bib31)]、上下文建模[[36](#bib.bib36), [45](#bib.bib45)]、注意力策略[[23](#bib.bib23),
    [43](#bib.bib43)]和多任务学习[[22](#bib.bib22), [40](#bib.bib40)]。
- en: 'TABLE IV: Performance (%) comparison of representative methods on benchmark
    RGB-D scene datasets. For each dataset, the top three scores are highlighted.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表IV：基准RGB-D场景数据集上代表性方法的性能（%）比较。对于每个数据集，前三名的得分被突出显示。
- en: '| Group | Method | Architecture | Detailed Information | Results |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 组 | 方法 | 架构 | 详细信息 | 结果 |'
- en: '|  |  | RGB-CNN | Depth-CNN | Dimension | Modal Fusion | Classifier | NYUD2
    | SUN RGBD |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  |  | RGB-CNN | Depth-CNN | 维度 | 模态融合 | 分类器 | NYUD2 | SUN RGBD |'
- en: '| Dataset | SUN RGBD[[72](#bib.bib72)] | PL1-AlexNet | 8,192 | Feature-level
    concatenation | SVM | $-$ | $39$ |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | SUN RGBD[[72](#bib.bib72)] | PL1-AlexNet | 8,192 | 特征级拼接 | SVM | $-$
    | $39$ |'
- en: '| Feature learning | SS-CNN [[168](#bib.bib168)] | PL1-ASPP | 4,096 | Image-level
    stacking | Softmax | $-$ | $41.3$ |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 特征学习 | SS-CNN [[168](#bib.bib168)] | PL1-ASPP | 4,096 | 图像级堆叠 | Softmax |
    $-$ | $41.3$ |'
- en: '|  | MMML [[160](#bib.bib160)] | IN-DeCAF | 256 | Feature-level concatenation
    | SVM | $61.4$ | $-$ |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | MMML [[160](#bib.bib160)] | IN-DeCAF | 256 | 特征级拼接 | SVM | $61.4$ | $-$
    |'
- en: '|  | MSMM [[47](#bib.bib47)] | PL1-AlexNet | 12,288+ | Feature-level concatenation
    | wSVM | $66.7$ | $52.3$ |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | MSMM [[47](#bib.bib47)] | PL1-AlexNet | 12,288+ | 特征级拼接 | wSVM | $66.7$
    | $52.3$ |'
- en: '|  | MAPNet [[96](#bib.bib96)] | PL1-AlexNet | 5,120 | Local and semantic feature
    concatenation | Softmax | $67.7$ | *56.2* |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | MAPNet [[96](#bib.bib96)] | PL1-AlexNet | 5,120 | 局部和语义特征拼接 | Softmax
    | $67.7$ | *56.2* |'
- en: '|  | SOOR[[145](#bib.bib145)] | PL1-AlexNet | PL1-DCNN | 512 | Local and global
    feature concatenation | SVM | $67.4$ | $55.5$ |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | SOOR[[145](#bib.bib145)] | PL1-AlexNet | PL1-DCNN | 512 | 局部和全局特征拼接 |
    SVM | $67.4$ | $55.5$ |'
- en: '|  | ACM [[144](#bib.bib144)] | PL2-AlexNet | 8192+ | Feature-level concatenation
    | Softmax | $67.2$ | $55.1$ |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | ACM [[144](#bib.bib144)] | PL2-AlexNet | 8192+ | 特征级拼接 | Softmax | $67.2$
    | $55.1$ |'
- en: '|  | LM-CNN [[169](#bib.bib169)] | IN-AlexNet | 8,192 | Local feature concatenation
    | Softmax | $-$ | $48.7$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | LM-CNN [[169](#bib.bib169)] | IN-AlexNet | 8,192 | 局部特征拼接 | Softmax |
    $-$ | $48.7$ |'
- en: '| Depth feature learning | DCNN [[89](#bib.bib89)] | PL1-RCNN | PL1-DCNN |
    4,608 | Feature-level concatenation | wSVM | $67.5$ | $53.8$ |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 深度特征学习 | DCNN [[89](#bib.bib89)] | PL1-RCNN | PL1-DCNN | 4,608 | 特征级联接 |
    wSVM | $67.5$ | $53.8$ |'
- en: '|  | TRecgNet [[48](#bib.bib48)] | SUN RGBD & PL2 ResNet18 | 1024 | Feature-level
    concatenation | Softmax | *69.2* | *56.7* |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | TRecgNet [[48](#bib.bib48)] | SUN RGBD & PL2 ResNet18 | 1024 | 特征级联接 |
    Softmax | *69.2* | *56.7* |'
- en: '| Multiple modal fusion | DMMF [[50](#bib.bib50)] | PL1-AlexNet | 4096 | Inter-
    and intra- modal correlation and distinction | L-SVM | $-$ | $41.5$ |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 多模态融合 | DMMF [[50](#bib.bib50)] | PL1-AlexNet | 4096 | 模态间和模态内相关性与区分 | L-SVM
    | $-$ | $41.5$ |'
- en: '|  | MaCAFF [[46](#bib.bib46)] | PL1-AlexNet | $-$ | Local and global feature
    concatenation | L-SVM | $63.9$ | $48.1$ |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | MaCAFF [[46](#bib.bib46)] | PL1-AlexNet | $-$ | 局部和全局特征级联接 | L-SVM | $63.9$
    | $48.1$ |'
- en: '|  | DF2Net [[152](#bib.bib152)] | PL1-AlexNet | 512 | Modal correlation and
    distinction | Softmax | $65.4$ | $54.6$ |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | DF2Net [[152](#bib.bib152)] | PL1-AlexNet | 512 | 模态相关性与区分 | Softmax |
    $65.4$ | $54.6$ |'
- en: '|  | KFS [[103](#bib.bib103)] | PL1-AlexNet | 9,216+ | Modal correlation and
    distinction | Softmax | $67.8$ | 55.9 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | KFS [[103](#bib.bib103)] | PL1-AlexNet | 9,216+ | 模态相关性与区分 | Softmax |
    $67.8$ | 55.9 |'
- en: '|  | CBSC [[170](#bib.bib170)] | PL2-VGG16 | $-$ | Feature-level concatenation
    | Softmax | *69.7^∗* | *57.8^∗* |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | CBSC [[170](#bib.bib170)] | PL2-VGG16 | $-$ | 特征级联接 | Softmax | *69.7^∗*
    | *57.8^∗* |'
- en: '|  | MSN [[44](#bib.bib44)] | PL1-AlexNet | 9,216+ | Modal correlation and
    distinction | Softmax | *68.1* | *56.2* |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | MSN [[44](#bib.bib44)] | PL1-AlexNet | 9,216+ | 模态相关性与区分 | Softmax | *68.1*
    | *56.2* |'
- en: 'TABLE V: Ablation study on benchmark datasets to validate the performance (%)
    of depth information.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：对基准数据集的消融研究，以验证深度信息的性能（%）。
- en: '|  |  | MaCAFF[[46](#bib.bib46)] | MSMM[[47](#bib.bib47)] | DCNN[[89](#bib.bib89)]
    | DF2Net[[152](#bib.bib152)] | TRecgNet[[48](#bib.bib48)] | ACM[[144](#bib.bib144)]
    | CBCL[[170](#bib.bib170)] | KFS[[103](#bib.bib103)] | MSN[[44](#bib.bib44)] |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MaCAFF[[46](#bib.bib46)] | MSMM[[47](#bib.bib47)] | DCNN[[89](#bib.bib89)]
    | DF2Net[[152](#bib.bib152)] | TRecgNet[[48](#bib.bib48)] | ACM[[144](#bib.bib144)]
    | CBCL[[170](#bib.bib170)] | KFS[[103](#bib.bib103)] | MSN[[44](#bib.bib44)] |'
- en: '| NYUD2 | Baseline | 53.5 | $-$ | 53.4 | 61.1 | 53.7 | 55.4 | 66.4 | 53.5 |
    53.5 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| NYUD2 | 基线 | 53.5 | $-$ | 53.4 | 61.1 | 53.7 | 55.4 | 66.4 | 53.5 | 53.5
    |'
- en: '|  | $+$Depth | 63.9 ($\uparrow 10.4$) | $-$ | 67.5 ($\uparrow 14.1$) | 65.4 ($\uparrow
    4.3$) | 67.5 ($\uparrow 13.8$) | 67.4 ($\uparrow 12$) | 69.7 ($\uparrow 3.3$)
    | 67.8 ($\uparrow 14.3$) | 68.1 ($\uparrow 14.6$) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $+$深度 | 63.9 ($\uparrow 10.4$) | $-$ | 67.5 ($\uparrow 14.1$) | 65.4 ($\uparrow
    4.3$) | 67.5 ($\uparrow 13.8$) | 67.4 ($\uparrow 12$) | 69.7 ($\uparrow 3.3$)
    | 67.8 ($\uparrow 14.3$) | 68.1 ($\uparrow 14.6$) |'
- en: '| SUN RGBD | Baseline | 40.4 | 41.5 | 44.3 | 46.3 | 42.6 | 45.7 | 48.8 | 36.1
    | $-$ |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| SUN RGBD | 基线 | 40.4 | 41.5 | 44.3 | 46.3 | 42.6 | 45.7 | 48.8 | 36.1 | $-$
    |'
- en: '|  | $+$Depth | 48.1 ($\uparrow 7.7$) | 52.3 ($\uparrow 10.8$) | 53.8 ($\uparrow
    9.5$) | 54.6 ($\uparrow 8.3$) | 53.3 ($\uparrow 10.7$) | 55.1 ($\uparrow 9.4$)
    | 57.8 ($\uparrow 9.0$) | 41.3 ($\uparrow 5.2$) | $-$ |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | $+$深度 | 48.1 ($\uparrow 7.7$) | 52.3 ($\uparrow 10.8$) | 53.8 ($\uparrow
    9.5$) | 54.6 ($\uparrow 8.3$) | 53.3 ($\uparrow 10.7$) | 55.1 ($\uparrow 9.4$)
    | 57.8 ($\uparrow 9.0$) | 41.3 ($\uparrow 5.2$) | $-$ |'
- en: 4.2 Performance on RGB-D scene datasets
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 RGB-D 场景数据集上的性能
- en: 'The accuracy of different methods on RGB-D datasets is summarized in Table [IV](#S4.T4
    "TABLE IV ‣ 4.1 Performance on RGB scene data ‣ 4 Performance Comparison ‣ Deep
    Learning for Scene Classification: A Survey"). By adding depth information with
    different fusion strategies, accuracy results (see Table [V](#S4.T5 "TABLE V ‣
    4.1 Performance on RGB scene data ‣ 4 Performance Comparison ‣ Deep Learning for
    Scene Classification: A Survey")) are improved over 10.8% and 8.8% on average
    on NYUD2 and SUN RGBD datasets, respectively. Since depth data provide extra information
    to train classification model, this observation is within expectation. Noteworthily,
    it is more difficult to improve the effect on a large dataset (SUN RGBD) than
    a small dataset (NYUD2). Moreover, the best results on NYUD2 and SUN RGBD datasets
    achieved by CBSC [[170](#bib.bib170)] are as high as 69.7% and 57.84%, respectively.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 'RGB-D 数据集上不同方法的准确率总结见表 [IV](#S4.T4 "TABLE IV ‣ 4.1 Performance on RGB scene
    data ‣ 4 Performance Comparison ‣ Deep Learning for Scene Classification: A Survey")。通过使用不同的融合策略添加深度信息，准确率结果（见表
    [V](#S4.T5 "TABLE V ‣ 4.1 Performance on RGB scene data ‣ 4 Performance Comparison
    ‣ Deep Learning for Scene Classification: A Survey")）在 NYUD2 和 SUN RGBD 数据集上的平均提升分别为
    10.8% 和 8.8%。由于深度数据为训练分类模型提供了额外信息，这一观察结果在预期之内。值得注意的是，提升大数据集（SUN RGBD）的效果比小数据集（NYUD2）更为困难。此外，CBSC
    [[170](#bib.bib170)] 在 NYUD2 和 SUN RGBD 数据集上获得的最佳结果分别高达 69.7% 和 57.84%。'
- en: RGB-D scene data for training are relatively limited, while the dimension of
    scene features is high. Hence, Support Vector Machines (SVMs) are commonly used
    in RGB-D scene classification [[72](#bib.bib72), [160](#bib.bib160), [47](#bib.bib47),
    [89](#bib.bib89)] in the early stages. Thanks to data augmentation and back-propagation,
    Softmax classifier becomes progressively popular, and it is an important reason
    to yield a comparable performance [[96](#bib.bib96), [48](#bib.bib48), [170](#bib.bib170),
    [44](#bib.bib44)].
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练的RGB-D场景数据相对有限，而场景特征的维度较高。因此，支持向量机（SVM）在RGB-D场景分类的早期阶段被广泛使用[[72](#bib.bib72)、[160](#bib.bib160)、[47](#bib.bib47)、[89](#bib.bib89)]。得益于数据增强和反向传播，Softmax分类器变得越来越流行，这是实现可比性能的重要原因[[96](#bib.bib96)、[48](#bib.bib48)、[170](#bib.bib170)、[44](#bib.bib44)]。
- en: Many methods, such as [[46](#bib.bib46), [50](#bib.bib50)], fine-tune RGB-CNNs
    to extract deep features of depth modality, where the training process is simple,
    and the computational cost is low. To adapt to depth data, Song et al. [[171](#bib.bib171),
    [89](#bib.bib89)] used weakly-supervised learning to train depth-specific models
    from scratch, which achieves a gain of 3.5% accuracy, compared to the fine-tuned
    RGB-CNN. TRecgNet [[48](#bib.bib48)], which is based on semi-supervised learning,
    requires complicated training process and high computational cost, but it obtains
    comparable results (69.2% on NYUD2 and 56.7% on SUN RGBD).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 许多方法，如[[46](#bib.bib46)、[50](#bib.bib50)]，对RGB-CNN进行微调以提取深度模态的深层特征，其中训练过程简单，计算成本低。为了适应深度数据，Song等人[[171](#bib.bib171)、[89](#bib.bib89)]使用弱监督学习从头开始训练深度特定模型，这相比于微调的RGB-CNN，精度提高了3.5%。TRecgNet[[48](#bib.bib48)]基于半监督学习，需要复杂的训练过程和高计算成本，但其结果与RGB-CNN相当（在NYUD2上为69.2%，在SUN
    RGBD上为56.7%）。
- en: Feature-level fusion based methods are commonly used due to their high cost-effectiveness,
    *e.g.,* [[170](#bib.bib170), [96](#bib.bib96), [144](#bib.bib144)]. Along this
    way, consistent-feature based, and distinctive-feature based modal fusion use
    complex fusion layer with high cost, like inference speed and training complexity,
    but they generally yield more effective features [[152](#bib.bib152), [103](#bib.bib103),
    [44](#bib.bib44)].
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 特征级融合的方法由于其高性价比而被广泛使用，例如[[170](#bib.bib170)、[96](#bib.bib96)、[144](#bib.bib144)]。在这方面，一致性特征基础和区分特征基础的模态融合使用复杂的融合层，成本较高，如推理速度和训练复杂度，但它们通常会产生更有效的特征[[152](#bib.bib152)、[103](#bib.bib103)、[44](#bib.bib44)]。
- en: We can observe that the field of RGB-D scene classification has constantly been
    improving. Weakly-supervised and semi-supervised learning are useful to learn
    depth-specific features [[47](#bib.bib47), [89](#bib.bib89), [48](#bib.bib48)].
    Moreover, multi-modal feature fusion is a major issue to improve performance on
    public datasets [[152](#bib.bib152), [170](#bib.bib170), [44](#bib.bib44)]. In
    addition, effective strategies (like contextual strategy [[145](#bib.bib145),
    [144](#bib.bib144)] and attention mechanism [[96](#bib.bib96), [44](#bib.bib44)])
    are also popular for RGB-D scene classification. Nevertheless, the accuracy achieved
    by current methods is far from expectation and there remains significant rooms
    for future improvement.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到RGB-D场景分类领域不断取得进步。弱监督和半监督学习对学习深度特定特征非常有用[[47](#bib.bib47)、[89](#bib.bib89)、[48](#bib.bib48)]。此外，多模态特征融合是提高公共数据集性能的主要问题[[152](#bib.bib152)、[170](#bib.bib170)、[44](#bib.bib44)]。此外，有效策略（如上下文策略[[145](#bib.bib145)、[144](#bib.bib144)]和注意力机制[[96](#bib.bib96)、[44](#bib.bib44)]）在RGB-D场景分类中也非常流行。然而，当前方法所取得的准确性仍远未达到预期，未来仍有很大的改进空间。
- en: 5 Conclusion and Outlook
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与展望
- en: As a contemporary survey for scene classification using deep learning, this
    paper has highlighted the recent achievements, provided some structural taxonomy
    for various methods according to their roles in scene representation for scene
    classification, analyzed their advantages and limitations, summarized existing
    popular scene datasets, and discussed performance for the most representative
    approaches. Despite great progress, there are still many unsolved problems. Thus
    in this section, we will point out these problems and introduce some promising
    trends for future research. We hope that this survey not only provides a better
    understanding of scene classification for researchers but also stimulates future
    research activities.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用深度学习进行场景分类的现代综述，本文突出了最近的成就，提供了针对场景分类中场景表示角色的各种方法的结构分类，分析了这些方法的优点和局限，总结了现有的流行场景数据集，并讨论了最具代表性方法的性能。尽管取得了巨大进展，但仍然存在许多未解决的问题。因此，在本节中，我们将指出这些问题并介绍一些未来研究的有前景的趋势。我们希望这项综述不仅能让研究人员对场景分类有更好的理解，还能激发未来的研究活动。
- en: Develop more advanced network frameworks. With the development of deep CNN architectures,
    from generic CNNs [[17](#bib.bib17), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79)]
    to scene-specific CNNs [[23](#bib.bib23), [22](#bib.bib22)], the accuracy of scene
    classification is getting increasingly comparable. Nevertheless, there still exists
    lots of works to be explored on the theoretical research of deep learning [[172](#bib.bib172)].
    It is a further direction to solidify the theoretical basis so as to get more
    advanced network frameworks. In particular, it is essential to design specific
    frameworks for scene classification, such as using automated Neural Architecture
    Search (NAS) [[173](#bib.bib173), [174](#bib.bib174)], or according to scene attributes.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 开发更先进的网络框架。随着深度CNN架构的发展，从通用CNN [[17](#bib.bib17), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79)] 到特定场景的CNN [[23](#bib.bib23), [22](#bib.bib22)]，场景分类的准确性越来越具有可比性。尽管如此，在深度学习的理论研究方面仍然存在许多工作需要探索 [[172](#bib.bib172)]。进一步的方向是巩固理论基础，以获得更先进的网络框架。特别是，设计用于场景分类的特定框架是至关重要的，例如使用自动化的神经架构搜索（NAS） [[173](#bib.bib173),
    [174](#bib.bib174)]，或根据场景属性进行设计。
- en: Release rich scene datasets. Deep learning based models require enormous amounts
    of data to initialize their parameters so that they can learn the scene knowledge
    well [[18](#bib.bib18), [25](#bib.bib25)]. However, compared to scenes of real
    world, the publicly available datasets are not large or rich enough, so it is
    essential to release datasets that encompass richness and high-diversity of environmental
    scenes [[175](#bib.bib175)], especially large-scale RGB-D scene datasets. As opposed
    to object/texture datasets, scene appearance may be changed dramatically as time
    goes by, and there emerges new functional scenes as humans develop activity places.
    Therefore, it requires updating the original scene data and releasing new scene
    datasets regularly.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 发布丰富的场景数据集。基于深度学习的模型需要大量的数据来初始化其参数，以便它们能够很好地学习场景知识 [[18](#bib.bib18), [25](#bib.bib25)]。然而，与现实世界的场景相比，公开的数据集规模或丰富性仍不够，因此发布涵盖环境场景丰富性和高多样性的数据显示尤为重要 [[175](#bib.bib175)]，尤其是大规模RGB-D场景数据集。与对象/纹理数据集相比，场景外观可能随着时间的推移发生剧烈变化，随着人类活动场所的发展，新的功能场景也会出现。因此，需要定期更新原始场景数据并发布新的场景数据集。
- en: Reduce the dependence of labeled scene images. The success of deep learning
    heavily relies on gargantuan amounts of labeled images. However, the labeled training
    images are always very limited, so supervised learning is not scalable in the
    absence of fully labeled training data and its generalization ability to classify
    scenes frequently deteriorates. Therefore, it is desirable to reduce dependence
    on large amounts of labeled data. To alleviate this difficulty, if with large
    numbers of unlabel data, it is necessary to further study semi-supervised learning [[176](#bib.bib176)],
    unsupervised learning [[177](#bib.bib177)], or self-supervised learning [[178](#bib.bib178)].
    Even more constrained, without any unlabel training data, the ability to learn
    from only few labeled images, small-sample learning [[179](#bib.bib179)], is also
    appealing.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 减少对标注场景图像的依赖。深度学习的成功在很大程度上依赖于大量标注的图像。然而，标注的训练图像总是非常有限的，因此在缺乏完全标注的训练数据时，监督学习不可扩展，其场景分类的泛化能力经常恶化。因此，减少对大量标注数据的依赖是有益的。为了缓解这一困难，如果有大量未标注数据，则需要进一步研究半监督学习[[176](#bib.bib176)]、无监督学习[[177](#bib.bib177)]或自监督学习[[178](#bib.bib178)]。更具限制的是，在没有任何未标注训练数据的情况下，仅从少量标注图像中学习的能力，小样本学习[[179](#bib.bib179)]，也是很有吸引力的。
- en: Few shot scene classification. The success of generic CNNs for scene classification
    relies heavily on gargantuan amounts of labeled training data [[107](#bib.bib107)].
    Due to the large intra-variation among scenes, scene datasets cannot cover various
    classes so that the performance of CNNs frequently deteriorates and fails to generalize
    well. In contrast, humans can learn a visual concept quickly from very few given
    examples and often generalize well [[180](#bib.bib180), [181](#bib.bib181)]. Inspired
    by this, domain adaptation approaches utilize the knowledge of labeled data in
    task-relevant domains to execute new tasks in target domain [[182](#bib.bib182),
    [183](#bib.bib183)]. Furthermore, domain generalization methods aim at learning
    generic representation from multiple task-irrelevant domains to generalize unseen
    scenarios [[184](#bib.bib184), [185](#bib.bib185)].
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本场景分类。通用CNN在场景分类中的成功严重依赖于大量标注的训练数据[[107](#bib.bib107)]。由于场景间的大量变化，场景数据集无法覆盖各种类别，因此CNN的性能经常恶化，且难以很好地推广。相反，人类可以从少量给定示例中快速学习视觉概念，并且通常能很好地推广[[180](#bib.bib180),
    [181](#bib.bib181)]。受到这一点的启发，领域适应方法利用任务相关领域中的标注数据知识来执行目标领域的新任务[[182](#bib.bib182),
    [183](#bib.bib183)]。此外，领域泛化方法旨在从多个任务无关的领域中学习通用表示，以推广未知场景[[184](#bib.bib184), [185](#bib.bib185)]。
- en: Robust scene classification. Once scene classification in the laboratory environment
    is deployed in the real application scenario, there will still be a variety of
    unacceptable phenomena, that is, the robustness in open environments is a bottleneck
    to restrict pattern recognition technology. The main reasons why the pattern recognition
    systems are not robust are basic assumptions, *e.g.,* closed world assumption,
    independent identical distribution and big data assumption [[186](#bib.bib186)],
    which are main differences between machine learning and human intelligence; hence,
    it is a fundamental challenge to improve the robustness by breaking these assumptions.
    It is a nature consider via utilizing adversarial training and optimization [[187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189)], which have been applied to pattern recognition [[190](#bib.bib190),
    [191](#bib.bib191)].
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的场景分类。一旦实验室环境中的场景分类技术部署到实际应用场景中，仍会出现各种不可接受的现象，即开放环境中的鲁棒性成为限制模式识别技术的瓶颈。模式识别系统不具备鲁棒性的主要原因是基本假设，*例如*，封闭世界假设、独立同分布和大数据假设[[186](#bib.bib186)]，这些假设是机器学习与人类智能之间的主要区别；因此，打破这些假设以提高鲁棒性是一个根本挑战。利用对抗训练和优化[[187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189)]是一个自然的考虑，这些方法已被应用于模式识别[[190](#bib.bib190),
    [191](#bib.bib191)]。
- en: Realtime scene classification. Many methods for scene classification, trained
    in a multiple-stage manner, are computationally expensive for current mobile/wearable
    devices, which have limited storage and computational capability, therefore researchers
    have begun to develop convenient and efficient unified networks (encapsulating
    all computation in a one-stage network) [[31](#bib.bib31), [22](#bib.bib22), [44](#bib.bib44)].
    Moreover, it is also a challenge to keep the model scalable and efficient well
    when big data from smart wearables and mobile applications is growing rapidly
    in size temporally or spatially [[192](#bib.bib192)].
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 实时场景分类。许多场景分类方法采用多阶段训练方式，这对当前存储和计算能力有限的移动/可穿戴设备计算开销较大，因此研究人员开始开发方便高效的统一网络（将所有计算封装在一个阶段的网络中）[[31](#bib.bib31),
    [22](#bib.bib22), [44](#bib.bib44)]。此外，当来自智能可穿戴设备和移动应用的大数据在时间或空间上迅速增长时，保持模型的可扩展性和高效性也是一个挑战
    [[192](#bib.bib192)]。
- en: Imbalanced scene classification. The Places365 challenge dataset [[25](#bib.bib25)]
    has more than 8M training images, and the numbers of images in different classes
    range from 4,000 to 30,000 per class. It shows that scene categories are imbalanced,
    *i.e.,* some categories are abundant while others have scarce examples. Generally,
    the minority class samples are poorly predicted by the learned model [[193](#bib.bib193),
    [194](#bib.bib194)]. Therefore, learning a model which respects both type of categories
    and equally performs well on frequent and infrequent ones remains a great challenge
    and needs further exploration [[195](#bib.bib195), [196](#bib.bib196), [194](#bib.bib194)].
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡场景分类。Places365 挑战数据集 [[25](#bib.bib25)] 拥有超过 8M 的训练图像，不同类别中的图像数量从每类 4,000
    张到 30,000 张不等。这表明场景类别是不平衡的，*即*，有些类别丰富，而有些类别示例稀少。一般来说，少数类样本的预测效果较差 [[193](#bib.bib193),
    [194](#bib.bib194)]。因此，学习一个既能尊重所有类别类型又能在频繁和不频繁类别上表现均衡的模型仍然是一个巨大的挑战，需要进一步探索 [[195](#bib.bib195),
    [196](#bib.bib196), [194](#bib.bib194)]。
- en: 'Continuous scene classification. The ultimate goal is to develop methods capable
    of accurately and efficiently classifying samples in thousands or more unseen
    scene categories in open environments [[107](#bib.bib107)]. The classic deep learning
    paradigm learns in isolation, *i.e.,* it needs many training examples and is only
    suitable for well-defined tasks in closed environments [[76](#bib.bib76), [197](#bib.bib197)].
    In contrast, “human learning” is a continuous learning and adapting to new environments:
    humans accumulate the knowledge gained in the past and use this knowledge to help
    future learning and problem solving with possible adaptations [[198](#bib.bib198)].
    Ideally, it should also be capable to discover unknown scenarios and learn new
    works in a self-supervised manner. Inspired by this, it is necessary to do lifelong
    machine learning via developing versatile systems that continually accumulate
    and refine their knowledge over time [[199](#bib.bib199), [200](#bib.bib200)].
    Such lifelong machine learning has represented a long-standing challenge for deep
    learning and, consequently, artificial intelligence systems.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 连续场景分类。终极目标是开发能够准确高效地分类成千上万或更多未见场景类别的样本的方法 [[107](#bib.bib107)]。经典的深度学习范式在隔离中进行学习，*即*，它需要大量的训练示例，只适用于封闭环境中的明确任务
    [[76](#bib.bib76), [197](#bib.bib197)]。相比之下，“人类学习”是一种持续学习并适应新环境的过程：人类积累过去获得的知识，并利用这些知识帮助未来的学习和问题解决，可能还会进行调整
    [[198](#bib.bib198)]。理想情况下，它还应能够发现未知场景并以自监督的方式学习新任务。受到此启发，有必要通过开发多功能系统来进行终身机器学习，这些系统随着时间的推移不断积累和完善其知识
    [[199](#bib.bib199), [200](#bib.bib200)]。这样的终身机器学习已成为深度学习和人工智能系统长期存在的挑战。
- en: Multi-label scene classification. Many scenes are semantic multiplicity [[68](#bib.bib68),
    [69](#bib.bib69)], *i.e.,* a scene image may belong to multiple semantic classes.
    Such a problem poses a challenge to the classic pattern recognition paradigm and
    requires developing multi-label learning methods [[69](#bib.bib69), [201](#bib.bib201)].
    Moreover, when constructing scene datasets, most researchers either avoid labeling
    multi-label images or use the most obvious class (single label) to annotate subjectively
    each image [[68](#bib.bib68)]. Hence, it is hard to improve the generalization
    ability of the model trained on single-label datasets, which also brings problems
    to classification task.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签场景分类。许多场景具有语义的多重性[[68](#bib.bib68), [69](#bib.bib69)]，*即*，一个场景图像可能属于多个语义类别。这种问题对经典的模式识别范式构成挑战，需要开发多标签学习方法[[69](#bib.bib69),
    [201](#bib.bib201)]。此外，在构建场景数据集时，大多数研究人员要么避免标记多标签图像，要么主观地使用最明显的类别（单标签）来注释每张图像[[68](#bib.bib68)]。因此，很难提高在单标签数据集上训练的模型的泛化能力，这也给分类任务带来了问题。
- en: Other-modal scene classification. RGB images provide key features such as color,
    texture, and spectrum of objects. Nevertheless, the scenes reproduced by RGB images
    may have uneven lighting, target occlusion, *etc*. Therefore, the robustness of
    RGB scene classification is poor, and it is difficult to accurately extract key
    information such as target contours and spatial positions. In contrast, the rapid
    development of sensors has made the acquisition of other modal data easier and
    easier, such as RGB-D [[72](#bib.bib72)], video [[202](#bib.bib202)], 3D point
    clouds [[203](#bib.bib203)]. Recently, research on recognizing and understanding
    various modalities has attracted an increasing attention [[89](#bib.bib89), [204](#bib.bib204),
    [205](#bib.bib205)].
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 其他模态场景分类。RGB 图像提供了诸如颜色、纹理和物体光谱等关键特征。然而，RGB 图像所再现的场景可能存在不均匀的光照、目标遮挡*等*问题。因此，RGB
    场景分类的鲁棒性较差，很难准确提取目标轮廓和空间位置等关键信息。相比之下，传感器的快速发展使得获取其他模态数据变得越来越容易，如 RGB-D [[72](#bib.bib72)]、视频[[202](#bib.bib202)]、3D
    点云[[203](#bib.bib203)]。最近，对识别和理解各种模态的研究引起了越来越多的关注[[89](#bib.bib89), [204](#bib.bib204),
    [205](#bib.bib205)]。
- en: Appendix A A Road Map of Scene Classification in 20 years
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 场景分类的 20 年发展路线图
- en: 'Scene representation or scene feature extraction, the process of converting
    a scene image into feature vectors, plays the critical role in scene classification,
    and thus is the focus of research in this field. In the past two decades, remarkable
    progress has been witnessed in scene representation, which mainly consists of
    two important generations: handcrafted feature engineering, and deep learning
    (feature learning). The milestones of scene classification in the past two decades
    are presented in Fig. [12](#A1.F12 "Figure 12 ‣ Appendix A A Road Map of Scene
    Classification in 20 years ‣ Deep Learning for Scene Classification: A Survey"),
    in which two main stages (SIFT vs. DNN) are highlighted.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '场景表示或场景特征提取，即将场景图像转换为特征向量的过程，在场景分类中扮演着关键角色，因此是该领域研究的重点。在过去的二十年里，场景表示方面取得了显著进展，主要包括两个重要的发展阶段：手工特征工程和深度学习（特征学习）。过去二十年中场景分类的里程碑展示在图[12](#A1.F12
    "Figure 12 ‣ Appendix A A Road Map of Scene Classification in 20 years ‣ Deep
    Learning for Scene Classification: A Survey")中，其中突出了两个主要阶段（SIFT vs. DNN）。'
- en: '![Refer to caption](img/245fff10d608348dfc1bde2d02ed1531.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/245fff10d608348dfc1bde2d02ed1531.png)'
- en: 'Figure 12: Milestones of scene classification. Handcrafted features gained
    tremendous popularity, starting from SIFT [[206](#bib.bib206)] and GIST [[207](#bib.bib207)].
    Then, HoG [[208](#bib.bib208)] and CENTRIST [[209](#bib.bib209)] were proposed
    by Dalal et al. and Wu et al., respectively, further promoting the development
    of scene classification. In 2003, Sivic et al. [[210](#bib.bib210)] proposed BoVW
    model, marking the beginning of codebook learning. Along this way, more effective
    BoVW based methods, SPM [[14](#bib.bib14)], IFV [[104](#bib.bib104)] and VLAD [[105](#bib.bib105)],
    also emerged to deal with larger-scale tasks. In 2010, Object Bank [[15](#bib.bib15),
    [211](#bib.bib211)] represents the scene as object attributes, marking the beginning
    of more semantic representations. Then, Juneja et al. [[212](#bib.bib212)] proposed
    Bags of Part to learn distinctive parts of scenes automatically. In 2012, AlexNet [[17](#bib.bib17)]
    reignites the field of artificial neural networks. Since then, CNN-based methods,
    VGGNet [[77](#bib.bib77)], GoogLeNet [[78](#bib.bib78)] and ResNet [[79](#bib.bib79)],
    have begun to take over handcrafted methods. Additionally, Razavian et al. [[213](#bib.bib213)]
    highlights the effectiveness and generality of CNN representations for different
    tasks. Along this way, in 2014, Gong et al. [[28](#bib.bib28)] proposed MOP-CNN,
    the first deep learning methods for scene classification. Later, FV-CNN [[82](#bib.bib82)],
    Semantic FV [[30](#bib.bib30)] and GAP-CNN [[23](#bib.bib23)] are proposed one
    after another to learn more effective representations. For datasets, ImageNet
    [[56](#bib.bib56)] triggers the breakthrough of deep learning. Then, Xiao et al. [[57](#bib.bib57)]
    proposed SUN database to evaluate numerous algorithms for scene classification.
    Later, Places [[18](#bib.bib18), [25](#bib.bib25)], the largest scene database
    currently, emerged to satisfy the need of deep learning training. Additionally,
    SUN RGBD [[72](#bib.bib72)] has been introduced, marking the beginning of deep
    learning for RGB-D scene classification.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：场景分类的里程碑。手工特征由SIFT[[206](#bib.bib206)]和GIST[[207](#bib.bib207)]开始获得巨大的流行，然后Dalal等人和Wu等人分别提出了HoG[[208](#bib.bib208)]和CENTRIST[[209](#bib.bib209)]，进一步推动了场景分类的发展。2003年，Sivic等人[[210](#bib.bib210)]提出了BoVW模型，开启了码本学习的序章。沿着这条路，更有效的基于BoVW的方法，如SPM[[14](#bib.bib14)]、IFV[[104](#bib.bib104)]和VLAD[[105](#bib.bib105)]等，也出现了以处理更大规模的任务。2010年，Object
    Bank[[15](#bib.bib15), [211](#bib.bib211)]将场景表示为对象属性，标志着更多语义表示的开始。然后，Juneja等人[[212](#bib.bib212)]提出了部分袋来自动学习场景的独特部分。2012年，AlexNet[[17](#bib.bib17)]重新点燃了人工神经网络领域。从那时起，基于CNN的方法，如VGGNet[[77](#bib.bib77)]、GoogLeNet[[78](#bib.bib78)]和ResNet[[79](#bib.bib79)]，已经开始取代手工方法。此外，Razavian等人[[213](#bib.bib213)]强调了CNN表示对不同任务的有效性和普适性。沿着这条路，2014年，Gong等人[[28](#bib.bib28)]提出了MOP-CNN，这是用于场景分类的第一个深度学习方法。随后，FV-CNN[[82](#bib.bib82)]、语义FV[[30](#bib.bib30)]和GAP-CNN[[23](#bib.bib23)]相继提出，以学习更有效的表示。对于数据集，ImageNet[[56](#bib.bib56)]引发了深度学习的突破。然后，Xiao等人[[57](#bib.bib57)]提出了SUN数据库，以评估场景分类的许多算法。此后，Places[[18](#bib.bib18),
    [25](#bib.bib25)]，目前最大的场景数据库，出现以满足深度学习训练的需求。此外，SUN RGBD[[72](#bib.bib72)]已被引入，标志着深度学习用于RGB-D场景分类的开始。
- en: Handcrafted feature engineering era. From 1995 to 2012, the field was dominated
    by the Bag of Visual Word (BoVW) model [[214](#bib.bib214), [106](#bib.bib106),
    [210](#bib.bib210), [215](#bib.bib215)] borrowed from document classification
    which represents a document as a vector of word occurrence counts over a global
    word vocabulary. In the image domain, BoVW firstly probes an image with local
    feature descriptors such as Scale Invariant Feature Transform (SIFT) [[206](#bib.bib206),
    [216](#bib.bib216)], and then represents an image statistically as an orderless
    histogram over a pre-trained visual vocabulary, in a similar form to a document.
    Some important variants of BoVW such as Bag of Semantics [[15](#bib.bib15), [126](#bib.bib126),
    [140](#bib.bib140), [212](#bib.bib212)] and Improved Fisher Vector (IFV) [[104](#bib.bib104)],
    have also been proposed.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 手工特征工程时代。从1995年到2012年，这一领域被来自文件分类的词袋（BoVW）模型所主导[[214](#bib.bib214), [106](#bib.bib106),
    [210](#bib.bib210), [215](#bib.bib215)]，该模型将文档表示为全局词汇表上的单词出现次数的向量。在图像领域，BoVW首先使用诸如尺度不变特征变换（SIFT）[[206](#bib.bib206),
    [216](#bib.bib216)]等局部特征描述符对图像进行探测，然后以统计方式将图像表示为在预先训练的视觉词汇上的无序直方图，形式类似于文档。诸如语义词袋（Bag
    of Semantics）[[15](#bib.bib15), [126](#bib.bib126), [140](#bib.bib140), [212](#bib.bib212)]和改进的Fisher向量（IFV）[[104](#bib.bib104)]等对BoVW的重要变体也已被提出。
- en: Local invariant feature descriptors play an important role in BoVW because they
    are discriminative, yet less sensitive to image variations such as illumination,
    scale, rotation, viewpoint *etc*, and thus have been widely studied. Representative
    local descriptors for scene classification have started from SIFT [[206](#bib.bib206),
    [216](#bib.bib216)] and Global Information Systems Technology (GIST) [[207](#bib.bib207),
    [217](#bib.bib217)]. Other local descriptors, such as Local Binary Patterns (LBP) [[218](#bib.bib218)],
    Deformable Part Model (DPM) [[219](#bib.bib219), [220](#bib.bib220), [221](#bib.bib221)],
    CENsus TRansform hISTogram (CENTRIST) [[209](#bib.bib209)], also contribute to
    the development of scene classification. To improve the performance, research
    focus shifts to feature encoding and aggregation, mainly including Bag-of-Visual-Words
    (BoVW) [[106](#bib.bib106)], Latent Dirichlet Allocation (LDA) [[222](#bib.bib222)],
    Histogram of Gradients (HoG) [[208](#bib.bib208)], Spatial Pyramid Matching (SPM) [[223](#bib.bib223),
    [14](#bib.bib14)], Vector of Locally Aggregated Descriptors (VLAD) [[105](#bib.bib105)],
    Fisher kernel coding [[125](#bib.bib125), [104](#bib.bib104)], Multi-Resolution
    BoVW (MR-BoVW) [[224](#bib.bib224)], and Orientational Pyramid Matching (OPM)
    [[225](#bib.bib225)]. The quality of the learned codebook has a great impact on
    the coding procedure. The generic codebooks mainly include Fisher kernels [[125](#bib.bib125),
    [104](#bib.bib104)], sparse codebook [[226](#bib.bib226), [227](#bib.bib227)],
    Locality-constrained Linear Codes (LLC) [[228](#bib.bib228)], Histogram Intersection
    Kernels (HIK) [[229](#bib.bib229)], contextual visual words [[230](#bib.bib230)],
    Efficient Match Kernels (EMK) [[231](#bib.bib231)] and Supervised Kernel Descriptors
    (SKDES) [[232](#bib.bib232)]. Particularly, semantic codebooks generate from salient
    regions, like Object Bank [[211](#bib.bib211), [15](#bib.bib15), [233](#bib.bib233)],
    object-to-class [[234](#bib.bib234)], Latent Pyramidal Regions (LPR) [[235](#bib.bib235)],
    Bags of Parts (BoP) [[212](#bib.bib212)] and Pairwise Constraints based Multiview
    Subspace Learning (PC-MSL) [[236](#bib.bib236)], capturing more discriminative
    features for scene classification.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 局部不变特征描述符在 BoVW 中扮演着重要角色，因为它们具有区分性，同时对图像的变化（如光照、尺度、旋转、视角*等*）较不敏感，因此已被广泛研究。用于场景分类的代表性局部描述符始于
    SIFT [[206](#bib.bib206), [216](#bib.bib216)] 和全球信息系统技术 (GIST) [[207](#bib.bib207),
    [217](#bib.bib217)]。其他局部描述符，如局部二值模式 (LBP) [[218](#bib.bib218)]、可变形部件模型 (DPM) [[219](#bib.bib219),
    [220](#bib.bib220), [221](#bib.bib221)]、CENsus TRansform hISTogram (CENTRIST)
    [[209](#bib.bib209)]，也对场景分类的发展做出了贡献。为了提高性能，研究重点转向特征编码和聚合，主要包括视觉词袋 (BoVW) [[106](#bib.bib106)]、潜在狄利克雷分配
    (LDA) [[222](#bib.bib222)]、梯度直方图 (HoG) [[208](#bib.bib208)]、空间金字塔匹配 (SPM) [[223](#bib.bib223),
    [14](#bib.bib14)]、局部聚合描述符向量 (VLAD) [[105](#bib.bib105)]、费舍尔核编码 [[125](#bib.bib125),
    [104](#bib.bib104)]、多分辨率 BoVW (MR-BoVW) [[224](#bib.bib224)] 和方向金字塔匹配 (OPM) [[225](#bib.bib225)]。学习到的代码本的质量对编码过程有很大影响。通用代码本主要包括费舍尔核
    [[125](#bib.bib125), [104](#bib.bib104)]、稀疏代码本 [[226](#bib.bib226), [227](#bib.bib227)]、局部约束线性编码
    (LLC) [[228](#bib.bib228)]、直方图交集核 (HIK) [[229](#bib.bib229)]、上下文视觉词 [[230](#bib.bib230)]、高效匹配核
    (EMK) [[231](#bib.bib231)] 和监督核描述符 (SKDES) [[232](#bib.bib232)]。特别地，语义代码本生成于显著区域，如对象库
    [[211](#bib.bib211), [15](#bib.bib15), [233](#bib.bib233)]、对象到类别 [[234](#bib.bib234)]、潜在金字塔区域
    (LPR) [[235](#bib.bib235)]、部件袋 (BoP) [[212](#bib.bib212)] 和基于对的约束的多视角子空间学习 (PC-MSL)
    [[236](#bib.bib236)]，捕捉到更多用于场景分类的区分特征。
- en: Deep learning era. In 2012, Krizhevsky et al. [[17](#bib.bib17)] introduced
    a DNN, commonly referred to as “AlexNet”, for the object classification task,
    and achieved breakthrough performance surpassing the best result of hand-engineered
    features by a large margin, and thus triggered the recent revolution in AI. Since
    then, deep learning has started to dominate various tasks (like computer vision [[80](#bib.bib80),
    [237](#bib.bib237), [107](#bib.bib107), [238](#bib.bib238)], speech recognition [[239](#bib.bib239)],
    autonomous driving [[240](#bib.bib240)], cancer detection [[241](#bib.bib241),
    [242](#bib.bib242)], machine translation [[243](#bib.bib243)], playing complex
    games [[244](#bib.bib244), [245](#bib.bib245), [246](#bib.bib246), [247](#bib.bib247)],
    earthquake forecasting [[248](#bib.bib248)], medicine discovery [[249](#bib.bib249),
    [250](#bib.bib250)]), and scene classification is no exception, leading to a new
    generation of scene representation methods with remarkable performance improvements.
    Such substantial progress can be mainly attributed to advances in deep models
    including VGGNet [[77](#bib.bib77)], GoogLeNet [[78](#bib.bib78)], ResNet [[79](#bib.bib79)],
    *etc.*, the availability of large-scale image datasets like ImageNet [[56](#bib.bib56)]
    and Places [[18](#bib.bib18), [25](#bib.bib25)] and more powerful computational
    resources.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习时代。2012年，Krizhevsky等人[[17](#bib.bib17)] 引入了一种深度神经网络，通常称为“AlexNet”，用于物体分类任务，并取得了突破性的表现，远超手工设计特征的最佳结果，从而引发了最近的人工智能革命。此后，深度学习开始主导各种任务（如计算机视觉[[80](#bib.bib80)、[237](#bib.bib237)、[107](#bib.bib107)、[238](#bib.bib238)]、语音识别[[239](#bib.bib239)]、自动驾驶[[240](#bib.bib240)]、癌症检测[[241](#bib.bib241)、[242](#bib.bib242)]、机器翻译[[243](#bib.bib243)]、复杂游戏[[244](#bib.bib244)、[245](#bib.bib245)、[246](#bib.bib246)、[247](#bib.bib247)]、地震预测[[248](#bib.bib248)]、药物发现[[249](#bib.bib249)、[250](#bib.bib250)]），场景分类也不例外，导致了新一代场景表示方法的显著性能提升。这一重大进展主要归功于深度模型的进步，包括VGGNet[[77](#bib.bib77)]、GoogLeNet[[78](#bib.bib78)]、ResNet[[79](#bib.bib79)]、*等*，以及大型图像数据集如ImageNet[[56](#bib.bib56)]和Places[[18](#bib.bib18)、[25](#bib.bib25)]的可用性和更强大的计算资源。
- en: Deep learning networks have gradually replaced the local feature descriptors
    of the first generation methods and are certainly the engine for scene classification.
    Although the major driving force of progress in scene classification has been
    the incorporation of deep learning networks, the general pipelines like BoVW,
    feature encoding and aggregation methods like Fisher Vector, VLAD of the first
    generation methods have also been adapted in current deep learning based scene
    methods, *e.g.,* MOP-CNN [[28](#bib.bib28)], SCFVC [[83](#bib.bib83)], MPP-CNN [[29](#bib.bib29)],
    DSP [[102](#bib.bib102)], Semantic FV [[30](#bib.bib30)], LatMoG [[58](#bib.bib58)],
    MFA-FS [[42](#bib.bib42)] and DUCA [[20](#bib.bib20)]. To take fully advantage
    of back-propagation, scene representations are extracted from end-to-end trainable
    CNNs, like DAG-CNN [[37](#bib.bib37)], MFAFVNet [[31](#bib.bib31)], VSAD [[32](#bib.bib32)],
    G-MS2F [[39](#bib.bib39)], and DL-CNN [[22](#bib.bib22)]. To focus on main content
    of the scene, object detection is used to capture salient regions, such as MetaObject-CNN
    [[33](#bib.bib33)], WELDON [[34](#bib.bib34)], SDO [[35](#bib.bib35)], and BiLSTM
    [[36](#bib.bib36)]. Since features from multiple CNN layers or multiple views
    are complementary, many literatures [[19](#bib.bib19), [40](#bib.bib40), [41](#bib.bib41),
    [24](#bib.bib24), [21](#bib.bib21)] also explored their complementarity to improve
    performance. In addition, there exists many strategies (like attention mechanism,
    contextual modeling, multi-task learning with regularization terms) to enhance
    representation ability, such as CFA [[24](#bib.bib24)], BiLSTM [[36](#bib.bib36)],
    MAPNet [[96](#bib.bib96)], MSN [[44](#bib.bib44)], and LGN [[45](#bib.bib45)].
    For datasets, because depth images from RGB-D cameras are not vulnerable to illumination
    changes, since 2015, researchers have started to explore RGB-D scene recognition.
    Some works [[47](#bib.bib47), [89](#bib.bib89), [48](#bib.bib48)] focus on depth-specific
    feature learning, while other alternatives, like DMMF [[50](#bib.bib50)], ACM [[144](#bib.bib144)],
    and MSN [[44](#bib.bib44)] focus on multi-modal feature fusion.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络逐渐取代了第一代方法的局部特征描述符，并且无疑是场景分类的核心。虽然推动场景分类进步的主要动力是深度学习网络的引入，但像BoVW、特征编码和聚合方法如Fisher
    Vector、VLAD等第一代方法也被应用于当前基于深度学习的场景方法中，*例如*，MOP-CNN [[28](#bib.bib28)]，SCFVC [[83](#bib.bib83)]，MPP-CNN [[29](#bib.bib29)]，DSP [[102](#bib.bib102)]，Semantic
    FV [[30](#bib.bib30)]，LatMoG [[58](#bib.bib58)]，MFA-FS [[42](#bib.bib42)] 和 DUCA [[20](#bib.bib20)]。为了充分利用反向传播，场景表示从端到端可训练的CNN中提取，如DAG-CNN [[37](#bib.bib37)]，MFAFVNet
    [[31](#bib.bib31)]，VSAD [[32](#bib.bib32)]，G-MS2F [[39](#bib.bib39)]，和DL-CNN [[22](#bib.bib22)]。为了关注场景的主要内容，物体检测用于捕捉显著区域，如MetaObject-CNN
    [[33](#bib.bib33)]，WELDON [[34](#bib.bib34)]，SDO [[35](#bib.bib35)] 和 BiLSTM [[36](#bib.bib36)]。由于来自多个CNN层或多个视图的特征是互补的，许多文献[[19](#bib.bib19)，[40](#bib.bib40)，[41](#bib.bib41)，[24](#bib.bib24)，[21](#bib.bib21)]也探索了它们的互补性以提高性能。此外，还存在许多策略（如注意机制、上下文建模、带有正则化项的多任务学习）来增强表示能力，如CFA
    [[24](#bib.bib24)]，BiLSTM [[36](#bib.bib36)]，MAPNet [[96](#bib.bib96)]，MSN [[44](#bib.bib44)]
    和 LGN [[45](#bib.bib45)]。对于数据集，由于RGB-D相机拍摄的深度图像不易受到光照变化的影响，自2015年以来，研究人员开始探索RGB-D场景识别。一些工作[[47](#bib.bib47)，[89](#bib.bib89)，[48](#bib.bib48)]集中于深度特定特征学习，而其他方法，如DMMF [[50](#bib.bib50)]，ACM [[144](#bib.bib144)]
    和MSN [[44](#bib.bib44)]，则专注于多模态特征融合。
- en: Appendix B A Brief Introduction to Deep Learning
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 深度学习简要介绍
- en: '![Refer to caption](img/b23802d25da463be9f70ee19c861fa47.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b23802d25da463be9f70ee19c861fa47.png)'
- en: (a) VGG
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: (a) VGG
- en: '![Refer to caption](img/62102b4ec42eda513cba7aa00192a52d.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/62102b4ec42eda513cba7aa00192a52d.png)'
- en: (b) Convolution operation
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 卷积操作
- en: 'Figure 13: (a) A typical CNN architecture VGGNet [[77](#bib.bib77)] with 16
    weight layers. The network has 13 convolutional layers, 5 max pooling layers (The
    last one is global max pooling) and 3 fully-connected layers (The last one uses
    Softmax function as nonlinear activation function). The whole network can be learned
    from labeled training data by optimizing a loss function (*e.g.,* , cross-entropy
    loss). (b) Illustration of basic operations (*i.e.,* , convolution, nonlinearity
    and downsampling) that are repeatedly applied for a typical CNN. (b1) The outputs
    (called the feature maps) of each layer (horizontally) of a typical CNN applied
    to a scene image. Each feature map in the second row corresponds to the output
    for one of the learned 3D filters (see (b2)).'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图13： (a) 一个典型的CNN架构VGGNet [[77](#bib.bib77)]，具有16个权重层。该网络有13个卷积层、5个最大池化层（最后一个是全局最大池化）和3个全连接层（最后一个使用Softmax函数作为非线性激活函数）。整个网络可以通过优化损失函数（*例如*，交叉熵损失）从标记的训练数据中学习。
    (b) 典型CNN中反复应用的基本操作（*即*，卷积、非线性和下采样）的示意图。 (b1) 典型CNN应用于场景图像时每层（横向）的输出（称为特征图）。第二行中的每个特征图对应于一个学习到的3D滤波器的输出（参见(b2)）。
- en: In 2012, the breakthrough object classification result on the large scale ImageNet
    dataset [[56](#bib.bib56)] achieved by a deep learning network [[17](#bib.bib17)]
    is arguably what reignited the field of Artificial Neural Networks (ANNs) and
    triggered the recent revolution in AI. Since then, deep learning, or Deep Neural
    Networks (DNNs) [[251](#bib.bib251)], shines in a broad range of areas, including
    computer vision [[17](#bib.bib17), [80](#bib.bib80), [107](#bib.bib107), [238](#bib.bib238),
    [237](#bib.bib237)], speech recognition [[239](#bib.bib239)], autonomous driving [[240](#bib.bib240)],
    cancer detection [[241](#bib.bib241), [242](#bib.bib242)], machine translation [[243](#bib.bib243)],
    playing complex games [[244](#bib.bib244), [245](#bib.bib245), [246](#bib.bib246),
    [247](#bib.bib247)], earthquake forecasting [[248](#bib.bib248)], medicine discovery [[249](#bib.bib249),
    [250](#bib.bib250)], *etc.* In many of these domains, DNNs have reached breakthrough
    levels of performance, often approaching and sometimes exceeding the abilities
    of human experts. Thanks to the growth of big data and more powerful computational
    resources, deep learning and AI for scientific research are evolving quickly,
    with new developments appearing continually for analyzing datasets, discovering
    patterns, and predicting behaviors in almost all fields of science and technology [[98](#bib.bib98)].
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，在大型ImageNet数据集[[56](#bib.bib56)]上取得的突破性对象分类结果，由深度学习网络[[17](#bib.bib17)]实现，可以说重新点燃了人工神经网络（ANNs）领域，并引发了近期的人工智能革命。从那时起，深度学习或深度神经网络（DNNs）[[251](#bib.bib251)]
    在计算机视觉[[17](#bib.bib17), [80](#bib.bib80), [107](#bib.bib107), [238](#bib.bib238),
    [237](#bib.bib237)]、语音识别[[239](#bib.bib239)]、自动驾驶[[240](#bib.bib240)]、癌症检测[[241](#bib.bib241),
    [242](#bib.bib242)]、机器翻译[[243](#bib.bib243)]、复杂游戏[[244](#bib.bib244), [245](#bib.bib245),
    [246](#bib.bib246), [247](#bib.bib247)]、地震预测[[248](#bib.bib248)]、医学发现[[249](#bib.bib249),
    [250](#bib.bib250)]等领域中表现出色。在许多这些领域，DNNs达到了突破性的性能水平，常常接近甚至超越人类专家的能力。由于大数据的增长和更强大的计算资源，深度学习和科学研究中的AI正在快速发展，几乎在所有科学和技术领域中不断出现新的进展，用于分析数据集、发现模式和预测行为[[98](#bib.bib98)]。
- en: 'In computer vision, the most commonly used type of DNNs is Convolutional Neural
    Networks (CNNs) [[76](#bib.bib76)]. As is illustrated in Figure [13](#A2.F13 "Figure
    13 ‣ Appendix B A Brief Introduction to Deep Learning ‣ Deep Learning for Scene
    Classification: A Survey") (a), the frontend of a typical CNN is a stack of CONV
    layers and pooling layers to learn generic-level features, and these features
    are further transformed into class-specific discriminative representations via
    training multiple layers on target datasets. As we slide a convolution filter
    over the width and height of the input of 3 color channels, we will produce a
    two-dimensional (2-D) activation map, as shown in Figure [13](#A2.F13 "Figure
    13 ‣ Appendix B A Brief Introduction to Deep Learning ‣ Deep Learning for Scene
    Classification: A Survey") (b1), giving the responses of that filter at every
    spatial position. As shown in Figure [13](#A2.F13 "Figure 13 ‣ Appendix B A Brief
    Introduction to Deep Learning ‣ Deep Learning for Scene Classification: A Survey") (b2),
    a 2-D convolution operates $x^{l-1}*w^{l}$, describing as a weighted average of
    an input map $x^{l-1}$ from previous layer $l-1$, where the corresponding weighting
    is given by $w^{l}$; since every filter extends through the full depth of the
    input maps with $C$ channels, so we calculate the sum of 2-D convolution as the
    result of a 3-D convolution, *i.e.,*'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '在计算机视觉中，最常用的深度神经网络（DNN）类型是卷积神经网络（CNNs）[[76](#bib.bib76)]。正如图 [13](#A2.F13 "Figure
    13 ‣ Appendix B A Brief Introduction to Deep Learning ‣ Deep Learning for Scene
    Classification: A Survey") (a) 所示，典型 CNN 的前端是由卷积层和池化层堆叠而成，用于学习通用级别的特征，这些特征通过在目标数据集上训练多个层进一步转化为特定类别的判别表示。当我们在三个颜色通道的输入图像的宽度和高度上滑动卷积滤波器时，将产生一个二维（2-D）激活图，如图 [13](#A2.F13
    "Figure 13 ‣ Appendix B A Brief Introduction to Deep Learning ‣ Deep Learning
    for Scene Classification: A Survey") (b1) 所示，给出该滤波器在每个空间位置的响应。如图 [13](#A2.F13
    "Figure 13 ‣ Appendix B A Brief Introduction to Deep Learning ‣ Deep Learning
    for Scene Classification: A Survey") (b2) 所示，二维卷积操作 $x^{l-1}*w^{l}$，描述为来自前一层 $l-1$
    的输入图像 $x^{l-1}$ 的加权平均，其中相应的加权由 $w^{l}$ 给出；由于每个滤波器跨越输入图像的所有通道，所以我们将二维卷积的和计算为三维卷积的结果，*即，*'
- en: '|  | $\sum_{i=1}^{C}{x_{i}^{l-1}*w_{i}^{l}}$ |  | (1) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{i=1}^{C}{x_{i}^{l-1}*w_{i}^{l}}$ |  | (1) |'
- en: During the forward pass, the $j$ neuron in $l$ CONV layer operates a 3-D convolution
    with $N^{l-1}$ channels between input maps $x^{l-1}$ and the corresponding filter
    $w_{j}^{l}$, plus a bias term $b_{j}^{l}$, and passes the above result to a nonlinear
    function $\sigma(\cdot)$ to obtain the final output $x_{j}^{l}$, *i.e.,*
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播过程中，$l$ 层的 $j$ 神经元对输入图像 $x^{l-1}$ 和相应的滤波器 $w_{j}^{l}$ 进行三维卷积操作，包含一个偏置项
    $b_{j}^{l}$，然后将结果传递给非线性函数 $\sigma(\cdot)$，以得到最终输出 $x_{j}^{l}$，*即，*
- en: '|  | $x_{j}^{l}=\sigma(\sum_{i=1}^{N^{l-1}}{x_{i}^{l-1}}*w_{i,j}^{l}+b_{j}^{l})$
    |  | (2) |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{j}^{l}=\sigma(\sum_{i=1}^{N^{l-1}}{x_{i}^{l-1}}*w_{i,j}^{l}+b_{j}^{l})$
    |  | (2) |'
- en: The nonlinear function $\sigma(\cdot)$ is typically a rectified linear unit
    (ReLU) for an element $x$, *i.e.,* $\sigma(x)=\mathrm{max}(x,0)$.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性函数 $\sigma(\cdot)$ 通常是一个修正线性单元（ReLU），对于一个元素 $x$，*即，* $\sigma(x)=\mathrm{max}(x,0)$。
- en: 'It is common to periodically insert a pooling (*i.e.,* downsampling) in-between
    successive convolutional layers in a CNN architecture. Its function is to progressively
    reduce the spatial size of the input maps, as shown in Figure [13](#A2.F13 "Figure
    13 ‣ Appendix B A Brief Introduction to Deep Learning ‣ Deep Learning for Scene
    Classification: A Survey") (b1), to reduce the number of parameters and computation
    in the network, and hence to also control overfitting. Finally, the output layer
    expresses a differentiable score function: from the discriminative representations
    $x$ on one end to class scores $y$ at the other.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '在卷积神经网络（CNN）结构中，通常会在连续的卷积层之间定期插入池化层（*即，* 下采样）。其功能是逐步减少输入图像的空间尺寸，如图 [13](#A2.F13
    "Figure 13 ‣ Appendix B A Brief Introduction to Deep Learning ‣ Deep Learning
    for Scene Classification: A Survey") (b1) 所示，以减少网络中的参数数量和计算量，从而控制过拟合。最后，输出层表示一个可微分的评分函数：从一端的判别特征
    $x$ 到另一端的类别分数 $y$。'
- en: 'Supervised by scene labels, Softmax classifier uses cross-entropy loss function
    to estimate model parameters, and formulas are as follow:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在场景标签的监督下，Softmax 分类器使用交叉熵损失函数来估计模型参数，其公式如下：
- en: '|  | $L_{softmax}=\sum_{i}^{N}L_{CE}(x_{i})$ |  | (3) |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{softmax}=\sum_{i}^{N}L_{CE}(x_{i})$ |  | (3) |'
- en: 'where $L_{CE}(x_{i})$ denotes the cross-entropy loss function of each image
    and its formula is as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{CE}(x_{i})$ 表示每个图像的交叉熵损失函数，其公式如下：
- en: '|  | $L_{CE}(x_{i})=-\sum_{k=1}^{K}(y_{k}\text{log}f_{k}(x_{i}))$ |  | (4)
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{CE}(x_{i})=-\sum_{k=1}^{K}(y_{k}\text{log}f_{k}(x_{i}))$ |  | (4)
    |'
- en: where $x_{i}$ denotes a discriminative feature of scene image $I_{i}$; $K$ denotes
    the count of scene categories; $y_{k}$ denotes the real label of scene image $I_{i}$,
    when $y_{k}=1$ represents that the real label of $I_{i}$ is category $k$, otherwise
    $y_{k}=0$.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{i}$ 表示场景图像 $I_{i}$ 的判别特征；$K$ 表示场景类别的数量；$y_{k}$ 表示场景图像 $I_{i}$ 的真实标签，当
    $y_{k}=1$ 时，表示 $I_{i}$ 的真实标签是类别 $k$，否则 $y_{k}=0$。
- en: Generally, the score function $f(\cdot)$ is Softmax function, denoting the probability
    of estimating scene image $I_{i}$ in class $k$.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，评分函数 $f(\cdot)$ 是Softmax函数，表示估计场景图像 $I_{i}$ 属于类别 $k$ 的概率。
- en: so the $k$ component of output layer is
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输出层的 $k$ 组件是
- en: '|  | $f_{k}(x)=e^{w_{k}x_{k}}/\sum_{j=1}^{K}{e^{w_{j}x_{j}}}$ |  | (5) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{k}(x)=e^{w_{k}x_{k}}/\sum_{j=1}^{K}{e^{w_{j}x_{j}}}$ |  | (5) |'
- en: In addition to the CNN architectures and datasets, some other training strategies
    or tricks have been proposed to achieve better performance. Overfitting happens
    when CNNs learn the detail and noise in the training data to the extent that it
    negatively impacts the generalization ability of models. To this end, some strategies
    have been proposed to reduce the overfitting tendency of CNNs, such as data augmentation,
    early stopping, dropout [[17](#bib.bib17)], smaller convolution kernel size [[77](#bib.bib77),
    [252](#bib.bib252)], and multi-scale cropping/warping [[77](#bib.bib77)]. In addition,
    some optimization techniques have been proposed to overcome the difficulties encountered
    in the training of deep CNNs, such as batch normalization [[123](#bib.bib123),
    [253](#bib.bib253)] and relay back propagation [[193](#bib.bib193)]. Comprehensive
    review of deep learning is out the scope of this survey, and we refer readers
    to [[76](#bib.bib76), [254](#bib.bib254), [255](#bib.bib255), [256](#bib.bib256),
    [257](#bib.bib257)] for more details.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 除了CNN架构和数据集外，还提出了一些其他训练策略或技巧，以实现更好的性能。过拟合发生在CNN学习到训练数据中的细节和噪声，从而对模型的泛化能力产生负面影响。为此，提出了一些策略来减少CNN的过拟合倾向，如数据增强、早停、dropout
    [[17](#bib.bib17)]、较小的卷积核尺寸 [[77](#bib.bib77), [252](#bib.bib252)] 和多尺度裁剪/变形 [[77](#bib.bib77)]。此外，还提出了一些优化技术来克服深度CNN训练中的困难，如批量归一化
    [[123](#bib.bib123), [253](#bib.bib253)] 和继电反向传播 [[193](#bib.bib193)]。对深度学习的全面综述超出了本调查的范围，我们建议读者参阅
    [[76](#bib.bib76), [254](#bib.bib254), [255](#bib.bib255), [256](#bib.bib256),
    [257](#bib.bib257)] 以获取更多详细信息。
- en: Acknowledgments
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank the pioneer researchers in scene classification
    and other related fields. This work was supported in part by grants from National
    Science Foundation of China (61872379, 91846301, 61571005), the Academy of Finland
    (331883), the fundamental research program of Guangdong, China (2020B1515310023),
    the Hunan Science and Technology Plan Project (2019GK2131), the China Scholarship
    Council (201806155037), the Science and Technology Research Program of Guangzhou,
    China (201804010429).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢场景分类及相关领域的开创性研究者。本研究部分得到了中国国家自然科学基金（61872379, 91846301, 61571005）、芬兰科学院（331883）、广东省基础研究计划（2020B1515310023）、湖南省科技计划项目（2019GK2131）、中国国家留学基金委员会（201806155037）、广州市科技研究计划（201804010429）的资助。
- en: References
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. M. Henderson and A. Hollingworth, “High-level scene perception,” *Annual
    review of psychology*, vol. 50, no. 1, pp. 243–271, 1999.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. M. Henderson 和 A. Hollingworth，“高级场景感知，” *Annual review of psychology*，第50卷，第1期，页243–271，1999年。'
- en: '[2] R. Epstein, “The cortical basis of visual scene processing,” *Visual Cognition*,
    vol. 12, no. 6, pp. 954–978, 2005.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] R. Epstein，“视觉场景处理的皮层基础，” *Visual Cognition*，第12卷，第6期，页954–978，2005年。'
- en: '[3] M. R. Greene and A. Oliva, “The briefest of glances: The time course of
    natural scene understanding,” *Psychological Science*, vol. 20, no. 4, pp. 464–472,
    2009.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. R. Greene 和 A. Oliva，“短暂的瞥见：自然场景理解的时间过程，” *Psychological Science*，第20卷，第4期，页464–472，2009年。'
- en: '[4] D. B. Walther, B. Chai, E. Caddigan, D. M. Beck, and L. Fei-Fei, “Simple
    line drawings suffice for functional MRI decoding of natural scene categories,”
    *PNAS*, vol. 108, no. 23, pp. 9661–9666, 2011.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] D. B. Walther, B. Chai, E. Caddigan, D. M. Beck 和 L. Fei-Fei，“简单的线条图足以用于自然场景类别的功能性MRI解码，”
    *PNAS*，第108卷，第23期，页9661–9666，2011年。'
- en: '[5] J. Vogel and B. Schiele, “Semantic modeling of natural scenes for content-based
    image retrieval,” *IJCV*, vol. 72, no. 2, pp. 133–157, 2007.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Vogel 和 B. Schiele，“基于内容的图像检索中的自然场景语义建模，” *IJCV*，第72卷，第2期，页133–157，2007年。'
- en: '[6] L. Zheng, Y. Yang, and Q. Tian, “SIFT meets CNN: A decade survey of instance
    retrieval,” *IEEE TPAMI*, vol. 40, no. 5, pp. 1224–1244, 2017.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] L. Zheng, Y. Yang 和 Q. Tian，“SIFT 遇见 CNN：实例检索的十年调查，” *IEEE TPAMI*，第 40
    卷，第 5 期，第 1224–1244 页，2017 年。'
- en: '[7] W. Zhang, X. Yu, and X. He, “Learning bidirectional temporal cues for video-based
    person re-identification,” *IEEE TCSVT*, vol. 28, no. 10, pp. 2768–2776, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] W. Zhang, X. Yu 和 X. He，“学习双向时间线索进行基于视频的人物再识别，” *IEEE TCSVT*，第 28 卷，第 10
    期，第 2768–2776 页，2017 年。'
- en: '[8] J. Hou, H. Zeng, J. Zhu, J. Hou, J. Chen, and K.-K. Ma, “Deep quadruplet
    appearance learning for vehicle re-identification,” *IEEE TVT*, vol. 68, no. 9,
    pp. 8512–8522, 2019.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Hou, H. Zeng, J. Zhu, J. Hou, J. Chen 和 K.-K. Ma，“用于车辆再识别的深度四重外观学习，”
    *IEEE TVT*，第 68 卷，第 9 期，第 8512–8522 页，2019 年。'
- en: '[9] T. Zhang, S. Liu, C. Xu, and H. Lu, “Mining semantic context information
    for intelligent video surveillance of traffic scenes,” *IEEE TII*, vol. 9, no. 1,
    pp. 149–160, 2012.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] T. Zhang, S. Liu, C. Xu 和 H. Lu，“挖掘语义上下文信息以实现交通场景的智能视频监控，” *IEEE TII*，第
    9 卷，第 1 期，第 149–160 页，2012 年。'
- en: '[10] G. Sreenu and M. S. Durai, “Intelligent video surveillance: A review through
    deep learning techniques for crowd analysis,” *Journal of Big Data*, vol. 6, no. 1,
    p. 48, 2019.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] G. Sreenu 和 M. S. Durai，“智能视频监控：通过深度学习技术进行人群分析的综述，” *大数据杂志*，第 6 卷，第 1
    期，第 48 页，2019 年。'
- en: '[11] A. H. Behzadan and V. R. Kamat, “Integrated information modeling and visual
    simulation of engineering operations using dynamic augmented reality scene graphs,”
    *ITcon*, vol. 16, no. 17, pp. 259–278, 2011.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. H. Behzadan 和 V. R. Kamat，“使用动态增强现实场景图的工程操作集成信息建模和视觉仿真，” *ITcon*，第
    16 卷，第 17 期，第 259–278 页，2011 年。'
- en: '[12] A. Y. Nee, S. Ong, G. Chryssolouris, and D. Mourtzis, “Augmented reality
    applications in design and manufacturing,” *CIRP annals*, vol. 61, no. 2, pp.
    657–679, 2012.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Y. Nee, S. Ong, G. Chryssolouris 和 D. Mourtzis，“设计和制造中的增强现实应用，” *CIRP
    年鉴*，第 61 卷，第 2 期，第 657–679 页，2012 年。'
- en: '[13] K. Muhammad, J. Ahmad, and S. W. Baik, “Early fire detection using convolutional
    neural networks during surveillance for effective disaster management,” *Neurocomputing*,
    vol. 288, pp. 30–42, 2018.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] K. Muhammad, J. Ahmad 和 S. W. Baik，“使用卷积神经网络进行早期火灾检测，以实现有效的灾害管理，” *神经计算*，第
    288 卷，第 30–42 页，2018 年。'
- en: '[14] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial
    pyramid matching for recognizing natural scene categories,” in *CVPR*, vol. 2,
    2006, pp. 2169–2178, https://figshare.com/articles/15-Scene_Image_Dataset/7007177.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] S. Lazebnik, C. Schmid 和 J. Ponce，“超越特征袋：用于识别自然场景类别的空间金字塔匹配，” 在 *CVPR*，第
    2 卷，2006 年，第 2169–2178 页，https://figshare.com/articles/15-Scene_Image_Dataset/7007177。'
- en: '[15] L.-J. Li, H. Su, L. Fei-Fei, and E. P. Xing, “Object bank: A high-level
    image representation for scene classification & semantic feature sparsification,”
    in *NeurIPS*, 2010, pp. 1378–1386.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] L.-J. Li, H. Su, L. Fei-Fei 和 E. P. Xing，“对象库：用于场景分类和语义特征稀疏化的高级图像表示，”
    在 *NeurIPS*，2010 年，第 1378–1386 页。'
- en: '[16] R. Margolin, L. Zelnik-Manor, and A. Tal, “OTC: A novel local descriptor
    for scene classification,” in *ECCV*, 2014, pp. 377–391.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] R. Margolin, L. Zelnik-Manor 和 A. Tal，“OTC：一种新颖的局部描述符用于场景分类，” 在 *ECCV*，2014
    年，第 377–391 页。'
- en: '[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NeurIPS*, 2012, pp. 1097–1105.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Krizhevsky, I. Sutskever 和 G. E. Hinton，“使用深度卷积神经网络的 Imagenet 分类，”
    在 *NeurIPS*，2012 年，第 1097–1105 页。'
- en: '[18] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, “Learning deep
    features for scene recognition using places database,” in *NeurIPS*, 2014, pp.
    487–495, http://places.csail.mit.edu/downloadData.html.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba 和 A. Oliva，“使用 Places 数据库进行场景识别的深度特征学习，”
    在 *NeurIPS*，2014 年，第 487–495 页，http://places.csail.mit.edu/downloadData.html。'
- en: '[19] L. Herranz, S. Jiang, and X. Li, “Scene recognition with CNNs: Objects,
    scales and dataset bias,” in *CVPR*, 2016, pp. 571–579.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] L. Herranz, S. Jiang 和 X. Li，“使用 CNN 进行场景识别：对象、尺度和数据集偏差，” 在 *CVPR*，2016
    年，第 571–579 页。'
- en: '[20] S. H. Khan, M. Hayat, M. Bennamoun, R. Togneri, and F. A. Sohel, “A discriminative
    representation of convolutional features for indoor scene recognition,” *IEEE
    TIP*, vol. 25, no. 7, pp. 3372–3383, 2016.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. H. Khan, M. Hayat, M. Bennamoun, R. Togneri 和 F. A. Sohel，“用于室内场景识别的卷积特征的判别表示，”
    *IEEE TIP*，第 25 卷，第 7 期，第 3372–3383 页，2016 年。'
- en: '[21] S. Liu, G. Tian, and Y. Xu, “A novel scene classification model combining
    ResNet based transfer learning and data augmentation with a filter,” *Neurocomputing*,
    vol. 338, pp. 191–206, 2019.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Liu, G. Tian 和 Y. Xu，“一种新型场景分类模型，结合了基于 ResNet 的迁移学习和数据增强与滤波器，” *神经计算*，第
    338 卷，第 191–206 页，2019 年。'
- en: '[22] Y. Liu, Q. Chen, W. Chen, and I. Wassell, “Dictionary learning inspired
    deep network for scene recognition,” in *AAAI*, 2018.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. Liu, Q. Chen, W. Chen, 和 I. Wassell，“受字典学习启发的深度网络用于场景识别，”在 *AAAI*，2018年。'
- en: '[23] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *CVPR*, 2016, pp. 2921–2929.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, 和 A. Torralba，“学习深度特征以进行判别性定位，”在
    *CVPR*，2016年，页码2921–2929。'
- en: '[24] N. Sun, W. Li, J. Liu, G. Han, and C. Wu, “Fusing object semantics and
    deep appearance features for scene recognition,” *IEEE TCSVT*, vol. 29, no. 6,
    pp. 1715–1728, 2018.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] N. Sun, W. Li, J. Liu, G. Han, 和 C. Wu，“融合对象语义和深度外观特征进行场景识别，”*IEEE TCSVT*，第29卷，第6期，页码1715–1728，2018年。'
- en: '[25] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, “Places:
    A 10 million image database for scene recognition,” *IEEE TPAMI*, vol. 40, no. 6,
    pp. 1452–1464, 2017, http://places2.csail.mit.edu/download.html.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, 和 A. Torralba，“Places: 一个包含1000万图像的场景识别数据库，”*IEEE
    TPAMI*，第40卷，第6期，页码1452–1464，2017年，[http://places2.csail.mit.edu/download.html](http://places2.csail.mit.edu/download.html)。'
- en: '[26] M. Hayat, S. H. Khan, M. Bennamoun, and S. An, “A spatial layout and scale
    invariant feature representation for indoor scene classification,” *IEEE TIP*,
    vol. 25, no. 10, pp. 4829–4841, 2016.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Hayat, S. H. Khan, M. Bennamoun, 和 S. An，“用于室内场景分类的空间布局和尺度不变特征表示，”*IEEE
    TIP*，第25卷，第10期，页码4829–4841，2016年。'
- en: '[27] Z. Zuo, B. Shuai, G. Wang, X. Liu, X. Wang, B. Wang, and Y. Chen, “Learning
    contextual dependence with convolutional hierarchical recurrent neural networks,”
    *IEEE TIP*, vol. 25, no. 7, pp. 2983–2996, 2016.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Z. Zuo, B. Shuai, G. Wang, X. Liu, X. Wang, B. Wang, 和 Y. Chen，“利用卷积层次递归神经网络学习上下文依赖，”*IEEE
    TIP*，第25卷，第7期，页码2983–2996，2016年。'
- en: '[28] Y. Gong, L. Wang, R. Guo, and S. Lazebnik, “Multi-scale orderless pooling
    of deep convolutional activation features,” in *ECCV*, 2014, pp. 392–407.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Gong, L. Wang, R. Guo, 和 S. Lazebnik，“深度卷积激活特征的多尺度无序池化，”在 *ECCV*，2014年，页码392–407。'
- en: '[29] D. Yoo, S. Park, J.-Y. Lee, and I. So Kweon, “Multi-scale pyramid pooling
    for deep convolutional representation,” in *CVPRW*, 2015, pp. 71–80.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] D. Yoo, S. Park, J.-Y. Lee, 和 I. So Kweon，“深度卷积表示的多尺度金字塔池化，”在 *CVPRW*，2015年，页码71–80。'
- en: '[30] M. Dixit, S. Chen, D. Gao, N. Rasiwasia, and N. Vasconcelos, “Scene classification
    with semantic fisher vectors,” in *CVPR*, 2015, pp. 2974–2983.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] M. Dixit, S. Chen, D. Gao, N. Rasiwasia, 和 N. Vasconcelos，“使用语义Fisher向量进行场景分类，”在
    *CVPR*，2015年，页码2974–2983。'
- en: '[31] Y. Li, M. Dixit, and N. Vasconcelos, “Deep scene image classification
    with the MFAFVNet,” in *ICCV*, 2017, pp. 5746–5754.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y. Li, M. Dixit, 和 N. Vasconcelos，“使用MFAFVNet进行深度场景图像分类，”在 *ICCV*，2017年，页码5746–5754。'
- en: '[32] Z. Wang, L. Wang, Y. Wang, B. Zhang, and Y. Qiao, “Weakly supervised patchnets:
    Describing and aggregating local patches for scene recognition,” *IEEE TIP*, vol. 26,
    no. 4, pp. 2028–2041, 2017.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Z. Wang, L. Wang, Y. Wang, B. Zhang, 和 Y. Qiao，“弱监督Patchnets: 描述和聚合局部补丁用于场景识别，”*IEEE
    TIP*，第26卷，第4期，页码2028–2041，2017年。'
- en: '[33] R. Wu, B. Wang, W. Wang, and Y. Yu, “Harvesting discriminative meta objects
    with deep CNN features for scene classification,” in *ICCV*, 2015, pp. 1287–1295.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] R. Wu, B. Wang, W. Wang, 和 Y. Yu，“利用深度CNN特征收获判别性元对象进行场景分类，”在 *ICCV*，2015年，页码1287–1295。'
- en: '[34] T. Durand, N. Thome, and M. Cord, “WELDON: Weakly supervised learning
    of deep convolutional neural networks,” in *CVPR*, 2016, pp. 4743–4752.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] T. Durand, N. Thome, 和 M. Cord，“WELDON: 弱监督学习深度卷积神经网络，”在 *CVPR*，2016年，页码4743–4752。'
- en: '[35] X. Cheng, J. Lu, J. Feng, B. Yuan, and J. Zhou, “Scene recognition with
    objectness,” *Pattern Recognition*, vol. 74, pp. 474–487, 2018.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] X. Cheng, J. Lu, J. Feng, B. Yuan, 和 J. Zhou，“利用对象特征进行场景识别，”*Pattern Recognition*，第74卷，页码474–487，2018年。'
- en: '[36] C. Laranjeira, A. Lacerda, and E. R. Nascimento, “On modeling context
    from objects with a long short-term memory for indoor scene recognition,” in *SIBGRAPI*,
    2019, pp. 249–256.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] C. Laranjeira, A. Lacerda, 和 E. R. Nascimento，“使用长短期记忆模型从对象中建模上下文以进行室内场景识别，”在
    *SIBGRAPI*，2019年，页码249–256。'
- en: '[37] S. Yang and D. Ramanan, “Multi-scale recognition with DAG-CNNs,” in *ICCV*,
    2015, pp. 1215–1223.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. Yang 和 D. Ramanan，“使用DAG-CNNs进行多尺度识别，”在 *ICCV*，2015年，页码1215–1223。'
- en: '[38] G.-S. Xie, X.-Y. Zhang, S. Yan, and C.-L. Liu, “Hybrid CNN and dictionary-based
    models for scene recognition and domain adaptation,” *IEEE TCSVT*, vol. 27, no. 6,
    pp. 1263–1274, 2015.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] G.-S. Xie, X.-Y. Zhang, S. Yan, 和 C.-L. Liu，“用于场景识别和领域适应的混合CNN和基于字典的模型，”*IEEE
    TCSVT*，第27卷，第6期，页码1263–1274，2015年。'
- en: '[39] P. Tang, H. Wang, and S. Kwong, “G-MS2F: Googlenet based multi-stage feature
    fusion of deep CNN for scene recognition,” *Neurocomputing*, vol. 225, pp. 188–197,
    2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] P. Tang, H. Wang, 和 S. Kwong，“G-MS2F: 基于Googlenet的多阶段特征融合深度CNN用于场景识别，”*Neurocomputing*，第225卷，页码188–197，2017年。'
- en: '[40] S. Guo, W. Huang, L. Wang, and Y. Qiao, “Locally supervised deep hybrid
    model for scene recognition,” *IEEE TIP*, vol. 26, no. 2, pp. 808–820, 2016.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. Guo, W. Huang, L. Wang, 和 Y. Qiao, “用于场景识别的局部监督深度混合模型”，*IEEE TIP*，第26卷，第2期，第808–820页，2016年。'
- en: '[41] L. Wang, S. Guo, W. Huang, Y. Xiong, and Y. Qiao, “Knowledge guided disambiguation
    for large-scale scene classification with multi-resolution CNNs,” *IEEE TIP*,
    vol. 26, no. 4, pp. 2055–2068, 2017.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] L. Wang, S. Guo, W. Huang, Y. Xiong, 和 Y. Qiao, “用于大规模场景分类的知识指导消歧，基于多分辨率CNNs”，*IEEE
    TIP*，第26卷，第4期，第2055–2068页，2017年。'
- en: '[42] M. D. Dixit and N. Vasconcelos, “Object based scene representations using
    fisher scores of local subspace projections,” in *NeurIPS*, 2016, pp. 2811–2819.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. D. Dixit 和 N. Vasconcelos, “基于Fisher分数的局部子空间投影的对象场景表示”，发表于 *NeurIPS*，2016年，第2811–2819页。'
- en: '[43] A. López-Cifuentes, M. Escudero-Viñolo, J. Bescós, and Á. García-Martín,
    “Semantic-aware scene recognition,” *Pattern Recognition*, vol. 102, p. 107256,
    2020.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] A. López-Cifuentes, M. Escudero-Viñolo, J. Bescós, 和 Á. García-Martín,
    “语义感知场景识别”，*Pattern Recognition*，第102卷，第107256页，2020年。'
- en: '[44] Z. Xiong, Y. Yuan, and Q. Wang, “MSN: Modality separation networks for
    RGB-D scene recognition,” *Neurocomputing*, vol. 373, pp. 81–89, 2020.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Z. Xiong, Y. Yuan, 和 Q. Wang, “MSN: 用于RGB-D场景识别的模态分离网络”，*Neurocomputing*，第373卷，第81–89页，2020年。'
- en: '[45] G. Chen, X. Song, H. Zeng, and S. Jiang, “Scene recognition with prototype-agnostic
    scene layout,” *IEEE TIP*, vol. 29, pp. 5877–5888, 2020.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] G. Chen, X. Song, H. Zeng, 和 S. Jiang, “使用原型无关场景布局的场景识别”，*IEEE TIP*，第29卷，第5877–5888页，2020年。'
- en: '[46] A. Wang, J. Cai, J. Lu, and T.-J. Cham, “Modality and component aware
    feature fusion for RGB-D scene classification,” in *CVPR*, 2016, pp. 5995–6004.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Wang, J. Cai, J. Lu, 和 T.-J. Cham, “针对RGB-D场景分类的模态和组件感知特征融合”，发表于 *CVPR*，2016年，第5995–6004页。'
- en: '[47] X. Song, S. Jiang, and L. Herranz, “Combining models from multiple sources
    for RGB-D scene recognition.” in *IJCAI*, 2017, pp. 4523–4529.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] X. Song, S. Jiang, 和 L. Herranz, “结合来自多个来源的模型进行RGB-D场景识别”，发表于 *IJCAI*，2017年，第4523–4529页。'
- en: '[48] D. Du, L. Wang, H. Wang, K. Zhao, and G. Wu, “Translate-to-recognize networks
    for RGB-D scene recognition,” in *CVPR*, 2019, pp. 11 836–11 845.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] D. Du, L. Wang, H. Wang, K. Zhao, 和 G. Wu, “用于RGB-D场景识别的翻译识别网络”，发表于 *CVPR*，2019年，第11 836–11 845页。'
- en: '[49] Y. Cheng, R. Cai, Z. Li, X. Zhao, and K. Huang, “Locality-sensitive deconvolution
    networks with gated fusion for RGB-D indoor semantic segmentation,” in *CVPR*,
    2017, pp. 3029–3037.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Y. Cheng, R. Cai, Z. Li, X. Zhao, 和 K. Huang, “具有门控融合的局部敏感去卷积网络用于RGB-D室内语义分割”，发表于
    *CVPR*，2017年，第3029–3037页。'
- en: '[50] H. Zhu, J.-B. Weibel, and S. Lu, “Discriminative multi-modal feature fusion
    for RGB-D indoor scene recognition,” in *CVPR*, 2016, pp. 2969–2976.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] H. Zhu, J.-B. Weibel, 和 S. Lu, “用于RGB-D室内场景识别的区分性多模态特征融合”，发表于 *CVPR*，2016年，第2969–2976页。'
- en: '[51] R. Girshick, “Fast R-CNN,” in *ICCV*, 2015, pp. 1440–1448.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] R. Girshick, “Fast R-CNN”，发表于 *ICCV*，2015年，第1440–1448页。'
- en: '[52] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time
    object detection with region proposal networks,” *IEEE TPAMI*, vol. 39, no. 6,
    pp. 1137–1149, 2016.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. Ren, K. He, R. Girshick, 和 J. Sun, “Faster R-CNN: 通过区域提议网络实现实时目标检测”，*IEEE
    TPAMI*，第39卷，第6期，第1137–1149页，2016年。'
- en: '[53] V. Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE TPAMI*, vol. 39, no. 12,
    pp. 2481–2495, 2017.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] V. Badrinarayanan, A. Kendall, 和 R. Cipolla, “SegNet: 一种用于图像分割的深度卷积编码器-解码器架构”，*IEEE
    TPAMI*，第39卷，第12期，第2481–2495页，2017年。'
- en: '[54] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE TPAMI*, vol. 40, no. 4, pp. 834–848, 2017.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, 和 A. L. Yuille, “Deeplab:
    使用深度卷积网络、空洞卷积和全连接CRF的语义图像分割”，*IEEE TPAMI*，第40卷，第4期，第834–848页，2017年。'
- en: '[55] S. Cai, J. Huang, D. Zeng, X. Ding, and J. Paisley, “MEnet: A metric expression
    network for salient object segmentation,” in *IJCAI*, 2018, pp. 598–605.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] S. Cai, J. Huang, D. Zeng, X. Ding, 和 J. Paisley, “MEnet: 一种用于显著目标分割的度量表达网络”，发表于
    *IJCAI*，2018年，第598–605页。'
- en: '[56] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *CVPR*, 2009, pp. 248–255, http://image-net.org/download.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, 和 L. Fei-Fei, “Imagenet:
    大规模层次图像数据库”，发表于 *CVPR*，2009年，第248–255页，http://image-net.org/download。'
- en: '[57] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, “Sun database:
    Large-scale scene recognition from abbey to zoo,” in *CVPR*, 2010, pp. 3485–3492,
    http://places2.csail.mit.edu/download.html.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, 和 A. Torralba, “Sun数据库: 从修道院到动物园的大规模场景识别”，发表于
    *CVPR*，2010年，第3485–3492页，http://places2.csail.mit.edu/download.html。'
- en: '[58] R. G. Cinbis, J. Verbeek, and C. Schmid, “Approximate fisher kernels of
    non-iid image models for image categorization,” *IEEE TPAMI*, vol. 38, no. 6,
    pp. 1084–1098, 2015.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] R. G. Cinbis, J. Verbeek, 和 C. Schmid，“用于图像分类的非独立同分布图像模型的近似Fisher核”，*IEEE
    TPAMI*，第38卷，第6期，第1084–1098页，2015年。'
- en: '[59] X. Wei, S. L. Phung, and A. Bouzerdoum, “Visual descriptors for scene
    categorization: Experimental evaluation,” *AI Review*, vol. 45, no. 3, pp. 333–368,
    2016.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] X. Wei, S. L. Phung, 和 A. Bouzerdoum，“场景分类的视觉描述符：实验评估”，*AI Review*，第45卷，第3期，第333–368页，2016年。'
- en: '[60] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene classification:
    Benchmark and state of the art,” *Proceedings of the IEEE*, vol. 105, no. 10,
    pp. 1865–1883, 2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] G. Cheng, J. Han, 和 X. Lu，“遥感图像场景分类：基准与最新进展”，*Proceedings of the IEEE*，第105卷，第10期，第1865–1883页，2017年。'
- en: '[61] L. Xie, F. Lee, L. Liu, K. Kotani, and Q. Chen, “Scene recognition: A
    comprehensive survey,” *Pattern Recognition*, vol. 102, p. 107205, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] L. Xie, F. Lee, L. Liu, K. Kotani, 和 Q. Chen，“场景识别：综合综述”，*Pattern Recognition*，第102卷，第107205页，2020年。'
- en: '[62] K. Nogueira, O. A. Penatti, and J. A. Dos Santos, “Towards better exploiting
    convolutional neural networks for remote sensing scene classification,” *Pattern
    Recognition*, vol. 61, pp. 539–556, 2017.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] K. Nogueira, O. A. Penatti, 和 J. A. Dos Santos，“更好地利用卷积神经网络进行遥感场景分类”，*Pattern
    Recognition*，第61卷，第539–556页，2017年。'
- en: '[63] F. Hu, G.-S. Xia, J. Hu, and L. Zhang, “Transferring deep convolutional
    neural networks for the scene classification of high-resolution remote sensing
    imagery,” *Remote Sensing*, vol. 7, no. 11, pp. 14 680–14 707, 2015.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] F. Hu, G.-S. Xia, J. Hu, 和 L. Zhang，“将深度卷积神经网络用于高分辨率遥感图像的场景分类”，*Remote
    Sensing*，第7卷，第11期，第14,680–14,707页，2015年。'
- en: '[64] A. Mesaros, T. Heittola, and T. Virtanen, “Tut database for acoustic scene
    classification and sound event detection,” in *EUSIPCO*, 2016, pp. 1128–1132.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] A. Mesaros, T. Heittola, 和 T. Virtanen，“用于声学场景分类和声音事件检测的TUT数据库”，发表于*EUSIPCO*，2016年，第1128–1132页。'
- en: '[65] Z. Ren, K. Qian, Y. Wang, Z. Zhang, V. Pandit, A. Baird, and B. Schuller,
    “Deep scalogram representations for acoustic scene classification,” *JAS*, vol. 5,
    no. 3, pp. 662–669, 2018.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Z. Ren, K. Qian, Y. Wang, Z. Zhang, V. Pandit, A. Baird, 和 B. Schuller，“用于声学场景分类的深度尺度图表示”，*JAS*，第5卷，第3期，第662–669页，2018年。'
- en: '[66] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, and
    M. J. Milford, “Visual place recognition: A survey,” *IEEE T-RO*, vol. 32, no. 1,
    pp. 1–19, 2015.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, 和
    M. J. Milford，“视觉场所识别：综述”，*IEEE T-RO*，第32卷，第1期，第1–19页，2015年。'
- en: '[67] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, “NetVLAD:
    CNN architecture for weakly supervised place recognition,” in *CVPR*, 2016, pp.
    5297–5307.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, 和 J. Sivic，“NetVLAD：用于弱监督场所识别的CNN架构”，发表于*CVPR*，2016年，第5297–5307页。'
- en: '[68] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown, “Learning multi-label
    scene classification,” *Pattern Recognition*, vol. 37, no. 9, pp. 1757–1771, 2004.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] M. R. Boutell, J. Luo, X. Shen, 和 C. M. Brown，“学习多标签场景分类”，*Pattern Recognition*，第37卷，第9期，第1757–1771页，2004年。'
- en: '[69] M.-L. Zhang and Z.-H. Zhou, “Multi-label learning by instance differentiation,”
    in *AAAI*, vol. 7, 2007, pp. 669–674.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M.-L. Zhang 和 Z.-H. Zhou，“通过实例区分进行多标签学习”，发表于*AAAI*，第7卷，2007年，第669–674页。'
- en: '[70] A. Quattoni and A. Torralba, “Recognizing indoor scenes,” in *CVPR*, 2009,
    pp. 413–420, http://web.mit.edu/torralba/www/indoor.html.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] A. Quattoni 和 A. Torralba，“识别室内场景”，发表于*CVPR*，2009年，第413–420页，http://web.mit.edu/torralba/www/indoor.html。'
- en: '[71] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
    and support inference from RGB-D images,” in *ECCV*, 2012, pp. 746–760, https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] N. Silberman, D. Hoiem, P. Kohli, 和 R. Fergus，“室内分割与RGB-D图像的支持推断”，发表于*ECCV*，2012年，第746–760页，https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html。'
- en: '[72] S. Song, S. P. Lichtenberg, and J. Xiao, “SUN RGB-D: A RGB-D scene understanding
    benchmark suite,” in *CVPR*, 2015, pp. 567–576, https://github.com/ankurhanda/sunrgbd-meta-data.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] S. Song, S. P. Lichtenberg, 和 J. Xiao，“SUN RGB-D：RGB-D场景理解基准套件”，发表于*CVPR*，2015年，第567–576页，https://github.com/ankurhanda/sunrgbd-meta-data。'
- en: '[73] S. Gupta, R. Girshick, P. Arbeláez, and J. Malik, “Learning rich features
    from RGB-D images for object detection and segmentation,” in *ECCV*, 2014, pp.
    345–360.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] S. Gupta, R. Girshick, P. Arbeláez, 和 J. Malik，“从RGB-D图像中学习丰富特征以进行对象检测和分割”，发表于*ECCV*，2014年，第345–360页。'
- en: '[74] G. A. Miller, “Wordnet: A lexical database for english,” *Communications
    of the ACM*, vol. 38, no. 11, pp. 39–41, 1995.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] G. A. Miller，“Wordnet：一个英语词汇数据库”，*Communications of the ACM*，第38卷，第11期，第39–41页，1995年。'
- en: '[75] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” *IJCV*, vol. 88, no. 2, pp.
    303–338, 2010.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. Everingham, L. Van Gool, C. K. Williams, J. Winn 和 A. Zisserman, “Pascal视觉对象类别（VOC）挑战，”
    *IJCV*, 第88卷，第2期, 第303–338页, 2010。'
- en: '[76] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. LeCun, Y. Bengio 和 G. Hinton, “深度学习，” *Nature*, 第521卷，第7553期, 第436–444页,
    2015。'
- en: '[77] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *ICLR*, 2015.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络，” *ICLR*, 2015。'
- en: '[78] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *CVPR*,
    2015, pp. 1–9.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke 和 A. Rabinovich, “通过卷积深入，” 见 *CVPR*, 2015, 第1–9页。'
- en: '[79] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] K. He, X. Zhang, S. Ren 和 J. Sun, “用于图像识别的深度残差学习，” 见 *CVPR*, 2016, 第770–778页。'
- en: '[80] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in *CVPR*, 2014, pp.
    580–587.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] R. Girshick, J. Donahue, T. Darrell 和 J. Malik, “用于准确目标检测和语义分割的丰富特征层次结构，”
    见 *CVPR*, 2014, 第580–587页。'
- en: '[81] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
    features in deep neural networks?” in *NeurIPS*, 2014, pp. 3320–3328.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] J. Yosinski, J. Clune, Y. Bengio 和 H. Lipson, “深度神经网络中的特征转移性如何？” 见 *NeurIPS*,
    2014, 第3320–3328页。'
- en: '[82] M. Cimpoi, S. Maji, and A. Vedaldi, “Deep filter banks for texture recognition
    and segmentation,” in *CVPR*, 2015, pp. 3828–3836.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] M. Cimpoi, S. Maji 和 A. Vedaldi, “用于纹理识别和分割的深度滤波器组，” 见 *CVPR*, 2015, 第3828–3836页。'
- en: '[83] L. Liu, C. Shen, L. Wang, A. Van Den Hengel, and C. Wang, “Encoding high
    dimensional local features by sparse coding based fisher vectors,” in *NeurIPS*,
    2014, pp. 1143–1151.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] L. Liu, C. Shen, L. Wang, A. Van Den Hengel 和 C. Wang, “通过基于稀疏编码的Fisher向量编码高维局部特征，”
    见 *NeurIPS*, 2014, 第1143–1151页。'
- en: '[84] Z. Bolei, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Object
    detectors emerge in deep scene CNNs,” *ICLR*, 2015.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Z. Bolei, A. Khosla, A. Lapedriza, A. Oliva 和 A. Torralba, “对象检测器在深度场景CNN中出现，”
    *ICLR*, 2015。'
- en: '[85] L. Liu, J. Chen, P. Fieguth, G. Zhao, R. Chellappa, and M. Pietikäinen,
    “From BoW to CNN: Two decades of texture representation for texture classification,”
    *IJCV*, vol. 127, no. 1, pp. 74–109, 2019.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] L. Liu, J. Chen, P. Fieguth, G. Zhao, R. Chellappa 和 M. Pietikäinen, “从BoW到CNN：纹理分类的二十年纹理表示，”
    *IJCV*, 第127卷，第1期, 第74–109页, 2019。'
- en: '[86] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance of
    initialization and momentum in deep learning,” in *ICML*, 2013, pp. 1139–1147.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] I. Sutskever, J. Martens, G. Dahl 和 G. Hinton, “深度学习中初始化和动量的重要性，” 见 *ICML*,
    2013, 第1139–1147页。'
- en: '[87] B. Liu, J. Liu, J. Wang, and H. Lu, “Learning a representative and discriminative
    part model with deep convolutional features for scene recognition,” in *ACCV*,
    2014, pp. 643–658.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] B. Liu, J. Liu, J. Wang 和 H. Lu, “使用深度卷积特征学习一个代表性和判别性的部分模型进行场景识别，” 见 *ACCV*,
    2014, 第643–658页。'
- en: '[88] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the
    devil in the details: Delving deep into convolutional nets,” *arXiv:1405.3531*,
    2014.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] K. Chatfield, K. Simonyan, A. Vedaldi 和 A. Zisserman, “细节中的魔鬼归来：深入探讨卷积网络，”
    *arXiv:1405.3531*, 2014。'
- en: '[89] X. Song, S. Jiang, L. Herranz, and C. Chen, “Learning effective RGB-D
    representations for scene recognition,” *IEEE TIP*, vol. 28, no. 2, pp. 980–993,
    2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] X. Song, S. Jiang, L. Herranz 和 C. Chen, “学习有效的RGB-D表示用于场景识别，” *IEEE TIP*,
    第28卷，第2期, 第980–993页, 2018。'
- en: '[90] M. Lin, Q. Chen, and S. Yan, “Network in network,” *ICLR*, 2014.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. Lin, Q. Chen 和 S. Yan, “网络中的网络，” *ICLR*, 2014。'
- en: '[91] J. Sun and J. Ponce, “Learning discriminative part detectors for image
    classification and cosegmentation,” in *ICCV*, 2013, pp. 3400–3407.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] J. Sun 和 J. Ponce, “用于图像分类和共同分割的判别部分检测器学习，” 见 *ICCV*, 2013, 第3400–3407页。'
- en: '[92] K. J. Shih, I. Endres, and D. Hoiem, “Learning discriminative collections
    of part detectors for object recognition,” *IEEE TPAMI*, vol. 37, no. 8, pp. 1571–1584,
    2014.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] K. J. Shih, I. Endres 和 D. Hoiem, “学习用于对象识别的判别性部分检测器集合，” *IEEE TPAMI*,
    第37卷，第8期, 第1571–1584页, 2014。'
- en: '[93] Y. Sun, X. Wang, and X. Tang, “Deeply learned face representations are
    sparse, selective, and robust,” in *CVPR*, 2015, pp. 2892–2900.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Y. Sun, X. Wang 和 X. Tang, “深度学习的面部表示是稀疏的、选择性的和鲁棒的，” 见 *CVPR*, 2015, 第2892–2900页。'
- en: '[94] D. L. Donoho, A. Maleki, and A. Montanari, “Message-passing algorithms
    for compressed sensing,” *PNAS*, vol. 106, no. 45, pp. 18 914–18 919, 2009.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] D. L. Donoho, A. Maleki 和 A. Montanari, “压缩感知的消息传递算法，” *PNAS*, 第106卷，第45期,
    第18 914–18 919页, 2009。'
- en: '[95] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] S. Hochreiter 和 J. Schmidhuber，“长短期记忆，” *Neural Computation*，第9卷，第8期，页码
    1735–1780，1997年。'
- en: '[96] Y. Li, Z. Zhang, Y. Cheng, L. Wang, and T. Tan, “MAPNet: Multi-modal attentive
    pooling network for RGB-D indoor scene classification,” *Pattern Recognition*,
    vol. 90, pp. 436–449, 2019.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Y. Li, Z. Zhang, Y. Cheng, L. Wang, 和 T. Tan，“MAPNet：用于 RGB-D 室内场景分类的多模态注意力池网络，”
    *Pattern Recognition*，第90卷，页码 436–449，2019年。'
- en: '[97] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
    M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu *et al.*, “Learning to navigate
    in complex environments,” *ICLR*, 2016.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
    M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu *等*，“学习在复杂环境中导航，” *ICLR*，2016年。'
- en: '[98] T. J. Sejnowski, “The unreasonable effectiveness of deep learning in artificial
    intelligence,” *PNAS*, 2020.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] T. J. Sejnowski，“深度学习在人工智能中的非凡效果，” *PNAS*，2020年。'
- en: '[99] L. Xie, L. Zheng, J. Wang, A. L. Yuille, and Q. Tian, “Interactive: Inter-layer
    activeness propagation,” in *CVPR*, 2016, pp. 270–279.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] L. Xie, L. Zheng, J. Wang, A. L. Yuille, 和 Q. Tian，“互动：层间活跃度传播，”在 *CVPR*，2016年，页码
    270–279。'
- en: '[100] L. Xie, J. Wang, W. Lin, B. Zhang, and Q. Tian, “Towards reversal-invariant
    image representation,” *IJCV*, vol. 123, no. 2, pp. 226–250, 2017.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] L. Xie, J. Wang, W. Lin, B. Zhang, 和 Q. Tian，“朝着反转不变图像表示的方向，” *IJCV*，第123卷，第2期，页码
    226–250，2017年。'
- en: '[101] M. Rezanejad, G. Downs, J. Wilder, D. B. Walther, A. Jepson, S. Dickinson,
    and K. Siddiqi, “Scene categorization from contours: Medial axis based salience
    measures,” in *CVPR*, 2019, pp. 4116–4124.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] M. Rezanejad, G. Downs, J. Wilder, D. B. Walther, A. Jepson, S. Dickinson,
    和 K. Siddiqi，“从轮廓进行场景分类：基于中轴的显著性度量，”在 *CVPR*，2019年，页码 4116–4124。'
- en: '[102] B.-B. Gao, X.-S. Wei, J. Wu, and W. Lin, “Deep spatial pyramid: The devil
    is once again in the details,” *arXiv:1504.05277*, 2015.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] B.-B. Gao, X.-S. Wei, J. Wu, 和 W. Lin，“深度空间金字塔：魔鬼再次在细节中，” *arXiv:1504.05277*，2015年。'
- en: '[103] Z. Xiong, Y. Yuan, and Q. Wang, “RGB-D scene recognition via spatial-related
    multi-modal feature learning,” *IEEE Access*, vol. 7, pp. 106 739–106 747, 2019.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Z. Xiong, Y. Yuan, 和 Q. Wang，“通过空间相关的多模态特征学习进行 RGB-D 场景识别，” *IEEE Access*，第7卷，页码
    106 739–106 747，2019年。'
- en: '[104] J. Sánchez, F. Perronnin, T. Mensink, and J. Verbeek, “Image classification
    with the Fisher vector: Theory and practice,” *IJCV*, vol. 105, no. 3, pp. 222–245,
    2013.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. Sánchez, F. Perronnin, T. Mensink, 和 J. Verbeek，“使用费舍尔向量进行图像分类：理论与实践，”
    *IJCV*，第105卷，第3期，页码 222–245，2013年。'
- en: '[105] H. Jégou, M. Douze, C. Schmid, and P. Pérez, “Aggregating local descriptors
    into a compact image representation,” in *CVPR*, 2010, pp. 3304–3311.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] H. Jégou, M. Douze, C. Schmid, 和 P. Pérez，“将局部描述符聚合成紧凑的图像表示，”在 *CVPR*，2010年，页码
    3304–3311。'
- en: '[106] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray, “Visual categorization
    with bags of keypoints,” in *ECCVW*, 2004, pp. 1–2.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] G. Csurka, C. Dance, L. Fan, J. Willamowski, 和 C. Bray，“使用关键点包进行视觉分类，”在
    *ECCVW*，2004年，页码 1–2。'
- en: '[107] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen,
    “Deep learning for generic object detection: A survey,” *IJCV*, vol. 128, no. 2,
    pp. 261–318, 2020.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, 和 M. Pietikäinen，“通用目标检测的深度学习：一项调查，”
    *IJCV*，第128卷，第2期，页码 261–318，2020年。'
- en: '[108] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders, “Selective
    search for object recognition,” *IJCV*, vol. 104, no. 2, pp. 154–171, 2013.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J. R. Uijlings, K. E. Van De Sande, T. Gevers, 和 A. W. Smeulders，“用于对象识别的选择性搜索，”
    *IJCV*，第104卷，第2期，页码 154–171，2013年。'
- en: '[109] S. Singh, A. Gupta, and A. A. Efros, “Unsupervised discovery of mid-level
    discriminative patches,” in *ECCV*, 2012, pp. 73–86.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] S. Singh, A. Gupta, 和 A. A. Efros，“无监督发现中级区分补丁，”在 *ECCV*，2012年，页码 73–86。'
- en: '[110] P. Arbeláez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik, “Multiscale
    combinatorial grouping,” in *CVPR*, 2014, pp. 328–335.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] P. Arbeláez, J. Pont-Tuset, J. T. Barron, F. Marques, 和 J. Malik，“多尺度组合分组，”在
    *CVPR*，2014年，页码 328–335。'
- en: '[111] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “SSD: Single shot multibox detector,” in *ECCV*, 2016, pp. 21–37.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, 和 A. C.
    Berg，“SSD：单次多框检测器，”在 *ECCV*，2016年，页码 21–37。'
- en: '[112] J. Redmon and A. Farhadi, “Yolo9000: Better, faster, stronger,” in *CVPR*,
    2017, pp. 7263–7271.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Redmon 和 A. Farhadi，“Yolo9000：更好、更快、更强，”在 *CVPR*，2017年，页码 7263–7271。'
- en: '[113] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in *CVPR*, 2016, pp. 779–788.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] J. Redmon, S. Divvala, R. Girshick, 和 A. Farhadi，“你只看一次：统一的实时目标检测，”在
    *CVPR*，2016年，页码 779–788。'
- en: '[114] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply-supervised
    nets,” in *AISTATS*, 2015, pp. 562–570.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang 和 Z. Tu，“深度监督网络”，在 *AISTATS*，2015
    年，第 562–570 页。'
- en: '[115] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *CVPR*, 2017, pp. 2117–2125.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan 和 S. Belongie，“用于目标检测的特征金字塔网络”，在
    *CVPR*，2017 年，第 2117–2125 页。'
- en: '[116] C. G. Snoek, M. Worring, and A. W. Smeulders, “Early versus late fusion
    in semantic video analysis,” in *Proceedings of the 13th annual ACM international
    conference on Multimedia*, 2005, pp. 399–402.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] C. G. Snoek, M. Worring 和 A. W. Smeulders，“语义视频分析中的早期与晚期融合”，在 *第 13 届
    ACM 国际多媒体会议论文集*，2005 年，第 399–402 页。'
- en: '[117] H. Gunes and M. Piccardi, “Affect recognition from face and body: Early
    fusion vs. late fusion,” in *International conference on systems, man and cybernetics*,
    vol. 4, 2005, pp. 3437–3443.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] H. Gunes 和 M. Piccardi，“面部和身体的情感识别：早期融合与晚期融合”，在 *国际系统、人类与控制会议*，第 4 卷，2005
    年，第 3437–3443 页。'
- en: '[118] Y. Dong, S. Gao, K. Tao, J. Liu, and H. Wang, “Performance evaluation
    of early and late fusion methods for generic semantics indexing,” *Pattern Analysis
    and Applications*, vol. 17, no. 1, pp. 37–50, 2014.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Y. Dong, S. Gao, K. Tao, J. Liu 和 H. Wang，“早期和晚期融合方法在通用语义索引中的性能评估”，*模式分析与应用*，第
    17 卷，第 1 期，第 37–50 页，2014 年。'
- en: '[119] J. Li, D. Lin, Y. Wang, G. Xu, Y. Zhang, C. Ding, and Y. Zhou, “Deep
    discriminative representation learning with attention map for scene classification,”
    *Remote Sensing*, vol. 12, no. 9, p. 1366, 2020.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] J. Li, D. Lin, Y. Wang, G. Xu, Y. Zhang, C. Ding 和 Y. Zhou，“用于场景分类的带有注意力图的深度判别表示学习”，*遥感*，第
    12 卷，第 9 期，第 1366 页，2020 年。'
- en: '[120] F. Zhang, B. Du, and L. Zhang, “Scene classification via a gradient boosting
    random convolutional network framework,” *IEEE TGRS*, vol. 54, no. 3, pp. 1793–1802,
    2015.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] F. Zhang, B. Du 和 L. Zhang，“通过梯度提升随机卷积网络框架进行场景分类”，*IEEE TGRS*，第 54 卷，第
    3 期，第 1793–1802 页，2015 年。'
- en: '[121] L. Wang, Z. Wang, W. Du, and Y. Qiao, “Object-scene convolutional neural
    networks for event recognition in images,” in *CVPRW*, 2015, pp. 30–35.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] L. Wang, Z. Wang, W. Du 和 Y. Qiao，“用于图像事件识别的对象-场景卷积神经网络”，在 *CVPRW*，2015
    年，第 30–35 页。'
- en: '[122] S. Xia, J. Zeng, L. Leng, and X. Fu, “WS-AM: Weakly supervised attention
    map for scene recognition,” *Electronics*, vol. 8, no. 10, p. 1072, 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] S. Xia, J. Zeng, L. Leng 和 X. Fu，“WS-AM：用于场景识别的弱监督注意力图”，*电子学*，第 8 卷，第
    10 期，第 1072 页，2019 年。'
- en: '[123] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” *ICML*, 2015.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] S. Ioffe 和 C. Szegedy，“批量归一化：通过减少内部协变量偏移加速深度网络训练”，*ICML*，2015 年。'
- en: '[124] H. Jin Kim and J.-M. Frahm, “Hierarchy of alternating specialists for
    scene recognition,” in *ECCV*, 2018, pp. 451–467.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] H. Jin Kim 和 J.-M. Frahm，“用于场景识别的交替专家层次结构”，在 *ECCV*，2018 年，第 451–467
    页。'
- en: '[125] F. Perronnin and C. Dance, “Fisher kernels on visual vocabularies for
    image categorization,” in *CVPR*, 2007, pp. 1–8.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] F. Perronnin 和 C. Dance，“用于图像分类的视觉词汇上的 Fisher 核”，在 *CVPR*，2007 年，第 1–8
    页。'
- en: '[126] R. Kwitt, N. Vasconcelos, and N. Rasiwasia, “Scene recognition on the
    semantic manifold,” in *ECCV*, 2012, pp. 359–372.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] R. Kwitt, N. Vasconcelos 和 N. Rasiwasia，“语义流形上的场景识别”，在 *ECCV*，2012 年，第
    359–372 页。'
- en: '[127] Z. Ghahramani, G. E. Hinton *et al.*, “The em algorithm for mixtures
    of factor analyzers,” University of Toronto, Tech. Rep., 1996.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Z. Ghahramani, G. E. Hinton *等*，“用于因子分析混合模型的 EM 算法”，多伦多大学，技术报告，1996 年。'
- en: '[128] J. Verbeek, “Learning nonlinear image manifolds by global alignment of
    local linear models,” *IEEE TPAMI*, vol. 28, no. 8, pp. 1236–1250, 2006.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] J. Verbeek，“通过全局对齐局部线性模型学习非线性图像流形”，*IEEE TPAMI*，第 28 卷，第 8 期，第 1236–1250
    页，2006 年。'
- en: '[129] P. Wei, F. Qin, F. Wan, Y. Zhu, J. Jiao, and Q. Ye, “Correlated topic
    vector for scene classification,” *IEEE TIP*, vol. 26, no. 7, pp. 3221–3234, 2017.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] P. Wei, F. Qin, F. Wan, Y. Zhu, J. Jiao 和 Q. Ye，“用于场景分类的相关主题向量”，*IEEE
    TIP*，第 26 卷，第 7 期，第 3221–3234 页，2017 年。'
- en: '[130] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-CAM: Visual explanations from deep networks via gradient-based localization,”
    in *ICCV*, 2017, pp. 618–626.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh 和 D. Batra，“Grad-CAM：通过基于梯度的定位进行深度网络的可视化解释”，在
    *ICCV*，2017 年，第 618–626 页。'
- en: '[131] J. Zhang, S. A. Bargal, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff,
    “Top-down neural attention by excitation backprop,” *IJCV*, vol. 126, no. 10,
    pp. 1084–1102, 2018.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] J. Zhang, S. A. Bargal, Z. Lin, J. Brandt, X. Shen 和 S. Sclaroff，“通过激发反向传播的自上而下神经注意力”，*IJCV*，第
    126 卷，第 10 期，第 1084–1102 页，2018 年。'
- en: '[132] H. Seong, J. Hyun, and E. Kim, “Fosnet: An end-to-end trainable deep
    neural network for scene recognition,” *IEEE Access*, vol. 8, pp. 82 066–82 077,
    2020.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] H. Seong, J. Hyun 和 E. Kim, “Fosnet: 一种用于场景识别的端到端可训练深度神经网络，” *IEEE Access*,
    第 8 卷, 第 82 066–82 077 页, 2020。'
- en: '[133] T. Joseph, K. G. Derpanis, and F. Z. Qureshi, “Joint spatial and layer
    attention for convolutional networks,” *arXiv:1901.05376*, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] T. Joseph, K. G. Derpanis 和 F. Z. Qureshi, “卷积网络的联合空间和层注意力，” *arXiv:1901.05376*,
    2019。'
- en: '[134] Z. Niu, G. Hua, X. Gao, and Q. Tian, “Context aware topic model for scene
    recognition,” in *CVPR*, 2012, pp. 2743–2750.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Z. Niu, G. Hua, X. Gao 和 Q. Tian, “用于场景识别的上下文感知主题模型，” 在 *CVPR*, 2012,
    第 2743–2750 页。'
- en: '[135] Z. Zuo, B. Shuai, G. Wang, X. Liu, X. Wang, B. Wang, and Y. Chen, “Convolutional
    recurrent neural networks: Learning spatial dependencies for image representation,”
    in *CVPRW*, 2015, pp. 18–26.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Z. Zuo, B. Shuai, G. Wang, X. Liu, X. Wang, B. Wang 和 Y. Chen, “卷积递归神经网络：用于图像表示的空间依赖学习，”
    在 *CVPRW*, 2015, 第 18–26 页。'
- en: '[136] X. Wang and E. Grimson, “Spatial latent dirichlet allocation,” in *NeurIPS*,
    2008, pp. 1577–1584.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] X. Wang 和 E. Grimson, “空间潜在狄利克雷分配，” 在 *NeurIPS*, 2008, 第 1577–1584 页。'
- en: '[137] J. L. Elman, “Finding structure in time,” *Cognitive science*, vol. 14,
    no. 2, pp. 179–211, 1990.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] J. L. Elman, “在时间中发现结构，” *Cognitive science*, 第 14 卷, 第 2 期, 第 179–211
    页, 1990。'
- en: '[138] G. R. Cross and A. K. Jain, “Markov random field texture models,” *IEEE
    TPAMI*, no. 1, pp. 25–39, 1983.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] G. R. Cross 和 A. K. Jain, “马尔可夫随机场纹理模型，” *IEEE TPAMI*, 第 1 期, 第 25–39
    页, 1983。'
- en: '[139] S. Z. Li, *Markov random field modeling in image analysis*.   Springer,
    2009.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] S. Z. Li, *图像分析中的马尔可夫随机场建模*。Springer, 2009。'
- en: '[140] N. Rasiwasia and N. Vasconcelos, “Holistic context models for visual
    recognition,” *IEEE TPAMI*, vol. 34, no. 5, pp. 902–917, 2012.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] N. Rasiwasia 和 N. Vasconcelos, “用于视觉识别的整体上下文模型，” *IEEE TPAMI*, 第 34 卷,
    第 5 期, 第 902–917 页, 2012。'
- en: '[141] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and
    locally connected networks on graphs,” *arXiv:1312.6203*, 2013.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] J. Bruna, W. Zaremba, A. Szlam 和 Y. LeCun, “图上的谱网络和局部连接网络，” *arXiv:1312.6203*,
    2013。'
- en: '[142] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” *ICLR*, 2016.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] T. N. Kipf 和 M. Welling, “使用图卷积网络进行半监督分类，” *ICLR*, 2016。'
- en: '[143] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun,
    “Graph neural networks: A review of methods and applications,” *arXiv:1812.08434*,
    2018.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li 和 M. Sun,
    “图神经网络：方法与应用综述，” *arXiv:1812.08434*, 2018。'
- en: '[144] Y. Yuan, Z. Xiong, and Q. Wang, “Acm: Adaptive cross-modal graph convolutional
    neural networks for RGB-D scene recognition,” in *AAAI*, vol. 33, 2019, pp. 9176–9184.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. Yuan, Z. Xiong 和 Q. Wang, “ACM：用于 RGB-D 场景识别的自适应跨模态图卷积神经网络，” 在 *AAAI*,
    第 33 卷, 2019, 第 9176–9184 页。'
- en: '[145] X. Song, S. Jiang, B. Wang, C. Chen, and G. Chen, “Image representations
    with spatial object-to-object relations for RGB-D scene recognition,” *IEEE TIP*,
    vol. 29, pp. 525–537, 2019.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] X. Song, S. Jiang, B. Wang, C. Chen 和 G. Chen, “具有空间对象间关系的图像表示用于 RGB-D
    场景识别，” *IEEE TIP*, 第 29 卷, 第 525–537 页, 2019。'
- en: '[146] S. A. Javed and A. K. Nelakanti, “Object-level context modeling for scene
    classification with context-CNN,” *arXiv:1705.04358*, 2017.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] S. A. Javed 和 A. K. Nelakanti, “用于场景分类的对象级上下文建模与 context-CNN,” *arXiv:1705.04358*,
    2017。'
- en: '[147] X. Song, S. Jiang, and L. Herranz, “Multi-scale multi-feature context
    modeling for scene recognition in the semantic manifold,” *IEEE TIP*, vol. 26,
    no. 6, pp. 2721–2735, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] X. Song, S. Jiang 和 L. Herranz, “在语义流形中用于场景识别的多尺度多特征上下文建模，” *IEEE TIP*,
    第 26 卷, 第 6 期, 第 2721–2735 页, 2017。'
- en: '[148] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” *ICLR*, 2017.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] T. N. Kipf 和 M. Welling, “使用图卷积网络进行半监督分类，” *ICLR*, 2017。'
- en: '[149] Z. Zuo, G. Wang, B. Shuai, L. Zhao, Q. Yang, and X. Jiang, “Learning
    discriminative and shareable features for scene classification,” in *ECCV*, 2014,
    pp. 552–568.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Z. Zuo, G. Wang, B. Shuai, L. Zhao, Q. Yang 和 X. Jiang, “用于场景分类的判别性和可共享特征学习，”
    在 *ECCV*, 2014, 第 552–568 页。'
- en: '[150] S. Jiang, G. Chen, X. Song, and L. Liu, “Deep patch representations with
    shared codebook for scene classification,” *ACM TOMM*, vol. 15, no. 1s, pp. 1–17,
    2019.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. Jiang, G. Chen, X. Song 和 L. Liu, “具有共享词汇表的深度补丁表示用于场景分类，” *ACM TOMM*,
    第 15 卷, 第 1s 期, 第 1–17 页, 2019。'
- en: '[151] B. E. Boser, I. M. Guyon, and V. N. Vapnik, “A training algorithm for
    optimal margin classifiers,” in *Annual workshop on Computational learning theory*,
    1992, pp. 144–152.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] B. E. Boser, I. M. Guyon 和 V. N. Vapnik, “用于最优边际分类器的训练算法，” 在 *年度计算学习理论研讨会*,
    1992, 第 144–152 页。'
- en: '[152] Y. Li, J. Zhang, Y. Cheng, K. Huang, and T. Tan, “Df2net: Discriminative
    feature learning and fusion network for RGB-D indoor scene classification,” in
    *AAAI*, 2018.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Y. Li, J. Zhang, Y. Cheng, K. Huang, 和 T. Tan, “Df2net: RGB-D室内场景分类的判别特征学习与融合网络，”发表于*AAAI*，2018年。'
- en: '[153] J. C. Van Gemert, C. J. Veenman, A. W. Smeulders, and J.-M. Geusebroek,
    “Visual word ambiguity,” *IEEE TPAMI*, vol. 32, no. 7, pp. 1271–1283, 2009.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] J. C. Van Gemert, C. J. Veenman, A. W. Smeulders, 和 J.-M. Geusebroek,
    “视觉词模糊性，”*IEEE TPAMI*，第32卷，第7期，页码1271–1283，2009年。'
- en: '[154] B. Schölkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson,
    “Estimating the support of a high-dimensional distribution,” *Neural computation*,
    vol. 13, no. 7, pp. 1443–1471, 2001.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] B. Schölkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, 和 R. C. Williamson,
    “高维分布的支持度估计，”*Neural computation*，第13卷，第7期，页码1443–1471，2001年。'
- en: '[155] T. Durand, N. Thome, and M. Cord, “Mantra: Minimum maximum latent structural
    svm for image classification and ranking,” in *ICCV*, 2015, pp. 2713–2721.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] T. Durand, N. Thome, 和 M. Cord, “Mantra: 最小最大潜在结构SVM用于图像分类和排序，”发表于*ICCV*，2015年，页码2713–2721。'
- en: '[156] R. Socher, B. Huval, B. Bath, C. D. Manning, and A. Y. Ng, “Convolutional-recursive
    deep learning for 3d object classification,” in *NeurIPS*, 2012, pp. 656–664.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] R. Socher, B. Huval, B. Bath, C. D. Manning, 和 A. Y. Ng, “用于3D物体分类的卷积递归深度学习，”发表于*NeurIPS*，2012年，页码656–664。'
- en: '[157] A. Wang, J. Cai, J. Lu, and T.-J. Cham, “MMSS: Multi-modal sharable and
    specific feature learning for RGB-D object recognition,” in *ICCV*, 2015, pp.
    1125–1133.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] A. Wang, J. Cai, J. Lu, 和 T.-J. Cham, “MMSS: 多模态可共享与特定特征学习用于RGB-D物体识别，”发表于*ICCV*，2015年，页码1125–1133。'
- en: '[158] Y. Cheng, X. Zhao, R. Cai, Z. Li, K. Huang, Y. Rui *et al.*, “Semi-supervised
    multimodal deep learning for RGB-D object recognition,” *IJCAI*, pp. 3346–3351,
    2016.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Y. Cheng, X. Zhao, R. Cai, Z. Li, K. Huang, Y. Rui *等*，“用于RGB-D物体识别的半监督多模态深度学习，”*IJCAI*，页码3346–3351，2016年。'
- en: '[159] Q. Wang, M. Chen, F. Nie, and X. Li, “Detecting coherent groups in crowd
    scenes by multiview clustering,” *IEEE TPAMI*, vol. 42, no. 1, pp. 46–58, 2018.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Q. Wang, M. Chen, F. Nie, 和 X. Li, “通过多视角聚类检测人群场景中的一致组，”*IEEE TPAMI*，第42卷，第1期，页码46–58，2018年。'
- en: '[160] Y. Zheng and X. Gao, “Indoor scene recognition via multi-task metric
    multi-kernel learning from RGB-D images,” *Multimedia Tools and Applications*,
    vol. 76, no. 3, pp. 4427–4443, 2017.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Y. Zheng 和 X. Gao, “通过多任务度量多核学习从RGB-D图像中进行室内场景识别，”*Multimedia Tools and
    Applications*，第76卷，第3期，页码4427–4443，2017年。'
- en: '[161] B. Thompson, “Canonical correlation analysis,” *Encyclopedia of statistics
    in behavioral science*, 2005.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] B. Thompson, “典型相关分析，”*行为科学统计百科全书*，2005年。'
- en: '[162] G. Andrew, R. Arora, J. Bilmes, and K. Livescu, “Deep canonical correlation
    analysis,” in *ICML*, 2013, pp. 1247–1255.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] G. Andrew, R. Arora, J. Bilmes, 和 K. Livescu, “深度典型相关分析，”发表于*ICML*，2013年，页码1247–1255。'
- en: '[163] J. Wu, B.-B. Gao, and G. Liu, “Representing sets of instances for visual
    recognition,” in *AAAI*, 2016, pp. 2237–2243.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] J. Wu, B.-B. Gao, 和 G. Liu, “用于视觉识别的实例集合表示，”发表于*AAAI*，2016年，页码2237–2243。'
- en: '[164] J. H. Bappy, S. Paul, and A. K. Roy-Chowdhury, “Online adaptation for
    joint scene and object classification,” in *ECCV*, 2016, pp. 227–243.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] J. H. Bappy, S. Paul, 和 A. K. Roy-Chowdhury, “联合场景与物体分类的在线适应，”发表于*ECCV*，2016年，页码227–243。'
- en: '[165] Z. Zhao and M. Larson, “From volcano to toyshop: Adaptive discriminative
    region discovery for scene recognition,” in *ACM MM*, 2018, pp. 1760–1768.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Z. Zhao 和 M. Larson, “从火山到玩具店：用于场景识别的自适应判别区域发现，”发表于*ACM MM*，2018年，页码1760–1768。'
- en: '[166] M. Koskela and J. Laaksonen, “Convolutional network features for scene
    recognition,” in *ACM MM*, 2014, pp. 1169–1172.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] M. Koskela 和 J. Laaksonen, “用于场景识别的卷积网络特征，”发表于*ACM MM*，2014年，页码1169–1172。'
- en: '[167] G. Nascimento, C. Laranjeira, V. Braz, A. Lacerda, and E. R. Nascimento,
    “A robust indoor scene recognition method based on sparse representation,” in
    *CIARP*, 2017, pp. 408–415.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] G. Nascimento, C. Laranjeira, V. Braz, A. Lacerda, 和 E. R. Nascimento,
    “基于稀疏表示的鲁棒室内场景识别方法，”发表于*CIARP*，2017年，页码408–415。'
- en: '[168] Y. Liao, S. Kodagoda, Y. Wang, L. Shi, and Y. Liu, “Understand scene
    categories by objects: A semantic regularized scene classifier using convolutional
    neural networks,” in *CRA*, 2016, pp. 2318–2325.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Liao, S. Kodagoda, Y. Wang, L. Shi, 和 Y. Liu, “通过对象理解场景类别：一种使用卷积神经网络的语义正则化场景分类器，”发表于*CRA*，2016年，页码2318–2325。'
- en: '[169] Z. Cai and L. Shao, “RGB-D scene classification via multi-modal feature
    learning,” *Cognitive Computation*, vol. 11, no. 6, pp. 825–840, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Z. Cai 和 L. Shao, “通过多模态特征学习进行RGB-D场景分类，”*Cognitive Computation*，第11卷，第6期，页码825–840，2019年。'
- en: '[170] A. Ayub and A. Wagner, “Cbcl: Brain-inspired model for RGB-D indoor scene
    classification,” *arXiv:1911.00155*, 2019.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] A. Ayub 和 A. Wagner, “Cbcl: 基于大脑的RGB-D室内场景分类模型，”*arXiv:1911.00155*，2019年。'
- en: '[171] X. Song, L. Herranz, and S. Jiang, “Depth CNNs for RGB-D scene recognition:
    Learning from scratch better than transferring from RGB-CNNs,” in *AAAI*, vol. 31,
    no. 1, 2017.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] X. Song、L. Herranz 和 S. Jiang，“RGB-D场景识别中的深度CNN：从头学习比从RGB-CNN迁移更好”，在
    *AAAI*，第31卷，第1期，2017年。'
- en: '[172] M. M. Najafabadi, F. Villanustre, T. M. Khoshgoftaar, N. Seliya *et al.*,
    “Deep learning applications and challenges in big data analytics,” *Journal of
    Big Data*, vol. 2, no. 1, p. 1, 2015.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] M. M. Najafabadi、F. Villanustre、T. M. Khoshgoftaar、N. Seliya *等*，“深度学习应用与大数据分析中的挑战”，*Journal
    of Big Data*，第2卷，第1期，第1页，2015年。'
- en: '[173] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement
    learning,” *ICLR*, 2017.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] B. Zoph 和 Q. V. Le，“使用强化学习进行神经架构搜索”，*ICLR*，2017年。'
- en: '[174] T. Elsken, J. H. Metzen, F. Hutter *et al.*, “Neural architecture search:
    A survey.” *JMLR*, vol. 20, no. 55, pp. 1–21, 2019.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] T. Elsken、J. H. Metzen、F. Hutter *等*，“神经架构搜索：综述”，*JMLR*，第20卷，第55期，页1–21，2019年。'
- en: '[175] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, and A. Oliva, “SUN database:
    Exploring a large collection of scene categories,” *IJCV*, vol. 119, no. 1, pp.
    3–22, 2016.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] J. Xiao、K. A. Ehinger、J. Hays、A. Torralba 和 A. Oliva，“SUN数据库：探索大量场景类别”，*IJCV*，第119卷，第1期，页3–22，2016年。'
- en: '[176] O. Chapelle, B. Scholkopf, and A. Zien, “Semi-supervised learning,” *IEEE
    TNN*, vol. 20, no. 3, pp. 542–542, 2009.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] O. Chapelle、B. Scholkopf 和 A. Zien，“半监督学习”，*IEEE TNN*，第20卷，第3期，页542–542，2009年。'
- en: '[177] H. B. Barlow, “Unsupervised learning,” *Neural computation*, vol. 1,
    no. 3, pp. 295–311, 1989.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] H. B. Barlow，“无监督学习”，*Neural computation*，第1卷，第3期，页295–311，1989年。'
- en: '[178] A. Kolesnikov, X. Zhai, and L. Beyer, “Revisiting self-supervised visual
    representation learning,” in *CVPR*, 2019, pp. 1920–1929.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] A. Kolesnikov、X. Zhai 和 L. Beyer，“重新审视自监督视觉表征学习”，在 *CVPR*，2019年，页1920–1929。'
- en: '[179] Y.-X. Wang and M. Hebert, “Learning to learn: Model regression networks
    for easy small sample learning,” in *ECCV*, 2016, pp. 616–634.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Y.-X. Wang 和 M. Hebert，“学习学习：用于简易小样本学习的模型回归网络”，在 *ECCV*，2016年，页616–634。'
- en: '[180] L. Fei-Fei, R. Fergus, and P. Perona, “One-shot learning of object categories,”
    *IEEE TPAMI*, vol. 28, no. 4, pp. 594–611, 2006.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] L. Fei-Fei、R. Fergus 和 P. Perona，“单次学习物体类别”，*IEEE TPAMI*，第28卷，第4期，页594–611，2006年。'
- en: '[181] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, “Human-level concept
    learning through probabilistic program induction,” *Science*, vol. 350, no. 6266,
    pp. 1332–1338, 2015.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] B. M. Lake、R. Salakhutdinov 和 J. B. Tenenbaum，“通过概率程序归纳进行人类级别的概念学习”，*Science*，第350卷，第6266期，页1332–1338，2015年。'
- en: '[182] M. Long, Y. Cao, J. Wang, and M. Jordan, “Learning transferable features
    with deep adaptation networks,” in *ICML*, 2015, pp. 97–105.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] M. Long、Y. Cao、J. Wang 和 M. Jordan，“使用深度自适应网络学习可迁移特征”，在 *ICML*，2015年，页97–105。'
- en: '[183] M. Wang and W. Deng, “Deep visual domain adaptation: A survey,” *Neurocomputing*,
    vol. 312, pp. 135–153, 2018.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] M. Wang 和 W. Deng，“深度视觉领域自适应：综述”，*Neurocomputing*，第312卷，页135–153，2018年。'
- en: '[184] K.-C. Peng, Z. Wu, and J. Ernst, “Zero-shot deep domain adaptation,”
    in *ECCV*, 2018, pp. 764–781.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] K.-C. Peng、Z. Wu 和 J. Ernst，“零样本深度领域自适应”，在 *ECCV*，2018年，页764–781。'
- en: '[185] H. Li, S. Jialin Pan, S. Wang, and A. C. Kot, “Domain generalization
    with adversarial feature learning,” in *CVPR*, 2018, pp. 5400–5409.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] H. Li、S. Jialin Pan、S. Wang 和 A. C. Kot，“通过对抗特征学习进行领域泛化”，在 *CVPR*，2018年，页5400–5409。'
- en: '[186] X.-Y. Zhang, C.-L. Liu, and C. Y. Suen, “Towards robust pattern recognition:
    A review,” *Proceedings of the IEEE*, vol. 108, no. 6, pp. 894–922, 2020.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] X.-Y. Zhang、C.-L. Liu 和 C. Y. Suen，“迈向稳健的模式识别：综述”，*Proceedings of the
    IEEE*，第108卷，第6期，页894–922，2020年。'
- en: '[187] U. Shaham, Y. Yamada, and S. Negahban, “Understanding adversarial training:
    Increasing local stability of supervised models through robust optimization,”
    *Neurocomputing*, vol. 307, pp. 195–204, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] U. Shaham、Y. Yamada 和 S. Negahban，“理解对抗训练：通过鲁棒优化提高监督模型的局部稳定性”，*Neurocomputing*，第307卷，页195–204，2018年。'
- en: '[188] C. Qin, J. Martens, S. Gowal, D. Krishnan, K. Dvijotham, A. Fawzi, S. De,
    R. Stanforth, and P. Kohli, “Adversarial robustness through local linearization,”
    in *NeurIPS*, 2019, pp. 13 847–13 856.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] C. Qin、J. Martens、S. Gowal、D. Krishnan、K. Dvijotham、A. Fawzi、S. De、R.
    Stanforth 和 P. Kohli，“通过局部线性化实现对抗鲁棒性”，在 *NeurIPS*，2019年，页13 847–13 856。'
- en: '[189] A. Shafahi, M. Najibi, M. A. Ghiasi, Z. Xu, J. Dickerson, C. Studer,
    L. S. Davis, G. Taylor, and T. Goldstein, “Adversarial training for free!” in
    *NeurIPS*, 2019, pp. 3358–3369.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] A. Shafahi、M. Najibi、M. A. Ghiasi、Z. Xu、J. Dickerson、C. Studer、L. S.
    Davis、G. Taylor 和 T. Goldstein，“免费的对抗训练！”，在 *NeurIPS*，2019年，页3358–3369。'
- en: '[190] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, and V. Lempitsky, “Domain-adversarial training of neural networks,”
    *JMLR*, vol. 17, no. 1, pp. 2096–2030, 2016.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, 和 V. Lempitsky, “神经网络的领域对抗训练”，*JMLR*，第 17 卷，第 1 期，第 2096–2030 页，2016
    年。'
- en: '[191] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing robust
    adversarial examples,” in *ICML*, 2018, pp. 284–293.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] A. Athalye, L. Engstrom, A. Ilyas, 和 K. Kwok, “合成鲁棒的对抗样本”，在 *ICML*，2018
    年，第 284–293 页。'
- en: '[192] A. R. Dargazany, P. Stegagno, and K. Mankodiya, “WearableDL: Wearable
    internet-of-things and deep learning for big data analytics—concept, literature,
    and future,” *Mobile Information Systems*, vol. 2018, 2018.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] A. R. Dargazany, P. Stegagno, 和 K. Mankodiya, “WearableDL：可穿戴物联网和深度学习用于大数据分析—概念、文献及未来”，*Mobile
    Information Systems*，第 2018 卷，2018 年。'
- en: '[193] L. Shen, Z. Lin, and Q. Huang, “Relay backpropagation for effective learning
    of deep convolutional neural networks,” in *ECCV*, 2016, pp. 467–482.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] L. Shen, Z. Lin, 和 Q. Huang, “有效学习深度卷积神经网络的中继反向传播”，在 *ECCV*，2016 年，第
    467–482 页。'
- en: '[194] F. Thabtah, S. Hammoud, F. Kamalov, and A. Gonsalves, “Data imbalance
    in classification: Experimental evaluation,” *Information Sciences*, vol. 513,
    pp. 429–441, 2020.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] F. Thabtah, S. Hammoud, F. Kamalov, 和 A. Gonsalves, “分类中的数据不平衡：实验评估”，*Information
    Sciences*，第 513 卷，第 429–441 页，2020 年。'
- en: '[195] M. Buda, A. Maki, and M. A. Mazurowski, “A systematic study of the class
    imbalance problem in convolutional neural networks,” *Neural Networks*, vol. 106,
    pp. 249–259, 2018.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] M. Buda, A. Maki, 和 M. A. Mazurowski, “卷积神经网络中的类别不平衡问题的系统研究”，*Neural
    Networks*，第 106 卷，第 249–259 页，2018 年。'
- en: '[196] J. M. Johnson and T. M. Khoshgoftaar, “Survey on deep learning with class
    imbalance,” *Journal of Big Data*, vol. 6, no. 1, p. 27, 2019.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] J. M. Johnson 和 T. M. Khoshgoftaar, “关于深度学习与类别不平衡的调查”，*Journal of Big
    Data*，第 6 卷，第 1 期，第 27 页，2019 年。'
- en: '[197] Y. Guo, Y. Liu, A. Oerlemans, S. Lao, S. Wu, and M. S. Lew, “Deep learning
    for visual understanding: A review,” *Neurocomputing*, vol. 187, pp. 27–48, 2016.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Y. Guo, Y. Liu, A. Oerlemans, S. Lao, S. Wu, 和 M. S. Lew, “用于视觉理解的深度学习：综述”，*Neurocomputing*，第
    187 卷，第 27–48 页，2016 年。'
- en: '[198] Z. Chen and B. Liu, “Lifelong machine learning,” *Synthesis Lectures
    on Artificial Intelligence and Machine Learning*, vol. 12, no. 3, pp. 1–207, 2018.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] Z. Chen 和 B. Liu, “终身机器学习”，*Synthesis Lectures on Artificial Intelligence
    and Machine Learning*，第 12 卷，第 3 期，第 1–207 页，2018 年。'
- en: '[199] S. Thrun and T. M. Mitchell, “Lifelong robot learning,” *Robotics and
    autonomous systems*, vol. 15, no. 1-2, pp. 25–46, 1995.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] S. Thrun 和 T. M. Mitchell, “终身机器人学习”，*Robotics and autonomous systems*，第
    15 卷，第 1-2 期，第 25–46 页，1995 年。'
- en: '[200] D. Hassabis, D. Kumaran, C. Summerfield, and M. Botvinick, “Neuroscience-inspired
    artificial intelligence,” *Neuron*, vol. 95, no. 2, pp. 245–258, 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] D. Hassabis, D. Kumaran, C. Summerfield, 和 M. Botvinick, “受神经科学启发的人工智能”，*Neuron*，第
    95 卷，第 2 期，第 245–258 页，2017 年。'
- en: '[201] M.-L. Zhang and Z.-H. Zhou, “Ml-knn: A lazy learning approach to multi-label
    learning,” *Pattern Recognition*, vol. 40, no. 7, pp. 2038–2048, 2007.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] M.-L. Zhang 和 Z.-H. Zhou, “Ml-knn：一种懒惰学习的多标签学习方法”，*Pattern Recognition*，第
    40 卷，第 7 期，第 2038–2048 页，2007 年。'
- en: '[202] C. Feichtenhofer, A. Pinz, and R. P. Wildes, “Temporal residual networks
    for dynamic scene recognition,” in *CVPR*, 2017, pp. 4728–4737.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] C. Feichtenhofer, A. Pinz, 和 R. P. Wildes, “动态场景识别的时间残差网络”，在 *CVPR*，2017
    年，第 4728–4737 页。'
- en: '[203] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *CVPR*, 2017,
    pp. 5828–5839.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, 和 M. Nießner,
    “Scannet：室内场景的丰富注释 3D 重建”，在 *CVPR*，2017 年，第 5828–5839 页。'
- en: '[204] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *ICCV*, 2015, pp.
    4489–4497.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] D. Tran, L. Bourdev, R. Fergus, L. Torresani, 和 M. Paluri, “使用 3D 卷积网络学习时空特征”，在
    *ICCV*，2015 年，第 4489–4497 页。'
- en: '[205] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3D classification and segmentation,” in *CVPR*, 2018, pp. 652–660.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas, “Pointnet：用于 3D 分类和分割的点集深度学习”，在
    *CVPR*，2018 年，第 652–660 页。'
- en: '[206] D. G. Lowe, “Object recognition from local scale-invariant features,”
    in *IJCV*, vol. 2, 1999, pp. 1150–1157.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] D. G. Lowe, “基于局部尺度不变特征的对象识别”，在 *IJCV*，第 2 卷，1999 年，第 1150–1157 页。'
- en: '[207] A. Oliva and A. Torralba, “Modeling the shape of the scene: A holistic
    representation of the spatial envelope,” *IJCV*, vol. 42, no. 3, pp. 145–175,
    2001.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] A. Oliva 和 A. Torralba, “场景形状建模：空间信封的整体表示”，*IJCV*，第 42 卷，第 3 期，第 145–175
    页，2001 年。'
- en: '[208] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in *CVPR*, vol. 1, 2005, pp. 886–893.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] N. Dalal 和 B. Triggs，“用于人体检测的方向梯度直方图”，发表于*CVPR*，第1卷，2005年，第886–893页。'
- en: '[209] J. Wu and J. M. Rehg, “CENTRIST: A visual descriptor for scene categorization,”
    *IEEE TPAMI*, vol. 33, no. 8, pp. 1489–1501, 2010.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] J. Wu 和 J. M. Rehg，“CENTRIST：一种用于场景分类的视觉描述符”，*IEEE TPAMI*，第33卷，第8期，第1489–1501页，2010年。'
- en: '[210] J. Sivic and A. Zisserman, “Video google: A text retrieval approach to
    object matching in videos,” in *ICCV*, vol. 2, 2003, p. 1470–1477.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] J. Sivic 和 A. Zisserman，“视频谷歌：一种基于文本的对象匹配方法”，发表于*ICCV*，第2卷，2003年，第1470–1477页。'
- en: '[211] L.-J. Li, H. Su, Y. Lim, and L. Fei-Fei, “Objects as attributes for scene
    classification,” in *ECCV*, 2010, pp. 57–69.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] L.-J. Li, H. Su, Y. Lim, 和 L. Fei-Fei，“将对象作为场景分类的属性”，发表于*ECCV*，2010年，第57–69页。'
- en: '[212] M. Juneja, A. Vedaldi, C. Jawahar, and A. Zisserman, “Blocks that shout:
    Distinctive parts for scene classification,” in *CVPR*, 2013, pp. 923–930.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] M. Juneja, A. Vedaldi, C. Jawahar, 和 A. Zisserman，“发出声音的块：用于场景分类的独特部分”，发表于*CVPR*，2013年，第923–930页。'
- en: '[213] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “CNN features
    off-the-shelf: An astounding baseline for recognition,” in *CVPRW*, 2014, pp.
    806–813.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] A. Sharif Razavian, H. Azizpour, J. Sullivan, 和 S. Carlsson，“现成的CNN特征：惊人的识别基线”，发表于*CVPRW*，2014年，第806–813页。'
- en: '[214] N. Vasconcelos and A. Lippman, “A probabilistic architecture for content-based
    image retrieval,” in *CVPR*, 2000, pp. 216–221.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] N. Vasconcelos 和 A. Lippman，“一种基于内容的图像检索的概率架构”，发表于*CVPR*，2000年，第216–221页。'
- en: '[215] C. Wallraven, B. Caputo, and A. Graf, “Recognition with local features:
    The kernel recipe,” in *ICCV*, 2003, pp. 257–264.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] C. Wallraven, B. Caputo, 和 A. Graf，“使用局部特征的识别：核方法”，发表于*ICCV*，2003年，第257–264页。'
- en: '[216] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    *IJCV*, vol. 60, no. 2, pp. 91–110, 2004.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] D. G. Lowe，“尺度不变关键点的独特图像特征”，*IJCV*，第60卷，第2期，第91–110页，2004年。'
- en: '[217] A. Oliva and A. Torralba, “Building the gist of a scene: The role of
    global image features in recognition,” *Progress in brain research*, vol. 155,
    pp. 23–36, 2006.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] A. Oliva 和 A. Torralba，“构建场景的要点：全球图像特征在识别中的作用”，*Progress in brain research*，第155卷，第23–36页，2006年。'
- en: '[218] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale
    and rotation invariant texture classification with local binary patterns,” *IEEE
    TPAMI*, vol. 24, no. 7, pp. 971–987, 2002.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] T. Ojala, M. Pietikainen, 和 T. Maenpaa，“使用局部二进制模式的多分辨率灰度和旋转不变纹理分类”，*IEEE
    TPAMI*，第24卷，第7期，第971–987页，2002年。'
- en: '[219] P. Felzenszwalb, D. McAllester, and D. Ramanan, “A discriminatively trained,
    multiscale, deformable part model,” in *CVPR*, 2008, pp. 1–8.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] P. Felzenszwalb, D. McAllester, 和 D. Ramanan，“一种判别训练的多尺度可变形部分模型”，发表于*CVPR*，2008年，第1–8页。'
- en: '[220] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object
    detection with discriminatively trained part-based models,” *IEEE TPAMI*, vol. 32,
    no. 9, pp. 1627–1645, 2009.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, 和 D. Ramanan，“基于部分的模型的目标检测”，*IEEE
    TPAMI*，第32卷，第9期，第1627–1645页，2009年。'
- en: '[221] M. Pandey and S. Lazebnik, “Scene recognition and weakly supervised object
    localization with deformable part-based models,” in *ICCV*, 2011, pp. 1307–1314.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] M. Pandey 和 S. Lazebnik，“场景识别和弱监督的对象定位与变形部分模型”，发表于*ICCV*，2011年，第1307–1314页。'
- en: '[222] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
    *JMLR*, vol. 3, no. 1, pp. 993–1022, 2003.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] D. M. Blei, A. Y. Ng, 和 M. I. Jordan，“潜在狄利克雷分配”，*JMLR*，第3卷，第1期，第993–1022页，2003年。'
- en: '[223] K. Grauman and T. Darrell, “The pyramid match kernel: Discriminative
    classification with sets of image features,” in *ICCV*, vol. 2, 2005, pp. 1458–1465.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] K. Grauman 和 T. Darrell，“金字塔匹配核：使用图像特征集合的判别分类”，发表于*ICCV*，第2卷，2005年，第1458–1465页。'
- en: '[224] L. Zhou, Z. Zhou, and D. Hu, “Scene classification using a multi-resolution
    bag-of-features model,” *Pattern Recognition*, vol. 46, no. 1, pp. 424–433, 2013.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] L. Zhou, Z. Zhou, 和 D. Hu，“使用多分辨率特征袋模型的场景分类”，*Pattern Recognition*，第46卷，第1期，第424–433页，2013年。'
- en: '[225] L. Xie, J. Wang, B. Guo, B. Zhang, and Q. Tian, “Orientational pyramid
    matching for recognizing indoor scenes,” in *CVPR*, 2014, pp. 3734–3741.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] L. Xie, J. Wang, B. Guo, B. Zhang, 和 Q. Tian，“用于识别室内场景的方向金字塔匹配”，发表于*CVPR*，2014年，第3734–3741页。'
- en: '[226] J. Yang, K. Yu, Y. Gong, and T. Huang, “Linear spatial pyramid matching
    using sparse coding for image classification,” in *CVPR*, 2009, pp. 1794–1801.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] J. Yang, K. Yu, Y. Gong, 和 T. Huang，“使用稀疏编码的线性空间金字塔匹配用于图像分类”，发表于*CVPR*，2009年，第1794–1801页。'
- en: '[227] S. Gao, I. W.-H. Tsang, L.-T. Chia, and P. Zhao, “Local features are
    not lonely–laplacian sparse coding for image classification,” in *CVPR*, 2010,
    pp. 3555–3561.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] S. Gao, I. W.-H. Tsang, L.-T. Chia 和 P. Zhao，“局部特征并非孤立–拉普拉斯稀疏编码用于图像分类，”
    见 *CVPR*，2010年，页3555–3561。'
- en: '[228] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong, “Locality-constrained
    linear coding for image classification,” in *CVPR*, 2010, pp. 3360–3367.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang 和 Y. Gong，“用于图像分类的局部约束线性编码，”
    见 *CVPR*，2010年，页3360–3367。'
- en: '[229] J. Wu and J. M. Rehg, “Beyond the euclidean distance: Creating effective
    visual codebooks using the histogram intersection kernel,” in *ICCV*, 2009, pp.
    630–637.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] J. Wu 和 J. M. Rehg，“超越欧几里得距离：使用直方图交集核创建有效的视觉词典，” 见 *ICCV*，2009年，页630–637。'
- en: '[230] J. Qin and N. H. Yung, “Scene categorization via contextual visual words,”
    *Pattern Recognition*, vol. 43, no. 5, pp. 1874–1888, 2010.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] J. Qin 和 N. H. Yung，“通过上下文视觉词进行场景分类，” *模式识别*，第43卷，第5期，页1874–1888，2010年。'
- en: '[231] L. Bo and C. Sminchisescu, “Efficient match kernel between sets of features
    for visual recognition,” in *NeurIPS*, 2009, pp. 135–143.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] L. Bo 和 C. Sminchisescu，“特征集间的高效匹配核用于视觉识别，” 见 *NeurIPS*，2009年，页135–143。'
- en: '[232] P. Wang, J. Wang, G. Zeng, W. Xu, H. Zha, and S. Li, “Supervised kernel
    descriptors for visual recognition,” in *CVPR*, 2013, pp. 2858–2865.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] P. Wang, J. Wang, G. Zeng, W. Xu, H. Zha 和 S. Li，“用于视觉识别的监督型核描述符，” 见
    *CVPR*，2013年，页2858–2865。'
- en: '[233] L.-J. Li, H. Su, Y. Lim, and L. Fei-Fei, “Object bank: An object-level
    image representation for high-level visual recognition,” *IJCV*, vol. 107, no. 1,
    pp. 20–39, 2014.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] L.-J. Li, H. Su, Y. Lim 和 L. Fei-Fei，“对象库：用于高层视觉识别的对象级图像表示，” *IJCV*，第107卷，第1期，页20–39，2014年。'
- en: '[234] L. Zhang, X. Zhen, and L. Shao, “Learning object-to-class kernels for
    scene classification,” *IEEE TIP*, vol. 23, no. 8, pp. 3241–3253, 2014.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] L. Zhang, X. Zhen 和 L. Shao，“学习对象到类别的核用于场景分类，” *IEEE TIP*，第23卷，第8期，页3241–3253，2014年。'
- en: '[235] F. Sadeghi and M. F. Tappen, “Latent pyramidal regions for recognizing
    scenes,” in *ECCV*, 2012, pp. 228–241.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] F. Sadeghi 和 M. F. Tappen，“用于场景识别的潜在金字塔区域，” 见 *ECCV*，2012年，页228–241。'
- en: '[236] J. Yu, D. Tao, Y. Rui, and J. Cheng, “Pairwise constraints based multiview
    features fusion for scene classification,” *Pattern Recognition*, vol. 46, no. 2,
    pp. 483–496, 2013.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] J. Yu, D. Tao, Y. Rui 和 J. Cheng，“基于对偶约束的多视图特征融合用于场景分类，” *模式识别*，第46卷，第2期，页483–496，2013年。'
- en: '[237] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing the
    gap to human-level performance in face verification,” in *CVPR*, 2014, pp. 1701–1708.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] Y. Taigman, M. Yang, M. Ranzato 和 L. Wolf，“Deepface：缩小人脸验证中的人类水平表现差距，”
    见 *CVPR*，2014年，页1701–1708。'
- en: '[238] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun, “Deep learning
    for 3d point clouds: A survey,” *IEEE TPAMI*, 2020.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu 和 M. Bennamoun，“深度学习用于3D点云：综述，”
    *IEEE TPAMI*，2020年。'
- en: '[239] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed *et al.*, “Deep
    neural networks for acoustic modeling in speech recognition,” *IEEE Signal Processing
    Magazine*, vol. 29, no. 6, pp. 82–97, 2012.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed *等*，“用于语音识别的深度神经网络声学建模，”
    *IEEE信号处理杂志*，第29卷，第6期，页82–97，2012年。'
- en: '[240] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “Deepdriving: Learning
    affordance for direct perception in autonomous driving,” in *ICCV*, 2015, pp.
    2722–2730.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] C. Chen, A. Seff, A. Kornhauser 和 J. Xiao，“Deepdriving：学习自适应感知以实现自动驾驶，”
    见 *ICCV*，2015年，页2722–2730。'
- en: '[241] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau,
    and S. Thrun, “Dermatologist level classification of skin cancer with deep neural
    networks,” *Nature*, vol. 542, no. 7639, pp. 115–118, 2017.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau 和
    S. Thrun，“使用深度神经网络对皮肤癌进行皮肤科医生级别的分类，” *自然*，第542卷，第7639期，页115–118，2017年。'
- en: '[242] S. M. McKinney, M. Sieniek, V. Godbole, J. Godwin, N. Antropova *et al.*,
    “International evaluation of an AI system for breast cancer screening,” *Nature*,
    vol. 577, no. 7788, pp. 89–94, 2020.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] S. M. McKinney, M. Sieniek, V. Godbole, J. Godwin, N. Antropova *等*，“国际评估用于乳腺癌筛查的AI系统，”
    *自然*，第577卷，第7788期，页89–94，2020年。'
- en: '[243] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi *et al.*, “Google’s
    neural machine translation system: Bridging the gap between human and machine
    translation,” *arXiv:1609.08144*, 2016.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi *等*，“谷歌的神经机器翻译系统：弥合人类与机器翻译之间的差距，”
    *arXiv:1609.08144*，2016年。'
- en: '[244] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre *et al.*, “Mastering
    the game of go with deep neural networks and tree search,” *Nature*, vol. 529,
    no. 7587, pp. 484–489, 2016.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre *等*，“通过深度神经网络和树搜索掌握围棋，”
    *自然*，第529卷，第7587期，第484–489页，2016年。'
- en: '[245] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang *et al.*,
    “Mastering the game of go without human knowledge,” *Nature*, vol. 550, no. 7676,
    pp. 354–359, 2017.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang *等*，“在没有人类知识的情况下掌握围棋，”
    *自然*，第550卷，第7676期，第354–359页，2017年。'
- en: '[246] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai *et al.*,
    “A general reinforcement learning algorithm that masters chess, shogi, and go
    through self-play,” *Science*, vol. 362, no. 6419, pp. 1140–1144, 2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai *等*，“一种通用的强化学习算法，通过自我对弈掌握国际象棋、将棋和围棋，”
    *科学*，第362卷，第6419期，第1140–1144页，2018年。'
- en: '[247] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik *et al.*,
    “Grandmaster level in starcraft ii using multi-agent reinforcement learning,”
    *Nature*, vol. 575, no. 7782, pp. 350–354, 2019.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik *等*，“使用多智能体强化学习达到星际争霸
    II 的大师级水平，” *自然*，第575卷，第7782期，第350–354页，2019年。'
- en: '[248] P. M. DeVries, F. Viégas, M. Wattenberg, and B. J. Meade, “Deep learning
    of aftershock patterns following large earthquakes,” *Nature*, vol. 560, no. 7720,
    pp. 632–634, 2018.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] P. M. DeVries, F. Viégas, M. Wattenberg 和 B. J. Meade，“大地震后余震模式的深度学习，”
    *自然*，第560卷，第7720期，第632–634页，2018年。'
- en: '[249] https://www.technologyreview.com/lists/technologies/2020/.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] https://www.technologyreview.com/lists/technologies/2020/'
- en: '[250] J. M. Stokes, K. Yang, K. Swanson, W. Jin, A. Cubillos-Ruiz, N. M. Donghia,
    C. R. MacNair, S. French, L. A. Carfrae, Z. Bloom-Ackerman *et al.*, “A deep learning
    approach to antibiotic discovery,” *Cell*, vol. 180, no. 4, pp. 688–702, 2020.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] J. M. Stokes, K. Yang, K. Swanson, W. Jin, A. Cubillos-Ruiz, N. M. Donghia,
    C. R. MacNair, S. French, L. A. Carfrae, Z. Bloom-Ackerman *等*，“抗生素发现的深度学习方法，”
    *细胞*，第180卷，第4期，第688–702页，2020年。'
- en: '[251] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    networks*, vol. 61, pp. 85–117, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] J. Schmidhuber，“神经网络中的深度学习：概述，” *神经网络*，第61卷，第85–117页，2015年。'
- en: '[252] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional
    networks,” in *ECCV*, 2014, pp. 818–833.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] M. D. Zeiler 和 R. Fergus，“可视化和理解卷积网络，” 收录于 *ECCV*，2014年，第818–833页。'
- en: '[253] S. Ioffe, “Batch renormalization: Towards reducing minibatch dependence
    in batch-normalized models,” in *NeurIPS*, 2017, pp. 1945–1953.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] S. Ioffe，“批量重新标准化：减少批量归一化模型中小批量依赖的进展，” 收录于 *NeurIPS*，2017年，第1945–1953页。'
- en: '[254] I. Goodfellow, Y. Bengio, and A. Courville, *Deep learning*.   MIT press,
    2016.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] I. Goodfellow, Y. Bengio 和 A. Courville，*深度学习*。 MIT出版社，2016年。'
- en: '[255] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi *et al.*,
    “A survey on deep learning in medical image analysis,” *Medical image analysis*,
    vol. 42, pp. 60–88, 2017.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi *等*，“关于医学图像分析中深度学习的调查，”
    *医学图像分析*，第42卷，第60–88页，2017年。'
- en: '[256] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *et al.*, “Recent advances in convolutional neural networks,”
    *Pattern Recognition*, vol. 77, pp. 354–377, 2018.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *等*，“卷积神经网络的最新进展，” *模式识别*，第77卷，第354–377页，2018年。'
- en: '[257] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu,
    S.-C. Chen, and S. Iyengar, “A survey on deep learning: Algorithms, techniques,
    and applications,” *Computing Surveys*, vol. 51, no. 5, pp. 1–36, 2018.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu,
    S.-C. Chen 和 S. Iyengar，“深度学习综述：算法、技术和应用，” *计算机综述*，第51卷，第5期，第1–36页，2018年。'
- en: '| Delu Zeng received his Ph.D. degree in electronic and information engineering
    from South China University of Technology, China, in 2009\. He is now a full professor
    in the School of Mathematics in South China University of Technology, China. He
    has been the visiting scholar of Columbia University, University of Oulu, University
    of Waterloo. He has been focusing his research in applied mathematics and its
    interdisciplinary applications. His research interests include numerical calculations,
    applications of partial differential equations, optimizations, machine learning
    and their applications in image processing, and data analysis. |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| Delu Zeng 于2009年获得了中国南方科技大学电子与信息工程博士学位。他现在是南方科技大学数学学院的全职教授。他曾担任哥伦比亚大学、奥卢大学、滑铁卢大学的访问学者。他一直专注于应用数学及其跨学科应用的研究。他的研究兴趣包括数值计算、偏微分方程的应用、优化、机器学习及其在图像处理和数据分析中的应用。
    |'
- en: '| Minyu Liao received her B.S. degree in applied mathematics from Shantou University,
    China, in 2018\. She is currently pursuing the master’s degrees in computational
    mathematics with South China University of Technology, China. Her research interests
    include computer vision, scene recognition, and deep learning. |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| Minyu Liao 于2018年获得了中国汕头大学应用数学学士学位。她目前在南方科技大学攻读计算数学硕士学位。她的研究兴趣包括计算机视觉、场景识别和深度学习。
    |'
- en: '| Mohammad Tavakolian received the M.Sc. degree in electrical engineering from
    Tafresh University, Iran in 2013\. Currently, he is a Ph.D. student at the Center
    for Machine and Signal Analysis (CMVS) of the University of Oulu, Finland. He
    has authored several journal and conference papers in IJCV, PRL, ICCV, ECCV, and
    ACCV. His research interests include representation learning, data efficient learning,
    computer vision, healthcare, and face analysis. |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| Mohammad Tavakolian 于2013年获得了伊朗塔夫雷什大学的电气工程硕士学位。目前，他是芬兰奥卢大学机器与信号分析中心（CMVS）的博士生。他在IJCV、PRL、ICCV、ECCV
    和 ACCV等期刊和会议上发表了多篇论文。他的研究兴趣包括表征学习、数据高效学习、计算机视觉、医疗保健和面部分析。 |'
- en: '| Yulan Guo He received his Ph.D. degrees from National University of Defense
    Technology (NUDT) in 2015, where he is currently an associate professor. He was
    a visiting Ph.D. student with the University of Western Australia from 2011 to
    2014\. He has authored over 90 articles in journals and conferences, such as the
    IEEE TPAMI and IJCV. His current research interests focus on 3D vision, particularly
    on 3D feature learning, 3D modeling, 3D object recognition, and scene understanding.
    |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| Yulan Guo 于2015年获得了国防科技大学（NUDT）的博士学位，目前是该校的副教授。他曾在2011至2014年期间作为访问博士生在西澳大学进行研究。他在IEEE
    TPAMI 和 IJCV等期刊和会议上发表了90多篇文章。他目前的研究兴趣集中在3D视觉，特别是3D特征学习、3D建模、3D物体识别和场景理解。 |'
- en: '| Bolei Zhou is currently an assistant professor at the Chinese University
    of Hong Kong, China. He received his Ph.D. from the Massachusetts Institute of
    Technology in 2018\. He received his M.phil from the Chinese University of Hong
    Kong and B.Eng. degree from the Shanghai Jiao Tong University in 2010\. He has
    authored over 70 articles in journals and conferences, such as IEEE TPAMI, ECCV,
    CVPR and AAAI. He is interested in understanding various human-centric properties
    of AI models beyond their performance, such as explainability, interpretability,
    steerability, generalization, fairness and bias. |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| Bolei Zhou 目前是中国香港中文大学的助理教授。他于2018年获得了麻省理工学院的博士学位。他在2010年获得了香港中文大学的硕士学位以及上海交通大学的工程学士学位。他在IEEE
    TPAMI、ECCV、CVPR 和 AAAI等期刊和会议上发表了70多篇文章。他对理解AI模型的各种以人为本的属性感兴趣，如解释性、可解释性、可操控性、泛化能力、公平性和偏见。
    |'
- en: '| Dewen Hu received the B.S. and M.S. degrees from Xi’an Jiaotong University,
    China, in 1983 and 1986, respectively, and the Ph.D. degree from the National
    University of Defense Technology in 1999\. He is currently a Professor at School
    of Intelligent Science, National University of Defense Technology. From October
    1995 to October 1996, he was a Visiting Scholar with the University of Sheffield,
    U.K. His research interests include image processing, system identification and
    control, neural networks, and cognitive science. |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| 胡德文于1983年和1986年分别获得中国西安交通大学的学士和硕士学位，并于1999年获得国防科技大学的博士学位。他目前是国防科技大学智能科学学院的教授。1995年10月至1996年10月，他在英国谢菲尔德大学担任访问学者。他的研究兴趣包括图像处理、系统辨识与控制、神经网络和认知科学。
    |'
- en: '| Matti Pietikäinen received his Ph.D. degree from the University of Oulu,
    Finland. He is now emeritus professor with the Center for Machine Vision and Signal
    Analysis, University of Oulu. He is a fellow of the IEEE for fundamental contributions,
    *e.g.,* , to Local Binary Pattern (LBP) methodology, texture based image and video
    analysis, and facial image analysis. He has authored more than 350 refereed papers
    in international journals, books, and conferences. His papers have nearly 68,700
    citations in Google Scholar (h-index 92). He was the recipient of the IAPR King-Sun
    Fu Prize 2018 for fundamental contributions to texture analysis and facial image
    analysis. |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| 马蒂·皮耶提凯宁获得了芬兰奥卢大学的博士学位。他目前是奥卢大学机器视觉与信号分析中心的名誉教授。他因对局部二值模式（LBP）方法论、基于纹理的图像和视频分析以及面部图像分析等方面的**基础贡献**而成为IEEE会士。他在国际期刊、书籍和会议上发表了超过350篇经审稿的论文。他的论文在Google
    Scholar上的引用次数接近68,700次（h-index 92）。他曾获得2018年IAPR King-Sun Fu奖，以表彰其在纹理分析和面部图像分析方面的**基础贡献**。
    |'
- en: '| Li Liu received the Ph.D. degree in information and communication engineering
    from the National University of Defense Technology (NUDT), China, in 2012\. She
    is currently a professor with NUDT. She spent two years as a Visiting Student
    at the University of Waterloo, Canada, from 2008 to 2010\. From 2015 to 2016,
    she spent ten months visiting the Multimedia Laboratory at the Chinese University
    of Hong Kong. From 2016.12 to 2018.9, she worked as a senior researcher at the
    Machine Vision Group at the University of Oulu, Finland. Her current research
    interests include computer vision, pattern recognition and machine learning. Her
    papers have currently over 3500+ citations in Google Scholar. |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| 刘莉于2012年获得国防科技大学（NUDT）的信息与通信工程博士学位。她目前是国防科技大学的教授。她在2008年至2010年期间，作为访问学生在加拿大滑铁卢大学学习了两年。从2015年到2016年，她在香港中文大学多媒体实验室进行了十个月的访问。2016年12月至2018年9月，她在芬兰奥卢大学机器视觉组担任高级研究员。她当前的研究兴趣包括计算机视觉、模式识别和机器学习。她的论文在Google
    Scholar上的引用次数已超过3500次。 |'
