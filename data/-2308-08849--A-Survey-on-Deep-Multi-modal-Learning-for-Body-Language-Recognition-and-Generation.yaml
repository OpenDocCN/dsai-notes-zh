- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:37:26'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:37:26
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2308.08849] A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2308.08849] 深度多模态学习在身体语言识别与生成中的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.08849](https://ar5iv.labs.arxiv.org/html/2308.08849)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2308.08849](https://ar5iv.labs.arxiv.org/html/2308.08849)
- en: A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度多模态学习在身体语言识别与生成中的调查
- en: 'Li Liu,, Lufei Gao, Wentao Lei, Fengji Ma, Xiaotian Lin, Jinting Wang Li Liu,
    Wentao Lei, Fengji Ma, Xiaotian Lin, and Jinting Wang are with the Hong Kong University
    of Science and Technology (Guangzhou), Guangzhou 511458, China. E-mail: avrillliu@hkust-gz.edu.cn.Lufei
    Gao is with the Shenzhen Research Institute of Big Data, Shenzhen, China.All authors
    are equal contribution.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 李刘、卢飞、高文涛、冯继、林晓天、王锦婷，均为香港科技大学（广州），中国广州511458。电子邮件：avrillliu@hkust-gz.edu.cn。卢飞高为深圳大数据研究院，深圳，中国。所有作者贡献相同。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Body language (BL) refers to the non-verbal communication expressed through
    physical movements, gestures, facial expressions, and postures. It is a form of
    communication that conveys information, emotions, attitudes, and intentions without
    the use of spoken or written words. It plays a crucial role in interpersonal interactions
    and can complement or even override verbal communication. Deep multi-modal learning
    techniques have shown promise in understanding and analyzing these diverse aspects
    of BL, which often incorporate multiple modalities. The survey explores recent
    advances in deep multi-modal learning, emphasizing their applications to BL generation
    and recognition. Several common BLs are considered i.e., Sign Language (SL), Cued
    Speech (CS), Co-speech (CoS), and Talking Head (TH), and we have conducted an
    analysis and established the connections among these four BL for the first time.
    Their generation and recognition often involve multi-modal approaches, for example,
    multi-modal feature representation, multi-modal fusion, and multi-modal joint
    learning will be introduced. Benchmark datasets for BL research are well collected
    and organized, along with the evaluation of state-of-the-art (SOTA) methods on
    these datasets. The survey highlights challenges such as limited labeled data,
    multi-modal learning, and the need for domain adaptation to generalize models
    to unseen speakers or languages. Future research directions are presented, including
    exploring self-supervised learning techniques, integrating contextual information
    from other modalities, and exploiting large-scale pre-trained multi-modal models.
    Real-world applications and user-centric evaluations are emphasized to drive practical
    adoption. In summary, this survey paper provides a comprehensive understanding
    of deep multi-modal learning for various BL generations and recognitions for the
    first time. By analyzing advancements, challenges, and future directions, it serves
    as a valuable resource for researchers and practitioners in advancing this field.
    In addition, we maintain a continuously updated paper list for deep multi-modal
    learning for BL recognition and generation: https://github.com/wentaoL86/awesome-body-language.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 身体语言（BL）指的是通过身体动作、手势、面部表情和姿势表达的非语言交流。这是一种传递信息、情感、态度和意图的交流方式，无需使用口头或书面语言。它在人与人之间的互动中发挥着至关重要的作用，并且可以补充甚至超越语言交流。深度多模态学习技术在理解和分析这些多样化的BL方面表现出了良好的前景，这些BL通常涉及多个模态。该调查探讨了深度多模态学习的最新进展，强调了其在BL生成和识别中的应用。我们考虑了几种常见的BL，如手语（SL）、提示语音（CS）、共语（CoS）和谈话头（TH），并首次对这四种BL进行了分析和关联。它们的生成和识别通常涉及多模态方法，例如，将介绍多模态特征表示、多模态融合和多模态联合学习。BL研究的基准数据集得到了良好的收集和组织，同时评估了这些数据集上的最先进（SOTA）方法。调查突出了挑战，如有限的标注数据、多模态学习以及将模型推广到未见过的说话者或语言的领域适应需求。提出了未来的研究方向，包括探索自监督学习技术、整合来自其他模态的上下文信息以及利用大规模预训练的多模态模型。强调了现实世界应用和以用户为中心的评估，以推动实际应用。总之，本调查论文首次提供了对各种BL生成和识别的深度多模态学习的全面理解。通过分析进展、挑战和未来方向，它为研究人员和实践者提供了宝贵的资源，推动了该领域的发展。此外，我们维护了一个不断更新的深度多模态学习用于BL识别和生成的论文列表：[https://github.com/wentaoL86/awesome-body-language](https://github.com/wentaoL86/awesome-body-language)。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep Multi-modal Learning, Body Language, Sign Language, Cued Speech, Co-speech,
    Talking Head, Recognition and Generation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度多模态学习、身体语言、手语、提示语音、同步语音、谈话头像、识别与生成。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Body language (BL), as a vital component of non-verbal communication, holds
    great significance in facilitating effective communication and enhancing social
    interactions. The ability to analyze and understand BL has various applications,
    ranging from BL recognition and generation to digital human interaction and assistive
    technologies. Understanding BL often necessitates the incorporation of multiple
    modalities. Deep multi-modal learning, which combines visual, audio and text modalities
    have emerged as a promising approach to enhancing the accuracy and robustness
    of intelligent BL multi-modal conversion systems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 身体语言（BL）作为非言语交流的重要组成部分，在促进有效沟通和增强社会互动方面具有重要意义。分析和理解BL的能力具有多种应用，包括BL识别和生成、数字人机交互和辅助技术等。理解BL通常需要整合多种模态。深度多模态学习，结合视觉、音频和文本模态，已成为提高智能BL多模态转换系统的准确性和鲁棒性的有前景的方法。
- en: '![Refer to caption](img/ab2aa85738386556e1710801db1f1b5d.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ab2aa85738386556e1710801db1f1b5d.png)'
- en: 'Figure 1: Examples of Cued Speech, Sign Language, Co-speech and Talking Head,
    respectively.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：分别展示了提示语音、手语、同步语音和谈话头像的示例。
- en: In this survey, we primarily focus on four typical BLs and use them as examples
    to review and analyze the multi-modal BL recognition and generation. Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation") presents a simple diagram for the four types of BLs,
    i.e., Cued Speech (CS) [[1](#bib.bib1)], Sign Language (SL) [[2](#bib.bib2)],
    Co-speech (CoS) [[3](#bib.bib3)] and Talking Head (TH) [[4](#bib.bib4)]. In this
    field, there have been numerous previous works, which have made significant progress.
    However, despite the progress made in deep multi-modal learning for BL generation
    and recognition, several challenges and open research questions remain, such as
    the multi-modal learning of different types of data modalities, the scarcity of
    labeled datasets, representing fine-grained cues, modeling temporal dynamics,
    and limited computational resources. These challenges need to be addressed in
    multi-modal BL recognition and generation to further advance the field and make
    applications in human-computer interaction (HCI), social robotics, and affective
    computing more effective, etc.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本调查中，我们主要关注四种典型的身体语言（BL），并以它们为例来回顾和分析多模态BL的识别和生成。图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣
    关于深度多模态学习在身体语言识别与生成中的调查") 展示了四种类型的BL的简单图示，即提示语音（CS）[[1](#bib.bib1)]，手语（SL）[[2](#bib.bib2)]，同步语音（CoS）[[3](#bib.bib3)]
    和谈话头像（TH）[[4](#bib.bib4)]。在这一领域，已有大量前期研究取得了显著进展。然而，尽管在BL生成和识别的深度多模态学习方面取得了进展，但仍然存在一些挑战和未解的研究问题，例如不同数据模态的多模态学习、标注数据集的稀缺、细粒度线索的表示、建模时间动态以及有限的计算资源。这些挑战需要在多模态BL识别和生成中得到解决，以进一步推动该领域的发展，并使人机交互（HCI）、社会机器人和情感计算等应用更加有效。
- en: '![Refer to caption](img/636d32181f3350c5e1c56643e55a017d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/636d32181f3350c5e1c56643e55a017d.png)'
- en: 'Figure 2: The architecture of this survey.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：本调查的架构。
- en: Organization of This Survey. In this survey, we first introduce four typical
    variants of BL and establish the connections between these four types in Section
    2\. Then, We organize and present various types of datasets for BL recognition
    and generation, along with evaluation metrics in Section 3\. In Sections 4 and
    5, we provide detailed reviews of the BL recognition and generation of CS, SL,
    CoS and TH, respectively. Furthermore, in Section 6, we give a detailed analysis
    of the challenges for these types of BL. Finally, we discuss and conclude this
    survey by proposing multiple research directions that need to be studied. The
    architecture of this survey is visualized in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation").
    The structured taxonomy of the existing BL research and some representative works
    are shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation").
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的组织结构。我们首先介绍了四种典型的BL变体，并在第2节中建立了这四种类型之间的联系。然后，我们在第3节中组织并展示了用于BL识别和生成的各种数据集及评估指标。在第4节和第5节中，我们分别对CS、SL、CoS和TH的BL识别和生成进行了详细的回顾。此外，在第6节中，我们对这些BL类型面临的挑战进行了详细分析。最后，我们通过提出多个需要研究的方向来讨论并总结本调查。该调查的架构在图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation")中进行了可视化。现有BL研究的结构化分类和一些代表性工作展示在图[3](#S1.F3 "Figure
    3 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation")中。
- en: 'TABLE I: The number of existing reviews.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：现有评审的数量。
- en: '| Type | SL | CS | CoS | TH | LR | SL+CS | LR+TH | Total |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | SL | CS | CoS | TH | LR | SL+CS | LR+TH | 总计 |'
- en: '| R | 5 | 1 | 0 | 0 | 5 | 1 | 0 | 12 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| R | 5 | 1 | 0 | 0 | 5 | 1 | 0 | 12 |'
- en: '| G | 4 | 0 | 1 | 3 | 0 | 0 | 0 | 8 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| G | 4 | 0 | 1 | 3 | 0 | 0 | 0 | 8 |'
- en: '| R&G | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 2 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| R&G | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 2 |'
- en: '| Total | 10 | 1 | 1 | 3 | 5 | 1 | 1 | 22 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 10 | 1 | 1 | 3 | 5 | 1 | 1 | 22 |'
- en: 'The corresponding terms for the abbreviations are as follows: R – Recognition;
    G – Generation; SL – Sign Language; CS – Cued Speech; CoS – Co-speech; TH – Talking
    Head; LR – Lip Reading.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 缩写的对应术语如下：R – 识别；G – 生成；SL – 手语；CS – 口述语言；CoS – 语音同步；TH – 讲话头；LR – 唇读。
- en: 'Differences from Existing Reviews. Table [I](#S1.T1 "TABLE I ‣ 1 Introduction
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")
    presents the number of review articles related to BL recognition and generation
    in the relevant field. While there are already 22 existing surveys, the differences
    between our survey and these prior works can be summarized as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有评审的差异。表[I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation")展示了相关领域中与BL识别和生成相关的评审文章数量。尽管已有22篇现有综述，我们的调查与这些前人工作的差异可以总结如下：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scope. Existing reviews on BL [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]
    have only focused on specific subtasks within the field. For BL recognition, the
    reviews [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [5](#bib.bib5), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]
    concentrate on SL recognition. Regarding BL generation, [[6](#bib.bib6)] only
    explores CoS generation and [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]
    delves into TH generation. [[20](#bib.bib20)] integrates subtasks: LR recognition
    and TH generation. Unlike the reviews mentioned earlier, this paper focuses on
    two primary tasks: recognition and generation. Each task is expanded to incorporate
    four different types of BL: SL, CS, CoS, and TH. As far as we know, this is the
    first to encompass all four types of BL along with their corresponding recognition
    and generation tasks.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 范围。现有关于BL的评审[[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]仅关注该领域内的特定子任务。对于BL识别，评审[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [5](#bib.bib5),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]集中在SL识别方面。关于BL生成，[[6](#bib.bib6)]仅探讨了CoS生成，而[[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)]深入探讨了TH生成。[[20](#bib.bib20)]整合了子任务：LR识别和TH生成。与之前提到的评审不同，本文关注于两个主要任务：识别和生成。每个任务扩展为包含四种不同类型的BL：SL、CS、CoS和TH。据我们所知，这是首次涵盖所有四种BL类型及其对应的识别和生成任务的综述。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Timeline. This survey highlights the latest advances, major challenges and deep
    learning (DL)-based multi-modal approaches in the aforementioned research areas
    from 2017 to the present. Please note that we will consistently update the repository
    we maintain with the latest developments. It is expected that this study will
    facilitate knowledge accumulation and the creation of deep multi-modal BL methods,
    providing readers, researchers, and practitioners with a roadmap to guide future
    direction.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 时间线。该调查突出显示了2017年至今在上述研究领域的最新进展、主要挑战以及基于深度学习（DL）的多模态方法。请注意，我们将持续更新我们维护的**资源库**，以反映最新的发展。预计这项研究将促进知识的积累和深度多模态BL方法的创建，为读者、研究人员和从业者提供**指导未来方向的路线图**。
- en: To summarize, this survey provides a thorough examination of the progress made
    in deep multi-modal learning techniques for automatic BL recognition and generation.
    It also outlines the road ahead for future research in this area. The objective
    is to offer researchers and practitioners a consolidated understanding of the
    field, covering the foundational principles, multi-modal fusion methods, DL architectures,
    benchmark datasets, challenges, and potential directions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这项调查提供了对深度多模态学习技术在自动化BL识别和生成方面取得进展的**深入审查**。它还概述了未来研究的方向。目标是为研究人员和从业者提供对该领域的**综合理解**，涵盖基础原理、多模态融合方法、DL架构、基准数据集、挑战以及潜在方向。
- en: <svg   height="729.8" overflow="visible" version="1.1" width="1070.89"><g transform="translate(0,729.8)
    matrix(1 0 0 -1 0 0) translate(530.51,0) translate(0,324.13)"><g stroke="#000000"
    fill="#000000" stroke-width="1.2pt"><g transform="matrix(1.0 0.0 0.0 1.0 -69.19
    -0.83)" fill="#000000" stroke="#000000"><foreignobject width="138.37" height="17.71"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Body Language
    <g stroke="#80B3CC" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -16.01 8.41)"><path
    d="M 233.44 -103.78 C 233.44 -74.12 209.4 -50.09 179.75 -50.09 C 150.1 -50.09
    126.06 -74.12 126.06 -103.78 C 126.06 -133.43 150.1 -157.47 179.75 -157.47 C 209.4
    -157.47 233.44 -133.43 233.44 -103.78 Z M 179.75 -103.78" style="stroke:none"></path></g><g
    fill="#80B3CC" stroke="#80B3CC"><path d="M 233.44 -103.78 C 233.44 -74.12 209.4
    -50.09 179.75 -50.09 C 150.1 -50.09 126.06 -74.12 126.06 -103.78 C 126.06 -133.43
    150.1 -157.47 179.75 -157.47 C 209.4 -157.47 233.44 -133.43 233.44 -103.78 Z M
    179.75 -103.78"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 126.48 -106.08)"
    fill="#000000" stroke="#000000"><foreignobject width="106.55" height="14.76" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Body Language Generation</foreignobject></g>
    <clippath ><path d="M 68.19 -39.37 M 77.54 -13.67 C 74.33 -31.9 64.79 -48.42 50.61
    -60.32 C 65.69 -47.67 82.89 -53.28 99.94 -63.12 L 104.64 -54.99 C 87.59 -45.14
    74.13 -33.06 77.54 -13.67 Z M 112.35 -59.44 L 104.64 -54.99 L 99.94 -63.12 L 107.65
    -67.58 Z M 99.94 -63.12 M 145.24 -62.65 C 135.57 -70.76 129.07 -82.02 126.87 -94.46
    C 129.21 -81.24 119.28 -74.29 107.65 -67.58 L 112.35 -59.44 C 123.98 -66.15 134.96
    -71.28 145.24 -62.65 Z"></path></clippath><g clip-path="url(#pgfcp9)"><g transform="matrix(1.0
    0.0 0.0 1.0 68.19 -39.37)"><g fill="#CCCCCC"><path d="M 40.57 67.51 L -28.82 107.57
    L -107.57 -28.83 L -38.18 -68.89 Z M -107.57 -28.83" style="stroke:none"></path></g><g
    fill="#80B3CC"><path d="M 103.24 31.33 L 150.93 3.79 L 72.18 -132.61 L 24.49 -105.07
    Z M 72.18 -132.61" style="stroke:none"><g transform="matrix(0.46848 -0.27048 0.567
    0.98207 32.53 -18.78)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    stroke="#80B3CC" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -30.39 0.11)"><path
    d="M 377.03 -20.76 C 377.03 8.78 353.08 32.73 323.55 32.73 C 294.01 32.73 270.07
    8.78 270.07 -20.76 C 270.07 -50.29 294.01 -74.24 323.55 -74.24 C 353.08 -74.24
    377.03 -50.29 377.03 -20.76 Z M 323.55 -20.76" style="stroke:none"></path></g><g
    fill="#80B3CC" stroke="#80B3CC"><path d="M 377.03 -20.76 C 377.03 8.78 353.08
    32.73 323.55 32.73 C 294.01 32.73 270.07 8.78 270.07 -20.76 C 270.07 -50.29 294.01
    -74.24 323.55 -74.24 C 353.08 -74.24 377.03 -50.29 377.03 -20.76 Z M 323.55 -20.76"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 270.28 -24.29)" fill="#000000" stroke="#000000"><foreignobject
    width="106.55" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Sign Language production</foreignobject></g> <g fill="#80B3CC"><path
    d="M 226.25 -76.93 M 214.26 -62.65 C 223.93 -70.76 230.43 -82.02 232.63 -94.45
    C 230.29 -81.24 240.21 -74.27 251.84 -67.56 L 247.16 -59.46 C 235.53 -66.17 224.54
    -71.28 214.26 -62.65 Z M 251.74 -56.81 L 247.16 -59.46 L 251.84 -67.56 L 256.42
    -64.92 Z M 251.84 -67.56 M 270.88 -30.04 C 273.06 -42.42 279.54 -53.64 289.17
    -61.72 C 278.93 -53.13 267.99 -58.23 256.42 -64.92 L 251.74 -56.81 C 263.32 -50.13
    273.2 -43.21 270.88 -30.04 Z" style="stroke:none"></path></g><g stroke="#80B3CC"
    fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -30.39 16.72)"><path d="M 377.03
    -186.8 C 377.03 -157.26 353.08 -133.32 323.55 -133.32 C 294.01 -133.32 270.07
    -157.26 270.07 -186.8 C 270.07 -216.34 294.01 -240.28 323.55 -240.28 C 353.08
    -240.28 377.03 -216.34 377.03 -186.8 Z M 323.55 -186.8" style="stroke:none"></path></g><g
    fill="#80B3CC" stroke="#80B3CC"><path d="M 377.03 -186.8 C 377.03 -157.26 353.08
    -133.32 323.55 -133.32 C 294.01 -133.32 270.07 -157.26 270.07 -186.8 C 270.07
    -216.34 294.01 -240.28 323.55 -240.28 C 353.08 -240.28 377.03 -216.34 377.03 -186.8
    Z M 323.55 -186.8"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 270.28 -190.34)"
    fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Cued Speech Generation</foreignobject></g>
    <g fill="#80B3CC"><path d="M 226.25 -130.62 M 232.63 -113.1 C 230.43 -125.53 223.93
    -136.79 214.26 -144.91 C 224.54 -136.28 235.53 -141.39 247.16 -148.1 L 251.84
    -139.99 C 240.21 -133.28 230.29 -126.32 232.63 -113.1 Z M 256.42 -142.64 L 251.84
    -139.99 L 247.16 -148.1 L 251.74 -150.74 Z M 247.16 -148.1 M 289.17 -145.83 C
    279.54 -153.91 273.06 -165.13 270.88 -177.51 C 273.2 -164.35 263.32 -157.43 251.74
    -150.74 L 256.42 -142.64 C 267.99 -149.32 278.93 -154.43 289.17 -145.83 Z" style="stroke:none"></path></g><g
    stroke="#80B3CC" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -16.01 25.02)"><path
    d="M 233.23 -269.82 C 233.23 -240.28 209.29 -216.34 179.75 -216.34 C 150.21 -216.34
    126.27 -240.28 126.27 -269.82 C 126.27 -299.36 150.21 -323.3 179.75 -323.3 C 209.29
    -323.3 233.23 -299.36 233.23 -269.82 Z M 179.75 -269.82" style="stroke:none"></path></g><g
    fill="#80B3CC" stroke="#80B3CC"><path d="M 233.23 -269.82 C 233.23 -240.28 209.29
    -216.34 179.75 -216.34 C 150.21 -216.34 126.27 -240.28 126.27 -269.82 C 126.27
    -299.36 150.21 -323.3 179.75 -323.3 C 209.29 -323.3 233.23 -299.36 233.23 -269.82
    Z M 179.75 -269.82"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 126.48 -273.36)"
    fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Talking Head Generation</foreignobject></g>
    <g fill="#80B3CC"><path d="M 179.75 -157.47 M 198.11 -154.23 C 186.25 -158.55
    173.25 -158.55 161.38 -154.23 C 174 -158.82 175.07 -170.89 175.07 -184.32 L 184.43
    -184.32 C 184.43 -170.89 185.5 -158.82 198.11 -154.23 Z M 184.43 -189.6 L 184.43
    -184.32 L 175.07 -184.32 L 175.07 -189.6 Z M 175.07 -184.32 M 198.04 -219.57 C
    186.23 -215.27 173.27 -215.27 161.46 -219.57 C 174.02 -214.99 175.07 -202.97 175.07
    -189.6 L 184.43 -189.6 C 184.43 -202.97 185.48 -214.99 198.04 -219.57 Z" style="stroke:none"></path></g><g
    stroke="#80B3CC" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -1.63 16.72)"><path
    d="M 89.43 -186.8 C 89.43 -157.26 65.49 -133.32 35.95 -133.32 C 6.41 -133.32 -17.53
    -157.26 -17.53 -186.8 C -17.53 -216.34 6.41 -240.28 35.95 -240.28 C 65.49 -240.28
    89.43 -216.34 89.43 -186.8 Z M 35.95 -186.8" style="stroke:none"></path></g><g
    fill="#80B3CC" stroke="#80B3CC"><path d="M 89.43 -186.8 C 89.43 -157.26 65.49
    -133.32 35.95 -133.32 C 6.41 -133.32 -17.53 -157.26 -17.53 -186.8 C -17.53 -216.34
    6.41 -240.28 35.95 -240.28 C 65.49 -240.28 89.43 -216.34 89.43 -186.8 Z M 35.95
    -186.8"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -17.32 -190.34)" fill="#000000"
    stroke="#000000"><foreignobject width="106.55" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Co-speech Generation</foreignobject></g>
    <g fill="#80B3CC"><path d="M 133.25 -130.62 M 145.24 -144.91 C 135.57 -136.79
    129.06 -125.53 126.87 -113.1 C 129.2 -126.32 119.29 -133.28 107.66 -139.99 L 112.34
    -148.1 C 123.97 -141.39 134.95 -136.28 145.24 -144.91 Z M 107.76 -150.74 L 112.34
    -148.1 L 107.66 -139.99 L 103.08 -142.64 Z M 107.66 -139.99 M 88.62 -177.51 C
    86.43 -165.13 79.96 -153.91 70.32 -145.83 C 80.57 -154.43 91.5 -149.32 103.08
    -142.64 L 107.76 -150.74 C 96.18 -157.43 86.29 -164.35 88.62 -177.51 Z" style="stroke:none"></path></g><g
    stroke="#66C2A3" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 1.97 -22.72)"><path
    d="M 53.69 207.56 C 53.69 237.21 29.65 261.25 0 261.25 C -29.65 261.25 -53.69
    237.21 -53.69 207.56 C -53.69 177.9 -29.65 153.86 0 153.86 C 29.65 153.86 53.69
    177.9 53.69 207.56 Z M 0 207.56" style="stroke:none"></path></g><g fill="#66C2A3"
    stroke="#66C2A3"><path d="M 53.69 207.56 C 53.69 237.21 29.65 261.25 0 261.25
    C -29.65 261.25 -53.69 237.21 -53.69 207.56 C -53.69 177.9 -29.65 153.86 0 153.86
    C 29.65 153.86 53.69 177.9 53.69 207.56 Z M 0 207.56"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -53.27 205.25)" fill="#000000" stroke="#000000"><foreignobject width="106.55"
    height="14.76" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Typical
    Body Language</foreignobject></g> <clippath ><path d="M 0 78.74 M -26.93 73.99
    C -9.54 80.32 9.54 80.32 26.93 73.99 C 8.43 80.72 4.7 98.43 4.7 118.11 L -4.7
    118.11 C -4.7 98.43 -8.43 80.72 -26.93 73.99 Z M -4.7 127.02 L -4.7 118.11 L 4.7
    118.11 L 4.7 127.02 Z M 4.7 118.11 M -18.36 157.1 C -6.5 152.78 6.5 152.78 18.36
    157.1 C 5.75 152.51 4.7 140.44 4.7 127.02 L -4.7 127.02 C -4.7 140.44 -5.75 152.51
    -18.36 157.1 Z"></path></clippath><g clip-path="url(#pgfcp11)"><g transform="matrix(1.0
    0.0 0.0 1.0 0 78.74)"><g fill="#CCCCCC"><path d="M -78.75 1.38 L -78.75 -78.74
    L 78.75 -78.74 L 78.75 1.38 Z M 78.75 -78.74" style="stroke:none"></path></g><g
    fill="#66C2A3"><path d="M -78.75 73.74 L -78.75 128.82 L 78.75 128.82 L 78.75
    73.74 Z M 78.75 128.82" style="stroke:none"><g transform="matrix(0.0 0.54095 -1.134
    0.0 0 37.57)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    stroke="#66C2A3" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 18.57 -22.72)"><path
    d="M -112.56 207.56 C -112.56 237.09 -136.51 261.04 -166.04 261.04 C -195.58 261.04
    -219.53 237.09 -219.53 207.56 C -219.53 178.02 -195.58 154.07 -166.04 154.07 C
    -136.51 154.07 -112.56 178.02 -112.56 207.56 Z M -166.04 207.56" style="stroke:none"></path></g><g
    fill="#66C2A3" stroke="#66C2A3"><path d="M -112.56 207.56 C -112.56 237.09 -136.51
    261.04 -166.04 261.04 C -195.58 261.04 -219.53 237.09 -219.53 207.56 C -219.53
    178.02 -195.58 154.07 -166.04 154.07 C -136.51 154.07 -112.56 178.02 -112.56 207.56
    Z M -166.04 207.56"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -219.32 203.94)"
    fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Sign Language</foreignobject></g>
    <g fill="#66C2A3"><path d="M -53.69 207.56 M -50.45 189.19 C -54.77 201.05 -54.77
    214.06 -50.45 225.92 C -55.04 213.31 -67.11 212.23 -80.54 212.23 L -80.54 202.88
    C -67.11 202.88 -55.04 201.8 -50.45 189.19 Z M -85.82 202.88 L -80.54 202.88 L
    -80.54 212.23 L -85.82 212.23 Z M -80.54 212.23 M -115.79 189.26 C -111.49 201.08
    -111.49 214.03 -115.79 225.85 C -111.22 213.28 -99.19 212.23 -85.82 212.23 L -85.82
    202.88 C -99.19 202.88 -111.22 201.83 -115.79 189.26 Z" style="stroke:none"></path></g><g
    stroke="#66C2A3" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 10.27 -37.1)"><path
    d="M -29.54 351.35 C -29.54 380.89 -53.48 404.84 -83.02 404.84 C -112.56 404.84
    -136.5 380.89 -136.5 351.35 C -136.5 321.82 -112.56 297.87 -83.02 297.87 C -53.48
    297.87 -29.54 321.82 -29.54 351.35 Z M -83.02 351.35" style="stroke:none"></path></g><g
    fill="#66C2A3" stroke="#66C2A3"><path d="M -29.54 351.35 C -29.54 380.89 -53.48
    404.84 -83.02 404.84 C -112.56 404.84 -136.5 380.89 -136.5 351.35 C -136.5 321.82
    -112.56 297.87 -83.02 297.87 C -53.48 297.87 -29.54 321.82 -29.54 351.35 Z M -83.02
    351.35"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -136.29 347.82)" fill="#000000"
    stroke="#000000"><foreignobject width="106.55" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Cued Speech</foreignobject></g>
    <g fill="#66C2A3"><path d="M -26.85 254.05 M -41.13 242.07 C -33.02 251.74 -21.75
    258.24 -9.32 260.43 C -22.54 258.1 -29.5 268.02 -36.22 279.64 L -44.32 274.96
    C -37.61 263.34 -32.5 252.35 -41.13 242.07 Z M -46.97 279.54 L -44.32 274.96 L
    -36.22 279.64 L -38.86 284.22 Z M -36.22 279.64 M -73.74 298.69 C -61.35 300.87
    -50.14 307.35 -42.05 316.98 C -50.65 306.74 -45.55 295.8 -38.86 284.22 L -46.97
    279.54 C -53.65 291.12 -60.57 301.01 -73.74 298.69 Z" style="stroke:none"></path></g><g
    stroke="#66C2A3" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -6.34 -37.1)"><path
    d="M 136.5 351.35 C 136.5 380.89 112.56 404.84 83.02 404.84 C 53.48 404.84 29.54
    380.89 29.54 351.35 C 29.54 321.82 53.48 297.87 83.02 297.87 C 112.56 297.87 136.5
    321.82 136.5 351.35 Z M 83.02 351.35" style="stroke:none"></path></g><g fill="#66C2A3"
    stroke="#66C2A3"><path d="M 136.5 351.35 C 136.5 380.89 112.56 404.84 83.02 404.84
    C 53.48 404.84 29.54 380.89 29.54 351.35 C 29.54 321.82 53.48 297.87 83.02 297.87
    C 112.56 297.87 136.5 321.82 136.5 351.35 Z M 83.02 351.35"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 29.75 347.82)" fill="#000000" stroke="#000000"><foreignobject width="106.55"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Co-speech</foreignobject></g>
    <g fill="#66C2A3"><path d="M 26.85 254.05 M 9.32 260.43 C 21.75 258.24 33.02 251.74
    41.13 242.07 C 32.5 252.35 37.61 263.34 44.32 274.96 L 36.22 279.64 C 29.5 268.02
    22.54 258.1 9.32 260.43 Z M 38.86 284.22 L 36.22 279.64 L 44.32 274.96 L 46.97
    279.54 Z M 44.32 274.96 M 42.05 316.98 C 50.14 307.35 61.35 300.87 73.74 298.69
    C 60.57 301.01 53.65 291.12 46.97 279.54 L 38.86 284.22 C 45.55 295.8 50.65 306.74
    42.05 316.98 Z" style="stroke:none"></path></g><g stroke="#66C2A3" fill="#808080"
    transform="matrix(1.1 0.0 0.0 1.1 -14.64 -22.72)"><path d="M 219.53 207.56 C 219.53
    237.09 195.58 261.04 166.04 261.04 C 136.51 261.04 112.56 237.09 112.56 207.56
    C 112.56 178.02 136.51 154.07 166.04 154.07 C 195.58 154.07 219.53 178.02 219.53
    207.56 Z M 166.04 207.56" style="stroke:none"></path></g><g fill="#66C2A3" stroke="#66C2A3"><path
    d="M 219.53 207.56 C 219.53 237.09 195.58 261.04 166.04 261.04 C 136.51 261.04
    112.56 237.09 112.56 207.56 C 112.56 178.02 136.51 154.07 166.04 154.07 C 195.58
    154.07 219.53 178.02 219.53 207.56 Z M 166.04 207.56"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 112.77 204.02)" fill="#000000" stroke="#000000"><foreignobject width="106.55"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Talking
    Head</foreignobject></g> <g fill="#66C2A3"><path d="M 53.69 207.56 M 50.45 225.92
    C 54.77 214.06 54.77 201.05 50.45 189.19 C 55.04 201.8 67.11 202.88 80.54 202.88
    L 80.54 212.23 C 67.11 212.23 55.04 213.31 50.45 225.92 Z M 80.54 202.88 h 5.28
    v 9.36 h -5.28 Z M 115.79 225.85 C 111.49 214.03 111.49 201.08 115.79 189.26 C
    111.22 201.83 99.19 202.88 85.82 202.88 L 85.82 212.23 C 99.19 212.23 111.22 213.28
    115.79 225.85 Z" style="stroke:none"></path></g><g stroke="#FFD166" fill="#808080"
    transform="matrix(1.1 0.0 0.0 1.1 19.94 8.41)"><path d="M -126.06 -103.78 C -126.06
    -74.12 -150.1 -50.09 -179.75 -50.09 C -209.4 -50.09 -233.44 -74.12 -233.44 -103.78
    C -233.44 -133.43 -209.4 -157.47 -179.75 -157.47 C -150.1 -157.47 -126.06 -133.43
    -126.06 -103.78 Z M -179.75 -103.78" style="stroke:none"></path></g><g fill="#FFD166"
    stroke="#FFD166"><path d="M -126.06 -103.78 C -126.06 -74.12 -150.1 -50.09 -179.75
    -50.09 C -209.4 -50.09 -233.44 -74.12 -233.44 -103.78 C -233.44 -133.43 -209.4
    -157.47 -179.75 -157.47 C -150.1 -157.47 -126.06 -133.43 -126.06 -103.78 Z M -179.75
    -103.78"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -233.02 -106.08)" fill="#000000"
    stroke="#000000"><foreignobject width="106.55" height="14.76" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Body Language Recognition</foreignobject></g>
    <clippath ><path d="M -68.19 -39.37 M -50.61 -60.32 C -64.79 -48.42 -74.33 -31.9
    -77.54 -13.67 C -74.13 -33.06 -87.59 -45.14 -104.64 -54.99 L -99.94 -63.12 C -82.89
    -53.28 -65.69 -47.67 -50.61 -60.32 Z M -107.65 -67.58 L -99.94 -63.12 L -104.64
    -54.99 L -112.35 -59.44 Z M -104.64 -54.99 M -126.87 -94.46 C -129.07 -82.02 -135.57
    -70.76 -145.24 -62.65 C -134.96 -71.28 -123.98 -66.15 -112.35 -59.44 L -107.65
    -67.58 C -119.28 -74.29 -129.21 -81.24 -126.87 -94.46 Z"></path></clippath><g
    clip-path="url(#pgfcp13)"><g transform="matrix(1.0 0.0 0.0 1.0 -68.19 -39.37)"><g
    fill="#CCCCCC"><path d="M 38.18 -68.89 L 107.57 -28.83 L 28.82 107.57 L -40.57
    67.51 Z M 28.82 107.57" style="stroke:none"></path></g><g fill="#FFD166"><path
    d="M -24.49 -105.07 L -72.18 -132.61 L -150.93 3.79 L -103.24 31.33 Z M -150.93
    3.79" style="stroke:none"><g transform="matrix(-0.46848 -0.27048 0.567 -0.98207
    -32.53 -18.78)"><defs><lineargradient ></lineargradient></defs></g></path></g></g></g><g
    stroke="#FFD166" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 19.94 25.02)"><path
    d="M -126.27 -269.82 C -126.27 -240.28 -150.21 -216.34 -179.75 -216.34 C -209.29
    -216.34 -233.23 -240.28 -233.23 -269.82 C -233.23 -299.36 -209.29 -323.3 -179.75
    -323.3 C -150.21 -323.3 -126.27 -299.36 -126.27 -269.82 Z M -179.75 -269.82" style="stroke:none"></path></g><g
    fill="#FFD166" stroke="#FFD166"><path d="M -126.27 -269.82 C -126.27 -240.28 -150.21
    -216.34 -179.75 -216.34 C -209.29 -216.34 -233.23 -240.28 -233.23 -269.82 C -233.23
    -299.36 -209.29 -323.3 -179.75 -323.3 C -150.21 -323.3 -126.27 -299.36 -126.27
    -269.82 Z M -179.75 -269.82"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -233.02
    -273.36)" fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Lip reading
    recognition</foreignobject></g> <g fill="#FFD166"><path d="M -179.75 -157.47 M
    -161.38 -154.23 C -173.25 -158.55 -186.25 -158.55 -198.11 -154.23 C -185.5 -158.82
    -184.43 -170.89 -184.43 -184.32 L -175.07 -184.32 C -175.07 -170.89 -174 -158.82
    -161.38 -154.23 Z M -175.07 -189.6 L -175.07 -184.32 L -184.43 -184.32 L -184.43
    -189.6 Z M -184.43 -184.32 M -161.46 -219.57 C -173.27 -215.27 -186.23 -215.27
    -198.04 -219.57 C -185.48 -214.99 -184.43 -202.97 -184.43 -189.6 L -175.07 -189.6
    C -175.07 -202.97 -174.02 -214.99 -161.46 -219.57 Z" style="stroke:none"></path></g><g
    stroke="#FFD166" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 34.32 16.72)"><path
    d="M -270.07 -186.8 C -270.07 -157.26 -294.01 -133.32 -323.55 -133.32 C -353.08
    -133.32 -377.03 -157.26 -377.03 -186.8 C -377.03 -216.34 -353.08 -240.28 -323.55
    -240.28 C -294.01 -240.28 -270.07 -216.34 -270.07 -186.8 Z M -323.55 -186.8" style="stroke:none"></path></g><g
    fill="#FFD166" stroke="#FFD166"><path d="M -270.07 -186.8 C -270.07 -157.26 -294.01
    -133.32 -323.55 -133.32 C -353.08 -133.32 -377.03 -157.26 -377.03 -186.8 C -377.03
    -216.34 -353.08 -240.28 -323.55 -240.28 C -294.01 -240.28 -270.07 -216.34 -270.07
    -186.8 Z M -323.55 -186.8"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -376.82
    -190.34)" fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.3"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Cued Speech
    recognition</foreignobject></g> <g fill="#FFD166"><path d="M -226.25 -130.62 M
    -214.26 -144.91 C -223.93 -136.79 -230.43 -125.53 -232.63 -113.1 C -230.29 -126.32
    -240.21 -133.28 -251.84 -139.99 L -247.16 -148.1 C -235.53 -141.39 -224.54 -136.28
    -214.26 -144.91 Z M -251.74 -150.74 L -247.16 -148.1 L -251.84 -139.99 L -256.42
    -142.64 Z M -251.84 -139.99 M -270.88 -177.51 C -273.06 -165.13 -279.54 -153.91
    -289.17 -145.83 C -278.93 -154.43 -267.99 -149.32 -256.42 -142.64 L -251.74 -150.74
    C -263.32 -157.43 -273.2 -164.35 -270.88 -177.51 Z" style="stroke:none"></path></g><g
    stroke="#FFD166" fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 34.32 0.11)"><path
    d="M -270.07 -20.76 C -270.07 8.78 -294.01 32.73 -323.55 32.73 C -353.08 32.73
    -377.03 8.78 -377.03 -20.76 C -377.03 -50.29 -353.08 -74.24 -323.55 -74.24 C -294.01
    -74.24 -270.07 -50.29 -270.07 -20.76 Z M -323.55 -20.76" style="stroke:none"></path></g><g
    fill="#FFD166" stroke="#FFD166"><path d="M -270.07 -20.76 C -270.07 8.78 -294.01
    32.73 -323.55 32.73 C -353.08 32.73 -377.03 8.78 -377.03 -20.76 C -377.03 -50.29
    -353.08 -74.24 -323.55 -74.24 C -294.01 -74.24 -270.07 -50.29 -270.07 -20.76 Z
    M -323.55 -20.76"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -376.82 -24.37)"
    fill="#000000" stroke="#000000"><foreignobject width="106.55" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible" color="#000000">Sign Language recognition</foreignobject></g>
    <g fill="#FFD166"><path d="M -226.25 -76.93 M -232.63 -94.45 C -230.43 -82.02
    -223.93 -70.76 -214.26 -62.65 C -224.54 -71.28 -235.53 -66.17 -247.16 -59.46 L
    -251.84 -67.56 C -240.21 -74.27 -230.29 -81.24 -232.63 -94.45 Z M -256.42 -64.92
    L -251.84 -67.56 L -247.16 -59.46 L -251.74 -56.81 Z M -247.16 -59.46 M -289.17
    -61.72 C -279.54 -53.64 -273.06 -42.42 -270.88 -30.04 C -273.2 -43.21 -263.32
    -50.13 -251.74 -56.81 L -256.42 -64.92 C -267.99 -58.23 -278.93 -53.13 -289.17
    -61.72 Z" style="stroke:none"></path></g><g fill="#000000" stroke="#000000" stroke-width="1.5pt"><g
    fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -44.13 -0.21)"><path d="M 533.8
    8.36 L 388.03 8.36 C 384.98 8.36 382.5 5.88 382.5 2.83 L 382.5 -37.89 C 382.5
    -40.95 384.98 -43.43 388.03 -43.43 L 533.8 -43.43 C 536.86 -43.43 539.34 -40.95
    539.34 -37.89 L 539.34 2.83 C 539.34 5.88 536.86 8.36 533.8 8.36 Z M 382.5 -43.43"
    style="stroke:none"></path></g><g stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path
    d="M 533.8 8.36 L 388.03 8.36 C 384.98 8.36 382.5 5.88 382.5 2.83 L 382.5 -37.89
    C 382.5 -40.95 384.98 -43.43 388.03 -43.43 L 533.8 -43.43 C 536.86 -43.43 539.34
    -40.95 539.34 -37.89 L 539.34 2.83 C 539.34 5.88 536.86 8.36 533.8 8.36 Z M 382.5
    -43.43"></path></g><g transform="matrix(0.65 0.0 0.0 0.65 387.62 -3.05)" fill="#000000"
    stroke="#000000"><foreignobject width="225.54" height="63.93" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Text2sign[[21](#bib.bib21)] HLSTM[[22](#bib.bib22)]
    ESN[[23](#bib.bib23)]</foreignobject></g></g> <g fill="#000000" stroke="#000000"
    stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -1.63
    26.08)"><path d="M 102.99 -247.58 L -31.09 -247.58 C -34.15 -247.58 -36.62 -250.06
    -36.62 -253.11 L -36.62 -307.68 C -36.62 -310.74 -34.15 -313.22 -31.09 -313.22
    L 102.99 -313.22 C 106.04 -313.22 108.52 -310.74 108.52 -307.68 L 108.52 -253.11
    C 108.52 -250.06 106.04 -247.58 102.99 -247.58 Z M -36.62 -313.22" style="stroke:none"></path></g><g
    stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path d="M 102.99 -247.58
    L -31.09 -247.58 C -34.15 -247.58 -36.62 -250.06 -36.62 -253.11 L -36.62 -307.68
    C -36.62 -310.74 -34.15 -313.22 -31.09 -313.22 L 102.99 -313.22 C 106.04 -313.22
    108.52 -310.74 108.52 -307.68 L 108.52 -253.11 C 108.52 -250.06 106.04 -247.58
    102.99 -247.58 Z M -36.62 -313.22"></path></g><g transform="matrix(0.65 0.0 0.0
    0.65 -31.51 -258.99)" fill="#000000" stroke="#000000"><foreignobject width="207.56"
    height="85.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DiffGAN[[24](#bib.bib24)]
    RG[[25](#bib.bib25)] SEEG[[26](#bib.bib26)] HA2G[[27](#bib.bib27)]</foreignobject></g></g>
    <g fill="#000000" stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 -43.73 17.06)"><path d="M 523.94 -171.29 L 389.86 -171.29 C 386.8
    -171.29 384.33 -173.77 384.33 -176.83 L 384.33 -203.69 C 384.33 -206.75 386.8
    -209.23 389.86 -209.23 L 523.94 -209.23 C 526.99 -209.23 529.47 -206.75 529.47
    -203.69 L 529.47 -176.83 C 529.47 -173.77 526.99 -171.29 523.94 -171.29 Z M 384.33
    -209.23" style="stroke:none"></path></g><g stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path
    d="M 523.94 -171.29 L 389.86 -171.29 C 386.8 -171.29 384.33 -173.77 384.33 -176.83
    L 384.33 -203.69 C 384.33 -206.75 386.8 -209.23 389.86 -209.23 L 523.94 -209.23
    C 526.99 -209.23 529.47 -206.75 529.47 -203.69 L 529.47 -176.83 C 529.47 -173.77
    526.99 -171.29 523.94 -171.29 Z M 384.33 -209.23"></path></g><g transform="matrix(0.65
    0.0 0.0 0.65 389.44 -182.7)" fill="#000000" stroke="#000000"><foreignobject width="207.56"
    height="42.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Paul Duchnowski[[28](#bib.bib28)]
    Gérard Bailly[[29](#bib.bib29)]</foreignobject></g></g> <g fill="#000000" stroke="#000000"
    stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -29.35
    26.75)"><path d="M 380.14 -254.3 L 246.06 -254.3 C 243.01 -254.3 240.53 -256.78
    240.53 -259.83 L 240.53 -314.4 C 240.53 -317.46 243.01 -319.94 246.06 -319.94
    L 380.14 -319.94 C 383.2 -319.94 385.67 -317.46 385.67 -314.4 L 385.67 -259.83
    C 385.67 -256.78 383.2 -254.3 380.14 -254.3 Z M 240.53 -319.94" style="stroke:none"></path></g><g
    stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path d="M 380.14 -254.3
    L 246.06 -254.3 C 243.01 -254.3 240.53 -256.78 240.53 -259.83 L 240.53 -314.4
    C 240.53 -317.46 243.01 -319.94 246.06 -319.94 L 380.14 -319.94 C 383.2 -319.94
    385.67 -317.46 385.67 -314.4 L 385.67 -259.83 C 385.67 -256.78 383.2 -254.3 380.14
    -254.3 Z M 240.53 -319.94"></path></g><g transform="matrix(0.65 0.0 0.0 0.65 245.65
    -265.71)" fill="#000000" stroke="#000000"><foreignobject width="207.56" height="85.24"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Wav2lip [[30](#bib.bib30)]
    Audio2head[[31](#bib.bib31)] AD-NERF[[32](#bib.bib32)] DiffTalk[[33](#bib.bib33)]</foreignobject></g></g>
    <g fill="#000000" stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 33.28 26.75)"><path d="M -246.06 -254.3 L -380.14 -254.3 C -383.2
    -254.3 -385.67 -256.78 -385.67 -259.83 L -385.67 -314.4 C -385.67 -317.46 -383.2
    -319.94 -380.14 -319.94 L -246.06 -319.94 C -243.01 -319.94 -240.53 -317.46 -240.53
    -314.4 L -240.53 -259.83 C -240.53 -256.78 -243.01 -254.3 -246.06 -254.3 Z M -385.67
    -319.94" style="stroke:none"></path></g><g stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path
    d="M -246.06 -254.3 L -380.14 -254.3 C -383.2 -254.3 -385.67 -256.78 -385.67 -259.83
    L -385.67 -314.4 C -385.67 -317.46 -383.2 -319.94 -380.14 -319.94 L -246.06 -319.94
    C -243.01 -319.94 -240.53 -317.46 -240.53 -314.4 L -240.53 -259.83 C -240.53 -256.78
    -243.01 -254.3 -246.06 -254.3 Z M -385.67 -319.94"></path></g><g transform="matrix(0.65
    0.0 0.0 0.65 -380.56 -265.71)" fill="#000000" stroke="#000000"><foreignobject
    width="207.56" height="85.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Rule-based[[34](#bib.bib34)]
    LBP[[35](#bib.bib35)] SDF[[36](#bib.bib36)] PTSLP[[37](#bib.bib37)]</foreignobject></g></g>
    <g fill="#000000" stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 47.66 17.06)"><path d="M -389.86 -157.44 L -523.94 -157.44 C -526.99
    -157.44 -529.47 -159.92 -529.47 -162.97 L -529.47 -217.54 C -529.47 -220.6 -526.99
    -223.08 -523.94 -223.08 L -389.86 -223.08 C -386.8 -223.08 -384.33 -220.6 -384.33
    -217.54 L -384.33 -162.97 C -384.33 -159.92 -386.8 -157.44 -389.86 -157.44 Z M
    -529.47 -223.08" style="stroke:none"></path></g><g stroke="#000000" fill="#FFFFFF"
    stroke-width="1.5pt"><path d="M -389.86 -157.44 L -523.94 -157.44 C -526.99 -157.44
    -529.47 -159.92 -529.47 -162.97 L -529.47 -217.54 C -529.47 -220.6 -526.99 -223.08
    -523.94 -223.08 L -389.86 -223.08 C -386.8 -223.08 -384.33 -220.6 -384.33 -217.54
    L -384.33 -162.97 C -384.33 -159.92 -386.8 -157.44 -389.86 -157.44 Z M -529.47
    -223.08"></path></g><g transform="matrix(0.65 0.0 0.0 0.65 -524.35 -168.85)" fill="#000000"
    stroke="#000000"><foreignobject width="207.56" height="85.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Syn[[38](#bib.bib38)] MMFSL[[39](#bib.bib39)]
    Re-Syn[[40](#bib.bib40)] CMML[[41](#bib.bib41)]</foreignobject></g></g> <g fill="#000000"
    stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 47.66 0.46)"><path d="M -389.86 8.6 L -523.94 8.6 C -526.99 8.6 -529.47
    6.13 -529.47 3.07 L -529.47 -51.5 C -529.47 -54.56 -526.99 -57.03 -523.94 -57.03
    L -389.86 -57.03 C -386.8 -57.03 -384.33 -54.56 -384.33 -51.5 L -384.33 3.07 C
    -384.33 6.13 -386.8 8.6 -389.86 8.6 Z M -529.47 -57.03" style="stroke:none"></path></g><g
    stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path d="M -389.86 8.6 L
    -523.94 8.6 C -526.99 8.6 -529.47 6.13 -529.47 3.07 L -529.47 -51.5 C -529.47
    -54.56 -526.99 -57.03 -523.94 -57.03 L -389.86 -57.03 C -386.8 -57.03 -384.33
    -54.56 -384.33 -51.5 L -384.33 3.07 C -384.33 6.13 -386.8 8.6 -389.86 8.6 Z M
    -529.47 -57.03"></path></g><g transform="matrix(0.65 0.0 0.0 0.65 -524.35 -2.81)"
    fill="#000000" stroke="#000000"><foreignobject width="207.56" height="85.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">DTW[[42](#bib.bib42)] HMMs[[43](#bib.bib43)]
    FCN[[44](#bib.bib44)] RL[[45](#bib.bib45)]</foreignobject></g></g> <g fill="#000000"
    stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 23.6 -36.76)"><path d="M -149.34 366.86 L -283.41 366.86 C -286.47
    366.86 -288.95 364.39 -288.95 361.33 L -288.95 334.46 C -288.95 331.4 -286.47
    328.93 -283.41 328.93 L -149.34 328.93 C -146.28 328.93 -143.8 331.4 -143.8 334.46
    L -143.8 361.33 C -143.8 364.39 -146.28 366.86 -149.34 366.86 Z M -288.95 328.93"
    style="stroke:none"></path></g><g stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path
    d="M -149.34 366.86 L -283.41 366.86 C -286.47 366.86 -288.95 364.39 -288.95 361.33
    L -288.95 334.46 C -288.95 331.4 -286.47 328.93 -283.41 328.93 L -149.34 328.93
    C -146.28 328.93 -143.8 331.4 -143.8 334.46 L -143.8 361.33 C -143.8 364.39 -146.28
    366.86 -149.34 366.86 Z M -288.95 328.93"></path></g><g transform="matrix(0.65
    0.0 0.0 0.65 -283.83 355.45)" fill="#000000" stroke="#000000"><foreignobject width="207.56"
    height="42.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CS[[1](#bib.bib1)]
    MCCS[[46](#bib.bib46)]</foreignobject></g></g> <g fill="#000000" stroke="#000000"
    stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 31.91
    -22.38)"><path d="M -232.36 223.06 L -366.43 223.06 C -369.49 223.06 -371.97 220.59
    -371.97 217.53 L -371.97 190.66 C -371.97 187.61 -369.49 185.13 -366.43 185.13
    L -232.36 185.13 C -229.3 185.13 -226.82 187.61 -226.82 190.66 L -226.82 217.53
    C -226.82 220.59 -229.3 223.06 -232.36 223.06 Z M -371.97 185.13" style="stroke:none"></path></g><g
    stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path d="M -232.36 223.06
    L -366.43 223.06 C -369.49 223.06 -371.97 220.59 -371.97 217.53 L -371.97 190.66
    C -371.97 187.61 -369.49 185.13 -366.43 185.13 L -232.36 185.13 C -229.3 185.13
    -226.82 187.61 -226.82 190.66 L -226.82 217.53 C -226.82 220.59 -229.3 223.06
    -232.36 223.06 Z M -371.97 185.13"></path></g><g transform="matrix(0.65 0.0 0.0
    0.65 -366.85 211.65)" fill="#000000" stroke="#000000"><foreignobject width="207.56"
    height="42.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SLreview[[2](#bib.bib2)]
    UnorgSign[[47](#bib.bib47)]</foreignobject></g></g> <g fill="#000000" stroke="#000000"
    stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1 0.0 0.0 1.1 -19.67
    -36.76)"><path d="M 283.41 373.79 L 149.34 373.79 C 146.28 373.79 143.8 371.31
    143.8 368.25 L 143.8 327.54 C 143.8 324.48 146.28 322 149.34 322 L 283.41 322
    C 286.47 322 288.95 324.48 288.95 327.54 L 288.95 368.25 C 288.95 371.31 286.47
    373.79 283.41 373.79 Z M 143.8 322" style="stroke:none"></path></g><g stroke="#000000"
    fill="#FFFFFF" stroke-width="1.5pt"><path d="M 283.41 373.79 L 149.34 373.79 C
    146.28 373.79 143.8 371.31 143.8 368.25 L 143.8 327.54 C 143.8 324.48 146.28 322
    149.34 322 L 283.41 322 C 286.47 322 288.95 324.48 288.95 327.54 L 288.95 368.25
    C 288.95 371.31 286.47 373.79 283.41 373.79 Z M 143.8 322"></path></g><g transform="matrix(0.65
    0.0 0.0 0.65 148.92 362.38)" fill="#000000" stroke="#000000"><foreignobject width="207.56"
    height="63.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CoSreview[[6](#bib.bib6)]
    CoSreview[[3](#bib.bib3)] SE[[48](#bib.bib48)]</foreignobject></g></g> <g fill="#000000"
    stroke="#000000" stroke-width="1.5pt"><g fill="#808080" transform="matrix(1.1
    0.0 0.0 1.1 -27.98 -22.38)"><path d="M 366.43 229.99 L 232.36 229.99 C 229.3 229.99
    226.82 227.51 226.82 224.46 L 226.82 183.74 C 226.82 180.68 229.3 178.2 232.36
    178.2 L 366.43 178.2 C 369.49 178.2 371.97 180.68 371.97 183.74 L 371.97 224.46
    C 371.97 227.51 369.49 229.99 366.43 229.99 Z M 226.82 178.2" style="stroke:none"></path></g><g
    stroke="#000000" fill="#FFFFFF" stroke-width="1.5pt"><path d="M 366.43 229.99
    L 232.36 229.99 C 229.3 229.99 226.82 227.51 226.82 224.46 L 226.82 183.74 C 226.82
    180.68 229.3 178.2 232.36 178.2 L 366.43 178.2 C 369.49 178.2 371.97 180.68 371.97
    183.74 L 371.97 224.46 C 371.97 227.51 369.49 229.99 366.43 229.99 Z M 226.82
    178.2"></path></g><g transform="matrix(0.65 0.0 0.0 0.65 231.94 218.58)" fill="#000000"
    stroke="#000000"><foreignobject width="207.56" height="63.93" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">THreview[[4](#bib.bib4)] THE[[49](#bib.bib49)]
    VHTHG[[50](#bib.bib50)]</foreignobject></g></g>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Body Language <g stroke="#80B3CC" fill="#808080" transform="matrix(1.1 0.0 0.0
    1.1 -16.
- en: 'Figure 3: Structured taxonomy of the existing BL research which includes three
    genres. Only several representative methods of each category are demonstrated.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 现有 BL 研究的结构化分类法，包括三种类型。仅展示了每类的几个代表性方法。'
- en: '![Refer to caption](img/4f2f7967da9b444908ba912a5c099e7b.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4f2f7967da9b444908ba912a5c099e7b.png)'
- en: 'Figure 4: Element compositions of four typical body language cases.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 四种典型身体语言案例的元素组成。'
- en: 2 Typical Body Language
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 典型的身体语言
- en: BL through which humans convey information usually involves five aspects, i.e.,
    gestures, facial expressions, lip reading (LR), head pose and postures. In this
    survey, we refer to these five aspects as the basic elements of BL.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 人类传递信息的 BL 通常涉及五个方面，即手势、面部表情、读唇（LR）、头部姿势和体态。在本调查中，我们将这五个方面称为 BL 的基本元素。
- en: Gestures refer to the use of hand movements to convey meaning. People communicate
    through actions such as waving, pointing, or gesturing with their hands. Additionally,
    facial expressions play a crucial role as a basic element of BL. Humans express
    emotions and intentions by altering the facial muscles around the eyes, eyebrows,
    mouth, etc. Another fundamental element is LR, which involves interpreting speech
    by observing the movements of the lips and mouth. Furthermore, head pose, including
    tilting or turning the head, can also convey information related to attention,
    interest, or specific desires. Lastly, postures, such as standing, sitting, or
    body leaning, contribute to conveying emotional states and social intentions within
    BL.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 手势指的是使用手部动作来传达意义。人们通过挥手、指点或手势等动作进行交流。此外，面部表情作为 BL 的基本元素也起着至关重要的作用。人类通过改变眼睛、眉毛、嘴巴等周围的面部肌肉来表达情感和意图。另一个基本元素是
    LR，它涉及通过观察嘴唇和口部的运动来解释语音。此外，头部姿势，包括倾斜或转动头部，也可以传达与注意力、兴趣或特定愿望相关的信息。最后，体态，如站立、坐姿或身体倾斜，有助于在
    BL 中传达情感状态和社会意图。
- en: It is common for BL cases to consist of two or more of these modalities. As
    shown in Figure [4](#S1.F4 "Figure 4 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation"), we listed four typical
    BL cases that are discussed in this survey, and each of them can be regarded as
    a composition of the basic BL elements. In this section, we will provide a comprehensive
    overview of these four BL cases, including their concepts, significance, and the
    challenges that exist in their corresponding recognition or generation tasks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BL 案例通常由两种或更多这些模式组成。如图 [4](#S1.F4 "Figure 4 ‣ 1 Introduction ‣ A Survey on Deep
    Multi-modal Learning for Body Language Recognition and Generation") 所示，我们列出了本调查中讨论的四种典型
    BL 案例，每一种都可以视为基本 BL 元素的组合。在本节中，我们将提供这四种 BL 案例的全面概述，包括它们的概念、重要性以及在其对应的识别或生成任务中存在的挑战。
- en: 2.1 Sign Language
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 手语
- en: SL is categorized as a natural language commonly used in deaf communities [[2](#bib.bib2)].
    Based on data from the World Federation of the Deaf, the worldwide population
    of the deaf is estimated to be around 72 million, with over 80% living in developing
    nations[[47](#bib.bib47)]. Over 300 different SLs are used by these individuals,
    each having its own distinct vocabulary and grammar. SL is also known as a visual
    language which is generally composed of several visual partials, such as gestures,
    facial expressions, head pose and body postures. Specifically, six basic parameters
    are listed as the basic components of SL in [[51](#bib.bib51)], i.e., hand shape,
    orientation, movement, location, mouth shape and eyebrow movements. Taking an
    overall perspective into account, we regard gestures, facial expressions, and
    head poses as the primary visual modalities in SL.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SL 被归类为在聋人社区中常用的自然语言 [[2](#bib.bib2)]。根据世界聋人联合会的数据，全球聋人总数估计约为 7200 万人，其中超过 80%
    生活在发展中国家[[47](#bib.bib47)]。这些个体使用超过 300 种不同的 SL，每种都有其独特的词汇和语法。SL 也被称为视觉语言，通常由若干视觉部分组成，如手势、面部表情、头部姿势和体态。具体而言，[[51](#bib.bib51)]
    中列出了六个基本参数作为 SL 的基本组成部分，即手型、方向、动作、位置、口形和眉毛动作。从整体角度来看，我们将手势、面部表情和头部姿势视为 SL 中的主要视觉模式。
- en: SL is the major communication tool for the deaf, yet it is difficult to be mastered.
    In order to eliminate communication barriers, it is of great significance to develop
    technologies for automatic SL processing, including SL recognition (SLR) that
    extracts words or utterances by capturing and analyzing image or video sequences
    of the SL data, SL generation (SLG) that generates visualizable SL animations
    from input with semantic meaning and SL translation that translates the extracted
    information to another signed or spoken language [[52](#bib.bib52)][[53](#bib.bib53)].
    This survey mainly focuses on the literature review of SLR and SLG in order to
    deeply understand the important issues and difficulties in the field of SL processing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 手语是聋人主要的沟通工具，但却难以掌握。为了消除沟通障碍，开发自动手语处理技术具有重要意义，包括手语识别 (SLR)，通过捕捉和分析手语数据的图像或视频序列来提取词汇或语句；手语生成
    (SLG)，从输入的语义信息生成可视化的手语动画；以及手语翻译，将提取的信息翻译成另一种手语或口语 [[52](#bib.bib52)][[53](#bib.bib53)]。本调查主要集中在手语识别和手语生成的文献综述，以深入了解手语处理领域的重要问题和难点。
- en: As a highly dynamic and multi-modal visual language, SL involves a combination
    of multiple visual elements that have complementary semantics. Therefore, extracting
    and fusing high-dimensional features from different modalities effectively is
    an important task. Deep multi-modal learning techniques play a pivotal role in
    addressing these challenges and advancing the field of SL processing. By combining
    visual and spatial information from video or depth sensors with linguistic cues,
    these approaches have shown promising results in improving the accuracy and naturalness
    of SLR and SLG systems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种高度动态和多模态的视觉语言，手语涉及多种具有互补语义的视觉元素的组合。因此，从不同模态中有效提取和融合高维特征是一个重要任务。深度多模态学习技术在解决这些挑战和推动手语处理领域的发展方面发挥了关键作用。通过结合来自视频或深度传感器的视觉和空间信息与语言提示，这些方法在提高手语识别和手语生成系统的准确性和自然性方面显示了有希望的结果。
- en: In Section [4.1](#S4.SS1 "4.1 Sign Language Recognition ‣ 4 Automatic Body Language
    Recognition ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation") and [5.1](#S5.SS1 "5.1 Sign Language Generation ‣ 5 Automatic
    Body Language Generation ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), we investigate the recent research advancements
    and techniques in deep multi-modal learning specifically for SLR and SLG. Besides,
    we delve into the challenges that are associated with these tasks and emphasize
    the potential applications and future directions in this field.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [4.1](#S4.SS1 "4.1 手语识别 ‣ 4 自动身体语言识别 ‣ 深度多模态学习在身体语言识别与生成中的调查") 和 [5.1](#S5.SS1
    "5.1 手语生成 ‣ 5 自动身体语言生成 ‣ 深度多模态学习在身体语言识别与生成中的调查") 节中，我们研究了深度多模态学习在手语识别 (SLR) 和手语生成
    (SLG) 方面的最新研究进展和技术。此外，我们探讨了与这些任务相关的挑战，并强调了该领域的潜在应用和未来方向。
- en: 2.2 Cued Speech
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 提示言语
- en: CS is a visual communication system proposed by Cornett [[1](#bib.bib1)] to
    enhance speech perception for individuals with hearing loss. CS uses a set of
    hand shapes and positions, named cues, to code the phonemes such as consonants
    and vowels. Figure [5](#S2.F5 "Figure 5 ‣ 2.2 Cued Speech ‣ 2 Typical Body Language
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")
    presents the chart for Mandarin Chinese CS (MCCS) [[46](#bib.bib46), [54](#bib.bib54)].
    Gestures in CS functions as a complement to lip-reading, visualizing the phonetic
    details that can be observed from the mouth movements to remove ambiguities caused
    by lip-reading alone. As a clear and unambiguous visual counterpart to the auditory
    information in spoken language, CS enables individuals with hearing loss to better
    understand and distinguish speech sounds, facilitating their language acquisition,
    spoken capabilities, reading skills, and overall communication abilities.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: CS 是由 Cornett 提出的视觉沟通系统 [[1](#bib.bib1)]，旨在增强听力损失个体的言语感知。CS 使用一组手势和位置，称为提示，以编码音素，如辅音和元音。图
    [5](#S2.F5 "图 5 ‣ 2.2 提示言语 ‣ 2 典型的身体语言 ‣ 深度多模态学习在身体语言识别与生成中的调查") 展示了普通话 CS (MCCS)
    的图表 [[46](#bib.bib46), [54](#bib.bib54)]。CS 中的手势作为唇读的补充，直观地展示了从口部运动中观察到的语音细节，以消除仅靠唇读所带来的模糊性。作为口语中听觉信息的清晰且不含歧义的视觉对应物，CS
    使听力损失个体能够更好地理解和区分言语声音，促进他们的语言习得、口语能力、阅读技能和整体沟通能力。
- en: '![Refer to caption](img/66fe9c6984825888f54fc36da048b0cf.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/66fe9c6984825888f54fc36da048b0cf.png)'
- en: 'Figure 5: The corresponding table between hand shapes and hand positions in
    Mandarin Chinese CS for vowels and consonants, respectively.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：汉语 CS 中元音和辅音的手形和手位对应表。
- en: CS has currently been adapted to approximately 65 languages and dialects globally,
    including English, French, Chinese, etc. [[55](#bib.bib55)]. Recently, there has
    been growing interest in developing technologies for automatic recognition and
    generation in the CS research field [[56](#bib.bib56), [57](#bib.bib57)]. These
    technologies aim to enhance accessibility for people who primarily use CS for
    communication. For instance, utilizing automatic CS recognition (ACSR), people
    can effortlessly transcribe gestures and lip-reading into corresponding spoken
    language at a phonemic level [[58](#bib.bib58), [59](#bib.bib59)]. In the opposite
    direction, a digital agent equipped with automatic CS generation (ACSG) can convert
    spoken input into authentic CS expressions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: CS 目前已适应全球约 65 种语言和方言，包括英语、法语、中文等[[55](#bib.bib55)]。最近，对在 CS 研究领域开发自动识别和生成技术的兴趣日益增长[[56](#bib.bib56),
    [57](#bib.bib57)]。这些技术旨在增强主要使用 CS 进行沟通的人的可达性。例如，利用自动 CS 识别（ACSR），人们可以轻松地将手势和唇读转录为相应的语音语言，达到音位水平[[58](#bib.bib58),
    [59](#bib.bib59)]。相反，配备自动 CS 生成（ACSG）的数字代理可以将口语输入转换为真实的 CS 表达。
- en: 'To process CS data efficiently, it is crucial to effectively extract information
    from two modalities: hand and lip movement. However, this task poses challenges
    in several aspects. Firstly, there exists an inherent asynchrony phenomenon when
    the human brain processes speech with gestures [[40](#bib.bib40)]; secondly, recognizing
    or generating appropriate hand shapes and lip movements entails tackling fine-grained
    image processing problems [[60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62)].
    Consequently, deep multi-modal learning techniques have emerged as a prominent
    research trend to uncover the interplay between gestures and LR, aiming to achieve
    high-performance ACSR or ACSG systems.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效处理 CS 数据，从手部和唇部动作这两个模态中有效提取信息至关重要。然而，这一任务在多个方面都存在挑战。首先，人脑在处理带手势的语音时存在固有的异步现象[[40](#bib.bib40)]；其次，识别或生成适当的手形和唇部动作涉及解决细粒度的图像处理问题[[60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62)]。因此，深度多模态学习技术作为揭示手势与 LR 之间相互作用的突出研究趋势应运而生，旨在实现高性能的
    ACSR 或 ACSG 系统。
- en: In Section [4.2](#S4.SS2 "4.2 Cued Speech Recognition ‣ 4 Automatic Body Language
    Recognition ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation") and [5.2](#S5.SS2 "5.2 Cued Speech Generation ‣ 5 Automatic Body
    Language Generation ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), we will discuss some SOTA methods to solve the problems
    lying in CS processing. We explore the challenges and opportunities in leveraging
    deep multi-modal learning techniques in this new research area, aiming to enhance
    the accessibility and inclusivity of communication for individuals who rely on
    CS.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [4.2](#S4.SS2 "4.2 Cued Speech Recognition ‣ 4 Automatic Body Language Recognition
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")
    和 [5.2](#S5.SS2 "5.2 Cued Speech Generation ‣ 5 Automatic Body Language Generation
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")
    节中，我们将讨论一些 SOTA 方法，以解决 CS 处理中的问题。我们探讨在这一新研究领域利用深度多模态学习技术的挑战与机遇，旨在提升依赖 CS 进行沟通的个体的可达性和包容性。
- en: 2.3 Co-speech
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 同步言语
- en: CoS refers to the non-verbal behaviors and signals that accompany and complement
    spoken language during communication [[6](#bib.bib6)]. It encompasses various
    visual cues, such as gestures, body postures, and facial expressions such as eye
    gaze and blinking, which can be used in conjunction with speech to convey additional
    information and meaning [[63](#bib.bib63)][[64](#bib.bib64)]. CoS gestures contribute
    substantially to the overall comprehension and interpretation of spoken language
    [[65](#bib.bib65)]. They serve as contextual cues, accentuate salient points,
    convey emotional states, and facilitate social interactions [[66](#bib.bib66)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: CoS 指的是在沟通过程中伴随并补充口语的非语言行为和信号[[6](#bib.bib6)]。它包括各种视觉线索，如手势、身体姿势和面部表情，如眼神和眨眼，这些可以与言语结合使用，以传达额外的信息和意义[[63](#bib.bib63)][[64](#bib.bib64)]。CoS
    手势对口语的整体理解和解释有重要贡献[[65](#bib.bib65)]。它们作为上下文线索，强调重点，传达情感状态，并促进社交互动[[66](#bib.bib66)]。
- en: With the development of AI agents technologies, there has been extensive research
    exploration in CoS generation or synthesis to give AI agents such as digital humans
    more expressive and realistic BL [[67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69),
    [70](#bib.bib70), [27](#bib.bib27)]. The primary objective of this task is to
    generate a sequence of human BL by utilizing speech audio and transcripts as input,
    enhancing the performance of human-machine interaction systems. On the other hand,
    most existing gesture recognition methods primarily focus on recognizing specific
    types of gestures [[71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)],
    overlooking their connections with other modalities such as speech.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI代理技术的发展，在CoS生成或合成方面进行了广泛的研究探索，以使数字人等AI代理具有更具表现力和逼真的身体语言 [[67](#bib.bib67),
    [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70), [27](#bib.bib27)]。这项任务的主要目标是利用语音音频和转录作为输入生成一系列人类身体语言，从而提升人机交互系统的性能。另一方面，大多数现有的手势识别方法主要集中于识别特定类型的手势
    [[71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)]，忽视了它们与语音等其他模态的关系。
- en: CoS signals not only play a crucial role in enhancing the clarity, expressiveness
    and emotional content of verbal communication, but also capture the rich communicative
    context, and reveal the speaker’s social identity and cultural affiliation [[48](#bib.bib48)].
    Therefore, it is a growing trend towards exploring multi-modal approaches that
    take into account both the visual information from gestures and the accompanying
    speech signals, which allows for more comprehensive and accurate analysis in the
    areas such as emotion recognition and dialogue understanding [[75](#bib.bib75)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: CoS信号不仅在增强言语交流的清晰度、表现力和情感内容方面发挥着重要作用，还捕捉到丰富的交流背景，并揭示了说话者的社会身份和文化归属 [[48](#bib.bib48)]。因此，探索考虑手势视觉信息和伴随语音信号的多模态方法已成为一种发展趋势，这使得在情感识别和对话理解等领域能够进行更全面和准确的分析
    [[75](#bib.bib75)]。
- en: In Section [4.3](#S4.SS3 "4.3 Co-speech Recognition ‣ 4 Automatic Body Language
    Recognition ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation"), we provide a brief overview of the work on automatic CoS recognition.
    Due to its limited application scenarios, research in this field is relatively
    scarce. In Section [5.3](#S5.SS3 "5.3 Co-speech Generation ‣ 5 Automatic Body
    Language Generation ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), we review the SOTA techniques and advancements in
    deep multi-modal learning for CoS generation, highlighting the potential applications
    and future research directions in this field.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在[4.3](#S4.SS3 "4.3 Co-speech Recognition ‣ 4 Automatic Body Language Recognition
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")节中，我们简要概述了自动CoS识别的工作。由于其应用场景有限，该领域的研究相对稀缺。在[5.3](#S5.SS3
    "5.3 Co-speech Generation ‣ 5 Automatic Body Language Generation ‣ A Survey on
    Deep Multi-modal Learning for Body Language Recognition and Generation")节中，我们回顾了CoS生成的SOTA技术和进展，突出了该领域的潜在应用和未来研究方向。
- en: 'TABLE II: Multi-Modal Body Language Datasets.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：多模态身体语言数据集。
- en: '| Type | Name | Year | Scale | Modal | Language | Link |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 名称 | 年份 | 规模 | 模态 | 语言 | 链接 |'
- en: '| Sign Language | Dicta-Sign [[76](#bib.bib76)] | 2008 | $\sim$1k | Video-Text
    | English | [Link](https://www.sign-lang.uni-hamburg.de/dicta-sign/portal/) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 手语 | Dicta-Sign [[76](#bib.bib76)] | 2008 | $\sim$1k | 视频-文本 | 英文 | [链接](https://www.sign-lang.uni-hamburg.de/dicta-sign/portal/)
    |'
- en: '| PHOENIX-Weather[[77](#bib.bib77)] | 2012 | $\sim$3k | Video-Text | Germany
    | [Link](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| PHOENIX-Weather[[77](#bib.bib77)] | 2012 | $\sim$3k | 视频-文本 | 德国 | [链接](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/)
    |'
- en: '| ASLLVD[[78](#bib.bib78)] | 2012 | $\sim$3K | Video-Text | English | [Link](https://www.bu.edu/asllrp/av/dai-asllvd.html)
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ASLLVD[[78](#bib.bib78)] | 2012 | $\sim$3K | 视频-文本 | 英文 | [链接](https://www.bu.edu/asllrp/av/dai-asllvd.html)
    |'
- en: '| SIGNUM [[79](#bib.bib79)] | 2013 | $\sim$33K | Video-Text | Germany | [Link](https://www.phonetik.uni-muenchen.de/forschung/Bas/SIGNUM/)
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| SIGNUM [[79](#bib.bib79)] | 2013 | $\sim$33K | 视频-文本 | 德国 | [链接](https://www.phonetik.uni-muenchen.de/forschung/Bas/SIGNUM/)
    |'
- en: '| DEVISIGN[[80](#bib.bib80)] | 2014 | $\sim$24k | Video-Text | Chinese | [Link](http://vipl.ict.ac.cn/homepage/ksl/data_ch.html)
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| DEVISIGN[[80](#bib.bib80)] | 2014 | $\sim$24k | 视频-文本 | 中文 | [链接](http://vipl.ict.ac.cn/homepage/ksl/data_ch.html)
    |'
- en: '| ASL-LEX 1.0[[81](#bib.bib81)] | 2017 | $\sim$1K | Video-Text | English |
    [Link](https://asl-lex.org/download.html) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| ASL-LEX 1.0[[81](#bib.bib81)] | 2017 | $\sim$1K | 视频-文本 | 英文 | [链接](https://asl-lex.org/download.html)
    |'
- en: '| PHOENIX14T[[82](#bib.bib82)] | 2018 | $\sim$68K | Video-Text | Germany |
    [Link](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/)
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| PHOENIX14T[[82](#bib.bib82)] | 2018 | $\sim$68K | 视频-文本 | 德国 | [链接](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/)
    |'
- en: '| CMLR[[83](#bib.bib83)] | 2019 | $\sim$102K | Image-Text | Chinese | [Link](https://www.vipazoo.cn/CMLR.html)
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| CMLR[[83](#bib.bib83)] | 2019 | $\sim$102K | 图像-文本 | 中文 | [链接](https://www.vipazoo.cn/CMLR.html)
    |'
- en: '| KETI[[84](#bib.bib84)] | 2019 | $\sim$15K | Video-Text | Korean | Not Available
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| KETI[[84](#bib.bib84)] | 2019 | $\sim$15K | 视频-文本 | 韩语 | 不可用 |'
- en: '| GSL[[85](#bib.bib85)] | 2020 | $\sim$3K | Video-Text | Greek | [Link](https://zenodo.org/record/3941811#.ZHb2LXZBxD8)
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| GSL[[85](#bib.bib85)] | 2020 | $\sim$3K | 视频-文本 | 希腊语 | [链接](https://zenodo.org/record/3941811#.ZHb2LXZBxD8)
    |'
- en: '| ASL-LEX 2.0[[86](#bib.bib86)] | 2021 | $\sim$ 10K | Video-Text-Depth | English
    | [Link](https://asl-lex.org/download.html) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ASL-LEX 2.0[[86](#bib.bib86)] | 2021 | $\sim$ 10K | 视频-文本-深度 | 英语 | [链接](https://asl-lex.org/download.html)
    |'
- en: '| How2sign[[87](#bib.bib87)] | 2021 | $\sim$35K | Video-Text-Skelton(2D)-Depth
    | English | [Link](https://how2sign.github.io/) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| How2sign[[87](#bib.bib87)] | 2021 | $\sim$35K | 视频-文本-骨架（2D）-深度 | 英语 | [链接](https://how2sign.github.io/)'
- en: '| Slovo[[88](#bib.bib88)] | 2023 | $\sim$20K | Video-Text | Russian | [Link](https://github.com/hukenovs/slovo)
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Slovo[[88](#bib.bib88)] | 2023 | $\sim$20K | 视频-文本 | 俄语 | [链接](https://github.com/hukenovs/slovo)
    |'
- en: '| AASL[[89](#bib.bib89)] | 2023 | $\sim$8K | Image-Text | Arabic | [Link](https://www.kaggle.com/datasets/muhammadalbrham/rgb-arabic-alphabets-sign-language-dataset)
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| AASL[[89](#bib.bib89)] | 2023 | $\sim$8K | 图像-文本 | 阿拉伯语 | [链接](https://www.kaggle.com/datasets/muhammadalbrham/rgb-arabic-alphabets-sign-language-dataset)
    |'
- en: '| ASL-27C[[83](#bib.bib83)] | 2023 | $\sim$23K | Image-Text | English | [Link](https://www.kaggle.com/datasets/ardamavi/27-class-sign-language-dataset)
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ASL-27C[[83](#bib.bib83)] | 2023 | $\sim$23K | 图像-文本 | 英语 | [链接](https://www.kaggle.com/datasets/ardamavi/27-class-sign-language-dataset)
    |'
- en: '| Cued Speech | FCS[[90](#bib.bib90)] | 2018 | $\sim$13k | Video-Text-Audio
    | French | [Link](https://zenodo.org/record/5554849) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Cued Speech | FCS[[90](#bib.bib90)] | 2018 | $\sim$13k | 视频-文本-音频 | 法语 |
    [链接](https://zenodo.org/record/5554849) |'
- en: '| BEC[[59](#bib.bib59)] | 2019 | $\sim$3k | Video-Text-Audio | English | [Link](https://zenodo.org/record/3464212)
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| BEC[[59](#bib.bib59)] | 2019 | $\sim$3k | 视频-文本-音频 | 英语 | [链接](https://zenodo.org/record/3464212)
    |'
- en: '| PCSC [[91](#bib.bib91)] | 2020 | 20 (P) | Video-Text-Audio | Polish | [Link](https://phonbank.talkbank.org/access/Clinical/PCSC.html)
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| PCSC [[91](#bib.bib91)] | 2020 | 20 (P) | 视频-文本-音频 | 波兰语 | [链接](https://phonbank.talkbank.org/access/Clinical/PCSC.html)
    |'
- en: '| CLeLfPC[[92](#bib.bib92)] | 2022 | 350 | Video-Text-Audio | French | [Link](https://www.ortolang.fr/market/corpora/clelfpc)
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| CLeLfPC[[92](#bib.bib92)] | 2022 | 350 | 视频-文本-音频 | 法语 | [链接](https://www.ortolang.fr/market/corpora/clelfpc)
    |'
- en: '| MCCS-2023[[41](#bib.bib41)] | 2023 | $\sim$132k | Video-Text-Audio-Skelton(2,3D)
    | Chinese | [Link](https://mccs-2023.github.io/) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| MCCS-2023[[41](#bib.bib41)] | 2023 | $\sim$132k | 视频-文本-音频-骨架（2D,3D） | 中文
    | [链接](https://mccs-2023.github.io/) |'
- en: '| Co-speech | Trinity[[67](#bib.bib67)] | 2018 | 224(Min) | Videos-Text-Audio-Skelton(2,3D)
    | English | [Link](https://trinityspeechgesture.scss.tcd.ie/) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Co-speech | Trinity[[67](#bib.bib67)] | 2018 | 224（分钟） | 视频-文本-音频-骨架（2D,3D）
    | 英语 | [链接](https://trinityspeechgesture.scss.tcd.ie/) |'
- en: '| TED-Gesture[[68](#bib.bib68)] | 2019 | $\sim$252k | Videos-Text-Audio-Skelton(2D)
    | English | [Link](https://github.com/youngwoo-yoon/youtube-gesture-dataset) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| TED-Gesture[[68](#bib.bib68)] | 2019 | $\sim$252k | 视频-文本-音频-骨架（2D） | 英语
    | [链接](https://github.com/youngwoo-yoon/youtube-gesture-dataset) |'
- en: '| Talking With Hands [[69](#bib.bib69)] | 2019 | 200 | Videos-Text-Audio-Skelton(2,3D)
    | English | [Link](https://github.com/facebookresearch/TalkingWithHands32M) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Talking With Hands [[69](#bib.bib69)] | 2019 | 200 | 视频-文本-音频-骨架（2D,3D） |
    英语 | [链接](https://github.com/facebookresearch/TalkingWithHands32M) |'
- en: '| Speech2Gesture[[70](#bib.bib70)] | 2019 | $\sim$60k | Videos-Text-Audio-Skelton(2D)
    | English | [Link](http://people.eecs.berkeley.edu/~shiry/speech2gesture/) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Speech2Gesture[[70](#bib.bib70)] | 2019 | $\sim$60k | 视频-文本-音频-骨架（2D） | 英语
    | [链接](http://people.eecs.berkeley.edu/~shiry/speech2gesture/) |'
- en: '| TED-Expressive[[27](#bib.bib27)] | 2022 | $\sim$252k | Videos-Text-Audio-Skelton(2,3D)
    | English | [Link](https://github.com/alvinliu0/HA2G) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| TED-Expressive[[27](#bib.bib27)] | 2022 | $\sim$252k | 视频-文本-音频-骨架（2D,3D）
    | 英语 | [链接](https://github.com/alvinliu0/HA2G) |'
- en: '| Talking Head | GRID [[93](#bib.bib93)] | 2006 | $\sim$34k | Video-Text |
    English | [Link](https://spandh.dcs.shef.ac.uk/gridcorpus/) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Talking Head | GRID [[93](#bib.bib93)] | 2006 | $\sim$34k | 视频-文本 | 英语 |
    [链接](https://spandh.dcs.shef.ac.uk/gridcorpus/) |'
- en: '| eNTERFACE [[94](#bib.bib94)] | 2006 | $\sim$1k | Video-Text-Audio | Multiple
    | [Link](http://www.enterface.net/enterface05) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| eNTERFACE [[94](#bib.bib94)] | 2006 | $\sim$1k | 视频-文本-音频 | 多种语言 | [链接](http://www.enterface.net/enterface05)
    |'
- en: '| MIRACL-VC1 [[95](#bib.bib95)] | 2014 | $\sim$3k | Video-Text-Depth | English
    | [Link](https://sites.google.com/site/achrafbenhamadou/-datasets/miracl-vc1)
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| MIRACL-VC1 [[95](#bib.bib95)] | 2014 | $\sim$3k | 视频-文本-深度 | 英语 | [链接](https://sites.google.com/site/achrafbenhamadou/-datasets/miracl-vc1)
    |'
- en: '| CREMA-D[[96](#bib.bib96)] | 2015 | $\sim$7k | Video-Text-Audio | English
    | [Link](https://github.com/CheyneyComputerScience/CREMA-D) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| CREMA-D[[96](#bib.bib96)] | 2015 | $\sim$7k | 视频-文本-音频 | 英语 | [链接](https://github.com/CheyneyComputerScience/CREMA-D)
    |'
- en: '| TCD-TIMIT [[96](#bib.bib96)] | 2015 | $\sim$7k | Video-Text-Audio | English
    | [Link](https://sigmedia.tcd.ie/TCDTIMIT/) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| TCD-TIMIT [[96](#bib.bib96)] | 2015 | $\sim$7k | 视频-文本-音频 | 英语 | [链接](https://sigmedia.tcd.ie/TCDTIMIT/)
    |'
- en: '| MODALITY [[97](#bib.bib97)] | 2015 | $\sim$6k | Video-Text-Audio | English
    | [Link](http://www.modality-corpus.org/) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| MODALITY [[97](#bib.bib97)] | 2015 | $\sim$6k | 视频-文本-音频 | 英语 | [链接](http://www.modality-corpus.org/)
    |'
- en: '| LRW [[98](#bib.bib98)] | 2016 | $\sim$539k | Video-Text | English | [Link](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| LRW [[98](#bib.bib98)] | 2016 | $\sim$539k | 视频-文本 | 英语 | [链接](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)
    |'
- en: '| MSP-IMPROV [[99](#bib.bib99)] | 2016 | $\sim$ 1K | Video-Text-Audio | English
    | [Link](https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Improv.html)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| MSP-IMPROV [[99](#bib.bib99)] | 2016 | $\sim$1K | 视频-文本-音频 | 英语 | [链接](https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Improv.html)
    |'
- en: '| ObamaSet[[100](#bib.bib100)] | 2017 | $\sim$1k | Video-Text-Audio | English
    | [Link](https://github.com/supasorn/synthesizing_obama_network_training) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ObamaSet[[100](#bib.bib100)] | 2017 | $\sim$1k | 视频-文本-音频 | 英语 | [链接](https://github.com/supasorn/synthesizing_obama_network_training)
    |'
- en: '| VoxCeleb1[[101](#bib.bib101)] | 2017 | $\sim$22k | Video-Text-Audio | English
    | [Link](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| VoxCeleb1[[101](#bib.bib101)] | 2017 | $\sim$22k | 视频-文本-音频 | 英语 | [链接](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html)
    |'
- en: '| VoxCeleb2[[102](#bib.bib102)] | 2018 | $\sim$146k | Video-Text-Audio | English
    | [Link](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| VoxCeleb2[[102](#bib.bib102)] | 2018 | $\sim$146k | 视频-文本-音频 | 英语 | [链接](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html)
    |'
- en: '| LRS2 [[103](#bib.bib103)] | 2018 | $\sim$96k | Video-Text | English | [Link](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/)
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LRS2 [[103](#bib.bib103)] | 2018 | $\sim$96k | 视频-文本 | 英语 | [链接](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/)
    |'
- en: '| LRS3-TED [[104](#bib.bib104)] | 2018 | $\sim$119k | Video-Text | English
    | [Link](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| LRS3-TED [[104](#bib.bib104)] | 2018 | $\sim$119k | 视频-文本 | 英语 | [链接](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/)
    |'
- en: '| RAVDESS [[105](#bib.bib105)] | 2018 | $\sim$1k | Video-Text-Audio | English
    | [Link](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| RAVDESS [[105](#bib.bib105)] | 2018 | $\sim$1k | 视频-文本-音频 | 英语 | [链接](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)
    |'
- en: '| MELD [[106](#bib.bib106)] | 2018 | $\sim$13k | Video-Text-Audio | English
    | [Link](https://affective-meld.github.io/) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| MELD [[106](#bib.bib106)] | 2018 | $\sim$13k | 视频-文本-音频 | 英语 | [链接](https://affective-meld.github.io/)
    |'
- en: '| AVSpeech [[107](#bib.bib107)] | 2018 | $\sim$150k | Video-Audio | Multiple
    | [Link](http://looking-to-listen.github.io/) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| AVSpeech [[107](#bib.bib107)] | 2018 | $\sim$150k | 视频-音频 | 多语言 | [链接](http://looking-to-listen.github.io/)
    |'
- en: '| VOCASET[[108](#bib.bib108)] | 2019 | 480 | Video-Text-Audio-3DFace | English
    | [Link](https://voca.is.tue.mpg.de/) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| VOCASET[[108](#bib.bib108)] | 2019 | 480 | 视频-文本-音频-3D人脸 | 英语 | [链接](https://voca.is.tue.mpg.de/)
    |'
- en: '| LRW-1000 [[109](#bib.bib109)] | 2019 | $\sim$718K | Video-Text | Chinese
    | [Link](https://vipl.ict.ac.cn/resources/databases/201810/t20181017_32714.html)
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| LRW-1000 [[109](#bib.bib109)] | 2019 | $\sim$718K | 视频-文本 | 中文 | [链接](https://vipl.ict.ac.cn/resources/databases/201810/t20181017_32714.html)
    |'
- en: '| FaceForensics++[[110](#bib.bib110)] | 2019 | $\sim$1k | Video-Text-Audio
    | English | [Link](https://github.com/ondyari/FaceForensics) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| FaceForensics++[[110](#bib.bib110)] | 2019 | $\sim$1k | 视频-文本-音频 | 英语 | [链接](https://github.com/ondyari/FaceForensics)
    |'
- en: '| MEAD[[111](#bib.bib111)] | 2020 | $\sim$281k | Video-Text-Audio | English
    | [Link](https://wywu.github.io/projects/MEAD/MEAD.html) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| MEAD[[111](#bib.bib111)] | 2020 | $\sim$281k | 视频-文本-音频 | 英语 | [链接](https://wywu.github.io/projects/MEAD/MEAD.html)
    |'
- en: '| HDTF[[112](#bib.bib112)] | 2021 | $\sim$10k | Video-Text-Audio | English
    | [Link](https://github.com/MRzzm/HDTF) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| HDTF[[112](#bib.bib112)] | 2021 | $\sim$10k | 视频-文本-音频 | 英语 | [链接](https://github.com/MRzzm/HDTF)
    |'
- en: '| AnimeCeleb [[113](#bib.bib113)] | 2022 | $\sim$2.4M | Video-Text-Audio-3DFace
    | English | [Link](https://github.com/kangyeolk/AnimeCeleb) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| AnimeCeleb [[113](#bib.bib113)] | 2022 | $\sim$2.4M | 视频-文本-音频-3D人脸 | 英语
    | [链接](https://github.com/kangyeolk/AnimeCeleb) |'
- en: '| VLRDT [[114](#bib.bib114)] | 2022 | $\sim$2k | Video-Text | Turkish | [Link](https://data.mendeley.com/datasets/4t8vs4dr4v/1)
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| VLRDT [[114](#bib.bib114)] | 2022 | $\sim$2k | 视频-文本 | 土耳其语 | [链接](https://data.mendeley.com/datasets/4t8vs4dr4v/1)
    |'
- en: '| KoEBA [[115](#bib.bib115)] | 2023 | 104(P) | Video-Text-Audio | Korea | [Link](https://github.com/deepbrainai-research/koeba)
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| KoEBA [[115](#bib.bib115)] | 2023 | 104(P) | 视频-文本-音频 | 韩国 | [链接](https://github.com/deepbrainai-research/koeba)
    |'
- en: '| Others | AV Letters [[116](#bib.bib116)] | 2002 | $\sim$19k | Video-Text
    | English | [Link](http://www.ee.surrey.ac.uk/Projects/LILiR/datasets/avletters1/index.html)
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | AV Letters [[116](#bib.bib116)] | 2002 | $\sim$19k | 视频-文本 | 英语 | [链接](http://www.ee.surrey.ac.uk/Projects/LILiR/datasets/avletters1/index.html)
    |'
- en: '| AV Digits [[117](#bib.bib117)] | 2002 | $\sim$5k | Video-Text | English |
    [Link](https://ibug-avs.eu/) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| AV Digits [[117](#bib.bib117)] | 2002 | $\sim$5k | 视频-文本 | 英语 | [链接](https://ibug-avs.eu/)
    |'
- en: '| Aoyama Gakuin [[118](#bib.bib118)] | 2017 | $\sim$1k | Videos-Text-Audio-Skelton(2D)
    | Japanese | Not Available |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Aoyama Gakuin [[118](#bib.bib118)] | 2017 | $\sim$1k | 视频-文本-音频-骨架（2D） |
    日语 | 不可用 |'
- en: '| P2PSTORY[[119](#bib.bib119)] | 2018 | $\sim$13k | Video-Text-Audio | Multiple
    | [Link](https://www.media.mit.edu/projects/p2pstory/overview/) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| P2PSTORY[[119](#bib.bib119)] | 2018 | $\sim$13k | 视频-文本-音频 | 多个 | [链接](https://www.media.mit.edu/projects/p2pstory/overview/)
    |'
- en: '| AMASS[[120](#bib.bib120)] | 2019 | $\sim$18k | video-text-Skelton(3D) | English
    | [Link](https://amass.is.tue.mpg.de/download.php) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| AMASS[[120](#bib.bib120)] | 2019 | $\sim$18k | 视频-文本-骨架（3D） | 英语 | [链接](https://amass.is.tue.mpg.de/download.php)
    |'
- en: '| BoLD[[121](#bib.bib121)] | 2020 | $\sim$10k | Video-Text-Audio-Skelton(3D)
    | English | [Link](https://cydar.ist.psu.edu/emotionchallenge/index.php) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| BoLD[[121](#bib.bib121)] | 2020 | $\sim$10k | 视频-文本-音频-骨架（3D） | 英语 | [链接](https://cydar.ist.psu.edu/emotionchallenge/index.php)
    |'
- en: '| PATS [[122](#bib.bib122)] | 2020 | $\sim$84k | Videos-Text-Audio-Skelton(2D)
    | English | [Link](https://chahuja.com/pats/) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| PATS [[122](#bib.bib122)] | 2020 | $\sim$84k | 视频-文本-音频-骨架（2D） | 英语 | [链接](https://chahuja.com/pats/)
    |'
- en: '| BABEL[[123](#bib.bib123)] | 2021 | $\sim$28k | video-text-Skelton(3D) | English
    | [Link](https://babel.is.tue.mpg.de/data.html) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| BABEL[[123](#bib.bib123)] | 2021 | $\sim$28k | 视频-文本-骨架（3D） | 英语 | [链接](https://babel.is.tue.mpg.de/data.html)
    |'
- en: '| HumanML3D[[124](#bib.bib124)] | 2022 | $\sim$15k | video-text-Skelton | English
    | [Link](https://github.com/EricGuo5513/HumanML3D) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| HumanML3D[[124](#bib.bib124)] | 2022 | $\sim$15k | 视频-文本-骨架 | 英语 | [链接](https://github.com/EricGuo5513/HumanML3D)
    |'
- en: '| BEAT[[125](#bib.bib125)] | 2023 | $\sim$3k | Video-Text-Audio-Skelton(3D)
    | Multiple | [Link](https://pantomatrix.github.io/BEAT-Dataset/index.html) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| BEAT[[125](#bib.bib125)] | 2023 | $\sim$3k | 视频-文本-音频-骨架（3D） | 多个 | [链接](https://pantomatrix.github.io/BEAT-Dataset/index.html)
    |'
- en: 2.4 Talking Head
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 说话头部
- en: TH refers to a virtual or digital representation of the human face or head,
    typically used in multimedia applications, computer graphics, and HCI. It is usually
    an animated character that appears on a screen and can simulate various facial
    expressions, head actions, and speech with synchronized lip movements [[17](#bib.bib17),
    [126](#bib.bib126), [127](#bib.bib127)]. TH aims to enhance user experiences in
    various applications, from virtual assistants to entertainment platforms, by providing
    interactive and immersive communication interfaces.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TH 指的是人脸或头部的虚拟或数字化表示，通常用于多媒体应用、计算机图形学和人机交互。它通常是一个在屏幕上出现的动画角色，可以模拟各种面部表情、头部动作和语音，并同步嘴唇运动
    [[17](#bib.bib17), [126](#bib.bib126), [127](#bib.bib127)]。TH 旨在通过提供互动和沉浸式的沟通界面，提升各种应用中的用户体验，从虚拟助手到娱乐平台。
- en: In 2003, visual text-to-speech (VTTS) that generates talking faces driven by
    a speech synthesizer has been proposed for HCI systems [[128](#bib.bib128)]. Speech
    synthesis techniques are used to convert text input into synthesized speech, allowing
    the virtual character to speak. Facial animation algorithms are employed to animate
    the virtual character’s facial movements, including lip synchronization with the
    generated speech. These algorithms analyze the phonetic information in the speech
    and map it onto corresponding facial movements. Additionally, sophisticated computer
    graphics techniques are utilized to generate realistic textures, lighting, and
    shading for the virtual character, enhancing its visual appearance[[129](#bib.bib129)].
    Previous approaches to TH generation faced many limitations and were unable to
    achieve high-quality and realistic results due to constraints like limited data
    availability and computing power.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在2003年，提出了由语音合成器驱动的生成对话面孔的视觉文本到语音（VTTS）技术，用于HCI系统[[128](#bib.bib128)]。语音合成技术用于将文本输入转换为合成语音，使虚拟角色能够说话。面部动画算法被用来动画化虚拟角色的面部动作，包括与生成的语音同步的唇部动作。这些算法分析语音中的语音信息，并将其映射到相应的面部动作上。此外，还利用复杂的计算机图形技术生成虚拟角色的逼真纹理、光照和阴影，以增强其视觉效果[[129](#bib.bib129)]。之前的TH生成方法面临许多限制，由于数据可用性和计算能力等限制，未能实现高质量和逼真的结果。
- en: TH generation needs to fuse and synchronize information from different modalities
    to ensure consistency and coherence between the animation, sound, and text of
    the character. This involves the alignment, fusion, and synchronization of each
    modal to produce a more uniform response. In recent years, DL and multi-modal
    neural networks advance the performance of TH generation from multiple perspectives
    [[19](#bib.bib19), [50](#bib.bib50)]. By using multi-modal or cross-modal techniques
    based on a large amount of data, TH generation can integrate user input from different
    sources and interact in a more natural and realistic way. This enables multi-modal
    HCI systems to better understand user intentions, generate responses accordingly,
    and provide a more immersive and personalized interactive experience.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: TH生成需要融合和同步来自不同模态的信息，以确保角色动画、声音和文本之间的一致性和连贯性。这涉及到每个模态的对齐、融合和同步，以产生更统一的响应。近年来，深度学习和多模态神经网络从多个角度提升了TH生成的性能[[19](#bib.bib19),
    [50](#bib.bib50)]。通过基于大量数据的多模态或跨模态技术，TH生成可以整合来自不同来源的用户输入，并以更自然和逼真的方式互动。这使得多模态HCI系统能够更好地理解用户意图，相应地生成响应，并提供更具沉浸感和个性化的互动体验。
- en: In Section [4.4](#S4.SS4 "4.4 Talking Head Recognition ‣ 4 Automatic Body Language
    Recognition ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation"), we give a brief review of TH recognition. In Section [5.4](#S5.SS4
    "5.4 Talking Head Generation ‣ 5 Automatic Body Language Generation ‣ A Survey
    on Deep Multi-modal Learning for Body Language Recognition and Generation"), we
    explore the applications and advancements in deep multi-modal learning for TH
    generation. We discuss the challenges associated with creating realistic and expressive
    virtual characters, including the synthesis of natural-sounding speech and the
    accurate representation of facial expressions. We review the SOTA techniques and
    highlight the potential future developments in this field, aiming to improve the
    realism and interactivity of THs in various applications.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在[4.4](#S4.SS4 "4.4 Talking Head Recognition ‣ 4 Automatic Body Language Recognition
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")节中，我们简要回顾了TH识别。在[5.4](#S5.SS4
    "5.4 Talking Head Generation ‣ 5 Automatic Body Language Generation ‣ A Survey
    on Deep Multi-modal Learning for Body Language Recognition and Generation")节中，我们探讨了深度多模态学习在TH生成中的应用和进展。我们讨论了创建逼真和富有表现力的虚拟角色所面临的挑战，包括自然语音的合成和面部表情的准确表现。我们回顾了SOTA技术，并强调了该领域潜在的未来发展，旨在提升TH在各种应用中的逼真性和互动性。
- en: 3 Body Language Dataset and Evaluation Metrics
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 身体语言数据集与评价指标
- en: 3.1 Body Language Datasets
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 身体语言数据集
- en: Datasets have played a crucial role in the entire history of BL research, serving
    as a common foundation not only for measuring and comparing the performance of
    competing algorithms but also for driving the field toward increasingly complex
    and challenging problems. Particularly in recent years, DL techniques have brought
    significant success to BL research, with a substantial amount of annotated data
    being key to this success. The availability of large-scale image collections through
    the internet has made it possible to construct comprehensive datasets. Additionally,
    the availability of multi-modal data has provided richer information for related
    tasks, opening up new possibilities for future BL recognition and generation research.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在整个BL研究历史中发挥了关键作用，不仅为衡量和比较竞争算法的性能提供了共同的基础，还推动了该领域朝着越来越复杂和具有挑战性的问题发展。尤其是在最近几年，DL技术为BL研究带来了显著成功，而大量标注数据是这一成功的关键。通过互联网获取的大规模图像集合使得构建全面的数据集成为可能。此外，多模态数据的可用性为相关任务提供了更丰富的信息，为未来BL识别和生成研究开辟了新可能。
- en: 'In this section, we have collected and presented relevant datasets pertaining
    to BL tasks. As shown in Table [II](#S2.T2 "TABLE II ‣ 2.3 Co-speech ‣ 2 Typical
    Body Language ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation"), we have categorized them into five types based on data format
    and task purposes: CS, SL, CoS, TH, and others (Here “others” means these datasets
    are multi-modal BL datasets but are not designed for these four tasks). We have
    introduced the relevant information about these datasets, including publication
    year, dataset scale, available modalities, and the languages used in the datasets.
    Moreover, we have provided official links to these datasets to facilitate easier
    access for researchers. Please note that we measure the dataset scale based on
    the number of video clips/sequences. For datasets that do not provide these numbers,
    we provide the duration of the videos in minutes (represented as ”Min”) or the
    number of performers (represented as ”P”). Some examples of BL datasets are shown
    in Figure [6](#S3.F6 "Figure 6 ‣ 3.1 Body Language Datasets ‣ 3 Body Language
    Dataset and Evaluation Metrics ‣ A Survey on Deep Multi-modal Learning for Body
    Language Recognition and Generation") for more details.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们收集并展示了与BL任务相关的数据集。如表[II](#S2.T2 "TABLE II ‣ 2.3 Co-speech ‣ 2 Typical
    Body Language ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation")所示，我们根据数据格式和任务目的将它们分类为五种类型：CS、SL、CoS、TH和其他（这里的“其他”指的是这些数据集是多模态BL数据集，但并未专门针对这四个任务设计）。我们介绍了这些数据集的相关信息，包括出版年份、数据集规模、可用模态以及数据集使用的语言。此外，我们提供了这些数据集的官方链接，以便研究人员更方便地访问。请注意，我们根据视频片段/序列的数量来衡量数据集的规模。对于未提供这些数字的数据集，我们提供视频的时长（以“Min”表示）或表演者的数量（以“P”表示）。图[6](#S3.F6
    "Figure 6 ‣ 3.1 Body Language Datasets ‣ 3 Body Language Dataset and Evaluation
    Metrics ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation")中展示了一些BL数据集的示例以供参考。
- en: '![Refer to caption](img/f3eae72b0580e0b909ebb3292e3499d8.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f3eae72b0580e0b909ebb3292e3499d8.png)'
- en: 'Figure 6: Some examples of BL datasets.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：一些BL数据集的示例。
- en: We present the distribution of dataset languages in Figure [7](#S3.F7 "Figure
    7 ‣ 3.1 Body Language Datasets ‣ 3 Body Language Dataset and Evaluation Metrics
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation").
    The chart shows the related datasets are primarily English datasets, but it also
    includes datasets in other languages like English datasets, Chinese datasets,
    and German datasets. This illustrates that current BL research is predominantly
    focused on English, but there is also growing importance placed on cross-cultural
    and multilingual datasets. Another problem is the difference in the format and
    standards of BL datasets. Different datasets may have varying storage formats,
    recording requirements, and model standards.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[7](#S3.F7 "Figure 7 ‣ 3.1 Body Language Datasets ‣ 3 Body Language Dataset
    and Evaluation Metrics ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation")中展示了数据集语言的分布。图表显示相关数据集主要是英语数据集，但也包括其他语言的数据集，如中文数据集和德语数据集。这表明当前的BL研究主要集中在英语上，但也越来越重视跨文化和多语言数据集。另一个问题是BL数据集格式和标准的差异。不同数据集可能具有不同的存储格式、记录要求和模型标准。
- en: '![Refer to caption](img/56ca6b7321fed85c539d60f50b100934.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/56ca6b7321fed85c539d60f50b100934.png)'
- en: 'Figure 7: The Distribution of language used in BL datasets.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：BL 数据集中的语言分布。
- en: 3.2 BL Generation Metrics
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 BL 生成指标
- en: In order to evaluate the performance of BL gesture generation methods, we summarize
    the main gesture generation metrics and show these generation metrics, and corresponding
    calculation formulas in Table [VI](#S4.T6 "TABLE VI ‣ 4.4 Talking Head Recognition
    ‣ 4 Automatic Body Language Recognition ‣ A Survey on Deep Multi-modal Learning
    for Body Language Recognition and Generation"). A total of seven metrics are introduced
    for evaluation, namely PCK[[130](#bib.bib130)], FGD[[131](#bib.bib131)], MAE[[132](#bib.bib132)],
    STD[[132](#bib.bib132)], PMB[[25](#bib.bib25)], MAJE[[131](#bib.bib131)], and
    MAD[[131](#bib.bib131)]. Percentage of Correct Keypoints (PCK) assesses the accuracy
    of generated motion by comparing keypoints with actual motion. A predicted keypoint
    is considered correct if it falls within a specified threshold of the actual keypoints.
    Mean Absolute Error (MAE) quantifies the average difference between standardized
    coordinate values of generated and actual keypoints. Standard Deviation (STD)
    represents the variability or distribution of keypoints from their mean position
    after standardization. Fréchet Gesture Distance (FGD) measures dissimilarity between
    the distributions of latent features in generated and ground truth gestures, incorporating
    both location and spread. Percentage of Matched Beats (PMB) considers a motion
    beat matched if its temporal distance to an audio beat is below a threshold. Mean
    Absolute Joint Error (MAJE) calculates average errors between generated and ground
    truth joint positions across all time steps and joints. Mean Absolute Difference
    (MAD) computes average differences in joint accelerations, considering magnitude
    and direction. These criteria provide comprehensive insights into the accuracy,
    similarity, and alignment between generated and ground truth motion data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 BL 姿势生成方法的性能，我们总结了主要的姿势生成指标，并在表 [VI](#S4.T6 "表 VI ‣ 4.4 说话头识别 ‣ 4 自动身体语言识别
    ‣ 深度多模态学习在身体语言识别和生成中的应用概述") 中展示了这些生成指标及其相应的计算公式。总共介绍了七个指标用于评估，即 PCK[[130](#bib.bib130)]、FGD[[131](#bib.bib131)]、MAE[[132](#bib.bib132)]、STD[[132](#bib.bib132)]、PMB[[25](#bib.bib25)]、MAJE[[131](#bib.bib131)]
    和 MAD[[131](#bib.bib131)]。正确关键点百分比 (PCK) 通过将关键点与实际动作进行比较来评估生成运动的准确性。如果预测的关键点落在实际关键点的指定阈值内，则认为是正确的。均方绝对误差
    (MAE) 量化了生成和实际关键点的标准化坐标值之间的平均差异。标准差 (STD) 表示在标准化后关键点从其平均位置的变异性或分布。Fréchet 手势距离
    (FGD) 衡量生成手势和真实手势的潜在特征分布之间的差异，包括位置和分布。匹配节拍百分比 (PMB) 认为如果运动节拍与音频节拍的时间距离低于阈值，则该节拍为匹配。均方绝对关节误差
    (MAJE) 计算所有时间步骤和关节中生成的和真实的关节位置之间的平均误差。均方绝对差异 (MAD) 计算关节加速度的平均差异，考虑幅度和方向。这些标准提供了关于生成运动数据和真实运动数据之间准确性、相似性和对齐的全面见解。
- en: The TH generation results can be evaluated quantitatively from multiple perspectives.
    Evaluation metrics include identity-preserving metrics, audio-visual synchronization
    metrics, image quality-preserving metrics, expression metrics, and eye-blinking
    metrics.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: TH 生成结果可以从多个角度进行定量评估。评估指标包括身份保持指标、视听同步指标、图像质量保持指标、表情指标和眼睛眨动指标。
- en: '![Refer to caption](img/82c926449e7a8efe6eba8df636a5ee83.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/82c926449e7a8efe6eba8df636a5ee83.png)'
- en: 'Figure 8: The milestones of Datasets and Methods for BL recognition.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：BL 识别的数据集和方法的里程碑。
- en: 4 Automatic Body Language Recognition
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 自动身体语言识别
- en: Here, we will introduce the recognition for the four BL variants, with a particular
    focus on the application expansion and innovation of multi-modal learning. In
    Figure [8](#S3.F8 "Figure 8 ‣ 3.2 BL Generation Metrics ‣ 3 Body Language Dataset
    and Evaluation Metrics ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), we present a summary of some representative works
    for BL recognition.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍四种 BL 变体的识别，特别关注多模态学习的应用扩展和创新。在图 [8](#S3.F8 "图 8 ‣ 3.2 BL 生成指标 ‣ 3
    身体语言数据集和评估指标 ‣ 深度多模态学习在身体语言识别和生成中的应用概述") 中，我们展示了一些代表性 BL 识别工作的总结。
- en: '![Refer to caption](img/6bf7f060f190f74e9452554bf6599f71.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6bf7f060f190f74e9452554bf6599f71.png)'
- en: 'Figure 9: The comprehensive methods of SL recognition.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：SL 识别的综合方法。
- en: 4.1 Sign Language Recognition
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 手语识别
- en: 'SLR aims to utilize a classifier to recognize SL glosses from video streams.
    It can be classified into two types in general according to the content of the
    SL: continuous SL recognition and isolated SL recognition. In this paper, we focus
    on continuous SL recognition (CSLR), in which the feature encoder module first
    extracts semantic representations from the sign video, and then the sequential
    module performs the mapping from the extracted semantics to the text sequence.
    In addition, some training strategies have been investigated for sufficient training.
    The comprehensive approaches are presented in Figure [9](#S4.F9 "Figure 9 ‣ 4
    Automatic Body Language Recognition ‣ A Survey on Deep Multi-modal Learning for
    Body Language Recognition and Generation").'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: SLR旨在利用分类器从视频流中识别手语符号。根据手语内容的不同，SLR一般可以分为两类：连续手语识别和孤立手语识别。本文重点关注连续手语识别（CSLR），在这种方法中，特征编码器模块首先从手语视频中提取语义表示，然后序列模块将提取的语义映射到文本序列。此外，还研究了一些训练策略以确保充分的训练。全面的方法展示在图[9](#S4.F9
    "Figure 9 ‣ 4 Automatic Body Language Recognition ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation")中。
- en: Feature Encoder. Since the hand acts a dominant role in the expression of SL,
    it has evolved over the past three decades, and we can divide these methods into
    the two following types.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 特征编码器。由于手部在手语表达中发挥着主导作用，它在过去三十年中不断发展，我们可以将这些方法分为以下两类。
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Handcraft-based Method. In the early research, handcrafted features are used
    to extract the hand motion, shape and orientation, such as HOG[[77](#bib.bib77),
    [133](#bib.bib133)], Grassmann covariance matrix (GCM)[[134](#bib.bib134)] and
    SIFT[[135](#bib.bib135)]. However, these methods require manual feature extraction
    and cannot directly be applied to different gestures, which means that different
    gestures necessitate distinct feature extraction approaches, resulting in a substantial
    amount of work.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于手工特征的方法。在早期研究中，手工特征被用来提取手部运动、形状和方向，例如HOG[[77](#bib.bib77), [133](#bib.bib133)]、Grassmann协方差矩阵（GCM）[[134](#bib.bib134)]和SIFT[[135](#bib.bib135)]。然而，这些方法需要手动特征提取，不能直接应用于不同的手势，这意味着不同的手势需要不同的特征提取方法，从而导致大量的工作。
- en: •
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CNN-based Method. With the development of DL, CNNs[[136](#bib.bib136), [137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139)] generally replace the handcraft-based
    methods, becoming the most powerful feature extractor for SLR. Many researchers
    try to explore the reasonable CNN-based architecture to directly extract discriminative
    visual features from the video sequence. Specifically, exist works used 2D-CNN-TCN[[140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142), [44](#bib.bib44)] and 3D-CNN[[140](#bib.bib140),
    [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148)] as the backbone to extract spatial-temporal
    discriminative cues. For instance, IAN[[143](#bib.bib143)] utilizes 3D-ResNet[[138](#bib.bib138)]
    for visual representation. DNF[[149](#bib.bib149)] subtly designs 2D-CNN with
    the 1D temporal convolution, which has become one of the mainstream baseline methods.
    Although CNN-based methods can effectively capture spatial features in gesture
    images, they are limited in handling the temporal dynamics of gestures directly,
    and 3D-CNN-based methods involve significant computational overhead.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于CNN的方法。随着深度学习的发展，CNNs[[136](#bib.bib136), [137](#bib.bib137), [138](#bib.bib138),
    [139](#bib.bib139)]通常取代了基于手工特征的方法，成为SLR最强大的特征提取器。许多研究者试图探索合理的基于CNN的架构，以直接从视频序列中提取判别性视觉特征。具体而言，现有的工作使用了2D-CNN-TCN[[140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142), [44](#bib.bib44)]和3D-CNN[[140](#bib.bib140),
    [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148)]作为骨干来提取时空判别线索。例如，IAN[[143](#bib.bib143)]利用3D-ResNet[[138](#bib.bib138)]进行视觉表示。DNF[[149](#bib.bib149)]巧妙设计了结合1D时间卷积的2D-CNN，这已经成为主流的基线方法之一。尽管基于CNN的方法能够有效捕捉手势图像中的空间特征，但它们在处理手势的时间动态方面有限，并且3D-CNN的方法涉及显著的计算开销。
- en: Sequential Module. There exist three representative approaches[[150](#bib.bib150),
    [151](#bib.bib151)] for CSLR. In the early research, HMM[[152](#bib.bib152), [153](#bib.bib153),
    [72](#bib.bib72), [154](#bib.bib154)] is used to learn the correspondence between
    the visual representation and sign gloss sequence. However, gesture actions of
    SLR often have long-term dependencies, and HMM struggles to capture such complex
    sequential patterns. Additionally, HMM does not consider the alignment between
    input and output modalities. To this end, the RNN-based methods with CTC loss[[155](#bib.bib155),
    [156](#bib.bib156), [141](#bib.bib141), [143](#bib.bib143), [157](#bib.bib157)]
    are developed for CSLR to replace the HMM model, which improves the model’s ability
    to handle data with incomplete alignments, but the ability to model global information
    is still limited. Therefore, to better understand the semantic relationship of
    the entire sign language sequence, encoder-encoder[[22](#bib.bib22), [158](#bib.bib158),
    [159](#bib.bib159)] has become a commonly used sequential framework. For instance,
    Guo et al.[[22](#bib.bib22)] utilizes the encoder-decoder framework with hierarchical
    deep recurrent fusion to merge cues from RGB and skeleton modalities.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序模块。对于 CSLR，存在三种代表性的方法[[150](#bib.bib150), [151](#bib.bib151)]。早期研究中，HMM[[152](#bib.bib152),
    [153](#bib.bib153), [72](#bib.bib72), [154](#bib.bib154)] 被用来学习视觉表示与手势词汇序列之间的对应关系。然而，SLR
    的手势动作通常具有长期依赖性，而 HMM 很难捕捉这种复杂的序列模式。此外，HMM 不考虑输入和输出模态之间的对齐。因此，基于 RNN 的方法与 CTC 损失[[155](#bib.bib155),
    [156](#bib.bib156), [141](#bib.bib141), [143](#bib.bib143), [157](#bib.bib157)]
    被开发出来替代 HMM 模型，这提高了模型处理不完全对齐数据的能力，但建模全局信息的能力仍然有限。因此，为了更好地理解整个手语序列的语义关系，编码器-编码器[[22](#bib.bib22),
    [158](#bib.bib158), [159](#bib.bib159)] 已成为一种常用的顺序框架。例如，Guo 等人[[22](#bib.bib22)]
    利用具有层次深度递归融合的编码器-解码器框架来融合来自 RGB 和骨架模态的线索。
- en: Training Strategy. For sufficient training, some optimization strategies are
    widely used, with the most prominent being CTC[[160](#bib.bib160), [156](#bib.bib156),
    [161](#bib.bib161)] and Iterative Training[[141](#bib.bib141), [142](#bib.bib142),
    [143](#bib.bib143), [161](#bib.bib161)] strategies. On top of these two strategies,
    Pu et al. [[142](#bib.bib142)] introduce a cross-modality constraint called CMA
    to aid the training. Hao et al. [[160](#bib.bib160)] propose a three-stage optimization
    approach, which improves the recognition performance but it is time-consuming.
    Recently, Min et al. [[156](#bib.bib156)] further present two auxiliary constraints
    over the frame-level probability distributions, making the entire model end-to-end
    trainable.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 训练策略。为了充分训练，广泛使用一些优化策略，其中最显著的是 CTC[[160](#bib.bib160), [156](#bib.bib156), [161](#bib.bib161)]
    和 迭代训练[[141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143), [161](#bib.bib161)]
    策略。在这两种策略的基础上，Pu 等人[[142](#bib.bib142)] 引入了一种称为 CMA 的跨模态约束来辅助训练。Hao 等人[[160](#bib.bib160)]
    提出了一种三阶段优化方法，这提高了识别性能，但耗时较长。最近，Min 等人[[156](#bib.bib156)] 进一步提出了两种辅助约束，以帧级概率分布为基础，使整个模型可端到端训练。
- en: 'TABLE III: The timeline of some representative works for BL recognition.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：BL 识别的一些代表性工作的时间线。
- en: '| Type | Year | Ref | Feature Extraction | Sequence Model | Learning Paradigm
    | Dataset |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 年份 | 参考 | 特征提取 | 序列模型 | 学习范式 | 数据集 |'
- en: '| SL | 2019 | Pei et al. [[162](#bib.bib162)] | 3D-ResNet | BGRU | CTC | Phoenix-2014
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| SL | 2019 | Pei et al. [[162](#bib.bib162)] | 3D-ResNet | BGRU | CTC | Phoenix-2014
    |'
- en: '|  | 2019 | Pei et al. [[163](#bib.bib163)] | 3D-ResNet | Transformer | Reinforcement
    Learning | Phoenix-2014 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | Pei et al. [[163](#bib.bib163)] | 3D-ResNet | Transformer | 强化学习
    | Phoenix-2014 |'
- en: '|  | 2019 | Cui et al. [[141](#bib.bib141)] | CNN | RNN | Iterative Training
    | Phoenix-2014 and SIGNUM |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | Cui et al. [[141](#bib.bib141)] | CNN | RNN | 迭代训练 | Phoenix-2014
    和 SIGNUM |'
- en: '|  | 2020 | Niu et al. [[164](#bib.bib164)] | CNN | Transformer | CTC | Phoenix-2014
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | Niu et al. [[164](#bib.bib164)] | CNN | Transformer | CTC | Phoenix-2014
    |'
- en: '|  | 2020 | SAFI [[151](#bib.bib151)] | 2D-CNN *plus* 1D-CNN | SAN | ACE *plus*
    CTC | Phoenix-weather and SIGNUM |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | SAFI [[151](#bib.bib151)] | 2D-CNN *加* 1D-CNN | SAN | ACE *加* CTC
    | Phoenix-weather 和 SIGNUM |'
- en: '|  | 2021 | Koishybay et al. [[165](#bib.bib165)] | 2D-CNN *plus* 1D-CNN |
    RNN | Iterative GR *plus* CTC | Phoenix-weather and SIGNUM |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | Koishybay et al. [[165](#bib.bib165)] | 2D-CNN *加* 1D-CNN | RNN
    | 迭代 GR *加* CTC | Phoenix-weather 和 SIGNUM |'
- en: '|  | 2021 | SLRGAN [[166](#bib.bib166)] | CNN | BiLSTM | GAN | Phoenix-weather,
    CSL and GSL |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | SLRGAN [[166](#bib.bib166)] | CNN | BiLSTM | GAN | Phoenix-weather,
    CSL 和 GSL |'
- en: '|  | 2022 | Chen et al. [[167](#bib.bib167)] | S3D | BLC | CTC *plus* Self-distillation
    | Phoenix-2014 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | Chen et al. [[167](#bib.bib167)] | S3D | BLC | CTC *加* 自蒸馏 | Phoenix-2014
    |'
- en: '|  | 2022 | Zhou et al. [[161](#bib.bib161)] | SMC | BiLSTM *plus* SA-LSTM
    | CTC *plus* Keypoint Regression | PHOENIX-2014, CSL and PHOENIX-2014-T |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | Zhou et al. [[161](#bib.bib161)] | SMC | BiLSTM *加* SA-LSTM | CTC
    *加* 关键点回归 | PHOENIX-2014、CSL 和 PHOENIX-2014-T |'
- en: '|  | 2023 | Hu et al. [[168](#bib.bib168)] | 2D-CNN | 1D-CNN *plus* BiLSTM
    | SSTM *plus* TSEM | PHOENIX-2014, PHOENIX-2014-T, CSL and CSL-Daily |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | 2023 | Hu et al. [[168](#bib.bib168)] | 2D-CNN | 1D-CNN *加* BiLSTM | SSTM
    *加* TSEM | PHOENIX-2014、PHOENIX-2014-T、CSL 和 CSL-Daily |'
- en: '|  | 2023 | Zheng et al. [[169](#bib.bib169)] | CNN | VAE | CTC *plus* Contrastive
    Alignment Loss | PHOENIX-2014 and PHOENIX-2014-T |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 2023 | Zheng et al. [[169](#bib.bib169)] | CNN | VAE | CTC *加* 对比对齐损失
    | PHOENIX-2014 和 PHOENIX-2014-T |'
- en: '| CS | 2018 | Liu et al. [[90](#bib.bib90)] | CNN | HMM | - | French CS |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| CS | 2018 | Liu et al. [[90](#bib.bib90)] | CNN | HMM | - | 法语手势识别 |'
- en: '|  | 2021 | Papadimitriou et al. [[170](#bib.bib170)] | 2D-CNN *plus* 3D-CNN
    | Attention-based CNN | - | French and British English CS |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | Papadimitriou et al. [[170](#bib.bib170)] | 2D-CNN *加* 3D-CNN |
    基于注意力的 CNN | - | 法语和英式英语手势识别 |'
- en: '|  | 2021 | Liu et al. [[40](#bib.bib40)] | CNN | MSHMM | HPM | French and
    British English CS |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | Liu et al. [[40](#bib.bib40)] | CNN | MSHMM | HPM | 法语和英式英语手势识别
    |'
- en: '|  | 2021 | Wang et al. [[58](#bib.bib58)] | CNN *plus* ANN | BiLSTM *plus*
    FC | Cross-Modal Knowledge Distillation | French and British English CS |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | Wang et al. [[58](#bib.bib58)] | CNN *加* ANN | BiLSTM *加* FC |
    跨模态知识蒸馏 | 法语和英式英语手势识别 |'
- en: '|  | 2022 | Sankar et al. [[171](#bib.bib171)] | Bi-GRU | Bi-GRU | CTC | CSF18
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | Sankar et al. [[171](#bib.bib171)] | Bi-GRU | Bi-GRU | CTC | CSF18
    |'
- en: '|  | 2023 | Liu et al. [[41](#bib.bib41)] | ResNet-18 | Transformer | Cross-Modal
    Mutual Learning | Chinese, French, and British English CS |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | 2023 | Liu et al. [[41](#bib.bib41)] | ResNet-18 | Transformer | 跨模态互学习
    | 中文、法语和英式英语手势识别 |'
- en: 'TABLE IV: The timeline of SL generation works.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：手势生成工作的时间表。
- en: '| Type | Year | Ref | Input Modality | Framework | Dataset | Description |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 年份 | 参考文献 | 输入模态 | 框架 | 数据集 | 描述 |'
- en: '| SL | 2011 | kippet et al. [[172](#bib.bib172)] | RCB video | EMBR | ViSiCAST
    | A gloss-centric tool is proposed to enable the comparison of avatars with human
    signers. But it is necessary to incorporate non-manual features. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| SL | 2011 | kippet et al. [[172](#bib.bib172)] | RCB 视频 | EMBR | ViSiCAST
    | 提出了一个以手势为中心的工具，以实现与人类手势者的对比。但需要结合非手动特征。 |'
- en: '|  | 2016 | John et al. [[173](#bib.bib173)] | RGB video | Segmental framework
    | Own dataset | This approach achieves automatic realism in generated images with
    low complexity, but it requires positioning the shoulder and torso. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | John et al. [[173](#bib.bib173)] | RGB 视频 | 分段框架 | 自有数据集 | 该方法以低复杂度实现了生成图像的自动现实感，但需要对肩部和躯干进行定位。
    |'
- en: '|  | 2016 | Sign3D[[174](#bib.bib174)] | RGB video | Heterogeneous Database
    | Own dataset | This approach guarantees sign avatars that are easily understood
    and widely accepted by viewers, but it is restricted to a limited set of sign
    phrases. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | Sign3D[[174](#bib.bib174)] | RGB 视频 | 异质数据库 | 自有数据集 | 该方法保证了手势虚拟形象容易理解且被观众广泛接受，但仅限于有限的手势短语。
    |'
- en: '|  | 2018 | HLSTM[[22](#bib.bib22)] | RCB video | LSTM | Own dataset | This
    approach shows robustness in effectively aligning the word order with visual content
    in sentences. Nevertheless, a limitation arises when generalizing it to new datasets.
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | HLSTM[[22](#bib.bib22)] | RCB 视频 | LSTM | 自有数据集 | 该方法在有效对齐句子的词序和视觉内容方面表现出鲁棒性。然而，当将其推广到新数据集时，会出现限制。
    |'
- en: '|  | 2020 | Text2Sign[[21](#bib.bib21)] | Text | Transformer | PHOENIX14T |
    It demonstrates robustness in handling the dynamic length of the output sequence.
    However, It did not incorporate nonmanual information. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | Text2Sign[[21](#bib.bib21)] | 文本 | Transformer | PHOENIX14T | 证明了在处理输出序列动态长度方面的鲁棒性。然而，未结合非手动信息。
    |'
- en: '|  | 2020 | Zelinka et al. [[175](#bib.bib175)] | Text | CNN | Crech news |
    This method is robust to missing part, but face expression is not included. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | Zelinka et al. [[175](#bib.bib175)] | 文本 | CNN | Crech 新闻 | 该方法对缺失部分具有鲁棒性，但未包括面部表情。
    |'
- en: '|  | 2020 | ESN[[23](#bib.bib23)] | Text | GAN | PHOENIX14T | It shows Robustness
    to non-manual feature generation. But the genrated signs are not realistic . |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | ESN[[23](#bib.bib23)] | 文本 | GAN | PHOENIX14T | 显示了对非手动特征生成的鲁棒性。但生成的手势不够真实。
    |'
- en: '|  | 2020 | Necati et al. [[176](#bib.bib176)] | Text | Transformers | PHOENIX14T
    | It does not need the gloss information, but the model is complex |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | Necati et al. [[176](#bib.bib176)] | 文本 | Transformers | PHOENIX14T
    | 不需要手势信息，但模型复杂 |'
- en: '|  | 2020 | Saunders et al. [[177](#bib.bib177)] | Text | GAN | PHOENIX14T
    | Robust to manual feature generation. The generated signs are not realistic.
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | Saunders et al. [[177](#bib.bib177)] | 文本 | GAN | PHOENIX14T |
    对手动特征生成具有鲁棒性。生成的手势不够真实。 |'
- en: '|  | 2022 | DSM. [[178](#bib.bib178)] | Gloss | Transformer | PHOENIX14T |
    This work improves the prosody in generated Sign Languages by modeling intensification
    in a data-driven manner. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | DSM. [[178](#bib.bib178)] | 词汇 | Transformer | PHOENIX14T | 本研究通过数据驱动的方式对生成的手语进行韵律改进。
    |'
- en: '|  | 2022 | SignGAN. [[179](#bib.bib179)] | Text | FS-Net | meineDGS | It tackles
    large-scale SLP by learning to co-articulate between dictionary signs and improves
    the temporal alignment of interpolated dictionary signs to continuous signing
    sequences |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | SignGAN. [[179](#bib.bib179)] | 文本 | FS-Net | meineDGS | 该方法通过学习词典手势之间的共同发音来处理大规模的手语翻译问题，并改进了插值词典手势与连续手语序列的时间对齐。
    |'
- en: '|  | 2023 | PoseVQ-Diffusion. [[180](#bib.bib180)] | Gloss | CodeUnet | PHOENIX14T
    | It proposes a vector quantized diffusion method for conditional pose sequences
    generation and develops a novel sequential k-nearest-neighbors method to predict
    the variable lengths of pose sequences for corresponding gloss sequences |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | 2023 | PoseVQ-Diffusion. [[180](#bib.bib180)] | 词汇 | CodeUnet | PHOENIX14T
    | 提出了一个条件姿态序列生成的向量量化扩散方法，并开发了一种新颖的顺序k最近邻方法，用于预测对应词汇序列的姿态序列的可变长度。 |'
- en: 'TABLE V: The timeline of Co-speech and Cued Speech generation works.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: Co-speech 和 Cued Speech 生成工作的时间线。'
- en: '| Type | Year | Ref | Input Modality | Framework | Dataset | Description |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 年份 | 参考 | 输入模态 | 框架 | 数据集 | 描述 |'
- en: '| CoS | 2015 | DCNF[[181](#bib.bib181)] | Text | FC network | DIAC | This work
    integrated speech text, prosody, and part-of-speech tags to generate co-verbal
    gestures using a combination of FC networks and a Conditional Random Field (CRF).
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| CoS | 2015 | DCNF[[181](#bib.bib181)] | 文本 | FC网络 | DIAC | 本研究结合了语音文本、韵律和词性标签，使用FC网络和条件随机场（CRF）的组合生成共同发言的手势。
    |'
- en: '|  | 2019 | S2G[[70](#bib.bib70)] | RGB video | CNN | S2G | This work presents
    a method for generating gestures with audio speech, utilizing cross-modal translation
    and training on unlabeled videos. But it relies on noisy pseudo ground truth for
    training |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | S2G[[70](#bib.bib70)] | RGB视频 | CNN | S2G | 本研究提出了一种生成与音频语音匹配手势的方法，利用跨模态翻译和对未标记视频的训练。但它依赖于嘈杂的伪真实数据进行训练。
    |'
- en: '|  | 2020 | StyleGestures [[182](#bib.bib182)] | RCB video | LSTM | Trinity
    | It achieves natural variations without manual annotation and allows control
    over gesture style while maintaining perceived naturalness. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | StyleGestures [[182](#bib.bib182)] | RCB视频 | LSTM | Trinity | 实现了自然的变异而无需手动注释，并允许控制手势风格，同时保持感知的自然性。
    |'
- en: '|  | 2021 | A2G[[183](#bib.bib183)] | Text | CVAE | Trinity | This work employed
    a CVAE to generate diverse gestures from speech input and involved a one-to-many
    mapping of speech-to-gesture. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | A2G[[183](#bib.bib183)] | 文本 | CVAE | Trinity | 本研究使用CVAE从语音输入生成多样化手势，并涉及到语音到手势的一对多映射。
    |'
- en: '|  | 2021 | Text2Gestures[[184](#bib.bib184)] | Text | Transformer | MPI-EBEDB
    | Their approach employed Transformer-based encoders and decoders to generate
    sequential joint positions based on the text and previous pose. |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | Text2Gestures[[184](#bib.bib184)] | 文本 | Transformer | MPI-EBEDB
    | 他们的方法使用基于Transformer的编码器和解码器生成基于文本和先前姿态的顺序关节位置。 |'
- en: '|  | 2022 | ZeroEGGS[[185](#bib.bib185)] | Text | Variational Framework | Own
    dataset | A VAE-based framework is utilized to generate style-controllable CoS
    gestures and allowed for the generation of stylized gestures by conditioning on
    a zero-shot motion example |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | ZeroEGGS[[185](#bib.bib185)] | 文本 | 变分框架 | 自有数据集 | 使用基于VAE的框架生成可控风格的CoS手势，并通过对零样本运动示例的条件生成风格化手势。
    |'
- en: '|  | 2022 | DiffGAN[[24](#bib.bib24)] | Text | Diffusion Model | PATS | An
    adversarial domain-adaptation approach is proposed to personalize the gestures
    of a speaker |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | DiffGAN[[24](#bib.bib24)] | 文本 | 扩散模型 | PATS | 提出了一个对抗领域适应的方法，用于个性化演讲者的手势。'
- en: '|  | 2022 | RG[[25](#bib.bib25)] | Trinity and TED | QVAE | PHOENIX14T | This
    work introduces a novel CoS gesture synthesis method that effectively captures
    both rhythm and semantics. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | 2022 | RG[[25](#bib.bib25)] | Trinity 和 TED | QVAE | PHOENIX14T | 本研究介绍了一种新颖的CoS手势合成方法，有效捕捉了节奏和语义。
    |'
- en: '| CS | 1998 | Paul et al. [[28](#bib.bib28)] | Text | Template | Own dataset
    | Relying on manually selected keywords, low-context sentences, and pre-defined
    gesture templates. Its limitations included constrained expressiveness and increased
    manual effort. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| CS | 1998 | Paul et al. [[28](#bib.bib28)] | 文本 | 模板 | 自有数据集 | 依赖于手动选择的关键词、低语境句子和预定义的手势模板。其局限性包括表达能力受限和增加的手动工作量。|'
- en: '|  | 2008 | Gérard et al. [[29](#bib.bib29)] | RGB video | Template | Own dataset
    | A post-processing algorithm was introduced to fine-tune synthesized hand gestures
    by addressing rotation, translation, and adaptation to new images. However, it
    relies on prior knowledge for adapting to new images. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | 2008 | Gérard et al. [[29](#bib.bib29)] | RGB视频 | 模板 | 自有数据集 | 引入了一种后处理算法，通过解决旋转、平移和适应新图像来微调合成的手势。然而，它依赖于先验知识来适应新图像。|'
- en: 4.2 Cued Speech Recognition
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 提示语音识别
- en: Automatic lip reading is a crucial component of ACSR. Therefore, we will first
    introduce the research progress in automatic lip-reading and then review ACSR.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 自动唇读是自动提示语音识别的关键组成部分。因此，我们将首先介绍自动唇读的研究进展，然后回顾自动提示语音识别。
- en: 4.2.1 Automatic Lip Reading
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 自动唇读
- en: Advances in DL have led to a promising performance in lip-reading methods. Generally,
    DL-based lip-reading methods consist of two main parts, one is the extraction
    of visual feature information, and the other is the classification of sequence
    features.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的进展使得唇读方法表现出色。通常，基于深度学习的唇读方法包括两个主要部分，一是提取视觉特征信息，另一个是对序列特征进行分类。
- en: Feature Extraction. Traditional studies use pixel-based[[186](#bib.bib186)],
    shape-based[[187](#bib.bib187), [188](#bib.bib188)], and hybrid-based[[189](#bib.bib189),
    [190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193)]
    approaches to extract the visual feature. However, these methods are not only
    sensitive to image illumination change, lip deformation, and rotation but also
    cannot extract automatically.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取。传统研究使用基于像素[[186](#bib.bib186)]、基于形状[[187](#bib.bib187), [188](#bib.bib188)]和混合方法[[189](#bib.bib189),
    [190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193)]来提取视觉特征。然而，这些方法不仅对图像照明变化、唇部变形和旋转敏感，而且无法自动提取。
- en: Recently, DL has gradually become the mainstream research method in lip visual
    feature extraction, which can be divided into four categories. First, 2D-CNN-based
    methods are used[[194](#bib.bib194), [195](#bib.bib195)], which solves the problem
    of automatic feature extraction, but it can only process single-frame images and
    has a weak ability to process continuous frames, ignoring the spatio-temporal
    correlation between continuous frames. Then, 3D-CNN-based methods have received
    extensive attention[[196](#bib.bib196), [197](#bib.bib197), [198](#bib.bib198),
    [199](#bib.bib199)]. Although this method can solve the problem of spatio-temporal
    correlation of continuous frames, it loses the extraction of fine-grained feature
    information by 2D convolution to a certain extent. According to the aforementioned
    issues, the hybrid methods [[200](#bib.bib200), [201](#bib.bib201), [202](#bib.bib202)]
    of 2D-CNN and 3D-CNN are also introduced to solve the problem of spatio-temporal
    feature extraction and local fine-grained feature extraction simultaneously. This
    method utilizes 3D-CNN to extract spatio-temporal information and then directly
    accesses 2D-CNN to extract fine-grained local information. However, it still affects
    the time information of feature coding to some extent. For that purpose, some
    other neural networks have gradually become a popular choice for lip visual feature
    extraction, such as Autoencoder model[[203](#bib.bib203), [204](#bib.bib204),
    [205](#bib.bib205), [206](#bib.bib206)].
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习逐渐成为唇部视觉特征提取的主流研究方法，可以分为四类。首先，使用2D-CNN的[[194](#bib.bib194), [195](#bib.bib195)]方法解决了自动特征提取的问题，但它只能处理单帧图像，对连续帧的处理能力较弱，忽略了连续帧之间的时空关联。然后，3D-CNN的方法受到了广泛关注[[196](#bib.bib196),
    [197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199)]。虽然这种方法可以解决连续帧的时空关联问题，但在一定程度上丧失了2D卷积提取的细粒度特征信息。针对上述问题，引入了2D-CNN和3D-CNN的混合方法[[200](#bib.bib200),
    [201](#bib.bib201), [202](#bib.bib202)]，以同时解决时空特征提取和局部细粒度特征提取的问题。这种方法利用3D-CNN提取时空信息，然后直接使用2D-CNN提取细粒度的局部信息。然而，它仍然在一定程度上影响特征编码的时间信息。因此，一些其他神经网络逐渐成为唇部视觉特征提取的热门选择，如自编码器模型[[203](#bib.bib203),
    [204](#bib.bib204), [205](#bib.bib205), [206](#bib.bib206)]。
- en: Recognition Modeling. So far, there have been many works viewing lip reading
    as a sequence-to-sequence task and using sequence-based methods to deal with it,
    such as RNN, LSTM, and Transformer. It divides the feature representations extracted
    from the feature extractor into equal time steps, feeding each of them sequentially
    to the classification layer. For instance, [[207](#bib.bib207), [199](#bib.bib199),
    [208](#bib.bib208), [209](#bib.bib209), [202](#bib.bib202), [210](#bib.bib210)]
    utilize Long-Short Term Memory (LSTM) networks and Gated Recurrent Unit (GRU)
    to capture both global and local temporal information. Considering that Temporal
    Convolutional Network (TCN) has the advantage of faster converging speed with
    longer temporal memory than LSTM or RNN models, it is also widely used in this
    task. For example, Bai et al.[[211](#bib.bib211)] first propose a simple yet effective
    TCN architecture, indicating that TCN can become a reasonable alternative to RNN
    as a sequential model. Following this work, Martinez et al.[[212](#bib.bib212)]
    further demonstrate that multi-scale TCN can outperform RNN in lip reading isolated
    words. However, these methods are relatively weak in modeling long-term dependencies
    and cannot directly capture long-term dependencies in sequences. Therefore, a
    new trend in the use of Transformer[[213](#bib.bib213)] for lip-reading tasks
    has emerged[[214](#bib.bib214), [37](#bib.bib37)].
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 识别建模。到目前为止，已经有许多研究将唇读视为序列到序列任务，并使用基于序列的方法来处理它，如RNN、LSTM和Transformer。它将从特征提取器提取的特征表示分成等时间步，将每个时间步顺序输入到分类层。例如，[[207](#bib.bib207)、[199](#bib.bib199)、[208](#bib.bib208)、[209](#bib.bib209)、[202](#bib.bib202)、[210](#bib.bib210)]
    使用长短期记忆（LSTM）网络和门控递归单元（GRU）来捕捉全球和局部的时间信息。考虑到时间卷积网络（TCN）具有比LSTM或RNN模型更快的收敛速度和更长的时间记忆，它也广泛用于此任务。例如，Bai等人[[211](#bib.bib211)]
    首次提出了一种简单而有效的TCN架构，表明TCN可以成为RNN作为序列模型的合理替代品。继这项工作之后，Martinez等人[[212](#bib.bib212)]
    进一步证明，多尺度TCN可以在唇读孤立词方面超越RNN。然而，这些方法在建模长期依赖性方面相对较弱，无法直接捕捉序列中的长期依赖性。因此，针对唇读任务使用Transformer[[213](#bib.bib213)]的趋势已经出现[[214](#bib.bib214)、[37](#bib.bib37)]。
- en: Although the aforementioned methods achieve promising performance, they cannot
    solve the problem of inconsistency between the input and the output modality for
    lip reading. For that purpose, many advanced works are further developed in recent
    years, such as attention mechanisms[[215](#bib.bib215), [98](#bib.bib98), [197](#bib.bib197),
    [214](#bib.bib214), [216](#bib.bib216), [217](#bib.bib217), [218](#bib.bib218)]
    and contrastive learning[[219](#bib.bib219)].
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述方法取得了令人满意的性能，但它们无法解决唇读中输入和输出模态之间的不一致性问题。为此，近年来开发了许多先进的工作，如注意力机制[[215](#bib.bib215)、[98](#bib.bib98)、[197](#bib.bib197)、[214](#bib.bib214)、[216](#bib.bib216)、[217](#bib.bib217)、[218](#bib.bib218)]
    和对比学习[[219](#bib.bib219)]。
- en: 4.2.2 Automatic Cued Speech Recognition
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 自动提示语音识别
- en: 'The literature on ACSR can be classified into three main categories: Multimodal
    Feature Extraction, Multimodal Fusion, and ACSR Modeling. We discuss them separately
    in this section and review the representative works of CS in Table [III](#S4.T3
    "TABLE III ‣ 4.1 Sign Language Recognition ‣ 4 Automatic Body Language Recognition
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ACSR的文献可以分为三大类：多模态特征提取、多模态融合和ACSR建模。我们将在本节中分别讨论这些内容，并在表格[III](#S4.T3 "TABLE
    III ‣ 4.1 Sign Language Recognition ‣ 4 Automatic Body Language Recognition ‣
    A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")中回顾CS的代表性工作。
- en: Multi-modal Feature Extraction. In the literature, there are several popular
    methods for CS feature extraction (i.e., lips, hand position and hand shape).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态特征提取。在文献中，有几种流行的方法用于CS特征提取（即嘴唇、手部位置和手部形状）。
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Traditional Method. It uses artificial markings to record lips and hands from
    video images[[220](#bib.bib220), [221](#bib.bib221)]. For example, Burger et al.[[222](#bib.bib222)]
    let the speaker wear black gloves to obtain accurate hand segmentation, while
    Noureddine et al.[[40](#bib.bib40)] placed blue marks on the speaker’s fingers
    to obtain the coordinates of the fingers. However, both the speaker’s clothing
    color and the background color can affect the accuracy of the hand segmentation.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传统方法。它使用人工标记从视频图像中记录嘴唇和手[[220](#bib.bib220)、[221](#bib.bib221)]。例如，Burger等人[[222](#bib.bib222)]
    让讲者戴上黑色手套以获得准确的手部分割，而Noureddine等人[[40](#bib.bib40)] 在讲者的手指上放置蓝色标记以获取手指的坐标。然而，讲者的衣物颜色和背景颜色都会影响手部分割的准确性。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CNN-based Method. Recently, some CNN-based methods are utilized to get rid of
    artificial markings. For example, the CNN model is used in [[90](#bib.bib90),
    [40](#bib.bib40), [170](#bib.bib170)] to extract visual features from the regions
    of the lip and hand. On the basis of using the CNN model for the feature extraction
    of lips and hand shape, Liu et al. [[40](#bib.bib40)] further adopt the artificial
    neural network (ANN) to process the hand position feature. However, although CNN-based
    methods do not require artificial marks, their performances are limited by data
    scarcity.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于CNN的方法。最近，一些基于CNN的方法被用来去除人工标记。例如，CNN模型在[[90](#bib.bib90)、[40](#bib.bib40)、[170](#bib.bib170)]中被用于从嘴唇和手部区域提取视觉特征。在使用CNN模型提取嘴唇和手部形状特征的基础上，Liu等人[[40](#bib.bib40)]进一步采用人工神经网络（ANN）来处理手部位置特征。然而，尽管基于CNN的方法不需要人工标记，但其性能受到数据稀缺的限制。
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Other DL-based Method. Considering the data-hungry problem for multi-modal,
    some researchers try to introduce some advanced methods to solve this issue. For
    instance, Wang et al. [[58](#bib.bib58)] use lips, hand shape, and hand position
    to pre-train multi-modal feature extractor, using it for feature extraction of
    ACSR task. In addition, in another of their work [[206](#bib.bib206)], the three-stage
    multi-modal feature extraction model based on self-supervised contrastive learning
    and self-attention mechanism is proposed to model spatial and temporal features
    of CS hand shape, lips, and hand position.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他基于DL的方法。考虑到多模态数据饥饿的问题，一些研究人员尝试引入一些先进的方法来解决这一问题。例如，Wang等人[[58](#bib.bib58)]使用嘴唇、手部形状和手部位置来预训练多模态特征提取器，并将其用于ACSR任务的特征提取。此外，在他们的另一项工作[[206](#bib.bib206)]中，提出了一种基于自监督对比学习和自注意力机制的三阶段多模态特征提取模型，以建模CS手部形状、嘴唇和手部位置的空间和时间特征。
- en: Multi-modal Fusion. Most existing works in ACSR tend to direct concatenate the
    multi-modal feature flows, letting the model learn such features implicitly [[221](#bib.bib221),
    [206](#bib.bib206), [90](#bib.bib90), [170](#bib.bib170)]. For instance, [[221](#bib.bib221),
    [206](#bib.bib206)] utilize artificial marks to obtain regions of interest (ROIs)
    and directly concatenated features of lip and hand. MSHMM[[90](#bib.bib90)] merges
    different features by giving weights for different CS modalities. However, to
    the best of our knowledge, a critical issue in ACSR is the asynchrony between
    hand and lip articulations [[40](#bib.bib40), [57](#bib.bib57), [223](#bib.bib223)],
    while these researches mainly assume lip-hand movements are synchronous by default,
    ignoring the asynchronous issue.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态融合。现有的大多数ACSR研究倾向于直接连接多模态特征流，让模型隐式学习这些特征[[221](#bib.bib221)、[206](#bib.bib206)、[90](#bib.bib90)、[170](#bib.bib170)]。例如，[[221](#bib.bib221)、[206](#bib.bib206)]利用人工标记来获取感兴趣区域（ROIs），并直接连接嘴唇和手部的特征。MSHMM[[90](#bib.bib90)]通过为不同CS模态分配权重来合并不同的特征。然而，据我们所知，ACSR中的一个关键问题是手部和嘴唇发音之间的异步性[[40](#bib.bib40)、[57](#bib.bib57)、[223](#bib.bib223)]，而这些研究主要假设嘴唇和手部运动是同步的，忽略了异步性问题。
- en: Therefore, to tackle asynchronous modalities in the ACSR task, Liu et al. [[40](#bib.bib40)]
    propose to utilize the re-synchronization method to align the hand and lips features,
    which is realized by introducing the prior knowledge of the hand position and
    hand shape. Nevertheless, since the acquisition of prior knowledge depends on
    speakers and specific datasets, it is difficult to directly apply it to other
    languages. For that purpose, Liu et al. [[41](#bib.bib41)] further propose a Transformer-based
    cross-modal mutual learning framework for multi-modal feature fusion. The framework
    captures linguistic information by constructing a modality-invariant shared representation
    and uses this linguistic information to guide cross-modal information alignment.
    Recently, [[224](#bib.bib224)] proposes a novel Federated CS recognition (FedCSR)
    framework to train an model of CS recognition in the decentralized data scenario.
    Particularly, they design a mutual knowledge distillation fusion mechanism to
    maintain cross-modal semantic consistency of the CS multi-modalities, which learning
    a unified feature space for both speech and visual feature.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了应对ACSR任务中的异步模态，Liu等人[[40](#bib.bib40)] 提出了利用重同步方法来对齐手部和嘴唇特征，这通过引入手部位置和手部形状的先验知识来实现。然而，由于先验知识的获取依赖于说话者和特定的数据集，因此难以直接应用于其他语言。为此，Liu等人[[41](#bib.bib41)]
    进一步提出了一种基于Transformer的跨模态互学习框架，用于多模态特征融合。该框架通过构建模态不变的共享表示来捕捉语言信息，并使用这些语言信息来指导跨模态信息对齐。最近，[[224](#bib.bib224)]
    提出了一个新颖的联邦CS识别（FedCSR）框架，以在去中心化的数据场景中训练CS识别模型。特别是，他们设计了一个互知识蒸馏融合机制，以保持CS多模态的跨模态语义一致性，从而为语音和视觉特征学习统一的特征空间。
- en: ACSR Modeling. ACSR aims to transcript visual cues of speech to text. In the
    early research, traditional statistical methods are used, which map sequences
    of hand-crafted features to phonemes using statistical models, such as HMM [[221](#bib.bib221),
    [220](#bib.bib220)] and HMM-GMM[[90](#bib.bib90), [40](#bib.bib40)]. However,
    such methods only consider the relationships between the current state and the
    previous one, which means that longer contextual information cannot be captured.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ACSR建模。ACSR旨在将语音的视觉线索转录为文本。在早期的研究中，采用了传统的统计方法，这些方法使用统计模型（如HMM [[221](#bib.bib221),
    [220](#bib.bib220)] 和 HMM-GMM[[90](#bib.bib90), [40](#bib.bib40)]）将手工制作的特征序列映射到音素。然而，这些方法仅考虑当前状态与前一个状态之间的关系，这意味着无法捕捉到更长的上下文信息。
- en: More recently, traditional DL-based methods (i.e., CNN-based, LSTM-based) have
    been developed to alleviate the aforementioned problem. For instance, Sankar et
    al.[[39](#bib.bib39)] propose a novel RNN model trained with a Connectionist Temporal
    Classification (CTC) loss [[225](#bib.bib225)]. Papatimitriou et al.[[170](#bib.bib170)]
    propose a fully convolutional model with a time-depth separable block and attention-based
    decoder. However, such traditional DL-based methods still cannot capture long-time
    dependencies well, while it would be desirable to capture global dependency [[213](#bib.bib213)]
    over dynamic longer because of the context relationships of phonemes in long-time
    CS videos. For that purpose, Transformer-based methods [[170](#bib.bib170)] receive
    a lot of attention on the ACSR task in recent years. This kind of method achieves
    promising performance on the ACSR task, but it still requires powerful computing
    resources and a large dataset for training and parameter tuning.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，传统的基于深度学习的方法（即基于CNN、LSTM的方法）被开发出来以缓解上述问题。例如，Sankar等人[[39](#bib.bib39)] 提出了一个新颖的RNN模型，该模型通过连接时序分类（CTC）损失
    [[225](#bib.bib225)] 进行训练。Papatimitriou等人[[170](#bib.bib170)] 提出了一个完全卷积的模型，具有时间深度可分离块和基于注意力的解码器。然而，这些传统的深度学习方法仍然无法很好地捕捉长期依赖关系，而捕捉动态更长时间的全局依赖[[213](#bib.bib213)]是理想的，因为长时间CS视频中的音素具有上下文关系。为此，基于Transformer的方法[[170](#bib.bib170)]近年来在ACSR任务中受到广泛关注。这种方法在ACSR任务中取得了令人满意的性能，但仍然需要强大的计算资源和大规模数据集进行训练和参数调整。
- en: Therefore, considering the existing corpus for ACSR is limited, some advanced
    methods such as cross-modal knowledge distillation method [[58](#bib.bib58)] and
    contrastive learning method [[206](#bib.bib206)], are also introduced to this
    task.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到现有的ACSR语料库有限，一些先进的方法如跨模态知识蒸馏方法 [[58](#bib.bib58)] 和对比学习方法 [[206](#bib.bib206)]
    也被引入到这一任务中。
- en: 4.3 Co-speech Recognition
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 共语音识别
- en: Although the existing research on CoS mainly focuses on the generation of CoS
    gestures, some scholars have shown that recognizing emotional expressions in CoS
    are crucial to this generation task. For example, Bock et al. [[226](#bib.bib226)]
    is the first to use the EmoGes corpus for emotion recognition in CoS gesture generation.
    Bhattacharya et al. [[227](#bib.bib227)] proposed to leverage the Mel-frequency
    cepstral coefficients and the text transcript computed from the input speech in
    separate encoders in our generator to learn the desired sentiments and the associated
    affective cues.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有的研究主要集中在CoS手势生成上，但一些学者已经表明，识别CoS中的情感表达对这一生成任务至关重要。例如，Bock等人[[226](#bib.bib226)]首次使用EmoGes语料库进行CoS手势生成中的情感识别。Bhattacharya等人[[227](#bib.bib227)]提出利用梅尔频率倒谱系数和从输入语音中计算的文本转录，在我们的生成器中使用不同的编码器来学习期望的情感和相关的情感线索。
- en: 4.4 Talking Head Recognition
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 说话人识别
- en: Since the development of the TH generation still has a long way to go, the focus
    of recent studies is not on TH recognition. TH recognition is primarily treated
    as an evaluation metric for TH generation algorithms. However, humans have the
    ability to detect and identify a person from their face, even when there are changes
    in gender or facial expressions. However, it is difficult to build an automatic
    face recognition system. Therefore, the focus of TH recognition is primarily on
    capturing the essential facial attributes of the target speaker rather than full-fledged
    recognition of the speaker’s identity. In the work proposed by Wen et al. [[228](#bib.bib228)],
    they classified the face identity to assess the performance of voice-based face
    reconstruction for known subjects. For unknown subjects, they used a gender classifier
    to evaluate the gender of the generated faces. Additionally, the feature distance,
    such as Cosine, $L_{1}$, and $L_{2}$ distances, between the target face and the
    generated face can be calculated to measure the accuracy of the generated face.
    To achieve this, a pre-trained face recognition model like FaceNet [[229](#bib.bib229)]
    or ArcFace [[230](#bib.bib230)] is employed as a feature extractor. The landmark
    distance (LMD) can also be measured as the disparity between the generated face
    and the real-world target face images.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TH生成的发展仍有很长的路要走，近期的研究重点不在于TH识别。TH识别主要被视为TH生成算法的评估指标。然而，人类有能力从面部识别和识别一个人，即使性别或面部表情发生变化。然而，建立一个自动面部识别系统是困难的。因此，TH识别的重点主要是捕捉目标说话者的基本面部特征，而不是对说话者身份的全面识别。在Wen等人[[228](#bib.bib228)]提出的工作中，他们分类了面部身份，以评估基于语音的面部重建在已知对象上的性能。对于未知对象，他们使用性别分类器来评估生成面部的性别。此外，可以计算目标面部与生成面部之间的特征距离，如Cosine、$L_{1}$和$L_{2}$距离，以测量生成面部的准确性。为此，使用了预训练的面部识别模型，如FaceNet
    [[229](#bib.bib229)]或ArcFace [[230](#bib.bib230)]，作为特征提取器。还可以测量地标距离（LMD），作为生成面部和现实世界目标面部图像之间的差异。
- en: '![Refer to caption](img/77bfdf899a0c43b3a2ec69b957685183.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/77bfdf899a0c43b3a2ec69b957685183.png)'
- en: 'Figure 10: The milestones of Datasets and Methods for BL generation.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：BL生成的数据集和方法的里程碑。
- en: 'TABLE VI: Metrics for gesture generation.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表VI：手势生成的度量标准。
- en: '| Metrics | Calculation Formula |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 度量标准 | 计算公式 |'
- en: '| PCK [[130](#bib.bib130)] | $\text{PCK}=\frac{1}{N}\sum_{i=1}^{N}\textbf{1}(d_{i}\leq\tau){N}$
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| PCK [[130](#bib.bib130)] | $\text{PCK}=\frac{1}{N}\sum_{i=1}^{N}\textbf{1}(d_{i}\leq\tau){N}$
    |'
- en: '| FGD [[131](#bib.bib131)] | $\text{FGD}=\max_{\pi}\left(\frac{1}{T}\sum_{t=1}^{T}d(g_{t}^{*},g_{\pi(t)})\right)$
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| FGD [[131](#bib.bib131)] | $\text{FGD}=\max_{\pi}\left(\frac{1}{T}\sum_{t=1}^{T}d(g_{t}^{*},g_{\pi(t)})\right)$
    |'
- en: '| MAE [[132](#bib.bib132)] | $\text{MAE}=\frac{1}{T}\sum_{t=1}^{T}\left&#124;g_{t}-g_{t}^{*}\right&#124;$
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| MAE [[132](#bib.bib132)] | $\text{MAE}=\frac{1}{T}\sum_{t=1}^{T}\left&#124;g_{t}-g_{t}^{*}\right&#124;$
    |'
- en: '| STD [[132](#bib.bib132)] | $\text{STD}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}(g_{i}-\bar{g})^{2}}$
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| STD [[132](#bib.bib132)] | $\text{STD}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}(g_{i}-\bar{g})^{2}}$
    |'
- en: '| PMB [[25](#bib.bib25)] | $\operatorname{PMB}=\frac{1}{N_{m}}\sum_{i=1}^{N_{m}}\sum_{j=1}^{N_{a}}\textbf{1}\left[\left\&#124;\bm{b}_{i}^{m}-\bm{b}_{j}^{a}\right\&#124;_{1}<\delta\right]$
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| PMB [[25](#bib.bib25)] | $\operatorname{PMB}=\frac{1}{N_{m}}\sum_{i=1}^{N_{m}}\sum_{j=1}^{N_{a}}\textbf{1}\left[\left\&#124;\bm{b}_{i}^{m}-\bm{b}_{j}^{a}\right\&#124;_{1}<\delta\right]$
    |'
- en: '| MAJE [[131](#bib.bib131)] | $\text{MAJE}=\frac{1}{N\cdot T}\sum_{t=1}^{T}\sum_{n=1}^{N}\left&#124;g_{t}^{n}-g_{t}^{*n}\right&#124;$
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| MAJE [[131](#bib.bib131)] | $\text{MAJE}=\frac{1}{N\cdot T}\sum_{t=1}^{T}\sum_{n=1}^{N}\left&#124;g_{t}^{n}-g_{t}^{*n}\right&#124;$
    |'
- en: '| MAD [[131](#bib.bib131)] | $\text{MAD}=\frac{1}{N\cdot T}\sum_{t=1}^{T}\sum_{n=1}^{N}\left&#124;a_{t}^{n}-a_{t}^{*n}\right&#124;_{2}$
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| MAD [[131](#bib.bib131)] | $\text{MAD}=\frac{1}{N\cdot T}\sum_{t=1}^{T}\sum_{n=1}^{N}\left&#124;a_{t}^{n}-a_{t}^{*n}\right&#124;_{2}$
    |'
- en: 'The corresponding meanings for the alphabet are as follow: $N$ – The number
    of samples; $d_{i}$ – The distance of generated points and ground truth; $\tau$
    – The thresohold of PCK; $T$ – the number of generated frames; $g_{t}$ – the generated
    gestures; ${g_{t}}^{\*}$ – the ground truth; $b_{i}$ – The key frame corresponding
    to the beat. $a_{i}$ – The movement aceleration of generated gestures.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 字母对应的含义如下：$N$ – 样本数量；$d_{i}$ – 生成点与真实值之间的距离；$\tau$ – PCK的阈值；$T$ – 生成的帧数；$g_{t}$
    – 生成的手势；${g_{t}}^{\*}$ – 真实值；$b_{i}$ – 与节拍对应的关键帧；$a_{i}$ – 生成手势的运动加速度。
- en: 5 Automatic Body Language Generation
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 自动身体语言生成
- en: The gesture generation task aims to generate a continuous sequence of gestures
    (i.e., face, head, and hand) using multi-modal inputs (e.g., gloss, speech, and
    text). In this section, we present the related works on gesture language generation
    and review the development timeline of gesture language generation applications,
    such as CS, SL, CoS gesture generations, and TH Generation, respectively.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 手势生成任务旨在使用多模态输入（例如，手语、语音和文本）生成一系列连续的手势（即面部、头部和手部）。在本节中，我们介绍了手势语言生成的相关工作，并回顾了手势语言生成应用的发展时间线，如CS、SL、CoS手势生成以及TH生成。
- en: 5.1 Sign Language Generation
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 手语生成
- en: At the very beginning, we first present the difference between SL, CoS, and
    CS in Figure [4](#S1.F4 "Figure 4 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation"). SL generation has been
    studied for a long time. In this part, we mainly discuss the DL-based research
    on SL generation. For other SL generation methods, please refer to [[5](#bib.bib5)].
    In Table [IV](#S4.T4 "TABLE IV ‣ 4.1 Sign Language Recognition ‣ 4 Automatic Body
    Language Recognition ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), we present a summary of the details of the related
    SL generation works.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在一开始，我们首先在图[4](#S1.F4 "Figure 4 ‣ 1 Introduction ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation")中展示SL、CoS和CS之间的区别。SL生成研究已经进行了很长时间。在这一部分中，我们主要讨论基于DL的SL生成研究。有关其他SL生成方法，请参见[[5](#bib.bib5)]。在表[IV](#S4.T4
    "TABLE IV ‣ 4.1 Sign Language Recognition ‣ 4 Automatic Body Language Recognition
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation")中，我们总结了相关SL生成工作的详细信息。
- en: Multi-modal Feature Extraction. As a special visual language, the inputs of
    the SL gesture generation task are not only text and speech but also SL Gloss.
    It is a marking system for recording SL words and phrases, usually using written
    symbols and short descriptions to represent gestures, mouth movements as well
    as other non-gesture features. SL Gloss is suitable for recording the content
    of SL in written form to facilitate learners to learn and understand SL expressions.
    Previous work [[177](#bib.bib177), [231](#bib.bib231)] first converts spoken language
    to gloss and then uses gloss as input to extract features to generate SL gestures.
    Some work [[175](#bib.bib175)] use spoken language words and their characters
    as input to extract the word embedding of text, then the text features were used
    for gesture generation.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态特征提取。作为一种特殊的视觉语言，SL手势生成任务的输入不仅包括文本和语音，还包括SL Gloss。这是一种记录SL单词和短语的标记系统，通常使用书写符号和简短描述来表示手势、口部动作以及其他非手势特征。SL
    Gloss适用于以书面形式记录SL内容，以便学习者学习和理解SL表达。之前的工作[[177](#bib.bib177), [231](#bib.bib231)]首先将口语转换为Gloss，然后使用Gloss作为输入提取特征以生成SL手势。一些工作[[175](#bib.bib175)]使用口语单词及其字符作为输入，提取文本的词嵌入，然后将文本特征用于手势生成。
- en: 'Generative Methods. For the SL generation task, there are several popular DL-based
    methods: 1) Neural Machine Translation (NMT) method, which [[82](#bib.bib82),
    [22](#bib.bib22), [21](#bib.bib21)] views the SL generation as a translation task.
    It uses the neural machine translation model to process SL text input, which can
    handle the output SL sequence of dynamic length but needs to solve problems such
    as domain adaptation. 2) Motion Graph method [[21](#bib.bib21)] uses motion graphics
    technology to construct a directed graph from motion capture data and generate
    SL. This method can handle the continuity of SL, but it requires large scales
    of data and another challenge is the scalability and computational complexity
    of the graph to select the best transitions. 3) Conditional generation methods
    such as Generative Adversarial Networks (GAN) and Variational Auto-Encoders (VAEs)
    are also employed to generate SL videos. A hybrid model, including a VAE and GAN
    combination, has been proposed for the generation of people performing SL [[232](#bib.bib232),
    [233](#bib.bib233)]. However, the problems such as model complexity and video
    quality need to be solved. 4) Other methods. In addition to the previous work,
    some research tries to introduce novel transformer-based model architectures for
    SLP. For example, [[231](#bib.bib231)] proposes a Progressive Transformers to
    generate continuous sign sequences from spoken language sentences. [[234](#bib.bib234)]
    combines a transformer with a Mixture Density Network (MDN) to manage the translation
    from text to skeletal pose. Although these works have brought performance improvements,
    the cost of the model complexity cannot be ignored.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 生成方法。对于SL生成任务，有几种流行的基于DL的方法：1）神经机器翻译（NMT）方法 [[82](#bib.bib82), [22](#bib.bib22),
    [21](#bib.bib21)] 将SL生成视为翻译任务。它使用神经机器翻译模型处理SL文本输入，可以处理动态长度的输出SL序列，但需要解决领域适应等问题。2）动作图方法[[21](#bib.bib21)]
    使用动态图形技术从动作捕获数据构造有向图，并生成SL。该方法可以处理SL的连续性，但需要大量的数据，并且选择最佳转换时图的可扩展性和计算复杂性是另一个挑战。3）条件生成方法，如生成对抗网络（GAN）和变分自动编码器（VAEs），也被用于生成SL视频。已经提出了一个混合模型，包括VAE和GAN的组合，用于生成执行SL的人物
    [[232](#bib.bib232), [233](#bib.bib233)]。然而，模型复杂性和视频质量等问题需要解决。4）其他方法。除了之前的工作外，一些研究尝试引入新的基于Transformer的模型架构，用于SLP。例如，[[231](#bib.bib231)]
    提出了一种渐进转换器来从口语句子生成连续的手势序列。[[234](#bib.bib234)] 将Transformer与混合密度网络（MDN）结合起来，以管理从文本到骨骼姿势的翻译。尽管这些工作带来了性能改进，但模型复杂性的代价不能忽视。
- en: Even though the SL generation has made some progress, some challenges in the
    CSL generation are still unsolved, i.e., 1) The SL relies on facial expression
    to identify the specific meaning and avoid ambiguity. But few works consider facial
    expressions. 2) The scale of the SL gestures library is very large. According
    to the official Chinese SL dictionary, there are about 5600 kinds of frequently
    used SL gestures. Most of the dataset only covers a small portion of all gestures,
    for example, [[235](#bib.bib235)] builds a CSL dataset with 500 categories. The
    huge number of gestures brings a huge cost for the DL-based models to construct
    the mapping relationship.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SL生成取得了一些进展，但CSL生成仍然存在一些挑战，例如：1）SL依赖于面部表情来识别特定含义和避免歧义。但很少有研究考虑面部表情。2）SL手势库的规模非常庞大。根据官方的中国手语词典，有大约5600种常用的SL手势。大多数数据集只涵盖了其中一小部分手势，例如，[[235](#bib.bib235)]
    构建了一个包含500个类别的CSL数据集。庞大的手势数量为基于DL的模型构建映射关系带来了巨大的成本。
- en: '![Refer to caption](img/c5272d329189c4744511282ed8ce002f.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c5272d329189c4744511282ed8ce002f.png)'
- en: 'Figure 11: The overall framework of the conversion between CS and text/audio.
    Direction 1 means CS to text/audio recognition, and direction 2 means text/audio
    to CS gesture generation. The first direction aims to recognize text or audio
    to make normal hearing better understand the hearing-impaired people, and the
    second direction can help the hearing-impaired to visually understand normal-hearing
    people.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：CS和文本/音频之间转换的整体框架。方向1表示CS到文本/音频识别，方向2表示文本/音频到CS手势生成。第一个方向旨在通过识别文本或音频使正常听力更好地理解听力障碍者，而第二个方向可以帮助听力障碍者视觉上理解正常听力的人。
- en: 5.2 Cued Speech Generation
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 隐式语音生成
- en: As a lip-hand aided system, CS requires generating both lip and hand gestures
    simultaneously. Therefore, it is very important to extract multi-modal features
    such as speech features and text features. Among them, speech features have a
    strong correlation with lip movement. At the same time, text features play an
    important role in determining hand shape and position according to the coding
    system. As depicted in Figure [11](#S5.F11 "Figure 11 ‣ 5.1 Sign Language Generation
    ‣ 5 Automatic Body Language Generation ‣ A Survey on Deep Multi-modal Learning
    for Body Language Recognition and Generation"), the generation of multi-modal
    CS hand gestures from audio-text is a crucial component of the CS conversion system.
    Previous studies in the literature have made limited initial attempts at CS gesture
    generation, which is mainly from two perspectives of multi-modal feature extraction
    and generation methods. Since the related work is relatively small, we incorporate
    the summary of the related CS generation works with the CoS In Table [V](#S4.T5
    "TABLE V ‣ 4.1 Sign Language Recognition ‣ 4 Automatic Body Language Recognition
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation").
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种唇部与手部辅助系统，CS 需要同时生成唇部和手部手势。因此，提取诸如语音特征和文本特征等多模态特征非常重要。其中，语音特征与唇部运动有很强的关联。同时，文本特征在根据编码系统确定手部形状和位置方面起着重要作用。如图
    [11](#S5.F11 "图 11 ‣ 5.1 手语生成 ‣ 5 自动身体语言生成 ‣ 深度多模态学习在身体语言识别和生成中的调查") 所示，从音频文本生成多模态
    CS 手势是 CS 转换系统的一个关键组成部分。文献中的先前研究对 CS 手势生成进行了有限的初步尝试，主要从多模态特征提取和生成方法两个方面进行研究。由于相关工作相对较少，我们将与
    CoS 相关的 CS 生成工作的总结纳入了表 [V](#S4.T5 "表 V ‣ 4.1 手语识别 ‣ 4 自动身体语言识别 ‣ 深度多模态学习在身体语言识别和生成中的调查")
    中。
- en: Multi-modal Feature Extraction. For CS generation, the feature includes continuous
    lip shape and hand shape movements. [[28](#bib.bib28)] used specific manually
    selected keywords, along with low-context sentences [[236](#bib.bib236)] as a
    feature, and pre-defined corresponding manual templates for hand gestures. CS
    recognition was performed, followed by the mapping of recognized text to the hand
    templates. However, this approach heavily relied on manual designs, which not
    only constrained the expressiveness of CS gestures but also increased the amount
    of manual effort required.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态特征提取。对于 CS 生成，特征包括连续的唇部形状和手部形状运动。[[28](#bib.bib28)] 使用了特定手动选择的关键词，以及低语境句子
    [[236](#bib.bib236)] 作为特征，并为手势预定义了相应的手动模板。进行 CS 识别，然后将识别出的文本映射到手部模板上。然而，这种方法严重依赖于手动设计，不仅限制了
    CS 手势的表现力，还增加了所需的人工工作量。
- en: Generative Method. To the best of our knowledge, there is still a lack of research
    on end-to-end deep learning-based CS gesture generation. Only [[28](#bib.bib28)]
    proposed a post-processing algorithm to adjust synthesized hand gestures, involving
    correction of hand rotation and translation, as well as adaptation of the algorithm
    to new images. Nevertheless, this method requires prior human knowledge to adapt
    the algorithm to new images, leading to limited robustness.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 生成方法。尽我们所知，目前仍缺乏基于端到端深度学习的 CS 手势生成研究。只有 [[28](#bib.bib28)] 提出了一个后处理算法来调整合成的手势，包括手部旋转和位移的校正，以及将算法适配到新图像。然而，这种方法需要事先的人类知识来将算法适配到新图像，从而导致了有限的鲁棒性。
- en: 5.3 Co-speech Generation
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 共语生成
- en: The milestones of CoS generation in recent years are presented in Figure [10](#S4.F10
    "Figure 10 ‣ 4.4 Talking Head Recognition ‣ 4 Automatic Body Language Recognition
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation").
    The upper part is related datasets and the lower part is the algorithm. The target
    of CoS gesture generation is to generate a sequence of body movements based on
    the corresponding audio input. It has been widely used in virtual character animation,
    especially in virtual speech and advertising. We divided it into three stages
    based on performance and popularity, Which are rule/statistical-based methods,
    DL-based methods, and Diffusion-based methods. In Table [V](#S4.T5 "TABLE V ‣
    4.1 Sign Language Recognition ‣ 4 Automatic Body Language Recognition ‣ A Survey
    on Deep Multi-modal Learning for Body Language Recognition and Generation"), we
    present a summary of the details of the related SL generation works.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来的CoS生成里程碑展示在图[10](#S4.F10 "图 10 ‣ 4.4 讲话头部识别 ‣ 4 自动身体语言识别 ‣ 关于深度多模态学习在身体语言识别与生成中的调查")中。上半部分涉及相关数据集，下半部分涉及算法。CoS手势生成的目标是根据相应的音频输入生成一系列身体动作。它在虚拟角色动画中得到了广泛应用，特别是在虚拟演讲和广告中。我们根据性能和受欢迎程度将其分为三个阶段，即基于规则/统计的方法、基于深度学习的方法和基于扩散的方法。在表[V](#S4.T5
    "表 V ‣ 4.1 手语识别 ‣ 4 自动身体语言识别 ‣ 关于深度多模态学习在身体语言识别与生成中的调查")中，我们总结了相关SL生成工作的详细信息。
- en: Multi-modal Feature Extraction. In the CoS gesture generation task, the data
    of different modalities such as text and speech contain semantic and rhythmic
    information. How to extract and fuse these features to get a better representation
    is an important topic. [[131](#bib.bib131)] uses a tri-modal encoder to encode
    text, speech, and person IDs separately, and then perform feature fusion, sampling
    from the fused feature space to complete the generation task. [[237](#bib.bib237)]
    separately models speech and text information. Instead of directly fusing at the
    feature level, it establishes two pipelines to model the dynamic and semantic
    information of the gesture motion, so as to generate accurate and rhythmic gesture
    sequences.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态特征提取。在CoS手势生成任务中，不同模态如文本和语音的数据包含了语义和节奏信息。如何提取和融合这些特征以获得更好的表示是一个重要课题。[[131](#bib.bib131)]使用三模态编码器分别编码文本、语音和人物ID，然后进行特征融合，从融合特征空间中采样以完成生成任务。[[237](#bib.bib237)]则分别建模语音和文本信息。它没有在特征层面直接融合，而是建立了两个管道以建模手势运动的动态和语义信息，从而生成准确且有节奏的手势序列。
- en: Generative Model. Numerous endeavors have been made in the process of choosing
    the generative model for CoS gesture generation task. In the early research, rule-based
    approaches [[238](#bib.bib238), [239](#bib.bib239), [240](#bib.bib240)] were used,
    which required the manual construction of a gesture library and the development
    of rules mapping from spoken language to gestures in the library. These methods
    had limited flexibility and required expert knowledge, but it is easier to be
    interpreted and were effective at handling semantic gestures. Then, statistical-based
    methods[[241](#bib.bib241)] replaced the manually written rules with traditional
    statistical models (e.g., HMMs) trained on a dataset but still required the high-cost
    manual construction of a gesture library. In recent years, DL-based end-to-end
    approaches [[25](#bib.bib25), [242](#bib.bib242)] have been developed, which use
    raw “speech-gesture” datasets such as Trinity and TED [[67](#bib.bib67), [68](#bib.bib68)]
    to train deep neural networks for end-to-end gesture generation. These methods
    have reduced system complexity and produced more natural and fluid gestures, but
    they cannot guarantee the accuracy of generated rhythmic and semantic gestures.
    Meanwhile, most CoS research works do not consider the generation of the whole
    body, which also limits its expressiveness. Recently, diffusion models [[243](#bib.bib243)]
    have emerged as powerful deep generative models. Zhu et al. [[244](#bib.bib244)]
    introduced a novel diffusion-based framework called DiffGesture, which effectively
    captures the associations between audio and gestures and maintains temporal coherence
    to generate high-quality CoS gestures. However, the diffusion-based method has
    limitations in terms of training cost and the need for multiple steps to achieve
    satisfactory results, which hinders its real-time application in CoS gesture generation.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型。在 CoS 手势生成任务中，选择生成模型的过程中进行了大量的尝试。在早期研究中，使用了基于规则的方法[[238](#bib.bib238),
    [239](#bib.bib239), [240](#bib.bib240)]，这些方法需要手动构建手势库，并开发从口语到手势的映射规则。这些方法灵活性有限，需要专家知识，但更容易解释，并且在处理语义手势方面有效。随后，基于统计的方法[[241](#bib.bib241)]用传统的统计模型（如
    HMMs）替代了手动编写的规则，这些模型在数据集上进行训练，但仍然需要高成本的手动构建手势库。近年来，基于深度学习的端到端方法 [[25](#bib.bib25),
    [242](#bib.bib242)] 得到了发展，这些方法使用原始的“语音-手势”数据集，如 Trinity 和 TED [[67](#bib.bib67),
    [68](#bib.bib68)]，来训练深度神经网络以实现端到端的手势生成。这些方法减少了系统复杂性，并生成了更自然流畅的手势，但不能保证生成的节奏性和语义手势的准确性。同时，大多数
    CoS 研究工作并未考虑全身生成，这也限制了其表现力。近年来，扩散模型 [[243](#bib.bib243)] 作为强大的深度生成模型出现。Zhu 等人
    [[244](#bib.bib244)] 引入了一种新的基于扩散的框架，称为 DiffGesture，该框架有效捕捉了音频与手势之间的关联，并保持时间一致性，从而生成高质量的
    CoS 手势。然而，基于扩散的方法在训练成本和需要多个步骤才能达到令人满意的结果方面存在局限，这限制了其在 CoS 手势生成中的实时应用。
- en: 5.4 Talking Head Generation
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 谈话头生成
- en: 'TH generation has become an emerging research topic in recent years. As shown
    in Figure [12](#S5.F12 "Figure 12 ‣ 5.4 Talking Head Generation ‣ 5 Automatic
    Body Language Generation ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation"), talking face generation from an audio clip or dynamic
    TH generation from a target image and an audio clip are two fundamental research
    problems. The problems’ solutions are essential to enabling a wide range of practical
    applications: (a) Entertainment: Generating virtual characters with realistic
    expressions and voice output can be applied to virtual reality games, special
    effects in movies, and other fields, to enhance user experience; (b) Virtual assistants:
    Generating virtual assistants with natural language voice and facial expressions
    can be used in customer service, robot assistants, and other scenarios to improve
    natural language interaction experience; (c) Human-machine interaction: Generating
    virtual characters with realistic expressions and voice output can be used for
    virtual meetings, remote education, and other scenarios to improve human-machine
    interaction effectiveness. (d) Healthcare: Generating virtual doctors with natural
    speak voices and facial expressions can be used in telemedicine, psychotherapy,
    and other scenarios to improve service quality and user experience.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 语音到面部生成已成为近年来新兴的研究课题。如图 [12](#S5.F12 "图 12 ‣ 5.4 说话头生成 ‣ 5 自动体态语言生成 ‣ 深度多模态学习在体态语言识别与生成中的调查")
    所示，从音频片段生成说话面孔或从目标图像和音频片段动态生成说话面孔是两个基本的研究问题。这些问题的解决对于实现广泛的实际应用至关重要：(a) 娱乐：生成具有真实表情和声音输出的虚拟角色可以应用于虚拟现实游戏、电影特效和其他领域，以增强用户体验；(b)
    虚拟助手：生成具有自然语言声音和面部表情的虚拟助手可以用于客户服务、机器人助手等场景，以改善自然语言交互体验；(c) 人机交互：生成具有真实表情和声音输出的虚拟角色可以用于虚拟会议、远程教育等场景，以提高人机交互的有效性；(d)
    医疗保健：生成具有自然语音和面部表情的虚拟医生可以用于远程医疗、心理治疗等场景，以提高服务质量和用户体验。
- en: '![Refer to caption](img/17199def72011828923f2c26c1b54878.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/17199def72011828923f2c26c1b54878.png)'
- en: 'Figure 12: The two basic problems of speech-to-face generation.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：语音到面部生成的两个基本问题。
- en: 5.4.1 Speech-to-face Generation
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 语音到面部生成
- en: There is a strong connection between speech and face attributes, such as age,
    gender, and the shape of the mouth, which directly affect the mechanics of speech
    generation [[245](#bib.bib245)]. Additionally, properties of speech such as language,
    accent, speed, and pronunciation are frequently shared among various nationalities
    and cultures. These properties can consequently manifest as standard physical
    facial features.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 语音和面部属性之间有着强烈的关联，例如年龄、性别和嘴部形状，这些直接影响语音生成的机制 [[245](#bib.bib245)]。此外，语言、口音、速度和发音等语音特性在各种国籍和文化中经常共享。这些特性因此可以表现为标准的物理面部特征。
- en: Feature Extraction. For speech and face feature extraction, Oh et al. [[246](#bib.bib246)]
    employs a trained face recognition network [[247](#bib.bib247)] to obtain face
    embedding, and a voice encoder that takes a complex spectrogram of speech as input
    and output speech features. Duarte et al. [[248](#bib.bib248)] design a speech
    encoder modified from SEGAN discriminator [[249](#bib.bib249)] to learn audio
    embedding. Similarly, Wen et al. [[228](#bib.bib228)] develops a voice embedding
    network consisting of six convolution layers to learn speech features. A voice
    encoder included voice activity detection and V-net is used in Fang et al. [[250](#bib.bib250)]
    to output audio embedding.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取。对于语音和面部特征提取，Oh 等人 [[246](#bib.bib246)] 使用训练好的面部识别网络 [[247](#bib.bib247)]
    获取面部嵌入，并使用一个以复杂语音谱图作为输入的语音编码器输出语音特征。Duarte 等人 [[248](#bib.bib248)] 设计了一个改进自 SEGAN
    判别器 [[249](#bib.bib249)] 的语音编码器来学习音频嵌入。类似地，Wen 等人 [[228](#bib.bib228)] 开发了一个由六层卷积层组成的语音嵌入网络来学习语音特征。Fang
    等人 [[250](#bib.bib250)] 使用包含语音活动检测和 V-net 的语音编码器来输出音频嵌入。
- en: Generation Model. Oh et al. [[246](#bib.bib246)] employ a pre-trained face decoder
    [[251](#bib.bib251)] to reconstruct the face image. Motivated by the success of
    GAN [[252](#bib.bib252)] in generation images with high quality. Duarte et al.
    [[248](#bib.bib248)] developed a conditional GAN called WavPix that is able to
    generate face images directly from the speech. For better identity matching, Wen
    et al. [[228](#bib.bib228)] introduced the second discriminator to verify the
    identity of face image output. Considering that emotional expression is a key
    face attribute of a realistic face image, Fang et al. [[250](#bib.bib250)] applied
    two classifiers to measure identity and emotion semantic relevance in generating.
    In [[253](#bib.bib253)], a Face-based Residual Personalized Speech Synthesis Model
    (FR-PSS) containing a speech encoder, a speech synthesizer and a face encoder
    is designed for PSS.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型。Oh等人 [[246](#bib.bib246)] 使用预训练的面部解码器 [[251](#bib.bib251)] 来重建面部图像。受到GAN
    [[252](#bib.bib252)] 在生成高质量图像方面成功的启发，Duarte等人 [[248](#bib.bib248)] 开发了一种条件GAN，称为WavPix，能够直接从语音生成面部图像。为了更好的身份匹配，Wen等人
    [[228](#bib.bib228)] 引入了第二个鉴别器以验证面部图像输出的身份。考虑到情感表达是现实面部图像的一个关键属性，Fang等人 [[250](#bib.bib250)]
    应用了两个分类器来测量生成过程中的身份和情感语义相关性。在 [[253](#bib.bib253)] 中，设计了一个包含语音编码器、语音合成器和面部编码器的基于面部的残差个性化语音合成模型（FR-PSS）用于PSS。
- en: 'These aforementioned methods can generate face images from speech, however,
    the authenticity and accuracy of the reconstructed face image still need to be
    improved: (a) Explicit cross-modal correlation learning is vital for identity
    information preservation, which is not explored in the previous methods. (b) The
    face images synthesized by the GAN-based or CNN-based generator lack details and
    authenticity.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法可以从语音生成面部图像，但重建的面部图像的真实性和准确性仍需改进：(a) 明确的跨模态关联学习对于身份信息保留至关重要，这在之前的方法中没有探讨。(b)
    基于GAN或CNN的生成器合成的面部图像缺乏细节和真实性。
- en: Evaluations Metrics. For speech-to-face generation methods, identity information
    preservation is the key factor, therefore, quantitative metrics related to identity
    consistency are used to evaluate the performance, which includes landmark distance,
    feature distance, and face attributes evaluation. Landmark distance is to calculate
    the distance of landmark (LMD) of generated face image and true face, where the
    landmark is achieved by Dlib [[254](#bib.bib254)] pre-trained DL methods such
    as FaceNet [[229](#bib.bib229)]. Feature distances are Cosine, $L_{2}$, and $L_{1}$
    distances calculated between the feature of the true face and generated face.
    The face attributes are generally evaluated by attribution recognition accuracy
    like gender recognition, identity recognition, and face retrieval. The quality
    of generated face images is also important for speech-to-face generation, Fréchet
    Inception Distance (FID), and Inception score (IS) are two common metrics to evaluate
    performance. Those abovementioned metrics are highlighted in Table [VII](#S5.T7
    "TABLE VII ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking Head Generation ‣ 5
    Automatic Body Language Generation ‣ A Survey on Deep Multi-modal Learning for
    Body Language Recognition and Generation").
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。对于语音到面部生成方法，身份信息保留是关键因素，因此，使用与身份一致性相关的定量指标来评估性能，包括地标距离、特征距离和面部属性评估。地标距离用于计算生成面部图像和真实面部之间的地标距离（LMD），地标通过Dlib
    [[254](#bib.bib254)] 预训练DL方法如FaceNet [[229](#bib.bib229)] 获得。特征距离包括计算真实面部和生成面部特征之间的余弦、$L_{2}$
    和 $L_{1}$ 距离。面部属性通常通过属性识别准确率来评估，如性别识别、身份识别和面部检索。生成面部图像的质量对于语音到面部生成也很重要，Fréchet
    Inception Distance (FID) 和 Inception score (IS) 是两种常见的评估性能的指标。这些上述指标在表 [VII](#S5.T7
    "TABLE VII ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking Head Generation ‣ 5
    Automatic Body Language Generation ‣ A Survey on Deep Multi-modal Learning for
    Body Language Recognition and Generation") 中有所强调。
- en: 'TABLE VII: Summary of quantitative metrics of Speech-to-face generation'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：语音到面部生成的定量指标总结
- en: '| Metrics’ degree | Metrics |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 评估度量 | 指标 |'
- en: '| Identity preservation |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 身份保留 |'
- en: '&#124; LDM [[246](#bib.bib246)], &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LDM [[246](#bib.bib246)], &#124;'
- en: '&#124; Cosine, $L_{2}$, $L_{1}$ [[246](#bib.bib246)], &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 余弦、$L_{2}$、$L_{1}$ [[246](#bib.bib246)], &#124;'
- en: '&#124; Face retrieval [[246](#bib.bib246)], &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部检索 [[246](#bib.bib246)], &#124;'
- en: '&#124; Identity recognition [[228](#bib.bib228)], &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 身份识别 [[228](#bib.bib228)], &#124;'
- en: '&#124; Gender classification [[228](#bib.bib228)] &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 性别分类 [[228](#bib.bib228)] &#124;'
- en: '|'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Image quality | IS [[250](#bib.bib250)], FID [[250](#bib.bib250)] |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 图像质量 | IS [[250](#bib.bib250)], FID [[250](#bib.bib250)] |'
- en: 'TABLE VIII: Summary of recent studies related to Talking Head generation. The
    following aspects are concluded: the network architecture for image synthesis
    and driving source; the methods work for a specific target or arbitrary identity;
    the audio feature is synchronized with lip motions or not; the ability to generate
    personalized attributes, and if any intermediate face models are used.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VIII：关于谈话头生成的近期研究总结。以下方面已结论：图像合成和驱动源的网络架构；方法适用于特定目标或任意身份；音频特征是否与嘴唇动作同步；生成个性化属性的能力，以及是否使用了任何中间面部模型。
- en: '| Framework | Methods | Year | Driving source | Target | Audio features | Personalized
    | Face model |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 框架 | 方法 | 年份 | 驱动源 | 目标 | 音频特征 | 个性化 | 面部模型 |'
- en: '| GAN | Chen et al. [[255](#bib.bib255)] | 2018 | Audio | Arbitrary | Sync
    | No | No |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| GAN | 陈等 [[255](#bib.bib255)] | 2018 | 音频 | 任意 | 同步 | 否 | 否 |'
- en: '| Song et al. [[256](#bib.bib256)] | 2019 | Audio | Arbitrary | Sync | No |
    No |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Song 等 [[256](#bib.bib256)] | 2019 | 音频 | 任意 | 同步 | 否 | 否 |'
- en: '| Zhou et al. [[257](#bib.bib257)] | 2019 | Audio | Arbitrary | Sync | No |
    No |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Zhou 等 [[257](#bib.bib257)] | 2019 | 音频 | 任意 | 同步 | 否 | 否 |'
- en: '| ATVG [[258](#bib.bib258)] | 2019 | Audio | Arbitrary | not sync | No | 2D
    landmarks |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| ATVG [[258](#bib.bib258)] | 2019 | 音频 | 任意 | 不同步 | 否 | 2D 地标 |'
- en: '| Vougioukas et al. [[259](#bib.bib259)] | 2019 | Audio | Arbitrary | Sync
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Vougioukas 等 [[259](#bib.bib259)] | 2019 | 音频 | 任意 | 同步 |'
- en: '&#124; Eye blinks, &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛眨动, &#124;'
- en: '&#124; eyebrow &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眉毛 &#124;'
- en: '| No |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '| Kefalas et al . [[260](#bib.bib260)] | 2020 | Audio | Arbitrary | No sync
    | No | No |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Kefalas 等 [[260](#bib.bib260)] | 2020 | 音频 | 任意 | 不同步 | 否 | 否 |'
- en: '| Sinha et al. [[261](#bib.bib261)] | 2020 | Audio | Arbitrary | No Sync |
    Eye blinking | No |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Sinha 等 [[261](#bib.bib261)] | 2020 | 音频 | 任意 | 不同步 | 眼睛眨动 | 否 |'
- en: '| Wang et al. [[262](#bib.bib262)] | 2020 | Audio | Arbitrary | Sync | Head
    pose | 2D landmark |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 [[262](#bib.bib262)] | 2020 | 音频 | 任意 | 同步 | 头部姿态 | 2D 地标 |'
- en: '| Wav2lip [[30](#bib.bib30)] | 2020 | Audio | Arbitrary | Sync | No | No |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Wav2lip [[30](#bib.bib30)] | 2020 | 音频 | 任意 | 同步 | 否 | 否 |'
- en: '| Eskimez et al. [[263](#bib.bib263)] | 2020 | Audio | Arbitrary | Sync | No
    | No |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Eskimez 等 [[263](#bib.bib263)] | 2020 | 音频 | 任意 | 同步 | 否 | 否 |'
- en: '| Yi et al. [[264](#bib.bib264)] | 2020 | Video | Specific | Not sync | Head
    pose | 3DMM |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Yi 等 [[264](#bib.bib264)] | 2020 | 视频 | 特定 | 不同步 | 头部姿态 | 3DMM |'
- en: '| Chen et al. [[265](#bib.bib265)] | 2020 | Video | Arbitrary | Not sync |
    Head pose | 3DMM |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 陈等 [[265](#bib.bib265)] | 2020 | 视频 | 任意 | 不同步 | 头部姿态 | 3DMM |'
- en: '| Mittal et al. [[266](#bib.bib266)] | 2021 | Audio | Arbitrary | Not sync
    | No | No |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Mittal 等 [[266](#bib.bib266)] | 2021 | 音频 | 任意 | 不同步 | 否 | 否 |'
- en: '| MEAD [[111](#bib.bib111)] | 2020 | Audio | Arbitrary | Not sync | Emotion
    | No |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| MEAD [[111](#bib.bib111)] | 2020 | 音频 | 任意 | 不同步 | 情感 | 否 |'
- en: '| Zhu et al. [[267](#bib.bib267)] | 2021 | Audio | Arbitrary | No sync | No
    | No |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Zhu 等 [[267](#bib.bib267)] | 2021 | 音频 | 任意 | 不同步 | 否 | 否 |'
- en: '| FACIAL [[268](#bib.bib268)] | 2021 | Video | Arbitrary | Not sync |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| FACIAL [[268](#bib.bib268)] | 2021 | 视频 | 任意 | 不同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿态, &#124;'
- en: '&#124; eye blinking &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛眨动 &#124;'
- en: '| 3DMM |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 3DMM |'
- en: '| Zhang et al. [[112](#bib.bib112)] | 2021 | Audio | Arbitrary | Sync |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等 [[112](#bib.bib112)] | 2021 | 音频 | 任意 | 同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿态, &#124;'
- en: '&#124; eyebrow &#124;'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眉毛 &#124;'
- en: '| 3DMM |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 3DMM |'
- en: '| Si et al. [[269](#bib.bib269)] | 2021 | Audio | Arbitrary | No sync | Emotion
    | No |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Si 等 [[269](#bib.bib269)] | 2021 | 音频 | 任意 | 不同步 | 情感 | 否 |'
- en: '| Chen et al. [[270](#bib.bib270)] | 2021 | Audio | Arbitrary | Sync | No |
    No |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 陈等 [[270](#bib.bib270)] | 2021 | 音频 | 任意 | 同步 | 否 | 否 |'
- en: '| PC-AVS [[271](#bib.bib271)] | 2021 | Video | Arbitrary | Sync | Head pose
    | No |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| PC-AVS [[271](#bib.bib271)] | 2021 | 视频 | 任意 | 同步 | 头部姿态 | 否 |'
- en: '| GC-VAT [[272](#bib.bib272)] | 2022 | Video | Arbitrary | Sync |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| GC-VAT [[272](#bib.bib272)] | 2022 | 视频 | 任意 | 同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿态, &#124;'
- en: '&#124; expression &#124;'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 表情 &#124;'
- en: '| No |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '| Wang et al. [[273](#bib.bib273)] | 2022 | Audio | Arbitrary | Sync | Head
    pose | No |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 [[273](#bib.bib273)] | 2022 | 音频 | 任意 | 同步 | 头部姿态 | 否 |'
- en: '| EAMM [[274](#bib.bib274)] | 2022 | Video | Arbitrary | No sync | Emotion
    | No |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| EAMM [[274](#bib.bib274)] | 2022 | 视频 | 任意 | 不同步 | 情感 | 否 |'
- en: '| SPACE [[275](#bib.bib275)] | 2022 | Audio | Arbitrary | No sync |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| SPACE [[275](#bib.bib275)] | 2022 | 音频 | 任意 | 不同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿态, &#124;'
- en: '&#124; emotion &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 情感 &#124;'
- en: '| 2D landmark |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 2D 地标 |'
- en: '| DIRFA [[276](#bib.bib276)] | 2023 | Audio | Arbitrary | Sync | No | No |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| DIRFA [[276](#bib.bib276)] | 2023 | 音频 | 任意 | 同步 | 否 | 否 |'
- en: '| DisCoHead [[115](#bib.bib115)] | 2023 | Video | Arbitrary | Sync |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| DisCoHead [[115](#bib.bib115)] | 2023 | 视频 | 任意 | 同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿势，&#124;'
- en: '&#124; eye blinking, &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛眨动，&#124;'
- en: '&#124; eyebrow &#124;'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眉毛 &#124;'
- en: '| No |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '| OPT [[277](#bib.bib277)] | 2023 | Audio | Arbitrary | No sync |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| OPT [[277](#bib.bib277)] | 2023 | 音频 | 任意 | 无同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿势，&#124;'
- en: '&#124; expression &#124;'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 表情 &#124;'
- en: '| 3DMM |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 3DMM |'
- en: '| Wang et al. [[278](#bib.bib278)] | 2023 | Audio | Abitrary | Sync |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 [[278](#bib.bib278)] | 2023 | 音频 | 任意 | 同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿势，&#124;'
- en: '&#124; expression,gaze, &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 表情，注视，&#124;'
- en: '&#124; eye blinking &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛眨动 &#124;'
- en: '| No |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '| Zhang et al. [[279](#bib.bib279)] | 2023 | Audio | Abitrary | No sync | No
    | No |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等 [[279](#bib.bib279)] | 2023 | 音频 | 任意 | 无同步 | 否 | 否 |'
- en: 'TABLE IX: Summary of recent studies related to Talking Head generation. The
    following aspects are concluded: The network architecture for image synthesis;
    Driving source; The methods work for a specific target or arbitrary identity;
    The audio feature is synchronized with lip motions or not; The ability to generate
    personalized attributes, and if any intermediate face models are used.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IX: 最近的谈话头生成相关研究总结。以下方面被总结：图像合成的网络架构；驱动源；方法是否适用于特定目标或任意身份；音频特征是否与嘴唇动作同步；生成个性化属性的能力，以及是否使用任何中间面部模型。'
- en: '| Framework | Methods | Year | Driving source | Target | Audio features | Personalized
    | Face model |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 框架 | 方法 | 年份 | 驱动源 | 目标 | 音频特征 | 个性化 | 面部模型 |'
- en: '| CNN | X2Face [[280](#bib.bib280)] | 2018 | Audio, video | Arbitrary | Sync
    |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| CNN | X2Face [[280](#bib.bib280)] | 2018 | 音频，视频 | 任意 | 同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿势，&#124;'
- en: '&#124; expression &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 表情 &#124;'
- en: '| No |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '| Jamaludin et al. [[281](#bib.bib281)] | 2019 | Audio | Arbitrary | Sync |
    No | No |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Jamaludin 等 [[281](#bib.bib281)] | 2019 | 音频 | 任意 | 同步 | 否 | 否 |'
- en: '| Wen et al. [[282](#bib.bib282)] | 2020 | Video, audio | Arbitrary | No sync
    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Wen 等 [[282](#bib.bib282)] | 2020 | 视频，音频 | 任意 | 无同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿势，&#124;'
- en: '&#124; expression &#124;'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 表情 &#124;'
- en: '| 3DMM |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 3DMM |'
- en: '| LipSync3D [[283](#bib.bib283)] | 2021 | Video | Specific | No sync | No |
    3DMM |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| LipSync3D [[283](#bib.bib283)] | 2021 | 视频 | 特定 | 无同步 | 否 | 3DMM |'
- en: '| Audio2head [[31](#bib.bib31)] | 2021 | Audio | Arbitrary | Sync | Head pose
    | 2D landmark |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| Audio2head [[31](#bib.bib31)] | 2021 | 音频 | 任意 | 同步 | 头部姿势 | 2D 标记 |'
- en: '| Lu et al. [[284](#bib.bib284)] | 2021 | Audio | Specific | No sync |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Lu 等 [[284](#bib.bib284)] | 2021 | 音频 | 特定 | 无同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿势，&#124;'
- en: '&#124; eyebrow &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眉毛 &#124;'
- en: '|  |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| RNN | Bigioi et al. [[285](#bib.bib285)] | 2022 | Video, audio | Arbitrary
    | No sync | Head pose | 2D landmark. |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| RNN | Bigioi 等 [[285](#bib.bib285)] | 2022 | 视频，音频 | 任意 | 无同步 | 头部姿势 | 2D
    标记 |'
- en: '| VAE | SadTalker [[286](#bib.bib286)] | 2023 | Audio | Abitrary | No sync
    |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| VAE | SadTalker [[286](#bib.bib286)] | 2023 | 音频 | 任意 | 无同步 |'
- en: '&#124; Head pose, &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿势，&#124;'
- en: '&#124; eye blinking &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛眨动 &#124;'
- en: '| 3DMM |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 3DMM |'
- en: '| NeRF | AD-NeRF [[32](#bib.bib32)] | 2021 | Audio | Specific | No sync | No
    | No |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| NeRF | AD-NeRF [[32](#bib.bib32)] | 2021 | 音频 | 特定 | 无同步 | 否 | 否 |'
- en: '| DFA-NERF [[287](#bib.bib287)] | 2022 | Video | specific | Sync |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| DFA-NERF [[287](#bib.bib287)] | 2022 | 视频 | 特定 | 同步 |'
- en: '&#124; Eye blinking, &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛眨动，&#124;'
- en: '&#124; head pose &#124;'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿势 &#124;'
- en: '| No |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '| DFRF [[288](#bib.bib288)] | 2022 | Audio | Arbitrary | No sync | No | 3DMM
    |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| DFRF [[288](#bib.bib288)] | 2022 | 音频 | 任意 | 无同步 | 否 | 3DMM |'
- en: '| SSP-NeRF [[289](#bib.bib289)] | 2022 | Video | Arbitrary | No sync | No |
    3DMM |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| SSP-NeRF [[289](#bib.bib289)] | 2022 | 视频 | 任意 | 无同步 | 否 | 3DMM |'
- en: '| DM | Yu et al. [[290](#bib.bib290)] | 2022 | Audio | Arbitrary | Sync | Facial
    motion | No |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| DM | Yu 等 [[290](#bib.bib290)] | 2022 | 音频 | 任意 | 同步 | 面部动作 | 否 |'
- en: '| Zhua et al. [[291](#bib.bib291)] | 2023 | Video | Arbitrary | Sync |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| Zhua 等 [[291](#bib.bib291)] | 2023 | 视频 | 任意 | 同步 |'
- en: '&#124; Eye blinking, &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛眨动，&#124;'
- en: '&#124; head pose &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部姿势 &#124;'
- en: '| 3DMM |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 3DMM |'
- en: '| DiffTalk [[33](#bib.bib33)] | 2023 | Audio | Arbitrary | Sync | No | No |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| DiffTalk [[33](#bib.bib33)] | 2023 | 音频 | 任意 | 同步 | 否 | 否 |'
- en: '| Xu et al. [[292](#bib.bib292)] | 2023 | Audio, text | Arbitrary | No sync
    | Emotion | 3DMM |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Xu 等 [[292](#bib.bib292)] | 2023 | 音频，文本 | 任意 | 无同步 | 情感 | 3DMM |'
- en: 5.4.2 Talking Head Generation
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2 谈话头生成
- en: Given a target face image and a speech clip, TH generation aims at synthesizing
    a sequence of target face images where the lip motion, head pose, and facial expressions
    are synchronized with the audio. Significantly different from the speech-to-face
    generation task, which extracts the identity of the speaker from the given speech,
    the TH generation task focuses on the content of the speech.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 给定目标面部图像和语音片段，TH 生成的目标是合成一系列目标面部图像，其中嘴唇动作、头部姿态和面部表情与音频同步。与从给定语音中提取说话者身份的语音到面部生成任务显著不同，TH
    生成任务关注的是语音的内容。
- en: Multi-modal Feature Extraction. A VGG-M network pre-trained on the VGG Face
    dataset [[293](#bib.bib293)] is employed in [[281](#bib.bib281)] to learn face
    features and a speech encoder modified from VGG-M is used to learn speech embedding.
    Three temporal encoders are used to extract representations of the speaker’s identity,
    the audio segment, and the facial expressions, and a polynomial fusion layer is
    designed to generate a joint representation of the three encodings [[260](#bib.bib260)].
    Differently, Mittal et al. [[266](#bib.bib266)] develop a VAE to disentangle the
    phonetic content, emotional tone, and other factors into different representations
    solely from the input audio signal. To effectively disentangle each motion factor
    and achieve fine-grained controllable TH generation, Wang et al. [[278](#bib.bib278)]
    propose a progressive disentangled representation strategy by separating the factors
    in a coarse-to-fine manner, where we first extract unified motion feature from
    the driving signal, and then isolate each fine-grained motion from the unified
    feature. A pre-trained audio-to-AU module is employed in [[270](#bib.bib270)]
    to extract the speech-related AU information from speech.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态特征提取。一种在 VGG Face 数据集上预训练的 VGG-M 网络[[293](#bib.bib293)]被用于[[281](#bib.bib281)]以学习面部特征，经过修改的
    VGG-M 语音编码器用于学习语音嵌入。三个时间编码器用于提取说话者身份、音频片段和面部表情的表示，并且设计了一个多项式融合层来生成这三种编码的联合表示[[260](#bib.bib260)]。与此不同，Mittal
    等人[[266](#bib.bib266)]开发了一种变分自编码器 (VAE)，以从输入音频信号中将语音内容、情感语调和其他因素解缠成不同的表示。为了有效解缠每个运动因素并实现细粒度的可控
    TH 生成，Wang 等人[[278](#bib.bib278)]提出了一种渐进式解缠表示策略，通过粗到细的方式分离因素，其中首先从驱动信号中提取统一的运动特征，然后从统一特征中隔离每个细粒度运动。预训练的音频到
    AU 模块在[[270](#bib.bib270)]中用于从语音中提取与语音相关的 AU 信息。
- en: Multi-modal Learning. For TH video generation, speech-synchronized lip movement,
    facial expressions, and head pose generation are key factors. Therefore, in the
    training stage, audio-visual cross-modal correlation learning is necessary for
    the consistency of these facial movements in a sequence. Chen et al. [[255](#bib.bib255)]
    propose an audio-visual correlation loss to synchronize lip changes and speech
    changes in a video regarding that variation along the temporal axis between two
    modalities are more likely correlated, specifically, the cosine similarity loss
    is used to maximize the correlation between the derivative of audio feature and
    visual variations. For joint audio-visual representation learning, Zhou et al.
    [[257](#bib.bib257)] enforces the audio features and visual features to share
    a classifier so that they can share the same distribution, additionally, a contrastive
    loss is employed to close the paired audio and visual features. Eskimez et al.
    [[263](#bib.bib263)] designs a pair discriminator to improve the synchronization
    between the mouth shape and the input speech in the generated video. Zhu et al.
    [[267](#bib.bib267)] introduces the theory of mutual information neural estimation
    in talking face generation task to learn the cross-modal coherence.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习。对于 TH 视频生成，语音同步的嘴唇运动、面部表情和头部姿态生成是关键因素。因此，在训练阶段，需要进行音视频跨模态相关性学习，以确保这些面部动作在序列中的一致性。Chen
    等人[[255](#bib.bib255)]提出了一种音视频相关性损失，用于同步视频中的嘴唇变化和语音变化，考虑到两个模态之间沿时间轴的变化更可能是相关的，具体而言，使用余弦相似度损失来最大化音频特征和视觉变化之间的相关性。对于联合音视频表示学习，Zhou
    等人[[257](#bib.bib257)]强制音频特征和视觉特征共享一个分类器，以便它们可以共享相同的分布，此外，采用对比损失来缩小配对的音频和视觉特征之间的距离。Eskimez
    等人[[263](#bib.bib263)]设计了一对判别器，以提高生成视频中嘴型与输入语音之间的同步性。Zhu 等人[[267](#bib.bib267)]在谈话面孔生成任务中引入了互信息神经估计理论，以学习跨模态的一致性。
- en: Generation Model. The development of DL-based methods including CNN, RNN, GAN,
    Variational Autoencoder (VAE), Neural Radiance Fields (NeRF), and diffusion model
    (DM) have been explored in recent years. We compare the difference among them
    in Table [VIII](#S5.T8 "TABLE VIII ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking
    Head Generation ‣ 5 Automatic Body Language Generation ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation") and Table [IX](#S5.T9
    "TABLE IX ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking Head Generation ‣ 5
    Automatic Body Language Generation ‣ A Survey on Deep Multi-modal Learning for
    Body Language Recognition and Generation").
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型。近年来，基于DL的方法包括CNN、RNN、GAN、变分自编码器（VAE）、神经辐射场（NeRF）和扩散模型（DM）的发展得到了探索。我们在表格[
    VIII](#S5.T8 "TABLE VIII ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking Head
    Generation ‣ 5 Automatic Body Language Generation ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation")和表格[ IX](#S5.T9 "TABLE
    IX ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking Head Generation ‣ 5 Automatic
    Body Language Generation ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation")中比较了它们之间的差异。
- en: The GAN-based methods are the mainstream for TH generation, in particular, because
    of their ability to synthesize data before the stronger generator DM emerged.
    In Table [VIII](#S5.T8 "TABLE VIII ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking
    Head Generation ‣ 5 Automatic Body Language Generation ‣ A Survey on Deep Multi-modal
    Learning for Body Language Recognition and Generation"), we briefly list the recent
    works related to TH generation based on the GAN framework. Chen et al. [[255](#bib.bib255)]
    proposes a three-stream GAN to generate speech-synchronized lip video. Wang et
    al. [[262](#bib.bib262)] uses the GAN base network with an attentional mechanism
    to identify features related to head information. Zhang et al. [[268](#bib.bib268)]
    designs a FACIAL-GAN to encoder explicit and implicit attribute information for
    talking face video generation with audio-synchronized lip motion, personalized
    and natural head motion, and realistic eye blinks.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GAN的方法是TH生成的主流，特别是由于它们在更强大的生成器DM出现之前合成数据的能力。在表格[ VIII](#S5.T8 "TABLE VIII
    ‣ 5.4.1 Speech-to-face Generation ‣ 5.4 Talking Head Generation ‣ 5 Automatic
    Body Language Generation ‣ A Survey on Deep Multi-modal Learning for Body Language
    Recognition and Generation")中，我们简要列出了基于GAN框架的TH生成的近期相关工作。陈等人[[255](#bib.bib255)]提出了一种三流GAN来生成与语音同步的唇部视频。王等人[[262](#bib.bib262)]使用带有注意力机制的GAN基础网络来识别与头部信息相关的特征。张等人[[268](#bib.bib268)]设计了一种FACIAL-GAN，用于编码用于生成带有音频同步唇部动作、个性化和自然的头部动作以及逼真的眼睑眨动的显性和隐性属性信息的对话面部视频。
- en: In addition to GAN-based approaches, inspired by the NeRF [[294](#bib.bib294)],
    Guo et al. [[32](#bib.bib32)] develops the audio-driven NeRF (AD-NeRF) model for
    TH synthesis, in which an implicit neural scene representation function is learned
    to map audio features to dynamic neural radiation fields for speaker face rendering.
    However, AD-NeRF often suffers from head and torso separation during the rendering
    stage. Therefore, a semantic-aware speaker portrait NeRF (SSP-NeRF) is proposed
    by Liu et al. [[289](#bib.bib289)]. They employ the semantic awareness of speech
    to address the problem of incongruity between local dynamics and global torso.
    The problem of slow rendering speed can not be ignored. To improve the real-time
    performance, Yao et al. [[287](#bib.bib287)] proposes a NeRF method that takes
    lip movement features and personalized attributes as two disentangled conditions,
    where lip movements are directly predicted from the audio inputs to achieve lip-synchronized
    generation.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于GAN的方法之外，受到NeRF的启发[[294](#bib.bib294)]，郭等人[[32](#bib.bib32)]开发了用于TH合成的音频驱动NeRF（AD-NeRF）模型，其中学习了一个隐式神经场景表示函数，将音频特征映射到动态神经辐射场进行说话人面部渲染。然而，AD-NeRF在渲染阶段经常遭遇头部和躯干分离的问题。因此，刘等人[[289](#bib.bib289)]提出了一种语义感知的说话人肖像NeRF（SSP-NeRF）。他们利用语音的语义感知来解决局部动态与整体躯干之间的不一致问题。渲染速度慢的问题也不能忽视。为了提高实时性能，姚等人[[287](#bib.bib287)]提出了一种NeRF方法，该方法将唇部运动特征和个性化属性作为两个解耦条件，其中唇部运动直接从音频输入中预测，以实现唇部同步生成。
- en: Diffusion Probabilistic Models (DM) have shown strong ability in various generation
    tasks [[295](#bib.bib295), [296](#bib.bib296)]. Zhua et al. [[291](#bib.bib291)]
    proposes an audio-driven diffusion model for TH video generation, in which the
    lip motion features are aligned with the TH by contrastive learning. Yu et al.
    [[290](#bib.bib290)] proposes audio-to-visual diffusion prior trained on top of
    the mapping between audio and disentangled non-lip facial representations to semantically
    match the input audio while still maintaining both the photo-realism of audio-lip
    synchronization and the overall naturalness. Shen et al. [[33](#bib.bib33)] employs
    the emerging powerful diffusion models and model the TH generation as an audio-driven
    temporally coherent denoising process (DiffTalk). Xu et al. [[292](#bib.bib292)]
    first represents the emotion in the text prompt, which could inherit rich semantics
    from the CLIP, allowing flexible and generalized emotion control.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散概率模型（DM）在各种生成任务中表现出了强大的能力 [[295](#bib.bib295), [296](#bib.bib296)]。Zhua 等人
    [[291](#bib.bib291)] 提出了一个基于音频驱动的扩散模型用于 TH 视频生成，其中嘴唇运动特征通过对比学习与 TH 对齐。Yu 等人 [[290](#bib.bib290)]
    提出了在音频与解缠的非嘴唇面部表示之间映射的基础上训练的音频到视觉扩散先验，以语义匹配输入音频，同时保持音频-嘴唇同步的照片真实感和整体自然性。Shen 等人
    [[33](#bib.bib33)] 采用了新兴的强大扩散模型，将 TH 生成建模为音频驱动的时间一致的去噪过程（DiffTalk）。Xu 等人 [[292](#bib.bib292)]
    首次在文本提示中表示情感，这可以继承 CLIP 的丰富语义，从而实现灵活和广泛的情感控制。
- en: 'For better facial appearance transfer, intermediate faces such as 2D landmarks
    or 3DMM are widely used in TH generation. Figure. [13](#S5.F13 "Figure 13 ‣ 5.4.2
    Talking Head Generation ‣ 5.4 Talking Head Generation ‣ 5 Automatic Body Language
    Generation ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition
    and Generation") illustrates a simplified pipeline of the TH generation methods
    based on intermediate face, which mainly consists of two steps: low-dimensional
    driving source data are mapped into facial parameters; then rendering network
    is used to convert the learned facial parameters into high-dimensional video output.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地进行面部外观迁移，在 TH 生成中广泛使用了中间脸部数据，如 2D 关键点或 3DMM。图 [13](#S5.F13 "图 13 ‣ 5.4.2
    对话头生成 ‣ 5.4 对话头生成 ‣ 5 自动化肢体语言生成 ‣ 深度多模态学习在肢体语言识别与生成中的调查") 说明了基于中间脸部的 TH 生成方法的简化流程，主要包括两个步骤：将低维度的驱动源数据映射为面部参数；然后使用渲染网络将学到的面部参数转换为高维度的视频输出。
- en: '![Refer to caption](img/6838197f3fa0e06978982ea581322746.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6838197f3fa0e06978982ea581322746.png)'
- en: 'Figure 13: The typical pipeline of TH generation methods based on the intermediate
    face.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：基于中间脸部的 TH 生成方法的典型流程。
- en: 'Evaluation Metrics. Various perspectives reveal that the generated text-to-speech
    (TTS) output lacks the authenticity of human speech: (a) The target individual’s
    face should match that of the synthetic video’s speaker, (b) The generated speaker’s
    mouth should synchronize the audio, (c) The produced TH video should be of a good
    caliber, (d) The expression of the speaker in the generated video should be natural
    and match the emotion of the audio, and (e) Eye blinking should be expected when
    talking. Thus, the quantitative metrics of TH generation can be classified from
    these five views, as shown in Table [X](#S5.T10 "TABLE X ‣ 5.4.2 Talking Head
    Generation ‣ 5.4 Talking Head Generation ‣ 5 Automatic Body Language Generation
    ‣ A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation").'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。从不同角度来看，生成的文本到语音（TTS）输出缺乏人类语音的真实性：（a）目标个体的面孔应与合成视频的讲者面孔匹配，（b）生成的讲者的嘴唇应与音频同步，（c）生成的
    TH 视频应具备良好的质量，（d）生成视频中讲者的表情应自然，并与音频的情感匹配，以及（e）谈话时应有眨眼现象。因此，TH 生成的定量指标可以从这五个视角进行分类，如表
    [X](#S5.T10 "表 X ‣ 5.4.2 对话头生成 ‣ 5.4 对话头生成 ‣ 5 自动化肢体语言生成 ‣ 深度多模态学习在肢体语言识别与生成中的调查")
    所示。
- en: 'TABLE X: Summary of quantitative metrics of Talking Head Generation'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 表 X：对话头生成的定量指标总结
- en: '| Metrics’ degree | Metrics |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 指标级别 | 指标 |'
- en: '| Identity-preserving |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 保持身份 |'
- en: '&#124; PSNR, SSIM [[297](#bib.bib297)], FID, LMD, &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR, SSIM [[297](#bib.bib297)], FID, LMD, &#124;'
- en: '&#124; LPIPS [[298](#bib.bib298)], CSIM, IS, ACD [[299](#bib.bib299)] &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LPIPS [[298](#bib.bib298)], CSIM, IS, ACD [[299](#bib.bib299)] &#124;'
- en: '|'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Audio-visual &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视听 &#124;'
- en: '&#124; synchronization &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 同步 &#124;'
- en: '|'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; AV Conf, AV Off [[300](#bib.bib300)], &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AV Conf, AV Off [[300](#bib.bib300)], &#124;'
- en: '&#124; WER [[259](#bib.bib259)], LMD[m] [[272](#bib.bib272)], [[299](#bib.bib299)],
    &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WER [[259](#bib.bib259)], LMD[m] [[272](#bib.bib272)], [[299](#bib.bib299)],
    &#124;'
- en: '&#124; Sync[conf] [[272](#bib.bib272)], &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Sync[conf] [[272](#bib.bib272)], &#124;'
- en: '&#124; LRSD, LRA [[98](#bib.bib98)] &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRSD, LRA [[98](#bib.bib98)] &#124;'
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Image quality preserving | CPBD [[301](#bib.bib301)], FDBM [[302](#bib.bib302)]
    |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 图像质量保持 | CPBD [[301](#bib.bib301)], FDBM [[302](#bib.bib302)] |'
- en: '| Expression | Classification accuracy [[303](#bib.bib303)] |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 表达方式 | 分类准确率 [[303](#bib.bib303)] |'
- en: '| Eye blinking |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 眼睛眨动 |'
- en: '&#124; EAR [[304](#bib.bib304)], Blink rate, &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EAR [[304](#bib.bib304)], Blink rate, &#124;'
- en: '&#124; Blink median duration [[268](#bib.bib268)] &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Blink median duration [[268](#bib.bib268)] &#124;'
- en: '|'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Audio Input Pre-processing. Most of the TH generation works are audio signal
    driven. Here, we will introduce how previous work has dealt with speech signals
    in this field. In general, the audio waveform is resampled at 16KHz, and then
    the audio feature is computed [[305](#bib.bib305)]. Spectrogram, MFCC, and Fbank
    are the three mostly used audio features. Fang et al. [[250](#bib.bib250)] performs
    an ablation experiment on these three audio features, and they found that Fbank
    achieved the best performance, while the Spectrogram performed the worst FID.
    The reasons they guessed that Spectrogram contained much redundant information,
    MFCC discarded some related information, and Fbank kept balance. However, MFCC
    is used the most in the talking face generation.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 音频输入预处理。大多数TH生成工作是以音频信号为驱动的。在这里，我们将介绍以前的工作如何处理该领域的语音信号。一般来说，音频波形在16KHz下重新采样，然后计算音频特征[[305](#bib.bib305)]。谱图、MFCC和Fbank是三种最常用的音频特征。Fang等人[[250](#bib.bib250)]对这三种音频特征进行了消融实验，他们发现Fbank的表现最好，而谱图的FID表现最差。他们推测的原因是谱图包含了很多冗余信息，MFCC丢弃了一些相关信息，而Fbank保持了平衡。然而，MFCC在谈话面生成中使用最广泛。
- en: 6 Challenges of BL Recognition and Generation
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 BL识别和生成的挑战
- en: 'The existing BL recognition and generation methods have not been capable of
    meeting real-world requirements under exposure to various challenges. In order
    to fully demonstrate the typical challenges of BL recognition and generation in
    the field of BL, we elaborate in detail on SL, CS, and TH from three aspects:
    subtasks challenges, datasets challenges and evaluation metrics challenges.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的BL识别和生成方法尚未能够满足实际世界中的各种挑战。为了全面展示BL领域中的BL识别和生成的典型挑战，我们从三个方面详细阐述SL、CS和TH：子任务挑战、数据集挑战和评估指标挑战。
- en: 6.1 Subtasks Challenges
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 子任务挑战
- en: To more fully illustrate the challenges of BL recognition and generation tasks,
    we split each major task into three subtasks, i.e., Lip reading, SL recognition,
    and CS recognition. From the perspective of task definition, the TH task itself
    is more focused on the generation process. Moreover, limited by the development
    of the existing TH generation, it is difficult for researchers to capture the
    basic facial attributes of the target speaker. The existing studies lack an exploration
    of TH recognition, so the challenge of TH recognition is not included in the discussion
    of subtask challenges in this survey. Current research on CoS predominantly concentrates
    on CoS gesture generation. While some studies have demonstrated a positive impact
    on the CoS Generation task, the majority of recent works do not prioritize CoS
    Recognition as a primary focus. So the challenge of CoS Recognition is not included
    in the discussion of subtask challenges in this survey.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更全面地阐述BL识别和生成任务的挑战，我们将每个主要任务分为三个子任务，即唇读、手语识别和口语识别。从任务定义的角度来看，TH任务本身更侧重于生成过程。此外，由于现有TH生成技术的发展限制，研究人员难以捕捉目标说话者的基本面部属性。现有研究缺乏对TH识别的探索，因此TH识别的挑战没有在本调查中讨论子任务挑战时涉及。当前对CoS的研究主要集中在CoS手势生成上。尽管一些研究表明对CoS生成任务有积极影响，但大多数近期工作并未将CoS识别作为主要关注点。因此，CoS识别的挑战未包含在本调查的子任务挑战讨论中。
- en: The challenges of BL recognition tasks are mainly due to the efficiency of the
    cross-modal feature fusion, and the specific challenges of each subtask are as
    follows.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: BL识别任务的挑战主要由于跨模态特征融合的效率，而每个子任务的具体挑战如下。
- en: •
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Lip Reading. There are two primary challenges in automatic lip reading: intra-class
    difference and inter-class similarity. The former is hindered by factors such
    as speech emotion, speed, gender, age, skin color, and speech habits, making it
    difficult to distinguish variations within the same word category. Additionally,
    the semantic disparities between words used in different contexts significantly
    impact lip reading. The latter challenge stems from the abundance of word categories,
    leading to challenges in visually distinguishing similar-looking words belonging
    to different classes. Addressing these challenges is crucial for improving the
    accuracy and effectiveness of lip reading recognition systems, which have valuable
    implications for aiding communication for individuals with hearing impairments
    and advancing the field’s applications.'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**唇读**。自动唇读主要面临两个挑战：类内差异和类间相似性。前者受到诸如语音情感、速度、性别、年龄、肤色和语音习惯等因素的影响，使得区分同一单词类别内的变化变得困难。此外，不同语境下单词的语义差异对唇读有显著影响。后者挑战源于单词类别的丰富性，导致在视觉上区分属于不同类别的相似单词时出现困难。解决这些挑战对于提高唇读识别系统的准确性和有效性至关重要，这对帮助听障人士沟通和推动该领域应用具有重要意义。'
- en: •
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sign Language Recognition. SL recognition encounters significant challenges
    arising from the pronounced variations in gestures, which seriously impede its
    accuracy. Moreover, factors like hand shape, illumination conditions, and resolution
    play pivotal roles in limiting SL recognition performance. Additionally, occlusion,
    including self-occlusion between fingers and occlusion between hands and other
    body parts, adversely affects feature fusion, becoming a key influencing factor
    in SL recognition. Another pressing challenge is the development of a real-time
    multilingual SL recognition system. Addressing these complexities is essential
    to advance the field and improve the efficiency and inclusivity of SL recognition
    technologies.
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**手语识别**。手语识别面临的主要挑战是手势变化显著，这严重影响了其准确性。此外，手部形状、照明条件和分辨率等因素在限制手语识别性能方面发挥了关键作用。此外，自遮挡（如手指间的自遮挡和手与其他身体部位之间的遮挡）对特征融合产生负面影响，成为手语识别中的关键影响因素。另一个紧迫的挑战是开发实时多语言手语识别系统。解决这些复杂问题对推动该领域的发展和提高手语识别技术的效率和包容性至关重要。'
- en: •
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Cued Speech Recognition. The primary obstacle in CS recognition is the hand
    preceding phenomenon [[41](#bib.bib41)], where the hand movements often occur
    faster than the corresponding lip movements, anticipating the next phoneme. This
    phenomenon hampers the efficiency of lip and hand feature fusion in CS recognition.
    Besides, due to variations in individual CS coding habits and styles, adaptability
    in multi-cuer scenarios is also a challenge.
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**提示语识别**。提示语识别中的主要障碍是手部提前现象[[41](#bib.bib41)]，即手部动作通常发生得比对应的唇部动作更快，预示着下一个音素。这种现象妨碍了提示语识别中唇部和手部特征的融合效率。此外，由于个体提示语编码习惯和风格的差异，多提示符场景下的适应性也是一个挑战。'
- en: The challenge of BL generation mainly stems from the stability and quality of
    the generated gesture, and the specific challenges of each subtask are as follows.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: BL生成的挑战主要源于生成手势的稳定性和质量，各子任务的具体挑战如下。
- en: •
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Cued Speech Generation. In conclusion, the CS generation faces several challenges
    that need to be addressed for the development of effective systems. The lack of
    large-scale annotated datasets, the complexity of modeling CS gestures, and the
    need for accurate asynchronous alignment between cued signs and spoken words are
    key challenges. Additionally, integrating audio and visual modalities and achieving
    generalization to new speakers and languages are important considerations. Overcoming
    these challenges through advancements in modeling ability, multi-modal fusion,
    and the availability of diverse datasets will contribute to the improvement of
    CS generation systems.
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**提示语生成**。总的来说，提示语生成面临几个挑战，需要解决这些挑战以开发有效的系统。缺乏大规模的标注数据集、提示语手势建模的复杂性以及提示符号与口语单词之间需要精确的异步对齐是主要挑战。此外，整合音频和视觉模态以及实现对新说话者和语言的泛化也是重要的考虑因素。通过建模能力、跨模态融合的进步以及多样化数据集的可用性来克服这些挑战，将有助于提高提示语生成系统的性能。'
- en: •
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sign Language Generation. In the realm of SL production, numerous obstacles
    warrant attention, chief among them being domain adaptation and model collapse.
    The former obstacle arises from the inherent variations in word styles and meanings
    across different languages, necessitating effective adaptation strategies. Furthermore,
    a noteworthy challenge lies in the limited proficiency of generating uncommon
    and unseen words, hindering the overall performance of the system. Moreover, the
    persisting issues of model collapse, non-convergence, and instability within generative
    models further compound the complexities faced in Sign Language production. Addressing
    these multifaceted challenges is crucial for advancing the SOTA in this domain
    and facilitating more reliable and robust SL generation.
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手语生成。在SL生产领域，存在许多需要关注的障碍，其中最主要的是领域适应和模型崩溃。前者障碍源于不同语言中词汇风格和意义的固有变化，需要有效的适应策略。此外，生成不常见和未见过的词汇的能力有限，阻碍了系统的整体表现。而且，生成模型中持续存在的模型崩溃、非收敛和不稳定性问题进一步加剧了手语生产中的复杂性。解决这些多方面的挑战对于推进该领域的SOTA并实现更可靠和稳健的SL生成至关重要。
- en: •
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Co-speech Generation. The generation process of CoS encounters challenges due
    to the presence of highly idiosyncratic and non-periodic spontaneous gestures.
    The accurate capture of finger motion poses difficulties, resulting in the manifestation
    of idiosyncratic gestures. Furthermore, the non-periodic nature of gestures arises
    from the substantial variation in gesture behavior.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语音同步生成。CoS的生成过程面临挑战，原因在于存在高度个性化和非周期性的自发手势。准确捕捉手指运动存在困难，导致个性化手势的表现。此外，手势的非周期性特征源于手势行为的显著变化。
- en: •
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Talking Head Generation. TH generation confronts two primary challenges: information
    coupling and diversity targets. The former encompasses the synchronization of
    multiple facial elements, such as head posture, facial expression, lip movement,
    and background motion, while also addressing the “uncanny valley effect” [[306](#bib.bib306)],
    a phenomenon common in face generation where generated faces appear almost human-like
    but lack true realism, leading to discomfort. The latter challenge pertains to
    harmonizing temporal resolution and speech features across diverse data modalities,
    along with the complexity of defining visual quality as a clear training objective.
    Overcoming these challenges is crucial for advancing the field and achieving a
    more realistic and visually coherent TH generation.'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 头部生成。TH生成面临两个主要挑战：信息耦合和多样性目标。前者包括多个面部元素的同步，如头部姿势、面部表情、嘴唇运动和背景运动，同时还要解决“恐怖谷效应”[[306](#bib.bib306)]，即生成的面孔看起来几乎像人类，但缺乏真正的逼真感，从而导致不适。后者挑战涉及在不同数据模态间协调时间分辨率和语音特征，以及将视觉质量明确作为训练目标的复杂性。克服这些挑战对于推动该领域的发展和实现更真实且视觉一致的TH生成至关重要。
- en: 6.2 Datasets Challenges
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 数据集挑战
- en: The current datasets for SL recognition and generation encounter significant
    limitations due to the high costs associated with data collection and manual annotation.
    This results in datasets with small-scale and weak annotations, hindering the
    progress of BL-related tasks. To create BL datasets, collaboration between language
    experts and native speakers is essential, further adding to the complexities and
    expenses involved. A potential solution to address these challenges is to explore
    self-supervised learning using unlabeled BL data [[307](#bib.bib307)], which could
    alleviate the need for extensive manual annotation.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，SL识别和生成的数据集由于数据收集和人工标注的高成本而面临重大限制。这导致数据集规模小且标注不足，阻碍了BL相关任务的进展。创建BL数据集需要语言专家和母语者之间的合作，这进一步增加了复杂性和费用。解决这些挑战的潜在方案是探索使用未标记BL数据的自监督学习[[307](#bib.bib307)]，这可能减轻对大量人工标注的需求。
- en: Moreover, privacy protection poses another hurdle, as some large BL datasets [[308](#bib.bib308),
    [309](#bib.bib309)] are not publicly accessible. In light of the high costs and
    privacy concerns, a viable approach is to leverage existing wild online videos
    to collect the necessary BL data. Similar to the training datasets used for Contrastive
    Language-Image Pre-training (CLIP) [[310](#bib.bib310)] and DALL-E [[311](#bib.bib311)],
    employing very large datasets can enhance the generalization capabilities of BL
    recognition and generation models.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，隐私保护构成了另一道障碍，因为一些大型BL数据集[[308](#bib.bib308), [309](#bib.bib309)]并未公开访问。鉴于高成本和隐私问题，一个可行的方法是利用现有的野外在线视频来收集所需的BL数据。类似于用于对比语言-图像预训练（CLIP）[[310](#bib.bib310)]和DALL-E[[311](#bib.bib311)]的训练数据集，使用非常大的数据集可以增强BL识别和生成模型的泛化能力。
- en: Apart from the dataset challenges, the high costs associated with collecting
    and annotating 3D data contribute to the scarcity of large-scale 3D BL datasets.
    Consequently, the development of 3D BL Generation faces significant obstacles
    in understanding and processing 3D BL data effectively. Overcoming these challenges
    is essential to advance the field of BL recognition and generation, allowing for
    more efficient and accurate communication support for individuals with hearing
    impairments.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据集挑战，收集和标注3D数据的高成本也导致了大型3D BL数据集的稀缺。因此，3D BL生成的发展面临着有效理解和处理3D BL数据的重大障碍。克服这些挑战对于推进BL识别和生成领域至关重要，从而为听力障碍人士提供更高效和准确的沟通支持。
- en: 6.3 Evaluation Metrics Challenges
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 评价指标挑战
- en: The primary nature of the BL recognition task lies in its classification essence,
    where simple and efficient classification accuracy serves as the prevalent evaluation
    metric. However, this paper shifts its focus to the BL generation task and the
    challenges it poses in terms of evaluation metrics. Subjective metrics utilized
    in the BL generation task prove to be costly, time-consuming and lack scalability.
    Metrics like human likeness and gesture appropriateness, although valuable, suffer
    from non-replicability and instability issues. On the other hand, objective metrics
    such as PSNR, SSIM, FID [[312](#bib.bib312)] and LRSD [[17](#bib.bib17)] offer
    advantages over subjective ones but come with limitations in assessing the similarity
    between gesture and speech, as well as the semantic appropriateness of gestures.
    Notably, unlike subjective metrics that evaluate human likeness, the existing
    literature rarely quantifies objective metrics measuring gesture diversity or
    various motion appropriateness aspects. These challenges highlight the need for
    robust and comprehensive evaluation metrics in the BL generation domain to ensure
    an accurate and meaningful assessment of generated Sign Language outputs.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: BL识别任务的主要性质在于其分类本质，其中简单高效的分类准确度作为流行的评价指标。然而，本文将焦点转向BL生成任务及其在评价指标方面的挑战。在BL生成任务中使用的主观指标成本高、耗时且缺乏可扩展性。虽然人类相似度和手势适当性等指标具有价值，但存在不可重复性和不稳定性问题。另一方面，客观指标如PSNR、SSIM、FID[[312](#bib.bib312)]和LRSD[[17](#bib.bib17)]相比主观指标有其优势，但在评估手势与语音的相似性以及手势的语义适当性方面存在局限性。值得注意的是，与评估人类相似度的主观指标不同，现有文献中很少量化客观指标来衡量手势多样性或各种运动适当性方面。这些挑战突显了在BL生成领域中需要稳健且全面的评价指标，以确保对生成的手语输出进行准确和有意义的评估。
- en: 7 Future Discussions
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 未来讨论
- en: 'Through an extensive summary and analysis of the existing literature, this
    survey offers the following discussions and new insights:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对现有文献的广泛总结和分析，本调查提供了以下讨论和新见解：
- en: '1.'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The integration of large-scale multi-modal BL datasets and the establishment
    of a unified low-loss data format are key factors in advancing BL recognition
    and generation tasks. By collecting extensive datasets from diverse online videos,
    we can enhance the generalization and robustness of BL recognition and generation
    models for real-world scenarios. Additionally, the adoption of a unified data
    standard and adaptable conversion method allows for the seamless integration of
    different datasets and facilitates collaboration among researchers. This promotes
    interoperability between models, enabling efficient sharing and utilization of
    resources within the research community.
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大规模多模态BL数据集的整合和统一低损数据格式的建立是推进BL识别和生成任务的关键因素。通过从各种在线视频中收集广泛的数据集，我们可以增强BL识别和生成模型在现实世界场景中的泛化能力和鲁棒性。此外，采用统一的数据标准和适应性转换方法可以实现不同数据集的无缝集成，并促进研究人员之间的合作。这有助于模型之间的互操作性，促进资源的高效共享和利用。
- en: '2.'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Recently, large-scale pre-training models such as ChatGPT have achieved outstanding
    performance in various visual-linguistic cross-modal tasks. For instance, CLIP
    and various variations of the multi-modal CLIP model have emerged. However, they
    have the following drawbacks: a) they might not deeply connect different types
    of data as effectively as specialized models; b) they demand in terms of computing
    power due to their size. c) This model might not allow fine-tuning for specific
    tasks and could struggle with specialized knowledge; d) it needs a lot of diverse
    data to work well and could be hard to interpret. To this end, how to build a
    large-scale multi-modal model for BL recognition and generation is a promising
    topic.'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近，大规模预训练模型如ChatGPT在各种视觉-语言跨模态任务中取得了出色的表现。例如，CLIP及其各种多模态CLIP模型的变体已出现。然而，它们存在以下缺陷：a)
    它们可能无法像专业模型那样有效地深度连接不同类型的数据；b) 由于其规模，它们对计算能力的需求较高；c) 这些模型可能无法针对特定任务进行微调，并且可能在处理专业知识时存在困难；d)
    它们需要大量多样的数据才能良好运行，且可能难以解释。为此，如何构建一个用于BL识别和生成的大规模多模态模型是一个有前景的课题。
- en: '3.'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Besides, it was found that the ability of existing large-scale pre-training
    models to learn fine-grain features still needs to be improved [[313](#bib.bib313)].
    In BL, fine-grained feature learning is essential, For example, hand positions
    and lip movements in CS and CoS needed to be accurately recognized and generated
    to ensure clarity and avoid ambiguity. Therefore, fine-grained BL recognition
    and generation is a feasible direction to improve their performance.
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，现有大规模预训练模型在学习细粒度特征的能力上仍有待提高[[313](#bib.bib313)]。在BL中，细粒度特征学习至关重要，例如，CS和CoS中的手部位置和唇部动作需要准确识别和生成，以确保清晰度并避免歧义。因此，细粒度BL识别和生成是提高其性能的一个可行方向。
- en: '4.'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: The multi-modal models in the task of BL recognition and generation are very
    susceptible to the perturbations (attacks) of different modalities, resulting
    in serious performance degradation. How to pre-train a robust and secure multimodal
    large-scale model for BL recognition and generation is an urgent problem to be
    solved.
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在BL识别和生成任务中，多模态模型对不同模态的扰动（攻击）非常敏感，导致性能严重下降。如何为BL识别和生成预训练一个鲁棒且安全的大规模多模态模型是一个亟待解决的问题。
- en: '5.'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: An essential requirement for BL recognition and generation systems is real-time
    capability, especially for multilingual and multiple-speakers scenarios. Creating
    a real-time system is vital to cater to the needs of both the deaf and speaking
    communities. However, existing audio-visual datasets are predominantly monolingual,
    with English being the most commonly represented language. In practical applications,
    multilingual communication is often necessary, highlighting the need for diverse
    datasets. Additionally, current methods for BL recognition and generation are
    often limited to specific target identities, as different speakers exhibit significant
    variations in appearance and habits. Overcoming these challenges is crucial to
    develop adaptable and effective real-time BL systems that accommodate various
    languages and diverse speakers.
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BL 识别和生成系统的一个基本要求是实时能力，尤其是在多语言和多说话者场景中。创建一个实时系统对于满足聋人和听力正常人群的需求至关重要。然而，现有的音视频数据集主要是单语言的，其中以英语为最常见的语言。在实际应用中，往往需要多语言沟通，这突显了多样化数据集的必要性。此外，目前的
    BL 识别和生成方法通常仅限于特定目标身份，因为不同的说话者在外貌和习惯上存在显著差异。克服这些挑战对于开发适应各种语言和多样说话者的有效实时 BL 系统至关重要。
- en: 8 Conclusion
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: This survey has delved into the realm of deep multi-modal learning for automatic
    BL recognition and generation, shedding light on its potential and challenges.
    This survey focuses on four classical BL variants, i.e., Sign Language, Cued Speech,
    Co-speech, and Talking Head. Through a meticulous examination of various modalities,
    including visual, auditory, and textual data, and their integration, we have explored
    the intricacies of capturing and interpreting these four BL. By reviewing SOTA
    methodologies, such as feature fusion, representation learning, recognition, and
    generation methods, we have uncovered the strengths and limitations of current
    approaches. The significance of datasets and benchmarks in facilitating research
    progress was also emphasized, with a focus on annotation methodologies and evaluation
    metrics. Despite the progress, challenges persist, demanding the creation of diverse
    datasets, addressing limited labeled data, enhancing model interpretability, and
    ensuring robustness across environments and cultural contexts. Looking ahead,
    the future holds promises of more sophisticated architectures and training strategies,
    harnessing the complementary nature of multi-modal data and leveraging advancements
    in multi-modal learning, large-scale pre-trained model, self-supervised learning,
    and reinforcement learning. As this research area evolves, it is poised to revolutionize
    human-human and human-machine interactions, fostering natural and effective communication
    across domains.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查深入探讨了用于自动 BL 识别和生成的深度多模态学习领域，揭示了其潜力和挑战。本调查关注四种经典的 BL 变体，即手语、提示语音、共语音和发言头。通过对包括视觉、听觉和文本数据在内的各种模态及其整合的详细审查，我们探讨了捕捉和解释这四种
    BL 的复杂性。通过回顾 SOTA 方法，如特征融合、表示学习、识别和生成方法，我们揭示了当前方法的优点和局限性。我们还强调了数据集和基准在促进研究进展中的重要性，重点关注了标注方法和评估指标。尽管取得了一些进展，挑战仍然存在，需要创建多样化的数据集，解决标注数据有限的问题，增强模型的可解释性，并确保在不同环境和文化背景下的鲁棒性。展望未来，前景充满希望，包括更复杂的架构和训练策略，利用多模态数据的互补性，发挥多模态学习、大规模预训练模型、自监督学习和强化学习的进步。随着这一研究领域的发展，它有望彻底改变人际和人机交互，促进跨领域的自然和有效沟通。
- en: References
  id: totrans-438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. O. Cornett, “Cued speech,” *American annals of the deaf*, vol. 112,
    no. 1, pp. 3–13, 1967.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. O. Cornett， “提示语音，” *American annals of the deaf*，第 112 卷，第 1 期，第 3–13
    页，1967年。'
- en: '[2] B. Joksimoski, E. Zdravevski, P. Lameski, I. M. Pires, F. J. Melero, T. P.
    Martinez, N. M. Garcia, M. Mihajlov, I. Chorbev, and V. Trajkovik, “Technological
    solutions for sign language recognition: A scoping review of research trends,
    challenges, and opportunities,” *IEEE Access*, vol. 10, pp. 40 979–40 998, 2022.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] B. Joksimoski, E. Zdravevski, P. Lameski, I. M. Pires, F. J. Melero, T.
    P. Martinez, N. M. Garcia, M. Mihajlov, I. Chorbev, 和 V. Trajkovik， “手语识别的技术解决方案：对研究趋势、挑战和机遇的范围审查，”
    *IEEE Access*，第 10 卷，第 40 979–40 998 页，2022年。'
- en: '[3] X. Liu, Q. Wu, H. Zhou, Y. Du, W. Wu, D. Lin, and Z. Liu, “Audio-driven
    co-speech gesture video generation,” *Advances in Neural Information Processing
    Systems (NIPS)*, vol. 35, pp. 21 386–21 399, 2022.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] X. Liu, Q. Wu, H. Zhou, Y. Du, W. Wu, D. Lin, 和 Z. Liu, “音频驱动的共语手势视频生成，”
    *Advances in Neural Information Processing Systems (NIPS)*，第35卷，第21 386–21 399页，2022年。'
- en: '[4] B. Zhang, C. Qi, P. Zhang, B. Zhang, H. Wu, D. Chen, Q. Chen, Y. Wang,
    and F. Wen, “Metaportrait: Identity-preserving talking head generation with fast
    personalized adaptation,” in *Proc. IEEE/CVF-CVRP*, 2023, p. 22096–22105.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] B. Zhang, C. Qi, P. Zhang, B. Zhang, H. Wu, D. Chen, Q. Chen, Y. Wang,
    和 F. Wen, “Metaportrait：具有快速个性化适应的身份保留式谈话头生成，” 发表在 *Proc. IEEE/CVF-CVRP*，2023年，第22096–22105页。'
- en: '[5] R. Rastgoo, K. Kiani, S. Escalera, and M. Sabokrou, “Sign language production:
    A review,” in *Proc. IEEE/CVF-CVRP*, 2021, pp. 3451–3461.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] R. Rastgoo, K. Kiani, S. Escalera, 和 M. Sabokrou, “手语生成：综述，” 发表在 *Proc.
    IEEE/CVF-CVRP*，2021年，第3451–3461页。'
- en: '[6] S. Nyatsanga, T. Kucherenko, C. Ahuja, G. E. Henter, and M. Neff, “A comprehensive
    review of data-driven co-speech gesture generation,” in *Computer Graphics Forum*,
    vol. 42, no. 2.   Wiley Online Library, 2023, pp. 569–596.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] S. Nyatsanga, T. Kucherenko, C. Ahuja, G. E. Henter, 和 M. Neff, “基于数据驱动的共语手势生成的全面综述，”
    发表在 *Computer Graphics Forum*，第42卷，第2期。 Wiley Online Library，2023年，第569–596页。'
- en: '[7] R. Rastgoo, K. Kiani, and S. Escalera, “Sign language recognition: A deep
    survey,” *Expert Systems with Applications*, vol. 164, p. 113794, 2021.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] R. Rastgoo, K. Kiani, 和 S. Escalera, “手语识别：深度综述，” *Expert Systems with
    Applications*，第164卷，第113794页，2021年。'
- en: '[8] A. Fernandez-Lopez and F. M. Sukno, “Survey on automatic lip-reading in
    the era of deep learning,” *Image and Vision Computing*, vol. 78, pp. 53–72, 2018.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Fernandez-Lopez 和 F. M. Sukno, “深度学习时代的自动唇读综述，” *Image and Vision Computing*，第78卷，第53–72页，2018年。'
- en: '[9] S. Fenghour, D. Chen, K. Guo, B. Li, and P. Xiao, “Deep learning-based
    automated lip-reading: A survey,” *IEEE Access*, vol. 9, pp. 121 184–121 205,
    2021.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Fenghour, D. Chen, K. Guo, B. Li, 和 P. Xiao, “基于深度学习的自动唇读：综述，” *IEEE
    Access*，第9卷，第121 184–121 205页，2021年。'
- en: '[10] R. Chand, P. Jain, A. Mathur, S. Raj, and P. Kanikar, “Survey on visual
    speech recognition using deep learning techniques,” in *Proc. IEEE-CSCITA*, 2023,
    pp. 72–77.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. Chand, P. Jain, A. Mathur, S. Raj, 和 P. Kanikar, “基于深度学习技术的视觉语音识别综述，”
    发表在 *Proc. IEEE-CSCITA*，2023年，第72–77页。'
- en: '[11] S. Bhaskar, T. Thasleema, and R. Rajesh, “A survey on different visual
    speech recognition techniques,” in *Data Analytics and Learning (DAL)*, 2018,
    pp. 307–316.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Bhaskar, T. Thasleema, 和 R. Rajesh, “不同视觉语音识别技术的综述，” 发表在 *Data Analytics
    and Learning (DAL)*，2018年，第307–316页。'
- en: '[12] N. Radha, A. Shahina *et al.*, “A survey on visual speech recognition
    approaches,” in *Proc. IEEE-ICAIS*, 2021, pp. 934–939.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] N. Radha, A. Shahina *等*，“视觉语音识别方法综述，” 发表在 *Proc. IEEE-ICAIS*，2021年，第934–939页。'
- en: '[13] O. Koller, “Quantitative survey of the state of the art in sign language
    recognition,” *arXiv preprint arXiv:2008.09918*, 2020.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] O. Koller, “手语识别前沿技术的定量综述，” *arXiv 预印本 arXiv:2008.09918*，2020年。'
- en: '[14] I. Adeyanju, O. Bello, and M. Adegboye, “Machine learning methods for
    sign language recognition: A critical review and analysis,” *Intelligent Systems
    with Applications*, vol. 12, p. 200056, 2021.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] I. Adeyanju, O. Bello, 和 M. Adegboye, “手语识别的机器学习方法：批判性综述与分析，” *Intelligent
    Systems with Applications*，第12卷，第200056页，2021年。'
- en: '[15] I. Papastratis, C. Chatzikonstantinou, D. Konstantinidis, K. Dimitropoulos,
    and P. Daras, “Artificial intelligence technologies for sign language,” *Sensors*,
    vol. 21, no. 17, p. 5843, 2021.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] I. Papastratis, C. Chatzikonstantinou, D. Konstantinidis, K. Dimitropoulos,
    和 P. Daras, “用于手语的人工智能技术，” *Sensors*，第21卷，第17期，第5843页，2021年。'
- en: '[16] D. M. Madhiarasan, P. Roy, and P. Pratim, “A comprehensive review of sign
    language recognition: Different types, modalities, and datasets,” *arXiv preprint
    arXiv:2204.03328*, 2022.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] D. M. Madhiarasan, P. Roy, 和 P. Pratim, “手语识别的全面综述：不同类型、模态和数据集，” *arXiv
    预印本 arXiv:2204.03328*，2022年。'
- en: '[17] L. Chen, G. Cui, Z. Kou, H. Zheng, and C. Xu, “What comprises a good talking-head
    video generation?: A survey and benchmark,” *arXiv preprint arXiv:2005.03201*,
    2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] L. Chen, G. Cui, Z. Kou, H. Zheng, 和 C. Xu, “一个好的谈话头视频生成包含什么？：综述与基准，”
    *arXiv 预印本 arXiv:2005.03201*，2020年。'
- en: '[18] T. Sha, W. Zhang, T. Shen, Z. Li, and T. Mei, “Deep person generation:
    A survey from the perspective of face, pose, and cloth synthesis,” *ACM Computing
    Surveys*, vol. 55, no. 12, pp. 1–37, 2023.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] T. Sha, W. Zhang, T. Shen, Z. Li, 和 T. Mei, “深度人物生成：从面部、姿态和服装合成的视角看综述，”
    *ACM Computing Surveys*，第55卷，第12期，第1–37页，2023年。'
- en: '[19] R. Zhen, W. Song, Q. He, J. Cao, L. Shi, and J. Luo, “Human-computer interaction
    system: A survey of talking-head generation,” *Electronics*, vol. 12, no. 1, p.
    218, 2023.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] R. Zhen, W. Song, Q. He, J. Cao, L. Shi, 和 J. Luo, “人机交互系统：谈话头像生成的综述，”
    *电子学*，第12卷，第1期，第218页，2023年。'
- en: '[20] C. Sheng, G. Kuang, L. Bai, C. Hou, Y. Guo, X. Xu, M. Pietikäinen, and
    L. Liu, “Deep learning for visual speech analysis: A survey,” *arXiv preprint
    arXiv:2205.10839*, 2022.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] C. Sheng, G. Kuang, L. Bai, C. Hou, Y. Guo, X. Xu, M. Pietikäinen, 和 L.
    Liu, “深度学习在视觉语音分析中的应用：综述，” *arXiv 预印本 arXiv:2205.10839*，2022年。'
- en: '[21] S. Stoll, N. C. Camgoz, S. Hadfield, and R. Bowden, “Text2sign: Towards
    sign language production using neural machine translation and generative adversarial
    networks,” *International Journal of Computer Vision*, vol. 128, pp. 891–908,
    2020.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Stoll, N. C. Camgoz, S. Hadfield, 和 R. Bowden, “Text2sign：利用神经机器翻译和生成对抗网络实现手语生产，”
    *国际计算机视觉杂志*，第128卷，第891–908页，2020年。'
- en: '[22] D. Guo, W. Zhou, H. Li, and M. Wang, “Hierarchical lstm for sign language
    translation,” in *Proc. Conf AAAI Artif. Intell.*, vol. 32, no. 1, 2018.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] D. Guo, W. Zhou, H. Li, 和 M. Wang, “用于手语翻译的层次LSTM，” 见 *Proc. Conf AAAI
    Artif. Intell.*，第32卷，第1期，2018年。'
- en: '[23] B. Saunders, N. C. Camgoz, and R. Bowden, “Everybody sign now: Translating
    spoken language to photo realistic sign language video,” *arXiv preprint arXiv:2011.09846*,
    2020.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] B. Saunders, N. C. Camgoz, 和 R. Bowden, “现在每个人都来手语：将口语翻译为照片级真实手语视频，” *arXiv
    预印本 arXiv:2011.09846*，2020年。'
- en: '[24] C. Ahuja, D. W. Lee, and L.-P. Morency, “Low-resource adaptation for personalized
    co-speech gesture generation,” in *Proc. IEEE/CVF-CVPR*, June 2022, pp. 20 566–20 576.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] C. Ahuja, D. W. Lee, 和 L.-P. Morency, “针对个性化共同语音手势生成的低资源适应，” 见 *Proc.
    IEEE/CVF-CVPR*，2022年6月，页码 20 566–20 576。'
- en: '[25] T. Ao, Q. Gao, Y. Lou, B. Chen, and L. Liu, “Rhythmic gesticulator: Rhythm-aware
    co-speech gesture synthesis with hierarchical neural embeddings,” *ACM Transactions
    on Graphics (TOG)*, vol. 41, no. 6, p. 1–19, 2022.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] T. Ao, Q. Gao, Y. Lou, B. Chen, 和 L. Liu, “节奏手势生成器：基于节奏的共同语音手势合成与分层神经嵌入，”
    *ACM Transactions on Graphics (TOG)*，第41卷，第6期，第1–19页，2022年。'
- en: '[26] Y. Liang, Q. Feng, L. Zhu, L. Hu, P. Pan, and Y. Yang, “Seeg: Semantic
    energized co-speech gesture generation,” in *Proc. IEEE/CVF-CVPR*, June 2022,
    pp. 10 473–10 482.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. Liang, Q. Feng, L. Zhu, L. Hu, P. Pan, 和 Y. Yang, “Seeg：语义驱动的共同语音手势生成，”
    见 *Proc. IEEE/CVF-CVPR*，2022年6月，页码 10 473–10 482。'
- en: '[27] X. Liu, Q. Wu, H. Zhou, Y. Xu, R. Qian, X. Lin, X. Zhou, W. Wu, B. Dai,
    and B. Zhou, “Learning hierarchical cross-modal association for co-speech gesture
    generation,” in *Proc. IEEE/CVF-CVPR*, 2022, pp. 10 462–10 472.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] X. Liu, Q. Wu, H. Zhou, Y. Xu, R. Qian, X. Lin, X. Zhou, W. Wu, B. Dai,
    和 B. Zhou, “学习层次化跨模态关联以生成共同语音手势，” 见 *Proc. IEEE/CVF-CVPR*，2022年，页码 10 462–10 472。'
- en: '[28] P. Duchnowski, L. D. Braida, D. Lum, M. Sexton, J. Krause, and S. Banthia,
    “Automatic generation of cued speech for the deaf: status and outlook,” in *International
    Conference on Auditory-Visual Speech Processing (AVSP)*, 1998.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] P. Duchnowski, L. D. Braida, D. Lum, M. Sexton, J. Krause, 和 S. Banthia,
    “为听障人士自动生成提示语音：现状与展望，” 见 *国际听觉-视觉语音处理会议 (AVSP)*，1998年。'
- en: '[29] G. Bailly, Y. Fang, F. Elisei, and D. Beautemps, “Retargeting cued speech
    hand gestures for different talking heads and speakers,” in *Retargeting cued
    speech hand gestures for different talking heads and speakers*, September 2008,
    p. 8.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] G. Bailly, Y. Fang, F. Elisei, 和 D. Beautemps, “为不同的谈话头像和说话者重新定位提示语音手势，”
    见 *为不同的谈话头像和说话者重新定位提示语音手势*，2008年9月，第8页。'
- en: '[30] P. KR, M. Rudrabha, P. Namboodir, and C. Jawahar, “A lip sync expert is
    all you need for speech to lip generation in the wild,” in *Proc. ACM MM*, 2020.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] P. KR, M. Rudrabha, P. Namboodir, 和 C. Jawahar, “一个唇同步专家是你所需的一切，用于生成自然唇动的演讲，”
    见 *Proc. ACM MM*，2020年。'
- en: '[31] S. Wang, L. Li, Y. Ding, C. Fan, and X. Yu, “Audio2head: Audio-driven
    one-shot talking-head generation with natural head motion,” in *Proc. IJCAI*,
    2021.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Wang, L. Li, Y. Ding, C. Fan, 和 X. Yu, “Audio2head：基于音频的单次谈话头像生成与自然头部运动，”
    见 *Proc. IJCAI*，2021年。'
- en: '[32] Y. Guo, K. Chen, S. Liang, Y.-J. Liu, H. Bao, and J. Zhang, “Ad-nerf:
    Audio driven neural radiance fields for talking head synthesis,” in *Proc. IEEE/CVF-ICCV*,
    2021, pp. 5784–5794.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Y. Guo, K. Chen, S. Liang, Y.-J. Liu, H. Bao, 和 J. Zhang, “Ad-nerf：用于谈话头像合成的音频驱动神经辐射场，”
    见 *Proc. IEEE/CVF-ICCV*，2021年，页码 5784–5794。'
- en: '[33] S. Shen, W. Zhao, Z. Meng, W. Li, Z. Zhu, J. Zhou, and J. Lu, “Difftalk:
    Crafting diffusion models for generalized audio-driven portraits animation,” in
    *Proc. IEEE/CVF-CVPR*, 2023, pp. 1982–1991.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Shen, W. Zhao, Z. Meng, W. Li, Z. Zhu, J. Zhou, 和 J. Lu, “Difftalk：为广泛应用的音频驱动肖像动画设计扩散模型，”
    见 *Proc. IEEE/CVF-CVPR*，2023年，页码 1982–1991。'
- en: '[34] P. Lucey, G. Potamianos, and S. Sridharan, “Patch-based analysis of visual
    speech from multiple views,” in *International Conference on Auditory-Visual Speech
    Processing (AVSP)*.   AVISA, 2008, pp. 69–74.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] P. 露西, G. 波塔米亚诺斯, 和 S. 斯里达兰, “来自多个视角的视觉语音基于补丁的分析,” 在 *国际听觉视觉语音处理会议 (AVSP)*.
    AVISA, 2008年, 第69–74页。'
- en: '[35] Z. Zhou, G. Zhao, and M. Pietikäinen, “Towards a practical lipreading
    system,” in *Proc. IEEE/CVF-CVPR*, 2011, pp. 137–144.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Z. 周, G. 赵, 和 M. 皮埃蒂凯宁, “迈向实用的唇读系统,” 在 *Proc. IEEE/CVF-CVPR*, 2011年, 第137–144页。'
- en: '[36] P. Wu, H. Liu, X. Li, T. Fan, and X. Zhang, “A novel lip descriptor for
    audio-visual keyword spotting based on adaptive decision fusion,” *IEEE Transactions
    on Multimedia*, vol. 18, no. 3, pp. 326–338, 2016.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] P. 吴, H. 刘, X. 李, T. 范, 和 X. 张, “一种基于自适应决策融合的新型唇部描述符用于音视频关键词检测,” *IEEE
    Multimedia 期刊*, 第18卷, 第3期, 第326–338页, 2016年。'
- en: '[37] P. Ma, S. Petridis, and M. Pantic, “End-to-end audio-visual speech recognition
    with conformers,” in *Proc. IEEE-ICASSP*, 2021, pp. 7613–7617.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] P. 马, S. 佩特里迪斯, 和 M. 潘蒂奇, “使用变换器的端到端音视频语音识别,” 在 *Proc. IEEE-ICASSP*, 2021年,
    第7613–7617页。'
- en: '[38] L. Liu, G. Feng, D. Beautemps, and X.-P. Zhang, “A novel resynchronization
    procedure for hand-lips fusion applied to continuous french cued speech recognition,”
    in *Proc. IEEE-EUSIPCO*, 2019, pp. 1–5.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] L. 刘, G. 冯, D. 博欧坦普斯, 和 X.-P. 张, “一种用于手唇融合的新的重新同步程序，应用于连续法语提示语音识别,” 在
    *Proc. IEEE-EUSIPCO*, 2019年, 第1–5页。'
- en: '[39] K. Papadimitriou, M. Parelli, G. Sapountzaki, G. Pavlakos, P. Maragos,
    and G. Potamianos, “Multimodal fusion and sequence learning for cued speech recognition
    from videos,” in *International Conference on Human-Computer Interaction*, 2021,
    pp. 277–290.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] K. 帕帕迪米特里乌, M. 帕雷利, G. 萨彭扎基, G. 帕夫拉科斯, P. 马拉戈斯, 和 G. 波塔米亚诺斯, “多模态融合和序列学习用于从视频中进行提示语音识别,”
    在 *国际人机交互会议*, 2021年, 第277–290页。'
- en: '[40] L. Liu, G. Feng, B. Denis, and X.-P. Zhang, “Re-synchronization using
    the hand preceding model for multi-modal fusion in automatic continuous cued speech
    recognition,” *IEEE Transactions on Multimedia*, vol. 23, pp. 292–305, 2020.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] L. 刘, G. 冯, B. 丹尼斯, 和 X.-P. 张, “基于手前模型的多模态融合中的重新同步，用于自动连续提示语音识别,” *IEEE
    Multimedia 期刊*, 第23卷, 第292–305页, 2020年。'
- en: '[41] L. Liu and L. Liu, “Cross-modal mutual learning for cued speech recognition,”
    in *Proc. IEEE-ICASSP*, 2023, pp. 1–5.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] L. 刘 和 L. 刘, “用于提示语音识别的跨模态互学,” 在 *Proc. IEEE-ICASSP*, 2023年, 第1–5页。'
- en: '[42] J. Zhang, W. Zhou, and H. Li, “A threshold-based HMM-DTW approach for
    continuous sign language recognition,” in *Proceedings of International Conference
    on Internet Multimedia Computing and Service*.   Association for Computing Machinery,
    2014, pp. 237–240.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. 张, W. 周, 和 H. 李, “一种基于阈值的HMM-DTW方法用于连续手语识别,” 在 *国际互联网多媒体计算与服务会议论文集*.
    计算机协会, 2014年, 第237–240页。'
- en: '[43] W. Yang, J. Tao, and Z. Ye, “Continuous sign language recognition using
    level building based on fast hidden markov model,” *Pattern Recognition Letters*,
    vol. 78, pp. 28–35, 2016.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] W. 杨, J. 陶, 和 Z. 叶, “使用基于快速隐马尔可夫模型的水平构建进行连续手语识别,” *模式识别快报*, 第78卷, 第28–35页,
    2016年。'
- en: '[44] K. L. Cheng, Z. Yang, Q. Chen, and Y.-W. Tai, “Fully convolutional networks
    for continuous sign language recognition,” in *Proc. ECCV*, 2020, pp. 697–714.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] K. L. 郑, Z. 杨, Q. 陈, 和 Y.-W. 戴, “用于连续手语识别的全卷积网络,” 在 *Proc. ECCV*, 2020年,
    第697–714页。'
- en: '[45] C. Wei, J. Zhao, W. Zhou, and H. Li, “Semantic boundary detection with
    reinforcement learning for continuous sign language recognition,” *IEEE Transactions
    on Circuits and Systems for Video Technology*, vol. 31, no. 3, pp. 1138–1149,
    2020.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C. 韦, J. 赵, W. 周, 和 H. 李, “用于连续手语识别的语义边界检测与强化学习,” *IEEE 视频技术电路与系统学报*,
    第31卷, 第3期, 第1138–1149页, 2020年。'
- en: '[46] L. Liu and G. Feng, “A pilot study on mandarin chinese cued speech,” *American
    Annals of the Deaf*, vol. 164, no. 4, pp. 496–518, 2019.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] L. 刘 和 G. 冯, “关于普通话提示语音的初步研究,” *美国聋人年鉴*, 第164卷, 第4期, 第496–518页, 2019年。'
- en: '[47] “Sign languages unite us!” un.org, 2022\. [Online]. Available: [https://www.un.org/en/observances/sign-languages-day](https://www.un.org/en/observances/sign-languages-day)'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] “手语让我们团结！” un.org, 2022\. [在线]. 可用: [https://www.un.org/en/observances/sign-languages-day](https://www.un.org/en/observances/sign-languages-day)'
- en: '[48] Z. Pan, X. Qian, and H. Li, “Speaker extraction with co-speech gestures
    cue,” *IEEE Signal Processing Letters*, vol. 29, pp. 1467–1471, 2022.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Z. 潘, X. 钱, 和 H. 李, “通过共语手势线索进行说话者提取,” *IEEE 信号处理快报*, 第29卷, 第1467–1471页,
    2022年。'
- en: '[49] C. Sondermann and M. Merkt, “Like it or learn from it: Effects of talking
    heads in educational videos,” *Computers & Education*, vol. 193, p. 104675, 2023.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] C. 索恩曼 和 M. 梅克特, “喜欢它还是从中学习：教育视频中谈话头的效果,” *计算机与教育*, 第193卷, 104675号, 2023年。'
- en: '[50] W. Song, Q. He, and G. Chen, “Virtual human talking-head generation,”
    in *Proceedings of the 2023 2nd Asia Conference on Algorithms, Computing and Machine
    Learning*, 2023, pp. 1–5.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] W. Song, Q. He, 和 G. Chen，“虚拟人谈话头生成，”在 *2023 年第二届亚洲算法、计算与机器学习会议论文集*，2023，pp.
    1–5。'
- en: '[51] D. Kothadiya, C. Bhatt, K. Sapariya, K. Patel, A.-B. Gil-González, and
    J. M. Corchado, “Deepsign: Sign language detection and recognition using deep
    learning,” *Electronics*, vol. 11, no. 11, p. 1780, 2022.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] D. Kothadiya, C. Bhatt, K. Sapariya, K. Patel, A.-B. Gil-González, 和 J.
    M. Corchado，“Deepsign：使用深度学习进行手语检测和识别，” *电子学*，第 11 卷，第 11 期，p. 1780，2022。'
- en: '[52] M. De Coster, D. Shterionov, M. Van Herreweghe, and J. Dambre, “Machine
    translation from signed to spoken languages: State of the art and challenges,”
    *Universal Access in the Information Society*, pp. 1–27, 2023.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. De Coster, D. Shterionov, M. Van Herreweghe, 和 J. Dambre，“从手语到口语的机器翻译：现状与挑战，”
    *信息社会中的普遍访问*，pp. 1–27，2023。'
- en: '[53] N. K. Kahlon and W. Singh, “Machine translation from text to sign language:
    a systematic review,” *Universal Access in the Information Society*, vol. 22,
    no. 1, pp. 1–35, 2023.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] N. K. Kahlon 和 W. Singh，“从文本到手语的机器翻译：系统综述，” *信息社会中的普遍访问*，第 22 卷，第 1 期，pp.
    1–35，2023。'
- en: '[54] L. Liu, G. Feng, X. Ren, and X. Ma, “Objective hand complexity comparison
    between two mandarin chinese cued speech systems,” in *Proc. IEEE-ISCSLP*, 2022,
    p. 215–219.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] L. Liu, G. Feng, X. Ren, 和 X. Ma，“两个普通话提示语音系统的手部复杂性客观比较，”在 *IEEE-ISCSLP
    会议记录*，2022，p. 215–219。'
- en: '[55] “Find your cued language,” cuedspeech.org. [Online]. Available: [https://cuedspeech.org/learn/find-your-cued-language/](https://cuedspeech.org/learn/find-your-cued-language/)'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] “找到你的提示语语言，”cuedspeech.org. [在线]. 可用： [https://cuedspeech.org/learn/find-your-cued-language/](https://cuedspeech.org/learn/find-your-cued-language/)'
- en: '[56] L. Liu, “Modeling for continuous cued speech recognition in french using
    advanced machine learning methods,” Ph.D. dissertation, Universite Grenoble Alpes,
    2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] L. Liu，“使用先进机器学习方法对法语连续提示语音识别建模，”博士论文，格勒诺布尔阿尔卑斯大学，2018。'
- en: '[57] L. Liu, G. Feng, and D. Beautemps, “Automatic temporal segmentation of
    hand movements for hand positions recognition in french cued speech,” in *Proc.
    IEEE-ICASSP*, 2018, pp. 3061–3065.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] L. Liu, G. Feng, 和 D. Beautemps，“法语提示语音中手部动作的时间分段自动化用于手部位置识别，”在 *IEEE-ICASSP
    会议记录*，2018，pp. 3061–3065。'
- en: '[58] J. Wang, Z. Tang, X. Li, M. Yu, Q. Fang, and L. Liu, “Cross-modal knowledge
    distillation method for automatic cued speech recognition,” in *Proc. Interspeech*,
    2021, p. 2986–2990.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] J. Wang, Z. Tang, X. Li, M. Yu, Q. Fang, 和 L. Liu，“用于自动提示语音识别的跨模态知识蒸馏方法，”在
    *Interspeech 会议记录*，2021，p. 2986–2990。'
- en: '[59] L. Liu, J. Li, G. Feng, and X.-P. S. Zhang, “Automatic detection of the
    temporal segmentation of hand movements in british english cued speech.” in *Proc.
    Interspeech*, 2019, pp. 2285–2289.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] L. Liu, J. Li, G. Feng, 和 X.-P. S. Zhang，“英国英语提示语音中手部动作的时间分段自动检测。”在 *Interspeech
    会议记录*，2019，pp. 2285–2289。'
- en: '[60] S. J. Park, M. Kim, J. Hong, J. Choi, and Y. M. Ro, “Synctalkface: Talking
    face generation with precise lip-syncing via audio-lip memory,” in *Proc. Conf
    AAAI Artif. Intell.*, vol. 36, no. 2, 2022, pp. 2062–2070.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. J. Park, M. Kim, J. Hong, J. Choi, 和 Y. M. Ro，“Synctalkface：通过音频-嘴唇记忆生成精确的唇动同步谈话面孔，”在
    *AAAI 人工智能会议记录*，第 36 卷，第 2 期，2022，pp. 2062–2070。'
- en: '[61] L. Liu, G. Feng, and D. Beautemps, “Inner lips parameter estimation based
    on adaptive ellipse model,” in *International Conference on Auditory-Visual Speech
    Processing (AVSP)*, 2017.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] L. Liu, G. Feng, 和 D. Beautemps，“基于自适应椭圆模型的内嘴唇参数估计，”在 *听觉-视觉语音处理国际会议 (AVSP)*，2017。'
- en: '[62] ——, “Automatic dynamic template tracking of inner lips based on clnf,”
    in *Proc. IEEE-ICASSP*, 2017, p. 5130–5134.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] ——, “基于 CLNF 的内嘴唇自动动态模板跟踪，”在 *IEEE-ICASSP 会议记录*，2017，p. 5130–5134。'
- en: '[63] M. W. Alibali, M. Bassok, K. O. Solomon, S. E. Syc, and S. Goldin-Meadow,
    “Illuminating mental representations through speech and gesture,” *Psychological
    Science*, vol. 10, no. 4, pp. 327–333, 1999.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] M. W. Alibali, M. Bassok, K. O. Solomon, S. E. Syc, 和 S. Goldin-Meadow，“通过语言和手势揭示心理表征，”
    *心理科学*，第 10 卷，第 4 期，pp. 327–333，1999。'
- en: '[64] S. Kang and B. Tversky, “From hands to minds: Gestures promote understanding,”
    *Cognitive Research: Principles and Implications*, vol. 1, no. 1, pp. 1–15, 2016.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] S. Kang 和 B. Tversky，“从手到脑：手势促进理解，” *认知研究：原理与应用*，第 1 卷，第 1 期，pp. 1–15，2016。'
- en: '[65] A. Kendon, “Do gestures communicate? a review,” *Research on language
    and social interaction*, vol. 27, no. 3, pp. 175–200, 1994.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] A. Kendon，“手势是否传达信息？一个回顾，” *语言和社会互动研究*，第 27 卷，第 3 期，pp. 175–200，1994。'
- en: '[66] K. Adam, *Gesture: Visible action as utterance*.   Cambridge University
    Press, 2004.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] K. Adam，*Gesture: Visible action as utterance*。剑桥大学出版社，2004年。'
- en: '[67] Y. Ferstl and R. McDonnell, “Investigating the use of recurrent motion
    modelling for speech gesture generation,” in *Proc. ACM IVA*, 2018, pp. 93–98.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Y. Ferstl 和 R. McDonnell，“研究递归运动建模在语音手势生成中的应用，”在 *Proc. ACM IVA*，2018年，第93–98页。'
- en: '[68] Y. Yoon, W.-R. Ko, M. Jang, J. Lee, J. Kim, and G. Lee, “Robots learn
    social skills: End-to-end learning of co-speech gesture generation for humanoid
    robots,” in *Proc. IEEE-International Conference in Robotics and Automation (ICRA)*,
    2019, pp. 4303–4309.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Y. Yoon, W.-R. Ko, M. Jang, J. Lee, J. Kim 和 G. Lee，“机器人学习社交技能：面向类人机器人端到端的语音手势生成学习，”在
    *Proc. IEEE-International Conference in Robotics and Automation (ICRA)*，2019年，第4303–4309页。'
- en: '[69] Y. Yoon, P. Wolfert, T. Kucherenko, C. Viegas, T. Nikolov, M. Tsakov,
    and G. E. Henter, “The genea challenge 2022: A large evaluation of data-driven
    co-speech gesture generation,” in *Proc. ACM-International Conference on Multimodal
    Interaction*, 2022, pp. 736–747.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Yoon, P. Wolfert, T. Kucherenko, C. Viegas, T. Nikolov, M. Tsakov 和
    G. E. Henter，“Genea挑战赛2022：大规模数据驱动的语音手势生成评估，”在 *Proc. ACM-International Conference
    on Multimodal Interaction*，2022年，第736–747页。'
- en: '[70] S. Ginosar, A. Bar, G. Kohavi, C. Chan, A. Owens, and J. Malik, “Learning
    individual styles of conversational gesture,” in *Proc. IEEE/CVF-CVPR*, 2019,
    pp. 3497–3506.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Ginosar, A. Bar, G. Kohavi, C. Chan, A. Owens 和 J. Malik，“学习个体的对话手势风格，”在
    *Proc. IEEE/CVF-CVPR*，2019年，第3497–3506页。'
- en: '[71] R. Poppe, “A survey on vision-based human action recognition,” *Image
    and vision computing*, vol. 28, no. 6, pp. 976–990, 2010.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] R. Poppe，“基于视觉的人类动作识别综述，” *Image and vision computing*，第28卷，第6期，第976–990页，2010年。'
- en: '[72] D. Wu, L. Pigou, P.-J. Kindermans, N. D.-H. Le, L. Shao, J. Dambre, and
    J.-M. Odobez, “Deep dynamic neural networks for multimodal gesture segmentation
    and recognition,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 38, no. 8, pp. 1583–1597, 2016.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] D. Wu, L. Pigou, P.-J. Kindermans, N. D.-H. Le, L. Shao, J. Dambre 和 J.-M.
    Odobez，“深度动态神经网络用于多模态手势分割和识别，” *IEEE transactions on pattern analysis and machine
    intelligence*，第38卷，第8期，第1583–1597页，2016年。'
- en: '[73] J. Wan, Y. Zhao, S. Zhou, I. Guyon, S. Escalera, and S. Z. Li, “Chalearn
    looking at people rgb-d isolated and continuous datasets for gesture recognition,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition
    workshops*, 2016, pp. 56–64.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] J. Wan, Y. Zhao, S. Zhou, I. Guyon, S. Escalera 和 S. Z. Li，“Chalearn looking
    at people rgb-d 独立和连续数据集用于手势识别，”在 *Proceedings of the IEEE conference on computer
    vision and pattern recognition workshops*，2016年，第56–64页。'
- en: '[74] J. Materzynska, G. Berger, I. Bax, and R. Memisevic, “The jester dataset:
    A large-scale video dataset of human gestures,” in *Proceedings of the IEEE/CVF
    international conference on computer vision workshops*, 2019, pp. 0–0.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Materzynska, G. Berger, I. Bax 和 R. Memisevic，“Jester 数据集：一个大规模的人类手势视频数据集，”在
    *Proceedings of the IEEE/CVF international conference on computer vision workshops*，2019年，第0–0页。'
- en: '[75] H. Zeng, X. Wang, Y. Wang, A. Wu, T.-C. Pong, and H. Qu, “Gesturelens:
    Visual analysis of gestures in presentation videos,” *IEEE Transactions on Visualization
    and Computer Graphics*, 2022.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] H. Zeng, X. Wang, Y. Wang, A. Wu, T.-C. Pong 和 H. Qu，“Gesturelens：演示视频中的手势视觉分析，”
    *IEEE Transactions on Visualization and Computer Graphics*，2022年。'
- en: '[76] E. Efthimiou, S.-E. Fotinea, T. Hanke, J. Glauert, R. Bowden, A. Braffort,
    C. Collet, P. Maragos, and F. Goudenove, “Dicta-sign: sign language recognition,
    generation and modelling with application in deaf communication,” in *LREC*.   European
    Language Resources Association (ELRA), 2010, pp. 80–83.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] E. Efthimiou, S.-E. Fotinea, T. Hanke, J. Glauert, R. Bowden, A. Braffort,
    C. Collet, P. Maragos 和 F. Goudenove，“Dicta-sign：手语识别、生成与建模及其在聋人交流中的应用，”在 *LREC*。欧洲语言资源协会（ELRA），2010年，第80–83页。'
- en: '[77] O. Koller, J. Forster, and H. Ney, “Continuous sign language recognition:
    Towards large vocabulary statistical recognition systems handling multiple signers,”
    *Computer Vision and Image Understanding*, vol. 141, pp. 108–125, 2015.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] O. Koller, J. Forster 和 H. Ney，“连续手语识别：朝着处理多个签名者的大词汇统计识别系统迈进，” *Computer
    Vision and Image Understanding*，第141卷，第108–125页，2015年。'
- en: '[78] C. Neidle, A. Thangali, and S. Sclaroff, “Challenges in development of
    the american sign language lexicon video dataset (asllvd) corpus,” in *LREC*.   Citeseer,
    2012.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] C. Neidle, A. Thangali 和 S. Sclaroff，“美国手语词汇视频数据集（asllvd）语料库的发展挑战，”在 *LREC*。Citeseer，2012年。'
- en: '[79] U. v. Agris and K.-F. Kraiss, “Signum database: Video corpus for signer-independent
    continuous sign language recognition,” in *LREC*.   European Language Resources
    Association (ELRA), 2010, pp. 243–246.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] U. v. Agris 和 K.-F. Kraiss，“Signum数据库：用于签名者独立连续手语识别的视频语料库，”在 *LREC*。欧洲语言资源协会（ELRA），2010年，第243–246页。'
- en: '[80] Y. Lin, X. Chai, Y. Zhou, and X. Chen, “Curve matching from the view of
    manifold for sign language recognition,” in *ACCV Workshops*, 2014.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Lin, X. Chai, Y. Zhou, 和 X. Chen, “从流形视角进行曲线匹配以识别手语，” 在 *ACCV Workshops*,
    2014。'
- en: '[81] N. K. Caselli, Z. S. Sehyr, A. M. Cohen-Goldberg, and K. Emmorey, “Asl-lex:
    A lexical database of american sign language,” *Behavior research methods*, vol. 49,
    pp. 784–801, 2017.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] N. K. Caselli, Z. S. Sehyr, A. M. Cohen-Goldberg, 和 K. Emmorey, “Asl-lex:
    美国手语的词汇数据库，” *行为研究方法*, vol. 49, 页 784–801, 2017。'
- en: '[82] N. C. Camgoz, S. Hadfield, O. Koller, H. Ney, and R. Bowden, “Neural sign
    language translation,” in *Proc. IEEE-CVPR*, 2018, pp. 7784–7793.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] N. C. Camgoz, S. Hadfield, O. Koller, H. Ney, 和 R. Bowden, “神经手语翻译，” 在
    *Proc. IEEE-CVPR*, 2018, 页 7784–7793。'
- en: '[83] A. Mavi and Z. Dikle, “A new 27 class sign language dataset collected
    from 173 individuals,” *arXiv preprint arXiv:2203.03859*, 2022.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] A. Mavi 和 Z. Dikle, “从 173 个人收集的全新 27 类手语数据集，” *arXiv 预印本 arXiv:2203.03859*,
    2022。'
- en: '[84] S.-K. Ko, C. J. Kim, H. Jung, and C. Cho, “Neural sign language translation
    based on human keypoint estimation,” *Applied sciences*, vol. 9, no. 13, p. 2683,
    2019.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] S.-K. Ko, C. J. Kim, H. Jung, 和 C. Cho, “基于人体关键点估计的神经手语翻译，” *应用科学*, vol.
    9, no. 13, 页 2683, 2019。'
- en: '[85] N. Adaloglou, T. Chatzis, I. Papastratis, A. Stergioulas, G. T. Papadopoulos,
    V. Zacharopoulou, G. J. Xydopoulos, K. Atzakas, D. Papazachariou, and P. Daras,
    “A comprehensive study on deep learning-based methods for sign language recognition,”
    *IEEE Transactions on Multimedia*, vol. 24, pp. 1750–1762, 2021.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] N. Adaloglou, T. Chatzis, I. Papastratis, A. Stergioulas, G. T. Papadopoulos,
    V. Zacharopoulou, G. J. Xydopoulos, K. Atzakas, D. Papazachariou, 和 P. Daras,
    “关于基于深度学习的手语识别方法的全面研究，” *IEEE 多媒体学报*, vol. 24, 页 1750–1762, 2021。'
- en: '[86] Z. S. Sehyr, N. Caselli, A. M. Cohen-Goldberg, and K. Emmorey, “The asl-lex
    2.0 project: A database of lexical and phonological properties for 2,723 signs
    in american sign language,” *The Journal of Deaf Studies and Deaf Education*,
    vol. 26, no. 2, pp. 263–277, 2021.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Z. S. Sehyr, N. Caselli, A. M. Cohen-Goldberg, 和 K. Emmorey, “ASL-LEX
    2.0 项目: 一个包含 2,723 个美国手语手势的词汇和语音特性数据库，” *耳聋研究与耳聋教育杂志*, vol. 26, no. 2, 页 263–277,
    2021。'
- en: '[87] A. Duarte, S. Palaskar, L. Ventura, D. Ghadiyaram, K. DeHaan, F. Metze,
    J. Torres, and X. Giro-i Nieto, “How2sign: a large-scale multimodal dataset for
    continuous american sign language,” in *Proc. IEEE/CVF-CVPR*, 2021, pp. 2735–2744.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] A. Duarte, S. Palaskar, L. Ventura, D. Ghadiyaram, K. DeHaan, F. Metze,
    J. Torres, 和 X. Giro-i Nieto, “How2sign: 大规模多模态数据集用于连续美国手语，” 在 *Proc. IEEE/CVF-CVPR*,
    2021, 页 2735–2744。'
- en: '[88] A. Kapitanov, K. Kvanchiani, A. Nagaev, and E. Petrova, “Slovo: Russian
    sign language dataset,” *arXiv preprint arXiv:2305.14527*, 2023.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] A. Kapitanov, K. Kvanchiani, A. Nagaev, 和 E. Petrova, “Slovo: 俄罗斯手语数据集，”
    *arXiv 预印本 arXiv:2305.14527*, 2023。'
- en: '[89] M. Al-Barham, A. Alsharkawi, M. Al-Yaman, M. Al-Fetyani, A. Elnagar, A. A.
    SaAleek, and M. Al-Odat, “Rgb arabic alphabets sign language dataset,” *arXiv
    preprint arXiv:2301.11932*, 2023.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] M. Al-Barham, A. Alsharkawi, M. Al-Yaman, M. Al-Fetyani, A. Elnagar, A.
    A. SaAleek, 和 M. Al-Odat, “RGB 阿拉伯字母手语数据集，” *arXiv 预印本 arXiv:2301.11932*, 2023。'
- en: '[90] L. Liu, H. Thomas, G. Feng, and B. Denis, “Visual recognition of continuous
    cued speech using a tandem cnn-hmm approach.” in *Proc. Interspeech*, 2018, pp.
    2643–2647.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] L. Liu, H. Thomas, G. Feng, 和 B. Denis, “使用串联CNN-HMM方法对连续提示语音的视觉识别。” 在
    *Proc. Interspeech*, 2018, 页 2643–2647。'
- en: '[91] T. A., “VOT and durational properties of selected segments in the speech
    of deaf and normally hearing children,” *Studia Phonetica Posnaniensia*, vol. 8,
    pp. 111–142, 2007.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] T. A., “聋童与正常听力儿童语音中选定段落的发音时间和持续性特征，” *Studia Phonetica Posnaniensia*,
    vol. 8, 页 111–142, 2007。'
- en: '[92] B. Bigi, M. Zimmermann, and C. André, “Clelfpc: a large open multi-speaker
    corpus of french cued speech,” in *LREC*, 2022, pp. 987–994.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] B. Bigi, M. Zimmermann, 和 C. André, “Clelfpc: 大型开放多说话人法语提示语料库，” 在 *LREC*,
    2022, 页 987–994。'
- en: '[93] M. Cooke, J. Barker, S. Cunningham, and X. Shao, “The grid audio-visual
    speech corpus (1.0) [data set],” in *Zenodo*.   Zenodo, 2006.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] M. Cooke, J. Barker, S. Cunningham, 和 X. Shao, “The grid 音频视觉语音语料库 (1.0)
    [数据集]，” 在 *Zenodo*。 Zenodo, 2006。'
- en: '[94] O. Martin, I. Kotsia, B. Macq, and I. Pitas, “The enterface’05 audio-visual
    emotion database,” in *Proc. IEEE-22nd international conference on data engineering
    workshops*, 2006, pp. 8–8.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] O. Martin, I. Kotsia, B. Macq, 和 I. Pitas, “The enterface’05 音频视觉情感数据库，”
    在 *Proc. IEEE-22nd 国际数据工程会议工作坊*, 2006, 页 8–8。'
- en: '[95] A. Rekik, A. Ben-Hamadou, and W. Mahdi, “An adaptive approach for lip-reading
    using image and depth data,” *Multimedia Tools and Applications*, vol. 75, pp.
    8609–8636, 2016.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] A. Rekik, A. Ben-Hamadou, 和 W. Mahdi, “一种基于图像和深度数据的自适应唇读方法，” *多媒体工具与应用*,
    vol. 75, 页 8609–8636, 2016。'
- en: '[96] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova, and R. Verma,
    “Crema-d: Crowd-sourced emotional multimodal actors dataset,” *IEEE Transactions
    on Affective Computing*, vol. 5, no. 4, pp. 377–390, 2014.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova, 和 R. Verma，“Crema-d:
    众包情感多模态演员数据集，”*IEEE Transactions on Affective Computing*，第 5 卷，第 4 期，第 377–390
    页，2014 年。'
- en: '[97] A. Czyzewski, B. Kostek, P. Bratoszewski, J. Kotus, and M. Szykulski,
    “An audio-visual corpus for multimodal automatic speech recognition,” *Journal
    of Intelligent Information Systems*, vol. 49, pp. 167–192, 2017.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] A. Czyzewski, B. Kostek, P. Bratoszewski, J. Kotus, 和 M. Szykulski，“用于多模态自动语音识别的音频-视觉语料库，”*Journal
    of Intelligent Information Systems*，第 49 卷，第 167–192 页，2017 年。'
- en: '[98] J. S. Chung and A. Zisserman, “Lip reading in the wild,” *Proc. ACCV*,
    pp. 87–103, 2017.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. S. Chung 和 A. Zisserman，“野外唇读，”*Proc. ACCV*，第 87–103 页，2017 年。'
- en: '[99] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab, N. Sadoughi, and
    E. M. Provost, “MSP-IMPROV: An acted corpus of dyadic interactions to study emotion
    perception,” *IEEE Transactions on Affective Computing*, vol. 8, no. 1, pp. 67–80,
    2016.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab, N. Sadoughi, 和
    E. M. Provost，“MSP-IMPROV: 用于研究情感感知的双人互动表演语料库，”*IEEE Transactions on Affective
    Computing*，第 8 卷，第 1 期，第 67–80 页，2016 年。'
- en: '[100] S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher-Shlizerman, “Synthesizing
    obama: learning lip sync from audio,” *ACM Transactions on Graphics (ToG)*, vol. 36,
    no. 4, pp. 1–13, 2017.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] S. Suwajanakorn, S. M. Seitz, 和 I. Kemelmacher-Shlizerman，“合成奥巴马：从音频中学习唇部同步，”*ACM
    Transactions on Graphics (ToG)*，第 36 卷，第 4 期，第 1–13 页，2017 年。'
- en: '[101] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: a large-scale speaker
    identification dataset,” *Telephony*, vol. 3, pp. 33–039, 2017.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] A. Nagrani, J. S. Chung, 和 A. Zisserman，“Voxceleb: 一个大规模说话人识别数据集，”*Telephony*，第
    3 卷，第 33–039 页，2017 年。'
- en: '[102] J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep speaker recognition,”
    *Proc. Interspeech*, 2018.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. S. Chung, A. Nagrani, 和 A. Zisserman，“Voxceleb2: 深度说话人识别，”*Proc. Interspeech*，2018
    年。'
- en: '[103] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, “Deep
    audio-visual speech recognition,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 44, no. 12, pp. 8717–8727, 2018.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, 和 A. Zisserman，“深度音频-视觉语音识别，”*IEEE
    Transactions on Pattern Analysis and Machine Intelligence*，第 44 卷，第 12 期，第 8717–8727
    页，2018 年。'
- en: '[104] T. Afouras, J. S. Chung, and A. Zisserman, “LRS3-TED: a large-scale dataset
    for visual speech recognition,” *arXiv preprint arXiv:1809.00496*, 2018.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] T. Afouras, J. S. Chung, 和 A. Zisserman，“LRS3-TED: 一种大规模视觉语音识别数据集，”*arXiv
    preprint arXiv:1809.00496*，2018 年。'
- en: '[105] S. R. Livingstone and F. A. Russo, “The ryerson audio-visual database
    of emotional speech and song (ravdess): A dynamic, multimodal set of facial and
    vocal expressions in north american english,” *PloS one*, vol. 13, no. 5, p. e0196391,
    2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] S. R. Livingstone 和 F. A. Russo，“Ryerson 语音-视觉数据库（ravdess）：一套动态的、多模态的北美英语面部和声音表达，”*PloS
    One*，第 13 卷，第 5 期，第 e0196391 页，2018 年。'
- en: '[106] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihalcea,
    “Meld: A multimodal multi-party dataset for emotion recognition in conversations,”
    *arXiv preprint arXiv:1810.02508*, 2018.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, 和 R. Mihalcea，“MELD:
    用于对话中情感识别的多模态多人数据集，”*arXiv preprint arXiv:1810.02508*，2018 年。'
- en: '[107] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T.
    Freeman, and M. Rubinstein, “Looking to listen at the cocktail party: a speaker-independent
    audio-visual model for speech separation,” *ACM Transactions on Graphics (TOG)*,
    vol. 37, no. 4, pp. 1–11, 2018.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W.
    T. Freeman, 和 M. Rubinstein，“在鸡尾酒会中观察以听取：一种无声源音频-视觉模型用于语音分离，”*ACM Transactions
    on Graphics (TOG)*，第 37 卷，第 4 期，第 1–11 页，2018 年。'
- en: '[108] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, and M. J. Black, “Capture,
    learning, and synthesis of 3d speaking styles,” in *Proc. IEEE/CVF-CVPR*, 2019,
    pp. 10 101–10 111.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, 和 M. J. Black，“捕捉、学习和合成
    3D 说话风格，”在 *Proc. IEEE/CVF-CVPR*，2019，第 10 101–10 111 页。'
- en: '[109] S. Yang, Y. Zhang, D. Feng, M. Yang, C. Wang, J. Xiao, K. Long, S. Shan,
    and X. Chen, “LRW-1000: A naturally-distributed large-scale benchmark for lip
    reading in the wild,” in *Proceedings of 14th IEEE international conference on
    automatic face & gesture recognition*, 2019, pp. 1–8.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] S. Yang, Y. Zhang, D. Feng, M. Yang, C. Wang, J. Xiao, K. Long, S. Shan,
    和 X. Chen，“LRW-1000: 一个自然分布的大规模唇读基准数据集，”在 *Proceedings of 14th IEEE International
    Conference on Automatic Face & Gesture Recognition*，2019，第 1–8 页。'
- en: '[110] A. Rossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nießner,
    “Faceforensics++: Learning to detect manipulated facial images,” in *Proc. IEEE/CVF-ICCV*,
    2019, pp. 1–11.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] A. Rossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, 和 M. Nießner，“Faceforensics++:
    学习检测篡改的面部图像，”在 *Proc. IEEE/CVF-ICCV*，2019，第 1–11 页。'
- en: '[111] K. Wang, Q. Wu, L. Song, Z. Yang, W. Wu, C. Qian, R. He, Y. Qiao, and
    C. C. Loy, “Mead: A large-scale audio-visual dataset for emotional talking-face
    generation,” in *Proc. ECCV*, 2020, pp. 700–717.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] K. Wang, Q. Wu, L. Song, Z. Yang, W. Wu, C. Qian, R. He, Y. Qiao, 和 C.
    C. Loy, “Mead: 大规模音视频数据集用于情感谈话面生成，” 在 *Proc. ECCV*, 2020, pp. 700–717.'
- en: '[112] Z. Zhang, L. Li, Y. Ding, and C. Fan, “Flow-guided one-shot talking face
    generation with a high-resolution audio-visual dataset,” in *Proc. IEEE/CVF-CVPR*,
    2021, pp. 3661–3670.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Z. Zhang, L. Li, Y. Ding, 和 C. Fan, “流引导的一次性谈话面生成与高分辨率音视频数据集，” 在 *Proc.
    IEEE/CVF-CVPR*, 2021, pp. 3661–3670.'
- en: '[113] K. Kim, S. Park, J. Lee, S. Chung, J. Lee, and J. Choo, “AnimeCeleb:
    Large-scale animation celebheads dataset for head reenactment,” in *Proc. ECCV*,
    2022, pp. 414–430.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] K. Kim, S. Park, J. Lee, S. Chung, J. Lee, 和 J. Choo, “AnimeCeleb: 大规模动画名人头像数据集用于头部重演，”
    在 *Proc. ECCV*, 2022, pp. 414–430.'
- en: '[114] A. Berkol, T. Tümer-Sivri, N. Pervan-Akman, M. Çolak, and H. Erdem, “Visual
    lip reading dataset in turkish,” *Data*, vol. 8, no. 1, p. 15, 2023.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] A. Berkol, T. Tümer-Sivri, N. Pervan-Akman, M. Çolak, 和 H. Erdem, “土耳其语视觉唇读数据集，”
    *Data*, vol. 8, no. 1, p. 15, 2023.'
- en: '[115] G. Hwang, S. Hong, S. Lee, S. Park, and G. Chae, “DisCoHead: Audio-and-video-driven
    talking head generation by disentangled control of head pose and facial expressions,”
    in *Proc. IEEE-ICASSP*, 2023, pp. 1–5.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] G. Hwang, S. Hong, S. Lee, S. Park, 和 G. Chae, “DisCoHead: 通过解耦控制头部姿态和面部表情的音视频驱动谈话头生成，”
    在 *Proc. IEEE-ICASSP*, 2023, pp. 1–5.'
- en: '[116] I. Matthews, T. F. Cootes, J. A. Bangham, S. Cox, and R. Harvey, “Extraction
    of visual features for lipreading,” *IEEE Transactions on Pattern Analysis and
    Machine Intelligence*, vol. 24, no. 2, pp. 198–213, 2002.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] I. Matthews, T. F. Cootes, J. A. Bangham, S. Cox, 和 R. Harvey, “视觉特征提取用于唇读，”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 24, no.
    2, pp. 198–213, 2002.'
- en: '[117] S. Petridis, J. Shen, D. Cetin, and M. Pantic, “Visual-only recognition
    of normal, whispered and silent speech,” in *Proc. IEEE-ICASSP*, 2018, pp. 6219–6223.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. Petridis, J. Shen, D. Cetin, 和 M. Pantic, “仅视觉识别正常、耳语和无声语音，” 在 *Proc.
    IEEE-ICASSP*, 2018, pp. 6219–6223.'
- en: '[118] K. Takeuchi, S. Kubota, K. Suzuki, D. Hasegawa, and H. Sakuta, “Creating
    a gesture-speech dataset for speech-based automatic gesture generation,” *Communications
    in Computer and Information Science*, pp. 198–202, 2017.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] K. Takeuchi, S. Kubota, K. Suzuki, D. Hasegawa, 和 H. Sakuta, “为基于语音的自动手势生成创建手势-语音数据集，”
    *Communications in Computer and Information Science*, pp. 198–202, 2017.'
- en: '[119] N. Singh, J. J. Lee, I. Grover, and C. Breazeal, “P2pstory: dataset of
    children as storytellers and listeners in peer-to-peer interactions,” in *Proceedings
    of the CHI Conference on Human Factors in Computing Systems*, 2018, pp. 1–11.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] N. Singh, J. J. Lee, I. Grover, 和 C. Breazeal, “P2pstory: 儿童作为讲述者和听众的点对点互动数据集，”
    在 *Proceedings of the CHI Conference on Human Factors in Computing Systems*, 2018,
    pp. 1–11.'
- en: '[120] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black,
    “AMASS: Archive of motion capture as surface shapes,” in *Proc. IEEE/CVF-ICCV*,
    2019, pp. 5442–5451.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, 和 M. J. Black, “AMASS:
    动作捕捉表面形状档案，” 在 *Proc. IEEE/CVF-ICCV*, 2019, pp. 5442–5451.'
- en: '[121] Y. Luo, J. Ye, R. B. Adams, J. Li, M. G. Newman, and J. Z. Wang, “ARBEE:
    Towards automated recognition of bodily expression of emotion in the wild,” *International
    journal of computer vision*, vol. 128, pp. 1–25, 2020.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Y. Luo, J. Ye, R. B. Adams, J. Li, M. G. Newman, 和 J. Z. Wang, “ARBEE:
    朝着自动识别自然场景中的身体情感表达迈进，” *International journal of computer vision*, vol. 128, pp.
    1–25, 2020.'
- en: '[122] C. Ahuja, D. W. Lee, R. Ishii, and L.-P. Morency, “No gestures left behind:
    Learning relationships between spoken language and freeform gestures,” in *Findings
    of the Association for Computational Linguistics: EMNLP*, 2020, pp. 1884–1895.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] C. Ahuja, D. W. Lee, R. Ishii, 和 L.-P. Morency, “不遗留手势：学习口语和自由形式手势之间的关系，”
    在 *Findings of the Association for Computational Linguistics: EMNLP*, 2020, pp.
    1884–1895.'
- en: '[123] A. R. Punnakkal, A. Chandrasekaran, N. Athanasiou, A. Quiros-Ramirez,
    and M. J. Black, “Babel: Bodies, action and behavior with english labels,” in
    *Proc. IEEE/CVF-CVPR*, 2021, pp. 722–731.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] A. R. Punnakkal, A. Chandrasekaran, N. Athanasiou, A. Quiros-Ramirez,
    和 M. J. Black, “Babel: 英文标签下的身体、动作和行为，” 在 *Proc. IEEE/CVF-CVPR*, 2021, pp. 722–731.'
- en: '[124] C. Guo, S. Zou, X. Zuo, S. Wang, W. Ji, X. Li, and L. Cheng, “Generating
    diverse and natural 3d human motions from text,” in *Proc. IEEE/CVF-CVPR*, 2022,
    pp. 5152–5161.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] C. Guo, S. Zou, X. Zuo, S. Wang, W. Ji, X. Li, 和 L. Cheng, “从文本生成多样化和自然的3D人类动作，”
    在 *Proc. IEEE/CVF-CVPR*, 2022, pp. 5152–5161.'
- en: '[125] H. Liu, Z. Zhu, N. Iwamoto, Y. Peng, Z. Li, Y. Zhou, E. Bozkurt, and
    B. Zheng, “Beat: A large-scale semantic and emotional multi-modal dataset for
    conversational gestures synthesis,” in *Proc. ECCV*, 2022, pp. 612–630.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] H. Liu、Z. Zhu、N. Iwamoto、Y. Peng、Z. Li、Y. Zhou、E. Bozkurt 和 B. Zheng，“Beat：用于对话手势合成的大规模语义和情感多模态数据集，”在
    *Proc. ECCV*，2022 年，页码 612–630。'
- en: '[126] J. Wang, Y. Zhao, L. Liu, T. Xu, Q. Li, and S. Li, “Emotional talking
    head generation based on memory-sharing and attention-augmented networks,” *arXiv
    preprint arXiv:2306.03594*, 2023.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Wang、Y. Zhao、L. Liu、T. Xu、Q. Li 和 S. Li，“基于记忆共享和注意力增强网络的情感谈话头生成，”
    *arXiv preprint arXiv:2306.03594*，2023 年。'
- en: '[127] J. Wang, Y. Zhao, H. Fan, T. Xu, Q. Li, S. Li, and L. Liu, “Memory-augmented
    contrastive learning for talking head generation,” in *Proc. IEEE-ICASSP*, 2023,
    p. 1–5.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Wang、Y. Zhao、H. Fan、T. Xu、Q. Li、S. Li 和 L. Liu，“基于记忆增强的对比学习用于谈话头生成，”在
    *Proc. IEEE-ICASSP*，2023 年，页码 1–5。'
- en: '[128] E. Cosatto, J. Ostermann, H. P. Graf, and J. Schroeter, “Lifelike talking
    faces for interactive services,” *Proceedings of the IEEE*, vol. 91, no. 9, pp.
    1406–1429, 2003.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] E. Cosatto、J. Ostermann、H. P. Graf 和 J. Schroeter，“用于交互服务的逼真谈话面孔，” *Proceedings
    of the IEEE*，第 91 卷，第 9 期，页码 1406–1429，2003 年。'
- en: '[129] O. Gambino, A. Augello, A. Caronia, G. Pilato, R. Pirrone, and S. Gaglio,
    “Virtual conversation with a real talking head,” in *Proc. IEEE-Conference on
    Human System Interactions*, 2008, pp. 263–268.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] O. Gambino、A. Augello、A. Caronia、G. Pilato、R. Pirrone 和 S. Gaglio，“与真实谈话头的虚拟对话，”在
    *Proc. IEEE-Conference on Human System Interactions*，2008 年，页码 263–268。'
- en: '[130] Y. Yang and D. Ramanan, “Articulated human detection with flexible mixtures
    of parts,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 35,
    no. 12, pp. 2878–2890, 2012.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Y. Yang 和 D. Ramanan，“具有灵活部件混合的关节人类检测，” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*，第 35 卷，第 12 期，页码 2878–2890，2012 年。'
- en: '[131] Y. Yoon, B. Cha, J.-H. Lee, M. Jang, J. Lee, J. Kim, and G. Lee, “Speech
    gesture generation from the trimodal context of text, audio, and speaker identity,”
    *ACM Transactions on Graphics (TOG)*, vol. 39, no. 6, pp. 1–16, 2020.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Y. Yoon、B. Cha、J.-H. Lee、M. Jang、J. Lee、J. Kim 和 G. Lee，“基于文本、音频和说话者身份的三模态上下文的语音手势生成，”
    *ACM Transactions on Graphics (TOG)*，第 39 卷，第 6 期，页码 1–16，2020 年。'
- en: '[132] E. Asakawa, N. Kaneko, D. Hasegawa, and S. Shirakawa, “Evaluation of
    text-to-gesture generation model using convolutional neural network,” *Neural
    Networks*, vol. 151, pp. 365–375, 2022.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] E. Asakawa、N. Kaneko、D. Hasegawa 和 S. Shirakawa，“基于卷积神经网络的文本到手势生成模型评估，”
    *Neural Networks*，第 151 卷，页码 365–375，2022 年。'
- en: '[133] P. Buehler, A. Zisserman, and M. Everingham, “Learning sign language
    by watching tv (using weakly aligned subtitles),” in *Proc. IEEE-CVPR*, 2009,
    pp. 2961–2968.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] P. Buehler、A. Zisserman 和 M. Everingham，“通过观看电视学习手语（使用弱对齐的字幕），”在 *Proc.
    IEEE-CVPR*，2009 年，页码 2961–2968。'
- en: '[134] H. Wang, X. Chai, and X. Chen, “A novel sign language recognition framework
    using hierarchical grassmann covariance matrix,” *IEEE Transactions on Multimedia*,
    vol. 21, no. 11, pp. 2806–2814, 2019.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] H. Wang、X. Chai 和 X. Chen，“一种新的使用分层 Grassmann 协方差矩阵的手语识别框架，” *IEEE Transactions
    on Multimedia*，第 21 卷，第 11 期，页码 2806–2814，2019 年。'
- en: '[135] T. Pfister, J. Charles, and A. Zisserman, “Large-scale learning of sign
    language by watching tv (using co-occurrences).” in *Proc. BMVC*.   British Machine
    Vision Association, 2013.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] T. Pfister、J. Charles 和 A. Zisserman，“通过观看电视的大规模手语学习（使用共现），”在 *Proc.
    BMVC*。 英国机器视觉协会，2013 年。'
- en: '[136] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proc. IEEE-CVPR*, 2016, pp. 770–778.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] K. He、X. Zhang、S. Ren 和 J. Sun，“用于图像识别的深度残差学习，”在 *Proc. IEEE-CVPR*，2016
    年，页码 770–778。'
- en: '[137] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model
    and the kinetics dataset,” in *Proc. IEEE-CVPR*, 2017, pp. 6299–6308.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] J. Carreira 和 A. Zisserman，“行动识别的未来？一种新模型和 Kinetics 数据集，”在 *Proc. IEEE-CVPR*，2017
    年，页码 6299–6308。'
- en: '[138] Z. Qiu, T. Yao, and T. Mei, “Learning spatio-temporal representation
    with pseudo-3d residual networks,” in *Proc. IEEE-ICCV*, 2017, pp. 5533–5541.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Z. Qiu、T. Yao 和 T. Mei，“使用伪 3D 残差网络学习时空表示，”在 *Proc. IEEE-ICCV*，2017 年，页码
    5533–5541。'
- en: '[139] Z. Qiu, T. Yao, C.-W. Ngo, X. Tian, and T. Mei, “Learning spatio-temporal
    representation with local and global diffusion,” in *Proc. IEEE/CVF-CVPR*, 2019,
    pp. 12 056–12 065.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Z. Qiu、T. Yao、C.-W. Ngo、X. Tian 和 T. Mei，“利用局部和全局扩散学习时空表示，”在 *Proc. IEEE/CVF-CVPR*，2019
    年，页码 12 056–12 065。'
- en: '[140] H. Hu, J. Pu, W. Zhou, and H. Li, “Collaborative multilingual continuous
    sign language recognition: A unified framework,” *IEEE Transactions on Multimedia*,
    2022.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] H. Hu、J. Pu、W. Zhou 和 H. Li，“协作多语言连续手语识别：统一框架，” *IEEE Transactions on
    Multimedia*，2022 年。'
- en: '[141] R. Cui, H. Liu, and C. Zhang, “A deep neural framework for continuous
    sign language recognition by iterative training,” *IEEE Transactions on Multimedia*,
    vol. 21, no. 7, pp. 1880–1891, 2019.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] R. Cui, H. Liu, 和 C. Zhang，“通过迭代训练进行连续手语识别的深度神经框架”，*IEEE多媒体汇刊*，第21卷，第7期，页1880–1891，2019年。'
- en: '[142] J. Pu, W. Zhou, H. Hu, and H. Li, “Boosting continuous sign language
    recognition via cross modality augmentation,” in *Proc. ACM MM*, 2020, pp. 1497–1505.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] J. Pu, W. Zhou, H. Hu, 和 H. Li，“通过跨模态增强提升连续手语识别”，见于 *ACM MM 会议录*，2020年，页1497–1505。'
- en: '[143] J. Pu, W. Zhou, and H. Li, “Iterative alignment network for continuous
    sign language recognition,” in *Proc. IEEE/CVF-CVPR*, 2019, pp. 4165–4174.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] J. Pu, W. Zhou, 和 H. Li，“用于连续手语识别的迭代对齐网络”，见于 *IEEE/CVF-CVPR 会议录*，2019年，页4165–4174。'
- en: '[144] J. Huang, W. Zhou, Q. Zhang, H. Li, and W. Li, “Video-based sign language
    recognition without temporal segmentation,” in *Proc. Conf AAAI Artif. Intell.*,
    vol. 32, no. 1, 2018.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. Huang, W. Zhou, Q. Zhang, H. Li, 和 W. Li，“基于视频的手语识别无需时间分割”，见于 *AAAI人工智能会议录*，第32卷，第1期，2018年。'
- en: '[145] H. Hu, W. Zhou, J. Pu, and H. Li, “Global-local enhancement network for
    nmf-aware sign language recognition,” *ACM transactions on multimedia computing,
    communications, and applications (TOMM)*, vol. 17, no. 3, pp. 1–19, 2021.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] H. Hu, W. Zhou, J. Pu, 和 H. Li，“用于NMF感知手语识别的全局-局部增强网络”，*ACM多媒体计算、通信与应用（TOMM）*，第17卷，第3期，页1–19，2021年。'
- en: '[146] C. Wei, W. Zhou, J. Pu, and H. Li, “Deep grammatical multi-classifier
    for continuous sign language recognition,” in *International Conference on Multimedia
    Big Data (BigMM)*, 2019, pp. 435–442.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] C. Wei, W. Zhou, J. Pu, 和 H. Li，“用于连续手语识别的深度语法多分类器”，见于 *国际多媒体大数据会议（BigMM）*，2019年，页435–442。'
- en: '[147] D. Guo, S. Wang, Q. Tian, and M. Wang, “Dense temporal convolution network
    for sign language translation.” in *Proc.IJCAI*, 2019, pp. 744–750.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] D. Guo, S. Wang, Q. Tian, 和 M. Wang，“用于手语翻译的密集时间卷积网络”，见于 *IJCAI会议录*，2019年，页744–750。'
- en: '[148] H. Zhou, W. Zhou, and H. Li, “Dynamic pseudo label decoding for continuous
    sign language recognition,” in *International conference on multimedia and expo
    (ICME)*, 2019, pp. 1282–1287.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] H. Zhou, W. Zhou, 和 H. Li，“用于连续手语识别的动态伪标签解码”，见于 *国际多媒体与博览会（ICME）*，2019年，页1282–1287。'
- en: '[149] S. NadeemHashmi, H. Gupta, D. Mittal, K. Kumar, A. Nanda, and S. Gupta,
    “A lip reading model using cnn with batch normalization,” in *Proc. IEEE-11th
    international conference on contemporary computing (IC3)*, 2018, pp. 1–6.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] S. NadeemHashmi, H. Gupta, D. Mittal, K. Kumar, A. Nanda, 和 S. Gupta，“使用批量归一化的CNN唇读模型”，见于
    *IEEE第11届当代计算国际会议（IC3）*，2018年，页1–6。'
- en: '[150] F. B. Slimane and M. Bouguessa, “Context matters: Self-attention for
    sign language recognition,” in *International Conference on Pattern Recognition
    (ICPR)*, 2021, pp. 7884–7891.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] F. B. Slimane 和 M. Bouguessa，“背景很重要：用于手语识别的自注意力”，见于 *国际模式识别会议（ICPR）*，2021年，页7884–7891。'
- en: '[151] M. Zhou, M. Ng, Z. Cai, and K. C. Cheung, “Self-attention-based fully-inception
    networks for continuous sign language recognition,” in *24th European Conference
    on Artificial Intelligence*, 2020, pp. 2832–2839.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] M. Zhou, M. Ng, Z. Cai, 和 K. C. Cheung，“基于自注意力的完全嵌入网络用于连续手语识别”，见于 *第24届欧洲人工智能会议*，2020年，页2832–2839。'
- en: '[152] O. Koller, N. C. Camgoz, H. Ney, and R. Bowden, “Weakly supervised learning
    with multi-stream cnn-lstm-hmms to discover sequential parallelism in sign language
    videos,” *IEEE transactions on pattern analysis and machine intelligence*, vol. 42,
    no. 9, pp. 2306–2320, 2019.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] O. Koller, N. C. Camgoz, H. Ney, 和 R. Bowden，“通过多流CNN-LSTM-HMMs的弱监督学习发现手语视频中的序列平行性”，*IEEE模式分析与机器智能汇刊*，第42卷，第9期，页2306–2320，2019年。'
- en: '[153] O. Koller, S. Zargaran, and H. Ney, “Re-sign: Re-aligned end-to-end sequence
    modelling with deep recurrent CNN-HMMs,” in *Proc. IEEE-CVPR*, 2017, pp. 4297–4305.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] O. Koller, S. Zargaran, 和 H. Ney，“Re-sign：使用深度递归CNN-HMMs的重新对齐端到端序列建模”，见于
    *IEEE-CVPR 会议录*，2017年，页4297–4305。'
- en: '[154] O. Koller, S. Zargaran, H. Ney, and R. Bowden, “Deep sign: Enabling robust
    statistical continuous sign language recognition via hybrid cnn-hmms,” *International
    Journal of Computer Vision*, vol. 126, pp. 1311–1325, 2018.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] O. Koller, S. Zargaran, H. Ney, 和 R. Bowden，“深度手语：通过混合CNN-HMMs实现鲁棒的统计连续手语识别”，*计算机视觉国际杂志*，第126卷，页1311–1325，2018年。'
- en: '[155] R. Cui, H. Liu, and C. Zhang, “Recurrent convolutional neural networks
    for continuous sign language recognition by staged optimization,” in *Proc. IEEE-CVPR*,
    2017, pp. 7361–7369.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] R. Cui, H. Liu, 和 C. Zhang，“通过阶段优化的递归卷积神经网络进行连续手语识别”，见于 *IEEE-CVPR 会议录*，2017年，页7361–7369。'
- en: '[156] Y. Min, A. Hao, X. Chai, and X. Chen, “Visual alignment constraint for
    continuous sign language recognition,” in *Proc. IEEE/CVF-ICCV*, 2021, pp. 11 542–11 551.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Y. Min, A. Hao, X. Chai 和 X. Chen，“用于连续手语识别的视觉对齐约束”，见于 *Proc. IEEE/CVF-ICCV*，2021年，第11 542–11 551页。'
- en: '[157] N. Cihan Camgoz, S. Hadfield, O. Koller, and R. Bowden, “Subunets: End-to-end
    hand shape and continuous sign language recognition,” in *Proc. IEEE-ICCV*, 2017,
    pp. 3056–3065.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] N. Cihan Camgoz, S. Hadfield, O. Koller 和 R. Bowden，“Subunets：端到端的手形和连续手语识别”，见于
    *Proc. IEEE-ICCV*，2017年，第3056–3065页。'
- en: '[158] D. Guo, W. Zhou, A. Li, H. Li, and M. Wang, “Hierarchical recurrent deep
    fusion using adaptive clip summarization for sign language translation,” *IEEE
    Transactions on Image Processing*, vol. 29, pp. 1575–1590, 2019.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] D. Guo, W. Zhou, A. Li, H. Li 和 M. Wang，“利用自适应剪辑摘要的层次递归深度融合进行手语翻译”，*IEEE
    Transactions on Image Processing*，第29卷，第1575–1590页，2019年。'
- en: '[159] H. Li, L. Gao, R. Han, L. Wan, and W. Feng, “Key action and joint ctc-attention
    based sign language recognition,” in *IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2020, pp. 2348–2352.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] H. Li, L. Gao, R. Han, L. Wan 和 W. Feng，“基于关键动作和联合CTC-注意力的手语识别”，见于 *IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*，2020年，第2348–2352页。'
- en: '[160] A. Hao, Y. Min, and X. Chen, “Self-mutual distillation learning for continuous
    sign language recognition,” in *Proc. IEEE/CVF-ICCV*, 2021, pp. 11 303–11 312.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] A. Hao, Y. Min 和 X. Chen，“用于连续手语识别的自互蒸馏学习”，见于 *Proc. IEEE/CVF-ICCV*，2021年，第11 303–11 312页。'
- en: '[161] H. Zhou, W. Zhou, Y. Zhou, and H. Li, “Spatial-temporal multi-cue network
    for sign language recognition and translation,” *IEEE Transactions on Multimedia*,
    vol. 24, pp. 768–779, 2021.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] H. Zhou, W. Zhou, Y. Zhou 和 H. Li，“用于手语识别和翻译的时空多线索网络”，*IEEE Transactions
    on Multimedia*，第24卷，第768–779页，2021年。'
- en: '[162] X. Pei, D. Guo, and Y. Zhao, “Continuous sign language recognition based
    on pseudo-supervised learning,” in *Proceedings of the 2nd Workshop on Multimedia
    for Accessible Human Computer Interfaces*, 2019, pp. 33–39.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] X. Pei, D. Guo 和 Y. Zhao，“基于伪监督学习的连续手语识别”，见于 *Proceedings of the 2nd
    Workshop on Multimedia for Accessible Human Computer Interfaces*，2019年，第33–39页。'
- en: '[163] Z. Zhang, J. Pu, L. Zhuang, W. Zhou, and H. Li, “Continuous sign language
    recognition via reinforcement learning,” in *Proc. IEEE-ICIP*.   IEEE, 2019, pp.
    285–289.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Z. Zhang, J. Pu, L. Zhuang, W. Zhou 和 H. Li，“通过强化学习进行连续手语识别”，见于 *Proc.
    IEEE-ICIP*。 IEEE，2019年，第285–289页。'
- en: '[164] Z. Niu and B. Mak, “Stochastic fine-grained labeling of multi-state sign
    glosses for continuous sign language recognition,” in *Proc. ECCV*, 2020, pp.
    172–186.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Z. Niu 和 B. Mak，“用于连续手语识别的多状态符号的随机细粒度标注”，见于 *Proc. ECCV*，2020年，第172–186页。'
- en: '[165] K. Koishybay, M. Mukushev, and A. Sandygulova, “Continuous sign language
    recognition with iterative spatiotemporal fine-tuning,” in *Proc. IEEE-ICPR*,
    2021, pp. 10 211–10 218.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] K. Koishybay, M. Mukushev 和 A. Sandygulova，“具有迭代时空微调的连续手语识别”，见于 *Proc.
    IEEE-ICPR*，2021年，第10 211–10 218页。'
- en: '[166] I. Papastratis, K. Dimitropoulos, and P. Daras, “Continuous sign language
    recognition through a context-aware generative adversarial network,” *Sensors*,
    vol. 21, no. 7, 2021.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] I. Papastratis, K. Dimitropoulos 和 P. Daras，“通过上下文感知生成对抗网络进行连续手语识别”，*Sensors*，第21卷，第7期，2021年。'
- en: '[167] Y. Chen, R. Zuo, F. Wei, Y. Wu, S. Liu, and B. Mak, “Two-stream network
    for sign language recognition and translation,” *Advances in Neural Information
    Processing Systems (NIPS)*, vol. 35, pp. 17 043–17 056, 2022.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Y. Chen, R. Zuo, F. Wei, Y. Wu, S. Liu 和 B. Mak，“用于手语识别和翻译的双流网络”，*Advances
    in Neural Information Processing Systems (NIPS)*，第35卷，第17 043–17 056页，2022年。'
- en: '[168] L. Hu, L. Gao, Z. Liu, and W. Feng, “Self-emphasizing network for continuous
    sign language recognition,” in *Proc. Conf AAAI Artif. Intell.*, vol. 37, no. 1,
    2023, pp. 854–862.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] L. Hu, L. Gao, Z. Liu 和 W. Feng，“用于连续手语识别的自强调网络”，见于 *Proc. Conf AAAI
    Artif. Intell.*，第37卷，第1期，2023年，第854–862页。'
- en: '[169] J. Zheng, Y. Wang, C. Tan, S. Li, G. Wang, J. Xia, Y. Chen, and S. Z.
    Li, “Cvt-slr: Contrastive visual-textual transformation for sign language recognition
    with variational alignment,” in *Proc. IEEE/CVF-CVPR*, 2023, pp. 23 141–23 150.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] J. Zheng, Y. Wang, C. Tan, S. Li, G. Wang, J. Xia, Y. Chen 和 S. Z. Li，“Cvt-slr：用于手语识别的对比视觉-文本变换与变分对齐”，见于
    *Proc. IEEE/CVF-CVPR*，2023年，第23 141–23 150页。'
- en: '[170] K. Papadimitriou and G. Potamianos, “A fully convolutional sequence learning
    approach for cued speech recognition from videos,” in *Proc. IEEE-EUSIPCO*, 2021,
    pp. 326–330.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] K. Papadimitriou 和 G. Potamianos，“一种完全卷积的序列学习方法用于视频中的提示语音识别”，见于 *Proc.
    IEEE-EUSIPCO*，2021年，第326–330页。'
- en: '[171] S. Sankar, D. Beautemps, and T. Hueber, “Multistream neural architectures
    for cued speech recognition using a pre-trained visual feature extractor and constrained
    ctc decoding,” in *Proc. IEEE-ICASSP*, 2022, pp. 8477–8481.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] S. Sankar, D. Beautemps, 和 T. Hueber，“用于提示语音识别的多流神经架构，结合预训练视觉特征提取器和约束CTC解码”，发表于*Proc.
    IEEE-ICASSP*，2022年，第8477–8481页。'
- en: '[172] M. Kipp, A. Heloir, and Q. Nguyen, “Sign language avatars: Animation
    and comprehensibility,” in *Intelligent Virtual Agents*.   Springer Berlin Heidelberg,
    2011, pp. 113–126.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] M. Kipp, A. Heloir, 和 Q. Nguyen，“手语头像：动画和可理解性”，发表于*Intelligent Virtual
    Agents*。  Springer Berlin Heidelberg，2011年，第113–126页。'
- en: '[173] J. McDonald, R. Wolfe, J. Schnepp, J. Hochgesang, D. G. Jamrozik, M. Stumbo,
    L. Berke, M. Bialek, and F. Thomas, “An automated technique for real-time production
    of lifelike animations of american sign language,” *Universal Access in the Information
    Society*, vol. 15, pp. 551–566, 2016.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] J. McDonald, R. Wolfe, J. Schnepp, J. Hochgesang, D. G. Jamrozik, M.
    Stumbo, L. Berke, M. Bialek, 和 F. Thomas，“一种自动化技术用于实时生成逼真的美国手语动画”，*Universal Access
    in the Information Society*，第15卷，第551–566页，2016年。'
- en: '[174] S. Gibet, F. Lefebvre-Albaret, L. Hamon, R. Brun, and A. Turki, “Interactive
    editing in french sign language dedicated to virtual signers: Requirements and
    challenges,” *Universal Access in the Information Society*, vol. 15, pp. 525–539,
    2016.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] S. Gibet, F. Lefebvre-Albaret, L. Hamon, R. Brun, 和 A. Turki，“专门针对虚拟手语者的法语手语交互编辑：需求与挑战”，*Universal
    Access in the Information Society*，第15卷，第525–539页，2016年。'
- en: '[175] J. Zelinka and J. Kanis, “Neural sign language synthesis: Words are our
    glosses,” in *Proc. IEEE/CVF-WACV*, March 2020.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] J. Zelinka 和 J. Kanis，“神经手语合成：词语是我们的注释”，发表于*Proc. IEEE/CVF-WACV*，2020年3月。'
- en: '[176] N. C. Camgoz, O. Koller, S. Hadfield, and R. Bowden, “Multi-channel transformers
    for multi-articulatory sign language translation,” 2020.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] N. C. Camgoz, O. Koller, S. Hadfield, 和 R. Bowden，“用于多发音手语翻译的多通道变换器”，2020年。'
- en: '[177] B. Saunders, N. C. Camgöz, and R. Bowden, “Adversarial training for multi-channel
    sign language production,” in *The 31st British Machine Vision Virtual Conference*.   British
    Machine Vision Association, 2020.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] B. Saunders, N. C. Camgöz, 和 R. Bowden，“用于多通道手语生产的对抗训练”，发表于*第31届英国机器视觉虚拟会议*。
     British Machine Vision Association，2020年。'
- en: '[178] M. Inan, Y. Zhong, S. Hassan, L. Quandt, and M. Alikhani, “Modeling intensification
    for sign language generation: A computational approach,” in *Findings of the Association
    for Computational Linguistics*, 2022, pp. 2897–2911.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] M. Inan, Y. Zhong, S. Hassan, L. Quandt, 和 M. Alikhani，“手语生成的强化建模：一种计算方法”，发表于*Findings
    of the Association for Computational Linguistics*，2022年，第2897–2911页。'
- en: '[179] B. Saunders, N. C. Camgoz, and R. Bowden, “Signing at scale: Learning
    to co-articulate signs for large-scale photo-realistic sign language production,”
    in *Proc. IEEE/CVF-CVPR*, 2022, pp. 5141–5151.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] B. Saunders, N. C. Camgoz, 和 R. Bowden，“大规模手语生产：学习共同发音的手势以实现大规模照片级真实感手语生产”，发表于*Proc.
    IEEE/CVF-CVPR*，2022年，第5141–5151页。'
- en: '[180] P. Xie, Q. Zhang, Z. Li, H. Tang, Y. Du, and X. Hu, “Vector quantized
    diffusion model with codeunet for text-to-sign pose sequences generation,” *arXiv
    preprint arXiv:2208.09141*, 2022.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] P. Xie, Q. Zhang, Z. Li, H. Tang, Y. Du, 和 X. Hu，“带有Codeunet的向量量化扩散模型用于文本到手势姿势序列生成”，*arXiv预印本
    arXiv:2208.09141*，2022年。'
- en: '[181] C.-C. Chiu, L.-P. Morency, and S. Marsella, “Predicting co-verbal gestures:
    A deep and temporal modeling approach,” in *Proc. Intelligent Virtual Agents*.   Springer
    International Publishing, 2015, pp. 152–166.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] C.-C. Chiu, L.-P. Morency, 和 S. Marsella，“预测共语手势：一种深度和时间建模方法”，发表于*Proc.
    Intelligent Virtual Agents*。  Springer International Publishing，2015年，第152–166页。'
- en: '[182] S. Alexanderson, G. E. Henter, T. Kucherenko, and J. Beskow, “Style-controllable
    speech-driven gesture synthesis using normalising flows,” in *Computer Graphics
    Forum*, vol. 39, no. 2.   Wiley Online Library, 2020, pp. 487–496.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] S. Alexanderson, G. E. Henter, T. Kucherenko, 和 J. Beskow，“使用标准化流进行风格可控的语音驱动手势合成”，发表于*Computer
    Graphics Forum*，第39卷，第2期。  Wiley Online Library，2020年，第487–496页。'
- en: '[183] J. Li, D. Kang, W. Pei, X. Zhe, Y. Zhang, Z. He, and L. Bao, “Audio2gestures:
    Generating diverse gestures from speech audio with conditional variational autoencoders,”
    in *Proc. IEEE/CVF-ICCV*, 2021, pp. 11 293–11 302.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] J. Li, D. Kang, W. Pei, X. Zhe, Y. Zhang, Z. He, 和 L. Bao，“Audio2gestures:
    利用条件变分自编码器从语音音频生成多样化手势”，发表于*Proc. IEEE/CVF-ICCV*，2021年，第11 293–11 302页。'
- en: '[184] U. Bhattacharya, N. Rewkowski, A. Banerjee, P. Guhan, A. Bera, and D. Manocha,
    “Text2gestures: A transformer-based network for generating emotive body gestures
    for virtual agents,” in *IEEE virtual reality and 3D user interfaces (VR)*, 2021,
    pp. 1–10.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] U. Bhattacharya, N. Rewkowski, A. Banerjee, P. Guhan, A. Bera 和 D. Manocha，“Text2gestures：一种基于变压器的网络，用于为虚拟代理生成情感化的身体手势”，在
    *IEEE 虚拟现实与 3D 用户界面 (VR)*，2021，第 1–10 页。'
- en: '[185] S. Ghorbani, Y. Ferstl, and M.-A. Carbonneau, “Exemplar-based stylized
    gesture generation from speech: An entry to the GENEA challenge 2022,” in *Proc.
    ACM-International Conference on Multimodal Interaction*, 2022, pp. 778–783.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] S. Ghorbani, Y. Ferstl 和 M.-A. Carbonneau，“基于示例的风格化手势生成：GENEA 2022 挑战赛的参赛作品”，在
    *Proc. ACM-International Conference on Multimodal Interaction*，2022，第 778–783
    页。'
- en: '[186] M. Li and Y.-m. Cheung, “A novel motion based lip feature extraction
    for lip-reading,” in *Proc. IEEE-International Conference on Computational Intelligence
    and Security*, vol. 1, 2008, pp. 361–365.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] M. Li 和 Y.-m. Cheung，“一种新型运动基础的唇部特征提取方法用于唇读”，在 *Proc. IEEE-International
    Conference on Computational Intelligence and Security*，第 1 卷，2008，第 361–365 页。'
- en: '[187] S. Alizadeh, R. Boostani, and V. Asadpour, “Lip feature extraction and
    reduction for hmm-based visual speech recognition systems,” in *Pro. IEEE-9th
    International Conference on Signal Processing*, 2008, pp. 561–564.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] S. Alizadeh, R. Boostani 和 V. Asadpour，“基于 HMM 的视觉语音识别系统中的唇部特征提取与降维”，在
    *Proc. IEEE-9th International Conference on Signal Processing*，2008，第 561–564
    页。'
- en: '[188] X. Ma, L. Yan, and Q. Zhong, “Lip feature extraction based on improved
    jumping-snake model,” in *Proc. IEEE-35th Chinese Control Conference (CCC)*, 2016,
    pp. 6928–6933.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] X. Ma, L. Yan 和 Q. Zhong，“基于改进跳跃蛇模型的唇部特征提取”，在 *Proc. IEEE-35th Chinese
    Control Conference (CCC)*，2016，第 6928–6933 页。'
- en: '[189] Y. Lan, B.-J. Theobald, and R. Harvey, “View independent computer lip-reading,”
    in *Proc. IEEE-International Conference on Multimedia and Expo*, 2012, pp. 432–437.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Y. Lan, B.-J. Theobald 和 R. Harvey，“视角独立的计算机唇读”，在 *Proc. IEEE-International
    Conference on Multimedia and Expo*，2012，第 432–437 页。'
- en: '[190] T. Watanabe, K. Katsurada, and Y. Kanazawa, “Lip reading from multi view
    facial images using 3D-AAM,” in *Proc. ACCV*.   Springer Verlag, 2017, pp. 303–316.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] T. Watanabe, K. Katsurada 和 Y. Kanazawa，“基于 3D-AAM 的多视角面部图像唇读”，在 *Proc.
    ACCV*。Springer Verlag，2017，第 303–316 页。'
- en: '[191] L. Liu, G. Feng, and D. Beautemps, “Inner lips feature extraction based
    on clnf with hybrid dynamic template for cued speech,” *EURASIP Journal on Image
    and Video Processing*, vol. 2017, p. 1–15, 2017.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] L. Liu, G. Feng 和 D. Beautemps，“基于 clnf 和混合动态模板的内唇特征提取用于提示语音”，*EURASIP
    Journal on Image and Video Processing*，第 2017 卷，第 1–15 页，2017。'
- en: '[192] ——, “Extraction automatique de contour de levre a partir du modele clnf,”
    in *JEP-TALN-RECITAL 2016-conference conjointe 31e Journees d’Etudes sur la Parole,
    23e Traitement Automatique des Langues Naturelles, 18e Rencontre des Etudiants
    Chercheurs en Informatique pour le Traitement Automatique des Langues*, 2016.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] ——，“基于 clnf 模型的自动唇部轮廓提取”，在 *JEP-TALN-RECITAL 2016-联合会议第 31 届语言研究日，第 23
    届自然语言处理，第 18 届计算机科学学生研究者会议*，2016。'
- en: '[193] J. Wang, T. Wu, S. Wang, M. Yu, Q. Fang, J. Zhang, and L. Liu, “Three-dimensional
    lip motion network for text-independent speaker recognition,” in *Proc. IEEE-ICPR*,
    2021, p. 3380–3387.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] J. Wang, T. Wu, S. Wang, M. Yu, Q. Fang, J. Zhang 和 L. Liu，“用于文本独立说话人识别的三维唇部运动网络”，在
    *Proc. IEEE-ICPR*，2021，第 3380–3387 页。'
- en: '[194] A. Garg, J. Noyola, and S. Bagadia, “Lip reading using cnn and lstm,”
    *Technical report, Stanford University, CS231 n project report*, 2016.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] A. Garg, J. Noyola 和 S. Bagadia，“基于 CNN 和 LSTM 的唇读”，*技术报告，斯坦福大学，CS231
    n 项目报告*，2016。'
- en: '[195] D. Lee, J. Lee, and K.-E. Kim, “Multi-view automatic lip-reading using
    neural network,” in *Proc. ACCV Workshop on Multi-view Lip-reading Challenges*,
    2016.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] D. Lee, J. Lee 和 K.-E. Kim，“基于神经网络的多视角自动唇读”，在 *Proc. ACCV Workshop on
    Multi-view Lip-reading Challenges*，2016。'
- en: '[196] I. Fung and B. Mak, “End-to-end low-resource lip-reading with maxout
    CNN and LSTM,” in *Proc. IEEE-ICASSP*, 2018, pp. 2511–2515.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] I. Fung 和 B. Mak，“基于 maxout CNN 和 LSTM 的端到端低资源唇读”，在 *Proc. IEEE-ICASSP*，2018，第
    2511–2515 页。'
- en: '[197] K. Xu, D. Li, N. Cassimatis, and X. Wang, “LCANet: End-to-end lipreading
    with cascaded attention-CTC,” in *Proc. IEEE-FG*, 2018, pp. 548–555.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] K. Xu, D. Li, N. Cassimatis 和 X. Wang，“LCANet：端到端的唇读与级联注意力-CTC”，在 *Proc.
    IEEE-FG*，2018，第 548–555 页。'
- en: '[198] P. Wiriyathammabhum, “Spotfast networks with memory augmented lateral
    transformers for lipreading,” in *International Conference on Neural Information
    Processing*, 2020, pp. 554–561.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] P. Wiriyathammabhum，“带记忆增强的横向变压器的 Spotfast 网络用于唇读”，在 *国际神经信息处理会议*，2020，第
    554–561 页。'
- en: '[199] X. Weng and K. Kitani, “Learning spatio-temporal features with two-stream
    deep 3d cnns for lipreading,” *arXiv preprint arXiv:1905.02540*, 2019.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] X. Weng 和 K. Kitani，“使用双流深度3D卷积网络学习时空特征用于唇读”，*arXiv预印本 arXiv:1905.02540*，2019年。'
- en: '[200] D. Feng, S. Yang, and S. Shan, “An efficient software for building lip
    reading models without pains,” in *Proc. IEEE-ICMEW*, 2021, pp. 1–2.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] D. Feng, S. Yang 和 S. Shan，“一种高效的软件用于轻松构建唇读模型”，在 *IEEE-ICMEW* 会议录，2021年，第1–2页。'
- en: '[201] B. Xu, C. Lu, Y. Guo, and J. Wang, “Discriminative multi-modality speech
    recognition,” in *Proc. IEEE/CVF-CVPR*, 2020, pp. 14 433–14 442.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] B. Xu, C. Lu, Y. Guo 和 J. Wang，“区分性多模态语音识别”，在 *IEEE/CVF-CVPR* 会议录，2020年，第14 433–14 442页。'
- en: '[202] M. Luo, S. Yang, S. Shan, and X. Chen, “Pseudo-convolutional policy gradient
    for sequence-to-sequence lip-reading,” in *Proc. IEEE-FG*, 2020, pp. 273–280.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] M. Luo, S. Yang, S. Shan 和 X. Chen，“用于序列到序列唇读的伪卷积策略梯度”，在 *IEEE-FG* 会议录，2020年，第273–280页。'
- en: '[203] J. Gehring, Y. Miao, F. Metze, and A. Waibel, “Extracting deep bottleneck
    features using stacked auto-encoders,” in *Proc. IEEE-ICASSP*, 2013, pp. 3377–3381.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] J. Gehring, Y. Miao, F. Metze 和 A. Waibel，“使用堆叠自编码器提取深度瓶颈特征”，在 *IEEE-ICASSP*
    会议录，2013年，第3377–3381页。'
- en: '[204] K. Noda, Y. Yamaguchi, K. Nakadai, H. G. Okuno, and T. Ogata, “Audio-visual
    speech recognition using deep learning,” *Applied intelligence*, vol. 42, pp.
    722–737, 2015.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] K. Noda, Y. Yamaguchi, K. Nakadai, H. G. Okuno 和 T. Ogata，“使用深度学习的视听语音识别”，*应用智能*，第42卷，第722–737页，2015年。'
- en: '[205] S. Petridis and M. Pantic, “Deep complementary bottleneck features for
    visual speech recognition,” in *Proc. IEEE-ICASSP*, 2016, pp. 2304–2308.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] S. Petridis 和 M. Pantic，“用于视觉语音识别的深度互补瓶颈特征”，在 *IEEE-ICASSP* 会议录，2016年，第2304–2308页。'
- en: '[206] J. Wang, N. Gu, M. Yu, X. Li, Q. Fang, and L. Liu, “An attention self-supervised
    contrastive learning based three-stage model for hand shape feature representation
    in cued speech,” *arXiv preprint arXiv:2106.14016*, 2021.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] J. Wang, N. Gu, M. Yu, X. Li, Q. Fang 和 L. Liu，“一种基于注意力自监督对比学习的三阶段模型用于提示语音中的手形特征表示”，*arXiv预印本
    arXiv:2106.14016*，2021年。'
- en: '[207] M. Wand, J. Schmidhuber, and N. T. Vu, “Investigations on end-to-end
    audiovisual fusion,” in *Proc. IEEE-ICASSP*, 2018, pp. 3041–3045.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] M. Wand, J. Schmidhuber 和 N. T. Vu，“端到端视听融合的研究”，在 *IEEE-ICASSP* 会议录，2018年，第3041–3045页。'
- en: '[208] Y. Zhang, S. Yang, J. Xiao, S. Shan, and X. Chen, “Can we read speech
    beyond the lips? rethinking roi selection for deep visual speech recognition,”
    in *Proc. IEEE-FG*, 2020, pp. 356–363.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Y. Zhang, S. Yang, J. Xiao, S. Shan 和 X. Chen，“我们能否超越唇部阅读语音？重新思考深度视觉语音识别的ROI选择”，在
    *IEEE-FG* 会议录，2020年，第356–363页。'
- en: '[209] J. Xiao, S. Yang, Y. Zhang, S. Shan, and X. Chen, “Deformation flow based
    two-stream network for lip reading,” in *Proc. IEEE-FG*, 2020, pp. 364–370.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] J. Xiao, S. Yang, Y. Zhang, S. Shan 和 X. Chen，“基于形变流的双流网络用于唇读”，在 *IEEE-FG*
    会议录，2020年，第364–370页。'
- en: '[210] X. Zhao, S. Yang, S. Shan, and X. Chen, “Mutual information maximization
    for effective lip reading,” in *Proc. IEEE-FG*, 2020, pp. 420–427.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] X. Zhao, S. Yang, S. Shan 和 X. Chen，“有效唇读的互信息最大化”，在 *IEEE-FG* 会议录，2020年，第420–427页。'
- en: '[211] S. Bai, J. Z. Kolter, and V. Koltun, “An empirical evaluation of generic
    convolutional and recurrent networks for sequence modeling,” *arXiv preprint arXiv:1803.01271*,
    2018.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] S. Bai, J. Z. Kolter 和 V. Koltun，“通用卷积和递归网络在序列建模中的经验评估”，*arXiv预印本 arXiv:1803.01271*，2018年。'
- en: '[212] B. Martinez, P. Ma, S. Petridis, and M. Pantic, “Lipreading using temporal
    convolutional networks,” in *Proc. IEEE-ICASSP*, 2020, pp. 6319–6323.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] B. Martinez, P. Ma, S. Petridis 和 M. Pantic，“使用时间卷积网络的唇读”，在 *IEEE-ICASSP*
    会议录，2020年，第6319–6323页。'
- en: '[213] V. Ashish, S. Noam, P. Niki, U. Jakob, J. Llion, G. A. N, K. Łukasz,
    and P. Illia, “Attention is all you need,” in *Advances in Neural Information
    Processing Systems (NIPS)*, 2017.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] V. Ashish, S. Noam, P. Niki, U. Jakob, J. Llion, G. A. N, K. Łukasz 和
    P. Illia，“注意力即一切”，在 *神经信息处理系统进展（NIPS）*，2017年。'
- en: '[214] T. Afouras, J. S. Chung, and A. Zisserman, “Deep lip reading: a comparison
    of models and an online application,” *arXiv preprint arXiv:1806.06053*, 2018.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] T. Afouras, J. S. Chung 和 A. Zisserman，“深度唇读：模型比较及在线应用”，*arXiv预印本 arXiv:1806.06053*，2018年。'
- en: '[215] J. Son Chung, A. Senior, O. Vinyals, and A. Zisserman, “Lip reading sentences
    in the wild,” in *Proc. IEEE-CVPR*, 2017, pp. 6447–6456.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] J. Son Chung, A. Senior, O. Vinyals 和 A. Zisserman，“野外的唇读句子”，在 *IEEE-CVPR*
    会议录，2017年，第6447–6456页。'
- en: '[216] Y. Lu and H. Li, “Automatic lip-reading system based on deep convolutional
    neural network and attention-based long short-term memory,” *Applied Sciences*,
    vol. 9, no. 8, p. 1599, 2019.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] Y. Lu 和 H. Li，“基于深度卷积神经网络和基于注意力的长短期记忆的自动唇读系统”，*应用科学*，第9卷，第8期，第1599页，2019年。'
- en: '[217] P. Zhou, W. Yang, W. Chen, Y. Wang, and J. Jia, “Modality attention for
    end-to-end audio-visual speech recognition,” in *Proc. IEEE-ICASSP*, 2019, pp.
    6565–6569.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] P. Zhou, W. Yang, W. Chen, Y. Wang, 和 J. Jia, “端到端音频视觉语音识别中的模态注意力，” 在
    *Proc. IEEE-ICASSP*，2019 年，第 6565–6569 页。'
- en: '[218] X. Zhang, H. Gong, X. Dai, F. Yang, N. Liu, and M. Liu, “Understanding
    pictograph with facial features: End-to-end sentence-level lip reading of chinese,”
    in *Proc. Conf AAAI Artif. Intell.*, vol. 33, no. 01, 2019, pp. 9211–9218.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] X. Zhang, H. Gong, X. Dai, F. Yang, N. Liu, 和 M. Liu, “通过面部特征理解象形文字：中文的端到端句子级唇读，”
    在 *Proc. Conf AAAI Artif. Intell.*，第 33 卷，第 01 期，2019 年，第 9211–9218 页。'
- en: '[219] A. Torfi, S. M. Iranmanesh, N. Nasrabadi, and J. Dawson, “3d convolutional
    neural networks for cross audio-visual matching recognition,” *IEEE Access*, vol. 5,
    pp. 22 081–22 091, 2017.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] A. Torfi, S. M. Iranmanesh, N. Nasrabadi, 和 J. Dawson, “用于跨音频视觉匹配识别的
    3D 卷积神经网络，” *IEEE Access*，第 5 卷，第 22 081–22 091 页，2017 年。'
- en: '[220] P. Heracleous, D. Beautemps, and N. Aboutabit, “Cued speech automatic
    recognition in normal-hearing and deaf subjects,” *Speech Communication*, vol. 52,
    no. 6, pp. 504–512, 2010.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] P. Heracleous, D. Beautemps, 和 N. Aboutabit, “正常听力和听力障碍者中的提示语音自动识别，”
    *Speech Communication*，第 52 卷，第 6 期，第 504–512 页，2010 年。'
- en: '[221] P. Heracleous, D. Beautemps, and N. Hagita, “Continuous phoneme recognition
    in cued speech for french,” in *Proc. IEEE-EUSIPCO*, 2012, pp. 2090–2093.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] P. Heracleous, D. Beautemps, 和 N. Hagita, “法语提示语音中的连续音素识别，” 在 *Proc.
    IEEE-EUSIPCO*，2012 年，第 2090–2093 页。'
- en: '[222] T. Burger, A. Caplier, and S. Mancini, “Cued speech hand gestures recognition
    tool,” in *Proc. IEEE-EUSIPCO*, 2005, pp. 1–4.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] T. Burger, A. Caplier, 和 S. Mancini, “提示语音手势识别工具，” 在 *Proc. IEEE-EUSIPCO*，2005
    年，第 1–4 页。'
- en: '[223] L. Gao, S. Huang, and L. Liu, “A novel interpretable and generalizable
    re-synchronization model for cued speech based on a multi-cuer corpus,” *arXiv
    preprint arXiv:2306.02596*, 2023.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] L. Gao, S. Huang, 和 L. Liu, “一种新颖的可解释且可推广的基于多提示语料库的提示语音重新同步模型，” *arXiv
    预印本 arXiv:2306.02596*，2023 年。'
- en: '[224] Y. Zhang, L. Liu, and L. Liu, “Cuing without sharing: A federated cued
    speech recognition framework via mutual knowledge distillation,” *arXiv preprint
    arXiv:2308.03432*, 2023.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Y. Zhang, L. Liu, 和 L. Liu, “无共享的提示语音识别框架：通过相互知识蒸馏，” *arXiv 预印本 arXiv:2308.03432*，2023
    年。'
- en: '[225] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, “Connectionist
    temporal classification: labelling unsegmented sequence data with recurrent neural
    networks,” in *Proc. ICML*, 2006, pp. 369–376.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] A. Graves, S. Fernández, F. Gomez, 和 J. Schmidhuber, “连接主义时间分类：用递归神经网络对未分割序列数据进行标注，”
    在 *Proc. ICML*，2006 年，第 369–376 页。'
- en: '[226] R. Boeck, K. Bergmann, and P. Jaecks, “Disposition recognition from spontaneous
    speech towards a combination with co-speech gestures,” in *Proceedings of the
    2nd International Workshop on Multimodal Analyses enabling Artificial Agents in
    Human-Machine Interaction*, 2014.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] R. Boeck, K. Bergmann, 和 P. Jaecks, “从自发语音中识别情绪倾向，朝着与语音手势结合的方向发展，” 在
    *Proceedings of the 2nd International Workshop on Multimodal Analyses enabling
    Artificial Agents in Human-Machine Interaction*，2014 年。'
- en: '[227] U. Bhattacharya, E. Childs, N. Rewkowski, and D. Manocha, “Speech2affectivegestures:
    Synthesizing co-speech gestures with generative adversarial affective expression
    learning,” in *Proc. ACM MM*, 2021, pp. 2027–2036.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] U. Bhattacharya, E. Childs, N. Rewkowski, 和 D. Manocha, “Speech2affectivegestures：利用生成对抗性情感表达学习合成语音手势，”
    在 *Proc. ACM MM*，2021 年，第 2027–2036 页。'
- en: '[228] Y. Wen, B. Raj, and R. Singh, “Face reconstruction from voice using generative
    adversarial networks,” *Advances in Neural Information Processing Systems (NIPS)*,
    vol. 32, 2019.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Y. Wen, B. Raj, 和 R. Singh, “利用生成对抗网络从语音中重建人脸，” *Advances in Neural Information
    Processing Systems (NIPS)*，第 32 卷，2019 年。'
- en: '[229] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embedding
    for face recognition and clustering,” in *Proc. IEEE-CVPR*, 2015, pp. 815–823.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] F. Schroff, D. Kalenichenko, 和 J. Philbin, “Facenet：用于人脸识别和聚类的统一嵌入，”
    在 *Proc. IEEE-CVPR*，2015 年，第 815–823 页。'
- en: '[230] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular
    margin loss for deep face recognition,” in *Proc. IEEE/CVF-CVPR*, 2019, pp. 4690–4699.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] J. Deng, J. Guo, N. Xue, 和 S. Zafeiriou, “Arcface：深度人脸识别的加性角度边距损失，” 在
    *Proc. IEEE/CVF-CVPR*，2019 年，第 4690–4699 页。'
- en: '[231] B. Saunders, N. C. Camgoz, and R. Bowden, “Progressive transformers for
    end-to-end sign language production,” in *Proc. ECCV*, 2020, pp. 687–705.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] B. Saunders, N. C. Camgoz, 和 R. Bowden, “端到端手语生成的渐进式变换器，” 在 *Proc. ECCV*，2020
    年，第 687–705 页。'
- en: '[232] S. Stoll, N. C. Camgöz, S. Hadfield, and R. Bowden, “Sign language production
    using neural machine translation and generative adversarial networks,” in *Proc.
    BMVC*.   British Machine Vision Association, 2018.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] S. Stoll, N. C. Camgöz, S. Hadfield, 和 R. Bowden，“使用神经机器翻译和生成对抗网络的手语生成，”在*Proc.
    BMVC*。英国机器视觉协会，2018年。'
- en: '[233] N. Vasani, P. Autee, S. Kalyani, and R. Karani, “Generation of indian
    sign language by sentence processing and generative adversarial networks,” in
    *Proc. IEEE-ICISS*, 2020, pp. 1250–1255.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] N. Vasani, P. Autee, S. Kalyani, 和 R. Karani，“通过句子处理和生成对抗网络生成印度手语，”在*Proc.
    IEEE-ICISS*，2020年，第1250–1255页。'
- en: '[234] L. Ventura, A. Duarte, and X. Giró-i Nieto, “Can everybody sign now?
    exploring sign language video generation from 2d poses,” *arXiv preprint arXiv:2012.10941*,
    2020.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] L. Ventura, A. Duarte, 和 X. Giró-i Nieto，“现在每个人都能手语吗？探索从2D姿态生成手语视频，”*arXiv预印本
    arXiv:2012.10941*，2020年。'
- en: '[235] Q. Xiao, M. Qin, and Y. Yin, “Skeleton-based chinese sign language recognition
    and generation for bidirectional communication between deaf and hearing people,”
    *Neural networks*, vol. 125, pp. 41–55, 2020.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] Q. Xiao, M. Qin, 和 Y. Yin，“基于骨架的中文手语识别与生成，用于聋人与听人之间的双向交流，”*Neural networks*，第125卷，第41–55页，2020年。'
- en: '[236] E. Rothauser, “Ieee recommended practice for speech quality measurements,”
    *IEEE Transactions on Audio and Electroacoustics*, vol. 17, no. 3, pp. 225–246,
    1969.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] E. Rothauser，“IEEE推荐的语音质量测量实践，”*IEEE音频与电声学学报*，第17卷，第3期，第225–246页，1969年。'
- en: '[237] J. Kim, J. Kim, and S. Choi, “Flame: Free-form language-based motion
    synthesis & editing,” in *Proc. Conf AAAI Artif. Intell.*, vol. 37, no. 7, 2023,
    pp. 8255–8263.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] J. Kim, J. Kim, 和 S. Choi，“Flame：基于自由形式语言的运动合成与编辑，”在*Proc. Conf AAAI
    Artif. Intell.*，第37卷，第7期，2023年，第8255–8263页。'
- en: '[238] J. Cassell, H. H. Vilhjálmsson, and T. Bickmore, “Beat: the behavior
    expression animation toolkit,” in *Proc. ACM SIGGRAPH*, 2001, pp. 477–486.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] J. Cassell, H. H. Vilhjálmsson, 和 T. Bickmore，“Beat：行为表达动画工具包，”在*Proc.
    ACM SIGGRAPH*，2001年，第477–486页。'
- en: '[239] J. Cassell, C. Pelachaud, N. Badler, M. Steedman, B. Achorn, T. Becket,
    B. Douville, S. Prevost, and M. Stone, “Animated conversation: rule-based generation
    of facial expression, gesture & spoken intonation for multiple conversational
    agents,” in *Proc. ACM SIGGRAPH*, 1994, pp. 413–420.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] J. Cassell, C. Pelachaud, N. Badler, M. Steedman, B. Achorn, T. Becket,
    B. Douville, S. Prevost, 和 M. Stone，“动画对话：基于规则的面部表情、手势和口语语调生成用于多个对话体，”在*Proc.
    ACM SIGGRAPH*，1994年，第413–420页。'
- en: '[240] P. Wagner, Z. Malisz, and S. Kopp, “Gesture and speech in interaction:
    An overview,” *Speech Communication*, vol. 57, pp. 209–232, 2014.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] P. Wagner, Z. Malisz, 和 S. Kopp，“手势与语音的互动：概述，”*语音通信*，第57卷，第209–232页，2014年。'
- en: '[241] S. Levine, P. Krähenbühl, S. Thrun, and V. Koltun, “Gesture controllers,”
    *ACM Transactions on Graphics (TOG)*, vol. 29, no. 4, pp. 1–11, 2010.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] S. Levine, P. Krähenbühl, S. Thrun, 和 V. Koltun，“手势控制器，”*ACM图形学交易（TOG）*，第29卷，第4期，第1–11页，2010年。'
- en: '[242] S. Qian, Z. Tu, Y. Zhi, W. Liu, and S. Gao, “Speech drives templates:
    Co-speech gesture synthesis with learned templates,” in *Proc. IEEE/CVF-ICCV*,
    2021, pp. 11 077–11 086.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] S. Qian, Z. Tu, Y. Zhi, W. Liu, 和 S. Gao，“语音驱动模板：基于学习模板的语音手势合成，”在*Proc.
    IEEE/CVF-ICCV*，2021年，第11 077–11 086页。'
- en: '[243] P. Dhariwal and A. Q. Nichol, “Diffusion models beat GANs on image synthesis,”
    in *Advances in Neural Information Processing Systems (NIPS)*, 2021.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] P. Dhariwal 和 A. Q. Nichol，“扩散模型在图像合成上胜过GANs，”在*神经信息处理系统（NIPS）进展*，2021年。'
- en: '[244] L. Zhu, X. Liu, X. Liu, R. Qian, Z. Liu, and L. Yu, “Taming diffusion
    models for audio-driven co-speech gesture generation,” in *Proc. IEEE/CVF-CVPR*,
    2023, pp. 10 544–10 553.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] L. Zhu, X. Liu, X. Liu, R. Qian, Z. Liu, 和 L. Yu，“驯化扩散模型用于音频驱动的语音手势生成，”在*Proc.
    IEEE/CVF-CVPR*，2023年，第10 544–10 553页。'
- en: '[245] H. Teager and S. Teager, “Evidence for nonlinear sound production mechanisms
    in the vocal tract,” *Speech production and speech modelling*, pp. 241–261, 1990.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] H. Teager 和 S. Teager，“声道中非线性声音产生机制的证据，”*语音产生与语音建模*，第241–261页，1990年。'
- en: '[246] T.-H. Oh, T. Dekel, C. Kim, I. Mosseri, W. T. Freeman, M. Rubinstein,
    and W. Matusik, “Speech2face: Learning the face behind a voice,” in *Proc. IEEE/CVF-CVPR*,
    2019, pp. 7539–7548.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] T.-H. Oh, T. Dekel, C. Kim, I. Mosseri, W. T. Freeman, M. Rubinstein,
    和 W. Matusik，“Speech2face：学习声音背后的面孔，”在*Proc. IEEE/CVF-CVPR*，2019年，第7539–7548页。'
- en: '[247] O. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,” in
    *Proc. BMVC*.   British Machine Vision Association, 2015.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] O. Parkhi, A. Vedaldi, 和 A. Zisserman，“深度人脸识别，”在*Proc. BMVC*。英国机器视觉协会，2015年。'
- en: '[248] A. C. Duarte, F. Roldan, M. Tubau, J. Escur, S. Pascual, A. Salvador,
    E. Mohedano, K. McGuinness, J. Torres, and X. Giro-i Nieto, “Wav2pix: Speech-conditioned
    face generation using generative adversarial networks.” in *Proc. IEEE-ICASSP*,
    2019, pp. 8633–8637.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] A. C. Duarte, F. Roldan, M. Tubau, J. Escur, S. Pascual, A. Salvador,
    E. Mohedano, K. McGuinness, J. Torres, 和 X. Giro-i Nieto，“Wav2pix：基于生成对抗网络的语音条件面部生成”，见于*Proc.
    IEEE-ICASSP*，2019年，第8633–8637页。'
- en: '[249] S. Pascual, A. Bonafonte, and J. Serrà, “SEGAN: Speech enhancement generative
    adversarial network,” *Proc. Interspeech*, pp. 3642–3646, 2017.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] S. Pascual, A. Bonafonte, 和 J. Serrà，“SEGAN：语音增强生成对抗网络”，见于*Proc. Interspeech*，第3642–3646页，2017年。'
- en: '[250] Z. Fang, Z. Liu, T. Liu, C.-C. Hung, J. Xiao, and G. Feng, “Facial expression
    gan for voice-driven face generation,” *The Visual Computer*, pp. 1–14, 2022.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] Z. Fang, Z. Liu, T. Liu, C.-C. Hung, J. Xiao, 和 G. Feng，“用于语音驱动面部生成的面部表情GAN”，*The
    Visual Computer*，第1–14页，2022年。'
- en: '[251] F. Cole, D. Belanger, D. Krishnan, A. Sarna, I. Mosseri, and W. T. Freeman,
    “Synthesizing normalized faces from facial identity features,” in *Proc. IEEE-CVPR*,
    2017, pp. 3703–3712.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] F. Cole, D. Belanger, D. Krishnan, A. Sarna, I. Mosseri, 和 W. T. Freeman，“从面部身份特征合成标准化面孔”，见于*Proc.
    IEEE-CVPR*，2017年，第3703–3712页。'
- en: '[252] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial networks,” *Communications
    of the ACM*, vol. 63, no. 11, pp. 139–144, 2020.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络”，*Communications of the ACM*，第63卷，第11期，第139–144页，2020年。'
- en: '[253] J. Wang, Z. Wang, X. Hu, X. Li, Q. Fang, and L. Liu, “Residual-guided
    personalized speech synthesis based on face image,” in *Proc. IEEE-ICASSP*, 2022,
    p. 4743–4747.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] J. Wang, Z. Wang, X. Hu, X. Li, Q. Fang, 和 L. Liu，“基于面部图像的残差引导个性化语音合成”，见于*Proc.
    IEEE-ICASSP*，2022年，第4743–4747页。'
- en: '[254] D. E. King, “Dlib-ml: A machine learning toolkit,” *The Journal of Machine
    Learning Research*, vol. 10, pp. 1755–1758, 2009.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] D. E. King，“Dlib-ml：一个机器学习工具包”，*The Journal of Machine Learning Research*，第10卷，第1755–1758页，2009年。'
- en: '[255] L. Chen, Z. Li, R. K. Maddox, Z. Duan, and C. Xu, “Lip movements generation
    at a glance,” in *Proc. ECCV*, 2018, pp. 520–535.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] L. Chen, Z. Li, R. K. Maddox, Z. Duan, 和 C. Xu，“瞬间生成唇部运动”，见于*Proc. ECCV*，2018年，第520–535页。'
- en: '[256] Y. Song, J. Zhu, D. Li, A. Wang, and H. Qi, “Talking face generation
    by conditional recurrent adversarial network,” in *Proc. IJCAI*, 2019, pp. 919–925.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] Y. Song, J. Zhu, D. Li, A. Wang, 和 H. Qi，“通过条件递归对抗网络生成谈话面孔”，见于*Proc.
    IJCAI*，2019年，第919–925页。'
- en: '[257] H. Zhou, Y. Liu, Z. Liu, P. Luo, and X. Wang, “Talking face generation
    by adversarially disentangled audio-visual representation,” in *Proc. Conf AAAI
    Artif. Intell.*, vol. 33, no. 01, 2019, pp. 9299–9306.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] H. Zhou, Y. Liu, Z. Liu, P. Luo, 和 X. Wang，“通过对抗性解耦的音频-视觉表示生成谈话面孔”，见于*Proc.
    Conf AAAI Artif. Intell.*，第33卷，第01期，2019年，第9299–9306页。'
- en: '[258] L. Chen, R. K. Maddox, Z. Duan, and C. Xu, “Hierarchical cross-modal
    talking face generation with dynamic pixel-wise loss,” in *Proc. IEEE/CVF-CVPR*,
    2019, pp. 7832–7841.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] L. Chen, R. K. Maddox, Z. Duan, 和 C. Xu，“具有动态像素级损失的分层跨模态谈话面孔生成”，见于*Proc.
    IEEE/CVF-CVPR*，2019年，第7832–7841页。'
- en: '[259] K. Vougioukas, S. Petridis, and M. Pantic, “End-to-end speech-driven
    realistic facial animation with temporal GANs.” in *CVPR Workshops*, 2019, pp.
    37–40.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] K. Vougioukas, S. Petridis, 和 M. Pantic，“端到端语音驱动的逼真面部动画与时序GAN”，见于*CVPR
    Workshops*，2019年，第37–40页。'
- en: '[260] T. Kefalas, K. Vougioukas, Y. Panagakis, S. Petridis, J. Kossaifi, and
    M. Pantic, “Speech-driven facial animation using polynomial fusion of features,”
    in *Proc. IEEE-ICASSP*, 2020, pp. 3487–3491.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] T. Kefalas, K. Vougioukas, Y. Panagakis, S. Petridis, J. Kossaifi, 和
    M. Pantic，“基于多项式特征融合的语音驱动面部动画”，见于*Proc. IEEE-ICASSP*，2020年，第3487–3491页。'
- en: '[261] S. Sinha, S. Biswas, and B. Bhowmick, “Identity-preserving realistic
    talking face generation,” in *Proc. IEEE-IJCNN*, 2020, pp. 1–10.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] S. Sinha, S. Biswas, 和 B. Bhowmick，“保持身份的逼真谈话面孔生成”，见于*Proc. IEEE-IJCNN*，2020年，第1–10页。'
- en: '[262] W. Wang, Y. Wang, J. Sun, Q. Liu, J. Liang, and T. Li, “Speech driven
    talking head generation via attentional landmarks based representation.” in *Proc.
    Interspeech*, 2020, pp. 1326–1330.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] W. Wang, Y. Wang, J. Sun, Q. Liu, J. Liang, 和 T. Li，“通过基于注意力的标记表示生成语音驱动的谈话头像”，见于*Proc.
    Interspeech*，2020年，第1326–1330页。'
- en: '[263] S. E. Eskimez, R. K. Maddox, C. Xu, and Z. Duan, “End-to-end generation
    of talking faces from noisy speech,” in *Proc. IEEE-ICASSP*, 2020, pp. 1948–1952.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] S. E. Eskimez, R. K. Maddox, C. Xu, 和 Z. Duan，“从噪声语音生成谈话面孔的端到端方法”，见于*Proc.
    IEEE-ICASSP*，2020年，第1948–1952页。'
- en: '[264] R. Yi, Z. Ye, J. Zhang, H. Bao, and Y.-J. Liu, “Audio-driven talking
    face video generation with learning-based personalized head pose,” *arXiv preprint
    arXiv:2002.10137*, 2020.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] R. Yi, Z. Ye, J. Zhang, H. Bao, 和 Y.-J. Liu，“基于学习的个性化头部姿势的音频驱动谈话面孔视频生成”，*arXiv
    preprint arXiv:2002.10137*，2020年。'
- en: '[265] L. Chen, G. Cui, C. Liu, Z. Li, Z. Kou, Y. Xu, and C. Xu, “Talking-head
    generation with rhythmic head motion,” in *Proc. ECCV*, 2020.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] L. Chen, G. Cui, C. Liu, Z. Li, Z. Kou, Y. Xu, 和 C. Xu，“具有节奏头部运动的对话头生成”，见
    *Proc. ECCV*，2020 年。'
- en: '[266] G. Mittal and B. Wang, “Animating face using disentangled audio representations,”
    in *Proc. IEEE/CVF-WACV*, 2020, pp. 3290–3298.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] G. Mittal 和 B. Wang，“使用解耦音频表示进行面部动画”，见 *Proc. IEEE/CVF-WACV*，2020 年，页
    3290–3298。'
- en: '[267] H. Zhu, H. Huang, Y. Li, A. Zheng, and R. He, “Arbitrary talking face
    generation via attentional audio-visual coherence learning,” in *Proc. IJCAI*,
    2021, pp. 2362–2368.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] H. Zhu, H. Huang, Y. Li, A. Zheng, 和 R. He，“通过注意力音频-视觉一致性学习生成任意对话面孔”，见
    *Proc. IJCAI*，2021 年，页 2362–2368。'
- en: '[268] C. Zhang, Y. Zhao, Y. Huang, M. Zeng, S. Ni, M. Budagavi, and X. Guo,
    “Facial: Synthesizing dynamic talking face with implicit attribute learning,”
    in *Proc. IEEE/CVF-ICCV*, 2021, pp. 3867–3876.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] C. Zhang, Y. Zhao, Y. Huang, M. Zeng, S. Ni, M. Budagavi, 和 X. Guo，“Facial:
    通过隐式属性学习合成动态对话面孔”，见 *Proc. IEEE/CVF-ICCV*，2021 年，页 3867–3876。'
- en: '[269] S. Si, J. Wang, X. Qu, N. Cheng, W. Wei, X. Zhu, and J. Xiao, “Speech2video:
    Cross-modal distillation for speech to video generation,” *arXiv preprint arXiv:2107.04806*,
    2021.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] S. Si, J. Wang, X. Qu, N. Cheng, W. Wei, X. Zhu, 和 J. Xiao，“Speech2video:
    语音到视频生成的跨模态蒸馏”，*arXiv 预印本 arXiv:2107.04806*，2021 年。'
- en: '[270] S. Chen, Z. Liu, J. Liu, Z. Yan, and L. Wang, “Talking head generation
    with audio and speech related facial action units,” *arXiv preprint arXiv:2110.09951*,
    2021.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] S. Chen, Z. Liu, J. Liu, Z. Yan, 和 L. Wang，“结合音频和语音相关面部动作单元生成对话头”，*arXiv
    预印本 arXiv:2110.09951*，2021 年。'
- en: '[271] H. Zhou, Y. Sun, W. Wu, C. C. Loy, X. Wang, and Z. Liu, “Pose-controllable
    talking face generation by implicitly modularized audio-visual representation,”
    in *Proc. IEEE/CVF-CVPR*, 2021, pp. 4176–4186.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] H. Zhou, Y. Sun, W. Wu, C. C. Loy, X. Wang, 和 Z. Liu，“通过隐式模块化音频-视觉表示生成姿态可控的对话面孔”，见
    *Proc. IEEE/CVF-CVPR*，2021 年，页 4176–4186。'
- en: '[272] B. Liang, Y. Pan, Z. Guo, H. Zhou, Z. Hong, X. Han, J. Han, J. Liu, E. Ding,
    and J. Wang, “Expressive talking head generation with granular audio-visual control,”
    in *Proc. IEEE/CVF-CVPR*, 2022, pp. 3387–3396.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] B. Liang, Y. Pan, Z. Guo, H. Zhou, Z. Hong, X. Han, J. Han, J. Liu, E.
    Ding, 和 J. Wang，“具有粒度音频-视觉控制的富有表现力的对话头生成”，见 *Proc. IEEE/CVF-CVPR*，2022 年，页 3387–3396。'
- en: '[273] S. Wang, L. Li, Y. Ding, and X. Yu, “One-shot talking face generation
    from single-speaker audio-visual correlation learning,” in *Proc. Conf AAAI Artif.
    Intell.*, vol. 36, no. 3, 2022, pp. 2531–2539.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] S. Wang, L. Li, Y. Ding, 和 X. Yu，“从单一说话者音频-视觉相关学习生成一-shot 对话面孔”，见 *Proc.
    Conf AAAI Artif. Intell.*，第 36 卷，第 3 期，2022 年，页 2531–2539。'
- en: '[274] X. Ji, H. Zhou, K. Wang, Q. Wu, W. Wu, F. Xu, and X. Cao, “Eamm: One-shot
    emotional talking face via audio-based emotion-aware motion model,” in *Proc.
    ACM SIGGRAPH*, 2022, pp. 1–10.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] X. Ji, H. Zhou, K. Wang, Q. Wu, W. Wu, F. Xu, 和 X. Cao，“Eamm: 通过基于音频的情感感知运动模型生成一-shot
    情感对话面孔”，见 *Proc. ACM SIGGRAPH*，2022 年，页 1–10。'
- en: '[275] S. Gururani, A. Mallya, T.-C. Wang, R. Valle, and M.-Y. Liu, “Spacex:
    Speech-driven portrait animation with controllable expression,” *arXiv preprint
    arXiv:2211.09809*, 2022.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] S. Gururani, A. Mallya, T.-C. Wang, R. Valle, 和 M.-Y. Liu，“Spacex: 具有可控表情的语音驱动肖像动画”，*arXiv
    预印本 arXiv:2211.09809*，2022 年。'
- en: '[276] R. Wu, Y. Yu, F. Zhan, J. Zhang, X. Zhang, and S. Lu, “Audio-driven talking
    face generation with diverse yet realistic facial animations,” *arXiv preprint
    arXiv:2304.08945*, 2023.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] R. Wu, Y. Yu, F. Zhan, J. Zhang, X. Zhang, 和 S. Lu，“音频驱动的对话面孔生成，具有多样化而现实的面部动画”，*arXiv
    预印本 arXiv:2304.08945*，2023 年。'
- en: '[277] J. Liu, X. Wang, X. Fu, Y. Chai, C. Yu, J. Dai, and J. Han, “Opt: One-shot
    pose-controllable talking head generation,” in *Proc. IEEE-ICASSP*, 2023, pp.
    1–5.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] J. Liu, X. Wang, X. Fu, Y. Chai, C. Yu, J. Dai, 和 J. Han，“Opt: 一-shot
    姿态可控对话头生成”，见 *Proc. IEEE-ICASSP*，2023 年，页 1–5。'
- en: '[278] D. Wang, Y. Deng, Z. Yin, H.-Y. Shum, and B. Wang, “Progressive disentangled
    representation learning for fine-grained controllable talking head synthesis,”
    in *Proc. IEEE/CVF-CVPR*, 2023, pp. 17 979–17 989.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] D. Wang, Y. Deng, Z. Yin, H.-Y. Shum, 和 B. Wang，“用于精细可控对话头合成的渐进解耦表示学习”，见
    *Proc. IEEE/CVF-CVPR*，2023 年，页 17 979–17 989。'
- en: '[279] L. Zhang, Q. Chen, and Z. Liu, “Talking head generation for media interaction
    system with feature disentanglement,” in *Proc. IEEE-ICPADS*.   IEEE, 2023, pp.
    403–410.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] L. Zhang, Q. Chen, 和 Z. Liu，“用于媒体交互系统的对话头生成与特征解耦”，见 *Proc. IEEE-ICPADS*，IEEE，2023
    年，页 403–410。'
- en: '[280] O. Wiles, A. Koepke, and A. Zisserman, “X2face: A network for controlling
    face generation using images, audio, and pose codes,” in *Proc. ECCV*, 2018, pp.
    670–686.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] O. Wiles, A. Koepke, 和 A. Zisserman，“X2face: 使用图像、音频和姿态编码控制面孔生成的网络”，见
    *Proc. ECCV*，2018 年，页 670–686。'
- en: '[281] A. Jamaludin, J. S. Chung, and A. Zisserman, “You said that?: Synthesising
    talking faces from audio,” *International Journal of Computer Vision*, vol. 127,
    pp. 1767–1779, 2019.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] A. Jamaludin, J. S. Chung, 和 A. Zisserman，“你说了什么？：从音频合成说话人脸，” *International
    Journal of Computer Vision*，第127卷，页码1767–1779，2019年。'
- en: '[282] X. Wen, M. Wang, C. Richardt, Z.-Y. Chen, and S.-M. Hu, “Photorealistic
    audio-driven video portraits,” *IEEE Transactions on Visualization and Computer
    Graphics*, vol. 26, no. 12, pp. 3457–3466, 2020.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] X. Wen, M. Wang, C. Richardt, Z.-Y. Chen, 和 S.-M. Hu，“逼真的音频驱动视频肖像，” *IEEE
    Transactions on Visualization and Computer Graphics*，第26卷，第12期，页码3457–3466，2020年。'
- en: '[283] A. Lahiri, V. Kwatra, C. Frueh, J. Lewis, and C. Bregler, “Lipsync3d:
    Data-efficient learning of personalized 3d talking faces from video using pose
    and lighting normalization,” in *Proc. IEEE/CVF-CVPR*, 2021, pp. 2755–2764.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] A. Lahiri, V. Kwatra, C. Frueh, J. Lewis, 和 C. Bregler，“Lipsync3d：从视频中高效学习个性化的3D说话人脸，使用姿态和光照归一化，”见于
    *Proc. IEEE/CVF-CVPR*，2021年，页码2755–2764。'
- en: '[284] Y. Lu, J. Chai, and X. Cao, “Live speech portraits: real-time photorealistic
    talking-head animation,” *ACM Transactions on Graphics (TOG)*, vol. 40, no. 6,
    pp. 1–17, 2021.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] Y. Lu, J. Chai, 和 X. Cao，“实时语音肖像：实时逼真说话头动画，” *ACM Transactions on Graphics
    (TOG)*，第40卷，第6期，页码1–17，2021年。'
- en: '[285] D. Bigioi, H. Jordan, R. Jain, R. McDonnell, and P. Corcoran, “Pose-aware
    speech driven facial landmark animation pipeline for automated dubbing,” *IEEE
    Access*, vol. 10, pp. 133 357–133 369, 2022.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] D. Bigioi, H. Jordan, R. Jain, R. McDonnell, 和 P. Corcoran，“姿态感知语音驱动的面部标志动画管道用于自动配音，”
    *IEEE Access*，第10卷，页码133357–133369，2022年。'
- en: '[286] W. Zhang, X. Cun, X. Wang, Y. Zhang, X. Shen, Y. Guo, Y. Shan, and F. Wang,
    “Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven
    single image talking face animation,” in *Proc. IEEE/CVF-CVPR*, 2023, pp. 8652–8661.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] W. Zhang, X. Cun, X. Wang, Y. Zhang, X. Shen, Y. Guo, Y. Shan, 和 F. Wang，“Sadtalker：学习真实感3D运动系数以进行风格化音频驱动的单图像人脸动画，”见于
    *Proc. IEEE/CVF-CVPR*，2023年，页码8652–8661。'
- en: '[287] S. Yao, R. Zhong, Y. Yan, G. Zhai, and X. Yang, “Dfa-nerf: personalized
    talking head generation via disentangled face attributes neural rendering,” *arXiv
    preprint arXiv:2201.00791*, 2022.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] S. Yao, R. Zhong, Y. Yan, G. Zhai, 和 X. Yang，“Dfa-nerf：通过解耦面部属性神经渲染生成个性化说话人脸，”
    *arXiv preprint arXiv:2201.00791*，2022年。'
- en: '[288] S. Shen, W. Li, Z. Zhu, Y. Duan, J. Zhou, and J. Lu, “Learning dynamic
    facial radiance fields for few-shot talking head synthesis,” in *Proc. ECCV*,
    2022, pp. 666–682.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] S. Shen, W. Li, Z. Zhu, Y. Duan, J. Zhou, 和 J. Lu，“学习动态面部辐射场以进行少样本人脸合成，”见于
    *Proc. ECCV*，2022年，页码666–682。'
- en: '[289] X. Liu, Y. Xu, Q. Wu, H. Zhou, W. Wu, and B. Zhou, “Semantic-aware implicit
    neural audio-driven video portrait generation,” in *Proc. ECCV*, 2022, pp. 106–125.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] X. Liu, Y. Xu, Q. Wu, H. Zhou, W. Wu, 和 B. Zhou，“语义感知隐式神经音频驱动视频肖像生成，”见于
    *Proc. ECCV*，2022年，页码106–125。'
- en: '[290] Z. Yu, Z. Yin, D. Zhou, D. Wang, F. Wong, and B. Wang, “Talking head
    generation with probabilistic audio-to-visual diffusion priors,” *arXiv preprint
    arXiv:2212.04248*, 2022.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] Z. Yu, Z. Yin, D. Zhou, D. Wang, F. Wong, 和 B. Wang，“基于概率音频到视觉扩散先验的人脸生成，”
    *arXiv preprint arXiv:2212.04248*，2022年。'
- en: '[291] Y. Zhua, C. Zhanga, Q. Liub, and X. Zhoub, “Audio-driven talking head
    video generation with diffusion model,” in *Proc. IEEE-ICASSP*.   IEEE, 2023,
    pp. 1–5.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] Y. Zhu, C. Zhang, Q. Liu, 和 X. Zhou，“基于扩散模型的音频驱动人脸视频生成，”见于 *Proc. IEEE-ICASSP*。
    IEEE，2023年，页码1–5。'
- en: '[292] C. Xu, S. Zhu, J. Zhu, T. Huang, J. Zhang, Y. Tai, and Y. Liu, “Multimodal-driven
    talking face generation via a unified diffusion-based generator.” *CoRR*, 2023.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] C. Xu, S. Zhu, J. Zhu, T. Huang, J. Zhang, Y. Tai, 和 Y. Liu，“通过统一扩散生成器进行多模态驱动的说话面孔生成。”
    *CoRR*，2023年。'
- en: '[293] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the
    devil in the details: delving deep into convolutional nets,” in *Proc. BMVC*.   British
    Machine Vision Association, 2014.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] K. Chatfield, K. Simonyan, A. Vedaldi, 和 A. Zisserman，“细节中的魔鬼再现：深入探讨卷积网络，”见于
    *Proc. BMVC*。 英国机器视觉协会，2014年。'
- en: '[294] R. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Dosovitskiy,
    and D. Duckworth, “Nerf in the wild: Neural radiance fields for unconstrained
    photo collections,” in *Proc. IEEE/CVF-CVPR*, 2021, pp. 7210–7219.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] R. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Dosovitskiy,
    和 D. Duckworth，“Nerf 在野外：用于无约束照片集的神经辐射场，”见于 *Proc. IEEE/CVF-CVPR*，2021年，页码7210–7219。'
- en: '[295] R. Huang, Z. Zhao, H. Liu, J. Liu, C. Cui, and Y. Ren, “Prodiff: Progressive
    fast diffusion model for high-quality text-to-speech,” in *Proc. ACM MM*, 2022,
    pp. 2595–2605.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] R. Huang, Z. Zhao, H. Liu, J. Liu, C. Cui, 和 Y. Ren，“Prodiff：高质量文本到语音的渐进快速扩散模型，”见于
    *Proc. ACM MM*，2022年，页码2595–2605。'
- en: '[296] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans, D. Fleet,
    and M. Norouzi, “Palette: Image-to-image diffusion models,” in *Proc. ACM SIGGRAPH*,
    2022, pp. 1–10.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans, D. Fleet,
    和 M. Norouzi，“Palette: 图像到图像扩散模型，”发表于*Proc. ACM SIGGRAPH*，2022，第1–10页。'
- en: '[297] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE transactions
    on image processing*, vol. 13, no. 4, pp. 600–612, 2004.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] Z. Wang, A. C. Bovik, H. R. Sheikh, 和 E. P. Simoncelli，“图像质量评估：从错误可见性到结构相似性，”*IEEE
    transactions on image processing*，第13卷，第4期，第600–612页，2004年。'
- en: '[298] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *Proc. IEEE-CVPR*,
    2018, pp. 586–595.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, 和 O. Wang，“深度特征作为感知度量的不可思议的有效性，”发表于*Proc.
    IEEE-CVPR*，2018，第586–595页。'
- en: '[299] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz, “Mocogan: Decomposing
    motion and content for video generation,” in *Proc. IEEE-CVPR*, 2018, pp. 1526–1535.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] S. Tulyakov, M.-Y. Liu, X. Yang, 和 J. Kautz，“Mocogan：运动和内容的分解用于视频生成，”发表于*Proc.
    IEEE-CVPR*，2018，第1526–1535页。'
- en: '[300] J. Chung and A. Zisserman, “Out of time: automated lip sync in the wild,”
    in *Proc. ACCV*, 2017.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] J. Chung 和 A. Zisserman，“超越时间：野外自动化唇同步，”发表于*Proc. ACCV*，2017年。'
- en: '[301] N. D. Narvekar and L. J. Karam, “A no-reference perceptual image sharpness
    metric based on a cumulative probability of blur detection,” in *Proc. IEEE-QoMEX*,
    2009, pp. 87–91.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] N. D. Narvekar 和 L. J. Karam，“一种基于模糊检测累积概率的无参考感知图像清晰度度量，”发表于*Proc. IEEE-QoMEX*，2009，第87–91页。'
- en: '[302] K. De and V. Masilamani, “Image sharpness measure for blurred images
    in frequency domain,” *Procedia Engineering*, vol. 64, pp. 149–158, 2013.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] K. De 和 V. Masilamani，“频域模糊图像的图像清晰度测量，”*Procedia Engineering*，第64卷，第149–158页，2013年。'
- en: '[303] D. Zeng, S. Zhao, J. Zhang, H. Liu, and K. Li, “Expression-tailored talking
    face generation with adaptive cross-modal weighting,” *Neurocomputing*, vol. 511,
    pp. 117–130, 2022.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] D. Zeng, S. Zhao, J. Zhang, H. Liu, 和 K. Li，“表达定制的谈话面孔生成与自适应跨模态加权，”*Neurocomputing*，第511卷，第117–130页，2022年。'
- en: '[304] K. Vougioukas, S. Petridis, and M. Pantic, “Realistic speech-driven facial
    animation with GANs,” *International Journal of Computer Vision*, vol. 128, pp.
    1398–1413, 2020.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] K. Vougioukas, S. Petridis, 和 M. Pantic，“基于GAN的逼真语音驱动面部动画，”*International
    Journal of Computer Vision*，第128卷，第1398–1413页，2020年。'
- en: '[305] J. Wang, J. Liu, L. Zhao, S. Wang, R. Yu, and L. Liu, “Acoustic-to-articulatory
    inversion based on speech decomposition and auxiliary feature,” in *Proc. IEEE-ICASSP*,
    2022, p. 4808–4812.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] J. Wang, J. Liu, L. Zhao, S. Wang, R. Yu, 和 L. Liu，“基于语音分解和辅助特征的声学到发音的反演，”发表于*Proc.
    IEEE-ICASSP*，2022，第4808–4812页。'
- en: '[306] M. Mori, K. MacDorman, and N. Kageki, “The uncanny valley [from the field],”
    *IEEE Robotics and Automation Magazine*, vol. 19, pp. 98–100, 06 2012.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] M. Mori, K. MacDorman, 和 N. Kageki，“恐怖谷[来自现场]，”*IEEE Robotics and Automation
    Magazine*，第19卷，第98–100页，2012年6月。'
- en: '[307] C. Sheng, M. Pietikäinen, Q. Tian, and L. Liu, “Cross-modal self-supervised
    learning for lip reading: When contrastive learning meets adversarial training,”
    in *Proc. ACM MM*, 2021, pp. 2456–2464.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] C. Sheng, M. Pietikäinen, Q. Tian, 和 L. Liu，“面部读唇的跨模态自监督学习：当对比学习遇上对抗训练，”发表于*Proc.
    ACM MM*，2021，第2456–2464页。'
- en: '[308] Y. Mroueh, E. Marcheret, and V. Goel, “Deep multimodal learning for audio-visual
    speech recognition,” in *Proc. IEEE-ICASSP*, 2015, pp. 2130–2134.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] Y. Mroueh, E. Marcheret, 和 V. Goel，“用于音视频语音识别的深度多模态学习，”发表于*Proc. IEEE-ICASSP*，2015，第2130–2134页。'
- en: '[309] B. Shillingford, Y. Assael, M. W. Hoffman, T. Paine, C. Hughes, U. Prabhu,
    H. Liao, H. Sak, K. Rao, L. Bennett *et al.*, “Large-scale visual speech recognition,”
    *Proc. Interspeech*, pp. 4135–4139, 2019.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] B. Shillingford, Y. Assael, M. W. Hoffman, T. Paine, C. Hughes, U. Prabhu,
    H. Liao, H. Sak, K. Rao, L. Bennett *等*，“大规模视觉语音识别，”*Proc. Interspeech*，第4135–4139页，2019年。'
- en: '[310] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *Proc. ICML*, 2021, pp. 8748–8763.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G.
    Sastry, A. Askell, P. Mishkin, J. Clark *等*，“从自然语言监督中学习可转移的视觉模型，”发表于*Proc. ICML*，2021，第8748–8763页。'
- en: '[311] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,
    and I. Sutskever, “Zero-shot text-to-image generation,” in *Proc. ICML*, 2021,
    pp. 8821–8831.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,
    和 I. Sutskever，“零样本文本到图像生成，”发表于*Proc. ICML*，2021，第8821–8831页。'
- en: '[312] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
    “GANs trained by a two time-scale update rule converge to a local nash equilibrium,”
    *Proc. Advances in neural information processing systems (NIPS)*, vol. 30, 2017.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, 和 S. Hochreiter,
    “由两种时间尺度更新规则训练的 GAN 收敛于局部纳什均衡，” *《神经信息处理系统进展（NIPS）》*, 第30卷, 2017年。'
- en: '[313] E. Mu, K. M. Lewis, A. V. Dalca, and J. Guttag, “Generating image-specific
    text improves fine-grained image classification,” *arXiv preprint arXiv:2307.11315*,
    2023.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] E. Mu, K. M. Lewis, A. V. Dalca, 和 J. Guttag, “生成特定图像的文本改善细粒度图像分类，” *arXiv
    预印本 arXiv:2307.11315*, 2023年。'
