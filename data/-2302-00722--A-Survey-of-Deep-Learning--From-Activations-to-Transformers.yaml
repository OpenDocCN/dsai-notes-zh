- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:42:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:42:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2302.00722] A Survey of Deep Learning: From Activations to Transformers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2302.00722] 深度学习综述：从激活到Transformer'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.00722](https://ar5iv.labs.arxiv.org/html/2302.00722)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2302.00722](https://ar5iv.labs.arxiv.org/html/2302.00722)
- en: 'A Survey of Deep Learning: From Activations to Transformers'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习综述：从激活到Transformer
- en: Johannes Schneider¹ and Michalis Vlachos²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Johannes Schneider¹ 和 Michalis Vlachos²
- en: ¹ University of Liechtenstein, Vaduz, Liechtenstein
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 列支敦士登大学，瓦杜兹，列支敦士登
- en: ² University of Lausanne, Lausanne, Switzerland
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 瑞士洛桑大学，洛桑，瑞士
- en: johannes.schneider@uni.li, michalis.vlachos@unil.ch
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 邮箱：johannes.schneider@uni.li, michalis.vlachos@unil.ch
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning has made tremendous progress in the last decade. A key success
    factor is the large amount of architectures, layers, objectives, and optimization
    techniques. They include a myriad of variants related to attention, normalization,
    skip connections, transformers and self-supervised learning schemes – to name
    a few. We provide a comprehensive overview of the most important, recent works
    in these areas to those who already have a basic understanding of deep learning.
    We hope that a holistic and unified treatment of influential, recent works helps
    researchers to form new connections between diverse areas of deep learning. We
    identify and discuss multiple patterns that summarize the key strategies for many
    of the successful innovations over the last decade as well as works that can be
    seen as rising stars. We also include a discussion on recent commercially built,
    closed-source models such as OpenAI’s GPT-4 and Google’s PaLM 2.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，深度学习取得了巨大的进展。其中一个关键的成功因素是大量的架构、层、目标和优化技术。它们包括与注意力、规范化、跳连接、Transformer
    和自监督学习方案相关的众多变体，仅举几例。我们对这些领域内最重要的最新研究工作提供了全面的概述，面向那些对深度学习有基本了解的人。我们希望对有影响力的最新工作进行全面和统一的处理，帮助研究人员在深度学习的不同领域之间建立新的联系。我们确定并讨论了多个总结过去十年中许多成功创新的关键策略的模式，以及可能被视为新星的作品。我们还包括了对最近基于商业构建的闭源模型的讨论，如OpenAI的GPT-4和Google的PaLM
    2。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Deep learning is widely regarded as the driving force behind artificial intelligence.
    Its models have achieved top leaderboard rankings in various fields, including
    computer vision, speech, and natural language processing. One of the major advantages
    of deep learning is its layered, modular structure, which allows for the construction
    of models from individual components in a flexible manner. Researchers have created
    a large selection of layers, architectures, and objectives. Keeping up with the
    ongoing developments in the various aspects of deep learning is a difficult task.
    Although specific surveys are available, there is currently no comprehensive overview
    of recent progress covering multiple aspects of deep learning such as learning,
    layers and architecture. There exist multiple reviews with a narrow focus such
    as large language models ( e.g. [[Min et al., 2021](#bib.bibx50)]) and convolutional
    neural networks (e.g. [[Khan et al., 2020](#bib.bibx35)]). Previous studies [[Alom
    et al., 2019](#bib.bibx1), [Shrestha and Mahmood, 2019](#bib.bibx68), [Dong et al.,
    2021](#bib.bibx13), [Alzubaidi et al., 2021](#bib.bibx2)] with a wider focus have
    often overlooked new developments such as transformers and supervised-learning.
    However, taking a more comprehensive and more holistic look at various disciplines
    can be extremely advantageous: For example, NLP and computer vision have often
    influenced each other; CNNs were initially introduced in computer vision, but
    were later applied in NLP, while transformers were introduced in NLP and later
    adapted in computer vision. Therefore, removing barriers between disciplines can
    be highly beneficial. This paper takes this motivation by surveying the recent
    progress of deep learning from a holistic standpoint, rather than focusing on
    a particular niche area. We also believe that this is a necessary step, since
    major innovations have slowed down in terms, i.e., now most architectures are
    based on the transformer architecture, which dates back to 2017[[Vaswani et al.,
    2017](#bib.bibx75)].'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习被广泛认为是人工智能的推动力。其模型在计算机视觉、语音和自然语言处理等多个领域中都取得了顶级排行榜的成绩。深度学习的主要优势之一是其分层的、模块化的结构，这使得可以以灵活的方式从单个组件构建模型。研究人员创建了大量的层、架构和目标。跟上深度学习各方面的持续发展是一项困难的任务。尽管有特定的调查可用，但目前没有涵盖深度学习多个方面（如学习、层和架构）的综合概述。存在多个狭义关注的综述，例如大语言模型（例如
    [[Min et al., 2021](#bib.bibx50)]）和卷积神经网络（例如 [[Khan et al., 2020](#bib.bibx35)]）。以前的研究
    [[Alom et al., 2019](#bib.bibx1), [Shrestha and Mahmood, 2019](#bib.bibx68), [Dong
    et al., 2021](#bib.bibx13), [Alzubaidi et al., 2021](#bib.bibx2)] 广泛关注的往往忽视了变换器和监督学习等新发展。然而，从更全面、更整体的角度来看各种学科可能是极具优势的：例如，自然语言处理和计算机视觉常常相互影响；卷积神经网络最初是在计算机视觉中引入的，但后来被应用于自然语言处理，而变换器则是在自然语言处理中的引入，后来被适用于计算机视觉。因此，打破学科之间的障碍可能会非常有益。本文以这种动机，通过整体视角调查了深度学习的最新进展，而不是专注于特定的小领域。我们还相信这是一个必要的步骤，因为重大创新在某些方面已经放缓，即现在大多数架构都基于自2017年起源的变换器架构[[Vaswani
    et al., 2017](#bib.bibx75)]。
- en: It is difficult, if not impossible, to provide an encompassing overview of the
    field due to the sheer number of articles published yearly and the continual increase
    in relevant topics, such as transformers and self-supervised learning that have
    become popular only recently. Our strategy is to choose influential works through
    (i) usage statistics and (ii) specialized surveys. We also offer an invigorating
    discussion of shared design patterns across areas that have been successful.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每年发表的大量文章和相关话题的不断增加，例如最近才流行的变换器和自监督学习，提供全面的领域概述是困难的，甚至是不可能的。我们的策略是通过（i）使用统计数据和（ii）专业调查来选择有影响力的工作。我们还提供了对成功领域之间共享设计模式的振奋人心的讨论。
- en: '![Refer to caption](img/625214a3fb114f03c400b10bd1d521b1.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/625214a3fb114f03c400b10bd1d521b1.png)'
- en: 'Figure 1: Categorization of deep learning and areas covered in the survey'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：深度学习的分类及调查中涉及的领域
- en: 2 Overview
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 概述
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning: From
    Activations to Transformers") provides an overview of the areas included in this
    survey. We have investigated deep learning design, including objectives and training.
    We have also given special attention to works that have been somewhat established
    based on the usage statistics from the popular platform ”Paperswithcode.com.”
    There has been an increase in these types of platforms that enable the upload
    of papers (and models) and provide information on citations, as well as leaderboards.
    Although there are drawbacks when utilizing data from these platforms, we believe
    that it offers a new perspective compared to traditional survey methods that often
    select more arbitrarily. We have only included a selection of the most influential
    works published from 2016 onwards, as well as rising stars (from 2020 or newer)
    that have gained significant popularity in a short time.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning: From Activations
    to Transformers") 提供了本调查所涵盖领域的概述。我们调查了深度学习设计，包括目标和训练。我们还特别关注了基于流行平台“Paperswithcode.com”的使用统计数据已有一定基础的工作。虽然利用这些平台的数据存在缺陷，但我们相信它提供了与传统调查方法相比的新视角，这些传统方法往往选择更具随意性。我们仅包括了从2016年起发布的最具影响力的工作以及在短时间内获得显著流行的“新星”（从2020年或更新）的论文。'
- en: The extent to which each topic is covered depends on the amount of recent research
    that has been conducted and its foundational nature. We do not discuss data or
    computational aspects such as data augmentation, model compression, and distributed
    machine learning. As a result of limited space, we had to be selective when it
    came to model families and left out relevant ones such as multi-modal models and
    autoencoders.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主题的覆盖程度取决于最近进行的研究数量及其基础性质。我们不讨论数据或计算方面的内容，如数据增强、模型压缩和分布式机器学习。由于空间有限，我们在模型家族的选择上必须有所取舍，遗漏了一些相关的模型，如多模态模型和自编码器。
- en: 3 Loss functions and Optimization
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 损失函数与优化
- en: We discuss common loss functions and optimizers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了常见的损失函数和优化器。
- en: 3.1 Loss Functions
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 损失函数
- en: Loss functions (surveyed in [[Wang et al., 2020](#bib.bibx78)]) often consist
    of multiple terms that are enhanced with a regularization term. Loss functions
    are often task-specific but some general ideas are applicable across tasks. Commonly,
    multiple loss terms are aggregated in a weighted manner. Many papers improve prior
    work (simply) by using a different loss function.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数（见 [[Wang et al., 2020](#bib.bibx78)]）通常包含多个加上正则化项的项。损失函数往往是任务特定的，但一些通用思想在各任务中都适用。通常，多个损失项以加权方式进行汇总。许多论文通过使用不同的损失函数（简单地）改进了之前的工作。
- en: 'The Triplet Loss [[Dong and Shen, 2018](#bib.bibx14)] was introduced for Siamese
    networks (Its origin dates back further [[Schultz and Joachims, 2003](#bib.bibx65)].)
    The high level idea is to compare a given input to a positive and a negative input
    and maximize association between positively associated inputs, while minimizing
    those of negative ones. It takes input pairs $(x,y)$, each processed by a separate
    but identical network. It maximizes the joint probability $p(x,y)$ of all pairs
    $(x,y)$:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Triplet Loss [[Dong and Shen, 2018](#bib.bibx14)] 被引入用于 Siamese 网络（其起源更早 [[Schultz
    and Joachims, 2003](#bib.bibx65)]）。其核心思想是将给定的输入与正输入和负输入进行比较，最大化正相关输入之间的关联，同时最小化负相关输入之间的关联。它处理输入对
    $(x,y)$，每对由一个独立但相同的网络处理。它最大化所有对 $(x,y)$ 的联合概率 $p(x,y)$：
- en: '|  | $\displaystyle L(\mathcal{V}_{p},\mathcal{V}_{n})$ | $\displaystyle=-\frac{1}{&#124;\mathcal{V}_{p}&#124;\cdot&#124;\mathcal{V}_{n}&#124;}\sum_{x\in\mathcal{V}_{p}}\sum_{y\in\mathcal{V}_{n}}\log
    p(x,y)$ |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(\mathcal{V}_{p},\mathcal{V}_{n})$ | $\displaystyle=-\frac{1}{|\mathcal{V}_{p}|\cdot|\mathcal{V}_{n}|}\sum_{x\in\mathcal{V}_{p}}\sum_{y\in\mathcal{V}_{n}}\log
    p(x,y)$ |  | (1) |'
- en: '|  |  | $\displaystyle=-\frac{1}{&#124;\mathcal{V}_{p}&#124;\cdot&#124;\mathcal{V}_{n}&#124;}\sum_{x\in\mathcal{V}_{p}}\sum_{y\in\mathcal{V}_{n}}\log(1+e^{x-y})$
    |  | (2) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\frac{1}{|\mathcal{V}_{p}|\cdot|\mathcal{V}_{n}|}\sum_{x\in\mathcal{V}_{p}}\sum_{y\in\mathcal{V}_{n}}\log(1+e^{x-y})$
    |  | (2) |'
- en: Here, $\mathcal{V}_{p}$ and $\mathcal{V}_{n}$ are the positive and negative
    score set respectively.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\mathcal{V}_{p}$ 和 $\mathcal{V}_{n}$ 分别是正评分集和负评分集。
- en: Focal Loss[[Lin et al., 2017](#bib.bibx42)] focuses learning on hard misclassified
    samples by altering the cross entropy loss. It adds a factor $(1-p)^{\gamma}$,
    where $p$ denotes the probability of a sample stemming from the cross entropy
    loss and $\gamma$ is a tunable parameter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Focal Loss[[Lin et al., 2017](#bib.bibx42)] 通过改变交叉熵损失来将学习集中在难以分类的样本上。它添加了一个因子
    $(1-p)^{\gamma}$，其中 $p$ 表示样本的交叉熵损失概率，$\gamma$ 是一个可调参数。
- en: '|  | $\displaystyle L(p)=(1-p)^{\gamma}\log(p)$ |  | (3) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(p)=(1-p)^{\gamma}\log(p)$ |  | (3) |'
- en: The Cycle Consistency Loss[[Zhu et al., 2017](#bib.bibx87)] is tailored towards
    unpaired image-to-image translation of generative adversarial networks. For two
    image domains $X$ and $Y$, the loss supports the learning of mappings $G:X\rightarrow
    Y$ and $F:Y\rightarrow X$ so that one reverses the other, i.e., $F(G(x))\approx
    x$ and $G(F(y))\approx y$.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 循环一致性损失[[Zhu et al., 2017](#bib.bibx87)] 针对生成对抗网络中的无配对图像到图像翻译而设计。对于两个图像域 $X$
    和 $Y$，该损失支持映射 $G:X\rightarrow Y$ 和 $F:Y\rightarrow X$ 的学习，使得一个映射可以反转另一个，即 $F(G(x))\approx
    x$ 和 $G(F(y))\approx y$。
- en: '|  | $\displaystyle L(G,F)$ | $\displaystyle=\mathbb{E}_{x\sim p_{data}(x)}[&#124;&#124;F(G(x))-x&#124;&#124;_{1}]$
    |  | (4) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(G,F)$ | $\displaystyle=\mathbb{E}_{x\sim p_{data}(x)}[&#124;&#124;F(G(x))-x&#124;&#124;_{1}]$
    |  | (4) |'
- en: '|  |  | $\displaystyle+\mathbb{E}_{y\sim p_{data}(y)}[&#124;&#124;G(F(y))-y&#124;&#124;_{1}]$
    |  | (5) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\mathbb{E}_{y\sim p_{data}(y)}[&#124;&#124;G(F(y))-y&#124;&#124;_{1}]$
    |  | (5) |'
- en: The Supervised Contrastive Loss[[Khosla et al., 2020](#bib.bibx37)] pulls together
    clusters of points of the same class in embedding space and pushes samples of
    different classes apart. It aims at leveraging label information more effectively
    than cross-entropy loss.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 监督对比损失[[Khosla et al., 2020](#bib.bibx37)] 将同一类别的点集群拉近，并将不同类别的样本分开。它旨在比交叉熵损失更有效地利用标签信息。
- en: '|  | $\displaystyle\mathcal{L}_{i}^{sup}$ | $\displaystyle=\frac{-1}{2N_{\boldsymbol{\tilde{y}}_{i}}-1}\cdot$
    |  | (6) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{i}^{sup}$ | $\displaystyle=\frac{-1}{2N_{\boldsymbol{\tilde{y}}_{i}}-1}\cdot$
    |  | (6) |'
- en: '|  |  | $\displaystyle\sum_{j=1}^{2N}\mathbf{1}_{i\neq j}\cdot\mathbf{1}_{\boldsymbol{\tilde{y}}_{i}=\boldsymbol{\tilde{y}}_{j}}\cdot\log{\frac{\exp{(\boldsymbol{z}_{i}\cdot\boldsymbol{z}_{j}/\tau)}}{\sum_{k=1}^{2N}\mathbf{1}_{i\neq
    k}\cdot\exp{(\boldsymbol{z}_{i}\cdot\boldsymbol{z}_{k}/\tau)}}}$ |  | (7) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\sum_{j=1}^{2N}\mathbf{1}_{i\neq j}\cdot\mathbf{1}_{\boldsymbol{\tilde{y}}_{i}=\boldsymbol{\tilde{y}}_{j}}\cdot\log{\frac{\exp{(\boldsymbol{z}_{i}\cdot\boldsymbol{z}_{j}/\tau)}}{\sum_{k=1}^{2N}\mathbf{1}_{i\neq
    k}\cdot\exp{(\boldsymbol{z}_{i}\cdot\boldsymbol{z}_{k}/\tau)}}}$ |  | (7) |'
- en: 'where $N_{\boldsymbol{\tilde{y}}_{i}}$ is the total number of images in the
    minibatch that have the same label $\boldsymbol{\tilde{y}}_{i}$ as the anchor
    $i$. The total loss is the sum over the loss of all anchors $i$, i.e., $\mathcal{L}=\sum_{i}\mathcal{L}_{i}^{sup}$.
    The loss has important properties well suited for supervised learning:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{\boldsymbol{\tilde{y}}_{i}}$ 是在小批量中具有与锚点 $i$ 相同标签 $\boldsymbol{\tilde{y}}_{i}$
    的图像总数。总损失是所有锚点 $i$ 的损失之和，即 $\mathcal{L}=\sum_{i}\mathcal{L}_{i}^{sup}$。该损失具有适合监督学习的重要属性：
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: generalization to an arbitrary number of positives
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任意数量正样本的泛化
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: contrastive power increases with more negatives.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对比能力随着负样本的增加而增加。
- en: 3.2 Regularization
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 正则化
- en: Regularization techniques in machine learning (surveyed in [[Moradi et al.,
    2020](#bib.bibx53)]) have proven very helpful for deep learning. Explicit regularization
    adds a loss term $R(f)$ for a network $f$ to the loss function $L(x)$ for data
    $(x_{i},y_{i})$ with a trade-off parameter $\lambda$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的正则化技术（在[[Moradi et al., 2020](#bib.bibx53)]中进行了调查）已被证明对深度学习非常有帮助。显式正则化在数据
    $(x_{i},y_{i})$ 的损失函数 $L(x)$ 中为网络 $f$ 添加了一个损失项 $R(f)$，并具有权衡参数 $\lambda$。
- en: '|  | $\displaystyle\min_{f}\sum_{i}L(x_{i},y_{i})+\lambda R(f)$ |  | (8) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{f}\sum_{i}L(x_{i},y_{i})+\lambda R(f)$ |  | (8) |'
- en: Implicit regularization is all other regularization, e.g., early stopping or
    using a robust loss function. Classical $L2$-regularization and dropout[[Srivastava
    et al., 2014](#bib.bibx70)], where activations of a random set of neurons are
    set to 0, are among the most wildly used regularization.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式正则化是所有其他正则化方法，例如早停或使用稳健的损失函数。经典的 $L2$ 正则化和 dropout[[Srivastava et al., 2014](#bib.bibx70)]，其中随机一组神经元的激活被置为0，是最广泛使用的正则化方法之一。
- en: '$R_{1}$ Regularization [[Mescheder et al., 2018](#bib.bibx49)] is used to penalize
    the discriminator in generative adversarial networks based on the gradient with
    the goal of stabilizing training:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $R_{1}$ 正则化[[Mescheder et al., 2018](#bib.bibx49)] 用于根据梯度对生成对抗网络中的判别器进行惩罚，以稳定训练：
- en: '|  | $\displaystyle R_{1}(\psi)=\frac{\gamma}{2}E_{p_{D}(x)}[&#124;&#124;\nabla{D_{\psi}(x)}&#124;&#124;^{2}]$
    |  | (9) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R_{1}(\psi)=\frac{\gamma}{2}E_{p_{D}(x)}[&#124;&#124;\nabla{D_{\psi}(x)}&#124;&#124;^{2}]$
    |  | (9) |'
- en: Technically, the regularization term penalizes gradients orthogonal to the data
    manifold.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，正则化项惩罚与数据流形正交的梯度。
- en: 'Entropy Regularization [[Mnih et al., 2016](#bib.bibx52)] aims at fostering
    diversity. Specifically, asynchronous methods for deep reinforcement learning
    [[Williams and Peng, 1991](#bib.bibx79), [Mnih et al., 2016](#bib.bibx52)]. [[Mnih
    et al., 2016](#bib.bibx52)] ensures diversity of actions in reinforcment learning,
    i.e., it prevents overoptimizion towards a small fraction of the environment.
    The entropy is simply computed over the probability distribution of actions given
    by the policy $\pi(x)$ as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 熵正则化[[Mnih et al., 2016](#bib.bibx52)]旨在促进多样性。具体而言，深度强化学习的异步方法[[Williams and
    Peng, 1991](#bib.bibx79), [Mnih et al., 2016](#bib.bibx52)]。[[Mnih et al., 2016](#bib.bibx52)]确保强化学习中的行动多样性，即防止对环境的某小部分进行过度优化。熵简单地计算策略$\pi(x)$给定的动作概率分布上的熵：
- en: '|  | $\displaystyle H(x)=\sum_{x}\pi(x)\cdot\log(\pi(x))$ |  | (10) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H(x)=\sum_{x}\pi(x)\cdot\log(\pi(x))$ |  | (10) |'
- en: 'Path Length Regularization [[Karras et al., 2020a](#bib.bibx33)] for generative
    adversarial networks aims at ensuring that the fixed-size step length in the latent
    space matches the fixed-magnitude change in the image. The idea is to encourage
    that a fixed-size step in the latent space $\mathcal{W}$ results in a non-zero,
    fixed-magnitude change in the image. The goal is to ensure better conditioning
    of GANs, simplifying architecture search and generator inversion. Gradients with
    respect to $\mathbf{w}\in\mathcal{W}$ stemming from random directions in the image
    space should be almost equal in length independent of $\mathbf{w}$ or the image
    space direction. The local metric scaling characteristics of the generator $g:\mathcal{W}\rightarrow\mathcal{Y}$
    are captured by the Jacobian matrix $\mathbf{J_{w}}=\delta{g}(\mathbf{w})/\delta{\mathbf{w}}$.
    The regularizer becomes:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 路径长度正则化[[Karras et al., 2020a](#bib.bibx33)]用于生成对抗网络，旨在确保潜在空间中的固定大小步长与图像中的固定幅度变化匹配。其思想是鼓励潜在空间$\mathcal{W}$中的固定大小步长在图像中产生非零、固定幅度的变化。目标是确保GANs的更好条件，简化架构搜索和生成器反演。来自图像空间中随机方向的关于$\mathbf{w}\in\mathcal{W}$的梯度应在长度上几乎相等，无论$\mathbf{w}$或图像空间方向如何。生成器$g:\mathcal{W}\rightarrow\mathcal{Y}$的局部度量缩放特性由雅可比矩阵$\mathbf{J_{w}}=\delta{g}(\mathbf{w})/\delta{\mathbf{w}}$捕捉。正则化项变为：
- en: '|  | $\displaystyle\mathbb{E}_{\mathbf{w},\mathbf{y}\sim\mathcal{N}(0,\mathbf{I})}(&#124;&#124;\mathbf{J}^{\mathbf{T}}_{\mathbf{w}}\mathbf{y}&#124;&#124;_{2}-a)^{2}$
    |  | (11) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\mathbf{w},\mathbf{y}\sim\mathcal{N}(0,\mathbf{I})}(&#124;&#124;\mathbf{J}^{\mathbf{T}}_{\mathbf{w}}\mathbf{y}&#124;&#124;_{2}-a)^{2}$
    |  | (11) |'
- en: where $y$ are random images with normally distributed pixel values, and $w\sim
    f(z)$, where $z$ is normally distributed. The constant $a$ is the exponential
    moving average of $||\mathbf{J}^{\mathbf{T}}_{\mathbf{w}}\mathbf{y}||_{2}$. The
    paper further avoids the computationally expensive, explicit computation of the
    Jacobian.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$y$是像素值服从正态分布的随机图像，$w\sim f(z)$，其中$z$也服从正态分布。常数$a$是$||\mathbf{J}^{\mathbf{T}}_{\mathbf{w}}\mathbf{y}||_{2}$的指数移动平均。论文进一步避免了计算开销大的雅可比矩阵显式计算。
- en: DropBlock[[Ghiasi et al., 2018](#bib.bibx19)] drops correlated areas of features
    maps rather than selecting features to drop independently. This is especially
    suitable for convolutional neural networks where features maps exhibit spatial
    correlation and a (real-world) feature often corresponds to a contiguous spatial
    area in feature maps.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: DropBlock[[Ghiasi et al., 2018](#bib.bibx19)]会丢弃特征图中的相关区域，而不是独立选择要丢弃的特征。这对于特征图展示空间相关性的卷积神经网络尤其适用，其中一个（现实世界的）特征通常对应于特征图中的连续空间区域。
- en: 3.3 Optimization
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 优化
- en: Optimization(surveyed in [[Sun, 2020](#bib.bibx71)]) is the process of estimating
    all network parameters so that the loss function is minimized. The two most wildly
    known technique is stochastic gradient descent(SGD) and Adam. None strictly outperforms
    in all cases in terms of generalization performance. SGD dates back at least to
    the 50ies[[Kiefer and Wolfowitz, 1952](#bib.bibx38)], while Adam stems from 2014[[Kingma
    and Ba, 2014](#bib.bibx39)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 优化（详见[[Sun, 2020](#bib.bibx71)]）是估计所有网络参数以最小化损失函数的过程。最广为人知的两种技术是随机梯度下降（SGD）和Adam。没有一种方法在所有情况下的泛化性能都能严格超越其他方法。SGD
    至少可以追溯到50年代[[Kiefer and Wolfowitz, 1952](#bib.bibx38)]，而Adam源自2014年[[Kingma and
    Ba, 2014](#bib.bibx39)]。
- en: Adafactor [[Shazeer and Stern, 2018](#bib.bibx67)] reduces the memory needs
    of the Adam optimization by maintaining only row- and column-wise statistics of
    parameter matrixes rather than per-element information.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Adafactor [[Shazeer 和 Stern, 2018](#bib.bibx67)] 通过仅保持参数矩阵的行和列统计信息，而不是每个元素的信息，来减少
    Adam 优化的内存需求。
- en: Layerwise adaptive large batch optimization (LAMB)[[You et al., 2019](#bib.bibx85)]
    builds on Adam and accelerates training using large mini-batches. It performs
    per-dimension and layerwise normalization.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 分层自适应大批量优化 (LAMB)[[You 等, 2019](#bib.bibx85)] 基于 Adam 构建，通过使用大批量加速训练。它执行每维和每层的归一化。
- en: 'Two Time-scale Update Rule(TTUR): For generative adversarial networks trained
    with stochastic gradient descent TTUR[[Heusel et al., 2017](#bib.bibx29)] uses
    a separate learning rate for the discriminator and generator. For a fixed generator,
    the discriminator reaches a local minimum. This still holds if the generator converges
    slowly, e.g., using a small(er) learning rate. This helps in convergence of the
    GAN and it can improve performance since the generator captures the feedback of
    the discriminator more profoundly before pushing it into new regions.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 双时间尺度更新规则 (TTUR)：对于使用随机梯度下降训练的生成对抗网络，TTUR[[Heusel 等, 2017](#bib.bibx29)] 为判别器和生成器使用不同的学习率。对于固定的生成器，判别器达到局部最小值。如果生成器收敛较慢，例如使用较小的学习率，这一点仍然成立。这有助于
    GAN 的收敛，并且可以提高性能，因为生成器在将其推向新区域之前更深刻地捕捉到判别器的反馈。
- en: 'Decoupled Weight Decay Regularization for ADAM: AdamW[[Loshchilov and Hutter,
    2017](#bib.bibx48)] is built on a simple observation and implemenation. The orginal
    Adam optimization changes weights due to (L2-)regularization after computation
    of gradients for Adam. But intuitively moving averages of gradients should not
    include regularization.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: AdamW[[Loshchilov 和 Hutter, 2017](#bib.bibx48)] 是基于一个简单的观察和实现。原始的 Adam 优化在计算梯度后会由于
    (L2-)正则化而改变权重。但直观地说，梯度的移动平均不应该包括正则化。
- en: 'RAdam and AMSGrad: Both techniques tackle the convergence problem of Adam.
    Rectified Adam[[Liu et al., 2019a](#bib.bibx43)] rectifies the variance of the
    adaptive learning rate, which is large initially. Thus, similar to the warm-up
    heuristic small initial learning rates can help. AMSGrad [[Reddi et al., 2019](#bib.bibx62)]
    uses the maximum of past squared gradients rather than the exponential average.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: RAdam 和 AMSGrad：这两种技术都解决了 Adam 的收敛问题。Rectified Adam[[Liu 等, 2019a](#bib.bibx43)]
    校正了自适应学习率的方差，该方差最初较大。因此，类似于预热启发式，小的初始学习率可能会有帮助。AMSGrad [[Reddi 等, 2019](#bib.bibx62)]
    使用过去平方梯度的最大值，而不是指数平均值。
- en: 'Stochastic Weight Averaging: Simple averaging of weights from different epochs
    during stochastic gradient descent with constant or cycling learning rate improves
    performance.[[Izmailov et al., 2018](#bib.bibx32)]'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 随机权重平均：在随机梯度下降过程中对不同周期的权重进行简单平均，使用常量或循环学习率可以提高性能。[ [Izmailov 等, 2018](#bib.bibx32)
    ]
- en: 'Sharpness-Aware Minimization[[Foret et al., 2020](#bib.bibx18)] minimizes loss
    value and sharpness, which improves generalization. It finds parameters with neighborhoods
    of low loss value (rather than parameters that only themselves have low loss value).
    The loss is:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 鲜明度感知最小化[[Foret 等, 2020](#bib.bibx18)] 通过最小化损失值和鲜明度来改善泛化。它找到具有低损失值邻域的参数（而不仅仅是具有低损失值的参数）。损失函数为：
- en: '|  | $\displaystyle\min_{w}\max_{&#124;&#124;\epsilon&#124;&#124;_{p}\leq\rho}L(w+\epsilon)$
    |  | (12) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{w}\max_{&#124;&#124;\epsilon&#124;&#124;_{p}\leq\rho}L(w+\epsilon)$
    |  | (12) |'
- en: 4 Self, Semi-supervised and Contrastive learning
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 自我、半监督和对比学习
- en: Semi-supervised learning leverages a large amount of unlabelled data based on
    a small amount of labeled data (see [[Yang et al., 2022](#bib.bibx83)] for a survey).
    Self-supervised learning benefits from self-generated (pseudo)labels stemming
    from artificial tasks. Both reduce the burden of collecting (human) labeled data.
    Self-supervised (pre-)training combined with fine-tuning on a (small) human-annotated
    dataset can lead to state-of-the-art results. The paradigm has grown extensively
    in recent years (surveyed in [[Ericsson et al., 2022](#bib.bibx17)]). It is commonly
    combined with contrastive learning. In contrastive learning, the goal is to learn
    to distinguish between similar and dissimilar data. Since data can be automatically
    distorted to different extents, creating “pseudo-labeled” data for self-supervised
    learning can be straightforward.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习利用大量未标记的数据，基于少量标记数据（参见 [[Yang et al., 2022](#bib.bibx83)] 的调查）。自监督学习受益于来自人工任务的自生成（伪）标签。两者都减少了收集（人工）标记数据的负担。自监督（预）训练结合（小型）人工标注数据集的微调可以取得最先进的结果。近年来，该范式得到了广泛发展（在
    [[Ericsson et al., 2022](#bib.bibx17)] 中进行了调查）。它通常与对比学习相结合。在对比学习中，目标是学习区分相似和不相似的数据。由于数据可以被自动扭曲到不同的程度，因此为自监督学习创建“伪标签”数据可以很简单。
- en: The simple framework for contrastive learning (SimCLR)[[Chen et al., 2020](#bib.bibx9)]
    maximizes agreement between two inputs that result from augmenting the same data
    sample differently. Augmentation can be random cropping, color distortions, and
    Gaussian blur. To obtain reprsentation vectors, a standard ResNet[[He et al.,
    2016](#bib.bibx27)] is used. Representations are further processed using a simple
    MLP before the contrastive loss is applied.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习的简单框架（SimCLR）[[Chen et al., 2020](#bib.bibx9)] 最大化通过不同的方式增强相同数据样本所产生的两个输入之间的一致性。增强可以是随机裁剪、颜色扭曲和高斯模糊。为了获得表示向量，使用了标准的
    ResNet[[He et al., 2016](#bib.bibx27)]。在应用对比损失之前，表示会进一步通过一个简单的 MLP 进行处理。
- en: Bootstrap Your Own Latent (BYOL) [[Grill et al., 2020](#bib.bibx21)] uses an
    online and a target network. Both have the same architecture consisting of an
    encoder, a projector, and a predictor but they do not share weights. The target
    network’s parameters are an exponential moving average of the online network’s
    parameters. The online network has to predict the target network’s representation
    given an augmentation of the (same) input.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 自助式潜在（BYOL）[[Grill et al., 2020](#bib.bibx21)] 使用一个在线网络和一个目标网络。两者具有相同的架构，包括一个编码器、一个投影器和一个预测器，但它们不共享权重。目标网络的参数是在线网络参数的指数移动平均。在线网络需要根据（相同的）输入的增强预测目标网络的表示。
- en: 'Barlow Twins[[Zbontar et al., 2021](#bib.bibx86)] rely on an objective function
    that aims to reduce cross-correlation $C$ between outputs for a set of image $Y^{A}$
    and their distorted versions $Y^{B}$ as close to the identity as possible, i.e.,
    the loss (including $\lambda$ as a tuning parameter) is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Barlow Twins[[Zbontar et al., 2021](#bib.bibx86)] 依赖于一个目标函数，该函数旨在将一组图像 $Y^{A}$
    及其扭曲版本 $Y^{B}$ 之间的交叉相关 $C$ 尽可能接近身份矩阵，即，损失（包括 $\lambda$ 作为调整参数）为：
- en: '|  | $\displaystyle L=\sum_{i}(1-C_{i,i})^{2}+\lambda\cdot\sum_{i}\sum_{j\neq
    i}C_{i,j}^{2}$ |  | (13) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L=\sum_{i}(1-C_{i,i})^{2}+\lambda\cdot\sum_{i}\sum_{j\neq
    i}C_{i,j}^{2}$ |  | (13) |'
- en: Momentum Contrast (MoCo) [[He et al., 2020](#bib.bibx26)] builds a dynamic dictionary
    represented by an encoder using unsupervised contrastive learning. Training performs
    look-ups and enforces that an encoded query should be similar to its matching
    encoded key and dissimilar to others. The dictionary is a queue of data samples.
    For every mini-batch, encoded samples are added, and the oldest mini-batch are
    dequeud. The key encoder is a momentum-based moving average of the query encoder,
    which should help to maintain consistency.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 动量对比（MoCo）[[He et al., 2020](#bib.bibx26)] 构建了一个由编码器表示的动态字典，使用无监督对比学习。训练过程中执行查找，并强制编码查询应与其匹配的编码键相似，而与其他键不相似。字典是一个数据样本队列。对于每个小批次，编码样本会被添加，最旧的小批次会被出队。键编码器是查询编码器的基于动量的移动平均，这有助于保持一致性。
- en: 'Noisy Student: The paper[[Xie et al., 2020](#bib.bibx81)] describes training
    an (EfficientNet) model on labeled data. This model is used as a teacher to generate
    pseudo labels for unlabeled images. A larger (EfficientNet) model is trained on
    the union of all data. This process is repeated, i.e., the student becomes the
    teacher of a new student. During student training, noise such as dropout and data
    augmentation are applied so that the student’s learning is harder and it can improve
    on the teacher.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'Noisy Student: 论文[[Xie et al., 2020](#bib.bibx81)]描述了在标记数据上训练一个 (EfficientNet)
    模型。该模型被用作教师，为未标记图像生成伪标签。一个更大的 (EfficientNet) 模型在所有数据的并集中进行训练。这个过程会重复，即学生成为新学生的教师。在学生训练期间，应用了如
    dropout 和数据增强等噪声，使得学生的学习更加困难，从而可以超越教师。'
- en: FixMatch [[Sohn et al., 2020](#bib.bibx69)] predicts the label of a weakly-augmented
    image. If the confidence for a label is above a threshold, then the model is trained
    to produce the same label for the strongly-augmented version of the image.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: FixMatch [[Sohn et al., 2020](#bib.bibx69)]预测一个弱增强图像的标签。如果标签的置信度超过阈值，则模型被训练以对图像的强增强版本产生相同的标签。
- en: 5 Architectures and Layers
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 架构与层
- en: We elaborate on four important layers types, i.e., activation-, skip-, normalization-,
    and attention layers followed by numerous contemporary architectures based on
    transformers as well as graph neural networks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们详细阐述了四种重要的层类型，即激活层、跳跃层、归一化层和注意力层，随后介绍了基于变换器和图神经网络的众多现代架构。
- en: 5.1 Activation
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 激活
- en: Activation functions are usually non-linear. They have a profound impact on
    gradient flow and, thus, on learning. Early activation functions commonly used
    from the 1960s throuhout the early 2000s such as sigmoid and tanh make training
    deep networks difficult due to the vanishing gradient when these functions saturate.
    The introduction of the rectified linear unit $ReLU$ in 2010[[Nair and Hinton,
    2010](#bib.bibx54)] marked a breakthrough result. While its original version is
    still commonly used, transformer architectures have popularized other activation
    functions and ReLU variants. Most of them still share qualitatively the behavior
    of ReLU, i.e., for negative inputs, outputs are of small magnitude and for positive
    inputs, they are unbounded (see [[Apicella et al., 2021](#bib.bibx3)] for a survey).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数通常是非线性的。它们对梯度流和学习有深远的影响。早期激活函数（如 sigmoid 和 tanh）从 1960 年代到 2000 年代初期常被使用，当这些函数饱和时，梯度消失使得深度网络的训练变得困难。2010
    年 $ReLU$ 的引入[[Nair and Hinton, 2010](#bib.bibx54)]标志着一个突破性的成果。虽然其原始版本仍然被广泛使用，但变换器架构已经普及了其他激活函数和
    ReLU 的变体。大多数它们仍然在质上共享 ReLU 的行为，即对于负输入，输出幅度较小，而对于正输入，则没有界限（参见 [[Apicella et al.,
    2021](#bib.bibx3)] 以获取综述）。
- en: Gaussian Error Linear Units (GELU)[[Hendrycks and Gimpel, 2016](#bib.bibx28)]
    weigh inputs by their precentile (ReLUs only use the sign). Activation is the
    product of the input and the standard Gaussian cumulative distribution function
    $\Phi(x)$, i.e.,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯误差线性单元（GELU）[[Hendrycks and Gimpel, 2016](#bib.bibx28)]通过其百分位数对输入进行加权（ReLU
    仅使用符号）。激活是输入和标准高斯累积分布函数 $\Phi(x)$ 的乘积，即：
- en: '|  | $\displaystyle GELU(x)=x\cdot\Phi(x)$ |  | (14) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle GELU(x)=x\cdot\Phi(x)$ |  | (14) |'
- en: 'The Mish activation[[Misra, 2019](#bib.bibx51)] originates from systematic
    experimentation inspired by Swish and ReLU:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Mish 激活[[Misra, 2019](#bib.bibx51)]源于受到 Swish 和 ReLU 启发的系统实验：
- en: '|  | $\displaystyle f(x)=x\cdot\tanh(soft^{+}(x))$ |  | (15) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(x)=x\cdot\tanh(soft^{+}(x))$ |  | (15) |'
- en: '|  | $\displaystyle\text{ with }soft^{+}(x):=\ln(1+e^{x})$ |  | (16) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{ with }soft^{+}(x):=\ln(1+e^{x})$ |  | (16) |'
- en: 'In comparison, the Swish activation[[Ramachandran et al., 2017](#bib.bibx61)]
    is:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Swish 激活[[Ramachandran et al., 2017](#bib.bibx61)]是：
- en: '|  | $\displaystyle f(x)=x\cdot sigmoid(\beta x)$ |  | (17) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(x)=x\cdot sigmoid(\beta x)$ |  | (17) |'
- en: Here $\beta$ is a learnable parameter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $\beta$ 是一个可学习的参数。
- en: 5.2 Skip connections
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 跳跃连接
- en: Skip connections originate from residual networks[[He et al., 2016](#bib.bibx27)].
    In the simplest form, the output $y$ for an input $x$ of a single layer $L$ (or
    a set of a few layers) with a skip connection is $y(x)=L(x)+x$. The original paper
    used the term residual since the layer $L$ has to learn a residual $L(x)=H(x)-x$
    rather than the desired mapping $H$ itself. Since then, skip connections have
    been used in many variations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃连接起源于残差网络[[He et al., 2016](#bib.bibx27)]。在最简单的形式中，具有跳跃连接的单层 $L$（或一组少数层）对输入
    $x$ 的输出 $y$ 为 $y(x)=L(x)+x$。原始论文使用了“残差”一词，因为层 $L$ 需要学习一个残差 $L(x)=H(x)-x$，而不是期望的映射
    $H$ 本身。从那时起，跳跃连接已经在许多变体中使用。
- en: 'Inverted Residual Block[[Sandler et al., 2018](#bib.bibx63)]: By inverting
    the channel width to a narrow-wide-narrow layer sequence from the original wide-narrow-wide
    order[[He et al., 2016](#bib.bibx27)] in combination with depthwise convolutions
    for the wide-layer, parameters are reduced, and residual blocks execute faster.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 反向残差块[[Sandler et al., 2018](#bib.bibx63)]：通过将通道宽度从原始的宽-窄-宽顺序逆转为窄-宽-窄层序列，并结合对宽层的深度卷积，减少了参数，并使残差块执行得更快。
- en: A Dense Block[[Huang et al., 2017](#bib.bibx30)] receives inputs from all prior
    layers (with matching feature-map sizes) and connects to all subsequent layers
    (with matching feature-map sizes).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密块[[Huang et al., 2017](#bib.bibx30)]从所有先前层（具有匹配的特征图大小）接收输入，并连接到所有后续层（具有匹配的特征图大小）。
- en: 'ResNeXt Block[[Xie et al., 2017](#bib.bibx82)]: This split-transform-merge
    approach for residual blocks entails evaluating multiple residual blocks in parallel
    and aggregating them back into a single output.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ResNeXt 块[[Xie et al., 2017](#bib.bibx82)]：这种分裂-变换-合并的方法对于残差块涉及并行评估多个残差块，并将它们汇总回一个单一输出。
- en: 5.3 Normalization
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 归一化
- en: Since the introduction of batch-normalization[[Ioffe and Szegedy, 2015](#bib.bibx31)],
    normalization has been a very successful concept in improving training speed,
    stability, and generalization of neural networks. However, their need is debated[[Shao
    et al., 2020](#bib.bibx66)], e.g., for some applications careful initialization
    and adjustments of learning rates might make them at least partially redundant.
    The idea of normalization is to transform a value $x$ to a normalized value $\tilde{x}$,
    by subtracting the mean $\mu$ and scaling by the standard deviation $\sigma$,
    i.e., $\tilde{x}=\frac{x-\mu}{\sigma}$. Normalization approaches differ in the
    computation of $\mu$ and $\sigma$, e.g., $\mu$ and $\sigma$ can be computed across
    different channels.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 自从引入批量归一化[[Ioffe and Szegedy, 2015](#bib.bibx31)]以来，归一化已成为提高神经网络训练速度、稳定性和泛化能力的一个非常成功的概念。然而，其必要性也存在争议[[Shao
    et al., 2020](#bib.bibx66)]，例如，对于某些应用，仔细的初始化和学习率调整可能使其至少部分多余。归一化的思想是将一个值 $x$ 转换为归一化值
    $\tilde{x}$，方法是减去均值 $\mu$ 并按标准差 $\sigma$ 进行缩放，即 $\tilde{x}=\frac{x-\mu}{\sigma}$。归一化方法在
    $\mu$ 和 $\sigma$ 的计算上有所不同，例如，$\mu$ 和 $\sigma$ 可以在不同的通道之间计算。
- en: 'Layer Normalization: Given summed inputs, normalization statistics are computed[[Ba
    et al., 2016](#bib.bibx4)] for a layer $L$ with $|L|$ neurons as:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化：给定总输入，计算层 $L$ 的归一化统计量[[Ba et al., 2016](#bib.bibx4)]，其中 $|L|$ 为神经元数量：
- en: '|  | $\displaystyle\mu=\frac{1}{&#124;L&#124;}\sum_{i=0}^{&#124;L&#124;-1}a_{i}\text{\phantom{abcd}}\sigma=\sqrt{\frac{1}{&#124;L&#124;}\sum_{i=0}^{&#124;L&#124;-1}(a_{i}-\mu)^{2}}$
    |  | (18) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mu=\frac{1}{\vert L \vert}\sum_{i=0}^{\vert L \vert-1}a_{i}\text{\phantom{abcd}}\sigma=\sqrt{\frac{1}{\vert
    L \vert}\sum_{i=0}^{\vert L \vert-1}(a_{i}-\mu)^{2}}$ |  | (18) |'
- en: In contrast to batch-normalization, it poses no restrictions on batch size and
    also no dependencies between batches. In particular, it can be used with batch
    size 1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与批量归一化相比，它对批量大小没有限制，并且批量之间没有依赖关系。特别是，它可以与批量大小为 1 的情况一起使用。
- en: 'Instance Normalization[[Ulyanov et al., 2016](#bib.bibx74)] computes for a
    4-dimensional input, such as an image with height $H$, width $W$, channels $C$,
    and batch size $T$:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 实例归一化[[Ulyanov et al., 2016](#bib.bibx74)] 针对一个四维输入进行计算，例如一个高度为 $H$、宽度为 $W$、通道为
    $C$ 和批量大小为 $T$ 的图像：
- en: '|  | $\displaystyle\mu_{t,c}=\frac{1}{HWT}\sum_{t<T,w<W,h<H}x_{t,c,w,h}$ |  |
    (19) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mu_{t,c}=\frac{1}{HWT}\sum_{t<T,w<W,h<H}x_{t,c,w,h}$ |  |
    (19) |'
- en: '|  | $\displaystyle\sigma_{t,c}=\sqrt{\frac{1}{HWT}\sum_{t<T,w<W,h<H}(x_{t,c,w,h}-\mu_{t,c})^{2}}$
    |  | (20) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sigma_{t,c}=\sqrt{\frac{1}{HWT}\sum_{t<T,w<W,h<H}(x_{t,c,w,h}-\mu_{t,c})^{2}}$
    |  | (20) |'
- en: It can be used, e.g., to normalize contrast for an image. There exist multiple
    versions of it, e.g., a version that scales based on weight norms[[Karras et al.,
    2020b](#bib.bibx34)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用来，例如，归一化图像的对比度。它有多个版本，例如，根据权重范数进行缩放的版本 [[Karras 等, 2020b](#bib.bibx34)]。
- en: 'LayerScale[[Touvron et al., 2021](#bib.bibx73)] has been introduced in the
    context of transformers as a per-channel multiplication of outputs of a residual
    block with a diagonal matrix:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: LayerScale [[Touvron 等, 2021](#bib.bibx73)] 是在 transformers 的背景下引入的，它通过对残差块输出与对角矩阵进行逐通道乘法：
- en: '|  | $\displaystyle x_{l^{\prime}}=x_{l}+diag(\lambda_{1},...,\lambda_{d})\cdot
    SA(\eta(x))$ |  | (21) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x_{l^{\prime}}=x_{l}+diag(\lambda_{1},...,\lambda_{d})\cdot
    SA(\eta(x))$ |  | (21) |'
- en: '|  | $\displaystyle x_{l+1}=x_{l^{\prime}}+diag(\lambda_{1},...,\lambda_{d})\cdot
    FFN(\eta(x))$ |  | (22) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x_{l+1}=x_{l^{\prime}}+diag(\lambda_{1},...,\lambda_{d})\cdot
    FFN(\eta(x))$ |  | (22) |'
- en: '$SA$ is the self-attention layer, $FFN$ is the feed forward network, and $\eta$
    the layer-normalisation (see Figure [2](#S5.F2 "Figure 2 ‣ 5.5 Transformers ‣
    5 Architectures and Layers ‣ A Survey of Deep Learning: From Activations to Transformers")).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: $SA$ 是自注意力层，$FFN$ 是前馈网络，而 $\eta$ 是层归一化（参见图 [2](#S5.F2 "图 2 ‣ 5.5 Transformers
    ‣ 5 架构和层 ‣ 深度学习概述：从激活到 Transformers")）。
- en: 5.4 Attention
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 注意力
- en: Attention mechanisms (surveyed in [[Brauwers and Frasincar, 2021](#bib.bibx7),
    [Guo et al., 2022b](#bib.bibx24)]) allow for learning relevance scores for inputs,
    similar to how cognitive attention works. Some parts of the inputs can be deemed
    highly important, while others are disregarded as irrelevant. The relevance of
    a particular input can often be determined by contextual information, such as
    the relevance of a word in a text document often depends on nearby words.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制（见[[Brauwers 和 Frasincar, 2021](#bib.bibx7), [Guo 等, 2022b](#bib.bibx24)]）允许对输入的相关性进行学习，类似于认知注意力的工作方式。输入的某些部分可以被认为是高度重要的，而其他部分则被忽略为不相关的。特定输入的相关性通常可以通过上下文信息来确定，例如，文本文件中一个词的相关性通常依赖于附近的词。
- en: 'Scaled Dot-Product Multi-Head Attention [[Vaswani et al., 2017](#bib.bibx75)]:
    Using dot products combined with down-scaling has proven very successful in computing
    attention scores. Attention takes a query $Q$, a key $K$ and a value $V$ as inputs
    and outputs an attention score:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放点积多头注意力 [[Vaswani 等, 2017](#bib.bibx75)]：结合点积和下缩放已被证明在计算注意力分数方面非常成功。注意力机制将查询
    $Q$、键 $K$ 和值 $V$ 作为输入，并输出一个注意力分数：
- en: '|  | $\displaystyle\text{Att}(Q,K,V)=\text{softmax}\big{(}\frac{QK^{T}}{\sqrt{d_{k}}}\big{)}\cdot
    V$ |  | (23) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Att}(Q,K,V)=\text{softmax}\big{(}\frac{QK^{T}}{\sqrt{d_{k}}}\big{)}\cdot
    V$ |  | (23) |'
- en: 'Using multiple, independent attention mechanisms in parallel allows attending
    to various aspects of the input. Formally, in multi-head attention, we learn matrixes
    W:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个独立的注意力机制并行处理可以关注输入的不同方面。形式上，在多头注意力中，我们学习矩阵 W：
- en: '|  | $\displaystyle\text{MultiHead}(\textbf{Q},\textbf{K},\textbf{V})=[\text{h}_{0},\dots,\text{h}_{n-1}]\textbf{W}_{0}$
    |  | (24) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{MultiHead}(\textbf{Q},\textbf{K},\textbf{V})=[\text{h}_{0},\dots,\text{h}_{n-1}]\textbf{W}_{0}$
    |  | (24) |'
- en: '|  | $\displaystyle\text{where }\text{head h}_{i}=\text{Att}(\textbf{Q}\textbf{W}_{i}^{Q},\textbf{K}\textbf{W}_{i}^{K},\textbf{V}\textbf{W}_{i}^{V})$
    |  | (25) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{其中 }\text{head h}_{i}=\text{Att}(\textbf{Q}\textbf{W}_{i}^{Q},\textbf{K}\textbf{W}_{i}^{K},\textbf{V}\textbf{W}_{i}^{V})$
    |  | (25) |'
- en: Factorized (Self-)Attention [[Child et al., 2019](#bib.bibx10)] reduces the
    computational and memory footprint of attention. While (full) self-attention[[Vaswani
    et al., 2017](#bib.bibx75)] allows attending to every prior input element, factorized
    self-attention allows only to attend to a subset thereof. Formally, an output
    matrix is computed given a matrix of input embeddings $X$ and the connectivity
    pattern $S=\{S_{1},...,S_{n}\}$, where $S_{i}$ is the set of indices of input
    vectors attended to by the $i$th output vector.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 分解（自）注意力 [[Child 等, 2019](#bib.bibx10)] 减少了注意力的计算和内存开销。虽然（完整的）自注意力 [[Vaswani
    等, 2017](#bib.bibx75)] 允许关注每个之前的输入元素，但分解自注意力仅允许关注其中的一个子集。形式上，给定输入嵌入矩阵 $X$ 和连接模式
    $S=\{S_{1},...,S_{n}\}$ 计算输出矩阵，其中 $S_{i}$ 是第 $i$ 个输出向量所关注的输入向量的索引集。
- en: '|  | $\displaystyle\text{FacAtt}(X,S)=(A(\mathbf{x}_{i},S_{i}))_{i\in[1,n]}$
    |  | (26) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{FacAtt}(X,S)=(A(\mathbf{x}_{i},S_{i}))_{i\in[1,n]}$
    |  | (26) |'
- en: '|  | $\displaystyle a(\mathbf{x}_{i},S_{i})=\text{softmax}(\frac{(W_{q}\mathbf{x}_{i})K^{T}_{S_{i}}}{\sqrt{d}})\cdot
    V_{S_{i}}$ |  | (27) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle a(\mathbf{x}_{i},S_{i})=\text{softmax}(\frac{(W_{q}\mathbf{x}_{i})K^{T}_{S_{i}}}{\sqrt{d}})\cdot
    V_{S_{i}}$ |  | (27) |'
- en: '|  | $\displaystyle K_{Si}=(W_{k}\mathbf{x}_{j})_{j\in{S_{i}}}\text{\phantom{abc}}V_{S_{i}}=(W_{v}\mathbf{x}_{j})_{j\in{S_{i}}}$
    |  | (28) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K_{Si}=(W_{k}\mathbf{x}_{j})_{j\in{S_{i}}}\text{\phantom{abc}}V_{S_{i}}=(W_{v}\mathbf{x}_{j})_{j\in{S_{i}}}$
    |  | (28) |'
- en: 'For full self-attention $S^{F}_{i}:=\{j|j\neq i\}$ (indexes to prior inputs
    to $i$). In contrast, factorized self-attention has $p$ separate attention heads,
    where the $m$th head defines a subset $A_{i}^{(m)}\subset S^{F}_{i}$ and lets
    $S_{i}=A_{i}^{(m)}$. For strided self-attention:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完整的自注意力 $S^{F}_{i}:=\{j|j\neq i\}$（索引到 $i$ 的先前输入）。相比之下，分解自注意力有 $p$ 个独立的注意力头，其中第
    $m$ 个头定义了一个子集 $A_{i}^{(m)}\subset S^{F}_{i}$ 并且让 $S_{i}=A_{i}^{(m)}$。对于步幅自注意力：
- en: '|  | $\displaystyle A_{i}^{(1)}=\{t,t+1,...i\}\text{ for }t=\max(0,i-l)$ |  |
    (29) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A_{i}^{(1)}=\{t,t+1,...i\}\text{ for }t=\max(0,i-l)$ |  |
    (29) |'
- en: '|  | $\displaystyle A_{i}^{(2)}=\{j:(i-j)\mod l=0\}$ |  | (30) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A_{i}^{(2)}=\{j:(i-j)\mod l=0\}$ |  | (30) |'
- en: 'This pattern is suitable, when structure aligns with the stride-like images.
    For data without a periodic structure like text, fixed attention can be preferable:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当结构与步幅类似的图像对齐时，这种模式是合适的。对于没有周期性结构的数据，如文本，固定注意力可能更为合适：
- en: '|  | $\displaystyle A_{i}^{(1)}=\{j:\lfloor j/l\rfloor=\lfloor i/l\rfloor\}$
    |  | (31) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A_{i}^{(1)}=\{j:\lfloor j/l\rfloor=\lfloor i/l\rfloor\}$
    |  | (31) |'
- en: '|  | $\displaystyle A_{i}^{(2)}=\{j:j\mod l\in\{t,t+1,...l\}\}$ |  | (32) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A_{i}^{(2)}=\{j:j\mod l\in\{t,t+1,...l\}\}$ |  | (32) |'
- en: where $t=l-c$ and $c$ is a hyperparameter. For example, for stride 128 and $c=8$,
    all future positions greater than 128 can attend to positions 120-128, all greater
    256 to 248-256, etc.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t=l-c$ 且 $c$ 是一个超参数。例如，对于步幅 128 和 $c=8$，所有大于 128 的未来位置可以关注于位置 120-128，所有大于
    256 的位置可以关注于 248-256，等等。
- en: A Residual Attention Network (RAN)[[Wang et al., 2017](#bib.bibx77)] module
    leverages the idea of skip connections. It consists of a mask and a trunk branch.
    The trunk branch performs feature processing. It can be any network. The mask
    branch represents feature weights. The output of an attention module is
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 残差注意力网络（RAN）[[Wang et al., 2017](#bib.bibx77)] 模块利用了跳过连接的思想。它由一个掩码和一个主干分支组成。主干分支执行特征处理。它可以是任何网络。掩码分支表示特征权重。注意力模块的输出是
- en: '|  | $\displaystyle H_{i,c}(x)=(1+M_{i,c}(x))\cdot F_{i,c}(X)$ |  | (33) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{i,c}(x)=(1+M_{i,c}(x))\cdot F_{i,c}(X)$ |  | (33) |'
- en: Here $i$ is a spatial position and $c$ is a channel. $M(x)$ should be approximatedly
    0, $H(x)$ approximates original features $F(x)$.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $i$ 是空间位置，$c$ 是通道。$M(x)$ 应该接近 0，$H(x)$ 接近原始特征 $F(x)$。
- en: Large Kernel Attention[[Guo et al., 2022a](#bib.bibx23)] decomposes a large
    scale convolution into three smaller scale convolutions using common ideas, i.e.,
    depth-wise dilated convolution, a non-dilated depthwise convolution, and a channel-wise
    1x1 convolution. For the output of these convolutions, an attention map is learned.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 大核注意力[[Guo et al., 2022a](#bib.bibx23)]将大规模卷积分解为三个较小规模的卷积，使用常见的思想，即深度可分卷积、非扩张深度卷积和通道方向的
    1x1 卷积。对于这些卷积的输出，学习了一个注意力图。
- en: Sliding Window Attention[[Beltagy et al., 2020](#bib.bibx6)] aims at improving
    the time and memory complexity of attention. It reduces the number of considered
    input pairs. More precisely, for a given window size $w$ each token attends to
    $\frac{w}{2}$ tokens on each side.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口注意力[[Beltagy et al., 2020](#bib.bibx6)]旨在提高注意力机制的时间和内存复杂度。它减少了考虑的输入对的数量。更准确地说，对于给定的窗口大小
    $w$，每个标记关注于每侧的 $\frac{w}{2}$ 个标记。
- en: 5.5 Transformers
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 Transformer
- en: Transformers have quickly become the dominant architecture in deep learning.
    Combined with self-supervised training on large datasets, they have reached state-of-the-art
    on many benchmarks in NLP(see [[Liu et al., 2023](#bib.bibx44)] for a survey)
    and computer vision (surveyed in [[Han et al., 2022](#bib.bibx25), [Khan et al.,
    2022](#bib.bibx36)]). Since their introduction in 2017[[Vaswani et al., 2017](#bib.bibx75)]
    countless versions have emerged that tackle issues of the original transformer
    such as computational overhead and data efficiency.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 已迅速成为深度学习中的主流架构。结合对大数据集的自监督训练，它们在许多 NLP（参见 [[Liu et al., 2023](#bib.bibx44)]
    的综述）和计算机视觉（在 [[Han et al., 2022](#bib.bibx25), [Khan et al., 2022](#bib.bibx36)]
    中进行了综述）基准测试中达到了最先进水平。自 2017 年引入以来[[Vaswani et al., 2017](#bib.bibx75)]，出现了无数个版本，解决了原始
    transformer 的问题，如计算开销和数据效率。
- en: 'Transformers are said to have less inductive bias and are in turn more flexible
    than other architectures, such as convolutional neural networks and recurrent
    networks. Thus, they also require more training data to compensate for the lack
    of inductive bias. Since large amounts of labeled data are difficult to obtain,
    transformers are commonly trained using self-supervised learning, i.e., pseudo-labels.
    The original transformer[[Vaswani et al., 2017](#bib.bibx75)], developed for natural
    language processing, employs an encoder and decoder like earlier recurrent neural
    networks. It stacks multiple transformer blocks on top of each other, as illustrated
    in Figure [2](#S5.F2 "Figure 2 ‣ 5.5 Transformers ‣ 5 Architectures and Layers
    ‣ A Survey of Deep Learning: From Activations to Transformers"). Key elements
    are multi-head attention, layer normalization, and skip connections. Furthermore,
    positional encodings and embeddings of inputs play an important role. The absolute
    positional encodings $PE$ for position $pos$ in [[Vaswani et al., 2017](#bib.bibx75)]
    uses sine and cosine functions varying in frequency:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '变换器被认为比其他架构（如卷积神经网络和递归网络）具有更少的归纳偏置，从而更加灵活。因此，它们也需要更多的训练数据来弥补归纳偏置的缺乏。由于大量标注数据难以获得，变换器通常采用自监督学习，即伪标签进行训练。最初的变换器[[Vaswani
    et al., 2017](#bib.bibx75)]，为自然语言处理开发，采用了类似于早期递归神经网络的编码器和解码器。它在彼此之上堆叠多个变换器块，如图[2](#S5.F2
    "Figure 2 ‣ 5.5 Transformers ‣ 5 Architectures and Layers ‣ A Survey of Deep Learning:
    From Activations to Transformers")所示。关键元素包括多头注意力、层归一化和跳跃连接。此外，位置编码和输入的嵌入也起着重要作用。[[Vaswani
    et al., 2017](#bib.bibx75)]中的位置绝对编码$PE$使用频率变化的正弦和余弦函数：'
- en: '|  | $\displaystyle\text{PE}(pos,2i)=\sin(pos/10000^{2i/d})$ |  | (34) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{PE}(pos,2i)=\sin(pos/10000^{2i/d})$ |  | (34) |'
- en: '|  | $\displaystyle\text{PE}(pos,2i+1)=\cos(pos/10000^{(2i)/d})$ |  | (35)
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{PE}(pos,2i+1)=\cos(pos/10000^{(2i)/d})$ |  | (35)
    |'
- en: where $i$ is the dimension of the encoding and $d$ is the number of dimensions.
    The choice was motivated by the fact that relative positions, which might be equally
    relevant to absolute ones, are a linear function of absolute position encodings.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$i$是编码的维度，$d$是维度的数量。选择这一点的原因是相对位置可能与绝对位置同样相关，是绝对位置编码的线性函数。
- en: '![Refer to caption](img/7c856046a9c5d74c64a641c5a57470d3.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c856046a9c5d74c64a641c5a57470d3.png)'
- en: 'Figure 2: Transformer with the four basic blocks on top and the encoder and
    decoder at the bottom'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：顶部四个基本块和底部的编码器和解码器的变换器
- en: Bidirectional Encoder Representations from Transformers(BERT) [[Devlin et al.,
    2018](#bib.bibx12)] yields contextual word-embeddings using the encoder of the
    transformer architecture. It relies on a masked language model pre-training objective
    and self-supervised learning. The model must predict randomly chosen, masked input
    tokens given its context. Thus, the model has bidirectional information, i.e.,
    it is fed tokens before and after the masked words. In classical next-word prediction
    no tokens after the word to predict are given. As a second prediction task, the
    model must predict if a sentence pair $(A,B)$ consists of two consecutive sentences
    $A$ and $B$ within some document (or two possibly unrelated sentences). The pre-trained
    model based on self-supervised training can be fine-tuned for downstream tasks
    using labeled data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 双向编码器表示变换器（BERT）[[Devlin et al., 2018](#bib.bibx12)]利用变换器架构的编码器生成上下文词嵌入。它依赖于掩蔽语言模型预训练目标和自监督学习。模型必须根据其上下文预测随机选择的掩蔽输入标记。因此，模型具有双向信息，即，它接收掩蔽词之前和之后的标记。在经典的下一个词预测中，无法获取预测词之后的标记。作为第二个预测任务，模型必须预测句子对$(A,B)$是否由某些文档中的两个连续句子$A$和$B$组成（或两个可能不相关的句子）。基于自监督训练的预训练模型可以通过标注数据进行下游任务的微调。
- en: The original BERT model has since then improved in many ways, e.g., [[Sanh et al.,
    2019](#bib.bibx64)] reduced the computational burden of BERT, and [[Liu et al.,
    2019b](#bib.bibx46)] trained models longer, on longer sequences, with bigger batches
    over more data, etc. This led to more robust and generalizable representations.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的BERT模型自那时以来在许多方面得到了改进，例如[[Sanh et al., 2019](#bib.bibx64)]减少了BERT的计算负担，[[Liu
    et al., 2019b](#bib.bibx46)]在更长的序列上用更大的批次和更多的数据训练了模型等。这导致了更稳健和更具泛化能力的表示。
- en: 'GPT to GPT-3 on to ChatGPT and GPT-4: GPT is based on the decoder of a transformer
    to predict tokens sequentially. GPT[[Radford et al., 2018](#bib.bibx58)] first
    performs pre-training in an unsupervised way before applying supervised fine-tuning.
    Pre-training takes place on a large corpus of tokens $U=(u_{0},u_{1},...,u_{n-1})$
    by maximizing the likelihood of the next token given prior tokens:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从GPT到GPT-3再到ChatGPT和GPT-4：GPT基于变换器的解码器顺序预测标记。GPT[[Radford et al., 2018](#bib.bibx58)]首先以无监督的方式进行预训练，然后应用有监督的微调。预训练在大量标记$U=(u_{0},u_{1},...,u_{n-1})$上进行，通过最大化给定先前标记的下一个标记的可能性：
- en: '|  | $\displaystyle L(U)=\sum_{i}p(u_{i}&#124;u_{i-k},...,u_{i-1})$ |  | (36)
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(U)=\sum_{i}p(u_{i}\mid u_{i-k},...,u_{i-1})$ |  | (36)
    |'
- en: 'where $k$ is the size of the context window and the conditional probability
    is modeled using a neural network, i.e., using a multi-layer transformer decoder[[Liu
    et al., 2018](#bib.bibx45)] by dropping the encoder in [[Vaswani et al., 2017](#bib.bibx75)].
    Rather than only predicting the next token given an input, the model is also trained
    to predict input tokens. Furthermore, the memory footprint of attention is lowered.
    GPT-2 [[Radford et al., 2019](#bib.bibx59)] builds on GPT with few modifications,
    e.g., layer normalization locations were changed (moved to the input of each sub-block,
    and an extra normalization was added after the final self-attention block), initialization
    of residual weights was scaled, and the vocabulary, context, and batch size were
    increased. GPT-3’s[[Brown et al., 2020](#bib.bibx8)] architecture is almost identical
    to that of GPT-2, but the number of parameters is more than 100 times larger and
    it differs in (amount of) training data. ChatGPT[[OpenAI, 2022](#bib.bibx55)]
    is a sibling to InstructGPT[[Ouyang et al., 2022](#bib.bibx57)], which is optimized
    towards following user intentions. InstructGPT applies fine-tuning of GPT-3 in
    a two-step process: (i) based on labeler demonstrations through supervised learning
    and (ii) based on human rankings of model outputs using reinforcement learning.
    ChatGPT follows the same procedure, i.e., (i) for supervised learning, human AI
    trainers provided conversations by playing both the human user and the AI assistant.
    The resulting dialogue dataset was enhanced with the InstructGPT dataset, which
    was transformed into a dialogue format. (ii) Conversations of AI trainers with
    ChatGPT were ranked, i.e., for a randomly selected model-written message, AI trainers
    ranked several alternative completions. The ranking dataset was used for reinforcement
    learning. The process was repeated multiple times.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$k$是上下文窗口的大小，条件概率通过神经网络建模，即使用多层变换器解码器[[Liu et al., 2018](#bib.bibx45)]，并在[[Vaswani
    et al., 2017](#bib.bibx75)]中省略了编码器。与仅仅根据输入预测下一个标记不同，该模型还被训练以预测输入标记。此外，注意力机制的内存占用降低了。GPT-2
    [[Radford et al., 2019](#bib.bibx59)] 在GPT的基础上进行了少量修改，例如层归一化的位置被改变（移动到每个子块的输入处，并在最终的自注意力块之后增加了额外的归一化），残差权重的初始化进行了缩放，并且词汇表、上下文和批量大小都增加了。GPT-3的[[Brown
    et al., 2020](#bib.bibx8)]架构几乎与GPT-2相同，但参数数量大了100倍以上，并且在（训练数据的）量上有所不同。ChatGPT[[OpenAI,
    2022](#bib.bibx55)] 是 InstructGPT[[Ouyang et al., 2022](#bib.bibx57)]的兄弟，后者优化了对用户意图的响应。InstructGPT通过两个步骤对GPT-3进行微调：（i）基于标注者演示的监督学习，（ii）基于对模型输出的人工排名使用强化学习。ChatGPT遵循相同的程序，即，（i）在监督学习中，人类AI训练师通过扮演人类用户和AI助手提供对话。结果对话数据集经过增强，结合了InstructGPT数据集，并转换为对话格式。（ii）对AI训练师与ChatGPT的对话进行排名，即，对于随机选择的模型生成的消息，AI训练师对多个替代完成进行了排名。排名数据集用于强化学习。这个过程重复了多次。
- en: Technical details of the successor of ChatGPT, i.e., GPT-4 have not been disclosed[[OpenAI,
    2023](#bib.bibx56)]. The provided technical report indicates that it is similar
    to ChatGPT. GPT-4 is multi-modal, i.e., it can also process images, however, details
    are unknown. The report only points towards major improvements in training efficiency.
    The accomplishment was to predict the performance of large scale models using
    the performance of small models (possibly trained on less data). This is highly
    important as computational costs and time can be a key factor for large deep learning
    models.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的继任者，即GPT-4的技术细节尚未披露[[OpenAI, 2023](#bib.bibx56)]。提供的技术报告指出，它与ChatGPT类似。GPT-4是多模态的，即它也可以处理图像，但具体细节未知。报告仅指出在训练效率方面有重大改进。成就是通过使用小规模模型（可能训练在较少的数据上）的性能来预测大规模模型的性能。这一点非常重要，因为计算成本和时间可能是大型深度学习模型的关键因素。
- en: Text-to-Text Transfer Transformer (T5)[[Raffel et al., 2020](#bib.bibx60)] views
    every text-based language models as generating an output text from a given input
    text. It differs from BERT[[Devlin et al., 2018](#bib.bibx12)] by using causal
    masking during training for predicting the target. Causal masking prevents the
    network from accessing “future” tokens of the target. T5 also differs in pre-training
    tasks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Text-to-Text Transfer Transformer (T5)[[Raffel et al., 2020](#bib.bibx60)] 将每个基于文本的语言模型视为从给定的输入文本生成输出文本。它与BERT[[Devlin
    et al., 2018](#bib.bibx12)]的不同之处在于在训练期间使用了因果掩码以预测目标。因果掩码防止网络访问目标的“未来”标记。T5在预训练任务上也有所不同。
- en: BART[[Lewis et al., 2020](#bib.bibx41)] is a denoising autoencoder for pretraining
    sequence-to-sequence models that uses a standard transformer based machine translation
    architecture. It has been shown to be effective for language generation, translation,
    and comprehension. Training is based on corrupting text with noising functions
    ranging from token deletion, masking onto sentence permutation and document rotation.
    Learning stems form reconstructing the original text from its corrputed version.
    The flexibility in noising options is attributed due to BART’s generalization
    of prior works such as BERT and GPT, i.e., the encoder is bi-directional (like
    BERT), while the decoder is autoregressive (like GPT).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: BART[[Lewis et al., 2020](#bib.bibx41)] 是一种用于预训练序列到序列模型的去噪自编码器，采用了标准的基于变换器的机器翻译架构。它已被证明对语言生成、翻译和理解非常有效。训练是基于用噪声函数破坏文本，这些噪声函数包括从标记删除、掩码到句子置换和文档旋转。学习源于从其破坏版本重建原始文本。噪声选项的灵活性归因于BART对BERT和GPT等先前工作的推广，即编码器是双向的（如BERT），而解码器是自回归的（如GPT）。
- en: 'XLNet [[Yang et al., 2019](#bib.bibx84)] combines advantages of autoregressive
    modeling like GPT, predicting the next token, and denoising auto-encoding BERT[[Devlin
    et al., 2018](#bib.bibx12)], reconstructing $x$ given a noisy input $\hat{x}$
    that originates through masking words of $x$. It does so by using a permutation
    language model that samples a permutation of $Z={z_{0},z_{1},...,z_{T-1}}$ of
    the sequence $(0,1,2,...,T-1)$ leading to the objective:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: XLNet [[Yang et al., 2019](#bib.bibx84)] 结合了自回归建模（如GPT）预测下一个标记的优点和去噪自编码BERT[[Devlin
    et al., 2018](#bib.bibx12)] 通过重建$x$给定一个通过掩蔽词的嘈杂输入$\hat{x}$。它通过使用排列语言模型来实现，这个模型对序列$(0,1,2,...,T-1)$的$Z={z_{0},z_{1},...,z_{T-1}}$进行排列，从而达到目标：
- en: '|  | $\displaystyle\max p(u_{z_{T}}&#124;u_{z_{0}},...,u_{z_{T-1})}$ |  | (37)
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max p(u_{z_{T}}&#124;u_{z_{0}},...,u_{z_{T-1})}$ |  | (37)
    |'
- en: There is no actual permutation of inputs, which would be unnatural (and not
    occurring during later fine-tuning tasks). Rather, the permutation impacts the
    attention mask to ensure that the factorization order by $Z$ is maintained.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上没有输入的排列，这会是不自然的（且不会发生在后续的微调任务中）。相反，排列影响注意力掩码，以确保按$Z$的因式分解顺序保持。
- en: The Vision Transformer [[Dosovitskiy et al., 2020](#bib.bibx15)] relies heavily
    on the original transformer. An image is partitioned into small patches, which
    are flattened and linearly embedded with position embeddings. A standard transformer
    encoder then processes the created vector of each patch.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Vision Transformer [[Dosovitskiy et al., 2020](#bib.bibx15)] 在很大程度上依赖于原始变换器。图像被划分为小块，这些小块被展平并与位置嵌入线性嵌入。标准的变换器编码器随后处理每个小块创建的向量。
- en: The Swin Transformer [[Liu et al., 2021](#bib.bibx47)] for computer vision builds
    hierarchical feature maps rather than just a single (resolution) feature map.
    It also only computes self-attention within a local window reducing computation
    time.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Swin Transformer [[Liu et al., 2021](#bib.bibx47)] 用于计算机视觉，构建层次特征图，而不是仅仅构建单一（分辨率）特征图。它还仅在局部窗口内计算自注意力，从而减少计算时间。
- en: 'PaLM (2): The original PaLM[[Chowdhery et al., 2022](#bib.bibx11)] is a large
    language model consisting of 540 billion parameters similar to other more prominent
    such as GPT-3\. Technical innovation discussed is mostly on the scaling of model
    training, i.e., a single model can be trained across tens of thousands of accelerator
    chips efficiently. The original transformer architecture[[Vaswani et al., 2017](#bib.bibx75)]
    is also adjusted slightly, e.g., SwiGLU activations are used, i.e.,'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'PaLM (2): 原始PaLM[[Chowdhery et al., 2022](#bib.bibx11)] 是一个包含5400亿参数的大型语言模型，类似于其他更突出的模型如GPT-3。讨论的技术创新主要集中在模型训练的扩展上，即单个模型可以高效地在成千上万的加速器芯片上进行训练。原始变换器架构[[Vaswani
    et al., 2017](#bib.bibx75)] 也略有调整，例如，使用了SwiGLU激活函数，即，'
- en: '|  | $\displaystyle Swish(xW)\cdot xV$ |  | (38) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Swish(xW)\cdot xV$ |  | (38) |'
- en: ', where Swish is given by Eq. [17](#S5.E17 "In 5.1 Activation ‣ 5 Architectures
    and Layers ‣ A Survey of Deep Learning: From Activations to Transformers"), different
    positional embeddings (better for long sequences), and multi-query attention (faster
    computation), no biases (better training stability), and shared input-output embeddings.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '，其中Swish由Eq. [17](#S5.E17 "In 5.1 Activation ‣ 5 Architectures and Layers ‣
    A Survey of Deep Learning: From Activations to Transformers")给出，不同的位置信息嵌入（更适合长序列），以及多查询注意力（更快的计算），无偏差（更好的训练稳定性），以及共享输入-输出嵌入。'
- en: PaLM 2[[Google, 2023](#bib.bibx20)] is the better performing successor of PaLM
    that differs in terms of dataset mixtures, e.g., using more diverse languages
    as well as domains (e.g., programing languages, mathematics). It also uses the
    classical transformer architecture. However, it uses a smaller model than the
    first PaLM version but more training compute. It also relies on more diverse pre-training
    objectives (than simple next word or masked word prediction).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: PaLM 2[[Google, 2023](#bib.bibx20)] 是PaLM的表现更好的继任者，在数据集混合方面有所不同，例如，使用更多样化的语言以及领域（例如编程语言、数学）。它也使用经典的变换器架构。然而，它使用比第一个PaLM版本更小的模型，但需要更多的训练计算。它还依赖于更多样化的预训练目标（而非简单的下一个词或掩蔽词预测）。
- en: 5.6 Graph Neural Networks
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 图神经网络
- en: Graph neural networks (surveyed in [[Wu et al., 2020](#bib.bibx80)]) can be
    seen as a generalization of CNNs and transformers. They operate on graph data,
    i.e., nodes connected with edges. We discuss graph models, including models to
    obtain node embeddings that can be used for downstream tasks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（在[[Wu et al., 2020](#bib.bibx80)]中进行了综述）可以视为CNNs和变换器的推广。它们在图数据上操作，即连接的节点。我们讨论图模型，包括用于获取可以用于下游任务的节点嵌入的模型。
- en: Graph Convolutional Networks[[Kipf and Welling, 2016](#bib.bibx40)] use CNNs
    for semi-supervised learning. They approximate spectral graph convolutions using
    polynomials of order $k$, which a CNN can compute with $k$ linear layers.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Graph Convolutional Networks[[Kipf and Welling, 2016](#bib.bibx40)] 使用CNNs进行半监督学习。他们使用
    $k$ 阶多项式来近似谱图卷积，而CNN可以通过 $k$ 个线性层来计算。
- en: Graph Attention Networks[[Veličković et al., 2017](#bib.bibx76)] rely on masked
    self-attention layers allowing nodes to attend flexibly over their neighborhoods’
    features, i.e., node $j$ obtains importance scores for node $i$’s features. Masking
    allows to only consider edges between node pairs that are actually connected.
    In contrast to GCN, different importances for nodes in the same neighborhood can
    be assigned. Also, it does not rely on costly matrix operations for eigendecompositions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Graph Attention Networks[[Veličković et al., 2017](#bib.bibx76)] 依赖于掩蔽自注意力层，允许节点灵活地关注其邻域的特征，即节点
    $j$ 为节点 $i$ 的特征获取重要性评分。掩蔽允许仅考虑实际连接的节点对之间的边缘。与GCN不同，可以为同一邻域中的节点分配不同的重要性。此外，它不依赖于代价高昂的矩阵运算进行特征分解。
- en: Graph Transformer[[Dwivedi and Bresson, 2020](#bib.bibx16)] extends the original
    transformer to graphs by using attention over neighborhood connectivity for each
    node, generalizing the position encoding, replacing layer- with batch-normalization,
    and learning edge representations (in addition to node representations).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Graph Transformer[[Dwivedi and Bresson, 2020](#bib.bibx16)] 通过对每个节点的邻域连接使用注意力机制，将原始变换器扩展到图形中，概括了位置编码，替换了层归一化为批归一化，并学习边缘表示（除了节点表示）。
- en: TuckER[[Balažević et al., 2019](#bib.bibx5)] performs factorization for link
    prediction in knowledge graph. Knowledge is represented as (subject, relation,
    object) triplets, and the task is to predict whether two entities are related.
    The graph can be represented as a binary tensor with the subjects, relations,
    and objects as dimensions. They use Tucker decompositions to decompose the binary
    tensor into a product of a core matrix and embedding matrices for subjects, relations,
    and objects.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: TuckER[[Balažević et al., 2019](#bib.bibx5)] 执行知识图谱中的链接预测因式分解。知识表示为（主体、关系、对象）三元组，任务是预测两个实体是否相关。图可以表示为一个二元张量，其中主体、关系和对象作为维度。他们使用Tucker分解将二元张量分解为核心矩阵和主体、关系以及对象的嵌入矩阵的乘积。
- en: Embedding by Relational Rotation (RotatE)[[Sun et al., 2019](#bib.bibx72)] performs
    missing link prediction in knowledge graphs (like the priorly described TuckER[[Balažević
    et al., 2019](#bib.bibx5)]) to model more relational properties such as composition
    and inversion. They embed entities into a complex space and treat the relation
    as an element-wise rotation that is optimized to lead from one entity to the other.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过关系旋转（RotatE）[[Sun et al., 2019](#bib.bibx72)]进行知识图谱中的缺失链接预测（如先前描述的TuckER[[Balažević
    et al., 2019](#bib.bibx5)]），以建模更多的关系属性，如组合和逆转。他们将实体嵌入到复数空间中，并将关系视为元素级旋转，优化以使得从一个实体到另一个实体。
- en: Scalable Feature Learning for Networks(Node2Vec)[[Grover and Leskovec, 2016](#bib.bibx22)]
    learns feature vectors that preserve a node’s neighborhood. They use random walks
    to generate sample neighborhoods, thereby, nodes are viewed based on their role
    or communities they belong to.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的可扩展特征学习（Node2Vec）[[Grover and Leskovec, 2016](#bib.bibx22)]学习保留节点邻域的特征向量。他们使用随机游走生成样本邻域，从而根据节点的角色或其所属社区来观察节点。
- en: 6 Discussion
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: Our survey focused on key design elements in building deep learning models.
    Taking a practical approach, we chose to ignore theoretical works, which should
    be further explored in future studies. Our findings suggest that despite many
    small and creative innovations since the original transformer architecture, there
    have not been any significant ”breakthrough” discoveries that have led to much
    better leaderboard results. The last few years have been characterized by the
    enlargement of existing networks such as GPT, the increase of data volume (and
    quality), and a shift towards self-supervised learning. This could indicate a
    need for more daring approaches to research rather than incremental improvements
    of existing works. Combining different elements as outlined in this work could
    be one way to achieve this.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查集中在构建深度学习模型的关键设计元素上。采取实用的方法，我们选择忽略理论工作，这些工作应在未来的研究中进一步探讨。我们的发现表明，尽管自原始Transformer架构以来出现了许多小而创新的改进，但没有任何显著的“突破性”发现导致了更好的排行榜结果。过去几年以现有网络的扩展（如GPT）、数据量（及质量）的增加和自监督学习的转变为特征。这可能表明需要更大胆的研究方法，而不是对现有工作的逐步改进。结合本文中概述的不同元素可能是一种实现这一目标的方法。
- en: 'In addition, we noted a few general patterns that have been proven effective
    in many areas:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们注意到了一些在多个领域中已被证明有效的一般模式：
- en: •
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: “Multi-X”, i.e., using the same element multiple times in parallel, such as
    using multiple residual blocks (ResNeXt) or multi-head attention. This idea is
    also closely related to “ensemble learning”.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “Multi-X”，即在并行中多次使用相同的元素，例如使用多个残差块（ResNeXt）或多头注意力。这一思想也与“集成学习”密切相关。
- en: •
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: “Higher order layers”, i.e., classical CNNs and MLPs only apply linear layers
    and simple ReLU, but layers like Mish or attention layers perform more complex
    operations.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “高阶层”，即经典的卷积神经网络（CNNs）和多层感知机（MLPs）仅应用线性层和简单的ReLU，但像Mish或注意力层这样的层执行更复杂的操作。
- en: •
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: “Moving average”, i.e., averaging weights such as for SGD and BYOL.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “移动平均”，即对权重进行平均，例如SGD和BYOL。
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: “Decompose”, i.e., decomposing matrixes such as for TuckER and large kernel
    attention.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “分解”，即分解矩阵，例如TuckER和大核注意力。
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: “Weighing functions”, i.e., using parameterized weighing functions of inputs
    can be seen within the attention mechanism but also for GELU units. Therefore,
    rather than naively aggregating inputs, inputs are weighed and aggregated. The
    weight might stem from a function with learnt parameters. Such functions can also
    be seen as “gates” that only permit the flow of information within some range
    of the input parameters.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “加权函数”，即使用参数化的输入加权函数，这可以在注意力机制中看到，也可以用于GELU单元。因此，输入不是简单地聚合，而是被加权和聚合。权重可能来自具有学习参数的函数。这些函数也可以视为“门”，仅允许在某些输入参数范围内的信息流动。
- en: Our survey was also deliberately geared towards more recent works, but still
    well-established works; this could be perceived as a strength or as a limitation.
    The selection of papers and areas was driven by a prominent platform providing
    leaderboards. While a reader looking for “what works well and what is very promising”
    benefits from this approach, it could potentially leave out works with exciting
    ideas that require more research to reveal their full capabilities. This could
    be seen as perpetuating the ”winner-takes-all” paradigm that reinforces already
    successful ideas. However, due to the sheer amount of papers, a selection is necessary
    for conducting a holistic survey of deep learning. We acknowledge that online
    platforms providing leaderboards etc. are very beneficial to the research community
    and that they should be further advanced. Still, we found that manual verification
    (e.g., by double checking relevance with Google scholar citations and by reading
    surveys and papers) was required as we identified works and methods that were
    not listed correctly on the platform.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查也故意针对了较新的研究成果，但仍包括了经过充分验证的工作；这可能被视为一种优势或局限性。论文和领域的选择受到一个提供排行榜的突出平台的驱动。虽然寻求“什么效果好，什么非常有前景”的读者会从这种方法中受益，但这可能会排除一些充满激动人心的想法的工作，这些想法需要更多研究才能揭示其全部潜力。这可能被视为
    perpetuating “赢家通吃”的范式，强化了已经成功的想法。然而，由于论文数量庞大，为了进行全面的深度学习调查，选择是必要的。我们承认，提供排行榜等在线平台对研究社区非常有益，并且应该进一步发展。然而，我们发现手动验证（例如，通过
    Google Scholar 引用的双重检查相关性和阅读调查与论文）是必要的，因为我们发现了一些在平台上未正确列出的工作和方法。
- en: 7 Conclusions
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We have presented a brief but comprehensive overview of the deep learning design
    landscape. We have summarized key works from various significant areas that have
    emerged in recent years. We believe that our holistic overview in one paper can
    establish connections that could inspire novel ideas. We have also identified
    four patterns that characterize many improvements. To further advance the development
    of deep learning, we need to generate fundamentally new and successful approaches,
    as the improvements made in the past few years were numerous and often very creative
    but mainly incremental.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们呈现了一个简短但全面的深度学习设计景观概述。我们总结了近年来出现的各种重要领域的关键工作。我们相信，我们在一篇论文中的整体概述可以建立联系，激发新的想法。我们还识别出了四种特征许多改进的模式。为了进一步推动深度学习的发展，我们需要生成根本新的成功方法，因为过去几年中的改进虽然众多且常常非常有创意，但主要是渐进的。
- en: REFERENCES
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Alom et al., 2019 Alom, M. Z., Taha, T. M., Yakopcic, C., Westberg, S., Sidike,
    P., Nasrin, M. S., Hasan, M., Van Essen, B. C., Awwal, A. A., and Asari, V. K.
    (2019). A state-of-the-art survey on deep learning theory and architectures. electronics.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alom 等，2019 Alom, M. Z., Taha, T. M., Yakopcic, C., Westberg, S., Sidike, P.,
    Nasrin, M. S., Hasan, M., Van Essen, B. C., Awwal, A. A., 和 Asari, V. K.（2019）。深度学习理论和架构的最新调查。电子学。
- en: 'Alzubaidi et al., 2021 Alzubaidi, L., Zhang, J., Humaidi, A. J., Al-Dujaili,
    A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M. A., Al-Amidie, M., and
    Farhan, L. (2021). Review of deep learning: Concepts, CNN architectures, challenges,
    applications, future directions. Journal of big Data.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alzubaidi 等，2021 Alzubaidi, L., Zhang, J., Humaidi, A. J., Al-Dujaili, A., Duan,
    Y., Al-Shamma, O., Santamaría, J., Fadhel, M. A., Al-Amidie, M., 和 Farhan, L.（2021）。深度学习回顾：概念、CNN
    架构、挑战、应用、未来方向。大数据杂志。
- en: Apicella et al., 2021 Apicella, A., Donnarumma, F., Isgrò, F., and Prevete,
    R. (2021). A survey on modern trainable activation functions. Neural Networks.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apicella 等，2021 Apicella, A., Donnarumma, F., Isgrò, F., 和 Prevete, R.（2021）。现代可训练激活函数的调查。神经网络。
- en: Ba et al., 2016 Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization.
    arXiv:1607.06450.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba 等，2016 Ba, J. L., Kiros, J. R., 和 Hinton, G. E.（2016）。层归一化。arXiv:1607.06450。
- en: 'Balažević et al., 2019 Balažević, I., Allen, C., and Hospedales, T. M. (2019).
    Tucker: Tensor factorization for knowledge graph completion. arXiv:1901.09590.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balažević 等，2019 Balažević, I., Allen, C., 和 Hospedales, T. M.（2019）。Tucker：用于知识图谱补全的张量分解。arXiv:1901.09590。
- en: 'Beltagy et al., 2020 Beltagy, I., Peters, M. E., and Cohan, A. (2020). Longformer:
    The long-document transformer. arXiv:2004.05150.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltagy 等，2020 Beltagy, I., Peters, M. E., 和 Cohan, A.（2020）。Longformer：长文档变换器。arXiv:2004.05150。
- en: Brauwers and Frasincar, 2021 Brauwers, G. and Frasincar, F. (2021). A general
    survey on attention mechanisms in deep learning. Transactions on Knowledge and
    Data Engineering.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brauwers 和 Frasincar，2021 Brauwers, G. 和 Frasincar, F.（2021）。深度学习中注意力机制的全面调查。知识与数据工程学报。
- en: Brown et al., 2020 Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020).
    Language models are few-shot learners. Advances in neural information processing
    systems.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人，2020 Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., 等 (2020). 语言模型是少样本学习者。神经信息处理系统进展。
- en: Chen et al., 2020 Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020).
    A simple framework for contrastive learning of visual representations. In Int.
    Conf. on machine learning.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人，2020 Chen, T., Kornblith, S., Norouzi, M., 和 Hinton, G. (2020). 一种简单的视觉表示对比学习框架。在国际机器学习会议上。
- en: Child et al., 2019 Child, R., Gray, S., Radford, A., and Sutskever, I. (2019).
    Generating long sequences with sparse transformers. arXiv:1904.10509.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等人，2019 Child, R., Gray, S., Radford, A., 和 Sutskever, I. (2019). 使用稀疏变换器生成长序列。arXiv:1904.10509。
- en: 'Chowdhery et al., 2022 Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. (2022).
    Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等人，2022 Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., 等 (2022).
    Palm: 通过路径扩展语言建模。arXiv 预印本 arXiv:2204.02311。'
- en: 'Devlin et al., 2018 Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018).
    Bert: Pre-training of deep bidirectional transformers for language understanding.
    arXiv:1810.04805.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等人，2018 Devlin, J., Chang, M.-W., Lee, K., 和 Toutanova, K. (2018). Bert:
    用于语言理解的深度双向变换器预训练。arXiv:1810.04805。'
- en: Dong et al., 2021 Dong, S., Wang, P., and Abbas, K. (2021). A survey on deep
    learning and its applications. Computer Science Review.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人，2021 Dong, S., Wang, P., 和 Abbas, K. (2021). 深度学习及其应用的综述。计算机科学评论。
- en: Dong and Shen, 2018 Dong, X. and Shen, J. (2018). Triplet loss in siamese network
    for object tracking. In European Conf. on computer vision (ECCV).
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 和 Shen，2018 Dong, X. 和 Shen, J. (2018). 在孪生网络中使用三元组损失进行目标跟踪。在欧洲计算机视觉会议
    (ECCV) 上。
- en: 'Dosovitskiy et al., 2020 Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al. (2020). An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv:2010.11929.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等人，2020 Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., 等 (2020). 一张图片胜过 16x16 个词：用于大规模图像识别的变换器。arXiv:2010.11929。
- en: Dwivedi and Bresson, 2020 Dwivedi, V. P. and Bresson, X. (2020). A generalization
    of transformer networks to graphs. arXiv:2012.09699.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwivedi 和 Bresson，2020 Dwivedi, V. P. 和 Bresson, X. (2020). 将变换器网络推广到图形的概括。arXiv:2012.09699。
- en: 'Ericsson et al., 2022 Ericsson, L., Gouk, H., Loy, C. C., and Hospedales, T. M.
    (2022). Self-supervised representation learning: Introduction, advances, and challenges.
    Signal Processing Magazine.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ericsson 等人，2022 Ericsson, L., Gouk, H., Loy, C. C., 和 Hospedales, T. M. (2022).
    自监督表示学习：介绍、进展和挑战。信号处理杂志。
- en: Foret et al., 2020 Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B. (2020).
    Sharpness-aware minimization for efficiently improving generalization. arXiv:2010.01412.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foret 等人，2020 Foret, P., Kleiner, A., Mobahi, H., 和 Neyshabur, B. (2020). 针对高效提升泛化能力的尖锐度感知最小化。arXiv:2010.01412。
- en: 'Ghiasi et al., 2018 Ghiasi, G., Lin, T.-Y., and Le, Q. V. (2018). Dropblock:
    A regularization method for convolutional networks. Advances in neural information
    processing systems.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ghiasi 等人，2018 Ghiasi, G., Lin, T.-Y., 和 Le, Q. V. (2018). Dropblock: 一种用于卷积网络的正则化方法。神经信息处理系统进展。'
- en: Google, 2023 Google (2023). Palm 2 technical report. https://ai.google/static/documents/palm2techreport.pdf.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google，2023 Google (2023). Palm 2 技术报告。https://ai.google/static/documents/palm2techreport.pdf。
- en: Grill et al., 2020 Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond,
    P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M.,
    et al. (2020). Bootstrap your own latent-a new approach to self-supervised learning.
    Adv. in neural information processing systems.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grill 等人，2020 Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P.,
    Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., 等
    (2020). 引导你自己的潜在变量——一种新的自监督学习方法。神经信息处理系统进展。
- en: 'Grover and Leskovec, 2016 Grover, A. and Leskovec, J. (2016). node2vec: Scalable
    feature learning for networks. In ACM SIGKDD Int. Conf. on Knowledge discovery
    and data mining.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Grover 和 Leskovec，2016 Grover, A. 和 Leskovec, J. (2016). node2vec: 网络的可扩展特征学习。在
    ACM SIGKDD 知识发现与数据挖掘国际会议上。'
- en: Guo et al., 2022a Guo, M.-H., Lu, C.-Z., Liu, Z.-N., Cheng, M.-M., and Hu, S.-M.
    (2022a). Visual attention network. arXiv:2202.09741.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人，2022a Guo, M.-H., Lu, C.-Z., Liu, Z.-N., Cheng, M.-M., 和 Hu, S.-M. (2022a).
    视觉注意力网络。arXiv:2202.09741。
- en: 'Guo et al., 2022b Guo, M.-H., Xu, T.-X., Liu, J.-J., Liu, Z.-N., Jiang, P.-T.,
    Mu, T.-J., Zhang, S.-H., Martin, R. R., Cheng, M.-M., and Hu, S.-M. (2022b). Attention
    mechanisms in computer vision: A survey. Computational Visual Media.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等，2022b Guo, M.-H., Xu, T.-X., Liu, J.-J., Liu, Z.-N., Jiang, P.-T., Mu,
    T.-J., Zhang, S.-H., Martin, R. R., Cheng, M.-M., 和 Hu, S.-M. (2022b). 计算机视觉中的注意机制：综述。计算视觉媒体。
- en: Han et al., 2022 Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang,
    Y., Xiao, A., Xu, C., Xu, Y., et al. (2022). A survey on vision transformer. transactions
    on pattern analysis and machine intelligence.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等，2022 Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y.,
    Xiao, A., Xu, C., Xu, Y., 等 (2022). 视觉变换器的综述。模式分析与机器智能杂志。
- en: He et al., 2020 He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. (2020). Momentum
    contrast for unsupervised visual representation learning. In Conf. on computer
    vision and pattern recognition.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等，2020 He, K., Fan, H., Wu, Y., Xie, S., 和 Girshick, R. (2020). 用于无监督视觉表示学习的动量对比。会议论文：计算机视觉与模式识别。
- en: He et al., 2016 He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual
    learning for image recognition. In Conf. on computer vision and pattern recognition.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等，2016 He, K., Zhang, X., Ren, S., 和 Sun, J. (2016). 用于图像识别的深度残差学习。会议论文：计算机视觉与模式识别。
- en: Hendrycks and Gimpel, 2016 Hendrycks, D. and Gimpel, K. (2016). Gaussian error
    linear units (gelus). arXiv:1606.08415.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 和 Gimpel, 2016 Hendrycks, D. 和 Gimpel, K. (2016). 高斯误差线性单元（gelus）。arXiv:1606.08415。
- en: Heusel et al., 2017 Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B.,
    and Hochreiter, S. (2017). Gans trained by a two time-scale update rule converge
    to a local nash equilibrium. Advances in neural information processing systems.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heusel 等，2017 Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., 和 Hochreiter,
    S. (2017). 由双时间尺度更新规则训练的生成对抗网络收敛到局部纳什均衡。神经信息处理系统进展。
- en: Huang et al., 2017 Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q.
    (2017). Densely connected convolutional networks. In Conf. on computer vision
    and pattern recognition.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等，2017 Huang, G., Liu, Z., Van Der Maaten, L., 和 Weinberger, K. Q. (2017).
    密集连接卷积网络。会议论文：计算机视觉与模式识别。
- en: 'Ioffe and Szegedy, 2015 Ioffe, S. and Szegedy, C. (2015). Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. In Int.
    Conf. on machine learning.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe 和 Szegedy, 2015 Ioffe, S. 和 Szegedy, C. (2015). 批量归一化：通过减少内部协方差偏移来加速深度网络训练。国际机器学习会议论文。
- en: Izmailov et al., 2018 Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D.,
    and Wilson, A. G. (2018). Averaging weights leads to wider optima and better generalization.
    arXiv:1803.05407.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izmailov 等，2018 Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., 和 Wilson,
    A. G. (2018). 权重平均导致更广泛的最优解和更好的泛化能力。arXiv:1803.05407。
- en: Karras et al., 2020a Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen,
    J., and Aila, T. (2020a). Analyzing and improving the image quality of stylegan.
    In Conf. on computer vision and pattern recognition.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras 等，2020a Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J.,
    和 Aila, T. (2020a). 分析和改进 stylegan 的图像质量。会议论文：计算机视觉与模式识别。
- en: Karras et al., 2020b Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen,
    J., and Aila, T. (2020b). Analyzing and improving the image quality of stylegan.
    In Conf. on computer vision and pattern recognition.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras 等，2020b Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J.,
    和 Aila, T. (2020b). 分析和改进 stylegan 的图像质量。会议论文：计算机视觉与模式识别。
- en: Khan et al., 2020 Khan, A., Sohail, A., Zahoora, U., and Qureshi, A. S. (2020).
    A survey of the recent architectures of deep convolutional neural networks. Artificial
    intelligence review.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等，2020 Khan, A., Sohail, A., Zahoora, U., 和 Qureshi, A. S. (2020). 深度卷积神经网络的最新架构综述。人工智能评论。
- en: 'Khan et al., 2022 Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S.,
    and Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等，2022 Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., 和 Shah,
    M. (2022). 视觉中的变换器：综述。ACM 计算调查（CSUR）。
- en: Khosla et al., 2020 Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y.,
    Isola, P., Maschinot, A., Liu, C., and Krishnan, D. (2020). Supervised contrastive
    learning. Advances in neural information processing systems.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosla 等，2020 Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola,
    P., Maschinot, A., Liu, C., 和 Krishnan, D. (2020). 监督对比学习。神经信息处理系统进展。
- en: Kiefer and Wolfowitz, 1952 Kiefer, J. and Wolfowitz, J. (1952). Stochastic estimation
    of the maximum of a regression function. The Annals of Mathematical Statistics.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiefer 和 Wolfowitz, 1952 Kiefer, J. 和 Wolfowitz, J. (1952). 回归函数最大值的随机估计。数学统计年鉴。
- en: 'Kingma and Ba, 2014 Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic
    optimization. arXiv:1412.6980.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Ba, 2014 Kingma, D. P. and Ba, J. (2014). Adam：一种随机优化方法。arXiv:1412.6980。
- en: Kipf and Welling, 2016 Kipf, T. N. and Welling, M. (2016). Semi-supervised classification
    with graph convolutional networks. arXiv:1609.02907.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf and Welling, 2016 Kipf, T. N. and Welling, M. (2016). 使用图卷积网络的半监督分类。arXiv:1609.02907。
- en: 'Lewis et al., 2020 Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed,
    A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2020). BART: Denoising Sequence-to-Sequence
    Pre-training for Natural Language Generation, Translation, and Comprehension.
    In Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, pages 7871–7880.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al., 2020 Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed,
    A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2020). BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练。发表于第58届计算语言学协会年会论文集，第7871–7880页。
- en: Lin et al., 2017 Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollár, P.
    (2017). Focal loss for dense object detection. In Int. Conf. on computer vision.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al., 2017 Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollár, P.
    (2017). 用于密集目标检测的焦点损失。发表于国际计算机视觉会议。
- en: Liu et al., 2019a Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and
    Han, J. (2019a). On the variance of the adaptive learning rate and beyond. arXiv:1908.03265.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al., 2019a Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and
    Han, J. (2019a). 自适应学习率的方差及其超越。arXiv:1908.03265。
- en: 'Liu et al., 2023 Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig,
    G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods
    in natural language processing. ACM Computing Surveys.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al., 2023 Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig,
    G. (2023). 预训练、提示和预测：自然语言处理中的提示方法系统调查。ACM计算调查。
- en: Liu et al., 2018 Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R.,
    Kaiser, L., and Shazeer, N. (2018). Generating wikipedia by summarizing long sequences.
    arXiv:1801.10198.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al., 2018 Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R.,
    Kaiser, L., and Shazeer, N. (2018). 通过总结长序列生成维基百科。arXiv:1801.10198。
- en: 'Liu et al., 2019b Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019b). Roberta: A robustly
    optimized bert pretraining approach. arXiv:1907.11692.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al., 2019b Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019b). Roberta：一种强健优化的BERT预训练方法。arXiv:1907.11692。
- en: 'Liu et al., 2021 Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
    S., and Guo, B. (2021). Swin transformer: Hierarchical vision transformer using
    shifted windows. In Int. Conf. on computer vision.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al., 2021 Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
    S., and Guo, B. (2021). Swin Transformer：使用移动窗口的分层视觉变换器。发表于国际计算机视觉会议。
- en: Loshchilov and Hutter, 2017 Loshchilov, I. and Hutter, F. (2017). Decoupled
    weight decay regularization. arXiv:1711.05101.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov and Hutter, 2017 Loshchilov, I. and Hutter, F. (2017). 解耦权重衰减正则化。arXiv:1711.05101。
- en: Mescheder et al., 2018 Mescheder, L., Geiger, A., and Nowozin, S. (2018). Which
    training methods for GANs do actually converge? In Int. Conf. on machine learning.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mescheder et al., 2018 Mescheder, L., Geiger, A., and Nowozin, S. (2018). 哪些GAN训练方法实际上会收敛？发表于国际机器学习会议。
- en: 'Min et al., 2021 Min, B., Ross, H., Sulem, E., Veyseh, A. P. B., Nguyen, T. H.,
    Sainz, O., Agirre, E., Heinz, I., and Roth, D. (2021). Recent advances in natural
    language processing via large pre-trained language models: A survey. arXiv preprint
    arXiv:2111.01243.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al., 2021 Min, B., Ross, H., Sulem, E., Veyseh, A. P. B., Nguyen, T.
    H., Sainz, O., Agirre, E., Heinz, I., and Roth, D. (2021). 通过大型预训练语言模型的自然语言处理的最新进展：一项调查。arXiv预印本
    arXiv:2111.01243。
- en: 'Misra, 2019 Misra, D. (2019). Mish: A self regularized non-monotonic activation
    function. arXiv:1908.08681.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Misra, 2019 Misra, D. (2019). Mish: 一种自我正则化的非单调激活函数。arXiv:1908.08681。'
- en: Mnih et al., 2016 Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
    T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016). Asynchronous methods for
    deep reinforcement learning. In Int. Conf. on machine learning.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih et al., 2016 Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
    T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016). 深度强化学习的异步方法。发表于国际机器学习会议。
- en: Moradi et al., 2020 Moradi, R., Berangi, R., and Minaei, B. (2020). A survey
    of regularization strategies for deep models. Artificial Intelligence Review.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moradi et al., 2020 Moradi, R., Berangi, R., and Minaei, B. (2020). 深度模型的正则化策略综述。人工智能评论。
- en: Nair and Hinton, 2010 Nair, V. and Hinton, G. E. (2010). Rectified linear units
    improve restricted boltzmann machines. In Int. Conf. on machine learning (ICML-).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair and Hinton, 2010 Nair, V. and Hinton, G. E. (2010). 经过修正的线性单元改进了限制玻尔兹曼机。发表于国际机器学习会议（ICML-）。
- en: 'OpenAI, 2022 OpenAI (2022). Chatgpt: Optimizing language models for dialogue.
    https://openai.com/blog/chatgpt/.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI, 2022 OpenAI (2022). Chatgpt: 优化对话的语言模型。https://openai.com/blog/chatgpt/。'
- en: OpenAI, 2023 OpenAI (2023). Gpt-4 technical report.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI, 2023 OpenAI (2023). GPT-4 技术报告。
- en: Ouyang et al., 2022 Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022).
    Training language models to follow instructions with human feedback. arXiv:2203.02155.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等, 2022 Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L.,
    Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., 等. (2022). 通过人类反馈训练语言模型以遵循指令。arXiv:2203.02155。
- en: Radford et al., 2018 Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,
    et al. (2018). Improving language understanding by generative pre-training.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等, 2018 Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., 等.
    (2018). 通过生成预训练改进语言理解。
- en: Radford et al., 2019 Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever,
    I., et al. (2019). Language models are unsupervised multitask learners. OpenAI
    blog.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等, 2019 Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever,
    I., 等. (2019). 语言模型是无监督多任务学习者。OpenAI 博客。
- en: Raffel et al., 2020 Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
    Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer
    learning with a unified text-to-text transformer. The Journal of Machine Learning
    Research.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等, 2020 Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., 和 Liu, P. J. (2020). 通过统一的文本到文本变换器探索迁移学习的极限。机器学习研究杂志。
- en: Ramachandran et al., 2017 Ramachandran, P., Zoph, B., and Le, Q. V. (2017).
    Searching for activation functions. arXiv preprint arXiv:1710.05941.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandran 等, 2017 Ramachandran, P., Zoph, B., 和 Le, Q. V. (2017). 搜索激活函数。arXiv
    预印本 arXiv:1710.05941。
- en: Reddi et al., 2019 Reddi, S. J., Kale, S., and Kumar, S. (2019). On the convergence
    of adam and beyond. arXiv:1904.09237.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reddi 等, 2019 Reddi, S. J., Kale, S., 和 Kumar, S. (2019). 关于 Adam 的收敛性及其超越。arXiv:1904.09237。
- en: 'Sandler et al., 2018 Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen,
    L.-C. (2018). Mobilenetv2: Inverted residuals and linear bottlenecks. In Conf.
    on computer vision and pattern recognition.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sandler 等, 2018 Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., 和 Chen, L.-C.
    (2018). Mobilenetv2: 反向残差和线性瓶颈。在计算机视觉与模式识别会议上。'
- en: 'Sanh et al., 2019 Sanh, V., Debut, L., Chaumond, J., and Wolf, T. (2019). DistilBERT,
    a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv:1910.01108.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sanh 等, 2019 Sanh, V., Debut, L., Chaumond, J., 和 Wolf, T. (2019). DistilBERT,
    BERT 的蒸馏版本: 更小、更快、更便宜、更轻便。arXiv:1910.01108。'
- en: Schultz and Joachims, 2003 Schultz, M. and Joachims, T. (2003). Learning a distance
    metric from relative comparisons. Advances in neural information processing systems,
    16.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schultz 和 Joachims, 2003 Schultz, M. 和 Joachims, T. (2003). 从相对比较中学习距离度量。神经信息处理系统进展,
    16。
- en: Shao et al., 2020 Shao, J., Hu, K., Wang, C., Xue, X., and Raj, B. (2020). Is
    normalization indispensable for training deep neural network? Advances in Neural
    Information Processing Systems.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等, 2020 Shao, J., Hu, K., Wang, C., Xue, X., 和 Raj, B. (2020). 规范化是否对训练深度神经网络不可或缺？神经信息处理系统进展。
- en: 'Shazeer and Stern, 2018 Shazeer, N. and Stern, M. (2018). Adafactor: Adaptive
    learning rates with sublinear memory cost. In Int. Conf. on Machine Learning.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shazeer 和 Stern, 2018 Shazeer, N. 和 Stern, M. (2018). Adafactor: 具有亚线性内存成本的自适应学习率。国际机器学习会议。'
- en: Shrestha and Mahmood, 2019 Shrestha, A. and Mahmood, A. (2019). Review of deep
    learning algorithms and architectures. IEEE access.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shrestha 和 Mahmood, 2019 Shrestha, A. 和 Mahmood, A. (2019). 深度学习算法和架构的综述。IEEE
    access。
- en: 'Sohn et al., 2020 Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H.,
    Raffel, C. A., Cubuk, E. D., Kurakin, A., and Li, C.-L. (2020). Fixmatch: Simplifying
    semi-supervised learning with consistency and confidence. Advances in neural information
    processing systems.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sohn 等, 2020 Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel,
    C. A., Cubuk, E. D., Kurakin, A., 和 Li, C.-L. (2020). Fixmatch: 通过一致性和自信简化半监督学习。神经信息处理系统进展。'
- en: 'Srivastava et al., 2014 Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever,
    I., and Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks
    from overfitting. The journal of machine learning research.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Srivastava 等, 2014 Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
    和 Salakhutdinov, R. (2014). Dropout: 防止神经网络过拟合的一种简单方法。机器学习研究杂志。'
- en: 'Sun, 2020 Sun, R.-Y. (2020). Optimization for deep learning: An overview. Operations
    Research Society of China.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun, 2020 Sun, R.-Y. (2020). 深度学习优化概述。中国运筹学会。
- en: 'Sun et al., 2019 Sun, Z., Deng, Z.-H., Nie, J.-Y., and Tang, J. (2019). Rotate:
    Knowledge graph embedding by relational rotation in complex space. arXiv:1902.10197.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun等，2019 Sun, Z., Deng, Z.-H., Nie, J.-Y., 和 Tang, J. (2019). Rotate: 通过复数空间中的关系旋转进行知识图谱嵌入。arXiv:1902.10197。'
- en: Touvron et al., 2021 Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G.,
    and Jégou, H. (2021). Going deeper with image transformers. In Int. Conf. on Computer
    Vision.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等，2021 Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., 和 Jégou,
    H. (2021). 使用图像变换器深入探索。发表于国际计算机视觉会议。
- en: 'Ulyanov et al., 2016 Ulyanov, D., Vedaldi, A., and Lempitsky, V. (2016). Instance
    normalization: The missing ingredient for fast stylization. arXiv:1607.08022.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ulyanov等，2016 Ulyanov, D., Vedaldi, A., 和 Lempitsky, V. (2016). 实例归一化：快速风格化的缺失成分。arXiv:1607.08022。
- en: Vaswani et al., 2017 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all you
    need. Advances in neural information processing systems.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等，2017 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
    Gomez, A. N., Kaiser, Ł., 和 Polosukhin, I. (2017). 注意力即你所需的一切。神经信息处理系统进展。
- en: Veličković et al., 2017 Veličković, P., Cucurull, G., Casanova, A., Romero,
    A., Lio, P., and Bengio, Y. (2017). Graph attention networks. arXiv:1710.10903.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veličković等，2017 Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio,
    P., 和 Bengio, Y. (2017). 图注意网络。arXiv:1710.10903。
- en: Wang et al., 2017 Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H.,
    Wang, X., and Tang, X. (2017). Residual attention network for image classification.
    In Conf. on computer vision and pattern recognition.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等，2017 Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang,
    X., 和 Tang, X. (2017). 用于图像分类的残差注意网络。发表于计算机视觉与模式识别会议。
- en: Wang et al., 2020 Wang, Q., Ma, Y., Zhao, K., and Tian, Y. (2020). A comprehensive
    survey of loss functions in machine learning. Annals of Data Science.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等，2020 Wang, Q., Ma, Y., Zhao, K., 和 Tian, Y. (2020). 机器学习中损失函数的全面调查。数据科学年鉴。
- en: Williams and Peng, 1991 Williams, R. J. and Peng, J. (1991). Function optimization
    using connectionist reinforcement learning algorithms. Connection Science.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams和Peng，1991 Williams, R. J. 和 Peng, J. (1991). 使用连接主义强化学习算法进行函数优化。连接科学。
- en: Wu et al., 2020 Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip,
    S. Y. (2020). A comprehensive survey on graph neural networks. Transactions on
    neural networks and learning systems.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等，2020 Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., 和 Philip, S. Y. (2020).
    关于图神经网络的全面调查。神经网络与学习系统汇刊。
- en: Xie et al., 2020 Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. (2020). Self-training
    with noisy student improves imagenet classification. In Conf. on computer vision
    and pattern recognition.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等，2020 Xie, Q., Luong, M.-T., Hovy, E., 和 Le, Q. V. (2020). 带噪声学生的自我训练提升了ImageNet分类性能。发表于计算机视觉与模式识别会议。
- en: Xie et al., 2017 Xie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. (2017).
    Aggregated residual transformations for deep neural networks. In Conf. on computer
    vision and pattern recognition.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等，2017 Xie, S., Girshick, R., Dollár, P., Tu, Z., 和 He, K. (2017). 深度神经网络的聚合残差变换。发表于计算机视觉与模式识别会议。
- en: Yang et al., 2022 Yang, X., Song, Z., King, I., and Xu, Z. (2022). A survey
    on deep semi-supervised learning. Transactions on Knowledge and Data Engineering.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等，2022 Yang, X., Song, Z., King, I., 和 Xu, Z. (2022). 深度半监督学习的调查。知识与数据工程汇刊。
- en: 'Yang et al., 2019 Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov,
    R. R., and Le, Q. V. (2019). Xlnet: Generalized autoregressive pretraining for
    language understanding. Advances in neural information processing systems.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等，2019 Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R.,
    和 Le, Q. V. (2019). XLNet: 用于语言理解的广义自回归预训练。神经信息处理系统进展。'
- en: 'You et al., 2019 You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli,
    S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J. (2019). Large batch optimization
    for deep learning: Training bert in 76 minutes. arXiv:1904.00962.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You等，2019 You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S.,
    Song, X., Demmel, J., Keutzer, K., 和 Hsieh, C.-J. (2019). 深度学习的大批量优化：76分钟内训练BERT。arXiv:1904.00962。
- en: 'Zbontar et al., 2021 Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny,
    S. (2021). Barlow twins: Self-supervised learning via redundancy reduction. In
    Int. Conf. on Machine Learning.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zbontar等，2021 Zbontar, J., Jing, L., Misra, I., LeCun, Y., 和 Deny, S. (2021).
    Barlow twins: 通过冗余减少进行自监督学习。发表于国际机器学习会议。'
- en: Zhu et al., 2017 Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. (2017). Unpaired
    image-to-image translation using cycle-consistent adversarial networks. In Int.
    Conf. on computer vision.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等，2017 Zhu, J.-Y., Park, T., Isola, P., 和 Efros, A. A. (2017). 使用循环一致对抗网络的无配对图像到图像转换。发表于国际计算机视觉会议。
