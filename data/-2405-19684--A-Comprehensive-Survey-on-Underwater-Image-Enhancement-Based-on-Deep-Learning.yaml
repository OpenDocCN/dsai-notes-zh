- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:32:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:32:07'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2405.19684] A Comprehensive Survey on Underwater Image Enhancement Based on
    Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2405.19684] 基于深度学习的水下图像增强综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.19684](https://ar5iv.labs.arxiv.org/html/2405.19684)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.19684](https://ar5iv.labs.arxiv.org/html/2405.19684)
- en: A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的水下图像增强综合调查
- en: 'Xiaofeng Cong, Yu Zhao, Jie Gui, Junming Hou, Dacheng Tao (X. Cong and Y. Zhao
    contributed equally to this work.) (Corresponding author: J. Gui.) X. Cong and
    Y. Zhao are with the School of Cyber Science and Engineering, Southeast University,
    Nanjing 210000, China (e-mail: cxf_svip@163.com, zyzzustc@gmail.com). J. Gui is
    with the School of Cyber Science and Engineering, Southeast University and with
    Purple Mountain Laboratories, Nanjing 210000, China (e-mail: guijie@seu.edu.cn).
    J. Hou is with the State Key Laboratory of Millimeter Waves, School of Information
    Science and Engineering, Southeast University, Nanjing 210096, China (e-mails:
    junming_hou@seu.edu.cn). D. Tao is with the College of Computing and Data Science,
    Nanyang Technological University, Singapore 639798 (e-mail: dacheng.tao@gmail.com).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 宋晓峰、赵宇、桂杰、侯俊明、陶大成（**宋晓峰**和**赵宇**对这项工作贡献相同。）（通讯作者：**桂杰**。）**宋晓峰**和**赵宇**来自中国南京210000，东南大学，网络科学与工程学院（电子邮件：`cxf_svip@163.com`，`zyzzustc@gmail.com`）。**桂杰**在中国南京210000，东南大学网络科学与工程学院和紫金山实验室（电子邮件：`guijie@seu.edu.cn`）。**侯俊明**在中国南京210096，东南大学信息科学与工程学院毫米波国家重点实验室（电子邮件：`junming_hou@seu.edu.cn`）。**陶大成**在新加坡639798，南洋理工大学计算与数据科学学院（电子邮件：`dacheng.tao@gmail.com`）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Underwater image enhancement (UIE) is a challenging research task in the field
    of computer vision. Although hundreds of UIE algorithms have been proposed, a
    comprehensive and systematic review is still lacking. To promote future research,
    we summarize the UIE task from multiple perspectives. First, the physical models,
    data construction processes, evaluation metrics, and loss functions are introduced.
    Second, according to the contributions brought by different literatures, recent
    proposed algorithms are discussed and classified from six perspectives, namely
    network architecture, learning strategy, learning stage, assistance task, domain
    perspective and disentanglement fusion, respectively. Third, considering the inconsistencies
    in experimental settings in different literatures, a comprehensive and fair comparison
    does not yet exist. To this end, we quantitatively and qualitatively evaluate
    state-of-the-art algorithms on multiple benchmark datasets. Finally, issues worthy
    of further research in the UIE task are raised. A collection of useful materials
    is available at https://github.com/YuZhao1999/UIE.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 水下图像增强（UIE）是计算机视觉领域的一个具有挑战性的研究任务。尽管提出了数百种UIE算法，但仍然缺乏全面和系统的综述。为了推动未来的研究，我们从多个角度总结了UIE任务。首先，介绍了物理模型、数据构建过程、评估指标和损失函数。其次，根据不同文献所带来的贡献，讨论并分类了最近提出的算法，从网络结构、学习策略、学习阶段、辅助任务、领域视角和解耦融合六个角度进行讨论。第三，考虑到不同文献中实验设置的不一致，尚不存在全面公正的比较。为此，我们在多个基准数据集上定量和定性地评估了最先进的算法。最后，提出了UIE任务中值得进一步研究的问题。更多有用材料请访问
    [https://github.com/YuZhao1999/UIE](https://github.com/YuZhao1999/UIE)。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Underwater Image Enhancement, Quality Degradation, Color Distortion, Light Attenuation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 水下图像增强、质量退化、颜色失真、光线衰减。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Underwater imaging is an important task in the field of computer vision [[6](#bib.bib6),
    [123](#bib.bib123)]. High-quality underwater images are necessary for underwater
    resource exploration, film shooting, personal entertainment, etc. However, due
    to the absorption and scattering effects of light in underwater scenes, the quality
    of underwater images may be degraded to varying degrees [[17](#bib.bib17)]. As
    shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A Comprehensive Survey on
    Underwater Image Enhancement Based on Deep Learning")-(a), as the depth of water
    increases, the red, orange, yellow and green light components disappear in sequence.
    Meanwhile, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning")-(b), diverse distortions
    may occur in underwater environments. Common types of distortion include
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 水下成像是计算机视觉领域中的一项重要任务 [[6](#bib.bib6)、[123](#bib.bib123)]。高质量的水下图像对于水下资源勘探、电影拍摄、个人娱乐等是必要的。然而，由于光在水下场景中的吸收和散射效应，水下图像的质量可能会不同程度地下降
    [[17](#bib.bib17)]。如图 [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning")-(a) 所示，随着水深的增加，红色、橙色、黄色和绿色光成分依次消失。同时，如图
    [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A Comprehensive Survey on Underwater Image
    Enhancement Based on Deep Learning")-(b) 所示，水下环境中可能出现各种畸变。常见的畸变类型包括
- en: •
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Color distortion and contrast reduction: Due to the different attenuation degrees
    of light with different wavelengths, the color of images tends to be blue-green
    [[33](#bib.bib33)].'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 色彩失真和对比度降低：由于不同波长光的衰减程度不同，图像的颜色往往偏向蓝绿色 [[33](#bib.bib33)]。
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hazy, noisy and blurry effect: Light may be significantly attenuated in water,
    which usually caused by suspended particles or muddy water. The image obtained
    in such environment may appear hazy, noisy or blurry [[124](#bib.bib124)].'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模糊、噪声和模糊效果：光线在水中可能会显著衰减，这通常是由悬浮颗粒或泥泞水造成的。在这种环境中获得的图像可能显得模糊、有噪声或模糊 [[124](#bib.bib124)]。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Low illumination: When the depth of water exceeds a certain value, the environment
    lighting approaches a low-light state, which requires an auxiliary light source.
    [[43](#bib.bib43)].'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 低光照：当水的深度超过一定值时，环境照明接近低光状态，这需要辅助光源。 [[43](#bib.bib43)].
- en: Aiming at improving the quality of degraded underwater images collected in complex
    underwater environments, various underwater images enhancement (UIE) methods have
    been proposed. Existing algorithms can be divided into non-deep learning-based
    UIE (NDL-UIE) and deep learning-based UIE (DL-UIE). Various prior assumptions,
    physical models and non-data-driven classical image processing methods are widely
    utilized by NDL-UIE [[114](#bib.bib114), [24](#bib.bib24), [115](#bib.bib115),
    [159](#bib.bib159), [175](#bib.bib175)]. However, due to the complexity of the
    underwater environment, the strategy adopted by NDL-UIE may be inaccurate in certain
    scenarios. Problems include (i) perfect physical modeling of underwater scenarios
    does not exist, (ii) inherent errors exist in the estimation of physical parameters,
    (iii) specific prior assumptions may not applicable in every scene, and (iv) scene-specific
    representations not held by classical image processing methods [[144](#bib.bib144),
    [7](#bib.bib7), [109](#bib.bib109), [58](#bib.bib58)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高在复杂水下环境中收集的退化水下图像的质量，已经提出了各种水下图像增强（UIE）方法。现有的算法可以分为非深度学习基础的UIE（NDL-UIE）和深度学习基础的UIE（DL-UIE）。NDL-UIE广泛利用了各种先验假设、物理模型和非数据驱动的经典图像处理方法
    [[114](#bib.bib114)、[24](#bib.bib24)、[115](#bib.bib115)、[159](#bib.bib159)、[175](#bib.bib175)]。然而，由于水下环境的复杂性，NDL-UIE采用的策略在某些情况下可能不准确。问题包括（i）不存在完美的水下场景物理建模，（ii）物理参数估计中存在固有误差，（iii）特定的先验假设可能不适用于每个场景，以及（iv）经典图像处理方法不具备的场景特定表示
    [[144](#bib.bib144)、[7](#bib.bib7)、[109](#bib.bib109)、[58](#bib.bib58)]。
- en: Considering the impressive performance of data-driven algorithms in the field
    of computer vision and image processing, learning-based solutions have received
    more attention from researchers. Aiming at further improving the performance of
    the UIE task, various DL-UIE algorithms have been proposed and verified [[141](#bib.bib141),
    [53](#bib.bib53), [15](#bib.bib15), [30](#bib.bib30), [99](#bib.bib99), [122](#bib.bib122),
    [28](#bib.bib28), [83](#bib.bib83), [74](#bib.bib74), [105](#bib.bib105), [47](#bib.bib47)].
    Focusing on the problems faced by the UIE task, various efforts are expended by
    researchers. To promote future research, we summarize the UIE task from multiple
    perspectives include
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数据驱动算法在计算机视觉和图像处理领域的卓越性能，基于学习的解决方案已受到研究人员的更多关注。旨在进一步提高UIE任务的性能，已提出并验证了各种DL-UIE算法[[141](#bib.bib141),
    [53](#bib.bib53), [15](#bib.bib15), [30](#bib.bib30), [99](#bib.bib99), [122](#bib.bib122),
    [28](#bib.bib28), [83](#bib.bib83), [74](#bib.bib74), [105](#bib.bib105), [47](#bib.bib47)]。针对UIE任务面临的问题，研究人员付出了各种努力。为了促进未来的研究，我们从多个角度总结了UIE任务，包括
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Solutions have been proposed.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已提出了各种解决方案。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The results achieved by existing solutions.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有解决方案所取得的结果。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Issues worthy of further exploration.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 值得进一步探索的问题。
- en: Solutions have been proposed. On the one hand, we summarize the physical models,
    data generation methods, and evaluation metrics which constructed to serve as
    the basis for research. On the other hand, numerous network structures and training
    strategies are classified and discussed. In order to know which methods have been
    widely explored, we need to understand the similarities and differences between
    them. Within our perspective, existing research can be subjectively viewed as
    being carried out from six different aspects, namely network architecture, learning
    strategy, learning stage, assistance task, domain perspective and disentanglement
    $\&amp;$ fusion. To facilitate future research that can easily build on existing
    work. According to the main contributions of different algorithms, we divide the
    existing work into 6 first-level categories. Then, for each first-level category,
    we give the corresponding second-level category. The taxonomy of algorithms is
    shown in Table [I](#S1.T1 "TABLE I ‣ I-B A guide for reading this survey ‣ I Introduction
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning").
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出了各种解决方案。一方面，我们总结了构建研究基础的物理模型、数据生成方法和评估指标。另一方面，分类并讨论了许多网络结构和训练策略。为了了解哪些方法已被广泛探索，我们需要了解它们之间的相似性和差异。从我们的角度来看，现有研究可以主观地视为从六个不同方面进行的，即网络架构、学习策略、学习阶段、辅助任务、领域视角和解缠结与融合。为了促进未来研究能够轻松地在现有工作基础上进行。根据不同算法的主要贡献，我们将现有工作分为6个一级类别。然后，对于每个一级类别，我们给出相应的二级类别。算法的分类见表[Ⅰ](#S1.T1
    "TABLE I ‣ I-B A guide for reading this survey ‣ I Introduction ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning")。
- en: The results achieved by existing solutions. In existing papers, state-of-the-art
    (SOTA) algorithms have been verified on benchmark datasets. Meanwhile, their performance
    have been compared with other algorithms. However, the experimental settings adopted
    in different papers may be inconsistent. A comprehensive and fair comparative
    experiment for the UIE task has not yet been conducted. Here, we provide a fair
    comparison by unifying various experimental settings with the aim of obtaining
    answers on the following two questions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现有解决方案所取得的结果。在现有论文中，最先进（SOTA）的算法已在基准数据集上进行了验证。同时，它们的性能也与其他算法进行了比较。然而，不同论文采用的实验设置可能存在不一致之处。尚未进行针对UIE任务的全面且公平的比较实验。在这里，我们通过统一各种实验设置，提供一个公平的比较，目的是回答以下两个问题。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Under fair settings, how do SOTA algorithms perform?
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在公平设置下，SOTA算法表现如何？
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Which algorithms achieves the best performance?
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 哪些算法取得了最佳性能？
- en: Issues worthy of further exploration. Based on our systematic review of existing
    algorithms and comprehensive experimental evaluation, we further discuss issues
    worthy of future research. Specifically, high-quality data synthesis, cooperation
    with text-image multi-modal models, non-uniform illumination, reliable evaluation
    metrics, and combination with other image restoration tasks are topics that still
    need to be studied in depth. In a word, the UIE is in an emerging stage, rather
    than a task that has been almost perfectly solved.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进一步探索的问题。基于我们对现有算法的系统性回顾和全面的实验评估，我们进一步讨论了值得未来研究的问题。具体来说，高质量的数据合成、与文本-图像多模态模型的合作、不均匀照明、可靠的评估指标以及与其他图像恢复任务的结合是仍需深入研究的主题。总之，UIE仍处于一个新兴阶段，而不是一个几乎已解决的任务。
- en: '![Refer to caption](img/3f670b4d3f1f0a1b2a5d8e272c8cbdbe.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3f670b4d3f1f0a1b2a5d8e272c8cbdbe.png)'
- en: (a) Underwater distortion [[157](#bib.bib157), [170](#bib.bib170), [128](#bib.bib128)]
                                                (b) Various distortions
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 水下失真 [[157](#bib.bib157), [170](#bib.bib170), [128](#bib.bib128)]                                            
    (b) 各种失真
- en: 'Figure 1: Underwater degradation.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：水下退化。
- en: I-A The difference between this survey and others
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 本调查与其他调查的区别
- en: The [[157](#bib.bib157), [144](#bib.bib144), [109](#bib.bib109), [58](#bib.bib58),
    [128](#bib.bib128)] mainly introduce the traditional UIE algorithms, while the
    latest DL-UIE algorithms are rarely mentioned. [[7](#bib.bib7)] classifies DL
    methods from the perspective of network architecture. But the latest progress
    in the research of the UIE task such as Fourier operation [[131](#bib.bib131)],
    contrastive learning [[86](#bib.bib86)], and rank learning [[37](#bib.bib37)]
    are not mentioned. The perspective of [[170](#bib.bib170)]’s research is the difference
    between hardware-based and software-based algorithms, so a comprehensive discussion
    of DL-based algorithms is not the goal of [[170](#bib.bib170)]. In this survey,
    we provide a comprehensive discussion of recent DL-based advancements.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[[157](#bib.bib157), [144](#bib.bib144), [109](#bib.bib109), [58](#bib.bib58),
    [128](#bib.bib128)]主要介绍传统的UIE算法，而最新的DL-UIE算法鲜有提及。[[7](#bib.bib7)]从网络架构的角度分类DL方法。但如傅里叶操作[[131](#bib.bib131)]、对比学习[[86](#bib.bib86)]和排名学习[[37](#bib.bib37)]等UIE任务的最新研究进展未被提及。[[170](#bib.bib170)]的研究视角是基于硬件和软件的算法之间的差异，因此[[170](#bib.bib170)]的目标并非全面讨论DL基础的算法。在本调查中，我们对近期的DL基础进展进行了全面讨论。'
- en: I-B A guide for reading this survey
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 本调查阅读指南
- en: The terminology in the existing literature may not be consistent. To facilitate
    the reading of this paper, especially the figures, we summarize the important
    terminologies and their meanings as follows.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现有文献中的术语可能不一致。为了方便阅读本文，特别是图形部分，我们总结了重要术语及其含义如下。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Degradation: Quality degradation during underwater imaging due to absorption
    and scattering phenomena.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 退化：由于吸收和散射现象导致的水下成像质量退化。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Distortion $x$: The degraded quality underwater image taken in different types
    of water.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 失真$x$：在不同类型水中拍摄的质量退化水下图像。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reference $y$: The underwater image with subjectively higher quality. There
    is no such thing as a perfect, distortion-free underwater image.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参考$y$：主观上质量更高的水下图像。不存在完美的、无失真的水下图像。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prediction $\hat{y}$: A visual quality enhanced underwater image obtained by
    an UIE algorithm.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测$\hat{y}$：由UIE算法获得的视觉质量增强的水下图像。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Paired images: A distortion image $x$ with the corresponding reference image
    $y$.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配对图像：带有失真图像$x$及其对应参考图像$y$。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'In-air images: Images taken in a terrestrial scene, which may be indoors or
    outdoors.'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空气中的图像：在地面场景中拍摄的图像，可以是室内或室外。
- en: The terminology used for the task of improving underwater image quality has
    certain differences in existing literatures, which includes underwater image enhancement,
    underwater image restoration and underwater image dehazing. Since certain similarities
    exist between the task we studied and the image dehazing task, such as similar
    imaging principles [[11](#bib.bib11)], some literature uses “underwater image
    dehazing” as a definition. This definition has been used less frequently in recent
    literature [[7](#bib.bib7)]. Furthermore, “enhancement” and “restoration” are
    often used in different literatures. There is no significant difference between
    these two terms in the task of improving underwater image quality. Therefore,
    in this survey, we use “enhancement” uniformly to define the task we discuss.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 提高水下图像质量的任务中使用的术语在现有文献中存在一定差异，包括水下图像增强、水下图像修复和水下图像去雾。由于我们研究的任务与图像去雾任务存在某些相似性，例如相似的成像原理
    [[11](#bib.bib11)]，一些文献将“水下图像去雾”作为定义。然而，近年来这种定义的使用频率较低 [[7](#bib.bib7)]。此外，“增强”和“修复”在不同的文献中经常被使用。在提高水下图像质量的任务中，这两个术语没有显著区别。因此，在本综述中，我们统一使用“增强”来定义我们讨论的任务。
- en: For ease of reading, abbreviations are used to refer to each UIE algorithm mentioned
    in the paper. Many papers provide abbreviations for their algorithms, such as
    URanker [[37](#bib.bib37)]. For papers that did not provide an algorithm abbreviation,
    we constructed the abbreviation using the first letters of the words in the title
    that illustrate their main contributions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于阅读，本文中使用了缩写来指代提到的每个 UIE 算法。许多论文为其算法提供了缩写，例如 URanker [[37](#bib.bib37)]。对于未提供算法缩写的论文，我们使用标题中说明其主要贡献的单词的首字母构造了缩写。
- en: The commonly used physical models, data generation methods, evaluation metrics
    and loss functions are summarized in Section [II](#S2 "II Related Work ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning"). The taxonomy
    and analysis of existing algorithms are placed in Section [III](#S3 "III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning").
    Section [IV](#S4 "IV Experiments ‣ A Comprehensive Survey on Underwater Image
    Enhancement Based on Deep Learning") provides comprehensive experiments and summarizes
    the conclusions. Challenging and valuable issues that have not yet been solved
    are discussed in Section [V](#S5 "V Future Work ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning"). A summary of this paper is in Section
    [VI](#S6 "VI Conclusion ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning").
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的物理模型、数据生成方法、评估指标和损失函数在第 [II](#S2 "II Related Work ‣ A Comprehensive Survey
    on Underwater Image Enhancement Based on Deep Learning") 节中进行了总结。现有算法的分类和分析放在第
    [III](#S3 "III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning") 节中。第 [IV](#S4 "IV Experiments ‣ A Comprehensive Survey
    on Underwater Image Enhancement Based on Deep Learning") 节提供了全面的实验和总结结论。尚未解决的具有挑战性和有价值的问题在第
    [V](#S5 "V Future Work ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning") 节中进行了讨论。本文的总结在第 [VI](#S6 "VI Conclusion ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning") 节中给出。
- en: 'TABLE I: A taxonomy of UIE algorithms.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: UIE 算法的分类。'
- en: '| Category | Key Idea | Methods |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 关键思想 | 方法 |'
- en: '| --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Network Architecture | Convolution Operation |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 网络架构 | 卷积操作 |'
- en: '&#124; UWCNN [[72](#bib.bib72)], UWNet [[99](#bib.bib99)], UResNet [[83](#bib.bib83)],
    DUIR [[25](#bib.bib25)], &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UWCNN [[72](#bib.bib72)], UWNet [[99](#bib.bib99)], UResNet [[83](#bib.bib83)],
    DUIR [[25](#bib.bib25)], &#124;'
- en: '&#124; UIR-Net [[95](#bib.bib95)], FloodNet [[32](#bib.bib32)], LAFFNet [[143](#bib.bib143)],
    UICoE-Net [[106](#bib.bib106)], PUIE-Net [[31](#bib.bib31)] &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UIR-Net [[95](#bib.bib95)], FloodNet [[32](#bib.bib32)], LAFFNet [[143](#bib.bib143)],
    UICoE-Net [[106](#bib.bib106)], PUIE-Net [[31](#bib.bib31)] &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Attention Mechanism |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 注意力机制 |'
- en: '&#124; WaveNet [[111](#bib.bib111)], ADMNNet [[141](#bib.bib141)], LightEnhanceNet
    [[156](#bib.bib156)], &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WaveNet [[111](#bib.bib111)], ADMNNet [[141](#bib.bib141)], LightEnhanceNet
    [[156](#bib.bib156)], &#124;'
- en: '&#124; CNMS [[152](#bib.bib152)], HAAM-GAN [[151](#bib.bib151)], MFEF [[169](#bib.bib169)]
    &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNMS [[152](#bib.bib152)], HAAM-GAN [[151](#bib.bib151)], MFEF [[169](#bib.bib169)]
    &#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Transformer Module |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Transformer 模块 |'
- en: '&#124; PTT [[10](#bib.bib10)], U-Trans [[102](#bib.bib102)], UWAGA [[48](#bib.bib48)],
    WaterFormer [[132](#bib.bib132)], Spectroformer [[66](#bib.bib66)] &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PTT [[10](#bib.bib10)], U-Trans [[102](#bib.bib102)], UWAGA [[48](#bib.bib48)],
    WaterFormer [[132](#bib.bib132)], Spectroformer [[66](#bib.bib66)] &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Fourier Transformation |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 傅里叶变换 |'
- en: '&#124; UHD [[131](#bib.bib131)], WFI2-Net [[161](#bib.bib161)], TANet [[150](#bib.bib150)],
    UIE-INN [[18](#bib.bib18)], SFGNet [[162](#bib.bib162)] &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UHD [[131](#bib.bib131)], WFI2-Net [[161](#bib.bib161)], TANet [[150](#bib.bib150)],
    UIE-INN [[18](#bib.bib18)], SFGNet [[162](#bib.bib162)] &#124;'
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Wavelet Decomposition |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 小波分解 |'
- en: '&#124; UIE-WD [[94](#bib.bib94)], EUIE [[56](#bib.bib56)], PRWNet [[49](#bib.bib49)],
    MWEN [[127](#bib.bib127)] &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UIE-WD [[94](#bib.bib94)], EUIE [[56](#bib.bib56)], PRWNet [[49](#bib.bib49)],
    MWEN [[127](#bib.bib127)] &#124;'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Neural Architecture Search |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 神经架构搜索 |'
- en: '&#124; AutoEnhancer [[116](#bib.bib116)] &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AutoEnhancer [[116](#bib.bib116)] &#124;'
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Learning Strategy | Adversarial Learning |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 学习策略 | 对抗学习 |'
- en: '&#124; DGD-cGAN [[34](#bib.bib34)], FGAN [[76](#bib.bib76)], UIE-cGAN [[145](#bib.bib145)],
    TOPAL [[63](#bib.bib63)], &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DGD-cGAN [[34](#bib.bib34)], FGAN [[76](#bib.bib76)], UIE-cGAN [[145](#bib.bib145)],
    TOPAL [[63](#bib.bib63)], &#124;'
- en: '&#124; FUnIEGAN [[53](#bib.bib53)], EUIGAN [[28](#bib.bib28)], CE-CGAN [[1](#bib.bib1)],
    RUIG [[22](#bib.bib22)] &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FUnIEGAN [[53](#bib.bib53)], EUIGAN [[28](#bib.bib28)], CE-CGAN [[1](#bib.bib1)],
    RUIG [[22](#bib.bib22)] &#124;'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Rank Learning |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 排名学习 |'
- en: '&#124; URanker [[37](#bib.bib37)], PDD-Net [[61](#bib.bib61)], CLUIE-Net [[79](#bib.bib79)]
    &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; URanker [[37](#bib.bib37)], PDD-Net [[61](#bib.bib61)], CLUIE-Net [[79](#bib.bib79)]
    &#124;'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Contrastive Learning |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 对比学习 |'
- en: '&#124; TACL [[86](#bib.bib86)], Semi-UIR [[47](#bib.bib47)], CWR [[40](#bib.bib40)],
    DRDCL [[148](#bib.bib148)], TFUIE [[149](#bib.bib149)], &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TACL [[86](#bib.bib86)], Semi-UIR [[47](#bib.bib47)], CWR [[40](#bib.bib40)],
    DRDCL [[148](#bib.bib148)], TFUIE [[149](#bib.bib149)], &#124;'
- en: '&#124; RUIESR [[80](#bib.bib80)], CL-UIE [[118](#bib.bib118)], HCLR-Net [[168](#bib.bib168)],
    UIE-CWR [[39](#bib.bib39)] &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RUIESR [[80](#bib.bib80)], CL-UIE [[118](#bib.bib118)], HCLR-Net [[168](#bib.bib168)],
    UIE-CWR [[39](#bib.bib39)] &#124;'
- en: '|'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Reinforcement Learning |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 强化学习 |'
- en: '&#124; HPUIE-RL [[113](#bib.bib113)] &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HPUIE-RL [[113](#bib.bib113)] &#124;'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Learning Stage | Single Stage | UWCNN [[72](#bib.bib72)], UWNet [[99](#bib.bib99)],
    UResNet [[83](#bib.bib83)] |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 学习阶段 | 单阶段 | UWCNN [[72](#bib.bib72)], UWNet [[99](#bib.bib99)], UResNet
    [[83](#bib.bib83)] |'
- en: '| Coarse-to-fine |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 粗到细 |'
- en: '&#124; MBANet [[138](#bib.bib138)], GSL [[81](#bib.bib81)], CUIE [[67](#bib.bib67)],
    DMML [[27](#bib.bib27)] &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MBANet [[138](#bib.bib138)], GSL [[81](#bib.bib81)], CUIE [[67](#bib.bib67)],
    DMML [[27](#bib.bib27)] &#124;'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Diffusion Learning |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 扩散学习 |'
- en: '&#124; UIE-DM [[117](#bib.bib117)], SU-DDPM [[92](#bib.bib92)], CPDM [[112](#bib.bib112)]
    &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UIE-DM [[117](#bib.bib117)], SU-DDPM [[92](#bib.bib92)], CPDM [[112](#bib.bib112)]
    &#124;'
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Assistance Task | Semantic Assistance | SGUIE [[105](#bib.bib105)], SATS
    [[122](#bib.bib122)], DAL-UIE [[64](#bib.bib64)], WaterFlow [[160](#bib.bib160)],
    HDGAN [[13](#bib.bib13)] |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 辅助任务 | 语义辅助 | SGUIE [[105](#bib.bib105)], SATS [[122](#bib.bib122)], DAL-UIE
    [[64](#bib.bib64)], WaterFlow [[160](#bib.bib160)], HDGAN [[13](#bib.bib13)] |'
- en: '| Depth Assistance |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 深度辅助 |'
- en: '&#124; Joint-ID [[142](#bib.bib142)], DAUT [[8](#bib.bib8)], DepthCue [[20](#bib.bib20)],
    HybrUR [[140](#bib.bib140)] &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Joint-ID [[142](#bib.bib142)], DAUT [[8](#bib.bib8)], DepthCue [[20](#bib.bib20)],
    HybrUR [[140](#bib.bib140)] &#124;'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Domain Perspective | Knowledge Transfer |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 域视角 | 知识迁移 |'
- en: '&#124; TUDA [[130](#bib.bib130)], TSDA [[62](#bib.bib62)], DAAL [[171](#bib.bib171)],
    IUIE [[9](#bib.bib9)], &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TUDA [[130](#bib.bib130)], TSDA [[62](#bib.bib62)], DAAL [[171](#bib.bib171)],
    IUIE [[9](#bib.bib9)], &#124;'
- en: '&#124; WSDS-GAN [[84](#bib.bib84)], TRUDGCR [[55](#bib.bib55)], UW-CycleGAN
    [[139](#bib.bib139)] &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WSDS-GAN [[84](#bib.bib84)], TRUDGCR [[55](#bib.bib55)], UW-CycleGAN
    [[139](#bib.bib139)] &#124;'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Domain Translation |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 域翻译 |'
- en: '&#124; URD-UIE [[173](#bib.bib173)], TACL [[86](#bib.bib86)] &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; URD-UIE [[173](#bib.bib173)], TACL [[86](#bib.bib86)] &#124;'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Diversified Output |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 多样化输出 |'
- en: '&#124; UIESS [[16](#bib.bib16)], UMRD [[174](#bib.bib174)], PWAE [[69](#bib.bib69)],
    CECF [[19](#bib.bib19)] &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UIESS [[16](#bib.bib16)], UMRD [[174](#bib.bib174)], PWAE [[69](#bib.bib69)],
    CECF [[19](#bib.bib19)] &#124;'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Disentanglement $\&amp;$ Fusion | Physical Embedding |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 解耦 $\&amp;$ 融合 | 物理嵌入 |'
- en: '&#124; IPMGAN [[87](#bib.bib87)], ACPAB [[82](#bib.bib82)], USUIR [[29](#bib.bib29)],
    AquaGAN [[21](#bib.bib21)], PhysicalNN [[15](#bib.bib15)], GUPDM [[98](#bib.bib98)]
    &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; IPMGAN [[87](#bib.bib87)], ACPAB [[82](#bib.bib82)], USUIR [[29](#bib.bib29)],
    AquaGAN [[21](#bib.bib21)], PhysicalNN [[15](#bib.bib15)], GUPDM [[98](#bib.bib98)]
    &#124;'
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Retinex Model |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Retinex模型 |'
- en: '&#124; CCMSR-Net [[104](#bib.bib104)], ReX-Net [[153](#bib.bib153)], ASSU-Net
    [[65](#bib.bib65)] &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CCMSR-Net [[104](#bib.bib104)], ReX-Net [[153](#bib.bib153)], ASSU-Net
    [[65](#bib.bib65)] &#124;'
- en: '|'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Color Space Fusion |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 色彩空间融合 |'
- en: '&#124; UIEC²-Net [[125](#bib.bib125)], UGIF-Net [[166](#bib.bib166)], TCTL-Net
    [[78](#bib.bib78)], MTNet [[97](#bib.bib97)], UDNet [[110](#bib.bib110)], &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UIEC²-Net [[125](#bib.bib125)], UGIF-Net [[166](#bib.bib166)], TCTL-Net
    [[78](#bib.bib78)], MTNet [[97](#bib.bib97)], UDNet [[110](#bib.bib110)], &#124;'
- en: '&#124; P2CNet [[108](#bib.bib108)], JLCL-Net [[137](#bib.bib137)], TBDNN [[45](#bib.bib45)],
    MSTAF [[57](#bib.bib57)], UColor [[71](#bib.bib71)] &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; P2CNet [[108](#bib.bib108)], JLCL-Net [[137](#bib.bib137)], TBDNN [[45](#bib.bib45)],
    MSTAF [[57](#bib.bib57)], UColor [[71](#bib.bib71)] &#124;'
- en: '|'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Water Type Focus |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 水体类型聚焦 |'
- en: '&#124; SCNet [[30](#bib.bib30)], DAL [[119](#bib.bib119)], IACC [[165](#bib.bib165)]
    &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SCNet [[30](#bib.bib30)], DAL [[119](#bib.bib119)], IACC [[165](#bib.bib165)]
    &#124;'
- en: '|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Multi-input Fusion |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 多输入融合 |'
- en: '&#124; WaterNet [[74](#bib.bib74)], MFEF [[169](#bib.bib169)], F2UIE [[121](#bib.bib121)]
    &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WaterNet [[74](#bib.bib74)], MFEF [[169](#bib.bib169)], F2UIE [[121](#bib.bib121)]
    &#124;'
- en: '|'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: II Related Work
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: In this section, the six aspects involved in the UIE task are introduced. The
    definition of UIE research and the scope of this paper are given in [II-A](#S2.SS1
    "II-A Problem Definition and Research Scope ‣ II Related Work ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning"). Similar problems
    faced by the UIE task and other image restoration tasks are discussed in [II-B](#S2.SS2
    "II-B The connections between the UIE task and other image restoration tasks ‣
    II Related Work ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning"). Physical models that are considered reliable are presented
    in Section [II-C](#S2.SS3 "II-C Physical Models ‣ II Related Work ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning"). Section [II-D](#S2.SS4
    "II-D Datasets for Model Training and Performance Evaluation ‣ II Related Work
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")
    summarizes common data generation and collection methods. The widely used evaluation
    metrics and loss functions are given in Section [II-E](#S2.SS5 "II-E Evaluation
    Metrics ‣ II Related Work ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning") and Section [II-F](#S2.SS6 "II-F Loss Functions ‣ II
    Related Work ‣ A Comprehensive Survey on Underwater Image Enhancement Based on
    Deep Learning"), respectively.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了UIE任务涉及的六个方面。UIE研究的定义和本文的范围在[II-A](#S2.SS1 "II-A 问题定义和研究范围 ‣ II 相关工作 ‣
    基于深度学习的水下图像增强的综合调查")中给出。UIE任务和其他图像恢复任务面临的类似问题在[II-B](#S2.SS2 "II-B UIE任务与其他图像恢复任务的关系
    ‣ II 相关工作 ‣ 基于深度学习的水下图像增强的综合调查")中讨论。被认为可靠的物理模型在[II-C](#S2.SS3 "II-C 物理模型 ‣ II
    相关工作 ‣ 基于深度学习的水下图像增强的综合调查")中介绍。常见的数据生成和收集方法总结在[II-D](#S2.SS4 "II-D 模型训练和性能评估的数据集
    ‣ II 相关工作 ‣ 基于深度学习的水下图像增强的综合调查")中。广泛使用的评估指标和损失函数分别在[II-E](#S2.SS5 "II-E 评估指标 ‣
    II 相关工作 ‣ 基于深度学习的水下图像增强的综合调查")和[II-F](#S2.SS6 "II-F 损失函数 ‣ II 相关工作 ‣ 基于深度学习的水下图像增强的综合调查")中给出。
- en: II-A Problem Definition and Research Scope
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 问题定义和研究范围
- en: Following the way shown in [[73](#bib.bib73)], we give a commonly used definition
    of the DL-UIE. For a given distorted underwater image $x$, the enhancement process
    can be regarded as
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[73](#bib.bib73)]中所示的方法，我们给出了DL-UIE的一个常用定义。对于给定的失真水下图像$x$，增强过程可以视为
- en: '|  | $\hat{y}=\Phi(x;\theta),$ |  | (1) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}=\Phi(x;\theta),$ |  | (1) |'
- en: where $\Phi$ denotes the neural network with the learnable parameters $\theta$.
    Both $x$ and $\hat{y}$ belong to $\mathcal{R}^{H\times W\times 3}$. For UIE models,
    a common optimization purpose is to minimize the error
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Phi$表示具有可学习参数$\theta$的神经网络。$x$和$\hat{y}$都属于$\mathcal{R}^{H\times W\times
    3}$。对于UIE模型，一个常见的优化目标是最小化误差
- en: '|  | $\hat{\theta}=\arg\min\mathcal{L}(\hat{y},y),$ |  | (2) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\theta}=\arg\min\mathcal{L}(\hat{y},y),$ |  | (2) |'
- en: where $\mathcal{L}(\hat{y},y)$ denote the loss function for obtaining the optimal
    parameters $\hat{\theta}$. $\mathcal{L}(\hat{y},y)$ may be supervised or unsupervised
    loss functions with any form.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{L}(\hat{y},y)$表示用于获取最优参数$\hat{\theta}$的损失函数。$\mathcal{L}(\hat{y},y)$可以是任何形式的监督或无监督损失函数。
- en: Based on the above definition, it is worth clearly pointing out that our survey
    is aimed at the investigation of DL-UIE algorithms with single-frame underwater
    images as input. Therefore, algorithms based on multi-frame images [[135](#bib.bib135)]
    or that do not utilize deep learning at all [[75](#bib.bib75), [103](#bib.bib103),
    [11](#bib.bib11)] are outside the scope of this paper.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述定义，值得明确指出的是，我们的调查旨在研究以单帧水下图像为输入的DL-UIE算法。因此，基于多帧图像的算法[[135](#bib.bib135)]或完全不使用深度学习的算法[[75](#bib.bib75),
    [103](#bib.bib103), [11](#bib.bib11)]不在本文范围之内。
- en: II-B The connections between the UIE task and other image restoration tasks
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B UIE任务与其他图像恢复任务之间的联系
- en: The UIE task is regarded as a sub-research field of low-level image restoration
    tasks. Similar problems arise in the UIE task and other image restoration tasks.
    The common faced issues include, (i) hazy effect caused by absorption and scattering,
    like image dehazing [[41](#bib.bib41)], (ii) blurry details caused by camera shake,
    light scattering, or fast target motion, like deblurring [[155](#bib.bib155)],
    (iii) noise caused by suspended particles [[58](#bib.bib58), [59](#bib.bib59)],
    like image denoising [[12](#bib.bib12)], (iv) low-illumination due to poor light,
    like low-light image enhancement [[136](#bib.bib136)], (v) non-uniform illumination
    caused by artificial light sources, like nighttime dehazing [[89](#bib.bib89)].
    In general, the problems faced by underwater imaging may be regarded as a complex
    combination of multiple image restoration tasks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: UIE任务被视为低级图像恢复任务的一个子研究领域。在UIE任务和其他图像恢复任务中出现了类似的问题。常见的问题包括：(i) 由吸收和散射引起的雾霾效应，如图像去雾[[41](#bib.bib41)]，(ii)
    由相机抖动、光散射或快速目标运动引起的模糊细节，如去模糊[[155](#bib.bib155)]，(iii) 由悬浮颗粒引起的噪声[[58](#bib.bib58),
    [59](#bib.bib59)]，如图像去噪[[12](#bib.bib12)]，(iv) 由于光线不足引起的低光照，如低光图像增强[[136](#bib.bib136)]，(v)
    由于人工光源引起的不均匀光照，如夜间去雾[[89](#bib.bib89)]。总体来说，水下成像面临的问题可以看作是多个图像恢复任务的复杂组合。
- en: II-C Physical Models
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 物理模型
- en: To the best of our knowledge, there is currently no model that perfectly describes
    the underwater imaging process [[7](#bib.bib7)], which is totally suitable for
    algorithmic solution. Here, two widely used and proven effective imaging models
    are introduced, namely the atmospheric scattering model [[23](#bib.bib23)] and
    revised underwater image formation model [[2](#bib.bib2)]. The atmospheric scattering
    model [[7](#bib.bib7)] is widely adopted in the research of image dehazing and
    underwater image enhancement [[147](#bib.bib147)], which can be expressed as
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，目前还没有一个模型能够完美描述水下成像过程[[7](#bib.bib7)]，完全适用于算法解决方案。在这里，介绍了两个广泛使用且已证明有效的成像模型，即大气散射模型[[23](#bib.bib23)]和修订后的水下图像形成模型[[2](#bib.bib2)]。大气散射模型[[7](#bib.bib7)]在图像去雾和水下图像增强研究中被广泛采用，可以表示为
- en: '|  | $x=y\cdot e^{-\beta d}+B^{\infty}\cdot(1-e^{-\beta d}),$ |  | (3) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $x=y\cdot e^{-\beta d}+B^{\infty}\cdot(1-e^{-\beta d}),$ |  | (3) |'
- en: where $d$, $B^{\infty}$ and $\beta$ denote the scene depth, background light
    and attenuation coefficient, respectively. The revised underwater image formation
    model [[2](#bib.bib2)], which is defined as
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d$、$B^{\infty}$ 和 $\beta$ 分别表示场景深度、背景光和衰减系数。修订后的水下图像形成模型[[2](#bib.bib2)]定义为
- en: '|  | $x=ye^{-\beta^{D}(v_{D})d}+B^{\infty}\left(1-e^{\beta^{B}(v_{B})d}\right),$
    |  | (4) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $x=ye^{-\beta^{D}(v_{D})d}+B^{\infty}\left(1-e^{\beta^{B}(v_{B})d}\right),$
    |  | (4) |'
- en: where $\beta^{B}$ and $\beta^{D}$ represent the backscatter and direct transmission
    attenuation coefficients [[7](#bib.bib7)]. The $v_{B}$ and $v_{D}$ are the dependent
    coefficients for $\beta^{B}$ and $\beta^{D}$. The atmospheric scattering model
    and revised underwater image formation model can be used for synthesize of data
    and design of algorithms. Since an in-depth discussion of the imaging mechanism
    may be beyond the scope of this paper, more details on the physical model can
    be found in [[2](#bib.bib2), [4](#bib.bib4), [3](#bib.bib3)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\beta^{B}$ 和 $\beta^{D}$ 表示背向散射和直接传输衰减系数[[7](#bib.bib7)]。$v_{B}$ 和 $v_{D}$
    是 $\beta^{B}$ 和 $\beta^{D}$ 的依赖系数。大气散射模型和修订后的水下图像形成模型可用于数据的合成和算法的设计。由于对成像机制的深入讨论可能超出了本文的范围，更多关于物理模型的细节可以在[[2](#bib.bib2),
    [4](#bib.bib4), [3](#bib.bib3)]中找到。
- en: II-D Datasets for Model Training and Performance Evaluation
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 模型训练和性能评估的数据集
- en: Pairs of real-world distorted underwater images and reference images are difficult
    to obtain [[38](#bib.bib38), [76](#bib.bib76)]. The data used for the training
    and evaluation processes need to be collected by various reliable ways. We group
    existing ways of building datasets into the following categories.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中畸变的水下图像和参考图像难以获取[[38](#bib.bib38), [76](#bib.bib76)]。用于训练和评估过程的数据需要通过各种可靠的方式收集。我们将现有的数据集构建方式分为以下几类。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Voted by SOTA UIE algorithms. 12 algorithms are used by UIEB [[74](#bib.bib74)]
    to generate diverse reference images. The best enhancement effect for each scene
    is then voted on by 50 volunteers as the final reference image. Meanwhile, 18
    UIE algorithms and 20 volunteers are involved in the voting process of LSUI’s
    [[102](#bib.bib102)] reference images. SAUD [[60](#bib.bib60)] proposes a subjectively
    annotated benchmark that contains both real-world raw underwater images and available
    ranking scores under 10 different UIE algorithms.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由 SOTA UIE 算法投票。 UIEB [[74](#bib.bib74)] 使用 12 种算法生成多样的参考图像。然后，50 名志愿者对每个场景的最佳增强效果进行投票，作为最终参考图像。同时，18
    种 UIE 算法和 20 名志愿者参与 LSUI [[102](#bib.bib102)] 参考图像的投票过程。 SAUD [[60](#bib.bib60)]
    提出了一个主观注释基准，包含真实世界的原始水下图像和 10 种不同 UIE 算法下的可用排名分数。
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generated by domain transformation algorithms. EUVP [[53](#bib.bib53)] and UFO-120
    [[51](#bib.bib51)] treat the unpaired distorted image and the reference image
    as two domains. Pairs of images in EUVP and UFO-120 are generated via an unsupervised
    domain transformation algorithm [[172](#bib.bib172)] with cycle consistency constraint.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由领域转换算法生成。 EUVP [[53](#bib.bib53)] 和 UFO-120 [[51](#bib.bib51)] 将未配对的失真图像和参考图像视为两个领域。
    EUVP 和 UFO-120 中的图像对是通过无监督领域转换算法 [[172](#bib.bib172)] 生成的，具有循环一致性约束。
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Rendered by light field. UWNR [[147](#bib.bib147)] designs a light field retention
    mechanism to transfer the style from natural existing underwater images to objective
    generated images. UWNR adopts a data-driven strategy, which means that it may
    avoid the accuracy limitations caused by the complexity of the physical model.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由光场渲染。 UWNR [[147](#bib.bib147)] 设计了一种光场保留机制，将自然存在的水下图像的风格转移到目标生成图像上。 UWNR 采用了数据驱动的策略，这意味着它可能避免了由于物理模型复杂性导致的准确性限制。
- en: •
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Collected by the undersea image capturing system. By setting up a multi-view
    imaging system under seawater, RUIE [[85](#bib.bib85)] constructs an underwater
    benchmark under natural light. The scenes in RUIE show a natural marine ecosystem
    containing various sea life, such as fish, sea urchins, sea cucumbers, scallops,
    etc.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由水下图像捕捉系统收集。 RUIE [[85](#bib.bib85)] 通过在海水下设置多视角成像系统，构建了一个自然光下的水下基准。 RUIE 中的场景展示了包含各种海洋生物的自然海洋生态系统，如鱼类、海胆、海参、扇贝等。
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Synthesized by physical model. Physical models are used by SUID [[44](#bib.bib44)],
    RUIG [[22](#bib.bib22)] and WaterGAN [[77](#bib.bib77)] to estimate the parameters
    that describe the imaging process.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由物理模型合成。 SUID [[44](#bib.bib44)]、RUIG [[22](#bib.bib22)] 和 WaterGAN [[77](#bib.bib77)]
    使用物理模型来估计描述成像过程的参数。
- en: In addition to the methods introduced above, neural radiance field is used by
    a recent work to synthesize both degraded and clear multi-view images [[167](#bib.bib167)].
    Currently, there is no reliable evidence on which way of constructing a dataset
    is optimal. Despite the impressive advances that have been achieved, existing
    methods of constructing datasets still have limitations in various aspects. The
    quality of the label images produced by the voting method is inherently limited,
    that is, the best performance of the model being evaluated may not exceed the
    various algorithms used in the voting process. The diversity of attenuation patterns
    of images obtained by attenuation synthesis processes inspired by generative models
    may be limited. Since the generation process with cycle consistency is limited
    by the one-to-one training mode, which results in cross-domain information not
    being fully taken into account [[5](#bib.bib5)]. The complexity of underwater
    imaging prevents physical models from accurately controlling diverse degradation
    parameters [[147](#bib.bib147)]. Effective data acquisition strategies remain
    a pressing challenge.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述介绍的方法外，最近的研究使用神经辐射场合成了退化和清晰的多视角图像 [[167](#bib.bib167)]。目前，对于哪种构建数据集的方法是最佳的，还没有可靠的证据。尽管已经取得了令人印象深刻的进展，但现有的数据集构建方法在各个方面仍存在局限性。通过投票方法产生的标注图像质量本质上是有限的，即被评估模型的最佳性能可能无法超过投票过程中使用的各种算法。由生成模型启发的衰减合成过程获得的图像衰减模式的多样性可能有限。由于循环一致性生成过程受到一对一训练模式的限制，导致跨领域信息未被充分考虑
    [[5](#bib.bib5)]。水下成像的复杂性阻碍了物理模型准确控制多样的退化参数 [[147](#bib.bib147)]。有效的数据采集策略仍然是一个紧迫的挑战。
- en: II-E Evaluation Metrics
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E 评估指标
- en: The way to evaluate the performance of UIE models is a challenging topic under
    exploration. Currently, full-reference and no-reference evaluation metrics are
    the most widely used metrics. In addition, human subjective evaluation, downstream
    task evaluation and model efficiency are also adopted by various literatures.
    Details are as follows.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 评估 UIE 模型性能的方法是一个正在探索中的挑战性话题。目前，全参考和无参考评价度量是最广泛使用的度量。此外，人为主观评价、下游任务评价和模型效率也被各种文献采用。详细情况如下。
- en: •
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Full-reference metrics. The Peak Signal-to-Noise Ratio (PSNR) [[36](#bib.bib36)]
    and Structural Similarity (SSIM) [[129](#bib.bib129)] are full-reference evaluation
    metrics that are widely used for datasets with paired images. They can accurately
    describe the distance between prediction and reference.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全参考度量。峰值信噪比 (PSNR) [[36](#bib.bib36)] 和结构相似性 (SSIM) [[129](#bib.bib129)] 是全参考评价度量，广泛用于成对图像的数据集。它们可以准确描述预测与参考之间的距离。
- en: •
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: No-reference metrics. For the evaluation of real-world enhancement performance
    without reference images, no-reference metrics are needed. The Underwater Image
    Quality Measures (UIQM) [[101](#bib.bib101)] and Underwater Color Image Quality
    Evaluation (UCIQE) [[146](#bib.bib146)] are widely adopted as no-reference metrics
    in literatures. Efforts [[164](#bib.bib164)], [[60](#bib.bib60)] are still being
    made to develop more reliable metrics for no-reference assessment.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无参考度量。对于没有参考图像的实际增强性能评估，需要使用无参考度量。水下图像质量度量 (UIQM) [[101](#bib.bib101)] 和水下彩色图像质量评估
    (UCIQE) [[146](#bib.bib146)] 被广泛采用作为无参考度量。努力 [[164](#bib.bib164)], [[60](#bib.bib60)]
    仍在进行中，以开发更可靠的无参考评估度量。
- en: •
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Human subjective evaluation. Human vision-friendly enhanced underwater images
    are one of the purposes of UIE models. On the one hand, UIE-related papers [[141](#bib.bib141),
    [53](#bib.bib53), [15](#bib.bib15), [30](#bib.bib30)] usually illustrate the superiority
    of their proposed methods in color correction, detail restoration, and edge sharpening
    from the perspective of quantitative analysis. Such an evaluation is usually performed
    by the authors of the paper themselves. On the other hand, multiple people with
    or without image processing experience are involved in the evaluation of some
    works [[74](#bib.bib74), [37](#bib.bib37)]. They can give subjective ratings to
    different algorithms. The best enhancement result may be selected [[74](#bib.bib74)],
    or the enhancement results obtained by different algorithms may be ranked [[37](#bib.bib37)].
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人为主观评价。人眼友好的增强水下图像是 UIE 模型的一个目的。一方面，UIE 相关论文 [[141](#bib.bib141), [53](#bib.bib53),
    [15](#bib.bib15), [30](#bib.bib30)] 通常从定量分析的角度展示其提出的方法在颜色校正、细节恢复和边缘锐化方面的优越性。这种评价通常由论文的作者自己进行。另一方面，参与一些作品评价的人有或没有图像处理经验
    [[74](#bib.bib74), [37](#bib.bib37)]。他们可以对不同的算法给予主观评分。可以选择最佳的增强结果 [[74](#bib.bib74)]，或者对不同算法获得的增强结果进行排序
    [[37](#bib.bib37)]。
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Evaluation by downstream tasks. The UIE task can be used as an upstream task
    for other image understanding tasks. Due to the inherent difficulties in UIE models
    evaluation, that is, the inability to obtain perfect ground-truth, the use of
    downstream tasks to evaluate UIE models has been widely used in the existing literature.
    Representative downstream tasks include
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过下游任务进行评价。UIE 任务可以作为其他图像理解任务的上游任务。由于 UIE 模型评估的固有困难，即无法获得完美的真实值，现有文献中广泛使用下游任务来评估
    UIE 模型。具有代表性的下游任务包括
- en: –
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: object detection [[163](#bib.bib163)] used in [[134](#bib.bib134), [166](#bib.bib166)],
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标检测 [[163](#bib.bib163)] 用于 [[134](#bib.bib134), [166](#bib.bib166)]，
- en: –
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: feature matching [[91](#bib.bib91)] used in [[142](#bib.bib142), [169](#bib.bib169)],
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征匹配 [[91](#bib.bib91)] 用于 [[142](#bib.bib142), [169](#bib.bib169)]，
- en: –
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: saliency detection [[52](#bib.bib52)] used in [[142](#bib.bib142), [168](#bib.bib168)],
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 显著性检测 [[52](#bib.bib52)] 用于 [[142](#bib.bib142), [168](#bib.bib168)]，
- en: –
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: semantic segmentation [[50](#bib.bib50)] used in [[111](#bib.bib111), [149](#bib.bib149)].
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语义分割 [[50](#bib.bib50)] 用于 [[111](#bib.bib111), [149](#bib.bib149)]。
- en: •
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Model efficiency. An UIE model may be used in underwater devices that do not
    have significant computing power or storage space. Therefore, the computing and
    storage resources required by the model are important factors for evaluating an
    UIE model. The commonly adopted metrics include Flops (G), Params (M) and Inference
    Time (seconds per image) [[156](#bib.bib156), [104](#bib.bib104), [148](#bib.bib148)].
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型效率。UIE 模型可能用于计算能力或存储空间有限的水下设备。因此，模型所需的计算和存储资源是评估 UIE 模型的重要因素。常用的指标包括 Flops
    (G)，Params (M) 和推理时间（每图像秒数） [[156](#bib.bib156), [104](#bib.bib104), [148](#bib.bib148)]。
- en: II-F Loss Functions
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-F 损失函数
- en: Although different UIE models may adopt different training strategies. Effective
    loss functions are commonly used by UIE models, as follows.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不同的 UIE 模型可能采用不同的训练策略，但有效的损失函数是 UIE 模型常用的，具体如下。
- en: •
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: L1 loss [[34](#bib.bib34), [169](#bib.bib169)], Smooth L1 loss [[153](#bib.bib153),
    [166](#bib.bib166)] and L2 loss [[20](#bib.bib20), [111](#bib.bib111)] provide
    the pixel-level constraint, which is used as fidelity loss.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L1 损失 [[34](#bib.bib34), [169](#bib.bib169)]，Smooth L1 损失 [[153](#bib.bib153),
    [166](#bib.bib166)] 和 L2 损失 [[20](#bib.bib20), [111](#bib.bib111)] 提供像素级约束，用作保真度损失。
- en: •
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Perceptual loss [[51](#bib.bib51), [153](#bib.bib153), [169](#bib.bib169), [168](#bib.bib168)],
    also known as content loss, is used to measure the distance between two images
    in feature space.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 感知损失 [[51](#bib.bib51), [153](#bib.bib153), [169](#bib.bib169), [168](#bib.bib168)]，也称为内容损失，用于测量特征空间中两幅图像之间的距离。
- en: •
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SSIM is the evaluation metric for UIE tasks, which is differentiable. Therefore,
    SSIM loss [[165](#bib.bib165), [20](#bib.bib20), [111](#bib.bib111)] is widely
    used for structural constraints.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SSIM 是 UIE 任务的评估指标，具有可微性。因此，SSIM 损失 [[165](#bib.bib165), [20](#bib.bib20), [111](#bib.bib111)]
    被广泛用于结构约束。
- en: •
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The attenuation of underwater images may be accompanied by the loss of edge
    information. So, the edge loss [[51](#bib.bib51), [38](#bib.bib38), [97](#bib.bib97)]
    calculated by the gradient operator is exploited.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 水下图像的衰减可能伴随边缘信息的丧失。因此，利用梯度算子计算的边缘损失 [[51](#bib.bib51), [38](#bib.bib38), [97](#bib.bib97)]
    被应用。
- en: •
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial losses [[22](#bib.bib22), [25](#bib.bib25), [84](#bib.bib84)] can
    provide domain discriminative capabilities, which may improve the visual quality
    of the restored image.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗损失 [[22](#bib.bib22), [25](#bib.bib25), [84](#bib.bib84)] 可以提供领域区分能力，可能改善恢复图像的视觉质量。
- en: III UIE Methods
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III UIE 方法
- en: According to main contributions of each paper, we divide UIE algorithms into
    six categories, namely (i) Section [III-A](#S3.SS1 "III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning") Network Architecture, (ii) Section [III-B](#S3.SS2 "III-B Learning
    Strategy ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning") Learning Strategy, (iii) Section [III-C](#S3.SS3 "III-C
    Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image
    Enhancement Based on Deep Learning") Learning Stage, (iv) Section [III-D](#S3.SS4
    "III-D Assistance Task ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning") Assistance Task, (v) Section [III-E](#S3.SS5
    "III-E Domain Perspective ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning") Domain Perspective and (vi) Section
    [III-F](#S3.SS6 "III-F Disentanglement & Fusion ‣ III UIE Methods ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning") Disentanglement
    $\&amp;$ Fusion, respectively. Details are as
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 根据每篇论文的主要贡献，我们将 UIE 算法分为六类，即 (i) 第 [III-A](#S3.SS1 "III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning") 节 网络架构，(ii) 第 [III-B](#S3.SS2 "III-B Learning Strategy ‣ III
    UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based on
    Deep Learning") 节 学习策略，(iii) 第 [III-C](#S3.SS3 "III-C Learning Stage ‣ III UIE
    Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep
    Learning") 节 学习阶段，(iv) 第 [III-D](#S3.SS4 "III-D Assistance Task ‣ III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")
    节 辅助任务，(v) 第 [III-E](#S3.SS5 "III-E Domain Perspective ‣ III UIE Methods ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning") 节 领域视角和 (vi) 第
    [III-F](#S3.SS6 "III-F Disentanglement & Fusion ‣ III UIE Methods ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning") 节 解耦 $\&amp;$
    融合，详见
- en: •
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Network Architecture. Network architecture is crucial to the performance of
    UIE models. The construction of UIE models mainly uses convolution, attention
    [[100](#bib.bib100)], Transformer [[90](#bib.bib90)], Fourier [[26](#bib.bib26)]
    and Wavelet [[46](#bib.bib46)] operations.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网络架构。网络架构对于 UIE 模型的性能至关重要。UIE 模型的构建主要使用卷积、注意力 [[100](#bib.bib100)]、Transformer
    [[90](#bib.bib90)]、傅里叶 [[26](#bib.bib26)] 和小波 [[46](#bib.bib46)] 操作。
- en: •
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Learning Strategy. Beyond conventional end-to-end supervised training, due to
    the complexity of UIE scenarios, diverse learning processes are adopted by the
    UIE algorithm. The typical learning process includes adversarial learning [[54](#bib.bib54)],
    rank learning [[158](#bib.bib158)], contrastive learning [[68](#bib.bib68)] and
    reinforcement learning [[133](#bib.bib133)].
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习策略。由于 UIE 场景的复杂性，除了传统的端到端监督训练外，UIE 算法采用了多样的学习过程。典型的学习过程包括对抗学习 [[54](#bib.bib54)]、排名学习
    [[158](#bib.bib158)]、对比学习 [[68](#bib.bib68)] 和强化学习 [[133](#bib.bib133)]。
- en: •
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Learning Stage. The mapping from distortion to prediction may be implemented
    by the single stage, coarse-to-fine, or stepwise diffusion process [[42](#bib.bib42)].
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习阶段。从失真到预测的映射可以通过单阶段、粗到细或逐步扩散过程 [[42](#bib.bib42)] 实现。
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Assistance Task. Semantic segmentation, object detection, or depth estimation
    tasks are common auxiliary tasks for the UIE task. The training of joint models
    for different tasks is beneficial to each other.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 辅助任务。语义分割、目标检测或深度估计任务是 UIE 任务的常见辅助任务。不同任务的联合模型训练对彼此有益。
- en: •
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Domain Perspective. In-air clear images, underwater distorted images, and underwater
    clear images can all be regarded as independent domains. From a domain perspective,
    knowledge transfer, degradation conversion, and enhancement effect adjustment
    can all be achieved through domain-aware training.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域视角。空气中的清晰图像、水下失真图像和水下清晰图像都可以视为独立的领域。从领域的角度来看，知识迁移、退化转换和增强效果调整都可以通过领域感知训练实现。
- en: •
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Disentanglement $\&amp;$ Fusion. Considering the degradation properties of underwater
    images, disentanglement and fusion are used as two effective ways to improve the
    interpretability of the model. Physical models and Retinex models are usually
    used as the theoretical basis for disentanglement. Color space fusion, water type
    fusion and multi-input fusion are common solutions for fusion.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解耦与融合。考虑到水下图像的退化特性，解耦和融合被用作提高模型可解释性的两种有效方式。物理模型和 Retinex 模型通常被用作解耦的理论基础。色彩空间融合、水体类型融合和多输入融合是常见的融合解决方案。
- en: As we introduce the algorithms in each category, schematic diagrams are used
    to illustrate the differences between the different algorithms. It is worth pointing
    out that what is expressed in the schematic diagram is not the details of a specific
    algorithm, but the main idea of a type of algorithms.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍每个类别的算法时，使用示意图来说明不同算法之间的差异。值得指出的是，示意图所表达的不是特定算法的细节，而是一类算法的主要思想。
- en: III-A Network Architecture
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 网络架构
- en: '![Refer to caption](img/afdc58bd105f62bf51d58b6ef4422b50.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/afdc58bd105f62bf51d58b6ef4422b50.png)'
- en: (a) Window-based Transformer                     (b) Fourier Transformation
                                  (c) Wavelet Decomposition [[127](#bib.bib127)]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 基于窗口的 Transformer                     (b) 傅里叶变换                              
    (c) 小波分解 [[127](#bib.bib127)]
- en: 'Figure 2: Building feature maps with high-quality representation by window-based
    Transformer, Fourier Transformation and Wavelet Decomposition, respectively.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：分别通过基于窗口的 Transformer、傅里叶变换和小波分解构建高质量表示的特征图。
- en: '![Refer to caption](img/7444f1c68a493996018a8f6f54d870fc.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/7444f1c68a493996018a8f6f54d870fc.png)'
- en: (a) Single Stage                     (b) Coarse-to-fine [[67](#bib.bib67)]                                       
    (c) Diffusion Process [[117](#bib.bib117)]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 单阶段                     (b) 粗到细 [[67](#bib.bib67)]                                       
    (c) 扩散过程 [[117](#bib.bib117)]
- en: 'Figure 3: Generating high-quality enhanced images with varying numbers of stages.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：生成不同阶段数量的高质量增强图像。
- en: III-A1 Convolution Operation
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 卷积操作
- en: Convolution is one of the most commonly used operations in UIE models. Network
    block constructed methods by convolution operation include naive convolution (UWCNN
    [[72](#bib.bib72)], UWNet [[99](#bib.bib99)]), residual connection (UResNet [[83](#bib.bib83)]),
    residual-dense connection (DUIR [[25](#bib.bib25)], FloodNet [[32](#bib.bib32)]),
    and multi-scale fusion (LAFFNet [[143](#bib.bib143)]). The computational consumption
    of UIE models based on convolutional architecture is usually less.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是 UIE 模型中最常用的操作之一。通过卷积操作构建的网络块方法包括朴素卷积（UWCNN [[72](#bib.bib72)], UWNet [[99](#bib.bib99)]）、残差连接（UResNet
    [[83](#bib.bib83)]）、残差密集连接（DUIR [[25](#bib.bib25)], FloodNet [[32](#bib.bib32)]）和多尺度融合（LAFFNet
    [[143](#bib.bib143)]）。基于卷积架构的 UIE 模型的计算消耗通常较少。
- en: III-A2 Attention Mechanism
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 注意力机制
- en: In the UIE task, the attention mechanism is mainly used to help the model focus
    on different spatial locations or weight different channel information. By analyzing
    the wavelength-driven contextual size relationship of the underwater scene, WaveNet
    [[111](#bib.bib111)] assigns operations with different receptive fields to different
    color channels. Then, attention operations along spatial and channel dimensions
    are used to enhance convolutional paths with different receptive fields. Based
    on the selective kernel mechanism, a nonlinear strategy is proposed by ADMNNet
    [[141](#bib.bib141)] to change receptive field sizes reasonably by adopting soft
    attention. Meanwhile, a module with the ability to dynamically aggregate channel
    information is designed by ADMNNet. Aiming at calibrating the detailed information
    of the distorted input, MFEF [[169](#bib.bib169)] constructs a pixel-weighted
    channel attention calculation flow.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在 UIE 任务中，注意力机制主要用于帮助模型关注不同的空间位置或加权不同的通道信息。通过分析水下场景的波长驱动的上下文大小关系，WaveNet [[111](#bib.bib111)]
    为不同的颜色通道分配具有不同感受野的操作。然后，沿空间和通道维度的注意力操作用于增强具有不同感受野的卷积路径。基于选择性核机制，ADMNNet [[141](#bib.bib141)]
    提出了通过采用软注意力合理改变感受野大小的非线性策略。同时，ADMNNet 设计了一个能够动态聚合通道信息的模块。针对校准失真输入的详细信息，MFEF [[169](#bib.bib169)]
    构建了一个像素加权的通道注意力计算流程。
- en: III-A3 Transformer Module
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 Transformer 模块
- en: The Transformer architecture [[120](#bib.bib120), [90](#bib.bib90)] has been
    widely used in natural language and computer vision research. UWAGA [[48](#bib.bib48)]
    points out that an automatic selection mechanism for grouping the input channels
    is beneficial for mining relationships between channels. Based on this view, an
    adaptive group attention operation embedded in Swin-Transformer [[90](#bib.bib90)]
    is designed by UWAGA. As shown in Fig. [2](#S3.F2 "Figure 2 ‣ III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning")-(a), under the principle of Swin-Transformer, the image (or
    feature map) can be split into window-based patches. Image Transformers pretrained
    on ImageNet are used by PTT [[10](#bib.bib10)] to fine-tune on the UIE task, which
    shows that features with long-distance dependencies are effective for handling
    the underwater distortion. From both the channel and spatial perspectives, a multi-scale
    feature fusion transformer block and a global feature modeling transformer block
    are proposed by U-Trans [[102](#bib.bib102)]. A multi-domain query cascaded Transformer
    architecture is designed by Spectroformer [[66](#bib.bib66)], where localized
    transmission representation and global illumination information are considered.
    Although these Transformer-based architectures have proven beneficial for the
    UIE task, they often incur computational costs that exceed conventional convolution
    operations.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构 [[120](#bib.bib120), [90](#bib.bib90)] 已广泛应用于自然语言和计算机视觉研究。UWAGA
    [[48](#bib.bib48)] 指出，自动选择输入通道的分组机制有助于挖掘通道之间的关系。基于这一观点，UWAGA 设计了一个嵌入在 Swin-Transformer
    [[90](#bib.bib90)] 中的自适应组注意力操作。如图 [2](#S3.F2 "Figure 2 ‣ III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning")-(a) 所示，在 Swin-Transformer 的原理下，图像（或特征图）可以被拆分成基于窗口的块。PTT [[10](#bib.bib10)]
    使用在 ImageNet 上预训练的图像 Transformer 对 UIE 任务进行微调，这表明具有长距离依赖的特征在处理水下失真方面是有效的。从通道和空间两个角度来看，U-Trans
    [[102](#bib.bib102)] 提出了一个多尺度特征融合 Transformer 块和一个全局特征建模 Transformer 块。Spectroformer
    [[66](#bib.bib66)] 设计了一个多领域查询级联 Transformer 架构，考虑了局部传输表示和全局照明信息。尽管这些基于 Transformer
    的架构已被证明对 UIE 任务有益，但它们通常会产生超过传统卷积操作的计算成本。
- en: III-A4 Fourier Transformation
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 傅里叶变换
- en: The Fourier transform [[18](#bib.bib18)] provides a perspective for analyzing
    features in the frequency domain. In the UIE task, operations in the Fourier domain
    and the spatial domain are often used jointly. The Fourier operation for a 2D
    image signal $x\in\mathcal{R}^{H\times W}$ is
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 傅里叶变换 [[18](#bib.bib18)] 提供了在频域分析特征的视角。在UIE任务中，傅里叶域和空间域的操作通常是联合使用的。二维图像信号 $x\in\mathcal{R}^{H\times
    W}$ 的傅里叶操作为：
- en: '|  | $\mathcal{F}(x)(u,v)=\frac{1}{\sqrt{HW}}\sum_{h=0}^{H-1}\sum_{w=0}^{W-1}x(h,w)e^{-j2{\pi}(\frac{h}{H}u+\frac{w}{W}v)},$
    |  | (5) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{F}(x)(u,v)=\frac{1}{\sqrt{HW}}\sum_{h=0}^{H-1}\sum_{w=0}^{W-1}x(h,w)e^{-j2{\pi}(\frac{h}{H}u+\frac{w}{W}v)},$
    |  | (5) |'
- en: where $(h,w)$ and $(u,v)$ denote coordinates in spatial and Fourier domain,
    respectively. The process of decomposing an underwater image into an amplitude
    spectrum and a phase spectrum is visualized in Fig. [2](#S3.F2 "Figure 2 ‣ III-A
    Network Architecture ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(b). A dual-path enhancement module
    that jointly performs feature processing in the spatial and Fourier domains was
    designed by UHD [[131](#bib.bib131)]. In the Fourier domain, channel mixers are
    used by UHD to extract the frequency domain features of the distorted image. Specifically,
    the real part and the imaginary part are the objects processed by UHD feature
    extraction. In the spatial domain, a contracting path and an expansive path are
    adopted to construct the spatial feature descriptor. Finally, the features in
    the frequency domain and spatial domain are fused. TANet [[150](#bib.bib150)]
    designs an atmospheric light removal Fourier module by utilizing the feature information
    of the real and imaginary parts. WFI2-Net [[161](#bib.bib161)] and SFGNet [[162](#bib.bib162)]
    also adopt the feature extraction and fusion method in the space-frequency domain.
    Unlike UHD and TANet, which process the real and imaginary parts, WFI2-Net and
    SFGNet dynamically filter the amplitude spectrum and phase spectrum. Although
    Fourier operations have been widely used by the UIE task, to the best of our knowledge,
    in-depth observations of degradation information in the frequency domain have
    not been given.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(h,w)$ 和 $(u,v)$ 分别表示空间域和傅里叶域的坐标。将水下图像分解为幅度谱和相位谱的过程如图 [2](#S3.F2 "Figure
    2 ‣ III-A Network Architecture ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(b) 所示。UHD [[131](#bib.bib131)] 设计了一个双路径增强模块，该模块在空间域和傅里叶域中联合执行特征处理。在傅里叶域中，UHD
    使用通道混合器提取失真图像的频域特征。具体而言，实部和虚部是 UHD 特征提取处理的对象。在空间域中，采用了收缩路径和扩展路径来构建空间特征描述符。最终，将频域和空间域的特征进行融合。TANet
    [[150](#bib.bib150)] 通过利用实部和虚部的特征信息设计了一个大气光去除傅里叶模块。WFI2-Net [[161](#bib.bib161)]
    和 SFGNet [[162](#bib.bib162)] 也采用了空间-频率域中的特征提取和融合方法。与处理实部和虚部的 UHD 和 TANet 不同，WFI2-Net
    和 SFGNet 动态滤波幅度谱和相位谱。尽管傅里叶操作已被 UIE 任务广泛使用，但据我们所知，尚未对频域中的退化信息进行深入观察。
- en: III-A5 Wavelet Decomposition
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A5 小波分解
- en: The description of different frequency sub-bands can be obtained by wavelet
    decomposition. Four frequency bands can be obtained from the original 2D signal
    $x$ by Discrete Wavelet Transform (DWT) as
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通过小波分解可以获得不同频率子带的描述。通过离散小波变换（DWT），可以从原始的二维信号 $x$ 中获得四个频率带，如下所示：
- en: '|  | $I_{LL},I_{LH},I_{HL},I_{HH}=DWT(x),$ |  | (6) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{LL},I_{LH},I_{HL},I_{HH}=DWT(x),$ |  | (6) |'
- en: where $I_{LL},I_{LH},I_{HL},I_{HH}$ denote the low-low, low-high, high-low and
    high-high sub-bands, respectively. The process of Wavelet decomposition of underwater
    images is visualized in Fig. [2](#S3.F2 "Figure 2 ‣ III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning")-(c). UIE-WD [[94](#bib.bib94)] uses a dual-branch network to
    process images of different frequency sub-bands separately, in which each branch
    handles color distortion and detail blur problems respectively. By analyzing the
    limitations of a single pooling operation, wavelet pooling and unpooling layers
    based on Haar wavelets are proposed by EUIE [[56](#bib.bib56)]. PRWNet [[49](#bib.bib49)]
    uses wavelet boost learning to obtain low-frequency and high-frequency features.
    It proposes that high-frequency sub-bands represent texture and edge information,
    while low-frequency sub-bands contain color and lighting information. By using
    normalization and attention, the information of different sub-bands can be refined.
    A frequency subband-aware multi-level interactive wavelet enhancement module is
    designed by MWEN [[127](#bib.bib127)], aiming at building a feature extraction
    branch with more expressive capabilities.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{LL},I_{LH},I_{HL},I_{HH}$ 分别表示低-低、低-高、高-低和高-高子带。水下图像的小波分解过程如图 [2](#S3.F2
    "Figure 2 ‣ III-A Network Architecture ‣ III UIE Methods ‣ A Comprehensive Survey
    on Underwater Image Enhancement Based on Deep Learning")-(c) 所示。UIE-WD [[94](#bib.bib94)]
    使用双分支网络分别处理不同频率子带的图像，其中每个分支分别处理颜色失真和细节模糊问题。通过分析单一池化操作的局限性，EUIE [[56](#bib.bib56)]
    提出了基于Haar小波的小波池化和反池化层。PRWNet [[49](#bib.bib49)] 使用小波提升学习以获得低频和高频特征。它提出高频子带代表纹理和边缘信息，而低频子带包含颜色和光照信息。通过使用归一化和注意机制，可以对不同子带的信息进行精细化。MWEN
    [[127](#bib.bib127)] 设计了一个频率子带感知的多级交互小波增强模块，旨在建立一个具有更强表达能力的特征提取分支。
- en: III-A6 Neural Architecture Search
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A6 神经架构搜索
- en: The overall network architecture and the way modules are connected are important
    components of the UIE algorithm. Effective network design solutions require designers
    to have extensive experience and knowledge. Not only that, a large number of tentative
    experiments need to be performed repeatedly. Therefore, neural architecture search
    is explored by AutoEnhancer [[116](#bib.bib116)] for building UIE network structures
    with impressive performance. An encoder-decoder architecture is chosen by AutoEnhancer
    as the supernet. Three processes are used as the basis of AutoEnhancer, namely
    supernet training, subnet searching and subnet retraining. The first step is to
    obtain the parameter $W^{*}$
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 整体网络架构及模块连接方式是UIE算法的重要组成部分。有效的网络设计解决方案需要设计师拥有丰富的经验和知识。此外，需要反复进行大量的尝试实验。因此，AutoEnhancer
    [[116](#bib.bib116)] 探索了神经架构搜索以构建具有出色性能的UIE网络结构。AutoEnhancer 选择了编码器-解码器架构作为超网络。AutoEnhancer
    以超网络训练、子网络搜索和子网络重新训练三个过程为基础。第一步是获得参数 $W^{*}$
- en: '|  | $W^{*}=\underset{W}{\arg\min}L_{train}(N(S,W)),$ |  | (7) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $W^{*}=\underset{W}{\arg\min}L_{train}(N(S,W)),$ |  | (7) |'
- en: where $S$ and $W$ denotes the searching space and network parameters for the
    supernet $N(S,W)$. The $L_{train}$ means the training objective function. Then
    the optimal subnets $S^{*}$ are searched by the trained supernet as
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S$ 和 $W$ 表示超网络 $N(S,W)$ 的搜索空间和网络参数。$L_{train}$ 代表训练目标函数。然后通过训练后的超网络搜索到最优子网络
    $S^{*}$ 如下
- en: '|  | $s^{*}=\underset{s\in S}{\arg\max}Acc_{val}(N(s,W^{*}(s))),$ |  | (8)
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $s^{*}=\underset{s\in S}{\arg\max}Acc_{val}(N(s,W^{*}(s))),$ |  | (8)
    |'
- en: where $Acc_{val}$ denotes the accuracy of the validation dataset. The finally
    optimal network parameters $W^{{}^{\prime}}$ is obtain by re-training the optimal
    subnet $M(\cdot)$ under data $X$ as
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Acc_{val}$ 代表验证数据集的准确度。最终的最优网络参数 $W^{{}^{\prime}}$ 通过在数据 $X$ 下重新训练最优子网络
    $M(\cdot)$ 获得如下
- en: '|  | $W^{{}^{\prime}}=\underset{W}{\arg\min}L_{train}(M(X;W)).$ |  | (9) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | $W^{{}^{\prime}}=\underset{W}{\arg\min}L_{train}(M(X;W)).$ |  | (9) |'
- en: There is currently little discussion on the automation of UIE network design
    and it deserves further exploration.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 目前关于UIE网络设计自动化的讨论较少，值得进一步探讨。
- en: '![Refer to caption](img/ec2aebcafa38d4b5acaa925208ea104d.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec2aebcafa38d4b5acaa925208ea104d.png)'
- en: (a) Adversarial Learning                     (b) Rank Learning                                   
    (c) Contrastive Learning
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 对抗学习                     (b) 排序学习                                    (c)
    对比学习
- en: 'Figure 4: Schematic diagrams of the Adversarial Learning, Rank Learning and
    Contrastive Learning in the UIE task.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：UIE任务中对抗学习、排名学习和对比学习的示意图。
- en: III-B Learning Strategy
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 学习策略
- en: III-B1 Adversarial Learning
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 对抗学习
- en: The adversarial training used in the generative adversarial network [[35](#bib.bib35)]
    can provide additional supervision signals for the image quality improvement process
    of the UIE task. The general form of adversarial training is
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络中使用的对抗训练[[35](#bib.bib35)]可以为UIE任务中的图像质量提升过程提供额外的监督信号。对抗训练的一般形式是
- en: '|  | $\mathcal{L}(G,D)=\mathbb{E}_{y\sim p(y)}\log{D(y)}+\mathbb{E}_{x\sim
    p(x)}[\log{(1-D(G(x)))}],$ |  | (10) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(G,D)=\mathbb{E}_{y\sim p(y)}\log{D(y)}+\mathbb{E}_{x\sim
    p(x)}[\log{(1-D(G(x)))}],$ |  | (10) |'
- en: where $G$ and $D$ denote the generator and discriminator, respectively. For
    the UIE task, the generator $G$ is usually used to generate enhanced images or
    physical parameters. The process of distinguishing enhanced and reference underwater
    images through adversarial training is visualized in Fig. [4](#S3.F4 "Figure 4
    ‣ III-A6 Neural Architecture Search ‣ III-A Network Architecture ‣ III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")-(a).
    An architecture with a dual generator and a single discriminator is designed by
    DGD-cGAN [[34](#bib.bib34)]. One generator is responsible for learning the mapping
    of distorted images to enhanced images, while another generator has the ability
    to simulate the imaging process through transmission information. In order to
    integrate information at different scales simultaneously, UIE-cGAN [[145](#bib.bib145)]
    and TOPAL [[63](#bib.bib63)] adopt a single generator and dual discriminator architecture.
    One discriminator is responsible for optimizing local detailed features, while
    the other discriminator is designed to distinguish global semantic information.
    CE-CGAN [[1](#bib.bib1)] and RUIG [[22](#bib.bib22)] utilize the discriminative
    process provided by conditional generative adversarial networks as an auxiliary
    loss. By utilizing the adversarial training, enhanced images obtained by generator
    may be more consistent with the distribution of reference underwater high-quality
    images perceptually.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$G$和$D$分别表示生成器和判别器。对于UIE任务，生成器$G$通常用于生成增强的图像或物理参数。通过对抗训练区分增强图像和参考水下图像的过程在图[4](#S3.F4
    "Figure 4 ‣ III-A6 Neural Architecture Search ‣ III-A Network Architecture ‣ III
    UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based on
    Deep Learning")-(a)中可视化。DGD-cGAN[[34](#bib.bib34)]设计了一个具有双生成器和单判别器的架构。其中一个生成器负责学习扭曲图像到增强图像的映射，而另一个生成器则具有通过传输信息模拟成像过程的能力。为了同时集成不同尺度的信息，UIE-cGAN[[145](#bib.bib145)]和TOPAL[[63](#bib.bib63)]采用了单生成器和双判别器的架构。一个判别器负责优化局部细节特征，而另一个判别器则旨在区分全局语义信息。CE-CGAN[[1](#bib.bib1)]和RUIG[[22](#bib.bib22)]利用条件生成对抗网络提供的判别过程作为辅助损失。通过利用对抗训练，生成器获得的增强图像可能在感知上与参考水下高质量图像的分布更加一致。
- en: III-B2 Rank Learning
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 排名学习
- en: The optimization goal of supervised loss is to make the enhanced image as close
    as possible to the reference image. However, model performance may be limited
    by imperfect reference images. Therefore, ranking learning is explored to provide
    “better” guidance to UIE models. From a ranking perspective, the UIE model does
    not need to know a priori which reference image is the best choice but looks for
    which reference image is the better choice. The process of selecting the best
    enhanced image through the ranking strategy is visualized in Fig. [4](#S3.F4 "Figure
    4 ‣ III-A6 Neural Architecture Search ‣ III-A Network Architecture ‣ III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")-(b).
    URanker [[37](#bib.bib37)] proposes that quality evaluation metrics designed for
    the UIE task can be used to guide the optimization of UIE models. To this end,
    a ranker loss is designed by URanker to rank the order of underwater images under
    the same scene content in terms of visual qualities. For $(x_{n},x_{m})$ selected
    from the dataset $\{x_{0},x_{1},...,x_{N}\}$ that contains $N$ images, URanker
    can predict the corresponding score $(s_{n},s_{m})$ that conforms to the ranking
    relationship by
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 监督损失的优化目标是使增强图像尽可能接近参考图像。然而，模型性能可能会受到不完美参考图像的限制。因此，探索排名学习为UIE模型提供“更好的”指导。从排名的角度来看，UIE模型无需事先知道哪个参考图像是最佳选择，而是寻找哪个参考图像是更好的选择。通过排名策略选择最佳增强图像的过程在图[4](#S3.F4
    "Figure 4 ‣ III-A6 Neural Architecture Search ‣ III-A Network Architecture ‣ III
    UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based on
    Deep Learning")-(b)中进行了可视化。URanker [[37](#bib.bib37)] 提出为UIE任务设计的质量评估指标可以用来指导UIE模型的优化。为此，URanker设计了一个排名损失，用于按视觉质量对同一场景内容下的水下图像进行排序。对于从包含$N$张图像的数据集$\{x_{0},x_{1},...,x_{N}\}$中选择的$(x_{n},x_{m})$，URanker可以预测符合排名关系的相应得分$(s_{n},s_{m})$，其计算公式为
- en: '|  | <math   alttext="\mathcal{L}(s_{n},s_{m})=\begin{cases}\max{(0,(s_{m}-s_{n})+\epsilon)},q_{n}>q_{m},\\
    \max{(0,(s_{n}-s_{m})+\epsilon)},q_{n}<q_{m},\\'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathcal{L}(s_{n},s_{m})=\begin{cases}\max{(0,(s_{m}-s_{n})+\epsilon)},q_{n}>q_{m},\\
    \max{(0,(s_{n}-s_{m})+\epsilon)},q_{n}<q_{m},\\'
- en: \end{cases}" display="block"><semantics ><mrow ><mrow  ><mi >ℒ</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mi
    >n</mi></msub><mo >,</mo><msub ><mi  >s</mi><mi >m</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mrow ><mrow ><mrow ><mi >max</mi><mo
    >⁡</mo><mrow ><mo stretchy="false" >(</mo><mn >0</mn><mo >,</mo><mrow ><mrow ><mo
    stretchy="false" >(</mo><mrow ><msub ><mi >s</mi><mi >m</mi></msub><mo >−</mo><msub
    ><mi >s</mi><mi >n</mi></msub></mrow><mo stretchy="false"  >)</mo></mrow><mo >+</mo><mi
    >ϵ</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo >,</mo><msub ><mi
    >q</mi><mi >n</mi></msub></mrow><mo >></mo><msub ><mi >q</mi><mi >m</mi></msub></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow  ><mrow ><mrow
    ><mrow ><mi >max</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mn >0</mn><mo
    >,</mo><mrow ><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi >s</mi><mi
    >n</mi></msub><mo >−</mo><msub ><mi >s</mi><mi >m</mi></msub></mrow><mo stretchy="false"  >)</mo></mrow><mo
    >+</mo><mi >ϵ</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo >,</mo><msub
    ><mi >q</mi><mi >n</mi></msub></mrow><mo ><</mo><msub ><mi >q</mi><mi >m</mi></msub></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><ci >ℒ</ci><interval closure="open" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑛</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑠</ci><ci  >𝑚</ci></apply></interval></apply><apply ><csymbol cd="latexml" >cases</csymbol><apply
    ><list  ><apply ><cn type="integer"  >0</cn><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑚</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑛</ci></apply></apply><ci >italic-ϵ</ci></apply></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑛</ci></apply></list><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑚</ci></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><list ><apply ><cn type="integer" >0</cn><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑛</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑚</ci></apply></apply><ci
    >italic-ϵ</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑛</ci></apply></list><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑚</ci></apply></apply><ci ><mtext >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathcal{L}(s_{n},s_{m})=\begin{cases}\max{(0,(s_{m}-s_{n})+\epsilon)},q_{n}>q_{m},\\
    \max{(0,(s_{n}-s_{m})+\epsilon)},q_{n}<q_{m},\\ \end{cases}</annotation></semantics></math>
    |  | (11) |
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><mrow  ><mi >ℒ</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >s</mi><mi
    >n</mi></msub><mo >,</mo><msub ><mi  >s</mi><mi >m</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mrow ><mrow ><mrow ><mi >max</mi><mo
    >⁡</mo><mrow ><mo stretchy="false" >(</mo><mn >0</mn><mo >,</mo><mrow ><mrow ><mo
    stretchy="false" >(</mo><mrow ><msub ><mi >s</mi><mi >m</mi></msub><mo >−</mo><msub
    ><mi >s</mi><mi >n</mi></msub></mrow><mo stretchy="false"  >)</mo></mrow><mo >+</mo><mi
    >ϵ</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo >,</mo><msub ><mi
    >q</mi><mi >n</mi></msub></mrow><mo >></mo><msub ><mi >q</mi><mi >m</mi></msub></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow  ><mrow ><mrow
    ><mrow ><mi >max</mi><mo >⁡</mo><mrow ><mo stretchy="false" >(</mo><mn >0</mn><mo
    >,</mo><mrow ><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi >s</mi><mi
    >n</mi></msub><mo >−</mo><msub ><mi >s</mi><mi >m</mi></msub></mrow><mo stretchy="false"  >)</mo></mrow><mo
    >+</mo><mi >ϵ</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo >,</mo><msub
    ><mi >q</mi><mi >n</mi></msub></mrow><mo ><</mo><msub ><mi >q</mi><mi >m</mi></msub></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><ci >ℒ</ci><interval closure="open" ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑛</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑠</ci><ci  >𝑚</ci></apply></interval></apply><apply ><csymbol cd="latexml" >cases</csymbol><apply
    ><list  ><apply ><cn type="integer"  >0</cn><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑚</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><ci >𝑛</ci></apply></apply><ci >italic-ϵ</ci></apply></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑛</ci></apply></list><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑞</ci><ci >𝑚</ci></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><list ><apply ><cn type="integer" >0</cn><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑛</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑚</ci></apply></apply><ci
    >italic-ϵ</ci></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑛</ci></apply></list><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑞</ci><ci >𝑚</ci></apply></apply><ci ><mtext >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathcal{L}(s_{n},s_{m})=\begin{cases}\max{(0,(s_{m}-s_{n})+\epsilon)},q_{n}>q_{m},\\
    \max{(0,(s_{n}-s_{m})+\epsilon)},q_{n}<q_{m},\\ \end{cases}</annotation></semantics></math>
    |  | (11) |
- en: where $\mathcal{L}(s_{n},s_{m})$ is regarded as the margin-ranking loss. The
    $q_{n}$ and $q_{m}$ denote the quality of $x_{n}$ and $x_{m}$, respectively. Moreover,
    PDD-Net [[61](#bib.bib61)] designs a pairwise quality ranking loss that is used
    in the form of a pre-trained model to guide the enhanced images toward the higher
    visual quality. The comparative mechanism is utilized by CLUIE-Net [[79](#bib.bib79)]
    to learn from multiple candidates of the reference. The main advantage of comparative
    learning is to make the image quality generated by the network better than the
    current enhancement candidates.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}(s_{n},s_{m})$ 被视为边际排名损失。$q_{n}$ 和 $q_{m}$ 分别表示 $x_{n}$ 和 $x_{m}$
    的质量。此外，PDD-Net [[61](#bib.bib61)] 设计了一种成对质量排名损失，该损失以预训练模型的形式使用，以引导增强后的图像向更高的视觉质量迈进。CLUIE-Net
    [[79](#bib.bib79)] 利用比较机制从多个参考候选中进行学习。比较学习的主要优点在于使网络生成的图像质量优于当前的增强候选图像。
- en: III-B3 Contrastive Learning
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 对比学习
- en: Real-world pairs of distorted images and reference images are difficult to obtain.
    Therefore, reference images that are not accurate enough may provide imperfect
    supervision signals for the UIE network. Contrastive learning, a method that can
    alleviate the problem of missing labeled data, is used by UIE models to provide
    additional supervision signals. The optimization goals of UIE algorithms inspired
    by contrastive learning are generally consistent, which is to increase the distance
    between the anchor and the negative sample while reducing the distance between
    the anchor and the positive sample. The process of optimizing the distance between
    different types of samples through contrastive learning is visualized in Fig.
    [4](#S3.F4 "Figure 4 ‣ III-A6 Neural Architecture Search ‣ III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning")-(c). Research on contrast patterns in the UIE task focuses
    on two perspectives. The first is how to construct positive samples, negative
    samples and anchors. The second is how to optimize the distance between samples
    or features. TACL [[86](#bib.bib86)] treats the observed distorted underwater
    image as the negative sample and the clear image as the positive sample. To construct
    a reliable representation space, TACL adopts a pre-trained feature extractor to
    obtain the perceptual feature for the computing of the contrastive loss. A hybrid
    contrastive learning regularization $\mathcal{L}_{hclr}$ is proposed by HCLR-Net
    [[168](#bib.bib168)] as
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中扭曲图像和参考图像的对很难获得。因此，可能提供不准确的参考图像可能为UIE网络提供不完美的监督信号。对比学习是一种可以缓解缺乏标注数据问题的方法，UIE模型使用这种方法提供额外的监督信号。受到对比学习启发的UIE算法的优化目标通常是一致的，即增加锚点和负样本之间的距离，同时减少锚点和正样本之间的距离。通过对比学习优化不同类型样本之间的距离的过程在图
    [4](#S3.F4 "Figure 4 ‣ III-A6 Neural Architecture Search ‣ III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning")-(c) 中进行了可视化。UIE任务中的对比模式研究集中在两个方面。首先是如何构造正样本、负样本和锚点。其次是如何优化样本或特征之间的距离。TACL
    [[86](#bib.bib86)] 将观察到的扭曲水下图像视为负样本，将清晰图像视为正样本。为了构建可靠的表示空间，TACL 采用了预训练的特征提取器来获取用于计算对比损失的感知特征。HCLR-Net
    [[168](#bib.bib168)] 提出了混合对比学习正则化 $\mathcal{L}_{hclr}$。
- en: '|  | $\mathcal{L}_{hclr}=\sum_{i=1}^{5}w_{i}\frac{D(G_{i}(y),G_{i}(\phi(x)))}{D(G_{i}(x_{r},G_{i}(\phi(x))))},$
    |  | (12) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{hclr}=\sum_{i=1}^{5}w_{i}\frac{D(G_{i}(y),G_{i}(\phi(x)))}{D(G_{i}(x_{r},G_{i}(\phi(x))))},$
    |  | (12) |'
- en: where $D(\cdot,\cdot)$ denotes the L1 loss. $G_{i}$ and $w_{i}$ represent the
    $i$-th layer of the pretrained feature extraction model and weight factor, respectively.
    $\phi(\cdot)$ is combined by feature and detail network branches. $x_{r}$ means
    a randomly selected distorted underwater image. The reliable bank and augment
    methods are used by Semi-UIR [[47](#bib.bib47)] to generate positive and negative
    samples. No-reference evaluation metrics are used in the triplet construction
    process of Semi-UIR as a guide for sample selection. A closed-loop approach is
    adopted by TFUIE [[149](#bib.bib149)], which uses two paths to simultaneously
    enhance and synthesize distorted images. Correspondingly, a triplet contrastive
    loss is applied to both the enhancement and synthesis processes.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D(\cdot,\cdot)$ 表示 L1 损失。$G_{i}$ 和 $w_{i}$ 分别代表预训练特征提取模型的第 $i$ 层和权重因子。$\phi(\cdot)$
    是由特征网络和细节网络分支组合而成。$x_{r}$ 表示随机选择的失真水下图像。Semi-UIR [[47](#bib.bib47)] 使用可靠的库和增强方法来生成正负样本。在
    Semi-UIR 的三元组构建过程中，使用无参考评估指标作为样本选择的指导。TFUIE [[149](#bib.bib149)] 采用了闭环方法，通过两条路径同时增强和合成失真图像。因此，三元组对比损失被应用于增强和合成过程。
- en: III-B4 Reinforcement Learning
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B4 强化学习
- en: Deep learning has been widely proven to be effective for the UIE task. Beyond
    conventional research on neural networks that use fidelity losses for optimization,
    HPUIE-RL [[113](#bib.bib113)] explores how to take advantage of both deep learning
    and reinforcement learning. A two-stage pipeline with pre-training and fine-tuning
    is adopted by HPUIE-RL. The update method of model parameters during pre-training
    is consistent with that of conventional deep neural networks. For the fine-tuning
    phase of the model, a reinforcement learning strategy based on the reward function
    is designed. The reward function consists of three no-reference metrics that evaluate
    underwater image quality from different perspectives, which is
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已被广泛证明对 UIE 任务有效。除了使用保真度损失进行优化的传统神经网络研究之外，HPUIE-RL [[113](#bib.bib113)]
    探索了如何利用深度学习和强化学习。HPUIE-RL 采用了一个包含预训练和微调的两阶段流程。在预训练期间，模型参数的更新方法与传统深度神经网络一致。在模型的微调阶段，设计了基于奖励函数的强化学习策略。奖励函数由三种无参考指标组成，从不同角度评估水下图像质量，公式为
- en: '|  | $\mathcal{R}=\beta_{1}\times&#124;\mathcal{R}_{a}-r^{max}_{a}&#124;+\beta_{2}\times\mathcal{R}_{b}+\beta_{3}\times&#124;\mathcal{R}_{c}-r^{max}_{c}&#124;,$
    |  | (13) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{R}=\beta_{1}\times&#124;\mathcal{R}_{a}-r^{max}_{a}&#124;+\beta_{2}\times\mathcal{R}_{b}+\beta_{3}\times&#124;\mathcal{R}_{c}-r^{max}_{c}&#124;,$
    |  | (13) |'
- en: where $a$, $b$ and $c$ represent the metric UCIQE [[146](#bib.bib146)], NIQE
    [[96](#bib.bib96)] and URanker [[37](#bib.bib37)], respectively. The $\beta_{1}$,
    $\beta_{2}$ and $\beta_{3}$ denote weight factors of the reward $\mathcal{R}_{a}$,
    $\mathcal{R}_{b}$ and $\mathcal{R}_{c}$, respectively. The $r^{max}_{a}$ and $r^{max}_{c}$
    are the upper bounds to constrain the value range. The performance of HPUIE-RL
    can be continuously improved through iterative refinement optimization.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a$、$b$ 和 $c$ 分别代表指标 UCIQE [[146](#bib.bib146)]、NIQE [[96](#bib.bib96)] 和
    URanker [[37](#bib.bib37)]。$\beta_{1}$、$\beta_{2}$ 和 $\beta_{3}$ 分别表示奖励 $\mathcal{R}_{a}$、$\mathcal{R}_{b}$
    和 $\mathcal{R}_{c}$ 的权重因子。$r^{max}_{a}$ 和 $r^{max}_{c}$ 是约束值范围的上界。HPUIE-RL 的性能可以通过迭代优化持续提升。
- en: III-C Learning Stage
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 学习阶段
- en: III-C1 Single Stage
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 单阶段
- en: With the help of the data fitting capabilities of neural networks, the mapping
    from the distortion to the prediction can be learned in a single-stage manner
    as shown in Fig. [3](#S3.F3 "Figure 3 ‣ III-A Network Architecture ‣ III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning").
    Representative single-stage enhancement methods include UWCNN [[72](#bib.bib72)],
    UWNet [[99](#bib.bib99)] and UResNet [[83](#bib.bib83)], etc.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 借助神经网络的数据拟合能力，如图 [3](#S3.F3 "Figure 3 ‣ III-A Network Architecture ‣ III UIE
    Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep
    Learning") 所示，可以以单阶段方式学习从失真到预测的映射。代表性的单阶段增强方法包括 UWCNN [[72](#bib.bib72)]、UWNet
    [[99](#bib.bib99)] 和 UResNet [[83](#bib.bib83)] 等。
- en: III-C2 Coarse-to-fine
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 粗到精
- en: Considering the difficulty of obtaining optimal enhancement results at one time,
    the coarse-to-fine model with different purposes at different stages is utilized.
    By analyzing the color shifts and veil phenomena, a multi-branch multi-variable
    network for obtaining coarse results and attenuation factors is designed by MBANet
    [[138](#bib.bib138)]. Then, the attenuation factors and coarse results are put
    into a physically inspired model to obtain refined enhancement results. A way
    to fuse two intermediate results through soft attention is proposed by GSL [[81](#bib.bib81)].
    One intermediate result contains structure and color information estimated by
    the global flow, while the other intermediate result handles overexposure and
    artifacts by the local flow. CUIE [[67](#bib.bib67)] builds a two-stage framework,
    in which the first stage obtains preliminary enhancement results through the global-local
    path, while the second stage uses histogram equalization and neural networks to
    improve the contrast and brightness of the image. The process of enhancing image
    quality by refinement in a two-stage manner proposed by CUIE is visualized in
    Fig. [3](#S3.F3 "Figure 3 ‣ III-A Network Architecture ‣ III UIE Methods ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning")-(b). A three-stage
    pipeline is designed by DMML [[27](#bib.bib27)], namely supervised training, adversarial
    training and fusion training. The goal of supervised training is to achieve high
    full-reference evaluation metric values, which usually represent better performance
    on synthetic data. Adversarial training is responsible for improving the no-reference
    evaluation metric values, which may make the model more friendly to real-world
    distorted images. The final fusion training takes advantage of both.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到一次性获得最佳增强结果的难度，采用了在不同阶段具有不同目的的粗到精模型。通过分析色彩变化和面纱现象，设计了一种由MBANet [[138](#bib.bib138)]
    提出的多分支多变量网络，用于获取粗略结果和衰减因子。然后，将衰减因子和粗略结果输入到一个以物理为基础的模型中，以获得精细的增强结果。GSL [[81](#bib.bib81)]
    提出了通过软注意力融合两个中间结果的方法。其中一个中间结果包含由全局流估计的结构和颜色信息，而另一个中间结果通过局部流处理过度曝光和伪影。CUIE [[67](#bib.bib67)]
    构建了一个两阶段框架，其中第一阶段通过全局-局部路径获得初步增强结果，而第二阶段使用直方图均衡化和神经网络来改善图像的对比度和亮度。CUIE 提出的通过两阶段方式进行图像质量精细化的过程在图
    Fig. [3](#S3.F3 "Figure 3 ‣ III-A Network Architecture ‣ III UIE Methods ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning")-(b) 中进行了可视化。DMML
    [[27](#bib.bib27)] 设计了一个三阶段管道，即监督训练、对抗训练和融合训练。监督训练的目标是实现高的全参考评估指标值，这通常代表在合成数据上的较好性能。对抗训练负责提高无参考评估指标值，这可能使模型更适合真实世界的失真图像。最终的融合训练则兼顾了两者的优势。
- en: '![Refer to caption](img/3bfa8cd336c12bab65ffea0fabd4cf94.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3bfa8cd336c12bab65ffea0fabd4cf94.png)'
- en: (a) Semantic Assistance                                    (b) Guided by Depth
                                  (c) Taking Depth as Input
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 语义辅助                                    (b) 深度引导                              
    (c) 以深度为输入
- en: 'Figure 5: Schematic diagrams of improving the UIE model performance by different
    auxiliary tasks. The gray and green arrows represent forward and backward propagation,
    respectively.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：通过不同辅助任务提高UIE模型性能的示意图。灰色和绿色箭头分别代表前向传播和后向传播。
- en: '![Refer to caption](img/dd7a661add8f2f8f37d4df2e5a100fe2.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd7a661add8f2f8f37d4df2e5a100fe2.png)'
- en: (a) Knowledge Transfer                               (b) Domain Translation
                                  (c) Diversified Output
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 知识转移                               (b) 领域翻译                              
    (c) 多样化输出
- en: 'Figure 6: Schematic diagrams of UIE models designed from a domain perspective.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：从领域角度设计的UIE模型示意图。
- en: III-C3 Diffusion Process
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C3 扩散过程
- en: The diffusion model is a powerful generative model that has been widely studied
    recently. The forward process of the diffusion model is a Markov process that
    continuously adds noise to the image [[42](#bib.bib42)]. High-quality images can
    be generated by the reverse process. The process of enhancing underwater image
    quality through the diffusion strategy is visualized in Fig. [3](#S3.F3 "Figure
    3 ‣ III-A Network Architecture ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(c). The generative process cannot
    produce a specific enhanced underwater image in the desired form. Therefore, conditional
    information $c$ is added by UIE-DM [[117](#bib.bib117)] to the diffusion process
    for the UIE task. The training loss of UIE-DM is
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型是一个强大的生成模型，最近被广泛研究。扩散模型的前向过程是一个马尔科夫过程，它持续向图像中添加噪声 [[42](#bib.bib42)]。通过反向过程可以生成高质量的图像。通过扩散策略增强水下图像质量的过程在图
    [3](#S3.F3 "Figure 3 ‣ III-A Network Architecture ‣ III UIE Methods ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning")-(c) 中进行了可视化。生成过程无法产生特定的增强水下图像，因此，UIE-DM
    [[117](#bib.bib117)] 向扩散过程中添加了条件信息 $c$ 以完成 UIE 任务。UIE-DM 的训练损失是
- en: '|  | $\mathcal{L}_{s}=&#124;&#124;\epsilon_{t}-\epsilon_{\theta}(x_{t},c,t)&#124;&#124;_{1},$
    |  | (14) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{s}=&#124;&#124;\epsilon_{t}-\epsilon_{\theta}(x_{t},c,t)&#124;&#124;_{1},$
    |  | (14) |'
- en: where $x_{t}$ and $t$ denote the noisy image and time step, respectively. The
    $\epsilon_{t}$ and $\epsilon_{\theta}$ represent the noise image and predicted
    noise image, respectively. Therefore, the mean value $\mu_{\theta}$ can be defined
    as
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{t}$ 和 $t$ 分别表示有噪声的图像和时间步。$\epsilon_{t}$ 和 $\epsilon_{\theta}$ 分别表示噪声图像和预测的噪声图像。因此，均值
    $\mu_{\theta}$ 可以定义为
- en: '|  | $\mu_{\theta}(x_{t},c,t)=\frac{1}{\sqrt{1-\beta_{t}}}\left(x_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(x_{t},c,t)\right).$
    |  | (15) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu_{\theta}(x_{t},c,t)=\frac{1}{\sqrt{1-\beta_{t}}}\left(x_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(x_{t},c,t)\right).$
    |  | (15) |'
- en: According to the above definition, the distorted underwater image can be used
    as condition information, which makes the image generated by the diffusion process
    deterministic. The relatively high computational cost of diffusion processes is
    an unsolved problem. SU-DDPM [[92](#bib.bib92)] proposes that the computational
    cost can be reduced by using different initial distributions. Although the diffusion
    model has been proven effective for the UIE task, its powerful data generation
    capabilities have not been fully explored.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述定义，失真的水下图像可以作为条件信息，使得扩散过程生成的图像具有确定性。扩散过程相对较高的计算成本是一个未解决的问题。SU-DDPM [[92](#bib.bib92)]
    提出可以通过使用不同的初始分布来降低计算成本。尽管扩散模型已被证明对 UIE 任务有效，但其强大的数据生成能力尚未得到充分探索。
- en: III-D Assistance Task
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 辅助任务
- en: III-D1 Semantic Assistance
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D1 语义辅助
- en: High-level semantic information has not been widely explored in conventional
    UIE algorithms. In order to generate diverse features to UIE models, classification,
    segmentation and detection tasks has been embedded into UIE models by recent research.
    A schematic diagram of the UIE network training assisted by semantic information
    is visualized in Fig. [5](#S3.F5 "Figure 5 ‣ III-C2 Coarse-to-fine ‣ III-C Learning
    Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning")-(a). Regional information related to semantic segmentation
    maps is embedded into the UIE model by SGUIE [[105](#bib.bib105)] to provide richer
    semantic information. A pre-trained classification model trained on a large-scale
    dataset is used as the feature extraction network by SATS [[122](#bib.bib122)].
    DAL-UIE [[64](#bib.bib64)] embeds a classifier that can constrain the latent space
    between the encoder and decoder. The classification loss $\mathcal{L}_{N}$ used
    for distinguish difference water types is
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 UIE 算法中尚未广泛探索高层语义信息。为了为 UIE 模型生成多样的特征，最近的研究将分类、分割和检测任务嵌入到 UIE 模型中。图 [5](#S3.F5
    "Figure 5 ‣ III-C2 Coarse-to-fine ‣ III-C Learning Stage ‣ III UIE Methods ‣ A
    Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")-(a)
    显示了语义信息辅助下的 UIE 网络训练示意图。与语义分割图相关的区域信息通过 SGUIE [[105](#bib.bib105)] 嵌入到 UIE 模型中，以提供更丰富的语义信息。SATS
    [[122](#bib.bib122)] 使用在大规模数据集上训练的预训练分类模型作为特征提取网络。DAL-UIE [[64](#bib.bib64)] 嵌入了一个分类器，可以约束编码器和解码器之间的潜在空间。用于区分不同水类型的分类损失
    $\mathcal{L}_{N}$ 是
- en: '|  | $\mathcal{L}_{N}=-(1-p_{t})^{\gamma}\log(p_{t}),$ |  | (16) |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{N}=-(1-p_{t})^{\gamma}\log(p_{t}),$ |  | (16) |'
- en: where $p_{t}$ is the probability of classification for category $t$. The $\gamma$
    denote hyperparameter for the weight factor. Meanwhile, by minimizing the maximum
    mean divergence between the encoder and the classifier, robust features can be
    learned by DAL-UIE. A detection perception module is designed by WaterFlow [[160](#bib.bib160)]
    to extract the local position information of the object. Through two-stage training
    of the UIE model and detection model from independent to joint, the network can
    obtain features that represent position-related semantic information. Exploring
    how to combine low-level and high-level features can be beneficial for both tasks.
    The goal of the UIE task is not only to obtain images with good human subjective
    perception, but also to promote the performance of downstream tasks.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$p_{t}$是类别$t$的分类概率。$\gamma$表示权重因子的超参数。同时，通过最小化编码器和分类器之间的最大均值散度，DAL-UIE可以学习到鲁棒的特征。WaterFlow
    [[160](#bib.bib160)]设计了一个检测感知模块来提取对象的局部位置信息。通过从独立到联合的两阶段训练UIE模型和检测模型，网络可以获得表示位置相关语义信息的特征。探索如何结合低级和高级特征对这两个任务都可能有益。UIE任务的目标不仅是获得具有良好人类主观感知的图像，还要提升下游任务的性能。
- en: III-D2 Depth Assistance
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D2 深度辅助
- en: The degree of attenuation of underwater images is related to the depth of the
    scene. Therefore, depth maps are used by UIE algorithms to assist in the training
    of the network. As shown in Fig. [5](#S3.F5 "Figure 5 ‣ III-C2 Coarse-to-fine
    ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(b) and Fig. [5](#S3.F5 "Figure 5 ‣
    III-C2 Coarse-to-fine ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning")-(c), there are
    two ways to use depth information to assist the training of UIE models. The first
    one uses depth maps as the prediction targets, while the second one uses depth
    maps as the fusion inputs. Joint-ID [[142](#bib.bib142)] treats the UIE task and
    the depth estimation task as a unified task. It uses a joint training method to
    enable the decoder to output enhanced images and depth images simultaneously.
    For the predicted depth map $\hat{d}$ and ground-truth depth map $d$, the depth
    loss $\mathcal{L}_{depth}$ can be defined as
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 水下图像的衰减程度与场景的深度有关。因此，UIE算法使用深度图来辅助网络的训练。如图[5](#S3.F5 "Figure 5 ‣ III-C2 Coarse-to-fine
    ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(b)和图[5](#S3.F5 "Figure 5 ‣ III-C2
    Coarse-to-fine ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey
    on Underwater Image Enhancement Based on Deep Learning")-(c)所示，有两种方式使用深度信息来辅助UIE模型的训练。第一种方法使用深度图作为预测目标，而第二种方法使用深度图作为融合输入。Joint-ID
    [[142](#bib.bib142)]将UIE任务和深度估计任务视为统一任务。它使用联合训练方法，使解码器能够同时输出增强图像和深度图像。对于预测的深度图$\hat{d}$和真实深度图$d$，深度损失$\mathcal{L}_{depth}$可以定义为
- en: '|  | $g_{j}=\log{\hat{d}_{j}}-\log{d_{j}},$ |  | (17) |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $g_{j}=\log{\hat{d}_{j}}-\log{d_{j}},$ |  | (17) |'
- en: '|  | $\mathcal{L}_{depth}(\hat{d},d)=\lambda_{1}\sqrt{\frac{1}{T}\sum_{j}{g_{j}^{2}-\frac{\lambda_{2}}{T^{2}}\left(\sum_{i}{g_{j}^{2}}\right)^{2}}},$
    |  | (18) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{depth}(\hat{d},d)=\lambda_{1}\sqrt{\frac{1}{T}\sum_{j}{g_{j}^{2}-\frac{\lambda_{2}}{T^{2}}\left(\sum_{i}{g_{j}^{2}}\right)^{2}}},$
    |  | (18) |'
- en: where $\lambda_{1}$ and $\lambda_{2}$ are two weight factors. The $j$ represents
    the index of the total number of pixels $T$. A combination of depth estimation
    loss $\mathcal{L}_{depth}$ and UIE loss is used for the training of Joint-ID.
    A two-stage architecture is designed by DAUT [[8](#bib.bib8)], whose first and
    second stages are depth estimation and enhancement, respectively. In the enhancement
    stage, namely the second stage, the depth map and the distortion map are simultaneously
    used as inputs to the enhancement network to provide richer prior information.
    DepthCue [[20](#bib.bib20)] utilizes the depth map obtained by a pre-trained depth
    estimation network as auxiliary information for the decoder of the enhancement
    network. HybrUR [[140](#bib.bib140)] trains a depth estimation network from scratch
    that provides depth information which fits the degeneration factors. Although
    these studies show that depth maps can bring positive effects to the UIE task,
    it is worth pointing out that the monocular depth estimation task itself has inherent
    experimental errors. Since it is almost impossible to obtain perfect depth information
    from a monocular image.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{1}$ 和 $\lambda_{2}$ 是两个权重因子。$j$ 表示总像素数 $T$ 的索引。联合 ID 的训练使用了深度估计损失
    $\mathcal{L}_{depth}$ 和 UIE 损失的组合。DAUT [[8](#bib.bib8)] 设计了一个两阶段的架构，其第一阶段和第二阶段分别为深度估计和增强。在增强阶段，即第二阶段，深度图和失真图同时作为输入提供给增强网络，以提供更丰富的先验信息。DepthCue
    [[20](#bib.bib20)] 利用由预训练深度估计网络获得的深度图作为增强网络解码器的辅助信息。HybrUR [[140](#bib.bib140)]
    从头开始训练一个深度估计网络，以提供适合退化因素的深度信息。尽管这些研究表明深度图对 UIE 任务有积极的影响，但值得指出的是，单目深度估计任务本身存在固有的实验误差，因为从单目图像中几乎不可能获得完美的深度信息。
- en: III-E Domain Perspective
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 领域视角
- en: III-E1 Knowledge Transfer
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-E1 知识迁移
- en: Synthetic data cannot accurately reflect the complex underwater attenuation
    of the real world, which results in a model trained on synthetic data that may
    not achieve satisfactory performance in the real-world UIE task. Domain discrepancy
    between synthetic and real-world data are inherent. Therefore, domain adaptation
    strategies are explored to reduce this domain bias. A triple-alignment network
    with translation path and a task-oriented enhancement path is designed by TUDA
    [[130](#bib.bib130)]. Domain gap is handled at image-level ($\mathcal{L}^{img}$),
    output-level ($\mathcal{L}^{out}$), feature-level ($\mathcal{L}^{feat}$) by discriminators
    $D^{img}$, $D^{out}$ and $D^{feat}$, respectively. The inter-domain adaptation
    at image-level and output-level involves the real-world underwater image $x_{r}$
    and the corresponding enhanced version $y_{r}$ as
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据无法准确反映现实世界中复杂的水下衰减，这导致基于合成数据训练的模型在实际 UIE 任务中可能无法达到令人满意的性能。合成数据与现实世界数据之间的领域差异是固有的。因此，探索领域适配策略以减少这种领域偏差。TUDA
    [[130](#bib.bib130)] 设计了一个具有翻译路径和任务导向增强路径的三重对齐网络。领域差距通过判别器 $D^{img}$、$D^{out}$
    和 $D^{feat}$ 在图像级别 ($\mathcal{L}^{img}$)、输出级别 ($\mathcal{L}^{out}$)、特征级别 ($\mathcal{L}^{feat}$)
    处理。图像级别和输出级别的领域适配涉及实际水下图像 $x_{r}$ 及其对应的增强版本 $y_{r}$。
- en: '|  | $\begin{split}\mathcal{L}^{img}&amp;=\mathbb{E}_{x_{st}}[D^{img}(x_{st})]-\mathbb{E}_{x_{r}}[D^{img}(x_{r})]\\
    &amp;=+\lambda_{img}\mathbb{E}_{\hat{I}}(&#124;&#124;\nabla_{\hat{I}}D^{img}(\hat{I})&#124;&#124;_{2}-1)^{2},\end{split}$
    |  | (19) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}^{img}&amp;=\mathbb{E}_{x_{st}}[D^{img}(x_{st})]-\mathbb{E}_{x_{r}}[D^{img}(x_{r})]\\
    &amp;=+\lambda_{img}\mathbb{E}_{\hat{I}}(&#124;&#124;\nabla_{\hat{I}}D^{img}(\hat{I})&#124;&#124;_{2}-1)^{2},\end{split}$
    |  | (19) |'
- en: '|  | $\begin{split}\mathcal{L}^{out}&amp;=\mathbb{E}_{y_{st}}[D^{out}(y_{st})]-\mathbb{E}_{y_{r}}[D^{out}(y_{r})]\\
    &amp;+\lambda_{out}\mathbb{E}_{\hat{I}}(&#124;&#124;\nabla_{\hat{I}}D^{out}(\hat{I})&#124;&#124;_{2}-1)^{2},\end{split}$
    |  | (20) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}^{out}&amp;=\mathbb{E}_{y_{st}}[D^{out}(y_{st})]-\mathbb{E}_{y_{r}}[D^{out}(y_{r})]\\
    &amp;+\lambda_{out}\mathbb{E}_{\hat{I}}(&#124;&#124;\nabla_{\hat{I}}D^{out}(\hat{I})&#124;&#124;_{2}-1)^{2},\end{split}$
    |  | (20) |'
- en: where $x_{st}$ is translated from an in-air image $x_{s}$, while the $y_{st}$
    is an enhanced version of $x_{st}$. The $\hat{I}$ is sampled from $\{x_{r},x_{st}\}$.
    The domain adaptation at feature-level is
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{st}$ 是从空中图像 $x_{s}$ 转换而来的，而 $y_{st}$ 是 $x_{st}$ 的增强版本。$\hat{I}$ 从 $\{x_{r},x_{st}\}$
    中采样。特征级的领域适配为
- en: '|  | $\begin{split}\mathcal{L}^{feat}&amp;=\mathbb{E}_{x_{st}}[D^{feat}(\mathcal{G}_{r}(x_{st}))]-\mathbb{E}_{x_{r}}[D^{feat}(\mathcal{G}_{r}(x_{r}))]\\
    &amp;+\lambda_{feat}\mathbb{E}_{\hat{I}}(&#124;&#124;\nabla_{\hat{I}}D^{feat}(\hat{I})&#124;&#124;_{2}-1)^{2},\end{split}$
    |  | (21) |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}^{feat}&amp;=\mathbb{E}_{x_{st}}[D^{feat}(\mathcal{G}_{r}(x_{st}))]-\mathbb{E}_{x_{r}}[D^{feat}(\mathcal{G}_{r}(x_{r}))]\\
    &amp;+\lambda_{feat}\mathbb{E}_{\hat{I}}(&#124;&#124;\nabla_{\hat{I}}D^{feat}(\hat{I})&#124;&#124;_{2}-1)^{2},\end{split}$
    |  | (21) |'
- en: where $\mathcal{G}_{r}$ is the inter-class encoder. $\lambda_{img}$, $\lambda_{out}$
    and $\lambda_{feat}$ are weight factors. The schematic diagram of knowledge transfer
    is shown in Fig. [6](#S3.F6 "Figure 6 ‣ III-C2 Coarse-to-fine ‣ III-C Learning
    Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning")-(a). By utilizing domain transformation algorithms, in-air
    images and underwater images are simultaneously used by TSDA [[62](#bib.bib62)]
    to construct intermediate domains. Then, the domain discrepancy between the intermediate
    domain and in-air domain is reduced by a carefully designed enhancement network.
    Unpaired underwater images and paired in-air images are used by IUIE [[9](#bib.bib9)]
    for semi-supervised training. Through weight sharing strategy and channel prior
    loss, in-air images and prior knowledge are used to enhance the quality of underwater
    distorted images. WSDS-GAN [[84](#bib.bib84)] designs a weak-strong two-stage
    process, in which weak learning and strong learning are implemented in unsupervised
    and supervised ways, respectively. Weak learning uses in-air and underwater image
    domains to learn information such as the content and brightness, while the goal
    of strong learning is to reduce the blurring of details caused by training with
    domain differences.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{G}_{r}$ 是类间编码器。$\lambda_{img}$、$\lambda_{out}$ 和 $\lambda_{feat}$
    是权重因子。知识转移的示意图见图 [6](#S3.F6 "图 6 ‣ III-C2 由粗到细 ‣ III-C 学习阶段 ‣ III UIE 方法 ‣ 基于深度学习的水下图像增强的全面综述")-(a)。通过利用领域转换算法，TSDA
    [[62](#bib.bib62)] 同时使用空气图像和水下图像来构建中间领域。然后，通过精心设计的增强网络减少中间领域与空气领域之间的领域差异。IUIE
    [[9](#bib.bib9)] 使用无配对水下图像和配对空气图像进行半监督训练。通过权重共享策略和通道先验损失，利用空气图像和先验知识来提升水下扭曲图像的质量。WSDS-GAN
    [[84](#bib.bib84)] 设计了一个弱强两阶段过程，其中弱学习和强学习分别以无监督和监督的方式进行。弱学习使用空气和水下图像领域来学习内容和亮度等信息，而强学习的目标是减少由于领域差异训练造成的细节模糊。
- en: III-E2 Domain Translation
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-E2 领域翻译
- en: Distorted and clear underwater images can be viewed as two different domains.
    Through unsupervised domain transformation, the dependence on pairs of underwater
    images can be alleviated. The unsupervised disentanglement of content information
    and style information is designed by URD-UIE [[173](#bib.bib173)]. URD-UIE proposes
    that in the UIE task, content information usually represents the texture and semantics,
    while style information can represent degradation such as noise or blur. By leveraging
    an adversarial process to learn content and style encoding, the degradation information
    can be manipulated in the latent space. Aiming at achieving cross-domain translation,
    URD-UIE employs a domain bidirectional loop reconstruction process, that is, $x\rightarrow
    x_{x-y}\rightarrow\dot{x}$ and $y\rightarrow y_{y-x}\rightarrow\dot{y}$, where
    $\dot{x}$ and $\dot{y}$ represent reconstructed image after two-time domain translations.
    The reconstruction is adopted on the image ($\mathcal{L}^{img}$), style ($\mathcal{L}^{sty}$)
    and content ($\mathcal{L}^{con}$) spaces as
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 扭曲和清晰的水下图像可以视为两个不同的领域。通过无监督领域转换，可以减轻对水下图像对的依赖。URD-UIE [[173](#bib.bib173)] 设计了内容信息和风格信息的无监督解缠结。URD-UIE
    提出在 UIE 任务中，内容信息通常表示纹理和语义，而风格信息可以表示噪声或模糊等退化。通过利用对抗过程学习内容和风格编码，可以在潜在空间中操作退化信息。为了实现跨领域翻译，URD-UIE
    采用领域双向循环重建过程，即 $x\rightarrow x_{x-y}\rightarrow\dot{x}$ 和 $y\rightarrow y_{y-x}\rightarrow\dot{y}$，其中
    $\dot{x}$ 和 $\dot{y}$ 表示经过两次领域翻译后的重建图像。重建在图像（$\mathcal{L}^{img}$）、风格（$\mathcal{L}^{sty}$）和内容（$\mathcal{L}^{con}$）空间上进行，如下所示：
- en: '|  | <math   alttext="\begin{cases}\mathcal{L}^{img}=&#124;&#124;x-\dot{x}&#124;&#124;_{1}+&#124;&#124;y-\dot{y}&#124;&#124;_{1},\\
    \mathcal{L}^{sty}=&#124;&#124;s_{x}-\dot{s_{x}}&#124;&#124;_{1}+&#124;&#124;s_{y}-\dot{s_{y}}&#124;&#124;_{1},\\'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{cases}\mathcal{L}^{img}=&#124;&#124;x-\dot{x}&#124;&#124;_{1}+&#124;&#124;y-\dot{y}&#124;&#124;_{1},\\
    \mathcal{L}^{sty}=&#124;&#124;s_{x}-\dot{s_{x}}&#124;&#124;_{1}+&#124;&#124;s_{y}-\dot{s_{y}}&#124;&#124;_{1},\\'
- en: \mathcal{L}^{con}=&#124;&#124;c_{x}-\dot{c_{x}}&#124;&#124;_{1}+&#124;&#124;c_{y}-\dot{c_{y}}&#124;&#124;_{1},\\
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: \mathcal{L}^{con}=&#124;&#124;c_{x}-\dot{c_{x}}&#124;&#124;_{1}+&#124;&#124;c_{y}-\dot{c_{y}}&#124;&#124;_{1},\\
- en: \end{cases}" display="block"><semantics ><mrow ><mo  >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mrow
    ><msup  ><mi >ℒ</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi></mrow></msup><mo >=</mo><mrow
    ><msub ><mrow ><mo stretchy="false"  >‖</mo><mrow ><mi >x</mi><mo >−</mo><mover
    accent="true"  ><mi >x</mi><mo >˙</mo></mover></mrow><mo stretchy="false"  >‖</mo></mrow><mn
    >1</mn></msub><mo >+</mo><msub ><mrow ><mo stretchy="false"  >‖</mo><mrow ><mi
    >y</mi><mo >−</mo><mover accent="true"  ><mi >y</mi><mo >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn >1</mn></msub></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow  ><msup ><mi >ℒ</mi><mrow ><mi >s</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >y</mi></mrow></msup><mo >=</mo><mrow ><msub ><mrow ><mo stretchy="false"  >‖</mo><mrow
    ><msub ><mi >s</mi><mi >x</mi></msub><mo >−</mo><mover accent="true"  ><msub ><mi
    >s</mi><mi >x</mi></msub><mo >˙</mo></mover></mrow><mo stretchy="false"  >‖</mo></mrow><mn
    >1</mn></msub><mo >+</mo><msub ><mrow ><mo stretchy="false"  >‖</mo><mrow ><msub
    ><mi >s</mi><mi >y</mi></msub><mo >−</mo><mover accent="true"  ><msub ><mi >s</mi><mi
    >y</mi></msub><mo >˙</mo></mover></mrow><mo stretchy="false"  >‖</mo></mrow><mn
    >1</mn></msub></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mrow  ><msup ><mi >ℒ</mi><mrow ><mi >c</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi></mrow></msup><mo >=</mo><mrow
    ><msub ><mrow ><mo stretchy="false"  >‖</mo><mrow ><msub ><mi >c</mi><mi >x</mi></msub><mo
    >−</mo><mover accent="true"  ><msub ><mi >c</mi><mi >x</mi></msub><mo >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn >1</mn></msub><mo >+</mo><msub ><mrow ><mo
    stretchy="false"  >‖</mo><mrow ><msub ><mi >c</mi><mi >y</mi></msub><mo >−</mo><mover
    accent="true"  ><msub ><mi >c</mi><mi >y</mi></msub><mo >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn >1</mn></msub></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="latexml"  >cases</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℒ</ci><apply ><ci
    >𝑖</ci><ci >𝑚</ci><ci >𝑔</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><csymbol cd="latexml" >norm</csymbol><apply ><ci >𝑥</ci><apply
    ><ci >˙</ci><ci >𝑥</ci></apply></apply></apply><cn type="integer"  >1</cn></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="latexml"  >norm</csymbol><apply
    ><ci >𝑦</ci><apply ><ci >˙</ci><ci >𝑦</ci></apply></apply></apply><cn type="integer"  >1</cn></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ℒ</ci><apply ><ci >𝑠</ci><ci >𝑡</ci><ci >𝑦</ci></apply></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="latexml" >norm</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply
    ><ci >˙</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci
    >𝑥</ci></apply></apply></apply></apply><cn type="integer"  >1</cn></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="latexml"  >norm</csymbol><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑦</ci></apply><apply
    ><ci >˙</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci
    >𝑦</ci></apply></apply></apply></apply><cn type="integer"  >1</cn></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ℒ</ci><apply ><ci >𝑐</ci><ci >𝑜</ci><ci >𝑛</ci></apply></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="latexml" >norm</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑐</ci><ci >𝑥</ci></apply><apply
    ><ci >˙</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑐</ci><ci
    >𝑥</ci></apply></apply></apply></apply><cn type="integer"  >1</cn></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="latexml"  >norm</csymbol><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑐</ci><ci >𝑦</ci></apply><apply
    ><ci >˙</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑐</ci><ci
    >𝑦</ci></apply></apply></apply></apply><cn type="integer"  >1</cn></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{cases}\mathcal{L}^{img}=&#124;&#124;x-\dot{x}&#124;&#124;_{1}+&#124;&#124;y-\dot{y}&#124;&#124;_{1},\\
    \mathcal{L}^{sty}=&#124;&#124;s_{x}-\dot{s_{x}}&#124;&#124;_{1}+&#124;&#124;s_{y}-\dot{s_{y}}&#124;&#124;_{1},\\
    \mathcal{L}^{con}=&#124;&#124;c_{x}-\dot{c_{x}}&#124;&#124;_{1}+&#124;&#124;c_{y}-\dot{c_{y}}&#124;&#124;_{1},\\
    \end{cases}</annotation></semantics></math> |  | (22) |
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><mo  >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mrow
    ><msup  ><mi >ℒ</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi></mrow></msup><mo >=</mo><mrow
    ><msub ><mrow ><mo stretchy="false"  >‖</mo><mrow ><mi >x</mi><mo >−</mo><mover
    accent="true"  ><mi >x</mi><mo >˙</mo></mover></mrow><mo stretchy="false"  >‖</mo></mrow><mn
    >1</mn></msub><mo >+</mo><msub ><mrow ><mo stretchy="false"  >‖</mo><mrow ><mi
    >y</mi><mo >−</mo><mover accent="true"  ><mi >y</mi><mo >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn >1</mn></msub></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow  ><msup ><mi >ℒ</mi><mrow ><mi >s</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >y</mi></mrow></msup><mo >=</mo><mrow ><msub ><mrow ><mo stretchy="false"  >‖</mo><mrow
    ><msub ><mi >s</mi><mi >x</mi></msub><mo >−</mo><mover accent="true"  ><msub ><mi
    >s</mi><mi >x</mi></msub><mo >˙</mo></mover></mrow><mo stretchy="false"  >‖</mo></mrow><mn
    >1</mn></msub><mo >+</mo><msub ><mrow ><mo stretchy="false"  >‖</mo><mrow ><msub
    ><mi >s</mi><mi >y</mi></msub><mo >−</mo><mover accent="true"  ><msub ><mi >s</mi><mi
    >y</mi></msub><mo >˙</mo></mover></mrow><mo stretchy="false"  >‖</mo></mrow><mn
    >1</mn></msub></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mrow  ><msup ><mi >ℒ</mi><mrow ><mi >c</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi></mrow></msup><mo >=</mo><mrow
    ><msub ><mrow ><mo stretchy="false"  >‖</mo><mrow ><msub ><mi >c</mi><mi >x</mi></msub><mo
    >−</mo><mover accent="true"  ><msub ><mi >c</mi><mi >x</mi></msub><mo >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn >1</mn></msub><mo >+</mo><msub ><mrow ><mo
    stretchy="false"  >‖</mo><mrow ><msub ><mi >c</mi><mi >y</mi></msub><mo >−</mo><mover
    accent="true"  ><msub ><mi >c</mi><mi >y</mi></msub><mo >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn >1</mn></msub></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="latexml"  >cases</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℒ</ci><apply ><ci
    >𝑖</ci><ci >𝑚</ci><ci >𝑔</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><csymbol cd="latexml" >norm</csymbol><apply ><ci >𝑥</ci><apply
    ><ci >˙</ci><ci >𝑥</ci></apply></apply></apply><cn type="integer"  >1</cn></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="latexml"  >norm</csymbol><apply
    ><ci >𝑦</ci><apply ><ci >˙</ci><ci >𝑦</ci></apply></apply></apply><cn type="integer"  >1</cn></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ℒ</ci><apply ><ci >𝑠</ci><ci >𝑡</ci><ci >𝑦</ci></apply></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="latexml" >norm</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci >𝑥</ci></apply><apply
    ><ci >˙</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><ci
    >𝑥</ci></apply></apply></apply></apply><cn type="integer"  >1</cn></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="latexml"  >norm</csymbol><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci >𝑦</ci></apply><apply
    ><ci >˙</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><ci
    >𝑦</ci></apply></apply></apply></apply><cn type="integer"  >1</cn></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ℒ</ci><apply ><ci >𝑐</ci><ci >𝑜</ci><ci >𝑛</ci></apply></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="latexml" >norm</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑐</ci><ci >𝑥</ci></apply><apply
    ><ci >˙</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑐</ci><ci
    >𝑥</ci></apply></apply></apply></apply><cn type="integer"  >1</cn></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="latexml"  >norm</csymbol><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑐</ci><ci >𝑦</ci></apply><apply
    ><ci >˙</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑐</ci><ci
    >𝑦</ci></apply></apply></apply></apply><cn type="integer"  >1</cn></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{cases}\mathcal{L}^{img}=&#124;&#124;x-\dot{x}&#124;&#124;_{1}+&#124;&#124;y-\dot{y}&#124;&#124;_{1},\\
    \mathcal{L}^{sty}=&#124;&#124;s_{x}-\dot{s_{x}}&#124;&#
- en: where $\{s_{x},s_{y}\}$ and $\{c_{x},c_{y}\}$ denote style and content codes
    for $x$ and $y$, respectively. The $\dot{s_{x}},\dot{s_{y}},\dot{c_{x}}$ and $\dot{c_{y}}$
    mean the corresponding reconstructed versions. TACL [[86](#bib.bib86)] constructs
    a closed-loop path to learn bidirectional mappings simultaneously. The forward
    branch is responsible for learning distortion to clean images, while the optimization
    goal of the reverse network is to learn the attenuation process. The process of
    domain translation is shown in Fig. [6](#S3.F6 "Figure 6 ‣ III-C2 Coarse-to-fine
    ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(b).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\{s_{x},s_{y}\}$ 和 $\{c_{x},c_{y}\}$ 分别表示 $x$ 和 $y$ 的风格和内容编码。$\dot{s_{x}},\dot{s_{y}},\dot{c_{x}}$
    和 $\dot{c_{y}}$ 表示相应的重建版本。TACL [[86](#bib.bib86)] 构建了一个闭环路径以同时学习双向映射。正向分支负责学习失真到清晰图像，而反向网络的优化目标是学习衰减过程。领域翻译的过程如图
    [6](#S3.F6 "Figure 6 ‣ III-C2 Coarse-to-fine ‣ III-C Learning Stage ‣ III UIE
    Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep
    Learning")-(b) 所示。
- en: III-E3 Diversified Output
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-E3 多样化输出
- en: For an image enhancement task which no perfect solution exists, it may be beneficial
    to provide the user with selectable and diverse outputs. The pixel-wise wasserstein
    autoencoder architecture is designed by PWAE [[69](#bib.bib69)], which has a two-dimensional
    latent tensor representation. The tensor $z_{h}$ and $z_{s}$ are used to represent
    enhancement space and style space. The space translation can be implemented by
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有完美解决方案的图像增强任务，提供可选择和多样化的输出可能是有益的。PWAE [[69](#bib.bib69)] 设计了像素级 Wasserstein
    自编码器架构，该架构具有二维潜在张量表示。张量 $z_{h}$ 和 $z_{s}$ 用于表示增强空间和风格空间。空间转换可以通过以下公式实现：
- en: '|  | $z_{h\rightarrow s}=\sigma(z_{s})\left(\frac{z_{h}-\mu{(z_{h})}}{\sigma(z_{h})}\right)+\mu{(z_{s})},$
    |  | (23) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{h\rightarrow s}=\sigma(z_{s})\left(\frac{z_{h}-\mu{(z_{h})}}{\sigma(z_{h})}\right)+\mu{(z_{s})},$
    |  | (23) |'
- en: where $\mu(\cdot)$ and $\sigma(\cdot)$ denote the mean and standard deviation,
    respectively. The degree of the fused tensor $z^{s}_{h}$ can be controlled by
    the parameter $\alpha$ as
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu(\cdot)$ 和 $\sigma(\cdot)$ 分别表示均值和标准差。融合张量 $z^{s}_{h}$ 的程度可以通过参数 $\alpha$
    控制，如下所示
- en: '|  | $z^{s}_{h}=\alpha z_{h\rightarrow s}+(1-\alpha)z_{h}.$ |  | (24) |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | $z^{s}_{h}=\alpha z_{h\rightarrow s}+(1-\alpha)z_{h}.$ |  | (24) |'
- en: By fusing the latent code provided by the style image, PWAE is able to change
    the illumination and style of the enhanced result. Inspired by the multi-domain
    image-to-image algorithm, UIESS [[16](#bib.bib16)] decomposes the UIE process
    into content and style learning flows. By manipulating the encoding values in
    the latent space, images with different degrees of enhancement can be produced
    by the decoder of UIESS. Beyond the content and style codes studied by UIESS,
    CECF [[19](#bib.bib19)] proposes the concept of color code. CECF assumes that
    when there exists images with less local distortion in the dataset, these images
    may have colors with long wavelengths that are approximately invariant during
    the enhancement process. By learning a color representation with such invariance,
    CECF can obtain color-adjustable underwater creatures from guidance images. The
    diverse output provided by CECF is shown in Fig. [6](#S3.F6 "Figure 6 ‣ III-C2
    Coarse-to-fine ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey
    on Underwater Image Enhancement Based on Deep Learning")-(c).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 通过融合风格图像提供的潜在编码，PWAE 能够改变增强结果的光照和风格。受到多领域图像到图像算法的启发，UIESS [[16](#bib.bib16)]
    将 UIE 过程分解为内容和风格学习流程。通过操作潜在空间中的编码值，UIESS 的解码器可以生成不同程度增强的图像。除了 UIESS 研究的内容和风格编码外，CECF
    [[19](#bib.bib19)] 提出了颜色编码的概念。CECF 假设当数据集中存在局部失真较少的图像时，这些图像可能具有在增强过程中近似不变的长波长颜色。通过学习具有这种不变性的颜色表示，CECF
    可以从引导图像中获得颜色可调节的水下生物。CECF 提供的多样化输出如图 [6](#S3.F6 "Figure 6 ‣ III-C2 Coarse-to-fine
    ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(c) 所示。
- en: III-F Disentanglement $\&amp;$ Fusion
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-F 解耦与融合
- en: III-F1 Physical Embedding
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-F1 物理嵌入
- en: Neural network-based UIE methods can often achieve impressive performance on
    specific datasets, but the generalization capabilities and interpretability they
    exhibit may be limited. Therefore, physical models are integrated into the data-driven
    training process by various algorithms. IPMGAN [[87](#bib.bib87)] embeds the Akkaynak-Treibitz
    model [[2](#bib.bib2)] introduced in Section [II-C](#S2.SS3 "II-C Physical Models
    ‣ II Related Work ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning") into a generative adversarial network. The desired reference
    image can be refered as
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经网络的UIE方法通常在特定数据集上能够实现令人印象深刻的性能，但它们表现出的泛化能力和解释性可能有限。因此，物理模型被各种算法集成到数据驱动的训练过程中。IPMGAN
    [[87](#bib.bib87)] 将第[II-C](#S2.SS3 "II-C Physical Models ‣ II Related Work ‣
    A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")节中介绍的Akkaynak-Treibitz模型
    [[2](#bib.bib2)] 嵌入到生成对抗网络中。期望的参考图像可以参考
- en: '|  | $y=\frac{x-B^{\infty}\left(1-e^{-\beta^{B}(v_{B})d}\right)}{e^{-\beta^{D}(v_{D})d}}.$
    |  | (25) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=\frac{x-B^{\infty}\left(1-e^{-\beta^{B}(v_{B})d}\right)}{e^{-\beta^{D}(v_{D})d}}.$
    |  | (25) |'
- en: This physical representation can be reformulated for obtaining enhanced image
    $\hat{y}$ as
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 该物理表示可以重新表述为获取增强图像$\hat{y}$为
- en: '|  | $\hat{y}=\frac{x-\widehat{B}^{\infty}(1-\widehat{S})}{\widehat{T}},$ |  |
    (26) |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}=\frac{x-\widehat{B}^{\infty}(1-\widehat{S})}{\widehat{T}},$ |  |
    (26) |'
- en: where $\widehat{T}$ and $\widehat{S}$ are the estimated $e^{-\beta^{D}(v_{D})z}$
    and $e^{-\beta^{B}(v_{B})d}$, respectively. The $\widehat{B}^{\infty}$ denotes
    the estimated veiling light. Through weight-sharing physical parameters ($\widehat{T}$,
    $\widehat{S}$ and $\widehat{B}^{\infty}$) estimation, the enhanced image can be
    derived from the inverse imaging model. The distortion process of the underwater
    image is considered as a horizontal and vertical process by ACPAB [[82](#bib.bib82)].
    From a horizontal perspective, the parameters required by the physical model,
    namely attenuation coefficient, transmission map and background light, are estimated
    by the neural network. From a vertical perspective, an attenuation coefficient
    prior is embedded into the enhancement network. USUIR [[29](#bib.bib29)] builds
    a physically decoupled closed-loop system. Three physical parameters are estimated
    by the network and priori assumptions. Then, the original attenuation image is
    synthesized inversely through the physical parameters. Through such a self-feedback
    mechanism, the training of USUIR can be carried out in an unsupervised manner.
    It is worth pointing out that the modeling of the imaging process of the underwater
    environment is still a task under exploration, and no perfect model has yet been
    proposed.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\widehat{T}$和$\widehat{S}$分别是估计的$e^{-\beta^{D}(v_{D})z}$和$e^{-\beta^{B}(v_{B})d}$。$\widehat{B}^{\infty}$表示估计的遮光光。通过权重共享物理参数（$\widehat{T}$、$\widehat{S}$和$\widehat{B}^{\infty}$）的估计，可以从逆成像模型中获得增强图像。ACPAB
    [[82](#bib.bib82)] 将水下图像的失真过程视为水平和垂直过程。从水平视角来看，物理模型所需的参数，即衰减系数、传输图和背景光，通过神经网络进行估计。从垂直视角来看，衰减系数先验被嵌入到增强网络中。USUIR
    [[29](#bib.bib29)] 构建了一个物理解耦的闭环系统。网络和先验假设估计三个物理参数。然后，通过物理参数逆向合成原始衰减图像。通过这种自反馈机制，USUIR的训练可以以无监督的方式进行。值得指出的是，水下环境成像过程的建模仍然是一个正在探索的任务，目前尚未提出完美的模型。
- en: III-F2 Retinex Model
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-F2 Retinex模型
- en: 'According to the Retinex model [[104](#bib.bib104)], the image can be decomposed
    into the Hadamard product of two components: reflection and illumination. The
    form of the Retinex model is simpler than the physical models commonly used in
    the UIE task since it requires fewer parameters to be estimated. The multi-scale
    Retinex is employed by CCMSR-Net [[104](#bib.bib104)], which is'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Retinex模型 [[104](#bib.bib104)]，图像可以分解为两个组件的Hadamard乘积：反射和照明。Retinex模型的形式比在UIE任务中常用的物理模型更简单，因为它需要估计的参数较少。CCMSR-Net
    [[104](#bib.bib104)] 使用了多尺度Retinex。
- en: '|  | $R=\sum_{n=1}^{N}w_{n}[\log{x}-\log{x\ast G_{n}}],$ |  | (27) |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  | $R=\sum_{n=1}^{N}w_{n}[\log{x}-\log{x\ast G_{n}}],$ |  | (27) |'
- en: where $n$ and $w_{n}$ represent the scale index and corresponding weight factor,
    respectively. $G_{n}$ stand for the Gaussian kernel under scale $n$. The $R$ denote
    the reflectance. CCMSR-Net decomposes the enhancement task into two procedures.
    A subnetwork for color correction and a multiscale Retinex subnetwork for estimating
    the illumination, which take into account two consecutive procedures, are integrated
    by CCMSR-Net. ReX-Net [[153](#bib.bib153)] uses two encoders, namely the original
    image encoder and the Retinex-based reflectance encoder, to extract complementary
    content and color information. Based on the self-information extracted which utilizes
    the Retinex decomposition consistency, ASSU-Net [[65](#bib.bib65)] designs a pipeline
    to improve the contrast of the weak and backlit areas.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$n$ 和 $w_{n}$ 分别表示尺度索引和相应的权重因子。$G_{n}$ 代表尺度 $n$ 下的高斯核。$R$ 代表反射率。CCMSR-Net
    将增强任务分解为两个过程：一个用于颜色校正的子网络和一个多尺度 Retinex 子网络用于估计照明，CCMSR-Net 将这两个连续过程整合在一起。ReX-Net
    [[153](#bib.bib153)] 使用两个编码器，即原始图像编码器和基于 Retinex 的反射率编码器，来提取互补的内容和颜色信息。基于提取的自我信息，该信息利用了
    Retinex 分解一致性，ASSU-Net [[65](#bib.bib65)] 设计了一个管道，以改善弱光和背光区域的对比度。
- en: '![Refer to caption](img/b79d54f8f3a9044806df01dcd7a41f3a.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b79d54f8f3a9044806df01dcd7a41f3a.png)'
- en: (a) Physical Embedding                                    (b) Color Space Fusion
                                  (c) Multi-inputs Fusion
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 物理嵌入                                    (b) 颜色空间融合                              
    (c) 多输入融合
- en: 'Figure 7: Schematic diagrams of disentanglement and fusion.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：解耦和融合的示意图。
- en: III-F3 Color Space Fusion
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-F3 颜色空间融合
- en: Different attributes are held by different color spaces. RGB representation
    is the most commonly used color space for the UIE task. The three color channels
    of RGB space are highly correlated. This property makes it possible for the RGB
    space to be easily affected by the changes in luminance, occlusion, and shadow
    [[125](#bib.bib125)]. Therefore, color spaces with other properties are explored
    by UIE algorithms. A HSV global-adjust module is designed in UIEC²-Net [[125](#bib.bib125)],
    which can be used for adjusting the luminance, color and saturation by utilizing
    a piece-wise linear scaling curve layer. Meanwhile, a HSV loss $\mathcal{L}_{hsv}$
    is adopted by UIEC²-Net as
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的颜色空间具有不同的属性。RGB 表示是 UIE 任务中最常用的颜色空间。RGB 空间的三个颜色通道高度相关。这一特性使得 RGB 空间容易受到亮度、遮挡和阴影变化的影响
    [[125](#bib.bib125)]。因此，UIE 算法探索了具有其他属性的颜色空间。UIEC²-Net [[125](#bib.bib125)] 设计了一个
    HSV 全局调整模块，该模块可以通过利用分段线性缩放曲线层来调整亮度、颜色和饱和度。同时，UIEC²-Net 采用了 HSV 损失 $\mathcal{L}_{hsv}$。
- en: '|  | $\mathcal{L}_{hsv}=&#124;&#124;\widehat{S}\widehat{V}\cos{\widehat{H}}-{SV}\cos{H}&#124;&#124;_{1},$
    |  | (28) |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{hsv}=&#124;&#124;\widehat{S}\widehat{V}\cos{\widehat{H}}-{SV}\cos{H}&#124;&#124;_{1},$
    |  | (28) |'
- en: where $\widehat{H}$, $\widehat{S}$ and $\widehat{V}$ denote the predictions
    of $H\in[0,2\pi)$, $S\in[0,1]$ and $C\in[0,1]$, respectively. MTNet [[97](#bib.bib97)]
    proposes a loss function measured from HSV space to better restore contrast and
    saturation information. An RGB-HSV dual-color space-guided color estimation block
    is proposed by UGIF-Net [[166](#bib.bib166)] to generate comprehensive color information.
    The LAB space is explored by TCTL-Net [[78](#bib.bib78)] and P2CNet [[108](#bib.bib108)]
    to improve color recovery performance. JLCL-Net [[137](#bib.bib137)] converts
    the distorted image and reference image into YCbCr space to improve the sensitivity
    of luminance and chrominance. RGB, HSV and LAB are simultaneously embedded into
    the encoding path by UColor [[71](#bib.bib71)] to construct features with a unified
    representation.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\widehat{H}$、$\widehat{S}$ 和 $\widehat{V}$ 分别表示 $H\in[0,2\pi)$、$S\in[0,1]$
    和 $C\in[0,1]$ 的预测值。MTNet [[97](#bib.bib97)] 提出了一个从 HSV 空间测量的损失函数，以更好地恢复对比度和饱和度信息。UGIF-Net
    [[166](#bib.bib166)] 提出了一个 RGB-HSV 双颜色空间引导的颜色估计模块，以生成全面的颜色信息。LAB 空间由 TCTL-Net
    [[78](#bib.bib78)] 和 P2CNet [[108](#bib.bib108)] 探索，以提高颜色恢复性能。JLCL-Net [[137](#bib.bib137)]
    将失真的图像和参考图像转换为 YCbCr 空间，以提高亮度和色度的敏感性。RGB、HSV 和 LAB 同时嵌入到 UColor [[71](#bib.bib71)]
    的编码路径中，以构建具有统一表示的特征。
- en: III-F4 Water Type Focus
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-F4 水体类型聚焦
- en: Due to the complexity of underwater scene imaging, underwater distorted images
    may exhibit different properties. Common imaging environments include shallow
    coastal waters, deep oceanic waters, and muddy waters. The lighting conditions
    of different underwater environments are diverse. Therefore, a challenging topic
    is how to handle different water types using a single UIE model. Aiming at learning
    water type’s desensitized features, SCNet [[30](#bib.bib30)] performs normalization
    operations in both spatial and channel dimensions. An instance whitening is designed
    in the encoding-encoding structure of SCNet. For the $n$-th example $x_{n}\in\mathbb{R}^{C\times
    HW}$ in a mini-batch, the designed instance whitening $\Gamma(\cdot)$ can be expressed
    as
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 由于水下场景成像的复杂性，水下失真图像可能会表现出不同的特性。常见的成像环境包括浅海沿岸水域、深海水域和泥泞水域。不同水下环境的光照条件各异。因此，一个具有挑战性的话题是如何使用单一的
    UIE 模型处理不同的水体类型。SCNet [[30](#bib.bib30)] 旨在学习水体类型的去敏感特征，在空间和通道维度上执行归一化操作。SCNet
    在编码-解码结构中设计了一个实例白化。对于 mini-batch 中的第 $n$ 个样本 $x_{n}\in\mathbb{R}^{C\times HW}$，设计的实例白化
    $\Gamma(\cdot)$ 可以表示为
- en: '|  | $\Gamma(x_{n})=\Sigma^{-1/2}(x_{n}-\mu)\gamma+\beta,$ |  | (29) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Gamma(x_{n})=\Sigma^{-1/2}(x_{n}-\mu)\gamma+\beta,$ |  | (29) |'
- en: where $\gamma$ and $\beta$ denote scale and shift which can be dynamically learned.
    The mean vector $\mu$ and covariance matrix $\Sigma$ are computed with each individual
    sample by
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma$ 和 $\beta$ 表示尺度和偏移量，可以动态学习。均值向量 $\mu$ 和协方差矩阵 $\Sigma$ 是通过对每个单独样本进行计算得到的，计算方式为
- en: '|  | <math   alttext="\begin{cases}\mu=\frac{1}{HW}x_{n},\\ \Sigma=\frac{1}{HW}(x_{n}-\mu)(x_{n}-\mu)^{T}+\alpha{I},\\'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{cases}\mu=\frac{1}{HW}x_{n},\\ \Sigma=\frac{1}{HW}(x_{n}-\mu)(x_{n}-\mu)^{T}+\alpha{I},\\'
- en: \end{cases}" display="block"><semantics ><mrow ><mo  >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mrow
    ><mi  >μ</mi><mo >=</mo><mrow ><mstyle displaystyle="false"  ><mfrac ><mn >1</mn><mrow
    ><mi >H</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >W</mi></mrow></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >x</mi><mi >n</mi></msub></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mrow  ><mi mathvariant="normal"  >Σ</mi><mo
    >=</mo><mrow ><mrow ><mstyle displaystyle="false"  ><mfrac ><mn >1</mn><mrow ><mi
    >H</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >W</mi></mrow></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow ><msub
    ><mi >x</mi><mi >n</mi></msub><mo >−</mo><mi >μ</mi></mrow><mo stretchy="false"  >)</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><msup ><mrow ><mo stretchy="false"  >(</mo><mrow
    ><msub ><mi >x</mi><mi >n</mi></msub><mo >−</mo><mi >μ</mi></mrow><mo stretchy="false"  >)</mo></mrow><mi
    >T</mi></msup></mrow><mo >+</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >I</mi></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="latexml"  >cases</csymbol><apply
    ><ci >𝜇</ci><apply ><apply ><cn type="integer" >1</cn><apply ><ci >𝐻</ci><ci >𝑊</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><apply ><ci  >Σ</ci><apply ><apply ><apply ><cn
    type="integer"  >1</cn><apply ><ci >𝐻</ci><ci >𝑊</ci></apply></apply><apply ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply><ci
    >𝜇</ci></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply><ci
    >𝜇</ci></apply><ci >𝑇</ci></apply></apply><apply ><ci >𝛼</ci><ci >𝐼</ci></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{cases}\mu=\frac{1}{HW}x_{n},\\ \Sigma=\frac{1}{HW}(x_{n}-\mu)(x_{n}-\mu)^{T}+\alpha{I},\\
    \end{cases}</annotation></semantics></math> |  | (30) |
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><mo  >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mrow
    ><mi  >μ</mi><mo >=</mo><mrow ><mstyle displaystyle="false"  ><mfrac ><mn >1</mn><mrow
    ><mi >H</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >W</mi></mrow></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >x</mi><mi >n</mi></msub></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><mrow  ><mi mathvariant="normal"  >Σ</mi><mo
    >=</mo><mrow ><mrow ><mstyle displaystyle="false"  ><mfrac ><mn >1</mn><mrow ><mi
    >H</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >W</mi></mrow></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow ><msub
    ><mi >x</mi><mi >n</mi></msub><mo >−</mo><mi >μ</mi></mrow><mo stretchy="false"  >)</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><msup ><mrow ><mo stretchy="false"  >(</mo><mrow
    ><msub ><mi >x</mi><mi >n</mi></msub><mo >−</mo><mi >μ</mi></mrow><mo stretchy="false"  >)</mo></mrow><mi
    >T</mi></msup></mrow><mo >+</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >I</mi></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol cd="latexml"  >cases</csymbol><apply
    ><ci >𝜇</ci><apply ><apply ><cn type="integer" >1</cn><apply ><ci >𝐻</ci><ci >𝑊</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply></apply></apply><ci
    ><mtext >否则</mtext></ci><apply ><ci  >Σ</ci><apply ><apply ><apply ><cn type="integer"  >1</cn><apply
    ><ci >𝐻</ci><ci >𝑊</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑛</ci></apply><ci >𝜇</ci></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply><ci
    >𝜇</ci></apply><ci >𝑇</ci></apply></apply><apply ><ci >𝛼</ci><ci >𝐼</ci></apply></apply></apply><ci
    ><mtext >否则</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{cases}\mu=\frac{1}{HW}x_{n},\\ \Sigma=\frac{1}{HW}(x_{n}-\mu)(x_{n}-\mu)^{T}+\alpha{I},\\
    \end{cases}</annotation></semantics></math> |  | (30) |
- en: where $\alpha$ and $I$ denote a small positive number and identity matrix, respectively.
    By using the instance whitening operation, the influence of diverse water types
    can be reduced. An adversarial process involving latent variable analysis is used
    by DAL [[119](#bib.bib119)] to disentangle the unwanted nuisances corresponding
    to water types. A nuisance classifier is designed by DAL, which classifies the
    water type of the distorted image according to its latent vector. IACC [[165](#bib.bib165)]
    designs an underwater convolution module that can learn channel-specific features
    and adapt to diverse underwater environments by leveraging the mini-batch insensitivity
    of instance normalization.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha$和$I$分别表示一个小的正数和单位矩阵。通过使用实例白化操作，可以减少不同水类型的影响。DAL [[119](#bib.bib119)]通过潜变量分析的对抗过程来解开与水类型相关的不必要干扰。DAL设计了一个干扰分类器，根据图像的潜在向量对扭曲图像的水类型进行分类。IACC
    [[165](#bib.bib165)]设计了一个水下卷积模块，可以通过利用实例归一化的迷你批次不敏感性来学习通道特定特征，并适应不同的水下环境。
- en: III-F5 Multi-input Fusion
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-F5 多输入融合
- en: Research [[74](#bib.bib74)] shows that preprocessing input can be beneficial
    for the UIE task. WaterNet [[74](#bib.bib74)] applies White Balance (WB), Histogram
    Equalization (HE) and Gamma Correction (GC) algorithms to distorted underwater
    images. The WB may adjust the color distortion. The HE and GC can increase contrast
    and optimize dark regions. A gated fusion approach is used by WaterNet to obtain
    an integrated output. The gated enhanced result is
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 研究[[74](#bib.bib74)]显示，预处理输入对UIE任务可能有利。WaterNet [[74](#bib.bib74)]应用了白平衡（WB）、直方图均衡化（HE）和伽马校正（GC）算法来处理扭曲的水下图像。WB可能会调整颜色失真。HE和GC可以增加对比度并优化暗区。WaterNet采用了门控融合方法来获得综合输出。门控增强结果是
- en: '|  | $\hat{y}=R_{WB}\odot C_{WB}+R_{HE}\odot C_{HE}+R_{GC}\odot C_{GC},$ |  |
    (31) |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}=R_{WB}\odot C_{WB}+R_{HE}\odot C_{HE}+R_{GC}\odot C_{GC},$ |  |
    (31) |'
- en: where $R_{WB}$, $R_{HE}$ and $R_{GC}$ mean the refined images obtained by the
    corresponding pre-process methods. The $C_{WB}$, $C_{HE}$ and $C_{GC}$ denote
    the learned confidence maps. The $\odot$ is element-wise production. MFEF [[169](#bib.bib169)]
    and F2UIE [[121](#bib.bib121)] utilize the WB and Contrast-limited Adaptive Histogram
    Equalization (CLAHE) to obtain high-quality input with better contrast.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$R_{WB}$、$R_{HE}$和$R_{GC}$表示通过相应的预处理方法获得的精细图像。$C_{WB}$、$C_{HE}$和$C_{GC}$表示学习到的置信度图。$\odot$是逐元素乘积。MFEF
    [[169](#bib.bib169)]和F2UIE [[121](#bib.bib121)]利用WB和对比度限制自适应直方图均衡化（CLAHE）来获得高质量的输入，具有更好的对比度。
- en: '![Refer to caption](img/0915934e88086c9db7864c81d081797d.png)![Refer to caption](img/b470d6e564d4271aa258092caf46918f.png)![Refer
    to caption](img/2828e8ed817a7544be826309ebcdad15.png)![Refer to caption](img/7e7e48c0bc247e674ecc3aaf54c0cb35.png)![Refer
    to caption](img/e50236e40b9ca689f243eb5dadae4e1c.png)![Refer to caption](img/1cf4feca048b8a66201c9eb2801b0987.png)![Refer
    to caption](img/8a8008684e8361a024afb244a6ea9abc.png)![Refer to caption](img/2358aedba316db7c5fad377debe42ace.png)![Refer
    to caption](img/be37f24d4f38e99211adb1862ee9d3a8.png)![Refer to caption](img/b96e3a5bcc8e00412d5585f7df770993.png)![Refer
    to caption](img/0b5bfa5a4896aadf23e48c22a0a7f863.png)![Refer to caption](img/7175da679ba0e41e2bfc639ca31206e0.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0915934e88086c9db7864c81d081797d.png)![参见说明](img/b470d6e564d4271aa258092caf46918f.png)![参见说明](img/2828e8ed817a7544be826309ebcdad15.png)![参见说明](img/7e7e48c0bc247e674ecc3aaf54c0cb35.png)![参见说明](img/e50236e40b9ca689f243eb5dadae4e1c.png)![参见说明](img/1cf4feca048b8a66201c9eb2801b0987.png)![参见说明](img/8a8008684e8361a024afb244a6ea9abc.png)![参见说明](img/2358aedba316db7c5fad377debe42ace.png)![参见说明](img/be37f24d4f38e99211adb1862ee9d3a8.png)![参见说明](img/b96e3a5bcc8e00412d5585f7df770993.png)![参见说明](img/0b5bfa5a4896aadf23e48c22a0a7f863.png)![参见说明](img/7175da679ba0e41e2bfc639ca31206e0.png)'
- en: Distortion              UWNet              FUnIEGAN            WaterNet            
    ADMNNet             UColor
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 失真              UWNet              FUnIEGAN            WaterNet            
    ADMNNet             UColor
- en: '![Refer to caption](img/7671d258b13a7f6919e1d35be6bb58a6.png)![Refer to caption](img/a1a1d86c583bcd4e450299f79d0a0f17.png)![Refer
    to caption](img/5806af330dbf636987885bcaf84f3195.png)![Refer to caption](img/1a6cf0389ffd9e94bc9fd919a84ad003.png)![Refer
    to caption](img/afbaa027582e8654db14fd3ebd9706ee.png)![Refer to caption](img/43a352dfb715334da44b3a0c86660a0a.png)![Refer
    to caption](img/db833ddb00063ad2899f7d30086cbbc7.png)![Refer to caption](img/670097c2f037fe17bac3f8be0192ec9e.png)![Refer
    to caption](img/2d13f0040c2bc6f296f4382e1515f21a.png)![Refer to caption](img/f93cc42282253a9eff0d9db7fd8baec8.png)![Refer
    to caption](img/38b913c61e618508c16a5d89a793cc81.png)![Refer to caption](img/afb5c52bb02b8d2445efd23af44ea212.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7671d258b13a7f6919e1d35be6bb58a6.png)![参见说明](img/a1a1d86c583bcd4e450299f79d0a0f17.png)![参见说明](img/5806af330dbf636987885bcaf84f3195.png)![参见说明](img/1a6cf0389ffd9e94bc9fd919a84ad003.png)![参见说明](img/afbaa027582e8654db14fd3ebd9706ee.png)![参见说明](img/43a352dfb715334da44b3a0c86660a0a.png)![参见说明](img/db833ddb00063ad2899f7d30086cbbc7.png)![参见说明](img/670097c2f037fe17bac3f8be0192ec9e.png)![参见说明](img/2d13f0040c2bc6f296f4382e1515f21a.png)![参见说明](img/f93cc42282253a9eff0d9db7fd8baec8.png)![参见说明](img/38b913c61e618508c16a5d89a793cc81.png)![参见说明](img/afb5c52bb02b8d2445efd23af44ea212.png)'
- en: UGAN              SGUIE                 UIE-WD               SCNet                
    STSC                  U-Trans
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: UGAN              SGUIE                 UIE-WD               SCNet                
    STSC                  U-Trans
- en: '![Refer to caption](img/f7a2512cbb5fc4670a1d0a72d1ec6bb5.png)![Refer to caption](img/af67abe19e6813b733f44718a1b809ff.png)![Refer
    to caption](img/4576b303e1ec02fd5f35bd2c1a98f7cc.png)![Refer to caption](img/fa2c836da44827c030948d9d08764382.png)![Refer
    to caption](img/86f3192d6a53797542c1c1b91c540884.png)![Refer to caption](img/21cb3438a1a186bd88b11c6521199294.png)![Refer
    to caption](img/14259c1102f91f097928a13d79f4f61b.png)![Refer to caption](img/cc247e39584e9c3b05eeb575a6a051df.png)![Refer
    to caption](img/db8bfa3faeb4618059977593156b8d27.png)![Refer to caption](img/8e94a1f2a69522f89452341efd2f7bac.png)![Refer
    to caption](img/a97c8c2a1f5f2d658e205c33ac606395.png)![Refer to caption](img/a4f66d45fe397f9103e01499957151d6.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f7a2512cbb5fc4670a1d0a72d1ec6bb5.png)![参见说明](img/af67abe19e6813b733f44718a1b809ff.png)![参见说明](img/4576b303e1ec02fd5f35bd2c1a98f7cc.png)![参见说明](img/fa2c836da44827c030948d9d08764382.png)![参见说明](img/86f3192d6a53797542c1c1b91c540884.png)![参见说明](img/21cb3438a1a186bd88b11c6521199294.png)![参见说明](img/14259c1102f91f097928a13d79f4f61b.png)![参见说明](img/cc247e39584e9c3b05eeb575a6a051df.png)![参见说明](img/db8bfa3faeb4618059977593156b8d27.png)![参见说明](img/8e94a1f2a69522f89452341efd2f7bac.png)![参见说明](img/a97c8c2a1f5f2d658e205c33ac606395.png)![参见说明](img/a4f66d45fe397f9103e01499957151d6.png)'
- en: CECF              Semi-UIR              UIE-DM              HCLR-Net            
    GUPDM              Reference
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: CECF              Semi-UIR              UIE-DM              HCLR-Net            
    GUPDM              参考
- en: 'Figure 8: Visual results obtained on UIEB full-reference test set. The curve
    figure denotes the pixel histogram. The bar figure gives the average pixel value
    of each channel.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：在UIEB全参考测试集上获得的视觉结果。曲线图表示像素直方图。柱状图显示每个通道的平均像素值。
- en: '![Refer to caption](img/7610932a375b2bf8838826bd87d2fe5c.png)![Refer to caption](img/1b5b381c7f3b5131fa3b2228250d5fe9.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7610932a375b2bf8838826bd87d2fe5c.png)![参见说明](img/1b5b381c7f3b5131fa3b2228250d5fe9.png)'
- en: (a) Challenge-60                                                               
    (b) U45
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Challenge-60                                                               
    (b) U45
- en: '![Refer to caption](img/cc013fd57c087bfb27927fb64e3ba1e2.png)![Refer to caption](img/fed1ba509e715c915281e740c727f5a0.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cc013fd57c087bfb27927fb64e3ba1e2.png)![参见说明](img/fed1ba509e715c915281e740c727f5a0.png)'
- en: (c) UCCS                                                                    
    (d) EUVP-330
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: (c) UCCS                                                                    
    (d) EUVP-330
- en: 'Figure 9: URANKER values on no-reference benchmark datasets. The horizontal
    dashed line represents the mean of the quantitative results obtained by all algorithms.
    The correspondence between the letters on the horizontal axis and the algorithm
    is: A (FUnIEGAN), B (WaterNet), C (ADMNNet), D(UColor), E (UGAN), F (SGUIE), G
    (UIE-WD), H (SCNet), I (STSC), J (U-Trans), K (CECF), L (Semi-UIR), M (UIE-DM),
    N (HCLR-Net) and O (GUPDM). Best results are in red font and second best results
    are with blue font.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：在无参考基准数据集上的URANKER值。水平虚线表示所有算法获得的定量结果的均值。水平轴上的字母与算法的对应关系是：A（FUnIEGAN）、B（WaterNet）、C（ADMNNet）、D（UColor）、E（UGAN）、F（SGUIE）、G（UIE-WD）、H（SCNet）、I（STSC）、J（U-Trans）、K（CECF）、L（Semi-UIR）、M（UIE-DM）、N（HCLR-Net）和O（GUPDM）。最佳结果用红色字体表示，第二最佳结果用蓝色字体表示。
- en: '![Refer to caption](img/044ffddce626c178f2d3cfdf246120ff.png)![Refer to caption](img/1c45fbcc18253eec2fd2dfb40c6e7a23.png)![Refer
    to caption](img/363e00d3bd98615dde6e06e7e6fe82f0.png)![Refer to caption](img/2d4bc6dbccff270c7a63c7ea578d1fab.png)![Refer
    to caption](img/9777089af4ac16a610b3bc3633f94955.png)![Refer to caption](img/42ad8d93c8b829b210deb3c615f47d80.png)![Refer
    to caption](img/9aeb866eeab6e4d017f283fd684861b3.png)![Refer to caption](img/de0bb070b09bfe8928ac6a20c4e2712d.png)![Refer
    to caption](img/50e9bd42a8ff11a2e7ecf8e2830ca765.png)![Refer to caption](img/4c3a6ea50cd565c897f416a563686195.png)![Refer
    to caption](img/633c2ded4b6c623efba91379b9723ff6.png)![Refer to caption](img/914a55d519100847b3f660205b184e4c.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/044ffddce626c178f2d3cfdf246120ff.png)![参见说明](img/1c45fbcc18253eec2fd2dfb40c6e7a23.png)![参见说明](img/363e00d3bd98615dde6e06e7e6fe82f0.png)![参见说明](img/2d4bc6dbccff270c7a63c7ea578d1fab.png)![参见说明](img/9777089af4ac16a610b3bc3633f94955.png)![参见说明](img/42ad8d93c8b829b210deb3c615f47d80.png)![参见说明](img/9aeb866eeab6e4d017f283fd684861b3.png)![参见说明](img/de0bb070b09bfe8928ac6a20c4e2712d.png)![参见说明](img/50e9bd42a8ff11a2e7ecf8e2830ca765.png)![参见说明](img/4c3a6ea50cd565c897f416a563686195.png)![参见说明](img/633c2ded4b6c623efba91379b9723ff6.png)![参见说明](img/914a55d519100847b3f660205b184e4c.png)'
- en: Distortion              UWNet              PhysicalNN           FUnIEGAN          
    WaterNet             ADMNNet
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Distortion              UWNet              PhysicalNN           FUnIEGAN          
    WaterNet             ADMNNet
- en: '![Refer to caption](img/5963076e6ddc95e80cb1b7039690bc74.png)![Refer to caption](img/0d5ac138ad567254996a6e48ba028f24.png)![Refer
    to caption](img/efa18d17b0da8a00a8736a4ce1799759.png)![Refer to caption](img/3b1aa67038e23fb323558dbba3ece45e.png)![Refer
    to caption](img/c0fc43be8b25ae15c3ff89dd2a510a2f.png)![Refer to caption](img/2ed2dbc3568347882663e5bee5931516.png)![Refer
    to caption](img/c7c65beef56c1c41a4ca2814a2dd9030.png)![Refer to caption](img/517c0573381c15adfac19254177abc7e.png)![Refer
    to caption](img/9279e499654ff97154ab319b6b2197d3.png)![Refer to caption](img/b9804cc7f4b87cf1578cbdb2b3d5f17b.png)![Refer
    to caption](img/ca047d79938409d38ada405c8dae2664.png)![Refer to caption](img/6d14cf974b4ade3fc50bd23ee4ba7f87.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5963076e6ddc95e80cb1b7039690bc74.png)![参见说明](img/0d5ac138ad567254996a6e48ba028f24.png)![参见说明](img/efa18d17b0da8a00a8736a4ce1799759.png)![参见说明](img/3b1aa67038e23fb323558dbba3ece45e.png)![参见说明](img/c0fc43be8b25ae15c3ff89dd2a510a2f.png)![参见说明](img/2ed2dbc3568347882663e5bee5931516.png)![参见说明](img/c7c65beef56c1c41a4ca2814a2dd9030.png)![参见说明](img/517c0573381c15adfac19254177abc7e.png)![参见说明](img/9279e499654ff97154ab319b6b2197d3.png)![参见说明](img/b9804cc7f4b87cf1578cbdb2b3d5f17b.png)![参见说明](img/ca047d79938409d38ada405c8dae2664.png)![参见说明](img/6d14cf974b4ade3fc50bd23ee4ba7f87.png)'
- en: UColor               UGAN               SGUIE                 UIE-WD               
    SCNet                 STSC
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: UColor               UGAN               SGUIE                 UIE-WD               
    SCNet                 STSC
- en: '![Refer to caption](img/e638cdd94ce65ddb819000bbbcca7893.png)![Refer to caption](img/7bbaee969af052fc72d0a2103f3b0f20.png)![Refer
    to caption](img/489112061d2fc2bbf713e190f0893625.png)![Refer to caption](img/509d5866c1c6786ee319da3ad3dcb98c.png)![Refer
    to caption](img/690a14f72dfd7c293f6b0de64e3ab999.png)![Refer to caption](img/245eb71f4b252e180a3bda2f018f7dea.png)![Refer
    to caption](img/5b9659a9e6e4664a782eeefc67016870.png)![Refer to caption](img/416be6abfb66d35ff580735a239a6fee.png)![Refer
    to caption](img/6e354b8c031bdb25730c30830574a4e8.png)![Refer to caption](img/e2e23a5c471bba644eccf2323038b674.png)![Refer
    to caption](img/3a7d3cbe548573375af4e5f425a999f5.png)![Refer to caption](img/efe5ef3f9891a7c30bdb51f53d23fe53.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e638cdd94ce65ddb819000bbbcca7893.png)![参见说明](img/7bbaee969af052fc72d0a2103f3b0f20.png)![参见说明](img/489112061d2fc2bbf713e190f0893625.png)![参见说明](img/509d5866c1c6786ee319da3ad3dcb98c.png)![参见说明](img/690a14f72dfd7c293f6b0de64e3ab999.png)![参见说明](img/245eb71f4b252e180a3bda2f018f7dea.png)![参见说明](img/5b9659a9e6e4664a782eeefc67016870.png)![参见说明](img/416be6abfb66d35ff580735a239a6fee.png)![参见说明](img/6e354b8c031bdb25730c30830574a4e8.png)![参见说明](img/e2e23a5c471bba644eccf2323038b674.png)![参见说明](img/3a7d3cbe548573375af4e5f425a999f5.png)![参见说明](img/efe5ef3f9891a7c30bdb51f53d23fe53.png)'
- en: U-Trans              CECF               Semi-UIR              UIE-DM             
    HCLR-Net             GUPDM
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: U-Trans              CECF               Semi-UIR              UIE-DM             
    HCLR-Net             GUPDM
- en: 'Figure 10: Visual results obtained on challenge-60\. The curve figure denotes
    the pixel histogram. The bar figure gives the average pixel value of each channel.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 在 challenge-60 上获得的视觉结果。曲线图表示像素直方图。条形图给出了每个通道的平均像素值。'
- en: IV Experiments
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: To promote research on the UIE task, we provide a fair evaluation of UIE algorithms
    that have been proven to be effective on benchmark underwater datasets.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了推动 UIE 任务的研究，我们提供了对在基准水下数据集上已被证明有效的 UIE 算法的公平评估。
- en: IV-A Settings
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 设置
- en: Algorithms. The DL-UIE algorithms include UWNet [[99](#bib.bib99)], PhysicalNN
    [[15](#bib.bib15)], FUnIEGAN [[53](#bib.bib53)], WaterNet [[74](#bib.bib74)],
    ADMNNet [[141](#bib.bib141)], UColor [[71](#bib.bib71)], UGAN [[28](#bib.bib28)],
    SGUIE [[105](#bib.bib105)], UIE-WD [[94](#bib.bib94)], SCNet [[30](#bib.bib30)],
    STSC [[122](#bib.bib122)], U-Trans [[102](#bib.bib102)], CECF [[19](#bib.bib19)],
    Semi-UIR [[47](#bib.bib47)], UIE-DM [[117](#bib.bib117)], HCLR-Net [[168](#bib.bib168)]
    and GUPDM [[98](#bib.bib98)]. They are all open source code implemented in PyTorch.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 算法。DL-UIE算法包括UWNet [[99](#bib.bib99)]、PhysicalNN [[15](#bib.bib15)]、FUnIEGAN
    [[53](#bib.bib53)]、WaterNet [[74](#bib.bib74)]、ADMNNet [[141](#bib.bib141)]、UColor
    [[71](#bib.bib71)]、UGAN [[28](#bib.bib28)]、SGUIE [[105](#bib.bib105)]、UIE-WD [[94](#bib.bib94)]、SCNet
    [[30](#bib.bib30)]、STSC [[122](#bib.bib122)]、U-Trans [[102](#bib.bib102)]、CECF
    [[19](#bib.bib19)]、Semi-UIR [[47](#bib.bib47)]、UIE-DM [[117](#bib.bib117)]、HCLR-Net
    [[168](#bib.bib168)]和GUPDM [[98](#bib.bib98)]。它们都是在PyTorch中实现的开源代码。
- en: Metrics. The full-reference metrics include PSNR and SSIM. The no-reference
    metrics include UIQM [[101](#bib.bib101)], UCIQE [[146](#bib.bib146)] and URANKER
    [[37](#bib.bib37)]. For all experimental analyzes and discussions below, we assume
    that the quantitative evaluation metrics that widely used in existing literatures
    are reliable. Otherwise, we can not draw any conclusions.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 指标。全参考指标包括PSNR和SSIM。无参考指标包括UIQM [[101](#bib.bib101)]、UCIQE [[146](#bib.bib146)]和URANKER
    [[37](#bib.bib37)]。对于以下所有实验分析和讨论，我们假设现有文献中广泛使用的定量评价指标是可靠的。否则，我们无法得出任何结论。
- en: Datasets. The benchmark datasets with paired images include UIEB [[74](#bib.bib74)],
    LSUI [[102](#bib.bib102)], {EUVP-D/EUVP-I/EUVP-S} from EUVP [[53](#bib.bib53)]
    and UFO-120 [[51](#bib.bib51)]. The benchmark datasets without references include
    Challenge-60 from UIEB [[74](#bib.bib74)], U45 [[76](#bib.bib76)], UCCS from RUIE
    [[85](#bib.bib85)], and EUVP-330 from EUVP [[53](#bib.bib53)]. Different semi-supervised
    algorithms require different types of external data. In order to ensure fairness
    as much as possible, we use the pretrained models on the UIEB (full-reference
    training set) dataset for the performance test of no-reference datasets.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。带有配对图像的基准数据集包括UIEB [[74](#bib.bib74)]、LSUI [[102](#bib.bib102)]、EUVP [[53](#bib.bib53)]中的{EUVP-D/EUVP-I/EUVP-S}以及UFO-120
    [[51](#bib.bib51)]。不带参考的基准数据集包括UIEB [[74](#bib.bib74)]中的Challenge-60、U45 [[76](#bib.bib76)]、RUIE
    [[85](#bib.bib85)]中的UCCS以及EUVP [[53](#bib.bib53)]中的EUVP-330。不同的半监督算法需要不同类型的外部数据。为了尽可能确保公平，我们使用了UIEB（全参考训练集）数据集上的预训练模型来测试无参考数据集的性能。
- en: Factor Settings. In order to do our best to keep the comparative experiments
    fair, we have unified the following factors that may affect the experimental results.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 因素设置。为了尽力保持比较实验的公平性，我们统一了以下可能影响实验结果的因素。
- en: •
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The batch size is set to 8.
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量大小设置为8。
- en: •
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The image size in both training and testing phases is set to $256\times 256$.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练和测试阶段中的图像大小设置为$256\times 256$。
- en: •
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Three data augmentation methods are used, namely horizontal random flipping,
    vertical random flipping and random cropping.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用了三种数据增强方法，即水平随机翻转、垂直随机翻转和随机裁剪。
- en: •
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The feature extraction part of any model does not use pre-trained patterns guided
    by external data.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任何模型的特征提取部分不使用由外部数据指导的预训练模式。
- en: •
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The partition ratio of training and testing data may be different in existing
    literature. Here we use a uniform ratio to divide the training and test data.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练和测试数据的划分比例在现有文献中可能有所不同。这里我们使用统一的比例来划分训练和测试数据。
- en: •
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Different models may require different initial learning rates, learning rate
    decay strategies, and optimizers. We follow the settings given in their respective
    papers. We did not conduct additional hyperparameter searches for either model.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同模型可能需要不同的初始学习率、学习率衰减策略和优化器。我们遵循各自论文中给出的设置。我们没有对任何模型进行额外的超参数搜索。
- en: •
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In order to prevent differences in metric values caused by different toolbox
    or implementation [[14](#bib.bib14)], we unified the code for metric calculations.
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了防止因不同工具箱或实现导致的度量值差异[[14](#bib.bib14)]，我们统一了度量计算的代码。
- en: IV-B Comparison among DL-UIE Algorithms
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B DL-UIE算法之间的比较
- en: Evaluation of Fitting Ability on Full-Reference Benchmark Datasets. Table LABEL:tab:full_reference_results
    shows the results obtained by various DL-UIE algorithms on datasets with paired
    data. Overall, the quantitative evaluation metrics obtained by UIE-DM are the
    best. However, on most datasets, there is no significant difference in the quantitative
    evaluation results (i.e., PSNR and SSIM) obtained by the top 5 algorithms. What
    is particularly noteworthy is that for EUVP-D, EUVP-I and EUVP-S, the PSNR and
    SSIM of the top 10 algorithms are approaching the same level. The visualization
    results in Fig. [8](#S3.F8 "Figure 8 ‣ III-F5 Multi-input Fusion ‣ III-F Disentanglement
    & Fusion ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning") also illustrate that multiple DL-UIE algorithms achieve
    similar visual effects. Not only that, the color histogram also shows that the
    results obtained by different algorithms are close in terms of pixel statistics.
    For these UIE algorithms, the representation capabilities of the neural networks
    they designed with different architectures may be very close. This phenomenon
    implies that the pursuit of higher PSNR and SSIM may be difficult.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在完整参考基准数据集上的拟合能力评估。表格 LABEL:tab:full_reference_results 显示了各种 DL-UIE 算法在配对数据集上的结果。总体而言，UIE-DM
    获得的定量评估指标是最好的。然而，在大多数数据集上，前 5 名算法获得的定量评估结果（即 PSNR 和 SSIM）没有显著差异。特别值得注意的是，对于 EUVP-D、EUVP-I
    和 EUVP-S，前 10 名算法的 PSNR 和 SSIM 接近于同一水平。图 [8](#S3.F8 "Figure 8 ‣ III-F5 Multi-input
    Fusion ‣ III-F Disentanglement & Fusion ‣ III UIE Methods ‣ A Comprehensive Survey
    on Underwater Image Enhancement Based on Deep Learning") 的可视化结果也表明，多个 DL-UIE 算法实现了类似的视觉效果。不仅如此，颜色直方图也显示不同算法获得的结果在像素统计方面接近。对于这些
    UIE 算法，它们设计的具有不同架构的神经网络的表现能力可能非常接近。这一现象暗示追求更高的 PSNR 和 SSIM 可能会很困难。
- en: Evaluation of Generalization Ability on No-Reference Benchmark Datasets. We
    tested two kinds no-reference metrics. The first is the manually calculated metrics
    UIQM and UCIQE, which are shown in Table LABEL:tab:non_reference_results. The
    UIQM and UCIQE values of different DL-UIE algorithms given in Table LABEL:tab:non_reference_results
    show that UGAN achieves the overall best performance. Meanwhile, in terms of the
    evaluation of generalization ability, the UIQM and UCIQE values obtained by different
    algorithms are close. The second is URANKER obtained by data-driven training,
    which is given in Fig. [9](#S3.F9 "Figure 9 ‣ III-F5 Multi-input Fusion ‣ III-F
    Disentanglement & Fusion ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning"). The best URANKER values are obtained
    by M (UIE-DM), K (CECF), E (UGAN), and I (STSC), respectively. The second best
    URANKER values are obtained by K (CECF), I (STSC), H (SCNet), M (UIE-DM) respectively.
    For the evaluation of URANKER, UIE-DM has the best overall evaluation effect.
    Considering Table LABEL:tab:non_reference_results and Fig. [9](#S3.F9 "Figure
    9 ‣ III-F5 Multi-input Fusion ‣ III-F Disentanglement & Fusion ‣ III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")
    simultaneously, the generalization ability of UGAN is the best. Fig. [10](#S3.F10
    "Figure 10 ‣ III-F5 Multi-input Fusion ‣ III-F Disentanglement & Fusion ‣ III
    UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based on
    Deep Learning") shows the visual enhancement results of the evaluation of generalization
    ability. There are obvious differences in the visual results and pixel histograms
    of images obtained by different algorithms. Some enhanced images still show blue-green
    of green effect.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 无参考基准数据集上的泛化能力评估。我们测试了两种无参考度量指标。第一种是手动计算的指标 UIQM 和 UCIQE，如表 LABEL:tab:non_reference_results
    所示。表 LABEL:tab:non_reference_results 中不同 DL-UIE 算法的 UIQM 和 UCIQE 值显示，UGAN 实现了整体最佳性能。同时，在泛化能力评估方面，不同算法获得的
    UIQM 和 UCIQE 值接近。第二种是通过数据驱动训练获得的 URANKER，如图 [9](#S3.F9 "Figure 9 ‣ III-F5 Multi-input
    Fusion ‣ III-F Disentanglement & Fusion ‣ III UIE Methods ‣ A Comprehensive Survey
    on Underwater Image Enhancement Based on Deep Learning") 所示。最佳的 URANKER 值由 M (UIE-DM)、K
    (CECF)、E (UGAN) 和 I (STSC) 分别获得。第二好的 URANKER 值分别由 K (CECF)、I (STSC)、H (SCNet)
    和 M (UIE-DM) 获得。在 URANKER 的评估中，UIE-DM 拥有最佳的整体评估效果。综合考虑表 LABEL:tab:non_reference_results
    和图 [9](#S3.F9 "Figure 9 ‣ III-F5 Multi-input Fusion ‣ III-F Disentanglement &
    Fusion ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning")，UGAN 的泛化能力最佳。图 [10](#S3.F10 "Figure 10 ‣ III-F5 Multi-input
    Fusion ‣ III-F Disentanglement & Fusion ‣ III UIE Methods ‣ A Comprehensive Survey
    on Underwater Image Enhancement Based on Deep Learning") 展示了泛化能力评估的视觉增强结果。不同算法获得的图像在视觉效果和像素直方图上存在明显差异。一些增强的图像仍显示出蓝绿色效果。
- en: Overall Conclusions. Under our experimental settings, UIE-DM and UGAN achieved
    the best performance in fitting ability on full-reference datasets and generalization
    ability on no-reference datasets, respectively.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 总体结论。在我们的实验设置下，UIE-DM 和 UGAN 在全参考数据集上的拟合能力和在无参考数据集上的泛化能力分别取得了最佳性能。
- en: V Future Work
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 未来工作
- en: According to our discussion of existing progress and experiments, there are
    still challenging problems that remain unsolved. We raise the following issues
    worthy of study.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对现有进展和实验的讨论，仍然存在一些未解决的挑战性问题。我们提出以下值得研究的问题。
- en: •
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Towards high-quality pairwise data synthesis by game engines. Constructing a
    large-scale database contains real-world paired underwater images is almost impossible.
    Existing methods that synthesizing data by algorithms are difficult to accurately
    simulate different influencing factors, such as illumination intensity, number
    of suspended particles, water depth, scene content, etc. For game engines that
    can customize extended functions, controlling influencing factors is an inherent
    advantage. For example, Liu et al. [[88](#bib.bib88)] use the UNREAL game engine
    to simulate non-uniform lighting, low-illumination, multiple light sources, diverse
    scene content and hazy effects at nighttime. It is worth exploring how game engines
    can be used to build datasets for the UIE task.
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过游戏引擎进行高质量的成对数据合成。构建一个包含真实世界配对水下图像的大规模数据库几乎是不可能的。现有的通过算法合成数据的方法很难准确模拟不同的影响因素，如光照强度、悬浮颗粒数量、水深、场景内容等。对于能够自定义扩展功能的游戏引擎，控制这些影响因素是一个固有的优势。例如，刘等人[[88](#bib.bib88)]使用UNREAL游戏引擎模拟非均匀光照、低光照、多光源、多样化场景内容以及夜间雾霾效果。值得探索的是，游戏引擎如何用于构建UIE任务的数据集。
- en: •
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Effects on downstream vision tasks. High-level vision tasks, or be called as
    downstream tasks of the UIE task, have been used by existing research as a strategy
    to evaluate the performance of the UIE model itself. An intuitive and widely adopted
    hypothesis is that the UIE model’s ability to enhance the distorted images is
    positively related to its ability to facilitate downstream tasks. However, a recent
    study [[126](#bib.bib126)] on the object detection task and the UIE task discovered
    a surprising phenomenon, and here we directly quote their conclusion, “One of
    the most significant findings is that underwater image enhancement suppresses
    the performance of object detection.”. A comprehensive research on the correlation
    between the performance of the UIE and downstream tasks may provides a reliable
    conclusion.
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对下游视觉任务的影响。高级视觉任务，或称为UIE任务的下游任务，已经被现有研究用作评估UIE模型性能的策略。一个直观且被广泛接受的假设是，UIE模型增强失真图像的能力与其促进下游任务的能力正相关。然而，一项最近的研究[[126](#bib.bib126)]在物体检测任务和UIE任务上发现了一个令人惊讶的现象，这里我们直接引用他们的结论，“最显著的发现之一是水下图像增强抑制了物体检测的性能。”。对UIE性能与下游任务之间相关性的全面研究可能会提供一个可靠的结论。
- en: •
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Cooperation with large-scale pretrained vision-language models. The texture
    and local semantics contained in underwater images have been extensively studied.
    However, the global semantics that may be provided by language models have not
    been embedded into UIE models by existing research. Large-scale pretrained vision-language
    models, such as CLIP [[107](#bib.bib107), [154](#bib.bib154)], can provide human-level
    high-level semantic information. Text-image multi-modal restoration models have
    been partially studied in rain and haze removal [[93](#bib.bib93)]. Language features
    may facilitate the performance of UIE models.
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与大规模预训练视觉-语言模型的合作。水下图像中的纹理和局部语义已经被广泛研究。然而，现有研究尚未将语言模型可能提供的全球语义嵌入到UIE模型中。大规模预训练的视觉-语言模型，如CLIP[[107](#bib.bib107),
    [154](#bib.bib154)]，可以提供人类水平的高级语义信息。文本-图像多模态恢复模型在雨雾去除[[93](#bib.bib93)]方面已经得到了一定程度的研究。语言特征可能有助于提升UIE模型的性能。
- en: •
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Non-uniform illumination. Artificial light sources are used by underwater equipment
    when the depth of the water exceeds the range that can obtain a properly illuminated
    image. However, unlike the smoothness of natural light, the illumination of images
    captured under artificial light sources may be non-uniform [[43](#bib.bib43)].
    The enhancement task under non-uniform lighting is worth digging into.
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非均匀光照。水下设备在水深超过能够获得适当光照图像的范围时会使用人工光源。然而，与自然光的平滑度不同，人工光源下拍摄的图像光照可能是不均匀的[[43](#bib.bib43)]。在非均匀光照下的增强任务值得深入研究。
- en: •
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reliable evaluation metrics. The difference between the enhanced result and
    the reference can be reliably evaluated by using full-reference evaluation metrics.
    However, obtaining ground-truth in the underwater imaging process is extremely
    challenging. Therefore, numerous literatures use no-reference evaluation metrics
    to evaluate the effectiveness of their proposed algorithms. The reliability of
    no-reference metrics needs to be consistent with human subjective aesthetics.
    Currently, in-depth research on related topics is still scarce. More research
    about subjective and objective evaluations are worthy of conduct.
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可靠的评估指标。使用全参考评估指标可以可靠地评估增强结果与参考结果之间的差异。然而，在水下成像过程中获取真实数据极具挑战性。因此，许多文献使用无参考评估指标来评估其提出的算法的有效性。无参考指标的可靠性需要与人类主观审美一致。目前，相关主题的深入研究仍然稀缺。值得进行更多关于主观和客观评估的研究。
- en: •
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Combination with other image restoration tasks. As we discuss in this paper,
    there are inherent differences between the UIE task and other image restoration
    tasks. Preliminary attempts [[70](#bib.bib70)] have been made to combine tasks
    such as haze, rain and noise removal. The low-level features learned by different
    in-air image restoration models may be beneficial to each other. Utilizing auxiliary
    knowledge provided by other restoration tasks may improve the performance of UIE
    models.
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与其他图像恢复任务的结合。正如我们在本文中讨论的，UIE任务与其他图像恢复任务之间存在固有差异。已经进行了一些初步尝试 [[70](#bib.bib70)]
    来结合诸如雾霾、雨水和噪声去除等任务。不同的空中图像恢复模型学习到的低级特征可能对彼此有益。利用其他恢复任务提供的辅助知识可能会提高UIE模型的性能。
- en: VI Conclusion
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: In this paper, we attempt to conduct a systematic review of the research of
    the underwater image enhancement task. We first investigated the research background
    and related work, which include physical models describing the degradation process,
    data construction strategies for model training, evaluation metrics implemented
    from different perspectives, and commonly used loss functions. We then provide
    a comprehensive taxonomy of existing UIE algorithms. According to the main contributions
    of each algorithm, state-of-the-art algorithms are discussed and analyzed from
    different perspectives. Further, we perform quantitative and qualitative evaluations
    on multiple benchmark datasets containing synthetic and real-world distorted underwater
    images. Finally, based on our summary of algorithms and analysis of the experiments,
    open issues and challenging topics are raised.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们尝试对水下图像增强任务的研究进行系统综述。我们首先调查了研究背景和相关工作，包括描述退化过程的物理模型、用于模型训练的数据构建策略、从不同角度实施的评估指标以及常用的损失函数。然后，我们提供了现有UIE算法的综合分类。根据每种算法的主要贡献，最先进的算法从不同角度进行讨论和分析。此外，我们对包含合成和现实世界失真水下图像的多个基准数据集进行了定量和定性评估。最后，根据我们对算法的总结和实验分析，提出了未解决的问题和挑战性话题。
- en: References
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Agarwal, S. Gupta, and M. Vashishath. Contrast enhancement of underwater
    images using conditional generative adversarial network. Multimedia Tools and
    Applications, pages 1–30, 2023.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Agarwal, S. Gupta, and M. Vashishath. 使用条件生成对抗网络增强水下图像的对比度。《多媒体工具与应用》，第1–30页，2023年。'
- en: '[2] D. Akkaynak and T. Treibitz. A revised underwater image formation model.
    In IEEE Conference on Computer Vision and Pattern Recognition, pages 6723–6732,
    2018.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Akkaynak 和 T. Treibitz. 修订的水下图像形成模型。发表于 IEEE 计算机视觉与模式识别会议，第6723–6732页，2018年。'
- en: '[3] D. Akkaynak and T. Treibitz. Sea-thru: A method for removing water from
    underwater images. In IEEE Conference on Computer Vision and Pattern Recognition,
    pages 1682–1691, 2019.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] D. Akkaynak 和 T. Treibitz. Sea-thru：一种从水下图像中去除水的的方法。发表于 IEEE 计算机视觉与模式识别会议，第1682–1691页，2019年。'
- en: '[4] D. Akkaynak, T. Treibitz, T. Shlesinger, Y. Loya, R. Tamir, and D. Iluz.
    What is the space of attenuation coefficients in underwater computer vision? In
    IEEE Conference on Computer Vision and Pattern Recognition, pages 4931–4940, 2017.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] D. Akkaynak, T. Treibitz, T. Shlesinger, Y. Loya, R. Tamir 和 D. Iluz. 水下计算机视觉中的衰减系数空间是什么？发表于
    IEEE 计算机视觉与模式识别会议，第4931–4940页，2017年。'
- en: '[5] A. Almahairi, S. Rajeshwar, A. Sordoni, P. Bachman, and A. Courville. Augmented
    cyclegan: Learning many-to-many mappings from unpaired data. In International
    conference on machine learning, pages 195–204, 2018.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. 阿尔马哈伊里，S. 拉杰什瓦尔，A. 索尔多尼，P. 巴赫曼和A. 库维尔。增强循环GAN：从不匹配数据学习到多对多映射。在国际机器学习会议上，195–204页，2018。'
- en: '[6] C. O. Ancuti, C. Ancuti, C. De Vleeschouwer, and P. Bekaert. Color balance
    and fusion for underwater image enhancement. IEEE Transactions on Image Processing,
    27(1):379–393, 2017.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C. O.安库蒂，C. 安库蒂，C. 德维利斯视厉，和P. 贝卡特。水下图像增强的颜色平衡和融合。IEEE图像处理交易，27（1）：379–393页，2017。'
- en: '[7] S. Anwar and C. Li. Diving deeper into underwater image enhancement: A
    survey. Signal Processing: Image Communication, 89:115978, 2020.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] S. 阿努瓦尔和C. 李。深入了解水下图像增强：一项调查。信号处理：图像通信，89：115978，2020。'
- en: '[8] M. Badran and M. Torki. Daut: Underwater image enhancement using depth
    aware u-shape transformer. In IEEE International Conference on Image Processing,
    pages 1830–1834, 2023.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. 巴德兰和M. 托尔基。深度感知U形变压器进行水下图像增强。在IEEE国际图像处理会议上，1830–1834页，2023。'
- en: '[9] X. Bing, W. Ren, Y. Tang, G. G. Yen, and Q. Sun. Domain adaptation for
    in-air to underwater image enhancement via deep learning. IEEE Transactions on
    Emerging Topics in Computational Intelligence, 2023.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] X. 病，W. 任，Y. 唐，G. Y. Yen，和Q. 孙。通过深度学习进行从空中到水下图像增强的领域适应。IEEE计算智能新兴主题交易，2023。'
- en: '[10] A. Boudiaf, Y. Guo, A. Ghimire, N. Werghi, G. De Masi, S. Javed, and J. Dias.
    Underwater image enhancement using pre-trained transformer. In International Conference
    on Image Analysis and Processing, pages 480–488, 2022.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. 布迪亚夫，Y. 郭，A. 吉米莱，N. 韦尔格希，G. 德马西，S. 贾维德和J. 迪亚斯。使用预训练变压器进行水下图像增强。在国际图像分析与处理会议上，480–488页，2022。'
- en: '[11] N. Carlevaris-Bianco, A. Mohan, and R. M. Eustice. Initial results in
    underwater single image dehazing. In Oceans 2010 Mts/IEEE Seattle, pages 1–8,
    2010.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] N. 卡利瓦里斯-比安科，A. 莫汉，和R. M.尤斯蒂斯。水下单张图像去雾的初步结果。在Oceans 2010 Mts/IEEE Seattle，1–8页，2010。'
- en: '[12] H. Chen, J. Gu, Y. Liu, S. A. Magid, C. Dong, Q. Wang, H. Pfister, and
    L. Zhu. Masked image training for generalizable deep image denoising. In IEEE
    Conference on Computer Vision and Pattern Recognition, pages 1692–1703, 2023.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] H. 陈，J. 古，Y. 刘，S. A.马吉德，C. 董，Q. 王，H. 菲斯特和L. 朱。用于通用深度图像去噪的蒙版图像训练。在IEEE计算机视觉与模式识别会议上，1692–1703页，2023。'
- en: '[13] L. Chen, Z. Jiang, L. Tong, Z. Liu, A. Zhao, Q. Zhang, J. Dong, and H. Zhou.
    Perceptual underwater image enhancement with deep learning and physical priors.
    IEEE Transactions on Circuits and Systems for Video Technology, 31(8):3078–3092,
    2020.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] L. 陈，Z. 江，L. 童，Z. 刘，A. 赵，Q. 张，J. 董，和H. 周。具有深度学习和物理先验的感知水下图像增强。IEEE视频技术电路与系统交易，31（8）：3078–3092页，2020。'
- en: '[14] X. Chen, J. Pan, J. Dong, and J. Tang. Towards unified deep image deraining:
    A survey and a new benchmark. arXiv preprint arXiv:2310.03535, 2023.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] X. 陈，J. 潘，J. 董，和J. 唐。走向统一的深度图像去雨水：一项调查和一个新的基准。arXiv预印本arXiv：2310.03535，2023。'
- en: '[15] X. Chen, P. Zhang, L. Quan, C. Yi, and C. Lu. Underwater image enhancement
    based on deep learning and image formation model. arXiv preprint arXiv:2101.00991,
    2021.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] X. 陈，P. 张，L. 全，C. 易，和C. 陆。基于深度学习和图像形成模型的水下图像增强。arXiv预印本arXiv：2101.00991，2021。'
- en: '[16] Y.-W. Chen and S.-C. Pei. Domain adaptation for underwater image enhancement
    via content and style separation. IEEE Access, 10:90523–90534, 2022.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y.-W.陈和S.-C.裴。通过内容和样式分离进行水下图像增强的领域适应。IEEE访问，10：90523–90534，2022。'
- en: '[17] J. Y. Chiang and Y.-C. Chen. Underwater image enhancement by wavelength
    compensation and dehazing. IEEE Transactions on Image Processing, 21(4):1756–1769,
    2011.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. Y.姜和Y.-C.陈。通过波长补偿和去雾进行水下图像增强。IEEE图像处理交易，21（4）：1756–1769页，2011。'
- en: '[18] X. Chu, Z. Fu, S. Yu, X. Tu, Y. Huang, and X. Ding. Underwater image enhancement
    and super-resolution using implicit neural networks. In IEEE International Conference
    on Image Processing, pages 1295–1299, 2023.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] X. 楚，Z. 傅，S. 俞，X. 图，Y. 黄和X. 丁。使用隐式神经网络进行水下图像增强和超分辨率。在IEEE国际图像处理会议上，1295–1299页，2023。'
- en: '[19] X. Cong, J. Gui, and J. Hou. Underwater organism color fine-tuning via
    decomposition and guidance. In AAAI Conference on Artificial Intelligence, volume 38,
    pages 1389–1398, 2024.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] X. 丛，J. 桂，和J. 侯。通过分解和指导进行水下生物颜色微调。在AAAI人工智能大会上，38卷，1389–1398页，2024。'
- en: '[20] C. Desai, S. Benur, R. A. Tabib, U. Patil, and U. Mudenagudi. Depthcue:
    Restoration of underwater images using monocular depth as a clue. In IEEE Winter
    Conference on Applications of Computer Vision Workshops, pages 196–205, 2023.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] C. Desai, S. Benur, R. A. Tabib, U. Patil, and U. Mudenagudi. 深度提示：使用单眼深度作为线索恢复水下图像。在IEEE冬季应用计算机视觉研讨会上，第196至205页，2023年。'
- en: '[21] C. Desai, B. S. S. Reddy, R. A. Tabib, U. Patil, and U. Mudenagudi. Aquagan:
    Restoration of underwater images. In IEEE Conference on Computer Vision and Pattern
    Recognition Workshops, pages 296–304, 2022.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] C. Desai, B. S. S. Reddy, R. A. Tabib, U. Patil, and U. Mudenagudi. Aquagan：水下图像恢复。在IEEE计算机视觉和模式识别研讨会上，第296至304页，2022年。'
- en: '[22] C. Desai, R. A. Tabib, S. S. Reddy, U. Patil, and U. Mudenagudi. Ruig:
    Realistic underwater image generation towards restoration. In IEEE Conference
    on Computer Vision and Pattern Recognition Workshops, pages 2181–2189, 2021.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] C. Desai, R. A. Tabib, S. S. Reddy, U. Patil, and U. Mudenagudi. Ruig：真实水下图像生成及恢复。在IEEE计算机视觉和模式识别研讨会上，第2181至2189页，2021年。'
- en: '[23] P. Drews, E. Nascimento, F. Moraes, S. Botelho, and M. Campos. Transmission
    estimation in underwater single images. In IEEE International Conference on Computer
    Vision Workshops, pages 825–830, 2013.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] P. Drews, E. Nascimento, F. Moraes, S. Botelho, and M. Campos. 水下单图像传输估计。在IEEE计算机视觉大会上，第825至830页，2013年。'
- en: '[24] P. L. Drews, E. R. Nascimento, S. S. Botelho, and M. F. M. Campos. Underwater
    depth estimation and image restoration based on single images. IEEE computer graphics
    and applications, 36(2):24–35, 2016.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] P. L. Drews, E. R. Nascimento, S. S. Botelho, and M. F. M. Campos. 基于单图像的水下深度估计和图像恢复。IEEE计算机图形与应用，36(2)：24至35页，2016年。'
- en: '[25] A. Dudhane, P. Hambarde, P. Patil, and S. Murala. Deep underwater image
    restoration and beyond. IEEE Signal Processing Letters, 27:675–679, 2020.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Dudhane, P. Hambarde, P. Patil, and S. Murala. 深度水下图像恢复及其应用。IEEE信号处理通信，27：675至679页，2020年。'
- en: '[26] P. Duhamel and M. Vetterli. Fast fourier transforms: a tutorial review
    and a state of the art. Signal processing, 19(4):259–299, 1990.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] P. Duhamel and M. Vetterli. 快速傅立叶变换：教程评论和现状。信号处理，19(4)：259至299页，1990年。'
- en: '[27] A. Esmaeilzehi, Y. Ou, M. O. Ahmad, and M. Swamy. Dmml: Deep multi-prior
    and multi-discriminator learning for underwater image enhancement. IEEE Transactions
    on Broadcasting, 2024.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Esmaeilzehi, Y. Ou, M. O. Ahmad, and M. Swamy. DMML：深度多先验和多鉴别器学习用于水下图像增强。IEEE广播交易，2024年。'
- en: '[28] C. Fabbri, M. J. Islam, and J. Sattar. Enhancing underwater imagery using
    generative adversarial networks. In International Conference on Robotics and Automation,
    pages 7159–7165, 2018.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] C. Fabbri, M. J. Islam, and J. Sattar. 使用生成对抗网络增强水下图像。在国际机器人和自动化大会上，第7159至7165页，2018年。'
- en: '[29] Z. Fu, H. Lin, Y. Yang, S. Chai, L. Sun, Y. Huang, and X. Ding. Unsupervised
    underwater image restoration: From a homology perspective. In AAAI Conference
    on Artificial Intelligence, pages 643–651, 2022.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Z. Fu, H. Lin, Y. Yang, S. Chai, L. Sun, Y. Huang, and X. Ding. 无监督的水下图像恢复：从同源性角度。在AAAI人工智能大会上，第643至651页，2022年。'
- en: '[30] Z. Fu, X. Lin, W. Wang, Y. Huang, and X. Ding. Underwater image enhancement
    via learning water type desensitized representations. In IEEE International Conference
    on Acoustics, Speech and Signal Processing, pages 2764–2768, 2022.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Z. Fu, X. Lin, W. Wang, Y. Huang, and X. Ding. 通过学习水类型抗敏感表示来增强水下图像。在IEEE国际声学、语音和信号处理会议上，第2764至2768页，2022年。'
- en: '[31] Z. Fu, W. Wang, Y. Huang, X. Ding, and K.-K. Ma. Uncertainty inspired
    underwater image enhancement. In European Conference on Computer Vision, pages
    465–482, 2022.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Z. Fu, W. Wang, Y. Huang, X. Ding, and K.-K. Ma. 不确定性启发的水下图像增强。在欧洲计算机视觉大会上，第465至482页，2022年。'
- en: '[32] S. Gangisetty and R. R. Rai. Floodnet: Underwater image restoration based
    on residual dense learning. Signal Processing: Image Communication, 104:116647,
    2022.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. Gangisetty and R. R. Rai. 洪水网：基于残差密集学习的水下图像恢复。信号处理：图像通信，104：116647，2022年。'
- en: '[33] S.-B. Gao, M. Zhang, Q. Zhao, X.-S. Zhang, and Y.-J. Li. Underwater image
    enhancement using adaptive retinal mechanisms. IEEE Transactions on Image Processing,
    28(11):5580–5595, 2019.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S.-B. Gao, M. Zhang, Q. Zhao, X.-S. Zhang, and Y.-J. Li. 使用自适应视网膜机制增强水下图像。IEEE图像处理期刊，28(11)：5580至5595页，2019年。'
- en: '[34] S. Gonzalez-Sabbagh, A. Robles-Kelly, and S. Gao. Dgd-cgan: A dual generator
    for image dewatering and restoration. Pattern Recognition, 148:110159, 2024.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Gonzalez-Sabbagh, A. Robles-Kelly, and S. Gao. Dgd-cgan: 双生成器用于图像脱水和恢复。图案识别，148：110159，2024年。'
- en: '[35] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio. Generative adversarial networks. Communications of
    the ACM, 63(11):139–144, 2020.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio. 生成对抗网络。ACM 通讯，63(11):139–144, 2020。'
- en: '[36] J. Gui, X. Cong, Y. Cao, W. Ren, J. Zhang, J. Zhang, J. Cao, and D. Tao.
    A comprehensive survey and taxonomy on single image dehazing based on deep learning.
    ACM Computing Surveys, 55(13s):1–37, 2023.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Gui, X. Cong, Y. Cao, W. Ren, J. Zhang, J. Zhang, J. Cao, 和 D. Tao.
    基于深度学习的单幅图像去雾的全面调查与分类。ACM 计算调查，55(13s):1–37, 2023。'
- en: '[37] C. Guo, R. Wu, X. Jin, L. Han, W. Zhang, Z. Chai, and C. Li. Underwater
    ranker: Learn which is better and how to be better. In AAAI Conference on Artificial
    Intelligence, pages 702–709, 2023.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] C. Guo, R. Wu, X. Jin, L. Han, W. Zhang, Z. Chai, 和 C. Li. 水下排名器：学习哪个更好以及如何变得更好。在
    AAAI 人工智能会议，页码 702–709, 2023。'
- en: '[38] P. Hambarde, S. Murala, and A. Dhall. Uw-gan: Single-image depth estimation
    and image enhancement for underwater images. IEEE Transactions on Instrumentation
    and Measurement, 70:1–12, 2021.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] P. Hambarde, S. Murala, 和 A. Dhall. Uw-gan：水下图像的单幅图像深度估计和图像增强。IEEE 测量与仪器学报，70:1–12,
    2021。'
- en: '[39] J. Han, M. Shoeiby, T. Malthus, E. Botha, J. Anstee, S. Anwar, R. Wei,
    M. A. Armin, H. Li, and L. Petersson. Underwater image restoration via contrastive
    learning and a real-world dataset. Remote Sensing, 14(17):4297, 2022.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Han, M. Shoeiby, T. Malthus, E. Botha, J. Anstee, S. Anwar, R. Wei,
    M. A. Armin, H. Li, 和 L. Petersson. 通过对比学习和真实世界数据集进行水下图像恢复。遥感，14(17):4297, 2022。'
- en: '[40] J. Han, M. Shoeiby, T. Malthus, E. Botha, J. Anstee, S. Anwar, R. Wei,
    L. Petersson, and M. A. Armin. Single underwater image restoration by contrastive
    learning. In IEEE International Geoscience and Remote Sensing Symposium, pages
    2385–2388, 2021.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Han, M. Shoeiby, T. Malthus, E. Botha, J. Anstee, S. Anwar, R. Wei,
    L. Petersson, 和 M. A. Armin. 通过对比学习进行单幅水下图像恢复。在 IEEE 国际地球科学与遥感研讨会，页码 2385–2388,
    2021。'
- en: '[41] M. Han, Z. Lyu, T. Qiu, and M. Xu. A review on intelligence dehazing and
    color restoration for underwater images. IEEE Transactions on Systems, Man, and
    Cybernetics: Systems, 50(5):1820–1832, 2018.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. Han, Z. Lyu, T. Qiu, 和 M. Xu. 水下图像智能去雾与颜色恢复的综述。IEEE 系统、人类与控制论学报：系统，50(5):1820–1832,
    2018。'
- en: '[42] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models.
    Advances in Neural Information Processing Systems, 33:6840–6851, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. Ho, A. Jain, 和 P. Abbeel. 去噪扩散概率模型。神经信息处理系统进展，33:6840–6851, 2020。'
- en: '[43] G. Hou, N. Li, P. Zhuang, K. Li, H. Sun, and C. Li. Non-uniform illumination
    underwater image restoration via illumination channel sparsity prior. IEEE Transactions
    on Circuits and Systems for Video Technology, 2023.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] G. Hou, N. Li, P. Zhuang, K. Li, H. Sun, 和 C. Li. 通过光照通道稀疏先验进行非均匀光照水下图像恢复。IEEE
    视频技术电路与系统学报，2023。'
- en: '[44] G. Hou, X. Zhao, Z. Pan, H. Yang, L. Tan, and J. Li. Benchmarking underwater
    image enhancement and restoration, and beyond. IEEE Access, 8:122078–122091, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] G. Hou, X. Zhao, Z. Pan, H. Yang, L. Tan, 和 J. Li. 水下图像增强和恢复的基准测试及其他。IEEE
    Access，8:122078–122091, 2020。'
- en: '[45] J. Hu, Q. Jiang, R. Cong, W. Gao, and F. Shao. Two-branch deep neural
    network for underwater image enhancement in hsv color space. IEEE Signal Processing
    Letters, 28:2152–2156, 2021.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] J. Hu, Q. Jiang, R. Cong, W. Gao, 和 F. Shao. 用于 HSV 颜色空间下水下图像增强的双分支深度神经网络。IEEE
    信号处理快报，28:2152–2156, 2021。'
- en: '[46] H. Huang, R. He, Z. Sun, and T. Tan. Wavelet-srnet: A wavelet-based cnn
    for multi-scale face super resolution. In IEEE International Conference on Computer
    Vision, pages 1689–1697, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] H. Huang, R. He, Z. Sun, 和 T. Tan. Wavelet-srnet：一种基于小波的 CNN 用于多尺度人脸超分辨率。在
    IEEE 计算机视觉国际会议，页码 1689–1697, 2017。'
- en: '[47] S. Huang, K. Wang, H. Liu, J. Chen, and Y. Li. Contrastive semi-supervised
    learning for underwater image restoration via reliable bank. In IEEE Conference
    on Computer Vision and Pattern Recognition, pages 18145–18155, 2023.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] S. Huang, K. Wang, H. Liu, J. Chen, 和 Y. Li. 通过可靠银行进行水下图像恢复的对比半监督学习。在
    IEEE 计算机视觉与模式识别会议，页码 18145–18155, 2023。'
- en: '[48] Z. Huang, J. Li, Z. Hua, and L. Fan. Underwater image enhancement via
    adaptive group attention-based multiscale cascade transformer. IEEE Transactions
    on Instrumentation and Measurement, 71:1–18, 2022.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Z. Huang, J. Li, Z. Hua, 和 L. Fan. 基于自适应组注意力的多尺度级联变换器的水下图像增强。IEEE 测量与仪器学报，71:1–18,
    2022。'
- en: '[49] F. Huo, B. Li, and X. Zhu. Efficient wavelet boost learning-based multi-stage
    progressive refinement network for underwater image enhancement. In IEEE International
    Conference on Computer Vision Workshops, pages 1944–1952, 2021.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] F. Huo, B. Li, 和 X. Zhu. 高效的小波提升学习基础的多阶段渐进细化网络用于水下图像增强。IEEE国际计算机视觉会议研讨会论文集，页码
    1944–1952，2021。'
- en: '[50] M. J. Islam, C. Edge, Y. Xiao, P. Luo, M. Mehtaz, C. Morse, S. S. Enan,
    and J. Sattar. Semantic segmentation of underwater imagery: Dataset and benchmark.
    In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages
    1769–1776\. IEEE, 2020.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] M. J. Islam, C. Edge, Y. Xiao, P. Luo, M. Mehtaz, C. Morse, S. S. Enan,
    和 J. Sattar. 水下图像的语义分割：数据集和基准测试。在IEEE/RSJ国际智能机器人与系统会议，页码 1769–1776。IEEE，2020。'
- en: '[51] M. J. Islam, P. Luo, and J. Sattar. Simultaneous enhancement and super-resolution
    of underwater imagery for improved visual perception. In 16th Robotics: Science
    and Systems, RSS 2020, 2020.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] M. J. Islam, P. Luo, 和 J. Sattar. 水下图像的同时增强与超分辨率以改善视觉感知。第十六届机器人学：科学与系统会议，RSS
    2020，2020。'
- en: '[52] M. J. Islam, R. Wang, and J. Sattar. Svam: Saliency-guided visual attention
    modeling by autonomous underwater robots. In Robotics: Science and Systems, NY,
    USA, 2022.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. J. Islam, R. Wang, 和 J. Sattar. Svam：由自主水下机器人引导的视觉注意力建模。机器人学：科学与系统，纽约，美国，2022。'
- en: '[53] M. J. Islam, Y. Xia, and J. Sattar. Fast underwater image enhancement
    for improved visual perception. IEEE Robotics and Automation Letters, 5(2):3227–3234,
    2020.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. J. Islam, Y. Xia, 和 J. Sattar. 快速水下图像增强以改善视觉感知。IEEE机器人与自动化快报，5(2):3227–3234，2020。'
- en: '[54] A. Jabbar, X. Li, and B. Omar. A survey on generative adversarial networks:
    Variants, applications, and training. ACM Computing Surveys, 54(8):1–49, 2021.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Jabbar, X. Li, 和 B. Omar. 生成对抗网络调查：变体、应用与训练。ACM计算调查，54(8):1–49，2021。'
- en: '[55] N. Jain, G. R. Matta, and K. Mitra. Towards realistic underwater dataset
    generation and color restoration. In Proceedings of the Thirteenth Indian Conference
    on Computer Vision, Graphics and Image Processing, pages 1–9, 2022.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] N. Jain, G. R. Matta, 和 K. Mitra. 走向真实的水下数据集生成与色彩恢复。在第十三届印度计算机视觉、图形和图像处理会议论文集，页码
    1–9，2022。'
- en: '[56] A. Jamadandi and U. Mudenagudi. Exemplar-based underwater image enhancement
    augmented by wavelet corrected transforms. In IEEE Conference on Computer Vision
    and Pattern Recognition Workshops, pages 11–17, 2019.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Jamadandi 和 U. Mudenagudi. 基于示例的水下图像增强，通过小波校正变换增强。IEEE计算机视觉与模式识别会议研讨会论文集，页码
    11–17，2019。'
- en: '[57] K. Ji, W. Lei, and W. Zhang. A real-world underwater image enhancement
    method based on multi-color space and two-stage adaptive fusion. Signal, Image
    and Video Processing, pages 1–15, 2024.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] K. Ji, W. Lei, 和 W. Zhang. 基于多彩色空间和两阶段自适应融合的现实世界水下图像增强方法。信号、图像与视频处理，页码
    1–15，2024。'
- en: '[58] M. Jian, X. Liu, H. Luo, X. Lu, H. Yu, and J. Dong. Underwater image processing
    and analysis: A review. Signal Processing: Image Communication, 91:116088, 2021.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. Jian, X. Liu, H. Luo, X. Lu, H. Yu, 和 J. Dong. 水下图像处理与分析：综述。信号处理：图像通信，91:116088，2021。'
- en: '[59] Q. Jiang, Y. Chen, G. Wang, and T. Ji. A novel deep neural network for
    noise removal from underwater image. Signal Processing: Image Communication, 87:115921,
    2020.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Q. Jiang, Y. Chen, G. Wang, 和 T. Ji. 一种新颖的深度神经网络用于水下图像噪声去除。信号处理：图像通信，87:115921，2020。'
- en: '[60] Q. Jiang, Y. Gu, C. Li, R. Cong, and F. Shao. Underwater image enhancement
    quality evaluation: Benchmark dataset and objective metric. IEEE Transactions
    on Circuits and Systems for Video Technology, 32(9):5959–5974, 2022.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Q. Jiang, Y. Gu, C. Li, R. Cong, 和 F. Shao. 水下图像增强质量评估：基准数据集和客观度量。IEEE视频技术电路与系统汇刊，32(9):5959–5974，2022。'
- en: '[61] Q. Jiang, Y. Kang, Z. Wang, W. Ren, and C. Li. Perception-driven deep
    underwater image enhancement without paired supervision. IEEE Transactions on
    Multimedia, 2023.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Q. Jiang, Y. Kang, Z. Wang, W. Ren, 和 C. Li. 基于感知驱动的深度水下图像增强，无需配对监督。IEEE多媒体汇刊，2023。'
- en: '[62] Q. Jiang, Y. Zhang, F. Bao, X. Zhao, C. Zhang, and P. Liu. Two-step domain
    adaptation for underwater image enhancement. Pattern Recognition, 122:108324,
    2022.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Q. Jiang, Y. Zhang, F. Bao, X. Zhao, C. Zhang, 和 P. Liu. 用于水下图像增强的两步域适应。模式识别，122:108324，2022。'
- en: '[63] Z. Jiang, Z. Li, S. Yang, X. Fan, and R. Liu. Target oriented perceptual
    adversarial fusion network for underwater image enhancement. IEEE Transactions
    on Circuits and Systems for Video Technology, 32(10):6584–6598, 2022.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Z. Jiang, Z. Li, S. Yang, X. Fan, 和 R. Liu. 面向目标的感知对抗融合网络用于水下图像增强。IEEE视频技术电路与系统汇刊，32(10):6584–6598，2022。'
- en: '[64] M. Kapoor, R. Baghel, B. N. Subudhi, V. Jakhetiya, and A. Bansal. Domain
    adversarial learning towards underwater image enhancement. In IEEE International
    Conference on Computer Vision Workshops, pages 2241–2251, 2023.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] M. 卡普尔, R. 巴盖尔, B. N. 苏布迪, V. 杰克赫蒂亚, 和 A. 班萨尔。向水下图像增强的域对抗学习。在IEEE国际计算机视觉会议研讨会上，页码2241–2251，2023年。'
- en: '[65] R. Khan, A. Mehmood, S. Akbar, and Z. Zheng. Underwater image enhancement
    with an adaptive self supervised network. In IEEE International Conference on
    Multimedia and Expo, pages 1355–1360, 2023.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] R. 汗, A. 梅姆德, S. 阿克巴, 和 Z. 郑。自适应自监督网络进行水下图像增强。在IEEE多媒体与博览会国际会议上，页码1355–1360，2023年。'
- en: '[66] R. Khan, P. Mishra, N. Mehta, S. S. Phutke, S. K. Vipparthi, S. Nandi,
    and S. Murala. Spectroformer: Multi-domain query cascaded transformer network
    for underwater image enhancement. In IEEE Winter Conference on Applications of
    Computer Vision, pages 1454–1463, 2024.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] R. 汗, P. 米什拉, N. 梅赫塔, S. S. 阿赫图凯, S. K. 维帕尔提, S. 南迪, 和 S. 穆拉拉。Spectroformer：用于水下图像增强的多域查询级联变换器网络。在IEEE计算机视觉应用冬季会议上，页码1454–1463，2024年。'
- en: '[67] A. Khandouzi and M. Ezoji. Coarse-to-fine underwater image enhancement
    with lightweight cnn and attention-based refinement. Journal of Visual Communication
    and Image Representation, page 104068, 2024.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] A. 阿尔多斯和M. 埃佐吉。轻量级CNN和基于注意力的精细水下图像增强。视觉交流和图像表示期刊，页码104068，2024年。'
- en: '[68] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot,
    C. Liu, and D. Krishnan. Supervised contrastive learning. Advances in neural information
    processing systems, 33:18661–18673, 2020.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] P. 科斯拉, P. 特特瓦克, C. 王, A. 萨尔纳, Y. 天, P. 伊索拉, A. 马奇诺特, C. 刘, 和 D. 克里希南。监督对比学习。神经信息处理系统进展，33：18661–18673，2020年。'
- en: '[69] G. Kim, S. W. Park, and J. Kwon. Pixel-wise wasserstein autoencoder for
    highly generative dehazing. IEEE Transactions on Image Processing, 30:5452–5462,
    2021.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] G. 金, S. W. 帕克, 和 J. 权。用于高度生成去雾的像素级Wasserstein自编码器。IEEE图像处理交易，30：5452–5462，2021年。'
- en: '[70] B. Li, X. Liu, P. Hu, Z. Wu, J. Lv, and X. Peng. All-in-one image restoration
    for unknown corruption. In IEEE Conference on Computer Vision and Pattern Recognition,
    pages 17452–17462, 2022.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] B. 李, X. 刘, P. 胡, Z. 吴, J. 吕, 和 X. 彭。未知损坏的图像恢复一体化解决方案。在IEEE计算机视觉和模式识别会议上，页码17452–17462，2022年。'
- en: '[71] C. Li, S. Anwar, J. Hou, R. Cong, C. Guo, and W. Ren. Underwater image
    enhancement via medium transmission-guided multi-color space embedding. IEEE Transactions
    on Image Processing, 30:4985–5000, 2021.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] C. 李, S. 安瓦尔, J. 侯, R. 丛, C. 郭, 和 W. 任。通过介质透射引导的多色彩空间嵌入进行水下图像增强。IEEE图像处理交易，30：4985–5000，2021年。'
- en: '[72] C. Li, S. Anwar, and F. Porikli. Underwater scene prior inspired deep
    underwater image and video enhancement. Pattern Recognition, 98:107038, 2020.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] C. 李, S. 安瓦尔, 和 F. 波里克利。受水下场景先验启发的深度水下图像和视频增强。模式识别，98：107038，2020年。'
- en: '[73] C. Li, C. Guo, L. Han, J. Jiang, M.-M. Cheng, J. Gu, and C. C. Loy. Low-light
    image and video enhancement using deep learning: A survey. IEEE transactions on
    pattern analysis and machine intelligence, 44(12):9396–9416, 2021.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] C. 李, C. 郭, L. 韩, J. 江, M.-M. 成, J. 谷, 和 C. C. 罗伊。利用深度学习进行低光图像和视频增强：一项调查。IEEE模式分析与机器智能交易，44(12)：9396–9416，2021年。'
- en: '[74] C. Li, C. Guo, W. Ren, R. Cong, J. Hou, S. Kwong, and D. Tao. An underwater
    image enhancement benchmark dataset and beyond. IEEE Transactions on Image Processing,
    29:4376–4389, 2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] C. 李, C. 郭, W. 任, R. 丛, J. 侯, S. 光, 和 D. 陶。水下图像增强基准数据集及更多。IEEE图像处理交易，29：4376–4389，2019年。'
- en: '[75] C. Li, J. Quo, Y. Pang, S. Chen, and J. Wang. Single underwater image
    restoration by blue-green channels dehazing and red channel correction. In IEEE
    International Conference on Acoustics, Speech and Signal Processing, pages 1731–1735,
    2016.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] C. 李, J. 乔, Y. 庞, S. 陈, 和 J. 王。蓝绿通道去雾和红通道校正的单个水下图像恢复。在IEEE国际声学、语音和信号处理会议上，页码1731–1735，2016年。'
- en: '[76] H. Li, J. Li, and W. Wang. A fusion adversarial underwater image enhancement
    network with a public test dataset. arXiv preprint arXiv:1906.06819, 2019.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] H. 李, J. 李, 和 W. 王。具有公共测试数据集的融合对抗水下图像增强网络。arXiv预印本arXiv：1906.06819，2019年。'
- en: '[77] J. Li, K. A. Skinner, R. M. Eustice, and M. Johnson-Roberson. Watergan:
    Unsupervised generative network to enable real-time color correction of monocular
    underwater images. IEEE Robotics and Automation letters, 3(1):387–394, 2017.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] J. 李, K. A. 斯金纳, R. M. 尤斯蒂斯, 和 M. 约翰逊-罗伯森。Watergan：无监督生成网络，实现单目水下图像的实时色彩校正。IEEE机器人与自动化信函，3(1)：387–394，2017年。'
- en: '[78] K. Li, H. Fan, Q. Qi, C. Yan, K. Sun, and Q. J. Wu. Tctl-net: Template-free
    color transfer learning for self-attention driven underwater image enhancement.
    IEEE Transactions on Circuits and Systems for Video Technology, 2023.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] K. Li, H. Fan, Q. Qi, C. Yan, K. Sun, 和 Q. J. Wu. Tctl-net: 无模板颜色迁移学习用于自注意力驱动的水下图像增强。IEEE
    电路与系统视频技术汇刊, 2023。'
- en: '[79] K. Li, L. Wu, Q. Qi, W. Liu, X. Gao, L. Zhou, and D. Song. Beyond single
    reference for training: underwater image enhancement via comparative learning.
    IEEE Transactions on Circuits and Systems for Video Technology, 2022.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] K. Li, L. Wu, Q. Qi, W. Liu, X. Gao, L. Zhou, 和 D. Song. 超越单一参考的训练: 通过比较学习进行水下图像增强。IEEE
    电路与系统视频技术汇刊, 2022。'
- en: '[80] Y. Li, L. Shen, M. Li, Z. Wang, and L. Zhuang. Ruiesr: Realistic underwater
    image enhancement and super resolution. IEEE Transactions on Circuits and Systems
    for Video Technology, 2023.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Li, L. Shen, M. Li, Z. Wang, 和 L. Zhuang. Ruiesr: 现实水下图像增强和超分辨率。IEEE
    电路与系统视频技术汇刊, 2023。'
- en: '[81] R. Lin, J. Liu, R. Liu, and X. Fan. Global structure-guided learning framework
    for underwater image enhancement. The Visual Computer, 38(12):4419–4434, 2022.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] R. Lin, J. Liu, R. Liu, 和 X. Fan. 全球结构指导学习框架用于水下图像增强。视觉计算, 38(12):4419–4434,
    2022。'
- en: '[82] Y. Lin, L. Shen, Z. Wang, K. Wang, and X. Zhang. Attenuation coefficient
    guided two-stage network for underwater image restoration. IEEE Signal Processing
    Letters, 28:199–203, 2020.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Lin, L. Shen, Z. Wang, K. Wang, 和 X. Zhang. 衰减系数指导的两阶段网络用于水下图像修复。IEEE
    信号处理快报, 28:199–203, 2020。'
- en: '[83] P. Liu, G. Wang, H. Qi, C. Zhang, H. Zheng, and Z. Yu. Underwater image
    enhancement with a deep residual framework. IEEE Access, 7:94614–94629, 2019.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] P. Liu, G. Wang, H. Qi, C. Zhang, H. Zheng, 和 Z. Yu. 使用深度残差框架的水下图像增强。IEEE
    Access, 7:94614–94629, 2019。'
- en: '[84] Q. Liu, Q. Zhang, W. Liu, W. Chen, X. Liu, and X. Wang. Wsds-gan: A weak-strong
    dual supervised learning method for underwater image enhancement. Pattern Recognition,
    page 109774, 2023.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Q. Liu, Q. Zhang, W. Liu, W. Chen, X. Liu, 和 X. Wang. Wsds-gan: 一种弱强双重监督学习方法用于水下图像增强。模式识别,
    页 109774, 2023。'
- en: '[85] R. Liu, X. Fan, M. Zhu, M. Hou, and Z. Luo. Real-world underwater enhancement:
    Challenges, benchmarks, and solutions under natural light. IEEE transactions on
    circuits and systems for video technology, 30(12):4861–4875, 2020.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] R. Liu, X. Fan, M. Zhu, M. Hou, 和 Z. Luo. 现实世界水下增强: 在自然光下的挑战、基准和解决方案。IEEE
    电路与系统视频技术汇刊, 30(12):4861–4875, 2020。'
- en: '[86] R. Liu, Z. Jiang, S. Yang, and X. Fan. Twin adversarial contrastive learning
    for underwater image enhancement and beyond. IEEE Transactions on Image Processing,
    31:4922–4936, 2022.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] R. Liu, Z. Jiang, S. Yang, 和 X. Fan. 双重对抗性对比学习用于水下图像增强及其他。IEEE 图像处理汇刊,
    31:4922–4936, 2022。'
- en: '[87] X. Liu, Z. Gao, and B. M. Chen. Ipmgan: Integrating physical model and
    generative adversarial network for underwater image enhancement. Neurocomputing,
    453:538–551, 2021.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] X. Liu, Z. Gao, 和 B. M. Chen. Ipmgan: 结合物理模型和生成对抗网络的水下图像增强。神经计算, 453:538–551,
    2021。'
- en: '[88] Y. Liu, Z. Yan, S. Chen, T. Ye, W. Ren, and E. Chen. Nighthazeformer:
    Single nighttime haze removal using prior query transformer. In ACM International
    Conference on Multimedia, pages 4119–4128, 2023.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Liu, Z. Yan, S. Chen, T. Ye, W. Ren, 和 E. Chen. Nighthazeformer: 使用先验查询变换器的单夜间雾霾去除。在
    ACM 国际多媒体会议, 页 4119–4128, 2023。'
- en: '[89] Y. Liu, Z. Yan, J. Tan, and Y. Li. Multi-purpose oriented single nighttime
    image haze removal based on unified variational retinex model. IEEE Transactions
    on Circuits and Systems for Video Technology, 33(4):1643–1657, 2022.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. Liu, Z. Yan, J. Tan, 和 Y. Li. 基于统一变分 Retinex 模型的多用途单夜间图像雾霾去除。IEEE 电路与系统视频技术汇刊,
    33(4):1643–1657, 2022。'
- en: '[90] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin
    transformer: Hierarchical vision transformer using shifted windows. In IEEE International
    Conference on Computer Vision, pages 10012–10022, 2021.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, 和 B. Guo. Swin
    transformer: 使用移位窗口的分层视觉变换器。在 IEEE 国际计算机视觉大会, 页 10012–10022, 2021。'
- en: '[91] D. G. Lowe. Distinctive image features from scale-invariant keypoints.
    International journal of computer vision, 60:91–110, 2004.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] D. G. Lowe. 从尺度不变关键点提取的独特图像特征。国际计算机视觉期刊, 60:91–110, 2004。'
- en: '[92] S. Lu, F. Guan, H. Zhang, and H. Lai. Speed-up ddpm for real-time underwater
    image enhancement. IEEE Transactions on Circuits and Systems for Video Technology,
    2023.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. Lu, F. Guan, H. Zhang, 和 H. Lai. 加速 ddpm 实现实时水下图像增强。IEEE 电路与系统视频技术汇刊,
    2023。'
- en: '[93] Z. Luo, F. K. Gustafsson, Z. Zhao, J. Sjölund, and T. B. Schön. Controlling
    vision-language models for universal image restoration. In International Conference
    on Learning Representations, 2024.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Z. Luo, F. K. Gustafsson, Z. Zhao, J. Sjölund, 和 T. B. Schön. 控制视觉-语言模型进行通用图像恢复。发表于国际学习表征会议，2024年。'
- en: '[94] Z. Ma and C. Oh. A wavelet-based dual-stream network for underwater image
    enhancement. In IEEE International Conference on Acoustics, Speech and Signal
    Processing, pages 2769–2773, 2022.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Z. Ma 和 C. Oh. 一种基于小波的双流网络用于水下图像增强。发表于IEEE国际声学、语音与信号处理会议，页码2769–2773，2022年。'
- en: '[95] X. Mei, X. Ye, X. Zhang, Y. Liu, J. Wang, J. Hou, and X. Wang. Uir-net:
    A simple and effective baseline for underwater image restoration and enhancement.
    Remote Sensing, 15(1):39, 2022.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] X. Mei, X. Ye, X. Zhang, Y. Liu, J. Wang, J. Hou, 和 X. Wang. Uir-net:
    一个简单而有效的水下图像恢复和增强基线。遥感，15(1):39，2022年。'
- en: '[96] A. Mittal, R. Soundararajan, and A. C. Bovik. Making a completely blind
    image quality analyzer. IEEE Signal processing letters, 20(3):209–212, 2012.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] A. Mittal, R. Soundararajan, 和 A. C. Bovik. 制作一个完全盲的图像质量分析器。IEEE信号处理快报，20(3):209–212，2012年。'
- en: '[97] J. Moran and H. Qing. Mtnet: A multi-task cascaded network for underwater
    image enhancement. Multimedia Tools and Applications, pages 1–15, 2023.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. Moran 和 H. Qing. Mtnet: 用于水下图像增强的多任务级联网络。多媒体工具与应用，页码1–15，2023年。'
- en: '[98] P. Mu, H. Xu, Z. Liu, Z. Wang, S. Chan, and C. Bai. A generalized physical-knowledge-guided
    dynamic model for underwater image enhancement. In ACM International Conference
    on Multimedia, pages 7111–7120, 2023.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] P. Mu, H. Xu, Z. Liu, Z. Wang, S. Chan, 和 C. Bai. 一种基于物理知识指导的动态水下图像增强模型。发表于ACM国际多媒体会议，页码7111–7120，2023年。'
- en: '[99] A. Naik, A. Swarnakar, and K. Mittal. Shallow-uwnet: Compressed model
    for underwater image enhancement (student abstract). In AAAI Conference on Artificial
    Intelligence, pages 15853–15854, 2021.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Naik, A. Swarnakar, 和 K. Mittal. Shallow-uwnet: 压缩模型用于水下图像增强（学生摘要）。发表于AAAI人工智能会议，页码15853–15854，2021年。'
- en: '[100] Z. Niu, G. Zhong, and H. Yu. A review on the attention mechanism of deep
    learning. Neurocomputing, 452:48–62, 2021.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Z. Niu, G. Zhong, 和 H. Yu. 深度学习注意力机制的综述。Neurocomputing，452:48–62，2021年。'
- en: '[101] K. Panetta, C. Gao, and S. Agaian. Human-visual-system-inspired underwater
    image quality measures. IEEE Journal of Oceanic Engineering, 41(3):541–551, 2015.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] K. Panetta, C. Gao, 和 S. Agaian. 受人类视觉系统启发的水下图像质量度量。IEEE海洋工程学报，41(3):541–551，2015年。'
- en: '[102] L. Peng, C. Zhu, and L. Bian. U-shape transformer for underwater image
    enhancement. IEEE Transactions on Image Processing, 2023.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] L. Peng, C. Zhu, 和 L. Bian. 用于水下图像增强的U形变换器。IEEE图像处理学报，2023年。'
- en: '[103] Y.-T. Peng and P. C. Cosman. Underwater image restoration based on image
    blurriness and light absorption. IEEE transactions on image processing, 26(4):1579–1594,
    2017.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y.-T. Peng 和 P. C. Cosman. 基于图像模糊度和光吸收的水下图像恢复。IEEE图像处理学报，26(4):1579–1594，2017年。'
- en: '[104] H. Qi, H. Zhou, J. Dong, and X. Dong. Deep color-corrected multi-scale
    retinex network for underwater image enhancement. IEEE Transactions on Geoscience
    and Remote Sensing, 2023.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] H. Qi, H. Zhou, J. Dong, 和 X. Dong. 深度色彩校正多尺度Retinex网络用于水下图像增强。IEEE地球科学与遥感学报，2023年。'
- en: '[105] Q. Qi, K. Li, H. Zheng, X. Gao, G. Hou, and K. Sun. Sguie-net: Semantic
    attention guided underwater image enhancement with multi-scale perception. IEEE
    Transactions on Image Processing, 31:6816–6830, 2022.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Q. Qi, K. Li, H. Zheng, X. Gao, G. Hou, 和 K. Sun. Sguie-net: 具有多尺度感知的语义注意力引导水下图像增强。IEEE图像处理学报，31:6816–6830，2022年。'
- en: '[106] Q. Qi, Y. Zhang, F. Tian, Q. J. Wu, K. Li, X. Luan, and D. Song. Underwater
    image co-enhancement with correlation feature matching and joint learning. IEEE
    Transactions on Circuits and Systems for Video Technology, 32(3):1133–1147, 2021.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Q. Qi, Y. Zhang, F. Tian, Q. J. Wu, K. Li, X. Luan, 和 D. Song. 水下图像的协同增强与特征匹配和联合学习。IEEE电路与系统视频技术学报，32(3):1133–1147，2021年。'
- en: '[107] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from
    natural language supervision. In International Conference on Machine Learning,
    pages 8748–8763\. PMLR, 2021.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G.
    Sastry, A. Askell, P. Mishkin, J. Clark, 等. 从自然语言监督中学习可迁移的视觉模型。发表于国际机器学习会议，页码8748–8763。PMLR，2021年。'
- en: '[108] Y. Rao, W. Liu, K. Li, H. Fan, S. Wang, and J. Dong. Deep color compensation
    for generalized underwater image enhancement. IEEE Transactions on Circuits and
    Systems for Video Technology, 2023.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Y. Rao, W. Liu, K. Li, H. Fan, S. Wang, 和 J. Dong. 泛化水下图像增强的深度颜色补偿. 《IEEE
    视频技术电路与系统事务》，2023 年。'
- en: '[109] S. Raveendran, M. D. Patil, and G. K. Birajdar. Underwater image enhancement:
    a comprehensive review, recent trends, challenges and applications. Artificial
    Intelligence Review, 54:5413–5467, 2021.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] S. Raveendran, M. D. Patil, 和 G. K. Birajdar. 水下图像增强：全面评述、近期趋势、挑战及应用.
    《人工智能评论》，54:5413–5467，2021 年。'
- en: '[110] A. Saleh, M. Sheaves, D. Jerry, and M. R. Azghadi. Adaptive uncertainty
    distribution in deep learning for unsupervised underwater image enhancement. arXiv
    preprint arXiv:2212.08983, 2022.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] A. Saleh, M. Sheaves, D. Jerry, 和 M. R. Azghadi. 深度学习中的自适应不确定性分布用于无监督水下图像增强.
    arXiv 预印本 arXiv:2212.08983，2022 年。'
- en: '[111] P. Sharma, I. Bisht, and A. Sur. Wavelength-based attributed deep neural
    network for underwater image restoration. ACM Transactions on Multimedia Computing,
    Communications and Applications, 19(1):1–23, 2023.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] P. Sharma, I. Bisht, 和 A. Sur. 基于波长的属性深度神经网络用于水下图像恢复. 《ACM 多媒体计算、通信与应用事务》，19(1):1–23，2023
    年。'
- en: '[112] X. Shi and Y.-G. Wang. Cpdm: Content-preserving diffusion model for underwater
    image enhancement. arXiv preprint arXiv:2401.15649, 2024.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] X. Shi 和 Y.-G. Wang. Cpdm: 内容保留扩散模型用于水下图像增强. arXiv 预印本 arXiv:2401.15649，2024
    年。'
- en: '[113] W. Song, Z. Shen, M. Zhang, Y. Wang, and A. Liotta. A hierarchical probabilistic
    underwater image enhancement model with reinforcement tuning. Journal of Visual
    Communication and Image Representation, page 104052, 2024.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] W. Song, Z. Shen, M. Zhang, Y. Wang, 和 A. Liotta. 一种具有强化调整的分层概率水下图像增强模型.
    《视觉通信与图像表示杂志》，第 104052 页，2024 年。'
- en: '[114] W. Song, Y. Wang, D. Huang, A. Liotta, and C. Perra. Enhancement of underwater
    images with statistical model of background light and optimization of transmission
    map. IEEE Transactions on Broadcasting, 66(1):153–169, 2020.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] W. Song, Y. Wang, D. Huang, A. Liotta, 和 C. Perra. 基于背景光的统计模型和传输图优化的水下图像增强.
    《IEEE 广播事务》，66(1):153–169，2020 年。'
- en: '[115] W. Song, Y. Wang, D. Huang, and D. Tjondronegoro. A rapid scene depth
    estimation model based on underwater light attenuation prior for underwater image
    restoration. In Advances in Multimedia Information Processing Pacific-Rim Conference
    on Multimedia, pages 678–688, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] W. Song, Y. Wang, D. Huang, 和 D. Tjondronegoro. 基于水下光衰减先验的快速场景深度估计模型用于水下图像恢复.
    见于《多媒体信息处理进展太平洋-环会议》，第 678–688 页，2018 年。'
- en: '[116] Y. Tang, T. Iwaguchi, H. Kawasaki, R. Sagawa, and R. Furukawa. Autoenhancer:
    Transformer on u-net architecture search for underwater image enhancement. In
    Proceedings of the Asian Conference on Computer Vision, pages 1403–1420, 2022.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Y. Tang, T. Iwaguchi, H. Kawasaki, R. Sagawa, 和 R. Furukawa. Autoenhancer:
    基于 u-net 架构的变换器用于水下图像增强. 见于《亚洲计算机视觉会议论文集》，第 1403–1420 页，2022 年。'
- en: '[117] Y. Tang, H. Kawasaki, and T. Iwaguchi. Underwater image enhancement by
    transformer-based diffusion model with non-uniform sampling for skip strategy.
    In ACM International Conference on Multimedia, pages 5419–5427, 2023.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Y. Tang, H. Kawasaki, 和 T. Iwaguchi. 通过基于变换器的扩散模型与非均匀采样的跳过策略进行水下图像增强.
    见于《ACM 国际多媒体会议》，第 5419–5427 页，2023 年。'
- en: '[118] J. Tian, X. Guo, W. Liu, D. Tao, and B. Liu. Deformable convolutional
    network constrained by contrastive learning for underwater image enhancement.
    IEEE Geoscience and Remote Sensing Letters, 2023.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] J. Tian, X. Guo, W. Liu, D. Tao, 和 B. Liu. 通过对比学习约束的可变形卷积网络用于水下图像增强.
    《IEEE 地球科学与遥感通讯》，2023 年。'
- en: '[119] P. M. Uplavikar, Z. Wu, and Z. Wang. All-in-one underwater image enhancement
    using domain-adversarial learning. In IEEE Conference on Computer Vision and Pattern
    Recognition Workshops, pages 1–8, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] P. M. Uplavikar, Z. Wu, 和 Z. Wang. 一体化水下图像增强使用领域对抗学习. 见于《IEEE 计算机视觉与模式识别会议研讨会》，第
    1–8 页，2019 年。'
- en: '[120] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information
    processing systems, 30, 2017.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin. 注意力即一切. 《神经信息处理系统进展》，30，2017 年。'
- en: '[121] G. Verma, M. Kumar, and S. Raikwar. F2uie: feature transfer-based underwater
    image enhancement using multi-stackcnn. Multimedia Tools and Applications, pages
    1–22, 2023.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] G. Verma, M. Kumar, 和 S. Raikwar. F2uie: 基于特征迁移的水下图像增强使用多层卷积神经网络. 《多媒体工具与应用》，第
    1–22 页，2023 年。'
- en: '[122] D. Wang, L. Ma, R. Liu, and X. Fan. Semantic-aware texture-structure
    feature collaboration for underwater image enhancement. In International Conference
    on Robotics and Automation, pages 4592–4598, 2022.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] D. Wang, L. Ma, R. Liu, 和 X. Fan. 语义感知的纹理-结构特征协作用于水下图像增强. 在国际机器人与自动化会议,
    页码 4592–4598, 2022.'
- en: '[123] H. Wang, W. Zhang, L. Bai, and P. Ren. Metalantis: A comprehensive underwater
    image enhancement framework. IEEE Transactions on Geoscience and Remote Sensing,
    2024.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] H. Wang, W. Zhang, L. Bai, 和 P. Ren. Metalantis: 一种综合水下图像增强框架. IEEE 地球科学与遥感汇刊,
    2024.'
- en: '[124] Y. Wang, Y. Cao, J. Zhang, F. Wu, and Z.-J. Zha. Leveraging deep statistics
    for underwater image enhancement. ACM Transactions on Multimedia Computing, Communications,
    and Applications, 17(3s):1–20, 2021.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Y. Wang, Y. Cao, J. Zhang, F. Wu, 和 Z.-J. Zha. 利用深度统计进行水下图像增强. ACM 多媒体计算、通信与应用汇刊,
    17(3s):1–20, 2021.'
- en: '[125] Y. Wang, J. Guo, H. Gao, and H. Yue. Uiec2̂-net: Cnn-based underwater
    image enhancement using two color space. Signal Processing: Image Communication,
    96:116250, 2021.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Y. Wang, J. Guo, H. Gao, 和 H. Yue. Uiec2̂-net: 基于CNN的使用两种颜色空间的水下图像增强.
    信号处理：图像通信, 96:116250, 2021.'
- en: '[126] Y. Wang, J. Guo, W. He, H. Gao, H. Yue, Z. Zhang, and C. Li. Is underwater
    image enhancement all object detectors need? IEEE Journal of Oceanic Engineering,
    2023.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Y. Wang, J. Guo, W. He, H. Gao, H. Yue, Z. Zhang, 和 C. Li. 水下图像增强是否是所有目标检测器需要的？
    IEEE 海洋工程杂志, 2023.'
- en: '[127] Y. Wang, S. Hu, S. Yin, Z. Deng, and Y.-H. Yang. A multi-level wavelet-based
    underwater image enhancement network with color compensation prior. Expert Systems
    with Applications, 242:122710, 2024.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Y. Wang, S. Hu, S. Yin, Z. Deng, 和 Y.-H. Yang. 一种基于多级小波变换的水下图像增强网络，带有颜色补偿先验.
    专家系统与应用, 242:122710, 2024.'
- en: '[128] Y. Wang, W. Song, G. Fortino, L.-Z. Qi, W. Zhang, and A. Liotta. An experimental-based
    review of image enhancement and image restoration methods for underwater imaging.
    IEEE access, 7:140233–140251, 2019.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Y. Wang, W. Song, G. Fortino, L.-Z. Qi, W. Zhang, 和 A. Liotta. 基于实验的水下图像增强与图像恢复方法综述.
    IEEE 访问, 7:140233–140251, 2019.'
- en: '[129] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality
    assessment: from error visibility to structural similarity. IEEE transactions
    on image processing, 13(4):600–612, 2004.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Z. Wang, A. C. Bovik, H. R. Sheikh, 和 E. P. Simoncelli. 图像质量评估：从错误可见性到结构相似性.
    IEEE 图像处理汇刊, 13(4):600–612, 2004.'
- en: '[130] Z. Wang, L. Shen, M. Xu, M. Yu, K. Wang, and Y. Lin. Domain adaptation
    for underwater image enhancement. IEEE Transactions on Image Processing, 32:1442–1457,
    2023.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Z. Wang, L. Shen, M. Xu, M. Yu, K. Wang, 和 Y. Lin. 用于水下图像增强的领域适应. IEEE
    图像处理汇刊, 32:1442–1457, 2023.'
- en: '[131] Y. Wei, Z. Zheng, and X. Jia. Uhd underwater image enhancement via frequency-spatial
    domain aware network. In Proceedings of the Asian Conference on Computer Vision,
    pages 299–314, 2022.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Y. Wei, Z. Zheng, 和 X. Jia. 通过频域-空间域感知网络进行超高分辨率水下图像增强. 在亚洲计算机视觉会议论文集,
    页码 299–314, 2022.'
- en: '[132] J. Wen, J. Cui, G. Yang, B. Zhao, Y. Zhai, Z. Gao, L. Dou, and B. M.
    Chen. Waterformer: A global–local transformer for underwater image enhancement
    with environment adaptor. IEEE Robotics & Automation Magazine, 2024.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] J. Wen, J. Cui, G. Yang, B. Zhao, Y. Zhai, Z. Gao, L. Dou, 和 B. M. Chen.
    Waterformer: 一种用于水下图像增强的全局-局部变换器，配备环境适配器. IEEE 机器人与自动化杂志, 2024.'
- en: '[133] R. J. Williams. Simple statistical gradient-following algorithms for
    connectionist reinforcement learning. Machine learning, 8:229–256, 1992.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] R. J. Williams. 简单的统计梯度跟随算法用于连接主义强化学习. 机器学习, 8:229–256, 1992.'
- en: '[134] Z. Wu, Z. Wu, X. Chen, Y. Lu, and J. Yu. Self-supervised underwater image
    generation for underwater domain pre-training. IEEE Transactions on Instrumentation
    and Measurement, 2024.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Z. Wu, Z. Wu, X. Chen, Y. Lu, 和 J. Yu. 自监督水下图像生成用于水下领域预训练. IEEE 仪器与测量汇刊,
    2024.'
- en: '[135] Y. Xie, L. Kong, K. Chen, Z. Zheng, X. Yu, Z. Yu, and B. Zheng. Uveb:
    A large-scale benchmark and baseline towards real-world underwater video enhancement.
    In IEEE Conference on Computer Vision and Pattern Recognition, 2024.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Y. Xie, L. Kong, K. Chen, Z. Zheng, X. Yu, Z. Yu, 和 B. Zheng. Uveb: 一个大规模基准和基线，用于现实世界的水下视频增强.
    在 IEEE 计算机视觉与模式识别会议, 2024.'
- en: '[136] X. Xu, R. Wang, and J. Lu. Low-light image enhancement via structure
    modeling and guidance. In IEEE Conference on Computer Vision and Pattern Recognition,
    pages 9893–9903, 2023.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] X. Xu, R. Wang, 和 J. Lu. 通过结构建模与引导进行低光图像增强. 在 IEEE 计算机视觉与模式识别会议, 页码 9893–9903,
    2023.'
- en: '[137] X. Xue, Z. Hao, L. Ma, Y. Wang, and R. Liu. Joint luminance and chrominance
    learning for underwater image enhancement. IEEE Signal Processing Letters, 28:818–822,
    2021.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] X. 薛, Z. 郝, L. 马, Y. 王, 和 R. 刘. 联合亮度和色度学习用于水下图像增强. 《IEEE 信号处理通讯》，28:818–822,
    2021。'
- en: '[138] X. Xue, Z. Li, L. Ma, Q. Jia, R. Liu, and X. Fan. Investigating intrinsic
    degradation factors by multi-branch aggregation for real-world underwater image
    enhancement. Pattern Recognition, 133:109041, 2023.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] X. 薛, Z. 李, L. 马, Q. 贾, R. 刘, 和 X. 范. 通过多分支聚合研究内在退化因素用于现实世界水下图像增强. 《模式识别》，133:109041,
    2023。'
- en: '[139] H. Yan, Z. Zhang, J. Xu, T. Wang, P. An, A. Wang, and Y. Duan. Uw-cyclegan:
    Model-driven cyclegan for underwater image restoration. IEEE Transactions on Geoscience
    and Remote Sensing, 2023.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] H. 闫, Z. 张, J. 许, T. 王, P. 安, A. 王, 和 Y. 段. Uw-cyclegan: 基于模型的 Cyclegan
    用于水下图像修复. 《IEEE 地球科学与遥感学报》，2023。'
- en: '[140] S. Yan, X. Chen, Z. Wu, M. Tan, and J. Yu. Hybrur: A hybrid physical-neural
    solution for unsupervised underwater image restoration. IEEE Transactions on Image
    Processing, 2023.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] S. 闫, X. 陈, Z. 吴, M. 谭, 和 J. 余. Hybrur: 一种混合物理-神经解决方案用于无监督水下图像修复. 《IEEE
    图像处理学报》，2023。'
- en: '[141] X. Yan, W. Qin, Y. Wang, G. Wang, and X. Fu. Attention-guided dynamic
    multi-branch neural network for underwater image enhancement. Knowledge-Based
    Systems, 258:110041, 2022.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] X. 闫, W. 秦, Y. 王, G. 王, 和 X. 傅. 注意力引导的动态多分支神经网络用于水下图像增强. 《知识基础系统》，258:110041,
    2022。'
- en: '[142] G. Yang, G. Kang, J. Lee, and Y. Cho. Joint-id: Transformer-based joint
    image enhancement and depth estimation for underwater environments. IEEE Sensors
    Journal, 2023.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] G. 杨, G. 康, J. 李, 和 Y. 曹. Joint-id: 基于 Transformer 的水下环境图像增强与深度估计. 《IEEE
    传感器期刊》，2023。'
- en: '[143] H.-H. Yang, K.-C. Huang, and W.-T. Chen. Laffnet: A lightweight adaptive
    feature fusion network for underwater image enhancement. In IEEE International
    Conference on Robotics and Automation, pages 685–692, 2021.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] H.-H. 杨, K.-C. 黄, 和 W.-T. 陈. Laffnet: 一种轻量级自适应特征融合网络用于水下图像增强. 见于 IEEE
    国际机器人与自动化会议，页码 685–692, 2021。'
- en: '[144] M. Yang, J. Hu, C. Li, G. Rohde, Y. Du, and K. Hu. An in-depth survey
    of underwater image enhancement and restoration. IEEE Access, 7:123638–123657,
    2019.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] M. 杨, J. 胡, C. 李, G. 罗德, Y. 杜, 和 K. 胡. 水下图像增强与恢复的深入调查. 《IEEE Access》，7:123638–123657,
    2019。'
- en: '[145] M. Yang, K. Hu, Y. Du, Z. Wei, Z. Sheng, and J. Hu. Underwater image
    enhancement based on conditional generative adversarial network. Signal Processing:
    Image Communication, 81:115723, 2020.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] M. 杨, K. 胡, Y. 杜, Z. 韦, Z. 盛, 和 J. 胡. 基于条件生成对抗网络的水下图像增强. 《信号处理：图像通信》，81:115723,
    2020。'
- en: '[146] M. Yang and A. Sowmya. An underwater color image quality evaluation metric.
    IEEE Transactions on Image Processing, 24(12):6062–6071, 2015.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] M. 杨 和 A. Sowmya. 一种水下彩色图像质量评价指标. 《IEEE 图像处理学报》，24(12):6062–6071, 2015。'
- en: '[147] T. Ye, S. Chen, Y. Liu, Y. Ye, E. Chen, and Y. Li. Underwater light field
    retention: Neural rendering for underwater imaging. In IEEE Conference on Computer
    Vision and Pattern Recognition Workshops, pages 488–497, 2022.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] T. 叶, S. 陈, Y. 刘, Y. 叶, E. 陈, 和 Y. 李. 水下光场保留：水下成像的神经渲染. 见于 IEEE 计算机视觉与模式识别会议研讨会，页码
    488–497, 2022。'
- en: '[148] J. Yin, Y. Wang, B. Guan, X. Zeng, and L. Guo. Unsupervised underwater
    image enhancement based on disentangled representations via double-order contrastive
    loss. IEEE Transactions on Geoscience and Remote Sensing, 2024.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] J. 尹, Y. 王, B. 关, X. 曾, 和 L. 郭. 基于双重对比损失的解耦表示的无监督水下图像增强. 《IEEE 地球科学与遥感学报》，2024。'
- en: '[149] M. Yu, L. Shen, Z. Wang, and X. Hua. Task-friendly underwater image enhancement
    for machine vision applications. IEEE Transactions on Geoscience and Remote Sensing,
    2023.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] M. 余, L. 沈, Z. 王, 和 X. 花. 面向机器视觉应用的任务友好型水下图像增强. 《IEEE 地球科学与遥感学报》，2023。'
- en: '[150] D. Zhang, Y. Guo, J. Zhou, W. Zhang, Z. Lin, K. Polat, F. Alenezi, and
    A. Alhudhaif. Tanet: Transmission and atmospheric light driven enhancement of
    underwater images. Expert Systems with Applications, 242:122693, 2024.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] D. 张, Y. 郭, J. 周, W. 张, Z. 林, K. Polat, F. Alenezi, 和 A. Alhudhaif. Tanet:
    传输和大气光驱动的水下图像增强. 《专家系统与应用》，242:122693, 2024。'
- en: '[151] D. Zhang, C. Wu, J. Zhou, W. Zhang, C. Li, and Z. Lin. Hierarchical attention
    aggregation with multi-resolution feature learning for gan-based underwater image
    enhancement. Engineering Applications of Artificial Intelligence, 125:106743,
    2023.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] D. 张, C. 吴, J. 周, W. 张, C. 李, 和 Z. 林. 基于多分辨率特征学习的分层注意力聚合用于基于 GAN 的水下图像增强.
    《工程应用人工智能》，125:106743, 2023。'
- en: '[152] D. Zhang, C. Wu, J. Zhou, W. Zhang, Z. Lin, K. Polat, and F. Alenezi.
    Robust underwater image enhancement with cascaded multi-level sub-networks and
    triple attention mechanism. Neural Networks, 169:685–697, 2024.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] D. Zhang, C. Wu, J. Zhou, W. Zhang, Z. Lin, K. Polat, 和 F. Alenezi. 通过级联多层子网络和三重注意力机制的鲁棒水下图像增强。神经网络，169:685–697，2024年。'
- en: '[153] D. Zhang, J. Zhou, W. Zhang, Z. Lin, J. Yao, K. Polat, F. Alenezi, and
    A. Alhudhaif. Rex-net: A reflectance-guided underwater image enhancement network
    for extreme scenarios. Expert Systems with Applications, page 120842, 2023.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] D. Zhang, J. Zhou, W. Zhang, Z. Lin, J. Yao, K. Polat, F. Alenezi, 和
    A. Alhudhaif. Rex-net: 一种反射指导的水下图像增强网络，用于极端场景。专家系统与应用，页码 120842，2023年。'
- en: '[154] F. Zhang, S. You, Y. Li, and Y. Fu. Atlantis: Enabling underwater depth
    estimation with stable diffusion. In IEEE Conference on Computer Vision and Pattern
    Recognition, 2024.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] F. Zhang, S. You, Y. Li, 和 Y. Fu. Atlantis: 通过稳定扩散实现水下深度估计。发表于 IEEE 计算机视觉与模式识别会议，2024年。'
- en: '[155] K. Zhang, W. Ren, W. Luo, W.-S. Lai, B. Stenger, M.-H. Yang, and H. Li.
    Deep image deblurring: A survey. International Journal of Computer Vision, 130(9):2103–2130,
    2022.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] K. Zhang, W. Ren, W. Luo, W.-S. Lai, B. Stenger, M.-H. Yang, 和 H. Li.
    深度图像去模糊：综述。国际计算机视觉期刊，130(9):2103–2130，2022年。'
- en: '[156] S. Zhang, S. Zhao, D. An, D. Li, and R. Zhao. Liteenhancenet: A lightweight
    network for real-time single underwater image enhancement. Expert Systems with
    Applications, 240:122546, 2024.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] S. Zhang, S. Zhao, D. An, D. Li, 和 R. Zhao. Liteenhancenet: 一种用于实时单幅水下图像增强的轻量级网络。专家系统与应用，240:122546，2024年。'
- en: '[157] W. Zhang, L. Dong, X. Pan, P. Zou, L. Qin, and W. Xu. A survey of restoration
    and enhancement for underwater images. IEEE Access, 7:182259–182279, 2019.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] W. Zhang, L. Dong, X. Pan, P. Zou, L. Qin, 和 W. Xu. 水下图像的修复与增强综述。IEEE
    Access，7:182259–182279，2019年。'
- en: '[158] W. Zhang, Y. Liu, C. Dong, and Y. Qiao. Ranksrgan: Super resolution generative
    adversarial networks with learning to rank. IEEE Transactions on Pattern Analysis
    and Machine Intelligence, 44(10):7149–7166, 2021.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] W. Zhang, Y. Liu, C. Dong, 和 Y. Qiao. Ranksrgan: 学习排名的超分辨率生成对抗网络。IEEE
    模式分析与机器智能汇刊，44(10):7149–7166，2021年。'
- en: '[159] W. Zhang, P. Zhuang, H.-H. Sun, G. Li, S. Kwong, and C. Li. Underwater
    image enhancement via minimal color loss and locally adaptive contrast enhancement.
    IEEE Transactions on Image Processing, 31:3997–4010, 2022.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] W. Zhang, P. Zhuang, H.-H. Sun, G. Li, S. Kwong, 和 C. Li. 通过最小颜色损失和局部自适应对比度增强进行水下图像增强。IEEE
    图像处理汇刊，31:3997–4010，2022年。'
- en: '[160] Z. Zhang, Z. Jiang, J. Liu, X. Fan, and R. Liu. Waterflow: heuristic
    normalizing flow for underwater image enhancement and beyond. In ACM International
    Conference on Multimedia, pages 7314–7323, 2023.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Z. Zhang, Z. Jiang, J. Liu, X. Fan, 和 R. Liu. Waterflow: 启发式归一化流用于水下图像增强及其他应用。发表于
    ACM 国际多媒体会议，页码 7314–7323，2023年。'
- en: '[161] C. Zhao, W. Cai, C. Dong, and C. Hu. Wavelet-based fourier information
    interaction with frequency diffusion adjustment for underwater image restoration.
    arXiv preprint arXiv:2311.16845, 2023.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] C. Zhao, W. Cai, C. Dong, 和 C. Hu. 基于小波的傅里叶信息交互与频率扩散调整用于水下图像修复。arXiv
    预印本 arXiv:2311.16845，2023年。'
- en: '[162] C. Zhao, W. Cai, C. Dong, and Z. Zeng. Toward sufficient spatial-frequency
    interaction for gradient-aware underwater image enhancement. arXiv preprint arXiv:2309.04089,
    2023.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] C. Zhao, W. Cai, C. Dong, 和 Z. Zeng. 针对梯度感知水下图像增强的充分空间频率交互。arXiv 预印本
    arXiv:2309.04089，2023年。'
- en: '[163] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu. Object detection with deep
    learning: A review. IEEE transactions on neural networks and learning systems,
    30(11):3212–3232, 2019.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Z.-Q. Zhao, P. Zheng, S.-t. Xu, 和 X. Wu. 基于深度学习的物体检测：综述。IEEE 神经网络与学习系统汇刊，30(11):3212–3232，2019年。'
- en: '[164] Y. Zheng, W. Chen, R. Lin, T. Zhao, and P. Le Callet. Uif: An objective
    quality assessment for underwater image enhancement. IEEE Transactions on Image
    Processing, 31:5456–5468, 2022.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Y. Zheng, W. Chen, R. Lin, T. Zhao, 和 P. Le Callet. Uif: 水下图像增强的客观质量评估。IEEE
    图像处理汇刊，31:5456–5468，2022年。'
- en: '[165] J. Zhou, Q. Gai, D. Zhang, K.-M. Lam, W. Zhang, and X. Fu. Iacc: Cross-illumination
    awareness and color correction for underwater images under mixed natural and artificial
    lighting. IEEE Transactions on Geoscience and Remote Sensing, 62:1–15, 2024.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] J. Zhou, Q. Gai, D. Zhang, K.-M. Lam, W. Zhang, 和 X. Fu. Iacc: 混合自然和人工光照下的水下图像交叉照明感知和颜色校正。IEEE
    地球科学与遥感汇刊，62:1–15，2024年。'
- en: '[166] J. Zhou, B. Li, D. Zhang, J. Yuan, W. Zhang, Z. Cai, and J. Shi. Ugif-net:
    An efficient fully guided information flow network for underwater image enhancement.
    IEEE Transactions on Geoscience and Remote Sensing, 2023.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] J. Zhou, B. Li, D. Zhang, J. Yuan, W. Zhang, Z. Cai, 和 J. Shi. Ugif-net:
    一种高效的完全引导信息流网络用于水下图像增强。IEEE 地球科学与遥感学报, 2023。'
- en: '[167] J. Zhou, T. Liang, Z. He, D. Zhang, W. Zhang, X. Fu, and C. Li. Waterhe-nerf:
    Water-ray tracing neural radiance fields for underwater scene reconstruction.
    arXiv preprint arXiv:2312.06946, 2023.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. Zhou, T. Liang, Z. He, D. Zhang, W. Zhang, X. Fu, 和 C. Li. Waterhe-nerf:
    用于水下场景重建的水光线追踪神经辐射场。arXiv 预印本 arXiv:2312.06946, 2023。'
- en: '[168] J. Zhou, J. Sun, C. Li, Q. Jiang, M. Zhou, K.-M. Lam, W. Zhang, and X. Fu.
    Hclr-net: Hybrid contrastive learning regularization with locally randomized perturbation
    for underwater image enhancement. International Journal of Computer Vision, pages
    1–25, 2024.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] J. Zhou, J. Sun, C. Li, Q. Jiang, M. Zhou, K.-M. Lam, W. Zhang, 和 X.
    Fu. Hclr-net: 用于水下图像增强的混合对比学习正则化与局部随机扰动。国际计算机视觉杂志, 页码 1–25, 2024。'
- en: '[169] J. Zhou, J. Sun, W. Zhang, and Z. Lin. Multi-view underwater image enhancement
    method via embedded fusion mechanism. Engineering Applications of Artificial Intelligence,
    121:105946, 2023.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] J. Zhou, J. Sun, W. Zhang, 和 Z. Lin. 通过嵌入式融合机制的多视角水下图像增强方法。工程应用人工智能,
    121:105946, 2023。'
- en: '[170] J. Zhou, T. Yang, and W. Zhang. Underwater vision enhancement technologies:
    a comprehensive review, challenges, and recent trends. Applied Intelligence, 53(3):3594–3621,
    2023.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] J. Zhou, T. Yang, 和 W. Zhang. 水下视觉增强技术：全面回顾、挑战及最新趋势。应用智能, 53(3):3594–3621,
    2023。'
- en: '[171] Y. Zhou and K. Yan. Domain adaptive adversarial learning based on physics
    model feedback for underwater image enhancement. arXiv preprint arXiv:2002.09315,
    2020.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Y. Zhou 和 K. Yan. 基于物理模型反馈的领域自适应对抗学习用于水下图像增强。arXiv 预印本 arXiv:2002.09315,
    2020。'
- en: '[172] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image
    translation using cycle-consistent adversarial networks. In IEEE International
    Conference on Computer Vision, pages 2223–2232, 2017.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] J.-Y. Zhu, T. Park, P. Isola, 和 A. A. Efros. 使用循环一致对抗网络的无配对图像到图像转换。IEEE
    国际计算机视觉大会, 页码 2223–2232, 2017。'
- en: '[173] P. Zhu, Y. Liu, Y. Wen, M. Xu, X. Fu, and S. Liu. Unsupervised underwater
    image enhancement via content-style representation disentanglement. Engineering
    Applications of Artificial Intelligence, 126:106866, 2023.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] P. Zhu, Y. Liu, Y. Wen, M. Xu, X. Fu, 和 S. Liu. 通过内容-风格表示解耦的无监督水下图像增强。工程应用人工智能,
    126:106866, 2023。'
- en: '[174] P. Zhu, Y. Liu, M. Xu, X. Fu, N. Wang, and S. Liu. Unsupervised multiple
    representation disentanglement framework for improved underwater visual perception.
    IEEE Journal of Oceanic Engineering, 2023.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] P. Zhu, Y. Liu, M. Xu, X. Fu, N. Wang, 和 S. Liu. 无监督多重表示解耦框架以改善水下视觉感知。IEEE
    海洋工程期刊, 2023。'
- en: '[175] P. Zhuang, J. Wu, F. Porikli, and C. Li. Underwater image enhancement
    with hyper-laplacian reflectance priors. IEEE Transactions on Image Processing,
    31:5442–5455, 2022.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] P. Zhuang, J. Wu, F. Porikli, 和 C. Li. 使用超拉普拉斯反射先验的水下图像增强。IEEE 图像处理学报,
    31:5442–5455, 2022。'
