- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:33:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:33:15
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2404.10076] Field-Programmable Gate Array Architecture for Deep Learning:
    Survey & Future Directions'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2404.10076] 深度学习的现场可编程门阵列架构：调查与未来方向'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10076](https://ar5iv.labs.arxiv.org/html/2404.10076)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10076](https://ar5iv.labs.arxiv.org/html/2404.10076)
- en: This work has been submitted to the IEEE for possible publication. Copyright
    may be transferred without notice, after which this version may no longer be accessible.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作已提交至 IEEE 以供可能发表。版权可能会在未经通知的情况下转移，此版本可能不再可访问。
- en: 'Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的现场可编程门阵列架构：调查与未来方向
- en: 'Andrew Boutros,  Aman Arora,  Vaughn Betz A. Boutros and V. Betz are with the
    Department of Electrical and Computer Engineering, University of Toronto, Toronto,
    ON M5S 3G4, Canada (e-mails: andrew.boutros@mail.utoronto.ca; vaughn@eecg.utoronto.ca).
    A. Arora is with the School of Computing and Augmented Intelligence, Arizona State
    University, Tempe, AZ 85281, USA (e-mail: aman.kbm@asu.edu).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Andrew Boutros, Aman Arora, Vaughn Betz A. Boutros 和 V. Betz 现任职于加拿大多伦多大学电气与计算机工程系，地址：Toronto,
    ON M5S 3G4 (电子邮件：andrew.boutros@mail.utoronto.ca; vaughn@eecg.utoronto.ca)。A.
    Arora 现任职于美国亚利桑那州立大学计算与增强智能学院，地址：Tempe, AZ 85281 (电子邮件：aman.kbm@asu.edu)。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning (DL) is becoming the cornerstone of numerous applications both
    in large-scale datacenters and at the edge. Specialized hardware is often necessary
    to meet the performance requirements of state-of-the-art DL models, but the rapid
    pace of change in DL models and the wide variety of systems integrating DL make
    it impossible to create custom computer chips for all but the largest markets.
    Field-programmable gate arrays (FPGAs) present a unique blend of reprogrammability
    and direct hardware execution that make them suitable for accelerating DL inference.
    They offer the ability to customize processing pipelines and memory hierarchies
    to achieve lower latency and higher energy efficiency compared to general-purpose
    CPUs and GPUs, at a fraction of the development time and cost of custom chips.
    Their diverse and high-speed IOs also enable directly interfacing the FPGA to
    the network and/or a variety of external sensors, making them suitable for both
    datacenter and edge use cases.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）正成为大规模数据中心和边缘计算中众多应用的基石。为了满足最先进的 DL 模型的性能要求，通常需要专用硬件，但 DL 模型的快速变化和集成
    DL 的系统种类繁多，使得除非是最大的市场，否则几乎不可能为所有需求定制计算芯片。现场可编程门阵列（FPGAs）具备可重新编程和直接硬件执行的独特组合，使其适合加速
    DL 推理。与通用 CPU 和 GPU 相比，它们能够定制处理管道和内存层次结构，以实现更低的延迟和更高的能效，同时开发时间和成本仅为定制芯片的很小一部分。其多样化和高速的
    IO 还使得 FPGA 可以直接与网络和/或各种外部传感器接口，使其适合于数据中心和边缘应用场景。
- en: As DL has become an ever more important workload, FPGA architectures are evolving
    to enable higher DL performance. In this article, we survey both academic and
    industrial FPGA architecture enhancements for DL. First, we give a brief introduction
    on the basics of FPGA architecture and how its components lead to strengths and
    weaknesses for DL applications. Next, we discuss different styles of DL inference
    accelerators on FPGAs that achieve state-of-the-art performance and productive
    development flows, ranging from model-specific dataflow styles to software-programmable
    overlay styles. We survey DL-specific enhancements to traditional FPGA building
    blocks including the logic blocks, arithmetic circuitry, and on-chip memories,
    as well as new DL-specialized blocks that integrate into the FPGA fabric to accelerate
    tensor computations. Finally, we discuss hybrid devices that combine processors
    and coarse-grained accelerator blocks with FPGA-like interconnect and networks-on-chip,
    and highlight promising future research directions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 DL 成为越来越重要的工作负载，FPGA 架构正在发展以实现更高的 DL 性能。本文综述了针对 DL 的学术界和工业界 FPGA 架构改进。首先，我们简要介绍了
    FPGA 架构的基本原理及其组件如何影响 DL 应用的优缺点。接下来，我们讨论了在 FPGA 上实现最先进性能和高效开发流程的不同 DL 推理加速器风格，从特定模型的数据流风格到软件可编程的覆盖风格。我们调查了对传统
    FPGA 构建块（包括逻辑块、算术电路和片上内存）的 DL 特定增强，以及集成到 FPGA 结构中的新型 DL 专用块，用于加速张量计算。最后，我们讨论了结合处理器和粗粒度加速器块的混合设备，这些设备具有类似
    FPGA 的互连和片上网络，并强调了有前景的未来研究方向。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: FPGA, architecture, deep learning, acceleration
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: FPGA，架构，深度学习，加速
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: For many years, computing machines were used to solve problems by efficiently
    executing sequences of simple and repetitive operations at very high speeds. A
    human would think of an algorithmic approach to solve a given problem and then
    use a programming language to precisely describe its steps. However, some tasks
    that are easy for the human brain to perform are very difficult to describe algorithmically.
    Detecting a human in an image is one example of such a task. The high dimensionality
    of the input and the large number of variations in body pose, size within the
    image, clothing, image-capture angle, and lighting conditions make it impossible
    to formulate a set of conditions whose satisfaction implies a human is in the
    image. Therefore, solving such tasks classically required a domain expert to hand-craft
    a set of (lower dimensional) features that can be extracted from the high-dimensional
    input. Then, these features are used to train a statistical classifier with many
    examples and their corresponding output predictions [[1](#bib.bib1)]. This approach,
    typically referred to as classical *machine learning* (ML), requires designing
    a new feature extractor for each use case and its achieved accuracy is highly
    dependent on how well these hand-crafted features capture the key relevant data
    patterns from the original high-dimensional input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，计算机通过以非常高的速度高效地执行简单且重复的操作序列来解决问题。人类会构思一种算法方法来解决特定问题，然后使用编程语言精确描述其步骤。然而，一些对人脑来说简单的任务对于算法描述却非常困难。检测图像中的人类就是一个这样的任务。输入的高维度以及身体姿势、图像内大小、衣物、拍摄角度和光照条件的众多变化使得很难制定一组条件来判断图像中是否有一个人。因此，经典方法需要领域专家手工设计一组（低维度）特征，从高维输入中提取这些特征。然后，这些特征用于用许多示例及其对应的输出预测来训练统计分类器[[1](#bib.bib1)]。这种方法，通常称为经典*机器学习*（ML），需要为每个用例设计新的特征提取器，其实现的准确度高度依赖于这些手工设计的特征从原始高维输入中捕获关键相关数据模式的能力。
- en: '![Refer to caption](img/af2404d0f2eae87cce3b3283ea32d141.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/af2404d0f2eae87cce3b3283ea32d141.png)'
- en: 'Figure 1: Comparison between the classic ML approach using hand-crafted features
    to train a statistical classifier (top), and DL models trained directly on input
    data to perform both feature extraction and classification (bottom).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：经典的机器学习方法通过手工设计的特征训练统计分类器（上图），与直接在输入数据上训练的深度学习模型以执行特征提取和分类（下图）之间的比较。
- en: 'With the abundance of training data and the continuous improvement of compute
    capabilities in recent years, it became feasible to train large artificial neural
    networks using theory and algorithms that have been formulated back in the 1980s [[2](#bib.bib2)].
    These neural networks are typically composed of a deep cascade of layers, and
    therefore are referred to as *deep learning* (DL) models. Each layer contains
    a number of *neurons* performing a weighted sum of their inputs. As illustrated
    in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Field-Programmable Gate Array
    Architecture for Deep Learning: Survey & Future Directions"), the key distinction
    from classical ML methods is that DL models can learn to extract features and
    classify them directly from the input training data instead of relying on features
    hand-crafted by a domain expert, resulting in better accuracy and wider applicability
    to different domains. Since deep neural networks (DNNs) first demonstrated their
    superior quality of results in 2012 on visual recognition and image classification
    tasks [[3](#bib.bib3)], there has been an avalanche of innovations in building
    better DL models that can achieve higher accuracy in many different domains such
    as natural language processing (NLP) [[4](#bib.bib4)], recommendation systems [[5](#bib.bib5)],
    and content generation [[6](#bib.bib6)]. Nowadays, DL models enable a myriad of
    day-to-day end user applications [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)],
    facilitate software development [[10](#bib.bib10)], boost computer chip design
    productivity [[11](#bib.bib11)], and push the boundaries of human knowledge by
    discovering more efficient computational algorithms [[12](#bib.bib12)] and solving
    long-standing scientific problems [[13](#bib.bib13)].'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '随着近年来训练数据的丰富和计算能力的持续提升，利用自1980年代制定的理论和算法训练大型人工神经网络变得可行[[2](#bib.bib2)]。这些神经网络通常由深层级联的层组成，因此被称为*深度学习*（DL）模型。每一层包含多个*神经元*，这些神经元对输入进行加权求和。如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Field-Programmable Gate Array Architecture for Deep
    Learning: Survey & Future Directions")所示，与经典的机器学习方法相比，DL模型的关键区别在于其能够直接从输入训练数据中学习提取特征并进行分类，而无需依赖领域专家手工设计的特征，这导致了更高的准确性和更广泛的适用性。自2012年深度神经网络（DNNs）首次在视觉识别和图像分类任务中展示出卓越的结果[[3](#bib.bib3)]以来，围绕构建更优DL模型的创新如雪崩般涌现，这些模型能够在自然语言处理（NLP）[[4](#bib.bib4)]、推荐系统[[5](#bib.bib5)]和内容生成[[6](#bib.bib6)]等多个不同领域实现更高的准确率。如今，DL模型推动了各种日常用户应用[[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)]，促进了软件开发[[10](#bib.bib10)]，提升了计算机芯片设计的生产力[[11](#bib.bib11)]，并通过发现更高效的计算算法[[12](#bib.bib12)]和解决长期存在的科学问题[[13](#bib.bib13)]推动了人类知识的边界。'
- en: However, this comes at the cost of a significantly higher computational complexity
    and memory footprint compared to classical ML methods [[14](#bib.bib14)]. Google
    has estimated that if their users perform voice search for only 3 minutes per
    day using DL-based speech recognition on general-purpose CPUs, it would require
    doubling their datacenters’ compute capacity [[15](#bib.bib15)]. A recent report [[16](#bib.bib16)]
    estimates that ChatGPT, OpenAI’s conversational DL model, costs around $0.7M in
    compute hardware costs per day to serve a small fraction of queries compared to
    those processed by the Google search engine. These DL compute demands are rapidly
    growing, challenging the capabilities of conventional general-purpose compute
    platforms. Therefore, application-specific (ASIC) accelerators are deployed in
    both datacenters and edge devices to increase the efficiency of DL computation [[15](#bib.bib15)].
    In addition, with DL being such a pervasive workload, it is also driving architectural
    innovations in almost all forms of general-purpose compute platforms to improve
    their DL compute efficiency. For example, Intel’s fourth generation Xeon (Sapphire
    Rapids) central processing units (CPUs) support more efficient DL-targeted tensor
    instructions [[17](#bib.bib17)] and next-generation AMD Ryzen 7000 processors
    are integrating acceleration engines for artificial intelligence (AI) workloads [[18](#bib.bib18)].
    Modern graphics processing units (GPUs) also include specialized tensor cores
    to improve the efficiency of the matrix multiplications extensively used in DL
    workloads. While tensor operations are key in DL, they are not the entire compute
    pipeline and bottlenecks can still occur elsewhere. As an example, Nvidia recently
    integrated dedicated engines for DL preprocessing operations such as JPEG image
    decoders in their A100 GPUs to address one such bottleneck [[19](#bib.bib19)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与传统的机器学习方法相比，这会导致显著更高的计算复杂性和内存占用[[14](#bib.bib14)]。谷歌估计，如果其用户每天仅使用基于深度学习的语音识别进行3分钟的语音搜索，就需要将其数据中心的计算能力翻倍[[15](#bib.bib15)]。最近的一份报告[[16](#bib.bib16)]估计，OpenAI的对话式深度学习模型ChatGPT，每天在计算硬件上的成本约为70万美元，仅能处理一小部分查询，相比之下，谷歌搜索引擎处理的查询量要大得多。这些深度学习计算需求迅速增长，挑战了传统通用计算平台的能力。因此，应用专用集成电路（ASIC）加速器被部署在数据中心和边缘设备中，以提高深度学习计算的效率[[15](#bib.bib15)]。此外，由于深度学习的广泛应用，它也推动了几乎所有形式的通用计算平台在架构上的创新，以提高其深度学习计算效率。例如，英特尔第四代Xeon（Sapphire
    Rapids）中央处理单元（CPU）支持更高效的深度学习目标张量指令[[17](#bib.bib17)]，而下一代AMD Ryzen 7000处理器则集成了人工智能（AI）工作负载的加速引擎[[18](#bib.bib18)]。现代图形处理单元（GPU）还包括专门的张量核心，以提高在深度学习工作负载中广泛使用的矩阵乘法的效率。尽管张量操作在深度学习中至关重要，但它们并不是整个计算管道的全部，瓶颈仍然可能出现在其他地方。例如，Nvidia最近在其A100
    GPU中集成了专用的深度学习预处理操作引擎，如JPEG图像解码器，以解决其中一个瓶颈[[19](#bib.bib19)]。
- en: The architecture of field-programmable gate arrays (FPGAs) is similarly driven
    by their key use cases. Therefore, we are also starting to witness many FPGA architecture
    changes targeted at making them more efficient for DL. In this article, we survey
    proposals and innovations from both academia and industry to optimize FPGA architecture
    specifically for DL. We first present an introductory tutorial on FPGA architecture
    through a DL lens, highlighting the key strengths, weaknesses, and opportunities
    of FPGAs in the area of DL acceleration. Then, we highlight the key design styles
    of DL accelerators on FPGAs with selected examples from the broad literature available
    on this topic. However, this article is not intended to be a comprehensive survey
    of FPGA-based DL accelerator implementations and toolflows. We refer interested
    readers to [[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)] which cover
    this area in more detail. Next, we explain the general methodology used to model
    and evaluate new FPGA architectures. We discuss DL-driven architecture enhancements
    in modern FPGAs, starting from conventional FPGA blocks (e.g. logic elements and
    embedded hard blocks) and moving on to new DL-specific blocks (e.g. tensor blocks)
    as well as on-die coarse-grained accelerators (e.g. AI engines) and in-package
    DL chiplets. Finally, we present our perspective on the future of reconfigurable
    devices in the DL domain and identify interesting research directions in this
    area.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件可编程门阵列（FPGAs）的架构同样受到其关键应用场景的驱动。因此，我们也开始看到许多针对使FPGAs在深度学习（DL）中更高效的架构变化。在本文中，我们调查了来自学术界和工业界的提案和创新，旨在优化FPGAs的架构以专门用于DL。我们首先通过DL的视角呈现一个关于FPGA架构的介绍教程，突出FPGAs在DL加速领域的关键优势、劣势和机会。接着，我们重点介绍了DL加速器在FPGAs上的关键设计风格，并从广泛的文献中选取了相关示例。然而，本文并不打算对FPGA基础的DL加速器实现和工具流程进行全面的综述。我们建议感兴趣的读者参考[[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)]，这些文献对该领域进行了更详细的覆盖。接下来，我们解释了用于建模和评估新FPGA架构的一般方法。我们讨论了现代FPGAs中的DL驱动架构增强，从传统FPGA模块（例如逻辑元素和嵌入式硬块）开始，再到新的DL专用模块（例如张量块）、在芯片上的粗粒度加速器（例如AI引擎）以及封装内的DL芯片模块。最后，我们提出了对未来可重构设备在DL领域的展望，并识别了该领域中的有趣研究方向。
- en: II FPGA for DL Acceleration
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II FPGA在深度学习加速中的应用
- en: II-A Key DL Acceleration Requirements
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 关键的深度学习加速需求
- en: '![Refer to caption](img/4d95e2cec7b4a396b2f1995afad739c5.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![请参考说明](img/4d95e2cec7b4a396b2f1995afad739c5.png)'
- en: 'Figure 2: The training and inference phases of a DL model. Training is performed
    by a few DL experts on large-scale compute clusters and requires many design iterations.
    When model accuracy is satisfactory, it is deployed for inference with varying
    performance requirements depending on the application (latency-tolerant vs. real-time).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：深度学习模型的训练和推理阶段。训练由少数深度学习专家在大规模计算集群上进行，并且需要多次设计迭代。当模型精度令人满意时，它会根据应用的不同性能要求（容忍延迟与实时）进行推理部署。
- en: 'As illustrated in Fig. [2](#S2.F2 "Figure 2 ‣ II-A Key DL Acceleration Requirements
    ‣ II FPGA for DL Acceleration ‣ Field-Programmable Gate Array Architecture for
    Deep Learning: Survey & Future Directions"), a DL model goes through two main
    phases during its life-cycle: training and inference. During training, a small
    group of DL scientists architect the model and train it using huge datasets (e.g.
    $>$570 GB for the GPT-3 language model [[23](#bib.bib23)]) to achieve the desired
    quality of results. This process is very compute/memory intensive due to the large
    amount of data used for training and usually takes tens to hundreds of design
    iterations to optimize the model. Therefore, it is typically performed on large
    clusters of compute machines and accelerators in a datacenter. The final product
    of the training process is a model architecture and values of its trainable parameters
    or *weights*, which are then deployed in a production-scale application to perform
    inference on new data samples that were not part of the training dataset. Depending
    on the deployment environment (cloud/datacenter vs. edge/embedded) and the nature
    of the application (latency-tolerant vs. real-time), DL inference can have different
    compute requirements and constraints. Consequently, the hardware used to accelerate
    DL training and inference has to be optimized for different metrics and use cases,
    creating potential markets for different acceleration platforms (e.g. GPUs, FPGAs,
    and ASICs) based on their characteristic strengths and weaknesses.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[2](#S2.F2 "Figure 2 ‣ II-A Key DL Acceleration Requirements ‣ II FPGA for
    DL Acceleration ‣ Field-Programmable Gate Array Architecture for Deep Learning:
    Survey & Future Directions")所示，深度学习（DL）模型在其生命周期中经历两个主要阶段：训练和推理。在训练阶段，一小组深度学习科学家设计模型并使用大量数据集（例如，GPT-3语言模型的训练数据超过570
    GB[[23](#bib.bib23)]）进行训练，以实现期望的结果质量。由于训练过程中使用的数据量庞大，这一过程计算/内存消耗非常高，并且通常需要数十到数百次设计迭代来优化模型。因此，训练通常在数据中心的大型计算机集群和加速器上进行。训练过程的最终产品是模型架构和其可训练参数或*权重*的值，这些模型随后会在生产规模的应用中部署，用于对不在训练数据集中的新数据样本进行推理。根据部署环境（云/数据中心与边缘/嵌入式）和应用的性质（容忍延迟与实时），深度学习推理的计算需求和约束可能会有所不同。因此，用于加速深度学习训练和推理的硬件必须针对不同的指标和使用场景进行优化，从而为不同加速平台（如GPU、FPGA和ASIC）基于其特性优势和劣势创造了潜在市场。'
- en: II-A1 Performance
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 性能
- en: 'Performance of DL accelerators is measured using two metrics: throughput and
    latency. Throughput is the number of input examples a specific accelerator can
    process per unit time on a given DL workload. To facilitate accelerator efficiency
    comparisons across models of different computational complexity, throughput is
    typically reported in giga or tera operations per second (GOPS or TOPS) where
    the operations are typically computed as the number of multiplies plus the number
    of accumulates as multiply-accumulate (MAC) is the dominant operation in DL workloads.
    Each accelerator has a *peak throughput* which is workload-independent, and is
    determined by the number of MACs it can perform per cycle and its maximum operating
    frequency. However, in practice it is not possible to achieve 100% utilization
    of these MAC units¹¹1Depending on the model architecture and the hardware organization,
    the MAC units can be idle during some cycles. For example, computation can stall
    when model weights are being loaded from external memory or transported from on-chip
    buffers to compute units, and some compute units can be idle when the input size
    does not exactly match the hardware compute parallelism (e.g. performing a dot
    product between two 6-element vectors on an 8-lane dot product engine)., and thus
    the *effective throughput* of an accelerator is the more realistic metric and
    is typically evaluated for each DL workload [[24](#bib.bib24)]. An efficient accelerator
    architecture aims to maximize its *compute utilization* (i.e. minimize the gap
    between peak and effective throughput). To improve their effective throughput,
    many accelerators *batch* a group of inputs to be processed at the same time.
    This enables reusing the same set of model weights across the many inputs in a
    batch to hide the memory latency of loading the next set of weights and reduces
    the number of cycles in which the MAC units remain idle. On the other hand, latency
    is the amount of time it takes the accelerator to process a single input, which
    is a key metric for real-time applications. Although batching can help improve
    the effective throughput of an accelerator, it typically increases latency since
    more time is needed to form a batch, process the entire batch of inputs, and output
    all their results at the same time. As an example, for the ResNet-50 image classification
    model, increasing the batch size from 1 to 8 inputs improves throughput by 3$\times$
    at the cost of 2.2$\times$ higher latency on an Nvidia V100 GPU [[25](#bib.bib25)].'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DL 加速器的性能通过两个指标来衡量：吞吐量和延迟。吞吐量是指特定加速器在给定 DL 负载下单位时间内可以处理的输入示例数量。为了方便在不同计算复杂度模型之间比较加速器的效率，吞吐量通常以每秒亿次或万亿次操作（GOPS
    或 TOPS）来报告，其中操作通常计算为乘法加上累加，因为乘加（MAC）是 DL 负载中的主要操作。每个加速器都有一个*峰值吞吐量*，这是与工作负载无关的，由它每周期可以执行的
    MAC 数量和其最大操作频率决定。然而，实际上不可能达到这些 MAC 单元的 100% 利用率¹¹1根据模型架构和硬件组织，MAC 单元在某些周期内可能会空闲。例如，当模型权重从外部内存加载或从片上缓冲区传输到计算单元时，计算可能会暂停，而当输入大小与硬件计算并行性不完全匹配时，一些计算单元可能会空闲（例如，在
    8 通道点积引擎上对两个 6 元素向量进行点积）。因此，加速器的*有效吞吐量*是更现实的指标，通常针对每个 DL 负载进行评估[[24](#bib.bib24)]。高效的加速器架构旨在最大化其*计算利用率*（即最小化峰值吞吐量与有效吞吐量之间的差距）。为了提高有效吞吐量，许多加速器*批处理*一组输入以同时处理。这使得可以在批处理中的多个输入上重用相同的一组模型权重，从而隐藏加载下一组权重的内存延迟，并减少
    MAC 单元空闲的周期数。另一方面，延迟是加速器处理单个输入所需的时间，这是实时应用程序的关键指标。尽管批处理可以帮助提高加速器的有效吞吐量，但它通常会增加延迟，因为需要更多时间来形成一个批次、处理整个批次的输入并同时输出所有结果。例如，对于
    ResNet-50 图像分类模型，将批量大小从 1 增加到 8 个输入可以将吞吐量提高 3$\times$，但在 Nvidia V100 GPU 上延迟增加了
    2.2$\times$[[25](#bib.bib25)]。
- en: For the training phase, latency is not a concern, and therefore DL training
    accelerators are throughput-optimized to maximize the number of training samples
    that can be processed per second. For inference, the optimization target depends
    on the use case. Applications such as DL-based image search engines or video copyright
    checks focus mainly on maximizing the number of user queries that can be served
    per second with a loose latency requirement, and therefore are throughput-oriented.
    In other applications such as pedestrian or obstacle detection in an autonomous
    vehicle, the latency of acquiring inputs from several cameras or sensor readings,
    detecting pedestrians/obstacles using one or multiple cascaded DL models, and
    then taking an action (e.g. adjusting direction or applying the brakes) is crucial
    for safety reasons.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练阶段，延迟不是问题，因此深度学习训练加速器是通过优化吞吐量来最大化每秒处理的训练样本数量。对于推理阶段，优化目标取决于使用场景。诸如基于深度学习的图像搜索引擎或视频版权检查等应用主要关注在松散的延迟要求下最大化每秒服务的用户查询数量，因此是以吞吐量为导向的。在自动驾驶车辆中进行行人或障碍物检测等应用中，从多个摄像头或传感器读取输入、使用一个或多个级联的深度学习模型检测行人/障碍物，然后采取行动（例如调整方向或刹车）的延迟对安全至关重要。
- en: II-A2 Cost and Energy Efficiency
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 成本和能源效率
- en: For both the training and inference phases, energy and cost efficiency are major
    design optimization targets for all DL accelerators. It is estimated that 35%
    of the total cost of ownership of a datacenter is spent on power [[26](#bib.bib26)].
    Therefore, with DL becoming a prominent datacenter workload, more energy efficient
    DL compute hardware directly translates to significant cost savings for service
    providers. For example, Google reported that using their ASIC tensor processing
    unit (TPUs) reduced the cost of training a ResNet-50 model for image classification
    by 38% [[27](#bib.bib27)]. For modern NLP models such as BERT [[28](#bib.bib28)],
    this cost can reach millions of dollars for each full training [[29](#bib.bib29)].
    Additionally, reducing the power consumption of datacenter compute has significant
    environmental impact, as datacenters consume very large amounts of electricity
    and by some estimates will account for $\sim$8% of the world’s electricity demand
    by 2030 [[30](#bib.bib30)]. At the other end of the deployment spectrum, DL inference
    on battery-operated edge devices usually has a very tight power budget and therefore
    requires energy-efficient compute hardware. For example, Tesla’s full self-driving
    DL inference chip was custom designed to meet an aggressive power budget of less
    than 40W [[31](#bib.bib31)]. Although such custom ASICs can offer superior energy-efficiency,
    they lack the flexibility to adapt to different systems and algorithms. In addition,
    their significant non-recurring engineering (NRE) cost and longer time-to-solution
    for design, fabrication and testing can be prohibitive for small and medium-scale
    markets.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练和推理阶段，能源和成本效率是所有深度学习加速器的重要设计优化目标。据估计，数据中心总拥有成本的35%用于电力[[26](#bib.bib26)]。因此，随着深度学习成为数据中心的重要负载，更高效的深度学习计算硬件直接转化为服务提供商的显著成本节约。例如，谷歌报告称，使用他们的ASIC张量处理单元（TPUs）将训练ResNet-50图像分类模型的成本降低了38%[[27](#bib.bib27)]。对于现代自然语言处理模型如BERT[[28](#bib.bib28)]，每次完整训练的成本可能达到数百万美元[[29](#bib.bib29)]。此外，降低数据中心计算的功耗具有显著的环境影响，因为数据中心消耗大量电力，根据一些估计，到2030年数据中心将占全球电力需求的$\sim$8%[[30](#bib.bib30)]。在部署的另一端，电池供电的边缘设备上的深度学习推理通常有非常紧张的电力预算，因此需要高效能的计算硬件。例如，特斯拉的全自驾深度学习推理芯片是专门设计的，以满足少于40W的激进电力预算[[31](#bib.bib31)]。虽然这些定制的ASIC可以提供优越的能效，但它们缺乏适应不同系统和算法的灵活性。此外，其显著的非重复工程（NRE）成本以及设计、制造和测试的较长解决时间对于小型和中型市场可能是不可承受的。
- en: II-A3 Adaptability
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 适应性
- en: Besides achieving high performance and energy-efficiency, DL accelerators must
    also be flexible to adapt to frequent algorithmic changes. New state-of-the-art
    DL models are introduced at a much faster rate than the typical design and deployment
    cycle of computer hardware. Adaptability can be achieved by making some or all
    of the system software-programmable, but software programmability adds energy
    and performance overheads (e.g. instruction fetching and decoding) compared to
    fixed-function dedicated hardware. In addition, the DL accelerator, especially
    in edge deployments, is typically part of a bigger system where it needs to interface
    with a wide variety of sensors (e.g. cameras, LiDARs) and actuators (e.g. to control
    brakes) which can have different communication protocols as well as different
    data pre- and post-processing needs. Therefore, adaptability is not only a requirement
    for the DL compute hardware, but also for interfacing with the rest of the system
    to ensure usability across a wide range of deployment use cases. Enabling such
    flexible interfacing often requires electrical adaptability due to the different
    voltage and timing requirements of different interface protocols, which can not
    be achieved by software programmability alone.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了实现高性能和能源效率，DL 加速器还必须具备灵活性，以适应频繁的算法变化。新的最先进 DL 模型的引入速度远快于计算机硬件的典型设计和部署周期。通过使系统软件部分或全部可编程可以实现适应性，但与固定功能专用硬件相比，软件可编程性会增加能源和性能开销（例如，指令获取和解码）。此外，DL
    加速器，特别是在边缘部署中，通常是更大系统的一部分，需要与各种传感器（例如，摄像头、LiDAR）和执行器（例如，控制刹车）接口，这些传感器和执行器可能具有不同的通信协议以及不同的数据预处理和后处理需求。因此，适应性不仅是
    DL 计算硬件的要求，也是与系统其他部分接口的要求，以确保在各种部署使用案例中具有可用性。实现这种灵活的接口通常需要电气适应性，因为不同接口协议具有不同的电压和时序要求，这仅靠软件编程无法实现。
- en: II-B FPGA Strengths for DL Acceleration
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B FPGA 在 DL 加速中的优势
- en: '![Refer to caption](img/18d062101d307fc305c3c75d15ed08f7.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/18d062101d307fc305c3c75d15ed08f7.png)'
- en: 'Figure 3: Unique features of FPGAs that make them an efficient acceleration
    platform for DL.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：FPGAs 的独特特性使其成为 DL 高效加速平台。
- en: 'Based on the DL acceleration requirements discussed in the previous subsection,
    we can identify key FPGA strengths (summarized in Fig. [3](#S2.F3 "Figure 3 ‣
    II-B FPGA Strengths for DL Acceleration ‣ II FPGA for DL Acceleration ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions")) that
    make them a desirable and efficient acceleration platform for specific DL use
    cases.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上一小节讨论的 DL 加速要求，我们可以识别出使 FPGA 成为特定 DL 用例理想且高效加速平台的关键优势（见图 [3](#S2.F3 "图 3
    ‣ II-B FPGA 在 DL 加速中的优势 ‣ II FPGA 在 DL 加速中的作用 ‣ 深度学习的现场可编程门阵列架构：调查与未来方向")）。
- en: Firstly, FPGAs offer fine-grained hardware programmability which allows building
    customized compute datapaths and on-chip memory sub-systems that match exactly
    the application needs. Therefore, FPGAs have an advantage compared to general-purpose
    processors (e.g. CPUs and GPUs) in use cases where customization is desired. For
    example, DL inference quality of results is tolerant of low-precision computation
    and since the energy and area of compute units drop rapidly with precision, this
    can be exploited for a more efficient acceleration solution [[32](#bib.bib32)].
    Unlike CPUs and GPUs that support only specific arithmetic precisions (e.g. int4,
    int8, fp16, fp32), an FPGA can implement custom compute units for any precision,
    including binary/ternary, narrow floating-point formats (e.g. fp8 [[33](#bib.bib33)]),
    or floating-point numbers with shared exponents (bfp) [[32](#bib.bib32)]. Since
    the most efficient precision varies across DL models and even across layers within
    a single model, this flexibility is very useful. However, the fine-grained bit-level
    hardware programmability comes with speed and area overheads as functions are
    implemented with programmable blocks and programmable routing which are slower
    and bigger than standard cell logic gates and direct wires. Therefore, the customization
    gains should outweigh these programmability overheads for an FPGA acceleration
    solution to be competitive.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，FPGA 提供了细粒度的硬件可编程性，这使得可以构建与应用需求完全匹配的定制计算数据路径和片上内存子系统。因此，与通用处理器（例如 CPU 和 GPU）相比，FPGA
    在需要定制的应用场景中具有优势。例如，深度学习推断结果对低精度计算具有容忍度，并且由于计算单元的能量和面积随着精度的降低迅速下降，这可以用于更高效的加速解决方案[[32](#bib.bib32)]。与仅支持特定算术精度（例如
    int4、int8、fp16、fp32）的 CPU 和 GPU 不同，FPGA 可以实现任何精度的自定义计算单元，包括二进制/三元组、窄浮点格式（例如 fp8
    [[33](#bib.bib33)]）或具有共享指数的浮点数（bfp）[[32](#bib.bib32)]。由于最有效的精度在不同的深度学习模型甚至单个模型中的各层之间有所不同，这种灵活性非常有用。然而，细粒度的比特级硬件可编程性带来了速度和面积开销，因为函数是用可编程块和可编程路由实现的，这些通常比标准单元逻辑门和直接电线要慢且大。因此，定制化的收益应当超过这些可编程性开销，以使
    FPGA 加速解决方案具有竞争力。
- en: Secondly, FPGAs are spatial computing devices. This means that data does not
    have to move through a memory hierarchy of caches and register files for the computation
    to be performed, and compute cores do not have to communicate through memory.
    Instead, in an FPGA, data can flow directly from distributed on-chip buffers and
    through *chained* compute units using the flexible programmable routing, without
    the need for an instruction sequence to orchestrate data movements and computations.
    This can reduce the overall compute latency as fewer cycles are spent on data
    movement across different levels of the memory hierarchy. It can also result in
    significant energy savings; for example, $\sim$99% of the energy consumed by an
    integer add operation in a 45nm CPU is spent on cache/register file accesses and
    control logic [[34](#bib.bib34)], a large portion of which can be saved when performing
    computations spatially on an FPGA.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，FPGA 是空间计算设备。这意味着数据不必通过缓存和寄存器文件的内存层次结构来执行计算，计算核心也不需要通过内存进行通信。相反，在 FPGA 中，数据可以直接从分布式的片上缓冲区流动，并通过使用灵活的可编程路由在*串联*计算单元之间传输，而无需指令序列来协调数据移动和计算。这可以减少总体计算延迟，因为在不同内存层次结构级别的数据移动所花费的周期较少。它还可以显著节省能源；例如，在45nm
    CPU 中，一个整数加法操作所消耗的能量中约有 $\sim$99% 被用于缓存/寄存器文件访问和控制逻辑[[34](#bib.bib34)]，当在 FPGA
    上进行空间计算时，可以节省其中很大一部分。
- en: Thirdly, FPGAs are flexible. Reconfiguring the FPGA with a new bitstream changes
    its hardware functionality. This offers a clear advantage over ASIC accelerators
    since the hardware can flexibly adapt to rapid changes in DL algorithms, model
    architectures and application-specific pre- or post-processing. New operations
    can be implemented in hardware, integrated into an FPGA-based accelerator architecture,
    and deployed in production in a matter of weeks [[35](#bib.bib35)]. On the other
    hand, an ASIC accelerator would need to implement this new operation on a software-programmable
    core or its host CPU resulting in degraded performance until the next generation
    chip is designed, fabricated and deployed, which can take years.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，FPGA是灵活的。通过重新配置FPGA的新比特流可以改变其硬件功能。这相对于ASIC加速器来说具有明显优势，因为硬件可以灵活适应深度学习算法、模型架构和应用特定的预处理或后处理的快速变化。新操作可以在硬件中实现，集成到基于FPGA的加速器架构中，并在几周内投入生产[[35](#bib.bib35)]。另一方面，ASIC加速器需要在软件可编程核心或其主CPU上实现这一新操作，这会导致性能下降，直到下一代芯片设计、制造和部署完成，这可能需要几年时间。
- en: Fourthly, FPGAs provide a myriad of programmable input/output (IO) interfaces.
    These IOs can flexibly implement a wide variety of protocols with different electrical
    characteristics and timing specifications. Modern FPGAs also implement hardened
    controllers for various widely-used standards for datacenter deployments such
    as ethernet, peripheral component interconnect express (PCIe), and double data
    rate (DDR) and high bandwidth (HBM) external memories. This allows efficient communication
    between the FPGA as a server accelerator card and the host CPU, and also enables
    directly connecting multiple FPGAs over the network to create many-device accelerators
    such as Microsoft’s Brainwave datacenter-scale DL accelerator [[36](#bib.bib36)].
    Additionally, the FPGA programmable logic can also implement other custom standards
    for interfacing with different sensors/actuators in embedded systems at the edge [[37](#bib.bib37)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，FPGA提供了大量可编程的输入/输出（IO）接口。这些IO可以灵活地实现各种具有不同电气特性和时序规格的协议。现代FPGA还实现了针对数据中心部署的各种广泛使用标准的硬化控制器，如以太网、外设组件互连快速（PCIe）和双倍数据率（DDR）及高带宽（HBM）外部内存。这允许FPGA作为服务器加速卡与主CPU之间进行高效通信，并且还可以直接通过网络连接多个FPGA，创建多个设备的加速器，如微软的Brainwave数据中心规模的深度学习加速器[[36](#bib.bib36)]。此外，FPGA的可编程逻辑还可以实现其他自定义标准，以便在边缘嵌入式系统中与不同传感器/执行器进行接口[[37](#bib.bib37)]。
- en: 'These unique FPGA characteristics lead to certain DL use cases where FPGAs
    have advantages compared to other acceleration solutions such as general-purpose
    CPUs/GPUs and ASIC accelerators. These are use cases that:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些独特的FPGA特性导致了某些深度学习使用场景，在这些场景中，FPGA相对于其他加速解决方案（如通用CPU/GPU和ASIC加速器）具有优势。这些使用场景包括：
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: can perform computations using low precision or non-standard number formats
    which require customized datapaths [[38](#bib.bib38), [32](#bib.bib32)]. These
    precisions are more common and generally easier to use in inference, while training
    is typically performed in higher precision floating-point formats (e.g. fp32,
    fp16) that are natively supported in general-purpose CPUs and GPUs.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用低精度或非标准数字格式进行计算，这些格式需要定制的数据路径[[38](#bib.bib38), [32](#bib.bib32)]。这些精度在推理中更为常见且通常更易使用，而训练则通常在一般用途的CPU和GPU原生支持的更高精度浮点格式（例如fp32,
    fp16）中进行。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: have tight latency constraints that prohibit batching a large number of inputs
    for processing. If an application is more throughput-oriented with loose latency
    constraints, multiple inputs can be batched and processed simultaneously. This
    provides more opportunities for reuse of on-chip values, and while this can benefit
    all computational devices it is usually particularly helpful in keeping a GPU’s
    massively parallel functional units busy. This is another reason why (latency-constrained)
    inference is a better match for FPGAs than (throughput-oriented) training.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具有严格延迟约束的应用程序不允许将大量输入进行批处理。如果应用程序更注重吞吐量且延迟约束较松，可以将多个输入批量处理。这提供了更多的芯片内部值重用机会，虽然这对所有计算设备都有好处，但通常特别有助于保持GPU的大规模并行功能单元的忙碌。这是另一个原因，为什么（延迟受限的）推理比（吞吐量导向的）训练更适合FPGA。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: can fit all model weights in the FPGA’s on-chip memory (i.e. *persistent weights*).
    The spatial nature of FPGAs enables near-memory compute with low-latency memory
    accesses and application-tailored memory organization. For bigger models, the
    diverse FPGA IOs allow directly connecting multiple FPGAs over the network to
    create bigger multi-FPGA fabrics with more on-chip memory [[36](#bib.bib36)].
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以将所有模型权重存储在FPGA的片上内存中（即*持久性权重*）。FPGA的空间特性使得近内存计算成为可能，并且具有低延迟的内存访问和应用程序定制的内存组织。对于更大的模型，多样化的FPGA
    I/O允许通过网络直接连接多个FPGA，创建更大的多FPGA织物，具有更多的片上内存 [[36](#bib.bib36)]。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: implement a DL component in a bigger system, in which the FPGA’s flexibility
    and rich IOs can play a crucial role, such as in autonomous driving systems. A
    variety of sensor/camera inputs might require classical signal/image preprocessing
    before being used as inputs to a DL model, and then the output of the DL model
    is used to control different actuators. In such cases, FPGAs offer a platform
    for accelerating the full system with custom interfaces and application-dependent
    pre- and/or post-processing [[39](#bib.bib39)].
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在更大的系统中实现DL组件，其中FPGA的灵活性和丰富的I/O可以发挥关键作用，例如在自动驾驶系统中。各种传感器/相机输入可能需要经典的信号/图像预处理，然后作为DL模型的输入，DL模型的输出用于控制不同的执行器。在这种情况下，FPGA提供了一个加速整个系统的平台，具有定制的接口和应用程序相关的预处理和/或后处理 [[39](#bib.bib39)]。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: require periodic changes to the DL model architecture with new operations and
    irregular compute graphs [[40](#bib.bib40)]. In contrast to an ASIC, these new
    changes can be implemented in hardware by simply programming the FPGA with a new
    bitstream. However, if these changes are very frequent (e.g. daily), compiling
    a new bitstream every time the model changes can be challenging due to the high
    runtime of FPGA computer-aided design (CAD) tools. In such cases, a software-programmable
    solution could be more desirable.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要对DL模型架构进行周期性的更改，添加新的操作和不规则的计算图 [[40](#bib.bib40)]。与ASIC不同，这些新更改可以通过简单地用新的比特流编程FPGA来在硬件中实现。然而，如果这些更改非常频繁（例如每天），每次模型更改时编译新的比特流可能会因FPGA计算机辅助设计（CAD）工具的高运行时间而具有挑战性。在这种情况下，软件可编程解决方案可能更具吸引力。
- en: Thus, for the rest of this article, we mainly focus on DL inference acceleration
    which better matches the unique FPGA characteristics and strengths.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本文的其余部分，我们主要关注与独特的FPGA特性和优势更匹配的DL推理加速。
- en: II-C FPGA-Based DL Inference Acceleration Styles
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C FPGA基础的DL推理加速风格
- en: This subsection presents commonly-used styles for accelerating DL inference
    on FPGAs. It is not intended to be a comprehensive survey of DL inference accelerators
    implemented on FPGAs, as our focus is primarily on enhancements to the underlying
    FPGA chip architecture.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节介绍了在FPGA上加速DL推理的常用风格。它并不打算成为FPGA上实现的DL推理加速器的全面调查，因为我们的重点主要是对基础FPGA芯片架构的改进。
- en: In 2012, AlexNet was the first convolutional neural network (CNN) to demonstrate
    the superior quality of results of DL in image classification tasks compared to
    prior machine-learning-based approaches [[3](#bib.bib3)]. Its significantly higher
    computational complexity sparked interest in accelerating DL inference using specialized
    hardware on FPGAs as co-processors. A host or an embedded CPU would offload the
    computation of the whole CNN (or specific compute-intensive layers) to an FPGA
    accelerator, and at the end perform a final softmax operation to calculate prediction
    probabilities from the final output of the accelerator, if needed. In this case,
    the FPGA accelerator is usually hand-crafted and optimized for a specific DL model
    or a group of similar models [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)]. This approach achieved significant performance and energy efficiency
    gains compared to software-based solutions on contemporaneous multi-core CPUs
    and GPUs. Although the main focus was on CNN acceleration and computer vision
    applications, several works also investigated FPGA acceleration of other types
    of DL models such as recurrent neural networks (RNNs) for sequence inputs and
    natural language processing [[45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，AlexNet成为首个展示深度学习（DL）在图像分类任务中优于以往基于机器学习方法的卷积神经网络（CNN）[[3](#bib.bib3)]。其显著更高的计算复杂度引发了使用FPGA作为协处理器加速DL推理的兴趣。主机或嵌入式CPU会将整个CNN（或特定计算密集型层）的计算转移到FPGA加速器，并在最后执行最终的softmax操作，从加速器的最终输出中计算预测概率（如有需要）。在这种情况下，FPGA加速器通常是为特定的DL模型或一组相似模型手工设计和优化的[[41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]。这种方法相比于当时的软件解决方案，在多核CPU和GPU上实现了显著的性能和能效提升。尽管主要关注的是CNN加速和计算机视觉应用，但也有一些研究调查了FPGA对其他类型DL模型的加速，如用于序列输入的递归神经网络（RNNs）和自然语言处理[[45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47)]。
- en: However, with the continuous and rapid advances in state-of-the-art DL models,
    it quickly became evident that building a custom FPGA accelerator for each model
    is extremely laborious and cannot keep pace with DL model evolution. Therefore,
    building custom hardware generators to automate this process became a major research
    focus. These hardware generators are domain-specific compilers that take as inputs
    the specifications of a target FPGA and the dataflow graph of a DL model in the
    same formats used by common DL frameworks such as TensorFlow [[48](#bib.bib48)]
    or PyTorch [[49](#bib.bib49)]. They optimize the input dataflow graph by reordering,
    simplifying and/or fusing model layers/operations, and then use a library of parameterized
    implementations of hardware modules to generate an optimized model-specific FPGA
    implementation given the target FPGA resource constraints. Although these DL hardware
    generation toolflows all share the same fundamental concepts, their generated
    accelerator architectures can be very different. Some toolflows generate streaming
    layer-pipelined architectures in which each layer has a dedicated compute engine
    and all layers coexist on the FPGA in a pipelined fashion (i.e. *spatial* execution).
    Examples of such toolflows are HPIPE [[25](#bib.bib25)], fpgaConvNet [[50](#bib.bib50)],
    DNNBuilder [[51](#bib.bib51)], and FINN [[52](#bib.bib52)]. Other toolflows generate
    architectures that have a number of more flexible processing elements (PEs) on
    which layers of a given model are mapped and executed sequentially (i.e. *temporal*
    execution) as orchestrated by control finite-state machines and microcodes [[53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56)]. Many of these toolflows
    automatically apply different FPGA-friendly DL model optimizations to further
    enhance performance such as quantizing to lower numerical precisions and exploiting
    sparsity to skip ineffectual computations with zero weights. Some work has gone
    even further, and directly uses the look-up tables (LUTs) in FPGA fabrics as the
    trainable building blocks of a neural network, instead of using only MAC operations [[57](#bib.bib57),
    [58](#bib.bib58)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着最先进的深度学习（DL）模型的不断快速发展，迅速显现出为每个模型构建定制FPGA加速器是极其繁重的，并且无法跟上DL模型的演变。因此，构建定制硬件生成器以自动化这一过程成为了主要的研究重点。这些硬件生成器是特定领域的编译器，它们将目标FPGA的规格和DL模型的数据流图作为输入，格式与常见DL框架如TensorFlow [[48](#bib.bib48)]或PyTorch [[49](#bib.bib49)]所使用的格式相同。它们通过重新排序、简化和/或融合模型层/操作来优化输入的数据流图，然后利用参数化的硬件模块库生成针对目标FPGA资源约束的优化模型特定FPGA实现。尽管这些DL硬件生成工具链都共享相同的基本概念，但它们生成的加速器架构可能非常不同。一些工具链生成流式层流水线架构，其中每一层都有一个专用的计算引擎，所有层在FPGA上以流水线方式共存（即*空间*执行）。这些工具链的例子有HPIPE [[25](#bib.bib25)]、fpgaConvNet [[50](#bib.bib50)]、DNNBuilder [[51](#bib.bib51)]和FINN [[52](#bib.bib52)]。其他工具链生成具有多个更灵活处理元素（PEs）的架构，在这些处理元素上，给定模型的层被映射并按顺序执行（即*时间*执行），由控制有限状态机和微码协调 [[53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56)]。这些工具链中的许多会自动应用不同的FPGA友好的DL模型优化，以进一步提升性能，如量化到较低的数值精度，并利用稀疏性跳过无效的零权重计算。一些工作甚至更进一步，直接使用FPGA布线中的查找表（LUTs）作为神经网络的可训练构建块，而不仅仅是使用MAC操作 [[57](#bib.bib57),
    [58](#bib.bib58)]。
- en: The generation of custom DL hardware exploits the unique reconfigurable nature
    of FPGAs by optimizing the accelerator architecture to exactly match the needs
    of a specific model or class of models, minimizing the additional overheads of
    generality. However, this comes at the cost of (1) long FPGA compile time (on
    the order of hours for synthesis, place and route) to generate a different FPGA
    bitstream for each new model or slight change in an existing model and (2) the need
    to reprogram the FPGA (which can take tens to hundreds of milliseconds) when switching
    between pre-compiled bitstreams implementing different models. These drawbacks
    can be prohibitive for use cases in which very frequent (e.g. daily) model updates
    are deployed in production or when real-time or input-dependent switching between
    several models is needed. Another approach for designing DL accelerators on FPGAs
    is to build software-programmable domain-specific overlays. Similarly to CPUs
    and GPUs, an FPGA overlay defines an instruction set architecture (ISA) that decouples
    the hardware and software stacks. The ISA abstracts away all the micro-architecture
    and hardware implementation details from application developers who write their
    algorithms in a high-level programming language and then compile them into sequences
    of instructions that can run on any processor that supports the same ISA. For
    a generic processor architecture and ISA (e.g. RISC-V), a *hard* ASIC implementation
    will always be more efficient than an FPGA overlay due to the overhead of reconfigurability [[59](#bib.bib59),
    [60](#bib.bib60)]. However, a soft processor can enhance efficiency by exploiting
    the FPGA’s flexibility to implement a customized datapath and memory hierarchy,
    as well as a domain-specific ISA.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 定制深度学习硬件的生成利用了 FPGA 的独特可重构特性，通过优化加速器架构以完全符合特定模型或模型类别的需求，从而最小化通用性的额外开销。然而，这也带来了以下成本：(1)
    长时间的 FPGA 编译时间（合成、布局和布线需要几个小时）以为每个新模型或现有模型的轻微变化生成不同的 FPGA 比特流，(2) 在切换不同的预编译比特流时需要重新编程
    FPGA（这可能需要几十到几百毫秒）。这些缺点可能会对需要频繁（例如每日）模型更新的生产使用案例或需要实时或输入依赖的多模型切换的场景构成障碍。另一种在 FPGA
    上设计深度学习加速器的方法是构建软件可编程的领域特定覆盖层。与 CPU 和 GPU 类似，FPGA 覆盖层定义了一个指令集架构（ISA），将硬件和软件栈解耦。ISA
    抽象掉了所有微架构和硬件实现细节，使得应用开发者可以用高级编程语言编写算法，然后将其编译成可以在任何支持相同 ISA 的处理器上运行的指令序列。对于通用处理器架构和
    ISA（例如 RISC-V），由于可重构性的开销，*硬件* ASIC 实现始终比 FPGA 覆盖层更高效[[59](#bib.bib59), [60](#bib.bib60)]。然而，软处理器可以通过利用
    FPGA 的灵活性来实现定制的数据路径和内存层次结构，以及领域特定的 ISA，从而提高效率。
- en: '![Refer to caption](img/6b8364c2b35fcf436848e38e77894169.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6b8364c2b35fcf436848e38e77894169.png)'
- en: 'Figure 4: The overlay design approach enables application experts to use FPGAs
    for accelerating their DL workloads without any hardware design expertise or suffering
    from long runtime of FPGA CAD tools.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：覆盖层设计方法使得应用专家可以使用 FPGA 加速其深度学习工作负载，而无需任何硬件设计专业知识或忍受 FPGA CAD 工具的长时间运行。
- en: 'As illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ II-C FPGA-Based DL Inference
    Acceleration Styles ‣ II FPGA for DL Acceleration ‣ Field-Programmable Gate Array
    Architecture for Deep Learning: Survey & Future Directions"), to build an FPGA
    DL overlay, architects would first design the overlay ISA and processor architecture.
    The microarchitecture of the overlay is then heavily-optimized to generate a single
    high-quality FPGA implementation that is deployed on an FPGA and programmed through
    software to execute different DL models. To program the overlay, users are provided
    with a compiler that translates a high-level description of a DL model (e.g. TensorFlow
    or PyTorch) to a sequence of instructions to be executed on the FPGA overlay.
    In this approach, users do not need to have any hardware design expertise, significantly
    reducing the barrier of entry for DL application developers to use FPGAs. In addition,
    their iteration time to compile a new DL model is much faster, as they are performing
    a fast software compile (seconds) to create a new sequence of overlay instructions
    instead of a long FPGA hardware compile (hours) to create a new bitstream. There
    are many DL overlay examples from both industry and academia optimized for different
    types of models [[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [24](#bib.bib24),
    [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66)], including Microsoft’s datacenter-scale
    DL inference accelerator, Brainwave [[36](#bib.bib36)].'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[4](#S2.F4 "图 4 ‣ II-C 基于 FPGA 的深度学习推理加速风格 ‣ II FPGA 用于深度学习加速 ‣ 深度学习的现场可编程门阵列架构：调查与未来方向")所示，为了构建一个FPGA深度学习叠加层，架构师首先设计叠加层ISA和处理器架构。然后，对叠加层的微架构进行大量优化，以生成一个高质量的FPGA实现，并将其部署在FPGA上，通过软件编程执行不同的深度学习模型。为了编程叠加层，用户会获得一个编译器，该编译器将深度学习模型的高层次描述（如TensorFlow或PyTorch）转换为在FPGA叠加层上执行的指令序列。在这种方法中，用户无需具备任何硬件设计专业知识，大大降低了深度学习应用开发者使用FPGA的门槛。此外，他们编译新深度学习模型的迭代时间更快，因为他们执行的是快速的软件编译（几秒钟），以创建新的叠加层指令序列，而不是长时间的FPGA硬件编译（几小时）以创建新的比特流。许多深度学习叠加层示例来自工业界和学术界，针对不同类型的模型进行了优化[[61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [24](#bib.bib24), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66)]，包括微软的数据中心级深度学习推理加速器Brainwave[[36](#bib.bib36)]。
- en: II-D Examples of DL Acceleration on FPGAs
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D FPGA上的深度学习加速示例
- en: When using FPGAs for DL inference acceleration, regardless of the accelerator
    design style, there are two main concerns. The first is ease of use; FPGAs are
    generally harder to design for, use and debug compared to other compute platforms
    such as GPUs and CPUs. Even with advances in high-level synthesis (HLS) technology,
    using FPGAs still requires extensive hardware design expertise, making them harder
    to use by DL application developers. The second concern is whether FPGAs can deliver
    state-of-the-art DL inference performance despite their inherent overhead of reconfigurability.
    As discussed in the previous subsection, both the custom hardware generation and
    overlay design approaches address the first concern. Using these approaches, DL
    application developers can go from a high-level DL model description to an FPGA
    deployment with little or no hardware design expertise. In this subsection, we
    cover two examples from these two design approaches to showcase that FPGAs can
    deliver best-in-class DL inference performance. We also show that even higher
    performance can be realized by optimizing the underlying FPGA architecture specifically
    for DL.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用FPGA进行深度学习推理加速时，无论加速器设计风格如何，都有两个主要关注点。第一个是易用性；与GPU和CPU等其他计算平台相比，FPGA的设计、使用和调试通常更为复杂。即使有了高级综合（HLS）技术的进步，使用FPGA仍需广泛的硬件设计专业知识，这使得深度学习应用开发者使用它们更加困难。第二个关注点是，尽管FPGA具有固有的可重构开销，但它们是否能够提供最先进的深度学习推理性能。正如前一节所讨论的，自定义硬件生成和叠加设计方法都解决了第一个关注点。通过这些方法，深度学习应用开发者可以从高层次的深度学习模型描述直接到FPGA部署，而几乎无需硬件设计专业知识。在这一节中，我们涵盖了这两种设计方法的两个示例，以展示FPGA可以提供一流的深度学习推理性能。我们还展示了通过专门为深度学习优化底层FPGA架构，可以实现更高的性能。
- en: II-D1 Custom Hardware Generation Example (HPIPE)
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D1 自定义硬件生成示例（HPIPE）
- en: '![Refer to caption](img/af8f1dd8db1823fd4e5d036bb51f9894.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/af8f1dd8db1823fd4e5d036bb51f9894.png)'
- en: 'Figure 5: Temporal mapping of DL models to an array of PEs (top left) vs. building
    per-layer customized compute units as in HPIPE (bottom left) and an overview of
    the HPIPE hardware generation flow (right).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：DL模型与一系列PEs的时间映射（左上）与HPIPE中构建每层定制计算单元（左下）的对比，以及HPIPE硬件生成流程的概览（右）。
- en: 'HPIPE [[25](#bib.bib25)] is a domain-specific compiler that generates layer-pipelined
    dataflow FPGA accelerators for persistent CNNs, where all the weights can be stored
    in the on-chip SRAMs. It builds a unique processing module for each layer in a
    CNN and chains them using latency-insensitive FIFOs. It also exploits weight sparsity
    by skipping ineffectual zero-weight computations, which can significantly reduce
    on-chip memory requirements and improve performance by executing fewer operations.
    When compared to the one-size-fits-all approach in which the same array of PEs
    is used for all layers (see left side of Fig. [5](#S2.F5 "Figure 5 ‣ II-D1 Custom
    Hardware Generation Example (HPIPE) ‣ II-D Examples of DL Acceleration on FPGAs
    ‣ II FPGA for DL Acceleration ‣ Field-Programmable Gate Array Architecture for
    Deep Learning: Survey & Future Directions")), the per-layer customized modules
    in HPIPE result in better utilization of the compute resources and exploit the
    additional dimension of pipeline parallelism in which all the CNN layers are executing
    simultaneously on different parts of an image or on different images. As illustrated
    on the right side of Fig. [5](#S2.F5 "Figure 5 ‣ II-D1 Custom Hardware Generation
    Example (HPIPE) ‣ II-D Examples of DL Acceleration on FPGAs ‣ II FPGA for DL Acceleration
    ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions"), the HPIPE compiler takes as inputs a TensorFlow description of the
    CNN and specifications of the target FPGA. Then, it performs a number optimizations
    on the CNN dataflow graph (e.g. fuse layers for a more efficient implementation).
    After that, it allocates hardware resources to each CNN layer to balance the throughput
    of all the pipelined layers and maximize overall performance. The HPIPE compiler
    also performs several physical design optimizations that consider the spatial
    layout of the layer modules and implements optimized interconnect structures for
    high-fanout and long-span connections, resulting in high operating frequencies.
    Finally, it generates the accelerator RTL files and memory initialization files
    to store the CNN weights in the on-chip memories; the resulting RTL is compiled
    to a bitstream using the conventional FPGA CAD tools.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'HPIPE [[25](#bib.bib25)] 是一种领域特定编译器，它为持久性CNN生成层级流水线数据流FPGA加速器，其中所有权重都可以存储在片上SRAM中。它为CNN中的每一层构建一个独特的处理模块，并使用不敏感延迟的FIFO将它们连接起来。它还通过跳过无效的零权重计算来利用权重稀疏性，这可以显著减少片上内存需求并通过执行更少的操作来提高性能。与用于所有层的相同PE阵列的一刀切方法相比（见图[5](#S2.F5
    "Figure 5 ‣ II-D1 Custom Hardware Generation Example (HPIPE) ‣ II-D Examples of
    DL Acceleration on FPGAs ‣ II FPGA for DL Acceleration ‣ Field-Programmable Gate
    Array Architecture for Deep Learning: Survey & Future Directions")左侧），HPIPE中的每层定制模块能够更好地利用计算资源，并利用额外的流水线并行维度，使所有CNN层在图像的不同部分或不同图像上同时执行。如图[5](#S2.F5
    "Figure 5 ‣ II-D1 Custom Hardware Generation Example (HPIPE) ‣ II-D Examples of
    DL Acceleration on FPGAs ‣ II FPGA for DL Acceleration ‣ Field-Programmable Gate
    Array Architecture for Deep Learning: Survey & Future Directions")右侧所示，HPIPE编译器以CNN的TensorFlow描述和目标FPGA规格作为输入。然后，它对CNN数据流图执行多个优化（例如融合层以实现更高效的实现）。之后，它为每个CNN层分配硬件资源，以平衡所有流水线层的吞吐量并最大化整体性能。HPIPE编译器还执行了几项物理设计优化，考虑到层模块的空间布局，并实现了优化的互连结构，以便高风扇出和长跨度连接，从而实现高操作频率。最后，它生成加速器RTL文件和内存初始化文件，以将CNN权重存储在片上内存中；生成的RTL使用传统的FPGA
    CAD工具编译为比特流。'
- en: Using an Intel Stratix 10 GX2800, the largest monolithic (single die) 14nm Stratix
    10 FPGA, HPIPE outperforms all other FPGA-based CNN accelerators on same-generation
    FPGAs. It can also achieve 4$\times$ higher ResNet-50 throughput at the same latency
    ($<1$ms) compared to batch-1 inference on the Nvidia V100 GPU, which is on a comparable
    process techology (12nm). Increasing the batch size improves GPU utilization but
    worsens latency; HPIPE still achieves 1.4$\times$ higher throughput but at 2.2$\times$
    lower latency compared to the V100 GPU running at a batch size of 8. This highlights
    the utility of FPGAs for low-latency inference; in this case the FPGA’s flexibility
    enables extreme per-model customization, yielding efficiency gains that outweigh
    its inherent reconfigurability overheads. The automatic generation of hardware
    from a high-level model description eliminates the need for FPGA design expertise,
    but it still requires a time-consuming compilation of a new FPGA bitstream for
    each different model to be deployed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Intel Stratix 10 GX2800，这是最大的单片（单芯片）14nm Stratix 10 FPGA，HPIPE在同代FPGA中超越了所有其他基于FPGA的CNN加速器。与在类似工艺技术（12nm）的Nvidia
    V100 GPU上的批处理-1推断相比，它还可以在相同延迟（`<1`ms）下实现4$\times$更高的ResNet-50吞吐量。增加批量大小会提高GPU利用率，但延迟会变差；HPIPE仍然实现了比V100
    GPU（批量大小为8）高1.4$\times$的吞吐量，但延迟低2.2$\times$。这突显了FPGA在低延迟推断中的实用性；在这种情况下，FPGA的灵活性实现了极端的每模型定制，带来了超越其固有重配置开销的效率提升。从高层模型描述自动生成硬件消除了对FPGA设计专业知识的需求，但仍需要为每个不同的模型编译新的FPGA比特流，这一过程耗时。
- en: II-D2 Overlay Example (NPU)
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D2 覆盖示例（NPU）
- en: '![Refer to caption](img/5d41615ce03f50b30b4e15c08d675f0d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5d41615ce03f50b30b4e15c08d675f0d.png)'
- en: 'Figure 6: The NPU overlay architecture. It is a VLIW processor consisting of
    five chained coarse-grained units: a matrix-vector multiplication unit (MVU),
    an external vector register file (eVRF), two multi-function units (MFUs) for vector
    elementwise operations, and a loader unit.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：NPU覆盖架构。它是一个VLIW处理器，由五个串联的粗粒度单元组成：一个矩阵-向量乘法单元（MVU），一个外部向量寄存器文件（eVRF），两个用于向量元素操作的多功能单元（MFUs）和一个加载单元。
- en: 'The neural processing unit (NPU) [[67](#bib.bib67)] is a very-long-instruction-word
    (VLIW) processor architecture targeting low-latency batch-1 inference of DL models
    with no data reuse (i.e. memory-bound) such as different types of RNNs and multi-layer
    perceptrons (MLPs). The NPU overlay design relies on two key principles. First,
    it exploits the massive parallelism of DL models to amortize the energy and area
    cost of software programmability. A single coarse-grained VLIW instruction can
    trigger the execution of thousands of operations, much like an extreme example
    of a complex instruction set computer (CISC) architecture. Second, it customizes
    the processor’s memory subsystem to utilize the tremendous bandwidth between the
    distributed FPGA on-chip memories and processing elements, performing near-memory
    compute. The memory subsystem is explicitly managed (no caches), uses several
    different register files with specific purposes and wide data words rather than
    one general purpose group, and directly chains many operations between functional
    units with no register file access. Fig. [6](#S2.F6 "Figure 6 ‣ II-D2 Overlay
    Example (NPU) ‣ II-D Examples of DL Acceleration on FPGAs ‣ II FPGA for DL Acceleration
    ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions") shows the architecture of the NPU overlay, which consists of five
    coarse-grained chained units such that the output of one unit feeds the next.
    The pipeline starts with a matrix-vector multiplication unit (MVU). The MVU consists
    of $T$ compute tiles, each of which has $D$ dot-product engines (DPEs) of size
    $L$ multiplication lanes. Vector operands are broadcast from a vector register
    file (VRF) to all DPEs in a single tile, while persistent model weights come from
    the matrix register files (MRFs). The MVU is followed by an external VRF (eVRF)
    that enables skipping the MVU if an instruction does not start with a matrix-vector
    multiplication. The rest of the pipeline consists of two multi-function units
    (MFUs) that implement vector elementwise operations (e.g. activation functions,
    addition, multiplication), and a loader unit which can write back results to any
    of the processor architecture states (i.e. VRFs) or communicate with external
    components (e.g. a network interface) through input/output FIFOs. DL application
    developers describe their models using a subset of the Tensorflow Keras API [[68](#bib.bib68)]
    which is then compiled into a sequence of NPU VLIW instructions to be executed
    on the FPGA overlay.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '神经处理单元（NPU）[[67](#bib.bib67)] 是一种非常长指令字（VLIW）处理器架构，旨在低延迟批量1推理DL模型，这些模型没有数据重用（即内存绑定），如不同类型的RNN和多层感知机（MLP）。NPU叠加设计依赖于两个关键原则。首先，它利用DL模型的巨大并行性来摊销软件可编程性的能量和面积成本。一个粗粒度的VLIW指令可以触发成千上万次操作，类似于复杂指令集计算机（CISC）架构的极端例子。第二，它定制了处理器的内存子系统，以利用分布式FPGA片上内存和处理元素之间的巨大带宽，执行近内存计算。内存子系统是显式管理的（没有缓存），使用几个具有特定用途的不同寄存器文件和宽数据字，而不是一个通用组，并且直接在功能单元之间链式连接许多操作，没有寄存器文件访问。图[6](#S2.F6
    "Figure 6 ‣ II-D2 Overlay Example (NPU) ‣ II-D Examples of DL Acceleration on
    FPGAs ‣ II FPGA for DL Acceleration ‣ Field-Programmable Gate Array Architecture
    for Deep Learning: Survey & Future Directions")显示了NPU叠加的架构，它由五个粗粒度链式单元组成，其中一个单元的输出供给下一个单元。管道以矩阵-向量乘法单元（MVU）开始。MVU由$T$个计算模块组成，每个模块有$D$个点积引擎（DPE），每个DPE有$L$个乘法通道。向量操作数从向量寄存器文件（VRF）广播到单个模块中的所有DPE，而持久的模型权重来自矩阵寄存器文件（MRF）。MVU之后是一个外部VRF（eVRF），如果指令不是以矩阵-向量乘法开始，则允许跳过MVU。其余的管道由两个多功能单元（MFU）组成，它们实现向量逐元素操作（例如激活函数、加法、乘法），以及一个加载单元，该单元可以将结果写回处理器架构状态（即VRF）中的任何位置或通过输入/输出FIFO与外部组件（例如网络接口）进行通信。DL应用开发人员使用Tensorflow
    Keras API的一个子集[[68](#bib.bib68)]来描述他们的模型，然后将其编译成一系列NPU VLIW指令，以便在FPGA叠加上执行。'
- en: The NPU implemented on an Intel Stratix 10 GX2800 FPGA achieves 2.7$\times$
    lower batch-1 latency than the equivalent Nvidia V100 GPU for various RNN workloads
    from the DeepBench suite [[69](#bib.bib69)] when using the same fp32 numerical
    precision as the GPU. When using the more FPGA-friendly 8-bit integer precision,
    this performance gap grows to 8.6$\times$. This shows that a domain-specific FPGA
    overlay with a custom architecture and ISA can deliver significantly higher performance
    compared to generic processor pipelines such as those of GPUs and CPUs, while
    providing similar software-level programmability.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在Intel Stratix 10 GX2800 FPGA上实现的NPU，在使用与GPU相同的fp32数值精度时，相较于等效的Nvidia V100 GPU，在DeepBench套件中的各种RNN工作负载下，批处理1的延迟降低了2.7$\times$[[69](#bib.bib69)]。使用更适合FPGA的8位整数精度时，这一性能差距扩大到8.6$\times$。这表明，具有定制架构和ISA的领域特定FPGA叠加层可以比通用处理器管线（如GPU和CPU）提供显著更高的性能，同时提供类似的软件级可编程性。
- en: II-D3 Effect of FPGA Architecture Enhancements for DL
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D3 FPGA架构增强对深度学习的影响
- en: Both HPIPE and the NPU overlay were designed to best match the underlying FPGA
    architecture. For example, they both organize their fundamental MAC compute units
    to efficiently utilize the embedded digital signal processing blocks (DSPs) in
    the target FPGA. HPIPE used the dedicated (non-programmable) interconnects between
    DSP blocks to build efficient pipelined dot products with minimal utilization
    of the FPGA’s programmable logic and routing. On the other hand, the NPU used
    a small amount of soft logic for post-multiplier correction to enable the dense
    packing of four int8 multipliers to the two int18 multipliers available in a single
    DSP block on an Intel Stratix 10 FPGA [[70](#bib.bib70)]. These optimizations
    are used to enhance DL performance assuming that the FPGA architecture itself
    is a constant. However, FPGA architecture has been continuously evolving to better
    suit key FPGA use cases and market segments throughout the past three decades.
    Thus, with DL becoming such a prominent workload, many DL-targeted FPGA architecture
    enhancements have been proposed in the past few years.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: HPIPE和NPU叠加层都被设计为最适配底层FPGA架构。例如，它们都组织其基本的MAC计算单元，以高效利用目标FPGA中的嵌入式数字信号处理块（DSP）。HPIPE使用了DSP块之间的专用（非可编程）互连来构建高效的流水线点积，最小化对FPGA的可编程逻辑和路由的使用。另一方面，NPU使用少量的软逻辑进行乘法器后校正，以便将四个int8乘法器密集地打包到Intel
    Stratix 10 FPGA的一个DSP块中两个int18乘法器的空间[[70](#bib.bib70)]。这些优化用于提升深度学习的性能，前提是假设FPGA架构本身是固定的。然而，FPGA架构在过去三十年里一直在不断发展，以更好地适应关键的FPGA应用场景和市场细分。因此，随着深度学习成为如此突出的工作负载，近年来提出了许多针对深度学习的FPGA架构增强方案。
- en: One example of such FPGA architecture enhancements (which we cover in more detail
    later in this article) is the replacement of conventional DSP blocks with DL-optimized
    tensor blocks in the Intel Stratix 10 NX FPGA [[71](#bib.bib71)]. These tensor
    blocks replace the legacy DSP block modes of operation and precisions with DL-targeted
    ones that can implement significantly more int8 and int4 multiplications per block.
    By restricting the data input and output patterns that can achieve peak throughput
    (and thereby avoiding adding expensive additional programmable interconnect),
    these tensor blocks achieve area similar to that of a conventional DSP block.
    Both HPIPE and the NPU were upgraded to use these new DL-optimized FPGAs with
    tensor blocks resulting in a significant performance boost. For HPIPE, the tensor
    blocks improved inference throughput by 4.8$\times$ from 6,000 to 29,400 batch-1
    inferences per second for the MobileNet-V2 CNN compared to the conventional FPGA
    with DSP blocks [[72](#bib.bib72)]. Compared to the Nvidia V100 GPU (which is
    more than 1.5$\times$ bigger in die size than the Stratix 10 NX FPGA), the tensor-block-enhanced
    HPIPE achieves 17$\times$ higher throughput and 3$\times$ lower latency at batch-1
    or 1.3$\times$ higher throughput and 29$\times$ lower latency at batch-128. On
    the other hand, the NPU performance is improved by 3.5$\times$ when using the
    tensor blocks compared to conventional DSPs, resulting in 11$\times$ higher performance
    than the V100 GPU [[24](#bib.bib24)].
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个FPGA架构增强的例子（我们将在本文后面详细讨论）是将传统的DSP模块替换为DL优化的张量块，在Intel Stratix 10 NX FPGA中[[71](#bib.bib71)]。这些张量块用DL目标模式替代了传统DSP模块的操作模式和精度，可以在每个模块中实现更多的int8和int4乘法。通过限制能够实现峰值吞吐量的数据输入和输出模式（从而避免添加昂贵的额外可编程互连），这些张量块在面积上与传统的DSP模块相当。HPIPE和NPU都升级为使用这些新的DL优化FPGA，并采用了张量块，显著提高了性能。对于HPIPE，张量块将MobileNet-V2
    CNN的推理吞吐量从传统的DSP模块FPGA的6,000提升到29,400 batch-1推理每秒，提升了4.8$\times$[[72](#bib.bib72)]。相比之下，Nvidia
    V100 GPU（其晶圆尺寸比Stratix 10 NX FPGA大1.5$\times$以上），使用张量块增强的HPIPE在batch-1下实现了17$\times$更高的吞吐量和3$\times$更低的延迟，在batch-128下则实现了1.3$\times$更高的吞吐量和29$\times$更低的延迟。另一方面，与传统DSP相比，使用张量块的NPU性能提高了3.5$\times$，使其性能比V100
    GPU高出11$\times$[[24](#bib.bib24)]。
- en: 'These two examples highlight the significant impact DL-specific architecture
    enhancements can have on FPGA inference performance. In this article, we cover
    many such architecture innovations from both industry and academia that all share
    the same goal: to make FPGAs better at DL.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个例子突显了专门针对深度学习（DL）的架构增强对FPGA推理性能的重大影响。本文将介绍许多来自行业和学术界的这类架构创新，它们都具有相同的目标：使FPGA在深度学习中表现更佳。
- en: III FPGA Architecture & Opportunities for DL
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III FPGA架构与深度学习机会
- en: In this section, we briefly describe the key building blocks of an FPGA architecture
    and highlight the opportunities for optimizing these different components for
    DL compute. For a more comprehensive survey on the design principles and evolution
    of FPGA architecture, we refer the reader to [[73](#bib.bib73)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要描述了FPGA架构的关键构建块，并突出了优化这些不同组件以进行深度学习计算的机会。有关FPGA架构设计原则和演变的更全面调查，请参阅[[73](#bib.bib73)]。
- en: III-A Programmable Logic & Routing
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 可编程逻辑与路由
- en: 'The programmable logic blocks (LBs) are the most abundant resource in an FPGA.
    An LB is a group of $N$ logic elements (LEs) in addition to local routing, generally
    built with programmable multiplexers, that allow the LB inputs to connect to different
    LEs or feed the LE outputs back to their inputs, as illustrated in Fig. [7](#S3.F7
    "Figure 7 ‣ III-A Programmable Logic & Routing ‣ III FPGA Architecture & Opportunities
    for DL ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions"). In their simplest form, each LE combines an SRAM-based
    $K$-input look-up table (LUT) with a bypassable flip-flop (FF) and can implement
    any optionally-registered $K$-input Boolean logic function. The LEs in many modern
    FPGA architectures can also be fractured to implement two logic functions that
    use at most $K-1$ inputs each and together do not use more distinct inputs than
    the local routing provides to a single LE. They also include dedicated circuitry
    (the pink box in Fig. [7](#S3.F7 "Figure 7 ‣ III-A Programmable Logic & Routing
    ‣ III FPGA Architecture & Opportunities for DL ‣ Field-Programmable Gate Array
    Architecture for Deep Learning: Survey & Future Directions")) to efficiently implement
    adders, which are abundant in many FPGA designs [[74](#bib.bib74)] and very common
    in DL accelerators. Most commercial FPGAs from both AMD and Intel implement LBs
    of eight to ten 6-input LEs (i.e. $N=8-10,K=6$). A key distinction is that each
    LE in Intel FPGAs includes dedicated circuitry to implement two bits of addition,
    while each AMD LE can only implement a single bit of addition. The LBs (and other
    FPGA fabric components and IOs) are surrounded by programmable routing that can
    flexibly connect between various blocks. This programmable routing consists of
    SRAM-controlled multiplexers to connect block outputs and routing wires to different
    routing wires (green MUXes in Fig. [7](#S3.F7 "Figure 7 ‣ III-A Programmable Logic
    & Routing ‣ III FPGA Architecture & Opportunities for DL ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions")) and routing
    wires to block inputs (yellow MUXes in Fig. [7](#S3.F7 "Figure 7 ‣ III-A Programmable
    Logic & Routing ‣ III FPGA Architecture & Opportunities for DL ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions")). These
    multiplexers consume a large fraction of an FPGA’s die area; they constitute more
    than 50% of the area of a logic *tile* (i.e. LB and its programmable routing) [[75](#bib.bib75)].
    As adding more inputs and outputs to an FPGA LB or hard block implies more routing
    multiplexers, architecture changes that increase the number of inputs/output to/from
    a block require careful consideration of the functionality gain vs. the area cost.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '可编程逻辑块（LBs）是 FPGA 中最丰富的资源。一个 LB 是一组 $N$ 个逻辑元素（LEs），以及局部布线，通常由可编程多路复用器构建，这些多路复用器允许
    LB 输入连接到不同的 LEs 或将 LE 输出反馈到它们的输入，如图 [7](#S3.F7 "Figure 7 ‣ III-A Programmable
    Logic & Routing ‣ III FPGA Architecture & Opportunities for DL ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions") 所示。在最简单的形式中，每个
    LE 结合了基于 SRAM 的 $K$ 输入查找表（LUT）和一个可旁路的触发器（FF），可以实现任何可选注册的 $K$ 输入布尔逻辑函数。许多现代 FPGA
    架构中的 LEs 也可以被分裂以实现两个逻辑函数，每个函数使用至多 $K-1$ 个输入，且总共不使用超过单个 LE 的局部布线所提供的不同输入。它们还包括专用电路（图
    [7](#S3.F7 "Figure 7 ‣ III-A Programmable Logic & Routing ‣ III FPGA Architecture
    & Opportunities for DL ‣ Field-Programmable Gate Array Architecture for Deep Learning:
    Survey & Future Directions") 中的粉色框），以高效实现加法器，这在许多 FPGA 设计中很丰富[[74](#bib.bib74)]，并且在
    DL 加速器中非常常见。来自 AMD 和 Intel 的大多数商用 FPGA 实现了八到十个 6 输入 LE 的 LB（即 $N=8-10,K=6$）。一个关键的区别是，Intel
    FPGA 中的每个 LE 包括专用电路来实现两个加法位，而 AMD LE 只能实现一个加法位。LB（以及其他 FPGA 结构组件和 IO）被可编程布线包围，这些布线可以在各种块之间灵活连接。这些可编程布线由
    SRAM 控制的多路复用器组成，用于将块输出和布线线缆连接到不同的布线线缆（图 [7](#S3.F7 "Figure 7 ‣ III-A Programmable
    Logic & Routing ‣ III FPGA Architecture & Opportunities for DL ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions") 中的绿色 MUX）和布线线缆到块输入（图
    [7](#S3.F7 "Figure 7 ‣ III-A Programmable Logic & Routing ‣ III FPGA Architecture
    & Opportunities for DL ‣ Field-Programmable Gate Array Architecture for Deep Learning:
    Survey & Future Directions") 中的黄色 MUX）。这些多路复用器占用了 FPGA 芯片面积的大部分；它们构成了逻辑 *瓷砖*（即
    LB 及其可编程布线）面积的 50% 以上[[75](#bib.bib75)]。由于向 FPGA LB 或硬块添加更多输入和输出意味着更多的布线多路复用器，因此增加块的输入/输出数量的架构变化需要仔细考虑功能增益与面积成本的权衡。'
- en: '![Refer to caption](img/18bcbdf214535166e2ae3fe5fa3948da.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/18bcbdf214535166e2ae3fe5fa3948da.png)'
- en: 'Figure 7: Logic block (LB) and routing architecture. An LB consists of $N$
    Logic Elements (LEs) and local interconnect. SRAM-controlled programmable routing
    MUXes connect general routing wires to each other and to LB inputs/outputs.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：逻辑块（LB）和路由架构。一个 LB 包含 $N$ 个逻辑元素（LEs）和本地互连。SRAM 控制的可编程路由 MUX 将通用路由线连接到彼此以及
    LB 的输入/输出。
- en: 'The programmable logic and routing are the key to the FPGA’s bit-level programmability
    and allow it to implement any functionality by setting LUT and routing configuration
    SRAMs (shown in Fig. [7](#S3.F7 "Figure 7 ‣ III-A Programmable Logic & Routing
    ‣ III FPGA Architecture & Opportunities for DL ‣ Field-Programmable Gate Array
    Architecture for Deep Learning: Survey & Future Directions")) accordingly. For
    DL, custom low-precision MAC units are typically implemented using the LUTs, FF,
    and dedicated adder circuitry in the FPGA’s LEs. For example, the Microsoft Brainwave
    FPGA-based DL accelerator implemented custom 8-bit floating-point (msfp8) compute
    units in the FPGA’s programmable logic. This custom floating point format, which
    emphasizes dynamic range over precision, has 2.9$\times$ higher MAC density than
    traditional int8 compute units [[76](#bib.bib76)] and yields inference accuracy
    comparable to 32-bit floating point (fp32). When somewhat reduced accuracy is
    tolerable, low precision binary MACs which are realized as XNOR and population
    count (popcount) operations [[77](#bib.bib77)] can also be used in DL models.
    This results in very small and efficient compute units when implemented on an
    FPGA. In other devices, the compute units are pre-fabricated to implement conventional,
    higher precision MACs and word-wide logic operations and hence the full efficiency
    gains of such extremely low precisions are not realized. While narrow bitwidth
    operations are already a strength of FPGAs, current FPGA LBs were architected
    before the rise of DL and its high demand for low-precision MACs, raising the
    question of whether DL-targeted LB changes could further improve their MAC efficiency.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可编程逻辑和路由是 FPGA 位级可编程性的关键，允许通过设置 LUT 和路由配置 SRAM（如图 [7](#S3.F7 "图 7 ‣ III-A 可编程逻辑与路由
    ‣ III FPGA 架构与深度学习机会 ‣ 面向深度学习的现场可编程门阵列架构：调查与未来方向")）来实现任何功能。对于深度学习，定制的低精度 MAC 单元通常使用
    FPGA 的 LEs 中的 LUT、FF 和专用加法电路来实现。例如，微软 Brainwave 基于 FPGA 的深度学习加速器在 FPGA 的可编程逻辑中实现了定制的
    8 位浮点（msfp8）计算单元。这种定制浮点格式强调动态范围而非精度，具有比传统 int8 计算单元高 2.9$\times$ 的 MAC 密度[[76](#bib.bib76)]，并且推理精度与
    32 位浮点（fp32）相当。当能够容忍一定的精度降低时，低精度二进制 MAC（实现为 XNOR 和人口计数（popcount）操作[[77](#bib.bib77)]）也可以用于深度学习模型。这在
    FPGA 上实现时会产生非常小且高效的计算单元。在其他设备中，计算单元是预先制造的，以实现传统的、更高精度的 MAC 和字宽逻辑操作，因此无法实现如此极低精度的全部效率提升。虽然窄位宽操作已经是
    FPGA 的强项，但当前的 FPGA LB 在深度学习及其对低精度 MAC 的高需求出现之前就已经架构完成，这引发了一个问题，即面向深度学习的 LB 改变是否可以进一步提高它们的
    MAC 效率。
- en: '*Opportunity 1: Enhancing logic block architecture to implement more efficient
    narrow-bitwidth multiplication and addition operations can result in significant
    gains for low-precision DL acceleration.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*机会 1：增强逻辑块架构以实现更高效的窄位宽乘法和加法操作，可以为低精度深度学习加速带来显著收益。*'
- en: III-B Digital Signal Processing Blocks
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 数字信号处理模块
- en: Since DL workloads are dominated by MAC operations, digital signal processing
    (DSP) blocks are crucial when implementing FPGA-based DL accelerators. DSP blocks
    are *hard* (ASIC-style) blocks embedded in the FPGA fabric that implement multipliers
    and adders. However, they are designed with some programmability to increase their
    usability in various FPGA designs while still maintaining their ASIC-like efficiency.
    For example, DSP blocks in the Intel Arria 10 and Stratix 10 FPGA families have
    configurable circuitry to perform multiplications of different precisions (e.g.
    one int27 or two int18 multiplications) as well as optional pre-multiplication
    adders, a post-multiplication adder/accumulator, bypassable pipeline registers,
    and dedicated routing wires between DSP blocks in the same FPGA column. These
    DSP blocks were originally designed for wireless communication and filtering applications,
    which remain a major market for FPGAs. Therefore, they natively support numerical
    precisions that are widely used in this domain (e.g. 27$\times$27 and 18$\times$18
    multiplications in Intel FPGAs, and 27$\times$18 multiplications in AMD FPGAs).
    Although they can be used to implement MAC units in DL accelerators, these precisions
    are typically higher than what is commonly needed for DL inference resulting in
    underutilization of the DSP block features (or equivalently, wastage of silicon
    area as the DSP blocks are over-engineered for DL requirements).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DL工作负载以MAC操作为主，因此数字信号处理（DSP）模块在实现基于FPGA的DL加速器时至关重要。DSP模块是嵌入FPGA结构中的*硬件*（ASIC风格）模块，实施乘法器和加法器。然而，它们设计了一些可编程性，以增加其在各种FPGA设计中的可用性，同时保持其类似ASIC的效率。例如，Intel
    Arria 10和Stratix 10 FPGA系列中的DSP模块具有可配置电路，可以执行不同精度的乘法（例如，一个int27或两个int18乘法），以及可选的预乘法加法器、后乘法加法器/累加器、可绕过的流水线寄存器以及同一FPGA列内DSP模块之间的专用布线。这些DSP模块最初是为无线通信和过滤应用设计的，这些领域仍然是FPGA的主要市场。因此，它们原生支持在该领域广泛使用的数值精度（例如，Intel
    FPGA中的27$\times$27和18$\times$18乘法，以及AMD FPGA中的27$\times$18乘法）。尽管它们可以用于实现DL加速器中的MAC单元，但这些精度通常高于DL推断所需的精度，导致DSP模块功能的使用不足（或者等同于因DSP模块过度工程化而导致的硅面积浪费）。
- en: '*Opportunity 2: Adding low-cost reconfiguration circuitry to enable fracturing
    the multipliers inside DSP blocks into more lower-precision multipliers (while
    maintaining backward compatibility) can enhance DL performance.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*机会 2：添加低成本的重配置电路，以便将DSP模块内的乘法器分解为更多低精度乘法器（同时保持向后兼容性），可以提高DL性能。*'
- en: Besides numerical precisions, these DSP blocks include other features beneficial
    to traditional FPGA applications like wireless communication. For example, the
    Intel Stratix 10 DSP block has a small constant coefficient bank and input cascading
    registers/interconnect for implementing efficient finite impulse response (FIR)
    filters. These DSP features consume silicon area, but are less useful for DL computations;
    replacing them with more DL-focused features could improve DL efficiency at the
    cost of losing backward compatibility with traditional DSP blocks.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数值精度之外，这些DSP模块还包括对传统FPGA应用（如无线通信）有益的其他功能。例如，Intel Stratix 10 DSP模块具有一个小型常数系数库和输入级联寄存器/互连，用于实现高效的有限脉冲响应（FIR）滤波器。这些DSP功能虽然占用硅面积，但对DL计算的帮助较小；用更多面向DL的功能替代它们，可能会提高DL效率，但会失去与传统DSP模块的向后兼容性。
- en: '*Opportunity 3: Replacing the DSP blocks originally designed for the wireless
    communications domain with new DL-targeted blocks can increase the compute density
    and efficiency of FPGAs for DL.*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*机会 3：用新的DL目标模块替代原本为无线通信领域设计的DSP模块，可以提高FPGA在DL中的计算密度和效率。*'
- en: III-C On-Chip Block Memories
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 芯片内块存储器
- en: 'FPGAs also include a large number of on-chip SRAM memory blocks typically referred
    to as block RAMs (BRAMs). These BRAMs (more than 10,000 blocks in modern FPGAs)
    are spatially distributed in columns throughout the FPGA fabric, as shown in Fig. [10](#S4.F10
    "Figure 10 ‣ IV-C Taxonomy of FPGA Architecture Enhancements for DL ‣ IV FPGA
    Architecture Exploration ‣ Field-Programmable Gate Array Architecture for Deep
    Learning: Survey & Future Directions"). The latest generations of Intel FPGAs
    contain a single type of BRAM with 20Kb capacity [[78](#bib.bib78)], while AMD
    FPGAs have 36Kb BRAMs as well as larger but less common 288Kb RAMs (typically
    referred to as Ultra RAMs or URAMs) [[79](#bib.bib79)]. The core of these BRAMs
    is a fixed size SRAM array with the conventional peripheral circuitry for read/write
    operations such as row decoders, sense amplifiers, and write drivers. However,
    similarly to DSP blocks, these BRAMs include low-cost reconfiguration circuitry
    in their peripherals to enable implementing buffers with different width/depth
    (narrow and deep vs. shallow and wide buffers) and number of ports depending on
    the application needs  [[80](#bib.bib80)]. For example, by setting a few configuration
    SRAM cells, a 20Kb BRAM can be used as a read-only memory (ROM), a single-port
    RAM, or a dual-port RAM with a 1b$\times$16K, 2b$\times$8K, 4b$\times$4K, 8b$\times$2K,
    16b$\times$1K, 32b$\times$512, or 40b$\times$512 organization. The FPGA BRAMs
    can all be accessed in parallel providing massive on-chip memory bandwidth (on
    the order of petabits per second) with only one or two cycles of access latency.
    Additionally, they can be independently controlled and directly connected to the
    compute units by exploiting the flexibility of the FPGA’s programmable routing.
    These features are useful for low-latency massively parallel DL compute on FPGAs.
    However, with the increasingly pressing need to bring compute even closer to data
    for higher efficiency, the thousands of on-chip memory blocks in an FPGA can potentially
    do more than just store data to be used by the compute units implemented in LBs
    and DSPs.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: FPGA 还包括大量的片上 SRAM 内存块，通常称为块 RAM（BRAM）。这些 BRAM（现代 FPGA 中超过 10,000 个块）在 FPGA
    结构中按列空间分布，如图 [10](#S4.F10 "图 10 ‣ IV-C FPGA 架构增强的分类 ‣ IV FPGA 架构探索 ‣ 用于深度学习的现场可编程门阵列架构：调查与未来方向")
    所示。最新一代的 Intel FPGA 包含一种 20Kb 容量的 BRAM [[78](#bib.bib78)]，而 AMD FPGA 则具有 36Kb
    的 BRAM 以及更大但较少见的 288Kb RAM（通常称为超 RAM 或 URAM）[[79](#bib.bib79)]。这些 BRAM 的核心是一个固定大小的
    SRAM 数组，配有传统的外围电路以进行读/写操作，如行解码器、感应放大器和写入驱动器。然而，与 DSP 块类似，这些 BRAM 在其外围电路中包含低成本的重配置电路，以便根据应用需求实现具有不同宽度/深度（窄而深或浅而宽缓冲区）和端口数量的缓冲区
    [[80](#bib.bib80)]。例如，通过设置几个配置 SRAM 单元，20Kb 的 BRAM 可以用作只读存储器（ROM）、单端口 RAM 或双端口
    RAM，组织方式为 1b$\times$16K、2b$\times$8K、4b$\times$4K、8b$\times$2K、16b$\times$1K、32b$\times$512
    或 40b$\times$512。FPGA 的 BRAM 都可以并行访问，提供巨大的片上内存带宽（每秒宠物位级别），且仅需一到两个访问周期的延迟。此外，它们可以独立控制，并通过利用
    FPGA 可编程路由的灵活性直接连接到计算单元。这些特性对 FPGA 上的低延迟大规模并行深度学习计算非常有用。然而，随着将计算更接近数据以提高效率的需求越来越迫切，FPGA
    中的成千上万片上内存块可能不仅仅用于存储数据以供 LB 和 DSP 中的计算单元使用。
- en: '*Opportunity 4: With advances in processing-in-memory technology, enhancing
    BRAMs with in-memory compute capabilities can provide thousands of parallel compute
    units on the FPGA at a relatively low cost.*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*机会 4：随着内存计算技术的进步，通过在内存中增强 BRAM 可以在 FPGA 上以相对较低的成本提供数千个并行计算单元。*'
- en: III-D Interposers
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 中介层
- en: '![Refer to caption](img/2657feaf366920d8c92df8605f5b77d8.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2657feaf366920d8c92df8605f5b77d8.png)'
- en: 'Figure 8: Passive interposer technology for creating larger FPGA devices by
    integrating multiple smaller and higher yield chips in the same package.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：被动中介层技术通过将多个较小且更高产量的芯片集成到同一封装中来创建更大的 FPGA 设备。
- en: 'Since FPGAs are typically early adopters of a new process technology, creating
    large single-silicon-die FPGAs results in poor yield (due to manufacturing defects)
    especially early in the process life cycle. To face this challenge, many recent
    FPGAs use passive interposer technology to integrate multiple (smaller) silicon
    dice in the same package. This not only improves yield but also enables agile
    hardware development by combining FPGA fabrics with pre-fabricated *chiplets*
    that implement different functionalities and (possibly) use different process
    technologies into a complete system-in-package. As illustrated in Fig. [8](#S3.F8
    "Figure 8 ‣ III-D Interposers ‣ III FPGA Architecture & Opportunities for DL ‣
    Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions"), an interposer is a silicon die with conventional metal layers but
    has no active transistors implemented on it (thus the name *passive interposer*).
    The top metal layer of the interposer die can connect to the top metal layer of
    multiple dice flipped on top of it through densely packed (typically tens of $\mu
    m$ pitch) solder balls known as *microbumps* [[81](#bib.bib81)], providing a high
    density of routing tracks between different chips in the same package. AMD FPGAs
    have been using this technology starting from their 28nm 7-series family to integrate
    multiple FPGA dice which are presented to users as a single large FPGA with multiple
    super logic regions (SLRs) [[82](#bib.bib82)]. Intel FPGAs also use a similar
    technology, known as the embedded multi-die interconnect bridge (EMIB) [[83](#bib.bib83)],
    to integrate an FPGA die with multiple transceiver or high-bandwidth memory (HBM)
    chiplets starting from their 14nm Stratix 10 family [[84](#bib.bib84)]. These
    technologies enable the creation of many device variations for different markets
    depending on the application-specific ASIC chips integrated with the FPGA in the
    same package. Even with the rapid change in state-of-the-art DL models, massively
    parallel MAC operations are a core component of almost all models and hence can
    be potentially offloaded to a highly-efficient ASIC side chiplet. In this case,
    the FPGA in the same package can provide the needed flexibility for any DL model
    changes and diverse IOs to the rest of the system.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 FPGA 通常是新工艺技术的早期采用者，因此在工艺生命周期早期制造大单芯片 FPGA 会导致较差的良品率（由于制造缺陷）。为应对这一挑战，许多最近的
    FPGA 使用被动中介技术将多个（较小的）硅芯片集成到同一封装中。这不仅提高了良品率，还通过将 FPGA 结构与预制的*芯片块*结合，能够实现灵活的硬件开发，这些芯片块实现了不同的功能，并且（可能）使用不同的工艺技术，形成一个完整的封装系统。如图
    [8](#S3.F8 "图 8 ‣ III-D 中介 ‣ III FPGA 架构与深度学习机会 ‣ 用于深度学习的现场可编程门阵列架构：调研与未来方向")
    所示，中介是一块具有常规金属层但没有活跃晶体管实现的硅芯片（因此得名*被动中介*）。中介芯片的顶部金属层可以通过密集排列的（通常为数十微米间距）焊球，称为*微凸点*
    [[81](#bib.bib81)]，与其上方翻转的多个芯片的顶部金属层连接，从而在同一封装中的不同芯片之间提供高密度的布线轨道。AMD 的 FPGA 从其
    28nm 7 系列开始使用这一技术，将多个 FPGA 芯片集成在一起，并呈现给用户作为一个具有多个超逻辑区域（SLRs）的单大 FPGA [[82](#bib.bib82)]。Intel
    FPGA 也使用类似的技术，称为嵌入式多芯片互连桥（EMIB）[[83](#bib.bib83)]，从其 14nm Stratix 10 系列开始，将一个
    FPGA 芯片与多个收发器或高带宽内存（HBM）芯片块集成 [[84](#bib.bib84)]。这些技术使得根据应用特定的 ASIC 芯片与 FPGA 在同一封装中的集成，能够创建适用于不同市场的多种设备变体。即使在最先进的深度学习模型迅速变化的情况下，大规模并行
    MAC 操作几乎是所有模型的核心组成部分，因此可以潜在地卸载到高效的 ASIC 辅助芯片上。在这种情况下，同一封装中的 FPGA 可以提供对任何深度学习模型更改所需的灵活性，以及对系统其余部分的多样化
    IO。
- en: '*Opportunity 5: Integrating FPGAs and DL-specialized ASICs using advanced package
    integration technologies can combine the best of both worlds: FPGA flexibility
    for bespoke parts of the system and ASIC efficiency for common functionality.*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*机会 5：使用先进的封装集成技术整合 FPGA 和专门针对深度学习的 ASIC，可以结合两者的最佳特性：FPGA 的灵活性用于系统的定制部分，而 ASIC
    的高效性用于常见功能。*'
- en: III-E Networks-on-Chip and Embedded Accelerators
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 芯片网络与嵌入式加速器
- en: More recently, new *beyond-FPGA* reconfigurable acceleration devices (RADs)
    have emerged. An example is the AMD Versal architecture, which combines an FPGA
    fabric with general-purpose processor cores and an array of software-programmable
    vector processors in a single device [[85](#bib.bib85)]. All these components
    are connected via a packet-switched network-on-chip (NoC) for efficient system-level
    communication [[86](#bib.bib86)]. The NoC enables faster and easier integration
    of systems combining various *soft* design IPs implemented on the programmable
    FPGA fabric along with *hard* coarse-grained application-specific embedded accelerators.
    The AMD Versal architecture is a single design point from a broad space of potential
    novel reconfigurable computing devices that could benefit DL acceleration.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了新的*超越FPGA*的可重配置加速设备（RAD）。一个例子是AMD Versal架构，它将FPGA结构与通用处理器核心以及一组软件可编程的向量处理器集成在一个设备中[[85](#bib.bib85)]。所有这些组件通过分组交换的片上网络（NoC）进行连接，以实现高效的系统级通信[[86](#bib.bib86)]。NoC使得将各种*软*设计IP与*硬*粗粒度应用特定嵌入加速器一起实现的系统更快、更容易地集成。AMD
    Versal架构是从广泛的潜在新型可重配置计算设备中选出的一个设计点，这些设备可能有利于深度学习加速。
- en: '*Opportunity 6: Exploring the design space of new DL-targeted RADs that combine
    the unique features of FPGAs with more efficient coarse-grained DL accelerator
    cores.*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*机会6：探索新的针对深度学习的可重配置加速器（RAD）的设计空间，这些加速器结合了FPGA的独特特性和更高效的粗粒度深度学习加速核心。*'
- en: For the remainder of this article, we review recent proposals from both academia
    and industry to enhance FPGA architecture for DL, capitalizing on the opportunities
    that we highlighted in this section. Before that, we first explain the commonly
    used methodology for exploring and evaluating new FPGA architectures quantitatively.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的剩余部分，我们回顾了来自学术界和工业界的最新提议，以增强FPGA架构以支持深度学习，利用我们在本节中强调的机会。在此之前，我们首先解释常用的定量探索和评估新FPGA架构的方法。
- en: IV FPGA Architecture Exploration
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV FPGA架构探索
- en: IV-A Tools and Benchmarks
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 工具和基准测试
- en: 'Fig. [9](#S4.F9 "Figure 9 ‣ IV-A Tools and Benchmarks ‣ IV FPGA Architecture
    Exploration ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions") shows the flow typically used to evaluate FPGA architecture
    modifications. At the core of this flow is a *retargettable* FPGA CAD tool that
    can flexibly synthesize, place and route a set of benchmark designs on a wide
    range of input FPGA architectures. Architects can then evaluate different candidate
    architectures by comparing the the timing, area, and power metrics reported by
    the CAD tools. Verilog-to-routing (VTR) is an open-source flow that is widely
    used for FPGA architecture and CAD research [[87](#bib.bib87)]. It combines several
    tools such as ODIN [[88](#bib.bib88)] or Yosys [[89](#bib.bib89)] for Verilog
    synthesis, ABC [[90](#bib.bib90)] for logic optimization and technology mapping,
    VPR for packing, placement and routing, and Tatum [[91](#bib.bib91)] for static
    timing analysis. VTR takes as an input an XML-based FPGA architecture description
    file which specifies the high-level organization of an FPGA (e.g. number and type
    of blocks, distribution of wire segment lengths, size of logic clusters and logic
    elements), its micro-architectural details (e.g. DSP and BRAM modes of operation,
    hard arithmetic in logic blocks, switch block patterns), as well as transistor-level
    circuit implementation parameters (e.g. switch/wire delays and areas). Tools such
    as COFFE [[92](#bib.bib92)] automate the transistor-level design and modeling
    of FPGA circuitry and generate the delay and area of different components to be
    included in the VTR FPGA architecture description file.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '图[9](#S4.F9 "Figure 9 ‣ IV-A Tools and Benchmarks ‣ IV FPGA Architecture Exploration
    ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions")展示了用于评估FPGA架构修改的典型流程。该流程的核心是一个*可重定向*的FPGA CAD工具，能够灵活地合成、布置和路由一组基准设计到各种输入FPGA架构上。架构师可以通过比较CAD工具报告的时序、面积和功耗指标来评估不同的候选架构。Verilog-to-routing
    (VTR) 是一个广泛用于FPGA架构和CAD研究的开源流程[[87](#bib.bib87)]。它结合了多个工具，如ODIN [[88](#bib.bib88)]
    或 Yosys [[89](#bib.bib89)] 用于Verilog合成，ABC [[90](#bib.bib90)] 用于逻辑优化和技术映射，VPR用于打包、布置和路由，以及Tatum
    [[91](#bib.bib91)] 用于静态时序分析。VTR的输入是一个基于XML的FPGA架构描述文件，该文件指定了FPGA的高级组织（例如块的数量和类型、导线段长度的分布、逻辑集群和逻辑元素的大小）、其微架构细节（例如DSP和BRAM操作模式、逻辑块中的硬件算术、开关块模式），以及晶体管级电路实现参数（例如开关/导线延迟和面积）。像COFFE
    [[92](#bib.bib92)]这样的工具自动化了FPGA电路的晶体管级设计和建模，并生成延迟和面积数据，这些数据将被包含在VTR FPGA架构描述文件中。'
- en: '![Refer to caption](img/db74fcb7b1f4216b9f56eb3564ec7784.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/db74fcb7b1f4216b9f56eb3564ec7784.png)'
- en: 'Figure 9: Key ingredients for FPGA architecture exploration: benchmark circuits,
    architecture description, and a retargettable CAD flow.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: FPGA架构探索的关键要素：基准电路、架构描述和*可重定向* CAD流程。'
- en: Optimizing an FPGA architecture also requires benchmark designs that cover a
    variety of use cases in the target application domains. Typically, FPGA vendors
    have carefully curated benchmark suites comprising proprietary designs representative
    of different customer use cases. There are also several academic open-source benchmark
    suites such as the classic MCNC20, the VTR [[87](#bib.bib87)], and the Titan23
    [[93](#bib.bib93)] suites which are commonly used in academic FPGA architecture
    and CAD research. While early academic FPGA research used the MCNC circuits, they
    are now too small (thousands of logic primitives) and simple (only IOs and logic)
    to represent modern FPGA applications. The VTR and particularly the Titan suites
    are larger and more complex, making them more representative. However, none of
    these benchmark suites contain any FPGA designs representative of the DL domain.
    The Koios benchmark suite [[94](#bib.bib94)] was introduced to address this gap.
    It contains 40 DL circuits that capture a wide variety of sizes, implementation
    styles, target neural networks, numerical precisions, and circuit properties.
    It also introduced a methodology for generating synthetic or *proxy* circuits
    that have similar characteristics as various real DL circuits. The Koios benchmarks
    are open-sourced and integrated into the VTR flow, enabling the exploration of
    new FPGA architectures optimized specifically for DL.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 优化 FPGA 架构也需要涵盖目标应用领域各种用例的基准设计。通常，FPGA 供应商会精心策划基准套件，这些套件包含代表不同客户用例的专有设计。此外，还有几个学术开源基准套件，如经典的
    MCNC20、VTR [[87](#bib.bib87)] 和 Titan23 [[93](#bib.bib93)]，这些套件在学术 FPGA 架构和 CAD
    研究中常被使用。虽然早期学术 FPGA 研究使用了 MCNC 电路，但它们现在太小（数千个逻辑原件）且简单（仅有 IO 和逻辑）以代表现代 FPGA 应用。VTR
    和特别是 Titan 套件更大更复杂，更具代表性。然而，这些基准套件中没有任何 FPGA 设计能够代表 DL 领域。为填补这一空白，推出了 Koios 基准套件
    [[94](#bib.bib94)]。它包含 40 个 DL 电路，涵盖了多种尺寸、实现风格、目标神经网络、数值精度和电路特性。它还引入了一种生成合成或 *代理*
    电路的方法，这些电路具有与各种实际 DL 电路相似的特性。Koios 基准套件是开源的，并集成到 VTR 流程中，使得能够探索专门为 DL 优化的新 FPGA
    架构。
- en: IV-B Methodology
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 方法论
- en: In this subsection, we explain the general methodology for evaluating new FPGA
    architecture ideas using the tools and benchmarks that we introduced in the previous
    subsection. A similar methodology is used for evaluating the gains and cost of
    most of the proposed FPGA fabric architecture enhancements discussed in the rest
    of this article.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们解释了使用前一小节中介绍的工具和基准来评估新 FPGA 架构思想的一般方法。类似的方法也用于评估本文其余部分讨论的大多数提议 FPGA
    结构架构增强的收益和成本。
- en: 'A common FPGA architecture enhancement for a specific target domain is to introduce
    a new type of hard block to the FPGA fabric (or change an existing one) to efficiently
    implement common functionalities in application designs from this domain. As an
    example, for the DL target domain, an FPGA architect might evaluate adding hard
    convolution engines to the FPGA fabric. This involves many design trade-offs and
    questions including: how much of the FPGA die area should be dedicated to these
    convolution blocks? How flexible should they be? Do they implement only convolutions
    or can be re-configured to implement other operations and used by a broader set
    of applications? What impact does their addition have on the demand for programmable
    routing? How much do they improve the overall target application performance and
    at what cost to other application domains?'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 针对特定目标领域的一种常见 FPGA 架构增强是向 FPGA 结构中引入一种新的硬块类型（或更改现有的硬块），以有效实现来自该领域应用设计的常见功能。例如，对于
    DL 目标领域，FPGA 架构师可能会评估将硬卷积引擎添加到 FPGA 结构中的效果。这涉及许多设计权衡和问题，包括：应将 FPGA 芯片面积的多少分配给这些卷积块？它们应该有多灵活？它们仅实现卷积还是可以重新配置以实现其他操作并被更广泛的应用程序使用？它们的添加对可编程布线需求有何影响？它们在整体目标应用性能方面的提升程度及其对其他应用领域的成本是多少？
- en: To answer these questions, the architect first writes an RTL implementation
    for their new proposed hard block (a convolution engine in our example). This
    implementation describes the cycle-accurate functionality of the block as well
    as its different reconfigurable modes of operation. Then, they perform the circuit-level
    evaluation using a tool like COFFE. FPGA circuitry consists of both standard cell
    (ASIC) components and full custom (hand-optimized transistors and layouts) components,
    and COFFE models and evaluates both types. The functionality of the hard block
    is implemented using standard cell ASIC EDA tools, while the interface to the
    programmable routing uses automated full-custom design and SPICE modeling. The
    outcomes of this step are the area, timing and power models of the proposed hard
    block. These models are then plugged into an FPGA CAD flow (such as VTR) to perform
    the architecture-level evaluation by mapping a set of representative benchmark
    circuits (such as Koios) to the FPGA architecture including the proposed new hard
    block. This mapping can be done by modifying the benchmarks to directly instantiate
    an instance of the new hard block or by extending the synthesis tools to automatically
    extract circuit netlist components and map them to the new hard block. This last
    step evaluates the resource utilization, timing, and routability of the benchmarks
    of interest on the proposed FPGA architecture. Enhancements to the programmable
    logic blocks and BRAMs can be evaluated using the same general methodology, except
    that the core of these blocks is also custom designed and laid out instead of
    being implemented with ASIC standard cells.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，架构师首先为他们新提议的硬块（在我们的例子中是一个卷积引擎）编写RTL实现。这一实现描述了硬块的周期准确功能以及其不同的可重配置操作模式。然后，他们使用像COFFE这样的工具进行电路级评估。FPGA电路包括标准单元（ASIC）组件和全定制（手工优化的晶体管和布局）组件，COFFE对这两种类型进行建模和评估。硬块的功能通过标准单元ASIC
    EDA工具实现，而可编程路由的接口则使用自动化全定制设计和SPICE建模。这一步的结果是所提议硬块的面积、时序和功耗模型。这些模型随后被插入FPGA CAD流程（如VTR），通过将一组代表性基准电路（如Koios）映射到包括所提议的新硬块在内的FPGA架构，来进行架构级评估。此映射可以通过修改基准电路以直接实例化新硬块的实例，或通过扩展综合工具以自动提取电路网表组件并将其映射到新硬块来完成。最后一步评估了所关注基准电路在提议FPGA架构上的资源利用率、时序和可路由性。对可编程逻辑块和BRAM的增强可以使用相同的一般方法进行评估，只是这些块的核心也是定制设计和布局的，而不是使用ASIC标准单元实现的。
- en: IV-C Taxonomy of FPGA Architecture Enhancements for DL
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 深度学习的FPGA架构增强分类
- en: '![Refer to caption](img/99c1843338860ba00e0306d0acd4d2ec.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/99c1843338860ba00e0306d0acd4d2ec.png)'
- en: 'Figure 10: Taxonomy of FPGA architecture enhancements for DL.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：深度学习的FPGA架构增强分类。
- en: 'In the rest of this article, we present several DL-targeted FPGA architecture
    enhancements from both academic research and industry. Fig. [10](#S4.F10 "Figure
    10 ‣ IV-C Taxonomy of FPGA Architecture Enhancements for DL ‣ IV FPGA Architecture
    Exploration ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions") illustrates the taxonomy of these proposals which include
    <svg   height="19.62" overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg> enhancing existing
    conventional FPGA fabric blocks (e.g. LBs, DSPs, BRAMs), <svg height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
    introducing new DL-specific in-fabric hard blocks (e.g. tensor blocks), <svg height="19.62"
    overflow="visible" version="1.1" width="19.62"><g transform="translate(0,19.62)
    matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg> tightly integrating
    coarse-grained DL accelerators on the same die with an FPGA fabric (e.g. AMD AI
    engines), as well as <svg height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    integrating FPGAs and other DL chiplets in the same package.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文的其余部分，我们将介绍几种面向深度学习的 FPGA 架构增强技术，包括学术研究和工业界的最新进展。图 [10](#S4.F10 "Figure
    10 ‣ IV-C Taxonomy of FPGA Architecture Enhancements for DL ‣ IV FPGA Architecture
    Exploration ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions") 展示了这些提案的分类，其中包括 <svg   height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    提升现有传统 FPGA 结构模块（例如 LBs、DSPs、BRAMs），<svg height="19.62" overflow="visible" version="1.1"
    width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0)
    translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
    引入新的深度学习专用硬块（例如张量块），<svg height="19.62" overflow="visible" version="1.1" width="19.62"><g
    transform="translate(0,19.62) matrix(1 0 0 -1 0 0) translate(9.81,0) translate(0,9.81)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    在同一芯片上与 FPGA 结构紧密集成粗粒度深度学习加速器（例如 AMD AI 引擎），以及 <svg height="19.62" overflow="visible"
    version="1.1" width="19.62"><g transform="translate(0,19.62) matrix(1 0 0 -1 0
    0) translate(9.81,0) translate(0,9.81)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    在同一封装中集成 FPGA 和其他深度学习芯片。'
- en: V Enhancing Existing FPGA Fabric Blocks
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 提升现有 FPGA 结构模块
- en: V-A Logic Blocks
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 逻辑块
- en: 'As discussed in Section [III-A](#S3.SS1 "III-A Programmable Logic & Routing
    ‣ III FPGA Architecture & Opportunities for DL ‣ Field-Programmable Gate Array
    Architecture for Deep Learning: Survey & Future Directions"), many prior works
    have shown that various DL models can be quantized down to lower precisions with
    little to no accuracy loss during inference [[95](#bib.bib95)]. Narrow integer
    MAC operations (e.g. int8, int4) are now natively supported in many commercial
    DL accelerators [[96](#bib.bib96), [97](#bib.bib97)]. In addition, new low-precision
    floating point formats are being standardized (e.g. fp8 [[33](#bib.bib33)]) and
    are expected to be supported in the next generation of compute platforms for DL.
    FPGAs offer unique flexibility to implement any desired numerical precision directly
    in hardware using their fine-grained programmable LBs and since the logic usage
    of a multiplier grows quadratically with its precision, large hardware savings
    are possible by cutting precision to the minimum. LBs are the most abundant resource
    in a conventional FPGA fabric, so architecture changes that increase their efficiency
    for implementing low-precision multipliers will have high impact in DL applications.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '如[III-A](#S3.SS1 "III-A Programmable Logic & Routing ‣ III FPGA Architecture
    & Opportunities for DL ‣ Field-Programmable Gate Array Architecture for Deep Learning:
    Survey & Future Directions")节所述，许多先前的研究表明，各种深度学习模型可以被量化到更低的精度，在推理过程中几乎没有准确度损失[[95](#bib.bib95)]。许多商业深度学习加速器现在原生支持窄整数MAC操作（例如int8、int4）[[96](#bib.bib96),
    [97](#bib.bib97)]。此外，新型低精度浮点格式正在标准化中（例如fp8[[33](#bib.bib33)]），并预计将在下一代深度学习计算平台中得到支持。FPGA提供了独特的灵活性，可以直接在硬件中实现任何所需的数值精度，利用其精细的可编程逻辑块（LBs），由于乘法器的逻辑使用量随精度呈平方增长，通过将精度降低到最低，可以获得巨大的硬件节省。LBs是传统FPGA结构中最丰富的资源，因此，在实现低精度乘法器时提高其效率的架构变更将在深度学习应用中产生重大影响。'
- en: '![Refer to caption](img/c771f859a233559de68ca52b490dde95.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c771f859a233559de68ca52b490dde95.png)'
- en: 'Figure 11: Architecture of a logic element similar to that of Intel Stratix
    10 and Agilex. It can operate as four 4-LUTs followed by two additions, two 5-LUTs,
    or one 6-LUT.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：类似于Intel Stratix 10和Agilex的逻辑元素架构。它可以作为四个4-LUTs加上两个加法器、两个5-LUTs或一个6-LUT进行操作。
- en: '![Refer to caption](img/4b490a324ad72cfdaea7e298e4c1b11a.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b490a324ad72cfdaea7e298e4c1b11a.png)'
- en: 'Figure 12: Mapping of 4-bit multiplication to conventional logic elements.
    The LUTs are significantly underutilized: 4/5-LUTs are used to implement 2-input
    ANDs or the identity function (i.e. pass-through) to access the adders.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：4位乘法映射到传统逻辑元素。LUTs的利用率明显不足：4/5-LUTs用于实现2输入AND门或恒等函数（即直通）以访问加法器。
- en: 'Fig. [11](#S5.F11 "Figure 11 ‣ V-A Logic Blocks ‣ V Enhancing Existing FPGA
    Fabric Blocks ‣ Field-Programmable Gate Array Architecture for Deep Learning:
    Survey & Future Directions") shows the internal architecture of a modern FPGA
    LE, similar to that in the Intel Stratix 10 and Agilex FPGAs. It has 8 distinct
    inputs and 4 optionally-registered outputs, as well as two chained hard adders
    that are fed by four 4-input LUTs (4-LUTs). Therefore, at a high-level, each LE
    can implement four 4-input logic functions followed by 2-bits of addition, two
    5-input logic functions, or one 6-input logic function as long as no more than
    8 distinct inputs are needed. Fig. [12](#S5.F12 "Figure 12 ‣ V-A Logic Blocks
    ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable Gate Array Architecture
    for Deep Learning: Survey & Future Directions") illustrates how a 4-bit multiplication
    is mapped to this LE architecture as an example, where bits of the two multiplication
    operands are represented by different shapes and colors. The first step of multiplication
    is performing an AND between each bit of one operand and all bits of the other
    operand to generate partial products, as illustrated by combining the color and
    shape of the bits in Fig. [12](#S5.F12 "Figure 12 ‣ V-A Logic Blocks ‣ V Enhancing
    Existing FPGA Fabric Blocks ‣ Field-Programmable Gate Array Architecture for Deep
    Learning: Survey & Future Directions"). These partial products are then reduced
    over one or multiple stages of addition to produce the final multiplication result.
    Thus, multiplications can be fundamentally viewed as bit-level ANDs followed by
    adder trees (usually referred to as *compressor trees*). In an LE implementation,
    the 2-input ANDs are mapped to the 4-LUTs followed by adders to realize the first
    level of reduction. Then, other LEs are used only for the adders (i.e. the 4-LUTs
    implement identity functions) to perform the subsequent reductions until the final
    result is produced. This highlights a major source of inefficiency: the LUTs are
    significantly underutilized. Many of the used LUTs in Fig. [12](#S5.F12 "Figure
    12 ‣ V-A Logic Blocks ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions") are just
    pass-throughs to access the adders, and even the LUTs that implement partial products
    perform a 2-input AND function, wasting half of the functionality of a 4-LUT.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [11](#S5.F11 "Figure 11 ‣ V-A Logic Blocks ‣ V Enhancing Existing FPGA Fabric
    Blocks ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions") 显示了现代 FPGA 逻辑单元（LE）的内部结构，类似于 Intel Stratix 10 和 Agilex FPGAs。它具有
    8 个不同的输入和 4 个可选择注册的输出，以及两个由四个 4 输入查找表（4-LUTs）供给的链式硬加法器。因此，从高层次来看，每个 LE 可以实现四个
    4 输入逻辑函数，接着是 2 位的加法，两个 5 输入逻辑函数，或者一个 6 输入逻辑函数，只要不需要超过 8 个不同的输入。图 [12](#S5.F12
    "Figure 12 ‣ V-A Logic Blocks ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions") 说明了 4
    位乘法如何映射到这种 LE 架构作为一个示例，其中两个乘法操作数的位由不同的形状和颜色表示。乘法的第一步是对一个操作数的每个位与另一个操作数的所有位进行 AND
    操作，以生成部分积，如图 [12](#S5.F12 "Figure 12 ‣ V-A Logic Blocks ‣ V Enhancing Existing
    FPGA Fabric Blocks ‣ Field-Programmable Gate Array Architecture for Deep Learning:
    Survey & Future Directions") 中通过位的颜色和形状的组合所示。这些部分积随后在一个或多个加法阶段中被减少，以生成最终的乘法结果。因此，乘法本质上可以看作是位级
    AND 操作，接着是加法树（通常称为 *压缩器树*）。在 LE 实现中，2 输入 AND 操作映射到 4-LUTs，再由加法器实现第一层的减少。然后，其他
    LE 仅用于加法器（即 4-LUTs 实现恒等函数）以执行随后的减少，直到生成最终结果。这突出了一个主要的低效来源：LUTs 的利用率明显不足。图 [12](#S5.F12
    "Figure 12 ‣ V-A Logic Blocks ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions") 中使用的许多
    LUTs 仅是通道访问加法器，甚至那些实现部分积的 LUTs 执行了 2 输入 AND 功能，浪费了 4-LUT 一半的功能。'
- en: 'The authors of [[98](#bib.bib98), [99](#bib.bib99)] highlight these inefficiencies
    and propose four architectural modifications (summarized in Fig. [13](#S5.F13
    "Figure 13 ‣ V-A Logic Blocks ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions")) to address
    them at both the LE and LB levels. The first proposal adds another cascaded adder
    chain fed by the two sum outputs of the existing chain and two independent inputs.
    This can efficiently implement compressor trees by obviating the need for a second
    level of LEs used only as adders.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '作者在[[98](#bib.bib98)、[99](#bib.bib99)]中指出了这些低效，并提出了四种架构修改（总结见图[13](#S5.F13
    "Figure 13 ‣ V-A Logic Blocks ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions")）以在 LE
    和 LB 级别解决这些问题。第一个提案通过在现有链的两个和输出和两个独立输入的输入下添加另一个级联加法器链。这可以通过省去仅用作加法器的第二级 LE 来有效实现压缩器树。'
- en: 'The second proposal implements a single 4-bit adder chain by adding circuitry
    to allow further fracturing of each 4-LUT into two 3-LUTs. Fracturing a 6-LUT
    all the way down to 3-LUTs generates 8 signals which can feed two inputs into
    each of four adders, and a 3-LUT can still implement the 2-input AND gate function
    needed in multiplier partial products. This results in a higher density of adders
    in general at the cost of sacrificing the ability to map 4-input logic functions
    feeding adders in the same LE, which does not occur in multipliers and is expected
    not to be very common in other non-DL FPGA designs. While one might consider continuing
    this process, fracturing down to 2-LUTs feeding 8 chained adders per LE, this
    would exceed the 8 inputs the programmable routing provides to an LE, and adding
    additional inputs from the programmable routing is costly in terms of area [[73](#bib.bib73)].
    Another variation on this idea arranges the four adders into two 2-bit chains
    per LE instead of a single 4-bit chain, as shown in Fig. [13](#S5.F13 "Figure
    13 ‣ V-A Logic Blocks ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions"). This
    is different from the cascaded adder chain in the first proposal, since all the
    adders in both chains are fed directly by the LUTs.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '第二个提案通过添加电路来实现单个 4 位加法器链，以允许进一步将每个 4-LUT 分裂成两个 3-LUT。将 6-LUT 分裂到 3-LUT 生成 8
    个信号，可以将两个输入送入四个加法器中的每一个，而 3-LUT 仍能实现乘法器部分乘积中所需的 2 输入 AND 门功能。这通常会提高加法器的密度，但牺牲了在同一
    LE 中映射加法器的 4 输入逻辑功能的能力，这在乘法器中不会发生，并且预计在其他非深度学习 FPGA 设计中不太常见。虽然可以考虑继续这一过程，将 2-LUT
    分裂为每个 LE 8 个链式加法器，但这将超过可编程路由为 LE 提供的 8 个输入，并且从可编程路由添加额外输入在面积上代价高[[73](#bib.bib73)]。这一想法的另一种变体是将四个加法器安排成每
    LE 两个 2 位链，而不是单个 4 位链，如图[13](#S5.F13 "Figure 13 ‣ V-A Logic Blocks ‣ V Enhancing
    Existing FPGA Fabric Blocks ‣ Field-Programmable Gate Array Architecture for Deep
    Learning: Survey & Future Directions")所示。这与第一个提案中的级联加法器链不同，因为两个链中的所有加法器都直接由 LUT
    提供输入。'
- en: '![Refer to caption](img/f1a89c6221b548b9e67a82e07a404f27.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f1a89c6221b548b9e67a82e07a404f27.png)'
- en: 'Figure 13: Four architectural modifications to the FPGA LEs and LBs for increasing
    the density of low precision MAC in soft logic.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：增加低精度 MAC 在软逻辑中密度的四种架构修改。
- en: The fourth idea modifies the LB architecture by adding a low-precision hard
    multiplier in some or all of the FPGA LBs. They are referred to as shadow multipliers
    as, when used, they steal the input and output ports of some of the LEs; this
    makes those LEs unusable but avoids adding expensive input and output ports to
    the programmable routing. The shadow multipliers from different LBs can also be
    combined to implement higher-precision multiplications using the programmable
    routing and some LE-based glue logic.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个想法通过在一些或所有 FPGA LB 中添加低精度硬乘法器来修改 LB 架构。这些乘法器被称为影子乘法器，因为使用时它们会窃取一些 LE 的输入和输出端口；这使得这些
    LE 无法使用，但避免了在可编程路由中添加昂贵的输入和输出端口。来自不同 LB 的影子乘法器也可以结合使用，通过可编程路由和一些基于 LE 的胶水逻辑实现更高精度的乘法。
- en: These four ideas vary in their area costs and performance gains. For example,
    the two 2-bit adder chains proposal results in 1.5$\times$ denser matrix multiplications,
    while being 10% faster and also benefiting other non-DL benchmarks. These gains
    come at a modest cost of only 3% increase in die area compared to a Stratix-10-like
    baseline fabric. On the other hand, adding a 9-bit shadow multiplier to each LB
    results in 2.4$\times$ denser and 17% faster matrix multiplications, at the cost
    of a 15% increase in die area. A patent filed by Intel [[100](#bib.bib100)] further
    enhanced the cascaded adder chains proposal to achieve denser MAC mappings, but
    is not yet adopted in commercial FPGA architectures.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个想法在面积成本和性能提升方面各不相同。例如，两个2位加法器链的提案使矩阵乘法密度提高了1.5倍，同时速度提升了10%，并且对其他非深度学习基准测试也有益处。这些提升以仅3%的芯片面积增加为代价，相比Stratix-10类似的基准架构。另一方面，在每个逻辑块中添加9位阴影乘法器使矩阵乘法的密度提高了2.4倍，速度提高了17%，但芯片面积增加了15%。英特尔申请的专利[[100](#bib.bib100)]进一步增强了级联加法器链提案，以实现更密集的MAC映射，但尚未在商业FPGA架构中采用。
- en: 'While [[98](#bib.bib98), [99](#bib.bib99)] focused on adding more full adders
    to LEs for denser arithmetic, both MAC and pop-count operations in low-precision
    and binarized DL models typically require the addition of more than 3 bits and
    can benefit from generalized parallel adders or *compressors*. A full adder is
    a simple compressor that takes 3 bits as inputs ($A,B,C_{in}$) and *compresses*
    them into 2 bits (a sum $S$ and a higher significance carry $C_{out}$). Therefore,
    a full adder is typically referred to as a $C3:11$ compressor (3 inputs $\rightarrow$
    1 same significance + 1 higher significance outputs). This concept can be generalized
    to any number of input bits, where the compressor output is simply a count of
    the number of ones in the input bits. The authors of [[101](#bib.bib101)] analyzed
    a variety of microbenchmarks and found that more than 35% of the compressors in
    these designs are $C6:111$ compressors. A $C6:111$ can be viewed as three 6-input
    logic functions (one for each output bit) and thus can be mapped to 3 LEs. One
    of these three logic functions is a simple 6-input XOR. In [[101](#bib.bib101)],
    the authors evaluated adding a hardened 6-input XOR gate to a typical recent LE
    architecture (similar to that in Fig. [11](#S5.F11 "Figure 11 ‣ V-A Logic Blocks
    ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable Gate Array Architecture
    for Deep Learning: Survey & Future Directions")). Since all three logic functions
    share the same 6 inputs and the LE has up to 4 outputs, the added XOR gate enables
    a single LE to implement two of the three logic functions in a $C6:111$ compressor.
    This results in up to 36% denser compressor implementations at the cost of less
    than 0.5% increase in die area.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然[[98](#bib.bib98), [99](#bib.bib99)]的研究专注于在逻辑单元中添加更多的全加器以实现更密集的算术运算，但在低精度和二值化深度学习模型中的MAC和pop-count操作通常需要添加超过3位的比特，并且可以从通用并行加法器或*压缩器*中获益。全加器是一个简单的压缩器，它接受3个位的输入（$A,B,C_{in}$）并将其*压缩*为2个位（一个和$S$和一个更高位的进位$C_{out}$）。因此，全加器通常被称为$C3:11$压缩器（3个输入$\rightarrow$
    1个同位输出+ 1个更高位输出）。这一概念可以推广到任意数量的输入比特，其中压缩器输出只是输入比特中1的数量。[[101](#bib.bib101)]的作者分析了各种微基准测试，并发现这些设计中超过35%的压缩器是$C6:111$压缩器。$C6:111$可以视为三个6输入逻辑函数（每个输出比特一个），因此可以映射到3个逻辑单元。三个逻辑函数中的一个是简单的6输入异或门。在[[101](#bib.bib101)]中，作者评估了在典型的最新逻辑单元架构中添加一个增强的6输入异或门（类似于图[11](#S5.F11
    "Figure 11 ‣ V-A Logic Blocks ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions")中的架构）。由于所有三个逻辑函数共享相同的6个输入，并且逻辑单元具有最多4个输出，添加的异或门使得单个逻辑单元能够实现$C6:111$压缩器中的两个逻辑函数。这使得压缩器实现的密度提高了多达36%，而芯片面积仅增加了不到0.5%。'
- en: Kim et al. [[102](#bib.bib102)] also proposed two architectural modifications
    to the hard adder chains in LBs for enhancing the efficiency of popcount operations
    in binarized DL models. The first proposal adds a new popcount adder chain that
    propagates the sum bits on the dedicated chain interconnect and produces the carry
    out bits to the outputs of the LEs (which differs from the conventional adder
    chain that propagates carry bits and produces sum bits). The second proposal further
    optimizes popcount implementation by adding another full adder that can sum the
    two carry out bits of the two popcount adders in an LE. These two architectural
    changes reduce the logic utilization of popcount operations of different widths
    by 23-44% and 36-40% at the cost of only 1.9% and 2.4% increase in the LB’s silicon
    footprint, respectively.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Kim等人[[102](#bib.bib102)]还提出了对LB中硬加法链的两个架构修改，以提高二值DL模型中popcount操作的效率。第一个提议增加了一个新的popcount加法链，该链通过专用链互连传播和产生输出到LE的进位位（与传统的加法链传播进位位和产生和位不同）。第二个提议通过增加另一个全加器来进一步优化popcount实现，该全加器可以对LE中两个popcount加法器的两个进位位进行求和。这两个架构变更将不同宽度popcount操作的逻辑利用率分别降低了23-44%和36-40%，而LB的硅面积增加仅为1.9%和2.4%。
- en: V-B DSPs
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B DSPs
- en: 'Along the same vein of increasing low-precision MAC efficiency on FPGAs, both
    academic research and FPGA vendors have investigated adding native support for
    low precisions in conventional DSP blocks. As discussed in Section [III-B](#S3.SS2
    "III-B Digital Signal Processing Blocks ‣ III FPGA Architecture & Opportunities
    for DL ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions"), filtering and wireless communication applications were
    historically the key drivers of DSP block architecture decisions. Therefore, DSP
    blocks in commercial FPGAs until the 14nm process generation from both Intel (Stratix
    10) and AMD (Ultrascale+) had native support for numerical precisions suitable
    for wireless communication applications. In 2013, Intel added native support for
    single-precision floating-point (fp32) in the DSPs of their Arria 10 (and later
    Stratix 10) devices to enhance their efficiency for high-performance computing.
    The rapid growth in the DL domain motivated the work in [[103](#bib.bib103)],
    which was the first to investigate DSP micro-architecture optimizations for low-precision
    DL.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '在提高FPGA上低精度MAC效率的相同思路下，学术研究和FPGA供应商都探讨了在传统DSP块中增加对低精度的本机支持。如在第[III-B](#S3.SS2
    "III-B Digital Signal Processing Blocks ‣ III FPGA Architecture & Opportunities
    for DL ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions")节中讨论的那样，过滤和无线通信应用历来是DSP块架构决策的主要驱动因素。因此，直到14nm工艺代的商业FPGA（包括Intel的Stratix
    10和AMD的Ultrascale+）中的DSP块都具有适合无线通信应用的数值精度本机支持。2013年，Intel在其Arria 10（及后来的Stratix
    10）设备的DSP中增加了对单精度浮点数（fp32）的本机支持，以提高其高性能计算的效率。DL领域的快速增长激励了[[103](#bib.bib103)]的研究，这项研究首次探讨了低精度DL的DSP微架构优化。'
- en: '![Refer to caption](img/a142661ad7543ef2cd01d5cecffd83dc.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a142661ad7543ef2cd01d5cecffd83dc.png)'
- en: 'Figure 14: Enhancements to the FPGA DSP blocks for DL such as: (1) fracturing
    larger multipliers into smaller ones while keeping the same interface to the programmable
    routing (left), and (2) adding more dedicated interconnect between DSPs in a column
    for efficient 2D systolic array implementation and integrating an internal FIFO
    or register file for efficient data reuse near compute (right).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：FPGA DSP块在DL方面的增强，例如：（1）将较大的乘法器分解为较小的乘法器，同时保持与可编程路由的相同接口（左），以及（2）在DSP列之间增加更多专用互连，以实现高效的2D脉动阵列实现，并集成内部FIFO或寄存器文件，以在计算附近实现高效的数据重用（右）。
- en: 'This work enhanced an Arria-10-like DSP block that can implement one int27
    or two int18 multiplications to also natively support four int9 and eight int4
    multiply and MAC operations at a low area cost, as illustrated on the left side
    of Fig. [14](#S5.F14 "Figure 14 ‣ V-B DSPs ‣ V Enhancing Existing FPGA Fabric
    Blocks ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions"). This was achieved by balancing the addition of new small
    4-bit multiplier arrays and low-cost circuitry that enables the fracturing of
    existing multiplier arrays into multiple independent sub-arrays. In addition,
    the chain reduction and accumulation was split into two lanes as shown in Fig. [14](#S5.F14
    "Figure 14 ‣ V-B DSPs ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions") to minimize
    the area and delay cost of supporting the MAC mode for these precisions. The design
    of this new enhanced DSP block was guided by three key design principles: (1)
    Ensure backward compatibility such that the DSP blocks are still efficiently usable
    for non-DL applications, (2) Have minimal effect on DSP block area footprint and
    operating frequency to minimize the negative impact on other applications that
    do not benefit from the added modes of operation, and (3) Keep the same number
    of input/output ports to/from the DSP block to avoid both the expensive area cost
    of additional interfaces to the programmable routing and the creation of routing
    hot spots in the proximity of these blocks.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作增强了类似Arria-10的DSP模块，使其不仅可以实现一个int27或两个int18乘法，还原生支持四个int9和八个int4的乘法及MAC操作，且在低面积成本下实现，如图 [14](#S5.F14
    "图 14 ‣ V-B DSPs ‣ V 增强现有FPGA功能块 ‣ 深度学习的现场可编程门阵列架构：综述与未来方向")左侧所示。这是通过平衡添加新的4位乘法器阵列和低成本电路实现的，这些电路使得现有乘法器阵列能够分裂成多个独立的子阵列。此外，如图 [14](#S5.F14
    "图 14 ‣ V-B DSPs ‣ V 增强现有FPGA功能块 ‣ 深度学习的现场可编程门阵列架构：综述与未来方向")所示，链式减法和累加被分成了两个通道，以最小化支持这些精度的MAC模式的面积和延迟成本。这种新增强的DSP模块的设计遵循了三个关键设计原则：（1）确保向后兼容，使得DSP模块仍然可以有效用于非深度学习应用，（2）对DSP模块的面积占用和操作频率影响最小，以减少对不受新增操作模式影响的其他应用的负面影响，（3）保持与DSP模块的输入/输出端口数量相同，以避免额外接口的高面积成本和这些模块附近的布线热点生成。
- en: The enhanced DSP block from [[103](#bib.bib103)] increased the area of the DSP
    block by 12% which corresponds to only a 0.6% increase in the overall die area
    of DSP-rich devices, with no effect on its operating frequency. When used in several
    DL accelerator designs, the new DSP blocks enhanced performance by 1.3$\times$
    and 1.6$\times$ while reducing resource utilization by 15% and 30% for int9 and
    int4 precisions, respectively. Subsequent commercial FPGA architectures from both
    Intel (Agilex) and Xilinx (Versal) added similar native support for four and three
    int8/int9 multiplications per DSP block, respectively.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[[103](#bib.bib103)]的增强型DSP模块使DSP模块的面积增加了12%，这仅对应于DSP丰富设备整体芯片面积的0.6%的增加，对其操作频率没有影响。在多个深度学习加速器设计中，新的DSP模块提高了1.3$\times$和1.6$\times$的性能，同时将资源利用率分别减少了15%和30%用于int9和int4精度。随后，Intel（Agilex）和Xilinx（Versal）的商业FPGA架构分别增加了对每个DSP模块四个和三个int8/int9乘法的类似原生支持。
- en: 'Conventional DSP blocks also have dedicated wires that can pass the inputs/outputs
    of one DSP block to the next block in the same column. This was originally designed
    to help implement more efficient 1D systolic arrays for finite impulse response
    (FIR) filters in wireless communication applications. However, for the DL domain,
    efficient matrix-matrix multiplication and convolution operations can be implemented
    as 2D systolic arrays. Therefore, Rasoulinezhad et al. [[104](#bib.bib104)] explored
    adding a special pattern of dedicated interconnect between DSP blocks that can
    efficiently map 2D systolic arrays to a 1D column of DSP blocks without using
    the general programmable routing, as shown on the right side of Fig. [14](#S5.F14
    "Figure 14 ‣ V-B DSPs ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions"). They
    also proposed integrating a small memory inside the DSP block (register file or
    FIFO) to enhance energy efficiency by storing data very close to compute. This
    enables reusing the same set of operands across many computations (which is common
    in many DL compute kernels) without the need to read and transport it from distributed
    LUT-based memories or BRAMs to DSP blocks. Their PIR-DSP block significantly reduced
    energy consumption by 70%, 82%, and 87% (on average across several neural network
    implementations) compared to a baseline Xilinx-like DSP block for int9, int4,
    and int2 precisions, respectively. These improvements come at the cost of 28%
    increase in the DSP block area footprint.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '传统的 DSP 模块也具有专用的连接线，这些连接线可以将一个 DSP 模块的输入/输出传递到同一列中的下一个模块。这最初是为了帮助实现更高效的 1D
    脉动阵列，用于无线通信应用中的有限冲激响应（FIR）滤波器。然而，在深度学习领域，效率更高的矩阵-矩阵乘法和卷积操作可以作为 2D 脉动阵列来实现。因此，Rasoulinezhad
    等人[[104](#bib.bib104)] 探讨了在 DSP 模块之间添加一种特殊的专用互连模式，以高效地将 2D 脉动阵列映射到 1D 的 DSP 模块列中，而不使用通用可编程布线，如图
    [14](#S5.F14 "Figure 14 ‣ V-B DSPs ‣ V Enhancing Existing FPGA Fabric Blocks ‣
    Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions") 右侧所示。他们还提出在 DSP 模块内部集成一个小型内存（寄存器文件或 FIFO），通过将数据存储在非常接近计算的位置来提高能源效率。这使得可以在多个计算中重复使用同一组操作数（这在许多深度学习计算内核中很常见），而无需从分布式
    LUT 基础的内存或 BRAM 中读取和传输到 DSP 模块。他们的 PIR-DSP 模块在 int9、int4 和 int2 精度下，相比于基线的 Xilinx
    类 DSP 模块，分别将能源消耗显著降低了 70%、82% 和 87%（平均在几个神经网络实现中）。这些改进的代价是 DSP 模块面积增加了 28%。'
- en: V-C BRAMs
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C BRAMs
- en: 'In DL applications, the FPGA BRAMs are used as on-chip user-managed scratchpads
    to store computation operands (weights and activations) and results, feeding the
    compute units with data at a very high bandwidth due to their distributed nature.
    However, the separation of compute units (implemented using LBs and DSPs) from
    storage units (BRAMs) implies data movement to feed the compute units with input
    data and store the outputs back to the BRAMs. This uses a large amount of the
    FPGA’s programmable routing, leading to routing hot spots between the memory and
    compute units and increased power consumption. To address these challenges, several
    research efforts have proposed adding compute capabilities to the FPGA BRAMs by
    introducing lightweight bit-level PEs inside the BRAM itself to bring compute
    closer to the data. This provides three major advantages: (1) It increases the
    compute throughput of the FPGA because a larger portion of the FPGA die area can
    now perform computation, (2) it reduces the data movement saving both energy and
    valuable programmable routing resources, and (3) it provides massive compute parallelism
    as the large number of BRAM bitlines can operate as bit-serial SIMD lanes executing
    the same operation on all the bits of a memory word. Similarly to the DSP block
    enhancements in Section [V-B](#S5.SS2 "V-B DSPs ‣ V Enhancing Existing FPGA Fabric
    Blocks ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions"), the new compute-capable BRAMs should be functionally backward
    compatible, incur minimal performance loss for traditional usage, and avoid adding
    new input/output ports to/from the programmable routing to avoid area overheads
    for designs that do not use in-BRAM compute.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '在深度学习应用中，FPGA 的 BRAM 被用作芯片内用户管理的缓存区，用于存储计算操作数（权重和激活值）及结果，由于其分布式的特性，以非常高的带宽将数据输送到计算单元。然而，将计算单元（使用
    LBs 和 DSPs 实现）与存储单元（BRAMs）分离意味着需要移动数据以为计算单元提供输入数据，并将输出数据存回 BRAMs。这会使用大量 FPGA 的可编程布线资源，导致内存与计算单元之间出现布线热点，并增加功耗。为了解决这些挑战，若干研究工作建议通过在
    BRAM 内部引入轻量级位级处理单元（PEs）来将计算能力添加到 FPGA BRAM 中，从而将计算更接近数据。这带来了三个主要优势：（1）它提高了 FPGA
    的计算吞吐量，因为 FPGA 晶片上更大的一部分区域现在可以执行计算，（2）它减少了数据移动，节省了能源和宝贵的可编程布线资源，以及（3）它提供了大量的计算并行性，因为大量的
    BRAM 位线可以作为位串行 SIMD 通道，在内存字的所有位上执行相同的操作。类似于第 [V-B](#S5.SS2 "V-B DSPs ‣ V Enhancing
    Existing FPGA Fabric Blocks ‣ Field-Programmable Gate Array Architecture for Deep
    Learning: Survey & Future Directions") 节中 DSP 模块的增强，新型计算能力 BRAM 应在功能上向后兼容，对传统使用造成最小性能损失，并避免增加新的输入/输出端口以避免为不使用
    BRAM 内部计算的设计增加面积开销。'
- en: At a high level, enabling in-BRAM compute requires adding PEs that perform bit-serial
    computations on the outputs of the sense amplifiers inside the BRAM. Two $N$-bit
    rows (i.e. wordlines) are read simultaneously from the RAM cell array, the PEs
    perform $N$ parallel binary operations between the corresponding bits of the two
    words, and the result is stored back to another row in the RAM array. This read-modify-write
    operation happens completely inside the BRAM in a single clock cycle that is longer
    than the normal read/write period in a conventional BRAM. In addition to the sense
    amplifier PEs, lightweight control logic (a finite state machine) may be required
    inside the BRAM to sequence these steps. The specific rows to read, computation
    to perform in the PEs, and row to write constitute a compute instruction which
    is provided to the BRAM through the existing programmable routing ports.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，启用 BRAM 内部计算需要添加执行位串行计算的 PEs，这些 PEs 在 BRAM 内部的感应放大器输出上进行操作。两个 $N$ 位的行（即字线）同时从
    RAM 单元阵列中读取，PEs 在两个字的相应位之间执行 $N$ 个并行的二进制操作，结果存回 RAM 阵列的另一行。这个读-修改-写操作完全在 BRAM
    内部发生，并在一个时钟周期内完成，该时钟周期比传统 BRAM 中的正常读/写周期要长。除了感应放大器 PEs 外，可能还需要在 BRAM 内部添加轻量级控制逻辑（有限状态机）来对这些步骤进行排序。读取的具体行、在
    PEs 中执行的计算以及写入的行构成一个计算指令，这些指令通过现有的可编程布线端口提供给 BRAM。
- en: '![Refer to caption](img/aa929160b2be92423b08837d84e8e732.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考图示](img/aa929160b2be92423b08837d84e8e732.png)'
- en: 'Figure 15: FPGA BRAM internal architecture with the components changed or added
    for in-memory compute highlighted in red.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：FPGA BRAM 内部结构图，其中用于内存计算的组件的变更或添加部分用红色突出显示。
- en: 'Fig. [15](#S5.F15 "Figure 15 ‣ V-C BRAMs ‣ V Enhancing Existing FPGA Fabric
    Blocks ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions") shows a top-level diagram of an FPGA BRAM with the modified/added
    components to incorporate compute capabilities highlighted in red. The dual-port
    memory cell array at the core of the BRAM remains unmodified. In a conventional
    BRAM, the column decoder activates a subset of the bits in a row to be read by
    the sense amplifiers or written by the write drivers. For example, a 20Kb SRAM
    array (similar to that in the BRAMs of modern Intel FPGAs) is arranged as 128$\times$160-bit
    rows [[105](#bib.bib105)]. However, the maximum read/write width of the BRAM is
    40 bits to limit the cost of the programmable routing interfaces. Therefore, the
    BRAM block includes 40 sense amplifiers and 40 write drivers and a column decoder
    selects one 40-bit portion of the 160-bit row to be read/written. To enable maximum
    parallelism for in-BRAM compute, additional sense amplifiers and write drivers
    as well as bit-level PEs are introduced to read/compute/write the full width of
    the array row. The sequencing logic that controls the events of the read/write
    operations (wordline activation, precharge, sense amplifier enable, etc.) in the
    memory is also modified to support reading, computing, and writing in one (longer)
    cycle. One extra interface pin is added to the BRAM; when it is asserted the input
    data and addresses are treated as a compute-in-memory (CIM) instruction. In this
    case the CIM mode glue logic decodes the instruction into low-level control signals
    to various BRAM internal components.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图[15](#S5.F15 "图15 ‣ V-C BRAMs ‣ V 增强现有FPGA织物块 ‣ 用于深度学习的现场可编程门阵列架构：调查与未来方向")展示了一个FPGA
    BRAM的顶层示意图，图中红色突出显示了修改/添加的组件以融入计算能力。BRAM核心的双端口存储单元阵列保持不变。在传统BRAM中，列解码器激活一行中的子集比特，由感应放大器读取或由写入驱动器写入。例如，一个20Kb
    SRAM阵列（类似于现代Intel FPGA中的BRAM）被排列为128$\times$160位行[[105](#bib.bib105)]。然而，为了限制可编程路由接口的成本，BRAM的最大读/写宽度为40位。因此，BRAM块包括40个感应放大器和40个写入驱动器，列解码器选择160位行中的一个40位部分进行读取/写入。为了实现BRAM内计算的最大并行性，引入了额外的感应放大器、写入驱动器以及位级PE，以读取/计算/写入阵列行的全部宽度。控制存储器中读取/写入操作（字线激活、预充电、感应放大器使能等）事件的排序逻辑也进行了修改，以支持在一个（较长的）周期内进行读取、计算和写入。BRAM中增加了一个额外的接口引脚；当该引脚被激活时，输入数据和地址被视为计算内存（CIM）指令。在这种情况下，CIM模式胶合逻辑将指令解码为各种BRAM内部组件的低级控制信号。
- en: 'Fig. [16(a)](#S5.F16.sf1 "In Figure 16 ‣ V-C BRAMs ‣ V Enhancing Existing FPGA
    Fabric Blocks ‣ Field-Programmable Gate Array Architecture for Deep Learning:
    Survey & Future Directions") shows an example architecture of a CIM PE that can
    perform bit-serial addition. On the read path, A and B are the two operand bits
    read from two rows of the SRAM cell array by the sense amplifiers (SA) for the
    two SRAM array ports. The two XOR gates (SGEN) generate the sum bit (Sum) using
    the two operand bits (A and B) and the previous cycle’s carry (Cin). Another set
    of gates (CGEN) are used to compute the carry bit, which is stored in the carry
    FF (C) for the next cycle computation. The read outputs A and B are also sent
    to Dout ports, which is the normal read path. On the write path, 2-input multiplexers
    (Ws) are added before the write drivers (WD) for the two ports. These multiplexers
    determine what to write to the SRAM cell array; the Ws multiplexers select between
    the sum/carry bit and the normal write path inputs (Din). The select lines of
    these multiplexers are driven by the CIM mode glue logic, depending on the mode
    setting and the instruction written to the BRAM input ports.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图[16(a)](#S5.F16.sf1 "图16 ‣ V-C BRAMs ‣ V 增强现有FPGA织物块 ‣ 用于深度学习的现场可编程门阵列架构：调查与未来方向")展示了一个可以执行比特串行加法的CIM
    PE示例架构。在读取路径上，A和B是通过感应放大器（SA）从SRAM单元阵列的两行中读取的两个操作数比特，分别对应两个SRAM阵列端口。两个XOR门（SGEN）使用这两个操作数比特（A和B）以及前一周期的进位（Cin）生成和比特（Sum）。另一组门（CGEN）用于计算进位比特，该比特存储在进位触发器（C）中，以便用于下一周期的计算。读取输出A和B也会送到Dout端口，这是正常的读取路径。在写入路径上，在写入驱动器（WD）之前为两个端口添加了2输入多路选择器（Ws）。这些多路选择器决定写入到SRAM单元阵列的内容；Ws多路选择器在和/进位比特和正常写入路径输入（Din）之间进行选择。这些多路选择器的选择线由CIM模式胶合逻辑驱动，具体取决于模式设置和写入到BRAM输入端口的指令。
- en: '![Refer to caption](img/0a3cec5197f91f5497c4b572fc27fcc3.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0a3cec5197f91f5497c4b572fc27fcc3.png)'
- en: (a)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/64a305ce0aa4ce75b7d518edb5f6b76c.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/64a305ce0aa4ce75b7d518edb5f6b76c.png)'
- en: (b)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 16: (a) Compute-in-memory processing element circuitry for bit-serial
    addition. (b) In-BRAM compute operation example for elementwise addition of two
    $N$-element vectors with 4-bit operands. The input and result vectors are all
    stored in a transposed layout.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '图16: (a) 用于位串加法的内存计算处理单元电路。(b) 例子展示了对两个$N$元素向量进行元素级加法的BRAM计算操作，操作数为4位。输入和结果向量都以转置布局存储。'
- en: 'Fig. [16(b)](#S5.F16.sf2 "In Figure 16 ‣ V-C BRAMs ‣ V Enhancing Existing FPGA
    Fabric Blocks ‣ Field-Programmable Gate Array Architecture for Deep Learning:
    Survey & Future Directions") illustrates the operation of a compute-capable BRAM
    used to perform elementwise addition of two $N$-element vectors (operands 1 and
    2), where each element is a 4-bit integer. The vector elements are first stored
    in a transposed memory layout, where each of the elements of the first vector
    is stored in a different column over 4 rows ($i$ to $i+3$) and elements of the
    second vector are stored in the same columns over 4 different rows ($j$ to $j+3$).
    In one cycle, rows $i$ and $j$ are read, one on each port of the dual-port SRAM
    array. Each PE receives two bits (one from row $i$ and one from row $j$) and computes
    the sum of the two bits and the carry from the previous cycle. The carry-out is
    stored in the carry FF in the PE, and the sum is written to row $k$ using one
    port. This process is repeated for 4 cycles while incrementing the row addresses
    read from and written to. In the fifth cycle, the last carry bits stored in each
    PE are written to row $k+4$ using the second write port and the final result vector
    of the elementwise addition operation is now available in rows $k$ to $k+4$. More
    complex operations such as multiplication or reduction can be performed as a sequence
    of additions and memory copies.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图[16(b)](#S5.F16.sf2 "在图16 ‣ V-C BRAMs ‣ V 增强现有FPGA架构 ‣ 适用于深度学习的现场可编程门阵列架构：调查与未来方向")展示了一个具有计算能力的BRAM的操作，用于对两个$N$元素向量（操作数1和2）进行元素级加法，每个元素为4位整数。向量元素首先以转置的内存布局存储，其中第一个向量的每个元素存储在4行（$i$到$i+3$）的不同列中，第二个向量的元素则存储在相同列中的4行（$j$到$j+3$）。在一个周期内，$i$行和$j$行被读取，分别在双端口SRAM阵列的每个端口上。每个PE接收两个比特（一个来自$i$行，一个来自$j$行），计算这两个比特的和以及前一个周期的进位。进位输出被存储在PE中的进位触发器中，和被写入到行$k$中。这个过程重复4个周期，同时递增读取和写入的行地址。在第五个周期，存储在每个PE中的最后进位位通过第二个写入端口写入行$k+4$，元素级加法操作的最终结果向量现在可以在行$k$到$k+4$中获得。更复杂的操作，如乘法或归约，可以作为加法和内存复制的序列进行。
- en: There are several academic proposals to enhance FPGAs with in-BRAM compute capabilities;
    they make different design choices for the compute paradigm used (bit-serial vs.
    bit-parallel), supported operations in the added PEs, how to store data and intermediate
    results, and how to program/control the BRAMs to execute a sequence of operations [[106](#bib.bib106),
    [107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110)].
    The work by Wang et al. [[106](#bib.bib106)] was the first to propose adding compute
    capabilities similar to that demonstrated for CPU caches [[111](#bib.bib111)]
    to FPGA BRAMs. Their compute-capable BRAM (CCB) used a bit-serial addition PE;
    however, it uses only one port by activating two wordlines simultaneously to perform
    an analog AND operation on the bitlines. This makes the PE slightly cheaper and
    frees up one of the two ports of the BRAM, enabling overlap of data loading and
    computation. However, this technique is less robust, more sensitive to process
    variations, and requires lowering the wordline voltage (and therefore the operating
    frequency) to avoid corruption of the cell contents. A DL accelerator designed
    for CCB achieves 1.25$\times$ and 3$\times$ higher performance compared to the
    Microsoft Brainwave accelerator [[36](#bib.bib36)] for int8 and bfp8 precisions
    across RNN, GRU, and LSTM workloads, at the cost of only 1.8% increase in the
    FPGA die area. Like CCB, Compute RAM [[107](#bib.bib107)] performs analog AND
    operations on the bitlines and uses bit serial processing elements for addition,
    but introduces a small secondary memory array to store instructions inside the
    BRAM block.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个学术提案旨在增强 FPGAs 的内 BRAM 计算能力；它们对计算范式（位串与位并行）、附加 PE 中支持的操作、如何存储数据和中间结果、以及如何编程/控制
    BRAM 执行操作序列做出了不同的设计选择 [[106](#bib.bib106), [107](#bib.bib107), [108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110)]。Wang 等人的工作 [[106](#bib.bib106)] 是第一个提出将类似于
    CPU 缓存 [[111](#bib.bib111)] 的计算能力添加到 FPGA BRAM 的提案。他们的计算能力 BRAM (CCB) 使用了位串加法
    PE；然而，它通过同时激活两个字线来执行位线上的模拟 AND 操作，只使用一个端口。这使得 PE 略微便宜，并释放了 BRAM 的两个端口之一，从而实现数据加载和计算的重叠。然而，这种技术鲁棒性较差，对工艺变化更敏感，并且需要降低字线电压（因此降低操作频率）以避免单元内容的损坏。针对
    CCB 设计的 DL 加速器在 int8 和 bfp8 精度下，在 RNN、GRU 和 LSTM 工作负载中，相较于 Microsoft Brainwave
    加速器 [[36](#bib.bib36)] 实现了 1.25$\times$ 和 3$\times$ 的性能提升， FPGA 晶圆面积仅增加 1.8%。与
    CCB 类似，Compute RAM [[107](#bib.bib107)] 在位线上的操作执行模拟 AND 操作，并使用位串处理元件进行加法，但在 BRAM
    块内引入了一个小的次级存储器阵列来存储指令。
- en: 'CoMeFa [[108](#bib.bib108)] improves robustness over CCB by avoiding an analog
    AND on the bitlines; instead it exploits the dual-port nature of FPGA BRAMs to
    obtain two operands and uses the bit-serial addition PE from Fig. [16(a)](#S5.F16.sf1
    "In Figure 16 ‣ V-C BRAMs ‣ V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions"). This
    technique also can achieve higher operating speeds, but it comes at the cost of
    using both BRAM ports during compute, and hence it cannot overlap loading and
    compute. The CoMeFa architecture has both area- and delay-optimized variants;
    the delay-optimized version increases FPGA die area by 3.8% and achieves a 2.5$\times$
    performance improvement across a variety of DL workloads on a Microsoft-Brainwave-like
    accelerator architecture that uses in-BRAM compute.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'CoMeFa [[108](#bib.bib108)]通过避免在位线上的模拟 AND 操作来提高了对比 CCB 的鲁棒性；它利用 FPGA BRAM
    的双端口特性来获取两个操作数，并使用图 [16(a)](#S5.F16.sf1 "In Figure 16 ‣ V-C BRAMs ‣ V Enhancing
    Existing FPGA Fabric Blocks ‣ Field-Programmable Gate Array Architecture for Deep
    Learning: Survey & Future Directions") 中的位串加法 PE。这个技术也可以实现更高的操作速度，但代价是计算过程中需要使用两个
    BRAM 端口，因此无法重叠加载和计算。CoMeFa 架构有面积优化和延迟优化两种变体；延迟优化版本使 FPGA 晶圆面积增加 3.8%，并在使用内 BRAM
    计算的 Microsoft-Brainwave 类似加速器架构上，在各种 DL 工作负载中实现了 2.5$\times$ 的性能提升。'
- en: 'Both CCB and CoMeFa followed the same bit-serial compute paradigm where the
    operands are laid out in memory in a transposed format. In DL applications, one
    set of operands (the model weights) are fixed and therefore can be transposed
    offline and stored in the BRAMs. However, [[108](#bib.bib108)] shows that implementing
    a data transformation unit to transpose the other set of operands (model activations)
    at runtime uses a significant amount of soft logic resources. Chen et al. [[109](#bib.bib109)]
    instead proposed a compute-in-BRAM architecture for multiply accumulate (BRAMAC)
    that uses a mix of bit-serial and bit-parallel computations to reduce latency
    and enable the use of non-transposed activation values. The PEs in BRAMAC are
    variable-precision adders that can take inputs from groups of bitlines to perform
    bit-parallel addition; multiplications are then implemented by serially accumulating
    addition results. This significantly reduces the compute latency compared to the
    purely bit-serial approach: from $O(m^{2})$ to $O(m)$ cycles for $m$-bit operands.
    However, it limits the possible numerical precisions to a pre-defined set supported
    by the architecture whereas the bit-serial approach can implement any precision.
    BRAMAC also added a smaller secondary SRAM memory array with only a few wordlines
    inside the BRAM block. In the compute mode, the operands are first copied internally
    (two 40-bit data words per cycle) to this secondary array, where the computations
    are performed. This increases the BRAM compute mode frequency as it is faster
    to charge/discharge the much shorter bitlines of the secondary array; it also
    frees up both ports of the main memory array to be used for normal read/write
    operations while computations are performed in the secondary memory array. Different
    variations of the BRAMAC architecture show performance improvements ranging from
    1.3$\times$ to 2$\times$ for different CNNs running on an accelerator similar
    to Intel’s DLA [[62](#bib.bib62)] at the cost of a 3.4-6.8% increase in FPGA die
    area. M4BRAM [[110](#bib.bib110)] augments the BRAMAC PE by adding duplication/shuffling
    logic to enable more efficient data reuse and adding support for mixed-precision
    operations in which the weights and activations have different bitwidths. These
    enhancements improve the performance by 1.4$\times$ on average compared to BRAMAC.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: CCB 和 CoMeFa 都遵循了相同的位串计算范式，其中操作数以转置格式在内存中布局。在深度学习应用中，一组操作数（模型权重）是固定的，因此可以离线转置并存储在
    BRAM 中。然而，[[108](#bib.bib108)] 显示，实施一个数据转换单元以在运行时转置另一组操作数（模型激活）会使用大量的软逻辑资源。陈等人
    [[109](#bib.bib109)] 则提出了一种用于乘加运算的 BRAM 计算架构（BRAMAC），该架构结合了位串和位并行计算，以减少延迟并允许使用未转置的激活值。BRAMAC
    中的处理单元（PEs）是可变精度的加法器，可以从位线组中接收输入以进行位并行加法；乘法则通过串行累加加法结果来实现。这大大减少了计算延迟，相较于纯位串方法，从
    $O(m^{2})$ 降低到 $O(m)$ 周期，其中 $m$ 为位操作数。然而，这限制了可能的数值精度到架构支持的预定义集合，而位串方法可以实现任何精度。BRAMAC
    还在 BRAM 块内添加了一个较小的次级 SRAM 内存阵列，内部仅有少量的字线。在计算模式下，操作数首先被内部复制（每个周期两个 40 位数据字）到该次级阵列中，然后在该阵列中进行计算。这提高了
    BRAM 计算模式的频率，因为充电/放电次级阵列中较短的位线更快；同时，它还释放了主内存阵列的两个端口用于正常的读写操作，而计算则在次级内存阵列中进行。BRAMAC
    架构的不同变体在类似于 Intel DLA 的加速器上运行不同 CNN 时表现出 1.3$\times$ 到 2$\times$ 的性能提升，但 FPGA
    晶片面积增加了 3.4% 到 6.8%。M4BRAM [[110](#bib.bib110)] 通过添加重复/重排逻辑来增强 BRAMAC PE，以实现更高效的数据重用，并支持混合精度操作，其中权重和激活具有不同的位宽。这些改进使性能比
    BRAMAC 平均提高了 1.4$\times$。
- en: VI In-Fabric Tensor Blocks
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 内嵌张量块
- en: '![Refer to caption](img/aca511d0e1b741d63bca237e6ffd4261.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aca511d0e1b741d63bca237e6ffd4261.png)'
- en: 'Figure 17: In-fabric 2D systolic tensor block consisting of 16 PEs that can
    operate collectively in tensor mode or independently in scalar mode as configured
    by the multiplexing logic. Each tensor block is 3.5$\times$ wider than a LB and
    spans 8 rows in the FPGA grid.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：内嵌 2D 衰减张量块，由 16 个 PEs 组成，可以在张量模式下集体操作，或根据多路复用逻辑独立操作于标量模式。每个张量块的宽度是 LB
    的 3.5$\times$，并且跨越 FPGA 网格的 8 行。
- en: 'Another line of work has investigated the integration of new hard blocks for
    tensor computations in the FPGA fabric to enhance DL inference efficiency. Arora
    et al. [[112](#bib.bib112), [113](#bib.bib113)] proposed adding 2D systolic tensor
    blocks to the FPGA fabric, as illustrated in Fig. [17](#S6.F17 "Figure 17 ‣ VI
    In-Fabric Tensor Blocks ‣ Field-Programmable Gate Array Architecture for Deep
    Learning: Survey & Future Directions"); these tensor blocks are in addition to
    (rather than a replacement for) the traditional DSP blocks in the fabric. These
    blocks contain 16 PEs, input logic for preparing data to be consumed by the PEs
    (e.g. delay registers for staggering inputs in 2D systolic processing), output
    logic for marshalling output data from different PEs, and multiplexing logic for
    configuring the block to operate in different modes. The multiplexing logic allows
    the tensor block to operate in tensor or scalar modes. In tensor mode, all the
    PEs are collectively calculating a matrix-matrix multiplication, matrix-vector
    multiplication, or matrix-matrix elementwise addition/subtraction/multiplication.
    In scalar mode, each PE is calculating an independent multiply or MAC operation.
    The mode of operation can be dynamically changed at runtime by appropriately setting
    control inputs to the block. Each PE in the tensor block can implement 1$\times$
    int16, fp16, or 16-bit Brain floating-point (bfloat16) [[114](#bib.bib114)] MAC,
    and it can also be fractured to implement 4$\times$ int8 MACs.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '另一项工作研究了在FPGA结构中集成新的硬件模块以增强深度学习推理效率。Arora 等人[[112](#bib.bib112), [113](#bib.bib113)]
    提议在FPGA结构中添加二维收缩张量块，如图[17](#S6.F17 "Figure 17 ‣ VI In-Fabric Tensor Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions")所示；这些张量块是传统DSP块的补充，而非替代。这些块包含16个PEs，为PEs准备数据的输入逻辑（例如用于在二维收缩处理中的输入延迟寄存器）、从不同PEs汇集输出数据的输出逻辑，以及用于配置块以不同模式操作的多路复用逻辑。多路复用逻辑允许张量块在张量模式或标量模式下操作。在张量模式下，所有PEs共同计算矩阵-矩阵乘法、矩阵-向量乘法或矩阵-矩阵逐元素加法/减法/乘法。在标量模式下，每个PE计算一个独立的乘法或MAC操作。操作模式可以通过适当设置控制输入在运行时动态改变。张量块中的每个PE可以实现1$\times$
    int16、fp16或16位Brain浮点数（bfloat16）[[114](#bib.bib114)] MAC，也可以分裂实现4$\times$ int8
    MACs。'
- en: This tensor block has a 4.4$\times$ higher area footprint than an Intel Agilex-like
    DSP block, with 2.4$\times$ and 4$\times$ more input and output pins interfacing
    with the programmable routing, respectively. To accommodate their higher area
    and increased signal demand, these blocks occupy multiple locations in the *grid*
    defined by the FPGA routing channels. Hence they can connect to multiple routing
    channels; a single block spans 8 rows of the FPGA grid. A tensor block column
    is also physically 3.5$\times$ wider than an LB column. On a set of 9 DL benchmarks,
    the addition of these in-fabric tensor blocks increased the maximum operating
    frequency by 65% and decreased the routed wirelength by 55% on average. A large
    number of MAC operations and the interconnect between them can be mapped to the
    PEs of a single tensor block, leading to these speed and wirelength gains over
    distributed LBs and DSPs connected together using the programmable routing. For
    non-DL benchmarks the tensor blocks not only remain idle but also, due to their
    coarse granularity, force other circuit components to be placed physically further
    away from each other with longer connections between them. This results in a 0.5-2.5%
    degradation in frequency and a 2-8% increase in routed wirelength as the portion
    of die area dedicated for tensor blocks is varied from 5% to 30%.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该张量块的面积占用比类似Intel Agilex的DSP块高4.4$\times$，与可编程路由接口的输入和输出引脚数量分别多2.4$\times$和4$\times$。为了适应其更高的面积和增加的信号需求，这些块占据了FPGA路由通道定义的*网格*中的多个位置。因此，它们可以连接到多个路由通道；单个块横跨FPGA网格的8行。张量块列的物理宽度也比LB列宽3.5$\times$。在一组9个深度学习基准测试中，添加这些结构内张量块使最大操作频率提高了65%，并且平均减少了55%的路由线长。大量的MAC操作和它们之间的互连可以映射到单个张量块的PEs上，从而在速度和线长方面相对于通过可编程路由连接的分布式LBs和DSPs获得了这些提升。对于非深度学习基准测试，张量块不仅保持闲置，而且由于其粗粒度，迫使其他电路组件物理上相互远离，连接更长。这导致频率下降0.5-2.5%，以及随着张量块所占芯片面积从5%增加到30%，路由线长增加2-8%。
- en: '![Refer to caption](img/18745baaec0705a0e7f4533b9e8202eb.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/18745baaec0705a0e7f4533b9e8202eb.png)'
- en: 'Figure 18: The Achronix Speedster7t machine learning processor block (MLPB)
    internal architecture. Tightly coupling BRAMs and register files with MAC arrays
    restricts the high-bandwidth data transfers internally in the MLPB and limits
    the number of external interfaces to the programmable routing.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：Achronix Speedster7t 机器学习处理器块（MLPB）的内部架构。紧密耦合的 BRAM 和寄存器文件与 MAC 数组限制了 MLPB
    内部的高带宽数据传输，并限制了对可编程布线的外部接口数量。
- en: 'As the market for FPGA-based DL acceleration continues to grow rapidly, several
    FPGA vendors have also started to offer DL-optimized FPGA families that integrate
    different forms of in-fabric tensor blocks. These devices sacrifice backward compatibility
    by entirely replacing the wireless-communication-targeted conventional DSP blocks
    with new tensor blocks optimized specifically for the compute patterns and numerical
    precisions of DL workloads. The Achronix Speedster7t FPGA [[115](#bib.bib115)]
    integrates machine learning processor blocks (MLPBs²²2Although Achronix abbreviates
    their machine learning processor blocks as MLPs, we use MLPBs to avoid confusion
    with MLPs for multi-layer perceptron models.), that tightly couple BRAMs and MAC
    units with dedicated high-bandwidth routing between them, as shown in Fig. [18](#S6.F18
    "Figure 18 ‣ VI In-Fabric Tensor Blocks ‣ Field-Programmable Gate Array Architecture
    for Deep Learning: Survey & Future Directions"). This tight coupling reduces the
    number of expensive interfaces to the programmable routing needed to feed the
    compute units inside the block. New weight and/or activation data can be written
    to a double-buffered internal BRAM using relatively narrower external interfaces,
    while another set of weights and/or activations is reused for many compute operations
    with wide internal dedicated connections between the BRAM and compute units.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于 FPGA 的 DL 加速市场的快速增长，几家 FPGA 供应商也开始提供集成不同形式的内嵌张量块的 DL 优化 FPGA 系列。这些设备通过完全用针对
    DL 工作负载的计算模式和数值精度优化的新张量块替换了针对无线通信的传统 DSP 块，从而牺牲了向后兼容性。Achronix Speedster7t FPGA
    [[115](#bib.bib115)] 集成了机器学习处理器块（MLPBs²²2虽然 Achronix 将其机器学习处理器块缩写为 MLPs，但我们使用
    MLPBs 以避免与多层感知机模型的 MLPs 混淆），这些块紧密耦合 BRAM 和 MAC 单元，并在它们之间提供专用的高带宽布线，如图 [18](#S6.F18
    "图 18 ‣ VI 内嵌张量块 ‣ 用于深度学习的现场可编程门阵列架构：调查与未来方向") 所示。这种紧密耦合减少了供给块内计算单元所需的昂贵的可编程布线接口数量。新的权重和/或激活数据可以通过相对较窄的外部接口写入双缓冲的内部
    BRAM，而另一组权重和/或激活则通过 BRAM 和计算单元之间的宽内部专用连接被重复用于许多计算操作。
- en: Another key benefit of this tight coupling of BRAMs and MAC units is that it
    enables these hard MLPBs to operate on a higher frequency clock domain (up to
    750MHz) than the rest of the design implemented in soft logic, without the need
    to use the (slower and less efficient) fine-grained programmable routing for transporting
    data between memory and compute as in conventional FPGA fabrics. These MLPBs also
    natively support a wide variety of numerical precisions suitable for DL training
    and inference such as int4/8/16, bfp12/16, bfloat16, and fp16/24. The largest
    Speedster7t devices include 2,560 MLPBs that can provide up to 61.4 and 122.8
    TOPS of int8/bfp16 and int4/bfp12 performance, respectively. Cairncross et al. [[116](#bib.bib116)]
    demonstrated the use of the Speedster7t MLPBs to implement a 4-core FPGA DL overlay
    for low-latency inference use cases. The overlay can clock the MLPBs at 560MHz,
    achieving a peak int8 performance of 36.4 TOPS with 80-100% utilization of the
    compute units across a variety of GEMV, MLP, and RNN workloads at a batch size
    of 4.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: BRAM 和 MAC 单元的紧密耦合的另一个关键好处是，它使得这些硬件 MLPBs 能够在比其他软逻辑设计更高的频率时钟域（高达 750MHz）下运行，而无需使用（更慢且效率较低的）细粒度可编程布线来传输内存和计算之间的数据，这在传统
    FPGA 结构中是常见的。这些 MLPBs 还原生支持多种适用于 DL 训练和推理的数值精度，如 int4/8/16、bfp12/16、bfloat16 和
    fp16/24。最大的 Speedster7t 设备包括 2,560 个 MLPBs，分别提供高达 61.4 和 122.8 TOPS 的 int8/bfp16
    和 int4/bfp12 性能。Cairncross 等人 [[116](#bib.bib116)] 演示了使用 Speedster7t MLPBs 实现
    4 核 FPGA DL 覆盖的低延迟推理用例。该覆盖可以将 MLPBs 时钟频率设定为 560MHz，在各种 GEMV、MLP 和 RNN 工作负载的批量大小为
    4 的情况下，达到 36.4 TOPS 的峰值 int8 性能，并使计算单元的利用率达到 80-100%。
- en: '![Refer to caption](img/17c20f595f1238f3c3768c6958b2cb12.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/17c20f595f1238f3c3768c6958b2cb12.png)'
- en: 'Figure 19: Different int8 modes of operation of the Intel Stratix 10 NX AI
    tensor block: scalar mode with 3 independent MACs (top), vector mode with one
    dot-6 operation without input restrictions (middle), and tensor mode with three
    dot-10 operations using input broadcast and input reuse register chains (bottom).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：Intel Stratix 10 NX AI 张量块的不同 int8 操作模式：标量模式，具有 3 个独立的 MAC（顶部），向量模式，具有一个没有输入限制的
    dot-6 操作（中间），以及张量模式，具有三个使用输入广播和输入重用寄存器链的 dot-10 操作（底部）。
- en: 'The artificial intelligence tensor blocks (AITBs) in the Intel Stratix 10 NX
    device [[117](#bib.bib117)] are another example of commercial in-fabric tensor
    compute. Although the end goal is the same (to integrate in-fabric tensor compute
    for DL), Intel adopted a different design approach from the academic tensor blocks
    and the Achronix MLPBs. The AITBs were designed as a drop-in replacement for conventional
    Stratix 10 DSP blocks in terms of silicon area footprint and interfaces to the
    programmable routing (i.e. only the block internals are different). A single AITB
    has enough silicon footprint to implement up to 30 int8 or 60 int4 multipliers.
    However, this would require 480 input and 480 output interfaces to the programmable
    routing, which is much higher (and would be much larger) than the 96 inputs and
    72 outputs in the conventional Stratix 10 DSP block. Most DL workloads are dominated
    by operations where the results of many multiplies are accumulated and there is
    re-use of input data. Intel exploits this by designing three different AITB modes
    (shown in Fig. [19](#S6.F19 "Figure 19 ‣ VI In-Fabric Tensor Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions")) that
    enable different levels of arithmetic density while staying within the 96 input
    / 72 output limit; more dense modes support increasingly constrained (but useful
    in DL) compute patterns.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 'Intel Stratix 10 NX 设备中的人工智能张量块（AITBs）[[117](#bib.bib117)] 是商业化集成张量计算的另一个例子。尽管最终目标相同（将集成张量计算用于深度学习），但
    Intel 采用了不同于学术张量块和 Achronix MLPBs 的设计方法。AITBs 设计为可以替代传统 Stratix 10 DSP 块的组件，其硅面积足迹和与可编程路由的接口相同（即只有块内部不同）。单个
    AITB 具有足够的硅面积来实现多达 30 个 int8 或 60 个 int4 乘法器。然而，这将需要 480 个输入和 480 个输出接口与可编程路由连接，这比传统
    Stratix 10 DSP 块的 96 个输入和 72 个输出接口要高得多（也会大得多）。大多数深度学习工作负载都以许多乘法结果的累加为主，并且有输入数据的重用。Intel
    通过设计三种不同的 AITB 模式（如图 [19](#S6.F19 "Figure 19 ‣ VI In-Fabric Tensor Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions")）来利用这一点，这些模式在保持
    96 个输入 / 72 个输出限制的同时，支持不同级别的算术密度；更密集的模式支持越来越受限（但在深度学习中有用）的计算模式。'
- en: In *scalar mode*, the AITB performs completely independent multiplies. This
    mode is easy to use, but compute density is limited by the number of outputs to
    the general programmable routing; the AITB can perform only three independent
    int8 MAC operations with a 24-bit accumulator each (i.e. a total of 72 outputs).
    The *vector mode* internally sums its multiplies to produce one output, making
    it well suited for dot products. In this case, the AITB is limited by the number
    of inputs and can perform six int8 multiplies in a dot-6 operation (i.e. 2 operands
    $\times$ 6 elements $\times$ 8 bits = 96 external inputs). Finally, the *tensor
    mode* provides the highest arithmetic density, but with more restrictions on the
    inputs and outputs of the AITB. To limit outputs to 72, it performs three int8
    dot-10 operations, each of which has an accumulator and dedicated interconnects
    to reduce results across AITBs in the same column. To limit inputs to 96, an input
    vector is broadcast to all 3 dot product units while three other input vectors
    (one per dot product unit) are fed locally by ping pong input reuse register chains.
    While inputs are reused for some time in many DL computations (e.g. by computing
    multiple output feature maps in a CNN from the same input maps), they must eventually
    be reloaded to proceed to the next set of inputs. This means that the AITB needs
    to either stall computations and load the reuse registers in parallel, or use
    the first block in a group of cascaded AITBs to sequentially load inputs to one
    of the reuse register chains using the dedicated AITB-to-AITB interconnect while
    the other chain is used for computation. Additional lightweight circuitry is also
    added to the AITB to reuse the int8 and int4 multipliers for natively supporting
    bfp16 and bfp12 precisions, respectively.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在*标量模式*下，AITB 执行完全独立的乘法运算。这个模式易于使用，但计算密度受限于通用可编程路由的输出数量；AITB 只能执行三个独立的 int8
    MAC 操作，每个操作具有 24 位累加器（即总共 72 个输出）。*向量模式*将其乘法结果内部求和以生成一个输出，使其非常适合点积运算。在这种情况下，AITB
    的限制在于输入数量，并且可以在点-6 操作中执行六个 int8 乘法（即 2 个操作数 $\times$ 6 个元素 $\times$ 8 位 = 96 个外部输入）。最后，*张量模式*提供了最高的算术密度，但对
    AITB 的输入和输出有更多限制。为了将输出限制为 72，它执行三个 int8 点-10 操作，每个操作都有一个累加器和专用的互连以减少同一列 AITB 之间的结果。为了将输入限制为
    96，一个输入向量被广播到所有 3 个点积单元，而其他三个输入向量（每个点积单元一个）通过乒乓输入重用寄存器链局部输入。尽管在许多深度学习计算中输入被重用一段时间（例如，通过计算来自相同输入图的多个输出特征图），它们最终必须重新加载以继续处理下一组输入。这意味着
    AITB 需要要么暂停计算并并行加载重用寄存器，要么使用级联 AITB 组中的第一个块通过专用的 AITB 到 AITB 互连顺序加载一个重用寄存器链的输入，同时其他链用于计算。此外，AITB
    还增加了额外的轻量电路，以重用 int8 和 int4 乘法器，分别支持原生的 bfp16 和 bfp12 精度。
- en: '![Refer to caption](img/568639442eb280bc5b1434fc6827bf52.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/568639442eb280bc5b1434fc6827bf52.png)'
- en: 'Figure 20: Mapping of different convolution operations in HPIPE to different
    modes of the Intel Stratix 10 NX AITB modes of operation.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：HPIPE 中不同卷积操作映射到 Intel Stratix 10 NX AITB 操作模式的不同模式。
- en: 'As discussed in Sec. [II-D3](#S2.SS4.SSS3 "II-D3 Effect of FPGA Architecture
    Enhancements for DL ‣ II-D Examples of DL Acceleration on FPGAs ‣ II FPGA for
    DL Acceleration ‣ Field-Programmable Gate Array Architecture for Deep Learning:
    Survey & Future Directions"), both the HPIPE [[72](#bib.bib72)] and NPU [[24](#bib.bib24)]
    accelerators have been re-architected to make best use of the Intel Stratix 10
    NX AITBs. In HPIPE, all 3 modes of operation of the AITBs are used for different
    CNN operations as illustrated in Fig. [20](#S6.F20 "Figure 20 ‣ VI In-Fabric Tensor
    Blocks ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey
    & Future Directions"). To exploit unstructured sparsity, HPIPE builds a multiplexing
    network in the soft logic to gather the activations matching non-zero weights.
    This maps well to the AITB vector mode due to its input flexibility (dot-6 operations
    with arbitrary inputs each cycle), and enables a 1.9$\times$ overall inference
    speedup compared to conventional DSP blocks. When running dense regular and pointwise
    convolutions, HPIPE can exploit the high arithmetic density of the tensor mode
    by pre-loading activations to the reuse register chains and broadcasting weights
    to all 3 dot units in the AITB. However, the scalar mode remains necessary for
    implementing the depthwise convolutions as they do not have reduction or data
    reuse across the input channel dimension. Combining tensor and scalar modes speeds
    up HPIPE dense CNN inference by 5$\times$ compared to using conventional DSP blocks.
    The NPU can also exploit the tensor mode of the AITBs, but in its case it is necessary
    to increase the batch size from 1 to 3\. Activations from a batch of 3 different
    inputs are pre-loaded to the reuse register chains while weights are broadcast
    to the 3 dot product units. This results in 3.5$\times$ higher throughput than
    the baseline NPU using DSP blocks.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[II-D3](#S2.SS4.SSS3 "II-D3 FPGA架构增强对深度学习的影响 ‣ II-D 深度学习在FPGA上的加速示例 ‣ II 深度学习加速的FPGA
    ‣ 用于深度学习的现场可编程门阵列架构：综述与未来方向")节所述，HPIPE [[72](#bib.bib72)]和NPU [[24](#bib.bib24)]加速器都经过了重新设计，以充分利用Intel
    Stratix 10 NX AITBs。在HPIPE中，AITBs的所有3种操作模式都用于不同的CNN操作，如图[20](#S6.F20 "图20 ‣ VI
    现场张量块 ‣ 用于深度学习的现场可编程门阵列架构：综述与未来方向")所示。为了利用非结构稀疏性，HPIPE在软逻辑中建立了一个多路复用网络，以收集与非零权重匹配的激活。这与AITB的向量模式非常匹配，因为其输入灵活性（每个周期具有任意输入的点-6操作），并使整体推理速度比传统DSP块提高1.9$\times$。在运行密集的常规和逐点卷积时，HPIPE可以通过将激活预加载到重用寄存器链中，并将权重广播到AITB中的所有3个点单元来利用张量模式的高算术密度。然而，标量模式仍然是实现深度卷积所必需的，因为深度卷积在输入通道维度上没有缩减或数据重用。结合张量模式和标量模式，使HPIPE的密集CNN推理速度比使用传统DSP块快5$\times$。NPU也可以利用AITBs的张量模式，但在这种情况下需要将批量大小从1增加到3。来自3个不同输入的批量激活被预加载到重用寄存器链中，而权重则广播到3个点积单元。这使得吞吐量比基线NPU使用DSP块高出3.5$\times$。
- en: These performance gains all come with no increase in the FPGA die size, since
    the AITBs have the exact same area footprint and programmable routing interfaces
    as the DSP blocks they replace. While the gains are significant, they fall short
    of the 15$\times$ increase in peak int8 TOPS compared to DSP blocks. The peak
    performance can only be achieved if all operations match the compute pattern of
    the AITB tensor mode, all vector operands are a multiple of 10 elements to exactly
    fit the dot product units, and there is no overhead for loading data to the input
    reuse chains. One or more of these 3 requirements for ideal efficiency are not
    met in most application designs. In addition, efficiently using the AITBs requires
    considerable changes to a design that originally targeted conventional DSP blocks;
    one cannot simply re-compile an RTL or HLS design to target these new AITBs. The
    design computations must first be restructured to match one of the compute patterns
    supported by the different AITB modes, and then AITBs are instantiated as *black-box*
    IPs in RTL to implement these computations. The resulting optimized designs are
    less portable between different FPGA families.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这些性能提升都没有增加 FPGA 晶片的尺寸，因为 AITB 的面积占用和可编程路由接口与它们替代的 DSP 模块完全相同。尽管增益显著，但相较于 DSP
    模块，峰值 int8 TOPS 的提升仍未达到 15$\times$。只有在所有操作都符合 AITB 张量模式的计算模式、所有向量操作数都是 10 元素的倍数以完全适配点积单元，并且没有数据加载到输入复用链的开销时，才能实现峰值性能。在大多数应用设计中，理想效率的这三项要求中的一项或多项未被满足。此外，高效使用
    AITB 需要对原本针对传统 DSP 模块的设计进行大量更改；不能简单地重新编译 RTL 或 HLS 设计以针对这些新的 AITB。设计计算必须首先重构以匹配不同
    AITB 模式所支持的计算模式之一，然后在 RTL 中将 AITB 实例化为 *黑箱* IP 以实现这些计算。优化后的设计在不同 FPGA 系列之间的可移植性较差。
- en: Moving forward, the DSP blocks in the upcoming Intel Agilex 5 FPGA family will
    support modes from conventional DSP blocks (6$\times$int9, 2$\times$int18, 1$\times$int27),
    as well as a variation of the AITB tensor mode with only two (instead of three)
    int8 dot-10 operations per block [[118](#bib.bib118)]. Both the NPU and HPIPE
    require not only many low precision MAC operations but a few higher precision
    ones, so they will benefit from the ability to do both efficiently in one block.
    These hybrid blocks are targeted at edge DL applications in which the FPGA implements
    a full system where inference is a component along side other signal processing
    functionalities.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 向前发展，未来的英特尔 Agilex 5 FPGA 系列中的 DSP 模块将支持传统 DSP 模块的模式（6$\times$int9, 2$\times$int18,
    1$\times$int27），以及 AITB 张量模式的一种变体，每个模块仅有两个（而不是三个）int8 dot-10 操作[[118](#bib.bib118)]。NPU
    和 HPIPE 不仅需要许多低精度的 MAC 操作，还需要一些高精度操作，因此它们将受益于在一个模块中高效地同时处理两者。这些混合模块专门用于边缘深度学习应用，其中
    FPGA 实现了一个完整的系统，推理是其他信号处理功能的一个组成部分。
- en: VII Beyond the FPGA Fabric
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII FPGA 结构之外
- en: '![Refer to caption](img/6cac6940ce91cb8187acb8f6d9fcd9cf.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6cac6940ce91cb8187acb8f6d9fcd9cf.png)'
- en: 'Figure 21: The AMD Versal architecture combining an FPGA fabric, general-purpose
    processors, AI engines, and a packet-switched NoC with a modified mesh topology.
    The AI engines are arranged in a 2D grid with dedicated interconnect cascading
    their accumulators and an AXI packet/circuit-switched bus-based interconnect.
    Each AI engine is a VLIW vector processor with a peak throughput of 256 int8 GOPS.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：AMD Versal 架构结合了 FPGA 结构、通用处理器、AI 引擎和具有修改过的网格拓扑的分组交换 NoC。AI 引擎被排列成 2D 网格，具有专用的互连级联它们的累加器和基于
    AXI 数据包/电路交换总线的互连。每个 AI 引擎是一个 VLIW 向量处理器，峰值吞吐量为 256 int8 GOPS。
- en: 'Sections [V](#S5 "V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions") and [VI](#S6
    "VI In-Fabric Tensor Blocks ‣ Field-Programmable Gate Array Architecture for Deep
    Learning: Survey & Future Directions") described DL-targeted enhancements to existing
    FPGA fabric components as well as the embedding of new tensor compute blocks in
    the fabric. Beyond these improvements to the *fine-grained* programmable fabric,
    several other architecture enhancements have been proposed to significantly increase
    peak performance by integrating coarse-grained accelerator cores either on the
    same monolithic die or in package using advanced chip integration technologies.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '第[V](#S5 "V Enhancing Existing FPGA Fabric Blocks ‣ Field-Programmable Gate
    Array Architecture for Deep Learning: Survey & Future Directions")节和[VI](#S6 "VI
    In-Fabric Tensor Blocks ‣ Field-Programmable Gate Array Architecture for Deep
    Learning: Survey & Future Directions")节描述了针对深度学习的现有FPGA织物组件的增强以及在织物中嵌入新的张量计算块。除了对*细粒度*可编程织物的这些改进外，还提出了几种其他架构改进，通过在相同的单片芯片上或使用先进的芯片集成技术在封装中集成粗粒度加速器核心，显著提高峰值性能。'
- en: VII-A Reconfigurable Acceleration Devices
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 可重构加速设备
- en: Recently, a new class of reconfigurable acceleration devices (RADs) [[119](#bib.bib119)]
    has emerged that combine the reconfigurability of conventional FPGA fabrics with
    the efficiency of coarse-grained application-specific accelerators and the flexibility
    of general-purpose processor cores. These components are all connected via high-performance
    packet-switched networks-on-chip (NoCs) for system-wide communication.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了一种新型的可重构加速设备（RADs）[[119](#bib.bib119)]，它们将传统FPGA织物的可重构性与粗粒度应用特定加速器的高效性以及通用处理器核心的灵活性相结合。这些组件通过高性能的分组交换网络（NoCs）进行系统级通信。
- en: 'One example of such a RAD is the AMD Versal architecture which tightly integrates
    a 7nm FPGA programmable fabric, general-purpose Arm Cortex cores, and a 2D array
    of specialized vector processors termed adaptive intelligent engines (AIEs) on
    the same monolithic die [[85](#bib.bib85)]. These different system components
    as well as the modules on the FPGA fabric communicate using a hard packet-switched
    NoC. The NoC is also the only way to access external memories (e.g. DDR or HBM).
    The Versal NoC [[86](#bib.bib86)] has a modified mesh topology were several columns
    are grouped together and rows are squished to the top and bottom of the device
    as illustrated in Fig. [21](#S7.F21 "Figure 21 ‣ VII Beyond the FPGA Fabric ‣
    Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions"). This topology matches the columnar nature of the FPGA fabric, simplifies
    the layout of the chip, and also provides higher bandwidth for horizontal communication
    at the top and bottom of the device where the high-speed IOs, memory controllers,
    and the AIE array are located. The presence of a hard NoC significantly boosts
    FPGA designer productivity and facilitates timing closure; it is no longer necessary
    to go through many design iterations to optimize the system-level interconnect
    built using the (relatively less efficient) programmable routing resources [[120](#bib.bib120)].
    Different modules implemented on the programmable fabric and communicating via
    latency-insensitive interfaces can be independently and locally optimized to close
    timing as standalone components. Then the compiled modules can be connected to
    one of the pervasive NoC *fabric ports* to communicate with other fabric modules,
    coarse-grained accelerators (e.g. AIEs), and external interfaces. This can be
    extremely useful, especially for large and complex FPGA systems with many compute
    modules and high external memory bandwidth requirements such as DL acceleration
    designs.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '一个这样的 RAD 示例是 AMD Versal 架构，它在同一单片上紧密集成了一个 7nm FPGA 可编程结构、通用 Arm Cortex 核心和一个称为适应性智能引擎（AIEs）的二维阵列的专用向量处理器
    [[85](#bib.bib85)]。这些不同的系统组件以及 FPGA 结构上的模块通过硬件分组交换的 NoC 进行通信。NoC 也是访问外部内存（例如 DDR
    或 HBM）的唯一方式。Versal NoC [[86](#bib.bib86)] 具有修改后的网格拓扑，其中几个列被组合在一起，行被压缩到设备的顶部和底部，如图
    [21](#S7.F21 "Figure 21 ‣ VII Beyond the FPGA Fabric ‣ Field-Programmable Gate
    Array Architecture for Deep Learning: Survey & Future Directions") 所示。这种拓扑匹配 FPGA
    结构的列状特性，简化了芯片布局，同时为设备顶部和底部的水平通信提供了更高的带宽，这里集成了高速 IO、内存控制器和 AIE 阵列。硬件 NoC 的存在显著提高了
    FPGA 设计师的生产力并促进了时序收敛；不再需要通过多次设计迭代来优化使用（相对较低效的）可编程路由资源构建的系统级互连 [[120](#bib.bib120)]。在可编程结构上实现的不同模块通过延迟不敏感接口进行通信，可以独立并本地优化以作为独立组件完成时序。然后，编译后的模块可以连接到一个普遍的
    NoC *fabric ports*，与其他结构模块、粗粒度加速器（例如 AIEs）和外部接口进行通信。这对于具有许多计算模块和高外部内存带宽需求的大型复杂
    FPGA 系统，如 DL 加速设计，尤其有用。'
- en: 'In addition, the AIEs significantly enhance the compute capabilities of the
    Versal architecture by combining an array of vector processors with FPGA-like
    spatial interconnect and distributed state in a hybrid computational paradigm.
    Each AIE tile contains a 1 GHz very-long-instruction-word (VLIW) vector processor
    that can execute 7 operations simultaneously (2 vector loads, 1 vector store,
    1 vector operation, and 2 scalar operations). The fixed-point vector unit in a
    single AIE is capable of performing 128 int8 MAC operations per cycle for a peak
    throughput of 256 GOPS. The vector processor is tightly coupled with 32KB of local
    SRAM memory and a direct memory access engine for non-neighbor data communication.
    As illustrated in Fig. [21](#S7.F21 "Figure 21 ‣ VII Beyond the FPGA Fabric ‣
    Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions"), the AIE tiles are arranged in a 2D grid with an AXI-Stream interconnect
    network that can implement both circuit-switched and packet-switched communication
    between remote tiles. In addition, there is a dedicated interconnect that cascades
    accumulators between neighbouring AIEs in a serpentine pattern; this interconnect
    is conceptually similar to the accumulation cascade chains between DSP blocks
    in a conventional programmable fabric. Each AIE also has the ability to directly
    read from and write to the local memory of 3 adjacent neighbours (north, south,
    and east or west depending on the physical layout of the SRAM memories as shown
    in Fig. [21](#S7.F21 "Figure 21 ‣ VII Beyond the FPGA Fabric ‣ Field-Programmable
    Gate Array Architecture for Deep Learning: Survey & Future Directions")). The
    biggest Versal device has an array of 400 AIEs that can provide more than 100
    TOPS of int8 compute, in addition to the compute units that can be implemented
    in the conventional FPGA fabric.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，AIE显著增强了Versal架构的计算能力，通过将一组向量处理器与类似FPGA的空间互连和分布式状态相结合，形成一种混合计算范式。每个AIE瓦片包含一个1
    GHz的非常长指令字（VLIW）向量处理器，能够同时执行7个操作（2个向量加载、1个向量存储、1个向量操作和2个标量操作）。单个AIE中的定点向量单元每周期能够执行128个int8
    MAC操作，峰值吞吐量为256 GOPS。向量处理器与32KB的本地SRAM内存及一个直接内存访问引擎紧密耦合，用于非邻近数据通信。如图[21](#S7.F21
    "Figure 21 ‣ VII Beyond the FPGA Fabric ‣ Field-Programmable Gate Array Architecture
    for Deep Learning: Survey & Future Directions")所示，AIE瓦片排列在一个二维网格中，具有一个AXI-Stream互连网络，可以在远程瓦片之间实现电路交换和分组交换通信。此外，还有一个专用互连，以蛇形模式级联相邻AIE之间的累加器；这个互连在概念上类似于传统可编程结构中DSP块之间的累加级联链。每个AIE还能够直接读写3个相邻邻居（北、南以及东或西，具体取决于SRAM内存的物理布局，如图[21](#S7.F21
    "Figure 21 ‣ VII Beyond the FPGA Fabric ‣ Field-Programmable Gate Array Architecture
    for Deep Learning: Survey & Future Directions")所示）。最大的Versal设备拥有400个AIE阵列，可以提供超过100
    TOPS的int8计算能力，此外还可以实现传统FPGA结构中的计算单元。'
- en: Several works have demonstrated the use of the Versal AIEs for accelerating
    different DL workloads, such as CNNs [[121](#bib.bib121), [122](#bib.bib122)],
    transformer networks [[123](#bib.bib123)], and graph neural networks [[124](#bib.bib124),
    [125](#bib.bib125)]. The use of such spatial coarse-grained cores running at a
    much higher frequency than the FPGA’s programmable fabric can significantly improve
    DL inference efficiency. However, efficiently mapping an application to a large
    number of software-programmable vector processors can be a challenging task. Since
    the AIEs introduce a new reconfigurable acceleration paradigm, the CAD tools to
    support them are not yet mature; improving the quality of results the CAD flow
    can achieve with less designer intervention is an active area of research. Several
    works have introduced frameworks that can optimize the mapping of matrix-matrix
    multiplication kernels of different sizes and compositions to an AIE array such
    as CHARM [[123](#bib.bib123)] and MaxEVA [[126](#bib.bib126)]. The Versal AIEs
    can also be used to accelerate other non-DL workloads that can benefit from their
    vector processing capabilities and spatial nature such as stencil-based scientific
    simulations [[127](#bib.bib127), [128](#bib.bib128)].
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 多项研究已经展示了使用 Versal AIE 加速不同 DL 工作负载的效果，例如 CNNs [[121](#bib.bib121), [122](#bib.bib122)]，transformer
    网络 [[123](#bib.bib123)]，以及图神经网络 [[124](#bib.bib124), [125](#bib.bib125)]。这些空间粗粒度核心运行的频率远高于
    FPGA 的可编程结构，可以显著提高 DL 推理效率。然而，将应用程序高效映射到大量软件可编程向量处理器上可能是一个具有挑战性的任务。由于 AIE 引入了一种新的可重构加速范式，支持它们的
    CAD 工具尚不成熟；提高 CAD 流程在减少设计人员干预下所能达到的结果质量是一个活跃的研究领域。多项研究提出了可以优化不同大小和组成的矩阵-矩阵乘法内核映射到
    AIE 阵列的框架，例如 CHARM [[123](#bib.bib123)] 和 MaxEVA [[126](#bib.bib126)]。Versal AIE
    还可以用于加速其他受益于其向量处理能力和空间特性的非 DL 工作负载，如基于模板的科学模拟 [[127](#bib.bib127), [128](#bib.bib128)]。
- en: '![Refer to caption](img/87b5ee42fcc025a31f4cd990f77e7c88.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/87b5ee42fcc025a31f4cd990f77e7c88.png)'
- en: 'Figure 22: Architecture exploration and evaluation flow for novel RADs.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22：新型 RAD 的架构探索和评估流程。
- en: 'The AMD Versal architecture is one specific RAD instance from the huge design
    space arising from the combination of fine-grained programmable fabrics, coarse-grained
    accelerator cores, and NoC(s) for system-level interconnect. This vast design
    space remains little explored due to the dearth of tools that can model different
    RAD architectures and enable evaluation of the complex interactions between their
    various components. The work presented in [[119](#bib.bib119), [129](#bib.bib129),
    [130](#bib.bib130)] aims to fill this gap by introducing a complete architecture
    exploration and evaluation flow for RADs, as shown in Fig. [22](#S7.F22 "Figure
    22 ‣ VII-A Reconfigurable Acceleration Devices ‣ VII Beyond the FPGA Fabric ‣
    Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions"). The first component of this flow is RAD-Sim, a cycle accurate architecture
    simulator for RADs. It provides the infrastructure for simulating RADs with different
    architecture parameters, NoC specifications, and hardened accelerator cores. A
    user can define application modules to be implemented on the RAD FPGA fabric and/or
    coarse-grained accelerator cores in SystemC, connect them to the RAD NoC, and
    simulate the entire system. RAD-Sim can then report the end-to-end application
    performance and NoC traffic congestion, as well as verify the application functionality
    on a candidate RAD architecture. This can be used to rapidly explore the design
    of both RAD architectures and applications [[129](#bib.bib129)]. These RAD devices
    introduce a new placement problem: to which physical NoC router should each accelerator
    or programmable logic module connect? Architects can either experiment with different
    placements manually, or RAD-Sim can interact with the VTR placement engine to
    automatically determine an optimized NoC placement [[131](#bib.bib131)].'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 'AMD Versal 架构是从细粒度可编程织物、粗粒度加速器核心和用于系统级互连的 NoC(s) 的组合中产生的大量设计空间中的一个具体 RAD 实例。由于缺乏能够建模不同
    RAD 架构并评估其各种组件之间复杂交互的工具，这个广阔的设计空间仍然很少被探索。文献[[119](#bib.bib119), [129](#bib.bib129),
    [130](#bib.bib130)]中提出的工作旨在填补这一空白，通过引入完整的 RAD 架构探索和评估流程，如图[22](#S7.F22 "Figure
    22 ‣ VII-A Reconfigurable Acceleration Devices ‣ VII Beyond the FPGA Fabric ‣
    Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions")所示。该流程的第一个组成部分是 RAD-Sim，一种用于 RAD 的周期精确架构模拟器。它提供了一个基础设施，用于模拟具有不同架构参数、NoC
    规格和加固加速器核心的 RAD。用户可以在 SystemC 中定义要在 RAD FPGA 织物和/或粗粒度加速器核心上实现的应用模块，将它们连接到 RAD
    NoC，并模拟整个系统。RAD-Sim 然后可以报告端到端应用性能和 NoC 流量拥堵，并验证候选 RAD 架构上的应用功能。这可以用于快速探索 RAD 架构和应用的设计[[129](#bib.bib129)]。这些
    RAD 设备引入了一个新的布局问题：每个加速器或可编程逻辑模块应该连接到哪个物理 NoC 路由器？架构师可以手动实验不同的布局，或者 RAD-Sim 可以与
    VTR 布局引擎交互，自动确定优化的 NoC 布局[[131](#bib.bib131)]。'
- en: After the design space is narrowed down to a few RAD candidates, the second
    component of this flow, RAD-Gen, can be used to evaluate their implementation
    feasibility and cost. For example, an accelerator core might significantly improve
    performance in terms of cycle count when modeled in RAD-Sim, but might not fit
    within its silicon area budget or might run at a slower frequency than assumed.
    RAD-Gen takes as inputs RTL descriptions of common RAD components (e.g. NoC routers)
    and/or candidate accelerator cores with a list of parameter values to be swept,
    as well as high-level ASIC implementation flow configurations and the design kit
    for the target process. Then, it automatically runs the ASIC synthesis, place
    and route, and timing analysis tools to evaluate the area and performance of different
    variations of these RAD components. By using both RAD-Sim and RAD-Gen, an architect
    can evaluate the performance-cost tradeoff for different RAD candidates as demonstrated
    for DL recommendation model acceleration in [[130](#bib.bib130)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计空间缩小到几个 RAD 候选者之后，流程的第二个组成部分 RAD-Gen 可以用于评估它们的实现可行性和成本。例如，当在 RAD-Sim 中建模时，加速器核心可能显著提高周期数性能，但可能无法满足其硅区域预算，或者运行频率可能比假设的要低。RAD-Gen
    接受常见 RAD 组件（例如 NoC 路由器）和/或候选加速器核心的 RTL 描述作为输入，以及待扫参数值列表、高级 ASIC 实现流程配置和目标工艺的设计工具包。然后，它自动运行
    ASIC 合成、布局与布线和时序分析工具，以评估这些 RAD 组件不同变体的面积和性能。通过使用 RAD-Sim 和 RAD-Gen，架构师可以评估不同 RAD
    候选者的性能-成本权衡，如文献[[130](#bib.bib130)]中所示的 DL 推荐模型加速。
- en: VII-B DL Chiplets
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B DL 芯片模块
- en: '![Refer to caption](img/a30e367a10704c55ce16043f7dc4a265.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a30e367a10704c55ce16043f7dc4a265.png)'
- en: 'Figure 23: Passive interposers for integrating an FPGA fabric with accelerator
    chiplets (left) and 3D stacked RADs (right).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23：用于将 FPGA 结构与加速器芯片集成的被动互连层（左）和 3D 堆叠 RADs（右）。
- en: 'Most modern FPGAs from AMD and Intel have been using interposer technology
    to integrate either multiple FPGA dice or an FPGA die and one or multiple I/O
    transceiver *chiplets* in the same package with high interconnect density between
    them, as depicted in Fig. [8](#S3.F8 "Figure 8 ‣ III-D Interposers ‣ III FPGA
    Architecture & Opportunities for DL ‣ Field-Programmable Gate Array Architecture
    for Deep Learning: Survey & Future Directions"). Interposers can also be used
    to build FPGA devices targeting a certain application domain by integrating a
    specialized ASIC chiplet in the same package, as depicted in the left side of
    Fig. [23](#S7.F23 "Figure 23 ‣ VII-B DL Chiplets ‣ VII Beyond the FPGA Fabric
    ‣ Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future
    Directions"). Work from Intel Labs proposed the integration of different accelerator
    chiplets in the Stratix 10 FPGA package to enhance DL inference efficiency for
    different workloads [[132](#bib.bib132), [67](#bib.bib67)]. In this case, the
    chiplet implements tensor operations that are common across a wide variety of
    DL workloads, freeing up FPGA fabric resources to implement either model layers
    that can change over time (e.g. residual connections or activation functions)
    or other system components such as pre/post-processing stages. Nurvitadhi et al. [[67](#bib.bib67)]
    integrated TensorRAM, a chiplet optimized for memory-bound DL models, with a small
    Stratix 10 FPGA in the same package to achieve 16$\times$ lower latency and 34$\times$
    higher energy efficiency compared to the largest same-generation Nvidia DL-optimized
    GPU.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的 AMD 和 Intel FPGAs 多数已经使用了互连层技术，将多个 FPGA 芯片或一个 FPGA 芯片与一个或多个 I/O 收发器*芯片*集成在同一个封装中，它们之间具有高密度的互连，如图
    [8](#S3.F8 "图 8 ‣ III-D 互连层 ‣ III FPGA 架构与深度学习机会 ‣ 深度学习的现场可编程门阵列架构：调查与未来方向") 所示。互连层还可以用来构建针对特定应用领域的
    FPGA 设备，通过在同一封装中集成一个专用的 ASIC 芯片，如图 [23](#S7.F23 "图 23 ‣ VII-B DL 芯片 ‣ VII 超越 FPGA
    结构 ‣ 深度学习的现场可编程门阵列架构：调查与未来方向") 左侧所示。Intel Labs 的研究提出了在 Stratix 10 FPGA 封装中集成不同的加速器芯片，以增强不同工作负载下的深度学习推理效率[[132](#bib.bib132),
    [67](#bib.bib67)]。在这种情况下，芯片实现了在各种深度学习工作负载中普遍存在的张量操作，从而释放 FPGA 结构资源以实现可以随时间变化的模型层（如残差连接或激活函数）或其他系统组件，例如前处理/后处理阶段。Nurvitadhi
    等人[[67](#bib.bib67)] 将优化用于内存绑定深度学习模型的 TensorRAM 芯片与一个小型的 Stratix 10 FPGA 集成在同一封装中，从而实现了相比于同代
    Nvidia 深度学习优化 GPU 的 16 倍更低延迟和 34 倍更高能效。
- en: 'More recent advances in chip integration technology have enabled the stacking
    of multiple active dice on top of each other [[133](#bib.bib133)]. For example,
    the announced Instinct MI300 datacenter GPU accelerator family from AMD uses active
    die stacking technology to integrate 13 chiplets including CPU and GPU cores on
    top of dice that handle IO and memory traffic [[134](#bib.bib134)]. This also
    opens the door for a myriad of possibilities for 3D RADs that integrate an FPGA
    fabric on top of an ASIC base die that implements larger on-chip memories, application-specific
    accelerators for DL, and system-level NoCs, as shown in Fig. [23](#S7.F23 "Figure
    23 ‣ VII-B DL Chiplets ‣ VII Beyond the FPGA Fabric ‣ Field-Programmable Gate
    Array Architecture for Deep Learning: Survey & Future Directions") [[75](#bib.bib75)].'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的芯片集成技术进展使得多个活跃芯片可以互相叠加在一起[[133](#bib.bib133)]。例如，AMD 宣布的 Instinct MI300 数据中心
    GPU 加速器系列采用了活跃芯片叠加技术，将包括 CPU 和 GPU 核心在内的 13 个芯片集成在处理 IO 和内存流量的芯片上[[134](#bib.bib134)]。这也为集成
    FPGA 结构在实现更大片上内存、应用专用加速器以及系统级 NoCs 的 ASIC 基础芯片上的 3D RADs 开辟了无限可能，如图 [23](#S7.F23
    "图 23 ‣ VII-B DL 芯片 ‣ VII 超越 FPGA 结构 ‣ 深度学习的现场可编程门阵列架构：调查与未来方向") 所示[[75](#bib.bib75)]。
- en: VIII Summary
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第八章 总结
- en: 'With DL becoming the cornerstone of a myriad of applications running on large-scale
    datacenter clusters as well as edge devices, there is a pressing need for efficient
    compute platforms that can keep up with the growing compute demands of DL models.
    This has driven architectural innovations for general-purpose CPUs and GPUs and
    the creation of myriad ASIC DL accelerators. FPGAs offer several unique features
    compared to these other compute platforms: (1) their fine-grained hardware programmability
    enables customizing numerical precisions and the on-chip memory hierarchy to exactly
    match the needs of the target DL models, (2) their spatial architecture can exploit
    massive parallelism and direct communication between compute units for inference
    applications with tight latency constraints, (3) their reconfigurability allows
    adding or changing hardware features as DL models evolve, and (4) their diverse
    IOs enable building end-to-end DL systems in which an inference component is interfaced
    with different sensors and actuators in edge applications or high-speed networking
    in datacenters.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习（DL）成为运行在大规模数据中心集群以及边缘设备上的各种应用的基石，迫切需要高效的计算平台以跟上DL模型日益增长的计算需求。这推动了通用CPU和GPU的架构创新以及大量ASIC
    DL加速器的创建。与这些其他计算平台相比，FPGA提供了几个独特的特性：（1）其精细的硬件可编程性使得可以定制数值精度和片上内存层次结构，以完全符合目标DL模型的需求，（2）其空间架构能够利用大规模的并行性以及计算单元之间的直接通信，以应对具有严格延迟约束的推理应用，（3）其可重构性允许在DL模型演变时添加或更改硬件特性，（4）其多样化的IO接口使得能够构建端到端的DL系统，其中推理组件与边缘应用中的不同传感器和执行器或数据中心中的高速网络接口相连。
- en: In this article, we described different design styles of DL accelerators on
    FPGAs that achieve state-of-the-art performance while improving ease-of-use for
    application developers. Similarly to other compute platforms such as CPUs and
    GPUs, FPGA architecture is also evolving to better suit DL workloads. We surveyed
    different proposals on how to enhance the FPGA underlying architecture to be even
    better at DL. These enhancements include modifying conventional FPGA fabric blocks
    (logic blocks, DSPs, BRAMs), adding new in-fabric blocks for tensor compute, and
    integrating conventional FPGA fabrics with different coarse-grained accelerator
    cores and chiplets in future RADs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们描述了在FPGA上实现最先进性能的不同DL加速器设计风格，同时提升了应用开发者的易用性。类似于CPU和GPU等其他计算平台，FPGA架构也在不断发展，以更好地适应DL工作负载。我们调查了如何增强FPGA底层架构以更好地支持DL的不同提案。这些增强包括修改传统的FPGA织物块（逻辑块、DSPs、BRAMs），增加用于张量计算的新织物块，以及将传统FPGA织物与未来RADs中的不同粗粒度加速器核心和芯片组集成。
- en: The design space of RAD architectures is very large, as it comprises fabric
    optimizations, new coarse-grained accelerator blocks, and different methods to
    interconnect them using traditional programmable routing, NoCs, and 2.D or 3D
    integration. We expect exploration of these architectures to improve DL inference
    efficiency will remain a dynamic research area for years to come.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: RAD架构的设计空间非常大，因为它包括了织物优化、新的粗粒度加速器块，以及使用传统的可编程路由、网络互连（NoCs）和2D或3D集成将它们互连的不同方法。我们预计，对这些架构的探索以提高DL推理效率将继续是一个动态的研究领域，未来几年内仍将如此。
- en: References
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection,”
    in *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, vol. 1,
    2005, pp. 886–893.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] N. Dalal 和 B. Triggs, “用于人类检测的方向梯度直方图，” 收录于 *IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*，第1卷，2005年，页886–893。'
- en: '[2] D. Rumelhart, G. Hinton, and R. Williams, “Learning Internal Representations
    by Error Propagation,” in *Neurocomputing: Foundations of Research*, 1988, pp.
    673–695.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Rumelhart, G. Hinton, 和 R. Williams, “通过误差传播学习内部表示，” 收录于 *Neurocomputing:
    Foundations of Research*，1988年，页673–695。'
- en: '[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classification
    with Deep Convolutional Neural Networks,” *Advances in Neural Information Processing
    Systems (NeurIPS)*, 2012.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行ImageNet分类，”
    *Advances in Neural Information Processing Systems (NeurIPS)*，2012年。'
- en: '[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is All You Need,” *Advances in Neural
    Information Processing Systems (NeurIPS)*, vol. 30, 2017.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin, “注意力机制即一切所需，” *Advances in Neural Information Processing
    Systems (NeurIPS)*，第30卷，2017年。'
- en: '[5] M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman, J. Park,
    X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini *et al.*, “Deep Learning Recommendation
    Model for Personalization and Recommendation Systems,” *arXiv preprint arXiv:1906.00091*,
    2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman, J. Park,
    X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini *等*，“个性化和推荐系统的深度学习推荐模型，” *arXiv预印本
    arXiv:1906.00091*，2019年。'
- en: '[6] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and
    I. Sutskever, “Zero-Shot Text-to-Image Generation,” in *International Conference
    on Machine Learning (ICML)*.   PMLR, 2021, pp. 8821–8831.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, 和
    I. Sutskever，“零样本文本到图像生成，” 在 *国际机器学习会议 (ICML)*。 PMLR，2021年，页码8821–8831。'
- en: '[7] M. Haldar, M. Abdool, P. Ramanathan, T. Xu, S. Yang, H. Duan, Q. Zhang,
    N. Barrow-Williams, B. C. Turnbull, B. M. Collins *et al.*, “Applying Deep Learning
    to Airbnb Search,” in *proceedings of the 25th ACM SIGKDD international conference
    on knowledge discovery & Data Mining*, 2019, pp. 1927–1935.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Haldar, M. Abdool, P. Ramanathan, T. Xu, S. Yang, H. Duan, Q. Zhang,
    N. Barrow-Williams, B. C. Turnbull, B. M. Collins *等*，“将深度学习应用于Airbnb搜索，” 在 *第25届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*，2019年，页码1927–1935。'
- en: '[8] T. Capes, P. Coles, A. Conkie, L. Golipour, A. Hadjitarkhani, Q. Hu, N. Huddleston,
    M. Hunt, J. Li, M. Neeracher *et al.*, “Siri On-Device Deep Learning-Guided Unit
    Selection Text-to-Speech System,” in *Interspeech*, 2017, pp. 4011–4015.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] T. Capes, P. Coles, A. Conkie, L. Golipour, A. Hadjitarkhani, Q. Hu, N.
    Huddleston, M. Hunt, J. Li, M. Neeracher *等*，“Siri 设备内深度学习指导的单元选择文本到语音系统，” 在 *国际语音通信大会*，2017年，页码4011–4015。'
- en: '[9] H. Steck, L. Baltrunas, E. Elahi, D. Liang, Y. Raimond, and J. Basilico,
    “Deep Learning for Recommender Systems: A Netflix Case Study,” *AI Magazine*,
    vol. 42, no. 3, pp. 7–18, 2021.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] H. Steck, L. Baltrunas, E. Elahi, D. Liang, Y. Raimond, 和 J. Basilico，“推荐系统的深度学习：Netflix案例研究，”
    *AI Magazine*，第42卷，第3期，页码7–18，2021年。'
- en: '[10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards,
    Y. Burda, N. Joseph, G. Brockman *et al.*, “Evaluating Large Language Models Trained
    on Code,” *arXiv preprint arXiv:2107.03374*, 2021.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H.
    Edwards, Y. Burda, N. Joseph, G. Brockman *等*，“评估在代码上训练的大型语言模型，” *arXiv预印本 arXiv:2107.03374*，2021年。'
- en: '[11] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang, J. Alben,
    H. Anand, S. Banerjee, I. Bayraktaroglu *et al.*, “ChipNeMo: Domain-Adapted LLMs
    for Chip Design,” *arXiv preprint arXiv:2311.00176*, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang, J. Alben,
    H. Anand, S. Banerjee, I. Bayraktaroglu *等*，“ChipNeMo：面向芯片设计的领域自适应LLMs，” *arXiv预印本
    arXiv:2311.00176*，2023年。'
- en: '[12] A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain,
    A. Novikov, F. J. R Ruiz, J. Schrittwieser, G. Swirszcz *et al.*, “Discovering
    Faster Matrix Multiplication Algorithms with Reinforcement Learning,” *Nature*,
    vol. 610, no. 7930, pp. 47–53, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain,
    A. Novikov, F. J. R Ruiz, J. Schrittwieser, G. Swirszcz *等*，“通过强化学习发现更快的矩阵乘法算法，”
    *Nature*，第610卷，第7930期，页码47–53，2022年。'
- en: '[13] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger,
    K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko *et al.*, “Highly Accurate
    Protein Structure Prediction with AlphaFold,” *Nature*, vol. 596, no. 7873, pp.
    583–589, 2021.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger,
    K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko *等*，“通过AlphaFold进行高度准确的蛋白质结构预测，”
    *Nature*，第596卷，第7873期，页码583–589，2021年。'
- en: '[14] A. Suleiman, Y.-H. Chen, J. Emer, and V. Sze, “Towards Closing the Energy
    Gap between HOG and CNN Features for Embedded Vision,” in *IEEE International
    Symposium on Circuits and Systems (ISCAS)*, 2017, pp. 1–4.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. Suleiman, Y.-H. Chen, J. Emer, 和 V. Sze，“缩小HOG与CNN特征在嵌入式视觉中的能耗差距，”
    在 *IEEE国际电路与系统研讨会 (ISCAS)*，2017年，页码1–4。'
- en: '[15] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
    S. Bates, S. Bhatia, N. Boden, A. Borchers *et al.*, “In-Datacenter Performance
    Analysis of a Tensor Processing Unit,” in *International Symposium on Computer
    Architecture (ISCA)*, 2017, pp. 1–12.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
    S. Bates, S. Bhatia, N. Boden, A. Borchers *等*，“数据中心中张量处理单元的性能分析，” 在 *国际计算机架构研讨会
    (ISCA)*，2017年，页码1–12。'
- en: '[16] D. Patel and A. Ahmad, “The Inference Cost of Search Disruption: Large
    Language Model Cost Analysis,” in *SemiAnalysis*, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] D. Patel 和 A. Ahmad，“搜索中断的推理成本：大型语言模型成本分析，” 在 *SemiAnalysis*，2023年。'
- en: '[17] D. Khaldi, Y. Luo, B. Yu, A. Sotkin, B. Morais, and M. Girkar, “Extending
    LLVM IR for DPC++ Matrix Support: A Case Study with Intel Advanced Matrix Extensions
    (Intel AMX),” in *IEEE Workshop on the LLVM Compiler Infrastructure in HPC (LLVM-HPC)*,
    2021, pp. 20–26.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] D. Khaldi, Y. Luo, B. Yu, A. Sotkin, B. Morais, 和 M. Girkar, “扩展 LLVM
    IR 以支持 DPC++ 矩阵：以 Intel 高级矩阵扩展（Intel AMX）为例”，在 *IEEE Workshop on the LLVM Compiler
    Infrastructure in HPC (LLVM-HPC)*，2021，pp. 20–26。'
- en: '[18] L. Su, “AMD Keynote Presentation,” in *Consumer Electronics Show (CES)*,
    2023\. [Online]. Available: [https://www.youtube.com/watch?v=OMxU4BDIm4M](https://www.youtube.com/watch?v=OMxU4BDIm4M)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] L. Su, “AMD 主旨演讲”，在 *Consumer Electronics Show (CES)*，2023。 [在线]. 可用：
    [https://www.youtube.com/watch?v=OMxU4BDIm4M](https://www.youtube.com/watch?v=OMxU4BDIm4M)'
- en: '[19] A. Weißenberger and B. Schmidt, “Accelerating JPEG Decompression on GPUs,”
    in *International Conference on High Performance Computing, Data, and Analytics
    (HiPC)*.   IEEE, 2021, pp. 121–130.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Weißenberger 和 B. Schmidt, “在 GPU 上加速 JPEG 解压缩”，在 *International Conference
    on High Performance Computing, Data, and Analytics (HiPC)*。IEEE，2021，pp. 121–130。'
- en: '[20] K. Guo, S. Zeng, J. Yu, Y. Wang, and H. Yang, “A Survey of FPGA-based
    Neural Network Inference Accelerators,” *ACM Transactions on Reconfigurable Technology
    and Systems (TRETS)*, vol. 12, no. 1, pp. 1–26, 2019.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Guo, S. Zeng, J. Yu, Y. Wang, 和 H. Yang, “基于 FPGA 的神经网络推理加速器综述”，*ACM
    Transactions on Reconfigurable Technology and Systems (TRETS)*，第 12 卷，第 1 期，pp.
    1–26，2019。'
- en: '[21] K. Abdelouahab, M. Pelcat, J. Serot, and F. Berry, “Accelerating CNN Inference
    on FPGAs: A Survey,” *arXiv preprint arXiv:1806.01683*, 2018.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] K. Abdelouahab, M. Pelcat, J. Serot, 和 F. Berry, “在 FPGA 上加速 CNN 推理：综述”，*arXiv
    预印本 arXiv:1806.01683*，2018。'
- en: '[22] S. I. Venieris, A. Kouris, and C.-S. Bouganis, “Toolflows for Mapping
    Convolutional Neural Networks on FPGAs: A Survey and Future Directions,” *ACM
    Computing Surveys (CSUR)*, vol. 51, no. 3, pp. 1–39, 2018.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] S. I. Venieris, A. Kouris, 和 C.-S. Bouganis, “在 FPGA 上映射卷积神经网络的工具流：综述和未来方向”，*ACM
    Computing Surveys (CSUR)*，第 51 卷，第 3 期，pp. 1–39，2018。'
- en: '[23] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language Models are Few-Shot Learners,”
    *Advances in Neural Information Processing Systems (NeurIPS)*, vol. 33, pp. 1877–1901,
    2020.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell *等*，“语言模型是少样本学习者”，*Advances in Neural
    Information Processing Systems (NeurIPS)*，第 33 卷，pp. 1877–1901，2020。'
- en: '[24] A. Boutros, E. Nurvitadhi, R. Ma, S. Gribok, Z. Zhao, J. C. Hoe, V. Betz,
    and M. Langhammer, “Beyond Peak Performance: Comparing the Real Performance of
    AI-Optimized FPGAs and GPUs,” in *International Conference on Field-Programmable
    Technology (FPT)*, 2020, pp. 10–19.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. Boutros, E. Nurvitadhi, R. Ma, S. Gribok, Z. Zhao, J. C. Hoe, V. Betz,
    和 M. Langhammer, “超越峰值性能：比较 AI 优化 FPGA 和 GPU 的实际性能”，在 *International Conference
    on Field-Programmable Technology (FPT)*，2020，pp. 10–19。'
- en: '[25] M. Hall and V. Betz, “From TensorFlow Graphs to LUTs and Wires: Automated
    Sparse and Physically Aware CNN Hardware Generation,” in *IEEE International Conference
    on Field-Programmable Technology (FPT)*, 2020, pp. 56–65.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. Hall 和 V. Betz, “从 TensorFlow 图到 LUTs 和电线：自动化稀疏和物理意识的 CNN 硬件生成”，在 *IEEE
    International Conference on Field-Programmable Technology (FPT)*，2020，pp. 56–65。'
- en: '[26] L. Ganesh, H. Weatherspoon, T. Marian, and K. Birman, “Integrated Approach
    to Data Center Power Management,” *IEEE Transactions on Computers*, vol. 62, no. 6,
    pp. 1086–1096, 2013.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] L. Ganesh, H. Weatherspoon, T. Marian, 和 K. Birman, “数据中心电力管理的综合方法”，*IEEE
    Transactions on Computers*，第 62 卷，第 6 期，pp. 1086–1096，2013。'
- en: '[27] Z. Stone. (2018) Now You Can Train TensorFlow Machine Learning Models
    Faster and at Lower Cost on Cloud TPU Pods. [Online]. Available: [https://cloud.google.com/blog/products/ai-machine-learning/now-you-can-train-ml-models-faster-and-lower-cost-cloud-tpu-pods](https://cloud.google.com/blog/products/ai-machine-learning/now-you-can-train-ml-models-faster-and-lower-cost-cloud-tpu-pods)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Z. Stone. (2018) 现在你可以在 Cloud TPU Pods 上更快、更低成本地训练 TensorFlow 机器学习模型。
    [在线]. 可用： [https://cloud.google.com/blog/products/ai-machine-learning/now-you-can-train-ml-models-faster-and-lower-cost-cloud-tpu-pods](https://cloud.google.com/blog/products/ai-machine-learning/now-you-can-train-ml-models-faster-and-lower-cost-cloud-tpu-pods)'
- en: '[28] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-Training
    of Deep Bidirectional Transformers for Language Understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova, “BERT：用于语言理解的深度双向变换器的预训练”，*arXiv
    预印本 arXiv:1810.04805*，2018。'
- en: '[29] O. Sharir, B. Peleg, and Y. Shoham, “The Cost of Training NLP Models:
    A Concise Overview,” *arXiv preprint arXiv:2004.08900*, 2020.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] O. Sharir, B. Peleg, 和 Y. Shoham, “训练 NLP 模型的成本：简明概述”，*arXiv 预印本 arXiv:2004.08900*，2020。'
- en: '[30] N. Jones *et al.*, “How to Stop Data Centres from Gobbling Up the World’s
    Electricity,” *Nature*, vol. 561, no. 7722, pp. 163–166, 2018.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] N. Jones *等*，“如何阻止数据中心吞噬全球电力，” *自然*，第561卷，第7722期，第163–166页，2018年。'
- en: '[31] E. Talpes, D. D. Sarma, G. Venkataramanan, P. Bannon, B. McGee, B. Floering,
    A. Jalote, C. Hsiong, S. Arora, A. Gorti *et al.*, “Compute Solution for Tesla’s
    Full Self-Driving Computer,” *IEEE Micro*, vol. 40, no. 2, pp. 25–35, 2020.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] E. Talpes, D. D. Sarma, G. Venkataramanan, P. Bannon, B. McGee, B. Floering,
    A. Jalote, C. Hsiong, S. Arora, A. Gorti *等*，“特斯拉全自动驾驶计算机的计算解决方案，” *IEEE微型计算机*，第40卷，第2期，第25–35页，2020年。'
- en: '[32] B. Darvish Rouhani, D. Lo, R. Zhao, M. Liu, J. Fowers, K. Ovtcharov, A. Vinogradsky,
    S. Massengill, L. Yang, R. Bittner *et al.*, “Pushing the Limits of Narrow Precision
    Inferencing at Cloud Scale with Microsoft Floating Point,” *Advances in Neural
    Information Processing Systems (NeurIPS)*, vol. 33, pp. 10 271–10 281, 2020.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] B. Darvish Rouhani, D. Lo, R. Zhao, M. Liu, J. Fowers, K. Ovtcharov, A.
    Vinogradsky, S. Massengill, L. Yang, R. Bittner *等*，“利用微软浮点推进云规模窄精度推理的极限，” *神经信息处理系统进展（NeurIPS）*，第33卷，第10 271–10 281页，2020年。'
- en: '[33] P. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisenthwaite,
    S. Ha, A. Heinecke, P. Judd, J. Kamalu *et al.*, “FP8 Formats for Deep Learning,”
    *arXiv preprint arXiv:2209.05433*, 2022.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisenthwaite,
    S. Ha, A. Heinecke, P. Judd, J. Kamalu *等*，“FP8格式用于深度学习，” *arXiv预印本 arXiv:2209.05433*，2022年。'
- en: '[34] M. Horowitz, “Computing’s Energy Problem (and What We Can Do About It),”
    in *International Solid-State Circuits Conference (ISSCC)*, 2014, pp. 10–14.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. Horowitz，“计算的能源问题（以及我们可以做什么），” *国际固态电路会议（ISSCC）*，2014年，第10–14页。'
- en: '[35] E. S. Chung, D. Burger, J. Fowers, M. Ghandi, G. Weisz, S. Lanka, and
    S. K. Reinhardt, “RETROSPECTIVE: A Configurable Cloud-Scale DNN Processor for
    Real-Time AI,” in *ISCA@50 25-Year Retrospective: 1996-2020*.   ACM SIGARCH and
    IEEE TCCA, 2023\. [Online]. Available: [https://bit.ly/isca50_retrospective](https://bit.ly/isca50_retrospective)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] E. S. Chung, D. Burger, J. Fowers, M. Ghandi, G. Weisz, S. Lanka, 和 S.
    K. Reinhardt，“RETROSPECTIVE：用于实时AI的可配置云规模DNN处理器，” *ISCA@50 25年回顾：1996-2020*。ACM
    SIGARCH和IEEE TCCA，2023年。[在线]。可用链接：[https://bit.ly/isca50_retrospective](https://bit.ly/isca50_retrospective)'
- en: '[36] J. Fowers, K. Ovtcharov, M. Papamichael, T. Massengill, M. Liu, D. Lo,
    S. Alkalay, M. Haselman, L. Adams, M. Ghandi *et al.*, “A Configurable Cloud-Scale
    DNN Processor for Real-Time AI,” in *International Symposium on Computer Architecture
    (ISCA)*, 2018, pp. 1–14.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Fowers, K. Ovtcharov, M. Papamichael, T. Massengill, M. Liu, D. Lo,
    S. Alkalay, M. Haselman, L. Adams, M. Ghandi *等*，“用于实时AI的可配置云规模DNN处理器，” *计算机体系结构国际研讨会（ISCA）*，2018年，第1–14页。'
- en: '[37] M. Urbina, T. Acosta, J. Lázaro, A. Astarloa, and U. Bidarte, “Smart Sensor:
    SoC Architecture for the Industrial Internet of Things,” *IEEE Internet of Things
    Journal*, vol. 6, no. 4, pp. 6567–6577, 2019.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] M. Urbina, T. Acosta, J. Lázaro, A. Astarloa, 和 U. Bidarte，“智能传感器：工业物联网的SoC架构，”
    *IEEE物联网期刊*，第6卷，第4期，第6567–6577页，2019年。'
- en: '[38] A. Mishra, E. Nurvitadhi, J. J. Cook, and D. Marr, “WRPN: Wide Reduced-Precision
    Networks,” *arXiv preprint arXiv:1709.01134*, 2017.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Mishra, E. Nurvitadhi, J. J. Cook, 和 D. Marr，“WRPN：宽减少精度网络，” *arXiv预印本
    arXiv:1709.01134*，2017年。'
- en: '[39] Y. Cheng, D. Li, Z. Guo, B. Jiang, J. Lin, X. Fan, J. Geng, X. Yu, W. Bai,
    L. Qu *et al.*, “DLBooster: Boosting End-to-End Deep Learning Workflows with Offloading
    Data Preprocessing Pipelines,” in *Proceedings of the International Conference
    on Parallel Processing (ICPP)*, 2019, pp. 1–11.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y. Cheng, D. Li, Z. Guo, B. Jiang, J. Lin, X. Fan, J. Geng, X. Yu, W.
    Bai, L. Qu *等*，“DLBooster：通过卸载数据预处理管道提升端到端深度学习工作流程，” *国际并行处理会议（ICPP）*，2019年，第1–11页。'
- en: '[40] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean, “Efficient Neural Architecture
    Search via Parameters Sharing,” in *International Conference on Machine Learning
    (ICML)*, 2018, pp. 4095–4104.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] H. Pham, M. Guan, B. Zoph, Q. Le, 和 J. Dean，“通过参数共享进行高效的神经架构搜索，” *国际机器学习会议（ICML）*，2018年，第4095–4104页。'
- en: '[41] N. Suda, V. Chandra, G. Dasika, A. Mohanty, Y. Ma, S. Vrudhula, J.-s.
    Seo, and Y. Cao, “Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale
    Convolutional Neural Networks,” in *ACM/SIGDA International Symposium on Field-Programmable
    Gate Arrays (FPGA)*, 2016, pp. 16–25.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] N. Suda, V. Chandra, G. Dasika, A. Mohanty, Y. Ma, S. Vrudhula, J.-s.
    Seo, 和 Y. Cao，“优化吞吐量的基于OpenCL的FPGA加速器，用于大规模卷积神经网络，” *ACM/SIGDA国际现场可编程门阵列研讨会（FPGA）*，2016年，第16–25页。'
- en: '[42] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong, “Optimizing FPGA-Based
    Accelerator Design for Deep Convolutional Neural Networks,” in *ACM/SIGDA International
    Symposium on Field-Programmable Gate Arrays (FPGA)*, 2015, pp. 161–170.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, 和 J. Cong，“优化基于FPGA的深度卷积神经网络加速器设计，”发表于*ACM/SIGDA国际现场可编程门阵列研讨会（FPGA）*，2015年，页码161–170。'
- en: '[43] J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu, T. Tang, N. Xu,
    S. Song *et al.*, “Going Deeper with Embedded FPGA Platform for Convolutional
    Neural Network,” in *ACM/SIGDA International Symposium on Field-Programmable Gate
    Arrays (FPGA)*, 2016, pp. 26–35.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu, T. Tang, N. Xu,
    S. Song *等*，“使用嵌入式FPGA平台深入研究卷积神经网络，”发表于*ACM/SIGDA国际现场可编程门阵列研讨会（FPGA）*，2016年，页码26–35。'
- en: '[44] H. Li, X. Fan, L. Jiao, W. Cao, X. Zhou, and L. Wang, “A High Performance
    FPGA-Based Accelerator for Large-Scale Convolutional Neural Networks,” in *IEEE
    International Conference on Field Programmable Logic and Applications (FPL)*,
    2016, pp. 1–9.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] H. Li, X. Fan, L. Jiao, W. Cao, X. Zhou, 和 L. Wang，“一种高性能的基于FPGA的大规模卷积神经网络加速器，”发表于*IEEE国际现场可编程逻辑和应用会议（FPL）*，2016年，页码1–9。'
- en: '[45] E. Nurvitadhi, J. Sim, D. Sheffield, A. Mishra, S. Krishnan, and D. Marr,
    “Accelerating Recurrent Neural Networks in Analytics Servers: Comparison of FPGA,
    CPU, GPU, and ASIC,” in *International Conference on Field Programmable Logic
    and Applications (FPL)*, 2016, pp. 1–4.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] E. Nurvitadhi, J. Sim, D. Sheffield, A. Mishra, S. Krishnan, 和 D. Marr，“在分析服务器中加速递归神经网络：FPGA、CPU、GPU和ASIC的比较，”发表于*国际现场可编程逻辑和应用会议（FPL）*，2016年，页码1–4。'
- en: '[46] A. X. M. Chang, B. Martini, and E. Culurciello, “Recurrent Neural Networks
    Hardware Implementation on FPGA,” *arXiv preprint arXiv:1511.05552*, 2015.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. X. M. Chang, B. Martini, 和 E. Culurciello，“在FPGA上的递归神经网络硬件实现，”*arXiv预印本
    arXiv:1511.05552*，2015年。'
- en: '[47] S. Li, C. Wu, H. Li, B. Li, Y. Wang, and Q. Qiu, “FPGA Acceleration of
    Recurrent Neural Network Based Language Model,” in *IEEE International Symposium
    on Field-Programmable Custom Computing Machines (FCCM)*, 2015, pp. 111–118.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] S. Li, C. Wu, H. Li, B. Li, Y. Wang, 和 Q. Qiu，“基于FPGA的递归神经网络语言模型加速，”发表于*IEEE国际现场可编程定制计算机研讨会（FCCM）*，2015年，页码111–118。'
- en: '[48] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
    G. Irving, M. Isard *et al.*, “TensorFlow: A System for Large-Scale Machine Learning,”
    in *USENIX Symposium on Operating Systems Design and Implementation (OSDI)*, 2016,
    pp. 265–283.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S.
    Ghemawat, G. Irving, M. Isard *等*，“TensorFlow：一个用于大规模机器学习的系统，”发表于*USENIX操作系统设计与实现研讨会（OSDI）*，2016年，页码265–283。'
- en: '[49] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga *et al.*, “PyTorch: An Imperative Style, High-Performance
    Deep Learning Library,” *Advances in Neural Information Processing Systems (NeurIPS)*,
    vol. 32, 2019.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga *等*，“PyTorch：一种命令式风格的高性能深度学习库，”*神经信息处理系统进展（NeurIPS）*，第32卷，2019年。'
- en: '[50] S. I. Venieris and C.-S. Bouganis, “fpgaConvNet: Mapping Regular and Irregular
    Convolutional Neural Networks on FPGAs,” *IEEE Transactions on Neural Networks
    and Learning Systems*, vol. 30, no. 2, pp. 326–342, 2018.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. I. Venieris 和 C.-S. Bouganis，“fpgaConvNet：在FPGA上映射规则和不规则卷积神经网络，”*IEEE神经网络与学习系统汇刊*，第30卷，第2期，页码326–342，2018年。'
- en: '[51] X. Zhang, J. Wang, C. Zhu, Y. Lin, J. Xiong, W.-m. Hwu, and D. Chen, “DNNBuilder:
    An Automated Tool for Building High-Performance DNN Hardware Accelerators for
    FPGAs,” in *IEEE/ACM International Conference on Computer-Aided Design (ICCAD)*,
    2018, pp. 1–8.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] X. Zhang, J. Wang, C. Zhu, Y. Lin, J. Xiong, W.-m. Hwu, 和 D. Chen，“DNNBuilder：一个用于构建高性能DNN硬件加速器的自动化工具，”发表于*IEEE/ACM国际计算机辅助设计会议（ICCAD）*，2018年，页码1–8。'
- en: '[52] Y. Umuroglu, N. J. Fraser, G. Gambardella, M. Blott, P. Leong, M. Jahre,
    and K. Vissers, “FINN: A Framework for Fast, Scalable Binarized Neural Network
    Inference,” in *ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
    (FPGA)*, 2017, pp. 65–74.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Umuroglu, N. J. Fraser, G. Gambardella, M. Blott, P. Leong, M. Jahre,
    和 K. Vissers，“FINN：一个用于快速、可扩展的二值化神经网络推理的框架，”发表于*ACM/SIGDA国际现场可编程门阵列研讨会（FPGA）*，2017年，页码65–74。'
- en: '[53] Y. Ma, N. Suda, Y. Cao, J.-s. Seo, and S. Vrudhula, “Scalable and Modularized
    RTL Compilation of Convolutional Neural Networks onto FPGA,” in *IEEE International
    Conference on Field Programmable Logic and Applications (FPL)*, 2016, pp. 1–8.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Ma, N. Suda, Y. Cao, J.-s. Seo, 和 S. Vrudhula，“卷积神经网络在FPGA上的可扩展和模块化RTL编译，”发表于*IEEE国际现场可编程逻辑和应用会议（FPL）*，2016年，页码1–8。'
- en: '[54] H. Sharma, J. Park, D. Mahajan, E. Amaro, J. K. Kim, C. Shao, A. Mishra,
    and H. Esmaeilzadeh, “From High-Level Deep Neural M0odels to FPGAs,” in *IEEE/ACM
    International Symposium on Microarchitecture (MICRO)*, 2016, pp. 1–12.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] H. Sharma, J. Park, D. Mahajan, E. Amaro, J. K. Kim, C. Shao, A. Mishra,
    和 H. Esmaeilzadeh, “从高层深度神经网络模型到 FPGA,” 载于 *IEEE/ACM 国际微架构研讨会 (MICRO)*, 2016,
    第 1–12 页。'
- en: '[55] Y. Guan, H. Liang, N. Xu, W. Wang, S. Shi, X. Chen, G. Sun, W. Zhang,
    and J. Cong, “FP-DNN: An Automated Framework for Mapping Deep Neural Networks
    onto FPGAs with RTL-HLS Hybrid Templates,” in *IEEE International Symposium on
    Field-Programmable Custom Computing Machines (FCCM)*, 2017, pp. 152–159.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Guan, H. Liang, N. Xu, W. Wang, S. Shi, X. Chen, G. Sun, W. Zhang,
    和 J. Cong, “FP-DNN: 一种将深度神经网络映射到 FPGA 的自动化框架，使用 RTL-HLS 混合模板,” 载于 *IEEE 国际现场可编程定制计算机研讨会
    (FCCM)*, 2017, 第 152–159 页。'
- en: '[56] S. Abi-Karam and C. Hao, “GNNBuilder: An Automated Framework for Generic
    Graph Neural Network Accelerator Generation, Simulation, and Optimization,” *arXiv
    preprint arXiv:2303.16459*, 2023.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] S. Abi-Karam 和 C. Hao, “GNNBuilder: 一种用于通用图神经网络加速器生成、仿真和优化的自动化框架,” *arXiv
    预印本 arXiv:2303.16459*, 2023。'
- en: '[57] E. Wang, J. J. Davis, P. Y. Cheung, and G. A. Constantinides, “LUTNet:
    Learning FPGA Configurations for Highly Efficient Neural Network Inference,” *IEEE
    Transactions on Computers*, vol. 69, no. 12, pp. 1795–1808, 2020.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] E. Wang, J. J. Davis, P. Y. Cheung, 和 G. A. Constantinides, “LUTNet: 学习
    FPGA 配置以实现高效神经网络推理,” *IEEE 计算机学报*, 第 69 卷，第 12 期，第 1795–1808 页, 2020。'
- en: '[58] M. Andronic and G. A. Constantinides, “PolyLUT: Learning Piecewise Polynomials
    for Ultra-Low Latency FPGA LUT-based Inference,” 2023.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. Andronic 和 G. A. Constantinides, “PolyLUT: 学习分段多项式以实现超低延迟 FPGA LUT
    基于的推理,” 2023。'
- en: '[59] H. Wong, V. Betz, and J. Rose, “Comparing FPGA vs. Custom CMOS and the
    Impact on Processor Microarchitecture,” in *ACM/SIGDA International Symposium
    on Field Programmable Gate Arrays (FPGA)*, 2011.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] H. Wong, V. Betz, 和 J. Rose, “比较 FPGA 与定制 CMOS 以及对处理器微架构的影响,” 载于 *ACM/SIGDA
    国际现场可编程门阵列研讨会 (FPGA)*, 2011。'
- en: '[60] A. Boutros, S. Yazdanshenas, and V. Betz, “You Cannot Improve What You
    Do Not Measure: FPGA vs. ASIC Efficiency Gaps for Convolutional Neural Network
    Inference,” *ACM Transactions on Reconfigurable Technology and Systems (TRETS)*,
    vol. 11, no. 3, pp. 1–23, 2018.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] A. Boutros, S. Yazdanshenas, 和 V. Betz, “你无法改进你未测量的东西: FPGA 与 ASIC 在卷积神经网络推理中的效率差距,”
    *ACM 可重构技术与系统学报 (TRETS)*, 第 11 卷，第 3 期，第 1–23 页, 2018。'
- en: '[61] U. Aydonat, S. O’Connell, D. Capalija, A. C. Ling, and G. R. Chiu, “An
    OpenCL Deep Learning Accelerator on Arria 10,” in *International Symposium on
    Field-Programmable Gate Arrays (FPGA)*, 2017, pp. 55–64.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] U. Aydonat, S. O’Connell, D. Capalija, A. C. Ling, 和 G. R. Chiu, “Arria
    10 上的 OpenCL 深度学习加速器,” 载于 *国际现场可编程门阵列研讨会 (FPGA)*, 2017, 第 55–64 页。'
- en: '[62] M. S. Abdelfattah, D. Han, A. Bitar, R. DiCecco, S. O’Connell, N. Shanker,
    J. Chu, I. Prins, J. Fender, A. C. Ling *et al.*, “DLA: Compiler and FPGA Overlay
    for Neural Network Inference Acceleration,” in *International Conference on Field
    Programmable Logic and Applications (FPL)*, 2018, pp. 411–4117.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] M. S. Abdelfattah, D. Han, A. Bitar, R. DiCecco, S. O’Connell, N. Shanker,
    J. Chu, I. Prins, J. Fender, A. C. Ling *等*, “DLA: 神经网络推理加速的编译器和 FPGA 覆盖层,” 载于
    *国际现场可编程逻辑与应用大会 (FPL)*, 2018, 第 411–4117 页。'
- en: '[63] Advanced Micro Devices, Inc., “DPUCADF8H for Convolutional Neural Networks
    Product Guide (PG400),” 2022.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Advanced Micro Devices, Inc., “DPUCADF8H 用于卷积神经网络的产品指南 (PG400),” 2022。'
- en: '[64] Y. Yu, C. Wu, T. Zhao, K. Wang, and L. He, “OPU: An FPGA-based Overlay
    Processor for Convolutional Neural Networks,” *IEEE Transactions on Very Large
    Scale Integration Systems (TVLSI)*, vol. 28, no. 1, pp. 35–47, 2019.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. Yu, C. Wu, T. Zhao, K. Wang, 和 L. He, “OPU: 一种基于 FPGA 的卷积神经网络覆盖处理器,”
    *IEEE 超大规模集成系统学报 (TVLSI)*, 第 28 卷，第 1 期，第 35–47 页, 2019。'
- en: '[65] Y. Bai, H. Zhou, K. Zhao, J. Chen, J. Yu, and K. Wang, “Transformer-OPU:
    An FPGA-based Overlay Processor for Transformer Networks,” in *International Symposium
    on Field-Programmable Custom Computing Machines (FCCM)*, 2023, pp. 221–221.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Y. Bai, H. Zhou, K. Zhao, J. Chen, J. Yu, 和 K. Wang, “Transformer-OPU:
    一种基于 FPGA 的 Transformer 网络覆盖处理器,” 载于 *国际现场可编程定制计算机研讨会 (FCCM)*, 2023, 第 221–221
    页。'
- en: '[66] S. Hur, S. Na, D. Kwon, J. Kim, A. Boutros, E. Nurvitadhi, and J. Kim,
    “A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural
    Networks,” *ACM Transactions on Architecture and Code Optimization (TACO)*, vol. 20,
    no. 1, pp. 1–24, 2023.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. Hur, S. Na, D. Kwon, J. Kim, A. Boutros, E. Nurvitadhi, 和 J. Kim, “一种快速且灵活的基于
    FPGA 的自然语言处理神经网络加速器,” *ACM 架构与代码优化学报 (TACO)*, 第 20 卷，第 1 期，第 1–24 页, 2023。'
- en: '[67] E. Nurvitadhi, D. Kwon, A. Jafari, A. Boutros, J. Sim, P. Tomson, H. Sumbul,
    G. Chen, P. Knag, R. Kumar *et al.*, “Why Compete When You Can Work Together:
    FPGA-ASIC Integration for Persistent RNNs,” in *International Symposium on Field-Programmable
    Custom Computing Machines (FCCM)*, 2019.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] E. Nurvitadhi, D. Kwon, A. Jafari, A. Boutros, J. Sim, P. Tomson, H. Sumbul,
    G. Chen, P. Knag, R. Kumar *等*， “为何竞争，当你可以合作：FPGA-ASIC 集成用于持久 RNN，” *国际现场可编程定制计算机器研讨会
    (FCCM)*，2019年。'
- en: '[68] TensorFlow, “Keras: The High-Level API for TensorFlow,” in *[https://www.tensorflow.org/guide/keras](https://www.tensorflow.org/guide/keras)*,
    [Online; last accessed October 2023].'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] TensorFlow， “Keras：TensorFlow 的高级 API，” 在 *[https://www.tensorflow.org/guide/keras](https://www.tensorflow.org/guide/keras)*，
    [在线；最后访问时间 2023年10月]。'
- en: '[69] Baidu, “DeepBench,” in *[https://github.com/baidu-research/DeepBench](https://github.com/baidu-research/DeepBench)*,
    [Online; last accessed October 2023].'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Baidu， “DeepBench，” 在 *[https://github.com/baidu-research/DeepBench](https://github.com/baidu-research/DeepBench)*，
    [在线；最后访问时间 2023年10月]。'
- en: '[70] M. Langhammer, B. Pasca, G. Baeckler, and S. Gribok, “Extracting INT8
    Multipliers from INT18 Multipliers,” in *International Conference on Field Programmable
    Logic and Applications (FPL)*, 2019, pp. 114–120.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] M. Langhammer, B. Pasca, G. Baeckler, 和 S. Gribok， “从 INT18 乘法器提取 INT8
    乘法器，” *国际现场可编程逻辑与应用会议 (FPL)*，2019年，第 114–120 页。'
- en: '[71] M. Langhammer, E. Nurvitadhi, B. Pasca, and S. Gribok, “Stratix 10 NX
    Architecture and Applications,” in *International Symposium on Field-Programmable
    Gate Arrays (FPGA)*, 2021, pp. 57–67.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Langhammer, E. Nurvitadhi, B. Pasca, 和 S. Gribok， “Stratix 10 NX 架构及应用，”
    *国际现场可编程门阵列研讨会 (FPGA)*，2021年，第 57–67 页。'
- en: '[72] M. Stan, M. Hall, M. Ibrahim, and V. Betz, “HPIPE NX: Boosting CNN Inference
    Acceleration Performance with AI-Optimized FPGAs,” in *International Conference
    on Field-Programmable Technology (FPT)*, 2022, pp. 1–9.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. Stan, M. Hall, M. Ibrahim, 和 V. Betz， “HPIPE NX：利用 AI 优化 FPGA 提升 CNN
    推理加速性能，” *国际现场可编程技术会议 (FPT)*，2022年，第 1–9 页。'
- en: '[73] A. Boutros and V. Betz, “FPGA Architecture: Principles and Progression,”
    *IEEE Circuits and Systems Magazine*, vol. 21, no. 2, pp. 4–29, 2021.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] A. Boutros 和 V. Betz， “FPGA 架构：原理与进展，” *IEEE Circuits and Systems Magazine*，第
    21 卷，第 2 期，第 4–29 页，2021年。'
- en: '[74] K. E. Murray, J. Luu, M. J. Walker, C. McCullough, S. Wang, S. Huda, B. Yan,
    C. Chiasson, K. B. Kent, J. Anderson *et al.*, “Optimizing FPGA Logic Block Architectures
    for Arithmetic,” *IEEE Transactions on Very Large Scale Integration (VLSI) Systems*,
    vol. 28, no. 6, pp. 1378–1391, 2020.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] K. E. Murray, J. Luu, M. J. Walker, C. McCullough, S. Wang, S. Huda, B. Yan,
    C. Chiasson, K. B. Kent, J. Anderson *等*， “优化 FPGA 逻辑块架构以进行算术运算，” *IEEE 超大规模集成
    (VLSI) 系统汇刊*，第 28 卷，第 6 期，第 1378–1391 页，2020年。'
- en: '[75] A. Boutros, F. Mahmoudi, A. Mohaghegh, S. More, and V. Betz, “Into the
    Third Dimension: Architecture Exploration Tools for 3D Reconfigurable Acceleration
    Devices,” in *International Conference on Field-Programmable Technology (FPT)*,
    2023.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] A. Boutros, F. Mahmoudi, A. Mohaghegh, S. More, 和 V. Betz， “进入第三维度：3D
    可重构加速器的架构探索工具，” *国际现场可编程技术会议 (FPT)*，2023年。'
- en: '[76] E. Chung, J. Fowers, K. Ovtcharov, M. Papamichael, A. Caulfield, T. Massengil,
    M. Liu, D. Lo, S. Alkalay, M. Haselman *et al.*, “Accelerating Persistent Neural
    Networks at Datacenter Scale,” in *Hot Chips*, vol. 29, 2017.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] E. Chung, J. Fowers, K. Ovtcharov, M. Papamichael, A. Caulfield, T. Massengil,
    M. Liu, D. Lo, S. Alkalay, M. Haselman *等*， “在数据中心规模下加速持久神经网络，” *Hot Chips*，第
    29 卷，2017年。'
- en: '[77] Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen, and Z. Zhang, “FracBNN: Accurate
    and FPGA-Efficient Binary Neural Networks with Fractional Activations,” in *International
    Symposium on Field-Programmable Gate Arrays (FPGA)*, 2021, pp. 171–182.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen, 和 Z. Zhang， “FracBNN：具有分数激活的准确且
    FPGA 高效的二进制神经网络，” *国际现场可编程门阵列研讨会 (FPGA)*，2021年，第 171–182 页。'
- en: '[78] Intel Corp., “Intel Agilex Embedded Memory User Guide (UG-20208),” 2022.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Intel Corp.， “Intel Agilex 嵌入式内存用户指南 (UG-20208)，” 2022年。'
- en: '[79] AMD Inc., “Versal ACAP Memory Resources (AM007 v1.1),” 2020.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] AMD Inc.， “Versal ACAP 内存资源 (AM007 v1.1)，” 2020年。'
- en: '[80] S. Yazdanshenas, K. Tatsumura, and V. Betz, “Don’t Forget the Memory:
    Automatic Block RAM Modelling, Optimization, and Architecture Exploration,” in
    *International Symposium on Field-Programmable Gate Arrays (FPGA)*, 2017, pp.
    115–124.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] S. Yazdanshenas, K. Tatsumura, 和 V. Betz， “别忘了内存：自动块 RAM 建模、优化和架构探索，”
    *国际现场可编程门阵列研讨会 (FPGA)*，2017年，第 115–124 页。'
- en: '[81] J. H. Lau, “Recent Advances and Trends in Advanced Packaging,” *IEEE Transactions
    on Components, Packaging and Manufacturing Technology*, vol. 12, no. 2, pp. 228–252,
    2022.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] J. H. Lau， “先进封装的最新进展和趋势，” *IEEE 组件、包装与制造技术汇刊*，第 12 卷，第 2 期，第 228–252
    页，2022年。'
- en: '[82] C. Ravishankar, D. Gaitonde, and T. Bauer, “Placement Strategies for 2.5D
    FPGA Fabric Architectures,” in *International Conference on Field Programmable
    Logic and Applications (FPL)*, 2018, pp. 16–164.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] C. Ravishankar, D. Gaitonde 和 T. Bauer，“2.5D FPGA 结构的布局策略”，发表于*国际现场可编程逻辑与应用会议
    (FPL)*，2018年，第16–164页。'
- en: '[83] R. Mahajan, R. Sankman, N. Patel, D.-W. Kim, K. Aygun, Z. Qian, Y. Mekonnen,
    I. Salama, S. Sharan, D. Iyengar *et al.*, “Embedded Multi-Die Interconnect Bridge
    (EMIB): A High Density, High Bandwidth Packaging Interconnect,” in *Electronic
    Components and Technology Conference (ECTC)*, 2016, pp. 557–565.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] R. Mahajan, R. Sankman, N. Patel, D.-W. Kim, K. Aygun, Z. Qian, Y. Mekonnen,
    I. Salama, S. Sharan, D. Iyengar *等*，“嵌入式多芯片互连桥 (EMIB): 高密度、高带宽的封装互连”，发表于*电子组件与技术会议
    (ECTC)*，2016年，第557–565页。'
- en: '[84] Greenhill, David and Ho, Ron and Lewis, David and Schmit, Herman and Chan,
    Kok Hong and Tong, Andy and Atsatt, Sean and How, Dana and McElheny, Peter and
    Duwel, Keith and others, “A 14nm 1GHz FPGA with 2.5D Transceiver Integration,”
    in *2017 IEEE International Solid-State Circuits Conference (ISSCC)*, 2017, pp.
    54–55.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Greenhill, David 和 Ho, Ron 和 Lewis, David 和 Schmit, Herman 和 Chan, Kok
    Hong 和 Tong, Andy 和 Atsatt, Sean 和 How, Dana 和 McElheny, Peter 和 Duwel, Keith
    及其他人，“一个14nm 1GHz FPGA与2.5D 收发器集成”，发表于*2017 IEEE 国际固态电路会议 (ISSCC)*，2017年，第54–55页。'
- en: '[85] B. Gaide, D. Gaitonde, C. Ravishankar, and T. Bauer, “Xilinx Adaptive
    Compute Acceleration Platform: Versal Architecture,” in *International Symposium
    on Field-Programmable Gate Arrays (FPGA)*, 2019, pp. 84–93.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] B. Gaide, D. Gaitonde, C. Ravishankar 和 T. Bauer，“Xilinx 自适应计算加速平台: Versal
    架构”，发表于*国际现场可编程门阵列会议 (FPGA)*，2019年，第84–93页。'
- en: '[86] I. Swarbrick, D. Gaitonde, S. Ahmad, B. Gaide, and Y. Arbel, “Network-on-Chip
    Programmable Platform in Versal ACAP Architecture,” in *International Symposium
    on Field-Programmable Gate Arrays (FPGA)*, 2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] I. Swarbrick, D. Gaitonde, S. Ahmad, B. Gaide 和 Y. Arbel，“Versal ACAP
    架构中的片上网络可编程平台”，发表于*国际现场可编程门阵列会议 (FPGA)*，2019年。'
- en: '[87] K. E. Murray, O. Petelin, S. Zhong, J. M. Wang, M. Eldafrawy, J.-P. Legault,
    E. Sha, A. G. Graham, J. Wu, M. J. Walker *et al.*, “VTR 8: High-Performance CAD
    and Customizable FPGA Architecture Modelling,” *ACM Transactions on Reconfigurable
    Technology and Systems (TRETS)*, vol. 13, no. 2, pp. 1–55, 2020.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] K. E. Murray, O. Petelin, S. Zhong, J. M. Wang, M. Eldafrawy, J.-P. Legault,
    E. Sha, A. G. Graham, J. Wu, M. J. Walker *等*，“VTR 8: 高性能 CAD 和可定制 FPGA 架构建模”，*ACM
    可重构技术与系统交易 (TRETS)*，第13卷，第2期，第1–55页，2020年。'
- en: '[88] P. Jamieson, K. B. Kent, F. Gharibian, and L. Shannon, “Odin II: An Open-Source
    Verilog HDL Synthesis Tool for CAD Research,” in *International Symposium on Field-Programmable
    Custom Computing Machines (FCCM)*, 2010, pp. 149–156.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] P. Jamieson, K. B. Kent, F. Gharibian 和 L. Shannon，“Odin II: 一个开源的 Verilog
    HDL 合成工具用于 CAD 研究”，发表于*国际现场可编程定制计算机会议 (FCCM)*，2010年，第149–156页。'
- en: '[89] C. Wolf and J. Glaser, “Yosys: A Free Verilog Synthesis Suite,” in *Austrian
    Workshop on Microelectronics (AustroChip)*, 2013.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] C. Wolf 和 J. Glaser，“Yosys: 一个免费的 Verilog 合成套件”，发表于*奥地利微电子研讨会 (AustroChip)*，2013年。'
- en: '[90] R. Brayton and A. Mishchenko, “ABC: An Academic Industrial-Strength Verification
    Tool,” in *International Conference on Computer Aided Verification (CAV)*, 2010,
    pp. 24–40.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] R. Brayton 和 A. Mishchenko，“ABC: 一个学术级工业强度验证工具”，发表于*国际计算机辅助验证会议 (CAV)*，2010年，第24–40页。'
- en: '[91] K. E. Murray and V. Betz, “Tatum: Parallel Timing Analysis for Faster
    Design Cycles and Improved Optimization,” in *International Conference on Field-Programmable
    Technology (FPT)*, 2018, pp. 110–117.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] K. E. Murray 和 V. Betz，“Tatum: 并行时序分析以加快设计周期和改进优化”，发表于*国际现场可编程技术会议 (FPT)*，2018年，第110–117页。'
- en: '[92] S. Yazdanshenas and V. Betz, “COFFE 2: Automatic Modelling and Optimization
    of Complex and Heterogeneous FPGA Architectures,” *ACM Transactions on Reconfigurable
    Technology and Systems (TRETS)*, vol. 12, no. 1, pp. 1–27, 2019.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. Yazdanshenas 和 V. Betz，“COFFE 2: 复杂和异构 FPGA 架构的自动建模与优化”，*ACM 可重构技术与系统交易
    (TRETS)*，第12卷，第1期，第1–27页，2019年。'
- en: '[93] K. E. Murray, S. Whitty, S. Liu, J. Luu, and V. Betz, “Titan: Enabling
    Large and Complex Benchmarks in Academic CAD,” in *International Conference on
    Field programmable Logic and Applications (FPL)*, 2013, pp. 1–8.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] K. E. Murray, S. Whitty, S. Liu, J. Luu 和 V. Betz，“Titan: 在学术 CAD 中支持大型和复杂基准测试”，发表于*国际现场可编程逻辑与应用会议
    (FPL)*，2013年，第1–8页。'
- en: '[94] A. Arora, A. Boutros, S. A. Damghani, K. Mathur, V. Mohanty, T. Anand,
    M. A. Elgammal, K. B. Kent, V. Betz, and L. K. John, “Koios 2.0: Open-Source Deep
    Learning Benchmarks for FPGA Architecture and CAD Research,” *IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems (TCAD)*, 2023.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] A. Arora, A. Boutros, S. A. Damghani, K. Mathur, V. Mohanty, T. Anand,
    M. A. Elgammal, K. B. Kent, V. Betz, 和 L. K. John, “Koios 2.0: 用于FPGA架构和CAD研究的开源深度学习基准”，*IEEE计算机辅助设计与系统交易
    (TCAD)*，2023年。'
- en: '[95] H. Wu, P. Judd, X. Zhang, M. Isaev, and P. Micikevicius, “Integer Quantization
    for Deep Learning Inference: Principles and Empirical Evaluation,” *arXiv preprint
    arXiv:2004.09602*, 2020.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] H. Wu, P. Judd, X. Zhang, M. Isaev, 和 P. Micikevicius, “深度学习推理的整数量化：原理与实证评估”，*arXiv预印本
    arXiv:2004.09602*，2020年。'
- en: '[96] D. Abts, J. Ross, J. Sparling, M. Wong-VanHaren, M. Baker, T. Hawkins,
    A. Bell, J. Thompson, T. Kahsai, G. Kimmell *et al.*, “Think Fast: A Tensor Streaming
    Processor (TSP) for Accelerating Deep Learning Workloads,” in *ACM/IEEE International
    Symposium on Computer Architecture (ISCA)*, 2020, pp. 145–158.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] D. Abts, J. Ross, J. Sparling, M. Wong-VanHaren, M. Baker, T. Hawkins,
    A. Bell, J. Thompson, T. Kahsai, G. Kimmell *等*，“快速思考：用于加速深度学习工作负载的张量流处理器 (TSP)”，发表于
    *ACM/IEEE国际计算机架构研讨会 (ISCA)*，2020年，第145–158页。'
- en: '[97] M. Anderson, B. Chen, S. Chen, S. Deng, J. Fix, M. Gschwind, A. Kalaiah,
    C. Kim, J. Lee, J. Liang *et al.*, “First-Generation Inference Accelerator Deployment
    at Facebook,” *arXiv preprint arXiv:2107.04140*, 2021.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] M. Anderson, B. Chen, S. Chen, S. Deng, J. Fix, M. Gschwind, A. Kalaiah,
    C. Kim, J. Lee, J. Liang *等*，“第一代推理加速器在Facebook的部署”，*arXiv预印本 arXiv:2107.04140*，2021年。'
- en: '[98] A. Boutros, M. Eldafrawy, S. Yazdanshenas, and V. Betz, “Math Doesn’t
    Have to Be Hard: Logic Block Architectures to Enhance Low-Precision Multiply-Accumulate
    on FPGAs,” in *ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
    (FPGA)*, 2019, pp. 94–103.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] A. Boutros, M. Eldafrawy, S. Yazdanshenas, 和 V. Betz, “数学不必艰难：提升FPGA上低精度乘加的逻辑块架构”，发表于
    *ACM/SIGDA国际现场可编程门阵列研讨会 (FPGA)*，2019年，第94–103页。'
- en: '[99] M. Eldafrawy, A. Boutros, S. Yazdanshenas, and V. Betz, “FPGA Logic Block
    Architectures for Efficient Deep Learning Inference,” *ACM Transactions on Reconfigurable
    Technology and Systems (TRETS)*, vol. 13, no. 3, pp. 1–34, 2020.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] M. Eldafrawy, A. Boutros, S. Yazdanshenas, 和 V. Betz, “用于高效深度学习推理的FPGA逻辑块架构”，*ACM可重构技术与系统交易
    (TRETS)*，第13卷，第3期，第1–34页，2020年。'
- en: '[100] S. Yazdanshenas and T. Vanderhoek, “Efficient Logic Blocks Architectures
    for Dense Mapping of Multipliers,” 2021, US Patent App. 16/729,256.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] S. Yazdanshenas 和 T. Vanderhoek, “用于密集映射乘法器的高效逻辑块架构”，2021年，美国专利申请号16/729,256。'
- en: '[101] S. Rasoulinezhad, Siddhartha, H. Zhou, L. Wang, D. Boland, and P. H.
    Leong, “LUXOR: An FPGA Logic Cell Architecture for Efficient Compressor Tree Implementations,”
    in *ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)*,
    2020, pp. 161–171.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. Rasoulinezhad, Siddhartha, H. Zhou, L. Wang, D. Boland, 和 P. H. Leong,
    “LUXOR: 一种用于高效压缩树实现的FPGA逻辑单元架构”，发表于 *ACM/SIGDA国际现场可编程门阵列研讨会 (FPGA)*，2020年，第161–171页。'
- en: '[102] J. H. Kim, J. Lee, and J. H. Anderson, “FPGA Architecture Enhancements
    for Efficient BNN Implementation,” in *International Conference on Field-Programmable
    Technology (FPT)*, 2018.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. H. Kim, J. Lee, 和 J. H. Anderson, “FPGA架构优化以实现高效的BNN实现”，发表于 *国际现场可编程技术会议
    (FPT)*，2018年。'
- en: '[103] A. Boutros, S. Yazdanshenas, and V. Betz, “Embracing Diversity: Enhanced
    DSP Blocks for Low-Precision Deep Learning on FPGAs,” in *International Conference
    on Field Programmable Logic and Applications (FPL)*, 2018, pp. 35–357.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] A. Boutros, S. Yazdanshenas, 和 V. Betz, “拥抱多样性：针对FPGA上低精度深度学习的增强型DSP块”，发表于
    *国际现场可编程逻辑与应用会议 (FPL)*，2018年，第35–357页。'
- en: '[104] S. Rasoulinezhad, H. Zhou, L. Wang, and P. H. Leong, “PIR-DSP: An FPGA
    DSP Block Architecture for Multi-Precision Deep Neural Networks,” in *International
    Symposium on Field-Programmable Custom Computing Machines (FCCM)*, 2019, pp. 35–44.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] S. Rasoulinezhad, H. Zhou, L. Wang, 和 P. H. Leong, “PIR-DSP: 一种用于多精度深度神经网络的FPGA
    DSP块架构”，发表于 *国际现场可编程定制计算机机器研讨会 (FCCM)*，2019年，第35–44页。'
- en: '[105] K. Tatsumura, S. Yazdanshenas, and V. Betz, “High Density, Low Energy,
    Magnetic Tunnel Junction Based Block RAMs for Memory-Rich FPGAs,” in *IEEE International
    Conference on Field-Programmable Technology (FPT)*, 2016.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] K. Tatsumura, S. Yazdanshenas, 和 V. Betz, “高密度、低能耗的基于磁隧道结的块RAM用于内存丰富的FPGA”，发表于
    *IEEE国际现场可编程技术会议 (FPT)*，2016年。'
- en: '[106] X. Wang, V. Goyal, J. Yu, V. Bertacco, A. Boutros, E. Nurvitadhi, C. Augustine,
    R. Iyer, and R. Das, “Compute-Capable Block RAMs for Efficient Deep Learning Acceleration
    on FPGAs,” in *IEEE International Symposium on Field-Programmable Custom Computing
    Machines (FCCM)*, 2021.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] X. Wang, V. Goyal, J. Yu, V. Bertacco, A. Boutros, E. Nurvitadhi, C.
    Augustine, R. Iyer, 和 R. Das, “适用于 FPGA 高效深度学习加速的计算能力块 RAMs，”在 *IEEE 国际现场可编程定制计算机会议（FCCM）*，2021。'
- en: '[107] A. Arora, B. Hanindhito, and L. K. John, “Compute RAMs: Adaptable Compute
    and Storage Blocks for DL-Optimized FPGAs,” in *IEEE Asilomar Conference on Signals,
    Systems, and Computers*, 2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] A. Arora, B. Hanindhito, 和 L. K. John, “计算 RAMs：适应性计算和存储块用于 DL 优化 FPGA，”在
    *IEEE Asilomar 信号、系统与计算机会议*，2021。'
- en: '[108] A. Arora, T. Anand, A. Borda, R. Sehgal, B. Hanindhito, J. Kulkarni,
    and L. K. John, “CoMeFa: Compute-in-Memory Blocks for FPGAs,” in *IEEE International
    Symposium on Field-Programmable Custom Computing Machines (FCCM)*, 2022.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] A. Arora, T. Anand, A. Borda, R. Sehgal, B. Hanindhito, J. Kulkarni,
    和 L. K. John, “CoMeFa: 针对 FPGAs 的内存计算块，”在 *IEEE 国际现场可编程定制计算机会议（FCCM）*，2022。'
- en: '[109] Y. Chen and M. S. Abdelfattah, “BRAMAC: Compute-in-BRAM Architectures
    for Multiply-Accumulate on FPGAs,” in *IEEE International Symposium on Field-Programmable
    Custom Computing Machines (FCCM)*, 2023.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Y. Chen 和 M. S. Abdelfattah, “BRAMAC：用于 FPGA 上乘法累加的内存计算架构，”在 *IEEE 国际现场可编程定制计算机会议（FCCM）*，2023。'
- en: '[110] Y. Chen, J. Dotzel, and M. S. Abdelfattah, “M4BRAM: Mixed-Precision Matrix-Matrix
    Multiplication in FPGA Block RAMs,” in *IEEE International Conference on Field-Programmable
    Technology (FPT)*, 2023.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Y. Chen, J. Dotzel, 和 M. S. Abdelfattah, “M4BRAM：FPGA 块 RAM 中的混合精度矩阵-矩阵乘法，”在
    *IEEE 国际现场可编程技术会议（FPT）*，2023。'
- en: '[111] C. Eckert, X. Wang, J. Wang, A. Subramaniyan, R. Iyer, D. Sylvester,
    D. Blaaauw, and R. Das, “Neural Cache: Bit-Serial In-Cache Acceleration of Deep
    Neural Networks,” in *ACM/IEEE International Symposium on Computer Architecture
    (ISCA)*, 2018, pp. 383–396.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] C. Eckert, X. Wang, J. Wang, A. Subramaniyan, R. Iyer, D. Sylvester,
    D. Blaaauw, 和 R. Das, “神经缓存：深度神经网络的位序列缓存加速，”在 *ACM/IEEE 国际计算机架构研讨会（ISCA）*，2018，页
    383–396。'
- en: '[112] A. Arora, S. Mehta, V. Betz, and L. K. John, “Tensor Slices to the Rescue:
    Supercharging ML Acceleration on FPGAs,” in *The 2021 ACM/SIGDA International
    Symposium on Field-Programmable Gate Arrays*, 2021, pp. 23–33.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] A. Arora, S. Mehta, V. Betz, 和 L. K. John, “张量切片的救援：超级加速 FPGA 上的 ML，”在
    *2021 年 ACM/SIGDA 国际现场可编程门阵列研讨会*，2021，页 23–33。'
- en: '[113] A. Arora, M. Ghosh, S. Mehta, V. Betz, and L. K. John, “Tensor Slices:
    FPGA Building Blocks for the Deep Learning Era,” *ACM Transactions on Reconfigurable
    Technology and Systems (TRETS)*, vol. 15, no. 4, pp. 1–34, 2022.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Arora, M. Ghosh, S. Mehta, V. Betz, 和 L. K. John, “张量切片：深度学习时代的 FPGA
    构建块，” *ACM 可重构技术与系统交易（TRETS）*，第 15 卷，第 4 期，页 1–34，2022。'
- en: '[114] D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha,
    D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen *et al.*, “A Study of BFLOAT16
    for Deep Learning Training,” *arXiv preprint arXiv:1905.12322*, 2019.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha,
    D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen *等*，“BFLOAT16 在深度学习训练中的研究，”
    *arXiv 预印本 arXiv:1905.12322*，2019。'
- en: '[115] Achronix Corp., “Speedster7t Machine Learning Processing User Guide (UG088),”
    2019.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Achronix Corp., “Speedster7t 机器学习处理用户指南 (UG088)”，2019。'
- en: '[116] A. Cairncross, B. Henry, C. Chalmers, D. Reid, J. Shipton, J. Fowler,
    L. Corrigan, and M. Ashby, “AI Benchmarking on Achronix Speedster7t FPGAs,” *White
    Paper*, 2023.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] A. Cairncross, B. Henry, C. Chalmers, D. Reid, J. Shipton, J. Fowler,
    L. Corrigan, 和 M. Ashby, “在 Achronix Speedster7t FPGAs 上的 AI 基准测试，” *白皮书*，2023。'
- en: '[117] M. Langhammer, E. Nurvitadhi, S. Gribok, and B. Pasca, “Stratix 10 NX
    Architecture,” *ACM Transactions on Reconfigurable Technology and Systems (TRETS)*,
    vol. 15, no. 4, pp. 1–32, 2022.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] M. Langhammer, E. Nurvitadhi, S. Gribok, 和 B. Pasca, “Stratix 10 NX 架构，”
    *ACM 可重构技术与系统交易（TRETS）*，第 15 卷，第 4 期，页 1–32，2022。'
- en: '[118] Intel Corp., “Intel Agilex® 5 FPGAs and SoCs Device Overview (762191),”
    2023.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Intel Corp., “Intel Agilex® 5 FPGAs 和 SoCs 设备概述 (762191)”，2023。'
- en: '[119] A. Boutros, E. Nurvitadhi, and V. Betz, “RAD-Sim: Rapid Architecture
    Exploration for Novel Reconfigurable Acceleration Devices,” in *International
    Conference on Field-Programmable Logic and Applications (FPL)*, 2022.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] A. Boutros, E. Nurvitadhi, 和 V. Betz, “RAD-Sim：新型可重构加速设备的快速架构探索，”在 *国际现场可编程逻辑与应用会议（FPL）*，2022。'
- en: '[120] M. S. Abdelfattah and V. Betz, “The Case for Embedded Networks on Chip
    on Field-Programmable Gate Arrays,” *IEEE Micro*, vol. 34, no. 1, pp. 80–89, 2013.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] M. S. Abdelfattah 和 V. Betz, “嵌入式芯片网络在现场可编程门阵列上的应用案例，” *IEEE Micro*，第
    34 卷，第 1 期，页 80–89，2013。'
- en: '[121] X. Jia, Y. Zhang, G. Liu, X. Yang, T. Zhang, J. Zheng, D. Xu, H. Wang,
    R. Zheng, S. Pareek *et al.*, “XVDPU: A High Performance CNN Accelerator on the
    Versal Platform Powered by the AI Engine,” in *International Conference on Field-Programmable
    Logic and Applications (FPL)*, 2022.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] X. Jia, Y. Zhang, G. Liu, X. Yang, T. Zhang, J. Zheng, D. Xu, H. Wang,
    R. Zheng, S. Pareek *等*，“XVDPU：一种基于 AI 引擎的高性能 CNN 加速器”，发表于 *国际现场可编程逻辑与应用会议（FPL）*，2022
    年。'
- en: '[122] T. Zhang, D. Li, H. Wang, Y. Li, X. Ma, W. Luo, Y. Wang, Y. Huang, Y. Li,
    Y. Zhang *et al.*, “A-U3D: A Unified 2D/3D CNN Accelerator on the Versal Platform
    for Disparity Estimation,” in *International Conference on Field-Programmable
    Logic and Applications (FPL)*, 2022.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] T. Zhang, D. Li, H. Wang, Y. Li, X. Ma, W. Luo, Y. Wang, Y. Huang, Y.
    Li, Y. Zhang *等*，“A-U3D：基于 Versal 平台的统一 2D/3D CNN 加速器用于视差估计”，发表于 *国际现场可编程逻辑与应用会议（FPL）*，2022
    年。'
- en: '[123] J. Zhuang, J. Lau, H. Ye, Z. Yang, Y. Du, J. Lo, K. Denolf, S. Neuendorffer,
    A. Jones, J. Hu *et al.*, “CHARM: Composing Heterogeneous Accelerators for Matrix
    Multiply on Versal ACAP Architecture,” in *International Symposium on Field Programmable
    Gate Arrays (FPGA)*, 2023.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] J. Zhuang, J. Lau, H. Ye, Z. Yang, Y. Du, J. Lo, K. Denolf, S. Neuendorffer,
    A. Jones, J. Hu *等*，“CHARM：在 Versal ACAP 架构上组合异构加速器进行矩阵乘法”，发表于 *国际现场可编程门阵列会议（FPGA）*，2023
    年。'
- en: '[124] C. Zhang, T. Geng, A. Guo, J. Tian, M. Herbordt, A. Lit, and D. Tao,
    “H-GCN: A Graph Convolutional Network Accelerator on Versal ACAP Architecture,”
    in *International Conference on Field-Programmable Logic and Applications (FPL)*,
    2022.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] C. Zhang, T. Geng, A. Guo, J. Tian, M. Herbordt, A. Lit 和 D. Tao，“H-GCN：在
    Versal ACAP 架构上的图卷积网络加速器”，发表于 *国际现场可编程逻辑与应用会议（FPL）*，2022 年。'
- en: '[125] P. Chen, P. Manjunath, S. Wijeratne, B. Zhang, and V. Prasanna, “Exploiting
    On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration,”
    in *International Conference on Field-Programmable Logic and Applications (FPL)*,
    2023.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] P. Chen, P. Manjunath, S. Wijeratne, B. Zhang 和 V. Prasanna，“利用 Versal
    架构的片上异质性进行 GNN 推理加速”，发表于 *国际现场可编程逻辑与应用会议（FPL）*，2023 年。'
- en: '[126] E. Taka, A. Arora, K.-C. Wu, and D. Marculescu, “MaxEVA: Maximizing the
    Efficiency of Matrix Multiplication on Versal AI Engine,” *arXiv preprint arXiv:2311.04980*,
    2023.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] E. Taka, A. Arora, K.-C. Wu 和 D. Marculescu，“MaxEVA：最大化 Versal AI 引擎上的矩阵乘法效率”，*arXiv
    预印本 arXiv:2311.04980*，2023 年。'
- en: '[127] N. Brown, “Exploring the Versal AI Engines for Accelerating Stencil-Based
    Atmospheric Advection Simulation,” in *International Symposium on Field Programmable
    Gate Arrays (FPGA)*, 2023.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] N. Brown，“探索 Versal AI 引擎在加速基于模板的大气扩散模拟中的应用”，发表于 *国际现场可编程门阵列会议（FPGA）*，2023
    年。'
- en: '[128] G. Singh, A. Khodamoradi, K. Denolf, J. Lo, J. Gomez-Luna, J. Melber,
    A. Bisca, H. Corporaal, and O. Mutlu, “SPARTA: Spatial Acceleration for Efficient
    and Scalable Horizontal Diffusion Weather Stencil Computation,” in *International
    Conference on Supercomputing (SC)*, 2023.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] G. Singh, A. Khodamoradi, K. Denolf, J. Lo, J. Gomez-Luna, J. Melber,
    A. Bisca, H. Corporaal 和 O. Mutlu，“SPARTA：高效且可扩展的水平扩散天气模板计算的空间加速”，发表于 *国际超级计算会议（SC）*，2023
    年。'
- en: '[129] A. Boutros, E. Nurvitadhi, and V. Betz, “Architecture and Application
    Co-Design for Beyond-FPGA Reconfigurable Acceleration Devices,” *IEEE Access*,
    vol. 10, pp. 95 067–95 082, 2022.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] A. Boutros, E. Nurvitadhi 和 V. Betz，“超越 FPGA 的重构加速器的架构与应用协同设计”，*IEEE
    Access*，第 10 卷，第 95 067–95 082 页，2022 年。'
- en: '[130] A. Boutros, S. More, and V. Betz, “A Whole New World: How to Architect
    Beyond-FPGA Reconfigurable Acceleration Devices?” in *International Conference
    on Field-Programmable Logic and Applications (FPL)*, 2023.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] A. Boutros, S. More 和 V. Betz，“一个全新的世界：如何设计超越 FPGA 的重构加速器？”发表于 *国际现场可编程逻辑与应用会议（FPL）*，2023
    年。'
- en: '[131] S. Srinivasan, A. Boutros, F. Mahmoudi, and V. Betz, “Placement Optimization
    for NoC-Enhanced FPGAs,” in *International Symposium on Field-Programmable Custom
    Computing Machines (FCCM)*, 2023.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] S. Srinivasan, A. Boutros, F. Mahmoudi 和 V. Betz，“针对 NoC 增强型 FPGA 的布局优化”，发表于
    *国际现场可编程定制计算机会议（FCCM）*，2023 年。'
- en: '[132] E. Nurvitadhi, J. Cook, A. Mishra, D. Marr, K. Nealis, P. Colangelo,
    A. Ling, D. Capalija, U. Aydonat, A. Dasu *et al.*, “In-Package Domain-Specific
    ASICs for Intel Stratix 10 FPGAs: A Case Study of Accelerating Deep Learning using
    TensorTile ASIC,” in *International Conference on Field Programmable Logic and
    Applications (FPL)*, 2018.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] E. Nurvitadhi, J. Cook, A. Mishra, D. Marr, K. Nealis, P. Colangelo,
    A. Ling, D. Capalija, U. Aydonat, A. Dasu *等*，“针对 Intel Stratix 10 FPGA 的封装内领域专用
    ASIC：使用 TensorTile ASIC 加速深度学习的案例研究”，发表于 *国际现场可编程逻辑与应用会议（FPL）*，2018 年。'
- en: '[133] D. Ingerly *et al.*, “Foveros: 3D Integration and the Use of Face-to-Face
    Chip Stacking for Logic Devices,” in *IEEE International Electron Devices Meeting
    (IEDM)*, 2019.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] D. Ingerly *等*，“Foveros: 3D 集成及面对面芯片堆叠在逻辑设备中的应用，”发表于 *IEEE 国际电子器件会议 (IEDM)*，2019
    年。'
- en: '[134] L. Su, “AMD Keynote,” in *Consumer Electronics Show (CES)*, 2023. [Online].
    Available: [https://tinyurl.com/amdceskeynote](https://tinyurl.com/amdceskeynote)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] L. Su，“AMD 主题演讲，”发表于 *消费者电子展 (CES)*，2023 年。[在线]. 可用: [https://tinyurl.com/amdceskeynote](https://tinyurl.com/amdceskeynote)'
- en: IX Biography Section
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 传记部分
- en: '| ![[Uncaptioned image]](img/53f66c4e52ca7f36aa8a14b4c81fee22.png) | Andrew
    Boutros received his B.Sc. degree in electronics engineering from the German University
    in Cairo in 2016, and his M.A.Sc. degree in electrical and computer engineering
    from the University of Toronto in 2018\. He was a research scientist at Intel’s
    Accelerator Architecture Lab in Oregon before returning to the University of Toronto,
    where he is currently pursuing his Ph.D. degree under the supervision of Prof.
    Vaughn Betz. His research interests include FPGA architecture and CAD, deep learning
    acceleration, and next-generation reconfigurable acceleration devices. He is an
    affiliate of the Intel/VMware Crossroads 3D-FPGA Academic Research Center and
    the Center for Spatial Computational Learning. He has more than 30 publications
    in top conferences and journals in the field of FPGAs, including 4 best paper
    awards. |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/53f66c4e52ca7f36aa8a14b4c81fee22.png) | 安德鲁·布特罗斯于2016年获得开罗德国大学电子工程学士学位，并于2018年获得多伦多大学电气与计算机工程硕士学位。他曾在俄勒冈州英特尔加速器架构实验室担任研究科学家，之后回到多伦多大学，目前在沃恩·贝茨教授的指导下攻读博士学位。他的研究兴趣包括FPGA架构和CAD、深度学习加速以及下一代可重构加速设备。他是英特尔/VMware
    Crossroads 3D-FPGA学术研究中心和空间计算学习中心的成员。他在FPGA领域的顶级会议和期刊上发表了30多篇论文，包括4篇最佳论文奖。'
- en: '| ![[Uncaptioned image]](img/4cb7018ae4e819ad2855126a30dc8be6.png) | Aman Arora
    received his B.Tech. degree in electronics and communications engineering from
    the National Institute of Technology Kurukshetra in 2007, and his M.S. and Ph.D.
    in electrical and computer engineering from the University of Texas at Austin
    in 2012 and 2023, respectively. He is currently an assistant professor at Arizona
    State University, where he leads a research lab that focuses on next-generation
    FPGA architectures and hardware for machine learning. During his graduate school,
    he focused on optimizing FPGA architecture for deep learning workloads. He has
    received a best paper at FCCM 2022 and has over 10 years of experience in the
    semiconductor industry in design, verification, testing and architecture roles.
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/4cb7018ae4e819ad2855126a30dc8be6.png) | 阿曼·阿罗拉于2007年获得国立技术学院库鲁克舍特拉电子与通信工程学士学位，并于2012年和2023年分别在德克萨斯大学奥斯汀分校获得电气与计算机工程硕士和博士学位。他目前是亚利桑那州立大学的助理教授，领导一个研究实验室，专注于下一代FPGA架构和机器学习硬件。在研究生阶段，他专注于优化FPGA架构以处理深度学习工作负载。他曾在FCCM
    2022获得最佳论文奖，并在半导体行业拥有超过10年的设计、验证、测试和架构经验。'
- en: '| ![[Uncaptioned image]](img/ecf162258c275209d7138346586fa808.png) | Vaughn
    Betz received his B.Sc. degree in electrical engineering from the University of
    Manitoba in 1991, his M.S. degree in electrical and computer engineering from
    the University of Illinois at Urbana–Champaign in 1993, and his Ph.D. degree in
    electrical and computer engineering from the University of Toronto in 1998\. He
    is the original developer of the widely used VPR FPGA CAD flow. He co-founded
    Right Track CAD to commercialize VPR, and joined Altera upon its acquisition of
    Right Track CAD. He spent 11 years at Altera, ultimately as Senior Director of
    software engineering, and is one of the architects of the Quartus CAD system and
    the first five generations of the Stratix and Cyclone FPGA families. He is currently
    a professor and the NSERC/Intel Industrial Research Chair in Programmable Silicon
    at the University of Toronto. He holds 102 US patents and has published over 100
    technical articles in the FPGA area, sixteen of which have won best or most significant
    paper awards. He is a Fellow of the IEEE, the ACM, the National Academy of Inventors,
    and the Engineering Institute of Canada. He is also a Faculty Affiliate of the
    Vector Institute for Artificial Intelligence. |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/ecf162258c275209d7138346586fa808.png) | Vaughn Betz 于 1991
    年获得曼尼托巴大学电气工程学士学位，1993 年获得伊利诺伊大学厄尔巴纳-香槟分校电气与计算机工程硕士学位，并于 1998 年获得多伦多大学电气与计算机工程博士学位。他是广泛使用的
    VPR FPGA CAD 流程的原始开发者。他共同创办了 Right Track CAD，以商业化 VPR，并在 Right Track CAD 被收购后加入了
    Altera。他在 Altera 工作了 11 年，最终担任软件工程高级总监，并且是 Quartus CAD 系统和前五代 Stratix 和 Cyclone
    FPGA 系列的架构师之一。他目前是多伦多大学可编程硅 NSERC/Intel 工业研究主席的教授。他持有 102 项美国专利，并在 FPGA 领域发表了
    100 多篇技术文章，其中十六篇获得了最佳或最重要论文奖。他是 IEEE、ACM、国家发明家学会和加拿大工程学会的会士。他还是向量人工智能研究所的教员附属成员。'
