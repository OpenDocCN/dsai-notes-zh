- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:52:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:52:37'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2107.09287] Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking
    and Steganography'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2107.09287] 利用深度学习的数据隐藏：统一数字水印和隐写的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2107.09287](https://ar5iv.labs.arxiv.org/html/2107.09287)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2107.09287](https://ar5iv.labs.arxiv.org/html/2107.09287)
- en: 'Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and
    Steganography'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用深度学习的数据隐藏：统一数字水印和隐写的综述
- en: Zihan Wang CSIRO’s Data61, Australia The University of Queensland, Australia
    Olivia Byrnes The University of Adelaide, Australia Hu Wang The University of
    Adelaide, Australia Ruoxi Sun CSIRO’s Data61, Australia Congbo Ma The University
    of Adelaide, Australia Huaming Chen The University of Sydney, Australia Qi Wu
    The University of Adelaide, Australia Minhui Xue CSIRO’s Data61, Australia
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 王子涵 CSIRO’s Data61，澳大利亚 昆士兰大学，澳大利亚 奥利维亚·伯恩斯 阿德莱德大学，澳大利亚 胡望 阿德莱德大学，澳大利亚 容玺 CSIRO’s
    Data61，澳大利亚 毛聪博 阿德莱德大学，澳大利亚 陈华明 悉尼大学，澳大利亚 齐吴 阿德莱德大学，澳大利亚 许敏辉 CSIRO’s Data61，澳大利亚
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The advancement of secure communication and identity verification fields has
    significantly increased through the use of deep learning techniques for data hiding.
    By embedding information into a noise-tolerant signal such as audio, video, or
    images, digital watermarking and steganography techniques can be used to protect
    sensitive intellectual property and enable confidential communication, ensuring
    that the information embedded is only accessible to authorized parties. This survey
    provides an overview of recent developments in deep learning techniques deployed
    for data hiding, categorized systematically according to model architectures and
    noise injection methods. The objective functions, evaluation metrics, and datasets
    used for training these data hiding models are comprehensively summarised. Additionally,
    potential future research directions that unite digital watermarking and steganography
    on software engineering to enhance security and mitigate risks are suggested and
    deliberated. This contribution furthers the creation of a more trustworthy digital
    world and advances Responsible AI.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用深度学习技术进行数据隐藏，安全通信和身份验证领域的进步显著提升。通过将信息嵌入到耐噪声的信号中，如音频、视频或图像，数字水印和隐写技术可以用于保护敏感的知识产权并实现保密通信，确保嵌入的信息仅对授权方可访问。本综述概述了近年来深度学习技术在数据隐藏中的应用，系统地按模型架构和噪声注入方法进行分类。目标函数、评估指标和用于训练这些数据隐藏模型的数据集被全面总结。此外，提出并探讨了结合数字水印和隐写的潜在未来研究方向，以提升安全性和降低风险。这一贡献进一步推动了更值得信赖的数字世界的创建，并推动负责任的人工智能的进步。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Artificial intelligence, Cybersecurity, Software engineering.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能，网络安全，软件工程。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Serious concerns are raised about the ability of Artificial Intelligence to
    make responsible decisions or behave responsibly, such as generating unfair outcomes,
    causing job displacement or insufficient protection of privacy and data security.
    In response, Responsible AI aims to address these issues and create accountability
    for AI systems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 关于人工智能是否能做出负责任的决策或行为负责任，提出了严重的担忧，例如产生不公平的结果、导致工作岗位流失或隐私和数据安全保护不足。作为回应，负责任的人工智能旨在解决这些问题，并为人工智能系统创建问责机制。
- en: 'Data hiding is considered a promising method to achieve data security towards
    Responsible AI. Typically, data hiding involves concealing information in a specific
    form within another type of media. It can take different forms, such as encoding
    confidential information into an existing text piece or embedding audio files
    into a digital image. As digital assets become more diverse and ubiquitous, the
    importance and scope of data hiding applications will only continue to grow [[1](#bib.bib1)].
    In today’s digital age, as digital communication and multimedia data become increasingly
    prevalent, the data hiding process has become crucial. Ensuring responsible AI,
    such as machine learning as services, per requisite, necessitates secure communication
    across all mediums, and the accountability of responsible AI, such as digital
    intellectual property, necessitates protection against theft and misuse. In its
    traditional form, the data hiding process can be categorized into three types:
    watermarking, steganography, and cryptography [[2](#bib.bib2)]. This survey focuses
    on the former two (watermarking and steganography), as both have demonstrated
    superior capability in safeguarding sensitive data for AI systems learning.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐藏被认为是实现负责任AI数据安全的一种有前景的方法。通常，数据隐藏涉及将信息以特定形式隐藏在另一种媒体中。它可以采用不同的形式，例如将机密信息编码到现有的文本中，或将音频文件嵌入数字图像中。随着数字资产变得更加多样化和普及，数据隐藏应用的重要性和范围只会不断增长[[1](#bib.bib1)]。在今天的数字时代，随着数字通信和多媒体数据的日益普及，数据隐藏过程变得至关重要。确保负责任的AI，例如作为服务的机器学习，按要求，需要在所有媒介中进行安全通信，而负责任AI的责任，例如数字知识产权，需保护免受盗窃和滥用。在其传统形式中，数据隐藏过程可以分为三类：水印、隐写术和加密[[2](#bib.bib2)]。本调查关注前两类（水印和隐写术），因为它们在保护AI系统学习的敏感数据方面表现出优越的能力。
- en: Digital watermarking utilizes data hiding techniques to embed an identification
    (ID) into digital media that communicates with the owners of the intellectual
    property (IP) to prevent unauthorized copying or alteration. Thus, in case of
    any attempt to copy or modify the original media, the ID can be extracted to identify
    the owners. The primary application of digital watermarking is for the authentication
    of digital assets, however, the process has also been used for licences and identification
    [[3](#bib.bib3)], digital forensics [[4](#bib.bib4)], and data protection in smart
    cities [[5](#bib.bib5), [6](#bib.bib6)]. Digital watermarking is not only useful
    for marking images and documents, but also for real-time audios and videos [[7](#bib.bib7),
    [8](#bib.bib8)], languages [[9](#bib.bib9)], as well as chip and hardware protection
    in electronics [[10](#bib.bib10)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数字水印利用数据隐藏技术将标识（ID）嵌入数字媒体中，以与知识产权（IP）的所有者进行通信，防止未经授权的复制或修改。因此，如果尝试复制或修改原始媒体，ID可以被提取以识别所有者。数字水印的主要应用是数字资产的认证，但该过程也被用于许可证和身份识别[[3](#bib.bib3)]、数字取证[[4](#bib.bib4)]以及智能城市中的数据保护[[5](#bib.bib5),
    [6](#bib.bib6)]。数字水印不仅对标记图像和文档有用，还对实时音频和视频[[7](#bib.bib7), [8](#bib.bib8)]、语言[[9](#bib.bib9)]以及电子产品中的芯片和硬件保护[[10](#bib.bib10)]有用。
- en: Steganography and watermarking share similarities in that both involve embedding
    data into a piece of media. However, while watermarking aims to identify the creator
    of an artifact, steganography embeds secret messages in a way that avoids detection,
    interception, or decoding. Unlike cryptography, which is designed to secure data
    by taking advantage of complexity, steganography’s primary goal is to keep the
    cover media’s format readable and not distorted after data hiding. The public
    should still be able to see the original cover media without noticing the embedded
    messages. Steganography is applied in various industries, including medicine,
    defense, and multimedia fields, wherever confidentiality is crucial for secure
    communication [[11](#bib.bib11)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 隐写术和水印技术有相似之处，均涉及将数据嵌入媒体中。然而，水印的目的是识别工件的创作者，而隐写术则以避免被检测、拦截或解码的方式嵌入秘密信息。与加密技术不同，加密旨在利用复杂性来保护数据，隐写术的主要目标是确保隐藏数据后，覆盖媒体的格式可读且未失真。公众仍应能够看到原始的覆盖媒体，而不注意到嵌入的消息。隐写术被应用于包括医学、国防和多媒体领域在内的各个行业，凡是机密性对安全通信至关重要的地方[[11](#bib.bib11)]。
- en: '![Refer to caption](img/39bc88fc61e8da39e502501d1c534f5e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/39bc88fc61e8da39e502501d1c534f5e.png)'
- en: 'Figure 1: A hierarchical diagram showing different methods for classifying
    deep learning-based data hiding techniques. Blindness refers to the functionality
    of the data hiding method, further explained in Section [II](#S2 "II Problem Statement
    ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography").'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：一个层次图展示了不同的深度学习数据隐藏技术分类方法。盲性指的是数据隐藏方法的功能，详细说明见第[II](#S2 "II Problem Statement
    ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography)节。'
- en: Historically, data hiding has been accomplished using specialized algorithms
    that are classified based on the domain in which they operate, either spatial
    or frequency. Spatial domain techniques directly embed data into the cover media
    by manipulating bit streams or pixel values. They are computationally simple compared
    to other techniques, and hence are more susceptible to removal and distortion
    from adversaries. Frequency domain techniques rely on the manipulation of frequency
    coefficients in the signal medium. These techniques achieve a higher degree of
    robustness to attacks, but are more computationally complex [[1](#bib.bib1)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，数据隐藏使用了根据其操作领域（空间域或频率域）分类的专门算法。空间域技术通过操控比特流或像素值直接将数据嵌入到覆盖媒体中。与其他技术相比，它们在计算上较为简单，因此更容易受到对手的移除和失真影响。频率域技术依赖于信号介质中频率系数的操控。这些技术在抗攻击方面具有更高的鲁棒性，但计算上更为复杂[[1](#bib.bib1)]。
- en: The drawback of these traditional algorithms is that their applications are
    narrow, and the creators of these algorithms require expert knowledge of the embedding
    process. Particular techniques are useful for certain limited tasks, and the growing
    sophistication of watermark removal and degradation attacks means that the effectiveness
    of these algorithms may be compromised in the near future [[12](#bib.bib12), [13](#bib.bib13)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些传统算法的缺点在于其应用范围狭窄，并且这些算法的创造者需要对嵌入过程有专家级的知识。特定的技术适用于某些有限的任务，随着水印去除和退化攻击的日益复杂，这些算法的有效性可能在不久的将来受到影响[[12](#bib.bib12),
    [13](#bib.bib13)]。
- en: New advancements in the field of deep learning span many industries due to the
    strong representation abilities of deep neural networks. In the field of data
    hiding, deep learning models provide adaptable, generalized frameworks that can
    be used for a variety of applications in watermarking and steganography. Currently,
    most works in this area concentrate on image-based data hiding, and these are
    the works that will be compared in this survey. These machine learning models
    are able to learn advanced embedding patterns that are able to resist a much wider
    range of attacks with far more effectiveness than traditional watermarking or
    steganography algorithms [[12](#bib.bib12)]. Further research on [[14](#bib.bib14),
    [15](#bib.bib15), [9](#bib.bib9), [16](#bib.bib16)] established the potential
    and criticality of developing generalized frameworks for data hiding that can
    work with a range of cover media types to robustly embed data and provide highly
    secure content authentication and communication services. The advantage of the
    deep learning approach is that networks can be retrained to become resistant to
    new types of attacks, or to emphasise particular goals such as payload capacity
    or imperceptibility without creating specialised algorithms for each new application
    [[16](#bib.bib16)]. An additional advantage of deep data hiding techniques is
    that they can enhance the security of the embedded messages. The high non-linearity
    of deep neural models makes it virtually impossible for an adversary to retrieve
    the embedded information [[12](#bib.bib12)]. Compared to traditional methods,
    deep learning-based methods are not only more secure and adaptable to different
    applications, but they also offer enhanced robustness to adversarial attacks and
    distortions. They are also able to achieve more imperceptible forms of data embedding.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域的新进展涉及许多行业，因深度神经网络具有强大的表示能力。在数据隐藏领域，深度学习模型提供了适应性强、通用的框架，可用于水印和隐写等各种应用。目前，该领域的大多数工作集中在基于图像的数据隐藏上，这些工作将在本调查中进行比较。这些机器学习模型能够学习先进的嵌入模式，这些模式比传统的水印或隐写算法更有效地抵抗更广泛的攻击[[12](#bib.bib12)]。进一步的研究[[14](#bib.bib14),
    [15](#bib.bib15), [9](#bib.bib9), [16](#bib.bib16)]确立了开发通用数据隐藏框架的潜力和关键性，这些框架能够处理各种封面媒体类型，以强健地嵌入数据并提供高度安全的内容认证和通信服务。深度学习方法的优势在于网络可以重新训练，以抵御新类型的攻击，或强调特定目标，如有效载荷容量或不可察觉性，而无需为每个新应用创建专门的算法[[16](#bib.bib16)]。深度数据隐藏技术的另一个优势是它们可以增强嵌入消息的安全性。深度神经模型的高度非线性使得对手几乎不可能检索嵌入的信息[[12](#bib.bib12)]。与传统方法相比，基于深度学习的方法不仅更安全，更能适应不同的应用，而且在对抗性攻击和失真方面提供了更强的鲁棒性。同时，它们还能实现更不可察觉的数据嵌入形式。
- en: 'The process of data hiding, comprising message embedding and extraction, can
    be naturally mapped onto an encoder-decoder network architecture. This approach
    involves partitioning the learning model into two networks: the encoder network,
    which learns to embed input messages into images, and the decoder network, which
    extracts the original message from the distorted image after subjecting it to
    various forms of attacks, such as blurring, cropping, compression, etc. The network
    is trained by minimizing an objective function that accounts for the differences
    between the cover image (i.e., the data carrier) and the encoded image, as well
    as the differences between the embedded and extracted input message.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐藏过程，包括消息嵌入和提取，可以自然地映射到编码器-解码器网络架构中。这种方法涉及将学习模型划分为两个网络：编码器网络，它学习将输入消息嵌入到图像中；解码器网络，它从经过各种攻击（如模糊、裁剪、压缩等）处理后的失真图像中提取原始消息。该网络通过最小化目标函数进行训练，该函数考虑了封面图像（即数据载体）和编码图像之间的差异，以及嵌入和提取的输入消息之间的差异。
- en: The first papers exploring the capabilities of neural network technology for
    data hiding were released in 2017, and were based on convolutional neural networks
    [[12](#bib.bib12), [17](#bib.bib17)]. In recent years, GAN-based approaches have
    gained traction, popularised by the HiDDeN model [[16](#bib.bib16)], which was
    the first end-to-end trainable model for digital watermarking and steganography.
    These deep learning models employ different message embedding strategies in order
    to improve robustness, such as using adversarial examples, attention masks, and
    channel coding. The continued development of deep learning-based data hiding models
    will greatly improve the effectiveness and security of digital IP protection,
    and secure secret communication.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首批探索神经网络技术在数据隐藏方面能力的论文发布于2017年，基于卷积神经网络[[12](#bib.bib12), [17](#bib.bib17)]。近年来，基于生成对抗网络（GAN）的方法获得了关注，由HiDDeN模型[[16](#bib.bib16)]推广，该模型是第一个端到端可训练的数字水印和隐写术模型。这些深度学习模型采用不同的消息嵌入策略来提高鲁棒性，例如使用对抗样本、注意力掩模和通道编码。深度学习数据隐藏模型的持续发展将大大提高数字知识产权保护的有效性和安全性，并确保秘密通信。
- en: 'Since this is a relatively new area of research, current surveys on data hiding
    primarily concentrate on traditional algorithms. There are existing works examining
    deep learning-based techniques for steganography and cryptography [[18](#bib.bib18),
    [19](#bib.bib19)], but there is a lack of works examining deep watermarking techniques.
    There is an existing survey looking at deep learning-based watermarking and steganography [[20](#bib.bib20)];
    however, a comprehensive survey regarding deep data hiding models unifying digital
    watermarking and steganography is still lacking. To the best of our knowledge,
    ours is the first survey to examine deep learning techniques for deep learning-based
    digital watermarking and steganography that includes the most extensive range
    of recent works. As this research area continues to expand, it is important to
    summarise and review the current methods. The survey aims to systematically categorise
    and discuss existing deep learning models for data hiding, separated based on
    applications in either watermarking or steganography, as well as present future
    directions that research may take. The key contributions of the survey are listed
    as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个相对较新的研究领域，目前关于数据隐藏的调查主要集中在传统算法上。虽然已有一些研究审视了基于深度学习的隐写术和加密技术[[18](#bib.bib18),
    [19](#bib.bib19)]，但对深度水印技术的研究仍然缺乏。现有的调查研究了基于深度学习的水印和隐写术[[20](#bib.bib20)]；然而，统一数字水印和隐写术的深度数据隐藏模型的全面调查仍然缺乏。据我们所知，本调查是第一个审视深度学习技术用于深度学习基础的数字水印和隐写术的调查，涵盖了最新的广泛工作。随着这一研究领域的不断扩展，总结和审查当前方法是非常重要的。本调查旨在系统地分类和讨论现有的深度学习数据隐藏模型，按照水印或隐写术的应用进行分离，并展示研究可能的发展方向。本调查的主要贡献如下：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey systematically categorises and compares deep learning-based models
    for data hiding in either watermarking or steganography based on network architecture
    and noise injection methods.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查系统地对基于深度学习的数据隐藏模型进行分类和比较，无论是在水印技术还是隐写术方面，基于网络架构和噪声注入方法进行分析。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We comprehensively discuss and compare different objective strategies, evaluation
    metrics, and training datasets used in current state-of-the-art deep data hiding
    techniques.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们全面讨论和比较了当前最先进的深度数据隐藏技术中使用的不同目标策略、评估指标和训练数据集。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We also outline a broad spectrum of potential research avenues for the future
    development of deep learning-based data hiding.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还概述了深度学习数据隐藏未来发展的广泛研究方向。
- en: Paper Collation. Our survey involved the collection, analysis, and discussion
    of over 30 papers retrieved from Google Scholar¹¹1https://scholar.google.com/
    and DBLP²²2https://dblp.org/, using the search keywords data hiding, digital watermarking,
    steganography, deep neural networks, and generative adversarial networks (GANs).
    Those papers were selected from a plethora of top-tier Security and Privacy, Computer
    Vision and Machine Learning conferences and journals.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 论文整理。我们的调查涉及从Google Scholar¹¹1https://scholar.google.com/ 和 DBLP²²2https://dblp.org/
    收集、分析和讨论超过30篇论文，使用的搜索关键词包括数据隐藏、数字水印、隐写术、深度神经网络和生成对抗网络（GAN）。这些论文从顶级的安全与隐私、计算机视觉和机器学习会议及期刊中挑选出来。
- en: 'Organization of the Survey. In the following sections, our survey will cover
    recent advanced deep learning based data hiding methods in two forms: digital
    watermarking and steganography. Section [II](#S2 "II Problem Statement ‣ Data
    Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography")
    gives the problem formulation of data hiding. Section [III](#S3 "III Deep Learning-based
    Data Hiding Techniques ‣ Data Hiding with Deep Learning: A Survey Unifying Digital
    Watermarking and Steganography") highlights the architecture of data hiding and
    offers a comprehensive review of deep learning based data hiding techniques. This
    survey also summarises noise injection techniques in Section [IV](#S4 "IV Noise
    Injection Techniques ‣ Data Hiding with Deep Learning: A Survey Unifying Digital
    Watermarking and Steganography"), objective functions in Section [V](#S5 "V Objective
    Functions ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking
    and Steganography"), evaluation metrics in Section [VI](#S6 "VI Evaluation Metrics
    ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography"),
    and existing datasets in Section [VII](#S7 "VII Datasets ‣ Data Hiding with Deep
    Learning: A Survey Unifying Digital Watermarking and Steganography"). Finally,
    open questions and future work for deep learning based data hiding tasks are discussed
    in Section [VIII](#S8 "VIII Open Questions and Future Work ‣ Data Hiding with
    Deep Learning: A Survey Unifying Digital Watermarking and Steganography"). Section
    [IX](#S9 "IX Conclusion ‣ Data Hiding with Deep Learning: A Survey Unifying Digital
    Watermarking and Steganography") concludes the paper.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '调查组织。在接下来的部分中，我们的调查将覆盖最近的基于深度学习的数据隐藏方法，分为两种形式：数字水印和隐写术。第 [II](#S2 "II Problem
    Statement ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking
    and Steganography") 节给出了数据隐藏的问题描述。第 [III](#S3 "III Deep Learning-based Data Hiding
    Techniques ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking
    and Steganography") 节重点介绍了数据隐藏的架构，并提供了基于深度学习的数据隐藏技术的全面综述。本调查还总结了第 [IV](#S4 "IV
    Noise Injection Techniques ‣ Data Hiding with Deep Learning: A Survey Unifying
    Digital Watermarking and Steganography") 节中的噪声注入技术，第 [V](#S5 "V Objective Functions
    ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography")
    节中的目标函数，第 [VI](#S6 "VI Evaluation Metrics ‣ Data Hiding with Deep Learning: A
    Survey Unifying Digital Watermarking and Steganography") 节中的评估指标，以及第 [VII](#S7
    "VII Datasets ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking
    and Steganography") 节中的现有数据集。最后，第 [VIII](#S8 "VIII Open Questions and Future Work
    ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography")
    节讨论了深度学习基于的数据隐藏任务的开放问题和未来工作。第 [IX](#S9 "IX Conclusion ‣ Data Hiding with Deep
    Learning: A Survey Unifying Digital Watermarking and Steganography") 节总结了本文。'
- en: II Problem Statement
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 问题陈述
- en: When we evaluate the effectiveness of data hiding techniques, there are many
    factors that should be considered. The three most important are capacity, how
    much information can be embedded into the cover media, imperceptibility, how easy
    the data is to detect, and robustness, how resistant the data is to attacks. Here,
    attacks refer to any alterations made to the embedded media with the intent to
    degrade or remove the embedded data. There is an implicit trade-off between these
    three aforementioned characteristics. For instance, if there is a high payload
    capacity, then the message will be easier to detect, resulting in a lower level
    of imperceptibility. Similarly, improving robustness against attacks can potentially
    decrease both payload capacity and imperceptibility, since there is added redundancy
    to the encoded image that allows it to resist distortions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们评估数据隐藏技术的有效性时，应该考虑许多因素。最重要的三个因素是容量，即可以嵌入到载体媒体中的信息量；隐蔽性，即数据被检测到的难易程度；以及鲁棒性，即数据对攻击的抵抗能力。在这里，攻击指的是对嵌入媒体的任何修改，目的是降低或移除嵌入的数据。这三种特性之间存在隐含的权衡。例如，如果负载容量很高，那么信息将更容易被检测到，导致隐蔽性降低。同样，提高对攻击的鲁棒性可能会降低负载容量和隐蔽性，因为编码图像中增加了冗余以抵御失真。
- en: 'In digital watermarking, robustness is generally favoured over secrecy because
    the ability to resist attacks and distortions is more important than the watermark’s
    imperceptibility. Conversely, in steganography, imperceptibility is favoured since
    the highest priority is that the message remains a secret. This relationship is
    illustrated in Figure [2](#S2.F2 "Figure 2 ‣ II Problem Statement ‣ Data Hiding
    with Deep Learning: A Survey Unifying Digital Watermarking and Steganography").
    Due to the adaptable nature of deep learning-based approaches, the trade-off between
    these metrics can be explicitly controlled by the user, and the key properties
    of robustness and imperceptibility underpin the objective of the deep learning
    system.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '在数字水印中，通常更看重鲁棒性而非隐秘性，因为抵御攻击和失真的能力比水印的不可察觉性更为重要。相反，在隐写术中，隐秘性更受青睐，因为最高优先级是信息保持秘密。这个关系在图 [2](#S2.F2
    "Figure 2 ‣ II Problem Statement ‣ Data Hiding with Deep Learning: A Survey Unifying
    Digital Watermarking and Steganography") 中得到了说明。由于基于深度学习的方法具有适应性，用户可以显著控制这些指标之间的权衡，而鲁棒性和隐秘性这两个关键属性支撑着深度学习系统的目标。'
- en: '![Refer to caption](img/eb5867e4386e887f88d7da88916a27ed.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb5867e4386e887f88d7da88916a27ed.png)'
- en: 'Figure 2: A figure showing the trade-off between the three primary data hiding
    properties; robustness, imperceptibility, and capacity, as well as which data
    hiding applications favour each property over the others.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：展示了三种主要数据隐藏属性之间的权衡；鲁棒性、隐秘性和容量，以及各数据隐藏应用偏向的属性。
- en: II-A Digital Watermarking and Steganography Terminology
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 数字水印和隐写术术语
- en: 'The basic data hiding process consists of an encoding and decoding process.
    The encoder $E$ receives the cover media $C$ and message to be hidden $M$, and
    outputs the encoded media $C^{\prime}$ such that: $E(C,M)=C^{\prime}$. Then the
    decoder receives the encoded media as input and extracts the message $M^{\prime}$,
    such that: $D(C^{\prime})=M^{\prime}$. In a robust implementation, $M$ and $M^{\prime}$
    should be as similar as possible in an effective strategy. Similarly, maximising
    the imperceptibility property is done by minimising the difference between $C$
    and $C^{\prime}$. Types of data hiding can be classified based on a variety of
    properties as follows [[1](#bib.bib1)]:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的数据隐藏过程包括编码和解码过程。编码器 $E$ 接收封面媒体 $C$ 和待隐藏的信息 $M$，并输出编码后的媒体 $C^{\prime}$，使得：$E(C,M)=C^{\prime}$。然后，解码器接收编码后的媒体作为输入并提取信息
    $M^{\prime}$，使得：$D(C^{\prime})=M^{\prime}$。在一个鲁棒的实现中，$M$ 和 $M^{\prime}$ 应该在有效策略中尽可能相似。类似地，通过最小化
    $C$ 和 $C^{\prime}$ 之间的差异来最大化隐秘性属性。数据隐藏的类型可以根据各种属性进行分类，如下 [[1](#bib.bib1)]：
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Blindness: related to how much information is required to extract the original
    data from the encoded media. Blind techniques do not require the original cover
    media or original data to extract the embedded data, and are the most practically
    useful. Semi-blind techniques require only the original data, whereas Non-blind
    techniques require both the original cover media and data in order to perform
    data extraction.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 盲性：与从编码媒体中提取原始数据所需的信息量相关。盲技术不需要原始封面媒体或原始数据即可提取嵌入的数据，且最具实际用途。半盲技术只需要原始数据，而非盲技术则需要原始封面媒体和数据以进行数据提取。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fragility: related to how the embedded data reacts to attacks and distortions
    applied to it. Fragile data are designed to show all attacks applied to it so
    that, when extracted, it is possible to verify which attacks have been applied
    to the media. This is useful when verifying the integrity of the media. Semi-fragile
    data are not robust against intentional distortions such as warping and noise
    filtering that attempt to degrade the embedded data, but are robust against content-preserving
    distortions such as compression and enhancement. Therefore, it can be used to
    trace any illegal distortions made to the media.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 脆弱性：与嵌入的数据对攻击和失真反应的程度相关。脆弱的数据设计为显示所有施加的攻击，以便在提取时可以验证对媒体施加了哪些攻击。这在验证媒体的完整性时非常有用。半脆弱的数据对意图性失真（如变形和噪声过滤）不够鲁棒，但对保持内容不变的失真（如压缩和增强）却足够鲁棒。因此，可以用来追踪对媒体所做的任何非法失真。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Visibility: whether the embedded data is visible to the human eye.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可见性：嵌入的数据是否对人眼可见。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Invertibility: whether the embedded data can be removed from the cover media
    once embedded. Invertible data can be removed and non-invertible ones cannot.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可逆性：指嵌入的数据是否可以从封面介质中移除。可逆数据可以被移除，而不可逆数据则不能。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Robustness: the ability of the embedded data to remain unchanged when attacks
    are applied to it.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鲁棒性：嵌入的数据在遭受攻击时保持不变的能力。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Security: determines how difficult it is for an adversarial party to extract
    the data from the cover image.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全性：决定了对抗方从封面图像中提取数据的难易程度。
- en: III Deep Learning-based Data Hiding Techniques
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基于深度学习的数据隐藏技术
- en: Deep learning-based data hiding models utilise the encoder-decoder network structure
    to train models to imperceptibly and robustly hide information. They present an
    advantage over traditional data hiding algorithms because they can be retrained
    to become resistant to a range of attacks, and be applied to different end-use
    scenarios. Deep learning methods negate the need for expert knowledge when crafting
    data hiding algorithms, and improve security due to the black-box nature of deep
    learning models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的数据隐藏模型利用编码器-解码器网络结构来训练模型，以隐秘且稳健地隐藏信息。与传统数据隐藏算法相比，它们具有优势，因为可以重新训练以抵御多种攻击，并应用于不同的最终使用场景。深度学习方法消除了在制定数据隐藏算法时对专家知识的需求，并由于深度学习模型的黑箱特性而提高了安全性。
- en: 'The following section discusses deep learning-based data hiding techniques
    separated into techniques focused on watermarking and steganography. The detection
    and removal mechanisms are then discussed at the end of this section. The classification
    of deep learning-based data hiding techniques detailed in this section is outlined
    in Figure [3](#S3.F3 "Figure 3 ‣ III Deep Learning-based Data Hiding Techniques
    ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography").
    It should be noted that CNNs incorporating adversarial training are different
    to GAN-based methods. Adversarial training in this instance refers to the use
    of trained CNNs for noise injection during the attack simulation stage, while
    GAN-based methods incorporate a discriminator to scrutinise encoded and cover
    images to improve embedding imperceptibility [[21](#bib.bib21)].'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分讨论了基于深度学习的数据隐藏技术，这些技术分为关注水印和隐写术的技术。然后在本节末尾讨论检测和移除机制。本节详细分类的基于深度学习的数据隐藏技术在图[3](#S3.F3
    "图3 ‣ III 基于深度学习的数据隐藏技术 ‣ 基于深度学习的数据隐藏：统一数字水印和隐写术的调查")中概述。需要注意的是，包含对抗训练的CNN与基于GAN的方法不同。在此情况下，对抗训练指的是在攻击模拟阶段使用训练好的CNN进行噪声注入，而基于GAN的方法则结合了一个鉴别器来审查编码和封面图像，以改善嵌入的隐秘性[[21](#bib.bib21)]。
- en: '![Refer to caption](img/be7ef6e4eb959ef7bfec75b776be970d.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/be7ef6e4eb959ef7bfec75b776be970d.png)'
- en: 'Figure 3: A hierarchical diagram showing the classification of deep learning-based
    data hiding models presented in this survey. ‘Adversarial training’ refers to
    attack simulation during training, which includes noise-based attacks generated
    by a trained CNN.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：一个层次结构图，展示了本调查中提出的基于深度学习的数据隐藏模型的分类。‘对抗训练’指的是在训练过程中进行攻击模拟，包括由训练好的CNN生成的噪声攻击。
- en: 'Currently, the majority of new watermarking models use an encoder-decoder architecture
    based on Convolutional Neural Networks (CNNs). A simple diagram showing the deep
    learning-based data hiding process can be found in Figure [4](#S3.F4 "Figure 4
    ‣ III Deep Learning-based Data Hiding Techniques ‣ Data Hiding with Deep Learning:
    A Survey Unifying Digital Watermarking and Steganography").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数新的水印模型使用基于卷积神经网络（CNN）的编码器-解码器架构。基于深度学习的数据隐藏过程的简单示意图可以在图[4](#S3.F4 "图4
    ‣ III 基于深度学习的数据隐藏技术 ‣ 基于深度学习的数据隐藏：统一数字水印和隐写术的调查")中找到。
- en: '![Refer to caption](img/85824f0ad3d5166924ac0e6f5f1acded.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/85824f0ad3d5166924ac0e6f5f1acded.png)'
- en: 'Figure 4: A diagram showing a general encoder-decoder architecture for digital
    watermarking.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：展示数字水印的一般编码器-解码器架构的示意图。
- en: In these models, the encoder embeds data in a piece of cover media; this encoded
    media is subjected to attack simulation, and then the data is extracted by the
    decoder network. Through the iterative learning process, the embedding strategy
    becomes more resistant to the attacks applied during simulation, and the extraction
    process improves the integrity of the extracted data. The advantage of this technique
    over previous traditional algorithms is that they require no expert knowledge
    to program, and can simply be retrained for different applications and attack
    types instead of needing to be designed from scratch. The system exists as a black
    box with high non-linearity where the intricacies of the embedding system are
    unknown and impossible to ascertain. This makes deep learning-based methods highly
    secure, as well as adaptable to different end-use scenarios. Some variations of
    the simple CNN encoder-decoder approach shown above include convolutional auto-encoders,
    and CNNs with adversarial training components. The U-Net CNN architecture is common
    in steganography applications due to image segmentation abilities.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些模型中，编码器将数据嵌入到一块封面媒体中；这种编码媒体会经过攻击模拟，然后数据由解码网络提取。通过迭代学习过程，嵌入策略变得更加抵抗模拟中施加的攻击，而提取过程则提高了提取数据的完整性。这种技术相比于以往的传统算法的优势在于不需要专业知识进行编程，而是可以简单地重新训练以适应不同的应用和攻击类型，而不是需要从头开始设计。系统作为一个高度非线性的黑箱存在，其中嵌入系统的复杂性未知且无法确定。这使得基于深度学习的方法非常安全，并且能够适应不同的最终使用场景。上述简单的CNN编码器-解码器方法的一些变体包括卷积自编码器和具有对抗训练组件的CNN。由于图像分割能力，U-Net
    CNN架构在隐写术应用中很常见。
- en: Many models adopt the Generative Adversarial Network (GAN) structure [[22](#bib.bib22)].
    The GAN framework consists of a generative model and a discriminative model. In
    deep data hiding, the discriminator network is given a mixture of encoded and
    unaltered images and must classify them as such. Throughout the learning process,
    the generative model improves in its data embedding capabilities, producing highly
    imperceptible examples, while the discriminative model improves at identifying
    encoded images. The end point of training is reached when the discriminator can
    only identify legitimately encoded images 50% of the time – it is making random
    guesses. The use of discriminative networks can greatly increase data imperceptibility,
    and is therefore useful for steganography as well as watermarking applications.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型采用生成对抗网络（GAN）结构[[22](#bib.bib22)]。GAN框架由一个生成模型和一个判别模型组成。在深度数据隐匿中，判别网络接收编码过和未修改的图像的混合，并必须对其进行分类。在学习过程中，生成模型在数据嵌入能力方面不断改进，生成的样本几乎不可察觉，而判别模型在识别编码图像方面不断提高。当判别器只能以50%的概率识别真正的编码图像时，训练过程就结束了——它是在进行随机猜测。使用判别网络可以大大增加数据的隐蔽性，因此对于隐写术和水印应用非常有用。
- en: There are also further variations of the GAN framework, including Wasserstein
    GANs (WGANs) and CycleGANs. The CycleGAN architecture is useful for image-to-image
    translation, and includes two generative and two discriminative models. The primary
    benefit of CycleGANs is that the model can be trained without paired examples.
    Instead, the first generator generates images from domain A, and the second from
    domain B, where each generator takes an image from the other domain as input for
    the translation. Then, discriminator A takes as input both images from domain A
    and output images from generator A, and determines whether they are real or fake
    (and vice versa for discriminator B). The resulting architecture is highly useful
    for translating between images.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: GAN框架还有进一步的变体，包括Wasserstein GAN（WGAN）和CycleGAN。CycleGAN架构对于图像到图像的转换非常有用，并包括两个生成模型和两个判别模型。CycleGAN的主要优势在于模型可以在没有配对样本的情况下进行训练。相反，第一个生成器从领域A生成图像，第二个生成器从领域B生成图像，其中每个生成器将来自另一个领域的图像作为输入进行转换。然后，判别器A将领域A的图像和生成器A的输出图像作为输入，确定它们是真实的还是虚假的（反之亦然对于判别器B）。这种架构在图像之间的转换中非常有用。
- en: III-A Deep Learning-based Watermarking Techniques
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于深度学习的水印技术
- en: In this section, current deep learning models for digital watermarking are categorised
    based on their network architecture design.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，当前的数字水印深度学习模型根据其网络架构设计进行分类。
- en: III-A1 Encoder-decoder Framework
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 编码器-解码器框架
- en: 'Due to the encoding and decoding tasks central to the data hiding process,
    the encoder-decoder deep learning framework is well suited for data hiding models.
    The encoder and decoder networks incorporate CNNs, which are used in a variety
    of applications such as detection, recognition, and classification due to their
    unique capabilities in representing data with limited numbers of parameters. The
    layers in the CNN learn non-linear, complex feature sets, representing the inputs
    and outputs to the network using weight-sharing mechanisms. The following deep
    watermarking models adopt the encoder-decoder framework without including a discriminator,
    which characterises GAN-based architectures [[12](#bib.bib12), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [21](#bib.bib21)]. A simple
    diagram of the encoder-decoder deep watermarking structure can be found in Figure
    [4](#S3.F4 "Figure 4 ‣ III Deep Learning-based Data Hiding Techniques ‣ Data Hiding
    with Deep Learning: A Survey Unifying Digital Watermarking and Steganography").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据隐藏过程中编码和解码任务的核心地位，编码器-解码器深度学习框架非常适合数据隐藏模型。编码器和解码器网络结合了CNN，这些网络因其在检测、识别和分类等应用中的独特能力而被广泛使用，这些能力可以用有限数量的参数表示数据。CNN中的层学习非线性、复杂的特征集，通过权重共享机制表示网络的输入和输出。以下深度水印模型采用编码器-解码器框架，但不包括一个特征化GAN架构的判别器[[12](#bib.bib12)、[23](#bib.bib23)、[24](#bib.bib24)、[25](#bib.bib25)、[26](#bib.bib26)、[21](#bib.bib21)]。编码器-解码器深度水印结构的简单示意图可以在图[4](#S3.F4
    "图 4 ‣ III 基于深度学习的数据隐藏技术 ‣ 基于深度学习的数据隐藏：统一数字水印和隐写术的调查")中找到。
- en: Auto-encoder based Model. The encoder-decoder architecture is a general framework
    and auto-encoder is a special case of the encoder-decoder structure. However,
    auto-encoder is usually used in unsupervised-learning scenarios by reconstructing
    the inputs. The potential of the CNN-based encoder-decoder frameworks for digital
    image watermarking was first explored in [[12](#bib.bib12)], which uses two traditional
    Convolutional Auto-Encoders for watermark embedding and extraction. Auto-encoder
    based CNNs were chosen based on their uses in feature extraction and denoising
    in visual tasks, such as facial recognition and generation, and reconstructing
    handwritten digits. The intuition was that the auto-encoder CNN would be able
    to represent the watermark input in a form that was highly imperceptible when
    encoded within the cover image. This early CNN-based method applies two deep auto-encoders
    to rearrange the cover image pixels at the bit level to create a watermarked image.
    The technique was found to outperform the most robust traditional frequency domain
    methods in terms of both robustness and imperceptibility. Although promising for
    the future of deep watermarking, this technique is non-blind, and therefore not
    practically useful.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于自编码器的模型。编码器-解码器架构是一个通用框架，自编码器是编码器-解码器结构的一个特殊情况。然而，自编码器通常用于通过重建输入的无监督学习场景。CNN-based编码器-解码器框架在数字图像水印中的潜力首次在[[12](#bib.bib12)]中探索，该框架使用两个传统的卷积自编码器进行水印嵌入和提取。基于自编码器的CNN被选择用于特征提取和视觉任务中的去噪，如面部识别和生成，以及重建手写数字。直觉是，自编码器CNN能够以一种在嵌入到封面图像中时几乎不可察觉的形式表示水印输入。这种早期的CNN-based方法应用两个深度自编码器在位级别重新排列封面图像像素，以创建水印图像。该技术在鲁棒性和不可察觉性方面优于最强的传统频域方法。尽管对深度水印的未来充满希望，但该技术是非盲的，因此在实际中不够有用。
- en: 'Robust and blind digital watermarking results can be achieved using a relatively
    shallow network, as was shown by WMNet [[24](#bib.bib24)]. The watermarking process
    is separated into three stages: watermark embedding, attack simulation, where
    the CNN adaptively captures the robust features of various attacks, and updating,
    where the model’s weights are updated in order to minimise the loss function and
    thereby correctly extract the watermark message. Embedding is achieved by increasingly
    changing an image block to represent a watermark bit. The model is trained to
    extract watermark bits from the image blocks after attack simulations have been
    applied.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用相对较浅的网络实现鲁棒且盲的数字水印结果，正如WMNet [[24](#bib.bib24)] 所示。水印过程分为三个阶段：水印嵌入、攻击模拟，其中CNN自适应地捕捉各种攻击的鲁棒特征，以及更新，其中模型的权重被更新以最小化损失函数，从而正确提取水印消息。通过逐渐改变图像块来表示水印位来实现嵌入。模型经过训练，从应用攻击模拟后的图像块中提取水印位。
- en: The back-propagation embedding technique utilised in WMNet [[24](#bib.bib24)]
    used only a single detector network, which was found to cause performance degradation
    if the gradient computation in the backpropagation operation was affected by batch
    normalisation (BN). This deficiency was improved by adding an auto-encoder network
    as well as visual masking to allow flexible control of watermark visibility and
    robustness. The auto-encoder network was added to the encoder, and subsequently
    shorted time taken for both embedding and detection at the encoder because the
    feed-forward operation is generally much faster than back-propagation. These improvements
    were established in their follow-up work.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: WMNet [[24](#bib.bib24)] 中使用的反向传播嵌入技术仅使用了单个检测器网络，发现如果反向传播操作中的梯度计算受到批量归一化（BN）的影响，会导致性能下降。通过添加自编码器网络以及视觉遮罩来改善这一缺陷，以允许灵活控制水印的可见性和鲁棒性。自编码器网络被添加到编码器中，从而缩短了嵌入和检测所需的时间，因为前馈操作通常比反向传播要快。这些改进在其后续工作中得到了确认。
- en: Robustness Controls and Input Preprocessing. Subsequent works after the aforementioned
    early techniques in [[12](#bib.bib12), [24](#bib.bib24)] focused on generalising
    the watermarking process for multiple applications. Mechanisms such as robustness
    controls to influence the robustness/imperceptibility trade-off was introduced
    to gear models toward both watermarking and steganography applications, and mechanisms
    such as host and watermark adaptability were developed to pre-process inputs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性控制和输入预处理。在上述早期技术[[12](#bib.bib12), [24](#bib.bib24)]之后，后续工作集中于将水印过程概括到多个应用中。引入了诸如鲁棒性控制等机制，以影响鲁棒性/不可感知性的权衡，旨在使模型适用于水印和隐写术应用，并开发了诸如主机和水印适应性等机制来预处理输入。
- en: A blind and robust watermarking technique was achieved using the CNN-based system [[23](#bib.bib23)].
    The aim of this model is to generalise the watermarking process by training a
    deep neural network to learn the general rules of watermark embedding and extraction
    so that it can be used for a range of applications and combat unexpected distortions.
    The network structure is characterised by an invariance layer that functions to
    tolerate distortions not seen during network training. This layer uses a regularisation
    term to achieve sparse neuron activation, which enhances watermark robustness
    and computational efficiency. The layer also includes a redundancy parameter that
    can be adjusted to increase levels of redundancy in the resulting image, giving
    the model a higher tolerance of errors and increasing robustness. The primary
    aim of these features is to generalise watermarking rules without succumbing to
    overfitting. The model was compared with two auto-encoder CNN methods [[12](#bib.bib12),
    [24](#bib.bib24)], and was found to achieve greater robustness due to the new
    features it adopted.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用基于 CNN 的系统实现了一种盲目且强健的水印技术[[23](#bib.bib23)]。该模型的目的是通过训练深度神经网络学习水印嵌入和提取的一般规则，从而使其能够用于各种应用并应对意外的失真。网络结构的特点是一个不变性层，该层能够容忍在网络训练过程中未见的失真。该层使用正则化项来实现稀疏的神经元激活，这增强了水印的鲁棒性和计算效率。该层还包括一个冗余参数，可调整以增加结果图像中的冗余水平，从而使模型具有更高的错误容忍度并增加鲁棒性。这些特性的主要目的是使水印规则得到概括，而不至于过拟合。该模型与两种自编码器
    CNN 方法[[12](#bib.bib12), [24](#bib.bib24)]进行了比较，发现由于采用了新特性，其鲁棒性得到了提高。
- en: ReDMark [[25](#bib.bib25)] uses two Full Convolutional Neural Networks (FCNs)
    for embedding and extraction along with a differentiable attack layer to simulate
    different distortions, creating an end-to-end training scheme. ReDMark is capable
    of learning many embedding patterns in different transform domains and can be
    trained for specific attacks, or against a range of attacks. The model also includes
    a diffusion mechanism based on circular convolutional layers, allowing watermark
    data to be diffused across a wide area of an image rather than being confined
    to one image block. This improves robustness against heavy attacks, because if
    one image block is cropped out or corrupted, the watermark can still be recovered.
    The trade-off between robustness and imperceptibility can be controlled via a
    strength factor that can influence the pattern strength of the embedding network.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ReDMark [[25](#bib.bib25)] 使用两个全卷积神经网络（FCNs）进行嵌入和提取，并结合一个可微分攻击层来模拟不同的失真，创建了一个端到端的训练方案。ReDMark
    能够在不同的变换域中学习许多嵌入模式，并可以针对特定攻击或一系列攻击进行训练。该模型还包括一个基于圆形卷积层的扩散机制，使水印数据能够在图像的广泛区域内扩散，而不是局限于一个图像块。这提高了对强攻击的鲁棒性，因为即使一个图像块被裁剪或损坏，水印仍然可以被恢复。鲁棒性和不可察觉性之间的权衡可以通过一个强度因子来控制，这个因子可以影响嵌入网络的模式强度。
- en: The watermarking model developed by Lee et al.[[26](#bib.bib26)] uses a simple
    CNN for both embedding and extraction, without using any resolution-dependent
    layers. This allows for host image resolution adaptability – meaning that images
    of any resolution can be used as input to the system to be watermarked. There
    is an image pre-processing network that can adapt images of any resolution for
    the watermarking process. There is also watermark pre-processing, meaning the
    system can handle user-defined watermark data. This is achieved by using random
    binary data as the watermark that is updated at each iteration of training. The
    model also adopts a strength scaling factor, which allows for the controllability
    of the trade-off between robustness and imperceptibility. The method showed comparable,
    if not better, performance compared to ReDMark [[25](#bib.bib25)], and two generative
    adversarial-based models [[16](#bib.bib16), [27](#bib.bib27)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Lee 等人开发的水印模型[[26](#bib.bib26)] 使用一个简单的 CNN 进行嵌入和提取，而不使用任何分辨率依赖的层。这允许宿主图像分辨率的适应性——意味着任何分辨率的图像都可以作为输入进行水印处理。该模型具有一个图像预处理网络，可以将任何分辨率的图像适应于水印处理过程。还有水印预处理，意味着系统可以处理用户定义的水印数据。这是通过使用随机二进制数据作为水印，在每次训练迭代中更新来实现的。该模型还采用了强度缩放因子，这使得鲁棒性和不可察觉性之间的权衡具有可控性。该方法表现出与
    ReDMark [[25](#bib.bib25)] 和两个基于生成对抗的模型 [[16](#bib.bib16), [27](#bib.bib27)] 相当甚至更好的性能。
- en: Adversarial Training. A further improvement to the CNN-based encoder-decoder
    framework was made by adopting trained CNNs for attack simulation. While many
    other works use a fixed pool of attacks or a differentiable attack layer, using
    a trained CNN to generate attacks can greatly improve robustness, and introduces
    an adversarial component to model training. The focus of the Distortion Agnostic
    (DA) model [[21](#bib.bib21)] was to directly improve the HiDDeN model [[16](#bib.bib16)],
    primarily through adding robustness to the watermarking system in situations where
    the model is trained on a combination of distortions rather than one predetermined
    type. Instead of explicitly modelling different distortions during training from
    a fixed pool, the distortions are generated via adversarial training by a trained
    CNN. This technique was found to perform better in terms of robustness than HiDDeN
    [[16](#bib.bib16)] when distortions not seen during training were applied to images.
    The DA framework also incorporates channel coding, a means of detecting and correcting
    errors during signal transmission, to add an additional layer of robustness to
    images by injecting extra redundancy. The watermark message is initially fed through
    the channel encoder to add redundancy before being input into the encoder model.
    Similarly, prior to extraction the redundant watermark message is input to a channel
    decoder to retrieve the final message.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性训练。通过采用训练的CNN进行攻击模拟，对基于CNN的编码器-解码器框架进行了进一步改进。虽然许多其他工作使用固定的攻击池或可微分攻击层，但使用训练的CNN生成攻击可以大大提高鲁棒性，并为模型训练引入对抗性组件。Distortion
    Agnostic（DA）模型[[21](#bib.bib21)]的重点是直接改进HiDDeN模型[[16](#bib.bib16)]，主要通过在模型训练过程中添加对水印系统的鲁棒性，特别是在模型训练过程中处理多种失真而非单一预定类型的情况下。与从固定池中显式建模不同，失真通过训练的CNN进行对抗性训练生成。当应用训练过程中未见过的失真到图像时，发现DA框架在鲁棒性方面表现优于HiDDeN[[16](#bib.bib16)]。DA框架还结合了信道编码，即在信号传输过程中检测和纠正错误的方法，通过注入额外的冗余来增加图像的额外鲁棒性。水印消息最初通过信道编码器添加冗余，然后输入到编码器模型中。类似地，在提取之前，冗余的水印消息会输入到信道解码器中以检索最终消息。
- en: III-A2 Generative Adversarial Networks
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 生成对抗网络
- en: The second primary approach for deep watermarking uses GANs, building upon the
    aforementioned techniques. Current models that adopt the GAN framework include
    [[16](#bib.bib16), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [27](#bib.bib27)].
    Many of the following models also use CNNs within their network architecture,
    but their use of the generative and discriminative components of the GAN framework
    set them apart from the aforementioned implementations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 深度水印的第二种主要方法使用GAN，基于前述技术。目前采用GAN框架的模型包括[[16](#bib.bib16)、[28](#bib.bib28)、[29](#bib.bib29)、[30](#bib.bib30)、[27](#bib.bib27)]。许多以下模型也在其网络架构中使用CNN，但它们对GAN框架生成和判别组件的使用使其与前述实现区别开来。
- en: The first end-to-end trainable framework for data hiding was a model called
    HiDDeN [[16](#bib.bib16)], which uses an adversarial discriminator to improve
    performance. It was a highly influential paper that has informed the development
    of deep watermarking models since its release. The model consists of an encoder
    network, trained to embed an encoded bit string in a cover image whilst minimising
    perceptual perturbations, a decoder network, which receives the encoded image
    and attempts to extract the information, and an adversary network, which predicts
    whether or not an image has been encoded.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个端到端可训练的数据隐藏框架是一个名为HiDDeN的模型[[16](#bib.bib16)]，它使用对抗性判别器来提高性能。这是一篇具有高度影响力的论文，自发布以来对深度水印模型的发展产生了重要影响。该模型由一个编码器网络组成，训练以在覆盖图像中嵌入编码位串，同时最小化感知扰动；一个解码器网络，接收编码图像并尝试提取信息；以及一个对抗性网络，预测图像是否已被编码。
- en: HiDDeN [[16](#bib.bib16)] uses a novel embedding strategy based on adversarial
    examples. When neural networks classify image examples to a particular target
    class, invisible perturbations in the image can fool the network into misclassifying
    that example [[31](#bib.bib31)]. These perturbations have been shown to remain
    preserved when exposed to a variety of image transformations [[32](#bib.bib32)].
    Adversarial examples are ordinarily a deficiency in neural networks since they
    reduce classification accuracy. However, since meaningful information can be extracted
    from imperceptible image perturbations, it was theorised that in a similar fashion
    meaningful information could be encoded in adversarial distortions and used as
    a watermark embedding strategy. This embedding technique, paired with the GAN
    framework, is able to achieve a higher payload capacity (measured in bits per
    pixel) than other common data hiding mechanisms such as Highly Undetectable Steganography
    (HUGO) [[33](#bib.bib33)], Wavelet Obtained Weights (WOW) [[34](#bib.bib34)],
    and S-UNIWARD [[35](#bib.bib35)]. One drawback of HiDDeN [[16](#bib.bib16)] was
    that loss between encoded and decoded messages was minimised when trained only
    on a specific kind of attack compared to a combination of different attack types.
    This shows that the model is best when trained specifically to combat one type
    of attack, but not as effective when trained on a variety.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: HiDDeN [[16](#bib.bib16)] 使用了一种基于对抗样本的新型嵌入策略。当神经网络将图像样本分类到特定目标类别时，图像中的不可见扰动可能会使网络错误分类该样本
    [[31](#bib.bib31)]。这些扰动已被证明在各种图像变换下能够保持不变 [[32](#bib.bib32)]。对抗样本通常是神经网络的缺陷，因为它们降低了分类准确性。然而，由于可以从不可察觉的图像扰动中提取有意义的信息，因此有人推测，可以以类似的方式将有意义的信息编码在对抗性失真中，并用作水印嵌入策略。这种嵌入技术结合了GAN框架，能够比其他常见的数据隐藏机制（如高度不可检测的隐写术（HUGO）[[33](#bib.bib33)]、小波获得的权重（WOW）[[34](#bib.bib34)]和S-UNIWARD
    [[35](#bib.bib35)]）实现更高的负载能力（以每像素位数衡量）。HiDDeN [[16](#bib.bib16)] 的一个缺点是，仅在针对特定攻击类型训练时，编码和解码消息之间的损失被最小化，而在多种攻击类型的组合下则不然。这表明，该模型在针对一种攻击类型进行专门训练时效果最佳，但在训练多种攻击类型时效果不佳。
- en: This shortcoming was improved in the model ROMark [[28](#bib.bib28)], which
    builds upon the framework from HiDDeN [[16](#bib.bib16)] by using a min-max formulation
    for robust optimisation. This was done by addressing two main goals; first, to
    obtain the worst-case watermarked images with the largest decoding error, and
    second, to optimise the model’s parameters when dealing with the worst-case scenario
    so that decoding loss is minimised. The idea of this technique is to minimise
    decoding loss across a range of attacks, rather than training the model to resist
    specialised attacks, creating a more versatile and adaptable framework. Due to
    the optimisation for worst-case distortions in ROMark [[28](#bib.bib28)], it performed
    better when trained on a combination of attacks, particularly on those that had
    not been seen during training. ROMark [[28](#bib.bib28)] was also more robust
    in some specialised attack categories, though HiDDeN [[16](#bib.bib16)] had higher
    accuracy in these categories.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这种缺点在模型ROMark [[28](#bib.bib28)]中得到了改进，该模型在HiDDeN [[16](#bib.bib16)]的框架基础上，通过使用最小-最大公式进行鲁棒优化。这样做是为了实现两个主要目标：首先，获取具有最大解码误差的最坏情况水印图像；其次，在处理最坏情况时优化模型的参数，以最小化解码损失。该技术的理念是跨越多种攻击最小化解码损失，而不是训练模型以抵御特定攻击，从而创建一个更通用和适应性强的框架。由于ROMark
    [[28](#bib.bib28)] 针对最坏情况失真进行了优化，当模型在多种攻击的组合上进行训练时，特别是在训练期间未见过的攻击上表现更好。ROMark
    [[28](#bib.bib28)] 在一些专门攻击类别中也更具鲁棒性，尽管HiDDeN [[16](#bib.bib16)] 在这些类别中的准确性更高。
- en: Additional improvements were made to the framework from HiDDeN [[16](#bib.bib16)]
    by Hamamoto et al. [[36](#bib.bib36)]. This work uses a neural network for attack
    simulation rather than a single differentiable noise layer. There is a rotation
    layer followed by an additive noise layer, allowing the model to learn robustness
    against geometric rotation attacks. It also features a noise strength factor to
    control the robustness/imperceptibility trade-off. It was tested against HiDDeN
    [[16](#bib.bib16)] and found to achieve greater image quality after watermark
    embedding, as well as greater robustness against JPEG compression. This model
    was the first to be trained to meet the Information Hiding Criteria (IHC) for
    robustness while simultaneously resisting geometric rotation attacks. It is suggested
    for future work to combine the architecture with a Scale Invariant Feature Transform
    (SIFT) detector [[37](#bib.bib37)], which is able to detect image features that
    are robust for embedding to withstand geometric attacks in traditional watermarking
    algorithms.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Hamamoto等人[[36](#bib.bib36)]对HiDDeN [[16](#bib.bib16)]框架进行了额外改进。这项工作使用神经网络进行攻击模拟，而不是单一的可微噪声层。它包含一个旋转层和一个附加噪声层，使模型能够学习对几何旋转攻击的鲁棒性。它还具有噪声强度因子，以控制鲁棒性/隐蔽性的权衡。经过测试，发现该模型在水印嵌入后实现了更高的图像质量，并且对JPEG压缩的鲁棒性更强。这个模型是第一个在满足信息隐藏标准（IHC）以增强鲁棒性的同时，能够抵抗几何旋转攻击的模型。建议未来的工作将该架构与尺度不变特征变换（SIFT）检测器[[37](#bib.bib37)]相结合，SIFT检测器能够检测出对传统水印算法中的几何攻击具有鲁棒性的图像特征。
- en: A novel embedding strategy using Inverse Gradient Attention (IGA) [[30](#bib.bib30)]
    was adopted recently. Similar to Attention-based Data Hiding [[29](#bib.bib29)],
    the focus is on identifying robust pixels for data hiding by using an attention
    mask. In the IGA method, the attention mask indicates the gradient values of the
    input cover image, which shows the robustness of each pixel for message reconstruction.
    It builds on the idea of adversarial examples first used for digital watermarking
    in HiDDeN [[16](#bib.bib16)], and uses the attention mechanism to locate the worst-case
    pixels for perturbation. By identifying robust pixel regions, this method further
    improves the payload capacity and robustness of watermarked images in tests against
    the following papers [[16](#bib.bib16), [38](#bib.bib38), [21](#bib.bib21)].
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最近采用了一种使用逆梯度注意力（IGA）的新嵌入策略[[30](#bib.bib30)]。与基于注意力的数据隐藏[[29](#bib.bib29)]类似，重点是通过使用注意力掩码来识别用于数据隐藏的鲁棒像素。在IGA方法中，注意力掩码表示输入封面图像的梯度值，显示了每个像素在消息重建中的鲁棒性。它建立在第一次用于数字水印的对抗样本的思想之上，在HiDDeN
    [[16](#bib.bib16)]中使用，并利用注意力机制定位最坏情况的扰动像素。通过识别鲁棒像素区域，这种方法进一步提高了水印图像在对以下文献[[16](#bib.bib16),
    [38](#bib.bib38), [21](#bib.bib21)]测试中的负载能力和鲁棒性。
- en: A novel two-stage separable deep learning (TSDL) framework for watermarking
    is introduced by Liu et al.[[27](#bib.bib27)], which addresses the problems with
    one-stage end-to-end training (OET) such as slow convergence leading to image
    quality degradation, and having to simulate noise attacks using a differentiable
    layer. Instead of relying on differentiable approximations, the TDSL framework
    can use true non-differentiable noise attacks such as JPEG compression during
    training. This is because, in OET models, the encoder and decoder are trained
    using a differentiable noise layer, which means the noise must support backpropagation.
    The TDSL framework consists of noise-free end-to-end adversary training (FEAT),
    which is used to train an encoder that autonomously encodes watermarks with redundancy
    and without reference to any noise. Noise aware decoder only training (ADOT) is
    used to train the decoder so that it is robust and able to extract the watermark
    from any type of noise attack. The model was tested against HiDDeN [[16](#bib.bib16)]
    and ReDMark [[25](#bib.bib25)], and found to achieve superior robustness for all
    but one attack category (crop). However, the two-stage training method is also
    robust against black-box noise attacks that are encapsulated in image processing
    software, which have not been tested in previous works.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人介绍了一种新颖的两阶段可分离深度学习（TSDL）框架用于水印[[27](#bib.bib27)]，该框架解决了一阶段端到端训练（OET）存在的问题，例如收敛慢导致图像质量下降，以及必须使用可微分层模拟噪声攻击。TSDL框架能够在训练过程中使用真正的非可微噪声攻击，如JPEG压缩，而不是依赖于可微分的近似。在OET模型中，编码器和解码器使用可微噪声层进行训练，这意味着噪声必须支持反向传播。TSDL框架包括噪声自由的端到端对抗训练（FEAT），用于训练一个自主编码水印的编码器，且无需参考任何噪声。噪声感知解码器仅训练（ADOT）用于训练解码器，使其具有鲁棒性并能从任何类型的噪声攻击中提取水印。该模型经过HiDDeN[[16](#bib.bib16)]和ReDMark[[25](#bib.bib25)]测试，发现对所有攻击类别（除了裁剪）表现出优越的鲁棒性。然而，两阶段训练方法对封装在图像处理软件中的黑盒噪声攻击也具有鲁棒性，而这些攻击在以往的工作中未被测试。
- en: Wasserstein GAN. A popular variation of the traditional GAN technique is the
    Wasserstein GAN (WGAN) [[39](#bib.bib39)]. This technique improves the model stability
    during training, as well as decreases the sensitivity of the training process
    to model architecture and hyperparameter configurations. The WGAN framework also
    provides a loss function that correlates with the quality of generated images.
    This is particularly useful for watermarking and steganography in the image domain,
    since image quality must be effectively optimised. Instead of the discriminator
    component of the network, WGANs include a critic. Rather than predicting the probability
    that a given image is real or fake, as is the discriminator’s goal, the critic
    outputs a score denoting the ‘realness’ of the input image. In a data hiding scenario,
    the encoder’s aim is to maximise the score given by the critic for real instances
    – which corresponds to an encoded image. Current papers adopting the WGAN framework
    for their watermarking models include [[38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein GAN。传统GAN技术的一个流行变种是Wasserstein GAN（WGAN）[[39](#bib.bib39)]。这种技术提高了训练过程中的模型稳定性，并降低了训练过程对模型架构和超参数配置的敏感性。WGAN框架还提供了一个与生成图像质量相关的损失函数。这对于图像领域中的水印和隐写尤其有用，因为图像质量必须得到有效优化。WGANs用一个评论器代替了网络中的鉴别器。评论器不是预测给定图像是真实还是虚假的概率，而是输出一个表示输入图像“真实性”的分数。在数据隐藏场景中，编码器的目标是最大化评论器对真实实例（即编码图像）给出的分数。当前采用WGAN框架进行水印模型的论文包括[[38](#bib.bib38),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42)]。
- en: Zhang et al.[[38](#bib.bib38)] introduced SteganoGAN. Three variants of encoder
    architecture are explored, each with different connectivity patterns. The basic
    variant applies two convolution blocks, where the encoded image is the output
    of the second block. Residual connections have been shown to improve model stability
    [[43](#bib.bib43)], and in the residual variant, the cover image is added to the
    encoder outputs so that it learns to produce a residual image. The third variant
    uses a method inspired by DenseNet [[44](#bib.bib44)], in which there are additional
    feature connections between convolutional blocks that allow the feature maps generated
    by earlier blocks to be concatenated to those generated by subsequent blocks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人 [[38](#bib.bib38)] 引入了 SteganoGAN。探索了三种不同的编码器架构变体，每种变体具有不同的连接模式。基本变体应用了两个卷积块，其中编码图像是第二个块的输出。残差连接已被证明能提高模型稳定性
    [[43](#bib.bib43)]，在残差变体中，将封面图像添加到编码器输出中，使其学习生成残差图像。第三种变体使用了受 DenseNet [[44](#bib.bib44)]
    启发的方法，其中在卷积块之间存在额外的特征连接，使得早期块生成的特征图可以与后续块生成的特征图进行拼接。
- en: Using adversarial training, this model achieves a relative payload of 4.4 BPP,
    10 times higher than competing deep learning methods. Although both works [[16](#bib.bib16),
    [38](#bib.bib38)] have mechanisms in place for handling arbitrarily-sized cover
    images as input, the higher payload capabilities of [[38](#bib.bib38)] means it
    can support a greater range of watermark data. The paper also proposes a new metric,
    Reed Solomon Bits Per Pixel (RS-BPP), to measure the payload capacity of deep
    learning-based data hiding techniques so that results can be compared with traditional
    data hiding methods. Although the primary focus for SteganoGAN is steganography,
    the high payload capacity and low detection rate of SteganoGAN produced images
    can also be applied to watermarking.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对抗训练，该模型实现了 4.4 BPP 的相对负载，比竞争的深度学习方法高出 10 倍。尽管两篇工作 [[16](#bib.bib16), [38](#bib.bib38)]
    都有处理任意大小封面图像作为输入的机制，但 [[38](#bib.bib38)] 更高的负载能力意味着它可以支持更广泛的水印数据。该论文还提出了一种新的度量标准，Reed
    Solomon Bits Per Pixel (RS-BPP)，用于测量基于深度学习的数据隐藏技术的负载能力，以便将结果与传统的数据隐藏方法进行比较。尽管
    SteganoGAN 的主要重点是隐写术，但 SteganoGAN 生成图像的高负载能力和低检测率也可以应用于水印。
- en: Two further works by the same authors also use the WGAN framework. Plata et
    al.introduced a new embedding technique where the watermark is spread over the
    spatial domain of the image [[40](#bib.bib40)]. The watermark message is converted
    into a sequence of tuples, where the first element of each is converted to a binary
    representation. The spatial message is created by randomly assigning binary converted
    tuples to sections of the message, including redundant data for increased robustness.
    The paper also introduces a new technique for differentiable noise approximation
    of non-differentiable distortions which allows the simulation of subsampling attacks.
    The attack training pool is expanded from previous works to include subsampling
    and resizing attacks, and this wide range of attacks used during training increases
    general robustness. However, the spatial spread embedding technique reduces the
    embedding capacity, so is only useful for applications where capacity is not a
    priority. Furthermore, the training framework proposed requires half as much time
    as prior methods [[16](#bib.bib16), [25](#bib.bib25), [21](#bib.bib21)]. The authors
    expand upon this work in a follow-up paper [[41](#bib.bib41)], which introduces
    the double discriminator-detector architecture. The discriminator is placed after
    the noise layer, and receives both noised cover images and noised encoded images,
    and thus the discriminator learns to distinguish watermarked and non-watermarked
    images with attacks already applied. In practical contexts, this is useful because
    it reduces the likelihood of false accusations being made of IP theft. If the
    image has already been attacked and must be proven to contain a watermark, this
    training technique is useful. Crucially, it does not degrade the overall robustness
    of the encoded images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 同一作者的另外两项工作也使用了WGAN框架。Plata等人引入了一种新的嵌入技术，其中水印在图像的空间域上展开[[40](#bib.bib40)]。水印信息被转换为一系列元组，其中每个元组的第一个元素被转换为二进制表示。空间信息通过将二进制转换后的元组随机分配到信息的各个部分来创建，包括冗余数据以增加鲁棒性。论文还引入了一种用于不可微失真差异化噪声近似的新技术，这使得模拟子采样攻击成为可能。攻击训练池从以前的工作中扩展，包含了子采样和调整大小攻击，这种广泛的攻击范围在训练过程中增加了整体鲁棒性。然而，空间扩展嵌入技术减少了嵌入容量，因此仅适用于容量不是优先考虑的应用。此外，所提出的训练框架所需的时间仅为以前方法的一半[[16](#bib.bib16),
    [25](#bib.bib25), [21](#bib.bib21)]。作者在后续论文[[41](#bib.bib41)]中进一步扩展了这项工作，引入了双重判别器-检测器架构。判别器位于噪声层之后，并接收噪声覆盖图像和噪声编码图像，因此判别器学习区分已应用攻击的水印图像和未水印图像。在实际情况下，这很有用，因为它减少了对IP盗窃的错误指控。如果图像已经被攻击并且必须证明包含水印，这种训练技术是有用的。关键是，它不会降低编码图像的整体鲁棒性。
- en: A technique for encoded image quality improvement is introduced by Wang et al.[[42](#bib.bib42)]
    based on texture analysis. The cover image texture features are analysed by a
    grey co-occurrence matrix which divides the image into complex and flat regions.
    The paper utilises the StegaStamp network [[45](#bib.bib45)] for embedding the
    watermark in the flat texture regions. This reduces the degree of image modification
    and improves the quality, and hence imperceptibility, of encoded images. The network
    in StegaStamp [[45](#bib.bib45)] produces higher quality images from low contrast
    examples; therefore the contrast value is used to calculate the texture complexity
    of the image.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Wang等人基于纹理分析引入了一种改进编码图像质量的技术[[42](#bib.bib42)]。封面图像的纹理特征通过灰度共现矩阵进行分析，该矩阵将图像划分为复杂和简单区域。论文利用StegaStamp网络[[45](#bib.bib45)]将水印嵌入到简单纹理区域。这减少了图像修改的程度，提升了编码图像的质量和*隐蔽性*。StegaStamp[[45](#bib.bib45)]中的网络从低对比度样本生成高质量图像，因此对比度值被用来计算图像的纹理复杂度。
- en: 'TABLE I: Summary table of deep learning-based digital watermarking methods.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：基于深度学习的数字水印方法汇总表。
- en: '| Arch | Model | Domain | Embedding Network | Extractor Network | RA | RC |
    Remarks |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 模型 | 领域 | 嵌入网络 | 提取网络 | RA | RC | 备注 |'
- en: '| AE CNN | [[12](#bib.bib12)] | Frequency | AE CNN | AE CNN |  |  | First CNN
    for watermarking. Blind technique |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| AE CNN | [[12](#bib.bib12)] | 频率 | AE CNN | AE CNN |  |  | 首个用于水印的CNN。盲方法
    |'
- en: '| [[24](#bib.bib24)] | Spatial | AE & RB | RB |  | ✓ | Autoencoder + visual
    mask |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| [[24](#bib.bib24)] | 空间 | AE & RB | RB |  | ✓ | 自编码器 + 视觉掩码 |'
- en: '| CNN | [[21](#bib.bib21)] | Spatial | Channel Coding & CNN | Channel coding
    & CNN |  |  | Channel Coding & CNN for attacks |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| CNN | [[21](#bib.bib21)] | 空间 | 通道编码 & CNN | 通道编码 & CNN |  |  | 攻击的通道编码 &
    CNN |'
- en: '| [[25](#bib.bib25)] | Frequency | CC layer & DCT layer | DCT layer | ✓ | ✓
    | Diffusion mechanism |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| [[25](#bib.bib25)] | 频率 | CC层 & DCT层 | DCT层 | ✓ | ✓ | 扩散机制 |'
- en: '| [[23](#bib.bib23)] | Spatial | CNN | CNN | ✓ | ✓ | Invariance layer and image
    fusion |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| [[23](#bib.bib23)] | 空间 | CNN | CNN | ✓ | ✓ | 不变层和图像融合 |'
- en: '| [[26](#bib.bib26)] | Spatial | CNN & AP | CNN | ✓ | ✓ | Robustness against
    geometric attacks |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bib26)] | 空间 | CNN & AP | CNN | ✓ | ✓ | 针对几何攻击的鲁棒性 |'
- en: '| GAN | [[16](#bib.bib16)] | Spatial | AL | GP & CNN |  |  | Adversarial examples
    for embedding |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: GAN | [[16](#bib.bib16)] | 空间 | AL | GP & CNN |  |  | 嵌入的对抗性示例 |
- en: '| [[30](#bib.bib30)] | Frequency | IG attention mask & CNN | CNN |  |  | IG
    mask improves capacity and robustness |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| [[30](#bib.bib30)] | 频率 | IG注意力掩码 & CNN | CNN |  |  | IG掩码提高容量和鲁棒性 |'
- en: '| [[28](#bib.bib28)] | Spatial | AL | FC layer |  | ✓ | Min-max formulation
    for robust optimisation |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)] | 空间 | AL | FC层 |  | ✓ | 用于鲁棒优化的最小-最大公式 |'
- en: '| [[27](#bib.bib27)] | Spatial | AL | FC layer | ✓ | ✓ | Two stage training
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] | 空间 | AL | FC层 | ✓ | ✓ | 两阶段训练 |'
- en: '| [[36](#bib.bib36)] | Spatial | CNN | CNN + FC layers |  | ✓ | Robust against
    rotation and JPEG compression |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [[36](#bib.bib36)] | 空间 | CNN | CNN + FC层 |  | ✓ | 针对旋转和JPEG压缩的鲁棒性 |'
- en: '| WGAN | [[38](#bib.bib38)] | Spatial | CB & ASP | CNN |  |  | High payload
    capacity |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| WGAN | [[38](#bib.bib38)] | 空间 | CB & ASP | CNN |  |  | 高负载能力 |'
- en: '| [[40](#bib.bib40)] | Spatial | CNN | CNN + AP |  |  | Spatial spread embedding
    technique |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| [[40](#bib.bib40)] | 空间 | CNN | CNN + AP |  |  | 空间扩展嵌入技术 |'
- en: '| [[41](#bib.bib41)] | Spatial | CNN | CNN + AP |  |  | Double discriminator-detector
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| [[41](#bib.bib41)] | 空间 | CNN | CNN + AP |  |  | 双重鉴别器-检测器 |'
- en: '| [[42](#bib.bib42)] | Spatial | CNN & MP | CNN & MP |  | ✓ | Uses texture
    analysis |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| [[42](#bib.bib42)] | 空间 | CNN & MP | CNN & MP |  | ✓ | 使用纹理分析 |'
- en: 'AE: auto-encoder, RB: residual block, AP: average pooling, ASP: adaptive spatial
    pooling, AL: adversarial loss, CB: convolutional block, FC: fully connected, IG:
    inverse gradient, GP: global pooling, MP: max pooling, CC: circular convolutional,
    RA: resolution adaptability, and RC: robustness trade-off controls.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AE: 自编码器，RB: 残差块，AP: 平均池化，ASP: 自适应空间池化，AL: 对抗损失，CB: 卷积块，FC: 全连接，IG: 逆梯度，GP:
    全局池化，MP: 最大池化，CC: 圆形卷积，RA: 分辨率适应性，RC: 鲁棒性权衡控制。'
- en: Attention-based Data Hiding (ADBH) [[29](#bib.bib29)] introduced an attention
    mechanism that helps the generative encoder pinpoint areas on the cover image
    best suited for data embedding. The attention model is an input processing technique
    that allows the network to focus on specific aspects of a complex input one at
    a time. The attention model generates an attention mask, which represents the
    attention sensitivity of each pixel in the cover image. The value is regularised
    to a probability denoting whether altering the corresponding pixel will lead to
    obvious perceptual differences in the image that will be picked up by the discriminator.
    This improves the embedding process of the encoder network.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的数据隐藏（ADBH）[[29](#bib.bib29)] 引入了一种注意力机制，帮助生成编码器确定最适合数据嵌入的覆盖图像区域。注意力模型是一种输入处理技术，使网络能够一次关注复杂输入的特定方面。注意力模型生成一个注意力掩码，表示覆盖图像中每个像素的注意力敏感度。该值被正则化为一个概率，表示更改相应像素是否会导致图像中显著的感知差异，从而被鉴别器检测到。这改善了编码器网络的嵌入过程。
- en: Rather than hiding a binary watermark message, this technique hides a secret
    image in the cover image. In the adopted CycleGAN framework, there is a target
    image generative model and a secret image generative model. The former generates
    watermarked images which are then fed to the cover image discriminative model,
    and builds on the PatchGAN model to operate on one image patch at a time [[46](#bib.bib46)].
    Similarly, the secret image generative model serves as input for the secret image
    discriminative model. This architecture uses adversarial learning for both the
    embedding and extraction processes, ensuring that the distributions between both
    the cover and encoded images, and the encoded and extracted watermarks, are indistinguishable.
    The extractor network also operates on unmarked cover images, and inconsistent
    loss is used to make sure that, when this happens, there should be no correlation
    between the extracted data and the original watermark.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与隐藏二进制水印信息不同，这种技术在封面图像中隐藏秘密图像。在采用的 CycleGAN 框架中，有一个目标图像生成模型和一个秘密图像生成模型。前者生成带水印的图像，然后将其输入到封面图像判别模型中，并基于
    PatchGAN 模型对每个图像块进行操作[[46](#bib.bib46)]。类似地，秘密图像生成模型作为秘密图像判别模型的输入。该架构在嵌入和提取过程中均使用对抗学习，确保封面图像与编码图像之间，以及编码水印与提取水印之间的分布不可区分。提取网络也作用于未标记的封面图像，并使用不一致损失确保在这种情况下，提取的数据与原始水印之间没有相关性。
- en: The three central contributions of this paper – the attention model, the use
    of cycle discriminative models, and the extra inconsistent loss objective – were
    all tested in isolation and found to improve the performance of the model overall
    in terms of both image quality and robustness.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的三个核心贡献——注意力模型、循环判别模型的使用以及额外的不一致损失目标——都经过了单独测试，发现它们在图像质量和鲁棒性方面都能整体提升模型性能。
- en: III-A3 Discussion
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 讨论
- en: From the state-of-the-art deep learning methods discussed above, it seems that
    the GAN framework is the most promising in terms of robustness and secrecy optimisation
    due to the inclusion of adversarial loss in the objective calculations. To illustrate
    this, in HiDDeN [[16](#bib.bib16)], tests were conducted without including the
    adversary network and found to include perceptible alterations, whereas including
    the adversary greatly improved performance to produce an invisible watermarking
    technique. In tests comparing robustness, GAN-based models performed well, but
    were improved by techniques to target robust pixels for watermark embedding, such
    as the attention mechanisms used in Attention-based Data Hiding [[29](#bib.bib29)]
    and the IGA method [[30](#bib.bib30)].
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述最先进的深度学习方法来看，由于在目标计算中包含对抗损失，GAN 框架在鲁棒性和保密性优化方面似乎最具前景。以 HiDDeN [[16](#bib.bib16)]
    为例，在未包括对抗网络的情况下进行的测试发现包含了可察觉的变化，而包括对抗者则大大提高了性能，产生了不可见的水印技术。在鲁棒性比较测试中，基于 GAN 的模型表现良好，但通过针对鲁棒像素进行水印嵌入的技术，如
    Attention-based Data Hiding [[29](#bib.bib29)] 和 IGA 方法 [[30](#bib.bib30)]，效果得到了提升。
- en: It is also important for models to be robust against a range of attacks. Papers
    such as those by Hamamoto et al.[[36](#bib.bib36)] and Plata et al.[[40](#bib.bib40)]
    incorporate geometric rotation techniques, and subsampling and resizing attacks
    respectively. Having a wider range of attack types during training increases general
    robustness. Additionally, using true non-differentiable noise, as shown in the
    two stage training technique developed by Liu et al.[[27](#bib.bib27)], provides
    better results for JPEG compression than models that use differentiable approximations.
    Using a trained CNN to generate attacks, Distortion Agnostic Watermarking [[21](#bib.bib21)]
    is also a promising approach for diversifying the attacks encountered during training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对各种攻击的鲁棒性也很重要。Hamamoto 等人[[36](#bib.bib36)] 和 Plata 等人[[40](#bib.bib40)] 的论文分别涉及几何旋转技术和子采样及重采样攻击。在训练过程中拥有更广泛的攻击类型可以提高通用鲁棒性。此外，如
    Liu 等人[[27](#bib.bib27)] 所开发的两阶段训练技术中所示，使用真实的非可微噪声在 JPEG 压缩方面比使用可微近似模型的效果更佳。利用训练好的
    CNN 生成攻击，Distortion Agnostic Watermarking [[21](#bib.bib21)] 也是一种有前景的方法，用于多样化训练过程中遇到的攻击。
- en: To produce an adaptable, generalised framework, it is important to include features
    such as host and watermark resolution adaptability, as well as robustness controls
    to influence the robustness/imperceptibility trade-off. Robustness controls make
    these models suitable for both watermarking and stenography, which relies more
    heavily on imperceptibility. The ideal digital watermarking model would include
    pre-processing networks for any watermark data or cover image to be input, as
    the framework was developed by Lee et al.[[26](#bib.bib26)], while also taking
    advantage of an adversarial discriminator or critic. The techniques used in [[28](#bib.bib28)]
    to improve robustness by obtaining and optimising parameters for ROMark worst-case
    examples is also a promising technique for improving robustness.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成一个可适应的、通用的框架，包含诸如主机和水印分辨率适应性以及影响鲁棒性/不可察觉性权衡的控制等特性是重要的。鲁棒性控制使这些模型适用于水印和隐写术，后者更侧重于不可察觉性。理想的数字水印模型应包括用于输入任何水印数据或封面图像的预处理网络，因为该框架是由Lee等人开发的[[26](#bib.bib26)]，同时还应利用对抗性判别器或评论家。[[28](#bib.bib28)]中用于通过获取和优化ROMark最坏情况示例的参数来提高鲁棒性的技术也是一种有前景的提高鲁棒性的方法。
- en: If a future model could combine these features; attention mechanisms to improve
    embedding, robustness against geometric attacks, host and watermark resolution
    controls, robustness controls, and handling worst-case distortions, it could result
    in a highly robust and adaptable framework. However, the added overhead of all
    these added features could be an issue in practice.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未来的模型能够结合这些特性：改进嵌入的注意机制、抵御几何攻击的鲁棒性、主机和水印分辨率控制、鲁棒性控制以及处理最坏情况失真，它可能会导致一个高度鲁棒和适应性强的框架。然而，所有这些附加特性的额外开销在实践中可能会成为一个问题。
- en: 'Table [I](#S3.T1 "TABLE I ‣ III-A2 Generative Adversarial Networks ‣ III-A
    Deep Learning-based Watermarking Techniques ‣ III Deep Learning-based Data Hiding
    Techniques ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking
    and Steganography") shows a summary of the deep watermarking models reviewed in
    the above section. It shows whether the embedding strategy operates in the spatial
    or frequency domain, the nature of the watermark embedding and extracting networks,
    whether the technique supports host resolution adaptability (so that any cover
    image resolution can be used), and whether it includes controls for influencing
    the trade-off between imperceptibility and robustness. The remarks column describes
    any important information or novel contributions of the paper.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表[I](#S3.T1 "表 I ‣ III-A2 生成对抗网络 ‣ III-A 基于深度学习的水印技术 ‣ III 基于深度学习的数据隐藏技术 ‣ 基于深度学习的数据隐藏：统一数字水印和隐写术的调查")总结了上述部分评审的深度水印模型。它显示了嵌入策略是否在空间域或频率域中操作、水印嵌入和提取网络的性质、技术是否支持主机分辨率适应性（以便使用任何封面图像分辨率），以及是否包括影响不可察觉性和鲁棒性之间权衡的控制。备注栏描述了论文中的任何重要信息或新颖贡献。
- en: III-B Deep Learning-based Steganography Techniques
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于深度学习的隐写术技术
- en: 'This section classifies deep steganography techniques based on model architecture.
    Most techniques adopt the encoder-decoder structure shown in Figure [4](#S3.F4
    "Figure 4 ‣ III Deep Learning-based Data Hiding Techniques ‣ Data Hiding with
    Deep Learning: A Survey Unifying Digital Watermarking and Steganography"), and
    are based on convolutional neural networks (CNNs). Many steganography implementations
    differ from watermarking implementations in their use of the U-Net structure for
    image segmentation [[47](#bib.bib47)]. Table [II](#S3.T2 "TABLE II ‣ III-B1 Encoder-decoder
    Framework ‣ III-B Deep Learning-based Steganography Techniques ‣ III Deep Learning-based
    Data Hiding Techniques ‣ Data Hiding with Deep Learning: A Survey Unifying Digital
    Watermarking and Steganography") shows a summary of deep learning-based steganography
    methods reviewed in this section.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本节根据模型架构对深度隐写技术进行分类。大多数技术采用图[4](#S3.F4 "图 4 ‣ III 基于深度学习的数据隐藏技术 ‣ 基于深度学习的数据隐藏：统一数字水印和隐写术的调查")中所示的编码器-解码器结构，并基于卷积神经网络（CNNs）。许多隐写实现与水印实现的不同之处在于使用了U-Net结构进行图像分割[[47](#bib.bib47)]。表[II](#S3.T2
    "表 II ‣ III-B1 编码器-解码器框架 ‣ III-B 基于深度学习的隐写术技术 ‣ III 基于深度学习的数据隐藏技术 ‣ 基于深度学习的数据隐藏：统一数字水印和隐写术的调查")总结了本节评审的基于深度学习的隐写方法。
- en: III-B1 Encoder-decoder Framework
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 编码器-解码器框架
- en: 'CNN-based encoder-decoder structures have been adopted in the following papers
    [[17](#bib.bib17), [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [45](#bib.bib45), [52](#bib.bib52), [53](#bib.bib53)]. A simple diagram of the
    encoder-decoder deep watermarking structure can be found in Figure [4](#S3.F4
    "Figure 4 ‣ III Deep Learning-based Data Hiding Techniques ‣ Data Hiding with
    Deep Learning: A Survey Unifying Digital Watermarking and Steganography").'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN的编码-解码结构已在以下论文中采用 [[17](#bib.bib17), [48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51), [45](#bib.bib45), [52](#bib.bib52), [53](#bib.bib53)]。编码-解码深度水印结构的简单示意图见图
    [4](#S3.F4 "图 4 ‣ III 深度学习数据隐匿技术 ‣ 基于深度学习的数据隐匿：统一数字水印和隐写术的调查")。
- en: A paper from Google research [[17](#bib.bib17)], published in 2017, presented
    a deep steganography technique for hiding images inside other images. The structure
    contains three components in total. The first being the prep network, which serves
    two purposes; to adjust the size of a smaller secret image to fit into the cover
    image, and to transform the colour-based pixels to recognisable features to be
    encoded into the cover image. The second layer of the encoder is called the hiding
    network. As the name suggests, this layer creates the final stego-image. The final
    layer is the reveal network which is used to decode the output from the second
    layer. The experiments in the work [[17](#bib.bib17)] were mainly conducted to
    show that it was possible to completely encode a large amount of data with limited
    visual disturbances in the cover media. However, such a technique lacked robustness,
    security, and was not of high quality. It was possible for attackers to recover
    both the cover and secret image with a trained network.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Google研究的论文 [[17](#bib.bib17)]，发表于2017年，提出了一种深度隐写技术，用于将图像隐藏在其他图像中。该结构总共有三个组成部分。第一个是预处理网络，具有两个目的；将较小的秘密图像调整到适合隐藏图像的大小，并将基于颜色的像素转换为可编码到隐藏图像中的可识别特征。编码器的第二层被称为隐藏网络。顾名思义，这一层生成最终的隐写图像。最后一层是揭示网络，用于解码第二层的输出。该研究
    [[17](#bib.bib17)] 的实验主要旨在表明可以在覆盖介质中完全编码大量数据而不会造成明显的视觉干扰。然而，该技术缺乏鲁棒性、安全性，且质量不高。攻击者可以利用训练好的网络恢复覆盖图像和秘密图像。
- en: Another image encoding technique called StegNet [[48](#bib.bib48)] was released
    in 2018\. StegNet used structures from both auto-encoders and GANs to set up the
    encoding network. The cover image and the secret image were concatenated by a
    channel prior to the CNN encoding structure. Variance loss was included in loss
    calculations for the encoder and decoder. It was found that including the variance
    loss helped the neural network distribute the loss throughout the image rather
    than having concentrated areas of perceptual loss, improving the overall imperceptibility
    of embedding. The presented technique was highly robust against statistical analysis
    and when used against StegExpose [[54](#bib.bib54)], a commonly used steganalysis
    tool, it was also resistant against those attacks. Although robust, there were
    still some limitations to this method. Secret images and cover images must match
    in size and noise is still somewhat prominent in smoother regions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种图像编码技术称为StegNet [[48](#bib.bib48)]，于2018年发布。StegNet结合了自编码器和GAN的结构来建立编码网络。在CNN编码结构之前，覆盖图像和秘密图像通过一个通道进行串联。在编码器和解码器的损失计算中加入了方差损失。研究发现，包含方差损失有助于神经网络将损失分布到整个图像中，而不是集中在感知损失的区域，从而提高了嵌入的整体隐蔽性。所提出的技术对统计分析具有高度鲁棒性，当用于对抗常用的隐写分析工具StegExpose
    [[54](#bib.bib54)]时，它也能抵抗这些攻击。尽管稳健，但该方法仍存在一些限制。秘密图像和覆盖图像必须匹配大小，并且在平滑区域中噪声仍然较为明显。
- en: Comparing the method of Baluja et al. [[17](#bib.bib17)] and StegNet [[48](#bib.bib48)],
    there has been a vast improvement in image hiding. The inclusion of components
    adapted from GANs and auto-encoders in StegNet [[48](#bib.bib48)] increased the
    robustness of the stego-images and was therefore more resistant to StegExpose [[54](#bib.bib54)].
    Though StegNet [[48](#bib.bib48)] is quite robust, it is still lacking in some
    areas such as quality, image size restrictions and noise.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 比较Baluja等人 [[17](#bib.bib17)] 和StegNet [[48](#bib.bib48)]的方法，图像隐藏技术有了显著的改进。StegNet
    [[48](#bib.bib48)] 中结合了从GAN和自编码器中改编的组件，提高了隐写图像的鲁棒性，因此对StegExpose [[54](#bib.bib54)]
    更具抵抗力。尽管StegNet [[48](#bib.bib48)] 颇具鲁棒性，但在一些领域如质量、图像尺寸限制和噪声方面仍然存在不足。
- en: A faster R-CNN (region-based CNN) method was introduced in [[49](#bib.bib49)].
    Firstly, the cover image is passed through a region proposal network, which makes
    the selection for feature extraction faster. Softmax loss is used to box these
    regions and then specific existing steganographic algorithms are selected and
    assigned to the boxed regions. Since [[49](#bib.bib49)] uses a technique of selecting
    different steganography algorithms, using a combination of HUGO [[33](#bib.bib33)],
    S-UNIWARD [[35](#bib.bib35)] and WOW [[34](#bib.bib34)] algorithms, it is able
    to achieve highly imperceptible embedding. Being able to select effective areas
    on the cover image also allows Meng et al.[[49](#bib.bib49)] to maintain a high
    level of robustness.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[49](#bib.bib49)]中介绍了一种更快的R-CNN（区域卷积神经网络）方法。首先，封面图像通过一个区域提议网络，这使得特征提取的选择变得更快。使用Softmax损失来框定这些区域，然后选择并分配特定的现有隐写算法到这些框定的区域。由于[[49](#bib.bib49)]使用了选择不同隐写算法的技术，结合使用了HUGO
    [[33](#bib.bib33)]、S-UNIWARD [[35](#bib.bib35)] 和 WOW [[34](#bib.bib34)] 算法，它能够实现高度不可察觉的嵌入。能够选择有效区域的封面图像还使得Meng等人[[49](#bib.bib49)]能够保持较高的鲁棒性。
- en: The adaptation of the fusion technique [[49](#bib.bib49)] allows for minimal
    distortion in the extracted stego-image. When looking at StegNet [[48](#bib.bib48)],
    there is a concern for noise in smoother areas of the cover photo. Although [[48](#bib.bib48)]
    has been shown to be robust, it could be further improved in this area with the
    box selection the fusion method [[49](#bib.bib49)] has. This does lead to a capacity
    dilemma if there was a method combining both StegNet [[48](#bib.bib48)] and the
    fusion method [[49](#bib.bib49)], since the latter would naturally be using less
    cover image area and therefore have a smaller capacity compared to StegNet [[48](#bib.bib48)].
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 融合技术[[49](#bib.bib49)]的适应性允许提取的隐写图像具有最小的失真。在查看StegNet [[48](#bib.bib48)]时，关注点在于封面照片中较平滑区域的噪声。尽管[[48](#bib.bib48)]已经被证明具有鲁棒性，但如果结合了融合方法[[49](#bib.bib49)]的框选择功能，它可以在这方面得到进一步改进。这确实会导致容量困境，因为如果有一种方法将StegNet
    [[48](#bib.bib48)]和融合方法[[49](#bib.bib49)]结合起来，后者自然会使用较少的封面图像区域，因此相对于StegNet [[48](#bib.bib48)]具有较小的容量。
- en: A unique approach for steganography is shown by Sharma et al.in [[50](#bib.bib50)].
    In this paper, both the encoder and decoder consist of two layers. In the first
    layer, the prep layer, smaller images are increased in size to correctly fit the
    cover image, there is a reconstruction of colour-based pixels to create more useful
    features, and the pixels are scrambled and then permutated. The second layer,
    the hiding layer, produces the stego-image with the output of the first layer
    and the cover image inputted. The decoder consists of the reveal layer and the
    decrypt layer. The reveal layer removes the cover image and the decrypt layer
    decrypts the output of the reveal layer. The advantage of this technique is that
    the first layer and its encryption method are similar to cryptography practices,
    allowing for a more secure embedded stego-image. Even when the cover media is
    known to the attacker, it is far more secure and difficult for the attacker to
    decode the secret image. This technique can also be applied to audio.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Sharma等人[[50](#bib.bib50)]展示了一种独特的隐写方法。在这篇论文中，编码器和解码器都由两层组成。在第一层，即准备层，将较小的图像放大以正确适应封面图像，进行基于颜色的像素重构以创建更多有用的特征，并且对像素进行混洗和置换。第二层，即隐藏层，生成具有第一层输出和封面图像输入的隐写图像。解码器由揭示层和解密层组成。揭示层去除封面图像，解密层解密揭示层的输出。这种技术的优势在于第一层及其加密方法类似于密码学实践，从而允许嵌入的隐写图像更加安全。即使攻击者知道封面媒体，解码秘密图像仍然要安全得多且困难。这种技术也可以应用于音频。
- en: A reversible image hiding method was introduced by Chang et al. [[53](#bib.bib53)].
    The structure relies on a concept called long short term memory. To encode, the
    cover image goes through a neural network in order to get a prediction, called
    the reference image. By subtracting the cover image from the reference image,
    the cover residuals (prediction errors) are calculated. Using histogram shifting
    (HS) on the cover residuals then produces stego residuals, along with an overflow
    map that is later used for the decoder. The stego-image is then created by adding
    the stego residual to the reference image. Where there is a pixel intensity flow,
    the overflow map is pre-calculated to flag these pixels. The decoder is essentially
    the reverse of the encoder, where the stego-image goes through a neural network
    to get a reference image, and the rest follows in reverse. This technique creates
    high quality, high capacity images that contain minimal noise. Its invertible
    feature to recover the cover image is unique to many other CNN based steganography.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Chang 等人 [[53](#bib.bib53)] 介绍了一种可逆图像隐藏方法。该结构依赖于一种称为长短期记忆的概念。为了编码，覆盖图像通过一个神经网络以获得预测，称为参考图像。通过从参考图像中减去覆盖图像，计算覆盖残差（预测误差）。在覆盖残差上使用直方图移位（HS）产生隐写残差，以及一个后来用于解码器的溢出图。隐写图像通过将隐写残差加到参考图像上来创建。在像素强度流动的地方，预先计算溢出图以标记这些像素。解码器本质上是编码器的反向过程，其中隐写图像通过神经网络以获得参考图像，其余部分按反向顺序进行。这种技术生成高质量、高容量且噪声最小的图像。其可恢复覆盖图像的可逆特性在许多其他基于CNN的隐写方法中独特。
- en: Tang et al. [[55](#bib.bib55)] produced a steganography technique that uses
    adversarial embedding, called ADV-EMB. The network is able to hide a stego-message
    while being able to fool a CNN based steganalyser. The encoding network consists
    of a distortion minimalisation framework that adjusts and minimises the costs
    of the image according to features (gradients backpropagation) from the CNN steganalyser.
    The focus of ADV-EMB [[55](#bib.bib55)] is to prevent steganalysers from being
    able to detect the stego-image. This shows in their results, with a high security
    rate and also increased imperceptibility. They are also able to train the system
    to counter unknown steganalysers by using a local well-performing CNN steganalyser
    as the target analyser, allowing for diverse applications. Although ADV-EMB [[55](#bib.bib55)]
    is able to decrease the effectiveness of adversary-aware steganalysers, it has
    a weak pixel domain. However, an increase in payload capacity would increase the
    detection rate of steganalysers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Tang 等人 [[55](#bib.bib55)] 提出了一个使用对抗嵌入的隐写技术，称为ADV-EMB。该网络能够隐藏隐写消息，同时欺骗基于CNN的隐写分析器。编码网络包括一个失真最小化框架，该框架根据来自CNN隐写分析器的特征（梯度反向传播）调整和最小化图像的成本。ADV-EMB
    [[55](#bib.bib55)] 的重点在于防止隐写分析器检测隐写图像。其结果显示出高安全性和更高的隐蔽性。他们还能够通过使用本地表现良好的CNN隐写分析器作为目标分析器来训练系统以应对未知的隐写分析器，从而实现多样化的应用。尽管ADV-EMB
    [[55](#bib.bib55)] 能够降低对抗性隐写分析器的有效性，但其在像素域上较弱。然而，增加负载容量会提高隐写分析器的检测率。
- en: U-Net CNN. U-Net CNNs are used to facilitate more nuanced feature mapping through
    image segmentation. This is useful in image steganography applications because
    a cover image can be broken up into distinct segments based on a certain property
    (for example, [[51](#bib.bib51)] uses a heatmap to score suitable embedding areas).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net CNN。U-Net CNN 用于通过图像分割促进更细致的特征映射。这在图像隐写应用中很有用，因为覆盖图像可以根据某个特性（例如，[[51](#bib.bib51)]
    使用热图来评分适合的嵌入区域）被分成不同的段。
- en: 'TABLE II: Summary table of deep learning-based steganography methods.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：基于深度学习的隐写方法总结表。
- en: '| Arch | Model | Embedding Network | Extractor Network | Remarks |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 模型 | 嵌入网络 | 提取网络 | 备注 |'
- en: '| CNN | [[17](#bib.bib17)] | CNN | CNN | Embeds coloured image in coloured
    cover image |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| CNN | [[17](#bib.bib17)] | CNN | CNN | 在彩色覆盖图像中嵌入彩色图像 |'
- en: '| [[48](#bib.bib48)] | CNN | CNN | Concatenating channel for embedding |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| [[48](#bib.bib48)] | CNN | CNN | 连接通道进行嵌入 |'
- en: '| [[49](#bib.bib49)] | R-CNN & CNN | N/A | Uses multiple techniques-based on
    selected regions |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| [[49](#bib.bib49)] | R-CNN & CNN | N/A | 基于选定区域使用多种技术 |'
- en: '| [[50](#bib.bib50)] | CNN | CNN | Embeds by scrambling and permutating pixels
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| [[50](#bib.bib50)] | CNN | CNN | 通过打乱和排列像素进行嵌入 |'
- en: '| [[53](#bib.bib53)] | LSTM w/ CNN & HS | LSTM w/ Conv. layers & HS | Uses
    HS for reversibility |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| [[53](#bib.bib53)] | LSTM w/ CNN & HS | 带卷积层的LSTM和HS | 使用HS实现可逆性 |'
- en: '| [[55](#bib.bib55)] | CNN & Adversarial Emb | N/A | Trains stego-images with
    steganalysers |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| [[55](#bib.bib55)] | CNN 与对抗嵌入 | 无 | 使用隐写分析器训练隐写图像 |'
- en: '| CNN w/ Adv. Training | [[56](#bib.bib56)] | CNN with ReLU | CNN with ReLU
    | Several models for different priorities |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 带有对抗训练的 CNN | [[56](#bib.bib56)] | 带 ReLU 的 CNN | 带 ReLU 的 CNN | 针对不同优先级的多个模型
    |'
- en: '| U-Net CNN | [[51](#bib.bib51)] | U-Net CNN w/ BN & ReLU | CNN w/ BN and ReLU
    | Can withstand high and low frequencies |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| U-Net CNN | [[51](#bib.bib51)] | 带 BN 和 ReLU 的 U-Net CNN | 带 BN 和 ReLU 的
    CNN | 能够承受高频和低频 |'
- en: '| [[45](#bib.bib45)] | CNN U-Net | CNN | Embeds hyperlinks in images |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [[45](#bib.bib45)] | CNN U-Net | CNN | 将超链接嵌入图像 |'
- en: '|  | [[52](#bib.bib52)] | U-Net from Cycle-GAN | CNN | Improvement on [[16](#bib.bib16)]
    for diverse use |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | [[52](#bib.bib52)] | U-Net 来自 Cycle-GAN | CNN | 针对多样化用途对[[16](#bib.bib16)]的改进
    |'
- en: '| GAN | [[57](#bib.bib57)] | CNN | CNN | Hides data in Y channel of cover image
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| GAN | [[57](#bib.bib57)] | CNN | CNN | 隐藏数据于封面图像的 Y 通道 |'
- en: '| [[58](#bib.bib58)] | GAN & CNN | N/A | Generates a textured cover image |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [[58](#bib.bib58)] | GAN 与 CNN | 无 | 生成纹理化的封面图像 |'
- en: '| [[59](#bib.bib59)] | CNN | CNN | Improvement on [[38](#bib.bib38)] in capacity
    and security |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[59](#bib.bib59)] | CNN | CNN | 在容量和安全性方面对[[38](#bib.bib38)]的改进 |'
- en: A U-Net CNN technique for reversible steganography was developed by Ni et al.
    [[51](#bib.bib51)]. This technique is capable of directly encoding the secret
    image into the cover image by concatenating the secret image into a six-channel
    tensor. Its decoder is formed from six convolution layers, each of which is followed
    by a batch normalisation and a ReLu activation layer. The embedding technique
    is not easily affected by excessively high or low frequency areas. It is also
    able to produce a high-quality image with a capacity that is in general better
    than other cover-selection and cover-synthesis based steganography techniques.
    Even with its high capacity capabilities, like other steganographic techniques
    that do not focus on robustness, too high of an embedding rate will increase the
    distortion rate more dramatically compared to robustness based steganography.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Ni等人开发了一种用于可逆隐写的 U-Net CNN 技术[[51](#bib.bib51)]。该技术通过将秘密图像连接到六通道张量中，能够直接将秘密图像编码到封面图像中。其解码器由六个卷积层组成，每个卷积层后面跟随一个批量归一化层和一个
    ReLu 激活层。该嵌入技术不易受高频或低频区域的影响。它还能够生成高质量图像，其容量通常优于其他基于封面选择和封面合成的隐写技术。即使具有高容量能力，像其他不注重鲁棒性的隐写技术一样，过高的嵌入率会比基于鲁棒性的隐写技术更显著地增加失真率。
- en: Universal Deep Hiding (UDH) is a model proposed for uses in digital watermarking,
    steganography, and light field messaging [[52](#bib.bib52)]. The encoder uses
    the simplified U-Net from Cycle-GAN [[60](#bib.bib60)], and a dense CNN for the
    decoder. The encoder hides the image in a cover-agnostic manner, meaning that
    it is not dependent on the cover image. UDH is an effective method due to the
    high-frequency discrepancy between the encoded image and the cover image. This
    discrepancy makes embedding robust in low-frequency cover images. UDH is also
    less sensitive to pixel intensity shifts on the cover image. The UDH method was
    compared with cover-dependent deep hiding (DDH). Since the encoding of the secret
    image was independent of the cover image, there was no method to adapt the encoding
    mechanism according to the cover image. The cover image may have some smoother
    areas which may not be ideal to embed data into, but with UDH there was no method
    of finding out this type of information. UDH is also unable to work well with
    severe uniform random noise.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Universal Deep Hiding (UDH) 是一种用于数字水印、隐写和光场消息传递的模型[[52](#bib.bib52)]。编码器使用简化的
    Cycle-GAN 的 U-Net[[60](#bib.bib60)]，解码器使用密集的 CNN。编码器以无关封面图像的方式隐藏图像，意味着它不依赖于封面图像。由于编码图像与封面图像之间的高频差异，UDH
    是一种有效的方法。这种差异使得在低频封面图像中的嵌入更为鲁棒。UDH 对封面图像上的像素强度变化也不太敏感。UDH 方法与基于封面的深度隐写 (DDH) 进行了比较。由于秘密图像的编码与封面图像独立，因此没有方法可以根据封面图像调整编码机制。封面图像可能有一些较平滑的区域，这些区域可能不适合嵌入数据，但使用
    UDH 没有方法找出这些信息。UDH 也无法很好地处理严重的均匀随机噪声。
- en: CNN with Adversarial Training. The inclusion of adversarial attack networks
    during training can be used to help improve against steganalysis and promote robustness.
    Adversarial training helps the system distinguish small perturbations that an
    untrained steganography method may bypass. This is an important feature to have
    in steganography, since its main focus is to protect message security. Chen et
    al.developed a model that incorporates a trained CNN-based attack network that
    generates distortions [[56](#bib.bib56)], similar to the technique used in the
    watermarking framework used by Xiyang et al.in Distortion Agnostic Watermarking
    [[21](#bib.bib21)]. The encoding structure is based off of a simple model that
    hides a secret grey-scale image into channel B (blue) of a coloured cover image,
    where both must have the same resolution. From this basic model, the paper was
    able to add two other enhanced models, a secure model and a secure robust model.
    The secure model inserts a steganalysis network into the basic model where its
    goal is to increase security against steganalysis. The secure and robust model
    uses the secure model and inserts an attack network to increase the robustness
    of the system. The separation of each model allows the framework by Chen et al.[[56](#bib.bib56)]
    to be used in several different scenarios, allowing the user to adjust to their
    needs. A downside is that users are unable to send RGB pictures and are limited
    to just grey-scale images.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 含有对抗性训练的CNN。在训练过程中加入对抗攻击网络可以帮助提高抗隐写分析能力并增强鲁棒性。对抗性训练可以帮助系统区分未经训练的隐写方法可能会绕过的微小扰动。这是隐写术中一个重要的特性，因为它的主要目标是保护信息安全。陈等人开发了一种模型，该模型结合了一个训练有素的基于CNN的攻击网络，该攻击网络生成失真图像[[56](#bib.bib56)]，类似于西央等人在《失真不敏感水印技术》[[21](#bib.bib21)]中使用的技术。编码结构基于一个简单的模型，将秘密的灰度图像隐藏到彩色盖图的通道B（蓝色）中，两者必须具有相同的分辨率。在这个基本模型的基础上，该论文还能够添加另外两个增强型模型，一个安全模型和一个安全鲁棒模型。安全模型将隐写分析网络插入到基本模型中，其目标是增加对隐写分析的安全性。安全和鲁棒模型使用安全模型，并插入一个攻击网络以增强系统的鲁棒性。每个模型的分离允许陈等人的框架[[56](#bib.bib56)]在几种不同的场景中使用，使用户能够根据自己的需求进行调整。不足之处是用户无法发送RGB图片，只能限制在灰度图像上。
- en: The secure model was shown to have the best invisibility against all models
    and was still the best when compared to the following methods [[17](#bib.bib17),
    [61](#bib.bib61)]. The secure and robust model did not perform as well as the
    secure model but was still able to improve. The basic model and the secure model
    showed increased visual results in this comparison, with the secure model having
    the best visual results. The visual results of the secure and robust model were
    similar to ISGAN [[57](#bib.bib57)].
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 安全模型在所有模型中显示了最好的隐蔽性，并且与以下方法[[17](#bib.bib17), [61](#bib.bib61)]相比仍然是最好的。安全和鲁棒模型的性能不及安全模型，但仍然能够得到改善。基本模型和安全模型在这个比较中显示出了增加的视觉效果，其中安全模型具有最好的视觉效果。安全和鲁棒模型的视觉效果与ISGAN[[57](#bib.bib57)]类似。
- en: III-B2 Generative Adversarial Networks based models
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 基于生成式对抗网络的模型
- en: Generative Adversarial Networks (GANs) are used extensively in deep steganography.
    Various new structures have allowed the simple GAN structure to be improved, increasing
    the effectiveness of steganography. It has brought up interesting techniques such
    as coverless steganography, and the ability to generate cover images. GAN-based
    architectures have been used in the following papers [[57](#bib.bib57), [58](#bib.bib58),
    [62](#bib.bib62), [59](#bib.bib59), [63](#bib.bib63)].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度隐写领域，生成式对抗网络（GANs）被广泛使用。各种新的结构使得简单的GAN结构得以改进，从而增强了隐写术的有效性。它带来了一些有趣的技术，比如无盖隐写和生成盖图的能力。GAN-based架构已经在以下论文中使用[[57](#bib.bib57),
    [58](#bib.bib58), [62](#bib.bib62), [59](#bib.bib59), [63](#bib.bib63)]。
- en: ISGAN is a steganography technique that uses the GAN structure [[57](#bib.bib57)].
    Similarly to the secure technique developed by Chen et al.[[56](#bib.bib56)],
    ISGAN is only able to send a grey-scale secret image. The encoder first converts
    the cover image into the YCrCb colour space where only the Y channel is used to
    hide the secret image since it holds luminance but no colour information. This
    colour space conversion does not affect backpropagation. The encoder also uses
    an inception module, which helps to fuse feature maps with different receptive
    field sizes. To aid the speed of training, the technique also adds a residual
    module and batch normalisation. A CNN is used to decode with batch normalisation
    added after every convolutional layer excluding the final layer. The results of
    this model showed that it was able to achieve a high level of robustness, with
    low detectability when scrutinised using steganalysis tools.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ISGAN 是一种使用 GAN 结构的隐写技术[[57](#bib.bib57)]。类似于 Chen 等人开发的安全技术[[56](#bib.bib56)]，ISGAN
    只能发送灰度秘密图像。编码器首先将封面图像转换到 YCrCb 颜色空间，其中仅使用 Y 通道来隐藏秘密图像，因为它包含亮度信息而不包含颜色信息。这种颜色空间转换不会影响反向传播。编码器还使用了一个
    inception 模块，帮助融合具有不同接收域大小的特征图。为了提高训练速度，该技术还添加了一个残差模块和批量归一化。在解码时使用 CNN，并在每个卷积层后添加批量归一化，除了最后一层。该模型的结果显示它能够实现高水平的鲁棒性，在使用隐写分析工具检查时具有低可检测性。
- en: In comparison to [[61](#bib.bib61)], ISGAN residuals were less obvious, showing
    that the extracted secret image of ISGAN is much closer to the original than [[61](#bib.bib61)].
    ISGAN is also able to achieve higher levels of invisibility since ISGAN uses a
    grey scale image. Thus, there is a trade-off between the complexity of the image
    (i.e. RGB values) and the imperceptibility of embedded information.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于[[61](#bib.bib61)]，ISGAN 的残差不那么明显，这表明 ISGAN 提取的秘密图像比[[61](#bib.bib61)]更接近原图。由于
    ISGAN 使用灰度图像，它还能够实现更高程度的隐蔽性。因此，图像的复杂性（即 RGB 值）与嵌入信息的不可感知性之间存在权衡。
- en: Two separate designs are introduced by Li et al.in [[58](#bib.bib58)], which
    studies embedding data into texture images. The first model separates the texture
    image generation and secret image embedding processes. The texture image is generated
    using a deep convolutional generative neural network. The output of this is then
    used as the input for the concealing network for image hiding. The second model
    integrates the concealing network with the deep convolutional generative network.
    This second network should be able to generate the texture image while simultaneously
    embedding another image. The first model is easier to train and can be used in
    more diverse applications than the second model. Detection rates from steganalysis
    tools are almost 0 in both cases. These models provide high security, but with
    a few limitations. Firstly, the cover images generated are only textures and other
    subjects are not considered. Colour distortions also occur in the second model
    when the cover and secret images differ too much.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人提出了[[58](#bib.bib58)]中的两个独立设计，研究了将数据嵌入纹理图像中的方法。第一个模型将纹理图像生成和秘密图像嵌入过程分开。使用深度卷积生成对抗网络生成纹理图像，然后将其作为隐蔽网络的输入进行图像隐藏。第二个模型将隐蔽网络与深度卷积生成对抗网络整合在一起。第二个网络应能够在生成纹理图像的同时嵌入另一张图像。第一个模型更容易训练，且比第二个模型适用于更多样化的应用。隐写分析工具的检测率在两种情况下几乎为0。这些模型提供了高安全性，但也有一些局限性。首先，生成的封面图像仅为纹理，其他主题未被考虑。第二个模型中，当封面图像和秘密图像相差过大时，还会发生颜色失真。
- en: Coverless steganography is possible because of the features of GAN. The general
    encoding idea of the proposed coverless method [[59](#bib.bib59)] was that at
    first convolutional blocks were used to process the cover image to get a tensor,
    a. The secret message was then concatenated to a and processed through another
    convolutional block to get b which was the same size as a. The paper details two
    different models, one called the basic model and the dense model. The basic model
    uses the aforementioned encoding scheme, whereas the dense model includes a skip
    connection to increase the embedding rate. The decoder for both models uses Reed
    Solomon algorithms on the tensor produced from the stego-image. The aim of this
    model was to improve the capacity and quality that other coverless steganography
    has not been able to achieve. Encoded image quality and payload capacity of the
    stego-images were improved when compared to [[38](#bib.bib38)]. The basic model
    introduced by Quin et al. [[59](#bib.bib59)] was able to perform significantly
    better in these aspects while the dense model was able only to match SteganoGAN
    [[38](#bib.bib38)]. The decoding network used in the coverless method [[59](#bib.bib59)]
    was also more accurate than the one proposed for SteganoGAN [[38](#bib.bib38)].
    But there is a difference in the media encoded, where the coverless method [[59](#bib.bib59)]
    encodes a string of binary messages while SteganoGAN [[38](#bib.bib38)] is able
    to encode an image.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 无需盖体的隐写术之所以可行，是因为GAN的特性。所提议的无盖隐写方法的总体编码思路[[59](#bib.bib59)]是首先使用卷积块处理盖图像以获取张量a。然后，将秘密消息连接到a，并通过另一个卷积块处理，以获得与a大小相同的b。论文详细介绍了两种不同的模型，一种叫做基本模型，另一种叫做密集模型。基本模型使用上述编码方案，而密集模型则包括一个跳跃连接以增加嵌入率。两个模型的解码器都在从隐写图像生成的张量上使用Reed
    Solomon算法。该模型的目标是提高其他无盖隐写术未能实现的容量和质量。与[[38](#bib.bib38)]相比，编码图像质量和隐写图像的负载能力得到了改善。由Quin等人提出的基本模型[[59](#bib.bib59)]在这些方面表现显著优越，而密集模型只能匹敌SteganoGAN
    [[38](#bib.bib38)]。无盖方法[[59](#bib.bib59)]中使用的解码网络也比SteganoGAN [[38](#bib.bib38)]中提出的网络更准确。但在编码的介质方面存在差异，无盖方法[[59](#bib.bib59)]编码的是一串二进制消息，而SteganoGAN
    [[38](#bib.bib38)]则能够编码图像。
- en: Many steganography techniques have not been invertible and leave the cover image
    distorted after removing the secret piece of media. With the method proposed by
    Chang et al.[[63](#bib.bib63)], the model was able to achieve invertible steganography.
    The method used is based on the Regular-Singular (RS) method. RS realises lossless
    data embedding through invertible noise hiding. There are three discriminate blocks
    used in this technique – regular, singular and unusable – in order to get the
    RS map. The adversarial learning component serves to capture the regularity of
    natural images. The model uses conditional GAN to synthesise, where the generator
    uses a U-Net structure and a Markovian discriminator is used. The use of GAN in
    conjunction with the RS method greatly improved upon the results of previous RS-based
    models.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 许多隐写术技术并不可逆，并且在去除秘密介质后会使盖图像失真。通过Chang等人提出的方法[[63](#bib.bib63)]，模型能够实现可逆隐写术。该方法基于Regular-Singular
    (RS) 方法。RS通过可逆噪声隐藏实现无损数据嵌入。该技术中使用了三个判别块——常规、奇异和不可用——以获得RS映射。对抗学习组件用于捕捉自然图像的规律性。该模型使用条件GAN进行合成，其中生成器使用U-Net结构，并使用Markovian判别器。将GAN与RS方法结合使用大大改善了以前基于RS的模型的结果。
- en: Currently, the paper [[51](#bib.bib51)] uses a basic GAN structure and could
    be further improved or diversified with the adaption of other GAN structures.
    This could lead to further improvements in cover media recovery in terms of quality.
    Overall, the adversarial learning adopted in GAN-based models leads to greater
    robustness in steganography applications.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，论文[[51](#bib.bib51)]使用了基本GAN结构，并可以通过其他GAN结构的适配进一步改进或多样化。这可能会在盖图像恢复的质量方面带来进一步的改进。总体而言，GAN模型中采用的对抗学习使隐写术应用具有更大的鲁棒性。
- en: CycleGAN. CycleGAN is a variation of the GAN architecture for image-to-image
    translation. The CycleGAN framework was adopted for steganography in S-CycleGAN
    [[62](#bib.bib62)]. Within the model, there were three discriminators, two of
    which used the same function as the original CycleGAN framework. The third was
    an increased steganalysis module used to distinguish the stego-image from the
    generated images. The training cycle consists of three stages. First is the translation
    of the image from the X-domain to the style of the Y-domain. Second is the use
    of an LSB matching algorithm to embed the secret message into the output of the
    first stage. The third stage is where the stego-image is reconstructed to the
    input image of the first generator to the second generator. The full objective
    function included adversarial loss for all discriminators, as well as cycle consistency
    loss for the generative models. The advantage of S-CycleGAN is its ability to
    produce high quality images that are also robust against steganalysis. Overall
    S-CycleGAN [[62](#bib.bib62)] showed results that were more resistant to detection,
    increasing the invisibility of embedded stego-images.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN。CycleGAN 是一种用于图像到图像转换的 GAN 架构变体。CycleGAN 框架被用于隐写术中的 S-CycleGAN [[62](#bib.bib62)]。在该模型中，有三个鉴别器，其中两个使用与原始
    CycleGAN 框架相同的功能。第三个是增加的隐写分析模块，用于区分隐写图像和生成的图像。训练周期包括三个阶段。第一阶段是将图像从 X 域转换到 Y 域的风格。第二阶段是使用
    LSB 匹配算法将秘密信息嵌入到第一阶段的输出中。第三阶段是将隐写图像重建为从第一个生成器到第二个生成器的输入图像。完整的目标函数包括所有鉴别器的对抗损失以及生成模型的循环一致性损失。S-CycleGAN
    的优点在于能够生成高质量的图像，同时对隐写分析具有较强的鲁棒性。总体而言，S-CycleGAN [[62](#bib.bib62)] 展现了更强的抗检测能力，提高了嵌入隐写图像的隐蔽性。
- en: When S-CycleGAN [[62](#bib.bib62)] is compared against SGAN [[64](#bib.bib64)],
    the quality of the image is much higher, with 2.6x the Inception Score and 7x
    the Frechet Inception Distance of SGAN. In general, the results of S-CycleGAN
    [[62](#bib.bib62)] showed that it was much more robust than SGAN, and the combined
    use of the original CycleGAN framework and traditional steganography algorithm
    S-UNIWARD [[35](#bib.bib35)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SGAN [[64](#bib.bib64)] 相比，S-CycleGAN [[62](#bib.bib62)] 的图像质量要高得多，Inception
    Score 高出 2.6 倍，Frechet Inception Distance 高出 7 倍。总体而言，S-CycleGAN [[62](#bib.bib62)]
    的结果显示，它比 SGAN 更加鲁棒，并且其效果结合了原始 CycleGAN 框架和传统隐写算法 S-UNIWARD [[35](#bib.bib35)]。
- en: III-C Data Hiding Detection and Removal Mechanisms
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 数据隐藏检测与移除机制
- en: Improved data hiding techniques have led to the development of more advanced
    strategies for detecting and removing hidden data. In the context of steganography,
    the field of steganalysis is devoted to identifying and extracting covert messages.
    Unlike steganography, watermarking is primarily intended for identifying the owner
    of the media rather than hiding secret messages. As a result, preventing the detection
    of data in watermarking is less critical, as long as the data cannot be modified
    or removed from the cover media. In watermarking, adversaries are more concerned
    with removing or degrading the watermark without altering the original cover media.
    It is important to understand the goals and methods of adversaries in data hiding
    depending on the application, since these scenarios should be mitigated by the
    data hiding strategy.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的数据隐藏技术促使了更先进的隐藏数据检测与移除策略的发展。在隐写术的背景下，隐写分析领域致力于识别和提取隐秘信息。与隐写术不同，水印技术主要用于识别媒体的所有者，而非隐藏秘密信息。因此，在水印中防止数据被检测的优先级较低，只要数据不能被修改或从封面媒体中移除即可。在水印技术中，对手更关心的是去除或降低水印的效果，而不改变原始的封面媒体。根据应用的不同，理解对手在数据隐藏中的目标和方法是很重要的，因为这些场景应通过数据隐藏策略来应对。
- en: III-C1 Steganlysis
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 隐写分析
- en: 'Steganalysis refers to the process of detecting covert steganographic messages
    from the perspective of an adversary. These techniques involve analyzing the media
    to identify any flaws in the embedding process and determine whether it has been
    encoded. There are two main categories of steganalysis techniques: signature steganalysis
    and statistical steganalysis, as noted in reference [[11](#bib.bib11)].'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 隐写分析是指从对手的角度检测隐秘隐写信息的过程。这些技术涉及分析媒体，以识别嵌入过程中的任何缺陷，并确定其是否已被编码。隐写分析技术主要有两大类：签名隐写分析和统计隐写分析，如参考文献
    [[11](#bib.bib11)] 所述。
- en: Signature steganalysis is comprised of two types called specific signature steganalysis
    and universal signature steganalysis. In specific signature steganalysis, the
    adversary is aware of the embedding method used, whereas universal signature steganalysis
    does not require this knowledge. Therefore, universal signature steganalysis can
    be used to detect several types of steganographic techniques [[11](#bib.bib11),
    [65](#bib.bib65)].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 签名隐写分析包括两种类型，分别是特定签名隐写分析和通用签名隐写分析。在特定签名隐写分析中，攻击者知道所使用的嵌入方法，而通用签名隐写分析则不需要这些知识。因此，通用签名隐写分析可以用于检测多种隐写技术[[11](#bib.bib11),
    [65](#bib.bib65)]。
- en: The development of steganalysis, along with new steganography techniques, is
    crucial since it shows how robust new embedding techniques are to ever-improving
    detection technologies.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 隐写分析的发展以及新隐写技术的出现至关重要，因为它显示了新嵌入技术对不断改进的检测技术的鲁棒性。
- en: III-C2 Watermark Removal Strategies
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 水印去除策略
- en: Watermark removal techniques can be separated into three categories [[66](#bib.bib66)].
    The first, blind watermark removal, is the technique that deep learning methods
    can defend against through varied attack simulation strategies. In blind removal
    techniques, the adversary has no knowledge of the watermarking process and attempts
    to degrade the watermark through attacks such as compression and geometric distortions.
    In key estimation attacks, the adversary has some knowledge of the watermarking
    scheme, and is then able to estimate the secret key used for embedding. Similarly,
    in tampering attacks the adversary has perfect knowledge of the watermarking scheme,
    resulting in a complete breakdown of the watermarking system where the secret
    key is explicitly obtained. In deep learning-based watermarking, the system is
    a black box even to its creators, hence the watermarking scheme cannot be uncovered
    by adversaries. Deep learning-based strategies only need to protect against the
    first type of attack, which is blind attacks. However, as watermark removal techniques
    improve through deep learning methods of their own, this is likely to change [[13](#bib.bib13)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 水印去除技术可以分为三类[[66](#bib.bib66)]。第一类是盲水印去除技术，这是深度学习方法可以通过多样的攻击模拟策略进行防御的技术。在盲去除技术中，攻击者对水印过程一无所知，试图通过压缩和几何失真等攻击来削弱水印。在密钥估计攻击中，攻击者对水印方案有一些了解，并能够估计用于嵌入的秘密密钥。同样，在篡改攻击中，攻击者对水印方案有完全的了解，导致水印系统完全崩溃，秘密密钥被明确获得。在基于深度学习的水印技术中，系统即使对其创建者也是一个黑箱，因此水印方案无法被攻击者揭示。基于深度学习的策略只需防御第一类攻击，即盲攻击。然而，随着水印去除技术通过深度学习方法的改进，这种情况可能会发生变化[[13](#bib.bib13)]。
- en: III-D Results
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 结果
- en: In this section, the robustness and imperceptibility of various deep data hiding
    models are presented and compared. Although there is no universally accepted standard
    dataset for testing deep data hiding models, COCO [[67](#bib.bib67)] is the most
    widely used dataset. Hence, the results achieved using this dataset were compared.
    Bit Error Rate (BER) and Peak Signal-to-Noise Ratio (PSNR) are the two commonly
    recorded metrics, although some papers only report one. It is important to note
    that the tests were conducted using different cover images and watermark dimensions,
    as indicated in the tables. Moreover, some papers employed a fixed pool of attacks
    during testing. Therefore, the results are not directly comparable but provide
    a general indication of the relative performance of the models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示并比较了各种深度数据隐藏模型的鲁棒性和隐蔽性。尽管目前没有被普遍接受的标准数据集用于测试深度数据隐藏模型，但COCO [[67](#bib.bib67)]是使用最广泛的数据集。因此，使用该数据集取得的结果进行了比较。比特错误率（BER）和峰值信噪比（PSNR）是两项常用的度量指标，尽管有些论文仅报告其中之一。需要注意的是，测试是使用不同的封面图像和水印尺寸进行的，如表中所示。此外，一些论文在测试期间使用了固定的攻击池。因此，结果并不完全可比，但提供了模型相对性能的一般指示。
- en: 'Table [III](#S3.T3 "TABLE III ‣ III-D Results ‣ III Deep Learning-based Data
    Hiding Techniques ‣ Data Hiding with Deep Learning: A Survey Unifying Digital
    Watermarking and Steganography") compares deep data hiding models based on Bit
    Error Rate (BER), a measure of robustness. All models were tested using the COCO
    [[67](#bib.bib67)] dataset. The BER for all models is not directly comparable
    due to the different resolutions of cover images and the size of watermark payload.
    For example, a watermark embedded in a higher-resolution image can retain integrity
    without sacrificing encoded image quality, therefore improving robustness. Conversely,
    a smaller watermark payload can be embedded with less of a degradation in cover
    image quality, also improving robustness.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [III](#S3.T3 "TABLE III ‣ III-D Results ‣ III Deep Learning-based Data Hiding
    Techniques ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking
    and Steganography") 比较了基于比特错误率（BER）的深度数据隐藏模型，这是衡量鲁棒性的一种指标。所有模型都使用 COCO [[67](#bib.bib67)]
    数据集进行测试。由于封面图像的分辨率和水印负载的大小不同，所有模型的 BER 不能直接比较。例如，嵌入高分辨率图像中的水印可以在不牺牲编码图像质量的情况下保持完整性，从而提高鲁棒性。相反，较小的水印负载可以嵌入而不会显著降低封面图像质量，也能提高鲁棒性。'
- en: 'TABLE III: Comparing deep learning-based watermarking models based on their
    robustness, as measured by Bit Error Rate (BER). The ‘Results Origin’ column indicates
    the source paper for each BER result.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：比较基于深度学习的水印模型的鲁棒性，通过比特错误率（BER）进行测量。‘Results Origin’ 列指示每个 BER 结果的来源论文。
- en: '| Model | Results Origin | Architecture | Cover Image Dimensions | Watermark
    Bits | BER (%) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Model | Results Origin | Architecture | Cover Image Dimensions | Watermark
    Bits | BER (%) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| HiDDeN [[16](#bib.bib16)] | [[30](#bib.bib30)] | GAN | 128 x 128 | 30 | 20.12
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| HiDDeN [[16](#bib.bib16)] | [[30](#bib.bib30)] | GAN | 128 x 128 | 30 | 20.12
    |'
- en: '| IGA [[30](#bib.bib30)] | [[30](#bib.bib30)] | GAN | 128 x 128 | 30 | 17.88
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| IGA [[30](#bib.bib30)] | [[30](#bib.bib30)] | GAN | 128 x 128 | 30 | 17.88
    |'
- en: '| DA [[21](#bib.bib21)] | [[30](#bib.bib30)] | GAN | 128 x 128 | 30 | 20.48
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| DA [[21](#bib.bib21)] | [[30](#bib.bib30)] | GAN | 128 x 128 | 30 | 20.48
    |'
- en: '| ReDMark [[25](#bib.bib25)] | [[25](#bib.bib25)] | GAN | 128 x 128 | 30 |
    18.18 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| ReDMark [[25](#bib.bib25)] | [[25](#bib.bib25)] | GAN | 128 x 128 | 30 |
    18.18 |'
- en: '| Rotation [[36](#bib.bib36)] | [[36](#bib.bib36)] | GAN | 64 x 64 | 8 | 8.0
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Rotation [[36](#bib.bib36)] | [[36](#bib.bib36)] | GAN | 64 x 64 | 8 | 8.0
    |'
- en: '| DNN [[23](#bib.bib23)] | [[23](#bib.bib23)] | CNN | 128 x 128 | 32 | 20.12
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| DNN [[23](#bib.bib23)] | [[23](#bib.bib23)] | CNN | 128 x 128 | 32 | 20.12
    |'
- en: '| Double Detector-Discriminator [[41](#bib.bib41)] | [[41](#bib.bib41)] | GAN
    | 256 x 256 | 32 | 5.53 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Double Detector-Discriminator [[41](#bib.bib41)] | [[41](#bib.bib41)] | GAN
    | 256 x 256 | 32 | 5.53 |'
- en: '| Spatial-Spread [[40](#bib.bib40)] | [[41](#bib.bib41)] | GAN | 256 x 256
    | 32 | 8.38 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Spatial-Spread [[40](#bib.bib40)] | [[41](#bib.bib41)] | GAN | 256 x 256
    | 32 | 8.38 |'
- en: '| Two-Stage [[27](#bib.bib27)] | [[27](#bib.bib27)] | GAN | 128 x 128 | 30
    | 8.3 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Two-Stage [[27](#bib.bib27)] | [[27](#bib.bib27)] | GAN | 128 x 128 | 30
    | 8.3 |'
- en: 'Table [IV](#S3.T4 "TABLE IV ‣ III-D Results ‣ III Deep Learning-based Data
    Hiding Techniques ‣ Data Hiding with Deep Learning: A Survey Unifying Digital
    Watermarking and Steganography") compares deep data hiding models based on Peak
    Signal-to-Noise Ratio (PSNR), a measure of encoded image quality. All models were
    tested using the COCO [[67](#bib.bib67)] dataset.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [IV](#S3.T4 "TABLE IV ‣ III-D Results ‣ III Deep Learning-based Data Hiding
    Techniques ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking
    and Steganography") 比较了基于峰值信噪比（PSNR）的深度数据隐藏模型，这是衡量编码图像质量的一种指标。所有模型都使用 COCO [[67](#bib.bib67)]
    数据集进行测试。'
- en: 'TABLE IV: Comparing deep data hiding models based on encoded image quality,
    measured using PSNR. The ‘Results Origin’ column shows the paper which the PSNR
    result is taken from.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：比较基于编码图像质量的深度数据隐藏模型，通过 PSNR 进行测量。‘Results Origin’ 列显示了 PSNR 结果的来源论文。
- en: '| Model | Results Origin | Architecture | Cover Image Dimensions | Watermark
    Bits | PSNR (dB) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Model | Results Origin | Architecture | Cover Image Dimensions | Watermark
    Bits | PSNR (dB) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Two-Stage [[27](#bib.bib27)] | [[27](#bib.bib27)] | GAN | 128 x 128 | 30
    | 33.51 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Two-Stage [[27](#bib.bib27)] | [[27](#bib.bib27)] | GAN | 128 x 128 | 30
    | 33.51 |'
- en: '| ABDH [[29](#bib.bib29)] | [[29](#bib.bib29)] | CycleGAN | 512 x 512 | 256
    | 31.79 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| ABDH [[29](#bib.bib29)] | [[29](#bib.bib29)] | CycleGAN | 512 x 512 | 256
    | 31.79 |'
- en: '| ISGAN [[57](#bib.bib57)] | [[29](#bib.bib29)] | GAN | 512 x 512 | 256 | 24.08
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ISGAN [[57](#bib.bib57)] | [[29](#bib.bib29)] | GAN | 512 x 512 | 256 | 24.08
    |'
- en: '| DA [[21](#bib.bib21)] | [[21](#bib.bib21)] | GAN | 128 x 128 | 30 | 33.7
    |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| DA [[21](#bib.bib21)] | [[21](#bib.bib21)] | GAN | 128 x 128 | 30 | 33.7
    |'
- en: '| HiDDeN [[16](#bib.bib16)] | [[21](#bib.bib21)] | GAN | 128 x 128 | 30 | 32.3
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| HiDDeN [[16](#bib.bib16)] | [[21](#bib.bib21)] | GAN | 128 x 128 | 30 | 32.3
    |'
- en: '| DNN [[23](#bib.bib23)] | [[23](#bib.bib23)] | CNN | 128 x 128 | 32 | 39.93
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| DNN [[23](#bib.bib23)] | [[23](#bib.bib23)] | CNN | 128 x 128 | 32 | 39.93
    |'
- en: '| WMNET [[26](#bib.bib26)] | [[23](#bib.bib23)] | CNN | 128 x 128 | 32 | 38.01
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| WMNET [[26](#bib.bib26)] | [[23](#bib.bib23)] | CNN | 128 x 128 | 32 | 38.01
    |'
- en: '| ROMark [[28](#bib.bib28)] | [[28](#bib.bib28)] | GAN | 128 x 128 | 30 | 27.80
    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| ROMark [[28](#bib.bib28)] | [[28](#bib.bib28)] | GAN | 128 x 128 | 30 | 27.80
    |'
- en: '| IGA [[30](#bib.bib30)] | [[30](#bib.bib30)] | GAN | 128 x 128 | 30 | 32.80
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| IGA [[30](#bib.bib30)] | [[30](#bib.bib30)] | GAN | 128 x 128 | 30 | 32.80
    |'
- en: '| SteganoGAN [[38](#bib.bib38)] | [[30](#bib.bib30)] | GAN | 128 x 128 | 30
    | 30.10 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| SteganoGAN [[38](#bib.bib38)] | [[30](#bib.bib30)] | GAN | 128 x 128 | 30
    | 30.10 |'
- en: IV Noise Injection Techniques
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 噪声注入技术
- en: Apart from model architecture, deep learning-based data hiding techniques can
    also be categorized based on the type of noise injection methods used. These methods
    are employed during training to simulate attacks and improve the robustness of
    the final model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型架构，基于深度学习的数据隐藏技术还可以根据所使用的噪声注入方法进行分类。这些方法在训练过程中使用，以模拟攻击并提高最终模型的鲁棒性。
- en: Several techniques have been utilized to enhance the robustness of deep learning-based
    data hiding models through attack simulation. The most frequently used approach
    involves employing a pre-defined set of attacks, which are simulated using a differentiable
    noise layer and subsequent geometric layers. Other methods include using a trained
    Convolutional Neural Network (CNN) to generate novel noise patterns, as discussed
    in references [[21](#bib.bib21)] and [[56](#bib.bib56)], and adopting a two-stage
    training process to increase the complexity of the simulated attacks, as described
    in reference [[27](#bib.bib27)]. The following section provides an overview of
    various noise injection techniques, along with their respective strengths and
    weaknesses.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通过攻击模拟，已经使用了几种技术来增强基于深度学习的数据隐藏模型的鲁棒性。最常用的方法是使用预定义的攻击集，这些攻击通过可微分的噪声层和后续的几何层进行模拟。其他方法包括使用训练好的卷积神经网络（CNN）生成新的噪声模式，如参考文献
    [[21](#bib.bib21)] 和 [[56](#bib.bib56)] 中讨论的，以及采用两阶段训练过程来增加模拟攻击的复杂性，如参考文献 [[27](#bib.bib27)]
    中描述的。以下部分提供了各种噪声注入技术的概述，以及它们各自的优缺点。
- en: IV-A Identity
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A Identity
- en: Some earlier deep learning-based data hiding techniques [[12](#bib.bib12)] did
    not involve attack simulation and were primarily intended as proof-of-concept
    demonstrations. Subsequent models generally include a control training scenario
    where no noise is added, such as the ”Identity” layer in HiDDeN [[16](#bib.bib16)].
    It is unsurprising that techniques without attack simulation during training result
    in significantly lower levels of robustness. For instance, when subjected to JPEG
    and Crop distortions, the HiDDeN model trained exclusively with the Identity layer
    had only a 50% success rate in successfully extracting watermarks and performed
    considerably worse when exposed to other types of attacks.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一些早期的基于深度学习的数据隐藏技术 [[12](#bib.bib12)] 并未涉及攻击模拟，主要作为概念验证展示。随后的模型通常包括一个控制训练场景，在该场景中没有添加噪声，例如
    HiDDeN [[16](#bib.bib16)] 中的“Identity”层。训练过程中没有攻击模拟的技术通常会导致显著较低的鲁棒性。例如，当遭遇 JPEG
    和裁剪失真时，仅使用 Identity 层训练的 HiDDeN 模型成功提取水印的概率仅为 50%，在面对其他类型的攻击时表现更差。
- en: IV-B Pre-defined Attack Pool
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 预定义攻击池
- en: The most common technique for noise injection is to use a fixed pool of pre-defined
    attack types. Then, during training, the model can be exposed to one, multiple,
    or the entire range of attack types. Each attack includes an intensity factor
    that can be adjusted in order to vary the strength of the attack during training.
    In an ideal scenario, the model would be just as effective with all attack types
    when trained against the entire range as when trained with one specific attack.
    In most scenarios, the model uses a differentiable noise layer, meaning that all
    noise-based attacks must support back-propagation to cohesively work with the
    neural network [[16](#bib.bib16)]. Since non-differentiable noise attacks such
    as JPEG compression are common in real attack scenarios, they are incorporated
    into training using differentiable approximations [[16](#bib.bib16)]. The accuracy
    of these approximations is crucial to creating models that are robust against
    non-differentiable noise, therefore efforts have been made to revise and improve
    these approximations [[40](#bib.bib40)].
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的噪声注入技术是使用固定池的预定义攻击类型。然后，在训练过程中，模型可以暴露于一种、多种或全部攻击类型。每种攻击包含一个强度因子，可以调整以改变训练过程中攻击的强度。在理想情况下，模型在对抗整个攻击范围时应该和只训练一种特定攻击时一样有效。在大多数情况下，模型使用可微分噪声层，这意味着所有基于噪声的攻击必须支持反向传播，以便与神经网络协同工作[[16](#bib.bib16)]。由于JPEG压缩等不可微分噪声攻击在实际攻击场景中很常见，因此它们通过可微分近似方法被纳入训练[[16](#bib.bib16)]。这些近似的准确性对于创建抗不可微分噪声的鲁棒模型至关重要，因此已做出努力来修订和改进这些近似[[40](#bib.bib40)]。
- en: Further improvements to the pre-defined attack pool include a greater range
    of attacks. For example, a rotation layer was added to the framework by Hamamoto
    et al. [[36](#bib.bib36)], and subsampling and resizing attacks were incorporated
    by Plata et al. [[40](#bib.bib40)]. Various techniques have been introduced to
    improve the performance of models on a combined range of attacks, since early
    techniques using this method [[16](#bib.bib16)] showed improved performance when
    trained with specific attacks rather than a range. For instance, ROMarK [[28](#bib.bib28)]
    adopts a technique wherein distortions that generate the largest decoding error
    are generated and used to optimise the model’s parameters during training.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对预定义攻击池的进一步改进包括更广泛的攻击范围。例如，Hamamoto等人[[36](#bib.bib36)]在框架中添加了旋转层，Plata等人[[40](#bib.bib40)]则加入了子采样和调整大小攻击。为了提高模型在多种攻击下的性能，已引入各种技术，因为早期使用此方法的技术[[16](#bib.bib16)]显示，在针对特定攻击而非范围进行训练时，性能有所提高。例如，ROMarK[[28](#bib.bib28)]采用了一种技术，即生成产生最大解码错误的失真，并在训练过程中用以优化模型的参数。
- en: IV-C Adversarial Training
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 对抗训练
- en: Another technique for noise injection does not use a fixed set of pre-defined
    attacks, but rather uses a separately trained CNN to generate novel noise-based
    distortions [[21](#bib.bib21), [56](#bib.bib56)]. The Distortion Agnostic model
    [[21](#bib.bib21)] generates distortions based on adversarial examples, using
    a CNN to generate a diverse set of image distortions. The Distortion Agnostic
    model was tested against HiDDeN and exposed to a number of distortions not seen
    during training, such as saturation, hue, and resizing attacks, and was found
    to be more effective. Although the Distortion Agnostic model could not surpass
    HiDDeN’s effectiveness against a specific attack when trained only with that type,
    but this is not a practical training setup for an end use scenario. Using a trained
    CNN for attack generation can greatly improve robustness since distortions are
    generated in an adversarial training scenario. This means that distortions are
    generated explicitly to foil the decoder network throughout training.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种噪声注入技术不使用固定的预定义攻击集，而是使用单独训练的CNN生成新型的基于噪声的失真[[21](#bib.bib21), [56](#bib.bib56)]。失真无关模型[[21](#bib.bib21)]基于对抗样本生成失真，使用CNN生成一组多样的图像失真。失真无关模型在对HiDDeN进行测试时暴露于训练过程中未见过的多种失真，例如饱和度、色调和调整大小攻击，发现效果更佳。虽然失真无关模型在仅使用特定攻击训练时无法超越HiDDeN的效果，但这种训练设置并不适合实际应用场景。使用训练好的CNN进行攻击生成可以大大提高鲁棒性，因为失真是在对抗训练场景中生成的。这意味着在训练过程中，失真是明确生成的，以干扰解码网络。
- en: IV-D Two-stage Training for Noise Injection
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 噪声注入的两阶段训练
- en: A novel training method developed by Liu et al. [[27](#bib.bib27)] improves
    noise injection techniques in two ways. Firstly, a training method dubbed Two-Stage
    Separable Deep Learning (TSDL) is used. This second stage of training is adopted
    for the decoder so that can work with any type of noise attacks, improving practicality.
    Because of this novel training scenario, this model can be trained using true
    non-differentiable distortion, rather than differentiable approximations. Aside
    from this distinction, a fixed pool of pre-defined attacks is still used during
    training. The use of true non-differentiable noise during training improved robustness
    against such attacks, including GIF and JPEG compression, but it is unclear from
    this work whether the increased overhead of the TDSL framework is a suitable trade-off
    for this improved performance, given the relative accuracy of differentiable approximations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人开发了一种新颖的训练方法[[27](#bib.bib27)]，在两个方面改进了噪声注入技术。首先，使用了一种称为“两阶段可分离深度学习（TSDL）”的训练方法。这种训练的第二阶段被应用于解码器，使其能够应对任何类型的噪声攻击，从而提高了实用性。由于这种新颖的训练场景，该模型可以使用真实的不可微分失真进行训练，而不是微分近似。除了这一区别之外，训练过程中仍使用固定的预定义攻击池。使用真实的不可微分噪声进行训练提高了对这些攻击的鲁棒性，包括GIF和JPEG压缩，但从这项工作中尚不清楚TDSL框架增加的开销是否为这种性能提升提供了合适的权衡，考虑到微分近似的相对准确性。
- en: V Objective Functions
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 目标函数
- en: Objective functions are used when training machine learning models to optimise
    their performance. The function measures the difference between the actual and
    predicted data points. By optimising the loss function, the model iteratively
    learns to reduce prediction errors. The loss functions used for the discussed
    data hiding models fall into the regression category, which deals with predicting
    values over a continuous range, rather than a set number of categories.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数在训练机器学习模型时用于优化其性能。该函数测量实际数据点和预测数据点之间的差异。通过优化损失函数，模型会迭代地学习减少预测错误。用于讨论的数据隐藏模型的损失函数属于回归类别，这涉及到在连续范围内预测值，而不是固定数量的类别。
- en: The aim of data hiding models is to optimise both robustness and imperceptibility,
    finding a balance between these two properties depending on the application’s
    needs. The most common architecture for deep data hiding models is the encoder-decoder
    architecture where the model is partitioned into two separate networks for encoding
    and decoding. A common method for evaluating loss is to have two separate equations,
    one for the encoder and one for the decoder, and compute the total loss across
    the system as a weighted sum. Architectures that include an adversarial network
    also include adversarial loss in this sum. When optimising imperceptibility, the
    loss between the initial cover image and the final encoded image must be minimised.
    When optimising robustness, the loss between the initial secret message or watermark
    information and the final extracted message must be minimised. This is the bases
    for the encoder-decoder loss method. This section will go through some common
    objective functions utilised in deep learning models for data hiding, as well
    as novel strategies that do not follow the encoder-decoder loss framework.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐藏模型的目标是优化鲁棒性和不可察觉性，在这两个属性之间找到平衡，具体取决于应用的需求。深度数据隐藏模型最常见的架构是编码器-解码器架构，其中模型被分为两个独立的网络，用于编码和解码。评估损失的常用方法是对编码器和解码器分别使用两个独立的方程式，并将系统的总损失计算为加权和。包含对抗网络的架构还包括对抗损失。在优化不可察觉性时，必须最小化初始封面图像和最终编码图像之间的损失。在优化鲁棒性时，必须最小化初始秘密信息或水印信息与最终提取信息之间的损失。这是编码器-解码器损失方法的基础。本节将介绍一些在深度学习数据隐藏模型中常用的目标函数，以及不遵循编码器-解码器损失框架的新策略。
- en: V-A Mean Squared Error (MSE)
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 均方误差（MSE）
- en: 'A function used to compute the difference between two sets of data. It squares
    the difference between data points in order to highlight errors further away from
    the regression line. In image-based data hiding, it can be used to compare the
    cover image to the encoded image by taking the square of the difference between
    all pixels in each image and dividing this by the number of pixels. The equation’s
    sensitivity to outliers makes it useful when pinpointing obvious perceptual differences
    between two images. For example, a cluster of pixels that are obviously different
    from the original is perceptually obvious, but a change distributed over all image
    pixels is harder to spot. The equation is also used to compute the difference
    between the original and encoded watermark in some applications. For two images
    $X$ and $Y$ of dimensions W $\times$ H, the mean square error is given by the
    following equation:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算两组数据之间差异的函数。它平方数据点之间的差异，以突出回归线远离的错误。在基于图像的数据隐藏中，它可以通过取每个图像中所有像素之间差异的平方，并将其除以像素数量，来比较覆盖图像和编码图像。该方程对离群值的敏感性使其在定位两幅图像之间明显的感知差异时非常有用。例如，明显不同于原始图像的像素簇在感知上很明显，而分布在所有图像像素上的变化则更难察觉。该方程还用于计算某些应用中原始水印和编码水印之间的差异。对于尺寸为
    W $\times$ H 的两幅图像 $X$ 和 $Y$，均方误差由以下方程给出：
- en: '|  | $MSE(X,Y)=\frac{1}{WH}\sum_{i=1}^{W}\sum_{j=1}^{H}(X_{i,j}-Y_{i,j})^{2}.$
    |  | (1) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | $MSE(X,Y)=\frac{1}{WH}\sum_{i=1}^{W}\sum_{j=1}^{H}(X_{i,j}-Y_{i,j})^{2}.$
    |  | (1) |'
- en: 'MSE is used in these works [[26](#bib.bib26), [38](#bib.bib38), [36](#bib.bib36),
    [40](#bib.bib40), [41](#bib.bib41), [59](#bib.bib59), [57](#bib.bib57), [58](#bib.bib58)]
    to calculate loss at the encoder, while many works [[27](#bib.bib27), [25](#bib.bib25),
    [50](#bib.bib50), [48](#bib.bib48), [51](#bib.bib51), [56](#bib.bib56), [16](#bib.bib16),
    [21](#bib.bib21), [42](#bib.bib42), [45](#bib.bib45)] use it to compute both encoder
    and decoder loss. MSE is used in conjunction with SSIM [17](#S6.E17 "In VI-B2
    Structural Similarity Index (SSIM) ‣ VI-B Quality ‣ VI Evaluation Metrics ‣ Data
    Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography")
    to calculate perceptual difference at the encoder [[57](#bib.bib57), [58](#bib.bib58)].
    The papers [[42](#bib.bib42), [45](#bib.bib45)] also use LPIPS perceptual loss
    [[68](#bib.bib68)] to evaluate loss at the encoder, which is discussed in Section
    [VI-B](#S6.SS2 "VI-B Quality ‣ VI Evaluation Metrics ‣ Data Hiding with Deep Learning:
    A Survey Unifying Digital Watermarking and Steganography"). The Distortion Agnostic
    Model [[21](#bib.bib21)] incorporates additional adversarial loss into its Euclidean
    Distance functions. Loss at the encoder incorporates GAN loss with spectral normalisation
    to further control the quality of the encoded image. It is similar to the adversarial
    loss function used in [[16](#bib.bib16)] in Equation ([6](#S5.E6 "In V-E Adversarial
    Loss ‣ V Objective Functions ‣ Data Hiding with Deep Learning: A Survey Unifying
    Digital Watermarking and Steganography")), but is incorporated into the encoder
    loss function rather than being weighted separately in the overall weighted sum.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 'MSE 在这些工作中 [[26](#bib.bib26), [38](#bib.bib38), [36](#bib.bib36), [40](#bib.bib40),
    [41](#bib.bib41), [59](#bib.bib59), [57](#bib.bib57), [58](#bib.bib58)] 被用来计算编码器的损失，而许多工作
    [[27](#bib.bib27), [25](#bib.bib25), [50](#bib.bib50), [48](#bib.bib48), [51](#bib.bib51),
    [56](#bib.bib56), [16](#bib.bib16), [21](#bib.bib21), [42](#bib.bib42), [45](#bib.bib45)]
    使用它来计算编码器和解码器的损失。MSE 与 SSIM [17](#S6.E17 "In VI-B2 Structural Similarity Index
    (SSIM) ‣ VI-B Quality ‣ VI Evaluation Metrics ‣ Data Hiding with Deep Learning:
    A Survey Unifying Digital Watermarking and Steganography") 结合使用来计算编码器的感知差异 [[57](#bib.bib57),
    [58](#bib.bib58)]。论文 [[42](#bib.bib42), [45](#bib.bib45)] 还使用 LPIPS 感知损失 [[68](#bib.bib68)]
    来评估编码器的损失，这在第 [VI-B](#S6.SS2 "VI-B Quality ‣ VI Evaluation Metrics ‣ Data Hiding
    with Deep Learning: A Survey Unifying Digital Watermarking and Steganography")
    节中讨论。扭曲无关模型 [[21](#bib.bib21)] 在其欧几里得距离函数中加入了额外的对抗损失。编码器的损失结合了 GAN 损失和谱归一化，以进一步控制编码图像的质量。它类似于
    [[16](#bib.bib16)] 中在公式 ([6](#S5.E6 "In V-E Adversarial Loss ‣ V Objective Functions
    ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography"))
    中使用的对抗损失函数，但它被纳入编码器损失函数中，而不是在整体加权和中单独加权。'
- en: V-B Mean Absolute Error (MAE)
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 平均绝对误差 (MAE)
- en: 'Similar to MSE, MAE is used to compute the difference between two sets of data,
    but instead takes the absolute value of the difference between data points. Therefore,
    it does not highlight errors further from the regression line or give any special
    significance to outliers. It is used to compute loss at the decoder in [[26](#bib.bib26)].
    Since the secret message or watermark consists of binary values, it is more suitable
    to use MAE, which is suited to discrete values, to compute the difference to ensure
    balanced training of both the encoder and decoder networks. Additionally, it is
    used to calculate encoder and decoder loss in [[63](#bib.bib63), [53](#bib.bib53),
    [48](#bib.bib48), [52](#bib.bib52)]. For two binary watermarks $M$ and $M^{\prime}$
    of dimensions $X$ x $Y$, the mean square error is given by the following equation:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于MSE，MAE用于计算两个数据集之间的差异，但它取数据点差异的绝对值。因此，它不会突出回归线之外的误差或对离群点给予特别意义。它用于计算[[26](#bib.bib26)]中的解码器损失。由于秘密消息或水印由二进制值组成，因此使用适合离散值的MAE来计算差异，以确保编码器和解码器网络的平衡训练。此外，它还用于计算[[63](#bib.bib63)，[53](#bib.bib53)，[48](#bib.bib48)，[52](#bib.bib52)]中的编码器和解码器损失。对于尺寸为$X$
    x $Y$的两个二进制水印$M$和$M^{\prime}$，均方误差由以下公式给出：
- en: '|  | $MAE(M,M^{\prime})=\frac{1}{XY}\sum_{i=1}^{X}\sum_{j=1}^{Y}&#124;M{(i,j)}-M^{\prime}{(i,j)}&#124;$
    |  | (2) |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | $MAE(M,M^{\prime})=\frac{1}{XY}\sum_{i=1}^{X}\sum_{j=1}^{Y}&#124;M{(i,j)}-M^{\prime}{(i,j)}&#124;$
    |  | (2) |'
- en: V-C Cross Entropy Loss
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C交叉熵损失
- en: 'This function is used to measure the difference between two probability distributions
    for a given variable, in this case the initial and extracted binary watermark
    message. The equation takes in $p(x)$, the true distribution, and $q(x)$, the
    estimated distribution. In digital watermarking this corresponds to the original
    watermark and the extracted watermark respectively, where $q(x)$ is the output
    of the decoder network representing the probability of watermark bits. Cross entropy
    loss takes the log of the predicted probability, therefore, the model will be
    ‘punished’ for a making a high probability prediction further from the true distribution.
    Cross entropy loss is given by the following equation:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数用于测量给定变量的两个概率分布之间的差异，在这里是初始和提取的二进制水印消息。该方程使用$p(x)$，即真实分布，以及$q(x)$，即估计分布。在数字水印中，这对应于原始水印和提取水印，其中$q(x)$是解码器网络的输出，表示水印位的概率。交叉熵损失取预测概率的对数，因此，模型将因对真实分布的高概率预测而受到‘惩罚’。交叉熵损失由以下方程给出：
- en: '|  | $H(p,q)=-\sum_{x}p(x)log(q(x)).$ |  | (3) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(p,q)=-\sum_{x}p(x)log(q(x)).$ |  | (3) |'
- en: This technique is used for measuring loss at the decoder in [[38](#bib.bib38),
    [25](#bib.bib25), [42](#bib.bib42), [36](#bib.bib36), [45](#bib.bib45)], and used
    to compute adversarial loss in [[55](#bib.bib55), [63](#bib.bib63)]
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术用于测量[[38](#bib.bib38)，[25](#bib.bib25)，[42](#bib.bib42)，[36](#bib.bib36)，[45](#bib.bib45)]中的解码器损失，并用于计算[[55](#bib.bib55)，[63](#bib.bib63)]中的对抗性损失。
- en: V-D Mean-Variance Loss
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D均方方差损失
- en: Variance loss is used to calculate how loss varies across an entire distribution.
    Using variance loss in conjunction with mean calculations leads to loss being
    distributed throughout an image rather than concentrated in certain areas. When
    calculating loss at the encoder, it is important that there are no concentrated
    areas of difference, since this will be easily perceptible. The paper [[48](#bib.bib48)]
    adopts variance loss and MAE loss for both encoder and decoder loss calculations.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 方差损失用于计算损失在整个分布中的变化。将方差损失与均值计算结合使用可以使损失分布在整个图像中，而不是集中在某些区域。在计算编码器的损失时，重要的是没有集中差异的区域，因为这将很容易被察觉。论文[[48](#bib.bib48)]采用了方差损失和MAE损失来计算编码器和解码器的损失。
- en: 'A similar technique is applied by [[40](#bib.bib40), [41](#bib.bib41)], but
    for a different reason. Variance loss is used to calculate loss at the decoder
    rather than MSE since the redundant data used to create the watermark message
    means the original watermark and extracted watermark need not be identical. Rather
    than extract this redundant data, as was done in [[26](#bib.bib26)], the loss
    function was modified to a joint mean and variance function. For original message
    $M$ and extracted message $M^{\prime}$, the mean and variance are given by:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[[40](#bib.bib40), [41](#bib.bib41)] 采用了类似的技术，但原因不同。方差损失用于在解码器中计算损失，而不是使用 MSE，因为用于创建水印消息的冗余数据意味着原始水印和提取的水印不必完全相同。与
    [[26](#bib.bib26)] 中提取冗余数据的方法不同，这里的损失函数被修改为联合均值和方差函数。对于原始消息 $M$ 和提取的消息 $M^{\prime}$，均值和方差由以下公式给出：'
- en: '|  | $L_{m}=(M,M^{\prime})=\frac{b^{2}}{HW(n+k)}&#124;&#124;M-M^{\prime}&#124;&#124;_{1},$
    |  | (4) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{m}=(M,M^{\prime})=\frac{b^{2}}{HW(n+k)}&#124;&#124;M-M^{\prime}&#124;&#124;_{1},$
    |  | (4) |'
- en: '|  | $L_{v}=(M,M^{\prime})=\frac{b^{2}}{HW}\sum^{H_{b}}_{h=0}\sum^{W_{b}}_{w=0}Var(&#124;M-M^{\prime}&#124;),$
    |  | (5) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{v}=(M,M^{\prime})=\frac{b^{2}}{HW}\sum^{H_{b}}_{h=0}\sum^{W_{b}}_{w=0}Var(&#124;M-M^{\prime}&#124;),$
    |  | (5) |'
- en: where $b$ is the number of times every ‘slice’ of $M$ is replicated across the
    image in the spatial spread embedding technique [[40](#bib.bib40)], and $k$ relates
    to the number of tuples in the sequence created in this technique.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$ 是每个 $M$ 的“切片”在空间扩展嵌入技术中在图像中复制的次数 [[40](#bib.bib40)]，$k$ 与该技术中创建的序列的元组数量相关。
- en: V-E Adversarial Loss
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 对抗损失
- en: 'For deep learning models that use a discriminator network for adversarial learning,
    an additional loss function is added to the overall optimisation that accounts
    for the discriminator’s ability to detect an encoded image. Adversarial loss can
    be combined with loss at the encoder, since the encoder is the generative network,
    to improve the imperceptibility of the encoded watermark. The discriminator takes
    in an image $X$ that is either encoded or unaltered, and predicts $A(I)\epsilon[0,1]$,
    denoting the probability that $X$ has been encoded with a watermark. The discriminator
    optimises the function for adversarial loss from its predictions given by the
    following equation:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用判别网络进行对抗学习的深度学习模型，整体优化中会增加一个额外的损失函数，以考虑判别器检测编码图像的能力。对抗损失可以与编码器的损失结合，因为编码器是生成网络，以提高编码水印的不可察觉性。判别器接收一个图像
    $X$，该图像可能是编码的或未更改的，并预测 $A(I)\epsilon[0,1]$，表示 $X$ 被编码了水印的概率。判别器优化以下方程给出的对抗损失函数：
- en: '|  | $l_{a}(X,Y)=log(1-A(I_{c})+log(A(Y)),$ |  | (6) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{a}(X,Y)=log(1-A(I_{c})+log(A(Y)),$ |  | (6) |'
- en: where $X$ is the cover image, $Y$ is the encoded image. This equation is combined
    with the loss function for encoder loss in [[16](#bib.bib16), [28](#bib.bib28),
    [36](#bib.bib36)].
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X$ 是封面图像，$Y$ 是编码图像。这个方程与 [[16](#bib.bib16), [28](#bib.bib28), [36](#bib.bib36)]
    中的编码器损失函数结合使用。
- en: V-F KL Divergence
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-F KL 散度
- en: 'KL divergence is used to quantify the difference between probability distributions
    for a given random variable, for instance, the difference between an actual and
    observed probability distribution. It is often used in Generative Adversarial
    Networks to approximate the target probability distribution. This equation was
    used in [[59](#bib.bib59)] to calculate adversarial loss, where it is used in
    conjunction with JS divergence. This method for calculating adversarial loss can
    be represented in the following way:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度用于量化给定随机变量的概率分布之间的差异，例如实际概率分布与观察到的概率分布之间的差异。它通常在生成对抗网络中用于逼近目标概率分布。这个方程在
    [[59](#bib.bib59)] 中用于计算对抗损失，在该方程中与 JS 散度一起使用。计算对抗损失的方法可以表示为以下形式：
- en: '|  | $KL(P&#124;&#124;Q)=\sum_{c\epsilon{C}}P(c)log\frac{P(c)}{Q(c)},$ |  |
    (7) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | $KL(P&#124;&#124;Q)=\sum_{c\epsilon{C}}P(c)log\frac{P(c)}{Q(c)},$ |  |
    (7) |'
- en: '|  | $JS(P&#124;&#124;Q)=\frac{1}{2}KL(P&#124;&#124;\frac{P+Q}{2})+\frac{1}{2}KL(Q&#124;&#124;\frac{P+Q}{2}),$
    |  | (8) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $JS(P&#124;&#124;Q)=\frac{1}{2}KL(P&#124;&#124;\frac{P+Q}{2})+\frac{1}{2}KL(Q&#124;&#124;\frac{P+Q}{2}),$
    |  | (8) |'
- en: where $c$ is the cover image, $P$ is the probability distribution function for
    all cover images, and $Q$ is the probability distribution function for the generated
    steganographic images.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c$ 是封面图像，$P$ 是所有封面图像的概率分布函数，$Q$ 是生成的隐写图像的概率分布函数。
- en: V-G Cycle Consistency Loss
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-G 循环一致性损失
- en: 'This function is often used for CycleGAN implementations. The function performs
    unpaired image-to-image translation and is useful for problems where forward and
    backwards consistency between mapping functions is important. In both [[29](#bib.bib29),
    [62](#bib.bib62)], there are two generative and two discriminative models used,
    therefore the framework needs to learn the bijective mapping relationship between
    two image collections, where the first contains the original cover images and
    the second contains the secret images that serve as watermarks. The generative
    model needs to learn to transform from one source image domain to another, and
    this mapping relationship cannot ensure that the extracted watermark is the same
    as the original. Cycle consistency loss is detailed in [[62](#bib.bib62)] as an
    equation for the transformation of an X-domain style image to a Y-domain style
    image, where the generator models for both domains satisfy backward cycle consistency.
    Cycle adversarial loss is abstracted as the following:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通常用于 CycleGAN 实现。该函数执行无配对的图像到图像的转换，适用于映射函数之间的前向和反向一致性很重要的问题。在 [[29](#bib.bib29),
    [62](#bib.bib62)] 中，使用了两个生成模型和两个判别模型，因此框架需要学习两个图像集合之间的双射映射关系，第一个集合包含原始的封面图像，第二个集合包含作为水印的秘密图像。生成模型需要学习从一个源图像域到另一个图像域的转换，这种映射关系无法确保提取的水印与原始水印相同。Cycle一致性损失在
    [[62](#bib.bib62)] 中详细说明了将 X 域风格图像转换为 Y 域风格图像的方程，其中两个域的生成器模型满足反向循环一致性。循环对抗损失被抽象为以下内容：
- en: '|  | $D_{M}(M,M^{\prime})=G_{S}(G_{e}(X,M,A)),$ |  | (9) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | $D_{M}(M,M^{\prime})=G_{S}(G_{e}(X,M,A)),$ |  | (9) |'
- en: where $D_{S}$ represents the cycle discriminative network judging the extracted
    watermarks, $M$ is the original secret watermark, $M^{\prime}$ is the extracted
    watermark, $G_{S}$ is the generative model that generates secret images to use
    as watermarks, $X$ is the cover image, and $A$ is the attention mask.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{S}$ 代表判断提取水印的循环判别网络，$M$ 是原始秘密水印，$M^{\prime}$ 是提取的水印，$G_{S}$ 是生成模型，用于生成作为水印的秘密图像，$X$
    是封面图像，$A$ 是注意力掩膜。
- en: V-H Wasserstein Loss
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-H Wasserstein 损失
- en: 'The following models [[38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [45](#bib.bib45)] use a WGAN framework, therefore using a critic
    network rather than a discriminator to train the encoder. Rather than classifying
    an example as ‘real’ or ‘fake’, the critic outputs a number. The aim is to maximise
    this number for ‘real’ instances, which in this case are images that have been
    encoded with a message. Wasserstein loss is given by the following equation:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型 [[38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [45](#bib.bib45)] 使用 WGAN 框架，因此使用评论网络而非判别网络来训练编码器。评论网络输出一个数字，而不是将样本分类为“真实”或“虚假”。目标是最大化“真实”实例的这个数字，在此情况下，“真实”实例是已经编码了消息的图像。Wasserstein
    损失由以下方程给出：
- en: '|  | $l_{w}=C(X)-C(E(X,M)),$ |  | (10) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{w}=C(X)-C(E(X,M)),$ |  | (10) |'
- en: where $C(X)$ is the critic output for the initial cover image, and $C(E(X,M))$
    represents the critic output for the encoded image $E$ consisting of the cover
    image and watermark message $M$.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $C(X)$ 是初始封面图像的评论输出，$C(E(X,M))$ 代表由封面图像和水印消息 $M$ 组成的编码图像 $E$ 的评论输出。
- en: V-I Novel Approaches
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-I 新颖方法
- en: Certain approaches to performance optimisation do not follow the above framework
    for optimising at the encoder and decoder, along with a third possible adversarial
    component.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 某些性能优化方法并不遵循上述优化框架，而是采用编码器和解码器的优化方法，并可能包含第三个对抗组件。
- en: Image Fusion. The technique developed in [[23](#bib.bib23)] differs from many
    other deep learning techniques for digital watermarking by approaching the process
    as an image fusion task, aiming to maximise the correlation between the feature
    spaces of images.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图像融合。[[23](#bib.bib23)] 中开发的技术与许多其他数字水印深度学习技术不同，通过将过程视为图像融合任务，旨在最大化图像特征空间之间的相关性。
- en: 'The previous techniques focus on preserving certain parts of the cover image
    in the resulting encoded image. Different optimisations are applied to control
    embedding and extraction, and weights are used to control watermarking strength.
    Conversely, [[23](#bib.bib23)] takes 2 input spaces, $C$ denoting the cover image,
    and $W$ denoting the watermark. $W$ is mapped onto one of its latent feature spaces
    $W_{f}$. Watermark embedding is performed by the function $\{W_{f},C\}\to M$,
    where $M$ denotes the fusion feature space of the watermark and cover image, creating
    an intermediate latent space. In this system, the loss is computed using the correlation
    function given by:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的技术重点在于在生成的编码图像中保留覆盖图像的某些部分。应用了不同的优化方法来控制嵌入和提取，并使用权重来控制水印强度。相反，[[23](#bib.bib23)]
    采用了两个输入空间，$C$ 表示覆盖图像，$W$ 表示水印。$W$ 被映射到其潜在特征空间 $W_{f}$ 之一。水印嵌入由函数 $\{W_{f},C\}\to
    M$ 执行，其中 $M$ 表示水印和覆盖图像的融合特征空间，创建了一个中间潜在空间。在该系统中，使用以下公式计算损失：
- en: '|  | $\displaystyle\psi(m_{i},w_{f}^{i})=\frac{1}{2}(&#124;&#124;g(f_{1}(w_{f}^{i})),g(f_{1}(m_{i}))&#124;&#124;_{1}+$
    |  | (11) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\psi(m_{i},w_{f}^{i})=\frac{1}{2}(\|g(f_{1}(w_{f}^{i})),g(f_{1}(m_{i}))\|_{1}+$
    |  | (11) |'
- en: '|  | $\displaystyle&#124;&#124;g(f_{2}(w_{f}^{i})),g(f_{2}(m_{i}))&#124;&#124;_{1}),$
    |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|g(f_{2}(w_{f}^{i})),g(f_{2}(m_{i}))\|_{1}),$ |  |'
- en: where $m_{i}$ and $w_{f}^{i}$ are examples of the spaces $M$ and $W$ respectively.
    $g$ denotes the Gram matrix of all possible inner products and $f$ represents
    the convolutional blocks in the encoder network. A similar image fusion approach
    developed independently is utilised in [[49](#bib.bib49)]
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m_{i}$ 和 $w_{f}^{i}$ 分别是空间 $M$ 和 $W$ 的示例。$g$ 表示所有可能内积的 Gram 矩阵，$f$ 代表编码器网络中的卷积块。在[[49](#bib.bib49)]中独立开发的类似图像融合方法被应用于此。
- en: 'Distortion Minimisation Framework. The steganography model is detailed in [[55](#bib.bib55)],
    steganography is formulated as an optimisation problem with a payload constraint:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 失真最小化框架。隐写模型的详细信息见[[55](#bib.bib55)]，隐写被表述为具有负载约束的优化问题：
- en: '|  | $min_{S}D(X,Y),\psi(Y)=k,$ |  | (12) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $min_{S}D(X,Y),\psi(Y)=k,$ |  | (12) |'
- en: where $X$ is the cover image, $Y$ is the encoded stego-image, and $D(X,Y)$ is
    a function measuring the distortion caused by modifying $X$ to create $Y$. $\psi(S)$
    represents the data extracted from $Y$ and $k$ is the number of bits that make
    up the data. Different additive distortion functions may be used for $D$ that
    measure the cost of increasing each pixel value in the cover image by 1\. Large
    cost values are assigned to pixels more likely to cause perpetual differences
    in the final stego-image, and will therefore have a low probability of being modified
    during embedding.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X$ 是覆盖图像，$Y$ 是编码后的隐写图像，$D(X,Y)$ 是一个函数，用于测量通过修改 $X$ 生成 $Y$ 所引起的失真。$\psi(S)$
    表示从 $Y$ 中提取的数据，$k$ 是组成数据的位数。不同的附加失真函数可能用于 $D$，这些函数测量将覆盖图像中每个像素值增加 1 的成本。较大的成本值分配给更可能在最终隐写图像中造成永久性差异的像素，因此在嵌入过程中这些像素的修改概率较低。
- en: 'Inconsistent Loss. The CycleGAN model is introduced in [[29](#bib.bib29)],
    there is an additional loss function called inconsistent loss that ensures that
    a secret watermark image can only be extracted from an encoded image. Since the
    second generative model $G_{S}$ that extracts the watermarks also receives unencoded
    cover images as input, any data it extracts should be completely different from
    the real secret watermark image. This is given by the following equation:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 不一致损失。[[29](#bib.bib29)] 中介绍了 CycleGAN 模型，其中有一个额外的损失函数称为不一致损失，确保秘密水印图像只能从编码图像中提取出来。由于提取水印的第二个生成模型
    $G_{S}$ 也接受未编码的覆盖图像作为输入，因此它提取的任何数据应该与真实的秘密水印图像完全不同。这由以下方程给出：
- en: '|  | $max_{G_{S}}(G_{S},M^{\prime})=max_{G_{S}}&#124;G_{S}(X)-G_{S}(Y)&#124;,$
    |  | (13) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | $max_{G_{S}}(G_{S},M^{\prime})=max_{G_{S}}\|G_{S}(X)-G_{S}(Y)\|,$ |  |
    (13) |'
- en: where $M^{\prime}$ is the extracted watermark image, $X$ is the original cover
    image, and $Y$ is the encoded ‘target’ image. [[29](#bib.bib29)] combines this
    with adversarial loss and cycle adversarial loss to evaluate the model’s overall
    performance.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M^{\prime}$ 是提取的水印图像，$X$ 是原始覆盖图像，$Y$ 是编码后的‘目标’图像。[[29](#bib.bib29)] 将此与对抗损失和循环对抗损失相结合，以评估模型的整体性能。
- en: VI Evaluation Metrics
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 评估指标
- en: Evaluation metrics for data hiding techniques can be divided into two primary
    categories; those evaluating robustness and those evaluating encoded image quality.
    Additionally, there are metrics for measuring information capacity.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐藏技术的评估指标可以分为两个主要类别：评估鲁棒性的指标和评估编码图像质量的指标。此外，还有用于测量信息容量的指标。
- en: VI-A Robustness
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 鲁棒性
- en: The following metrics are used to evaluate the robustness of the embedded data.
    After attacks are applied to the encoded image and the data is extracted, these
    metrics are applied to compute the difference between the original message data
    and the data extracted from the image to judge how well that data survived distortions
    applied to it.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下指标用于评估嵌入数据的鲁棒性。在对编码图像施加攻击并提取数据后，这些指标用于计算原始消息数据和从图像中提取的数据之间的差异，以判断数据在经过失真处理后存活的情况。
- en: VI-A1 Bit Error Rate (BER)
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A1 比特误码率 (BER)
- en: 'is the number of bit errors divided by the total number of bits transferred
    over a certain time frame. It therefore gives the percentage of erroneous bits
    during the transmission. It is used to compare the extracted message and the initial
    message, each converted into binary. The BER between the original message $X$
    and extracted message $Y$ is given by the following equation:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 是在某个时间框架内传输的总比特数与比特错误数的比率。因此，它给出了传输过程中错误比特的百分比。它用于比较提取的消息和初始消息，每个消息都转换为二进制。原始消息
    $X$ 和提取消息 $Y$ 之间的 BER 由以下方程给出：
- en: '|  | $BER(X,Y)=100\,\frac{\text{{\small\#}\;Bits in error}}{\,\text{{\small\#}}_{Total}\;\text{Bits
    transmitted}}$ |  | (14) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $BER(X,Y)=100\,\frac{\text{{\small\#}\;Bits in error}}{\,\text{{\small\#}}_{Total}\;\text{Bits
    transmitted}}$ |  | (14) |'
- en: VI-A2 Normalised Correlation (NC)
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A2 归一化相关性 (NC)
- en: 'is a measure of the similarity between two signals as a relative displacement
    function. Normalised correlation gives values in the range [0,1] with a higher
    value denoting a higher similarity between images. The normalised correlation
    between the original message $X$ and extracted message $Y$ is given by the following
    equation:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 是对两个信号之间相似性的相对位移函数的度量。归一化相关性给出 [0,1] 范围内的值，值越高表示图像之间的相似度越高。原始消息 $X$ 和提取消息 $Y$
    之间的归一化相关性由以下方程给出：
- en: '|  | $NC(X,Y)=\frac{1}{WH}\sum_{i=0}^{W-1}\sum_{j=0}^{H-1}\delta(X_{i,j},Y_{i,j}),$
    |  | (15) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $NC(X,Y)=\frac{1}{WH}\sum_{i=0}^{W-1}\sum_{j=0}^{H-1}\delta(X_{i,j},Y_{i,j}),$
    |  | (15) |'
- en: where
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\delta({X,Y})=\begin{cases}1,&amp;\text{if}\ X=Y\\ 0,&amp;\text{otherwise}\end{cases}$
    |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | $\delta({X,Y})=\begin{cases}1,&\text{if}\ X=Y\\ 0,&\text{otherwise}\end{cases}$
    |  |'
- en: VI-B Quality
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 质量
- en: The following metrics are used to evaluate the quality of the encoded image.
    It is linked to the property of imperceptibility since embedded data should not
    produce any detectable perturbations in the image.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下指标用于评估编码图像的质量。这与不可感知性的特性相关，因为嵌入的数据不应在图像中产生任何可检测的扰动。
- en: VI-B1 Peak Signal to Noise Ratio (PSNR)
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B1 峰值信噪比 (PSNR)
- en: computes the difference between two images by taking the ratio between the maximum
    power of the signal and the power of corrupting noise that affects the signal.
    In data hiding, it is used to evaluate the difference between the cover image
    and the encoded image, showing the effectiveness of the embedding technique. PSNR
    is expressed in decibels (dB) on a logarithmic scale, with a value of above 30dB
    generally indicating that the image difference is not visible to the human eye.
    Therefore, PSNR can be used to quantify data imperceptibility. PSNR is defined
    by taking the Mean Square Error (MSE) of two images (cover image $I_{c}$ and watermarked
    image $I_{w}$) using the following equation.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 计算两幅图像之间的差异，通过取信号的最大功率与影响信号的噪声功率之间的比率。在数据隐藏中，它用于评估原始图像和编码图像之间的差异，显示嵌入技术的有效性。PSNR
    以分贝 (dB) 的对数刻度表示，值超过 30dB 通常表示图像差异对人眼不可见。因此，PSNR 可用于量化数据的不可感知性。PSNR 的定义是通过以下方程计算两幅图像（原始图像
    $I_{c}$ 和加水印图像 $I_{w}$）的均方误差 (MSE)。
- en: '|  | $PSNR(I_{c},I_{w})=10\,log_{10}\left(\frac{255^{2}}{MSE(I_{c},I_{w})}\right)$
    |  | (16) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $PSNR(I_{c},I_{w})=10\,log_{10}\left(\frac{255^{2}}{MSE(I_{c},I_{w})}\right)$
    |  | (16) |'
- en: VI-B2 Structural Similarity Index (SSIM)
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B2 结构相似性指数 (SSIM)
- en: a perception-based model that measures image degradation as perceived changes
    in structural information. It considers variance and covariance, which measure
    the dynamic range of pixels in an image. Unlike MSE and PSNR, which measure absolute
    error, SSIM considers the dependencies between spatially close pixels, incorporating
    luminance and contrast masking. SSIM is given in the range [-1.0,1.0], with 1
    indicating two identical images. The SSIM of two images is denoted as follows,
    where the mean of the two images is given by $\mu_{x}$ and $\mu_{y}$, and their
    variances are given by $\sigma^{2}_{x}$ and $\sigma^{2}_{y}$.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一种基于感知的模型，用于衡量图像退化作为结构信息的感知变化。它考虑了方差和协方差，这些量度了图像中像素的动态范围。与测量绝对误差的 MSE 和 PSNR
    不同，SSIM 考虑了空间上相近像素之间的依赖关系，结合了亮度和对比度掩蔽。SSIM 的范围是[-1.0,1.0]，1 表示两个图像完全相同。两个图像的 SSIM
    表示如下，其中两个图像的均值由 $\mu_{x}$ 和 $\mu_{y}$ 给出，其方差由 $\sigma^{2}_{x}$ 和 $\sigma^{2}_{y}$
    给出。
- en: '|  | $SSIM(x,y)=\frac{(2\mu_{x}\mu_{y}+k_{1})(2\sigma_{x}\sigma_{y}+k_{2})}{(\mu^{2}_{x}+\mu^{2}_{y}+k_{1})(\sigma^{2}_{x}+\sigma^{2}_{y}+k_{2})}$
    |  | (17) |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $SSIM(x,y)=\frac{(2\mu_{x}\mu_{y}+k_{1})(2\sigma_{x}\sigma_{y}+k_{2})}{(\mu^{2}_{x}+\mu^{2}_{y}+k_{1})(\sigma^{2}_{x}+\sigma^{2}_{y}+k_{2})}$
    |  | (17) |'
- en: The constants $k_{1}$ and $k_{2}$ exist to stop a 0/0 calculation. In the default
    configuration, $k_{1}=0.01$ and $k_{2}=0.03$.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 常数 $k_{1}$ 和 $k_{2}$ 的存在是为了防止 0/0 计算。在默认配置下，$k_{1}=0.01$ 和 $k_{2}=0.03$。
- en: VI-B3 Learned Perceptual Image Patch Similarity (LPIPS)
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B3 学习感知图像块相似度（LPIPS）
- en: 'is another type of perceptual loss. The aim of LPIPS [[68](#bib.bib68)] is
    to measure human perceptual capabilities by going beyond the simpler SSIM [17](#S6.E17
    "In VI-B2 Structural Similarity Index (SSIM) ‣ VI-B Quality ‣ VI Evaluation Metrics
    ‣ Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography")
    and PSNR [16](#S6.E16 "In VI-B1 Peak Signal to Noise Ratio (PSNR) ‣ VI-B Quality
    ‣ VI Evaluation Metrics ‣ Data Hiding with Deep Learning: A Survey Unifying Digital
    Watermarking and Steganography") metrics, studying the deeper nuances of human
    perception. It is a model that evaluates the distance between image patches. The
    model uses a large set of distortions and real algorithm outputs. There is consideration
    of traditional distortions, noise patterns, filtering, spatial warping operations
    and CNN-based algorithm outputs. LPIPS is able to perform better due to a larger
    dataset used compared to other datasets similar to this kind, as well as being
    able to use the outputs of real algorithms. LPIPS has a scoring system where the
    lower the score, the more similar to the compared image while a higher score indicates
    a larger difference between the images.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 是另一种类型的感知损失。LPIPS [[68](#bib.bib68)] 的目标是通过超越更简单的 SSIM [17](#S6.E17 "在 VI-B2
    结构相似性指数（SSIM） ‣ VI-B 质量 ‣ VI 评估指标 ‣ 深度学习的数据隐藏：统一数字水印和隐写术的调查") 和 PSNR [16](#S6.E16
    "在 VI-B1 峰值信噪比（PSNR） ‣ VI-B 质量 ‣ VI 评估指标 ‣ 深度学习的数据隐藏：统一数字水印和隐写术的调查") 指标，来衡量人类的感知能力，研究人类感知的更深层次细微差别。它是一个评估图像块之间距离的模型。该模型使用了大量的失真和真实算法输出。考虑了传统的失真、噪声模式、滤波、空间变形操作和基于
    CNN 的算法输出。由于使用了比其他类似数据集更大的数据集，并能够使用真实算法的输出，LPIPS 的表现更佳。LPIPS 具有一个评分系统，分数越低，表示与比较图像越相似，而分数越高则表示图像之间的差异越大。
- en: VI-B4 Frechet Inception Distance (FID)
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B4 Frechet Inception 距离（FID）
- en: is a model that measures the quality of a synthetic image. It was mainly developed
    to check the quality and performance of images generated by GAN structures. FID
    uses the inception v3 model [[69](#bib.bib69)] where in the last layer of FID,
    it is used to get the computer-vision-specific features of an input image. They
    are calculated for the activations for all images, real and generated. These two
    distributions are then calculated using FID. The score compares the quality of
    synthetic images based on how well inception v3 classifies them as one of the
    1,000 known objects. Essentially the generated images are not compared to real
    images, instead, they are compared to other synthetic images that have already
    been compared to real images and scoring their similarity. A low FID score indicates
    a higher quality image while a high FID score indicates a lower quality image.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个衡量合成图像质量的模型。它主要用于检查由GAN结构生成的图像的质量和性能。FID使用inception v3模型[[69](#bib.bib69)]，在FID的最后一层中，用于获取输入图像的计算机视觉特征。这些特征会对所有图像（真实的和生成的）进行激活计算。然后使用FID计算这两个分布。该评分通过inception
    v3如何将图像分类为1,000个已知对象之一来比较合成图像的质量。本质上，生成的图像不是与真实图像比较，而是与其他已经与真实图像比较过的合成图像进行比较，从而评分其相似性。低FID分数表示图像质量较高，而高FID分数表示图像质量较低。
- en: VI-C Capacity
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 容量
- en: Capacity is a measure of the amount of information that can be included in the
    data payload and subsequently embedded into the cover media.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 容量是衡量可以包含在数据负载中的信息量，并随后嵌入到载体媒体中的度量。
- en: VI-C1 Bits Per Pixel (BPP)
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C1 每像素位数（BPP）
- en: The number of bits is used to define the colour value of a pixel. A higher BPP
    value denotes a higher number of possible colour values for a pixel. Therefore,
    a higher BPP value for payload capacity shows that more identifying data can be
    embedded in the pixels of an image. BPP is used to calculate data payload capacity
    in [[38](#bib.bib38)].
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 位数用于定义像素的颜色值。较高的BPP值表示像素可能的颜色值更多。因此，较高的BPP值表明更高的数据承载能力，显示更多的识别数据可以嵌入到图像的像素中。BPP用于计算数据承载能力，如[[38](#bib.bib38)]所述。
- en: VI-C2 Reed Solomon Bits Per Pixel (RS-BPP)
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C2 Reed-Solomon 每像素位数（RS-BPP）
- en: 'The paper [[38](#bib.bib38)] introduces a new metric RS-BPP based on Reed-Solomon
    codes. In deep data hiding models, measuring the number of bits that can be embedded
    per pixel depends on the model, the cover image, and the model itself, and is
    therefore non-trivial to measure just in BPP. Given an algorithm that returns
    an erroneous bit with probability $p$, the aim is to have a number of incorrect
    bits that is less than or equal to the number of bits that can be corrected. This
    can be represented by the equation:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[[38](#bib.bib38)]介绍了一种基于Reed-Solomon编码的新指标RS-BPP。在深度数据隐藏模型中，每像素可以嵌入的位数的测量取决于模型、覆盖图像和模型本身，因此仅用BPP来测量并非易事。给定一个以概率$p$返回错误位的算法，目标是使错误位的数量小于或等于可以纠正的位数。这可以用以下方程表示：
- en: '|  | $pn\leq\frac{n-k}{2},$ |  | (18) |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | $pn\leq\frac{n-k}{2},$ |  | (18) |'
- en: where the $\frac{k}{n}$ ratio is the number of data bits that can be transmitted
    for each bit of the message. RS-BPP allows deep data hiding techniques such as
    [[38](#bib.bib38)] to be directly compared to traditional algorithms in terms
    of capacity.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\frac{k}{n}$ 比率是每个消息位可以传输的数据位数。RS-BPP允许将深度数据隐藏技术（如[[38](#bib.bib38)]）与传统算法在容量方面进行直接比较。
- en: VII Datasets
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 数据集
- en: This section presents a table that displays the image databases utilized to
    train the deep data hiding models discussed in this survey.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一个表格，展示了用于训练本调查中讨论的深度数据隐藏模型的图像数据库。
- en: 'Describable Textures Dataset (DTD): The DTD dataset includes multiple kinds
    of labelled texture images, totalling 5640 images, and was used to train [[58](#bib.bib58)].'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性纹理数据集（DTD）：DTD数据集包括多种标签纹理图像，总计5640张图像，用于训练[[58](#bib.bib58)]。
- en: 'TABLE V: Summary table of datasets used to train deep learning-based data hiding
    models.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 表V：用于训练基于深度学习的数据隐藏模型的数据集汇总表。
- en: '| Name | Trained | Description |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 训练 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| COCO [[67](#bib.bib67)] | [[16](#bib.bib16), [38](#bib.bib38), [30](#bib.bib30),
    [29](#bib.bib29), [21](#bib.bib21), [36](#bib.bib36), [40](#bib.bib40), [41](#bib.bib41),
    [58](#bib.bib58), [49](#bib.bib49)] | 330 images of everyday scenes. Cluttered
    images useful for DH |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| COCO [[67](#bib.bib67)] | [[16](#bib.bib16), [38](#bib.bib38), [30](#bib.bib30),
    [29](#bib.bib29), [21](#bib.bib21), [36](#bib.bib36), [40](#bib.bib40), [41](#bib.bib41),
    [58](#bib.bib58), [49](#bib.bib49)] | 330张日常场景图像。对数据隐藏有用的杂乱图像 |'
- en: '| DIV2K [[70](#bib.bib70)] | [[38](#bib.bib38), [30](#bib.bib30), [59](#bib.bib59)]
    | 1k low resolution images. Includes open scenery difficult for DH |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| DIV2K [[70](#bib.bib70)] | [[38](#bib.bib38), [30](#bib.bib30), [59](#bib.bib59)]
    | 1k 低分辨率图像。包括对 DH 来说具有挑战性的开放风景 |'
- en: '| CIFAR-10 [[71](#bib.bib71)] | [[25](#bib.bib25), [23](#bib.bib23)] | 60k
    32x32 images of singular objects |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-10 [[71](#bib.bib71)] | [[25](#bib.bib25), [23](#bib.bib23)] | 60k
    32x32 单物体图像 |'
- en: '| Pascal VOC [[72](#bib.bib72)] | [[25](#bib.bib25), [57](#bib.bib57)] | 15k
    images of singular objects |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Pascal VOC [[72](#bib.bib72)] | [[25](#bib.bib25), [57](#bib.bib57)] | 15k
    单物体图像 |'
- en: '| BOSSBase [[73](#bib.bib73)] | [[24](#bib.bib24), [26](#bib.bib26), [63](#bib.bib63),
    [55](#bib.bib55), [53](#bib.bib53)] | 9k 512x512 greyscale images in PGM format
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| BOSSBase [[73](#bib.bib73)] | [[24](#bib.bib24), [26](#bib.bib26), [63](#bib.bib63),
    [55](#bib.bib55), [53](#bib.bib53)] | 9k 512x512 灰度图像，PGM 格式 |'
- en: '| ImageNet [[74](#bib.bib74)] | [[23](#bib.bib23), [57](#bib.bib57), [62](#bib.bib62),
    [55](#bib.bib55), [56](#bib.bib56), [17](#bib.bib17), [48](#bib.bib48), [51](#bib.bib51),
    [52](#bib.bib52), [45](#bib.bib45)] | 14mil images organised based on WordNet
    hierarchy |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet [[74](#bib.bib74)] | [[23](#bib.bib23), [57](#bib.bib57), [62](#bib.bib62),
    [55](#bib.bib55), [56](#bib.bib56), [17](#bib.bib17), [48](#bib.bib48), [51](#bib.bib51),
    [52](#bib.bib52), [45](#bib.bib45)] | 基于 WordNet 层次结构组织的 14百万图像 |'
- en: '| MIRFLICKR [[75](#bib.bib75)] | [[42](#bib.bib42), [45](#bib.bib45)] | 1mil
    Flickr images under Creative Commons license |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| MIRFLICKR [[75](#bib.bib75)] | [[42](#bib.bib42), [45](#bib.bib45)] | 100
    万 Flickr 图像，采用 Creative Commons 许可证 |'
- en: '| Flickr30k [[76](#bib.bib76)] | [[50](#bib.bib50)] | 31k images from Flickr
    under Creative Commons license |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Flickr30k [[76](#bib.bib76)] | [[50](#bib.bib50)] | 31k 来自 Flickr 的图像，采用
    Creative Commons 许可证 |'
- en: '| Labeled Faces in the Wild (LFW) [[77](#bib.bib77)] | [[57](#bib.bib57)] |
    13k images of faces from the web |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Labeled Faces in the Wild (LFW) [[77](#bib.bib77)] | [[57](#bib.bib57)] |
    13k 来自网络的面部图像 |'
- en: '| Describable Textures Dataset (DTD) [[78](#bib.bib78)] | [[58](#bib.bib58)]
    | 5k labelled texture images |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Describable Textures Dataset (DTD) [[78](#bib.bib78)] | [[58](#bib.bib58)]
    | 5k 标签纹理图像 |'
- en: 'COCO: COCO is a large-scale object detection, segmentation, and captioning
    dataset [[67](#bib.bib67)]. The dataset contains 330 thousand images of complex,
    everyday scenes including common objects. The goal of the dataset was to advance
    AI scene understanding by combining the more narrow goals of image classification,
    object localisation, and semantic segmentation. Since the dataset consists of
    relatively cluttered images picturing multiple objects, there are more surfaces
    and variations in textures in which to embed watermarking data, making it useful
    and popular for data hiding applications. COCO was used to train [[16](#bib.bib16),
    [38](#bib.bib38), [30](#bib.bib30), [29](#bib.bib29), [21](#bib.bib21), [36](#bib.bib36),
    [40](#bib.bib40), [41](#bib.bib41), [58](#bib.bib58), [49](#bib.bib49)] and to
    test [[25](#bib.bib25), [23](#bib.bib23), [27](#bib.bib27), [28](#bib.bib28)].'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 'COCO: COCO 是一个大规模的目标检测、分割和标注数据集 [[67](#bib.bib67)]。该数据集包含 33 万张复杂的日常场景图像，包括常见物体。数据集的目标是通过结合图像分类、物体定位和语义分割的较窄目标来推进
    AI 场景理解。由于数据集包含相对杂乱的多物体图像，因此具有更多的表面和纹理变化以嵌入水印数据，使其在数据隐藏应用中非常有用和受欢迎。COCO 被用于训练
    [[16](#bib.bib16), [38](#bib.bib38), [30](#bib.bib30), [29](#bib.bib29), [21](#bib.bib21),
    [36](#bib.bib36), [40](#bib.bib40), [41](#bib.bib41), [58](#bib.bib58), [49](#bib.bib49)]
    和测试 [[25](#bib.bib25), [23](#bib.bib23), [27](#bib.bib27), [28](#bib.bib28)]。'
- en: 'DIV2K: Div2K is a single-image super-resolution dataset of 1000 images of different
    scenes. The dataset consists of low resolution images with different types of
    degradations applied. Compared to COCO, it contains more images of open scenery,
    which can present a challenge for data embedding. Therefore, it is used to train
    [[38](#bib.bib38), [30](#bib.bib30)] in conjunction with [[67](#bib.bib67)] to
    ensure the model is trained on a variety of different types of images. On its
    own, it is used to train [[59](#bib.bib59)].'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 'DIV2K: Div2K 是一个包含 1000 张不同场景图像的单图像超分辨率数据集。该数据集包含不同类型退化处理的低分辨率图像。与 COCO 相比，它包含更多的开放风景图像，这对数据嵌入提出了挑战。因此，它与
    [[67](#bib.bib67)] 一起用于训练 [[38](#bib.bib38), [30](#bib.bib30)]，以确保模型在多种不同类型的图像上进行训练。单独使用时，它被用于训练
    [[59](#bib.bib59)]。'
- en: 'CIFAR-10: CIFAR-10 is a labelled subset of the 80-million Tiny Images Dataset
    [[71](#bib.bib71)], and contains 60 thousand small 32x32 images. They mostly picture
    single objects such as animals or vehicles, and are separated into 10 classes
    depending on the subject, though these are ignored for data hiding applications.
    CIFAR-10 was used to train [[25](#bib.bib25), [23](#bib.bib23)].'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 'CIFAR-10: CIFAR-10是80百万Tiny Images数据集的一个标注子集[[71](#bib.bib71)]，包含6万张32x32的小图像。这些图像大多描绘单一物体，如动物或车辆，并根据主题分为10个类别，尽管这些类别在数据隐藏应用中被忽略。CIFAR-10被用于训练[[25](#bib.bib25),
    [23](#bib.bib23)]。'
- en: 'Pascal VOC: Similar to [[71](#bib.bib71)], the Pascal VOC dataset [[72](#bib.bib72)]
    includes pictures of objects such as vehicles and animals rather than full scenes
    with multiple objects. This dataset has been widely used as a benchmark for object
    detection, semantic segmentation, and classification tasks. It was used to train
    [[25](#bib.bib25), [57](#bib.bib57)].'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pascal VOC: 与[[71](#bib.bib71)]类似，Pascal VOC数据集[[72](#bib.bib72)]包括了如车辆和动物等物体的图像，而不是多个物体的完整场景。该数据集已被广泛用于对象检测、语义分割和分类任务的基准测试。它被用于训练[[25](#bib.bib25),
    [57](#bib.bib57)]。'
- en: 'BOSSBase: This dataset contains 9074 512x512 greyscale images in PGM format
    [[73](#bib.bib73)]. It is widely used for training steganography algorithms. The
    dataset was used to train [[24](#bib.bib24), [26](#bib.bib26), [63](#bib.bib63),
    [55](#bib.bib55), [53](#bib.bib53)].'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 'BOSSBase: 该数据集包含了9074张512x512的灰度PGM格式图像[[73](#bib.bib73)]。它被广泛用于训练隐写算法。该数据集被用于训练[[24](#bib.bib24),
    [26](#bib.bib26), [63](#bib.bib63), [55](#bib.bib55), [53](#bib.bib53)]。'
- en: 'ImageNet: ImageNet contains 14 million images organized according to the WordNet
    hierarchy [[74](#bib.bib74)]. Images are classified into ‘synsets’ based on their
    subjects, all sorted and labelled accordingly. It was used to train [[23](#bib.bib23),
    [57](#bib.bib57), [62](#bib.bib62), [55](#bib.bib55), [56](#bib.bib56), [17](#bib.bib17),
    [48](#bib.bib48), [51](#bib.bib51), [52](#bib.bib52), [45](#bib.bib45)].'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 'ImageNet: ImageNet包含了1400万张图像，这些图像根据WordNet层级进行组织[[74](#bib.bib74)]。图像被根据其主题分类为“同义词集”，所有这些都被相应地排序和标记。它被用于训练[[23](#bib.bib23),
    [57](#bib.bib57), [62](#bib.bib62), [55](#bib.bib55), [56](#bib.bib56), [17](#bib.bib17),
    [48](#bib.bib48), [51](#bib.bib51), [52](#bib.bib52), [45](#bib.bib45)]。'
- en: 'MIRFLICKR: The MIRFLICKR dataset [[75](#bib.bib75)] contains 1 million Flickr
    images under the Creative Commons license. It is used to train [[42](#bib.bib42),
    [45](#bib.bib45)].'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 'MIRFLICKR: MIRFLICKR数据集[[75](#bib.bib75)]包含了100万张在知识共享许可证下的Flickr图像。它被用于训练[[42](#bib.bib42),
    [45](#bib.bib45)]。'
- en: 'Flickr30k: The Flickr30k dataset [[76](#bib.bib76)] contains 31,000 images
    collected from Flickr, together with 5 reference sentences provided by human annotators.
    It was used to train [[50](#bib.bib50)].'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 'Flickr30k: Flickr30k数据集[[76](#bib.bib76)]包含了从Flickr收集的31,000张图像，以及由人工标注员提供的5个参考句子。它被用于训练[[50](#bib.bib50)]。'
- en: 'Labeled Faces in the Wild (LFW): The LFW dataset [[77](#bib.bib77)] public
    benchmark for face verification, contains more than 13,000 images of faces collected
    from the web. The dataset was used to train [[57](#bib.bib57)].'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 'Labeled Faces in the Wild (LFW): LFW数据集[[77](#bib.bib77)]是一个用于人脸验证的公共基准，包含了从网络收集的超过13,000张人脸图像。该数据集被用于训练[[57](#bib.bib57)]。'
- en: VIII Open Questions and Future Work
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第八章 开放问题与未来工作
- en: Deep learning for data hiding is a new and evolving research field, and there
    remain many different avenues to consider moving forward. This section will discuss
    some open questions that we believe warrant further research and consideration
    including expanding the applications of digital watermarking to other media domains,
    pursuing deep learning-based language watermarking, improving robustness against
    deep learning-based watermark removal attacks, watermarking machine learning models,
    combating the use of watermarking to launch backdoor attacks on machine learning
    models, and exploring the applications of watermarking for detecting and identifying
    synthetic media. Finally, future directions for steganography are discussed, including
    its potential use for spreading malware.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐藏的深度学习是一个新兴且不断发展的研究领域，未来还有许多不同的方向需要考虑。本节将讨论一些我们认为值得进一步研究和考虑的开放问题，包括将数字水印的应用扩展到其他媒体领域，探索基于深度学习的语言水印，提升对基于深度学习的水印去除攻击的鲁棒性，对机器学习模型进行水印处理，打击利用水印发起的机器学习模型后门攻击，以及探索水印在检测和识别合成媒体中的应用。最后，讨论了隐写术的未来方向，包括其在传播恶意软件中的潜在使用。
- en: VIII-A Expanding Applications for Deep Learning Digital Watermarking Models
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第八章-A 扩展深度学习数字水印模型的应用
- en: While the deep watermarking models discussed in this survey were primarily focused
    on image watermarking, there is significant potential for applying watermarking
    to other types of media. Although there are many traditional algorithms focused
    on watermarking video, audio, 3D models, and electronics, there is yet to be any
    deep learning models focusing on these areas. For instance, various techniques
    have been proposed for watermarking 3D models [[79](#bib.bib79)], including new
    promising methods for watermarking vertices data [[80](#bib.bib80)]. There are
    also existing works concerning GANs generating 3D models [[81](#bib.bib81), [82](#bib.bib82)].
    Furthermore, a recent paper from Google [[15](#bib.bib15)] details a promising
    deep learning technique of embedding watermark messages into simple 3D meshes
    which can then be decoded from 2D rendered images of the model from multiple angles
    and lighting configurations. It is noted in this work that robustness and capacity
    will need to be improved before practical application, particularly robustness
    to non-differentiable 3D attacks. More complex models and lighting arrangements
    could be explored with a better-quality renderer. A paper from Google research
    by Innfarn et al. [[15](#bib.bib15)] sets the precedent for watermarking 3D models
    using deep learning, but more work is required before a generalised, practically-applicable
    framework for 3D watermarking can be developed.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本研究中讨论的深度水印模型主要集中在图像水印上，但在其他类型媒体上应用水印仍存在巨大潜力。虽然已经有许多传统算法专注于视频水印，音频水印，3D模型和电子设备的水印，但目前还没有专注于这些领域的深度学习模型。例如，已经有各种技术提出用于水印3D模型[[79](#bib.bib79)],
    包括用于水印顶点数据的新方法[[80](#bib.bib80)]。还有一些关于GAN生成3D模型 [[81](#bib.bib81), [82](#bib.bib82)]
    的研究。此外，谷歌最近发表的一篇论文[[15](#bib.bib15)]详细介绍了一种有前途的深度学习技术，将水印消息嵌入简单的3D网格中，然后可以从多个角度和光照配置的2D渲染图像中解码。这篇论文指出，在实际应用之前，需要改进的有鲁棒性和容量，特别是非可微分的3D攻击的鲁棒性。更复杂的模型和照明设置可以通过更高质量的渲染器进行探索。由Google研究的Innfarn等人的一篇论文[[15](#bib.bib15)]为基于深度学习的3D模型水印制定了先例，但在开发出通用的、实用的3D水印框架之前，还需要进行更多工作。
- en: Similarly, audio watermarking faces a comparable situation. While there are
    existing traditional methods for audio watermarking, and GAN-based frameworks
    for audio generation [[83](#bib.bib83), [84](#bib.bib84)], there is currently
    a lack of exploration of deep learning frameworks specifically designed for audio
    watermarking. This suggests that machine learning models have the potential to
    learn audio embedding techniques and apply them to audio databases. As far as
    we are aware, there are currently no works that investigate deep learning frameworks
    for audio watermarking. However, given the growing interest in this field, it
    is likely that there will be forthcoming contributions.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，音频水印也面临类似情况。尽管已经存在用于音频水印的传统方法和基于GAN的音频生成框架[[83](#bib.bib83), [84](#bib.bib84)],
    但目前还没有专门针对音频水印设计的深度学习框架的深入探索。这表明机器学习模型有可能学习音频嵌入技术并将其应用于音频数据库。据我们所知，目前没有研究深度学习框架进行音频水印。然而，鉴于对这一领域的兴趣日益增长，很可能会有相关的工作出现。
- en: In addition to image and audio watermarking, video watermarking is another promising
    direction for research. There are existing traditional algorithms for video watermarking
    [[8](#bib.bib8)], and deep learning techniques are also being considered, for
    example in [[85](#bib.bib85)]. This paper introduces RIVAGAN, a new architecture
    for robust video watermarking. The attention-based architecture is robust against
    common video processing attacks such as scaling, cropping, and compression. In
    the framework, a 32-bit watermark is embedded into a sequence of frames, and the
    watermark can be extracted from any individual or collection of frames. The framework
    uses an attention mechanism to identify robust areas for embedding, and produces
    watermarked footage that is nearly indistinguishable from human observers (approximately
    52% detection accuracy). There is also DVMark [[14](#bib.bib14)] from Google Research,
    which employs a multiscale design where the watermark is distributed across multiple
    spatial-temporal scales. It was found to be more robust than traditional video
    watermarking algorithms (3D-DWT) and the deep learning-based image watermarking
    framework from HiDDeN [[16](#bib.bib16)] against video distortions, while retaining
    a high level of quality. A 3D-CNN that simulates video compression attacks was
    used during training to achieve high levels of robustness. The framework in DVMark
    [[14](#bib.bib14)] also includes a watermark detector that can analyse a long
    video and locate multiple short instances of copyrighted content. This could be
    useful for reliably identifying copyright infringement on online video platforms
    such as YouTube.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像和音频水印外，视频水印也是一个有前景的研究方向。目前已有传统的视频水印算法[[8](#bib.bib8)]，同时也在考虑深度学习技术，例如在[[85](#bib.bib85)]中提到。本文介绍了RIVAGAN，一种用于强健视频水印的新架构。基于注意力机制的架构对常见的视频处理攻击（如缩放、裁剪和压缩）具有很强的鲁棒性。在该框架中，32位水印被嵌入到一系列帧中，水印可以从任何单个帧或帧集提取。该框架使用注意力机制来识别嵌入的鲁棒区域，并生成几乎无法被人眼识别的水印视频（检测准确率约为52%）。此外，Google
    Research还开发了DVMark [[14](#bib.bib14)]，它采用了多尺度设计，将水印分布在多个时空尺度上。与传统的视频水印算法（3D-DWT）和HiDDeN
    [[16](#bib.bib16)]的基于深度学习的图像水印框架相比，它在对抗视频失真方面更具鲁棒性，同时保持了较高的质量。在训练过程中使用了模拟视频压缩攻击的3D-CNN，以实现高水平的鲁棒性。DVMark
    [[14](#bib.bib14)]框架还包括一个水印检测器，可以分析长视频并定位多个短时段的版权内容。这对于在YouTube等在线视频平台上可靠地识别版权侵权可能非常有用。
- en: VIII-B Text Watermarking for Combating Misinformation
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 文本水印用于打击虚假信息
- en: Another promising application for deep watermarking is in watermarking text.
    As natural language generation technology improves, machine learning models are
    capable of generating highly fluent language that can fool human detectors [[86](#bib.bib86)].
    There is growing concern that such models will be used to spread misinformation
    and ‘fake news’ online. The Adversarial Watermarking Transformer (AWT) developed
    by Sahar et al. [[9](#bib.bib9)] is the first end-to-end deep learning model for
    language watermarking. Language watermarking is inherently more complex than image,
    video, and audio watermarking because the language itself must be altered, which
    can cause drastic syntactic and semantic changes. Previous techniques include
    synonym replacement and altering sentence structure, which rely on fixed rule-based
    techniques. The aim of such techniques is to achieve high effectiveness, secrecy,
    and robustness, while sustaining only subtle changes to the text, preserving correct
    structure and grammar, as well as language statistics. The deep learning model
    AWT [[9](#bib.bib9)] uses an attention mechanism and adversarial training to achieve
    robustness against attacks such as denoising, random changes, and re-watermarking.
    The model undergoes human evaluation and achieves better results than the state-of-the-art
    synonym substitution baseline. This technique only works effectively for long
    pieces of text such as news articles, whereas shorter pieces would require longer
    relative watermarks, thereby noticeably degrading the original text. For practical
    scenarios, it is suggested to combine this AWT technique with automated or human
    fact-checking to reduce the likelihood of false positives. Further research into
    deep learning-based models for language watermarking is highly important as text-generating
    models continue to improve and become widely available to potentially malicious
    actors. This will help to identify misinformation and differentiate generated
    from genuine text.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 深度水印技术的另一个有前景的应用是文本水印。随着自然语言生成技术的进步，机器学习模型能够生成高度流畅的语言，可能会欺骗人工检测器[[86](#bib.bib86)]。越来越多的人担心这些模型将被用来传播虚假信息和‘假新闻’。Sahar等人开发的对抗水印变换器（AWT）[[9](#bib.bib9)]是第一个端到端的深度学习语言水印模型。语言水印本质上比图像、视频和音频水印更复杂，因为必须改变语言本身，这可能导致语法和语义的剧烈变化。之前的技术包括同义词替换和句子结构调整，这些技术依赖于固定的规则基础技术。这些技术的目标是实现高效性、保密性和鲁棒性，同时对文本进行微小的改变，保持正确的结构和语法以及语言统计。深度学习模型AWT[[9](#bib.bib9)]使用注意机制和对抗训练来实现对去噪、随机变化和再水印等攻击的鲁棒性。该模型经过人工评估，取得了比最先进的同义词替换基线更好的结果。这项技术仅在长文本（如新闻文章）中有效，而较短的文本则需要较长的相对水印，从而显著降低原始文本的质量。在实际场景中，建议将此AWT技术与自动化或人工事实检查结合使用，以减少假阳性的可能性。随着文本生成模型的持续改进和可能恶意行为者的广泛使用，进一步研究基于深度学习的语言水印模型非常重要。这将有助于识别虚假信息，并区分生成的文本和真实文本。
- en: VIII-C Mitigating Watermark Removal Attacks
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-C 缓解水印移除攻击
- en: As technology for watermark embedding improves, so too does technology for attacking
    and removing those watermarks. Thus, the process can be regarded as an ever-evolving,
    adversarial game between content owners and attackers. Currently, deep learning-based
    techniques for watermark removal exist that are able to remove information robustly
    embedded using the top frequency domain algorithms [[13](#bib.bib13)]. This technique
    uses a simple CNN to perform denoising removal attacks, and is able to not only
    remove the watermark, but also recover the original cover images without significant
    quality degradation. It was tested on a dataset of watermarked images created
    using state-of-the-art DCT, DWT, and DFT traditional algorithms in a black-box
    setting. Although this technique is focused on traditional algorithms, as deep
    learning techniques for digital watermarking evolve, it is inevitable that adversarial
    techniques will continue to be developed that aim to remove these watermarks.
    Therefore, it is important to continue to improve the robustness of watermarking
    techniques so they can resist these emerging, deep learning-based removal methods.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 随着水印嵌入技术的提高，攻击和去除这些水印的技术也在不断进步。因此，这一过程可以被视为内容拥有者和攻击者之间不断演变的对抗游戏。目前，存在基于深度学习的水印去除技术，能够去除通过顶频域算法鲁棒嵌入的信息[[13](#bib.bib13)]。这一技术使用简单的CNN执行去噪去除攻击，能够不仅去除水印，还能在没有显著质量退化的情况下恢复原始封面图像。该技术在使用先进的DCT、DWT和DFT传统算法创建的水印图像数据集上进行了测试，采用了黑箱设置。尽管这一技术专注于传统算法，但随着数字水印深度学习技术的发展，针对去除这些水印的对抗性技术必将继续发展。因此，继续提升水印技术的鲁棒性以抵抗这些新兴的基于深度学习的去除方法是非常重要的。
- en: Current deep watermarking strategies have been tested against a range of attacks
    including cropping, pixel dropout, compression, and blurring. However, in most
    models these attacks are encapsulated by a differentiable attack layer, as was
    implemented in [[16](#bib.bib16), [25](#bib.bib25), [28](#bib.bib28)], meaning
    that they must support backpropagation, which is not representative of many real-world
    attack scenarios. However, it should be noted that these models can still simulate
    JPEG compression, which is non-differentiable, by using differentiable approximations.
    Promising techniques for improving the scope of attacks used during training include
    generating attacks using adversarial examples from a trained CNN [[21](#bib.bib21)],
    and the use of black-box noise [[27](#bib.bib27)], which are from algorithms encapsulated
    in the image processing software that is difficult to simulate. To achieve optimal
    robustness, it is important to train models on attacks generated from an adversarial
    network to improve results, rather than generating attacks from a fixed pool of
    differentiable attacks, as was done in earlier implementations.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的深度水印策略已经经过了对一系列攻击的测试，包括裁剪、像素丢失、压缩和模糊。然而，在大多数模型中，这些攻击被封装在一个可微分的攻击层中，如[[16](#bib.bib16),
    [25](#bib.bib25), [28](#bib.bib28)]所实施的，这意味着它们必须支持反向传播，这并不代表许多现实世界中的攻击场景。然而，需要注意的是，这些模型仍然可以通过使用可微分的近似来模拟非可微分的JPEG压缩。用于改善训练期间攻击范围的有前途的技术包括使用经过训练的CNN生成对抗性示例[[21](#bib.bib21)]，以及使用黑箱噪声[[27](#bib.bib27)]，这些噪声来自于图像处理软件中封装的算法，这些算法难以模拟。为了实现最佳的鲁棒性，重要的是训练模型以应对来自对抗网络生成的攻击，以改善结果，而不是从固定的可微分攻击池中生成攻击，如早期实现中所做的那样。
- en: VIII-D Watermarking for Protecting Machine Learning Models
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-D 保护机器学习模型的水印技术
- en: Another important application for digital watermarking is protecting machine
    learning models as intellectual property. Although the survey has discussed watermarking
    digital media such as images, audio, and video, machine learning models themselves
    require increasingly large amounts of computational resources and private training
    data to train and operate. Therefore, there is a growing need to protect machine
    learning models as intellectual property. Digital watermarking is just one of
    many tasks that were once done using traditional algorithms, but are now being
    offloaded to the effectiveness of machine learning strategies. As this happens,
    it is important to protect these models from theft and misuse, not only because
    of the resource expenditure by the owners in creating these models, but because
    their immense computational capabilities could be used for malicious activities.
    There are many techniques currently being researched for watermarking machine
    learning models, most of which rely on embedded identifying information into training
    datasets [[87](#bib.bib87)]. However, as was learned through the concept of adversarial
    examples, even small perturbations in training instances can cause extreme degradation
    in the model’s performance. Therefore, many watermarking strategies also sacrifice
    the model’s classification accuracy.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 数字水印的另一个重要应用是保护机器学习模型作为知识产权。尽管调查已讨论了图像、音频和视频等数字媒体的水印，但机器学习模型本身需要越来越多的计算资源和私人训练数据来训练和操作。因此，保护机器学习模型作为知识产权的需求不断增长。数字水印只是许多曾经使用传统算法完成的任务中的一个，现在被转移到机器学习策略的有效性上。随着这种情况的发展，保护这些模型免受盗窃和滥用变得至关重要，不仅因为所有者在创建这些模型时的资源支出，还因为其巨大的计算能力可能被用于恶意活动。目前，许多技术正在研究用于水印机器学习模型，其中大多数依赖于将识别信息嵌入到训练数据集中
    [[87](#bib.bib87)]。然而，正如通过对抗样本的概念所了解到的，即使训练实例中的小扰动也会导致模型性能的极端下降。因此，许多水印策略也牺牲了模型的分类准确性。
- en: A famous example is DeepSigns [[88](#bib.bib88)], an end-to-end IP protection
    framework that inserts digital watermarks into deep learning models by embedding
    the watermark into the probability density function of the activation sets in
    different layers of the network. It is robust against a range of attacks, including
    model compression, fine-tuning, and watermark overwriting, which proved challenging
    for previous model watermarking techniques.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的例子是 DeepSigns [[88](#bib.bib88)]，这是一个端到端的 IP 保护框架，通过将数字水印嵌入到网络不同层次的激活集合的概率密度函数中来将水印插入到深度学习模型中。它对一系列攻击具有鲁棒性，包括模型压缩、微调和水印覆盖，这些都是以前的模型水印技术面临的挑战。
- en: One recent and promising technique for watermarking training datasets is Entangled
    Watermarking Embeddings [[89](#bib.bib89)]. Instead of only learning features
    sampled from the task distribution, the defending model also samples data that
    encode watermarks when classifying data. Therefore, watermark data is entangled
    with legitimate training data, so an adversary attempting to remove the watermarks
    cannot do this without damaging the training data itself, and thereby sacrificing
    performance. The method uses Soft Nearest Neighbour Loss to increase entanglement,
    a new loss function. And the method is evaluated in the image and audio domains,
    showing that with this method an owner can claim with 95% confidence that model
    extraction has taken place to produce a similarly performing model by extracting
    prediction vectors. This technique notably shows robustness against adaptive adversaries,
    meaning the adversary has knowledge of the watermarking technique being used.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 一种近期且有前景的训练数据集水印技术是 Entangled Watermarking Embeddings [[89](#bib.bib89)]。与仅从任务分布中学习特征不同，防御模型在分类数据时还会采样编码水印的数据。因此，水印数据与合法的训练数据纠缠在一起，因此试图去除水印的对手必须损害训练数据本身，从而牺牲性能。这种方法使用
    Soft Nearest Neighbour Loss 来增加纠缠度，这是一种新的损失函数。该方法在图像和音频领域进行评估，结果表明，使用这种方法，所有者可以以
    95% 的信心声称，模型提取已经发生，从而通过提取预测向量生成了具有相似性能的模型。这种技术特别显示出对适应性对手的鲁棒性，这意味着对手知道正在使用的水印技术。
- en: As machine learning models become more ubiquitous across a range of industries,
    it is important to implement digital watermarking strategies within the models
    themselves so that the owner’s private information remains secure. However, there
    will inevitably be technology developed to remove watermarks, even from machine
    learning models. For example, REFIT is a recent unified watermark removal framework
    based on fine-tuning. It does not require knowledge of the watermarks being removed,
    and is effective against a wide range of current watermarking techniques [[90](#bib.bib90)].
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型在各种行业中的普及，将数字水印策略实施到模型内部变得尤为重要，以确保所有者的私人信息保持安全。然而，不可避免地，未来会有技术被开发出来以去除水印，即便是从机器学习模型中去除。例如，REFIT
    是一个基于微调的最新统一水印去除框架。它不需要了解被去除的水印，并且对当前多种水印技术都有效 [[90](#bib.bib90)]。
- en: VIII-E Watermarking for Launching Backdoor Attacks
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-E 发起后门攻击的水印技术
- en: Deep neural networks have been proven vulnerable to backdoor attacks, where
    triggers can be embedded into DNNs through data hiding and they can trick the
    model into producing unexpected behaviour with crafted triggers. For instance,
    watermarks can be embedded into the training examples of machine learning models
    in order to cause inaccurate classifications when the model is deployed. Many
    third-party cloud computing providers, such as Google, Microsoft, Azure, and Amazon,
    provide Machine Learning as a Service (MLaaS) to train machine learning models.
    A malicious party can embed watermarks into training data images and train the
    model to misclassify such examples, either randomly (random target attack), or
    mislabel as a different example (single target attack). As more third-party providers
    offer MLaaS, and machine learning models become more ubiquitous, backdoor attacks
    on neural networks are certain to become more common. Therefore, it is important
    to be able to detect triggers embedded in training examples. As digital watermarking
    technologies become more advanced through deep learning as discussed in this survey,
    this will become more difficult. In this case, the improving watermark removal
    techniques as discussed could be pursued for a benign rather than malicious purpose.
    Although existing works discuss visible watermarks embedded in training images
    [[91](#bib.bib91), [92](#bib.bib92)], there are growing efforts to construct invisible
    backdoor attacks that cannot be detected by a human moderator, or by automated
    backdoor detection techniques [[93](#bib.bib93), [94](#bib.bib94)]. Although the
    techniques presented in this survey are promising for digital IP protection purposes,
    they will inevitably be utilised for malicious purposes such as embedding undetectable,
    invisible backdoors in machine learning models.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络已经被证明易受后门攻击，其中触发器可以通过数据隐藏嵌入到深度神经网络中，并且可以通过精心设计的触发器欺骗模型产生意外行为。例如，可以将水印嵌入到机器学习模型的训练样本中，以便在模型部署时导致分类不准确。许多第三方云计算服务提供商，如谷歌、微软、Azure
    和亚马逊，提供机器学习即服务（MLaaS）来训练机器学习模型。恶意方可以将水印嵌入到训练数据图像中，并训练模型使其错误分类这些样本，可能是随机的（随机目标攻击），或者错误标记为不同的样本（单一目标攻击）。随着越来越多的第三方提供商提供
    MLaaS，并且机器学习模型变得越来越普及，对神经网络的后门攻击必然会变得更加常见。因此，能够检测训练样本中嵌入的触发器非常重要。随着数字水印技术通过深度学习变得越来越先进，如本次调查所述，这将变得更加困难。在这种情况下，可以追求改进的水印去除技术，以便用于善意而非恶意的目的。尽管现有的研究讨论了嵌入在训练图像中的可见水印
    [[91](#bib.bib91), [92](#bib.bib92)]，但也有越来越多的努力在构建无法被人工审核者或自动化后门检测技术检测到的不可见后门攻击
    [[93](#bib.bib93), [94](#bib.bib94)]。虽然本次调查中提出的技术在数字知识产权保护方面前景广阔，但它们不可避免地也会被用于恶意目的，例如在机器学习模型中嵌入不可检测的隐形后门。
- en: VIII-F Deepfake Detection and Identification
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-F 深度伪造检测与识别
- en: Synthetic media technologies are rapidly advancing, and it is more painless
    to generate media such as images, audios, and videos that look and sound increasingly
    realistic. Since deepfakes often present a person saying or doing something they
    have not done or said, people need to be able to identify the original source
    of the media that has been manipulated. Similarly, it is important to be able
    to identify a piece of media as synthetic in the first place, without malicious
    parties removing this identifying tag and presenting the synthetic media as genuine.
    To this end, the identification and detection of synthetic media become a promising
    application for digital watermarking.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 合成媒体技术正在迅速发展，更容易生成看起来和听起来越来越真实的媒体，如图像、音频和视频。由于深度伪造经常呈现出一个人说或做一些他们没有做过或说过的事情，人们需要能够识别已经被编辑的媒体的原始来源。同样，重要的是能够首先将一段媒体识别为合成媒体，而不是让恶意方将识别标记移除，并呈现合成媒体为真实的。为此，合成媒体的识别和检测成为数字水印的一个有前景的应用。
- en: A recent paper presents DeepTag, an end-to-end deep watermarking framework that
    includes a GAN simulator that applies common distortions to facial images [[95](#bib.bib95)].
    The watermark can be recovered to identify the original unaltered facial image.
    In future, as regulations surrounding deepfakes arise, watermarking techniques
    that are robust to GAN-based distortions will become increasingly important. A
    connected application is embedding watermarks into synthetic media so that they
    can be easily identified as such.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇最近的论文提出了DeepTag，一个端到端的深度水印框架，其中包括一个GAN模拟器，用于给面部图像应用常见的扭曲[[95](#bib.bib95)]。水印可以被还原以识别原始未经改变的面部图像。未来，随着围绕深度伪造的法规的出现，对抗GAN扭曲的水印技术将变得越来越重要。一个相关的应用是将水印嵌入到合成媒体中，以便可以轻松地识别其为合成媒体。
- en: VIII-G Malware using Steganography
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-G 利用隐写术的恶意软件
- en: Digital steganography can be used maliciously to spread malware to victim technology.
    The ability to hide an executable file within an image or audio file gives attackers
    an easy attack vector to target unaware users. There have also been known cases
    of attackers using steganography to pass data through unsuspecting platforms.
    They can easily set a time for the receiver and either upload or update an image
    temporarily. At this set time the receiver can download and save the photo, decode
    the message, and then the attacker can restore the image to the original or delete
    the image. This could be almost impossible to detect when third parties are unaware
    of where the attack will take place, and would be even more unlikely to catch
    the act if the stego-image used was highly imperceptible. These high levels of
    imperceptibility can now be achieved easily through modern deep learning approaches.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 数字隐写术可以被恶意使用来将恶意软件传播到受害技术上。在图像或音频文件中隐藏可执行文件的能力为攻击者提供了一个轻松的攻击矢量，以针对毫不知情的用户。也已知攻击者使用隐写术通过毫不知情的平台传输数据的情况。他们可以轻松地为接收者设置一个时间，并上传或临时更新一张图像。在设定的时间，接收者可以下载并保存照片，解码消息，然后攻击者可以将图像恢复为原始状态或删除图像。当第三方不知道攻击将在何处发生时，几乎不可能检测到这一点，如果使用的隐写图像高度不可察觉，更不太可能捕捉到行为。现代深度学习方法现在很容易实现这种高水平的不可察觉性。
- en: Steganalysis could be implemented into antivirus software that not only scans
    images but scans sites before entering. Though this would be a difficult task
    due to the large amount of power it would require, constantly scanning almost
    every website or item on the web page just in case there is malware present. There
    could be a filter that decides when steganalysis could be used such as situations
    when users decide to continue onto an already scanned suspicious website. Currently,
    the possibilities are limited, but the implementation of steganalysis in antivirus
    software may be essential in the future as steganography techniques both improve
    in performance and become more widely accessible to the public.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 隐写分析可以被实现到杀毒软件中，不仅扫描图像，还在进入网站之前扫描网站。尽管这将是一项艰巨的任务，因为它需要大量的电力，不断扫描几乎每个网站或网页上的物品，以防存在恶意软件。可能会有一个过滤器，决定何时可以使用隐写分析，比如用户决定继续访问已扫描的可疑网站的情况。目前的可能性是有限的，但随着隐写术技术的性能提高和对公众变得更加普遍可获取，隐写分析在杀毒软件中的实施可能在未来是必不可少的。
- en: IX Conclusion
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 结论
- en: This survey has provided an extensive overview of current deep learning techniques
    for data hiding, encompassing watermarking and steganography methods. Through
    analysis of network architecture and model performance, the survey has demonstrated
    how digital watermarking and steganography share a common goal of embedding information
    in digital media, and how both can benefit from deep learning techniques. Additionally,
    the survey explored future research directions and highlighted the potential for
    this field to revolutionize the protection of digital IP and communication security
    in Responsible AI software industries. As deep learning techniques continue to
    advance, they are expected to surpass traditional algorithms in all types of media,
    ultimately enhancing the accountability and safety of AI. This promising field
    holds great potential and is expected to have a significant impact on digital
    security.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查提供了当前数据隐藏深度学习技术的广泛概述，包括水印和隐写方法。通过分析网络架构和模型性能，本调查展示了数字水印和隐写技术如何共享嵌入信息于数字媒体的共同目标，以及这两者如何从深度学习技术中受益。此外，本调查探讨了未来研究方向，并强调了这一领域在负责任的AI软件产业中革新数字知识产权保护和通信安全的潜力。随着深度学习技术的不断进步，它们预计将在所有类型的媒体中超越传统算法，*最终*提升AI的问责性和安全性。这个充满希望的领域具有巨大潜力，预计将在数字安全方面产生重大影响。
- en: Acknowledgments
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank Wendy La for the discussion and feedback on the paper.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢 Wendy La 对论文的讨论和反馈。
- en: References
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] C. Kumar, A. K. Singh, and P. Kumar, “A recent survey on image watermarking
    techniques and its application in e-governance,” *Multimedia Tools and Applications*,
    vol. 77, pp. 3597–3622, 2018.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] C. Kumar, A. K. Singh, 和 P. Kumar，“图像水印技术及其在电子政务中的应用的最新调查，” *多媒体工具与应用*，第77卷，页码
    3597–3622, 2018年。'
- en: '[2] A. Rasmi, B. Arunkumar, and V. M. Anees, “A comprehensive review of digital
    data hiding techniques,” *Pattern Recognition and Image Analysis*, vol. 29, pp.
    639–646, 2019.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Rasmi, B. Arunkumar, 和 V. M. Anees，“数字数据隐藏技术的全面回顾，” *模式识别与图像分析*，第29卷，页码
    639–646, 2019年。'
- en: '[3] A. Herrigel, S. Voloshynovskiy, and Z. Hrytskiv, “An optical/digital identification/verification
    system based on digital watermarking technology,” *Proceedings of SPIE - The International
    Society for Optical Engineering*, 03 2000.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Herrigel, S. Voloshynovskiy, 和 Z. Hrytskiv，“基于数字水印技术的光学/数字识别/验证系统，”
    *SPIE - 国际光学工程学会会议录*，2000年03月。'
- en: '[4] S. Bhattacharya, “Survey on digital watermarking- a digital forensics &
    security application,” *International Journal of Advanced Research in Computer
    Science and Software Engineering ISSN number:2277-128X*, vol. 4, p. 11, 11 2014.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Bhattacharya，“数字水印调查——数字取证与安全应用，” *计算机科学与软件工程高级研究国际期刊 ISSN 编号：2277-128X*，第4卷，第11页,
    2014年11月。'
- en: '[5] R. Lu, G. Zhang, L. Kou, L. Zhang, C. Liu, Q. Da, and J. Sun, “A new digital
    watermarking method for data integrity protection in the perception layer of iot,”
    *Security and Communication Networks*, vol. 2017, p. 12, 10 2017.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] R. Lu, G. Zhang, L. Kou, L. Zhang, C. Liu, Q. Da, 和 J. Sun，“一种用于物联网感知层数据完整性保护的新数字水印方法，”
    *安全与通信网络*，第2017卷，第12页, 2017年10月。'
- en: '[6] A. Ferdowsi and W. Saad, “Deep learning-based dynamic watermarking for
    secure signal authentication in the internet of things,” in *2018 IEEE International
    Conference on Communications (ICC)*.   IEEE, 2018, pp. 1–6.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Ferdowsi 和 W. Saad，“基于深度学习的动态水印技术用于物联网中的安全信号认证，”收录于 *2018 IEEE 国际通信大会（ICC）*。
    IEEE, 2018, 页码 1–6。'
- en: '[7] G. Hua, J. Huang, Y. Q. Shi, J. Goh, and V. L. Thing, “Twenty years of
    digital audio watermarking—a comprehensive review,” *Signal processing*, vol.
    128, pp. 222–242, 2016.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] G. Hua, J. Huang, Y. Q. Shi, J. Goh, 和 V. L. Thing，“二十年数字音频水印——全面回顾，” *信号处理*，第128卷，页码
    222–242, 2016年。'
- en: '[8] A. Jadhav and M. Kolhekar, “Digital watermarking in video for copyright
    protection,” in *Proceedings - International Conference on Electronic Systems,
    Signal Processing, and Computing Technologies, ICESC 2014*.   Nagpur, India: IEEE,
    01 2014, pp. 140–144.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Jadhav 和 M. Kolhekar，“视频中的数字水印用于版权保护，”收录于 *国际电子系统、信号处理和计算技术会议论文集，ICESC
    2014*。 印度那格浦尔：IEEE, 2014年01月，页码 140–144。'
- en: '[9] S. Abdelnabi and M. Fritz, “Adversarial watermarking transformer: Towards
    tracing text provenance with data hiding,” in *2021 IEEE Symposium on Security
    and Privacy (SP)*.   IEEE, 2021, pp. 121–140.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Abdelnabi 和 M. Fritz，“对抗性水印变换器：利用数据隐藏追踪文本来源，”收录于 *2021 IEEE 安全与隐私研讨会（SP）*。
    IEEE, 2021年，页码 121–140。'
- en: '[10] S. P. Mohanty, A. Sengupta, P. Guturu, and E. Kougianos, “Everything you
    want to know about watermarking: From paper marks to hardware protection: From
    paper marks to hardware protection.” *IEEE Consumer Electronics Magazine*, vol. 6,
    pp. 83–91, 2017.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. P. Mohanty, A. Sengupta, P. Guturu, 和 E. Kougianos, “你需要了解的关于水印的一切：从纸质标记到硬件保护。”
    *IEEE消费电子杂志*，第6卷，页83–91，2017年。'
- en: '[11] I. J. Kadhim, P. Premaratne, P. J. Vial, and B. Halloran, “Comprehensive
    survey of image steganography: Techniques, evaluations, and trends in future research,”
    *Neurocomputing*, vol. 335, pp. 299–326, 2019.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] I. J. Kadhim, P. Premaratne, P. J. Vial, 和 B. Halloran, “图像隐写术的全面调查：技术、评估及未来研究趋势，”
    *神经计算*，第335卷，页299–326，2019年。'
- en: '[12] H. Kandi, D. Mishra, and S. R. S. Gorthi, “Exploring the learning capabilities
    of convolutional neural networks for robust image watermarking,” *Computers &
    Security*, vol. 65, pp. 247–268, 2017.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] H. Kandi, D. Mishra, 和 S. R. S. Gorthi, “探索卷积神经网络在强健图像水印中的学习能力，” *计算机与安全*，第65卷，页247–268，2017年。'
- en: '[13] L. Geng, W. Zhang, H. Chen, H. Fang, and N. Yu, “Real-time attacks on
    robust watermarking tools in the wild by cnn,” *Journal of Real-Time Image Processing*,
    vol. 17, 6 2020.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] L. Geng, W. Zhang, H. Chen, H. Fang, 和 N. Yu, “对强健水印工具的实时攻击：基于CNN，” *实时图像处理期刊*，第17卷，2020年6月。'
- en: '[14] Luo, Xiyang and Li, Yinxiao and Chang, Huiwen and Liu, Ce and Milanfar,
    Peyman and Yang, Feng, “Dvmark: a deep multiscale framework for video watermarking,”
    *arXiv preprint arXiv:2104.12734*, 2021.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Luo, Xiyang 和 Li, Yinxiao 和 Chang, Huiwen 和 Liu, Ce 和 Milanfar, Peyman
    和 Yang, Feng, “Dvmark：一种用于视频水印的深度多尺度框架，” *arXiv预印本 arXiv:2104.12734*，2021年。'
- en: '[15] I. Yoo, H. Chang, X. Luo, O. Stava, C. Liu, P. Milanfar, and F. Yang,
    “Deep 3d-to-2d watermarking: Embedding messages in 3d meshes and extracting them
    from 2d renderings,” *ArXiv*, vol. abs/2104.13450, 2021.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] I. Yoo, H. Chang, X. Luo, O. Stava, C. Liu, P. Milanfar, 和 F. Yang, “深度3D到2D水印：在3D网格中嵌入消息并从2D渲染中提取消息，”
    *ArXiv*，第abs/2104.13450卷，2021年。'
- en: '[16] J. Zhu, R. Kaplan, J. Johnson, and L. Fei-Fei, “Hidden: Hiding data with
    deep networks,” in *Proceedings of the European conference on computer vision
    (ECCV)*, 2018, pp. 657–672.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Zhu, R. Kaplan, J. Johnson, 和 L. Fei-Fei, “隐藏：利用深度网络隐藏数据，” 见于 *欧洲计算机视觉会议（ECCV）论文集*，2018年，页657–672。'
- en: '[17] S. Baluja, “Hiding images in plain sight: Deep steganography,” in *Proceedings
    of the 31st International Conference on Neural Information Processing Systems*,
    2017, pp. 2066–2076.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Baluja, “隐形图像：深度隐写术，” 见于 *第31届国际神经信息处理系统会议论文集*，2017年，页2066–2076。'
- en: '[18] S. Kartik, A. Aggarwal, T. Singhania, D. Gupta, and A. Khanna, “Hiding
    data in images using cryptography and deep neural network,” *Journal of Artificial
    Intelligence and Systems*, pp. 143–162, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Kartik, A. Aggarwal, T. Singhania, D. Gupta, 和 A. Khanna, “利用密码学和深度神经网络在图像中隐藏数据，”
    *人工智能与系统期刊*，页143–162，2019年。'
- en: '[19] S. T. Hussain I, Zeng J, “Survey on deep convolutional neural networks
    for image steganography and steganalysis,” *KSII Transactions on Internet and
    Information Systems*, vol. 14, pp. 1228–1248, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. T. Hussain I, Zeng J, “深度卷积神经网络在图像隐写术和隐写分析中的调查，” *KSII互联网与信息系统交易*，第14卷，页1228–1248，2020年。'
- en: '[20] C. Zhang, C. Lin, P. Benz, K. Chen, W. Zhang, and I. S. Kweon, “A brief
    survey on deep learning based data hiding, steganography and watermarking,” 2021.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] C. Zhang, C. Lin, P. Benz, K. Chen, W. Zhang, 和 I. S. Kweon, “基于深度学习的数据隐藏、隐写术和水印技术简要调查，”
    2021年。'
- en: '[21] X. Luo, R. Zhan, H. Chang, F. Yang, and P. Milanfar, “Distortion agnostic
    deep watermarking,” in *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*, 2020, pp. 13 548–13 557.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] X. Luo, R. Zhan, H. Chang, F. Yang, 和 P. Milanfar, “失真无关的深度水印技术，” 见于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页13 548–13 557。'
- en: '[22] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” *Advances
    in Neural Information Processing Systems*, vol. 3, no. 11, 2014.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, 和 Y. Bengio, “生成对抗网络，” *神经信息处理系统进展*，第3卷，第11期，2014年。'
- en: '[23] X. Zhong, P.-C. Huang, S. Mastorakis, and F. Y. Shih, “An automated and
    robust image watermarking scheme based on deep neural networks,” *IEEE Transactions
    on Multimedia*, vol. 23, pp. 1951–1961, 2020.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] X. Zhong, P.-C. Huang, S. Mastorakis, 和 F. Y. Shih, “基于深度神经网络的自动化和强健的图像水印方案，”
    *IEEE多媒体学报*，第23卷，页1951–1961，2020年。'
- en: '[24] S.-M. Mun, S.-H. Nam, H. Jang, D. Kim, and H.-K. Lee, “Finding robust
    domain from attacks: A learning framework for blind watermarking,” *Neurocomputing*,
    vol. 337, pp. 191–202, 2019.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S.-M. Mun, S.-H. Nam, H. Jang, D. Kim 和 H.-K. Lee，"从攻击中寻找鲁棒域：盲水印的学习框架"，*神经计算*，第
    337 卷，页码 191–202，2019年。'
- en: '[25] M. Ahmadi, A. Norouzi, S. M. R. Soroushmehr, N. Karimi, K. Najarian, S. Samavi,
    and A. Emami, “Redmark: Framework for residual diffusion watermarking on deep
    networks,” *Expert Systems with Applications*, vol. 146, p. 113157, 2020.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. Ahmadi, A. Norouzi, S. M. R. Soroushmehr, N. Karimi, K. Najarian, S.
    Samavi 和 A. Emami，"Redmark: 深度网络上的残差扩散水印框架"，*专家系统与应用*，第 146 卷，页码 113157，2020年。'
- en: '[26] J.-E. Lee, Y.-H. Seo, and D.-W. Kim, “Convolutional neural network-based
    digital image watermarking adaptive to the resolution of image and watermark,”
    *Applied Sciences*, vol. 10, no. 19, p. 6854, 2020.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J.-E. Lee, Y.-H. Seo 和 D.-W. Kim，"基于卷积神经网络的数字图像水印，自适应于图像和水印的分辨率"，*应用科学*，第
    10 卷，第 19 期，页码 6854，2020年。'
- en: '[27] Y. Liu, M. Guo, J. Zhang, Y. Zhu, and X. Xie, “A novel two-stage separable
    deep learning framework for practical blind watermarking,” in *Proceedings of
    the 27th ACM International conference on multimedia*, 2019, pp. 1509–1517.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Liu, M. Guo, J. Zhang, Y. Zhu 和 X. Xie，"一种新颖的两阶段可分离深度学习框架用于实际盲水印"，*第27届
    ACM 国际多媒体会议论文集*，2019年，页码 1509–1517。'
- en: '[28] B. Wen and S. Aydore, “Romark: A robust watermarking system using adversarial
    training,” *arXiv preprint arXiv:1910.01221*, 2019.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] B. Wen 和 S. Aydore，"Romark: 使用对抗训练的鲁棒水印系统"，*arXiv 预印本 arXiv:1910.01221*，2019年。'
- en: '[29] C. Yu, “Attention based data hiding with generative adversarial networks,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 34, no. 01,
    2020, pp. 1120–1128.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] C. Yu，"基于注意力的数据隐藏与生成对抗网络"，*AAAI 人工智能会议论文集*，第 34 卷，第 01 期，2020年，页码 1120–1128。'
- en: '[30] H. Zhang, H. Wang, Y. Cao, C. Shen, and Y. Li, “Robust data hiding using
    inverse gradient attention,” *arXiv preprint arXiv:2011.10850*, 2020.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] H. Zhang, H. Wang, Y. Cao, C. Shen 和 Y. Li，"使用逆梯度注意力的鲁棒数据隐藏"，*arXiv 预印本
    arXiv:2011.10850*，2020年。'
- en: '[31] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
    adversarial examples,” *arXiv preprint arXiv:1412.6572*, 2014.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] I. J. Goodfellow, J. Shlens 和 C. Szegedy，"解释和利用对抗样本"，*arXiv 预印本 arXiv:1412.6572*，2014年。'
- en: '[32] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples in
    the physical world,” in *Artificial intelligence safety and security*.   Chapman
    and Hall/CRC, 2018, pp. 99–112.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Kurakin, I. J. Goodfellow 和 S. Bengio，"物理世界中的对抗样本"，*人工智能安全与保障*。Chapman
    和 Hall/CRC，2018年，页码 99–112。'
- en: '[33] G. Gul and F. Kurugollu, “A new methodology in steganalysis: Breaking
    highly undetectable steganograpy (hugo),” *International Workshop on Information
    Hiding*, vol. 6958, pp. 71–84, 2011.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] G. Gul 和 F. Kurugollu，"隐写分析中的新方法：打破高度不可检测的隐写术 (hugo)"，*国际信息隐藏研讨会*，第 6958
    卷，页码 71–84，2011年。'
- en: '[34] V. Holub and J. Fridrich, “Designing steganographic distortion using directional
    filters,” *IEEE International Workshop on Information Forensics and Security*,
    pp. 234–239, Dec. 2012.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] V. Holub 和 J. Fridrich，"使用方向滤波器设计隐写失真"，*IEEE 国际信息取证与安全研讨会*，页码 234–239，2012年12月。'
- en: '[35] V. Holub, J. Fridrich, and T. Denemark, “Universal distortion function
    for steganography in an arbitrary domain,” *EURASIP Journal on Information Security*,
    2014.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] V. Holub, J. Fridrich 和 T. Denemark，"用于任意领域隐写术的通用失真函数"，*EURASIP 信息安全期刊*，2014年。'
- en: '[36] I. HAMAMOTO and M. KAWAMURA, “Neural watermarking method including an
    attack simulator against rotation and compression attacks,” *IEICE Transactions
    on Information and Systems*, vol. E103.D, no. 1, pp. 33–41, 2020.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] I. HAMAMOTO 和 M. KAWAMURA，"包括针对旋转和压缩攻击的攻击模拟器的神经水印方法"，*IEICE 信息与系统事务*，第
    E103.D 卷，第 1 期，页码 33–41，2020年。'
- en: '[37] D. Lowe, “Distinctive image features from scale-invariant keypoints,”
    *International Journal of Computer Vision*, vol. 60, pp. 91–110, 11 2004.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] D. Lowe，"来自尺度不变关键点的独特图像特征"，*国际计算机视觉期刊*，第 60 卷，页码 91–110，2004年11月。'
- en: '[38] K. A. Zhang, A. Cuesta-Infante, L. Xu, and K. Veeramachaneni, “Steganogan:
    High capacity image steganography with gans,” *arXiv preprint arXiv:1901.03892*,
    2019.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] K. A. Zhang, A. Cuesta-Infante, L. Xu 和 K. Veeramachaneni，"Steganogan:
    使用GAN的高容量图像隐写术"，*arXiv 预印本 arXiv:1901.03892*，2019年。'
- en: '[39] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adversarial
    networks,” in *International conference on machine learning*.   PMLR, 2017, pp.
    214–223.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. Arjovsky, S. Chintala 和 L. Bottou，"Wasserstein 生成对抗网络"，*国际机器学习大会*。PMLR，2017年，页码
    214–223。'
- en: '[40] M. Plata and P. Syga, “Robust spatial-spread deep neural image watermarking,”
    in *2020 IEEE 19th International Conference on Trust, Security and Privacy in
    Computing and Communications (TrustCom)*, 2020, pp. 62–70.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. Plata 和 P. Syga, “稳健的空间扩展深度神经图像水印技术，” 收录于 *2020 IEEE第19届计算与通信中的信任、安全与隐私国际会议
    (TrustCom)*，2020年，第62–70页。'
- en: '[41] Plata, Marcin and Syga, Piotr, “Robust watermarking with double detector-discriminator
    approach,” *arXiv preprint arXiv:2006.03921*, 2020.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Plata, Marcin 和 Syga, Piotr, “双重检测器-判别器方法的稳健水印技术，” *arXiv 预印本 arXiv:2006.03921*，2020年。'
- en: '[42] K. Wang, L. Li, T. Luo, and C.-C. Chang, “Deep neural network watermarking
    based on texture analysis,” in *Artificial Intelligence and Security*, X. Sun,
    J. Wang, and E. Bertino, Eds.   Singapore: Springer Singapore, 2020, pp. 558–569.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] K. Wang, L. Li, T. Luo, 和 C.-C. Chang, “基于纹理分析的深度神经网络水印技术，” 收录于 *人工智能与安全*，X.
    Sun, J. Wang, 和 E. Bertino 编。 新加坡: Springer Singapore，2020年，第558–569页。'
- en: '[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*.   Las Vegas, NV, USA: IEEE, 2016, pp. 770–778.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] K. He, X. Zhang, S. Ren, 和 J. Sun, “用于图像识别的深度残差学习，” 收录于 *2016 IEEE计算机视觉与模式识别会议
    (CVPR)*。 拉斯维加斯，NV，美国: IEEE，2016年，第770–778页。'
- en: '[44] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2017, pp. 4700–4708.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] G. Huang, Z. Liu, L. Van Der Maaten, 和 K. Q. Weinberger, “密集连接的卷积网络，”
    收录于 *IEEE计算机视觉与模式识别会议论文集*，2017年，第4700–4708页。'
- en: '[45] M. Tancik, B. Mildenhall, and R. Ng, “Stegastamp: Invisible hyperlinks
    in physical photographs,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 2117–2126.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M. Tancik, B. Mildenhall, 和 R. Ng, “Stegastamp: 物理照片中的隐形超链接，” 收录于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第2117–2126页。'
- en: '[46] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
    with conditional adversarial networks,” 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] P. Isola, J.-Y. Zhu, T. Zhou, 和 A. A. Efros, “使用条件对抗网络的图像到图像翻译，” 2018年。'
- en: '[47] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18*.   Springer, 2015, pp. 234–241.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] O. Ronneberger, P. Fischer, 和 T. Brox, “U-net: 用于生物医学图像分割的卷积网络，” 收录于 *医学图像计算与计算机辅助干预–MICCAI
    2015: 第18届国际会议，德国慕尼黑，2015年10月5-9日，论文集，第三部分 18*。 Springer，2015年，第234–241页。'
- en: '[48] P. Wu, Y. Yang, and X. Li, “Stegnet: Mega image steganography capacity
    with deep convolutional network,” *Future Internet*, vol. 10, no. 6, p. 54, 2018.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] P. Wu, Y. Yang, 和 X. Li, “Stegnet: 基于深度卷积网络的超大图像隐写容量，” *未来互联网*，第10卷，第6期，第54页，2018年。'
- en: '[49] R. Meng, S. G. Rice, J. Wang, and X. Sun, “A fusion steganographic algorithm
    based on faster r-cnn,” *Computers, Materials & Continua*, vol. 55, no. 1, pp.
    1–16, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] R. Meng, S. G. Rice, J. Wang, 和 X. Sun, “基于faster r-cnn的融合隐写算法，” *计算机、材料与连续介质*，第55卷，第1期，第1–16页，2018年。'
- en: '[50] K. Sharma, A. Aggarwal, T. Singhania, D. Gupta, and A. Khanna, “Hiding
    data in images using cryptography and deep neural network,” *arXiv preprint arXiv:1912.10413*,
    2019.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] K. Sharma, A. Aggarwal, T. Singhania, D. Gupta, 和 A. Khanna, “利用密码学和深度神经网络在图像中隐藏数据，”
    *arXiv 预印本 arXiv:1912.10413*，2019年。'
- en: '[51] X. Duan, K. Jia, B. Li, D. Guo, E. Zhang, and C. Qin, “Reversible image
    steganography scheme based on a u-net structure,” *IEEE Access*, vol. 7, pp. 9314–9323,
    2019.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] X. Duan, K. Jia, B. Li, D. Guo, E. Zhang, 和 C. Qin, “基于u-net结构的可逆图像隐写方案，”
    *IEEE Access*，第7卷，第9314–9323页，2019年。'
- en: '[52] C. Zhang, P. Benz, A. Karjauv, G. Sun, and I. S. Kweon, “Udh: Universal
    deep hiding for steganography, watermarking, and light field messaging,” *Advances
    in Neural Information Processing Systems*, vol. 33, pp. 10 223–10 234, 2020.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] C. Zhang, P. Benz, A. Karjauv, G. Sun, 和 I. S. Kweon, “UDH: 用于隐写、水印和光场消息的通用深度隐藏技术，”
    *神经信息处理系统进展*，第33卷，第10 223–10 234页，2020年。'
- en: '[53] C.-C. Chang, “Neural reversible steganography with long short-term memory,”
    *Security and Communication Networks*, vol. 2021, 2021.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] C.-C. Chang, “具有长短期记忆的神经可逆隐写术，” *安全与通信网络*，第2021卷，2021年。'
- en: '[54] B. Boehm, “Stegexpose-a tool for detecting lsb steganography,” *arXiv
    preprint arXiv:1410.6656*, 2014.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] B. Boehm, “Stegexpose: 一种检测LSB隐写的工具，” *arXiv 预印本 arXiv:1410.6656*，2014年。'
- en: '[55] W. Tang, B. Li, S. Tan, M. Barni, and J. Huang, “Cnn-based adversarial
    embedding for image steganography,” *IEEE Transactions on Information Forensics
    and Security*, vol. 14, no. 8, pp. 2074–2087, 2019.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] W. 唐, B. 李, S. 谭, M. 巴尔尼, 和 J. 黄, “基于 CNN 的对抗嵌入用于图像隐写术，” *IEEE 信息取证与安全交易*,
    第 14 卷，第 8 期，页码 2074–2087，2019 年。'
- en: '[56] B. Chen, J. Wang, Y. Chen, Z. Jin, H. J. Shim, and Y.-Q. Shi, “High-capacity
    robust image steganography via adversarial network.” *KSII Transactions on Internet
    & Information Systems*, vol. 14, no. 1, 2020.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] B. 陈, J. 王, Y. 陈, Z. 金, H. J. 沈, 和 Y.-Q. 施, “通过对抗网络实现高容量鲁棒图像隐写术。” *KSII
    网络与信息系统交易*, 第 14 卷，第 1 期，2020 年。'
- en: '[57] R. Zhang, S. Dong, and J. Liu, “Invisible steganography via generative
    adversarial networks,” *Multimedia tools and applications*, vol. 78, no. 7, pp.
    8559–8575, 2019.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] R. 张, S. 董, 和 J. 刘, “通过生成对抗网络的隐形隐写术，” *多媒体工具与应用*, 第 78 卷，第 7 期，页码 8559–8575，2019
    年。'
- en: '[58] C. Li, Y. Jiang, and M. Cheslyar, “Embedding image through generated intermediate
    medium using deep convolutional generative adversarial network,” *Computers, Materials
    & Continua*, vol. 56, no. 2, pp. 313–324, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] C. 李, Y. 蒋, 和 M. 切斯利亚尔, “通过生成的中间介质嵌入图像，使用深度卷积生成对抗网络，” *计算机、材料与连续介质*, 第
    56 卷，第 2 期，页码 313–324，2018 年。'
- en: '[59] J. Qin, J. Wang, Y. Tan, H. Huang, X. Xiang, and Z. He, “Coverless image
    steganography based on generative adversarial network,” *Mathematics*, vol. 8,
    no. 9, p. 1394, 2020.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] J. 秦, J. 王, Y. 谭, H. 黄, X. 项, 和 Z. 贺, “基于生成对抗网络的无覆盖图像隐写术，” *数学*, 第 8 卷，第
    9 期，文章 1394，2020 年。'
- en: '[60] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” 2020.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J.-Y. 朱, T. 朴, P. 伊索拉, 和 A. A. 艾弗罗斯, “使用循环一致对抗网络进行非配对图像到图像转换，” 2020 年。'
- en: '[61] R. Rahim, S. Nadeem, *et al.*, “End-to-end trained cnn encoder-decoder
    networks for image steganography,” in *Proceedings of the European Conference
    on Computer Vision (ECCV) Workshops*, 2018, pp. 0–0.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] R. 拉希姆, S. 纳迪姆, *等*, “用于图像隐写术的端到端训练 CNN 编码器-解码器网络，” 见 *欧洲计算机视觉会议（ECCV）研讨会论文集*，2018
    年，页码 0–0。'
- en: '[62] R. Meng, Q. Cui, Z. Zhou, Z. Fu, and X. Sun, “A steganography algorithm
    based on cyclegan for covert communication in the internet of things,” *IEEE Access*,
    vol. 7, pp. 90 574–90 584, 2019.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] R. 孟, Q. 崔, Z. 周, Z. 傅, 和 X. 孙, “基于 CycleGAN 的隐写术算法用于物联网中的隐秘通信，” *IEEE
    Access*, 第 7 卷，页码 90 574–90 584，2019 年。'
- en: '[63] C.-C. Chang, “Adversarial learning for invertible steganography,” *IEEE
    Access*, vol. 8, pp. 198 425–198 435, 2020.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] C.-C. 张, “用于可逆隐写术的对抗学习，” *IEEE Access*, 第 8 卷，页码 198 425–198 435，2020
    年。'
- en: '[64] D. Volkhonskiy, I. Nazarov, and E. Burnaev, “Steganographic generative
    adversarial networks,” in *Twelfth International Conference on Machine Vision
    (ICMV 2019)*, vol. 11433.   International Society for Optics and Photonics, 2020,
    p. 114333M.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] D. 沃尔洪斯基, I. 纳扎罗夫, 和 E. 布尔纳耶夫, “隐写生成对抗网络，” 见 *第十二届国际机器视觉会议（ICMV 2019）*，第
    11433 卷。国际光学与光子学学会，2020 年，第 114333M 页。'
- en: '[65] A. Nissar and A. Mir, “Classification of steganalysis techniques: A study,”
    *Digital Signal Processing*, vol. 20, no. 6, pp. 1758–1770, 2010.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] A. 尼萨尔 和 A. 米尔, “隐写分析技术分类：研究，” *数字信号处理*, 第 20 卷，第 6 期，页码 1758–1770，2010
    年。'
- en: '[66] L. Pérez-Freire, P. Comesaña, J. R. Troncoso-Pastoriza, and F. Pérez-González,
    “Watermarking security: A survey.” *Lecture Notes in Computer Science*, vol. 4300,
    pp. 41–72, 01 2006.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] L. 佩雷斯-弗雷雷, P. 科梅萨尼亚, J. R. 特龙科索-帕斯托里萨, 和 F. 佩雷斯-冈萨雷斯, “水印安全性：综述。” *计算机科学讲义*,
    第 4300 卷，页码 41–72，2006 年 1 月。'
- en: '[67] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona,
    D. Ramanan, C. L. Zitnick, and P. Dollár, “Microsoft coco: Common objects in context,”
    2015.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] T.-Y. 林, M. 梅尔, S. 贝隆吉, L. 博尔德夫, R. 吉尔什克, J. 海斯, P. 佩罗纳, D. 拉马南, C. L.
    齐特尼克, 和 P. 多拉尔, “Microsoft COCO：上下文中的常见物体，” 2015 年。'
- en: '[68] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” 2018.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] R. 张, P. 伊索拉, A. A. 艾弗罗斯, E. 谢赫特曼, 和 O. 王, “深度特征作为感知度量的非凡效果，” 2018 年。'
- en: '[69] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, June
    2015.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] C. 塞戈迪, W. 刘, Y. 贾, P. 塞尔马内特, S. 里德, D. 安吉洛夫, D. 埃尔汉, V. 范霍克, 和 A. 拉宾诺维奇,
    “通过卷积深入探讨，” 见 *IEEE 计算机视觉与模式识别会议（CVPR）论文集*，2015 年 6 月。'
- en: '[70] E. Agustsson and R. Timofte, “Ntire 2017 challenge on single image super-resolution:
    Dataset and study,” in *2017 IEEE Conference on Computer Vision and Pattern Recognition
    Workshops (CVPRW)*, 2017, pp. 1122–1131.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] E. Agustsson 和 R. Timofte，“Ntire 2017 单幅图像超分辨率挑战赛：数据集与研究”，发表于 *2017 IEEE
    计算机视觉与模式识别会议工作坊（CVPRW）*，2017年，pp. 1122–1131。'
- en: '[71] A. Krizhevsky, “Learning multiple layers of features from tiny images,”
    *University of Toronto*, 05 2012.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] A. Krizhevsky，“从微小图像中学习多层特征”，*多伦多大学*，2012年5月。'
- en: '[72] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” *International Journal of
    Computer Vision*, vol. 88, no. 2, pp. 303–338, June 2010.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn 和 A. Zisserman，“PASCAL
    视觉目标分类（VOC）挑战赛”，*国际计算机视觉期刊*，第88卷，第2期，pp. 303–338，2010年6月。'
- en: '[73] P. Bas, T. Filler, and T. Pevný, “”break our steganographic system”: The
    ins and outs of organizing boss,” in *Information Hiding*, T. Filler, T. Pevný,
    S. Craver, and A. Ker, Eds.   Berlin, Heidelberg: Springer Berlin Heidelberg,
    2011, pp. 59–70.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] P. Bas, T. Filler 和 T. Pevný，““打破我们的隐写系统”：组织老板的来龙去脉”，发表于 *信息隐藏*，T. Filler,
    T. Pevný, S. Craver 和 A. Ker 编著。柏林，海德堡：施普林格·柏林·海德堡，2011年，pp. 59–70。'
- en: '[74] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “Imagenet large
    scale visual recognition challenge,” 2015.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg 和 L. Fei-Fei，“ImageNet 大规模视觉识别挑战赛”，2015年。'
- en: '[75] M. J. Huiskes and M. S. Lew, “The mir flickr retrieval evaluation,” in
    *Proceedings of the 1st ACM international conference on Multimedia information
    retrieval*, 2008, pp. 39–43.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. J. Huiskes 和 M. S. Lew，“MIR Flickr 检索评估”，发表于 *第1届 ACM 国际多媒体信息检索会议论文集*，2008年，pp.
    39–43。'
- en: '[76] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions
    to visual denotations: New similarity metrics for semantic inference over event
    descriptions,” *Transactions of the Association for Computational Linguistics*,
    vol. 2, pp. 67–78, 02 2014\. [Online]. Available: [https://doi.org/10.1162/tacl_a_00166](https://doi.org/10.1162/tacl_a_00166)'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] P. Young, A. Lai, M. Hodosh 和 J. Hockenmaier，“从图像描述到视觉指称：针对事件描述的语义推断的新相似度度量”，*计算语言学协会会刊*，第2卷，pp.
    67–78，2014年2月。[在线]. 可用：[https://doi.org/10.1162/tacl_a_00166](https://doi.org/10.1162/tacl_a_00166)'
- en: '[77] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, “Labeled faces
    in the wild: A database for studying face recognition in unconstrained environments,”
    University of Massachusetts, Amherst, Tech. Rep. 07-49, October 2007.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] G. B. Huang, M. Ramesh, T. Berg 和 E. Learned-Miller，“野外标记人脸：一个用于研究不受约束环境中人脸识别的数据库”，马萨诸塞大学安姆斯特，技术报告
    07-49，2007年10月。'
- en: '[78] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi, “Describing
    textures in the wild,” in *Proceedings of the IEEE Conf. on Computer Vision and
    Pattern Recognition (CVPR)*, 2014.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed 和 A. Vedaldi，“在野外描述纹理”，发表于
    *IEEE 计算机视觉与模式识别会议（CVPR）论文集*，2014年。'
- en: '[79] C.-M. Chou and D. Tseng, “Technologies for 3d model watermarking: A survey,”
    *International Journal of Engineering and Applied Sciences*, vol. 2, pp. 126–136,
    07 2008.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] C.-M. Chou 和 D. Tseng，“3D 模型水印技术：综述”，*国际工程与应用科学期刊*，第2卷，pp. 126–136，2008年7月。'
- en: '[80] M. Botta, D. Cavagnino, M. Gribaudo, and P. Piazzolla, “Fragile watermarking
    of 3d models in a transformed domain,” *Applied Sciences*, vol. 10, no. 9, 2020\.
    [Online]. Available: [https://www.mdpi.com/2076-3417/10/9/3244](https://www.mdpi.com/2076-3417/10/9/3244)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M. Botta, D. Cavagnino, M. Gribaudo 和 P. Piazzolla，“在变换域中对 3D 模型进行脆弱水印处理”，*应用科学*，第10卷，第9期，2020年。[在线].
    可用：[https://www.mdpi.com/2076-3417/10/9/3244](https://www.mdpi.com/2076-3417/10/9/3244)'
- en: '[81] C. Öngün and A. Temizel, “Paired 3d model generation with conditional
    generative adversarial networks,” 2019.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] C. Öngün 和 A. Temizel，“使用条件生成对抗网络生成配对的 3D 模型”，2019年。'
- en: '[82] S. Bendale, “A study of generative adversarial networks in 3d modelling,”
    *International Research Journal of Engineering and Technology*, vol. 6, pp. 826–830,
    05 2020.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] S. Bendale，“生成对抗网络在 3D 建模中的研究”，*国际工程与技术研究期刊*，第6卷，pp. 826–830，2020年5月。'
- en: '[83] J.-Y. Liu, Y.-H. Chen, Y.-C. Yeh, and Y.-H. Yang, “Unconditional audio
    generation with generative adversarial networks and cycle regularization,” *arXiv
    preprint arXiv:2005.08526*, 2020.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J.-Y. Liu, Y.-H. Chen, Y.-C. Yeh 和 Y.-H. Yang，“利用生成对抗网络和循环正则化进行无条件音频生成”，*arXiv
    预印本 arXiv:2005.08526*，2020年。'
- en: '[84] S. Ji, J. Luo, and X. Yang, “A comprehensive survey on deep music generation:
    Multi-level representations, algorithms, evaluations, and future directions,”
    *arXiv preprint arXiv:2011.06801*, 2020.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] S. Ji, J. Luo, 和 X. Yang, “深度音乐生成的全面调查：多层次表示、算法、评估和未来方向，” *arXiv预印本 arXiv:2011.06801*，2020年。'
- en: '[85] X. Luo, Y. Li, H. Chang, C. Liu, P. Milanfar, and F. Yang, “Robust invisible
    video watermarking with attention,” *arXiv preprint arXiv:2104.12734*, 2021.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] X. Luo, Y. Li, H. Chang, C. Liu, P. Milanfar, 和 F. Yang, “具有注意力的鲁棒隐形视频水印，”
    *arXiv预印本 arXiv:2104.12734*，2021年。'
- en: '[86] D. I. Adelani, H. Mai, F. Fang, H. H. Nguyen, J. Yamagishi, and I. Echizen,
    “Generating sentiment-preserving fake online reviews using neural language models
    and their human- and machine-based detection,” in *Advanced Information Networking
    and Applications: Proceedings of the 34th International Conference on Advanced
    Information Networking and Applications (AINA-2020)*.   Springer, 2020, pp. 1341–1354.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] D. I. Adelani, H. Mai, F. Fang, H. H. Nguyen, J. Yamagishi, 和 I. Echizen,
    “使用神经语言模型生成情感保持的虚假在线评论及其人工和机器检测，” 在 *高级信息网络与应用：第34届国际高级信息网络与应用会议（AINA-2020）论文集*。  Springer,
    2020年，pp. 1341–1354。'
- en: '[87] F. Boenisch, “A survey on model watermarking neural networks,” 2020.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] F. Boenisch, “关于模型水印神经网络的调查，” 2020年。'
- en: '[88] B. D. Rouhani, H. Chen, and F. Koushanfar, “Deepsigns: A generic watermarking
    framework for ip protection of deep learning models,” *arXiv preprint arXiv:1804.00750*,
    2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] B. D. Rouhani, H. Chen, 和 F. Koushanfar, “Deepsigns: 一个通用的深度学习模型知识产权保护水印框架，”
    *arXiv预印本 arXiv:1804.00750*，2018年。'
- en: '[89] H. Jia, C. A. Choquette-Choo, V. Chandrasekaran, and N. Papernot, “Entangled
    watermarks as a defense against model extraction.” in *USENIX Security Symposium*,
    2021, pp. 1937–1954.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] H. Jia, C. A. Choquette-Choo, V. Chandrasekaran, 和 N. Papernot, “纠缠水印作为防御模型提取的手段。”
    在 *USENIX安全研讨会*，2021年，pp. 1937–1954。'
- en: '[90] X. Chen, W. Wang, C. Bender, Y. Ding, R. Jia, B. Li, and D. Song, “REFIT:
    a unified watermark removal framework for deep learning systems with limited data,”
    *CoRR*, vol. abs/1911.07205, 2019\. [Online]. Available: [http://arxiv.org/abs/1911.07205](http://arxiv.org/abs/1911.07205)'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] X. Chen, W. Wang, C. Bender, Y. Ding, R. Jia, B. Li, 和 D. Song, “REFIT:
    一个用于数据有限的深度学习系统的统一水印去除框架，” *CoRR*, vol. abs/1911.07205, 2019\. [在线]. 可用: [http://arxiv.org/abs/1911.07205](http://arxiv.org/abs/1911.07205)'
- en: '[91] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Badnets: Evaluating backdooring
    attacks on deep neural networks,” *IEEE Access*, vol. 7, pp. 47 230–47 244, 01
    2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] T. Gu, K. Liu, B. Dolan-Gavitt, 和 S. Garg, “Badnets: 评估对深度神经网络的后门攻击，”
    *IEEE Access*, vol. 7, pp. 47 230–47 244, 01 2019。'
- en: '[92] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang, “Trojaning
    attack on neural networks,” *The Network and Distributed System Security Symposium
    (NDSS)*, 2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, 和 X. Zhang, “神经网络上的特洛伊木马攻击，”
    *网络和分布式系统安全研讨会（NDSS）*，2017年。'
- en: '[93] S. Li, M. Xue, B. Zhao, H. Zhu, and X. Zhang, “Invisible backdoor attacks
    on deep neural networks via steganography and regularization,” *IEEE Transactions
    on Dependable and Secure Computing*, vol. PP, pp. 1–1, 09 2020.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. Li, M. Xue, B. Zhao, H. Zhu, 和 X. Zhang, “通过隐写术和正则化对深度神经网络进行隐形后门攻击，”
    *IEEE可靠和安全计算学报*，vol. PP, pp. 1–1, 09 2020。'
- en: '[94] S. Li, H. Liu, T. Dong, B. Z. H. Zhao, M. Xue, H. Zhu, and J. Lu, “Hidden
    backdoors in human-centric language models,” *ACM Conference on Computer and Communications
    Security (CCS)*, 2021.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] S. Li, H. Liu, T. Dong, B. Z. H. Zhao, M. Xue, H. Zhu, 和 J. Lu, “以人为中心的语言模型中的隐藏后门，”
    *ACM计算机与通信安全会议（CCS）*，2021年。'
- en: '[95] R. Wang, F. Juefei-Xu, Q. Guo, Y. Huang, L. Ma, Y. Liu, and L. Wang, “Deeptag:
    Robust image tagging for deepfake provenance,” *arXiv preprint arXiv:2009.09869*,
    vol. 3, 2020.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] R. Wang, F. Juefei-Xu, Q. Guo, Y. Huang, L. Ma, Y. Liu, 和 L. Wang, “Deeptag:
    对深度伪造溯源的鲁棒图像标记，” *arXiv预印本 arXiv:2009.09869*，vol. 3, 2020年。'
