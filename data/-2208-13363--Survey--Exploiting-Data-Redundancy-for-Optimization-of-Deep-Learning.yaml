- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:44:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:44:29'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2208.13363] Survey: Exploiting Data Redundancy for Optimization of Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2208.13363] 调查：利用数据冗余优化深度学习'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2208.13363](https://ar5iv.labs.arxiv.org/html/2208.13363)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2208.13363](https://ar5iv.labs.arxiv.org/html/2208.13363)
- en: 'Survey: Exploiting Data Redundancy for Optimization of Deep Learning'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查：利用数据冗余优化深度学习
- en: Jou-An Chen [jchen73@ncsu.edu](mailto:jchen73@ncsu.edu) Department of Computer
    Science, North Carolina State UniversityUSA ,  Wei Niu [wniu@email.wm.edu](mailto:wniu@email.wm.edu)
    ,  Bin Ren [bren@cs.wm.edu](mailto:bren@cs.wm.edu) Department of Computer Science,
    William & MaryUSA ,  Yanzhi Wang [yanz.wang@northeastern.edu](mailto:yanz.wang@northeastern.edu)
    Department of Electrical and Computer Engineering, Northeastern UniversityUSA
     and  Xipeng Shen [xshen5@ncsu.edu](mailto:xshen5@ncsu.edu) Department of Computer
    Science, North Carolina State UniversityUSA(2020)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Jou-An Chen [jchen73@ncsu.edu](mailto:jchen73@ncsu.edu) 北卡罗来纳州立大学计算机科学系，美国，Wei
    Niu [wniu@email.wm.edu](mailto:wniu@email.wm.edu)，Bin Ren [bren@cs.wm.edu](mailto:bren@cs.wm.edu)
    威廉与玛丽学院计算机科学系，美国，Yanzhi Wang [yanz.wang@northeastern.edu](mailto:yanz.wang@northeastern.edu)
    诺斯伊斯特大学电气与计算机工程系，美国 和 Xipeng Shen [xshen5@ncsu.edu](mailto:xshen5@ncsu.edu) 北卡罗来纳州立大学计算机科学系，美国（2020）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Abstract.
- en: Data redundancy is ubiquitous in the inputs and intermediate results of Deep
    Neural Networks (DNN). It offers many significant opportunities for improving
    DNN performance and efficiency and has been explored in a large body of work.
    These studies have scattered in many venues across several years. The targets
    they focus on range from images to videos and texts, and the techniques they use
    to detect and exploit data redundancy also vary in many aspects. There is not
    yet a systematic examination and summary of the many efforts, making it difficult
    for researchers to get a comprehensive view of the prior work, the state of the
    art, differences and shared principles, and the areas and directions yet to explore.
    This article tries to fill the void. It surveys hundreds of recent papers on the
    topic, introduces a novel taxonomy to put the various techniques into a single
    categorization framework, offers a comprehensive description of the main methods
    used for exploiting data redundancy in improving multiple kinds of DNNs on data,
    and points out a set of research opportunities for future to explore.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据冗余在深度神经网络（DNN）的输入和中间结果中无处不在。它为提高DNN的性能和效率提供了许多重要机会，并在大量研究中得到了探讨。这些研究散布在多个领域和数年间，它们关注的目标范围从图像到视频和文本，而用于检测和利用数据冗余的技术也在许多方面有所不同。尚未对这些努力进行系统的审查和总结，这使得研究人员难以全面了解以往的工作、最新进展、差异和共通原则以及尚待探索的领域和方向。本文试图填补这一空白。它调查了数百篇相关的最新论文，引入了一个新的分类体系，将各种技术纳入一个分类框架，提供了对利用数据冗余改进多种DNN的数据处理的主要方法的全面描述，并指出了一系列未来研究的机会。
- en: 'Data Redundancy, Representation Redundancy, Deep Neural Network, Convolutional
    Neural Network, Transformer^†^†copyright: acmcopyright^†^†journalyear: 2020^†^†doi:
    XXX^†^†booktitle: Survey: Leverage Data Redundancy for DNN Optimizations^†^†price:
    15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs: Computing methodologies Machine
    Learning^†^†ccs: Computing methodologies Knowledge representation and reasoning'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据冗余、表示冗余、深度神经网络、卷积神经网络、Transformer^†^†版权：acmcopyright^†^†期刊年份：2020^†^†doi：XXX^†^†书名：调查：利用数据冗余优化DNN^†^†价格：15.00^†^†isbn：978-1-4503-XXXX-X/18/06^†^†ccs：计算方法
    机器学习^†^†ccs：计算方法 知识表示与推理
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'Deep learning, primarily powered by Deep Neural Networks (DNN), has repeatedly
    demonstrated its success and tremendous potential in revolutionizing the development
    of Artificial Intelligence and its applications in various domains. At a high
    level, DNN has two pillars: algorithm and data. DNN models embody the algorithm,
    and data is represented by DNN inputs and intermediate results (or called activation
    maps). Both algorithm and data are essential for the quality of a DNN, determining
    its accuracy, speed, size, energy efficiency, robustness, and so on.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，主要依赖于深度神经网络（DNN），已经多次证明了其在革命性发展人工智能及其在各个领域应用中的成功和巨大潜力。从高层次来看，DNN有两个支柱：算法和数据。DNN模型体现了算法，而数据则由DNN输入和中间结果（或称为激活图）表示。算法和数据对于DNN的质量至关重要，它们决定了DNN的准确性、速度、大小、能效、鲁棒性等。
- en: For DNNs, redundancy exists in both models and data. As many studies have shown,
    reducing some DNN layers or parameters or precision often leaves the model accuracy
    intact. The observation has prompted a large body of research in DNN compression,
    which tries to find effective ways to reduce the size of a DNN model and achieve
    more compact models or/and faster speeds. Many of the efforts resort to model
    pruning and quantization, while some compress models in the frequency domains (Wang
    et al., [2016](#bib.bib154)) or optimize attentions in Transformer models (Clark
    et al., [2019](#bib.bib33); Michel et al., [2019](#bib.bib114); Voita et al.,
    [2019](#bib.bib148)). We call those efforts model redundancy exploitations. Several
    recent survey papers (Cheng et al., [2018](#bib.bib29); Blalock et al., [2020](#bib.bib10);
    Ganesh et al., [2021](#bib.bib52); Choudhary et al., [2020](#bib.bib32); Hubara
    et al., [2017](#bib.bib77); Guo, [2018](#bib.bib62)) have provided comprehensive
    overviews on the topic.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度神经网络（DNN），模型和数据中都存在冗余。正如许多研究所示，减少一些DNN层或参数或精度通常不会影响模型的准确性。这一观察促使了大量关于DNN压缩的研究，这些研究试图找到有效的方法来减少DNN模型的大小，从而实现更紧凑的模型或/和更快的速度。许多努力依赖于模型剪枝和量化，而一些则在频域（Wang
    et al., [2016](#bib.bib154)）中压缩模型或优化Transformer模型中的注意力（Clark et al., [2019](#bib.bib33);
    Michel et al., [2019](#bib.bib114); Voita et al., [2019](#bib.bib148)）。我们将这些努力称为模型冗余利用。几篇最近的综述论文（Cheng
    et al., [2018](#bib.bib29); Blalock et al., [2020](#bib.bib10); Ganesh et al.,
    [2021](#bib.bib52); Choudhary et al., [2020](#bib.bib32); Hubara et al., [2017](#bib.bib77);
    Guo, [2018](#bib.bib62)）对该主题提供了全面的概述。
- en: Data redundancy exploitation is no less critical for DNN and has received much
    and increasing attention in recent several years as well. Many studies have noticed
    similar patches within an image, activation maps, or between adjacent video frames.
    Other studies have confirmed that some words in a sentence carry no significant
    meanings for the sentence. These observations have prompted many recent efforts
    in creating methods for detecting and leveraging the redundancy of various dimensions
    in all kinds of data. Hundreds of papers have been published, scattering in many
    venues over several years. But unlike model redundancy, there is not yet a systematic
    examination and summary of the many efforts, making it difficult for researchers
    to get a comprehensive view of the prior work, to learn about state of the art,
    to understand the differences and common principles among the many published studies,
    or to find out the areas and directions yet to explore.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据冗余利用对DNN同样至关重要，近年来也引起了越来越多的关注。许多研究发现图像、激活图或相邻视频帧中存在类似的片段。其他研究确认，句子中的某些词对句子没有重要意义。这些观察促使了许多近期的努力，致力于创建检测和利用各种数据维度冗余的方法。数以百计的论文已经发表，分布在许多场所，历时数年。但与模型冗余不同的是，目前尚无系统的检验和总结，使得研究人员难以全面了解先前的工作、掌握最新进展、理解诸多已发表研究之间的差异和共同原则，或发现尚待探索的领域和方向。
- en: To the best of our knowledge, this paper offers the first one-stop resource
    for people interested in learning about studies on data redundancy for DNN. It
    provides a holistic view of the various techniques and their connections, offers
    insights on the limitations of state of the art, and potential directions and
    opportunities for the future to explore. The paper introduces the first known
    taxonomy on DNN data redundancy exploitation, putting the various topics’ various
    techniques into a single categorization framework. It offers a comprehensive description
    of the main techniques used for detecting and exploiting data redundancy in improving
    multiple kinds of DNNs, from CNNs to RNNs, Transformers, and so on. It presents
    numerous data redundancy opportunities in images, videos, and texts and how existing
    studies tap into them with various techniques at a spectrum of granularities and
    scopes. It discusses the commonalities among the techniques, their differences,
    limitations, and promising research directions worth future explorations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，本文提供了首个针对有意了解DNN数据冗余研究的人的一站式资源。它提供了对各种技术及其相互联系的全面视角，洞察了当前最先进技术的局限性，以及未来探索的潜在方向和机会。本文介绍了首个已知的DNN数据冗余利用分类法，将各种主题的技术整合到一个分类框架中。它全面描述了用于检测和利用数据冗余以改进多种DNN（从CNN到RNN、Transformer等）的主要技术。它展示了图像、视频和文本中的众多数据冗余机会，以及现有研究如何通过各种技术在不同粒度和范围内加以利用。它讨论了技术之间的共性、差异、局限性以及值得未来探索的有前途的研究方向。
- en: 'We organize the rest of the paper as follows. Section [2](#S2 "2\. Terminology
    and Scope of Discussion ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning") provides a formal definition of data redundancy and defines
    the scope of this survey. Section [3](#S3 "3\. Taxonomy ‣ Survey: Exploiting Data
    Redundancy for Optimization of Deep Learning") presents a taxonomy of studies
    on DNN data redundancy. Sections [4](#S4 "4\. Leverage Redundancy in Image Data
    ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning"), [5](#S5
    "5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning"), and [6](#S6 "6\. Leverage Data Redundancy
    in Transformer Optimization for Text ‣ Survey: Exploiting Data Redundancy for
    Optimization of Deep Learning") describe the practical techniques that prior work
    has developed in detecting and exploiting data redundancy for DNNs respectively
    on images, videos, and text data. Section [7](#S7 "7\. Future Research Directions
    ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning") discusses
    the limitations of existing explorations and points out several future directions.
    Section [8](#S8 "8\. Conclusion ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning") concludes the survey with a summary.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将本文的其余部分组织如下。第[2](#S2 "2\. Terminology and Scope of Discussion ‣ Survey:
    Exploiting Data Redundancy for Optimization of Deep Learning")节提供了数据冗余的正式定义，并定义了本调查的范围。第[3](#S3
    "3\. Taxonomy ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")节呈现了DNN数据冗余研究的分类法。第[4](#S4
    "4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting Data Redundancy for
    Optimization of Deep Learning")、第[5](#S5 "5\. Leverage Data Redundancy in DNN
    for Videos ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")和第[6](#S6
    "6\. Leverage Data Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning")节分别描述了先前工作在图像、视频和文本数据中检测和利用数据冗余的实用技术。第[7](#S7
    "7\. Future Research Directions ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning")节讨论了现有探索的局限性，并指出了几个未来方向。第[8](#S8 "8\. Conclusion ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning")节总结了本调查。'
- en: 2\. Terminology and Scope of Discussion
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 术语与讨论范围
- en: We start this section with clarifications of several terms essential for the
    rest of the paper, then define the scope of this survey, and explain the relations
    with several relevant concepts to our focus.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节开始时对一些对本文其余部分至关重要的术语进行澄清，然后定义本调查的范围，并解释与我们关注的几个相关概念的关系。
- en: 2.1\. Terminology
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 术语
- en: Activation Map. In the traditional terminology, an activation map (or called
    feature map) is the activations of filters applied to the input of a DNN also
    to the output of the previous layer. For the visual representation of an image
    or a single image frame in a video with RGB channels, it is a 3D tensor with $Height\times
    Width\times Channel$; for a video clip, it is a 4D tensor with $Height\times Width\times
    Channel\times Depth$, the Depth is the number of frames. For ease of explanation,
    in this article, we extend the term activation map to include the inputs to a
    DNN (the values of the input layer neurons).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 激活图。在传统术语中，激活图（或称特征图）是应用于 DNN 输入的滤波器的激活，也包括前一层的输出。对于具有 RGB 通道的图像或视频中的单帧图像，它是一个三维张量，维度为
    $Height\times Width\times Channel$；对于视频片段，它是一个四维张量，维度为 $Height\times Width\times
    Channel\times Depth$，其中 Depth 是帧的数量。为了便于解释，在本文中，我们将激活图的术语扩展到包括 DNN 的输入（输入层神经元的值）。
- en: Hidden State. For text representation, before input to the first layer of the
    model, the text is transformed into word vectors representation via a word embedding
    (the collective name for a set of language modeling and feature learning techniques
    in natural language processing (NLP) where words or phrases from the vocabulary
    are mapped to vectors of real numbers (Embedding, [2020](#bib.bib44))). Multiplied
    with weights, the output activations of a layer are called a hidden state (which
    is also called encoder state). It is represented as a 2D tensor with sequence
    length $\times$ dimensionality of word embedding.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态。对于文本表示，在输入到模型的第一层之前，文本通过词嵌入（在自然语言处理（NLP）中，词嵌入是对一组语言建模和特征学习技术的统称，其中词汇表中的词或短语被映射到实数向量中（Embedding,
    [2020](#bib.bib44)））转换为词向量表示。与权重相乘后，层的输出激活被称为隐藏状态（也称为编码器状态）。它表示为一个二维张量，维度为序列长度
    $\times$ 词嵌入的维度。
- en: Redundancy. According to the dictionary (of Redundancy by Oxford Dictionary,
    [2020](#bib.bib118)), redundancy is the state of being not or no longer needed
    or valuable. It is hence a concept relative to a particular purpose. In the context
    of DNN, being useful usually refers to contributing to the quality of the DNN
    outputs, which is often measured by a certain kind of accuracy metric. Data redundancy
    in DNN is hence defined as the data that is not or no longer useful for the quality
    of the outputs of DNN. Data redundancy exploitation for DNN refers to techniques
    that try to avoid data redundancy while keeping DNN outputs meeting the needs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余。根据牛津词典（[2020](#bib.bib118)）的定义，冗余是指不再需要或不再有价值的状态。因此，它是一个相对于特定目的的概念。在 DNN
    的上下文中，有用通常是指对 DNN 输出质量的贡献，这通常通过某种准确度指标来衡量。因此，DNN 中的数据冗余被定义为对 DNN 输出质量不再有用的数据。DNN
    中数据冗余的利用是指在保持 DNN 输出满足需求的同时，尝试避免数据冗余的技术。
- en: 2.2\. Scope
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 范围
- en: 'Data redundancy in DNN manifests in different kinds of forms. Regardless of
    the concrete structure, data redundancy in DNN must fall into one of the following
    three categories: (1) Repeated information: Some part of the data conveys the
    identical (or similar) information as some other parts do. (2) Irrelevant information:
    The information conveyed by some part of the data is irrelevant to the targeted
    outputs of the DNN. (3) Over-detailed information: The information is conveyed
    in an unnecessarily detailed manner (e.g., image resolution). So any technique
    that tries to make a DNN avoid spending time and computations on any of the three
    types of information can be regarded as a technique for data redundancy exploitation.
    However, this general definition would blur the boundary between DNN algorithm,
    model, and data-centered optimizations. Some model optimizations (e.g., channel
    pruning (He et al., [2017](#bib.bib67); Wang et al., [2018](#bib.bib149); Zhou
    et al., [2019](#bib.bib165); Zhuang et al., [2018](#bib.bib171); Hou and Kung,
    [2020a](#bib.bib71); Li et al., [2020](#bib.bib102); Hou and Kung, [2020b](#bib.bib72);
    Liu et al., [2018b](#bib.bib111))) are fundamentally driven by data redundancy
    (e.g., redundancy across channels). There are already surveys, particularly on
    model optimizations, but this survey focuses on data redundancy exploitation beyond
    model optimizations. These techniques help DNN avoid data redundancy in its computations
    to efficiently execute DNN in inference or training or eliminate noise to boost
    DNN accuracy.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 中的数据冗余表现为不同形式。无论具体结构如何，DNN 中的数据冗余必须属于以下三类之一：（1）重复信息：数据的某部分传达了与其他部分相同（或类似）的信息。（2）无关信息：数据的某部分传达的信息与
    DNN 的目标输出无关。（3）过于详细的信息：信息以不必要的详细方式传达（例如，图像分辨率）。因此，任何试图让 DNN 避免在这三种信息上花费时间和计算的技术，都可以视为数据冗余利用技术。然而，这一定义会模糊
    DNN 算法、模型和数据中心优化之间的界限。一些模型优化（例如，通道剪枝（He 等，[2017](#bib.bib67)；Wang 等，[2018](#bib.bib149)；Zhou
    等，[2019](#bib.bib165)；Zhuang 等，[2018](#bib.bib171)；Hou 和 Kung，[2020a](#bib.bib71)；Li
    等，[2020](#bib.bib102)；Hou 和 Kung，[2020b](#bib.bib72)；Liu 等，[2018b](#bib.bib111)）从根本上受到数据冗余的驱动（例如，跨通道冗余）。已有一些调查，特别是关于模型优化的，但本次调查集中在超越模型优化的数据冗余利用上。这些技术帮助
    DNN 避免在计算中遇到数据冗余，从而高效地执行 DNN 推理或训练，或消除噪声以提高 DNN 准确性。
- en: There is a body of work on considering data redundancy in the hardware design
    of DNN accelerators, such as cache buffer designs for data reuse in 2D CNNs (Kim
    et al., [2020b](#bib.bib90); Mocerino et al., [2019](#bib.bib115); Jiao et al.,
    [2018](#bib.bib83); Ma et al., [2020](#bib.bib112); Salamat et al., [2018](#bib.bib129);
    Hegde et al., [2018b](#bib.bib69); Wang et al., [2019a](#bib.bib151)) and 3D CNNs (Wang
    et al., [2020](#bib.bib153), [2019b](#bib.bib152); Fan et al., [2017a](#bib.bib45);
    Wang et al., [2017](#bib.bib150); Shen et al., [2018](#bib.bib133); Hegde et al.,
    [2018a](#bib.bib68); Chen et al., [2019c](#bib.bib20)), or integrating activation
    pruning into the hardware architecture designs (Samal et al., [2020](#bib.bib130);
    Piyasena et al., [2019](#bib.bib124)). The techniques reduce the number of computations
    and bring speed and energy benefits. In this survey, we mainly focus our discussion
    on *software-based techniques* for exploiting data redundancy.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在 DNN 加速器的硬件设计中考虑数据冗余的研究已经存在，例如用于 2D CNNs 数据重用的缓存缓冲区设计（Kim 等，[2020b](#bib.bib90)；Mocerino
    等，[2019](#bib.bib115)；Jiao 等，[2018](#bib.bib83)；Ma 等，[2020](#bib.bib112)；Salamat
    等，[2018](#bib.bib129)；Hegde 等，[2018b](#bib.bib69)；Wang 等，[2019a](#bib.bib151)）和
    3D CNNs（Wang 等，[2020](#bib.bib153)，[2019b](#bib.bib152)；Fan 等，[2017a](#bib.bib45)；Wang
    等，[2017](#bib.bib150)；Shen 等，[2018](#bib.bib133)；Hegde 等，[2018a](#bib.bib68)；Chen
    等，[2019c](#bib.bib20)），或将激活剪枝集成到硬件架构设计中（Samal 等，[2020](#bib.bib130)；Piyasena 等，[2019](#bib.bib124)）。这些技术减少了计算量，带来了速度和能效上的好处。在本次调查中，我们主要关注*基于软件的技术*来利用数据冗余。
- en: 2.3\. Relations with Relevant Concepts
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 与相关概念的关系
- en: Several concepts are closely related to data redundancy exploitation. We next
    describe the relations with these concepts to further clarify our scope.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 几个概念与数据冗余利用密切相关。接下来，我们描述这些概念之间的关系，以进一步澄清我们的范围。
- en: Model Redundancy Another aspect of redundancy in DNNs is model redundancy, referring
    to the redundancy within the parameters and architecture of a DNN. Model compression (Cheng
    et al., [2018](#bib.bib29); Blalock et al., [2020](#bib.bib10); Ganesh et al.,
    [2021](#bib.bib52); Choudhary et al., [2020](#bib.bib32); Hubara et al., [2017](#bib.bib77);
    Guo, [2018](#bib.bib62); Kim et al., [2020a](#bib.bib91); Acharya et al., [2019](#bib.bib2);
    Chen et al., [2016](#bib.bib26)), including knowledge distillation (Hinton et al.,
    [2014](#bib.bib70); Urban et al., [2017](#bib.bib147)), parameter pruning, and
    weight quantization, are popular approaches to exploit model redundancy. This
    article focuses on *data redundancy* of DNN, which refers to the redundancy within
    the input data of each DNN layer (usually in the form of multi-dimensional tensors).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 模型冗余 DNN中冗余性的另一个方面是模型冗余，指的是DNN的参数和架构中的冗余性。模型压缩（Cheng等，[2018](#bib.bib29); Blalock等，[2020](#bib.bib10);
    Ganesh等，[2021](#bib.bib52); Choudhary等，[2020](#bib.bib32); Hubara等，[2017](#bib.bib77);
    Guo，[2018](#bib.bib62); Kim等，[2020a](#bib.bib91); Acharya等，[2019](#bib.bib2);
    Chen等，[2016](#bib.bib26))，包括知识蒸馏（Hinton等，[2014](#bib.bib70); Urban等，[2017](#bib.bib147))、参数修剪和权重量化，是利用模型冗余的流行方法。本文的重点是DNN的*数据冗余*，它指的是每个DNN层的输入数据中的冗余性（通常以多维张量的形式）。
- en: Data Preprocessing and Augmentation Before data are fed into a DNN model, they
    often go through a preprocessing process. For training, that process is sometimes
    part of dataset augmentation (Shorten and Khoshgoftaar, [2019](#bib.bib137); Cubuk
    et al., [2019](#bib.bib34)), increasing data variance to increase the models’
    generality. In computer vision, example operations include rotating, inverting,
    equalizing, Color changes, brightness adjustment, etc. The samples created by
    the transformations naturally carry some redundancy with the original data. For
    instance, with images inverted, two images contain the same set of pixel values,
    despite that the positions of individual image patches differ in the images. Such
    redundancy is implicitly considered when a technique exploits redundancy in input
    data. This survey exploits data redundancy and leaves data augmentation out of
    the scope.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理和增强 在将数据馈送到DNN模型之前，它们通常会经过预处理过程。对于训练来说，这个过程有时是数据增强的一部分（Shorten和Khoshgoftaar，[2019](#bib.bib137);
    Cubuk等，[2019](#bib.bib34))，它增加了数据的方差以增加模型的泛化能力。在计算机视觉中，示例操作包括旋转、反转、均衡化、颜色变化、亮度调整等。通过变换创建的样本自然包含一些与原始数据的冗余性。例如，对于图像反转，两幅图像包含相同的像素值集，尽管个别图像块的位置在图像中可能会有所不同。当技术利用输入数据的冗余性时，这种冗余性被隐含地考虑在内。本次调查利用了数据的冗余性，并将数据增强排除在外。
- en: Feature Extraction and Data Embedding. Feature extraction is a term in machine
    learning, referring to techniques that select or derive some features from a set
    of data that are regarded as most relevant to a specific machine learning task.
    Therefore, theoretically speaking, feature extraction can be viewed as a kind
    of exploitation of data redundancy as it reduces the raw data to an often smaller
    set of features. In a similar vein, data embeddings that map raw data to vectors
    in a space smaller than the original data space could also be regarded as a kind
    of reduction of data redundancy. Studies on these topics are usually considered
    as a separate research area named feature extraction and representation. They
    are typically not designed explicitly to exploit data redundancy, even though
    they may sometimes show such effects. We leave discussions on these studies out
    of the main focus of this paper.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取和数据嵌入。特征提取是机器学习中的一个术语，指的是从一组数据中选择或派生出一些被认为与特定机器学习任务最相关的特征的技术。因此，从理论上讲，特征提取可以被视为一种利用数据冗余的方法，因为它将原始数据减少到通常更小的一组特征。类似地，将原始数据映射到比原始数据空间更小的向量空间的数据嵌入也可以被视为一种减少数据冗余的方法。对这些主题的研究通常被视为一个名为特征提取和表示的独立研究领域。它们通常不是专门设计用于利用数据冗余，尽管它们有时可能显示出这样的效果。我们将不讨论这些研究，把主要焦点放在本文以外。
- en: Dataset Selection. In the learning phase of DNN, *training data selection* (Fan
    et al., [2017b](#bib.bib46); Feng et al., [2019](#bib.bib48); Zheng et al., [2017](#bib.bib164))
    is a process that tries to select the training data appropriate for the learner
    to achieve the learning task. To a certain degree, it may also remove some redundancy
    in the dataset (e.g., some data items in an over-sampled population). In this
    survey, we do not focus our discussion on this direction but on how data redundancy
    is addressed during the execution of the model (after the training dataset is
    selected or during inference).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集选择。在DNN的学习阶段，*训练数据选择*（Fan 等， [2017b](#bib.bib46)；Feng 等， [2019](#bib.bib48)；Zheng
    等， [2017](#bib.bib164)）是一个尝试选择适合学习者完成学习任务的训练数据的过程。在一定程度上，它也可能去除数据集中一些冗余的数据项（例如，过度采样的人群中的一些数据项）。在这次调查中，我们不专注于这一方向，而是关注模型执行过程中（训练数据集选择之后或推理过程中）如何解决数据冗余问题。
- en: 3\. Taxonomy
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 分类
- en: Many studies detect or leverage data redundancy in DNN from several angles.
    A taxonomy that puts all the investigations into one categorization framework
    is essential for a holistic view of their differences, tradeoffs and shared principles.
    The taxonomy also offers a comprehensive view at the various dimensions of DNN
    data redundancy elimination, which can potentially guide the design of new redundancy
    elimination techniques.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究从不同角度检测或利用DNN中的数据冗余。将所有研究纳入一个分类框架的分类是全面了解它们差异、权衡和共同原则的重要手段。该分类还提供了DNN数据冗余消除各个维度的全面视角，可能会指导新冗余消除技术的设计。
- en: 'Figure [1](#S3.F1 "Figure 1 ‣ 3\. Taxonomy ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning") presents a taxonomy we build after surveying
    hundreds of papers on DNN data redundancy exploitation. As far as we know, this
    is the first taxonomy on this topic. It shows the six most essential dimensions
    in DNN data redundancy exploitation. Data redundancy exploitation is usually the
    combination of one or more items in each dimension. We explain each of the dimensions
    next.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S3.F1 "图 1 ‣ 3\. 分类 ‣ 调查：利用数据冗余优化深度学习")展示了我们在调查了数百篇关于深度神经网络（DNN）数据冗余利用的论文后建立的分类。就我们所知，这是关于这个主题的首个分类。它展示了在DNN数据冗余利用中最重要的六个维度。数据冗余利用通常是每个维度中一个或多个项目的组合。我们接下来将解释每一个维度。
- en: '![Refer to caption](img/e2a26b8741a9b2e873e1fc259cb5f30a.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e2a26b8741a9b2e873e1fc259cb5f30a.png)'
- en: Figure 1\. Taxonomy of data redundancy exploitation in DNN.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. DNN中数据冗余利用的分类。
- en: 3.1\. Granularity
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 粒度
- en: This dimension refers to the unit of the data examined for data redundancy,
    which we will call data unit in the following discussions. There are five granularities
    listed below in increasing order in size.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个维度指的是检查数据冗余的数据单位，我们将在以下讨论中称之为数据单元。以下列出了五种粒度，按大小递增排列。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Bit: At this granularity, every bit in a value is the unit for examination.
    Data quantization and precision relaxation to the shorter representation of a
    value are commonly seen strategies exploiting bit-level redundancy. Besides, bit
    representation can also be used directly for pattern-based bucketing. In RNSNet (Salamat
    et al., [2018](#bib.bib129)), for instance, an input value is transformed into
    its n-bits binary format and gets represented with a residue number system (RNS).
    The multiplications in neural networks are simplified to only addition and memory
    lookup, allowing memory-friendly operations.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位：在这种粒度下，每个位是检查的单位。数据量化和对值的精度放宽到较短表示是常见的利用位级冗余的策略。此外，位表示还可以直接用于基于模式的分桶。例如，在RNSNet（Salamat
    等， [2018](#bib.bib129)）中，输入值被转换为其n位二进制格式，并用残差数系统（RNS）表示。神经网络中的乘法被简化为仅加法和内存查找，从而实现了内存友好的操作。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Neuron: The unit at this granularity is the value of a single neuron. An image
    is a pixel value at one channel or the value carried by a neuron in a DNN.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经元：在这种粒度下，单位是单个神经元的值。图像是一个通道的像素值或DNN中一个神经元所承载的值。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tile/Patch: The values carried by a set of neurons, which correspond to a part
    of an activation map. It could be, for instance, a bounding box or region of interest
    (RoI) or channel in an image; in the case of text, it could be one or multiple
    sub-word embeddings.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 瓦片/补丁：由一组神经元承载的值，对应于激活图的一部分。例如，它可以是图像中的一个边界框或感兴趣区域（RoI）或通道；在文本的情况下，它可以是一个或多个子词嵌入。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Activation Map: The values in an entire activation map.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 激活图：整个激活图中的值。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Batch or Sequence: A set of activation maps that the DNN carries at a layer,
    either at once or in order. The activation maps within the set can be either related
    or unrelated to each other. For example, sequential sentence input in a paragraph
    to a DNN can be correlated, while in a randomly shuffled set of training images,
    image activation maps are generally unrelated.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批次或序列：DNN在一层中携带的一组激活图，无论是一次性还是按顺序。集合中的激活图可以彼此相关，也可以彼此无关。例如，在DNN中，段落中的连续句子输入可以是相关的，而在随机打乱的训练图像集合中，图像激活图通常是无关的。
- en: 'In general, the larger the granularity is, the less redundancy is there, but
    at the same time, the ratio between the benefits from removing a redundancy and
    the overhead in finding the redundancy is larger¹¹1Bit level is special, limited
    by the number of bits in one value, which is usually up to 32.: Avoiding processing
    a whole batch of images saves more time than avoiding processing a neuron, but
    the chances in finding two identical batches are generally smaller than finding
    two identical neurons. On the other hand, the removals of different granularities
    of redundancy are not exclusive; one optimization could exploit redundancies at
    multiple granularities.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，粒度越大，冗余越少，但与此同时，消除冗余的收益与发现冗余的开销之间的比率更大¹¹1比特级别是特殊的，受限于一个值中的比特数，通常最多为32。：避免处理整批图像比避免处理单个神经元节省的时间更多，但找到两个完全相同的批次的机会通常比找到两个完全相同的神经元的机会要小。另一方面，不同粒度的冗余的移除并不是互斥的；一种优化可以利用多个粒度的冗余。
- en: 3.2\. Scope of Consideration
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 考虑范围
- en: 'This dimension is about the scope in which different data units are compared
    to identify similar units and hence redundancy. This dimension is related to the
    previous extent, granularity, but differ: For a given data unit, such as a patch
    of activation map, the comparisons for similar units can still vary (e.g., within
    one activation map, across activation maps in a batch, or batches). Specifically,
    the scopes considered in prior studies fall into one or more following.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个维度是关于比较不同数据单元以识别相似单元及其冗余的范围。这个维度与前述的范围、粒度相关，但有所不同：对于给定的数据单元，例如激活图的一个块，相似单元的比较仍然可以变化（例如，在一个激活图内、批次中的激活图之间，或批次之间）。具体而言，先前研究中考虑的范围可以归入一个或多个以下类型。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Single Value: Redundancy within a single representation of a value—the corresponding
    granularity is a bit.'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单一值：单一值表示中的冗余——对应的粒度是比特。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Single Channel Activation Map: In the case of an image, it refers to redundancy
    within a single channel activation map; in the case of text, it relates to redundancy
    within a single hidden state. The corresponding granularity can be either neuron
    or tile/patch.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单通道激活图：在图像的情况下，它指的是单个通道激活图中的冗余；在文本的情况下，它涉及单个隐藏状态中的冗余。相应的粒度可以是神经元或块/瓷砖。
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multiple Channels of Activation Map: This is for visual data (image and video)
    only. Redundancy across multiple channels of an activation map between neurons,
    patches/tiles, or single-channel activation maps.'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 激活图的多通道：仅适用于视觉数据（图像和视频）。神经元之间、块/瓷砖之间或单通道激活图之间的多个通道的冗余。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Temporal Sequence of Activation Maps: In the case of video, it refers to redundancy
    across a single video clip representation (4D tensor) between neurons, patches/tiles,
    single-channel activation maps, or activation maps. In the case of sequential
    text sequence input, it refers to redundancy across hidden states of the same
    sentence label (token type id), between neurons, sub-word embeddings, or single-sequence
    hidden states.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 激活图的时间序列：在视频的情况下，它指的是单个视频片段表示（4D 张量）中神经元、块/瓷砖、单通道激活图或激活图之间的冗余。在序列文本输入的情况下，它指的是同一句子标签（标记类型
    ID）的隐藏状态之间的冗余，神经元之间、子词嵌入之间或单序列隐藏状态之间。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Within a Batch: Inputs to a DNN are often provided in a batch each time. The
    scope for data redundancy consideration can be across inputs within a batch.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在一个批次内：DNN的输入通常是每次以批次的形式提供的。数据冗余考虑的范围可以是批次内的输入之间。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cross Batches: The scope can also cross the boundaries of batches of inputs.'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨批次：范围也可以跨越输入批次的边界。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cross Layers: All the earlier scopes typically assume that the comparisons
    are about activation maps at the same layer of a Neural Network. Some studies
    even expand the scope to activation maps on different layers of a Neural Network (Park
    and Kim, [2019](#bib.bib120); Dalvi et al., [2020](#bib.bib39)).'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨层：所有早期范围通常假设比较的是神经网络同一层的激活图。一些研究甚至扩展到神经网络不同层的激活图（Park 和 Kim，[2019](#bib.bib120)；Dalvi
    等，[2020](#bib.bib39)）。
- en: 'In general, a large scope subsumes a smaller scope: Redundancy discoverable
    in a smaller scope must be discoverable in a larger scope. But on the other hand,
    checking values in a larger scope also entails more overhead and complexity.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，大范围包含较小范围：在较小范围内发现的冗余必须在较大范围内也能发现。但另一方面，检查较大范围的值也意味着更多的开销和复杂性。
- en: 3.3\. Types of Data Redundancy
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 数据冗余的类型
- en: Data redundancy exploitation techniques can also be categorized based on what
    types of data redundancy they exploit. As earlier sections have mentioned, there
    are mainly three types of data redundancy. We list them here for the completeness
    of the discussion on the taxonomy.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据冗余利用技术也可以根据它们利用的数据冗余类型进行分类。如前面章节所述，数据冗余主要有三种类型。我们在此列出它们，以完整讨论分类学。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Repeated Information: Similarity or duplication between data units. When the
    same operations operate on either identical or similar data units (e.g., multiplications
    on the same values, convolutions on similar image tiles), the results could be
    indistinguishable; the computations are redundant.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重复信息：数据单元之间的相似性或重复。当相同的操作应用于相同或相似的数据单元时（例如，对相同值进行乘法运算，对相似图像块进行卷积），结果可能难以区分；这些计算是冗余的。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Irrelevant Information: Unnecessary data units; if they are removed, will not
    harm the DNN computation results.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无关信息：不必要的数据单元；如果将其移除，不会影响 DNN 的计算结果。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Over-detailed Information: High-precision representation is not always necessary.
    For example, high-resolution images can be replaced with their low-resolution
    approximation in some application scenarios without causing the DNN to lose accuracy.
    Activation maps with lower precision sometimes give the same results for a DNN.
    A video with more frames than needed is another example. A sub-word vector with
    a higher-dimension representation (e.g., $1\times 512$ versus $1\times 256$ for
    a single word representation) or a higher precision value representation is also
    an example.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过于详细的信息：高精度表示并不总是必要的。例如，在某些应用场景中，高分辨率图像可以用其低分辨率近似替代，而不会导致 DNN 精度下降。较低精度的激活图有时会为
    DNN 给出相同的结果。一个例子是帧数超过需求的视频。具有更高维度表示的子词向量（例如，$1\times 512$ 对比 $1\times 256$ 单词表示）或更高精度值表示也是一个例子。
- en: The types of data redundancy define how redundancy appears. These different
    types of redundancy are complementary to each other; they require different ways
    to detect but at the same time can be capitalized together. Previous work has
    each focused on one of the three types of redundancy; it remains yet to investigate
    how to effectively combine the removals of multiple types of redundancy in one
    framework. Among the potential challenges, how to minimize the overhead is one
    of them.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据冗余的类型定义了冗余的表现形式。这些不同类型的冗余是互补的；它们需要不同的方法来检测，但同时可以一起利用。之前的工作主要关注三种冗余类型中的一种；尚未研究如何在一个框架中有效结合多种冗余的去除。潜在的挑战之一是如何最小化开销。
- en: 3.4\. Ways to Detect Data Redundancy
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 检测数据冗余的方法
- en: 'The techniques can also be classified based on the ways they detect redundancy:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术也可以根据它们检测冗余的方式进行分类：
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Value Similarity-based Grouping: This includes all kinds of similarity-based
    data grouping methods (e.g., various data clustering methods).'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于值相似度的分组：包括所有基于相似度的数据分组方法（例如，各种数据聚类方法）。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Learning-based Methods: These methods learn data redundancy on some data samples
    (often sampled from the training or validation datasets) and apply the knowledge
    at runtime execution of the model. An example is the learning-based quantization
    of activation maps.'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于学习的方法：这些方法在一些数据样本上学习数据冗余（通常从训练或验证数据集中抽样），并在模型运行时应用这些知识。例如，基于学习的激活图量化。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Projection: These techniques project the data representation to another domain
    space (e.g., frequency domain) and infer redundancy in that space.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 投影：这些技术将数据表示投影到另一个领域空间（例如，频域），并推断该空间中的冗余。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Implicit Detection: These techniques assume redundancy exists in a specific
    scope based on some prior knowledge about the domain. One of the examples is the
    techniques that leverage similarities between neighboring video frames.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐式检测：这些技术基于对领域的某些先验知识，假设在特定范围内存在冗余。例如，利用相邻视频帧之间的相似性技术就是一个例子。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Other Heuristics: Methods that leverage some other domain-specific heuristics,
    such as the relative attentions or the average number of zeros appearing at a
    specific pixel location.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他启发式方法：利用其他领域特定启发式方法的方法，例如相对注意力或在特定像素位置出现的零的平均数量。
- en: The different ways apply to different domains and scenarios. The first two in
    the list are general, the third applies to the domains where projection across
    certain spaces makes sense, the fourth and fifth apply to domains where there
    is already certain prior knowledge about the redundancy in the domains.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的方法适用于不同的领域和场景。列表中的前两项是通用的，第三项适用于在某些空间上进行投影的领域，第四项和第五项适用于在领域内已有一定冗余先验知识的领域。
- en: 3.5\. Ways to Leverage Redundancy
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5. 利用冗余的方法
- en: The different techniques take different ways to leverage data redundancy. They,
    however, in principle fall into the following three main categories.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的技术采取不同的方法来利用数据冗余。然而，它们原则上可分为以下三类。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reuse: This approach reuses computation results on some data items for other
    data items. It applies to a similar kind of data redundancy. For example, Deep
    Reuse (Ning and Shen, [2019](#bib.bib117)) applies Locality Sensitive Hashing
    (LSH) (Gionis et al., [1999](#bib.bib57)), a fast unsupervised clustering scheme,
    to cluster data into similarity groups, and then uses the results computed on
    the cluster centroids as the results for all data items in the same cluster.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重用：这种方法将某些数据项上的计算结果用于其他数据项。它适用于类似的数据冗余。例如，Deep Reuse (Ning 和 Shen, [2019](#bib.bib117))
    采用局部敏感哈希 (LSH) (Gionis 等人, [1999](#bib.bib57))，一种快速的无监督聚类方案，将数据聚类为相似组，然后将计算结果应用于同一簇中的所有数据项。
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Skipping: This approach skips computations on some data items. It applies to
    data redundancy caused by irrelevant data items. For instance, in Perforated CNN (Figurnov
    et al., [2016](#bib.bib50)), the authors deal with spatial redundancy by masking
    some pixels in CNN feature maps. It also includes techniques that select a subset
    of data representations. Some work does ad-hoc sampling, while some others make
    the selection in a more sophisticated way. In PoWER-BERT (Goyal et al., [2020](#bib.bib59)),
    for instance, a retention configuration is defined to observe that cosine-similarity
    between vectors progressively increases as the layer goes deeper. Based on that
    setting, they employ strategies for word-vector selection. The retention configuration
    is further designed into learnable hyper-parameters.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跳过：这种方法跳过某些数据项上的计算。它适用于由于无关数据项引起的数据冗余。例如，在 Perforated CNN (Figurnov 等人, [2016](#bib.bib50))
    中，作者通过在 CNN 特征图中遮蔽某些像素来处理空间冗余。它还包括选择数据表示子集的技术。一些工作采用即兴采样，而另一些则以更复杂的方式进行选择。例如，在
    PoWER-BERT (Goyal 等人, [2020](#bib.bib59)) 中，定义了一个保留配置，以观察随着层数加深，向量之间的余弦相似度逐渐增加。基于该设置，他们采用了词向量选择策略。保留配置进一步设计为可学习的超参数。
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Value Approximation: This approach uses an approximation to generate data representation.
    It corresponds to data redundancy caused by over-detailed information. Quantization
    and binarization on data items fall into this category. Previous reviews (Hubara
    et al., [2017](#bib.bib77); Simons and Lee, [2019](#bib.bib138); Qin et al., [2020](#bib.bib125))
    have provided comprehensive coverage on both research topics, so we do not emphasize
    them in the following discussion.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 值近似：这种方法使用近似生成数据表示。它对应于由于过于详细的信息而引起的数据冗余。数据项上的量化和二值化属于此类别。之前的综述 (Hubara 等人,
    [2017](#bib.bib77); Simons 和 Lee, [2019](#bib.bib138); Qin 等人, [2020](#bib.bib125))
    已经对这两个研究主题进行了全面覆盖，因此我们在以下讨论中不再强调。
- en: 'These three ways to leverage redundancy are complementary. Even though only
    the third in the list carries ”approximation” in the name, all three could cause
    accuracy loss: Reuse may reuse results across similar but not identical data,
    and similarly, skipping may skip the computations of some similar but not identical
    data. These ways of leveraging redundancy can be used together. Some prior studies
    have already done that. One of them (Dalvi et al., [2020](#bib.bib39)), for instance,
    exploits a layer selector (”skipping”) and correlation clustering (”reuse”) practice
    at the same time.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种利用冗余的方法是互补的。尽管列表中的第三种方法名称中包含“近似”，但所有三种方法都可能导致准确性损失：重用可能在相似但不完全相同的数据上重用结果，跳过则可能跳过一些相似但不完全相同的数据的计算。这些利用冗余的方法可以一起使用。一些先前的研究已经做到了这一点。例如，其中一项研究（Dalvi
    等， [2020](#bib.bib39)）同时利用了层选择器（“跳过”）和相关聚类（“重用”）的实践。
- en: 3.6\. Types of Data
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. 数据类型
- en: 'We can also classify the studies on data redundancy based on the types of data
    they deal with. Although there are many kinds of data, most existing studies on
    data redundancy are on the following three types of data:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以根据处理的数据类型对数据冗余研究进行分类。虽然数据种类繁多，但大多数现有的数据冗余研究集中在以下三种数据类型上：
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Images. This type of data includes various kinds of images, including those
    generated from higher dimension sensors (e.g., those collected through Lidar).
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像。这种数据类型包括各种图像，包括那些从更高维度的传感器（例如通过激光雷达收集的图像）生成的图像。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Videos. This type of data features an extra-temporal dimension than images,
    which provides special opportunities for data redundancy exploitation.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频。这种数据类型比图像具有额外的时间维度，为数据冗余利用提供了特殊的机会。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Texts. This type of data consists of texts from various kinds of sources, usually
    written in Natural Languages.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文本。这种数据类型包括来自各种来源的文本，通常用自然语言书写。
- en: 'There are some other types of data that DNN has been applied to, such as graphs,
    genes, computer programs, and so on. We will briefly discuss them in Section [7](#S7
    "7\. Future Research Directions ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning").'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 还被应用于其他类型的数据，例如图形、基因、计算机程序等。我们将在第 [7](#S7 "7\. 未来研究方向 ‣ 调查：深度学习优化中的数据冗余利用")
    节中简要讨论这些数据类型。
- en: 3.7\. Use of the Taxonomy
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7\. 分类法的使用
- en: As far as we know, this is the first taxonomy on data redundancy exploitation.
    It was created based on our survey of hundreds of papers on data redundancy exploitation.
    The taxonomy can be used to position work in the big picture, to recognize the
    possible missing opportunities yet to explore for a certain kind of data, to guide
    the designs of future DNNs for efficiency, and to assist the possible creation
    of future automatic frameworks that may apply redundancy exploitation for new
    Deep Learning tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，这是关于数据冗余利用的首个分类法。它是基于我们对数百篇关于数据冗余利用论文的调查创建的。这个分类法可以用来在大背景中定位工作，识别尚待探索的某种数据可能存在的机会，指导未来
    DNN 的效率设计，以及协助未来可能创建的自动化框架，这些框架可能应用冗余利用于新的深度学习任务。
- en: 'We categorize a set of representative papers listed in Table [1](#S3.T1 "Table
    1 ‣ 3.7\. Use of the Taxonomy ‣ 3\. Taxonomy ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning") using the taxonomy. We will discuss them in
    more detail in the next several sections. Specifically, we organize the following
    discussions based on several layers of the taxonomy. At the highest level, we
    divide the studies based on the types of data they handle into three sections,
    with Section [4](#S4 "4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning") on images, Section [5](#S5
    "5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning") on videos, and Section [6](#S6 "6\. Leverage
    Data Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting Data
    Redundancy for Optimization of Deep Learning") on texts. We organize the various
    studies based on a dimension that captures the most prominent differences among
    the studies in each area.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用分类法对表[1](#S3.T1 "表 1 ‣ 3.7\. 分类法的使用 ‣ 3\. 分类法 ‣ 调查：利用数据冗余优化深度学习")中列出的一组代表性论文进行分类。我们将在接下来的几个部分中详细讨论它们。具体来说，我们根据分类法的多个层级组织以下讨论。在最高层次上，我们根据数据类型将研究分为三个部分，其中第[4](#S4
    "4\. 利用图像数据的冗余 ‣ 调查：利用数据冗余优化深度学习")部分讨论图像，第[5](#S5 "5\. 在视频中利用数据冗余 ‣ 调查：利用数据冗余优化深度学习")部分讨论视频，第[6](#S6
    "6\. 在文本中利用数据冗余优化 Transformer ‣ 调查：利用数据冗余优化深度学习")部分讨论文本。我们根据能够捕捉各领域研究中最显著差异的维度来组织各种研究。
- en: Table 1\. Some representative papers on data redundancy exploitation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 一些关于数据冗余利用的代表性论文。
- en: '| Data Type | Paper | Redundancy Type | Granularity | Scope | Detection | Exploitation
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 论文 | 冗余类型 | 粒度 | 范围 | 检测 | 利用 |'
- en: '| Image | (Ning and Shen, [2019](#bib.bib117)) | Repeated | Patch/Tile | Multiple
    Channels, Within-Batch, Cross-Batch | Similarity | Reuse |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 图像 | (Ning 和 Shen，[2019](#bib.bib117)) | 重复 | 区块/切片 | 多通道、批内、批间 | 相似性 | 重用
    |'
- en: '| (Georgiadis, [2019](#bib.bib56)) | Irrelevant, Repeated | Bit, Neuron | Single
    Value, Cross-Layer | Learning, Similarity | Value Approximation, Reuse |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| (Georgiadis，[2019](#bib.bib56)) | 不相关、重复 | 位、神经元 | 单值、跨层 | 学习、相似性 | 值近似、重用
    |'
- en: '| (Park and Kim, [2019](#bib.bib120)) | Repeated | Activation Map | Cross-Layer
    | Similarity | Reuse |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (Park 和 Kim，[2019](#bib.bib120)) | 重复 | 激活图 | 跨层 | 相似性 | 重用 |'
- en: '| (de Moura et al., [2019](#bib.bib40)) | Repeated | Activation Map | Multiple
    Channels | Similarity | Reuse |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| (de Moura 等，[2019](#bib.bib40)) | 重复 | 激活图 | 多通道 | 相似性 | 重用 |'
- en: '| (Chen et al., [2019b](#bib.bib19)) | Repeated | Activation Map | Single Channel
    | Learning | Value Approximation, Reuse |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| (Chen 等，[2019b](#bib.bib19)) | 重复 | 激活图 | 单通道 | 学习 | 值近似、重用 |'
- en: '| (Figurnov et al., [2016](#bib.bib50)) | Irrelevant | Neuron | Multiple Channels
    | Implicit Detection, Learning | Skipping |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| (Figurnov 等，[2016](#bib.bib50)) | 不相关 | 神经元 | 多通道 | 隐式检测、学习 | 跳过 |'
- en: '| (Akhlaghi et al., [2018](#bib.bib3); Hu et al., [2016](#bib.bib74); Shomron
    et al., [2020](#bib.bib136)) | Irrelevant | Neuron | Multiple Channels | Heuristic
    | Skipping |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| (Akhlaghi 等，[2018](#bib.bib3)；Hu 等，[2016](#bib.bib74)；Shomron 等，[2020](#bib.bib136))
    | 不相关 | 神经元 | 多通道 | 启发式 | 跳过 |'
- en: '| (Chen et al., [2019d](#bib.bib23); Suzuki et al., [2020](#bib.bib144)) |
    Irrelevant | Neuron | Multiple Channels | Learning | Skipping |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| (Chen 等，[2019d](#bib.bib23)；Suzuki 等，[2020](#bib.bib144)) | 不相关 | 神经元 | 多通道
    | 学习 | 跳过 |'
- en: '| (Gao et al., [2019](#bib.bib55)) | Over-detailed | Activation Map | Multiple
    Channel | Learning | Skipping |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| (Gao 等，[2019](#bib.bib55)) | 过于详细 | 激活图 | 多通道 | 学习 | 跳过 |'
- en: '| (Ibrokhimov et al., [2020](#bib.bib79)) | Irrelevant | Neuron | Multiple
    Channels | Heuristic | Skipping (Subset Selection) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| (Ibrokhimov 等，[2020](#bib.bib79)) | 不相关 | 神经元 | 多通道 | 启发式 | 跳过（子集选择） |'
- en: '| (Lee and Nirjon, [2020](#bib.bib99)) | Irrelevant | Neuron | Cross-Layer
    | Heuristic | Skipping (Subset Selection) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| (Lee 和 Nirjon，[2020](#bib.bib99)) | 不相关 | 神经元 | 跨层 | 启发式 | 跳过（子集选择） |'
- en: '| (Huang and Wang, [2018](#bib.bib76)) | Irrelevant | Neuron | Multiple Channels,
    Cross-Layer | Learning | Skipping (Subset Selection) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| (Huang 和 Wang，[2018](#bib.bib76)) | 不相关 | 神经元 | 多通道、跨层 | 学习 | 跳过（子集选择） |'
- en: '| (Chitsaz et al., [2020](#bib.bib31); Chen et al., [2020a](#bib.bib21), [2019a](#bib.bib25))
    | Over-detailed | Activation Map | Multiple Channels | Projection | Value Approximation
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| (Chitsaz 等，[2020](#bib.bib31)；Chen 等，[2020a](#bib.bib21)，[2019a](#bib.bib25))
    | 过于详细 | 激活图 | 多通道 | 投影 | 值近似 |'
- en: '| (Chen et al., [2020b](#bib.bib28)) | Over-detailed | Activation Map | Multiple
    Channels | Implicit Detection | Skipping (Ad-hoc Sampling) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| (Chen et al., [2020b](#bib.bib28)) | 过于详细 | 激活图 | 多通道 | 隐式检测 | 跳过（临时采样） |'
- en: '| (Gao et al., [2018](#bib.bib53); Li et al., [2019b](#bib.bib101)) | Irrelevant
    | Patch/Tile | Multiple Channels | Learning | Skipping (Subset Selection) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| (Gao et al., [2018](#bib.bib53); Li et al., [2019b](#bib.bib101)) | 不相关 |
    区块/瓦片 | 多通道 | 学习 | 跳过（子集选择） |'
- en: '| Video | (Kang et al., [2017a](#bib.bib85)) | Irrelevant | Activation Map
    | Temporal Sequence | Similarity | Skipping |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 视频 | (Kang et al., [2017a](#bib.bib85)) | 不相关 | 激活图 | 时间序列 | 相似性 | 跳过 |'
- en: '| (Cavigelli et al., [2017](#bib.bib17); Cavigelli and Benini, [2019](#bib.bib16))
    | Irrelevant | Neuron | Temporal Sequence | Similarity | Skipping |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| (Cavigelli et al., [2017](#bib.bib17); Cavigelli and Benini, [2019](#bib.bib16))
    | 不相关 | 神经元 | 时间序列 | 相似性 | 跳过 |'
- en: '| (Chin et al., [2019](#bib.bib30)) | Over-detailed | Patch/Tile | Multiple
    Channels | Learning | Value Approximation |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| (Chin et al., [2019](#bib.bib30)) | 过于详细 | 区块/瓦片 | 多通道 | 学习 | 值近似 |'
- en: '| (Zhang et al., [2017](#bib.bib163)) | Irrelevant | Patch/Tile | Multiple
    Channels | Implicit Detection | Skipping (Subset Selection) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| (Zhang et al., [2017](#bib.bib163)) | 不相关 | 区块/瓦片 | 多通道 | 隐式检测 | 跳过（子集选择）
    |'
- en: '| (Mao et al., [2019](#bib.bib113)) | Repeated | Patch/Tile | Temporal Sequence
    | Implicit Detection | Skipping (Subset Selection) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| (Mao et al., [2019](#bib.bib113)) | 重复 | 区块/瓦片 | 时间序列 | 隐式检测 | 跳过（子集选择） |'
- en: '| (Yeung et al., [2016](#bib.bib161); Alwassel et al., [2018](#bib.bib5); Wu
    et al., [2019b](#bib.bib157), [a](#bib.bib155); Korbar et al., [2019](#bib.bib95))
    | Over-detailed | Activation Map | Temporal Sequence | Learning | Skipping (Subset
    Selection) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| (Yeung et al., [2016](#bib.bib161); Alwassel et al., [2018](#bib.bib5); Wu
    et al., [2019b](#bib.bib157), [a](#bib.bib155); Korbar et al., [2019](#bib.bib95))
    | 过于详细 | 激活图 | 时间序列 | 学习 | 跳过（子集选择） |'
- en: '| (Su and Grauman, [2016](#bib.bib143)) | Over-detailed | Batch or Sequence
    | Temporal Sequence | Learning | Skipping (Subset Selection) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| (Su and Grauman, [2016](#bib.bib143)) | 过于详细 | 批次或序列 | 时间序列 | 学习 | 跳过（子集选择）
    |'
- en: '| (Zhu et al., [2017b](#bib.bib170), [a](#bib.bib169), [2018](#bib.bib168))
    | Repeated | Activation Map | Temporal Sequence | Implicit Detection | Reuse (Temporal
    Propagation) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| (Zhu et al., [2017b](#bib.bib170), [a](#bib.bib169), [2018](#bib.bib168))
    | 重复 | 激活图 | 时间序列 | 隐式检测 | 重用（时间传播） |'
- en: '| (Kang et al., [2017b](#bib.bib87), [2018](#bib.bib88)) | Repeated | Activation
    Map | Temporal Sequence | Implicit Detection | Reuse (Temporal Propagation) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| (Kang et al., [2017b](#bib.bib87), [2018](#bib.bib88)) | 重复 | 激活图 | 时间序列
    | 隐式检测 | 重用（时间传播） |'
- en: '| (Chen et al., [2018](#bib.bib22)) | Repeated | Activation Map | Temporal
    Sequence | Implicit Detection | Reuse (Temporal Propagation) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| (Chen et al., [2018](#bib.bib22)) | 重复 | 激活图 | 时间序列 | 隐式检测 | 重用（时间传播） |'
- en: '| (Zhu and Liu, [2018](#bib.bib166)) | Repeated | Activation Map | Temporal
    Sequence | Implicit Detection | Reuse (Temporal Propagation) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| (Zhu and Liu, [2018](#bib.bib166)) | 重复 | 激活图 | 时间序列 | 隐式检测 | 重用（时间传播） |'
- en: '| Text | (Liu et al., [2018a](#bib.bib109); Dai et al., [2020](#bib.bib36))
    | Over-detailed | Patch/Tile | Activation Map | Implicit Detection | Skipping
    (Ad-hoc Sampling) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | (Liu et al., [2018a](#bib.bib109); Dai et al., [2020](#bib.bib36)) |
    过于详细 | 区块/瓦片 | 激活图 | 隐式检测 | 跳过（临时采样） |'
- en: '| (Goyal et al., [2020](#bib.bib59)) | Irrelevant | Patch/Tile | Activation
    Map | Learning | Skipping (Subset Selection) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| (Goyal et al., [2020](#bib.bib59)) | 不相关 | 区块/瓦片 | 激活图 | 学习 | 跳过（子集选择） |'
- en: '| (Kitaev et al., [2020](#bib.bib93)) | Repeated | Patch/Tile | Activation
    Map | Similarity | Reuse |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| (Kitaev et al., [2020](#bib.bib93)) | 重复 | 区块/瓦片 | 激活图 | 相似性 | 重用 |'
- en: '| (Dalvi et al., [2020](#bib.bib39)) | Repeated, Irrelevant | Batch or Sequence
    | Temporal Sequence, Cross-Layer | Similarity | Reuse, Skipping (Subset Selection)
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| (Dalvi et al., [2020](#bib.bib39)) | 重复，不相关 | 批次或序列 | 时间序列，跨层 | 相似性 | 重用，跳过（子集选择）
    |'
- en: 'In Section [4](#S4 "4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning"), we divide studies on image
    data redundancy based on the types of redundancy they are dealing with, as the
    number of works in the three sub-categories (Repeated Information, Irrelevant
    Information, and Over-detailed Information) are relatively balanced and offers
    comprehensive coverage of the differences among the techniques. We point out the
    discrepancies between the work in each group in other dimensions (e.g., data representations,
    redundancy exploitation techniques) when necessary during the discussion. While
    many image redundancy exploitation techniques could potentially be applied to
    each frame in a video, Section [5](#S5 "5\. Leverage Data Redundancy in DNN for
    Videos ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")
    focuses on the methods specially designed for videos. These efforts extend the
    work in Section [4](#S4 "4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning") by exploring how redundancy
    is leveraged on the patch unit and the temporal dimension. Section [6](#S6 "6\.
    Leverage Data Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning") discusses how data redundancy
    in the text is handled. Most of the work is based on Transformer models. In both
    Sections [5](#S5 "5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning") and [6](#S6 "6\. Leverage
    Data Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting Data
    Redundancy for Optimization of Deep Learning"), the primary dimension we use to
    group the various explorations is how data redundancy is exploited.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[4](#S4 "4\. 利用图像数据中的冗余 ‣ 调查：利用数据冗余优化深度学习")节中，我们根据处理的冗余类型将图像数据冗余的研究进行了划分，因为三种子类别（重复信息、无关信息和过度详细信息）的研究数量相对均衡，并且对技术之间的差异提供了全面的覆盖。在讨论过程中，我们在必要时指出了每组工作在其他维度（例如，数据表示、冗余利用技术）上的差异。尽管许多图像冗余利用技术可能适用于视频中的每一帧，第[5](#S5
    "5\. 利用深度神经网络中的数据冗余 ‣ 调查：利用数据冗余优化深度学习")节关注于专为视频设计的方法。这些工作扩展了第[4](#S4 "4\. 利用图像数据中的冗余
    ‣ 调查：利用数据冗余优化深度学习")节的内容，通过探索如何在图像块单元和时间维度上利用冗余。第[6](#S6 "6\. 利用Transformer优化中的数据冗余
    ‣ 调查：利用数据冗余优化深度学习")节讨论了如何处理文本中的数据冗余。大部分工作基于Transformer模型。在第[5](#S5 "5\. 利用深度神经网络中的数据冗余
    ‣ 调查：利用数据冗余优化深度学习")节和第[6](#S6 "6\. 利用Transformer优化中的数据冗余 ‣ 调查：利用数据冗余优化深度学习")节中，我们用来对各种探索进行分组的主要维度是数据冗余的利用方式。
- en: 4\. Leverage Redundancy in Image Data
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 利用图像数据中的冗余
- en: 'Image data are generally presented as three-dimensional pixel values (height,
    width, and channel). The inherent locality within data values provides opportunities
    for advanced optimizations. This section groups the relevant studies at a high
    level based on the types of redundancy they mainly target: repeated information,
    irrelevant information, and over-detailed information. The discussion in each
    kind then divides the techniques to detect and handle data redundancy. Table LABEL:tab:image
    presents these works by their applications (tasks) and reported performance for
    fast reference. The second column (“Paper”) shows the references to the relevant
    papers; the third column (“Code”) shows the link to the source code of the implementation;
    the fourth column (”Key Idea”) briefly summarizes the key idea of each work. The
    fifth column (“Performance”) summarizes the performance gains by the data redundant
    optimization techniques.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据通常以三维像素值（高度、宽度和通道）呈现。数据值内在的局部性提供了高级优化的机会。本节根据主要目标的冗余类型对相关研究进行了高层次的分组：重复信息、无关信息和过度详细信息。每种类型的讨论随后将技术分为检测和处理数据冗余的方式。表LABEL:tab:image按应用（任务）和报告的性能呈现这些工作，供快速参考。第二列（“论文”）显示了相关论文的参考文献；第三列（“代码”）显示了实现源代码的链接；第四列（“关键思想”）简要总结了每项工作的关键思想。第五列（“性能”）总结了数据冗余优化技术的性能提升。
- en: 4.1\. Repeated Information
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 重复信息
- en: Due to the common coherence of data values in an image, image data often show
    a certain degree of data locality along the spatial dimensions. The most common
    optimizations take advantage of the value similarities among pixels. The initial
    motivation is to minimize the storage space of CNN’s intermediate activation maps
    on a resource-constrained platform and reduce the data movements between CPU and
    GPU and between processing units and memory. The optimizations also reduce the
    number of computations and improve computation speeds.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像数据值的常见一致性，图像数据通常在空间维度上显示出一定程度的数据局部性。最常见的优化方法利用像素之间的值相似性。最初的动机是最小化CNN在资源受限平台上的中间激活图的存储空间，并减少CPU与GPU之间、处理单元与内存之间的数据移动。这些优化还减少了计算次数并提高了计算速度。
- en: Table (or cache) lookup (Park and Kim, [2019](#bib.bib120); de Moura et al.,
    [2019](#bib.bib40); Mocerino et al., [2019](#bib.bib115); Razlighi et al., [2017](#bib.bib126);
    Ma et al., [2020](#bib.bib112); Hegde et al., [2018b](#bib.bib69)) and clustering (Kim
    et al., [2020b](#bib.bib90); Ning and Shen, [2019](#bib.bib117)) are the most
    typical approaches taken to discover and leverage repeated information in images.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表（或缓存）查找 (Park and Kim, [2019](#bib.bib120); de Moura et al., [2019](#bib.bib40);
    Mocerino et al., [2019](#bib.bib115); Razlighi et al., [2017](#bib.bib126); Ma
    et al., [2020](#bib.bib112); Hegde et al., [2018b](#bib.bib69))和聚类 (Kim et al.,
    [2020b](#bib.bib90); Ning and Shen, [2019](#bib.bib117))是发现和利用图像中重复信息的最典型方法。
- en: To detect similarity among data points, the various techniques differ in the
    level of pixel units used for the similarity detection, some at the level of bits (Jiao
    et al., [2018](#bib.bib83); Ma et al., [2020](#bib.bib112); Salamat et al., [2018](#bib.bib129)),
    some at the level of pixels (Razlighi et al., [2017](#bib.bib126)), patches (or
    tiles, sub-vectors) (Kim et al., [2020b](#bib.bib90); Ning and Shen, [2019](#bib.bib117);
    Hegde et al., [2018b](#bib.bib69)), or even entire feature maps as a whole (Park
    and Kim, [2019](#bib.bib120); de Moura et al., [2019](#bib.bib40); Mocerino et al.,
    [2019](#bib.bib115); Jiao et al., [2018](#bib.bib83)). Some use clustering with
    predefined distance metrics and thresholds to identify the similarity between
    units, while others use hash functions like locality sensitive hashing (Ning and
    Shen, [2019](#bib.bib117)) or bloom filters (Jiao et al., [2018](#bib.bib83)).
    RNSnet (Salamat et al., [2018](#bib.bib129)) utilizes a unique bit-handling technique
    that maps the binary representation into the Residue Number System (RNS). Each
    binary representation is divided by a modulus set; with the values represented
    by the corresponding modules and reminders, the technique efficiently identifies
    similar values. The method could be regarded as a kind of hashing function.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 检测数据点之间的相似性时，各种技术在相似性检测所用的像素单元级别上有所不同，有的在位级别 (Jiao et al., [2018](#bib.bib83);
    Ma et al., [2020](#bib.bib112); Salamat et al., [2018](#bib.bib129))，有的在像素级别 (Razlighi
    et al., [2017](#bib.bib126))，有的在补丁（或瓦片，子向量）级别 (Kim et al., [2020b](#bib.bib90);
    Ning and Shen, [2019](#bib.bib117); Hegde et al., [2018b](#bib.bib69))，甚至是整个特征图的整体 (Park
    and Kim, [2019](#bib.bib120); de Moura et al., [2019](#bib.bib40); Mocerino et al.,
    [2019](#bib.bib115); Jiao et al., [2018](#bib.bib83))。一些技术使用预定义的距离度量和阈值进行聚类，以识别单元之间的相似性，而另一些则使用哈希函数，如局部敏感哈希 (Ning
    and Shen, [2019](#bib.bib117))或布隆过滤器 (Jiao et al., [2018](#bib.bib83))。RNSnet (Salamat
    et al., [2018](#bib.bib129))利用一种独特的位处理技术，将二进制表示映射到剩余数系统（RNS）。每个二进制表示被除以一组模数，通过相应的模数和余数表示的值，该技术高效地识别相似的值。这种方法可以视为一种哈希函数。
- en: After gathering similar data points, each bucket (or cluster) of the data is
    represented by usually one or a few data points in the bucket. The determination
    of such representatives depends on the tasks (e.g., image classification, object
    detection) that the DNN performs. For a relatively simple image classification
    task, if the number of buckets is enough to showcase the overall discrepancy between
    predicted classes without hurting the training or inference accuracy, the representative
    can be simply an average of data values in that bucket. To the extreme, when the
    dataset contains a significant proportion of identical data samples, a representative
    is sometimes set as an arbitrary data point taken from a bucket. The representative
    is used in place of the other issues in the bucket during the CNN inference or
    training, such that similar computations can be avoided.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集了相似的数据点后，每个数据桶（或簇）通常由一个或几个桶中的数据点来表示。这样的代表性点的确定依赖于 DNN 执行的任务（例如，图像分类、对象检测）。对于相对简单的图像分类任务，如果桶的数量足够展示预测类别之间的整体差异而不影响训练或推理的准确性，那么代表性点可以仅仅是该桶中数据值的平均值。在极端情况下，当数据集包含大量相同的数据样本时，代表性点有时会设置为从桶中随机取出的数据点。代表性点在
    CNN 推理或训练中代替了桶中的其他问题，以避免类似的计算。
- en: 'Representative work in this category is the Deep Reuse work by Ning and others (Ning
    and Shen, [2019](#bib.bib117); Ning et al., [2019](#bib.bib116)). As shown in
    Fig. [2](#S4.F2 "Figure 2 ‣ 4.1\. Repeated Information ‣ 4\. Leverage Redundancy
    in Image Data ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning"),
    the approach divides input images or activation maps into sub-vectors during runtime.
    Based on their similarities, it clusters them into several buckets through efficient
    online LSH-based clustering (Gionis et al., [1999](#bib.bib57)), with each cluster
    represented by its centroid. Besides handling repeated information in a 2D scope,
    Deep Reuse also addresses the redundancy in data batches. Based on the clustering
    results, Deep Reuse reduces a convolution to multiplications between a small matrix
    formed by the clusters centroids and the filter matrix. The results are used to
    reconstruct the full activation map through data duplications. Even with the online
    clustering overhead, the work shows that the reuse of computation results yields
    an overall $1.77-2\times$ speedup (up to $4.3\times$ layer-wise) with negligible
    accuracy loss and without model retraining. The method yields more speedups when
    applied to CNN training in an adaptive manner (Ning et al., [2019](#bib.bib116)).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类别中的代表性工作是 Ning 等人的深度重用工作（Ning 和 Shen，[2019](#bib.bib117)；Ning 等，[2019](#bib.bib116)）。如图
    [2](#S4.F2 "图 2 ‣ 4.1\. 重复信息 ‣ 4\. 利用图像数据中的冗余 ‣ 调查：利用数据冗余优化深度学习") 所示，该方法在运行时将输入图像或激活图分割为子向量。根据它们的相似性，通过高效的在线
    LSH 基于聚类（Gionis 等，[1999](#bib.bib57)）将其聚类到几个桶中，每个簇由其质心表示。除了处理 2D 范围内的重复信息外，深度重用还解决了数据批次中的冗余问题。基于聚类结果，深度重用将卷积操作减少为由簇质心形成的小矩阵与滤波器矩阵之间的乘法。结果用于通过数据重复重建完整的激活图。即使考虑到在线聚类的开销，该工作表明，计算结果的重用总体上实现了
    $1.77-2\times$ 的加速（在层级上最高可达 $4.3\times$），且准确性损失微乎其微，无需模型重新训练。该方法在自适应应用于 CNN 训练时能获得更多的加速（Ning
    等，[2019](#bib.bib116)）。
- en: '![Refer to caption](img/ec930134314b79166967a4545bbdee23.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ec930134314b79166967a4545bbdee23.png)'
- en: Figure 2\. Illustration of sub-vector clustering in Deep Reuse. In a convolution
    layer, the activation maps of 2D images are unfolded as $x$ and are multiplied
    by weights $W$ of corresponding filters. With sub-vector clustering, $x$ is first
    divided into sub-vectors and labeled with corresponding cluster ids. In the matrix
    multiplication, only the representative of each cluster is used. Afterward, the
    output activation maps are re-constructed with the representative results of each
    cluster. Figure adapted from reference (Ning and Shen, [2019](#bib.bib117)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 深度重用中的子向量聚类示意图。在卷积层中，2D 图像的激活图被展开为 $x$，并与对应滤波器的权重 $W$ 相乘。通过子向量聚类，$x$ 首先被划分为子向量，并标记上相应的簇
    ID。在矩阵乘法中，只使用每个簇的代表性点。随后，输出激活图通过每个簇的代表性结果重新构建。图源自参考文献（Ning 和 Shen，[2019](#bib.bib117)）。
- en: A fundamental tradeoff facing the methods in this category is the accuracy and
    the amount of computation reuse. It is determined by the aggressiveness in similarity-based
    clustering. For instance, in the Deep Reuse work, the aggressiveness is determined
    by two hyper-parameters, the length of a sub-vector in an activation map (granularity),
    and then the number of LSH hashing vectors. As the appropriate values of the hyperparameters
    are data and model dependent, the previous work (Ning and Shen, [2019](#bib.bib117))
    employs some offline tuning process to select the values such that the accuracy
    is not compromised. One of the directions worth some future explorations is to
    adapt the clustering method to data and models. For instance, in Deep Reuse, the
    hashing vectors are randomly generated. If they can be learned from data for a
    given CNN, more aggressive clustering might be possible without causing accuracy
    losses.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别方法面临的基本权衡是准确性和计算重用量。这取决于基于相似性的聚类的激进程度。例如，在Deep Reuse工作中，激进程度由两个超参数决定：激活图中子向量的长度（粒度），以及LSH哈希向量的数量。由于超参数的适当值依赖于数据和模型，之前的工作（宁和申，
    [2019](#bib.bib117)）采用一些离线调整过程来选择这些值，以确保准确性不受影响。值得未来探索的一个方向是将聚类方法适应数据和模型。例如，在Deep
    Reuse中，哈希向量是随机生成的。如果它们可以从给定CNN的数据中学习到，可能会在不造成准确性损失的情况下实现更激进的聚类。
- en: 4.2\. Irrelevant Information
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2. 无关信息
- en: 4.2.1\. Feature Pruning
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1. 特征修剪
- en: Image data or intermediate activation maps may have unnecessary information
    as well. For example, most well-trained CNNs can still maintain their prediction
    accuracy when some pixels in the activation maps are removed. Subsequently, the
    skipped portion of the data reduces the amount of computation and thus offers
    acceleration during inference or training. We introduce some details about how
    the optimizations in this line identify these irrelevant values at different scales
    and get rid of them while keeping the overall accuracy.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据或中间激活图也可能包含不必要的信息。例如，大多数训练良好的CNN在激活图中的某些像素被移除时仍能保持其预测准确性。随后，跳过的数据部分减少了计算量，从而在推断或训练期间提供了加速。我们介绍了一些关于这一系列优化如何在不同尺度下识别这些无关值并去除它们，同时保持整体准确性的细节。
- en: Hu et al. (Hu et al., [2016](#bib.bib74)) identify many zeros in the activation
    of CNNs. They quantify the neurons to be pruned based on the Average Percentage
    of Zeros (APoZ). APoZ is calculated on a per-layer basis to measure the probability
    each fixed position appears to be zero-valued in validation data samples. With
    the same motivation, Akhlaghi et al. (Akhlaghi et al., [2018](#bib.bib3)) and
    Piyasena et al. (Piyasena et al., [2019](#bib.bib124)) propose detecting zero
    pixels that may occur after the ReLU activations. It is by identifying the negative
    activations earlier using a low-cost approximation scheme since the negative values
    turn into zeros after going through the ReLU layers. In particular, Akhlaghi et
    al. (Akhlaghi et al., [2018](#bib.bib3)) construct SnaPEA with a runtime technique
    to speculate on the convolution outputs’ sign before going through negative weights.
    The aggressiveness of the speculation is controlled by thresholding on parameters
    and the predefined associated number of MAC operations that can maintain accuracy.
    The parameters are tuned offline within the search space. Piyasena et al. (Piyasena
    et al., [2019](#bib.bib124)) augment the CNN implementation with a lightweight
    approximation scheme that consists of ApproxConv and ReLupred stages. The two
    operations work sequentially to handle the sign prediction on an approximate convolution
    computation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 胡等人（Hu et al.， [2016](#bib.bib74)）在卷积神经网络（CNN）的激活中识别出许多零。他们基于平均零百分比（APoZ）来量化需要修剪的神经元。APoZ是在每一层基础上计算的，用于测量每个固定位置在验证数据样本中出现零值的概率。出于相同的动机，阿赫拉基等人（Akhlaghi
    et al.， [2018](#bib.bib3)）和皮亚森娜等人（Piyasena et al.， [2019](#bib.bib124)）提出检测在ReLU激活之后可能出现的零像素。这是通过使用低成本的近似方案提前识别负激活来实现的，因为负值在经过ReLU层后会变成零。特别是，阿赫拉基等人（Akhlaghi
    et al.， [2018](#bib.bib3)）构建了SnaPEA，采用一种运行时技术在经过负权重之前推测卷积输出的符号。推测的激进程度通过在参数和预定义的相关MAC操作数上进行阈值控制来调整，以保持准确性。这些参数在搜索空间内进行离线调整。皮亚森娜等人（Piyasena
    et al.， [2019](#bib.bib124)）通过一种轻量级近似方案增强了CNN的实现，该方案包括ApproxConv和ReLupred阶段。这两个操作顺序执行，以处理近似卷积计算中的符号预测。
- en: 'A more direct way to remove irrelevant information is to assume the existence
    of redundancy based on domain knowledge and prune the data or activation maps
    at different scales. Figurnov et al. (Figurnov et al., [2016](#bib.bib50)) prunes
    the data at pixel and patch level. The proposed mechanism inherits the idea of
    the loop perforation technique in code optimization to skip certain spatial positions
    in images for CNN classification inference. The selection of pixels to prune is
    random within a predefined limited number of points. They exploit four input-independent
    perforation masks: Uniform mask, Grid mask, Pooling Structure mask, and Impact
    mask, as shown in Fig. [3](#S4.F3 "Figure 3 ‣ 4.2.1\. Feature Pruning ‣ 4.2\.
    Irrelevant Information ‣ 4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning"). These masks are used to confine
    the random point selected into corresponding patterns. Afterward, perforated CNN
    uses interpolation to reconstruct the output feature maps. The CNN wights are
    retrained to adapt to this optimization as well.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 更直接的去除无关信息的方法是基于领域知识假设冗余的存在，并在不同尺度上修剪数据或激活图。Figurnov等（Figurnov等，[2016](#bib.bib50)）在像素和补丁级别上修剪数据。提出的机制继承了代码优化中循环穿孔技术的思想，以跳过图像中的某些空间位置来进行CNN分类推断。选择修剪的像素是在预定义的有限点数内随机的。他们利用了四种输入无关的穿孔掩码：均匀掩码、网格掩码、池化结构掩码和影响掩码，如图[3](#S4.F3
    "图 3 ‣ 4.2.1\. 特征修剪 ‣ 4.2\. 无关信息 ‣ 4\. 利用图像数据中的冗余 ‣ 调查：利用数据冗余优化深度学习")所示。这些掩码用于将随机选择的点限制为相应的模式。之后，穿孔CNN使用插值来重建输出特征图。CNN权重也会被重新训练以适应这一优化。
- en: '![Refer to caption](img/a7733fbcdbc09c493c33bc2f22efb89b.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a7733fbcdbc09c493c33bc2f22efb89b.png)'
- en: Figure 3\. Input-independent perforation masks in PerforatedCNNs. According
    to the paper, the mass layout is for the AlexNet Conv2 layer. (a) Uniform perforation
    mask is $N$ points chosen randomly without replacement from the set $\Omega$.
    (b) Grid perforation mask is a set of points $I={a(1),...,a(Kx)}\times{b(1),...,b(Ky)}$
    the values of a(i), b(i) using the pseudorandom integer sequence generation scheme (Graham,
    [2014](#bib.bib60)). (c) Pooling structure mask exploits the structure of the
    overlaps of pooling operators. Denote by $A(x,y)$ the number of times the convolutional
    layer output is used in the pooling operators. The pooling structure mask is obtained
    by picking top-N positions with the highest values of $A(x,y)$. (d) Impact mask
    estimates the impact of perforation of each position on the CNN loss function
    and then removes the least important positions. When activation maps are computed,
    the coordinates of black pixels are skipped. Figures and description adapted from
    reference (Figurnov et al., [2016](#bib.bib50)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 在PerforatedCNNs中输入无关的穿孔掩码。根据论文，质量布局适用于AlexNet Conv2层。(a) 均匀穿孔掩码是从集合$\Omega$中随机选择的$N$个点，不放回。(b)
    网格穿孔掩码是一组点$I={a(1),...,a(Kx)}\times{b(1),...,b(Ky)}$，其中a(i)，b(i)使用伪随机整数序列生成方案（Graham，[2014](#bib.bib60)）。(c)
    池化结构掩码利用池化操作符重叠的结构。用$A(x,y)$表示卷积层输出在池化操作符中的使用次数。池化结构掩码通过选择$A(x,y)$值最高的前N个位置来获得。(d)
    影响掩码估计每个位置穿孔对CNN损失函数的影响，然后移除最不重要的位置。当计算激活图时，跳过黑色像素的坐标。图及描述改编自参考文献（Figurnov等，[2016](#bib.bib50)）。
- en: 'Aside from the pruning technique within the scope of a single activation map,
    irrelevant information can be addressed across the channel dimensions by, for
    instance, removing a set of outside 2D feature maps. Specifically, Gao et al. (Gao
    et al., [2019](#bib.bib55)) apply feature boosting and suppression by formulating
    a channel saliency predictor. The predictor serves as an indicator to skip execution
    at some feature maps at runtime. Chakraborty et al. (Chakraborty et al., [2019](#bib.bib18))
    show that in a simple handwritten digit recognition task, directly discarding
    redundant feature maps randomly before the full connection does not affect the
    prediction accuracy much. On the same axis, Singh et al. (Singh et al., [2019](#bib.bib141))
    conduct a comprehensive analysis on pruning redundant activation maps in 1D CNN,
    SoundNet (Aytar et al., [2016](#bib.bib6)), at a per map level. The pruned feature
    map selection is based on the following measurements: ANOVA-based method (Penny
    and Henson, [2006](#bib.bib121)), Entropy-based (DE) method (Pérez-Cruz, [2009](#bib.bib122)),
    Cosine-similarity (CS) based method, and a greedy algorithm for selection of feature
    maps based on KL divergence. Gaikwad et al. (Gaikwad and El-Sharkawy, [2019](#bib.bib51))
    apply the L2 norm to decide feature map importance in pruning feature maps of
    SqueezeNet (Iandola et al., [2016](#bib.bib78)). Although the technique is not
    for image data, the selection criteria are worthy of being noted under the context
    of irrelevancy elimination. Furthermore, multiple granularities of redundancy
    can be handled at the same time. For example, Yu et al. (Yu et al., [2020](#bib.bib162))
    combine spatial and channel-wise neuron pruning on activation maps. The criteria
    of what to be pruned are based on the attention mechanism. The attention mechanism
    is trained with targeted dropout, enhancing inference time robustness without
    accuracy loss after pruning.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在单个激活图的范围内应用剪枝技术外，还可以通过例如移除一组外部二维特征图来处理跨通道维度的无关信息。具体来说，Gao 等人（Gao et al.,
    [2019](#bib.bib55)）通过制定通道显著性预测器来应用特征增强和抑制。该预测器作为指示器，指导在运行时跳过某些特征图的执行。Chakraborty
    等人（Chakraborty et al., [2019](#bib.bib18)）表明，在一个简单的手写数字识别任务中，直接在全连接之前随机丢弃冗余特征图对预测准确性影响不大。在同一方向上，Singh
    等人（Singh et al., [2019](#bib.bib141)）对在一维卷积神经网络（1D CNN）、SoundNet（Aytar et al.,
    [2016](#bib.bib6)）中剪枝冗余激活图进行了全面分析。剪枝特征图的选择基于以下测量：基于 ANOVA 的方法（Penny and Henson,
    [2006](#bib.bib121)）、基于熵（DE）的方法（Pérez-Cruz, [2009](#bib.bib122)）、基于余弦相似度（CS）的方法以及基于
    KL 散度的贪婪算法。Gaikwad 等人（Gaikwad and El-Sharkawy, [2019](#bib.bib51)）使用 L2 范数来决定在
    SqueezeNet（Iandola et al., [2016](#bib.bib78)）中剪枝特征图的重要性。尽管该技术不适用于图像数据，但在无关性消除的背景下，选择标准仍值得关注。此外，可以同时处理多种冗余粒度。例如，Yu
    等人（Yu et al., [2020](#bib.bib162)）在激活图上结合了空间和通道级的神经元剪枝。剪枝标准基于注意力机制。注意力机制通过有针对性的
    dropout 进行训练，增强了推理时间的鲁棒性，同时在剪枝后不会造成准确性损失。
- en: Moreover, the optimizations can be classified by whether the irrelevancy is
    subject to the input datasets or not. Namely, they are input-dependent or input-independent.
    Input-dependent irrelevant information removal can be achieved by detecting data
    irrelevancy based on statistical (Hu et al., [2016](#bib.bib74); Singh et al.,
    [2019](#bib.bib141); Gao et al., [2019](#bib.bib55)) or learning-based heuristic (Akhlaghi
    et al., [2018](#bib.bib3); Yu et al., [2020](#bib.bib162)), or estimation of values (Piyasena
    et al., [2019](#bib.bib124)). On the contrary, input-independent approaches (Figurnov
    et al., [2016](#bib.bib50); Chakraborty et al., [2019](#bib.bib18)) get rid of
    those irrelevant values by implicitly assuming them to appear randomly with a
    limit threshold in structural or non-structural manners.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，优化可以根据无关性是否依赖于输入数据集来进行分类，即输入依赖型或输入独立型。输入依赖型无关信息去除可以通过基于统计（Hu et al., [2016](#bib.bib74)；Singh
    et al., [2019](#bib.bib141)；Gao et al., [2019](#bib.bib55)）或基于学习的启发式方法（Akhlaghi
    et al., [2018](#bib.bib3)；Yu et al., [2020](#bib.bib162)），或通过值估计（Piyasena et al.,
    [2019](#bib.bib124)）来实现。相反，输入独立型方法（Figurnov et al., [2016](#bib.bib50)；Chakraborty
    et al., [2019](#bib.bib18)）通过隐式假设这些无关值以结构性或非结构性的方式随机出现，来去除这些无关值。
- en: 4.2.2\. Neuron or Feature Subset Selection
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2. 神经元或特征子集选择
- en: Another granularity used in feature map pruning is the selection of neurons
    (or activation). It could be regarded as a feature selection process at the value
    granularity. The neurons not chosen can be interpreted as ones being pruned. A
    subset of activations is determined based on specific criteria in neuron selection.
    Lee et al. (Lee and Nirjon, [2020](#bib.bib99)) apply ranking on the neurons of
    DNNs based on their contribution to the inference accuracy. Huang et al. (Huang
    and Wang, [2018](#bib.bib76)) add a sparsity regularization on neurons to force
    some of them to become zeros and guide the selection of more informative neurons
    without further tuning. Besides, Ibrokhimov et al. (Ibrokhimov et al., [2020](#bib.bib79))
    propose the technique that selects neurons based on the magnitude of their average
    activations and keeps only the exact amount of the most minor activated neurons
    that work well. They report a reduction in the total number of operations and
    corresponding speedups.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 特征图剪枝中使用的另一种粒度是选择神经元（或激活）。这可以看作是一个值粒度的特征选择过程。未选择的神经元可以被解释为被剪枝的神经元。基于神经元选择的特定标准，确定一部分激活。Lee等（Lee和Nirjon，[2020](#bib.bib99)）对DNNs的神经元根据其对推断准确度的贡献进行排名。Huang等（Huang和Wang，[2018](#bib.bib76)）在神经元上增加了稀疏性正则化，强制部分神经元变为零，并指导选择更有信息量的神经元，而无需进一步调整。此外，Ibrokhimov等（Ibrokhimov等，[2020](#bib.bib79)）提出了一种基于神经元平均激活量的大小选择神经元的技术，仅保留有效的最少激活神经元。他们报告了总操作数的减少和相应的速度提升。
- en: 4.2.3\. Dynamic Region of Interest
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3. 动态感兴趣区域
- en: 'In object detection, dynamic zooming in (Gao et al., [2018](#bib.bib53); Li
    et al., [2019b](#bib.bib101)) of a particular region of interest can help reduce
    computations on irrelevant background information. Gao et al. (Gao et al., [2018](#bib.bib53))
    present such a technique. They employ zoom-in accuracy gain regression network
    (R-net) and zoom-in Q function network (Q-net). The R-net learns the correlation
    between coarse and fine detection and predicts the accuracy gain for zooming in
    to a specific region. The Q-net learns via reinforcement learning. It sequentially
    selects the optimal zoom locations and scales them correspondingly by analyzing
    the new output of R-net and its history. In the inference phase, a down-sampled
    image is input to the R-net and predicts the accuracy gain. The gain serves as
    an indicator for the Q-net to select the target on regions to concentrate sequentially.
    The workflow is illustrated in Fig. [4](#S4.F4 "Figure 4 ‣ 4.2.3\. Dynamic Region
    of Interest ‣ 4.2\. Irrelevant Information ‣ 4\. Leverage Redundancy in Image
    Data ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning").
    The mechanism reduces the total pixels in computation by over $50\%$ and reduces
    the average detection time by $25\%$ on the Caltech Pedestrian Detection dataset.
    It also reduces the pixels by $70\%$ and obtains over $50\%$ detection time reduction
    on the YFCC100M dataset.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标检测中，动态缩放（Gao等，[2018](#bib.bib53)；Li等，[2019b](#bib.bib101)）特定感兴趣区域可以帮助减少对无关背景信息的计算。Gao等（Gao等，[2018](#bib.bib53)）提出了这种技术。他们使用了缩放准确度增益回归网络（R-net）和缩放Q函数网络（Q-net）。R-net学习粗检测与细检测之间的相关性，并预测缩放到特定区域的准确度增益。Q-net通过强化学习进行学习。它通过分析R-net的新输出及其历史，顺序选择最佳的缩放位置并相应地调整它们。在推断阶段，向R-net输入下采样图像并预测准确度增益。该增益作为Q-net选择要集中关注区域的指标。工作流程如图[4](#S4.F4
    "图 4 ‣ 4.2.3. 动态感兴趣区域 ‣ 4.2. 无关信息 ‣ 4. 利用图像数据的冗余 ‣ 调查：利用数据冗余优化深度学习")所示。该机制将计算中的总像素减少了超过$50\%$，并在Caltech行人检测数据集上减少了平均检测时间的$25\%$。在YFCC100M数据集上，它还将像素减少了$70\%$，检测时间减少了超过$50\%$。
- en: '![Refer to caption](img/41febdae49e67e56a841562a1790087b.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/41febdae49e67e56a841562a1790087b.png)'
- en: Figure 4\. The workflow of R-net and Q-net. Given a down-sampled image as input,
    the R-net generates an initial accuracy gain (AG) map indicating the potential
    zoom-in accuracy gain of different regions (initial state). The Q-net is applied
    iteratively on the AG map to select regions. Once a region is selected, the AG
    map will be updated to reflect the history of actions. Two parallel pipelines
    are used for the Q-net, each of which outputs an action-reward map that corresponds
    to selecting zoom-in regions with a specific size. The map’s value indicates the
    likelihood that the action will increase accuracy at a low cost. Action rewards
    from all maps are considered to select the optimal zoom-in region at each iteration.
    The notation $128\times 15\times 20:(7,10)$ means 128 convolution kernels with
    size $15\times 20$, and stride of 7/10 in height/width. Each grid cell in the
    output maps is given a unique color, and a bounding box of the same color is drawn
    on the image to denote the corresponding zoom region size and location. Figure
    and description adapted from reference (Gao et al., [2018](#bib.bib53)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. R-net 和 Q-net 的工作流程。给定一个降采样图像作为输入，R-net 生成一个初始准确度增益（AG）图，指示不同区域的潜在放大准确度增益（初始状态）。Q-net
    会在 AG 图上迭代应用以选择区域。一旦选择了一个区域，AG 图会更新以反映动作的历史。Q-net 使用两个并行的流程，每个流程输出一个动作-奖励图，对应于选择具有特定大小的放大区域。该图的值表示该动作在低成本下增加准确度的可能性。所有图中的动作奖励都会被考虑，以在每次迭代中选择最优的放大区域。符号
    $128\times 15\times 20:(7,10)$ 表示128个大小为 $15\times 20$ 的卷积核，在高度/宽度方向的步幅为 7/10。输出图中的每个网格单元都有一个唯一的颜色，并且在图像上绘制一个相同颜色的边界框，以标记相应的放大区域的大小和位置。图和描述改编自参考文献（Gao
    等，[2018](#bib.bib53)）。
- en: 4.3\. Over-detailed Information
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 过于详细的信息
- en: 'Yet another direction in image data redundancy exploitation is eliminating
    over-detailed information, represented by adaption to variant image resolutions,
    especially resolution lowering. Chen et al. (Chen et al., [2019a](#bib.bib25))
    propose Octave convolution (OctConv) (Fig. [5](#S4.F5 "Figure 5 ‣ 4.3\. Over-detailed
    Information ‣ 4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting Data
    Redundancy for Optimization of Deep Learning")) to leverage the spatial redundancy
    and factorize the activation maps into high and low frequency (resolution) groups
    to save both memory and computation cost. The operations at each layer enable
    the information to be exchanged within both groups during computation. It is implemented
    as a portable plug-and-play convolution unit and can be applied together with
    a compressed model or optimization that reduces channel-wise redundancy. An OctConv-equipped
    ResNet-152 can achieve $82.9\%$ top-1 classification accuracy on ImageNet with
    $22.2$ GFLOPs.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据冗余利用的另一个方向是消除过于详细的信息，通过适应不同的图像分辨率，特别是降低分辨率。Chen 等（Chen 等，[2019a](#bib.bib25)）提出了
    Octave 卷积（OctConv）（图 [5](#S4.F5 "图 5 ‣ 4.3\. 过于详细的信息 ‣ 4\. 利用图像数据的冗余 ‣ 调查：优化深度学习的数据冗余利用")），以利用空间冗余，并将激活图分解为高频和低频（分辨率）组，以节省内存和计算成本。每一层的操作使信息在计算过程中能够在两个组之间交换。它被实现为一个便携的即插即用卷积单元，并可以与压缩模型或优化方法一起应用，这些方法减少了通道冗余。配备
    OctConv 的 ResNet-152 可以在 ImageNet 上实现 $82.9\%$ 的 top-1 分类准确度，并且计算量为 $22.2$ GFLOPs。
- en: '![Refer to caption](img/9114ead39ec0a042f316876409901a4b.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9114ead39ec0a042f316876409901a4b.png)'
- en: Figure 5\. Illustration of Octave Conv in the factorization of high and low
    spatial frequency. (a) Motivation. The spatial frequency model for vision (Campbell
    and Robson, [1968](#bib.bib12); De Valois and De Valois, [1980](#bib.bib41)) shows
    that raw images can be decomposed into a low and a high spatial frequency part.
    (b) The output maps of a convolution layer can also be factorized and grouped
    by their spatial frequency. (c) The proposed multifrequency feature representation
    stores the smoothly changing, low-frequency maps in a low-resolution tensor to
    reduce spatial redundancy. (d) The proposed Octave Convolution operates directly
    on this representation. It updates the information for each group and further
    enables information exchange between groups. Figures and descriptions adapted
    from reference (Chen et al., [2019a](#bib.bib25)).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 高低空间频率因子分解中的 Octave Conv 示意图。 (a) 动机。视觉的空间频率模型（Campbell 和 Robson，[1968](#bib.bib12)；De
    Valois 和 De Valois，[1980](#bib.bib41)）显示，原始图像可以分解为低空间频率和高空间频率部分。 (b) 卷积层的输出图也可以根据其空间频率进行因子分解和分组。
    (c) 提出的多频率特征表示将平滑变化的低频图存储在低分辨率张量中，以减少空间冗余。 (d) 提出的 Octave 卷积直接在这一表示上操作。它更新每个组的信息，并进一步实现组间信息交换。图形和描述改编自参考文献（Chen
    等，[2019a](#bib.bib25)）。
- en: 'ViP proposed by Chen et al. (Chen et al., [2020b](#bib.bib28)) also targets
    the spatial redundancy in feature maps. It takes advantage of virtual pooling,
    as shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.3\. Over-detailed Information ‣ 4\.
    Leverage Redundancy in Image Data ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning"), such that the larger stride is used to obtain a smaller feature
    map and at the same time save convolution computation. To recover the output feature
    map dimension, linear interpolation is applied. ViP delivers $2.1\times$ speedup
    with less than $1.5\%$ accuracy degradation in ImageNet classification on VGG16,
    and $1.8\times$ speedup with $0.025$ mAP degradation in PASCAL VOC object detection
    with Faster-RCNN. ViP also reduces mobile GPU and CPU energy consumption by up
    to $55\%$ and $70\%$, respectively.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chen 等（Chen 等，[2020b](#bib.bib28)）提出的 ViP 也针对特征图中的空间冗余。它利用虚拟池化，如图 [6](#S4.F6
    "Figure 6 ‣ 4.3\. Over-detailed Information ‣ 4\. Leverage Redundancy in Image
    Data ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")
    所示，这样使用更大的步幅来获得更小的特征图，同时节省卷积计算。为了恢复输出特征图的维度，采用线性插值。ViP 在 VGG16 的 ImageNet 分类中实现了
    $2.1\times$ 的加速，且准确率下降不到 $1.5\%$，在 Faster-RCNN 的 PASCAL VOC 目标检测中实现了 $1.8\times$
    的加速，并且 mAP 降低了 $0.025$。ViP 还将移动 GPU 和 CPU 的能耗分别减少了 $55\%$ 和 $70\%$。'
- en: '![Refer to caption](img/c24fd92ac60afcaf6f0f4b3d56ba62d2.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c24fd92ac60afcaf6f0f4b3d56ba62d2.png)'
- en: Figure 6\. Illustration of virtual polling (LinkViz, [2020](#bib.bib105)). By
    using a larger stride, it save computation in conv layers and, to recover the
    output size, it use linear interpolation which is fast to compute. Figure and
    description adapted from reference (Chen et al., [2020b](#bib.bib28)).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 虚拟池化示意图（LinkViz，[2020](#bib.bib105)）。通过使用更大的步幅，它在卷积层中节省计算，并且为了恢复输出尺寸，使用线性插值，这种方法计算速度快。图形和描述改编自参考文献（Chen
    等，[2020b](#bib.bib28)）。
- en: 5\. Leverage Data Redundancy in DNN for Videos
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 在视频的 DNN 中利用数据冗余
- en: Existing DNN frameworks for video analysis are built on well-developed CNNs
    such as VGG (Simonyan and Zisserman, [2015](#bib.bib140)) or ResNet (He et al.,
    [2016](#bib.bib66)) and image-based object detection algorithms such as Faster-RCNN (Ren
    et al., [2015](#bib.bib127)). They treat the video as a continuous input of images.
    Intuitively, the data redundancy in this line of works, especially the duplication
    along the temporal axis, provides much room for performance enhancement. DNN frameworks
    that leverage data redundancy in video data can be categorized based on how they
    exploit redundancy. These techniques fall into the three categories listed in
    our taxonomy (skipping, reuse, and approximation); this section groups them more
    detailed to capture the more complicated differences among the techniques. Table LABEL:tab:video
    shows the works with video data redundancy exploitation by their applications.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的视频分析 DNN 框架建立在成熟的 CNN 上，如 VGG （Simonyan 和 Zisserman，[2015](#bib.bib140)）或
    ResNet （He 等，[2016](#bib.bib66)），以及基于图像的目标检测算法，如 Faster-RCNN （Ren 等，[2015](#bib.bib127)）。它们将视频视作一系列连续的图像。直观来看，这一系列工作的数据冗余，特别是时间轴上的重复，为性能提升提供了大量空间。利用视频数据中数据冗余的
    DNN 框架可以根据它们如何利用冗余进行分类。这些技术分为我们分类法中列出的三类（跳过、重用和近似）；本节对它们进行更详细的分组，以捕捉技术之间更复杂的差异。表 LABEL:tab:video
    展示了按应用对视频数据冗余利用的相关工作。
- en: 5.1\. Pruning
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 剪枝
- en: 'A way to avoid recomputing similar values across video frames is through skipping
    some of them along the temporal dimension (or the batch dimension in 2D CNN).
    Kang et al. propose NoScope (Kang et al., [2017a](#bib.bib85)) to optimize the
    processing of video querying. As shown in Fig. [7](#S5.F7 "Figure 7 ‣ 5.1\. Pruning
    ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning"), they insert a difference detector and a specialized
    model with short-circuit evaluation that allows computation reduction on almost
    identical neighboring frames in a video sequence.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '避免在视频帧间重新计算相似值的一种方法是跳过某些帧（或 2D CNN 中的批处理维度）。Kang 等人提出了 NoScope （Kang 等，[2017a](#bib.bib85)）来优化视频查询的处理。如图 [7](#S5.F7
    "Figure 7 ‣ 5.1\. Pruning ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey:
    Exploiting Data Redundancy for Optimization of Deep Learning") 所示，他们插入了一个差异检测器和一个短路评估的专用模型，使得在几乎相同的相邻帧中实现计算减少。'
- en: '![Refer to caption](img/4cdc162686a83ea8658a0617a0e51d59.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4cdc162686a83ea8658a0617a0e51d59.png)'
- en: Figure 7\. Illustration of the NoScope framework. NoScope is a system for accelerating
    neural network analysis over videos via an inference-optimized model search. Given
    an input video, target object, and reference neural network, NoScope automatically
    searches for and trains a cascade of models—including difference detectors and
    specialized networks—that can reproduce the binarized outputs of the reference
    network with high accuracy—but up to three orders of magnitude faster. Figure
    and description adapted from reference (Kang et al., [2017a](#bib.bib85)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. NoScope 框架示意图。NoScope 是一个通过推理优化的模型搜索来加速神经网络分析的视频系统。给定一个输入视频、目标对象和参考神经网络，NoScope
    自动搜索并训练一个模型级联，包括差异检测器和专用网络，这些模型能够以高精度重现参考网络的二值化输出，但速度快达三倍数量级。图示及描述改编自参考文献（Kang
    等，[2017a](#bib.bib85)）。
- en: 'The redundancy across continuous frames can also be addressed at the pixel
    level. Cavigelli et al. propose CBinfer (Cavigelli et al., [2017](#bib.bib17);
    Cavigelli and Benini, [2019](#bib.bib16)), a change-based evaluation of CNN for
    video data recorded with a static camera, to exploit the spatial-temporal sparsity
    of pixel changes. As shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.2\. Approximate Representation
    ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning"), they modify each convolution layer to include
    an additional change detection mechanism and change indexes extraction before
    the matrix multiplication. The output is updated with both non-changed output
    pixels from the previous frame and changed output pixels calculated at this time.
    CBinfer achieves an average speed-up of $8.6\times$ over a cuDNN baseline on a
    realistic benchmark with a negligible accuracy loss of less than $0.1\%$ and no
    network retraining. The resulting energy efficiency is $10\times$ higher than
    per-frame evaluation and reaches an equivalent of $328$ $GOp/s/W$ on the Nvidia
    Tegra X1 platform.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '连续帧之间的冗余也可以在像素级别上解决。Cavigelli 等人提出了 CBinfer （Cavigelli 等人，[2017](#bib.bib17)；Cavigelli
    和 Benini，[2019](#bib.bib16)），一种基于变化的 CNN 评估方法，旨在利用静态相机记录的视频数据中的空间-时间稀疏性。正如图 [8](#S5.F8
    "Figure 8 ‣ 5.2\. Approximate Representation ‣ 5\. Leverage Data Redundancy in
    DNN for Videos ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")
    所示，他们修改了每个卷积层，以在矩阵乘法之前包含额外的变化检测机制和变化索引提取。输出将同时更新来自前一帧的未变化输出像素和本次计算的变化输出像素。CBinfer
    在一个真实基准上相对于 cuDNN 基线实现了 $8.6\times$ 的平均加速，准确度损失极小，低于 $0.1\%$，且无需网络重新训练。最终的能效比每帧评估高
    $10\times$，在 Nvidia Tegra X1 平台上达到了 $328$ $GOp/s/W$ 的相当水平。'
- en: 5.2\. Approximate Representation
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 近似表示
- en: 'Regional re-scaling is a type of approximate representation. Chin et al. propose
    AdaScale (Chin et al., [2019](#bib.bib30)) that exploits the potential benefit
    bought by adaptive image scaling. They resort to image down-sampling to realize
    performance improvement on both accuracy and speedup for video object detection.
    To accomplish this, AdaScale adopts an object detector to generate the optimal
    scale labels for images carried out by a learning-based method. The generated
    optimal scale is later used to train the scale regressor that dynamically re-scales
    the image. The visualization of the effect is shown in Fig. [9](#S5.F9 "Figure
    9 ‣ 5.2\. Approximate Representation ‣ 5\. Leverage Data Redundancy in DNN for
    Videos ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning").
    When the images are down-sampled, they are supposed to reduce the number of false
    positives introduced by overly detail-oriented messages, which in turn dismiss
    the redundancy. The image scaling increases the number of true positives by turning
    the objects too large to smaller ones for the detector to be more confident. For
    ImageNet VID and mini YouTube-BB datasets, AdaScale demonstrates $1.3\%$ and $2.7\%$
    mAP improvement with $1.6\times$ and $1.8\times$ speedup respectively. The framework
    can also work complementary with video acceleration work (Zhu et al., [2017b](#bib.bib170)),
    of which they experience a speedup gain by $25\%$ and a slight mAP boost on the
    ImageNet VID dataset.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '区域重新缩放是一种近似表示方法。Chin 等人提出了 AdaScale （Chin 等人，[2019](#bib.bib30)），利用自适应图像缩放带来的潜在好处。他们采用图像下采样来提高视频目标检测的准确性和速度。为实现这一目标，AdaScale
    采用对象检测器生成用于图像的最佳缩放标签，该方法基于学习的方法生成的最佳缩放标签。生成的最佳缩放标签随后用于训练缩放回归器，动态地对图像进行重新缩放。效果的可视化如图 [9](#S5.F9
    "Figure 9 ‣ 5.2\. Approximate Representation ‣ 5\. Leverage Data Redundancy in
    DNN for Videos ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")
    所示。当图像被下采样时，它们可以减少由过于细节化的信息引入的假阳性，从而消除冗余。图像缩放通过将对象从过大的尺寸缩小到更小的尺寸，增加了真正阳性的数量，使检测器更加自信。对于
    ImageNet VID 和 mini YouTube-BB 数据集，AdaScale 分别展示了 $1.3\%$ 和 $2.7\%$ 的 mAP 提升，并实现了
    $1.6\times$ 和 $1.8\times$ 的加速。该框架还可以与视频加速工作互补（Zhu 等人，[2017b](#bib.bib170)），在 ImageNet
    VID 数据集上实现了 $25\%$ 的加速增益和轻微的 mAP 提升。'
- en: '![Refer to caption](img/850f08f851748107e94a2093d68734bc.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/850f08f851748107e94a2093d68734bc.png)'
- en: Figure 8\. Processing flow of the change-based convolution algorithm. Custom
    processing kernels are shown in blue, processing steps using available libraries
    are shown in green, variables sharable among layers are shown in yellow, and variables
    to be stored per layer are colored orange. The size and data type of the tensor
    storing the intermediate results is indicated below each variable name. Figure
    and description adapted from reference (Cavigelli et al., [2017](#bib.bib17)).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 基于变化的卷积算法处理流程。自定义处理内核用蓝色显示，使用可用库的处理步骤用绿色显示，各层之间共享的变量用黄色显示，每层需要存储的变量用橙色显示。存储中间结果的张量的大小和数据类型在每个变量名称下方指示。图和描述改编自参考文献 (Cavigelli
    et al., [2017](#bib.bib17))。
- en: '![Refer to caption](img/c37255a553734d364c5bf3481c80a7f3.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c37255a553734d364c5bf3481c80a7f3.png)'
- en: Figure 9\. AdaScale detection result visualization. Examples where down-sampled
    images have better detection results. Blue boxes are the detection results, and
    the numbers are the confidence. The detector is trained on a single scale (pixels
    of the shortest side) of 600\. Column (a) and (c) are tested at scale of 600.
    Column (b) is tested at scale 240, and column (d) is tested at scale 480\. Figures
    and descriptions adapted from reference (Chin et al., [2019](#bib.bib30)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. AdaScale检测结果可视化。下采样图像具有更好检测结果的示例。蓝色框是检测结果，数字是置信度。检测器在单一尺度（最短边的像素）为600上进行训练。列（a）和（c）在600尺度下测试。列（b）在240尺度下测试，列（d）在480尺度下测试。图和描述改编自参考文献 (Chin
    et al., [2019](#bib.bib30))。
- en: 5.3\. Subset Selection
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 子集选择
- en: Subset selection is for obtaining a subgroup of potentially representative video
    frames or corresponding feature maps. These techniques adopt a variety of dimensions
    in preference, from the spatial to the temporal, batch, and streaming.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 子集选择是为了获得潜在的代表性视频帧或相应特征图的子组。这些技术采用各种维度的优先级，从空间到时间、批次和流处理。
- en: 5.3.1\. Selection in the Spatial Dimension
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 空间维度的选择
- en: Patch-of-Interest
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 感兴趣区域
- en: 'Zhang et al. (Zhang et al., [2017](#bib.bib163)) propose Kill Two Bird with
    One Stone to aggregate patch-of-interest (usually the moving object within images)
    for construction of a compact patch composition for video object detection as
    shown in Fig. [10](#S5.F10 "Figure 10 ‣ Patch-of-Interest ‣ 5.3.1\. Selection
    in the Spatial Dimension ‣ 5.3\. Subset Selection ‣ 5\. Leverage Data Redundancy
    in DNN for Videos ‣ Survey: Exploiting Data Redundancy for Optimization of Deep
    Learning"). In the detection steps, the parts of the spatial dimension data patches
    are eliminated to save unnecessary computation. The set of patch-of-interest is
    mapped back by their numbers and location offsets to reconstruct the final result.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang et al. (Zhang et al., [2017](#bib.bib163)) 提出了“杀死两只鸟”的方法来聚合感兴趣区域（通常是图像中的运动物体），以构建用于视频物体检测的紧凑区域组成，如图
    [10](#S5.F10 "图10 ‣ 感兴趣区域 ‣ 5.3.1\. 空间维度的选择 ‣ 5.3\. 子集选择 ‣ 5\. 利用DNN在视频中的数据冗余
    ‣ 调查：利用数据冗余优化深度学习") 所示。在检测步骤中，空间维度数据补丁的部分被去除以节省不必要的计算。感兴趣区域的集合通过其编号和位置偏移量映射回去以重建最终结果。
- en: '![Refer to caption](img/8deaf1050c985fd08232097d1c15b324.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/8deaf1050c985fd08232097d1c15b324.png)'
- en: Figure 10\. Patch-of-Interest composition. (a) The input image. (b) Detected
    patches. (c) The patch composition (left) and sub-frames(right). (d) Detection
    results on sub-frames. (e) Map back and get the final result on the original image.
    Figures and descriptions adapted from reference (Zhang et al., [2017](#bib.bib163)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图10\. 感兴趣区域的组成。(a) 输入图像。(b) 检测到的区域。(c) 区域组成（左）和子帧（右）。(d) 子帧上的检测结果。(e) 反向映射并获得原始图像上的最终结果。图和描述改编自参考文献 (Zhang
    et al., [2017](#bib.bib163))。
- en: Patch Refinement
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 区域精炼
- en: 'Mao et al. (Mao et al., [2019](#bib.bib113)) present CaTDeT (Cascaded Tracking
    Detector), which applies a tracker as assistance for refining intermediate feature
    maps in video object detection. The illustration of the algorithm is shown in
    Fig. [11](#S5.F11 "Figure 11 ‣ Patch Refinement ‣ 5.3.1\. Selection in the Spatial
    Dimension ‣ 5.3\. Subset Selection ‣ 5\. Leverage Data Redundancy in DNN for Videos
    ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning"). The
    tracker provides information on the region of interest based on historical detection.
    It reduces the operation count by $5.1\times$ to $8.7\times$ with a slight delay.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Mao 等人（Mao et al., [2019](#bib.bib113)）提出了 CaTDeT（级联跟踪检测器），该方法在视频目标检测中应用了跟踪器，以帮助细化中间特征图。算法的示意图见图
    [11](#S5.F11 "图 11 ‣ Patch Refinement ‣ 5.3.1. 空间维度中的选择 ‣ 5.3. 子集选择 ‣ 5. 利用 DNN
    中的数据冗余进行视频处理 ‣ 调查：利用数据冗余优化深度学习")。跟踪器根据历史检测提供感兴趣区域的信息。它通过 $5.1\times$ 至 $8.7\times$
    的减少操作量，并且有轻微的延迟。
- en: '![Refer to caption](img/08e3f89fba83ae08a02342e41c3e809b.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/08e3f89fba83ae08a02342e41c3e809b.png)'
- en: 'Figure 11\. Illustration of algorithm flow in CaTDeT. Compare the inference-time
    workflows of the standard Faster R-CNN model and the refinement network. (a) Standard
    Faster R-CNN detector: the RPN takes the feature maps from the feature extractor.
    (b) Faster R-CNN detector for the refinement network: the proposals from the proposal
    network and the tracker instruct the feature extractor only to compute features
    on regions of interest. Regions of interest are a mask of all proposals over the
    frame. Figures and descriptions adapted from reference (Mao et al., [2019](#bib.bib113)).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11. CaTDeT 算法流程示意图。比较标准 Faster R-CNN 模型和精细化网络的推理时间工作流。（a）标准 Faster R-CNN 检测器：RPN
    从特征提取器处获取特征图。（b）用于精细化网络的 Faster R-CNN 检测器：提案网络和跟踪器的提案指示特征提取器仅计算感兴趣区域的特征。感兴趣区域是整个帧的所有提案的掩模。图形和描述改编自参考文献（Mao
    et al., [2019](#bib.bib113)）。
- en: 5.3.2\. Selection in the Temporal Dimension
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2. 时间维度中的选择
- en: Korbar et al. propose SCSampler (Korbar et al., [2019](#bib.bib95)), a lightweight
    model for clip sampling that can invoke recognition on only the most salient clips.
    The paper targets scenarios where the videos are untrimmed and long. They aim
    to reduce the high inference cost when every clip is executed on the clip classifier.
    They apply both visual and audio samplers and combine their saliency for prediction.
    The visual sampler proposes a learning-based clip-level saliency model that provides
    each clip a saliency score between [0,1]. In particular, the model takes the input
    clip features that are fast to compute from the raw clip and have low dimensionality
    (lower than that of the classifier) to analyze each clip very efficiently. The
    clips with top-K saliency scores are extracted with features by the classifier
    and aggregated together to form the representation for prediction. The sampler
    learning phase labels a saliency score of video with $1$ if the action is contained
    in that clip; otherwise, it is labeled as $0$. The approach elevates the accuracy
    of an already state-of-the-art action classifier by $7\%$ and reduces its computational
    cost by more than 15 times.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Korbar 等人提出了 SCSampler（Korbar et al., [2019](#bib.bib95)），这是一个轻量级的剪辑采样模型，可以仅对最显著的剪辑进行识别。该论文针对的是视频未裁剪且较长的场景。他们的目标是减少每个剪辑在剪辑分类器上执行时的高推理成本。他们应用了视觉和音频采样器，并将它们的显著性结合起来进行预测。视觉采样器提出了一种基于学习的剪辑级显著性模型，为每个剪辑提供
    [0,1] 范围内的显著性评分。具体而言，该模型采用从原始剪辑中快速计算得到的输入剪辑特征，这些特征维度较低（低于分类器的维度），以非常高效地分析每个剪辑。显著性评分排名前
    K 的剪辑由分类器提取特征并聚合在一起，形成用于预测的表示。采样器学习阶段对视频进行标注，如果该剪辑中包含动作，则显著性评分标记为 $1$；否则，标记为 $0$。该方法使已经达到最先进水平的动作分类器的准确率提高了
    $7\%$，并将其计算成本降低了超过 15 倍。
- en: On the other hand, subset selection in the temporal dimension of the video can
    sometimes help improve task accuracy. This work typically involves the addition
    of some sophisticated mechanisms into the DNN. As a result, they often slow down
    the DNN rather than speed it up. But their goal is on the accuracy of the outputs
    rather than speed. A class of the work uses recurrent Neural Networks, such as
    Long Short Term Memory networks (LSTM), for capturing long-term dependencies in
    sequence data. They use them to generate compact video representations along with
    CNN activation maps (Yeung et al., [2016](#bib.bib161); Alwassel et al., [2018](#bib.bib5);
    Wu et al., [2019b](#bib.bib157)). They mimic the behavior of humans viewing videos
    that often consist of a glimpse followed by several refinements. In addition,
    some work (Wu et al., [2019a](#bib.bib155)) combines gated recurrent units (GRUs)
    and policy networks in reinforcement learning to adjust video sampling location
    accordingly at each time step.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，视频在时间维度上的子集选择有时可以帮助提高任务的准确性。这项工作通常涉及在深度神经网络（DNN）中添加一些复杂的机制。因此，它们通常会使DNN变慢而不是加快速度。但它们的目标是提高输出的准确性，而不是速度。一类工作使用递归神经网络，如长短期记忆网络（LSTM），来捕捉序列数据中的长期依赖关系。它们利用这些网络生成紧凑的视频表示，以及卷积神经网络（CNN）激活图（Yeung
    et al., [2016](#bib.bib161); Alwassel et al., [2018](#bib.bib5); Wu et al., [2019b](#bib.bib157)）。这些网络模仿人类观看视频的行为，这种行为通常包括一次快速浏览，随后是几个细化步骤。此外，一些工作（Wu
    et al., [2019a](#bib.bib155)）结合了门控递归单元（GRUs）和强化学习中的策略网络，以便在每个时间步调整视频采样位置。
- en: 5.3.3\. Selection in the Streaming Scenario
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3\. 流媒体场景中的选择
- en: 'The selection in a streaming scenario is more challenging. For each selection,
    the mechanisms need to make decisions on future frames based on historical records.
    Su et al. (Su and Grauman, [2016](#bib.bib143)) introduce an active mechanism
    that prioritizes ”which feature to compute when” to make timely action predictions.
    The core idea is to learn a policy that dynamically schedules the sequence of
    features to compute on selected frames of a given test video. The selection procedure
    can be invoked for either batch or streaming dimension. They formulate the problem
    with a Markov decision process (MDP) to learn the feature prioritization policy.
    The MDP sequentially selects frames with the most promising bag-of-objects or
    CNN features that can improve accuracy when combined with accumulated history
    results. Fig. [12](#S5.F12 "Figure 12 ‣ 5.3.3\. Selection in the Streaming Scenario
    ‣ 5.3\. Subset Selection ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey:
    Exploiting Data Redundancy for Optimization of Deep Learning") illustrates the
    action spaces explored in the mechanism. On two challenging datasets (Activities
    of Daily Living (Pirsiavash and Ramanan, [2012](#bib.bib123)) and UCF-101 (Soomro
    et al., [2012](#bib.bib142))), their method provides significantly better accuracy
    than previous techniques under given computational budgets on two challenging
    datasets (Activities of Daily Living  and UCF-101 ).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '在流媒体场景中的选择更具挑战性。对于每次选择，机制需要根据历史记录对未来帧做出决策。Su et al.（Su and Grauman, [2016](#bib.bib143)）提出了一种主动机制，优先考虑“何时计算哪个特征”，以便进行及时的动作预测。核心思想是学习一种策略，该策略动态安排在给定测试视频的选定帧上计算特征的顺序。选择过程可以在批处理或流媒体维度上调用。他们使用马尔可夫决策过程（MDP）来学习特征优先级策略。MDP依次选择那些在与累积历史结果结合时能提高准确性的最有前途的对象包或CNN特征的帧。图[12](#S5.F12
    "Figure 12 ‣ 5.3.3\. Selection in the Streaming Scenario ‣ 5.3\. Subset Selection
    ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning")展示了机制中探索的动作空间。在两个具有挑战性的数据集（Activities of Daily
    Living（Pirsiavash and Ramanan, [2012](#bib.bib123)）和 UCF-101（Soomro et al., [2012](#bib.bib142)））上，他们的方法在给定的计算预算下提供了比以前技术显著更好的准确性。'
- en: '![Refer to caption](img/31245a2dfc5829ada4a90076f5593b26.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/31245a2dfc5829ada4a90076f5593b26.png)'
- en: 'Figure 12\. Action spaces. Top: In batch, the whole video is divided into sub-volumes,
    and actions are defined by the volume and object category to detect. Middle: In
    streaming, the video is divided into segments by the buffer at each step, and
    actions are the object category to detect in the buffer plus a “skip” action.
    Bottom: Our method learns a video-specific policy to select a sequence of valuable
    features to extract dynamically. Figure and description adapted from reference (Su
    and Grauman, [2016](#bib.bib143)).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. 行为空间。顶部：在批处理模式中，整个视频被划分为子体积，动作由体积和要检测的对象类别定义。中部：在流模式中，视频在每一步通过缓冲区划分为段，动作是缓冲区中要检测的对象类别加上一个“跳过”动作。底部：我们的方法学习一个特定于视频的策略，以动态选择一系列有价值的特征进行提取。图形和描述改编自参考文献
    (Su and Grauman, [2016](#bib.bib143))。
- en: 5.4\. Temporal Propagation
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 时间传播
- en: The neighboring frames in a video usually show some inherent continuity. In
    this type of optimization, the intermediate computation results of adjacent video
    frames such as feature maps, bounding boxes, or classification probability and
    confidence are propagated along the temporal dimension to reduce the need for
    re-computation on similar contents. Usually, some key frames are chosen, and the
    results are propagated from them to other frames. The optimizations are different
    in the representations used in propagation and update. The representations of
    frame features include Optical Flow, Motion History Image (MHI), and Convolutional
    LSTMs. To clarify, in Convolutional LSTMs, frames are represented in normal CNN
    feature maps, but the corresponding information is updated via LSTM networks.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 视频中的相邻帧通常显示出一些固有的连续性。在这种优化类型中，相邻视频帧的中间计算结果，如特征图、边界框或分类概率和置信度，会沿时间维度传播，以减少对相似内容的重新计算需求。通常会选择一些关键帧，并从这些关键帧向其他帧传播结果。优化的不同之处在于传播和更新中使用的表示。帧特征的表示包括光流、运动历史图像
    (MHI) 和卷积 LSTM。为了澄清，在卷积 LSTM 中，帧以普通的 CNN 特征图表示，但相应的信息通过 LSTM 网络进行更新。
- en: 5.4.1\. Using Optical flow
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1\. 使用光流
- en: 'Zhu et al. (Zhu et al., [2017b](#bib.bib170)) propose a deep feature flow network
    (DFF). In DFF, neighboring frames are propagated with feature maps via a flow
    field for partial updates. The updates are obtained by a flow estimation algorithm
    like SIFT Flow (Liu et al., [2008](#bib.bib106)) or FlowNet (Dosovitskiy et al.,
    [2015](#bib.bib42)). Fig. [13](#S5.F13 "Figure 13 ‣ 5.4.1\. Using Optical flow
    ‣ 5.4\. Temporal Propagation ‣ 5\. Leverage Data Redundancy in DNN for Videos
    ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning") shows
    how information is propagated within neighboring frames. Compared to re-computing
    each frame through convolution layers, DFF is $10\times$ faster with only a few
    percent accuracy loss. Subsequently, the team comes up with the flow-guided feature
    aggregation (FGFA) (Zhu et al., [2017a](#bib.bib169)) that associates and assembles
    the rich appearance information in consecutive frames to improve feature representation
    and accuracy. Afterward, the authors unify the solutions into a common framework (Zhu
    et al., [2018](#bib.bib168)), and achieve $77.8\%$ mAP score at a speed of $15.22$
    frame per second for video object detection.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zhu 等人 (Zhu et al., [2017b](#bib.bib170)) 提出了一个深度特征流网络 (DFF)。在 DFF 中，相邻帧通过流场传播特征图以进行部分更新。更新是通过类似
    SIFT Flow (Liu et al., [2008](#bib.bib106)) 或 FlowNet (Dosovitskiy et al., [2015](#bib.bib42))
    的流估计算法获得的。图 [13](#S5.F13 "Figure 13 ‣ 5.4.1\. Using Optical flow ‣ 5.4\. Temporal
    Propagation ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting
    Data Redundancy for Optimization of Deep Learning") 显示了信息在相邻帧中的传播方式。与通过卷积层重新计算每一帧相比，DFF
    速度快 $10\times$，且仅有几百分比的准确率损失。随后，团队提出了流引导特征聚合 (FGFA) (Zhu et al., [2017a](#bib.bib169))，该方法将连续帧中的丰富外观信息关联和组合，以改善特征表示和准确性。之后，作者将这些解决方案统一到一个通用框架中
    (Zhu et al., [2018](#bib.bib168))，并在视频目标检测中以每秒 $15.22$ 帧的速度达到了 $77.8\%$ 的 mAP
    分数。'
- en: Kang et al. (Kang et al., [2018](#bib.bib88)) leverage motion-guided propagation
    for minor temporal information refinement across adjacent frames. The assistance
    of temporal information reduces the potential false negatives prediction when
    frames are processed per frame.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Kang 等人 (Kang et al., [2018](#bib.bib88)) 利用运动引导传播对相邻帧进行微小的时间信息细化。时间信息的辅助减少了在逐帧处理时潜在的假阴性预测。
- en: '![Refer to caption](img/e7945e9a555fa31b74454bab14bb8c6a.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e7945e9a555fa31b74454bab14bb8c6a.png)'
- en: Figure 13\. Illustration of temporal propagation in deep feature flow. (a) Video
    recognition using per-frame network evaluation (b) The proposed deep feature flow.
    Figures and descriptions adapted from reference (Zhu et al., [2017b](#bib.bib170)).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13\. 深度特征流中的时间传播示意图。 (a) 使用逐帧网络评估的视频识别 (b) 提出的深度特征流。图示及描述改编自参考文献 (Zhu et al.,
    [2017b](#bib.bib170))。
- en: 5.4.2\. Using Motion History Image
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2\. 使用运动历史图像
- en: 'Unlike previous approaches that propagate key frames information at fixed intervals,
    Chen et al. (Chen et al., [2018](#bib.bib22)) propose Scale-Time Lattice that
    selects the key frames adaptively. The Scale-Time Lattice is a directed acyclic
    graph as shown in Fig. [14](#S5.F14 "Figure 14 ‣ 5.4.2\. Using Motion History
    Image ‣ 5.4\. Temporal Propagation ‣ 5\. Leverage Data Redundancy in DNN for Videos
    ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning"). Inside
    the graph, the node stands for the bounding box detection result at a particular
    spatial resolution and time point; the edge represents an operation that performs
    temporal propagation or spatial refinement. During the execution, given a video
    input, the framework first applies expensive object detectors to the key frames
    selected sparsely and adaptively based on the object motion and scale. Next, within
    the lattice, those bounding boxes are propagated to intermediate frames and refined
    across scales (from coarse to fine) via cheaper networks. A Propagation Refinement
    Unit (PRU) takes the detection results of two consecutive frames as input, propagates
    them to other frames, and re-scales them. The authors adopt Motion History Image
    (MHI) (Bobick and Davis, [2001](#bib.bib11)) since the motion representation computation
    is relatively cheap than optical flows. Overall, the proposed method yields 79.6
    mAP at 20 FPS and 79.0 mAP at 62 FPS on ImageNet VID dataset.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '与之前在固定时间间隔传播关键帧信息的方法不同，Chen et al. (Chen et al., [2018](#bib.bib22)) 提出了选择自适应关键帧的尺度-时间晶格。尺度-时间晶格是一个有向无环图，如图 [14](#S5.F14
    "Figure 14 ‣ 5.4.2\. Using Motion History Image ‣ 5.4\. Temporal Propagation ‣
    5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning") 所示。在图中，节点表示在特定空间分辨率和时间点的边界框检测结果；边表示执行时间传播或空间细化的操作。在执行过程中，给定一个视频输入，框架首先对根据物体运动和尺度稀疏自适应选择的关键帧应用昂贵的目标检测器。接下来，在晶格内，这些边界框被传播到中间帧，并通过较便宜的网络在尺度之间（从粗到细）进行细化。一个传播细化单元（PRU）将两帧连续帧的检测结果作为输入，传播到其他帧，并重新缩放。作者采用了运动历史图像（MHI） (Bobick
    and Davis, [2001](#bib.bib11))，因为运动表示计算相对便宜于光流。总体而言，所提出的方法在 ImageNet VID 数据集上以
    20 FPS 得到 79.6 mAP，以 62 FPS 得到 79.0 mAP。'
- en: '![Refer to caption](img/a006f6407c4ee5421324a38f176fc763.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a006f6407c4ee5421324a38f176fc763.png)'
- en: Figure 14\. The Scale-Time Lattice, where each node represents the detection
    results at a specific scale and time point, and each edge represents an operation
    from one node to another. In particular, the horizontal edges (in blue color)
    represent the temporal propagation from one time step to the next, while the vertical
    edges (in green color) represent the spatial refinement from low to high resolutions.
    Given a video, the image-based detection is only done at sparsely chosen key frames,
    and the results are propagated along a pre-defined path to the bottom row. The
    final results at the bottom cover all the time points. Figure and description
    adapted from reference (Chen et al., [2018](#bib.bib22)).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14\. 尺度-时间晶格，其中每个节点表示特定尺度和时间点的检测结果，每条边表示从一个节点到另一个节点的操作。特别地，水平边（蓝色）表示从一个时间步骤到下一个时间步骤的时间传播，而垂直边（绿色）表示从低分辨率到高分辨率的空间细化。给定一个视频，基于图像的检测仅在稀疏选择的关键帧上进行，结果沿预定义路径传播到底部行。底部的最终结果覆盖了所有时间点。图示及描述改编自参考文献 (Chen
    et al., [2018](#bib.bib22))。
- en: 5.4.3\. Using Convolutional LSTMs
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3\. 使用卷积 LSTM
- en: 'Liu et al. (Zhu and Liu, [2018](#bib.bib166)) argue that temporal cues can
    be efficiently propagated through an LSTM network. The LSTM serves as an augmented
    structure to assist in refining temporal context for the generated features of
    each frame in the video. In the presented architecture in Fig. [15](#S5.F15 "Figure
    15 ‣ 5.4.3\. Using Convolutional LSTMs ‣ 5.4\. Temporal Propagation ‣ 5\. Leverage
    Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning"), a hypothesis feature map for each frame is generated by the
    CNN feature extractor and then fed into the LSTM to be fused with temporal context
    from previous frames. The output for that frame is a temporally-aware refined
    feature map. The experiment integrates the convolutional LSTMs with Single Shot
    Detector (SSD) (Liu et al., [2016](#bib.bib110)) and provides a quick inference
    speed with only 15 FPS on a mobile CPU.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'Liu 等人（Zhu 和 Liu，[2018](#bib.bib166)）认为，时间线索可以通过 LSTM 网络高效传播。LSTM 作为一种增强结构，帮助细化视频中每一帧生成特征的时间上下文。在图
    [15](#S5.F15 "Figure 15 ‣ 5.4.3\. Using Convolutional LSTMs ‣ 5.4\. Temporal Propagation
    ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning") 所示的架构中，CNN 特征提取器生成每帧的假设特征图，然后输入到 LSTM 中，与来自前几帧的时间上下文融合。该帧的输出是一个时间感知的细化特征图。实验将卷积
    LSTM 与单次检测器（SSD）（Liu 等人，[2016](#bib.bib110)）集成在一起，并在移动 CPU 上以每秒 15 帧的速度提供了快速推理速度。'
- en: '![Refer to caption](img/a1170929078a35432f695df2184c47db.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a1170929078a35432f695df2184c47db.png)'
- en: Figure 15\. An example illustration of our joint LSTM-SSD model. Multiple Convolutional
    LSTM layers are inserted into the network. Each propagates and refines feature
    maps at a particular scale. Figure and description adapted from reference (Zhu
    and Liu, [2018](#bib.bib166)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15\. 我们联合 LSTM-SSD 模型的示例插图。网络中插入了多个卷积 LSTM 层。每一层在特定的尺度上传播并细化特征图。图和描述改编自参考文献（Zhu
    和 Liu，[2018](#bib.bib166)）。
- en: 'Kang et al. (Kang et al., [2017b](#bib.bib87)) leverage LSTMs instead of bounding
    box proposal refinement in video object recognition. The temporal information
    is propagated across each proposal tublet to improve accuracy. As shown in Fig. [16](#S5.F16
    "Figure 16 ‣ 5.4.3\. Using Convolutional LSTMs ‣ 5.4\. Temporal Propagation ‣
    5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning"), an encoder-decoder LSTM structure is placed
    after the Tublet Proposal Network. It generates the output probability of each
    class label as a prediction result for each proposal.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kang 等人（Kang 等人，[2017b](#bib.bib87)）在视频物体识别中利用 LSTM，而不是边界框提议细化。时间信息在每个提议管道上进行传播，以提高准确性。如图
    [16](#S5.F16 "Figure 16 ‣ 5.4.3\. Using Convolutional LSTMs ‣ 5.4\. Temporal Propagation
    ‣ 5\. Leverage Data Redundancy in DNN for Videos ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning") 所示，编码器-解码器 LSTM 结构放置在管道提议网络之后。它为每个提议生成每个类别标签的输出概率作为预测结果。'
- en: '![Refer to caption](img/9239909bcc281eed4acdaf64d1e19b19.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9239909bcc281eed4acdaf64d1e19b19.png)'
- en: Figure 16\. The proposed object detection system consists of two main parts.
    The first is a tubelet proposal network to generate tubelet proposals efficiently.
    The tubelet proposal network extracts multi-frame features within the spatial
    anchors, predicts the object motion patterns relative to the spatial anchors,
    and generates tubelet proposals. The gray box indicates the video clip, and different
    colors indicate the proposal process of other spatial anchors. The second part
    is an encoder-decoder CNN-LSTM network to extract tubelet features and classify
    each proposal box into different classes. The tubelet features are first fed into
    the encoder LSTM by a forward pass to capture the appearance features of the entire
    sequence. Then the states are copied to the decoder LSTM for a backward pass with
    the tubelet features. The encoder-decoder LSTM processes the whole clip before
    outputting class probabilities for each frame. Figure and description reference
    adapted from reference (Kang et al., [2017b](#bib.bib87)).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16\. 提议的物体检测系统包括两个主要部分。第一个部分是一个管道提议网络，用于高效生成管道提议。管道提议网络在空间锚点内提取多帧特征，预测相对于空间锚点的物体运动模式，并生成管道提议。灰色框表示视频片段，不同的颜色表示其他空间锚点的提议过程。第二部分是一个编码器-解码器
    CNN-LSTM 网络，用于提取管道特征并将每个提议框分类为不同的类别。管道特征首先通过前向传递输入到编码器 LSTM，以捕获整个序列的外观特征。然后，将状态复制到解码器
    LSTM 进行后向传递，与管道特征一起。编码器-解码器 LSTM 在输出每帧的类别概率之前处理整个剪辑。图和描述参考文献改编自（Kang 等人，[2017b](#bib.bib87)）。
- en: 5.5\. Data Redundancy based Optimizations on 3D CNNs
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5\. 基于数据冗余的3D CNN优化
- en: So far, we have discussed how the performance of 2D CNN is advanced with works
    that exploit redundancy, especially at the temporal dimension where contiguous
    frames involve much identical information. Most of the optimizations are for the
    performance of video object detection, which is rather heavy on computation as
    the DNN has an additional region proposal phase. Still, some DNNs enable the analytical
    capability for videos with either 3D CNN (Ji et al., [2013](#bib.bib81); Tran
    et al., [2015](#bib.bib146)) or two-stream models (Simonyan and Zisserman, [2014](#bib.bib139);
    Karpathy et al., [2014](#bib.bib89)). Unlike how 2D CNN is operated on video data,
    3D CNN can end-to-end training and inference on videos. It is augmented with capturing
    semantics along the temporal dimension using stridden convolution filters.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了如何通过利用冗余来提升2D CNN的性能，特别是在时间维度上，连续帧涉及大量相同的信息。大多数优化是为了视频对象检测的性能，这相当依赖计算，因为DNN具有额外的区域提议阶段。然而，一些DNN通过3D
    CNN（Ji 等人，[2013](#bib.bib81)；Tran 等人，[2015](#bib.bib146)）或双流模型（Simonyan 和 Zisserman，[2014](#bib.bib139)；Karpathy
    等人，[2014](#bib.bib89)）使视频分析能力得以实现。与2D CNN处理视频数据的方式不同，3D CNN可以在视频上进行端到端的训练和推理。它通过使用步长卷积滤波器来增强沿时间维度捕捉语义的能力。
- en: Redundancy removal on video data for 3D CNN has been less explored at software-based
    optimizations. Some works (Carreira and Zisserman, [2017](#bib.bib13); Kopuklu
    et al., [2019](#bib.bib94)) propose to analyze the potential redundancy of the
    underlying network structures by progressively expanding the operator dimension
    from a 2D CNN basis. For example, Feichtenhofer et al. (Feichtenhofer, [2020](#bib.bib47))
    explore the 2D to 3D extension by considering the necessity of parameters or data
    in multiple dimensions. These include frame rate under fixed duration, temporal
    duration, spatial resolution, number of layers per residual stage, number of channels,
    and inner channel width in a residual block. These initiatives introduce another
    angle of viewing data redundancy on video DNNs. Future works offer prospective
    clues for advancing redundancy removal-based algorithms on deep models.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 视频数据中的冗余去除在3D CNN软件优化方面探索较少。一些研究（Carreira 和 Zisserman，[2017](#bib.bib13)；Kopuklu
    等人，[2019](#bib.bib94)）提出通过逐步扩展操作符维度，从2D CNN基础开始，分析基础网络结构的潜在冗余。例如，Feichtenhofer
    等人（Feichtenhofer，[2020](#bib.bib47)）通过考虑在多个维度中参数或数据的必要性，探索2D到3D的扩展。这些维度包括固定时长下的帧率、时间持续性、空间分辨率、每个残差阶段的层数、通道数量和残差块中的内通道宽度。这些举措为视频DNN中的数据冗余提供了另一种视角。未来的研究提供了推进基于冗余去除的深度模型算法的前景线索。
- en: Still, existing works (Wang et al., [2020](#bib.bib153), [2019b](#bib.bib152);
    Fan et al., [2017a](#bib.bib45); Wang et al., [2017](#bib.bib150); Shen et al.,
    [2018](#bib.bib133); Hegde et al., [2018a](#bib.bib68); Chen et al., [2019c](#bib.bib20))
    have developed performance optimizations for 3D CNN that extend the fruitful efforts
    on the 2D CNN counterparts in hardware-based optimizations. The promising outcomes
    lie in that computation along the temporal dimensions is only required on the
    deltas (value differences) between adjacent video frames. They effectively alleviate
    the memory consumption and inference time bottleneck on resource-constraint devices.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，现有研究（Wang 等人，[2020](#bib.bib153)，[2019b](#bib.bib152)；Fan 等人，[2017a](#bib.bib45)；Wang
    等人，[2017](#bib.bib150)；Shen 等人，[2018](#bib.bib133)；Hegde 等人，[2018a](#bib.bib68)；Chen
    等人，[2019c](#bib.bib20)）已经开发了3D CNN的性能优化，这些优化扩展了在硬件优化方面对2D CNN的丰硕努力。令人鼓舞的结果在于，计算仅在相邻视频帧之间的增量（值差异）上进行，从而有效缓解了在资源受限设备上的内存消耗和推理时间瓶颈。
- en: 6\. Leverage Data Redundancy in Transformer Optimization for Text
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 在Transformer优化中利用数据冗余以提升文本性能
- en: Text-based redundancy optimization can be applied to Recurrent Neural Networks
    (RNNs), the widely used deep learning model for texts before the invention of
    the Transformer-based model. Guan et al. (Guan et al., [2021](#bib.bib61)) propose
    to leverage context-free grammar (CFG) and a hierarchical compression algorithm
    to squeeze the repeated contexts in sequence prediction tasks. The optimization
    accelerates RNN inference up to a thousand times and expends the prediction scope
    without losing task accuracy. Similar optimization insights can be found in many
    recent Transformer models as well. Aside from reducing model execution time or
    increasing task-specific accuracy, many data redundancy-based optimizations in
    Transformer models are initially motivated by the need to scale up the capacity
    of the models. Each time, the model can only process a sequence limited in length
    (the number of token representations) for token-level text inputs, subject to
    the hardware capability. The problem is especially crucial when it is necessary
    to process the representations of a paragraph or an extended sequence at once.
    As a result, researchers have systematically identified redundancy within the
    raw data or intermediate data representations to scale the model correctly.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本的冗余优化可以应用于递归神经网络（RNNs），这是在Transformer模型发明之前广泛使用的深度学习文本模型。Guan等人（Guan et
    al., [2021](#bib.bib61)）建议利用上下文无关文法（CFG）和层次压缩算法来压缩序列预测任务中的重复上下文。这种优化将RNN推理速度提高了多达千倍，并扩大了预测范围，而不会丧失任务准确性。类似的优化见解也可以在许多最近的Transformer模型中找到。除了减少模型执行时间或提高任务特定准确性外，Transformer模型中的许多基于数据冗余的优化最初是出于扩展模型容量的需要。每次，模型只能处理有限长度的序列（标记表示的数量）作为标记级文本输入，受限于硬件能力。当需要一次性处理段落或扩展序列的表示时，这个问题尤为重要。因此，研究人员系统地识别了原始数据或中间数据表示中的冗余，以正确地扩展模型。
- en: Unlike CNN optimizations which leverage data redundancy at input data or intermediate
    feature maps between convolution layers, optimizations on Transformers can occur
    at different stages of interim data representations. A sequence of text input
    to the transformer model goes through several encoding phases, each capturing
    an extra level of meaning. The stages focus on the initial sentences or sub-words
    input, the sub-words embeddings, the hidden states produced by positional encoding,
    multi-head attention, or the encoder layer as a whole (multi-head attention and
    feed-forward layer together). Data redundancy can exist in each of the phases.
    Existing studies leverage the redundancy of various insights.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 与利用输入数据或卷积层之间的中间特征图数据冗余的CNN优化不同，Transformers上的优化可以发生在不同阶段的中间数据表示上。输入到transformer模型的文本序列经历几个编码阶段，每个阶段都捕捉额外的意义。这些阶段关注于初始句子或子词输入、子词嵌入、由位置编码产生的隐藏状态、多头注意力，或编码器层整体（多头注意力和前馈层一起）。每个阶段中可能存在数据冗余。现有研究利用了各种见解的冗余性。
- en: 'For the first stage, sub-word segmentation algorithms have been carried out
    with the motivation to remove redundancy in text data. Byte-Pair Encoding (BPE) (Sennrich
    et al., [2016](#bib.bib132)) is one such case. For the second stage, redundancy
    may exist in input embeddings. Compression at the vector representation of word
    embeddings was introduced long before the wide use of Transformer models (Kim
    et al., [2020a](#bib.bib91); Acharya et al., [2019](#bib.bib2); Chen et al., [2016](#bib.bib26)).
    For the remaining stages, there are optimizations that leverage redundancy at
    intermediate representation, ”hidden state” (a single sample output of a transformer
    layer; the unit is the same as activation map in CNN), of the model in general.
    Optimizations that leverage data redundancy in Transformer are classified based
    on how they exploit redundancy: clustering for reuse, skipping, and other ad-hoc
    sampling techniques. Table [2](#S6.T2 "Table 2 ‣ 6\. Leverage Data Redundancy
    in Transformer Optimization for Text ‣ Survey: Exploiting Data Redundancy for
    Optimization of Deep Learning") lists the surveyed work by their applications
    for better reference.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '在第一阶段，子词分割算法旨在消除文本数据中的冗余。例如 Byte-Pair Encoding (BPE) (Sennrich 等人，[2016](#bib.bib132))。在第二阶段，冗余可能存在于输入嵌入中。在
    Transformer 模型广泛使用之前，已经引入了对词嵌入的向量表示进行压缩的技术 (Kim 等人，[2020a](#bib.bib91)；Acharya
    等人，[2019](#bib.bib2)；Chen 等人，[2016](#bib.bib26))。在剩余阶段，存在利用模型的中间表示，即“隐藏状态”（transformer
    层的单一样本输出；单位与 CNN 中的激活图相同）的优化。利用 Transformer 中数据冗余的优化方法根据它们如何利用冗余进行分类：重用的聚类、跳过以及其他临时采样技术。表
    [2](#S6.T2 "Table 2 ‣ 6\. Leverage Data Redundancy in Transformer Optimization
    for Text ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")
    列出了按其应用分类的调查工作，以便更好地参考。'
- en: Table 2\. Summary of works on text data redundancy
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 文本数据冗余的工作总结
- en: '| Application | Paper | Code | Key Idea | Performance |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 论文 | 代码 | 关键思想 | 性能 |'
- en: '| text summarization | (Liu et al., [2018a](#bib.bib109)) | (Liu, [2018](#bib.bib108))
    | T-DMCA is a modified multi-head self-attention structure to reduce the memory
    footprint in self-attention layers | possible to learn on sequence length of 12,000
    on GPU (Nvidia P100) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 文本摘要 | (Liu 等人，[2018a](#bib.bib109)) | (Liu，[2018](#bib.bib108)) | T-DMCA
    是一种修改后的多头自注意力结构，用于减少自注意力层中的内存占用 | 在 GPU (Nvidia P100) 上能够处理长度为 12,000 的序列 |'
- en: '| text classification | (Dai et al., [2020](#bib.bib36)) | (Dai, [2020](#bib.bib35))
    | Funnel-Transformer augments the transformer model with an encoder to gradually
    pooled the representation to reduce the sequence-length of the hidden states as
    the layer goes deeper | $1.3\times$-$1.5\times$ speedup on GPU/TPU |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 | (Dai 等人，[2020](#bib.bib36)) | (Dai，[2020](#bib.bib35)) | Funnel-Transformer
    通过一个编码器增强 transformer 模型，逐步汇总表示，以减少隐藏状态的序列长度，随着层级的深入 | 在 GPU/TPU 上加速 $1.3\times$-$1.5\times$
    |'
- en: '|  | (Goyal et al., [2020](#bib.bib59)) | (Goyal, [2020](#bib.bib58)) | PoWER-BERT
    applies extract layers and learning-based retention configuration to retain only
    the key embedding vectors in the transformer model | $4.5\times$ speedup on BERT-base
    models ($6.8\times$ with ALBERT) on GPU (Nvidia K80) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | (Goyal 等人，[2020](#bib.bib59)) | (Goyal，[2020](#bib.bib58)) | PoWER-BERT
    应用提取层和基于学习的保留配置，只保留 transformer 模型中的关键嵌入向量 | 在 GPU (Nvidia K80) 上，BERT-base 模型加速$4.5\times$（使用
    ALBERT 时加速$6.8\times$） |'
- en: '|  | (Dalvi et al., [2020](#bib.bib39)) | (Dalvi, [2020](#bib.bib37)) | Exploit
    LayerSelector (LS) and Correlation Clustering Feature Selection (CCFS) to select
    the essence layers and features for transformer models | Evaluated on BERT and
    XLNET. Reduce forward pass parameters up to $47\%$. Reduce feature set to $4\%$
    (sequence labeling), $<1\%$ (sequence classification) on GPU (Nvidia Titan X)
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | (Dalvi 等人，[2020](#bib.bib39)) | (Dalvi，[2020](#bib.bib37)) | 利用 LayerSelector
    (LS) 和相关聚类特征选择 (CCFS) 选择 transformer 模型的核心层和特征 | 在 BERT 和 XLNET 上评估。减少向前传递参数最多
    $47\%$。在 GPU (Nvidia Titan X) 上将特征集减少到 $4\%$（序列标注），$<1\%$（序列分类） |'
- en: '| question answering | (Kitaev et al., [2020](#bib.bib93)) | (Kitaev, [2020](#bib.bib92))
    | Propose Locality-sensitive Hashing Attention and Reversible Transformer to address
    data redundancy | Attention speed can maintain as low as 0.1-0.5 seconds per step
    when sequence length per batch scale from 32 to 32768s (baseline is 0.2-5 seconds
    per step) on GPU/TPU |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 问答系统 | (Kitaev 等人，[2020](#bib.bib93)) | (Kitaev，[2020](#bib.bib92)) | 提出局部敏感哈希注意力和可逆
    Transformer 以解决数据冗余问题 | 当每批次的序列长度从 32 扩展到 32768 时，注意力速度可以保持在每步 0.1-0.5 秒（基线为每步
    0.2-5 秒），在 GPU/TPU 上 |'
- en: 6.1\. Reuse via Clustering
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 通过聚类实现重用
- en: 'Kitaev and Kaiser et al. propose Reformer (Kitaev et al., [2020](#bib.bib93))
    with two main contributions in improving the transformer model: Locality-sensitive
    Hashing Attention and Reversible Transformer. In particular, Locality-sensitive
    Hashing Attention tackles data redundancy with a specific function design. The
    function uses an approximation in the scaled dot-product attention in a transformer
    model. The scaled dot-product attention is defined as $\frac{QK^{T}}{\sqrt{d_{k}}}V$,
    which $Q$, $K$, and $V$ are the query, key, and value matrices, respectively.
    The authors identify that the computation in the softmax function is dominated
    by key-query pairs that are in proximity. It indicates the calculation can pay
    attention to only a small subset of the closet key-query pairs. As exemplified
    in Fig. [17](#S6.F17 "Figure 17 ‣ 6.1\. Reuse via Clustering ‣ 6\. Leverage Data
    Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting Data Redundancy
    for Optimization of Deep Learning"), locality-sensitive hashing (LSH) (Gionis
    et al., [1999](#bib.bib57)) is employed to first cluster the keys and queries
    into hash buckets. Queries and keys in each bucket are sorted to form chunks to
    avoid sparsity in the attention matrix. Afterward, the scaled dot-product attention
    is substituted by allowing multiplication only between pairs in the same hash
    buckets (or in chunks in the attention matrix). Furthermore, the Locality-sensitive
    Hashing Attention introduces a multi-round LSH attention that enables multiple
    rounds of hashing to alleviate the problems that similar items may fall into different
    buckets after hashing. In the decoder of the transformer model, a masking mechanism
    is implemented to re-order the position indices by the same permutations that
    were previously applied to sort the key and query vectors.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kitaev 和 Kaiser 等人提出了 Reformer (Kitaev et al., [2020](#bib.bib93))，其在改进 Transformer
    模型方面有两个主要贡献：局部敏感哈希注意力和可逆 Transformer。特别是，局部敏感哈希注意力通过特定函数设计解决了数据冗余问题。该函数在 Transformer
    模型的缩放点积注意力中使用了近似。缩放点积注意力定义为 $\frac{QK^{T}}{\sqrt{d_{k}}}V$，其中 $Q$、$K$ 和 $V$ 分别是查询、键和值矩阵。作者发现，softmax
    函数中的计算被接近的键-查询对主导。这表明计算可以仅关注少量接近的键-查询对。如图 [17](#S6.F17 "Figure 17 ‣ 6.1\. Reuse
    via Clustering ‣ 6\. Leverage Data Redundancy in Transformer Optimization for
    Text ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")
    所示，局部敏感哈希 (LSH) (Gionis et al., [1999](#bib.bib57)) 首先将键和查询分成哈希桶。每个桶中的查询和键被排序以形成块，从而避免注意力矩阵中的稀疏性。随后，缩放点积注意力被替换为仅允许在同一哈希桶（或注意力矩阵中的块）中的对进行乘法。此外，局部敏感哈希注意力引入了多轮
    LSH 注意力，允许多轮哈希以减轻相似项在哈希后可能落入不同桶的问题。在 Transformer 模型的解码器中，实施了掩蔽机制，通过先前应用于排序键和查询向量的相同置换来重新排序位置索引。'
- en: '![Refer to caption](img/c7b71d35c454f80c020696d6122c1e81.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c7b71d35c454f80c020696d6122c1e81.png)'
- en: Figure 17\. Locality-sensitive hashing attention in Reformer. LSH Attention
    shows the hash-bucketing, sorting, and chunking steps and the resulting causal
    attentions. (a-d) Attention matrices for these varieties of attention. Figures
    adapted from reference (Kitaev et al., [2020](#bib.bib93)).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17\. Reformer 中的局部敏感哈希注意力。LSH 注意力展示了哈希分桶、排序和分块步骤及其结果因果注意力。 (a-d) 这些注意力的矩阵。图像改编自参考文献
    (Kitaev et al., [2020](#bib.bib93))。
- en: 6.2\. Skipping
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 跳过
- en: Output representations from attention layers in Transformer models have been
    recognized as containing much redundant information (Michel et al., [2019](#bib.bib114);
    Voita et al., [2019](#bib.bib148)). They have been addressed through ways of attention
    head pruning (Michel et al., [2019](#bib.bib114); Voita et al., [2019](#bib.bib148)).
    Recent studies have investigated the criteria for pruning neurons (activations)
    in Transformer models. Specifically, Gupta et al. (Gupta et al., [2020](#bib.bib64))
    suggest neurons are to be pruned when either the importance of that neuron is
    low or when the same activation multiplies with the same weights. The neuron importance
    function (Sanh et al., [2019](#bib.bib131)) is defined on entropy, output-weights
    norm, or the input-weights norm.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型中的注意力层输出表示被认为包含了大量冗余信息 (Michel et al., [2019](#bib.bib114); Voita
    et al., [2019](#bib.bib148))。这些问题通过注意力头剪枝的方式得到了处理 (Michel et al., [2019](#bib.bib114);
    Voita et al., [2019](#bib.bib148))。近期的研究探讨了在 Transformer 模型中剪枝神经元（激活值）的标准。具体而言，Gupta
    et al. (Gupta et al., [2020](#bib.bib64)) 建议当神经元的重要性较低或相同的激活值与相同的权重相乘时，可以剪枝该神经元。神经元重要性函数
    (Sanh et al., [2019](#bib.bib131)) 是基于熵、输出权重范数或输入权重范数来定义的。
- en: 'The hidden state of a sequence of word vectors may contain repetitive information
    themselves. Goyal et al. (Goyal et al., [2020](#bib.bib59)) identify that the
    pair-wise cosine similarity between word vectors in hidden states increases as
    layers go progressively deeper (encode by more phases). They propose PoWER-BERT
    to exploit the redundancy to reduce the inference time of the BERT model in text
    classification and regression tasks. As illustrated in Fig. [18](#S6.F18 "Figure
    18 ‣ 6.2\. Skipping ‣ 6\. Leverage Data Redundancy in Transformer Optimization
    for Text ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning"),
    PoWER-BERT uses an Extract Layer to manage the retention configuration after the
    Self-Attention Layer and before the Feed Forward Network (FFN) in each encoder
    layer of the Transformer model. The retention configuration can be set manually
    or set with parameters from training. The model is trained with a loss function
    proposed to obtain the learning-based design. This method avoids the exponential
    search space in retention configuration. Furthermore, they employ two kinds of
    strategies to retain the word representations: static and the dynamic strategy.
    The static strategy keeps the word vectors at the same positions across all input
    sequences in the dataset. The dynamic strategy retains the word vectors based
    on the attention scoring and a soft-extract layer. In the experiment, they obtain
    a $4.5\times$ inference time acceleration over the baseline BERT model with less
    than $1\%$ accuracy loss. The proposed technique is also compatible with the compression
    scheme such as ALBERT (Lan et al., [2020](#bib.bib97)). The combination of both
    compression methods achieves a $6.8\times$ inference time reduction with less
    than $1\%$ accuracy loss.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列词向量的隐藏状态可能包含重复的信息。Goyal 等（Goyal 等，[2020](#bib.bib59)）指出，随着层次逐渐深入（通过更多阶段进行编码），隐藏状态中词向量的成对余弦相似度会增加。他们提出了
    PoWER-BERT 来利用冗余以减少 BERT 模型在文本分类和回归任务中的推断时间。如图 [18](#S6.F18 "图 18 ‣ 6.2\. 跳过 ‣
    6\. 利用变换器优化中的数据冗余进行文本处理 ‣ 调查：利用数据冗余优化深度学习") 所示，PoWER-BERT 使用 Extract Layer 来管理每个变换器模型编码器层中的
    Self-Attention Layer 之后和 Feed Forward Network (FFN) 之前的保留配置。保留配置可以手动设置或使用训练中的参数进行设置。该模型使用提出的损失函数进行训练，以获得基于学习的设计。这种方法避免了保留配置中的指数搜索空间。此外，他们采用了两种保留词表示的策略：静态策略和动态策略。静态策略在数据集中的所有输入序列中保持词向量在相同的位置。动态策略基于注意力评分和软提取层来保留词向量。在实验中，他们在基准
    BERT 模型上获得了 $4.5\times$ 的推断时间加速，准确率损失不到 $1\%$。所提技术也与 ALBERT（Lan 等，[2020](#bib.bib97)）等压缩方案兼容。这两种压缩方法的结合实现了
    $6.8\times$ 的推断时间缩短，准确率损失不到 $1\%$。
- en: '![Refer to caption](img/58656fad897364e9aca1943c0d6c6d26.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/58656fad897364e9aca1943c0d6c6d26.png)'
- en: Figure 18\. Illustration of PoWER-BERT. Word-vector selection over the first
    two encoders. Here, $N=6$, $l_{1}=4$ and $l_{2}=2$. The first encoder eliminates
    two word-vectors $w2$ and $w4$ with least significance scores; the second encoder
    further eliminates word-vectors $w1$ and $w5$. Figure and description adapted
    from reference (Goyal et al., [2020](#bib.bib59)).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图18\. PoWER-BERT 的示意图。前两个编码器中的词向量选择。这里，$N=6$，$l_{1}=4$ 和 $l_{2}=2$。第一个编码器淘汰了两个具有最小重要性分数的词向量
    $w2$ 和 $w4$；第二个编码器进一步淘汰了词向量 $w1$ 和 $w5$。图及描述改编自参考文献（Goyal 等，[2020](#bib.bib59)）。
- en: 'Dalvi et al. (Dalvi et al., [2020](#bib.bib39)) propose an efficient transfer
    learning method that exploits LayerSelector (LS) and Correlation Clustering Feature
    Selection (CCFS) to select the essence layers and features required during transformer
    model execution. There are three main steps in the selection: (1) LayerSelector
    (LS) uses the layer-classifier to select the lowest layer that maintains oracle
    performance. (2) Correlation Clustering filters out redundant neurons given the
    hidden state’s output of the previous layer. (3) Feature Selection (FS) selects
    a minimal set of neurons required to reach optimum performance on the given task.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Dalvi 等（Dalvi 等，[2020](#bib.bib39)）提出了一种高效的迁移学习方法，该方法利用 LayerSelector (LS) 和
    Correlation Clustering Feature Selection (CCFS) 来选择在变换器模型执行过程中所需的核心层和特征。选择过程主要包括三个步骤：（1）LayerSelector
    (LS) 使用层分类器选择保持最佳性能的最低层。（2）Correlation Clustering 根据前一层的隐藏状态输出过滤掉冗余神经元。（3）Feature
    Selection (FS) 选择达到给定任务最佳性能所需的最小神经元集。
- en: For the first step, the authors analyze task-specific layer-level redundancy
    by training linear probing classifiers (Shi et al., [2016](#bib.bib134); Belinkov
    et al., [2017](#bib.bib8)) on each layer $l_{i}$ as layer-classifier. They use
    the layer-classifier to select the lowest layer that maintains oracle performance
    (maintaining $99\%$ performance). In LS, a concentration of all layers until the
    chosen layer is used.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一步，作者通过在每一层$l_{i}$上训练线性探测分类器（Shi et al., [2016](#bib.bib134); Belinkov et al.,
    [2017](#bib.bib8)）来分析任务特定的层级冗余。他们使用层级分类器选择保持神谕性能的最低层（保持$99\%$性能）。在LS中，使用集中到所选择层的所有层。
- en: 'For the following two steps, the CCFS exploits data redundancy by a combination
    of clustering and subset selection:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个步骤中，CCFS通过聚类和子集选择的组合来利用数据冗余。
- en: •
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Correlation Clustering (CC) (Bansal et al., [2004](#bib.bib7)): For every length-N
    sequence vector representation, the product-moment correlation between each of
    the two features is calculated. This gives a $N$-by-$N$ matrix $corr(x,y)$ representing
    the correlation between $fx$ and $fy$. The correlation value ranges from $-1$
    to $1$, which provides a relative scale to compare any two neurons. The distance
    metric $cdist(x,y)$ between these two features is defined by $1-|corr(x,y)|$.
    A hyper-parameter $ct$ defines the maximum distance between any two features to
    be considered as a cluster.'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相关聚类（CC） (Bansal et al., [2004](#bib.bib7))：对于每个长度为N的序列向量表示，计算每两个特征之间的积矩相关性。这生成一个$N$乘$N$矩阵$corr(x,y)$，表示$fx$和$fy$之间的相关性。相关值范围从$-1$到$1$，提供了一个相对尺度来比较任何两个神经元。这两个特征之间的距离度量$cdist(x,y)$定义为$1-|corr(x,y)|$。超参数$ct$定义了被认为是一个簇的两个特征之间的最大距离。
- en: •
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Feature Selection (FS): It identifies a minimal set of neurons that match the
    oracle performance. The Linguistic Correlation Analysis (Dalvi et al., [2019](#bib.bib38))
    ranks the neurons concerning a downstream task.'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征选择（FS）：它识别出一组最小的神经元，这些神经元与神谕性能匹配。语言相关性分析 (Dalvi et al., [2019](#bib.bib38))
    对神经元进行排名，以配合下游任务。
- en: Overall, the work finds out that up to $85\%$ and $95\%$ neurons are redundant
    in BERT and XLNet (Yang et al., [2019](#bib.bib159)) respectively. In the experiment,
    the proposed method accelerates sequence labeling tasks by $2.8\times$ and $6.2\times$
    on BERT and XLNet, respectively. While for sequence classification tasks, the
    average speedups are $1.1\times$ and $2.8\times$ correspondingly.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，研究发现BERT和XLNet中多达$85\%$和$95\%$的神经元是冗余的（Yang et al., [2019](#bib.bib159)）。在实验中，所提出的方法在BERT和XLNet上分别加速了序列标注任务$2.8\times$和$6.2\times$。而对于序列分类任务，平均加速分别为$1.1\times$和$2.8\times$。
- en: 6.3\. Other Sampling
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 其他采样
- en: 'Humans implicitly use prior linguistic knowledge to merge nearby tokens or
    words in paragraphs. This, in turn, forms more extensive semantic phrases or units
    to understand sentences or paragraphs in a general form. Inspired by that, Dai
    et al. (Dai et al., [2020](#bib.bib36)) have proposed Funnel-Transformer. As shown
    in Fig. [19](#S6.F19 "Figure 19 ‣ 6.3\. Other Sampling ‣ 6\. Leverage Data Redundancy
    in Transformer Optimization for Text ‣ Survey: Exploiting Data Redundancy for
    Optimization of Deep Learning"), the Funnel-Transformer is equipped with an additional
    encoder to gradually pool the representation to reduce the sequence length of
    the hidden states as the layer goes deeper. The sub-modules have an extra residual
    connection and layer normalization operation in the Self-Attention Layer and the
    Feed Forward Network (FFN) of the Transformer model. The decoder is applied to
    reconstruct the full-sequence of representation when the token-level outputs are
    required, such as pretraining.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 人类隐式使用先前的语言知识来合并段落中的相邻标记或单词。这反过来形成更广泛的语义短语或单元，以便以一般形式理解句子或段落。受到启发，Dai et al. (Dai
    et al., [2020](#bib.bib36)) 提出了Funnel-Transformer。如图 [19](#S6.F19 "图 19 ‣ 6.3\.
    其他采样 ‣ 6\. 利用Transformer优化中的数据冗余 ‣ 调查：利用数据冗余优化深度学习")所示，Funnel-Transformer 配备了一个额外的编码器，用于逐渐汇聚表示，以减少隐藏状态的序列长度，随着层数的增加，隐藏状态的序列长度逐渐减小。子模块在Transformer模型的自注意力层和前馈网络（FFN）中具有额外的残差连接和层归一化操作。解码器用于在需要标记级别输出时重建完整的序列表示，如预训练。
- en: '![Refer to caption](img/05e3da46148e89369dc676341f146f9e.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/05e3da46148e89369dc676341f146f9e.png)'
- en: Figure 19\. Illustration of Pooling in Funnel-Transformer. The encoder on the
    left consists of several blocks of consecutive Transformer layers. Within each
    block, the sequence length of the hidden states always remains the same. But when
    going from a lower-level block to a higher-level block, the size of the hidden
    sequence is reduced by performing a particular type of pooling along the sequence
    dimension. The right part showcases the up-sampling to re-construct the arrangement
    of the original length. Figure and description adapted from reference (Dai et al.,
    [2020](#bib.bib36)).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图19\. 漏斗变换器中池化的示意图。左侧的编码器由多个连续的Transformer层块组成。在每个块中，隐藏状态的序列长度始终保持不变。但在从较低级别块到较高级别块时，通过在序列维度上执行特定类型的池化操作，隐藏序列的大小会减少。右侧部分展示了上采样以重建原始长度的排列。图示及描述改编自参考文献（Dai
    et al., [2020](#bib.bib36)）。
- en: 'To achieve the generation of Wikipedia-styled summarization over multi-documents,
    Liu et al. (Liu et al., [2018a](#bib.bib109)) introduce a compressive scheme for
    long text sequences. The Transformer Decoder with Memory-Compressed Attention(T-DMCA)
    is a modified multi-head self-attention structure to reduce the memory footprint
    via limiting the dot product between the query and key of a Self-Attention Layer.
    The mechanism consists of the Memory-Compressed Attention and the Local Attention
    as shown in Fig. [20](#S6.F20 "Figure 20 ‣ 6.3\. Other Sampling ‣ 6\. Leverage
    Data Redundancy in Transformer Optimization for Text ‣ Survey: Exploiting Data
    Redundancy for Optimization of Deep Learning"). The Memory-Compressed Attention
    reduces the number of keys and values by using a stridden convolution (for sampling).
    The Local Attention divides a sequence of tokens into blocks of similar length
    of tokens (they use 256 tokens) to allow constant attention memory cost per block
    regardless of sequence length. After feeding the sub-sequence to their corresponding
    Multi-head Attention to capture local information, the method merges the results
    to get the final output sequence. For both Local and Memory-Compressed Attention,
    masking is cast to prevent the queries from attending to future keys and values.
    Finally, the design enables the model to process 3$\times$-longer sequences than
    the default model.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现对多文档的维基百科风格的摘要生成，Liu等人（Liu et al., [2018a](#bib.bib109)）引入了一种长文本序列的压缩方案。带记忆压缩注意力的变换器解码器（T-DMCA）是一种修改过的多头自注意力结构，通过限制自注意力层中查询和键的点积来减少内存占用。该机制包括如图[20](#S6.F20
    "图20 ‣ 6.3\. 其他采样 ‣ 6\. 利用变换器优化中的数据冗余 ‣ 调查：利用数据冗余优化深度学习")所示的记忆压缩注意力和局部注意力。记忆压缩注意力通过使用有跨度的卷积（用于采样）来减少键和值的数量。局部注意力将令牌序列分割成相似长度的块（他们使用256个令牌），以便每个块的注意力内存成本保持不变，不受序列长度影响。在将子序列馈送到相应的多头注意力以捕捉局部信息后，该方法将结果合并以获得最终的输出序列。对于局部和记忆压缩注意力，都施加了掩码以防止查询关注未来的键和值。最后，该设计使模型能够处理比默认模型长3$\times$的序列。
- en: '![Refer to caption](img/e8f2be5e05b7b970a963e7b035af32f7.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/e8f2be5e05b7b970a963e7b035af32f7.png)'
- en: 'Figure 20\. Transformer Decoder with Memory-Compressed Attention(T-DMCA). Every
    attention layer takes a sequence of tokens as input and produces a sequence of
    similar length as the output. Left: Original self-attention as used in the transformer
    decoder. Middle: Memory-compressed attention, which reduces the number of keys/values.
    Right: Local attention, which splits the sequence into individual smaller sub-sequences.
    The sub-sequences are then merged to get the final output sequence. Figure and
    reference adapted from reference (Liu et al., [2018a](#bib.bib109)).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图20\. 带记忆压缩注意力的变换器解码器（T-DMCA）。每个注意力层接受一个令牌序列作为输入，并生成一个相似长度的序列作为输出。左：原始自注意力，用于变换器解码器。中：记忆压缩注意力，减少了键/值的数量。右：局部注意力，将序列分割成单独的较小子序列。子序列随后合并以获得最终的输出序列。图示及参考文献改编自参考文献（Liu
    et al., [2018a](#bib.bib109)）。
- en: 7\. Future Research Directions
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 未来研究方向
- en: The many previous studies have clearly shown the significant benefits of exploitation
    of data redundancy in Deep Learning. In creating the summary of the many explorations
    from various aspects, we have recognized the broad coverage of the existing innovations,
    but at the same, observed some open problems and several research directions worth
    pursuing in the future.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 许多之前的研究清楚地显示了在深度学习中利用数据冗余的显著好处。在总结各种方面的众多探索时，我们认识到现有创新的广泛覆盖面，但同时也观察到一些尚未解决的问题和未来值得追寻的几个研究方向。
- en: (i) Principled understanding of the relations between data redundancy exploitation
    and accuracy. Data redundancy exploitation could cause changes in the output of
    a DNN because the removed data are often not guaranteed to be redundant. In most
    cases, it faces the risks of degrading the accuracy, but in some cases, as mentioned
    earlier (Chin et al., [2019](#bib.bib30); Korbar et al., [2019](#bib.bib95); Su
    and Grauman, [2016](#bib.bib143)), it may also improve the accuracy for its noise
    removal effects. There is a lack of principled understanding of how data redundancy
    elimination affects model accuracy. The accuracy of a DNN model can be affected
    by existing variance related to data. Dataset itself may already contain bias
    over DNN models (Torralba and Efros, [2011](#bib.bib145)). Data preprocessing,
    such as data augmentation (Shorten and Khoshgoftaar, [2019](#bib.bib137); Cubuk
    et al., [2019](#bib.bib34)), makes the discussion even more complicated. Previous
    work has either resorted to empirical tuning to reach a satisfying point or used
    some heuristic methods to rate the salience of some data. An open question is
    how to achieve a principled understanding of the impact and transform the knowledge
    into principled data redundancy exploitation techniques. Some rigorous studies
    may help.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 数据冗余利用与准确性之间关系的原则性理解。数据冗余利用可能会改变深度神经网络（DNN）的输出，因为被移除的数据通常不一定是冗余的。在大多数情况下，它面临降低准确性的风险，但在某些情况下，如前述的(Chin
    et al., [2019](#bib.bib30); Korbar et al., [2019](#bib.bib95); Su and Grauman,
    [2016](#bib.bib143))，它也可能通过去噪效果提高准确性。对于数据冗余消除如何影响模型准确性，缺乏原则性的理解。DNN模型的准确性可能会受到与数据相关的现有方差的影响。数据集本身可能已经对DNN模型存在偏差(Torralba
    and Efros, [2011](#bib.bib145))。数据预处理，如数据增强(Shorten and Khoshgoftaar, [2019](#bib.bib137);
    Cubuk et al., [2019](#bib.bib34))，使讨论变得更加复杂。以往的工作要么依赖经验调优以达到令人满意的点，要么使用一些启发式方法来评估某些数据的显著性。一个悬而未决的问题是如何实现对影响的原则性理解，并将知识转化为原则性的数据冗余利用技术。一些严谨的研究可能会有所帮助。
- en: (ii) Enhancing interpretability. Related to the first direction, the second
    direction that may be worth exploring is interpretability, which refers to both
    the interpretability of neuron representation and the interpretability of DNN
    in general. Utilizing evidence in data redundancy has led to some initial success
    in improving the interpretability of CNN’s feature representation, contributing
    to the CNN model pruning processes. For example, Li et al. (Li et al., [2019a](#bib.bib103))
    propose a kernel sparsity and entropy indicator (KSE) to quantify the importance
    of each pixel in the activation map to provide feature-agonistic guidance in weight
    compression. Identifications of data redundancy also benefit the interpretation
    of the importance of each word in text processing. For example, Dalvi et al. (Dalvi
    et al., [2019](#bib.bib38)) present Linguistic Correlation Analysis and Cross-model
    Correlation Analysis to recognize the relative importance of a neuron in a Transformer
    model. Their method offers an ablation view on the saliency of words in the paragraphs
    to guide future representation development. These several studies show some promise
    in exploring data redundancy for the interpretability of deep learning. As interpretability
    is important for connecting Deep Learning with domain understanding, more efforts
    are worthwhile to develop further.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 提高可解释性。与第一个方向相关，可能值得探索的第二个方向是可解释性，这既涉及神经元表示的可解释性，也涉及深度神经网络（DNN）的一般可解释性。利用数据冗余中的证据已在提高卷积神经网络（CNN）特征表示的可解释性方面取得了一些初步成功，并对CNN模型的剪枝过程有所贡献。例如，Li等人（Li
    et al., [2019a](#bib.bib103)）提出了一种核稀疏性和熵指标（KSE），用于量化激活图中每个像素的重要性，以提供特征对抗性指导，用于权重压缩。数据冗余的识别也有助于解释文本处理中文本中每个词的重要性。例如，Dalvi等人（Dalvi
    et al., [2019](#bib.bib38)）提出了语言相关性分析和跨模型相关性分析，以识别Transformer模型中神经元的相对重要性。他们的方法提供了一个词语显著性的消融视角，以指导未来的表示发展。这些研究表明，探索数据冗余以提高深度学习的可解释性具有一定的前景。由于可解释性对将深度学习与领域理解联系起来至关重要，因此进一步的努力是值得的。
- en: (iii) Other data types and models. A large portion of the prior work on data
    redundancy is about image and video data. There is still some room left for more
    explorations on these data types. An example is learning from point cloud (Guo
    et al., [2020](#bib.bib63)) where data redundancy has not yet been explored. But
    in comparison to images and videos, redundancy in text data is much less explored,
    as Transformer is more recently developed than the more mature models used in
    images and videos. More explorations are especially in demand to better understand
    data redundancy in texts. Moreover, besides the three types of data, there are
    other types of data on which DNN is also playing an important role. These data
    include graphs, scientific simulation results, genes, computer programs, and so
    on. Correspondingly, the special properties of these data have prompted the development
    of some new DNN models, such as Graph Convolutional Networks (GCN) (Wu et al.,
    [2020](#bib.bib156)). The differences in data properties, DNN models, and learning
    tasks suggest that some research could be fruitful in understanding and exploiting
    data redundancy in these areas.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 其他数据类型和模型。之前关于数据冗余的大部分工作都集中在图像和视频数据上。这些数据类型还有进一步探索的空间。一个例子是点云学习（Guo et
    al., [2020](#bib.bib63)），数据冗余尚未被探索。但与图像和视频相比，文本数据中的冗余研究要少得多，因为Transformer是比用于图像和视频的更成熟模型更晚开发的。特别是需要更多的探索来更好地理解文本中的数据冗余。此外，除了这三种数据类型，还有其他类型的数据在DNN中也扮演着重要角色。这些数据包括图形、科学模拟结果、基因、计算机程序等。因此，这些数据的特殊属性促使了一些新型DNN模型的发展，例如图卷积网络（GCN）（Wu
    et al., [2020](#bib.bib156)）。数据属性、DNN模型和学习任务的差异表明，在这些领域理解和利用数据冗余的研究可能会取得成果。
- en: '(iv) Relations among optimizations and systematic solutions. One of the questions
    that have not received much attention is the relations among the many kinds of
    optimizations on data redundancy elimination, as well as their relations with
    model optimizations. On images, for instance, as Section [4](#S4 "4\. Leverage
    Redundancy in Image Data ‣ Survey: Exploiting Data Redundancy for Optimization
    of Deep Learning") and Table [1](#S3.T1 "Table 1 ‣ 3.7\. Use of the Taxonomy ‣
    3\. Taxonomy ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")
    show, there are many ways to exploit data redundancy from various angles. Can
    these optimizations be used together? Is it worth doing that? Would there be some
    conflicts besides that they may all affect the model accuracy? How to use them
    together in the best way? More ambitiously, is it possible to build up frameworks
    that automatically apply one or more kinds of data redundancy exploitation techniques
    on a given dataset and learning task? How about the interplay with model optimizations?
    Answers to these questions may significantly advance the current understanding
    of data redundancy exploitation and tap into the potential better.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '(iv) 优化与系统解决方案之间的关系。一个尚未受到太多关注的问题是各种数据冗余消除优化之间的关系，以及它们与模型优化的关系。例如，如第[4](#S4
    "4\. Leverage Redundancy in Image Data ‣ Survey: Exploiting Data Redundancy for
    Optimization of Deep Learning")节和表[1](#S3.T1 "Table 1 ‣ 3.7\. Use of the Taxonomy
    ‣ 3\. Taxonomy ‣ Survey: Exploiting Data Redundancy for Optimization of Deep Learning")所示，在图像中，有许多从不同角度利用数据冗余的方法。这些优化是否可以一起使用？这样做是否值得？除了可能影响模型精度外，会有其他冲突吗？如何以最佳方式将它们结合使用？更雄心勃勃的是，是否可以建立框架，自动对给定数据集和学习任务应用一种或多种数据冗余利用技术？与模型优化的互动如何？这些问题的答案可能会显著推动对数据冗余利用的当前理解，并挖掘其潜力。'
- en: (v) Synergy with DNN accelerator designs. Yet another direction worth looking
    into is the synergy between the many data redundancy exploitation techniques and
    DNN hardware designs. In modern application-specific integrated circuit (ASIC)
    design, dataflow analysis (Kwon et al., [2019](#bib.bib96); Ben-Nun et al., [2019](#bib.bib9);
    Ivanov et al., [2021](#bib.bib80)) has been recognized as a crucial consideration.
    Some work on edge devices has tried to address memory capacity issues by exploiting
    data redundancy (e.g., through data compression) (Mocerino et al., [2019](#bib.bib115);
    Salamat et al., [2018](#bib.bib129); Hegde et al., [2018b](#bib.bib69); Wang et al.,
    [2020](#bib.bib153), [2019b](#bib.bib152); Fan et al., [2017a](#bib.bib45); Wang
    et al., [2017](#bib.bib150); Shen et al., [2018](#bib.bib133); Hegde et al., [2018a](#bib.bib68)).
    But in general, there is no systematic understanding of the relations of the many
    data redundancy exploitation techniques and the designs of DNN hardware accelerators.
    On the one hand, do those techniques stay profitable when DNN runs on hardware
    accelerators? On the other hand, can hardware accelerator designs gain some efficiency
    benefits by adopting some of the ideas in those techniques? Answers to these questions
    could lead to the novel synergy between the two strands of efforts.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: (v) 与 DNN 加速器设计的协同。另一个值得探索的方向是许多数据冗余利用技术与 DNN 硬件设计之间的协同效应。在现代应用特定集成电路（ASIC）设计中，数据流分析（Kwon
    等，[2019](#bib.bib96)；Ben-Nun 等，[2019](#bib.bib9)；Ivanov 等，[2021](#bib.bib80)）已被认为是一个关键考虑因素。一些边缘设备上的研究尝试通过利用数据冗余（例如，通过数据压缩）来解决内存容量问题（Mocerino
    等，[2019](#bib.bib115)；Salamat 等，[2018](#bib.bib129)；Hegde 等，[2018b](#bib.bib69)；Wang
    等，[2020](#bib.bib153)，[2019b](#bib.bib152)；Fan 等，[2017a](#bib.bib45)；Wang 等，[2017](#bib.bib150)；Shen
    等，[2018](#bib.bib133)；Hegde 等，[2018a](#bib.bib68)）。但一般来说，尚未系统理解许多数据冗余利用技术与 DNN
    硬件加速器设计的关系。一方面，当 DNN 在硬件加速器上运行时，这些技术是否仍然有利可图？另一方面，硬件加速器设计是否能通过采纳这些技术中的一些理念获得效率上的好处？这些问题的答案可能会带来两者努力之间的新协同效应。
- en: 8\. Conclusion
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: Machine learning is based on two pillars, models and data. As an essential part
    of deep learning, data plays an important role. Making the best use of data determines
    the quality, speed, efficiency, and interpretability of machine learning. This
    survey summarizes the efforts in the research community in detecting and leveraging
    data redundancy in a variety of dimensions, introduces the first known taxonomy
    to categorize the existing techniques, and points out a set of directions yet
    to explore. As the landscape of machine learning evolves continuously at an unprecedented
    speed, we hope that this survey can provide a one-stop resource for researchers
    to attain a quick understanding of state of the art and open issues, and hence
    benefit the industry practitioners and research community in selecting existing
    solutions for solving a problem at hands or creating new solutions to advance
    this critical field further.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习基于两个支柱：模型和数据。作为深度学习的核心部分，数据扮演着重要角色。数据的最佳利用决定了机器学习的质量、速度、效率和可解释性。这项调查总结了研究界在检测和利用数据冗余方面的努力，介绍了第一个已知的分类法来归类现有技术，并指出了一系列尚待探索的方向。随着机器学习领域以空前的速度不断发展，我们希望这项调查能为研究人员提供一个一站式资源，快速了解最先进的技术和开放性问题，从而帮助行业从业者和研究社区选择现有解决方案来解决手头的问题或创建新解决方案以进一步推动这一关键领域。
- en: References
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Acharya et al. (2019) Anish Acharya, Rahul Goel, Angeliki Metallinou, and Inderjit
    Dhillon. 2019. Online embedding compression for text classification using low
    rank matrix factorization. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 33\. None, None, 6196–6203.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Acharya 等 (2019) Anish Acharya, Rahul Goel, Angeliki Metallinou, 和 Inderjit
    Dhillon. 2019. 使用低秩矩阵分解的文本分类在线嵌入压缩。见 *AAAI人工智能会议论文集*，第33卷。无，无，6196–6203。
- en: 'Akhlaghi et al. (2018) Vahideh Akhlaghi, Amir Yazdanbakhsh, Kambiz Samadi,
    Rajesh K Gupta, and Hadi Esmaeilzadeh. 2018. Snapea: Predictive early activation
    for reducing computation in deep convolutional neural networks. In *Proceedings
    of the 45th ACM/IEEE Annual International Symposium on Computer Architecture (ISCA)*.
    None, None, 662–673.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akhlaghi 等 (2018) Vahideh Akhlaghi, Amir Yazdanbakhsh, Kambiz Samadi, Rajesh
    K Gupta, 和 Hadi Esmaeilzadeh. 2018. Snapea：预测性早期激活以减少深度卷积神经网络中的计算。见 *第45届ACM/IEEE计算机架构国际研讨会（ISCA）*。无，无，662–673。
- en: 'Alwassel (2017) Humam Alwassel. 2017. GitHub. Action Search: Spotting Targets
    in Videos and Its Application to Temporal Action Localization. [https://github.com/HumamAlwassel/action-search](https://github.com/HumamAlwassel/action-search)
    (2021).'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alwassel (2017) Humam Alwassel. 2017. GitHub. 行动搜索：在视频中定位目标及其在时间动作定位中的应用。 [https://github.com/HumamAlwassel/action-search](https://github.com/HumamAlwassel/action-search)
    (2021)。
- en: 'Alwassel et al. (2018) Humam Alwassel, Fabian Caba Heilbron, and Bernard Ghanem.
    2018. Action search: Spotting actions in videos and its application to temporal
    action localization. In *Proceedings of the European Conference on Computer Vision
    (ECCV)*. None, None, 251–266.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alwassel 等 (2018) Humam Alwassel, Fabian Caba Heilbron, 和 Bernard Ghanem. 2018.
    行动搜索：在视频中定位动作及其在时间动作定位中的应用。见 *欧洲计算机视觉会议（ECCV）*。无，无，251–266。
- en: 'Aytar et al. (2016) Yusuf Aytar, Carl Vondrick, and Antonio Torralba. 2016.
    Soundnet: Learning sound representations from unlabeled video. In *Advances in
    Neural Information Processing Systems*. None, None, 892–900.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aytar 等 (2016) Yusuf Aytar, Carl Vondrick, 和 Antonio Torralba. 2016. Soundnet：从未标记视频中学习声音表示。见
    *神经信息处理系统进展*。无，无，892–900。
- en: Bansal et al. (2004) Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004. Correlation
    clustering. *Machine learning* 56, 1-3 (2004), 89–113.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bansal 等 (2004) Nikhil Bansal, Avrim Blum, 和 Shuchi Chawla. 2004. 相关聚类。*机器学习*
    56, 1-3 (2004), 89–113。
- en: 'Belinkov et al. (2017) Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan
    Sajjad, and James Glass. 2017. What do Neural Machine Translation Models Learn
    about Morphology?. In *Proceedings of the 55th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*. Association for Computational
    Linguistics, Vancouver, Canada, 861–872.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belinkov 等 (2017) Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad,
    和 James Glass. 2017. 神经机器翻译模型对形态学的学习情况如何？见 *第55届计算语言学协会年会论文集（第1卷：长论文）*。计算语言学协会，加拿大温哥华，861–872。
- en: 'Ben-Nun et al. (2019) Tal Ben-Nun, Johannes de Fine Licht, Alexandros N Ziogas,
    Timo Schneider, and Torsten Hoefler. 2019. Stateful Dataflow Multigraphs: A data-centric
    model for performance portability on heterogeneous architectures. In *Proceedings
    of the International Conference for High Performance Computing, Networking, Storage
    and Analysis*. None, None, 1–14.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ben-Nun 等 (2019) Tal Ben-Nun, Johannes de Fine Licht, Alexandros N Ziogas,
    Timo Schneider 和 Torsten Hoefler. 2019. Stateful Dataflow Multigraphs: A data-centric
    model for performance portability on heterogeneous architectures. 在 *国际高性能计算、网络、存储和分析会议*
    上。无, 无, 1–14。'
- en: Blalock et al. (2020) Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle,
    and John Guttag. 2020. What is the state of neural network pruning? *Proceedings
    of machine learning and systems* 2, None (2020), 129–146.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blalock 等 (2020) Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle
    和 John Guttag. 2020. What is the state of neural network pruning? *机器学习与系统进展*
    2, 无 (2020), 129–146。
- en: Bobick and Davis (2001) Aaron F. Bobick and James W. Davis. 2001. The recognition
    of human movement using temporal templates. *IEEE Transactions on pattern analysis
    and machine intelligence* 23, 3 (2001), 257–267.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bobick 和 Davis (2001) Aaron F. Bobick 和 James W. Davis. 2001. The recognition
    of human movement using temporal templates. *IEEE模式分析与机器智能汇刊* 23, 3 (2001), 257–267。
- en: Campbell and Robson (1968) Fergus W Campbell and John G Robson. 1968. Application
    of Fourier analysis to the visibility of gratings. *The Journal of physiology*
    197, 3 (1968), 551.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Campbell 和 Robson (1968) Fergus W Campbell 和 John G Robson. 1968. Application
    of Fourier analysis to the visibility of gratings. *生理学杂志* 197, 3 (1968), 551。
- en: Carreira and Zisserman (2017) Joao Carreira and Andrew Zisserman. 2017. Quo
    vadis, action recognition? a new model and the kinetics dataset. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.
    None, None, 6299–6308.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carreira 和 Zisserman (2017) Joao Carreira 和 Andrew Zisserman. 2017. Quo vadis,
    action recognition? a new model and the kinetics dataset. 在 *IEEE/CVF计算机视觉与模式识别会议
    (CVPR)* 上。无, 无, 6299–6308。
- en: 'Cavigelli (2017) Lukas Cavigelli. 2017. Papers with Code. CBinfer: Change-Based
    Inference for Convolutional Neural Networks on Video Data. [https://paperswithcode.com/paper/cbinfer-change-based-inference-for](https://paperswithcode.com/paper/cbinfer-change-based-inference-for)
    (2021).'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cavigelli (2017) Lukas Cavigelli. 2017. Papers with Code. CBinfer: Change-Based
    Inference for Convolutional Neural Networks on Video Data. [https://paperswithcode.com/paper/cbinfer-change-based-inference-for](https://paperswithcode.com/paper/cbinfer-change-based-inference-for)
    (2021)。'
- en: 'Cavigelli (2018) Lukas Cavigelli. 2018. Papers with Code. CBinfer: Exploiting
    Frame-to-Frame Locality for Faster Convolutional Network Inference on Video Streams.
    [https://paperswithcode.com/paper/cbinfer-exploiting-frame-to-frame-locality](https://paperswithcode.com/paper/cbinfer-exploiting-frame-to-frame-locality)
    (2021).'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cavigelli (2018) Lukas Cavigelli. 2018. Papers with Code. CBinfer: Exploiting
    Frame-to-Frame Locality for Faster Convolutional Network Inference on Video Streams.
    [https://paperswithcode.com/paper/cbinfer-exploiting-frame-to-frame-locality](https://paperswithcode.com/paper/cbinfer-exploiting-frame-to-frame-locality)
    (2021)。'
- en: 'Cavigelli and Benini (2019) Lukas Cavigelli and Luca Benini. 2019. CBinfer:
    Exploiting frame-to-frame locality for faster convolutional network inference
    on video streams. *IEEE Transactions on Circuits and Systems for Video Technology*
    30, 5 (2019), 1451–1465.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cavigelli 和 Benini (2019) Lukas Cavigelli 和 Luca Benini. 2019. CBinfer: Exploiting
    frame-to-frame locality for faster convolutional network inference on video streams.
    *IEEE视频技术电路与系统汇刊* 30, 5 (2019), 1451–1465。'
- en: 'Cavigelli et al. (2017) Lukas Cavigelli, Philippe Degen, and Luca Benini. 2017.
    CBinfer: Change-Based Inference for Convolutional Neural Networks on Video Data.
    In *Proceedings of the 11th International Conference on Distributed Smart Cameras*
    (Stanford, CA, USA) *(ICDSC 2017)*. Association for Computing Machinery, New York,
    NY, USA, 1–8.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cavigelli 等 (2017) Lukas Cavigelli, Philippe Degen 和 Luca Benini. 2017. CBinfer:
    Change-Based Inference for Convolutional Neural Networks on Video Data. 在 *第11届国际分布式智能摄像头会议*
    (斯坦福, CA, USA) *(ICDSC 2017)* 中。计算机协会, 纽约, NY, USA, 1–8。'
- en: Chakraborty et al. (2019) Sinjan Chakraborty, Sayantan Paul, Ram Sarkar, and
    Mita Nasipuri. 2019. Feature map reduction in CNN for handwritten digit recognition.
    In *Recent Developments in Machine Learning and Data Analytics*. Springer, None,
    143–148.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakraborty 等 (2019) Sinjan Chakraborty, Sayantan Paul, Ram Sarkar 和 Mita Nasipuri.
    2019. Feature map reduction in CNN for handwritten digit recognition. 在 *机器学习与数据分析的最新进展*
    中。Springer, 无, 143–148。
- en: 'Chen et al. (2019b) Hesen Chen, Ming Lin, Xiuyu Sun, Qian Qi, Hao Li, and Rong
    Jin. 2019b. Muffnet: Multi-layer feature federation for mobile deep learning.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*.
    None, None, 0–0.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2019b) Hesen Chen, Ming Lin, Xiuyu Sun, Qian Qi, Hao Li 和 Rong Jin.
    2019b. Muffnet: Multi-layer feature federation for mobile deep learning. 在 *IEEE/CVF国际计算机视觉研讨会*
    上。无, 无, 0–0。'
- en: Chen et al. (2019c) Huixiang Chen, Mingcong Song, Jiechen Zhao, Yuting Dai,
    and Tao Li. 2019c. 3D-based video recognition acceleration by leveraging temporal
    locality. In *Proceedings of the 46th ACM/IEEE Annual International Symposium
    on Computer Architecture (ISCA)*. None, None, 79–90.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019c）惠翔·陈、明聪·宋、洁晨·赵、雨婷·戴和涛·李。2019c。利用时间局部性加速基于 3D 的视频识别。见于 *第 46 届 ACM/IEEE
    年度国际计算机架构研讨会（ISCA）论文集*。无，无，79–90。
- en: Chen et al. (2020a) Hanting Chen, Yunhe Wang, Han Shu, Yehui Tang, Chunjing
    Xu, Boxin Shi, Chao Xu, Qi Tian, and Chang Xu. 2020a. Frequency Domain Compact
    3D Convolutional Neural Networks. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*. None, None, 1641–1650.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020a）寒亭·陈、云鹤·王、韩·舒、叶辉·唐、春静·徐、博信·石、超·徐、齐·田和昌·徐。2020a。频域紧凑的 3D 卷积神经网络。见于 *IEEE/CVF
    计算机视觉与模式识别会议（CVPR）论文集*。无，无，1641–1650。
- en: Chen et al. (2018) Kai Chen, Jiaqi Wang, Shuo Yang, Xingcheng Zhang, Yuanjun
    Xiong, Chen Change Loy, and Dahua Lin. 2018. Optimizing Video Object Detection
    via a Scale-Time Lattice. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*. None, None, 7814–7823.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2018）凯·陈、佳琪·王、硕·杨、兴成·张、元俊·熊、陈·张忠和大华·林。2018。通过尺度时间格优化视频目标检测。见于 *IEEE/CVF 计算机视觉与模式识别会议（CVPR）论文集*。无，无，7814–7823。
- en: Chen et al. (2019d) Weijie Chen, Yuan Zhang, Di Xie, and Shiliang Pu. 2019d.
    A layer decomposition-recomposition framework for neuron pruning towards accurate
    lightweight networks. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 33\. None, None, 3355–3362.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019d）魏杰·陈、袁张、迪·谢和石良·蒲。2019d。用于神经元剪枝的层分解-重组框架，旨在实现准确的轻量级网络。见于 *AAAI 人工智能会议论文集*，第
    33 卷。无，无，3355–3362。
- en: 'Chen (2019) Yunpeng Chen. 2019. Papers with Code. Drop an Octave: Reducing
    Spatial Redundancy in Convolutional Neural Networks with Octave Convolution. [https://paperswithcode.com/paper/drop-an-octave-reducing-spatial-redundancy-in](https://paperswithcode.com/paper/drop-an-octave-reducing-spatial-redundancy-in)
    (2021).'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈（2019）云鹏·陈。2019。Papers with Code。丢弃一个八度：通过八度卷积减少卷积神经网络中的空间冗余。[https://paperswithcode.com/paper/drop-an-octave-reducing-spatial-redundancy-in](https://paperswithcode.com/paper/drop-an-octave-reducing-spatial-redundancy-in)（2021）。
- en: 'Chen et al. (2019a) Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis
    Kalantidis, Marcus Rohrbach, Shuicheng Yan, and Jiashi Feng. 2019a. Drop an octave:
    Reducing spatial redundancy in convolutional neural networks with octave convolution.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*.
    None, None, 3435–3444.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019a）云鹏·陈、浩琪·范、冰·徐、志成·阎、雅尼斯·卡兰迪迪斯、马库斯·罗尔巴赫、帅成·严和佳士·冯。2019a。丢弃一个八度：通过八度卷积减少卷积神经网络中的空间冗余。见于
    *IEEE/CVF 国际计算机视觉大会（ICCV）论文集*。无，无，3435–3444。
- en: 'Chen et al. (2016) Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, and Zhi Jin. 2016.
    Compressing Neural Language Models by Sparse Word Representations. In *Proceedings
    of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*. Association for Computational Linguistics, Berlin, Germany,
    226–235.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2016）云川·陈、丽丽·牟、颜·徐、戈·李和智·金。2016。通过稀疏词表示压缩神经语言模型。见于 *第 54 届计算语言学协会年会（第 1 卷：长篇论文）论文集*。计算语言学协会，德国柏林，226–235。
- en: 'Chen (2020) Zhuo Chen. 2020. Papers with Code. ViP: Virtual Pooling for Accelerating
    CNN-based Image Classification and Object Detection. [https://paperswithcode.com/paper/vip-virtual-pooling-for-accelerating-cnn](https://paperswithcode.com/paper/vip-virtual-pooling-for-accelerating-cnn)
    (2021).'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈（2020）卓·陈。2020。Papers with Code。ViP：虚拟池化以加速 CNN 基于的图像分类和目标检测。[https://paperswithcode.com/paper/vip-virtual-pooling-for-accelerating-cnn](https://paperswithcode.com/paper/vip-virtual-pooling-for-accelerating-cnn)（2021）。
- en: 'Chen et al. (2020b) Zhuo Chen, Jiyuan Zhang, Ruizhou Ding, and Diana Marculescu.
    2020b. Vip: Virtual pooling for accelerating cnn-based image classification and
    object detection. In *The IEEE Winter Conference on Applications of Computer Vision*.
    None, None, 1180–1189.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020b）卓·陈、纪元·张、瑞洲·丁和戴安娜·马库雷斯库。2020b。ViP：虚拟池化以加速基于 CNN 的图像分类和目标检测。见于 *IEEE
    冬季计算机视觉应用会议*。无，无，1180–1189。
- en: 'Cheng et al. (2018) Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2018. Model
    Compression and Acceleration for Deep Neural Networks: The Principles, Progress,
    and Challenges. *IEEE Signal Processing Magazine* 35, 1 (2018), 126–136.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成等（2018）余·成、多·王、潘·周和陶·张。2018。深度神经网络的模型压缩与加速：原理、进展与挑战。*IEEE 信号处理杂志* 35，1（2018），126–136。
- en: 'Chin et al. (2019) Ting-Wu Chin, Ruizhou Ding, and Diana Marculescu. 2019.
    AdaScale: Towards Real-time Video Object Detection using Adaptive Scaling. In
    *Proceedings of Machine Learning and Systems 2019*. None, None, 431–441.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chin 等（2019）Ting-Wu Chin、Ruizhou Ding 和 Diana Marculescu。2019。AdaScale：基于自适应缩放的实时视频对象检测。发表于
    *机器学习与系统2019年会议录*。无，无，431–441。
- en: Chitsaz et al. (2020) Kamran Chitsaz, Mohsen Hajabdollahi, Nader Karimi, Shadrokh
    Samavi, and Shahram Shirani. 2020. Acceleration of convolutional neural network
    using FFT-based split convolutions. *arXiv preprint arXiv:2003.12621* None, None
    (2020), 0–0.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chitsaz 等（2020）Kamran Chitsaz、Mohsen Hajabdollahi、Nader Karimi、Shadrokh Samavi
    和 Shahram Shirani。2020。使用基于FFT的分裂卷积加速卷积神经网络。*arXiv预印本 arXiv:2003.12621* 无，无（2020），0–0。
- en: Choudhary et al. (2020) Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and
    Jagannathan Sarangapani. 2020. A comprehensive survey on model compression and
    acceleration. *Artificial Intelligence Review* None, None (2020), 1–43.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choudhary 等（2020）Tejalal Choudhary、Vipul Mishra、Anurag Goswami 和 Jagannathan
    Sarangapani。2020。关于模型压缩和加速的全面调查。*人工智能评论* 无，无（2020），1–43。
- en: Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D.
    Manning. 2019. What Does BERT Look At? An Analysis of BERT’s Attention. In *BlackBoxNLP@ACL*.
    None, None, 0–0.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等（2019）Kevin Clark、Urvashi Khandelwal、Omer Levy 和 Christopher D. Manning。2019。BERT
    关注什么？BERT 注意力的分析。发表于 *BlackBoxNLP@ACL*。无，无，0–0。
- en: 'Cubuk et al. (2019) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan,
    and Quoc V Le. 2019. Autoaugment: Learning augmentation strategies from data.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. None, None, 113–123.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cubuk 等（2019）Ekin D Cubuk、Barret Zoph、Dandelion Mane、Vijay Vasudevan 和 Quoc
    V Le。2019。Autoaugment：从数据中学习数据增强策略。发表于 *IEEE/CVF计算机视觉与模式识别会议（CVPR）会议录*。无，无，113–123。
- en: Dai (2020) Zihang Dai. 2020. GitHub. Funnel-Transformer. [https://github.com/laiguokun/Funnel-Transformer](https://github.com/laiguokun/Funnel-Transformer)
    (2021).
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai（2020）Zihang Dai。2020。GitHub。Funnel-Transformer。 [https://github.com/laiguokun/Funnel-Transformer](https://github.com/laiguokun/Funnel-Transformer)（2021）。
- en: 'Dai et al. (2020) Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-Transformer:
    Filtering out Sequential Redundancy for Efficient Language Processing. In *Advances
    in Neural Information Processing Systems*. None, None, 0–0.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2020）Zihang Dai、Guokun Lai、Yiming Yang 和 Quoc Le。2020。Funnel-Transformer：过滤序列冗余以实现高效语言处理。发表于
    *神经信息处理系统进展*。无，无，0–0。
- en: Dalvi (2020) Fahim Dalvi. 2020. Papers with Code. Analyzing Redundancy in Pretrained
    Transformer Models. [https://paperswithcode.com/paper/exploiting-redundancy-in-pre-trained-language](https://paperswithcode.com/paper/exploiting-redundancy-in-pre-trained-language)
    (2021).
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalvi（2020）Fahim Dalvi。2020。Papers with Code。分析预训练变换器模型中的冗余。 [https://paperswithcode.com/paper/exploiting-redundancy-in-pre-trained-language](https://paperswithcode.com/paper/exploiting-redundancy-in-pre-trained-language)（2021）。
- en: Dalvi et al. (2019) Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov,
    Anthony Bau, and James Glass. 2019. What is one grain of sand in the desert? analyzing
    individual neurons in deep nlp models. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 33\. None, None, 6309–6317.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalvi 等（2019）Fahim Dalvi、Nadir Durrani、Hassan Sajjad、Yonatan Belinkov、Anthony
    Bau 和 James Glass。2019。沙漠中的一粒沙是什么？分析深度 NLP 模型中的单个神经元。发表于 *AAAI 人工智能大会会议录*，第 33
    卷。无，无，6309–6317。
- en: Dalvi et al. (2020) Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov.
    2020. Analyzing redundancy in pretrained transformer models. In *Proceedings of
    the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
    None, None, 4908–4926.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalvi 等（2020）Fahim Dalvi、Hassan Sajjad、Nadir Durrani 和 Yonatan Belinkov。2020。分析预训练变换器模型中的冗余。发表于
    *2020 年自然语言处理实证方法会议（EMNLP）会议录*。无，无，4908–4926。
- en: de Moura et al. (2019) Rafael Fão de Moura, Paulo C Santos, João Paulo C de
    Lima, Marco AZ Alves, Antonio CS Beck, and Luigi Carro. 2019. Skipping CNN convolutions
    through efficient memoization. In *International Conference on Embedded Computer
    Systems*. Springer, None, None, 65–76.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Moura 等（2019）Rafael Fão de Moura、Paulo C Santos、João Paulo C de Lima、Marco
    AZ Alves、Antonio CS Beck 和 Luigi Carro。2019。通过高效备忘录跳过 CNN 卷积。发表于 *嵌入式计算系统国际会议*。Springer，无，无，65–76。
- en: De Valois and De Valois (1980) Russell L De Valois and Karen K De Valois. 1980.
    Spatial vision. *Annual review of psychology* 31, 1 (1980), 309–341.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Valois 和 De Valois（1980）Russell L De Valois 和 Karen K De Valois。1980。空间视觉。*心理学年评*
    31，1（1980），309–341。
- en: 'Dosovitskiy et al. (2015) Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
    Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers,
    and Thomas Brox. 2015. Flownet: Learning optical flow with convolutional networks.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*.
    None, None, 2758–2766.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dosovitskiy et al. (2015) Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
    Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers
    和 Thomas Brox。 2015。 Flownet: Learning optical flow with convolutional networks。
    在*Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*。
    None, None, 2758–2766。'
- en: Du Tran et al. (2017) Heng Wang Du Tran, Lorenzo Torresani, Jamie Ray, Yann
    LeCun, and Manohar Paluri. 2017. A closer look at spatiotemporal convolutions
    for action recognition. 2018 IEEE. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. None, None, 6450–6459.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du Tran et al. (2017) Heng Wang Du Tran, Lorenzo Torresani, Jamie Ray, Yann
    LeCun 和 Manohar Paluri。 2017。 A closer look at spatiotemporal convolutions for
    action recognition。 2018 IEEE。 在*Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*。 None, None, 6450–6459。
- en: Embedding (2020) Word Embedding. 2020. Retrieved from:. [https://en.wikipedia.org/wiki/Word_embedding](https://en.wikipedia.org/wiki/Word_embedding)
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Embedding (2020) Word Embedding。 2020。 Retrieved from:. [https://en.wikipedia.org/wiki/Word_embedding](https://en.wikipedia.org/wiki/Word_embedding)
- en: 'Fan et al. (2017a) Hongxiang Fan, Xinyu Niu, Qiang Liu, and Wayne Luk. 2017a.
    F-c3d: Fpga-based 3-dimensional convolutional neural network. In *2017 27th International
    Conference on Field Programmable Logic and Applications (FPL)*. IEEE, None, None,
    1–4.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan et al. (2017a) Hongxiang Fan, Xinyu Niu, Qiang Liu 和 Wayne Luk。 2017a.
    F-c3d: Fpga-based 3-dimensional convolutional neural network。 在*2017 27th International
    Conference on Field Programmable Logic and Applications (FPL)*。 IEEE, None, None,
    1–4。'
- en: Fan et al. (2017b) Yang Fan, Fei Tian, Tao Qin, Jiang Bian, and Tie-Yan Liu.
    2017b. Neural data filter for boot-strapping stochastic gradient descent.. In
    *Proceedings of the 5th International Conference on Learning Representations (ICLR)*.
    None, None, 0–0.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan et al. (2017b) Yang Fan, Fei Tian, Tao Qin, Jiang Bian 和 Tie-Yan Liu。 2017b。
    Neural data filter for boot-strapping stochastic gradient descent.. 在*Proceedings
    of the 5th International Conference on Learning Representations (ICLR)*。 None,
    None, 0–0。
- en: 'Feichtenhofer (2020) Christoph Feichtenhofer. 2020. X3D: Expanding Architectures
    for Efficient Video Recognition. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*. None, None, 0–0.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feichtenhofer (2020) Christoph Feichtenhofer。 2020。 X3D: Expanding Architectures
    for Efficient Video Recognition。 在*Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*。 None, None, 0–0。'
- en: Feng et al. (2019) Shuo Feng, Huiyu Zhou, and Hongbiao Dong. 2019. Using deep
    neural network with small dataset to predict material defects. *Materials & Design*
    162 (2019), 300–310.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2019) Shuo Feng, Huiyu Zhou 和 Hongbiao Dong。 2019。 Using deep neural
    network with small dataset to predict material defects。 *Materials & Design* 162
    (2019), 300–310。
- en: Figurnov (2016) Michael Figurnov. 2016. GitHub. perforated-cnn-matconvnet. [https://github.com/mfigurnov/perforated-cnn-matconvnet](https://github.com/mfigurnov/perforated-cnn-matconvnet)
    (2021).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Figurnov (2016) Michael Figurnov。 2016。 GitHub。 perforated-cnn-matconvnet。 [https://github.com/mfigurnov/perforated-cnn-matconvnet](https://github.com/mfigurnov/perforated-cnn-matconvnet)
    (2021)。
- en: 'Figurnov et al. (2016) Mikhail Figurnov, Aizhan Ibraimova, Dmitry P Vetrov,
    and Pushmeet Kohli. 2016. Perforatedcnns: Acceleration through elimination of
    redundant convolutions. In *Advances in Neural Information Processing Systems*.
    None, None, 947–955.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Figurnov et al. (2016) Mikhail Figurnov, Aizhan Ibraimova, Dmitry P Vetrov
    和 Pushmeet Kohli。 2016。 Perforatedcnns: Acceleration through elimination of redundant
    convolutions。 在*Advances in Neural Information Processing Systems*。 None, None,
    947–955。'
- en: Gaikwad and El-Sharkawy (2019) Akash Sunil Gaikwad and Mohamed El-Sharkawy.
    2019. Pruning the Convolution Neural Network (SqueezeNet) based on L 2 Normalization
    of Activation Maps. In *2019 IEEE 9th Annual Computing and Communication Workshop
    and Conference (CCWC)*. IEEE, None, None, 0392–0396.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaikwad 和 El-Sharkawy (2019) Akash Sunil Gaikwad 和 Mohamed El-Sharkawy。 2019。
    Pruning the Convolution Neural Network (SqueezeNet) based on L 2 Normalization
    of Activation Maps。 在*2019 IEEE 9th Annual Computing and Communication Workshop
    and Conference (CCWC)*。 IEEE, None, None, 0392–0396。
- en: 'Ganesh et al. (2021) Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan,
    Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. 2021.
    Compressing large-scale transformer-based models: A case study on bert. *Transactions
    of the Association for Computational Linguistics* 9, None (2021), 1061–1080.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ganesh et al. (2021) Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan,
    Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen 和 Marianne Winslett。 2021。
    Compressing large-scale transformer-based models: A case study on bert。 *Transactions
    of the Association for Computational Linguistics* 9, None (2021), 1061–1080。'
- en: Gao et al. (2018) Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu, and Larry S
    Davis. 2018. Dynamic zoom-in network for fast object detection in large images.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. None, None, 6926–6935.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2018) Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu, 和 Larry S Davis.
    2018. 用于大图像快速目标检测的动态缩放网络。载于 *IEEE/CVF 计算机视觉与模式识别会议（CVPR）论文集*。无， 无，6926–6935。
- en: 'Gao (2019) Xitong Gao. 2019. Papers with Code. Dynamic Channel Pruning: Feature
    Boosting and Suppression. [https://paperswithcode.com/paper/dynamic-channel-pruning-feature-boosting-and](https://paperswithcode.com/paper/dynamic-channel-pruning-feature-boosting-and)
    (2021).'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao (2019) Xitong Gao. 2019. Papers with Code. 动态通道剪枝：特征提升与抑制。 [https://paperswithcode.com/paper/dynamic-channel-pruning-feature-boosting-and](https://paperswithcode.com/paper/dynamic-channel-pruning-feature-boosting-and)
    (2021)。
- en: 'Gao et al. (2019) Xitong Gao, Yiren Zhao, Lukasz Dudziak, Robert D. Mullins,
    and Cheng-Zhong Xu. 2019. Dynamic Channel Pruning: Feature Boosting and Suppression.
    In *Proceedings of the 7th International Conference on Learning Representations
    (ICLR)*. None, None, 0–0.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2019) Xitong Gao, Yiren Zhao, Lukasz Dudziak, Robert D. Mullins, 和 Cheng-Zhong
    Xu. 2019. 动态通道剪枝：特征提升与抑制。载于 *第七届国际学习表征会议（ICLR）论文集*。无， 无，0–0。
- en: Georgiadis (2019) Georgios Georgiadis. 2019. Accelerating Convolutional Neural
    Networks via Activation Map Compression. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. None, None, 7085–7095.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Georgiadis (2019) Georgios Georgiadis. 2019. 通过激活图压缩加速卷积神经网络。载于 *IEEE/CVF 计算机视觉与模式识别会议（CVPR）论文集*。无，
    无，7085–7095。
- en: Gionis et al. (1999) Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 1999.
    Similarity Search in High Dimensions via Hashing. In *Proceedings of the 25th
    International Conference on Very Large Data Bases* *(VLDB ’99)*. Morgan Kaufmann
    Publishers Inc., San Francisco, CA, USA, 518–529.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gionis 等人 (1999) Aristides Gionis, Piotr Indyk, 和 Rajeev Motwani. 1999. 通过哈希进行高维相似性搜索。载于
    *第 25 届国际大型数据库会议* *(VLDB ’99)*。Morgan Kaufmann Publishers Inc., 旧金山，加利福尼亚州，美国，518–529。
- en: 'Goyal (2020) Saurabh Goyal. 2020. Papers with Code. PoWER-BERT: Accelerating
    BERT Inference via Progressive Word-vector Elimination. [https://paperswithcode.com/paper/power-bert-accelerating-bert-inference-for](https://paperswithcode.com/paper/power-bert-accelerating-bert-inference-for)
    (2021).'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal (2020) Saurabh Goyal. 2020. Papers with Code. PoWER-BERT：通过渐进词向量消除加速 BERT
    推理。 [https://paperswithcode.com/paper/power-bert-accelerating-bert-inference-for](https://paperswithcode.com/paper/power-bert-accelerating-bert-inference-for)
    (2021)。
- en: 'Goyal et al. (2020) Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan
    Chakaravarthy, Yogish Sabharwal, and Ashish Verma. 2020. PoWER-BERT: Accelerating
    BERT Inference via Progressive Word-vector Elimination. In *Proceedings of the
    International Conference on Machine Learning*, Vol. 119\. None, None, 3690–3699.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 等人 (2020) Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan
    Chakaravarthy, Yogish Sabharwal, 和 Ashish Verma. 2020. PoWER-BERT：通过渐进词向量消除加速
    BERT 推理。载于 *国际机器学习会议论文集*，第 119 卷。无， 无，3690–3699。
- en: Graham (2014) Benjamin Graham. 2014. Fractional Max-Pooling. *CoRR* abs/1412.6071
    (2014), 0–0.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graham (2014) Benjamin Graham. 2014. 分数最大池化。 *CoRR* abs/1412.6071 (2014)，0–0。
- en: 'Guan et al. (2021) Hui Guan, Umang Chaudhary, Yuanchao Xu, Lin Ning, Lijun
    Zhang, and Xipeng Shen. 2021. Recurrent Neural Networks Meet Context-Free Grammar:
    Two Birds with One Stone. In *2021 IEEE International Conference on Data Mining
    (ICDM)*. None, None, 1078–1083.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guan 等人 (2021) Hui Guan, Umang Chaudhary, Yuanchao Xu, Lin Ning, Lijun Zhang,
    和 Xipeng Shen. 2021. 循环神经网络遇上上下文无关文法：一箭双雕。载于 *2021 年 IEEE 国际数据挖掘会议（ICDM）*。无， 无，1078–1083。
- en: Guo (2018) Yunhui Guo. 2018. A Survey on Methods and Theories of Quantized Neural
    Networks. *CoRR* abs/1808.04752 (2018), 0–0.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo (2018) Yunhui Guo. 2018. 量化神经网络的方法和理论综述。 *CoRR* abs/1808.04752 (2018)，0–0。
- en: 'Guo et al. (2020) Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and
    Mohammed Bennamoun. 2020. Deep learning for 3d point clouds: A survey. *IEEE transactions
    on pattern analysis and machine intelligence* None, None (2020), 0–0.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 (2020) Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, 和 Mohammed
    Bennamoun. 2020. 深度学习用于三维点云：综述。 *IEEE 模式分析与机器智能汇刊* 无，无 (2020)，0–0。
- en: Gupta et al. (2020) Manish Gupta, Vasudeva Varma, Sonam Damani, and Kedhar Nath
    Narahari. 2020. Compression of Deep Learning Models for NLP. In *Proceedings of
    the 29th ACM International Conference on Information & Knowledge Management*.
    None, None, 3507–3508.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人 (2020) Manish Gupta, Vasudeva Varma, Sonam Damani, 和 Kedhar Nath Narahari.
    2020. NLP 的深度学习模型压缩。载于 *第 29 届 ACM 国际信息与知识管理会议论文集*。无， 无，3507–3508。
- en: Han et al. (2016) Wei Han, Pooya Khorrami, Tom Le Paine, Prajit Ramachandran,
    Mohammad Babaeizadeh, Honghui Shi, Jianan Li, Shuicheng Yan, and Thomas S. Huang.
    2016. Seq-NMS for Video Object Detection. *CoRR* abs/1602.08465, None (2016),
    0–0.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人（2016）Wei Han, Pooya Khorrami, Tom Le Paine, Prajit Ramachandran, Mohammad
    Babaeizadeh, Honghui Shi, Jianan Li, Shuicheng Yan, 和 Thomas S. Huang. 2016. 用于视频目标检测的
    Seq-NMS。*CoRR* abs/1602.08465，无（2016），0–0。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*. None, None, 770–778.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2016）Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 2016. 深度残差学习用于图像识别。在
    *IEEE/CVF 计算机视觉与模式识别会议（CVPR）* 上发表。无，无，770–778。
- en: He et al. (2017) Yihui He, Xiangyu Zhang, and Jian Sun. 2017. Channel pruning
    for accelerating very deep neural networks. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*. None, None, 1389–1397.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2017）Yihui He, Xiangyu Zhang, 和 Jian Sun. 2017. 用于加速非常深的神经网络的通道修剪。在 *IEEE/CVF
    国际计算机视觉大会（ICCV）* 上发表。无，无，1389–1397。
- en: 'Hegde et al. (2018a) Kartik Hegde, Rohit Agrawal, Yulun Yao, and Christopher W
    Fletcher. 2018a. Morph: Flexible acceleration for 3D CNN-based video understanding.
    In *Proceedings of the 51th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)*. None, None, 933–946.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hegde 等人（2018a）Kartik Hegde, Rohit Agrawal, Yulun Yao, 和 Christopher W Fletcher.
    2018a. Morph: 3D CNN 基础的视频理解的灵活加速。在 *第51届 IEEE/ACM 微架构国际研讨会（MICRO）* 上发表。无，无，933–946。'
- en: 'Hegde et al. (2018b) Kartik Hegde, Jiyong Yu, Rohit Agrawal, Mengjia Yan, Michael
    Pellauer, and Christopher Fletcher. 2018b. Ucnn: Exploiting computational reuse
    in deep neural networks via weight repetition. In *Proceedings of the 45th ACM/IEEE
    Annual International Symposium on Computer Architecture (ISCA)*. None, None, 674–687.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hegde 等人（2018b）Kartik Hegde, Jiyong Yu, Rohit Agrawal, Mengjia Yan, Michael
    Pellauer, 和 Christopher Fletcher. 2018b. Ucnn: 通过权重重复利用计算重用于深度神经网络。在 *第45届 ACM/IEEE
    计算机架构年度国际研讨会（ISCA）* 上发表。无，无，674–687。'
- en: Hinton et al. (2014) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014. Distilling
    the Knowledge in a Neural Network. In *Advances in Neural Information Processing
    Systems Deep Learning Workshop*. None, None, 0–0.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等人（2014）Geoffrey Hinton, Oriol Vinyals, 和 Jeff Dean. 2014. 在神经网络中提取知识。在
    *神经信息处理系统进展 深度学习研讨会* 上发表。无，无，0–0。
- en: Hou and Kung (2020a) Zejiang Hou and Sun-Yuan Kung. 2020a. A Feature-map Discriminant
    Perspective for Pruning Deep Neural Networks. *CoRR* abs/2005.13796, None (2020),
    0–0.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou 和 Kung（2020a）Zejiang Hou 和 Sun-Yuan Kung. 2020a. 从特征图判别角度进行深度神经网络的修剪。*CoRR*
    abs/2005.13796，无（2020），0–0。
- en: Hou and Kung (2020b) Zejiang Hou and Sun-Yuan Kung. 2020b. Efficient Image Super
    Resolution Via Channel Discriminative Deep Neural Network Pruning. In *ICASSP
    2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*. IEEE, None, None, 3647–3651.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou 和 Kung（2020b）Zejiang Hou 和 Sun-Yuan Kung. 2020b. 通过通道区分深度神经网络修剪实现高效图像超分辨率。在
    *ICASSP 2020-2020 IEEE 国际声学、语音与信号处理会议（ICASSP）* 上发表。IEEE，无，无，3647–3651。
- en: 'Hu (2016) Hengyuan Hu. 2016. Papers with Code. Network Trimming: A Data-Driven
    Neuron Pruning Approach towards Efficient Deep Architectures. [https://paperswithcode.com/paper/network-trimming-a-data-driven-neuron-pruning](https://paperswithcode.com/paper/network-trimming-a-data-driven-neuron-pruning)
    (2021).'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu（2016）Hengyuan Hu. 2016. 论文与代码。网络修剪：一种数据驱动的神经元修剪方法以实现高效深度架构。 [https://paperswithcode.com/paper/network-trimming-a-data-driven-neuron-pruning](https://paperswithcode.com/paper/network-trimming-a-data-driven-neuron-pruning)
    （2021）。
- en: 'Hu et al. (2016) Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. 2016.
    Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep
    Architectures. *CoRR* abs/1607.03250, None (2016), 0–0.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2016）Hengyuan Hu, Rui Peng, Yu-Wing Tai, 和 Chi-Keung Tang. 2016. 网络修剪：一种数据驱动的神经元修剪方法以实现高效深度架构。*CoRR*
    abs/1607.03250，无（2016），0–0。
- en: Huang (2018) Zehao Huang. 2018. Papers with Code. Data-Driven Sparse Structure
    Selection for Deep Neural Networks. [https://paperswithcode.com/paper/data-driven-sparse-structure-selection-for](https://paperswithcode.com/paper/data-driven-sparse-structure-selection-for)
    (2021).
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang（2018）Zehao Huang. 2018. 论文与代码。数据驱动的稀疏结构选择用于深度神经网络。 [https://paperswithcode.com/paper/data-driven-sparse-structure-selection-for](https://paperswithcode.com/paper/data-driven-sparse-structure-selection-for)
    （2021）。
- en: Huang and Wang (2018) Zehao Huang and Naiyan Wang. 2018. Data-driven sparse
    structure selection for deep neural networks. In *Proceedings of the European
    Conference on Computer Vision (ECCV)*. None, None, 304–320.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang and Wang (2018) Zehao Huang and Naiyan Wang. 2018. 基于数据驱动的深度神经网络稀疏结构选择。在
    *欧洲计算机视觉会议（ECCV）论文集*。无, 无, 304–320.
- en: 'Hubara et al. (2017) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran
    El-Yaniv, and Yoshua Bengio. 2017. Quantized neural networks: Training neural
    networks with low precision weights and activations. *The Journal of Machine Learning
    Research* 18, 1 (2017), 6869–6898.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara et al. (2017) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
    and Yoshua Bengio. 2017. 量化神经网络：使用低精度权重和激活的神经网络训练。*机器学习研究杂志* 18, 1 (2017), 6869–6898.
- en: 'Iandola et al. (2016) Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf,
    Song Han, William J. Dally, and Kurt Keutzer. 2016. SqueezeNet: AlexNet-level
    accuracy with 50x fewer parameters and <1MB model size. *CoRR* abs/1602.07360,
    None (2016), 0–0.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iandola et al. (2016) Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf,
    Song Han, William J. Dally, and Kurt Keutzer. 2016. SqueezeNet：具有 50 倍更少参数和 <1MB
    模型大小的 AlexNet 级别准确性。*CoRR* abs/1602.07360, 无 (2016), 0–0.
- en: Ibrokhimov et al. (2020) Bunyodbek Ibrokhimov, Cheonghwan Hur, and Sanggil Kang.
    2020. Effective node selection technique towards sparse learning. *APPLIED INTELLIGENCE*
    None, None (2020), 0–0.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ibrokhimov et al. (2020) Bunyodbek Ibrokhimov, Cheonghwan Hur, and Sanggil Kang.
    2020. 针对稀疏学习的有效节点选择技术。*应用智能* 无, 无 (2020), 0–0.
- en: 'Ivanov et al. (2021) Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li,
    and Torsten Hoefler. 2021. Data Movement Is All You Need: A Case Study on Optimizing
    Transformers. In *Proceedings of Machine Learning and Systems*, Vol. 3\. None,
    None, 711–732.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivanov et al. (2021) Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li,
    and Torsten Hoefler. 2021. 数据移动就是你所需要的：优化变换器的案例研究。在 *机器学习与系统会议论文集*，第 3 卷。无, 无,
    711–732.
- en: Ji et al. (2013) S. Ji, W. Xu, M. Yang, and K. Yu. 2013. 3D Convolutional Neural
    Networks for Human Action Recognition. *IEEE Transactions on Pattern Analysis
    and Machine Intelligence* 35, 1 (2013), 221–231.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2013) S. Ji, W. Xu, M. Yang, and K. Yu. 2013. 用于人类动作识别的 3D 卷积神经网络。*IEEE
    模式分析与机器智能汇刊* 35, 1 (2013), 221–231.
- en: Jiang et al. (2017) Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, and Shih-Fu
    Chang. 2017. Exploiting feature and class relationships in video categorization
    with regularized deep neural networks. *IEEE transactions on pattern analysis
    and machine intelligence* 40, 2 (2017), 352–364.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2017) Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, and Shih-Fu
    Chang. 2017. 在视频分类中利用特征和类别关系，通过正则化的深度神经网络。*IEEE 模式分析与机器智能汇刊* 40, 2 (2017), 352–364.
- en: Jiao et al. (2018) Xun Jiao, Vahideh Akhlaghi, Yu Jiang, and Rajesh K Gupta.
    2018. Energy-efficient neural networks using approximate computation reuse. In
    *2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE,
    None, None, 1223–1228.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao et al. (2018) Xun Jiao, Vahideh Akhlaghi, Yu Jiang, and Rajesh K Gupta.
    2018. 使用近似计算重用的节能神经网络。在 *2018 欧洲设计、自动化与测试会议及展览（DATE）*。IEEE, 无, 无, 1223–1228.
- en: Kang (2017a) Daniel Kang. 2017a. GitHub. NoScope. [https://github.com/stanford-futuredata/noscope](https://github.com/stanford-futuredata/noscope)
    (2021).
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang (2017a) Daniel Kang. 2017a. GitHub. NoScope. [https://github.com/stanford-futuredata/noscope](https://github.com/stanford-futuredata/noscope)
    (2021).
- en: 'Kang et al. (2017a) Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis,
    and Matei Zaharia. 2017a. NoScope: Optimizing Neural Network Queries over Video
    at Scale. *Proc. VLDB Endow.* 10, 11 (Aug. 2017), 1586–1597.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kang et al. (2017a) Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis,
    and Matei Zaharia. 2017a. NoScope: 大规模优化神经网络视频查询。*Proc. VLDB Endow.* 10, 11 (2017年8月),
    1586–1597.'
- en: 'Kang (2017b) Kai Kang. 2017b. GitHub. TPN: Tubelet Proposal Network. [https://github.com/myfavouritekk/TPN](https://github.com/myfavouritekk/TPN)
    (2021).'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kang (2017b) Kai Kang. 2017b. GitHub. TPN: Tubelet Proposal Network. [https://github.com/myfavouritekk/TPN](https://github.com/myfavouritekk/TPN)
    (2021).'
- en: Kang et al. (2017b) Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie
    Yan, Xihui Liu, and Xiaogang Wang. 2017b. Object Detection in Videos with Tubelet
    Proposal Networks. *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)* None, None (Jul 2017), 0–0.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang et al. (2017b) Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie
    Yan, Xihui Liu, and Xiaogang Wang. 2017b. 使用 Tubelet Proposal 网络的视频目标检测。*IEEE/CVF
    计算机视觉与模式识别会议（CVPR）论文集* 无, 无 (2017年7月), 0–0.
- en: 'Kang et al. (2018) Kai Kang, Hongsheng Li, Junjie Yan, Xingyu Zeng, Bin Yang,
    Tong Xiao, Cong Zhang, Zhe Wang, Ruohui Wang, Xiaogang Wang, and et al. 2018.
    T-CNN: Tubelets With Convolutional Neural Networks for Object Detection From Videos.
    *IEEE Transactions on Circuits and Systems for Video Technology* 28, 10 (Oct 2018),
    2896–2907.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kang et al. (2018) Kai Kang, Hongsheng Li, Junjie Yan, Xingyu Zeng, Bin Yang,
    Tong Xiao, Cong Zhang, Zhe Wang, Ruohui Wang, Xiaogang Wang, 等. 2018. T-CNN: 利用卷积神经网络进行视频中物体检测的
    Tubelets. *IEEE 视频技术电路与系统学报* 28, 10 (2018年10月), 2896–2907。'
- en: Karpathy et al. (2014) Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
    Leung, Rahul Sukthankar, and Li Fei-Fei. 2014. Large-scale Video Classification
    with Convolutional Neural Networks. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. None, None, 0–0.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy et al. (2014) Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
    Leung, Rahul Sukthankar, 和 Li Fei-Fei. 2014. 大规模视频分类与卷积神经网络. 见 *IEEE/CVF 计算机视觉与模式识别会议
    (CVPR) 论文集*。无，无，0–0。
- en: 'Kim et al. (2020b) Sangyeob Kim, Juhyoung Lee, Sanghoon Kang, Jinsu Lee, and
    Hoi-Jun Yoo. 2020b. A Power-Efficient CNN Accelerator With Similar Feature Skipping
    for Face Recognition in Mobile Devices. *IEEE Transactions on Circuits and Systems
    I: Regular Papers* 67, 4 (2020), 1181–1193.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim et al. (2020b) Sangyeob Kim, Juhyoung Lee, Sanghoon Kang, Jinsu Lee, 和
    Hoi-Jun Yoo. 2020b. 一种具有类似特征跳过机制的高效 CNN 加速器用于移动设备中的面部识别. *IEEE 电路与系统学报 I: 定期论文*
    67, 4 (2020), 1181–1193。'
- en: Kim et al. (2020a) Yeachan Kim, Kang-Min Kim, and SangKeun Lee. 2020a. Adaptive
    compression of word embeddings. In *Proceedings of the 58th annual meeting of
    the association for computational linguistics*. None, None, 3950–3959.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2020a) Yeachan Kim, Kang-Min Kim, 和 SangKeun Lee. 2020a. 词嵌入的自适应压缩.
    见 *第58届计算语言学协会年会论文集*。无，无，3950–3959。
- en: 'Kitaev (2020) Nikita Kitaev. 2020. Papers with Code. Reformer: The Efficient
    Transformer. [https://paperswithcode.com/paper/reformer-the-efficient-transformer-1](https://paperswithcode.com/paper/reformer-the-efficient-transformer-1)
    (2021).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kitaev (2020) Nikita Kitaev. 2020. Papers with Code. Reformer: 高效的 Transformer.
    [https://paperswithcode.com/paper/reformer-the-efficient-transformer-1](https://paperswithcode.com/paper/reformer-the-efficient-transformer-1)
    (2021)。'
- en: 'Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020.
    Reformer: The Efficient Transformer. In *Proceedings of the 8th International
    Conference on Learning Representations (ICLR)*. None, None, 0–0.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, 和 Anselm Levskaya. 2020.
    Reformer: 高效的 Transformer. 见 *第八届国际学习表征会议 (ICLR) 论文集*。无，无，0–0。'
- en: Kopuklu et al. (2019) Okan Kopuklu, Neslihan Kose, Ahmet Gunduz, and Gerhard
    Rigoll. 2019. Resource efficient 3d convolutional neural networks. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops*.
    None, None, 0–0.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kopuklu et al. (2019) Okan Kopuklu, Neslihan Kose, Ahmet Gunduz, 和 Gerhard Rigoll.
    2019. 资源高效的 3D 卷积神经网络. 见 *IEEE/CVF 国际计算机视觉会议 (ICCV) 工作坊论文集*。无，无，0–0。
- en: 'Korbar et al. (2019) Bruno Korbar, Du Tran, and Lorenzo Torresani. 2019. Scsampler:
    Sampling salient clips from video for efficient action recognition. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision (ICCV)*. None, None,
    6232–6242.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Korbar et al. (2019) Bruno Korbar, Du Tran, 和 Lorenzo Torresani. 2019. Scsampler:
    从视频中采样显著片段以提高动作识别效率. 见 *IEEE/CVF 国际计算机视觉会议 (ICCV) 论文集*。无，无，6232–6242。'
- en: 'Kwon et al. (2019) Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer, Angshuman
    Parashar, Vivek Sarkar, and Tushar Krishna. 2019. Understanding Reuse, Performance,
    and Hardware Cost of DNN Dataflow: A Data-Centric Approach. In *Proceedings of
    the 52th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)*
    (Columbus, OH, USA) *(MICRO ’52)*. Association for Computing Machinery, New York,
    NY, USA, 754–768.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kwon et al. (2019) Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer, Angshuman
    Parashar, Vivek Sarkar, 和 Tushar Krishna. 2019. 理解 DNN 数据流的重用、性能和硬件成本: 一种数据中心方法.
    见 *第52届 IEEE/ACM 微架构国际研讨会 (MICRO)*（美国俄亥俄州哥伦布市）*(MICRO ’52)*. 计算机协会, 纽约, NY, USA,
    754–768。'
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised
    Learning of Language Representations. In *Proceedings of the 8th International
    Conference on Learning Representations (ICLR)*. None, None, 0–0.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, 和 Radu Soricut. 2020. ALBERT: 一种轻量级的 BERT 用于自监督语言表示学习. 见 *第八届国际学习表征会议
    (ICLR) 论文集*。无，无，0–0。'
- en: 'Lee (2020) Seulki Lee. 2020. GitHub. [RTAS 2020] SubFlow: A Dynamic Induced-Subgraph
    Strategy Toward Real-Time DNN Inference and Training. [https://github.com/learning1234embed/SubFlow](https://github.com/learning1234embed/SubFlow)
    (2021).'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee (2020) Seulki Lee. 2020. GitHub. [RTAS 2020] SubFlow: 一种面向实时 DNN 推断和训练的动态诱导子图策略。
    [https://github.com/learning1234embed/SubFlow](https://github.com/learning1234embed/SubFlow)
    (2021)。'
- en: 'Lee and Nirjon (2020) Seulki Lee and Shahriar Nirjon. 2020. SubFlow: A Dynamic
    Induced-Subgraph Strategy Toward Real-Time DNN Inference and Training. In *2020
    IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)*. IEEE,
    None, None, 15–29.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee and Nirjon (2020) Seulki Lee 和 Shahriar Nirjon. 2020. SubFlow: 一种面向实时 DNN
    推断和训练的动态诱导子图策略。在 *2020 IEEE 实时与嵌入技术及应用研讨会（RTAS）* 中。IEEE，None，None，15–29。'
- en: Li (2019) Hongyang Li. 2019. GitHub. Zoom-out-and-in Network for region proposal
    and object detection. [https://github.com/hli2020/zoom_network](https://github.com/hli2020/zoom_network)
    (2021).
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li (2019) Hongyang Li. 2019. GitHub. 用于区域提议和目标检测的缩放网络。 [https://github.com/hli2020/zoom_network](https://github.com/hli2020/zoom_network)
    (2021)。
- en: Li et al. (2019b) Hongyang Li, Yu Liu, Wanli Ouyang, and Xiaogang Wang. 2019b.
    Zoom out-and-in network with map attention decision for region proposal and object
    detection. *International Journal of Computer Vision* 127, 3 (2019), 225–238.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019b) Hongyang Li, Yu Liu, Wanli Ouyang, 和 Xiaogang Wang. 2019b.
    带有地图注意力决策的缩放网络用于区域提议和目标检测。*计算机视觉国际杂志* 127, 3 (2019)，225–238。
- en: Li et al. (2020) Hang Li, Chen Ma, Wei Xu, and Xue Liu. 2020. Feature Statistics
    Guided Efficient Filter Pruning. In *Proceedings of the Twenty-Ninth International
    Joint Conference on Artificial Intelligence, IJCAI-20*. International Joint Conferences
    on Artificial Intelligence Organization, None, 2619–2625. [https://doi.org/10.24963/ijcai.2020/363](https://doi.org/10.24963/ijcai.2020/363)
    Main track.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2020) Hang Li, Chen Ma, Wei Xu, 和 Xue Liu. 2020. 特征统计引导的高效滤波器剪枝。在
    *第二十九届国际人工智能联合会议论文集，IJCAI-20* 中。国际人工智能联合会议组织，None，2619–2625。 [https://doi.org/10.24963/ijcai.2020/363](https://doi.org/10.24963/ijcai.2020/363)
    主轨道。
- en: Li et al. (2019a) Yuchao Li, Shaohui Lin, Baochang Zhang, Jianzhuang Liu, David
    Doermann, Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2019a. Exploiting kernel
    sparsity and entropy for interpretable CNN compression. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*. None,
    None, 2800–2809.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019a) Yuchao Li, Shaohui Lin, Baochang Zhang, Jianzhuang Liu, David
    Doermann, Yongjian Wu, Feiyue Huang, 和 Rongrong Ji. 2019a. 利用核稀疏性和熵进行可解释的 CNN
    压缩。在 *IEEE/CVF 计算机视觉与模式识别会议（CVPR）论文集* 中。None，None，2800–2809。
- en: Lin et al. (2014) Min Lin, Qiang Chen, and Shuicheng Yan. 2014. Network In Network.
    In *Proceedings of the 2nd International Conference on Learning Representations
    (ICLR)*. None, None, 0–0.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2014) Min Lin, Qiang Chen, 和 Shuicheng Yan. 2014. 网络中的网络。在 *第二届国际学习表示会议（ICLR）论文集*
    中。None，None，0–0。
- en: LinkViz (2020) LinkViz. 2020. Retrieved from:. [https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats5cc01b214e59](https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats5cc01b214e59)
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LinkViz (2020) LinkViz. 2020. 获取自： [https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats5cc01b214e59](https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats5cc01b214e59)
- en: 'Liu et al. (2008) Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and William T
    Freeman. 2008. Sift flow: Dense correspondence across different scenes. In *Proceedings
    of the European Conference on Computer Vision (ECCV)*. None, None, 28–42.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2008) Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, 和 William
    T Freeman. 2008. Sift 流：不同场景之间的密集对应关系。在 *欧洲计算机视觉会议（ECCV）论文集* 中。None，None，28–42。
- en: Liu (2017) Mason Liu. 2017. Papers with Code. Mobile Video Object Detection
    with Temporally-Aware Feature Maps. [https://paperswithcode.com/paper/mobile-video-object-detection-with-temporally](https://paperswithcode.com/paper/mobile-video-object-detection-with-temporally)
    (2021).
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu (2017) Mason Liu. 2017. Papers with Code. 带有时间感知特征图的移动视频目标检测。 [https://paperswithcode.com/paper/mobile-video-object-detection-with-temporally](https://paperswithcode.com/paper/mobile-video-object-detection-with-temporally)
    (2021)。
- en: Liu (2018) Peter J. Liu. 2018. Papers with Code. Generating Wikipedia by Summarizing
    Long Sequences. , 0–0 pages. [https://paperswithcode.com/paper/generating-wikipedia-by-summarizing-long](https://paperswithcode.com/paper/generating-wikipedia-by-summarizing-long)
    (2021).
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu (2018) Peter J. Liu. 2018. Papers with Code. 通过总结长序列生成维基百科。0–0 页。 [https://paperswithcode.com/paper/generating-wikipedia-by-summarizing-long](https://paperswithcode.com/paper/generating-wikipedia-by-summarizing-long)
    (2021)。
- en: Liu et al. (2018a) Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan
    Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018a. Generating Wikipedia by Summarizing
    Long Sequences. In *Proceedings of the 6th International Conference on Learning
    Representations (ICLR)*. None, None, 0–0.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2018a）Peter J Liu、Mohammad Saleh、Etienne Pot、Ben Goodrich、Ryan Sepassi、Lukasz
    Kaiser 和 Noam Shazeer。2018a。通过总结长序列生成维基百科。发表于 *第六届国际学习表征会议论文集（ICLR）*。无，无，0–0。
- en: 'Liu et al. (2016) Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy,
    Scott Reed, Cheng-Yang Fu, and Alexander C Berg. 2016. Ssd: Single shot multibox
    detector. In *Proceedings of the European Conference on Computer Vision (ECCV)*.
    Springer, None, None, 21–37.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2016）Wei Liu、Dragomir Anguelov、Dumitru Erhan、Christian Szegedy、Scott Reed、Cheng-Yang
    Fu 和 Alexander C Berg。2016。SSD：单次多框检测器。发表于 *欧洲计算机视觉会议论文集（ECCV）*。Springer，无，无，21–37。
- en: Liu et al. (2018b) Zhenhua Liu, Jizheng Xu, Xiulian Peng, and Ruiqin Xiong.
    2018b. Frequency-domain dynamic pruning for convolutional neural networks. In
    *Advances in Neural Information Processing Systems*. None, None, 1043–1053.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2018b）Zhenhua Liu、Jizheng Xu、Xiulian Peng 和 Ruiqin Xiong。2018b。用于卷积神经网络的频域动态剪枝。发表于
    *神经信息处理系统进展*。无，无，1043–1053。
- en: 'Ma et al. (2020) Dongning Ma, Xunzhao Yin, Michael Niemier, X Sharon Hu, and
    Xun Jiao. 2020. AxR-NN: Approximate Computation Reuse for Energy-Efficient Convolutional
    Neural Networks. In *Proceedings of the 2020 on Great Lakes Symposium on VLSI*.
    None, None, 363–368.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2020）Dongning Ma、Xunzhao Yin、Michael Niemier、X Sharon Hu 和 Xun Jiao。2020。AxR-NN：用于节能卷积神经网络的近似计算重用。发表于
    *2020年大湖区VLSI研讨会论文集*。无，无，363–368。
- en: 'Mao et al. (2019) Huizi Mao, Taeyoung Kong, and Bill Dally. 2019. CaTDet: Cascaded
    Tracked Detector for Efficient Object Detection from Video. In *Proceedings of
    Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April
    2, 2019*. mlsys.org, None, 0–0.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等（2019）Huizi Mao、Taeyoung Kong 和 Bill Dally。2019。CaTDet：用于高效视频物体检测的级联跟踪检测器。发表于
    *2019年机器学习与系统会议，MLSys 2019，斯坦福，加州，美国，2019年3月31日 - 4月2日*。mlsys.org，无，0–0。
- en: Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen
    heads really better than one?. In *Advances in Neural Information Processing Systems*.
    None, None, 14014–14024.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michel 等（2019）Paul Michel、Omer Levy 和 Graham Neubig。2019。十六个头真的比一个更好吗？。发表于 *神经信息处理系统进展*。无，无，14014–14024。
- en: Mocerino et al. (2019) Luca Mocerino, Valerio Tenace, and Andrea Calimera. 2019.
    Energy-efficient convolutional neural networks via recurrent data reuse. In *2019
    Design, Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE, None,
    None, 848–853.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mocerino 等（2019）Luca Mocerino、Valerio Tenace 和 Andrea Calimera。2019。通过递归数据重用实现节能卷积神经网络。发表于
    *2019年欧洲设计、自动化与测试会议与展览（DATE）*。IEEE，无，无，848–853。
- en: 'Ning et al. (2019) L. Ning, H. Guan, and X. Shen. 2019. Adaptive Deep Reuse:
    Accelerating CNN Training on the Fly. In *2019 IEEE 35th International Conference
    on Data Engineering (ICDE)*. None, None, 1538–1549.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ning 等（2019）L. Ning、H. Guan 和 X. Shen。2019。自适应深度重用：实时加速CNN训练。发表于 *2019年IEEE第35届国际数据工程会议（ICDE）*。无，无，1538–1549。
- en: 'Ning and Shen (2019) Lin Ning and Xipeng Shen. 2019. Deep reuse: streamline
    CNN inference on the fly via coarse-grained computation reuse. In *Proceedings
    of the ACM International Conference on Supercomputing*. None, None, 438–448.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ning 和 Shen（2019）Lin Ning 和 Xipeng Shen。2019。深度重用：通过粗粒度计算重用实时优化CNN推理。发表于 *ACM国际超级计算大会论文集*。无，无，438–448。
- en: of Redundancy by Oxford Dictionary (2020) Definition of Redundancy by Oxford Dictionary.
    2020. Retrieved from:. [https://www.lexico.com/en/definition/redundancy](https://www.lexico.com/en/definition/redundancy)
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oxford Dictionary（2020）《Oxford Dictionary》中“冗余”的定义。2020年。取自：[https://www.lexico.com/en/definition/redundancy](https://www.lexico.com/en/definition/redundancy)
- en: 'Ouyang et al. (2015) Wanli Ouyang, Xiaogang Wang, Xingyu Zeng, Shi Qiu, Ping
    Luo, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Chen-Change Loy, and Xiaoou
    Tang. 2015. DeepID-Net: Deformable deep convolutional neural networks for object
    detection. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*. None, None, 2403–2412.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等（2015）Wanli Ouyang、Xiaogang Wang、Xingyu Zeng、Shi Qiu、Ping Luo、Yonglong
    Tian、Hongsheng Li、Shuo Yang、Zhe Wang、Chen-Change Loy 和 Xiaoou Tang。2015。DeepID-Net：用于物体检测的可变形深度卷积神经网络。发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集（CVPR）*。无，无，2403–2412。
- en: Park and Kim (2019) Keunyoung Park and Doo-Hyun Kim. 2019. Accelerating image
    classification using feature map similarity in convolutional neural networks.
    *Applied Sciences* 9, 1 (2019), 108.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 和 Kim（2019）Keunyoung Park 和 Doo-Hyun Kim。2019。利用卷积神经网络中的特征图相似性加速图像分类。*应用科学*
    9, 1（2019），108。
- en: 'Penny and Henson (2006) W Penny and R Henson. 2006. Analysis of variance. *Statistical
    parametric mapping: The analysis of functional brain images* None, None (2006),
    166–177.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penny 和 Henson（2006）W Penny 和 R Henson。2006。方差分析。*统计参数映射：功能脑图像分析* 无，无（2006），166–177。
- en: Pérez-Cruz (2009) Fernando Pérez-Cruz. 2009. Estimation of information theoretic
    measures for continuous random variables. In *Advances in Neural Information Processing
    Systems*. None, None, 1257–1264.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pérez-Cruz（2009）Fernando Pérez-Cruz。2009。连续随机变量的信息理论度量估计。见于 *神经信息处理系统进展*。无，无，1257–1264。
- en: Pirsiavash and Ramanan (2012) Hamed Pirsiavash and Deva Ramanan. 2012. Detecting
    activities of daily living in first-person camera views. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*. IEEE,
    None, None, 2847–2854.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pirsiavash 和 Ramanan（2012）Hamed Pirsiavash 和 Deva Ramanan。2012。检测第一人称视角中的日常活动。见于
    *IEEE/CVF 计算机视觉与模式识别会议（CVPR）论文集*。IEEE，无，无，2847–2854。
- en: Piyasena et al. (2019) Duvindu Piyasena, Rukshan Wickramasinghe, Debdeep Paul,
    Siew-Kei Lam, and Meiqing Wu. 2019. Reducing dynamic power in streaming cnn hardware
    accelerators by exploiting computational redundancies. In *2019 29th International
    Conference on Field Programmable Logic and Applications (FPL)*. IEEE, None, None,
    354–359.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piyasena 等人（2019）Duvindu Piyasena、Rukshan Wickramasinghe、Debdeep Paul、Siew-Kei
    Lam 和 Meiqing Wu。2019。通过利用计算冗余减少流式 CNN 硬件加速器中的动态功耗。见于 *2019 年第 29 届现场可编程逻辑与应用国际会议（FPL）*。IEEE，无，无，354–359。
- en: 'Qin et al. (2020) Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan
    Song, and Nicu Sebe. 2020. Binary neural networks: A survey. *Pattern Recognition*
    None, None (2020), 107281.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人（2020）Haotong Qin、Ruihao Gong、Xianglong Liu、Xiao Bai、Jingkuan Song 和 Nicu
    Sebe。2020。二值神经网络：综述。*模式识别* 无，无（2020），107281。
- en: 'Razlighi et al. (2017) Mohammad Samragh Razlighi, Mohsen Imani, Farinaz Koushanfar,
    and Tajana Rosing. 2017. Looknn: Neural network with no multiplication. In *Design,
    Automation & Test in Europe Conference & Exhibition (DATE), 2017*. IEEE, None,
    None, 1775–1780.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Razlighi 等人（2017）Mohammad Samragh Razlighi、Mohsen Imani、Farinaz Koushanfar 和
    Tajana Rosing。2017。Looknn：无乘法神经网络。见于 *2017 年设计、自动化与测试欧洲会议与展览（DATE）*。IEEE，无，无，1775–1780。
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.
    Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.
    In *Advances in Neural Information Processing Systems*, Vol. 28. Curran Associates,
    Inc., None, 0–0.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人（2015）Shaoqing Ren、Kaiming He、Ross Girshick 和 Jian Sun。2015。Faster R-CNN：基于区域提议网络的实时目标检测。见于
    *神经信息处理系统进展*，第 28 卷。Curran Associates, Inc.，无，0–0。
- en: Sainath and Parada (2015) Tara N Sainath and Carolina Parada. 2015. Convolutional
    neural networks for small-footprint keyword spotting. In *Sixteenth Annual Conference
    of the International Speech Communication Association*. None, None, 0–0.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sainath 和 Parada（2015）Tara N Sainath 和 Carolina Parada。2015。用于小型关键词检测的卷积神经网络。见于
    *国际语音通信协会第十六届年会*。无，无，0–0。
- en: 'Salamat et al. (2018) Sahand Salamat, Mohsen Imani, Sarangh Gupta, and Tajana
    Rosing. 2018. Rnsnet: In-memory neural network acceleration using residue number
    system. In *2018 IEEE International Conference on Rebooting Computing (ICRC)*.
    IEEE, None, None, 1–12.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salamat 等人（2018）Sahand Salamat、Mohsen Imani、Sarangh Gupta 和 Tajana Rosing。2018。Rnsnet：使用剩余数系统的内存神经网络加速。见于
    *2018 年 IEEE 计算重启国际会议（ICRC）*。IEEE，无，无，1–12。
- en: 'Samal et al. (2020) Kruttidipta Samal, Marilyn Wolf, and Saibal Mukhopadhyay.
    2020. Attention-based Activation Pruning to Reduce Data Movement in Real-time
    AI: A Case-study on Local Motion Planning in Autonomous Vehicles. *IEEE Journal
    on Emerging and Selected Topics in Circuits and Systems* None, None (2020), 0–0.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samal 等人（2020）Kruttidipta Samal、Marilyn Wolf 和 Saibal Mukhopadhyay。2020。基于注意力的激活剪枝以减少实时
    AI 中的数据移动：以自主车辆中的局部运动规划为案例研究。*IEEE 电路与系统新兴和精选主题期刊* 无，无（2020），0–0。
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper
    and lighter. In *5th Workshop on Energy Efficient Machine Learning and Cognitive
    Computing NeurIPS 2019*. None, None, 0–0.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等人（2019）Victor Sanh、Lysandre Debut、Julien Chaumond 和 Thomas Wolf。2019。DistilBERT，BERT
    的精简版：更小、更快、更便宜、更轻便。见于 *第五届节能机器学习与认知计算研讨会 NeurIPS 2019*。无，无，0–0。
- en: 'Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
    Neural Machine Translation of Rare Words with Subword Units. In *Proceedings of
    the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*. Association for Computational Linguistics, Berlin, Germany,
    1715–1725.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sennrich 等人（2016）Rico Sennrich、Barry Haddow 和 Alexandra Birch。2016年。使用子词单元的稀有词汇的神经机器翻译。在
    *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*。计算语言学协会，德国柏林，1715–1725。'
- en: Shen et al. (2018) Junzhong Shen, You Huang, Zelong Wang, Yuran Qiao, Mei Wen,
    and Chunyuan Zhang. 2018. Towards a uniform template-based architecture for accelerating
    2D and 3D CNNs on FPGA. In *Proceedings of the 2018 ACM/SIGDA International Symposium
    on Field-Programmable Gate Arrays*. None, None, 97–106.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人（2018）Junzhong Shen、You Huang、Zelong Wang、Yuran Qiao、Mei Wen 和 Chunyuan
    Zhang。2018年。朝着统一的基于模板的架构推进，加速 FPGA 上的 2D 和 3D CNNs。在 *Proceedings of the 2018
    ACM/SIGDA International Symposium on Field-Programmable Gate Arrays*。无，无，97–106。
- en: Shi et al. (2016) Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does string-based
    neural MT learn source syntax?. In *Proceedings of the 2016 Conference on Empirical
    Methods in Natural Language Processing*. None, None, 1526–1534.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2016）Xing Shi、Inkit Padhi 和 Kevin Knight。2016年。基于字符串的神经机器翻译是否学习了源语言语法？
    在 *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*。无，无，1526–1534。
- en: 'Shomron (2019) Gil Shomron. 2019. GitHub. Thanks for Nothing: Predicting Zero-Valued
    Activations with Lightweight Convolutional Neural Networks. [https://github.com/gilshm/zap](https://github.com/gilshm/zap)
    (2021).'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shomron（2019）Gil Shomron。2019年。GitHub。谢天谢地：用轻量级卷积神经网络预测零值激活。[https://github.com/gilshm/zap](https://github.com/gilshm/zap)（2021）。
- en: 'Shomron et al. (2020) Gil Shomron, Ron Banner, Moran Shkolnik, and Uri Weiser.
    2020. Thanks for nothing: Predicting zero-valued activations with lightweight
    convolutional neural networks. In *Proceedings of the European Conference on Computer
    Vision (ECCV)*. Springer, None, None, 234–250.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shomron 等人（2020）Gil Shomron、Ron Banner、Moran Shkolnik 和 Uri Weiser。2020年。谢天谢地：用轻量级卷积神经网络预测零值激活。在
    *Proceedings of the European Conference on Computer Vision (ECCV)*。Springer，无，无，234–250。
- en: Shorten and Khoshgoftaar (2019) Connor Shorten and Taghi M Khoshgoftaar. 2019.
    A survey on image data augmentation for deep learning. *Journal of Big Data* 6,
    1 (2019), 1–48.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shorten 和 Khoshgoftaar（2019）Connor Shorten 和 Taghi M Khoshgoftaar。2019年。关于深度学习的图像数据增强的调查。*Journal
    of Big Data* 6，1（2019），1–48。
- en: Simons and Lee (2019) Taylor Simons and Dah-Jye Lee. 2019. A review of binarized
    neural networks. *Electronics* 8, 6 (2019), 661.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simons 和 Lee（2019）Taylor Simons 和 Dah-Jye Lee。2019年。二值神经网络的综述。*Electronics*
    8，6（2019），661。
- en: Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. 2014. Two-Stream
    Convolutional Networks for Action Recognition in Videos. In *Advances in Neural
    Information Processing Systems*. Curran Associates, Inc., None, 568–576.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman（2014）Karen Simonyan 和 Andrew Zisserman。2014年。用于视频动作识别的双流卷积网络。在
    *Advances in Neural Information Processing Systems*。Curran Associates, Inc.，无，568–576。
- en: Simonyan and Zisserman (2015) Karen Simonyan and Andrew Zisserman. 2015. Very
    Deep Convolutional Networks for Large-Scale Image Recognition. In *Proceedings
    of the 3rd International Conference on Learning Representations (ICLR)*. None,
    None, 0–0.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman（2015）Karen Simonyan 和 Andrew Zisserman。2015年。用于大规模图像识别的非常深的卷积网络。在
    *Proceedings of the 3rd International Conference on Learning Representations (ICLR)*。无，无，0–0。
- en: 'Singh et al. (2019) Arshdeep Singh, Padmanabhan Rajan, and Arnav Bhavsar. 2019.
    Deep hidden analysis: A statistical framework to prune feature maps. In *ICASSP
    2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*. IEEE, None, None, 820–824.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人（2019）Arshdeep Singh、Padmanabhan Rajan 和 Arnav Bhavsar。2019年。深度隐藏分析：修剪特征图的统计框架。在
    *ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*。IEEE，无，无，820–824。
- en: 'Soomro et al. (2012) Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012.
    UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild. *CoRR*
    abs/1212.0402, None (2012), 0–0.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soomro 等人（2012）Khurram Soomro、Amir Roshan Zamir 和 Mubarak Shah。2012年。UCF101：一个包含101个人类动作类别的视频数据集。*CoRR*
    abs/1212.0402，无（2012），0–0。
- en: 'Su and Grauman (2016) Yu-Chuan Su and Kristen Grauman. 2016. Leaving some stones
    unturned: dynamic feature prioritization for activity detection in streaming video.
    In *European Conference on Computer Vision*. Springer, None, None, 783–800.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 和 Grauman（2016）Yu-Chuan Su 和 Kristen Grauman。2016年。留下一些未解决的问题：流视频中的动态特征优先级调整。
    在 *European Conference on Computer Vision*。Springer，无，无，783–800。
- en: 'Suzuki et al. (2020) Taiji Suzuki, Hiroshi Abe, Tomoya Murata, Shingo Horiuchi,
    Kotaro Ito, Tokuma Wachi, So Hirai, Masatoshi Yukishima, and Tomoaki Nishimura.
    2020. Spectral Pruning: Compressing Deep Neural Networks via Spectral Analysis
    and its Generalization Error. In *IJCAI*. None, None, 0–0.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzuki 等人（2020）铃木泰司、安部博史、村田智哉、堀内慎悟、伊藤幸太郎、和智和子、平井奏、雪岛正敏和西村智明。2020。谱剪枝：通过谱分析压缩深度神经网络及其泛化误差。载于*IJCAI*。无，无，0–0。
- en: Torralba and Efros (2011) Antonio Torralba and Alexei A Efros. 2011. Unbiased
    look at dataset bias. In *CVPR 2011*. IEEE, None, None, 1521–1528.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torralba 和 Efros（2011）Antonio Torralba 和 Alexei A Efros。2011。对数据集偏差的无偏见看法。载于*CVPR
    2011*。IEEE，无，无，1521–1528。
- en: Tran et al. (2015) Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
    and Manohar Paluri. 2015. Learning Spatiotemporal Features with 3D Convolutional
    Networks. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision (ICCV)*. None, None, 4489–4497.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等人（2015）杜·Tran、Lubomir Bourdev、Rob Fergus、Lorenzo Torresani 和 Manohar Paluri。2015。使用
    3D 卷积网络学习时空特征。载于*IEEE/CVF 国际计算机视觉会议（ICCV）论文集*。无，无，4489–4497。
- en: Urban et al. (2017) Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou,
    Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose,
    and Matt Richardson. 2017. Do deep convolutional nets really need to be deep and
    convolutional?. In *Proceedings of the 5th International Conference on Learning
    Representations (ICLR)*. None, None, 0–0.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Urban 等人（2017）Gregor Urban、Krzysztof J Geras、Samira Ebrahimi Kahou、Ozlem Aslan、Shengjie
    Wang、Rich Caruana、Abdelrahman Mohamed、Matthai Philipose 和 Matt Richardson。2017。深度卷积网络真的需要深度和卷积吗？载于*第五届国际学习表征会议（ICLR）论文集*。无，无，0–0。
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. 2019. Analyzing Multi-Head Self-Attention: Specialized Heads Do
    the Heavy Lifting, the Rest Can Be Pruned. In *Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics*. Association for Computational
    Linguistics, Florence, Italy, 5797–5808.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita 等人（2019）Elena Voita、David Talbot、Fedor Moiseev、Rico Sennrich 和 Ivan Titov。2019。分析多头自注意力：专用头承担主要任务，其余可以被剪枝。载于*第57届计算语言学协会年会论文集*。计算语言学协会，意大利佛罗伦萨，5797–5808。
- en: Wang et al. (2018) Dong Wang, Lei Zhou, Xueni Zhang, Xiao Bai, and Jun Zhou.
    2018. Exploring Linear Relationship in Feature Map Subspace for ConvNets Compression.
    *CoRR* abs/1803.05729, None (2018), 0–0.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2018）董王、周磊、张雪妮、白晓和周军。2018。探索特征图子空间中的线性关系用于 ConvNets 压缩。*CoRR* abs/1803.05729，无（2018），0–0。
- en: Wang et al. (2017) Hai Wang, Mengjun Shao, Yan Liu, and Wei Zhao. 2017. Enhanced
    efficiency 3D convolution based on optimal FPGA accelerator. *IEEE Access* 5,
    None (2017), 6909–6916.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2017）王海、邵梦君、刘艳和赵伟。2017。基于最优 FPGA 加速器的增强效率 3D 卷积。*IEEE Access* 5，无（2017），6909–6916。
- en: Wang et al. (2019a) Ying Wang, Shengwen Liang, Huawei Li, and Xiaowei Li. 2019a.
    A None-Sparse Inference Accelerator that Distills and Reuses the Computation Redundancy
    in CNNs. In *Proceedings of the 56th Annual Design Automation Conference 2019*.
    None, None, 1–6.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2019a）王颖、梁胜文、李华为和李晓伟。2019a。一种非稀疏推断加速器，提取并重用 CNN 中的计算冗余。载于*2019年第56届年设计自动化会议论文集*。无，无，1–6。
- en: 'Wang et al. (2019b) Yongchen Wang, Ying Wang, Huawei Li, Cong Shi, and Xiaowei
    Li. 2019b. Systolic Cube: A Spatial 3D CNN Accelerator Architecture for Low Power
    Video Analysis. In *Proceedings of the 56th Annual Design Automation Conference
    2019*. None, None, 1–6.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2019b）王永晨、王颖、李华为、施聪和李晓伟。2019b。Systolic Cube: 一种用于低功耗视频分析的空间 3D CNN
    加速器架构。载于*2019年第56届年设计自动化会议论文集*。无，无，1–6。'
- en: Wang et al. (2020) Ying Wang, Yongchen Wang, Cong Shi, Long Cheng, Huawei Li,
    and Xiaowei Li. 2020. An Edge 3D CNN Accelerator for Low Power Activity Recognition.
    *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems*
    None, None (2020), 0–0.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2020）王颖、王永晨、施聪、程龙、李华为和李晓伟。2020。用于低功耗活动识别的边缘 3D CNN 加速器。*IEEE计算机辅助设计与系统集成杂志*
    无，无（2020），0–0。
- en: 'Wang et al. (2016) Yunhe Wang, Chang Xu, Shan You, Dacheng Tao, and Chao Xu.
    2016. Cnnpack: Packing convolutional neural networks in the frequency domain.
    In *Advances in Neural Information Processing Systems*. None, None, 253–261.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2016）王云鹤、徐昌、游珊、陶达成和徐超。2016。Cnnpack: 在频域中打包卷积神经网络。载于*神经信息处理系统进展*。无，无，253–261。'
- en: Wu et al. (2019a) Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, and Shilei
    Wen. 2019a. Multi-agent reinforcement learning based frame sampling for effective
    untrimmed video recognition. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision (ICCV)*. None, None, 6222–6231.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2019a）吴文浩、贺东亮、谭晓、陈世锋、温时磊。2019a。基于多智能体强化学习的帧采样，用于有效的未修剪视频识别。在*IEEE/CVF国际计算机视觉会议（ICCV）论文集*。无，无，6222–6231。
- en: Wu et al. (2020) Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi
    Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks.
    *IEEE Transactions on Neural Networks and Learning Systems* None, None (2020),
    0–0.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2020）吴宗汉、潘世睿、陈凤文、龙国栋、张成奇、S·余·菲利普。2020。图神经网络的全面调查。*IEEE神经网络与学习系统汇刊* 无，无（2020），0–0。
- en: 'Wu et al. (2019b) Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and
    Larry S Davis. 2019b. Adaframe: Adaptive frame selection for fast video recognition.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. None, None, 1278–1287.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2019b）吴祖轩、熊才铭、马志尧、理查德·索彻、拉里·S·戴维斯。2019b。Adaframe：用于快速视频识别的自适应帧选择。在*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*。无，无，1278–1287。
- en: Yang et al. (2016) Bin Yang, Junjie Yan, Zhen Lei, and Stan Z Li. 2016. Craft
    objects from images. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*. None, None, 6043–6051.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等（2016）杨斌、阎俊杰、雷真、李斯坦。2016。基于图像的工艺品制作。在*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*。无，无，6043–6051。
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
    Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining
    for language understanding. In *Advances in Neural Information Processing Systems*.
    None, None, 5753–5763.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等（2019）杨志林、戴自航、杨奕鸣、哈梅·卡本内尔、拉斯·萨拉赫丁诺夫、黎奎克。2019。Xlnet：用于语言理解的广义自回归预训练。在*神经信息处理系统进展*。无，无，5753–5763。
- en: Yeung (2015) Serena Yeung. 2015. GitHub. End-to-end Learning of Action Detection
    from Frame Glimpses in Videos. [https://github.com/syyeung/frameglimpses](https://github.com/syyeung/frameglimpses)
    (2021).
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨（2015）谢丽娜·杨。2015。GitHub。端到端的视频帧闪烁动作检测学习。[https://github.com/syyeung/frameglimpses](https://github.com/syyeung/frameglimpses)（2021）。
- en: Yeung et al. (2016) Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei.
    2016. End-to-end learning of action detection from frame glimpses in videos. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. None, None, 2678–2687.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等（2016）谢丽娜·杨、奥尔加·鲁萨科夫斯基、格雷格·莫里、李飞飞。2016。从视频帧闪烁中进行端到端的动作检测学习。在*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*。无，无，2678–2687。
- en: 'Yu et al. (2020) Fuxun Yu, Chenchen Liu, Di Wang, Yanzhi Wang, and Xiang Chen.
    2020. AntiDote: attention-based dynamic optimization for neural network runtime
    efficiency. In *2020 Design, Automation & Test in Europe Conference & Exhibition
    (DATE)*. IEEE, None, None, 951–956.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 于等（2020）于复迅、刘晨晨、王迪、王燕之、陈翔。2020. AntiDote：基于注意力的神经网络运行时效率动态优化。在*2020欧洲设计、自动化与测试会议及展览（DATE）*。IEEE，无，无，951–956。
- en: 'Zhang et al. (2017) Shihao Zhang, Weiyao Lin, Ping Lu, Weihua Li, and Shuo
    Deng. 2017. Kill two birds with one stone: Boosting both object detection accuracy
    and speed with adaptive patch-of-interest composition. In *2017 IEEE International
    Conference on Multimedia & Expo Workshops (ICMEW)*. IEEE, None, None, 447–452.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2017）张石浩、林伟尧、陆平、李伟华、邓硕。2017。一石二鸟：通过自适应感兴趣区域组合提升目标检测的准确性和速度。在*2017 IEEE国际多媒体与展览研讨会（ICMEW）*。IEEE，无，无，447–452。
- en: Zheng et al. (2017) Jian Zheng, Wei Yang, and Xiaohua Li. 2017. Training data
    reduction in deep neural networks with partial mutual information based feature
    selection and correlation matching based active learning. In *2017 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, None, None,
    2362–2366.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2017）郑建、杨伟、李晓华。2017。通过基于部分互信息的特征选择和基于相关匹配的主动学习减少深度神经网络中的训练数据。在*2017 IEEE国际声学、语音与信号处理会议（ICASSP）*。IEEE，无，无，2362–2366。
- en: Zhou et al. (2019) Yuefu Zhou, Ya Zhang, Yanfeng Wang, and Qi Tian. 2019. Accelerate
    cnn via recursive bayesian pruning. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*. None, None, 3306–3315.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等（2019）周岳夫、张雅、王彦峰、田奇。2019。通过递归贝叶斯剪枝加速CNN。在*IEEE/CVF国际计算机视觉会议（ICCV）论文集*。无，无，3306–3315。
- en: Zhu and Liu (2018) Menglong Zhu and Mason Liu. 2018. Mobile Video Object Detection
    with Temporally-Aware Feature Maps. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. None, None, 5686–5695.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 和 Liu (2018) Menglong Zhu 和 Mason Liu. 2018. 基于时间感知特征图的移动视频物体检测。发表于*IEEE/CVF计算机视觉与模式识别会议
    (CVPR)*。无，无，5686–5695。
- en: Zhu (2017) Xizhou Zhu. 2017. Papers with Code. Towards High Performance Video
    Object Detection for Mobiles. [https://paperswithcode.com/paper/towards-high-performance-video-object](https://paperswithcode.com/paper/towards-high-performance-video-object)
    (2021).
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu (2017) Xizhou Zhu. 2017. Papers with Code. 面向移动设备的高性能视频物体检测。 [https://paperswithcode.com/paper/towards-high-performance-video-object](https://paperswithcode.com/paper/towards-high-performance-video-object)
    (2021)。
- en: Zhu et al. (2018) Xizhou Zhu, Jifeng Dai, Lu Yuan, and Yichen Wei. 2018. Towards
    high performance video object detection. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. None, None, 7210–7218.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 (2018) Xizhou Zhu, Jifeng Dai, Lu Yuan, 和 Yichen Wei. 2018. 迈向高性能视频物体检测。发表于*IEEE/CVF计算机视觉与模式识别会议
    (CVPR)*。无，无，7210–7218。
- en: Zhu et al. (2017a) Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, and Yichen Wei.
    2017a. Flow-guided feature aggregation for video object detection. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision (ICCV)*. None, None,
    408–417.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 (2017a) Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, 和 Yichen Wei. 2017a.
    基于流引导的特征聚合用于视频物体检测。发表于*IEEE/CVF国际计算机视觉会议 (ICCV)*。无，无，408–417。
- en: Zhu et al. (2017b) Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen
    Wei. 2017b. Deep feature flow for video recognition. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*. None, None, 2349–2358.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 (2017b) Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, 和 Yichen Wei. 2017b.
    用于视频识别的深度特征流。发表于*IEEE/CVF计算机视觉与模式识别会议 (CVPR)*。无，无，2349–2358。
- en: Zhuang et al. (2018) Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu,
    Yong Guo, Qingyao Wu, Junzhou Huang, and Jinhui Zhu. 2018. Discrimination-aware
    channel pruning for deep neural networks. In *Advances in Neural Information Processing
    Systems*. None, None, 875–886.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang 等 (2018) Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong
    Guo, Qingyao Wu, Junzhou Huang, 和 Jinhui Zhu. 2018. 面向深度神经网络的判别感知通道剪枝。发表于*神经信息处理系统进展*。无，无，875–886。
