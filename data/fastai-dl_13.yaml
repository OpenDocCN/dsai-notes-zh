- en: 'Deep Learning 2: Part 2 Lesson 13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习2：第2部分第13课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0)'
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自*[*fast.ai 课程*](http://www.fast.ai/)*的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*[*Jeremy*](https://twitter.com/jeremyphoward)
    *和*[*Rachel*](https://twitter.com/math_rachel) *给了我这个学习的机会。*'
- en: '[Forum](http://forums.fast.ai/t/lesson-13-discussion-and-wiki/15297/1) / [Video](https://youtu.be/xXXiC4YRGrQ)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[论坛](http://forums.fast.ai/t/lesson-13-discussion-and-wiki/15297/1) / [视频](https://youtu.be/xXXiC4YRGrQ)'
- en: Image enhancement — we’ll cover things like this painting that you might be
    familiar with. However, you might not have noticed before that this painting of
    an eagle in it. The reason you may not have noticed that before is this painting
    didn’t used to have an eagle in it. By the same token, the painting on the first
    slide did not used to have Captain America’s shield on it either.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图像增强 - 我们将涵盖您可能熟悉的这幅画。然而，您可能之前没有注意到这幅画中有一只鹰。您之前可能没有注意到的原因是这幅画以前没有鹰。同样地，第一张幻灯片上的画以前也没有美国队长的盾牌。
- en: This is a cool new paper that just came out a couple of days ago called [Deep
    Painterly Harmonization](https://arxiv.org/abs/1804.03189) and it uses almost
    exactly the technique we are going to learn in this lesson with some minor tweaks.
    But you can see the basic idea is to take one picture pasted on top of another
    picture, and then use some kind of approach to combine the two. The approach is
    called a “style transfer”.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一篇很酷的新论文，几天前刚发表，名为[Deep Painterly Harmonization](https://arxiv.org/abs/1804.03189)，它几乎完全使用了我们将在本课程中学习的技术，只是进行了一些微小的调整。但您可以看到基本思想是将一张图片粘贴在另一张图片上，然后使用某种方法将两者结合起来。这种方法被称为“风格转移”。
- en: 'Before we talk about that, I wanted to mention this really cool contribution
    by William Horton who added this stochastic weight averaging technique to the
    fastai library that is now all merged and ready to go. He’s written a whole post
    about that which I strongly recommend you check out not just because stochastic
    weight averaging lets you get higher performance from your existing neural network
    with basically no extra work (it’s as simple as adding two parameters to your
    fit function: `use_swa`, `swa_start`) but also he’s described his process of building
    this and how he tested it and how he contributed to the library. So I think it’s
    interesting if you are interested in doing something like this. I think William
    had not built this kind of library before so he describes how he did it.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论之前，我想提一下William Horton的这个非常酷的贡献，他将这种随机权重平均技术添加到了fastai库中，现在已经全部合并并准备就绪。他写了一整篇关于这个的文章，我强烈建议您查看，不仅因为随机权重平均让您可以从现有的神经网络中获得更高的性能，而且基本上不需要额外的工作（只需向您的fit函数添加两个参数：`use_swa`，`swa_start`），而且他描述了他构建这个过程以及他如何测试它以及他如何为库做出贡献。所以如果您有兴趣做类似的事情，我认为这很有趣。我认为William以前没有建立过这种类型的库，所以他描述了他是如何做到的。
- en: '[https://medium.com/@hortonhearsafoo/adding-a-cutting-edge-deep-learning-training-technique-to-the-fast-ai-library-2cd1dba90a49](/@hortonhearsafoo/adding-a-cutting-edge-deep-learning-training-technique-to-the-fast-ai-library-2cd1dba90a49)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/@hortonhearsafoo/adding-a-cutting-edge-deep-learning-training-technique-to-the-fast-ai-library-2cd1dba90a49](/@hortonhearsafoo/adding-a-cutting-edge-deep-learning-training-technique-to-the-fast-ai-library-2cd1dba90a49)'
- en: TrainPhase [[2:01](https://youtu.be/xXXiC4YRGrQ?t=2m1s)]
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TrainPhase [[2:01](https://youtu.be/xXXiC4YRGrQ?t=2m1s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/training_phase.ipynb)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/training_phase.ipynb)'
- en: Another vert cool contribution to the fastai library is a new train phase API.
    And I’m going to do something I’ve never done before which is I’m going to present
    somebody else’s notebook. The reason I haven’t done it before is because I haven’t
    liked any notebooks enough to think they are worth presenting it, but Sylvain
    has done a fantastic job here of not just creating this new API but also creating
    a beautiful notebook describing what it is and how it works and so forth. The
    background here is as you guys know we’ve been trying to train networks faster,
    partly as part of this Dawn bench competition and also for a reason that you’ll
    learn about next week. I mentioned on the forum last week it would be really handy
    for our experiments if we had an easier way to try out different learning rate
    schedules etc, and I laid out an API that I had in mind as it’d be really cool
    if somebody could write this because I am going to bed now and I kind of need
    it by tomorrow. And Sylvain replied on the forum well that sounds like a good
    challenge and by 24 hours later, it was done and it’s been super cool. I want
    to take you through it because it’s going to allow you to research things that
    nobody has tried before.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: fastai库的另一个非常酷的贡献是一个新的训练阶段API。我将做一件我以前从未做过的事情，那就是我将展示别人的笔记本。之前我没有这样做的原因是因为我没有喜欢到足够好的笔记本，认为值得展示，但Sylvain在这里做得非常出色，不仅创建了这个新API，还创建了一个描述它是什么以及如何工作等等的精美笔记本。背景是，正如大家所知，我们一直在努力更快地训练网络，部分原因是作为这个Dawn
    bench竞赛的一部分，还有一个下周您将了解的原因。我上周在论坛上提到，如果我们有一个更容易尝试不同的学习率调度等的方法，那对我们的实验将非常方便，我提出了我心目中的API，如果有人能写出来那将非常酷，因为我现在要睡觉了，明天我有点需要它。Sylvain在论坛上回复说，听起来是一个不错的挑战，24小时后，它就完成了，而且效果非常酷。我想带您了解一下，因为它将使您能够研究以前没有人尝试过的东西。
- en: It’s called the TrainPhase API [[3:32](https://youtu.be/xXXiC4YRGrQ?t=3m32s)]
    and the easiest way to show it is to show an example of what it does. Here is
    an iteration against learning rate chart as you are familiar with seeing. This
    is one where we train for a while at the learning rate of 0.01 and then we train
    for a while at the learning rate of 0.001\. I actually wanted to create something
    very much like that learning rate chart because most people that trained ImageNet
    use this stepwise approach and it’s actually not something that’s built into fastai
    because it’s not generally something we recommend. But in order to replicate existing
    papers, I wanted to do it the same way. So rather than writing a number of fit,
    fit, fit calls with different learning rates, it would be nice to be able to say
    train for n epochs at this learning rate and then m epochs at that learning rate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为TrainPhase API，最简单的方法是展示它的示例。这是一个迭代学习率图表，你应该很熟悉。我们在学习率为0.01的情况下训练一段时间，然后在学习率为0.001的情况下训练一段时间。我实际上想创建一个非常类似于学习率图表的东西，因为大多数训练ImageNet的人都使用这种分阶段的方法，而这实际上并不是fastai内置的，因为我们通常不建议这样做。但为了复制现有的论文，我想以同样的方式做。因此，与其写一系列不同学习率的fit、fit、fit调用，不如能够说在这个学习率下训练n个周期，然后在那个学习率下训练m个周期。
- en: 'So here is how you do that:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你如何做到的：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A phase is a period of training with particular optimizer parameters and `phases`
    consist of a number of training phase objects. A training phase object says how
    many epochs to train for, what optimization function to use, and what learning
    rate amongst other things that we will see. Here, you’ll see the two training
    phases that you just saw on that graph. So now, rather than calling `learn.fit`,
    you say:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个阶段是一个具有特定优化器参数的训练期，`phases`由许多训练阶段对象组成。一个训练阶段对象说明要训练多少个周期，要使用什么优化函数，以及其他我们将看到的东西。在这里，你会看到你刚刚在那张图上看到的两个训练阶段。所以现在，不再调用`learn.fit`，而是说：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In other words, `learn.fit` with an optimizer scheduler with these phases.
    From there, most of the things you pass in can just get sent across to the fit
    function as per usual, so most of the usual parameter will work fine. Generally
    speaking, we can just use these training phases and you will see it fits in a
    usual way. Then when you say `plot_lr` you will see the graphs above. Not only
    does it plot the learning rate, it also plots momentum, and for each phase, it
    tells you what optimizer it used. You can turn off the printing of the optimizers
    (`show_text=False`), you can turn off the printing of momentums (`show_moms=False`),
    and you can do other little things like a training phase could have a `lr_decay`
    parameter [[5:47](https://youtu.be/xXXiC4YRGrQ?t=5m47s) ]:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，`learn.fit`与一个具有这些阶段的优化器调度器。大多数传递的参数都可以像往常一样传递给fit函数，所以大多数通常的参数都可以正常工作。一般来说，我们只需使用这些训练阶段，你会看到它以一种通常的方式适应。然后当你说`plot_lr`时，你会看到上面的图表。它不仅绘制学习率，还绘制动量，并且对于每个阶段，它告诉你使用了什么优化器。你可以关闭优化器的打印（`show_text=False`），你可以关闭动量的打印（`show_moms=False`），你还可以做其他一些小事情，比如一个训练阶段可以有一个`lr_decay`参数：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'So here is a fixed learning rate, then a linear decay learning rate, and then
    a fixed learning rate which gives up this picture:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个固定的学习率，然后是线性衰减的学习率，然后是放弃这个图像的固定学习率：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This might be quite a good way to train because we know at high learning rates,
    you get to explore better, and at low learning rates, you get to fine-tune better.
    And it’s probably better to gradually slide between the two. So this actually
    isn’t a bad approach, I suspect.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个很好的训练方式，因为我们知道在高学习率下，你可以更好地探索，在低学习率下，你可以更好地微调。逐渐在两者之间滑动可能更好。所以我认为这实际上不是一个坏方法。
- en: 'You can use other decay types such as cosine [[6:25](https://youtu.be/xXXiC4YRGrQ?t=6m25s)]:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用其他衰减类型，比如余弦：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This probably makes even more sense as a genuinely potentially useful learning
    rate annealing shape.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能更有意义，作为一个真正有用的学习率退火形状。
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Exponential which is super popular approach:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 指数，这是一个非常流行的方法：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Polynomial which isn’t terribly popular but actually in the literature works
    better than just about anything else, but seems to have been largely ignored.
    So polynomial is good to be aware of. And what Sylvain has done is he’s given
    us the formula for each of these curves. So with a polynomial, you get to pick
    what polynomial to use. I believe p of 0.9 is the one I’ve seen really good results
    for — FYI.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式并不是非常流行，但实际上在文献中比其他任何方法都要好，但似乎已经被大多数人忽视了。所以多项式是值得注意的。Sylvain已经为每个曲线给出了公式。因此，使用多项式，你可以选择使用哪个多项式。我相信p为0.9的多项式是我看到的效果非常好的一个
    - FYI。
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you don’t give a tuple of learning rates when there is an LR decay, then
    it will decay all the way down to zero [[7:26](https://youtu.be/xXXiC4YRGrQ?t=7m26s)].
    And as you can see, you can happily start the next cycle at a different point.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在LR衰减时不提供学习率的元组，那么它将一直衰减到零。如你所见，你可以愉快地从不同的点开始下一个周期。
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: SGDR [[7:43](https://youtu.be/xXXiC4YRGrQ?t=7m43s)]
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SGDR
- en: So the cool thing is, now we can replicate all of our existing schedules using
    nothing but these training phases . So here is a function called `phases_sgdr`
    which does SGDR using the new training phase API.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以酷的是，现在我们可以仅仅使用这些训练阶段来复制所有我们现有的计划。这里有一个名为`phases_sgdr`的函数，它使用新的训练阶段API来进行SGDR。
- en: '[PRE9]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'So you can see, if he runs this schedule, here is what it looks like:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到，如果他按照这个计划运行，这就是它的样子：
- en: He’s even done the little trick I have where you’re training at really low learning
    rate just for a little bit and then pop up and do a few cycles, and the cycles
    are increasing in length [[8:05](https://youtu.be/xXXiC4YRGrQ?t=8m5s)]. And that’s
    all done in a single function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 他甚至做了我训练时使用非常低的学习率一小段时间然后突然增加并进行几个周期的小技巧，而且这些周期的长度在增加[[8:05](https://youtu.be/xXXiC4YRGrQ?t=8m5s)]。而且这一切都在一个函数中完成。
- en: 1cycle [[8:20](https://youtu.be/xXXiC4YRGrQ?t=8m20s)]
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1cycle
- en: The new 1cycle we can now implement with, again, a single little function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用一个小函数来实现新的1cycle。
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So if we fit with that, we get this triangle followed by a little flatter bit
    and the momentum is a cool thing — the momentum has a momentum decay. And in the
    third TrainingPhase, we have a fixed momentum. So it’s doing the momentum and
    the learning rate at the same time.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们符合这个，我们会得到这个三角形，然后是一个稍微平坦的部分，动量是一个很酷的东西 - 动量有一个动量衰减。在第三个训练阶段，我们有一个固定的动量。所以它同时处理动量和学习率。
- en: Discriminative learning rates + 1cycle [[8:53](https://youtu.be/xXXiC4YRGrQ?t=8m53s)]
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分学习率+ 1cycle
- en: So something that I haven’t tried yet, but I think would be really interesting
    is to use the combination of discriminative learning rates and 1cycle. No one
    has tried yet. So that would be really interesting. The only paper I’ve come across
    which has discriminative learning rate uses something called LARS. It was used
    to train ImageNet with very very large batch sizes by looking at the ratio between
    the gradient and the mean at each layer and using that to change the learning
    rate of each layer automatically. They found that they could use much larger batch
    sizes. That’s the only other place I’ve seen this kind of approach used, but there’s
    lots of interesting things you could try with combining discriminative learning
    rates and different interesting schedules.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我还没有尝试过的一件事，但我认为会非常有趣的是使用区分学习率和1cycle的组合。还没有人尝试过。这将非常有趣。我遇到的唯一一篇使用区分学习率的论文使用了一种称为LARS的东西。它被用来通过查看每层的梯度和均值之间的比率并使用该比率自动更改每层的学习率来训练ImageNet，从而使用非常大的批量大小。他们发现他们可以使用更大的批量大小。这是我看到这种方法使用的唯一其他地方，但是您可以尝试结合区分学习率和不同有趣的调度尝试很多有趣的事情。
- en: Your own LR finder [[10:06](https://youtu.be/xXXiC4YRGrQ?t=10m6s)]
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 您自己的LR查找器
- en: You can now write your own LR finger of different types, specifically because
    there is now this `stop_div` parameter which basically means that it’ll use whatever
    schedule you asked for but when the loss gets too bad, it’ll stop training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以编写不同类型的LR finder，特别是因为现在有这个`stop_div`参数，基本上意味着当损失变得太糟糕时，它将停止训练。
- en: One useful thing that’s been added is the `linear` parameter to the `plot` function.
    If you use linear schedule rather than an exponential schedule in your learning
    rate finder which is a good idea if you fine-tuned into roughly the right area,
    then you can use linear to find exactly the right area. Then you probably want
    to plot it with a linear scale. So that’s why you can also pass linear to plot
    now as well.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 添加的一个有用功能是`plot`函数中的`linear`参数。如果您在学习率查找器中使用线性调度而不是指数调度，这是一个好主意，如果您调整到大致正确的区域，那么您可以使用线性来找到确切的区域。然后您可能希望使用线性比例来绘制它。因此，您现在也可以将linear传递给plot。
- en: You can change the optimizer each phase [[11:06](https://youtu.be/xXXiC4YRGrQ?t=11m6s)].
    That’s more important than you might imagine because actually the current state-of-the-art
    for training on really large batch sizes really quickly for ImageNet actually
    starts with RMSProp for the first bit, then they switch to SGD for the second
    bit. So that could be something interesting to experiment more with because at
    least one paper has now shown that that can work well. Again, it’s something that
    isn’t well appreciated as yet.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在每个阶段更改优化器。这比您想象的更重要，因为实际上针对ImageNet在非常大的批量大小上快速训练的当前最先进技术实际上是从RMSProp开始的，然后他们在第二部分切换到SGD。因此，这可能是一个有趣的实验，因为至少有一篇论文现在已经表明这样可以很好地工作。再次强调，这是一个尚未被充分认识的问题。
- en: Changing data [[11:49](https://youtu.be/xXXiC4YRGrQ?t=11m49s)]
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更改数据
- en: Then the bit I find most interesting is you can change your data. Why would
    we want to change our data? Because you remember from lesson 1 and 2, you could
    use small images at the start and bigger images later. The theory is that you
    could use that to train the first bit more quickly with smaller images, and remember
    if you halve the height and halve the width, you’ve got the quarter of the activations
    every layer, so it can be a lot faster. It might even generalize better. So you
    can now create a couple of different sizes, for example, he’s got 28 and 32 sized
    images. This is CIFAR10 so there’s only so much you can do. Then if you pass in
    an array of data in this `data_list` parameter when you call `fit_opt_sched`,
    it’ll use different dataset for each phase.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我发现最有趣的部分是您可以更改您的数据。为什么我们要更改我们的数据？因为您还记得第1和第2课，您可以在开始时使用小图像，然后稍后使用更大的图像。理论上，您可以使用这种方法更快地训练第一部分，然后记住，如果您将高度减半并将宽度减半，则每层的激活数量就会减少四分之一，因此速度可能会更快。它甚至可能泛化得更好。因此，您现在可以创建几种不同大小，例如，他有28和32大小的图像。这是CIFAR10，所以您可以做的事情有限。然后，如果您在调用`fit_opt_sched`时在`data_list`参数中传入数据数组，它将在每个阶段使用不同的数据集。
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'That’s really cool because we can use that now like we could use that in our
    DAWN bench entries and see what happens when we actually increase the size with
    very little code. So what happens when we do that [[13:02](https://youtu.be/xXXiC4YRGrQ?t=13m2s)]?
    The answer is here in DAWN bench training on ImageNet:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的很酷，因为我们现在可以像在DAWN bench条目中那样使用它，并查看当我们实际上用很少的代码增加大小时会发生什么。那么当我们这样做时会发生什么？答案在DAWN
    bench上对ImageNet的训练中。
- en: 'You can see here that Google has won this with half an hour on a cluster of
    TPUs. The best non-cluster of TPU result is fast.ai + students under 3 hours beating
    out Intel on 128 computers, where else, we ran on a single computer. We also beat
    Google running on a TPU so using this approach, we’ve shown:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，谷歌用半小时在一组TPU上赢得了比赛。最好的非TPU集群结果是fast.ai + 学生在不到3小时内击败了拥有128台计算机的英特尔，而我们只用了一台计算机。我们还击败了在TPU上运行的谷歌，所以使用这种方法，我们已经证明了：
- en: the fastest GPU result
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最快的GPU结果
- en: the fastest single machine result
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最快的单机结果
- en: the fastest publicly available infrastructure result
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最快的公开可用基础设施结果
- en: These TPU pods, you can’t use unless you’re Google. Also the cost is tiny ($72.54),
    this Intel one costs them $1,200 worth of compute — they haven’t even written
    it here, but that’s what you get if you use 128 computers in parallel each one
    with 36 cores, each one with 140G compare to our single AWS instance. So this
    is kind of a breakthrough in what we can do. The idea that we can train ImageNet
    on a single publicly available machine and this is $72, by the way, it was actually
    $25 because we used a spot instance. One of our students Andrew Shaw built this
    whole system to allow us to throw a whole bunch of spot instance experiments up
    and run them simultaneously and pretty much automatically, but DAWN bench doesn’t
    quote the actual number we used. So it’s actually $25, not $72\. So this `data_list`
    idea is super important and helpful.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些TPU机架，除非你是谷歌，否则无法使用。而且成本很低（72.54美元），这个英特尔的成本是1200美元的计算成本——他们甚至没有写在这里，但如果你同时使用128台计算机，每台有36个核心，每台有140G，那就是你得到的结果，与我们的单个AWS实例相比。所以这在我们可以做的事情方面是一种突破。我们可以在一个公开可用的机器上训练ImageNet，这个成本是72美元，顺便说一句，实际上是25美元，因为我们使用了一个spot实例。我们的学生Andrew
    Shaw建立了整个系统，让我们可以同时运行一堆spot实例实验，并且几乎自动化，但DAWN bench没有引用我们使用的实际数字。所以实际上是25美元，而不是72美元。所以这个data_list的想法非常重要和有帮助。
- en: CIFAR10 result [[15:15](https://youtu.be/xXXiC4YRGrQ?t=15m15s)]
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CIFAR10结果
- en: Our CIFAR10 results are also now up there officially and you might remember
    the previous best was a bit over an hour. The trick here was using 1cycle, so
    all of this stuff that’s in Sylvain’s training phase API is really all the stuff
    that we used to get these top results. And another fast.ai student who goes by
    the name bkj has taken that and done his own version, he took a Resnet18 and added
    the concat pooling that you might remember that we learnt about on top, and used
    Leslie Smith’s 1cycle and so he’s got on the leaderboard. So all the top 3 are
    fast.ai students which wonderful.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的CIFAR10结果现在也正式发布了，你可能还记得之前最好的结果是一个多小时。这里的诀窍是使用1cycle，所以Sylvain的训练阶段API中的所有东西实际上都是我们用来获得这些顶级结果的东西。另一位fast.ai学生bkj采用了这个方法，并做了自己的版本，他采用了一个Resnet18，并在顶部添加了我们学到的concat
    pooling，并使用了Leslie Smith的1cycle，所以他上了排行榜。所以前三名都是fast.ai的学生，这太棒了。
- en: CIFAR10 cost result [[16:05](https://youtu.be/xXXiC4YRGrQ?t=16m5s)]
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CIFAR10成本结果
- en: Same for cost — the top 3 and you can see, Paperspace. Brett ran this on Paperspace
    and got the cheapest result just ahead of bkj.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 成本也是一样的——前三名，你可以看到，Paperspace。Brett在Paperspace上运行，得到了最便宜的结果，略胜于bkj。
- en: So I think you can see [[16:25](https://youtu.be/xXXiC4YRGrQ?t=16m25s)], a lot
    of the interesting opportunities at the moment for the training stuff more quickly
    and cheaply are all about learning rate annealing, size annealing, and training
    with different parameters at different times, and I still think everybody is scratching
    the surface. I think we can go a lot faster and a lot cheaper. That’s really helpful
    for people in resource constrained environment which is basically everybody except
    Google, maybe Facebook.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我认为你可以看到，目前训练更快、更便宜的有趣机会很多都是关于学习率退火、尺寸退火，以及在不同时间使用不同参数进行训练，我仍然认为大家只是触及了表面。我认为我们可以做得更快、更便宜。这对于资源受限的环境中的人们非常有帮助，基本上除了谷歌，也许还有Facebook。
- en: 'Architectures are interesting as well though [[17:00](https://youtu.be/xXXiC4YRGrQ?t=17m)],
    and one of the things we looked at last week was creating a simpler version of
    darknet architecture. But there’s a piece of architecture we haven’t talk about
    which is necessary to understand the Inception network. The Inception network
    is actually pretty interesting because they use some tricks to make things more
    efficient. We are not currently using these tricks and I feel that maybe we should
    try it. The most interesting and most successful Inception network is their Inception-ResNet-v2
    network and most of the blocks in that looks something like this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 架构也很有趣，上周我们看了一下简化版本的darknet架构。但有一个架构我们还没有谈到，那就是理解Inception网络所必需的。Inception网络实际上非常有趣，因为他们使用了一些技巧使得事情更加高效。我们目前没有使用这些技巧，我觉得也许我们应该尝试一下。最有趣、最成功的Inception网络是他们的Inception-ResNet-v2网络，其中大部分块看起来像这样：
- en: It looks a lot like a standard ResNet block in that there’s an identity connection,
    and there’s a conv path, and we add them up together [[17:47](https://youtu.be/xXXiC4YRGrQ?t=17m47s)].
    But it’s not quite that. The first is the middle conv path is a 1x1 conv, and
    it’s worth thinking about what a 1x1 conv actually is.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来很像标准的ResNet块，因为有一个恒等连接，还有一个卷积路径，我们把它们加在一起。但实际上并不完全是这样。首先，中间的卷积路径是一个1x1卷积，值得思考一下1x1卷积实际上是什么。
- en: 1x1 convolution [[18:23](https://youtu.be/xXXiC4YRGrQ?t=18m23s)]
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1x1卷积
- en: 1x1 conv is simply saying for each grid cell in your input, you’ve got basically
    a vector. 1 by 1 by number of filters tensor is basically a vector. For each grid
    cell in your input, you’re just doing a dot product with that tensor. Then of
    course, it’s going to be one of those vectors for each of the 192 activations
    we are creating. So basically do 192 dot products with grid cell (1, 1) and then
    192 with grid cell (1, 2) or (1, 3) and so forth. So you will end up with something
    which has the same grid size as the input and 192 channels in the output. So that’s
    a really good way to either reduce the dimensionality or increase the dimensionality
    of an input without changing the grid size. That’s normally what we use 1x1 convs
    for. Here, we have a 1x1 conv and another 1x1 conv, and then they add it together.
    Then there is a third path and this third path is not added. It is not explicitly
    mentioned but this third path is concatenated. There is a form of ResNet which
    is basically identical to ResNet but we don’t do plus, we do concat. That’s called
    a DenseNet. It’s just a ResNet where we do concat instead of plus. That’s an interesting
    approach because then the kind of the identity path is literally being copied.
    So you get that flow all the way through and so as we’ll see next week, that tends
    to be good for segmentation and stuff like that whe re you really want to keep
    the original pixels, the first layer of pixels, and the second layer of pixels
    untouched.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 1x1卷积简单地说，对于输入中的每个网格单元，您基本上有一个向量。1乘1乘滤波器数量的张量基本上是一个向量。对于输入中的每个网格单元，您只需与该张量进行点积。然后，当然，对于我们正在创建的192个激活之一，它将是这些向量之一。因此，基本上对网格单元（1,1）进行192个点积，然后对网格单元（1,2）或（1,3）等进行192个点积。因此，您将得到与输入具有相同网格大小和输出中的192个通道的内容。因此，这是一种非常好的方法，可以减少或增加输入的维度，而不改变网格大小。这通常是我们使用1x1卷积的方式。在这里，我们有一个1x1卷积和另一个1x1卷积，然后将它们相加。然后有第三个路径，这第三个路径没有被添加。虽然没有明确提到，但这第三个路径是被连接的。有一种形式的ResNet基本上与ResNet相同，但我们不使用加号，而是使用连接。这被称为DenseNet。这只是一个使用连接而不是加法的ResNet。这是一个有趣的方法，因为这样，身份路径实际上被复制。因此，您可以一直保持这种流动，因此正如我们将在下周看到的那样，这对于分割等需要保留原始像素、第一层像素和第二层像素不变的情况非常有用。
- en: Concatenating rather than adding branches is a very useful thing to do and we
    are concatenating the middle branch and the right right branch [[20:22](https://youtu.be/xXXiC4YRGrQ?t=20m22s)].
    The right most branch is doing something interesting which is it’s doing, first
    of all, the 1x1 conv, and then a 1x7, and then 7x1\. What’s going on there? So,
    what’s going on there is basically what we really want to do is do 7x7 conv. The
    reason we want to do 7x7 conv is that if you have multiple paths (each of which
    has different kernel sizes), then it’s able to look at different amounts of the
    image. The original Inception network had 1x1, 3x3, 5x5, 7x7 getting concatenated
    together or something like that. So if we can have a 7x7 filter, then we get to
    look at a lot of the image at once and create a really rich representation. So
    the stem of the Inception network that is the first few layers of the Inception
    network actually also used this kind fo 7x7 conv because you start out with this
    224 by 224 by 3, and you want to turn it into something that’s 112 by 112 by 64\.
    By using a 7x7 conv, you can get a lot of information in each one of those outputs
    to get those 64 filters. But the problem is that 7x7 conv is a lot of work. You’ve
    got 49 kernel values to multiply by 49 inputs for every input pixel across every
    channel. So the compute is crazy. You can kind of get away with it (maybe) for
    the very first layer, and in fact, the very first conv of ResNet is a 7x7 conv.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 连接而不是添加分支是一件非常有用的事情，我们正在连接中间分支和右侧分支。最右侧的分支正在做一些有趣的事情，首先是1x1卷积，然后是1x7，然后是7x1。那里发生了什么？所以，那里发生的事情基本上是我们真正想要做的是7x7卷积。我们想要做7x7卷积的原因是，如果有多个路径（每个路径具有不同的内核大小），那么它可以查看图像的不同部分。最初的Inception网络将1x1、3x3、5x5、7x7连接在一起或类似的东西。因此，如果我们可以有一个7x7滤波器，那么我们可以一次查看图像的很多部分并创建一个非常丰富的表示。因此，Inception网络的干部，即Inception网络的前几层实际上也使用了这种7x7卷积，因为您从这个224x224x3开始，希望将其转换为112x112x64。通过使用7x7卷积，您可以在每个输出中获得大量信息以获得这些64个滤波器。但问题是7x7卷积是很费力的。您需要将49个内核值乘以每个通道的每个输入像素的49个输入。因此，计算量很大。您可能可以在第一层中使用它（也许可以），实际上，ResNet的第一个卷积就是7x7卷积。
- en: But not so for Inception [[22:30](https://youtu.be/xXXiC4YRGrQ?t=22m30s)]. They
    don’t do a 7x7 conv, instead, they do a 1x7 followed by 7x1\. So to explain, the
    basic idea of the Inception networks or all the different versions of it that
    you have a number of separate paths which have different convolution widths. In
    this case, conceptually the idea is the middle path is 1x1 convolution width,
    and the right path is going to be a 7 convolution width, so they are looking at
    different amount of data and then we combine them together. But we don’t want
    to have a 7x7 conv through out the network because it’s just too computationally
    expensive.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于《盗梦空间》来说并非如此。它们不使用7x7卷积，而是使用1x7接着7x1。因此，基本思想是Inception网络或其所有不同版本的基本思想是有许多不同的卷积宽度的独立路径。在这种情况下，概念上的想法是中间路径是1x1卷积宽度，右侧路径将是7卷积宽度，因此它们正在查看不同数量的数据，然后将它们组合在一起。但我们不希望在整个网络中都使用7x7卷积，因为这太耗费计算资源了。
- en: But if you think about it [[23:18](https://youtu.be/xXXiC4YRGrQ?t=23m18s)],
    if we’ve got some input coming in and we have some big filter that we want and
    it’s too big to deal with. What could we do? Let’s do 5x5\. What we can do is
    to create two filters — one which is 1x5, one which is 5x1\. We take our activations
    of the previous layer, and we put it through the 1x5\. We take the activations
    out of that, and put it through the 5x1, and something comes out the other end.
    Now what comes out the other end? Rather than thinking of it as, first of all,
    we take the activations, then we put it through the 1x5 then we put it through
    the 5x1, what if instead we think of these two operations together and say what
    is a 5x1 dot product and a 1x5 dot product do together? Effectively, you could
    take a 1x5 and 5x1 and the outer product of that is going to give you a 5x5\.
    Now you can’t create any possible 5x5 matrix by taking that product, but there’s
    a lot of 5x5 matrices that you can create. So the basic idea here is when you
    think about the order of operations (if you are interested in more of the theory
    here, you should check out Rachel’s numerical linear algebra course which is basically
    a whole course about this). But conceptually, the idea is that very often the
    computation you want to do is actually more simple than an entire 5x5 convolution.
    Very often, the term we use in linear algebra is that there’s some lower rank
    approximation. In other words, that the 1x5 and the 5x1 combined together — that
    5x5 matrix is nearly as good as the 5x5 matrix you ideally would have computed
    if you were able to. So this is very often the case in practice — just because
    the nature of the real world is that the real world tends to have more structure
    than randomness.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你考虑一下[[23:18](https://youtu.be/xXXiC4YRGrQ?t=23m18s)]，如果我们有一些输入进来，我们有一些我们想要的大滤波器，但它太大了无法处理。我们能做什么？让我们做5x5。我们可以创建两个滤波器
    —— 一个是1x5，一个是5x1。我们将前一层的激活传递给1x5。我们从中取出激活，然后通过5x1传递，最后得到一些结果。现在另一端出来了什么？与其将其视为首先我们取激活，然后通过1x5，然后通过5x1，不如一起考虑这两个操作，看看一个5x1点积和一个1x5点积一起做会发生什么？实际上，你可以取一个1x5和5x1，它们的外积将给你一个5x5。现在你不能通过取这个积来创建任何可能的5x5矩阵，但是你可以创建很多5x5矩阵。所以这里的基本思想是当你考虑操作的顺序时（如果你对这里的理论更感兴趣，你应该查看Rachel的数值线性代数课程，这基本上是关于这个的整个课程）。但从概念上来说，很多时候你想要做的计算实际上比整个5x5卷积更简单。在线性代数中我们经常使用的术语是有一些低秩近似。换句话说，1x5和5x1结合在一起
    —— 那个5x5矩阵几乎和你理想情况下应该计算的5x5矩阵一样好。所以在实践中这往往是情况 —— 因为现实世界的本质是现实世界往往比随机性更具结构性。
- en: The cool thing is [26:16], if we replace our 7x7 conv with a 1x7 and 7x1, for
    each cell, it has 14 by input channel by output channel dot products to do, whereas
    7x7 one has 49 to do. So it’s going to be a lot faster and we have to hope that
    it’s going to be nearly as good. It’s certainly capturing as much width of information
    by definition.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 酷的地方是[26:16]，如果我们用1x7和7x1替换我们的7x7卷积，对于每个单元格，它有14个输入通道乘以输出通道的点积要做，而7x7卷积则有49个要做。所以速度会快得多，我们希望它的效果几乎一样好。从定义上来说，它肯定捕捉到了尽可能多的信息宽度。
- en: 'If you are interested in learning more about this, specifically in a deep learning
    area, you can google for **Factored Convolutions**. The idea was come up with
    3 or 4 years ago now. It’s probably been around for longer, but that was when
    I first saw it. It turned out to work really well and the Inception network uses
    it quite widely. They actually use it in their stem. We’ve talked before about
    how we tend to add-on — we tend to say this is main backbone when we have ResNet34,
    for example. This is main backbone which is all of the convolutions, and then
    we can add on to it a custom head that tends to be a max pooling or a fully connected
    layer. It’s better to talk about the backbone is containing two pieces: one is
    the stem and the other is the main backbone. The reason is that the thing that’s
    coming in has only 3 channels, so we want some sequence of operations which is
    going to expand that out into something richer — generally something like 64 channels.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这方面的知识感兴趣，特别是在深度学习领域，你可以搜索**分解卷积**。这个想法是3年或4年前提出的。它可能已经存在更长时间了，但那是我第一次看到它的时候。结果表明它的效果非常好，Inception网络广泛使用它。他们实际上在他们的干部中使用它。我们之前谈过，我们倾向于添加-on
    —— 我们倾向于说这是主干，例如我们有ResNet34。这是主干，其中包含所有的卷积，然后我们可以添加一个自定义头部，通常是最大池化或全连接层。更好的做法是谈论主干包含两个部分：一个是干部，另一个是主干。原因是进来的东西只有3个通道，所以我们希望有一系列操作将其扩展为更丰富的东西
    —— 通常是64个通道之类的东西。
- en: In ResNet, the stem is super simple. It’s a 7x7 stride 2 conv followed by a
    stride 2 max pool (I think that’s it if memory serves correctly). Inception have
    a much more complex stem with multiple paths getting combined and concatenated
    including factored conv (1x7 and 7x1). I’m interested in what would happen if
    you stacked a standard ResNet on top of an Inception stem, for instance. I think
    that would be a really interesting thing to try because an Inception stem is quite
    a carefully engineered thing, and this thing of how you take 3 channel input and
    turn it into something richer seems really important. And all of that work seems
    to have gotten thrown away for ResNet. We like ResNet, it works really well. But
    what if we put a dense net backbone on top of an Inception stem? Or what if we
    replaced the 7x7 conv with a 1x7 and 7x1 factored conv in standard ResNet? There
    are lots of things we could try and I think it would be really interesting. So
    there’s some more thoughts about potential research directions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在ResNet中，干部非常简单。它是一个7x7步幅2卷积，后面跟着一个步幅2最大池（如果我记得正确的话）。Inception有一个更复杂的干部，其中包括多个路径的组合和连接，包括因子化卷积（1x7和7x1）。我很感兴趣的是，如果你在Inception干部上堆叠一个标准的ResNet会发生什么。我认为这将是一个非常有趣的尝试，因为Inception干部是一个非常精心设计的东西，以及如何将3通道输入转换为更丰富的东西似乎非常重要。而所有这些工作似乎都被抛弃了。我们喜欢ResNet，它的效果非常好。但是如果我们在Inception干部上放置一个密集的网络骨干呢？或者如果我们用标准ResNet中的1x7和7x1因子化卷积替换7x7卷积呢？有很多事情我们可以尝试，我认为这将是非常有趣的。所以这是关于潜在研究方向的一些想法。
- en: 'So that was kind of my little bunch of random stuff section [[29:51](https://youtu.be/xXXiC4YRGrQ?t=29m51s)].
    Moving a little bit closer to the actual main topic of this which is image enhancement.
    I’m going to talk about a new paper briefly because it really connects what I
    just discussed with what we are going to discuss next. It’s a paper on progressive
    GANs which came from Nvidia: [Progressive Growing of GANs for Improved Quality,
    Stability, and Variation](http://research.nvidia.com/publication/2017-10_Progressive-Growing-of).
    Progressive GANs takes this idea of gradually increasing the image size. It’s
    the only other direction I am aware of that people have actually gradually increase
    the image size. It surprises me because this paper is actually very popular, well
    known, and well liked and yet, people haven’t taken the basic idea of gradually
    increasing the image size and use it anywhere else which shows you the general
    level of creativity you can expect to find in the deep learning research community,
    perhaps.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我小小一堆随机东西部分的内容[[29:51](https://youtu.be/xXXiC4YRGrQ?t=29m51s)]。稍微接近这个实际主题的是图像增强。我将简要谈一下一篇新论文，因为它与我刚刚讨论的内容和我们接下来要讨论的内容有很大联系。这是一篇关于渐进式GAN的论文，来自Nvidia：[渐进增长的GANs用于提高质量、稳定性和变化](http://research.nvidia.com/publication/2017-10_Progressive-Growing-of)。渐进式GANs采用了逐渐增加图像大小的想法。这是我所知道的唯一另一个人们实际上逐渐增加图像大小的方向。令我惊讶的是，这篇论文实际上非常受欢迎，知名度很高，而且受欢迎，但是人们还没有将逐渐增加图像大小的基本思想应用到其他地方，这显示了你可以在深度学习研究社区中期望找到的创造力水平。
- en: They really go back and they start with 4x4 GAN [[31:47](https://youtu.be/xXXiC4YRGrQ?t=31m47s)].
    Literally, they are trying to replicate 4x4 pixel, and then 8x8 (the upper left
    ones above). This is the CelebA dataset so we are trying to recreate pictures
    of celebrities. Then they go 16x16, 32, 64, 128, then 256\. One of the really
    nifty things they do is that as they increase the size, they also add more layers
    to the network. Which kind of makes sense because if you are doing more of a ResNet-y
    type thing, then you are spitting out something which hopefully makes sense at
    each grid cell size, so you should be able to layer stuff on top. They do another
    nifty thing where they add a skip connection when they do that, and they gradually
    change the linear interpolation parameter that moves it more and more away from
    the old 4x4 network and towards the new 8x8 network. Then once this totally moved
    it across, they throw away that extra connection. The details don’t matter too
    much but it uses the basic ideas we’ve talked about, gradually increasing the
    image size and skip connections. It’s a great paper to study because it is one
    of these rare things where good engineers actually built something that just works
    in a really sensible way. Now it’s not surprising this actually comes from Nvidia
    themselves. Nvidia don’t do a lot of papers and it’s interesting that when they
    do, they build something that is so throughly practical and sensible. So I think
    it’s a great paper to study if you want to put together lots of the different
    things we’ve learned and there aren’t many re-implementation of this so it’s an
    interesting thing to project, and maybe you could build on and find something
    else.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 他们真的回到了4x4 GAN开始[[31:47](https://youtu.be/xXXiC4YRGrQ?t=31m47s)]。实际上，他们试图复制4x4像素，然后是8x8（上面左上角的那些）。这是CelebA数据集，所以我们试图重新创建名人的图片。然后他们去16x16，32，64，128，然后256。他们做的一个非常聪明的事情是，随着尺寸的增加，他们还向网络添加更多层。这有点说得通，因为如果你在做更多的ResNet类型的事情，那么你应该能够在每个网格单元大小输出一些有意义的东西，所以你应该能够在其上叠加东西。当他们这样做时，他们做了另一个聪明的事情，他们添加了一个跳过连接，并逐渐改变线性插值参数，使其越来越远离旧的4x4网络，朝向新的8x8网络。一旦完全移动到新网络，他们就会丢弃那个额外的连接。细节并不太重要，但它使用了我们谈论过的基本思想，逐渐增加图像大小和跳过连接。这是一篇很棒的论文，因为这是一种罕见的情况，好的工程师实际上构建了一些以非常明智的方式工作的东西。现在这并不奇怪，这实际上来自Nvidia自己。Nvidia并不发表很多论文，有趣的是，当他们这样做时，他们构建了一些非常实用和明智的东西。所以我认为这是一篇很棒的论文，如果你想整合我们学到的许多不同的东西，而且没有太多的重新实现，所以这是一个有趣的项目，也许你可以继续研究并找到其他东西。
- en: 'Here is what happens next [[33:45](https://youtu.be/xXXiC4YRGrQ?t=33m45s)].
    We eventually go up to 1024x1024, and you’ll see that the images are not only
    getting higher resolution but they are getting better. So I am going to see if
    you can guess which one of the following is fake:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来会发生什么[[33:45](https://youtu.be/xXXiC4YRGrQ?t=33m45s)]。我们最终会升级到1024x1024，你会看到图像不仅分辨率更高，而且质量更好。所以我要看看你能否猜出以下哪一个是假的：
- en: 'They are all fake. That’s the next stage. You go up up up up and them BOOM.
    So GANs and stuff are getting crazy and some of you may have seen this during
    the week [[34:16](https://youtu.be/xXXiC4YRGrQ?t=34m16s)]. This video just came
    out and it’s a speech by Barack Obama and let’s check it out:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 它们全都是假的。这是下一个阶段。你一直往上走，然后突然爆炸。所以GANs和其他东西变得疯狂，你们中的一些人可能在这周看到了这个[[34:16](https://youtu.be/xXXiC4YRGrQ?t=34m16s)]。这个视频刚刚发布，是巴拉克·奥巴马的演讲，让我们来看一下：
- en: As you can see, they’ve used this kind of technology to literally move Obama’s
    face in the way that Jordan Peele’s face was moving. You basically have all the
    techniques you need now to do that. Is that a good idea?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，他们使用这种技术来实际移动奥巴马的脸，就像乔丹·皮尔的脸在移动一样。你现在基本上拥有了所有需要的技术。这是一个好主意吗？
- en: Ethics in AI [[35:31](https://youtu.be/xXXiC4YRGrQ?t=35m31s)]
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能伦理[[35:31](https://youtu.be/xXXiC4YRGrQ?t=35m31s)]
- en: This is the bit where we talk about what’s most important which is now that
    we can do all this stuff, what should we be doing and how do we think about that?
    The TL;DR version is I actually don’t know. Recently a lot of you saw the founders
    of the spaCy prodigy folks down at the Explosion AI did a talk, Matthew and Ines,
    and I went to dinner with them afterwards, and we basically spent the entire evening
    talking, debating, arguing about what does it mean the companies like ours are
    building tools that are democratizing access to tools that can be used in harmful
    ways. They are incredibly thoughtful people and we, I wouldn’t say we didn’t agree,
    we just couldn’t come to a conclusion ourselves. So I’m just going to lay out
    some of the questions and point to some of the research, and when I say research,
    most of the actual literature review and putting this together was done by Rachel,
    so thanks Rachel.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们谈论最重要的部分，现在我们可以做所有这些事情，我们应该做什么，我们如何考虑？简而言之，我其实不知道。最近，你们中的许多人看到了spaCy prodigy公司的创始人在Explosion
    AI做了一个演讲，Matthew和Ines，之后我和他们一起吃饭，我们基本上整个晚上都在讨论，辩论，争论我们这样的公司正在构建可以以有害方式使用的工具，这意味着什么。他们是非常深思熟虑的人，我们，我不会说我们没有达成一致意见，我们只是无法得出结论。所以我只是列出一些问题，并指出一些研究，当我说研究时，实际上大部分文献综述和整理工作都是由Rachel完成的，所以谢谢Rachel。
- en: Let me start by saying the models we build are often pretty crappy in ways which
    are not immediately apparent [[36:52](https://youtu.be/xXXiC4YRGrQ?t=36m52s)].
    You won’t know how crappy they are unless the people that are building them with
    you are a range of people and the people that are using them with you are a range
    of people. For example, a couple of wonderful researchers, [Timnit Gebru](https://twitter.com/timnitGebru)
    is at Microsoft and [Joy Buolamwini](https://twitter.com/jovialjoy) just finished
    PhD from MIT, they did this really interesting research where they looked at some
    off-the-shelf face recognizers, one from FACE++ which is a huge Chinese company,
    IBM’s, and Microsoft’s, and they looked for a range of different face types.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我先说一下，我们构建的模型通常在某些方面相当糟糕，这些问题并不立即显现[[36:52](https://youtu.be/xXXiC4YRGrQ?t=36m52s)]。除非与你一起构建它们的人是各种各样的人，与你一起使用它们的人也是各种各样的人，否则你不会知道它们有多糟糕。例如，一对出色的研究人员，[Timnit
    Gebru](https://twitter.com/timnitGebru)在微软工作，[Joy Buolamwini](https://twitter.com/jovialjoy)刚从麻省理工学院获得博士学位，他们进行了一项非常有趣的研究，他们查看了一些现成的人脸识别器，其中包括来自FACE++的一个，这是一家庞大的中国公司，IBM的，以及微软的，他们寻找了一系列不同类型的人脸。
- en: Generally speaking, Microsoft one in particular was incredibly accurate unless
    the face type happened to be dark-skinned when suddenly it went 25 times worse.
    IBM got it wrong nearly half the time. For a big company like this to release
    a product that, for large percentage of the world, doesn’t work is more than a
    technical failure. It’s a really deep failure of understanding what kind of team
    needs to be used to create such a technology and to test such a technology or
    even an understanding of who your customers are. Some of your customers have dark
    skin. “I was also going to add that the classifiers all did worse on women than
    on men” (Rachel). Shocking. It’s funny that Rachel tweeted about something like
    this the other day, and some guy said “What’s this all about? What are you saying?
    Don’t you know people made cars for a long time — are you saying you need women
    to make cars too?” And Rachel pointed out — well actually yes. For most of the
    history of car safety, women in cars have been far more at risk of death than
    men in cars because the men created male looking, feeling, sized crash test dummies,
    so car safety was literally not tested on women size bodies. Crappy product management
    with a total failure of diversity and understanding is not new to our field.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，微软的一个特别准确，除非人脸类型恰好是深色皮肤，突然间糟糕了25倍。IBM几乎一半的时间都搞错了。对于这样一个大公司来说，发布一个对世界上大部分人来说都不起作用的产品，不仅仅是技术上的失败。这是对理解需要使用什么样的团队来创建这样的技术以及测试这样的技术，甚至对你的客户是谁的一种深刻失败。你的一些客户有深色皮肤。“我还要补充说，分类器在女性身上的表现都比在男性身上差”（Rachel）。令人震惊。有趣的是，Rachel前几天在推特上发表了类似的言论，有人说“这是怎么回事？你在说什么？难道你不知道人们很长时间以来一直在制造汽车吗——你是在说你需要女性来制造汽车吗？”Rachel指出——实际上是的。在汽车安全的大部分历史中，女性在汽车中的死亡风险远远高于男性，因为男性创造了看起来像男性、感觉像男性、尺寸像男性的碰撞测试假人，所以汽车安全实际上没有在女性身材上进行测试。产品管理糟糕，缺乏多样性和理解的失败在我们领域并不新鲜。
- en: “I was just going to say that was comparing impacts of similar strength for
    men and women ” (Rachel). I don’t know why whenever you say something like this
    on Twitter, Rachel has to say this because anytime you say something like this
    on Twitter, there’s about 10 people who’ll say “oh, you have to compare all these
    other things” as if we didn’t know that.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: “我只是想说，这是在比较男性和女性的影响力”（Rachel）。我不知道为什么每当你在Twitter上说这样的话时，Rachel都要这样说，因为每当你在Twitter上说这样的话时，大约有10个人会说“哦，你必须比较所有这些其他事情”，好像我们不知道一样。
- en: Other things our very best most famous systems do like Microsoft’s face recognizer
    or Google’s language translator, you turn “She is a doctor. He is a nurse.” into
    Turkish and quite correctly — both pronouns become O because there is no gendered
    pronouns in Turkish. Go the other direction, what does it get turned into? “He
    is a doctor. She is a nurse.” So we’ve got these kind of biases built into tools
    that we are all using every day. And again, people say “oh, it’s just showing
    us what’s in the world” and okay, there’s lots of problems with that basic assertion,
    but as you know, machine learning algorithms love to generalize.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 像微软的人脸识别器或谷歌的语言翻译器这样的我们最好最著名的系统做的其他事情，你把“她是医生。他是护士。”翻译成土耳其语，非常正确——两个代词都变成了O，因为土耳其语中没有性别代词。反过来，它会变成什么？“他是医生。她是护士。”所以我们在每天使用的工具中内置了这种偏见。而且，人们会说“哦，它只是展示了世界上的东西”，好吧，这个基本断言有很多问题，但正如你所知，机器学习算法喜欢概括。
- en: So because they love to generalize, this is one fo the cool things about you
    guys knowing the technical details now, because they love to generalize when you
    see something like 60% of people cooking are women in the pictures they used to
    build this model and then you run the model on a separate set of pictures, then
    84% of the people they choose as cooking are women rather than the correct 67%.
    Which is a really understandable thing for an algorithm to do as it took a biased
    input and created a more biased output because for this particular loss function,
    that’s where it ended up. This is a really common kind of model amplification.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因为他们喜欢概括，这是你们现在了解技术细节的一个很酷的事情，因为当你看到像60%的照片中烹饪的人是女性，而他们用来构建这个模型的照片，然后你在另一组照片上运行模型时，84%被选择为烹饪的人是女性，而不是正确的67%。这对于算法来说是一个非常可以理解的事情，因为它接受了有偏见的输入，并创造了一个更有偏见的输出，因为对于这个特定的损失函数来说，这就是它的结果。这是一种非常常见的模型放大。
- en: This stuff matters [[41:41](https://youtu.be/xXXiC4YRGrQ?t=41m41s)]. It matters
    in ways more than just awkward translations or black people’s photos not being
    classified correctly. Maybe there’s some wins too as well — like horrifying surveillance
    everywhere and maybe won’t work on black people. “Or it’ll be even worse because
    it’s horrifying surveillance and it’s flat-out racist and wrong” (Rachel). But
    let’s go deeper. For all we say about human failings, there is a long history
    of civilization and societies creating layers of human judgement which avoid,
    hopefully, the most horrible things happening. And sometimes companies which love
    technology think “let’s throw away humans and replace them with technology” like
    Facebook did. A couple years ago, Facebook literally got rid of their human editors,
    and this was in the news at the time. And they were replaced with algorithms.
    So now as algorithms put all the stuff on your news feed and human editors were
    out of the loop. What happened next?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些事情很重要。它的重要性不仅仅体现在尴尬的翻译或黑人照片未被正确分类的方式上。也许也有一些胜利，比如到处可怕的监视，也许对黑人不起作用。“或者会更糟，因为这是可怕的监视，而且是彻头彻尾的种族主义和错误”（Rachel）。但让我们深入一点。尽管我们谈论人类的缺陷，但文明和社会有着长期的历史，创造了层层人类判断，希望避免最可怕的事情发生。有时候，热爱技术的公司会认为“让我们抛弃人类，用技术取代他们”，就像Facebook所做的那样。几年前，Facebook真的摆脱了他们的人类编辑，当时这成为了新闻。他们被算法取代了。现在，当算法将所有内容放在你的新闻源上，而人类编辑却被排除在外时，接下来会发生什么？
- en: Many things happened next. One of which was a massive horrifying genocide in
    Myanmar. Babies getting torn out of their mothers arms and thrown into fires.
    Mass rape, murder, and an entire people exiled from their homeland.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来发生了很多事情。其中之一是缅甸发生了大规模的可怕种族灭绝。婴儿被从母亲怀里夺走，扔进火里。大规模的强奸、谋杀，整个民族被流放出境。
- en: Okay, I’m not gonna say that was because Facebook did this, but what I will
    say is that when the leaders of this horrifying project are interviewed, they
    regularly talk about how everything they learnt about the disgusting animal behaviors
    of Rohingyas that need to be thrown off the earth, they learnt from Facebook.
    Because the algorithms just want to feed you more stuff that gets you clicking.
    If you get told these people that don’t look like you and you don’t know the bad
    people and here’s lots of stories about bad people and then you start clicking
    on them and then they feed you more of those things. Next thing you know, you
    have this extraordinary cycle. People have been studying this, so for example,
    we’ve been told a few times people click on our fast.ai videos and then the next
    thing recommended to them is like conspiracy theory videos from Alex Jones, and
    then continues from there. Because humans click on things that shock us, surprise
    us, and horrify us. At so many levels, this decision has had extraordinary consequences
    which we’re only beginning to understand. Again, this is not to say this particular
    consequence is because of this one thing, but to say it’s entirely unrelated would
    be clearly ignoring all of the evidence and information that we have.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我不会说那是因为Facebook这样做的，但我要说的是，当这个可怕项目的领导接受采访时，他们经常谈论他们从Facebook学到的关于罗辛亚人恶劣动物行为的一切，这些行为需要被扫除。因为算法只是想要给你更多让你点击的东西。如果你被告知这些人不像你，你不认识这些坏人，这里有很多关于坏人的故事，然后你开始点击它们，然后他们会给你更多这些东西。接下来你会发现，你陷入了这个不寻常的循环。人们一直在研究这个问题，比如，我们被告知有几次人们点击我们的fast.ai视频，然后推荐给他们的下一个东西是来自Alex
    Jones的阴谋论视频，然后继续下去。因为人类点击那些让我们震惊、惊讶和恐惧的东西。在很多层面上，这个决定产生了不同寻常的后果，我们只是开始理解。再次强调，这并不是说这个特定的后果是因为这一个原因，但说它与此毫无关联显然是在忽视我们所拥有的所有证据和信息。
- en: Unintended consequences [[45:04](https://youtu.be/xXXiC4YRGrQ?t=45m4s)]
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有意外的后果
- en: The key takeaway is to think what are you building and how could it be used.
    Lots and lots of effort now being put into face detection including in our course.
    We’ve been spending a lot of time thinking about how to recognize stuff and where
    it is. There’s lots of good reasons to want to be good at that for improving crop
    yields in agriculture, for improving diagnostic and treatment planning in medicine,
    for improving your LEGO sorting robot system, etc. But it’s also being widely
    used in surveillance, propaganda, and disinformation. Again, the question is what
    do I do about that? I don’t exactly know. But it’s definitely at least important
    to be thinking about it, talking about it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是要考虑你正在构建什么，以及它可能如何被使用。现在有很多努力投入到人脸识别中，包括我们的课程。我们一直在花费大量时间思考如何识别东西以及它在哪里。有很多很好的理由希望在这方面做得更好，比如改善农业产量、改善医学诊断和治疗规划、改善你的乐高分类机器人系统等等。但它也被广泛用于监视、宣传和虚假信息。再次，问题是我该怎么办？我不完全知道。但至少重要的是要考虑这个问题，谈论这个问题。
- en: Runaway feedback loops [[46:10](https://youtu.be/xXXiC4YRGrQ?t=46m10s)]
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 失控的反馈循环
- en: Sometimes you can do really good things. For example, meetup.com did something
    which I would put in the category of really good thing which is they recognized
    early a potential problem which is that more men are tending to go to their meet
    ups. And that was causing their collaborative filtering systems, which you are
    familiar building now to recommend more technical content to men. And that was
    causing more men to go to more technical content which was causing the recommendation
    system to suggest more technical content to men. This kind of runaway feedback
    loop is extremely common when we interface the algorithm and the human together.
    So what did Meetup do? They intentionally made the decision to recommend more
    technical content to women, not because highfalutin idea about how the world should
    be, but just because that makes sense. Runaway feedback loop was a bug — there
    are women that want to go to tech meetups, but when you turn up for a tech meet
    up and it’s all men and you don’t go, then it recommends more to men and so on
    and so forth. So Meetup made a really strong product management decision here
    which was to not do what the algorithm said to do. Unfortunately this is rare.
    Most of these runaway feedback loops, for example, in predictive policing where
    algorithms tell policemen where to go which very often is more black neighborhoods
    which end up crawling with more policemen which leads to more arrests which is
    assisting to tell more policemen to go to more black neighborhoods and so forth.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你可以做一些非常好的事情。例如，meetup.com做了一件我认为是非常好的事情的事情，他们早就意识到一个潜在的问题，即更多的男性倾向于参加他们的聚会。这导致他们的协同过滤系统，你现在熟悉正在构建的系统，向男性推荐更多技术内容。这导致更多的男性参加更多的技术内容，从而导致推荐系统向男性推荐更多技术内容。当我们将算法和人类结合在一起时，这种失控的反馈循环是非常常见的。那么Meetup做了什么？他们故意做出了向女性推荐更多技术内容的决定，不是因为对世界应该如何的高尚想法，而只是因为这是有道理的。失控的反馈循环是一个bug——有女性想要参加技术聚会，但当你去参加一个技术聚会，里面全是男性，你就不去了，然后它就会向男性推荐更多，依此类推。因此，Meetup在这里做出了一个非常强有力的产品管理决策，即不按照算法建议的做。不幸的是，这种情况很少见。大多数这种失控的反馈循环，例如在预测性警务中，算法告诉警察去哪里，很多时候是更多的黑人社区，这些社区最终会涌入更多的警察，导致更多的逮捕，这有助于告诉更多的警察去更多的黑人社区等等。
- en: Bias in AI [[48:09](https://youtu.be/xXXiC4YRGrQ?t=48m9s)]
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI中的偏见
- en: This problem of algorithmic bias is now very wide spread and as algorithms become
    more and more widely used for specific policy decisions, judicial decisions, day-to-day
    decisions about who to give what offer to, this just keeps becoming a bigger problem.
    Some of them are really things that the people involved in the product management
    decision should have seen at the very start, didn’t make sense, and unreasonable
    under any definition of the term. For example, this stuff Abe Gong pointed out
    — these were questions that were used for both pretrial so who was required to
    post bail, so these are people that haven’t even been convicted, as well as for
    sentencing and for who gets parole. This was upheld by the Wisconsin Supreme Court
    last year despite all the flaws. So whether you have to stay in jail because you
    can’t pay the bail and how long your sentence is for, and how long you stay in
    jail for depends on what your father did, whether your parents stayed married,
    who your friends are, and where you live. Now turns out these algorithms are actually
    terribly terribly bad so some recent analysis showed that they are basically worse
    than chance. But even if the company’s building them were confident on these were
    statistically accurate correlations, does anybody imagine there’s a world where
    it makes sense to decide what happens to you based on what your dad did?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 算法偏见的问题现在非常普遍，随着算法在特定政策决策、司法决策以及日常决策中的广泛使用，这个问题变得越来越严重。其中一些问题实际上是产品管理决策中的人员在最初就应该看到的，这些问题在任何定义下都是没有意义和不合理的。例如，阿贝·龚指出的这些问题——这些问题既用于预审，即谁需要支付保释金，这些人甚至还没有被定罪，也用于判决以及谁获得假释。尽管存在所有的缺陷，这在去年被威斯康星州最高法院维持了。所以你是否必须因为支付不起保释金而留在监狱，你的刑期有多长，你在监狱里待多久取决于你父亲做了什么，你的父母是否离婚，你的朋友是谁，以及你住在哪里。现在事实证明这些算法实际上非常糟糕，最近的一些分析显示它们基本上比随机还要糟糕。但即使公司在这些统计上的相关性上很有信心，有人能想象出一个世界，在那里根据你父亲的行为来决定发生什么吗？
- en: A lot of this stuff at the basic level is obviously unreasonable and a lot of
    it just fails in these ways that you can see empirically that these kind of runaway
    feedback loops must have happened and these over generalizations must have happened.
    For example, these are the cross tabs that anybody working in any field using
    these algorithm should be preparing. So prediction of likelihood of reoffending
    for black vs. white defendants, we can just calculate this very simply. Of the
    people that were labeled high-risk but didn’t reoffend — they were 23.5% white
    but about twice that African American. Where else, those that were labeled lower
    risk but did reoffend was half the white people and only 28% of the African American.
    This is the kind of stuff where at least if you are taking the technologies we’ve
    been talking about and putting the production in any way, building an API for
    other people, providing training for people, or whatever — then at least make
    sure that what you are doing can be tracked in a way that people know what’s going
    on so at least they are informed. I think it’s a mistake in my opinion to assume
    that people are evil and trying to break society. I think I would prefer to start
    with an assumption of if people are doing dumb stuff, it’s because they don’t
    know better. So at least make sure they have this information. I find very few
    ML practitioners thinking about what is the information they should be presenting
    in their interface. Then often I’ll talk to data scientists who will say “oh,
    the stuff I’m working on doesn’t have a societal impact.” Really? A number of
    people who think that what they are doing is entirely pointless? Come on. People
    are paying you to do it for a reason. It’s going to impact people in some way.
    So think about what that is.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本层面上，很多事情显然是不合理的，很多事情只是以这种你可以从经验上看到的方式失败了，这种失控的反馈循环一定发生过，这种过度概括一定发生过。例如，任何领域工作的人都应该准备这些交叉表，使用这些算法。因此，对黑人和白人被告重新犯罪的可能性的预测，我们可以很简单地计算出来。那些被标记为高风险但没有再犯的人中，23.5%是白人，而非洲裔美国人大约是白人的两倍。而那些被标记为低风险但再犯的人中，白人只有非洲裔美国人的一半，而非洲裔美国人只有28%。这就是这种情况，至少如果你正在使用我们谈论过的技术，并以任何方式将其投入生产，为其他人构建API，为人们提供培训，或者其他什么——那么至少确保你所做的事情可以被追踪，以便人们知道发生了什么，至少他们是知情的。我认为假设人们是邪恶的，试图破坏社会是错误的。我认为我更愿意从一个假设开始，即如果人们做了愚蠢的事情，那是因为他们不知道更好的方法。所以至少确保他们有这些信息。我发现很少有机器学习从业者考虑他们的界面中应该呈现什么信息。然后我经常会和数据科学家交谈，他们会说“哦，我正在研究的东西对社会没有影响。”真的吗？有很多人认为他们正在做的事情完全毫无意义吗？来吧。人们付钱让你做这件事是有原因的。它会以某种方式影响人们。所以考虑一下这是什么。
- en: Responsibility in hiring [[52:46](https://youtu.be/xXXiC4YRGrQ?t=52m46s)]
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在招聘中的责任
- en: The other thing I know is a lot of people involved here are hiring people and
    if you are hiring people, I guess you are all very familiar with the fast.ai philosophy
    now which is the basic premise that, and I thin it comes back to this idea that
    I don’t think people on the whole are evil, I think they need to be informed and
    have tools. So we are trying to give as many people the tools as possible that
    they need and particularly we are trying to put those tools in the hands of a
    more diverse range of people. So if you are involved in hiring decisions, perhaps
    you can keep this kind of philosophy in mind as well. If you are not just hiring
    a wider range of people, but also promoting a wider range of people, and providing
    appropriate career management for a wider range of people, apart from anything
    else, your company will do better. It actually turns out that more diverse teams
    are more creative and tend to solve problems more quickly and better than less
    diverse teams, but also you might avoid these kind of awful screw-ups which, at
    one level, are bad for the world and another level if you ever get found out,
    they can destroy your company.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道的另一件事是很多参与其中的人都在招聘人才，如果你在招聘人才，我想你现在都非常熟悉fast.ai的理念，这基本上是这样一个前提，我认为人们总体上并不邪恶，我认为他们需要被告知并拥有工具。因此，我们正在尽可能地为尽可能多的人提供他们需要的工具，特别是我们正在尝试将这些工具交到更广泛人群的手中。因此，如果你参与招聘决策，也许你可以记住这种理念。如果你不仅仅是招聘更广泛的人才，而且还提拔更广泛的人才，并为更广泛的人提供适当的职业管理，除了其他任何事情，你的公司会做得更好。事实证明，更多样化的团队更有创造力，往往比不那么多样化的团队更快更好地解决问题，而且你也可能避免这些糟糕的失误，这在某种程度上对世界是有害的，而在另一层面，如果你被发现，它们可能毁掉你的公司。
- en: IBM & “Death’s Calculator” [[54:08](https://youtu.be/xXXiC4YRGrQ?t=54m8s)]
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IBM和“死亡计算器”
- en: Also they can destroy you or at least make you look pretty bad in history. A
    couple of examples, one is going right back to the second world war. IBM provided
    all of the infrastructure necessary to track the Holocaust. These are the forms
    they used and they had different code — Jews were 8, Gypsies were 12, death in
    the gas chambers was 6, and they all went on these punch cards. You can go and
    look at these punch cards in museums now and this has actually been reviewed by
    a Swiss judge who said that IBM’s technical assistance facilitated the task of
    the Nazis and the commission their crimes against humanity. It is interesting
    to read back the history from these times to see what was going through the minds
    of people at IBM at that time. What was clearly going through the minds was the
    opportunity to show technical superiority, the opportunity to test out their new
    systems, and of course the extraordinary amount of money that they were making.
    When you do something which at some point down the line turns out to be a problem,
    even if you were told to do it, that can turn out to be a problem for you personally.
    For example, you all remember the diesel emission scandal in VW. Who is the one
    guy that went to jail? It was the engineer just doing his job. If all of this
    stuff about actually not messing up the world isn’t enough to convince you, it
    can mess up your life too. If you do something that turns out to cause problems
    even though somebody told you to do it, you can absolutely be held criminally
    responsible. Aleksandr Kogan was the guy that handed over the Cambridge Analytica
    data. He is a Cambridge academic. Now a very famous Cambridge academic the world
    over for doing his part to destroy the foundations of democracy. This is not how
    we want to go down in history.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 他们也可以摧毁你，或者至少让你在历史上看起来很糟糕。举几个例子，一个是回到第二次世界大战。IBM提供了跟踪大屠杀所需的所有基础设施。这些是他们使用的表格，它们有不同的代码
    - 犹太人是8，吉普赛人是12，毒气室中的死亡是6，所有这些都记录在这些打孔卡上。现在你可以去博物馆看这些打孔卡，这实际上已经被一位瑞士法官审查过，他说IBM的技术支持促进了纳粹分子的任务并促使他们犯下反人类罪行。回顾这些时期的历史，看看当时IBM的人们在想什么是很有趣的。当时人们明显在想的是展示技术优势的机会，测试他们的新系统的机会，当然还有他们赚取的巨额利润。当你做了一些事情，即使在某个时候会变成问题，即使你被告知要这样做，这对你个人来说也可能成为问题。例如，大家都记得大众柴油排放丑闻。谁是唯一一个入狱的人？那就是只是在做他的工作的工程师。如果所有这些关于实际上不要搞砸世界的东西还不足以说服你，那么它也可能毁掉你的生活。如果你做了一些事情，结果导致问题，即使有人告诉你要这样做，你绝对可能被追究刑事责任。亚历山大·科根就是那个交出剑桥分析数据的人。他是一位剑桥学者。现在是一位全球著名的剑桥学者，因为他为摧毁民主的基础做出了自己的贡献。这不是我们想要留在历史上的方式。
- en: '**Question:** In one of your tweets, you said dropout is patented [[56:50](https://youtu.be/xXXiC4YRGrQ?t=56m50s)].
    I think this is about WaveNet patent from Google. What does it mean? Can you please
    share more insight on this subject? Does it mean that we will have to pay to use
    dropout in the future? One of the patent holders is Geoffrey Hinton. So what?
    Isn’t that great? Invention is all about patents, blah blah. My answer is no.
    Patents have gone wildly crazy. The amount of things that are patentable that
    we talk about every week would be dozens. It’s so easy to come up with a little
    tweak and then if you turn that into a patent to stop everybody from using that
    little tweak for the next 14 years and you end up with a situation we have now
    where everything is patented in 50 different ways. Then you get these patent trolls
    who have made a very good business out of buying lots of crappy little patents
    and then suing anybody who accidentally turned out did that thing like putting
    rounded corners on buttons. So what does it mean for us that a lot of stuff is
    patented in deep learning? I don’t know.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：** 在你的一条推特中，你说dropout被专利化了[[56:50](https://youtu.be/xXXiC4YRGrQ?t=56m50s)]。我认为这是关于Google的WaveNet专利。这是什么意思？你能分享更多关于这个主题的见解吗？这意味着我们将来要付费使用dropout吗？专利持有人之一是Geoffrey
    Hinton。那又怎样？这不是很棒吗？发明就是关于专利的，啦啦啦。我的答案是否定的。专利已经变得疯狂。我们每周讨论的可以被专利化的东西数量会有几十个。很容易想出一个小调整，然后如果你把它变成专利来阻止每个人在接下来的14年内使用那个小调整，最终我们就会面临现在的情况，所有东西都以50种不同的方式被专利化。然后你会遇到这些专利流氓，他们通过购买大量垃圾专利然后起诉任何无意中做了那件事的人，比如给按钮加上圆角。那么对于我们来说，深度学习中有很多东西被专利化意味着什么？我不知道。'
- en: One of the main people doing this is Google and people from Google who replied
    to this patent tend to assume that Google doing it because they want to have it
    defensively so if somebody sues them, they can say don’t sue us we’ll sue you
    back because we have all these patents. The problem is that as far as I know,
    they haven’t signed what’s called a defensive patent pledge so basically you can
    sign a legally binding document that says our patent portfolio will only be used
    in defense and not offense. Even if you believe all the management of Google would
    never turn into a patent troll, you’ve got to remember that management changes.
    To give you a specific example I know, the somewhat recent CFO of Google has a
    much more aggressive stance towards the PNL, I don’t know, maybe she might decide
    that they should start monetizing their patents or maybe the group that made that
    patent might get spun off and then sold to another company that might end up in
    private equity hands and decide to monetize the patents or whatever. So I think
    it’s a problem. There has been a big shift legally recently away from software
    patents actually having any legal standing, so it’s possible that these will all
    end up thrown out of court but the reality is that anything but a big company
    is unlikely to have the financial ability to defend themselves against one of
    these huge patent trolls.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 做这个工作的主要人员之一是Google，而且来自Google的人回应这个专利时倾向于认为Google这样做是因为他们想要防御性地拥有它，所以如果有人起诉他们，他们可以说不要起诉我们，我们会反诉你，因为我们有所有这些专利。问题是据我所知，他们还没有签署所谓的防御性专利承诺，所以基本上你可以签署一个法律约束文件，说我们的专利组合只会用于防御而不是进攻。即使你相信Google的所有管理层永远不会变成专利流氓，你必须记住管理层会变化。给你一个具体的例子，我知道，Google的最近的CFO对PNL有更积极的态度，我不知道，也许她会决定他们应该开始变现他们的专利，或者也许做出那个专利的团队可能会被分拆然后卖给另一家公司，最终可能会落入私募股权手中并决定变现专利或其他。所以我认为这是一个问题。最近在法律上有一个从软件专利转向实际上没有任何法律地位的大变化，所以这些可能最终都会被驳回，但现实是，任何不是大公司的人都不太可能有财务能力来抵抗这些庞大的专利流氓。
- en: You can’t avoid using patented stuff if you write code. I wouldn’t be surprised
    if most lines of code you write have patents on them. Actually funnily enough,
    the best thing to do is not to study the patents because if you do and you infringe
    knowingly then the penalties are worse. So the best thing to do is to put your
    hands in your ear, sing a song, and get back to work. So the thing about dropouts
    patented, forget I said that. You don’t know that. You skipped that bit.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你写代码，就无法避免使用专利的东西。我不会感到惊讶，如果你写的大部分代码都有专利。实际上，有趣的是，最好的做法不是研究专利，因为如果你故意侵犯，惩罚会更严重。所以最好的做法是把手放在耳朵上，唱首歌，然后继续工作。所以关于dropout被专利化的事情，忘记我说过的。你不知道那个。你跳过那部分。
- en: Style Transfer [[1:01:28](https://youtu.be/xXXiC4YRGrQ?t=1h1m28s)]
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风格迁移[[1:01:28](https://youtu.be/xXXiC4YRGrQ?t=1h1m28s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/style-transfer.ipynb)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/style-transfer.ipynb)'
- en: '[https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)'
- en: This is super fun — artistic style. We are going a bit retro here because this
    is actually the original artistic style paper and there’s been a lot of updates
    to it and a lot of different approaches and I actually think in many ways the
    original is the best. We are going to look at some of the newer approaches as
    well, but I actually think the original is a terrific way to do it even with everything
    that’s gone since. Let’s jump to the code.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常有趣——艺术风格。我们在这里有点复古，因为这实际上是最初的艺术风格论文，后来有很多更新和很多不同的方法，我实际上认为在很多方面最初的方法是最好的。我们也会看一些更新的方法，但我实际上认为最初的方法是一个很棒的方式，即使在之后的一切发展之后。让我们来看看代码。
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The idea here is that we want to take a photo of a bird, and we want to create
    a painting that looks like Van Gogh painted the picture of the bird. Quite a bit
    of the stuff that I’m doing, by the way, uses an ImageNet. You don’t have to download
    the whole of ImageNet for any of the things I’m doing. There is an ImageNet sample
    in [files.fast.ai/data](http://files.fast.ai/data/) which has a couple of gig
    which should be plenty good enough for everything we are doing. If you want to
    get really great result, you can grab ImageNet. You can download it from [Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/data).
    The localization competition actually contains all of the classification data
    as well. If you’ve got room, it’s good to have a copy of ImageNet because it comes
    in handy all the time.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是我们想要拍摄一只鸟的照片，并且我们想要创作一幅看起来像梵高画了这只鸟的画。顺便说一句，我正在做的很多事情都使用了ImageNet。你不必为我所做的任何事情下载整个ImageNet。在[files.fast.ai/data](http://files.fast.ai/data/)中有一个ImageNet样本，它有几个G的数据，对我们正在做的一切来说应该足够了。如果你想要得到真正出色的结果，你可以获取ImageNet。你可以从[Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/data)下载。定位竞赛实际上包含了所有的分类数据。如果你有空间，最好拥有一份ImageNet的副本，因为它随时都会派上用场。
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'So I just grabbed the bird out of my ImageNet folder and there is my bird:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我刚从我的ImageNet文件夹中拿出了这只鸟，这就是我的鸟：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'What I’m going to do is I’m going to start with this picture:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我要做的是从这张图片开始：
- en: And I’m going to try to make it more and more like a picture of the bird painted
    by Van Gogh. The way I do that is actually very simple. You’re all familiar with
    it [[1:03:44](https://youtu.be/xXXiC4YRGrQ?t=1h3m44s)]. We will create a loss
    function which we will call *f*. The loss function is going to take as input a
    picture and spit out as output a value. The value will be lower if the image looks
    more like the bird photo painted by Van Gogh. Having written that loss function,
    we will then use the PyTorch gradient and optimizers. Gradient times the learning
    rate, and and we are not going to update any weights, we are going to update the
    pixels of the input image to make it a little bit more like a picture which would
    be a bird painted by Van Gogh. And we will stick it through the loss function
    again to get more gradients, and do it again and again. That’s it. So it’s identical
    to how we solve every problem. You know I’m a one-trick pony, right? This is my
    only trick. Create a loss function, use it to get some gradients, multiply it
    by learning rates to update something, always before, we’ve updated weights in
    a model but today, we are not going to do that. They’re going to update the pixels
    in the input. But it’s no different at all. We are just taking the gradient with
    respect to the input rather than respect to the weights. That’s it. So we are
    nearly done.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我将尝试让它越来越像梵高画的鸟的图片。我做的方法实际上非常简单。你们都很熟悉它。我们将创建一个损失函数，我们将称之为*f*。损失函数将以一张图片作为输入，并输出一个值。如果图像看起来更像梵高画的鸟照片，那么这个值将更低。编写了这个损失函数之后，我们将使用PyTorch的梯度和优化器。梯度乘以学习率，我们不会更新任何权重，而是会更新输入图像的像素，使其更像梵高画的鸟的图片。然后我们再次通过损失函数来获取更多的梯度，一遍又一遍地进行。就是这样。所以这与我们解决每个问题的方式是相同的。你们知道我是一个只会一招的人，对吧？这是我的唯一招数。创建一个损失函数，用它来获取一些梯度，乘以学习率来更新某些东西，以前我们总是更新模型中的权重，但今天我们不会这样做。我们将更新输入图像中的像素。但实际上并没有什么不同。我们只是针对输入而不是针对权重来获取梯度。就是这样。我们快要完成了。
- en: Let’s do a couple more things [[1:05:49](https://youtu.be/xXXiC4YRGrQ?t=1h5m49s)].
    Let’s mention here that there’s going to be two more inputs to our loss function
    One is the picture of the bird. The second is an artwork by Van Gogh. By having
    those as inputs as well, that means we’ll be able to rerun the function later
    to make it look like a bird painted by Monet or a jumbo jet painted by Van Gogh,
    etc. Those are going to be the three inputs. Initially, as we discussed, our input
    here is some random noise. We start with some random noise, use the loss function,
    get the gradients, make it a little bit more like a bird painted by Van Gogh,
    and so forth.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做更多的事情。让我们在这里提到，我们的损失函数将有两个额外的输入。一个是鸟的图片。第二个是梵高的一幅艺术作品。通过将它们作为输入，这意味着我们以后可以重新运行这个函数，使其看起来像梵高画的鸟，或者像莫奈画的鸟，或者像梵高画的大型喷气式飞机等。这些将是三个输入。最初，正如我们讨论过的，我们的输入是一些随机噪音。我们从一些随机噪音开始，使用损失函数，获取梯度，使其更像梵高画的鸟，依此类推。
- en: 'So the only outstanding question which I guess we can talk about briefly is
    how we calculate how much our image looks like this bird painted by Van Gogh [[1:07:09](https://youtu.be/xXXiC4YRGrQ?t=1h7m9s)].
    Let’s split it into two parts:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我猜我们可以简要讨论的唯一未解决的问题是我们如何计算我们的图像看起来有多像梵高画的这只鸟。让我们将其分为两部分：
- en: '**Content Loss**: Returns a value that’s lower if it looks more like the bird
    (not just any bird, the specific bird that we have coming in).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**内容损失**：返回一个值，如果看起来更像这只鸟（不只是任何鸟，而是我们要处理的特定鸟）。'
- en: '**Style Loss**: Returns a lower number if the image is more like V.G.’s style'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**风格损失**：如果图像更像V.G.的风格，则返回一个较低的数字。'
- en: There is one way to do the content loss which is very simple — we could look
    at the pixel of the output, compare them to the pixel of the bird, and do a mean
    squared error, and add them up. So if we did that, I ran this for a while. Eventually
    our image would turn into an image of the bird. You should try it. You should
    try this as an exercise. Try to use the optimizer in PyTorch to start with a random
    image and turn it into another image by using mean squared error pixel loss. Not
    terribly exciting but that would be step one.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种非常简单的计算内容损失的方法——我们可以查看输出的像素，将它们与鸟的像素进行比较，计算均方误差，然后相加。所以如果我们这样做，我运行了一段时间。最终我们的图像会变成一只鸟的图像。你应该尝试一下。你应该尝试这个作为练习。尝试使用PyTorch中的优化器，从一个随机图像开始，通过使用均方误差像素损失将其转变为另一幅图像。这并不是非常令人兴奋，但这将是第一步。
- en: The problem is, even if we already had our style loss function working beautifully
    and then presumably, what we are going to do is we are going to add these two
    together, and then one of them, we’ll multiply by some lambda to adjust how much
    style versus how much content. Assuming we had a style loss and we picked some
    sensible lambda, if we used pixel wise content loss then anything that makes it
    look more like Van Gogh and less like the exact photo, the exact background, the
    exact contrast, lighting, everything will increase the content loss — which is
    not what we want. We want it to look like the bird but not in the same way. It
    is still going to have the same two eyes in the same place and be the same kind
    of shape and so forth, but not the same representation. So what we are going to
    do is, this is going to shock you, we are going to use a neural network! We are
    going to use the VGG neural network because that’s what I used last year and I
    didn’t have time to see if other things worked so you can try that yourself during
    the week.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，即使我们已经有了我们的风格损失函数运行得很好，然后假设我们要做的是将这两者相加，然后其中一个，我们将乘以一些λ来调整风格与内容的比例。假设我们有一个风格损失并选择了一些合理的λ，如果我们使用像素级的内容损失，那么任何使其看起来更像梵高而不是完全像照片、背景、对比度、光照等的东西都会增加内容损失——这不是我们想要的。我们希望它看起来像鸟，但不是以相同的方式。它仍然会有相同位置的两只眼睛，相同的形状等等，但不是相同的表示。所以我们要做的是，这可能会让您震惊，我们要使用一个神经网络！我们将使用VGG神经网络，因为那是我去年使用的，我没有时间看其他东西是否有效，所以您可以在这一周自己尝试。
- en: The VGG network is something which takes in an input and sticks it through a
    number of layers, and I’m going to treat these as just the convolutional layers
    there’s obviously ReLU there and if it’s a VGG with batch norm, which most are
    today, then it’s also got batch norm. There’s some max pooling and so forth but
    that’s fine. What we could do is, we could take one of these convolutional activations
    and then rather than comparing the pixels of this bird, we could instead compare
    the VGG layer 5 activations of this (bird painted by V.G.) to the VGG layer 5
    activations of our original bird (or layer 6, or layer 7, etc). So why might that
    be more interesting? Well for one thing, it wouldn’t be the same bird. It wouldn’t
    be exactly the same because we are not checking the pixels. We are checking some
    later set of activations. So what are those later sets of activations contain?
    Assuming it’s after some max pooling, they contain a smaller grid — so it’s less
    specific about where things are. And rather than containing pixel color values,
    they are more like semantic things like is this kind of an eyeball, is this kind
    of furry, is this kind of bright, or is this kind of reflective, or laying flat,
    or whatever. So we would hope that there’s some level of semantic features through
    those layers where if we get a picture that matches those activations, then any
    picture that matches those activations looks like the bird but it’s not the same
    representation of the bird. So that’s what we are going to do. That’s what our
    content loss is going to be. People generally call this a **perceptual loss**
    because it’s really important in deep learning that you always create a new name
    for every obvious thing you do. If you compare two activations together, you are
    doing a perceptual loss. That’s it. Our content loss is going to be a perceptual
    loss. Then we will do the style loss later.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: VGG网络是一个接受输入并将其通过多个层的网络，我将把这些层视为卷积层，显然还有ReLU，如果是带有批量归一化的VGG，那么它也有批量归一化。还有一些最大池化等等，但没关系。我们可以做的是，我们可以取其中一个卷积激活，而不是比较这只鸟的像素，我们可以比较这个（由V.G.绘制的）鸟的VGG层5激活与我们原始鸟的VGG层5激活（或第6层，第7层等）。那么为什么这样更有趣呢？首先，它不会是同一只鸟。它不会完全相同，因为我们不是在检查像素。我们在检查一些后续的激活。那么这些后续的激活包含什么？假设它经过一些最大池化后，它包含一个较小的网格——所以它对事物的位置不那么具体。而不是包含像素颜色值，它们更像是语义的东西，比如这是一种眼球，这是一种毛茸茸的，这是一种明亮的，或者这是一种反射的，或者平放的，或者其他什么。因此，我们希望通过这些层有一定程度的语义特征，如果我们得到一个与这些激活匹配的图片，那么任何匹配这些激活的图片看起来像鸟，但不是相同的鸟的表示。这就是我们要做的。这就是我们的内容损失将是什么。人们通常称之为**感知损失**，因为在深度学习中，您总是为您做的每件明显的事情创造一个新名称。如果您将两个激活进行比较，您正在进行感知损失。就是这样。我们的内容损失将是感知损失。然后我们将稍后进行风格损失。
- en: Let’s start by trying to create a bird that initially is random noise and we
    are going to use perceptual loss to create something that is bird-like but it’s
    not the particular bird [[1:13:13](https://youtu.be/xXXiC4YRGrQ?t=1h13m13s)].
    We are going to start with 288 by 288\. Because we are going to do one bird, there
    is going to be no GPU memory problems. I was actually disappointed that I realized
    that I picked a rather small input image. It would be fun to try this with something
    much bigger to create a really grand scale piece. The other thing to remember
    is if you are productionizing this, you could do a whole batch at a time. People
    sometimes complain about this approach (Gatys is the lead author) the Gatys’ style
    transfer approaches being slow, and I don’t agree it’s slow. It takes a few seconds
    and you can do a whole batch in a few seconds.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从尝试创建一只最初是随机噪音的鸟开始，我们将使用感知损失来创建类似鸟的东西，但不是特定的鸟。我们将从288x288开始。因为我们只做一只鸟，所以不会出现GPU内存问题。我实际上很失望地意识到我选择了一个相当小的输入图像。尝试使用更大的图像创建一个真正宏伟的作品会很有趣。另一件事要记住的是，如果您要将其投入生产，可以一次处理整个批次。有时人们会抱怨这种方法（Gatys是主要作者）——Gatys的风格迁移方法很慢，但我不同意它很慢。只需要几秒钟，您就可以在几秒钟内处理整个批次。
- en: '[PRE15]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So we are going to stick it through some transforms for VGG16 model as per usual
    [[1:14:12](https://youtu.be/xXXiC4YRGrQ?t=1h14m12s)]. Remember, the transform
    class has dunder call method (`__call__`) so we can treat it as if it’s a function.
    If you pass an image into that, then we get the transformed image. Try not to
    treat the fast.ai and PyTorch infrastructure as a black box because it’s all designed
    to be really easy to use in a decoupled way. So this idea of that transforms are
    just “callables” (i.e. things that you can do with parentheses) comes from PyTorch
    and we totally plagiarized the idea. So with torch.vision or with fast.ai, your
    transforms are just callables. And the whole pipelines of transforms is just a
    callable.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将按照通常的做法将其通过VGG16模型的一些转换。记住，转换类有dunder call方法（`__call__`），所以我们可以将其视为一个函数。如果你将一个图像传递给它，那么我们将得到转换后的图像。尽量不要将fast.ai和PyTorch基础设施视为黑盒，因为它们都设计成非常易于以解耦的方式使用。所以这个转换只是“可调用”的想法（即用括号括起来的东西）来自于PyTorch，我们完全抄袭了这个想法。所以在torch.vision或fast.ai中，你的转换只是可调用的。整个转换流水线只是一个可调用的。
- en: '[PRE16]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we have something of 3 by 288 by 288 because PyTorch likes the channel to
    be first [[1:15:05](https://youtu.be/xXXiC4YRGrQ?t=1h15m5s)]. As you can see,
    it’s been turned into a square for us, it’s been normalized to (0, 1), all that
    normal stuff.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个3x288x288的东西，因为PyTorch喜欢通道在前面。正如你所看到的，它已经被转化为一个方形，被归一化为（0,1），所有这些正常的东西。
- en: Now we are creating a random image.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们正在创建一个随机图像。
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is something I discovered. Trying to turn this into a picture of anything
    is actually really hard. I found it very difficult to actually get an optimizer
    to get reasonable gradients that went anywhere. And just as I thought I was going
    to run out of time for this class and really embarrass myself, I realized the
    key issue is that pictures don’t look like this. They have more smoothness, so
    I turned this into the following by blurring it a little bit:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我发现的一件事。试图将这个转化为任何东西的图片实际上非常困难。我发现实际上很难让优化器获得合理的梯度，使其有所作为。就在我以为我要在这门课上耗尽时间并真正让自己尴尬的时候，我意识到关键问题是图片不是这样的。它们更加平滑，所以我稍微模糊了一下，将其转化为以下内容：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: I used a median filter — basically it is like a median pooling, effectively.
    As soon as I change it to this, it immediately started training really well. A
    number of little tweaks you have to do to get these things to work is kind of
    insane, but here is a little tweak.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了一个中值滤波器——基本上就像一个中值池化。一旦我将其改为这样，它立即开始训练得非常好。你必须做一些微小的调整才能让这些东西工作起来，这有点疯狂，但这里有一个小调整。
- en: So we start with a random image which is at least somewhat smooth [[1:16:21](https://youtu.be/xXXiC4YRGrQ?t=1h16m21s)].
    I found that my bird image had a mean of pixels that was about half of this, so
    I divided it by 2 just trying to make it a little bit easier for it to match (I
    don’t know if it matters). Turn that into a variable because this image, remember,
    we are going to be modifying those pixels with an optimization algorithm, so anything
    that’s involved in the loss function needs to be a variable. And specifically,
    it requires a gradient because we are actually updating the image.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们从一个随机图像开始，这个图像至少有一定的平滑度。我发现我的鸟类图像的像素均值大约是这个值的一半，所以我将其除以2，只是试图让匹配变得更容易一些（我不知道这是否重要）。将其转化为一个变量，因为这个图像，记住，我们将使用优化算法修改这些像素，所以任何涉及损失函数的东西都需要是一个变量。并且，它需要梯度，因为我们实际上是在更新图像。
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: So we now have a mini batch of 1, 3 channels, 288 by 288 random noise.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们有了一个大小为1的小批量，3个通道，288x288的随机噪声。
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We are going to use, for no particular reason, the 37th layer of VGG. If you
    print out the VGG network (you can just type in `m_vgg` and prints it out), you’ll
    see that this is mid to late stage layer. So we can just grab the first 37 layers
    and turn it into a sequential model. So now we have a subset of VGG that will
    spit out some mid layer activations, and that’s what the model is going to be.
    So we can take our actual bird image and we want to create a mini batch of one.
    Remember, if you slice in Numpy with `None`, also known as `np.newaxis`, it introduces
    a new unit axis in that point. Here, I want to create an axis of size 1 to say
    this is a mini batch of size one. So slicing with None just like I did here (`opt_img_v
    = V(opt_img[**None**], requires_grad=**True**)`) to get one unit axis at the front.
    Then we turn that into a variable and this one doesn’t need to be updated, so
    we use `VV` to say you don’t need gradients for this guy. So that is going to
    give us our target activations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用，没有特定原因，VGG的第37层。如果你打印出VGG网络（你只需输入`m_vgg`并打印出来），你会看到这是中后期的层。所以我们可以只获取前37层并将其转化为一个顺序模型。现在我们有了一个VGG的子集，它将输出一些中间层的激活，这就是模型将要做的事情。所以我们可以拿到我们实际的鸟类图像，我们想创建一个大小为一的小批量。记住，如果你在Numpy中使用`None`进行切片，也就是`np.newaxis`，它会在那个点引入一个新的单位轴。这里，我想创建一个大小为1的轴，表示这是一个大小为一的小批量。所以就像我在这里做的一样（`opt_img_v
    = V(opt_img[**None**], requires_grad=**True**)`）使用`None`进行切片，在前面得到一个单位轴。然后我们将其转化为一个变量，这个变量不需要更新，所以我们使用`VV`来表示你不需要为这个变量计算梯度。这将给我们我们的目标激活。
- en: We’ve taken our bird image
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经拿到了我们的鸟类图像。
- en: Turned it into a variable
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其转化为一个变量
- en: Stuck it through our model to grab the 37th layer activations which is our target.
    We want our content loss to be this set of activations.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其通过我们的模型传递，以获取第37层的激活，这是我们的目标。我们希望我们的内容损失是这组激活。
- en: We are going to create an optimizer (we will go back to the details of this
    in a moment)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将创建一个优化器（我们稍后会回到这个细节）
- en: We are going to step a bunch of times
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将进行多次迭代
- en: Zero the gradients
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度清零
- en: Call some loss function
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用一些损失函数
- en: Loss.backward()
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失反向传播()
- en: That’s the high level version. I’m going to come back to the details in a moment,
    but the key thing is that the loss function we are passing in that randomly generated
    image — the variable of optimization image. So we pass that to our loss function
    and it’s going to update this using the loss function, and the loss function is
    the mean squared error loss comparing our current optimization image passed through
    our VGG to get the intermediate activations and comparing it to our target activations.
    We run that bunch of times and we’ll print it out. And we have our bird but not
    the representation of it.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是高层次的版本。我一会儿会回到细节，但关键是我们传入那个随机生成的图像的损失函数——优化图像的变量。因此，我们将该图像传递给我们的损失函数，它将使用损失函数进行更新，而损失函数是通过将我们当前的优化图像通过我们的VGG获取中间激活，并将其与目标激活进行比较来计算均方误差损失。我们运行一堆次数，然后将其打印出来。我们有我们的鸟，但没有它的表示形式。
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Broyden–Fletcher–Goldfarb–Shanno (BFGS) [[1:20:18](https://youtu.be/xXXiC4YRGrQ?t=1h20m18s)]
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Broyden–Fletcher–Goldfarb–Shanno（BFGS）
- en: A couple of new details here. One is a weird optimizer (`optim.LBFGS`). Anybody
    who’s done certain parts of math and computer science courses comes into deep
    learning discovers we use all this stuff like Adam and the SGD and always assume
    that nobody in the field knows the first thing about computer science and immediately
    says “any of you guys tried using BFGS?” There’s basically a long history of a
    totally different kind of algorithm for optimization that we don’t use to train
    neural networks. And of course the answer is actually the people who have spent
    decades studying neural networks do know a thing or two about computer science
    and it turns out these techniques on the whole don’t work very well. But it’s
    actually going to work well for this, and it’s a good opportunity to talk about
    an interesting algorithm for those of you that haven’t studied this type of optimization
    algorithm at school. BFGS (initials of four different people) and the L stands
    for limited memory. It is an optimizer so as an optimizer, that means that there’s
    some loss function and it’s going to use some gradients (not all optimizers use
    gradients but all the ones we use do) to find a direction to go and try to make
    the loss function go lower and lower by adjusting some parameters. It’s just an
    optimizer. But it’s an interesting kind of optimizer because it does a bit more
    work than the ones we’re used to on each step. Specifically, the way it works
    is it starts the same way that we are used to which is we just pick somewhere
    to get started and in this case, we’ve picked a random image as you saw. As per
    usual, we calculate the gradient. But we then don’t just take a step but we actually
    do is as well as finding the gradient, we also try to find the second derivative.
    The second derivative says how fast does the gradient change.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些新的细节。其中一个是一个奇怪的优化器（`optim.LBFGS`）。任何完成过某些数学和计算机科学课程的人进入深度学习领域都会发现我们使用像Adam和SGD这样的东西，并且总是假设该领域的人对计算机科学一无所知，立即说“你们有人尝试过使用BFGS吗？”实际上，我们并没有使用来训练神经网络的完全不同类型的优化算法的长期历史。当然，事实上，那些花了几十年研究神经网络的人确实对计算机科学有所了解，结果表明这些技术整体上并不工作得很好。但实际上，这对我们来说会很有效，并且这是一个很好的机会，让那些在学校没有学习过这种类型的优化算法的人了解一个有趣的算法。BFGS（四个不同人的首字母缩写），L代表有限内存。它是一个优化器，也就是说，有一些损失函数，它将使用一些梯度（并非所有优化器都使用梯度，但我们使用的所有优化器都会）来找到一个方向，并尝试通过调整一些参数使损失函数降低。它只是一个优化器。但它是一种有趣的优化器，因为它在每一步上做的工作比我们习惯的要多一点。具体来说，它的工作方式与我们习惯的方式相同，即我们只是选择一个起点，而在这种情况下，我们选择了一个随机图像，正如你所看到的。像往常一样，我们计算梯度。但我们不仅仅是采取一步，而是实际上在找到梯度的同时，我们还尝试找到二阶导数。二阶导数表示梯度变化的速度。
- en: '**Gradient**: how fast the function change'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度**：函数变化的速度'
- en: '**The second derivative**: how fast the gradient change'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**二阶导数**：梯度变化的速度'
- en: In other words, how curvy is it? The basic idea is that if you know that it’s
    not very curvy, then you can probably jump farther. But if it’s very curvy then
    you probably don’t want to jump as far. So in higher dimensions, the gradient
    is called the Jacobian and the second derivative is called the Hessian. You’ll
    see those words all the time, but that’s all they mean. Again, mathematicians
    have to invent your words for everything as well. They are just like deep learning
    researchers — maybe a bit more snooty. With BFGS, we are going to try and calculate
    the second derivative and then we are going to use that to figure out what direction
    to go and how far to go — so it’s less of a wild jump into the unknown.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它有多曲折？基本思想是，如果你知道它不太曲折，那么你可能可以跳得更远。但如果它非常曲折，那么你可能不想跳得太远。因此，在更高维度中，梯度被称为雅可比矩阵，而二阶导数被称为海森矩阵。你会经常看到这些词，但它们的意思就是这样。再次强调，数学家们也必须为每件事发明新词。他们就像深度学习研究人员一样——也许有点傲慢。使用BFGS，我们将尝试计算二阶导数，然后我们将使用它来确定前进的方向和距离——因此，这不是对未知领域的一次疯狂跳跃。
- en: Now the problem is that actually calculating the Hessian (the second derivative)
    is almost certainly not a good idea[[1:24:15](https://youtu.be/xXXiC4YRGrQ?t=1h24m15s)].
    Because in each possible direction that you are going to head, for each direction
    that you’re measuring the gradient in, you also have to calculate the Hessian
    in every direction. It gets ridiculously big. So rather than actually calculating
    it, we take a few steps and we basically look at how much the gradient is changing
    as we do each step, and we approximate the Hessian using that little function.
    Again, this seems like a really obvious thing to do but nobody thought of it until
    someone did surprisingly a long time later. Keeping track of every single step
    you take takes a lot of memory, so duh, don’t keep track of every step you take
    — just keep the last ten or twenty. And the second bit there, that’s the L to
    the LBFGS. So a limited-memory BFGS means keep the last 10 or 20 gradients, use
    that to approximate the amount of curvature, and then use the curvature in gradient
    to estimate what direction to travel and how far. That’s normally not a good idea
    in deep learning for a number of reasons. It’s obviously more work to do than
    than Adam or SGD update, and it also uses more memory — memory is much more of
    a big issue when you’ve got a GPU to store it on and hundreds of millions of weights.
    But more importantly, the mini-batch is super bumpy so figuring out curvature
    to decide exactly how far to travel is kind of polishing turds as we say (yeah,
    Australian and English expression — you get the idea). Interestingly, actually
    using the second derivative information, it turns out, is like a magnet for saddle
    points. So there’s some interesting theoretical results that basically say it
    actually sends you towards nasty flat areas of the function if you use second
    derivative information. So normally not a good idea.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，实际计算Hessian（二阶导数）几乎肯定不是一个好主意。因为在你要前进的每个可能方向上，对于你测量梯度的每个方向，你还必须在每个方向上计算Hessian。这变得非常庞大。所以我们不是真的计算它，我们走几步，基本上看一下梯度在每一步变化了多少，然后用那个小函数来近似Hessian。再次强调，这似乎是一个非常明显的事情，但直到后来有人想到了，这花了相当长的时间。跟踪每一步都需要大量内存，所以别跟踪每一步，只保留最后的十步或二十步。第二部分，就是L到LBFGS。有限内存的BFGS意味着保留最后的10或20个梯度，用它来近似曲率的量，然后用曲率和梯度来估计前进的方向和距离。在深度学习中通常不是一个好主意，有很多原因。这比Adam或SGD更新更费力，也使用更多内存，当你有一个GPU来存储和数亿个权重时，内存就成了一个更大的问题。但更重要的是，小批量是非常颠簸的，所以弄清楚曲率以决定到底要前进多远，有点像我们说的磨亮了粪便（是的，澳大利亚和英国的表达方式，你懂的）。有趣的是，实际上使用二阶导数信息，结果就像是一个吸引鞍点的磁铁。因此，有一些有趣的理论结果基本上说，如果使用二阶导数信息，它实际上会把你引向函数的恶劣平坦区域。所以通常不是一个好主意。
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: But in this case [[1:26:40](https://youtu.be/xXXiC4YRGrQ?t=1h26m40s)], we are
    not optimizing weights, we are optimizing pixels so all the rules change and actually
    turns out BFGS does make sense. Because it does more work each time, it’s a different
    kind of optimizer, the API is a little bit different in PyTorch. As you can see
    here, when you say `optimizer.step`, you actually pass in the loss function. So
    our loss function is to call `step` with a particular loss function which is our
    activation loss (`actn_loss`). And inside the loop, you don’t say step, step,
    step. But rather it looks like this. So it’s a little bit different and you’re
    welcome to try and rewrite this to use SGD, it’ll still work. It’ll just take
    a bit longer — I haven’t tried it with SGD yet and I’d be interested to know how
    much longer it takes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这种情况下，我们不是在优化权重，而是在优化像素，所以所有规则都改变了，实际上BFGS是有意义的。因为每次它做更多的工作，它是一种不同类型的优化器，PyTorch中的API也有点不同。正如你在这里看到的，当你说`optimizer.step`时，你实际上传入了损失函数。所以我们的损失函数是调用`step`，传入一个特定的损失函数，即我们的激活损失（`actn_loss`）。在循环内部，你不会说step，step，step。而是看起来像这样。所以有点不同，你可以尝试重写这个来使用SGD，它仍然会工作。只是会花更长的时间，我还没有尝试过用SGD，我很想知道它需要多长时间。
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'So you can see the loss function going down [[1:27:38](https://youtu.be/xXXiC4YRGrQ?t=1h27m38s)].
    The mean squared error between the activations at layer 37 of our VGG model for
    our optimized image vs. the target activations, remember the target activations
    were the VGG applied to our bird. Make sense? So we’ve now got a content loss.
    Now, one thing I’ll say about this content loss is we don’t know which layer is
    going to work the best. So it would be nice if we were able to experiment a little
    bit more. And the way it is here is annoying:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到损失函数在下降。我们的VGG模型第37层的激活与目标激活之间的均方误差，记住目标激活是应用于我们的鸟的VGG。明白了吗？所以现在我们有了一个内容损失。现在，关于这个内容损失，我要说的一件事是我们不知道哪一层会起到最好的作用。所以如果我们能多做一些实验就好了。现在的情况很烦人：
- en: Maybe we even want to use multiple layers. So rather than lopping off all of
    the layers after the one we want, wouldn’t it be nice if we could somehow grab
    the activations of a few layers as it calculates. Now, we already know one way
    to do that back when we did SSD, we actually wrote our own network which had a
    number of outputs. Remember? The different convolutional layers, we spat out a
    different `oconv` thing? But I don’t really want to go and add that to the torch.vision
    ResNet model especially not if later on, I want to try torch.vision VGG model,
    and then I want to try NASNet-A model, I don’t want to go into all of them and
    change their outputs. Beside which, I’d like to easily be able to turn certain
    activations on and off on demand. So we briefly touched before this idea that
    PyTorch has these fantastic things called hooks. You can have forward hooks that
    let you plug anything you like into the forward pass of a calculation or a backward
    hook that lets you plug anything you like into the backward pass. So we are going
    to create the world’s simplest forward hook.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们甚至想使用多个层。所以，与其截断我们想要的层之后的所有层，不如我们能够以某种方式抓取几个层的激活值。现在，我们已经知道一种方法可以在我们做SSD时做到这一点，我们实际上编写了一个具有多个输出的网络。记得吗？不同的卷积层，我们吐出了一个不同的`oconv`东西？但我真的不想去添加到torch.vision
    ResNet模型中，特别是如果以后我想尝试torch.vision VGG模型，然后我想尝试NASNet-A模型，我不想去修改它们的输出。此外，我希望能够轻松地按需打开和关闭某些激活。所以我们之前简要提到过这个想法，PyTorch有这些名为hooks的奇妙东西。您可以有前向钩子，让您将任何您喜欢的东西插入到计算的前向传递中，或者有后向钩子，让您将任何您喜欢的东西插入到后向传递中。所以我们将创建世界上最简单的前向钩子。
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Forward hook [[1:29:42](https://youtu.be/xXXiC4YRGrQ?t=1h29m42s)]
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向钩子[[1:29:42](https://youtu.be/xXXiC4YRGrQ?t=1h29m42s)]
- en: This is one of these things that almost nobody knows about so almost any code
    you find on the internet that implements style transfer will have all kind of
    horrible hacks rather than using forward hooks. But forward hook is really easy.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是几乎没有人知道的事情之一，因此几乎在互联网上找到的任何实现风格转移的代码都会有各种可怕的黑客，而不是使用前向钩子。但前向钩子真的很容易。
- en: To create a forward hook, you just create a class. The class has to have something
    called `hook_fn`. And your hook function is going to receive the `module` that
    you’ve hooked, the `input` for the forward pass, and the `output` then you do
    whatever you’d like. So what I’m going to do is I’m just going to store the output
    of this module in some attribute. That’s it. So `hook_fn` can actually be called
    anything you like, but “hook function” seems to be the standard because, as you
    can see, what happens in the constructor is I store inside some attribute the
    result of `m.register_forward_hook` (`m` is going to be the layer that I’m going
    to hook) and pass in the function that you want to be called when the module’s
    forward method is called. When its forward method is called, it will call `self.hook_fn`
    which will store the output in an attribute called `features`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个前向钩子，只需创建一个类。该类必须有一个名为`hook_fn`的东西。您的钩子函数将接收您挂钩的`module`，前向传递的`input`和`output`，然后您可以做任何您喜欢的事情。所以我要做的就是将这个模块的输出存储在某个属性中。就是这样。所以`hook_fn`实际上可以被称为您喜欢的任何东西，但“hook
    function”似乎是标准，因为您可以看到，在构造函数中发生的是我在某个属性中存储了`m.register_forward_hook`的结果（`m`将是我要挂钩的层），并传入您希望在调用模块的前向方法时调用的函数。当调用其前向方法时，它将调用`self.hook_fn`，该函数将在名为`features`的属性中存储输出。
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So now what we can do is we can create a VGG as before. And let’s set it to
    not trainable so we don’t waste time and memory calculating gradients for it.
    And let’s go through and find all the max pool layers. So let’s go through all
    of the children of this module and if it’s a max pool layer, let’s spit out index
    minus 1 — so that’s going to give me the layer before the max pool. In general,
    the layer before a max pool or stride 2 conv is a very layer. It’s the most complete
    representation we have at that grid cell size because the very next layer is changing
    the grid. So that seems to me like a good place to grab the content loss from.
    The best most semantic, most interesting content we have at that grid size. So
    that’s why I’m going to pick those indexes.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以像以前一样创建一个VGG。让我们将其设置为不可训练，这样我们就不会浪费时间和内存来计算梯度。让我们遍历并找到所有的最大池层。让我们遍历这个模块的所有子层，如果是一个最大池层，让我们输出索引减1——这样就会给我最大池之前的层。通常，最大池或步长2卷积之前的层是一个非常完整的表示，因为下一层正在改变网格。所以这对我来说是一个很好的地方来获取内容损失。我们在该网格大小上拥有的最语义化、最有趣的内容。这就是为什么我要选择这些索引。
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: These are the indexes of the last layer before each max pool in VGG [[1:32:30](https://youtu.be/xXXiC4YRGrQ?t=1h32m30s)].
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是VGG中每个最大池之前的最后一层的索引[[1:32:30](https://youtu.be/xXXiC4YRGrQ?t=1h32m30s)]。
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: I’m going to grab `32` — no particular reason, just try something else. So I’m
    going to say `block_ends[3]` (i.e. 32). `children(m_vgg)[block_ends[3]]` will
    give me the 32nd layer of VGG as a module.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我要获取`32`——没有特定的原因，只是尝试其他东西。所以我要说`block_ends[3]`（即32）。`children(m_vgg)[block_ends[3]]`会给我VGG的第32层作为一个模块。
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then if I call the `SaveFeatures` constructor, it’s going to go:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果我调用`SaveFeatures`构造函数，它会执行：
- en: '`self.hook = {32nd layer of VGG}.register_forward_hook(self.hook_fn)`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`self.hook = {VGG的第32层}.register_forward_hook(self.hook_fn)`'
- en: Now, every time I do a forward pass on this VGG model, it’s going to store the
    32nd layer’s output inside `sf.features`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每当我对这个VGG模型进行前向传递时，它都会将第32层的输出存储在`sf.features`中。
- en: '[PRE29]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: See here [[1:33:33](https://youtu.be/xXXiC4YRGrQ?t=1h33m33s)], I’m calling my
    VGG network, but I’m not storing it anywhere. I’m not saying `activations = m_vgg(VV(img_tfm[**None**]))`.
    I’m calling it, throwing away the answer, and then grabbing the features we stored
    in our `SaveFeatures` object.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里[[1:33:33](https://youtu.be/xXXiC4YRGrQ?t=1h33m33s)]，我调用了我的VGG网络，但我没有将其存储在任何地方。我没有说`activations
    = m_vgg(VV(img_tfm[**None**]))`。我调用它，丢弃答案，然后抓取我们在`SaveFeatures`对象中存储的特征。
- en: '`m_vgg()` — this is how you do a forward path in PyTorch. You don’t say `m_vgg.forward()`,
    you just use it as a callable. Using as a callable on an `nn.module` automatically
    calls `forward`. That’s how PyTorch modules work.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`m_vgg()` — 这是在 PyTorch 中进行前向路径的方法。你不会说 `m_vgg.forward()`，你只是将其用作可调用。在 `nn.module`
    上使用可调用会自动调用 `forward`。这就是 PyTorch 模块的工作方式。'
- en: So we call it as a callable, that ends up calling our forward hook, that forward
    hook stores the activations in `sf.features`, and so now we have our target variable
    — just like before but in a much more flexible way.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们称之为可调用的，最终调用我们的前向钩子，前向钩子将激活存储在 `sf.features` 中，所以现在我们有了我们的目标变量 — 就像以前一样，但以一种更加灵活的方式。
- en: '`get_opt` contains the same 4 lines of code we had earlier [[1:34:34](https://youtu.be/xXXiC4YRGrQ?t=1h34m34s)].
    It is just giving me my random image to optimize and an optimizer to optimize
    that image.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_opt` 包含了我们之前的相同的4行代码[[1:34:34](https://youtu.be/xXXiC4YRGrQ?t=1h34m34s)]。它只是给我一个要优化的随机图像和一个优化器来优化该图像。'
- en: '[PRE30]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now I can go ahead and do exactly the same thing. But now I’m going to use
    a different loss function `actn_loss2` (activation loss #2) which doesn’t say
    `out=m_vgg`, again, it calls `m_vgg` to do a forward pass, throws away the results,
    and and grabs `sf.features`. So that’s now my 32nd layer activations which I can
    then do my MSE loss on. You might have noticed, the last loss function and this
    one are both multiplied by a thousand. Why are they multiplied by a thousand?
    This was like all the things that were trying to get this lesson to not work correctly.
    I didn’t used to have a thousand and it wasn’t training. Lunch time today, nothing
    was working. After days of trying to get this thing to work, and finally just
    randomly noticed “gosh, the loss functions — the numbers are really low (like
    10E-7)” and I thought what if they weren’t so low. So I multiplied them by a thousand
    and it started working. So why did it not work? Because we are doing single precision
    floating point, and single precision floating point isn’t that precise. Particularly
    once you’re getting gradients that are kind of small and then you are multiplying
    by the learning rate that can be small, and you end up with a small number. If
    it’s so small, they could get rounded to zero and that’s what was happening and
    my model wasn’t ready. I’m sure there are better ways than multiplying by a thousand,
    but whatever. It works fine. It doesn’t matter what you multiply a loss function
    by because all you care about is its direction and the relative size. Interestingly,
    this is something similar we do for when we were training ImageNet. We were using
    half precision floating point because Volta tensor cores require that. And it’s
    actually a standard practice if you want to get the half precision floating to
    train, you actually have to multiply the loss function by a scaling factor. We
    were using 1024 or 512\. I think fast.ai is now the first library that has all
    of the tricks necessary to train in half precision floating point built-in, so
    if you are lucky enough to have a Volta or you can pay for a AWS P3, if you’ve
    got a learner object, you can just say `learn.half`, it’ll now just magically
    train correctly half precision floating point. It’s built into the model data
    object as well, and it’s all automatic. Pretty sure no other library does that.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我可以继续做完全相同的事情。但现在我将使用不同的损失函数 `actn_loss2`（激活损失 #2），它不会说 `out=m_vgg`，再次，它调用
    `m_vgg` 进行前向传递，丢弃结果，并获取 `sf.features`。所以现在这是我的第32层激活，然后我可以在其上执行均方误差损失。你可能已经注意到，最后一个损失函数和这个都乘以了一千。为什么它们乘以一千？这就像所有试图使这个课程不正确的事情。我以前没有使用一千，它就无法训练。今天午餐时间，什么都不起作用。经过几天的尝试让这个东西工作，最终偶然注意到“天哪，损失函数的数字真的很低（如
    10E-7）”，我想如果它们不那么低会怎样。所以我将它们乘以一千，然后它开始工作了。那为什么它不起作用呢？因为我们正在使用单精度浮点数，而单精度浮点数并不那么精确。特别是当你得到的梯度有点小，然后你乘以学习率可能也很小，最终得到一个很小的数字。如果它太小，它们可能会被四舍五入为零，这就是发生的事情，我的模型还没有准备好。我相信有比乘以一千更好的方法，但无论如何。它运行得很好。无论你将损失函数乘以多少，因为你关心的只是它的方向和相对大小。有趣的是，这与我们在训练
    ImageNet 时所做的事情类似。我们使用了半精度浮点数，因为 Volta 张量核要求如此。如果你想要训练半精度浮点数，实际上你必须将损失函数乘以一个缩放因子。我们使用了
    1024 或 512。我认为 fast.ai 现在是第一个具有所有必要技巧以在半精度浮点数中进行训练的库，因此如果你有幸拥有 Volta 或者你可以支付 AWS
    P3，如果你有一个学习对象，你只需说 `learn.half`，它现在就会神奇地正确地训练半精度浮点数。它也内置在模型数据对象中，一切都是自动的。我相信没有其他库能做到这一点。'
- en: '[PRE31]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This is just doing the same thing on a slightly earlier layer [[1:37:35](https://youtu.be/xXXiC4YRGrQ?t=1h37m35s)].
    And the bird looks more bird-like. Hopefully that makes sense to you that earlier
    layers are getting closer to the pixels. There are more grid cells, each cell
    is smaller, smaller receptive field, less complex semantic features. So the earlier
    we get, the more it’s going to look like a bird.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是在稍早的层上做同样的事情[[1:37:35](https://youtu.be/xXXiC4YRGrQ?t=1h37m35s)]。这只是让鸟看起来更像鸟。希望你能理解，较早的层越接近像素。有更多的网格单元，每个单元更小，更小的感受野，更简单的语义特征。所以我们越早得到，它看起来就越像一只鸟。
- en: '[PRE32]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In fact, the paper has a nice picture of that showing various different layers
    and zooming into this house [[1:38:17](https://youtu.be/xXXiC4YRGrQ?t=1h38m17s)].
    They are trying to make this house look like The Starry Night picture. And you
    can see that later on, it’s pretty messy, and earlier on, it looks like the house.
    So this is just doing what we just did. One of the things I’ve noticed in our
    study group is anytime I say to somebody to answer a question, anytime I say read
    the paper there is a thing in the paper that tells you the answer to that question,
    there’s always this shocked look “read the paper? me?” but seriously the papers
    have done these experiments and drawn the pictures. There’s all this stuff in
    the papers. It doesn’t mean you have to read every part of the paper. But at least
    look at the pictures. So check out Gatys’ paper, it’s got nice pictures. So they’ve
    done the experiment for us but looks like they didn’t go as deep — they just got
    some earlier ones.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这篇论文有一张很好的图片展示了各种不同的层，并放大到这座房子[[1:38:17](https://youtu.be/xXXiC4YRGrQ?t=1h38m17s)]。他们试图让这座房子看起来像《星夜》的图片。你可以看到后来，它变得非常混乱，而之前看起来像这座房子。所以这只是在做我们刚刚做的事情。我在我们的学习小组中注意到的一件事是，每当我告诉某人回答一个问题，每当我说去读这篇论文中有一些东西告诉你问题的答案时，总会有一种震惊的表情“读这篇论文？我？”但是说真的，论文已经做了这些实验并绘制了这些图片。论文中有很多东西。这并不意味着你必须读完论文的每一部分。但至少看看图片。所以看看Gatys的论文，里面有很好的图片。所以他们已经为我们做了实验，但看起来他们没有深入研究
    — 他们只是得到了一些早期的结果。
- en: Style match [[1:39:29](https://youtu.be/xXXiC4YRGrQ?t=1h39m29s)]
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风格匹配[[1:39:29](https://youtu.be/xXXiC4YRGrQ?t=1h39m29s)]
- en: The next thing we need to do is to create style loss. We’ve already got the
    loss which is how much like the bird is it. Now we need how like this painting
    style is it. And we are going to do nearly the same thing. We are going to grab
    the activations of some layer. Now the problem is, the activations of some layer,
    let’s say it was a 5x5 layer (of course there are no 5x5 layers, it’s 224x224,
    but we’ll pretend). So here’re some activations and we could get these activations
    both per the image we are optimizing and for our Van Gogh painting. Let’s look
    at our Van Gogh painting. There it is — The Starry Night
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来需要做的是创建风格损失。我们已经有了损失，即它有多像鸟。现在我们需要知道它有多像这幅绘画的风格。我们将做几乎相同的事情。我们将获取某一层的激活。现在问题是，某一层的激活，假设它是一个5x5的层（当然没有5x5的层，它是224x224，但我们假装）。这里是一些激活，我们可以获取这些激活，无论是针对我们正在优化的图像还是我们的梵高绘画。让我们看看我们的梵高绘画。这就是它
    —《星夜》
- en: '[PRE34]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: I downloaded this from Wikipedia and I was wondering what is taking son long
    to load [[1:40:39](https://youtu.be/xXXiC4YRGrQ?t=1h40m39s)] — turns out, the
    Wikipedia version I downloaded was 30,000 by 30,000 pixels. It’s pretty cool that
    they’ve got this serious gallery quality archive stuff there. I didn’t know it
    existed. Don’t try to run a neural net on that. Totally killed my Jupyter notebook.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我从维基百科下载了这幅图像，我想知道为什么加载如此缓慢[[1:40:39](https://youtu.be/xXXiC4YRGrQ?t=1h40m39s)]
    — 结果，我下载的维基百科版本是30,000 x 30,000像素。他们有这种严肃的画廊品质存档真的很酷。我不知道这个存在。不要试图在上面运行神经网络。完全毁了我的Jupyter笔记本。
- en: So we can do that for our Van Gogh image and we can do that for our optimized
    image. Then we can compare the two and we would end up creating an image that
    has content like the painting but it’s not the painting — that’s not what we want.
    We want something with the same style but it’s not the painting and doesn’t have
    the content. So we want to throw away all of the spatial information. We are not
    trying to create something that has a moon here, stars here, and a church here.
    We don’t want any of that. So how do we throw away all the special information?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以为我们的梵高图像做到这一点，也可以为我们的优化图像做到这一点。然后我们可以比较这两者，最终我们会创建一幅内容类似于绘画但并非绘画的图像 —
    这不是我们想要的。我们想要的是具有相同风格但不是绘画且没有内容的东西。所以我们想要丢弃所有的空间信息。我们不想要创造出一个这里有月亮，这里有星星，这里有教堂的东西。我们不想要任何这些。那么我们如何丢弃所有的特殊信息呢？
- en: 'In this case, there are 19 faces on this — 19 slices. So let’s grab this top
    slice that’s going to be a 5x5 matrix. Now, let’s flatten it and we’ve got a 25
    long vector. In one stroke, we’ve thrown away the bulk of the spacial information
    by flattening it. Now let’s grab a second slice (i.e. another channel) and do
    the same thing. So we have channel 1 flattened and channel 2 flattened, and they
    both have 25 elements. Now, let’s take the dot product which we can do with `@`
    in Numpy (Note: [here is Jeremy’s answer to my dot product vs. matrix multiplication
    question](http://forums.fast.ai/t/part-2-lesson-13-wiki/15297/140?u=hiromi)).
    So the dot product is going to give us one number. What’s that number? What is
    it telling us? Assuming the activations are somewhere around the middle layer
    of the VGG network, we might expect some of these activations to be how textured
    is the brush stroke, and some of them to be like how bright is this area, and
    some of them to be like is this part of a house or a part of a circular thing,
    or other parts to be, how dark is this part of the painting. So a dot product
    is basically a correlation. If this element and and this element are both highly
    positive or both highly negative, it gives us a big result. Where else, if they
    are the opposite, it gives a small results. If they are both close to zero, it
    gives no result. So basically a dot product is a measure of how similar these
    two things are. So if the activations of channel 1 and channel 2 are similar,
    then it basically says — Let’s give an example [[1:44:28](https://youtu.be/xXXiC4YRGrQ?t=1h44m28s)].
    Let’s say the first one was how textured are the brushstrokes (C1) and that one
    there says how diagonally oriented are the brush strokes (C2).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这里有19个面 - 19个切片。所以让我们拿到这个顶部切片，这将是一个5x5矩阵。现在，让我们展平它，我们得到一个25个元素的长向量。一下子，我们通过展平抛弃了大部分空间信息。现在让我们拿到第二个切片（即另一个通道）并做同样的事情。所以我们有通道1展平和通道2展平，它们都有25个元素。现在，让我们进行点积，我们可以在Numpy中用
    `@` 来做（注：[这里是Jeremy对我的点积与矩阵乘法问题的回答](http://forums.fast.ai/t/part-2-lesson-13-wiki/15297/140?u=hiromi)）。点积将给我们一个数字。那个数字是什么？它告诉我们什么？假设激活在VGG网络的中间层附近，我们可能期望其中一些激活是笔触纹理有多强，一些是这个区域有多明亮，一些是这部分是房子的一部分还是圆形的一部分，或者其他部分是这幅画的哪部分有多暗。所以点积基本上是一个相关性。如果这个元素和这个元素都非常正或都非常负，它会给我们一个大结果。另外，如果它们相反，它会给一个小结果。如果它们都接近零，它不会给结果。所以基本上点积是衡量这两个东西有多相似的一个指标。所以如果通道1和通道2的激活相似，那么它基本上是在说
    - 让我们举个例子[[1:44:28](https://youtu.be/xXXiC4YRGrQ?t=1h44m28s)]。比如第一个是笔触纹理有多强（C1），而另一个是笔触有多倾斜（C2）。
- en: If C1 and C2 are both high for a cell (1, 1) at the same time, and same is true
    for a cell (4, 2), then it’s saying grid cells that would have texture tend to
    also have diagonal. So dot product would be high when grid cells that have texture
    also have diagonal, and when they don’t, they don’t (have high dot product). So
    that’s `C1 @ C2`. Where else, `C1 @ C1` is the 2-norm effectively (i.e. the sum
    of the squares of C1). This is basically saying how many grid cells in the textured
    channel is active and how active it is. So in other words, `C1 @ C1` tells us
    how much textured painting is going on. And `C2 @ C2` tells us how much diagonal
    paint stroke is going on. Maybe C3 is “is it bright colors?” so `C3 @ C3` would
    be how often do we have bright colored cells.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果细胞（1,1）的C1和C2同时高，细胞（4,2）也是如此，那么它表明具有纹理的网格单元也倾向于具有对角线。因此，当具有纹理的网格单元也具有对角线时，点积会很高，当它们没有时，点积也不高。所以这就是
    `C1 @ C2`。另外，`C1 @ C1` 实际上是2-范数（即C1的平方和）。这基本上是在说纹理通道中有多少网格单元是活跃的，以及它们有多活跃。换句话说，`C1
    @ C1` 告诉我们纹理绘画进行了多少。而 `C2 @ C2` 告诉我们对角线绘画进行了多少。也许C3是“颜色是否明亮？”，所以 `C3 @ C3` 将告诉我们明亮颜色单元有多频繁。
- en: So what we could do then is we could create a 19 by 19 matrix containing every
    dot product [[1:47:17](https://youtu.be/xXXiC4YRGrQ?t=1h47m17s)]. And like we
    discussed, mathematicians have to give everything a name, so this particular matrix
    where you flatten something out and then do all the dot product is called Gram
    matrix.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们可以创建一个包含每个点积的19x19矩阵[[1:47:17](https://youtu.be/xXXiC4YRGrQ?t=1h47m17s)]。就像我们讨论过的，数学家们必须给每样东西起个名字，所以这个特定的矩阵，将其展平然后进行所有点积的操作，被称为Gram矩阵。
- en: I’ll tell you a secret [[1:48:29](https://youtu.be/xXXiC4YRGrQ?t=1h48m29s)].
    Most deep learning practitioners either don’t know or don’t remember all these
    things like what is a Gram matrix if they ever did study at university. They probably
    forgot it because they had a big night afterwards. And the way it works in practice
    is you realize “oh, I could create a kind of non-spacial representation of how
    the channels correlate with each other” and then when I write up the paper, I
    have to go and ask around and say “does this thing have a name?” and somebody
    will be like “isn’t that the Gram matrix?” and you go and look it up and it is.
    So don’t think you have to go study all of math first. Use your intuition and
    common sense and then you worry about what the math is called later, normally.
    Sometimes it works the other way, not with me because I can’t do math.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我告诉你一个秘密[[1:48:29](https://youtu.be/xXXiC4YRGrQ?t=1h48m29s)]。大多数深度学习从业者要么不知道，要么不记得所有这些东西，比如如果他们曾经在大学学过Gram矩阵。他们可能忘记了，因为之后他们可能熬夜了。实际上的工作方式是你意识到“哦，我可以创建一个非空间表示，展示通道之间的相关性”，然后当我写论文时，我不得不去问周围的人，“这个东西有个名字吗？”
    然后有人会说“这不就是Gram矩阵吗？” 你去查一下，确实是。所以不要认为你必须先学习所有的数学。先运用你的直觉和常识，然后再担心数学叫什么，通常是这样。有时候也会反过来，不过不是对我，因为我不擅长数学。
- en: So this is called the Gram matrix [[1:49:22](https://youtu.be/xXXiC4YRGrQ?t=1h49m22s)].
    And of course, if you are a real mathematician, it’s very important that you say
    this as if you always knew it was a Gram matrix and you kind of just go oh yes,
    we just calculate the Gram matrix. So the Gram matrix then is this kind of map
    — the diagonal is perhaps the most interesting. The diagonal is which channels
    are the most active and then the off diagonal is which channels tend to appear
    together. And overall, if two pictures have the same style, then we are expecting
    that some layer of activations, they will have similar Gram matrices. Because
    if we found the level of activations that capture a lot of stuff about like paint
    strokes and colors, then the diagonal alone (in Gram matrices) might even be enough.
    That’s another interesting homework assignment, if somebody wants to take it,
    is try doing Gatys’ style transfer not using the Gram matrix but just using the
    diagonal of the Gram matrix. That would be like a single line of code to change.
    But I haven’t seen it tried and I don’t know if it would work at all, but it might
    work fine.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这被称为Gram矩阵。当然，如果你是一个真正的数学家，非常重要的是你要说得好像你一直知道这是一个Gram矩阵，然后你就会说，哦是的，我们只是计算Gram矩阵。所以Gram矩阵就是这种映射——对角线可能是最有趣的部分。对角线显示哪些通道最活跃，然后非对角线显示哪些通道倾向于一起出现。总的来说，如果两幅图片有相同的风格，那么我们期望某些激活层会有相似的Gram矩阵。因为如果我们找到了捕捉很多关于画笔笔触和颜色的东西的激活层，那么仅仅对角线（在Gram矩阵中）可能就足够了。这是另一个有趣的作业，如果有人想尝试的话，可以尝试使用Gatys的风格迁移，而不是使用Gram矩阵，而是只使用Gram矩阵的对角线。这只需要改变一行代码。但我还没有看到有人尝试过，也不知道它是否会起作用，但它可能会很好。
- en: “Okay, yes Christine, you’ve tried it” [[1:50:51](https://youtu.be/xXXiC4YRGrQ?t=1h50m51s)].
    “I have tried that and it works most of the time except when you have funny pictures
    where you need two styles to appear in the same spot. So it seems like grass in
    one half and a crowd in one half, and you need the two styles.” (Christine). Cool,
    you’re still gonna do your homework, but Christine says she’ll do it for you.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: “好的，是的，克里斯汀，你已经尝试过了。”“我已经尝试过了，大多数时候都有效，除非你有需要两种风格出现在同一个地方的有趣图片。所以看起来像是一半是草，一半是人群，你需要这两种风格。”（克里斯汀）。很酷，你仍然会做你的作业，但克里斯汀说她会替你做。
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: So here is our painting [[1:51:22](https://youtu.be/xXXiC4YRGrQ?t=1h51m22s)].
    I’ve tried to resize the painting so it’s the same size as my bird picture. So
    that’s all this is just doing. It doesn’t matter too much which bit I use as long
    as it’s got lots of the nice style in it.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的绘画。我尝试调整绘画的大小，使其与我的鸟类图片大小相同。所以这就是所有这些在做的事情。不管我使用哪一部分，只要它有很多漂亮的风格就可以了。
- en: 'I grab my optimizer and my random image just like before:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我像以前一样获取了我的优化器和随机图像：
- en: '[PRE36]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: And this time, I call `SaveFeatures` for all of my `block_ends` and that’s going
    to give me an array of SaveFeatures objects — one for each module that appears
    the layer before the max pool. Because this time, I want to play around with different
    activation layer styles, or more specifically I want to let you play around with
    it. So now I’ve got a whole array of them.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我为所有的`block_ends`调用`SaveFeatures`，这将给我一个SaveFeatures对象的数组——每个模块都会出现在最大池化之前的层中。因为这一次，我想玩弄不同的激活层风格，更具体地说，我想让你来玩。所以现在我有了一个完整的数组。
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`style_img` is my Van Gogh painting. So I take my `style_img`, put it through
    my transformations to create my transform style image (`style_tfm`).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`style_img`是我的梵高的绘画。所以我拿我的`style_img`，通过我的转换来创建我的转换风格图像（`style_tfm`）。'
- en: '[PRE38]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Turn that into a variable, put it through the forward pass of my VGG module,
    and now I can go through all of my SaveFeatures objects and grab each set of features.
    Notice I call `clone` because later on, if I call my VGG object again, it’s going
    to replace those contents. I haven’t quite thought about whether this is necessary.
    If you take it away and it’s not, that’s fine. But I was just being careful. So
    here is now an array of the activations at every `block_end` layer. And here,
    you can see all of those shapes:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 将其转换为一个变量，通过我的VGG模块的前向传播，现在我可以遍历所有的SaveFeatures对象并获取每组特征。请注意，我调用`clone`，因为以后，如果我再次调用我的VGG对象，它将替换这些内容。我还没有想过这是否有必要。如果你把它拿走了，那没关系。但我只是小心翼翼。现在这是每个`block_end`层的激活的数组。在这里，你可以看到所有这些形状：
- en: '[PRE39]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: And you can see, being able to whip up a list comprehension really quickly,
    it’s really important in your Jupyter fiddling around [[1:53:30](https://youtu.be/xXXiC4YRGrQ?t=1h53m30s)].
    Because you really want to be able to immediately see here’s my channel (64, 128,
    256, …), and grid size halving as we would expect (288, 144, 72…) because all
    of these appear just before a max pool.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，能够快速地编写一个列表推导式在你的Jupyter玩耍中非常重要。因为你真的希望能够立即看到这是我的通道（64、128、256，...），以及我们期望的网格大小减半（288、144、72...），因为所有这些都出现在最大池化之前。
- en: So to do a Gram MSE loss, it’s going to be the MSE loss on the Gram matrix of
    the input vs. the gram matrix of the target. And the Gram matrix is just the matrix
    multiply of `x` with `x` transpose (`x.t()`) where x is simply equal to my input
    where I’ve flattened the batch and channel axes all down together. I’ve only got
    one image, so you can ignore the batch part — it’s basically channel. Then everything
    else (`-1`), which in this case is the height and width, is the other dimension
    because there’s now going to be channel by height and width, and then as we discussed
    we can them just do the matrix multiply of that by its transpose. And just to
    normalize it, we’ll divide that by the number of elements (`b*c*h*w`) — it would
    actually be more elegant if I had said `input.numel` (number of elements) that
    would be the same thing. Again, this gave me tiny numbers so I multiply it by
    a big number to make it something more sensible. So that’s basically my loss.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要进行Gram MSE损失，它将是输入的Gram矩阵与目标的Gram矩阵的MSE损失。Gram矩阵只是`x`与`x`转置(`x.t()`)的矩阵乘积，其中x简单地等于我已经将批处理和通道轴全部展平的输入。我只有一个图像，所以可以忽略批处理部分——基本上是通道。然后其他所有部分(`-1`)，在这种情况下是高度和宽度，是另一个维度，因为现在将是通道乘以高度和宽度，然后正如我们讨论过的，我们可以将其与其转置进行矩阵乘积。为了归一化，我们将其除以元素的数量(`b*c*h*w`)——如果我说`input.numel`（元素的数量）会更优雅，这将是相同的事情。再次，这给我了很小的数字，所以我乘以一个大数字使其变得更合理。所以这基本上就是我的损失。
- en: '[PRE40]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: So now my style loss is to take my image to optimize, throw it through VGG forward
    pass, grab an array of the features in all of the SaveFeatures objects, and then
    call my Gram MSE loss on every one of those layers [[1:55:13](https://youtu.be/xXXiC4YRGrQ?t=1h55m13s)].
    And that’s going to give me an array and then I just add them up. Now you could
    add them up with different weightings, you could add up subsets, or whatever.
    In this case, I’m just grabbing all of them.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我的风格损失是将我的图像优化，通过VGG前向传递，获取所有SaveFeatures对象中特征的数组，然后在每一层上调用我的Gram MSE损失。这将给我一个数组，然后我只需将它们相加。现在你可以用不同的权重将它们相加，你可以添加子集，或者其他。在这种情况下，我只是获取了所有的。
- en: '[PRE41]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Pass that into my optimizer as before:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 像以前一样将其传递给我的优化器：
- en: '[PRE42]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: And here we have a random image in the style of Van Gogh which I think is kind
    of cool.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一张随机图像，风格类似于梵高，我觉得挺酷的。
- en: '[PRE43]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Again Gatys has done it for us. Here is different layers of random image in
    the style of Van Gogh. So the first one, as you can see, the activations are simple
    geometric things — not very interesting at all. The later layers are much more
    interesting. So we kind of have a suspicion that we probably want to use later
    layers largely for our style loss if we wanted to look good.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，Gatys已经为我们做好了。这里是不同层次的随机图像，风格类似于梵高。所以第一个，你可以看到，激活是简单的几何图形——一点也不有趣。后面的层次更有趣。所以我们有一种怀疑，我们可能想要主要使用后面的层次来进行风格损失，如果我们想要看起来好的话。
- en: 'I added this `SaveFeatures.close` [[1:56:35](https://youtu.be/xXXiC4YRGrQ?t=1h56m35s)]
    which just calls `self.hook.remove()`. Remember, I stored the hook as `self.hook`
    so `hook.remove()` gets rid of it. It’s a good idea to get rid of it because otherwise
    you can potentially just keep using memory. So at the end, I just go through each
    of my SaveFeatures object and close it:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加了这个`SaveFeatures.close`，它只是调用`self.hook.remove()`。记住，我将hook存储为`self.hook`，所以`hook.remove()`会将其删除。最好将其删除，否则可能会一直使用内存。因此，在最后，我只需遍历每个SaveFeatures对象并关闭它：
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Style transfer [[1:57:08](https://youtu.be/xXXiC4YRGrQ?t=1h57m8s)]
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风格转移
- en: Style transfer is adding content loss and style loss together with some weight.
    So there is no much to show.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 风格转移是将内容损失和风格损失加在一起，并加上一些权重。所以没有太多可以展示的。
- en: 'Grab my optimizer, grab my image:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 获取我的优化器，获取我的图像：
- en: '[PRE45]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: And my combined loss is the MSE loss at one particular layer, my style loss
    at all of my layers, sum up the style losses, add them to the content loss, the
    content loss I’m scaling. Actually the style loss, I scaled already by 1E6\. So
    they are both scaled exactly the same. Add them together. Again, you could trying
    weighting the different style losses or you could maybe remove some of them, so
    this is the simplest possible version.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我的综合损失是一个特定层次的MSE损失，我所有层次的风格损失，将风格损失相加，加到内容损失上，我正在缩放内容损失。实际上，我已经将风格损失缩放为1E6。所以它们都被精确地缩放了。将它们加在一起。再次，你可以尝试对不同的风格损失进行加权，或者你可以删除其中一些，所以这是最简单的版本。
- en: '[PRE46]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Train that:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 训练它：
- en: '[PRE47]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: And holy crap, it actually looks good. So I think that’s pretty awesome. The
    main take away here is if you want to solve something with a neural network, all
    you’ve got to do is set up a loss function and then optimize something. And the
    loss function is something which a lower number is something that you’re happier
    with. Because then when you optimize it, it’s going to make that number as low
    as you can, and it’ll do what you wanted it to do. So here, Gatys came up with
    the loss function that does a good job of being a smaller number when it looks
    like the thing we want it to look like, and it looks like the style of the thing
    we want to be in the style of. That’s all we had to do.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 天啊，它看起来真的很好。所以我觉得这很棒。这里的主要要点是，如果你想用神经网络解决问题，你所要做的就是设置一个损失函数，然后优化某些东西。而损失函数是一个较低的数字是你更满意的东西。因为当你优化它时，它会使那个数字尽可能低，它会做你想要它做的事情。所以在这里，Gatys提出了一个损失函数，当它看起来像我们想要的东西时，它会是一个较小的数字，看起来像我们想要的风格。这就是我们所要做的。
- en: 'What it actually comes to it [[1:59:10](https://youtu.be/xXXiC4YRGrQ?t=1h59m10s)],
    apart from implementing Gram MSE loss which was like 6 lines of code if that,
    that’s our loss function:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，除了实现了Gram MSE损失，这只是6行代码，这就是我们的损失函数：
- en: Pass it to our optimizer, and wait about 5 seconds, and we are done. And remember,
    we could do a batch of these at a time, so we could wait 5 seconds and 64 of these
    will be done. So I think that’s really interesting and since this paper came out,
    it has really inspired a lot of interesting work. To me though, most of the interesting
    work hasn’t happened yet because to me, the interesting work is the work where
    you combine human creativity with these kinds of tools. I haven’t seen much in
    the way of tools that you can download or use where the artist is in control and
    can kind of do things interactively. It’s interesting talking to the guys at [Google
    Magenta](https://magenta.tensorflow.org/) project which is their creative AI project,
    all of the stuff they are doing with music is specifically about this. It’s building
    tools that musicians can use to perform in real time. And you’ll see much more
    of that on the music space thanks to Magenta. If you go to their website, there’s
    all kinds of things where you can press the buttons to actually change the drum
    beats, melodies, keys, etc. You can definitely see Adobe or Nvidia is starting
    to release little prototypes and starting to do this but this kind of creative
    AI explosion hasn’t happened yet. I think we have pretty much all the technology
    we need but no one’s put it together into a thing and said “look at the thing
    I built and look at the stuff that people built with my thing.” So that’s just
    a huge area of opportunity.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 将其传递给我们的优化器，等大约5秒钟，我们就完成了。记住，我们可以一次处理一批，所以我们可以等待5秒钟，64个就完成了。所以我认为这真的很有趣，自从这篇论文发表以来，它确实激发了很多有趣的工作。不过对我来说，大部分有趣的工作还没有发生，因为对我来说，有趣的工作是将人类创造力与这些工具结合起来的工作。我还没有看到可以下载或使用的工具，艺术家可以控制并可以以交互方式进行操作。与[Google
    Magenta](https://magenta.tensorflow.org/)项目的人交谈很有趣，这是他们的创意人工智能项目，他们在音乐方面所做的一切都是关于这个的。它正在构建音乐家可以实时使用的工具。由于Magenta的存在，您将在音乐领域看到更多这样的东西。如果您访问他们的网站，您会看到各种按键，可以实际更改鼓点、旋律、音调等。您肯定会看到Adobe或Nvidia开始发布一些小型原型并开始这样做，但这种创意人工智能的爆发尚未发生。我认为我们已经拥有了我们所需的所有技术，但没有人将其整合到一起并说“看看我建造的东西，看看人们用我的东西建造的东西”。所以这只是一个巨大的机会领域。
- en: So the paper that I mentioned at the start of class in passing [[2:01:16](https://youtu.be/xXXiC4YRGrQ?t=2h1m16s)]
    — the one where we can add Captain America’s shield to arbitrary paintings basically
    used this technique. The trick was though some minor tweaks to make the pasted
    Captain America shield blend in nicely. But that paper is only a couple of days
    old, so that would be a really interesting project to try because you can use
    all this code. It really does leverage this approach. Then you could start by
    making the content image be like the painting with the shield and then the style
    image could be the painting without the shield. That would be a good start, and
    then you could see what specific problems they try to solve in this paper to make
    it better. But you could have a start on it right now.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我在课堂开始时提到的那篇论文[[2:01:16](https://youtu.be/xXXiC4YRGrQ?t=2h1m16s)] ——基本上是将美国队长的盾牌添加到任意绘画中使用了这种技术。不过，诀窍是通过一些微小的调整使粘贴的美国队长盾牌能够很好地融入其中。但那篇论文只有几天的历史，所以尝试这个项目将是一个非常有趣的项目，因为您可以使用所有这些代码。它确实利用了这种方法。然后，您可以从使内容图像类似于带有盾牌的绘画开始，然后样式图像可以是不带盾牌的绘画。这将是一个很好的开始，然后您可以看看他们在这篇论文中尝试解决的具体问题，以使其更好。但您现在可以开始。
- en: '**Question**: Two questions — earlier there were a number of people that expressed
    interest in your thoughts on Pyro and probabilistic programming [[2:02:34](https://youtu.be/xXXiC4YRGrQ?t=2h2m34s)].
    So TensorFlow has now got this TensorFlow probability or something. There’s a
    bunch of probabilistic programming framework out there. I think they are intriguing,
    but as yet unproven in the sense that I haven’t seen anything done with any probabilistic
    programming system which hasn’t been done better without them. The basic premise
    is that it allows you to create more of a model of how you think the world works
    and then plug in the parameters. So back when I used to work in management consulting
    20 years ago, we used to do a lot of stuff where we would use a spreadsheet and
    then we would have these Monte Carlo simulation plugins — there was one called
    At Risk(?) and one called Crystal Ball. I don’t know if they still exist decades
    later. Basically they would let you change a spreadsheet cell to say this is not
    a specific value but it actually represents a distribution of values with this
    mean and the standard deviation or it’s got this distribution, and then you would
    hit a button and the spreadsheet would recalculate a thousand times pulling random
    numbers from these distributions and show you the distribution of your outcome
    that might be profit or market share or whatever. We used them all the time back
    then. Apparently feel that a spreadsheet is a more obvious place to do that kind
    of work because you can see it all much more naturally, but I don’t know. We’ll
    see. At this stage, I hope it turns out to be useful because I find it very appealing
    and it appeals to as I say the kind of work I used to do a lot of. There’s actually
    whole practices around this stuff they used to call system dynamics which really
    was built on top of this kind of stuff, but it’s not quite gone anywhere.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：之前有很多人表达了对Pyro和概率编程的兴趣。所以TensorFlow现在有了这个TensorFlow概率或其他东西。有很多概率编程框架。我认为它们很有趣，但至今未经证明，因为我还没有看到任何使用概率编程系统完成的事情，而没有使用它们更好。基本前提是它允许你创建更多关于你认为世界是如何运作的模型，然后插入参数。所以20年前当我还在管理咨询行业工作时，我们经常使用电子表格，然后我们会使用这些蒙特卡洛模拟插件——有一个叫做At
    Risk(?)，一个叫做Crystal Ball。我不知道几十年后它们是否还存在。基本上它们让你可以更改电子表格中的一个单元格，说这不是一个具体的值，而实际上代表一个具有这个均值和标准差的值分布，或者它有这个分布，然后你会点击一个按钮，电子表格会从这些分布中随机抽取一千次数字重新计算，并显示你的结果的分布，可能是利润或市场份额或其他什么。那时我们经常使用它们。显然，人们认为电子表格是做这种工作的更明显的地方，因为你可以更自然地看到所有这些，但我不知道。我们将看到。在这个阶段，我希望它能够被证明有用，因为我觉得它非常吸引人，符合我过去经常做的工作。实际上，围绕这种东西有整个实践，他们过去称之为系统动力学，这实际上是建立在这种东西之上的，但它并没有走得太远。
- en: '**Question**: Then there was a question about pre-training for generic style
    transfer [[2:04:57](https://youtu.be/xXXiC4YRGrQ?t=2h4m57s)]. I don’t think you
    can pre-train for a generic style, but you can pre-train for a generic photo for
    a particular style which is where we are going to get to. Although, it may end
    up being a homework. I haven’t decided yet. But I’m going to do all the pieces.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：然后有一个关于通用风格转移预训练的问题。我不认为你可以为通用风格进行预训练，但你可以为特定风格的通用照片进行预训练，这就是我们要达到的目标。尽管可能最终会成为一项作业。我还没有决定。但我会做所有的部分。
- en: '**Question**: Please ask him to talk about multi-GPU [[2:05:31](https://youtu.be/xXXiC4YRGrQ?t=2h5m31s)].
    Oh yeah, I haven’t had a slide about that. We’re about to hit it.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：请让他谈谈多GPU。哦，是的，我还没有关于那个的幻灯片。我们马上就要谈到了。
- en: Before we do, just another interesting picture from the Gatys’ paper. They’ve
    got a few more just didn’t fit in my slide but different convolutional layers
    for the style. Different style to content ratios, and here’s the different images.
    Obviously this isn’t Van Gogh any more, this is a different combination. So you
    can see, if you just do all style, you don’t see any image. If you do lots of
    content, but you use low enough convolutional layer, it looks okay but the back
    ground is kind of dumb. So you kind of want somewhere in the middle. So you can
    play around with it and experiment, but also use the paper to help guide you.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，再分享一张来自Gatys论文的有趣图片。他们有更多图片，只是没有适合我的幻灯片，但是有不同的卷积层用于风格。不同的风格和内容比例，这里是不同的图片。显然这不再是梵高的风格，这是一个不同的组合。所以你可以看到，如果你只做风格，你看不到任何图片。如果你做很多内容，但是使用足够低的卷积层，看起来还可以，但背景有点愚蠢。所以你可能想要在中间某个地方。所以你可以尝试一下，做一些实验，但也可以使用论文来帮助指导你。
- en: The Math [[2:06:33](https://youtu.be/xXXiC4YRGrQ?t=2h6m33s)]
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学
- en: Actually, I think I might work on the math now and we’ll talk about multi GPU
    and super resolution next week because this is from the paper and one of the things
    I really do want you to do after we talk about a paper is to read the paper and
    then ask questions on the forum anything that’s not clear. But there’s a key part
    of this paper which I wanted to talk about and discuss how to interpret it. So
    the paper says, we’re going to be given an input image *x* and this little thing
    means normally it means it’s a vector, Rachel, but this one is a matrix. I guess
    it could mean either. I don’t know. Normally small letter bold means vector or
    a small letter with an arrow on top means vector. And normally big letter means
    matrix or small letter with two arrows on top means matrix. In this case, our
    image is a matrix. We are going to basically treat it as a vector, so maybe we’re
    just getting ahead of ourselves.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我想现在开始研究数学，下周我们将讨论多GPU和超分辨率，因为这是来自论文的内容，我真的希望在我们讨论完论文后，你们能阅读论文，并在论坛上提出任何不清楚的问题。但这篇论文中有一部分我想谈谈，讨论如何解释它。所以论文说，我们将得到一个输入图像*x*，这个小东西通常表示它是一个向量，Rachel，但这个是一个矩阵。我猜它可能是两者之一。我不知道。通常小写粗体字母表示向量，或者带有上箭头的小写字母表示向量。通常大写字母表示矩阵，或者带有两个箭头的小写字母表示矩阵。在这种情况下，我们的图像是一个矩阵。我们基本上将其视为向量，所以也许我们只是在超前一步。
- en: 'So we’ve got an input image *x* and it can be encoded in a particular layer
    of the CNN by the filter responses (i.e. activations). Filter responses are activations.
    Hopefully, that’s something you all understand. That’s basically what a CNN does
    is it produces layers of activations. A layer has a bunch of filters which produce
    a number of channels. This year says that layer number L has capital N*l* filters.
    Again, this capital does not mean matrix. So I don’t know, math notation is so
    inconsistent. So capital Nl distinct filters at layer L which means it has also
    that many feature maps. So make sure you can see this letter Nl is the same as
    this letter. So you’ve got to be very careful to read the letters and recognize
    it’s like snap, that’s the same letter as that. So obviously, Nl filters create
    create Nl feature maps or channels, each one of size M*l* (okay, I can see this
    is where the unrolling is happening). So this is like M[*l*] in numpy notation.
    It’s the *l*th layer. So M for the *l*th layer. The size is height times width
    — so we flattened it out. So the responses in a layer l can be stored in a matrix
    F (and now the *l* goes at the top for some reason). So this is not f^*l*, it’s
    just another indexing. We are just moving it around for fun. This thing here where
    we say it’s an element of R — this is a special R meaning the real numbers N times
    M (this is saying that the dimensions of this is N by M). So this is really important,
    you don’t move on. It’s just like with PyTorch, making sure that you understand
    the rank and size of your dimensions first, same with math. These are the bits
    where you stop and think why is it N by M? N is a number of filters, M is height
    by width. So do you remember that thing when we did `.view(b*c, -1)`? Here that
    is. So try to map the code to the math. So F is `x`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有一个输入图像*x*，它可以通过CNN的特定层中的滤波器响应（即激活）进行编码。滤波器响应就是激活。希望你们都能理解。CNN的基本功能就是生成激活层。一个层有一堆滤波器，产生一定数量的通道。今年表示第L层有大写N*l*个滤波器。再次强调，这里的大写字母不代表矩阵。所以我不知道，数学符号是如此不一致。所以在第L层有Nl个不同的滤波器，这意味着也有同样数量的特征图。所以确保你能看到这个字母Nl和这个字母是一样的。所以你必须非常小心地阅读字母并认识到它就像啪一下，这个字母和那个字母是一样的。显然，Nl个滤波器创建了Nl个特征图或通道，每个尺寸为M*l*（好吧，我看到这里正在发生展开）。这就像numpy符号中的M[*l*]。这是第*l*层。所以M是第*l*层。尺寸是高度乘以宽度——所以我们将其展平。所以第l层的响应可以存储在矩阵F中（现在*l*在顶部，出于某种原因）。这不是f^*l*，这只是另一个索引。我们只是为了好玩而移动它。这里我们说它是R的元素——这是一个特殊的R，表示实数N乘以M（这表示它的维度是N乘以M）。这非常重要，不要继续。就像PyTorch一样，确保你首先理解维度的秩和大小，数学也是一样。这些是你停下来思考为什么是N乘以M的地方。N是滤波器的数量，M是高度乘以宽度。所以你还记得我们做`.view(b*c,
    -1)`的时候吗？这就是。所以尝试将代码映射到数学上。所以F是`x`：
- en: If I was nicer to you, I would have used the same letters as the paper. But
    I was too busy getting this darn thing working to do that carefully. So you can
    go back and rename it as capital F.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我对你更友好，我会使用与论文相同的字母。但我太忙于让这个该死的东西运行起来，无法仔细做到这一点。所以你可以回去将其重命名为大写F。
- en: So this is why we moved the L to the top is because we’re now going to have
    some more indexing. Where else in Numpy or PyTorch, we index things by square
    brackets and then lots of things with commas between. The approach in math is
    to surround your letter by little letters all around it — just throw them up there
    everywhere. So here, F*l* is the *l*th layer of F and then *ij* is the activation
    of the *i*th filter at position *j* of layer *l*. So position *j* is up to size
    M which is up to size height by width. This is the kind of thing that would be
    easy to get confused. Often you’d see an *ij* and assume that’s indexing into
    a position of an image like height by width, but it’s totally not, is it? It’s
    indexing into channel by flattened image. It even tells you — it’s the *i*th filter/channel
    in the *j*th position in the flattened out image in layer *l*. So you’re not gonna
    be able to get any further in the paper unless you understand what F is. That’s
    why these are the bits where you stop and make sure you’re comfortable.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将L移到顶部是因为我们现在要有更多的索引。在Numpy或PyTorch中，我们通过方括号索引事物，然后用逗号分隔很多东西。在数学中的方法是用小写字母围绕你的字母——到处都扔上去。所以这里，F*l*是F的第*l*层，然后*ij*是第*l*层中第*i*个滤波器在第*j*位置的激活。所以位置*j*的大小是M，即高度乘以宽度的大小。这是容易混淆的事情。通常你会看到一个*ij*，然后假设它是在图像的高度乘以宽度的位置进行索引，但实际上不是，对吧？它是在通道中对展平图像的第*i*个滤波器/通道的第*j*个位置进行索引。它甚至告诉你——它是第*l*层中展平图像中第*j*个位置的第*i*个滤波器/通道。所以除非你理解F是什么，否则你将无法进一步阅读论文。这就是为什么这些是你停下来确保你感到舒适的地方。
- en: 'So now, the content loss, I’m not going to spend much time on but basically
    we are going to just check out the values of the activations vs. the predictions
    squared [[2:12:03](https://youtu.be/xXXiC4YRGrQ?t=2h12m3s)]. So there’s our content
    loss. The style loss will be much the same thing, but using the Gram matrix G:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在，内容损失，我不会花太多时间，但基本上我们只是要检查激活值与预测值的平方[[2:12:03](https://youtu.be/xXXiC4YRGrQ?t=2h12m3s)]。所以这就是我们的内容损失。风格损失将是类似的，但使用格拉姆矩阵G：
- en: I really wanted to show you this one. I think it’s super. Sometimes I really
    like things you can do in math notation, and they’re things that you can also
    generally do in J and APL which is this kind of this implicit loop going on here.
    What this is saying is there’s a whole bunch of values of *i* and a whole bunch
    of values of *j*, and I’m going to define G for all of them. And there’s whole
    bunch of values of *l* as well, and I’m going to define G for all of those as
    well. So for all of my G at every *l* of every *i* at every *j*, it’s going to
    be equal to something. And you can see that something has an *i* and a *j* and
    a *l*, matching G, and it also has a *k* and that’s part of the sum. So what’s
    going on here? Well, it’s saying that my Gram matrix in layer *l* for the *i*th
    position in one axis and the *j*th position in another axis is equal to my F matrix
    (so my flattened out matrix) for the *i*th channel in that layer vs. the *j*th
    channel in the same layer, then I’m going to sum over. We are going to take the
    *k*th position and multiply them together and then add them all up. So that’s
    exactly what we just did before when we calculated our Gram matrix. So this, there’s
    a lot going on because of some, to me, very neat notation — which is there are
    three implicit loops all going on at the same time, plus one explicit loop in
    the sum, then they all work together to create this Gram matrix for every layer.
    So let’s go back and see if you can match this. Sl all that’s happening all at
    once which is pretty great.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我真的很想向你展示这个。我觉得这很棒。有时我真的喜欢数学符号中可以做的事情，它们也是你通常可以在J和APL中做的事情，这种隐式循环正在这里进行。这是在说什么呢？嗯，它在说我的层*l*中的格拉姆矩阵，对于一个轴上的第*i*个位置和另一个轴上的第*j*个位置等于我的F矩阵（所以我的展平矩阵）对于该层中的第*i*个通道与同一层中的第*j*个通道，然后我将进行求和。我们将取第*k*个位置并将它们相乘然后将它们全部加起来。所以这正是我们之前计算格拉姆矩阵时所做的事情。所以这里发生了很多事情，因为对我来说，这是非常巧妙的符号
    —— 有三个隐式循环同时进行，加上求和中的一个显式循环，然后它们一起工作来为每一层创建这个格拉姆矩阵。所以让我们回去看看你是否能匹配这个。所以所有这一切都同时发生，这非常棒。
- en: That’s it. So next week, we’re going to be looking at a very similar approach,
    basically doing style transfer all over again but in a way where we actually going
    to train a neural network to do it for us rather than having to do the optimization.
    We’ll also see that you can do the same thing to do super resolution. And we are
    also going to go back and revisit some of the SSD stuff as well as doing some
    segmentation. So if you’ve forgotten SSD, might be worth doing a little bit of
    revision this week. Alright, thanks everybody. See you next week.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。所以下周，我们将看到一个非常类似的方法，基本上再次进行风格转移，但这次我们实际上会训练一个神经网络来为我们做这件事，而不是进行优化。我们还将看到你可以做同样的事情来进行超分辨率。我们还将回顾一些SSD的内容，以及进行一些分割。所以如果你忘记了SSD，这周可能值得进行一点复习。好的，谢谢大家。下周见。
