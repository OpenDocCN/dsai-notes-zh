- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:07:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1812.03288] No Peek: A Survey of private distributed deep learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1812.03288] No Peek: 私有分布式深度学习调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1812.03288](https://ar5iv.labs.arxiv.org/html/1812.03288)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1812.03288](https://ar5iv.labs.arxiv.org/html/1812.03288)
- en: '¹¹institutetext: Massachusetts Institute of Technology'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构文本：麻省理工学院
- en: Cambridge, MA 02139, U.S.A
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 美国麻省剑桥市 02139
- en: 'No Peek: A Survey of private distributed deep learning'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'No Peek: 私有分布式深度学习调查'
- en: 'Praneeth Vepakomma Corresponding author e-mail: vepakom@mit.edu    Tristan
    Swedish    Ramesh Raskar    Otkrist Gupta    Abhimanyu Dubey'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Praneeth Vepakomma 通讯作者 电子邮件：vepakom@mit.edu    Tristan Swedish    Ramesh Raskar
       Otkrist Gupta    Abhimanyu Dubey
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We survey distributed deep learning models for training or inference without
    accessing raw data from clients. These methods aim to protect confidential patterns
    in data while still allowing servers to train models. The distributed deep learning
    methods of federated learning, split learning and large batch stochastic gradient
    descent are compared in addition to private and secure approaches of differential
    privacy, homomorphic encryption, oblivious transfer and garbled circuits in the
    context of neural networks. We study their benefits, limitations and trade-offs
    with regards to computational resources, data leakage and communication efficiency
    and also share our anticipated future trends.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了在不访问客户端原始数据的情况下进行训练或推理的分布式深度学习模型。这些方法旨在保护数据中的机密模式，同时仍允许服务器进行模型训练。我们比较了联邦学习、分割学习和大批量随机梯度下降的分布式深度学习方法，并在神经网络的背景下，与差分隐私、同态加密、隐匿传输和混淆电路等私有和安全方法进行了比较。我们研究了它们在计算资源、数据泄漏和通信效率方面的优缺点及权衡，并分享了我们预计的未来趋势。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Emerging technologies in domains such as biomedicine, health and finance benefit
    from distributed deep learning methods which can allow multiple entities to train
    a deep neural network without requiring data sharing or resource aggregation at
    one single place. In particular, we are interested in distributed deep learning
    approaches that bridge the gap between distributed data sources (clients) and
    a powerful centralized computing resource (server) under the constraint that local
    data sources of clients are not allowed to be shared with the server or amongst
    other clients.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物医学、健康和金融等领域的新兴技术中，分布式深度学习方法可以让多个实体训练深度神经网络，而无需在一个地方共享数据或整合资源。特别地，我们对在本地数据源（客户端）不能与服务器或其他客户端共享的情况下，将分布式数据源（客户端）与强大的集中计算资源（服务器）连接起来的分布式深度学习方法感兴趣。
- en: We survey and compare such distributed deep learning techniques and classify
    them across various dimensions of level and type of protection offered, model
    performance and resources required such as memory, time, communications bandwidth
    and synchronization requirements. We introduce the terminology of ‘no peek’ to
    refer to distributed deep learning techniques that do not share their data in
    raw form. We note that such no peek techniques allow the server to train models
    without ’peeking at’, or directly observing, raw data belonging to clients.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查并比较了这些分布式深度学习技术，并根据提供的保护级别和类型、模型性能以及所需资源（如内存、时间、通信带宽和同步要求）对其进行了分类。我们引入了“no
    peek”术语，以指代那些不以原始形式共享数据的分布式深度学习技术。我们指出，这些“no peek”技术允许服务器在不“窥视”或直接观察客户端原始数据的情况下进行模型训练。
- en: Additionally, we survey some generic approaches to protecting data and models.
    Some of these approaches have already been used in combination with distributed
    deep learning methods that possess varying levels of the no peek property. These
    generic approaches include de-identification methods like anonymization [[52](#bib.bib52)],
    obfuscation methods like differential privacy [[100](#bib.bib100), [101](#bib.bib101),
    [102](#bib.bib102)] and cryptographic techniques like homomorphic encryption [[19](#bib.bib19),
    [93](#bib.bib93), [96](#bib.bib96)] and secure multi-party computation (MPC) protocols
    like oblivious transfer [[84](#bib.bib84), [47](#bib.bib47)] and garbled circuits
    [[41](#bib.bib41)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们调查了一些通用的数据和模型保护方法。其中一些方法已经与具有不同程度禁止窥探属性的分布式深度学习方法结合使用。这些通用方法包括去标识化方法，如匿名化
    [[52](#bib.bib52)]，模糊化方法，如差分隐私 [[100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102)]
    和加密技术，如同态加密 [[19](#bib.bib19), [93](#bib.bib93), [96](#bib.bib96)] 以及安全多方计算（MPC）协议，如盲转移
    [[84](#bib.bib84), [47](#bib.bib47)] 和加密电路 [[41](#bib.bib41)]。
- en: In the rest of the paper, we focus on distributed deep learning techniques such
    as splitNN [[31](#bib.bib31), [2](#bib.bib2)], large batch synchronous stochastic
    gradient descent (SGD) [[20](#bib.bib20), [9](#bib.bib9)], federated learning[[3](#bib.bib3)]
    and other variants [[107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110)] in the context of protecting data and models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的其余部分，我们将重点探讨分布式深度学习技术，如 splitNN [[31](#bib.bib31), [2](#bib.bib2)]，大批量同步随机梯度下降（SGD）[[20](#bib.bib20),
    [9](#bib.bib9)]，联邦学习[[3](#bib.bib3)] 和其他变体 [[107](#bib.bib107), [108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110)]，在保护数据和模型的背景下。
- en: 1.1 No peek rule
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 禁止窥探规则
- en: We refer to techniques of distributed deep learning that do not look at raw
    data once it leaves the client as satisfying the property of ’no peek’. No peek
    is necessitated by trust and regulatory issues. For example, hospitals are typically
    not allowed to share data with for-profit entities due to trust issues. They also
    are unable to share it with external entities (data cannot physically leave the
    premises) due to limited consent of the patients, and regulations such as HIPAA
    [[5](#bib.bib5), [1](#bib.bib1), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]
    that prevent sharing many aspects of the data to external entities. Some techniques
    go a step ahead by also not revealing details of the model architecture as well.
    In these techniques, neither the server nor client can access the details of the
    other’s architecture or weights.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称那些在数据离开客户端后不再查看原始数据的分布式深度学习技术为满足“禁止窥探”属性的技术。禁止窥探是由信任和监管问题所决定的。例如，由于信任问题，医院通常不被允许与盈利性实体分享数据。由于患者的有限同意以及如HIPAA
    [[5](#bib.bib5), [1](#bib.bib1), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]
    等法规，医院也无法与外部实体共享数据（数据无法物理离开医院）。一些技术更进一步，通过不揭示模型架构的细节来实现。在这些技术中，服务器和客户端都无法访问对方的架构或权重细节。
- en: 1.2 What needs to be protected
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 需要保护的内容
- en: Protection mechanisms in the context of distributed deep learning should protect
    various aspects of datasets such as
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式深度学习的背景下，保护机制应保护数据集的各个方面，如
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Input features
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入特征
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Output labels or responses
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出标签或响应
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Model details including the architecture, parameters and loss function
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型细节，包括架构、参数和损失函数
- en: '4.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Identifiable information such as which party contributed to a specific record
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可识别的信息，如某方对特定记录的贡献
- en: 1.3 Computational Goals
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 计算目标
- en: It is also quite important that any mechanism that aims to protect these details
    also preserves utility of the model above an acceptable level. These goals are
    to be ideally achieved at a low cost with regards to
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，任何旨在保护这些细节的机制也必须在可接受的水平上保持模型的实用性。这些目标理想情况下应以低成本实现。
- en: '1.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Memory
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存
- en: '2.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Computational time
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算时间
- en: '3.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Communications bandwidth
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通信带宽
- en: '4.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Synchronization
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同步
- en: As shown in Fig 1\. below smaller hospitals or tele-healthcare screening centers
    do not acquire an enormous number of diagnostic images and they could also be
    limited by diagnostic manpower. A distributed machine learning method for diagnosis
    in this setting should ideally not share any raw data (no peek) and at same time
    achieve high accuracy while using significantly lower resources. This helps smaller
    hospitals to effectively serve those in need while benefiting from decentralized
    entities.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如图1所示，下方的小型医院或远程医疗筛查中心不会获取大量的诊断图像，并且可能受到诊断人力资源的限制。在这种情况下，分布式机器学习方法应理想地不共享任何原始数据（不窥探），同时在使用显著更少的资源时仍能实现高准确性。这有助于小型医院有效服务有需要的患者，同时从分散的实体中受益。
- en: '| Distributed Method | Partial/Full Leakage | Differential Privacy | Homomorphic
    Encryption | Oblivious Transfer, Garbled Circuits |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 分布式方法 | 部分/完全泄漏 | 差分隐私 | 同态加密 | 不可知转移，混淆电路 |'
- en: '| Distributed NN | [Dean2012, Wen2017, Das2016, Ooi2015], Ben2018] | [Hynes2018,
    Abadi2016, Shokri2015, Papernot2016] | [Juvekar2018, Gilad2016] | [Rouhani2017,
    Mohassel2017, Riazi2018, Orlandi2007] |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 分布式神经网络 | [Dean2012, Wen2017, Das2016, Ooi2015], Ben2018] | [Hynes2018, Abadi2016,
    Shokri2015, Papernot2016] | [Juvekar2018, Gilad2016] | [Rouhani2017, Mohassel2017,
    Riazi2018, Orlandi2007] |'
- en: '| Large Batch Synchronous SGD | [Konečný2015, Chen2016] |  |  |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 大批量同步SGD | [Konečný2015, Chen2016] |  |  |  |'
- en: '| Federated Learning | [McMahan2017, Nock2018] | [Geyer2017] | [Aono2018, Hardy2017]
    | [Bonawitz2016] |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 联邦学习 | [McMahan2017, Nock2018] | [Geyer2017] | [Aono2018, Hardy2017] | [Bonawitz2016]
    |'
- en: '| SplitNN | [Gupta2018, Vepakomma2018] |  |  |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| SplitNN | [Gupta2018, Vepakomma2018] |  |  |  |'
- en: 'Table 1: This is a survey of distributed deep learning methods with decreasing
    levels of leakage from distributed NN to splitNN. Hybrid approaches of these techniques
    and differential privacy, homomorphic encryption and MPC are also included. The
    citations for these 9 groups have been grouped separately with subtitles in the
    references section for convenience.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：这是一个关于分布式深度学习方法的调查，按从分布式神经网络到SplitNN的泄漏水平递减。这些技术的混合方法以及差分隐私、同态加密和多方安全计算也包括在内。这9组的引文已经在参考文献部分按标题分组，以方便查阅。
- en: 2 No peek approaches for distributed deep learning
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 种用于分布式深度学习的无窥探方法
- en: In table 1 we provide corresponding references to various combinations of distributed
    deep learning techniques along with generic approach
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在表1中，我们提供了各种分布式深度学习技术组合的对应参考文献以及通用方法。
- en: '![Refer to caption](img/314b981622d71e5c688553239f74ecfe.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/314b981622d71e5c688553239f74ecfe.png)'
- en: 'Figure 1: Non-cooperating health units'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：不合作的健康单位
- en: '![Refer to caption](img/890bdf5d1e58b2594b57f8405b46ac5b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/890bdf5d1e58b2594b57f8405b46ac5b.png)'
- en: 'Figure 2: Distributed learning without raw data sharing'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：没有原始数据共享的分布式学习
- en: es for protection that are not specific to deep learning such as differential
    privacy homomorphic encryption and secure multi-party computation. The distributed
    deep learning techniques such as splitNN, federated learning and large batch synchronous
    SGD are ‘no peek’. In addition splitNN also protects model details of the architecture
    and weights, unlike the other techniques. We detail this below in table 2 in terms
    of the levels of protection offered on data, intermediate representations and
    hyperparameters that include the deep learning architecture and learnt weights.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 针对深度学习之外的保护方法，例如差分隐私、同态加密和安全多方计算。分布式深度学习技术如SplitNN、联邦学习和大批量同步SGD是‘无窥探’的。此外，SplitNN还保护了模型的架构和权重细节，与其他技术不同。我们将在下面的表2中详细说明这些技术在数据、中间表示和超参数（包括深度学习架构和学习到的权重）上的保护级别。
- en: In federated learning and large batch synchronous SGD, the architecture and
    parameters are shared between the client and server along with intermediate representations
    of the model that include the gradients, activations and weight updates which
    are shared during the learning process. Although the data is not explicitly shared
    in raw form in these two techniques, works like [[106](#bib.bib106)] have shown
    that raw data can be reconstructed up to an approximation by adversaries, especially
    given the fact that the architecture and parameters are not completely protected.
    SplitNN [[31](#bib.bib31)] has an added advantage in this context in that it does
    not share the architecture and weights of the model. The protection offered by
    splitNN lies in the compact representations found in deeper layers of neural networks
    and the difficulty of recovering the underlying data from these representations
    without knowing the model weights used to produce them. Such representations form
    after passing the data through numerous activations whose inverse in the case
    of ReLU are nonlinear and ill-defined (the inverse of a zero-valued ReLU can map
    to any negative number). Such representations have been shown to preserve information
    important for certain tasks (path following [[4](#bib.bib4)]), without revealing
    information about the underlying data (such as image features in a 3D coordinate
    system). The intermediate representation shared by splitNN also requires minimal
    bandwidth in comparison to federated learning and large batch synchronous SGD,
    as only the activations from one layer of the client called the cut layer are
    shared with the server without any associated functions required to invert them
    back to raw data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在联邦学习和大批量同步SGD中，客户端和服务器之间共享模型的架构和参数，以及包括梯度、激活和权重更新的中间表示，这些内容在学习过程中被共享。尽管在这两种技术中数据不会以原始形式明确共享，但如[[106](#bib.bib106)]等工作已表明，原始数据可以通过对手进行近似重建，尤其是考虑到架构和参数并未完全保护。在这种背景下，SplitNN
    [[31](#bib.bib31)]具有额外的优势，因为它不共享模型的架构和权重。SplitNN的保护在于神经网络深层的紧凑表示以及在不知道用于生成这些表示的模型权重的情况下，从这些表示中恢复基础数据的难度。这些表示是在通过众多激活后形成的，其中ReLU的逆变换是非线性且不确定的（零值ReLU的逆变换可以映射到任何负数）。这些表示被证明在某些任务（路径跟踪[[4](#bib.bib4)]）中保留了重要信息，同时没有揭示基础数据的信息（如3D坐标系统中的图像特征）。与联邦学习和大批量同步SGD相比，SplitNN共享的中间表示所需带宽最少，因为只有来自客户端的一个层，即切分层的激活被共享给服务器，而没有任何与之相关的函数用于将其反向转换为原始数据。
- en: '| No Peek Deep Learning | Data revealed |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 无窥探深度学习 | 数据揭示 |'
- en: '&#124; Hyperparameters &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超参数 &#124;'
- en: '&#124; revealed &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 揭示 &#124;'
- en: '|'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Intermediate &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 中级 &#124;'
- en: '&#124; representation &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 表示 &#124;'
- en: '&#124; revealed &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 揭示 &#124;'
- en: '|'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Large Batch &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大批量 &#124;'
- en: '&#124; Synchronous SGD &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 同步SGD &#124;'
- en: '| No | Yes | Yes |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 是 | 是 |'
- en: '| Federated Learning | No | Yes | Yes |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 联邦学习 | 否 | 是 | 是 |'
- en: '| SplitNN | No | No | Yes |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| SplitNN | 否 | 否 | 是 |'
- en: 'Table 2: In this table, we compare the level of privacy offered over data,
    model architecture, model parameters and intermediate representations by techniques
    like federated learning, large batch synchronous SGD and splitNN. On all these
    aspects, splitNN out performs federated learning or large batch synchronous SGD.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在此表中，我们比较了联邦学习、大批量同步SGD和SplitNN等技术在数据、模型架构、模型参数和中间表示方面提供的隐私保护水平。在所有这些方面，SplitNN的表现优于联邦学习或大批量同步SGD。
- en: In table 3, we compare these techniques based on resources required such as
    computations, communication bandwidth, memory and synchronization. We categorize
    the techniques across these dimensions as having low, medium and high requirements.
    As shown, splitNN requires the lowest resources on the client side. This is because
    the architecture is cut (arbitrary shape and not necessarily vertical) at a layer
    where the computations are only performed up to that cut on the client side. The
    rest of the computations happen on the server side. The experimental results in
    [[31](#bib.bib31)], quantify these comparisons.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 3 中，我们根据所需资源（如计算、通信带宽、内存和同步）对这些技术进行了比较。我们根据这些维度将技术分类为低、中和高要求。如图所示，splitNN
    在客户端侧所需的资源最低。这是因为架构在一个层上被切割（形状任意且不一定是垂直的），计算仅在客户端侧进行到该切割层。其余计算发生在服务器端。实验结果在 [[31](#bib.bib31)]
    中量化了这些比较。
- en: 3 Federated Learning
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 联邦学习
- en: 'Key idea: In this approach the clients download the current model from central
    server and improve it by updating their model weights based on their local data.
    The client model parameter updates are aggregated to generate server model. This
    model is again downloaded by the clients and the process continues. There is no
    explicit sharing of raw data in this setup.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想：在这种方法中，客户端从中央服务器下载当前模型，并通过基于本地数据更新其模型权重来改进模型。客户端模型参数更新被聚合以生成服务器模型。该模型再次被客户端下载，过程持续进行。在这种设置中，原始数据没有明确的共享。
- en: 0:   Server executes at round $t\geq 0$:  Distribute $\mathbf{W}_{t}$ to a subset
    $S_{t}$ of $n_{t}$ clients  for each client $k\in S_{t}$ in parallel do     $\mathbf{H}^{k}_{t}\leftarrow\text{ClientUpdate}(k,w_{t})$  Set
    $\mathbf{H}_{t}:=\tfrac{1}{n_{t}}\textstyle\sum_{i\in S_{t}}\mathbf{H}^{i}_{t}$  Set
    $\mathbf{W}_{t+1}=\mathbf{W}_{t}+\eta_{t}\mathbf{H}_{t}$   ClientUpdate($k,\mathbf{W}_{t}$):   
    // *Run on client $k$*  $\mathcal{B}\leftarrow$ (split $\mathcal{P}_{k}$ into
    batches of size $B$)  Set $\mathbf{W}^{k}_{t}=\mathbf{W}_{t}$  for each local
    epoch $i$ from $1$ to $E$ do     for batch $b\in\mathcal{B}$ do        $\mathbf{W}^{k}_{t}\leftarrow\mathbf{W}^{k}_{t}-\eta\triangledown\ell(\mathbf{W}^{k}_{t};b)$  return
    $\mathbf{H}^{k}_{t}=\mathbf{W}^{k}_{t}-\mathbf{W}_{t}$ to server\captionof
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '0:  服务器在轮次 $t\geq 0$ 执行： 将 $\mathbf{W}_{t}$ 分发到 $n_{t}$ 个客户端的子集 $S_{t}$ 对于每个客户端
    $k\in S_{t}$ 并行执行     $\mathbf{H}^{k}_{t}\leftarrow\text{ClientUpdate}(k,w_{t})$  设置
    $\mathbf{H}_{t}:=\tfrac{1}{n_{t}}\textstyle\sum_{i\in S_{t}}\mathbf{H}^{i}_{t}$  设置
    $\mathbf{W}_{t+1}=\mathbf{W}_{t}+\eta_{t}\mathbf{H}_{t}$   ClientUpdate($k,\mathbf{W}_{t}$)：
    // *在客户端 $k$ 上运行*  $\mathcal{B}\leftarrow$（将 $\mathcal{P}_{k}$ 分割成大小为 $B$ 的批次）  设置
    $\mathbf{W}^{k}_{t}=\mathbf{W}_{t}$  对于每个本地纪元 $i$ 从 $1$ 到 $E$ 执行     对于批次 $b\in\mathcal{B}$
    执行        $\mathbf{W}^{k}_{t}\leftarrow\mathbf{W}^{k}_{t}-\eta\triangledown\ell(\mathbf{W}^{k}_{t};b)$  返回
    $\mathbf{H}^{k}_{t}=\mathbf{W}^{k}_{t}-\mathbf{W}_{t}$ 到服务器'
- en: 'algorithmNaive Federated Learning. Goal: To learn $\mathbf{W}\in\mathbb{R}^{d_{1}\times
    d_{2}}$ from data stored across a large number of clients. The $K$ clients are
    indexed by $k$; $B$ is the local minibatch size, $E$ is the number of local epochs,
    and $\eta$ is the learning rate.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 算法朴素联邦学习。目标：从存储在大量客户端上的数据中学习 $\mathbf{W}\in\mathbb{R}^{d_{1}\times d_{2}}$。$K$
    个客户端由 $k$ 索引； $B$ 是本地小批量大小，$E$ 是本地纪元的数量，$\eta$ 是学习率。
- en: 0:   Server executes:  initialize $w_{0}$  for each round $t=1,2,\dots$ do     $m\leftarrow\max(C\cdot
    K,1)$     $S_{t}\leftarrow$ (random set of $m$ clients)     for each client $k\in
    S_{t}$ in parallel do        $w_{t+1}^{k}\leftarrow\text{ClientUpdate}(k,w_{t})$     $w_{t+1}\leftarrow\sum_{k=1}^{K}\frac{n_{k}}{n}w_{t+1}^{k}$  
    ClientUpdate($k,w$):    // *Run on client $k$*  $\mathcal{B}\leftarrow$ (split
    $\mathcal{P}_{k}$ into batches of size $B$)  for each local epoch $i$ from $1$
    to $E$ do     for batch $b\in\mathcal{B}$ do        $w\leftarrow w-\eta\triangledown\ell(w;b)$  return
    $w$ to server\captionof
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '0:  服务器执行： 初始化 $w_{0}$ 对于每轮 $t=1,2,\dots$ 执行     $m\leftarrow\max(C\cdot K,1)$     $S_{t}\leftarrow$（随机选择
    $m$ 个客户端）     对于每个客户端 $k\in S_{t}$ 并行执行        $w_{t+1}^{k}\leftarrow\text{ClientUpdate}(k,w_{t})$     $w_{t+1}\leftarrow\sum_{k=1}^{K}\frac{n_{k}}{n}w_{t+1}^{k}$  
    ClientUpdate($k,w$)： // *在客户端 $k$ 上运行*  $\mathcal{B}\leftarrow$（将 $\mathcal{P}_{k}$
    分割成大小为 $B$ 的批次）  对于每个本地纪元 $i$ 从 $1$ 到 $E$ 执行     对于批次 $b\in\mathcal{B}$ 执行        $w\leftarrow
    w-\eta\triangledown\ell(w;b)$  返回 $w$ 到服务器'
- en: algorithm(Communication-Efficient Learning of Deep Networks from Decentralized
    Data):Federated Averaging. The $K$ clients are indexed by $k$; $B$ is the local
    minibatch size, $E$ is the number of local epochs, and $\eta$ is the learning
    rate.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 算法（高效通信的去中心化数据深度网络学习）：联邦平均。$K$ 个客户端由 $k$ 索引； $B$ 是本地小批量大小，$E$ 是本地纪元的数量，$\eta$
    是学习率。
- en: 3.1 Benefits
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 好处
- en: There is no explicit sharing of raw data. It has been shown in the convex case
    with IID data that in the worst-case, the global model produced is no better than
    training a model on a single client [[9](#bib.bib9), [3](#bib.bib3)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 没有显式共享原始数据。已在凸情况下证明，对于独立同分布数据，最坏情况下，生成的全局模型不比在单个客户端上训练的模型更好[[9](#bib.bib9),
    [3](#bib.bib3)]。
- en: 3.2 Limitations
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 局限性
- en: Performance drops sharply when local data with clients is non i.i.d. That said,
    recent work in [[112](#bib.bib112)] on federated learning in this setting shows
    positive results. It also requires large network bandwidth, memory and computation
    requirements on the client side depending on size of model, computation needs
    of complete forward and backward propagation. Advanced compression methods can
    be used instead to reduce this overload. There has been active recent work for
    neural network compression such as [[35](#bib.bib35), [36](#bib.bib36), [111](#bib.bib111)].
    These works can thereby reduce the costs for communication bandwidth when used
    in distributed learning. Federated learning has no theoretical guarantees or trade-offs
    of privacy or security to date.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户端的本地数据非独立同分布时，性能急剧下降。不过，最近[[112](#bib.bib112)]在这种设置下的联邦学习研究显示了积极的结果。它还需要大带宽、内存和计算要求，具体取决于模型的大小、完整的前向和反向传播计算需求。可以使用先进的压缩方法来减少这种负担。最近在神经网络压缩方面有活跃的研究，如[[35](#bib.bib35),
    [36](#bib.bib36), [111](#bib.bib111)]。这些研究可以减少在分布式学习中使用时的通信带宽成本。到目前为止，联邦学习尚无理论保障或隐私、安全的权衡。
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Client Resources &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 客户端资源 &#124;'
- en: '&#124; Required &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要 &#124;'
- en: '| Compute | Bandwidth | Memory | Synchronization |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 计算 | 带宽 | 内存 | 同步 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Large Batch &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大批量 &#124;'
- en: '&#124; Synchronous SGD &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 同步SGD &#124;'
- en: '| High | High | High |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 高 | 高 | 高 |'
- en: '&#124; Synchronous updates with backup &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带备份的同步更新 &#124;'
- en: '&#124; workers to compensate slow machines. &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工人以补偿慢速机器。 &#124;'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Federated Learning | Medium | Medium | High | Synchronous client-server updates.
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 联邦学习 | 中等 | 中等 | 高 | 同步客户端-服务器更新。 |'
- en: '| SplitNN | Low | Low | Low | Synchronous client-server updates. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| SplitNN | 低 | 低 | 低 | 同步客户端-服务器更新。 |'
- en: 'Table 3: In this table, we compare the resources required for computation,
    bandwidth, memory and synchronization by techniques like federated learning, large
    batch synchronous SGD and SplitNN. SplitNN consumes fewer resources than federated
    learning and large batch synchronous SGD in these aspects except for synchronization
    requirements that are similar across all three techniques.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在此表中，我们比较了联邦学习、大批量同步SGD和SplitNN等技术在计算、带宽、内存和同步方面所需的资源。在这些方面，SplitNN的资源消耗少于联邦学习和大批量同步SGD，除了同步需求在三种技术中相似。
- en: 3.3 Future Trends
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 未来趋势
- en: Data poisoning attacks on federated learning [[37](#bib.bib37)] where malicious
    users can inject false training data to negatively effect the classification performance
    of the model have been proposed. Adversarial robustness to such attacks need to
    be improved. Using neural-network compression schemes in conjunction with federated
    learning to reduce the communication overload is an avenue for future work. Looking
    at combinations of federated learning and differential privacy, secure multi-party
    computation is an interesting direction for future work given that there has been
    active research in the recent time in all these areas.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 关于联邦学习中的数据中毒攻击[[37](#bib.bib37)]，恶意用户可以注入虚假的训练数据，负面影响模型的分类性能。需要提高对这些攻击的对抗鲁棒性。将神经网络压缩方案与联邦学习结合，以减少通信负担，是未来研究的一个方向。鉴于近期在这些领域的活跃研究，联邦学习与差分隐私、以及安全多方计算的结合，是未来研究的一个有趣方向。
- en: 4 Large Batch Synchronous SGD
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 大批量同步SGD
- en: 4.1 Key Idea
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 关键思想
- en: The technique introduces additional backup workers to work on updating the weights,
    and chooses to synchronously update the aggregated model, as soon as any of the
    fastest N workers finish their updates. This is an improvement in accuracy over
    asynchronous SGD where some of the local workers might be updating the weights
    of a more stale model as the client-server updates are asynchronous. It also is
    relatively faster than synchronous SGD with no back-up workers where the servers
    wait for all the clients to finish their updates before aggregating the model
    parameters to update the model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术引入了额外的备份工作节点来更新权重，并选择在任何最快的 N 个工作节点完成更新时同步更新聚合模型。这比异步 SGD 更准确，因为异步 SGD 中的一些本地工作节点可能会更新更过时模型的权重，因为客户端-服务器更新是异步的。它也比没有备份工作节点的同步
    SGD 更快，其中服务器在聚合模型参数以更新模型之前等待所有客户端完成其更新。
- en: 4.2 Benefits
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 优势
- en: It allows for faster synchronous SGD, and is more accurate than asynchronous
    SGD approaches where some clients end up updating the weights based on a more
    stale model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 它允许更快的同步 SGD，比异步 SGD 方法更准确，其中一些客户端最终会基于更过时的模型更新权重。
- en: 4.3 Limitations
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 限制
- en: The computational requirements and communication bandwidth required is much
    higher than other distributed deep learning methods.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的计算资源和通信带宽远高于其他分布式深度学习方法。
- en: 4.4 Future Trends
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 未来趋势
- en: The future trends are similar to that of federated learning as this method is
    very similar in essence to federated learning although it instead runs on a single
    batch of data. This method has high computational overload and network footprint.
    To make this method more sustainable in data center or decentralized settings,
    future work in improving its efficiency is important.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 未来趋势类似于联邦学习，因为该方法在本质上与联邦学习非常相似，尽管它是在单个数据批次上运行。这种方法具有较高的计算负荷和网络开销。为了使这种方法在数据中心或去中心化环境中更具可持续性，未来在提高其效率方面的工作是重要的。
- en: Algorithm 1 Large-Batch SGD
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 大批量 SGD
- en: '0:   Worker Update($k$), where $k=1,\dots,N+b$  Input: Dataset $\mathcal{X}$,
    $B$ mini-batch size.  for $t=0,1,\dots$ do     Wait to read ${\mathbb{\theta}}^{(t)}=({\mathbb{\theta}}^{(t)}[0],\dots,{\mathbb{\theta}}^{(t)}[M])$
          from parameter servers.     $G_{k}^{(t)}:=0$     for $i=1,\dots,B$ do        Sample
    datapoint $\widetilde{x}_{k,i}$ from $\mathcal{X}$.         $G_{k}^{(t)}\leftarrow
    G_{k}^{(t)}+\frac{1}{B}\nabla F(\widetilde{x}_{k,i};{\mathbb{\theta}}^{(t)})$.
    Send $(G_{k}^{(t)},t)$ to parameter servers.   Parameter Server Update($j$), where
    $k=1,\dots,N+b$  Input $\gamma_{0},\gamma_{1},\dots$ learning rates, $\alpha$
    decay rate, $N$ number of mini-batches to aggregate, ${\mathbb{\theta}}^{(0)}$
    model initialization.  for $t=0,1,\dots$ do     $\mathcal{G}=\{\}$      while $|\mathcal{G}|<N$ do        Wait
    for $(G,t^{\prime})$ from any worker.         if $t^{\prime}==t$ then           $\mathcal{G}\leftarrow\mathcal{G}\cup\{G\}$.        else           Drop
    gradient $G$.     ${\mathbb{\theta}}^{(t+1)}[j]\leftarrow{\mathbb{\theta}}^{(t)}[j]-\frac{\gamma_{t}}{N}\sum_{G\in\mathcal{G}}G[j]$.      $\bar{\mathbb{\theta}}^{(t)}[j]=\alpha\bar{\mathbb{\theta}}^{(t-1)}[j]+(1-\alpha){\mathbb{\theta}}^{(t)}[j]$.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '0:   工作节点更新($k$)，其中 $k=1,\dots,N+b$  输入: 数据集 $\mathcal{X}$, $B$ mini-batch
    大小。  对于 $t=0,1,\dots$ 执行     等待从参数服务器读取 ${\mathbb{\theta}}^{(t)}=({\mathbb{\theta}}^{(t)}[0],\dots,{\mathbb{\theta}}^{(t)}[M])$
          $G_{k}^{(t)}:=0$     对于 $i=1,\dots,B$ 执行        从 $\mathcal{X}$ 中采样数据点 $\widetilde{x}_{k,i}$。        $G_{k}^{(t)}\leftarrow
    G_{k}^{(t)}+\frac{1}{B}\nabla F(\widetilde{x}_{k,i};{\mathbb{\theta}}^{(t)})$。
    将 $(G_{k}^{(t)},t)$ 发送到参数服务器。   参数服务器更新($j$)，其中 $k=1,\dots,N+b$  输入 $\gamma_{0},\gamma_{1},\dots$
    学习率，$\alpha$ 衰减率，$N$ 要聚合的小批量数量，${\mathbb{\theta}}^{(0)}$ 模型初始化。  对于 $t=0,1,\dots$
    执行     $\mathcal{G}=\{\}$      当 $|\mathcal{G}|<N$ 时        等待来自任何工作节点的 $(G,t^{\prime})$。         如果
    $t^{\prime}==t$ 则           $\mathcal{G}\leftarrow\mathcal{G}\cup\{G\}$。        否则           丢弃梯度
    $G$。    ${\mathbb{\theta}}^{(t+1)}[j]\leftarrow{\mathbb{\theta}}^{(t)}[j]-\frac{\gamma_{t}}{N}\sum_{G\in\mathcal{G}}G[j]$。      $\bar{\mathbb{\theta}}^{(t)}[j]=\alpha\bar{\mathbb{\theta}}^{(t-1)}[j]+(1-\alpha){\mathbb{\theta}}^{(t)}[j]$。'
- en: 5 Split Learning (SplitNN)
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 分裂学习 (SplitNN)
- en: 5.1 Key Idea
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 关键理念
- en: In this method each client trains the network upto a certain layer known as
    the cut layer and sends the weights to server. The server then trains the network
    for rest of the layers. This completes the forward propagation. Server then generates
    the gradients for the final layer and back-propagates the error until the cut
    layer. The gradient is then passed over to the client. The rest of the back-propagation
    is completed by client. This is continued till the network is trained. The shape
    of the cut could be arbitrary and not necessarily, vertical. In this framework
    as well there is no explicit sharing of raw data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，每个客户端将网络训练到称为切分层的某一层，并将权重发送到服务器。服务器随后训练网络的其余层。这完成了前向传播。服务器然后生成最终层的梯度，并将误差反向传播到切分层。梯度随后传递给客户端。其余的反向传播由客户端完成。这一过程持续进行，直到网络训练完成。切分的形状可以是任意的，不一定是垂直的。在这个框架中，原始数据也没有明确的共享。
- en: 5.2 Benefits
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 好处
- en: Client-side communication costs are significantly reduced as the data to be
    transmitted is restricted to first few layers of the splitNN prior to the split.
    The client-side computation costs of learning the weights of the network are also
    significantly reduced for the same reason. In terms of model performance, the
    accuracies of Split NN remained much higher than federated learning and large
    batch synchronous SGD with a drastically smaller client side computational burden
    when training on a larger number of clients.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端的通信成本显著降低，因为需要传输的数据仅限于`splitNN`切分前的前几层。由于同样的原因，学习网络权重的客户端计算成本也显著降低。在模型性能方面，`Split
    NN`的准确度远高于联邦学习和大批量同步SGD，同时在训练大量客户端时，客户端的计算负担大幅减少。
- en: 5.3 Limitations
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 局限性
- en: It requires a relatively larger overall communication bandwidth when training
    over a smaller number of clients although it ends up being much lower than other
    methods in settings with large number of clients. Advanced neural network compression
    methods such as [[35](#bib.bib35), [36](#bib.bib36), [111](#bib.bib111)] can be
    used to reduce the communication load. The communication bandwidth can also be
    traded for computation on client by allowing for more layers at client to represent
    further compressed representations.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在训练较少数量的客户端时需要相对较大的整体通信带宽，但在客户端数量较多的设置中，它的带宽需求远低于其他方法。可以使用先进的神经网络压缩方法，如[[35](#bib.bib35)、[36](#bib.bib36)、[111](#bib.bib111)]来减少通信负担。通过允许客户端使用更多层来表示进一步压缩的表示，可以将通信带宽与客户端计算进行权衡。
- en: 5.4 Future Trends
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 未来趋势
- en: Given its no peek properties, no model detail sharing and high resource efficiency
    of this recently proposed method, it is well placed to provide direct applications
    to important domains like distributed healthcare, distributed clinical trials,
    inter and intra organizational collaboration and finance. Using neural-network
    compression schemes in conjunction with splitNN to reduce communication overload
    is a promising avenue for future work. Looking at combinations of federated learning
    and differential privacy, secure multi-party computation is an interesting direction
    for future work as well given that there has been active research in recent time
    in all these areas.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种新提出的方法具有不窥探属性、不共享模型细节以及高资源效率，它非常适合直接应用于分布式医疗保健、分布式临床试验、组织间和组织内协作以及金融等重要领域。结合神经网络压缩方案和`splitNN`以减少通信负担是未来工作的一个有前景的方向。考虑到联邦学习和差分隐私的结合，安全多方计算也是一个有趣的未来研究方向，因为近期在这些领域都有积极的研究。
- en: 0:   Server executes at round $t\geq 0$:  for each client $k\in S_{t}$ in parallel do     $\mathbf{A}^{k}_{t}\leftarrow\text{ClientUpdate}(k,t)$     Compute
    $\mathbf{W}_{t}\leftarrow\mathbf{W}_{t}-\eta\triangledown\ell(\mathbf{W}_{t};\mathbf{A}_{t})$     Send
    $\triangledown\ell(\mathbf{A}_{t};\mathbf{W}_{t})$ to client $k$ for $\text{ClientBackprop}(k,t)$  
    ClientUpdate($k,t$):    // *Run on client $k$*  $\mathbf{A}^{k}_{t}=\phi$  for each
    local epoch $i$ from $1$ to $E$ do     for batch $b\in\mathcal{B}$ do        Concatenate
    $f(b,\mathbf{H}^{k}_{t})$ to $\mathbf{A}^{k}_{t}$  return $\mathbf{A}^{k}_{t}$
    to server   ClientBackprop($k,t,\triangledown\ell(\mathbf{A}_{t};\mathbf{W}_{t})$):   
    // *Run on client $k$*  for batch $b\in\mathcal{B}$ do     $\mathbf{H}^{k}_{t}=\mathbf{H}^{k}_{t}-\eta\triangledown\ell(\mathbf{A}_{t};\mathbf{W}_{t};b)$\captionof
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '0: 服务器在回合$t\geq 0$执行：对每个客户端$k\in S_{t}$并行执行 $\mathbf{A}^{k}_{t}\leftarrow\text{ClientUpdate}(k,t)$
    计算 $\mathbf{W}_{t}\leftarrow\mathbf{W}_{t}-\eta\triangledown\ell(\mathbf{W}_{t};\mathbf{A}_{t})$
    将 $\triangledown\ell(\mathbf{A}_{t};\mathbf{W}_{t})$ 发送给客户端$k$进行 $\text{ClientBackprop}(k,t)$
    客户端更新($k,t$): // *在客户端$k$上运行* $\mathbf{A}^{k}_{t}=\phi$ 对每个本地时期$i$从$1$到$E$ 执行
    对每个批次$b\in\mathcal{B}$ 执行 将 $f(b,\mathbf{H}^{k}_{t})$ 连接到 $\mathbf{A}^{k}_{t}$
    返回 $\mathbf{A}^{k}_{t}$ 给服务器 客户端反向传播($k,t,\triangledown\ell(\mathbf{A}_{t};\mathbf{W}_{t})$):
    // *在客户端$k$上运行* 对每个批次$b\in\mathcal{B}$ 执行 $\mathbf{H}^{k}_{t}=\mathbf{H}^{k}_{t}-\eta\triangledown\ell(\mathbf{A}_{t};\mathbf{W}_{t};b)$\captionof'
- en: algorithmSplitNN. The $K$ clients are indexed by $k$; $B$ is the local minibatch
    size, and $\eta$ is the learning rate.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: algorithmSplitNN。$K$个客户端由$k$索引；$B$是本地小批量大小，$\eta$是学习率。
- en: 6 Methods to Further Reduce Leakage and Improve Efficiency
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 种进一步减少泄露和提高效率的方法
- en: 6.1 Obfuscation with Differential Privacy for NN
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 使用差分隐私对神经网络进行混淆
- en: '6.1.1 Key Idea:'
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 关键思想：
- en: The methods in [[14](#bib.bib14)] modify stochastic gradient descent (SGD) based
    optimization used in learning neural networks by clipping the gradient for each
    lot of data and adding Gaussian noise to it during the optimization as opposed
    to adding noise to final parameters of the model, which could be overly conservative
    thereby affecting the utility of the trained model.The sigma for the noise is
    chosen at each step so as to maintain a guaranteed epsilon-delta differential
    privacy for a given lot of data. The tradeoff between the conflicting objectives
    of accuracy and privacy is determined by the lot size.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[[14](#bib.bib14)]中的方法通过剪切每批数据的梯度并在优化过程中添加高斯噪声来修改用于学习神经网络的随机梯度下降（SGD）优化方法，而不是将噪声添加到模型的最终参数，这可能过于保守，从而影响训练模型的实用性。噪声的sigma在每个步骤中选择，以维持对特定数据批次的保证epsilon-delta差分隐私。准确性和隐私的冲突目标之间的权衡由批次大小决定。'
- en: '6.1.2 Benefits and Limitations:'
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 优势和局限性：
- en: The privacy is always dependent on a limited privacy budget while this also
    has an inversely proportional dependency with model accuracy. This is unlike in
    SplitNN where high accuracies are achieved without sharing raw data. The guarantees
    of differential privacy are currently theoretically backed unlike in SplitNN or
    Federated Learning. It also violates the no-peak rule when the privacy budget
    is over.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私始终依赖于有限的隐私预算，同时这与模型准确性呈反比例关系。这与SplitNN不同，在SplitNN中无需共享原始数据即可实现高准确性。差分隐私的保障目前有理论支持，而在SplitNN或联邦学习中没有。隐私预算耗尽时，它也违反了不窥探规则。
- en: '6.1.3 Future Trends:'
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 未来趋势：
- en: There is a lot of scope in combining differential privacy with distributed deep
    learning methods like splitNN, federated learning and large batch SGD as it adds
    to stronger theoretical guarantees on preventing data leakage.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 将差分隐私与分布式深度学习方法如splitNN、联邦学习和大批量SGD结合具有广泛的应用前景，因为这能提供更强的理论保障以防止数据泄露。
- en: 6.2 Homomorphic Encryption for NN
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 同态加密用于神经网络
- en: 6.2.1 Key Idea
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 关键思想
- en: Homomorphic encryption aims to preserve the structure of ciphers such that addition
    and multiplicative operations can be performed after the encryption. All operations
    in a neural network except for activation functions are sum and product operations
    which can be encoded using Homomorphic encryption. Activation functions are approximated
    with either higher degree polynomials, Taylor series, standard or modified Chebyshev
    polynomials that are then implemented as part of Homomorphic encryption schemes.
    The works in [[17](#bib.bib17), [19](#bib.bib19)] apply these ideas in the context
    of deep learning. A greatly detailed survey comparing various software libraries
    for homomorphic encryption is provided in [[115](#bib.bib115)].
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 同态加密旨在保持密码的结构，以便加密后可以执行加法和乘法操作。除了激活函数外，神经网络中的所有操作都是求和和乘积操作，可以使用同态加密进行编码。激活函数用高阶多项式、泰勒级数、标准或改进的切比雪夫多项式来近似，然后作为同态加密方案的一部分实现。[[17](#bib.bib17)、[19](#bib.bib19)]中的工作在深度学习的背景下应用了这些思想。[[115](#bib.bib115)]提供了一个详细的比较各种同态加密软件库的调查。
- en: Algorithm 2 Differentially private SGD
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 差分隐私SGD
- en: '0:  Examples $\{x_{1},...,x_{N}\}$, loss function $\mathcal{L}({\mathbb{\theta}})=\frac{1}{N}\sum_{i}\mathcal{L}({\mathbb{\theta}},x_{i})$.
    Parameters: learning rate $\eta_{t}$, noise scale $\sigma$, group size $L$, gradient
    norm bound $C$.  Initialize ${\mathbb{\theta}}_{0}$ randomly  for $t\in[T]$ do     Take
    a random sample $L_{t}$ with sampling probability $L/N$     Compute gradient     For
    each $i\in L_{t}$, compute $\mathbf{g}_{t}(x_{i})\leftarrow\nabla_{{\mathbb{\theta}}_{t}}\mathcal{L}({\mathbb{\theta}}_{t},x_{i})$     Clip
    gradient     $\bar{\mathbf{g}}_{t}(x_{i})\leftarrow\mathbf{g}_{t}(x_{i})/\max\big{(}1,\frac{\|\mathbf{g}_{t}(x_{i})\|_{2}}{C}\big{)}$     Add
    noise     $\tilde{\mathbf{g}}_{t}\leftarrow\frac{1}{L}\left(\sum_{i}\bar{\mathbf{g}}_{t}(x_{i})+\mathcal{N}(0,\sigma^{2}C^{2}\mathbf{I})\right)$     Descent     
    ${\mathbb{\theta}}_{t+1}\leftarrow{\mathbb{\theta}}_{t}-\eta_{t}\tilde{\mathbf{g}}_{t}$  Output
    ${\mathbb{\theta}}_{T}$ and compute the overall privacy cost $(\varepsilon,\delta)$
    using a privacy accounting method.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '0:  示例 $\{x_{1},...,x_{N}\}$，损失函数 $\mathcal{L}({\mathbb{\theta}})=\frac{1}{N}\sum_{i}\mathcal{L}({\mathbb{\theta}},x_{i})$。参数：学习率
    $\eta_{t}$，噪声尺度 $\sigma$，组大小 $L$，梯度范数界限 $C$。  初始化 ${\mathbb{\theta}}_{0}$ 随机  对于 $t\in[T]$ 做
         以采样概率 $L/N$ 进行随机样本 $L_{t}$      计算梯度      对于每个 $i\in L_{t}$，计算 $\mathbf{g}_{t}(x_{i})\leftarrow\nabla_{{\mathbb{\theta}}_{t}}\mathcal{L}({\mathbb{\theta}}_{t},x_{i})$
         剪裁梯度      $\bar{\mathbf{g}}_{t}(x_{i})\leftarrow\mathbf{g}_{t}(x_{i})/\max\big{(}1,\frac{\|\mathbf{g}_{t}(x_{i})\|_{2}}{C}\big{)}$
         添加噪声      $\tilde{\mathbf{g}}_{t}\leftarrow\frac{1}{L}\left(\sum_{i}\bar{\mathbf{g}}_{t}(x_{i})+\mathcal{N}(0,\sigma^{2}C^{2}\mathbf{I})\right)$
         下降      ${\mathbb{\theta}}_{t+1}\leftarrow{\mathbb{\theta}}_{t}-\eta_{t}\tilde{\mathbf{g}}_{t}$  输出
    ${\mathbb{\theta}}_{T}$ 并使用隐私会计方法计算整体隐私成本 $(\varepsilon,\delta)$。'
- en: 6.2.2 Algorithm
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 算法
- en: There are a variety of schemes which have been shown to have homomorphic properties,
    and are provably secure. The most common use the security of the LWE (learning
    with errors) problem, which seeks to solve a linear system after adding noise.
    This problem is difficult to solve in certain conditions (when the dimension of
    the vector space is much larger than the computational range), and has even been
    shown to be secure under known quantum attacks. In short, LWE contains an algebraic
    structure with homomorphisms for addition and multiplication under the finite
    field of integers (so all multiplication/addition of finite integers can be encrypted
    and evaluated homomorphically as a LWE problem). In practice, implementations
    use R-LWE (Ring-LWE, use polynomial rings instead of vector spaces explicitly),
    which uses a slightly different representation, but the underlying algebraic structure
    remains largely the same.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方案已被证明具有同态性质，并且是可证明安全的。最常见的方案利用LWE（带错误的学习）问题的安全性，该问题在加入噪声后寻求解线性系统。在某些条件下（当向量空间的维度远大于计算范围时），此问题很难解决，甚至已被证明在已知量子攻击下仍然安全。简而言之，LWE包含一个在有限整数域下进行加法和乘法的同态结构（因此，所有有限整数的乘法/加法可以作为LWE问题加密和同态评估）。在实践中，实施使用R-LWE（环LWE，明确使用多项式环而不是向量空间），它使用略有不同的表示，但底层代数结构基本保持不变。
- en: 'Simple LWE example: The key generation, encryption/decryption and corresponding
    add/multiply operations for a simple LWE example are given below.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的LWE示例：下面给出了简单LWE示例的密钥生成、加密/解密以及对应的加法/乘法操作。
- en: 'Keygen:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 'Keygen:'
- en: '|  | $\displaystyle A\in\mathbb{Z}^{m\times n}_{q}$ |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A\in\mathbb{Z}^{m\times n}_{q}$ |  |'
- en: '|  | $\displaystyle S\sim\mathbb{Z}^{n}_{q}$ |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S\sim\mathbb{Z}^{n}_{q}$ |  |'
- en: '|  | $\displaystyle e\sim\mathcal{N}^{n}$ |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle e\sim\mathcal{N}^{n}$ |  |'
- en: '|  | $\displaystyle b=As+e$ |  | (1) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle b=As+e$ |  | (1) |'
- en: 'Encrypt/Encode:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 加密/编码：
- en: '|  | $\displaystyle r_{1},e_{1}\sim\mathcal{N}$ |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{1},e_{1}\sim\mathcal{N}$ |  |'
- en: '|  | $\displaystyle c=\left(c_{a},c_{b}\right)=\left(A^{T}r_{1},b^{T}r_{1}+m_{1}+e_{1}\right)$
    |  | (2) |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c=\left(c_{a},c_{b}\right)=\left(A^{T}r_{1},b^{T}r_{1}+m_{1}+e_{1}\right)$
    |  | (2) |'
- en: 'Add/Multiply:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 加法/乘法：
- en: '|  | $\displaystyle c_{\text{add}}=c_{1}+c_{2}$ |  | (3) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c_{\text{add}}=c_{1}+c_{2}$ |  | (3) |'
- en: '|  | $\displaystyle c_{\text{mult}}=D\left(c_{1}\otimes c_{2}\right)$ |  |
    (4) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c_{\text{mult}}=D\left(c_{1}\otimes c_{2}\right)$ |  |
    (4) |'
- en: where $\otimes$ is the tensor product and $D$ is a dimension switching matrix
    that simplifies the resulting ciphertext. A proof of correctness and further sophistication
    addressing this scheme as a practical system can be found in the LWE literature.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\otimes$ 是张量积，$D$ 是一个维度转换矩阵，它简化了结果密文。关于这一方案作为实际系统的正确性证明和进一步的复杂化，可以在 LWE
    文献中找到。
- en: 6.2.3 Benefits and Limitations
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 优势与限制
- en: These techniques need specialized hardware or extensive computational resources
    to scale. They are capable of providing a higher level of security that allows
    for perfect decryption and are not dependent on trade-offs of obfuscation vs.
    accuracy. The tradeoffs involved in this case are more with regards to computational
    efficiency. For example, some work (Microsoft’s SEAL) shows that to perform logistic
    regression on 1MB of data, 10GB of memory are required, and massive parallelization
    is necessary to achieve real-time throughput on practical problems (some tasks
    may not be parallelizable as such). LWE hardness is believed to be valid even
    in a post-quantum cryptographic environment.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术需要专业的硬件或大量的计算资源来扩展。它们能够提供更高水平的安全性，允许完美解密，并且不依赖于混淆与准确性之间的权衡。这种情况下涉及的权衡更多的是计算效率。例如，一些工作（微软的
    SEAL）表明，要对 1MB 数据执行逻辑回归，需要 10GB 内存，并且需要大规模并行化才能在实际问题中实现实时吞吐（有些任务可能无法如此并行化）。LWE
    硬度被认为在后量子密码环境中仍然有效。
- en: 6.2.4 Future Trends
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.4 未来趋势
- en: This method requires very high computational resources, to make it scalable
    to practical deep learning architectures. Current techniques have only been benchmarked
    on simple networks over small datasets such as MNIST hand-written digit recognition.
    Development of faster methods for large scale deep learning and specialized hardware
    is an important avenue for future work.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法需要非常高的计算资源，以便使其能够扩展到实际的深度学习架构。目前的技术仅在简单网络和小数据集（如 MNIST 手写数字识别）上进行过基准测试。开发针对大规模深度学习的更快方法和专业硬件是未来工作的一个重要方向。
- en: 6.3 Multi-Party Computation (MPC) and Garbled Circuits
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 多方计算 (MPC) 和加密电路
- en: '6.3.1 Key Idea:'
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1 关键思想：
- en: These techniques are based on the idea of secret sharing and zero knowledge
    proof that we describe. The protection is achieved by sharing a secret message
    with different entities and requiring that these entities cooperate together in
    order to gain accessibility. There are certain problems where two entities collaborate
    to compute a function without sharing information about the inputs to the function
    with each other. The classic example is the millionaire’s problem, where $f(x_{1},x_{2})$
    is computed by two parties, when one party has $x_{1}$ and the other has $x_{2}$,
    and it’s impossible for the party to the learn the value the other party holds.
    $f(x_{1},x_{2})$ will return a positive number if $x_{1}>x_{2}$ and a negative
    value if $x_{2}>x_{1}$. In this way, two millionaire’s can determine who has more
    money without sharing the total value at each hold. This has practical applications
    to untrusted “credit checks” or as an example for a particular kind of “Zero Knowledge
    Proof.” Yao’s [[48](#bib.bib48)] garbled circuit protocol for the millionaire’s
    problem and 1–2 oblivious transfer [[114](#bib.bib114), [113](#bib.bib113)] are
    prominent works in this direction. Computational implementations and frameworks
    for this work such as Obliv-C, ObliVM, SPDZ and Sharemind are prominent.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术基于我们描述的秘密共享和零知识证明的理念。保护是通过将秘密信息与不同实体共享，并要求这些实体共同合作以获得访问权限来实现的。有些问题涉及两个实体协作计算一个函数，而不共享函数输入的信息。经典的例子是百万富翁问题，其中$f(x_{1},x_{2})$由两个方计算，一个方拥有$x_{1}$，另一个方拥有$x_{2}$，而一个方无法了解另一个方持有的值。如果$x_{1}>x_{2}$，$f(x_{1},x_{2})$将返回一个正数，如果$x_{2}>x_{1}$，则返回一个负数。通过这种方式，两个百万富翁可以确定谁的钱更多，而无需共享每个人持有的总金额。这在不信任的“信用检查”中有实际应用，或者作为某种“零知识证明”的例子。姚的[[48](#bib.bib48)]百万富翁问题的混淆电路协议和1–2盲传输[[114](#bib.bib114),
    [113](#bib.bib113)]是这一方向的重要工作。像Obliv-C、ObliVM、SPDZ和Sharemind这样的计算实现和框架也很突出。
- en: '6.3.2 Benefits and Limitations:'
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2 优势与局限：
- en: These techniques have been studied for problems like secure stable matching,
    linear system solving and parallel graph algorithms. There is not much work done
    at intersection of MPC with deep learning.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术已经被研究用于如安全稳定匹配、线性系统求解和并行图算法等问题。但在MPC与深度学习交集方面的工作较少。
- en: '6.3.3 Future Trends:'
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3 未来趋势：
- en: Specialized hardwares for MPC are being developed to realize practical applications
    of these protocols. As current day machine learning relies heavily on large scale
    deep-learning architectures on large datasets, bridging this gap between MPC frameworks
    and distributed deep learning is an important avenue for future work.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 专用的MPC硬件正在开发中，以实现这些协议的实际应用。由于当前的机器学习在大数据集上的深度学习架构中依赖较重，因此弥合MPC框架与分布式深度学习之间的差距是未来工作的重要方向。
- en: '| Method | 100 Clients | 500 Clients |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 100个客户端 | 500个客户端 |'
- en: '| --- | --- | --- |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Large Batch SGD | 29.4 TFlops | 5.89 TFlops |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 大批量SGD | 29.4 TFlops | 5.89 TFlops |'
- en: '| Federated Learning | 29.4 TFlops | 5.89 TFlops |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 联邦学习 | 29.4 TFlops | 5.89 TFlops |'
- en: '| SplitNN | 0.1548 TFlops | 0.03 TFlops |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| SplitNN | 0.1548 TFlops | 0.03 TFlops |'
- en: 'Table 4: Computation resources consumed per client when training CIFAR 10 over
    VGG (in teraflops) are drastically lower for SplitNN than Large Batch SGD and
    Federated Learning.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在VGG上训练CIFAR 10时，每个客户端消耗的计算资源（以teraflops为单位），SplitNN显著低于大批量SGD和联邦学习。
- en: '| Method | 100 Clients | 500 Clients |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 100个客户端 | 500个客户端 |'
- en: '| --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Large Batch SGD | 13 GB | 14 GB |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 大批量SGD | 13 GB | 14 GB |'
- en: '| Federated Learning | 3 GB | 2.4 GB |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 联邦学习 | 3 GB | 2.4 GB |'
- en: '| SplitNN | 6 GB | 1.2 GB |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| SplitNN | 6 GB | 1.2 GB |'
- en: 'Table 5: Computation bandwidth required per client when training CIFAR 100
    over ResNet (in gigabytes) is lower for splitNN than large batch SGD and federated
    learning with a large number of clients. For setups with a smaller number of clients,
    federated learning requires a lower bandwidth than splitNN. Large batch SGD methods
    popular in data centers use a heavy bandwidth in both settings.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：在ResNet上训练CIFAR 100时，每个客户端所需的计算带宽（以吉字节为单位），SplitNN低于大批量SGD和联邦学习，特别是在大量客户端的情况下。对于客户端数量较少的设置，联邦学习所需的带宽低于SplitNN。数据中心中流行的大批量SGD方法在两种设置中都使用了大量带宽。
- en: 7 Comparison of resource efficiency across no peek distributed deep learning
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 无窥视分布式深度学习的资源效率比较
- en: We now share a comparison from [[31](#bib.bib31)] of validation accuracy and
    required client computational resources in Figure 1 for the three techniques of
    federated learning, large batch synchronous SGD and splitNN as they are tailored
    for distributed deep learning. As seen in this figure, the comparisons were done
    on datasets of CIFAR 10 and CIFAR 100 using VGG and Resnet-50 architectures for
    100 and 500 client based setups respectively. In this distributed learning experiment
    we clearly see that SplitNN outperforms the techniques of federated learning and
    large batch synchronous SGD in terms of higher accuracies with drastically lower
    computational requirements on the side of clients. In tables 4 and 5 we share
    more comparisons from [[31](#bib.bib31)] on computing resources in TFlops and
    communication bandwidth in GB required by these techniques. SplitNN again has
    a drastic improvement of computational resource efficiency on the client side.
    In the case with a relatively smaller number of clients the communication bandwidth
    required by federated learning is less than splitNN.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在展示了来自[[31](#bib.bib31)]的验证准确度与所需客户端计算资源的比较，见图1，涉及到三种针对分布式深度学习的技术：联邦学习、大批量同步SGD和SplitNN。从图中可以看出，比较是基于使用VGG和Resnet-50架构的CIFAR
    10和CIFAR 100数据集进行的，分别针对100个和500个客户端的设置。在这项分布式学习实验中，我们清楚地看到SplitNN在准确度更高且客户端计算需求显著较低的情况下优于联邦学习和大批量同步SGD。在表4和表5中，我们展示了来自[[31](#bib.bib31)]的更多关于这些技术所需计算资源（以TFlops计）和通信带宽（以GB计）的比较。SplitNN在客户端计算资源效率上再次表现出显著的改进。在客户端数量相对较少的情况下，联邦学习所需的通信带宽低于SplitNN。
- en: '![Refer to caption](img/9df8f8ea932255255cc2c0ff8ab3eb92.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9df8f8ea932255255cc2c0ff8ab3eb92.png)'
- en: (a) Accuracy vs client-side flops on 100 clients with VGG on CIFAR 10
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在100个客户端上使用VGG在CIFAR 10上的准确度与客户端端计算资源的比较
- en: '![Refer to caption](img/498c97194193956a085b15b36df05f36.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/498c97194193956a085b15b36df05f36.png)'
- en: (b) Accuracy vs client-side flops on 500 clients with Resnet-50 on CIFAR 100
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 在500个客户端上使用Resnet-50在CIFAR 100上的准确度与客户端端计算资源的比较
- en: 'Figure 3: We show dramatic reduction in computational burden (in tflops) while
    maintaining higher accuracies when training over large number of clients with
    splitNN. Blue line denotes distributed deep learning using splitNN, red line indicate
    federated averaging and green line indicates large batch SGD.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：我们展示了在使用SplitNN训练大量客户端时，计算负担（以tflops计）的显著减少，同时保持更高的准确度。蓝线表示使用SplitNN的分布式深度学习，红线表示联邦平均，绿线表示大批量SGD。
- en: 8 Conclusion and Future Work
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论与未来工作
- en: 'No peek deep neural networks require new thinking when compared to existing
    data protection methods that attempt to aggregate siloed data for the benefit
    of server models. We describe the emergence of three methods in this setting:
    splitNN, federated learning and large batch SGD. Novel combinations of these methods
    with differential privacy, homomorphic encryption and secure MPC could further
    exploit theoretical guarantees. We show that in settings with large number of
    clients, splitNN needs the least communications bandwidth while federated learning
    does better with relatively smaller number of clients. In this direction, improving
    resource and communication efficiencies of no peek methods would be another avenue
    for impactful future work. Using advanced neural network compression methods [[35](#bib.bib35),
    [111](#bib.bib111), [36](#bib.bib36)] will help further reduce the required network
    footprint. It is also important to study adversarial robustness to data poisoning
    attacks [[37](#bib.bib37)] where malicious users can inject false training data
    to negatively effect the classification performance of the model. Adversarial
    attack schemes from this parallel research area need to be taken into consideration
    while developing no peek mechanisms. Efficient no peek methods have direct applications
    to important domains like distributed healthcare, distributed clinical trials,
    inter and intra organizational collaboration and finance. We therefore contemplate
    novel no peek distributed deep learning applications in the future.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有的数据保护方法相比，不需要窥探的深度神经网络需要新的思维，这些现有方法尝试将孤立的数据汇总以使服务器模型受益。我们描述了在这种背景下出现的三种方法：splitNN、联邦学习和大批量SGD。这些方法与差分隐私、同态加密和安全多方计算的创新组合可能会进一步利用理论保证。我们展示了在客户数量较多的情况下，splitNN
    需要最少的通信带宽，而联邦学习在客户数量相对较少时表现更好。在这方面，提高无窥探方法的资源和通信效率将是另一个有影响力的未来工作方向。使用先进的神经网络压缩方法[[35](#bib.bib35)、[111](#bib.bib111)、[36](#bib.bib36)]将有助于进一步减少所需的网络足迹。研究针对数据中毒攻击的对抗鲁棒性[[37](#bib.bib37)]也很重要，其中恶意用户可以注入虚假训练数据，负面影响模型的分类性能。在开发无窥探机制时，需要考虑这一平行研究领域的对抗攻击方案。高效的无窥探方法在分布式医疗、分布式临床试验、组织间和组织内协作以及金融等重要领域具有直接应用。因此，我们在未来展望新颖的无窥探分布式深度学习应用。
- en: 9 References
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 参考文献
- en: References
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Centers for Disease Control and Prevention,HIPAA privacy rule and public
    health. Guidance from CDC and the US Department of Health and Human Services,
    MMWR: Morbidity and mortality weekly report, US Centers for Disease Control and
    Prevention, 2003'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 疾病控制和预防中心，《HIPAA隐私规则与公共健康》。来自CDC和美国卫生与公共服务部的指导，《MMWR：疾病和死亡周报》，美国疾病控制与预防中心，2003年。'
- en: '[2] keywords = block10, Vepakomma, Praneeth and Gupta, Otkrist and Swedish,
    Tristan and Raskar, Ramesh, Split learning for health: Distributed deep learning
    without sharing raw patient data, arXiv1812.00564,2018'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 关键字 = block10, Vepakomma, Praneeth 和 Gupta, Otkrist 和 Swedish, Tristan
    和 Raskar, Ramesh，《健康中的分割学习：无共享原始病人数据的分布式深度学习》，arXiv1812.00564，2018年。'
- en: '[3] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson and Blaise
    Aguera y Arcas, Communication-efficient learning of deep networks from decentralized
    data, 20’th International Conference on Artificial Intelligence and Statistics
    (AISTATS), 2017.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson 和 Blaise Aguera
    y Arcas，《从分散数据中高效通信学习深度网络》，第20届国际人工智能与统计会议（AISTATS），2017年。'
- en: '[4] Swedish, Tristan and Raskar, Ramesh, Deep Visual Teach and Repeat on Path
    Networks, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
    Workshops, 2018.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Swedish, Tristan 和 Raskar, Ramesh，《路径网络上的深度视觉教学与重复》，IEEE计算机视觉与模式识别大会（CVPR）工作坊，2018年。'
- en: '[5] Annas, George J. , HIPAA regulations-a new era of medical-record privacy?,
    New England Journal of Medicine, Vol.348 (15), pp.1486–1490, 2003.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Annas, George J.，《HIPAA规章——医疗记录隐私的新纪元？》，《新英格兰医学杂志》，第348卷（15期），第1486–1490页，2003年。'
- en: '[6] Mercuri, Rebecca T. , The HIPAA-potamus in health care data security, Communications
    of the ACM, Vol.47 (7), pp.25–28, 2004.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Mercuri, Rebecca T.，《医疗数据安全中的HIPAA大象》，《ACM通信》，第47卷（7期），第25–28页，2004年。'
- en: '[7] Gostin, Lawrence O., Levit, Laura A. and Nass, Sharyl J. , Beyond the HIPAA
    privacy rule: enhancing privacy, improving health through research, National Academies
    Press, 2009.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Gostin, Lawrence O.，Levit, Laura A. 和 Nass, Sharyl J.，《超越HIPAA隐私规则：增强隐私，通过研究改善健康》，国家科学院出版社，2009年。'
- en: '[8] Luxton, David D and Kayl, Robert A and Mishkind, Matthew C. , mHealth data
    security: The need for HIPAA-compliant standardization, Telemedicine and e-Health,
    Vol.18(4), pp. 284–288, 2012.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Luxton, David D 和 Kayl, Robert A 和 Mishkind, Matthew C. ，mHealth数据安全：对HIPAA合规标准化的需求，远程医疗与电子健康，第18卷（4期），页284–288，2012年。'
- en: '[9] Konečny, Jakub and McMahan, H Brendan and Yu, Felix X and Richtárik, Peter
    and Suresh, Ananda Theertha and Bacon, D. , Federated learning: Strategies for
    improving communication efficiency, arXiv preprint arXiv:1610.05492, 2016.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Konečny, Jakub 和 McMahan, H Brendan 和 Yu, Felix X 和 Richtárik, Peter 和
    Suresh, Ananda Theertha 和 Bacon, D. ，联邦学习：提高通信效率的策略，arXiv 预印本 arXiv:1610.05492，2016年。'
- en: '[10] Hynes, Nick and Cheng, Raymond and Song, Dawn , Efficient Deep Learning
    on Multi-Source Private Data, arXiv preprint arXiv:1807.06689, 2018.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Hynes, Nick 和 Cheng, Raymond 和 Song, Dawn ，在多源私有数据上进行高效深度学习，arXiv 预印本
    arXiv:1807.06689，2018年。'
- en: '[11] Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan
    and Mironov, Ilya and Talwar, Kunal and Zhang, Li, Deep learning with differential
    privacy, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
    Security, pp.308–318, 2016.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Abadi, Martin 和 Chu, Andy 和 Goodfellow, Ian 和 McMahan, H Brendan 和 Mironov,
    Ilya 和 Talwar, Kunal 和 Zhang, Li，具有差分隐私的深度学习，第2016年ACM SIGSAC计算机与通信安全会议论文集，页308–318，2016年。'
- en: '[12] Shokri, Reza and Shmatikov, Vitaly, Privacy-preserving deep learning,
    Proceedings of the 22nd ACM SIGSAC conference on computer and communications security,
    pp.1310–1321 2015.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Shokri, Reza 和 Shmatikov, Vitaly，隐私保护的深度学习，第22届ACM SIGSAC计算机与通信安全会议论文集，页1310–1321，2015年。'
- en: '[13] Papernot, Nicolas and Abadi, Martín and Erlingsson, Ulfar and Goodfellow,
    Ian and Talwar, Kunal, Semi-supervised knowledge transfer for deep learning from
    private training data, arXiv preprint arXiv:1610.05755, 2016.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Papernot, Nicolas 和 Abadi, Martín 和 Erlingsson, Ulfar 和 Goodfellow, Ian
    和 Talwar, Kunal，从私有训练数据中进行半监督知识迁移深度学习，arXiv 预印本 arXiv:1610.05755，2016年。'
- en: '[14] Geyer, Robin C and Klein, Tassilo and Nabi, Moin, Differentially private
    federated learning: A client level perspective, arXiv preprint arXiv:1712.07557,
    2017.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Geyer, Robin C 和 Klein, Tassilo 和 Nabi, Moin，差分隐私的联邦学习：客户端级别的视角，arXiv
    预印本 arXiv:1712.07557，2017年。'
- en: '[15] Rouhani, Bita Darvish and Riazi, M Sadegh and Koushanfar, Farinaz, Deepsecure:
    Scalable provably-secure deep learning, arXiv preprint arXiv:1705.08963, 2017.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Rouhani, Bita Darvish 和 Riazi, M Sadegh 和 Koushanfar, Farinaz，Deepsecure：可扩展的可证明安全深度学习，arXiv
    预印本 arXiv:1705.08963，2017年。'
- en: '[16] Rouhani, Bita Darvish and Riazi, M Sadegh and Koushanfar, Farinaz, SecureML:
    A system for scalable privacy-preserving machine learning, 38th IEEE Symposium
    on Security and Privacy (SP), 2017.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Rouhani, Bita Darvish 和 Riazi, M Sadegh 和 Koushanfar, Farinaz，SecureML：一个可扩展隐私保护机器学习系统，第38届IEEE安全与隐私研讨会（SP），2017年。'
- en: '[17] Hesamifard, Ehsan and Takabi, Hassan and Ghasemi, Mehdi, CryptoDL: Deep
    Neural Networks over Encrypted Data, arXiv preprint arXiv:1711.05189, 2017.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Hesamifard, Ehsan 和 Takabi, Hassan 和 Ghasemi, Mehdi，CryptoDL：对加密数据进行深度神经网络分析，arXiv
    预印本 arXiv:1711.05189，2017年。'
- en: '[18] Hardy, Stephen and Henecka, Wilko and Ivey-Law, Hamish and Nock, Richard
    and Patrini, Giorgio and Smith, Guillaume and Thorne, Brian, Private federated
    learning on vertically partitioned data via entity resolution and additively homomorphic
    encryption, arXiv preprint arXiv:1711.10677, 2017.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Hardy, Stephen 和 Henecka, Wilko 和 Ivey-Law, Hamish 和 Nock, Richard 和 Patrini,
    Giorgio 和 Smith, Guillaume 和 Thorne, Brian，通过实体解析和加法同态加密在垂直分区数据上进行私有联邦学习，arXiv
    预印本 arXiv:1711.10677，2017年。'
- en: '[19] Aono, Yoshinori and Hayashi, Takuya and Wang, Lihua and Moriai, Shiho,
    Privacy-preserving deep learning via additively homomorphic encryptionn, IEEE
    Transactions on Information Forensics and Security, Vol.13(5), pp.1333–1345arXiv
    preprint arXiv:1711.10677, 2018.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Aono, Yoshinori 和 Hayashi, Takuya 和 Wang, Lihua 和 Moriai, Shiho，通过加法同态加密实现隐私保护的深度学习，IEEE信息取证与安全交易，第13卷（5期），页1333–1345，arXiv
    预印本 arXiv:1711.10677，2018年。'
- en: '[20] Chen, Jianmin and Pan, Xinghao and Monga, Rajat and Bengio, Samy and Jozefowicz,
    Rafal, Revisiting distributed synchronous SGD, IEEE Transactions on Information
    Forensics and Security, Vol.13(5), arXiv preprint arXiv:1604.00981, 2016.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Chen, Jianmin 和 Pan, Xinghao 和 Monga, Rajat 和 Bengio, Samy 和 Jozefowicz,
    Rafal，重新审视分布式同步SGD，IEEE信息取证与安全交易，第13卷（5期），arXiv 预印本 arXiv:1604.00981，2016年。'
- en: '[21] Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio
    and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and
    Seth, Karn, Practical secure aggregation for privacy-preserving machine learning,
    Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,
    pp.1175–1191, 2017.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Bonawitz, Keith 和 Ivanov, Vladimir 和 Kreuter, Ben 和 Marcedone, Antonio
    和 McMahan, H Brendan 和 Patel, Sarvar 和 Ramage, Daniel 和 Segal, Aaron 和 Seth, Karn，《用于隐私保护机器学习的实用安全聚合》，2017年ACM
    SIGSAC计算机与通信安全会议论文集，第1175–1191页，2017年。'
- en: '[22] Ben-Nun, Tal and Hoefler, Torsten, Demystifying Parallel and Distributed
    Deep Learning: An In-Depth Concurrency Analysis, arXiv preprint arXiv:1802.09941,
    2018.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Ben-Nun, Tal 和 Hoefler, Torsten，《揭秘并行和分布式深度学习：深入的并发分析》，arXiv预印本 arXiv:1802.09941，2018年。'
- en: '[23] Shickel, Benjamin and Tighe, Patrick James and Bihorac, Azra and Rashidi,
    Parisa, Deep EHR: A survey of recent advances in deep learning techniques for
    electronic health record (EHR) analysis, IEEE journal of biomedical and health
    informatics, Vol.22(5) pp.1589–1604, 2018.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Shickel, Benjamin 和 Tighe, Patrick James 和 Bihorac, Azra 和 Rashidi, Parisa，《Deep
    EHR：电子健康记录（EHR）分析中深度学习技术的最新进展综述》，《IEEE生物医学与健康信息学杂志》，第22卷（5期），第1589–1604页，2018年。'
- en: '[24] Ching, Travers and Himmelstein, Daniel S and Beaulieu-Jones, Brett K and
    Kalinin, Alexandr A and Do, Brian T and Way, Gregory P and Ferrero, Enrico and
    Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael M , Opportunities
    and obstacles for deep learning in biology and medicine, Journal of The Royal
    Society Interface, Vol.15(141), 2018.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Ching, Travers 和 Himmelstein, Daniel S 和 Beaulieu-Jones, Brett K 和 Kalinin,
    Alexandr A 和 Do, Brian T 和 Way, Gregory P 和 Ferrero, Enrico 和 Agapow, Paul-Michael
    和 Zietz, Michael 和 Hoffman, Michael M，《生物学和医学中深度学习的机遇与挑战》，《皇家学会界面杂志》，第15卷（141期），2018年。'
- en: '[25] Miotto, Riccardo and Wang, Fei and Wang, Shuang and Jiang, Xiaoqian and
    Dudley, Joel T., Deep learning for healthcare: review, opportunities and challenges,
    Briefings in bioinformatics, 2017.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Miotto, Riccardo 和 Wang, Fei 和 Wang, Shuang 和 Jiang, Xiaoqian 和 Dudley,
    Joel T.，《医疗保健中的深度学习：综述、机遇与挑战》，《生物信息学简报》，2017年。'
- en: '[26] Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar,
    Ameet S., Advances in Neural Information Processing Systems, pp.4424–4434, 2017.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Smith, Virginia 和 Chiang, Chao-Kai 和 Sanjabi, Maziar 和 Talwalkar, Ameet
    S.，《神经信息处理系统进展》，第4424–4434页，2017年。'
- en: '[27] Syverson, Paul and Dingledine, R and Mathewson, N, Tor: The second generation
    onion router,Usenix Security, 2004.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Syverson, Paul 和 Dingledine, R 和 Mathewson, N，《Tor：第二代洋葱路由器》，Usenix Security，2004年。'
- en: '[28] Ravı, Daniele and Wong, Charence and Deligianni, Fani and Berthelot, Melissa
    and Andreu-Perez, Javier and Lo, Benny and Yang, Guang-Zhong, Deep learning for
    health informatics, IEEE journal of biomedical and health informatics, Vol.21(1),
    pp.4–21, 2017.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Ravı, Daniele 和 Wong, Charence 和 Deligianni, Fani 和 Berthelot, Melissa
    和 Andreu-Perez, Javier 和 Lo, Benny 和 Yang, Guang-Zhong，《健康信息学中的深度学习》，《IEEE生物医学与健康信息学杂志》，第21卷（1期），第4–21页，2017年。'
- en: '[29] Alipanahi, Babak and Delong, Andrew and Weirauch, Matthew T and Frey,
    Brendan J, Predicting the sequence specificities of DNA-and RNA-binding proteins
    by deep learning, Nature biotechnology, Vol.33(8), 2015.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Alipanahi, Babak 和 Delong, Andrew 和 Weirauch, Matthew T 和 Frey, Brendan
    J，《通过深度学习预测DNA和RNA结合蛋白的序列特异性》，《自然生物技术》，第33卷（8期），2015年。'
- en: '[30] Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio,
    Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der
    Laak, Jeroen AWM and Van Ginneken, Bram and Sánchez, Clara I, A survey on deep
    learning in medical image analysis, Medical image analysis, Vol.42, pp.60–88,
    2017.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Litjens, Geert 和 Kooi, Thijs 和 Bejnordi, Babak Ehteshami 和 Setio, Arnaud
    Arindra Adiyoso 和 Ciompi, Francesco 和 Ghafoorian, Mohsen 和 van der Laak, Jeroen
    AWM 和 Van Ginneken, Bram 和 Sánchez, Clara I，《医学图像分析中的深度学习调查》，《医学图像分析》，第42卷，第60–88页，2017年。'
- en: '[31] Gupta, Otkrist and Raskar, Ramesh, Distributed learning of deep neural
    network over multiple agents, Journal of Network and Computer Applications, Vol.116,
    pp.1–8, 2018.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Gupta, Otkrist 和 Raskar, Ramesh，《在多个代理上分布式深度神经网络学习》，《网络与计算机应用杂志》，第116卷，第1–8页，2018年。'
- en: '[32] Navathe, Shamkant and Ceri, Stefano and Wiederhold, Gio and Dou, Jinglie,
    Vertical partitioning algorithms for database design, ACM Transactions on Database
    Systems (TODS), Vol.9(4), pp.680–710, 1984.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Navathe, Shamkant 和 Ceri, Stefano 和 Wiederhold, Gio 和 Dou, Jinglie，《数据库设计的垂直分区算法》，《ACM数据库系统事务》（TODS），第9卷（4期），第680–710页，1984年。'
- en: '[33] Agrawal, Sanjay and Narasayya, Vivek and Yang, Beverly, Integrating vertical
    and horizontal partitioning into automated physical database design, Proceedings
    of the 2004 ACM SIGMOD international conference on Management of data, pp.359–370,
    2004.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Agrawal, Sanjay 和 Narasayya, Vivek 和 Yang, Beverly, 将垂直和水平分区集成到自动化物理数据库设计中，第2004届ACM
    SIGMOD国际数据管理会议论文集，第359–370页, 2004年'
- en: '[34] Abadi, Daniel J and Marcus, Adam and Madden, Samuel R and Hollenbach,
    Kate, Scalable semantic web data management using vertical partitioning, Proceedings
    of the 33rd international conference on Very large data bases, pp.411–422, 2007.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Abadi, Daniel J 和 Marcus, Adam 和 Madden, Samuel R 和 Hollenbach, Kate,
    使用垂直分区进行可扩展的语义网数据管理，第33届国际大数据大会论文集，第411–422页, 2007年'
- en: '[35] Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William
    J, Deep gradient compression: Reducing the communication bandwidth for distributed
    training, arXiv preprint arXiv:1712.01887, 2017.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Lin, Yujun 和 Han, Song 和 Mao, Huizi 和 Wang, Yu 和 Dally, William J, 深度梯度压缩：减少分布式训练的通信带宽，arXiv
    预印本 arXiv:1712.01887, 2017年'
- en: '[36] Han, Song and Mao, Huizi and Dally, William J, Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding, arXiv
    preprint arXiv:1510.00149, 2015.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Han, Song 和 Mao, Huizi 和 Dally, William J, 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络，arXiv
    预印本 arXiv:1510.00149, 2015年'
- en: '[37] Fung, Clement and Yoon, Chris JM and Beschastnikh, Ivan, Mitigating Sybils
    in Federated Learning Poisoning, arXiv preprint arXiv:1808.04866, 2018.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Fung, Clement 和 Yoon, Chris JM 和 Beschastnikh, Ivan, 减轻联邦学习中的 Sybil 攻击，arXiv
    预印本 arXiv:1808.04866, 2018年'
- en: '[38] Xie, Liyang and Lin, Kaixiang and Wang, Shu and Wang, Fei and Zhou, Jiayu,
    Differentially Private Generative Adversarial Network, arXiv preprint arXiv:1802.06739,
    2018.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Xie, Liyang 和 Lin, Kaixiang 和 Wang, Shu 和 Wang, Fei 和 Zhou, Jiayu, 差分隐私生成对抗网络，arXiv
    预印本 arXiv:1802.06739, 2018年'
- en: '[39] Crawford, Jack LH and Gentry, Craig and Halevi, Shai and Platt, Daniel
    and Shoup, Victor, Doing Real Work with FHE: The Case of Logistic Regression,
    2018'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Crawford, Jack LH 和 Gentry, Craig 和 Halevi, Shai 和 Platt, Daniel 和 Shoup,
    Victor, 使用同态加密进行实际工作：逻辑回归的案例，2018年'
- en: '[40] Sans, Edouard Dufour and Gay, Romain and Pointcheval, David, Reading in
    the Dark: Classifying Encrypted Digits with Functional Encryption, 2018'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Sans, Edouard Dufour 和 Gay, Romain 和 Pointcheval, David, 在黑暗中阅读：使用功能加密对加密数字进行分类,
    2018年'
- en: '[41] Rosulek, Mike, Improvements for Gate-Hiding Garbled Circuits, International
    Conference in Cryptology in India, 2017'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Rosulek, Mike, Gate-Hiding 加密电路的改进，2017年印度密码学国际会议'
- en: '[42] Choudhury, Ashish and Loftus, Jake and Orsini, Emmanuela and Patra, Arpita
    and Smart, Nigel P, Between a Rock and a Hard Place: Interpolating between MPC
    and FHE, International Conference on the Theory and Application of Cryptology
    and Information Security, 2013'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Choudhury, Ashish 和 Loftus, Jake 和 Orsini, Emmanuela 和 Patra, Arpita 和
    Smart, Nigel P, 进退两难：在 MPC 和同态加密之间插值，第2013届国际密码学与信息安全理论与应用大会, 2013年'
- en: '[43] Keller, Marcel and Pastro, Valerio and Rotaru, Dragos, Overdrive: making
    SPDZ great again, Annual International Conference on the Theory and Applications
    of Cryptographic Techniques, 2018'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Keller, Marcel 和 Pastro, Valerio 和 Rotaru, Dragos, Overdrive: 让 SPDZ 再次伟大，第2018届国际密码学理论与应用大会,
    2018年'
- en: '[44] Juvekar, Chiraag and Vaikuntanathan, Vinod and Chandrakasan, Anantha,
    Gazelle: A low latency framework for secure neural network inference, arXiv preprint
    arXiv:1801.05507, 2018'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Juvekar, Chiraag 和 Vaikuntanathan, Vinod 和 Chandrakasan, Anantha, Gazelle:
    一个低延迟的安全神经网络推理框架，arXiv 预印本 arXiv:1801.05507, 2018年'
- en: '[45] Gilad-Bachrach, Ran and Dowlin, Nathan and Laine, Kim and Lauter, Kristin
    and Naehrig, Michael and Wernsing, John, Cryptonets: Applying neural networks
    to encrypted data with high throughput and accuracy, International Conference
    on Machine Learning, 2016'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Gilad-Bachrach, Ran 和 Dowlin, Nathan 和 Laine, Kim 和 Lauter, Kristin 和
    Naehrig, Michael 和 Wernsing, John, Cryptonets: 在加密数据上应用神经网络，实现高吞吐量和高准确率，第2016届国际机器学习会议'
- en: '[46] Riazi, M Sadegh and Weinert, Christian and Tkachenko, Oleksandr and Songhori,
    Ebrahim M and Schneider, Thomas and Koushanfar, Farinaz, Chameleon: A hybrid secure
    computation framework for machine learning applications, Proceedings of the 2018
    on Asia Conference on Computer and Communications Security, 2018'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Riazi, M Sadegh 和 Weinert, Christian 和 Tkachenko, Oleksandr 和 Songhori,
    Ebrahim M 和 Schneider, Thomas 和 Koushanfar, Farinaz, Chameleon: 一种用于机器学习应用的混合安全计算框架，第2018届亚洲计算机与通信安全会议论文集,
    2018年'
- en: '[47] Liu, Jian and Juuti, Mika and Lu, Yao and Asokan, N, Oblivious neural
    network predictions via minionn transformations, Proceedings of the 2017 ACM SIGSAC
    Conference on Computer and Communications Security, 2017'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Liu, Jian 和 Juuti, Mika 和 Lu, Yao 和 Asokan, N，《通过 minionn 转换的无意识神经网络预测》，2017
    年 ACM SIGSAC 计算机与通信安全会议论文，2017'
- en: '[48] , Andrew C. Yao, Protocols for Secure Computations, University of California
    Berkeley, California, IEEE Foundations of Computer Science, 23rd Annual Symposium
    on, [https://research.cs.wisc.edu/areas/sec/yao1982-ocr.pdf](https://research.cs.wisc.edu/areas/sec/yao1982-ocr.pdf),
    1982'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Andrew C. Yao，《安全计算协议》，加州大学伯克利分校，加州，IEEE 计算机科学基础， 第23届年会，[https://research.cs.wisc.edu/areas/sec/yao1982-ocr.pdf](https://research.cs.wisc.edu/areas/sec/yao1982-ocr.pdf)，1982'
- en: '[49] Ziad, M Tarek Ibn and Alanwar, Amr and Alzantot, Moustafa and Srivastava,
    Mani, Cryptoimg: Privacy preserving processing over encrypted images, IEEE Conference
    on Communications and Network Security, 2016'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Ziad, M Tarek Ibn 和 Alanwar, Amr 和 Alzantot, Moustafa 和 Srivastava, Mani，《Cryptoimg：对加密图像的隐私保护处理》，IEEE
    通信与网络安全会议，2016'
- en: '[50] Mironov, Ilya, Renyi differential privacy, IEEE 30th Computer Security
    Foundations Symposium, 2017'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Mironov, Ilya，《Renyi 差分隐私》，IEEE 第30届计算机安全基础研讨会，2017'
- en: '[51] Kuo, Yu-Hsuan and Chiu, Cho-Chun and Kifer, Daniel and Hay, Michael and
    Machanavajjhala, Ashwin, Differentially private hierarchical count-of-counts histograms,
    Proceedings of the VLDB Endowment, 2018'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Kuo, Yu-Hsuan 和 Chiu, Cho-Chun 和 Kifer, Daniel 和 Hay, Michael 和 Machanavajjhala,
    Ashwin，《差分隐私分层计数直方图》，VLDB 基金会会议论文，2018'
- en: '[52] Li, Ninghui and Li, Tiancheng and Venkatasubramanian, Suresh, t-closeness:
    Privacy beyond k-anonymity and l-diversity, IEEE 23rd International Conference
    on Data Engineering (ICDE), 2007'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Li, Ninghui 和 Li, Tiancheng 和 Venkatasubramanian, Suresh，《t-closeness：超越
    k-匿名性和 l-多样性的隐私》，IEEE 第23届国际数据工程会议 (ICDE)，2007'
- en: '[53] He, Xi and Machanavajjhala, Ashwin and Ding, Bolin, Blowfish privacy:
    Tuning privacy-utility trade-offs using policies, Proceedings of the 2014 ACM
    SIGMOD international conference on Management of data, 2014'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] He, Xi 和 Machanavajjhala, Ashwin 和 Ding, Bolin，《Blowfish 隐私：使用策略调整隐私-效用权衡》，2014
    年 ACM SIGMOD 国际数据管理会议论文，2014'
- en: '[54] He, Xi and Cormode, Graham and Machanavajjhala, Ashwin and Procopiuc,
    Cecilia M and Srivastava, Divesh, DPT: differentially private trajectory synthesis
    using hierarchical reference systems, Proceedings of the VLDB Endowment, 2015'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] He, Xi 和 Cormode, Graham 和 Machanavajjhala, Ashwin 和 Procopiuc, Cecilia
    M 和 Srivastava, Divesh，《DPT：使用分层参考系统的差分隐私轨迹合成》，VLDB 基金会会议论文，2015'
- en: '[55] Carlini, Nicholas and Liu, Chang and Kos, Jernej and Erlingsson, Úlfar
    and Song, Dawn, The Secret Sharer: Measuring Unintended Neural Network Memorization
    & Extracting Secrets,arXiv preprint arXiv:1802.08232, 2018'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Carlini, Nicholas 和 Liu, Chang 和 Kos, Jernej 和 Erlingsson, Úlfar 和 Song,
    Dawn，《秘密分享者：测量神经网络无意记忆和提取秘密》，arXiv 预印本 arXiv:1802.08232，2018'
- en: '[56] Hisham Husain, Zac Cranko, Richard Nock, Integral Privacy for Sampling
    from Mollifier Densities with Approximation Guarantees, arXiv:1806.04819, 2018'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Hisham Husain, Zac Cranko, Richard Nock，《对来自 Mollifier 密度的采样进行整体隐私保护，具有近似保证》，arXiv:1806.04819，2018'
- en: '[57] Zhang, Zuhe and Rubinstein, Benjamin IP and Dimitrakakis, Christos, On
    the Differential Privacy of Bayesian Inference, AAAI conference on artificial
    intelligence, AAAI, 2016'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Zhang, Zuhe 和 Rubinstein, Benjamin IP 和 Dimitrakakis, Christos，《贝叶斯推断的差分隐私》，AAAI
    人工智能会议，AAAI，2016'
- en: '[58] Jain, Prateek and Kothari, Pravesh and Thakurta, Abhradeep, Differentially
    private online learningConference on Learning Theory,2012'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Jain, Prateek 和 Kothari, Pravesh 和 Thakurta, Abhradeep，《差分隐私在线学习》，学习理论会议，2012'
- en: '[59] Xi Wu and Fengan Li and Arun Kumar and Kamalika Chaudhuri and Somesh Jha
    and Jeffrey F. Naughton, Bolt-on Differential Privacy for Scalable Stochastic
    Gradient Descent-based Analytics, SIGMOD Conference, 2017'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Xi Wu 和 Fengan Li 和 Arun Kumar 和 Kamalika Chaudhuri 和 Somesh Jha 和 Jeffrey
    F. Naughton，《用于可扩展随机梯度下降分析的附加差分隐私》，SIGMOD 会议，2017'
- en: '[60] Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah
    and Shmatikov, Vitaly, How To Backdoor Federated Learning, arXiv preprint arXiv:1807.00459,
    2018'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Bagdasaryan, Eugene 和 Veit, Andreas 和 Hua, Yiqing 和 Estrin, Deborah 和
    Shmatikov, Vitaly，《如何背门联邦学习》，arXiv 预印本 arXiv:1807.00459，2018'
- en: '[61] Jalaj Upadhyay, Differentially Private Linear Algebra in the Streaming
    Model, IACR Cryptology ePrint Archive, 2014'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Jalaj Upadhyay，《流模型中的差分隐私线性代数》，IACR 密码学 ePrint 归档，2014'
- en: '[62] Nikita Mishra, Private Stochastic Multi-arm Bandits: From Theory to Practice,
    31 st International Conference on Machine Learning (ICML), 2014'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Nikita Mishra，《私有随机多臂老虎机：从理论到实践》，第31届国际机器学习会议（ICML），2014年'
- en: '[63] Ruochi Zhang and Parv Venkitasubramaniam, Mutual-Information-Private Online
    Gradient Descent Algorithm, 2018 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP), 2018'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Ruochi Zhang 和 Parv Venkitasubramaniam，《互信息私有在线梯度下降算法》，2018年IEEE国际声学、语音与信号处理会议（ICASSP），2018年'
- en: '[64] Seth Gilbert and Xiao Liu and Haifeng Yu, On Differentially Private Online
    Collaborative Recommendation Systems, International Conference on Information
    Security and Cryptology, 2015'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Seth Gilbert 和 Xiao Liu 和 Haifeng Yu，《关于差分隐私在线协同推荐系统》，国际信息安全与密码学会议，2015年'
- en: '[65] John N. Tsitsiklis and Kuang Xu and Zhi Xu, Private Sequential Learning,
    COLT, 2018'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] John N. Tsitsiklis 和 Kuang Xu 和 Zhi Xu，《私有序列学习》，COLT，2018年'
- en: '[66] Depeng Xu and Shuhan Yuan and Xintao Wu, Differential Privacy Preserving
    Causal Graph Discovery, IEEE Symposium on Privacy-Aware Computing (PAC), 2017'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Depeng Xu 和 Shuhan Yuan 和 Xintao Wu，《差分隐私保护的因果图发现》，IEEE隐私保护计算研讨会（PAC），2017年'
- en: '[67] Jacob D. Abernethy and Chansoo Lee and Audra McMillan and Ambuj Tewari,
    Online Learning via Differential Privacy, CoRR, abs/1711.10019, 2017'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Jacob D. Abernethy 和 Chansoo Lee 和 Audra McMillan 和 Ambuj Tewari，《通过差分隐私进行在线学习》，CoRR，abs/1711.10019，2017年'
- en: '[68] Ngoc-Son Phan and Xintao Wu and Dejing Dou, Preserving differential privacy
    in convolutional deep belief networks, Machine Learning Journal, Springer, 2017'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Ngoc-Son Phan 和 Xintao Wu 和 Dejing Dou，《在卷积深度信念网络中保护差分隐私》，《机器学习杂志》，Springer，2017年'
- en: '[69] Giulia C. Fanti and Vasyl Pihur and Úlfar Erlingsson, Building a RAPPOR
    with the Unknown: Privacy-Preserving Learning of Associations and Data Dictionaries,
    2016'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Giulia C. Fanti 和 Vasyl Pihur 和 Úlfar Erlingsson，《与未知构建RAPPOR：隐私保护的关联学习和数据字典》，2016年'
- en: '[70] Ziteng Wang and Chi Jin and Kai Fan and Jiaqi Zhang and Junliang Huang
    and Yiqiao Zhong and Liwei Wang, Differentially Private Data Releasing for Smooth
    Queries, Journal of Machine Learning Research, 2016'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Ziteng Wang 和 Chi Jin 和 Kai Fan 和 Jiaqi Zhang 和 Junliang Huang 和 Yiqiao
    Zhong 和 Liwei Wang，《用于平滑查询的差分隐私数据发布》，《机器学习研究杂志》，2016年'
- en: '[71] Raghavendran Balu and Teddy Furon, Differentially Private Matrix Factorization
    using Sketching Techniques, Proceedings of the 4th ACM Workshop on Information
    Hiding and Multimedia Security, 2016'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Raghavendran Balu 和 Teddy Furon，《使用草图技术的差分隐私矩阵分解》，第4届ACM信息隐藏与多媒体安全研讨会论文集，2016年'
- en: '[72] Mohsen Ghassemi and Anand D. Sarwate and Rebecca N. Wright, Differentially
    Private Online Active Learning with Applications to Anomaly Detection, AISecCCS,
    2016'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Mohsen Ghassemi 和 Anand D. Sarwate 和 Rebecca N. Wright，《带应用于异常检测的差分隐私在线主动学习》，AISecCCS，2016年'
- en: '[73] Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney,
    Michael W, Hessian-based Analysis of Large Batch Training and Robustness to Adversaries,
    arXiv preprint arXiv:1802.08241, 2018'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Yao, Zhewei 和 Gholami, Amir 和 Lei, Qi 和 Keutzer, Kurt 和 Mahoney, Michael
    W，《基于Hessian的大批量训练分析及对对手的鲁棒性》，arXiv预印本 arXiv:1802.08241，2018年'
- en: '[74] Yuncheng Wu and Yao Wu and Hui Peng and Juru Zeng and Hong Chen and Cuiping
    Li, Differentially private density estimation via Gaussian mixtures model, IEEE/ACM
    24th International Symposium on Quality of Service (IWQoS), 2016'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Yuncheng Wu 和 Yao Wu 和 Hui Peng 和 Juru Zeng 和 Hong Chen 和 Cuiping Li，《通过高斯混合模型的差分隐私密度估计》，IEEE/ACM第24届国际服务质量研讨会（IWQoS），2016年'
- en: '[75] Shiva Prasad Kasiviswanathan and Hongxia Jin, Efficient Private Empirical
    Risk Minimization for High-dimensional Learning, Interncational Conference on
    Machine Learning, ICML, 2016'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Shiva Prasad Kasiviswanathan 和 Hongxia Jin，《高维学习中的高效私有经验风险最小化》，国际机器学习会议（ICML），2016年'
- en: '[76] Lu Tian and Bargav Jayaraman and Q. Gu and David Evans, Aggregating Private
    Sparse Learning Models Using Multi-Party Computation, 2016'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Lu Tian 和 Bargav Jayaraman 和 Q. Gu 和 David Evans，《使用多方计算聚合私有稀疏学习模型》，2016年'
- en: '[77] John C. Duchi and Michael I. Jordan and Martin J. Wainwright, Privacy
    Aware Learning, Neural Information Processing Systems (NIPS), 2012'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] John C. Duchi 和 Michael I. Jordan 和 Martin J. Wainwright，《隐私意识学习》，神经信息处理系统会议（NIPS），2012年'
- en: '[78] Marco Gaboardi and Emilio Jesús Gallego Arias and Justin Hsu and Aaron
    Roth and Zhiwei Steven Wu, Dual Query: Practical Private Query Release for High
    Dimensional Data, Interncational Conference on Machine Learning, ICML, 2014'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Marco Gaboardi 和 Emilio Jesús Gallego Arias 和 Justin Hsu 和 Aaron Roth
    和 Zhiwei Steven Wu，《双查询：高维数据的实用私有查询发布》，国际机器学习会议（ICML），2014年'
- en: '[79] Nikolaenko, Valeria and Weinsberg, Udi and Ioannidis, Stratis and Joye,
    Marc and Boneh, Dan and Taft, Nina, Privacy-preserving ridge regression on hundreds
    of millions of records, IEEE Symposium on Security and Privacy (SP), 2013'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Nikolaenko, Valeria 和 Weinsberg, Udi 和 Ioannidis, Stratis 和 Joye, Marc
    和 Boneh, Dan 和 Taft, Nina, 在数亿条记录上进行隐私保护的岭回归，IEEE 安全与隐私研讨会（SP），2013'
- en: '[80] Miran Kim and Kristin E. Lauter, Private Genome Analysis through Homomorphic
    Encryption, BMC medical informatics and decision making, 2015'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Miran Kim 和 Kristin E. Lauter, 通过同态加密进行私人基因组分析，BMC 医学信息学与决策，2015'
- en: '[81] Thore Graepel and Kristin E. Lauter and Michael Naehrig, ML Confidential:
    Machine Learning on Encrypted Data, IACR Cryptology ePrint Archive, 2012'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Thore Graepel 和 Kristin E. Lauter 和 Michael Naehrig, ML Confidential：加密数据上的机器学习，IACR
    密码学 ePrint 存档，2012'
- en: '[82] Raphael Bost and Raluca A. Popa and Stephen Tu and Shafi Goldwasser, Machine
    Learning Classification over Encrypted Data, IACR Cryptology ePrint Archive, 2014'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Raphael Bost 和 Raluca A. Popa 和 Stephen Tu 和 Shafi Goldwasser, 加密数据上的机器学习分类，IACR
    密码学 ePrint 存档，2014'
- en: '[83] Oded Goldreich and Shafi Goldwasser and Dana Ron, Property Testing and
    its connection to Learning and Approximation, Journal of the ACM, 1996'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Oded Goldreich 和 Shafi Goldwasser 和 Dana Ron, 属性测试及其与学习和近似的关系，ACM 杂志，1996'
- en: '[84] Orlandi, Claudio and Piva, Alessandro and Barni, Mauro, Oblivious neural
    network computing via homomorphic encryption, EURASIP Journal on Information Security,
    Springer, 2007'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Orlandi, Claudio 和 Piva, Alessandro 和 Barni, Mauro, 通过同态加密进行的盲神经网络计算，EURASIP
    信息安全期刊，Springer，2007'
- en: '[85] Shengshan Hu and Qian Wang and Jingjun Wang and Sherman S. M. Chow and
    Qin Zou, Securing Fast Learning! Ridge Regression over Encrypted Big Data, IEEE
    Trustcom/BigDataSE/ISPA, 2016'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Shengshan Hu 和 Qian Wang 和 Jingjun Wang 和 Sherman S. M. Chow 和 Qin Zou,
    保护快速学习！加密大数据上的岭回归，IEEE Trustcom/BigDataSE/ISPA，2016'
- en: '[86] Mauro Barni and Pierluigi Failla and Riccardo Lazzeretti and Ahmad-Reza
    Sadeghi and Thomas Schneider, Privacy-Preserving ECG Classification With Branching
    Programs and Neural Networks, IEEE Transactions on Information Forensics and Security,
    2011'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Mauro Barni 和 Pierluigi Failla 和 Riccardo Lazzeretti 和 Ahmad-Reza Sadeghi
    和 Thomas Schneider, 使用分支程序和神经网络的隐私保护 ECG 分类，IEEE 信息取证与安全交易，2011'
- en: '[87] Zhan Qin and Jingbo Yan and Kui Ren and Chang Wen Chen and Xinyu Wang,
    SecSIFT: Secure Image SIFT Feature Extraction in Cloud Computing, TOMCCAP, 2016'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Zhan Qin 和 Jingbo Yan 和 Kui Ren 和 Chang Wen Chen 和 Xinyu Wang, SecSIFT：云计算中的安全图像
    SIFT 特征提取，TOMCCAP，2016'
- en: '[88] Yifeng Zheng and Helei Cui and Xinyu Wang and Jiantao Zhou, Privacy-Preserving
    Image Denoising From External Cloud Databases, IEEE Transactions on Information
    Forensics and Security, 2017'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Yifeng Zheng 和 Helei Cui 和 Xinyu Wang 和 Jiantao Zhou, 来自外部云数据库的隐私保护图像去噪，IEEE
    信息取证与安全交易，2017'
- en: '[89] Reda Bellafqira and Gouenou Coatrieux and Dalel Bouslimi and Gwénolé Quellec
    and Michel Cozic, Secured Outsourced Content Based Image Retrieval Based on Encrypted
    Signatures Extracted From Homomorphically Encrypted Images, CoRR abs/1704.00457,2017'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Reda Bellafqira 和 Gouenou Coatrieux 和 Dalel Bouslimi 和 Gwénolé Quellec
    和 Michel Cozic, 基于同态加密图像提取的加密签名的安全外包内容图像检索，CoRR abs/1704.00457，2017'
- en: '[90] Reda Bellafqira and Gouenou Coatrieux and Emmanuelle Génin and Michel
    Cozic Secure Multilayer Perceptron Based On Homomorphic Encryption, CoRR abs/1806.02709,
    2018'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Reda Bellafqira 和 Gouenou Coatrieux 和 Emmanuelle Génin 和 Michel Cozic
    基于同态加密的安全多层感知机，CoRR abs/1806.02709，2018'
- en: '[91] Ryo Yonetani and Vishnu Naresh Boddeti and Kris M. Kitani and Yoichi Sato,
    Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic Encryption,
    2017 IEEE International Conference on Computer Vision (ICCV), 2017'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Ryo Yonetani 和 Vishnu Naresh Boddeti 和 Kris M. Kitani 和 Yoichi Sato, 使用双重排列同态加密的隐私保护视觉学习，2017
    IEEE 国际计算机视觉会议（ICCV），2017'
- en: '[92] Tribhuvanesh Orekondy and Seong Joon Oh and Bernt Schiele and Mario Fritz,
    Understanding and Controlling User Linkability in Decentralized Learning, CoRR
    abs/1805.05838, 2018'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Tribhuvanesh Orekondy 和 Seong Joon Oh 和 Bernt Schiele 和 Mario Fritz, 理解和控制去中心化学习中的用户链接性，CoRR
    abs/1805.05838，2018'
- en: '[93] David Wu, Using Homomorphic Encryption for Large Scale Statistical Analysis,
    Stanford report, 2012'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] David Wu, 使用同态加密进行大规模统计分析，斯坦福报告，2012'
- en: '[94] Pedro M. Esperança and Louis J. M. Aslett and Chris C. Holmes, Encrypted
    accelerated least squares regression, Artificial intelligence and statistics,
    AISTATS, 2017'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Pedro M. Esperança 和 Louis J. M. Aslett 和 Chris C. Holmes, 加密加速的最小二乘回归，人工智能与统计，AISTATS，2017'
- en: '[95] Richard Nock and Stephen Hardy and Wilko Henecka and Hamish Ivey-Law and
    Giorgio Patrini and Guillaume Smith and Brian Thorne, Entity Resolution and Federated
    Learning get a Federated Resolution, CoRR, abs/1803.04035, 2018'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] 理查德·诺克、斯蒂芬·哈代、维尔科·赫内卡、哈米什·艾维-劳、乔治奥·帕特里尼、吉约姆·史密斯和布赖恩·索恩，《实体解析与联邦学习的联邦解析》，CoRR,
    abs/1803.04035, 2018'
- en: '[96] Wen-Jie Lu and Jun Sakuma, Using Fully Homomorphic Encryption for Statistical
    Analysis of Categorical, Ordinal and Numerical Data, 2016'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] 吕文杰和坂本淳，《使用完全同态加密进行分类、顺序和数值数据的统计分析》，2016'
- en: '[97] Yuchen Zhang and Wenrui Dai and Xiaoqian Jiang and Hongkai Xiong and Shuang
    Wang, FORESEE: Fully Outsourced secuRe gEnome Study basEd on homomorphic Encryption,
    BMC medical informatics and decision making, 2015'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] 余晨张、文睿戴、肖倩姜、洪凯熊和双王，《FORESEE：基于同态加密的完全外包安全基因组研究》，《BMC医学信息学与决策制定》，2015'
- en: '[98] Md. Nazmus Sadat and Md Momin Al Aziz and Noman Mohammed and Feng Chen
    and Shuang Wang and Xiaoqian Jiang, SAFETY: Secure gwAs in Federated Environment
    Through a hYbrid solution with Intel SGX and Homomorphic Encryption, IEEE/ACM
    transactions on computational biology and bioinformatics, 2018'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Md. Nazmus Sadat、Md Momin Al Aziz、诺曼·穆罕默德、冯晨和双王、肖倩姜，《SAFETY：通过与Intel SGX和同态加密的混合解决方案在联邦环境中实现安全gwAs》，《IEEE/ACM计算生物学与生物信息学交易》，2018'
- en: '[99] Md Momin Al Aziz and Mohammad Zahidul Hasan and Noman Mohammed and Dima
    Alhadidi, Secure and Efficient Multiparty Computation on Genomic Data, 2016'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Md Momin Al Aziz、穆罕默德·扎希杜尔·哈桑、诺曼·穆罕默德和迪玛·阿尔哈迪，《基因组数据上的安全高效多方计算》，2016'
- en: '[100] Dwork, Cynthia and Roth, Aaron, The algorithmic foundations of differential
    privacy, Foundations and Trends® in Theoretical Computer Science, Now Publishers
    Inc, 2014'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] 辛西娅·德沃克、亚伦·罗斯，《差分隐私的算法基础》，《理论计算机科学基础与趋势》, Now Publishers Inc, 2014'
- en: '[101] Cynthia Dwork and Frank McSherry and Kobbi Nissim and Adam D. Smith,
    Calibrating Noise to Sensitivity in Private Data Analysis, TCC, 2006'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] 辛西娅·德沃克、弗兰克·麦克谢里、科比·尼西姆和亚当·D·史密斯，《在私人数据分析中将噪声校准到敏感性》，TCC, 2006'
- en: '[102] Dwork, Cynthia, A Firm Foundation for Private Data Analysis, Commun.
    ACM, 10.1145/1866739.1866758, ACM'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] 辛西娅·德沃克，《私人数据分析的坚实基础》，《通信 ACM》，10.1145/1866739.1866758, ACM'
- en: '[103] Bourse, Florian and Minelli, Michele and Minihold, Matthias and Paillier,
    Pascal Fast homomorphic evaluation of deep discretized neural networks, Annual
    International Cryptology Conference, Springer, 2018'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] 弗洛里安·布尔斯、米歇尔·米内利、马蒂亚斯·米尼霍尔德和帕斯卡尔·帕耶，《深度离散神经网络的快速同态评估》，《年度国际密码学会议》，Springer,
    2018'
- en: '[104] Upadhyay, Jalaj, The Price of Differential Privacy for Low-Rank Factorization,
    Neural Information Processing Systems,2018'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] 贾拉吉·乌帕德雅，《低秩分解的差分隐私代价》，《神经信息处理系统》，2018'
- en: '[105] Awan, Jordan and Slavkovic, Aleksandra, Differentially Private Uniformly
    Most Powerful Tests for Binomial Data, Neural Information Processing Systems,
    2018'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] 乔丹·阿万和亚历山德拉·斯拉夫科维奇，《用于二项数据的差分隐私均匀最强检验》，《神经信息处理系统》，2018'
- en: '[106] Hitaj, Briland and Ateniese, Giuseppe and Perez-Cruz, Fernando, Deep
    models under the GAN: information leakage from collaborative deep learning, Proceedings
    of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] 布赖兰·希塔杰、朱塞佩·阿特尼泽和费尔南多·佩雷斯-克鲁兹，《GAN下的深度模型：协作深度学习中的信息泄漏》，《2017年ACM SIGSAC计算机与通信安全会议论文集》，2017'
- en: '[107] Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin,
    Matthieu and Mao, Mark and Senior, Andrew and Tucker, Paul and Yang, Ke and Le,
    Quoc V, Large scale distributed deep networks, Advances in neural information
    processing systems, 2012'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] 杰弗里·迪恩、格雷格·科拉多、拉贾特·蒙加、凯·陈、马修·德文、马克·毛、安德鲁·西尼尔、保罗·塔克、科恩·杨和阮·阮，《大规模分布式深度网络》，《神经信息处理系统进展》，2012'
- en: '[108] Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan
    and Chen, Yiran and Li, Hai, Terngrad: Ternary gradients to reduce communication
    in distributed deep learning, Advances in neural information processing systems,
    2017'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] 温伟、徐聪、颜锋、吴春鹏、王研丹、陈怡然和李海，《Terngrad：减少分布式深度学习中的通信的三元梯度》，《神经信息处理系统进展》，2017'
- en: '[109] Das, Dipankar and Avancha, Sasikanth and Mudigere, Dheevatsa and Vaidynathan,
    Karthikeyan and Sridharan, Srinivas and Kalamkar, Dhiraj and Kaul, Bharat and
    Dubey, Pradeep, Distributed deep learning using synchronous stochastic gradient
    descent, arXiv preprint arXiv:1602.06709, 2016'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] 迪潘卡尔·达斯、萨西坎特·阿万查、迪维萨·穆迪戈雷、卡尔基扬·维达伊纳坦、斯里尼瓦斯·斯里达兰、迪拉吉·卡尔姆卡尔、巴拉特·考尔和普拉迪普·杜贝，《使用同步随机梯度下降的分布式深度学习》，arXiv预印本
    arXiv:1602.06709, 2016'
- en: '[110] Ooi, Beng Chin and Tan, Kian-Lee and Wang, Sheng and Wang, Wei and Cai,
    Qingchao and Chen, Gang and Gao, Jinyang and Luo, Zhaojing and Tung, Anthony KH
    and Wang, Yuan, SINGA: A distributed deep learning platform, Proceedings of the
    23rd ACM international conference on Multimedia, 2015'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Ooi, Beng Chin 和 Tan, Kian-Lee 和 Wang, Sheng 和 Wang, Wei 和 Cai, Qingchao
    和 Chen, Gang 和 Gao, Jinyang 和 Luo, Zhaojing 和 Tung, Anthony KH 和 Wang, Yuan，《SINGA：一个分布式深度学习平台》，第23届ACM国际多媒体会议论文集，2015'
- en: '[111] Louizos, Christos and Ullrich, Karen and Welling, Max Bayesian compression
    for deep learning, Advances in Neural Information Processing Systems, 2017'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Louizos, Christos 和 Ullrich, Karen 和 Welling, Max，《深度学习的贝叶斯压缩》，神经信息处理系统进展，2017'
- en: '[112] Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin,
    Damon and Chandra, Vikas, Federated Learnin'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Zhao, Yue 和 Li, Meng 和 Lai, Liangzhen 和 Suda, Naveen 和 Civin, Damon 和
    Chandra, Vikas，《联邦学习》'
- en: '[113] Even, Shimon and Goldreich, Oded and Lempel, Abraham, A randomized protocol
    for signing contracts, Communications of the ACM, 1985'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Even, Shimon 和 Goldreich, Oded 和 Lempel, Abraham，《签署合同的随机化协议》，ACM通讯，1985'
- en: '[114] Rabin, Michael O, How To Exchange Secrets with Oblivious Transfer, IACR
    Cryptology ePrint Archive, 2005'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Rabin, Michael O，《如何通过盲传输交换秘密》，IACR密码学电子档案，2005'
- en: '[115] Sathya, Sai Sri and Vepakomma, Praneeth and Raskar, Ramesh and Ramachandra,
    Ranjan and Bhattacharya, Santanu, A Review of Homomorphic Encryption Libraries
    for Secure Computation, arXiv1812.02428, 2018'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Sathya, Sai Sri 和 Vepakomma, Praneeth 和 Raskar, Ramesh 和 Ramachandra,
    Ranjan 和 Bhattacharya, Santanu，《同态加密库的安全计算综述》，arXiv1812.02428，2018'
