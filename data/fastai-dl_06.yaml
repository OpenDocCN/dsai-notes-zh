- en: 'Deep Learning 2: Part 1 Lesson 6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习2：第1部分第6课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c)
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从* [*fast.ai 课程*](http://www.fast.ai/)*中的个人笔记。这些笔记将继续更新和改进，因为我继续复习这门课程以“真正”理解它。非常感谢*
    [*Jeremy*](https://twitter.com/jeremyphoward) *和* [*Rachel*](https://twitter.com/math_rachel)
    *给了我这个学习的机会。*'
- en: '[Lesson 6](http://forums.fast.ai/t/wiki-lesson-6/9404)'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[第6课](http://forums.fast.ai/t/wiki-lesson-6/9404)'
- en: '[## Optimization for Deep Learning Highlights in 2017'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 2017年深度学习优化的亮点'
- en: 'Table of contents: Deep Learning ultimately is about finding a minimum that
    generalizes well -- with bonus points for…'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目录：深度学习最终是关于找到一个很好泛化的最小值--附加分为...
- en: ruder.io](http://ruder.io/deep-learning-optimization-2017/index.html?source=post_page-----de70d626976c--------------------------------)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ruder.io](http://ruder.io/deep-learning-optimization-2017/index.html?source=post_page-----de70d626976c--------------------------------)
- en: Review from last week [[2:15](https://youtu.be/sHcLkfRrgoQ?t=2m15s)]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 上周的回顾[[2:15](https://youtu.be/sHcLkfRrgoQ?t=2m15s)]
- en: We took a deep dive to collaborative filtering last week, and we ended up re-creating
    `EmbeddingDotBias` class (`column_data.py`) in fast.ai library. Let’s visualize
    what the embeddings look like [[notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb)].
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上周我们深入研究了协同过滤，最终在fast.ai库中重新创建了`EmbeddingDotBias`类（`column_data.py`）。让我们看看嵌入是什么样子的[[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb)]。
- en: Inside of a learner `learn`, you can get a PyTorch model itself by calling `learn.model`
    . `@property` looks like a regular function, but requires no parenthesis when
    you call it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个学习者`learn`内部，通过调用`learn.model`可以获得一个PyTorch模型本身。`@property`看起来像一个普通的函数，但当你调用它时不需要括号。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`learn.models` is an instance of `CollabFilterModel` which is a thin wrapper
    of PyTorch model that allows us to use “layer groups” which is not a concept available
    in PyTorch and fast.ai uses it to apply different learning rates to different
    sets of layers (layer group).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`learn.models`是`CollabFilterModel`的一个实例，它是PyTorch模型的一个薄包装，允许我们使用“层组”，这是PyTorch中没有的概念，fast.ai使用它来对不同的层组应用不同的学习率。'
- en: PyTorch model prints out the layers nicely including layer name which is what
    we called them in the code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch模型很好地打印出层，包括层名称，这就是我们在代码中称呼它们的方式。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`m.ib` refers to an embedding layer for an item bias — movie bias, in our case.
    What is nice about PyTorch models and layers is that we can call them as if they
    are functions. So if you want to get a prediction, you call `m(...)` and pass
    in variables.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`m.ib`指的是一个项目偏差的嵌入层--在我们的例子中是电影偏差。PyTorch模型和层的好处是我们可以像调用函数一样调用它们。所以如果你想得到一个预测，你调用`m(...)`并传入变量。'
- en: Layers require variables not tensors because it needs to keep track of the derivatives
    — that is the reason for `V(...)` to convert tensor to variable. PyTorch 0.4 will
    get rid of variables and we will be able to use tensors directly.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 层需要变量而不是张量，因为它需要跟踪导数--这就是`V(...)`将张量转换为变量的原因。PyTorch 0.4将摆脱变量，我们将能够直接使用张量。
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `to_np` function will take a variable or a tensor (regardless of being on
    the CPU or GPU) and returns a numpy array. Jeremy’s approach [[12:03](https://youtu.be/sHcLkfRrgoQ?t=12m3s)]
    is to use numpy for everything except when he explicitly needs something to run
    on the GPU or he needs its derivatives — in which case he uses PyTorch. Numpy
    has been around longer than PyTorch and works well with other libraries such as
    OpenCV, Pandas, etc.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_np`函数将获取一个变量或张量（无论是在CPU还是GPU上）并返回一个numpy数组。Jeremy的方法[[12:03](https://youtu.be/sHcLkfRrgoQ?t=12m3s)]是除了在他明确需要在GPU上运行的时候或者需要它的导数时使用PyTorch外，其他情况下都使用numpy。Numpy比PyTorch存在的时间更长，与其他库如OpenCV、Pandas等很好地配合。'
- en: 'A question regarding CPU vs. GPU in production. The suggested approach is to
    do inference on CPU as it is more scalable and you do not need to put things in
    batches. You can move a model onto the CPU by typing `m.cpu()`, similarly a variable
    by typing`V(topMovieIndex).cpu()` (from CPU to GPU would be `m.cuda()`).If your
    server does not have GPU, it will run inference on CPU automatically. For loading
    a saved model that was trained on GPU, take a look at this line of code in `torch_imports.py`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生产中的CPU vs. GPU的问题。建议的方法是在CPU上进行推断，因为它更具可扩展性，而且你不需要将事物放入批处理中。你可以通过键入`m.cpu()`将模型移动到CPU上，类似地，通过键入`V(topMovieIndex).cpu()`将变量移动到CPU上（从CPU到GPU的操作是`m.cuda()`）。如果你的服务器没有GPU，它将自动在CPU上运行推断。要加载在GPU上训练的保存模型，请查看`torch_imports.py`中的这行代码：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we have movie bias for top 3000 movies, and let’s take a look at ratings:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了前3000部电影的电影偏差，让我们来看一下评分：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`zip` will allow you to iterate through multiple lists at the same time.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`zip`将允许你同时迭代多个列表。'
- en: Worst movies
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最差的电影
- en: About sorting key — Python has `itemgetter` function but plain `lambda` is just
    one more character.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于排序键--Python有`itemgetter`函数，但普通的`lambda`只是多了一个字符。
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Best movies
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳电影
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Embedding interpretation [[18:42](https://youtu.be/sHcLkfRrgoQ?t=18m42s)]
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入解释[[18:42](https://youtu.be/sHcLkfRrgoQ?t=18m42s)]
- en: 'Each movie has 50 embeddings and it is hard to visualize 50 dimensional space,
    so we will turn it into a three dimensional space. We can compress dimensions
    using several techniques: Principal Component Analysis ([PCA](https://plot.ly/ipython-notebooks/principal-component-analysis/))
    (Rachel’s Computational Linear Algebra class covers this in detail — which is
    almost identical to Singular Value Decomposition (SVD))'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每部电影有50个嵌入，很难可视化50维空间，所以我们将其转换为三维空间。我们可以使用几种技术来压缩维度：主成分分析（PCA）（Rachel的计算线性代数课程详细介绍了这一点——几乎与奇异值分解（SVD）相同）
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will take a look at the first dimension “easy watching vs. serious” (we
    do not know what it represents but can certainly speculate by looking at them):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看一下第一个维度“轻松观看 vs. 严肃”（我们不知道它代表什么，但可以通过观察来推测）：
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The second dimension “dialog driven vs. CGI”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个维度“对话驱动 vs. CGI”
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Plot
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图
- en: '[PRE10]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: What actually happens when you say `learn.fit` ?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当你说`learn.fit`时实际发生了什么？
- en: '[Entity Embeddings of Categorical Variables](https://arxiv.org/pdf/1604.06737.pdf)
    [[24:42](https://youtu.be/sHcLkfRrgoQ?t=24m42s)]'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[分类变量的实体嵌入](https://arxiv.org/pdf/1604.06737.pdf) [[24:42](https://youtu.be/sHcLkfRrgoQ?t=24m42s)]'
- en: The second paper to talk about categorical embeddings. FIG. 1\. caption should
    sound familiar as they talk about how entity embedding layers are equivalent to
    one-hot encoding followed by a matrix multiplication.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第二篇论文讨论了分类嵌入。图1的标题应该听起来很熟悉，因为它们讨论了实体嵌入层等效于一个独热编码后跟着一个矩阵乘法。
- en: The interesting thing they did was, they took the entity embeddings trained
    by a neural network, replaced each categorical variable with the learned entity
    embeddings, then fed that into Gradient Boosting Machine (GBM), Random Forest
    (RF), and KNN — which reduced the error to something almost as good as neural
    network (NN). This is a great way to give the power of neural net within your
    organization without forcing others to learn deep learning because they can continue
    to use what they currently use and use the embeddings as input. GBM and RF train
    much faster than NN.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 他们做的有趣的事情是，他们用神经网络训练的实体嵌入替换了每个分类变量，然后将其输入到梯度提升机（GBM）、随机森林（RF）和KNN中——这将误差降低到几乎与神经网络（NN）一样好。这是一个很好的方法，可以在组织内提供神经网络的能力，而不需要强迫其他人学习深度学习，因为他们可以继续使用他们目前使用的东西，并将嵌入作为输入。GBM和RF的训练速度比NN快得多。
- en: They also plotted the embeddings of states in Germany which interestingly (“whackingly
    enough” as Jeremy would call it) resembled an actual map.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还绘制了德国各州的嵌入，有趣的是（正如Jeremy所说的那样）它们与实际地图相似。
- en: They also plotted the distances of stores in physical space and embedding space
    — which showed a beautiful and clear correlation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还绘制了实体在物理空间和嵌入空间中的距离——显示了一个美丽而清晰的相关性。
- en: There also seems to be correlation between days of the week, or months of the
    year. Visualizing embeddings can be interesting as it shows you what you expected
    see or what you didn’t.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 星期几或一年中的月份之间似乎也存在相关性。可视化嵌入可能很有趣，因为它向你展示了你期望看到的或者你没有预料到的内容。
- en: A question about Skip-Gram to generate embeddings [[31:31](https://youtu.be/sHcLkfRrgoQ?t=31m31s)]
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于Skip-Gram生成嵌入的问题 [[31:31](https://youtu.be/sHcLkfRrgoQ?t=31m31s)]
- en: Skip-Gram is specific to NLP. A good way to turn an unlabeled problem into a
    labeled problem is to “invent” labels. Word2Vec’s approach was to take a sentence
    of 11 words, delete the middle word, and replace it with a random word. Then they
    gave a label 1 to the original sentence; 0 to the fake one, and built a machine
    learning model to find the fake sentences. As a result, they now have embeddings
    they can use for other purposes. If you do this as a single matrix multiplier
    (shallow model) rather than deep neural net, you can train this very quickly —
    the disadvantage is that it is a less predictive model, but the advantages are
    that you can train on a very large dataset and more importantly, the resulting
    embeddings have *linear characteristics* which allow us to add, subtract, or draw
    nicely. In NLP, we should move past Word2Vec and Glove (i.e. linear based methods)
    because these embeddings are less predictive. The state of the art language model
    uses deep RNN.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-Gram是特定于NLP的。将一个无标签的问题转化为有标签的问题的一个好方法是“发明”标签。Word2Vec的方法是取一个包含11个单词的句子，删除中间的单词，然后用一个随机单词替换它。然后他们给原始句子一个标签1；虚假句子一个标签0，并构建一个机器学习模型来找出虚假句子。结果，他们现在有了可以用于其他目的的嵌入。如果你将这个作为一个单一的矩阵乘法器（浅层模型）而不是深度神经网络来训练，你可以训练得非常快速——缺点是这是一个预测性较差的模型，但优点是你可以在一个非常大的数据集上训练，更重要的是，所得到的嵌入具有*线性特征*，这使我们可以很好地进行加减或绘制。在NLP中，我们应该超越Word2Vec和Glove（即基于线性的方法），因为这些嵌入的预测性较差。最先进的语言模型使用深度RNN。
- en: To learn any kind of feature space, you either need labeled data or you need
    to invent a fake task [[35:45](https://youtu.be/sHcLkfRrgoQ?t=35m45s)]
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 要学习任何类型的特征空间，你要么需要有标记的数据，要么需要发明一个虚假任务 [[35:45](https://youtu.be/sHcLkfRrgoQ?t=35m45s)]
- en: Is one fake task better than another? Not well studied yet.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个虚假任务比另一个更好吗？尚未研究清楚。
- en: Intuitively, we want a task which helps a machine to learn the kinds of relationships
    that you care about.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直觉上，我们希望有一个任务可以帮助机器学习你关心的关系类型。
- en: In computer vision, a type of fake task people use is to apply unreal and unreasonable
    data augmentations.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算机视觉中，人们使用一种虚假任务的类型是应用不真实和不合理的数据增强。
- en: If you can’t come up with great fake tasks, just use crappy one — it is often
    surprising how little you need.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想不出很好的虚假任务，只需使用糟糕的任务——令人惊讶的是你需要的很少。
- en: '**Autoencoder** [[38:10](https://youtu.be/sHcLkfRrgoQ?t=38m10s)] — it recently
    won an [insurance claim competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629).
    Take a single policy, run it through neural net, and have it reconstruct itself
    (make sure that intermediate layers have less activations than the input variable).
    Basically, it is a task whose input = output which works surprisingly well as
    a fake task.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动编码器** - 它最近赢得了一场[保险索赔竞赛](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629)。拿一个单一的政策，通过神经网络运行它，并让它重建自己（确保中间层的激活少于输入变量）。基本上，这是一个输入=输出的任务，作为一个虚假任务效果惊人。'
- en: In computer vision, you can train on cats and dogs and use it for CT scans.
    Maybe it might work for language/NLP! (future research)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，您可以训练猫和狗，并将其用于CT扫描。也许它对语言/NLP也有效！（未来研究）
- en: '[Rossmann](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb)
    [[41:04](https://youtu.be/sHcLkfRrgoQ?t=41m4s)]'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Rossmann](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb)'
- en: A way to use test set properly was added to the notebook.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本中添加了正确使用测试集的方法。
- en: For more detailed explanations, see Machine Learning course.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关更详细的解释，请参见机器学习课程。
- en: '`apply_cats(joined_test, joined)` is used to make sure that the test set and
    the training set have the same categorical codes.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_cats(joined_test, joined)` 用于确保测试集和训练集具有相同的分类代码。'
- en: Keep track of `mapper` which contains the mean and standard deviation of each
    continuous column, and apply the same `mapper` to the test set.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪包含每个连续列的均值和标准差的`mapper`，并将相同的`mapper`应用于测试集。
- en: Do not rely on Kaggle public board — rely on your own thoughtfully created validation
    set.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要依赖Kaggle公共板块 - 依赖您自己精心创建的验证集。
- en: Going over a good [Kernel](https://www.kaggle.com/thie1e/exploratory-analysis-rossmann)
    for Rossmann
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看Rossmann的一个好的[Kernel](https://www.kaggle.com/thie1e/exploratory-analysis-rossmann)
- en: Sunday effect on sales
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周日对销售的影响
- en: There is a jump on sales before and after the store closing. 3rd place winner
    deleted closed store rows before they started any analysis.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在店铺关闭前后销售有所增长。第三名获奖者在开始任何分析之前删除了关闭的店铺行。
- en: '**Don’t touch your data unless you, first of all, analyze to see what you are
    doing is okay — no assumptions.**'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**除非您首先分析以确保您所做的是正确的 - 不要触碰您的数据。**'
- en: Vim tricks [[49:12](https://youtu.be/sHcLkfRrgoQ?t=49m12s)]
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vim技巧
- en: '`:tag ColumnarModelData` will take you to the class definition'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`:tag ColumnarModelData`将带您到类定义处'
- en: '`ctrl + ]` will take you to a definition of what’s under the cursor'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctrl + ]`将带您到光标下的定义'
- en: '`ctrl + t` to go back'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctrl + t`返回'
- en: '`*` to find the usage of what’s under the cursor'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*`查找光标下的内容的用法'
- en: You can switch between tabs with `:tabn` and `:tabp`, With `:tabe <filepath>`
    you can add a new tab; and with a regular `:q` or `:wq` you close a tab. If you
    map `:tabn` and `:tabp` to your F7/F8 keys you can easily switch between files.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用`:tabn`和`:tabp`在选项卡之间切换，使用`:tabe <filepath>`可以添加一个新选项卡；使用常规的`:q`或`:wq`关闭一个选项卡。如果将`:tabn`和`:tabp`映射到F7/F8键，您可以轻松地在文件之间切换。
- en: Inside of ColumnarModelData [[51:01](https://youtu.be/sHcLkfRrgoQ?t=51m1s)]
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在ColumnarModelData内部
- en: 'Slowly but surely, what used to be just “magic” start to look familiar. As
    you can see, `get_learner` returns `Learner` which is fast.ai concept that wraps
    data and PyTorch model:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 慢慢地，曾经只是“魔术”的东西开始变得熟悉起来。正如您所看到的，`get_learner`返回`Learner`，这是fast.ai概念，它包装了数据和PyTorch模型：
- en: Inside of `MixedInputModel` you see how it is creating `Embedding` which we
    now know more about. `nn.ModuleList` is used to register a list of layers. We
    will talk about `BatchNorm` next week, but rest, we have seen before.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在`MixedInputModel`内部，您可以看到它是如何创建我们现在更多了解的`Embedding`的。`nn.ModuleList`用于注册一系列层。我们将在下周讨论`BatchNorm`，但是其他部分，我们之前已经见过。
- en: Similarly, we now understand what’s going on in the `forward` function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们现在了解了`forward`函数中发生的事情。
- en: call embedding layer with *i*th categorical variable and concatenate them all
    together
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用第*i*个分类变量调用嵌入层，并将它们全部连接在一起
- en: put that through dropout
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过dropout处理
- en: go through each one of our linear layers, call it, apply relu and dropout
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐个遍历我们的线性层，称之为，应用relu和dropout
- en: then final linear layer has a size of 1
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后最终的线性层大小为1
- en: if `y_range` is passed in, apply sigmoid and fit the output within a range (which
    we learned last week)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果传入`y_range`，则应用sigmoid并将输出拟合在一个范围内（我们上周学到的）
- en: '[Stochastic Gradient Descent — SGD](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson6-sgd.ipynb)
    [[59:56](https://youtu.be/sHcLkfRrgoQ?t=59m56s)]'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[随机梯度下降 - SGD](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson6-sgd.ipynb)'
- en: To make sure we are totally comfortable with SGD, we will use it to learn `*y
    = ax + b*` . If we can solve something with 2 parameters, we can use the same
    technique to solve 100 million parameters.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们完全熟悉SGD，我们将使用它来学习`*y = ax + b*`。如果我们可以用2个参数解决问题，我们可以使用相同的技术来解决1亿个参数。
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To get started, we need a loss function. This is a regression problem since
    the output is continuous output, and the most common loss function is the mean
    squared error (MSE).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们需要一个损失函数。这是一个回归问题，因为输出是连续输出，最常见的损失函数是均方误差（MSE）。
- en: Regression — the target output is a real number or a whole vector of real numbers
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 回归 - 目标输出是一个实数或一整个实数向量
- en: ''
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Classification — the target output is a class label
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 分类 - 目标输出是一个类标签
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`y_hat` — predictions'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_hat` - 预测'
- en: 'We will make 10,000 more fake data and turn them into PyTorch variables because
    Jeremy doesn’t like taking derivatives and PyTorch can do that for him:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建10,000个更多的虚假数据，并将它们转换为PyTorch变量，因为Jeremy不喜欢求导，PyTorch可以为他做到这一点：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then create random weight for `a` and `b` , they are the variables we want to
    learn, so set `requires_grad=True` .
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后为`a`和`b`创建随机权重，它们是我们想要学习的变量，因此设置`requires_grad=True`。
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then set the learning rate and do 10,000 epoch of full gradient descent (not
    SGD as each epoch will look at all of the data):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后设置学习率，并进行10000个完全梯度下降的周期（不是SGD，因为每个周期将查看所有数据）：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: calculate the loss (remember, `a` and `b` are set to random initially)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失（记住，`a`和`b`最初是随机设置的）
- en: from time to time (every 1000 epochs), print out the loss
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偶尔（每1000个周期）打印出损失
- en: '`loss.backward()` will calculate gradients for all variables with `requires_grad=True`
    and fill in `.grad` property'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss.backward()`将计算所有`requires_grad=True`的变量的梯度，并填充`.grad`属性'
- en: update `a` to whatever it was minus LR * `grad` ( `.data` accesses a tensor
    inside of a variable)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`a`更新为原来的值减去LR * `grad`（`.data`访问变量内的张量）
- en: when there are multiple loss functions or many output layers contributing to
    the gradient, PyTorch will add them together. So you need to tell when to set
    gradients back to zero (`zero_()` in the `_` means that the variable is changed
    in-place).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当有多个损失函数或许多输出层对梯度有贡献时，PyTorch会将它们相加。所以你需要告诉何时将梯度设置回零（`zero_()`中的`_`表示变量是原地更改的）。
- en: The last 4 lines of code is what is wrapped in `optim.SGD.step` function
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码的最后4行是包含在`optim.SGD.step`函数中的内容
- en: Let’s do this with just Numpy (without PyTorch) [[1:07:01](https://youtu.be/sHcLkfRrgoQ?t=1h7m1s)]
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们只用Numpy（不用PyTorch）来做这个[[1:07:01](https://youtu.be/sHcLkfRrgoQ?t=1h7m1s)]
- en: 'We actually have to do calculus, but everything else should look similar:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上需要做微积分，但其他方面应该看起来类似：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Just for fun, you can use `matplotlib.animation.FuncAnimation` to animate:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 只是为了好玩，你可以使用`matplotlib.animation.FuncAnimation`来制作动画：
- en: 'Tip: Fast.ai AMI did not come with `ffmpeg` . So if you see `KeyError: ''ffmpeg''`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '提示：Fast.ai AMI没有附带`ffmpeg`。所以如果你看到`KeyError: ''ffmpeg''`'
- en: Run `print(animation.writers.list())` and print out a list of available MovieWriters
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`print(animation.writers.list())`并打印出可用的MovieWriters列表
- en: If `ffmpeg` is among it. Otherwise [install it](https://github.com/adaptlearning/adapt_authoring/wiki/Installing-FFmpeg).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`ffmpeg`在其中。否则[安装它](https://github.com/adaptlearning/adapt_authoring/wiki/Installing-FFmpeg)。
- en: '[Recurrent Neural Network — RNN](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson6-rnn.ipynb)
    [[1:09:16](https://youtu.be/sHcLkfRrgoQ?t=1h9m16s)]'
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[循环神经网络 - RNN](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson6-rnn.ipynb)
    [[1:09:16](https://youtu.be/sHcLkfRrgoQ?t=1h9m16s)]'
- en: Let’s learn how to write philosophy like Nietzsche. This is similar to a language
    model we learned in lesson 4, but this time, we will do it one character at a
    time. RNN is no different from what we have already learned.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何像尼采一样写哲学。这类似于我们在第4课学到的语言模型，但这次，我们将一次一个字符地做。RNN与我们已经学过的内容没有区别。
- en: 'Some examples:'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些例子：
- en: '[SwiftKey](https://blog.swiftkey.com/neural-networks-a-meaningful-leap-for-mobile-typing/)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SwiftKey](https://blog.swiftkey.com/neural-networks-a-meaningful-leap-for-mobile-typing/)'
- en: '[Andrej Karpathy LaTex generator](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Andrej Karpathy LaTex生成器](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
- en: Basic NN with single hidden layer
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有单隐藏层的基本NN
- en: All shapes are activations (an activation is a number that has been calculated
    by a relu, matrix product, etc.). An arrow is a layer operation (possibly more
    than one). Check out Machine Learning course lesson 9–11 for creating this from
    scratch.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 所有形状都是激活（激活是通过relu、矩阵乘积等计算得到的数字）。箭头是层操作（可能不止一个）。查看机器学习课程第9-11课，从头开始创建这个。
- en: Image CNN with single dense hidden layer
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有单个密集隐藏层的图像CNN
- en: We will cover how to flatten a layer next week more, but the main method is
    called “adaptive max pooling” — where we average across the height and the width
    and turn it into a vector.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下周更详细地介绍如何展平一个层，但主要方法被称为“自适应最大池化”——在高度和宽度上取平均值，将其转换为向量。
- en: '`batch_size` dimension and activation function (e.g. relu, softmax) are not
    shown here'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有显示`batch_size`维度和激活函数（例如relu，softmax）
- en: Predicting char 3 using chars 1 & 2 [[1:18:04](https://youtu.be/sHcLkfRrgoQ?t=1h18m4s)]
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用字符1和2预测字符3[[1:18:04](https://youtu.be/sHcLkfRrgoQ?t=1h18m4s)]
- en: We are going to implement this one for NLP.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为NLP实现这个。
- en: 'Input can be one-hot-encoded character (length of vector = # of unique characters)
    or a single integer and pretend it is one-hot-encoded by using an embedding layer.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入可以是一个独热编码字符（向量长度=唯一字符数）或一个整数，并通过使用嵌入层假装它是独热编码。
- en: The difference from the CNN one is that then char 2 inputs gets added.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与CNN的区别在于然后char 2输入被添加。
- en: layer operations not shown; remember arrows represent layer operations
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 未显示层操作；记住箭头代表层操作
- en: Let’s implement this without torchtext or fast.ai library so we can see.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现这个，没有torchtext或fast.ai库，这样我们就可以看到。
- en: '`set` will return all unique characters.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`set`将返回所有唯一字符。'
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Always good to put a null or an empty character for padding.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是好的为填充放置一个空字符或空字符。
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Mapping of every character to a unique ID, and a unique ID to a character
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个字符映射到唯一ID，并将唯一ID映射到字符
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we can represent the text with its ID’s:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用它的ID来表示文本：
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Question: Character based model vs. word based model [[1:22:30](https://youtu.be/sHcLkfRrgoQ?t=1h22m30s)]'
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题：基于字符的模型与基于单词的模型[[1:22:30](https://youtu.be/sHcLkfRrgoQ?t=1h22m30s)]
- en: Generally, you want to combine character level model and word level model (e.g.
    for translation).
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，你希望结合字符级模型和单词级模型（例如用于翻译）。
- en: Character level model is useful when a vocabulary contains unusual words — which
    word level model will just treat as “unknown”. When you see a word you have not
    seen before, you can use a character level model.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当词汇表包含不寻常的单词时，字符级模型很有用——而单词级模型将其视为“未知”。当你看到一个以前没有见过的单词时，你可以使用字符级模型。
- en: There is also something in between that is called Byte Pair Encoding (BPE) which
    looks at n-gram of characters.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有一种叫做字节对编码（BPE）的东西，它查看字符的n-gram。
- en: Create inputs [[1:23:48](https://youtu.be/sHcLkfRrgoQ?t=1h23m48s)]
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建输入[[1:23:48](https://youtu.be/sHcLkfRrgoQ?t=1h23m48s)]
- en: '[PRE21]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that `c1_dat[n+1] == c4_dat[n]` since we are skipping by 3 (the third argument
    of `range`)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`c1_dat[n+1] == c4_dat[n]`，因为我们是按3跳过的（`range`的第三个参数）
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`x`’s are our inputs, `y` is our target value.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`x`是我们的输入，`y`是我们的目标值。'
- en: Build a model [[1:26:08](https://youtu.be/sHcLkfRrgoQ?t=1h26m8s)]
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个模型[[1:26:08](https://youtu.be/sHcLkfRrgoQ?t=1h26m8s)]
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`n_hiddein` — “# activations” in the diagram.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_hiddein`-图表中的“#激活”。'
- en: '`n_fac` — the size of the embedding matrix.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_fac`-嵌入矩阵的大小。'
- en: Here is the updated version of the previous diagram. Notice that now arrows
    are colored. All the arrows with the same color will use the same weight matrix.
    The idea here is that a character would not have different meaning (semantically
    or conceptually) depending on whether it is the first, the second, or the third
    item in a sequence, so treat them the same.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是上一个图表的更新版本。请注意，现在箭头是彩色的。所有具有相同颜色的箭头将使用相同的权重矩阵。这里的想法是，一个字符不会根据它在序列中是第一个、第二个还是第三个项目而具有不同的含义（语义上或概念上），因此将它们视为相同。
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[Video [1:27:57]](https://youtu.be/sHcLkfRrgoQ?t=1h27m57s)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[视频[1:27:57]](https://youtu.be/sHcLkfRrgoQ?t=1h27m57s)'
- en: '[[1:29:58](https://youtu.be/sHcLkfRrgoQ?t=1h29m58s)] It is important that this
    `l_hidden` uses a square weight matrix whose size matches the output of `l_in`.
    Then `h` and `in2` will be the same shape allowing us to sum them together as
    you see in `self.l_hidden(h+in2)`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[[1:29:58](https://youtu.be/sHcLkfRrgoQ?t=1h29m58s)]重要的是，这个`l_hidden`使用一个大小与`l_in`的输出匹配的方形权重矩阵。然后`h`和`in2`将具有相同的形状，允许我们像在`self.l_hidden(h+in2)`中看到的那样将它们相加。'
- en: '`V(torch.zeros(in1.size()).cuda())` is only there to make the three lines identical
    to make it easier to put in a for loop later.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V(torch.zeros(in1.size()).cuda())`只是为了使这三行相同，以便稍后更容易放入循环中。'
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will reuse `ColumnarModelData`[[1:32:20](https://youtu.be/sHcLkfRrgoQ?t=1h32m20s)].
    If we stack `x1` , `x2`, and `x3`, we will get `c1`, `c2`, `c3` in the `forward`
    method. `ColumnarModelData.from_arrays` will come in handy when you want to train
    a model in raw-er approach, what you put in `[x1, x2, x3]` , you will get back
    in `**def** **forward**(self, c1, c2, c3)`
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用`ColumnarModelData`[[1:32:20](https://youtu.be/sHcLkfRrgoQ?t=1h32m20s)]。如果我们堆叠`x1`，`x2`和`x3`，我们将在`forward`方法中得到`c1`，`c2`，`c3`。当您想以原始方式训练模型时，`ColumnarModelData.from_arrays`会派上用场，您在`[x1,
    x2, x3]`中放入的内容，将在`**def** **forward**(self, c1, c2, c3)`中返回。
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We create a standard PyTorch model (not `Learner`)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建一个标准的PyTorch模型（不是`Learner`）
- en: Because it is a standard PyTorch model, don’t forget `.cuda`
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为它是一个标准的PyTorch模型，不要忘记`.cuda`
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`iter` to grab an iterator'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iter`来获取一个迭代器'
- en: '`next` returns a mini-batch'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next`返回一个小批量'
- en: “Variabize” the `xs` tensor, and put it through the model — which will give
    us 512x85 tensor containing prediction (batch size * unique character)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “变量化”`xs`张量，并将其通过模型-这将给我们一个包含预测的512x85张量（批量大小*独特字符）
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Create a standard PyTorch optimizer — for which you need to pass in a list of
    things to optimize, which is returned by `m.parameters()`
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个标准的PyTorch优化器-需要传入一个要优化的列表，该列表由`m.parameters()`返回
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We do not find a learning rate finder and SGDR because we are not using `Learner`,
    so we would need to manually do learning rate annealing (set LR a little bit lower)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们找不到学习率查找器和SGDR，因为我们没有使用`Learner`，所以我们需要手动进行学习率退火（将LR设置得稍低一些）
- en: Testing a model [[1:35:58](https://youtu.be/sHcLkfRrgoQ?t=1h35m58s)]
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试一个模型[[1:35:58](https://youtu.be/sHcLkfRrgoQ?t=1h35m58s)]
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This function takes three characters and return what the model predict as the
    fourth. Note: `np.argmax` returns index of the maximum values.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受三个字符，并返回模型预测的第四个。注意：`np.argmax`返回最大值的索引。
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Let’s create our first RNN [[1:37:45](https://youtu.be/sHcLkfRrgoQ?t=1h37m45s)]
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们创建我们的第一个RNN[[1:37:45](https://youtu.be/sHcLkfRrgoQ?t=1h37m45s)]
- en: 'We can simplify the previous diagram as below:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简化上一个图表如下：
- en: Predicting char n using chars 1 to n-1
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用1到n-1个字符预测第n个字符
- en: 'Let’s implement this. This time, we will use the first 8 characters to predict
    the 9th. Here is how we create inputs and output just like the last time:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现这个。这次，我们将使用前8个字符来预测第9个。这是如何创建输入和输出的，就像上次一样：
- en: '[PRE32]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Notice that they are overlaps (i.e. 0–7 to predict 8, 1–8 to predict 9).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它们是重叠的（即0-7预测8，1-8预测9）。
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Create the model [[1:43:03](https://youtu.be/sHcLkfRrgoQ?t=1h43m3s)]
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建模型[[1:43:03](https://youtu.be/sHcLkfRrgoQ?t=1h43m3s)]
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Most of the code is the same as before. You will notice that there is one `for`
    loop in `forward` function.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码与以前相同。您会注意到`forward`函数中有一个`for`循环。
- en: Hyperbolic Tangent (Tanh) [[1:43:43](https://youtu.be/sHcLkfRrgoQ?t=1h43m43s)]
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 双曲正切（Tanh）[[1:43:43](https://youtu.be/sHcLkfRrgoQ?t=1h43m43s)]
- en: ''
  id: totrans-176
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is a sigmoid that is offset. It is common to use hyperbolic tanh in the hidden
    state to hidden state transition because it stops it from flying off too high
    or too low. For other purposes, relu is more common.
  id: totrans-177
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是一个偏移的sigmoid函数。在隐藏状态到隐藏状态的转换中使用双曲正切是常见的，因为它可以阻止其飞得太高或太低。对于其他目的，relu更常见。
- en: This now is a quite deep network as it uses 8 characters instead of 2\. And
    as networks get deeper, they become harder to train.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是一个相当深的网络，因为它使用8个字符而不是2个。随着网络变得更深，它们变得更难训练。
- en: '[PRE35]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Adding vs. Contatenating
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加vs.连接
- en: We now will try something else for `self.l_hidden(**h+inp**)`[[1:46:04](https://youtu.be/sHcLkfRrgoQ?t=1h46m4s)].
    The reason is that the input state and the hidden state are qualitatively different.
    Input is the encoding of a character, and h is an encoding of series of characters.
    So adding them together, we might lose information. Let’s concatenate them instead.
    Don’t forget to change the input to match the shape (`n_fac+n_hidden` instead
    of `n_fac`).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将尝试为`self.l_hidden(**h+inp**)`[[1:46:04](https://youtu.be/sHcLkfRrgoQ?t=1h46m4s)]尝试其他方法。原因是输入状态和隐藏状态在质上是不同的。输入是字符的编码，h是一系列字符的编码。因此，将它们相加，我们可能会丢失信息。让我们改为连接它们。不要忘记更改输入以匹配形状（`n_fac+n_hidden`而不是`n_fac`）。
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This gives some improvement.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这带来了一些改进。
- en: RNN with PyTorch [[1:48:47](https://youtu.be/sHcLkfRrgoQ?t=1h48m47s)]
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch的RNN[[1:48:47](https://youtu.be/sHcLkfRrgoQ?t=1h48m47s)]
- en: PyTorch will write the `for` loop automatically for us and also the linear input
    layer.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch将自动为我们编写`for`循环，还会编写线性输入层。
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: For reasons that will become apparent later on, `self.rnn` will return not only
    the output but also the hidden state.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出于以后会变得明显的原因，`self.rnn`将返回不仅输出，还有隐藏状态。
- en: The minor difference in PyTorch is that `self.rnn` will append a new hidden
    state to a tensor instead of replacing (in other words, it will give back all
    ellipses in the diagram) . We only want the final one so we do `outp[-1]`
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch中的一个微小差异是`self.rnn`会将一个新的隐藏状态附加到张量上，而不是替换（换句话说，它会在图表中返回所有省略号）。我们只想要最后一个，所以我们做`outp[-1]`
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In PyTorch version, a hidden state is rank 3 tensor `h = V(torch.zeros(1, bs,
    n_hidden))` (in our version, it was rank 2 tensor) [[1:51:58](https://youtu.be/sHcLkfRrgoQ?t=1h51m58s)].
    We will learn more about this later, but it turns out you can have a second RNN
    that goes backwards. The idea is that it is going to be better at finding relationships
    that go backwards — it is called “bi-directional RNN”. Also you can have an RNN
    feeds to an RNN which is called “multi layer RNN”. For these RNN’s, you will need
    the additional axis in the tensor to keep track of additional layers of hidden
    state. For now, we will just have 1 there, and get back 1.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch版本中，隐藏状态是一个秩为3的张量`h = V(torch.zeros(1, bs, n_hidden)`（在我们的版本中，它是秩为2的张量）[[1:51:58](https://youtu.be/sHcLkfRrgoQ?t=1h51m58s)]。我们以后会学到更多关于这个，但事实证明你可以有第二个向后运行的RNN。这个想法是它会更好地找到向后的关系——它被称为“双向RNN”。你也可以有一个RNN馈送到一个RNN，这被称为“多层RNN”。对于这些RNN，你将需要张量中的额外轴来跟踪额外层的隐藏状态。现在，我们只有1个，然后返回1个。
- en: Test the model
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试模型
- en: '[PRE39]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This time, we loop `n` times calling `get_next` each time, and each time we
    will replace our input by removing the first character and adding the character
    we just predicted.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们循环`n`次，每次调用`get_next`，每次我们将我们的输入替换为删除第一个字符并添加我们刚预测的字符。
- en: For an interesting homework, try writing your own `nn.RNN` “`JeremysRNN`” without
    looking at PyTorch source code.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有趣的作业，尝试编写自己的`nn.RNN`“`JeremysRNN`”，而不查看PyTorch源代码。
- en: Multi-output [[1:55:31](https://youtu.be/sHcLkfRrgoQ?t=1h55m31s)]
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多输出[[1:55:31](https://youtu.be/sHcLkfRrgoQ?t=1h55m31s)]
- en: From the last diagram, we can simplify even further by treating char 1 the same
    as char 2 to n-1\. You notice the triangle (the output) also moved inside of the
    loop, in other words, we create a prediction after each character.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 从最后一个图表中，我们可以进一步简化，将字符1视为字符2到n-1相同。你会注意到三角形（输出）也移动到循环内部，换句话说，我们在每个字符之后创建一个预测。
- en: Predicting chars 2 to n using chars 1 to n-1
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用字符1到n-1预测字符2到n
- en: 'One of the reasons we may want to do this is the redundancies we had seen before:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想要这样做的原因之一是我们之前看到的冗余：
- en: '[PRE40]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We can make it more efficient by taking **non-overlapping** sets of character
    this time. Because we are doing multi-output, for an input char 0 to 7, the output
    would be the predictions for char 1 to 8.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们可以通过采用**不重叠**的字符集来使其更有效。因为我们正在进行多输出，对于输入字符0到7，输出将是字符1到8的预测。
- en: '[PRE41]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This will not make our model any more accurate, but we can train it more efficiently.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这不会使我们的模型更准确，但我们可以更有效地训练它。
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Notice that we are no longer doing `outp[-1]` since we want to keep all of
    them. But everything else is identical. One complexity[[2:00:37](https://youtu.be/sHcLkfRrgoQ?t=2h37s)]
    is that we want to use the negative log-likelihood loss function as before, but
    it expects two rank 2 tensors (two mini-batches of vectors). But here, we have
    rank 3 tensor:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不再做`outp[-1]`，因为我们想保留所有这些。但其他一切都是相同的。一个复杂性[[2:00:37](https://youtu.be/sHcLkfRrgoQ?t=2h37s)]是我们想要像以前一样使用负对数似然损失函数，但它期望两个秩为2的张量（两个矢量的小批量）。但在这里，我们有秩为3的张量：
- en: 8 characters (time steps)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 8个字符（时间步）
- en: 84 probabilities
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 84个概率
- en: for 512 minibatch
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于512个小批量
- en: 'Let’s write a custom loss function [[2:02:10](https://youtu.be/sHcLkfRrgoQ?t=2h2m10s)]:'
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '让我们编写一个自定义损失函数[[2:02:10](https://youtu.be/sHcLkfRrgoQ?t=2h2m10s)]:'
- en: '[PRE43]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`F.nll_loss` is the PyTorch loss function.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F.nll_loss`是PyTorch的损失函数。'
- en: Flatten our inputs and targets.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展平我们的输入和目标。
- en: Transpose the first two axes because PyTorch expects 1\. sequence length (how
    many time steps), 2\. batch size, 3\. hidden state itself. `yt.size()` is 512
    by 8, whereas `sl, bs` is 8 by 512.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转置前两个轴，因为PyTorch期望1.序列长度（多少个时间步），2.批量大小，3.隐藏状态本身。`yt.size()`是512乘以8，而`sl, bs`是8乘以512。
- en: PyTorch does not generally actually shuffle the memory order when you do things
    like ‘transpose’, but instead it keeps some internal metadata to treat it as if
    it is transposed. When you transpose a matrix, PyTorch just updates the metadata
    . If you ever see an error that says “this tensor is not continuous” , add `.contiguous()`
    after it and error goes away.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你做像“transpose”这样的事情时，PyTorch通常不会实际洗牌内存顺序，而是保留一些内部元数据来处理它，就好像它被转置了。当你转置一个矩阵时，PyTorch只是更新元数据。如果你看到一个错误说“这个张量不连续”，在它后面加上`.contiguous()`，错误就会消失。
- en: '`.view` is same as `np.reshape`. `-1` indicates as long as it needs to be.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.view`与`np.reshape`相同。`-1`表示它需要多长。'
- en: '[PRE44]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Remember that `fit(...)` is the lowest level fast.ai abstraction that implements
    the training loop. So all the arguments are standard PyTorch things except for
    `md` which is our model data object which wraps up the test set, the training
    set, and the validation set.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 记住`fit(...)`是fast.ai实现训练循环的最低级抽象。因此，除了`md`是包装测试集、训练集和验证集的模型数据对象之外，所有参数都是标准的PyTorch东西。
- en: 'Question [[2:06:04](https://youtu.be/sHcLkfRrgoQ?t=2h6m4s)]: Now that we put
    a triangle inside of the loop, do we need a bigger sequence size?'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '问题[[2:06:04](https://youtu.be/sHcLkfRrgoQ?t=2h6m4s)]: 现在我们在循环内部放了一个三角形，我们需要更大的序列大小吗？'
- en: If we have a short sequence like 8, the first character has nothing to go on.
    It starts with an empty hidden state of zeros.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们有一个短序列像8这样，第一个字符没有任何依据。它从零开始的空隐藏状态。
- en: We will learn how to avoid that problem next week.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将学习如何避免这个问题下周。
- en: The basic idea is “why should we reset the hidden state to zeros every time?”
    (see code below). If we can line up these mini-batches somehow so that the next
    mini-batch joins up correctly representingthe next letter in Nietsche’s works,
    then we can move `h = V(torch.zeros(1, bs, n_hidden))` to the constructor.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本思想是“为什么我们每次都要将隐藏状态重置为零？”（见下面的代码）。如果我们可以以某种方式排列这些小批量，使得下一个小批量正确连接起来，代表尼采作品中的下一个字母，那么我们可以将`h
    = V(torch.zeros(1, bs, n_hidden))`移到构造函数中。
- en: '[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Gradient Explosion [[2:08:21](https://youtu.be/sHcLkfRrgoQ?t=2h8m21s)]
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度爆炸 [[2:08:21](https://youtu.be/sHcLkfRrgoQ?t=2h8m21s)]
- en: '`self.rnn(inp, h)` is a loop applying the same matrix multiply again and again.
    If that matrix multiply tends to increase the activations each time, we are effectively
    doing that to the power of 8 — we call this a gradient explosion. We want to make
    sure the initial `l_hidden` will not cause our activations on average to increase
    or decrease.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`self.rnn(inp, h)` 是一个循环，一遍又一遍地应用相同的矩阵乘法。如果那个矩阵乘法倾向于每次增加激活，我们实际上是将其乘以8次 — 我们称之为梯度爆炸。我们希望确保初始的`l_hidden`不会导致我们的激活平均增加或减少。'
- en: 'A nice matrix that does exactly that is called identity matrix:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的能做到这一点的矩阵被称为单位矩阵：
- en: 'We can overwrite the randomly initialized hidden-hidden weight with an identity
    matrix:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用单位矩阵覆盖随机初始化的隐藏-隐藏权重：
- en: '[PRE46]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This was introduced by Geoffrey Hinton et. al. in 2015 ([A Simple Way to Initialize
    Recurrent Networks of Rectified Linear Units](https://arxiv.org/abs/1504.00941))
    — after RNN has been around for decades. It works very well, and you can use higher
    learning rate since it is well behaved.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由Geoffrey Hinton等人在2015年介绍的（[一种初始化修正线性单元循环网络的简单方法](https://arxiv.org/abs/1504.00941)）
    — 在RNN存在几十年后。它效果非常好，你可以使用更高的学习率，因为它表现良好。
