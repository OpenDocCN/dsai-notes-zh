- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 19:40:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 19:40:16
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.00241] When Deep Learning Meets Polyhedral Theory: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.00241] å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2305.00241](https://ar5iv.labs.arxiv.org/html/2305.00241)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2305.00241](https://ar5iv.labs.arxiv.org/html/2305.00241)
- en: 'When Deep Learning Meets Polyhedral Theory: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°
- en: Joey Huchette
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Joey Huchette
- en: Google Research, USA â€ƒâ€ƒ Gonzalo MuÃ±oz
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è°·æ­Œç ”ç©¶ï¼Œç¾å›½ â€ƒâ€ƒ å†ˆè¨æ´›Â·ç©†å°¼å¥¥æ–¯
- en: Universidad de Oâ€™Higgins, Chile â€ƒâ€ƒ Thiago Serra
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¥¥ä¼Šé‡‘æ–¯å¤§å­¦ï¼Œæ™ºåˆ© â€ƒâ€ƒ è¿­æˆˆÂ·å¡æ‹‰
- en: Bucknell University, USA â€ƒâ€ƒ Calvin Tsay
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å·´å…‹å†…å°”å¤§å­¦ï¼Œç¾å›½ â€ƒâ€ƒ å¡å°”æ–‡Â·è”¡
- en: Imperial College London, UK(September 2023)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼¦æ•¦å¸å›½å­¦é™¢ï¼Œè‹±å›½ï¼ˆ2023å¹´9æœˆï¼‰
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In the past decade, deep learning became the prevalent methodology for predictive
    modeling thanks to the remarkable accuracy of deep neural networks in tasks such
    as computer vision and natural language processing. Meanwhile, the structure of
    neural networks converged back to simpler representations based on piecewise constant
    and piecewise linear functions such as the Rectified Linear UnitÂ (ReLU), which
    became the most commonly used type of activation function in neural networks.
    That made certain types of network structure â€”such as the typical fully-connected
    feedforward neural networkâ€” amenable to analysis through polyhedral theory and
    to the application of methodologies such as Linear ProgrammingÂ (LP) and Mixed-Integer
    Linear ProgrammingÂ (MILP) for a variety of purposes. In this paper, we survey
    the main topics emerging from this fast-paced area of work, which brings a fresh
    perspective to understanding neural networks in more detail as well as to applying
    linear optimization techniques to train, verify, and reduce the size of such networks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿‡å»çš„åå¹´é‡Œï¼Œæ·±åº¦å­¦ä¹ ç”±äºæ·±åº¦ç¥ç»ç½‘ç»œåœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä»»åŠ¡ä¸­çš„æ˜¾è‘—å‡†ç¡®æ€§ï¼Œæˆä¸ºé¢„æµ‹å»ºæ¨¡çš„ä¸»è¦æ–¹æ³•ã€‚åŒæ—¶ï¼Œç¥ç»ç½‘ç»œçš„ç»“æ„å›å½’åˆ°åŸºäºåˆ†æ®µå¸¸æ•°å’Œåˆ†æ®µçº¿æ€§å‡½æ•°çš„ç®€å•è¡¨ç¤ºï¼Œä¾‹å¦‚æ•´æµçº¿æ€§å•å…ƒï¼ˆReLUï¼‰ï¼Œè¿™æˆä¸ºç¥ç»ç½‘ç»œä¸­æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚è¿™ä½¿å¾—æŸäº›ç±»å‹çš„ç½‘ç»œç»“æ„â€”â€”ä¾‹å¦‚å…¸å‹çš„å…¨è¿æ¥å‰é¦ˆç¥ç»ç½‘ç»œâ€”â€”èƒ½å¤Ÿé€šè¿‡å¤šé¢ä½“ç†è®ºè¿›è¡Œåˆ†æï¼Œå¹¶åº”ç”¨çº¿æ€§è§„åˆ’ï¼ˆLPï¼‰å’Œæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆMILPï¼‰ç­‰æ–¹æ³•ç”¨äºå„ç§ç›®çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†è¿™ä¸€å¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­å‡ºç°çš„ä¸»è¦ä¸»é¢˜ï¼Œè¿™ä¸ºæ›´è¯¦ç»†åœ°ç†è§£ç¥ç»ç½‘ç»œä»¥åŠåº”ç”¨çº¿æ€§ä¼˜åŒ–æŠ€æœ¯æ¥è®­ç»ƒã€éªŒè¯å’Œç¼©å‡è¿™äº›ç½‘ç»œçš„è§„æ¨¡æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: 'Deep learning has continuously achieved new landmarks in varied areas of artificial
    intelligence for the past decade. Examples of those areas include predictive tasks
    in computer vision (Krizhevsky etÂ al., [2012](#bib.bib178), Ciresan etÂ al., [2012](#bib.bib60),
    Szegedy etÂ al., [2015](#bib.bib301), He etÂ al., [2016](#bib.bib144), Xie etÂ al.,
    [2020b](#bib.bib343)), natural language processing (Sutskever etÂ al., [2014](#bib.bib299),
    Peters etÂ al., [2018](#bib.bib246), Radford etÂ al., [2018](#bib.bib252), Devlin
    etÂ al., [2019](#bib.bib80)), and speech recognition (Hinton etÂ al., [2012](#bib.bib148),
    Graves and Jaitly, [2014](#bib.bib130), Park etÂ al., [2019](#bib.bib240)). The
    artificial neural networks behind such feats are being used in many applications,
    and there is a growing interest for analytical insights to help design such networks
    and then to leverage the model that they have learned. For the most commonly used
    types of neural networks, some of those results and methods are coming from operations
    research tools such as polyhedral theory and associated optimization techniques
    such as Linear ProgrammingÂ (LP) and Mixed-Integer Linear ProgrammingÂ (MILP). Among
    other things, these connections with mathematical optimization may help us understand
    what neural networks can represent, how to train them, and how to make them more
    compact. For example, consider the popular task of classifying images (Figure
    [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")); polyhedral theory and associated optimization techniques may help
    us answer questions such as the following. How should we train the classifier
    model? How large should it be? How robust to perturbations is it?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨è¿‡å»åå¹´é‡Œï¼Œæ·±åº¦å­¦ä¹ åœ¨äººå·¥æ™ºèƒ½çš„å„ä¸ªé¢†åŸŸä¸æ–­å–å¾—æ–°è¿›å±•ã€‚è¿™äº›é¢†åŸŸçš„ä¾‹å­åŒ…æ‹¬è®¡ç®—æœºè§†è§‰ä¸­çš„é¢„æµ‹ä»»åŠ¡ï¼ˆKrizhevsky ç­‰ï¼Œ[2012](#bib.bib178)ï¼ŒCiresan
    ç­‰ï¼Œ[2012](#bib.bib60)ï¼ŒSzegedy ç­‰ï¼Œ[2015](#bib.bib301)ï¼ŒHe ç­‰ï¼Œ[2016](#bib.bib144)ï¼ŒXie
    ç­‰ï¼Œ[2020b](#bib.bib343)ï¼‰ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆSutskever ç­‰ï¼Œ[2014](#bib.bib299)ï¼ŒPeters ç­‰ï¼Œ[2018](#bib.bib246)ï¼ŒRadford
    ç­‰ï¼Œ[2018](#bib.bib252)ï¼ŒDevlin ç­‰ï¼Œ[2019](#bib.bib80)ï¼‰ï¼Œä»¥åŠè¯­éŸ³è¯†åˆ«ï¼ˆHinton ç­‰ï¼Œ[2012](#bib.bib148)ï¼ŒGraves
    å’Œ Jaitlyï¼Œ[2014](#bib.bib130)ï¼ŒPark ç­‰ï¼Œ[2019](#bib.bib240)ï¼‰ã€‚è¿™äº›æˆå°±èƒŒåçš„äººå·¥ç¥ç»ç½‘ç»œè¢«å¹¿æ³›åº”ç”¨ï¼Œå¹¶ä¸”å¯¹åˆ†ææ´å¯Ÿçš„å…´è¶£ä¸æ–­å¢é•¿ï¼Œä»¥å¸®åŠ©è®¾è®¡è¿™äº›ç½‘ç»œå¹¶åˆ©ç”¨å®ƒä»¬æ‰€å­¦åˆ°çš„æ¨¡å‹ã€‚å¯¹äºæœ€å¸¸ç”¨çš„ç¥ç»ç½‘ç»œç±»å‹ï¼Œè¿™äº›ç»“æœå’Œæ–¹æ³•æœ‰äº›æ¥æºäºè¿ç­¹å­¦å·¥å…·ï¼Œå¦‚å¤šé¢ä½“ç†è®ºåŠç›¸å…³çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚çº¿æ€§è§„åˆ’ï¼ˆLPï¼‰å’Œæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆMILPï¼‰ã€‚è¿™äº›ä¸æ•°å­¦ä¼˜åŒ–çš„è”ç³»å¯èƒ½å¸®åŠ©æˆ‘ä»¬æ·±å…¥äº†è§£ç¥ç»ç½‘ç»œå¯ä»¥è¡¨ç¤ºçš„å†…å®¹ã€å¦‚ä½•è®­ç»ƒå®ƒä»¬ä»¥åŠå¦‚ä½•ä½¿å…¶æ›´åŠ ç´§å‡‘ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘åˆ†ç±»å›¾åƒè¿™ä¸€å¸¸è§ä»»åŠ¡ï¼ˆå›¾
    [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey)ï¼‰ï¼›å¤šé¢ä½“ç†è®ºåŠç›¸å…³çš„ä¼˜åŒ–æŠ€æœ¯å¯èƒ½å¸®åŠ©æˆ‘ä»¬è§£ç­”ä»¥ä¸‹é—®é¢˜ã€‚æˆ‘ä»¬åº”è¯¥å¦‚ä½•è®­ç»ƒåˆ†ç±»å™¨æ¨¡å‹ï¼Ÿå®ƒåº”è¯¥æœ‰å¤šå¤§ï¼Ÿå¯¹æ‰°åŠ¨çš„é²æ£’æ€§å¦‚ä½•ï¼Ÿ'
- en: '![Refer to caption](img/17c4f1fa42e56b26a18a8a37fef55a33.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/17c4f1fa42e56b26a18a8a37fef55a33.png)'
- en: 'Figure 1: Example classification task on the MNIST database of handwritten
    digits, in which the image of a handwritten digit is given as input and the probability
    of that digit being from each possible class is provided as output.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šMNIST æ‰‹å†™æ•°å­—æ•°æ®åº“ä¸­çš„åˆ†ç±»ä»»åŠ¡ç¤ºä¾‹ï¼Œå…¶ä¸­è¾“å…¥ä¸ºæ‰‹å†™æ•°å­—çš„å›¾åƒï¼Œè¾“å‡ºä¸ºè¯¥æ•°å­—å±äºæ¯ä¸ªå¯èƒ½ç±»åˆ«çš„æ¦‚ç‡ã€‚
- en: 1.1 What neural networks can model
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 ç¥ç»ç½‘ç»œå¯ä»¥å»ºæ¨¡çš„å†…å®¹
- en: 'We can essentially think of artificial neural networks as functions mapping
    an input ${\bm{x}}$ from a given domain to an output ${\bm{y}}$ for a given application.
    For the classification task in Figure [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£
    When Deep Learning Meets Polyhedral Theory: A Survey"), inputs ${\bm{x}}$ correspond
    to images from the dataset, and ${\bm{y}}$ to the associated predicted labels,
    or probabilities for labels describing the content of those images. The basic
    units of neural networks mimic biological neurons in that they receive inputs
    from adjacent units, transform those inputs, and may produce an output to subsequent
    units of the network. In other words, every unit is also a function, and in fact
    the output of most units is defined by the composition of a nonlinear function
    with a linear function. The nonlinear function is often denoted as the *activation
    function* in analogy to how a biological neuron is triggered to send a signal
    to adjacent neurons when the stimulus caused by the input exceeds a certain activation
    threshold. Such non-linearity is behind the remarkable expressiveness of neural
    networks.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åŸºæœ¬ä¸Šå°†äººå·¥ç¥ç»ç½‘ç»œè§†ä¸ºå°†è¾“å…¥${\bm{x}}$ä»ç»™å®šé¢†åŸŸæ˜ å°„åˆ°è¾“å‡º${\bm{y}}$çš„å‡½æ•°ï¼Œç”¨äºç‰¹å®šåº”ç”¨ã€‚åœ¨å›¾[1](#S1.F1 "å›¾
    1 â€£ 1 ä»‹ç» â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºçš„ç»“åˆ")ä¸­çš„åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¾“å…¥${\bm{x}}$å¯¹åº”äºæ•°æ®é›†ä¸­çš„å›¾åƒï¼Œè€Œ${\bm{y}}$å¯¹åº”äºä¸è¿™äº›å›¾åƒå†…å®¹ç›¸å…³çš„é¢„æµ‹æ ‡ç­¾æˆ–æ ‡ç­¾æ¦‚ç‡ã€‚ç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒï¼Œå› ä¸ºå®ƒä»¬æ¥æ”¶æ¥è‡ªç›¸é‚»å•å…ƒçš„è¾“å…¥ï¼Œè½¬æ¢è¿™äº›è¾“å…¥ï¼Œå¹¶å¯èƒ½å°†è¾“å‡ºä¼ é€’ç»™ç½‘ç»œçš„åç»­å•å…ƒã€‚æ¢å¥è¯è¯´ï¼Œæ¯ä¸ªå•å…ƒä¹Ÿæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œäº‹å®ä¸Šï¼Œå¤§å¤šæ•°å•å…ƒçš„è¾“å‡ºæ˜¯é€šè¿‡å°†éçº¿æ€§å‡½æ•°ä¸çº¿æ€§å‡½æ•°ç»„åˆæ¥å®šä¹‰çš„ã€‚éçº¿æ€§å‡½æ•°é€šå¸¸è¢«ç§°ä¸º*æ¿€æ´»å‡½æ•°*ï¼Œç±»ä¼¼äºç”Ÿç‰©ç¥ç»å…ƒåœ¨è¾“å…¥çš„åˆºæ¿€è¶…è¿‡æŸä¸€æ¿€æ´»é˜ˆå€¼æ—¶è¢«è§¦å‘ä»¥å‘ç›¸é‚»ç¥ç»å…ƒå‘é€ä¿¡å·çš„æ–¹å¼ã€‚è¿™ç§éçº¿æ€§æ˜¯ç¥ç»ç½‘ç»œå“è¶Šè¡¨è¾¾èƒ½åŠ›çš„åŸºç¡€ã€‚
- en: This model was pioneered byÂ McCulloch and Pitts ([1943](#bib.bib217)), who considered
    a thresholding function for activation that is now often denoted as the Linear
    Threshold UnitÂ (LTU). That activation is also the basis of the classic *perceptron*
    algorithm by Â Rosenblatt ([1957](#bib.bib261)), which yields a binary classifier
    of the form
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€æ¨¡å‹ç”±éº¦å¡æ´›å…‹å’Œçš®èŒ¨ï¼ˆ[1943](#bib.bib217)ï¼‰é¦–æ¬¡æå‡ºï¼Œä»–ä»¬è€ƒè™‘äº†ä¸€ç§æ¿€æ´»çš„é˜ˆå€¼å‡½æ•°ï¼Œç°åœ¨é€šå¸¸ç§°ä¸ºçº¿æ€§é˜ˆå€¼å•å…ƒï¼ˆLTUï¼‰ã€‚è¯¥æ¿€æ´»å‡½æ•°ä¹Ÿæ˜¯ç»å…¸çš„*æ„ŸçŸ¥å™¨*ç®—æ³•çš„åŸºç¡€ï¼Œç”±ç½—æ£®å¸ƒæ‹‰ç‰¹ï¼ˆ[1957](#bib.bib261)ï¼‰æå‡ºï¼Œè¯¥ç®—æ³•ç”Ÿæˆä¸€ä¸ªå½¢å¼ä¸ºäºŒåˆ†ç±»å™¨
- en: '|  | $f({\bm{x}})=\left\{\begin{array}[]{cl}1&amp;\text{if }{\bm{w}}\cdot{\bm{x}}+b>0;\\
    0&amp;\text{otherwise}\end{array}\right.$ |  | (1) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | $f({\bm{x}})=\left\{\begin{array}[]{cl}1&amp;\text{å¦‚æœ }{\bm{w}}\cdot{\bm{x}}+b>0;\\
    0&amp;\text{å¦åˆ™}\end{array}\right.$ |  | (1) |'
- en: 'for an input ${\bm{x}}\in\mathbb{R}^{n_{0}}$ and with parameters ${\bm{w}}\in\mathbb{R}^{n_{0}}$
    and $b\in\mathbb{R}$. Those parameters are chosen by optimizing the predictions
    for a given task, as discussed below and in Section [5](#S5 "5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey"). The term *single-layer perceptron* is used for a neural network consisting
    of a set of such units processing the same input in parallel. The term *multi-layer
    perceptron* is used for a generalization of this concept, by which the output
    of a *layer* â€”a set of units with same inputâ€” is the input for a subsequent layer.
    This perceptron terminology has also been loosely applied to neural networks with
    other activation functions.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¾“å…¥${\bm{x}}\in\mathbb{R}^{n_{0}}$ï¼Œä»¥åŠå‚æ•°${\bm{w}}\in\mathbb{R}^{n_{0}}$å’Œ$b\in\mathbb{R}$ã€‚è¿™äº›å‚æ•°æ˜¯é€šè¿‡ä¼˜åŒ–ç»™å®šä»»åŠ¡çš„é¢„æµ‹æ¥é€‰æ‹©çš„ï¼Œå¦‚ä¸‹æ–‡å’Œç¬¬[5](#S5
    "5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºçš„ç»“åˆï¼šè°ƒæŸ¥")èŠ‚æ‰€è®¨è®ºçš„ã€‚*å•å±‚æ„ŸçŸ¥å™¨*è¿™ä¸€æœ¯è¯­ç”¨äºæè¿°ç”±ä¸€ç»„å¹¶è¡Œå¤„ç†ç›¸åŒè¾“å…¥çš„å•å…ƒç»„æˆçš„ç¥ç»ç½‘ç»œã€‚*å¤šå±‚æ„ŸçŸ¥å™¨*è¿™ä¸€æœ¯è¯­ç”¨äºæè¿°è¯¥æ¦‚å¿µçš„æ¨å¹¿ï¼Œå…¶ä¸­ä¸€ä¸ª*å±‚*â€”â€”ä¸€ç»„å…·æœ‰ç›¸åŒè¾“å…¥çš„å•å…ƒâ€”â€”çš„è¾“å‡ºæ˜¯åç»­å±‚çš„è¾“å…¥ã€‚è¿™ç§æ„ŸçŸ¥å™¨æœ¯è¯­ä¹Ÿè¢«å®½æ³›åœ°åº”ç”¨äºå…·æœ‰å…¶ä»–æ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œã€‚
- en: More generally, neural networks that successively transform inputs through an
    ordered sequence of layers are also denoted *feedforward networks*. The layers
    that do not produce the final output of the neural network are denoted *hidden
    layers*. For a network with $L$ layers, we denote $n_{l}$ as the number of units
    â€”or *width*â€” of layer $l\in{\mathbb{L}}:=\{1,2,\ldots,L\}$ and $h_{i}^{l}$ as
    the output of the $i$-th unit in layer $l$, where $i\in\{1,2,\ldots,n_{l}\}$.
    The output of a unit is given by
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ä¸€èˆ¬åœ°è¯´ï¼Œé€šè¿‡ä¸€ç³»åˆ—æœ‰åºå±‚ä¾æ¬¡è½¬æ¢è¾“å…¥çš„ç¥ç»ç½‘ç»œä¹Ÿç§°ä¸º*å‰é¦ˆç½‘ç»œ*ã€‚é‚£äº›ä¸äº§ç”Ÿç¥ç»ç½‘ç»œæœ€ç»ˆè¾“å‡ºçš„å±‚ç§°ä¸º*éšå±‚*ã€‚å¯¹äºä¸€ä¸ªæœ‰$L$å±‚çš„ç½‘ç»œï¼Œæˆ‘ä»¬å°†$n_{l}$è¡¨ç¤ºä¸ºç¬¬$l$å±‚çš„å•å…ƒæ•°é‡â€”â€”æˆ–*å®½åº¦*â€”â€”$l\in{\mathbb{L}}:=\{1,2,\ldots,L\}$ï¼Œ$h_{i}^{l}$è¡¨ç¤ºç¬¬$l$å±‚ä¸­ç¬¬$i$ä¸ªå•å…ƒçš„è¾“å‡ºï¼Œå…¶ä¸­$i\in\{1,2,\ldots,n_{l}\}$ã€‚ä¸€ä¸ªå•å…ƒçš„è¾“å‡ºç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼š
- en: '|  | $h_{i}^{l}=\sigma^{l}\left({\bm{w}}^{l}_{i}\cdot{\bm{h}}^{l-1}+b^{l}_{i}\right),$
    |  | (2) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{i}^{l}=\sigma^{l}\left({\bm{w}}^{l}_{i}\cdot{\bm{h}}^{l-1}+b^{l}_{i}\right),$
    |  | (2) |'
- en: 'where the *weights* ${\bm{w}}^{l}_{i}\in\mathbb{R}^{n_{l-1}}$ and the *bias*
    $b^{l}_{i}\in\mathbb{R}$ are parameters of the unit. Those parameters can be aggregated
    across the layer as the matrix ${\bm{W}}^{l}\in\mathbb{R}^{n_{l}\times n_{l-1}}$
    and the vector ${\bm{b}}^{l}\in\mathbb{R}^{n_{l}}$. The vector ${\bm{h}}^{l-1}\in\mathbb{R}^{n_{l-1}}$
    represents the aggregated outputs from layer $(l-1)$. The activation function
    $\sigma^{l}:\mathbb{R}\rightarrow\mathbb{R}$ is applied by the units in layer
    $l$. These definitions implicitly assume that $n_{0}$ is the size of the network
    input ${\bm{x}}\in\mathbb{R}^{n_{0}}$ and that ${\bm{h}}^{0}$ and ${\bm{x}}$ are
    the same. FigureÂ [2](#S1.F2 "Figure 2 â€£ 1.1 What neural networks can model â€£ 1
    Introduction â€£ When Deep Learning Meets Polyhedral Theory: A Survey") illustrates
    the operation of a feedforward network as described above.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ï¼Œ*æƒé‡* ${\bm{w}}^{l}_{i}\in\mathbb{R}^{n_{l-1}}$ å’Œ *åç½®* $b^{l}_{i}\in\mathbb{R}$
    æ˜¯è¯¥å•å…ƒçš„å‚æ•°ã€‚è¿™äº›å‚æ•°å¯ä»¥åœ¨å±‚å†…æ±‡æ€»ä¸ºçŸ©é˜µ ${\bm{W}}^{l}\in\mathbb{R}^{n_{l}\times n_{l-1}}$ å’Œå‘é‡ ${\bm{b}}^{l}\in\mathbb{R}^{n_{l}}$ã€‚å‘é‡
    ${\bm{h}}^{l-1}\in\mathbb{R}^{n_{l-1}}$ è¡¨ç¤ºæ¥è‡ªå±‚ $(l-1)$ çš„æ±‡æ€»è¾“å‡ºã€‚æ¿€æ´»å‡½æ•° $\sigma^{l}:\mathbb{R}\rightarrow\mathbb{R}$
    ç”±ç¬¬ $l$ å±‚çš„å•å…ƒåº”ç”¨ã€‚è¿™äº›å®šä¹‰éšå«åœ°å‡è®¾ $n_{0}$ æ˜¯ç½‘ç»œè¾“å…¥ ${\bm{x}}\in\mathbb{R}^{n_{0}}$ çš„å¤§å°ï¼Œå¹¶ä¸” ${\bm{h}}^{0}$
    å’Œ ${\bm{x}}$ æ˜¯ç›¸åŒçš„ã€‚å›¾Â [2](#S1.F2 "Figure 2 â€£ 1.1 What neural networks can model
    â€£ 1 Introduction â€£ When Deep Learning Meets Polyhedral Theory: A Survey") è¯´æ˜äº†ä¸Šè¿°å‰é¦ˆç½‘ç»œçš„æ“ä½œã€‚'
- en: '![Refer to caption](img/2db8fd387b48da880f36dff11bad8a69.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/2db8fd387b48da880f36dff11bad8a69.png)'
- en: 'Figure 2: Mapping from ${\bm{x}}\in\mathbb{R}^{n_{0}}$ to ${\bm{y}}\in\mathbb{R}^{n_{L}}$
    through a feedforward neural network with $L$ layers, layer widths $\{n_{l}\}_{l\in{\mathbb{L}}}$,
    and activation functions $\{\sigma_{l}\}_{l\in{\mathbb{L}}}$.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 2: é€šè¿‡å…·æœ‰ $L$ å±‚ã€å±‚å®½åº¦ $\{n_{l}\}_{l\in{\mathbb{L}}}$ å’Œæ¿€æ´»å‡½æ•° $\{\sigma_{l}\}_{l\in{\mathbb{L}}}$
    çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå°† ${\bm{x}}\in\mathbb{R}^{n_{0}}$ æ˜ å°„åˆ° ${\bm{y}}\in\mathbb{R}^{n_{L}}$ã€‚'
- en: 1.2 How neural networks are obtained and evaluated
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 ç¥ç»ç½‘ç»œçš„è·å–ä¸è¯„ä¼°
- en: In resemblance to how other models for *supervised learning* in machine learning
    are obtained, we can *train* a neural network for a given task by adjusting its
    behavior with respect to the examples of a *training set* and then evaluate the
    final trained network on a *test set*. Both of these sets consist of inputs for
    which the correct output $\hat{y}$ is known. We can define an objective function
    to model a measure of distance between the output $y$ and the correct output $\hat{y}$,
    which is typically denoted as the *loss function*, and then iteratively update
    parameters such as $\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}}$ and $\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}$
    to minimize that loss function over the training set. A common objective function
    is the square error $\|y-\hat{y}\|^{2}$ summed over the points in the training
    set. The test set contains a separate collection of inputs and their outputs,
    which is used to evaluate the trained neural network with examples that were not
    seen during training. A good performance on the test set may indicate that the
    trained neural network is able to *generalize* beyond the seen examples, whereas
    a bad performance may suggest that it *overfits* for the training set. Neural
    networks also have *hyperparameters* that are often chosen manually and do not
    change during training, such as the *depth* $L$, the widths of the layers $\{n_{l}\}_{l\in{\mathbb{L}}}$,
    and the activation functions used in each layer $\{\sigma^{l}\}_{l\in{\mathbb{L}}}$.
    Different models can be produced by varying the hyperparameters. In such a case,
    a *validation set* disjoint from the training and test sets can be used to compare
    models with different hyperparameters. Whereas the validation set may serve as
    a benchmark to different trained models corresponding to different choices of
    hyperparameters, the test set can only be used to evaluate a single neural network
    chosen among those evaluated with the validation set. The emergent field of *neural
    architecture search* â€”recently surveyed by Elsken etÂ al. ([2019](#bib.bib89))â€”
    concerns with automatically choosing such hyperparameters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æœºå™¨å­¦ä¹ ä¸­å…¶ä»–*ç›‘ç£å­¦ä¹ *æ¨¡å‹çš„è·å–æ–¹å¼ç±»ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´ç¥ç»ç½‘ç»œçš„è¡Œä¸ºä»¥é€‚åº”*è®­ç»ƒé›†*çš„ä¾‹å­æ¥*è®­ç»ƒ*ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œç„¶ååœ¨*æµ‹è¯•é›†*ä¸Šè¯„ä¼°æœ€ç»ˆè®­ç»ƒå¥½çš„ç½‘ç»œã€‚è¿™ä¸¤ä¸ªæ•°æ®é›†éƒ½åŒ…å«äº†å·²çŸ¥æ­£ç¡®è¾“å‡º$\hat{y}$çš„è¾“å…¥ã€‚æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç›®æ ‡å‡½æ•°æ¥è¡¡é‡è¾“å‡º$y$å’Œæ­£ç¡®è¾“å‡º$\hat{y}$ä¹‹é—´çš„è·ç¦»ï¼Œè¿™é€šå¸¸è¢«ç§°ä¸º*æŸå¤±å‡½æ•°*ï¼Œç„¶åè¿­ä»£æ›´æ–°å‚æ•°ï¼Œå¦‚$\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}}$å’Œ$\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}$ï¼Œä»¥æœ€å°åŒ–è®­ç»ƒé›†ä¸Šçš„æŸå¤±å‡½æ•°ã€‚ä¸€ä¸ªå¸¸è§çš„ç›®æ ‡å‡½æ•°æ˜¯å¹³æ–¹è¯¯å·®$\|y-\hat{y}\|^{2}$ï¼Œå®ƒåœ¨è®­ç»ƒé›†ä¸­çš„æ‰€æœ‰ç‚¹ä¸Šè¿›è¡Œæ±‚å’Œã€‚æµ‹è¯•é›†åŒ…å«ä¸€ç»„ç‹¬ç«‹çš„è¾“å…¥åŠå…¶è¾“å‡ºï¼Œç”¨äºè¯„ä¼°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§è¿‡çš„ä¾‹å­çš„è®­ç»ƒç¥ç»ç½‘ç»œã€‚æµ‹è¯•é›†ä¸Šçš„è‰¯å¥½è¡¨ç°å¯èƒ½è¡¨æ˜è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œèƒ½å¤Ÿ*æ³›åŒ–*è¶…è¶Šå·²è§çš„ä¾‹å­ï¼Œè€Œå·®çš„è¡¨ç°å¯èƒ½è¡¨æ˜å®ƒå¯¹è®­ç»ƒé›†*è¿‡æ‹Ÿåˆ*ã€‚ç¥ç»ç½‘ç»œè¿˜å…·æœ‰*è¶…å‚æ•°*ï¼Œè¿™äº›è¶…å‚æ•°é€šå¸¸æ˜¯æ‰‹åŠ¨é€‰æ‹©çš„ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šæ”¹å˜ï¼Œä¾‹å¦‚*æ·±åº¦*
    $L$ã€å±‚çš„å®½åº¦$\{n_{l}\}_{l\in{\mathbb{L}}}$å’Œæ¯å±‚ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°$\{\sigma^{l}\}_{l\in{\mathbb{L}}}$ã€‚é€šè¿‡æ”¹å˜è¶…å‚æ•°å¯ä»¥äº§ç”Ÿä¸åŒçš„æ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨ä¸è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸åŒçš„*éªŒè¯é›†*æ¥æ¯”è¾ƒå…·æœ‰ä¸åŒè¶…å‚æ•°çš„æ¨¡å‹ã€‚è™½ç„¶éªŒè¯é›†å¯ä»¥ä½œä¸ºä¸åŒè®­ç»ƒæ¨¡å‹çš„åŸºå‡†ï¼Œè¿™äº›æ¨¡å‹å¯¹åº”äºä¸åŒçš„è¶…å‚æ•°é€‰æ‹©ï¼Œä½†æµ‹è¯•é›†åªèƒ½ç”¨äºè¯„ä¼°åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°çš„é‚£äº›ç¥ç»ç½‘ç»œä¸­çš„ä¸€ä¸ªã€‚æ–°å…´é¢†åŸŸ*ç¥ç»æ¶æ„æœç´¢*â€”â€”æœ€è¿‘ç”±Elskenç­‰äºº([2019](#bib.bib89))è¿›è¡Œçš„è°ƒç ”â€”â€”æ¶‰åŠè‡ªåŠ¨é€‰æ‹©è¿™äº›è¶…å‚æ•°ã€‚
- en: One of the key factors for the success of deep learning is that first-order
    methods for continuous optimization can be effectively applied to train deep networks.
    The interest in neural networks first vanished due to negative results in the
    Perceptrons book byÂ Minsky and Papert ([1969](#bib.bib219)), which showed that
    single-layer perceptrons cannot represent functions such as the Boolean XOR. However,
    moving to multi-layer perceptrons capable of expressing the Boolean XOR as well
    as other more expressive models would require a clever training strategy. Hence,
    the interest was regained with papers that popularized the use of *backpropagation*,
    such as Â Rumelhart etÂ al. ([1986](#bib.bib266)) and Â LeCun etÂ al. ([1989](#bib.bib185)).
    Note that backpropagation was first discussed much earlier in the context of networks
    byÂ Linnainmaa ([1970](#bib.bib194)) and of neural networks explicitly byÂ Werbos
    ([1974](#bib.bib331)). The backpropagation algorithm calculates the derivative
    of the loss function with respect to each neural network parameter by applying
    the chain rule through the units of the neural network, which is considerably
    more efficient than explicitly evaluating the derivative of each network parameter.
    Consequently, neural networks are generally trained with gradient descent methods
    in which the parameters are updated sequentially from the output to the input
    layer in each step. In fact, most algorithms for training neural networks are
    based on Stochastic Gradient DescentÂ (SGD), which is a form of the stochastic
    approximation through sampling pioneered byÂ Robbins and Monro ([1951](#bib.bib258)).
    SGD approximates the partial derivatives of the loss function at each step by
    using only a subset of the data in order to make the training process more efficient.
    Examples of popular SGD algorithms include momentum (Polyak, [1964](#bib.bib250)),
    Adam (Kingma and Ba, [2014](#bib.bib175)), and Nesterov Adaptive Gradient (Sutskever
    etÂ al., [2013](#bib.bib298)) â€”the later inspired byÂ Nesterov ([1983](#bib.bib231)).
    Interestingly, however, we generally cannot guarantee convergence to a global
    optimum with gradient descent due to the nonconvexity of the loss function. Nevertheless,
    neural networks trained with adequately parameterized SGD algorithms tend to generalize
    well.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ æˆåŠŸçš„ä¸€ä¸ªå…³é”®å› ç´ æ˜¯å¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨ä¸€é˜¶æ–¹æ³•è¿›è¡Œè¿ç»­ä¼˜åŒ–ï¼Œä»è€Œè®­ç»ƒæ·±åº¦ç½‘ç»œã€‚å¯¹ç¥ç»ç½‘ç»œçš„å…´è¶£æœ€åˆå› **Minsky**å’Œ**Papert**åœ¨ã€Šæ„ŸçŸ¥æœºã€‹ä¸€ä¹¦ä¸­çš„è´Ÿé¢ç»“æœè€Œæ¶ˆå¤±ï¼ˆ[1969](#bib.bib219)ï¼‰ï¼Œä¹¦ä¸­è¡¨æ˜å•å±‚æ„ŸçŸ¥æœºæ— æ³•è¡¨ç¤ºåƒå¸ƒå°”å¼‚æˆ–è¿™æ ·çš„å‡½æ•°ã€‚ç„¶è€Œï¼Œè½¬å‘èƒ½å¤Ÿè¡¨ç¤ºå¸ƒå°”å¼‚æˆ–ä»¥åŠå…¶ä»–æ›´å…·è¡¨ç°åŠ›çš„æ¨¡å‹çš„å¤šå±‚æ„ŸçŸ¥æœºåˆ™éœ€è¦å·§å¦™çš„è®­ç»ƒç­–ç•¥ã€‚å› æ­¤ï¼Œéšç€**Rumelhart**ç­‰äººï¼ˆ[1986](#bib.bib266)ï¼‰å’Œ**LeCun**ç­‰äººï¼ˆ[1989](#bib.bib185)ï¼‰ç­‰è®ºæ–‡æ¨å¹¿äº†*åå‘ä¼ æ’­*çš„ä½¿ç”¨ï¼Œå…´è¶£å¾—ä»¥æ¢å¤ã€‚è¯·æ³¨æ„ï¼Œåå‘ä¼ æ’­ç®—æ³•æœ€æ—©ç”±**Linnainmaa**ï¼ˆ[1970](#bib.bib194)ï¼‰åœ¨ç½‘ç»œä¸Šä¸‹æ–‡ä¸­è®¨è®ºè¿‡ï¼Œè€Œ**Werbos**ï¼ˆ[1974](#bib.bib331)ï¼‰åˆ™æ˜ç¡®è®¨è®ºäº†ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­ã€‚åå‘ä¼ æ’­ç®—æ³•é€šè¿‡åœ¨ç¥ç»ç½‘ç»œçš„å„ä¸ªå•å…ƒä¸­åº”ç”¨é“¾å¼æ³•åˆ™æ¥è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¯ä¸ªç¥ç»ç½‘ç»œå‚æ•°çš„å¯¼æ•°ï¼Œè¿™æ¯”æ˜¾å¼è®¡ç®—æ¯ä¸ªç½‘ç»œå‚æ•°çš„å¯¼æ•°è¦é«˜æ•ˆå¾—å¤šã€‚å› æ­¤ï¼Œç¥ç»ç½‘ç»œé€šå¸¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­å‚æ•°åœ¨æ¯ä¸€æ­¥ä¸­ä»è¾“å‡ºå±‚åˆ°è¾“å…¥å±‚é€æ­¥æ›´æ–°ã€‚äº‹å®ä¸Šï¼Œå¤§å¤šæ•°ç¥ç»ç½‘ç»œè®­ç»ƒç®—æ³•åŸºäºéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”±**Robbins**å’Œ**Monro**ï¼ˆ[1951](#bib.bib258)ï¼‰å¼€åˆ›çš„é€šè¿‡é‡‡æ ·è¿›è¡Œçš„éšæœºè¿‘ä¼¼å½¢å¼ã€‚SGDé€šè¿‡ä»…ä½¿ç”¨æ•°æ®å­é›†æ¥é€¼è¿‘æŸå¤±å‡½æ•°çš„åå¯¼æ•°ï¼Œä»¥æé«˜è®­ç»ƒè¿‡ç¨‹çš„æ•ˆç‡ã€‚æµè¡Œçš„SGDç®—æ³•åŒ…æ‹¬åŠ¨é‡ï¼ˆ**Polyak**ï¼Œ[1964](#bib.bib250)ï¼‰ã€Adamï¼ˆ**Kingma**å’Œ**Ba**ï¼Œ[2014](#bib.bib175)ï¼‰å’ŒNesterovè‡ªé€‚åº”æ¢¯åº¦ï¼ˆ**Sutskever**ç­‰äººï¼Œ[2013](#bib.bib298)ï¼‰â€”â€”åè€…å—åˆ°**Nesterov**ï¼ˆ[1983](#bib.bib231)ï¼‰çš„å¯å‘ã€‚æœ‰è¶£çš„æ˜¯ï¼Œç”±äºæŸå¤±å‡½æ•°çš„éå‡¸æ€§ï¼Œæˆ‘ä»¬é€šå¸¸æ— æ³•ä¿è¯æ¢¯åº¦ä¸‹é™æ³•æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚ç„¶è€Œï¼Œä½¿ç”¨é€‚å½“å‚æ•°åŒ–çš„SGDç®—æ³•è®­ç»ƒçš„ç¥ç»ç½‘ç»œå¾€å¾€èƒ½å¾ˆå¥½åœ°æ³›åŒ–ã€‚
- en: 1.3 Why nonlinearity is important in artificial neurons
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 ä¸ºä»€ä¹ˆéçº¿æ€§åœ¨äººå·¥ç¥ç»å…ƒä¸­å¾ˆé‡è¦
- en: 'The nonlinearity of the activation function leads to such nonconvexity of the
    loss function. However, as we will see in SectionÂ [3](#S3 "3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"),
    that same nonlinearity enables the neural network to represent more complex functions
    as a whole. In fact, removing such nonlinearities by using an identity activation
    function $\sigma^{l}(u)=u~{}\forall l\in{\mathbb{L}}$ would reduce the entire
    neural network to an affine transformation of the form $f(x)={\bm{W}}^{L}({\bm{W}}^{L-1}\left(\ldots\left({\bm{W}}^{2}\left({\bm{W}}^{1}x+{\bm{b}}^{1}\right)+{\bm{b}}^{2}\right)+\ldots\right)+{\bm{b}}^{L-1})+{\bm{b}}^{L}$.
    Hence, a feedforward network without nonlinear activation functions is equivalent
    to a linear regression model. However, in that case we can easily obtain such
    a model without resorting to neural networks and backpropagation: the loss function
    is convex and the optimal solution is given by a closed formula, such as in least
    squares regression. In contrast, neural networks with a single hidden layer of
    arbitrary width have been long known to be universal function approximators for
    a broad variety of activation functions (Cybenko, [1989](#bib.bib71), Funahashi,
    [1989](#bib.bib112), Hornik etÂ al., [1989](#bib.bib153)), as well as for ReLU
    more recently (Yarotsky, [2017](#bib.bib350)). These results have also been extended
    to the converse case of limited width but arbitrarily large depth (Lu etÂ al.,
    [2017](#bib.bib202), Hanin and Sellke, [2017](#bib.bib139), Park etÂ al., [2021a](#bib.bib241)).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¿€æ´»å‡½æ•°çš„éçº¿æ€§å¯¼è‡´äº†æŸå¤±å‡½æ•°çš„éå‡¸æ€§ã€‚ç„¶è€Œï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨ç¬¬[3](#S3 "3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")èŠ‚ä¸­çœ‹åˆ°çš„ï¼Œè¿™ç§éçº¿æ€§ä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿæ•´ä½“è¡¨ç¤ºæ›´å¤æ‚çš„å‡½æ•°ã€‚äº‹å®ä¸Šï¼Œé€šè¿‡ä½¿ç”¨æ’ç­‰æ¿€æ´»å‡½æ•°
    $\sigma^{l}(u)=u~{}\forall l\in{\mathbb{L}}$ï¼Œå»é™¤è¿™ç§éçº¿æ€§å°†ä½¿æ•´ä¸ªç¥ç»ç½‘ç»œç®€åŒ–ä¸ºå½¢å¼ä¸º $f(x)={\bm{W}}^{L}({\bm{W}}^{L-1}\left(\ldots\left({\bm{W}}^{2}\left({\bm{W}}^{1}x+{\bm{b}}^{1}\right)+{\bm{b}}^{2}\right)+\ldots\right)+{\bm{b}}^{L-1})+{\bm{b}}^{L}$
    çš„ä»¿å°„å˜æ¢ã€‚å› æ­¤ï¼Œæ²¡æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°çš„å‰é¦ˆç½‘ç»œç›¸å½“äºçº¿æ€§å›å½’æ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°è·å¾—è¿™æ ·çš„æ¨¡å‹ï¼Œè€Œæ— éœ€ä¾èµ–ç¥ç»ç½‘ç»œå’Œåå‘ä¼ æ’­ï¼šæŸå¤±å‡½æ•°æ˜¯å‡¸çš„ï¼Œæœ€ä¼˜è§£ç”±ä¸€ä¸ªå°é—­å…¬å¼ç»™å‡ºï¼Œä¾‹å¦‚åœ¨æœ€å°äºŒä¹˜å›å½’ä¸­ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…·æœ‰å•ä¸ªéšè—å±‚çš„ä»»æ„å®½åº¦çš„ç¥ç»ç½‘ç»œé•¿æœŸä»¥æ¥è¢«è®¤ä¸ºæ˜¯å¹¿æ³›æ¿€æ´»å‡½æ•°çš„é€šç”¨å‡½æ•°è¿‘ä¼¼å™¨ï¼ˆCybenkoï¼Œ[1989](#bib.bib71)ï¼ŒFunahashiï¼Œ[1989](#bib.bib112)ï¼ŒHornik
    ç­‰ï¼Œ[1989](#bib.bib153)ï¼‰ï¼Œä»¥åŠæœ€è¿‘çš„ ReLUï¼ˆYarotskyï¼Œ[2017](#bib.bib350)ï¼‰ã€‚è¿™äº›ç»“æœä¹Ÿå·²æ‰©å±•åˆ°å®½åº¦æœ‰é™ä½†æ·±åº¦ä»»æ„å¤§çš„åå‘æƒ…å†µï¼ˆLu
    ç­‰ï¼Œ[2017](#bib.bib202)ï¼ŒHanin å’Œ Sellkeï¼Œ[2017](#bib.bib139)ï¼ŒPark ç­‰ï¼Œ[2021a](#bib.bib241)ï¼‰ã€‚'
- en: 'Although nonlinear activation functions are important for obtaining more complex
    models, these functions do not need to be overly complex to produce good results.
    In the past, it was common practice to use sigmoid functions for activation (LeCun
    etÂ al., [1998](#bib.bib186)). Those are monotonically increasing functions that
    approach finite values for arbitrarily large positive and negative inputs, such
    as the standard logistic function $\sigma(u)=\frac{1}{1+e^{-u}}$ and the hyperbolic
    tangent $\sigma(u)=\tanh(u)$. In the present, the most commonly used activation
    function is the Rectified Linear UnitÂ (ReLU) $\sigma(u)=\max\{0,u\}$ (LeCun etÂ al.,
    [2015](#bib.bib187), Ramachandran etÂ al., [2018](#bib.bib255)), which was proposed
    byÂ Hahnloser etÂ al. ([2000](#bib.bib135)) and first applied to neural networks
    byÂ Nair and Hinton ([2010](#bib.bib228)). The popularity of ReLU is in part due
    to experiments byÂ Nair and Hinton ([2010](#bib.bib228)) andÂ Glorot etÂ al. ([2011](#bib.bib119))
    showing that this simpler form of activation yields competitive results. Thinking
    back in terms of the analogy with biological neurons, we say that a ReLU is *active*
    when the output is positive and *inactive* when the output is zero. ReLUs have
    a linear output behavior on the inputs associated with the same ReLUs being active
    and inactive; this property also holds for other piecewise linear and piecewise
    constant functions that are used as activation functions in neural networks. TableÂ [1](#S1.T1
    "Table 1 â€£ 1.3 Why nonlinearity is important in artificial neurons â€£ 1 Introduction
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey") lists some of the most
    commonly used activation functions of that kind. For more comprehensive lists
    of activation functions, including several other variations based on ReLU, we
    refer toÂ Dubey etÂ al. ([2021](#bib.bib82)) andÂ Tao etÂ al. ([2022](#bib.bib303)).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'å°½ç®¡éçº¿æ€§æ¿€æ´»å‡½æ•°å¯¹äºè·å¾—æ›´å¤æ‚çš„æ¨¡å‹å¾ˆé‡è¦ï¼Œä½†è¿™äº›å‡½æ•°ä¸éœ€è¦è¿‡äºå¤æ‚å³å¯äº§ç”Ÿè‰¯å¥½çš„ç»“æœã€‚è¿‡å»ï¼Œä½¿ç”¨ sigmoid å‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°æ˜¯ä¸€ç§å¸¸è§åšæ³•ï¼ˆLeCun
    et al., [1998](#bib.bib186)ï¼‰ã€‚è¿™äº›å‡½æ•°æ˜¯å•è°ƒé€’å¢çš„ï¼Œå¯¹äºä»»æ„å¤§çš„æ­£è´Ÿè¾“å…¥ï¼Œéƒ½ä¼šè¶‹è¿‘äºæœ‰é™å€¼ï¼Œä¾‹å¦‚æ ‡å‡† logistic å‡½æ•° $\sigma(u)=\frac{1}{1+e^{-u}}$
    å’ŒåŒæ›²æ­£åˆ‡å‡½æ•° $\sigma(u)=\tanh(u)$ã€‚ç›®å‰ï¼Œæœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯ä¿®æ­£çº¿æ€§å•å…ƒï¼ˆReLUï¼‰ $\sigma(u)=\max\{0,u\}$ï¼ˆLeCun
    et al., [2015](#bib.bib187), Ramachandran et al., [2018](#bib.bib255)ï¼‰ï¼Œè¯¥å‡½æ•°ç”± Hahnloser
    et al. ([2000](#bib.bib135)) æå‡ºï¼Œå¹¶ç”± Nair and Hinton ([2010](#bib.bib228)) é¦–æ¬¡åº”ç”¨äºç¥ç»ç½‘ç»œã€‚ReLU
    çš„æµè¡Œéƒ¨åˆ†å½’åŠŸäº Nair and Hinton ([2010](#bib.bib228)) å’Œ Glorot et al. ([2011](#bib.bib119))
    çš„å®éªŒï¼Œè¿™äº›å®éªŒè¡¨æ˜è¿™ç§ç®€å•å½¢å¼çš„æ¿€æ´»å‡½æ•°èƒ½å¤Ÿäº§ç”Ÿç«äº‰åŠ›çš„ç»“æœã€‚ä»ç”Ÿç‰©ç¥ç»å…ƒçš„ç±»æ¯”è§’åº¦è€ƒè™‘ï¼Œæˆ‘ä»¬è¯´å½“è¾“å‡ºä¸ºæ­£æ—¶ ReLU æ˜¯ *æ´»è·ƒçš„*ï¼Œå½“è¾“å‡ºä¸ºé›¶æ—¶åˆ™æ˜¯
    *ä¸æ´»è·ƒçš„*ã€‚ReLU åœ¨ä¸åŒä¸€ ReLU æ´»è·ƒå’Œä¸æ´»è·ƒç›¸å…³çš„è¾“å…¥ä¸Šå…·æœ‰çº¿æ€§è¾“å‡ºè¡Œä¸ºï¼›è¿™ä¸€ç‰¹æ€§ä¹Ÿé€‚ç”¨äºå…¶ä»–ä½œä¸ºç¥ç»ç½‘ç»œæ¿€æ´»å‡½æ•°çš„åˆ†æ®µçº¿æ€§å’Œåˆ†æ®µå¸¸æ•°å‡½æ•°ã€‚è¡¨
    [1](#S1.T1 "Table 1 â€£ 1.3 Why nonlinearity is important in artificial neurons
    â€£ 1 Introduction â€£ When Deep Learning Meets Polyhedral Theory: A Survey") åˆ—å‡ºäº†è¿™äº›å¸¸ç”¨æ¿€æ´»å‡½æ•°ä¸­çš„ä¸€äº›ã€‚æœ‰å…³æ¿€æ´»å‡½æ•°çš„æ›´å…¨é¢åˆ—è¡¨ï¼ŒåŒ…æ‹¬åŸºäº
    ReLU çš„å‡ ç§å…¶ä»–å˜ä½“ï¼Œè¯·å‚è§ Dubey et al. ([2021](#bib.bib82)) å’Œ Tao et al. ([2022](#bib.bib303))ã€‚'
- en: 'Table 1: Main piecewise constant and piecewise linear activation functions.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨1ï¼šä¸»è¦çš„åˆ†æ®µå¸¸æ•°å’Œåˆ†æ®µçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
- en: '| Name | Function | Reference |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| åç§° | å‡½æ•° | å‚è€ƒæ–‡çŒ® |'
- en: '| --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| LTU | $\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if }u>0\\ 0&amp;\text{if
    }u\leq 0\end{array}\right.$ | McCulloch and Pitts ([1943](#bib.bib217)) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| LTU | $\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if }u>0\\ 0&amp;\text{if
    }u\leq 0\end{array}\right.$ | McCulloch and Pitts ([1943](#bib.bib217)) |'
- en: '| ReLU | $\sigma(u)=\max\{0,u\}$ | Hahnloser etÂ al. ([2000](#bib.bib135)),
    Nair and Hinton ([2010](#bib.bib228)) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| ReLU | $\sigma(u)=\max\{0,u\}$ | Hahnloser et al. ([2000](#bib.bib135)),
    Nair and Hinton ([2010](#bib.bib228)) |'
- en: '| leaky ReLU | <math   alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&amp;\text{if
    }u>0\\ \varepsilon u&amp;\text{if }u\leq 0\end{array}\right.\\'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '| leaky ReLU | <math alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&amp;\text{if
    }u>0\\ \varepsilon u&amp;\text{if }u\leq 0\end{array}\right.\\'
- en: \text{($\varepsilon$ is small and fixed)}\end{array}" display="inline"><semantics
    ><mtable rowspacing="0pt"  ><mtr ><mtd ><mrow  ><mrow ><mi >Ïƒ</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mi >u</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt"
    rowspacing="0pt" ><mtr ><mtd  ><mi >u</mi></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >ifÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >u</mi></mrow><mo
    >></mo><mn >0</mn></mrow></mtd></mtr><mtr ><mtd ><mrow ><mi >Îµ</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mi >u</mi></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow
    ><mtext >ifÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >u</mi></mrow><mo
    >â‰¤</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr ><mtd
    ><mrow  ><mtext >(</mtext><mi >Îµ</mi><mtext >Â is small and fixed)</mtext></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><apply ><apply  ><ci >ğœ</ci><ci
    >ğ‘¢</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow
    ><ci >ğ‘¢</ci><apply ><apply ><ci ><mtext >ifÂ </mtext></ci><ci >ğ‘¢</ci></apply><cn
    type="integer" >0</cn></apply></matrixrow><matrixrow ><apply ><ci >ğœ€</ci><ci >ğ‘¢</ci></apply><apply
    ><apply ><ci ><mtext >ifÂ </mtext></ci><ci >ğ‘¢</ci></apply><cn type="integer"  >0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    ><ci ><mrow  ><mtext >(</mtext><mi >Îµ</mi><mtext >Â is small and fixed)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{if
    }u>0\\ \varepsilon u&\text{if }u\leq 0\end{array}\right.\\ \text{($\varepsilon$
    is small and fixed)}\end{array}</annotation></semantics></math> | Maas etÂ al.
    ([2013](#bib.bib205)) |
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \text{($\varepsilon$ æ˜¯å°ä¸”å›ºå®šçš„)}\end{array}" display="inline"><semantics ><mtable
    rowspacing="0pt"  ><mtr ><mtd ><mrow  ><mrow ><mi >Ïƒ</mi><mo lspace="0em" rspace="0em"
    >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mi >u</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mi
    >u</mi></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext >å¦‚æœÂ </mtext><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mi >u</mi></mrow><mo >></mo><mn >0</mn></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mi >Îµ</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi >u</mi></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >å¦‚æœÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >u</mi></mrow><mo >â‰¤</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mtext >(</mtext><mi >Îµ</mi><mtext >Â æ˜¯å°ä¸”å›ºå®šçš„)</mtext></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><apply ><apply  ><ci >ğœ</ci><ci
    >ğ‘¢</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow
    ><ci >ğ‘¢</ci><apply ><apply ><ci ><mtext >å¦‚æœÂ </mtext></ci><ci >ğ‘¢</ci></apply><cn
    type="integer" >0</cn></apply></matrixrow><matrixrow ><apply ><ci >ğœ€</ci><ci >ğ‘¢</ci></apply><apply
    ><apply ><ci ><mtext >å¦‚æœÂ </mtext></ci><ci >ğ‘¢</ci></apply><cn type="integer"  >0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    ><ci ><mrow  ><mtext >(</mtext><mi >Îµ</mi><mtext >Â æ˜¯å°ä¸”å›ºå®šçš„)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{å¦‚æœ
    }u>0\\ \varepsilon u&\text{å¦‚æœ }u\leq 0\end{array}\right.\\ \text{($\varepsilon$
    æ˜¯å°ä¸”å›ºå®šçš„)}\end{array}</annotation></semantics></math> | Maas ç­‰äºº ([2013](#bib.bib205))
    |
- en: '| parametric ReLU | <math   alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&amp;\text{if
    }u>0\\ au&amp;\text{if }u\leq 0\end{array}\right.\\'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '| å‚æ•°åŒ– ReLU | <math   alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{å¦‚æœ
    }u>0\\ \varepsilon u&\text{å¦‚æœ }u\leq 0\end{array}\right.\\'
- en: \text{($a$ is a trainable parameter)}\end{array}" display="inline"><semantics
    ><mtable rowspacing="0pt"  ><mtr ><mtd ><mrow  ><mrow ><mi >Ïƒ</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mi >u</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt"
    rowspacing="0pt" ><mtr ><mtd  ><mi >u</mi></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >ifÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >u</mi></mrow><mo
    >></mo><mn >0</mn></mrow></mtd></mtr><mtr ><mtd ><mrow ><mi >a</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mi >u</mi></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow
    ><mtext >ifÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >u</mi></mrow><mo
    >â‰¤</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr ><mtd
    ><mrow  ><mtext >(</mtext><mi >a</mi><mtext >Â is a trainable parameter)</mtext></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><apply ><apply  ><ci >ğœ</ci><ci
    >ğ‘¢</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow
    ><ci >ğ‘¢</ci><apply ><apply ><ci ><mtext >ifÂ </mtext></ci><ci >ğ‘¢</ci></apply><cn
    type="integer" >0</cn></apply></matrixrow><matrixrow ><apply ><ci >ğ‘</ci><ci >ğ‘¢</ci></apply><apply
    ><apply ><ci ><mtext >ifÂ </mtext></ci><ci >ğ‘¢</ci></apply><cn type="integer"  >0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    ><ci ><mrow  ><mtext >(</mtext><mi >a</mi><mtext >Â is a trainable parameter)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{if
    }u>0\\ au&\text{if }u\leq 0\end{array}\right.\\ \text{($a$ is a trainable parameter)}\end{array}</annotation></semantics></math>
    | He etÂ al. ([2015](#bib.bib143)) |
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \text{($a$ æ˜¯ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°)}\end{array}" display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mrow  ><mrow ><mi >Ïƒ</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo
    stretchy="false" >(</mo><mi >u</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mi
    >u</mi></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext >å¦‚æœÂ </mtext><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mi >u</mi></mrow><mo >></mo><mn >0</mn></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mi >a</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi >u</mi></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >å¦‚æœÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >u</mi></mrow><mo >â‰¤</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mtext >(</mtext><mi >a</mi><mtext >Â æ˜¯ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°)</mtext></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><apply ><apply  ><ci >ğœ</ci><ci
    >ğ‘¢</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow
    ><ci >ğ‘¢</ci><apply ><apply ><ci ><mtext >å¦‚æœÂ </mtext></ci><ci >ğ‘¢</ci></apply><cn
    type="integer" >0</cn></apply></matrixrow><matrixrow ><apply ><ci >ğ‘</ci><ci >ğ‘¢</ci></apply><apply
    ><apply ><ci ><mtext >å¦‚æœÂ </mtext></ci><ci >ğ‘¢</ci></apply><cn type="integer"  >0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    ><ci ><mrow  ><mtext >(</mtext><mi >a</mi><mtext >Â æ˜¯ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{å¦‚æœ
    }u>0\\ au&\text{å¦‚æœ }u\leq 0\end{array}\right.\\ \text{($a$ æ˜¯ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°)}\end{array}</annotation></semantics></math>
    | He ç­‰äºº ([2015](#bib.bib143)) |
- en: '| hard tanh | <math   alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if
    }u>1\\ u&amp;\text{if }-1\leq u\leq 1\\'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '| ç¡¬ tanh | <math   alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{å¦‚æœ
    }u>1\\ u&amp;\text{å¦‚æœ }-1\leq u\leq 1\\'
- en: -1&amp;\text{if }u<-1\end{array}\right." display="inline"><semantics ><mrow
    ><mrow ><mi  >Ïƒ</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"
    >(</mo><mi >u</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow  ><mo
    >{</mo><mtable columnspacing="5pt" rowspacing="0pt"  ><mtr ><mtd ><mn  >1</mn></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >ifÂ </mtext><mo lspace="0em" rspace="0em"
    >â€‹</mo><mi >u</mi></mrow><mo >></mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd ><mi  >u</mi></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >ifÂ </mtext><mo >âˆ’</mo><mn >1</mn></mrow><mo
    >â‰¤</mo><mi >u</mi><mo >â‰¤</mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd  ><mrow
    ><mo >âˆ’</mo><mn >1</mn></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext
    >ifÂ </mtext><mo lspace="0em" rspace="0em" >â€‹</mo><mi >u</mi></mrow><mo ><</mo><mrow
    ><mo >âˆ’</mo><mn >1</mn></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><ci >ğœ</ci><ci >ğ‘¢</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix ><matrixrow  ><cn type="integer"  >1</cn><apply
    ><apply ><ci ><mtext >ifÂ </mtext></ci><ci >ğ‘¢</ci></apply><cn type="integer" >1</cn></apply></matrixrow><matrixrow
    ><ci  >ğ‘¢</ci><apply ><apply ><apply  ><ci ><mtext >ifÂ </mtext></ci><cn type="integer"  >1</cn></apply><ci
    >ğ‘¢</ci></apply><apply ><cn type="integer" >1</cn></apply></apply></matrixrow><matrixrow
    ><apply ><cn type="integer" >1</cn></apply><apply ><apply  ><ci ><mtext >ifÂ </mtext></ci><ci
    >ğ‘¢</ci></apply><apply ><cn type="integer" >1</cn></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\sigma(u)=\left\{\begin{array}[]{cl}1&\text{if }u>1\\
    u&\text{if }-1\leq u\leq 1\\ -1&\text{if }u<-1\end{array}\right.</annotation></semantics></math>
    | Collobert ([2004](#bib.bib62)) |
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sigma(u)=\left\{\begin{array}[]{cl}1&\text{å¦‚æœ }u>1\\ u&\text{å¦‚æœ }-1\leq u\leq
    1\\ -1&\text{å¦‚æœ }u<-1\end{array}\right.\)
- en: '| hard sigmoid | <math   alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if
    }u>\frac{1}{2}\\ u+\frac{1}{2}&amp;\text{if }-\frac{1}{2}\leq u\leq\frac{1}{2}\\'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '| ç¡¬ sigmoid | <math alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{å¦‚æœ
    }u>\frac{1}{2}\\ u+\frac{1}{2}&amp;\text{å¦‚æœ }-\frac{1}{2}\leq u\leq\frac{1}{2}\\'
- en: 0&amp;\text{if }u<-\frac{1}{2}\end{array}\right." display="inline"><semantics
    ><mrow  ><mrow ><mi >Ïƒ</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mrow  ><mo stretchy="false"  >(</mo><mi
    >u</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" rowspacing="0pt"  ><mtr ><mtd ><mn  >1</mn></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >ifÂ </mtext><mo lspace="0em" rspace="0em" >â€‹</mo><mi >u</mi></mrow><mo
    >></mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow></mtd></mtr><mtr ><mtd ><mrow  ><mi
    >u</mi><mo >+</mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >ifÂ </mtext><mo >âˆ’</mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow><mo
    >â‰¤</mo><mi >u</mi><mo >â‰¤</mo><mfrac ><mn  >1</mn><mn >2</mn></mfrac></mrow></mtd></mtr><mtr
    ><mtd ><mn  >0</mn></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext >ifÂ </mtext><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >u</mi></mrow><mo ><</mo><mrow ><mo >âˆ’</mo><mfrac
    ><mn >1</mn><mn >2</mn></mfrac></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><ci >ğœ</ci><ci >ğ‘¢</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix ><matrixrow  ><cn type="integer"  >1</cn><apply
    ><apply ><ci ><mtext >ifÂ </mtext></ci><ci >ğ‘¢</ci></apply><apply ><cn type="integer"
    >1</cn><cn type="integer" >2</cn></apply></apply></matrixrow><matrixrow ><apply
    ><ci  >ğ‘¢</ci><apply ><cn type="integer" >1</cn><cn type="integer"  >2</cn></apply></apply><apply
    ><apply ><apply  ><ci ><mtext >ifÂ </mtext></ci><apply ><cn type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply></apply><ci >ğ‘¢</ci></apply><apply ><apply ><cn
    type="integer" >1</cn><cn type="integer" >2</cn></apply></apply></apply></matrixrow><matrixrow
    ><cn type="integer" >0</cn><apply ><apply  ><ci ><mtext >ifÂ </mtext></ci><ci >ğ‘¢</ci></apply><apply
    ><apply ><cn type="integer"  >1</cn><cn type="integer"  >2</cn></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\sigma(u)=\left\{\begin{array}[]{cl}1&\text{if }u>\frac{1}{2}\\
    u+\frac{1}{2}&\text{if }-\frac{1}{2}\leq u\leq\frac{1}{2}\\ 0&\text{if }u<-\frac{1}{2}\end{array}\right.</annotation></semantics></math>
    | Courbariaux etÂ al. ([2015](#bib.bib63)) |
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 0&\text{å¦‚æœ }u<-\frac{1}{2}\end{array}\right." display="inline"><semantics ><mrow  ><mrow
    ><mi >Ïƒ</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mrow  ><mo stretchy="false"  >(</mo><mi
    >u</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" rowspacing="0pt"  ><mtr ><mtd ><mn  >1</mn></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >å¦‚æœÂ </mtext><mo lspace="0em" rspace="0em" >â€‹</mo><mi >u</mi></mrow><mo
    >></mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow></mtd></mtr><mtr ><mtd ><mrow  ><mi
    >u</mi><mo >+</mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >å¦‚æœÂ </mtext><mo >âˆ’</mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow><mo
    >â‰¤</mo><mi >u</mi><mo >â‰¤</mo><mfrac ><mn  >1</mn><mn >2</mn></mfrac></mrow></mtd></mtr><mtr
    ><mtd ><mn  >0</mn></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext >å¦‚æœÂ </mtext><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >u</mi></mrow><mo ><</mo><mrow ><mo >âˆ’</mo><mfrac
    ><mn >1</mn><mn >2</mn></mfrac></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><ci >ğœ</ci><ci >ğ‘¢</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix ><matrixrow  ><cn type="integer"  >1</cn><apply
    ><apply ><ci ><mtext >å¦‚æœÂ </mtext></ci><ci >ğ‘¢</ci></apply><apply ><cn type="integer"
    >1</cn><cn type="integer" >2</cn></apply></apply></matrixrow><matrixrow ><apply
    ><ci  >ğ‘¢</ci><apply ><cn type="integer" >1</cn><cn type="integer"  >2</cn></apply></apply><apply
    ><apply ><apply  ><ci ><mtext >å¦‚æœÂ </mtext></ci><apply ><cn type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply></apply><ci >ğ‘¢</ci></apply><apply ><apply ><cn
    type="integer" >1</cn><cn type="integer" >2</cn></apply></apply></apply></matrixrow><matrixrow
    ><cn type="integer" >0</cn><apply ><apply  ><ci ><mtext >å¦‚æœÂ </mtext></ci><ci >ğ‘¢</ci></apply><apply
    ><apply ><cn type="integer"  >1</cn><cn type="integer"  >2</cn></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\sigma(u)=\left\{\begin{array}[]{cl}1&\text{å¦‚æœ }u>\frac{1}{2}\\
    u+\frac{1}{2}&\text{å¦‚æœ }-\frac{1}{2}\leq u\leq\frac{1}{2}\\ 0&\text{å¦‚æœ }u<-\frac{1}{2}\end{array}\right.</annotation></semantics></math>
    | Courbariaux ç­‰äºº ([2015](#bib.bib63)) |
- en: '| max pooling | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{0,u_{1},\ldots,u_{k}\}\\
    \text{(each $u_{i}$ is the output of another neuron)}\end{array}$ | Weng etÂ al.
    ([1992](#bib.bib329)) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| æœ€å¤§æ± åŒ– | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{0,u_{1},\ldots,u_{k}\}\\
    \text{ï¼ˆæ¯ä¸ª $u_{i}$ æ˜¯å¦ä¸€ä¸ªç¥ç»å…ƒçš„è¾“å‡ºï¼‰}\end{array}$ | Weng ç­‰äºº ([1992](#bib.bib329)) |'
- en: '| maxout | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{u_{1},\ldots,u_{k}\}\\
    \text{(each $u_{i}$ is an affine function)}\end{array}$ | Goodfellow etÂ al. ([2013](#bib.bib123))
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| æœ€å¤§è¾“å‡º | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{u_{1},\ldots,u_{k}\}\\
    \text{ï¼ˆæ¯ä¸ª $u_{i}$ æ˜¯ä¸€ä¸ªä»¿å°„å‡½æ•°ï¼‰}\end{array}$ | Goodfellow ç­‰äºº ([2013](#bib.bib123))
    |'
- en: 1.4 When deep learning meets polyhedral theory
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®º
- en: 'It is commonly accepted in machine learning that a simpler model is preferred
    if it trains as well as a more complex one, since a simpler model is less likely
    to overfit. Conveniently, the successful return of neural networks to relatively
    simpler activation functions prepared the ground for deep learning to meet polyhedral
    theory. In other words, we are now able to analyze and leverage neural networks
    through the same lenses and tools that have been successfully used for linear
    and discrete optimization in operations research for many decades. We explain
    this connection in more detail and some lines of research that it has opened up
    in SectionÂ [2](#S2 "2 The Polyhedral Perspective â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œé€šå¸¸è®¤ä¸ºå¦‚æœä¸€ä¸ªæ›´ç®€å•çš„æ¨¡å‹èƒ½å¤Ÿåƒæ›´å¤æ‚çš„æ¨¡å‹ä¸€æ ·è®­ç»ƒå¥½ï¼Œé‚£ä¹ˆæ›´ç®€å•çš„æ¨¡å‹æ˜¯æ›´å—æ¬¢è¿çš„ï¼Œå› ä¸ºæ›´ç®€å•çš„æ¨¡å‹ä¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚å¹¸è¿çš„æ˜¯ï¼Œç¥ç»ç½‘ç»œæˆåŠŸåœ°å›å½’åˆ°ç›¸å¯¹ç®€å•çš„æ¿€æ´»å‡½æ•°ï¼Œä¸ºæ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºçš„ç»“åˆå¥ å®šäº†åŸºç¡€ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡ä¸è¿ç­¹å­¦ä¸­ç”¨äºçº¿æ€§å’Œç¦»æ•£ä¼˜åŒ–çš„ç›¸åŒè§†è§’å’Œå·¥å…·æ¥åˆ†æå’Œåˆ©ç”¨ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬åœ¨ç¬¬[2](#S2
    "2 The Polyhedral Perspective â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")èŠ‚ä¸­æ›´è¯¦ç»†åœ°è§£é‡Šäº†è¿™ç§è”ç³»åŠå…¶å¼€å¯çš„ä¸€äº›ç ”ç©¶æ–¹å‘ã€‚'
- en: 1.5 Scope of this survey and related work
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5 æœ¬è°ƒæŸ¥çš„èŒƒå›´åŠç›¸å…³å·¥ä½œ
- en: The interplay between mathematical optimization and machine learning has also
    been discussed by other recent surveys. Bengio etÂ al. ([2021](#bib.bib19)) review
    the use of machine learning in mathematical optimization, whereasÂ Gambella etÂ al.
    ([2021](#bib.bib115)) formulate mathematical optimization problems with the main
    focus of obtaining machine learning models, such as by training neural networks.
    A similar scope has been previously surveyed byÂ Curtis and Scheinberg ([2017](#bib.bib70))
    andÂ Bottou etÂ al. ([2018](#bib.bib38)). Our survey complements those by focusing
    exclusively on neural networks while outlining how linear optimization can be
    used more broadly in that context, from network training and verification to model
    embedding and compression, as well as refined through formulation strengthening.
    In addition, we illustrate how polyhedral theory can ground the use of such linear
    formulations and also provide a more nuanced understanding of the discriminative
    ability of neural networks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°å­¦ä¼˜åŒ–å’Œæœºå™¨å­¦ä¹ ä¹‹é—´çš„ç›¸äº’ä½œç”¨ä¹Ÿå·²åœ¨å…¶ä»–æœ€è¿‘çš„è°ƒæŸ¥ä¸­è®¨è®ºè¿‡ã€‚Bengioç­‰äººï¼ˆ[2021](#bib.bib19)ï¼‰å›é¡¾äº†æœºå™¨å­¦ä¹ åœ¨æ•°å­¦ä¼˜åŒ–ä¸­çš„åº”ç”¨ï¼Œè€ŒGambellaç­‰äººï¼ˆ[2021](#bib.bib115)ï¼‰åˆ™ä»¥è·å–æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ºä¸»è¦ç„¦ç‚¹æ¥å½¢å¼åŒ–æ•°å­¦ä¼˜åŒ–é—®é¢˜ï¼Œä¾‹å¦‚é€šè¿‡è®­ç»ƒç¥ç»ç½‘ç»œã€‚Curtiså’ŒScheinbergï¼ˆ[2017](#bib.bib70)ï¼‰ä»¥åŠBottouç­‰äººï¼ˆ[2018](#bib.bib38)ï¼‰ä¹‹å‰ä¹Ÿæœ‰ç±»ä¼¼çš„èŒƒå›´è¿›è¡Œè¿‡è°ƒæŸ¥ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥é€šè¿‡ä¸“æ³¨äºç¥ç»ç½‘ç»œï¼ŒåŒæ—¶æ¦‚è¿°äº†å¦‚ä½•åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹æ›´å¹¿æ³›åœ°ä½¿ç”¨çº¿æ€§ä¼˜åŒ–ï¼Œä»ç½‘ç»œè®­ç»ƒå’ŒéªŒè¯åˆ°æ¨¡å‹åµŒå…¥å’Œå‹ç¼©ï¼Œä»¥åŠé€šè¿‡å½¢å¼åŒ–å¼ºåŒ–è¿›è¡Œçš„æ”¹è¿›ï¼Œæ¥è¡¥å……è¿™äº›è°ƒæŸ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯´æ˜äº†å¤šé¢ä½“ç†è®ºå¦‚ä½•ä¸ºè¿™äº›çº¿æ€§å½¢å¼çš„ä½¿ç”¨å¥ å®šåŸºç¡€ï¼Œå¹¶æä¾›å¯¹ç¥ç»ç½‘ç»œåŒºåˆ†èƒ½åŠ›çš„æ›´ç»†è‡´ç†è§£ã€‚
- en: 'The presentation in this survey is centered on *feedforward rectifier networks*.
    These are very commonly used networks with only ReLU activations and for which
    most polyhedral results and applications of linear optimization are known. The
    focus on a single type of neural network is intended to help the reader capture
    the intuition behind different developments and understand the nuances involved.
    Despite our focus on *fully-connected* models, which are those in which every
    unit is connected to all units in the subsequent layer, there are many variants
    of interest with fewer or different types of connection that can be interpreted
    as a special case of fully-connected models. For example, the units of Convolutional
    Neural NetworksÂ (CNNs or ConvNets) (Fukushima, [1980](#bib.bib111)) have local
    connectivity: only a subset of adjacent units defines the output of each unit
    in the next layer, and the same parameters are used to define the output of different
    units. In fact, multiple *filters* of parameters can be applied to a set of adjacent
    units through the output of different units in the next layer. CNNs are often
    applied to identify and aggregate the same local features in different parts of
    a picture, and we can interpret them as a special case of feedforward networks.
    Another common variant, the Residual NetworkÂ (ResNet) (He etÂ al., [2016](#bib.bib144)),
    includes *skip connections* that directly connect units in nonadjacent layers.
    Those connections can be emulated by adding units passing their outputs through
    the intermediary layers. Hence, many of the results and applications discussed
    along the survey are relevant to other variants (e.g., LTU and maxout activations,
    or those other connectivity patterns), and we also provide references to more
    specific results and applications involving them.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è°ƒæŸ¥ä¸­çš„æ¼”ç¤ºä»¥*å‰é¦ˆæ•´æµç½‘ç»œ*ä¸ºä¸­å¿ƒã€‚è¿™äº›ç½‘ç»œéå¸¸å¸¸è§ï¼Œä»…ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ï¼Œå¯¹äºå®ƒä»¬ï¼Œå¤§å¤šæ•°å¤šé¢ä½“ç»“æœå’Œçº¿æ€§ä¼˜åŒ–åº”ç”¨éƒ½å·²çŸ¥ã€‚ä¸“æ³¨äºå•ä¸€ç±»å‹çš„ç¥ç»ç½‘ç»œæ—¨åœ¨å¸®åŠ©è¯»è€…æ•æ‰ä¸åŒå‘å±•çš„ç›´è§‰å¹¶ç†è§£å…¶ä¸­çš„ç»†å¾®å·®åˆ«ã€‚å°½ç®¡æˆ‘ä»¬ä¸“æ³¨äº*å…¨è¿æ¥*æ¨¡å‹ï¼Œå³æ¯ä¸ªå•å…ƒéƒ½ä¸åç»­å±‚çš„æ‰€æœ‰å•å…ƒè¿æ¥ï¼Œä½†ä»æœ‰è®¸å¤šæœ‰è¶£çš„å˜ä½“å…·æœ‰è¾ƒå°‘æˆ–ä¸åŒç±»å‹çš„è¿æ¥ï¼Œå¯ä»¥è¢«è§£é‡Šä¸ºå…¨è¿æ¥æ¨¡å‹çš„ç‰¹ä¾‹ã€‚ä¾‹å¦‚ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsæˆ–ConvNetsï¼‰ï¼ˆFukushimaï¼Œ[1980](#bib.bib111)ï¼‰çš„å•å…ƒå…·æœ‰å±€éƒ¨è¿æ¥æ€§ï¼šä»…ç›¸é‚»å•å…ƒçš„å­é›†å®šä¹‰äº†ä¸‹ä¸€å±‚ä¸­æ¯ä¸ªå•å…ƒçš„è¾“å‡ºï¼Œå¹¶ä¸”ç›¸åŒçš„å‚æ•°ç”¨äºå®šä¹‰ä¸åŒå•å…ƒçš„è¾“å‡ºã€‚å®é™…ä¸Šï¼Œå¯ä»¥é€šè¿‡ä¸‹ä¸€å±‚ä¸­ä¸åŒå•å…ƒçš„è¾“å‡ºå°†å¤šä¸ª*æ»¤æ³¢å™¨*å‚æ•°åº”ç”¨äºä¸€ç»„ç›¸é‚»å•å…ƒã€‚CNNsé€šå¸¸ç”¨äºè¯†åˆ«å’Œæ±‡èšå›¾åƒä¸­ä¸åŒéƒ¨åˆ†çš„ç›¸åŒå±€éƒ¨ç‰¹å¾ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬è§£é‡Šä¸ºå‰é¦ˆç½‘ç»œçš„ç‰¹ä¾‹ã€‚å¦ä¸€ä¸ªå¸¸è§çš„å˜ä½“ï¼Œæ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰ï¼ˆHe
    et al., [2016](#bib.bib144)ï¼‰ï¼ŒåŒ…æ‹¬*è·³è·ƒè¿æ¥*ï¼Œç›´æ¥è¿æ¥éç›¸é‚»å±‚ä¸­çš„å•å…ƒã€‚è¿™äº›è¿æ¥å¯ä»¥é€šè¿‡åœ¨ä¸­é—´å±‚ä¼ é€’å®ƒä»¬çš„è¾“å‡ºæ¥æ¨¡æ‹Ÿã€‚å› æ­¤ï¼Œè°ƒæŸ¥ä¸­è®¨è®ºçš„è®¸å¤šç»“æœå’Œåº”ç”¨ä¹Ÿé€‚ç”¨äºå…¶ä»–å˜ä½“ï¼ˆä¾‹å¦‚ï¼ŒLTUå’Œmaxoutæ¿€æ´»ï¼Œæˆ–å…¶ä»–è¿æ¥æ¨¡å¼ï¼‰ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†æ¶‰åŠå®ƒä»¬çš„æ›´å…·ä½“ç»“æœå’Œåº”ç”¨çš„å‚è€ƒã€‚
- en: We also discuss the extent to which other variants remain relevant or can be
    analyzed through the same lenses. For example, *feedback connections* in *recurrent
    networks* (Little, [1974](#bib.bib195), Hopfield, [1982](#bib.bib152)) allow the
    output of a unit to be used as an input of units in previous layers. Recurrent
    networks such as Long Short-Term MemoryÂ (LSTM) (Hochreiter and Schmidhuber, [1997](#bib.bib151))
    produce outputs that depend on their internal state, and they may consequently
    process sequential inputs with arbitrary length. While feedback connections may
    not be emulated with a feeforward network, we discuss in the following paragraph
    how recurrent networks have been replaced with great success by attention mechanisms,
    which are implemented with feedforward networks. In the realm of variants that
    remain relevant, it is very common to apply a different type of activation to
    the output layer of the network, such as the layer-wise softmax function $\sigma:\mathbb{R}^{n_{L}}\rightarrow\mathbb{R}^{n_{L}}$
    in which $\sigma(u)_{i}=e^{u_{i}}/\sum_{j=1}^{n_{L}}e^{u_{j}}~{}\forall i\in\{1,\ldots,n_{L}\}$
    (Bridle, [1990](#bib.bib39)), which is used to normalize a multidimensional output
    as a probability distribution. While softmax is not piecewise linear, we describe
    how its output can also be analyzed from a polyhedral perspective.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜è®¨è®ºäº†å…¶ä»–å˜ä½“åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä»ç„¶ç›¸å…³æˆ–å¯ä»¥é€šè¿‡ç›¸åŒçš„è§†è§’è¿›è¡Œåˆ†æã€‚ä¾‹å¦‚ï¼Œ*åé¦ˆè¿æ¥*åœ¨*é€’å½’ç½‘ç»œ*ï¼ˆLittleï¼Œ[1974](#bib.bib195)ï¼ŒHopfieldï¼Œ[1982](#bib.bib152)ï¼‰ä¸­å…è®¸ä¸€ä¸ªå•å…ƒçš„è¾“å‡ºä½œä¸ºå‰ä¸€å±‚å•å…ƒçš„è¾“å…¥ã€‚é€’å½’ç½‘ç»œå¦‚é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰ï¼ˆHochreiter
    å’Œ Schmidhuberï¼Œ[1997](#bib.bib151)ï¼‰äº§ç”Ÿä¾èµ–äºå…¶å†…éƒ¨çŠ¶æ€çš„è¾“å‡ºï¼Œå› æ­¤å®ƒä»¬å¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—è¾“å…¥ã€‚å°½ç®¡åé¦ˆè¿æ¥æ— æ³•é€šè¿‡å‰é¦ˆç½‘ç»œè¿›è¡Œæ¨¡æ‹Ÿï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„æ®µè½ä¸­è®¨è®ºé€’å½’ç½‘ç»œå¦‚ä½•è¢«æˆåŠŸæ›¿ä»£ä¸ºä½¿ç”¨å‰é¦ˆç½‘ç»œå®ç°çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨ä»ç„¶ç›¸å…³çš„å˜ä½“é¢†åŸŸä¸­ï¼Œé€šå¸¸ä¼šå¯¹ç½‘ç»œçš„è¾“å‡ºå±‚åº”ç”¨ä¸åŒç±»å‹çš„æ¿€æ´»ï¼Œä¾‹å¦‚å±‚çº§softmaxå‡½æ•°
    $\sigma:\mathbb{R}^{n_{L}}\rightarrow\mathbb{R}^{n_{L}}$ï¼Œå…¶ä¸­ $\sigma(u)_{i}=e^{u_{i}}/\sum_{j=1}^{n_{L}}e^{u_{j}}~{}\forall
    i\in\{1,\ldots,n_{L}\}$ï¼ˆBridleï¼Œ[1990](#bib.bib39)ï¼‰ï¼Œç”¨äºå°†å¤šç»´è¾“å‡ºå½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚è™½ç„¶softmaxä¸æ˜¯åˆ†æ®µçº¿æ€§çš„ï¼Œæˆ‘ä»¬æè¿°äº†å¦‚ä½•ä»å¤šé¢ä½“çš„è§’åº¦åˆ†æå…¶è¾“å‡ºã€‚
- en: Other uses of deep learning
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ çš„å…¶ä»–åº”ç”¨
- en: Deep learning is also being used in machine learning beyond the realm of supervised
    learning. In *unsupervised learning*, the focus is on drawing inferences from
    unlabeled datasets. For example, Generative Adversarial NetworksÂ (GANs) (Goodfellow
    etÂ al., [2014](#bib.bib126)) have been used to generate realistic images using
    a pair of neural networks. One of these networks is a *discriminator* trained
    to identify elements from a dataset and the other is a *generator* aiming to mislead
    the discriminator with synthetic inputs that could be classified as belonging
    to the dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ ä¸ä»…è¢«ç”¨äºç›‘ç£å­¦ä¹ çš„é¢†åŸŸï¼Œä¹Ÿåº”ç”¨äºæœºå™¨å­¦ä¹ çš„å…¶ä»–æ–¹é¢ã€‚åœ¨*æ— ç›‘ç£å­¦ä¹ *ä¸­ï¼Œé‡ç‚¹æ˜¯ä»æœªæ ‡è®°çš„æ•°æ®é›†ä¸­æ¨æ–­å‡ºç»“è®ºã€‚ä¾‹å¦‚ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼ˆGoodfellow
    ç­‰ï¼Œ[2014](#bib.bib126)ï¼‰è¢«ç”¨æ¥é€šè¿‡ä¸€å¯¹ç¥ç»ç½‘ç»œç”ŸæˆçœŸå®æ„Ÿçš„å›¾åƒã€‚è¿™äº›ç½‘ç»œä¸­çš„ä¸€ä¸ªæ˜¯*åˆ¤åˆ«å™¨*ï¼Œå®ƒè¢«è®­ç»ƒæ¥è¯†åˆ«æ•°æ®é›†ä¸­çš„å…ƒç´ ï¼Œå¦ä¸€ä¸ªæ˜¯*ç”Ÿæˆå™¨*ï¼Œæ—¨åœ¨ç”¨åˆæˆè¾“å…¥è¯¯å¯¼åˆ¤åˆ«å™¨ï¼Œä½¿å…¶å°†è¿™äº›è¾“å…¥å½’ç±»ä¸ºå±äºæ•°æ®é›†ã€‚
- en: In *reinforcement learning*, the focus is on modeling agents that can interact
    with their environment through actions and associated rewards. Examples of such
    applications include neural networks designed for the navigation of self-driving
    vehicles (Gao etÂ al., [2020](#bib.bib116)) and for playing Atari games (Mnih etÂ al.,
    [2015](#bib.bib222)), more contemporary electronic games such as Dota 2 (OpenAI
    etÂ al., [2019](#bib.bib237)) and StarCraft II (Vinyals etÂ al., [2017](#bib.bib321)),
    and the game of Go (Silver etÂ al., [2017](#bib.bib288)) at levels that are either
    better or at least comparable to human players.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å¼ºåŒ–å­¦ä¹ *ä¸­ï¼Œé‡ç‚¹æ˜¯å»ºæ¨¡èƒ½å¤Ÿé€šè¿‡åŠ¨ä½œå’Œç›¸å…³å¥–åŠ±ä¸ç¯å¢ƒäº’åŠ¨çš„ä»£ç†ã€‚è¿™æ ·çš„åº”ç”¨å®ä¾‹åŒ…æ‹¬ç”¨äºè‡ªé©¾è½¦å¯¼èˆªçš„ç¥ç»ç½‘ç»œï¼ˆGao ç­‰ï¼Œ[2020](#bib.bib116)ï¼‰å’Œç”¨äºç©Atariæ¸¸æˆçš„ç¥ç»ç½‘ç»œï¼ˆMnih
    ç­‰ï¼Œ[2015](#bib.bib222)ï¼‰ï¼Œä»¥åŠç°ä»£ç”µå­æ¸¸æˆå¦‚Dota 2ï¼ˆOpenAI ç­‰ï¼Œ[2019](#bib.bib237)ï¼‰å’ŒStarCraft
    IIï¼ˆVinyals ç­‰ï¼Œ[2017](#bib.bib321)ï¼‰ï¼Œè¿˜æœ‰å›´æ£‹ï¼ˆSilver ç­‰ï¼Œ[2017](#bib.bib288)ï¼‰ï¼Œè¿™äº›æ¸¸æˆåœ¨æŸäº›æ°´å¹³ä¸Šå¯ä»¥ä¸äººç±»ç©å®¶ç›¸åª²ç¾æˆ–æ›´èƒœä¸€ç­¹ã€‚
- en: A more recent and popular example are generative transformers (Radford etÂ al.,
    [2018](#bib.bib252)), such as DALLÂ·E 2 (Ramesh etÂ al., [2022](#bib.bib257)) producing
    realistic images from text prompts in mid-2022 and ChatGPT (OpenAI, [2022](#bib.bib236))
    producing realistic dialogues with users in early 2023, the latter belonging to
    the fast growing family of large language models. They are based on replacing
    architectures based on feedback connections, such as LSTM, with the attention
    mechanisms aimed at scoring the relevance of past states (Bahdanau etÂ al., [2015](#bib.bib10)),
    which is the foundation of the transformer architecture (Vaswani etÂ al., [2017](#bib.bib315)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè¾ƒæ–°ä¸”æµè¡Œçš„ä¾‹å­æ˜¯ç”Ÿæˆå‹å˜æ¢å™¨ï¼ˆRadford ç­‰ï¼Œ[2018](#bib.bib252)ï¼‰ï¼Œä¾‹å¦‚ DALLÂ·E 2ï¼ˆRamesh ç­‰ï¼Œ[2022](#bib.bib257)ï¼‰åœ¨
    2022 å¹´ä¸­æœŸä»æ–‡æœ¬æç¤ºç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä»¥åŠ ChatGPTï¼ˆOpenAIï¼Œ[2022](#bib.bib236)ï¼‰åœ¨ 2023 å¹´åˆä¸ç”¨æˆ·ç”Ÿæˆé€¼çœŸçš„å¯¹è¯ï¼Œåè€…å±äºå¿«é€Ÿå¢é•¿çš„å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—ã€‚å®ƒä»¬åŸºäºç”¨æ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£åŸºäºåé¦ˆè¿æ¥çš„æ¶æ„ï¼Œå¦‚
    LSTMï¼Œæ³¨æ„åŠ›æœºåˆ¶æ—¨åœ¨è¯„åˆ†è¿‡å»çŠ¶æ€çš„ç›¸å…³æ€§ï¼ˆBahdanau ç­‰ï¼Œ[2015](#bib.bib10)ï¼‰ï¼Œè¿™æ˜¯å˜æ¢å™¨æ¶æ„çš„åŸºç¡€ï¼ˆVaswani ç­‰ï¼Œ[2017](#bib.bib315)ï¼‰ã€‚
- en: Further reading
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ·±å…¥é˜…è¯»
- en: For a historical perspective on neural networks, we recommend Schmidhuber ([2015](#bib.bib273)).
    For a recent and broad introduction to the fundamentals of deep learning, we recommendÂ Zhang
    etÂ al. ([2023](#bib.bib353)). For other forms of measuring model complexity in
    neural networks, we refer toÂ Hu etÂ al. ([2021](#bib.bib156)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºç¥ç»ç½‘ç»œçš„å†å²è§†è§’ï¼Œæˆ‘ä»¬æ¨è Schmidhuber ([2015](#bib.bib273))ã€‚å…³äºæ·±åº¦å­¦ä¹ åŸºç¡€çš„æœ€æ–°å’Œå¹¿æ³›çš„ä»‹ç»ï¼Œæˆ‘ä»¬æ¨è Zhang
    ç­‰ ([2023](#bib.bib353))ã€‚æœ‰å…³ç¥ç»ç½‘ç»œæ¨¡å‹å¤æ‚æ€§çš„å…¶ä»–æµ‹é‡å½¢å¼ï¼Œè¯·å‚è€ƒ Hu ç­‰ ([2021](#bib.bib156))ã€‚
- en: 2 The Polyhedral Perspective
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 å¤šé¢ä½“è§†è§’
- en: A feedforward rectifier network models a piecewise linear function (Arora etÂ al.,
    [2018](#bib.bib8)) in which every such piece is a polyhedron (Raghu etÂ al., [2017](#bib.bib253)),
    and represents a special case among neural networks modeling piecewise polynomials
    (Balestriero and Baraniuk, [2018](#bib.bib15)). Therefore, training a rectifier
    network is equivalent to performing a piecewise linear regression, and we can
    potentially interpret such neural networks in terms of what happens in each piece
    of the function that they model. However, we are only beginning to answer some
    of the questions entailed by such a remark. In this survey, we discuss how insights
    on this subject may help us answer the following questions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå‰é¦ˆæ•´æµç½‘ç»œå»ºæ¨¡äº†ä¸€ä¸ªåˆ†æ®µçº¿æ€§å‡½æ•°ï¼ˆArora ç­‰ï¼Œ[2018](#bib.bib8)ï¼‰ï¼Œå…¶ä¸­æ¯ä¸€æ®µéƒ½æ˜¯ä¸€ä¸ªå¤šé¢ä½“ï¼ˆRaghu ç­‰ï¼Œ[2017](#bib.bib253)ï¼‰ï¼Œå¹¶ä¸”åœ¨å»ºæ¨¡åˆ†æ®µå¤šé¡¹å¼çš„ç¥ç»ç½‘ç»œä¸­ä»£è¡¨ä¸€ä¸ªç‰¹æ®Šçš„æƒ…å†µï¼ˆBalestriero
    å’Œ Baraniukï¼Œ[2018](#bib.bib15)ï¼‰ã€‚å› æ­¤ï¼Œè®­ç»ƒä¸€ä¸ªæ•´æµç½‘ç»œç­‰åŒäºè¿›è¡Œåˆ†æ®µçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬å¯ä»¥æ½œåœ¨åœ°æ ¹æ®å®ƒä»¬å»ºæ¨¡çš„æ¯ä¸€æ®µå‡½æ•°çš„è¡¨ç°æ¥è§£é‡Šè¿™äº›ç¥ç»ç½‘ç»œã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¿˜åªæ˜¯å¼€å§‹å›ç­”ç”±æ­¤ç±»è§‚å¯Ÿå¼•å‘çš„ä¸€äº›é—®é¢˜ã€‚åœ¨è¿™é¡¹è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å…³äºè¿™ä¸€ä¸»é¢˜çš„è§è§£å¦‚ä½•å¸®åŠ©æˆ‘ä»¬å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚
- en: '1.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Which piecewise linear functions can or cannot be obtained from training a neural
    network given its architecture?
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å“ªäº›åˆ†æ®µçº¿æ€§å‡½æ•°å¯ä»¥æˆ–ä¸èƒ½é€šè¿‡è®­ç»ƒç¥ç»ç½‘ç»œæ ¹æ®å…¶æ¶æ„è·å¾—ï¼Ÿ
- en: '2.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Which neural networks are more susceptible to adversarial exploitation?
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å“ªäº›ç¥ç»ç½‘ç»œæ›´å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„åˆ©ç”¨ï¼Ÿ
- en: '3.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Can we integrate the model learned by a neural network into a broader decision-making
    problem for which we want to find an optimal solution?
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½å¦å°†ç¥ç»ç½‘ç»œå­¦ä¹ åˆ°çš„æ¨¡å‹æ•´åˆåˆ°ä¸€ä¸ªæ›´å¹¿æ³›çš„å†³ç­–é—®é¢˜ä¸­ï¼Œä»¥æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜è§£ï¼Ÿ
- en: '4.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Is it possible to obtain a smaller neural network that models exactly the same
    function as another trained neural network?
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ˜¯å¦å¯ä»¥è·å¾—ä¸€ä¸ªæ¨¡å‹å®Œå…¨ç›¸åŒå‡½æ•°çš„æ›´å°ç¥ç»ç½‘ç»œï¼Ÿ
- en: '5.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Can we exploit the polyhedral geometry present in neural networks in the training
    phase?
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½å¦åœ¨è®­ç»ƒé˜¶æ®µåˆ©ç”¨ç¥ç»ç½‘ç»œä¸­å­˜åœ¨çš„å¤šé¢ä½“å‡ ä½•ï¼Ÿ
- en: '6.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Can we efficiently incorporate extra structure when training neural network,
    such as linear constraints over the weights?
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½å¦åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶æœ‰æ•ˆåœ°èå…¥é¢å¤–çš„ç»“æ„ï¼Œä¾‹å¦‚å¯¹æƒé‡çš„çº¿æ€§çº¦æŸï¼Ÿ
- en: 'The first question complements the universal approximation results for neural
    networks. Namely, there is a limit to what functions can be well approximated
    when limited computational resources are translated into constraints on the depth
    and width of the layers of neural networks that can be used in practice. The functions
    that can be modeled depend on the particular choice of hyperparameters subject
    to the computational resources available, and in the long run that may also lead
    to a more principled approach for the choice of hyperparameters than the current
    approaches of neural architecture search. In SectionÂ [3](#S3 "3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"),
    we analyze how a rectifier network partitions the input space into pieces in which
    it behaves linearly, which we denote as *linear regions*. We discuss the geometry
    of linear regions, the effect of parameters and hyperparameters on the number
    of linear regions of a neural network, and the extent to which such number of
    linear regions relates to the accuracy of the network.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€ä¸ªé—®é¢˜è¡¥å……äº†ç¥ç»ç½‘ç»œçš„é€šç”¨é€¼è¿‘ç»“æœã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“è®¡ç®—èµ„æºæœ‰é™è¢«è½¬åŒ–ä¸ºå®é™…åº”ç”¨ä¸­å±‚æ·±åº¦å’Œå®½åº¦çš„çº¦æŸæ—¶ï¼Œå¯ä»¥è‰¯å¥½é€¼è¿‘çš„å‡½æ•°æ˜¯æœ‰é™çš„ã€‚å¯ä»¥å»ºæ¨¡çš„å‡½æ•°ä¾èµ–äºç‰¹å®šçš„è¶…å‚æ•°é€‰æ‹©ï¼Œè¿™äº›è¶…å‚æ•°å—é™äºå¯ç”¨çš„è®¡ç®—èµ„æºï¼Œä»é•¿è¿œæ¥çœ‹ï¼Œè¿™å¯èƒ½ä¹Ÿä¼šå¯¼è‡´æ¯”å½“å‰çš„ç¥ç»æ¶æ„æœç´¢æ–¹æ³•æ›´ä¸ºæœ‰åŸåˆ™çš„è¶…å‚æ•°é€‰æ‹©æ–¹æ³•ã€‚åœ¨ç¬¬[3](#S3
    "3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†æ•´æµç½‘ç»œå¦‚ä½•å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸ºåœ¨å…¶ä¸­çº¿æ€§è¡Œä¸ºçš„åŒºåŸŸï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º*çº¿æ€§åŒºåŸŸ*ã€‚æˆ‘ä»¬è®¨è®ºäº†çº¿æ€§åŒºåŸŸçš„å‡ ä½•å½¢çŠ¶ã€å‚æ•°å’Œè¶…å‚æ•°å¯¹ç¥ç»ç½‘ç»œçº¿æ€§åŒºåŸŸæ•°é‡çš„å½±å“ï¼Œä»¥åŠè¿™ç§çº¿æ€§åŒºåŸŸæ•°é‡ä¸ç½‘ç»œå‡†ç¡®æ€§ä¹‹é—´çš„å…³ç³»ã€‚'
- en: 'The second question relies on formal verification methods to evaluate the robustness
    of neural networks, which can be approached with mathematical optimization formulations
    that are also relevant for the third and fourth questions. Such formulations are
    convenient since a direct inspection of every piece of the function modeled by
    a neural network is prohibitive given how quickly their number scale with the
    size of the network. The linear behavior of the network for every choice of active
    and inactive units implies that we can use linear formulations with binary variables
    corresponding to the activation of units to model trained neural networks using
    MILP. Therefore, we are able to solve a variety of optimization problems over
    a trained neural network, such as the neural network verification problem, identifying
    the range of outputs for each ReLU of the network, and modeling a trained neural
    network as part of a larger decision-making problem. In SectionÂ [4](#S4 "4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey"), we discuss how to formulate optimization problems over a trained neural
    network, the applications of such formulations, and the progress toward obtaining
    stronger formulations that scale more easily with the network size.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬äºŒä¸ªé—®é¢˜ä¾èµ–äºå½¢å¼åŒ–éªŒè¯æ–¹æ³•æ¥è¯„ä¼°ç¥ç»ç½‘ç»œçš„é²æ£’æ€§ï¼Œè¿™å¯ä»¥é€šè¿‡æ•°å­¦ä¼˜åŒ–å…¬å¼æ¥è§£å†³ï¼Œè¿™äº›å…¬å¼ä¹Ÿä¸ç¬¬ä¸‰ä¸ªå’Œç¬¬å››ä¸ªé—®é¢˜ç›¸å…³ã€‚è¿™äº›å…¬å¼å¾ˆæ–¹ä¾¿ï¼Œå› ä¸ºç›´æ¥æ£€æŸ¥ç¥ç»ç½‘ç»œå»ºæ¨¡çš„æ¯ä¸€ä¸ªåŠŸèƒ½å—æ˜¯ä¸ç°å®çš„ï¼Œå› ä¸ºå…¶æ•°é‡éšç€ç½‘ç»œè§„æ¨¡çš„æ‰©å¤§è€Œè¿…é€Ÿå¢é•¿ã€‚ç½‘ç»œåœ¨æ¯ç§æ¿€æ´»å’Œéæ¿€æ´»å•å…ƒçš„é€‰æ‹©ä¸‹çš„çº¿æ€§è¡Œä¸ºæ„å‘³ç€æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¸¦æœ‰äºŒè¿›åˆ¶å˜é‡çš„çº¿æ€§å…¬å¼æ¥é€šè¿‡MILPå¯¹è®­ç»ƒåçš„ç¥ç»ç½‘ç»œè¿›è¡Œå»ºæ¨¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè§£å†³å¤šç§ä¼˜åŒ–é—®é¢˜ï¼Œä¾‹å¦‚ç¥ç»ç½‘ç»œéªŒè¯é—®é¢˜ã€è¯†åˆ«ç½‘ç»œä¸­æ¯ä¸ªReLUçš„è¾“å‡ºèŒƒå›´ï¼Œä»¥åŠå°†è®­ç»ƒåçš„ç¥ç»ç½‘ç»œå»ºæ¨¡ä¸ºæ›´å¤§å†³ç­–é—®é¢˜çš„ä¸€éƒ¨åˆ†ã€‚åœ¨ç¬¬[4](#S4
    "4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•åœ¨è®­ç»ƒåçš„ç¥ç»ç½‘ç»œä¸Šåˆ¶å®šä¼˜åŒ–é—®é¢˜ï¼Œè¿™äº›å…¬å¼çš„åº”ç”¨ï¼Œä»¥åŠæœç€è·å¾—æ›´å¼ºçš„å…¬å¼çš„è¿›å±•ï¼Œè¿™äº›å…¬å¼èƒ½æ›´å®¹æ˜“åœ°ä¸ç½‘ç»œè§„æ¨¡åŒ¹é…ã€‚'
- en: 'The fifth and sixth questions involve the training procedure of a DNN, where
    linear programming tools have been applied to partially answer them. In SectionÂ [5](#S5
    "5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey"), we overview these developments. In terms of the
    fifth question â€”exploiting polyhedrality in training neural networksâ€” we describe
    algorithms that use the polyhedral geometry induced by activation sets to solve
    training problems. We also cover a recently proposed polyhedral construction that
    can approximately encode multiple training problems at once, showing a strong
    relationship across training problems that arise from different datasets, for
    a fixed architecture. Additionally, we review some recent uses of mixed-integer
    linear programming in the training phase as an alternative to SGD when the weights
    are required to be integer. Regarding the sixth question â€”the incorporation of
    extra structure when trainingâ€” we review multiple approaches that have included
    techniques related to linear programming within SGD to impose a desirable structure
    when training, or to find better step-lengths in the execution of SGD.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬äº”ä¸ªå’Œç¬¬å…­ä¸ªé—®é¢˜æ¶‰åŠæ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…¶ä¸­åº”ç”¨äº†çº¿æ€§è§„åˆ’å·¥å…·æ¥éƒ¨åˆ†å›ç­”è¿™äº›é—®é¢˜ã€‚åœ¨ç¬¬[5](#S5 "5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†è¿™äº›è¿›å±•ã€‚å…³äºç¬¬äº”ä¸ªé—®é¢˜â€”â€”åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶åˆ©ç”¨å¤šé¢ä½“æ€§â€”â€”æˆ‘ä»¬æè¿°äº†ä½¿ç”¨æ¿€æ´»é›†è¯±å¯¼çš„å¤šé¢ä½“å‡ ä½•æ¥è§£å†³è®­ç»ƒé—®é¢˜çš„ç®—æ³•ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†æœ€è¿‘æå‡ºçš„ä¸€ç§å¤šé¢ä½“æ„é€ ï¼Œå®ƒå¯ä»¥å¤§è‡´ç¼–ç å¤šä¸ªè®­ç»ƒé—®é¢˜ï¼Œæ˜¾ç¤ºå‡ºåœ¨å›ºå®šæ¶æ„ä¸‹ï¼Œä¸åŒæ•°æ®é›†äº§ç”Ÿçš„è®­ç»ƒé—®é¢˜ä¹‹é—´çš„å¼ºå…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å›é¡¾äº†ä¸€äº›æœ€è¿‘åœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ä½œä¸ºæ›¿ä»£SGDçš„æ–¹æ³•ï¼Œå½“æƒé‡éœ€è¦ä¸ºæ•´æ•°æ—¶ã€‚å…³äºç¬¬å…­ä¸ªé—®é¢˜â€”â€”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥é¢å¤–ç»“æ„â€”â€”æˆ‘ä»¬å›é¡¾äº†åŒ…æ‹¬ä¸çº¿æ€§è§„åˆ’ç›¸å…³çš„æŠ€æœ¯åœ¨SGDä¸­ä»¥æ–½åŠ æœŸæœ›ç»“æ„æˆ–åœ¨æ‰§è¡ŒSGDæ—¶æ‰¾åˆ°æ›´å¥½æ­¥é•¿çš„å¤šç§æ–¹æ³•ã€‚'
- en: 3 The Linear Regions of a Neural Network
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒºåŸŸ
- en: Every piece of the piecewise linear function modeled by a neural network is
    a linear region, and â€”without loss of generalityâ€” we can think of it as a polyhedron.
    In this section, we define a linear region, exemplify how they can be so numerous,
    and what may affect their count in a neural network. We also discuss the practical
    implications of such insights, as well as other related forms of analyzing the
    ability of a neural network to represent expressive models.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œå»ºæ¨¡çš„åˆ†æ®µçº¿æ€§å‡½æ•°çš„æ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯ä¸€ä¸ªçº¿æ€§åŒºåŸŸï¼Œè€Œä¸”â€”â€”æ²¡æœ‰æŸå¤±çš„ä¸€èˆ¬æ€§â€”â€”æˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºä¸€ä¸ªå¤šé¢ä½“ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†çº¿æ€§åŒºåŸŸï¼Œä¸¾ä¾‹è¯´æ˜äº†å®ƒä»¬ä¸ºä½•å¦‚æ­¤ä¼—å¤šï¼Œä»¥åŠå¯èƒ½å½±å“ç¥ç»ç½‘ç»œä¸­çº¿æ€§åŒºåŸŸæ•°é‡çš„å› ç´ ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†è¿™äº›è§è§£çš„å®é™…æ„ä¹‰ï¼Œä»¥åŠå…¶ä»–ä¸åˆ†æç¥ç»ç½‘ç»œè¡¨ç¤ºèƒ½åŠ›ç›¸å…³çš„å½¢å¼ã€‚
- en: Definition 1
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 1
- en: A linear region corresponds to the set of points from the input space that activates
    the same units along the neural network, and hence can be characterized by the
    set ${\mathbb{S}}^{l}$ of units that are active in each layer $l\in{\mathbb{L}}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§åŒºåŸŸå¯¹åº”äºè¾“å…¥ç©ºé—´ä¸­æ¿€æ´»ç¥ç»ç½‘ç»œä¸­ç›¸åŒå•å…ƒçš„ç‚¹é›†ï¼Œå› æ­¤å¯ä»¥é€šè¿‡åœ¨æ¯å±‚$l\in{\mathbb{L}}$ä¸­æ¿€æ´»çš„å•å…ƒé›†${\mathbb{S}}^{l}$æ¥æè¿°ã€‚
- en: Since a neural network behaves uniformly over a linear region, the latter is
    the smallest finite scale in which we can analyze its behavior. If we restrict
    the domain of a neural network to a linear region ${\mathbb{I}}\subseteq\mathbb{R}^{n_{0}}$,
    then the neural network behaves as an affine transformation ${\bm{y}}_{\mathbb{I}}:{\mathbb{I}}\rightarrow\mathbb{R}^{n_{L}}$
    of the form ${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$ with a
    matrix ${\bm{T}}\in\mathbb{R}^{n_{L}\times n_{0}}$ and a vector ${\bm{t}}\in\mathbb{R}^{n_{L}}$
    that are directly defined by the network parameters and the set of neurons that
    are activated by any input ${\bm{x}}\in{\mathbb{I}}$. For a small perturbationÂ $\varepsilon$
    to some input $\overline{{\bm{x}}}\in{\mathbb{I}}$ such that $\overline{{\bm{x}}}+\varepsilon\in{\mathbb{I}}$,
    the network output for $\bar{x}+\varepsilon$ is given by ${\bm{y}}_{\mathbb{I}}(\overline{{\bm{x}}}+\varepsilon)$.
    While it is possible that two adjacent regions defined in such way correspond
    to the same affine transformation, thinking of each linear region as having a
    distinct signature of active units makes it easier to analyze them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç¥ç»ç½‘ç»œåœ¨ä¸€ä¸ªçº¿æ€§åŒºåŸŸå†…è¡¨ç°ä¸€è‡´ï¼Œå› æ­¤çº¿æ€§åŒºåŸŸæ˜¯åˆ†æå…¶è¡Œä¸ºçš„æœ€å°æœ‰é™å°ºåº¦ã€‚å¦‚æœæˆ‘ä»¬å°†ç¥ç»ç½‘ç»œçš„å®šä¹‰åŸŸé™åˆ¶åœ¨ä¸€ä¸ªçº¿æ€§åŒºåŸŸ ${\mathbb{I}}\subseteq\mathbb{R}^{n_{0}}$ï¼Œé‚£ä¹ˆç¥ç»ç½‘ç»œè¡¨ç°ä¸ºå½¢å¼ä¸º
    ${\bm{y}}_{\mathbb{I}}:{\mathbb{I}}\rightarrow\mathbb{R}^{n_{L}}$ çš„ä»¿å°„å˜æ¢ ${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$ï¼Œå…¶ä¸­çŸ©é˜µ
    ${\bm{T}}\in\mathbb{R}^{n_{L}\times n_{0}}$ å’Œå‘é‡ ${\bm{t}}\in\mathbb{R}^{n_{L}}$
    ç›´æ¥ç”±ç½‘ç»œå‚æ•°å’Œç”±ä»»ä½•è¾“å…¥ ${\bm{x}}\in{\mathbb{I}}$ æ¿€æ´»çš„ç¥ç»å…ƒé›†åˆå®šä¹‰ã€‚å¯¹äºæŸä¸ªè¾“å…¥ $\overline{{\bm{x}}}\in{\mathbb{I}}$
    çš„å°æ‰°åŠ¨ $\varepsilon$ï¼Œä½¿å¾— $\overline{{\bm{x}}}+\varepsilon\in{\mathbb{I}}$ï¼Œç½‘ç»œè¾“å‡ºä¸º ${\bm{y}}_{\mathbb{I}}(\overline{{\bm{x}}}+\varepsilon)$ã€‚è™½ç„¶ä»¥è¿™ç§æ–¹å¼å®šä¹‰çš„ä¸¤ä¸ªç›¸é‚»åŒºåŸŸå¯èƒ½å¯¹åº”äºç›¸åŒçš„ä»¿å°„å˜æ¢ï¼Œä½†å°†æ¯ä¸ªçº¿æ€§åŒºåŸŸè§†ä¸ºå…·æœ‰ç‹¬ç‰¹çš„æ¿€æ´»å•å…ƒç­¾åä½¿å¾—åˆ†æå®ƒä»¬æ›´å®¹æ˜“ã€‚
- en: The number of linear regions defined by a neural network is one form with which
    we can measure the complexity of the models that it can represent (Bengio, [2009](#bib.bib18)).
    Hence, if a more complex model is desired, we may want to design a neural network
    that can potentially define more linear regions. On the one hand, the number of
    linear regions may grow exponentially on the depth of a neural network. On the
    other hand, such a number depends on the interplay between network parameters
    and hyperparameters. As we consider how the inputs from adjacent linear regions
    are evaluated, the change to the affine transformation can be characterized in
    algebraic and geometric terms. Understanding such changes may help us grasp how
    a neural network is capable of telling its inputs apart, including what are the
    sources of the complexity of the model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œå®šä¹‰çš„çº¿æ€§åŒºåŸŸæ•°é‡æ˜¯ä¸€ç§è¡¡é‡å…¶è¡¨ç¤ºæ¨¡å‹å¤æ‚åº¦çš„å½¢å¼ï¼ˆBengioï¼Œ[2009](#bib.bib18)ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœéœ€è¦æ›´å¤æ‚çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›è®¾è®¡ä¸€ä¸ªå¯ä»¥å®šä¹‰æ›´å¤šçº¿æ€§åŒºåŸŸçš„ç¥ç»ç½‘ç»œã€‚ä¸€æ–¹é¢ï¼Œçº¿æ€§åŒºåŸŸçš„æ•°é‡å¯èƒ½éšç€ç¥ç»ç½‘ç»œæ·±åº¦çš„å¢åŠ è€Œå‘ˆæŒ‡æ•°å¢é•¿ã€‚å¦ä¸€æ–¹é¢ï¼Œè¿™ç§æ•°é‡å–å†³äºç½‘ç»œå‚æ•°å’Œè¶…å‚æ•°ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚è€ƒè™‘åˆ°å¦‚ä½•è¯„ä¼°ç›¸é‚»çº¿æ€§åŒºåŸŸçš„è¾“å…¥ï¼Œå¯¹ä»¿å°„å˜æ¢çš„å˜åŒ–å¯ä»¥ç”¨ä»£æ•°å’Œå‡ ä½•æœ¯è¯­æ¥æè¿°ã€‚ç†è§£è¿™äº›å˜åŒ–å¯èƒ½æœ‰åŠ©äºæˆ‘ä»¬æŒæ¡ç¥ç»ç½‘ç»œå¦‚ä½•åŒºåˆ†å…¶è¾“å…¥ï¼ŒåŒ…æ‹¬æ¨¡å‹å¤æ‚åº¦çš„æ¥æºã€‚
- en: For neural networks in which the activation function is not piecewise linear,
    Bianchini and Scarselli ([2014](#bib.bib29)) have used more elaborate topological
    measures to compare the expressiveness of shallow and deep neural networks. Hu
    etÂ al. ([2020b](#bib.bib155)) followed a closer approach by producing a linear
    approximation neural network in which the number of linear regions can be counted.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¿€æ´»å‡½æ•°ä¸æ˜¯åˆ†æ®µçº¿æ€§çš„ç¥ç»ç½‘ç»œï¼ŒBianchini å’Œ Scarselliï¼ˆ[2014](#bib.bib29)ï¼‰ä½¿ç”¨äº†æ›´å¤æ‚çš„æ‹“æ‰‘åº¦é‡æ¥æ¯”è¾ƒæµ…å±‚å’Œæ·±å±‚ç¥ç»ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ã€‚Hu
    ç­‰äººï¼ˆ[2020b](#bib.bib155)ï¼‰é€šè¿‡ç”Ÿæˆä¸€ä¸ªçº¿æ€§è¿‘ä¼¼ç¥ç»ç½‘ç»œæ¥è·Ÿè¿›ï¼Œå…¶ä¸­çº¿æ€§åŒºåŸŸçš„æ•°é‡å¯ä»¥è¢«è®¡æ•°ã€‚
- en: 3.1 The combinatorial aspect of linear regions
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 çº¿æ€§åŒºåŸŸçš„ç»„åˆæ–¹é¢
- en: One of the most striking aspects about analyzing a neural network in terms of
    its linear regions is how quickly such number grows. Early work on this topic
    byÂ Pascanu etÂ al. ([2014](#bib.bib243)) and MontÃºfar etÂ al. ([2014](#bib.bib224))
    have drawn two important observations. First, that it is possible to construct
    simple deep neural networks with a number of linear regions that grows exponentially
    in the depth. Second, that the number of linear regions can be exponential in
    the number of neurons alone.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æç¥ç»ç½‘ç»œçº¿æ€§åŒºåŸŸçš„ä¸€ä¸ªæœ€å¼•äººæ³¨ç›®çš„æ–¹é¢æ˜¯è¿™ç§æ•°é‡å¢é•¿çš„é€Ÿåº¦ã€‚Pascanu ç­‰äººï¼ˆ[2014](#bib.bib243)ï¼‰å’Œ MontÃºfar ç­‰äººï¼ˆ[2014](#bib.bib224)ï¼‰åœ¨æ—©æœŸçš„å·¥ä½œä¸­å¾—å‡ºäº†ä¸¤ä¸ªé‡è¦çš„è§‚å¯Ÿç»“æœã€‚é¦–å…ˆï¼Œå¯èƒ½æ„é€ å‡ºç®€å•çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå…¶çº¿æ€§åŒºåŸŸæ•°é‡éšç€æ·±åº¦çš„å¢åŠ è€ŒæŒ‡æ•°å¢é•¿ã€‚å…¶æ¬¡ï¼Œçº¿æ€§åŒºåŸŸçš„æ•°é‡ä»…åœ¨ç¥ç»å…ƒçš„æ•°é‡ä¸Šä¹Ÿå¯ä»¥æ˜¯æŒ‡æ•°çº§çš„ã€‚
- en: 'The first observation comes from analyzing the role of ReLUs in a very simple
    setting. Namely, that of a neural network in which we regard every layer as having
    a single input in the $[0,1]$ domain, which is produced by combining the outputs
    of the units from the preceding layer, as illustrated by ExampleÂ [1](#Thmexample1
    "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€ä¸ªè§‚å¯Ÿæ¥è‡ªäºåœ¨ä¸€ä¸ªéå¸¸ç®€å•çš„è®¾ç½®ä¸­åˆ†æ ReLU çš„ä½œç”¨ã€‚å³åœ¨ä¸€ä¸ªç¥ç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬å°†æ¯ä¸€å±‚è§†ä¸ºæœ‰ä¸€ä¸ªåœ¨ $[0,1]$ èŒƒå›´å†…çš„è¾“å…¥ï¼Œè¯¥è¾“å…¥æ˜¯é€šè¿‡ç»„åˆå‰ä¸€å±‚å•å…ƒçš„è¾“å‡ºç”Ÿæˆçš„ï¼Œå¦‚ç¤ºä¾‹
    [1](#Thmexample1 "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£
    3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey") æ‰€ç¤ºã€‚'
- en: Example 1
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ 1
- en: 'Consider a neural network with input $x$ from the domain $[0,1]$ and layers
    having 4 neurons with ReLU activation. For the first layer, assume that the output
    of the neurons are given by the following functions: $f_{1}(x)=\max\{4x,0\}$,
    $f_{2}(x)=\max\{8x-2,0\}$, $f_{3}(x)=\max\{6.5x-3.25,0\}$, and $f_{4}(x)=\max\{12.5x-11.25,0\}$.
    In other words, ${\bm{h}}^{1}_{i}=f_{i}(x)~{}\forall i\in\{1,2,3,4\}$. For the
    subsequent layers, assume that the outputs coming from the previous layer are
    combined through the function $F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$, which
    substitutes $x$ as the input to the next layer; upon which the same set of functions
    $\{f_{i}(x)\}_{i=1}^{4}$ defines the output of the next layer. In other words,
    ${\bm{h}}^{l}_{i}=f_{i}(F({\bm{h}}^{l-1}))=f_{i}({h}_{1}^{l-1}-{h}_{2}^{l-1}+{h}_{3}^{l-1}-{h}_{4}^{l-1})~{}\forall
    i\in\{1,2,3,4\},l\in{\mathbb{L}}\setminus\{1\}$.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªè¾“å…¥ä¸º $x$ çš„ç¥ç»ç½‘ç»œï¼Œè¾“å…¥åŸŸä¸º $[0,1]$ï¼Œä¸”å±‚å…·æœ‰ 4 ä¸ªå¸¦æœ‰ ReLU æ¿€æ´»çš„ç¥ç»å…ƒã€‚å¯¹äºç¬¬ä¸€å±‚ï¼Œå‡è®¾ç¥ç»å…ƒçš„è¾“å‡ºç”±ä»¥ä¸‹å‡½æ•°ç»™å‡ºï¼š$f_{1}(x)=\max\{4x,0\}$ï¼Œ$f_{2}(x)=\max\{8x-2,0\}$ï¼Œ$f_{3}(x)=\max\{6.5x-3.25,0\}$ï¼Œå’Œ
    $f_{4}(x)=\max\{12.5x-11.25,0\}$ã€‚æ¢å¥è¯è¯´ï¼Œ${\bm{h}}^{1}_{i}=f_{i}(x)~{}\forall i\in\{1,2,3,4\}$ã€‚å¯¹äºéšåçš„å±‚ï¼Œå‡è®¾æ¥è‡ªå‰ä¸€å±‚çš„è¾“å‡ºé€šè¿‡å‡½æ•°
    $F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$ è¿›è¡Œç»„åˆï¼Œè¯¥å‡½æ•°å°† $x$ æ›¿æ¢ä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼›ç„¶ååŒæ ·çš„å‡½æ•°é›† $\{f_{i}(x)\}_{i=1}^{4}$
    å®šä¹‰äº†ä¸‹ä¸€å±‚çš„è¾“å‡ºã€‚æ¢å¥è¯è¯´ï¼Œ${\bm{h}}^{l}_{i}=f_{i}(F({\bm{h}}^{l-1}))=f_{i}({h}_{1}^{l-1}-{h}_{2}^{l-1}+{h}_{3}^{l-1}-{h}_{4}^{l-1})~{}\forall
    i\in\{1,2,3,4\},l\in{\mathbb{L}}\setminus\{1\}$ã€‚
- en: When the output of the units in the first layer is combined as $F(x)$, we obtain
    a zigzagging function with 4 slopes in the $[0,1]$ domain, each of which defining
    a bijection between segments of the input â€”namely, $[0,0.25]$, $[0.25,0.5]$, $[0.5,0.9]$,
    and $[0.9,1.0]$â€” and the image $[0,1]$. The effect of repeating such structure
    in the second layer is that of composing $F(x)$ with itself, with 4 slopes being
    produced within each of those 4 initial segments. Hence, the number of slopes
    â€”and therefore of linear regionsâ€” in the output of such a neural network with
    $L$ layers of activation functions is $4^{L}$, which implies an exponential growth
    on depth.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç¬¬ä¸€å±‚å•å…ƒçš„è¾“å‡ºç»„åˆä¸º $F(x)$ æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªåœ¨ $[0,1]$ èŒƒå›´å†…å…·æœ‰ 4 ä¸ªæ–œç‡çš„é”¯é½¿çŠ¶å‡½æ•°ï¼Œæ¯ä¸ªæ–œç‡å®šä¹‰äº†è¾“å…¥åŒºé—´çš„ä¸€ä¸ªåŒå°„ â€”å³
    $[0,0.25]$ã€$[0.25,0.5]$ã€$[0.5,0.9]$ å’Œ $[0.9,1.0]$â€” å’Œå›¾åƒ $[0,1]$ ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚å°†è¿™ç§ç»“æ„åœ¨ç¬¬äºŒå±‚ä¸­é‡å¤çš„æ•ˆæœæ˜¯å°†
    $F(x)$ ä¸è‡ªèº«è¿›è¡Œç»„åˆï¼Œåœ¨æ¯ä¸ªè¿™ 4 ä¸ªåˆå§‹åŒºé—´å†…äº§ç”Ÿ 4 ä¸ªæ–œç‡ã€‚å› æ­¤ï¼Œè¿™ç§å…·æœ‰ $L$ å±‚æ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸­çš„æ–œç‡æ•°é‡ â€”ä¹Ÿå°±æ˜¯çº¿æ€§åŒºåŸŸçš„æ•°é‡â€”
    æ˜¯ $4^{L}$ï¼Œè¿™æ„å‘³ç€æ·±åº¦å‘ˆæŒ‡æ•°å¢é•¿ã€‚
- en: 'The network structure and the parameters of the neurons in the first two layers
    are illustrated in FigureÂ [3](#S3.F3 "Figure 3 â€£ 3.1 The combinatorial aspect
    of linear regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey"); the set of functions $\{f_{i}(x)\}_{i=1}^{4}$
    and the combined outputs of the first two layers â€”$F(x)$ and $F(F(x))$â€” are illustrated
    in FigureÂ [4](#S3.F4 "Figure 4 â€£ 3.1 The combinatorial aspect of linear regions
    â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç½‘ç»œç»“æ„å’Œå‰ä¸¤å±‚ä¸­ç¥ç»å…ƒçš„å‚æ•°å¦‚å›¾ [3](#S3.F3 "Figure 3 â€£ 3.1 The combinatorial aspect of linear
    regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey") æ‰€ç¤ºï¼›å‡½æ•°é›† $\{f_{i}(x)\}_{i=1}^{4}$ å’Œå‰ä¸¤å±‚çš„ç»„åˆè¾“å‡º â€”$F(x)$
    å’Œ $F(F(x))$â€” å¦‚å›¾ [4](#S3.F4 "Figure 4 â€£ 3.1 The combinatorial aspect of linear
    regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey") æ‰€ç¤ºã€‚'
- en: 'In ExampleÂ [1](#Thmexample1 "Example 1 â€£ 3.1 The combinatorial aspect of linear
    regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey"), every neuron changes the slope of the resulting
    function once it becomes active, in which we purposely alternate between positive
    and negative slopes once the function reaches either 0 or 1, respectively. By
    selecting the network parameters accordingly, MontÃºfar etÂ al. ([2014](#bib.bib224))
    were the first to show that a layer with $n$ ReLUs can be used to create a zigzagging
    function with $n$ slopes on the $[0,1]$ domain, with the image along every slope
    also corresponding to the interval $[0,1]$. Consequently, stacking $L$ of such
    layers results in a neural network with $n^{L}$ linear regions.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ç¤ºä¾‹Â [1](#Thmexample1 "Example 1 â€£ 3.1 The combinatorial aspect of linear regions
    â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")ä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒåœ¨æ¿€æ´»åä¼šæ”¹å˜ç»“æœå‡½æ•°çš„æ–œç‡ï¼Œå…¶ä¸­æˆ‘ä»¬æ•…æ„åœ¨å‡½æ•°åˆ†åˆ«è¾¾åˆ°0æˆ–1æ—¶ï¼Œåœ¨æ­£æ–œç‡å’Œè´Ÿæ–œç‡ä¹‹é—´äº¤æ›¿ã€‚é€šè¿‡ç›¸åº”é€‰æ‹©ç½‘ç»œå‚æ•°ï¼ŒMontÃºfarç­‰äººï¼ˆ[2014](#bib.bib224)ï¼‰é¦–æ¬¡å±•ç¤ºäº†ä¸€ä¸ªå…·æœ‰$n$ä¸ªReLUçš„å±‚å¯ä»¥ç”¨æ¥åˆ›å»ºä¸€ä¸ªåœ¨$[0,1]$åŸŸä¸Šå…·æœ‰$n$ä¸ªæ–œç‡çš„é”¯é½¿çŠ¶å‡½æ•°ï¼Œæ¯ä¸ªæ–œç‡çš„æ˜ åƒä¹Ÿå¯¹åº”äºåŒºé—´$[0,1]$ã€‚å› æ­¤ï¼Œå †å $L$ä¸ªè¿™æ ·çš„å±‚ä¼šå¯¼è‡´ä¸€ä¸ªå…·æœ‰$n^{L}$ä¸ªçº¿æ€§åŒºåŸŸçš„ç¥ç»ç½‘ç»œã€‚'
- en: '![Refer to caption](img/0bac8f009269c114fb6255ef917600fd.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜æ–‡å­—](img/0bac8f009269c114fb6255ef917600fd.png)'
- en: 'Figure 3: Mapping from the input $x\in[0,1]$ to the intermediary output ${\bm{h}}^{2}\in[0,1]^{4}$
    through the first two layers of a neural network in which the number of linear
    regions growths exponentially on the depth, as described in ExampleÂ [1](#Thmexample1
    "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").
    The parameters of subsequent layers are the same as those in the second layer.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾3ï¼šé€šè¿‡ç¥ç»ç½‘ç»œçš„å‰ä¸¤å±‚å°†è¾“å…¥$x\in[0,1]$æ˜ å°„åˆ°ä¸­é—´è¾“å‡º${\bm{h}}^{2}\in[0,1]^{4}$ï¼Œå…¶ä¸­çº¿æ€§åŒºåŸŸçš„æ•°é‡éšæ·±åº¦å‘ˆæŒ‡æ•°å¢é•¿ï¼Œå¦‚ç¤ºä¾‹Â [1](#Thmexample1
    "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")ä¸­æ‰€è¿°ã€‚åç»­å±‚çš„å‚æ•°ä¸ç¬¬äºŒå±‚çš„å‚æ•°ç›¸åŒã€‚'
- en: '![Refer to caption](img/285adf5284d089caf1d99c5051b857ed.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜æ–‡å­—](img/285adf5284d089caf1d99c5051b857ed.png)'
- en: 'Figure 4: Set of activation functions $\{f_{i}(x)\}_{i=1}^{4}$ of the units
    in the first layer and combined outputs of the first two layers â€”$F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$
    for the first and $F(F(x))$ for the secondâ€” of a neural network in which the number
    of linear regions grows exponentially on the depth, as described in ExampleÂ [1](#Thmexample1
    "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾4ï¼šç¬¬ä¸€å±‚ä¸­å•ä½çš„æ¿€æ´»å‡½æ•°é›†åˆ$\{f_{i}(x)\}_{i=1}^{4}$ä»¥åŠå‰ä¸¤å±‚çš„ç»„åˆè¾“å‡ºâ€”â€”ç¬¬ä¸€ä¸ªæ˜¯$F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$ï¼Œç¬¬äºŒä¸ªæ˜¯$F(F(x))$â€”â€”åœ¨ä¸€ä¸ªç¥ç»ç½‘ç»œä¸­ï¼Œçº¿æ€§åŒºåŸŸçš„æ•°é‡éšç€æ·±åº¦å‘ˆæŒ‡æ•°å¢é•¿ï¼Œå¦‚ç¤ºä¾‹Â [1](#Thmexample1
    "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")ä¸­æ‰€è¿°ã€‚'
- en: 'The second observation â€”that the number of linear regions can grow exponentially
    in the number of neurons aloneâ€” comes from the interplay between the parts of
    the input space in which each the units are active, especially in higher-dimensional
    spaces. This is based on some geometric observations that we discuss in SectionÂ [3.3](#S3.SS3
    "3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey"). Even for a *shallow*
    network â€”i.e., the number of layers being $L=1$â€” such a number of linear regions
    may approach $2^{n}$, which corresponds to every possible activation set ${\mathbb{S}}\subseteq\{1,\ldots,n\}$
    defining a nonempty linear region. However, as we discuss later, that is not always
    the case due to architectural choices such as the number of layers and their width.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬äºŒä¸ªè§‚å¯Ÿç»“æœâ€”â€”å³çº¿æ€§åŒºåŸŸçš„æ•°é‡ä»…ä»…åœ¨ç¥ç»å…ƒæ•°é‡ä¸Šå°±å¯ä»¥å‘ˆæŒ‡æ•°å¢é•¿â€”â€”æ¥æºäºæ¯ä¸ªå•ä½åœ¨è¾“å…¥ç©ºé—´ä¸­æ´»è·ƒéƒ¨åˆ†çš„ç›¸äº’ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç»´ç©ºé—´ä¸­ã€‚è¿™æ˜¯åŸºäºæˆ‘ä»¬åœ¨ç¬¬[3.3èŠ‚](#S3.SS3
    "3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")è®¨è®ºçš„ä¸€äº›å‡ ä½•è§‚å¯Ÿã€‚å³ä½¿å¯¹äºä¸€ä¸ª*æµ…å±‚*ç½‘ç»œâ€”â€”å³å±‚æ•°ä¸º$L=1$â€”â€”è¿™æ ·çš„çº¿æ€§åŒºåŸŸæ•°é‡å¯èƒ½æ¥è¿‘$2^{n}$ï¼Œè¿™å¯¹åº”äºæ¯ä¸ªå¯èƒ½çš„æ¿€æ´»é›†${\mathbb{S}}\subseteq\{1,\ldots,n\}$å®šä¹‰ä¸€ä¸ªéç©ºçš„çº¿æ€§åŒºåŸŸã€‚ç„¶è€Œï¼Œæ­£å¦‚æˆ‘ä»¬åé¢è®¨è®ºçš„ï¼Œç”±äºè¯¸å¦‚å±‚æ•°å’Œå®½åº¦ç­‰æ¶æ„é€‰æ‹©ï¼Œè¿™ç§æƒ…å†µå¹¶ä¸æ€»æ˜¯æˆç«‹ã€‚'
- en: 3.2 The algebra of linear regions
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 çº¿æ€§åŒºåŸŸçš„ä»£æ•°
- en: 'Given the activation sets $\{{\mathbb{S}}^{l}\}_{l\in{\mathbb{L}}}$ denoting
    which neurons are active for each layer of the neural network, we can explicitly
    describe the affine transformation ${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$
    associated with the corresponding linear region ${\mathbb{I}}$. For every activation
    set ${\mathbb{S}}^{l}$, layer $l$ defines an affine transformation of the form
    $\Omega^{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$, where $\Omega^{{\mathbb{S}}^{l}}$
    is a diagonal $n_{l}\times n_{l}$ matrix in which $\Omega^{{\mathbb{S}}^{l}}_{ii}=1$
    if $i\in{\mathbb{S}}^{l}$ and $\Omega^{{\mathbb{S}}^{l}}_{ii}=0$ otherwise. Hence,
    the matrix ${\bm{T}}$ and vector ${\bm{t}}$ are as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šæ¿€æ´»é›†$\{{\mathbb{S}}^{l}\}_{l\in{\mathbb{L}}}$ï¼Œå®ƒè¡¨ç¤ºç¥ç»ç½‘ç»œæ¯ä¸€å±‚å“ªäº›ç¥ç»å…ƒæ˜¯æ¿€æ´»çš„ï¼Œæˆ‘ä»¬å¯ä»¥æ˜ç¡®æè¿°ä¸ç›¸åº”çº¿æ€§åŒºåŸŸ${\mathbb{I}}$ç›¸å…³çš„ä»¿å°„å˜æ¢${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$ã€‚å¯¹äºæ¯ä¸ªæ¿€æ´»é›†${\mathbb{S}}^{l}$ï¼Œç¬¬$l$å±‚å®šä¹‰äº†å¦‚ä¸‹å½¢å¼çš„ä»¿å°„å˜æ¢$\Omega^{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$ï¼Œå…¶ä¸­$\Omega^{{\mathbb{S}}^{l}}$æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µ$n_{l}\times
    n_{l}$ï¼Œå…¶ä¸­å¦‚æœ$i\in{\mathbb{S}}^{l}$ï¼Œåˆ™$\Omega^{{\mathbb{S}}^{l}}_{ii}=1$ï¼Œå¦åˆ™$\Omega^{{\mathbb{S}}^{l}}_{ii}=0$ã€‚å› æ­¤ï¼ŒçŸ©é˜µ${\bm{T}}$å’Œå‘é‡${\bm{t}}$å¦‚ä¸‹ï¼š
- en: '|  | ${\bm{T}}=\prod_{l=1}^{L}\Omega^{{\mathbb{S}}^{l}}{\bm{W}}^{l},$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{T}}=\prod_{l=1}^{L}\Omega^{{\mathbb{S}}^{l}}{\bm{W}}^{l},$ |  |'
- en: '|  | ${\bm{t}}=\sum_{l_{1}=1}^{L}\left(\prod_{l_{2}=l_{1}+1}^{L}\Omega^{{\mathbb{S}}^{l_{2}}}{\bm{W}}^{l_{2}}\right)\Omega^{{\mathbb{S}}^{l_{1}}}{\bm{b}}^{l_{1}}.$
    |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{t}}=\sum_{l_{1}=1}^{L}\left(\prod_{l_{2}=l_{1}+1}^{L}\Omega^{{\mathbb{S}}^{l_{2}}}{\bm{W}}^{l_{2}}\right)\Omega^{{\mathbb{S}}^{l_{1}}}{\bm{b}}^{l_{1}}.$
    |  |'
- en: On a side note, Takai etÂ al. ([2021](#bib.bib302)) proposed a related metric
    for networks modeling piecewise linear functions by counting the number of distinct
    functions among linear regions upon equivalence through isometric affine transformation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿æä¸€ä¸‹ï¼ŒTakaiç­‰äººï¼ˆ[2021](#bib.bib302)ï¼‰æå‡ºäº†ä¸€ç§ç›¸å…³çš„åº¦é‡ï¼Œç”¨äºé€šè¿‡è®¡æ•°åœ¨ç­‰è·ä»¿å°„å˜æ¢ä¸‹çº¿æ€§åŒºåŸŸä¹‹é—´çš„ä¸åŒå‡½æ•°æ•°é‡ï¼Œä»¥å¯¹æ¨¡å‹åŒ–åˆ†æ®µçº¿æ€§å‡½æ•°çš„ç½‘ç»œè¿›è¡Œåº¦é‡ã€‚
- en: 'Each linear region is associated with a polyhedron, and we can describe the
    union of polyhedra $\mathcal{D}$ on the space $({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$
    that covers the entire input space $x$ of the neural network as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªçº¿æ€§åŒºåŸŸéƒ½ä¸ä¸€ä¸ªå¤šé¢ä½“ç›¸å…³ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹æè¿°è¦†ç›–ç¥ç»ç½‘ç»œæ•´ä¸ªè¾“å…¥ç©ºé—´$x$çš„ç©ºé—´$({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$ä¸Šçš„å¤šé¢ä½“çš„å¹¶é›†$\mathcal{D}$ï¼š
- en: '|  | <math   alttext="\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&amp;\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&amp;\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&amp;\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&amp;\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\'
- en: '{\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq 0&amp;\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '{\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq 0&amp;\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\'
- en: h_{i}^{l}=0&amp;\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\end{array}\right)."
    display="block"><semantics ><mrow  ><mrow ><mi >ğ’Ÿ</mi><mo rspace="0.111em" >=</mo><mrow
    ><munder  ><mo movablelimits="false" rspace="0em"  >â‹</mo><mrow ><mrow ><mo stretchy="false"
    >(</mo><msup  ><mi >ğ•Š</mi><mn >1</mn></msup><mo >,</mo><mi mathvariant="normal"
    >â€¦</mi><mo  >,</mo><msup ><mi >ğ•Š</mi><mi  >L</mi></msup><mo stretchy="false"  >)</mo></mrow><mo
    >âŠ†</mo><mrow ><mrow  ><mo stretchy="false"  >{</mo><mn >1</mn><mo >,</mo><mi mathvariant="normal"
    >â€¦</mi><mo  >,</mo><msub ><mi >n</mi><mn >1</mn></msub><mo rspace="0.055em" stretchy="false"
    >}</mo></mrow><mo rspace="0.222em"  >Ã—</mo><mi mathvariant="normal"  >â€¦</mi><mo
    lspace="0.222em" rspace="0.222em"  >Ã—</mo><mrow ><mo stretchy="false" >{</mo><mn
    >1</mn><mo  >,</mo><mi mathvariant="normal"  >â€¦</mi><mo >,</mo><msub ><mi  >n</mi><mi
    >L</mi></msub><mo stretchy="false"  >}</mo></mrow></mrow></mrow></munder><mrow
    ><mo >(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  ><mrow ><mrow ><mrow  ><msubsup ><mi >ğ’˜</mi><mi >i</mi><mi >l</mi></msubsup><mo
    lspace="0.222em" rspace="0.222em"  >â‹…</mo><msup ><mi >ğ’‰</mi><mrow ><mi >l</mi><mo
    >âˆ’</mo><mn >1</mn></mrow></msup></mrow><mo >+</mo><msubsup ><mi  >b</mi><mi >i</mi><mi
    >l</mi></msubsup></mrow><mo >â‰¥</mo><mn >0</mn></mrow></mtd><mtd ><mrow  ><mrow
    ><mrow ><mo rspace="0.167em" >âˆ€</mo><mi >l</mi></mrow><mo >âˆˆ</mo><mi >ğ•ƒ</mi></mrow><mo
    >,</mo><mrow ><mi  >i</mi><mo >âˆˆ</mo><msup ><mi >ğ•Š</mi><mi >l</mi></msup></mrow></mrow></mtd></mtr><mtr
    ><mtd  ><mrow ><msubsup ><mi  >h</mi><mi >i</mi><mi >l</mi></msubsup><mo >=</mo><mrow
    ><mrow  ><msubsup ><mi >ğ’˜</mi><mi >i</mi><mi >l</mi></msubsup><mo lspace="0.222em"
    rspace="0.222em"  >â‹…</mo><msup ><mi >ğ’‰</mi><mrow ><mi >l</mi><mo >âˆ’</mo><mn >1</mn></mrow></msup></mrow><mo
    >+</mo><msubsup ><mi  >b</mi><mi >i</mi><mi >l</mi></msubsup></mrow></mrow></mtd><mtd
    ><mrow ><mrow  ><mrow ><mo rspace="0.167em" >âˆ€</mo><mi >l</mi></mrow><mo >âˆˆ</mo><mi
    >ğ•ƒ</mi></mrow><mo >,</mo><mrow ><mi >i</mi><mo >âˆˆ</mo><msup ><mi >ğ•Š</mi><mi >l</mi></msup></mrow></mrow></mtd></mtr><mtr
    ><mtd  ><mrow ><mrow ><mrow  ><msubsup ><mi >ğ’˜</mi><mi >i</mi><mi >l</mi></msubsup><mo
    lspace="0.222em" rspace="0.222em"  >â‹…</mo><msup ><mi >h</mi><mrow ><mi >l</mi><mo
    >âˆ’</mo><mn >1</mn></mrow></msup></mrow><mo >+</mo><msubsup ><mi  >b</mi><mi >i</mi><mi
    >l</mi></msubsup></mrow><mo >â‰¤</mo><mn >0</mn></mrow></mtd><mtd ><mrow  ><mrow
    ><mrow ><mo rspace="0.167em" >âˆ€</mo><mi >l</mi></mrow><mo >âˆˆ</mo><mi >ğ•ƒ</mi></mrow><mo
    >,</mo><mrow ><mi  >i</mi><mo >âˆ‰</mo><msup ><mi >ğ•Š</mi><mi >l</mi></msup></mrow></mrow></mtd></mtr><mtr
    ><mtd  ><mrow ><msubsup ><mi  >h</mi><mi >i</mi><mi >l</mi></msubsup><mo >=</mo><mn
    >0</mn></mrow></mtd><mtd ><mrow  ><mrow ><mrow ><mo rspace="0.167em" >âˆ€</mo><mi
    >l</mi></mrow><mo >âˆˆ</mo><mi >ğ•ƒ</mi></mrow><mo >,</mo><mrow ><mi  >i</mi><mo >âˆ‰</mo><msup
    ><mi >ğ•Š</mi><mi >l</mi></msup></mrow></mrow></mtd></mtr></mtable><mo >)</mo></mrow></mrow></mrow><mo
    lspace="0em"  >.</mo></mrow><annotation-xml encoding="MathML-Content" ><apply
    ><ci  >ğ’Ÿ</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><vector  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ•Š</ci><cn
    type="integer" >1</cn></apply><ci >â€¦</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ğ•Š</ci><ci  >ğ¿</ci></apply></vector><apply ><set ><cn type="integer"  >1</cn><ci
    >â€¦</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘›</ci><cn type="integer"
    >1</cn></apply></set><ci >â€¦</ci><set  ><cn type="integer"  >1</cn><ci >â€¦</ci><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘›</ci><ci >ğ¿</ci></apply></set></apply></apply></apply><matrix
    ><matrixrow ><apply  ><apply ><apply ><ci  >â‹…</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ’˜</ci><ci >ğ‘–</ci></apply><ci
    >ğ‘™</ci></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >ğ’‰</ci><apply
    ><ci >ğ‘™</ci><cn type="integer"  >1</cn></apply></apply></apply><apply ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘–</ci></apply><ci >ğ‘™</ci></apply></apply><cn type="integer"  >0</cn></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="latexml" >for-all</csymbol><ci >ğ‘™</ci></apply><ci >ğ•ƒ</ci></apply><apply ><ci  >ğ‘–</ci><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >ğ•Š</ci><ci >ğ‘™</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >â„</ci><ci >ğ‘–</ci></apply><ci >ğ‘™</ci></apply><apply
    ><apply ><ci  >â‹…</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ’˜</ci><ci >ğ‘–</ci></apply><ci
    >ğ‘™</ci></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >ğ’‰</ci><apply
    ><ci >ğ‘™</ci><cn type="integer"  >1</cn></apply></apply></apply><apply ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘–</ci></apply><ci >ğ‘™</ci></apply></apply></apply><apply ><csymbol
    cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol cd="latexml"
    >for-all</csymbol><ci >ğ‘™</ci></apply><ci >ğ•ƒ</ci></apply><apply ><ci  >ğ‘–</ci><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >ğ•Š</ci><ci >ğ‘™</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><apply ><ci >â‹…</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ’˜</ci><ci >ğ‘–</ci></apply><ci
    >ğ‘™</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >â„</ci><apply
    ><ci >ğ‘™</ci><cn type="integer" >1</cn></apply></apply></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘–</ci></apply><ci >ğ‘™</ci></apply></apply><cn type="integer" >0</cn></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="latexml" >for-all</csymbol><ci >ğ‘™</ci></apply><ci >ğ•ƒ</ci></apply><apply ><ci  >ğ‘–</ci><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >ğ•Š</ci><ci >ğ‘™</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >â„</ci><ci >ğ‘–</ci></apply><ci >ğ‘™</ci></apply><cn
    type="integer"  >0</cn></apply><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="latexml" >for-all</csymbol><ci >ğ‘™</ci></apply><ci >ğ•ƒ</ci></apply><apply
    ><ci  >ğ‘–</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >ğ•Š</ci><ci
    >ğ‘™</ci></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ {\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq
    0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\ h_{i}^{l}=0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\end{array}\right).</annotation></semantics></math>
    |  |
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: \(\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ {\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq
    0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\ h_{i}^{l}=0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\end{array}\right).\)
- en: 'Such partitioning entails an overlap between adjacent linear regions when ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}=0$,
    i.e., at the boundary in which unit $i$ in layer $l$ is active in one region and
    inactive in another. Nevertheless, for any input $\overline{{\bm{x}}}$ associated
    with a point at such a boundary between two linear regions ${\mathbb{I}}_{1}$
    and ${\mathbb{I}}_{2}$, it holds that ${\bm{y}}_{{\mathbb{I}}_{1}}(\overline{{\bm{x}}})={\bm{y}}_{{\mathbb{I}}_{2}}(\overline{{\bm{x}}})$
    even if those affine transformations are not entirely identical since the output
    of the neural network is continuous. More importantly, such overlap implies that
    each term of $\mathcal{D}$ is defined using only equalities and nonstrict inequalities,
    and therefore that each linear region corresponds to polyhedra in the extended
    space $({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$. Consequently, those linear
    regions also define polyhedra if projected to the input space ${\bm{x}}$, since
    by using Fourier-Motzkin elimination (Fourier, [1826](#bib.bib107), Motzkin, [1936](#bib.bib226))
    we can obtain a polyhedral description of the linear region in ${\bm{x}}$. Moreover,
    the interior of those polyhedra are disjoint. If one of those polyhedra does not
    have an interior, which means that it is not full-dimensional, then that linear
    region lies entirely on the boundary of other linear regions. In such a case,
    we do not regard it as a proper linear region. By looking at the geometry of those
    linear regions from a different perspective in SectionÂ [3.3](#S3.SS3 "3.3 The
    geometry of linear regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey") and understanding its impact on the
    number of linear regions in SectionÂ [3.4](#S3.SS4 "3.4 The number of linear regions
    â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"), we will see that many terms of $\mathcal{D}$ may actually
    be empty.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿™ç§åˆ†åŒºæ¶‰åŠç›¸é‚»çº¿æ€§åŒºåŸŸä¹‹é—´çš„é‡å ï¼Œå½“${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}=0$æ—¶ï¼Œå³åœ¨ç¬¬$l$å±‚çš„å•å…ƒ$i$åœ¨ä¸€ä¸ªåŒºåŸŸå†…æ˜¯æ¿€æ´»çš„è€Œåœ¨å¦ä¸€ä¸ªåŒºåŸŸå†…æ˜¯éæ¿€æ´»çš„è¾¹ç•Œå¤„ã€‚ç„¶è€Œï¼Œå¯¹äºä»»ä½•ä¸è¿™ç§è¾¹ç•Œä¸Šçš„ç‚¹ç›¸å…³çš„è¾“å…¥$\overline{{\bm{x}}}$ï¼Œåœ¨ä¸¤ä¸ªçº¿æ€§åŒºåŸŸ${\mathbb{I}}_{1}$å’Œ${\mathbb{I}}_{2}$ä¹‹é—´ï¼Œè¯¥ç‚¹æ»¡è¶³${\bm{y}}_{{\mathbb{I}}_{1}}(\overline{{\bm{x}}})={\bm{y}}_{{\mathbb{I}}_{2}}(\overline{{\bm{x}}})$ï¼Œå³ä½¿è¿™äº›ä»¿å°„å˜æ¢å¹¶ä¸å®Œå…¨ç›¸åŒï¼Œå› ä¸ºç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯è¿ç»­çš„ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™ç§é‡å æ„å‘³ç€$\mathcal{D}$çš„æ¯ä¸€é¡¹ä»…é€šè¿‡ç­‰å¼å’Œéä¸¥æ ¼ä¸ç­‰å¼å®šä¹‰ï¼Œå› æ­¤æ¯ä¸ªçº¿æ€§åŒºåŸŸå¯¹åº”äºæ‰©å±•ç©ºé—´$({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$ä¸­çš„å¤šé¢ä½“ã€‚å› æ­¤ï¼Œå¦‚æœå°†è¿™äº›çº¿æ€§åŒºåŸŸæŠ•å½±åˆ°è¾“å…¥ç©ºé—´${\bm{x}}$ï¼Œè¿™äº›çº¿æ€§åŒºåŸŸä¹Ÿå®šä¹‰äº†å¤šé¢ä½“ï¼Œå› ä¸ºé€šè¿‡ä½¿ç”¨å‚…é‡Œå¶-è«èŒ¨é‡‘æ¶ˆå…ƒï¼ˆå‚…é‡Œå¶ï¼Œ[1826](#bib.bib107)ï¼Œè«èŒ¨é‡‘ï¼Œ[1936](#bib.bib226)ï¼‰æˆ‘ä»¬å¯ä»¥è·å¾—${\bm{x}}$ä¸­çº¿æ€§åŒºåŸŸçš„å¤šé¢ä½“æè¿°ã€‚æ­¤å¤–ï¼Œè¿™äº›å¤šé¢ä½“çš„å†…éƒ¨æ˜¯äº’ä¸é‡å çš„ã€‚å¦‚æœå…¶ä¸­ä¸€ä¸ªå¤šé¢ä½“æ²¡æœ‰å†…éƒ¨ï¼Œè¿™æ„å‘³ç€å®ƒä¸æ˜¯å…¨ç»´çš„ï¼Œé‚£ä¹ˆè¯¥çº¿æ€§åŒºåŸŸå®Œå…¨ä½äºå…¶ä»–çº¿æ€§åŒºåŸŸçš„è¾¹ç•Œä¸Šã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸å°†å…¶è§†ä¸ºä¸€ä¸ªé€‚å½“çš„çº¿æ€§åŒºåŸŸã€‚é€šè¿‡ä»ä¸åŒçš„è§’åº¦æŸ¥çœ‹è¿™äº›çº¿æ€§åŒºåŸŸçš„å‡ ä½•å½¢çŠ¶ï¼ˆå‚è§ç¬¬[3.3](#S3.SS3
    "3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey)èŠ‚ï¼‰å¹¶ç†è§£å…¶å¯¹çº¿æ€§åŒºåŸŸæ•°é‡çš„å½±å“ï¼ˆå‚è§ç¬¬[3.4](#S3.SS4
    "3.4 The number of linear regions â€£ 3 The Linear Regions of a Neural Network â€£
    When Deep Learning Meets Polyhedral Theory: A Survey)èŠ‚ï¼‰ï¼Œæˆ‘ä»¬ä¼šå‘ç°$\mathcal{D}$çš„è®¸å¤šé¡¹å®é™…ä¸Šå¯èƒ½æ˜¯ç©ºçš„ã€‚'
- en: 'The optimization over the union of polyhedra is the subject of disjunctive
    programming, which has contributed to the development of stronger formulations
    and better algorithms to solve discrete optimization problems. These are formulated
    as MILPs as well as more general types of problems in recent years (Balas, [2018](#bib.bib12)),
    including generalized disjunctive programming for Mixed-Integer Non-Linear ProgrammingÂ (MINLP)
    (Raman and Grossmann, [1994](#bib.bib256), Grossmann and Ruiz, [2012](#bib.bib134)).
    One of such contributions is the generation of valid inequalities to strengthen
    MILP formulations, which are also denoted as cutting planes, through the lift-and-project
    technique (Balas etÂ al., [1993](#bib.bib13), [1996](#bib.bib14)). In fact, we
    can develop stronger formulations for optimization problems involving neural networks
    through the lenses of disjunctive programming, as we discuss later in SectionÂ [4.2](#S4.SS2
    "4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹å¤šé¢ä½“å¹¶é›†çš„ä¼˜åŒ–æ˜¯ç¦»æ•£è§„åˆ’çš„ä¸»é¢˜ï¼Œè¿™æœ‰åŠ©äºå¼€å‘æ›´å¼ºçš„æ•°å­¦æ¨¡å‹å’Œæ›´å¥½çš„ç®—æ³•æ¥è§£å†³ç¦»æ•£ä¼˜åŒ–é—®é¢˜ã€‚è¿™äº›é—®é¢˜åœ¨è¿‘å¹´æ¥è¢«è¡¨è¿°ä¸ºæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆMILPsï¼‰ä»¥åŠæ›´ä¸€èˆ¬ç±»å‹çš„é—®é¢˜ï¼ˆBalas,
    [2018](#bib.bib12)ï¼‰ï¼ŒåŒ…æ‹¬é’ˆå¯¹æ··åˆæ•´æ•°éçº¿æ€§è§„åˆ’ï¼ˆMINLPï¼‰çš„å¹¿ä¹‰ç¦»æ•£è§„åˆ’ï¼ˆRaman and Grossmann, [1994](#bib.bib256),
    Grossmann and Ruiz, [2012](#bib.bib134)ï¼‰ã€‚å…¶ä¸­ä¸€ç§è´¡çŒ®æ˜¯é€šè¿‡å‡ç»´ä¸æŠ•å½±æŠ€æœ¯ç”Ÿæˆæœ‰æ•ˆä¸ç­‰å¼ä»¥å¼ºåŒ–MILPæ¨¡å‹ï¼Œè¿™äº›ä¹Ÿè¢«ç§°ä¸ºå‰²å¹³é¢ï¼ˆBalas
    et al., [1993](#bib.bib13), [1996](#bib.bib14)ï¼‰ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç¦»æ•£è§„åˆ’çš„è§†è§’å¼€å‘é’ˆå¯¹ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–é—®é¢˜çš„æ›´å¼ºæ¨¡å‹ï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬[4.2](#S4.SS2
    "4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")èŠ‚ä¸­è¿›ä¸€æ­¥è®¨è®ºã€‚'
- en: 3.3 The geometry of linear regions
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 çº¿æ€§åŒºåŸŸçš„å‡ ä½•
- en: Another form of looking at the geometry of linear regions is through their transformation
    along the layers of the neural network. Namely, we can think of the input space
    as initially being partitioned by the units of the first layer, and then each
    resulting linear region being further partitioned by the subsequent layers. In
    that sense, we can think of every layer as a particular form of â€œslicingâ€ the
    input. In fact, a layer may slice each linear region that is defined by the preceding
    layer in a different way due to which neurons are active or not in previous layers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§è§‚å¯Ÿçº¿æ€§åŒºåŸŸå‡ ä½•çš„æ–¹æ³•æ˜¯é€šè¿‡å…¶æ²¿ç¥ç»ç½‘ç»œå±‚çš„å˜æ¢ã€‚å³ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¾“å…¥ç©ºé—´è§†ä¸ºæœ€åˆç”±ç¬¬ä¸€å±‚çš„å•å…ƒè¿›è¡Œåˆ’åˆ†ï¼Œç„¶åæ¯ä¸ªç»“æœçº¿æ€§åŒºåŸŸè¿›ä¸€æ­¥è¢«åç»­å±‚åˆ’åˆ†ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè®²ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¯ä¸€å±‚çœ‹ä½œæ˜¯å¯¹è¾“å…¥çš„æŸç§â€œåˆ‡ç‰‡â€ã€‚å®é™…ä¸Šï¼Œç”±äºå‰ä¸€å±‚ä¸­å“ªäº›ç¥ç»å…ƒå¤„äºæ¿€æ´»çŠ¶æ€æˆ–æœªæ¿€æ´»çŠ¶æ€ï¼Œä¸€å±‚å¯èƒ½ä»¥ä¸åŒçš„æ–¹å¼åˆ‡åˆ†ç”±å‰ä¸€å±‚å®šä¹‰çš„æ¯ä¸ªçº¿æ€§åŒºåŸŸã€‚
- en: 'Let us begin by illustrating how a given layer $l\in{\mathbb{L}}$ partitions
    its input space ${\bm{h}}^{l-1}$. Every neuron $i$ in layer $l$ is associated
    with an *activation hyperplane* of the form ${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}=0$,
    which divides the possible inputs of its layer into an open half-space in which
    the unit is active (${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}>0$) and a closed
    half-space in which the unit is inactive (${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\leq
    0$). These hyperplanes define the boundary between adjacent linear regions, and
    the arrangement of such hyperplanes for a given layer $l\in{\mathbb{L}}$ determines
    how that layer partitions the ${\bm{h}}^{l-1}$ space. In other words, every input
    in ${\bm{h}}^{l-1}$ can be located with respect to each of those hyperplanes,
    which corresponds to the activation set of the linear region to which it belongs.
    However, not every activation set out of the $2^{n_{l}}$ possible ones maps to
    a nonempty region of the input space. In the case of ExampleÂ [2](#Thmexample2
    "Example 2 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"), there is no
    linear region in which the activation set is empty.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'è®©æˆ‘ä»¬é¦–å…ˆè¯´æ˜ç»™å®šå±‚ $l\in{\mathbb{L}}$ å¦‚ä½•åˆ’åˆ†å…¶è¾“å…¥ç©ºé—´ ${\bm{h}}^{l-1}$ã€‚å±‚ $l$ ä¸­çš„æ¯ä¸ªç¥ç»å…ƒ $i$
    éƒ½ä¸å½¢å¼ä¸º ${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}=0$ çš„*æ¿€æ´»è¶…å¹³é¢*ç›¸å…³è”ï¼Œè¯¥è¶…å¹³é¢å°†å…¶å±‚çš„å¯èƒ½è¾“å…¥åˆ’åˆ†ä¸ºä¸€ä¸ªå•ä½æ¿€æ´»çš„å¼€åŠç©ºé—´ï¼ˆ${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}>0$ï¼‰å’Œä¸€ä¸ªå•ä½æœªæ¿€æ´»çš„é—­åŠç©ºé—´ï¼ˆ${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\leq
    0$ï¼‰ã€‚è¿™äº›è¶…å¹³é¢å®šä¹‰äº†ç›¸é‚»çº¿æ€§åŒºåŸŸä¹‹é—´çš„è¾¹ç•Œï¼Œç»™å®šå±‚ $l\in{\mathbb{L}}$ çš„è¿™äº›è¶…å¹³é¢çš„æ’åˆ—å†³å®šäº†è¯¥å±‚å¦‚ä½•åˆ’åˆ† ${\bm{h}}^{l-1}$
    ç©ºé—´ã€‚æ¢å¥è¯è¯´ï¼Œ${\bm{h}}^{l-1}$ ä¸­çš„æ¯ä¸ªè¾“å…¥éƒ½å¯ä»¥ç›¸å¯¹äºè¿™äº›è¶…å¹³é¢è¿›è¡Œå®šä½ï¼Œè¿™å¯¹åº”äºå…¶æ‰€å±çº¿æ€§åŒºåŸŸçš„æ¿€æ´»é›†ã€‚ç„¶è€Œï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰ $2^{n_{l}}$
    ä¸ªå¯èƒ½çš„æ¿€æ´»é›†éƒ½æ˜ å°„åˆ°è¾“å…¥ç©ºé—´çš„éç©ºåŒºåŸŸã€‚åœ¨ç¤ºä¾‹ [2](#Thmexample2 "Example 2 â€£ 3.3 The geometry of linear
    regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey") ä¸­ï¼Œæ²¡æœ‰ä»»ä½•çº¿æ€§åŒºåŸŸçš„æ¿€æ´»é›†æ˜¯ç©ºçš„ã€‚'
- en: Example 2
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ 2
- en: 'Consider a neural network with domain ${\bm{x}}\in\mathbb{R}^{2}$ and a single
    layer having 3 neurons $\alpha$, $\beta$, and $\gamma$ with outputs given as follows:
    $h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+0.6,0\}$, $h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+4.8,0\}$,
    and $h^{1}_{\gamma}=\max\{0x_{1}+3x_{2}-5,0\}$. These neurons define the activation
    hyperplanes ($\alpha$) $2.3x_{1}-1.9x_{2}+0.6=0$, ($\beta$) $-0.9x_{1}-0.7x_{2}+4.8=0$,
    and ($\gamma$) $0x_{1}+3x_{2}-5=0$ in the space ${\bm{x}}$, which are illustrated
    along with the activation sets of the linear regions in FigureÂ [5](#S3.F5 "Figure
    5 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 'è€ƒè™‘ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå…¶å®šä¹‰åŸŸä¸º ${\bm{x}}\in\mathbb{R}^{2}$ï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªåŒ…å« 3 ä¸ªç¥ç»å…ƒ $\alpha$ã€$\beta$
    å’Œ $\gamma$ çš„å•å±‚ï¼Œè¾“å‡ºå¦‚ä¸‹ï¼š$h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+0.6,0\}$ï¼Œ$h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+4.8,0\}$ï¼Œ$h^{1}_{\gamma}=\max\{0x_{1}+3x_{2}-5,0\}$ã€‚è¿™äº›ç¥ç»å…ƒå®šä¹‰äº†æ¿€æ´»è¶…å¹³é¢ï¼ˆ$\alpha$ï¼‰
    $2.3x_{1}-1.9x_{2}+0.6=0$ï¼Œï¼ˆ$\beta$ï¼‰ $-0.9x_{1}-0.7x_{2}+4.8=0$ï¼Œå’Œï¼ˆ$\gamma$ï¼‰ $0x_{1}+3x_{2}-5=0$ï¼Œè¿™äº›è¶…å¹³é¢åœ¨ç©ºé—´
    ${\bm{x}}$ ä¸­ç»˜åˆ¶ï¼Œå¹¶ä¸”ä¸çº¿æ€§åŒºåŸŸçš„æ¿€æ´»é›†ä¸€èµ·åœ¨å›¾Â [5](#S3.F5 "Figure 5 â€£ 3.3 The geometry of linear
    regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey") ä¸­å±•ç¤ºã€‚'
- en: Instead of $2^{3}$ linear regions corresponding to each possible activation
    set defined by a subset of the neurons in $\{\alpha,\beta,\gamma\}$, the arrangement
    of such hyperplanes produces 7 linear regions, which is in fact the maximum number
    of 2-dimensional regions that can be defined by drawing 3 lines on a plane.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ¯ä¸ªç”±ç¥ç»å…ƒå­é›† $\{\alpha,\beta,\gamma\}$ å®šä¹‰çš„å¯èƒ½æ¿€æ´»é›†å¯¹åº”çš„ $2^{3}$ çº¿æ€§åŒºåŸŸä¸åŒï¼Œè¿™äº›è¶…å¹³é¢çš„æ’åˆ—äº§ç”Ÿäº† 7
    ä¸ªçº¿æ€§åŒºåŸŸï¼Œè¿™å®é™…ä¸Šæ˜¯é€šè¿‡åœ¨å¹³é¢ä¸Šç»˜åˆ¶ 3 æ¡ç›´çº¿å¯ä»¥å®šä¹‰çš„æœ€å¤§äºŒç»´åŒºåŸŸæ•°ã€‚
- en: '![Refer to caption](img/275229d48fa5bb1b250445d5df8fa0b0.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/275229d48fa5bb1b250445d5df8fa0b0.png)'
- en: 'Figure 5: Linear regions defined by the shallow neural network described in
    ExampleÂ [2](#Thmexample2 "Example 2 â€£ 3.3 The geometry of linear regions â€£ 3 The
    Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey"). Every line corresponds to the activation hyperplane of a different
    neuron, which is given by $\alpha$, $\beta$, and $\gamma$ in parentheses. The
    arrow next to each line points to the half space in which the inputs activate
    that neuron. Every linear region has a subset of $\{\alpha,\beta,\gamma\}$ as
    its corresponding activation set.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 5ï¼šç”±ç¤ºä¾‹Â [2](#Thmexample2 "Example 2 â€£ 3.3 The geometry of linear regions â€£
    3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey") ä¸­æè¿°çš„æµ…å±‚ç¥ç»ç½‘ç»œå®šä¹‰çš„çº¿æ€§åŒºåŸŸã€‚æ¯æ¡ç›´çº¿å¯¹åº”äºä¸€ä¸ªä¸åŒç¥ç»å…ƒçš„æ¿€æ´»è¶…å¹³é¢ï¼Œè¿™äº›ç¥ç»å…ƒç”±æ‹¬å·ä¸­çš„ $\alpha$ã€$\beta$
    å’Œ $\gamma$ ç»™å‡ºã€‚æ¯æ¡ç›´çº¿æ—è¾¹çš„ç®­å¤´æŒ‡å‘æ¿€æ´»è¯¥ç¥ç»å…ƒçš„è¾“å…¥æ‰€åœ¨çš„åŠç©ºé—´ã€‚æ¯ä¸ªçº¿æ€§åŒºåŸŸéƒ½æœ‰ $\{\alpha,\beta,\gamma\}$ çš„ä¸€ä¸ªå­é›†ä½œä¸ºå…¶å¯¹åº”çš„æ¿€æ´»é›†ã€‚'
- en: 'The maximum number of full-dimensional regions resulting from a partitioning
    defined by $n$ hyperplanes depends on the dimension $d$ of the space in which
    those hyperplanes are defined (Zaslavsky, [1975](#bib.bib352)). That number never
    exceeds $\sum\limits_{i=1}^{\min\{d,n\}}\binom{n}{i}$. Such bound only coincides
    with $2^{n}$ if $d\geq n$; otherwise, as illustrated in ExampleÂ [2](#Thmexample2
    "Example 2 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"), that number
    can be smaller. As observed byÂ Hanin and Rolnick ([2019b](#bib.bib138)), that
    bound is $O\left(\frac{n^{d}}{d!}\right)$ when $n\gg d$.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä» $n$ ä¸ªè¶…å¹³é¢å®šä¹‰çš„åˆ†åŒºä¸­å¾—åˆ°çš„å…¨ç»´åŒºåŸŸçš„æœ€å¤§æ•°é‡å–å†³äºè¿™äº›è¶…å¹³é¢å®šä¹‰çš„ç©ºé—´çš„ç»´åº¦ $d$ï¼ˆZaslavsky, [1975](#bib.bib352)ï¼‰ã€‚è¿™ä¸ªæ•°é‡ä»ä¸è¶…è¿‡
    $\sum\limits_{i=1}^{\min\{d,n\}}\binom{n}{i}$ã€‚åªæœ‰å½“ $d\geq n$ æ—¶ï¼Œè¿™ä¸ªç•Œé™æ‰ä¸ $2^{n}$ ç›¸ç¬¦ï¼›å¦åˆ™ï¼Œå¦‚ç¤ºä¾‹Â [2](#Thmexample2
    "Example 2 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") ä¸­æ‰€ç¤ºï¼Œè¯¥æ•°é‡å¯èƒ½æ›´å°ã€‚æ­£å¦‚
    Hanin å’Œ Rolnick ([2019b](#bib.bib138)) æ‰€è§‚å¯Ÿåˆ°çš„ï¼Œå½“ $n\gg d$ æ—¶ï¼Œè¯¥ç•Œé™ä¸º $O\left(\frac{n^{d}}{d!}\right)$ã€‚'
- en: In fact, the above bound is all that we need to determine the maximum number
    of linear regions in shallow networks. While not every shallow network may define
    as many linear regions, it is always possible to put the hyperplanes in what is
    called a *general position* in order to reach that bound. Thus, the maximum number
    of linear regions defined by a shallow network is $\sum\limits_{i=0}^{\min\{n_{0},n_{1}\}}\binom{n_{1}}{n_{0}}$.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œä¸Šè¿°ç•Œé™æ˜¯æˆ‘ä»¬ç¡®å®šæµ…å±‚ç½‘ç»œä¸­çº¿æ€§åŒºåŸŸæœ€å¤§æ•°é‡æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯ã€‚è™½ç„¶å¹¶éæ¯ä¸ªæµ…å±‚ç½‘ç»œéƒ½èƒ½å®šä¹‰å¦‚æ­¤å¤šçš„çº¿æ€§åŒºåŸŸï¼Œä½†æ€»æ˜¯å¯ä»¥å°†è¶…å¹³é¢æ”¾ç½®åœ¨æ‰€è°“çš„ *ä¸€èˆ¬ä½ç½®*
    æ¥è¾¾åˆ°è¯¥ç•Œé™ã€‚å› æ­¤ï¼Œæµ…å±‚ç½‘ç»œå®šä¹‰çš„çº¿æ€§åŒºåŸŸçš„æœ€å¤§æ•°é‡æ˜¯ $\sum\limits_{i=0}^{\min\{n_{0},n_{1}\}}\binom{n_{1}}{n_{0}}$ã€‚
- en: For the polyhedron associated with each linear region, being in general position
    implies that each vertex lies on exactly $d$ activation hyperplanes. For context,
    the converse situation in linear programming â€”having more hyperplanes active on
    a vertex than the space dimensionâ€” characterizes degeneracy.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸æ¯ä¸ªçº¿æ€§åŒºåŸŸç›¸å…³çš„å¤šé¢ä½“ï¼Œå¤„äºä¸€èˆ¬ä½ç½®æ„å‘³ç€æ¯ä¸ªé¡¶ç‚¹æ°å¥½ä½äº$d$ä¸ªæ¿€æ´»è¶…å¹³é¢ä¸Šã€‚ä½œä¸ºèƒŒæ™¯ï¼Œçº¿æ€§è§„åˆ’ä¸­çš„é€†æƒ…å†µâ€”â€”é¡¶ç‚¹ä¸Šæœ‰è¶…è¿‡ç©ºé—´ç»´åº¦çš„æ¿€æ´»è¶…å¹³é¢â€”â€”åˆ™ç‰¹å¾åŒ–ä¸ºé€€åŒ–ã€‚
- en: 'In the case of deep networks, the partitioning of each linear region by the
    subsequent layers is based on the output of that linear region. This affects the
    shape and the number of the linear regions defined by the following layers, which
    may vary between adjacent linear regions due to which units are active or inactive
    from one linear region to another, as illustrated in ExampleÂ [3](#Thmexample3
    "Example 3 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨æ·±åº¦ç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œåç»­å±‚å¯¹æ¯ä¸ªçº¿æ€§åŒºåŸŸçš„åˆ’åˆ†æ˜¯åŸºäºè¯¥çº¿æ€§åŒºåŸŸçš„è¾“å‡ºã€‚è¿™å½±å“äº†åç»­å±‚å®šä¹‰çš„çº¿æ€§åŒºåŸŸçš„å½¢çŠ¶å’Œæ•°é‡ï¼Œç”±äºä»ä¸€ä¸ªçº¿æ€§åŒºåŸŸåˆ°å¦ä¸€ä¸ªçº¿æ€§åŒºåŸŸçš„æ¿€æ´»æˆ–éæ¿€æ´»å•å…ƒï¼Œè¿™äº›çº¿æ€§åŒºåŸŸä¹‹é—´çš„å˜åŒ–å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå¦‚ç¤ºä¾‹[3](#Thmexample3
    "Example 3 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")æ‰€ç¤ºã€‚'
- en: Example 3
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ 3
- en: 'Consider a neural network with domain ${\bm{x}}\in\mathbb{R}^{2}$ and 2 layers
    having 2 neurons each â€”say neurons $\alpha$ and $\beta$ in layer 1, and neurons
    $\gamma$ and $\delta$ in layer 2â€” with outputs given as follows: $h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+1.5,0\}$,
    $h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+5,0\}$, $h^{2}_{\gamma}=\max\{0.4h^{1}_{1}-3.1h^{1}_{2}+4,0\}$,
    $h^{2}_{\delta}=\max\{-0.6h^{1}_{1}-1.6h^{1}_{2}+5,0\}$. These neurons define
    the activation hyperplanes ($\alpha$) $2.3x_{1}-1.9x_{2}+1.5=0$ and ($\beta$)
    $-0.9x_{1}-0.7x_{2}+5=0$ in the ${\bm{x}}$ space and the activation hyperplanes
    ($\gamma$) $0.4h^{1}_{1}-3.1h^{1}_{2}+4=0$ and ($\delta$) $-0.6h^{1}_{1}-1.6h^{1}_{2}+5=0$
    in the ${\bm{h}}^{1}$ space, which are illustrated along with the activation sets
    of the linear regions in the first two plots of FigureÂ [6](#S3.F6 "Figure 6 â€£
    3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey"). The third plot illustrates
    the linear regions jointly defined by the two layers in terms of the input space
    ${\bm{x}}$.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'è€ƒè™‘ä¸€ä¸ªåŸŸä¸º${\bm{x}}\in\mathbb{R}^{2}$çš„ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­æœ‰2å±‚ï¼Œæ¯å±‚å„æœ‰2ä¸ªç¥ç»å…ƒâ€”â€”ä¾‹å¦‚ï¼Œç¬¬1å±‚çš„ç¥ç»å…ƒ$\alpha$å’Œ$\beta$ï¼Œç¬¬2å±‚çš„ç¥ç»å…ƒ$\gamma$å’Œ$\delta$â€”â€”å…¶è¾“å‡ºå¦‚ä¸‹ï¼š$h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+1.5,0\}$ï¼Œ$h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+5,0\}$ï¼Œ$h^{2}_{\gamma}=\max\{0.4h^{1}_{1}-3.1h^{1}_{2}+4,0\}$ï¼Œ$h^{2}_{\delta}=\max\{-0.6h^{1}_{1}-1.6h^{1}_{2}+5,0\}$ã€‚è¿™äº›ç¥ç»å…ƒåœ¨${\bm{x}}$ç©ºé—´ä¸­å®šä¹‰äº†æ¿€æ´»è¶…å¹³é¢ï¼ˆ$\alpha$ï¼‰$2.3x_{1}-1.9x_{2}+1.5=0$å’Œï¼ˆ$\beta$ï¼‰$-0.9x_{1}-0.7x_{2}+5=0$ï¼Œåœ¨${\bm{h}}^{1}$ç©ºé—´ä¸­å®šä¹‰äº†æ¿€æ´»è¶…å¹³é¢ï¼ˆ$\gamma$ï¼‰$0.4h^{1}_{1}-3.1h^{1}_{2}+4=0$å’Œï¼ˆ$\delta$ï¼‰$-0.6h^{1}_{1}-1.6h^{1}_{2}+5=0$ï¼Œè¿™äº›åœ¨å›¾[6](#S3.F6
    "Figure 6 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")çš„å‰ä¸¤ä¸ªå›¾ä¸­ä¸çº¿æ€§åŒºåŸŸçš„æ¿€æ´»é›†åˆä¸€èµ·å±•ç¤ºã€‚ç¬¬ä¸‰ä¸ªå›¾è¯´æ˜äº†ç”±ä¸¤å±‚å…±åŒå®šä¹‰çš„è¾“å…¥ç©ºé—´${\bm{x}}$ä¸­çš„çº¿æ€§åŒºåŸŸã€‚'
- en: 'The third plot is repeated in FigureÂ [7](#S3.F7 "Figure 7 â€£ 3.3 The geometry
    of linear regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey"), in which the shape of each linear region
    ${\mathbb{I}}$ is filled in accordance to the dimension of the image of $\bar{{\bm{y}}}_{{\mathbb{I}}}({\bm{x}})$
    â€”the output of the neural network for each linear region ${\mathbb{I}}$.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸‰ä¸ªå›¾åœ¨å›¾[7](#S3.F7 "Figure 7 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear
    Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")ä¸­é‡å¤å‡ºç°ï¼Œå…¶ä¸­æ¯ä¸ªçº¿æ€§åŒºåŸŸ${\mathbb{I}}$çš„å½¢çŠ¶æ ¹æ®$\bar{{\bm{y}}}_{{\mathbb{I}}}({\bm{x}})$çš„å›¾åƒç»´åº¦è¿›è¡Œå¡«å……â€”â€”å³ç¥ç»ç½‘ç»œå¯¹æ¯ä¸ªçº¿æ€§åŒºåŸŸ${\mathbb{I}}$çš„è¾“å‡ºã€‚'
- en: '![Refer to caption](img/d4b21733c24f690a28025ee4d147bf0d.png)![Refer to caption](img/66f9cc2c9bd7fbe539dcffe16058d0b5.png)![Refer
    to caption](img/7fc94c543268674eb63d5ca496258400.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/d4b21733c24f690a28025ee4d147bf0d.png)![å‚è§æ ‡é¢˜](img/66f9cc2c9bd7fbe539dcffe16058d0b5.png)![å‚è§æ ‡é¢˜](img/7fc94c543268674eb63d5ca496258400.png)'
- en: 'Figure 6: Linear regions defined by the 2 layers of the neural network described
    in ExampleÂ [3](#Thmexample3 "Example 3 â€£ 3.3 The geometry of linear regions â€£
    3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"), following the same notation as in FigureÂ [5](#S3.F5 "Figure
    5 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey"). The first and second
    plots show the linear regions and corresponding activation sets defined by the
    first and the second layers in terms of their input spaces (${\bm{x}}$ and ${\bm{h}}^{1}$).
    The third plot shows the linear regions defined by the combination of the 2 layers
    and the union of their activation sets in terms of the input space of the first
    layer (${\bm{x}}$).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šç”±ç¤ºä¾‹Â [3](#Thmexample3 "ç¤ºä¾‹ 3 â€£ 3.3 çº¿æ€§åŒºåŸŸçš„å‡ ä½• â€£ 3 ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒºåŸŸ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºç›¸é‡ï¼šä¸€é¡¹è°ƒæŸ¥")
    ä¸­æè¿°çš„ç¥ç»ç½‘ç»œçš„ 2 å±‚å®šä¹‰çš„çº¿æ€§åŒºåŸŸï¼Œéµå¾ªå›¾Â [5](#S3.F5 "å›¾ 5 â€£ 3.3 çº¿æ€§åŒºåŸŸçš„å‡ ä½• â€£ 3 ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒºåŸŸ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºç›¸é‡ï¼šä¸€é¡¹è°ƒæŸ¥")
    ä¸­çš„ç›¸åŒç¬¦å·ã€‚ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå›¾æ˜¾ç¤ºäº†ç”±ç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚åœ¨å…¶è¾“å…¥ç©ºé—´ (${\bm{x}}$ å’Œ ${\bm{h}}^{1}$) å®šä¹‰çš„çº¿æ€§åŒºåŸŸåŠç›¸åº”çš„æ¿€æ´»é›†ã€‚ç¬¬ä¸‰ä¸ªå›¾æ˜¾ç¤ºäº†ç”±è¿™
    2 å±‚ç»„åˆå®šä¹‰çš„çº¿æ€§åŒºåŸŸåŠå…¶æ¿€æ´»é›†çš„å¹¶é›†ï¼ŒåŸºäºç¬¬ä¸€å±‚çš„è¾“å…¥ç©ºé—´ (${\bm{x}}$)ã€‚
- en: 'ExampleÂ [3](#Thmexample3 "Example 3 â€£ 3.3 The geometry of linear regions â€£
    3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey") highlights two important aspects about the structure of linear
    regions in deep neural networks. First, the linear regions defined by a neural
    network with multiple layers are different because activation hyperplanes after
    the first layer may look â€œbentâ€ from the input space $x$, such as with the inflections
    of hyperplanes $(\gamma)$ and $(\delta)$ in the third plot of FigureÂ [6](#S3.F6
    "Figure 6 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") from one linear
    region defined by the first layer to another. This partitioning of the input space
    would not be possible with a single layer.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹Â [3](#Thmexample3 "ç¤ºä¾‹ 3 â€£ 3.3 çº¿æ€§åŒºåŸŸçš„å‡ ä½• â€£ 3 ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒºåŸŸ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºç›¸é‡ï¼šä¸€é¡¹è°ƒæŸ¥")
    çªå‡ºäº†æ·±åº¦ç¥ç»ç½‘ç»œä¸­çº¿æ€§åŒºåŸŸç»“æ„çš„ä¸¤ä¸ªé‡è¦æ–¹é¢ã€‚é¦–å…ˆï¼Œç”±å¤šå±‚ç¥ç»ç½‘ç»œå®šä¹‰çš„çº¿æ€§åŒºåŸŸæ˜¯ä¸åŒçš„ï¼Œå› ä¸ºåœ¨ç¬¬ä¸€å±‚ä¹‹åçš„æ¿€æ´»è¶…å¹³é¢å¯èƒ½çœ‹èµ·æ¥ä»è¾“å…¥ç©ºé—´ $x$ ä¸­â€œå¼¯æ›²â€ï¼Œä¾‹å¦‚ï¼Œå›¾Â [6](#S3.F6
    "å›¾ 6 â€£ 3.3 çº¿æ€§åŒºåŸŸçš„å‡ ä½• â€£ 3 ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒºåŸŸ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºç›¸é‡ï¼šä¸€é¡¹è°ƒæŸ¥") çš„ç¬¬ä¸‰ä¸ªå›¾ä¸­çš„è¶…å¹³é¢ $(\gamma)$ å’Œ
    $(\delta)$ ä»ç¬¬ä¸€å±‚å®šä¹‰çš„ä¸€ä¸ªçº¿æ€§åŒºåŸŸåˆ°å¦ä¸€ä¸ªçº¿æ€§åŒºåŸŸã€‚è¿™ç§è¾“å…¥ç©ºé—´çš„åˆ’åˆ†åœ¨å•å±‚ç½‘ç»œä¸­æ˜¯ä¸å¯èƒ½å®ç°çš„ã€‚
- en: 'By comparing side by side the first and the third plots of FigureÂ [6](#S3.F6
    "Figure 6 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"), we can see how
    every linear region of a given layer of a neural network may be partitioned differently
    by the following layer. When defined in terms of the input space ${\bm{x}}$, the
    hyperplanes associated with the second layer differ across the linear regions
    defined by the first layer because each of those linear regions is associated
    with a different affine transformation from ${\bm{x}}$ to ${\bm{h}}^{1}$. Hence,
    the activation hyperplanes of layer $l$ may break each linear region from layer
    $l-1$ differently. To every linear region defined by the hyperplane arrangement
    in the ${\bm{h}}^{l-1}$ space there is a linear transformation ${\bm{h}}^{l-1}=\Omega^{{\mathbb{S}}^{l-1}}({\bm{W}}^{l-1}{\bm{h}}^{l-2}+{\bm{b}}^{l-1})$
    to the points of that linear region based on the set of active neurons ${\mathbb{S}}^{l-1}$.
    Consequently, inputs in the ${\bm{h}}^{l-1}$ space that are associated with different
    linear regions are transformed differently to the ${\bm{h}}^{l}$ space, and therefore
    the form in which those linear regions are further partitioned by layer $l$ is
    not the same when seen from the ${\bm{h}}^{l-1}$ space.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'é€šè¿‡å¹¶æ’æ¯”è¾ƒå›¾[6](#S3.F6 "Figure 6 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear
    Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")ä¸­çš„ç¬¬ä¸€å¹…å’Œç¬¬ä¸‰å¹…å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¥ç»ç½‘ç»œä¸­ç»™å®šå±‚çš„æ¯ä¸ªçº¿æ€§åŒºåŸŸæ˜¯å¦‚ä½•è¢«ä¸‹ä¸€å±‚ä¸åŒåœ°åˆ’åˆ†çš„ã€‚å½“ä»¥è¾“å…¥ç©ºé—´${\bm{x}}$æ¥å®šä¹‰æ—¶ï¼Œä¸ç¬¬äºŒå±‚ç›¸å…³çš„è¶…å¹³é¢åœ¨ç¬¬ä¸€å±‚å®šä¹‰çš„çº¿æ€§åŒºåŸŸä¹‹é—´æœ‰æ‰€ä¸åŒï¼Œå› ä¸ºè¿™äº›çº¿æ€§åŒºåŸŸä¸­çš„æ¯ä¸€ä¸ªéƒ½ä¸ä»${\bm{x}}$åˆ°${\bm{h}}^{1}$çš„ä¸åŒä»¿å°„å˜æ¢ç›¸å…³ã€‚å› æ­¤ï¼Œç¬¬$l$å±‚çš„æ¿€æ´»è¶…å¹³é¢å¯èƒ½ä»¥ä¸åŒçš„æ–¹å¼æ‰“ç ´ç¬¬$l-1$å±‚çš„æ¯ä¸ªçº¿æ€§åŒºåŸŸã€‚å¯¹äº${\bm{h}}^{l-1}$ç©ºé—´ä¸­çš„æ¯ä¸ªç”±è¶…å¹³é¢æ’åˆ—å®šä¹‰çš„çº¿æ€§åŒºåŸŸï¼Œéƒ½æœ‰ä¸€ä¸ªçº¿æ€§å˜æ¢${\bm{h}}^{l-1}=\Omega^{{\mathbb{S}}^{l-1}}({\bm{W}}^{l-1}{\bm{h}}^{l-2}+{\bm{b}}^{l-1})$ï¼Œè¯¥å˜æ¢åŸºäºæ¿€æ´»ç¥ç»å…ƒé›†åˆ${\mathbb{S}}^{l-1}$å°†è¯¥çº¿æ€§åŒºåŸŸçš„ç‚¹è¿›è¡Œå˜æ¢ã€‚å› æ­¤ï¼Œ${\bm{h}}^{l-1}$ç©ºé—´ä¸­ä¸ä¸åŒçº¿æ€§åŒºåŸŸç›¸å…³çš„è¾“å…¥è¢«ä¸åŒåœ°è½¬æ¢åˆ°${\bm{h}}^{l}$ç©ºé—´ï¼Œå› æ­¤ï¼Œè¿™äº›çº¿æ€§åŒºåŸŸåœ¨ç¬¬$l$å±‚è¿›ä¸€æ­¥åˆ’åˆ†çš„å½¢å¼åœ¨ä»${\bm{h}}^{l-1}$ç©ºé—´æ¥çœ‹æ—¶å¹¶ä¸ç›¸åŒã€‚'
- en: 'Second, some combinations of activation sets of multiple layers do not correspond
    to linear regions even if the activation hyperplanes are in general position with
    respect to each layer. For each layer, the first two plots of FigureÂ [6](#S3.F6
    "Figure 6 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") show that every
    activation set corresponds to a nonempty region of the layer input. However, not
    every pair of such activation sets would define a nonempty linear region for the
    neural network. For example, the linear region of the first layer associated with
    the activation set ${\mathbb{S}}^{1}=\{\}$ defines a linear region in ${\bm{x}}$
    which is always mapped to ${\bm{h}}^{1}=0$, and thus only corresponds to activation
    set ${\mathbb{S}}^{2}=\{\gamma,\delta\}$ in the second layer because both units
    are active for such input. Thus, no linear region in ${\bm{x}}$ is associated
    with only the units in sets $\{\},\{\gamma\}$, and $\{\delta\}$ being active â€”i.e.,
    there is no linear region such that ${\mathbb{S}}^{1}\cup{\mathbb{S}}^{2}=\{\},\{\gamma\},\text{or}\{\delta\}$.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶æ¬¡ï¼Œå³ä½¿æ¿€æ´»è¶…å¹³é¢åœ¨æ¯ä¸€å±‚ä¸Šä¸ä¸€èˆ¬ä½ç½®ç›¸å¯¹åº”ï¼Œå¤šä¸ªå±‚çš„æ¿€æ´»é›†çš„æŸäº›ç»„åˆä¹Ÿä¸ä¼šå¯¹åº”äºçº¿æ€§åŒºåŸŸã€‚å¯¹äºæ¯ä¸€å±‚ï¼Œå›¾[6](#S3.F6 "Figure 6
    â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")çš„å‰ä¸¤å¹…å›¾æ˜¾ç¤ºäº†æ¯ä¸ªæ¿€æ´»é›†å¯¹åº”äºè¯¥å±‚è¾“å…¥çš„ä¸€ä¸ªéç©ºåŒºåŸŸã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰è¿™æ ·çš„æ¿€æ´»é›†å¯¹éƒ½èƒ½å®šä¹‰ä¸€ä¸ªç¥ç»ç½‘ç»œçš„éç©ºçº¿æ€§åŒºåŸŸã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€å±‚ä¸æ¿€æ´»é›†${\mathbb{S}}^{1}=\{\}$ç›¸å…³çš„çº¿æ€§åŒºåŸŸåœ¨${\bm{x}}$ä¸­å®šä¹‰äº†ä¸€ä¸ªçº¿æ€§åŒºåŸŸï¼Œè¯¥åŒºåŸŸå§‹ç»ˆæ˜ å°„åˆ°${\bm{h}}^{1}=0ï¼Œå› æ­¤åªå¯¹åº”äºç¬¬äºŒå±‚ä¸­çš„æ¿€æ´»é›†${\mathbb{S}}^{2}=\{\gamma,\delta\}$ï¼Œå› ä¸ºå¯¹äºè¿™æ ·çš„è¾“å…¥ï¼Œä¸¤è€…éƒ½å¤„äºæ¿€æ´»çŠ¶æ€ã€‚å› æ­¤ï¼Œåœ¨${\bm{x}}$ä¸­ï¼Œæ²¡æœ‰çº¿æ€§åŒºåŸŸä»…ä¸é›†åˆ$\{\},\{\gamma\}$å’Œ$\{\delta\}$ä¸­çš„å•å…ƒå¤„äºæ¿€æ´»çŠ¶æ€ç›¸å…³â€”â€”å³ï¼Œæ²¡æœ‰çº¿æ€§åŒºåŸŸä½¿å¾—${\mathbb{S}}^{1}\cup{\mathbb{S}}^{2}=\{\},\{\gamma\},\text{æˆ–}\{\delta\}$ã€‚'
- en: More generally, the number of units that is active on each linear region defined
    by the first layer also imposes a geometric limit to how that linear region can
    be further partitioned. If only one unit is active at a layer, that means that
    the output of the layer within that linear region has dimension 1, and, consequently,
    the subsequent hyperplane arrangements within that linear region are limited to
    a 1-dimensional space. For the network in the example, we thus expect no more
    than $\sum_{i=0}^{1}\binom{2}{i}=3$ linear regions being defined instead of $2^{2}=4$
    when only one unit is active. In fact, that is precisely the number of subdivisions
    by the second layer of the linear region defined by activation set ${\mathbb{S}}^{1}=\{\beta\}$
    from the first layer.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ä¸€èˆ¬åœ°è¯´ï¼Œç¬¬ä¸€ä¸ªå±‚å®šä¹‰çš„æ¯ä¸ªçº¿æ€§åŒºåŸŸä¸Šæ¿€æ´»çš„å•å…ƒæ•°ä¹Ÿå¯¹è¯¥çº¿æ€§åŒºåŸŸå¦‚ä½•è¿›ä¸€æ­¥åˆ’åˆ†æ–½åŠ äº†å‡ ä½•é™åˆ¶ã€‚å¦‚æœä¸€ä¸ªå±‚ä¸Šåªæœ‰ä¸€ä¸ªå•å…ƒæ¿€æ´»ï¼Œé‚£æ„å‘³ç€è¯¥çº¿æ€§åŒºåŸŸå†…çš„å±‚è¾“å‡ºç»´åº¦ä¸º1ï¼Œå› æ­¤ï¼Œè¯¥çº¿æ€§åŒºåŸŸå†…åç»­è¶…å¹³é¢æ’åˆ—è¢«é™åˆ¶åœ¨1ç»´ç©ºé—´å†…ã€‚å¯¹äºç¤ºä¾‹ä¸­çš„ç½‘ç»œï¼Œå› æ­¤æˆ‘ä»¬æœŸæœ›ä¸è¶…è¿‡$\sum_{i=0}^{1}\binom{2}{i}=3$ä¸ªçº¿æ€§åŒºåŸŸè¢«å®šä¹‰ï¼Œè€Œä¸æ˜¯$2^{2}=4$ï¼Œå½“åªæœ‰ä¸€ä¸ªå•å…ƒæ¿€æ´»æ—¶ã€‚å®é™…ä¸Šï¼Œè¿™æ­£æ˜¯ç¬¬äºŒå±‚æ ¹æ®ç¬¬ä¸€ä¸ªå±‚çš„æ¿€æ´»é›†${\mathbb{S}}^{1}=\{\beta\}$å¯¹çº¿æ€§åŒºåŸŸè¿›è¡Œç»†åˆ†çš„æ•°é‡ã€‚
- en: '![Refer to caption](img/738562b7813b118f7f87ea0943be8d01.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/738562b7813b118f7f87ea0943be8d01.png)'
- en: 'Figure 7: Dimension of the image of the affine function ${\bm{y}}_{{\mathbb{I}}}({\bm{x}})$
    associated with each linear region ${\mathbb{I}}$ defined by the neural network
    described in ExampleÂ [3](#Thmexample3 "Example 3 â€£ 3.3 The geometry of linear
    regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey"). The linear regions are the same illustrated in
    the third plot of FigureÂ [6](#S3.F6 "Figure 6 â€£ 3.3 The geometry of linear regions
    â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šä¸ç¥ç»ç½‘ç»œä¸­ç¤ºä¾‹[3](#Thmexample3 "ç¤ºä¾‹ 3 â€£ 3.3 çº¿æ€§åŒºåŸŸçš„å‡ ä½• â€£ 3 ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒºåŸŸ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºç›¸é‡ï¼šç»¼è¿°")å®šä¹‰çš„æ¯ä¸ªçº¿æ€§åŒºåŸŸ${\mathbb{I}}$ç›¸å…³çš„ä»¿å°„å‡½æ•°${\bm{y}}_{{\mathbb{I}}}({\bm{x}})$çš„å›¾åƒç»´åº¦ã€‚çº¿æ€§åŒºåŸŸä¸å›¾[6](#S3.F6
    "å›¾ 6 â€£ 3.3 çº¿æ€§åŒºåŸŸçš„å‡ ä½• â€£ 3 ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒºåŸŸ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºç›¸é‡ï¼šç»¼è¿°")ä¸­ç¬¬ä¸‰ä¸ªå›¾ç¤ºçš„åŒºåŸŸç›¸åŒã€‚
- en: 'For every linear region defined by layer $l$ with an activation set ${\mathbb{S}}^{l}$,
    the dimension of the output of the corresponding transformation $\Omega_{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$
    is at most $|{\mathbb{S}}^{l}|$ since $\text{rank}(\Omega_{{\mathbb{S}}^{l}})=|{\mathbb{S}}^{l}|$.
    Hence, the dimension of the output of every linear region defined by a rectifier
    network is upper bounded by its smallest activation set across all layers. This
    phenomenon was first identified byÂ Serra etÂ al. ([2018](#bib.bib282)) as the *bottleneck
    effect*. In neural networks with uniform width, this phenomenon leads to a surprising
    consequence: the number of linear regions with full-dimensional output is at most
    one. There are also consequences to the maximum number of linear regions that
    can be defined, as we discuss later.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç”±å±‚$l$å®šä¹‰çš„æ¯ä¸ªçº¿æ€§åŒºåŸŸï¼Œå…¶æ¿€æ´»é›†ä¸º${\mathbb{S}}^{l}$ï¼Œç›¸åº”å˜æ¢$\Omega_{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$çš„è¾“å‡ºç»´åº¦è‡³å¤šä¸º$|{\mathbb{S}}^{l}|$ï¼Œå› ä¸º$\text{rank}(\Omega_{{\mathbb{S}}^{l}})=|{\mathbb{S}}^{l}|$ã€‚å› æ­¤ï¼Œç”±æ•´æµç½‘ç»œå®šä¹‰çš„æ¯ä¸ªçº¿æ€§åŒºåŸŸçš„è¾“å‡ºç»´åº¦ç”±å…¶åœ¨æ‰€æœ‰å±‚ä¸­çš„æœ€å°æ¿€æ´»é›†ä¸Šé™ã€‚è¿™ä¸ªç°è±¡é¦–æ¬¡ç”±Serraç­‰äºº([2018](#bib.bib282))è¯†åˆ«ä¸º*ç“¶é¢ˆæ•ˆåº”*ã€‚åœ¨å®½åº¦å‡åŒ€çš„ç¥ç»ç½‘ç»œä¸­ï¼Œè¿™ä¸€ç°è±¡å¯¼è‡´äº†ä¸€ä¸ªä»¤äººæƒŠè®¶çš„ç»“æœï¼šå…·æœ‰å…¨ç»´è¾“å‡ºçš„çº¿æ€§åŒºåŸŸæœ€å¤šåªæœ‰ä¸€ä¸ªã€‚å…³äºå¯ä»¥å®šä¹‰çš„æœ€å¤§çº¿æ€§åŒºåŸŸæ•°é‡ï¼Œè¿˜æœ‰å…¶ä»–åæœï¼Œæˆ‘ä»¬å°†åœ¨åé¢è®¨è®ºã€‚
- en: 3.3.1 The geometry of decision regions
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 å†³ç­–åŒºåŸŸçš„å‡ ä½•
- en: It is also common to study what inputs are associated with each class by a neural
    network. The set of inputs associated with the same class define a *decision region*.
    Difficulties in modeling functions such as the Boolean XOR in shallow networks
    are related to limitations on the form of the decision regions, which may be limited
    by the depth of the neural network. For example, Makhoul etÂ al. ([1989](#bib.bib207))
    showed that two layers suffice to obtain disconnected decision regions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ç ”ç©¶ç¥ç»ç½‘ç»œä¸æ¯ä¸ªç±»åˆ«ç›¸å…³çš„è¾“å…¥ä¹Ÿå¾ˆå¸¸è§ã€‚ä¸åŒä¸€ç±»åˆ«ç›¸å…³çš„è¾“å…¥é›†åˆå®šä¹‰äº†ä¸€ä¸ª*å†³ç­–åŒºåŸŸ*ã€‚ä¾‹å¦‚ï¼ŒMakhoulç­‰äºº([1989](#bib.bib207))æ˜¾ç¤ºï¼Œä¸¤å±‚è¶³ä»¥è·å¾—ä¸è¿é€šçš„å†³ç­–åŒºåŸŸã€‚
- en: The softmax layer is typically used for the output of neural networks trained
    on classification problems, in which the largest output corresponds to the class
    to which the input is associated. In rectifier networks coupled with a softmax
    layer, the decision regions can also be defined by polyhedra. Although the output
    of the softmax layer is not piecewise linear, its largest output corresponds to
    its largest input. Hence, every linear region ${\mathbb{I}}$ defined by layers
    1 to $L-1$ is partitioned by the softmax layer into decision regions where ${\bm{h}}^{L-1}_{i}\geq{\bm{h}}^{L-1}_{j}~{}\forall
    j\neq i$ for each class $i$ associated with the input ${\bm{h}}_{i}^{L-1}$ to
    the softmax layer. Therefore, each decision region of a rectifier networks consist
    of a union of polyhedra.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax å±‚é€šå¸¸ç”¨äºè®­ç»ƒæœ‰åˆ†ç±»é—®é¢˜çš„ç¥ç»ç½‘ç»œçš„è¾“å‡ºï¼Œå…¶ä¸­æœ€å¤§è¾“å‡ºå¯¹åº”äºè¾“å…¥å…³è”çš„ç±»åˆ«ã€‚åœ¨ä¸ softmax å±‚è€¦åˆçš„æ•´æµå™¨ç½‘ç»œä¸­ï¼Œå†³ç­–åŒºåŸŸä¹Ÿå¯ä»¥ç”±å¤šé¢ä½“å®šä¹‰ã€‚å°½ç®¡
    softmax å±‚çš„è¾“å‡ºä¸æ˜¯åˆ†æ®µçº¿æ€§çš„ï¼Œä½†å…¶æœ€å¤§è¾“å‡ºå¯¹åº”äºå…¶æœ€å¤§è¾“å…¥ã€‚å› æ­¤ï¼Œç”±ç¬¬ 1 å±‚åˆ°ç¬¬ $L-1$ å±‚å®šä¹‰çš„æ¯ä¸ªçº¿æ€§åŒºåŸŸ ${\mathbb{I}}$
    è¢« softmax å±‚åˆ’åˆ†ä¸ºå†³ç­–åŒºåŸŸï¼Œå…¶ä¸­æ¯ä¸ªä¸è¾“å…¥ ${\bm{h}}_{i}^{L-1}$ ç›¸å…³çš„ç±»åˆ« $i$ æ»¡è¶³ ${\bm{h}}^{L-1}_{i}\geq{\bm{h}}^{L-1}_{j}~{}\forall
    j\neq i$ã€‚å› æ­¤ï¼Œæ•´æµå™¨ç½‘ç»œçš„æ¯ä¸ªå†³ç­–åŒºåŸŸéƒ½ç”±å¤šé¢ä½“çš„å¹¶é›†ç»„æˆã€‚
- en: 'In fact, we may say further in the typical setting where no hidden layer is
    wider than the input â€”i.e., $n_{0}\geq n_{l}~{}\forall l\in{\mathbb{L}}$: Nguyen
    etÂ al. ([2018](#bib.bib233)) showed that at least one layer $l\in{\mathbb{L}}$
    must be such that $n_{l}>n_{0}$ for the network to present disconnected decision
    regions; and Grigsby and Lindsey ([2022](#bib.bib131)) showed that, for an input
    size $n_{0}\geq 2$, the decision regions are either empty or unbounded.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥è¯´ï¼Œåœ¨æ²¡æœ‰ä»»ä½•éšè—å±‚å®½åº¦è¶…è¿‡è¾“å…¥å±‚çš„å…¸å‹è®¾ç½®ä¸­â€”â€”å³ $n_{0}\geq n_{l}~{}\forall l\in{\mathbb{L}}$ï¼šNguyen
    ç­‰äºº ([2018](#bib.bib233)) è¡¨æ˜ï¼Œè‡³å°‘æœ‰ä¸€ä¸ªå±‚ $l\in{\mathbb{L}}$ å¿…é¡»æ»¡è¶³ $n_{l}>n_{0}$ï¼Œä»¥ä½¿ç½‘ç»œå‘ˆç°å‡ºæ–­è£‚çš„å†³ç­–åŒºåŸŸï¼›è€Œ
    Grigsby å’Œ Lindsey ([2022](#bib.bib131)) è¯æ˜ï¼Œå¯¹äºè¾“å…¥å¤§å° $n_{0}\geq 2$ï¼Œå†³ç­–åŒºåŸŸè¦ä¹ˆä¸ºç©ºï¼Œè¦ä¹ˆæ˜¯æ— ç•Œçš„ã€‚
- en: 3.4 The number of linear regions
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 çº¿æ€§åŒºåŸŸçš„æ•°é‡
- en: 'We have seen conditions that affect the number of linear regions both positively
    and negatively. We discuss these and other analytical results in SectionÂ [3.4.1](#S3.SS4.SSS1
    "3.4.1 Analytical results â€£ 3.4 The number of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"),
    and then discuss work on counting linear regions in practice in SectionÂ [3.4.2](#S3.SS4.SSS2
    "3.4.2 Counting linear regions â€£ 3.4 The number of linear regions â€£ 3 The Linear
    Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬å·²ç»çœ‹åˆ°å½±å“çº¿æ€§åŒºåŸŸæ•°é‡çš„æ¡ä»¶ï¼Œè¿™äº›æ¡ä»¶æ—¢æœ‰æ­£é¢å½±å“ä¹Ÿæœ‰è´Ÿé¢å½±å“ã€‚æˆ‘ä»¬åœ¨ç¬¬[3.4.1èŠ‚](#S3.SS4.SSS1 "3.4.1 Analytical
    results â€£ 3.4 The number of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")ä¸­è®¨è®ºäº†è¿™äº›æ¡ä»¶ä»¥åŠå…¶ä»–åˆ†æç»“æœï¼Œç„¶ååœ¨ç¬¬[3.4.2èŠ‚](#S3.SS4.SSS2
    "3.4.2 Counting linear regions â€£ 3.4 The number of linear regions â€£ 3 The Linear
    Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")ä¸­è®¨è®ºäº†å®é™…è®¡ç®—çº¿æ€§åŒºåŸŸçš„å·¥ä½œã€‚'
- en: 3.4.1 Analytical results
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 åˆ†æç»“æœ
- en: At least three lines of work on analytical results have brought important insights.
    The first line is based on constructing networks with a large number of linear
    regions, which leads to lower bounds on the maximum number of linear regions.
    The second line is based on showing how the network architecture â€”in particular
    its hyperparametersâ€” may impact the hyperplane arrangements defined by the layers,
    which leads to upper bounds on the maximum number of linear regions. The third
    line is based on characterizing the parameters of neural networks according to
    how they are initialized and updated along training, which leads to results on
    the expected number of linear regions for such networks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³å°‘æœ‰ä¸‰æ¡åˆ†æç»“æœçš„å·¥ä½œå¸¦æ¥äº†é‡è¦çš„è§è§£ã€‚ç¬¬ä¸€æ¡å·¥ä½œåŸºäºæ„å»ºå…·æœ‰å¤§é‡çº¿æ€§åŒºåŸŸçš„ç½‘ç»œï¼Œè¿™å¯¼è‡´äº†çº¿æ€§åŒºåŸŸæœ€å¤§æ•°é‡çš„ä¸‹ç•Œã€‚ç¬¬äºŒæ¡å·¥ä½œåŸºäºå±•ç¤ºç½‘ç»œæ¶æ„â€”â€”ç‰¹åˆ«æ˜¯å…¶è¶…å‚æ•°â€”â€”å¦‚ä½•å½±å“ç”±å±‚å®šä¹‰çš„è¶…å¹³é¢æ’åˆ—ï¼Œè¿™å¯¼è‡´äº†çº¿æ€§åŒºåŸŸæœ€å¤§æ•°é‡çš„ä¸Šç•Œã€‚ç¬¬ä¸‰æ¡å·¥ä½œåŸºäºæ ¹æ®ç¥ç»ç½‘ç»œçš„åˆå§‹åŒ–å’Œè®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°å‚æ•°çš„æ–¹å¼æ¥è¡¨å¾è¿™äº›ç½‘ç»œçš„å‚æ•°ï¼Œè¿™å¯¼è‡´äº†å…³äºè¿™äº›ç½‘ç»œé¢„æœŸçº¿æ€§åŒºåŸŸæ•°é‡çš„ç»“æœã€‚
- en: Lower bounds
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ä¸‹ç•Œ
- en: 'The lower bounds on the maximum number of linear regions are obtained through
    a careful choice of network parameters aimed at increasing the number of linear
    regions. In some cases, they also depend on particular choices of hyperparameters.
    We present them by order of refinement in TableÂ [2](#S3.T2 "Table 2 â€£ Lower bounds
    â€£ 3.4.1 Analytical results â€£ 3.4 The number of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ€å¤§çº¿æ€§åŒºåŸŸæ•°é‡çš„ä¸‹ç•Œé€šè¿‡ä»”ç»†é€‰æ‹©ç½‘ç»œå‚æ•°æ¥è·å¾—ï¼Œæ—¨åœ¨å¢åŠ çº¿æ€§åŒºåŸŸçš„æ•°é‡ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå®ƒä»¬è¿˜ä¾èµ–äºç‰¹å®šçš„è¶…å‚æ•°é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨è¡¨ [2](#S3.T2
    "Table 2 â€£ Lower bounds â€£ 3.4.1 Analytical results â€£ 3.4 The number of linear
    regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey") ä¸­æŒ‰ç²¾ç»†ç¨‹åº¦é¡ºåºå±•ç¤ºäº†å®ƒä»¬ã€‚'
- en: 'The first lower bound was introduced byÂ Pascanu etÂ al. ([2014](#bib.bib243))
    and then improved by those authors with a new construction technique inÂ MontÃºfar
    etÂ al. ([2014](#bib.bib224)). In fact, ExampleÂ [1](#Thmexample1 "Example 1 â€£ 3.1
    The combinatorial aspect of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") shows the case
    in which $n_{0}=1$ for the technique inÂ MontÃºfar etÂ al. ([2014](#bib.bib224)).
    While a different construction is proposed byÂ Telgarsky ([2015](#bib.bib304)),
    subsequent developments in the literature have been based onÂ MontÃºfar etÂ al. ([2014](#bib.bib224)).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¬¬ä¸€ä¸ªä¸‹ç•Œç”±Pascanu ç­‰äºº ([2014](#bib.bib243)) æå‡ºï¼Œåæ¥ç”±MontÃºfar ç­‰äºº ([2014](#bib.bib224))
    ä½¿ç”¨æ–°çš„æ„é€ æŠ€æœ¯è¿›è¡Œäº†æ”¹è¿›ã€‚å®é™…ä¸Šï¼Œç¤ºä¾‹ [1](#Thmexample1 "Example 1 â€£ 3.1 The combinatorial aspect
    of linear regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey") å±•ç¤ºäº† $n_{0}=1$ çš„æƒ…å†µä¸‹ï¼ŒMontÃºfar ç­‰äºº ([2014](#bib.bib224))
    çš„æŠ€æœ¯ã€‚è™½ç„¶Telgarsky ([2015](#bib.bib304)) æå‡ºäº†ä¸åŒçš„æ„é€ æ–¹æ³•ï¼Œä½†åç»­æ–‡çŒ®çš„å‘å±•ä»åŸºäºMontÃºfar ç­‰äºº ([2014](#bib.bib224))
    çš„å·¥ä½œã€‚'
- en: 'The lower bound byÂ Arora etÂ al. ([2018](#bib.bib8)) is based on a different
    technique to construct a first wide layer based on zonotopes, which is then followed
    by the same layers as inÂ MontÃºfar etÂ al. ([2014](#bib.bib224)). The first lower
    bound byÂ Serra etÂ al. ([2018](#bib.bib282)) reflects a slight change to the technique
    used byÂ MontÃºfar etÂ al. ([2014](#bib.bib224)), which in terms of ExampleÂ [1](#Thmexample1
    "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") corresponds
    to using $n$ neurons to define $n+1$ instead of $n$ slopes on $[0,1]$. The second
    lower bound byÂ Serra etÂ al. ([2018](#bib.bib282)) extends that ofÂ Arora etÂ al.
    ([2018](#bib.bib8)) by changing in the same way the construction of the subsequent
    layers of the network.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'Arora ç­‰äºº ([2018](#bib.bib8)) çš„ä¸‹ç•ŒåŸºäºä¸€ç§ä¸åŒçš„æŠ€æœ¯ï¼Œå³åŸºäºzonotopesæ„é€ ç¬¬ä¸€å±‚å®½å±‚ï¼Œç„¶åç´§æ¥ç€ä½¿ç”¨ä¸MontÃºfar
    ç­‰äºº ([2014](#bib.bib224)) ç›¸åŒçš„å±‚ã€‚Serra ç­‰äºº ([2018](#bib.bib282)) çš„ç¬¬ä¸€ä¸ªä¸‹ç•Œåæ˜ äº†å¯¹MontÃºfar
    ç­‰äºº ([2014](#bib.bib224)) æŠ€æœ¯çš„è½»å¾®å˜åŒ–ï¼Œè¿™åœ¨ç¤ºä¾‹ [1](#Thmexample1 "Example 1 â€£ 3.1 The combinatorial
    aspect of linear regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey") ä¸­ç›¸å½“äºä½¿ç”¨ $n$ ä¸ªç¥ç»å…ƒæ¥å®šä¹‰ $n+1$ ä¸ªè€Œé $n$
    ä¸ªæ–œç‡åœ¨ $[0,1]$ ä¸Šã€‚Serra ç­‰äºº ([2018](#bib.bib282)) çš„ç¬¬äºŒä¸ªä¸‹ç•Œé€šè¿‡ä»¥ç›¸åŒçš„æ–¹å¼æ”¹å˜ç½‘ç»œåç»­å±‚çš„æ„é€ ï¼Œæ‰©å±•äº†Arora
    ç­‰äºº ([2018](#bib.bib8)) çš„ç»“æœã€‚'
- en: 'Table 2: Lower bounds on the maximum number of linear regions defined by a
    neural network.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨2ï¼šç¥ç»ç½‘ç»œå®šä¹‰çš„çº¿æ€§åŒºåŸŸæœ€å¤§æ•°é‡çš„ä¸‹ç•Œã€‚
- en: '| Reference | Bound and conditions |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| å‚è€ƒæ–‡çŒ® | ä¸‹ç•ŒåŠæ¡ä»¶ |'
- en: '| --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Â Pascanu etÂ al. ([2014](#bib.bib243)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Â Pascanu ç­‰äºº ([2014](#bib.bib243)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$
    |'
- en: '| Â MontÃºfar etÂ al. ([2014](#bib.bib224)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$,
    where $n_{l}\geq n_{0}~{}\forall l\in{\mathbb{L}}$ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Â MontÃºfar ç­‰äºº ([2014](#bib.bib224)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$ï¼Œå…¶ä¸­
    $n_{l}\geq n_{0}~{}\forall l\in{\mathbb{L}}$ |'
- en: '| Â Telgarsky ([2015](#bib.bib304)) | $2^{\frac{L-3}{2}}$, where $n_{i}=1$ for
    $i$ odd, $n_{i}=2$ for $i$ even, and $L-3$ divides by 2 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Â Telgarsky ([2015](#bib.bib304)) | $2^{\frac{L-3}{2}}$ï¼Œå…¶ä¸­ $n_{i}=1$ å¯¹äºå¥‡æ•°
    $i$ï¼Œ$n_{i}=2$ å¯¹äºå¶æ•° $i$ï¼Œä¸” $L-3$ èƒ½è¢«2æ•´é™¤ |'
- en: '| Â Arora etÂ al. ([2018](#bib.bib8)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}w^{L-1}$,
    where $2m=n_{1}$ and $w=n_{l}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Â Arora ç­‰äºº ([2018](#bib.bib8)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}w^{L-1}$ï¼Œå…¶ä¸­
    $2m=n_{1}$ ä¸” $w=n_{l}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$ |'
- en: '| Â Serra etÂ al. ([2018](#bib.bib282)) | $\left(\prod\limits_{l=1}^{L-1}\left(\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor+1\right)^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$,
    where $n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}$ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Serra ç­‰äºº ([2018](#bib.bib282)) | $\left(\prod\limits_{l=1}^{L-1}\left(\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor+1\right)^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$ï¼Œå…¶ä¸­
    $n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}$ |'
- en: '| Â Serra etÂ al. ([2018](#bib.bib282)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}(w+1)^{L-1}$,
    where $2m=n_{1}$ and $w=n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Serra ç­‰äºº ([2018](#bib.bib282)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}(w+1)^{L-1}$ï¼Œå…¶ä¸­
    $2m=n_{1}$ å’Œ $w=n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$ |'
- en: Upper bounds
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ä¸Šç•Œ
- en: 'The upper bounds on the maximum number of linear regions are obtained by primarily
    considering changes to the geometry of the linear regions from one layer to another,
    as previously outlined and revisited below. We present those with a close form
    by order of refinement in TableÂ [3](#S3.T3 "Table 3 â€£ Upper bounds â€£ 3.4.1 Analytical
    results â€£ 3.4 The number of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ€å¤§çº¿æ€§åŒºåŸŸæ•°é‡çš„ä¸Šç•Œä¸»è¦é€šè¿‡è€ƒè™‘ä»ä¸€å±‚åˆ°å¦ä¸€å±‚çº¿æ€§åŒºåŸŸå‡ ä½•å½¢çŠ¶çš„å˜åŒ–æ¥è·å¾—ï¼Œå¦‚ä¸‹æ‰€è¿°å¹¶å†æ¬¡å›é¡¾ã€‚æˆ‘ä»¬åœ¨è¡¨ [3](#S3.T3 "Table 3 â€£
    Upper bounds â€£ 3.4.1 Analytical results â€£ 3.4 The number of linear regions â€£ 3
    The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey") ä¸­æŒ‰ç²¾ç»†åŒ–é¡ºåºå±•ç¤ºäº†è¿™äº›ä¸Šç•Œçš„é—­å¼å½¢å¼ã€‚'
- en: Pascanu etÂ al. ([2014](#bib.bib243)) established the connection between linear
    regions and hyperplane arrangements, which lead to the tight bound for shallow
    networks based onÂ Zaslavsky ([1975](#bib.bib352)) for activation hyperplanes in
    general position. MontÃºfar etÂ al. ([2014](#bib.bib224)) defined the first bound
    for deep networks based on enumerating all activation sets. The subsequent upper
    bounds extended the result byÂ Pascanu etÂ al. ([2014](#bib.bib243)) to deep networks
    by considering its successive application through the sequence of layers of the
    network.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Pascanu ç­‰äºº ([2014](#bib.bib243)) å»ºç«‹äº†çº¿æ€§åŒºåŸŸä¸è¶…å¹³é¢æ’åˆ—ä¹‹é—´çš„è”ç³»ï¼Œè¿™å¯¼è‡´äº†åŸºäº Zaslavsky ([1975](#bib.bib352))
    çš„æµ…å±‚ç½‘ç»œçš„ç´§ç•Œé™ï¼Œç”¨äºä¸€èˆ¬ä½ç½®çš„æ¿€æ´»è¶…å¹³é¢ã€‚MontÃºfar ç­‰äºº ([2014](#bib.bib224)) å®šä¹‰äº†æ·±å±‚ç½‘ç»œçš„ç¬¬ä¸€ä¸ªç•Œé™ï¼ŒåŸºäºæšä¸¾æ‰€æœ‰æ¿€æ´»é›†åˆã€‚éšåï¼Œä¸Šç•Œé€šè¿‡è€ƒè™‘å…¶åœ¨ç½‘ç»œå±‚åºåˆ—ä¸­çš„è¿ç»­åº”ç”¨ï¼Œæ‰©å±•äº†
    Pascanu ç­‰äºº ([2014](#bib.bib243)) çš„ç»“æœåˆ°æ·±å±‚ç½‘ç»œã€‚
- en: In the case of *deep* networks, where $L>1$, we need to consider how the linear
    regions defined up to a given layer of the network can be further partitioned
    by the next layers. We start by assuming that every linear region defined by the
    first $l-1$ layers is then subdivided into the maximum possible number of linear
    regions defined by the activation hyperplanes of layer $l$. That leads to the
    bound inÂ Raghu etÂ al. ([2017](#bib.bib253)), which is implicit in their proof
    of an asymptotic bound of $O(n^{n_{0}L})$, where $n$ is used as the width of every
    layer. However, there are many ways in which this bound can be refined upon careful
    examination. First, the dimension of the input of layer $l$ â€”i.e., the output
    of layer $l-1$â€” within each linear region is never larger than the smallest dimension
    among layers $1$ to $l$, since for every linear region we have an affine transformation
    between inputs and outputs of each layer (MontÃºfar, [2017](#bib.bib223)). Second,
    the dimension of the input coming through each linear region is in fact bounded
    by the smallest number of active units in each of the previous layers (Serra etÂ al.,
    [2018](#bib.bib282)). This leads to a tight upper bound for $n_{0}=1$, since it
    matches the lower bound in Â Serra etÂ al. ([2018](#bib.bib282)). Finally, the activation
    hyperplane of some units may not partition the linear regions because all possible
    inputs to the unit are in the same half-space, and in some of those cases the
    unit may never produce a positive output. For the number $k$ of active units in
    a given layer $l$, we can use the network parameters to calculate the maximum
    number of units that can be active in the next layer, $\mathcal{A}_{l}(k)$, as
    well as the number of units that can be active or inactive for different inputs,
    $\mathcal{I}_{l}(k)$ (Serra and Ramalingam, [2020](#bib.bib281)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº*æ·±åº¦*ç½‘ç»œï¼Œå…¶ä¸­ $L>1$ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘åˆ°å¦‚ä½•é€šè¿‡åç»­å±‚è¿›ä¸€æ­¥åˆ’åˆ†åˆ°ç»™å®šå±‚çš„çº¿æ€§åŒºåŸŸã€‚æˆ‘ä»¬é¦–å…ˆå‡è®¾ç”±å‰ $l-1$ å±‚å®šä¹‰çš„æ¯ä¸ªçº¿æ€§åŒºåŸŸç„¶åè¢«åˆ’åˆ†æˆç”±ç¬¬
    $l$ å±‚çš„æ¿€æ´»è¶…å¹³é¢å®šä¹‰çš„æœ€å¤§æ•°é‡çš„çº¿æ€§åŒºåŸŸã€‚è¿™å¯¼è‡´äº† Raghu ç­‰äºº ([2017](#bib.bib253)) çš„ç•Œé™ï¼Œè¯¥ç•Œé™åœ¨ä»–ä»¬å¯¹ $O(n^{n_{0}L})$
    çš„æ¸è¿‘ç•Œé™çš„è¯æ˜ä¸­éšå«ï¼Œå…¶ä¸­ $n$ ä½œä¸ºæ¯å±‚çš„å®½åº¦ã€‚ç„¶è€Œï¼Œåœ¨ä»”ç»†æ£€æŸ¥åï¼Œè¿™ä¸ªç•Œé™æœ‰å¾ˆå¤šå¯ä»¥æ”¹è¿›çš„åœ°æ–¹ã€‚é¦–å…ˆï¼Œç¬¬ $l$ å±‚çš„è¾“å…¥ç»´åº¦â€”â€”å³ç¬¬ $l-1$
    å±‚çš„è¾“å‡ºâ€”â€”åœ¨æ¯ä¸ªçº¿æ€§åŒºåŸŸå†…ä»ä¸å¤§äºç¬¬ $1$ å±‚åˆ°ç¬¬ $l$ å±‚ä¸­çš„æœ€å°ç»´åº¦ï¼Œå› ä¸ºå¯¹äºæ¯ä¸ªçº¿æ€§åŒºåŸŸï¼Œæˆ‘ä»¬åœ¨æ¯ä¸€å±‚çš„è¾“å…¥å’Œè¾“å‡ºä¹‹é—´éƒ½æœ‰ä¸€ä¸ªä»¿å°„å˜æ¢ï¼ˆMontÃºfar,
    [2017](#bib.bib223)ï¼‰ã€‚å…¶æ¬¡ï¼Œé€šè¿‡æ¯ä¸ªçº¿æ€§åŒºåŸŸä¼ æ¥çš„è¾“å…¥ç»´åº¦å®é™…ä¸Šå—åˆ°å‰é¢æ¯å±‚ä¸­æ´»è·ƒå•å…ƒçš„æœ€å°æ•°é‡çš„é™åˆ¶ï¼ˆSerra ç­‰äºº, [2018](#bib.bib282)ï¼‰ã€‚è¿™å¯¼è‡´
    $n_{0}=1$ çš„ç´§ç•Œé™ï¼Œå› ä¸ºå®ƒä¸ Serra ç­‰äºº ([2018](#bib.bib282)) ä¸­çš„ä¸‹ç•Œç›¸åŒ¹é…ã€‚æœ€åï¼Œä¸€äº›å•å…ƒçš„æ¿€æ´»è¶…å¹³é¢å¯èƒ½ä¸ä¼šåˆ’åˆ†çº¿æ€§åŒºåŸŸï¼Œå› ä¸ºæ‰€æœ‰å¯èƒ½çš„è¾“å…¥éƒ½ä½äºåŒä¸€åŠç©ºé—´ä¸­ï¼Œåœ¨ä¸€äº›æƒ…å†µä¸‹ï¼Œè¿™äº›å•å…ƒå¯èƒ½æ°¸è¿œä¸ä¼šäº§ç”Ÿæ­£è¾“å‡ºã€‚å¯¹äºç»™å®šå±‚
    $l$ ä¸­çš„æ´»è·ƒå•å…ƒæ•°é‡ $k$ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç½‘ç»œå‚æ•°è®¡ç®—ä¸‹ä¸€å±‚ä¸­å¯ä»¥æ´»è·ƒçš„å•å…ƒçš„æœ€å¤§æ•°é‡ $\mathcal{A}_{l}(k)$ï¼Œä»¥åŠä¸åŒè¾“å…¥ä¸‹å¯ä»¥æ´»è·ƒæˆ–ä¸æ´»è·ƒçš„å•å…ƒæ•°é‡
    $\mathcal{I}_{l}(k)$ï¼ˆSerra å’Œ Ramalingam, [2020](#bib.bib281)ï¼‰ã€‚
- en: Hinz and vanÂ de Geer ([2019](#bib.bib150)) observed that the upper bound by
    Serra etÂ al. ([2018](#bib.bib282)) can be tightened by explicitly computing a
    recursive histogram of linear regions on the layers of the neural network according
    to the dimension of their image subspace. However, the resulting bound is not
    explicitly defined in terms of the network hyperparameters, and hence cannot be
    included on the table. This work is further extended inÂ Hinz ([2021](#bib.bib149))
    by also allowing a composition of bounds on subnetworks instead of only on the
    sequence of layers. Another extension of the framework fromÂ Hinz and vanÂ de Geer
    ([2019](#bib.bib150)) by Xie etÂ al. ([2020c](#bib.bib344)) highlights that residual
    connections prevent the bottleneck effect in ResNets, by which reason such networks
    tend to have more linear regions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Hinz å’Œ van de Geer ([2019](#bib.bib150)) è§‚å¯Ÿåˆ°ï¼Œé€šè¿‡æ ¹æ®å›¾åƒå­ç©ºé—´çš„ç»´åº¦æ˜¾å¼è®¡ç®—ç¥ç»ç½‘ç»œå±‚ä¸Šçš„çº¿æ€§åŒºåŸŸçš„é€’å½’ç›´æ–¹å›¾ï¼Œå¯ä»¥æ”¶ç´§
    Serra ç­‰äºº ([2018](#bib.bib282)) æå‡ºçš„ä¸Šç•Œã€‚ç„¶è€Œï¼Œç»“æœç•Œé™å¹¶æ²¡æœ‰ä»¥ç½‘ç»œè¶…å‚æ•°æ˜ç¡®çš„æ–¹å¼å®šä¹‰ï¼Œå› æ­¤ä¸èƒ½åŒ…å«åœ¨è¡¨æ ¼ä¸­ã€‚Hinz ([2021](#bib.bib149))
    è¿›ä¸€æ­¥æ‰©å±•äº†è¿™é¡¹å·¥ä½œï¼Œå…è®¸å¯¹å­ç½‘ç»œçš„ç•Œé™è¿›è¡Œç»„åˆï¼Œè€Œä¸ä»…ä»…æ˜¯å¯¹å±‚åºåˆ—è¿›è¡Œç»„åˆã€‚Xie ç­‰äºº ([2020c](#bib.bib344)) å¯¹ Hinz å’Œ
    van de Geer ([2019](#bib.bib150)) æ¡†æ¶çš„å¦ä¸€ä¸ªæ‰©å±•å¼ºè°ƒï¼Œæ®‹å·®è¿æ¥å¯ä»¥é˜²æ­¢ ResNets ä¸­çš„ç“¶é¢ˆæ•ˆåº”ï¼Œè¿™ä¹Ÿæ˜¯è¿™äº›ç½‘ç»œè¶‹å‘äºæ‹¥æœ‰æ›´å¤šçº¿æ€§åŒºåŸŸçš„åŸå› ã€‚
- en: Cai etÂ al. ([2023](#bib.bib46)) proposed a separate recursive bound based on
    Serra etÂ al. ([2018](#bib.bib282)) to account for the sparsity of the weight matrices,
    which illustrates how pruning connections may affect the maximum number of linear
    regions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Cai ç­‰äºº ([2023](#bib.bib46)) æå‡ºäº†ä¸€ä¸ªåŸºäº Serra ç­‰äºº ([2018](#bib.bib282)) çš„å•ç‹¬é€’å½’ç•Œé™ï¼Œä»¥è€ƒè™‘æƒé‡çŸ©é˜µçš„ç¨€ç–æ€§ï¼Œè¿™è¯´æ˜äº†å‰ªæè¿æ¥å¯èƒ½å¦‚ä½•å½±å“çº¿æ€§åŒºåŸŸçš„æœ€å¤§æ•°é‡ã€‚
- en: 'The results above have also been extended to other architectures. In some cases,
    results on other types of activations are also part of the papers previously mentioned:
    MontÃºfar etÂ al. ([2014](#bib.bib224)) and Serra etÂ al. ([2018](#bib.bib282)) present
    upper bounds for *maxout* networks; Raghu etÂ al. ([2017](#bib.bib253)) present
    an upper bound for networks using *hard tanh* activation. In other cases, the
    ideas discussed above have been adapted for sparser networks with parameter sharing:
    Xiong etÂ al. ([2020](#bib.bib345)) present upper and lower bounds for convolutional
    networks, which are shown to asymptotically define more linear regions per parameter
    than rectifier networks with the same input size and number of layers. Chen etÂ al.
    ([2022a](#bib.bib50)) present upper and lower bounds for graph convolutional networks.
    Matoba etÂ al. ([2022](#bib.bib214)) discuss the expresiveness of the maxpooling
    layers typically used in convolutional neural networks through their equivalence
    to a sequence of rectifier layers. Moreover, Goujon etÂ al. ([2022](#bib.bib128))
    present results for recently proposed activation functions, such as DeepSplineÂ (Agostinelli
    etÂ al., [2015](#bib.bib2), Unser, [2019](#bib.bib314), Bohra etÂ al., [2020](#bib.bib34))
    and GroupSortÂ (Anil etÂ al., [2019](#bib.bib6)).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ç»“æœä¹Ÿæ‰©å±•åˆ°äº†å…¶ä»–æ¶æ„ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå…¶ä»–ç±»å‹æ¿€æ´»å‡½æ•°çš„ç»“æœä¹ŸåŒ…å«åœ¨ä¹‹å‰æåˆ°çš„è®ºæ–‡ä¸­ï¼šMontÃºfar ç­‰äºº ([2014](#bib.bib224))
    å’Œ Serra ç­‰äºº ([2018](#bib.bib282)) æå‡ºäº†*maxout*ç½‘ç»œçš„ä¸Šç•Œï¼›Raghu ç­‰äºº ([2017](#bib.bib253))
    æå‡ºäº†ä½¿ç”¨*hard tanh*æ¿€æ´»å‡½æ•°çš„ç½‘ç»œçš„ä¸Šç•Œã€‚åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œä»¥ä¸Šè®¨è®ºçš„æ€æƒ³å·²è¢«æ”¹ç¼–ä¸ºå…·æœ‰å‚æ•°å…±äº«çš„ç¨€ç–ç½‘ç»œï¼šXiong ç­‰äºº ([2020](#bib.bib345))
    æå‡ºäº†å·ç§¯ç½‘ç»œçš„ä¸Šç•Œå’Œä¸‹ç•Œï¼Œè¿™äº›ç½‘ç»œåœ¨è¾“å…¥å¤§å°å’Œå±‚æ•°ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œè¢«è¯æ˜åœ¨æ¯ä¸ªå‚æ•°ä¸Šå®šä¹‰äº†æ›´å¤šçš„çº¿æ€§åŒºåŸŸã€‚Chen ç­‰äºº ([2022a](#bib.bib50))
    æå‡ºäº†å›¾å·ç§¯ç½‘ç»œçš„ä¸Šç•Œå’Œä¸‹ç•Œã€‚Matoba ç­‰äºº ([2022](#bib.bib214)) è®¨è®ºäº†å·ç§¯ç¥ç»ç½‘ç»œä¸­é€šå¸¸ä½¿ç”¨çš„maxpoolingå±‚çš„è¡¨è¾¾èƒ½åŠ›ï¼Œé€šè¿‡å®ƒä»¬ä¸ä¸€ç³»åˆ—rectifierå±‚çš„ç­‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒGoujon
    ç­‰äºº ([2022](#bib.bib128)) æå‡ºäº†æœ€è¿‘æå‡ºçš„æ¿€æ´»å‡½æ•°çš„ç»“æœï¼Œä¾‹å¦‚ DeepSpline (Agostinelli ç­‰äººï¼Œ[2015](#bib.bib2)ï¼ŒUnserï¼Œ[2019](#bib.bib314)ï¼ŒBohra
    ç­‰äººï¼Œ[2020](#bib.bib34)) å’Œ GroupSort (Anil ç­‰äººï¼Œ[2019](#bib.bib6))ã€‚
- en: Some of the results above were also revisited through the lenses of tropical
    algebra, in which every linear region corresponds to a tropical hypersurface (Zhang
    etÂ al., [2018b](#bib.bib357), Charisopoulos and Maragos, [2018](#bib.bib48), Maragos
    etÂ al., [2021](#bib.bib212)). Notably, MontÃºfar etÂ al. ([2022](#bib.bib225)) presented
    considerably tighter upper bounds for the number of linear regions in maxout networks
    with rank $k=3$ or greater.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ä¸€äº›ç»“æœä¹Ÿé€šè¿‡çƒ­å¸¦ä»£æ•°çš„è§†è§’é‡æ–°å®¡è§†ï¼Œå…¶ä¸­æ¯ä¸ªçº¿æ€§åŒºåŸŸå¯¹åº”äºçƒ­å¸¦è¶…è¡¨é¢ (Zhang ç­‰äººï¼Œ[2018b](#bib.bib357)ï¼ŒCharisopoulos
    å’Œ Maragosï¼Œ[2018](#bib.bib48)ï¼ŒMaragos ç­‰äººï¼Œ[2021](#bib.bib212))ã€‚ç‰¹åˆ«æ˜¯ï¼ŒMontÃºfar ç­‰äºº ([2022](#bib.bib225))
    æå‡ºäº†å…·æœ‰ç§© $k=3$ æˆ–æ›´é«˜çš„ maxout ç½‘ç»œä¸­çº¿æ€§åŒºåŸŸæ•°é‡çš„æ›´ç´§çš„ä¸Šç•Œã€‚
- en: Recently, a converse line of work started exploring the minimum dimensions of
    a neural network capable of representing a given piecewise linear function, starting
    with considerations about the minimum depth necessary (Arora etÂ al., [2018](#bib.bib8))
    and further refinements of bounds on the network dimensions (He etÂ al., [2020](#bib.bib142),
    Hertrich etÂ al., [2021](#bib.bib147), Chen etÂ al., [2022b](#bib.bib51)), with
    Chen etÂ al. ([2022b](#bib.bib51)) proposing an algorithm that can construct such
    a neural network. On a related note, Karg and Lucia ([2020](#bib.bib168)) show
    that linear time-invariant systems in model predictive control can be exactly
    expressed by rectifier networks and provide bounds on the width and number of
    layers necessary for a given system, whereas Ferlez and Shoukry ([2020](#bib.bib103))
    describe an algorithm for producing architectures that can be parameterized as
    an optimal model predictive control strategy.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œä¸€é¡¹ç›¸åçš„ç ”ç©¶å¼€å§‹æ¢ç´¢èƒ½å¤Ÿè¡¨ç¤ºç»™å®šåˆ†æ®µçº¿æ€§å‡½æ•°çš„ç¥ç»ç½‘ç»œçš„æœ€å°ç»´åº¦ï¼Œä»è€ƒè™‘å¿…è¦çš„æœ€å°æ·±åº¦å¼€å§‹ (Arora ç­‰äººï¼Œ[2018](#bib.bib8))ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åŒ–ç½‘ç»œç»´åº¦çš„ç•Œé™
    (He ç­‰äººï¼Œ[2020](#bib.bib142)ï¼ŒHertrich ç­‰äººï¼Œ[2021](#bib.bib147)ï¼ŒChen ç­‰äººï¼Œ[2022b](#bib.bib51))ï¼Œå…¶ä¸­
    Chen ç­‰äºº ([2022b](#bib.bib51)) æå‡ºäº†ä¸€ä¸ªç®—æ³•ï¼Œèƒ½å¤Ÿæ„é€ è¿™æ ·çš„ç¥ç»ç½‘ç»œã€‚ç›¸å…³åœ°ï¼ŒKarg å’Œ Lucia ([2020](#bib.bib168))
    æ˜¾ç¤ºæ¨¡å‹é¢„æµ‹æ§åˆ¶ä¸­çš„çº¿æ€§æ—¶é—´ä¸å˜ç³»ç»Ÿå¯ä»¥è¢«å‡†ç¡®åœ°è¡¨è¾¾ä¸º rectifier ç½‘ç»œï¼Œå¹¶æä¾›äº†ç»™å®šç³»ç»Ÿæ‰€éœ€çš„å®½åº¦å’Œå±‚æ•°çš„ç•Œé™ï¼Œè€Œ Ferlez å’Œ Shoukry
    ([2020](#bib.bib103)) æè¿°äº†ä¸€ä¸ªç”Ÿæˆå¯ä»¥å‚æ•°åŒ–ä¸ºæœ€ä¼˜æ¨¡å‹é¢„æµ‹æ§åˆ¶ç­–ç•¥çš„æ¶æ„çš„ç®—æ³•ã€‚
- en: 'Table 3: Upper bounds on the maximum number of linear regions defined by a
    neural network.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 3ï¼šç¥ç»ç½‘ç»œå®šä¹‰çš„æœ€å¤§çº¿æ€§åŒºåŸŸæ•°é‡çš„ä¸Šç•Œã€‚
- en: '| Reference | Bound and conditions |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| å‚è€ƒæ–‡çŒ® | ç•Œé™ä¸æ¡ä»¶ |'
- en: '| --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Â Pascanu etÂ al. ([2014](#bib.bib243)) | $\sum\limits_{i=0}^{n_{0}}\binom{n_{1}}{n_{0}}$
    for shallow networks, $n_{1}\geq n_{0}$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  Pascanu ç­‰äºº ([2014](#bib.bib243)) | å¯¹äºæµ…å±‚ç½‘ç»œï¼Œ$ \sum\limits_{i=0}^{n_{0}}\binom{n_{1}}{n_{0}}
    $ï¼Œå…¶ä¸­ $ n_{1} \geq n_{0} $ |'
- en: '| Â MontÃºfar etÂ al. ([2014](#bib.bib224)) | $2^{\sum\limits_{l=1}^{L}n_{l}}$
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Â MontÃºfar ç­‰äºº ([2014](#bib.bib224)) | $2^{\sum\limits_{l=1}^{L}n_{l}}$ |'
- en: '| Â Raghu etÂ al. ([2017](#bib.bib253)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{n_{l-1}}\binom{n_{l}}{j}$
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Â Raghu ç­‰äºº ([2017](#bib.bib253)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{n_{l-1}}\binom{n_{l}}{j}$
    |'
- en: '| Â MontÃºfar ([2017](#bib.bib223)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{d_{l}}\binom{n_{l}}{j}$,
    $d_{l}=\min\{n_{0},n_{1},\ldots,n_{l-1}\}$ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Â MontÃºfar ([2017](#bib.bib223)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{d_{l}}\binom{n_{l}}{j}$,
    $d_{l}=\min\{n_{0},n_{1},\ldots,n_{l-1}\}$ |'
- en: '| Â Serra etÂ al. ([2018](#bib.bib282)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{n_{l}}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l}~{}\forall l\in{\mathbb{L}}\},\\ d_{l}=\min\{n_{0},n_{1}-j_{1},\ldots,n_{l-1}-j_{l-1},n_{l}\}\end{array}$
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Â Serra ç­‰äºº ([2018](#bib.bib282)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{n_{l}}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l}~{}\forall l\in{\mathbb{L}}\},\\ d_{l}=\min\{n_{0},n_{1}-j_{1},\ldots,n_{l-1}-j_{l-1},n_{l}\}\end{array}$
    |'
- en: '| Â Serra and Ramalingam ([2020](#bib.bib281)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{\mathcal{I}_{l}(k_{l-1})}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l},\\ d_{l}=\min\{n_{0},k_{1},\ldots,k_{l-1},\mathcal{I}_{l}(k_{l-1})\},k_{0}=n_{0},k_{l}=\mathcal{A}_{l}(k_{l-1})-j_{l-1}~{}\forall
    l\in{\mathbb{L}}\}\end{array}$ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Â Serra å’Œ Ramalingam ([2020](#bib.bib281)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{\mathcal{I}_{l}(k_{l-1})}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l},\\ d_{l}=\min\{n_{0},k_{1},\ldots,k_{l-1},\mathcal{I}_{l}(k_{l-1})\},k_{0}=n_{0},k_{l}=\mathcal{A}_{l}(k_{l-1})-j_{l-1}~{}\forall
    l\in{\mathbb{L}}\}\end{array}$ |'
- en: Expected number
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æœŸæœ›æ•°é‡
- en: The third analytical approach has been the evaluation of the expected number
    of linear regions. In a pair of papers, Hanin and Rolnick studied the number of
    linear regions based on how the network parameters are typically initialized.
    In the first paper (Hanin and Rolnick, [2019a](#bib.bib137)), they show that the
    average number of linear regions along 1-dimensional subspaces of the input grows
    linearly with respect to the number of neurons, irrespective of the network depth.
    In the second paper (Hanin and Rolnick, [2019b](#bib.bib138)), they show that
    the average number of linear regions in higher-dimensional subspaces of the input
    also grows similarly in deep and shallow networks. For $N=\sum_{i=1}^{L}n_{i}$
    as the total number of linear regions, the expected number of linear regions is
    $O(2^{N})$ if $N\leq n_{0}$ and $O\left(\frac{(TN)^{n_{0}}}{n_{0}!}\right)$ otherwise,
    where $T>0$ is a constant based on the network parameters. Moreover, some of their
    experiments suggest that the number of linear regions in shallow networks is slightly
    greater. According to the authors, these bounds reflect the fact that the family
    of functions that can be represented by neural networks in the way that they are
    typically initialized is considerably smaller. They further argue that training
    as currently performed is unlikely to expand the family of functions much further,
    as illustrated by their experiments. Similar results on the expected number of
    linear regions for maxout networks are presented byÂ Tseran and MontÃºfar ([2021](#bib.bib313)),
    and an application of the results above results to data manifolds is explored
    byÂ Tiwari and Konidaris ([2022](#bib.bib307)). Additional results for specific
    architectures of rectifier networks are conjectured byÂ Wang ([2022](#bib.bib328)),
    although without proof.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ç§åˆ†ææ–¹æ³•æ˜¯è¯„ä¼°çº¿æ€§åŒºåŸŸçš„æœŸæœ›æ•°é‡ã€‚åœ¨ä¸€å¯¹è®ºæ–‡ä¸­ï¼ŒHanin å’Œ Rolnick ç ”ç©¶äº†åŸºäºç½‘ç»œå‚æ•°å…¸å‹åˆå§‹åŒ–æ–¹å¼çš„çº¿æ€§åŒºåŸŸæ•°é‡ã€‚åœ¨ç¬¬ä¸€ç¯‡è®ºæ–‡ï¼ˆHanin
    å’Œ Rolnick, [2019a](#bib.bib137)ï¼‰ä¸­ï¼Œä»–ä»¬å±•ç¤ºäº†æ²¿è¾“å…¥çš„ä¸€ç»´å­ç©ºé—´çš„å¹³å‡çº¿æ€§åŒºåŸŸæ•°é‡éšç€ç¥ç»å…ƒæ•°é‡çš„å¢åŠ è€Œçº¿æ€§å¢é•¿ï¼Œä¸ç½‘ç»œæ·±åº¦æ— å…³ã€‚åœ¨ç¬¬äºŒç¯‡è®ºæ–‡ï¼ˆHanin
    å’Œ Rolnick, [2019b](#bib.bib138)ï¼‰ä¸­ï¼Œä»–ä»¬å±•ç¤ºäº†è¾“å…¥çš„é«˜ç»´å­ç©ºé—´ä¸­çš„å¹³å‡çº¿æ€§åŒºåŸŸæ•°é‡åœ¨æ·±å±‚å’Œæµ…å±‚ç½‘ç»œä¸­ä¹Ÿæœ‰ç±»ä¼¼çš„å¢é•¿ã€‚å¯¹äº $N=\sum_{i=1}^{L}n_{i}$
    ä½œä¸ºçº¿æ€§åŒºåŸŸçš„æ€»æ•°é‡ï¼Œå¦‚æœ $N\leq n_{0}$ï¼ŒæœŸæœ›çš„çº¿æ€§åŒºåŸŸæ•°é‡æ˜¯ $O(2^{N})$ï¼Œå¦åˆ™æ˜¯ $O\left(\frac{(TN)^{n_{0}}}{n_{0}!}\right)$ï¼Œå…¶ä¸­
    $T>0$ æ˜¯åŸºäºç½‘ç»œå‚æ•°çš„å¸¸æ•°ã€‚æ­¤å¤–ï¼Œä»–ä»¬çš„ä¸€äº›å®éªŒè¡¨æ˜ï¼Œæµ…å±‚ç½‘ç»œä¸­çš„çº¿æ€§åŒºåŸŸæ•°é‡ç•¥å¤šã€‚æ ¹æ®ä½œè€…çš„è¯´æ³•ï¼Œè¿™äº›ç•Œé™åæ˜ äº†ç¥ç»ç½‘ç»œåœ¨å…¸å‹åˆå§‹åŒ–æ–¹å¼ä¸‹å¯ä»¥è¡¨ç¤ºçš„å‡½æ•°å®¶æ—è¦å°å¾—å¤šã€‚ä»–ä»¬è¿›ä¸€æ­¥è®¤ä¸ºï¼Œç›®å‰çš„è®­ç»ƒæ–¹å¼ä¸å¤ªå¯èƒ½å¤§å¹…æ‰©å±•å‡½æ•°å®¶æ—ï¼Œå¦‚ä»–ä»¬çš„å®éªŒæ‰€ç¤ºã€‚Tseran
    å’Œ MontÃºfar ([2021](#bib.bib313)) æå‡ºäº†å…³äº maxout ç½‘ç»œçš„æœŸæœ›çº¿æ€§åŒºåŸŸæ•°é‡çš„ç±»ä¼¼ç»“æœï¼ŒTiwari å’Œ Konidaris
    ([2022](#bib.bib307)) æ¢è®¨äº†ä¸Šè¿°ç»“æœå¯¹æ•°æ®æµå½¢çš„åº”ç”¨ã€‚Wang ([2022](#bib.bib328)) å¯¹ç‰¹å®šçš„æ•´æµç½‘ç»œæ¶æ„æå‡ºäº†é¢å¤–çš„ç»“æœï¼Œä½†å°šæœªè¯æ˜ã€‚
- en: 3.4.2 Counting linear regions
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 è®¡æ•°çº¿æ€§åŒºåŸŸ
- en: 'Counting the actual number of linear regions of a given network has been a
    more challenging topic to explore. Serra etÂ al. ([2018](#bib.bib282)) have shown
    that the linear regions of a trained network can be enumerated as the solutions
    of an MILP formulation, which has been slightly corrected inÂ Cai etÂ al. ([2023](#bib.bib46))Â¹Â¹1The
    MILP formulation of neural networks is discussed in SectionÂ [4](#S4 "4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey").. However, MILP solutions are generally counted one by one (Danna etÂ al.,
    [2007](#bib.bib72)), with exception of special cases (Serra and Hooker, [2020](#bib.bib280))
    and small subproblems (Serra, [2020](#bib.bib279)), which makes this approach
    impractical for large neural networks. Serra and Ramalingam ([2020](#bib.bib281))
    have shown that approximate model counting methods, which are commonly used to
    count the number of feasible assignments in propositional satisfiability, can
    be easily adapted to solution counting in MILP, which leads to an order-of-magnitude
    speedup in comparison with exact counting. This type of approach is particularly
    suitable for obtaining probabilistic lower bounds, which can complement the analytical
    upper bounds for the maximum number of linear regions. In Craighero etÂ al. ([2020a](#bib.bib64))
    and Craighero etÂ al. ([2020b](#bib.bib65)), a directed acyclic graph is used to
    model the sets of active neurons on each layer and how they connected with those
    in subsequent layers. Yang etÂ al. ([2020](#bib.bib348)) describe a method for
    decomposing the input space of rectifier networks into their linear regions by
    representing each linear region in terms of its face lattice, upon which the splitting
    operations corresponding to the transformations performed by each layer can be
    implemented. As the number of linear regions grow, these splitting operations
    can be processed in parallel. Yang etÂ al. ([2021](#bib.bib349)) extend that method
    to convolutional neural networks. Moreover, Wang ([2022](#bib.bib328)) describes
    an algorithm for enumerating linear regions that counts adjacent linear regions
    with same corresponding affine function as a single linear region.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'è®¡ç®—ç»™å®šç½‘ç»œçš„å®é™…çº¿æ€§åŒºåŸŸçš„æ•°é‡ä¸€ç›´æ˜¯ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„ç ”ç©¶è¯¾é¢˜ã€‚Serraç­‰äººï¼ˆ[2018](#bib.bib282)ï¼‰å·²ç»è¯æ˜ï¼Œè®­ç»ƒç½‘ç»œçš„çº¿æ€§åŒºåŸŸå¯ä»¥ä½œä¸ºMILPï¼ˆMixed-Integer
    Linear Programmingï¼‰å…¬å¼çš„è§£è¿›è¡Œæšä¸¾ï¼Œè¯¥å…¬å¼åœ¨Caiç­‰äººè¿›è¡Œäº†è½»å¾®ä¿®æ­£ï¼ˆ[2023](#bib.bib46)ï¼‰Â¹Â¹1 ç¥ç»ç½‘ç»œçš„MILPå…¬å¼åœ¨ç¬¬[4](#S4
    "4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")èŠ‚è¿›è¡Œäº†è®¨è®ºã€‚ç„¶è€Œï¼ŒMILPè§£å†³æ–¹æ¡ˆé€šå¸¸æ˜¯é€ä¸ªè®¡æ•°çš„ï¼ˆDannaç­‰äººï¼Œ[2007](#bib.bib72)ï¼‰ï¼Œé™¤äº†ç‰¹æ®Šæƒ…å†µï¼ˆSerraå’ŒHookerï¼Œ[2020](#bib.bib280)ï¼‰å’Œå°å‹å­é—®é¢˜ï¼ˆSerraï¼Œ[2020](#bib.bib279)ï¼‰ä¹‹å¤–ï¼Œè¿™ç§æ–¹æ³•å¯¹äºå¤§å‹ç¥ç»ç½‘ç»œæ¥è¯´æ˜¯ä¸å¯è¡Œçš„ã€‚Serraå’ŒRamalingamï¼ˆ[2020](#bib.bib281)ï¼‰å·²ç»è¯æ˜äº†è¿‘ä¼¼æ¨¡å‹è®¡æ•°æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºè®¡ç®—å‘½é¢˜å¯æ»¡è¶³æ€§ä¸­å¯è¡Œèµ‹å€¼çš„æ•°é‡ï¼Œå¯ä»¥è½»æ¾åœ°é€‚åº”MILPä¸­çš„è§£è®¡æ•°ï¼Œè¿™å¯¼è‡´ä¸ç²¾ç¡®è®¡æ•°ç›¸æ¯”çš„æ•°é‡çº§åŠ é€Ÿã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºè·å¾—æ¦‚ç‡ä¸‹ç•Œï¼Œå¯ä»¥ä¸æœ€å¤§çº¿æ€§åŒºåŸŸçš„åˆ†æä¸Šç•Œç›¸è¾…ç›¸æˆã€‚åœ¨Craigheroç­‰äººï¼ˆ[2020a](#bib.bib64)ï¼‰å’ŒCraigheroç­‰äººï¼ˆ[2020b](#bib.bib65)ï¼‰ä¸­ï¼Œä½¿ç”¨æœ‰å‘æ— ç¯å›¾æ¥å»ºæ¨¡æ¯ä¸€å±‚çš„æ´»è·ƒç¥ç»å…ƒé›†åˆä»¥åŠå®ƒä»¬ä¸åç»­å±‚ä¸­ç¥ç»å…ƒçš„è¿æ¥æ–¹å¼ã€‚Yangç­‰äººï¼ˆ[2020](#bib.bib348)ï¼‰æè¿°äº†ä¸€ç§å°†ä¿®æ­£å‡½æ•°ç½‘ç»œï¼ˆrectifier
    networksï¼‰çš„è¾“å…¥ç©ºé—´åˆ†è§£ä¸ºçº¿æ€§åŒºåŸŸçš„æ–¹æ³•ï¼Œé€šè¿‡ç”¨é¢æ ¼æè¿°æ¯ä¸ªçº¿æ€§åŒºåŸŸï¼Œå¯ä»¥å®ç°ä¸æ¯å±‚æ‰§è¡Œçš„è½¬æ¢ç›¸å¯¹åº”çš„åˆ†å‰²æ“ä½œã€‚éšç€çº¿æ€§åŒºåŸŸçš„æ•°é‡å¢åŠ ï¼Œè¿™äº›åˆ†å‰²æ“ä½œå¯ä»¥å¹¶è¡Œå¤„ç†ã€‚Yangç­‰äººï¼ˆ[2021](#bib.bib349)ï¼‰å°†è¯¥æ–¹æ³•æ¨å¹¿åˆ°å·ç§¯ç¥ç»ç½‘ç»œã€‚æ­¤å¤–ï¼ŒWangï¼ˆ[2022](#bib.bib328)ï¼‰æè¿°äº†ä¸€ç§æšä¸¾çº¿æ€§åŒºåŸŸçš„ç®—æ³•ï¼Œè¯¥ç®—æ³•å°†å…·æœ‰ç›¸åŒå¯¹åº”ä»¿å°„å‡½æ•°çš„ç›¸é‚»çº¿æ€§åŒºåŸŸè®¡æ•°ä¸ºå•ä¸ªçº¿æ€§åŒºåŸŸã€‚'
- en: Another approach is to enumerate the linear regions in subspaces, which limits
    their number and reduces the complexity of the task. This idea was first explored
    by Novak etÂ al. ([2018](#bib.bib235)) for measuring the complexity of a neural
    network in terms of the number of transitions along a single line. Hanin and Rolnick
    ([2019a](#bib.bib137), [b](#bib.bib138)) also use this method with a bounded line
    segment or rectangle as a single set representing the input and then sequentially
    partitioning it. If this first set is intersected by the activation hyperplane
    of a neuron in the first layer, then we replace this set by two sets corresponding
    to the parts of the input space in which that neuron is active and not. Once those
    sets are further subdivided by all activation hyperplanes associated with the
    neurons in the first layer, the process can be continued with the neurons in the
    following layers. This method is used to count the number of linear regions along
    subspaces of the input with dimension 1 in Hanin and Rolnick ([2019a](#bib.bib137))
    and dimension 2 in Hanin and Rolnick ([2019b](#bib.bib138)). A generalized version
    for counting the number of linear regions in affine subspaces spanned by a set
    of samples using an MILP formulation is presented in Cai etÂ al. ([2023](#bib.bib46)).
    An approximate approach for counting the number of linear regions along a line
    by computing the closest activation hyperplane in each layer is presented in Gamba
    etÂ al. ([2022](#bib.bib114)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æ–¹æ³•æ˜¯æšä¸¾å­ç©ºé—´ä¸­çš„çº¿æ€§åŒºåŸŸï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„æ•°é‡å¹¶å‡å°‘äº†ä»»åŠ¡çš„å¤æ‚æ€§ã€‚Novak ç­‰äºº ([2018](#bib.bib235)) é¦–æ¬¡æ¢ç´¢äº†è¿™ä¸ªæƒ³æ³•ï¼Œç”¨äºè¡¡é‡ç¥ç»ç½‘ç»œåœ¨å•ä¸€çº¿æ®µä¸Šçš„è¿‡æ¸¡æ•°é‡çš„å¤æ‚æ€§ã€‚Hanin
    å’Œ Rolnick ([2019a](#bib.bib137), [b](#bib.bib138)) ä¹Ÿä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œå°†ä¸€ä¸ªæœ‰ç•Œçº¿æ®µæˆ–çŸ©å½¢ä½œä¸ºè¡¨ç¤ºè¾“å…¥çš„å•ä¸€é›†åˆï¼Œç„¶åä¾æ¬¡å¯¹å…¶è¿›è¡Œåˆ’åˆ†ã€‚å¦‚æœç¬¬ä¸€ä¸ªé›†åˆè¢«ç¬¬ä¸€å±‚ç¥ç»å…ƒçš„æ¿€æ´»è¶…å¹³é¢ç›¸äº¤ï¼Œåˆ™ç”¨ä¸¤ä¸ªé›†åˆæ›¿ä»£è¿™ä¸ªé›†åˆï¼Œè¿™ä¸¤ä¸ªé›†åˆåˆ†åˆ«å¯¹åº”äºè¯¥ç¥ç»å…ƒæ¿€æ´»å’Œæœªæ¿€æ´»çš„è¾“å…¥ç©ºé—´éƒ¨åˆ†ã€‚ä¸€æ—¦è¿™äº›é›†åˆè¢«ä¸ç¬¬ä¸€å±‚ç¥ç»å…ƒç›¸å…³çš„æ‰€æœ‰æ¿€æ´»è¶…å¹³é¢è¿›ä¸€æ­¥ç»†åˆ†ï¼Œè¯¥è¿‡ç¨‹å¯ä»¥ç»§ç»­è¿›è¡Œåˆ°åç»­å±‚çš„ç¥ç»å…ƒã€‚è¿™ç§æ–¹æ³•ç”¨äºè®¡ç®—
    Hanin å’Œ Rolnick ([2019a](#bib.bib137)) ä¸­ç»´åº¦ä¸º 1 çš„è¾“å…¥å­ç©ºé—´å’Œ Hanin å’Œ Rolnick ([2019b](#bib.bib138))
    ä¸­ç»´åº¦ä¸º 2 çš„çº¿æ€§åŒºåŸŸæ•°é‡ã€‚Cai ç­‰äºº ([2023](#bib.bib46)) æå‡ºäº†ä¸€ä¸ªç”¨äºè®¡æ•°ç”±æ ·æœ¬é›†ç”Ÿæˆçš„ä»¿å°„å­ç©ºé—´ä¸­çº¿æ€§åŒºåŸŸæ•°é‡çš„ MILP
    å…¬å¼çš„å¹¿ä¹‰ç‰ˆæœ¬ã€‚Gamba ç­‰äºº ([2022](#bib.bib114)) æå‡ºäº†ä¸€ä¸ªé€šè¿‡è®¡ç®—æ¯å±‚ä¸­æœ€æ¥è¿‘çš„æ¿€æ´»è¶…å¹³é¢æ¥è®¡æ•°æ²¿ä¸€æ¡çº¿çš„çº¿æ€§åŒºåŸŸæ•°é‡çš„è¿‘ä¼¼æ–¹æ³•ã€‚
- en: Other approaches have obtained lower bounds on the number of linear regions
    of a trained network by limiting the enumeration or considering exclusively the
    inputs from the dataset. In Xiong etÂ al. ([2020](#bib.bib345)), the number of
    linear regions is estimated by sampling points from the input space and enumerating
    all activation patterns identified through this process. In Cohan etÂ al. ([2022](#bib.bib61)),
    the counting is restricted to the linear regions found between consecutive states
    of a neural network modeling a reinforcement learning policy.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–æ–¹æ³•é€šè¿‡é™åˆ¶æšä¸¾æˆ–ä¸“é—¨è€ƒè™‘æ•°æ®é›†ä¸­çš„è¾“å…¥æ¥è·å¾—è®­ç»ƒç½‘ç»œçº¿æ€§åŒºåŸŸçš„ä¸‹ç•Œã€‚åœ¨ Xiong ç­‰äºº ([2020](#bib.bib345)) ä¸­ï¼Œçº¿æ€§åŒºåŸŸçš„æ•°é‡é€šè¿‡ä»è¾“å…¥ç©ºé—´ä¸­é‡‡æ ·ç‚¹å¹¶æšä¸¾é€šè¿‡è¿™ä¸€è¿‡ç¨‹è¯†åˆ«çš„æ‰€æœ‰æ¿€æ´»æ¨¡å¼æ¥ä¼°è®¡ã€‚åœ¨
    Cohan ç­‰äºº ([2022](#bib.bib61)) ä¸­ï¼Œè®¡æ•°é™åˆ¶åœ¨ç¥ç»ç½‘ç»œå»ºæ¨¡å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„è¿ç»­çŠ¶æ€ä¹‹é—´å‘ç°çš„çº¿æ€§åŒºåŸŸã€‚
- en: 3.5 Applications and insights
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 åº”ç”¨ä¸è§è§£
- en: Thinking about neural networks in terms of linear regions led to a variety of
    applications. In turn, that inspired further studies on the structure and properties
    of linear regions under different settings. We organize the literature about applications
    and insights around some central themes in the subsections below.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ä»çº¿æ€§åŒºåŸŸçš„è§’åº¦æ€è€ƒç¥ç»ç½‘ç»œå¯¼è‡´äº†å„ç§åº”ç”¨ã€‚è¿™åè¿‡æ¥åˆæ¿€å‘äº†å¯¹ä¸åŒè®¾ç½®ä¸‹çº¿æ€§åŒºåŸŸç»“æ„å’Œå±æ€§çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚æˆ‘ä»¬åœ¨ä¸‹é¢çš„å­ç« èŠ‚ä¸­å›´ç»•ä¸€äº›ä¸­å¿ƒä¸»é¢˜ç»„ç»‡äº†å…³äºåº”ç”¨å’Œè§è§£çš„æ–‡çŒ®ã€‚
- en: 3.5.1 The number of linear regions
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 çº¿æ€§åŒºåŸŸçš„æ•°é‡
- en: From our discussion, the number of linear regions emerges as a potential proxy
    for the complexity of neural networks, which has been studied by some authors
    and exploited empirically by others. Novak etÂ al. ([2018](#bib.bib235)) observed
    that the number of transitions between linear regions in 1-dimensional subspaces
    correlates with generalization. Hu etÂ al. ([2020a](#bib.bib154)) used bounds on
    the number of linear regions as proxy to model the capacity of a neural network
    used for learning through distillation, in which a smaller network is trained
    based on the outputs of another network. Chen etÂ al. ([2021a](#bib.bib54)) and
    Chen etÂ al. ([2021b](#bib.bib55)) present one of the first approaches to training-free
    neural architectural search through the analysis of network properties. One of
    the two metrics that they have shown to be effective for that purpose is the number
    of linear regions associated with a sample of inputs from the training set on
    randomly initialized networks. Biau etÂ al. ([2021](#bib.bib30)) observed that
    obtaining a discriminator network for Wasserstein GANsÂ (Arjovsky etÂ al., [2017](#bib.bib7))
    that correctly approximates the Wasserstein distance entails that such a discriminator
    network has a growing number of linear regions as the complexity of the data distribution
    increases. Park etÂ al. ([2021b](#bib.bib242)) maximized the number of linear regions
    in unsupervised learning in order to produce more expressive encodings for downstream
    tasks using simpler classifiers. In neural networks modeling reinforcement learning
    policies, Cohan etÂ al. ([2022](#bib.bib61)) observed that the number of transitions
    between linear regions in inputs corresponding to consecutive states increases
    by 50% with training while the number of repeated linear regions decreases. Cai
    etÂ al. ([2023](#bib.bib46)) proposed a method for pruning different proportions
    of parameters from each layer by maximizes the bound on the number of linear regions,
    which lead to better accuracy than uniform pruning across layers. On a related
    note, Liang and Xu ([2021](#bib.bib192)) proposed a new variant of the ReLU activation
    function for dividing the input space into a greater number of linear regions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æˆ‘ä»¬çš„è®¨è®ºä¸­ï¼Œçº¿æ€§åŒºåŸŸçš„æ•°é‡æµ®ç°ä¸ºç¥ç»ç½‘ç»œå¤æ‚æ€§çš„æ½œåœ¨ä»£ç†ï¼Œè¿™ä¸€é¢†åŸŸå·²è¢«ä¸€äº›ä½œè€…ç ”ç©¶ï¼Œå¹¶è¢«å…¶ä»–äººç»éªŒæ€§åœ°åˆ©ç”¨ã€‚Novak ç­‰ ([2018](#bib.bib235))
    è§‚å¯Ÿåˆ°ä¸€ç»´å­ç©ºé—´ä¸­çº¿æ€§åŒºåŸŸä¹‹é—´çš„è½¬æ¢æ¬¡æ•°ä¸æ³›åŒ–èƒ½åŠ›ç›¸å…³ã€‚Hu ç­‰ ([2020a](#bib.bib154)) ä½¿ç”¨çº¿æ€§åŒºåŸŸæ•°é‡çš„ç•Œé™ä½œä¸ºä»£ç†ï¼Œä»¥å»ºæ¨¡ç”¨äºè’¸é¦å­¦ä¹ çš„ç¥ç»ç½‘ç»œçš„å®¹é‡ï¼Œå…¶ä¸­ä¸€ä¸ªè¾ƒå°çš„ç½‘ç»œæ˜¯åŸºäºå¦ä¸€ä¸ªç½‘ç»œçš„è¾“å‡ºè¿›è¡Œè®­ç»ƒçš„ã€‚Chen
    ç­‰ ([2021a](#bib.bib54)) å’Œ Chen ç­‰ ([2021b](#bib.bib55)) æå‡ºäº†é€šè¿‡ç½‘ç»œå±æ€§åˆ†æè¿›è¡Œæ— è®­ç»ƒç¥ç»ç½‘ç»œç»“æ„æœç´¢çš„æœ€åˆæ–¹æ³•ä¹‹ä¸€ã€‚ä»–ä»¬å±•ç¤ºçš„ä¸¤ç§æœ‰æ•ˆåº¦é‡ä¹‹ä¸€æ˜¯ä¸è®­ç»ƒé›†è¾“å…¥æ ·æœ¬ç›¸å…³çš„çº¿æ€§åŒºåŸŸæ•°é‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ç½‘ç»œã€‚Biau
    ç­‰ ([2021](#bib.bib30)) è§‚å¯Ÿåˆ°ï¼Œä¸º Wasserstein GANs (Arjovsky ç­‰, [2017](#bib.bib7))
    è·å¾—ä¸€ä¸ªæ­£ç¡®è¿‘ä¼¼ Wasserstein è·ç¦»çš„åˆ¤åˆ«ç½‘ç»œæ„å‘³ç€è¯¥ç½‘ç»œåœ¨æ•°æ®åˆ†å¸ƒå¤æ‚åº¦å¢åŠ æ—¶å…·æœ‰ä¸æ–­å¢é•¿çš„çº¿æ€§åŒºåŸŸæ•°é‡ã€‚Park ç­‰ ([2021b](#bib.bib242))
    åœ¨æ— ç›‘ç£å­¦ä¹ ä¸­æœ€å¤§åŒ–çº¿æ€§åŒºåŸŸçš„æ•°é‡ï¼Œä»¥ä¾¿ä¸ºä¸‹æ¸¸ä»»åŠ¡ç”Ÿæˆæ›´å…·è¡¨ç°åŠ›çš„ç¼–ç ï¼Œä½¿ç”¨æ›´ç®€å•çš„åˆ†ç±»å™¨ã€‚åœ¨ç¥ç»ç½‘ç»œå»ºæ¨¡å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¸­ï¼ŒCohan ç­‰ ([2022](#bib.bib61))
    è§‚å¯Ÿåˆ°è¾“å…¥å¯¹åº”äºè¿ç»­çŠ¶æ€ä¹‹é—´çš„çº¿æ€§åŒºåŸŸè½¬æ¢æ•°é‡éšç€è®­ç»ƒå¢åŠ äº† 50%ï¼Œè€Œé‡å¤çš„çº¿æ€§åŒºåŸŸæ•°é‡å‡å°‘ã€‚Cai ç­‰ ([2023](#bib.bib46)) æå‡ºäº†ä¸€ç§é€šè¿‡æœ€å¤§åŒ–çº¿æ€§åŒºåŸŸæ•°é‡çš„ç•Œé™æ¥ä¿®å‰ªæ¯ä¸€å±‚çš„ä¸åŒæ¯”ä¾‹å‚æ•°çš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•æ¯”è·¨å±‚å‡åŒ€ä¿®å‰ªæ›´èƒ½æé«˜å‡†ç¡®æ€§ã€‚åœ¨ç›¸å…³ç ”ç©¶ä¸­ï¼ŒLiang
    å’Œ Xu ([2021](#bib.bib192)) æå‡ºäº†ä¸€ä¸ªæ–°çš„ ReLU æ¿€æ´»å‡½æ•°å˜ä½“ï¼Œç”¨äºå°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸ºæ›´å¤šçš„çº¿æ€§åŒºåŸŸã€‚
- en: The number of linear regions also inspired further theoretical work. Amrami
    and Goldberg ([2021](#bib.bib3)) presented an argument for the benefit of depth
    in neural networks based on the number of linear regions for correctly separating
    samples associated with different classes. Liu and Liang ([2021](#bib.bib196))
    studied upper and lower bounds on the optimal approximation error of a convex
    univariate function based on the number of linear regions of a rectifier network.
    Henriksen etÂ al. ([2022](#bib.bib146)) used the maximum number of linear regions
    as a metric for capacity that may limit repairing incorrect classifications in
    a neural network.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§åŒºåŸŸçš„æ•°é‡ä¹Ÿæ¿€å‘äº†è¿›ä¸€æ­¥çš„ç†è®ºç ”ç©¶ã€‚Amrami å’Œ Goldberg ([2021](#bib.bib3)) åŸºäºæ­£ç¡®åˆ†ç¦»ä¸åŒç±»åˆ«æ ·æœ¬çš„çº¿æ€§åŒºåŸŸæ•°é‡ï¼Œæå‡ºäº†ç¥ç»ç½‘ç»œæ·±åº¦çš„å¥½å¤„çš„è®ºç‚¹ã€‚Liu
    å’Œ Liang ([2021](#bib.bib196)) ç ”ç©¶äº†åŸºäºæ•´æµå™¨ç½‘ç»œçš„çº¿æ€§åŒºåŸŸæ•°é‡çš„å‡¸ä¸€å…ƒå‡½æ•°çš„æœ€ä½³è¿‘ä¼¼è¯¯å·®çš„ä¸Šä¸‹ç•Œã€‚Henriksen ç­‰ ([2022](#bib.bib146))
    ä½¿ç”¨çº¿æ€§åŒºåŸŸçš„æœ€å¤§æ•°é‡ä½œä¸ºå®¹é‡çš„åº¦é‡ï¼Œè¿™å¯èƒ½é™åˆ¶äº†ç¥ç»ç½‘ç»œä¸­ä¿®å¤é”™è¯¯åˆ†ç±»çš„èƒ½åŠ›ã€‚
- en: 3.5.2 The shape of linear regions
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 çº¿æ€§åŒºåŸŸçš„å½¢çŠ¶
- en: Some studies aimed at understanding what affects the shape of linear regions
    in practice, including how to train neural networks in such a way to induce certain
    shapes in the linear regions. Zhang and Wu ([2020](#bib.bib359)) observed that
    multiple training techniques may lead to similar accuracy, but very different
    shape for the linear regions. For example, batch normalizationÂ (Ioffe and Szegedy,
    [2015](#bib.bib162)) and dropoutÂ (Srivastava etÂ al., [2014](#bib.bib294)) lead
    to more linear regions. While batch normalization breaks the space in regions
    with uniform size, more orthogonal norms, and more gradient variability across
    adjacent regions; dropout produces more linear regions around decision boundaries,
    norms are more parallel, and data points less likely to be in the region containing
    the decision boundary. Croce etÂ al. ([2019](#bib.bib67)) and Lee etÂ al. ([2019](#bib.bib188))
    applied regularization to the loss function to push the boundary of each linear
    region away from points in the training set that it contains, as long as those
    points are correctly classified. They show that this form of regularization improves
    the robustness of the neural network while making the linear regions larger. In
    fact, Zhu etÂ al. ([2020](#bib.bib362)) observed that the boundaries of the linear
    regions move away from the training data; and He etÂ al. ([2021](#bib.bib141))
    conjectured that the linear regions near training samples becomes smaller through
    training, or that conversely the activation patterns are denser around the training
    samples. Gamba etÂ al. ([2020](#bib.bib113)) presented an empirical study on the
    angles between activation hyperplanes defined by convolutional layers, and observed
    that their cosines tend to be similar and more negative with depth after training.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ç ”ç©¶æ—¨åœ¨ç†è§£åœ¨å®é™…æ“ä½œä¸­å½±å“çº¿æ€§åŒºåŸŸå½¢çŠ¶çš„å› ç´ ï¼ŒåŒ…æ‹¬å¦‚ä½•è®­ç»ƒç¥
- en: The geometry of linear regions also led to other theoretical and algorithmic
    advances. Theoretically, Phuong and Lampert ([2020](#bib.bib247)) proved that
    architectures with nonincreasing layer widths have unique parameters â€”upon permutation
    and scalingâ€” for representing certain functions. In other words, some pairs of
    neural networks are only equivalent if their parameters only differ by permutation
    and multiplication. Grigsby etÂ al. ([2023](#bib.bib132)) showed that equivalences
    other than by permutation are less likely to occur with greater input size and
    width, but more likely with greater depth. Algorithmically, Rolnick and Kording
    ([2020](#bib.bib260)) proposed a procedure to reconstruct a neural network by
    evaluating several inputs in order to determine regions of the input space for
    which the output of the neural network can be defined by an affine function â€”and
    thus consist of a single linear region. Depending on how the shape changes between
    adjacent linear regions, the boundaries of the linear regions are replicated with
    neurons in the first hidden layer or in subsequent layers of the reconstructed
    neural network. Masden ([2022](#bib.bib213)) provided theoretical results and
    an algorithm for characterizing the face lattice of the polyhedron associated
    with each linear region.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§åŒºåŸŸçš„å‡ ä½•å½¢æ€ä¹Ÿä¿ƒè¿›äº†å…¶ä»–ç†è®ºå’Œç®—æ³•ä¸Šçš„è¿›å±•ã€‚åœ¨ç†è®ºä¸Šï¼ŒPhuong å’Œ Lampert ([2020](#bib.bib247)) è¯æ˜äº†å…·æœ‰éå¢å®½å±‚å®½åº¦çš„æ¶æ„å…·æœ‰å”¯ä¸€çš„å‚æ•°ï¼Œå³å¯¹äºè¡¨ç¤ºæŸäº›å‡½æ•°çš„æ’åˆ—å’Œç¼©æ”¾ã€‚æ¢å¥è¯è¯´ï¼Œä¸€äº›ç¥ç»ç½‘ç»œåªæœ‰åœ¨å®ƒä»¬çš„å‚æ•°åªæœ‰æ’åˆ—å’Œä¹˜æ³•ä¹‹é—´çš„å·®å¼‚æ—¶æ‰æ˜¯ç­‰ä»·çš„ã€‚Grigsby
    ç­‰ ([2023](#bib.bib132)) è¡¨æ˜ï¼Œé™¤äº†é€šè¿‡æ’åˆ—ä¹‹å¤–çš„ç­‰ä»·æ€§åœ¨æ›´å¤§çš„è¾“å…¥å¤§å°å’Œå®½åº¦æ—¶æ›´ä¸å¤ªå¯èƒ½å‘ç”Ÿï¼Œä½†åœ¨æ›´å¤§çš„æ·±åº¦æ—¶æ›´æœ‰å¯èƒ½å‘ç”Ÿã€‚åœ¨ç®—æ³•ä¸Šï¼ŒRolnick
    å’Œ Kording ([2020](#bib.bib260)) æå‡ºäº†ä¸€ç§é€šè¿‡è¯„ä¼°å‡ ä¸ªè¾“å…¥æ¥é‡æ„ç¥ç»ç½‘ç»œçš„ç¨‹åºï¼Œä»¥ç¡®å®šè¾“å…¥ç©ºé—´çš„åŒºåŸŸï¼Œåœ¨è¿™äº›åŒºåŸŸå†…ç¥ç»ç½‘ç»œçš„è¾“å‡ºå¯ä»¥è¢«å®šä¹‰ä¸ºä¸€ä¸ªä»¿å°„å‡½æ•°ï¼Œå› æ­¤ç”±å•ä¸€çº¿æ€§åŒºåŸŸç»„æˆã€‚æ ¹æ®åœ¨ç›¸é‚»çº¿æ€§åŒºåŸŸä¹‹é—´çš„å½¢çŠ¶å˜åŒ–æ–¹å¼ï¼Œçº¿æ€§åŒºåŸŸçš„è¾¹ç•Œè¢«å¤åˆ¶åˆ°é‡æ„ç¥ç»ç½‘ç»œçš„ç¬¬ä¸€ä¸ªéšè—å±‚æˆ–åç»­å±‚çš„ç¥ç»å…ƒä¸­ã€‚Masden
    ([2022](#bib.bib213)) æä¾›äº†å…³äºè¡¨å¾æ¯ä¸ªçº¿æ€§åŒºåŸŸç›¸å…³çš„å¤šé¢ä½“çš„é¢æ ¼çš„ç†è®ºç»“æœå’Œç®—æ³•ã€‚
- en: 3.5.3 Activation patterns and the discrimination of inputs
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 æ¿€æ´»æ¨¡å¼å’Œè¾“å…¥çš„åŒºåˆ«
- en: Another common theme is understanding how inputs from the training and test
    sets are distributed among the linear regions, and what can be inferred the encoding
    of the activation patterns associated with the linear regions. Gopinath etÂ al.
    ([2019](#bib.bib127)) noted that many properties of neural networks, including
    the classes of different inputs, are associated with activation patterns â€”and
    thus with their linear regions. Several worksÂ (He etÂ al., [2021](#bib.bib141),
    Sattelberg etÂ al., [2020](#bib.bib271), Trimmel etÂ al., [2021](#bib.bib310)) observed
    that each training sample is typically located in a different linear region when
    the neural network is sufficiently expressive; whereas He etÂ al. ([2021](#bib.bib141))
    noted that simple machine learning algorithms can be applied using the activation
    patterns as features, and Sattelberg etÂ al. ([2020](#bib.bib271)) noted that there
    is some similarity between activation patterns of different neural networks under
    affine mapping, meaning that the training of these neural networks lead to similar
    models. Chaudhry etÂ al. ([2020](#bib.bib49)) exploited the idea of continual learning
    with different tasks being encoded in disjoint subspaces, which thus corresponds
    to a disjoint set of activation sets on each layer being associated with classifications
    for each of those tasks. Based on their approach for enumerating linear regions,
    Craighero etÂ al. ([2020a](#bib.bib64)) and Craighero etÂ al. ([2020b](#bib.bib65))
    have found that inputs from larger linear regions are often correctly classified
    by the neural network, that inputs from smaller linear regions are often incorrectly
    classified, and that the number of distinct activations sets reduces along the
    layers of the neural network. Gamba etÂ al. ([2022](#bib.bib114)) also discussed
    the issue of some linear regions being smaller and thus less likely to occur in
    practice. Moreover, they propose a measurement for the similarity of the affine
    functions associated with linear regions along a line and observed that the linear
    regions tend to be less similar to one another when the network is trained with
    incorrectly classified labels.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¸¸è§çš„ä¸»é¢˜æ˜¯ç†è§£è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„è¾“å…¥å¦‚ä½•åœ¨çº¿æ€§åŒºåŸŸä¸­åˆ†å¸ƒï¼Œä»¥åŠä»è¿™äº›çº¿æ€§åŒºåŸŸçš„æ¿€æ´»æ¨¡å¼ç¼–ç ä¸­å¯ä»¥æ¨æ–­å‡ºä»€ä¹ˆã€‚Gopinath ç­‰äººï¼ˆ[2019](#bib.bib127)ï¼‰æŒ‡å‡ºï¼Œç¥ç»ç½‘ç»œçš„è®¸å¤šå±æ€§ï¼ŒåŒ…æ‹¬ä¸åŒè¾“å…¥çš„ç±»åˆ«ï¼Œéƒ½ä¸æ¿€æ´»æ¨¡å¼æœ‰å…³â€”â€”å› æ­¤ä¹Ÿä¸å®ƒä»¬çš„çº¿æ€§åŒºåŸŸæœ‰å…³ã€‚ä¸€äº›ç ”ç©¶ï¼ˆHe
    ç­‰äººï¼Œ[2021](#bib.bib141)ï¼ŒSattelberg ç­‰äººï¼Œ[2020](#bib.bib271)ï¼ŒTrimmel ç­‰äººï¼Œ[2021](#bib.bib310)ï¼‰è§‚å¯Ÿåˆ°ï¼Œå½“ç¥ç»ç½‘ç»œå…·æœ‰è¶³å¤Ÿçš„è¡¨è¾¾èƒ½åŠ›æ—¶ï¼Œæ¯ä¸ªè®­ç»ƒæ ·æœ¬é€šå¸¸ä½äºä¸åŒçš„çº¿æ€§åŒºåŸŸï¼›è€Œ
    He ç­‰äººï¼ˆ[2021](#bib.bib141)ï¼‰æŒ‡å‡ºï¼Œå¯ä»¥ä½¿ç”¨æ¿€æ´»æ¨¡å¼ä½œä¸ºç‰¹å¾æ¥åº”ç”¨ç®€å•çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼ŒSattelberg ç­‰äººï¼ˆ[2020](#bib.bib271)ï¼‰æŒ‡å‡ºï¼Œä¸åŒç¥ç»ç½‘ç»œçš„æ¿€æ´»æ¨¡å¼åœ¨ä»¿å°„æ˜ å°„ä¸‹å­˜åœ¨ä¸€äº›ç›¸ä¼¼æ€§ï¼Œè¿™æ„å‘³ç€è¿™äº›ç¥ç»ç½‘ç»œçš„è®­ç»ƒä¼šå¯¼è‡´ç±»ä¼¼çš„æ¨¡å‹ã€‚Chaudhry
    ç­‰äººï¼ˆ[2020](#bib.bib49)ï¼‰åˆ©ç”¨äº†æŒç»­å­¦ä¹ çš„æ€æƒ³ï¼Œå°†ä¸åŒä»»åŠ¡ç¼–ç åˆ°ä¸ç›¸äº¤çš„å­ç©ºé—´ä¸­ï¼Œè¿™ä¸æ¯å±‚ä¸Šçš„æ¯ä¸ªä»»åŠ¡çš„åˆ†ç±»ç›¸å…³è”çš„æ¿€æ´»é›†åˆå¯¹åº”ã€‚æ ¹æ®ä»–ä»¬å¯¹çº¿æ€§åŒºåŸŸè¿›è¡Œæšä¸¾çš„æ–¹æ³•ï¼ŒCraighero
    ç­‰äººï¼ˆ[2020a](#bib.bib64)ï¼‰å’Œ Craighero ç­‰äººï¼ˆ[2020b](#bib.bib65)ï¼‰å‘ç°ï¼Œæ¥è‡ªè¾ƒå¤§çº¿æ€§åŒºåŸŸçš„è¾“å…¥é€šå¸¸è¢«ç¥ç»ç½‘ç»œæ­£ç¡®åˆ†ç±»ï¼Œæ¥è‡ªè¾ƒå°çº¿æ€§åŒºåŸŸçš„è¾“å…¥é€šå¸¸è¢«é”™è¯¯åˆ†ç±»ï¼Œè€Œä¸”ç¥ç»ç½‘ç»œå±‚ä¸Šçš„ä¸åŒæ¿€æ´»é›†åˆæ•°é‡ä¼šå‡å°‘ã€‚Gamba
    ç­‰äººï¼ˆ[2022](#bib.bib114)ï¼‰è¿˜è®¨è®ºäº†ä¸€äº›çº¿æ€§åŒºåŸŸè¾ƒå°ï¼Œå› æ­¤åœ¨å®è·µä¸­å‡ºç°çš„å¯èƒ½æ€§è¾ƒä½çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œä»–ä»¬æå‡ºäº†ä¸€ç§æµ‹é‡çº¿æ€§åŒºåŸŸæ²¿ç›´çº¿ç›¸å…³çš„ä»¿å°„å‡½æ•°ç›¸ä¼¼æ€§çš„æ–¹æ³•ï¼Œå¹¶è§‚å¯Ÿåˆ°å½“ç½‘ç»œåœ¨é”™è¯¯åˆ†ç±»æ ‡ç­¾ä¸‹è®­ç»ƒæ—¶ï¼Œçº¿æ€§åŒºåŸŸä¹‹é—´çš„ç›¸ä¼¼æ€§å¾€å¾€è¾ƒä½ã€‚
- en: 3.5.4 Function approximation
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4 å‡½æ•°é€¼è¿‘
- en: Because of the linear behavior of the output within each linear region, we can
    approximate the output of the neural network based on the output of its linear
    regions. Chu etÂ al. ([2018](#bib.bib59)) and Sudjianto etÂ al. ([2020](#bib.bib297))
    produced linear models based on this local behavior; whereas Glass etÂ al. ([2021](#bib.bib118))
    observed that we can interpret neural networks as equivalent to local linear model
    treesÂ (Nelles etÂ al., [2000](#bib.bib230)), in which a distinct linear model is
    used at each leaf of a decision tree, and provided a method to produce such models
    from neural networks. Trimmel etÂ al. ([2021](#bib.bib310)) described how to extract
    the linear regions associated with the inputs from the training set as means to
    approximate the output of the inputs from the test set. Robinson etÂ al. ([2019](#bib.bib259))
    presented another approach for explicitly representing the function modeled by
    a neural network through the enumeration of its linear regions. On a related note,
    Chaudhry etÂ al. ([2020](#bib.bib49)) used the assumption of training samples remaining
    within the same linear region during gradient descent to simplify the analysis
    of backpropagation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¯ä¸ªçº¿æ€§åŒºåŸŸå†…è¾“å‡ºçš„çº¿æ€§è¡Œä¸ºï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºå…¶çº¿æ€§åŒºåŸŸçš„è¾“å‡ºæ¥è¿‘ä¼¼ç¥ç»ç½‘ç»œçš„è¾“å‡ºã€‚Chu ç­‰äººï¼ˆ[2018](#bib.bib59)ï¼‰å’Œ Sudjianto
    ç­‰äººï¼ˆ[2020](#bib.bib297)ï¼‰åŸºäºè¿™ç§å±€éƒ¨è¡Œä¸ºç”Ÿæˆäº†çº¿æ€§æ¨¡å‹ï¼›è€Œ Glass ç­‰äººï¼ˆ[2021](#bib.bib118)ï¼‰è§‚å¯Ÿåˆ°æˆ‘ä»¬å¯ä»¥å°†ç¥ç»ç½‘ç»œè§£é‡Šä¸ºç­‰æ•ˆäºå±€éƒ¨çº¿æ€§æ¨¡å‹æ ‘ï¼ˆNelles
    ç­‰äººï¼Œ[2000](#bib.bib230)ï¼‰ï¼Œå…¶ä¸­åœ¨å†³ç­–æ ‘çš„æ¯ä¸ªå¶å­èŠ‚ç‚¹ä¸Šä½¿ç”¨ä¸€ä¸ªç‹¬ç‰¹çš„çº¿æ€§æ¨¡å‹ï¼Œå¹¶æä¾›äº†ä»ç¥ç»ç½‘ç»œç”Ÿæˆè¿™ç§æ¨¡å‹çš„æ–¹æ³•ã€‚Trimmel ç­‰äººï¼ˆ[2021](#bib.bib310)ï¼‰æè¿°äº†å¦‚ä½•ä»è®­ç»ƒé›†ä¸­æå–ä¸è¾“å…¥ç›¸å…³çš„çº¿æ€§åŒºåŸŸï¼Œä»¥è¿‘ä¼¼æµ‹è¯•é›†ä¸­è¾“å…¥çš„è¾“å‡ºã€‚Robinson
    ç­‰äººï¼ˆ[2019](#bib.bib259)ï¼‰æå‡ºäº†å¦ä¸€ç§é€šè¿‡æšä¸¾çº¿æ€§åŒºåŸŸæ¥æ˜¾å¼è¡¨ç¤ºç¥ç»ç½‘ç»œå»ºæ¨¡å‡½æ•°çš„æ–¹æ³•ã€‚ç›¸å…³åœ°ï¼ŒChaudhry ç­‰äººï¼ˆ[2020](#bib.bib49)ï¼‰åˆ©ç”¨è®­ç»ƒæ ·æœ¬åœ¨æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ä¸­ä¿æŒåœ¨åŒä¸€çº¿æ€§åŒºåŸŸçš„å‡è®¾æ¥ç®€åŒ–åå‘ä¼ æ’­åˆ†æã€‚
- en: This topic also relates to the broad literature on neural networks as universal
    function approximators, to which the concept of linear regions helps articulating
    ideal conditions. As observed by Mhaskar and Poggio ([2020](#bib.bib218)), the
    optimal number of linear regions in a neural network â€”or, correspondingly, of
    pieces of the piecewise linear function modeled by itâ€” depends on the function
    being approximated. In addition, linear regions were also used explicitly to build
    function approximations. Kumar etÂ al. ([2019](#bib.bib181)) have shown that rectifier
    networks can we approximated to arbitrary precision with two hidden layers, the
    largest of which having a neuron corresponding to each different activation pattern
    of the original network; an exact counterpart of this result was later presented
    byÂ Villani and Schoots ([2023](#bib.bib319)). Fan etÂ al. ([2020](#bib.bib99))
    described the transformation between sufficiently wide and deep networks while
    arguing that the fundamental measure of complexity should be counting simplices
    within linear regions. In subsequent work, Fan etÂ al. ([2023](#bib.bib100)) empirically
    observed that linear regions tend to have a small number of higher dimensional
    faces, or facets.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¯é¢˜è¿˜æ¶‰åŠåˆ°ç¥ç»ç½‘ç»œä½œä¸ºé€šç”¨å‡½æ•°é€¼è¿‘å™¨çš„å¹¿æ³›æ–‡çŒ®ï¼Œå…¶ä¸­çº¿æ€§åŒºåŸŸçš„æ¦‚å¿µæœ‰åŠ©äºé˜æ˜ç†æƒ³æ¡ä»¶ã€‚æ­£å¦‚ Mhaskar å’Œ Poggioï¼ˆ[2020](#bib.bib218)ï¼‰æ‰€è§‚å¯Ÿåˆ°çš„ï¼Œç¥ç»ç½‘ç»œä¸­çš„çº¿æ€§åŒºåŸŸçš„æœ€ä¼˜æ•°é‡â€”â€”æˆ–è€…ç›¸åº”åœ°ï¼Œå…¶æ‰€å»ºæ¨¡çš„åˆ†æ®µçº¿æ€§å‡½æ•°çš„éƒ¨åˆ†æ•°é‡â€”â€”å–å†³äºè¢«é€¼è¿‘çš„å‡½æ•°ã€‚æ­¤å¤–ï¼Œçº¿æ€§åŒºåŸŸä¹Ÿè¢«æ˜¾å¼åœ°ç”¨äºæ„å»ºå‡½æ•°é€¼è¿‘ã€‚Kumar
    ç­‰äººï¼ˆ[2019](#bib.bib181)ï¼‰å·²ç»è¯æ˜ï¼Œå¸¦æœ‰ä¸¤ä¸ªéšè—å±‚çš„æ•´æµç½‘ç»œå¯ä»¥è¾¾åˆ°ä»»æ„ç²¾åº¦ï¼Œå…¶ä¸­æœ€å¤§çš„å±‚æœ‰ä¸€ä¸ªç¥ç»å…ƒå¯¹åº”äºåŸå§‹ç½‘ç»œçš„æ¯ç§ä¸åŒæ¿€æ´»æ¨¡å¼ï¼›è¿™ä¸€ç»“æœçš„ç²¾ç¡®å¯¹ç­‰ç‰©åæ¥ç”±
    Villani å’Œ Schootsï¼ˆ[2023](#bib.bib319)ï¼‰æå‡ºã€‚Fan ç­‰äººï¼ˆ[2020](#bib.bib99)ï¼‰æè¿°äº†åœ¨è¶³å¤Ÿå®½å’Œæ·±çš„ç½‘ç»œä¹‹é—´çš„å˜æ¢ï¼ŒåŒæ—¶äº‰è®ºè¯´å¤æ‚æ€§çš„åŸºæœ¬åº¦é‡åº”ä¸ºçº¿æ€§åŒºåŸŸå†…çš„å•çº¯å½¢æ•°ã€‚åœ¨éšåçš„å·¥ä½œä¸­ï¼ŒFan
    ç­‰äººï¼ˆ[2023](#bib.bib100)ï¼‰å®è¯è§‚å¯Ÿåˆ°çº¿æ€§åŒºåŸŸå¾€å¾€å…·æœ‰å°‘é‡çš„é«˜ç»´é¢æˆ–é¢ç‰‡ã€‚
- en: 'More recent studies aimed at understanding the expressiveness and approximability
    of neural networks in terms of their number of parameters, in particular when
    the number of linear regions is greater than the number of parameters (Malach
    and Shalev-Shwartz, [2019](#bib.bib208), Dym etÂ al., [2020](#bib.bib86), Daubechies
    etÂ al., [2022](#bib.bib76)). They all discuss how the composition the modeled
    functions tend to present the self-similarity property of fractal distributions,
    which is one reason why they have so many linear regions. Keup and Helias ([2022](#bib.bib172))
    interpreted the connection between linear regions in different parts of the input
    space in terms of how paper origamis are constructed: by â€œfoldingâ€ the data for
    separability.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘çš„ç ”ç©¶æ—¨åœ¨ç†è§£ç¥ç»ç½‘ç»œåœ¨å…¶å‚æ•°æ•°é‡æ–¹é¢çš„è¡¨ç°åŠ›å’Œè¿‘ä¼¼èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å½“çº¿æ€§åŒºåŸŸçš„æ•°é‡å¤§äºå‚æ•°çš„æ•°é‡æ—¶ï¼ˆMalach å’Œ Shalev-Shwartzï¼Œ[2019](#bib.bib208)ï¼ŒDym
    ç­‰ï¼Œ[2020](#bib.bib86)ï¼ŒDaubechies ç­‰ï¼Œ[2022](#bib.bib76)ï¼‰ã€‚è¿™äº›ç ”ç©¶è®¨è®ºäº†å»ºæ¨¡å‡½æ•°çš„ç»„åˆå¦‚ä½•å‘ˆç°åˆ†å½¢åˆ†å¸ƒçš„è‡ªç›¸ä¼¼æ€§ç‰¹å¾ï¼Œè¿™ä¹Ÿæ˜¯å®ƒä»¬æœ‰å¦‚æ­¤å¤šçº¿æ€§åŒºåŸŸçš„åŸå› ä¹‹ä¸€ã€‚Keup
    å’Œ Helias ([2022](#bib.bib172)) é€šè¿‡â€œæŠ˜å â€æ•°æ®ä»¥å®ç°å¯åˆ†æ€§ï¼Œå°†è¾“å…¥ç©ºé—´ä¸­ä¸åŒéƒ¨åˆ†çš„çº¿æ€§åŒºåŸŸä¹‹é—´çš„è¿æ¥è¿›è¡Œäº†é˜é‡Šã€‚
- en: 'Another related topic is computing the Lipschitz constant $\rho$ of the function
    $f(x)$ modeled by the neural network, the smallest $\rho$ for which $\|f(x^{\prime})-f(x)\|\leq\rho\|x^{\prime}-x\|$
    for any two inputs $x$ and $x^{\prime}$. Note that the first derivative of the
    output of a linear region is constant, which is leveraged byÂ Hwang and Heinecke
    ([2020](#bib.bib160)) to evaluate the stability of the network by computing the
    constant across linear regions by changing the activation pattern. Interestingly,
    Zhou and Schoellig ([2019](#bib.bib361)) showed that the constant grows similarly
    to the number of linear regions: polynomial in width and exponential in depth.
    A smaller constant limits the susceptibility of the network to adversarial examplesÂ (Huster
    etÂ al., [2018](#bib.bib159)), which are discussed in SectionÂ [4](#S4 "4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey"), and also lead to smaller bias variance (Loukas etÂ al., [2021](#bib.bib201)).
    While calculating the exact Lipschitz constant is NP-hard and encourages approximations
    (Virmaux and Scaman, [2018](#bib.bib322), Patrick L.Â Combettes, [2019](#bib.bib244)),
    the exact constant can be computed using MILP (Jordan and Dimakis, [2020](#bib.bib166)).
    Notably, many studies have focused on relaxations such as linear programming (Zou
    etÂ al., [2019](#bib.bib363)), semidefinite programming (Fazlyab etÂ al., [2019](#bib.bib101),
    Chen etÂ al., [2020](#bib.bib53)), and polynomial optimization (Latorre etÂ al.,
    [2020](#bib.bib184)). An alternative approach is to use more sophisticated activation
    functions for limiting the value of the constant (Anil etÂ al., [2019](#bib.bib6),
    Aziznejad etÂ al., [2020](#bib.bib9)).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦ä¸€ä¸ªç›¸å…³ä¸»é¢˜æ˜¯è®¡ç®—ç”±ç¥ç»ç½‘ç»œå»ºæ¨¡çš„å‡½æ•° $f(x)$ çš„ Lipschitz å¸¸æ•° $\rho$ï¼Œå³å¯¹äºä»»ä½•ä¸¤ä¸ªè¾“å…¥ $x$ å’Œ $x^{\prime}$ï¼Œæ»¡è¶³
    $\|f(x^{\prime})-f(x)\|\leq\rho\|x^{\prime}-x\|$ çš„æœ€å° $\rho$ã€‚æ³¨æ„ï¼Œçº¿æ€§åŒºåŸŸè¾“å‡ºçš„ç¬¬ä¸€å¯¼æ•°æ˜¯å¸¸æ•°ï¼Œè¿™ä¸€ç‚¹è¢«
    Hwang å’Œ Heinecke ([2020](#bib.bib160)) åˆ©ç”¨ï¼Œé€šè¿‡æ”¹å˜æ¿€æ´»æ¨¡å¼è®¡ç®—çº¿æ€§åŒºåŸŸä¸­çš„å¸¸æ•°æ¥è¯„ä¼°ç½‘ç»œçš„ç¨³å®šæ€§ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒZhou
    å’Œ Schoellig ([2019](#bib.bib361)) è¯æ˜äº†è¯¥å¸¸æ•°çš„å¢é•¿ç±»ä¼¼äºçº¿æ€§åŒºåŸŸçš„æ•°é‡ï¼šå®½åº¦ä¸Šæ˜¯å¤šé¡¹å¼çš„ï¼Œæ·±åº¦ä¸Šæ˜¯æŒ‡æ•°çš„ã€‚è¾ƒå°çš„å¸¸æ•°é™åˆ¶äº†ç½‘ç»œå¯¹å¯¹æŠ—æ ·æœ¬çš„æ•æ„Ÿæ€§ï¼ˆHuster
    ç­‰ï¼Œ[2018](#bib.bib159)ï¼‰ï¼Œè¿™äº›å¯¹æŠ—æ ·æœ¬åœ¨[4](#S4 "4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")èŠ‚ä¸­è¿›è¡Œäº†è®¨è®ºï¼Œå¹¶ä¸”ä¹Ÿå¯¼è‡´è¾ƒå°çš„åå·®æ–¹å·®ï¼ˆLoukas
    ç­‰ï¼Œ[2021](#bib.bib201)ï¼‰ã€‚è™½ç„¶è®¡ç®—ç²¾ç¡®çš„ Lipschitz å¸¸æ•°æ˜¯ NP éš¾çš„ï¼Œå¹¶é¼“åŠ±ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•ï¼ˆVirmaux å’Œ Scamanï¼Œ[2018](#bib.bib322)ï¼ŒPatrick
    L. Combettesï¼Œ[2019](#bib.bib244)ï¼‰ï¼Œä½†å¯ä»¥é€šè¿‡ MILP è®¡ç®—ç²¾ç¡®å¸¸æ•°ï¼ˆJordan å’Œ Dimakisï¼Œ[2020](#bib.bib166)ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè®¸å¤šç ”ç©¶é›†ä¸­äºçº¿æ€§è§„åˆ’ï¼ˆZou
    ç­‰ï¼Œ[2019](#bib.bib363)ï¼‰ï¼ŒåŠæ­£å®šè§„åˆ’ï¼ˆFazlyab ç­‰ï¼Œ[2019](#bib.bib101)ï¼ŒChen ç­‰ï¼Œ[2020](#bib.bib53)ï¼‰ï¼Œå’Œå¤šé¡¹å¼ä¼˜åŒ–ï¼ˆLatorre
    ç­‰ï¼Œ[2020](#bib.bib184)ï¼‰ç­‰æ”¾æ¾æ–¹æ³•ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨æ›´å¤æ‚çš„æ¿€æ´»å‡½æ•°æ¥é™åˆ¶å¸¸æ•°çš„å€¼ï¼ˆAnil ç­‰ï¼Œ[2019](#bib.bib6)ï¼ŒAziznejad
    ç­‰ï¼Œ[2020](#bib.bib9)ï¼‰ã€‚'
- en: 3.5.5 Optimizing over linear regions
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.5 åœ¨çº¿æ€§åŒºåŸŸä¸­çš„ä¼˜åŒ–
- en: 'As an alternative to optimizing over neural networks as described next in SectionÂ [4](#S4
    "4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"), a number of approaches have resorted to techniques that are
    equivalent to systematically enumerating or traversing linear regions and optimizing
    over them (Croce and Hein, [2018](#bib.bib66), Croce etÂ al., [2020](#bib.bib68),
    Khedr etÂ al., [2020](#bib.bib174), Vincent and Schwager, [2021](#bib.bib320),
    Xu etÂ al., [2022](#bib.bib346)). Notably, Vincent and Schwager ([2021](#bib.bib320))
    and Xu etÂ al. ([2022](#bib.bib346)) are mindful of the facet-defining inequalities
    associated with a linear region, which are the ones to change when moving toward
    an adjacent linear region. On a related note, Seck etÂ al. ([2021](#bib.bib278))
    alternates between gradient steps and solving a linear programming model within
    the current linear region.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¼˜åŒ–ç¥ç»ç½‘ç»œçš„æ›¿ä»£æ–¹æ³•ï¼Œå¦‚ç¬¬[4](#S4 "4 ä¼˜åŒ–ç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")èŠ‚ä¸­æ‰€è¿°ï¼Œä¸€äº›æ–¹æ³•å·²è¯‰è¯¸äºç³»ç»Ÿåœ°åˆ—ä¸¾æˆ–éå†çº¿æ€§åŒºåŸŸå¹¶å¯¹å…¶è¿›è¡Œä¼˜åŒ–çš„æŠ€æœ¯ï¼ˆCroce
    å’Œ Heinï¼Œ[2018](#bib.bib66)ï¼ŒCroce ç­‰ï¼Œ[2020](#bib.bib68)ï¼ŒKhedr ç­‰ï¼Œ[2020](#bib.bib174)ï¼ŒVincent
    å’Œ Schwagerï¼Œ[2021](#bib.bib320)ï¼ŒXu ç­‰ï¼Œ[2022](#bib.bib346)ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVincent å’Œ Schwagerï¼ˆ[2021](#bib.bib320)ï¼‰ä»¥åŠ
    Xu ç­‰ï¼ˆ[2022](#bib.bib346)ï¼‰å…³æ³¨ä¸çº¿æ€§åŒºåŸŸç›¸å…³çš„é¢å®šä¹‰ä¸ç­‰å¼ï¼Œè¿™äº›æ˜¯ä¸ç­‰å¼åœ¨å‘ç›¸é‚»çº¿æ€§åŒºåŸŸç§»åŠ¨æ—¶éœ€è¦æ›´æ”¹çš„ã€‚ç›¸å…³åœ°ï¼ŒSeck ç­‰ï¼ˆ[2021](#bib.bib278)ï¼‰åœ¨å½“å‰çº¿æ€§åŒºåŸŸå†…äº¤æ›¿è¿›è¡Œæ¢¯åº¦æ­¥éª¤å’Œæ±‚è§£çº¿æ€§è§„åˆ’æ¨¡å‹ã€‚
- en: 4 Optimizing Over a Trained Neural Network
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 ä¼˜åŒ–ç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œ
- en: 'In Section [5](#S5 "5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey") we will see how polyhedral-based
    methods can be used to *train* a neural network. In this section, we will focus
    on how polyhedral-based methods can be used to do something with a neural network
    *after it has been trained.* Specifically, after the network architecture and
    all parameters have been fixed, a neural network $f$ is merely a function. If
    each activation function $\sigma$ used to describe the network is piecewise linear
    (as is the case with those presented in TableÂ [1](#S1.T1 "Table 1 â€£ 1.3 Why nonlinearity
    is important in artificial neurons â€£ 1 Introduction â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")), $f$ is also a piecewise linear function. Therefore,
    any optimization problem containing $f$ in some way will be a piecewise linear
    optimization problem. For example, in the simple case where the output of $f$
    is univariate, the optimization problem'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[5](#S5 "5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨åŸºäºå¤šé¢ä½“çš„æ–¹æ³•æ¥*è®­ç»ƒ*ç¥ç»ç½‘ç»œã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨å¦‚ä½•åœ¨ç¥ç»ç½‘ç»œ*è®­ç»ƒå®Œæˆå*ä½¿ç”¨åŸºäºå¤šé¢ä½“çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç½‘ç»œæ¶æ„å’Œæ‰€æœ‰å‚æ•°å·²å›ºå®šåï¼Œç¥ç»ç½‘ç»œ$f$ä»…ä»…æ˜¯ä¸€ä¸ªå‡½æ•°ã€‚å¦‚æœç”¨äºæè¿°ç½‘ç»œçš„æ¯ä¸ªæ¿€æ´»å‡½æ•°$\sigma$æ˜¯åˆ†æ®µçº¿æ€§çš„ï¼ˆå¦‚è¡¨[1](#S1.T1
    "è¡¨ 1 â€£ 1.3 ä¸ºä»€ä¹ˆéçº¿æ€§åœ¨äººå·¥ç¥ç»å…ƒä¸­é‡è¦ â€£ 1 ç®€ä»‹ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ä¸­æ‰€ç¤ºï¼‰ï¼Œ$f$ä¹Ÿæ˜¯ä¸€ä¸ªåˆ†æ®µçº¿æ€§å‡½æ•°ã€‚å› æ­¤ï¼Œä»»ä½•ä»¥æŸç§æ–¹å¼åŒ…å«$f$çš„ä¼˜åŒ–é—®é¢˜éƒ½å°†æ˜¯ä¸€ä¸ªåˆ†æ®µçº¿æ€§ä¼˜åŒ–é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œåœ¨ç®€å•çš„æƒ…å†µä¸‹ï¼Œå½“$f$çš„è¾“å‡ºæ˜¯å•å˜é‡æ—¶ï¼Œä¼˜åŒ–é—®é¢˜
- en: '|  | $\min_{x\in\mathcal{X}}f(x)$ |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{x\in\mathcal{X}}f(x)$ |  |'
- en: 'is a piecewise linear optimization problem. As discussed in Section [3](#S3
    "3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"), this problem can have an enormous number of â€œpiecesâ€ (linear
    regions) when $f$ is a neural network; solving this problem thus heavily depends
    on the size and structure of the neural network $f$. For example, the training
    procedure by which $f$ is obtained can greatly influence the performance of optimization
    strategies (Tjeng etÂ al., [2019](#bib.bib309), Xiao etÂ al., [2019](#bib.bib341)).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯ä¸€ä¸ªåˆ†æ®µçº¿æ€§ä¼˜åŒ–é—®é¢˜ã€‚å¦‚ç¬¬[3](#S3 "3 ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒºåŸŸ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")èŠ‚è®¨è®ºçš„é‚£æ ·ï¼Œå½“$f$æ˜¯ç¥ç»ç½‘ç»œæ—¶ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½æœ‰å¤§é‡çš„â€œç‰‡æ®µâ€ï¼ˆçº¿æ€§åŒºåŸŸï¼‰ï¼›å› æ­¤ï¼Œè§£å†³è¿™ä¸ªé—®é¢˜ä¸¥é‡ä¾èµ–äºç¥ç»ç½‘ç»œ$f$çš„å¤§å°å’Œç»“æ„ã€‚ä¾‹å¦‚ï¼Œè·å–$f$çš„è®­ç»ƒè¿‡ç¨‹å¯ä»¥æå¤§åœ°å½±å“ä¼˜åŒ–ç­–ç•¥çš„æ€§èƒ½ï¼ˆTjeng
    ç­‰ï¼Œ[2019](#bib.bib309)ï¼ŒXiao ç­‰ï¼Œ[2019](#bib.bib341)ï¼‰ã€‚
- en: In this section, we first explore situations in which you might want to optimize
    over a trained neural network in this manner. We will then survey available methods
    for solving this method (either exactly or on the dual side) using polyhedral-based
    methods. We conclude with a brief view of future directions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ¢è®¨ä½ å¯èƒ½æƒ³ä»¥è¿™ç§æ–¹å¼ä¼˜åŒ–ç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œçš„æƒ…å†µã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è°ƒæŸ¥ä½¿ç”¨åŸºäºå¤šé¢ä½“çš„æ–¹æ³•æ¥è§£å†³è¿™ç§æ–¹æ³•ï¼ˆæ— è®ºæ˜¯å‡†ç¡®è§£å†³è¿˜æ˜¯åœ¨å¯¹å¶ä¾§è§£å†³ï¼‰çš„å¯ç”¨æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬ç®€è¦å±•æœ›æœªæ¥çš„å‘å±•æ–¹å‘ã€‚
- en: 4.1 Applications of optimization over trained networks
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 ç»è¿‡è®­ç»ƒçš„ç½‘ç»œä¸Šçš„ä¼˜åŒ–åº”ç”¨
- en: 'Applications where you might want to optimize over a trained neural network
    $f$ broadly fall into two categories: those where $f$ is the â€œtrueâ€ object of
    interest, and those where $f$ is a convenient proxy for some unknown, underlying
    behavior.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼˜åŒ–è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œ $f$ çš„åº”ç”¨ä¸­ï¼Œå¤§è‡´å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šé‚£äº› $f$ æ˜¯â€œçœŸå®â€æ„Ÿå…´è¶£å¯¹è±¡çš„åº”ç”¨ï¼Œä»¥åŠé‚£äº› $f$ æ˜¯æŸç§æœªçŸ¥çš„ã€æ½œåœ¨è¡Œä¸ºçš„æ–¹ä¾¿ä»£ç†çš„åº”ç”¨ã€‚
- en: 4.1.1 Neural network verification
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 ç¥ç»ç½‘ç»œéªŒè¯
- en: 'Neural network verification is a burgeoning field of study in deep learning.
    Starting in the early 2000s, researchers began to recognize the importance of
    rigorously verifying the behavior of neural networks, mainly in aviation-related
    applications (Schumann etÂ al., [2003](#bib.bib274), Zakrzewski, [2001](#bib.bib351)).
    More recently, the seminal works of Szegedy etÂ al. ([2014](#bib.bib300)) and Goodfellow
    etÂ al. ([2015](#bib.bib124)) observed that neural networks are unusually susceptible
    to *adversarial attacks*. These are small, targeted perturbations that can drastically
    affect the output of the network; as shown in Figure [8](#S4.F8 "Figure 8 â€£ 4.1.1
    Neural network verification â€£ 4.1 Applications of optimization over trained networks
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"), even powerful models such as MobileNetV2 (Sandler etÂ al.,
    [2018](#bib.bib270)) are susceptible. The existence and prevalence of adversarial
    attacks in deep neural networks has raised justifiable concerns about the deployment
    of these models in mission-critical systems such as autonomous vehicles (Deng
    etÂ al., [2020](#bib.bib79)), aviation (Kouvaros etÂ al., [2021](#bib.bib177)),
    or medical systems (Finlayson etÂ al., [2019](#bib.bib105)). One fascinating empirical
    work by Eykholt etÂ al. ([2018](#bib.bib98)) showed the susceptibility of standard
    image classification networks that might be used in self-driving vehicles to a
    very analogue form of attacks: black/white stickers, placed in a careful way,
    could confuse these models enough that they would mis-classify road signs (e.g.,
    mistaking stop signs for â€œspeed limit 80â€ signs).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¥ç»ç½‘ç»œéªŒè¯æ˜¯æ·±åº¦å­¦ä¹ ä¸­ä¸€ä¸ªæ–°å…´çš„ç ”ç©¶é¢†åŸŸã€‚ä»2000å¹´ä»£åˆæœŸå¼€å§‹ï¼Œç ”ç©¶äººå‘˜é€æ¸è®¤è¯†åˆ°ä¸¥æ ¼éªŒè¯ç¥ç»ç½‘ç»œè¡Œä¸ºçš„é‡è¦æ€§ï¼Œä¸»è¦æ˜¯åœ¨èˆªç©ºç›¸å…³çš„åº”ç”¨ä¸­ï¼ˆSchumann
    et al., [2003](#bib.bib274), Zakrzewski, [2001](#bib.bib351)ï¼‰ã€‚æœ€è¿‘ï¼ŒSzegedy et al.
    ([2014](#bib.bib300)) å’Œ Goodfellow et al. ([2015](#bib.bib124)) çš„å¼€åˆ›æ€§å·¥ä½œè§‚å¯Ÿåˆ°ç¥ç»ç½‘ç»œå¯¹*å¯¹æŠ—æ”»å‡»*å¼‚å¸¸æ•æ„Ÿã€‚è¿™äº›æ˜¯å¯ä»¥æå¤§å½±å“ç½‘ç»œè¾“å‡ºçš„å°å‹ã€é’ˆå¯¹æ€§çš„æ‰°åŠ¨ï¼›å¦‚å›¾
    [8](#S4.F8 "Figure 8 â€£ 4.1.1 Neural network verification â€£ 4.1 Applications of
    optimization over trained networks â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey") æ‰€ç¤ºï¼Œå³ä¾¿æ˜¯å¼ºå¤§çš„æ¨¡å‹å¦‚ MobileNetV2
    (Sandler et al., [2018](#bib.bib270)) ä¹Ÿä¼šå—åˆ°å½±å“ã€‚å¯¹æŠ—æ”»å‡»åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„å­˜åœ¨å’Œæ™®éæ€§å¼•å‘äº†å¯¹è¿™äº›æ¨¡å‹åœ¨å…³é”®ä»»åŠ¡ç³»ç»Ÿä¸­éƒ¨ç½²çš„åˆç†æ‹…å¿§ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆDeng
    et al., [2020](#bib.bib79)ï¼‰ã€èˆªç©ºï¼ˆKouvaros et al., [2021](#bib.bib177)ï¼‰æˆ–åŒ»ç–—ç³»ç»Ÿï¼ˆFinlayson
    et al., [2019](#bib.bib105)ï¼‰ã€‚Eykholt et al. ([2018](#bib.bib98)) çš„ä¸€é¡¹å¼•äººæ³¨ç›®çš„å®è¯ç ”ç©¶å±•ç¤ºäº†æ ‡å‡†å›¾åƒåˆ†ç±»ç½‘ç»œåœ¨è‡ªé©¾è½¦ä¸­å¯èƒ½é­å—çš„ä¸€ç§éå¸¸ç±»ä¼¼çš„æ”»å‡»ï¼šé»‘ç™½è´´çº¸ï¼Œä»¥å·§å¦™çš„æ–¹å¼æ”¾ç½®ï¼Œå¯èƒ½ä¼šä½¿è¿™äº›æ¨¡å‹æ··æ·†ï¼Œä»¥è‡³äºé”™è¯¯åˆ†ç±»äº¤é€šæ ‡å¿—ï¼ˆä¾‹å¦‚ï¼Œå°†åœè½¦æ ‡å¿—è¯¯è®¤ä¸ºâ€œé™é€Ÿ80â€æ ‡å¿—ï¼‰ã€‚'
- en: <svg   height="130.22" overflow="visible" version="1.1" width="407.44"><g transform="translate(0,130.22)
    matrix(1 0 0 -1 0 0) translate(54.11,0) translate(0,65.11)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -49.5 -60.5)" fill="#000000"
    stroke="#000000"><foreignobject width="99" height="121" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![Refer to caption](img/eb8e031e8c492fc9ac38e777fd76ac60.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 67.05 -16.79)" fill="#000000" stroke="#000000"><foreignobject
    width="15.5" height="13.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 100.11 -59.74)" fill="#000000" stroke="#000000"><foreignobject
    width="99" height="99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/6ec4f02d882565048e57d58d5b3977d5.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 216.66 -15.47)" fill="#000000" stroke="#000000"><foreignobject width="15.5"
    height="7.31" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">=</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.71 -60.5)" fill="#000000" stroke="#000000"><foreignobject
    width="99" height="121" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/1df2f6052ecb61a06e1057f75e5d6880.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 111.82 43.78)" fill="#000000" stroke="#000000"><foreignobject width="75.57"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\times(\epsilon=0.15)$</foreignobject></g></g></svg>
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![å‚è§è¯´æ˜](img/eb8e031e8c492fc9ac38e777fd76ac60.png) + ![å‚è§è¯´æ˜](img/6ec4f02d882565048e57d58d5b3977d5.png)
    = ![å‚è§è¯´æ˜](img/1df2f6052ecb61a06e1057f75e5d6880.png) $\times(\epsilon=0.15)$'
- en: 'Figure 8: Example of adversarial attack on MobileNetV2 (Sandler etÂ al., [2018](#bib.bib270)).
    The original image taken by one of the survey authors is classified as â€˜siberian_husky,â€™
    but is re-classified as â€˜wallabyâ€™ with a small (in an $\ell_{\infty}$-norm sense)
    targeted attack.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8ï¼šå¯¹ MobileNetV2 çš„å¯¹æŠ—æ”»å‡»ç¤ºä¾‹ï¼ˆSandler ç­‰ï¼Œ[2018](#bib.bib270)ï¼‰ã€‚åŸå§‹å›¾åƒç”±ä¸€ä½è°ƒæŸ¥ä½œè€…æ‹æ‘„ï¼Œåˆ†ç±»ä¸ºâ€˜siberian_huskyâ€™ï¼Œä½†åœ¨ç»è¿‡ä¸€ä¸ªå°çš„ï¼ˆåœ¨$\ell_{\infty}$-èŒƒæ•°æ„ä¹‰ä¸Šï¼‰æœ‰é’ˆå¯¹æ€§çš„æ”»å‡»åï¼Œè¢«é‡æ–°åˆ†ç±»ä¸ºâ€˜wallabyâ€™ã€‚
- en: Neural network verification seeks to prove (or disprove) a given input-output
    relationship, i.e., $x\in\mathcal{X}\Rightarrow y\in\mathcal{Y}$, that gives some
    indication of model robustness. Methods for verifying this relationship are classified
    as being sound and/or complete. A method that is sound will only certify the relationship
    if it is indeed true (no false positives), while a method that is complete will
    (i) always return an answer and (ii) only disprove the relationship if it is false
    (no false negatives). An early set of papers (Fischetti and Jo, [2018](#bib.bib106),
    Lomuscio and Maganti, [2017](#bib.bib200), Tjeng etÂ al., [2019](#bib.bib309))
    recognized that MILP provides an avenue for verification that is both sound and
    complete, given that $\mathcal{X}$ and $f(x)$ are both linear, or piecewise linear.
    We refer the readers to recent reviews (Huang etÂ al., [2020](#bib.bib157), Leofante
    etÂ al., [2018](#bib.bib190), Li etÂ al., [2022](#bib.bib191), Liu etÂ al., [2021](#bib.bib197))
    for a more comprehensive treatment of the landscape of verification methods, including
    MILP- and LP-based technologies.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œéªŒè¯æ—¨åœ¨è¯æ˜ï¼ˆæˆ–åé©³ï¼‰ç»™å®šçš„è¾“å…¥è¾“å‡ºå…³ç³»ï¼Œå³ $x\in\mathcal{X}\Rightarrow y\in\mathcal{Y}$ï¼Œè¿™æä¾›äº†ä¸€äº›æ¨¡å‹é²æ£’æ€§çš„æŒ‡ç¤ºã€‚éªŒè¯è¿™ä¸€å…³ç³»çš„æ–¹æ³•è¢«åˆ†ç±»ä¸ºâ€œå¥å…¨çš„â€å’Œ/æˆ–â€œå®Œæ•´çš„â€ã€‚ä¸€ç§å¥å…¨çš„æ–¹æ³•åªä¼šåœ¨å…³ç³»ç¡®å®ä¸ºçœŸæ—¶æ‰ä¼šè®¤è¯è¯¥å…³ç³»ï¼ˆæ²¡æœ‰å‡é˜³æ€§ï¼‰ï¼Œè€Œä¸€ç§å®Œæ•´çš„æ–¹æ³•ä¼š
    (i) å§‹ç»ˆè¿”å›ä¸€ä¸ªç­”æ¡ˆï¼Œå¹¶ä¸” (ii) åªæœ‰åœ¨å…³ç³»ä¸ºå‡æ—¶æ‰ä¼šåé©³è¯¥å…³ç³»ï¼ˆæ²¡æœ‰å‡é˜´æ€§ï¼‰ã€‚æ—©æœŸçš„ä¸€äº›è®ºæ–‡ï¼ˆFischetti å’Œ Joï¼Œ[2018](#bib.bib106)ï¼ŒLomuscio
    å’Œ Magantiï¼Œ[2017](#bib.bib200)ï¼ŒTjeng ç­‰ï¼Œ[2019](#bib.bib309)ï¼‰è®¤è¯†åˆ°ï¼ŒMILP æä¾›äº†ä¸€æ¡æ—¢å¥å…¨åˆå®Œæ•´çš„éªŒè¯é€”å¾„ï¼Œå‰ææ˜¯
    $\mathcal{X}$ å’Œ $f(x)$ éƒ½æ˜¯çº¿æ€§çš„æˆ–åˆ†æ®µçº¿æ€§çš„ã€‚æˆ‘ä»¬æ¨èè¯»è€…å‚è€ƒè¿‘æœŸçš„ç»¼è¿°ï¼ˆHuang ç­‰ï¼Œ[2020](#bib.bib157)ï¼ŒLeofante
    ç­‰ï¼Œ[2018](#bib.bib190)ï¼ŒLi ç­‰ï¼Œ[2022](#bib.bib191)ï¼ŒLiu ç­‰ï¼Œ[2021](#bib.bib197)ï¼‰ä»¥è·å¾—æ›´å…¨é¢çš„éªŒè¯æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäº
    MILP å’Œ LP çš„æŠ€æœ¯ã€‚
- en: Example 4
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ 4
- en: Consider a classification network $f:[0,1]^{n_{0}}\to\mathbb{R}^{d}$ where the
    $j$-th output, $f(x)_{j}$, corresponds to the probability that input $x$ is of
    class $j$.Â²Â²2In actuality, we will instead typically work with the outputs corresponding
    to â€œlogitsâ€, or unnormalized probabilities. These are typically fed into a softmax
    layer that then normalize these values to correspond to a probability distribution
    over the classes. However, this nonlinear softmax transformation is not piecewise
    linear. Thankfully, it can be omitted in the context of the verification task
    without loss of generality. Then consider a labeled image $\hat{x}$ known to be
    of class $i$, and a â€œtargetâ€ adversarial class $k\neq i$. Then verifying local
    robustness of the prediction corresponds to checking $x\in\{x:||x-\hat{x}||\leq\epsilon\}\Rightarrow
    y=f(x)\in\{y:y_{i}\geq y_{k}\}$, where $\epsilon>0$ is a constant which prescribes
    the radius around which $\hat{x}$ we will search for an adversarial example.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªåˆ†ç±»ç½‘ç»œ $f:[0,1]^{n_{0}}\to\mathbb{R}^{d}$ï¼Œå…¶ä¸­ç¬¬ $j$ ä¸ªè¾“å‡º $f(x)_{j}$ å¯¹åº”äºè¾“å…¥ $x$
    å±äºç±»åˆ« $j$ çš„æ¦‚ç‡ã€‚Â²Â²2 å®é™…ä¸Šï¼Œæˆ‘ä»¬é€šå¸¸å¤„ç†çš„æ˜¯å¯¹åº”äºâ€œlogitsâ€æˆ–æœªå½’ä¸€åŒ–æ¦‚ç‡çš„è¾“å‡ºã€‚è¿™äº›è¾“å‡ºé€šå¸¸ä¼šè¾“å…¥åˆ°ä¸€ä¸ª softmax å±‚ï¼Œè¯¥å±‚ä¼šå°†è¿™äº›å€¼å½’ä¸€åŒ–ä¸ºå¯¹åº”äºç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒã€‚ç„¶è€Œï¼Œè¿™ç§éçº¿æ€§
    softmax å˜æ¢å¹¶ä¸æ˜¯åˆ†æ®µçº¿æ€§çš„ã€‚å¹¸è¿çš„æ˜¯ï¼Œåœ¨éªŒè¯ä»»åŠ¡çš„èƒŒæ™¯ä¸‹ï¼Œå¯ä»¥çœç•¥è¿™ä¸€æ­¥è€Œä¸ä¼šå¤±å»ä¸€èˆ¬æ€§ã€‚ç„¶åè€ƒè™‘ä¸€ä¸ªå·²çŸ¥å±äºç±»åˆ« $i$ çš„æ ‡è®°å›¾åƒ $\hat{x}$ï¼Œä»¥åŠä¸€ä¸ªâ€œç›®æ ‡â€å¯¹æŠ—ç±»åˆ«
    $k\neq i$ã€‚ç„¶åï¼ŒéªŒè¯é¢„æµ‹çš„å±€éƒ¨é²æ£’æ€§å¯¹åº”äºæ£€æŸ¥ $x\in\{x:||x-\hat{x}||\leq\epsilon\}\Rightarrow y=f(x)\in\{y:y_{i}\geq
    y_{k}\}$ï¼Œå…¶ä¸­ $\epsilon>0$ æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œè§„å®šäº†æˆ‘ä»¬å°†å›´ç»• $\hat{x}$ æœç´¢å¯¹æŠ—æ ·æœ¬çš„åŠå¾„ã€‚
- en: 'This verification task can be formulated as an optimization problem of the
    form:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªéªŒè¯ä»»åŠ¡å¯ä»¥è¢«è¡¨è¿°ä¸ºå¦‚ä¸‹å½¢å¼çš„ä¼˜åŒ–é—®é¢˜ï¼š
- en: '|  | $\displaystyle\max_{x\in[0,1]^{n_{0}}}$ | $\displaystyle f(x)_{k}-f(x)_{i}$
    |  | (3) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{x\in[0,1]^{n_{0}}}$ | $\displaystyle f(x)_{k}-f(x)_{i}$
    |  | (3) |'
- en: '|  | s.t. | $\displaystyle&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon.$ |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon.$ |  |'
- en: 'Any feasible solution $x$ to this problem with positive cost is an adversarial
    example: it is very â€œcloseâ€ to $\hat{x}$ which has true label $i$, yet the network
    believes it is more likely to be of class $k$.Â³Â³3Alternative objectives are sometimes
    used which would allow us to strengthen this statement to say that the network
    *will* classify $x$ to be of class $k$. However, this will require a more complex
    reformulation to model this problem via MILP, so we omit it for simplicity. If,
    on the other hand, it is proven that the optimal objective value is negative,
    this proves that $f$ is robust (at least in the neighborhood around $\tilde{x}$).
    Note that the verification problem can terminate once the sign of the optimal
    objective value is determined, but solving the problem returns an optimal adversarial
    example.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œä»»ä½•å…·æœ‰æ­£æˆæœ¬çš„å¯è¡Œè§£ $x$ éƒ½æ˜¯ä¸€ä¸ªå¯¹æŠ—æ ·æœ¬ï¼šå®ƒéå¸¸â€œæ¥è¿‘â€å…·æœ‰çœŸå®æ ‡ç­¾ $i$ çš„ $\hat{x}$ï¼Œä½†ç½‘ç»œè®¤ä¸ºå®ƒæ›´å¯èƒ½å±äºç±» $k$ã€‚Â³Â³3
    æœ‰æ—¶ä½¿ç”¨æ›¿ä»£ç›®æ ‡ï¼Œè¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤ŸåŠ å¼ºè¿™ä¸ªå£°æ˜ï¼Œè¡¨æ˜ç½‘ç»œ *å°†* æŠŠ $x$ åˆ†ç±»ä¸ºç±» $k$ã€‚ç„¶è€Œï¼Œè¿™å°†éœ€è¦æ›´å¤æ‚çš„é‡æ„æ¥é€šè¿‡ MILP å»ºæ¨¡è¿™ä¸ªé—®é¢˜ï¼Œå› æ­¤ä¸ºäº†ç®€ä¾¿èµ·è§ï¼Œæˆ‘ä»¬çœç•¥äº†å®ƒã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœè¯æ˜æœ€ä¼˜ç›®æ ‡å€¼ä¸ºè´Ÿï¼Œåˆ™è¯æ˜
    $f$ æ˜¯ç¨³å¥çš„ï¼ˆè‡³å°‘åœ¨ $\tilde{x}$ é™„è¿‘ï¼‰ã€‚è¯·æ³¨æ„ï¼Œä¸€æ—¦ç¡®å®šäº†æœ€ä¼˜ç›®æ ‡å€¼çš„ç¬¦å·ï¼ŒéªŒè¯é—®é¢˜å¯ä»¥ç»ˆæ­¢ï¼Œä½†è§£å†³è¯¥é—®é¢˜å°†è¿”å›ä¸€ä¸ªæœ€ä¼˜å¯¹æŠ—æ ·æœ¬ã€‚
- en: 'The objective function of ([3](#S4.E3 "In Example 4 â€£ 4.1.1 Neural network
    verification â€£ 4.1 Applications of optimization over trained networks â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")) models the desired input-output relationship, $x\in\mathcal{X}\Rightarrow
    y\in\mathcal{Y}$, while the constraints model the domain $\mathcal{X}$. The domain
    $\mathcal{X}$ is typically a box or hyperrectangular domain. Extensions to this
    are described in Section [4.4.1](#S4.SS4.SSS1 "4.4.1 Extending to other domains
    â€£ 4.4 Generalizing the single neuron model â€£ 4 Optimizing Over a Trained Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"). Some explanation-focused
    verification applications define the input-output relationship in a derivative
    sense, e.g., $x\in\mathcal{X}\Rightarrow\partial y/\partial x\in\mathcal{Y}^{\prime}$
    (Wicker etÂ al., [2022](#bib.bib333)). As the derivative of the ReLU function is
    also piecewise linear, this class of problems can also be modeled in MILP. For
    example, in the context of fairness and explainability, Liu etÂ al. ([2020](#bib.bib198))
    and Jordan and Dimakis ([2020](#bib.bib166)) used MILP to certify monotonicity
    and to compute local Lipschitz constants, respectively.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡å‡½æ•° ([3](#S4.E3 "åœ¨ç¤ºä¾‹ 4 â€£ 4.1.1 ç¥ç»ç½‘ç»œéªŒè¯ â€£ 4.1 è®­ç»ƒç½‘ç»œçš„ä¼˜åŒ–åº”ç”¨ â€£ 4 ä¼˜åŒ–è®­ç»ƒç¥ç»ç½‘ç»œ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºçš„ç»“åˆï¼šç»¼è¿°"))
    å»ºæ¨¡äº†æœŸæœ›çš„è¾“å…¥è¾“å‡ºå…³ç³»ï¼Œ$x\in\mathcal{X}\Rightarrow y\in\mathcal{Y}$ï¼Œè€Œçº¦æŸæ¡ä»¶å»ºæ¨¡äº†åŸŸ $\mathcal{X}$ã€‚åŸŸ
    $\mathcal{X}$ é€šå¸¸æ˜¯ä¸€ä¸ªç›’å­æˆ–è¶…çŸ©å½¢åŸŸã€‚å¯¹æ­¤çš„æ‰©å±•åœ¨ç¬¬ [4.4.1](#S4.SS4.SSS1 "4.4.1 æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸ â€£ 4.4 ä¸€èˆ¬åŒ–å•ç¥ç»å…ƒæ¨¡å‹
    â€£ 4 ä¼˜åŒ–è®­ç»ƒç¥ç»ç½‘ç»œ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºçš„ç»“åˆï¼šç»¼è¿°") èŠ‚ä¸­æè¿°ã€‚ä¸€äº›ä»¥è§£é‡Šä¸ºé‡ç‚¹çš„éªŒè¯åº”ç”¨åœ¨å¯¼æ•°æ„ä¹‰ä¸Šå®šä¹‰è¾“å…¥è¾“å‡ºå…³ç³»ï¼Œä¾‹å¦‚ $x\in\mathcal{X}\Rightarrow\partial
    y/\partial x\in\mathcal{Y}^{\prime}$ (Wicker ç­‰ï¼Œ[2022](#bib.bib333))ã€‚ç”±äº ReLU å‡½æ•°çš„å¯¼æ•°ä¹Ÿæ˜¯åˆ†æ®µçº¿æ€§çš„ï¼Œè¿™ç±»é—®é¢˜ä¹Ÿå¯ä»¥ç”¨
    MILP è¿›è¡Œå»ºæ¨¡ã€‚ä¾‹å¦‚ï¼Œåœ¨å…¬å¹³æ€§å’Œå¯è§£é‡Šæ€§çš„èƒŒæ™¯ä¸‹ï¼ŒLiu ç­‰ ([2020](#bib.bib198)) å’Œ Jordan ä¸ Dimakis ([2020](#bib.bib166))
    åˆ†åˆ«ä½¿ç”¨ MILP è®¤è¯å•è°ƒæ€§å’Œè®¡ç®—å±€éƒ¨ Lipschitz å¸¸æ•°ã€‚
- en: 'Although in this survey we focus on optimization over trained neural networks,
    it is important to note that polyhedral theory underlies numerous strategies for
    neural network verification. For example, SAT and SMT (Satisfiability Modulo Theories)
    solvers designed for Boolean satisfiability problems (and more general problems
    for the case of SMT) can also be used to search through activation patterns for
    a neural network (Pulina and Tacchella, [2010](#bib.bib251)), resulting in tools
    that are sound and complete, such as Planet (Ehlers, [2017](#bib.bib87)) and Reluplex
    (Katz etÂ al., [2017](#bib.bib169)). Bunel etÂ al. ([2018](#bib.bib43)) presented
    a unified view to compare MILP and SMT formulations, as well as the relaxations
    that result from these formulations (we will revisit this in SectionÂ [4.3](#S4.SS3
    "4.3 Scaling further: Convex relaxations and linear programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")). On the other hand, strategies such as ExactReachÂ (Xiang etÂ al., [2017](#bib.bib340))
    exploit polyhedral theory to compute reachable sets: given an input set to a ReLU
    function defined as a union of polytopes, the output reachable set is also a union
    of polytopes. Other methods over-approximate the reachable set to improve scalability,
    e.g., for vision models (Yang etÂ al., [2021](#bib.bib349)), often resulting in
    methods that are sound, but not complete.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'è™½ç„¶åœ¨è¿™é¡¹è°ƒæŸ¥ä¸­æˆ‘ä»¬å…³æ³¨çš„æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œä¸Šçš„ä¼˜åŒ–ï¼Œä½†å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç«‹ä½“å‡ ä½•ç†è®ºæ˜¯ç¥ç»ç½‘ç»œéªŒè¯ä¼—å¤šç­–ç•¥çš„åŸºç¡€ã€‚ä¾‹å¦‚ï¼Œè®¾è®¡ç”¨äºå¸ƒå°”å¯æ»¡è¶³æ€§é—®é¢˜çš„ SAT
    å’Œ SMTï¼ˆæ¨¡ç†è®ºå¯æ»¡è¶³æ€§ï¼‰æ±‚è§£å™¨ï¼ˆå¯¹äº SMT çš„ä¸€èˆ¬é—®é¢˜ï¼‰ä¹Ÿå¯ä»¥ç”¨äºæœç´¢ç¥ç»ç½‘ç»œçš„æ¿€æ´»æ¨¡å¼ï¼ˆPulina å’Œ Tacchellaï¼Œ[2010](#bib.bib251)ï¼‰ï¼Œä»è€Œç”Ÿæˆå¦‚
    Planetï¼ˆEhlersï¼Œ[2017](#bib.bib87)ï¼‰å’Œ Reluplexï¼ˆKatz ç­‰ï¼Œ[2017](#bib.bib169)ï¼‰ç­‰å¥å…¨ä¸”å®Œæ•´çš„å·¥å…·ã€‚Bunel
    ç­‰äººï¼ˆ[2018](#bib.bib43)ï¼‰æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è§†è§’æ¥æ¯”è¾ƒ MILP å’Œ SMT å…¬å¼ï¼Œä»¥åŠè¿™äº›å…¬å¼å¸¦æ¥çš„æ¾å¼›ï¼ˆæˆ‘ä»¬å°†åœ¨ç¬¬[4.3èŠ‚](#S4.SS3
    "4.3 Scaling further: Convex relaxations and linear programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")ä¸­é‡æ–°å®¡è§†è¿™ä¸€ç‚¹ï¼‰ã€‚å¦ä¸€æ–¹é¢ï¼Œåƒ ExactReachï¼ˆXiang ç­‰ï¼Œ[2017](#bib.bib340)ï¼‰è¿™æ ·çš„ç­–ç•¥åˆ©ç”¨ç«‹ä½“å‡ ä½•ç†è®ºæ¥è®¡ç®—å¯è¾¾é›†ï¼šç»™å®šä¸€ä¸ªå®šä¹‰ä¸ºå¤šé¢ä½“å¹¶é›†çš„
    ReLU å‡½æ•°çš„è¾“å…¥é›†ï¼Œè¾“å‡ºå¯è¾¾é›†ä¹Ÿæ˜¯ä¸€ä¸ªå¤šé¢ä½“å¹¶é›†ã€‚å…¶ä»–æ–¹æ³•é€šè¿‡è¿‡åº¦è¿‘ä¼¼å¯è¾¾é›†æ¥æé«˜å¯æ‰©å±•æ€§ï¼Œä¾‹å¦‚ï¼Œé’ˆå¯¹è§†è§‰æ¨¡å‹ï¼ˆYang ç­‰ï¼Œ[2021](#bib.bib349)ï¼‰ï¼Œé€šå¸¸ä¼šå¯¼è‡´å¥å…¨ä½†ä¸å®Œæ•´çš„æ–¹æ³•ã€‚'
- en: 4.1.2 Neural network as proxy
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 ç¥ç»ç½‘ç»œä½œä¸ºä»£ç†
- en: Another situation in which you may want to solve an optimization problem containing
    trained neural networks is when you would like to optimize some other, unknown
    function for which you have historical input/output data. A similar situation
    arises when you want to solve an optimization problem where (some of) the constraints
    are overly complicated, but you can query samples from the constraints on which
    to train a simpler surrogate model. In these cases, you might imagine training
    a neural network in a standard supervised learning setting to approximate this
    underlying, unknown or complicated function. Then, since the neural network is
    known, you are left with a deterministic piecewise linear optimization problem.
    Note that we focus here on using a neural network as a surrogate; neural networks
    can additionally learn other components of an optimization problem, e.g., uncertainty
    sets for robust optimization (Goerigk and Kurtz, [2023](#bib.bib122)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šå¸Œæœ›åœ¨åŒ…å«è®­ç»ƒç¥ç»ç½‘ç»œçš„ä¼˜åŒ–é—®é¢˜ä¸­è§£å†³ä¸€äº›å…¶ä»–æœªçŸ¥å‡½æ•°çš„ä¼˜åŒ–é—®é¢˜ï¼Œè¿™äº›å‡½æ•°çš„å†å²è¾“å…¥/è¾“å‡ºæ•°æ®å·²çŸ¥ã€‚ç±»ä¼¼çš„æƒ…å†µå‘ç”Ÿåœ¨ä½ æƒ³è¦è§£å†³ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­ï¼ˆæŸäº›ï¼‰çº¦æŸæ¡ä»¶è¿‡äºå¤æ‚ï¼Œä½†ä½ å¯ä»¥æŸ¥è¯¢çº¦æŸæ ·æœ¬ä»¥è®­ç»ƒä¸€ä¸ªæ›´ç®€å•çš„ä»£ç†æ¨¡å‹ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½ä¼šæƒ³è±¡åœ¨æ ‡å‡†ç›‘ç£å­¦ä¹ è®¾ç½®ä¸­è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥é€¼è¿‘è¿™ä¸ªåŸºç¡€çš„ã€æœªçŸ¥çš„æˆ–å¤æ‚çš„å‡½æ•°ã€‚ç„¶åï¼Œç”±äºç¥ç»ç½‘ç»œæ˜¯å·²çŸ¥çš„ï¼Œä½ å°†é¢ä¸´ä¸€ä¸ªç¡®å®šæ€§çš„åˆ†æ®µçº¿æ€§ä¼˜åŒ–é—®é¢˜ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è¿™é‡Œå…³æ³¨çš„æ˜¯å°†ç¥ç»ç½‘ç»œç”¨ä½œä»£ç†ï¼›ç¥ç»ç½‘ç»œè¿˜å¯ä»¥å­¦ä¹ ä¼˜åŒ–é—®é¢˜çš„å…¶ä»–ç»„æˆéƒ¨åˆ†ï¼Œä¾‹å¦‚ï¼Œç”¨äºé²æ£’ä¼˜åŒ–çš„ä¸ç¡®å®šæ€§é›†ï¼ˆGoerigk
    å’Œ Kurtzï¼Œ[2023](#bib.bib122)ï¼‰ã€‚
- en: Several software tools have been developed for this class of problems. For the
    case of constraint learning, JANOS (Bergman etÂ al., [2022](#bib.bib24)) and OptiCL
    (Maragno etÂ al., [2021](#bib.bib210)) both provide functionality for learning
    a ReLU neural network to approximate a constraint based on data and embedding
    the learned neural network in MILP. The reluMIP package (Lueg etÂ al., [2021](#bib.bib203))
    has also been introduced to handle the latter embedding step. More generally,
    OMLT (Ceccon etÂ al., [2022](#bib.bib47)) translates neural networks to pyomo optimization
    blocks, including various MILP formulations and activation functions. Finally,
    recent developments in gurobipyâ´â´4https://github.com/Gurobi/gurobi-machinelearning
    enable directly parsing in trained neural networks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ç±»é—®é¢˜å¼€å‘äº†å‡ ç§è½¯ä»¶å·¥å…·ã€‚å¯¹äºçº¦æŸå­¦ä¹ çš„æƒ…å†µï¼ŒJANOSï¼ˆBergman ç­‰ï¼Œ[2022](#bib.bib24)ï¼‰å’Œ OptiCLï¼ˆMaragno
    ç­‰ï¼Œ[2021](#bib.bib210)ï¼‰éƒ½æä¾›äº†å­¦ä¹  ReLU ç¥ç»ç½‘ç»œä»¥åŸºäºæ•°æ®è¿‘ä¼¼çº¦æŸçš„åŠŸèƒ½ï¼Œå¹¶å°†å­¦ä¹ åˆ°çš„ç¥ç»ç½‘ç»œåµŒå…¥åˆ° MILP ä¸­ã€‚reluMIP
    åŒ…ï¼ˆLueg ç­‰ï¼Œ[2021](#bib.bib203)ï¼‰ä¹Ÿè¢«å¼•å…¥ä»¥å¤„ç†åè€…çš„åµŒå…¥æ­¥éª¤ã€‚æ›´ä¸€èˆ¬æ¥è¯´ï¼ŒOMLTï¼ˆCeccon ç­‰ï¼Œ[2022](#bib.bib47)ï¼‰å°†ç¥ç»ç½‘ç»œè½¬åŒ–ä¸º
    pyomo ä¼˜åŒ–å—ï¼ŒåŒ…æ‹¬å„ç§ MILP è¡¨è¿°å’Œæ¿€æ´»å‡½æ•°ã€‚æœ€åï¼Œgurobipy çš„æœ€æ–°å‘å±•â´â´4https://github.com/Gurobi/gurobi-machinelearning
    ä½¿å¾—å¯ä»¥ç›´æ¥è§£æè®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œã€‚
- en: Applications of this paradigm can be envisioned in a number of domain areas.
    This approach is common in deep reinforcement learning, where neural networks
    are used to approximate an unknown â€œ$Q$-functionâ€ which models the long-term cost
    of taking a particular action in a particular state of the world. In $Q$-learning,
    this $Q$-function is optimized iteratively to produce new candidate policies,
    which are then evaluated (typically via simulation) to produce new training data
    for future iterations. Optimization over the learned $Q$-function must be relatively
    fast in control applications, and several practical methods have been proposed.
    When the action space is discrete, the $Q$-function neural network is trained
    with one output value for each possible action, simplifying optimization to evaluating
    the model and selecting the largest output. Continuous action spaces require the
    $Q$ network be optimized over (Burtea and Tsay, [2023](#bib.bib45), Delarue etÂ al.,
    [2020](#bib.bib78), Ryu etÂ al., [2020](#bib.bib267)), or an â€œactorâ€ network can
    be trained to learn the optimal actions (Lillicrap etÂ al., [2015](#bib.bib193)).
    In a related vein, ReLU neural networks can be used as a process model for optimal
    scheduling or control (Wu etÂ al., [2020](#bib.bib338)).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥åœ¨å¤šä¸ªé¢†åŸŸä¸­è®¾æƒ³è¿™ç§èŒƒå¼çš„åº”ç”¨ã€‚è¿™ç§æ–¹æ³•åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­å¾ˆå¸¸è§ï¼Œåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œç¥ç»ç½‘ç»œç”¨äºè¿‘ä¼¼æœªçŸ¥çš„â€œ$Q$-å‡½æ•°â€ï¼Œè¯¥å‡½æ•°æ¨¡å‹æè¿°äº†åœ¨ç‰¹å®šä¸–ç•ŒçŠ¶æ€ä¸‹é‡‡å–ç‰¹å®šè¡ŒåŠ¨çš„é•¿æœŸæˆæœ¬ã€‚åœ¨
    $Q$-å­¦ä¹ ä¸­ï¼Œè¿™ä¸ª $Q$-å‡½æ•°è¢«è¿­ä»£ä¼˜åŒ–ï¼Œä»¥äº§ç”Ÿæ–°çš„å€™é€‰ç­–ç•¥ï¼Œç„¶åé€šè¿‡æ¨¡æ‹Ÿç­‰æ–¹å¼è¿›è¡Œè¯„ä¼°ï¼Œä»è€Œç”Ÿæˆæœªæ¥è¿­ä»£çš„æ–°çš„è®­ç»ƒæ•°æ®ã€‚åœ¨æ§åˆ¶åº”ç”¨ä¸­ï¼Œå¯¹å­¦ä¹ åˆ°çš„ $Q$-å‡½æ•°çš„ä¼˜åŒ–å¿…é¡»ç›¸å¯¹å¿«é€Ÿï¼Œå·²ç»æå‡ºäº†å‡ ç§å®é™…æ–¹æ³•ã€‚å½“åŠ¨ä½œç©ºé—´æ˜¯ç¦»æ•£çš„æ—¶ï¼Œ$Q$-å‡½æ•°ç¥ç»ç½‘ç»œä¸ºæ¯ä¸ªå¯èƒ½çš„åŠ¨ä½œè®­ç»ƒä¸€ä¸ªè¾“å‡ºå€¼ï¼Œè¿™ç®€åŒ–äº†ä¼˜åŒ–ä¸ºè¯„ä¼°æ¨¡å‹å¹¶é€‰æ‹©æœ€å¤§çš„è¾“å‡ºã€‚è¿ç»­åŠ¨ä½œç©ºé—´éœ€è¦å¯¹
    $Q$ ç½‘ç»œè¿›è¡Œä¼˜åŒ–ï¼ˆBurtea å’Œ Tsayï¼Œ[2023](#bib.bib45)ï¼ŒDelarue ç­‰ï¼Œ[2020](#bib.bib78)ï¼ŒRyu ç­‰ï¼Œ[2020](#bib.bib267)ï¼‰ï¼Œæˆ–è€…å¯ä»¥è®­ç»ƒä¸€ä¸ªâ€œæ¼”å‘˜â€ç½‘ç»œæ¥å­¦ä¹ æœ€ä½³åŠ¨ä½œï¼ˆLillicrap
    ç­‰ï¼Œ[2015](#bib.bib193)ï¼‰ã€‚åœ¨ç›¸å…³é¢†åŸŸï¼ŒReLU ç¥ç»ç½‘ç»œå¯ä»¥ä½œä¸ºæœ€ä¼˜è°ƒåº¦æˆ–æ§åˆ¶çš„è¿‡ç¨‹æ¨¡å‹ï¼ˆWu ç­‰ï¼Œ[2020](#bib.bib338)ï¼‰ã€‚
- en: Chemical engineering also presents applications where surrogate models have
    proven beneficial for optimization, as is the subject of recent reviews (Bhosekar
    and Ierapetritou, [2018](#bib.bib28), McBride and Sundmacher, [2019](#bib.bib216),
    Tsay and Baldea, [2019](#bib.bib311)). In particular, ReLU neural networks can
    be seamlessly embedded in larger MILP problems such as flow networks and reservoir
    control where the other constraints are also mixed-integer linear (Grimstad and
    Andersson, [2019](#bib.bib133), Say etÂ al., [2017](#bib.bib272), Yang etÂ al.,
    [2022](#bib.bib347)). Focusing on control applications where the neural network
    is embedded in a MILP that must be solved repeatedly, Katz etÂ al. ([2020](#bib.bib171))
    showed how multiparametric programming can be used to learn the solution map of
    the resulting MILP itself, which is also piecewise affine. An emerging area of
    research uses verification tools to reason about neural networks used as controllers,
    e.g., see Johnson etÂ al. ([2020](#bib.bib165)). These applications involve optimization
    formulations combining the neural network with constraints defining the controlled
    system. For example, verification can be used to bound the reachable set (Sidrane
    etÂ al., [2022](#bib.bib286)) (alongside piecewise linear bounds on the dynamical
    system) or the maximum error against a baseline controller (Schwan etÂ al., [2022](#bib.bib275)).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ–å·¥å·¥ç¨‹ä¹Ÿå±•ç°äº†åœ¨ä¼˜åŒ–ä¸­æ›¿ä»£æ¨¡å‹çš„æœ‰ç›Šåº”ç”¨ï¼Œè¿™ä¹Ÿæ˜¯æœ€è¿‘è¯„è®ºçš„ä¸»é¢˜ï¼ˆBhosekar å’Œ Ierapetritouï¼Œ[2018](#bib.bib28)ï¼ŒMcBride
    å’Œ Sundmacherï¼Œ[2019](#bib.bib216)ï¼ŒTsay å’Œ Baldeaï¼Œ[2019](#bib.bib311)ï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼ŒReLUç¥ç»ç½‘ç»œå¯ä»¥æ— ç¼åœ°åµŒå…¥åˆ°æ›´å¤§çš„MILPé—®é¢˜ä¸­ï¼Œå¦‚æµç½‘ç»œå’Œå‚¨ç½æ§åˆ¶ï¼Œå…¶ä¸­å…¶ä»–çº¦æŸä¹Ÿæ˜¯æ··åˆæ•´æ•°çº¿æ€§çš„ï¼ˆGrimstad
    å’Œ Anderssonï¼Œ[2019](#bib.bib133)ï¼ŒSay ç­‰ï¼Œ[2017](#bib.bib272)ï¼ŒYang ç­‰ï¼Œ[2022](#bib.bib347)ï¼‰ã€‚ä¸“æ³¨äºæ§åˆ¶åº”ç”¨ï¼Œå…¶ä¸­ç¥ç»ç½‘ç»œåµŒå…¥åœ¨å¿…é¡»åå¤æ±‚è§£çš„MILPä¸­ï¼ŒKatz
    ç­‰ï¼ˆ[2020](#bib.bib171)ï¼‰å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¤šå‚æ•°ç¼–ç¨‹æ¥å­¦ä¹ ç»“æœMILPæœ¬èº«çš„è§£æ˜ å°„ï¼Œè¯¥æ˜ å°„ä¹Ÿæ˜¯åˆ†æ®µçº¿æ€§çš„ã€‚ä¸€ä¸ªæ–°å…´çš„ç ”ç©¶é¢†åŸŸä½¿ç”¨éªŒè¯å·¥å…·æ¥æ¨ç†ä½œä¸ºæ§åˆ¶å™¨ä½¿ç”¨çš„ç¥ç»ç½‘ç»œï¼Œä¾‹å¦‚ï¼Œè§
    Johnson ç­‰ï¼ˆ[2020](#bib.bib165)ï¼‰ã€‚è¿™äº›åº”ç”¨æ¶‰åŠä¼˜åŒ–å…¬å¼ï¼Œå°†ç¥ç»ç½‘ç»œä¸å®šä¹‰è¢«æ§ç³»ç»Ÿçš„çº¦æŸç»“åˆåœ¨ä¸€èµ·ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨éªŒè¯æ¥ç•Œå®šå¯è¾¾é›†ï¼ˆSidrane
    ç­‰ï¼Œ[2022](#bib.bib286)ï¼‰ï¼ˆä»¥åŠå¯¹åŠ¨æ€ç³»ç»Ÿçš„åˆ†æ®µçº¿æ€§ç•Œé™ï¼‰æˆ–ä¸åŸºçº¿æ§åˆ¶å™¨çš„æœ€å¤§è¯¯å·®ï¼ˆSchwan ç­‰ï¼Œ[2022](#bib.bib275)ï¼‰ã€‚
- en: Finally, applications for optimization over neural networks arise in machine
    learning applications. MILP formulations can be used to compress neural networks
    (Serra etÂ al., [2020](#bib.bib283), [2021](#bib.bib284), ElAraby etÂ al., [2020](#bib.bib88)),
    which consequently result in more tractable surrogate models (Kody etÂ al., [2022](#bib.bib176)).
    The main idea is to use MILP to identify stable nodes, i.e., nodes that are always
    on or off over an input domain, which can then be algebraically eliminated. Optimization
    has also been employed in techniques for feature selection, based on identifying
    strongest input nodes (Sildir and Aydin, [2022](#bib.bib287), Zhao etÂ al., [2023](#bib.bib360)).
    In the context of Bayesian optimization, Volpp etÂ al. ([2020](#bib.bib323)) use
    reinforcement learning to meta-learn acquisition functions parameterized as neural
    networks; selecting ensuing query points then requires optimization over the trained
    acquisition function. Later work modeled both the acquisition function and feasible
    region in black-box optimization as neural networks (Papalexopoulos etÂ al., [2022](#bib.bib239)).
    In that work, exploration and exploitation are balanced via Thompson sampling
    and training multiple neural networks from a random parameter initialization.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç¥ç»ç½‘ç»œä¼˜åŒ–çš„åº”ç”¨åœ¨æœºå™¨å­¦ä¹ åº”ç”¨ä¸­å‡ºç°ã€‚MILPï¼ˆæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ï¼‰å…¬å¼å¯ä»¥ç”¨æ¥å‹ç¼©ç¥ç»ç½‘ç»œï¼ˆSerra ç­‰ï¼Œ[2020](#bib.bib283)ï¼Œ[2021](#bib.bib284)ï¼ŒElAraby
    ç­‰ï¼Œ[2020](#bib.bib88)ï¼‰ï¼Œä»è€Œäº§ç”Ÿæ›´æ˜“å¤„ç†çš„æ›¿ä»£æ¨¡å‹ï¼ˆKody ç­‰ï¼Œ[2022](#bib.bib176)ï¼‰ã€‚ä¸»è¦æ€æƒ³æ˜¯ä½¿ç”¨MILPæ¥è¯†åˆ«ç¨³å®šçš„èŠ‚ç‚¹ï¼Œå³åœ¨è¾“å…¥åŸŸä¸­å§‹ç»ˆå¼€æˆ–å…³çš„èŠ‚ç‚¹ï¼Œè¿™äº›èŠ‚ç‚¹å¯ä»¥é€šè¿‡ä»£æ•°æ–¹å¼æ¶ˆé™¤ã€‚ä¼˜åŒ–æŠ€æœ¯è¿˜è¢«ç”¨äºç‰¹å¾é€‰æ‹©ï¼ŒåŸºäºè¯†åˆ«æœ€å¼ºçš„è¾“å…¥èŠ‚ç‚¹ï¼ˆSildir
    å’Œ Aydinï¼Œ[2022](#bib.bib287)ï¼ŒZhao ç­‰ï¼Œ[2023](#bib.bib360)ï¼‰ã€‚åœ¨è´å¶æ–¯ä¼˜åŒ–çš„èƒŒæ™¯ä¸‹ï¼ŒVolpp ç­‰ï¼ˆ[2020](#bib.bib323)ï¼‰ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥å…ƒå­¦ä¹ ä½œä¸ºç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„è·å–å‡½æ•°ï¼›é€‰æ‹©åç»­æŸ¥è¯¢ç‚¹åˆ™éœ€è¦å¯¹è®­ç»ƒå¥½çš„è·å–å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚åç»­çš„å·¥ä½œå°†è·å–å‡½æ•°å’Œé»‘ç®±ä¼˜åŒ–ä¸­çš„å¯è¡ŒåŒºåŸŸå»ºæ¨¡ä¸ºç¥ç»ç½‘ç»œï¼ˆPapalexopoulos
    ç­‰ï¼Œ[2022](#bib.bib239)ï¼‰ã€‚åœ¨è¯¥å·¥ä½œä¸­ï¼Œé€šè¿‡æ±¤æ™®æ£®é‡‡æ ·å’Œå¹³è¡¡å¤šä¸ªä»éšæœºå‚æ•°åˆå§‹åŒ–è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚
- en: A word of caution
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„æ˜¯
- en: Standard supervised learning algorithms aim to learn a function which fits the
    underlying function according to some distribution under which the data is generated.
    However, optimizing a function corresponds to evaluating it at a single point.
    This means that you may end up with a model that well-approximates the underlying
    function in distribution, but for which the pointwise minimizer is a poor approximation
    of the true function. This phenomena is referred to as the â€œOptimizerâ€™s curseâ€
    (Smith and Winkler, [2006](#bib.bib293)).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†çš„ç›‘ç£å­¦ä¹ ç®—æ³•æ—¨åœ¨å­¦ä¹ ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®ç”Ÿæˆæ•°æ®çš„æŸäº›åˆ†å¸ƒæ‹ŸåˆåŸºç¡€å‡½æ•°ã€‚ç„¶è€Œï¼Œä¼˜åŒ–ä¸€ä¸ªå‡½æ•°å¯¹åº”äºåœ¨ä¸€ä¸ªç‚¹ä¸Šè¯„ä¼°å®ƒã€‚è¿™æ„å‘³ç€ä½ å¯èƒ½ä¼šå¾—åˆ°ä¸€ä¸ªåœ¨åˆ†å¸ƒä¸Šå¾ˆå¥½åœ°é€¼è¿‘åŸºç¡€å‡½æ•°çš„æ¨¡å‹ï¼Œä½†å…¶é€ç‚¹æœ€å°åŒ–å™¨å¯¹çœŸå®å‡½æ•°çš„é€¼è¿‘å´è¾ƒå·®ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œä¼˜åŒ–å™¨çš„è¯…å’’â€ï¼ˆSmith
    and Winkler, [2006](#bib.bib293)ï¼‰ã€‚
- en: 4.1.3 Single neuron relaxations
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 å•ç¥ç»å…ƒæ¾å¼›
- en: 'For the following subsections, consider the $i$-th neuron in the $l$-th layer
    of a neural network, endowed with a ReLU activation function, whose behavior is
    governed by ([2](#S1.E2 "In 1.1 What neural networks can model â€£ 1 Introduction
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). Presume that a input
    domain of interest $\mathcal{D}^{l-1}\subset\mathbb{R}^{n_{l}}$ is a bounded region.
    Further, since $\mathcal{D}^{l-1}$ is bounded, presume that finite bounds are
    known on each input component, i.e. that vectors $L^{l-1},U^{l-1}\in\mathbb{R}^{n_{l}}$
    are known such that $\mathcal{D}^{l-1}\subseteq[L^{l-1},U^{l-1}]\subset\mathbb{R}^{n_{l}}$.
    We can then write the *graph* of the neuron, which couples together the input
    and the output of the nonlinear ReLU activation function:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä»¥ä¸‹å°èŠ‚ï¼Œè€ƒè™‘ç¥ç»ç½‘ç»œç¬¬$l$å±‚çš„ç¬¬$i$ä¸ªç¥ç»å…ƒï¼Œé…å¤‡æœ‰ReLUæ¿€æ´»å‡½æ•°ï¼Œå…¶è¡Œä¸ºç”±([2](#S1.E2 "åœ¨1.1 ç¥ç»ç½‘ç»œå¯ä»¥å»ºæ¨¡çš„å†…å®¹ â€£
    1 å¼•è¨€ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))æ‰€æ§åˆ¶ã€‚å‡è®¾æ„Ÿå…´è¶£çš„è¾“å…¥åŸŸ$\mathcal{D}^{l-1}\subset\mathbb{R}^{n_{l}}$æ˜¯ä¸€ä¸ªæœ‰ç•ŒåŒºåŸŸã€‚æ­¤å¤–ï¼Œç”±äº$\mathcal{D}^{l-1}$æ˜¯æœ‰ç•Œçš„ï¼Œå‡è®¾å·²çŸ¥æ¯ä¸ªè¾“å…¥åˆ†é‡çš„æœ‰é™ç•Œï¼Œå³å·²çŸ¥å‘é‡$L^{l-1},U^{l-1}\in\mathbb{R}^{n_{l}}$ï¼Œä½¿å¾—$\mathcal{D}^{l-1}\subseteq[L^{l-1},U^{l-1}]\subset\mathbb{R}^{n_{l}}$ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºç¥ç»å…ƒçš„*å›¾*ï¼Œå®ƒå°†éçº¿æ€§ReLUæ¿€æ´»å‡½æ•°çš„è¾“å…¥å’Œè¾“å‡ºç»“åˆåœ¨ä¸€èµ·ï¼š
- en: '|  | $\displaystyle\texttt{gr}=$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}=0\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}}$
    |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\texttt{gr}=$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}=0\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}}$
    |  |'
- en: '|  | $\displaystyle\cup$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}={\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\geq
    0}.$ |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\cup$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}={\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\geq
    0}.$ |  |'
- en: This is a disjunctive representation for gr in terms of two polyhedral alternatives.
    We assume that every included neuron exhibits this disjunction, i.e., every neuron
    can be on or off depending on the model input. This assumption of *strict activity*
    implies that $L^{l-1}<0$ and $U^{l-1}>0$, noting that neurons not satisfying this
    property can be exactly pruned from the model (Serra etÂ al., [2020](#bib.bib283)).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¯¹$gr$çš„ç¦»æ•£è¡¨ç¤ºï¼Œä»¥ä¸¤ç§å¤šé¢ä½“æ›¿ä»£å½¢å¼è¿›è¡Œè¡¨ç¤ºã€‚æˆ‘ä»¬å‡è®¾æ¯ä¸ªåŒ…å«çš„ç¥ç»å…ƒéƒ½è¡¨ç°å‡ºè¿™ç§ç¦»æ•£ï¼Œå³æ¯ä¸ªç¥ç»å…ƒå¯ä»¥æ ¹æ®æ¨¡å‹è¾“å…¥å¼€æˆ–å…³ã€‚è¿™ç§*ä¸¥æ ¼æ´»åŠ¨*çš„å‡è®¾æ„å‘³ç€$L^{l-1}<0$å’Œ$U^{l-1}>0$ï¼Œæ³¨æ„åˆ°ä¸æ»¡è¶³æ­¤å±æ€§çš„ç¥ç»å…ƒå¯ä»¥ä»æ¨¡å‹ä¸­ç²¾ç¡®ä¿®å‰ªæ‰ï¼ˆSerra
    et al., [2020](#bib.bib283)ï¼‰ã€‚
- en: We observe that, given this (or any) formulation for each individual unit, it
    is straightforward to construct a formulation for the entire network. For example,
    if we take $X^{l}_{i}=\Set{({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})}{\eqref{eqn:relu-big-m}}$
    for each layer $l$ and each unit $i$, we can construct a MILP formulation for
    the graph of the entire network, $\Set{(x,f(x)):x\in\mathcal{D}^{0}}$ as
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œç»™å®šè¿™ç§ï¼ˆæˆ–ä»»ä½•ï¼‰æ¯ä¸ªå•å…ƒçš„è¡¨è¿°ï¼Œæ„é€ æ•´ä¸ªç½‘ç»œçš„è¡¨è¿°æ˜¯ç®€å•çš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä¸ºæ¯å±‚$l$å’Œæ¯ä¸ªå•å…ƒ$i$å–$X^{l}_{i}=\Set{({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})}{\eqref{eqn:relu-big-m}}$ï¼Œæˆ‘ä»¬å¯ä»¥æ„é€ æ•´ä¸ªç½‘ç»œçš„MILPè¡¨è¿°ï¼Œ$\Set{(x,f(x)):x\in\mathcal{D}^{0}}$ã€‚
- en: '|  | $({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})\in X^{l}_{i}\quad\forall l\in{\mathbb{L}},i\in\llbracket
    n_{l}\rrbracket.$ |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})\in X^{l}_{i}\quad\forall l\in{\mathbb{L}},i\in\llbracket
    n_{l}\rrbracket.$ |  |'
- en: This also generalizes in a straightforward manner to more complex feedforward
    network architectures (e.g. convolutions, or sparse or skip connections), though
    we omit the explicit description for notational simplicity.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿå¯ä»¥ç®€å•åœ°æ¨å¹¿åˆ°æ›´å¤æ‚çš„å‰é¦ˆç½‘ç»œæ¶æ„ï¼ˆä¾‹å¦‚å·ç§¯ï¼Œæˆ–ç¨€ç–æˆ–è·³è·ƒè¿æ¥ï¼‰ï¼Œå°½ç®¡ä¸ºäº†ç¬¦å·çš„ç®€æ´æ€§ï¼Œæˆ‘ä»¬çœç•¥äº†æ˜ç¡®çš„æè¿°ã€‚
- en: 4.1.4 Beyond the scope of this survey
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 è¶…å‡ºæœ¬è°ƒæŸ¥çš„èŒƒå›´
- en: 'The effectiveness of the single-neuron formulations described above is bounded
    by the tightness of the optimal univariate formulation; this property is known
    as the â€œsingle-neuron barrierâ€ (Salman etÂ al., [2019](#bib.bib269)). This has
    motivated research in convex relaxations that jointly account for multiple neurons
    within a layer (Singh etÂ al., [2019a](#bib.bib290)). Nevertheless, the analysis
    of polyhedral formulations for multiple neurons simultaneously quickly becomes
    intractable, and is beyond the scope of this survey. Instead, we point the interested
    reader to the recent survey by Roth ([2021](#bib.bib263)), and highlight a few
    approaches taken in the literature. Multi-neuron analysis has been used to: improve
    bounds tightening schemes (RÃ¶ssig and Petkovic, [2021](#bib.bib262)), prune linearizable
    neurons (Botoeva etÂ al., [2020](#bib.bib37)), design dual decompositions (Ferrari
    etÂ al., [2022](#bib.bib104)), and generate strengthening inequalities (Serra and
    Ramalingam, [2020](#bib.bib281)). Similarly, we do not review formulations for
    ensembles of ReLU networks, though MILP formulations have been proposed (Wang
    etÂ al., [2021](#bib.bib324), [2023](#bib.bib325)).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å•ç¥ç»å…ƒå…¬å¼çš„æœ‰æ•ˆæ€§å—åˆ°æœ€ä¼˜å•å˜é‡å…¬å¼ç´§å¯†æ€§çš„é™åˆ¶ï¼›è¿™ä¸€å±æ€§è¢«ç§°ä¸ºâ€œå•ç¥ç»å…ƒéšœç¢â€ï¼ˆSalmanç­‰ï¼Œ[2019](#bib.bib269)ï¼‰ã€‚è¿™ä¿ƒä½¿äº†åœ¨å‡¸æ¾å¼›ä¸­è¿›è¡Œç ”ç©¶ï¼Œè”åˆè€ƒè™‘å±‚å†…çš„å¤šä¸ªç¥ç»å…ƒï¼ˆSinghç­‰ï¼Œ[2019a](#bib.bib290)ï¼‰ã€‚ç„¶è€Œï¼Œå¯¹å¤šä¸ªç¥ç»å…ƒçš„å¤šé¢ä½“å…¬å¼è¿›è¡Œåˆ†æè¿…é€Ÿå˜å¾—ä¸å¯å¤„ç†ï¼Œè¶…å‡ºäº†æœ¬æ¬¡è°ƒæŸ¥çš„èŒƒå›´ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†æ„Ÿå…´è¶£çš„è¯»è€…å¼•å¯¼åˆ°Rothï¼ˆ[2021](#bib.bib263)ï¼‰çš„æœ€æ–°è°ƒæŸ¥ï¼Œå¹¶å¼ºè°ƒæ–‡çŒ®ä¸­é‡‡å–çš„ä¸€äº›æ–¹æ³•ã€‚å¤šç¥ç»å…ƒåˆ†æå·²è¢«ç”¨äºï¼šæ”¹è¿›è¾¹ç•Œç´§ç¼©æ–¹æ¡ˆï¼ˆRÃ¶ssigå’ŒPetkovicï¼Œ[2021](#bib.bib262)ï¼‰ï¼Œä¿®å‰ªçº¿æ€§åŒ–ç¥ç»å…ƒï¼ˆBotoevaç­‰ï¼Œ[2020](#bib.bib37)ï¼‰ï¼Œè®¾è®¡å¯¹å¶åˆ†è§£ï¼ˆFerrariç­‰ï¼Œ[2022](#bib.bib104)ï¼‰ï¼Œä»¥åŠç”Ÿæˆå¼ºåŒ–ä¸ç­‰å¼ï¼ˆSerraå’ŒRamalingamï¼Œ[2020](#bib.bib281)ï¼‰ã€‚åŒæ ·ï¼Œæˆ‘ä»¬ä¸å¯¹ReLUç½‘ç»œé›†åˆçš„å…¬å¼è¿›è¡Œå›é¡¾ï¼Œå°½ç®¡å·²æå‡ºäº†MILPå…¬å¼ï¼ˆWangç­‰ï¼Œ[2021](#bib.bib324)ï¼Œ[2023](#bib.bib325)ï¼‰ã€‚
- en: Additionally, recent works have exploited polyhedral structure to develop sampling
    based strategies, which can be used to warm-start MILP or accelerate local search
    in verification (Perakis and Tsiourvas, [2022](#bib.bib245), Wu etÂ al., [2022](#bib.bib339)).
    Lombardi etÂ al. ([2017](#bib.bib199)) computationally compare MILP against local
    search and constraint programming approaches. In a related vein, Cheon ([2022](#bib.bib58))
    examines local solutions and proposes an outer approximation method to improve
    gradient-based optimization. Finally, following Raghunathan etÂ al. ([2018](#bib.bib254)),
    a large body of work has presented optimization-based methods for verification
    that use semidefinite programming concepts (Dathathri etÂ al., [2020](#bib.bib75),
    Fazlyab etÂ al., [2020](#bib.bib102), Newton and Papachristodoulou, [2021](#bib.bib232)).
    Notably, Batten etÂ al. ([2021](#bib.bib17)) showed how combining semidefinite
    and MILP formulations can produce a new formulation that is tighter than both.
    This was later extended with reformulation-linearization technique, or RLT, cuts
    (Lan etÂ al., [2022](#bib.bib183)). While related to linear programming and other
    methods based on convex relaxations, this stream of work is beyond the scope of
    this survey. We refer the reader to Zhang ([2020](#bib.bib358)) for a discussion
    of the tightness of these formulations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨äº†å¤šé¢ä½“ç»“æ„æ¥å¼€å‘åŸºäºé‡‡æ ·çš„ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥ç”¨äºä¸ºMILPæä¾›çƒ­å¯åŠ¨æˆ–åŠ é€ŸéªŒè¯ä¸­çš„å±€éƒ¨æœç´¢ï¼ˆPerakiså’ŒTsiourvasï¼Œ[2022](#bib.bib245)ï¼ŒWuç­‰ï¼Œ[2022](#bib.bib339)ï¼‰ã€‚Lombardiç­‰äººï¼ˆ[2017](#bib.bib199)ï¼‰å¯¹MILPä¸å±€éƒ¨æœç´¢å’Œçº¦æŸç¼–ç¨‹æ–¹æ³•è¿›è¡Œäº†è®¡ç®—æ¯”è¾ƒã€‚åœ¨ç›¸å…³çš„ç ”ç©¶ä¸­ï¼ŒCheonï¼ˆ[2022](#bib.bib58)ï¼‰è€ƒå¯Ÿäº†å±€éƒ¨è§£å†³æ–¹æ¡ˆï¼Œå¹¶æå‡ºäº†ä¸€ç§å¤–éƒ¨é€¼è¿‘æ–¹æ³•æ¥æ”¹è¿›åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ã€‚æœ€åï¼Œç»§Raghunathanç­‰äººï¼ˆ[2018](#bib.bib254)ï¼‰ä¹‹åï¼Œå¤§é‡å·¥ä½œæå‡ºäº†ç”¨äºéªŒè¯çš„åŸºäºä¼˜åŒ–çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä½¿ç”¨äº†åŠæ­£å®šè§„åˆ’æ¦‚å¿µï¼ˆDathathriç­‰ï¼Œ[2020](#bib.bib75)ï¼ŒFazlyabç­‰ï¼Œ[2020](#bib.bib102)ï¼ŒNewtonå’ŒPapachristodoulouï¼Œ[2021](#bib.bib232)ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒBattenç­‰äººï¼ˆ[2021](#bib.bib17)ï¼‰å±•ç¤ºäº†å¦‚ä½•å°†åŠæ­£å®šå’ŒMILPå…¬å¼ç»“åˆèµ·æ¥ï¼Œä»è€Œäº§ç”Ÿæ¯”ä¸¤è€…éƒ½æ›´ç´§çš„æ–°çš„å…¬å¼ã€‚ä¹‹åï¼Œè¿™ä¸€æ–¹æ³•é€šè¿‡é‡æ„-çº¿æ€§åŒ–æŠ€æœ¯ï¼ˆRLTï¼‰å¾—åˆ°äº†æ‰©å±•ï¼ˆLanç­‰ï¼Œ[2022](#bib.bib183)ï¼‰ã€‚è™½ç„¶ä¸çº¿æ€§è§„åˆ’åŠå…¶ä»–åŸºäºå‡¸æ¾å¼›çš„æ–¹æ³•æœ‰å…³ï¼Œä½†è¿™ä¸€ç ”ç©¶æ–¹å‘è¶…å‡ºäº†æœ¬æ¬¡è°ƒæŸ¥çš„èŒƒå›´ã€‚æˆ‘ä»¬å»ºè®®è¯»è€…å‚è€ƒZhangï¼ˆ[2020](#bib.bib358)ï¼‰è®¨è®ºè¿™äº›å…¬å¼çš„ç´§å¯†æ€§ã€‚
- en: 4.2 Exact models using mixed-integer programming
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹
- en: Mixed-integer programming offers a powerful algorithmic framework for *exactly*
    modeling nonconvex piecewise linear functions. The Operations Research community
    has studied has a long and storied history of developing MILP-based methods for
    piecewise linear optimization, with research spanning decades (Croxton etÂ al.,
    [2003](#bib.bib69), Dantzig, [1960](#bib.bib73), GeiÃŸler etÂ al., [2012](#bib.bib117),
    Huchette and Vielma, [2022](#bib.bib158), Lee and Wilson, [2001](#bib.bib189),
    Misener and Floudas, [2012](#bib.bib221), Padberg, [2000](#bib.bib238), Vielma
    etÂ al., [2010](#bib.bib318)). However, many of these techniques are specialized
    for low-dimensional or separable piecewise linear functions. While a reasonable
    assumption in many OR problems, this is not the case when modeling neurons in
    a neural network. Therefore, the standard approach in the literature is to apply
    general-purpose MILP formulation techniques to model neural networks.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆæ•´æ•°è§„åˆ’æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„ç®—æ³•æ¡†æ¶ï¼Œç”¨äº*å‡†ç¡®åœ°*å»ºæ¨¡éå‡¸åˆ†æ®µçº¿æ€§å‡½æ•°ã€‚è¿ç­¹å­¦ç•Œåœ¨å¼€å‘åŸºäºMILPçš„åˆ†æ®µçº¿æ€§ä¼˜åŒ–æ–¹æ³•æ–¹é¢æœ‰ç€æ‚ ä¹…çš„å†å²ï¼Œç ”ç©¶å†æ—¶å‡ åå¹´ï¼ˆCroxton
    ç­‰ï¼Œ[2003](#bib.bib69)ï¼ŒDantzigï¼Œ[1960](#bib.bib73)ï¼ŒGeiÃŸler ç­‰ï¼Œ[2012](#bib.bib117)ï¼ŒHuchette
    å’Œ Vielmaï¼Œ[2022](#bib.bib158)ï¼ŒLee å’Œ Wilsonï¼Œ[2001](#bib.bib189)ï¼ŒMisener å’Œ Floudasï¼Œ[2012](#bib.bib221)ï¼ŒPadbergï¼Œ[2000](#bib.bib238)ï¼ŒVielma
    ç­‰ï¼Œ[2010](#bib.bib318)ï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯ä¸­çš„è®¸å¤šéƒ½æ˜¯ä¸“é—¨é’ˆå¯¹ä½ç»´æˆ–å¯åˆ†ç¦»çš„åˆ†æ®µçº¿æ€§å‡½æ•°ã€‚è™½ç„¶è¿™æ˜¯è®¸å¤šè¿ç­¹å­¦é—®é¢˜ä¸­çš„åˆç†å‡è®¾ï¼Œä½†åœ¨å»ºæ¨¡ç¥ç»ç½‘ç»œä¸­çš„ç¥ç»å…ƒæ—¶æƒ…å†µå¹¶éå¦‚æ­¤ã€‚å› æ­¤ï¼Œæ–‡çŒ®ä¸­çš„æ ‡å‡†æ–¹æ³•æ˜¯åº”ç”¨é€šç”¨MILPå…¬å¼åŒ–æŠ€æœ¯æ¥å»ºæ¨¡ç¥ç»ç½‘ç»œã€‚
- en: Connection to Boolean satisfiability
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å¸ƒå°”å¯æ»¡è¶³æ€§å…³è”
- en: Some SMT-based methods such as Reluplex (Katz etÂ al., [2017](#bib.bib169)) and
    Planet (Ehlers, [2017](#bib.bib87)) effectively construct branching technologies
    similar to MILP solvers. Indeed, Marabou (Katz etÂ al., [2019](#bib.bib170)) builds
    on Reluplex, and a recent extension MarabouOpt can optimize over trained neural
    networks (Strong etÂ al., [2021](#bib.bib295)). The authors also outline general
    procedures to extend verification solvers to optimization. Our focus in this review
    is on more general MILP formulations, or those that can be incorporated into off-the-shelf
    MILP solvers with relative ease. Bunel etÂ al. ([2020b](#bib.bib42), [2018](#bib.bib43))
    provide a more comprehensive discussion of similarities and differences to SMT.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›åŸºäºSMTçš„æ–¹æ³•ï¼Œå¦‚ Reluplexï¼ˆKatz ç­‰ï¼Œ[2017](#bib.bib169)ï¼‰å’Œ Planetï¼ˆEhlersï¼Œ[2017](#bib.bib87)ï¼‰ï¼Œæœ‰æ•ˆåœ°æ„å»ºäº†ç±»ä¼¼äºMILPæ±‚è§£å™¨çš„åˆ†æ”¯æŠ€æœ¯ã€‚äº‹å®ä¸Šï¼ŒMarabouï¼ˆKatz
    ç­‰ï¼Œ[2019](#bib.bib170)ï¼‰åœ¨ Reluplex çš„åŸºç¡€ä¸Šæ„å»ºï¼Œè€Œæœ€è¿‘çš„æ‰©å±• MarabouOpt å¯ä»¥åœ¨è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œä¸Šè¿›è¡Œä¼˜åŒ–ï¼ˆStrong
    ç­‰ï¼Œ[2021](#bib.bib295)ï¼‰ã€‚ä½œè€…è¿˜æ¦‚è¿°äº†å°†éªŒè¯æ±‚è§£å™¨æ‰©å±•åˆ°ä¼˜åŒ–çš„ä¸€èˆ¬ç¨‹åºã€‚æˆ‘ä»¬åœ¨è¿™ç¯‡ç»¼è¿°ä¸­çš„é‡ç‚¹æ˜¯æ›´é€šç”¨çš„MILPå…¬å¼åŒ–ï¼Œæˆ–è€…é‚£äº›å¯ä»¥ç›¸å¯¹å®¹æ˜“åœ°å¹¶å…¥ç°æˆMILPæ±‚è§£å™¨ä¸­çš„å…¬å¼ã€‚Bunel
    ç­‰ï¼ˆ[2020b](#bib.bib42)ï¼Œ[2018](#bib.bib43)ï¼‰æä¾›äº†å¯¹SMTçš„ç›¸ä¼¼æ€§å’Œå·®å¼‚çš„æ›´å…¨é¢çš„è®¨è®ºã€‚
- en: 4.2.1 The big-$M$ formulation
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 å¤§-$M$ å…¬å¼åŒ–
- en: 'The big-$M$ method is a standard technique used to formulate logic and disjunctive
    constraints using mixed-integer programming (Bonami etÂ al., [2015](#bib.bib35),
    Vielma, [2015](#bib.bib316)). Big-$M$ formulations are typically very simple to
    reason about and implement, and are quite compact, though their convex relaxations
    can often be quite poor, leading to weak dual bounds and (often) slow convergence
    when passed to a mixed-integer programming solver. Since gr is a disjunctive set,
    the big-$M$ technique can be applied to produce the following formulation:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§-$M$ æ–¹æ³•æ˜¯ä¸€ç§æ ‡å‡†æŠ€æœ¯ï¼Œç”¨äºé€šè¿‡æ··åˆæ•´æ•°è§„åˆ’ï¼ˆBonami ç­‰ï¼Œ[2015](#bib.bib35)ï¼ŒVielmaï¼Œ[2015](#bib.bib316)ï¼‰æ¥å…¬å¼åŒ–é€»è¾‘å’Œæå–çº¦æŸã€‚å¤§-$M$
    å…¬å¼åŒ–é€šå¸¸éå¸¸ç®€å•æ˜“äºç†è§£å’Œå®ç°ï¼Œä¸”ç›¸å½“ç´§å‡‘ï¼Œä½†å®ƒä»¬çš„å‡¸æ¾å¼›é€šå¸¸è¾ƒå·®ï¼Œå¯¼è‡´å¯¹å¶ç•Œé™è¾ƒå¼±ï¼Œå¹¶ä¸”ï¼ˆé€šå¸¸ï¼‰åœ¨ä¼ é€’ç»™æ··åˆæ•´æ•°è§„åˆ’æ±‚è§£å™¨æ—¶æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ã€‚ç”±äº gr æ˜¯ä¸€ä¸ªæå–é›†åˆï¼Œå¤§-$M$
    æŠ€æœ¯å¯ä»¥åº”ç”¨äºäº§ç”Ÿä»¥ä¸‹å…¬å¼ï¼š
- en: '|  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (4a) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (4a) |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\left({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\right)-M^{l}_{i,-}(1-z)$
    |  | (4b) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\left({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\right)-M^{l}_{i,-}(1-z)$
    |  | (4b) |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq M^{l}_{i,+}z$ |  | (4c)
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq M^{l}_{i,+}z$ |  | (4c)
    |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}$ |  | (4d) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}$ |  | (4d) |'
- en: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\}.$ |  | (4e) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\}.$ |  | (4e) |'
- en: Here, $M^{l}_{i,-}$ and $M^{l}_{i,+}$ are data which must satisfy the inequalities
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ$M^{l}_{i,-}$ å’Œ $M^{l}_{i,+}$ æ˜¯éœ€è¦æ»¡è¶³ä¸ç­‰å¼çš„æ•°æ®ã€‚
- en: '|  | $\displaystyle M^{l}_{i,-}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle M^{l}_{i,-}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  |'
- en: '|  | $\displaystyle M^{l}_{i,+}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}.$
    |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle M^{l}_{i,+}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}.$
    |  |'
- en: This big-$M$ formulation for ReLU-based networks has been used extensively in
    the literature (Bunel etÂ al., [2018](#bib.bib43), Cheng etÂ al., [2017](#bib.bib56),
    Dutta etÂ al., [2018](#bib.bib83), Fischetti and Jo, [2018](#bib.bib106), Kumar
    etÂ al., [2019](#bib.bib181), Lomuscio and Maganti, [2017](#bib.bib200), Serra
    and Ramalingam, [2020](#bib.bib281), Serra etÂ al., [2018](#bib.bib282), Tjeng
    etÂ al., [2019](#bib.bib309), Xiao etÂ al., [2019](#bib.bib341)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§é’ˆå¯¹åŸºäº ReLU çš„ç½‘ç»œçš„ big-$M$ å½¢å¼åŒ–æ–¹æ³•åœ¨æ–‡çŒ®ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼ˆBunel et al., [2018](#bib.bib43), Cheng
    et al., [2017](#bib.bib56), Dutta et al., [2018](#bib.bib83), Fischetti å’Œ Jo,
    [2018](#bib.bib106), Kumar et al., [2019](#bib.bib181), Lomuscio å’Œ Maganti, [2017](#bib.bib200),
    Serra å’Œ Ramalingam, [2020](#bib.bib281), Serra et al., [2018](#bib.bib282), Tjeng
    et al., [2019](#bib.bib309), Xiao et al., [2019](#bib.bib341))ã€‚
- en: 'The big-$M$ formulation is compact, with one binary variable and $\mathcal{O}(1)$
    general inequality constraints for each neuron. Applied for each unit in the network,
    this leads to a MILP formulation with $\mathcal{O}(\sum_{l\in{\mathbb{L}}}n_{l})=\mathcal{O}(Ln_{\max})$
    binary variables and general inequality constraints, where $n_{\max}=\max_{l\in{\mathbb{L}}}n_{L}$.
    However, it has been observed (Anderson etÂ al., [2019](#bib.bib4), [2020](#bib.bib5))
    that this big-$M$ formulation is not strong in the sense that its LP relaxation
    does not, in general, capture the convex hull of the graph of a given unit; see
    FigureÂ [9](#S4.F9 "Figure 9 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using
    mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When
    Deep Learning Meets Polyhedral Theory: A Survey") for an illustration. In fact,
    this LP relaxation can be arbitrarily bad (Anderson etÂ al., [2019](#bib.bib4),
    Example 2), even in fixed input dimension. As MILP solvers often bound the objective
    function between the best feasible point and its tightest optimal continuous relaxation,
    a weak formulation can negatively impact performance, often substantially.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 'big-$M$ å½¢å¼åŒ–æ–¹æ³•æ˜¯ç´§å‡‘çš„ï¼Œæ¯ä¸ªç¥ç»å…ƒæœ‰ä¸€ä¸ªäºŒå…ƒå˜é‡å’Œ $\mathcal{O}(1)$ ä¸€èˆ¬ä¸ç­‰å¼çº¦æŸã€‚åº”ç”¨äºç½‘ç»œä¸­çš„æ¯ä¸ªå•å…ƒï¼Œè¿™å¯¼è‡´äº†ä¸€ä¸ªå…·æœ‰
    $\mathcal{O}(\sum_{l\in{\mathbb{L}}}n_{l})=\mathcal{O}(Ln_{\max})$ ä¸ªäºŒå…ƒå˜é‡å’Œä¸€èˆ¬ä¸ç­‰å¼çº¦æŸçš„
    MILP å½¢å¼ï¼Œå…¶ä¸­ $n_{\max}=\max_{l\in{\mathbb{L}}}n_{L}$ã€‚ç„¶è€Œï¼Œå·²ç»è§‚å¯Ÿåˆ°ï¼ˆAnderson et al., [2019](#bib.bib4),
    [2020](#bib.bib5)ï¼‰ï¼Œè¿™ç§ big-$M$ å½¢å¼åŒ–æ–¹æ³•åœ¨ LP æ¾å¼›æ–¹é¢å¹¶ä¸å¼ºï¼Œå› ä¸ºå®ƒé€šå¸¸æ— æ³•æ•æ‰åˆ°ç»™å®šå•å…ƒçš„å›¾çš„å‡¸åŒ…ï¼›è¯·å‚è§å›¾ [9](#S4.F9
    "Figure 9 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey") ä»¥è·å¾—è¯´æ˜ã€‚å®é™…ä¸Šï¼Œè¿™ç§ LP æ¾å¼›å¯èƒ½ä¼šéå¸¸ç³Ÿç³•ï¼ˆAnderson et al.,
    [2019](#bib.bib4), Example 2ï¼‰ï¼Œå³ä½¿åœ¨å›ºå®šè¾“å…¥ç»´åº¦ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç”±äº MILP æ±‚è§£å™¨é€šå¸¸åœ¨æœ€ä½³å¯è¡Œç‚¹å’Œå…¶æœ€ç´§å¯†çš„æœ€ä¼˜è¿ç»­æ¾å¼›ä¹‹é—´å¯¹ç›®æ ‡å‡½æ•°è¿›è¡Œç•Œå®šï¼Œå¼±å½¢å¼åŒ–æ–¹æ³•å¯èƒ½ä¼šå¯¹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œé€šå¸¸å½±å“å¾ˆå¤§ã€‚'
- en: 'It is worth dwelling on where this lack of strength comes from. If the input
    ${\bm{h}}^{l-1}$ is one dimensional, the big-$M$ formulation is *locally* ideal
    (Vielma, [2015](#bib.bib316)): the extreme points of the linear programming relaxation
    ([4a](#S4.E4.1 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")-[4d](#S4.E4.4 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) naturally
    satisfy the integrality constraints ([4e](#S4.E4.5 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). However,
    this fails to hold in the general multivariate input case. To see why, observe
    that the bounds on the input variables ${\bm{h}}^{l-1}$ are only coupled with
    the logic involving the binary variable $z$ only in an aggregated sense, through
    the coefficients $M^{l}_{i,-}$ and $M^{l}_{i,+}$. In other words, the â€œshapeâ€
    of the pre-activation domain is not incorporated directly into the big-$M$ formulation.
    Furthermore, the strength of this formulation highly depends on the big-$M$ coefficients.
    These coefficients can be obtained using techniques ranging from basic interval
    arithmetic to optimization-based bounds tightening. Grimstad and Andersson ([2019](#bib.bib133))
    show how constraints external to the neural network can yield tighter bounds via
    optimization- or feasibility-based bounds tightening. RÃ¶ssig and Petkovic ([2021](#bib.bib262))
    compare several methods for deriving bounds and further develop optimization-based
    bounds tightening based on pairwise dependencies between variables.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 'å€¼å¾—æ·±å…¥æ¢è®¨çš„æ˜¯è¿™ç§åŠ›é‡ä¸è¶³çš„æ¥æºã€‚å¦‚æœè¾“å…¥ ${\bm{h}}^{l-1}$ æ˜¯ä¸€ç»´çš„ï¼Œå¤§-$M$ å…¬å¼åœ¨*å±€éƒ¨*ä¸Šæ˜¯ç†æƒ³çš„ï¼ˆVielma, [2015](#bib.bib316)ï¼‰ï¼šçº¿æ€§è§„åˆ’æ¾å¼›çš„æç«¯ç‚¹
    ([4a](#S4.E4.1 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")-[4d](#S4.E4.4 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) è‡ªç„¶åœ°æ»¡è¶³æ•´æ•°çº¦æŸ
    ([4e](#S4.E4.5 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey"))ã€‚ç„¶è€Œï¼Œåœ¨ä¸€èˆ¬çš„å¤šå˜é‡è¾“å…¥æƒ…å†µä¸‹ï¼Œè¿™ç§æƒ…å†µå¹¶ä¸æˆç«‹ã€‚è¦ç†è§£åŸå› ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°ï¼Œè¾“å…¥å˜é‡
    ${\bm{h}}^{l-1}$ çš„ç•Œé™åªæ˜¯é€šè¿‡ç³»æ•° $M^{l}_{i,-}$ å’Œ $M^{l}_{i,+}$ åœ¨æ•´ä½“æ„ä¹‰ä¸Šä¸æ¶‰åŠäºŒè¿›åˆ¶å˜é‡ $z$ çš„é€»è¾‘ç›¸å…³è”ã€‚æ¢å¥è¯è¯´ï¼Œé¢„æ¿€æ´»åŸŸçš„â€œå½¢çŠ¶â€æ²¡æœ‰ç›´æ¥çº³å…¥å¤§-$M$
    å…¬å¼ä¸­ã€‚æ­¤å¤–ï¼Œè¿™ç§å…¬å¼çš„å¼ºåº¦é«˜åº¦ä¾èµ–äºå¤§-$M$ ç³»æ•°ã€‚è¿™äº›ç³»æ•°å¯ä»¥é€šè¿‡ä»åŸºæœ¬åŒºé—´ç®—æœ¯åˆ°åŸºäºä¼˜åŒ–çš„ç•Œé™æ”¶ç´§ç­‰æŠ€æœ¯è·å¾—ã€‚Grimstad å’Œ Andersson
    ([2019](#bib.bib133)) è¯´æ˜äº†ç¥ç»ç½‘ç»œå¤–éƒ¨çš„çº¦æŸå¦‚ä½•é€šè¿‡ä¼˜åŒ–æˆ–å¯è¡Œæ€§åŸºç¡€çš„ç•Œé™æ”¶ç´§è·å¾—æ›´ç´§çš„ç•Œé™ã€‚RÃ¶ssig å’Œ Petkovic ([2021](#bib.bib262))
    æ¯”è¾ƒäº†å‡ ç§ç•Œé™æ¨å¯¼æ–¹æ³•ï¼Œå¹¶è¿›ä¸€æ­¥å¼€å‘äº†åŸºäºå˜é‡é—´å¯¹å¶ä¾èµ–çš„ä¼˜åŒ–åŸºç¡€ç•Œé™æ”¶ç´§ã€‚'
- en: <svg   height="135.23" overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg><svg
    height="135.23" overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg>
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="135.23" overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg><svg
    height="135.23" overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg>
- en: 'Figure 9: Left: The convex hull of a ReLU neuron ([5](#S4.E5 "In 4.2.2 A stronger
    extended formulation â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")), and Right: the convex relaxation offered by the big-$M$ formulation
    ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) Adapted from Anderson et al. Anderson etÂ al.
    ([2020](#bib.bib5), [2019](#bib.bib4))'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9ï¼šå·¦ä¾§ï¼šReLU ç¥ç»å…ƒçš„å‡¸åŒ…ï¼ˆ[5](#S4.E5 "åœ¨ 4.2.2 æ›´å¼ºçš„æ‰©å±•å½¢å¼ â€£ 4.2 ç²¾ç¡®æ¨¡å‹ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’ â€£ 4 ä¼˜åŒ–è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œ
    â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰ï¼Œå³ä¾§ï¼šå¤§-$M$ å½¢å¼æä¾›çš„å‡¸æ¾å¼›ï¼ˆ[4](#S4.E4 "åœ¨ 4.2.1 å¤§-ğ‘€ å½¢å¼ â€£ 4.2 ç²¾ç¡®æ¨¡å‹ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’
    â€£ 4 ä¼˜åŒ–è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰ã€‚æ”¹ç¼–è‡ª Anderson ç­‰äºº ([2020](#bib.bib5)ï¼Œ[2019](#bib.bib4))ã€‚
- en: 4.2.2 A stronger extended formulation
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 æ›´å¼ºçš„æ‰©å±•å½¢å¼
- en: 'A much stronger MILP formulation can be constructed through a classical method,
    the extended formulation for disjunctions (Balas, [1998](#bib.bib11), Jeroslow
    and Lowe, [1984](#bib.bib163)). This formulation for a given ReLU neuron takes
    the following formÂ (Anderson etÂ al., [2019](#bib.bib4), Section 2.2):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç»å…¸æ–¹æ³•å¯ä»¥æ„é€ å‡ºæ›´å¼ºçš„ MILP å½¢å¼ï¼Œå³ç”¨äºåˆ†ç¦»çš„æ‰©å±•å½¢å¼ï¼ˆBalasï¼Œ[1998](#bib.bib11)ï¼ŒJeroslow å’Œ Loweï¼Œ[1984](#bib.bib163)ï¼‰ã€‚å¯¹äºç»™å®šçš„
    ReLU ç¥ç»å…ƒï¼Œè¿™ç§å½¢å¼å¦‚ä¸‹ï¼ˆAnderson ç­‰äººï¼Œ[2019](#bib.bib4)ï¼Œç¬¬ 2.2 èŠ‚ï¼‰ï¼š
- en: '|  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle=(x^{+},y^{+})+(x^{-},y^{-})$
    |  | (5a) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle=(x^{+},y^{+})+(x^{-},y^{-})$
    |  | (5a) |'
- en: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq{\bm{w}}^{l}_{i}x^{-}+b^{l}_{i}(1-z)$
    |  | (5b) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq{\bm{w}}^{l}_{i}x^{-}+b^{l}_{i}(1-z)$
    |  | (5b) |'
- en: '|  | $\displaystyle y^{+}$ | $\displaystyle={\bm{w}}^{l}_{i}x^{+}+b^{l}_{i}z\geq
    0$ |  | (5c) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{+}$ | $\displaystyle={\bm{w}}^{l}_{i}x^{+}+b^{l}_{i}z\geq
    0$ |  | (5c) |'
- en: '|  | $\displaystyle L^{l-1}(1-z)$ | $\displaystyle\leq x^{-}\leq U^{l-1}(1-z)$
    |  | (5d) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L^{l-1}(1-z)$ | $\displaystyle\leq x^{-}\leq U^{l-1}(1-z)$
    |  | (5d) |'
- en: '|  | $\displaystyle L^{l-1}z$ | $\displaystyle\leq x^{+}\leq U^{l-1}z$ |  |
    (5e) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L^{l-1}z$ | $\displaystyle\leq x^{+}\leq U^{l-1}z$ |  |
    (5e) |'
- en: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (5f) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (5f) |'
- en: This formulation requires one binary variable and $\mathcal{O}(n_{l-1})$ general
    linear constraints and auxiliary continuous variables. It is also locally ideal,
    i.e., as strong as possible. While the number of variables and constraints for
    an individual unit seems quite tame, applying this formulation for unit in a network
    leads to a formulation with $\mathcal{O}(n_{0}+\sum_{l\in{\mathbb{L}}}n_{l}n_{l-1})=\mathcal{O}(|{\mathbb{L}}|n_{\max}^{2})$
    continuous variables and linear constraints. Moreover, while the formulation for
    *an individual unit* is locally ideal, the composition of many locally ideal formulations
    will, in general, fail to be ideal itself. Consider that, while each node can
    be modeled as a two-part disjunction, the full network requires exponentially
    many disjuncts, each corresponding to one activation pattern.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å…¬å¼éœ€è¦ä¸€ä¸ªäºŒè¿›åˆ¶å˜é‡å’Œ $\mathcal{O}(n_{l-1})$ ä¸ªä¸€èˆ¬çº¿æ€§çº¦æŸä»¥åŠè¾…åŠ©è¿ç»­å˜é‡ã€‚å®ƒåœ¨å±€éƒ¨ä¸Šä¹Ÿæ˜¯ç†æƒ³çš„ï¼Œå³å°½å¯èƒ½å¼ºã€‚è™½ç„¶å•ä¸ªå•ä½çš„å˜é‡å’Œçº¦æŸæ•°é‡ä¼¼ä¹ç›¸å½“æ¸©å’Œï¼Œä½†åœ¨ç½‘ç»œä¸­åº”ç”¨æ­¤å…¬å¼ä¼šå¯¼è‡´ä¸€ä¸ªå…·æœ‰
    $\mathcal{O}(n_{0}+\sum_{l\in{\mathbb{L}}}n_{l}n_{l-1})=\mathcal{O}(|{\mathbb{L}}|n_{\max}^{2})$
    ä¸ªè¿ç»­å˜é‡å’Œçº¿æ€§çº¦æŸçš„å…¬å¼ã€‚æ­¤å¤–ï¼Œè™½ç„¶*å•ä¸ªå•ä½*çš„å…¬å¼åœ¨å±€éƒ¨ä¸Šæ˜¯ç†æƒ³çš„ï¼Œä½†è®¸å¤šå±€éƒ¨ç†æƒ³å…¬å¼çš„ç»„åˆé€šå¸¸ä¼šå¯¼è‡´æ•´ä½“ä¸Šä¸å†ç†æƒ³ã€‚è€ƒè™‘åˆ°è™½ç„¶æ¯ä¸ªèŠ‚ç‚¹å¯ä»¥å»ºæ¨¡ä¸ºä¸¤ä¸ªéƒ¨åˆ†çš„æå–ï¼Œä½†æ•´ä¸ªç½‘ç»œéœ€è¦æŒ‡æ•°çº§çš„æå–ï¼Œæ¯ä¸€ä¸ªå¯¹åº”äºä¸€ç§æ¿€æ´»æ¨¡å¼ã€‚
- en: 'Despite its strength and relatively modest increase in size relative to the
    big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models
    using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£
    When Deep Learning Meets Polyhedral Theory: A Survey")), it has been empirically
    observed that this formulation often performs worse than expected (Anderson etÂ al.,
    [2019](#bib.bib4), Vielma, [2019](#bib.bib317)), both in the verification setting
    and more broadly.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç›¸å¯¹äºå¤§-$M$ å…¬å¼è€Œè¨€ï¼Œå®ƒçš„å¼ºåº¦å’Œç›¸å¯¹è¾ƒå°çš„è§„æ¨¡å¢åŠ ï¼ˆ[4](#S4.E4 "åœ¨ 4.2.1 å¤§-ğ‘€ å…¬å¼ â€£ 4.2 ç²¾ç¡®æ¨¡å‹ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’
    â€£ 4 ä¼˜åŒ–è®­ç»ƒç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰æ˜¯ç›¸å¯¹æ¸©å’Œçš„ï¼Œä½†é€šè¿‡å®è¯è§‚å¯Ÿå‘ç°ï¼Œè¿™ç§å…¬å¼åœ¨éªŒè¯è®¾ç½®å’Œæ›´å¹¿æ³›çš„åº”ç”¨ä¸­é€šå¸¸è¡¨ç°å¾—æ¯”é¢„æœŸå·®ï¼ˆAnderson
    ç­‰ï¼Œ[2019](#bib.bib4)ï¼ŒVielmaï¼Œ[2019](#bib.bib317)ï¼‰ã€‚
- en: 4.2.3 A class of intermediate formulations
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 ä¸€ç±»ä¸­é—´å…¬å¼
- en: 'The previous sections observed that the big-$M$ formulation ([4](#S4.E4 "In
    4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) is compact, but may offer a weak convex relaxation, while
    the extended formulation ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) offers
    the tightest possible convex relaxation for an individual unit, at the expense
    of a much larger formulation. Kronqvist etÂ al. ([2022](#bib.bib180), [2021](#bib.bib179))
    present a strategy for obtaining formulations intermediate to ([4](#S4.E4 "In
    4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) and ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) in terms
    of both size and strength. The key idea is to partition ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$
    into a number of aggregated variables, ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}=\sum_{p=1}^{P}\hat{x}_{p}$.
    Each auxiliary variable $\hat{x}_{p}$ is defined as a sum of a subset of the $j$-th
    weighted inputs $\hat{x}_{p}=\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$,
    with $\mathbb{S}_{1},...,\mathbb{S}_{P}$ partitioning $\{1,...,n_{l-1}\}$. This
    technique can be applied to the ReLU function, giving the convex hull over the
    directions defined by $\hat{x}_{p}$ (Tsay etÂ al., [2021](#bib.bib312)):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¹‹å‰çš„ç« èŠ‚è§‚å¯Ÿåˆ°ï¼Œå¤§-$M$ å…¬å¼ ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models
    using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£
    When Deep Learning Meets Polyhedral Theory: A Survey")) æ˜¯ç´§å‡‘çš„ï¼Œä½†å¯èƒ½æä¾›äº†è¾ƒå¼±çš„å‡¸æ¾å¼›ï¼Œè€Œæ‰©å±•å…¬å¼
    ([5](#S4.E5 "In 4.2.2 A stronger extended formulation â€£ 4.2 Exact models using
    mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When
    Deep Learning Meets Polyhedral Theory: A Survey")) ä¸ºå•ä¸ªå•å…ƒæä¾›äº†å¯èƒ½æœ€ç´§çš„å‡¸æ¾å¼›ï¼Œä½†ä»£ä»·æ˜¯å…¬å¼è§„æ¨¡å¤§å¾—å¤šã€‚Kronqvist
    ç­‰ ([2022](#bib.bib180), [2021](#bib.bib179)) æå‡ºäº†åœ¨å¤§å°å’Œå¼ºåº¦ä¸Šä»‹äº ([4](#S4.E4 "In 4.2.1
    The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")) å’Œ ([5](#S4.E5 "In 4.2.2 A stronger extended formulation â€£ 4.2 Exact
    models using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) ä¹‹é—´çš„ä¸­é—´å…¬å¼çš„ç­–ç•¥ã€‚å…³é”®æ€æƒ³æ˜¯å°† ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$
    åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ªèšåˆå˜é‡ï¼Œ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}=\sum_{p=1}^{P}\hat{x}_{p}$ã€‚æ¯ä¸ªè¾…åŠ©å˜é‡ $\hat{x}_{p}$
    å®šä¹‰ä¸ºç¬¬ $j$ ä¸ªåŠ æƒè¾“å…¥çš„å­é›†çš„æ€»å’Œ $\hat{x}_{p}=\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$ï¼Œå…¶ä¸­
    $\mathbb{S}_{1},...,\mathbb{S}_{P}$ åˆ’åˆ†äº† $\{1,...,n_{l-1}\}$ã€‚è¿™ä¸€æŠ€æœ¯å¯ä»¥åº”ç”¨äº ReLU å‡½æ•°ï¼Œä»è€Œç»™å‡ºç”±
    $\hat{x}_{p}$ å®šä¹‰çš„æ–¹å‘ä¸Šçš„å‡¸åŒ…ï¼ˆTsay ç­‰ï¼Œ[2021](#bib.bib312)ï¼‰ï¼š'
- en: '|  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\left(\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1},h^{l}_{i}\right)$
    | $\displaystyle=(\hat{x}_{p}^{+},y^{+})+(\hat{x}_{p}^{-},y^{-})$ |  | (6a) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left(\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1},h^{l}_{i}\right)$
    | $\displaystyle=(\hat{x}_{p}^{+},y^{+})+(\hat{x}_{p}^{-},y^{-})$ |  | (6a) |'
- en: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq\sum_{p}\hat{x}_{p}^{-}+b^{l}_{i}(1-z)$
    |  | (6b) |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq\sum_{p}\hat{x}_{p}^{-}+b^{l}_{i}(1-z)$
    |  | (6b) |'
- en: '|  | $\displaystyle y^{+}$ | $\displaystyle=\sum_{p}\hat{x}_{p}^{+}+b^{l}_{i}z\geq
    0$ |  | (6c) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{+}$ | $\displaystyle=\sum_{p}\hat{x}_{p}^{+}+b^{l}_{i}z\geq
    0$ |  | (6c) |'
- en: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}(1-z)$ | $\displaystyle\leq\hat{x}^{-}\leq\hat{\bm{M}}_{i,+}^{l}(1-z)$
    |  | (6d) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}(1-z)$ | $\displaystyle\leq\hat{x}^{-}\leq\hat{\bm{M}}_{i,+}^{l}(1-z)$
    |  | (6d) |'
- en: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}z$ | $\displaystyle\leq\hat{x}^{+}\leq\hat{\bm{M}}_{i,+}^{l}z$
    |  | (6e) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}z$ | $\displaystyle\leq\hat{x}^{+}\leq\hat{\bm{M}}_{i,+}^{l}z$
    |  | (6e) |'
- en: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (6f) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (6f) |'
- en: Here, the $p$-th elements of $\hat{\bm{M}}_{i,-}^{l}$ and $\hat{\bm{M}}_{i,+}^{l}$
    must satisfy the inequalities
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ$\hat{\bm{M}}_{i,-}^{l}$ å’Œ $\hat{\bm{M}}_{i,+}^{l}$ çš„ç¬¬ $p$ ä¸ªå…ƒç´ å¿…é¡»æ»¡è¶³ä¸ç­‰å¼ã€‚
- en: '|  | $\displaystyle\hat{M}^{l}_{i,-,p}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$
    |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{M}^{l}_{i,-,p}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$
    |  |'
- en: '|  | $\displaystyle\hat{M}^{l}_{i,+,p}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}.$
    |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{M}^{l}_{i,+,p}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}.$
    |  |'
- en: 'These coefficients can be derived using techniques analagous to those for the
    big-$M$ formulation (note that tighter bounds may be derived by considering $\hat{x}^{-}$
    and $\hat{x}^{+}$ separately). Observe that when $P=1$, we recover the same tightness
    as the big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact
    models using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")), as, intuitively, the
    formulation is built over a single â€œdirectionâ€ corresponding to ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$.
    Conversely, when $P=n_{l-1}$, we recover the tightness of the extended formulation
    ([5](#S4.E5 "In 4.2.2 A stronger extended formulation â€£ 4.2 Exact models using
    mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When
    Deep Learning Meets Polyhedral Theory: A Survey")), as each direction corresponds
    to a single element of ${\bm{h}}^{l-1}$. Tsay etÂ al. ([2021](#bib.bib312)) study
    partitioning strategies and show that intermediate values of $P$ result in formulations
    that can outperform the two extremes, by balancing formulation size and strength.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç³»æ•°å¯ä»¥é€šè¿‡ç±»ä¼¼äºå¤§-$M$ å…¬å¼çš„æŠ€æœ¯å¯¼å‡ºï¼ˆè¯·æ³¨æ„ï¼Œé€šè¿‡åˆ†åˆ«è€ƒè™‘ $\hat{x}^{-}$ å’Œ $\hat{x}^{+}$ï¼Œå¯èƒ½ä¼šå¾—åˆ°æ›´ç´§çš„ç•Œé™ï¼‰ã€‚è§‚å¯Ÿåˆ°å½“
    $P=1$ æ—¶ï¼Œæˆ‘ä»¬æ¢å¤äº†ä¸å¤§-$M$ å…¬å¼ç›¸åŒçš„ç´§åº¦ï¼ˆ[4](#S4.E4 "åœ¨4.2.1 å¤§-ğ‘€ å…¬å¼ â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°ç¼–ç¨‹çš„ç²¾ç¡®æ¨¡å‹ â€£ 4
    ä¼˜åŒ–è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰ï¼Œå› ä¸ºä»ç›´è§‚ä¸Šè®²ï¼Œå…¬å¼æ˜¯å»ºç«‹åœ¨å•ä¸€çš„â€œæ–¹å‘â€ä¸Šï¼Œå¯¹åº”äº ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$ã€‚ç›¸åï¼Œå½“
    $P=n_{l-1}$ æ—¶ï¼Œæˆ‘ä»¬æ¢å¤äº†æ‰©å±•å…¬å¼çš„ç´§åº¦ï¼ˆ[5](#S4.E5 "åœ¨4.2.2 æ›´å¼ºçš„æ‰©å±•å…¬å¼ â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°ç¼–ç¨‹çš„ç²¾ç¡®æ¨¡å‹ â€£ 4
    ä¼˜åŒ–è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰ï¼Œå› ä¸ºæ¯ä¸ªæ–¹å‘å¯¹åº”äº ${\bm{h}}^{l-1}$ çš„å•ä¸ªå…ƒç´ ã€‚Tsay ç­‰äººï¼ˆ[2021](#bib.bib312)ï¼‰ç ”ç©¶äº†åˆ†å‰²ç­–ç•¥ï¼Œå¹¶è¡¨æ˜
    $P$ çš„ä¸­é—´å€¼å¯ä»¥äº§ç”Ÿæ¯”ä¸¤æç«¯æƒ…å†µæ›´å…·ä¼˜åŠ¿çš„å…¬å¼ï¼Œé€šè¿‡å¹³è¡¡å…¬å¼çš„å¤§å°å’Œå¼ºåº¦ã€‚
- en: '4.2.4 Cutting plane methods: Trading variables for inequalities'
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 å‰²å¹³é¢æ–¹æ³•ï¼šç”¨ä¸ç­‰å¼æ›¿æ¢å˜é‡
- en: 'The extended formulation ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) achieves
    its strength through the introduction of auxiliary continuous variables. However,
    it is possible to produce a formulation of equal strength by projecting out these
    auxiliary variables, leaving an ideal formulation in the â€œoriginalâ€ $({\bm{h}}^{l-1},h^{l}_{i},z)$
    variable space. While in general this projection may be difficult computationally,
    for the simple structure of a single ReLU neuron it is possible to characterize
    in closed form. The formulation is given by Anderson etÂ al. ([2020](#bib.bib5),
    [2019](#bib.bib4)) as'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©å±•å…¬å¼ï¼ˆ[5](#S4.E5 "åœ¨4.2.2 æ›´å¼ºçš„æ‰©å±•å…¬å¼ â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°ç¼–ç¨‹çš„ç²¾ç¡®æ¨¡å‹ â€£ 4 ä¼˜åŒ–è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰é€šè¿‡å¼•å…¥è¾…åŠ©è¿ç»­å˜é‡æ¥å®ç°å…¶å¼ºåº¦ã€‚ç„¶è€Œï¼Œé€šè¿‡å°†è¿™äº›è¾…åŠ©å˜é‡æŠ•å½±å‡ºå»ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªç­‰å¼ºåº¦çš„å…¬å¼ï¼Œä¿ç•™â€œåŸå§‹â€
    $({\bm{h}}^{l-1},h^{l}_{i},z)$ å˜é‡ç©ºé—´ä¸­çš„ç†æƒ³å…¬å¼ã€‚å°½ç®¡é€šå¸¸è¿™ç§æŠ•å½±åœ¨è®¡ç®—ä¸Šå¯èƒ½å›°éš¾ï¼Œä½†å¯¹äºå•ä¸€ ReLU ç¥ç»å…ƒçš„ç®€å•ç»“æ„ï¼Œå¯ä»¥ä»¥é—­å¼å½¢å¼è¿›è¡Œè¡¨å¾ã€‚è¯¥å…¬å¼ç”±
    Anderson ç­‰äººï¼ˆ[2020](#bib.bib5), [2019](#bib.bib4)ï¼‰ç»™å‡ºã€‚
- en: '|  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (7a) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (7a) |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{j\in J}w^{l}_{i,j}(h^{l-1}_{i}-\breve{L}^{l}_{j}(1-z))+\left(b+\sum_{j\not\in
    J}w^{l}_{i,j}\breve{U}_{j}\right)z\quad\forall J\subseteq\llbracket n_{l-1}\rrbracket$
    |  | (7b) |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{j\in J}w^{l}_{i,j}(h^{l-1}_{i}-\breve{L}^{l}_{j}(1-z))+\left(b+\sum_{j\not\in
    J}w^{l}_{i,j}\breve{U}_{j}\right)z\quad\forall J\subseteq\llbracket n_{l-1}\rrbracket$
    |  | (7b) |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}_{\geq
    0}$ |  | (7c) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}_{\geq
    0}$ |  | (7c) |'
- en: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\},$ |  | (7d) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\},$ |  | (7d) |'
- en: where notationally, for each $j\in\llbracket n_{l-1}\rrbracket$, we take
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¦å·è¡¨ç¤ºä¸Šï¼Œå¯¹äºæ¯ä¸ª $j\in\llbracket n_{l-1}\rrbracket$ï¼Œæˆ‘ä»¬å–
- en: '|  | <math   alttext="\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\ U^{l-1}_{j}&amp;w^{l}_{i,j}<0\end{cases}\quad\mathrm{and}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\ U^{l-1}_{j}&amp;w^{l}_{i,j}<0\end{cases}\quad\mathrm{and}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\'
- en: L^{l-1}_{j}&amp;w^{l}_{i,j}<0\end{cases}" display="block"><semantics ><mrow
    ><mrow ><msubsup  ><mover accent="true"  ><mi >L</mi><mo >Ë˜</mo></mover><mi >j</mi><mrow
    ><mi >l</mi><mo >âˆ’</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow  ><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><msubsup
    ><mi >L</mi><mi >j</mi><mrow ><mi  >l</mi><mo >âˆ’</mo><mn >1</mn></mrow></msubsup></mtd><mtd
    columnalign="left"  ><mrow ><msubsup ><mi  >w</mi><mrow ><mi >i</mi><mo >,</mo><mi
    >j</mi></mrow><mi >l</mi></msubsup><mo >â‰¥</mo><mn >0</mn></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><msubsup  ><mi >U</mi><mi >j</mi><mrow ><mi >l</mi><mo
    >âˆ’</mo><mn >1</mn></mrow></msubsup></mtd><mtd columnalign="left"  ><mrow ><msubsup  ><mi
    >w</mi><mrow ><mi >i</mi><mo >,</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo
    ><</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow><mi >and</mi></mrow></mrow><mrow
    ><msubsup ><mover accent="true" ><mi  >U</mi><mo >Ë˜</mo></mover><mi >j</mi><mrow
    ><mi >l</mi><mo >âˆ’</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow  ><mo >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr ><mtd columnalign="left"  ><msubsup
    ><mi >U</mi><mi >j</mi><mrow ><mi  >l</mi><mo >âˆ’</mo><mn >1</mn></mrow></msubsup></mtd><mtd
    columnalign="left"  ><mrow ><msubsup ><mi  >w</mi><mrow ><mi >i</mi><mo >,</mo><mi
    >j</mi></mrow><mi >l</mi></msubsup><mo >â‰¥</mo><mn >0</mn></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><msubsup ><mi  >L</mi><mi >j</mi><mrow ><mi >l</mi><mo
    >âˆ’</mo><mn >1</mn></mrow></msubsup></mtd><mtd columnalign="left"  ><mrow ><msubsup
    ><mi  >w</mi><mrow ><mi >i</mi><mo >,</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo
    ><</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><ci >Ë˜</ci><ci >ğ¿</ci></apply><apply ><ci  >ğ‘™</ci><cn
    type="integer"  >1</cn></apply></apply><ci >ğ‘—</ci></apply><list ><apply ><csymbol
    cd="latexml"  >cases</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ¿</ci><apply ><ci >ğ‘™</ci><cn
    type="integer" >1</cn></apply></apply><ci >ğ‘—</ci></apply><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >ğ‘¤</ci><ci >ğ‘™</ci></apply><list ><ci >ğ‘–</ci><ci >ğ‘—</ci></list></apply><cn type="integer"  >0</cn></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ğ‘ˆ</ci><apply ><ci >ğ‘™</ci><cn type="integer" >1</cn></apply></apply><ci >ğ‘—</ci></apply><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >ğ‘¤</ci><ci >ğ‘™</ci></apply><list ><ci >ğ‘–</ci><ci >ğ‘—</ci></list></apply><cn type="integer"  >0</cn></apply></apply><ci
    >and</ci></list></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><ci  >Ë˜</ci><ci >ğ‘ˆ</ci></apply><apply
    ><ci >ğ‘™</ci><cn type="integer" >1</cn></apply></apply><ci >ğ‘—</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ‘ˆ</ci><apply ><ci  >ğ‘™</ci><cn
    type="integer"  >1</cn></apply></apply><ci >ğ‘—</ci></apply><apply ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ğ‘¤</ci><ci >ğ‘™</ci></apply><list ><ci >ğ‘–</ci><ci >ğ‘—</ci></list></apply><cn type="integer"
    >0</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >ğ¿</ci><apply ><ci  >ğ‘™</ci><cn type="integer"  >1</cn></apply></apply><ci
    >ğ‘—</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ‘¤</ci><ci >ğ‘™</ci></apply><list
    ><ci >ğ‘–</ci><ci >ğ‘—</ci></list></apply><cn type="integer" >0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&w^{l}_{i,j}\geq
    0\\ U^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}\quad\mathrm{and}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&w^{l}_{i,j}\geq
    0\\ L^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}</annotation></semantics></math> |  |
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: \(\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&w^{l}_{i,j}\geq 0\\ U^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}\quad\mathrm{å’Œ}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&w^{l}_{i,j}\geq
    0\\ L^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}\)
- en: 'We note a few points of interest about this formulation. First, it is ideal,
    and so recovers the convex hull of a ReLU activation function, coupled with its
    preactivation affine function and bounds on each of the inputs to that affine
    function. Second, it can be shown that, under very mild conditions, each of the
    exponentially many constraints in ([7b](#S4.E7.2 "In 7 â€£ 4.2.4 Cutting plane methods:
    Trading variables for inequalities â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) are necessary to ensure this property; none are redundant
    and can be removed without affecting the relaxation quality. Third, note that
    by selecting only those constraints in ([7b](#S4.E7.2 "In 7 â€£ 4.2.4 Cutting plane
    methods: Trading variables for inequalities â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) corresponding to $J=\emptyset$ and $J=\llbracket
    n_{l-1}\rrbracket$, we recover the big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The
    big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")) in the case where $\mathcal{D}^{l-1}=[L^{l-1},U^{l-1}]$. This suggests
    a practical approach for using this large family of inequalities: Start with the
    big-$M$ formulation, and then dynamically generate violated inequalities from
    ([7b](#S4.E7.2 "In 7 â€£ 4.2.4 Cutting plane methods: Trading variables for inequalities
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) as-needed
    in a cutting plane procedure. As shown by Anderson etÂ al. ([2020](#bib.bib5)),
    this separation problem is separable in the input variables, and hence can be
    completed in $\mathcal{O}(n_{l-1})$ time.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬æ³¨æ„åˆ°å…³äºè¿™ä¸ªå…¬å¼çš„ä¸€äº›è¦ç‚¹ã€‚é¦–å…ˆï¼Œå®ƒæ˜¯ç†æƒ³çš„ï¼Œå› æ­¤æ¢å¤äº† ReLU æ¿€æ´»å‡½æ•°çš„å‡¸åŒ…ï¼Œç»“åˆå…¶é¢„æ¿€æ´»ä»¿å°„å‡½æ•°ä»¥åŠå¯¹è¯¥ä»¿å°„å‡½æ•°æ¯ä¸ªè¾“å…¥çš„ç•Œé™ã€‚ç¬¬äºŒï¼Œå¯ä»¥è¯æ˜ï¼Œåœ¨éå¸¸å®½æ¾çš„æ¡ä»¶ä¸‹ï¼Œ([7b](#S4.E7.2
    "In 7 â€£ 4.2.4 Cutting plane methods: Trading variables for inequalities â€£ 4.2
    Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) ä¸­çš„æ¯ä¸ªæŒ‡æ•°çº§å¤šçš„çº¦æŸéƒ½æ˜¯ç¡®ä¿è¿™ä¸€å±æ€§æ‰€å¿…éœ€çš„ï¼›æ²¡æœ‰ä¸€ä¸ªæ˜¯å¤šä½™çš„ï¼Œä¸”å¯ä»¥åœ¨ä¸å½±å“æ¾å¼›è´¨é‡çš„æƒ…å†µä¸‹è¢«ç§»é™¤ã€‚ç¬¬ä¸‰ï¼Œè¯·æ³¨æ„ï¼Œé€šè¿‡é€‰æ‹©ä»…å¯¹åº”äº
    $J=\emptyset$ å’Œ $J=\llbracket n_{l-1}\rrbracket$ çš„([7b](#S4.E7.2 "In 7 â€£ 4.2.4
    Cutting plane methods: Trading variables for inequalities â€£ 4.2 Exact models using
    mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When
    Deep Learning Meets Polyhedral Theory: A Survey")) ä¸­çš„é‚£äº›çº¦æŸï¼Œæˆ‘ä»¬æ¢å¤äº†å¤§-$M$ å…¬å¼ ([4](#S4.E4
    "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"))ï¼Œå½“ $\mathcal{D}^{l-1}=[L^{l-1},U^{l-1}]$ çš„æƒ…å†µä¸‹ã€‚è¿™è¡¨æ˜äº†ä¸€ç§å®é™…çš„æ–¹æ³•æ¥ä½¿ç”¨è¿™ä¸€å¤§èŒƒå›´çš„çº¦æŸï¼šä»å¤§-$M$
    å…¬å¼å¼€å§‹ï¼Œç„¶ååœ¨åˆ‡å¹³é¢ç¨‹åºä¸­æ ¹æ®éœ€è¦åŠ¨æ€ç”Ÿæˆ ([7b](#S4.E7.2 "In 7 â€£ 4.2.4 Cutting plane methods: Trading
    variables for inequalities â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) ä¸­çš„è¿åçº¦æŸã€‚æ­£å¦‚ Anderson ç­‰äºº ([2020](#bib.bib5)) æ‰€ç¤ºï¼Œè¿™ä¸€åˆ†ç¦»é—®é¢˜åœ¨è¾“å…¥å˜é‡ä¸­æ˜¯å¯åˆ†çš„ï¼Œå› æ­¤å¯ä»¥åœ¨
    $\mathcal{O}(n_{l-1})$ æ—¶é—´å†…å®Œæˆã€‚'
- en: 'The cutting plane strategy is in general compatible with weaker formulations,
    such as relaxation-based verification (Zhang etÂ al., [2022](#bib.bib356)) and
    formulations from the class ([6](#S4.E6 "In 4.2.3 A class of intermediate formulations
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). In fact,
    Tsay etÂ al. ([2021](#bib.bib312)) show that the intermediate formulations in ([6](#S4.E6
    "In 4.2.3 A class of intermediate formulations â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) effectively pre-select a number of inequalities
    from ([7b](#S4.E7.2 "In 7 â€£ 4.2.4 Cutting plane methods: Trading variables for
    inequalities â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")), in terms of their continuous relaxations. While adding these constraints
    results in a tighter continuous relaxation, the added constraints can eventually
    significantly increase the model size. Practical implementations may therefore
    only perform cut generation at a limited number of branch-and-bound search nodes
    (DeÂ Palma etÂ al., [2021](#bib.bib77), Tsay etÂ al., [2021](#bib.bib312)).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ‡å¹³é¢ç­–ç•¥é€šå¸¸ä¸è¾ƒå¼±çš„å…¬å¼å…¼å®¹ï¼Œä¾‹å¦‚åŸºäºæ¾å¼›çš„éªŒè¯ï¼ˆZhang et al., [2022](#bib.bib356)ï¼‰å’Œæ¥è‡ªç±»çš„å…¬å¼ï¼ˆ[6](#S4.E6
    "åœ¨ 4.2.3 ä¸­çš„ä¸­é—´å…¬å¼ç±» â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹ â€£ 4 ä¼˜åŒ–è®­ç»ƒç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰ã€‚äº‹å®ä¸Šï¼ŒTsay
    et al. ([2021](#bib.bib312)) è¡¨æ˜ï¼Œ[6](#S4.E6 "åœ¨ 4.2.3 ä¸­çš„ä¸­é—´å…¬å¼ç±» â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹
    â€£ 4 ä¼˜åŒ–è®­ç»ƒç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ä¸­çš„ä¸­é—´å…¬å¼æœ‰æ•ˆåœ°ä»[7b](#S4.E7.2 "åœ¨ 7 â€£ 4.2.4 åˆ‡å¹³é¢æ–¹æ³•ï¼šç”¨ä¸ç­‰å¼æ›¿æ¢å˜é‡
    â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹ â€£ 4 ä¼˜åŒ–è®­ç»ƒç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ä¸­é€‰æ‹©äº†ä¸€äº›ä¸ç­‰å¼ï¼Œå°±å…¶è¿ç»­æ¾å¼›è€Œè¨€ã€‚è™½ç„¶æ·»åŠ è¿™äº›çº¦æŸä¼šå¯¼è‡´æ›´ç´§çš„è¿ç»­æ¾å¼›ï¼Œä½†æ·»åŠ çš„çº¦æŸæœ€ç»ˆå¯èƒ½ä¼šæ˜¾è‘—å¢åŠ æ¨¡å‹çš„å¤§å°ã€‚å› æ­¤ï¼Œå®é™…å®ç°å¯èƒ½ä»…åœ¨æœ‰é™æ•°é‡çš„åˆ†æ”¯å®šç•Œæœç´¢èŠ‚ç‚¹ä¸Šæ‰§è¡Œåˆ‡å‰²ç”Ÿæˆï¼ˆDe
    Palma et al., [2021](#bib.bib77)ï¼ŒTsay et al., [2021](#bib.bib312)ï¼‰ã€‚
- en: 'A subtlety when using ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading
    variables for inequalities â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"))'
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ä½¿ç”¨([7](#S4.E7 "åœ¨ 4.2.4 åˆ‡å¹³é¢æ–¹æ³•ï¼šç”¨ä¸ç­‰å¼æ›¿æ¢å˜é‡ â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹ â€£ 4 ä¼˜åŒ–è®­ç»ƒç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))æ—¶çš„ä¸€ä¸ªç»†å¾®é—®é¢˜
- en: This third point above raises a subtlety discussed in the literatureÂ (DeÂ Palma
    etÂ al., [2021](#bib.bib77), Appendix F). Often, additional structural information
    is known about $\mathcal{D}^{l-1}$ beyond bounds on the variables. In this case,
    it is typically possible to derive tighter values for the big-$M$ coefficients.
    In this case, when using a separation-based approach it is preferable to initialize
    the formulation with these tightened big-$M$ constraints, and then proceed with
    the cutting plane approach as normal from there.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ç¬¬ä¸‰ç‚¹æå‡ºäº†æ–‡çŒ®ä¸­è®¨è®ºçš„ä¸€ä¸ªç»†å¾®é—®é¢˜ï¼ˆDe Palma et al., [2021](#bib.bib77)ï¼Œé™„å½• Fï¼‰ã€‚é€šå¸¸ï¼Œå…³äº$\mathcal{D}^{l-1}$çš„é™„åŠ ç»“æ„ä¿¡æ¯ä¼šè¶…å‡ºå˜é‡çš„ç•Œé™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šå¸¸å¯ä»¥æ¨å¯¼å‡ºæ›´ç´§çš„
    big-$M$ ç³»æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½“ä½¿ç”¨åŸºäºåˆ†ç¦»çš„æ–¹æ³•æ—¶ï¼Œæœ€å¥½ç”¨è¿™äº›æ”¶ç´§çš„ big-$M$ çº¦æŸåˆå§‹åŒ–æ¨¡å‹ï¼Œç„¶åä»é‚£é‡ŒæŒ‰å¸¸è§„ç»§ç»­è¿›è¡Œåˆ‡å¹³é¢æ–¹æ³•ã€‚
- en: '4.3 Scaling further: Convex relaxations and linear programming'
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 è¿›ä¸€æ­¥æ‰©å±•ï¼šå‡¸æ¾å¼›å’Œçº¿æ€§è§„åˆ’
- en: 'The above demonstrate MILP as a powerful framework for exactly modeling complex,
    nonconvex trained neural networks, but standard solvers are often not sufficiently
    scalable to adequately handle large-scale networks. A natural approach to increase
    the scalability, then, is to *relax* the network in some manner, and then apply
    convex optimization methods. For the verification problem discussed in SectionÂ [4.1.1](#S4.SS1.SSS1
    "4.1.1 Neural network verification â€£ 4.1 Applications of optimization over trained
    networks â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey"), this yields what is known as an *incomplete verifier*:
    any certification of robustness provided can be trusted (no false positives),
    but there may be robust instances that the method cannot prove are (some false
    negatives). In other words, over-approximation produces a verifier that is sound,
    but not complete.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å†…å®¹è¯æ˜äº†MILPä½œä¸ºç²¾ç¡®å»ºæ¨¡å¤æ‚çš„ã€éå‡¸çš„è®­ç»ƒç¥ç»ç½‘ç»œçš„å¼ºå¤§æ¡†æ¶ï¼Œä½†æ ‡å‡†æ±‚è§£å™¨å¾€å¾€ä¸è¶³ä»¥å……åˆ†å¤„ç†å¤§è§„æ¨¡ç½‘ç»œã€‚å› æ­¤ï¼Œå¢åŠ å¯æ‰©å±•æ€§çš„ä¸€ä¸ªè‡ªç„¶æ–¹æ³•æ˜¯ä»¥æŸç§æ–¹å¼*æ¾å¼›*ç½‘ç»œï¼Œç„¶ååº”ç”¨å‡¸ä¼˜åŒ–æ–¹æ³•ã€‚å¯¹äºç¬¬4.1.1èŠ‚ä¸­è®¨è®ºçš„éªŒè¯é—®é¢˜ï¼Œç»“æœæ˜¯è¢«ç§°ä¸º*ä¸å®Œå…¨éªŒè¯å™¨*ï¼šå¯ä»¥ä¿¡ä»»å¯¹é²æ£’æ€§çš„ä»»ä½•è®¤è¯ï¼ˆæ²¡æœ‰é”™è¯¯çš„é˜³æ€§ï¼‰ï¼Œä½†å¯èƒ½æœ‰ä¸€äº›è¯¥æ–¹æ³•æ— æ³•è¯æ˜çš„é²æ£’å®ä¾‹ï¼ˆæŸäº›é”™è¯¯çš„é˜´æ€§ï¼‰ã€‚æ¢å¥è¯è¯´ï¼Œè¿‡åº¦é€¼è¿‘ä¼šäº§ç”Ÿä¸€ä¸ªå¬èµ·æ¥æ˜¯ï¼Œä½†ä¸å®Œå…¨çš„éªŒè¯å™¨ã€‚
- en: 'While a variety of methods exist for accomplishing this, in this section we
    briefly outline techniques relevant to polyhedral theory. In particular, we focus
    on some techniques for building convex polyhedral relaxations. The most natural
    convex relaxation for a MILP formulation is its linear programming (LP) relaxation,
    constructed by dropping any integrality constraints. For example, the LP relaxation
    of ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) is given by the system ([4a](#S4.E4.1 "In
    4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")-[4d](#S4.E4.4 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact
    models using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). This is a compact linear
    programming relaxation for a ReLU-based network, and is the basis for methods
    due to Bunel etÂ al. ([2020a](#bib.bib41)) and Ehlers ([2017](#bib.bib87)).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å­˜åœ¨å„ç§æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä½†åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç®€è¦æ¦‚è¿°äº†ä¸å¤šé¢ä½“ç†è®ºç›¸å…³çš„æŠ€æœ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä¸€äº›æ„å»ºå‡¸å¤šé¢ä½“æ¾å¼›çš„æŠ€æœ¯ã€‚MILPå…¬å¼çš„æœ€è‡ªç„¶çš„å‡¸æ¾å¼›æ˜¯å…¶çº¿æ€§è§„åˆ’ï¼ˆLPï¼‰æ¾å¼›ï¼Œé€šè¿‡æ”¾å¼ƒä»»ä½•æ•´ä½“æ€§çº¦æŸè€Œæ„å»ºã€‚ä¾‹å¦‚ï¼Œ([4](#S4.E4
    "åœ¨4.2.1å¤§Må…¬å¼â€£4.2ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹â€£4åœ¨è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ–â€£å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥")) çš„LPæ¾å¼›ç”±ç³»ç»Ÿ([4a](#S4.E4.1
    "åœ¨4 â€£4.2.1å¤§Må…¬å¼â€£4.2ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹â€£4åœ¨è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ–â€£å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥")-[4d](#S4.E4.4
    "åœ¨4 â€£4.2.1å¤§Må…¬å¼â€£4.2ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹â€£4åœ¨è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ–â€£å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥")) ç»™å‡ºã€‚è¿™æ˜¯åŸºäºReLUç½‘ç»œçš„ç´§å‡‘çº¿æ€§è§„åˆ’æ¾å¼›ï¼Œä¹Ÿæ˜¯Bunelç­‰äºº([2020a](#bib.bib41))å’ŒEhlers([2017](#bib.bib87))æ–¹æ³•çš„åŸºç¡€ã€‚
- en: 4.3.1 Projecting the big-$M$ and ideal MILP formulations
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 æŠ•å½±å¤§-$M$å’Œç†æƒ³MILPå…¬å¼
- en: 'This section examines projections of the linear relaxations of formulations
    ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) and ([7](#S4.E7 "In 4.2.4 Cutting plane methods:
    Trading variables for inequalities â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")).'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚è€ƒå¯Ÿäº†å…¬å¼([4](#S4.E4 "åœ¨4.2.1å¤§Må…¬å¼â€£4.2ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹â€£4åœ¨è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ–â€£å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥"))å’Œ([7](#S4.E7
    "åœ¨4.2.4å‰²å¹³é¢æ–¹æ³•ï¼šä»¥ä¸ç­‰å¼äº¤æ¢å˜é‡â€£4.2ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹â€£4åœ¨è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ–â€£å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥")) çš„çº¿æ€§æ¾å¼›çš„æŠ•å½±ã€‚
- en: '(Projecting the big-$M$). Note that the LP relaxation given by ([4a](#S4.E4.1
    "In 4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")â€“[4d](#S4.E4.4 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact
    models using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) maintains the variables
    $z^{l}_{i}$ in the formulation, though they are no longer required to satisfy
    integrality. Since these variables are â€œauxiliaryâ€ and are no longer necessary
    to encode the nonconvexity of the problem, they can be projected out without altering
    the quality of the convex relaxation. Doing this yields what is commonly known
    as the â€œtriangleâ€ or â€œ$\Delta$â€ relaxation (Salman etÂ al., [2019](#bib.bib269)):'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆæŠ•å½±å¤§-$M$ï¼‰ã€‚è¯·æ³¨æ„ï¼Œç»™å®šçš„LPæ¾å¼›ï¼ˆ[4a](#S4.E4.1 "åœ¨4èŠ‚ â€£ 4.2.1èŠ‚ å¤§-ğ‘€å…¬å¼ â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹
    â€£ 4 åœ¨è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ– â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹ç»¼è¿°")â€“[4d](#S4.E4.4 "åœ¨4èŠ‚ â€£ 4.2.1èŠ‚ å¤§-ğ‘€å…¬å¼ â€£ 4.2
    ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹ â€£ 4 åœ¨è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ– â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹ç»¼è¿°")ï¼‰ä¿æŒäº†å…¬å¼ä¸­çš„å˜é‡$z^{l}_{i}$ï¼Œå°½ç®¡è¿™äº›å˜é‡ä¸å†éœ€è¦æ»¡è¶³æ•´æ•°æ€§ã€‚ç”±äºè¿™äº›å˜é‡æ˜¯â€œè¾…åŠ©â€çš„ï¼Œä¸å†å¿…è¦ç¼–ç é—®é¢˜çš„éå‡¸æ€§ï¼Œå®ƒä»¬å¯ä»¥åœ¨ä¸æ”¹å˜å‡¸æ¾å¼›è´¨é‡çš„æƒ…å†µä¸‹è¢«æŠ•å½±å‡ºå»ã€‚è¿™ä¼šäº§ç”Ÿé€šå¸¸æ‰€ç§°çš„â€œä¸‰è§’å½¢â€æˆ–â€œ$\Delta$â€æ¾å¼›ï¼ˆSalmanç­‰äººï¼Œ[2019](#bib.bib269)ï¼‰ã€‚
- en: '|  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (8a) |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (8a) |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\frac{M^{l}_{i,+}}{M^{l}_{i,+}-M^{l}_{i,-}}({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})$
    |  | (8b) |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\frac{M^{l}_{i,+}}{M^{l}_{i,+}-M^{l}_{i,-}}({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})$
    |  | (8b) |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}.$ |  | (8c) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}.$ |  | (8c) |'
- en: 'While the LP relaxation ([8](#S4.E8 "In 4.3.1 Projecting the big-ğ‘€ and ideal
    MILP formulations â€£ 4.3 Scaling further: Convex relaxations and linear programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) for an individual unit is compact, modern neural network architectures
    regularly comprise millions of units. The resulting LP relaxation for the entire
    network may then require millions of variables and constraints. Additionally,
    unless special precautions are taken, many of these constraints will be relatively
    dense. All this quickly leads to LP that are beyond the scope of modern off-the-shelf
    LP solvers. As a result, researchers have explored alternative schemes for scaling
    LP-based methods to these larger networks. Salman etÂ al. ([2019](#bib.bib269))
    present a framework for LP-based methods (LP solvers, propagation, dual methods),
    which we review in the following subsections. However, they do not account for
    the ideal formulation developed in later works (Anderson etÂ al., [2020](#bib.bib5),
    DeÂ Palma etÂ al., [2021](#bib.bib77)).'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å•ä¸ªå•å…ƒçš„LPæ¾å¼›ï¼ˆ[8](#S4.E8 "åœ¨4.3.1èŠ‚ä¸­æŠ•å½±å¤§-ğ‘€å’Œç†æƒ³MILPå…¬å¼ â€£ 4.3è¿›ä¸€æ­¥ç¼©æ”¾ï¼šå‡¸æ¾å¼›å’Œçº¿æ€§è§„åˆ’ â€£ 4 åœ¨è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ–
    â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹ç»¼è¿°")ï¼‰æ˜¯ç´§å‡‘çš„ï¼Œä½†ç°ä»£ç¥ç»ç½‘ç»œæ¶æ„é€šå¸¸åŒ…å«æ•°ç™¾ä¸‡ä¸ªå•å…ƒã€‚å› æ­¤ï¼Œæ•´ä¸ªç½‘ç»œçš„LPæ¾å¼›å¯èƒ½éœ€è¦æ•°ç™¾ä¸‡ä¸ªå˜é‡å’Œçº¦æŸã€‚æ­¤å¤–ï¼Œé™¤éé‡‡å–ç‰¹æ®Šé¢„é˜²æªæ–½ï¼Œå¦åˆ™è¿™äº›çº¦æŸä¸­çš„è®¸å¤šå°†æ˜¯ç›¸å¯¹å¯†é›†çš„ã€‚æ‰€æœ‰è¿™äº›å¾ˆå¿«å¯¼è‡´LPè¶…å‡ºç°ä»£ç°æˆLPæ±‚è§£å™¨çš„èŒƒå›´ã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜æ¢ç´¢äº†å°†åŸºäºLPçš„æ–¹æ³•æ‰©å±•åˆ°è¿™äº›æ›´å¤§ç½‘ç»œçš„æ›¿ä»£æ–¹æ¡ˆã€‚Salmanç­‰äººï¼ˆ[2019](#bib.bib269)ï¼‰æå‡ºäº†ä¸€ç§LPåŸºç¡€æ–¹æ³•ï¼ˆLPæ±‚è§£å™¨ã€ä¼ æ’­ã€å¯¹å¶æ–¹æ³•ï¼‰çš„æ¡†æ¶ï¼Œæˆ‘ä»¬å°†åœ¨ä»¥ä¸‹å°èŠ‚ä¸­å›é¡¾ã€‚ç„¶è€Œï¼Œä»–ä»¬æ²¡æœ‰è€ƒè™‘åœ¨åæ¥çš„å·¥ä½œä¸­å¼€å‘çš„ç†æƒ³å…¬å¼ï¼ˆAndersonç­‰äººï¼Œ[2020](#bib.bib5)ï¼ŒDe
    Palmaç­‰äººï¼Œ[2021](#bib.bib77)ï¼‰ã€‚
- en: '(Projecting the ideal). FigureÂ [9](#S4.F9 "Figure 9 â€£ 4.2.1 The big-ğ‘€ formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") shows
    that the triangle (big-$M$) relaxation fails to recover the convex hull of the
    ReLU activation function and the multivariate input to the affine pre-activation
    function. We can similarly project the LP relaxation of the ideal formulation
    ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading variables for inequalities
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) into
    the space of input/output variables (Anderson etÂ al., [2020](#bib.bib5)), yielding
    a description for the convex hull of $\{({\bm{h}}^{l-1},h^{l}_{i})|L^{l-1}\leq{\bm{h}}^{l-1}\leq
    U^{l-1},\>h^{l}_{i}=\sigma({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})\}$:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: (æŠ•å½±ç†æƒ³çš„). å›¾ [9](#S4.F9 "å›¾ 9 â€£ 4.2.1 å¤§-ğ‘€ å…¬å¼ â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹ â€£ åœ¨è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ–
    â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°") æ˜¾ç¤ºäº†ä¸‰è§’å½¢ (å¤§-$M$) æ¾å¼›æœªèƒ½æ¢å¤ ReLU æ¿€æ´»å‡½æ•°å’Œä»¿å°„å‰æ¿€æ´»å‡½æ•°çš„å¤šå˜é‡è¾“å…¥çš„å‡¸åŒ…ã€‚æˆ‘ä»¬å¯ä»¥ç±»ä¼¼åœ°å°†ç†æƒ³å…¬å¼çš„
    LP æ¾å¼› ([7](#S4.E7 "åœ¨ 4.2.4 åˆ‡å‰²å¹³é¢æ–¹æ³•ï¼šç”¨ä¸ç­‰å¼äº¤æ¢å˜é‡ â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹ â€£ åœ¨è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ– â€£
    å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) æŠ•å½±åˆ°è¾“å…¥/è¾“å‡ºå˜é‡çš„ç©ºé—´ (Anderson ç­‰äººï¼Œ[2020](#bib.bib5))ï¼Œä»è€Œä¸º $\{({\bm{h}}^{l-1},h^{l}_{i})|L^{l-1}\leq{\bm{h}}^{l-1}\leq
    U^{l-1},\>h^{l}_{i}=\sigma({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})\}$ çš„å‡¸åŒ…æä¾›æè¿°ï¼š
- en: '|  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (9a) |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (9a) |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{k\in I}w_{i,k}^{l}(x_{k}-\breve{L}_{k})+\frac{\ell(I)}{\breve{U}_{h}-\breve{L}_{h}}(x_{h}-\breve{L}_{h})\quad\forall(I,h)\in\mathcal{J}$
    |  | (9b) |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{k\in I}w_{i,k}^{l}(x_{k}-\breve{L}_{k})+\frac{\ell(I)}{\breve{U}_{h}-\breve{L}_{h}}(x_{h}-\breve{L}_{h})\quad\forall(I,h)\in\mathcal{J}$
    |  | (9b) |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0},$ |  | (9c) |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0},$ |  | (9c) |'
- en: where $l(I)\coloneqq\sum_{k\in I}w^{l}_{i,k}\breve{L}_{k}+\sum_{k\not\in I}w^{l}_{i,k}\breve{U}_{k}+b^{l}_{i}$
    and
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $l(I)\coloneqq\sum_{k\in I}w^{l}_{i,k}\breve{L}_{k}+\sum_{k\not\in I}w^{l}_{i,k}\breve{U}_{k}+b^{l}_{i}$
    å’Œ
- en: '|  | $\mathcal{J}\coloneqq\Set{(I,h)\in 2^{\llbracket n_{l-1}\rrbracket}\times\llbracket
    n_{l-1}\rrbracket}{l(I)\geq 0,\>l(I\cup\{h\}<0,\>w^{l}_{i,k}\neq 0\forall k\in
    I}.$ |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{J}\coloneqq\Set{(I,h)\in 2^{\llbracket n_{l-1}\rrbracket}\times\llbracket
    n_{l-1}\rrbracket}{l(I)\geq 0,\>l(I\cup\{h\}<0,\>w^{l}_{i,k}\neq 0\forall k\in
    I}.$ |  |'
- en: 'Anderson etÂ al. ([2020](#bib.bib5)) also show that the inequalities ([9b](#S4.E9.2
    "In 9 â€£ 4.3.1 Projecting the big-ğ‘€ and ideal MILP formulations â€£ 4.3 Scaling further:
    Convex relaxations and linear programming â€£ 4 Optimizing Over a Trained Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) can be separated
    over in $\mathcal{O}(n_{l-1})$ time. Interestingly, in contrast to ([7](#S4.E7
    "In 4.2.4 Cutting plane methods: Trading variables for inequalities â€£ 4.2 Exact
    models using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")), the number of facet-defining
    inequalities depends heavily on the affine function. While in the worst case the
    number of inequalities will grow exponentially in the input dimension, there exist
    instances where the convex hull can be fully described with only $\mathcal{O}(n_{l-1})$
    inequalities.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Anderson ç­‰äºº ([2020](#bib.bib5)) è¿˜æ˜¾ç¤ºè¿™äº›ä¸ç­‰å¼ ([9b](#S4.E9.2 "åœ¨ 9 â€£ 4.3.1 æŠ•å½±å¤§-ğ‘€ å’Œç†æƒ³çš„
    MILP å…¬å¼ â€£ 4.3 è¿›ä¸€æ­¥ç¼©æ”¾ï¼šå‡¸æ¾å¼›å’Œçº¿æ€§è§„åˆ’ â€£ åœ¨è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ– â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) å¯ä»¥åœ¨ $\mathcal{O}(n_{l-1})$
    æ—¶é—´å†…åˆ†ç¦»ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä¸ ([7](#S4.E7 "åœ¨ 4.2.4 åˆ‡å‰²å¹³é¢æ–¹æ³•ï¼šç”¨ä¸ç­‰å¼äº¤æ¢å˜é‡ â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹ â€£ åœ¨è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸Šä¼˜åŒ–
    â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) ç›¸æ¯”ï¼Œä¸ç­‰å¼çš„æ•°é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºä»¿å°„å‡½æ•°ã€‚è™½ç„¶åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œä¸ç­‰å¼çš„æ•°é‡ä¼šéšç€è¾“å…¥ç»´åº¦å‘ˆæŒ‡æ•°å¢é•¿ï¼Œä½†ä¹Ÿå­˜åœ¨è¿™æ ·çš„å®ä¾‹ï¼Œå…¶ä¸­å‡¸åŒ…å¯ä»¥ç”¨ä»…
    $\mathcal{O}(n_{l-1})$ ä¸ªä¸ç­‰å¼å®Œå…¨æè¿°ã€‚
- en: 4.3.2 Dual decomposition methods
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 å¯¹å¶åˆ†è§£æ–¹æ³•
- en: 'A first approach for greater scalability for LP-based methods is decomposition,
    a standard technique in the large-scale optimization community. Indeed, the cutting
    plane approach of SectionÂ [4.2.4](#S4.SS2.SSS4 "4.2.4 Cutting plane methods: Trading
    variables for inequalities â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey") can be viewed as a decomposition method operating in the original
    variable space. However, the method is initialized with the big-$M$ formulation
    for each neuron, and hence this initial model will be of size roughly equal to
    the size of the network. Therefore, it should be understood to use decomposition
    to provide a tighter verification bound, rather than for providing greater scalability
    to larger networks.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äºåŸºäºçº¿æ€§è§„åˆ’ï¼ˆLPï¼‰çš„æ–¹æ³•ï¼Œæå‡å¯æ‰©å±•æ€§çš„ç¬¬ä¸€ç§æ–¹æ³•æ˜¯åˆ†è§£ï¼Œè¿™æ˜¯å¤§è§„æ¨¡ä¼˜åŒ–é¢†åŸŸçš„æ ‡å‡†æŠ€æœ¯ã€‚å®é™…ä¸Šï¼Œ[4.2.4èŠ‚](#S4.SS2.SSS4 "4.2.4
    Cutting plane methods: Trading variables for inequalities â€£ 4.2 Exact models using
    mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When
    Deep Learning Meets Polyhedral Theory: A Survey")ä¸­çš„åˆ‡å¹³é¢æ–¹æ³•å¯ä»¥è§†ä¸ºåœ¨åŸå§‹å˜é‡ç©ºé—´ä¸­æ“ä½œçš„åˆ†è§£æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¯¥æ–¹æ³•ä»¥æ¯ä¸ªç¥ç»å…ƒçš„â€œå¤§$M$â€å…¬å¼åˆå§‹åŒ–ï¼Œå› æ­¤æ­¤åˆå§‹æ¨¡å‹çš„å¤§å°å¤§è‡´ç­‰äºç½‘ç»œçš„å¤§å°ã€‚å› æ­¤ï¼Œåº”ç†è§£ä¸ºä½¿ç”¨åˆ†è§£æ¥æä¾›æ›´ç´§çš„éªŒè¯ç•Œé™ï¼Œè€Œä¸æ˜¯ä¸ºäº†æä¾›æ›´å¤§çš„ç½‘ç»œå¯æ‰©å±•æ€§ã€‚'
- en: In contrast, dual decomposition can be used to scale inexact verification methods
    to larger networks. Such methods maintain dual feasible solutions throughout the
    algorithm, meaning that upon termination they yield valid dual bounds on the verification
    instance, and hence serve as incomplete verifiers.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŒé‡åˆ†è§£å¯ä»¥ç”¨äºå°†ä¸ç²¾ç¡®çš„éªŒè¯æ–¹æ³•æ‰©å±•åˆ°æ›´å¤§çš„ç½‘ç»œã€‚è¿™äº›æ–¹æ³•åœ¨æ•´ä¸ªç®—æ³•è¿‡ç¨‹ä¸­ä¿æŒåŒé‡å¯è¡Œè§£ï¼Œè¿™æ„å‘³ç€åœ¨ç»ˆæ­¢æ—¶ï¼Œå®ƒä»¬ä¼šå¯¹éªŒè¯å®ä¾‹äº§ç”Ÿæœ‰æ•ˆçš„åŒé‡ç•Œé™ï¼Œä»è€Œä½œä¸ºä¸å®Œå…¨éªŒè¯å™¨ã€‚
- en: 'Wong and Kolter ([2018](#bib.bib335)), Wong etÂ al. ([2018](#bib.bib336)) use
    as their starting point the triangle relaxation ([8](#S4.E8 "In 4.3.1 Projecting
    the big-ğ‘€ and ideal MILP formulations â€£ 4.3 Scaling further: Convex relaxations
    and linear programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")) for each neuron, and then take the
    standard LP dual of the (relaxed) verification problem. Alternatively, Dvijotham
    etÂ al. ([2018b](#bib.bib85)) propose a Lagrangian-based approach for decomposing
    the original nonlinear formulation of the problem ([3](#S4.E3 "In Example 4 â€£
    4.1.1 Neural network verification â€£ 4.1 Applications of optimization over trained
    networks â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")). Crucially, since the complicating constraints
    coupling the layers in the network are imposed as objective penalties instead
    of â€œhardâ€ constraints, the optimization problem (given fixed dual variables) decomposes
    along each layer and the subproblems induced by the separability can be solved
    in closed form. This approach dualizes separately the equations characterizing
    the pre-activation and post-activation functions:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wong å’Œ Kolter ([2018](#bib.bib335))ã€Wong ç­‰ ([2018](#bib.bib336)) ä»¥æ¯ä¸ªç¥ç»å…ƒçš„ä¸‰è§’æ¾å¼›
    ([8](#S4.E8 "In 4.3.1 Projecting the big-ğ‘€ and ideal MILP formulations â€£ 4.3 Scaling
    further: Convex relaxations and linear programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) ä½œä¸ºèµ·ç‚¹ï¼Œç„¶åå–æ ‡å‡†çš„
    LP å¯¹å¶ï¼ˆæ¾å¼›ï¼‰éªŒè¯é—®é¢˜ã€‚æˆ–è€…ï¼ŒDvijotham ç­‰ ([2018b](#bib.bib85)) æå‡ºäº†åŸºäºæ‹‰æ ¼æœ—æ—¥æ–¹æ³•çš„åˆ†è§£åŸå§‹éçº¿æ€§é—®é¢˜å…¬å¼çš„æ–¹æ³•
    ([3](#S4.E3 "In Example 4 â€£ 4.1.1 Neural network verification â€£ 4.1 Applications
    of optimization over trained networks â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey"))ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œç”±äºè€¦åˆç½‘ç»œå±‚çš„å¤æ‚çº¦æŸè¢«ä½œä¸ºç›®æ ‡æƒ©ç½šè€Œéâ€œç¡¬â€çº¦æŸæ–½åŠ ï¼Œå› æ­¤ä¼˜åŒ–é—®é¢˜ï¼ˆåœ¨å›ºå®šçš„å¯¹å¶å˜é‡ä¸‹ï¼‰æ²¿æ¯å±‚åˆ†è§£ï¼Œåˆ†ç¦»æ€§å¼•èµ·çš„å­é—®é¢˜å¯ä»¥ç”¨é—­å¼è§£æ³•è§£å†³ã€‚è¿™ç§æ–¹æ³•å°†æè¿°å‰æ¿€æ´»å’Œåæ¿€æ´»å‡½æ•°çš„æ–¹ç¨‹åˆ†åˆ«å¯¹å¶åŒ–ã€‚'
- en: '|  | $\displaystyle\max_{\mu,\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\mu_{k}^{T}(\hat{{\bm{h}}}^{k}-{\bm{W}}^{k}{\bm{h}}^{k-1}-{\bm{b}}^{k})+\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\mu,\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\mu_{k}^{T}(\hat{{\bm{h}}}^{k}-{\bm{W}}^{k}{\bm{h}}^{k-1}-{\bm{b}}^{k})+\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
- en: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
- en: '|  |  | $\displaystyle\sigma(L^{k})\leq{\bm{h}}^{k}\leq\sigma(U^{k})\quad\forall
    k\in\llbracket n-1\rrbracket.$ |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\sigma(L^{k})\leq{\bm{h}}^{k}\leq\sigma(U^{k})\quad\forall
    k\in\llbracket n-1\rrbracket.$ |  |'
- en: Here, the $\hat{{\bm{h}}}$ variables track the pre-activation values for the
    neurons in the network. The dual variables $\mu_{k}^{T}$ correspond to the equality
    constraints defining the pre-activation values, $\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+{\bm{b}}^{k}$.
    Likewise, the dual variables $\lambda_{k}^{T}$ correspond to enforcing the ReLU
    activation function, ${\bm{h}}^{k}=\sigma(\hat{{\bm{h}}}^{k})=\mathrm{max}(0,\hat{{\bm{h}}}^{k})$.
    Any feasible solution for the neural network is feasible for this dualized problem,
    making the multiplier terms for $\mu_{k}^{T}$ and $\lambda_{k}^{T}$ zero. Thus,
    the inner problem gives a lower bound for the original problemâ€”a property known
    as *weak duality*. The outer (dual) problem optimizing over the Lagrangian multipliers
    then seeks to maximize this lower bound, i.e., to give the tightest possible lower
    bound. This can be solved using a subgradient-based method, or learned along with
    the model parameters in a â€œpredictor-verifierâ€ approach (Dvijotham etÂ al., [2018a](#bib.bib84)).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ$\hat{{\bm{h}}}$ å˜é‡è·Ÿè¸ªç½‘ç»œä¸­ç¥ç»å…ƒçš„é¢„æ¿€æ´»å€¼ã€‚å¯¹å¶å˜é‡ $\mu_{k}^{T}$ å¯¹åº”äºå®šä¹‰é¢„æ¿€æ´»å€¼çš„ç­‰å¼çº¦æŸï¼Œå³ $\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+{\bm{b}}^{k}$ã€‚åŒæ ·ï¼Œå¯¹å¶å˜é‡
    $\lambda_{k}^{T}$ å¯¹åº”äºå¼ºåˆ¶å®æ–½ ReLU æ¿€æ´»å‡½æ•°ï¼Œå³ ${\bm{h}}^{k}=\sigma(\hat{{\bm{h}}}^{k})=\mathrm{max}(0,\hat{{\bm{h}}}^{k})$ã€‚ç¥ç»ç½‘ç»œçš„ä»»ä½•å¯è¡Œè§£éƒ½æ˜¯è¿™ä¸ªå¯¹å¶åŒ–é—®é¢˜çš„å¯è¡Œè§£ï¼Œä½¿å¾—
    $\mu_{k}^{T}$ å’Œ $\lambda_{k}^{T}$ çš„ä¹˜å­é¡¹ä¸ºé›¶ã€‚å› æ­¤ï¼Œå†…å±‚é—®é¢˜ä¸ºåŸå§‹é—®é¢˜æä¾›äº†ä¸€ä¸ªä¸‹ç•Œï¼Œè¿™ä¸€ç‰¹æ€§ç§°ä¸º*å¼±å¯¹å¶æ€§*ã€‚å¤–å±‚ï¼ˆå¯¹å¶ï¼‰é—®é¢˜é€šè¿‡ä¼˜åŒ–æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ¥æœ€å¤§åŒ–è¿™ä¸ªä¸‹ç•Œï¼Œå³æä¾›æœ€ç´§çš„ä¸‹ç•Œã€‚è¿™å¯ä»¥é€šè¿‡åŸºäºæ¬¡æ¢¯åº¦çš„æ–¹æ³•è§£å†³ï¼Œæˆ–è€…ä¸æ¨¡å‹å‚æ•°ä¸€èµ·ä»¥â€œé¢„æµ‹-éªŒè¯è€…â€æ–¹æ³•å­¦ä¹ ï¼ˆDvijotham
    ç­‰ï¼Œ[2018a](#bib.bib84)ï¼‰ã€‚
- en: 'On the other hand, this approach can be combined with other relaxation-based
    methods. The Lagrangian decomposition can be applied to dualize only the coupling
    constraints between layers, and a convex relaxation used for the activation function
    (Bunel etÂ al., [2020a](#bib.bib41)):'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥ä¸å…¶ä»–åŸºäºæ¾å¼›çš„æ–¹æ³•ç»“åˆã€‚æ‹‰æ ¼æœ—æ—¥åˆ†è§£å¯ä»¥ä»…å¯¹å±‚ä¹‹é—´çš„è€¦åˆçº¦æŸè¿›è¡Œå¯¹å¶åŒ–ï¼Œå¹¶å¯¹æ¿€æ´»å‡½æ•°ä½¿ç”¨å‡¸æ¾å¼›ï¼ˆBunel ç­‰ï¼Œ[2020a](#bib.bib41)ï¼‰ï¼š
- en: '|  | $\displaystyle\max_{\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
- en: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+b^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+b^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
- en: '|  |  | $\displaystyle{\bm{h}}^{k}\geq 0\quad\forall k\in\llbracket n-1\rrbracket$
    |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{h}}^{k}\geq 0\quad\forall k\in\llbracket n-1\rrbracket$
    |  |'
- en: '|  |  | $\displaystyle{\bm{h}}_{i}^{k}\geq\hat{{\bm{h}}}_{i}^{k}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket$ |  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{h}}_{i}^{k}\geq\hat{{\bm{h}}}_{i}^{k}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket$ |  |'
- en: '|  |  | $\displaystyle{\bm{h}}^{k}_{i}\leq\frac{U^{k}_{i}(\hat{{\bm{h}}}^{k}_{i}-L^{k}_{i})}{U^{k}_{i}-L^{k}_{i}}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket.$ |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{h}}^{k}_{i}\leq\frac{U^{k}_{i}(\hat{{\bm{h}}}^{k}_{i}-L^{k}_{i})}{U^{k}_{i}-L^{k}_{i}}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket.$ |  |'
- en: 'Note that the final three constraints apply the big-$M$/triangle relaxation
    ([8](#S4.E8 "In 4.3.1 Projecting the big-ğ‘€ and ideal MILP formulations â€£ 4.3 Scaling
    further: Convex relaxations and linear programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) to each
    ReLU activation function. The dual problem can then be solved via subgradient-based
    methods, proximal algorithms, or, more recently, a projected gradient descent
    method applied to a nonconvex reformulation of the problem (Bunel etÂ al., [2020c](#bib.bib44)).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæœ€åä¸‰ä¸ªçº¦æŸå¯¹æ¯ä¸ª ReLU æ¿€æ´»å‡½æ•°åº”ç”¨äº†å¤§-$M$/ä¸‰è§’å½¢æ¾å¼›ï¼ˆ[8](#S4.E8 "åœ¨ 4.3.1 ä¸­æŠ•å½±å¤§-ğ‘€ å’Œç†æƒ³ MILP å…¬å¼
    â€£ 4.3 è¿›ä¸€æ­¥ç¼©æ”¾ï¼šå‡¸æ¾å¼›å’Œçº¿æ€§è§„åˆ’ â€£ 4 ä¼˜åŒ–è®­ç»ƒç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰ã€‚å¯¹å¶é—®é¢˜éšåå¯ä»¥é€šè¿‡æ¬¡æ¢¯åº¦æ–¹æ³•ã€è¿‘ç«¯ç®—æ³•ï¼Œæˆ–è€…æ›´è¿‘æœŸçš„ï¼Œé€šè¿‡åº”ç”¨äºé—®é¢˜çš„éå‡¸é‡æ•´çš„æŠ•å½±æ¢¯åº¦ä¸‹é™æ–¹æ³•æ¥è§£å†³ï¼ˆBunel
    ç­‰ï¼Œ[2020c](#bib.bib44)ï¼‰ã€‚
- en: 'More recently, DeÂ Palma etÂ al. ([2021](#bib.bib77)) presented a dual decomposition
    approach based on ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading variables
    for inequalities â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")). However, creating a dual formulation from the exponential number of
    constraints produces an exponential number of dual variables. The authors therefore
    propose to maintain an â€œactive setâ€ of dual variables to keep the problem sparse.
    A selection algorithm (e.g., selecting entries that maximize an estimated super-gradient)
    can then be used to append the active set. Similar to the above discussion on
    cut generation, the frequency of appending the active set should be chosen strategically.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ€è¿‘ï¼ŒDe Palmaç­‰([2021](#bib.bib77))æå‡ºäº†ä¸€ç§åŸºäº([7](#S4.E7 "In 4.2.4 Cutting plane
    methods: Trading variables for inequalities â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey"))çš„å¯¹å¶åˆ†è§£æ–¹æ³•ã€‚ç„¶è€Œï¼Œä»æŒ‡æ•°æ•°é‡çš„çº¦æŸä¸­åˆ›å»ºå¯¹å¶å½¢å¼ä¼šäº§ç”ŸæŒ‡æ•°æ•°é‡çš„å¯¹å¶å˜é‡ã€‚å› æ­¤ï¼Œä½œè€…å»ºè®®ç»´æŒä¸€ä¸ªâ€œæ´»è·ƒé›†â€çš„å¯¹å¶å˜é‡ä»¥ä¿æŒé—®é¢˜çš„ç¨€ç–æ€§ã€‚ç„¶åå¯ä»¥ä½¿ç”¨é€‰æ‹©ç®—æ³•ï¼ˆä¾‹å¦‚ï¼Œé€‰æ‹©æœ€å¤§åŒ–ä¼°è®¡è¶…æ¢¯åº¦çš„æ¡ç›®ï¼‰æ¥æ‰©å±•æ´»è·ƒé›†ã€‚ç±»ä¼¼äºä¸Šè¿°å…³äºå‰ªåˆ‡ç”Ÿæˆçš„è®¨è®ºï¼Œæ‰©å±•æ´»è·ƒé›†çš„é¢‘ç‡åº”å½“ç­–ç•¥æ€§åœ°é€‰æ‹©ã€‚'
- en: 4.3.3 Fourier-Motzkin elimination and propagation algorithms
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 å‚…é‡Œå¶-è«å…¹é‡‘æ¶ˆå…ƒæ³•å’Œä¼ æ’­ç®—æ³•
- en: 'Alternatively, one can project out *all* of the decision variables. For example,
    in order to solve the linear programming problem $\min_{x\in\mathcal{X}}c\cdot
    x$, we can augment the problem with a new decision variable to $\min_{(x,y)\in\Gamma}y$
    for $\Gamma\vcentcolon=\Set{(x,y)\in\mathcal{X}\times\mathbb{R}:y=c\cdot x}$,
    and project out the $x$ variables. The transformed problem is the a trivial univariate
    optimization problem: $\min_{y\in\operatorname{Proj}_{y}(\Gamma)}y$.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œå¯ä»¥æŠ•å½±*æ‰€æœ‰*å†³ç­–å˜é‡ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†è§£å†³çº¿æ€§è§„åˆ’é—®é¢˜$\min_{x\in\mathcal{X}}c\cdot x$ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªæ–°çš„å†³ç­–å˜é‡æ¥æ‰©å±•é—®é¢˜ä¸º$\min_{(x,y)\in\Gamma}y$ï¼Œå…¶ä¸­$\Gamma\vcentcolon=\Set{(x,y)\in\mathcal{X}\times\mathbb{R}:y=c\cdot
    x}$ï¼Œå¹¶æŠ•å½±å‡º$x$å˜é‡ã€‚è½¬åŒ–åçš„é—®é¢˜æ˜¯ä¸€ä¸ªç®€å•çš„ä¸€ç»´ä¼˜åŒ–é—®é¢˜ï¼š$\min_{y\in\operatorname{Proj}_{y}(\Gamma)}y$ã€‚
- en: 'Of course, the complexity of the approach described is hidden in the projection
    step, or building $\operatorname{Proj}_{y}(\Gamma)$. The most well-known algorithm
    for computing projections of linear inequality systems is Fourier-Motzkin elimination,
    described by Dantzig and Eaves ([1973](#bib.bib74)), which is notorious for its
    practical inefficiency. The process effectively comprises replacing variables
    from a set of inequalities with all possible implied inequalities, which can produce
    many unnecessary constraints. However, it turns out that neural network verification
    problems are well-structured in such a way that Fourier-Motzkin elimination can
    be performed very efficiently: for instance, by imposing one inequality upper
    bounding and one inequality lower bounding each ReLU function. Note that while
    Section [3.2](#S3.SS2 "3.2 The algebra of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") describes
    the use of Fourier-Motzkin elimination to obtain *exact* input-output relationships
    in linear regions of neural networks, here we are interested in obtaining linear
    *bounds* for a nonlinear function.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 'å½“ç„¶ï¼Œæè¿°çš„æ–¹æ³•çš„å¤æ‚æ€§éšè—åœ¨æŠ•å½±æ­¥éª¤ä¸­ï¼Œå³æ„å»º$\operatorname{Proj}_{y}(\Gamma)$ã€‚è®¡ç®—çº¿æ€§ä¸ç­‰å¼ç³»ç»ŸæŠ•å½±çš„æœ€è‘—åç®—æ³•æ˜¯å‚…é‡Œå¶-è«å…¹é‡‘æ¶ˆå…ƒæ³•ï¼Œç”±ä¸¹é½æ ¼å’Œä¼Šå¤«æ–¯([1973](#bib.bib74))æè¿°ï¼Œè¯¥ç®—æ³•å› å®é™…æ•ˆç‡ä½ä¸‹è€Œè‡­åæ˜­è‘—ã€‚è¯¥è¿‡ç¨‹å®é™…ä¸ŠåŒ…æ‹¬å°†ä¸€ç»„ä¸ç­‰å¼ä¸­çš„å˜é‡æ›¿æ¢ä¸ºæ‰€æœ‰å¯èƒ½çš„éšå«ä¸ç­‰å¼ï¼Œè¿™å¯èƒ½ä¼šäº§ç”Ÿè®¸å¤šä¸å¿…è¦çš„çº¦æŸã€‚ç„¶è€Œï¼Œäº‹å®è¯æ˜ï¼Œç¥ç»ç½‘ç»œéªŒè¯é—®é¢˜çš„ç»“æ„ä½¿å¾—å‚…é‡Œå¶-è«å…¹é‡‘æ¶ˆå…ƒæ³•å¯ä»¥éå¸¸é«˜æ•ˆåœ°æ‰§è¡Œï¼šä¾‹å¦‚ï¼Œé€šè¿‡å¯¹æ¯ä¸ªReLUå‡½æ•°æ–½åŠ ä¸€ä¸ªä¸ç­‰å¼ä¸Šç•Œå’Œä¸€ä¸ªä¸ç­‰å¼ä¸‹ç•Œã€‚è¯·æ³¨æ„ï¼Œè™½ç„¶[3.2](#S3.SS2
    "3.2 The algebra of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")èŠ‚æè¿°äº†ä½¿ç”¨å‚…é‡Œå¶-è«å…¹é‡‘æ¶ˆå…ƒæ³•æ¥è·å¾—ç¥ç»ç½‘ç»œçº¿æ€§åŒºåŸŸä¸­çš„*ç²¾ç¡®*è¾“å…¥è¾“å‡ºå…³ç³»ï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯ä¸ºéçº¿æ€§å‡½æ•°è·å¾—çº¿æ€§*ç•Œé™*ã€‚'
- en: '![Refer to caption](img/123fb03cab46add54faaa496e472ed4b.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/123fb03cab46add54faaa496e472ed4b.png)'
- en: 'Figure 10: Convex approximations for the ReLU function commonly used by propagation
    algorithms, given as a function of the preactivation function $\hat{h_{i}^{l}}$.
    The ReLU applies $h_{i}^{l}=\max(0,\hat{h_{i}^{l}})$.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10ï¼šç»™å®šä¸ºé¢„æ¿€æ´»å‡½æ•°$\hat{h_{i}^{l}}$çš„å‡½æ•°ï¼ŒReLUå‡½æ•°çš„å‡¸è¿‘ä¼¼ï¼Œé€šå¸¸ç”±ä¼ æ’­ç®—æ³•ä½¿ç”¨ã€‚ReLUåº”ç”¨$h_{i}^{l}=\max(0,\hat{h_{i}^{l}})$ã€‚
- en: 'In fact, this general approach was independently developed in the verification
    community. While MILP research has focused on formulations tighter than the big-M,
    such as ([5](#S4.E5 "In 4.2.2 A stronger extended formulation â€£ 4.2 Exact models
    using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£
    When Deep Learning Meets Polyhedral Theory: A Survey")) and ([6](#S4.E6 "In 4.2.3
    A class of intermediate formulations â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")), the verification community often prefers greater scalability
    at the price of weaker convex relaxations. The continuous relaxation of the big-M
    is equivalent to the triangle relaxation ([8](#S4.E8 "In 4.3.1 Projecting the
    big-ğ‘€ and ideal MILP formulations â€£ 4.3 Scaling further: Convex relaxations and
    linear programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")): the optimal convex relaxation for a single
    input, or in terms of the aggregated pre-activation function, as shown in Figure
    [10](#S4.F10 "Figure 10 â€£ 4.3.3 Fourier-Motzkin elimination and propagation algorithms
    â€£ 4.3 Scaling further: Convex relaxations and linear programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey"). However, the lower bound involves two linear constraints, which is not
    used in several propagation-based verification tools owing to scalability or compatibility.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™ç§é€šç”¨æ–¹æ³•åœ¨éªŒè¯ç¤¾åŒºä¸­æ˜¯ç‹¬ç«‹å¼€å‘çš„ã€‚è™½ç„¶ MILP ç ”ç©¶é›†ä¸­åœ¨æ¯”å¤§-M æ›´ç´§å‡‘çš„å½¢å¼ä¸Šï¼Œä¾‹å¦‚ ([5](#S4.E5 "åœ¨ 4.2.2 æ›´å¼ºçš„æ‰©å±•å½¢å¼
    â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°ç¼–ç¨‹çš„ç²¾ç¡®æ¨¡å‹ â€£ 4 ä¼˜åŒ–ç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) å’Œ ([6](#S4.E6 "åœ¨ 4.2.3
    ä¸€ç±»ä¸­é—´å½¢å¼ â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°ç¼–ç¨‹çš„ç²¾ç¡®æ¨¡å‹ â€£ 4 ä¼˜åŒ–ç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ï¼Œä½†éªŒè¯ç¤¾åŒºé€šå¸¸æ›´å€¾å‘äºæ›´é«˜çš„å¯æ‰©å±•æ€§ï¼Œå³ä½¿è¿™æ„å‘³ç€æ›´å¼±çš„å‡¸æ¾å¼›ã€‚å¤§-M
    çš„è¿ç»­æ¾å¼›ç­‰åŒäºä¸‰è§’æ¾å¼› ([8](#S4.E8 "åœ¨ 4.3.1 æŠ•å½±å¤§-ğ‘€ å’Œç†æƒ³ MILP å½¢å¼ â€£ 4.3 è¿›ä¸€æ­¥æ‰©å±•ï¼šå‡¸æ¾å¼›å’Œçº¿æ€§è§„åˆ’ â€£ 4 ä¼˜åŒ–ç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œ
    â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ï¼šè¿™æ˜¯é’ˆå¯¹å•ä¸€è¾“å…¥çš„æœ€ä¼˜å‡¸æ¾å¼›ï¼Œæˆ–è€…ä»èšåˆçš„é¢„æ¿€æ´»å‡½æ•°æ¥çœ‹ï¼Œå¦‚å›¾ [10](#S4.F10 "å›¾ 10 â€£ 4.3.3
    Fourier-Motzkin æ¶ˆé™¤å’Œä¼ æ’­ç®—æ³• â€£ 4.3 è¿›ä¸€æ­¥æ‰©å±•ï¼šå‡¸æ¾å¼›å’Œçº¿æ€§è§„åˆ’ â€£ 4 ä¼˜åŒ–ç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")
    æ‰€ç¤ºã€‚ç„¶è€Œï¼Œä¸‹ç•Œæ¶‰åŠä¸¤ä¸ªçº¿æ€§çº¦æŸï¼Œè¿™åœ¨å‡ ä¸ªåŸºäºä¼ æ’­çš„éªŒè¯å·¥å…·ä¸­æœªè¢«ä½¿ç”¨ï¼ŒåŸå› æ˜¯å¯æ‰©å±•æ€§æˆ–å…¼å®¹æ€§é—®é¢˜ã€‚
- en: 'Such tools use methods such as abstract transformers to propagate polyhedral
    bounds, i.e., zonotopes, through the layers of a neural network. DeepZ (Singh
    etÂ al., [2018](#bib.bib289)), Fast-Lin (Weng etÂ al., [2018](#bib.bib330)), and
    Neurify (Wang etÂ al., [2018a](#bib.bib326)) employ a parallel approximation, with
    the latter also implementing a branch-and-bound procedure towards completeness.
    Subsequently, DeepPoly (Singh etÂ al., [2019b](#bib.bib291)) and CROWN (Zhang etÂ al.,
    [2018a](#bib.bib354)) select between the zero and identity approximations by minimizing
    over-approximation area. OSIP (Hashemi etÂ al., [2021](#bib.bib140)) selects between
    the three approximations using optimization: approximations for a layer are select
    jointly to minimise bounds for the following layer. These technologies are also
    compatible with interval bounds, propagating box domains (Mirman etÂ al., [2018](#bib.bib220)).
    Interestingly, bounds on neural network weights can also be propagated using similar
    methods, allowing reachability analysis of Bayesian neural networks (Wicker etÂ al.,
    [2020](#bib.bib332)).'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç±»å·¥å…·ä½¿ç”¨è¯¸å¦‚æŠ½è±¡å˜æ¢å™¨çš„æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œçš„å±‚çº§ä¼ æ’­å¤šé¢ä½“è¾¹ç•Œï¼Œå³ï¼Œzonotopesã€‚DeepZï¼ˆSingh ç­‰ï¼Œ[2018](#bib.bib289)ï¼‰ã€Fast-Linï¼ˆWeng
    ç­‰ï¼Œ[2018](#bib.bib330)ï¼‰å’Œ Neurifyï¼ˆWang ç­‰ï¼Œ[2018a](#bib.bib326)ï¼‰é‡‡ç”¨å¹¶è¡Œè¿‘ä¼¼æ–¹æ³•ï¼Œåè€…è¿˜å®ç°äº†ä¸€ä¸ªåˆ†æ”¯å®šç•Œç¨‹åºä»¥å®ç°å®Œæ•´æ€§ã€‚éšåï¼ŒDeepPolyï¼ˆSingh
    ç­‰ï¼Œ[2019b](#bib.bib291)ï¼‰å’Œ CROWNï¼ˆZhang ç­‰ï¼Œ[2018a](#bib.bib354)ï¼‰é€šè¿‡æœ€å°åŒ–è¿‡åº¦è¿‘ä¼¼é¢ç§¯æ¥é€‰æ‹©é›¶è¿‘ä¼¼å’Œå•ä½è¿‘ä¼¼ã€‚OSIPï¼ˆHashemi
    ç­‰ï¼Œ[2021](#bib.bib140)ï¼‰ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•åœ¨ä¸‰ç§è¿‘ä¼¼ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼šä¸€å±‚çš„è¿‘ä¼¼æ˜¯è”åˆé€‰æ‹©çš„ï¼Œä»¥æœ€å°åŒ–ä¸‹ä¸€å±‚çš„è¾¹ç•Œã€‚è¿™äº›æŠ€æœ¯ä¹Ÿå…¼å®¹åŒºé—´è¾¹ç•Œï¼Œä¼ æ’­ç›’åŸŸï¼ˆMirman
    ç­‰ï¼Œ[2018](#bib.bib220)ï¼‰ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ç±»ä¼¼çš„æ–¹æ³•ä¼ æ’­ç¥ç»ç½‘ç»œæƒé‡çš„è¾¹ç•Œï¼Œä»è€Œå®ç°è´å¶æ–¯ç¥ç»ç½‘ç»œçš„å¯è¾¾æ€§åˆ†æï¼ˆWicker ç­‰ï¼Œ[2020](#bib.bib332)ï¼‰ã€‚
- en: 'Tjandraatmadja etÂ al. ([2020](#bib.bib308)) provide an interpretation of these
    propagation techniques through the lens of Fourier-Motzkin elimination. Consider
    the problem of propagating bounds through a ReLU neural network: for a node $h_{i}^{l}=\mathrm{max}\{0,\hat{h_{i}^{l}}\}$,
    convex bounds for $h_{i}^{l}$ can be obtained given bounds for $\hat{h}_{i}^{l}$
    (Figure [10](#S4.F10 "Figure 10 â€£ 4.3.3 Fourier-Motzkin elimination and propagation
    algorithms â€£ 4.3 Scaling further: Convex relaxations and linear programming â€£
    4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")). Assuming the inputs are outputs of ReLU activations in the
    previous layer, $\hat{h}_{i}^{l}={\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$. Computing
    an upper bound can then be expressed as:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 'Tjandraatmadja ç­‰äºº ([2020](#bib.bib308)) é€šè¿‡å‚…é‡Œå¶-è«èŒ¨é‡‘æ¶ˆé™¤æ³•çš„è§†è§’è§£é‡Šäº†è¿™äº›ä¼ æ’­æŠ€æœ¯ã€‚è€ƒè™‘é€šè¿‡ ReLU
    ç¥ç»ç½‘ç»œä¼ æ’­ç•Œé™çš„é—®é¢˜ï¼šå¯¹äºä¸€ä¸ªèŠ‚ç‚¹ $h_{i}^{l}=\mathrm{max}\{0,\hat{h_{i}^{l}}\}$ï¼Œå¯ä»¥æ ¹æ® $\hat{h}_{i}^{l}$
    çš„ç•Œé™è·å¾— $h_{i}^{l}$ çš„å‡¸ç•Œé™ï¼ˆå›¾ [10](#S4.F10 "Figure 10 â€£ 4.3.3 Fourier-Motzkin elimination
    and propagation algorithms â€£ 4.3 Scaling further: Convex relaxations and linear
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")ï¼‰ã€‚å‡è®¾è¾“å…¥æ˜¯å‰ä¸€å±‚ ReLU æ¿€æ´»çš„è¾“å‡ºï¼Œ$\hat{h}_{i}^{l}={\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$ã€‚ç„¶åï¼Œä¸Šç•Œçš„è®¡ç®—å¯ä»¥è¡¨ç¤ºä¸ºï¼š'
- en: '|  | $\displaystyle\max_{{\bm{h}}^{l-1}}$ | $\displaystyle{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$
    |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{{\bm{h}}^{l-1}}$ | $\displaystyle{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$
    |  |'
- en: '|  | s.t. | $\displaystyle\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})\forall
    k\in\{1,...,n_{l-1}\}.$ |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})\forall
    k\in\{1,...,n_{l-1}\}.$ |  |'
- en: 'As the objective function is linear, the solution of this problem can be computed
    by propagation without explicit optimization. For each element in ${\bm{h}}^{l-1}$,
    we only need to consider the associated objective coefficient in ${\bm{w}}_{i}^{l}$
    to determine whether $\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}$ or $h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})$
    will be the active inequality at the optimal solution. We can thus replace $h^{l-1}_{k}$
    with $\mathcal{L}_{k}({\bm{h}}^{l-2})$ or $\mathcal{G}_{k}({\bm{h}}^{l-2})$ accordingly.
    This projection is mathematically equivalent to applying Fourier-Motzkin elimination,
    while avoiding redundant inequalities resulting from the â€˜non-selectedâ€™ bounding
    function. Repeating this procedure for each layer results in a convex relaxation
    for the outputs that only involves the input variables. We naturally observe the
    desirability of simple lower bounds $\mathcal{L}_{k}({\bm{h}}^{l-2})$: imposing
    two-part lower bounds in each layer would increase the number of propagated constraints
    in an exponential manner, similar to Fourier-Motzkin elimination.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç›®æ ‡å‡½æ•°æ˜¯çº¿æ€§çš„ï¼Œè¿™ä¸ªé—®é¢˜çš„è§£å¯ä»¥é€šè¿‡ä¼ æ’­æ¥è®¡ç®—ï¼Œè€Œæ— éœ€æ˜¾å¼ä¼˜åŒ–ã€‚å¯¹äº ${\bm{h}}^{l-1}$ ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬åªéœ€è€ƒè™‘ ${\bm{w}}_{i}^{l}$
    ä¸­çš„ç›¸å…³ç›®æ ‡ç³»æ•°ï¼Œä»¥ç¡®å®š $\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}$ è¿˜æ˜¯ $h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})$
    ä¼šåœ¨æœ€ä¼˜è§£ä¸­æˆä¸ºæœ‰æ•ˆä¸ç­‰å¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç›¸åº”åœ°ç”¨ $\mathcal{L}_{k}({\bm{h}}^{l-2})$ æˆ– $\mathcal{G}_{k}({\bm{h}}^{l-2})$
    æ›¿æ¢ $h^{l-1}_{k}$ã€‚è¿™ä¸ªæŠ•å½±åœ¨æ•°å­¦ä¸Šç­‰åŒäºåº”ç”¨å‚…é‡Œå¶-è«èŒ¨é‡‘æ¶ˆé™¤æ³•ï¼ŒåŒæ—¶é¿å…äº†ç”±äºâ€œæœªé€‰æ‹©â€è¾¹ç•Œå‡½æ•°äº§ç”Ÿçš„å†—ä½™ä¸ç­‰å¼ã€‚å¯¹æ¯ä¸€å±‚é‡å¤è¿™ä¸€è¿‡ç¨‹ï¼Œå¾—åˆ°çš„è¾“å‡ºå‡¸æ”¾æ¾ä»…æ¶‰åŠè¾“å…¥å˜é‡ã€‚æˆ‘ä»¬è‡ªç„¶åœ°è§‚å¯Ÿåˆ°ç®€å•ä¸‹ç•Œ
    $\mathcal{L}_{k}({\bm{h}}^{l-2})$ çš„å¯å–æ€§ï¼šåœ¨æ¯ä¸€å±‚æ–½åŠ ä¸¤éƒ¨åˆ†ä¸‹ç•Œä¼šä»¥æŒ‡æ•°æ–¹å¼å¢åŠ ä¼ æ’­çº¦æŸçš„æ•°é‡ï¼Œç±»ä¼¼äºå‚…é‡Œå¶-è«èŒ¨é‡‘æ¶ˆé™¤æ³•ã€‚
- en: A path towards completeness
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å®Œæ•´æ€§çš„è·¯å¾„
- en: 'Given an input-output bound, the reachable set can be refined by splitting
    the input space (Henriksen and Lomuscio, [2021](#bib.bib145), Rubies-Royo etÂ al.,
    [2019](#bib.bib265))â€”a strategy similar to spatial branch and bound. In other
    words, completeness is achieved by branching in the input space, rather than activation
    patterns: this strategy is especially effective when the input space is low dimensional
    (Strong etÂ al., [2022](#bib.bib296)). For example, ReluVal (Wang etÂ al., [2018b](#bib.bib327))
    propagates symbolic intervals and implements splitting procedures on the input
    domain. As the interval extension of ReLU is Lipschitz continuous, the method
    converges to arbitrary accuracy in a finite number of splits.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šè¾“å…¥-è¾“å‡ºç•Œé™ï¼Œé€šè¿‡æ‹†åˆ†è¾“å…¥ç©ºé—´å¯ä»¥è¿›ä¸€æ­¥ç»†åŒ–å¯è¾¾é›†ï¼ˆHenriksen å’Œ Lomuscio, [2021](#bib.bib145)ï¼ŒRubies-Royo
    ç­‰äººï¼Œ[2019](#bib.bib265)ï¼‰â€”â€”è¿™ç§ç­–ç•¥ç±»ä¼¼äºç©ºé—´åˆ†æ”¯å’Œç•Œé™ã€‚æ¢å¥è¯è¯´ï¼Œå®Œæ•´æ€§æ˜¯é€šè¿‡åœ¨è¾“å…¥ç©ºé—´ä¸­åˆ†æ”¯æ¥å®ç°çš„ï¼Œè€Œä¸æ˜¯æ¿€æ´»æ¨¡å¼ï¼šå½“è¾“å…¥ç©ºé—´ç»´åº¦è¾ƒä½æ—¶ï¼Œè¿™ç§ç­–ç•¥ç‰¹åˆ«æœ‰æ•ˆï¼ˆStrong
    ç­‰äººï¼Œ[2022](#bib.bib296)ï¼‰ã€‚ä¾‹å¦‚ï¼ŒReluVal (Wang ç­‰äººï¼Œ[2018b](#bib.bib327)) ä¼ æ’­ç¬¦å·åŒºé—´å¹¶å¯¹è¾“å…¥åŸŸå®ç°æ‹†åˆ†ç¨‹åºã€‚ç”±äº
    ReLU çš„åŒºé—´æ‰©å±•æ˜¯ Lipschitz è¿ç»­çš„ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰é™æ¬¡æ•°çš„æ‹†åˆ†ä¸­æ”¶æ•›åˆ°ä»»æ„ç²¾åº¦ã€‚
- en: 4.4 Generalizing the single neuron model
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 æ³›åŒ–å•ç¥ç»å…ƒæ¨¡å‹
- en: 4.4.1 Extending to other domains
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸ
- en: In general, we will expect that the effective input domain $\mathcal{D}^{l-1}$
    for a given unit may be quite complex. For the first layer ($l=1$) this may derive
    from explicitly stated constraints on the inputs of the networks, while for later
    layers this will typically derive from the complex nonlinear transformations applied
    by the preceding layers. For example, in the context of surrogate models Yang
    etÂ al. ([2022](#bib.bib347)) propose bounding the input to the convex hull of
    the training data set, while other works (Schweidtmann etÂ al., [2022](#bib.bib277),
    Shi etÂ al., [2022](#bib.bib285)) propose machine learning-inspired techniques
    for learning the trust region implied by the training data. In effect, these methods
    assume a trained model is locally accurate around training data, which is a property
    similar to that which verification seeks to prove.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬é¢„è®¡ç»™å®šå•å…ƒçš„æœ‰æ•ˆè¾“å…¥åŸŸ $\mathcal{D}^{l-1}$ å¯èƒ½ç›¸å½“å¤æ‚ã€‚å¯¹äºç¬¬ä¸€å±‚ ($l=1$)ï¼Œè¿™å¯èƒ½æºäºå¯¹ç½‘ç»œè¾“å…¥çš„æ˜¾å¼çº¦æŸï¼Œè€Œå¯¹äºåç»­å±‚ï¼Œè¿™é€šå¸¸æ¥æºäºå‰é¢å±‚æ–½åŠ çš„å¤æ‚éçº¿æ€§å˜æ¢ã€‚ä¾‹å¦‚ï¼Œåœ¨ä»£ç†æ¨¡å‹çš„èƒŒæ™¯ä¸‹ï¼ŒYang
    ç­‰äºº ([2022](#bib.bib347)) æå‡ºå°†è¾“å…¥ç•Œå®šåœ¨è®­ç»ƒæ•°æ®é›†çš„å‡¸åŒ…ä¸Šï¼Œè€Œå…¶ä»–ç ”ç©¶ï¼ˆSchweidtmann ç­‰äººï¼Œ[2022](#bib.bib277)ï¼ŒShi
    ç­‰äººï¼Œ[2022](#bib.bib285)ï¼‰æå‡ºäº†æœºå™¨å­¦ä¹ å¯å‘çš„æŠ€æœ¯æ¥å­¦ä¹ è®­ç»ƒæ•°æ®æ‰€éšå«çš„ä¿¡ä»»åŒºåŸŸã€‚å®é™…ä¸Šï¼Œè¿™äº›æ–¹æ³•å‡è®¾è®­ç»ƒæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®å‘¨å›´æ˜¯å±€éƒ¨å‡†ç¡®çš„ï¼Œè¿™æ˜¯ä¸€ç§ä¸éªŒè¯æ‰€è¯•å›¾è¯æ˜çš„å±æ€§ç±»ä¼¼çš„ç‰¹æ€§ã€‚
- en: 'Nevertheless, most research focuses on hyperrectangular input domains, largely
    motivated by practical considerations: i) there are efficient, well-studied methods
    for computing valid (though not necessarily optimally tight) variable bounds,
    ii) characterizing the exact effective domain may be computationally impractical,
    and iii) and the hyperrectangular structure makes analysis simpler for complex
    formulations like those presented in SectionÂ [4.2.4](#S4.SS2.SSS4 "4.2.4 Cutting
    plane methods: Trading variables for inequalities â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey"). We note that Jordan etÂ al. ([2019](#bib.bib167))
    use polyhedral analyses to perform verification over arbitrary (including non-polyhedral)
    norms, by fitting a $p$-norm ball in the decision region and checking adjacent
    linear regions. On the other hand, robust optimization can be employed to find
    $p$-norm adversarial regions (rather than verifying robustness), as opposed to
    a single point adversary (Maragno etÂ al., [2023](#bib.bib211)).'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œå¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨è¶…çŸ©å½¢è¾“å…¥åŸŸï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå®é™…è€ƒè™‘å› ç´ ï¼šiï¼‰å­˜åœ¨é«˜æ•ˆã€ç ”ç©¶å……åˆ†çš„æœ‰æ•ˆï¼ˆå°½ç®¡ä¸ä¸€å®šæ˜¯æœ€ç´§çš„ï¼‰å˜é‡è¾¹ç•Œè®¡ç®—æ–¹æ³•ï¼Œiiï¼‰ç²¾ç¡®æè¿°æœ‰æ•ˆåŸŸå¯èƒ½åœ¨è®¡ç®—ä¸Šä¸åˆ‡å®é™…ï¼Œiiiï¼‰è¶…çŸ©å½¢ç»“æ„ä½¿å¾—åˆ†æå¤æ‚å…¬å¼å˜å¾—æ›´ç®€å•ï¼Œä¾‹å¦‚ç¬¬[4.2.4](#S4.SS2.SSS4
    "4.2.4 åˆ‡å‰²å¹³é¢æ–¹æ³•ï¼šå°†å˜é‡æ¢æˆä¸ç­‰å¼ â€£ 4.2 ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„ç²¾ç¡®æ¨¡å‹ â€£ 4 ä¼˜åŒ–è®­ç»ƒè¿‡çš„ç¥ç»ç½‘ç»œ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥")èŠ‚ä¸­æå‡ºçš„å¤æ‚å…¬å¼ã€‚æˆ‘ä»¬æ³¨æ„åˆ°
    Jordan ç­‰äºº ([2019](#bib.bib167)) ä½¿ç”¨å¤šé¢ä½“åˆ†ææ¥å¯¹ä»»æ„ï¼ˆåŒ…æ‹¬éå¤šé¢ä½“ï¼‰èŒƒæ•°è¿›è¡ŒéªŒè¯ï¼Œé€šè¿‡åœ¨å†³ç­–åŒºåŸŸä¸­æ‹Ÿåˆ $p$-èŒƒæ•°çƒå¹¶æ£€æŸ¥ç›¸é‚»çš„çº¿æ€§åŒºåŸŸã€‚å¦ä¸€æ–¹é¢ï¼Œé²æ£’ä¼˜åŒ–å¯ä»¥ç”¨æ¥å¯»æ‰¾
    $p$-èŒƒæ•°å¯¹æŠ—åŒºåŸŸï¼ˆè€Œä¸æ˜¯éªŒè¯é²æ£’æ€§ï¼‰ï¼Œä¸å•ç‚¹å¯¹æŠ—è€…ï¼ˆMaragno ç­‰äººï¼Œ[2023](#bib.bib211)ï¼‰ç›¸å¯¹ã€‚
- en: Anderson etÂ al. ([2020](#bib.bib5)) present two closely related frameworks for
    constructing ideal and hereditarily sharp formulations for ReLU units with arbitrary
    polyhedral input domains. This characterization is derived from Lagrangian duality,
    and requires an infinite number of constraints (intuitively, one for each choice
    of dual multipliers). Nonetheless, separation can still be done over this infinite
    family of inequalities via a subgradient-based algorithm; this approach will be
    tractable if optimization over $\mathcal{D}^{l-1}$ is tractable. Many propagation
    algorithms are also fully compatible with arbitrary polyhedral input domains,
    as the projected problem (i.e., a linear input-output relaxation) remains an LP.
    Singh etÂ al. ([2021](#bib.bib292)) show that simplex input domains can actually
    be beneficial, creating tighter formulations by propagating constraints on the
    inputs through the network layers. Similarly, optimization-based bound tightening
    problems based on solving LPs can embed constraints defining polyhedral input
    domains.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: Anderson et al.ï¼ˆ[2020](#bib.bib5)ï¼‰æå‡ºäº†ä¸¤ä¸ªå¯†åˆ‡ç›¸å…³çš„æ¡†æ¶ï¼Œç”¨äºæ„å»ºç†æƒ³å’Œé—ä¼ ä¸Šå°–é”çš„ ReLU å•å…ƒå…¬å¼ï¼Œé€‚ç”¨äºä»»æ„å¤šé¢ä½“è¾“å…¥åŸŸã€‚è¯¥è¡¨å¾æºè‡ªæ‹‰æ ¼æœ—æ—¥å¯¹å¶æ€§ï¼Œå¹¶è¦æ±‚æ— é™æ•°é‡çš„çº¦æŸï¼ˆç›´è§‚åœ°ï¼Œå¯¹äºæ¯ä¸ªå¯¹å¶ä¹˜å­çš„é€‰æ‹©ï¼‰ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä»ç„¶å¯ä»¥é€šè¿‡åŸºäºæ¬¡æ¢¯åº¦çš„ç®—æ³•å¯¹è¿™ä¸€æ— é™ä¸ç­‰å¼å®¶æ—è¿›è¡Œåˆ†ç¦»ï¼›å¦‚æœåœ¨
    $\mathcal{D}^{l-1}$ ä¸Šçš„ä¼˜åŒ–æ˜¯å¯å¤„ç†çš„ï¼Œè¿™ç§æ–¹æ³•å°†æ˜¯å¯è¡Œçš„ã€‚è®¸å¤šä¼ æ’­ç®—æ³•ä¹Ÿå®Œå…¨å…¼å®¹ä»»æ„å¤šé¢ä½“è¾“å…¥åŸŸï¼Œå› ä¸ºæŠ•å½±é—®é¢˜ï¼ˆå³çº¿æ€§è¾“å…¥è¾“å‡ºæ¾å¼›ï¼‰ä»ç„¶æ˜¯çº¿æ€§è§„åˆ’ã€‚Singh
    et al.ï¼ˆ[2021](#bib.bib292)ï¼‰è¡¨æ˜ï¼Œå•çº¯å½¢è¾“å…¥åŸŸå®é™…ä¸Šå¯ä»¥æ˜¯æœ‰ç›Šçš„ï¼Œé€šè¿‡å°†è¾“å…¥ä¸Šçš„çº¦æŸä¼ æ’­åˆ°ç½‘ç»œå±‚ä¸­ï¼Œä»è€Œåˆ›å»ºæ›´ç´§çš„å…¬å¼ã€‚ç±»ä¼¼åœ°ï¼ŒåŸºäºè§£å†³çº¿æ€§è§„åˆ’çš„ä¼˜åŒ–çº¦æŸç´§ç¼©é—®é¢˜å¯ä»¥åµŒå…¥å®šä¹‰å¤šé¢ä½“è¾“å…¥åŸŸçš„çº¦æŸã€‚
- en: In certain cases, additional structural information about the input domain can
    be used to reduce this semi-infinite description to a finite one. For example,
    this can be done when $\mathcal{D}^{l-1}$ is a Cartesian product of unit simplices
    (Anderson etÂ al., [2020](#bib.bib5)) (note that this generalizes the box domain
    case, wherein each simplex is one-dimensional). This particular structure is particularly
    useful for modeling input domains with combinatorial constraints. For example,
    a network trained to predict binding propensity of a given length-$n$ DNA sequence
    is naturally modeled via an input domain that is the product of $n$ 4-dimensional
    simplicesâ€“one simplex for each letter in the sequence, each of which is selected
    from an alphabet of length 4.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå…³äºè¾“å…¥åŸŸçš„é¢å¤–ç»“æ„ä¿¡æ¯å¯ä»¥ç”¨æ¥å°†è¿™ç§åŠæ— é™æè¿°å‡å°‘ä¸ºæœ‰é™æè¿°ã€‚ä¾‹å¦‚ï¼Œå½“ $\mathcal{D}^{l-1}$ æ˜¯å•ä½å•çº¯å½¢çš„ç¬›å¡å°”ç§¯æ—¶ï¼ˆAnderson
    et al., [2020](#bib.bib5)ï¼‰ï¼ˆæ³¨æ„è¿™æ¨å¹¿äº†ç›’åŸŸæƒ…å†µï¼Œå…¶ä¸­æ¯ä¸ªå•çº¯å½¢æ˜¯ä¸€ç»´çš„ï¼‰ã€‚è¿™ç§ç‰¹å®šç»“æ„å¯¹äºå»ºæ¨¡å…·æœ‰ç»„åˆçº¦æŸçš„è¾“å…¥åŸŸç‰¹åˆ«æœ‰ç”¨ã€‚ä¾‹å¦‚ï¼Œè®­ç»ƒç½‘ç»œä»¥é¢„æµ‹ç»™å®šé•¿åº¦ä¸º
    $n$ çš„ DNA åºåˆ—çš„ç»“åˆå€¾å‘ï¼Œè‡ªç„¶é€šè¿‡ä¸€ä¸ªè¾“å…¥åŸŸè¿›è¡Œå»ºæ¨¡ï¼Œè¯¥è¾“å…¥åŸŸæ˜¯ $n$ ä¸ª 4 ç»´å•çº¯å½¢çš„ä¹˜ç§¯â€”â€”æ¯ä¸ªå•çº¯å½¢å¯¹åº”äºåºåˆ—ä¸­çš„ä¸€ä¸ªå­—æ¯ï¼Œæ¯ä¸ªå­—æ¯ä»é•¿åº¦ä¸º
    4 çš„å­—æ¯è¡¨ä¸­é€‰æ‹©ã€‚
- en: 4.4.2 Extending to other activation functions
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 æ‰©å±•åˆ°å…¶ä»–æ¿€æ´»å‡½æ•°
- en: The big-$M$ formulation technique can be any piecewise linear activation function.
    While much of the literature focuses on the ReLU due to its widespread popularity,
    models for other activation functions have been explored in the literature. For
    example, multiple papers (Serra etÂ al., [2018](#bib.bib282), Appendix K) (Tjeng
    etÂ al., [2019](#bib.bib309), Appendix A.2) present a big-$M$ formulation for the
    maxout activation function. Adapting a formulation from Anderson etÂ al. ([2020](#bib.bib5))
    (Anderson etÂ al., [2020](#bib.bib5), Proposition 10), a formulation for the maxout
    unit is
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§-$M$ å…¬å¼åŒ–æŠ€æœ¯å¯ä»¥æ˜¯ä»»ä½•åˆ†æ®µçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚è™½ç„¶æ–‡çŒ®ä¸­å¤§å¤šé›†ä¸­äº ReLU ç”±äºå…¶å¹¿æ³›çš„æµè¡Œæ€§ï¼Œä½†å…¶ä»–æ¿€æ´»å‡½æ•°çš„æ¨¡å‹ä¹Ÿåœ¨æ–‡çŒ®ä¸­æœ‰æ‰€æ¢è®¨ã€‚ä¾‹å¦‚ï¼Œå¤šä¸ªè®ºæ–‡ï¼ˆSerra
    et al., [2018](#bib.bib282)ï¼Œé™„å½• Kï¼‰ï¼ˆTjeng et al., [2019](#bib.bib309)ï¼Œé™„å½• A.2ï¼‰æå‡ºäº†
    maxout æ¿€æ´»å‡½æ•°çš„å¤§-$M$ å…¬å¼ã€‚å‚è€ƒ Anderson et al. çš„å…¬å¼ï¼ˆ[2020](#bib.bib5)ï¼‰ï¼ˆAnderson et al.,
    [2020](#bib.bib5)ï¼Œå‘½é¢˜ 10ï¼‰ï¼Œmaxout å•å…ƒçš„å…¬å¼ä¸º
- en: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\leq u_{j}({\bm{h}}^{l-1})+M^{l}_{i,j}(1-z_{j})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\leq u_{j}({\bm{h}}^{l-1})+M^{l}_{i,j}(1-z_{j})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
- en: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\geq u_{j}({\bm{h}}^{l-1})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\geq u_{j}({\bm{h}}^{l-1})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
- en: '|  | $\displaystyle\sum_{j=1}^{k}z_{j}$ | $\displaystyle=1$ |  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{j=1}^{k}z_{j}$ | $\displaystyle=1$ |  |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},v^{l}_{i},z)$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}\times\{0,1\}^{k},$
    |  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},v^{l}_{i},z)$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}\times\{0,1\}^{k},$
    |  |'
- en: where each $M^{l}_{i,j}$ is selected such that
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ¯ä¸ª $M^{l}_{i,j}$ éƒ½æ˜¯é€‰æ‹©çš„ï¼Œä»¥æ»¡è¶³
- en: '|  | $M^{l}_{i,j}\geq\max_{\tilde{{\bm{h}}}\in\mathcal{D}^{l-1}}u_{j}(\tilde{{\bm{h}}}).$
    |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | $M^{l}_{i,j}\geq\max_{\tilde{{\bm{h}}}\in\mathcal{D}^{l-1}}u_{j}(\tilde{{\bm{h}}}).$
    |  |'
- en: We can observe that the big-$M$ formulation can also handle other discontinuous
    activation functions, such as a binary/sign activations (Han and GÃ³mez, [2021](#bib.bib136))
    or more general quantized activations (Nguyen and Huchette, [2022](#bib.bib234)).
    Nevertheless, the binary activation function naturally lends itself towards Boolean
    satisfiability, and most work therefore focuses on alternative methods such as
    SAT (Cheng etÂ al., [2018](#bib.bib57), Jia and Rinard, [2020](#bib.bib164), Narodytska
    etÂ al., [2018](#bib.bib229)).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå¤§-$M$ å…¬å¼ä¹Ÿå¯ä»¥å¤„ç†å…¶ä»–ä¸è¿ç»­æ¿€æ´»å‡½æ•°ï¼Œä¾‹å¦‚äºŒè¿›åˆ¶/ç¬¦å·æ¿€æ´»å‡½æ•°ï¼ˆHanå’ŒGÃ³mezï¼Œ[2021](#bib.bib136)ï¼‰æˆ–æ›´ä¸€èˆ¬çš„é‡åŒ–æ¿€æ´»å‡½æ•°ï¼ˆNguyenå’ŒHuchetteï¼Œ[2022](#bib.bib234)ï¼‰ã€‚ç„¶è€Œï¼ŒäºŒè¿›åˆ¶æ¿€æ´»å‡½æ•°è‡ªç„¶é€‚ç”¨äºå¸ƒå°”å¯æ»¡è¶³æ€§ï¼Œå› æ­¤å¤§å¤šæ•°å·¥ä½œé›†ä¸­åœ¨è¯¸å¦‚SATï¼ˆChengç­‰ï¼Œ[2018](#bib.bib57)ï¼ŒJiaå’ŒRinardï¼Œ[2020](#bib.bib164)ï¼ŒNarodytskaç­‰ï¼Œ[2018](#bib.bib229)ï¼‰ç­‰æ›¿ä»£æ–¹æ³•ä¸Šã€‚
- en: While this survey focuses on neural networks with piecewise linear activation
    functions, we note that recent research has also studied smooth activation functions
    with a similar aim. For example, optimization over smooth activation functions
    can be handled by piecewise linear approximation and conversion to MILP (Sildir
    and Aydin, [2022](#bib.bib287)). Researchers have also studied convex/concave
    bounds for nonlinear activation functions, which can then be embedded in spatial
    branch-and-bound procedures (Schweidtmann and Mitsos, [2019](#bib.bib276), Wilhelm
    etÂ al., [2022](#bib.bib334)). In contrast to MILP formulations for ReLU neural
    networks, these problems are typically nonlinear programs that must be solved
    via spatial branch and bound.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æœ¬ç»¼è¿°ä¾§é‡äºå…·æœ‰åˆ†æ®µçº¿æ€§æ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œï¼Œä½†æˆ‘ä»¬æ³¨æ„åˆ°æœ€è¿‘çš„ç ”ç©¶ä¹Ÿç ”ç©¶äº†å…·æœ‰ç±»ä¼¼ç›®æ ‡çš„å¹³æ»‘æ¿€æ´»å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œå¹³æ»‘æ¿€æ´»å‡½æ•°ä¸Šçš„ä¼˜åŒ–å¯ä»¥é€šè¿‡åˆ†æ®µçº¿æ€§è¿‘ä¼¼å’Œè½¬æ¢ä¸ºMILPæ¥å¤„ç†ï¼ˆSildirå’ŒAydinï¼Œ[2022](#bib.bib287)ï¼‰ã€‚ç ”ç©¶äººå‘˜è¿˜ç ”ç©¶äº†éçº¿æ€§æ¿€æ´»å‡½æ•°çš„å‡¸/å‡¹ç•Œé™ï¼Œè¿™äº›ç•Œé™å¯ä»¥åµŒå…¥ç©ºé—´åˆ†æ”¯å®šç•Œç¨‹åºä¸­ï¼ˆSchweidtmannå’ŒMitsosï¼Œ[2019](#bib.bib276)ï¼ŒWilhelmç­‰ï¼Œ[2022](#bib.bib334)ï¼‰ã€‚ä¸ReLUç¥ç»ç½‘ç»œçš„MILPå…¬å¼ç›¸æ¯”ï¼Œè¿™äº›é—®é¢˜é€šå¸¸æ˜¯éçº¿æ€§ç¨‹åºï¼Œå¿…é¡»é€šè¿‡ç©ºé—´åˆ†æ”¯å’Œå®šç•Œæ¥è§£å†³ã€‚
- en: 'Propagation methods (Singh etÂ al., [2018](#bib.bib289), Zhang etÂ al., [2018a](#bib.bib354))
    can also naturally handle general activation functions: given convex polytopic
    bounds for an activation function, these tools can propagate them through network
    layers using the same techniques. For example, Fastened CROWN (Lyu etÂ al., [2020](#bib.bib204))
    employs a set of search heuristics to quickly select linear upper and lower bounds
    on ReLU, sigmoid, and hyperbolic tangent activation functions. Tighter polyhedral
    bounds can be employed, such as piecewise linear upper and lower bounds (Benussi
    etÂ al., [2022](#bib.bib23)).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ æ’­æ–¹æ³•ï¼ˆSinghç­‰ï¼Œ[2018](#bib.bib289)ï¼ŒZhangç­‰ï¼Œ[2018a](#bib.bib354)ï¼‰ä¹Ÿå¯ä»¥è‡ªç„¶åœ°å¤„ç†ä¸€èˆ¬æ¿€æ´»å‡½æ•°ï¼šç»™å®šæ¿€æ´»å‡½æ•°çš„å‡¸å¤šé¢ä½“ç•Œé™ï¼Œè¿™äº›å·¥å…·å¯ä»¥é€šè¿‡ç›¸åŒçš„æŠ€æœ¯å°†å…¶ä¼ æ’­åˆ°ç½‘ç»œå±‚ã€‚ä¾‹å¦‚ï¼ŒFastened
    CROWNï¼ˆLyuç­‰ï¼Œ[2020](#bib.bib204)ï¼‰ä½¿ç”¨ä¸€ç»„æœç´¢å¯å‘å¼æ–¹æ³•å¿«é€Ÿé€‰æ‹©ReLUã€sigmoidå’ŒåŒæ›²æ­£åˆ‡æ¿€æ´»å‡½æ•°çš„çº¿æ€§ä¸Šç•Œå’Œä¸‹ç•Œã€‚å¯ä»¥ä½¿ç”¨æ›´ç´§çš„å¤šé¢ä½“ç•Œé™ï¼Œä¾‹å¦‚åˆ†æ®µçº¿æ€§ä¸Šç•Œå’Œä¸‹ç•Œï¼ˆBenussiç­‰ï¼Œ[2022](#bib.bib23)ï¼‰ã€‚
- en: 4.4.3 Extending to adversarial training
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3 æ‰©å±•åˆ°å¯¹æŠ—è®­ç»ƒ
- en: 'As described in Section [1](#S1 "1 Introduction â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey"), the *training* of neural networks seeks to minimise
    a measure of distance between the output $y$ and the correct output $\hat{y}$.
    For instance, if this distance is prescribed as loss function $\mathcal{L}(y,\hat{y})$,
    this corresponds to solving the *training* optimization problem:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚ç¬¬[1](#S1 "1 Introduction â€£ When Deep Learning Meets Polyhedral Theory: A Survey")èŠ‚æ‰€è¿°ï¼Œç¥ç»ç½‘ç»œçš„*è®­ç»ƒ*æ—¨åœ¨æœ€å°åŒ–è¾“å‡º$y$ä¸æ­£ç¡®è¾“å‡º$\hat{y}$ä¹‹é—´çš„è·ç¦»åº¦é‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¿™ä¸ªè·ç¦»è¢«è§„å®šä¸ºæŸå¤±å‡½æ•°$\mathcal{L}(y,\hat{y})$ï¼Œè¿™å°±å¯¹åº”äºè§£å†³*è®­ç»ƒ*ä¼˜åŒ–é—®é¢˜ï¼š'
- en: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\mathcal{L}(y,\hat{y}).$
    |  | (10) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\mathcal{L}(y,\hat{y}).$
    |  | (10) |'
- en: 'Further details about the training problem and solution methods are described
    in the following section. Here, we briefly outline how verification techniques
    can be embedded in training. Specifically, solutions or bounds to the verification
    problem (Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Neural network verification â€£ 4.1
    Applications of optimization over trained networks â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) provide
    a metric of how robust a trained neural network is to perturbations. These metrics
    can be embedded in the training problem to obtain a more robust network during
    training, often resulting in a bilevel training problem. For instance, the verification
    problem ([3](#S4.E3 "In Example 4 â€£ 4.1.1 Neural network verification â€£ 4.1 Applications
    of optimization over trained networks â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) can be embedded as a
    lower-level problem, giving the robust optimization problem:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…³äºè®­ç»ƒé—®é¢˜å’Œè§£å†³æ–¹æ³•çš„æ›´å¤šç»†èŠ‚å°†åœ¨ä»¥ä¸‹éƒ¨åˆ†ä¸­æè¿°ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ç®€è¦æ¦‚è¿°äº†å¦‚ä½•å°†éªŒè¯æŠ€æœ¯åµŒå…¥è®­ç»ƒä¸­ã€‚å…·ä½“è€Œè¨€ï¼ŒéªŒè¯é—®é¢˜çš„è§£æˆ–è¾¹ç•Œï¼ˆç¬¬ [4.1.1](#S4.SS1.SSS1
    "4.1.1 Neural network verification â€£ 4.1 Applications of optimization over trained
    networks â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey") èŠ‚ï¼‰æä¾›äº†ä¸€ä¸ªæŒ‡æ ‡ï¼Œè¡¨æ˜è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œå¯¹æ‰°åŠ¨çš„é²æ£’æ€§ã€‚è¿™äº›æŒ‡æ ‡å¯ä»¥åµŒå…¥è®­ç»ƒé—®é¢˜ä¸­ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—æ›´é²æ£’çš„ç½‘ç»œï¼Œé€šå¸¸ä¼šå¯¼è‡´åŒå±‚è®­ç»ƒé—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒéªŒè¯é—®é¢˜
    ([3](#S4.E3 "In Example 4 â€£ 4.1.1 Neural network verification â€£ 4.1 Applications
    of optimization over trained networks â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) å¯ä»¥ä½œä¸ºä¸‹å±‚é—®é¢˜åµŒå…¥ï¼Œå¾—åˆ°é²æ£’ä¼˜åŒ–é—®é¢˜ï¼š'
- en: '|  | $\displaystyle\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\quad\underset{&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon}{\mathrm{max}}\quad\mathcal{L}(y=f(x),\hat{y}).$
    |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\quad\underset{&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon}{\mathrm{max}}\quad\mathcal{L}(y=f(x),\hat{y}).$
    |  |'
- en: Solving these problems generally involves either bilevel optimization, or computing
    an adversarial solution/bound at each training step, conceptually similar to the
    robust cutting plane approach. Madry etÂ al. ([2018](#bib.bib206)) proposed this
    formulation and solved the nonconvex inner problem using gradient descent, thereby
    losing a formal certification of robustness. These approaches may also benefit
    from reformulation strategies, such as by taking the dual of the inner problem
    and using any feasible solution as a bound (Wong and Kolter, [2018](#bib.bib335)).
    The resulting models are not only more robust, but several works have also found
    it to be empirically easier to verify robustness in them (Mirman etÂ al., [2018](#bib.bib220),
    Wong and Kolter, [2018](#bib.bib335)).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: è§£å†³è¿™äº›é—®é¢˜é€šå¸¸æ¶‰åŠåŒå±‚ä¼˜åŒ–ï¼Œæˆ–è€…åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­è®¡ç®—å¯¹æŠ—è§£/è¾¹ç•Œï¼Œè¿™åœ¨æ¦‚å¿µä¸Šç±»ä¼¼äºé²æ£’åˆ‡å‰²å¹³é¢æ–¹æ³•ã€‚Madry ç­‰äºº ([2018](#bib.bib206))
    æå‡ºäº†è¿™ä¸€å…¬å¼ï¼Œå¹¶ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è§£å†³éå‡¸çš„å†…å±‚é—®é¢˜ï¼Œä»è€Œå¤±å»äº†é²æ£’æ€§çš„æ­£å¼è¯æ˜ã€‚è¿™äº›æ–¹æ³•ä¹Ÿå¯ä»¥é€šè¿‡é‡æ–°è¡¨è¿°ç­–ç•¥å—ç›Šï¼Œä¾‹å¦‚é€šè¿‡å–å†…å±‚é—®é¢˜çš„å¯¹å¶å¹¶ä½¿ç”¨ä»»ä½•å¯è¡Œè§£ä½œä¸ºè¾¹ç•Œï¼ˆWong
    å’Œ Kolter, [2018](#bib.bib335)ï¼‰ã€‚å¾—åˆ°çš„æ¨¡å‹ä¸ä»…æ›´åŠ é²æ£’ï¼Œè€Œä¸”å¤šé¡¹ç ”ç©¶è¿˜å‘ç°éªŒè¯å…¶é²æ£’æ€§åœ¨è¿™äº›æ¨¡å‹ä¸­ç»éªŒä¸Šæ›´å®¹æ˜“ï¼ˆMirman
    ç­‰äºº, [2018](#bib.bib220), Wong å’Œ Kolter, [2018](#bib.bib335)ï¼‰ã€‚
- en: 'Alternatively, robustness can be induced by designing an additional penalty
    term for the training loss function, in a similar vein to regularization. For
    example:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æé«˜é²æ£’æ€§çš„æ–¹æ³•æ˜¯é€šè¿‡ä¸ºè®­ç»ƒæŸå¤±å‡½æ•°è®¾è®¡é¢å¤–çš„æƒ©ç½šé¡¹ï¼Œè¿™ä¸æ­£åˆ™åŒ–çš„åŸç†ç±»ä¼¼ã€‚ä¾‹å¦‚ï¼š
- en: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\kappa\mathcal{L}(y,\hat{y})+(1-\kappa)\mathcal{L}_{\mathrm{robust}}(\cdot).$
    |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\kappa\mathcal{L}(y,\hat{y})+(1-\kappa)\mathcal{L}_{\mathrm{robust}}(\cdot).$
    |  |'
- en: Additionally, if these robustness penalties are differentiable, they can be
    embedded into standard gradient descent based optimization algorithms (Dvijotham
    etÂ al., [2018b](#bib.bib85), Mirman etÂ al., [2018](#bib.bib220)). In the above
    formulation, the parameter $\kappa$ controls the relative weighting between fitting
    the training data and satisfying some robustness criterion, and its value can
    be scheduled during training, e.g., to first focus on model accuracy (Gowal etÂ al.,
    [2018](#bib.bib129)). In these cases, over-approximation of the reachable set
    is less problematic, as it merely produces a model *more* robust than required.
    Nevertheless, BalunoviÄ‡ and Vechev ([2020](#bib.bib16)) improve relaxation tightness
    by searching for adversarial examples in the â€œlatentâ€ space between hidden layers,
    reducing the number of propagation steps. Zhang etÂ al. ([2020](#bib.bib355)) provide
    an implementation that that tightens relaxations by also propagating bounds backwards
    through the network.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå¦‚æœè¿™äº›é²æ£’æ€§æƒ©ç½šæ˜¯å¯å¾®çš„ï¼Œå®ƒä»¬å¯ä»¥åµŒå…¥åˆ°åŸºäºæ¢¯åº¦ä¸‹é™çš„æ ‡å‡†ä¼˜åŒ–ç®—æ³•ä¸­ï¼ˆDvijotham ç­‰ï¼Œ[2018b](#bib.bib85)ï¼ŒMirman
    ç­‰ï¼Œ[2018](#bib.bib220)ï¼‰ã€‚åœ¨ä¸Šè¿°å…¬å¼ä¸­ï¼Œå‚æ•°$\kappa$ æ§åˆ¶ç€æ‹Ÿåˆè®­ç»ƒæ•°æ®å’Œæ»¡è¶³æŸäº›é²æ£’æ€§æ ‡å‡†ä¹‹é—´çš„ç›¸å¯¹æƒé‡ï¼Œå…¶å€¼å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œè°ƒèŠ‚ï¼Œä¾‹å¦‚ï¼Œé¦–å…ˆä¸“æ³¨äºæ¨¡å‹å‡†ç¡®æ€§ï¼ˆGowal
    ç­‰ï¼Œ[2018](#bib.bib129)ï¼‰ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå¯¹å¯è¾¾é›†åˆçš„è¿‡åº¦é€¼è¿‘é—®é¢˜è¾ƒå°ï¼Œå› ä¸ºå®ƒåªæ˜¯äº§ç”Ÿäº†ä¸€ä¸ª*æ¯”è¦æ±‚çš„*æ›´é²æ£’çš„æ¨¡å‹ã€‚ç„¶è€Œï¼ŒBalunoviÄ‡
    å’Œ Vechev ([2020](#bib.bib16)) é€šè¿‡åœ¨éšè—å±‚ä¹‹é—´çš„â€œæ½œåœ¨â€ç©ºé—´ä¸­æœç´¢å¯¹æŠ—æ ·æœ¬æ¥æé«˜æ¾å¼›çš„ç´§è‡´æ€§ï¼Œä»è€Œå‡å°‘äº†ä¼ æ’­æ­¥éª¤çš„æ•°é‡ã€‚Zhang
    ç­‰ ([2020](#bib.bib355)) æä¾›äº†ä¸€ç§é€šè¿‡å°†ç•Œé™å‘åä¼ æ’­é€šè¿‡ç½‘ç»œæ¥ç´§ç¼©æ¾å¼›çš„å®ç°ã€‚
- en: 5 Linear Programming and Polyhedral Theory in Training
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 è®­ç»ƒä¸­çš„çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®º
- en: 'In the previous sections, we have almost exclusively focused on tasks involving
    neural networks that have already been constructed, i.e., we have assumed that
    the training step has already concluded (with the exception of Section [4.4.3](#S4.SS4.SSS3
    "4.4.3 Extending to adversarial training â€£ 4.4 Generalizing the single neuron
    model â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")). In this section, we focus on the training phase,
    whose goal is to construct a neural network that can represent the relationship
    between the input and output of a given set of data points.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å‡ ä¹å®Œå…¨ä¸“æ³¨äºå·²ç»æ„å»ºå¥½çš„ç¥ç»ç½‘ç»œä»»åŠ¡ï¼Œå³æˆ‘ä»¬å‡è®¾è®­ç»ƒæ­¥éª¤å·²ç»ç»“æŸï¼ˆé™¤éåœ¨ç¬¬ [4.4.3](#S4.SS4.SSS3 "4.4.3
    Extending to adversarial training â€£ 4.4 Generalizing the single neuron model â€£
    4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")èŠ‚ï¼‰ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨è®­ç»ƒé˜¶æ®µï¼Œå…¶ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªèƒ½å¤Ÿè¡¨ç¤ºç»™å®šæ•°æ®ç‚¹è¾“å…¥å’Œè¾“å‡ºä¹‹é—´å…³ç³»çš„ç¥ç»ç½‘ç»œã€‚'
- en: Let us consider a set of points, or sample, $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$,
    and assume that these points are related via a function $\hat{f}$, i.e., $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$
    $i=1,\ldots,D$. In the training phase, we look for $\hat{f}$ in a pre-defined
    class (e.g. neural networks with a specific architecture) that approximates the
    relation $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$. Typically, this
    is done by solving an Empirical Risk Minimization problem
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªç‚¹é›†æˆ–æ ·æœ¬ $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$ï¼Œå¹¶å‡è®¾è¿™äº›ç‚¹é€šè¿‡å‡½æ•°
    $\hat{f}$ ç›¸å…³ï¼Œå³ $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$ $i=1,\ldots,D$ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªé¢„å®šä¹‰çš„ç±»ä¸­å¯»æ‰¾
    $\hat{f}$ï¼ˆä¾‹å¦‚ï¼Œå…·æœ‰ç‰¹å®šç»“æ„çš„ç¥ç»ç½‘ç»œï¼‰ï¼Œè¯¥ç±»è¿‘ä¼¼å…³ç³» $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$ã€‚é€šå¸¸ï¼Œé€šè¿‡è§£å†³ä¸€ä¸ªç»éªŒé£é™©æœ€å°åŒ–é—®é¢˜æ¥å®Œæˆè¿™é¡¹å·¥ä½œ
- en: '|  | $\min_{\hat{f}\in F}\frac{1}{D}\sum_{i=1}^{D}\ell(\hat{f}(\tilde{{\bm{x}}}_{i}),\tilde{{\bm{y}}}_{i})$
    |  | (11) |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\hat{f}\in F}\frac{1}{D}\sum_{i=1}^{D}\ell(\hat{f}(\tilde{{\bm{x}}}_{i}),\tilde{{\bm{y}}}_{i})$
    |  | (11) |'
- en: where $\ell$ is a loss function and $F$ is the class of functions we are restricted
    to. We usually assume the class $F$ is parametrized by $({\bm{W}},{\bm{b}})\in\Theta$
    (the network weights and biases), so we are further assuming that there exists
    a function $f(\cdot,\cdot,\cdot)$ (the network architecture) such that
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\ell$ æ˜¯ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œ$F$ æ˜¯æˆ‘ä»¬å—é™çš„å‡½æ•°ç±»ã€‚æˆ‘ä»¬é€šå¸¸å‡è®¾ç±» $F$ ç”± $({\bm{W}},{\bm{b}})\in\Theta$ï¼ˆç½‘ç»œæƒé‡å’Œåç½®ï¼‰å‚æ•°åŒ–ï¼Œå› æ­¤æˆ‘ä»¬è¿›ä¸€æ­¥å‡è®¾å­˜åœ¨ä¸€ä¸ªå‡½æ•°
    $f(\cdot,\cdot,\cdot)$ï¼ˆç½‘ç»œç»“æ„ï¼‰ï¼Œä½¿å¾—
- en: '|  | $\forall\hat{f}\in F,\,\exists({\bm{W}},{\bm{b}})\in\Theta,\,\hat{f}({\bm{x}})=f({\bm{x}},{\bm{W}},{\bm{b}}),$
    |  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | $\forall\hat{f}\in F,\,\exists({\bm{W}},{\bm{b}})\in\Theta,\,\hat{f}({\bm{x}})=f({\bm{x}},{\bm{W}},{\bm{b}}),$
    |  |'
- en: and thus, the optimization is performed over the space of parameters. In many
    cases, $\Theta=\mathbb{R}^{N}$â€”the parameters are unrestricted real numbersâ€”but
    we will see some cases when a different parameter space can be used.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¼˜åŒ–æ˜¯åœ¨å‚æ•°ç©ºé—´ä¸Šè¿›è¡Œçš„ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œ$\Theta=\mathbb{R}^{N}$â€”â€”å‚æ•°æ˜¯ä¸å—é™åˆ¶çš„å®æ•°â€”â€”ä½†æˆ‘ä»¬å°†çœ‹åˆ°åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥ä½¿ç”¨ä¸åŒçš„å‚æ•°ç©ºé—´ã€‚
- en: 'As mentioned in the introduction, nowadays, most of the practically successful
    *training* algorithms for neural networks, i.e., that solve or approximate ([11](#S5.E11
    "In 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")), are based on Stochastic Gradient Descent
    (SGD). From a fundamental perspective, optimization problem ([11](#S5.E11 "In
    5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) is typically a *non-convex, unconstrained* problem
    that needs to be solved efficiently and where finding a *local minimum* is sufficient.
    Thus, it is not too surprising that linear programming appears to be an unsuitable
    tool in this phase, in general. Nonetheless, there are some notable and surprising
    exceptions to this, which we review here.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å¼•è¨€ä¸­æ‰€è¿°ï¼Œå¦‚ä»Šï¼Œå¤§å¤šæ•°å®é™…æˆåŠŸçš„*è®­ç»ƒ*ç®—æ³•ï¼Œå³é‚£äº›è§£å†³æˆ–è¿‘ä¼¼([11](#S5.E11 "åœ¨ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºä¸­ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))çš„ç®—æ³•ï¼ŒåŸºäºéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ã€‚ä»åŸºæœ¬çš„è§’åº¦çœ‹ï¼Œä¼˜åŒ–é—®é¢˜([11](#S5.E11
    "åœ¨ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºä¸­ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))é€šå¸¸æ˜¯ä¸€ä¸ª*éå‡¸ã€æ— çº¦æŸ*é—®é¢˜ï¼Œéœ€è¦æœ‰æ•ˆè§£å†³ï¼Œå¹¶ä¸”æ‰¾åˆ°*å±€éƒ¨æœ€å°å€¼*å³å¯ã€‚å› æ­¤ï¼Œçº¿æ€§è§„åˆ’åœ¨è¿™ä¸€é˜¶æ®µé€šå¸¸çœ‹ä¼¼ä¸é€‚ç”¨ä¹Ÿå°±ä¸è¶³ä¸ºå¥‡ã€‚ç„¶è€Œï¼Œç¡®å®å­˜åœ¨ä¸€äº›æ˜¾è‘—ä¸”å‡ºäººæ„æ–™çš„ä¾‹å¤–ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œå›é¡¾è¿™äº›ä¾‹å¤–ã€‚
- en: 'Linear programming played an interesting role in training neural networks before
    SGD became the predominant training method and provided an efficient approach
    for constructing neural networks with 1 hidden layer in the 90s. This method has
    some common points in their polyhedral approach with the first known algorithm
    that can solve ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in
    Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) to provable
    optimality for a 1-hidden-layer ReLU neural network, which was proposed in 2018\.
    Recently, a stream of work has exploited similar polyhedral structures to obtain
    convex optimization reformulations of regularized training problems of ReLU networks.
    Linear programming tools have also been used within SGD-type methods in order
    to compute optimal *step-sizes* in the optimization of ([11](#S5.E11 "In 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) or to strictly enforce structure in $\Theta$. From a different
    perspective, a *data-independent* polytope was used to describe approximately
    all training problems that can arise from an uncertainty set. Additionally, a
    back-propagation-like algorithm for training neural networks, which solves mixed-integer
    linear problems in each layer, was proposed as an alternative to SGD. Furthermore,
    when the neural network weights are required to be discrete, the applicability
    of SGD is impaired, and mixed-integer linear models have been proposed to tackle
    the corresponding training problems.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çº¿æ€§è§„åˆ’æˆä¸ºä¸»è¦è®­ç»ƒæ–¹æ³•ä¹‹å‰ï¼Œå®ƒåœ¨è®­ç»ƒç¥ç»ç½‘ç»œä¸­å‘æŒ¥äº†æœ‰è¶£çš„ä½œç”¨ï¼Œå¹¶æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ç”¨äºæ„å»º1990å¹´ä»£å…·æœ‰1ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œã€‚è¿™ç§æ–¹æ³•åœ¨å¤šé¢ä½“æ–¹æ³•ä¸Šä¸ç¬¬ä¸€ä¸ªå·²çŸ¥çš„ç®—æ³•æœ‰ä¸€äº›å…±åŒç‚¹ï¼Œè¯¥ç®—æ³•èƒ½è¯æ˜1éšè—å±‚ReLUç¥ç»ç½‘ç»œçš„æœ€ä¼˜æ€§ï¼Œè¯¥ç®—æ³•äº2018å¹´æå‡ºã€‚æœ€è¿‘ï¼Œä¸€ç³»åˆ—å·¥ä½œåˆ©ç”¨ç±»ä¼¼çš„å¤šé¢ä½“ç»“æ„ï¼Œè·å¾—äº†ReLUç½‘ç»œæ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜çš„å‡¸ä¼˜åŒ–é‡æ„ã€‚çº¿æ€§è§„åˆ’å·¥å…·è¿˜è¢«åº”ç”¨äºSGDç±»å‹çš„æ–¹æ³•ä¸­ï¼Œä»¥è®¡ç®—ä¼˜åŒ–([11](#S5.E11
    "åœ¨ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºä¸­ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ä¸­çš„*æ­¥é•¿*æˆ–ä¸¥æ ¼æ‰§è¡Œç»“æ„$\Theta$ã€‚ä»ä¸åŒçš„è§’åº¦çœ‹ï¼Œä¸€ä¸ª*æ•°æ®æ— å…³*çš„å¤šé¢ä½“è¢«ç”¨æ¥å¤§è‡´æè¿°æ‰€æœ‰å¯èƒ½ä»ä¸ç¡®å®šæ€§é›†åˆä¸­å‡ºç°çš„è®­ç»ƒé—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç±»ä¼¼äºåå‘ä¼ æ’­çš„ç®—æ³•ï¼Œç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œï¼Œåœ¨æ¯ä¸€å±‚è§£å†³æ··åˆæ•´æ•°çº¿æ€§é—®é¢˜ï¼Œä½œä¸ºSGDçš„æ›¿ä»£æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œå½“ç¥ç»ç½‘ç»œæƒé‡éœ€è¦ç¦»æ•£æ—¶ï¼ŒSGDçš„é€‚ç”¨æ€§å—åˆ°é™åˆ¶ï¼Œå› æ­¤æå‡ºäº†æ··åˆæ•´æ•°çº¿æ€§æ¨¡å‹æ¥å¤„ç†ç›¸åº”çš„è®­ç»ƒé—®é¢˜ã€‚
- en: In what follows, we review these roles of (mixed-integer) linear programming
    and polyhedral theory within training contexts. We refer the reader to the book
    by Goodfellow etÂ al. ([2016](#bib.bib125)) and the surveys by Curtis and Scheinberg
    ([2017](#bib.bib70)), Bottou etÂ al. ([2018](#bib.bib38)), and Wright ([2018](#bib.bib337))
    for in-depth descriptions and analyses of the most commonly used training methods
    for neural networks.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„å†…å®¹ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†ï¼ˆæ··åˆæ•´æ•°ï¼‰çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒèƒŒæ™¯ä¸‹çš„ä½œç”¨ã€‚æˆ‘ä»¬å»ºè®®è¯»è€…å‚è€ƒGoodfellowç­‰äººï¼ˆ[2016](#bib.bib125)ï¼‰çš„ä¹¦ç±ä»¥åŠCurtiså’ŒScheinbergï¼ˆ[2017](#bib.bib70)ï¼‰ã€Bottouç­‰äººï¼ˆ[2018](#bib.bib38)ï¼‰å’ŒWrightï¼ˆ[2018](#bib.bib337)ï¼‰çš„ç»¼è¿°ï¼Œä»¥æ·±å…¥äº†è§£å’Œåˆ†ææœ€å¸¸ç”¨çš„ç¥ç»ç½‘ç»œè®­ç»ƒæ–¹æ³•ã€‚
- en: We remark that solving the training problem to global optimality for ReLU neural
    networks is computationally complex. Even in architectures with just one hidden
    node, the problem is NP-hard (Dey etÂ al., [2020](#bib.bib81), Goel etÂ al., [2021](#bib.bib121)).
    Also see Blum and Rivest ([1992](#bib.bib33)), Boob etÂ al. ([2022](#bib.bib36)),
    Chen etÂ al. ([2022c](#bib.bib52)), Froese etÂ al. ([2022](#bib.bib110)), Froese
    and Hertrich ([2023](#bib.bib109)) for other hardness results. Furthermore, it
    has been recently shown that training ReLU networks is $\exists\mathbb{R}$-complete
    (Abrahamsen etÂ al., [2021](#bib.bib1), Bertschinger etÂ al., [2022](#bib.bib27)),
    which implies that it is likely that the problem of optimally training ReLU neural
    networks is not even in NP. Therefore, it is not strange to see that some of the
    methods we review below, even when they are solving hard problems as sub-routines
    (like mixed-integer linear problems), either make some non-trivial assumptions
    or relax some requirements. For example, boundedness and/or integrality of the
    weights, architecture restrictions such as the output dimension, or not having
    optimality guarantees.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æŒ‡å‡ºï¼Œå¯¹äºReLUç¥ç»ç½‘ç»œï¼Œå°†è®­ç»ƒé—®é¢˜æ±‚è§£è‡³å…¨å±€æœ€ä¼˜æ˜¯è®¡ç®—ä¸Šå¤æ‚çš„ã€‚å³ä½¿åœ¨åªæœ‰ä¸€ä¸ªéšè—èŠ‚ç‚¹çš„æ¶æ„ä¸­ï¼Œè¿™ä¸ªé—®é¢˜ä¹Ÿæ˜¯NPéš¾çš„ï¼ˆDeyç­‰ï¼Œ[2020](#bib.bib81)ï¼ŒGoelç­‰ï¼Œ[2021](#bib.bib121)ï¼‰ã€‚å¦è§Blumå’ŒRivestï¼ˆ[1992](#bib.bib33)ï¼‰ï¼ŒBoobç­‰ï¼ˆ[2022](#bib.bib36)ï¼‰ï¼ŒChenç­‰ï¼ˆ[2022c](#bib.bib52)ï¼‰ï¼ŒFroeseç­‰ï¼ˆ[2022](#bib.bib110)ï¼‰ï¼ŒFroeseå’ŒHertrichï¼ˆ[2023](#bib.bib109)ï¼‰ä»¥è·å–å…¶ä»–å›°éš¾ç»“æœã€‚æ­¤å¤–ï¼Œæœ€è¿‘å·²æ˜¾ç¤ºè®­ç»ƒReLUç½‘ç»œæ˜¯$\exists\mathbb{R}$-å®Œæ•´çš„ï¼ˆAbrahamsenç­‰ï¼Œ[2021](#bib.bib1)ï¼ŒBertschingerç­‰ï¼Œ[2022](#bib.bib27)ï¼‰ï¼Œè¿™æ„å‘³ç€è®­ç»ƒReLUç¥ç»ç½‘ç»œçš„ä¼˜åŒ–é—®é¢˜å¯èƒ½ç”šè‡³ä¸åœ¨NPä¸­ã€‚å› æ­¤ï¼Œä¸éš¾ç†è§£ï¼Œæˆ‘ä»¬ä¸‹é¢å›é¡¾çš„ä¸€äº›æ–¹æ³•ï¼Œå³ä½¿åœ¨è§£å†³å›°éš¾é—®é¢˜ä½œä¸ºå­ä¾‹ç¨‹æ—¶ï¼ˆå¦‚æ··åˆæ•´æ•°çº¿æ€§é—®é¢˜ï¼‰ï¼Œä¹Ÿè¦ä¹ˆåšå‡ºä¸€äº›éå¹³å‡¡çš„å‡è®¾ï¼Œè¦ä¹ˆæ”¾æ¾ä¸€äº›è¦æ±‚ã€‚ä¾‹å¦‚ï¼Œæƒé‡çš„æœ‰ç•Œæ€§å’Œ/æˆ–æ•´æ•°æ€§ï¼Œæ¶æ„é™åˆ¶ï¼Œå¦‚è¾“å‡ºç»´åº¦ï¼Œæˆ–æ²¡æœ‰æœ€ä¼˜æ€§ä¿è¯ã€‚
- en: It is worth mentioning that, in contrast, for LTUs, exact exponential-time training
    algorithms are known for much more general architectures than in the ReLU case
    (Khalife and Basu, [2022](#bib.bib173), Ergen etÂ al., [2023](#bib.bib97)). These
    are out of scope for this survey, though we will provide a high-level overview
    of some of them, as they share some similarities to approaches designed for ReLU
    networks.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä¸æ­¤ç›¸åï¼Œå¯¹äºLTUsï¼Œå·²çŸ¥ç¡®åˆ‡çš„æŒ‡æ•°æ—¶é—´è®­ç»ƒç®—æ³•é€‚ç”¨äºæ¯”ReLUæƒ…å†µæ›´ä¸ºé€šç”¨çš„æ¶æ„ï¼ˆKhalifeå’ŒBasuï¼Œ[2022](#bib.bib173)ï¼ŒErgenç­‰ï¼Œ[2023](#bib.bib97)ï¼‰ã€‚å°½ç®¡è¿™äº›è¶…å‡ºäº†æœ¬è°ƒæŸ¥çš„èŒƒå›´ï¼Œä½†æˆ‘ä»¬å°†æä¾›ä¸€äº›é«˜çº§æ¦‚è¿°ï¼Œå› ä¸ºå®ƒä»¬ä¸ä¸ºReLUç½‘ç»œè®¾è®¡çš„æ–¹æ³•æœ‰ä¸€äº›ç›¸ä¼¼ä¹‹å¤„ã€‚
- en: 5.1 Training neural networks with a single hidden layer
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 è®­ç»ƒå…·æœ‰å•å±‚éšè—å±‚çš„ç¥ç»ç½‘ç»œ
- en: Following the well-known XOR limitation of the perceptron (Minsky and Papert,
    [1969](#bib.bib219)), a natural interest arose in the development of training
    algorithms that could handle at least one hidden layer. In this section, we review
    training algorithms that can successfully minimize the training error in a one-hidden-layer
    setting and rely on polyhedral approaches.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§è‘—åçš„æ„ŸçŸ¥æœºXORé™åˆ¶ï¼ˆMinskyå’ŒPapertï¼Œ[1969](#bib.bib219)ï¼‰ä¹‹åï¼Œè‡ªç„¶äº§ç”Ÿäº†å¼€å‘å¯ä»¥å¤„ç†è‡³å°‘ä¸€å±‚éšè—å±‚çš„è®­ç»ƒç®—æ³•çš„å…´è¶£ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†èƒ½å¤ŸæˆåŠŸæœ€å°åŒ–ä¸€å±‚éšè—å±‚è®¾ç½®ä¸­çš„è®­ç»ƒè¯¯å·®å¹¶ä¾èµ–äºå¤šé¢ä½“æ–¹æ³•çš„è®­ç»ƒç®—æ³•ã€‚
- en: 5.1.1 Problem setting and solution scheme
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 é—®é¢˜è®¾ç½®å’Œè§£å†³æ–¹æ¡ˆ
- en: Suppose we have a sample of size $D$ $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$
    where $\tilde{{\bm{x}}}_{i}\in\mathbb{R}^{n}$ and $\tilde{{\bm{y}}}_{i}\in\mathbb{R}$.
    In a training phase, we would like to find a neural network function $f(\cdot,\cdot,\cdot)$
    that represents in the best possible way the relation $f(\tilde{{\bm{x}}}_{i},{\bm{W}},{\bm{b}})=\tilde{{\bm{y}}}_{i}$.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤§å°ä¸º$D$çš„æ ·æœ¬$(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$ï¼Œå…¶ä¸­$\tilde{{\bm{x}}}_{i}\in\mathbb{R}^{n}$ä¸”$\tilde{{\bm{y}}}_{i}\in\mathbb{R}$ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ä¸ªç¥ç»ç½‘ç»œå‡½æ•°$f(\cdot,\cdot,\cdot)$ï¼Œè¯¥å‡½æ•°ä»¥æœ€ä½³æ–¹å¼è¡¨ç¤ºå…³ç³»$f(\tilde{{\bm{x}}}_{i},{\bm{W}},{\bm{b}})=\tilde{{\bm{y}}}_{i}$ã€‚
- en: Note that when a neural network $\hat{f}$ has only one hidden layer, its behavior
    is almost completely determined by the sign of each component of the vector
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“ç¥ç»ç½‘ç»œ$\hat{f}$åªæœ‰ä¸€å±‚éšè—å±‚æ—¶ï¼Œå…¶è¡Œä¸ºå‡ ä¹å®Œå…¨ç”±å‘é‡ä¸­æ¯ä¸ªåˆ†é‡çš„ç¬¦å·å†³å®šã€‚
- en: '|  | ${\bm{W}}^{1}x-{\bm{b}}^{1}.$ |  |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}^{1}x-{\bm{b}}^{1}.$ |  |'
- en: These are the cases of ReLU activations $\sigma(z)=\max\{0,z\}$ and LTU activations
    $\sigma(z)=\text{sgn}(z)$. The training algorithms we show here heavily exploit
    this observation and construct $({\bm{W}}^{1},{\bm{b}}^{1})$ by embedding in this
    phase a *hyperplane partition* problem based on the sample $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$.
    While the focus of this survey is mainly devoted to ReLU activations, we also
    discuss some selected cases with LTU activations as they share some similar ideas.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯ ReLU æ¿€æ´»å‡½æ•° $\sigma(z)=\max\{0,z\}$ å’Œ LTU æ¿€æ´»å‡½æ•° $\sigma(z)=\text{sgn}(z)$ çš„æƒ…å†µã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå±•ç¤ºçš„è®­ç»ƒç®—æ³•å……åˆ†åˆ©ç”¨äº†è¿™ä¸€è§‚å¯Ÿç»“æœï¼Œå¹¶é€šè¿‡åœ¨æ­¤é˜¶æ®µåµŒå…¥ä¸€ä¸ªåŸºäºæ ·æœ¬
    $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$ çš„*è¶…å¹³é¢åˆ’åˆ†*é—®é¢˜æ¥æ„é€  $({\bm{W}}^{1},{\bm{b}}^{1})$ã€‚å°½ç®¡æœ¬è°ƒæŸ¥ä¸»è¦é›†ä¸­äº
    ReLU æ¿€æ´»å‡½æ•°ï¼Œä½†æˆ‘ä»¬ä¹Ÿè®¨è®ºäº†ä¸€äº›å…·æœ‰ LTU æ¿€æ´»å‡½æ•°çš„é€‰å®šæ¡ˆä¾‹ï¼Œå› ä¸ºå®ƒä»¬åˆ†äº«ä¸€äº›ç›¸ä¼¼çš„ç†å¿µã€‚
- en: 5.1.2 LTU activations and variable number of nodes
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 LTU æ¿€æ´»å‡½æ•°å’Œå¯å˜èŠ‚ç‚¹æ•°é‡
- en: One stream of work dedicated to developing training algorithms for one-hidden-layer
    networks concerned the use of *backpropagation* (Rumelhart etÂ al., [1986](#bib.bib266),
    LeCun etÂ al., [1989](#bib.bib185), Werbos, [1974](#bib.bib331)). In the early
    90s, an alternative family of methods was proposed, which was heavily based on
    linear programs (see e.g. Bennett and Mangasarian ([1990](#bib.bib21), [1992](#bib.bib22)),
    Roy etÂ al. ([1993](#bib.bib264)), Mukhopadhyay etÂ al. ([1993](#bib.bib227))).
    These approaches can construct a 1-hidden-layer network without the need for an
    *a-priori* number of nodes in the network. We illustrate the high-level idea of
    these next, based on the survey by Mangasarian ([1993](#bib.bib209)).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ä¸“æ³¨äºä¸ºå•éšå±‚ç½‘ç»œå¼€å‘è®­ç»ƒç®—æ³•çš„ç ”ç©¶å·¥ä½œæ¶‰åŠäº†*åå‘ä¼ æ’­*çš„ä½¿ç”¨ï¼ˆRumelhart ç­‰ï¼Œ[1986](#bib.bib266)ï¼ŒLeCun ç­‰ï¼Œ[1989](#bib.bib185)ï¼ŒWerbosï¼Œ[1974](#bib.bib331)ï¼‰ã€‚åœ¨90å¹´ä»£åˆæœŸï¼Œæå‡ºäº†ä¸€ç±»æ›¿ä»£æ€§æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¸»è¦åŸºäºçº¿æ€§è§„åˆ’ï¼ˆä¾‹å¦‚
    Bennett å’Œ Mangasarianï¼ˆ[1990](#bib.bib21)ï¼Œ[1992](#bib.bib22)ï¼‰ï¼ŒRoy ç­‰ï¼ˆ[1993](#bib.bib264)ï¼‰ï¼ŒMukhopadhyay
    ç­‰ï¼ˆ[1993](#bib.bib227)ï¼‰ï¼‰ã€‚è¿™äº›æ–¹æ³•å¯ä»¥åœ¨ä¸éœ€è¦*å…ˆéªŒ*è®¾å®šç½‘ç»œèŠ‚ç‚¹æ•°é‡çš„æƒ…å†µä¸‹æ„å»ºä¸€ä¸ªå•éšå±‚ç½‘ç»œã€‚æˆ‘ä»¬å°†åŸºäº Mangasarian
    çš„è°ƒæŸ¥ï¼ˆ[1993](#bib.bib209)ï¼‰è¯´æ˜è¿™äº›æ–¹æ³•çš„é«˜çº§æ¦‚å¿µã€‚
- en: Suppose that $\tilde{{\bm{y}}}_{i}\in\{-1,1\}$, thus the NN we construct will
    be a classifier. The training phase can be tackled via the construction of a *polyhedral
    partition* of $\mathbb{R}^{n}$ such that no two points (or few) $\tilde{{\bm{x}}}_{i}$
    and $\tilde{{\bm{x}}}_{j}$ such that $\tilde{{\bm{y}}}_{i}\neq\tilde{{\bm{y}}}_{j}$
    lie in the same element of the partition. To achieve this, the following approach
    presented by Bennett and Mangasarian ([1992](#bib.bib22)) can be followed. Let
    $Y=\{i\in[D]\,:\,\tilde{{\bm{y}}}_{i}=1\}$ and $N=[D]\setminus Y$, and consider
    the following optimization problem
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ $\tilde{{\bm{y}}}_{i}\in\{-1,1\}$ï¼Œå› æ­¤æˆ‘ä»¬æ„é€ çš„ç¥ç»ç½‘ç»œå°†æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨ã€‚è®­ç»ƒé˜¶æ®µå¯ä»¥é€šè¿‡æ„é€  $\mathbb{R}^{n}$
    çš„*å¤šé¢ä½“åˆ’åˆ†*æ¥å¤„ç†ï¼Œä»¥ç¡®ä¿æ²¡æœ‰ä¸¤ä¸ªï¼ˆæˆ–å°‘æ•°ï¼‰ç‚¹ $\tilde{{\bm{x}}}_{i}$ å’Œ $\tilde{{\bm{x}}}_{j}$ æ»¡è¶³ $\tilde{{\bm{y}}}_{i}\neq\tilde{{\bm{y}}}_{j}$ï¼Œå®ƒä»¬ä¸ä¼šè½åœ¨åŒä¸€åˆ’åˆ†å…ƒç´ ä¸­ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œå¯ä»¥éµå¾ª
    Bennett å’Œ Mangasarianï¼ˆ[1992](#bib.bib22)ï¼‰æå‡ºçš„ä»¥ä¸‹æ–¹æ³•ã€‚ä»¤ $Y=\{i\in[D]\,:\,\tilde{{\bm{y}}}_{i}=1\}$
    å’Œ $N=[D]\setminus Y$ï¼Œç„¶åè€ƒè™‘ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜
- en: '|  |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\min_{{\bm{w}},b,y,z}\quad$ | $\displaystyle\frac{1}{&#124;Y&#124;}\sum_{i\in
    Y}y_{i}+\frac{1}{&#124;N&#124;}\sum_{i\in N}z_{i}$ |  | (12a) |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{{\bm{w}},b,y,z}\quad$ | $\displaystyle\frac{1}{&#124;Y&#124;}\sum_{i\in
    Y}y_{i}+\frac{1}{&#124;N&#124;}\sum_{i\in N}z_{i}$ |  | (12a) |'
- en: '|  |  | $\displaystyle{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+y\geq 1$ | $\displaystyle\forall
    i\in Y$ |  | (12b) |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+y\geq 1$ | $\displaystyle\forall
    i\in Y$ |  | (12b) |'
- en: '|  |  | $\displaystyle-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+z\geq 1$ | $\displaystyle\forall
    i\in N$ |  | (12c) |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+z\geq 1$ | $\displaystyle\forall
    i\in N$ |  | (12c) |'
- en: '|  |  | $\displaystyle y,z\geq 0.$ |  | (12d) |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle y,z\geq 0.$ |  | (12d) |'
- en: This LP aims at finding a hyperplane ${\bm{w}}^{\top}x=b$ separating the data
    according to their value of $\tilde{{\bm{y}}}_{i}$. Since the data may not be
    separable, the LP is minimizing the following classification error
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªçº¿æ€§è§„åˆ’æ—¨åœ¨æ‰¾åˆ°ä¸€ä¸ªè¶…å¹³é¢ ${\bm{w}}^{\top}x=b$ï¼Œæ ¹æ®æ•°æ®çš„ $\tilde{{\bm{y}}}_{i}$ å€¼å¯¹æ•°æ®è¿›è¡Œåˆ†éš”ã€‚ç”±äºæ•°æ®å¯èƒ½ä¸å¯åˆ†ï¼Œçº¿æ€§è§„åˆ’çš„ç›®æ ‡æ˜¯æœ€å°åŒ–ä»¥ä¸‹åˆ†ç±»é”™è¯¯
- en: '|  | $\frac{1}{&#124;Y&#124;}\sum_{i\in Y}(-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+1)_{+}+\frac{1}{&#124;N&#124;}\sum_{i\in
    N}({\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+1)_{+}.$ |  |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{1}{&#124;Y&#124;}\sum_{i\in Y}(-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+1)_{+}+\frac{1}{&#124;N&#124;}\sum_{i\in
    N}({\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+1)_{+}.$ |  |'
- en: 'The LP ([12](#S5.E12 "In 5.1.2 LTU activations and variable number of nodes
    â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is a linear reformulation of the latter minimization problem, where
    the auxiliary values $y,z$ take the value of each element in the sum.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: LP ([12](#S5.E12 "åœ¨ 5.1.2 LTU æ¿€æ´»å‡½æ•°å’ŒèŠ‚ç‚¹æ•°é‡å˜åŒ– â€£ 5.1 è®­ç»ƒå…·æœ‰å•ä¸€éšå±‚çš„ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º
    â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) æ˜¯åè€…æœ€å°åŒ–é—®é¢˜çš„çº¿æ€§é‡æ„ï¼Œå…¶ä¸­è¾…åŠ©å€¼$y,z$å–æ¯ä¸ªå…ƒç´ çš„å€¼ã€‚
- en: 'Once the LP ([12](#S5.E12 "In 5.1.2 LTU activations and variable number of
    nodes â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is solved, we obtain 2 halfspaces classifying our data points. In
    order to obtain a richer classification and lower error, we can iterate the procedure
    by means of the Multi-Surface Method Tree (MSMT, see Bennett ([1992](#bib.bib20))),
    which solves a sequence of LPs as ([12](#S5.E12 "In 5.1.2 LTU activations and
    variable number of nodes â€£ 5.1 Training neural networks with a single hidden layer
    â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) in order to produce a polyhedral partition
    of $\mathbb{R}^{n}$. Let us illustrate how this procedure works in a simplified
    case: assume that solving ([12](#S5.E12 "In 5.1.2 LTU activations and variable
    number of nodes â€£ 5.1 Training neural networks with a single hidden layer â€£ 5
    Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) results in a vector ${\bm{w}}_{1}$ such that'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è§£å†³äº†LP ([12](#S5.E12 "åœ¨ 5.1.2 LTU æ¿€æ´»å‡½æ•°å’ŒèŠ‚ç‚¹æ•°é‡å˜åŒ– â€£ 5.1 è®­ç»ƒå…·æœ‰å•ä¸€éšå±‚çš„ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º
    â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ï¼Œæˆ‘ä»¬å¾—åˆ°2ä¸ªåŠç©ºé—´å¯¹æ•°æ®ç‚¹è¿›è¡Œåˆ†ç±»ã€‚ä¸ºäº†è·å¾—æ›´ä¸°å¯Œçš„åˆ†ç±»å’Œæ›´ä½çš„é”™è¯¯ç‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¤šè¡¨é¢æ–¹æ³•æ ‘ï¼ˆMSMTï¼Œè§Bennett
    ([1992](#bib.bib20)))æ¥è¿­ä»£è¿™ä¸€è¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•è§£å†³ä¸€ç³»åˆ—LPé—®é¢˜ï¼Œå¦‚([12](#S5.E12 "åœ¨ 5.1.2 LTU æ¿€æ´»å‡½æ•°å’ŒèŠ‚ç‚¹æ•°é‡å˜åŒ–
    â€£ 5.1 è®­ç»ƒå…·æœ‰å•ä¸€éšå±‚çš„ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ï¼Œä»¥ç”Ÿæˆ$\mathbb{R}^{n}$çš„å¤šé¢ä½“åˆ’åˆ†ã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€åŒ–çš„ä¾‹å­æ¥è¯´æ˜è¿™ä¸€è¿‡ç¨‹æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼šå‡è®¾è§£å†³([12](#S5.E12
    "åœ¨ 5.1.2 LTU æ¿€æ´»å‡½æ•°å’ŒèŠ‚ç‚¹æ•°é‡å˜åŒ– â€£ 5.1 è®­ç»ƒå…·æœ‰å•ä¸€éšå±‚çš„ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))å¾—åˆ°ä¸€ä¸ªå‘é‡${\bm{w}}_{1}$ï¼Œä½¿å¾—
- en: '|  | $\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq b_{1}\}\subseteq
    Y\quad\land\quad\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq a_{1}\}\subseteq
    N,$ |  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq b_{1}\}\subseteq
    Y\quad\land\quad\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq a_{1}\}\subseteq
    N,$ |  |'
- en: 'for some $a_{1},b_{1}\in\mathbb{R}^{n}$ with $b_{1}>a_{1}$. We can remove the
    sets $\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq
    b_{1}\}$ and $\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq
    a_{1}\}$ from the data-set and redefine ([12](#S5.E12 "In 5.1.2 LTU activations
    and variable number of nodes â€£ 5.1 Training neural networks with a single hidden
    layer â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) accordingly, in order to obtain a new vector
    ${\bm{w}}_{2}$ and scalars $b_{2},a_{2}$ that would be used to classify within
    the region $\{x\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1}\}$.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸäº›$a_{1},b_{1}\in\mathbb{R}^{n}$ä¸”$b_{1}>a_{1}$ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ•°æ®é›†ä¸­ç§»é™¤é›†åˆ$\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq
    b_{1}\}$å’Œ$\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq
    a_{1}\}$ï¼Œå¹¶ç›¸åº”åœ°é‡æ–°å®šä¹‰([12](#S5.E12 "åœ¨ 5.1.2 LTU æ¿€æ´»å‡½æ•°å’ŒèŠ‚ç‚¹æ•°é‡å˜åŒ– â€£ 5.1 è®­ç»ƒå…·æœ‰å•ä¸€éšå±‚çš„ç¥ç»ç½‘ç»œ â€£
    5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ï¼Œä»¥è·å¾—æ–°çš„å‘é‡${\bm{w}}_{2}$å’Œæ ‡é‡$b_{2},a_{2}$ï¼Œè¿™äº›å°†ç”¨äºåœ¨åŒºåŸŸ$\{x\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1}\}$å†…è¿›è¡Œåˆ†ç±»ã€‚
- en: 'This procedure can be iterated, and the polyhedral partition of $\mathbb{R}^{n}$
    induced by the resulting hyperplanes can be easily transformed into a Neural Network
    with 1 hidden layer and LTU activations (see Bennett and Mangasarian ([1990](#bib.bib21))
    for details). We illustrate this transformation with the following example: suppose
    that after 3 iterations we have the following regions, with the arrow indicating
    to which class each region is associated to:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¿‡ç¨‹å¯ä»¥è¿­ä»£è¿›è¡Œï¼Œç»“æœè¶…å¹³é¢è¯±å¯¼çš„$\mathbb{R}^{n}$çš„å¤šé¢ä½“åˆ’åˆ†å¯ä»¥è½»æ¾è½¬æ¢ä¸ºå…·æœ‰1ä¸ªéšå±‚å’ŒLTUæ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œï¼ˆè¯¦ç»†ä¿¡æ¯è§Bennettå’ŒMangasarian
    ([1990](#bib.bib21))ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹ç¤ºä¾‹æ¥è¯´æ˜è¿™ç§è½¬æ¢ï¼šå‡è®¾ç»è¿‡3æ¬¡è¿­ä»£åï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹åŒºåŸŸï¼Œç®­å¤´æŒ‡ç¤ºæ¯ä¸ªåŒºåŸŸæ‰€å±çš„ç±»åˆ«ï¼š
- en: '|  |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\geq
    b_{1}\}\to Y,$ |  | (13a) |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\geq
    b_{1}\}\to Y,$ |  | (13a) |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\leq
    a_{1}\}\to N,$ |  | (13b) |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\leq
    a_{1}\}\to N,$ |  | (13b) |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\geq
    b_{2}\}\to Y,$ |  | (13c) |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\geq
    b_{2}\}\to Y,$ |  | (13c) |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\leq
    a_{2}\}\to N,$ |  | (13d) |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\leq
    a_{2}\}\to N,$ |  | (13d) |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x\geq(a_{3}+b_{3})/2\}\to
    Y,$ |  | (13e) |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x\geq(a_{3}+b_{3})/2\}\to
    Y,$ |  | (13e) |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x<(a_{3}+b_{3})/2\}\to
    N.$ |  | (13f) |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x<(a_{3}+b_{3})/2\}\to
    N.$ |  | (13f) |'
- en: 'Since regions ([13e](#S5.E13.5 "In 13 â€£ 5.1.2 LTU activations and variable
    number of nodes â€£ 5.1 Training neural networks with a single hidden layer â€£ 5
    Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) and ([13f](#S5.E13.6 "In 13 â€£ 5.1.2 LTU activations
    and variable number of nodes â€£ 5.1 Training neural networks with a single hidden
    layer â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) are the last defined by the algorithm (under
    some stopping criterion), they both use $(a_{3}+b_{3})/2$ in order to obtain a
    well-defined partition of $\mathbb{R}^{n}$. In Figure [11](#S5.F11 "Figure 11
    â€£ 5.1.2 LTU activations and variable number of nodes â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey") we show a one-hidden-layer
    neural network that represents such a classifier. The structure of the neural
    network can be easily extended to handle more regions.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåŒºåŸŸï¼ˆ[13e](#S5.E13.5 "åœ¨13 â€£ 5.1.2 LTUæ¿€æ´»å’Œå¯å˜èŠ‚ç‚¹æ•° â€£ 5.1 è®­ç»ƒå•éšå±‚ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨
    â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥")ï¼‰å’Œï¼ˆ[13f](#S5.E13.6 "åœ¨13 â€£ 5.1.2 LTUæ¿€æ´»å’Œå¯å˜èŠ‚ç‚¹æ•° â€£ 5.1 è®­ç»ƒå•éšå±‚ç¥ç»ç½‘ç»œ
    â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥")ï¼‰æ˜¯ç”±ç®—æ³•åœ¨æŸäº›åœæ­¢å‡†åˆ™ä¸‹æœ€åå®šä¹‰çš„ï¼Œå®ƒä»¬éƒ½ä½¿ç”¨$(a_{3}+b_{3})/2$ä»¥è·å¾—ä¸€ä¸ªå®šä¹‰è‰¯å¥½çš„$\mathbb{R}^{n}$åˆ’åˆ†ã€‚åœ¨å›¾[11](#S5.F11
    "å›¾11 â€£ 5.1.2 LTUæ¿€æ´»å’Œå¯å˜èŠ‚ç‚¹æ•° â€£ 5.1 è®­ç»ƒå•éšå±‚ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥")ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªè¡¨ç¤ºè¿™ç§åˆ†ç±»å™¨çš„å•éšå±‚ç¥ç»ç½‘ç»œã€‚è¿™ä¸ªç¥ç»ç½‘ç»œçš„ç»“æ„å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•ä»¥å¤„ç†æ›´å¤šçš„åŒºåŸŸã€‚
- en: <svg   height="226.19" overflow="visible" version="1.1" width="260.93"><g transform="translate(0,226.19)
    matrix(1 0 0 -1 0 0) translate(12.28,0) translate(0,113.19)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -7.39 -14.98)" fill="#000000"
    stroke="#000000"><g  transform="matrix(1 0 0 -1 0 25.435)"><g transform="matrix(1
    0 0 1 0 20.91)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><g transform="matrix(1 0 0 -1 0 14.685)"><g  transform="matrix(1
    0 0 1 0 8.45)"><g  transform="matrix(1 0 0 -1 0 0)"><g  transform="matrix(1 0
    0 -1 0 4.225)"><g transform="matrix(1 0 0 1 0 5.96)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="11.78"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{1}$</foreignobject></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 18.14)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="10.38" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 26.87)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="14.77" height="9.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{n_{0}}$</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 94.87)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 57.32)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 16.13)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 -21.42)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 109.73 -62.25)" fill="#000000" stroke="#000000"><foreignobject
    width="22.35" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{a_{3}+b_{3}}{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 110.13 -101.62)" fill="#000000" stroke="#000000"><foreignobject
    width="21.27" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{-a_{3}-b_{3}}{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 232.76 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 52.52 38.54)" fill="#000000"
    stroke="#000000"><foreignobject width="14.15" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 48.27 17.71)" fill="#000000" stroke="#000000"><foreignobject width="18.77"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.93 -1.46)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 47.5 -20.6)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 51.38 -40.35)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.16 -60.74)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.59 31.95)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 169.9 11.96)" fill="#000000" stroke="#000000"><foreignobject
    width="18.48" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.82 -6.91)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2^{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.1 -26.06)" fill="#000000" stroke="#000000"><foreignobject
    width="18.48" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-2^{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 174.88 -43.02)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.73 -62.57)" fill="#000000" stroke="#000000"><foreignobject
    width="14.61" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-1$</foreignobject></g></g></g></svg>
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="226.19" overflow="visible" version="1.1" width="260.93"><g transform="translate(0,226.19)
    matrix(1 0 0 -1 0 0) translate(12.28,0) translate(0,113.19)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -7.39 -14.98)" fill="#000000"
    stroke="#000000"><g  transform="matrix(1 0 0 -1 0 25.435)"><g transform="matrix(1
    0 0 1 0 20.91)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><g transform="matrix(1 0 0 -1 0 14.685)"><g  transform="matrix(1
    0 0 1 0 8.45)"><g  transform="matrix(1 0 0 -1 0 0)"><g  transform="matrix(1 0
    0 -1 0 4.225)"><g transform="matrix(1 0 0 1 0 5.96)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="11.78"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{1}$</foreignobject></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 18.14)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="10.38" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 26.87)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="14.77" height="9.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{n_{0}}$</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 94.87)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 57.32)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 16.13)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 -21.42)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 109.73 -62.25)" fill="#000000" stroke="#000000"><foreignobject
    width="22.35" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{a_{3}+b_{3}}{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 110.13 -101.62)" fill="#000000" stroke="#000000"><foreignobject
    width="21.27" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{-a_{3}-b_{3}}{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 232.76 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 52.52 38.54)" fill="#000000"
    stroke="#000000"><foreignobject width="14.15" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 48.27 17.71)" fill="#000000" stroke="#000000"><foreignobject width="18.77"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.93 -1.46)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 47.5 -20.6)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 51.38 -40.35)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.16 -60.74)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.59 31.95)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 169.9 11.96)" fill="#000000" stroke="#000000"><foreignobject
    width="18.48" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.82 -6.91)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2
- en: 'Figure 11: Illustration of Neural Network with LTU activations using MSMT.
    Inside each node of the hidden layer, we show the thresholds used in each LTU
    activation.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11ï¼šä½¿ç”¨ MSMT çš„ LTU æ¿€æ´»ç¥ç»ç½‘ç»œçš„ç¤ºæ„å›¾ã€‚æ¯ä¸ªéšè—å±‚èŠ‚ç‚¹å†…ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨æ¯ä¸ª LTU æ¿€æ´»ä¸­ä½¿ç”¨çš„é˜ˆå€¼ã€‚
- en: For other details, we refer the reader to Bennett ([1992](#bib.bib20)), and
    for variants and extensions see Mangasarian ([1993](#bib.bib209)) and references
    therein.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å…¶ä»–è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ Bennett ([1992](#bib.bib20))ï¼Œå…³äºå˜ä½“å’Œæ‰©å±•è¯·å‚è§ Mangasarian ([1993](#bib.bib209))
    åŠå…¶ä¸­çš„å‚è€ƒæ–‡çŒ®ã€‚
- en: 'Some key features of this procedure are the following:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¿‡ç¨‹çš„ä¸€äº›å…³é”®ç‰¹æ€§å¦‚ä¸‹ï¼š
- en: â€¢
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Each solution of ([12](#S5.E12 "In 5.1.2 LTU activations and variable number
    of nodes â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")), i.e., each new hyperplane, can be represented as a new node
    in the hidden layer of the resulting neural network.'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ([12](#S5.E12 "åœ¨ 5.1.2 LTU æ¿€æ´»å’ŒèŠ‚ç‚¹æ•°é‡å¯å˜ â€£ 5.1 ä½¿ç”¨å•éšè—å±‚è®­ç»ƒç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£
    å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) çš„æ¯ä¸€ä¸ªè§£ï¼Œå³æ¯ä¸€ä¸ªæ–°çš„è¶…å¹³é¢ï¼Œéƒ½å¯ä»¥è¡¨ç¤ºä¸ºç»“æœç¥ç»ç½‘ç»œä¸­éšè—å±‚çš„æ–°èŠ‚ç‚¹ã€‚
- en: â€¢
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: The addition of a new hyperplane comes with a reduction in the current loss;
    this can be iterated until a target loss is met.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ–°çš„è¶…å¹³é¢çš„åŠ å…¥ä¼šå¯¼è‡´å½“å‰æŸå¤±çš„å‡å°‘ï¼›è¿™ä¸ªè¿‡ç¨‹å¯ä»¥è¿­ä»£ï¼Œç›´åˆ°æ»¡è¶³ç›®æ ‡æŸå¤±ã€‚
- en: â€¢
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Thanks to the universal approximation theorem (Hornik etÂ al., [1989](#bib.bib153)),
    with enough nodes in the hidden layer, we can always obtain a neural network $\hat{f}$
    with zero classification error. Although this can lead to over-fitting.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é€šç”¨é€¼è¿‘å®šç†ï¼ˆHornik ç­‰äººï¼Œ[1989](#bib.bib153)ï¼‰ï¼Œåªè¦éšè—å±‚èŠ‚ç‚¹è¶³å¤Ÿå¤šï¼Œæˆ‘ä»¬æ€»æ˜¯èƒ½å¾—åˆ°ä¸€ä¸ªåˆ†ç±»é”™è¯¯ä¸ºé›¶çš„ç¥ç»ç½‘ç»œ $\hat{f}$ã€‚è™½ç„¶è¿™å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆã€‚
- en: 'The work of Roy etÂ al. ([1993](#bib.bib264)) and Mukhopadhyay etÂ al. ([1993](#bib.bib227))
    follow a related idea, although the classifiers which are built are quadratic
    functions. To illustrate the approach, we use the same set-up for ([12](#S5.E12
    "In 5.1.2 LTU activations and variable number of nodes â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). The approach in Roy
    etÂ al. ([1993](#bib.bib264)) and Mukhopadhyay etÂ al. ([1993](#bib.bib227)) aims
    at finding a function'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: Roy ç­‰äºº ([1993](#bib.bib264)) å’Œ Mukhopadhyay ç­‰äºº ([1993](#bib.bib227)) çš„å·¥ä½œéµå¾ªäº†ç›¸å…³çš„æ€è·¯ï¼Œå°½ç®¡ä»–ä»¬æ„å»ºçš„åˆ†ç±»å™¨æ˜¯äºŒæ¬¡å‡½æ•°ã€‚ä¸ºäº†è¯´æ˜è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç›¸åŒçš„è®¾ç½®
    ([12](#S5.E12 "åœ¨ 5.1.2 LTU æ¿€æ´»å’ŒèŠ‚ç‚¹æ•°é‡å¯å˜ â€£ 5.1 ä½¿ç”¨å•éšè—å±‚è®­ç»ƒç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ã€‚Roy
    ç­‰äºº ([1993](#bib.bib264)) å’Œ Mukhopadhyay ç­‰äºº ([1993](#bib.bib227)) çš„æ–¹æ³•æ—¨åœ¨æ‰¾åˆ°ä¸€ä¸ªå‡½æ•°
- en: '|  | $f_{{\bm{V}},{\bm{w}},b}(x)={\bm{x}}^{\top}{\bm{V}}{\bm{x}}+{\bm{w}}^{\top}{\bm{x}}+b$
    |  |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{{\bm{V}},{\bm{w}},b}(x)={\bm{x}}^{\top}{\bm{V}}{\bm{x}}+{\bm{w}}^{\top}{\bm{x}}+b$
    |  |'
- en: such that
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿å¾—
- en: '|  | $f_{{\bm{V}},{\bm{w}},b}(\tilde{{\bm{x}}}_{i})\geq 0\Longleftrightarrow
    i\in Y.$ |  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{{\bm{V}},{\bm{w}},b}(\tilde{{\bm{x}}}_{i})\geq 0\Longleftrightarrow
    i\in Y.$ |  |'
- en: Since this may not be possible, the authors propose solving the following LP
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™å¯èƒ½ä¸å¯è¡Œï¼Œä½œè€…æå‡ºäº†è§£å†³ä»¥ä¸‹çº¿æ€§è§„åˆ’ï¼ˆLPï¼‰çš„é—®é¢˜
- en: '|  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  | $\displaystyle\min_{W,w,b,\epsilon}\quad$ | $\displaystyle\epsilon$ |  |
    (14a) |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{W,w,b,\epsilon}\quad$ | $\displaystyle\epsilon$ |  |
    (14a) |'
- en: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\geq\epsilon$
    | $\displaystyle\forall i\in Y$ |  | (14b) |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\geq\epsilon$
    | $\displaystyle\forall i\in Y$ |  | (14b) |'
- en: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\leq-\epsilon$
    | $\displaystyle\forall i\not\in Y$ |  | (14c) |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\leq-\epsilon$
    | $\displaystyle\forall i\not\in Y$ |  | (14c) |'
- en: '|  |  | $\displaystyle\epsilon\geq\epsilon_{0}$ |  | (14d) |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\epsilon\geq\epsilon_{0}$ |  | (14d) |'
- en: 'for some fixed tolerance $\epsilon_{0}>0$. When this LP is infeasible, the
    class $Y$ is partitioned into $Y_{1}$ and $Y_{2}$, and an LP as ([14](#S5.E14
    "In 5.1.2 LTU activations and variable number of nodes â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) is solved for both $Y_{1}$
    and $Y_{2}$. The algorithm then follows iteratively (see below for comments on
    these iterations). In the end, the algorithm will construct $k$ quadratic functions
    $f_{1},\ldots,f_{k}$, which the authors call â€œmasking functionsâ€, that will classify
    an input ${\bm{x}}$ in the class $Y$ if and only if'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸä¸ªå›ºå®šçš„å®¹å·® $\epsilon_{0}>0$ã€‚å½“è¿™ä¸ªçº¿æ€§è§„åˆ’ï¼ˆLPï¼‰ä¸å¯è¡Œæ—¶ï¼Œç±»åˆ« $Y$ è¢«åˆ’åˆ†ä¸º $Y_{1}$ å’Œ $Y_{2}$ï¼Œå¹¶å¯¹
    $Y_{1}$ å’Œ $Y_{2}$ æ±‚è§£å¦‚ ([14](#S5.E14 "åœ¨ 5.1.2 LTU æ¿€æ´»å’ŒèŠ‚ç‚¹æ•°é‡å˜åŒ– â€£ 5.1 å•éšå±‚ç¥ç»ç½‘ç»œçš„è®­ç»ƒ â€£
    5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥")) çš„çº¿æ€§è§„åˆ’ã€‚ç„¶åç®—æ³•ä¼šä»¥è¿­ä»£çš„æ–¹å¼ç»§ç»­ï¼ˆå‚è§ä¸‹æ–‡å¯¹è¿™äº›è¿­ä»£çš„è¯„è®ºï¼‰ã€‚æœ€åï¼Œç®—æ³•å°†æ„é€ 
    $k$ ä¸ªäºŒæ¬¡å‡½æ•° $f_{1},\ldots,f_{k}$ï¼Œè¿™äº›å‡½æ•°è¢«ä½œè€…ç§°ä¸ºâ€œæ©ç å‡½æ•°â€ï¼Œå®ƒä»¬ä¼šå¯¹è¾“å…¥ ${\bm{x}}$ è¿›è¡Œåˆ†ç±»ï¼Œå½“ä¸”ä»…å½“
- en: '|  | $\exists i\in[k],\,f_{i}({\bm{x}})\geq 0.$ |  |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  | $\exists i\in[k],\,f_{i}({\bm{x}})\geq 0.$ |  |'
- en: 'In order to represent the resulting classifier as a single-layer neural network,
    the authors proceed in a similar manner to a linear classifier; the input layer
    of the resulting neural network not only includes each entry of ${\bm{x}}$, but
    also the bilinear terms ${\bm{x}}{\bm{x}}^{\top}$. Using this input, the classifier
    built by ([14](#S5.E14 "In 5.1.2 LTU activations and variable number of nodes
    â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be thought of as a linear classifier (much like a polynomial regression
    can be cast as a linear regression).'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å°†å¾—åˆ°çš„åˆ†ç±»å™¨è¡¨ç¤ºä¸ºå•å±‚ç¥ç»ç½‘ç»œï¼Œä½œè€…ä»¥ç±»ä¼¼äºçº¿æ€§åˆ†ç±»å™¨çš„æ–¹å¼è¿›è¡Œå¤„ç†ï¼›å¾—åˆ°çš„ç¥ç»ç½‘ç»œçš„è¾“å…¥å±‚ä¸ä»…åŒ…æ‹¬ ${\bm{x}}$ çš„æ¯ä¸ªæ¡ç›®ï¼Œè¿˜åŒ…æ‹¬åŒçº¿æ€§é¡¹
    ${\bm{x}}{\bm{x}}^{\top}$ã€‚åˆ©ç”¨è¿™äº›è¾“å…¥ï¼Œç”± ([14](#S5.E14 "åœ¨ 5.1.2 LTU æ¿€æ´»å’ŒèŠ‚ç‚¹æ•°é‡å˜åŒ– â€£ 5.1
    å•éšå±‚ç¥ç»ç½‘ç»œçš„è®­ç»ƒ â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥")) æ„å»ºçš„åˆ†ç±»å™¨å¯ä»¥è¢«è§†ä¸ºçº¿æ€§åˆ†ç±»å™¨ï¼ˆå°±åƒå¤šé¡¹å¼å›å½’å¯ä»¥è¢«çœ‹ä½œçº¿æ€§å›å½’ä¸€æ ·ï¼‰ã€‚
- en: 'As a last comment on the work of Roy etÂ al. ([1993](#bib.bib264)) and Mukhopadhyay
    etÂ al. ([1993](#bib.bib227)), the authorsâ€™ algorithm does not iterate in a straightforward
    fashion. They add clustering iterations alternating with the steps described above
    in order to (a) identify outliers and remove them from the training set, and (b)
    subdivide the training data when ([14](#S5.E14 "In 5.1.2 LTU activations and variable
    number of nodes â€£ 5.1 Training neural networks with a single hidden layer â€£ 5
    Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) is infeasible. These additions allow them to obtain
    a polynomial-time algorithm.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äº Roy ç­‰ï¼ˆ[1993](#bib.bib264)ï¼‰å’Œ Mukhopadhyay ç­‰ï¼ˆ[1993](#bib.bib227)ï¼‰çš„å·¥ä½œï¼Œä½œä¸ºæœ€åçš„è¯„è®ºï¼Œä½œè€…çš„ç®—æ³•å¹¶ä¸æ˜¯ç®€å•åœ°è¿›è¡Œè¿­ä»£ã€‚ä»–ä»¬é€šè¿‡æ·»åŠ èšç±»è¿­ä»£å¹¶ä¸ä¸Šè¿°æ­¥éª¤äº¤æ›¿è¿›è¡Œï¼Œä»¥ï¼ˆaï¼‰è¯†åˆ«å¼‚å¸¸å€¼å¹¶å°†å…¶ä»è®­ç»ƒé›†ä¸­ç§»é™¤ï¼Œä»¥åŠï¼ˆbï¼‰å½“
    ([14](#S5.E14 "åœ¨ 5.1.2 LTU æ¿€æ´»å’ŒèŠ‚ç‚¹æ•°é‡å˜åŒ– â€£ 5.1 å•éšå±‚ç¥ç»ç½‘ç»œçš„è®­ç»ƒ â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥"))
    ä¸å¯è¡Œæ—¶ç»†åˆ†è®­ç»ƒæ•°æ®ã€‚è¿™äº›é™„åŠ æ­¥éª¤ä½¿ä»–ä»¬èƒ½å¤Ÿè·å¾—ä¸€ä¸ªå¤šé¡¹å¼æ—¶é—´çš„ç®—æ³•ã€‚
- en: The methods described in this section are able to produce a neural network with
    arbitrary quality, however, there is no guarantee on the size of the resulting
    neural network. When the size of the network is fixed the story changes, which
    is the case we describe next.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä¸­æè¿°çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä»»æ„è´¨é‡çš„ç¥ç»ç½‘ç»œï¼Œä½†å¯¹äºç”Ÿæˆçš„ç¥ç»ç½‘ç»œçš„è§„æ¨¡æ²¡æœ‰ä¿è¯ã€‚å½“ç½‘ç»œè§„æ¨¡å›ºå®šæ—¶æƒ…å†µä¼šæœ‰æ‰€ä¸åŒï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æ¥ä¸‹æ¥è¦æè¿°çš„æƒ…å†µã€‚
- en: 5.1.3 Fixed number of nodes and ReLU activations
  id: totrans-434
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 å›ºå®šèŠ‚ç‚¹æ•°å’Œ ReLU æ¿€æ´»
- en: As mentioned at the beginning of this section, training a neural network is
    a complex optimization problem in general, with some results indicating that the
    problem is likely to not even be in NP (Abrahamsen etÂ al., [2021](#bib.bib1),
    Bertschinger etÂ al., [2022](#bib.bib27)).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æœ¬èŠ‚å¼€å¤´æåˆ°çš„ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œé€šå¸¸æ˜¯ä¸€ä¸ªå¤æ‚çš„ä¼˜åŒ–é—®é¢˜ï¼Œä¸€äº›ç ”ç©¶ç»“æœè¡¨æ˜è¿™ä¸ªé—®é¢˜å¯èƒ½ç”šè‡³ä¸å±äº NP ç±»ï¼ˆAbrahamsen ç­‰ï¼Œ[2021](#bib.bib1)ï¼ŒBertschinger
    ç­‰ï¼Œ[2022](#bib.bib27)ï¼‰ã€‚
- en: Nonetheless, by restricting the network architecture sufficiently and allowing
    exponential running times, exact algorithms can be conceived. An important step
    in the construction of such algorithms was taken by Arora etÂ al. ([2018](#bib.bib8)).
    In this work, the authors studied the training problem in detail, providing the
    first optimization algorithm capable of solving the training problem to provable
    optimality for a fixed network architecture with one hidden layer and with an
    output dimension of 1\. As we anticipated, this algorithm shares some similarities
    with the previous approach.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œé€šè¿‡å……åˆ†é™åˆ¶ç½‘ç»œæ¶æ„å¹¶å…è®¸æŒ‡æ•°çº§è¿è¡Œæ—¶é—´ï¼Œå¯ä»¥è®¾æƒ³åˆ°ç²¾ç¡®ç®—æ³•ã€‚Arora ç­‰äºº ([2018](#bib.bib8))åœ¨æ„å»ºè¿™äº›ç®—æ³•æ—¶è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä½œè€…è¯¦ç»†ç ”ç©¶äº†è®­ç»ƒé—®é¢˜ï¼Œæä¾›äº†ç¬¬ä¸€ä¸ªèƒ½å¤Ÿè§£å†³è®­ç»ƒé—®é¢˜å¹¶è¯æ˜æœ€ä¼˜çš„ä¼˜åŒ–ç®—æ³•ï¼Œé€‚ç”¨äºå…·æœ‰ä¸€ä¸ªéšè—å±‚å’Œè¾“å‡ºç»´åº¦ä¸º1çš„å›ºå®šç½‘ç»œæ¶æ„ã€‚æ­£å¦‚æˆ‘ä»¬é¢„æœŸçš„ï¼Œè¿™ä¸ªç®—æ³•ä¸ä¹‹å‰çš„æ–¹æ³•æœ‰ä¸€äº›ç›¸ä¼¼ä¹‹å¤„ã€‚
- en: Let us consider now a ReLU activation. Also, we no longer assume $\tilde{{\bm{y}}}_{i}\in\{-1,1\}$,
    but we keep the output dimension as 1\. The problem considered by Arora etÂ al.
    ([2018](#bib.bib8)) reads
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªReLUæ¿€æ´»ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¸å†å‡è®¾$\tilde{{\bm{y}}}_{i}\in\{-1,1\}$ï¼Œä½†æˆ‘ä»¬ä¿æŒè¾“å‡ºç»´åº¦ä¸º1ã€‚Arora
    ç­‰äºº ([2018](#bib.bib8))è€ƒè™‘çš„é—®é¢˜æ˜¯
- en: '|  | $\min_{{\bm{W}},{\bm{b}}}\frac{1}{D}\sum_{i=1}^{D}\ell({\bm{W}}^{2}(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i}),$
    |  | (15) |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{{\bm{W}},{\bm{b}}}\frac{1}{D}\sum_{i=1}^{D}\ell({\bm{W}}^{2}(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i}),$
    |  | (15) |'
- en: with $\ell:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$ a convex loss. Note that
    this problem, even if $\ell$ is convex, is a non-convex optimization problem.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\ell:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$æ˜¯ä¸€ä¸ªå‡¸æŸå¤±ã€‚è¯·æ³¨æ„ï¼Œå³ä½¿$\ell$æ˜¯å‡¸çš„ï¼Œè¿™ä¸ªé—®é¢˜ä¹Ÿæ˜¯ä¸€ä¸ªéå‡¸ä¼˜åŒ–é—®é¢˜ã€‚
- en: Theorem 1 (Arora etÂ al. ([2018](#bib.bib8)))
  id: totrans-440
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šç† 1ï¼ˆArora ç­‰äºº ([2018](#bib.bib8))ï¼‰
- en: 'Let $n_{1}$ be the number of nodes in the hidden layer. There exists an algorithm
    to find a global optimum of ([15](#S5.E15 "In 5.1.3 Fixed number of nodes and
    ReLU activations â€£ 5.1 Training neural networks with a single hidden layer â€£ 5
    Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) in time $O(2^{n_{1}}D^{{n_{0}}\cdot n_{1}}\text{poly}(D,{n_{0}},n_{1}))$.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾$n_{1}$ä¸ºéšè—å±‚ä¸­çš„èŠ‚ç‚¹æ•°é‡ã€‚å­˜åœ¨ä¸€ç§ç®—æ³•å¯ä»¥åœ¨æ—¶é—´$O(2^{n_{1}}D^{{n_{0}}\cdot n_{1}}\text{poly}(D,{n_{0}},n_{1}))$ä¸­æ‰¾åˆ°ï¼ˆ[15](#S5.E15
    "åœ¨5.1.3 å›ºå®šèŠ‚ç‚¹æ•°é‡å’ŒReLUæ¿€æ´» â€£ 5.1 è®­ç»ƒå…·æœ‰å•ä¸€éšè—å±‚çš„ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’ä¸è®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥")ï¼‰çš„å…¨å±€æœ€ä¼˜è§£ã€‚
- en: 'Roughly speaking, the algorithm works by noting that one can assume the weights
    in ${\bm{W}}^{2}$ are in $\{-1,1\}$, since $\sigma$ is positively-homogeneous.
    Thus, problem ([15](#S5.E15 "In 5.1.3 Fixed number of nodes and ReLU activations
    â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be restated as'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ç²—ç•¥åœ°è¯´ï¼Œè¯¥ç®—æ³•çš„å·¥ä½œåŸç†æ˜¯é€šè¿‡æ³¨æ„åˆ°å¯ä»¥å‡è®¾${\bm{W}}^{2}$ä¸­çš„æƒé‡åœ¨$\{-1,1\}$ä¸­ï¼Œå› ä¸º$\sigma$æ˜¯æ­£é½æ¬¡çš„ã€‚å› æ­¤ï¼Œé—®é¢˜ï¼ˆ[15](#S5.E15
    "åœ¨5.1.3 å›ºå®šèŠ‚ç‚¹æ•°é‡å’ŒReLUæ¿€æ´» â€£ 5.1 è®­ç»ƒå…·æœ‰å•ä¸€éšè—å±‚çš„ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’ä¸è®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥")ï¼‰å¯ä»¥é‡æ–°è¡¨è¿°ä¸º
- en: '|  | $\min_{{\bm{W}}^{1},{\bm{b}}^{1},s}\frac{1}{D}\sum_{i=1}^{D}\ell(s(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i})$
    |  | (16) |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{{\bm{W}}^{1},{\bm{b}}^{1},s}\frac{1}{D}\sum_{i=1}^{D}\ell(s(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i})$
    |  | (16) |'
- en: 'where $s\in\{-1,1\}^{n_{1}}$. In order to handle the non-linearity, Arora etÂ al.
    ([2018](#bib.bib8)) â€œguessâ€ the values of $s$ and the sign of each component of
    ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$. Enforcing a sign for each component
    of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$ is similar to the approach
    discussed in the previous section: it fixes how the input part of the data $(\tilde{{\bm{x}}}_{i})_{i=1}^{D}$
    is partitioned in polyhedral regions by a number of hyperplanes. The difference
    is that, in this case, the number of hyperplanes to be used is assumed to be fixed.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$s\in\{-1,1\}^{n_{1}}$ã€‚ä¸ºäº†å¤„ç†éçº¿æ€§ï¼ŒArora ç­‰äºº ([2018](#bib.bib8))â€œçŒœæµ‹â€$s$çš„å€¼å’Œ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$æ¯ä¸ªåˆ†é‡çš„ç¬¦å·ã€‚ä¸º${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$çš„æ¯ä¸ªåˆ†é‡å¼ºåˆ¶ä¸€ä¸ªç¬¦å·ç±»ä¼¼äºå‰ä¸€èŠ‚è®¨è®ºçš„æ–¹æ³•ï¼šå®ƒå›ºå®šäº†æ•°æ®è¾“å…¥éƒ¨åˆ†$(\tilde{{\bm{x}}}_{i})_{i=1}^{D}$å¦‚ä½•é€šè¿‡å¤šä¸ªè¶…å¹³é¢åˆ’åˆ†æˆå¤šé¢ä½“åŒºåŸŸã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡è®¾ä½¿ç”¨çš„è¶…å¹³é¢æ•°é‡æ˜¯å›ºå®šçš„ã€‚
- en: 'Using the hyperplane arrangement theorem (see e.g. (Matousek, [2002](#bib.bib215),
    Proposition 6.1.1)), there are at most $D^{{n_{0}}n_{1}}$ ways of fixing the signs
    of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$. Additionally, there are at
    most $2^{n_{1}}$ possible vectors in $\{-1,1\}^{n_{1}}$. Once these components
    are fixed, ([16](#S5.E16 "In 5.1.3 Fixed number of nodes and ReLU activations
    â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be solved as an optimization problem with a convex objective function
    and a polyhedral feasible region imposing the desired signs in ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$.
    This results in the $O(2^{n_{1}}D^{{n_{0}}n_{1}}\text{poly}(D,{n_{0}},n_{1}))$
    running time. This algorithm was recently generalized to concave loss functions
    by Froese etÂ al. ([2022](#bib.bib110)).'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä½¿ç”¨è¶…å¹³é¢æ’åˆ—å®šç†ï¼ˆè§ä¾‹å¦‚ï¼ˆMatousekï¼Œ[2002](#bib.bib215)ï¼Œå‘½é¢˜ 6.1.1)ï¼‰ï¼Œå›ºå®š ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$
    çš„ç¬¦å·æœ€å¤šæœ‰ $D^{{n_{0}}n_{1}}$ ç§æ–¹å¼ã€‚æ­¤å¤–ï¼Œåœ¨ $\{-1,1\}^{n_{1}}$ ä¸­æœ€å¤šæœ‰ $2^{n_{1}}$ ä¸ªå¯èƒ½çš„å‘é‡ã€‚ä¸€æ—¦è¿™äº›ç»„ä»¶å›ºå®šåï¼Œï¼ˆ[16](#S5.E16
    "In 5.1.3 Fixed number of nodes and ReLU activations â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")ï¼‰å¯ä»¥ä½œä¸ºä¸€ä¸ªå…·æœ‰å‡¸ç›®æ ‡å‡½æ•°å’Œå¤šé¢ä½“å¯è¡ŒåŒºåŸŸçš„ä¼˜åŒ–é—®é¢˜æ¥è§£å†³ï¼Œè¯¥åŒºåŸŸå¼ºåŠ äº†
    ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$ ä¸­æ‰€éœ€çš„ç¬¦å·ã€‚è¿™å¯¼è‡´äº† $O(2^{n_{1}}D^{{n_{0}}n_{1}}\text{poly}(D,{n_{0}},n_{1}))$
    çš„è¿è¡Œæ—¶é—´ã€‚Froese ç­‰äººï¼ˆ[2022](#bib.bib110)ï¼‰æœ€è¿‘å°†è¯¥ç®—æ³•æ¨å¹¿åˆ°äº†å‡¹æŸå¤±å‡½æ•°çš„æƒ…å†µã€‚'
- en: Dey etÂ al. ([2020](#bib.bib81)) developed a polynomial-time approximation algorithm
    in this setting for the case of $n_{1}=1$ (i.e., one ReLU neuron) and square loss.
    This approximation algorithm has a better performance when the input dimension
    is much larger than the sample size, i.e. ${n_{0}}\gg D$. The approach by Dey
    etÂ al. ([2020](#bib.bib81)) also relies on fixing the signs of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$,
    and then solving multiple convex optimization problems, but in different strategy
    than that of Arora etÂ al. ([2018](#bib.bib8)); in particular, Dey etÂ al. ([2020](#bib.bib81))
    only explore a polynomial number of the possible â€œfixingsâ€, which yields the approximation.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: Dey ç­‰äººï¼ˆ[2020](#bib.bib81)ï¼‰åœ¨è¿™ç§æƒ…å†µä¸‹ä¸º $n_{1}=1$ï¼ˆå³ä¸€ä¸ª ReLU ç¥ç»å…ƒï¼‰å’Œå¹³æ–¹æŸå¤±å¼€å‘äº†ä¸€ç§å¤šé¡¹å¼æ—¶é—´çš„è¿‘ä¼¼ç®—æ³•ã€‚å½“è¾“å…¥ç»´åº¦è¿œå¤§äºæ ·æœ¬å¤§å°ï¼Œå³
    ${n_{0}}\gg D$ æ—¶ï¼Œè¯¥è¿‘ä¼¼ç®—æ³•çš„è¡¨ç°æ›´ä½³ã€‚Dey ç­‰äººï¼ˆ[2020](#bib.bib81)ï¼‰çš„æ–¹æ³•ä¹Ÿä¾èµ–äºå›ºå®š ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$
    çš„ç¬¦å·ï¼Œç„¶åè§£å†³å¤šä¸ªå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œä½†ç­–ç•¥ä¸åŒäº Arora ç­‰äººï¼ˆ[2018](#bib.bib8)ï¼‰çš„æ–¹æ³•ï¼›ç‰¹åˆ«æ˜¯ï¼ŒDey ç­‰äººï¼ˆ[2020](#bib.bib81)ï¼‰åªæ¢ç´¢äº†å¤šé¡¹å¼æ•°é‡çš„å¯èƒ½â€œå›ºå®šâ€ï¼Œä»è€Œå¾—å‡ºè¿‘ä¼¼ç»“æœã€‚
- en: 'We note that the result by Arora etÂ al. ([2018](#bib.bib8)) shows that the
    training problem on their architecture is in NP. This is in contrast to Bertschinger
    etÂ al. ([2022](#bib.bib27)), who show that training a neural network with one
    hidden layer is likely to not be in NP. The big difference lies in the assumption
    on the output dimension: in the case of Bertschinger etÂ al. ([2022](#bib.bib27)),
    the output dimension is two. It is quite remarkable that such a sharp complexity
    gap is produced by a small change in the output dimension.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ³¨æ„åˆ°ï¼ŒArora ç­‰äººï¼ˆ[2018](#bib.bib8)ï¼‰çš„ç»“æœæ˜¾ç¤ºï¼Œä»–ä»¬çš„æ¶æ„ä¸Šçš„è®­ç»ƒé—®é¢˜å±äº NP ç±»ã€‚è¿™ä¸ Bertschinger ç­‰äººï¼ˆ[2022](#bib.bib27)ï¼‰çš„ç ”ç©¶å½¢æˆå¯¹æ¯”ï¼Œåè€…æ˜¾ç¤ºè®­ç»ƒä¸€ä¸ªå…·æœ‰éšè—å±‚çš„ç¥ç»ç½‘ç»œå¯èƒ½ä¸å±äº
    NPã€‚ä¸»è¦å·®å¼‚åœ¨äºè¾“å‡ºç»´åº¦çš„å‡è®¾ï¼šåœ¨ Bertschinger ç­‰äººï¼ˆ[2022](#bib.bib27)ï¼‰çš„æƒ…å†µä¸‹ï¼Œè¾“å‡ºç»´åº¦ä¸ºäºŒã€‚å¦‚æ­¤å°çš„è¾“å‡ºç»´åº¦å˜åŒ–ç«Ÿç„¶äº§ç”Ÿäº†å¦‚æ­¤æ˜¾è‘—çš„å¤æ‚æ€§å·®è·ï¼Œå®åœ¨æ˜¯éå¸¸å¼•äººæ³¨ç›®ã€‚
- en: 5.1.4 An exact training algorithm for arbitrary LTU architectures
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4 ä»»æ„ LTU æ¶æ„çš„ç²¾ç¡®è®­ç»ƒç®—æ³•
- en: Recently, Khalife and Basu ([2022](#bib.bib173)) presented a new algorithm,
    akin to that in Arora etÂ al. ([2018](#bib.bib8)), capable of solving the training
    problem to global optimality for any fixed LTU architecture with a convex loss
    function $\ell$. In this case, no assumption on the networkâ€™s depth is made. The
    algorithm runs in polynomial time on the sample size $D$ when the architecture
    is fixed.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼ŒKhalife å’Œ Basuï¼ˆ[2022](#bib.bib173)ï¼‰æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼Œç±»ä¼¼äº Arora ç­‰äººï¼ˆ[2018](#bib.bib8)ï¼‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä»»ä½•å›ºå®šçš„
    LTU æ¶æ„ä¸‹å¯¹å…·æœ‰å‡¸æŸå¤±å‡½æ•° $\ell$ çš„è®­ç»ƒé—®é¢˜æ±‚è§£è‡³å…¨å±€æœ€ä¼˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸å¯¹ç½‘ç»œçš„æ·±åº¦åšä»»ä½•å‡è®¾ã€‚è¯¥ç®—æ³•åœ¨æ¶æ„å›ºå®šæ—¶ï¼Œæ ·æœ¬å¤§å° $D$ ä¸Šè¿è¡Œæ—¶é—´ä¸ºå¤šé¡¹å¼æ—¶é—´ã€‚
- en: We will not describe this approach in detail, as it heavily relies on the structure
    given by LTU activations, which is intricate and beyond the scope of this survey.
    Although we note that it shares some high-level similarities to the algorithm
    of Arora etÂ al. ([2018](#bib.bib8)) for ReLU activations, such as â€œguessingâ€ the
    behavior of the neuronsâ€™ activity and then solving multiple convex optimization
    problems. However, the structural and algorithmic details are considerably different.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä¼šè¯¦ç»†æè¿°è¿™ç§æ–¹æ³•ï¼Œå› ä¸ºå®ƒä¸¥é‡ä¾èµ–äºLTUæ¿€æ´»å‡½æ•°æ‰€æä¾›çš„ç»“æ„ï¼Œè¿™ä¸€ç»“æ„å¤æ‚ä¸”è¶…å‡ºäº†æœ¬è°ƒæŸ¥çš„èŒƒå›´ã€‚å°½ç®¡æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå®ƒåœ¨é«˜å±‚æ¬¡ä¸Šä¸Aroraç­‰äººï¼ˆ[2018](#bib.bib8)ï¼‰é’ˆå¯¹ReLUæ¿€æ´»å‡½æ•°çš„ç®—æ³•å­˜åœ¨ä¸€äº›ç›¸ä¼¼ä¹‹å¤„ï¼Œä¾‹å¦‚â€œçŒœæµ‹â€ç¥ç»å…ƒæ´»åŠ¨çš„è¡Œä¸ºï¼Œç„¶åè§£å†³å¤šä¸ªå‡¸ä¼˜åŒ–é—®é¢˜ã€‚ç„¶è€Œï¼Œç»“æ„å’Œç®—æ³•ç»†èŠ‚æœ‰å¾ˆå¤§ä¸åŒã€‚
- en: It is important to note that this result reveals the big gap between what is
    known for LTU versus ReLU activations in terms of their training problems. In
    the case of the former, there is an exact algorithm for arbitrary architectures;
    in the case of the latter, the known results are much more restricted and strong
    computational limitations exist.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œè¿™ä¸€ç»“æœæ­ç¤ºäº†LTUå’ŒReLUæ¿€æ´»å‡½æ•°åœ¨å…¶è®­ç»ƒé—®é¢˜ä¸Šçš„å·¨å¤§å·®è·ã€‚åœ¨å‰è€…çš„æƒ…å†µä¸‹ï¼Œæœ‰ä¸€ä¸ªé’ˆå¯¹ä»»æ„æ¶æ„çš„ç²¾ç¡®ç®—æ³•ï¼›è€Œåœ¨åè€…çš„æƒ…å†µä¸‹ï¼Œå·²çŸ¥ç»“æœåˆ™å—åˆ°æ›´å¤§é™åˆ¶ï¼Œå¹¶ä¸”å­˜åœ¨å¼ºå¤§çš„è®¡ç®—é™åˆ¶ã€‚
- en: 5.2 Convex reformulations in regularized training problems
  id: totrans-452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 æ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜ä¸­çš„å‡¸é‡æ„
- en: 'For the case when the training problem is regularized, the following stream
    of work has developed several convex reformulations of it. Pilanci and Ergen ([2020](#bib.bib248))
    presented the first convex reformulation of a training problem for the case with
    one hidden layer and one-dimensional outputs. As the approach described in Section
    [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU activations â€£ 5.1 Training
    neural networks with a single hidden layer â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey"), this
    reformulation uses hyperplane arrangements according to the activation patterns
    of the ReLU units, but instead of using them algorithmically directly, they use
    them to find their convex reformulations. This framework was further extended
    to CNNs by Ergen and Pilanci ([2021c](#bib.bib93)). Higher-dimensional outputs
    in neural networks with one hidden layer were considered in Ergen and Pilanci
    ([2020](#bib.bib90), [2021a](#bib.bib91)), Sahiner etÂ al. ([2021](#bib.bib268)).
    This convex optimization perspective was also applied in Batch Normalization by
    Ergen etÂ al. ([2022](#bib.bib96)).'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äºæ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜çš„æƒ…å†µï¼Œä»¥ä¸‹å·¥ä½œæµå¼€å‘äº†å‡ ä¸ªå‡¸é‡æ„ã€‚Pilanciå’ŒErgenï¼ˆ[2020](#bib.bib248)ï¼‰é¦–æ¬¡æå‡ºäº†ä¸€ä¸ªå…·æœ‰ä¸€ä¸ªéšè—å±‚å’Œä¸€ç»´è¾“å‡ºçš„è®­ç»ƒé—®é¢˜çš„å‡¸é‡æ„ã€‚æ­£å¦‚ç¬¬[5.1.3](#S5.SS1.SSS3
    "5.1.3 Fixed number of nodes and ReLU activations â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")èŠ‚ä¸­æè¿°çš„æ–¹æ³•ï¼Œè¿™ç§é‡æ„æ ¹æ®ReLUå•å…ƒçš„æ¿€æ´»æ¨¡å¼ä½¿ç”¨è¶…å¹³é¢æ’åˆ—ï¼Œä½†å¹¶ä¸æ˜¯ç›´æ¥ç®—æ³•ä½¿ç”¨å®ƒä»¬ï¼Œè€Œæ˜¯åˆ©ç”¨å®ƒä»¬æ¥å¯»æ‰¾å…¶å‡¸é‡æ„ã€‚è¿™ä¸ªæ¡†æ¶åœ¨Ergenå’ŒPilanciï¼ˆ[2021c](#bib.bib93)ï¼‰ä¸­è¢«è¿›ä¸€æ­¥æ‰©å±•åˆ°å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€‚Ergenå’ŒPilanciï¼ˆ[2020](#bib.bib90)ã€[2021a](#bib.bib91)ï¼‰ï¼ŒSahinerç­‰ï¼ˆ[2021](#bib.bib268)ï¼‰è€ƒè™‘äº†å…·æœ‰ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œä¸­çš„é«˜ç»´è¾“å‡ºã€‚è¿™ç§å‡¸ä¼˜åŒ–è§†è§’è¿˜è¢«åº”ç”¨äºæ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch
    Normalizationï¼‰ï¼Œç”±Ergenç­‰ï¼ˆ[2022](#bib.bib96)ï¼‰å®Œæˆã€‚'
- en: These approaches provide polynomial-time algorithms when some parameters (e.g.,
    the input dimension ${n_{0}}$) are considered constant. We note that this does
    not contradict the hardness result of Froese and Hertrich ([2023](#bib.bib109)),
    as the latter does not include a regularizing term. We explain below where the
    regularizing term plays an important role. Training via convex optimization was
    further developed to handle deeper regularized neural networks in Ergen and Pilanci
    ([2021b](#bib.bib92), [d](#bib.bib94), [e](#bib.bib95)).
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ–¹æ³•åœ¨æŸäº›å‚æ•°ï¼ˆä¾‹å¦‚è¾“å…¥ç»´åº¦${n_{0}}$ï¼‰è¢«è§†ä¸ºå¸¸é‡æ—¶æä¾›äº†å¤šé¡¹å¼æ—¶é—´ç®—æ³•ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œè¿™å¹¶ä¸ä¸Froeseå’ŒHertrichï¼ˆ[2023](#bib.bib109)ï¼‰çš„å›°éš¾ç»“æœç›¸çŸ›ç›¾ï¼Œå› ä¸ºåè€…æ²¡æœ‰åŒ…å«æ­£åˆ™åŒ–é¡¹ã€‚æˆ‘ä»¬åœ¨ä¸‹æ–‡ä¸­è§£é‡Šäº†æ­£åˆ™åŒ–é¡¹çš„é‡è¦ä½œç”¨ã€‚é€šè¿‡å‡¸ä¼˜åŒ–è¿›è¡Œè®­ç»ƒçš„æŠ€æœ¯åœ¨Ergenå’ŒPilanciï¼ˆ[2021b](#bib.bib92)ã€[d](#bib.bib94)ã€[e](#bib.bib95)ï¼‰ä¸­å¾—åˆ°äº†è¿›ä¸€æ­¥çš„å‘å±•ï¼Œä»¥å¤„ç†æ›´æ·±çš„æ­£åˆ™åŒ–ç¥ç»ç½‘ç»œã€‚
- en: In what follows, we review the convex reformulation in Pilanci and Ergen ([2020](#bib.bib248))
    (one hidden layer and one-dimensional output) to illustrate some of the base strategies
    behind these approaches. We refer the reader to the previously mentioned articles
    for the most recent and intricate developments, as well as numerical experiments.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†Pilanciå’ŒErgenï¼ˆ[2020](#bib.bib248)ï¼‰ä¸­çš„å‡¸é‡æ„ï¼ˆä¸€ä¸ªéšè—å±‚å’Œä¸€ç»´è¾“å‡ºï¼‰ï¼Œä»¥å±•ç¤ºè¿™äº›æ–¹æ³•èƒŒåçš„ä¸€äº›åŸºç¡€ç­–ç•¥ã€‚æˆ‘ä»¬å»ºè®®è¯»è€…å‚è€ƒå‰è¿°æ–‡ç« ï¼Œä»¥è·å–æœ€æ–°çš„å¤æ‚å‘å±•å’Œæ•°å€¼å®éªŒã€‚
- en: As before, let $n_{1}$ be the number of nodes in the hidden layer. Let us consider
    the following regularized training problem; to simplify the discussion, we omit
    biases.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œè®¾ $n_{1}$ ä¸ºéšå±‚ä¸­çš„èŠ‚ç‚¹æ•°ã€‚æˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹æ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜ï¼›ä¸ºç®€åŒ–è®¨è®ºï¼Œæˆ‘ä»¬çœç•¥äº†åç½®é¡¹ã€‚
- en: '|  | $\min_{{\bm{W}}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{\beta}{2}\sum_{j=1}^{n_{1}}(\&#124;{\bm{W}}^{1}_{j}\&#124;^{2}+({\bm{W}}^{2}_{j})^{2})$
    |  | (17) |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{{\bm{W}}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{\beta}{2}\sum_{j=1}^{n_{1}}(\&#124;{\bm{W}}^{1}_{j}\&#124;^{2}+({\bm{W}}^{2}_{j})^{2})$
    |  | (17) |'
- en: 'Here, $\beta>0$, $\tilde{{\bm{X}}}$ is a matrix whose $i$-th row is $\tilde{{\bm{x}}}_{i}$
    and ${\bm{W}}^{1}_{j}$ is the vector of weights going into neuron $j$. Thus, $\tilde{{\bm{X}}}{\bm{W}}^{1}_{j}$
    is a vector whose $i$-th component is the input to neuron $j$ when evaluating
    the network on $\tilde{{\bm{x}}}_{i}$. ${\bm{W}}^{2}_{j}$ is a scalar: it is the
    weight on the arc from neuron $j$ to the output neuron (one-dimensional). Note
    that there is a slight notation overload: $({\bm{W}}^{2}_{j})^{2}$ is the square
    of the scalar ${\bm{W}}^{2}_{j}$. However, we will quickly remove this (pontentially
    confusing) term.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ$\beta>0$ï¼Œ$\tilde{{\bm{X}}}$ æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå…¶ç¬¬ $i$ è¡Œæ˜¯ $\tilde{{\bm{x}}}_{i}$ï¼Œè€Œ ${\bm{W}}^{1}_{j}$
    æ˜¯è¿›å…¥ç¥ç»å…ƒ $j$ çš„æƒé‡å‘é‡ã€‚å› æ­¤ï¼Œ$\tilde{{\bm{X}}}{\bm{W}}^{1}_{j}$ æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå…¶ä¸­ç¬¬ $i$ ä¸ªåˆ†é‡æ˜¯è¯„ä¼°ç½‘ç»œæ—¶ç¥ç»å…ƒ
    $j$ çš„è¾“å…¥ï¼ˆå¯¹äº $\tilde{{\bm{x}}}_{i}$ï¼‰ã€‚${\bm{W}}^{2}_{j}$ æ˜¯ä¸€ä¸ªæ ‡é‡ï¼šå®ƒæ˜¯ä»ç¥ç»å…ƒ $j$ åˆ°è¾“å‡ºç¥ç»å…ƒï¼ˆä¸€ç»´ï¼‰çš„å¼§ä¸Šçš„æƒé‡ã€‚æ³¨æ„ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªè½»å¾®çš„ç¬¦å·è¿‡è½½ï¼š$({\bm{W}}^{2}_{j})^{2}$
    æ˜¯æ ‡é‡ ${\bm{W}}^{2}_{j}$ çš„å¹³æ–¹ã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬å°†å¾ˆå¿«ç§»é™¤è¿™ä¸ªï¼ˆå¯èƒ½ä»¤äººå›°æƒ‘çš„ï¼‰é¡¹ã€‚
- en: 'Problem ([17](#S5.E17 "In 5.2 Convex reformulations in regularized training
    problems â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")) is a regularized version of ([15](#S5.E15
    "In 5.1.3 Fixed number of nodes and ReLU activations â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) when $\ell$ is the squared
    difference. We modified its presentation to match the structure in Pilanci and
    Ergen ([2020](#bib.bib248)). The authors first prove that ([17](#S5.E17 "In 5.2
    Convex reformulations in regularized training problems â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is equivalent to'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ ([17](#S5.E17 "åœ¨ 5.2 æ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜çš„å‡¸é‡æ„ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥")) æ˜¯ ([15](#S5.E15
    "åœ¨ 5.1.3 å›ºå®šèŠ‚ç‚¹æ•°å’Œ ReLU æ¿€æ´» â€£ 5.1 ç”¨å•éšå±‚è®­ç»ƒç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥")) çš„æ­£åˆ™åŒ–ç‰ˆæœ¬ï¼Œå…¶ä¸­
    $\ell$ æ˜¯å¹³æ–¹å·®ã€‚æˆ‘ä»¬ä¿®æ”¹äº†å…¶å‘ˆç°æ–¹å¼ä»¥åŒ¹é… Pilanci å’Œ Ergen ([2020](#bib.bib248)) ä¸­çš„ç»“æ„ã€‚ä½œè€…é¦–å…ˆè¯æ˜ ([17](#S5.E17
    "åœ¨ 5.2 æ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜çš„å‡¸é‡æ„ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šè°ƒæŸ¥")) ç­‰ä»·äº
- en: '|  | $\min_{\&#124;{\bm{W}}^{1}_{j}\&#124;\leq 1}\min_{{\bm{W}}^{2}_{j}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{n_{1}}&#124;{\bm{W}}^{2}_{j}&#124;$
    |  |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\&#124;{\bm{W}}^{1}_{j}\&#124;\leq 1}\min_{{\bm{W}}^{2}_{j}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{n_{1}}&#124;{\bm{W}}^{2}_{j}&#124;$
    |  |'
- en: Then, through a series of reformulations and duality arguments, the authors
    first show that this problem is lower bounded by
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œé€šè¿‡ä¸€ç³»åˆ—é‡æ„å’Œå¯¹å¶è®ºè¯ï¼Œä½œè€…é¦–å…ˆå±•ç¤ºäº†è¿™ä¸ªé—®é¢˜çš„ä¸‹ç•Œä¸º
- en: '|  |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\max$ | $\displaystyle\quad-\frac{1}{2}\left\&#124;v-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{1}{2}\&#124;\tilde{{\bm{y}}}\&#124;^{2}$
    |  | (18a) |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max$ | $\displaystyle\quad-\frac{1}{2}\left\&#124;v-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{1}{2}\&#124;\tilde{{\bm{y}}}\&#124;^{2}$
    |  | (18a) |'
- en: '|  | s.t | $\displaystyle\quad&#124;v^{\top}\sigma(\tilde{{\bm{X}}}u)&#124;\leq\beta$
    | $\displaystyle\forall u,\,\&#124;u\&#124;\leq 1$ |  | (18b) |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t | $\displaystyle\quad&#124;v^{\top}\sigma(\tilde{{\bm{X}}}u)&#124;\leq\beta$
    | $\displaystyle\forall u,\,\&#124;u\&#124;\leq 1$ |  | (18b) |'
- en: '|  |  | $\displaystyle\quad v\in\mathbb{R}^{D}$ |  | (18c) |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad v\in\mathbb{R}^{D}$ |  | (18c) |'
- en: 'Problem ([18](#S5.E18 "In 5.2 Convex reformulations in regularized training
    problems â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")) has $D$ variables and infinitely
    many constraints. The authors show that this lower bound is tight when the number
    of neurons in the hidden layer is large enough; specifically, they require $n_{1}\geq
    m^{*}$, where $m^{*}\in\{1,\ldots,D\}$ is defined as the number of Dirac deltas
    in an optimal solution of a dual of ([18](#S5.E18 "In 5.2 Convex reformulations
    in regularized training problems â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) (see Pilanci
    and Ergen ([2020](#bib.bib248)) for details).'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼ˆ[18](#S5.E18 "åœ¨ 5.2 èŠ‚ä¸­ï¼Œæ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜çš„å‡¸é‡æ„ â€£ 5 çº¿æ€§è§„åˆ’ä¸è®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰æœ‰
    $D$ ä¸ªå˜é‡å’Œæ— é™å¤šä¸ªçº¦æŸã€‚ä½œè€…å±•ç¤ºäº†å½“éšè—å±‚ä¸­çš„ç¥ç»å…ƒæ•°é‡è¶³å¤Ÿå¤§æ—¶ï¼Œè¿™ä¸ªä¸‹ç•Œæ˜¯ç´§çš„ï¼›å…·ä½“è€Œè¨€ï¼Œä»–ä»¬è¦æ±‚ $n_{1}\geq m^{*}$ï¼Œå…¶ä¸­ $m^{*}\in\{1,\ldots,D\}$
    å®šä¹‰ä¸ºï¼ˆ[18](#S5.E18 "åœ¨ 5.2 èŠ‚ä¸­ï¼Œæ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜çš„å‡¸é‡æ„ â€£ 5 çº¿æ€§è§„åˆ’ä¸è®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰çš„å¯¹å¶é—®é¢˜ä¸­ä¸€ä¸ªæœ€ä¼˜è§£ä¸­çš„
    Dirac Î´ çš„æ•°é‡ï¼ˆè¯¦ç»†ä¿¡æ¯è¯·å‚è§ Pilanci å’Œ Ergen ([2020](#bib.bib248)ï¼‰ï¼‰ã€‚
- en: 'Regarding the presence of infinitely many constraints, the authors address
    this by considering all possible patterns of signs of $\tilde{{\bm{X}}}u$ (similarly
    to Arora etÂ al. ([2018](#bib.bib8)), as discussed in Section [5.1.3](#S5.SS1.SSS3
    "5.1.3 Fixed number of nodes and ReLU activations â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). For each fixed sign
    pattern (hyperplane arrangement), they apply a duality argument which allows them
    to recast the constraint $\max_{u\in\mathcal{B}}|v^{\top}\sigma(\tilde{{\bm{X}}}u)|\leq\beta$
    as a finite collection of second-order cone constraints with $\beta$ on the right-hand
    side.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ— é™å¤šä¸ªçº¦æŸçš„å­˜åœ¨ï¼Œä½œè€…é€šè¿‡è€ƒè™‘ $\tilde{{\bm{X}}}u$ çš„æ‰€æœ‰å¯èƒ½çš„ç¬¦å·æ¨¡å¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼ˆç±»ä¼¼äº Arora ç­‰äºº ([2018](#bib.bib8)ï¼‰ï¼Œå¦‚ç¬¬
    [5.1.3](#S5.SS1.SSS3 "5.1.3 å›ºå®šèŠ‚ç‚¹æ•°å’Œ ReLU æ¿€æ´» â€£ 5.1 è®­ç»ƒå…·æœ‰å•ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’ä¸è®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º
    â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°") èŠ‚è®¨è®ºï¼‰ã€‚å¯¹äºæ¯ä¸ªå›ºå®šçš„ç¬¦å·æ¨¡å¼ï¼ˆè¶…å¹³é¢æ’åˆ—ï¼‰ï¼Œä»–ä»¬åº”ç”¨å¯¹å¶æ€§è®ºè¯ï¼Œä»è€Œå°†çº¦æŸ $\max_{u\in\mathcal{B}}|v^{\top}\sigma(\tilde{{\bm{X}}}u)|\leq\beta$
    è½¬åŒ–ä¸ºå³ä¾§å¸¦æœ‰ $\beta$ çš„äºŒé˜¶é”¥çº¦æŸçš„æœ‰é™é›†åˆã€‚
- en: 'Finally, using that $\beta>0$, they show that the reformulated problem satisfies
    Slaterâ€™s condition, and thus from strong duality they obtain the following convex
    optimization problem, which has the same objective value as ([18](#S5.E18 "In
    5.2 Convex reformulations in regularized training problems â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")).'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåˆ©ç”¨ $\beta>0$ï¼Œä»–ä»¬å±•ç¤ºäº†é‡æ„é—®é¢˜æ»¡è¶³ Slater æ¡ä»¶ï¼Œå› æ­¤é€šè¿‡å¼ºå¯¹å¶æ€§ï¼Œä»–ä»¬å¾—åˆ°ä»¥ä¸‹å‡¸ä¼˜åŒ–é—®é¢˜ï¼Œå…¶ç›®æ ‡å€¼ä¸ï¼ˆ[18](#S5.E18
    "åœ¨ 5.2 èŠ‚ä¸­ï¼Œæ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜çš„å‡¸é‡æ„ â€£ 5 çº¿æ€§è§„åˆ’ä¸è®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")ï¼‰ç›¸åŒã€‚
- en: '|  |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\min$ | $\displaystyle\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{P}M_{i}\tilde{{\bm{X}}}(v_{i}-w_{i})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{P}(\&#124;v_{i}\&#124;+\&#124;w_{i}\&#124;)$
    |  | (19a) |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min$ | $\displaystyle\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{P}M_{i}\tilde{{\bm{X}}}(v_{i}-w_{i})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{P}(\&#124;v_{i}\&#124;+\&#124;w_{i}\&#124;)$
    |  | (19a) |'
- en: '|  | s.t | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}v_{i}\geq 0$ |
    $\displaystyle\forall i\in[P]$ |  | (19b) |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}v_{i}\geq 0$ |
    $\displaystyle\forall i\in[P]$ |  | (19b) |'
- en: '|  |  | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}w_{i}\geq 0$ | $\displaystyle\forall
    i\in[P]$ |  | (19c) |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}w_{i}\geq 0$ | $\displaystyle\forall
    i\in[P]$ |  | (19c) |'
- en: '|  |  | $\displaystyle\quad v_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19d) |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad v_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19d) |'
- en: '|  |  | $\displaystyle\quad w_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19e) |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad w_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19e) |'
- en: 'Here, $I_{D}$ is the $D\times D$ identity matrix, $P$ is the number of possible
    activation patterns for $\tilde{{\bm{X}}}$, and each $M_{i}$ is a $D\times D$
    binary diagonal matrix whose diagonal indicates the $i$-th possible sign pattern
    of $\tilde{{\bm{X}}}u$. This means that $(M_{i})_{j,j}$ is 1 if and only if $\tilde{{\bm{x}}}_{j}^{\top}u\geq
    0$ in the $i$-th sign pattern of $\tilde{{\bm{X}}}u$. Moreover, the authors provide
    a formula to recover a solution of ([17](#S5.E17 "In 5.2 Convex reformulations
    in regularized training problems â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) from a solution
    of ([19](#S5.E19 "In 5.2 Convex reformulations in regularized training problems
    â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")).'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ$I_{D}$ æ˜¯ $D\times D$ å•ä½çŸ©é˜µï¼Œ$P$ æ˜¯ $\tilde{{\bm{X}}}$ çš„å¯èƒ½æ¿€æ´»æ¨¡å¼çš„æ•°é‡ï¼Œæ¯ä¸ª $M_{i}$
    æ˜¯ä¸€ä¸ª $D\times D$ çš„äºŒè¿›åˆ¶å¯¹è§’çŸ©é˜µï¼Œå…¶å¯¹è§’çº¿æŒ‡ç¤º $\tilde{{\bm{X}}}u$ çš„ç¬¬ $i$ ä¸ªå¯èƒ½ç¬¦å·æ¨¡å¼ã€‚è¿™æ„å‘³ç€ $(M_{i})_{j,j}$
    ä¸º 1 å½“ä¸”ä»…å½“ $\tilde{{\bm{x}}}_{j}^{\top}u\geq 0$ åœ¨ $\tilde{{\bm{X}}}u$ çš„ç¬¬ $i$ ä¸ªç¬¦å·æ¨¡å¼ä¸­ã€‚æ­¤å¤–ï¼Œä½œè€…æä¾›äº†ä¸€ä¸ªå…¬å¼ï¼Œç”¨ä»¥ä»
    ([19](#S5.E19 "åœ¨ 5.2 èŠ‚ æ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜ä¸­çš„å‡¸é‡æ„ â€£ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))
    çš„è§£æ¢å¤ ([17](#S5.E17 "åœ¨ 5.2 èŠ‚ æ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜ä¸­çš„å‡¸é‡æ„ â€£ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))
    çš„è§£ã€‚
- en: 'Using that $P\leq 2r(e(D-1)/r)^{r}$, where $r=\mbox{rank}(\tilde{{\bm{X}}})$,
    the authors note that the formulation ([19](#S5.E19 "In 5.2 Convex reformulations
    in regularized training problems â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) yields a
    training algorithm with complexity $O({n_{0}}^{3}r^{3}(D/r)^{3r})$. Note that
    if one fixes $r$, the resulting algorithm runs polynomial time. In particular,
    fixing ${n_{0}}$ fixes the rank of $\tilde{{\bm{X}}}$ and results in a polynomial
    time algorithm as well. In contrast, the algorithm by Arora etÂ al. ([2018](#bib.bib8))
    discussed in Section [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU
    activations â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey") remains exponential even after fixing ${n_{0}}$. Moreover,
    Froese and Hertrich ([2023](#bib.bib109)) showed that the training problem is
    NP-Hard even for fixed ${n_{0}}$. This apparent contradiction is explained by
    two key components of the convex reformulation: the regularization term and the
    presence of a â€œlarge enoughâ€ number of hidden neurons. This facilitates the exponential
    improvement of the training algorithm with respect to Arora etÂ al. ([2018](#bib.bib8)).'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ $P\leq 2r(e(D-1)/r)^{r}$ï¼Œå…¶ä¸­ $r=\mbox{rank}(\tilde{{\bm{X}}})$ï¼Œä½œè€…æŒ‡å‡ºï¼Œå…¬å¼ ([19](#S5.E19
    "åœ¨ 5.2 èŠ‚ æ­£åˆ™åŒ–è®­ç»ƒé—®é¢˜ä¸­çš„å‡¸é‡æ„ â€£ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) äº§ç”Ÿäº†ä¸€ä¸ªå¤æ‚åº¦ä¸º $O({n_{0}}^{3}r^{3}(D/r)^{3r})$
    çš„è®­ç»ƒç®—æ³•ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœå›ºå®š $r$ï¼Œå¾—åˆ°çš„ç®—æ³•åœ¨å¤šé¡¹å¼æ—¶é—´å†…è¿è¡Œã€‚ç‰¹åˆ«åœ°ï¼Œå›ºå®š ${n_{0}}$ ä¼šå›ºå®š $\tilde{{\bm{X}}}$ çš„ç§©ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªå¤šé¡¹å¼æ—¶é—´çš„ç®—æ³•ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒArora
    ç­‰äºº ([2018](#bib.bib8)) åœ¨ [5.1.3](#S5.SS1.SSS3 "5.1.3 èŠ‚ å›ºå®šèŠ‚ç‚¹æ•°å’Œ ReLU æ¿€æ´» â€£ 5.1 å•éšå±‚ç¥ç»ç½‘ç»œçš„è®­ç»ƒ
    â€£ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°") è®¨è®ºçš„ç®—æ³•å³ä½¿åœ¨å›ºå®š ${n_{0}}$ åä»ç„¶æ˜¯æŒ‡æ•°çº§çš„ã€‚æ­¤å¤–ï¼ŒFroese
    å’Œ Hertrich ([2023](#bib.bib109)) æ˜¾ç¤ºï¼Œå³ä½¿å¯¹äºå›ºå®šçš„ ${n_{0}}$ï¼Œè®­ç»ƒé—®é¢˜ä¹Ÿæ˜¯ NP-å›°éš¾çš„ã€‚è¿™ä¸€æ˜¾è‘—çš„çŸ›ç›¾å¯ä»¥é€šè¿‡å‡¸é‡æ„çš„ä¸¤ä¸ªå…³é”®ç»„ä»¶æ¥è§£é‡Šï¼šæ­£åˆ™åŒ–é¡¹å’Œâ€œè¶³å¤Ÿå¤§â€çš„éšè—ç¥ç»å…ƒæ•°é‡ã€‚è¿™æœ‰åŠ©äºè®­ç»ƒç®—æ³•ç›¸å¯¹äº
    Arora ç­‰äºº ([2018](#bib.bib8)) çš„æŒ‡æ•°çº§æ”¹è¿›ã€‚
- en: 5.3 Frank-Wolfe in DNN training algorithms
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 Frank-Wolfe åœ¨ DNN è®­ç»ƒç®—æ³•ä¸­çš„åº”ç”¨
- en: Another stream of work that has included components of linear programming in
    DNN training involves the Frank-Wolfe method. We briefly describe this method
    in the non-stochastic version next. In this section, we omit the biases ${\bm{b}}$
    to simplify the notation.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç±»åŒ…å«çº¿æ€§è§„åˆ’ç»„ä»¶çš„ DNN è®­ç»ƒç ”ç©¶æ¶‰åŠ Frank-Wolfe æ–¹æ³•ã€‚æˆ‘ä»¬æ¥ä¸‹æ¥ç®€è¦æè¿°è¯¥æ–¹æ³•çš„ééšæœºç‰ˆæœ¬ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬çœç•¥åå·® ${\bm{b}}$
    ä»¥ç®€åŒ–è¡¨ç¤ºã€‚
- en: Gradient descent (and its variants) is designed for problems of the form
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ï¼ˆåŠå…¶å˜ä½“ï¼‰æ—¨åœ¨è§£å†³å½¢å¼ä¸º
- en: '|  | $\min_{{\bm{W}}\in\mathbb{R}^{N}}\mathcal{L}({\bm{W}})$ |  | (20) |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{{\bm{W}}\in\mathbb{R}^{N}}\mathcal{L}({\bm{W}})$ |  | (20) |'
- en: and it is based on iterations of the form
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”åŸºäºä»¥ä¸‹å½¢å¼çš„è¿­ä»£
- en: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))$ |  |
    (21) |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))$ |  |
    (21) |'
- en: where $\alpha_{i}$ is known as the *learning rate*. In the stochastic versions,
    $\nabla\mathcal{L}({\bm{W}}(i))$ is replaced by a stochastic gradient. In this
    setting, these algorithms would find a local minimum, which is global when $\mathcal{L}$
    is convex.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\alpha_{i}$è¢«ç§°ä¸º*å­¦ä¹ ç‡*ã€‚åœ¨éšæœºç‰ˆæœ¬ä¸­ï¼Œ$\nabla\mathcal{L}({\bm{W}}(i))$è¢«ä¸€ä¸ªéšæœºæ¢¯åº¦æ‰€æ›¿ä»£ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™äº›ç®—æ³•å°†æ‰¾åˆ°å±€éƒ¨æœ€å°å€¼ï¼Œå½“$\mathcal{L}$æ˜¯å‡¸çš„æ—¶ï¼Œè¿™ä¸ªå±€éƒ¨æœ€å°å€¼ä¹Ÿæ˜¯å…¨å±€æœ€å°å€¼ã€‚
- en: 'In the presence of constraints ${\bm{W}}\in\Theta$, however, this strategy
    may not work directly. A regularizing term is typically used in the objective
    function instead of a constraint, that â€œencouragesâ€ ${\bm{W}}\in\Theta$ but does
    not enforce it. If we strictly require that ${\bm{W}}\in\Theta\neq\mathbb{R}^{n}$,
    and $\Theta$ is a convex set, one could modify ([21](#S5.E21 "In 5.3 Frank-Wolfe
    in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) to'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç„¶è€Œï¼Œåœ¨å­˜åœ¨çº¦æŸ${\bm{W}}\in\Theta$çš„æƒ…å†µä¸‹ï¼Œè¿™ç§ç­–ç•¥å¯èƒ½æ— æ³•ç›´æ¥å¥æ•ˆã€‚ç›®æ ‡å‡½æ•°ä¸­é€šå¸¸ä½¿ç”¨ä¸€ä¸ªæ­£åˆ™é¡¹æ¥â€œé¼“åŠ±â€${\bm{W}}\in\Theta$ï¼Œè€Œä¸æ˜¯å¼ºåˆ¶è¦æ±‚ã€‚å¦‚æœæˆ‘ä»¬ä¸¥æ ¼è¦æ±‚${\bm{W}}\in\Theta\neq\mathbb{R}^{n}$ï¼Œä¸”$\Theta$æ˜¯ä¸€ä¸ªå‡¸é›†ï¼Œå¯ä»¥å°†([21](#S5.E21
    "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey"))ä¿®æ”¹ä¸º'
- en: '|  | ${\bm{W}}(i+1)=\text{proj}_{\Theta}\left({\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))\right).$
    |  | (22) |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}(i+1)=\text{proj}_{\Theta}\left({\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))\right).$
    |  | (22) |'
- en: and thus ensure that all iterates ${\bm{W}}(i)\in\Theta$. Unfortunately, a projection
    is a costly routine. An alternative to this projection is the Frank-Wolfe method
    (Frank etÂ al., [1956](#bib.bib108)). Here, a direction ${\bm{d}}_{i}$ is computed
    via the following linear-objective convex optimization problem
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è€Œç¡®ä¿æ‰€æœ‰è¿­ä»£${\bm{W}}(i)\in\Theta$ã€‚ä¸å¹¸çš„æ˜¯ï¼ŒæŠ•å½±æ˜¯ä¸€ä¸ªæˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ã€‚æŠ•å½±çš„æ›¿ä»£æ–¹æ³•æ˜¯Frank-Wolfeæ–¹æ³•ï¼ˆFrankç­‰ï¼Œ[1956](#bib.bib108)ï¼‰ã€‚åœ¨è¿™é‡Œï¼Œé€šè¿‡ä»¥ä¸‹çº¿æ€§ç›®æ ‡å‡¸ä¼˜åŒ–é—®é¢˜è®¡ç®—æ–¹å‘${\bm{d}}_{i}$
- en: '|  | ${\bm{d}}_{i}\in\arg\min_{{\bm{d}}\in\Theta}{\bm{v}}_{i}^{\top}{\bm{d}}$
    |  | (23) |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{d}}_{i}\in\arg\min_{{\bm{d}}\in\Theta}{\bm{v}}_{i}^{\top}{\bm{d}}$
    |  | (23) |'
- en: where normally ${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$ (we consider variants
    below). The update is then computed as
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­é€šå¸¸${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$ï¼ˆæˆ‘ä»¬å°†åœ¨ä¸‹é¢è€ƒè™‘å˜ä½“ï¼‰ã€‚ç„¶åï¼Œæ›´æ–°è®¡ç®—ä¸º
- en: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)+\alpha_{i}({\bm{d}}_{i}-{\bm{W}}(i)),$ |  |
    (24) |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)+\alpha_{i}({\bm{d}}_{i}-{\bm{W}}(i)),$ |  |
    (24) |'
- en: 'for $\alpha_{i}\in[0,1]$. Note that, by convexity, we are assured that ${\bm{W}}(i+1)\in\Theta$
    as long as ${\bm{W}}(0)\in\Theta$. In many applications, $\Theta$ is polyhedral,
    which makes ([23](#S5.E23 "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) a linear program. Moreover, for simple sets $\Theta$, problem
    ([23](#S5.E23 "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) admits closed-form solutions.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äº$\alpha_{i}\in[0,1]$ã€‚è¯·æ³¨æ„ï¼Œç”±äºå‡¸æ€§ï¼Œåªè¦${\bm{W}}(0)\in\Theta$ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿${\bm{W}}(i+1)\in\Theta$ã€‚åœ¨è®¸å¤šåº”ç”¨ä¸­ï¼Œ$\Theta$æ˜¯å¤šé¢ä½“çš„ï¼Œè¿™ä½¿å¾—([23](#S5.E23
    "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey"))æˆä¸ºä¸€ä¸ªçº¿æ€§è§„åˆ’é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¯¹äºç®€å•çš„é›†åˆ$\Theta$ï¼Œé—®é¢˜([23](#S5.E23
    "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey"))å­˜åœ¨é—­å¼è§£ã€‚'
- en: 'In the context of deep neural network training, two notable applications of
    Frank-Wolfe have appeared. Firstly, the Deep Frank Wolfe algorithm, by Berrada
    etÂ al. ([2018](#bib.bib26)), which modifies iteration ([21](#S5.E21 "In 5.3 Frank-Wolfe
    in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) with an optimization
    problem that can be solved using Frank-Wolfe in its dual. Secondly, the use of
    a stochastic version of Frank-Wolfe in the training problem ([11](#S5.E11 "In
    5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) by Pokutta etÂ al. ([2020](#bib.bib249)) and Xie
    etÂ al. ([2020a](#bib.bib342)), which enforces structure in the neural network
    weights directly. We review these next, starting with the latter.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒçš„èƒŒæ™¯ä¸‹ï¼Œå‡ºç°äº†ä¸¤ä¸ªæ˜¾è‘—çš„Frank-Wolfeåº”ç”¨ã€‚é¦–å…ˆæ˜¯ Berrada ç­‰äºº ([2018](#bib.bib26)) æå‡ºçš„
    Deep Frank Wolfe ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡ä¸€ä¸ªå¯ä»¥ä½¿ç”¨å…¶å¯¹å¶é—®é¢˜çš„Frank-Wolfeè§£å†³çš„ä¼˜åŒ–é—®é¢˜æ¥ä¿®æ”¹è¿­ä»£ ([21](#S5.E21 "åœ¨5.3
    Frank-Wolfeåœ¨DNNè®­ç»ƒç®—æ³•ä¸­ â€£ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºä¸­çš„åŸ¹è®­ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºçš„ç¢°æ’ï¼šä¸€é¡¹è°ƒæŸ¥"))ã€‚å…¶æ¬¡ï¼ŒPokutta ç­‰äºº
    ([2020](#bib.bib249)) å’Œ Xie ç­‰äºº ([2020a](#bib.bib342)) åœ¨è®­ç»ƒé—®é¢˜ ([11](#S5.E11 "åœ¨5
    çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºä¸­çš„åŸ¹è®­ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºçš„ç¢°æ’ï¼šä¸€é¡¹è°ƒæŸ¥")) ä¸­ä½¿ç”¨äº†Frank-Wolfeçš„éšæœºç‰ˆæœ¬ï¼Œç›´æ¥åœ¨ç¥ç»ç½‘ç»œæƒé‡ä¸­æ–½åŠ ç»“æ„ã€‚æˆ‘ä»¬æ¥ä¸‹æ¥å›é¡¾è¿™äº›å†…å®¹ï¼Œä»åè€…å¼€å§‹ã€‚
- en: 5.3.1 Stochastic Frank-Wolfe
  id: totrans-492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 éšæœºFrank-Wolfe
- en: 'Note that problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) is of the
    form ([20](#S5.E20 "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) with'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åˆ°é—®é¢˜ ([11](#S5.E11 "åœ¨5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºä¸­çš„åŸ¹è®­ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºçš„ç¢°æ’ï¼šä¸€é¡¹è°ƒæŸ¥")) æ˜¯å½¢å¼ ([20](#S5.E20
    "åœ¨5.3 Frank-Wolfeåœ¨DNNè®­ç»ƒç®—æ³•ä¸­ â€£ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºä¸­çš„åŸ¹è®­ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºçš„ç¢°æ’ï¼šä¸€é¡¹è°ƒæŸ¥")) çš„
- en: '|  | $\mathcal{L}({\bm{W}})=\frac{1}{D}\sum_{i=1}^{D}\ell(f(\tilde{{\bm{x}}}_{i},{\bm{W}}),\tilde{{\bm{y}}}_{i}).$
    |  |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}({\bm{W}})=\frac{1}{D}\sum_{i=1}^{D}\ell(f(\tilde{{\bm{x}}}_{i},{\bm{W}}),\tilde{{\bm{y}}}_{i}).$
    |  |'
- en: We remind the reader that we are omitting the biases in this section to simplify
    notation, as they can be incorporated as part of ${\bm{W}}$.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æé†’è¯»è€…ï¼Œæœ¬èŠ‚ä¸­çœç•¥äº†åå·®ä»¥ç®€åŒ–ç¬¦å·ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä½œä¸º ${\bm{W}}$ çš„ä¸€éƒ¨åˆ†è¿›è¡Œæ•´åˆã€‚
- en: Usually, some structure of the weights is commonly desired, (e.g. sparsity or
    boundedness), which traditionally have been incorporated as regularizing terms
    in the objective, as mentioned above. The recent work by Xie etÂ al. ([2020a](#bib.bib342))
    and Pokutta etÂ al. ([2020](#bib.bib249)), on the other hand, enforce structure
    on $\Theta$ directly using Frank-Wolfe â€”more precisely, stochastic versions of
    it.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œä¸€äº›æƒé‡ç»“æ„æ˜¯å¸¸å¸¸æœŸæœ›çš„ï¼ˆä¾‹å¦‚ç¨€ç–æ€§æˆ–æœ‰ç•Œæ€§ï¼‰ï¼Œè¿™äº›é€šå¸¸ä½œä¸ºç›®æ ‡ä¸­çš„æ­£åˆ™åŒ–é¡¹è¢«çº³å…¥ï¼Œå¦‚ä¸Šæ‰€è¿°ã€‚å¦ä¸€æ–¹é¢ï¼ŒXie ç­‰äºº ([2020a](#bib.bib342))
    å’Œ Pokutta ç­‰äºº ([2020](#bib.bib249)) çš„æœ€æ–°å·¥ä½œç›´æ¥ä½¿ç”¨Frank-Wolfeï¼ˆæ›´å‡†ç¡®åœ°è¯´ï¼Œæ˜¯å…¶éšæœºç‰ˆæœ¬ï¼‰æ¥å¼ºåŠ å¯¹ $\Theta$
    çš„ç»“æ„ã€‚
- en: Xie etÂ al. ([2020a](#bib.bib342)) use a stochastic Frank-Wolfe approach to impose
    an $\ell_{1}$-norm constraint on the weights and biases ${\bm{W}}$ when training
    a neural network with 1 hidden layer. Note that $\ell_{1}$ constraints are polyhedral.
    Their algorithm is designed for a general Online Convex Optimization setting,
    where â€œlossesâ€ are revealed in each iteration. However, in their computational
    experiments, they included tests in an offline setting given by a DNN training
    problem.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: Xie ç­‰äºº ([2020a](#bib.bib342)) ä½¿ç”¨éšæœºFrank-Wolfeæ–¹æ³•åœ¨è®­ç»ƒä¸€ä¸ªå…·æœ‰1éšè—å±‚çš„ç¥ç»ç½‘ç»œæ—¶ï¼Œå¯¹æƒé‡å’Œåå·® ${\bm{W}}$
    æ–½åŠ äº† $\ell_{1}$-èŒƒæ•°çº¦æŸã€‚æ³¨æ„ï¼Œ$\ell_{1}$ çº¦æŸæ˜¯å¤šé¢ä½“çš„ã€‚ä»–ä»¬çš„ç®—æ³•è®¾è®¡ç”¨äºä¸€èˆ¬çš„åœ¨çº¿å‡¸ä¼˜åŒ–è®¾ç½®ï¼Œå…¶ä¸­â€œæŸå¤±â€åœ¨æ¯æ¬¡è¿­ä»£ä¸­æ˜¾ç°ã€‚ç„¶è€Œï¼Œåœ¨ä»–ä»¬çš„è®¡ç®—å®éªŒä¸­ï¼Œä»–ä»¬è¿˜åŒ…æ‹¬äº†ç”±DNNè®­ç»ƒé—®é¢˜æä¾›çš„ç¦»çº¿è®¾ç½®æµ‹è¯•ã€‚
- en: 'The approach follows the Frank-Wolfe method described above closely. The key
    difference lies in the estimation of the stochastic gradient they use, which is
    not standard and it is one of the most important aspects of the algorithm. Instead
    of using ${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$ in ([23](#S5.E23 "In 5.3
    Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")), the following
    *stochastic recursive estimator* of the gradient is used:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•ç´§å¯†è·Ÿéšä¸Šè¿°æè¿°çš„Frank-Wolfeæ–¹æ³•ã€‚å…³é”®çš„åŒºåˆ«åœ¨äºå®ƒä»¬ä½¿ç”¨çš„éšæœºæ¢¯åº¦ä¼°è®¡ï¼Œè¿™ä¸æ˜¯æ ‡å‡†çš„ï¼Œå¹¶ä¸”æ˜¯ç®—æ³•ä¸­æœ€é‡è¦çš„æ–¹é¢ä¹‹ä¸€ã€‚ä¸ ([23](#S5.E23
    "åœ¨5.3 Frank-Wolfeåœ¨DNNè®­ç»ƒç®—æ³•ä¸­ â€£ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºä¸­çš„åŸ¹è®­ â€£ æ·±åº¦å­¦ä¹ ä¸å¤šé¢ä½“ç†è®ºçš„ç¢°æ’ï¼šä¸€é¡¹è°ƒæŸ¥")) ä¸­ä½¿ç”¨çš„ ${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$
    ç›¸æ¯”ï¼Œä½¿ç”¨äº†ä»¥ä¸‹ *éšæœºé€’å½’ä¼°è®¡å™¨* è¿›è¡Œæ¢¯åº¦ä¼°è®¡ï¼š
- en: '|  | $\displaystyle{\bm{v}}_{0}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(0))$
    |  |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{v}}_{0}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(0))$
    |  |'
- en: '|  | $\displaystyle{\bm{v}}_{i}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(i))+(1-\rho_{i})(v_{i-1}-\tilde{\nabla}\mathcal{L}({\bm{W}}(i-1)))$
    |  |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{v}}_{i}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(i))+(1-\rho_{i})(v_{i-1}-\tilde{\nabla}\mathcal{L}({\bm{W}}(i-1)))$
    |  |'
- en: where $\tilde{\nabla}\mathcal{L}$ is a stochastic gradient, and $\rho_{i}$ is
    a parameter. The authors show that the gradient approximation error of this estimator
    converges to 0 at a sublinear rate, with high probability. This is important for
    them to analyze the â€œregret boundsâ€ they provide for the online setting.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\tilde{\nabla}\mathcal{L}$ æ˜¯éšæœºæ¢¯åº¦ï¼Œ$\rho_{i}$ æ˜¯ä¸€ä¸ªå‚æ•°ã€‚ä½œè€…å±•ç¤ºäº†è¿™ç§ä¼°è®¡å™¨çš„æ¢¯åº¦è¿‘ä¼¼è¯¯å·®ä»¥æ¬¡çº¿æ€§é€Ÿç‡æ”¶æ•›åˆ°0ï¼Œä¸”å…·æœ‰å¾ˆé«˜çš„æ¦‚ç‡ã€‚è¿™å¯¹äºä»–ä»¬åˆ†æâ€œåæ‚”ç•Œé™â€åœ¨åœ¨çº¿è®¾ç½®ä¸­çš„è¡¨ç°è‡³å…³é‡è¦ã€‚
- en: The experimental results in Xie etÂ al. ([2020a](#bib.bib342)) in DNN training
    are very positive. They test their approach in the MNIST and CIFAR10 datasets
    and outperform existing state-of-the-art approaches in terms of suboptimality,
    training accuracy, and test accuracy.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: Xie ç­‰äºº ([2020a](#bib.bib342)) åœ¨DNNè®­ç»ƒä¸­çš„å®éªŒç»“æœéå¸¸ç§¯æã€‚ä»–ä»¬åœ¨MNISTå’ŒCIFAR10æ•°æ®é›†ä¸Šæµ‹è¯•äº†ä»–ä»¬çš„æ–¹æ³•ï¼Œå¹¶åœ¨æ¬¡ä¼˜æ€§ã€è®­ç»ƒå‡†ç¡®æ€§å’Œæµ‹è¯•å‡†ç¡®æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚
- en: Pokutta etÂ al. ([2020](#bib.bib249)) implement and test several variants of
    stochastic versions of Frank-Wolfe in the training of neural networks, including
    the approach by Xie etÂ al. ([2020a](#bib.bib342)). Pokutta etÂ al. ([2020](#bib.bib249))
    focus their experiments on their main proposed variant, which they refer to simply
    as Stochastic Frank-Wolfe (SFW). This variant uses
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: Pokutta ç­‰äºº ([2020](#bib.bib249)) å®ç°å¹¶æµ‹è¯•äº†å‡ ç§Frank-Wolfeçš„éšæœºç‰ˆæœ¬å˜ä½“åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬Xieç­‰äºº
    ([2020a](#bib.bib342)) çš„æ–¹æ³•ã€‚Pokutta ç­‰äºº ([2020](#bib.bib249)) é‡ç‚¹ç ”ç©¶äº†ä»–ä»¬ä¸»è¦æå‡ºçš„å˜ä½“ï¼Œç®€ç§°ä¸ºéšæœºFrank-Wolfeï¼ˆSFWï¼‰ã€‚è¯¥å˜ä½“ä½¿ç”¨
- en: '|  | ${\bm{v}}_{i}=(1-\rho_{i}){\bm{v}}_{i-1}+\rho_{i}\tilde{\nabla}\mathcal{L}({\bm{W}}(i)),$
    |  |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{v}}_{i}=(1-\rho_{i}){\bm{v}}_{i-1}+\rho_{i}\tilde{\nabla}\mathcal{L}({\bm{W}}(i)),$
    |  |'
- en: where $\rho_{i}$ is a momentum parameter. The authors propose many different
    options for $\Theta$ including $\ell_{1},\ell_{2}$ and $\ell_{\infty}$ balls,
    and $K$-sparse polytopes. Of these, only the $\ell_{2}$ ball is non-polyhedral.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\rho_{i}$ æ˜¯åŠ¨é‡å‚æ•°ã€‚ä½œè€…æå‡ºäº†å¤šç§ä¸åŒçš„ $\Theta$ é€‰é¡¹ï¼ŒåŒ…æ‹¬ $\ell_{1}, \ell_{2}$ å’Œ $\ell_{\infty}$
    çƒä½“ï¼Œä»¥åŠ $K$-ç¨€ç–å¤šé¢ä½“ã€‚åœ¨è¿™äº›é€‰é¡¹ä¸­ï¼Œåªæœ‰ $\ell_{2}$ çƒä½“æ˜¯éå¤šé¢ä½“çš„ã€‚
- en: Overall, the computational experiments are promising for SFW. The authors advocate
    for this algorithm arguing that it provides excellent computational performances
    while being simple to implement and competitive with other state-of-the-art algorithms.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œè®¡ç®—å®éªŒå¯¹SFWçš„å‰æ™¯è¾ƒä¸ºä¹è§‚ã€‚ä½œè€…æå€¡è¿™ç§ç®—æ³•ï¼Œè®¤ä¸ºå®ƒåœ¨è®¡ç®—æ€§èƒ½æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶å®ç°ç®€å•ï¼Œå¹¶ä¸”ä¸å…¶ä»–æœ€å…ˆè¿›çš„ç®—æ³•å…·æœ‰ç«äº‰åŠ›ã€‚
- en: 5.3.2 Deep Frank-Wolfe
  id: totrans-507
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 æ·±åº¦Frank-Wolfe
- en: 'Another application of Frank-Wolfe within DNN training was proposed by Berrada
    etÂ al. ([2018](#bib.bib26)). While this approach does not make heavy use of linear
    programming techniques, the application of Frank-Wolfe is quite novel, and they
    do rely on one linear program needed when performing an update as ([24](#S5.E24
    "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")).'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: Berrada ç­‰äºº ([2018](#bib.bib26)) æå‡ºäº†Frank-Wolfeåœ¨DNNè®­ç»ƒä¸­çš„å¦ä¸€ä¸ªåº”ç”¨ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å¹¶æœªå¤§é‡ä½¿ç”¨çº¿æ€§è§„åˆ’æŠ€æœ¯ï¼Œä½†Frank-Wolfeçš„åº”ç”¨éå¸¸æ–°é¢–ï¼Œä»–ä»¬ç¡®å®ä¾èµ–äºåœ¨æ‰§è¡Œæ›´æ–°æ—¶æ‰€éœ€çš„ä¸€ä¸ªçº¿æ€§ç¨‹åºï¼Œå¦‚
    ([24](#S5.E24 "åœ¨5.3ä¸­ï¼ŒFrank-Wolfeåœ¨DNNè®­ç»ƒç®—æ³•ä¸­çš„åº”ç”¨ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ æ·±åº¦å­¦ä¹ é‡è§å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ã€‚
- en: 'The authors note that ([21](#S5.E21 "In 5.3 Frank-Wolfe in DNN training algorithms
    â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) can also be written as the solution to the
    following *proximal* problem (Bubeck etÂ al., [2015](#bib.bib40)):'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æŒ‡å‡º ([21](#S5.E21 "åœ¨5.3ä¸­ï¼ŒFrank-Wolfeåœ¨DNNè®­ç»ƒç®—æ³•ä¸­çš„åº”ç”¨ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ æ·±åº¦å­¦ä¹ é‡è§å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))
    ä¹Ÿå¯ä»¥è¢«å†™æˆä»¥ä¸‹ *è¿‘ä¼¼* é—®é¢˜çš„è§£ï¼ˆBubeckç­‰ï¼Œ[2015](#bib.bib40)ï¼‰ï¼š
- en: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\mathcal{T}_{{\bm{W}}(i)}(\mathcal{L}({\bm{W}}))\right\}$
    |  | (25) |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\mathcal{T}_{{\bm{W}}(i)}(\mathcal{L}({\bm{W}}))\right\}$
    |  | (25) |'
- en: 'where $\mathcal{T}_{{\bm{W}}(i)}$ represents the first-order Taylor expansion
    at ${\bm{W}}(i)$. We are omitting regularizing terms since they do not play a
    fundamental role in the approach; all this discussion can be directly extended
    to include regularizers. Berrada etÂ al. ([2018](#bib.bib26)) note that ([25](#S5.E25
    "In 5.3.2 Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) linearizes the loss function, and propose the following *loss-preserving
    proximal* problem to replace ([25](#S5.E25 "In 5.3.2 Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe
    in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")):'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathcal{T}_{{\bm{W}}(i)}$ ä»£è¡¨ ${\bm{W}}(i)$ å¤„çš„ä¸€çº§æ³°å‹’å±•å¼€ã€‚æˆ‘ä»¬çœç•¥äº†æ­£åˆ™åŒ–é¡¹ï¼Œå› ä¸ºå®ƒä»¬åœ¨è¿™ç§æ–¹æ³•ä¸­å¹¶ä¸å‘æŒ¥æ ¹æœ¬ä½œç”¨ï¼›æ‰€æœ‰è¿™äº›è®¨è®ºå¯ä»¥ç›´æ¥æ‰©å±•ä»¥åŒ…å«æ­£åˆ™åŒ–é¡¹ã€‚Berrada
    ç­‰äºº ([2018](#bib.bib26)) æŒ‡å‡º ([25](#S5.E25 "åœ¨ 5.3.2 æ·±åº¦ Frank-Wolfe â€£ 5.3 Frank-Wolfe
    åœ¨ DNN è®­ç»ƒç®—æ³•ä¸­ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) çº¿æ€§åŒ–äº†æŸå¤±å‡½æ•°ï¼Œå¹¶æå‡ºä»¥ä¸‹ *ä¿æŒæŸå¤±* çš„è¿‘ç«¯é—®é¢˜æ¥æ›¿ä»£
    ([25](#S5.E25 "åœ¨ 5.3.2 æ·±åº¦ Frank-Wolfe â€£ 5.3 Frank-Wolfe åœ¨ DNN è®­ç»ƒç®—æ³•ä¸­ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨
    â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ï¼š
- en: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\frac{1}{D}\sum_{i=1}^{D}\ell(\mathcal{T}_{{\bm{W}}(i)}(f(\tilde{{\bm{x}}}_{i},{\bm{W}})),\tilde{{\bm{y}}}_{i})\right\}$
    |  | (26) |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\frac{1}{D}\sum_{i=1}^{D}\ell(\mathcal{T}_{{\bm{W}}(i)}(f(\tilde{{\bm{x}}}_{i},{\bm{W}})),\tilde{{\bm{y}}}_{i})\right\}$
    |  | (26) |'
- en: 'Using the results by Lacoste-Julien etÂ al. ([2013](#bib.bib182)), the authors
    argue that ([26](#S5.E26 "In 5.3.2 Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe in DNN training
    algorithms â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")) is amenable to Frank-Wolfe in the
    dual when $\ell$ is piecewise linear and convex (e.g. the hinge loss). To be more
    specific, the authors show that in this case, and assuming $\alpha_{i}=\alpha$,
    there exists ${\bm{A}},{\bm{b}}$ such that the dual of ([26](#S5.E26 "In 5.3.2
    Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is simply'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Lacoste-Julien ç­‰äººçš„ç»“æœ ([2013](#bib.bib182))ï¼Œä½œè€…è®¤ä¸ºå½“ $\ell$ æ˜¯åˆ†æ®µçº¿æ€§å’Œå‡¸çš„ï¼ˆä¾‹å¦‚é“°é“¾æŸå¤±ï¼‰æ—¶ï¼Œ([26](#S5.E26
    "åœ¨ 5.3.2 æ·±åº¦ Frank-Wolfe â€£ 5.3 Frank-Wolfe åœ¨ DNN è®­ç»ƒç®—æ³•ä¸­ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£
    å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) å¯ä»¥åœ¨å¯¹å¶ä¸­åº”ç”¨ Frank-Wolfeã€‚æ›´å…·ä½“åœ°è¯´ï¼Œä½œè€…å±•ç¤ºäº†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡è®¾ $\alpha_{i}=\alpha$ï¼Œå­˜åœ¨
    ${\bm{A}},{\bm{b}}$ ä½¿å¾— ([26](#S5.E26 "åœ¨ 5.3.2 æ·±åº¦ Frank-Wolfe â€£ 5.3 Frank-Wolfe
    åœ¨ DNN è®­ç»ƒç®—æ³•ä¸­ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) çš„å¯¹å¶å½¢å¼ç®€å•ä¸º
- en: '|  |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\max_{\mathbf{\beta}}\quad$ | $\displaystyle\frac{-1}{2\alpha}\&#124;{\bm{A}}\mathbf{\beta}\&#124;^{2}+{\bm{b}}^{\top}\mathbf{\beta}$
    |  | (27a) |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\mathbf{\beta}}\quad$ | $\displaystyle\frac{-1}{2\alpha}\&#124;{\bm{A}}\mathbf{\beta}\&#124;^{2}+{\bm{b}}^{\top}\mathbf{\beta}$
    |  | (27a) |'
- en: '|  | s.t. | $\displaystyle\mathbf{1}^{\top}\mathbf{\beta}=1$ |  | (27b) |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle\mathbf{1}^{\top}\mathbf{\beta}=1$ |  | (27b) |'
- en: '|  |  | $\displaystyle\mathbf{\beta}\geq 0$ |  | (27c) |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbf{\beta}\geq 0$ |  | (27c) |'
- en: 'The authors consider applying Frank-Wolfe to this last problem, and to recover
    the primal solution using the primal-dual relation ${\bm{W}}=-{\bm{A}}\mathbf{\beta}$,
    which is a consequence of KKT. The Frank-Wolfe iteration ([24](#S5.E24 "In 5.3
    Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) in the notation
    of ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe in DNN training
    algorithms â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")) would look like'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…è€ƒè™‘å°† Frank-Wolfe åº”ç”¨äºæœ€åè¿™ä¸ªé—®é¢˜ï¼Œå¹¶ä½¿ç”¨åŸå§‹-å¯¹å¶å…³ç³» ${\bm{W}}=-{\bm{A}}\mathbf{\beta}$ æ¥æ¢å¤åŸå§‹è§£ï¼Œè¿™æ˜¯
    KKT çš„ä¸€ä¸ªç»“æœã€‚åœ¨ ([24](#S5.E24 "åœ¨ 5.3 Frank-Wolfe åœ¨ DNN è®­ç»ƒç®—æ³•ä¸­ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨
    â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) çš„ç¬¦å·ä¸­ï¼ŒFrank-Wolfe è¿­ä»£ ([27](#S5.E27 "åœ¨ 5.3.2 æ·±åº¦ Frank-Wolfe
    â€£ 5.3 Frank-Wolfe åœ¨ DNN è®­ç»ƒç®—æ³•ä¸­ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) å½¢å¼å¦‚ä¸‹
- en: '|  | $\mathbf{\beta}_{i+1}=\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i}).$
    |  | (28) |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\beta}_{i+1}=\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i}).$
    |  | (28) |'
- en: 'Here, ${\bm{d}}_{i}$ is feasible for ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe
    â€£ 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) and
    obtained using a linear programming oracle, and $\gamma_{i}$ the Frank-Wolfe step-length.
    Note that the feasible region of ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe â€£ 5.3
    Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) is a simplex:
    exploiting this, the authors show that an optimal $\gamma_{i}$ can be computed
    in closed-form: here, â€œoptimalâ€ refers to a minimizer of ([27](#S5.E27 "In 5.3.2
    Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) when restricted to points of the form $\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i})$.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ${\bm{d}}_{i}$æ˜¯å¯¹([27](#S5.E27 "åœ¨5.3.2æ·±åº¦Frank-Wolfeä¸­ â€£ 5.3 DNNè®­ç»ƒç®—æ³•ä¸­çš„Frank-Wolfe
    â€£ 5çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡è§å¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥"))çš„å¯è¡Œè§£ï¼Œæ˜¯ä½¿ç”¨çº¿æ€§è§„åˆ’Oracleè·å¾—çš„ï¼Œ$\gamma_{i}$æ˜¯Frank-Wolfeæ­¥é•¿ã€‚è¯·æ³¨æ„ï¼Œ([27](#S5.E27
    "åœ¨5.3.2æ·±åº¦Frank-Wolfeä¸­ â€£ 5.3 DNNè®­ç»ƒç®—æ³•ä¸­çš„Frank-Wolfe â€£ 5çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡è§å¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥"))çš„å¯è¡ŒåŒºåŸŸæ˜¯ä¸€ä¸ªå•çº¯å½¢ï¼šä½œè€…å±•ç¤ºäº†åœ¨è¿™ç§æƒ…å†µä¸‹å¯ä»¥å°é—­å½¢å¼åœ°è®¡ç®—å‡ºæœ€ä¼˜çš„$\gamma_{i}$ï¼šåœ¨è¿™é‡Œï¼Œâ€œæœ€ä¼˜â€æ˜¯æŒ‡å½“é™åˆ¶åœ¨å½¢å¼ä¸º$\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i})$çš„ç‚¹ä¸Šæ—¶ï¼Œæ˜¯([27](#S5.E27
    "åœ¨5.3.2æ·±åº¦Frank-Wolfeä¸­ â€£ 5.3 DNNè®­ç»ƒç®—æ³•ä¸­çš„Frank-Wolfe â€£ 5çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡è§å¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥"))çš„æœ€å°åŒ–å™¨ã€‚
- en: 'With all these considerations, the bottleneck in this application of Frank-Wolfe
    is obtaining ${\bm{d}}_{i}$; recall that this Frank-Wolfe routine is embedded
    within a single iteration of the overall training algorithm; therefore, in each
    iteration of the training algorithm, possibly multiple computations of ${\bm{d}}_{i}$
    would be required in order to solve ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe â€£
    5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) to
    optimality. To alleviate this, the authors propose to only perform one iteration
    of Frank-Wolfe: they set ${\bm{d}}_{0}$ to be the stochastic gradient and compute
    a closed-form expression for $\mathbf{\beta}_{1}$. This is the basic ingredient
    of the Deep Frank Wolfe (DFW). It is worth noting that this algorithm is not guaranteed
    to converge, however, its empirical performance is competitive.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°æ‰€æœ‰è¿™äº›å› ç´ ï¼ŒFrank-Wolfe åœ¨æ­¤åº”ç”¨ä¸­çš„ç“¶é¢ˆæ˜¯è·å¾—${\bm{d}}_{i}$ï¼›è¯·è®°ä½ï¼Œè¿™ä¸ªFrank-Wolfeä¾‹ç¨‹åµŒå…¥åˆ°æ•´ä¸ªè®­ç»ƒç®—æ³•çš„å•æ¬¡è¿­ä»£ä¸­ï¼›å› æ­¤ï¼Œåœ¨è®­ç»ƒç®—æ³•çš„æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå¯èƒ½éœ€è¦å¤šæ¬¡è®¡ç®—${\bm{d}}_{i}$æ‰èƒ½ä»¥æœ€ä¼˜æ–¹å¼è§£å†³([27](#S5.E27
    "åœ¨5.3.2æ·±åº¦Frank-Wolfeä¸­ â€£ 5.3 DNNè®­ç»ƒç®—æ³•ä¸­çš„Frank-Wolfe â€£ 5çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡è§å¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥"))ã€‚ä¸ºäº†å‡è½»è¿™ä¸€æƒ…å†µï¼Œä½œè€…å»ºè®®ä»…æ‰§è¡Œä¸€æ¬¡Frank-Wolfeçš„è¿­ä»£ï¼šä»–ä»¬å°†${\bm{d}}_{0}$è®¾ç½®ä¸ºéšæœºæ¢¯åº¦ï¼Œè®¡ç®—$\mathbf{\beta}_{1}$çš„å°é—­è¡¨è¾¾å¼ã€‚è¿™æ˜¯æ·±åº¦Frank
    Wolfe (DFW)çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥ç®—æ³•ä¸èƒ½ä¿è¯æ”¶æ•›ï¼Œä½†å…¶å®é™…è¡¨ç°æ˜¯æœ‰ç«äº‰åŠ›çš„ã€‚
- en: 'Other two important considerations are taken into account the implementation
    of this algorithm: smoothing of the loss function (as the Hinge loss is piecewise
    linear) and the adaptation of Nesterovâ€™s Momentum to this new setting. We refer
    the reader to the corresponding article for these details. One of the key features
    of DFW is that it only requires one hyperparameter ($\alpha$) to be tuned.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: å®æ–½è¿™ä¸€ç®—æ³•è¿˜è€ƒè™‘äº†å…¶ä»–ä¸¤ä¸ªé‡è¦å› ç´ ï¼šæŸå¤±å‡½æ•°çš„å¹³æ»‘åŒ–ï¼ˆå› ä¸ºHingeæŸå¤±æ˜¯åˆ†æ®µçº¿æ€§çš„ï¼‰å’Œå°†Nesterovçš„åŠ¨é‡è°ƒæ•´åˆ°è¿™ä¸ªæ–°ç¯å¢ƒä¸­ã€‚å…³äºè¿™äº›ç»†èŠ‚ï¼Œæˆ‘ä»¬å»ºè®®è¯»è€…å‚è€ƒç›¸åº”çš„æ–‡ç« ã€‚DFWçš„ä¸€ä¸ªå…³é”®ç‰¹ç‚¹æ˜¯å®ƒåªéœ€è¦è°ƒæ•´ä¸€ä¸ªè¶…å‚æ•°ï¼ˆ$\alpha$ï¼‰ã€‚
- en: 'The authors test DFW in image classification and natural language inference.
    Overall, the results obtained by DFW are very positive: in most cases, it can
    outperform adaptive gradient methods, and it is competitive with SGD while converging
    faster.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…åœ¨å›¾åƒåˆ†ç±»å’Œè‡ªç„¶è¯­è¨€æ¨ç†ä¸­æµ‹è¯•äº†DFWã€‚æ€»ä½“è€Œè¨€ï¼ŒDFWå–å¾—çš„ç»“æœéå¸¸ç§¯æï¼šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®ƒå¯ä»¥ä¼˜äºè‡ªé€‚åº”æ¢¯åº¦æ³•ï¼Œå¹¶ä¸”åœ¨æ”¶æ•›é€Ÿåº¦æ–¹é¢ä¸SGDç›¸ç«äº‰ã€‚
- en: 5.4 Polyhedral encoding of multiple training problems
  id: totrans-524
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 å¯¹å¤šä¸ªè®­ç»ƒé—®é¢˜çš„å¤šé¢ä½“ç¼–ç 
- en: 'One of the questions raised by Arora etÂ al. ([2018](#bib.bib8)) (see Section
    [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU activations â€£ 5.1 Training
    neural networks with a single hidden layer â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) was
    whether the dependency on $D$ of their algorithm could be improved since it is
    typically the largest coefficient in a training problem. This question was studied
    by Bienstock etÂ al. ([2023](#bib.bib32)), who show that, in an approximation setting,
    a more ambitious goal is achievable: there is a polyhedral encoding of multiple
    training problems whose size has a mild dependency on $D$.'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: Arora ç­‰äºº ([2018](#bib.bib8)) æå‡ºçš„ä¸€ä¸ªé—®é¢˜ï¼ˆè§ç¬¬ [5.1.3](#S5.SS1.SSS3 "5.1.3 å›ºå®šèŠ‚ç‚¹æ•°å’Œ ReLU
    æ¿€æ´» â€£ 5.1 è®­ç»ƒå…·æœ‰å•ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºä¸­çš„è®­ç»ƒ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°") èŠ‚ï¼‰æ˜¯ï¼Œä»–ä»¬çš„ç®—æ³•å¯¹ $D$
    çš„ä¾èµ–æ˜¯å¦å¯ä»¥å¾—åˆ°æ”¹å–„ï¼Œå› ä¸º $D$ é€šå¸¸æ˜¯è®­ç»ƒé—®é¢˜ä¸­çš„æœ€å¤§ç³»æ•°ã€‚Bienstock ç­‰äºº ([2023](#bib.bib32)) ç ”ç©¶äº†è¿™ä¸ªé—®é¢˜ï¼Œè¯æ˜åœ¨ä¸€ä¸ªè¿‘ä¼¼è®¾ç½®ä¸­ï¼Œå¯ä»¥å®ç°ä¸€ä¸ªæ›´é›„å¿ƒå‹ƒå‹ƒçš„ç›®æ ‡ï¼šå­˜åœ¨å¤šä¸ªè®­ç»ƒé—®é¢˜çš„å¤šé¢ä½“ç¼–ç ï¼Œå…¶å¤§å°å¯¹
    $D$ çš„ä¾èµ–è¾ƒå°ã€‚
- en: 'As in the previous section, we omit the biases ${\bm{b}}$ to simplify notation,
    as all parameters can be included in ${\bm{W}}$. Let us assume the class of neural
    networks $F$ in ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in
    Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) are restricted
    to have bounded parameters (we assume they lie in the interval $[-1,1]$), and
    let us assume the sample has been normalized in such a way that $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$.
    Furthermore, let $N$ be the dimension of $\Theta$ (the number of parameters in
    the neural network). With this notation, we define the following.'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰ä¸€èŠ‚æ‰€è¿°ï¼Œæˆ‘ä»¬çœç•¥äº†åå·® ${\bm{b}}$ ä»¥ç®€åŒ–ç¬¦å·ï¼Œå› ä¸ºæ‰€æœ‰å‚æ•°éƒ½å¯ä»¥åŒ…å«åœ¨ ${\bm{W}}$ ä¸­ã€‚æˆ‘ä»¬å‡è®¾ç¥ç»ç½‘ç»œç±» $F$ åœ¨ ([11](#S5.E11
    "åœ¨5çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºä¸­çš„è®­ç»ƒ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) ä¸­è¢«é™åˆ¶ä¸ºå…·æœ‰æœ‰ç•Œå‚æ•°ï¼ˆæˆ‘ä»¬å‡è®¾å®ƒä»¬ä½äºåŒºé—´ $[-1,1]$ ä¸­ï¼‰ï¼Œå¹¶ä¸”å‡è®¾æ ·æœ¬å·²è¢«æ ‡å‡†åŒ–ï¼Œä½¿å¾—
    $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$ã€‚æ­¤å¤–ï¼Œè®¾
    $N$ ä¸º $\Theta$ çš„ç»´åº¦ï¼ˆç¥ç»ç½‘ç»œä¸­çš„å‚æ•°æ•°é‡ï¼‰ã€‚ä½¿ç”¨è¿™ç§ç¬¦å·ï¼Œæˆ‘ä»¬å®šä¹‰å¦‚ä¸‹ã€‚
- en: Definition 2
  id: totrans-527
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šä¹‰ 2
- en: 'Consider the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) with
    parameters $D,\Theta,\ell,f$ â€” sample size, parameter space, loss function, network
    architecture, respectively. For a function $g$, let $\mathcal{K}_{\infty}(g)$
    be the Lipschitz constant of $g$ using the infinity norm. We define the *Architecture
    Lipschitz Constant* $\mathcal{K}(D,\Theta,\ell,f)$ as'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ ERM é—®é¢˜ ([11](#S5.E11 "åœ¨5çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºä¸­çš„è®­ç»ƒ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ï¼Œå‚æ•°ä¸º $D,\Theta,\ell,f$
    â€”â€” æ ·æœ¬å¤§å°ã€å‚æ•°ç©ºé—´ã€æŸå¤±å‡½æ•°ã€ç½‘ç»œæ¶æ„ï¼Œåˆ†åˆ«ã€‚å¯¹äºä¸€ä¸ªå‡½æ•° $g$ï¼Œè®¾ $\mathcal{K}_{\infty}(g)$ ä¸º $g$ çš„æ— é™èŒƒæ•°çš„åˆ©æ™®å¸ŒèŒ¨å¸¸æ•°ã€‚æˆ‘ä»¬å®šä¹‰
    *æ¶æ„åˆ©æ™®å¸ŒèŒ¨å¸¸æ•°* $\mathcal{K}(D,\Theta,\ell,f)$ ä¸º
- en: '|  | $\mathcal{K}(D,\Theta,\ell,f)\doteq\mathcal{K}_{\infty}(\ell(f(\cdot,\cdot),\cdot))$
    |  | (29) |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{K}(D,\Theta,\ell,f)\doteq\mathcal{K}_{\infty}(\ell(f(\cdot,\cdot),\cdot))$
    |  | (29) |'
- en: over the domain $[-1,1]^{n_{0}}\times\Theta\times[-1,1]^{n_{L+1}}$.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸŸ $[-1,1]^{n_{0}}\times\Theta\times[-1,1]^{n_{L+1}}$ ä¸Šã€‚
- en: Using this definition, and the boundedness of parameters, a straightforward
    approximate training algorithm can be devised whose running time is linear in
    $D$. Simply do a grid search in the parametersâ€™ space, and evaluate all data points
    in each possible parameter. It is not hard to see that, to achieve $\epsilon$-optimality,
    such an algorithm would run in time which is linear in $D$ and exponential in
    $\mathcal{K}(D,\Theta,\ell,f)/\epsilon$. What was proved by Bienstock etÂ al. ([2023](#bib.bib32))
    is that one can take a step further and represent multiple training problems at
    the same time.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªå®šä¹‰ï¼Œä»¥åŠå‚æ•°çš„æœ‰ç•Œæ€§ï¼Œå¯ä»¥è®¾è®¡ä¸€ä¸ªç®€å•çš„è¿‘ä¼¼è®­ç»ƒç®—æ³•ï¼Œå…¶è¿è¡Œæ—¶é—´ä¸ $D$ æˆçº¿æ€§å…³ç³»ã€‚åªéœ€åœ¨å‚æ•°ç©ºé—´ä¸­è¿›è¡Œç½‘æ ¼æœç´¢ï¼Œå¹¶è¯„ä¼°æ¯ä¸ªå¯èƒ½å‚æ•°ä¸­çš„æ‰€æœ‰æ•°æ®ç‚¹ã€‚å¯ä»¥çœ‹å‡ºï¼Œä¸ºäº†å®ç°
    $\epsilon$-æœ€ä¼˜æ€§ï¼Œè¿™ç§ç®—æ³•çš„è¿è¡Œæ—¶é—´å°†æ˜¯ä¸ $D$ æˆçº¿æ€§å…³ç³»ï¼Œå¹¶ä¸”ä¸ $\mathcal{K}(D,\Theta,\ell,f)/\epsilon$
    æˆæŒ‡æ•°å…³ç³»ã€‚Bienstock ç­‰äºº ([2023](#bib.bib32)) è¯æ˜äº†å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼ŒåŒæ—¶è¡¨ç¤ºå¤šä¸ªè®­ç»ƒé—®é¢˜ã€‚
- en: Theorem 2 (Bienstock etÂ al. ([2023](#bib.bib32)))
  id: totrans-532
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å®šç† 2ï¼ˆBienstock ç­‰äºº ([2023](#bib.bib32)))
- en: 'Consider the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) with
    parameters $D,\Theta,\ell,f$, and let $\mathcal{K}:=\mathcal{K}(D,\Theta,\ell,f)$
    be the corresponding network architecture. Consider $\epsilon>0$ arbitrary. There
    exists a polytope $P_{\epsilon}$ of sizeâµâµ5Here, the size of the polytope is the
    number of variables and constraints describing it.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘å¸¦æœ‰å‚æ•° $D,\Theta,\ell,f$ çš„ ERM é—®é¢˜ ([11](#S5.E11 "åœ¨ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºä¸­çš„è®­ç»ƒ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ï¼Œè®¾
    $\mathcal{K}:=\mathcal{K}(D,\Theta,\ell,f)$ ä¸ºå¯¹åº”çš„ç½‘ç»œæ¶æ„ã€‚è€ƒè™‘ä»»æ„çš„ $\epsilon>0$ã€‚å­˜åœ¨ä¸€ä¸ªå¤šé¢ä½“
    $P_{\epsilon}$ï¼Œå…¶å¤§å°âµâµ5 è¿™é‡Œï¼Œå¤šé¢ä½“çš„å¤§å°æ˜¯æè¿°å®ƒçš„å˜é‡å’Œçº¦æŸçš„æ•°é‡ã€‚
- en: '|  | $O(D\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$ |  |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '|  | $O(D\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$ |  |'
- en: 'with the following properties:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: å…·æœ‰ä»¥ä¸‹å±æ€§ï¼š
- en: '1.'
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: $P_{\epsilon}$ can be constructed in time $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N}D)$
    plus the time required for $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$
    evaluations of the loss function $\ell$ and $f$.
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $P_{\epsilon}$ å¯ä»¥åœ¨ $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N}D)$
    çš„æ—¶é—´å†…æ„é€ ï¼ŒåŠ ä¸Šå¯¹ $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$ æ¬¡æŸå¤±å‡½æ•°
    $\ell$ å’Œ $f$ è¿›è¡Œè¯„ä¼°æ‰€éœ€çš„æ—¶é—´ã€‚
- en: '2.'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'For *any* sample $(\tilde{X},\tilde{Y})=(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$,
    $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$, there
    is a face $\mathcal{F}_{\tilde{X},\tilde{Y}}$ of $P_{\epsilon}$ such that optimizing
    a linear function over $\mathcal{F}_{\tilde{X},\tilde{Y}}$ yields an $\epsilon$-approximation
    to the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")).'
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äº *ä»»ä½•* æ ·æœ¬ $(\tilde{X},\tilde{Y})=(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$ï¼Œ$(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$ï¼Œå­˜åœ¨ä¸€ä¸ª
    $P_{\epsilon}$ çš„é¢ $\mathcal{F}_{\tilde{X},\tilde{Y}}$ï¼Œä½¿å¾—åœ¨ $\mathcal{F}_{\tilde{X},\tilde{Y}}$
    ä¸Šä¼˜åŒ–çº¿æ€§å‡½æ•°ä¼šå¾—åˆ° ERM é—®é¢˜çš„ $\epsilon$-è¿‘ä¼¼ ([11](#S5.E11 "åœ¨ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºä¸­çš„è®­ç»ƒ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ã€‚
- en: '3.'
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The face $\mathcal{F}_{\tilde{X},\tilde{Y}}$ arises by simply substituting-in
    actual data for the data-variables $x,y$, which is used to fixed variables in
    the description of $P_{\epsilon}$.
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é¢ $\mathcal{F}_{\tilde{X},\tilde{Y}}$ é€šè¿‡ç®€å•åœ°å°†å®é™…æ•°æ®ä»£å…¥æ•°æ®å˜é‡ $x,y$ ä¸­å¾—åˆ°ï¼Œè¿™äº›æ•°æ®ç”¨äºå›ºå®š $P_{\epsilon}$
    æè¿°ä¸­çš„å˜é‡ã€‚
- en: 'This result is very abstract in nature but possesses some interesting features.
    Firstly, it encodes (approximately) *every* possible training problem arising
    from data in $[-1,1]^{{n_{0}}+{n_{L+1}}}$ using a benign dependency on $D$: the
    polytope size depends only linearly on $D$, while a discretized enumeration of
    all the possible samples of size $D$ would be exponential in $D$. Secondly, every
    possible ERM problem appears in a *face* of the polytope; this suggests a strong
    geometric structure across different ERM problems. And lastly, this result is
    applicable to a wide variety of network architectures; in order to obtain an architecture-specific
    result, it suffices to compute the corresponding value of $\mathcal{K}$ and plug
    it in. Regarding this last point, the authors computed the constant $\mathcal{K}$
    for various well-known architectures and obtained the results of Table [4](#S5.T4
    "Table 4 â€£ 5.4 Polyhedral encoding of multiple training problems â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey").'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»“æœåœ¨æœ¬è´¨ä¸Šéå¸¸æŠ½è±¡ï¼Œä½†å…·æœ‰ä¸€äº›æœ‰è¶£çš„ç‰¹æ€§ã€‚é¦–å…ˆï¼Œå®ƒç¼–ç ï¼ˆè¿‘ä¼¼åœ°ï¼‰*æ¯ä¸€ä¸ª* ä» $[-1,1]^{{n_{0}}+{n_{L+1}}}$ ä¸­äº§ç”Ÿçš„å¯èƒ½è®­ç»ƒé—®é¢˜ï¼Œå¯¹
    $D$ çš„ä¾èµ–æ˜¯è‰¯æ€§çš„ï¼šå¤šé¢ä½“çš„å¤§å°ä»…ä¸ $D$ çº¿æ€§ç›¸å…³ï¼Œè€Œå¯¹æ‰€æœ‰å¯èƒ½æ ·æœ¬çš„ç¦»æ•£æšä¸¾çš„å¤§å°æ˜¯æŒ‡æ•°çº§çš„ã€‚å…¶æ¬¡ï¼Œæ‰€æœ‰å¯èƒ½çš„ ERM é—®é¢˜éƒ½å‡ºç°åœ¨å¤šé¢ä½“çš„ä¸€ä¸ª
    *é¢* ä¸­ï¼›è¿™è¡¨æ˜ä¸åŒ ERM é—®é¢˜ä¹‹é—´å­˜åœ¨å¼ºå¤§çš„å‡ ä½•ç»“æ„ã€‚æœ€åï¼Œè¿™ä¸ªç»“æœé€‚ç”¨äºå„ç§ç½‘ç»œæ¶æ„ï¼›ä¸ºäº†è·å¾—ç‰¹å®šæ¶æ„çš„ç»“æœï¼Œåªéœ€è®¡ç®—å¯¹åº”çš„ $\mathcal{K}$
    å€¼å¹¶å°†å…¶ä»£å…¥å³å¯ã€‚å…³äºæœ€åä¸€ç‚¹ï¼Œä½œè€…è®¡ç®—äº†å„ç§è‘—åæ¶æ„çš„å¸¸æ•° $\mathcal{K}$ å¹¶å¾—åˆ°äº†è¡¨ [4](#S5.T4 "è¡¨ 4 â€£ 5.4 å¤šé¢ä½“ç¼–ç çš„å¤šä¸ªè®­ç»ƒé—®é¢˜
    â€£ 5 çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºä¸­çš„è®­ç»ƒ â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°") çš„ç»“æœã€‚
- en: 'Table 4: Summary of polyhedral encoding sizes for various architectures. DNN
    refers to a fully-connected Deep Neural Network, CNN to a Convolutional Neural
    Network, and ResNet to a Residual Network. $G$ is the graph defining the Network,
    $\Delta$ is the maximum in-degree in $G$, $L$ is the number of hidden layers,
    and ${n_{\max}}$ is the maximum width of a layer.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 4ï¼šå„ç§æ¶æ„çš„å¤šé¢ä½“ç¼–ç å¤§å°æ±‡æ€»ã€‚DNN æŒ‡å®Œå…¨è¿æ¥çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼ŒCNN æŒ‡å·ç§¯ç¥ç»ç½‘ç»œï¼ŒResNet æŒ‡æ®‹å·®ç½‘ç»œã€‚$G$ æ˜¯å®šä¹‰ç½‘ç»œçš„å›¾ï¼Œ$\Delta$
    æ˜¯ $G$ ä¸­çš„æœ€å¤§å…¥åº¦ï¼Œ$L$ æ˜¯éšè—å±‚çš„æ•°é‡ï¼Œ${n_{\max}}$ æ˜¯ä¸€å±‚çš„æœ€å¤§å®½åº¦ã€‚
- en: Type Loss Size of polytope Notes DNN Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ DNN Cross Entropy w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}}){n_{\max}}^{O(k^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ CNN Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\ll|E({G})|$ ResNet Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    ResNet Cross Entropy w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}})\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å‹ æŸå¤± å¤§å° å¤‡æ³¨ DNN ç»å¯¹/äºŒæ¬¡/é“°é“¾ $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ DNN äº¤å‰ç†µ w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}}){n_{\max}}^{O(k^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ CNN ç»å¯¹/äºŒæ¬¡/é“°é“¾ $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\ll|E({G})|$ ResNet ç»å¯¹/äºŒæ¬¡/é“°é“¾ $O\big{(}\big{(}{n_{L+1}}\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    ResNet äº¤å‰ç†µ w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}})\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
- en: 'The proof of this result relies on a graph theoretical concept called *treewidth*.
    This parameter is used for measuring structured sparsity, and in Bienstock and
    MuÃ±oz ([2018](#bib.bib31)) it was proved that any optimization problem admits
    an approximate polyhedral reformulation whose size is exponential only in the
    treewidth parameter. On a high level, the neural network result is obtained by
    noting that ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) connects different sample
    points only through a sum; therefore, the following reformulation of the optimization
    problem can be considered, which decouples the different data points:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»“æœçš„è¯æ˜ä¾èµ–äºä¸€ä¸ªç§°ä¸º*æ ‘å®½*çš„å›¾è®ºæ¦‚å¿µã€‚è¿™ä¸ªå‚æ•°ç”¨äºè¡¡é‡ç»“æ„åŒ–ç¨€ç–æ€§ï¼Œåœ¨Bienstockå’ŒMuÃ±ozï¼ˆ[2018](#bib.bib31)ï¼‰ä¸­è¯æ˜äº†ä»»ä½•ä¼˜åŒ–é—®é¢˜éƒ½æ‰¿è®¤ä¸€ä¸ªè¿‘ä¼¼çš„å¤šé¢ä½“é‡æ–°è¡¨è¿°ï¼Œå…¶å¤§å°ä»…åœ¨æ ‘å®½å‚æ•°ä¸Šæ˜¯æŒ‡æ•°çº§çš„ã€‚é«˜å±‚æ¬¡ä¸Šï¼Œç¥ç»ç½‘ç»œç»“æœæ˜¯é€šè¿‡æ³¨æ„åˆ°ï¼ˆ[11](#S5.E11
    "åœ¨5çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºä¸­çš„è®­ç»ƒ â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹ç»¼è¿°")ï¼‰ä»…é€šè¿‡ä¸€ä¸ªæ€»å’Œè¿æ¥ä¸åŒçš„æ ·æœ¬ç‚¹ï¼Œå› æ­¤å¯ä»¥è€ƒè™‘ä¼˜åŒ–é—®é¢˜çš„ä»¥ä¸‹é‡æ–°è¡¨è¿°ï¼Œè¿™æ ·å¯ä»¥è§£è€¦ä¸åŒçš„æ•°æ®ç‚¹ï¼š
- en: '|  | $\displaystyle\min_{{\bm{W}}\in\Theta,{\bm{L}}}\left\{\frac{1}{D}\sum_{d=1}^{D}{\bm{L}}_{d}\,\middle&#124;\,{\bm{L}}_{d}\,=\,\ell(f(\tilde{{\bm{x}}}_{d},{\bm{W}}),\tilde{{\bm{y}}}_{d})\quad\forall\,d\in[D]\right\}$
    |  | (30) |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{{\bm{W}}\in\Theta,{\bm{L}}}\left\{\frac{1}{D}\sum_{d=1}^{D}{\bm{L}}_{d}\,\middle&#124;\,{\bm{L}}_{d}\,=\,\ell(f(\tilde{{\bm{x}}}_{d},{\bm{W}}),\tilde{{\bm{y}}}_{d})\quad\forall\,d\in[D]\right\}$
    |  | (30) |'
- en: This reformulation does not seem useful at first, however, it has a *treewidth*
    that does not depend on $D$, even if the data points are considered variables.
    From this point, the authors are able to obtain the polytope whose size does not
    depend exponentially on $D$, and which is capable of encoding all possible ERM
    problems. The face structure the polytope has is more involved, and we refer the
    reader to Bienstock etÂ al. ([2023](#bib.bib32)) for these details.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§é‡æ–°è¡¨è¿°ä¹çœ‹ä¹‹ä¸‹ä¼¼ä¹ä¸å¤ªæœ‰ç”¨ï¼Œä½†å®ƒå…·æœ‰ä¸€ä¸ªä¸ä¾èµ–äº$D$çš„*æ ‘å®½*ï¼Œå³ä½¿æ•°æ®ç‚¹è¢«è§†ä¸ºå˜é‡ã€‚ä»è¿™ä¸€ç‚¹å¼€å§‹ï¼Œä½œè€…ä»¬èƒ½å¤Ÿè·å¾—ä¸€ä¸ªå¤šé¢ä½“ï¼Œå…¶å¤§å°ä¸ä¾èµ–äº$D$çš„æŒ‡æ•°ï¼Œå¹¶ä¸”èƒ½å¤Ÿç¼–ç æ‰€æœ‰å¯èƒ½çš„ERMé—®é¢˜ã€‚è¿™ä¸ªå¤šé¢ä½“çš„é¢ç»“æ„æ›´ä¸ºå¤æ‚ï¼Œæˆ‘ä»¬å°†è¿™äº›ç»†èŠ‚å‚è§Bienstockç­‰äººï¼ˆ[2023](#bib.bib32)ï¼‰ã€‚
- en: It is worth mentioning that the polytope size provided by Bienstock etÂ al. ([2023](#bib.bib32))
    in the setting of Arora etÂ al. ([2018](#bib.bib8)) is
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒBienstockç­‰äººï¼ˆ[2023](#bib.bib32)ï¼‰åœ¨Aroraç­‰äººï¼ˆ[2018](#bib.bib8)ï¼‰çš„è®¾å®šä¸‹æä¾›çš„å¤šé¢ä½“å¤§å°æ˜¯
- en: '|  | $O((2\mathcal{K}_{\infty}(\ell)n_{1}^{O(1)}/\epsilon)^{({n_{0}}+1)(n_{1})}D)$
    |  | (31) |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|  | $O((2\mathcal{K}_{\infty}(\ell)n_{1}^{O(1)}/\epsilon)^{({n_{0}}+1)(n_{1})}D)$
    |  | (31) |'
- en: where $\mathcal{K_{\infty}}(\ell)$ is the Lipschitz constant of the loss function
    with respect to the infinity norm over a specific domain. These two results are
    not completely comparable, but they give a good idea of how good the size of polytope
    constructed in Bienstock etÂ al. ([2023](#bib.bib32)) is. The dependency on $D$
    is better in the polytope size, the polytope encodes multiple training problems,
    and the result is more general (it applies to almost any architecture); however,
    the polytope only gives an approximation, and its construction requires boundedness.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\mathcal{K_{\infty}}(\ell)$æ˜¯å…³äºç‰¹å®šé¢†åŸŸçš„æ— ç©·èŒƒæ•°çš„æŸå¤±å‡½æ•°çš„åˆ©æ™®å¸ŒèŒ¨å¸¸æ•°ã€‚è¿™ä¸¤ä¸ªç»“æœå¹¶ä¸å®Œå…¨å¯æ¯”ï¼Œä½†å®ƒä»¬å¾ˆå¥½åœ°è¯´æ˜äº†Bienstockç­‰äººï¼ˆ[2023](#bib.bib32)ï¼‰æ„å»ºçš„å¤šé¢ä½“çš„å¤§å°çš„ä¼˜è¶Šæ€§ã€‚å¤šé¢ä½“çš„å¤§å°å¯¹$D$çš„ä¾èµ–æ€§æ›´å¥½ï¼Œè¯¥å¤šé¢ä½“ç¼–ç äº†å¤šä¸ªè®­ç»ƒé—®é¢˜ï¼Œå¹¶ä¸”ç»“æœæ›´å…·ä¸€èˆ¬æ€§ï¼ˆå‡ ä¹é€‚ç”¨äºä»»ä½•æ¶æ„ï¼‰ï¼›ç„¶è€Œï¼Œè¿™ä¸ªå¤šé¢ä½“ä»…æä¾›äº†ä¸€ä¸ªè¿‘ä¼¼å€¼ï¼Œå…¶æ„é€ éœ€è¦æœ‰ç•Œæ€§ã€‚
- en: 5.5 Backpropagation through MILP
  id: totrans-551
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 é€šè¿‡ MILP è¿›è¡Œåå‘ä¼ æ’­
- en: 'In the work by Goebbels ([2021](#bib.bib120)), a novel use of Mixed-Integer
    Linear Programming is proposed in training ReLU networks: to serve as an alternative
    to SGD. This new algorithm works as backpropagation, as it updates the weights
    of the neural network iteratively starting from the last layer. The key difference
    is that each update in a layer amounts to solving a MILP.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Goebbels çš„å·¥ä½œ ([2021](#bib.bib120)) ä¸­ï¼Œæå‡ºäº†ä¸€ç§æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’åœ¨è®­ç»ƒ ReLU ç½‘ç»œä¸­çš„æ–°ç”¨é€”ï¼šä½œä¸º SGD
    çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¿™ä¸ªæ–°ç®—æ³•åƒåå‘ä¼ æ’­ä¸€æ ·å·¥ä½œï¼Œå› ä¸ºå®ƒä»æœ€åä¸€å±‚å¼€å§‹è¿­ä»£åœ°æ›´æ–°ç¥ç»ç½‘ç»œçš„æƒé‡ã€‚å…³é”®çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œæ¯æ¬¡åœ¨ä¸€å±‚ä¸­çš„æ›´æ–°ç›¸å½“äºè§£å†³ä¸€ä¸ª MILP é—®é¢˜ã€‚
- en: 'Let us focus only on one hidden layer at a time (of width $n$), so we can assume
    we have an architecture as in Figure [11](#S5.F11 "Figure 11 â€£ 5.1.2 LTU activations
    and variable number of nodes â€£ 5.1 Training neural networks with a single hidden
    layer â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey"). Furthermore, we assume we have some target
    output vectors $\{{\bm{T}}_{d}\}_{d=1}^{D}$ (when processing the last hidden layer
    in the backpropagation, this corresponds to $\{\tilde{{\bm{y}}}_{d}\}_{d=1}^{D}$)
    and some layer input $\{{\bm{I}}_{d}\}_{d=1}^{D}$ (when processing the last hidden
    layer, this corresponds to evaluating the neural network on $\{\tilde{{\bm{x}}}_{d}\}_{d=1}^{D}$
    up to the second-to-last hidden layer). The algorithm proposed by Goebbels ([2021](#bib.bib120))
    solves the following optimization problem to update the weights ${\bm{W}}$ and
    biases ${\bm{b}}$ of the given layer:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸€æ¬¡åªå…³æ³¨ä¸€ä¸ªéšè—å±‚ï¼ˆå®½åº¦ä¸º $n$ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å‡è®¾æœ‰ä¸€ä¸ªå¦‚å›¾ [11](#S5.F11 "å›¾ 11 â€£ 5.1.2 LTU æ¿€æ´»å‡½æ•°ä¸èŠ‚ç‚¹æ•°çš„å¯å˜æ€§
    â€£ 5.1 è®­ç»ƒå…·æœ‰å•ä¸€éšè—å±‚çš„ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’ä¸è®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡ä¸Šå¤šé¢ä½“ç†è®ºï¼šç»¼è¿°") æ‰€ç¤ºçš„æ¶æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‡è®¾æœ‰ä¸€äº›ç›®æ ‡è¾“å‡ºå‘é‡
    $\{{\bm{T}}_{d}\}_{d=1}^{D}$ï¼ˆå½“å¤„ç†åå‘ä¼ æ’­ä¸­çš„æœ€åä¸€ä¸ªéšè—å±‚æ—¶ï¼Œè¿™å¯¹åº”äº $\{\tilde{{\bm{y}}}_{d}\}_{d=1}^{D}$ï¼‰ä»¥åŠä¸€äº›å±‚è¾“å…¥
    $\{{\bm{I}}_{d}\}_{d=1}^{D}$ï¼ˆå½“å¤„ç†æœ€åä¸€ä¸ªéšè—å±‚æ—¶ï¼Œè¿™å¯¹åº”äºåœ¨ $\{\tilde{{\bm{x}}}_{d}\}_{d=1}^{D}$
    ä¸Šè¯„ä¼°ç¥ç»ç½‘ç»œï¼Œç›´åˆ°å€’æ•°ç¬¬äºŒä¸ªéšè—å±‚ï¼‰ã€‚Goebbels æå‡ºçš„ç®—æ³• ([2021](#bib.bib120)) è§£å†³ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜ä»¥æ›´æ–°ç»™å®šå±‚çš„æƒé‡ ${\bm{W}}$
    å’Œåç½® ${\bm{b}}$ï¼š
- en: '|  |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\min_{{\bm{W}},\hat{{\bm{h}}},{\bm{b}},{\bm{h}},{\bm{z}}}\quad$
    | $\displaystyle\sum_{d=1}^{D}\sum_{j=1}^{n}&#124;{\bm{T}}_{d,j}-{\bm{h}}_{d,j}&#124;$
    |  | (32a) |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{{\bm{W}},\hat{{\bm{h}}},{\bm{b}},{\bm{h}},{\bm{z}}}\quad$
    | $\displaystyle\sum_{d=1}^{D}\sum_{j=1}^{n}\left|{\bm{T}}_{d,j}-{\bm{h}}_{d,j}\right|$
    |  | (32a) |'
- en: '|  | s.t. | $\displaystyle\hat{{\bm{h}}}_{d,j}=({\bm{W}}{\bm{I}}_{d})_{j}+{\bm{b}}_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32b) |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|  | æ»¡è¶³ | $\displaystyle\hat{{\bm{h}}}_{d,j}=({\bm{W}}{\bm{I}}_{d})_{j}+{\bm{b}}_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32b) |'
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32c) |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32c) |'
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\geq-M(1-{\bm{z}}_{d,j})$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32d) |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\geq-M(1-{\bm{z}}_{d,j})$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32d) |'
- en: '|  |  | $\displaystyle&#124;\hat{{\bm{h}}}_{d,j}-{\bm{h}}_{d,j}&#124;\leq M(1-{\bm{z}}_{d,j})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32e) |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\left|\hat{{\bm{h}}}_{d,j}-{\bm{h}}_{d,j}\right|\leq
    M(1-{\bm{z}}_{d,j})$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32e)
    |'
- en: '|  |  | $\displaystyle{\bm{h}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32f) |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{h}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32f) |'
- en: '|  |  | $\displaystyle{\bm{h}}_{d,j}\geq 0$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$
    |  | (32g) |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{h}}_{d,j}\geq 0$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$
    |  | (32g) |'
- en: '|  |  | $\displaystyle{\bm{z}}_{d,j}\in\{0,1\}$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$
    |  | (32h) |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{z}}_{d,j}\in\{0,1\}$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$
    |  | (32h) |'
- en: 'Here $M$ is a large constant that is assumed to bound the input to any neuron.
    Note that problem ([32](#S5.E32 "In 5.5 Backpropagation through MILP â€£ 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) can easily be linearized. This optimization problem finds
    the weights (${\bm{W}}$) and biases (${\bm{b}}$) that minimize the difference
    between the â€œrealâ€ output of the network for each sample (${\bm{h}}_{d}$) and
    the target output (${\bm{T}}_{d}$). The auxiliary variables $\hat{{\bm{h}}}_{d,j}$
    represent the input to the each neuron â€”so ${\bm{h}}_{d,j}=\sigma(\hat{{\bm{h}}}_{d,j})$â€”
    and ${\bm{z}}_{d,j}$ indicates if the $j$-th neuron is activated on input ${\bm{I}}_{d}$.'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œ $M$ æ˜¯ä¸€ä¸ªå¤§çš„å¸¸æ•°ï¼Œå‡è®¾å®ƒé™åˆ¶äº†ä»»ä½•ç¥ç»å…ƒçš„è¾“å…¥ã€‚è¯·æ³¨æ„ï¼Œé—®é¢˜ ([32](#S5.E32 "åœ¨ 5.5 èŠ‚ MILP åå‘ä¼ æ’­ â€£ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨
    â€£ æ·±åº¦å­¦ä¹ é‡è§å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) å¯ä»¥å¾ˆå®¹æ˜“åœ°çº¿æ€§åŒ–ã€‚è¿™ä¸ªä¼˜åŒ–é—®é¢˜å¯»æ‰¾èƒ½å¤Ÿæœ€å°åŒ–ç½‘ç»œå¯¹æ¯ä¸ªæ ·æœ¬çš„â€œçœŸå®â€è¾“å‡º (${\bm{h}}_{d}$) å’Œç›®æ ‡è¾“å‡º
    (${\bm{T}}_{d}$) ä¹‹é—´å·®å¼‚çš„æƒé‡ (${\bm{W}}$) å’Œåç½® (${\bm{b}}$)ã€‚è¾…åŠ©å˜é‡ $\hat{{\bm{h}}}_{d,j}$
    ä»£è¡¨æ¯ä¸ªç¥ç»å…ƒçš„è¾“å…¥â€”â€”å› æ­¤ ${\bm{h}}_{d,j}=\sigma(\hat{{\bm{h}}}_{d,j})$â€”â€”è€Œ ${\bm{z}}_{d,j}$
    è¡¨ç¤ºç¬¬ $j$ ä¸ªç¥ç»å…ƒæ˜¯å¦åœ¨è¾“å…¥ ${\bm{I}}_{d}$ ä¸Šè¢«æ¿€æ´»ã€‚
- en: 'When processing intermediate layers, the definition ${\bm{I}}_{d}$ can easily
    be adapted from what we mentioned above. However, the story is different for the
    case of ${\bm{T}}_{d}$. When processing the last layer, as previously mentioned,
    ${\bm{T}}_{d}$ simply corresponds to $\tilde{{\bm{y}}}_{d}$. For intermediate
    layers, to define ${\bm{T}}_{d}$, the author proposes to use a similar optimization
    problem to ([32](#S5.E32 "In 5.5 Backpropagation through MILP â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")), but leaving ${\bm{W}}$ and ${\bm{b}}$ fixed and having ${\bm{I}}_{d}$
    as variables; this defines â€œoptimal inputsâ€ of a layer. These optimal inputs are
    then used as target outputs ${\bm{T}}_{d}$ when processing the preceding layer,
    and thus the algorithm is iterated. For details, see Goebbels ([2021](#bib.bib120)).'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤„ç†ä¸­é—´å±‚æ—¶ï¼Œ${\bm{I}}_{d}$ çš„å®šä¹‰å¯ä»¥å¾ˆå®¹æ˜“åœ°ä»æˆ‘ä»¬ä¸Šè¿°æåˆ°çš„å†…å®¹è¿›è¡Œé€‚åº”ã€‚ç„¶è€Œï¼Œå¯¹äº ${\bm{T}}_{d}$ çš„æƒ…å†µï¼Œæƒ…å†µæœ‰æ‰€ä¸åŒã€‚åœ¨å¤„ç†æœ€åä¸€å±‚æ—¶ï¼Œå¦‚å‰æ‰€è¿°ï¼Œ${\bm{T}}_{d}$
    ç®€å•åœ°å¯¹åº”äº $\tilde{{\bm{y}}}_{d}$ã€‚å¯¹äºä¸­é—´å±‚ï¼Œä¸ºäº†å®šä¹‰ ${\bm{T}}_{d}$ï¼Œä½œè€…å»ºè®®ä½¿ç”¨ç±»ä¼¼äº ([32](#S5.E32
    "åœ¨ 5.5 èŠ‚ MILP åå‘ä¼ æ’­ â€£ 5 çº¿æ€§è§„åˆ’ä¸å¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­çš„åº”ç”¨ â€£ æ·±åº¦å­¦ä¹ é‡è§å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) çš„ä¼˜åŒ–é—®é¢˜ï¼Œä½†å›ºå®š ${\bm{W}}$
    å’Œ ${\bm{b}}$ï¼Œå¹¶å°† ${\bm{I}}_{d}$ ä½œä¸ºå˜é‡ï¼›è¿™å®šä¹‰äº†ä¸€ä¸ªå±‚çš„â€œæœ€ä¼˜è¾“å…¥â€ã€‚è¿™äº›æœ€ä¼˜è¾“å…¥éšåè¢«ç”¨ä½œå¤„ç†å‰ä¸€å±‚æ—¶çš„ç›®æ ‡è¾“å‡º ${\bm{T}}_{d}$ï¼Œå› æ­¤ç®—æ³•è¢«è¿­ä»£ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§
    Goebbels ([2021](#bib.bib120))ã€‚
- en: The computational results in that paper show that a similar level of accuracy
    to that of gradient descent can be achieved. However, the use of potentially expensive
    MILPs impairs the applicability of this approach to large networks. Nonetheless,
    it shows an interesting new avenue for training whose running times may be improved
    in future implementations.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è®ºæ–‡ä¸­çš„è®¡ç®—ç»“æœè¡¨æ˜ï¼Œå¯ä»¥å®ç°ä¸æ¢¯åº¦ä¸‹é™ç±»ä¼¼çš„å‡†ç¡®åº¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¯èƒ½æ˜‚è´µçš„ MILP ä¼šå½±å“è¯¥æ–¹æ³•åœ¨å¤§ç½‘ç»œä¸­çš„åº”ç”¨ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®ƒå±•ç¤ºäº†ä¸€ç§æœ‰è¶£çš„æ–°è®­ç»ƒé€”å¾„ï¼Œå…¶è¿è¡Œæ—¶é—´åœ¨æœªæ¥çš„å®ç°ä¸­å¯èƒ½ä¼šå¾—åˆ°æ”¹è¿›ã€‚
- en: 5.6 Training binarized neural networks using MILP
  id: totrans-566
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 ä½¿ç”¨ MILP è®­ç»ƒäºŒå€¼åŒ–ç¥ç»ç½‘ç»œ
- en: As mentioned before, the training problem of a DNN is an unrestricted non-convex
    optimization problem, which is typically continuous as the weights and biases
    frequently are allowed to have any real value. Nonetheless, if the weights and
    biases are required to be integer-valued, the training problem becomes a discrete
    optimization problem, for which gradient-descent-based methods may find some difficulties
    in their applicability.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼ŒDNN çš„è®­ç»ƒé—®é¢˜æ˜¯ä¸€ä¸ªä¸å—é™åˆ¶çš„éå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œè¿™é€šå¸¸æ˜¯è¿ç»­çš„ï¼Œå› ä¸ºæƒé‡å’Œåç½®é€šå¸¸å¯ä»¥å…·æœ‰ä»»ä½•å®å€¼ã€‚ç„¶è€Œï¼Œå¦‚æœæƒé‡å’Œåç½®è¦æ±‚ä¸ºæ•´æ•°å€¼ï¼Œåˆ™è®­ç»ƒé—®é¢˜å˜æˆäº†ä¸€ä¸ªç¦»æ•£ä¼˜åŒ–é—®é¢˜ï¼Œè¿™ä½¿å¾—åŸºäºæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•åœ¨åº”ç”¨ä¸Šå¯èƒ½ä¼šé‡åˆ°å›°éš¾ã€‚
- en: 'In this context, Icarte etÂ al. ([2019](#bib.bib161)) proposed a MILP formulation
    for the training problem of binarized neural networks (BNNs): these are neural
    networks where the weights and biases are restricted to be in $\{-1,0,1\}$ and
    where the activations are LTU (i.e. sign functions). Later on, Thorbjarnarson
    and Yorke-Smith ([2020](#bib.bib305), [2023](#bib.bib306)) used a similar technique
    to allow more general integer-valued weights. We review the core feature in these
    formulations that yield a *linear* formulation of the training problem.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§èƒŒæ™¯ä¸‹ï¼ŒIcarte ç­‰äºº ([2019](#bib.bib161)) æå‡ºäº†ä¸€ä¸ªç”¨äºäºŒå€¼åŒ–ç¥ç»ç½‘ç»œï¼ˆBNNsï¼‰è®­ç»ƒé—®é¢˜çš„ MILP å…¬å¼ï¼šè¿™äº›ç¥ç»ç½‘ç»œçš„æƒé‡å’Œåç½®è¢«é™åˆ¶ä¸º
    $\{-1,0,1\}$ï¼Œè€Œæ¿€æ´»å‡½æ•°æ˜¯ LTUï¼ˆå³ç¬¦å·å‡½æ•°ï¼‰ã€‚åæ¥ï¼ŒThorbjarnarson å’Œ Yorke-Smith ([2020](#bib.bib305),
    [2023](#bib.bib306)) ä½¿ç”¨äº†ç±»ä¼¼çš„æŠ€æœ¯æ¥å…è®¸æ›´ä¸€èˆ¬çš„æ•´æ•°å€¼æƒé‡ã€‚æˆ‘ä»¬å›é¡¾äº†è¿™äº›å…¬å¼ä¸­çš„æ ¸å¿ƒç‰¹å¾ï¼Œå®ƒä»¬äº§ç”Ÿäº†ä¸€ä¸ª *çº¿æ€§* çš„è®­ç»ƒé—®é¢˜å…¬å¼ã€‚
- en: 'Let us focus on an intermediate layer $i$ with width $n$, and let us omit biases
    to simplify the discussion. Using a DNNâ€™s layer-wise architecture, one usually
    aims at describing:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å…³æ³¨ä¸€ä¸ªå®½åº¦ä¸º $n$ çš„ä¸­é—´å±‚ $i$ï¼Œå¹¶ä¸ºç®€åŒ–è®¨è®ºè€Œçœç•¥åç½®ã€‚åˆ©ç”¨ DNN çš„å±‚æ¬¡ç»“æ„ï¼Œé€šå¸¸æ—¨åœ¨æè¿°ï¼š
- en: '|  |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle=({\bm{W}}^{i}{\bm{h}}^{i-1}_{d})_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (33a) |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle=({\bm{W}}^{i}{\bm{h}}^{i-1}_{d})_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (33a) |'
- en: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=\sigma(\hat{{\bm{h}}}^{i}_{d,j})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$ |  | (33b) |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=\sigma(\hat{{\bm{h}}}^{i}_{d,j})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$ |  | (33b) |'
- en: 'We remind the reader that $D$ is the cardinality of the training set. Additionally,
    for each data point indexed by $d$ and each layer $i$, each variable ${\bm{h}}_{d}^{i}$
    is the output vector of all the neurons of the layer and each variable $\hat{{\bm{h}}}_{d,j}^{i}$
    is the input of neuron $j$. Besides the difficulty posed by the activation function,
    one important issue with system ([33](#S5.E33 "In 5.6 Training binarized neural
    networks using MILP â€£ 5 Linear Programming and Polyhedral Theory in Training â€£
    When Deep Learning Meets Polyhedral Theory: A Survey")) is the non-linearity of
    the products between the ${\bm{W}}$ and ${\bm{h}}$ variables. Nonetheless, this
    issue disappears when each entry of ${\bm{W}}$ and ${\bm{h}}$ is bounded and integer,
    as in the case of BNNs.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æé†’è¯»è€…ï¼Œ$D$ æ˜¯è®­ç»ƒé›†çš„åŸºæ•°ã€‚æ­¤å¤–ï¼Œå¯¹äºæ¯ä¸ªç”± $d$ ç´¢å¼•çš„æ•°æ®ç‚¹å’Œæ¯ä¸€å±‚ $i$ï¼Œæ¯ä¸ªå˜é‡ ${\bm{h}}_{d}^{i}$ æ˜¯è¯¥å±‚æ‰€æœ‰ç¥ç»å…ƒçš„è¾“å‡ºå‘é‡ï¼Œè€Œæ¯ä¸ªå˜é‡
    $\hat{{\bm{h}}}_{d,j}^{i}$ æ˜¯ç¥ç»å…ƒ $j$ çš„è¾“å…¥ã€‚é™¤äº†æ¿€æ´»å‡½æ•°å¸¦æ¥çš„å›°éš¾å¤–ï¼Œç³»ç»Ÿ ([33](#S5.E33 "åœ¨5.6 ä½¿ç”¨MILPè®­ç»ƒäºŒå€¼åŒ–ç¥ç»ç½‘ç»œ
    â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°")) çš„ä¸€ä¸ªé‡è¦é—®é¢˜æ˜¯ ${\bm{W}}$ å’Œ ${\bm{h}}$ å˜é‡ä¹‹é—´çš„ä¹˜ç§¯çš„éçº¿æ€§ã€‚ç„¶è€Œï¼Œå½“
    ${\bm{W}}$ å’Œ ${\bm{h}}$ çš„æ¯ä¸ªæ¡ç›®è¢«é™åˆ¶ä¸ºæ•´æ•°æ—¶ï¼Œä¾‹å¦‚åœ¨BNNsçš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªé—®é¢˜å°±æ¶ˆå¤±äº†ã€‚
- en: 'Let us begin with reformulating ([33b](#S5.E33.2 "In 33 â€£ 5.6 Training binarized
    neural networks using MILP â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). We can introduce auxiliary
    variables ${\bm{u}}_{d,j}^{i}\in\{0,1\}$ that will indicate if the neuron is active.
    We also introduce a tolerance $\varepsilon>0$ to determine the activity of a neuron.
    Using this, we can (approximately) reformulate ([33b](#S5.E33.2 "In 33 â€£ 5.6 Training
    binarized neural networks using MILP â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) *linearly*
    using big-M constraints:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹é‡æ–°è¡¨è¿° ([33b](#S5.E33.2 "åœ¨33 â€£ 5.6 ä½¿ç”¨MILPè®­ç»ƒäºŒå€¼åŒ–ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ã€‚æˆ‘ä»¬å¯ä»¥å¼•å…¥è¾…åŠ©å˜é‡
    ${\bm{u}}_{d,j}^{i}\in\{0,1\}$ æ¥æŒ‡ç¤ºç¥ç»å…ƒæ˜¯å¦å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå®¹å¿åº¦ $\varepsilon>0$ æ¥ç¡®å®šç¥ç»å…ƒçš„æ´»åŠ¨çŠ¶æ€ã€‚åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ï¼ˆè¿‘ä¼¼åœ°ï¼‰ä½¿ç”¨å¤§-Mçº¦æŸ
    *çº¿æ€§åœ°* é‡æ–°è¡¨è¿° ([33b](#S5.E33.2 "åœ¨33 â€£ 5.6 ä½¿ç”¨MILPè®­ç»ƒäºŒå€¼åŒ–ç¥ç»ç½‘ç»œ â€£ 5 çº¿æ€§è§„åˆ’å’Œè®­ç»ƒä¸­çš„å¤šé¢ä½“ç†è®º â€£ å½“æ·±åº¦å­¦ä¹ é‡åˆ°å¤šé¢ä½“ç†è®ºï¼šç»¼è¿°"))ï¼š
- en: '|  |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=2{\bm{u}}_{d,j}^{i}-1$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34a) |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=2{\bm{u}}_{d,j}^{i}-1$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34a) |'
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\geq-M(1-{\bm{u}}_{d,j}^{i})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34b) |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\geq-M(1-{\bm{u}}_{d,j}^{i})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34b) |'
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\leq-\varepsilon+M{\bm{u}}_{d,j}^{i}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34c) |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\leq-\varepsilon+M{\bm{u}}_{d,j}^{i}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34c) |'
- en: 'where $M$ is a large constant. As for ([33a](#S5.E33.1 "In 33 â€£ 5.6 Training
    binarized neural networks using MILP â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")), note that'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $M$ æ˜¯ä¸€ä¸ªè¾ƒå¤§çš„å¸¸æ•°ã€‚è‡³äº ([33a](#S5.E33.1 "åœ¨33 â€£ 5.6ä½¿ç”¨MILPè®­ç»ƒäºŒå€¼ç¥ç»ç½‘ç»œ â€£ 5çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­
    â€£ å½“æ·±åº¦å­¦ä¹ é‡è§å¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥"))ï¼Œæ³¨æ„åˆ°
- en: '|  | $\hat{{\bm{h}}}^{i}_{d,j}=\sum_{k=1}{\bm{W}}^{i}_{j,k}{\bm{h}}^{i-1}_{d,k}.$
    |  |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{{\bm{h}}}^{i}_{d,j}=\sum_{k=1}{\bm{W}}^{i}_{j,k}{\bm{h}}^{i-1}_{d,k}.$
    |  |'
- en: 'Therefore, using ([34a](#S5.E34.1 "In 34 â€£ 5.6 Training binarized neural networks
    using MILP â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")), we see that it suffices to describe
    each product ${\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ linearly. We can introduce
    new variables ${\bm{z}}_{j,k,d}^{i}$ and note that'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½¿ç”¨ ([34a](#S5.E34.1 "åœ¨34 â€£ 5.6ä½¿ç”¨MILPè®­ç»ƒäºŒå€¼ç¥ç»ç½‘ç»œ â€£ 5çº¿æ€§è§„åˆ’å’Œå¤šé¢ä½“ç†è®ºåœ¨è®­ç»ƒä¸­ â€£ å½“æ·±åº¦å­¦ä¹ é‡è§å¤šé¢ä½“ç†è®ºï¼šä¸€é¡¹è°ƒæŸ¥"))ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºåªéœ€è¦çº¿æ€§æè¿°æ¯ä¸ªä¹˜ç§¯
    ${\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ã€‚æˆ‘ä»¬å¯ä»¥å¼•å…¥æ–°å˜é‡ ${\bm{z}}_{j,k,d}^{i}$ å¹¶æ³¨æ„åˆ°
- en: '|  | ${\bm{z}}_{j,k,d}^{i-1}={\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ |  |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{z}}_{j,k,d}^{i-1}={\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ |  |'
- en: if and only if the three variables satisfy
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä¸”ä»…å½“ä¸‰ä¸ªå˜é‡æ»¡è¶³æ—¶
- en: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}&#124;$ | $\displaystyle\leq{\bm{u}}_{d,k}^{i-1}$
    |  |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}&#124;$ | $\displaystyle\leq{\bm{u}}_{d,k}^{i-1}$
    |  |'
- en: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}-{\bm{W}}_{j,k}^{i}&#124;$ |
    $\displaystyle\leq 1-{\bm{u}}_{d,k}^{i-1}$ |  |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}-{\bm{W}}_{j,k}^{i}&#124;$ |
    $\displaystyle\leq 1-{\bm{u}}_{d,k}^{i-1}$ |  |'
- en: '|  | $\displaystyle{\bm{u}}_{d,k}^{i-1}$ | $\displaystyle\in\{0,1\}.$ |  |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{u}}_{d,k}^{i-1}$ | $\displaystyle\in\{0,1\}.$ |  |'
- en: This last system can be easily converted to a linear system, and thus the training
    problem in this setting can be cast as a mixed-integer linear optimization problem.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ€åä¸€ä¸ªç³»ç»Ÿå¯ä»¥å¾ˆå®¹æ˜“åœ°è½¬æ¢ä¸ºçº¿æ€§ç³»ç»Ÿï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒé—®é¢˜å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ä¸ªæ··åˆæ•´æ•°çº¿æ€§ä¼˜åŒ–é—®é¢˜ã€‚
- en: Other works have also relied on similar formulations to train neural networks.
    Icarte etÂ al. ([2019](#bib.bib161)) introduce different objective functions that
    can be used along with the linear system to produce a MILP that can train BNNs.
    They also introduce a Constraint-Programming-based model and a hybrid model, and
    then compare all of them computationally. Thorbjarnarson and Yorke-Smith ([2020](#bib.bib305))
    introduce more MILP-based training models that leverage piecewise linear approximations
    of well-known non-linear loss functions and that can handle integer weights beyond
    $\{-1,0,1\}$. A similar setting is studied by Sildir and Aydin ([2022](#bib.bib287)),
    where piecewise linear approximations of non-linear activations are used, and
    integer weights are exploited to formulate the training problem as a MILP. Finally,
    Bernardelli etÂ al. ([2022](#bib.bib25)) rely on a multi-objective MIP model for
    training BNNs; from here, they create a BNN ensemble to produce robust classifiers.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–ä½œå“ä¹Ÿä¾èµ–äºç±»ä¼¼çš„å…¬å¼æ¥è®­ç»ƒç¥ç»ç½‘ç»œã€‚Icarte ç­‰äºº (2019) å¼•å…¥äº†ä¸åŒçš„ç›®æ ‡å‡½æ•°ï¼Œå¯ä»¥ä¸çº¿æ€§ç³»ç»Ÿä¸€èµ·ä½¿ç”¨ï¼Œç”Ÿæˆä¸€ä¸ªå¯ä»¥è®­ç»ƒ BNNs çš„
    MILPã€‚ä»–ä»¬è¿˜å¼•å…¥äº†åŸºäºçº¦æŸç¼–ç¨‹çš„æ¨¡å‹å’Œæ··åˆæ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†è®¡ç®—æ¯”è¾ƒã€‚Thorbjarnarson å’Œ Yorke-Smith (2020) å¼•å…¥äº†æ›´å¤šåŸºäº
    MILP çš„è®­ç»ƒæ¨¡å‹ï¼Œåˆ©ç”¨äº†ä¼—æ‰€å‘¨çŸ¥çš„éçº¿æ€§æŸå¤±å‡½æ•°çš„åˆ†æ®µçº¿æ€§é€¼è¿‘ï¼Œå¯ä»¥å¤„ç† $\{-1,0,1\}$ ä¹‹å¤–çš„æ•´æ•°æƒé‡ã€‚Sildir å’Œ Aydin (2022)
    ç ”ç©¶äº†ç±»ä¼¼çš„æƒ…å†µï¼Œå…¶ä¸­ä½¿ç”¨åˆ†æ®µçº¿æ€§é€¼è¿‘çš„éçº¿æ€§æ¿€æ´»ï¼Œå¹¶åˆ©ç”¨æ•´æ•°æƒé‡æ¥å°†è®­ç»ƒé—®é¢˜å»ºæ¨¡ä¸º MILPã€‚æœ€åï¼ŒBernardelli ç­‰äºº (2022) ä¾é å¤šç›®æ ‡
    MIP æ¨¡å‹æ¥è®­ç»ƒ BNNsï¼›ä»è¿™é‡Œï¼Œä»–ä»¬åˆ›å»ºäº†ä¸€ä¸ª BNN é›†åˆæ¥äº§ç”Ÿå¼ºå¥çš„åˆ†ç±»å™¨ã€‚
- en: From these articles, we can conclude that the MILP-based approach to training
    their neural networks can result in high-quality neural networks, especially in
    terms of generalization. However, many of these MILP-based methods currently do
    not scale well, as opposed to gradient-descent-based methods. We believe that,
    even though there are some theoretical limitations to the efficiency of MILP-based
    methods, there is a considerable practical improvement potential with using them
    in neural network training.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™äº›æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼ŒåŸºäº MILP çš„æ–¹æ³•è®­ç»ƒç¥ç»ç½‘ç»œå¯ä»¥äº§ç”Ÿé«˜è´¨é‡çš„ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯åœ¨æ³›åŒ–æ–¹é¢ã€‚ç„¶è€Œï¼Œè®¸å¤šè¿™äº›åŸºäº MILP çš„æ–¹æ³•å½“å‰ä¸å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œä¸æ¢¯åº¦ä¸‹é™æ³•ç›¸åã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œå³ä½¿å¯¹äºåŸºäº
    MILP æ–¹æ³•çš„æ•ˆç‡å­˜åœ¨ä¸€äº›ç†è®ºé™åˆ¶ï¼Œä½†åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ä½¿ç”¨å®ƒä»¬ä»ç„¶å…·æœ‰ç›¸å½“å¤§çš„å®é™…æ”¹è¿›æ½œåŠ›ã€‚
- en: 6 Conclusion
  id: totrans-590
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 ç»“è®º
- en: 'The rapid advancement of neural networks and their ubiquity has given rise
    to numerous new challenges and opportunities in deep learning: we need to design
    them in more reliable ways, to better understand their limits, and to test their
    robustness, among other challenges. While, traditionally, continuous optimization
    has been the predominant technology used in the optimization tasks in deep learning,
    some of these new challenges have made discrete optimization tools gain a remarkable
    importance.'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„å¿«é€Ÿè¿›æ­¥åŠå…¶æ™®éå­˜åœ¨å¸¦æ¥äº†æ·±åº¦å­¦ä¹ ä¸­è®¸å¤šæ–°çš„æŒ‘æˆ˜å’Œæœºé‡ï¼šæˆ‘ä»¬éœ€è¦ä»¥æ›´å¯é çš„æ–¹å¼è®¾è®¡å®ƒä»¬ï¼Œæ›´å¥½åœ°ç†è§£å®ƒä»¬çš„å±€é™æ€§ï¼Œå¹¶æµ‹è¯•å®ƒä»¬çš„é²æ£’æ€§ç­‰æŒ‘æˆ˜ã€‚è™½ç„¶ä¼ ç»Ÿä¸Šï¼Œè¿ç»­ä¼˜åŒ–ä¸€ç›´æ˜¯æ·±åº¦å­¦ä¹ ä¼˜åŒ–ä»»åŠ¡ä¸­ä½¿ç”¨çš„ä¸»è¦æŠ€æœ¯ï¼Œä½†ä¸€äº›æ–°çš„æŒ‘æˆ˜ä½¿å¾—ç¦»æ•£ä¼˜åŒ–å·¥å…·å˜å¾—å°¤ä¸ºé‡è¦ã€‚
- en: In this survey, we have reviewed multiple areas where polyhedral theory and
    linear optimization have played a critical role. For example, in understanding
    the expressiveness of neural networks, in optimizing trained neural networks (e.g.
    for verification purposes), and even in designing new training algorithms. We
    hope this survey can provide perspective in a rapidly-changing field, and motivate
    further developments in both deep learning and discrete optimization. There is
    still much to be explored in the intersection of these fields.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†å¤šé¢†åŸŸä¸­å¤šé¢ä½“ç†è®ºå’Œçº¿æ€§ä¼˜åŒ–å‘æŒ¥å…³é”®ä½œç”¨çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼Œåœ¨ç†è§£ç¥ç»ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä¼˜åŒ–è®­ç»ƒç¥ç»ç½‘ç»œï¼ˆä¾‹å¦‚ç”¨äºéªŒè¯ç›®çš„ï¼‰ï¼Œç”šè‡³è®¾è®¡æ–°çš„è®­ç»ƒç®—æ³•æ–¹é¢ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹è°ƒæŸ¥èƒ½åœ¨å¿«é€Ÿå˜åŒ–çš„é¢†åŸŸä¸­æä¾›è§†è§’ï¼Œå¹¶æ¿€åŠ±æ·±åº¦å­¦ä¹ å’Œç¦»æ•£ä¼˜åŒ–çš„è¿›ä¸€æ­¥å‘å±•ã€‚å…³äºè¿™äº›é¢†åŸŸäº¤é›†çš„æ¢ç´¢ä»æœ‰å¾ˆå¤šã€‚
- en: Acknowledgments
  id: totrans-593
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: è‡´è°¢
- en: We thank Christian Tjandraatmadja and Toon Tran for early feedback on the manuscript
    and asking questions that helped shaping it.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ„Ÿè°¢ Christian Tjandraatmadja å’Œ Toon Tran å¯¹æ‰‹ç¨¿çš„æ—©æœŸåé¦ˆå’Œæå‡ºçš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¸®åŠ©å¡‘é€ äº†æ‰‹ç¨¿ã€‚
- en: Thiago Serra was supported by the National Science Foundation (NSF) award IIS
    2104583. Calvin Tsay was supported by the Engineering & Physical Sciences Research
    Council (EPSRC) grant EP/T001577/1.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: Thiago Serra è·å¾—äº†å›½å®¶ç§‘å­¦åŸºé‡‘ä¼šï¼ˆNSFï¼‰å¥–é¡¹ IIS 2104583 çš„æ”¯æŒã€‚Calvin Tsay è·å¾—äº†å·¥ç¨‹ä¸ç‰©ç†ç§‘å­¦ç ”ç©¶å§”å‘˜ä¼šï¼ˆEPSRCï¼‰æ‹¨æ¬¾
    EP/T001577/1 çš„æ”¯æŒã€‚
- en: References
  id: totrans-596
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: Abrahamsen etÂ al. (2021) M.Â Abrahamsen, L.Â Kleist, and T.Â Miltzow. Training
    neural networks is er-complete. In *Neural Information Processing Systems (NeurIPS)*,
    volumeÂ 34, 2021.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abrahamsen ç­‰ï¼ˆ2021ï¼‰M. Abrahamsen, L. Kleist, å’Œ T. Miltzow. è®­ç»ƒç¥ç»ç½‘ç»œæ˜¯ er-complete
    çš„ã€‚å‘è¡¨äº*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œç¬¬34å·ï¼Œ2021å¹´ã€‚
- en: Agostinelli etÂ al. (2015) F.Â Agostinelli, M.Â Hoffman, P.Â Sadowski, and P.Â Baldi.
    Learning activation functions to improve deep neural networks. In *International
    Conference on Learning Representations (ICLR) Workshop*, 2015.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agostinelli ç­‰ï¼ˆ2015ï¼‰F. Agostinelli, M. Hoffman, P. Sadowski, å’Œ P. Baldi. å­¦ä¹ æ¿€æ´»å‡½æ•°ä»¥æ”¹å–„æ·±åº¦ç¥ç»ç½‘ç»œã€‚å‘è¡¨äº*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰ç ”è®¨ä¼š*ï¼Œ2015å¹´ã€‚
- en: Amrami and Goldberg (2021) A.Â Amrami and Y.Â Goldberg. A simple geometric proof
    for the benefit of depth in ReLU networks. *arXiv:2101.07126*, 2021.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amrami å’Œ Goldbergï¼ˆ2021ï¼‰A. Amrami å’Œ Y. Goldberg. é€šè¿‡å‡ ä½•è¯æ˜ ReLU ç½‘ç»œæ·±åº¦çš„å¥½å¤„ã€‚*arXiv:2101.07126*ï¼Œ2021å¹´ã€‚
- en: Anderson etÂ al. (2019) R.Â Anderson, J.Â Huchette, C.Â Tjandraatmadja, and J.Â Vielma.
    Strong mixed-integer programming formulations for trained neural networks. In
    *Integer Programming and Combinatorial Optimization (IPCO)*, 2019.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson ç­‰ï¼ˆ2019ï¼‰R. Anderson, J. Huchette, C. Tjandraatmadja, å’Œ J. Vielma. é’ˆå¯¹è®­ç»ƒç¥ç»ç½‘ç»œçš„å¼ºæ··åˆæ•´æ•°è§„åˆ’å…¬å¼ã€‚å‘è¡¨äº*æ•´æ•°è§„åˆ’ä¸ç»„åˆä¼˜åŒ–ï¼ˆIPCOï¼‰*ï¼Œ2019å¹´ã€‚
- en: Anderson etÂ al. (2020) R.Â Anderson, J.Â Huchette, W.Â Ma, C.Â Tjandraatmadja, and
    J.Â P. Vielma. Strong mixed-integer programming formulations for trained neural
    networks. *Mathematical Programming*, 183(1-2):3â€“39, 2020.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson ç­‰ï¼ˆ2020ï¼‰R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, å’Œ J. P.
    Vielma. é’ˆå¯¹è®­ç»ƒç¥ç»ç½‘ç»œçš„å¼ºæ··åˆæ•´æ•°è§„åˆ’å…¬å¼ã€‚*æ•°å­¦ç¼–ç¨‹*ï¼Œ183(1-2):3â€“39ï¼Œ2020å¹´ã€‚
- en: Anil etÂ al. (2019) C.Â Anil, J.Â Lucas, and R.Â Grosse. Sorting out Lipschitz function
    approximation. In *International Conference on Machine Learning (ICML)*, 2019.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil ç­‰ï¼ˆ2019ï¼‰C. Anil, J. Lucas, å’Œ R. Grosse. åˆ†æ Lipschitz å‡½æ•°è¿‘ä¼¼ã€‚å‘è¡¨äº*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œ2019å¹´ã€‚
- en: Arjovsky etÂ al. (2017) M.Â Arjovsky, S.Â Chintala, and L.Â Bottou. Wasserstein
    generative adversarial networks. In *International Conference on Machine Learning
    (ICML)*, 2017.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky ç­‰ï¼ˆ2017ï¼‰M. Arjovsky, S. Chintala, å’Œ L. Bottou. Wasserstein ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚å‘è¡¨äº*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œ2017å¹´ã€‚
- en: Arora etÂ al. (2018) R.Â Arora, A.Â Basu, P.Â Mianjy, and A.Â Mukherjee. Understanding
    deep neural networks with rectified linear units. In *International Conference
    on Learning Representations (ICLR)*, 2018.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora ç­‰ï¼ˆ2018ï¼‰R. Arora, A. Basu, P. Mianjy, å’Œ A. Mukherjee. é€šè¿‡ä¿®æ­£çº¿æ€§å•å…ƒç†è§£æ·±åº¦ç¥ç»ç½‘ç»œã€‚å‘è¡¨äº*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2018å¹´ã€‚
- en: Aziznejad etÂ al. (2020) S.Â Aziznejad, H.Â Gupta, J.Â Campos, and M.Â Unser. Deep
    neural networks with trainable activations and controlled Lipschitz constant.
    *IEEE Transactions on Signal Processing*, 68:4688â€“4699, 2020.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aziznejad et al. (2020) S. Aziznejad, H. Gupta, J. Campos, å’Œ M. Unser. å¸¦æœ‰å¯è®­ç»ƒæ¿€æ´»å‡½æ•°å’Œæ§åˆ¶Lipschitzå¸¸æ•°çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚*IEEEä¿¡å·å¤„ç†å­¦æŠ¥*ï¼Œ68:4688â€“4699ï¼Œ2020å¹´ã€‚
- en: Bahdanau etÂ al. (2015) D.Â Bahdanau, K.Â Cho, and Y.Â Bengio. Neural machine translation
    by jointly learning to align and translate. In *International Conference on Learning
    Representations (ICLR)*, 2015.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau et al. (2015) D. Bahdanau, K. Cho, å’Œ Y. Bengio. é€šè¿‡è”åˆå­¦ä¹ å¯¹é½å’Œç¿»è¯‘çš„ç¥ç»æœºå™¨ç¿»è¯‘ã€‚å‘è¡¨äº*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2015å¹´ã€‚
- en: 'Balas (1998) E.Â Balas. Disjunctive programming: Properties of the convex hull
    of feasible points. *Discrete Applied Mathematics*, 89(1-3):3â€“44, 1998.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balas (1998) E. Balas. ç¦»æ•£è§„åˆ’ï¼šå¯è¡Œç‚¹å‡¸åŒ…çš„æ€§è´¨ã€‚*ç¦»æ•£åº”ç”¨æ•°å­¦*ï¼Œ89(1-3):3â€“44ï¼Œ1998å¹´ã€‚
- en: Balas (2018) E.Â Balas. *Disjunctive Programming*. Springer Cham, 2018.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balas (2018) E. Balas. *ç¦»æ•£è§„åˆ’*ã€‚Springer Chamï¼Œ2018å¹´ã€‚
- en: Balas etÂ al. (1993) E.Â Balas, S.Â Ceria, and G.Â CornuÃ©jols. A lift-and-project
    cutting plane algorithm for mixed 0â€“1 programs. *Mathematical Programming*, 58(1-3):295â€“324,
    1993.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balas et al. (1993) E. Balas, S. Ceria, å’Œ G. CornuÃ©jols. ä¸€ç§ç”¨äºæ··åˆ0-1ç¨‹åºçš„æå‡ä¸æŠ•å½±åˆ‡å‰²å¹³é¢ç®—æ³•ã€‚*æ•°å­¦ç¼–ç¨‹*ï¼Œ58(1-3):295â€“324ï¼Œ1993å¹´ã€‚
- en: Balas etÂ al. (1996) E.Â Balas, S.Â Ceria, and G.Â CornuÃ©jols. Mixed 0-1 programming
    by lift-and-project in a branch-and-cut framework. *Management Science*, 42(9):1229â€“1246,
    1996.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balas et al. (1996) E. Balas, S. Ceria, å’Œ G. CornuÃ©jols. åœ¨åˆ†æ”¯åˆ‡å‰²æ¡†æ¶ä¸­é€šè¿‡æå‡å’ŒæŠ•å½±è¿›è¡Œæ··åˆ0-1è§„åˆ’ã€‚*ç®¡ç†ç§‘å­¦*ï¼Œ42(9):1229â€“1246ï¼Œ1996å¹´ã€‚
- en: Balestriero and Baraniuk (2018) R.Â Balestriero and R.Â G. Baraniuk. A spline
    theory of deep networks. In *International Conference on Machine Learning (ICML)*,
    2018.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balestriero and Baraniuk (2018) R. Balestriero å’Œ R. G. Baraniuk. æ·±åº¦ç½‘ç»œçš„æ ·æ¡ç†è®ºã€‚å‘è¡¨äº*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œ2018å¹´ã€‚
- en: 'BalunoviÄ‡ and Vechev (2020) M.Â BalunoviÄ‡ and M.Â Vechev. Adversarial training
    and provable defenses: Bridging the gap. In *International Conference on Learning
    Representations (ICLR)*, 2020.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BalunoviÄ‡ and Vechev (2020) M. BalunoviÄ‡ å’Œ M. Vechev. å¯¹æŠ—è®­ç»ƒå’Œå¯è¯æ˜é˜²å¾¡ï¼šå¼¥åˆå·®è·ã€‚å‘è¡¨äº*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2020å¹´ã€‚
- en: Batten etÂ al. (2021) B.Â Batten, P.Â Kouvaros, A.Â Lomuscio, and Y.Â Zheng. Efficient
    neural network verification via layer-based semidefinite relaxations and linear
    cuts. In *International Joint Conference on Artificial Intelligence (IJCAI)*,
    pages 2184â€“2190, 2021.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Batten et al. (2021) B. Batten, P. Kouvaros, A. Lomuscio, å’Œ Y. Zheng. é€šè¿‡åŸºäºå±‚çš„åŠæ­£å®šæ¾å¼›å’Œçº¿æ€§åˆ‡å‰²å®ç°é«˜æ•ˆç¥ç»ç½‘ç»œéªŒè¯ã€‚å‘è¡¨äº*å›½é™…äººå·¥æ™ºèƒ½è”åˆä¼šè®®ï¼ˆIJCAIï¼‰*ï¼Œç¬¬2184â€“2190é¡µï¼Œ2021å¹´ã€‚
- en: Bengio (2009) Y.Â Bengio. Learning deep architectures for AI. *Foundations and
    TrendsÂ®in Machine Learning*, 2(1):1â€“127, 2009.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio (2009) Y. Bengio. ä¸ºAIå­¦ä¹ æ·±åº¦æ¶æ„ã€‚*æœºå™¨å­¦ä¹ åŸºç¡€ä¸è¶‹åŠ¿*ï¼Œ2(1):1â€“127ï¼Œ2009å¹´ã€‚
- en: 'Bengio etÂ al. (2021) Y.Â Bengio, A.Â Lodi, and A.Â Prouvost. Machine learning
    for combinatorial optimization: a methodological tour dâ€™horizon. *European Journal
    of Operational Research*, 290(2):405â€“421, 2021.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio et al. (2021) Y. Bengio, A. Lodi, å’Œ A. Prouvost. ç»„åˆä¼˜åŒ–çš„æœºå™¨å­¦ä¹ ï¼šæ–¹æ³•è®ºæ¦‚è¿°ã€‚*æ¬§æ´²è¿ç­¹å­¦æ‚å¿—*ï¼Œ290(2):405â€“421ï¼Œ2021å¹´ã€‚
- en: Bennett (1992) K.Â P. Bennett. Decision tree construction via linear programming.
    Technical report, University of Wisconsin-Madison Department of Computer Sciences,
    1992.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bennett (1992) K. P. Bennett. é€šè¿‡çº¿æ€§è§„åˆ’æ„å»ºå†³ç­–æ ‘ã€‚æŠ€æœ¯æŠ¥å‘Šï¼Œå¨æ–¯åº·æ˜Ÿå¤§å­¦éº¦è¿ªé€Šåˆ†æ ¡è®¡ç®—æœºç§‘å­¦ç³»ï¼Œ1992å¹´ã€‚
- en: Bennett and Mangasarian (1990) K.Â P. Bennett and O.Â L. Mangasarian. Neural network
    training via linear programming. Technical report, University of Wisconsin-Madison
    Department of Computer Sciences, 1990.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bennett and Mangasarian (1990) K. P. Bennett å’Œ O. L. Mangasarian. é€šè¿‡çº¿æ€§è§„åˆ’è®­ç»ƒç¥ç»ç½‘ç»œã€‚æŠ€æœ¯æŠ¥å‘Šï¼Œå¨æ–¯åº·æ˜Ÿå¤§å­¦éº¦è¿ªé€Šåˆ†æ ¡è®¡ç®—æœºç§‘å­¦ç³»ï¼Œ1990å¹´ã€‚
- en: Bennett and Mangasarian (1992) K.Â P. Bennett and O.Â L. Mangasarian. Robust linear
    programming discrimination of two linearly inseparable sets. *Optimization Methods
    and Software*, 1(1):23â€“34, 1992.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bennett and Mangasarian (1992) K. P. Bennett å’Œ O. L. Mangasarian. å¯¹ä¸¤ä¸ªçº¿æ€§ä¸å¯åˆ†é›†åˆçš„é²æ£’çº¿æ€§è§„åˆ’åŒºåˆ†ã€‚*ä¼˜åŒ–æ–¹æ³•ä¸è½¯ä»¶*ï¼Œ1(1):23â€“34ï¼Œ1992å¹´ã€‚
- en: Benussi etÂ al. (2022) E.Â Benussi, A.Â Patane, M.Â Wicker, L.Â Laurenti, and M.Â Kwiatkowska.
    Individual fairness guarantees for neural networks. In *International Joint Conference
    on Artificial Intelligence (IJCAI)*, pages 651â€“658, 2022.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benussi et al. (2022) E. Benussi, A. Patane, M. Wicker, L. Laurenti, å’Œ M. Kwiatkowska.
    ç¥ç»ç½‘ç»œçš„ä¸ªä½“å…¬å¹³æ€§ä¿è¯ã€‚å‘è¡¨äº*å›½é™…äººå·¥æ™ºèƒ½è”åˆä¼šè®®ï¼ˆIJCAIï¼‰*ï¼Œç¬¬651â€“658é¡µï¼Œ2022å¹´ã€‚
- en: 'Bergman etÂ al. (2022) D.Â Bergman, T.Â Huang, P.Â Brooks, A.Â Lodi, and A.Â U. Raghunathan.
    Janos: an integrated predictive and prescriptive modeling framework. *INFORMS
    Journal on Computing*, 34(2):807â€“816, 2022.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergman et al. (2022) D. Bergman, T. Huang, P. Brooks, A. Lodi, å’Œ A. U. Raghunathan.
    Janosï¼šä¸€ä¸ªé›†æˆçš„é¢„æµ‹å’ŒæŒ‡ç¤ºå»ºæ¨¡æ¡†æ¶ã€‚*INFORMSè®¡ç®—æœŸåˆŠ*ï¼Œ34(2):807â€“816ï¼Œ2022å¹´ã€‚
- en: 'Bernardelli etÂ al. (2022) A.Â M. Bernardelli, S.Â Gualandi, H.Â C. Lau, and S.Â Milanesi.
    The bemi stardust: a structured ensemble of binarized neural networks. *arXiv
    preprint arXiv:2212.03659*, 2022.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bernardelli ç­‰ï¼ˆ2022ï¼‰A. M. Bernardelli, S. Gualandi, H. C. Lau å’Œ S. Milanesiã€‚BEMI
    æ˜Ÿå°˜ï¼šä¸€ä¸ªç»“æ„åŒ–çš„äºŒå€¼åŒ–ç¥ç»ç½‘ç»œé›†æˆã€‚*arXiv é¢„å°æœ¬ arXiv:2212.03659*ï¼Œ2022ã€‚
- en: Berrada etÂ al. (2018) L.Â Berrada, A.Â Zisserman, and M.Â P. Kumar. Deep Frank-Wolfe
    for neural network optimization. *arXiv:1811.07591*, 2018.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berrada ç­‰ï¼ˆ2018ï¼‰L. Berrada, A. Zisserman å’Œ M. P. Kumarã€‚ç”¨äºç¥ç»ç½‘ç»œä¼˜åŒ–çš„æ·±åº¦ Frank-Wolfe
    æ–¹æ³•ã€‚*arXiv:1811.07591*ï¼Œ2018ã€‚
- en: Bertschinger etÂ al. (2022) D.Â Bertschinger, C.Â Hertrich, P.Â Jungeblut, T.Â Miltzow,
    and S.Â Weber. Training fully connected neural networks is $\exists\mathbb{R}$-complete.
    *arXiv:2204.01368*, 2022.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertschinger ç­‰ï¼ˆ2022ï¼‰D. Bertschinger, C. Hertrich, P. Jungeblut, T. Miltzow å’Œ
    S. Weberã€‚è®­ç»ƒå…¨è¿æ¥ç¥ç»ç½‘ç»œæ˜¯ $\exists\mathbb{R}$-å®Œå…¨çš„ã€‚*arXiv:2204.01368*ï¼Œ2022ã€‚
- en: 'Bhosekar and Ierapetritou (2018) A.Â Bhosekar and M.Â Ierapetritou. Advances
    in surrogate based modeling, feasibility analysis, and optimization: A review.
    *Computers & Chemical Engineering*, 108:250â€“267, 2018.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhosekar å’Œ Ierapetritouï¼ˆ2018ï¼‰A. Bhosekar å’Œ M. Ierapetritouã€‚ä»£ç†å»ºæ¨¡ã€å¯è¡Œæ€§åˆ†æå’Œä¼˜åŒ–çš„è¿›å±•ï¼šç»¼è¿°ã€‚*è®¡ç®—æœºä¸åŒ–å­¦å·¥ç¨‹*ï¼Œ108:250â€“267ï¼Œ2018ã€‚
- en: 'Bianchini and Scarselli (2014) M.Â Bianchini and F.Â Scarselli. On the complexity
    of neural network classifiers: A comparison between shallow and deep architectures.
    *IEEE Transactions on Neural Networks and Learning Systems*, 2014.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianchini å’Œ Scarselliï¼ˆ2014ï¼‰M. Bianchini å’Œ F. Scarselliã€‚ç¥ç»ç½‘ç»œåˆ†ç±»å™¨çš„å¤æ‚æ€§ï¼šæµ…å±‚ä¸æ·±å±‚æ¶æ„çš„æ¯”è¾ƒã€‚*IEEE
    ç¥ç»ç½‘ç»œä¸å­¦ä¹ ç³»ç»Ÿæ±‡åˆŠ*ï¼Œ2014ã€‚
- en: Biau etÂ al. (2021) G.Â Biau, M.Â Sangnier, and U.Â Tanielian. Some theoretical
    insights into Wasserstein GANs. *Journal of Machine Learning Research*, 22, 2021.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biau ç­‰ï¼ˆ2021ï¼‰G. Biau, M. Sangnier å’Œ U. Tanielianã€‚Wasserstein GAN çš„ä¸€äº›ç†è®ºè§è§£ã€‚*æœºå™¨å­¦ä¹ ç ”ç©¶æœŸåˆŠ*ï¼Œ22ï¼Œ2021ã€‚
- en: Bienstock and MuÃ±oz (2018) D.Â Bienstock and G.Â MuÃ±oz. Lp formulations for polynomial
    optimization problems. *SIAM Journal on Optimization*, 28(2):1121â€“1150, 2018.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bienstock å’Œ MuÃ±ozï¼ˆ2018ï¼‰D. Bienstock å’Œ G. MuÃ±ozã€‚å¤šé¡¹å¼ä¼˜åŒ–é—®é¢˜çš„ Lp å½¢å¼ã€‚*SIAM ä¼˜åŒ–æœŸåˆŠ*ï¼Œ28(2):1121â€“1150ï¼Œ2018ã€‚
- en: Bienstock etÂ al. (2023) D.Â Bienstock, G.Â MuÃ±oz, and S.Â Pokutta. Principled deep
    neural network training through linear programming. *Discrete Optimization*, 49:100795,
    2023.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bienstock ç­‰ï¼ˆ2023ï¼‰D. Bienstock, G. MuÃ±oz å’Œ S. Pokuttaã€‚é€šè¿‡çº¿æ€§è§„åˆ’è¿›è¡Œæœ‰åŸåˆ™çš„æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒã€‚*ç¦»æ•£ä¼˜åŒ–*ï¼Œ49:100795ï¼Œ2023ã€‚
- en: Blum and Rivest (1992) A.Â L. Blum and R.Â L. Rivest. Training a 3-node neural
    network is np-complete. *Neural Networks*, 5(1):117â€“127, 1992.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blum å’Œ Rivestï¼ˆ1992ï¼‰A. L. Blum å’Œ R. L. Rivestã€‚è®­ç»ƒä¸€ä¸ª 3 èŠ‚ç‚¹ç¥ç»ç½‘ç»œæ˜¯ NP-å®Œå…¨çš„ã€‚*ç¥ç»ç½‘ç»œ*ï¼Œ5(1):117â€“127ï¼Œ1992ã€‚
- en: Bohra etÂ al. (2020) P.Â Bohra, J.Â Campos, H.Â Gupta, S.Â Aziznejad, and M.Â Unser.
    Learning activation functions in deep (spline) neural networks. *IEEE Open Journal
    of Signal Processing*, 1:295â€“309, 2020.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bohra ç­‰ï¼ˆ2020ï¼‰P. Bohra, J. Campos, H. Gupta, S. Aziznejad å’Œ M. Unserã€‚åœ¨æ·±åº¦ï¼ˆæ ·æ¡ï¼‰ç¥ç»ç½‘ç»œä¸­å­¦ä¹ æ¿€æ´»å‡½æ•°ã€‚*IEEE
    å¼€æ”¾ä¿¡å·å¤„ç†æœŸåˆŠ*ï¼Œ1:295â€“309ï¼Œ2020ã€‚
- en: Bonami etÂ al. (2015) P.Â Bonami, A.Â Lodi, A.Â Tramontani, and S.Â Wiese. On mathematical
    programming with indicator constraints. *Mathematical Programming*, 151:191â€“223,
    2015.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bonami ç­‰ï¼ˆ2015ï¼‰P. Bonami, A. Lodi, A. Tramontani å’Œ S. Wieseã€‚å…³äºå…·æœ‰æŒ‡ç¤ºçº¦æŸçš„æ•°å­¦è§„åˆ’ã€‚*æ•°å­¦è§„åˆ’*ï¼Œ151:191â€“223ï¼Œ2015ã€‚
- en: Boob etÂ al. (2022) D.Â Boob, S.Â S. Dey, and G.Â Lan. Complexity of training ReLU
    neural network. *Discrete Optimization*, 44:100620, 2022.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boob ç­‰ï¼ˆ2022ï¼‰D. Boob, S. S. Dey å’Œ G. Lanã€‚è®­ç»ƒ ReLU ç¥ç»ç½‘ç»œçš„å¤æ‚æ€§ã€‚*ç¦»æ•£ä¼˜åŒ–*ï¼Œ44:100620ï¼Œ2022ã€‚
- en: Botoeva etÂ al. (2020) E.Â Botoeva, P.Â Kouvaros, J.Â Kronqvist, A.Â Lomuscio, and
    R.Â Misener. Efficient verification of relu-based neural networks via dependency
    analysis. In *AAAI Conference on Artificial Intelligence*, volumeÂ 34, pages 3291â€“3299,
    2020.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Botoeva ç­‰ï¼ˆ2020ï¼‰E. Botoeva, P. Kouvaros, J. Kronqvist, A. Lomuscio å’Œ R. Misenerã€‚é€šè¿‡ä¾èµ–åˆ†æé«˜æ•ˆéªŒè¯åŸºäº
    ReLU çš„ç¥ç»ç½‘ç»œã€‚åœ¨ *AAAI äººå·¥æ™ºèƒ½ä¼šè®®*ï¼Œç¬¬ 34 å·ï¼Œé¡µé¢ 3291â€“3299ï¼Œ2020ã€‚
- en: Bottou etÂ al. (2018) L.Â Bottou, F.Â E. Curtis, and J.Â Nocedal. Optimization methods
    for large-scale machine learning. *SIAM Review*, 60(2):223â€“311, 2018.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bottou ç­‰ï¼ˆ2018ï¼‰L. Bottou, F. E. Curtis å’Œ J. Nocedalã€‚å¤§è§„æ¨¡æœºå™¨å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•ã€‚*SIAM ç»¼è¿°*ï¼Œ60(2):223â€“311ï¼Œ2018ã€‚
- en: Bridle (1990) J.Â S. Bridle. Probabilistic interpretation of feedforward classification
    network outputs, with relationships to statistical pattern recognition. In *Neurocomputing*,
    pages 227â€“236\. 1990.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bridleï¼ˆ1990ï¼‰J. S. Bridleã€‚å‰é¦ˆåˆ†ç±»ç½‘ç»œè¾“å‡ºçš„æ¦‚ç‡è§£é‡ŠåŠå…¶ä¸ç»Ÿè®¡æ¨¡å¼è¯†åˆ«çš„å…³ç³»ã€‚åœ¨ *ç¥ç»è®¡ç®—*ï¼Œé¡µé¢ 227â€“236ã€‚1990ã€‚
- en: 'Bubeck etÂ al. (2015) S.Â Bubeck etÂ al. Convex optimization: Algorithms and complexity.
    *Foundations and TrendsÂ® in Machine Learning*, 2015.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck ç­‰ï¼ˆ2015ï¼‰S. Bubeck ç­‰ã€‚å‡¸ä¼˜åŒ–ï¼šç®—æ³•ä¸å¤æ‚æ€§ã€‚*æœºå™¨å­¦ä¹ åŸºç¡€ä¸è¶‹åŠ¿Â®*ï¼Œ2015ã€‚
- en: Bunel etÂ al. (2020a) R.Â Bunel, A.Â DeÂ Palma, A.Â Desmaison, K.Â Dvijotham, P.Â Kohli,
    P.Â Torr, and M.Â PawanÂ Kumar. Lagrangian decomposition for neural network verification.
    In *Conference on Uncertainty in Artificial Intelligence (UAI)*, volume 124, pages
    370â€“379, 2020a.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bunel ç­‰ï¼ˆ2020aï¼‰R. Bunel, A. De Palma, A. Desmaison, K. Dvijotham, P. Kohli, P.
    Torr å’Œ M. Pawan Kumar. ç¥ç»ç½‘ç»œéªŒè¯çš„æ‹‰æ ¼æœ—æ—¥åˆ†è§£ã€‚å‘è¡¨äº *Conference on Uncertainty in Artificial
    Intelligence (UAI)*, ç¬¬ 124 å·, é¡µç  370â€“379, 2020aã€‚
- en: Bunel etÂ al. (2020b) R.Â Bunel, P.Â Mudigonda, I.Â Turkaslan, P.Â Torr, J.Â Lu, and
    P.Â Kohli. Branch and bound for piecewise linear neural network verification. *Journal
    of Machine Learning Research*, 21(2020), 2020b.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bunel ç­‰ï¼ˆ2020bï¼‰R. Bunel, P. Mudigonda, I. Turkaslan, P. Torr, J. Lu å’Œ P. Kohli.
    åˆ†æ®µçº¿æ€§ç¥ç»ç½‘ç»œéªŒè¯çš„åˆ†æ”¯å®šç•Œæ–¹æ³•ã€‚*Journal of Machine Learning Research*, 21(2020), 2020bã€‚
- en: Bunel etÂ al. (2018) R.Â R. Bunel, I.Â Turkaslan, P.Â Torr, P.Â Kohli, and P.Â K.
    Mudigonda. A unified view of piecewise linear neural network verification. *Neural
    Information Processing Systems (NeurIPS)*, 31, 2018.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bunel ç­‰ï¼ˆ2018ï¼‰R. R. Bunel, I. Turkaslan, P. Torr, P. Kohli å’Œ P. K. Mudigonda.
    åˆ†æ®µçº¿æ€§ç¥ç»ç½‘ç»œéªŒè¯çš„ç»Ÿä¸€è§†è§’ã€‚*Neural Information Processing Systems (NeurIPS)*, ç¬¬ 31 å·, 2018
    å¹´ã€‚
- en: Bunel etÂ al. (2020c) R.Â R. Bunel, O.Â Hinder, S.Â Bhojanapalli, and K.Â Dvijotham.
    An efficient nonconvex reformulation of stagewise convex optimization problems.
    *Neural Information Processing Systems (NeurIPS)*, 33:8247â€“8258, 2020c.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bunel ç­‰ï¼ˆ2020cï¼‰R. R. Bunel, O. Hinder, S. Bhojanapalli å’Œ K. Dvijotham. å¯¹é˜¶æ®µæ€§å‡¸ä¼˜åŒ–é—®é¢˜çš„é«˜æ•ˆéå‡¸é‡æ„ã€‚*Neural
    Information Processing Systems (NeurIPS)*, 33:8247â€“8258, 2020cã€‚
- en: Burtea and Tsay (2023) R.-A. Burtea and C.Â Tsay. Safe deployment of reinforcement
    learning using deterministic optimization over neural networks. In *Computer Aided
    Chemical Engineering*, volumeÂ 52, pages 1643â€“1648\. Elsevier, 2023.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burtea å’Œ Tsayï¼ˆ2023ï¼‰R.-A. Burtea å’Œ C. Tsay. ä½¿ç”¨ç¡®å®šæ€§ä¼˜åŒ–è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„å®‰å…¨éƒ¨ç½²ã€‚å‘è¡¨äº *Computer
    Aided Chemical Engineering*, ç¬¬ 52 å·, é¡µç  1643â€“1648, Elsevier, 2023ã€‚
- en: 'Cai etÂ al. (2023) J.Â Cai, K.-N. Nguyen, N.Â Shrestha, A.Â Good, R.Â Tu, X.Â Yu,
    S.Â Zhe, and T.Â Serra. Getting away with more network pruning: From sparsity to
    geometry and linear regions. In *International Conference on the Integration of
    Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR)*,
    2023.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai ç­‰ï¼ˆ2023ï¼‰J. Cai, K.-N. Nguyen, N. Shrestha, A. Good, R. Tu, X. Yu, S. Zhe
    å’Œ T. Serra. è¿›ä¸€æ­¥æŒ–æ˜ç½‘ç»œå‰ªæçš„æ½œåŠ›ï¼šä»ç¨€ç–æ€§åˆ°å‡ ä½•ç»“æ„å’Œçº¿æ€§åŒºåŸŸã€‚å‘è¡¨äº *International Conference on the Integration
    of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR)*,
    2023ã€‚
- en: 'Ceccon etÂ al. (2022) F.Â Ceccon, J.Â Jalving, J.Â Haddad, A.Â Thebelt, C.Â Tsay,
    C.Â D. Laird, and R.Â Misener. Omlt: Optimization & machine learning toolkit. *Journal
    of Machine Learning Research*, 23(349):1â€“8, 2022.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ceccon ç­‰ï¼ˆ2022ï¼‰F. Ceccon, J. Jalving, J. Haddad, A. Thebelt, C. Tsay, C. D.
    Laird å’Œ R. Misener. Omlt: ä¼˜åŒ–ä¸æœºå™¨å­¦ä¹ å·¥å…·åŒ…ã€‚*Journal of Machine Learning Research*, 23(349):1â€“8,
    2022ã€‚'
- en: Charisopoulos and Maragos (2018) V.Â Charisopoulos and P.Â Maragos. A tropical
    approach to neural networks with piecewise linear activations. *arXiv:1805.08749*,
    2018.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charisopoulos å’Œ Maragosï¼ˆ2018ï¼‰V. Charisopoulos å’Œ P. Maragos. å¯¹å…·æœ‰åˆ†æ®µçº¿æ€§æ¿€æ´»çš„ç¥ç»ç½‘ç»œçš„çƒ­å¸¦æ–¹æ³•ã€‚*arXiv:1805.08749*,
    2018ã€‚
- en: Chaudhry etÂ al. (2020) A.Â Chaudhry, N.Â Khan, P.Â Dokania, and P.Â Torr. Continual
    learning in low-rank orthogonal subspaces. In *Neural Information Processing Systems
    (NeurIPS)*, volumeÂ 33, 2020.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaudhry ç­‰ï¼ˆ2020ï¼‰A. Chaudhry, N. Khan, P. Dokania å’Œ P. Torr. åœ¨ä½ç§©æ­£äº¤å­ç©ºé—´ä¸­çš„æŒç»­å­¦ä¹ ã€‚å‘è¡¨äº
    *Neural Information Processing Systems (NeurIPS)*, ç¬¬ 33 å·, 2020 å¹´ã€‚
- en: Chen etÂ al. (2022a) H.Â Chen, Y.Â G. Wang, and H.Â Xiong. Lower and upper bounds
    for numbers of linear regions of graph convolutional networks. *arXiv:2206.00228*,
    2022a.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen ç­‰ï¼ˆ2022aï¼‰H. Chen, Y. G. Wang å’Œ H. Xiong. å›¾å·ç§¯ç½‘ç»œçš„çº¿æ€§åŒºåŸŸæ•°é‡çš„ä¸‹ç•Œå’Œä¸Šç•Œã€‚*arXiv:2206.00228*,
    2022aã€‚
- en: Chen etÂ al. (2022b) K.-L. Chen, H.Â Garudadri, and B.Â D. Rao. Improved bounds
    on neural complexity for representing piecewise linear functions. In *Neural Information
    Processing Systems (NeurIPS)*, 2022b.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen ç­‰ï¼ˆ2022bï¼‰K.-L. Chen, H. Garudadri å’Œ B. D. Rao. è¡¨ç¤ºåˆ†æ®µçº¿æ€§å‡½æ•°çš„ç¥ç»ç½‘ç»œå¤æ‚æ€§çš„æ”¹è¿›ç•Œé™ã€‚å‘è¡¨äº
    *Neural Information Processing Systems (NeurIPS)*, 2022bã€‚
- en: Chen etÂ al. (2022c) S.Â Chen, A.Â R. Klivans, and R.Â Meka. Learning deep ReLU
    networks is fixed-parameter tractable. In *2021 IEEE 62nd Annual Symposium on
    Foundations of Computer Science (FOCS)*, pages 696â€“707\. IEEE, 2022c.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen ç­‰ï¼ˆ2022cï¼‰S. Chen, A. R. Klivans å’Œ R. Meka. å­¦ä¹ æ·±åº¦ ReLU ç½‘ç»œæ˜¯å›ºå®šå‚æ•°å¯å¤„ç†çš„ã€‚å‘è¡¨äº *2021
    IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)*, é¡µç  696â€“707,
    IEEE, 2022cã€‚
- en: Chen etÂ al. (2020) T.Â Chen, J.-B. Lasserre, V.Â Magron, and E.Â Pauwels. Semialgebraic
    optimization for Lipschitz constants of ReLU networks. In *Neural Information
    Processing Systems (NeurIPS)*, volumeÂ 33, 2020.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen ç­‰ï¼ˆ2020ï¼‰T. Chen, J.-B. Lasserre, V. Magron å’Œ E. Pauwels. ReLU ç½‘ç»œ Lipschitz
    å¸¸æ•°çš„åŠä»£æ•°ä¼˜åŒ–ã€‚å‘è¡¨äº *Neural Information Processing Systems (NeurIPS)*, ç¬¬ 33 å·, 2020 å¹´ã€‚
- en: 'Chen etÂ al. (2021a) W.Â Chen, X.Â Gong, and Z.Â Wang. Neural architecture search
    on ImageNet in four GPU hours: A theoretically inspired perspective. In *International
    Conference on Learning Representations (ICLR)*, 2021a.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021a) W. Chen, X. Gong, å’Œ Z. Wang. åœ¨å››ä¸ª GPU å°æ—¶å†…å¯¹ ImageNet çš„ç¥ç»æ¶æ„æœç´¢ï¼šä¸€ç§ç†è®ºå¯å‘çš„è§†è§’ã€‚åœ¨*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2021aå¹´ã€‚
- en: Chen etÂ al. (2021b) W.Â Chen, X.Â Gong, Y.Â Wei, H.Â Shi, Z.Â Yan, Y.Â Yang, and Z.Â Wang.
    Understanding and accelerating neural architecture search with training-free and
    theory-grounded metrics. *arXiv:2108.11939*, 2021b.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021b) W. Chen, X. Gong, Y. Wei, H. Shi, Z. Yan, Y. Yang, å’Œ Z.
    Wang. ç†è§£å’ŒåŠ é€Ÿç¥ç»æ¶æ„æœç´¢ï¼Œé€šè¿‡æ— è®­ç»ƒå’Œç†è®ºåŸºç¡€çš„åº¦é‡ã€‚*arXiv:2108.11939*ï¼Œ2021bå¹´ã€‚
- en: Cheng etÂ al. (2017) C.Â Cheng, G.Â NÃ¼hrenberg, and H.Â Ruess. Maximum resilience
    of artificial neural networks. In *Automated Technology for Verification and Analysis
    (ATVA)*, pages 251â€“268, 2017.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2017) C. Cheng, G. NÃ¼hrenberg, å’Œ H. Ruess. äººå·¥ç¥ç»ç½‘ç»œçš„æœ€å¤§å¼¹æ€§ã€‚åœ¨*è‡ªåŠ¨åŒ–æŠ€æœ¯éªŒè¯ä¸åˆ†æï¼ˆATVAï¼‰*ï¼Œç¬¬251â€“268é¡µï¼Œ2017å¹´ã€‚
- en: 'Cheng etÂ al. (2018) C.-H. Cheng, G.Â NÃ¼hrenberg, C.-H. Huang, and H.Â Ruess.
    Verification of binarized neural networks via inter-neuron factoring: (short paper).
    In *International Conference on Verified Software: Theories, Tools, and Experiments
    (VSTTE)*, pages 279â€“290\. Springer, 2018.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2018) C.-H. Cheng, G. NÃ¼hrenberg, C.-H. Huang, å’Œ H. Ruess. é€šè¿‡ç¥ç»å…ƒé—´åˆ†è§£éªŒè¯äºŒå€¼åŒ–ç¥ç»ç½‘ç»œï¼šï¼ˆçŸ­è®ºæ–‡ï¼‰ã€‚åœ¨*å›½é™…éªŒè¯è½¯ä»¶ï¼šç†è®ºã€å·¥å…·ä¸å®éªŒä¼šè®®ï¼ˆVSTTEï¼‰*ï¼Œç¬¬279â€“290é¡µï¼ŒSpringerï¼Œ2018å¹´ã€‚
- en: Cheon (2022) M.-S. Cheon. An outer-approximation guided optimization approach
    for constrained neural network inverse problems. *Mathematical Programming*, 196(1-2):173â€“202,
    2022.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheon (2022) M.-S. Cheon. ä¸€ç§å¤–éƒ¨è¿‘ä¼¼æŒ‡å¯¼çš„ä¼˜åŒ–æ–¹æ³•ç”¨äºçº¦æŸç¥ç»ç½‘ç»œåé—®é¢˜ã€‚*æ•°å­¦è§„åˆ’*ï¼Œ196(1-2):173â€“202ï¼Œ2022å¹´ã€‚
- en: 'Chu etÂ al. (2018) L.Â Chu, X.Â Hu, J.Â Hu, L.Â Wang, and J.Â Pei. Exact and consistent
    interpretation for piecewise linear neural networks: A closed form solution. In
    *ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)*, 2018.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu et al. (2018) L. Chu, X. Hu, J. Hu, L. Wang, å’Œ J. Pei. å¯¹åˆ†æ®µçº¿æ€§ç¥ç»ç½‘ç»œçš„ç²¾ç¡®ä¸€è‡´è§£é‡Šï¼šä¸€ä¸ªå°é—­å½¢å¼çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨*ACM
    SIGKDDçŸ¥è¯†å‘ç°ä¸æ•°æ®æŒ–æ˜ä¼šè®®ï¼ˆKDDï¼‰*ï¼Œ2018å¹´ã€‚
- en: Ciresan etÂ al. (2012) D.Â Ciresan, U.Â Meier, J.Â Masci, and J.Â Schmidhuber. Multi
    column deep neural network for traffic sign classification. *Neural Networks*,
    2012.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciresan et al. (2012) D. Ciresan, U. Meier, J. Masci, å’Œ J. Schmidhuber. ç”¨äºäº¤é€šæ ‡å¿—åˆ†ç±»çš„å¤šåˆ—æ·±åº¦ç¥ç»ç½‘ç»œã€‚*ç¥ç»ç½‘ç»œ*ï¼Œ2012å¹´ã€‚
- en: Cohan etÂ al. (2022) S.Â Cohan, N.Â H. Kim, D.Â Rolnick, and M.Â vanÂ de Panne. Understanding
    the evolution of linear regions in deep reinforcement learning. In *Neural Information
    Processing Systems (NeurIPS)*, 2022.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohan et al. (2022) S. Cohan, N. H. Kim, D. Rolnick, å’Œ M. van de Panne. ç†è§£æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çº¿æ€§åŒºåŸŸçš„æ¼”å˜ã€‚åœ¨*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œ2022å¹´ã€‚
- en: Collobert (2004) R.Â Collobert. *Large Scale Machine Learning*. PhD thesis, University
    Paris 6, 2004.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collobert (2004) R.Â Collobert. *å¤§è§„æ¨¡æœºå™¨å­¦ä¹ *ã€‚åšå£«è®ºæ–‡ï¼Œå·´é»ç¬¬å…­å¤§å­¦ï¼Œ2004å¹´ã€‚
- en: 'Courbariaux etÂ al. (2015) M.Â Courbariaux, Y.Â Bengio, and J.-P. David. BinaryConnect:
    Training deep neural networks with binary weights during propagations. *Neural
    Information Processing Systems (NeurIPS)*, 28, 2015.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Courbariaux et al. (2015) M. Courbariaux, Y. Bengio, å’Œ J.-P. David. BinaryConnectï¼šåœ¨ä¼ æ’­è¿‡ç¨‹ä¸­ç”¨äºŒå€¼æƒé‡è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œ28ï¼Œ2015å¹´ã€‚
- en: Craighero etÂ al. (2020a) F.Â Craighero, F.Â Angaroni, A.Â Graudenzi, F.Â Stella,
    and M.Â Antoniotti. Investigating the compositional structure of deep neural networks.
    In *Machine Learning, Optimization, and Data Science (LOD)*, pages 322â€“334, 2020a.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Craighero et al. (2020a) F. Craighero, F. Angaroni, A. Graudenzi, F. Stella,
    å’Œ M. Antoniotti. è°ƒæŸ¥æ·±åº¦ç¥ç»ç½‘ç»œçš„ç»„åˆç»“æ„ã€‚åœ¨*æœºå™¨å­¦ä¹ ã€ä¼˜åŒ–ä¸æ•°æ®ç§‘å­¦ï¼ˆLODï¼‰*ï¼Œç¬¬322â€“334é¡µï¼Œ2020aå¹´ã€‚
- en: Craighero etÂ al. (2020b) F.Â Craighero, F.Â Angaroni, A.Â Graudenzi, F.Â Stella,
    and M.Â Antoniotti. Understanding deep learning with activation pattern diagrams.
    In *Proceedings of the Italian Workshop on Explainable Artificial Intelligence
    co-located with 19th International Conference of the Italian Association for Artificial
    Intelligence*, 2020b.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Craighero et al. (2020b) F. Craighero, F. Angaroni, A. Graudenzi, F. Stella,
    å’Œ M. Antoniotti. é€šè¿‡æ¿€æ´»æ¨¡å¼å›¾ç†è§£æ·±åº¦å­¦ä¹ ã€‚åœ¨*æ„å¤§åˆ©å¯è§£é‡Šäººå·¥æ™ºèƒ½ç ”è®¨ä¼šè®ºæ–‡é›†*ï¼Œä¸ç¬¬19å±Šæ„å¤§åˆ©äººå·¥æ™ºèƒ½åä¼šå›½é™…ä¼šè®®å…±åŒä¸¾åŠï¼Œ2020bå¹´ã€‚
- en: Croce and Hein (2018) F.Â Croce and M.Â Hein. A randomized gradient-free attack
    on ReLU networks. In *German Conference on Pattern Recognition (GCPR)*, 2018.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croce and Hein (2018) F. Croce å’Œ M. Hein. å¯¹ ReLU ç½‘ç»œçš„éšæœºæ¢¯åº¦æ— æ”»å‡»ã€‚åœ¨*å¾·å›½æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆGCPRï¼‰*ï¼Œ2018å¹´ã€‚
- en: Croce etÂ al. (2019) F.Â Croce, M.Â Andriushchenko, and M.Â Hein. Provable robustness
    of relu networks via maximization of linear regions. In *International Conference
    on Artificial Intelligence and Statistics (AISTATS)*, 2019.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croce et al. (2019) F. Croce, M. Andriushchenko, å’Œ M. Hein. é€šè¿‡æœ€å¤§åŒ–çº¿æ€§åŒºåŸŸè¯æ˜ ReLU
    ç½‘ç»œçš„é²æ£’æ€§ã€‚åœ¨*å›½é™…äººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ä¼šè®®ï¼ˆAISTATSï¼‰*ï¼Œ2019å¹´ã€‚
- en: Croce etÂ al. (2020) F.Â Croce, J.Â Rauber, and M.Â Hein. Scaling up the randomized
    gradient-free adversarial attack reveals overestimation of robustness using established
    attacks. *International Journal of Computer Vision*, 128:1028â€“1046, 2020.
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croce ç­‰ (2020) F. Croce, J. Rauber, å’Œ M. Hein. æ‰©å¤§éšæœºæ¢¯åº¦æ— å…³å¯¹æŠ—æ”»å‡»æ­ç¤ºäº†ä½¿ç”¨å·²å»ºç«‹æ”»å‡»çš„é²æ£’æ€§è¿‡é«˜ä¼°è®¡ã€‚*è®¡ç®—æœºè§†è§‰å›½é™…æœŸåˆŠ*ï¼Œ128:1028â€“1046,
    2020ã€‚
- en: Croxton etÂ al. (2003) K.Â L. Croxton, B.Â Gendron, and T.Â L. Magnanti. A comparison
    of mixed-integer programming models for nonconvex piecewise linear cost minimization
    problems. *Management Science*, 49(9):1268â€“1273, 2003.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croxton ç­‰ (2003) K. L. Croxton, B. Gendron, å’Œ T. L. Magnanti. å¯¹éå‡¸åˆ†æ®µçº¿æ€§æˆæœ¬æœ€å°åŒ–é—®é¢˜çš„æ··åˆæ•´æ•°è§„åˆ’æ¨¡å‹çš„æ¯”è¾ƒã€‚*ç®¡ç†ç§‘å­¦*ï¼Œ49(9):1268â€“1273,
    2003ã€‚
- en: 'Curtis and Scheinberg (2017) F.Â E. Curtis and K.Â Scheinberg. Optimization methods
    for supervised machine learning: From linear models to deep learning. In *INFORMS
    TutORials in Operations Research*, pages 89â€“114. INFORMS, 2017.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Curtis å’Œ Scheinberg (2017) F. E. Curtis å’Œ K. Scheinberg. ç›‘ç£æœºå™¨å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•ï¼šä»çº¿æ€§æ¨¡å‹åˆ°æ·±åº¦å­¦ä¹ ã€‚è§
    *INFORMS æ“ä½œç ”ç©¶æ•™ç¨‹*ï¼Œç¬¬89â€“114é¡µã€‚INFORMS, 2017ã€‚
- en: Cybenko (1989) G.Â Cybenko. Approximation by superpositions of a sigmoidal function.
    *Mathematics of Control, Signals and Systems*, 1989.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cybenko (1989) G. Cybenko. ç”± sigmoid å‡½æ•°çš„å åŠ è¿›è¡Œçš„é€¼è¿‘ã€‚*æ§åˆ¶ã€ä¿¡å·ä¸ç³»ç»Ÿçš„æ•°å­¦*ï¼Œ1989ã€‚
- en: Danna etÂ al. (2007) E.Â Danna, M.Â Fenelon, Z.Â Gu, and R.Â Wunderling. Generating
    multiple solutions for mixed integer programming problems. In *Integer Programming
    and Combinatorial Optimization (IPCO)*, pages 280â€“294\. 2007.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Danna ç­‰ (2007) E. Danna, M. Fenelon, Z. Gu, å’Œ R. Wunderling. ç”Ÿæˆæ··åˆæ•´æ•°è§„åˆ’é—®é¢˜çš„å¤šä¸ªè§£ã€‚è§
    *æ•´æ•°è§„åˆ’ä¸ç»„åˆä¼˜åŒ– (IPCO)*ï¼Œç¬¬280â€“294é¡µã€‚2007ã€‚
- en: Dantzig (1960) G.Â B. Dantzig. On the significance of solving linear programming
    problems with some integer variables. *Econometrica, Journal of the Econometric
    Society*, pages 30â€“44, 1960.
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dantzig (1960) G. B. Dantzig. è§£å†³åŒ…å«ä¸€äº›æ•´æ•°å˜é‡çš„çº¿æ€§è§„åˆ’é—®é¢˜çš„æ„ä¹‰ã€‚*ç»æµè®¡é‡å­¦ï¼Œç»æµè®¡é‡å­¦ä¼šæœŸåˆŠ*ï¼Œç¬¬30â€“44é¡µï¼Œ1960ã€‚
- en: Dantzig and Eaves (1973) G.Â B. Dantzig and B.Â C. Eaves. Fourier-Motzkin elimination
    and its dual. *Journal of Combinatorial Theory (A)*, 14:288â€“297, 1973.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dantzig å’Œ Eaves (1973) G. B. Dantzig å’Œ B. C. Eaves. Fourier-Motzkin æ¶ˆå»åŠå…¶å¯¹å¶ã€‚*ç»„åˆç†è®ºæœŸåˆŠ
    (A)*, 14:288â€“297, 1973ã€‚
- en: Dathathri etÂ al. (2020) S.Â Dathathri, K.Â Dvijotham, A.Â Kurakin, A.Â Raghunathan,
    J.Â Uesato, R.Â R. Bunel, S.Â Shankar, J.Â Steinhardt, I.Â Goodfellow, P.Â S. Liang,
    etÂ al. Enabling certification of verification-agnostic networks via memory-efficient
    semidefinite programming. *Neural Information Processing Systems (NeurIPS)*, 33:5318â€“5331,
    2020.
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dathathri ç­‰ (2020) S. Dathathri, K. Dvijotham, A. Kurakin, A. Raghunathan, J.
    Uesato, R. R. Bunel, S. Shankar, J. Steinhardt, I. Goodfellow, P. S. Liang ç­‰.
    é€šè¿‡å†…å­˜é«˜æ•ˆçš„åŠæ­£å®šè§„åˆ’å¯ç”¨å¯¹éªŒè¯æ— å…³ç½‘ç»œçš„è®¤è¯ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿ (NeurIPS)*ï¼Œ33:5318â€“5331, 2020ã€‚
- en: Daubechies etÂ al. (2022) I.Â Daubechies, R.Â DeVore, S.Â Foucart, B.Â Hanin, and
    G.Â Petrova. Nonlinear approximation and (deep) ReLU networks. *Constructive Approximation*,
    55:127â€“172, 2022.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daubechies ç­‰ (2022) I. Daubechies, R. DeVore, S. Foucart, B. Hanin, å’Œ G. Petrova.
    éçº¿æ€§é€¼è¿‘ä¸ (æ·±åº¦) ReLU ç½‘ç»œã€‚*æ„é€ æ€§é€¼è¿‘*ï¼Œ55:127â€“172, 2022ã€‚
- en: DeÂ Palma etÂ al. (2021) A.Â DeÂ Palma, H.Â Behl, R.Â R. Bunel, P.Â Torr, and M.Â P.
    Kumar. Scaling the convex barrier with active sets. In *International Conference
    on Learning Representations (ICLR)*, 2021.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Palma ç­‰ (2021) A. De Palma, H. Behl, R. R. Bunel, P. Torr, å’Œ M. P. Kumar.
    ä½¿ç”¨æ´»åŠ¨é›†è¿›è¡Œå‡¸éšœç¢çš„ç¼©æ”¾ã€‚è§ *å›½é™…å­¦ä¹ è¡¨ç¤ºä¼šè®® (ICLR)*ï¼Œ2021ã€‚
- en: 'Delarue etÂ al. (2020) A.Â Delarue, R.Â Anderson, and C.Â Tjandraatmadja. Reinforcement
    learning with combinatorial actions: An application to vehicle routing. *Neural
    Information Processing Systems (NeurIPS)*, 33:609â€“620, 2020.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delarue ç­‰ (2020) A. Delarue, R. Anderson, å’Œ C. Tjandraatmadja. å…·æœ‰ç»„åˆåŠ¨ä½œçš„å¼ºåŒ–å­¦ä¹ ï¼šåº”ç”¨äºè½¦è¾†è·¯å¾„è§„åˆ’ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿ
    (NeurIPS)*ï¼Œ33:609â€“620, 2020ã€‚
- en: Deng etÂ al. (2020) Y.Â Deng, X.Â Zheng, T.Â Zhang, C.Â Chen, G.Â Lou, and M.Â Kim.
    An analysis of adversarial attacks and defenses on autonomous driving models.
    In *2020 IEEE international conference on pervasive computing and communications
    (PerCom)*, pages 1â€“10\. IEEE, 2020.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng ç­‰ (2020) Y. Deng, X. Zheng, T. Zhang, C. Chen, G. Lou, å’Œ M. Kim. å¯¹è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„å¯¹æŠ—æ”»å‡»å’Œé˜²å¾¡çš„åˆ†æã€‚è§
    *2020 IEEE å›½é™…æ™®é€‚è®¡ç®—ä¸é€šä¿¡ä¼šè®® (PerCom)*ï¼Œç¬¬1â€“10é¡µã€‚IEEEï¼Œ2020ã€‚
- en: 'Devlin etÂ al. (2019) J.Â Devlin, M.-W. Chang, K.Â Lee, and K.Â Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    *Conference of the North American Chapter of the Association for Computational
    Linguistics (NAACL)*, 2019.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin ç­‰ (2019) J. Devlin, M.-W. Chang, K. Lee, å’Œ K. Toutanova. BERTï¼šç”¨äºè¯­è¨€ç†è§£çš„æ·±åº¦åŒå‘å˜æ¢å™¨çš„é¢„è®­ç»ƒã€‚è§
    *åŒ—ç¾è®¡ç®—è¯­è¨€å­¦åä¼šä¼šè®® (NAACL)*ï¼Œ2019ã€‚
- en: Dey etÂ al. (2020) S.Â S. Dey, G.Â Wang, and Y.Â Xie. Approximation algorithms for
    training one-node relu neural networks. *IEEE Transactions on Signal Processing*,
    68:6696â€“6706, 2020.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dey et al. (2020) S. S. Dey, G. Wang, å’Œ Y. Xie. è®­ç»ƒå•èŠ‚ç‚¹reluç¥ç»ç½‘ç»œçš„è¿‘ä¼¼ç®—æ³•ã€‚*IEEEä¿¡å·å¤„ç†å­¦æŠ¥*ï¼Œ68:6696â€“6706ï¼Œ2020ã€‚
- en: Dubey etÂ al. (2021) S.Â R. Dubey, S.Â K. Singh, and B.Â B. Chaudhuri. A comprehensive
    survey and performance analysis of activation functions in deep learning. *arXiv:2109.14545*,
    2021.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey et al. (2021) S. R. Dubey, S. K. Singh, å’Œ B. B. Chaudhuri. æ·±åº¦å­¦ä¹ ä¸­æ¿€æ´»å‡½æ•°çš„ç»¼åˆè°ƒæŸ¥ä¸æ€§èƒ½åˆ†æã€‚*arXiv:2109.14545*ï¼Œ2021ã€‚
- en: 'Dutta etÂ al. (2018) S.Â Dutta, S.Â Jha, S.Â Sankaranarayanan, and A.Â Tiwari. Output
    range analysis for deep feedforward networks. In *NASA Formal Methods: 10th International
    Symposium, (NFM)*, 2018.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dutta et al. (2018) S. Dutta, S. Jha, S. Sankaranarayanan, å’Œ A. Tiwari. æ·±åº¦å‰é¦ˆç½‘ç»œçš„è¾“å‡ºèŒƒå›´åˆ†æã€‚å‘è¡¨äº*NASAå½¢å¼åŒ–æ–¹æ³•ï¼šç¬¬10å±Šå›½é™…ç ”è®¨ä¼š
    (NFM)*ï¼Œ2018ã€‚
- en: Dvijotham etÂ al. (2018a) K.Â Dvijotham, S.Â Gowal, R.Â Stanforth, R.Â Arandjelovic,
    B.Â Oâ€™Donoghue, J.Â Uesato, and P.Â Kohli. Training verified learners with learned
    verifiers. *arXiv:1805.10265*, 2018a.
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dvijotham et al. (2018a) K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic,
    B. Oâ€™Donoghue, J. Uesato, å’Œ P. Kohli. ä½¿ç”¨å­¦ä¹ çš„éªŒè¯å™¨è®­ç»ƒç»è¿‡éªŒè¯çš„å­¦ä¹ è€…ã€‚*arXiv:1805.10265*ï¼Œ2018aã€‚
- en: Dvijotham etÂ al. (2018b) K.Â Dvijotham, R.Â Stanforth, S.Â Gowal, T.Â A. Mann, and
    P.Â Kohli. A dual approach to scalable verification of deep networks. In *Conference
    on Uncertainty in Artificial Intelligence (UAI)*, volumeÂ 1, pageÂ 3, 2018b.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dvijotham et al. (2018b) K. Dvijotham, R. Stanforth, S. Gowal, T. A. Mann, å’Œ
    P. Kohli. ä¸€ç§å¯æ‰©å±•æ·±åº¦ç½‘ç»œéªŒè¯çš„åŒé‡æ–¹æ³•ã€‚å‘è¡¨äº*äººå·¥æ™ºèƒ½ä¸ç¡®å®šæ€§ä¼šè®® (UAI)*ï¼Œç¬¬1å·ï¼Œç¬¬3é¡µï¼Œ2018bã€‚
- en: Dym etÂ al. (2020) N.Â Dym, B.Â Sober, and I.Â Daubechies. Expression of fractals
    through neural network functions. *IEEE Journal on Selected Areas in Information
    Theory*, 1(1):57â€“66, 2020.
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dym et al. (2020) N. Dym, B. Sober, å’Œ I. Daubechies. é€šè¿‡ç¥ç»ç½‘ç»œå‡½æ•°è¡¨è¾¾åˆ†å½¢ã€‚*IEEEä¿¡æ¯ç†è®ºé€‰æ‹©é¢†åŸŸæœŸåˆŠ*ï¼Œ1(1):57â€“66ï¼Œ2020ã€‚
- en: Ehlers (2017) R.Â Ehlers. Formal verification of piece-wise linear feed-forward
    neural networks. In *Automated Technology for Verification and Analysis (ATVA)*,
    pages 269â€“286\. Springer, 2017.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ehlers (2017) R. Ehlers. åˆ†æ®µçº¿æ€§å‰é¦ˆç¥ç»ç½‘ç»œçš„å½¢å¼åŒ–éªŒè¯ã€‚å‘è¡¨äº*è‡ªåŠ¨åŒ–æŠ€æœ¯éªŒè¯ä¸åˆ†æ (ATVA)*ï¼Œç¬¬269â€“286é¡µã€‚Springerï¼Œ2017ã€‚
- en: ElAraby etÂ al. (2020) M.Â ElAraby, G.Â Wolf, and M.Â Carvalho. Identifying efficient
    sub-networks using mixed integer programming. In *OPT Workshop*, 2020.
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ElAraby et al. (2020) M. ElAraby, G. Wolf, å’Œ M. Carvalho. ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’è¯†åˆ«é«˜æ•ˆå­ç½‘ç»œã€‚å‘è¡¨äº*OPTç ”è®¨ä¼š*ï¼Œ2020ã€‚
- en: 'Elsken etÂ al. (2019) T.Â Elsken, J.Â H. Metzen, and F.Â Hutter. Neural architecture
    search: A survey. *Journal of Machine Learning Research*, 20:1â€“21, 2019.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsken et al. (2019) T. Elsken, J. H. Metzen, å’Œ F. Hutter. ç¥ç»æ¶æ„æœç´¢ï¼šä¸€é¡¹è°ƒæŸ¥ã€‚*æœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—*ï¼Œ20:1â€“21ï¼Œ2019ã€‚
- en: 'Ergen and Pilanci (2020) T.Â Ergen and M.Â Pilanci. Convex geometry of two-layer
    relu networks: Implicit autoencoding and interpretable models. In S.Â Chiappa and
    R.Â Calandra, editors, *International Conference on Artificial Intelligence and
    Statistics*, volume 108 of *Proceedings of Machine Learning Research*, pages 4024â€“4033\.
    PMLR, 26â€“28 Aug 2020.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen and Pilanci (2020) T. Ergen å’Œ M. Pilanci. ä¸¤å±‚reluç½‘ç»œçš„å‡¸å‡ ä½•ï¼šéšå¼è‡ªç¼–ç å’Œå¯è§£é‡Šæ¨¡å‹ã€‚ç¼–è¾‘ï¼šS.
    Chiappa å’Œ R. Calandraï¼Œ*å›½é™…äººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ä¼šè®®*ï¼Œ*æœºå™¨å­¦ä¹ ç ”ç©¶è®ºæ–‡é›†*ç¬¬108å·ï¼Œç¬¬4024â€“4033é¡µã€‚PMLRï¼Œ2020å¹´8æœˆ26â€“28æ—¥ã€‚
- en: Ergen and Pilanci (2021a) T.Â Ergen and M.Â Pilanci. Convex geometry and duality
    of over-parameterized neural networks. *The Journal of Machine Learning Research*,
    22(1):9646â€“9708, 2021a.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen and Pilanci (2021a) T. Ergen å’Œ M. Pilanci. å‡¸å‡ ä½•å’Œè¶…å‚æ•°ç¥ç»ç½‘ç»œçš„å¯¹å¶æ€§ã€‚*æœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—*ï¼Œ22(1):9646â€“9708ï¼Œ2021aã€‚
- en: 'Ergen and Pilanci (2021b) T.Â Ergen and M.Â Pilanci. Global optimality beyond
    two layers: Training deep relu networks via convex programs. In *International
    Conference on Machine Learning (ICLR)*, pages 2993â€“3003\. PMLR, 2021b.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen and Pilanci (2021b) T. Ergen å’Œ M. Pilanci. è¶…è¶Šä¸¤å±‚çš„å…¨å±€æœ€ä¼˜æ€§ï¼šé€šè¿‡å‡¸ç¨‹åºè®­ç»ƒæ·±åº¦reluç½‘ç»œã€‚å‘è¡¨äº*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®
    (ICLR)*ï¼Œç¬¬2993â€“3003é¡µã€‚PMLRï¼Œ2021bã€‚
- en: 'Ergen and Pilanci (2021c) T.Â Ergen and M.Â Pilanci. Implicit convex regularizers
    of cnn architectures: Convex optimization of two-and three-layer networks in polynomial
    time. In *International Conference on Learning Representations (ICLR)*, 2021c.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen and Pilanci (2021c) T. Ergen å’Œ M. Pilanci. CNNæ¶æ„çš„éšå¼å‡¸æ­£åˆ™åŒ–ï¼šåœ¨å¤šé¡¹å¼æ—¶é—´å†…å¯¹ä¸¤å±‚å’Œä¸‰å±‚ç½‘ç»œçš„å‡¸ä¼˜åŒ–ã€‚å‘è¡¨äº*å›½é™…å­¦ä¹ è¡¨ç¤ºä¼šè®®
    (ICLR)*ï¼Œ2021cã€‚
- en: 'Ergen and Pilanci (2021d) T.Â Ergen and M.Â Pilanci. Path regularization: A convexity
    and sparsity inducing regularization for parallel relu networks. *arXiv preprint
    arXiv:2110.09548*, 2021d.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen and Pilanci (2021d) T. Ergen å’Œ M. Pilanci. è·¯å¾„æ­£åˆ™åŒ–ï¼šä¸€ç§ä¸ºå¹¶è¡Œreluç½‘ç»œå¼•å…¥å‡¸æ€§å’Œç¨€ç–æ€§çš„æ­£åˆ™åŒ–ã€‚*arXivé¢„å°æœ¬
    arXiv:2110.09548*ï¼Œ2021dã€‚
- en: Ergen and Pilanci (2021e) T.Â Ergen and M.Â Pilanci. Revealing the structure of
    deep neural networks via convex duality. In *International Conference on Machine
    Learning*, pages 3004â€“3014\. PMLR, 2021e.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen å’Œ Pilanci (2021e) T. Ergen å’Œ M. Pilanci. é€šè¿‡å‡¸å¯¹å¶æ€§æ­ç¤ºæ·±åº¦ç¥ç»ç½‘ç»œçš„ç»“æ„ã€‚å‘è¡¨äº *International
    Conference on Machine Learning*, é¡µç  3004â€“3014\. PMLR, 2021eã€‚
- en: 'Ergen etÂ al. (2022) T.Â Ergen, A.Â Sahiner, B.Â Ozturkler, J.Â M. Pauly, M.Â Mardani,
    and M.Â Pilanci. Demystifying batch normalization in reLU networks: Equivalent
    convex optimization models and implicit regularization. In *International Conference
    on Learning Representations*, 2022.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen ç­‰äºº (2022) T. Ergen, A. Sahiner, B. Ozturkler, J. M. Pauly, M. Mardani,
    å’Œ M. Pilanci. æ­å¼€ ReLU ç½‘ç»œä¸­çš„æ‰¹é‡å½’ä¸€åŒ–çš„ç¥ç§˜é¢çº±ï¼šç­‰æ•ˆçš„å‡¸ä¼˜åŒ–æ¨¡å‹å’Œéšå¼æ­£åˆ™åŒ–ã€‚å‘è¡¨äº *International Conference
    on Learning Representations*, 2022ã€‚
- en: Ergen etÂ al. (2023) T.Â Ergen, H.Â I. Gulluk, J.Â Lacotte, and M.Â Pilanci. Globally
    optimal training of neural networks with threshold activation functions. In *International
    Conference on Learning Representations (ICLR)*, 2023.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen ç­‰äºº (2023) T. Ergen, H. I. Gulluk, J. Lacotte, å’Œ M. Pilanci. å…¨å±€æœ€ä¼˜çš„ç¥ç»ç½‘ç»œè®­ç»ƒä¸é˜ˆå€¼æ¿€æ´»å‡½æ•°ã€‚å‘è¡¨äº
    *International Conference on Learning Representations (ICLR)*, 2023ã€‚
- en: Eykholt etÂ al. (2018) K.Â Eykholt, I.Â Evtimov, E.Â Fernandes, B.Â Li, A.Â Rahmati,
    C.Â Xiao, A.Â Prakash, T.Â Kohno, and D.Â Song. Robust physical-world attacks on deep
    learning visual classification. In *Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2018.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eykholt ç­‰äºº (2018) K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C.
    Xiao, A. Prakash, T. Kohno, å’Œ D. Song. å¯¹æ·±åº¦å­¦ä¹ è§†è§‰åˆ†ç±»çš„å¼ºå¥ç‰©ç†ä¸–ç•Œæ”»å‡»ã€‚å‘è¡¨äº *Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2018å¹´6æœˆã€‚
- en: Fan etÂ al. (2020) F.-L. Fan, R.Â Lai, and G.Â Wang. Quasi-equivalence of width
    and depth of neural networks. *arXiv:2002.02515*, 2020.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan ç­‰äºº (2020) F.-L. Fan, R. Lai, å’Œ G. Wang. ç¥ç»ç½‘ç»œçš„å®½åº¦å’Œæ·±åº¦çš„å‡†ç­‰ä»·æ€§ã€‚*arXiv:2002.02515*,
    2020ã€‚
- en: Fan etÂ al. (2023) F.-L. Fan, W.Â Huang, X.Â Zhong, L.Â Ruan, T.Â Zeng, H.Â Xiong,
    and F.Â Wang. Deep relu networks have surprisingly simple polytopes. *arXiv:2305.09145*,
    2023.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan ç­‰äºº (2023) F.-L. Fan, W. Huang, X. Zhong, L. Ruan, T. Zeng, H. Xiong, å’Œ F.
    Wang. æ·±åº¦ ReLU ç½‘ç»œå…·æœ‰æƒŠäººç®€å•çš„å¤šé¢ä½“ã€‚*arXiv:2305.09145*, 2023ã€‚
- en: Fazlyab etÂ al. (2019) M.Â Fazlyab, A.Â Robey, H.Â Hassani, M.Â Morari, and G.Â J.
    Pappas. Efficient and accurate estimation of Lipschitz constants for deep neural
    networks. In *Neural Information Processing Systems (NeurIPS)*, volumeÂ 32, 2019.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fazlyab ç­‰äºº (2019) M. Fazlyab, A. Robey, H. Hassani, M. Morari, å’Œ G. J. Pappas.
    æ·±åº¦ç¥ç»ç½‘ç»œçš„ Lipschitz å¸¸æ•°çš„é«˜æ•ˆä¸”å‡†ç¡®çš„ä¼°è®¡ã€‚å‘è¡¨äº *Neural Information Processing Systems (NeurIPS)*,
    ç¬¬32å·, 2019ã€‚
- en: Fazlyab etÂ al. (2020) M.Â Fazlyab, M.Â Morari, and G.Â J. Pappas. Safety verification
    and robustness analysis of neural networks via quadratic constraints and semidefinite
    programming. *IEEE Transactions on Automatic Control*, 67(1):1â€“15, 2020.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fazlyab ç­‰äºº (2020) M. Fazlyab, M. Morari, å’Œ G. J. Pappas. é€šè¿‡äºŒæ¬¡çº¦æŸå’ŒåŠæ­£å®šè§„åˆ’è¿›è¡Œç¥ç»ç½‘ç»œçš„å®‰å…¨æ€§éªŒè¯å’Œé²æ£’æ€§åˆ†æã€‚*IEEE
    Transactions on Automatic Control*, 67(1):1â€“15, 2020ã€‚
- en: 'Ferlez and Shoukry (2020) J.Â Ferlez and Y.Â Shoukry. AReN: Assured ReLU NN architecture
    for model predictive control of LTI systems. In *HSCC*, 2020.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferlez å’Œ Shoukry (2020) J. Ferlez å’Œ Y. Shoukry. AReNï¼šç”¨äº LTI ç³»ç»Ÿæ¨¡å‹é¢„æµ‹æ§åˆ¶çš„å¯é  ReLU
    NN æ¶æ„ã€‚å‘è¡¨äº *HSCC*, 2020ã€‚
- en: Ferrari etÂ al. (2022) C.Â Ferrari, M.Â N. Mueller, N.Â JovanoviÄ‡, and M.Â Vechev.
    Complete verification via multi-neuron relaxation guided branch-and-bound. In
    *International Conference on Learning Representations (ICLR)*, 2022.
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrari ç­‰äºº (2022) C. Ferrari, M. N. Mueller, N. JovanoviÄ‡, å’Œ M. Vechev. é€šè¿‡å¤šç¥ç»å…ƒæ”¾æ¾å¼•å¯¼çš„åˆ†æ”¯ç•Œå®šå®ç°å®Œæ•´éªŒè¯ã€‚å‘è¡¨äº
    *International Conference on Learning Representations (ICLR)*, 2022ã€‚
- en: Finlayson etÂ al. (2019) S.Â G. Finlayson, J.Â D. Bowers, J.Â Ito, J.Â L. Zittrain,
    A.Â L. Beam, and I.Â S. Kohane. Adversarial attacks on medical machine learning.
    *Science*, 363(6433):1287â€“1289, 2019.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finlayson ç­‰äºº (2019) S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A.
    L. Beam, å’Œ I. S. Kohane. å¯¹åŒ»å­¦æœºå™¨å­¦ä¹ çš„å¯¹æŠ—æ”»å‡»ã€‚*Science*, 363(6433):1287â€“1289, 2019ã€‚
- en: Fischetti and Jo (2018) M.Â Fischetti and J.Â Jo. Deep neural networks and mixed
    integer linear optimization. *Constraints*, 2018.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischetti å’Œ Jo (2018) M. Fischetti å’Œ J. Jo. æ·±åº¦ç¥ç»ç½‘ç»œå’Œæ··åˆæ•´æ•°çº¿æ€§ä¼˜åŒ–ã€‚*Constraints*, 2018ã€‚
- en: Fourier (1826) J.Â Fourier. Solution dâ€™une question particuliÃ©re du calcul des
    inÃ©galitÃ©s. *Nouveau Bulletin des Sciences par la SociÃ©tÃ© Philomatique de Paris*,
    1826.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fourier (1826) J. Fourier. ç‰¹å®šä¸ç­‰å¼è®¡ç®—é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚*Nouveau Bulletin des Sciences par
    la SociÃ©tÃ© Philomatique de Paris*, 1826ã€‚
- en: Frank etÂ al. (1956) M.Â Frank, P.Â Wolfe, etÂ al. An algorithm for quadratic programming.
    *Naval Research Logistics Quarterly*, 3(1-2):95â€“110, 1956.
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frank ç­‰äºº (1956) M. Frank, P. Wolfe, ç­‰. äºŒæ¬¡è§„åˆ’ç®—æ³•ã€‚*Naval Research Logistics Quarterly*,
    3(1-2):95â€“110, 1956ã€‚
- en: Froese and Hertrich (2023) V.Â Froese and C.Â Hertrich. Training neural networks
    is NP-hard in fixed dimension. *arXiv preprint arXiv:2303.17045*, 2023.
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Froese å’Œ Hertrich (2023) V. Froese å’Œ C. Hertrich. åœ¨å›ºå®šç»´åº¦ä¸‹è®­ç»ƒç¥ç»ç½‘ç»œæ˜¯ NP éš¾çš„ã€‚*arXiv
    preprint arXiv:2303.17045*, 2023ã€‚
- en: Froese etÂ al. (2022) V.Â Froese, C.Â Hertrich, and R.Â Niedermeier. The computational
    complexity of relu network training parameterized by data dimensionality. *Journal
    of Artificial Intelligence Research*, 74:1775â€“1790, 2022.
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Froese ç­‰ï¼ˆ2022ï¼‰V. Froeseã€C. Hertrich å’Œ R. Niedermeierã€‚ç”±æ•°æ®ç»´åº¦å‚æ•°åŒ–çš„reluç½‘ç»œè®­ç»ƒçš„è®¡ç®—å¤æ‚åº¦ã€‚*äººå·¥æ™ºèƒ½ç ”ç©¶æ‚å¿—*ï¼Œ74ï¼š1775â€“1790ï¼Œ2022å¹´ã€‚
- en: 'Fukushima (1980) K.Â Fukushima. Neocognitron: A self-organizing neural network
    model for a mechanism of pattern recognition unaffected by shift in position.
    *Biological Cybernetics*, 36(4):193â€“202, 1980.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fukushimaï¼ˆ1980ï¼‰K. Fukushimaã€‚Neocognitronï¼šä¸€ç§è‡ªç»„ç»‡ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºä¸å—ä½ç½®åç§»å½±å“çš„æ¨¡å¼è¯†åˆ«æœºåˆ¶ã€‚*ç”Ÿç‰©è®¡ç®—å­¦*ï¼Œ36(4)ï¼š193â€“202ï¼Œ1980å¹´ã€‚
- en: Funahashi (1989) K.-I. Funahashi. On the approximate realization of continuous
    mappings by neural networks. *Neural Networks*, 2(3), 1989.
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Funahashiï¼ˆ1989ï¼‰K.-I. Funahashiã€‚å…³äºç¥ç»ç½‘ç»œå¯¹è¿ç»­æ˜ å°„çš„è¿‘ä¼¼å®ç°ã€‚*ç¥ç»ç½‘ç»œ*ï¼Œ2(3)ï¼Œ1989å¹´ã€‚
- en: Gamba etÂ al. (2020) M.Â Gamba, S.Â Carlsson, H.Â Azizpour, and M.Â BjÃ¶rkman. Hyperplane
    arrangements of trained ConvNets are biased. *arXiv:2003.07797*, 2020.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamba ç­‰ï¼ˆ2020ï¼‰M. Gambaã€S. Carlssonã€H. Azizpour å’Œ M. BjÃ¶rkmanã€‚è®­ç»ƒçš„ConvNetsçš„è¶…å¹³é¢æ’åˆ—å­˜åœ¨åå·®ã€‚*arXiv:2003.07797*ï¼Œ2020å¹´ã€‚
- en: Gamba etÂ al. (2022) M.Â Gamba, A.Â Chmielewski-Anders, J.Â Sullivan, H.Â Azizpour,
    and M.Â BjÃ¶rkman. Are all linear regions created equal? In *International Conference
    on Artificial Intelligence and Statistics (AISTATS)*, 2022.
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamba ç­‰ï¼ˆ2022ï¼‰M. Gambaã€A. Chmielewski-Andersã€J. Sullivanã€H. Azizpour å’Œ M. BjÃ¶rkmanã€‚æ‰€æœ‰çº¿æ€§åŒºåŸŸæ˜¯å¦éƒ½ç›¸åŒï¼Ÿåœ¨*å›½é™…äººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ä¼šè®®ï¼ˆAISTATSï¼‰*ï¼Œ2022å¹´ã€‚
- en: 'Gambella etÂ al. (2021) C.Â Gambella, B.Â Ghaddar, and J.Â Naoum-Sawaya. Optimization
    problems for machine learning: A survey. *European Journal of Operational Research*,
    290(3):807â€“828, 2021.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gambella ç­‰ï¼ˆ2021ï¼‰C. Gambellaã€B. Ghaddar å’Œ J. Naoum-Sawayaã€‚æœºå™¨å­¦ä¹ ä¸­çš„ä¼˜åŒ–é—®é¢˜ï¼šç»¼è¿°ã€‚*æ¬§æ´²è¿ç­¹å­¦æ‚å¿—*ï¼Œ290(3)ï¼š807â€“828ï¼Œ2021å¹´ã€‚
- en: 'Gao etÂ al. (2020) J.Â Gao, C.Â Sun, H.Â Zhao, Y.Â Shen, D.Â Anguelov, C.Â Li, and
    C.Â Schmid. VectorNet: Encoding HD maps and agent dynamics from vectorized representation.
    In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao ç­‰ï¼ˆ2020ï¼‰J. Gaoã€C. Sunã€H. Zhaoã€Y. Shenã€D. Anguelovã€C. Li å’Œ C. Schmidã€‚VectorNetï¼šä»å‘é‡åŒ–è¡¨ç¤ºä¸­ç¼–ç HDåœ°å›¾å’Œä»£ç†åŠ¨æ€ã€‚åœ¨*è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰*ï¼Œ2020å¹´ã€‚
- en: GeiÃŸler etÂ al. (2012) B.Â GeiÃŸler, A.Â Martin, A.Â Morsi, and L.Â Schewe. Using
    piecewise linear functions for solving MINLPs. In *Mixed Integer Nonlinear Programming*,
    pages 287â€“314. Springer, 2012.
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GeiÃŸler ç­‰ï¼ˆ2012ï¼‰B. GeiÃŸlerã€A. Martinã€A. Morsi å’Œ L. Scheweã€‚ä½¿ç”¨åˆ†æ®µçº¿æ€§å‡½æ•°è§£å†³MINLPsã€‚åœ¨*æ··åˆæ•´æ•°éçº¿æ€§è§„åˆ’*ï¼Œç¬¬287â€“314é¡µã€‚Springerï¼Œ2012å¹´ã€‚
- en: Glass etÂ al. (2021) L.Â Glass, W.Â Hilali, and O.Â Nelles. Compressing interpretable
    representations of piecewise linear neural networks using neuro-fuzzy models.
    In *IEEE Symposium Series on Computational Intelligence (SSCI)*, 2021.
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glass ç­‰ï¼ˆ2021ï¼‰L. Glassã€W. Hilali å’Œ O. Nellesã€‚åˆ©ç”¨ç¥ç»æ¨¡ç³Šæ¨¡å‹å‹ç¼©åˆ†æ®µçº¿æ€§ç¥ç»ç½‘ç»œçš„å¯è§£é‡Šè¡¨ç¤ºã€‚åœ¨*IEEEè®¡ç®—æ™ºèƒ½ç ”è®¨ä¼šç³»åˆ—ï¼ˆSSCIï¼‰*ï¼Œ2021å¹´ã€‚
- en: Glorot etÂ al. (2011) X.Â Glorot, A.Â Bordes, and Y.Â Bengio. Deep sparse rectifier
    neural networks. In *International Conference on Artificial Intelligence and Statistics
    (AISTATS)*, 2011.
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glorot ç­‰ï¼ˆ2011ï¼‰X. Glorotã€A. Bordes å’Œ Y. Bengioã€‚æ·±åº¦ç¨€ç–ä¿®æ­£ç¥ç»ç½‘ç»œã€‚åœ¨*å›½é™…äººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ä¼šè®®ï¼ˆAISTATSï¼‰*ï¼Œ2011å¹´ã€‚
- en: Goebbels (2021) S.Â Goebbels. Training of ReLU activated multilayerd neural networks
    with mixed integer linear programs. Technical report, Hochschule Niederrhein,
    Fachbereich Elektrotechnik & Informatik, 2021.
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goebbelsï¼ˆ2021ï¼‰S. Goebbelsã€‚ä½¿ç”¨æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’è®­ç»ƒReLUæ¿€æ´»çš„å¤šå±‚ç¥ç»ç½‘ç»œã€‚æŠ€æœ¯æŠ¥å‘Šï¼Œéœèµ«æ–½åº“å°”Â·å°¼å¾·æ‹‰å› ï¼Œç”µæ°”å·¥ç¨‹ä¸è®¡ç®—æœºç§‘å­¦ç³»ï¼Œ2021å¹´ã€‚
- en: Goel etÂ al. (2021) S.Â Goel, A.Â Klivans, P.Â Manurangsi, and D.Â Reichman. Tight
    hardness results for training depth-2 relu networks. In *12th Innovations in Theoretical
    Computer Science Conference (ITCS 2021)*. Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r
    Informatik, 2021.
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goel ç­‰ï¼ˆ2021ï¼‰S. Goelã€A. Klivansã€P. Manurangsi å’Œ D. Reichmanã€‚è®­ç»ƒæ·±åº¦-2 reluç½‘ç»œçš„ä¸¥æ ¼éš¾åº¦ç»“æœã€‚åœ¨*ç¬¬12å±Šç†è®ºè®¡ç®—æœºç§‘å­¦åˆ›æ–°ä¼šè®®ï¼ˆITCS
    2021ï¼‰*ã€‚Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatikï¼Œ2021å¹´ã€‚
- en: Goerigk and Kurtz (2023) M.Â Goerigk and J.Â Kurtz. Data-driven robust optimization
    using deep neural networks. *Computers & Operations Research*, 151:106087, 2023.
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goerigk å’Œ Kurtzï¼ˆ2023ï¼‰M. Goerigk å’Œ J. Kurtzã€‚ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œæ•°æ®é©±åŠ¨çš„é²æ£’ä¼˜åŒ–ã€‚*è®¡ç®—æœºä¸è¿ç­¹ç ”ç©¶*ï¼Œ151ï¼š106087ï¼Œ2023å¹´ã€‚
- en: Goodfellow etÂ al. (2013) I.Â Goodfellow, D.Â Warde-Farley, M.Â Mirza, A.Â Courville,
    and Y.Â Bengio. Maxout networks. In *International Conference on Machine Learning
    (ICML)*, 2013.
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow ç­‰ï¼ˆ2013ï¼‰I. Goodfellowã€D. Warde-Farleyã€M. Mirzaã€A. Courville å’Œ Y. Bengioã€‚Maxout
    ç½‘ç»œã€‚åœ¨*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œ2013å¹´ã€‚
- en: Goodfellow etÂ al. (2015) I.Â Goodfellow, J.Â Shlens, and C.Â Szegedy. Explaining
    and harnessing adversarial examples. In *International Conference on Learning
    Representations (ICLR)*, 2015.
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow ç­‰ï¼ˆ2015ï¼‰I. Goodfellowã€J. Shlens å’Œ C. Szegedyã€‚è§£é‡Šå’Œåˆ©ç”¨å¯¹æŠ—æ ·æœ¬ã€‚åœ¨*å›½é™…å­¦ä¹ è¡¨ç¤ºä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2015å¹´ã€‚
- en: Goodfellow etÂ al. (2016) I.Â Goodfellow, Y.Â Bengio, and A.Â Courville. *Deep learning*.
    MIT press, 2016.
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow ç­‰ (2016) I. Goodfellow, Y. Bengio, å’Œ A. Courville. *æ·±åº¦å­¦ä¹ *ã€‚MIT å‡ºç‰ˆç¤¾ï¼Œ2016ã€‚
- en: Goodfellow etÂ al. (2014) I.Â J. Goodfellow, J.Â Pouget-Abadie, M.Â Mirza, B.Â Xu,
    D.Â Warde-Farley, S.Â Ozair, A.Â Courville, and Y.Â Bengio. Generative adversarial
    nets. In *Neural Information Processing Systems (NeurIPS)*, volumeÂ 27, 2014.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow ç­‰ (2014) I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.
    Warde-Farley, S. Ozair, A. Courville, å’Œ Y. Bengio. ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚å‘è¡¨äº *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿ (NeurIPS)*ï¼Œç¬¬27å·ï¼Œ2014ã€‚
- en: Gopinath etÂ al. (2019) D.Â Gopinath, H.Â Converse, C.Â S. Pasareanu, and A.Â Taly.
    Property inference for deep neural networks. In *IEEE/ACM International Conference
    on Automated Software Engineering (ASE)*, 2019.
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gopinath ç­‰ (2019) D. Gopinath, H. Converse, C. S. Pasareanu, å’Œ A. Taly. æ·±åº¦ç¥ç»ç½‘ç»œçš„å±æ€§æ¨æ–­ã€‚å‘è¡¨äº
    *IEEE/ACM è‡ªåŠ¨åŒ–è½¯ä»¶å·¥ç¨‹å›½é™…ä¼šè®® (ASE)*ï¼Œ2019ã€‚
- en: Goujon etÂ al. (2022) A.Â Goujon, A.Â Etemadi, and M.Â Unser. The role of depth,
    width, and activation complexity in the number of linear regions of neural networks.
    *arXiv:2206.08615*, 2022.
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goujon ç­‰ (2022) A. Goujon, A. Etemadi, å’Œ M. Unser. æ·±åº¦ã€å®½åº¦å’Œæ¿€æ´»å¤æ‚æ€§åœ¨ç¥ç»ç½‘ç»œçº¿æ€§åŒºåŸŸæ•°é‡ä¸­çš„ä½œç”¨ã€‚*arXiv:2206.08615*ï¼Œ2022ã€‚
- en: Gowal etÂ al. (2018) S.Â Gowal, K.Â Dvijotham, R.Â Stanforth, R.Â Bunel, C.Â Qin,
    J.Â Uesato, R.Â Arandjelovic, T.Â Mann, and P.Â Kohli. On the effectiveness of interval
    bound propagation for training verifiably robust models. *arXiv:1810.12715*, 2018.
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gowal ç­‰ (2018) S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato,
    R. Arandjelovic, T. Mann, å’Œ P. Kohli. å…³äºåŒºé—´è¾¹ç•Œä¼ æ’­åœ¨è®­ç»ƒå¯éªŒè¯é²æ£’æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚*arXiv:1810.12715*ï¼Œ2018ã€‚
- en: Graves and Jaitly (2014) A.Â Graves and N.Â Jaitly. Towards end-to-end speech
    recognition with recurrent neural networks. In *International Conference on Machine
    Learning (ICML)*, 2014.
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves å’Œ Jaitly (2014) A. Graves å’Œ N. Jaitly. é€šè¿‡é€’å½’ç¥ç»ç½‘ç»œå®ç°ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«ã€‚å‘è¡¨äº *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®
    (ICML)*ï¼Œ2014ã€‚
- en: Grigsby and Lindsey (2022) J.Â E. Grigsby and K.Â Lindsey. On transversality of
    bent hyperplane arrangements and the topological expressiveness of ReLU neural
    networks. *SIAM Journal on Applied Algebra and Geometry*, 6(2), 2022.
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grigsby å’Œ Lindsey (2022) J. E. Grigsby å’Œ K. Lindsey. å…³äºå¼¯æ›²è¶…å¹³é¢æ’åˆ—çš„æ¨ªæˆªæ€§ä»¥åŠ ReLU ç¥ç»ç½‘ç»œçš„æ‹“æ‰‘è¡¨è¾¾èƒ½åŠ›ã€‚*SIAM
    åº”ç”¨ä»£æ•°ä¸å‡ ä½•æœŸåˆŠ*ï¼Œ6(2)ï¼Œ2022ã€‚
- en: Grigsby etÂ al. (2023) J.Â E. Grigsby, K.Â Lindsey, and D.Â Rolnick. Hidden symmetries
    of ReLU networks. In *International Conference on Machine Learning (ICML)*, 2023.
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grigsby ç­‰ (2023) J. E. Grigsby, K. Lindsey, å’Œ D. Rolnick. ReLU ç½‘ç»œçš„éšè—å¯¹ç§°æ€§ã€‚å‘è¡¨äº
    *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®® (ICML)*ï¼Œ2023ã€‚
- en: Grimstad and Andersson (2019) B.Â Grimstad and H.Â Andersson. ReLU networks as
    surrogate models in mixed-integer linear programs. *Computers & Chemical Engineering*,
    131:106580, 2019.
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grimstad å’Œ Andersson (2019) B. Grimstad å’Œ H. Andersson. ReLU ç½‘ç»œä½œä¸ºæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ä¸­çš„ä»£ç†æ¨¡å‹ã€‚*è®¡ç®—æœºä¸åŒ–å­¦å·¥ç¨‹*ï¼Œ131:106580ï¼Œ2019ã€‚
- en: 'Grossmann and Ruiz (2012) I.Â E. Grossmann and J.Â P. Ruiz. Generalized disjunctive
    programming: A framework for formulation and alternative algorithms for MINLP
    optimization. In *Mixed Integer Nonlinear Programming*, pages 93â€“115, New York,
    NY, 2012\. Springer New York.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grossmann å’Œ Ruiz (2012) I. E. Grossmann å’Œ J. P. Ruiz. å¹¿ä¹‰æå–ç¼–ç¨‹ï¼šMINLP ä¼˜åŒ–çš„æ¡†æ¶åŠæ›¿ä»£ç®—æ³•ã€‚å‘è¡¨äº
    *æ··åˆæ•´æ•°éçº¿æ€§ç¼–ç¨‹*ï¼Œç¬¬93-115é¡µï¼Œçº½çº¦ï¼ŒNYï¼Œ2012ã€‚Springer New Yorkã€‚
- en: Hahnloser etÂ al. (2000) R.Â Hahnloser, R.Â Sarpeshkar, M.Â Mahowald, R.Â Douglas,
    and S.Â Seung. Digital selection and analogue amplification coexist in a cortex-inspired
    silicon circuit. *Nature*, 405, 2000.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hahnloser ç­‰ (2000) R. Hahnloser, R. Sarpeshkar, M. Mahowald, R. Douglas, å’Œ S.
    Seung. æ•°å­—é€‰æ‹©ä¸æ¨¡æ‹Ÿæ”¾å¤§åœ¨ç±»ä¼¼çš®å±‚çš„ç¡…ç”µè·¯ä¸­å…±å­˜ã€‚*è‡ªç„¶*ï¼Œ405ï¼Œ2000ã€‚
- en: Han and GÃ³mez (2021) S.Â Han and A.Â GÃ³mez. Single-neuron convexification for
    binarized neural networks, 2021. URL https://optimization-online.org/?p=17148.
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han å’Œ GÃ³mez (2021) S. Han å’Œ A. GÃ³mez. é’ˆå¯¹äºŒå€¼ç¥ç»ç½‘ç»œçš„å•ç¥ç»å…ƒå‡¸åŒ–ï¼Œ2021ã€‚ç½‘å€ https://optimization-online.org/?p=17148ã€‚
- en: Hanin and Rolnick (2019a) B.Â Hanin and D.Â Rolnick. Complexity of linear regions
    in deep networks. In *International Conference on Machine Learning (ICML)*, 2019a.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanin å’Œ Rolnick (2019a) B. Hanin å’Œ D. Rolnick. æ·±åº¦ç½‘ç»œä¸­çº¿æ€§åŒºåŸŸçš„å¤æ‚æ€§ã€‚å‘è¡¨äº *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®® (ICML)*ï¼Œ2019aã€‚
- en: Hanin and Rolnick (2019b) B.Â Hanin and D.Â Rolnick. Deep ReLU networks have surprisingly
    few activation patterns. In *Neural Information Processing Systems (NeurIPS)*,
    volumeÂ 32, 2019b.
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanin å’Œ Rolnick (2019b) B. Hanin å’Œ D. Rolnick. æ·±åº¦ ReLU ç½‘ç»œå…·æœ‰ä»¤äººæƒŠè®¶çš„å°‘é‡æ¿€æ´»æ¨¡å¼ã€‚å‘è¡¨äº *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿ
    (NeurIPS)*ï¼Œç¬¬32å·ï¼Œ2019bã€‚
- en: Hanin and Sellke (2017) B.Â Hanin and M.Â Sellke. Approximating continuous functions
    by ReLU nets of minimal width. *arXiv:1710.11278*, 2017.
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanin å’Œ Sellke (2017) B. Hanin å’Œ M. Sellke. é€šè¿‡ ReLU ç½‘ç»œçš„æœ€å°å®½åº¦é€¼è¿‘è¿ç»­å‡½æ•°ã€‚*arXiv:1710.11278*ï¼Œ2017ã€‚
- en: 'Hashemi etÂ al. (2021) V.Â Hashemi, P.Â Kouvaros, and A.Â Lomuscio. OSIP: Tightened
    bound propagation for the verification of ReLU neural networks. In *International
    Conference on Software Engineering and Formal Methods (SEFM)*, pages 463â€“480\.
    Springer, 2021.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hashemi ç­‰ï¼ˆ2021ï¼‰V. Hashemi, P. Kouvaros, å’Œ A. Lomuscioã€‚OSIPï¼šç”¨äº ReLU ç¥ç»ç½‘ç»œéªŒè¯çš„æ”¶ç´§ç•Œé™ä¼ æ’­ã€‚è§äº
    *å›½é™…è½¯ä»¶å·¥ç¨‹ä¸å½¢å¼æ–¹æ³•ä¼šè®® (SEFM)*ï¼Œç¬¬463â€“480é¡µã€‚æ–½æ™®æ—æ ¼ï¼Œ2021å¹´ã€‚
- en: 'He etÂ al. (2021) F.Â He, S.Â Lei, J.Â Ji, and D.Â Tao. Neural networks behave as
    hash encoders: An empirical study. *arXiv:2101.05490*, 2021.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He ç­‰ï¼ˆ2021ï¼‰F. He, S. Lei, J. Ji, å’Œ D. Taoã€‚ç¥ç»ç½‘ç»œè¡¨ç°ä¸ºå“ˆå¸Œç¼–ç å™¨ï¼šä¸€é¡¹å®è¯ç ”ç©¶ã€‚*arXiv:2101.05490*ï¼Œ2021å¹´ã€‚
- en: He etÂ al. (2020) J.Â He, L.Â Li, J.Â Xu, and C.Â Zheng. ReLU deep neural networks
    and linear finite elements. *Journal of Computational Mathematics*, 38:502â€“527,
    2020.
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He ç­‰ï¼ˆ2020ï¼‰J. He, L. Li, J. Xu, å’Œ C. Zhengã€‚ReLU æ·±åº¦ç¥ç»ç½‘ç»œä¸çº¿æ€§æœ‰é™å…ƒã€‚*è®¡ç®—æ•°å­¦æ‚å¿—*ï¼Œ38:502â€“527ï¼Œ2020å¹´ã€‚
- en: 'He etÂ al. (2015) K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun. Delving deep into rectifiers:
    Surpassing human-level performance on ImageNet classification. In *IEEE International
    Conference on Computer Vision (ICCV)*, 2015.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He ç­‰ï¼ˆ2015ï¼‰K. He, X. Zhang, S. Ren, å’Œ J. Sunã€‚æ·±å…¥æ¢è®¨ä¿®æ­£å™¨ï¼šåœ¨ ImageNet åˆ†ç±»ä¸Šè¶…è¶Šäººç±»æ°´å¹³çš„æ€§èƒ½ã€‚è§äº
    *IEEE å›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®® (ICCV)*ï¼Œ2015å¹´ã€‚
- en: He etÂ al. (2016) K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun. Deep residual learning
    for image recognition. In *Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2016.
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He ç­‰ï¼ˆ2016ï¼‰K. He, X. Zhang, S. Ren, å’Œ J. Sunã€‚ç”¨äºå›¾åƒè¯†åˆ«çš„æ·±åº¦æ®‹å·®å­¦ä¹ ã€‚è§äº *è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®® (CVPR)*ï¼Œ2016å¹´ã€‚
- en: 'Henriksen and Lomuscio (2021) P.Â Henriksen and A.Â Lomuscio. DEEPSPLIT: an efficient
    splitting method for neural network verification via indirect effect analysis.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, pages
    2549â€“2555, 2021.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henriksen å’Œ Lomuscioï¼ˆ2021ï¼‰P. Henriksen å’Œ A. Lomuscioã€‚DEEPSPLITï¼šä¸€ç§é€šè¿‡é—´æ¥æ•ˆåº”åˆ†æè¿›è¡Œç¥ç»ç½‘ç»œéªŒè¯çš„é«˜æ•ˆæ‹†åˆ†æ–¹æ³•ã€‚è§äº
    *å›½é™…äººå·¥æ™ºèƒ½è”åˆä¼šè®® (IJCAI)*ï¼Œç¬¬2549â€“2555é¡µï¼Œ2021å¹´ã€‚
- en: Henriksen etÂ al. (2022) P.Â Henriksen, F.Â Leofante, and A.Â Lomuscio. Repairing
    misclassifications in neural networks using limited data. In *ACM/SIGAPP Symposium
    On Applied Computing (SAC)*, 2022.
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henriksen ç­‰ï¼ˆ2022ï¼‰P. Henriksen, F. Leofante, å’Œ A. Lomuscioã€‚ä½¿ç”¨æœ‰é™æ•°æ®ä¿®æ­£ç¥ç»ç½‘ç»œä¸­çš„é”™è¯¯åˆ†ç±»ã€‚è§äº
    *ACM/SIGAPP åº”ç”¨è®¡ç®—ç ”è®¨ä¼š (SAC)*ï¼Œ2022å¹´ã€‚
- en: Hertrich etÂ al. (2021) C.Â Hertrich, A.Â Basu, M.Â D. Summa, and M.Â Skutella. Towards
    lower bounds on the depth of ReLU neural networks. In *Neural Information Processing
    Systems (NeurIPS)*, 2021.
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hertrich ç­‰ï¼ˆ2021ï¼‰C. Hertrich, A. Basu, M. D. Summa, å’Œ M. Skutellaã€‚æœç€ ReLU ç¥ç»ç½‘ç»œæ·±åº¦ä¸‹ç•Œçš„æ–¹å‘å‰è¿›ã€‚è§äº
    *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿ (NeurIPS)*ï¼Œ2021å¹´ã€‚
- en: Hinton etÂ al. (2012) G.Â Hinton, L.Â Deng, G.Â Dahl, A.Â Mohamed, N.Â Jaitly, A.Â Senior,
    V.Â Vanhoucke, P.Â Nguyen, T.Â Sainath, and B.Â Kingsbury. Deep neural networks for
    acoustic modeling in speech recognition. *IEEE Signal Processing Magazine*, 2012.
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton ç­‰ï¼ˆ2012ï¼‰G. Hinton, L. Deng, G. Dahl, A. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. Sainath, å’Œ B. Kingsburyã€‚ç”¨äºè¯­éŸ³è¯†åˆ«çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚*IEEE ä¿¡å·å¤„ç†æ‚å¿—*ï¼Œ2012å¹´ã€‚
- en: Hinz (2021) P.Â Hinz. Using activation histograms to bound the number of affine
    regions in ReLU feed-forward neural networks. *arXiv:2103.17174*, 2021.
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinzï¼ˆ2021ï¼‰P. Hinzã€‚ä½¿ç”¨æ¿€æ´»ç›´æ–¹å›¾æ¥é™åˆ¶ ReLU å‰é¦ˆç¥ç»ç½‘ç»œä¸­çš„ä»¿å°„åŒºåŸŸæ•°é‡ã€‚*arXiv:2103.17174*ï¼Œ2021å¹´ã€‚
- en: Hinz and vanÂ de Geer (2019) P.Â Hinz and S.Â vanÂ de Geer. A framework for the
    construction of upper bounds on the number of affine linear regions of ReLU feed-forward
    neural networks. *IEEE Transactions on Information Theory*, 65(11):7304â€“7324,
    2019.
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinz å’Œ van de Geerï¼ˆ2019ï¼‰P. Hinz å’Œ S. van de Geerã€‚æ„å»º ReLU å‰é¦ˆç¥ç»ç½‘ç»œçš„ä»¿å°„çº¿æ€§åŒºåŸŸæ•°é‡ä¸Šç•Œçš„æ¡†æ¶ã€‚*IEEE
    ä¿¡æ¯ç†è®ºæ±‡åˆŠ*ï¼Œ65(11):7304â€“7324ï¼Œ2019å¹´ã€‚
- en: Hochreiter and Schmidhuber (1997) S.Â Hochreiter and J.Â Schmidhuber. Long short-term
    memory. *Neural Computation*, 9(8):1735â€“1780, 1997.
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter å’Œ Schmidhuberï¼ˆ1997ï¼‰S. Hochreiter å’Œ J. Schmidhuberã€‚é•¿çŸ­æœŸè®°å¿†ã€‚*ç¥ç»è®¡ç®—*ï¼Œ9(8):1735â€“1780ï¼Œ1997å¹´ã€‚
- en: Hopfield (1982) J.Â Hopfield. Neural networks and physical systems with emergent
    collective computational abilities. *Proceedings of the National Academy of Sciences*,
    79:2554â€“2558, 1982.
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopfieldï¼ˆ1982ï¼‰J. Hopfieldã€‚å…·æœ‰æ–°å…´é›†ä½“è®¡ç®—èƒ½åŠ›çš„ç¥ç»ç½‘ç»œå’Œç‰©ç†ç³»ç»Ÿã€‚*å›½å®¶ç§‘å­¦é™¢å­¦æŠ¥*ï¼Œ79:2554â€“2558ï¼Œ1982å¹´ã€‚
- en: Hornik etÂ al. (1989) K.Â Hornik, M.Â Stinchcombe, and H.Â White. Multilayer feedforward
    networks are universal approximators. *Neural Networks*, 2(5), 1989.
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hornik ç­‰ï¼ˆ1989ï¼‰K. Hornik, M. Stinchcombe, å’Œ H. Whiteã€‚å¤šå±‚å‰é¦ˆç½‘ç»œæ˜¯é€šç”¨é€¼è¿‘å™¨ã€‚*ç¥ç»ç½‘ç»œ*ï¼Œ2(5)ï¼Œ1989å¹´ã€‚
- en: Hu etÂ al. (2020a) T.Â Hu, Z.Â Shang, and G.Â Cheng. Sharp rate of convergence for
    deep neural network classifiers under the teacher-student setting. *arXiv:2001.06892*,
    2020a.
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu ç­‰ï¼ˆ2020aï¼‰T. Hu, Z. Shang, å’Œ G. Chengã€‚åœ¨æ•™å¸ˆ-å­¦ç”Ÿè®¾ç½®ä¸‹æ·±åº¦ç¥ç»ç½‘ç»œåˆ†ç±»å™¨çš„æ”¶æ•›é€Ÿåº¦ã€‚*arXiv:2001.06892*ï¼Œ2020å¹´ã€‚
- en: Hu etÂ al. (2020b) X.Â Hu, W.Â Liu, J.Â Bian, and J.Â Pei. Measuring model complexity
    of neural networks with curve activation functions. In *ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining (KDD)*, 2020b.
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu ç­‰ (2020b) X. Hu, W. Liu, J. Bian å’Œ J. Pei. ç”¨æ›²çº¿æ¿€æ´»å‡½æ•°æµ‹é‡ç¥ç»ç½‘ç»œçš„æ¨¡å‹å¤æ‚æ€§ã€‚å‘è¡¨äº *ACM SIGKDD
    çŸ¥è¯†å‘ç°ä¸æ•°æ®æŒ–æ˜ä¼šè®®ï¼ˆKDDï¼‰*ï¼Œ2020bå¹´ã€‚
- en: 'Hu etÂ al. (2021) X.Â Hu, L.Â Chu, J.Â Pei, W.Â Liu, and J.Â Bian. Model complexity
    of deep learning: a survey. *Knowledge and Information Systems*, 63:2585â€“2619,
    2021.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu ç­‰ (2021) X. Hu, L. Chu, J. Pei, W. Liu å’Œ J. Bian. æ·±åº¦å­¦ä¹ çš„æ¨¡å‹å¤æ‚æ€§ï¼šç»¼è¿°ã€‚*çŸ¥è¯†ä¸ä¿¡æ¯ç³»ç»Ÿ*ï¼Œ63:2585â€“2619ï¼Œ2021å¹´ã€‚
- en: 'Huang etÂ al. (2020) X.Â Huang, D.Â Kroening, W.Â Ruan, J.Â Sharp, Y.Â Sun, E.Â Thamo,
    M.Â Wu, and X.Â Yi. A survey of safety and trustworthiness of deep neural networks:
    Verification, testing, adversarial attack and defence, and interpretability. *Computer
    Science Review*, 37:100270, 2020.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang ç­‰ (2020) X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo, M.
    Wu å’Œ X. Yi. æ·±åº¦ç¥ç»ç½‘ç»œçš„å®‰å…¨æ€§å’Œå¯ä¿¡åº¦ç»¼è¿°ï¼šéªŒè¯ã€æµ‹è¯•ã€å¯¹æŠ—æ”»å‡»ä¸é˜²å¾¡ï¼Œä»¥åŠå¯è§£é‡Šæ€§ã€‚*è®¡ç®—æœºç§‘å­¦è¯„è®º*ï¼Œ37:100270ï¼Œ2020å¹´ã€‚
- en: 'Huchette and Vielma (2022) J.Â Huchette and J.Â P. Vielma. Nonconvex piecewise
    linear functions: Advanced formulations and simple modeling tools. *Operations
    Research*, 2022.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huchette å’Œ Vielma (2022) J. Huchette å’Œ J. P. Vielma. éå‡¸åˆ†æ®µçº¿æ€§å‡½æ•°ï¼šé«˜çº§å½¢å¼åŒ–å’Œç®€å•å»ºæ¨¡å·¥å…·ã€‚*è¿ç­¹å­¦*ï¼Œ2022å¹´ã€‚
- en: Huster etÂ al. (2018) T.Â Huster, C.-Y.Â J. Chiang, and R.Â Chadha. Limitations
    of the Lipschitz constant as a defense against adversarial examples. In *ECML
    PKDD Workshops*, 2018.
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huster ç­‰ (2018) T. Huster, C.-Y. J. Chiang å’Œ R. Chadha. Lipschitz å¸¸æ•°ä½œä¸ºå¯¹æŠ—ç¤ºä¾‹é˜²å¾¡çš„å±€é™æ€§ã€‚å‘è¡¨äº
    *ECML PKDD ç ”è®¨ä¼š*ï¼Œ2018å¹´ã€‚
- en: Hwang and Heinecke (2020) W.-L. Hwang and A.Â Heinecke. Un-rectifying non-linear
    networks for signal representation. *IEEE Transactions on Signal Processing*,
    68:196â€“210, 2020.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hwang å’Œ Heinecke (2020) W.-L. Hwang å’Œ A. Heinecke. ç”¨äºä¿¡å·è¡¨ç¤ºçš„éçº¿æ€§ç½‘ç»œçš„å»æ•´æµã€‚*IEEE ä¿¡å·å¤„ç†å­¦æŠ¥*ï¼Œ68:196â€“210ï¼Œ2020å¹´ã€‚
- en: Icarte etÂ al. (2019) R.Â T. Icarte, L.Â Illanes, M.Â P. Castro, A.Â A. Cire, S.Â A.
    McIlraith, and J.Â C. Beck. Training binarized neural networks using mip and cp.
    In *International Conference on Principles and Practice of Constraint Programming*,
    pages 401â€“417\. Springer, 2019.
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Icarte ç­‰ (2019) R. T. Icarte, L. Illanes, M. P. Castro, A. A. Cire, S. A. McIlraith
    å’Œ J. C. Beck. ä½¿ç”¨ mip å’Œ cp è®­ç»ƒäºŒå€¼åŒ–ç¥ç»ç½‘ç»œã€‚å‘è¡¨äº *çº¦æŸç¼–ç¨‹åŸç†ä¸å®è·µå›½é™…ä¼šè®®*ï¼Œ401â€“417é¡µã€‚Springerï¼Œ2019å¹´ã€‚
- en: 'Ioffe and Szegedy (2015) S.Â Ioffe and C.Â Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In *International
    Conference on Machine Learning (ICML)*, 2015.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe å’Œ Szegedy (2015) S. Ioffe å’Œ C. Szegedy. æ‰¹é‡å½’ä¸€åŒ–ï¼šé€šè¿‡å‡å°‘å†…éƒ¨åå˜é‡åç§»æ¥åŠ é€Ÿæ·±åº¦ç½‘ç»œè®­ç»ƒã€‚å‘è¡¨äº
    *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œ2015å¹´ã€‚
- en: Jeroslow and Lowe (1984) R.Â G. Jeroslow and J.Â K. Lowe. *Modelling with integer
    variables*. Springer, 1984.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeroslow å’Œ Lowe (1984) R. G. Jeroslow å’Œ J. K. Lowe. *æ•´æ•°å˜é‡å»ºæ¨¡*ã€‚Springerï¼Œ1984å¹´ã€‚
- en: Jia and Rinard (2020) K.Â Jia and M.Â Rinard. Efficient exact verification of
    binarized neural networks. *Neural Information Processing Systems (NeurIPS)*,
    33:1782â€“1795, 2020.
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia å’Œ Rinard (2020) K. Jia å’Œ M. Rinard. é«˜æ•ˆçš„äºŒå€¼åŒ–ç¥ç»ç½‘ç»œç²¾ç¡®éªŒè¯ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®ï¼ˆNeurIPSï¼‰*ï¼Œ33:1782â€“1795ï¼Œ2020å¹´ã€‚
- en: 'Johnson etÂ al. (2020) T.Â T. Johnson, D.Â M. Lopez, P.Â Musau, H.-D. Tran, E.Â Botoeva,
    F.Â Leofante, A.Â Maleki, C.Â Sidrane, J.Â Fan, and C.Â Huang. ARCH-COMP20 category
    report: Artificial intelligence and neural network control systems (AINNCS) for
    continuous and hybrid systems plants. In *International Workshop on Applied Verification
    of Continuous and Hybrid Systems (ARCH20)*, volumeÂ 74, pages 107â€“139, 2020.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson ç­‰ (2020) T. T. Johnson, D. M. Lopez, P. Musau, H.-D. Tran, E. Botoeva,
    F. Leofante, A. Maleki, C. Sidrane, J. Fan å’Œ C. Huang. ARCH-COMP20 ç±»åˆ«æŠ¥å‘Šï¼šç”¨äºè¿ç»­å’Œæ··åˆç³»ç»Ÿå·¥å‚çš„äººå·¥æ™ºèƒ½å’Œç¥ç»ç½‘ç»œæ§åˆ¶ç³»ç»Ÿï¼ˆAINNCSï¼‰ã€‚å‘è¡¨äº
    *å›½é™…è¿ç»­ä¸æ··åˆç³»ç»Ÿåº”ç”¨éªŒè¯ç ”è®¨ä¼šï¼ˆARCH20ï¼‰*ï¼Œç¬¬74å·ï¼Œ107â€“139é¡µï¼Œ2020å¹´ã€‚
- en: Jordan and Dimakis (2020) M.Â Jordan and A.Â G. Dimakis. Exactly computing the
    local Lipschitz constant of ReLU networks. In *Neural Information Processing Systems
    (NeurIPS)*, volumeÂ 33, 2020.
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jordan å’Œ Dimakis (2020) M. Jordan å’Œ A. G. Dimakis. ç²¾ç¡®è®¡ç®— ReLU ç½‘ç»œçš„å±€éƒ¨ Lipschitz
    å¸¸æ•°ã€‚å‘è¡¨äº *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®ï¼ˆNeurIPSï¼‰*ï¼Œç¬¬33å·ï¼Œ2020å¹´ã€‚
- en: 'Jordan etÂ al. (2019) M.Â Jordan, J.Â Lewis, and A.Â G. Dimakis. Provable certificates
    for adversarial examples: Fitting a ball in the union of polytopes. In *Neural
    Information Processing Systems (NeurIPS)*, volumeÂ 32, 2019.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jordan ç­‰ (2019) M. Jordan, J. Lewis å’Œ A. G. Dimakis. å¯¹æŠ—ç¤ºä¾‹çš„å¯è¯æ˜è¯ä¹¦ï¼šåœ¨å¤šé¢ä½“çš„å¹¶é›†ä¸­æ‹Ÿåˆä¸€ä¸ªçƒä½“ã€‚å‘è¡¨äº
    *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®ï¼ˆNeurIPSï¼‰*ï¼Œç¬¬32å·ï¼Œ2019å¹´ã€‚
- en: 'Karg and Lucia (2020) B.Â Karg and S.Â Lucia. Efficient representation and approximation
    of model predictive control laws via deep learning. *IEEE Transactions on Cybernetics*,
    50(9):3866â€“3878, 2020. doi: 10.1109/TCYB.2020.2999556.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Karg å’Œ Lucia (2020) B. Karg å’Œ S. Lucia. é€šè¿‡æ·±åº¦å­¦ä¹ é«˜æ•ˆè¡¨ç¤ºå’Œé€¼è¿‘æ¨¡å‹é¢„æµ‹æ§åˆ¶å¾‹ã€‚*IEEE æ§åˆ¶è®ºå­¦æŠ¥*ï¼Œ50(9):3866â€“3878ï¼Œ2020å¹´ã€‚doi:
    10.1109/TCYB.2020.2999556ã€‚'
- en: 'Katz etÂ al. (2017) G.Â Katz, C.Â Barrett, D.Â L. Dill, K.Â Julian, and M.Â J. Kochenderfer.
    Reluplex: An efficient SMT solver for verifying deep neural networks. In *Computer
    Aided Verification (CAV)*, pages 97â€“117\. Springer, 2017.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katzç­‰ï¼ˆ2017ï¼‰G. Katz, C. Barrett, D. L. Dill, K. Julian, å’Œ M. J. Kochenderfer.
    Reluplexï¼šä¸€ç§é«˜æ•ˆçš„SMTæ±‚è§£å™¨ç”¨äºéªŒè¯æ·±åº¦ç¥ç»ç½‘ç»œã€‚åœ¨ *è®¡ç®—æœºè¾…åŠ©éªŒè¯ï¼ˆCAVï¼‰*ï¼Œé¡µ97â€“117ã€‚Springerï¼Œ2017ã€‚
- en: Katz etÂ al. (2019) G.Â Katz, D.Â A. Huang, D.Â Ibeling, K.Â Julian, C.Â Lazarus,
    R.Â Lim, P.Â Shah, S.Â Thakoor, H.Â Wu, A.Â ZeljiÄ‡, etÂ al. The marabou framework for
    verification and analysis of deep neural networks. In *Computer Aided Verification
    (CAV)*, pages 443â€“452. Springer, 2019.
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katzç­‰ï¼ˆ2019ï¼‰G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus, R. Lim,
    P. Shah, S. Thakoor, H. Wu, A. ZeljiÄ‡, ç­‰. Marabou æ¡†æ¶ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œçš„éªŒè¯ä¸åˆ†æã€‚åœ¨ *è®¡ç®—æœºè¾…åŠ©éªŒè¯ï¼ˆCAVï¼‰*ï¼Œé¡µ443â€“452ã€‚Springerï¼Œ2019ã€‚
- en: Katz etÂ al. (2020) J.Â Katz, I.Â Pappas, S.Â Avraamidou, and E.Â N. Pistikopoulos.
    Integrating deep learning models and multiparametric programming. *Computers &
    Chemical Engineering*, 136:106801, 2020.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katzç­‰ï¼ˆ2020ï¼‰J. Katz, I. Pappas, S. Avraamidou, å’Œ E. N. Pistikopoulos. æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸å¤šå‚æ•°ç¼–ç¨‹çš„é›†æˆã€‚*è®¡ç®—æœºä¸åŒ–å­¦å·¥ç¨‹*ï¼Œ136:106801ï¼Œ2020ã€‚
- en: 'Keup and Helias (2022) C.Â Keup and M.Â Helias. Origami in N dimensions: How
    feed-forward networks manufacture linear separability. *arXiv:2203.11355*, 2022.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keupå’ŒHeliasï¼ˆ2022ï¼‰C. Keup å’Œ M. Helias. Nç»´çš„æŠ˜çº¸ï¼šå‰é¦ˆç½‘ç»œå¦‚ä½•åˆ¶é€ çº¿æ€§å¯åˆ†æ€§ã€‚*arXiv:2203.11355*ï¼Œ2022ã€‚
- en: 'Khalife and Basu (2022) S.Â Khalife and A.Â Basu. Neural networks with linear
    threshold activations: structure and algorithms. In *Integer Programming and Combinatorial
    Optimization (IPCO)*, pages 347â€“360\. Springer, 2022.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khalifeå’ŒBasuï¼ˆ2022ï¼‰S. Khalife å’Œ A. Basu. å…·æœ‰çº¿æ€§é˜ˆå€¼æ¿€æ´»çš„ç¥ç»ç½‘ç»œï¼šç»“æ„ä¸ç®—æ³•ã€‚åœ¨ *æ•´æ•°è§„åˆ’ä¸ç»„åˆä¼˜åŒ–ï¼ˆIPCOï¼‰*ï¼Œé¡µ347â€“360ã€‚Springerï¼Œ2022ã€‚
- en: Khedr etÂ al. (2020) H.Â Khedr, J.Â Ferlez, and Y.Â Shoukry. Effective formal verification
    of neural networks using the geometry of linear regions. *arXiv:2006.10864*, 2020.
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khedrç­‰ï¼ˆ2020ï¼‰H. Khedr, J. Ferlez, å’Œ Y. Shoukry. ä½¿ç”¨çº¿æ€§åŒºåŸŸå‡ ä½•çš„ç¥ç»ç½‘ç»œæœ‰æ•ˆå½¢å¼åŒ–éªŒè¯ã€‚*arXiv:2006.10864*ï¼Œ2020ã€‚
- en: 'Kingma and Ba (2014) D.Â P. Kingma and J.Â Ba. Adam: A method for stochastic
    optimization. *arXiv:1412.6980*, 2014.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingmaå’ŒBaï¼ˆ2014ï¼‰D. P. Kingma å’Œ J. Ba. Adamï¼šä¸€ç§éšæœºä¼˜åŒ–æ–¹æ³•ã€‚*arXiv:1412.6980*ï¼Œ2014ã€‚
- en: 'Kody etÂ al. (2022) A.Â Kody, S.Â Chevalier, S.Â Chatzivasileiadis, and D.Â Molzahn.
    Modeling the ac power flow equations with optimally compact neural networks: Application
    to unit commitment. *Electric Power Systems Research*, 213:108282, 2022.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kodyç­‰ï¼ˆ2022ï¼‰A. Kody, S. Chevalier, S. Chatzivasileiadis, å’Œ D. Molzahn. ç”¨æœ€ä¼˜ç´§å‡‘çš„ç¥ç»ç½‘ç»œå»ºæ¨¡äº¤æµç”µåŠŸç‡æµæ–¹ç¨‹ï¼šåœ¨å•ä½æ‰¿è¯ºä¸­çš„åº”ç”¨ã€‚*ç”µåŠ›ç³»ç»Ÿç ”ç©¶*ï¼Œ213:108282ï¼Œ2022ã€‚
- en: Kouvaros etÂ al. (2021) P.Â Kouvaros, T.Â Kyono, F.Â Leofante, A.Â Lomuscio, D.Â Margineantu,
    D.Â Osipychev, and Y.Â Zheng. Formal analysis of neural network-based systems in
    the aircraft domain. In *International Symposium on Formal Methods (FM)*, pages
    730â€“740\. Springer, 2021.
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kouvarosç­‰ï¼ˆ2021ï¼‰P. Kouvaros, T. Kyono, F. Leofante, A. Lomuscio, D. Margineantu,
    D. Osipychev, å’Œ Y. Zheng. é£æœºé¢†åŸŸåŸºäºç¥ç»ç½‘ç»œç³»ç»Ÿçš„å½¢å¼åŒ–åˆ†æã€‚åœ¨ *å½¢å¼æ–¹æ³•å›½é™…ç ”è®¨ä¼šï¼ˆFMï¼‰*ï¼Œé¡µ730â€“740ã€‚Springerï¼Œ2021ã€‚
- en: Krizhevsky etÂ al. (2012) A.Â Krizhevsky, I.Â Sutskever, and G.Â Hinton. Imagenet
    classification with deep convolutional neural networks. In *Neural Information
    Processing Systems (NeurIPS)*, volumeÂ 25, 2012.
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevskyç­‰ï¼ˆ2012ï¼‰A. Krizhevsky, I. Sutskever, å’Œ G. Hinton. ä½¿ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„ImageNetåˆ†ç±»ã€‚åœ¨
    *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œå·25ï¼Œ2012ã€‚
- en: 'Kronqvist etÂ al. (2021) J.Â Kronqvist, R.Â Misener, and C.Â Tsay. Between steps:
    Intermediate relaxations between big-M and convex hull formulations. In *International
    Conference on the Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, 2021.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kronqvistç­‰ï¼ˆ2021ï¼‰J. Kronqvist, R. Misener, å’Œ C. Tsay. æ­¥éª¤ä¹‹é—´ï¼šå¤§Mä¸å‡¸åŒ…å…¬å¼ä¹‹é—´çš„ä¸­é—´æ¾å¼›ã€‚åœ¨ *çº¦æŸç¼–ç¨‹ã€äººå·¥æ™ºèƒ½ä¸è¿ç­¹å­¦æ•´åˆå›½é™…ä¼šè®®ï¼ˆCPAIORï¼‰*ï¼Œ2021ã€‚
- en: 'Kronqvist etÂ al. (2022) J.Â Kronqvist, R.Â Misener, , and C.Â Tsay. P-split formulations:
    A class of intermediate formulations between big-M and convex hull for disjunctive
    constraints. *arXiv:2202.05198*, 2022.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kronqvistç­‰ï¼ˆ2022ï¼‰J. Kronqvist, R. Misener, å’Œ C. Tsay. P-split å…¬å¼ï¼šå¤§Mä¸å‡¸åŒ…ä¹‹é—´çš„ä¸€ç±»ä¸­é—´å…¬å¼ï¼Œç”¨äºç¦»æ•£çº¦æŸã€‚*arXiv:2202.05198*ï¼Œ2022ã€‚
- en: Kumar etÂ al. (2019) A.Â Kumar, T.Â Serra, and S.Â Ramalingam. Equivalent and approximate
    transformations of deep neural networks. *arXiv:1905.1142*, 2019.
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumarç­‰ï¼ˆ2019ï¼‰A. Kumar, T. Serra, å’Œ S. Ramalingam. æ·±åº¦ç¥ç»ç½‘ç»œçš„ç­‰æ•ˆå’Œè¿‘ä¼¼å˜æ¢ã€‚*arXiv:1905.1142*ï¼Œ2019ã€‚
- en: Lacoste-Julien etÂ al. (2013) S.Â Lacoste-Julien, M.Â Jaggi, M.Â Schmidt, and P.Â Pletscher.
    Block-coordinate Frank-Wolfe optimization for structural SVMs. In *International
    Conference on Machine Learning (ICML)*, pages 53â€“61, 2013.
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lacoste-Julienç­‰ï¼ˆ2013ï¼‰S. Lacoste-Julien, M. Jaggi, M. Schmidt, å’Œ P. Pletscher.
    ç»“æ„åŒ–SVMçš„å—åæ ‡Frank-Wolfeä¼˜åŒ–ã€‚åœ¨ *å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šï¼ˆICMLï¼‰*ï¼Œé¡µ53â€“61ï¼Œ2013ã€‚
- en: Lan etÂ al. (2022) J.Â Lan, Y.Â Zheng, and A.Â Lomuscio. Tight neural network verification
    via semidefinite relaxations and linear reformulations. In *AAAI Conference on
    Artificial Intelligence*, volumeÂ 36, pages 7272â€“7280, 2022.
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan ç­‰äºº (2022) J. Lan, Y. Zheng, å’Œ A. Lomuscio. é€šè¿‡åŠæ­£å®šæ¾å¼›å’Œçº¿æ€§é‡æ„è¿›è¡Œç´§è‡´ç¥ç»ç½‘ç»œéªŒè¯ã€‚å‘è¡¨äº *AAAI
    Conference on Artificial Intelligence*ï¼Œç¬¬ 36 å·ï¼Œé¡µ 7272â€“7280ï¼Œ2022 å¹´ã€‚
- en: Latorre etÂ al. (2020) F.Â Latorre, P.Â Rolland, and V.Â Cevher. Lipschitz constant
    estimation of neural networks via sparse polynomial optimization. In *International
    Conference on Learning Representations (ICLR)*, 2020.
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Latorre ç­‰äºº (2020) F. Latorre, P. Rolland, å’Œ V. Cevher. é€šè¿‡ç¨€ç–å¤šé¡¹å¼ä¼˜åŒ–è¿›è¡Œç¥ç»ç½‘ç»œçš„ Lipschitz
    å¸¸æ•°ä¼°è®¡ã€‚å‘è¡¨äº *International Conference on Learning Representations (ICLR)*ï¼Œ2020 å¹´ã€‚
- en: LeCun etÂ al. (1989) Y.Â LeCun, B.Â Boser, J.Â S. Denker, D.Â Henderson, R.Â E. Howard,
    W.Â Hubbard, and L.Â D. Jackel. Backpropagation applied to handwritten zip code
    recognition. *Neural Computation*, 1(4):541â€“551, 1989.
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun ç­‰äºº (1989) Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
    W. Hubbard, å’Œ L. D. Jackel. åå‘ä¼ æ’­åœ¨æ‰‹å†™é‚®æ”¿ç¼–ç è¯†åˆ«ä¸­çš„åº”ç”¨ã€‚*Neural Computation*ï¼Œ1(4):541â€“551ï¼Œ1989
    å¹´ã€‚
- en: 'LeCun etÂ al. (1998) Y.Â LeCun, L.Â Bottou, G.Â B. Orr, and K.-R. MÃ¼ller. Efficient
    backprop. In G.Â Montavon, G.Â Orr, and K.Â MÃ¼ller, editors, *Neural Networks: Tricks
    of the Trade*. Springer, 1998.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LeCun ç­‰äºº (1998) Y. LeCun, L. Bottou, G. B. Orr, å’Œ K.-R. MÃ¼ller. é«˜æ•ˆçš„åå‘ä¼ æ’­ã€‚åœ¨ G.
    Montavon, G. Orr å’Œ K. MÃ¼ller ä¸»ç¼–çš„ *Neural Networks: Tricks of the Trade* ä¸­ã€‚Springerï¼Œ1998
    å¹´ã€‚'
- en: LeCun etÂ al. (2015) Y.Â LeCun, Y.Â Bengio, and G.Â Hinton. Deep learning. *Nature*,
    521, 2015.
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun ç­‰äºº (2015) Y. LeCun, Y. Bengio, å’Œ G. Hinton. æ·±åº¦å­¦ä¹ ã€‚*Nature*ï¼Œ521ï¼Œ2015 å¹´ã€‚
- en: Lee etÂ al. (2019) G.-H. Lee, D.Â Alvarez-Melis, and T.Â S. Jaakkola. Towards robust,
    locally linear deep networks. In *International Conference on Learning Representations
    (ICLR)*, 2019.
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee ç­‰äºº (2019) G.-H. Lee, D. Alvarez-Melis, å’Œ T. S. Jaakkola. æœå‘é²æ£’çš„å±€éƒ¨çº¿æ€§æ·±åº¦ç½‘ç»œã€‚å‘è¡¨äº
    *International Conference on Learning Representations (ICLR)*ï¼Œ2019 å¹´ã€‚
- en: 'Lee and Wilson (2001) J.Â Lee and D.Â Wilson. Polyhedral methods for piecewise-linear
    functions I: the lambda method. *Discrete Applied Mathematics*, 108(3):269â€“285,
    2001.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee å’Œ Wilson (2001) J. Lee å’Œ D. Wilson. é’ˆå¯¹åˆ†æ®µçº¿æ€§å‡½æ•°çš„å¤šé¢ä½“æ–¹æ³• I: lambda æ–¹æ³•ã€‚*Discrete
    Applied Mathematics*ï¼Œ108(3):269â€“285ï¼Œ2001 å¹´ã€‚'
- en: 'Leofante etÂ al. (2018) F.Â Leofante, N.Â Narodytska, L.Â Pulina, and A.Â Tacchella.
    Automated verification of neural networks: Advances, challenges and perspectives.
    *arXiv:1805.09938*, 2018.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leofante ç­‰äºº (2018) F. Leofante, N. Narodytska, L. Pulina, å’Œ A. Tacchella. ç¥ç»ç½‘ç»œçš„è‡ªåŠ¨åŒ–éªŒè¯ï¼šè¿›å±•ã€æŒ‘æˆ˜å’Œå±•æœ›ã€‚*arXiv:1805.09938*ï¼Œ2018
    å¹´ã€‚
- en: 'Li etÂ al. (2022) L.Â Li, T.Â Xie, and B.Â Li. Sok: Certified robustness for deep
    neural networks. In *2023 IEEE Symposium on Security and Privacy (SP)*, pages
    94â€“115\. IEEE Computer Society, 2022.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li ç­‰äºº (2022) L. Li, T. Xie, å’Œ B. Li. Sok: æ·±åº¦ç¥ç»ç½‘ç»œçš„è®¤è¯é²æ£’æ€§ã€‚å‘è¡¨äº *2023 IEEE Symposium
    on Security and Privacy (SP)*ï¼Œé¡µ 94â€“115ã€‚IEEE è®¡ç®—æœºå­¦ä¼šï¼Œ2022 å¹´ã€‚'
- en: Liang and Xu (2021) X.Â Liang and J.Â Xu. Biased ReLU neural networks. *Neurocomputing*,
    423:71â€“79, 2021.
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang å’Œ Xu (2021) X. Liang å’Œ J. Xu. åç½® ReLU ç¥ç»ç½‘ç»œã€‚*Neurocomputing*ï¼Œ423:71â€“79ï¼Œ2021
    å¹´ã€‚
- en: Lillicrap etÂ al. (2015) T.Â P. Lillicrap, J.Â J. Hunt, A.Â Pritzel, N.Â Heess, T.Â Erez,
    Y.Â Tassa, D.Â Silver, and D.Â Wierstra. Continuous control with deep reinforcement
    learning. *arXiv:1509.02971*, 2015.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lillicrap ç­‰äºº (2015) T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,
    Y. Tassa, D. Silver, å’Œ D. Wierstra. åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è¿ç»­æ§åˆ¶ã€‚*arXiv:1509.02971*ï¼Œ2015 å¹´ã€‚
- en: Linnainmaa (1970) S.Â Linnainmaa. The representation of the cumulative rounding
    error of an algorithm as a Taylor expansion of the local rounding errors (in Finnish).
    Masterâ€™s thesis, Univ. Helsinki, 1970.
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linnainmaa (1970) S. Linnainmaa. ç®—æ³•ç´¯ç§¯èˆå…¥è¯¯å·®çš„æ³°å‹’å±•å¼€è¡¨ç¤ºï¼ˆèŠ¬å…°è¯­ï¼‰ã€‚ç¡•å£«è®ºæ–‡ï¼Œèµ«å°”è¾›åŸºå¤§å­¦ï¼Œ1970 å¹´ã€‚
- en: Little (1974) W.Â Little. The existence of persistent states in the brain. *Mathematical
    Biosciences*, 19:101â€“120, 1974.
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Little (1974) W. Little. å¤§è„‘ä¸­æŒä¹…çŠ¶æ€çš„å­˜åœ¨ã€‚*Mathematical Biosciences*ï¼Œ19:101â€“120ï¼Œ1974
    å¹´ã€‚
- en: Liu and Liang (2021) B.Â Liu and Y.Â Liang. Optimal function approximation with
    ReLU neural networks. *Neurocomputing*, 435:216â€“227, 2021.
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu å’Œ Liang (2021) B. Liu å’Œ Y. Liang. ä½¿ç”¨ ReLU ç¥ç»ç½‘ç»œçš„æœ€ä¼˜å‡½æ•°é€¼è¿‘ã€‚*Neurocomputing*ï¼Œ435:216â€“227ï¼Œ2021
    å¹´ã€‚
- en: Liu etÂ al. (2021) C.Â Liu, T.Â Arnon, C.Â Lazarus, C.Â Strong, C.Â Barrett, M.Â J.
    Kochenderfer, etÂ al. Algorithms for verifying deep neural networks. *Foundations
    and TrendsÂ® in Optimization*, 4(3-4):244â€“404, 2021.
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu ç­‰äºº (2021) C. Liu, T. Arnon, C. Lazarus, C. Strong, C. Barrett, M. J. Kochenderfer
    ç­‰äºº. éªŒè¯æ·±åº¦ç¥ç»ç½‘ç»œçš„ç®—æ³•ã€‚*Foundations and TrendsÂ® in Optimization*ï¼Œ4(3-4):244â€“404ï¼Œ2021
    å¹´ã€‚
- en: Liu etÂ al. (2020) X.Â Liu, X.Â Han, N.Â Zhang, and Q.Â Liu. Certified monotonic
    neural networks. In *Neural Information Processing Systems (NeurIPS)*, volumeÂ 33,
    2020.
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu ç­‰äºº (2020) X. Liu, X. Han, N. Zhang, å’Œ Q. Liu. è®¤è¯å•è°ƒç¥ç»ç½‘ç»œã€‚å‘è¡¨äº *Neural Information
    Processing Systems (NeurIPS)*ï¼Œç¬¬ 33 å·ï¼Œ2020 å¹´ã€‚
- en: Lombardi etÂ al. (2017) M.Â Lombardi, M.Â Milano, and A.Â Bartolini. Empirical decision
    model learning. *Artificial Intelligence*, 244:343â€“367, 2017.
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lombardi ç­‰äºº (2017) M. Lombardi, M. Milano, å’Œ A. Bartolini. å®è¯å†³ç­–æ¨¡å‹å­¦ä¹ ã€‚*Artificial
    Intelligence*ï¼Œ244:343â€“367ï¼Œ2017 å¹´ã€‚
- en: Lomuscio and Maganti (2017) A.Â Lomuscio and L.Â Maganti. An approach to reachability
    analysis for feed-forward ReLU neural networks. *arXiv:1706.07351*, 2017.
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lomuscioå’ŒMagantiï¼ˆ2017ï¼‰A. Lomuscio å’Œ L. Magantiã€‚ã€Šä¸€ç§ç”¨äºå‰é¦ˆReLUç¥ç»ç½‘ç»œçš„å¯è¾¾æ€§åˆ†ææ–¹æ³•ã€‹ã€‚*arXiv:1706.07351*ï¼Œ2017å¹´ã€‚
- en: Loukas etÂ al. (2021) A.Â Loukas, M.Â Poiitis, and S.Â Jegelka. What training reveals
    about neural network complexity. In *Neural Information Processing Systems (NeurIPS)*,
    volumeÂ 34, 2021.
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loukasç­‰ï¼ˆ2021ï¼‰A. Loukas, M. Poiitis, å’Œ S. Jegelkaã€‚ã€Šè®­ç»ƒæ­ç¤ºäº†ç¥ç»ç½‘ç»œå¤æ‚æ€§çš„ä»€ä¹ˆã€‹ã€‚å‘è¡¨äº*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œç¬¬34å·ï¼Œ2021å¹´ã€‚
- en: 'Lu etÂ al. (2017) Z.Â Lu, H.Â Pu, F.Â Wang, Z.Â Hu, and L.Â Wang. The expressive
    power of neural networks: A view from the width. In *Neural Information Processing
    Systems (NeurIPS)*, volumeÂ 30, 2017.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luç­‰ï¼ˆ2017ï¼‰Z. Lu, H. Pu, F. Wang, Z. Hu, å’Œ L. Wangã€‚ã€Šç¥ç»ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ï¼šä»å®½åº¦çš„è§†è§’ã€‹ã€‚å‘è¡¨äº*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œç¬¬30å·ï¼Œ2017å¹´ã€‚
- en: 'Lueg etÂ al. (2021) L.Â Lueg, B.Â Grimstad, A.Â Mitsos, and A.Â M. Schweidtmann.
    reluMIP: Open source tool for MILP optimization of ReLU neural networks, 2021.
    URL https://github.com/ChemEngAI/ReLU_ANN_MILP.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luegç­‰ï¼ˆ2021ï¼‰L. Lueg, B. Grimstad, A. Mitsos, å’Œ A. M. Schweidtmannã€‚ã€ŠreluMIPï¼šç”¨äºReLUç¥ç»ç½‘ç»œMILPä¼˜åŒ–çš„å¼€æºå·¥å…·ã€‹ï¼Œ2021å¹´ã€‚ç½‘å€
    https://github.com/ChemEngAI/ReLU_ANN_MILPã€‚
- en: 'Lyu etÂ al. (2020) Z.Â Lyu, C.-Y. Ko, Z.Â Kong, N.Â Wong, D.Â Lin, and L.Â Daniel.
    Fastened crown: Tightened neural network robustness certificates. In *AAAI Conference
    on Artificial Intelligence*, volumeÂ 34, pages 5037â€“5044, 2020.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyuç­‰ï¼ˆ2020ï¼‰Z. Lyu, C.-Y. Ko, Z. Kong, N. Wong, D. Lin, å’Œ L. Danielã€‚ã€Šç´§å›ºçš„çš‡å† ï¼šå¼ºåŒ–çš„ç¥ç»ç½‘ç»œé²æ£’æ€§è¯ä¹¦ã€‹ã€‚å‘è¡¨äº*AAAIäººå·¥æ™ºèƒ½ä¼šè®®*ï¼Œç¬¬34å·ï¼Œç¬¬5037â€“5044é¡µï¼Œ2020å¹´ã€‚
- en: Maas etÂ al. (2013) A.Â Maas, A.Â Hannun, and A.Â Ng. Rectifier nonlinearities improve
    neural network acoustic models. In *ICML Workshop on Deep Learning for Audio,
    Speech and Language Processing*, 2013.
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maasç­‰ï¼ˆ2013ï¼‰A. Maas, A. Hannun, å’Œ A. Ngã€‚ã€Šæ•´æµéçº¿æ€§æé«˜äº†ç¥ç»ç½‘ç»œå£°å­¦æ¨¡å‹ã€‹ã€‚å‘è¡¨äº*ICMLéŸ³é¢‘ã€è¯­éŸ³å’Œè¯­è¨€å¤„ç†æ·±åº¦å­¦ä¹ ç ”è®¨ä¼š*ï¼Œ2013å¹´ã€‚
- en: Madry etÂ al. (2018) A.Â Madry, A.Â Makelov, L.Â Schmidt, D.Â Tsipras, and A.Â Vladu.
    Towards deep learning models resistant to adversarial attacks. In *International
    Conference on Learning Representations (ICLR)*, 2018.
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madryç­‰ï¼ˆ2018ï¼‰A. Madry, A. Makelov, L. Schmidt, D. Tsipras, å’Œ A. Vladuã€‚ã€Šæœå‘å¯¹æŠ—æ”»å‡»å…·æœ‰é²æ£’æ€§çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‹ã€‚å‘è¡¨äº*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2018å¹´ã€‚
- en: Makhoul etÂ al. (1989) J.Â Makhoul, R.Â Schwartz, and A.Â El-Jaroudi. Classification
    capabilities of two-layer neural nets. In *International Conference on Acoustics,
    Speech, and Signal Processing (ICASSP)*, 1989.
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makhoulç­‰ï¼ˆ1989ï¼‰J. Makhoul, R. Schwartz, å’Œ A. El-Jaroudiã€‚ã€ŠåŒå±‚ç¥ç»ç½‘ç»œçš„åˆ†ç±»èƒ½åŠ›ã€‹ã€‚å‘è¡¨äº*å›½é™…å£°å­¦ã€è¯­éŸ³å’Œä¿¡å·å¤„ç†ä¼šè®®ï¼ˆICASSPï¼‰*ï¼Œ1989å¹´ã€‚
- en: Malach and Shalev-Shwartz (2019) E.Â Malach and S.Â Shalev-Shwartz. Is deeper
    better only when shallow is good? In *Neural Information Processing Systems (NeurIPS)*,
    volumeÂ 32, 2019.
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malachå’ŒShalev-Shwartzï¼ˆ2019ï¼‰E. Malach å’Œ S. Shalev-Shwartzã€‚ã€Šæ·±åº¦æ˜¯å¦ä»…åœ¨æµ…å±‚è‰¯å¥½æ—¶æ‰æ›´ä¼˜ï¼Ÿã€‹å‘è¡¨äº*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œç¬¬32å·ï¼Œ2019å¹´ã€‚
- en: Mangasarian (1993) O.Â L. Mangasarian. Mathematical programming in neural networks.
    *ORSA Journal on Computing*, 5(4):349â€“360, 1993.
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mangasarianï¼ˆ1993ï¼‰O. L. Mangasarianã€‚ã€Šç¥ç»ç½‘ç»œä¸­çš„æ•°å­¦è§„åˆ’ã€‹ã€‚*ORSAè®¡ç®—æœŸåˆŠ*ï¼Œ5(4):349â€“360ï¼Œ1993å¹´ã€‚
- en: Maragno etÂ al. (2021) D.Â Maragno, H.Â Wiberg, D.Â Bertsimas, S.Â I. Birbil, D.Â d.
    Hertog, and A.Â Fajemisin. Mixed-integer optimization with constraint learning.
    *arXiv:2111.04469*, 2021.
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maragnoç­‰ï¼ˆ2021ï¼‰D. Maragno, H. Wiberg, D. Bertsimas, S. I. Birbil, D. d. Hertog,
    å’Œ A. Fajemisinã€‚ã€Šå¸¦çº¦æŸå­¦ä¹ çš„æ··åˆæ•´æ•°ä¼˜åŒ–ã€‹ã€‚*arXiv:2111.04469*ï¼Œ2021å¹´ã€‚
- en: Maragno etÂ al. (2023) D.Â Maragno, J.Â Kurtz, T.Â E. RÃ¶ber, R.Â Goedhart, Å.Â I.
    Birbil, and D.Â d. Hertog. Finding regions of counterfactual explanations via robust
    optimization. *arXiv:2301.11113*, 2023.
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maragnoç­‰ï¼ˆ2023ï¼‰D. Maragno, J. Kurtz, T. E. RÃ¶ber, R. Goedhart, Å. I. Birbil,
    å’Œ D. d. Hertogã€‚ã€Šé€šè¿‡é²æ£’ä¼˜åŒ–å¯»æ‰¾åäº‹å®è§£é‡Šçš„åŒºåŸŸã€‹ã€‚*arXiv:2301.11113*ï¼Œ2023å¹´ã€‚
- en: Maragos etÂ al. (2021) P.Â Maragos, V.Â Charisopoulos, and E.Â Theodosis. Tropical
    geometry and machine learning. *Proceedings of the IEEE*, 109(5):728â€“755, 2021.
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maragosç­‰ï¼ˆ2021ï¼‰P. Maragos, V. Charisopoulos, å’Œ E. Theodosisã€‚ã€Šçƒ­å¸¦å‡ ä½•ä¸æœºå™¨å­¦ä¹ ã€‹ã€‚*IEEEæœŸåˆŠ*ï¼Œ109(5):728â€“755ï¼Œ2021å¹´ã€‚
- en: Masden (2022) M.Â Masden. Algorithmic determination of the combinatorial structure
    of the linear regions of ReLU neural networks. *arXiv:2207.07696*, 2022.
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masdenï¼ˆ2022ï¼‰M. Masdenã€‚ã€ŠReLUç¥ç»ç½‘ç»œçº¿æ€§åŒºåŸŸçš„ç»„åˆç»“æ„çš„ç®—æ³•ç¡®å®šã€‹ã€‚*arXiv:2207.07696*ï¼Œ2022å¹´ã€‚
- en: Matoba etÂ al. (2022) K.Â Matoba, N.Â Dimitriadis, and F.Â Fleuret. The theoretical
    expressiveness of maxpooling. *arXiv:2203.01016*, 2022.
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matobaç­‰ï¼ˆ2022ï¼‰K. Matoba, N. Dimitriadis, å’Œ F. Fleuretã€‚ã€Šæœ€å¤§æ± åŒ–çš„ç†è®ºè¡¨è¾¾èƒ½åŠ›ã€‹ã€‚*arXiv:2203.01016*ï¼Œ2022å¹´ã€‚
- en: Matousek (2002) J.Â Matousek. *Lectures on Discrete Geometry*, volume 212. Springer
    Science & Business Media, 2002.
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matousekï¼ˆ2002ï¼‰J. Matousekã€‚ã€Šç¦»æ•£å‡ ä½•è®²åº§ã€‹ï¼Œç¬¬212å·ã€‚Springer Science & Business Mediaï¼Œ2002å¹´ã€‚
- en: McBride and Sundmacher (2019) K.Â McBride and K.Â Sundmacher. Overview of surrogate
    modeling in chemical process engineering. *Chemie Ingenieur Technik*, 91(3):228â€“239,
    2019.
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McBride å’Œ Sundmacher (2019) K. McBride å’Œ K. Sundmacher. åŒ–å­¦è¿‡ç¨‹å·¥ç¨‹ä¸­æ›¿ä»£å»ºæ¨¡çš„æ¦‚è¿°ã€‚*åŒ–å­¦å·¥ç¨‹æŠ€æœ¯*ï¼Œ91(3):228â€“239ï¼Œ2019å¹´ã€‚
- en: McCulloch and Pitts (1943) W.Â McCulloch and W.Â Pitts. A logical calculus of
    the ideas immanent in nervous activity. *Bulletin of Mathematical Biophysics*,
    5:115â€“133, 1943.
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCulloch å’Œ Pitts (1943) W. McCulloch å’Œ W. Pitts. ç¥ç»æ´»åŠ¨ä¸­å›ºæœ‰æ€æƒ³çš„é€»è¾‘æ¼”ç®—ã€‚*æ•°å­¦ç”Ÿç‰©ç‰©ç†å­¦å…¬æŠ¥*ï¼Œ5:115â€“133ï¼Œ1943å¹´ã€‚
- en: Mhaskar and Poggio (2020) H.Â N. Mhaskar and T.Â Poggio. Function approximation
    by deep networks. *Communications on Pure & Applied Analysis*, 19(8):4085â€“4095,
    2020.
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mhaskar å’Œ Poggio (2020) H. N. Mhaskar å’Œ T. Poggio. é€šè¿‡æ·±åº¦ç½‘ç»œè¿›è¡Œå‡½æ•°é€¼è¿‘ã€‚*çº¯ä¸åº”ç”¨åˆ†æé€šè®¯*ï¼Œ19(8):4085â€“4095ï¼Œ2020å¹´ã€‚
- en: 'Minsky and Papert (1969) M.Â Minsky and S.Â Papert. *Perceptrons: An Introduction
    to Computational Geometry*. The MIT Press, 1969.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minsky å’Œ Papert (1969) M. Minsky å’Œ S. Papert. *æ„ŸçŸ¥å™¨ï¼šè®¡ç®—å‡ ä½•å…¥é—¨*ã€‚éº»çœç†å·¥å­¦é™¢å‡ºç‰ˆç¤¾ï¼Œ1969å¹´ã€‚
- en: Mirman etÂ al. (2018) M.Â Mirman, T.Â Gehr, and M.Â Vechev. Differentiable abstract
    interpretation for provably robust neural networks. In *International Conference
    on Machine Learning (ICML)*, volumeÂ 80, pages 3578â€“3586, 2018.
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirman ç­‰ (2018) M. Mirman, T. Gehr å’Œ M. Vechev. ç”¨äºå¯è¯æ˜é²æ£’ç¥ç»ç½‘ç»œçš„å¯å¾®æŠ½è±¡è§£é‡Šã€‚å‘è¡¨äº *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®
    (ICML)*ï¼Œç¬¬ 80 å·ï¼Œé¡µç  3578â€“3586ï¼Œ2018å¹´ã€‚
- en: Misener and Floudas (2012) R.Â Misener and C.Â A. Floudas. Global optimization
    of mixed-integer quadratically-constrained quadratic programs (MIQCQP) through
    piecewise-linear and edge-concave relaxations. *Mathematical Programming*, 136(1):155â€“182,
    2012.
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Misener å’Œ Floudas (2012) R. Misener å’Œ C. A. Floudas. é€šè¿‡åˆ†æ®µçº¿æ€§å’Œè¾¹ç¼˜å‡¹æ”¾æ¾è¿›è¡Œæ··åˆæ•´æ•°äºŒæ¬¡çº¦æŸäºŒæ¬¡è§„åˆ’
    (MIQCQP) çš„å…¨å±€ä¼˜åŒ–ã€‚*æ•°å­¦ç¼–ç¨‹*ï¼Œ136(1):155â€“182ï¼Œ2012å¹´ã€‚
- en: Mnih etÂ al. (2015) V.Â Mnih, K.Â Kavukcuoglu, D.Â Silver, A.Â A. Rusu, J.Â Veness,
    M.Â G. Bellemare, A.Â Graves, M.Â Riedmiller, A.Â K. Fidjeland, G.Â Ostrovski, S.Â Petersen,
    C.Â Beattie, A.Â Sadik, I.Â Antonoglou, H.Â King, D.Â Kumaran, D.Â Wierstra, S.Â Legg,
    and D.Â Hassabis. Human-level control through deep reinforcement learning. *Nature*,
    518:529â€“533, 2015.
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih ç­‰ (2015) V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M.
    G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen,
    C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg
    å’Œ D. Hassabis. é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ å®ç°äººç±»æ°´å¹³çš„æ§åˆ¶ã€‚*è‡ªç„¶*ï¼Œ518:529â€“533ï¼Œ2015å¹´ã€‚
- en: MontÃºfar (2017) G.Â MontÃºfar. Notes on the number of linear regions of deep neural
    networks. In *Sampling Theory and Applications (SampTA)*, 2017.
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MontÃºfar (2017) G. MontÃºfar. æ·±åº¦ç¥ç»ç½‘ç»œçº¿æ€§åŒºåŸŸæ•°é‡çš„ç¬”è®°ã€‚å‘è¡¨äº *é‡‡æ ·ç†è®ºä¸åº”ç”¨ (SampTA)*ï¼Œ2017å¹´ã€‚
- en: MontÃºfar etÂ al. (2014) G.Â MontÃºfar, R.Â Pascanu, K.Â Cho, and Y.Â Bengio. On the
    number of linear regions of deep neural networks. In *Neural Information Processing
    Systems (NeurIPS)*, volumeÂ 27, 2014.
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MontÃºfar ç­‰ (2014) G. MontÃºfar, R. Pascanu, K. Cho å’Œ Y. Bengio. æ·±åº¦ç¥ç»ç½‘ç»œçº¿æ€§åŒºåŸŸçš„æ•°é‡ã€‚å‘è¡¨äº
    *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿ (NeurIPS)*ï¼Œç¬¬ 27 å·ï¼Œ2014å¹´ã€‚
- en: MontÃºfar etÂ al. (2022) G.Â MontÃºfar, Y.Â Ren, and L.Â Zhang. Sharp bounds for the
    number of regions of maxout networks and vertices of Minkowski sums. *SIAM Journal
    on Applied Algebra and Geometry*, 6(4), 2022.
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MontÃºfar ç­‰ (2022) G. MontÃºfar, Y. Ren å’Œ L. Zhang. Maxout ç½‘ç»œçš„åŒºåŸŸæ•°é‡å’Œ Minkowski
    å’Œçš„é¡¶ç‚¹çš„å°–é”ç•Œé™ã€‚*SIAM åº”ç”¨ä»£æ•°ä¸å‡ ä½•æ‚å¿—*ï¼Œ6(4)ï¼Œ2022å¹´ã€‚
- en: Motzkin (1936) T.Â Motzkin. *Beitrage zur theorie der linearen Ungleichungen*.
    PhD thesis, University of Basel, 1936.
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Motzkin (1936) T. Motzkin. *çº¿æ€§ä¸ç­‰å¼ç†è®ºçš„è´¡çŒ®*ã€‚åšå£«è®ºæ–‡ï¼Œå·´å¡å°”å¤§å­¦ï¼Œ1936å¹´ã€‚
- en: 'Mukhopadhyay etÂ al. (1993) S.Â Mukhopadhyay, A.Â Roy, L.Â S. Kim, and S.Â Govil.
    A polynomial time algorithm for generating neural networks for pattern classification:
    Its stability properties and some test results. *Neural Computation*, 5(2):317â€“330,
    1993.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukhopadhyay ç­‰ (1993) S. Mukhopadhyay, A. Roy, L. S. Kim å’Œ S. Govil. ä¸€ç§ç”¨äºæ¨¡å¼åˆ†ç±»çš„å¤šé¡¹å¼æ—¶é—´ç®—æ³•ï¼šå…¶ç¨³å®šæ€§ç‰¹æ€§å’Œä¸€äº›æµ‹è¯•ç»“æœã€‚*ç¥ç»è®¡ç®—*ï¼Œ5(2):317â€“330ï¼Œ1993å¹´ã€‚
- en: Nair and Hinton (2010) V.Â Nair and G.Â Hinton. Rectified linear units improve
    restricted boltzmann machines. In *International Conference on Machine Learning
    (ICML)*, 2010.
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair å’Œ Hinton (2010) V. Nair å’Œ G. Hinton. ä¿®æ­£çº¿æ€§å•å…ƒæ”¹è¿›é™åˆ¶ç»å°”å…¹æ›¼æœºã€‚å‘è¡¨äº *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®® (ICML)*ï¼Œ2010å¹´ã€‚
- en: Narodytska etÂ al. (2018) N.Â Narodytska, S.Â Kasiviswanathan, L.Â Ryzhyk, M.Â Sagiv,
    and T.Â Walsh. Verifying properties of binarized deep neural networks. In *AAAI
    Conference on Artificial Intelligence*, volumeÂ 32, 2018.
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narodytska ç­‰ (2018) N. Narodytska, S. Kasiviswanathan, L. Ryzhyk, M. Sagiv å’Œ
    T. Walsh. éªŒè¯äºŒå€¼åŒ–æ·±åº¦ç¥ç»ç½‘ç»œçš„å±æ€§ã€‚å‘è¡¨äº *AAAI äººå·¥æ™ºèƒ½ä¼šè®®*ï¼Œç¬¬ 32 å·ï¼Œ2018å¹´ã€‚
- en: Nelles etÂ al. (2000) O.Â Nelles, A.Â Fink, and R.Â Isermann. Local linear model
    trees (LOLIMOT) toolbox for nonlinear system identification. In *IFAC Symposium
    on System Identification (SYSID)*, 2000.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nelles ç­‰ (2000) O. Nelles, A. Fink å’Œ R. Isermann. éçº¿æ€§ç³»ç»Ÿè¯†åˆ«çš„å±€éƒ¨çº¿æ€§æ¨¡å‹æ ‘ (LOLIMOT)
    å·¥å…·ç®±ã€‚å‘è¡¨äº *IFAC ç³»ç»Ÿè¯†åˆ«ç ”è®¨ä¼š (SYSID)*ï¼Œ2000å¹´ã€‚
- en: Nesterov (1983) Y.Â E. Nesterov. A method of solving a convex programming problem
    with convergence rate $o\bigl{(}\frac{1}{k^{2}}\bigr{)}$. *Doklady Akademii Nauk*,
    269:543â€“547, 1983.
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nesterovï¼ˆ1983ï¼‰Y. E. Nesterov. è§£å†³å‡¸ä¼˜åŒ–é—®é¢˜çš„ä¸€ä¸ªæ–¹æ³•ï¼Œæ”¶æ•›é€Ÿåº¦ä¸º $o\bigl{(}\frac{1}{k^{2}}\bigr{)}$ã€‚*ä¿„ç½—æ–¯ç§‘å­¦é™¢å…¬æŠ¥*ï¼Œ269:543â€“547ï¼Œ1983ã€‚
- en: Newton and Papachristodoulou (2021) M.Â Newton and A.Â Papachristodoulou. Exploiting
    sparsity for neural network verification. In *Learning for Dynamics and Control
    (L4DC)*, pages 715â€“727. PMLR, 2021.
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newton å’Œ Papachristodoulouï¼ˆ2021ï¼‰M. Newton å’Œ A. Papachristodoulou. åˆ©ç”¨ç¨€ç–æ€§è¿›è¡Œç¥ç»ç½‘ç»œéªŒè¯ã€‚å‘è¡¨äº
    *åŠ¨æ€ä¸æ§åˆ¶å­¦ä¹ ï¼ˆL4DCï¼‰*ï¼Œç¬¬715â€“727é¡µã€‚PMLRï¼Œ2021ã€‚
- en: Nguyen etÂ al. (2018) Q.Â Nguyen, M.Â C. Mukkamala, and M.Â Hein. Neural networks
    should be wide enough to learn disconnected decision regions. In *International
    Conference on Machine Learning (ICML)*, 2018.
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen ç­‰ï¼ˆ2018ï¼‰Q. Nguyen, M. C. Mukkamala, å’Œ M. Hein. ç¥ç»ç½‘ç»œåº”è¶³å¤Ÿå®½ï¼Œä»¥å­¦ä¹ ä¸è¿é€šçš„å†³ç­–åŒºåŸŸã€‚å‘è¡¨äº
    *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œ2018ã€‚
- en: 'Nguyen and Huchette (2022) T.Â Nguyen and J.Â Huchette. Neural network verification
    as piecewise linear optimization: Formulations for the composition of staircase
    functions. *arXiv:2211.14706*, 2022.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen å’Œ Huchetteï¼ˆ2022ï¼‰T. Nguyen å’Œ J. Huchette. ç¥ç»ç½‘ç»œéªŒè¯ä½œä¸ºåˆ†æ®µçº¿æ€§ä¼˜åŒ–ï¼šé˜¶æ¢¯å‡½æ•°ç»„åˆçš„å…¬å¼ã€‚*arXiv:2211.14706*ï¼Œ2022ã€‚
- en: 'Novak etÂ al. (2018) R.Â Novak, Y.Â Bahri, D.Â A. Abolafia, J.Â Pennington, and
    J.Â Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical
    study. In *International Conference on Learning Representations (ICLR)*, 2018.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Novak ç­‰ï¼ˆ2018ï¼‰R. Novak, Y. Bahri, D. A. Abolafia, J. Pennington, å’Œ J. Sohl-Dickstein.
    ç¥ç»ç½‘ç»œä¸­çš„æ•æ„Ÿæ€§å’Œæ³›åŒ–ï¼šä¸€é¡¹å®è¯ç ”ç©¶ã€‚å‘è¡¨äº *å›½é™…å­¦ä¹ è¡¨ç¤ºä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2018ã€‚
- en: OpenAI (2022) OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt.
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAIï¼ˆ2022ï¼‰OpenAI. ä»‹ç» chatgptï¼Œ2022ã€‚ç½‘å€ https://openai.com/blog/chatgptã€‚
- en: OpenAI etÂ al. (2019) OpenAI, C.Â Berner, G.Â Brockman, B.Â Chan, V.Â Cheung, P.Â DÈ©biak,
    C.Â Dennison, D.Â Farhi, Q.Â Fischer, S.Â Hashme, C.Â Hesse, R.Â JÃ³zefowicz, S.Â Gray,
    C.Â Olsson, J.Â Pachocki, M.Â Petrov, H.Â P. deÂ OliveiraÂ Pinto, J.Â Raiman, T.Â Salimans,
    J.Â Schlatter, J.Â Schneider, S.Â Sidor, I.Â Sutskever, J.Â Tang, F.Â Wolski, and S.Â Zhang.
    Dota 2 with large scale deep reinforcement learning. *arXiv:1912.06680*, 2019.
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI ç­‰ï¼ˆ2019ï¼‰OpenAI, C. Berner, G. Brockman, B. Chan, V. Cheung, P. DÈ©biak,
    C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. JÃ³zefowicz, S. Gray,
    C. Olsson, J. Pachocki, M. Petrov, H. P. de Oliveira Pinto, J. Raiman, T. Salimans,
    J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, å’Œ S. Zhang.
    ä½¿ç”¨å¤§è§„æ¨¡æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ Dota 2ã€‚*arXiv:1912.06680*ï¼Œ2019ã€‚
- en: Padberg (2000) M.Â Padberg. Approximating separable nonlinear functions via mixed
    zero-one programs. *Operations Research Letters*, 27(1):1â€“5, 2000.
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Padbergï¼ˆ2000ï¼‰M. Padberg. é€šè¿‡æ··åˆé›¶ä¸€ç¨‹åºè¿‘ä¼¼å¯åˆ†ç¦»éçº¿æ€§å‡½æ•°ã€‚*è¿ç­¹å­¦é€šè®¯*ï¼Œ27(1):1â€“5ï¼Œ2000ã€‚
- en: Papalexopoulos etÂ al. (2022) T.Â P. Papalexopoulos, C.Â Tjandraatmadja, R.Â Anderson,
    J.Â P. Vielma, and D.Â Belanger. Constrained discrete black-box optimization using
    mixed-integer programming. In *International Conference on Machine Learning (ICML)*,
    volume 162, pages 17295â€“17322, 2022.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papalexopoulos ç­‰ï¼ˆ2022ï¼‰T. P. Papalexopoulos, C. Tjandraatmadja, R. Anderson,
    J. P. Vielma, å’Œ D. Belanger. ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’çš„å—é™ç¦»æ•£é»‘ç®±ä¼˜åŒ–ã€‚å‘è¡¨äº *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œå·162ï¼Œç¬¬17295â€“17322é¡µï¼Œ2022ã€‚
- en: 'Park etÂ al. (2019) D.Â S. Park, W.Â Chan, Y.Â Zhang, C.-C. Chiu, B.Â Zoph, E.Â D.
    Cubuk, and Q.Â V. Le. SpecAugment: A simple data augmentation method for automatic
    speech recognition. In *Interspeech*, 2019.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park ç­‰ï¼ˆ2019ï¼‰D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,
    å’Œ Q. V. Le. SpecAugmentï¼šä¸€ç§ç®€å•çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚å‘è¡¨äº *å›½é™…è¯­éŸ³é€šè®¯å¤§ä¼šï¼ˆInterspeechï¼‰*ï¼Œ2019ã€‚
- en: Park etÂ al. (2021a) S.Â Park, C.Â Yun, J.Â Lee, and J.Â Shin. Minimum width for
    universal approximation. In *International Conference on Learning Representations
    (ICLR)*, 2021a.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park ç­‰ï¼ˆ2021aï¼‰S. Park, C. Yun, J. Lee, å’Œ J. Shin. é€šç”¨è¿‘ä¼¼çš„æœ€å°å®½åº¦ã€‚å‘è¡¨äº *å›½é™…å­¦ä¹ è¡¨ç¤ºä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2021aã€‚
- en: Park etÂ al. (2021b) Y.Â Park, S.Â Lee, G.Â Kim, and D.Â M. Blei. Unsupervised representation
    learning via neural activation coding. In *International Conference on Machine
    Learning (ICML)*, 2021b.
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park ç­‰ï¼ˆ2021bï¼‰Y. Park, S. Lee, G. Kim, å’Œ D. M. Blei. é€šè¿‡ç¥ç»æ¿€æ´»ç¼–ç çš„æ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€‚å‘è¡¨äº *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œ2021bã€‚
- en: Pascanu etÂ al. (2014) R.Â Pascanu, G.Â MontÃºfar, and Y.Â Bengio. On the number
    of response regions of deep feedforward networks with piecewise linear activations.
    In *International Conference on Learning Representations (ICLR)*, 2014.
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pascanu ç­‰ï¼ˆ2014ï¼‰R. Pascanu, G. MontÃºfar, å’Œ Y. Bengio. å…³äºå…·æœ‰åˆ†æ®µçº¿æ€§æ¿€æ´»çš„æ·±åº¦å‰é¦ˆç½‘ç»œçš„å“åº”åŒºåŸŸæ•°é‡ã€‚å‘è¡¨äº
    *å›½é™…å­¦ä¹ è¡¨ç¤ºä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2014ã€‚
- en: Patrick L.Â Combettes (2019) J.-C.Â P. Patrick L.Â Combettes. Lipschitz certificates
    for layered network structures driven by averaged activation operators. *arXiv:1903.01014*,
    2019.
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patrick L. Combettesï¼ˆ2019ï¼‰J.-C. P. Patrick L. Combettes. ç”±å¹³å‡æ¿€æ´»ç®—å­é©±åŠ¨çš„å±‚æ¬¡ç½‘ç»œç»“æ„çš„ Lipschitz
    è¯ä¹¦ã€‚*arXiv:1903.01014*ï¼Œ2019ã€‚
- en: Perakis and Tsiourvas (2022) G.Â Perakis and A.Â Tsiourvas. Optimizing objective
    functions from trained relu neural networks via sampling. *arXiv:2205.14189*,
    2022.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perakis å’Œ Tsiourvasï¼ˆ2022ï¼‰G. Perakis å’Œ A. Tsiourvasã€‚é€šè¿‡é‡‡æ ·ä¼˜åŒ–è®­ç»ƒçš„ ReLU ç¥ç»ç½‘ç»œçš„ç›®æ ‡å‡½æ•°ã€‚*arXiv:2205.14189*ï¼Œ2022ã€‚
- en: Peters etÂ al. (2018) M.Â E. Peters, M.Â Neumann, M.Â Iyyer, M.Â Gardner, C.Â Clark,
    K.Â Lee, and L.Â Zettlemoyer. Deep contextualized word representations. In *Conference
    of the North American Chapter of the Association for Computational Linguistics
    (NAACL)*, 2018.
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters ç­‰ï¼ˆ2018ï¼‰M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
    å’Œ L. Zettlemoyerã€‚æ·±åº¦ä¸Šä¸‹æ–‡åŒ–è¯è¡¨å¾ã€‚è§ *åŒ—ç¾è®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šï¼ˆNAACLï¼‰*ï¼Œ2018ã€‚
- en: Phuong and Lampert (2020) M.Â Phuong and C.Â H. Lampert. Functional vs. parametric
    equivalence of ReLU networks. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phuong å’Œ Lampertï¼ˆ2020ï¼‰M. Phuong å’Œ C. H. Lampertã€‚ReLU ç½‘ç»œçš„åŠŸèƒ½ä¸å‚æ•°ç­‰æ•ˆæ€§ã€‚è§ *å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2020ã€‚
- en: 'Pilanci and Ergen (2020) M.Â Pilanci and T.Â Ergen. Neural networks are convex
    regularizers: Exact polynomial-time convex optimization formulations for two-layer
    networks. In *International Conference on Machine Learning (ICML)*, pages 7695â€“7705\.
    PMLR, 2020.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pilanci å’Œ Ergenï¼ˆ2020ï¼‰M. Pilanci å’Œ T. Ergenã€‚ç¥ç»ç½‘ç»œæ˜¯å‡¸æ­£åˆ™åŒ–å™¨ï¼šç”¨äºä¸¤å±‚ç½‘ç»œçš„ç²¾ç¡®å¤šé¡¹å¼æ—¶é—´å‡¸ä¼˜åŒ–å…¬å¼ã€‚è§
    *å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šï¼ˆICMLï¼‰*ï¼Œç¬¬ 7695â€“7705 é¡µã€‚PMLRï¼Œ2020ã€‚
- en: Pokutta etÂ al. (2020) S.Â Pokutta, C.Â Spiegel, and M.Â Zimmer. Deep neural network
    training with frank-wolfe. *arXiv:2010.07243*, 2020.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pokutta ç­‰ï¼ˆ2020ï¼‰S. Pokutta, C. Spiegel, å’Œ M. Zimmerã€‚ä½¿ç”¨ Frank-Wolfe æ–¹æ³•è¿›è¡Œæ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒã€‚*arXiv:2010.07243*ï¼Œ2020ã€‚
- en: Polyak (1964) B.Â T. Polyak. Some methods of speeding up the convergence of iteration
    methods. *USSR Computational Mathematics and Mathematical Physics*, 4:1â€“17, 1964.
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polyakï¼ˆ1964ï¼‰B. T. Polyakã€‚åŠ é€Ÿè¿­ä»£æ–¹æ³•æ”¶æ•›çš„ä¸€äº›æ–¹æ³•ã€‚*è‹è”è®¡ç®—æ•°å­¦ä¸æ•°å­¦ç‰©ç†*ï¼Œ4:1â€“17ï¼Œ1964ã€‚
- en: Pulina and Tacchella (2010) L.Â Pulina and A.Â Tacchella. An abstraction-refinement
    approach to verification of artificial neural networks. In *Computer Aided Verification
    (CAV)*, pages 243â€“257, 2010.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pulina å’Œ Tacchellaï¼ˆ2010ï¼‰L. Pulina å’Œ A. Tacchellaã€‚äººå·¥ç¥ç»ç½‘ç»œéªŒè¯çš„æŠ½è±¡-ç²¾åŒ–æ–¹æ³•ã€‚è§ *è®¡ç®—æœºè¾…åŠ©éªŒè¯ï¼ˆCAVï¼‰*ï¼Œç¬¬
    243â€“257 é¡µï¼Œ2010ã€‚
- en: Radford etÂ al. (2018) A.Â Radford, K.Â Narasimhan, T.Â Salimans, and I.Â Sutskever.
    Improving language understanding by generative pre-training. Technical report,
    OpenAI, 2018.
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford ç­‰ï¼ˆ2018ï¼‰A. Radford, K. Narasimhan, T. Salimans, å’Œ I. Sutskeverã€‚é€šè¿‡ç”Ÿæˆé¢„è®­ç»ƒæé«˜è¯­è¨€ç†è§£èƒ½åŠ›ã€‚æŠ€æœ¯æŠ¥å‘Šï¼ŒOpenAIï¼Œ2018ã€‚
- en: Raghu etÂ al. (2017) M.Â Raghu, B.Â Poole, J.Â Kleinberg, S.Â Ganguli, and J.Â Dickstein.
    On the expressive power of deep neural networks. In *International Conference
    on Machine Learning (ICML)*, 2017.
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raghu ç­‰ï¼ˆ2017ï¼‰M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, å’Œ J. Dicksteinã€‚æ·±åº¦ç¥ç»ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ã€‚è§
    *å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šï¼ˆICMLï¼‰*ï¼Œ2017ã€‚
- en: Raghunathan etÂ al. (2018) A.Â Raghunathan, J.Â Steinhardt, and P.Â S. Liang. Semidefinite
    relaxations for certifying robustness to adversarial examples. *Neural Information
    Processing Systems (NeurIPS)*, 31, 2018.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raghunathan ç­‰ï¼ˆ2018ï¼‰A. Raghunathan, J. Steinhardt, å’Œ P. S. Liangã€‚åŠæ­£å®šæ¾å¼›ç”¨äºè¯æ˜å¯¹æŠ—æ ·æœ¬çš„é²æ£’æ€§ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œ31ï¼Œ2018ã€‚
- en: Ramachandran etÂ al. (2018) P.Â Ramachandran, B.Â Zoph, and Q.Â V. Le. Searching
    for activation functions. In *ICLR Workshop Track*, 2018.
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandran ç­‰ï¼ˆ2018ï¼‰P. Ramachandran, B. Zoph, å’Œ Q. V. Leã€‚å¯»æ‰¾æ¿€æ´»å‡½æ•°ã€‚è§ *ICLR Workshop
    Track*ï¼Œ2018ã€‚
- en: Raman and Grossmann (1994) R.Â Raman and I.Â Grossmann. Modelling and computational
    techniques for logic based integer programming. *Computers & Chemical Engineering*,
    18(7):563â€“578, 1994.
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raman å’Œ Grossmannï¼ˆ1994ï¼‰R. Raman å’Œ I. Grossmannã€‚åŸºäºé€»è¾‘çš„æ•´æ•°è§„åˆ’çš„å»ºæ¨¡ä¸è®¡ç®—æŠ€æœ¯ã€‚*è®¡ç®—æœºä¸åŒ–å­¦å·¥ç¨‹*ï¼Œ18(7):563â€“578ï¼Œ1994ã€‚
- en: Ramesh etÂ al. (2022) A.Â Ramesh, P.Â Dhariwal, A.Â Nichol, C.Â Chu, and M.Â Chen.
    Hierarchical text-conditional image generation with CLIP latents. *arXiv:2204.06125*,
    2022.
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh ç­‰ï¼ˆ2022ï¼‰A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, å’Œ M. Chenã€‚åŸºäº CLIP æ½œå˜é‡çš„åˆ†å±‚æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆã€‚*arXiv:2204.06125*ï¼Œ2022ã€‚
- en: Robbins and Monro (1951) H.Â Robbins and S.Â Monro. A stochastic approximation
    method. *The Annals of Mathematical Statistics*, 22(3):400â€“407, 1951.
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robbins å’Œ Monroï¼ˆ1951ï¼‰H. Robbins å’Œ S. Monroã€‚éšæœºé€¼è¿‘æ–¹æ³•ã€‚*æ•°å­¦ç»Ÿè®¡å¹´åˆŠ*ï¼Œ22(3):400â€“407ï¼Œ1951ã€‚
- en: Robinson etÂ al. (2019) H.Â Robinson, A.Â Rasheed, and O.Â San. Dissecting deep
    neural networks. *arXiv:1910.03879*, 2019.
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robinson ç­‰ï¼ˆ2019ï¼‰H. Robinson, A. Rasheed, å’Œ O. Sanã€‚æ·±åº¦ç¥ç»ç½‘ç»œçš„å‰–æã€‚*arXiv:1910.03879*ï¼Œ2019ã€‚
- en: Rolnick and Kording (2020) D.Â Rolnick and K.Â Kording. Reverse-engineering deep
    ReLU networks. In *International Conference on Machine Learning (ICML)*, 2020.
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rolnick å’Œ Kordingï¼ˆ2020ï¼‰D. Rolnick å’Œ K. Kordingã€‚åå‘å·¥ç¨‹æ·±åº¦ ReLU ç½‘ç»œã€‚è§ *å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šï¼ˆICMLï¼‰*ï¼Œ2020ã€‚
- en: Rosenblatt (1957) F.Â Rosenblatt. The Perceptron â€” a perceiving and recognizing
    automaton. Technical Report 85-460-1, Cornell Aeronautical Laboratory, 1957.
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenblattï¼ˆ1957ï¼‰F. Rosenblatt. æ„ŸçŸ¥æœºâ€”â€”ä¸€ä¸ªæ„ŸçŸ¥å’Œè¯†åˆ«çš„è‡ªåŠ¨è£…ç½®ã€‚æŠ€æœ¯æŠ¥å‘Š 85-460-1ï¼Œåº·å¥ˆå°”èˆªç©ºå®éªŒå®¤ï¼Œ1957ã€‚
- en: RÃ¶ssig and Petkovic (2021) A.Â RÃ¶ssig and M.Â Petkovic. Advances in verification
    of relu neural networks. *Journal of Global Optimization*, 81:109â€“152, 2021.
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RÃ¶ssig å’Œ Petkovicï¼ˆ2021ï¼‰A. RÃ¶ssig å’Œ M. Petkovic. ReLU ç¥ç»ç½‘ç»œéªŒè¯çš„è¿›å±•ã€‚*å…¨çƒä¼˜åŒ–æ‚å¿—*ï¼Œ81:109â€“152ï¼Œ2021ã€‚
- en: Roth (2021) K.Â Roth. A primer on multi-neuron relaxation-based adversarial robustness
    certification. In *ICML 2021 Workshop on Adversarial Machine Learning*, 2021.
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rothï¼ˆ2021ï¼‰K. Roth. å¤šç¥ç»å…ƒæ¾å¼›åŸºç¡€å¯¹æŠ—é²æ£’æ€§è®¤è¯çš„å…¥é—¨æŒ‡å—ã€‚*ICML 2021 å¯¹æŠ—æœºå™¨å­¦ä¹ ç ”è®¨ä¼š*ï¼Œ2021ã€‚
- en: Roy etÂ al. (1993) A.Â Roy, L.Â S. Kim, and S.Â Mukhopadhyay. A polynomial time
    algorithm for the construction and training of a class of multilayer perceptrons.
    *Neural Networks*, 6(4):535â€“545, 1993.
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy ç­‰ï¼ˆ1993ï¼‰A. Roy, L. S. Kim å’Œ S. Mukhopadhyay. ä¸€ç§ç”¨äºæ„å»ºå’Œè®­ç»ƒä¸€ç±»å¤šå±‚æ„ŸçŸ¥å™¨çš„å¤šé¡¹å¼æ—¶é—´ç®—æ³•ã€‚*ç¥ç»ç½‘ç»œ*ï¼Œ6(4):535â€“545ï¼Œ1993ã€‚
- en: Rubies-Royo etÂ al. (2019) V.Â Rubies-Royo, R.Â Calandra, D.Â M. Stipanovic, and
    C.Â Tomlin. Fast neural network verification via shadow prices. *arXiv:1902.07247*,
    2019.
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubies-Royo ç­‰ï¼ˆ2019ï¼‰V. Rubies-Royo, R. Calandra, D. M. Stipanovic å’Œ C. Tomlin.
    é€šè¿‡å½±å­ä»·æ ¼è¿›è¡Œå¿«é€Ÿç¥ç»ç½‘ç»œéªŒè¯ã€‚*arXiv:1902.07247*, 2019ã€‚
- en: Rumelhart etÂ al. (1986) D.Â E. Rumelhart, G.Â E. Hinton, and R.Â J. Williams. Learning
    representations by back-propagating errors. *Nature*, 323:533â€“536, 1986.
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart ç­‰ï¼ˆ1986ï¼‰D. E. Rumelhart, G. E. Hinton å’Œ R. J. Williams. é€šè¿‡åå‘ä¼ æ’­é”™è¯¯å­¦ä¹ è¡¨å¾ã€‚*è‡ªç„¶*ï¼Œ323:533â€“536ï¼Œ1986ã€‚
- en: 'Ryu etÂ al. (2020) M.Â Ryu, Y.Â Chow, R.Â Anderson, C.Â Tjandraatmadja, and C.Â Boutilier.
    Caql: Continuous action q-learning. In *International Conference on Learning Representations
    (ICLR)*, 2020.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ryu ç­‰ï¼ˆ2020ï¼‰M. Ryu, Y. Chow, R. Anderson, C. Tjandraatmadja å’Œ C. Boutilier.
    Caql: è¿ç»­åŠ¨ä½œ Q å­¦ä¹ ã€‚*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2020ã€‚'
- en: 'Sahiner etÂ al. (2021) A.Â Sahiner, T.Â Ergen, J.Â M. Pauly, and M.Â Pilanci. Vector-output
    re{lu} neural network problems are copositive programs: Convex analysis of two
    layer networks and polynomial-time algorithms. In *International Conference on
    Learning Representations (ICLR)*, 2021.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahiner ç­‰ï¼ˆ2021ï¼‰A. Sahiner, T. Ergen, J. M. Pauly å’Œ M. Pilanci. å‘é‡è¾“å‡º ReLU ç¥ç»ç½‘ç»œé—®é¢˜æ˜¯å…±æ­£ç¨‹åºï¼šä¸¤å±‚ç½‘ç»œçš„å‡¸åˆ†æå’Œå¤šé¡¹å¼æ—¶é—´ç®—æ³•ã€‚*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2021ã€‚
- en: Salman etÂ al. (2019) H.Â Salman, G.Â Yang, H.Â Zhang, C.-J. Hsieh, and P.Â Zhang.
    A convex relaxation barrier to tight robustness verification of neural networks.
    *Neural Information Processing Systems (NeurIPS)*, 32, 2019.
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salman ç­‰ï¼ˆ2019ï¼‰H. Salman, G. Yang, H. Zhang, C.-J. Hsieh å’Œ P. Zhang. ä¸€ç§ç”¨äºç´§å¯†é²æ£’æ€§éªŒè¯çš„å‡¸æ¾å¼›éšœç¢ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œ32ï¼Œ2019ã€‚
- en: 'Sandler etÂ al. (2018) M.Â Sandler, A.Â Howard, M.Â Zhu, A.Â Zhmoginov, and L.-C.
    Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In *Conference on
    Computer Vision and Pattern Recognition (CVPR)*, pages 4510â€“4520, 2018.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sandler ç­‰ï¼ˆ2018ï¼‰M. Sandler, A. Howard, M. Zhu, A. Zhmoginov å’Œ L.-C. Chen. Mobilenetv2:
    å€’ç½®æ®‹å·®å’Œçº¿æ€§ç“¶é¢ˆã€‚*è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰*ï¼Œç¬¬4510â€“4520é¡µï¼Œ2018ã€‚'
- en: Sattelberg etÂ al. (2020) B.Â Sattelberg, R.Â Cavalieri, M.Â Kirby, C.Â Peterson,
    and R.Â Beveridge. Locally linear attributes of ReLU neural networks. *arXiv:2012.01940*,
    2020.
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sattelberg ç­‰ï¼ˆ2020ï¼‰B. Sattelberg, R. Cavalieri, M. Kirby, C. Peterson å’Œ R. Beveridge.
    ReLU ç¥ç»ç½‘ç»œçš„å±€éƒ¨çº¿æ€§å±æ€§ã€‚*arXiv:2012.01940*, 2020ã€‚
- en: Say etÂ al. (2017) B.Â Say, G.Â Wu, Y.Â Q. Zhou, and S.Â Sanner. Nonlinear hybrid
    planning with deep net learned transition models and mixed-integer linear programming.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, pages
    750â€“756, 2017.
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Say ç­‰ï¼ˆ2017ï¼‰B. Say, G. Wu, Y. Q. Zhou å’Œ S. Sanner. ä½¿ç”¨æ·±åº¦ç½‘ç»œå­¦ä¹ çš„è¿‡æ¸¡æ¨¡å‹å’Œæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’çš„éçº¿æ€§æ··åˆè§„åˆ’ã€‚*å›½é™…äººå·¥æ™ºèƒ½è”åˆä¼šè®®ï¼ˆIJCAIï¼‰*ï¼Œç¬¬750â€“756é¡µï¼Œ2017ã€‚
- en: 'Schmidhuber (2015) J.Â Schmidhuber. Deep learning in neural networks: An overview.
    *Neural Networks*, 61:85â€“117, 2015.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidhuberï¼ˆ2015ï¼‰J. Schmidhuber. ç¥ç»ç½‘ç»œä¸­çš„æ·±åº¦å­¦ä¹ ï¼šæ¦‚è¿°ã€‚*ç¥ç»ç½‘ç»œ*ï¼Œ61:85â€“117ï¼Œ2015ã€‚
- en: Schumann etÂ al. (2003) J.Â Schumann, P.Â Gupta, and S.Â Nelson. On verification
    & validation of neural network based controllers. In *Engineering Applications
    of Neural Networks (EANN)*, 2003.
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schumann ç­‰ï¼ˆ2003ï¼‰J. Schumann, P. Gupta å’Œ S. Nelson. å…³äºåŸºäºç¥ç»ç½‘ç»œæ§åˆ¶å™¨çš„éªŒè¯ä¸ç¡®è®¤ã€‚*å·¥ç¨‹åº”ç”¨ç¥ç»ç½‘ç»œï¼ˆEANNï¼‰*ï¼Œ2003ã€‚
- en: Schwan etÂ al. (2022) R.Â Schwan, C.Â N. Jones, and D.Â Kuhn. Stability verification
    of neural network controllers using mixed-integer programming. *arXiv:2206.13374*,
    2022.
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwan ç­‰ï¼ˆ2022ï¼‰R. Schwan, C. N. Jones å’Œ D. Kuhn. ä½¿ç”¨æ··åˆæ•´æ•°ç¼–ç¨‹è¿›è¡Œç¥ç»ç½‘ç»œæ§åˆ¶å™¨çš„ç¨³å®šæ€§éªŒè¯ã€‚*arXiv:2206.13374*,
    2022ã€‚
- en: Schweidtmann and Mitsos (2019) A.Â M. Schweidtmann and A.Â Mitsos. Deterministic
    global optimization with artificial neural networks embedded. *Journal of Optimization
    Theory and Applications*, 180(3):925â€“948, 2019.
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schweidtmannå’ŒMitsosï¼ˆ2019ï¼‰A. M. Schweidtmann å’Œ A. Mitsosã€‚åµŒå…¥äººå·¥ç¥ç»ç½‘ç»œçš„ç¡®å®šæ€§å…¨å±€ä¼˜åŒ–ã€‚*Journal
    of Optimization Theory and Applications*ï¼Œ180(3):925â€“948ï¼Œ2019å¹´ã€‚
- en: Schweidtmann etÂ al. (2022) A.Â M. Schweidtmann, J.Â M. Weber, C.Â Wende, L.Â Netze,
    and A.Â Mitsos. Obey validity limits of data-driven models through topological
    data analysis and one-class classification. *Optimization and Engineering*, 23(2):855â€“876,
    2022.
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schweidtmannç­‰äººï¼ˆ2022ï¼‰A. M. Schweidtmann, J. M. Weber, C. Wende, L. Netze, å’Œ A.
    Mitsosã€‚é€šè¿‡æ‹“æ‰‘æ•°æ®åˆ†æå’Œå•ç±»åˆ†ç±»éµå®ˆæ•°æ®é©±åŠ¨æ¨¡å‹çš„æœ‰æ•ˆæ€§é™åˆ¶ã€‚*Optimization and Engineering*ï¼Œ23(2):855â€“876ï¼Œ2022å¹´ã€‚
- en: Seck etÂ al. (2021) I.Â Seck, G.Â Loosli, and S.Â Canu. Linear program powered attack.
    In *International Joint Conference on Neural Networks (IJCNN)*, 2021.
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seckç­‰äººï¼ˆ2021ï¼‰I. Seck, G. Loosli, å’Œ S. Canuã€‚çº¿æ€§è§„åˆ’é©±åŠ¨çš„æ”»å‡»ã€‚åœ¨*International Joint Conference
    on Neural Networks (IJCNN)*ï¼Œ2021å¹´ã€‚
- en: Serra (2020) T.Â Serra. Enumerative branching with less repetition. In *International
    Conference on Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, pages 399â€“416\. Springer, 2020.
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serraï¼ˆ2020ï¼‰T. Serraã€‚å‡å°‘é‡å¤çš„æšä¸¾åˆ†æ”¯ã€‚åœ¨*International Conference on Integration of Constraint
    Programming, Artificial Intelligence, and Operations Research (CPAIOR)*ï¼Œé¡µç 399â€“416ã€‚Springerï¼Œ2020å¹´ã€‚
- en: Serra and Hooker (2020) T.Â Serra and J.Â Hooker. Compact representation of near-optimal
    integer programming solutions. *Mathematical Programming*, 182:199â€“232, 2020.
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serraå’ŒHookerï¼ˆ2020ï¼‰T. Serraå’ŒJ. Hookerã€‚æ¥è¿‘æœ€ä¼˜çš„æ•´æ•°è§„åˆ’è§£çš„ç´§å‡‘è¡¨ç¤ºã€‚*Mathematical Programming*ï¼Œ182:199â€“232ï¼Œ2020å¹´ã€‚
- en: Serra and Ramalingam (2020) T.Â Serra and S.Â Ramalingam. Empirical bounds on
    linear regions of deep rectifier networks. In *AAAI Conference on Artificial Intelligence*,
    2020.
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serraå’ŒRamalingamï¼ˆ2020ï¼‰T. Serraå’ŒS. Ramalingamã€‚æ·±åº¦æ•´æµç½‘ç»œçº¿æ€§åŒºåŸŸçš„ç»éªŒç•Œé™ã€‚åœ¨*AAAI Conference
    on Artificial Intelligence*ï¼Œ2020å¹´ã€‚
- en: Serra etÂ al. (2018) T.Â Serra, C.Â Tjandraatmadja, and S.Â Ramalingam. Bounding
    and counting linear regions of deep neural networks. In *International Conference
    on Machine Learning (ICML)*, 2018.
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serraç­‰äººï¼ˆ2018ï¼‰T. Serra, C. Tjandraatmadja, å’Œ S. Ramalingamã€‚ç•Œå®šå’Œè®¡æ•°æ·±åº¦ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒºåŸŸã€‚åœ¨*International
    Conference on Machine Learning (ICML)*ï¼Œ2018å¹´ã€‚
- en: Serra etÂ al. (2020) T.Â Serra, A.Â Kumar, and S.Â Ramalingam. Lossless compression
    of deep neural networks. In *International Conference on the Integration of Constraint
    Programming, Artificial Intelligence, and Operations Research (CPAIOR)*, 2020.
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serraç­‰äººï¼ˆ2020ï¼‰T. Serra, A. Kumar, å’Œ S. Ramalingamã€‚æ·±åº¦ç¥ç»ç½‘ç»œçš„æ— æŸå‹ç¼©ã€‚åœ¨*International
    Conference on the Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*ï¼Œ2020å¹´ã€‚
- en: Serra etÂ al. (2021) T.Â Serra, X.Â Yu, A.Â Kumar, and S.Â Ramalingam. Scaling up
    exact neural network compression by ReLU stability. In *Neural Information Processing
    Systems (NeurIPS)*, volumeÂ 34, 2021.
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serraç­‰äººï¼ˆ2021ï¼‰T. Serra, X. Yu, A. Kumar, å’Œ S. Ramalingamã€‚é€šè¿‡ReLUç¨³å®šæ€§æ‰©å±•ç²¾ç¡®çš„ç¥ç»ç½‘ç»œå‹ç¼©ã€‚åœ¨*Neural
    Information Processing Systems (NeurIPS)*ï¼Œç¬¬34å·ï¼Œ2021å¹´ã€‚
- en: Shi etÂ al. (2022) C.Â Shi, M.Â Emadikhiav, L.Â Lozano, and D.Â Bergman. Careful!
    training relevance is real. *arXiv:2201.04429*, 2022.
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shiç­‰äººï¼ˆ2022ï¼‰C. Shi, M. Emadikhiav, L. Lozano, å’Œ D. Bergmanã€‚å°å¿ƒï¼è®­ç»ƒç›¸å…³æ€§æ˜¯çœŸå®çš„ã€‚*arXiv:2201.04429*ï¼Œ2022å¹´ã€‚
- en: 'Sidrane etÂ al. (2022) C.Â Sidrane, A.Â Maleki, A.Â Irfan, and M.Â J. Kochenderfer.
    Overt: An algorithm for safety verification of neural network control policies
    for nonlinear systems. *Journal of Machine Learning Research*, 23(117):1â€“45, 2022.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sidraneç­‰äººï¼ˆ2022ï¼‰C. Sidrane, A. Maleki, A. Irfan, å’Œ M. J. Kochenderferã€‚OVERTï¼šä¸€ç§ç”¨äºéçº¿æ€§ç³»ç»Ÿç¥ç»ç½‘ç»œæ§åˆ¶ç­–ç•¥å®‰å…¨éªŒè¯çš„ç®—æ³•ã€‚*Journal
    of Machine Learning Research*ï¼Œ23(117):1â€“45ï¼Œ2022å¹´ã€‚
- en: Sildir and Aydin (2022) H.Â Sildir and E.Â Aydin. A mixed-integer linear programming
    based training and feature selection method for artificial neural networks using
    piece-wise linear approximations. *Chemical Engineering Science*, 249:117273,
    2022.
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sildirå’ŒAydinï¼ˆ2022ï¼‰H. Sildirå’ŒE. Aydinã€‚åŸºäºæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’çš„è®­ç»ƒå’Œç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºä½¿ç”¨åˆ†æ®µçº¿æ€§é€¼è¿‘çš„äººå·¥ç¥ç»ç½‘ç»œã€‚*Chemical
    Engineering Science*ï¼Œ249:117273ï¼Œ2022å¹´ã€‚
- en: Silver etÂ al. (2017) D.Â Silver, J.Â Schrittwieser, K.Â Simonyan, I.Â Antonoglou,
    A.Â Huang, A.Â Guez, T.Â Hubert, L.Â Baker, M.Â Lai, A.Â Bolton, Y.Â Chen, T.Â Lillicrap,
    F.Â Hui, L.Â Sifre, G.Â vanÂ den Driessche, T.Â Graepel, and D.Â Hassabis. Mastering
    the game of go without human knowledge. *Nature*, 550:354â€“359, 2017.
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silverç­‰äººï¼ˆ2017ï¼‰D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
    A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui,
    L. Sifre, G. van den Driessche, T. Graepel, å’Œ D. Hassabisã€‚åœ¨æ²¡æœ‰äººç±»çŸ¥è¯†çš„æƒ…å†µä¸‹æŒæ¡å›´æ£‹ã€‚*Nature*ï¼Œ550:354â€“359ï¼Œ2017å¹´ã€‚
- en: Singh etÂ al. (2018) G.Â Singh, T.Â Gehr, M.Â Mirman, M.Â PÃ¼schel, and M.Â Vechev.
    Fast and effective robustness certification. *Neural Information Processing Systems
    (NeurIPS)*, 31, 2018.
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhç­‰äººï¼ˆ2018ï¼‰G. Singh, T. Gehr, M. Mirman, M. PÃ¼schel, å’Œ M. Vechevã€‚å¿«é€Ÿæœ‰æ•ˆçš„é²æ£’æ€§è®¤è¯ã€‚*Neural
    Information Processing Systems (NeurIPS)*ï¼Œ31ï¼Œ2018å¹´ã€‚
- en: Singh etÂ al. (2019a) G.Â Singh, R.Â Ganvir, M.Â PÃ¼schel, and M.Â Vechev. Beyond
    the single neuron convex barrier for neural network certification. *Neural Information
    Processing Systems (NeurIPS)*, 32, 2019a.
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh ç­‰ï¼ˆ2019aï¼‰G. Singh, R. Ganvir, M. PÃ¼schel å’Œ M. Vechev. è¶…è¶Šå•ä¸ªç¥ç»å…ƒçš„å‡¸éšœç¢è¿›è¡Œç¥ç»ç½‘ç»œè®¤è¯ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿ
    (NeurIPS)*ï¼Œ32ï¼Œ2019aã€‚
- en: Singh etÂ al. (2019b) G.Â Singh, T.Â Gehr, M.Â PÃ¼schel, and M.Â Vechev. An abstract
    domain for certifying neural networks. *Proceedings of the ACM on Programming
    Languages (POPL)*, 3:1â€“30, 2019b.
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh ç­‰ï¼ˆ2019bï¼‰G. Singh, T. Gehr, M. PÃ¼schel å’Œ M. Vechev. ç”¨äºè®¤è¯ç¥ç»ç½‘ç»œçš„æŠ½è±¡åŸŸã€‚*ACM ç¼–ç¨‹è¯­è¨€ä¼šè®®è®ºæ–‡é›†
    (POPL)*ï¼Œ3:1â€“30ï¼Œ2019bã€‚
- en: Singh etÂ al. (2021) H.Â Singh, M.Â P. Kumar, P.Â Torr, and K.Â D. Dvijotham. Overcoming
    the convex barrier for simplex inputs. In *Neural Information Processing Systems
    (NeurIPS)*, volumeÂ 34, 2021.
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh ç­‰ï¼ˆ2021ï¼‰H. Singh, M. P. Kumar, P. Torr å’Œ K. D. Dvijotham. å…‹æœç®€å•è¾“å…¥çš„å‡¸éšœç¢ã€‚å‘è¡¨äº
    *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿ (NeurIPS)*ï¼Œç¬¬34å·ï¼Œ2021ã€‚
- en: 'Smith and Winkler (2006) J.Â E. Smith and R.Â L. Winkler. The optimizerâ€™s curse:
    Skepticism and postdecision surprise in decision analysis. *Management Science*,
    52(3):311â€“322, 2006.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith å’Œ Winklerï¼ˆ2006ï¼‰J. E. Smith å’Œ R. L. Winkler. ä¼˜åŒ–å™¨çš„è¯…å’’ï¼šå†³ç­–åˆ†æä¸­çš„æ€€ç–‘ä¸»ä¹‰ä¸å†³ç­–åæƒŠè®¶ã€‚*ç®¡ç†ç§‘å­¦*ï¼Œ52(3):311â€“322ï¼Œ2006ã€‚
- en: 'Srivastava etÂ al. (2014) N.Â Srivastava, G.Â Hinton, A.Â Krizhevsky, I.Â Sutskever,
    and R.Â Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting.
    *Journal of Machine Learning Research*, 15(56):1929â€“1958, 2014.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava ç­‰ï¼ˆ2014ï¼‰N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever å’Œ R.
    Salakhutdinov. Dropoutï¼šé˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆçš„ç®€å•æ–¹æ³•ã€‚*æœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—*ï¼Œ15(56):1929â€“1958ï¼Œ2014ã€‚
- en: Strong etÂ al. (2021) C.Â A. Strong, H.Â Wu, A.Â ZeljiÄ‡, K.Â D. Julian, G.Â Katz,
    C.Â Barrett, and M.Â J. Kochenderfer. Global optimization of objective functions
    represented by ReLU networks. *Machine Learning*, pages 1â€“28, 2021.
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strong ç­‰ï¼ˆ2021ï¼‰C. A. Strong, H. Wu, A. ZeljiÄ‡, K. D. Julian, G. Katz, C. Barrett
    å’Œ M. J. Kochenderfer. ReLU ç½‘ç»œè¡¨ç¤ºçš„ç›®æ ‡å‡½æ•°çš„å…¨å±€ä¼˜åŒ–ã€‚*æœºå™¨å­¦ä¹ *ï¼Œç¬¬1â€“28é¡µï¼Œ2021ã€‚
- en: 'Strong etÂ al. (2022) C.Â A. Strong, S.Â M. Katz, A.Â L. Corso, and M.Â J. Kochenderfer.
    ZoPE: a fast optimizer for ReLU networks with low-dimensional inputs. In *NASA
    Formal Methods: 14th International Symposium, (NFM)*, pages 299â€“317\. Springer,
    2022.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strong ç­‰ï¼ˆ2022ï¼‰C. A. Strong, S. M. Katz, A. L. Corso å’Œ M. J. Kochenderfer. ZoPEï¼šä¸€ä¸ªé’ˆå¯¹ä½ç»´è¾“å…¥çš„
    ReLU ç½‘ç»œçš„å¿«é€Ÿä¼˜åŒ–å™¨ã€‚å‘è¡¨äº *NASA æ­£å¼æ–¹æ³•ï¼šç¬¬14å±Šå›½é™…ç ”è®¨ä¼š (NFM)*ï¼Œç¬¬299â€“317é¡µã€‚Springerï¼Œ2022ã€‚
- en: 'Sudjianto etÂ al. (2020) A.Â Sudjianto, W.Â Knauth, R.Â Singh, Z.Â Yang, and A.Â Zhang.
    Unwrapping the black box of deep ReLU networks: Interpretability, diagnostics,
    and simplification. *arXiv:2011.04041*, 2020.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sudjianto ç­‰ï¼ˆ2020ï¼‰A. Sudjianto, W. Knauth, R. Singh, Z. Yang å’Œ A. Zhang. æ­å¼€æ·±åº¦
    ReLU ç½‘ç»œçš„é»‘ç®±ï¼šå¯è§£é‡Šæ€§ã€è¯Šæ–­å’Œç®€åŒ–ã€‚*arXiv:2011.04041*ï¼Œ2020ã€‚
- en: Sutskever etÂ al. (2013) I.Â Sutskever, J.Â Martens, G.Â Dahl, and G.Â Hinton. On
    the importance of initialization and momentum in deep learning. In *International
    Conference on Machine Learning (ICML)*, 2013.
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever ç­‰ï¼ˆ2013ï¼‰I. Sutskever, J. Martens, G. Dahl å’Œ G. Hinton. åœ¨æ·±åº¦å­¦ä¹ ä¸­åˆå§‹åŒ–å’ŒåŠ¨é‡çš„é‡è¦æ€§ã€‚å‘è¡¨äº
    *å›½é™…æœºå™¨å­¦ä¹ ä¼šè®® (ICML)*ï¼Œ2013ã€‚
- en: Sutskever etÂ al. (2014) I.Â Sutskever, O.Â Vinyals, and Q.Â Le. Sequence to sequence
    learning with neural networks. In *Neural Information Processing Systems (NeurIPS)*,
    volumeÂ 27, 2014.
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever ç­‰ï¼ˆ2014ï¼‰I. Sutskever, O. Vinyals å’Œ Q. Le. ä½¿ç”¨ç¥ç»ç½‘ç»œçš„åºåˆ—åˆ°åºåˆ—å­¦ä¹ ã€‚å‘è¡¨äº *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿ
    (NeurIPS)*ï¼Œç¬¬27å·ï¼Œ2014ã€‚
- en: Szegedy etÂ al. (2014) C.Â Szegedy, W.Â Zaremba, I.Â Sutskever, J.Â Bruna, D.Â Erhan,
    I.Â Goodfellow, and R.Â Fergus. Intriguing properties of neural networks. In *International
    Conference on Learning Representations (ICLR)*, 2014.
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy ç­‰ï¼ˆ2014ï¼‰C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I.
    Goodfellow å’Œ R. Fergus. ç¥ç»ç½‘ç»œçš„æœ‰è¶£ç‰¹æ€§ã€‚å‘è¡¨äº *å­¦ä¹ è¡¨å¾å›½é™…ä¼šè®® (ICLR)*ï¼Œ2014ã€‚
- en: Szegedy etÂ al. (2015) C.Â Szegedy, W.Â Liu, Y.Â Jia, P.Â Sermanet, S.Â Reed, D.Â Anguelov,
    D.Â Erhan, V.Â Vanhoucke, and A.Â Rabinovich. Going deeper with convolutions. In
    *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2015.
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy ç­‰ï¼ˆ2015ï¼‰C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke å’Œ A. Rabinovich. æ·±å…¥å·ç§¯ç½‘ç»œã€‚å‘è¡¨äº *è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®® (CVPR)*ï¼Œ2015ã€‚
- en: 'Takai etÂ al. (2021) Y.Â Takai, A.Â Sannai, and M.Â Cordonnier. On the number of
    linear functions composing deep neural network: Towards a refined definition of
    neural networks complexity. In *International Conference on Artificial Intelligence
    and Statistics (AISTATS)*, 2021.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takai ç­‰ï¼ˆ2021ï¼‰Y. Takai, A. Sannai å’Œ M. Cordonnier. å…³äºæ„æˆæ·±åº¦ç¥ç»ç½‘ç»œçš„çº¿æ€§å‡½æ•°æ•°é‡ï¼šè¿ˆå‘ç¥ç»ç½‘ç»œå¤æ‚æ€§çš„ç²¾ç¡®å®šä¹‰ã€‚å‘è¡¨äº
    *å›½é™…äººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡ä¼šè®® (AISTATS)*ï¼Œ2021ã€‚
- en: Tao etÂ al. (2022) Q.Â Tao, L.Â Li, X.Â Huang, X.Â Xi, S.Â Wang, and J.Â A. Suykens.
    Piecewise linear neural networks and deep learning. *Nature Reviews Methods Primers*,
    2, 2022.
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao ç­‰ï¼ˆ2022ï¼‰Q. Tao, L. Li, X. Huang, X. Xi, S. Wang å’Œ J. A. Suykens. åˆ†æ®µçº¿æ€§ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‚*è‡ªç„¶è¯„è®ºæ–¹æ³•å…¥é—¨*ï¼Œ2ï¼Œ2022ã€‚
- en: Telgarsky (2015) M.Â Telgarsky. Representation benefits of deep feedforward networks.
    *arXiv:1509.08101*, 2015.
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Telgarsky (2015) M. Telgarsky. æ·±åº¦å‰é¦ˆç½‘ç»œçš„è¡¨ç¤ºä¼˜åŠ¿ã€‚*arXiv:1509.08101*ï¼Œ2015å¹´ã€‚
- en: Thorbjarnarson and Yorke-Smith (2020) T.Â Thorbjarnarson and N.Â Yorke-Smith.
    On training neural networks with mixed integer programming. *arXiv:2009.03825*,
    2020.
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thorbjarnarson å’Œ Yorke-Smith (2020) T. Thorbjarnarson å’Œ N. Yorke-Smith. å…³äºä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’è®­ç»ƒç¥ç»ç½‘ç»œã€‚*arXiv:2009.03825*ï¼Œ2020å¹´ã€‚
- en: Thorbjarnarson and Yorke-Smith (2023) T.Â Thorbjarnarson and N.Â Yorke-Smith.
    Optimal training of integer-valued neural networks with mixed integer programming.
    *PLOS One*, 18(2):e0261029, 2023.
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thorbjarnarson å’Œ Yorke-Smith (2023) T. Thorbjarnarson å’Œ N. Yorke-Smith. ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’å¯¹æ•´æ•°å€¼ç¥ç»ç½‘ç»œè¿›è¡Œä¼˜åŒ–è®­ç»ƒã€‚*PLOS
    One*ï¼Œ18(2):e0261029ï¼Œ2023å¹´ã€‚
- en: Tiwari and Konidaris (2022) S.Â Tiwari and G.Â Konidaris. Effects of data geometry
    in early deep learning. In *Neural Information Processing Systems (NeurIPS)*,
    2022.
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tiwari å’Œ Konidaris (2022) S. Tiwari å’Œ G. Konidaris. æ•°æ®å‡ ä½•åœ¨æ—©æœŸæ·±åº¦å­¦ä¹ ä¸­çš„å½±å“ã€‚åœ¨*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œ2022å¹´ã€‚
- en: 'Tjandraatmadja etÂ al. (2020) C.Â Tjandraatmadja, R.Â Anderson, J.Â Huchette, W.Â Ma,
    K.Â K. Patel, and J.Â P. Vielma. The convex relaxation barrier, revisited: Tightened
    single-neuron relaxations for neural network verification. *Neural Information
    Processing Systems (NeurIPS)*, 33:21675â€“21686, 2020.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tjandraatmadja et al. (2020) C. Tjandraatmadja, R. Anderson, J. Huchette, W.
    Ma, K. K. Patel, å’Œ J. P. Vielma. å‡¸æ¾å¼›å±éšœï¼Œé‡è®¿ï¼šé’ˆå¯¹ç¥ç»ç½‘ç»œéªŒè¯çš„æ”¶ç´§å•ç¥ç»å…ƒæ¾å¼›ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œ33:21675â€“21686ï¼Œ2020å¹´ã€‚
- en: Tjeng etÂ al. (2019) V.Â Tjeng, K.Â Xiao, and R.Â Tedrake. Evaluating robustness
    of neural networks with mixed integer programming. In *International Conference
    on Learning Representations (ICLR)*, 2019.
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tjeng et al. (2019) V. Tjeng, K. Xiao, å’Œ R. Tedrake. ä½¿ç”¨æ··åˆæ•´æ•°è§„åˆ’è¯„ä¼°ç¥ç»ç½‘ç»œçš„é²æ£’æ€§ã€‚åœ¨*å›½é™…å­¦ä¹ è¡¨ç¤ºå¤§ä¼šï¼ˆICLRï¼‰*ï¼Œ2019å¹´ã€‚
- en: 'Trimmel etÂ al. (2021) M.Â Trimmel, H.Â Petzka, and C.Â Sminchisescu. TropEx: An
    algorithm for extracting linear terms in deep neural networks. In *International
    Conference on Learning Representations (ICLR)*, 2021.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trimmel et al. (2021) M. Trimmel, H. Petzka, å’Œ C. Sminchisescu. TropExï¼šæå–æ·±åº¦ç¥ç»ç½‘ç»œä¸­çº¿æ€§é¡¹çš„ç®—æ³•ã€‚åœ¨*å›½é™…å­¦ä¹ è¡¨ç¤ºå¤§ä¼šï¼ˆICLRï¼‰*ï¼Œ2021å¹´ã€‚
- en: 'Tsay and Baldea (2019) C.Â Tsay and M.Â Baldea. 110th anniversary: using data
    to bridge the time and length scales of process systems. *Industrial & Engineering
    Chemistry Research*, 58(36):16696â€“16708, 2019.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsay å’Œ Baldea (2019) C. Tsay å’Œ M. Baldea. 110å‘¨å¹´çºªå¿µï¼šä½¿ç”¨æ•°æ®æ¡¥æ¥è¿‡ç¨‹ç³»ç»Ÿçš„æ—¶é—´å’Œé•¿åº¦å°ºåº¦ã€‚*å·¥ä¸šä¸å·¥ç¨‹åŒ–å­¦ç ”ç©¶*ï¼Œ58(36):16696â€“16708ï¼Œ2019å¹´ã€‚
- en: Tsay etÂ al. (2021) C.Â Tsay, J.Â Kronqvist, A.Â Thebelt, and R.Â Misener. Partition-based
    formulations for mixed-integer optimization of trained ReLU neural networks. In
    *Neural Information Processing Systems (NeurIPS)*, volumeÂ 34, 2021.
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsay et al. (2021) C. Tsay, J. Kronqvist, A. Thebelt, å’Œ R. Misener. é’ˆå¯¹è®­ç»ƒå¥½çš„ReLUç¥ç»ç½‘ç»œçš„æ··åˆæ•´æ•°ä¼˜åŒ–çš„åŸºäºåˆ†åŒºçš„å…¬å¼ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œç¬¬34å·ï¼Œ2021å¹´ã€‚
- en: Tseran and MontÃºfar (2021) H.Â Tseran and G.Â MontÃºfar. On the expected complexity
    of maxout networks. In *Neural Information Processing Systems (NeurIPS)*, volumeÂ 34,
    2021.
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tseran å’Œ MontÃºfar (2021) H. Tseran å’Œ G. MontÃºfar. å…³äºmaxoutç½‘ç»œçš„æœŸæœ›å¤æ‚åº¦ã€‚åœ¨*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œç¬¬34å·ï¼Œ2021å¹´ã€‚
- en: Unser (2019) M.Â Unser. A representer theorem for deep neural networks. *Journal
    of Machine Learning Research*, 20:1â€“30, 2019.
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unser (2019) M. Unser. æ·±åº¦ç¥ç»ç½‘ç»œçš„è¡¨ç¤ºå®šç†ã€‚*æœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—*ï¼Œ20:1â€“30ï¼Œ2019å¹´ã€‚
- en: Vaswani etÂ al. (2017) A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones,
    A.Â N. Gomez, L.Â Kaiser, and I.Â Polosukhin. Attention is all you need. In *Neural
    Information Processing Systems (NeurIPS)*, 2017.
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, å’Œ I. Polosukhin. æ³¨æ„åŠ›æœºåˆ¶æ‰æ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ã€‚åœ¨*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œ2017å¹´ã€‚
- en: Vielma (2015) J.Â P. Vielma. Mixed integer linear programming formulation techniques.
    *SIAM Review*, 57(1):3â€“57, 2015.
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vielma (2015) J. P. Vielma. æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’å…¬å¼åŒ–æŠ€æœ¯ã€‚*SIAM è¯„å®¡*ï¼Œ57(1):3â€“57ï¼Œ2015å¹´ã€‚
- en: Vielma (2019) J.Â P. Vielma. Small and strong formulations for unions of convex
    sets from the cayley embedding. *Mathematical Programming*, 177(1-2):21â€“53, 2019.
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vielma (2019) J. P. Vielma. ä»CayleyåµŒå…¥ä¸­å¾—åˆ°çš„å‡¸é›†å¹¶çš„ç®€æ´ä¸”å¼ºå¤§çš„å…¬å¼ã€‚*æ•°å­¦è§„åˆ’*ï¼Œ177(1-2):21â€“53ï¼Œ2019å¹´ã€‚
- en: 'Vielma etÂ al. (2010) J.Â P. Vielma, S.Â Ahmed, and G.Â Nemhauser. Mixed-integer
    models for nonseparable piecewise-linear optimization: Unifying framework and
    extensions. *Operations Research*, 58(2):303â€“315, 2010.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vielma et al. (2010) J. P. Vielma, S. Ahmed, å’Œ G. Nemhauser. éåˆ†ç¦»çš„åˆ†æ®µçº¿æ€§ä¼˜åŒ–çš„æ··åˆæ•´æ•°æ¨¡å‹ï¼šç»Ÿä¸€æ¡†æ¶åŠæ‰©å±•ã€‚*è¿ç­¹å­¦*ï¼Œ58(2):303â€“315ï¼Œ2010å¹´ã€‚
- en: Villani and Schoots (2023) M.Â J. Villani and N.Â Schoots. Any deep ReLU network
    is shallow. *arXiv:2306.11827*, 2023.
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Villani å’Œ Schoots (2023) M. J. Villani å’Œ N. Schoots. ä»»ä½•æ·±åº¦ReLUç½‘ç»œéƒ½æ˜¯æµ…å±‚çš„ã€‚*arXiv:2306.11827*ï¼Œ2023å¹´ã€‚
- en: 'Vincent and Schwager (2021) J.Â A. Vincent and M.Â Schwager. Reachable polyhedral
    marching (RPM): A safety verification algorithm for robotic systems with deep
    neural network components. In *IEEE International Conference on Robotics and Automation
    (ICRA)*, 2021.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent å’Œ Schwagerï¼ˆ2021ï¼‰J. A. Vincent å’Œ M. Schwager. å¯è¾¾çš„å¤šé¢ä½“è¡Œè¿›ï¼ˆRPMï¼‰ï¼šä¸€ç§ç”¨äºå…·æœ‰æ·±åº¦ç¥ç»ç½‘ç»œç»„ä»¶çš„æœºå™¨äººç³»ç»Ÿçš„å®‰å…¨éªŒè¯ç®—æ³•ã€‚åœ¨*IEEE
    å›½é™…æœºå™¨äººä¸è‡ªåŠ¨åŒ–ä¼šè®®ï¼ˆICRAï¼‰*ï¼Œ2021ã€‚
- en: 'Vinyals etÂ al. (2017) O.Â Vinyals, T.Â Ewalds, S.Â Bartunov, P.Â Georgiev, A.Â S.
    Vezhnevets, M.Â Yeo, A.Â Makhzani, H.Â KÃ¼ttler, J.Â Agapiou, J.Â Schrittwieser, J.Â Quan,
    S.Â Gaffney, S.Â Petersen, K.Â Simonyan, T.Â Schaul, H.Â van Hasselt, D.Â Silver, T.Â Lillicrap,
    K.Â Calderone, P.Â Keet, A.Â Brunasso, D.Â Lawrence, A.Â Ekermo, J.Â Repp, and R.Â Tsing.
    StarCraft II: A new challenge for reinforcement learning. *arXiv:1708.04782*,
    2017.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals ç­‰ï¼ˆ2017ï¼‰O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets,
    M. Yeo, A. Makhzani, H. KÃ¼ttler, J. Agapiou, J. Schrittwieser, J. Quan, S. Gaffney,
    S. Petersen, K. Simonyan, T. Schaul, H. van Hasselt, D. Silver, T. Lillicrap,
    K. Calderone, P. Keet, A. Brunasso, D. Lawrence, A. Ekermo, J. Repp, å’Œ R. Tsing.
    ã€ŠStarCraft IIï¼šå¼ºåŒ–å­¦ä¹ çš„æ–°æŒ‘æˆ˜ã€‹ã€‚*arXiv:1708.04782*ï¼Œ2017ã€‚
- en: 'Virmaux and Scaman (2018) A.Â Virmaux and K.Â Scaman. Lipschitz regularity of
    deep neural networks: analysis and efficient estimation. In *Neural Information
    Processing Systems (NeurIPS)*, volumeÂ 31, 2018.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Virmaux å’Œ Scamanï¼ˆ2018ï¼‰A. Virmaux å’Œ K. Scaman. æ·±åº¦ç¥ç»ç½‘ç»œçš„ Lipschitz æ­£åˆ™æ€§ï¼šåˆ†æä¸é«˜æ•ˆä¼°è®¡ã€‚åœ¨*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œç¬¬31å·ï¼Œ2018ã€‚
- en: Volpp etÂ al. (2020) M.Â Volpp, L.Â P. FrÃ¶hlich, K.Â Fischer, A.Â Doerr, S.Â Falkner,
    F.Â Hutter, and C.Â Daniel. Meta-learning acquisition functions for transfer learning
    in bayesian optimization. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Volpp ç­‰ï¼ˆ2020ï¼‰M. Volpp, L. P. FrÃ¶hlich, K. Fischer, A. Doerr, S. Falkner, F.
    Hutter, å’Œ C. Daniel. å…ƒå­¦ä¹ è·å–å‡½æ•°ç”¨äºè´å¶æ–¯ä¼˜åŒ–ä¸­çš„è¿ç§»å­¦ä¹ ã€‚åœ¨*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2020ã€‚
- en: Wang etÂ al. (2021) K.Â Wang, L.Â Lozano, D.Â Bergman, and C.Â Cardonha. A two-stage
    exact algorithm for optimization of neural network ensemble. In *International
    Conference on the Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, 2021.
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang ç­‰ï¼ˆ2021ï¼‰K. Wang, L. Lozano, D. Bergman, å’Œ C. Cardonha. ç”¨äºç¥ç»ç½‘ç»œé›†æˆä¼˜åŒ–çš„ä¸¤é˜¶æ®µç²¾ç¡®ç®—æ³•ã€‚åœ¨*çº¦æŸç¼–ç¨‹ã€äººå·¥æ™ºèƒ½ä¸è¿ç­¹å­¦æ•´åˆå›½é™…ä¼šè®®ï¼ˆCPAIORï¼‰*ï¼Œ2021ã€‚
- en: Wang etÂ al. (2023) K.Â Wang, L.Â Lozano, C.Â Cardonha, and D.Â Bergman. Optimizing
    over an ensemble of trained neural networks. *INFORMS Journal on Computing*, 2023.
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang ç­‰ï¼ˆ2023ï¼‰K. Wang, L. Lozano, C. Cardonha, å’Œ D. Bergman. å¯¹è®­ç»ƒç¥ç»ç½‘ç»œé›†æˆè¿›è¡Œä¼˜åŒ–ã€‚*INFORMS
    è®¡ç®—æœŸåˆŠ*ï¼Œ2023ã€‚
- en: Wang etÂ al. (2018a) S.Â Wang, K.Â Pei, J.Â Whitehouse, J.Â Yang, and S.Â Jana. Efficient
    formal safety analysis of neural networks. *Neural Information Processing Systems
    (NeurIPS)*, 31, 2018a.
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang ç­‰ï¼ˆ2018aï¼‰S. Wang, K. Pei, J. Whitehouse, J. Yang, å’Œ S. Jana. ç¥ç»ç½‘ç»œçš„é«˜æ•ˆæ­£å¼å®‰å…¨åˆ†æã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œç¬¬31å·ï¼Œ2018aã€‚
- en: Wang etÂ al. (2018b) S.Â Wang, K.Â Pei, J.Â Whitehouse, J.Â Yang, and S.Â Jana. Formal
    security analysis of neural networks using symbolic intervals. In *27th $\{$USENIX$\}$
    Security Symposium ($\{$USENIX$\}$ Security 18)*, pages 1599â€“1614, 2018b.
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang ç­‰ï¼ˆ2018bï¼‰S. Wang, K. Pei, J. Whitehouse, J. Yang, å’Œ S. Jana. ä½¿ç”¨ç¬¦å·åŒºé—´çš„ç¥ç»ç½‘ç»œæ­£å¼å®‰å…¨åˆ†æã€‚åœ¨*ç¬¬27å±Š
    $\{$USENIX$\}$ å®‰å…¨ç ”è®¨ä¼š ($\{$USENIX$\}$ å®‰å…¨ 18)*ï¼Œç¬¬1599â€“1614é¡µï¼Œ2018bã€‚
- en: Wang (2022) Y.Â Wang. Estimation and comparison of linear regions for relu networks.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, 2022.
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wangï¼ˆ2022ï¼‰Y. Wang. Relu ç½‘ç»œçš„çº¿æ€§åŒºåŸŸçš„ä¼°è®¡ä¸æ¯”è¾ƒã€‚åœ¨*å›½é™…äººå·¥æ™ºèƒ½è”åˆä¼šè®®ï¼ˆIJCAIï¼‰*ï¼Œ2022ã€‚
- en: 'Weng etÂ al. (1992) J.Â Weng, N.Â Ahuja, and T.Â Huang. Cresceptron: a self-organizing
    neural network which grows adaptively. In *International Joint Conference on Neural
    Networks (IJCNN)*, 1992.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng ç­‰ï¼ˆ1992ï¼‰J. Weng, N. Ahuja, å’Œ T. Huang. Cresceptronï¼šä¸€ç§è‡ªç»„ç»‡çš„è‡ªé€‚åº”å¢é•¿ç¥ç»ç½‘ç»œã€‚åœ¨*å›½é™…ç¥ç»ç½‘ç»œè”åˆä¼šè®®ï¼ˆIJCNNï¼‰*ï¼Œ1992ã€‚
- en: Weng etÂ al. (2018) L.Â Weng, H.Â Zhang, H.Â Chen, Z.Â Song, C.-J. Hsieh, L.Â Daniel,
    D.Â Boning, and I.Â Dhillon. Towards fast computation of certified robustness for
    ReLU networks. In *International Conference on Machine Learning (ICML)*, pages
    5276â€“5285, 2018.
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng ç­‰ï¼ˆ2018ï¼‰L. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel, D.
    Boning, å’Œ I. Dhillon. è¿ˆå‘å¯¹ ReLU ç½‘ç»œè®¤è¯é²æ£’æ€§çš„å¿«é€Ÿè®¡ç®—ã€‚åœ¨*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œç¬¬5276â€“5285é¡µï¼Œ2018ã€‚
- en: 'Werbos (1974) P.Â Werbos. *Beyond Regression: New Tools for Prediction and Analysis
    in the Behavioral Sciences*. PhD thesis, Harvard University, 1974.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Werbosï¼ˆ1974ï¼‰P. Werbos. *è¶…è¶Šå›å½’ï¼šè¡Œä¸ºç§‘å­¦ä¸­é¢„æµ‹å’Œåˆ†æçš„æ–°å·¥å…·*ã€‚åšå£«è®ºæ–‡ï¼Œå“ˆä½›å¤§å­¦ï¼Œ1974ã€‚
- en: Wicker etÂ al. (2020) M.Â Wicker, L.Â Laurenti, A.Â Patane, and M.Â Kwiatkowska.
    Probabilistic safety for Bayesian neural networks. In *Conference on Uncertainty
    in Artificial Intelligence (UAI)*, pages 1198â€“1207, 2020.
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wicker ç­‰ï¼ˆ2020ï¼‰M. Wicker, L. Laurenti, A. Patane, å’Œ M. Kwiatkowska. è´å¶æ–¯ç¥ç»ç½‘ç»œçš„æ¦‚ç‡å®‰å…¨æ€§ã€‚åœ¨*äººå·¥æ™ºèƒ½ä¸ç¡®å®šæ€§ä¼šè®®ï¼ˆUAIï¼‰*ï¼Œç¬¬1198â€“1207é¡µï¼Œ2020ã€‚
- en: Wicker etÂ al. (2022) M.Â Wicker, J.Â Heo, L.Â Costabello, and A.Â Weller. Robust
    explanation constraints for neural networks. *arXiv:2212.08507*, 2022.
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wicker ç­‰äººï¼ˆ2022ï¼‰M. Wicker, J. Heo, L. Costabello, å’Œ A. Wellerã€‚ç”¨äºç¥ç»ç½‘ç»œçš„é²æ£’è§£é‡Šçº¦æŸã€‚*arXiv:2212.08507*ï¼Œ2022å¹´ã€‚
- en: Wilhelm etÂ al. (2022) M.Â E. Wilhelm, C.Â Wang, and M.Â D. Stuber. Convex and concave
    envelopes of artificial neural network activation functions for deterministic
    global optimization. *Journal of Global Optimization*, pages 1â€“26, 2022.
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilhelm ç­‰äººï¼ˆ2022ï¼‰M. E. Wilhelm, C. Wang, å’Œ M. D. Stuberã€‚ç”¨äºç¡®å®šæ€§å…¨å±€ä¼˜åŒ–çš„äººå·¥ç¥ç»ç½‘ç»œæ¿€æ´»å‡½æ•°çš„å‡¸åŒ…å’Œå‡¹åŒ…ã€‚*å…¨å±€ä¼˜åŒ–æœŸåˆŠ*ï¼Œç¬¬1â€“26é¡µï¼Œ2022å¹´ã€‚
- en: Wong and Kolter (2018) E.Â Wong and Z.Â Kolter. Provable defenses against adversarial
    examples via the convex outer adversarial polytope. In *International Conference
    on Machine Learning (ICML)*, pages 5286â€“5295, 2018.
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong å’Œ Kolterï¼ˆ2018ï¼‰E. Wong å’Œ Z. Kolterã€‚é€šè¿‡å‡¸å¤–å¯¹æŠ—å¤šé¢ä½“æä¾›å¯è¯æ˜çš„å¯¹æŠ—é˜²å¾¡ã€‚å‘è¡¨äº*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œç¬¬5286â€“5295é¡µï¼Œ2018å¹´ã€‚
- en: Wong etÂ al. (2018) E.Â Wong, F.Â Schmidt, J.Â H. Metzen, and J.Â Z. Kolter. Scaling
    provable adversarial defenses. *Neural Information Processing Systems (NeurIPS)*,
    31, 2018.
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong ç­‰äººï¼ˆ2018ï¼‰E. Wong, F. Schmidt, J. H. Metzen, å’Œ J. Z. Kolterã€‚æ‰©å±•å¯è¯æ˜çš„å¯¹æŠ—é˜²å¾¡ã€‚*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿï¼ˆNeurIPSï¼‰*ï¼Œç¬¬31å·ï¼Œ2018å¹´ã€‚
- en: Wright (2018) S.Â J. Wright. Optimization algorithms for data analysis. *The
    Mathematics of Data*, 25:49, 2018.
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wrightï¼ˆ2018ï¼‰S. J. Wrightã€‚æ•°æ®åˆ†æçš„ä¼˜åŒ–ç®—æ³•ã€‚*æ•°æ®æ•°å­¦*ï¼Œ25:49ï¼Œ2018å¹´ã€‚
- en: Wu etÂ al. (2020) G.Â Wu, B.Â Say, and S.Â Sanner. Scalable planning with deep neural
    network learned transition models. *Journal of Artificial Intelligence Research*,
    68:571â€“606, 2020.
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu ç­‰äººï¼ˆ2020ï¼‰G. Wu, B. Say, å’Œ S. Sannerã€‚åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå­¦ä¹ çš„è½¬ç§»æ¨¡å‹è¿›è¡Œå¯æ‰©å±•è§„åˆ’ã€‚*äººå·¥æ™ºèƒ½ç ”ç©¶æœŸåˆŠ*ï¼Œ68:571â€“606ï¼Œ2020å¹´ã€‚
- en: Wu etÂ al. (2022) H.Â Wu, A.Â ZeljiÄ‡, G.Â Katz, and C.Â Barrett. Efficient neural
    network analysis with sum-of-infeasibilities. In *Tools and Algorithms for the
    Construction and Analysis of Systems (TACAS)*, pages 143â€“163\. Springer, 2022.
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu ç­‰äººï¼ˆ2022ï¼‰H. Wu, A. ZeljiÄ‡, G. Katz, å’Œ C. Barrettã€‚åˆ©ç”¨ä¸åˆè§„æ€§çš„å’Œè¿›è¡Œé«˜æ•ˆçš„ç¥ç»ç½‘ç»œåˆ†æã€‚å‘è¡¨äº*å·¥å…·ä¸ç®—æ³•ç³»ç»Ÿçš„æ„å»ºä¸åˆ†æï¼ˆTACASï¼‰*ï¼Œç¬¬143â€“163é¡µï¼ŒSpringerï¼Œ2022å¹´ã€‚
- en: Xiang etÂ al. (2017) W.Â Xiang, H.-D. Tran, and T.Â T. Johnson. Reachable set computation
    and safety verification for neural networks with ReLU activations. *arXiv:1712.08163*,
    2017.
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang ç­‰äººï¼ˆ2017ï¼‰W. Xiang, H.-D. Tran, å’Œ T. T. Johnsonã€‚å…·æœ‰ ReLU æ¿€æ´»çš„ç¥ç»ç½‘ç»œçš„å¯è¾¾é›†è®¡ç®—å’Œå®‰å…¨æ€§éªŒè¯ã€‚*arXiv:1712.08163*ï¼Œ2017å¹´ã€‚
- en: Xiao etÂ al. (2019) K.Â Xiao, V.Â Tjeng, N.Â Shafiullah, and A.Â Madry. Training
    for faster adversarial robustness verification via inducing ReLU stability. *International
    Conference on Learning Representations (ICLR)*, 2019.
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao ç­‰äººï¼ˆ2019ï¼‰K. Xiao, V. Tjeng, N. Shafiullah, å’Œ A. Madryã€‚é€šè¿‡å¼•å…¥ ReLU ç¨³å®šæ€§æ¥è®­ç»ƒä»¥åŠ é€Ÿå¯¹æŠ—é²æ£’æ€§éªŒè¯ã€‚*å›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼ˆICLRï¼‰*ï¼Œ2019å¹´ã€‚
- en: Xie etÂ al. (2020a) J.Â Xie, Z.Â Shen, C.Â Zhang, B.Â Wang, and H.Â Qian. Efficient
    projection-free online methods with stochastic recursive gradient. In *AAAI Conference
    on Artificial Intelligence*, volumeÂ 34, pages 6446â€“6453, 2020a.
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie ç­‰äººï¼ˆ2020aï¼‰J. Xie, Z. Shen, C. Zhang, B. Wang, å’Œ H. Qianã€‚æ— éœ€æŠ•å½±çš„é«˜æ•ˆåœ¨çº¿æ–¹æ³•ä¸éšæœºé€’å½’æ¢¯åº¦ã€‚å‘è¡¨äº*AAAI
    äººå·¥æ™ºèƒ½ä¼šè®®*ï¼Œç¬¬34å·ï¼Œç¬¬6446â€“6453é¡µï¼Œ2020aå¹´ã€‚
- en: Xie etÂ al. (2020b) Q.Â Xie, M.-T. Luong, E.Â Hovy, and Q.Â V. Le. Self-training
    with noisy student improves ImageNet classification. In *Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2020b.
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie ç­‰äººï¼ˆ2020bï¼‰Q. Xie, M.-T. Luong, E. Hovy, å’Œ Q. V. Leã€‚å¸¦å™ªå£°å­¦ç”Ÿçš„è‡ªæˆ‘è®­ç»ƒæå‡äº† ImageNet
    åˆ†ç±»ã€‚å‘è¡¨äº*è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰*ï¼Œ2020bå¹´ã€‚
- en: Xie etÂ al. (2020c) Y.Â Xie, G.Â Chen, and Q.Â Li. A general computational framework
    to measure the expressiveness of complex networks using a tighter upper bound
    of linear regions. *arXiv:2012.04428*, 2020c.
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie ç­‰äººï¼ˆ2020cï¼‰Y. Xie, G. Chen, å’Œ Q. Liã€‚ä½¿ç”¨æ›´ç´§çš„çº¿æ€§åŒºåŸŸä¸Šç•Œæ¥æµ‹é‡å¤æ‚ç½‘ç»œçš„è¡¨ç°åŠ›çš„ä¸€èˆ¬è®¡ç®—æ¡†æ¶ã€‚*arXiv:2012.04428*ï¼Œ2020cå¹´ã€‚
- en: Xiong etÂ al. (2020) H.Â Xiong, L.Â Huang, M.Â Yu, L.Â Liu, F.Â Zhu, and L.Â Shao.
    On the number of linear regions of convolutional neural networks. In *International
    Conference on Machine Learning (ICML)*, 2020.
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong ç­‰äººï¼ˆ2020ï¼‰H. Xiong, L. Huang, M. Yu, L. Liu, F. Zhu, å’Œ L. Shaoã€‚å·ç§¯ç¥ç»ç½‘ç»œçš„çº¿æ€§åŒºåŸŸæ•°é‡ã€‚å‘è¡¨äº*å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICMLï¼‰*ï¼Œ2020å¹´ã€‚
- en: Xu etÂ al. (2022) S.Â Xu, J.Â Vaughan, J.Â Chen, A.Â Zhang, and A.Â Sudjianto. Traversing
    the local polytopes of ReLU neural networks. In *AAAI Workshop AdvML*, 2022.
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu ç­‰äººï¼ˆ2022ï¼‰S. Xu, J. Vaughan, J. Chen, A. Zhang, å’Œ A. Sudjiantoã€‚éå† ReLU ç¥ç»ç½‘ç»œçš„å±€éƒ¨å¤šé¢ä½“ã€‚å‘è¡¨äº*AAAI
    Workshop AdvML*ï¼Œ2022å¹´ã€‚
- en: Yang etÂ al. (2022) D.Â Yang, P.Â Balaprakash, and S.Â Leyffer. Modeling design
    and control problems involving neural network surrogates. *Computational Optimization
    and Applications*, pages 1â€“42, 2022.
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang ç­‰äººï¼ˆ2022ï¼‰D. Yang, P. Balaprakash, å’Œ S. Leyfferã€‚æ¶‰åŠç¥ç»ç½‘ç»œæ›¿ä»£æ¨¡å‹çš„å»ºæ¨¡è®¾è®¡ä¸æ§åˆ¶é—®é¢˜ã€‚*è®¡ç®—ä¼˜åŒ–ä¸åº”ç”¨*ï¼Œç¬¬1â€“42é¡µï¼Œ2022å¹´ã€‚
- en: Yang etÂ al. (2020) X.Â Yang, H.-D. Tran, W.Â Xiang, and T.Â Johnson. Reachability
    analysis for feed-forward neural networks using face lattices. *arXiv:2003.01226*,
    2020.
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang ç­‰ï¼ˆ2020ï¼‰X. Yang, H.-D. Tran, W. Xiang, å’Œ T. Johnson. ä½¿ç”¨é¢æ ¼å¯¹å‰é¦ˆç¥ç»ç½‘ç»œçš„å¯è¾¾æ€§åˆ†æã€‚*arXiv:2003.01226*ï¼Œ2020ã€‚
- en: Yang etÂ al. (2021) X.Â Yang, T.Â Yamaguchi, H.-D. Tran, B.Â Hoxha, T.Â T. Johnson,
    and D.Â Prokhorov. Reachability analysis of convolutional neural networks. *arXiv:2106.12074*,
    2021.
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang ç­‰ï¼ˆ2021ï¼‰X. Yang, T. Yamaguchi, H.-D. Tran, B. Hoxha, T. T. Johnson, å’Œ D.
    Prokhorov. å·ç§¯ç¥ç»ç½‘ç»œçš„å¯è¾¾æ€§åˆ†æã€‚*arXiv:2106.12074*ï¼Œ2021ã€‚
- en: Yarotsky (2017) D.Â Yarotsky. Error bounds for approximations with deep ReLU
    networks. *Neural Networks*, 94, 2017.
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yarotskyï¼ˆ2017ï¼‰D. Yarotsky. æ·±åº¦ ReLU ç½‘ç»œçš„è¿‘ä¼¼è¯¯å·®ç•Œé™ã€‚*Neural Networks*ï¼Œ94ï¼Œ2017ã€‚
- en: Zakrzewski (2001) R.Â R. Zakrzewski. Verification of a trained neural network
    accuracy. In *International Joint Conference on Neural Networks (IJCNN)*, volumeÂ 3,
    pages 1657â€“1662\. IEEE, 2001.
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zakrzewskiï¼ˆ2001ï¼‰R. R. Zakrzewski. è®­ç»ƒç¥ç»ç½‘ç»œå‡†ç¡®æ€§çš„éªŒè¯ã€‚æ”¶å½•äº *International Joint Conference
    on Neural Networks (IJCNN)*ï¼Œç¬¬ 3 å·ï¼Œé¡µé¢ 1657â€“1662ã€‚IEEEï¼Œ2001ã€‚
- en: 'Zaslavsky (1975) T.Â Zaslavsky. *Facing Up to Arrangements: Face-Count Formulas
    for Partitions of Space by Hyperplanes*. American Mathematical Society, 1975.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaslavskyï¼ˆ1975ï¼‰T. Zaslavsky. *é¢å¯¹æ’åˆ—ï¼šè¶…å¹³é¢ç©ºé—´åˆ†å‰²çš„é¢è®¡æ•°å…¬å¼*ã€‚ç¾å›½æ•°å­¦å­¦ä¼šï¼Œ1975ã€‚
- en: Zhang etÂ al. (2023) A.Â Zhang, Z.Â C. Lipton, M.Â Li, and A.Â J. Smola. *Dive into
    Deep Learning*. 2023. https://d2l.ai.
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ï¼ˆ2023ï¼‰A. Zhang, Z. C. Lipton, M. Li, å’Œ A. J. Smola. *æ·±å…¥å­¦ä¹ *ã€‚2023ã€‚https://d2l.aiã€‚
- en: Zhang etÂ al. (2018a) H.Â Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L.Â Daniel.
    Efficient neural network robustness certification with general activation functions.
    *Neural Information Processing Systems (NeurIPS)*, 31, 2018a.
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ï¼ˆ2018aï¼‰H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, å’Œ L. Daniel. ä½¿ç”¨é€šç”¨æ¿€æ´»å‡½æ•°è¿›è¡Œé«˜æ•ˆç¥ç»ç½‘ç»œé²æ£’æ€§è®¤è¯ã€‚*Neural
    Information Processing Systems (NeurIPS)*ï¼Œ31ï¼Œ2018aã€‚
- en: Zhang etÂ al. (2020) H.Â Zhang, H.Â Chen, C.Â Xiao, S.Â Gowal, R.Â Stanforth, B.Â Li,
    D.Â Boning, and C.-J. Hsieh. Towards stable and efficient training of verifiably
    robust neural networks. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ï¼ˆ2020ï¼‰H. Zhang, H. Chen, C. Xiao, S. Gowal, R. Stanforth, B. Li, D. Boning,
    å’Œ C.-J. Hsieh. æœå‘ç¨³å®šå’Œé«˜æ•ˆçš„å¯éªŒè¯é²æ£’ç¥ç»ç½‘ç»œè®­ç»ƒã€‚æ”¶å½•äº *International Conference on Learning Representations
    (ICLR)*ï¼Œ2020ã€‚
- en: Zhang etÂ al. (2022) H.Â Zhang, S.Â Wang, K.Â Xu, L.Â Li, B.Â Li, S.Â Jana, C.-J. Hsieh,
    and J.Â Z. Kolter. General cutting planes for bound-propagation-based neural network
    verification. In *Neural Information Processing Systems (NeurIPS)*, volumeÂ 35,
    2022.
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ï¼ˆ2022ï¼‰H. Zhang, S. Wang, K. Xu, L. Li, B. Li, S. Jana, C.-J. Hsieh, å’Œ
    J. Z. Kolter. åŸºäºç•Œé™ä¼ æ’­çš„ç¥ç»ç½‘ç»œéªŒè¯çš„ä¸€èˆ¬å‰²å¹³é¢æ–¹æ³•ã€‚æ”¶å½•äº *Neural Information Processing Systems
    (NeurIPS)*ï¼Œç¬¬ 35 å·ï¼Œ2022ã€‚
- en: Zhang etÂ al. (2018b) L.Â Zhang, G.Â Naitzat, and L.-H. Lim. Tropical geometry
    of deep neural networks. In *International Conference on Machine Learning (ICML)*,
    2018b.
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang ç­‰ï¼ˆ2018bï¼‰L. Zhang, G. Naitzat, å’Œ L.-H. Lim. æ·±åº¦ç¥ç»ç½‘ç»œçš„çƒ­å¸¦å‡ ä½•ã€‚æ”¶å½•äº *International
    Conference on Machine Learning (ICML)*ï¼Œ2018bã€‚
- en: Zhang (2020) R.Â Zhang. On the tightness of semidefinite relaxations for certifying
    robustness to adversarial examples. *Neural Information Processing Systems (NeurIPS)*,
    33:3808â€“3820, 2020.
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhangï¼ˆ2020ï¼‰R. Zhang. é’ˆå¯¹å¯¹æŠ—æ ·æœ¬è®¤è¯çš„åŠæ­£å®šæ¾å¼›çš„ç´§ç•Œé™ã€‚*Neural Information Processing Systems
    (NeurIPS)*ï¼Œ33:3808â€“3820ï¼Œ2020ã€‚
- en: Zhang and Wu (2020) X.Â Zhang and D.Â Wu. Empirical studies on the properties
    of linear regions in deep neural networks. In *International Conference on Learning
    Representations (ICLR)*, 2020.
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang å’Œ Wuï¼ˆ2020ï¼‰X. Zhang å’Œ D. Wu. æ·±åº¦ç¥ç»ç½‘ç»œä¸­çº¿æ€§åŒºåŸŸå±æ€§çš„å®è¯ç ”ç©¶ã€‚æ”¶å½•äº *International Conference
    on Learning Representations (ICLR)*ï¼Œ2020ã€‚
- en: 'Zhao etÂ al. (2023) S.Â Zhao, C.Â Tsay, and J.Â Kronqvist. Model-based feature
    selection for neural networks: A mixed-integer programming approach. *arXiv:2302.10344*,
    2023.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao ç­‰ï¼ˆ2023ï¼‰S. Zhao, C. Tsay, å’Œ J. Kronqvist. åŸºäºæ¨¡å‹çš„ç¥ç»ç½‘ç»œç‰¹å¾é€‰æ‹©ï¼šä¸€ç§æ··åˆæ•´æ•°ç¼–ç¨‹æ–¹æ³•ã€‚*arXiv:2302.10344*ï¼Œ2023ã€‚
- en: Zhou and Schoellig (2019) S.Â Zhou and A.Â P. Schoellig. An analysis of the expressiveness
    of deep neural network architectures based on their Lipschitz constants. *arXiv:1912.11511*,
    2019.
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou å’Œ Schoelligï¼ˆ2019ï¼‰S. Zhou å’Œ A. P. Schoellig. åŸºäº Lipschitz å¸¸æ•°çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„è¡¨è¾¾èƒ½åŠ›åˆ†æã€‚*arXiv:1912.11511*ï¼Œ2019ã€‚
- en: Zhu etÂ al. (2020) R.Â Zhu, B.Â Lin, and H.Â Tang. Bounding the number of linear
    regions in local area for neural networks with ReLU activations. *arXiv:2007.06803*,
    2020.
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu ç­‰ï¼ˆ2020ï¼‰R. Zhu, B. Lin, å’Œ H. Tang. å¯¹å¸¦æœ‰ ReLU æ¿€æ´»çš„ç¥ç»ç½‘ç»œå±€éƒ¨åŒºåŸŸçº¿æ€§åŒºåŸŸæ•°é‡çš„ç•Œé™ã€‚*arXiv:2007.06803*ï¼Œ2020ã€‚
- en: Zou etÂ al. (2019) D.Â Zou, R.Â Balan, and M.Â Singh. On Lipschitz bounds of general
    convolutional neural networks. *IEEE Transactions on Information Theory*, 66(3):1738â€“1759,
    2019.
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou ç­‰ï¼ˆ2019ï¼‰D. Zou, R. Balan, å’Œ M. Singh. ä¸€èˆ¬å·ç§¯ç¥ç»ç½‘ç»œçš„ Lipschitz ç•Œé™ã€‚*IEEE Transactions
    on Information Theory*ï¼Œ66(3):1738â€“1759ï¼Œ2019ã€‚
