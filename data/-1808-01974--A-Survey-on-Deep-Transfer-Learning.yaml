- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:07:35'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:35
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1808.01974] A Survey on Deep Transfer Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1808.01974] 深度迁移学习调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1808.01974](https://ar5iv.labs.arxiv.org/html/1808.01974)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1808.01974](https://ar5iv.labs.arxiv.org/html/1808.01974)
- en: '¹¹institutetext: State Key Laboratory of Intelligent Technology and Systems'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构文本：智能技术与系统国家重点实验室
- en: Tsinghua National Laboratory for Information Science and Technology (TNList)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 清华大学国家信息科学技术实验室（TNList）
- en: Department of Computer Science and Technology, Tsinghua University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学与技术系，清华大学
- en: ¹{tcq15, kt14, zhangwc14, yang-c15}@mails.tsinghua.edu.cn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹{tcq15, kt14, zhangwc14, yang-c15}@mails.tsinghua.edu.cn
- en: ²{fcsun, cfliu1985}@tsinghua.edu.cn
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²{fcsun, cfliu1985}@tsinghua.edu.cn
- en: Authors’ Instructions
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作者说明
- en: A Survey on Deep Transfer Learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度迁移学习调查
- en: Chuanqi Tan¹    Fuchun Sun²    Tao Kong¹    Wenchang Zhang¹    Chao Yang¹   
    Chunfang Liu²
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 田传启¹    孙富春²    孔涛¹    张文昌¹    杨超¹    刘春芳²
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As a new classification platform, deep learning has recently received increasing
    attention from researchers and has been successfully applied to many domains.
    In some domains, like bioinformatics and robotics, it is very difficult to construct
    a large-scale well-annotated dataset due to the expense of data acquisition and
    costly annotation, which limits its development. Transfer learning relaxes the
    hypothesis that the training data must be independent and identically distributed
    (i.i.d.) with the test data, which motivates us to use transfer learning to solve
    the problem of insufficient training data. This survey focuses on reviewing the
    current researches of transfer learning by using deep neural network and its applications.
    We defined deep transfer learning, category and review the recent research works
    based on the techniques used in deep transfer learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个新的分类平台，深度学习最近受到了越来越多研究人员的关注，并已成功应用于许多领域。在某些领域，如生物信息学和机器人技术，由于数据采集和标注成本高昂，构建大规模的标注数据集非常困难，这限制了其发展。迁移学习放宽了训练数据必须与测试数据独立且同分布（i.i.d.）的假设，这激励我们使用迁移学习来解决训练数据不足的问题。本调查聚焦于回顾当前基于深度神经网络的迁移学习研究及其应用。我们定义了深度迁移学习，分类，并回顾了基于深度迁移学习技术的最新研究工作。
- en: 'Keywords:'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep Transfer Learning, Transfer Learning, Survey.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度迁移学习，迁移学习，调查。
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Deep learning has recently received increasing attention from researchers and
    has been successfully applied to numerous real-world applications. Deep learning
    algorithms attempt to learn high-level features from mass data, which make deep
    learning beyond traditional machine learning. It can automatic extract data features
    by unsupervised or semi-supervised feature learning algorithm and hierarchical
    feature extraction. In contrast, traditional machine learning methods need to
    design features manually that seriously increases the burden on users. It can
    be said that deep learning is an representation learning algorithm based on large-scale
    data in machine learning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习最近受到越来越多研究人员的关注，并已成功应用于许多实际应用中。深度学习算法试图从大量数据中学习高层次的特征，这使得深度学习超越了传统的机器学习。它可以通过无监督或半监督特征学习算法以及层次化特征提取自动提取数据特征。相比之下，传统的机器学习方法需要手动设计特征，这严重增加了用户的负担。可以说，深度学习是一种基于大规模数据的机器学习中的表示学习算法。
- en: Data dependence is one of the most serious problem in deep learning. Deep learning
    has a very strong dependence on massive training data compared to traditional
    machine learning methods, because it need a large amount of data to understand
    the latent patterns of data. An interesting phenomenon can be found that the scale
    of the model and the size of the required amount of data has a almost linear relationship.
    An acceptable explanation is that for a particular problem, the expressive space
    of the model must be large enough to discover the patterns under the data . The
    pre-order layers in the model can identify high-level features of training data,
    and the subsequent layers can identify the information needed to help make the
    final decision.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据依赖是深度学习中最严重的问题之一。与传统机器学习方法相比，深度学习对大量训练数据的依赖非常强，因为它需要大量数据来理解数据的潜在模式。一个有趣的现象是模型规模和所需数据量之间几乎呈线性关系。一个可以接受的解释是，对于特定问题，模型的表达空间必须足够大，以发现数据中的模式。模型中的预处理层可以识别训练数据的高级特征，随后层可以识别帮助做出最终决策所需的信息。
- en: Insufficient training data is a inescapable problem in some special domains.
    The collection of data is complex and expensive that make it is extremely difficult
    to build a large-scale, high-quality annotated dataset. For example, each sample
    in bioinformatics dataset often demonstration a clinical trial or a painful patient.
    In addition, even we obtain training dataset by paid an expensive price, it is
    very easy to get out of date and thus cannot be effectively applied in the new
    tasks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不足的训练数据是一些特殊领域中不可避免的问题。数据收集复杂且昂贵，使得建立大规模、高质量注释数据集极其困难。例如，生物信息学数据集中的每个样本通常展示了临床试验或痛苦的患者。此外，即使我们通过支付高昂价格获得了训练数据集，它也很容易过时，因此无法有效应用于新任务。
- en: Transfer learning relaxes the hypothesis that the training data must be independent
    and identically distributed (i.i.d.) with the test data, which motivates us to
    use transfer learning to against the problem of insufficient training data. In
    transfer learning, the training data and test data are not required to be i.i.d.,
    and the model in target domain is not need to trained from scratch, which can
    significantly reduce the demand of training data and training time in the target
    domain.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习放宽了训练数据必须与测试数据独立同分布（i.i.d.）的假设，这激励我们使用迁移学习来解决训练数据不足的问题。在迁移学习中，训练数据和测试数据不需要是i.i.d.，并且目标领域的模型不需要从头开始训练，这可以显著减少目标领域对训练数据和训练时间的需求。
- en: 'In the past, most studies of transfer learning were conducted in traditional
    machine learning methods. Due to the dominance position of deep learning in modern
    machine learning methods, a survey on deep transfer learning and its applications
    is particularly important. The contributions of this survey paper are as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，大多数迁移学习研究都在传统机器学习方法中进行。由于深度学习在现代机器学习方法中的主导地位，对深度迁移学习及其应用的调查尤其重要。这篇调查论文的贡献如下：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We define the deep transfer learning and categorizing it into four categories
    for the first time.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首次定义了深度迁移学习，并将其分类为四类。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We reviewing the current research works on each category of deep transfer learning,
    and given a standardized description and sketch map of every category.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了深度迁移学习每个类别的当前研究工作，并给出了每个类别的标准化描述和示意图。
- en: 2 Deep Transfer Learning
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度迁移学习
- en: Transfer learning is an important tool in machine learning to solve the basic
    problem of insufficient training data. It try to transfer the knowledge from the
    source domain to the target domain by relaxing the assumption that the training
    data and the test data must be i.i.d. This will leads to a great positive effect
    on many domains that are difficult to improve because of insufficient training
    data. The learning process of transfer learning illustrated in the Fig. [1](#S2.F1
    "Figure 1 ‣ 2 Deep Transfer Learning ‣ A Survey on Deep Transfer Learning").
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是机器学习中解决训练数据不足基本问题的重要工具。它试图通过放宽训练数据和测试数据必须是i.i.d.的假设，将知识从源领域转移到目标领域。这将对许多因训练数据不足而难以改进的领域产生积极的影响。迁移学习的学习过程如图
    [1](#S2.F1 "Figure 1 ‣ 2 Deep Transfer Learning ‣ A Survey on Deep Transfer Learning")
    所示。
- en: '![Refer to caption](img/5d67cf10cc28c9b73aa4a903de037dbd.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5d67cf10cc28c9b73aa4a903de037dbd.png)'
- en: 'Figure 1: Learning process of transfer learning.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：迁移学习的学习过程。
- en: 'Some notations used in this survey need to be clearly defined. First of all,
    we give the definitions of a domain and a task respectively: A domain can be represented
    by $\mathcal{D}=\{\chi,P(X)\}$, which contains two parts: the feature space $\chi$
    and the edge probability distribution $P(X)$ where $X=\{x_{1},...,x_{n}\}\in\chi$.
    A task can be represented by $\mathcal{T}=\{y,f(x)\}$. It consists of two parts:
    label space $y$ and target prediction function $f(x)$. $f(x)$ can also be regarded
    as a conditional probability function $P(y|x)$. Then, the transfer learning can
    be formal defined as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查中使用的一些符号需要明确说明。首先，我们分别给出领域和任务的定义：一个领域可以表示为$\mathcal{D}=\{\chi,P(X)\}$，它包含两个部分：特征空间$\chi$和边缘概率分布$P(X)$，其中$X=\{x_{1},...,x_{n}\}\in\chi$。一个任务可以表示为$\mathcal{T}=\{y,f(x)\}$。它由两部分组成：标签空间$y$和目标预测函数$f(x)$。$f(x)$也可以被视为条件概率函数$P(y|x)$。然后，迁移学习可以正式定义如下：
- en: Definition 1
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1
- en: (Transfer Learning). Given a learning task $\mathcal{T}_{t}$ based on $\mathcal{D}_{t}$,
    and we can get the help from $\mathcal{D}_{s}$ for the learning task $\mathcal{T}_{s}$.
    Transfer learning aims to improve the performance of predictive function $f_{\mathcal{T}}(\cdot)$
    for learning task $\mathcal{T}_{t}$ by discover and transfer latent knowledge
    from $\mathcal{D}_{s}$ and $\mathcal{T}_{s}$, where $\mathcal{D}_{s}\neq\mathcal{D}_{t}$
    and/or $\mathcal{T}_{s}\neq\mathcal{T}_{t}$. In addition, in the most case, the
    size of $\mathcal{D}_{s}$ is much larger than the size of $\mathcal{D}_{t}$, $N_{s}\gg
    N_{t}$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (迁移学习)。给定一个基于$\mathcal{D}_{t}$的学习任务$\mathcal{T}_{t}$，我们可以利用$\mathcal{D}_{s}$来帮助学习任务$\mathcal{T}_{s}$。迁移学习旨在通过从$\mathcal{D}_{s}$和$\mathcal{T}_{s}$中发现和迁移潜在知识，来提高预测函数$f_{\mathcal{T}}(\cdot)$在学习任务$\mathcal{T}_{t}$上的性能，其中$\mathcal{D}_{s}\neq\mathcal{D}_{t}$和/或$\mathcal{T}_{s}\neq\mathcal{T}_{t}$。此外，在大多数情况下，$\mathcal{D}_{s}$的大小远大于$\mathcal{D}_{t}$的大小，$N_{s}\gg
    N_{t}$。
- en: 'Surveys [[19](#bib.bib19)] and [[25](#bib.bib25)] divide the transfer learning
    methods into three major categories with the relationship between the source domain
    and the target domain, which has been widely accepted. These suverys are good
    summary of the past works on transfer learning, which introduced a number of classic
    transfer learning methods. Further more, many newer and better methods have been
    proposed recently. In recent years, transfer learning research community are mainly
    focused on the following two aspects: domain adaption and multi-source domains
    transfer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 调查[[19](#bib.bib19)]和[[25](#bib.bib25)]将迁移学习方法分为三个主要类别，基于源领域和目标领域之间的关系，这一点已被广泛接受。这些调查很好地总结了过去在迁移学习领域的工作，并介绍了一些经典的迁移学习方法。此外，最近提出了许多更新、更好的方法。近年来，迁移学习研究社区主要集中在以下两个方面：领域适应和多源领域迁移。
- en: 'Nowadays, deep learning has achieved dominating situation in many research
    fields in recent years. It is important to find how to effectively transfer knowledge
    by deep neural network, which called deep transfer learning that defined as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，深度学习在近年来的许多研究领域中已取得主导地位。重要的是要找出如何通过深度神经网络有效地迁移知识，这就是所谓的深度迁移学习，其定义如下：
- en: Definition 2
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2
- en: (Deep Transfer Learning). Given a transfer learning task defined by $\langle\mathcal{D}_{s},\mathcal{T}_{s},\mathcal{D}_{t},\mathcal{T}_{t},f_{\mathcal{T}}(\cdot)\rangle$.
    It is a deep transfer learning task where $f_{\mathcal{T}}(\cdot)$ is a non-linear
    function that reflected a deep neural network.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (深度迁移学习)。给定一个由$\langle\mathcal{D}_{s},\mathcal{T}_{s},\mathcal{D}_{t},\mathcal{T}_{t},f_{\mathcal{T}}(\cdot)\rangle$定义的迁移学习任务。这是一个深度迁移学习任务，其中$f_{\mathcal{T}}(\cdot)$是一个非线性函数，反映了深度神经网络。
- en: 3 Categories
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 类别
- en: 'Deep transfer learning studies how to utilize knowledge from other fields by
    deep neural networks. Since deep neural networks have become popular in various
    fields, a considerable amount of deep transfer learning methods have been proposed
    that it is very important to classify and summarize them. Based on the techniques
    used in deep transfer learning, this paper classifies deep transfer learning into
    four categories: instances-based deep transfer learning, mapping-based deep transfer
    learning, network-based deep transfer learning, and adversarial-based deep transfer
    learning, which are shown in Table [1](#S3.T1 "Table 1 ‣ 3 Categories ‣ A Survey
    on Deep Transfer Learning").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 深度迁移学习研究如何通过深度神经网络利用其他领域的知识。由于深度神经网络在各个领域中变得非常流行，已经提出了大量的深度迁移学习方法，因此对其进行分类和总结非常重要。根据深度迁移学习中使用的技术，本文将深度迁移学习分为四类：基于实例的深度迁移学习、基于映射的深度迁移学习、基于网络的深度迁移学习和基于对抗的深度迁移学习，如表[1](#S3.T1
    "Table 1 ‣ 3 Categories ‣ A Survey on Deep Transfer Learning")所示。
- en: 'Table 1: Categorizing of deep transfer learning.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：深度迁移学习的分类。
- en: '| Approach category | Brief description | Some related works |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 方法类别 | 简要描述 | 一些相关工作 |'
- en: '| --- | --- | --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Instances-based | Utilize instances in source domain by appropriate weight.
    | [[4](#bib.bib4)], [[27](#bib.bib27)], [[20](#bib.bib20)], [[24](#bib.bib24)],
    [[10](#bib.bib10)], [[26](#bib.bib26)], [[11](#bib.bib11)] |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 基于实例的 | 通过适当的权重利用源领域中的实例。 | [[4](#bib.bib4)], [[27](#bib.bib27)], [[20](#bib.bib20)],
    [[24](#bib.bib24)], [[10](#bib.bib10)], [[26](#bib.bib26)], [[11](#bib.bib11)]
    |'
- en: '| Mapping-based | Mapping instances from two domains into a new data space
    with better similarity. | [[23](#bib.bib23)], [[12](#bib.bib12)], [[8](#bib.bib8)],
    [[14](#bib.bib14)], [[2](#bib.bib2)] |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 基于映射的 | 将两个领域的实例映射到具有更好相似性的新数据空间。 | [[23](#bib.bib23)], [[12](#bib.bib12)],
    [[8](#bib.bib8)], [[14](#bib.bib14)], [[2](#bib.bib2)] |'
- en: '| Network-based | Reuse the partial of network pre-trained in the source domain.
    | [[9](#bib.bib9)], [[17](#bib.bib17)], [[15](#bib.bib15)], [[30](#bib.bib30)],
    [[3](#bib.bib3)], [[6](#bib.bib6)], [[28](#bib.bib28)] |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 基于网络的 | 重用在源领域预训练的部分网络。 | [[9](#bib.bib9)], [[17](#bib.bib17)], [[15](#bib.bib15)],
    [[30](#bib.bib30)], [[3](#bib.bib3)], [[6](#bib.bib6)], [[28](#bib.bib28)] |'
- en: '| Adversarial-based | Use adversarial technology to find transferable features
    that both suitable for two domains. | [[1](#bib.bib1)], [[5](#bib.bib5)], [[21](#bib.bib21)],
    [[22](#bib.bib22)], [[13](#bib.bib13)], [[16](#bib.bib16)] |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 基于对抗的 | 使用对抗技术寻找适用于两个领域的可迁移特征。 | [[1](#bib.bib1)], [[5](#bib.bib5)], [[21](#bib.bib21)],
    [[22](#bib.bib22)], [[13](#bib.bib13)], [[16](#bib.bib16)] |'
- en: 3.1 Instances-based deep transfer learning
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于实例的深度迁移学习
- en: Instances-based deep transfer learning refers to use a specific weight adjustment
    strategy, select partial instances from the source domain as supplements to the
    training set in the target domain by assigning appropriate weight values to these
    selected instances. It is based on the assumption that ”Although there are different
    between two domains, partial instances in the source domain can be utilized by
    the target domain with appropriate weights.”. The sketch map of instances-based
    deep transfer learning are shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3.1 Instances-based
    deep transfer learning ‣ 3 Categories ‣ A Survey on Deep Transfer Learning").
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基于实例的深度迁移学习指使用特定的权重调整策略，通过为选定的源领域实例分配适当的权重，将这些实例作为目标领域训练集的补充。它基于假设“虽然两个领域之间存在差异，但源领域中的部分实例可以通过适当的权重被目标领域利用。”
    基于实例的深度迁移学习的示意图如图[2](#S3.F2 "Figure 2 ‣ 3.1 Instances-based deep transfer learning
    ‣ 3 Categories ‣ A Survey on Deep Transfer Learning")所示。
- en: '![Refer to caption](img/c8da161a71d6f5c52878813e2575c9d5.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c8da161a71d6f5c52878813e2575c9d5.png)'
- en: 'Figure 2: Sketch map of instances-based deep transfer learning. Instances with
    light blue color in source domain meanings dissimilar with target domain are exclude
    from training dataset; Instances with dark blue color in source domain meanings
    similar with target domain are include in training dataset with appropriate weight.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于实例的深度迁移学习的示意图。源领域中浅蓝色的实例表示与目标领域含义不相似，排除在训练数据集中；源领域中深蓝色的实例表示与目标领域含义相似，包含在训练数据集中并赋予适当的权重。
- en: TrAdaBoost proposed by [[4](#bib.bib4)] use AdaBoost-based technology to filter
    out instances that are dissimilar to the target domain in source domains. Re-weighted
    instances in source domain to compose a distribution similar to target domain.
    Finally, training model by using the re-weighted instances from source domain
    and origin instances from target domain. It can reduce the weighted training error
    on different distribution domains that preserving the properties of AdaBoost.
    TaskTrAdaBoost proposed by [[27](#bib.bib27)] is a fast algorithm promote rapid
    retraining over new targets. Unlike TrAdaBoost is designed for classification
    problems, ExpBoost.R2 and TrAdaBoost.R2 were proposed by [[20](#bib.bib20)] to
    cover the regression problem. Bi-weighting domain adaptation (BIW) proposed [[24](#bib.bib24)]
    can aligns the feature spaces of two domains into the common coordinate system,
    and then assign an appropriate weight of the instances from source domain. [[10](#bib.bib10)]
    propose a enhanced TrAdaBoost to handle the problem of interregional sandstone
    microscopic image classification. [[26](#bib.bib26)] propose a metric transfer
    learning framework to learn instance weights and a distance of two different domains
    in a parallel framework to make knowledge transfer across domains more effective.
    [[11](#bib.bib11)] introduce an ensemble transfer learning to deep neural network
    that can utilize instances from source domain.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[[4](#bib.bib4)]提出的TrAdaBoost使用基于AdaBoost的技术来过滤源领域中与目标领域不相似的实例。重新加权源领域中的实例，以组成一个与目标领域相似的分布。最后，使用来自源领域的重新加权实例和来自目标领域的原始实例训练模型。它可以减少在不同分布领域上的加权训练误差，同时保持AdaBoost的特性。[[27](#bib.bib27)]提出的TaskTrAdaBoost是一种快速算法，促进在新目标上快速重训练。与TrAdaBoost不同，TaskTrAdaBoost是为分类问题设计的，而[[20](#bib.bib20)]提出的ExpBoost.R2和TrAdaBoost.R2则覆盖了回归问题。[[24](#bib.bib24)]提出的双重加权领域适应（BIW）可以将两个领域的特征空间对齐到公共坐标系统中，然后为源领域的实例分配适当的权重。[[10](#bib.bib10)]提出了一种增强的TrAdaBoost，以处理区域间砂岩显微图像分类的问题。[[26](#bib.bib26)]提出了一种度量迁移学习框架，以平行框架学习实例权重和两个不同领域之间的距离，从而使跨领域知识迁移更加有效。[[11](#bib.bib11)]引入了一种集合迁移学习方法，以深度神经网络为基础，可以利用源领域的实例。'
- en: 3.2 Mapping-based deep transfer learning
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于映射的深度迁移学习
- en: Mapping-based deep transfer learning refers to mapping instances from the source
    domain and target domain into a new data space. In this new data space, instances
    from two domains are similarly and suitable for a union deep neural network. It
    is based on the assumption that ”Although there are different between two origin
    domains, they can be more similarly in an elaborate new data space.”. The sketch
    map of instances-based deep transfer learning are shown in Fig. [3](#S3.F3 "Figure
    3 ‣ 3.2 Mapping-based deep transfer learning ‣ 3 Categories ‣ A Survey on Deep
    Transfer Learning").
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于映射的深度迁移学习指的是将源领域和目标领域的实例映射到一个新的数据空间。在这个新的数据空间中，两个领域的实例在某种程度上相似，并且适用于联合深度神经网络。它基于以下假设：“虽然两个原始领域之间存在差异，但它们可以在一个精细的新数据空间中变得更为相似。”基于实例的深度迁移学习的示意图见图[3](#S3.F3
    "图 3 ‣ 3.2 基于映射的深度迁移学习 ‣ 3 类别 ‣ 深度迁移学习综述")。
- en: '![Refer to caption](img/ad74a7a4916102f90eee4a35c8fc001b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad74a7a4916102f90eee4a35c8fc001b.png)'
- en: 'Figure 3: Sketch map of mapping-based deep transfer learning. Simultaneously,
    instances from source domain and target domain are mapping to a new data space
    with more similarly. Consider all instances in the new data space as the training
    set of the neural network.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于映射的深度迁移学习的示意图。同时，源领域和目标领域的实例被映射到一个更为相似的新数据空间中。将新数据空间中的所有实例视为神经网络的训练集。
- en: Transfer component analysis (TCA) introduced by [[18](#bib.bib18)] and TCA-based
    methods [[29](#bib.bib29)] had been widely used in many applications of traditional
    transfer learning. A natural idea is extend the TCA method to deep neural network.
    [[23](#bib.bib23)] extend MMD to comparing distributions in a deep neural network,
    by introduces an adaptation layer and an additional domain confusion loss to learn
    a representation that is both semantically meaningful and domain invariant. The
    MMD distance used in this work is defined as
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由[[18](#bib.bib18)]提出的迁移成分分析（TCA）以及基于TCA的方法[[29](#bib.bib29)]已经在许多传统迁移学习应用中得到了广泛使用。一个自然的想法是将TCA方法扩展到深度神经网络中。[[23](#bib.bib23)]通过引入适应层和额外的领域混淆损失来扩展MMD，以便在深度神经网络中比较分布，从而学习到一个在语义上有意义且领域不变的表示。本文中使用的MMD距离定义为
- en: '|  | $D_{\mathcal{MMD}}(X_{S},X_{T})=\left&#124;\left&#124;\dfrac{1}{&#124;X_{S}&#124;}\sum_{x_{s}\in
    X_{S}}\phi(x_{s})-\dfrac{1}{&#124;X_{T}&#124;}\sum_{x_{t}\in X_{T}}\phi(x_{t})\right&#124;\right&#124;$
    |  | (1) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $D_{\mathcal{MMD}}(X_{S},X_{T})=\left\|\left\|\dfrac{1}{|X_{S}|}\sum_{x_{s}\in
    X_{S}}\phi(x_{s})-\dfrac{1}{|X_{T}|}\sum_{x_{t}\in X_{T}}\phi(x_{t})\right\|\right\|$
    |  | (1) |'
- en: and the loss function is defined as
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 且损失函数定义为
- en: '|  | $\mathcal{L}=\mathcal{L}_{C}(X_{L},y)+\lambda D^{2}_{\mathcal{MMD}}(X_{S},X_{T}).$
    |  | (2) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathcal{L}_{C}(X_{L},y)+\lambda D^{2}_{\mathcal{MMD}}(X_{S},X_{T}).$
    |  | (2) |'
- en: '[[12](#bib.bib12)] improved previous work by replace MMD distance with multiple
    kernel variant MMD (MK-MMD) distance proposed by [[8](#bib.bib8)]. The hidden
    layer related with the learning task in the convolutional neural networks (CNN)
    is mapped into the reproducing kernel Hilbert space (RKHS), and the distance between
    different domains is minimized by the multi-core optimization method. [[14](#bib.bib14)]
    propose joint maximum mean discrepancy (JMMD) to measurement the relationship
    of joint distribution. JMMD was used to generalize the transfer learning ability
    of the deep neural networks (DNN) to adapt the data distribution in different
    domain and improved the previous works. Wasserstein distance proposed by [[2](#bib.bib2)]
    can be used as a new distance measurement of domains to find better mapping.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12](#bib.bib12)] 通过用 [[8](#bib.bib8)] 提出的多核变体 MMD（MK-MMD）距离替代了之前的 MMD 距离，从而改进了之前的工作。卷积神经网络（CNN）中与学习任务相关的隐藏层被映射到再生核希尔伯特空间（RKHS），并通过多核优化方法最小化不同领域之间的距离。[[14](#bib.bib14)]
    提出了联合最大均值离散度（JMMD）来度量联合分布的关系。JMMD 用于推广深度神经网络（DNN）的迁移学习能力，以适应不同领域的数据分布，并改进了之前的工作。[[2](#bib.bib2)]
    提出的 Wasserstein 距离可以作为领域之间新距离测量的手段，以寻找更好的映射。'
- en: 3.3 Network-based deep transfer learning
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于网络的深度迁移学习
- en: Network-based deep transfer learning refers to the reuse the partial network
    that pre-trained in the source domain, including its network structure and connection
    parameters, transfer it to be a part of deep neural network which used in target
    domain. It is based on the assumption that ”Neural network is similar to the processing
    mechanism of the human brain, and it is an iterative and continuous abstraction
    process. The front-layers of the network can be treated as a feature extractor,
    and the extracted features are versatile.”. The sketch map of network-based deep
    transfer learning are shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.3 Network-based deep
    transfer learning ‣ 3 Categories ‣ A Survey on Deep Transfer Learning").
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于网络的深度迁移学习指的是重用在源领域预训练的部分网络，包括其网络结构和连接参数，将其转移到目标领域中作为深度神经网络的一部分。这基于这样的假设：“神经网络类似于人脑的处理机制，是一个迭代和持续的抽象过程。网络的前层可以视为特征提取器，提取的特征是通用的。”
    基于网络的深度迁移学习的示意图如图 [4](#S3.F4 "Figure 4 ‣ 3.3 Network-based deep transfer learning
    ‣ 3 Categories ‣ A Survey on Deep Transfer Learning") 所示。
- en: '![Refer to caption](img/da25cfd2174d86a5121ba8813b096a28.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/da25cfd2174d86a5121ba8813b096a28.png)'
- en: 'Figure 4: Sketch map of network-based deep transfer learning. First, network
    was trained in source domain with large-scale training dataset. Second, partial
    of network pre-trained for source domain are transfer to be a part of new network
    designed for target domain. Finally, the transfered sub-network may be updated
    in fine-tune strategy.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：基于网络的深度迁移学习示意图。首先，在源领域用大规模训练数据集训练网络。其次，将源领域预训练的部分网络转移为目标领域新网络的一部分。最后，转移后的子网络可以在微调策略中更新。
- en: '[[9](#bib.bib9)] divide the network into two parts, the former part is the
    language-independent feature transform and the last layer is the language-relative
    classifier. The language-independent feature transform can be transfer between
    multi languages. [[17](#bib.bib17)] reuse front-layers trained by CNN on the ImageNet
    dataset to compute intermediate image representation for images in other datasets,
    CNN are trained to learning image representations that can be efficiently transferred
    to other visual recognition tasks with limited amount of training data. [[15](#bib.bib15)]
    proposed a approach to jointly learn adaptive classifiers and transferable features
    from labeled data in the source domain and unlabeled data in the target domain,
    which explicitly learn the residual function with reference to the target classifier
    by plugging several layers into deep network. [[30](#bib.bib30)] learning domain
    adaptation and deep hash features simultaneously in a DNN. [[3](#bib.bib3)] proposed
    a novel multi-scale convolutional sparse coding method. This method can automatically
    learns filter banks at different scales in a joint fashion with enforced scale-specificity
    of learned patterns, and provides an unsupervised solution for learning transferable
    base knowledge and fine-tuning it towards target tasks. [[6](#bib.bib6)] apply
    deep transfer learning to transfer knowledge from real-world object recognition
    tasks to glitch classifier for the detector of multiple gravitational wave signals.
    It demonstrate that DNN can be used as excellent feature extractors for unsupervised
    clustering methods to identify new classes based on their morphology, without
    any labeled examples.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9](#bib.bib9)]将网络分为两个部分，前半部分是语言无关的特征变换，最后一层是语言相关的分类器。语言无关的特征变换可以在多种语言之间迁移。[[17](#bib.bib17)]重用在ImageNet数据集上训练的前层，以计算其他数据集图像的中间图像表示，CNN被训练以学习可以有效迁移到其他视觉识别任务中的图像表示，尽管训练数据量有限。[[15](#bib.bib15)]提出了一种方法，从源领域的标记数据和目标领域的未标记数据中共同学习自适应分类器和可迁移特征，通过将多个层插入深度网络中，显式地学习残差函数以参考目标分类器。[[30](#bib.bib30)]在深度神经网络中同时学习领域适应和深度哈希特征。[[3](#bib.bib3)]提出了一种新颖的多尺度卷积稀疏编码方法。该方法可以以联合方式自动学习不同尺度的滤波器组，并强制学习模式的尺度特异性，为学习可迁移的基础知识并在目标任务中进行微调提供了一种无监督解决方案。[[6](#bib.bib6)]将深度迁移学习应用于将知识从现实世界的物体识别任务迁移到多重引力波信号探测器的故障分类器。它证明了DNN可以作为优秀的特征提取器，用于无监督聚类方法以根据形态识别新类别，而不需要任何标记示例。'
- en: Another very noteworthy result is that [[28](#bib.bib28)] point out the relationship
    between network structure and transferability. It demonstrated that some modules
    may not influence in-domain accuracy but influence the transferability. It point
    out what features are transferable in deep networks and which type of networks
    are more suitable for transfer. Given an conclusion that LeNet, AlexNet, VGG,
    Inception, ResNet are good chooses in network-based deep transfer learning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常值得注意的结果是[[28](#bib.bib28)]指出了网络结构与可迁移性之间的关系。它证明了一些模块可能不会影响领域内准确性，但会影响可迁移性。它指出了在深度网络中哪些特征是可迁移的，以及哪种类型的网络更适合迁移。结论是LeNet、AlexNet、VGG、Inception和ResNet是网络基深度迁移学习中的良好选择。
- en: 3.4 Adversarial-based deep transfer learning
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 对抗性深度迁移学习
- en: Adversarial-based deep transfer learning refers to introduce adversarial technology
    inspired by generative adversarial nets (GAN) [[7](#bib.bib7)] to find transferable
    representations that is applicable to both the source domain and the target domain.
    It is based on the assumption that ”For effective transfer, good representation
    should be discriminative for the main learning task and indiscriminate between
    the source domain and target domain.” The sketch map of adversarial-based deep
    transfer learning are shown in Fig. [5](#S3.F5 "Figure 5 ‣ 3.4 Adversarial-based
    deep transfer learning ‣ 3 Categories ‣ A Survey on Deep Transfer Learning").
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性深度迁移学习指的是引入受生成对抗网络（GAN）[[7](#bib.bib7)]启发的对抗性技术，以寻找适用于源领域和目标领域的可迁移表示。其基于这样的假设：“为了有效迁移，好的表示应当在主要学习任务中具有区分性，并且在源领域和目标领域之间无差别。”
    对抗性深度迁移学习的示意图如图[5](#S3.F5 "Figure 5 ‣ 3.4 Adversarial-based deep transfer learning
    ‣ 3 Categories ‣ A Survey on Deep Transfer Learning")所示。
- en: '![Refer to caption](img/2ffb8b3afbdf3ae06c53687ed676a637.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/2ffb8b3afbdf3ae06c53687ed676a637.png)'
- en: 'Figure 5: Sketch map of adversarial-based deep transfer learning. In the training
    process on large-scale dataset in the source domain, the front-layers of network
    is regarded as a feature extractor. It extracting features from two domains and
    sent them to adversarial layer. The adversarial layer try to discriminates the
    origin of the features. If the adversarial network achieves worse performance,
    it means a small difference between the two types of feature and better transferability,
    and vice versa. In the following training process, the performance of the adversarial
    layer will be considered to force the transfer network discover general features
    with more transferability.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基于对抗的深度迁移学习的示意图。在源领域的大规模数据集训练过程中，网络的前层被视为特征提取器。它从两个领域提取特征并将其发送到对抗层。对抗层试图区分特征的来源。如果对抗网络的表现更差，则意味着两种特征之间的差异较小，迁移性更好，反之亦然。在后续的训练过程中，将考虑对抗层的表现，以迫使迁移网络发现具有更高迁移性的通用特征。
- en: The adversarial-based deep transfer learning has obtained the flourishing development
    in recent years due to its good effect and strong practicality. [[1](#bib.bib1)]
    introduce adversarial technology to transfer learning for domain adaption, by
    using a domain adaptation regularization term in the loss function. [[5](#bib.bib5)]
    proposed an adversarial training method that suitable for most any feed-forward
    neural model by augmenting it with few standard layers and a simple new gradient
    reversal layer. [[21](#bib.bib21)] proposed a approach transfer knowledge cross-domain
    and cross-task simultaneity for sparsely labeled target domain data. A special
    joint loss function was used in this work to force CNN to optimize both the distance
    between domains which defined as $\mathcal{L}_{D}=\mathcal{L}_{c}+\lambda\mathcal{L}_{adver}$,
    where $\mathcal{L}_{c}$ is classification loss, $\mathcal{L}_{adver}$ is domain
    adversarial loss. Because the two losses stand in direct opposition to one another,
    an iterative optimize algorithm are introduced to update one loss when fixed another.
    [[22](#bib.bib22)] proposed a new GAN loss and combine with discriminative modeling
    to a new domain adaptation method. [[13](#bib.bib13)] proposed a randomized multi-linear
    adversarial networks to exploit multiple feature layers and the classifier layer
    based on a randomized multi-linear adversary to enable both deep and discriminative
    adversarial adaptation. [[16](#bib.bib16)] utilize a domain adversarial loss,
    and generalizes the embedding to novel task using a metric learning-based approach
    to find more tractable features in deep transfer learning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对抗的深度迁移学习近年来由于其良好的效果和强大的实用性取得了蓬勃发展。[[1](#bib.bib1)] 引入了对抗技术用于领域适应，通过在损失函数中使用领域适应正则化项。[[5](#bib.bib5)]
    提出了适用于大多数前馈神经网络模型的对抗训练方法，通过添加少量标准层和一个简单的新梯度反转层进行增强。[[21](#bib.bib21)] 提出了跨领域和跨任务同时转移知识的方法，用于稀疏标记的目标领域数据。在这项工作中使用了一个特殊的联合损失函数，强制CNN优化定义为$\mathcal{L}_{D}=\mathcal{L}_{c}+\lambda\mathcal{L}_{adver}$的领域间距离，其中$\mathcal{L}_{c}$是分类损失，$\mathcal{L}_{adver}$是领域对抗损失。由于这两种损失相互对立，因此引入了一种迭代优化算法，以在固定一个损失时更新另一个损失。[[22](#bib.bib22)]
    提出了新的GAN损失，并结合判别建模形成一种新的领域适应方法。[[13](#bib.bib13)] 提出了随机多线性对抗网络，利用多个特征层和基于随机多线性对抗的分类器层，以实现深度和判别对抗适应。[[16](#bib.bib16)]
    利用领域对抗损失，并使用基于度量学习的方法将嵌入推广到新任务中，以在深度迁移学习中发现更多可处理的特征。
- en: 4 Conclusion
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: 'In this survey paper, we have review and category current researches of deep
    transfer learning. Deep transfer learning is classified into four categories for
    the first time: instances-based deep transfer learning, mapping-based deep transfer
    learning, network-based deep transfer learning, and adversarial-based deep transfer
    learning. In most practical applications, the above multiple technologies are
    often used in combination to achieve better results. Most current researches focuses
    on supervised learning, how to transfer knowledge in unsupervised or semi-supervised
    learning by deep neural network may attract more and more attention in the future.
    Negative transfer and transferability measures are important issues in traditional
    transfer learning. The impact of these two issues in deep transfer learning also
    requires us to conduct further research. In addition, a very attractive research
    area is to find a stronger physical support for transfer knowledge in deep neural
    network, which requires the cooperation of physicists, neuroscientists and computer
    scientists. It can be predicted that deep transfer learning will be widely applied
    to solve many challenging problems with the development of deep neural network.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇综述论文中，我们回顾并分类了当前的深度转移学习研究。深度转移学习首次被分类为四类：基于实例的深度转移学习、基于映射的深度转移学习、基于网络的深度转移学习以及基于对抗的深度转移学习。在大多数实际应用中，上述多种技术通常结合使用以获得更好的结果。目前大多数研究集中在监督学习上，如何通过深度神经网络在无监督或半监督学习中转移知识可能会受到越来越多的关注。负转移和转移度量在传统转移学习中是重要问题。这两个问题在深度转移学习中的影响也需要我们进一步研究。此外，一个非常有吸引力的研究领域是寻找对深度神经网络中转移知识的更强物理支持，这需要物理学家、神经科学家和计算机科学家的合作。可以预测，随着深度神经网络的发展，深度转移学习将被广泛应用于解决许多具有挑战性的问题。
- en: References
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M.:
    Domain-adversarial neural networks. arXiv preprint arXiv:1412.4446 (2014)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M.:
    域对抗神经网络。arXiv预印本 arXiv:1412.4446 (2014)'
- en: '[2] Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein gan. arXiv preprint
    arXiv:1701.07875 (2017)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein GAN。arXiv预印本 arXiv:1701.07875
    (2017)'
- en: '[3] Chang, H., Han, J., Zhong, C., Snijders, A., Mao, J.H.: Unsupervised transfer
    learning via multi-scale convolutional sparse coding for biomedical applications.
    IEEE transactions on pattern analysis and machine intelligence (2017)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Chang, H., Han, J., Zhong, C., Snijders, A., Mao, J.H.: 通过多尺度卷积稀疏编码进行无监督转移学习，应用于生物医学领域。IEEE模式分析与机器智能交易
    (2017)'
- en: '[4] Dai, W., Yang, Q., Xue, G.R., Yu, Y.: Boosting for transfer learning. In:
    Proceedings of the 24th international conference on Machine learning. pp. 193–200\.
    ACM (2007)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Dai, W., Yang, Q., Xue, G.R., Yu, Y.: 转移学习的提升方法。发表于第24届国际机器学习会议论文集。第193–200页。ACM
    (2007)'
- en: '[5] Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation.
    arXiv preprint arXiv:1409.7495 (2014)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ganin, Y., Lempitsky, V.: 通过反向传播进行无监督领域适应。arXiv预印本 arXiv:1409.7495 (2014)'
- en: '[6] George, D., Shen, H., Huerta, E.: Deep transfer learning: A new deep learning
    glitch classification method for advanced ligo. arXiv preprint arXiv:1706.07446
    (2017)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] George, D., Shen, H., Huerta, E.: 深度转移学习：一种新的深度学习故障分类方法，适用于高级LIGO。arXiv预印本
    arXiv:1706.07446 (2017)'
- en: '[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances
    in neural information processing systems. pp. 2672–2680 (2014)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A., Bengio, Y.: 生成对抗网络。发表于神经信息处理系统进展。第2672–2680页 (2014)'
- en: '[8] Gretton, A., Sejdinovic, D., Strathmann, H., Balakrishnan, S., Pontil,
    M., Fukumizu, K., Sriperumbudur, B.K.: Optimal kernel choice for large-scale two-sample
    tests. In: Advances in neural information processing systems. pp. 1205–1213 (2012)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Gretton, A., Sejdinovic, D., Strathmann, H., Balakrishnan, S., Pontil,
    M., Fukumizu, K., Sriperumbudur, B.K.: 大规模双样本检验的最佳核选择。发表于神经信息处理系统进展。第1205–1213页
    (2012)'
- en: '[9] Huang, J.T., Li, J., Yu, D., Deng, L., Gong, Y.: Cross-language knowledge
    transfer using multilingual deep neural network with shared hidden layers. In:
    Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference
    on. pp. 7304–7308\. IEEE (2013)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Huang, J.T., Li, J., Yu, D., Deng, L., Gong, Y.: 使用共享隐藏层的多语言深度神经网络进行跨语言知识转移。发表于声学、语音与信号处理（ICASSP），2013
    IEEE国际会议。第7304–7308页。IEEE (2013)'
- en: '[10] Li, N., Hao, H., Gu, Q., Wang, D., Hu, X.: A transfer learning method
    for automatic identification of sandstone microscopic images. Computers & Geosciences
    103, 111–121 (2017)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Li, N., Hao, H., Gu, Q., Wang, D., Hu, X.：一种用于自动识别砂岩显微图像的迁移学习方法。计算机与地球科学
    103, 111–121 (2017)'
- en: '[11] Liu, X., Liu, Z., Wang, G., Cai, Z., Zhang, H.: Ensemble transfer learning
    algorithm. IEEE Access 6, 2389–2396 (2018)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Liu, X., Liu, Z., Wang, G., Cai, Z., Zhang, H.：集成迁移学习算法。IEEE Access 6,
    2389–2396 (2018)'
- en: '[12] Long, M., Cao, Y., Wang, J., Jordan, M.: Learning transferable features
    with deep adaptation networks. In: International Conference on Machine Learning.
    pp. 97–105 (2015)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Long, M., Cao, Y., Wang, J., Jordan, M.：使用深度适应网络学习可迁移特征。在：国际机器学习会议。第97–105页
    (2015)'
- en: '[13] Long, M., Cao, Z., Wang, J., Jordan, M.I.: Domain adaptation with randomized
    multilinear adversarial networks. arXiv preprint arXiv:1705.10667 (2017)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Long, M., Cao, Z., Wang, J., Jordan, M.I.：使用随机多线性对抗网络的领域适应。arXiv 预印本 arXiv:1705.10667
    (2017)'
- en: '[14] Long, M., Wang, J., Jordan, M.I.: Deep transfer learning with joint adaptation
    networks. arXiv preprint arXiv:1605.06636 (2016)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Long, M., Wang, J., Jordan, M.I.：带有联合适应网络的深度迁移学习。arXiv 预印本 arXiv:1605.06636
    (2016)'
- en: '[15] Long, M., Zhu, H., Wang, J., Jordan, M.I.: Unsupervised domain adaptation
    with residual transfer networks. In: Advances in Neural Information Processing
    Systems. pp. 136–144 (2016)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Long, M., Zhu, H., Wang, J., Jordan, M.I.：具有残差迁移网络的无监督领域适应。在：神经信息处理系统进展。第136–144页
    (2016)'
- en: '[16] Luo, Z., Zou, Y., Hoffman, J., Fei-Fei, L.F.: Label efficient learning
    of transferable representations acrosss domains and tasks. In: Advances in Neural
    Information Processing Systems. pp. 164–176 (2017)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Luo, Z., Zou, Y., Hoffman, J., Fei-Fei, L.F.：跨领域和任务的标签高效学习可迁移表征。在：神经信息处理系统进展。第164–176页
    (2017)'
- en: '[17] Oquab, M., Bottou, L., Laptev, I., Sivic, J.: Learning and transferring
    mid-level image representations using convolutional neural networks. In: Computer
    Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1717–1724\.
    IEEE (2014)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Oquab, M., Bottou, L., Laptev, I., Sivic, J.：使用卷积神经网络学习和迁移中级图像表征。在：计算机视觉与模式识别
    (CVPR)，2014 IEEE 会议。第1717–1724页。IEEE (2014)'
- en: '[18] Pan, S.J., Tsang, I.W., Kwok, J.T., Yang, Q.: Domain adaptation via transfer
    component analysis. IEEE Transactions on Neural Networks 22(2), 199–210 (2011)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Pan, S.J., Tsang, I.W., Kwok, J.T., Yang, Q.：通过迁移分量分析进行领域适应。IEEE 神经网络汇刊
    22(2), 199–210 (2011)'
- en: '[19] Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Transactions
    on knowledge and data engineering 22(10), 1345–1359 (2010)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Pan, S.J., Yang, Q.：迁移学习综述。IEEE 知识与数据工程汇刊 22(10), 1345–1359 (2010)'
- en: '[20] Pardoe, D., Stone, P.: Boosting for regression transfer. In: Proceedings
    of the 27th International Conference on International Conference on Machine Learning.
    pp. 863–870\. Omnipress (2010)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Pardoe, D., Stone, P.：回归迁移的提升。在：第27届国际机器学习会议论文集。第863–870页。Omnipress (2010)'
- en: '[21] Tzeng, E., Hoffman, J., Darrell, T., Saenko, K.: Simultaneous deep transfer
    across domains and tasks. In: Computer Vision (ICCV), 2015 IEEE International
    Conference on. pp. 4068–4076\. IEEE (2015)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Tzeng, E., Hoffman, J., Darrell, T., Saenko, K.：跨领域和任务的同时深度迁移。在：计算机视觉
    (ICCV)，2015 IEEE 国际会议。第4068–4076页。IEEE (2015)'
- en: '[22] Tzeng, E., Hoffman, J., Saenko, K., Darrell, T.: Adversarial discriminative
    domain adaptation. In: Computer Vision and Pattern Recognition (CVPR). vol. 1,
    p. 4 (2017)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Tzeng, E., Hoffman, J., Saenko, K., Darrell, T.：对抗性判别领域适应。在：计算机视觉与模式识别
    (CVPR)。第1卷，第4页 (2017)'
- en: '[23] Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T.: Deep domain
    confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474 (2014)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T.：深度领域混淆：最大化领域不变性。arXiv
    预印本 arXiv:1412.3474 (2014)'
- en: '[24] Wan, C., Pan, R., Li, J.: Bi-weighting domain adaptation for cross-language
    text classification. In: IJCAI Proceedings-International Joint Conference on Artificial
    Intelligence. vol. 22, p. 1535 (2011)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Wan, C., Pan, R., Li, J.：用于跨语言文本分类的双重加权领域适应。在：IJCAI 论文集-国际人工智能联合会议。第22卷，第1535页
    (2011)'
- en: '[25] Weiss, K., Khoshgoftaar, T.M., Wang, D.: A survey of transfer learning.
    Journal of Big Data 3(1),  9 (2016)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Weiss, K., Khoshgoftaar, T.M., Wang, D.：迁移学习综述。大数据杂志 3(1), 9 (2016)'
- en: '[26] Xu, Y., Pan, S.J., Xiong, H., Wu, Q., Luo, R., Min, H., Song, H.: A unified
    framework for metric transfer learning. IEEE Transactions on Knowledge and Data
    Engineering 29(6), 1158–1171 (2017)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Xu, Y., Pan, S.J., Xiong, H., Wu, Q., Luo, R., Min, H., Song, H.：度量迁移学习的统一框架。IEEE
    知识与数据工程汇刊 29(6), 1158–1171 (2017)'
- en: '[27] Yao, Y., Doretto, G.: Boosting for transfer learning with multiple sources.
    In: Computer vision and pattern recognition (CVPR), 2010 IEEE conference on. pp.
    1855–1862\. IEEE (2010)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 姚野，Doretto，G.：《多源迁移学习的提升》。发表于：计算机视觉与模式识别（CVPR），2010年IEEE会议. 第1855–1862页。IEEE（2010年）'
- en: '[28] Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are
    features in deep neural networks? In: Advances in neural information processing
    systems. pp. 3320–3328 (2014)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Yosinski，J.，Clune，J.，Bengio，Y.，Lipson，H.：《深度神经网络中的特征可迁移性如何？》。发表于：神经信息处理系统进展。第3320–3328页（2014年）'
- en: '[29] Zhang, J., Li, W., Ogunbona, P.: Joint geometrical and statistical alignment
    for visual domain adaptation. In: CVPR (2017)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 张杰，李伟，Ogunbona，P.：《视觉领域适应的几何与统计联合对齐》。发表于：CVPR（2017年）'
- en: '[30] Zhu, H., Long, M., Wang, J., Cao, Y.: Deep hashing network for efficient
    similarity retrieval. In: AAAI. pp. 2415–2421 (2016)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 朱华，龙铭，王杰，曹阳：《深度哈希网络用于高效相似性检索》。发表于：AAAI. 第2415–2421页（2016年）'
