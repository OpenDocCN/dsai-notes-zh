- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:38:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:38:48
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2306.11740] A survey on deep learning approaches for data integration in autonomous
    driving system'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2306.11740] 自动驾驶系统中数据集成的深度学习方法综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.11740](https://ar5iv.labs.arxiv.org/html/2306.11740)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2306.11740](https://ar5iv.labs.arxiv.org/html/2306.11740)
- en: A survey on deep learning approaches
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习方法的综述
- en: for data integration in autonomous driving system
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶系统中的数据集成
- en: 'Xi Zhu^∗, Likang Wang^∗, Caifa Zhou, Xiya Cao, Yue Gong, Lei Chen^† Manuscript
    received DATE; revised DATE. ^∗ denotes equal contribution.^† Corresponding author.
    Email: leichen@cse.ust.hkXi Zhu, Caifa Zhou and Xiya Cao are with Riemann Laboratory,
    2012 Laboratories, Huawei Technologies, China.Likang Wang and Lei Chen are with
    Department of Computer Science and Engineering, Hong Kong University of Science
    and Technology, Hong Kong, China. This work is done when Likang Wang is an intern
    at Huawei Technologies.Yue Gong is with Parallel Distributed Computing Laboratory,
    2012 Laboratories, Huawei Technologies, China.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 朱曦^∗，王利康^∗，周才法，曹希亚，龚悦，陈磊^† 手稿收到日期；修订日期。^∗ 表示同等贡献。^† 通讯作者。电子邮件：leichen@cse.ust.hk
    朱曦、周才法和曹希亚在中国华为技术有限公司的黎曼实验室、2012实验室工作。王利康和陈磊在中国香港科技大学计算机科学与工程系工作。这项工作是在王利康担任华为技术有限公司实习生期间完成的。龚悦在中国华为技术有限公司并行分布式计算实验室工作。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The perception module of self-driving vehicles relies on a multi-sensor system
    to understand its environment. Recent advancements in deep learning have led to
    the rapid development of approaches that integrate multi-sensory measurements
    to enhance perception capabilities. This paper surveys the latest deep learning
    integration techniques applied to the perception module in autonomous driving
    systems, categorizing integration approaches based on “what, how, and when to
    integrate.” A new taxonomy of integration is proposed, based on three dimensions:
    multi-view, multi-modality, and multi-frame. The integration operations and their
    pros and cons are summarized, providing new insights into the properties of an
    “ideal” data integration approach that can alleviate the limitations of existing
    methods. After reviewing hundreds of relevant papers, this survey concludes with
    a discussion of the key features of an optimal data integration approach.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶车辆的感知模块依赖于多传感器系统来理解其环境。深度学习的最新进展促使了集成多传感器测量以增强感知能力的方法的快速发展。本文综述了最新的深度学习集成技术，这些技术应用于自动驾驶系统的感知模块，根据“集成的内容、方式和时间”对集成方法进行分类。提出了一种新的集成分类法，基于三个维度：多视角、多模态和多帧。总结了集成操作及其优缺点，提供了关于“理想”数据集成方法的新见解，该方法可以缓解现有方法的局限性。在回顾了数百篇相关论文之后，本综述以讨论最佳数据集成方法的关键特征作为结尾。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: 'autonomous driving, multi-view, multi-modality, multi-frame, data integration,
    deep learning^†^†publicationid: pubid: 0000–0000/00$00.00 © 2021 IEEE'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '自动驾驶，多视角，多模态，多帧，数据集成，深度学习^†^†publicationid: pubid: 0000–0000/00$00.00 © 2021
    IEEE'
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Perception is a crucial component of Autonomous Driving System (ADS)[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)]. It enables self-driving vehicles to perceive
    and comprehend their surroundings and accurately position themselves. The performance
    of the perception module significantly influences downstream tasks, such as planning
    and control, as well as driving safety. The two primary functions of the perception
    module are environment perception and localization [[2](#bib.bib2)]. Environment
    perception involves the vehicle actively gathering information about both static
    elements (e.g., lane lines, road markings, and traffic signs) and dynamic objects
    (e.g., other vehicles, pedestrians). In contrast, localization requires the vehicle
    to also consider its own motion state measurements, such as speed, heading, and
    acceleration. All these perception data can be obtained with the help of a sensor
    suite built-in the vehicle [[4](#bib.bib4)]. Although the sensor suite consists
    of a bunch of sensors [[2](#bib.bib2), [5](#bib.bib5), [6](#bib.bib6)], the ones
    used for environmental perception are cameras, LiDARs, and Millimeter Wave Radar
    (MMW-Radar)[[1](#bib.bib1), [7](#bib.bib7), [4](#bib.bib4), [8](#bib.bib8)]. Detailed
    properties of these sensors are further explained in Section [II](#S2 "II Sensing
    modalities and pre-processing ‣ A survey on deep learning approaches for data
    integration in autonomous driving system").
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 感知是自动驾驶系统（ADS）中的一个关键组件[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]。它使自动驾驶车辆能够感知和理解其周围环境，并准确定位自身。感知模块的性能对下游任务，如规划和控制，以及驾驶安全性有重要影响。感知模块的两个主要功能是环境感知和定位[[2](#bib.bib2)]。环境感知涉及车辆主动收集有关静态元素（例如车道线、道路标记和交通标志）和动态物体（例如其他车辆、行人）的信息。相比之下，定位还要求车辆考虑自身的运动状态测量，如速度、航向和加速度。所有这些感知数据可以通过车辆内置的传感器组合来获得[[4](#bib.bib4)]。尽管传感器组合包含一系列传感器[[2](#bib.bib2),
    [5](#bib.bib5), [6](#bib.bib6)]，用于环境感知的传感器包括摄像头、激光雷达和毫米波雷达（MMW-Radar）[[1](#bib.bib1),
    [7](#bib.bib7), [4](#bib.bib4), [8](#bib.bib8)]。这些传感器的详细属性将在第[II](#S2 "II Sensing
    modalities and pre-processing ‣ A survey on deep learning approaches for data
    integration in autonomous driving system")节中进一步说明。
- en: Although all sensors contribute to information collection, single-sensor systems
    have limitations and shortages that make it difficult to perform complete, accurate,
    and real-time environmental perception in autonomous driving applications [[9](#bib.bib9),
    [10](#bib.bib10), [2](#bib.bib2)]. This is because different sensors have different
    temporal and spatial coverage, as well as areas of expertise and weakness [[11](#bib.bib11),
    [7](#bib.bib7), [12](#bib.bib12), [13](#bib.bib13)]. For example, cameras can
    capture high-resolution images with rich color information, but they cannot work
    well in low-visibility scenarios or provide reliable 3D geometry. Additionally,
    cameras’ angular coverage is limited by their field-of-view. LiDARs are superior
    at 3D geometry estimation, have a wide range of view, and can work in dim light,
    but the points they capture are usually sparse due to the low sampling rate. Radars
    cannot acquire texture information, but they can capture the velocity of moving
    objects, which neither LiDARs nor cameras can do. Furthermore, single-sensor systems
    may suffer from the problem of deprivation, which describes the circumstances
    of perception loss or failure when the sensor stops working or cannot function
    well [[9](#bib.bib9)]. High uncertainty and imprecision are also significant concerns
    of single-sensor systems when data are missing or or measurements are not accurate.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有传感器都对信息收集有贡献，但单一传感器系统存在限制和不足，使得在自动驾驶应用中难以进行完整、准确、实时的环境感知[[9](#bib.bib9),
    [10](#bib.bib10), [2](#bib.bib2)]。这是因为不同的传感器具有不同的时间和空间覆盖范围，以及各自的专业领域和弱点[[11](#bib.bib11),
    [7](#bib.bib7), [12](#bib.bib12), [13](#bib.bib13)]。例如，摄像头可以捕捉高分辨率的图像，具有丰富的色彩信息，但在低能见度场景中表现不佳，也无法提供可靠的3D几何信息。此外，摄像头的角度覆盖受其视场的限制。激光雷达在3D几何估计方面表现优越，视野范围广泛，并且可以在昏暗光线中工作，但由于采样率较低，它们捕捉的点通常较稀疏。雷达无法获取纹理信息，但可以捕捉移动物体的速度，这是激光雷达和摄像头无法做到的。此外，单一传感器系统可能会遭遇感知丧失的问题，即传感器停止工作或无法正常运作时的感知丧失或失败情况[[9](#bib.bib9)]。当数据缺失或测量不准确时，高不确定性和不精确性也是单一传感器系统的重要问题。
- en: To address the limitations and challenges of single-sensor systems, various
    methods have been proposed to integrate data from different sensors in recent
    years [[9](#bib.bib9), [14](#bib.bib14), [4](#bib.bib4), [15](#bib.bib15)]. In
    the literature of Autonomous Driving System (ADS), data integration is often referred
    to as data fusion, data integration, or sensor fusion, which are used interchangeably
    [[16](#bib.bib16), [11](#bib.bib11), [17](#bib.bib17), [18](#bib.bib18)]. Data
    integration involves logically or physically transforming information from different
    sensors or sources to obtain a more consistent, informative, accurate, and reliable
    output than what could be achieved by using one sensor alone [[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [18](#bib.bib18), [22](#bib.bib22), [17](#bib.bib17),
    [23](#bib.bib23), [16](#bib.bib16), [11](#bib.bib11)]. Data integration techniques
    can be broadly divided into classical algorithms [[9](#bib.bib9), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [2](#bib.bib2), [27](#bib.bib27)] and deep
    learning approaches [[2](#bib.bib2)], with the latter being the focus of this
    paper due to its increasing popularity and potential for higher accuracy results.
    Deep learning-based integration techniques have drawn increasing attention, thanks
    to the rapid development of deep learning in computer science and artificial intelligence
    [[11](#bib.bib11), [7](#bib.bib7), [22](#bib.bib22), [28](#bib.bib28), [12](#bib.bib12),
    [4](#bib.bib4), [14](#bib.bib14), [15](#bib.bib15)]. They not only yield better
    accuracy results but also eliminate the need for hand-crafted design and fully
    exploit information contained in big data per high computational power [[5](#bib.bib5)].
    Data integration techniques have also been explored in other research fields,
    such as smart living [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33)], medical field [[34](#bib.bib34)], transportation [[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37)], industry [[38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40)], and business [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)].
    However, since the current paper focuses on data integration in ADS, readers interested
    in other domains are referred to relevant reviews [[44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]. In the
    next, we will summarize the key issues that need to be addressed in data integration
    in ADS.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对单一传感器系统的局限性和挑战，近年来提出了各种方法来集成来自不同传感器的数据 [[9](#bib.bib9), [14](#bib.bib14),
    [4](#bib.bib4), [15](#bib.bib15)]。在自动驾驶系统 (ADS) 的文献中，数据集成通常被称为数据融合、数据集成或传感器融合，这些术语可以互换使用
    [[16](#bib.bib16), [11](#bib.bib11), [17](#bib.bib17), [18](#bib.bib18)]。数据集成涉及逻辑上或物理上转换来自不同传感器或来源的信息，以获得比单一传感器使用所能获得的更一致、信息丰富、准确和可靠的输出
    [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [18](#bib.bib18), [22](#bib.bib22),
    [17](#bib.bib17), [23](#bib.bib23), [16](#bib.bib16), [11](#bib.bib11)]。数据集成技术大致可以分为经典算法
    [[9](#bib.bib9), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [2](#bib.bib2),
    [27](#bib.bib27)] 和深度学习方法 [[2](#bib.bib2)]，由于后者越来越受关注且可能实现更高的准确性结果，因此本论文重点讨论深度学习方法。基于深度学习的集成技术由于计算机科学和人工智能中深度学习的快速发展而受到越来越多的关注
    [[11](#bib.bib11), [7](#bib.bib7), [22](#bib.bib22), [28](#bib.bib28), [12](#bib.bib12),
    [4](#bib.bib4), [14](#bib.bib14), [15](#bib.bib15)]。它们不仅能提供更好的准确性结果，还消除了手工设计的需要，并充分利用了大数据中的信息，通过高计算能力
    [[5](#bib.bib5)]。数据集成技术也在其他研究领域得到了探索，如智能生活 [[29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33)]，医学领域 [[34](#bib.bib34)]，交通
    [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]，工业 [[38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40)]，以及商业 [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]。然而，由于当前论文重点关注
    ADS 中的数据集成，对其他领域感兴趣的读者可以参考相关综述 [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]。接下来，我们将总结在 ADS 中需要解决的数据集成的关键问题。
- en: '![Refer to caption](img/11d34f0be6283d50afb6fd9423485b3c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/11d34f0be6283d50afb6fd9423485b3c.png)'
- en: (a)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/d2ada974d6c2fdc61341a4a929be2999.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/d2ada974d6c2fdc61341a4a929be2999.png)'
- en: (b)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 1: Bi-level taxonomy of “what to integrate”. (a) Seven upper-level categories
    based on three dimensions, describing the content of integration. (b) Three lower-level
    paradigms of each two-dimensional integration category, based on the order of
    integration.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: “集成内容” 的双层分类法。 (a) 基于三个维度的七个高级类别，描述集成的内容。 (b) 每个二维集成类别的三个低级范式，基于集成的顺序。'
- en: Key problems of data integration
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集成的关键问题
- en: Three widely accepted key problems to be addressed in data integration include
    “what to integrate”, “how to integrate”, and “when to integrate” [[50](#bib.bib50),
    [51](#bib.bib51), [14](#bib.bib14)]. Many reviews related to deep learning based
    data integration in ADS perception have been published in recent years, each of
    which partially covers these three problems and related techniques. While some
    reviews take data integration as a technical approach to address certain perception
    tasks under specific circumstances [[52](#bib.bib52), [7](#bib.bib7), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57)], others
    center their attention on the data integration itself, including techniques, categories,
    and applications [[11](#bib.bib11), [2](#bib.bib2), [28](#bib.bib28), [12](#bib.bib12),
    [4](#bib.bib4), [14](#bib.bib14), [58](#bib.bib58), [15](#bib.bib15)]. We mainly
    make comparisons with the latter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整合中三个广泛接受的关键问题包括“整合什么”、“如何整合”和“何时整合”[[50](#bib.bib50), [51](#bib.bib51), [14](#bib.bib14)]。近年来，许多与基于深度学习的数据整合相关的评论已被发布，其中每篇都部分涵盖了这三个问题及相关技术。虽然一些评论将数据整合作为在特定情况下解决某些感知任务的技术方法[[52](#bib.bib52),
    [7](#bib.bib7), [53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57)]，但其他评论则将重点放在数据整合本身，包括技术、类别和应用[[11](#bib.bib11), [2](#bib.bib2),
    [28](#bib.bib28), [12](#bib.bib12), [4](#bib.bib4), [14](#bib.bib14), [58](#bib.bib58),
    [15](#bib.bib15)]。我们主要与后者进行比较。
- en: I-1 What to integrate
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: I-1 整合什么
- en: '“What to integrate” is a question about the input to the integration process.
    In this survey, we divide “what to integrate” into two hierarchical sub-questions:
    what is the content to integrate, and what is the order to integrate.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: “整合什么”是一个关于整合过程输入的问题。在本调查中，我们将“整合什么”分为两个层次的子问题：整合的内容是什么，整合的顺序是什么。
- en: a) What is the content to integrate?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: a) 整合的内容是什么？
- en: 'We divide the content of integration into three dimensions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将整合的内容分为三个维度：
- en: '-'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-'
- en: Multi-view integration involves integrating representations or views from multiple
    sensors of the same type that point in different directions. For example, integrating
    point clouds from multiple LiDARs placed at different locations on a car or integrating
    images taken from multiple cameras with different orientations [[59](#bib.bib59)].
    However, combining different view representations of one LiDAR point cloud, such
    as integrating a Bird’s-Eye-View (BEV)and a range view of LiDAR point cloud, is
    not considered multi-view integration since it does not introduce new information.
    These view transformations are considered part of the pre-processing of sensed
    data.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多视角整合涉及整合来自相同类型但指向不同方向的多个传感器的表示或视图。例如，整合安装在车上不同位置的多个LiDAR的点云，或整合来自不同方向的多个摄像头拍摄的图像[[59](#bib.bib59)]。然而，将同一个LiDAR点云的不同视角表示进行组合，如整合LiDAR点云的鸟瞰图（BEV）和距离视图，并不被视为多视角整合，因为这并未引入新信息。这些视角转换被视为传感数据预处理的一部分。
- en: '-'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-'
- en: Multi-modality integration involves integrating data from different types of
    sensors, such as cameras, LiDAR, and Millimeter Wave Radar (MMW-Radar), in any
    combination. Research has shown that fusing camera and LiDAR data is one of the
    most common types of integration.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多模态整合涉及将来自不同类型传感器的数据进行整合，如摄像头、LiDAR和毫米波雷达（MMW-Radar），以任意组合进行。研究表明，融合摄像头和LiDAR数据是最常见的整合类型之一。
- en: '-'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-'
- en: Multi-frame integration involves integrating data over time, while the former
    two dimensions focus on spatial integration at one time step.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多帧整合涉及在时间上整合数据，而前两个维度则关注在一个时间步骤的空间整合。
- en: Given these three dimensions, we could classify all data integration methods
    into seven upper-level categories, as shown in Figure [1a](#S1.F1.sf1 "In Figure
    1 ‣ I Introduction ‣ A survey on deep learning approaches for data integration
    in autonomous driving system"). Data integration cannot only be implemented within
    one of three dimensions, but also be carried out over multiple dimensions. The
    former is denoted as single-dimensional integration.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这三个维度，我们可以将所有数据整合方法归类为七个高层次类别，如图 [1a](#S1.F1.sf1 "在图1 ‣ I 介绍 ‣ 关于自动驾驶系统的数据整合深度学习方法的调查")所示。数据整合不仅可以在一个维度内实现，还可以跨多个维度进行。前者称为单维整合。
- en: b) What is the order to integrate?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: b) 整合的顺序是什么？
- en: 'Multidimensional integration, such as “multi-modality and multi-view,” is different
    from single-dimensional integration as it requires multiple combinations of operations.
    We further categorize multidimensional integration into lower-level integration
    paradigms based on the integration order, as illustrated in Figure [1b](#S1.F1.sf2
    "In Figure 1 ‣ I Introduction ‣ A survey on deep learning approaches for data
    integration in autonomous driving system"). For instance, in two-dimensional “multi-modality
    and multi-view” integration, there are three sub-level categories or paradigms:
    modality-first integration, view-first integration, and deeply coupled integration.
    Three-dimensional integration is even more complex, and to our knowledge, the
    corresponding paradigms have not been discussed in existing studies. Therefore,
    we limit our discussion of relevant literature to one- or two-dimensional integration
    in Section [III](#S3 "III Data Integration: What to Integrate ‣ A survey on deep
    learning approaches for data integration in autonomous driving system").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 多维整合，例如“多模态和多视角”，不同于单维整合，因为它需要多种操作的组合。我们进一步根据整合顺序将多维整合分类为低级整合范式，如图 [1b](#S1.F1.sf2
    "在图1 ‣ I 引言 ‣ 关于自动驾驶系统中数据整合的深度学习方法综述")所示。例如，在二维“多模态和多视角”整合中，有三种子级别的分类或范式：模态优先整合、视角优先整合和深度耦合整合。三维整合则更加复杂，据我们所知，现有研究中尚未讨论相应的范式。因此，我们在第 [III](#S3
    "III 数据整合：整合什么 ‣ 关于自动驾驶系统中数据整合的深度学习方法综述")节中将相关文献的讨论限制在一维或二维整合。
- en: I-2 When to integrate
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: I-2 何时整合
- en: 'The topic of “when to integrate” pertains to the level of data abstraction
    at which integration occurs. This has been extensively covered in literature,
    with most adopting a widely used categorization scheme based on the input data
    abstraction stage. This scheme includes data-level (or pixel, signal, or low-level),
    feature-level (middle-level), and decision-level (or result or high-level) fusion
    [[2](#bib.bib2), [12](#bib.bib12), [14](#bib.bib14), [58](#bib.bib58)]. Data-level
    integration methods fuse raw or preprocessed data before feature extraction, while
    feature-level integration combines extracted features at intermediate neural network
    layers. Decision-level integration involves merging the output data separately
    estimated by each sensor. In addition to these levels of integration, a new category
    of “when-to-integrate,” called multi-level integration, has emerged in recent
    years as more studies attempt to integrate data at different levels [[28](#bib.bib28)].
    An example of multi-level integration is using result-level 2D bounding boxes
    from RGB images to select data-level or feature-level 3D LiDAR data [[60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62)]. Each level of data integration has its own
    advantages and disadvantages, and no evidence supports one level being superior
    to another [[14](#bib.bib14)]. In Section [IV](#S4 "IV Data Integration: When
    to Integrate ‣ A survey on deep learning approaches for data integration in autonomous
    driving system"), we review existing integration methods based on these four categories
    and summarize their pros and concons.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: “何时整合”这一主题涉及整合发生的数据抽象层级。这在文献中已有广泛的探讨，大多数采用基于输入数据抽象阶段的广泛分类方案。该方案包括数据级（或像素、信号、低级）、特征级（中级）和决策级（或结果、高级）融合
    [[2](#bib.bib2), [12](#bib.bib12), [14](#bib.bib14), [58](#bib.bib58)]。数据级整合方法在特征提取之前融合原始或预处理数据，而特征级整合在中间神经网络层结合提取的特征。决策级整合涉及合并由每个传感器单独估计的输出数据。除了这些整合级别，近年来出现了一种新的“何时整合”类别，称为多级整合，因为越来越多的研究尝试在不同级别整合数据
    [[28](#bib.bib28)]。多级整合的一个例子是使用来自RGB图像的结果级2D边界框来选择数据级或特征级3D LiDAR数据 [[60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62)]。每个数据整合级别都有其优缺点，目前没有证据表明某一层级优于其他层级 [[14](#bib.bib14)]。在第 [IV](#S4
    "IV 数据整合：何时整合 ‣ 关于自动驾驶系统中数据整合的深度学习方法综述")节中，我们基于这四个类别回顾了现有的整合方法，并总结了它们的优缺点。
- en: I-3 How to integrate
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: I-3 如何整合
- en: The question of “how to integrate” pertains to the specific integration operations
    used to mathematically combine data. These operations are often overlooked or
    not explicitly stated since they are typically simple and straightforward, such
    as addition or concatenation. However, in our survey, we present several common
    integration operations, including projection, concatenation, addition/average
    mean/weighted summation, probabilistic method, rule-based transaction, temporal
    integration approaches, and neural network/encoder-decoder structure. We provide
    a summary of each integration operation’s properties and practical applicationions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: “如何整合”的问题涉及到用于数学结合数据的具体整合操作。这些操作通常被忽视或未明确说明，因为它们通常是简单直接的，比如加法或连接。然而，在我们的调查中，我们展示了几种常见的整合操作，包括投影、连接、加法/平均值/加权求和、概率方法、基于规则的事务、时间整合方法，以及神经网络/编码器-解码器结构。我们提供了每种整合操作的属性和实际应用的总结。
- en: Purposes and paper organization
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 目的和论文组织
- en: Our survey aims to offer a comprehensive overview of the latest (from 2017 to
    2023) deep learning-based data integration techniques for camera, LiDAR, and MMW-Radar
    in ADS perception, following the previously mentioned taxonomy. After reviewing
    hundreds of pertinent papers, we identify limitations in existing integration
    methods and present a discussion addressing the open question of the ideal data
    integration approach for ADS perception.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查旨在提供最新的（从2017年到2023年）基于深度学习的ADS感知中相机、LiDAR和MMW雷达数据整合技术的全面概述，遵循之前提到的分类法。在审查了数百篇相关论文后，我们识别了现有整合方法的局限性，并提出了讨论，解决了关于ADS感知理想数据整合方法的开放问题。
- en: '| Reference | Task | Sensor | Data representation | Integration operation |
    Method Pros & Cons | Dataset | Calibration |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 任务 | 传感器 | 数据表示 | 整合操作 | 方法优缺点 | 数据集 | 校准 |  |'
- en: '| Wang et al. [[11](#bib.bib11)] | Various | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    |  |  |  |  |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[11](#bib.bib11)] | 各种 | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    |  |  |  |  |  |'
- en: '| Fayyad et al. [[2](#bib.bib2)] | Various | $\boldsymbol{\checkmark}$ |  |  |
    $\boldsymbol{\checkmark}$ |  |  |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Fayyad et al. [[2](#bib.bib2)] | 各种 | $\boldsymbol{\checkmark}$ |  |  | $\boldsymbol{\checkmark}$
    |  |  |  |'
- en: '| Cui et al. [[28](#bib.bib28)] | Various |  | $\boldsymbol{\checkmark}$ |  |
    $\boldsymbol{\checkmark}$ |  | $\boldsymbol{\checkmark}$ |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Cui et al. [[28](#bib.bib28)] | 各种 |  | $\boldsymbol{\checkmark}$ |  | $\boldsymbol{\checkmark}$
    |  | $\boldsymbol{\checkmark}$ |  |'
- en: '| Yeong et al. [[12](#bib.bib12)] | OD | $\boldsymbol{\checkmark}$ |  |  |
    $\boldsymbol{\checkmark}$ |  | $\boldsymbol{\checkmark}$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Yeong et al. [[12](#bib.bib12)] | OD | $\boldsymbol{\checkmark}$ |  |  |
    $\boldsymbol{\checkmark}$ |  | $\boldsymbol{\checkmark}$ |  |'
- en: '| Huang et al. [[4](#bib.bib4)] | Various |  | $\boldsymbol{\checkmark}$ |  |  |
    $\boldsymbol{\checkmark}$ |  |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al. [[4](#bib.bib4)] | 各种 |  | $\boldsymbol{\checkmark}$ |  |  |
    $\boldsymbol{\checkmark}$ |  |  |'
- en: '| Feng et al. [[14](#bib.bib14)] | OD & SS | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    | $\boldsymbol{\checkmark}$ |  | $\boldsymbol{\checkmark}$ |  |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Feng et al. [[14](#bib.bib14)] | OD & SS | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    | $\boldsymbol{\checkmark}$ |  | $\boldsymbol{\checkmark}$ |  |  |'
- en: '| Wang et al. [[58](#bib.bib58)] | OD | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    |  | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ |  |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[58](#bib.bib58)] | OD | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    |  | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ |  |  |'
- en: '| Wang et al. [[15](#bib.bib15)] | OD | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    |  |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[15](#bib.bib15)] | OD | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    |  |  |'
- en: '| Ours | Various | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ |
    $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ |  |  |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | 各种 | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    | $\boldsymbol{\checkmark}$ |  |  |  |'
- en: 'TABLE I: Brief summary of recent reviews on data integration in ADS perception
    using deep learning approaches. “OD” represents object detestion, and “SS” means
    semantic segmentation.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：关于使用深度学习方法进行ADS感知数据整合的近期综述的简要总结。“OD”代表对象检测，“SS”表示语义分割。
- en: 'In Table [I](#S1.T1 "TABLE I ‣ I-3 How to integrate ‣ I Introduction ‣ A survey
    on deep learning approaches for data integration in autonomous driving system"),
    we briefly compare the content covered by related surveys on deep learning-based
    data integration with ours. The key contributions of our survey are summarized
    as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [I](#S1.T1 "表格 I ‣ I-3 如何集成 ‣ I 介绍 ‣ 自动驾驶系统中数据集成的深度学习方法调查") 中，我们简要比较了与我们的基于深度学习的数据集成相关调查所涵盖的内容。我们调查的主要贡献总结如下：
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose three fundamental dimensions (multi-view, multi-modality, multi-frame)
    for data integration, based on which a new bi-level taxonomy is created. The upper-level
    taxonomy outlines the content to integrate with seven categories, while the lower
    level examines the order of integration. This bi-level taxonomy allows for a comprehensive
    categorization of data integration approaches.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了三个基本维度（多视图，多模态，多帧）用于数据集成，基于此创建了一个新的双层分类法。上层分类法概述了要与七个类别集成的内容，而下层则考察了集成的顺序。这种双层分类法可以全面分类数据集成方法。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We summarize the common integration operations used in deep learning models,
    as well as their pros and cons, which are often not fully explored in other reviews.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结了深度学习模型中常用的集成操作，以及它们的利弊，这在其他调查中往往未被充分探讨。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Based on our review, we provide a detailed summary and analysis of the limitations
    of existing integration techniques in ADS perception and suggest our vision for
    the ideal data integration approach.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据我们的审查，我们详细总结和分析了现有集成技术在ADS感知方面的局限性，并提出了我们对理想数据集成方法的愿景。
- en: 'The remainder of this paper is organized as follows: In Section [II](#S2 "II
    Sensing modalities and pre-processing ‣ A survey on deep learning approaches for
    data integration in autonomous driving system"), we discuss the measuring principles,
    characteristics, advantages, and disadvantages of three types of sensors commonly
    used in ADS perception and their representations. We then offer an overview of
    “what to integrate,” “when to integrate,” and “how to integrate” in Section [III](#S3
    "III Data Integration: What to Integrate ‣ A survey on deep learning approaches
    for data integration in autonomous driving system"), Section [IV](#S4 "IV Data
    Integration: When to Integrate ‣ A survey on deep learning approaches for data
    integration in autonomous driving system"), and Section [V](#S5 "V Data Integration:
    How to Integrate ‣ A survey on deep learning approaches for data integration in
    autonomous driving system"), respectively. In Section [VI](#S6 "VI Case Study
    ‣ A survey on deep learning approaches for data integration in autonomous driving
    system"), we present a case study illustrating the practical applications of these
    three questions. In the final section, we summarize our work and discuss ideal
    data integration approaches and future directions.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：在第 II 部分 [（链接）](#S2 "II 感知模态和预处理 ‣ 自动驾驶系统中数据集成的深度学习方法调查")，我们讨论了ADS感知中三种常用传感器的测量原理，特点，优点和缺点以及它们的表示形式。然后，我们在第
    III 部分 [（链接）](#S3 "III 数据集成：什么要集成 ‣ 自动驾驶系统中数据集成的深度学习方法调查") 、第 IV 部分 [（链接）](#S4
    "IV 数据集成：什么时候集成 ‣ 自动驾驶系统中数据集成的深度学习方法调查") 和第 V 部分 [（链接）](#S5 "V 数据集成：如何集成 ‣ 自动驾驶系统中数据集成的深度学习方法调查")
    中概述了“何时集成”，以及“如何集成”。在第 VI 部分 [（链接）](#S6 "VI 案例研究 ‣ 自动驾驶系统中数据集成的深度学习方法调查") 中，我们提供了一个案例研究，展示了这三个问题的实际应用。在最后一部分，我们总结了我们的工作，并讨论了理想的数据集成方法和未来方向。
- en: II Sensing modalities and pre-processing
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 感知模态和预处理
- en: 'Commonly used sensors in ADS perception can be classified into two groups according
    to their operational principles [[12](#bib.bib12), [28](#bib.bib28)]:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '根据它们的工作原理，ADS感知中常用的传感器可以分为两类[[12](#bib.bib12), [28](#bib.bib28)]： '
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Exteroceptive sensors, such as cameras, LiDARs, and MMW-Radar s, actively collect
    data of surroundings to perceive the external environment.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 外感传感器，如摄像头，LiDAR和MMW雷达，主动收集环境数据以感知外部环境。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Proprioceptive sensors, including IMU, wheelmeter, GPS receiver, etc., are mainly
    used to capture the internal states of vehicles and the dynamic measurements of
    the system. From the perspective of tasks, these sensors are widely employed together
    with exteroceptive sensors for positioning and localization.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本体感知传感器，包括IMU、车轮计、GPS接收器等，主要用于捕捉车辆的内部状态和系统的动态测量。从任务的角度来看，这些传感器与外部感知传感器一起广泛用于定位和定位。
- en: The data collected by the aforementioned sensors vary in terms of coordinate
    frames and characteristics, each with their own strengths and limitations. Integrating
    data from different coordinate frames can be challenging, and many existing deep
    learning architectures are designed to process specific data representations.
    To address this issue, a common approach is to apply pre-processing methods to
    transform raw data into appropriate representations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以上传感器收集的数据在坐标框架和特征方面各不相同，每种都有自己的优点和局限性。从不同坐标框架中整合数据可能具有挑战性，许多现有的深度学习架构被设计来处理特定的数据表示。为了解决这个问题，一种常见的方法是应用预处理方法，将原始数据转换为适当的表示。
- en: In this section, we focus primarily on three types of exteroceptive sensors,
    discussing their properties and pre-processing methods (representations). Table [II](#S2.T2
    "TABLE II ‣ II Sensing modalities and pre-processing ‣ A survey on deep learning
    approaches for data integration in autonomous driving system") provides a summary
    of the advantages and disadvantages of these sensors.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们主要关注三种外部感知传感器，讨论它们的属性和预处理方法（表示）。表[II](#S2.T2 "TABLE II ‣ II Sensing modalities
    and pre-processing ‣ A survey on deep learning approaches for data integration
    in autonomous driving system")总结了这些传感器的优缺点。
- en: 'TABLE II: Comparison of camera, LiDAR, and radar sensors.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：相机、LiDAR 和雷达传感器的比较。
- en: '| Sensor | Data Format | Resolution | HFoV | Geometry | Texture | Bad weather
    | Dim/Dark | Velocity | Cost |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 传感器 | 数据格式 | 分辨率 | HFoV | 几何 | 纹理 | 恶劣天气 | 昏暗/黑暗 | 速度 | 成本 |'
- en: '| Camera | 2D pixels | ++ | + | - | ++ | + | - | - | Low |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 相机 | 2D 像素 | ++ | + | - | ++ | + | - | - | 低 |'
- en: '| LiDAR | 3D points | + | ++ | ++ | - | + | ++ | - | High |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| LiDAR | 3D 点 | + | ++ | ++ | - | + | ++ | - | 高 |'
- en: '| Radar | 3D points | + | + | + | - | ++ | ++ | ++ | Low |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 雷达 | 3D 点 | + | + | + | - | ++ | ++ | ++ | 低 |'
- en: '++: Comparatively has strong capability. +: Has limited capability. -: Comparatively
    has weak capability.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '++: 相对具有强大的能力。 +: 具有有限的能力。 -: 相对具有较弱的能力。'
- en: II-A Camera
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 相机
- en: Cameras are optical devices capable of capturing 2D visual images. While some
    cameras (e.g., infrared) can detect invisible light, the term “camera” usually
    refers to those sensing visible light. Cameras can generate both grayscale and
    colored images, although most modern cameras default to producing colored images.
    Some cameras (e.g., gated, time of flight (TOF), structured light) emit waves
    and detect their responses, but we do not focus on these as they operate similarly
    to LiDAR. Instead, this section concentrates on RGB cameras, the most common optical
    sensors, which passively receive visible light with wavelengths between 400 and
    700 nm and output colored images.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 相机是能够捕捉2D视觉图像的光学设备。虽然一些相机（例如红外相机）可以检测到不可见光，但“相机”通常指的是那些感应可见光的设备。相机可以生成灰度图像和彩色图像，尽管大多数现代相机默认生成彩色图像。一些相机（例如门控相机、飞行时间（TOF）相机、结构光相机）会发射波并检测其响应，但我们不重点讨论这些，因为它们的工作原理类似于LiDAR。相反，本节重点介绍RGB相机，这些最常见的光学传感器被动接收波长在400到700
    nm之间的可见光并输出彩色图像。
- en: In general, cameras can be modeled with a pinhole model. Each point in 3D space
    is projected to a pixel according to an affine transformation determined by the
    projection matrix. This projection is related to both the camera’s intrinsic properties
    and its pose in the 3D world. They can be described by two affine transformations
    determined by the intrinsic and extrinsic matrices, respectively. Most cameras
    have only one lens, called monocular, while others, called stereo, may have multiple
    lenses. Stereo cameras mimic human binocular vision and can perceive 3D objects
    with algorithms. Monocular cameras can also recover 3D information by filming
    the same object with multiple cameras or from multiple poses. The main difference
    between these two types is that the relative positions of lenses and directions
    are fixed and known inside a stereo camera but are unknown and require further
    estimation for monocular systems.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，相机可以用针孔模型建模。每个 3D 空间中的点根据由投影矩阵确定的仿射变换投影到一个像素上。该投影与相机的内在属性及其在 3D 世界中的姿态有关。它们可以通过分别由内在和外在矩阵确定的两个仿射变换来描述。大多数相机只有一个镜头，称为单眼相机，而其他相机，称为立体相机，可能有多个镜头。立体相机模拟人类的双目视觉，并可以通过算法感知
    3D 物体。单眼相机也可以通过用多个相机或从多个姿态拍摄同一物体来恢复 3D 信息。这两种类型的主要区别在于立体相机内的镜头和方向的相对位置是固定且已知的，而单眼系统的相对位置则是未知的，需要进一步估计。
- en: RGB cameras aim to reproduce images perceived by human eyes and are essential
    for capturing colored and textured regions (e.g., road lanes, traffic signs, and
    traffic lights). These cameras generally have very high spatial (hundreds to thousands
    in height and width) and temporal (dozens to hundreds of frames per second) resolutions.
    They often have a long working range, with the maximum perception distance reaching
    1 km in good weather conditions. Their ability to capture color and texture information
    aids semantic comprehension. Moreover, cameras have low energy and manufacturing
    costs, allowing widespread deployment. However, RGB cameras have limitations in
    lighting conditions and line-of-sight visibility. Due to their working principle,
    they struggle to detect lightless objects (e.g., road lanes and obstacles) in
    poorly lit scenes and have weak occluded object detection capabilities. Additionally,
    single-frame images taken by monocular cameras lack geometric information, requiring
    multiple images and complex algorithms (e.g., depth recovery and 3D reconstruction)
    for depth and 3D structure estimation. Cameras are also vulnerable to external
    environments, as the lens may be blurred by liquid (e.g., rain, spray, and wheel
    splash), and distant objects may not be recognizable in fog.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RGB 相机旨在重现人眼感知的图像，对于捕捉彩色和纹理区域（如道路车道、交通标志和交通信号灯）至关重要。这些相机通常具有非常高的空间（高度和宽度从数百到数千）和时间（每秒几十到几百帧）分辨率。它们通常具有较长的工作范围，在良好天气条件下最大感知距离可达
    1 公里。它们捕捉颜色和纹理信息的能力有助于语义理解。此外，相机具有较低的能耗和制造成本，允许广泛部署。然而，RGB 相机在光照条件和视线可见性方面有局限性。由于工作原理，它们难以在光线不足的场景中检测无光物体（如道路车道和障碍物），并且在被遮挡物体检测能力较弱。此外，单帧图像由单眼相机拍摄，缺乏几何信息，需要多张图像和复杂算法（如深度恢复和
    3D 重建）来估计深度和 3D 结构。相机也容易受到外部环境的影响，因为镜头可能会被液体（如雨水、喷雾和车轮飞溅）模糊，且远处物体可能在雾中无法识别。
- en: In ADS, the integrated camera data are represented in either 2D or 3D formats.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ADS 中，集成的相机数据以 2D 或 3D 格式表示。
- en: II-A1 Pixel representation
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 像素表示
- en: The representation of pixels involves storing pixel features on a 2D image plane,
    where each pixel has multiple channels to describe its properties. This results
    in the entire image being stored in a 3D matrix with dimensions of $[height,\
    width,\ channel]$. Typically, RGB raw images have three colored channels, but
    other cameras may have different channels such as depth, gray, infrared, or gated
    channels.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 像素表示涉及在 2D 图像平面上存储像素特征，每个像素具有多个通道以描述其属性。这使得整个图像以 $[height,\ width,\ channel]$
    维度的 3D 矩阵存储。通常，RGB 原始图像具有三个颜色通道，但其他相机可能有不同的通道，如深度、灰度、红外或门控通道。
- en: II-A2 Point or voxel representation
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 点或体素表示
- en: The representation of points or voxels takes into account the depth information
    by projecting each pixel into 3D space. These 3D points can be stored as either
    point clouds or voxel grids. Point clouds assign each point a float number 3D
    coordinate, resulting in a matrix with dimensions of $[n,\ c+3]$, where $n$ represents
    the number of pixels and $c$ represents the number of channels. Voxel grids divide
    the space into grids with dimensions of $[height,\ width,\ depth]$, and points
    are placed into these grids.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 点或体素的表示通过将每个像素投影到 3D 空间中来考虑深度信息。这些 3D 点可以存储为点云或体素网格。点云为每个点分配一个浮点数 3D 坐标，结果是一个维度为
    $[n,\ c+3]$ 的矩阵，其中 $n$ 表示像素的数量，$c$ 表示通道的数量。体素网格将空间划分为 $[height,\ width,\ depth]$
    维度的网格，并将点放置到这些网格中。
- en: II-B LiDAR
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B LiDAR
- en: Light Detection and Ranging, a.k.a. LiDAR, is a technique commonly used for
    range measurements in autonomous driving [[63](#bib.bib63)]. Its working principle
    is to estimate the time intervals between emitted light pulses and received signals
    reflected by target objects, and obtain the distances with the time intervals
    and light speed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 光检测与测距，简称 LiDAR，是一种常用于自动驾驶中进行距离测量的技术[[63](#bib.bib63)]。其工作原理是估算发射光脉冲与目标物体反射信号之间的时间间隔，并通过时间间隔和光速来获取距离。
- en: Three types of LiDAR, including 1D, 2D, and 3D LiDAR, are used to collect different
    amount of environment information [[12](#bib.bib12)]. While 1D LiDAR can only
    provide measurement of distance, 2D LiDAR can obtain the spatial information on
    an X-Y coordinate horizontal plane of the target by spinning certain degrees horizontally.
    The horizontal degree a LiDAR sensor rotates over is called the Horizontal Field
    of View (HFoV)of the sensor. 3D LiDAR sensors expand the vertical view by firing
    multiple lasers vertically, making the data collected in the 3D X-Y-Z coordinate
    system. 3D LiDAR sensors are more commonly employed in autonomous vehicles, while
    the high price is the concern in implementation [[11](#bib.bib11), [64](#bib.bib64)].
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 三种类型的 LiDAR，包括 1D、2D 和 3D LiDAR，用于收集不同数量的环境信息[[12](#bib.bib12)]。虽然 1D LiDAR
    只能提供距离测量，2D LiDAR 可以通过水平旋转一定的角度来获得目标在 X-Y 坐标水平面上的空间信息。LiDAR 传感器的水平旋转角度称为传感器的水平视场
    (HFoV)。3D LiDAR 传感器通过垂直发射多束激光来扩展垂直视野，使得数据可以在 3D X-Y-Z 坐标系统中进行采集。3D LiDAR 传感器更常用于自动驾驶车辆中，但其高昂的价格是实施中的一个问题[[11](#bib.bib11),
    [64](#bib.bib64)]。
- en: The process of generating data using LiDAR sensors involves the use of beams
    of light to draw samples from the surfaces of objects in the surrounding environment.
    This laser-firing working principle allows LiDAR sensors to function well in low-visibility
    conditions but makes them susceptible to external weather conditions such as rain,
    fog, snow, and dusty environments [[65](#bib.bib65)]. Additionally, the color
    of the target can impact the performance of LiDAR sensors, with darker-colored
    objects absorbing light and being less reflective than lighter-colored ones [[66](#bib.bib66)].
    The sampling range of the scene is determined by the HFoV and Vertical Field of
    View (VFoV), while other parameters such as horizontal/vertical resolution and
    fragment per second (FPS)contribute to the data intensity. Horizontal and vertical
    resolution refer to the density of sampling in space, with smaller resolution
    resulting in denser sampling given fixed HFoV and VFoV. FPS describes the sampling
    density in time, i.e., how many scans the LiDAR conducts per second. Detailed
    specifications of different LiDAR sensors from different companies are provided
    by Yeong et al. [[12](#bib.bib12)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LiDAR 传感器生成数据的过程涉及使用光束从周围环境中物体的表面采样。这种激光发射的工作原理使 LiDAR 传感器在低能见度条件下表现良好，但也使其容易受到雨、雾、雪和尘土等外部天气条件的影响[[65](#bib.bib65)]。此外，目标的颜色也会影响
    LiDAR 传感器的性能，较暗的物体吸收光线并且反射率低于较亮的物体[[66](#bib.bib66)]。场景的采样范围由水平视场 (HFoV) 和垂直视场
    (VFoV) 决定，而其他参数如水平/垂直分辨率和每秒帧数 (FPS) 则影响数据的密度。水平和垂直分辨率指的是空间中的采样密度，分辨率越小，在固定的 HFoV
    和 VFoV 下采样越密集。FPS 描述了时间上的采样密度，即 LiDAR 每秒进行的扫描次数。Yeong 等人提供了来自不同公司不同 LiDAR 传感器的详细规格[[12](#bib.bib12)]。
- en: Unlike camera images, 3D LiDAR measurements are a set of irregular and unordered
    data points, referred to as point clouds in 3D structure [[28](#bib.bib28)]. To
    fit the input format of different deep learning models, point clouds can be transformed
    into several different representations using pre-processing methods. It is worth
    noting that LiDAR data is sparser compared to image data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与相机图像不同，3D LiDAR测量是一组不规则且无序的数据点，称为3D结构中的点云 [[28](#bib.bib28)]。为了适应不同深度学习模型的输入格式，点云可以通过预处理方法转换成几种不同的表示形式。值得注意的是，LiDAR数据相比于图像数据更为稀疏。
- en: II-B1 Point representation
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 点云表示
- en: 3D point clouds obtained from LiDAR sensors can be processed without format
    transformation with point processing deep learning networks such as PointNet [[67](#bib.bib67)],
    PointNet++ [[68](#bib.bib68)], PointCNN [[69](#bib.bib69)], and KPConv [[70](#bib.bib70)].
    LiDAR point clouds can be integrated with similar point-format data such as other
    LiDAR point clouds [[71](#bib.bib71)]. Though point clouds retain the original
    information and may provide larger receptive field [[68](#bib.bib68)], the volume
    of point clouds can be huge, requiring high computation power to process [[58](#bib.bib58),
    [28](#bib.bib28)]. Moreover, it is hard to integrate point clouds with other data
    formats such as images. Due to these two points, representations with additional
    pre-processing methods are developed and progressed rapidly.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从LiDAR传感器获得的3D点云可以通过点处理深度学习网络进行处理，而无需格式转换，例如 PointNet [[67](#bib.bib67)]、PointNet++
    [[68](#bib.bib68)]、PointCNN [[69](#bib.bib69)] 和 KPConv [[70](#bib.bib70)]。LiDAR点云可以与类似的点格式数据（如其他LiDAR点云
    [[71](#bib.bib71)]）进行集成。尽管点云保留了原始信息，并可能提供更大的感受野 [[68](#bib.bib68)]，但点云的体积可能非常庞大，需要高计算能力来处理
    [[58](#bib.bib58), [28](#bib.bib28)]。此外，将点云与其他数据格式（如图像）集成也很困难。由于这两点，带有额外预处理方法的表示形式得到了快速发展。
- en: II-B2 Voxel representation
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 体素表示
- en: Voxels are generated by dividing the whole 3D space into small regular 3D grids
    and partitioning the original points into corresponding grid based on the geometry.
    This gridization transforms irregular points into regular voxel representation,
    and make it possible to down-sample the original LiDAR points to reduce input
    volume. In fact, the volume and the resolution of the voxels can be adjusted by
    changing the grid size. Larger grids result in more information loss while smaller
    grids may still bring burdens to computation. Several 3D convolution methods can
    be used to process voxels and extract features, such as 3D ShapeNet [[72](#bib.bib72)],
    VoxelNet [[73](#bib.bib73)], and VoxNet [[74](#bib.bib74)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 体素是通过将整个3D空间划分为小的规则3D网格，并根据几何形状将原始点划分到相应的网格中来生成的。这种网格化将不规则点转化为规则的体素表示，并使得可以对原始LiDAR点进行降采样以减少输入体积。实际上，通过改变网格大小，可以调整体素的体积和分辨率。较大的网格会导致更多的信息丢失，而较小的网格可能仍会带来计算负担。可以使用几种3D卷积方法来处理体素并提取特征，如3D
    ShapeNet [[72](#bib.bib72)]、VoxelNet [[73](#bib.bib73)] 和 VoxNet [[74](#bib.bib74)]。
- en: II-B3 Pixel/View representation
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 像素/视图表示
- en: Pixel or view representation of LiDAR points is to convert 3D point clouds into
    2D image views by projection. Bird’s-Eye-View (BEV)and range views (also known
    as perspective views) are two common types of views on different 2D view planes
    that can be transformed from point clouds to. Pixel representation can leverage
    the existing well-developed CNN-family image processing methods, though the 3D
    geometry information retained in original point clouds may be lost in the projection
    process. The pixel representation, or projection technique, is commonly adopted
    when integrating LiDAR point clouds with camera images [[75](#bib.bib75), [76](#bib.bib76),
    [77](#bib.bib77)].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: LiDAR点的像素或视图表示是通过投影将3D点云转换为2D图像视图。鸟瞰图（BEV）和距离视图（也称为透视视图）是从点云转换到的两种常见的2D视图平面。像素表示可以利用现有的成熟CNN系列图像处理方法，尽管原始点云中保留的3D几何信息可能在投影过程中丢失。像素表示或投影技术在将LiDAR点云与相机图像集成时常被采用
    [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77)]。
- en: II-B4 Integrated representation
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B4 综合表示
- en: In addition to the previously mentioned representations, some researchers have
    attempted to combine various representations to obtain richer information or before
    further processing. They have developed point-voxel integration methods that merge
    the advantages of both representations. Point-Voxel CNN (PVCNN) [[78](#bib.bib78)]
    employs a dual-branch system consisting of a voxel-based branch and a point-based
    branch. It carries out convolutions on voxels while supplementing detailed geometry
    information from points. PV-RCNN [[79](#bib.bib79)] generates 3D proposals from
    voxel convolutions and selects a few key points in the space to serve as connections
    between voxel features and the refinement network for these proposals. Furthermore,
    different views obtained from point clouds can be integrated. Zhou et al. [[59](#bib.bib59)]
    propose a Multi-View Fusion (MVF) Network that converts Bird’s-Eye-View (BEV)and
    perspective views into voxels, fusing them together so that the complementary
    information can be used effectively. There are also studies integrating all aforementioned
    representations. In M3DETR [[80](#bib.bib80)], the point, voxel, and pixel representations
    are processed with PointNets, VoxelNet, and 2D ConvNets modules respectively,
    and fused with transformers at multiple scales. This design better extracts information
    in raw data by effectively exploiting the correlation between different representations
    leveraging the attention structure.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前提到的表示方式外，一些研究人员尝试将各种表示方式结合起来，以获取更丰富的信息或进行进一步处理。他们开发了点-体素融合方法，融合了两种表示的优点。Point-Voxel
    CNN (PVCNN) [[78](#bib.bib78)] 使用一个由体素分支和点分支组成的双分支系统。它对体素进行卷积，同时从点中补充详细的几何信息。PV-RCNN
    [[79](#bib.bib79)] 从体素卷积中生成3D提案，并在空间中选择一些关键点作为体素特征与这些提案的细化网络之间的连接。此外，还可以整合从点云中获得的不同视图。Zhou等人[[59](#bib.bib59)]
    提出了一个多视角融合 (MVF) 网络，将鸟瞰图 (BEV) 和透视图转换为体素，并将它们融合在一起，以便有效地利用互补信息。还有研究将所有上述表示方式进行整合。在M3DETR
    [[80](#bib.bib80)]中，点、体素和像素表示分别通过PointNets、VoxelNet和2D ConvNets模块处理，并通过transformers在多个尺度上融合。这种设计通过有效利用不同表示之间的相关性，利用注意力结构，更好地提取原始数据中的信息。
- en: II-C Millimeter wave radar (MMW-radar)
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 毫米波雷达 (MMW-radar)
- en: Radio Detection and Ranging, a.k.a. radar or MMW-Radar, is a technique that
    relies on radiating electromagnetic millimeter waves and scattered reflections
    to estimate the range information about targets [[12](#bib.bib12)]. Short-range,
    medium-range, and long-range radar detectors have different detection distances,
    and are commonly used for collision avoidance and obstacle detection.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 无线电探测与测距，即雷达或毫米波雷达，是一种依赖于发射电磁毫米波和散射反射来估计目标范围信息的技术[[12](#bib.bib12)]。短程、中程和长程雷达探测器具有不同的检测距离，通常用于防碰撞和障碍物检测。
- en: Different from LiDAR sensors and cameras that are easily affected by external
    conditions, radar sensors are more robust in extreme weather or dim light [[81](#bib.bib81),
    [82](#bib.bib82), [55](#bib.bib55)]. Another significant advantage of this type
    of low-price sensor is its capability of accurately detecting the velocity of
    dynamic targets based on the Doppler effect, which is very important for perception
    tasks in autonomous driving scenarios. However, radars also have some disadvantages.
    Comparing with cameras, radars lack texture or semantic information. Comparing
    with LiDAR sensors, radars have lower angle resolution (see Table 4 in [[12](#bib.bib12)]
    for detailed configurations). Therefore, radars are not suitable for tasks such
    as object recognition, and may have troubles when distinguish static and stationary
    objects [[11](#bib.bib11), [12](#bib.bib12)]. Besides, the clutters, i.e., the
    unwanted echoes in electronic systems, may cause false detections and performance
    issues in radar systems [[55](#bib.bib55)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与容易受外部条件影响的LiDAR传感器和相机不同，雷达传感器在极端天气或昏暗光线下更具鲁棒性[[81](#bib.bib81), [82](#bib.bib82),
    [55](#bib.bib55)]。这种低价传感器的另一个显著优点是其基于多普勒效应准确检测动态目标的速度，这对于自动驾驶场景中的感知任务非常重要。然而，雷达也有一些缺点。与相机相比，雷达缺乏纹理或语义信息。与LiDAR传感器相比，雷达的角分辨率较低（详见[[12](#bib.bib12)]中的表4）。因此，雷达不适合进行物体识别等任务，并且在区分静态和静止物体时可能会遇到困难[[11](#bib.bib11),
    [12](#bib.bib12)]。此外，电子系统中的杂波可能导致误检测和雷达系统的性能问题[[55](#bib.bib55)]。
- en: According to Wang et al. [[58](#bib.bib58)] and Zhou et al. [[55](#bib.bib55)],
    the data format of radar can be divided into raw data, cluster-layer data, and
    object-layer data according to different pre-processing stages. The raw output
    of radar is in the form of time frequency spectrograms. To improve its utility,
    signal processing methods such as those detailed presented in Zhou et al. [[83](#bib.bib83)]
    are often necessary. More commonly adopted radar data formats in autonomous driving
    applications are cluster-layer obtained after operating clustering algorithms,
    and object-layer after filtering and tracking. Comparing with the original raw
    data, the latter two formats provide more sparse and less noisy information.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 根据王等人[[58](#bib.bib58)]和周等人[[55](#bib.bib55)]的研究，雷达的数据格式可以根据不同的预处理阶段分为原始数据、聚类层数据和对象层数据。雷达的原始输出形式为时间频率谱图。为了提高其实用性，通常需要信号处理方法，例如周等人[[83](#bib.bib83)]中详细介绍的方法。自主驾驶应用中更常采用的雷达数据格式是经过聚类算法处理后的聚类层数据，以及经过过滤和跟踪后的对象层数据。与原始数据相比，后两种格式提供了更稀疏、噪声更少的信息。
- en: Two different representations of radar signals can be found in ADS related research.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在与自动驾驶相关的研究中可以找到雷达信号的两种不同表示。
- en: II-C1 Point representation
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 点表示
- en: Point-based representation is to represent and process radar data as point clouds
    [[84](#bib.bib84)]. However, as raised by Wang et al. [[58](#bib.bib58)], the
    properties of radar point clouds differ from LiDAR point clouds, thus issues may
    arise when directly using LiDAR models on radar points.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 点云表示是将雷达数据表示和处理为点云[[84](#bib.bib84)]。然而，正如王等人[[58](#bib.bib58)]所指出的那样，雷达点云的特性与
    LiDAR 点云不同，因此直接使用 LiDAR 模型对雷达点进行处理时可能会出现问题。
- en: II-C2 Map representation
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 地图表示
- en: Another representation is map-based, which is to accumulate radar data over
    several time-stamps and generate radar grid BEV maps [[14](#bib.bib14)]. Since
    grid maps alleviate the radar data sparsity issue, image processing networks such
    as CNN are used to extract features and get static environment classification.
    More details of different grid maps can be found in [[55](#bib.bib55)].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表示方式是基于地图的，即在多个时间戳上累计雷达数据并生成雷达网格 BEV 地图[[14](#bib.bib14)]。由于网格地图缓解了雷达数据稀疏性问题，因此使用图像处理网络，如
    CNN，来提取特征并进行静态环境分类。有关不同网格地图的更多细节可参见[[55](#bib.bib55)]。
- en: 'III Data Integration: What to Integrate'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 数据集成：集成内容
- en: In this section, we review and summarize the techniques with respect to the
    content to integrate by following the bi-level taxonomy presented in Figure [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A survey on deep learning approaches for data integration
    in autonomous driving system"). Specifically, we first summarize each one-dimensional
    integration as shown in Zone 1, 2, and 3 in Figure [1a](#S1.F1.sf1 "In Figure
    1 ‣ I Introduction ‣ A survey on deep learning approaches for data integration
    in autonomous driving system"). We then provide discussions for two-dimensional
    categories (Zone 4, 5, 6 in Figure [1a](#S1.F1.sf1 "In Figure 1 ‣ I Introduction
    ‣ A survey on deep learning approaches for data integration in autonomous driving
    system")), with an additional focus on the order to integrate (Figure [1b](#S1.F1.sf2
    "In Figure 1 ‣ I Introduction ‣ A survey on deep learning approaches for data
    integration in autonomous driving system")). As research related to three-dimensional
    integration (Zone 7 in Figure [1a](#S1.F1.sf1 "In Figure 1 ‣ I Introduction ‣
    A survey on deep learning approaches for data integration in autonomous driving
    system")) is rare, we omit three-dimensional integration in this section.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾并总结了按照图[1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A survey on deep learning
    approaches for data integration in autonomous driving system")中所示的双层分类法进行集成的技术。具体而言，我们首先总结了图[1a](#S1.F1.sf1
    "In Figure 1 ‣ I Introduction ‣ A survey on deep learning approaches for data
    integration in autonomous driving system")中显示的每个一维集成（区域 1、2 和 3）。然后，我们讨论了二维类别（图[1a](#S1.F1.sf1
    "In Figure 1 ‣ I Introduction ‣ A survey on deep learning approaches for data
    integration in autonomous driving system")中的区域 4、5、6），并特别关注集成顺序（图[1b](#S1.F1.sf2
    "In Figure 1 ‣ I Introduction ‣ A survey on deep learning approaches for data
    integration in autonomous driving system")）。由于与三维集成（图[1a](#S1.F1.sf1 "In Figure
    1 ‣ I Introduction ‣ A survey on deep learning approaches for data integration
    in autonomous driving system")中的区域 7）相关的研究较少，我们在本节中省略了三维集成。
- en: III-A Multi-view integration
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 多视角集成
- en: Under our definition, the most common type of multi-view data integration is
    to integrate multi-view camera images captured by multiple cameras from different
    directions (e.g., 6 monocular cameras installed on self-driving vehicles). This
    camera multi-view integration can be used to generate BEV representation of the
    surrounding environment [[85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87)]
    or assist object detection in 2D images [[88](#bib.bib88)]. A stereo camera can
    also generate multi-view images as it has two or more lens [[89](#bib.bib89)].
    Besides visual sensors, researchers also explore alternative methods to fuse multiple
    LiDARs’ measurements mounted on a vehicle for object detection [[71](#bib.bib71),
    [90](#bib.bib90)]. Due to high expense of LiDARs, however, this approach is not
    widely adopted in industry and relevant works are limited. Thus we skip the discussion
    of Lidar multi-view integration. Similarly, multi-radar integration is also considered
    as helpful in 360-degree environmental perception [[91](#bib.bib91), [92](#bib.bib92)],
    while they are more commonly integrated with other sensor modalities in applications.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的定义，最常见的多视角数据集成类型是整合由多个相机从不同方向拍摄的多视角图像（例如，安装在自动驾驶车辆上的6个单目相机）。这种相机多视角集成可以用于生成周围环境的BEV（鸟瞰视图）表示[[85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87)]，或在2D图像中辅助物体检测[[88](#bib.bib88)]。立体相机也可以生成多视角图像，因为它具有两个或更多镜头[[89](#bib.bib89)]。除了视觉传感器，研究人员还探索了将多个LiDAR测量结果融合的方法，这些LiDAR安装在车辆上用于物体检测[[71](#bib.bib71),
    [90](#bib.bib90)]。然而，由于LiDAR的高昂成本，这种方法在行业中并未广泛采用，相关工作也有限。因此，我们跳过了对LiDAR多视角集成的讨论。类似地，多雷达集成在360度环境感知中也被认为是有用的[[91](#bib.bib91),
    [92](#bib.bib92)]，但它们通常与其他传感器模态一起应用。
- en: III-A1 Camera multi-view
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 相机多视角
- en: Most monocular RGB cameras can only capture objects within a frustum whose shape
    is determined by camera lens. Besides, many objects inside the frustum cannot
    be observed in images if occluded by others. Thus, we need to have multiple images
    shot from different positions to describe the 3D world more thoroughly. The depth
    information (distance between a point in 3D space and the camera’s image plane)
    can be recovered from multi-view cameras given their relative camera parameters
    (intrinsic and extrinsic transformation matrix) based on the consistency and uniqueness
    of spatial geometry. Intrinsic parameters describe the mapping from 2D camera
    coordinate to 2D image coordinate, and extrinsic ones describe the mapping from
    3D world coordinate to 2D camera coordinate. If images are taken from two paired
    cameras, it is called binocular depth estimation or stereo matching, or disparity
    estimation. If more than two views are provided, it is called multi-view stereo
    (MVS).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数单目RGB相机只能捕捉到相机镜头确定的视锥体内的物体。此外，如果被其他物体遮挡，视锥体内的许多物体在图像中无法被观察到。因此，我们需要从不同位置拍摄多张图像来更全面地描述3D世界。基于空间几何的一致性和唯一性，可以从多视角相机中恢复深度信息（3D空间中一个点与相机图像平面之间的距离），前提是相机的相对参数（内参和外参变换矩阵）已知。内参描述了从2D相机坐标到2D图像坐标的映射，而外参描述了从3D世界坐标到2D相机坐标的映射。如果图像是从两个配对的相机拍摄的，这称为双目深度估计或立体匹配，或视差估计。如果提供了多个视角，则称为多视角立体（MVS）。
- en: 'The inputs of most camera multi-view methods [[93](#bib.bib93), [89](#bib.bib89),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103)] are RGB images, but there are also papers [[104](#bib.bib104),
    [105](#bib.bib105), [106](#bib.bib106)] fusing RGB and thermal images, and papers
    [[85](#bib.bib85)] fusing RGB and depths. We categorize camera multi-view inputs
    as following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数相机多视角方法的输入是RGB图像，但也有一些论文将RGB和热成像图像融合，[104](#bib.bib104), [105](#bib.bib105),
    [106](#bib.bib106)以及将RGB与深度图像融合的论文[[85](#bib.bib85)]。我们将相机多视角输入分为以下几类：
- en: 'a) Images: These images are captured by different cameras or by a monocular
    camera but from different positions and angles. The number of images is not limited,
    but at least two images are required. The images should be overlapped but not
    totally the same.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: a) 图像：这些图像由不同的相机拍摄，或者由单目相机从不同的位置和角度拍摄。图像的数量没有限制，但至少需要两张图像。图像应有重叠，但不能完全相同。
- en: 'b) Depth images: A 2D image with one channel, where each pixel denotes the
    camera’s distance to the corresponding 3D point. They can be obtained from either
    depth cameras or depth estimation algorithms.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: b) 深度图像：一个具有单通道的2D图像，其中每个像素表示相机到相应3D点的距离。它们可以通过深度相机或深度估计算法获得。
- en: 'c) Thermal images: A 2D image with one channel, where each pixel denotes the
    intensity of infrared radiation emitted by an object. They can be captured by
    thermal cameras.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: c) 热成像：一个具有单通道的2D图像，其中每个像素表示物体发射的红外辐射强度。它们可以通过热成像相机捕捉到。
- en: III-A2 Radar multi-view
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 雷达多视图
- en: 'Very few papers investigate integration among multiple radars. [[107](#bib.bib107)]
    uses three kinds of views: range-Doppler (RD), angle-Doppler (AD), and range-angle
    (RA). The RD view reveals the distance and velocity of objects. Specifically,
    non-moving objects respond at zero Doppler when the radar is stationary, and objects
    moving relative to the radar respond at nonzero Doppler. The AD view shows the
    direction and speed of objects. RA view represents the relationship between range
    and angle. All the views can be represented as 2D images.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有论文研究多雷达之间的融合。[[107](#bib.bib107)]使用三种视图：距离-多普勒（RD）、角度-多普勒（AD）和距离-角度（RA）。RD视图揭示了物体的距离和速度。具体来说，当雷达静止时，非移动物体在零多普勒下响应，相对雷达移动的物体在非零多普勒下响应。AD视图显示了物体的方向和速度。RA视图表示距离与角度之间的关系。所有视图都可以表示为2D图像。
- en: III-B Multi-modality integration
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 多模态融合
- en: Since sensors have their own advantages and shortcomings as shown in Section [II](#S2
    "II Sensing modalities and pre-processing ‣ A survey on deep learning approaches
    for data integration in autonomous driving system"), multi-modality data integration
    is expected to utilize the mutual supplementary between different sensors and
    achieve improved accuracy [[20](#bib.bib20)]. Various methods have been developed
    and applied to tasks including object detection and tracking, depth completion,
    and segmentation. A few studies employ camera-radar integration [[108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112)]
    or LiDAR-radar integration [[113](#bib.bib113)] for object detection. Several
    research further integrate all these three sensors together for environmental
    perception [[114](#bib.bib114), [115](#bib.bib115)].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于传感器各有其优缺点，如第[II](#S2 "II Sensing modalities and pre-processing ‣ A survey
    on deep learning approaches for data integration in autonomous driving system")节所示，多模态数据融合预计能够利用不同传感器之间的互补性，并实现更高的精度[[20](#bib.bib20)]。各种方法已经被开发并应用于对象检测与跟踪、深度完成和分割等任务。一些研究采用摄像头-雷达融合[[108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112)]或LiDAR-雷达融合[[113](#bib.bib113)]进行对象检测。还有一些研究进一步将这三种传感器集成在一起用于环境感知[[114](#bib.bib114),
    [115](#bib.bib115)]。
- en: 'In the following, we summarize the input of existing multi-modality integration
    research for four multi-modality combinations: 1) camera and LiDAR, 2) camera
    and radar, 3) LiDAR and radar, and 4) camera, LiDAR, and radar. As for the remained
    combination of modalities, there is few research on them.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在下文中，我们总结了现有多模态融合研究的输入，涵盖了四种多模态组合：1）摄像头和LiDAR，2）摄像头和雷达，3）LiDAR和雷达，以及4）摄像头、LiDAR和雷达。至于其他模态组合，相关研究较少。
- en: III-B1 Camera and LiDAR integration
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 相机与LiDAR融合
- en: Since 2D camera images and 3D LiDAR points are in different coordinate frame,
    they usually need to be transformed to the same space before integration. In the
    following, we present discussions of 2D space integration and 3D space integration,
    respectively.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于2D相机图像和3D LiDAR点处于不同的坐标系中，因此通常需要将它们转换到相同的空间后再进行融合。接下来，我们分别讨论2D空间融合和3D空间融合。
- en: a) 2D space integration
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: a) 2D空间融合
- en: Two sub-approaches can be identified according to different LiDAR projected
    planes. One approach is to integrate LiDAR and camera views in the same plane
    [[116](#bib.bib116)]. In other words, the LiDAR points are projected onto the
    image plane for integration. For example, Caltagirone et al. [[117](#bib.bib117)]
    transforms LiDAR points to dense LiDAR images by projecting LiDAR points to image
    plane and up-sampling. The LiDAR images then can be integrated with RGB images
    for road segmentation. Berrio et al. [[118](#bib.bib118)] projects LiDAR points
    to image with mask techniques and probabilistic distribution to handle occlusions
    in semantic mapping. Object detection model, EPNet [[119](#bib.bib119)], projects
    LiDAR points to image plane for multiple times in order to enhance the LiDAR point
    features with corresponding image semantic information. Cheng et al. [[120](#bib.bib120)]
    transforms LiDAR points to right and left lens of stereo, respectively, and integrate
    across both modality and lens to get dense depth. Models of [[121](#bib.bib121),
    [122](#bib.bib122), [123](#bib.bib123)] integrate RGB images with sparse depth
    map generated from LiDAR points for image depth completion. [[124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130)] also take RGB and LiDAR-generated depth
    image as inputs and return classification, bounding boxes, or mask proposals.
    There is also a study [[131](#bib.bib131)] expresses and fuses LiDAR features
    in the same coordinate system as the camera. [[132](#bib.bib132)] converts LiDAR
    points to range images of size $(5,w,h)$ before integration with RGB images.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 可以根据不同的LiDAR投影平面识别出两种子方法。一种方法是将LiDAR和相机视图整合到同一个平面[[116](#bib.bib116)]。换句话说，LiDAR点被投影到图像平面进行整合。例如，Caltagirone等人[[117](#bib.bib117)]通过将LiDAR点投影到图像平面并进行上采样，将LiDAR点转化为密集的LiDAR图像。这些LiDAR图像随后可以与RGB图像整合进行道路分割。Berrio等人[[118](#bib.bib118)]使用掩模技术和概率分布将LiDAR点投影到图像中，以处理语义映射中的遮挡问题。目标检测模型EPNet[[119](#bib.bib119)]多次将LiDAR点投影到图像平面，以增强LiDAR点特征与相应的图像语义信息。Cheng等人[[120](#bib.bib120)]将LiDAR点分别转换为立体视觉的左右镜头，并在两种模式和镜头之间整合以获得密集深度。模型[[121](#bib.bib121),
    [122](#bib.bib122), [123](#bib.bib123)]将RGB图像与从LiDAR点生成的稀疏深度图整合，用于图像深度补全。[[124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130)]也将RGB和LiDAR生成的深度图像作为输入，返回分类、边界框或掩模提案。还有一项研究[[131](#bib.bib131)]在与相机相同的坐标系中表达并融合LiDAR特征。[[132](#bib.bib132)]在与RGB图像整合之前，将LiDAR点转换为尺寸为$(5,w,h)$的深度图像。
- en: Another approach is to integrate views in different planes. For example, RGB
    image and BEV map generated by LiDAR projection are integrated in [[76](#bib.bib76),
    [133](#bib.bib133), [134](#bib.bib134)] with deep learning architectures. These
    works connect image feature map to BEV with different transformation or association
    methods. In [[135](#bib.bib135)], four LiDAR projection maps, including height
    bird view (HBV), intensity bird view (IBV), distance center view (DCV), and intensity
    center view (ICV), are integrated with RGB images and original LiDAR 3D points.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将不同平面中的视图整合。例如，在[[76](#bib.bib76), [133](#bib.bib133), [134](#bib.bib134)]中，RGB图像与LiDAR投影生成的BEV地图在深度学习架构中整合。这些工作通过不同的转换或关联方法将图像特征图连接到BEV。在[[135](#bib.bib135)]中，四个LiDAR投影地图，包括高度鸟瞰图（HBV）、强度鸟瞰图（IBV）、距离中心视图（DCV）和强度中心视图（ICV），与RGB图像和原始LiDAR
    3D点整合。
- en: b) 3D space integration
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: b) 3D空间整合
- en: One way to achieve 3D space integration is by projecting image data to 3D space
    and thus generate “pseudo-LiDAR points”. As RGB images lack depth estimates, a
    significant portion of these works leverage stereo or RGB-D data to obtain accurate
    geometry information. [[136](#bib.bib136), [137](#bib.bib137)] integrate LiDAR
    point cloud and pseudo-LiDAR point cloud generated from image depth completion
    for 3D object detection. Another way to integrate LiDAR and camera data in 3D
    space is via data association. 3D object detection models such as [[62](#bib.bib62),
    [61](#bib.bib61), [138](#bib.bib138)] project 2D bounding boxes obtained in RGB
    image to 3D space to get frustums as regions of interest to guide 3D point or
    feature searching. VPFNet [[139](#bib.bib139)] leverages virtual points from 3D
    proposals as bridge to associate image features and aggregate LiDAR point features.
    In LoGoNet [[140](#bib.bib140)], image features are associated with LiDAR generated
    3D voxel features by projecting voxel point centroid using camera projection matrix
    and generate reference points in the image plane.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 3D 空间集成的一种方法是将图像数据投影到 3D 空间，从而生成“伪 LiDAR 点”。由于 RGB 图像缺乏深度估计，这些工作中的大部分利用立体或
    RGB-D 数据来获取准确的几何信息。[[136](#bib.bib136), [137](#bib.bib137)] 将 LiDAR 点云与通过图像深度完成生成的伪
    LiDAR 点云集成用于 3D 目标检测。另一种在 3D 空间中集成 LiDAR 和相机数据的方法是通过数据关联。3D 目标检测模型如 [[62](#bib.bib62),
    [61](#bib.bib61), [138](#bib.bib138)] 将 RGB 图像中获得的 2D 边界框投影到 3D 空间，以获取视锥体作为兴趣区域，指导
    3D 点或特征搜索。VPFNet [[139](#bib.bib139)] 利用来自 3D 提案的虚拟点作为桥梁，将图像特征与 LiDAR 点特征关联并聚合。在
    LoGoNet [[140](#bib.bib140)] 中，通过使用相机投影矩阵投影体素点质心，将图像特征与 LiDAR 生成的 3D 体素特征进行关联，并在图像平面上生成参考点。
- en: III-B2 Camera and radar integration
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 相机和雷达集成
- en: Similar to LiDAR-RGB integration, camera and radar can also be integrated in
    either 2D space or 3D space. Examples of 2D space integration include [[110](#bib.bib110),
    [141](#bib.bib141)], in which radar points are projected onto the 2D plane of
    camera’s perspective view. In [[142](#bib.bib142)], two-channel (radar cross section
    and range channels) radar image is generated with the same size as the visual
    image and combined with image feature maps with an extended VGG network. In 3D
    space integration, image information need to be converted to 3D space. RGB image
    in [[143](#bib.bib143)] is first processed by CNN for feature extraction, and
    then converted to 3D points via back-projection. Radar and RGB can also be combined
    with data association. For example, CenterFusion [[112](#bib.bib112)] projects
    2D object bounding boxes to 3D space to connect image information with 3D pillars
    generated from radar points.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 LiDAR-RGB 集成，相机和雷达也可以在 2D 空间或 3D 空间中进行集成。2D 空间集成的例子包括 [[110](#bib.bib110),
    [141](#bib.bib141)]，其中雷达点被投影到相机的 2D 视图平面上。在 [[142](#bib.bib142)] 中，生成了与视觉图像大小相同的双通道（雷达截面和距离通道）雷达图像，并与扩展的
    VGG 网络中的图像特征图结合。在 3D 空间集成中，图像信息需要转换为 3D 空间。[[143](#bib.bib143)] 中的 RGB 图像首先通过
    CNN 进行特征提取，然后通过反向投影转换为 3D 点。雷达和 RGB 也可以通过数据关联进行结合。例如，CenterFusion [[112](#bib.bib112)]
    将 2D 目标边界框投影到 3D 空间，以将图像信息与从雷达点生成的 3D 柱体连接起来。
- en: III-B3 LiDAR and radar integration
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 LiDAR 和雷达集成
- en: Since both LiDAR and radar data capture 3D geometry information by nature, they
    are usually integrated in 3D space. Some researches integrate raw data of the
    two sensors in a point-by-point way. For example, [[144](#bib.bib144)] combines
    LiDAR and radar points based on heuristic rules to remove low-quality data affected
    by external environmental factors. LiDAR and radar points can also be combined
    via data association. [[145](#bib.bib145)] relies on radar detection to obtain
    frustums as regions of interest to filter relevant LiDAR points.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LiDAR 和雷达数据天生都能捕捉 3D 几何信息，因此它们通常在 3D 空间中集成。有些研究以逐点的方式集成这两个传感器的原始数据。例如，[[144](#bib.bib144)]
    基于启发式规则结合 LiDAR 和雷达点，以去除受外部环境因素影响的低质量数据。LiDAR 和雷达点也可以通过数据关联进行结合。[[145](#bib.bib145)]
    依赖雷达检测获取视锥体作为兴趣区域，以筛选相关的 LiDAR 点。
- en: III-B4 Camera, LiDAR, and radar integration
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B4 相机、LiDAR 和雷达集成
- en: Few studies are trying to integrate data from three sensors together for perception.
    The key process of the integration is still to transform the data from different
    sensors into the same space. Camera, LiDAR, and radar can be integrated in 2D
    space if LiDAR and radar points are projected to camera’s plane. The input of
    the integration neural network in [[146](#bib.bib146)] is camera image layers
    concatenated with projected LiDAR and radar channels. The sensors can also be
    integrated in higher dimensional space. [[147](#bib.bib147)] integrate RGB images,
    LiDAR points, and radar points by aggregating data information from each sensor
    together to generate high dimensional points before further processing.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 少有研究尝试将来自三种传感器的数据进行集成。集成的关键过程仍然是将来自不同传感器的数据转换到相同的空间中。如果将LiDAR和雷达点投影到相机的平面上，相机、LiDAR和雷达可以在二维空间中进行融合。在[[146](#bib.bib146)]中，集成神经网络的输入是与投影的LiDAR和雷达通道拼接的相机图像层。传感器也可以在更高维度的空间中进行集成。[[147](#bib.bib147)]通过将每个传感器的数据聚合在一起，生成高维点，然后再进行进一步处理，来集成RGB图像、LiDAR点和雷达点。
- en: III-C Multi-frame integration
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 多帧融合
- en: Multi-frame data, i.e., temporal data, refer to the data sampled at multiple
    timestamps along timeline. Compared to single-frame data, integration of multi-frame
    data brings more information, and can potentially improve the accuracy of environmental
    perception. In this subsection, we focus on the inputs of single-sensor multi-frame
    integration. As few studies can be found on radar temporal integration, we only
    present details on camera image sequence and LiDAR point cloud sequence integration.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 多帧数据，即时间数据，指的是沿时间轴在多个时间戳处采样的数据。与单帧数据相比，多帧数据的融合带来了更多信息，并可能提高环境感知的准确性。在本小节中，我们关注单传感器多帧融合的输入。由于关于雷达时间积分的研究较少，我们仅介绍相机图像序列和LiDAR点云序列的融合细节。
- en: III-C1 Camera image sequence
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 相机图像序列
- en: Camera multi-frame integration refers to fusing information of image sequences
    instead of a single image. Two types of data are commonly used to carry out the
    integration. One is the sequence of feature maps generated from each image, and
    the other is the sequence of processed information obtained from each pair of
    consecutive images.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 相机多帧融合指的是融合图像序列的信息，而不是单张图像。常用的两种数据进行融合。一种是从每张图像生成的特征图序列，另一种是从每对连续图像获得的处理信息序列。
- en: a) Sequence of feature maps generated from each image
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: a) 从每张图像生成的特征图序列
- en: A traditional two-stage temporal integration approach is to perform feature
    extraction operations corresponding to different tasks in each image frame and
    then create association or fusion cross frames. For example, in 2D MOT models
    such as [[148](#bib.bib148), [149](#bib.bib149), [150](#bib.bib150), [151](#bib.bib151),
    [152](#bib.bib152), [98](#bib.bib98), [99](#bib.bib99)], 2D object detection bounding
    boxes and feature vectors/maps are first generated in each image, and then associated
    in different frames. Zhou et al. [[153](#bib.bib153)] conduct tracking in a similar
    way, yet they introduce the center of feature map for each image as an additional
    input. The same two-stage pattern can also be observed in 3D monocular MOT, while
    geometry or depth information is additionally incorporated [[154](#bib.bib154),
    [155](#bib.bib155)]. Multi-frame integration of visual odometry (VO) algorithms
    also decouple feature extraction and temporal integration. For example, [[156](#bib.bib156),
    [157](#bib.bib157)] use CNN to extract features and RNN to integrate information
    from different frames. Monocular depth estimation also takes RGB image sequence
    as inputs to extract information and integrate over time [[158](#bib.bib158),
    [159](#bib.bib159), [160](#bib.bib160)].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的两阶段时间积分方法是在每个图像帧中执行对应于不同任务的特征提取操作，然后在不同帧之间进行关联或融合。例如，在二维多目标跟踪模型中，如[[148](#bib.bib148)、[149](#bib.bib149)、[150](#bib.bib150)、[151](#bib.bib151)、[152](#bib.bib152)、[98](#bib.bib98)、[99](#bib.bib99)]，首先在每个图像中生成二维目标检测边界框和特征向量/图谱，然后在不同帧中进行关联。Zhou
    等人[[153](#bib.bib153)]以类似的方式进行跟踪，但他们引入了每张图像的特征图中心作为额外输入。在三维单目多目标跟踪中也可以观察到相同的两阶段模式，同时额外融入了几何或深度信息[[154](#bib.bib154)、[155](#bib.bib155)]。视觉里程计（VO）算法的多帧融合也将特征提取和时间积分解耦。例如，[[156](#bib.bib156)、[157](#bib.bib157)]使用CNN提取特征，并使用RNN整合来自不同帧的信息。单目深度估计也以RGB图像序列作为输入来提取信息并进行时间积分[[158](#bib.bib158)、[159](#bib.bib159)、[160](#bib.bib160)]。
- en: One disadvantage of this two-stage approach is that the results of the later
    stage may be affected by the earlier stage. To avoid this issue, some studies
    leveraging Transformer to integrate both spatial and temporal information for
    multi-frame image MOT have emerged in recent years [[161](#bib.bib161), [162](#bib.bib162),
    [163](#bib.bib163)].
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这种两阶段方法的一个缺点是后期阶段的结果可能会受到前期阶段的影响。为避免这个问题，近年来出现了一些利用Transformer整合空间和时间信息以进行多帧图像MOT的研究[[161](#bib.bib161),
    [162](#bib.bib162), [163](#bib.bib163)]。
- en: b) Image pairs or sequence of processed information obtained from image pairs
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: b) 从图像对获得的图像对或处理信息序列
- en: Raw image pairs, specifically a preceding image and a current image, can directly
    be input into integration algorithms. For example, monocular depth estimation
    methods [[164](#bib.bib164), [165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167),
    [168](#bib.bib168)] utilize reference and target frame pairs for self-supervision.
    In these methods, features of the preceding image are re-projected to the view
    of target image with predicted depth and pose, thus a re-projection loss can be
    generated. In [[103](#bib.bib103)], image pairs from adjacent frames or different
    cameras can both be treated are images taken from different viewpoints with relative
    positions, and thus be used for weak supervision. Successive image pairs can also
    be exploited in knowledge distillation. By utilizing a three-level knowledge distillation
    method, Chen et al. [[169](#bib.bib169)] train the student model to incorporate
    adjacent frames which allows it to acquire a greater understanding of comprehensive
    representation knowledge from its corresponding teacher model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像对，特别是前一图像和当前图像，可以直接输入到融合算法中。例如，单目深度估计方法[[164](#bib.bib164), [165](#bib.bib165),
    [166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)]利用参考帧和目标帧对进行自监督。在这些方法中，前一图像的特征被重新投影到目标图像的视图中，并预测深度和姿态，从而生成重投影损失。在[[103](#bib.bib103)]中，来自相邻帧或不同相机的图像对都可以被视为从不同视点拍摄的图像，并因此用于弱监督。连续的图像对也可以用于知识蒸馏。通过利用三层知识蒸馏方法，Chen等人[[169](#bib.bib169)]训练学生模型以结合相邻帧，使其能够从相应的教师模型中获得更全面的知识表示。
- en: In addition, the outputs of multiple image pairs can further be fused temporally.
    In other words, this approach consists of two stages. First, raw image pairs are
    combined respectively to get corresponding fused outputs. Then, the fused output
    sequence are integrated over time. One typical example of this approach is to
    generate optical flow from each pair of raw images and then integrate sequence
    of optical flows to, for instance, get or assist pose estimation [[170](#bib.bib170),
    [171](#bib.bib171), [172](#bib.bib172), [157](#bib.bib157), [152](#bib.bib152)].
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，多个图像对的输出可以进一步进行时间上的融合。换句话说，这种方法分为两个阶段。首先，分别组合原始图像对以获取对应的融合输出。然后，对融合输出序列进行时间上的整合。此方法的一个典型示例是从每对原始图像生成光流，然后将光流序列整合以例如进行姿态估计[[170](#bib.bib170),
    [171](#bib.bib171), [172](#bib.bib172), [157](#bib.bib157), [152](#bib.bib152)]。
- en: III-C2 LiDAR scan sequence
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 LiDAR扫描序列
- en: 'LiDAR multi-frame integration refers to the fusion of LiDAR scan sequence.
    As LiDAR scans have different representations, two types of LiDAR scan sequence
    input can be found in existing works: point cloud sequence, and view sequence.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: LiDAR多帧融合指的是LiDAR扫描序列的融合。由于LiDAR扫描有不同的表示方式，现有工作中可以找到两种类型的LiDAR扫描序列输入：点云序列和视图序列。
- en: a) Point cloud sequence
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: a) 点云序列
- en: 'Point cloud sequence refers to multiple 3D LiDAR point clouds measured in continuous
    time series. Similar to image sequence integration, two-stage approaches can be
    applied here: first to extract features or obtain target objects from each 3D
    point cloud, and then to create association between LiDAR scans. In [[173](#bib.bib173),
    [174](#bib.bib174), [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177),
    [178](#bib.bib178), [179](#bib.bib179)], the inputs of temporal integration are
    the sequences of extracted features or other forms of outcomes generated from
    LiDAR point clouds to achieve 3D object tracking. Another approach is to generate
    a denser point cloud by combining multiple LiDAR scans in order to enhance the
    data quality. This is usually done via data alignment, which means to align multiple
    point clouds to a unified coordinate frame. Wang et al. the conduct the cross-frame
    integration by aligning and fusing “thing class” data points from multiple consecutive
    LiDAR scans to create an enriched point cloud [[180](#bib.bib180)]. The enriched
    point cloud is then treated as single-frame input for later processing stages
    such as sampling, training, and segmentation.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 点云序列指的是在连续时间序列中测量的多个3D LiDAR点云。类似于图像序列集成，这里可以应用两阶段方法：首先从每个3D点云中提取特征或获取目标对象，然后创建LiDAR扫描之间的关联。在[[173](#bib.bib173)、[174](#bib.bib174)、[175](#bib.bib175)、[176](#bib.bib176)、[177](#bib.bib177)、[178](#bib.bib178)、[179](#bib.bib179)]中，时间集成的输入是从LiDAR点云生成的提取特征或其他形式的结果序列，以实现3D对象跟踪。另一种方法是通过结合多个LiDAR扫描生成更密集的点云，以提高数据质量。这通常通过数据对齐来完成，即将多个点云对齐到统一的坐标框架。王等人通过对齐和融合来自多个连续LiDAR扫描的“事物类别”数据点进行跨帧集成，从而创建一个丰富的点云[[180](#bib.bib180)]。然后将丰富的点云作为单帧输入用于后续处理阶段，如采样、训练和分割。
- en: b) View sequence
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: b) 视图序列
- en: Another type of input for LiDAR temporal integration is view sequence, which
    is a sequence of 2D view representations generated from 3D point clouds via projection.
    Since this approach convert 3D data sequence to 2D format, image-based temporal
    integration approaches can be leveraged for processing. [[181](#bib.bib181)] projects
    3D points scans to range view images for semantic segmentation. The view image
    sequences (specifically, the feature maps extracted from views) are served as
    the input of temporal integration. LO-SLAM [[182](#bib.bib182)] converts LiDAR
    3D point clouds to cylinder views with cylinder projection. This model takes view
    pairs generated from pair-wise scans as input to infer relative 6-DoF pose.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种用于LiDAR时间集成的输入是视图序列，它是通过投影从3D点云生成的2D视图表示序列。由于这种方法将3D数据序列转换为2D格式，可以利用基于图像的时间集成方法进行处理。[[181](#bib.bib181)]
    将3D点扫描投射到范围视图图像上进行语义分割。视图图像序列（具体而言，是从视图中提取的特征图）作为时间集成的输入。LO-SLAM [[182](#bib.bib182)]
    将LiDAR 3D点云转换为带有圆柱体投影的圆柱视图。该模型以成对扫描生成的视图对作为输入，以推断相对的6自由度姿态。
- en: III-D Multi-view multi-modality integration
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 多视角多模态集成
- en: 'Researchers explore possible approaches to introduce multiple views into camera
    and LiDAR/radar integration to enhance the perception capability. Several works,
    including [[183](#bib.bib183), [184](#bib.bib184), [120](#bib.bib120), [185](#bib.bib185),
    [135](#bib.bib135), [186](#bib.bib186), [187](#bib.bib187)], integrate LiDAR with
    multiple camera images from different views. Some others such as [[135](#bib.bib135),
    [188](#bib.bib188), [189](#bib.bib189)] integrate multi-view LiDARs and monocular
    camera. MVFusion [[190](#bib.bib190)] propose a multi-view radar and multi-view
    camera fusion approach for 3D object detection. A mainstream pattern can be observed
    from these studies: first intra-modality (to fuse multiple views of a single modality),
    and then inter-modality (to fuse information from different modalities). Besides,
    research is also being conducted where multi-modality fusion is carried out first,
    followed by multi-view combination. In [[191](#bib.bib191)], LiDAR points are
    integrated with monocular camera to obtain single frame 3D point cloud at each
    view. Then multiple 3D point clouds obtained are incrementally spliced for excessive
    3D point cloud reconstruction with global optimizations.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员探索将多个视角引入相机和激光雷达/雷达融合的方法，以增强感知能力。一些研究，如[[183](#bib.bib183), [184](#bib.bib184),
    [120](#bib.bib120), [185](#bib.bib185), [135](#bib.bib135), [186](#bib.bib186),
    [187](#bib.bib187)]，将激光雷达与来自不同视角的多个相机图像融合。还有一些研究，如[[135](#bib.bib135), [188](#bib.bib188),
    [189](#bib.bib189)]，融合了多视角激光雷达和单目相机。MVFusion [[190](#bib.bib190)]提出了一种多视角雷达和多视角相机融合的方法，用于3D物体检测。从这些研究中可以观察到一个主流模式：首先是同模态内（融合单一模态的多个视角），然后是跨模态（融合不同模态的信息）。此外，还有一些研究是首先进行多模态融合，然后进行多视角组合。在[[191](#bib.bib191)]中，激光雷达点与单目相机融合以获得每个视角的单帧3D点云。然后，将获得的多个3D点云逐步拼接，以实现具有全局优化的过度3D点云重建。
- en: 'There is also research that fuses all three modalities: camera, LiDAR, and
    radar. For example, in [[192](#bib.bib192)], the raw LiDAR inputs are in the form
    of point clouds, and features to be integrated are in the form of a multi-scale
    BEV. As for radar modality, the raw inputs are properties of radar points (such
    as location, speed, and intensity), and the features to be integrated are per-point
    hidden representations extracted by MLPs. Besides radar and LiDAR data, inputs
    also contain images from different views and corresponding camera parameters.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 也有研究融合了三种模态：相机、激光雷达和雷达。例如，在[[192](#bib.bib192)]中，原始激光雷达输入以点云的形式存在，需要融合的特征则以多尺度BEV的形式存在。至于雷达模态，原始输入是雷达点的属性（如位置、速度和强度），需要融合的特征是由MLPs提取的每点隐藏表示。除了雷达和激光雷达数据，输入还包括来自不同视角的图像及其对应的相机参数。
- en: III-E Multi-view multi-frame integration
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 多视角多帧融合
- en: Multi-view multi-frame integration introduces temporal relation to multi-view
    fusion process. Most papers focus on adding time-series information into camera
    multi-view integration, and few works investigate LiDAR or radar multi-view. Therefore,
    we only present a brief discussion on camera multi-view multi-frame integration
    algorithms in this subsection.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角多帧融合为多视角融合过程引入了时间关系。大多数论文专注于将时间序列信息添加到相机多视角融合中，少数研究调查了激光雷达或雷达多视角。因此，本节仅简要讨论相机多视角多帧融合算法。
- en: BEV is an appropriate representation for multi-view camera data, thus some multi-view
    multi-frame papers adopt it into the integration pipeline. In [[193](#bib.bib193),
    [194](#bib.bib194), [195](#bib.bib195), [196](#bib.bib196), [197](#bib.bib197),
    [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200), [201](#bib.bib201)],
    images from different views are fused into a BEV representation, further propagating
    information along the temporal dimension for perception and prediction. It can
    be observed that in general, these algorithms tend to prioritize the step of spatial
    integration (multi-view) over temporal integration (multi-frame) in the fusion
    process.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: BEV是多视角相机数据的适当表示，因此一些多视角多帧的论文将其纳入融合流程。在[[193](#bib.bib193), [194](#bib.bib194),
    [195](#bib.bib195), [196](#bib.bib196), [197](#bib.bib197), [198](#bib.bib198),
    [199](#bib.bib199), [200](#bib.bib200), [201](#bib.bib201)]中，不同视角的图像被融合到BEV表示中，进一步在时间维度上传播信息以进行感知和预测。可以观察到，一般来说，这些算法倾向于在融合过程中优先考虑空间融合（多视角）而不是时间融合（多帧）。
- en: Besides BEV, many works use regular image representations for temporal view
    fusion. [[202](#bib.bib202)] and [[203](#bib.bib203)] use view pair sequences
    from stereo cameras to recover object scale or track objects. As stereo cameras
    are not always available, a great number of papers [[204](#bib.bib204), [172](#bib.bib172),
    [182](#bib.bib182), [205](#bib.bib205), [206](#bib.bib206), [157](#bib.bib157),
    [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209), [210](#bib.bib210),
    [211](#bib.bib211), [212](#bib.bib212), [213](#bib.bib213), [214](#bib.bib214),
    [197](#bib.bib197), [215](#bib.bib215)] utilize neighboring frames to form image
    pairs. These papers mainly target SLAM and reconstruction, which require multiple
    views to recover geometry.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 除了BEV，许多工作使用常规图像表示进行时间视图融合。[[202](#bib.bib202)]和[[203](#bib.bib203)]使用立体相机的视图对序列来恢复物体尺度或跟踪物体。由于立体相机并非总是可用，许多论文[[204](#bib.bib204),
    [172](#bib.bib172), [182](#bib.bib182), [205](#bib.bib205), [206](#bib.bib206),
    [157](#bib.bib157), [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209),
    [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212), [213](#bib.bib213),
    [214](#bib.bib214), [197](#bib.bib197), [215](#bib.bib215)]利用相邻帧来形成图像对。这些论文主要针对**SLAM**和重建，这些任务需要多视角来恢复几何结构。
- en: III-F Multi-modality multi-frame integration
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-F 多模态多帧整合
- en: 'Multi-modality multi-frame integration introduces a temporal aspect to the
    spatial integration process for enhanced perception performance. Current research
    on camera and LiDAR sequence integration is primarily focused on Mutiple Object
    Tracking (MOT)and 3D object detection. For instance, 3D MOT models combine features
    or detected objects from LiDAR and camera to obtain instances in a single frame,
    followed by a data association process for multi-frame tracking [[216](#bib.bib216),
    [217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219)]. The Transformer-based
    3D object detection model proposed by Zeng et al. [[220](#bib.bib220)] generates
    gridwise BEV images with RGB and LiDAR points, and then applies multi-sensor temporal
    integration operations given the BEV grids. These integration algorithms typically
    follow a two-stage paradigm: first, spatial integration (multi-modality fusion
    in a single frame), and then temporal integration (multi-frame association) [[221](#bib.bib221),
    [222](#bib.bib222), [218](#bib.bib218)].'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态多帧整合在空间整合过程中引入了时间方面，以增强感知性能。目前关于相机和LiDAR序列整合的研究主要集中在**多目标跟踪**（MOT）和3D目标检测上。例如，3D
    MOT模型结合来自LiDAR和相机的特征或检测到的目标，在单帧中获取实例，然后进行数据关联处理以进行多帧跟踪[[216](#bib.bib216), [217](#bib.bib217),
    [218](#bib.bib218), [219](#bib.bib219)]。Zeng等人提出的基于**Transformer**的3D目标检测模型[[220](#bib.bib220)]生成具有RGB和LiDAR点的网格化BEV图像，然后应用多传感器时间整合操作。这些整合算法通常遵循两阶段范式：首先，空间整合（单帧中的多模态融合），然后时间整合（多帧关联）[[221](#bib.bib221),
    [222](#bib.bib222), [218](#bib.bib218)]。
- en: There is limited research on the integration of LiDAR and radar point sequences.
    In one study [[113](#bib.bib113)], 2D regions of interest are detected in the
    BEV s converted from LiDAR and radar 3D points. The features are first integrated
    spatially according to the corresponding regions in LiDAR and radar BEV s and
    then temporally across frames, following the two-stage spatial-temporal integration
    pattern observed in camera and LiDAR integration.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LiDAR与雷达点序列整合的研究有限。在一项研究[[113](#bib.bib113)]中，从LiDAR和雷达3D点转换得到的BEV中检测2D感兴趣区域。特征首先根据LiDAR和雷达BEV中的对应区域进行空间整合，然后跨帧进行时间整合，遵循在相机与LiDAR整合中观察到的两阶段空间-时间整合模式。
- en: There are few studies related to the temporal integration of camera, LiDAR,
    and radar combinations, possibly due to the complexity of the integration. Existing
    studies break down the multi-sensor temporal integration task in various ways.
    For example, in one study [[223](#bib.bib223)], Extended Kalman Filter (EKF)is
    deployed to each sensor respectively, and a reliability function is used for cross-modality
    integration. In another study [[115](#bib.bib115)], the inputs of integration
    vary in different tasks, with camera images and LiDAR depth maps fused for 2D
    segmentation and object detection, while LiDAR and radar points over time are
    used for 3D obstacle detection and tracking. Liang et al. [[224](#bib.bib224)]
    propose a loosely-coupled integration architecture for vehicle state estimation
    based on Error-state EKF, where camera, LiDAR, and radar data are used as observations
    to correct the estimated priori state.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 关于相机、激光雷达和雷达组合的时间集成的研究较少，可能是由于集成的复杂性。现有研究以不同方式分解了多传感器时间集成任务。例如，在一项研究中[[223](#bib.bib223)]，将扩展卡尔曼滤波器（EKF）分别应用于每个传感器，并使用可靠性函数进行跨模态集成。在另一项研究中[[115](#bib.bib115)]，集成的输入在不同任务中有所变化，相机图像和激光雷达深度图用于2D分割和物体检测，而激光雷达和雷达点用于3D障碍物检测和跟踪。Liang
    等人[[224](#bib.bib224)] 提出了一种基于误差状态EKF的松耦合集成架构，用于车辆状态估计，其中相机、激光雷达和雷达数据作为观测值，用于修正估计的先验状态。
- en: 'IV Data Integration: When to Integrate'
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 数据集成：何时集成
- en: Since deep learning networks extract features with multiple neural layers, integration
    strategies can take place at different stages and in various ways [[14](#bib.bib14)].
    Broadly speaking, the strategies are commonly classified into early (data-level),
    middle (feature-level), and late stages (decision-level). Data-level integration
    methods involve the fusion of raw or preprocessed data before feature extraction.
    Feature-level integration, on the other hand, combines extracted features at intermediate
    neural network layers. Decision-level integration involves the merging of output
    data separately estimated by each sensor. As features can be obtained at different
    depths of a neural network, several detailed integration paradigms have been designed
    and applied for feature-level integration. Feng et al. [[14](#bib.bib14)] further
    classifies the middle integration into three patterns, including fusion in one
    layer, deep fusion, and short-cut fusion.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习网络通过多个神经层提取特征，集成策略可以在不同阶段和以不同方式进行[[14](#bib.bib14)]。广义上，这些策略通常分为早期（数据级）、中期（特征级）和晚期（决策级）。数据级集成方法涉及在特征提取之前融合原始或预处理的数据。特征级集成则是在中间神经网络层结合提取的特征。决策级集成涉及将每个传感器单独估计的输出数据进行合并。由于特征可以在神经网络的不同深度获得，因此为特征级集成设计并应用了多种详细的集成范式。Feng
    等人[[14](#bib.bib14)]进一步将中期集成细分为三种模式，包括单层融合、深度融合和短路融合。
- en: However, as described in Section [I-2](#S1.SS0.SSS2 "I-2 When to integrate ‣
    I Introduction ‣ A survey on deep learning approaches for data integration in
    autonomous driving system"), this method fails to classify methods where integrated
    features are not at the same level (depth) of data abstraction, limiting its applicability
    to fit the increased amount of fusion schemes. Cui et al. [[28](#bib.bib28)] proposes
    an approach to incorporate more integration circumstances by introducing “multi-level”
    integration in addition to the original three classes. Thus, we review the related
    works with respect to these four categories in this section.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如第[I-2](#S1.SS0.SSS2 "I-2 When to integrate ‣ I Introduction ‣ A survey on
    deep learning approaches for data integration in autonomous driving system")节所述，该方法未能分类集成特征不在相同数据抽象层级的方法，限制了其适应增加的融合方案的能力。Cui
    等人[[28](#bib.bib28)]提出了一种通过引入“多层次”集成来结合更多集成情况的方法，除了原有的三类。因此，我们在本节中回顾了与这四个类别相关的工作。
- en: IV-A Data-level integration
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 数据级集成
- en: Data-level integration is popular in SLAM [[202](#bib.bib202), [170](#bib.bib170),
    [204](#bib.bib204), [172](#bib.bib172), [182](#bib.bib182), [207](#bib.bib207),
    [144](#bib.bib144)], but rare in other tasks. [[170](#bib.bib170)] uses optical
    flow to exploit relationship between frames to learn poses and uses cost volume
    between frames to learn depth maps. [[204](#bib.bib204), [172](#bib.bib172), [182](#bib.bib182)]
    utilize convolutions to generate feature vectors. [[202](#bib.bib202)] stacks
    multi-frames together to regress pose. [[207](#bib.bib207)] regresses poses with
    convolutions. Fusing data in the early stage is also employed in detection [[225](#bib.bib225),
    [127](#bib.bib127), [143](#bib.bib143), [145](#bib.bib145)], depth completion
    [[123](#bib.bib123)] and semantic segmentation [[180](#bib.bib180)]. However,
    a large percent of these approaches lead to noncompetitive results.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 数据级别的融合在 SLAM 中非常流行 [[202](#bib.bib202), [170](#bib.bib170), [204](#bib.bib204),
    [172](#bib.bib172), [182](#bib.bib182), [207](#bib.bib207), [144](#bib.bib144)]，但在其他任务中则较为少见。[[170](#bib.bib170)]
    利用光流来挖掘帧之间的关系以学习姿态，并使用帧间的成本体积来学习深度图。[[204](#bib.bib204), [172](#bib.bib172), [182](#bib.bib182)]
    采用卷积生成特征向量。[[202](#bib.bib202)] 将多帧图像堆叠在一起进行姿态回归。[[207](#bib.bib207)] 通过卷积进行姿态回归。在检测
    [[225](#bib.bib225), [127](#bib.bib127), [143](#bib.bib143), [145](#bib.bib145)]、深度补全
    [[123](#bib.bib123)] 和语义分割 [[180](#bib.bib180)] 中也使用了早期数据融合。然而，这些方法中有很大一部分导致了不具竞争力的结果。
- en: IV-B Feature-level integration
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 特征级别融合
- en: Feature-level integration is widely adopted in most areas including object detection,
    depth completion [[122](#bib.bib122), [120](#bib.bib120)], semantic segmentation
    [[87](#bib.bib87), [130](#bib.bib130), [226](#bib.bib226), [106](#bib.bib106),
    [105](#bib.bib105), [227](#bib.bib227), [181](#bib.bib181)], tracking [[221](#bib.bib221),
    [217](#bib.bib217), [149](#bib.bib149), [222](#bib.bib222), [150](#bib.bib150)],
    3D reconstruction [[97](#bib.bib97), [100](#bib.bib100)], SLAM [[228](#bib.bib228),
    [156](#bib.bib156), [229](#bib.bib229), [171](#bib.bib171), [199](#bib.bib199),
    [169](#bib.bib169)], action classification [[227](#bib.bib227), [230](#bib.bib230)]
    and navigation [[114](#bib.bib114)]. The popularity of feature-level integration
    comes from the extracted features, which are more compact and representative than
    the raw data. Nevertheless, feature-level merging has the adverse effect of diluting
    the single modalities’ strengths. The majority of 2D and 3D object detection methods
    fuse in feature level [[133](#bib.bib133), [136](#bib.bib136), [76](#bib.bib76),
    [124](#bib.bib124), [119](#bib.bib119), [231](#bib.bib231), [232](#bib.bib232),
    [59](#bib.bib59), [233](#bib.bib233), [130](#bib.bib130), [125](#bib.bib125),
    [134](#bib.bib134), [129](#bib.bib129), [112](#bib.bib112), [113](#bib.bib113),
    [88](#bib.bib88), [192](#bib.bib192), [234](#bib.bib234), [197](#bib.bib197),
    [235](#bib.bib235), [199](#bib.bib199), [236](#bib.bib236), [237](#bib.bib237),
    [238](#bib.bib238), [220](#bib.bib220), [177](#bib.bib177), [131](#bib.bib131),
    [142](#bib.bib142), [132](#bib.bib132), [140](#bib.bib140), [201](#bib.bib201),
    [187](#bib.bib187)]. [[133](#bib.bib133)] uses deep parametric continuous convolution
    layers with MLPs to directly output target feature in each BEV grid. [[140](#bib.bib140)]
    integrates LiDAR voxel features and image features with attention designs. Studies
    [[86](#bib.bib86), [193](#bib.bib193), [195](#bib.bib195), [196](#bib.bib196),
    [197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199)] focusing on spatiotemporal
    fusion in BEV map generation integrate features of surrounding images or BEV feature
    maps at different timestamps with various designs of neural networks. In [[136](#bib.bib136)],
    features from both cloud points are grid-wisely combined based on learned weights.
    Besides, most 3D reconstruction methods [[209](#bib.bib209), [239](#bib.bib239),
    [240](#bib.bib240), [241](#bib.bib241), [242](#bib.bib242), [243](#bib.bib243),
    [244](#bib.bib244), [245](#bib.bib245)] integrate at feature level because of
    two reasons. On the one, the extracted features are more stable to environmental
    changes, e.g., lighting, which are crucial to photo-consistency measurement. Thus,
    it is less suitable to directly integrate raw data. On the other hand, the 3D
    estimates obtained from each single view is ambiguous in scale, which makes integrating
    at decision level challenging.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 特征级融合在大多数领域得到广泛应用，包括目标检测、深度完成 [[122](#bib.bib122), [120](#bib.bib120)]、语义分割
    [[87](#bib.bib87), [130](#bib.bib130), [226](#bib.bib226), [106](#bib.bib106),
    [105](#bib.bib105), [227](#bib.bib227), [181](#bib.bib181)]、跟踪 [[221](#bib.bib221),
    [217](#bib.bib217), [149](#bib.bib149), [222](#bib.bib222), [150](#bib.bib150)]、3D
    重建 [[97](#bib.bib97), [100](#bib.bib100)]、SLAM [[228](#bib.bib228), [156](#bib.bib156),
    [229](#bib.bib229), [171](#bib.bib171), [199](#bib.bib199), [169](#bib.bib169)]、动作分类
    [[227](#bib.bib227), [230](#bib.bib230)] 和导航 [[114](#bib.bib114)]。特征级融合的普及源于提取的特征比原始数据更紧凑且具有代表性。然而，特征级融合的副作用是稀释了单一模态的优势。大多数
    2D 和 3D 目标检测方法在特征级融合 [[133](#bib.bib133), [136](#bib.bib136), [76](#bib.bib76),
    [124](#bib.bib124), [119](#bib.bib119), [231](#bib.bib231), [232](#bib.bib232),
    [59](#bib.bib59), [233](#bib.bib233), [130](#bib.bib130), [125](#bib.bib125),
    [134](#bib.bib134), [129](#bib.bib129), [112](#bib.bib112), [113](#bib.bib113),
    [88](#bib.bib88), [192](#bib.bib192), [234](#bib.bib234), [197](#bib.bib197),
    [235](#bib.bib235), [199](#bib.bib199), [236](#bib.bib236), [237](#bib.bib237),
    [238](#bib.bib238), [220](#bib.bib220), [177](#bib.bib177), [131](#bib.bib131),
    [142](#bib.bib142), [132](#bib.bib132), [140](#bib.bib140), [201](#bib.bib201),
    [187](#bib.bib187)]。[[133](#bib.bib133)] 使用深度参数化连续卷积层与 MLP 直接在每个 BEV 网格中输出目标特征。[[140](#bib.bib140)]
    结合 LiDAR 体素特征和图像特征，使用注意力设计。研究 [[86](#bib.bib86), [193](#bib.bib193), [195](#bib.bib195),
    [196](#bib.bib196), [197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199)]
    专注于 BEV 地图生成中的时空融合，将不同时间戳的周围图像或 BEV 特征图的特征与各种神经网络设计进行融合。在 [[136](#bib.bib136)]
    中，来自云点的特征基于学习到的权重以网格方式结合。此外，大多数 3D 重建方法 [[209](#bib.bib209), [239](#bib.bib239),
    [240](#bib.bib240), [241](#bib.bib241), [242](#bib.bib242), [243](#bib.bib243),
    [244](#bib.bib244), [245](#bib.bib245)] 在特征级进行融合，原因有二。一方面，提取的特征对环境变化（例如光照）更加稳定，这对照片一致性测量至关重要。因此，直接融合原始数据不太适用。另一方面，每个单视图获得的
    3D 估计在尺度上模糊，这使得在决策级融合具有挑战性。
- en: IV-C Decision-level integration
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 决策级融合
- en: Decision-level integration has wide applications in many areas including semantic
    segmentation [[85](#bib.bib85)], reconstruction [[246](#bib.bib246), [118](#bib.bib118)],
    object detection, tracking, SLAM and annotation [[77](#bib.bib77)]. Among all
    these domains, it is most popular in object detection [[247](#bib.bib247), [248](#bib.bib248),
    [109](#bib.bib109), [126](#bib.bib126), [110](#bib.bib110), [71](#bib.bib71),
    [185](#bib.bib185), [249](#bib.bib249), [250](#bib.bib250), [250](#bib.bib250)],
    tracking [[216](#bib.bib216), [148](#bib.bib148), [176](#bib.bib176), [175](#bib.bib175),
    [154](#bib.bib154), [251](#bib.bib251), [252](#bib.bib252), [111](#bib.bib111),
    [89](#bib.bib89), [250](#bib.bib250)] and SLAM [[253](#bib.bib253), [254](#bib.bib254),
    [255](#bib.bib255), [224](#bib.bib224), [256](#bib.bib256), [223](#bib.bib223),
    [98](#bib.bib98), [99](#bib.bib99), [152](#bib.bib152)]. Decisions from different
    sources can be either fused with learning-based strategies or non-learning-based
    ones.[[85](#bib.bib85)] adds up all input features from multiple views to generate
    fused features. [[246](#bib.bib246)] fuses the depth maps and uncertainty maps
    according to the weighted scheme, then output the tensor after weighted addition.
    [[247](#bib.bib247)] transforms cooperative awareness messages (CAM) to relative
    measurements, then uses buffering strategies to deal with time synchronization
    in out-of-sequence measurements (OOSM). [[248](#bib.bib248)] fuses each pair of
    2D bounding box (predicted from image) and 3D bounding box (projected to image)
    candidate by first generating a new tensor based on IOU and confidence, then further
    fuse with convolution layers.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 决策级别集成在许多领域有广泛的应用，包括语义分割 [[85](#bib.bib85)]、重建 [[246](#bib.bib246), [118](#bib.bib118)]、物体检测、跟踪、SLAM
    和注释 [[77](#bib.bib77)]。在所有这些领域中，它在物体检测 [[247](#bib.bib247), [248](#bib.bib248),
    [109](#bib.bib109), [126](#bib.bib126), [110](#bib.bib110), [71](#bib.bib71),
    [185](#bib.bib185), [249](#bib.bib249), [250](#bib.bib250), [250](#bib.bib250)]、跟踪
    [[216](#bib.bib216), [148](#bib.bib148), [176](#bib.bib176), [175](#bib.bib175),
    [154](#bib.bib154), [251](#bib.bib251), [252](#bib.bib252), [111](#bib.bib111),
    [89](#bib.bib89), [250](#bib.bib250)] 和 SLAM [[253](#bib.bib253), [254](#bib.bib254),
    [255](#bib.bib255), [224](#bib.bib224), [256](#bib.bib256), [223](#bib.bib223),
    [98](#bib.bib98), [99](#bib.bib99), [152](#bib.bib152)] 中最为流行。从不同来源的决策可以通过基于学习的策略或非基于学习的策略进行融合。[[85](#bib.bib85)]
    汇总了来自多个视角的所有输入特征以生成融合特征。[[246](#bib.bib246)] 根据加权方案融合深度图和不确定性图，然后在加权相加后输出张量。[[247](#bib.bib247)]
    将协作感知消息（CAM）转换为相对测量值，然后使用缓冲策略处理时间同步中的时序异常测量（OOSM）。[[248](#bib.bib248)] 首先通过基于
    IOU 和置信度生成一个新张量来融合每一对 2D 边界框（从图像中预测）和 3D 边界框（投影到图像中），然后进一步通过卷积层进行融合。
- en: IV-D Multi-level integration
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 多级集成
- en: Multi-level integration is adopted in lots of applications including object
    detection, depth competition [[121](#bib.bib121)], semantic segmentation [[75](#bib.bib75)],
    [[128](#bib.bib128)], tracking, SLAM and 3D reconstruction [[208](#bib.bib208),
    [191](#bib.bib191)]. Most multi-level integration methods are designed for object
    detection and tracking. There are also quite a percent of papers focusing on SLAM.
    [[139](#bib.bib139)] fuses feature level information from the camera and data-level
    information from the LiDAR. [[62](#bib.bib62)] and [[61](#bib.bib61)] leverage
    the outputs of the image object detector as 2D proposals, which are then projected
    to form 3D searching space for 3D object detection. To specify, [[146](#bib.bib146)]
    projects LiDAR and radar information into 2D maps and use convolutions to fuse
    them with camera RGB. [[257](#bib.bib257)] fuses features and raw data fusion
    via concatenation. [[141](#bib.bib141)] fuses decision level information from
    the camera and data level information from the radar. [[258](#bib.bib258)] has
    one input at the object level and another at the feature level. [[121](#bib.bib121)]
    uses independent branches to extract features of images and LiDAR point cloud,
    and fuse (addition or concatenate) at multiple levels. [[208](#bib.bib208)] warps
    the former depth map prediction to current view’s hidden representation according
    to geometry. [[153](#bib.bib153)] uses tracking conditioned detector to detect
    objects in new frame (conditioned on previous frame and object detection result
    of previous frame) to get a temporally coherent set of detected objects, where
    the 2D displacement is predicted to associate detection results through time.
    [[173](#bib.bib173)] uses multiple faces features to refine locations. [[259](#bib.bib259)]
    firstly uses optical flow to get short tracklets, then uses pixelwise depth estimation
    based on camera ego-motion to get 3D motion, finally uses 3D motion consistency
    to get long term tracklets. [[155](#bib.bib155)] uses detected object position
    (decision-level) and feature vector from detected mask (feature-level) to associate
    frames, and uses nearby targets (neighbors) to constrain matching IoU distribution.
    [[218](#bib.bib218)] fuses 3D-2D detection by backproject 3D to 2D, and fuses
    frames by appearance association based on raw image and 2D/3D motion relation
    based on raw image and pointcloud. [[146](#bib.bib146)] fuses RGB features and
    raw data from LiDAR and radar. [[147](#bib.bib147)] fuses the region predicted
    by camera and raw data from LiDAR and radar.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 多层次融合被广泛应用于多个领域，包括目标检测、深度竞赛 [[121](#bib.bib121)]、语义分割 [[75](#bib.bib75)]、[[128](#bib.bib128)]、跟踪、SLAM
    和 3D 重建 [[208](#bib.bib208)、[191](#bib.bib191)]。大多数多层次融合方法是为了目标检测和跟踪设计的。也有相当一部分论文关注于
    SLAM。[[139](#bib.bib139)] 融合了来自相机的特征级信息和来自 LiDAR 的数据级信息。[[62](#bib.bib62)] 和 [[61](#bib.bib61)]
    利用图像目标检测器的
- en: 'V Data Integration: How to Integrate'
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 数据融合：如何融合
- en: 'The “how to integrate” component of data integration involves the mathematical
    operations used to combine data/features. Feng et al. [[14](#bib.bib14)] summarize
    the fusion operations utilized in deep learning networks into four categories:
    addition/average mean, concatenation, ensemble, and mixture of experts. The characteristics
    of different integration operations are not further elaborated in their study.
    Wang et al. [[15](#bib.bib15)] categorize data combination approaches for 3D object
    detection into two primary types: alignment and fusion. Alignment is further subcategorized
    into projection-based and model-based methods, while fusion is divided into learning-based
    and learning-agnostic approaches. The authors of the present paper believe that
    alignment also serves as a means of fusion in many circumstances, and the model-based
    alignment and learning-based fusion techniques showcase significant overlaps with
    each other. Besides, since both [[14](#bib.bib14)] and [[15](#bib.bib15)] focus
    on methodologies for specific perception tasks (object detection and semantic
    segmentation), other integration operations such as EKF [[185](#bib.bib185), [154](#bib.bib154)]
    or probabilistic map [[89](#bib.bib89), [118](#bib.bib118)] are not discussed.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集成中的“如何集成”部分涉及用于组合数据/特征的数学操作。Feng 等人 [[14](#bib.bib14)] 将深度学习网络中使用的融合操作总结为四类：加法/平均值、拼接、集成和专家混合。他们的研究中没有进一步详细说明不同集成操作的特征。Wang
    等人 [[15](#bib.bib15)] 将 3D 对象检测的数据组合方法分类为两种主要类型：对齐和融合。对齐进一步细分为基于投影的方法和基于模型的方法，而融合则分为基于学习的方法和不依赖学习的方法。本文的作者认为，对齐在许多情况下也作为融合的一种手段，基于模型的对齐方法和基于学习的融合技术在很大程度上存在重叠。此外，由于
    [[14](#bib.bib14)] 和 [[15](#bib.bib15)] 关注于特定感知任务（对象检测和语义分割）的方法，其他集成操作，如 EKF [[185](#bib.bib185),
    [154](#bib.bib154)] 或概率图 [[89](#bib.bib89), [118](#bib.bib118)]，没有被讨论。
- en: In Table [V](#A2.T5 "TABLE V ‣ Appendix B ‣ A survey on deep learning approaches
    for data integration in autonomous driving system"), we provide a brief summary
    of several commonly used operations in deep learning-based data integration processes,
    including projection, concatenation, addition-similar operations (addition/weighted
    summation/average mean), probabilistic methods, rule-based transaction, temporal
    integration approaches, and encoder-decoder methods. The application of integration
    operations may be highly dependent on the task, inputs, and data abstraction levels.
    Some operations are more versatile and applicable in various conditions, while
    others are more specific and implemented for certain purposes. Additionally, an
    integration algorithm may involve multiple different operations at various stages.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [V](#A2.T5 "TABLE V ‣ Appendix B ‣ A survey on deep learning approaches
    for data integration in autonomous driving system") 中，我们提供了深度学习数据集成过程中几种常用操作的简要总结，包括投影、拼接、加法类似操作（加法/加权求和/平均值）、概率方法、基于规则的事务、时间集成方法以及编码器-解码器方法。集成操作的应用可能高度依赖于任务、输入和数据抽象层次。一些操作更具通用性，可在各种条件下应用，而其他操作则更具特定性，为某些目的而实现。此外，一个集成算法可能在不同阶段涉及多种不同的操作。
- en: V-1 Projection
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-1 投影
- en: Projections, such as 3D-to-2D projection and 2D-to-3D back-projection (also
    known as “reverse projection”), are commonly used to connect images in 2D space
    and point clouds in 3D space, allowing for data to be operated within the same
    domain. 3D-to-2D projection reduces the dimensionality of 3D objects to a 2D plane,
    while 2D-to-3D back-projection may rely on geometric cues or depth information.
    Projections are frequently utilized to generate different data representations
    [[15](#bib.bib15)]. For example, pixel-based view representations of LiDAR points
    and point-based pseudo-LiDAR point representation of images, as mentioned in Section [II-B](#S2.SS2
    "II-B LiDAR ‣ II Sensing modalities and pre-processing ‣ A survey on deep learning
    approaches for data integration in autonomous driving system"), are outputs of
    projection and back-projection, respectively. Additionally, projection can also
    serve as an integration operation to combine data.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 投影，如3D到2D投影和2D到3D回投影（也称为“反向投影”），通常用于将二维空间中的图像与三维空间中的点云连接，使数据可以在同一领域内操作。3D到2D投影将3D对象的维度降低到二维平面，而2D到3D回投影可能依赖于几何线索或深度信息。投影经常用于生成不同的数据表示
    [[15](#bib.bib15)]。例如，LiDAR点的基于像素的视图表示和图像的基于点的伪LiDAR点表示，如在[II-B](#S2.SS2 "II-B
    LiDAR ‣ II Sensing modalities and pre-processing ‣ A survey on deep learning approaches
    for data integration in autonomous driving system")部分提到的，分别是投影和回投影的输出。此外，投影还可以作为一种集成操作来结合数据。
- en: As an integration operation, projection may take place at raw-data level, feature
    level, or decision level.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种集成操作，投影可以在原始数据级、特征级或决策级进行。
- en: •
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Raw-data level integration refers to projecting LiDAR or radar point clouds
    onto 2D plane such that they can be combined with corresponding RGB image information
    [[136](#bib.bib136), [118](#bib.bib118), [141](#bib.bib141), [77](#bib.bib77),
    [124](#bib.bib124), [130](#bib.bib130), [260](#bib.bib260)]. An example is LATTE
    [[77](#bib.bib77)], where the points are projected onto the RGB image plane at
    the beginning of the sensor fusion pipeline. Similarly, LiDAR points are projected
    onto the image to obtain segmentation information in PointPainting [[124](#bib.bib124)].
    In [[191](#bib.bib191)], dense 3D points depth map is generated via projecting
    3D LiDAR points to the corresponding semantic 2D image followed by interpolation.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 原始数据级集成指的是将LiDAR或雷达点云投影到二维平面上，以便将其与对应的RGB图像信息结合 [[136](#bib.bib136), [118](#bib.bib118),
    [141](#bib.bib141), [77](#bib.bib77), [124](#bib.bib124), [130](#bib.bib130),
    [260](#bib.bib260)]。一个例子是LATTE [[77](#bib.bib77)]，其中点在传感器融合管道开始时被投影到RGB图像平面上。类似地，在PointPainting
    [[124](#bib.bib124)]中，LiDAR点被投影到图像上以获得分割信息。在 [[191](#bib.bib191)] 中，通过将3D LiDAR点投影到对应的语义二维图像并进行插值，生成了稠密的3D点深度图。
- en: •
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Feature-level projection is to project features extracted from LiDAR or radar
    points to 2D space for integration. This integration usually takes place in the
    middle layers of a neural network. An example is EPNet [[119](#bib.bib119)], where
    point features and image semantic features are combined in multiple scales in
    2D domain. In LoGoNet’s global and local fusion modules [[140](#bib.bib140)],
    voxel point centroids or center points are projected to the image plane to generate
    reference points to sample image features, and utilizes attention structure for
    integration. A problem for raw-data and feature level projections is how to deal
    with the resolution consistency between LiDAR and camera branches. In [[130](#bib.bib130)],
    point clouds are projected onto extracted RGB feature maps which have lower resolution
    to avoid discarding image information.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征级投影是将从LiDAR或雷达点提取的特征投影到二维空间以进行集成。这种集成通常发生在神经网络的中间层。一个例子是EPNet [[119](#bib.bib119)]，其中点特征和图像语义特征在二维域中的多个尺度上进行结合。在LoGoNet的全局和局部融合模块
    [[140](#bib.bib140)] 中，体素点质心或中心点被投影到图像平面上以生成参考点来采样图像特征，并利用注意力结构进行集成。原始数据级和特征级投影的问题是如何处理LiDAR和摄像机分支之间的分辨率一致性。在
    [[130](#bib.bib130)] 中，点云被投影到提取的RGB特征图上，这些特征图具有较低的分辨率，以避免丢失图像信息。
- en: •
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Decision-level 3D outcomes are projected to 2D space for integration in some
    other works. For example, in [[216](#bib.bib216)], 3D bounding boxes obtained
    from point clouds are projected to 2D image plane, such that the original 3D bounding
    boxes and 2D bounding boxes generated from image can be associated based on box
    overlap in image domain. [[110](#bib.bib110)] projects radar points to image plane,
    and integrates radar predictions (slices in this paper) and camera predictions
    based on box overlaps. In the early stage of CAMO-MOT [[219](#bib.bib219)], 3D
    detection results of LiDAR point cloud are projected onto the pixel plane of the
    camera to obtain 2D detection results.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些其他研究将决策级3D结果投影到2D空间进行整合。例如，在[[216](#bib.bib216)]中，从点云中获得的3D边界框被投影到2D图像平面，从而原始3D边界框和从图像生成的2D边界框可以基于图像域中的框重叠进行关联。[[110](#bib.bib110)]将雷达点投影到图像平面，并根据框重叠整合雷达预测（本文中的切片）和相机预测。在CAMO-MOT的早期阶段[[219](#bib.bib219)]，LiDAR点云的3D检测结果被投影到相机的像素平面上，以获得2D检测结果。
- en: A few works conduct decision level back-projection to integrate 2D image data
    with 3D points. A common operation for object detection is to back-project 2D
    proposals to 3D space, generating frustums as the regions of interest (RoI) to
    guide 3D point searching and crop clouds [[61](#bib.bib61), [62](#bib.bib62),
    [147](#bib.bib147), [147](#bib.bib147)]. To reduce the irrelevant background information
    wrapped in RoIs, Yang et al. back-projects segmentation masks instead of bounding
    boxes to the point clouds [[261](#bib.bib261)]. Since LiDAR points capture geometric
    information and by nature can easily be clustered to distinguish foreground from
    background, the points belong to background can be projected back onto the image
    to refine 2D segmentation. This types of integration operation based on back-projection
    is called “ensemble” [[14](#bib.bib14)]. However, since this back-projection operation
    integrates LiDAR points and images in a data association or supervision way, it
    usually fails to fully exploit the information in images and is considered as
    “weak fusion” in [[4](#bib.bib4)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究进行决策级反向投影，以将2D图像数据与3D点整合。一个常见的操作是将2D建议反向投影到3D空间，生成视锥体作为兴趣区域（RoI）来指导3D点的搜索和裁剪云[[61](#bib.bib61),
    [62](#bib.bib62), [147](#bib.bib147), [147](#bib.bib147)]。为了减少包裹在RoI中的无关背景信息，Yang等人将分割掩膜而不是边界框反向投影到点云中[[261](#bib.bib261)]。由于LiDAR点捕捉几何信息，并且天生可以很容易地进行聚类以区分前景和背景，因此属于背景的点可以反向投影回图像中，以细化2D分割。这种基于反向投影的整合操作被称为“集成”[[14](#bib.bib14)]。然而，由于这种反向投影操作以数据关联或监督的方式整合LiDAR点和图像，通常未能充分利用图像中的信息，并在[[4](#bib.bib4)]中被认为是“弱融合”。
- en: After reviewing the aforementioned methods, we identify two common issues that
    need to be addressed when integrating data through projection techniques. For
    the first, since multi-modality data are collected with different sampling rates
    and in different coordinates, they must be carefully aligned into a unified coordinate
    before projection and mapping. This process heavily relies on the sensor extrinsic
    and intrinsic matrices, which may be vulnerable to parameter errors. For the second,
    how to deal with the resolution inconsistency may significantly impact the performance
    of the integration [[130](#bib.bib130)]. In existing works, these two issues are
    usually not fully explored or discussed.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在审查了上述方法后，我们发现了在通过投影技术整合数据时需要解决的两个常见问题。首先，由于多模态数据以不同的采样率和不同的坐标收集，它们必须在投影和映射之前仔细对齐到统一的坐标系。这一过程严重依赖于传感器的外部和内部矩阵，这些矩阵可能容易受到参数误差的影响。其次，如何处理分辨率不一致的问题可能会显著影响整合性能[[130](#bib.bib130)]。在现有的研究中，这两个问题通常没有得到充分的探讨或讨论。
- en: V-2 Concatenation
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-2 拼接
- en: Concatenation is an operation widely used to combine data or feature maps at
    different layers of a neural network [[139](#bib.bib139), [121](#bib.bib121),
    [122](#bib.bib122), [109](#bib.bib109), [262](#bib.bib262), [114](#bib.bib114),
    [106](#bib.bib106), [93](#bib.bib93), [263](#bib.bib263), [264](#bib.bib264),
    [184](#bib.bib184), [116](#bib.bib116), [97](#bib.bib97), [131](#bib.bib131)].
    Concatenation operation can either be in the way of stacking feature maps along
    the depth as additional channels, or be added to the end of flattened vectors.
    For example, [[127](#bib.bib127)] and [[115](#bib.bib115)] convert LiDAR point
    clouds to gray images, and concatenate with RGB image as additional channels.
    Similarly, the projected and processed LiDAR and radar channels are concatenated
    with camera images for multiple times in [[146](#bib.bib146)]. In [[217](#bib.bib217)],
    the image feature vector is obtained by concatenating motion and appearance feature
    vectors obtained from ResNet [[265](#bib.bib265)]. Then the image and LiDAR features
    are integrated again by concatenation. Qi et al. [[142](#bib.bib142)] combine
    camera and radar information by concatenating two-channel radar image with same-sized
    64-channel visual image feature maps. MVFusion [[190](#bib.bib190)] designs a
    semantic-aligned radar encoder (SARE) in which semantic indicator produced via
    all stages’ visual features are concatenated with radar inputs. In C2FNet’s [[97](#bib.bib97)]
    fine generation module, concatenation takes place to integrate global guidance
    features with point cloud features in different levels and in different views.
    In GARNet [[100](#bib.bib100)], the feature map of each view is concatenated with
    its deviation from the global feature maps in post merger block. One study [[117](#bib.bib117)]
    presents trials of early fusion approach in which LiDAR and camera data can be
    concatenated in depth dimension, and late fusion approach in which the multi-modality
    features are concatenated. Besides, concatenation can be used as a straightforward
    temporal fusion approach. In [[198](#bib.bib198), [197](#bib.bib197)], BEV features
    of previous frame are spatially aligned and concatenated with the ones of the
    current frame.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 级联是一
- en: Concatenation in depth dimension requires the inputs to be in the same spatial
    size. Specifically, the width and height of feature maps to be concatenated together
    should be the same, while the number of channels can be different. For example,
    in MVFusion [[190](#bib.bib190)], raw radar points are preprocessed into a representation
    with the same shape as images before concatenation, by first extending them to
    pillars and then projecting pillars to the corresponding image view. Because of
    this, concatenation can be used to combine data from more than two data sources.
    In [[114](#bib.bib114)], information from camera, LiDAR, radar, and localization
    and mapping are concatenated together in perception module. In [[178](#bib.bib178)],
    input features pairs for cross-frame aggragation are the concatenation outcomes
    of template feature, reference frame feature, and reference frame point.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 深度维度上的拼接要求输入具有相同的空间大小。具体来说，被拼接的特征图的宽度和高度应相同，而通道数可以不同。例如，在 MVFusion [[190](#bib.bib190)]
    中，原始雷达点在拼接之前被预处理成与图像形状相同的表示，通过首先将其扩展为柱体，然后将柱体投影到相应的图像视图中。由于这一点，拼接可以用来结合来自两个以上数据源的数据。在
    [[114](#bib.bib114)] 中，相机、激光雷达、雷达以及定位和地图信息在感知模块中被拼接在一起。在 [[178](#bib.bib178)]
    中，跨帧聚合的输入特征对是模板特征、参考帧特征和参考帧点的拼接结果。
- en: Concatenation is a straightforward operation with numerous applications, particularly
    as a fundamental combination technique in neural networks. The output format of
    concatenation closely resembles the input format, although the dimension might
    alter (for example, an increase in the number of channels or vector length). Nonetheless,
    concatenation has certain limitations in practice, apart from the stringent requirement
    of equal spatial size. Firstly, the input dimension of concatenation is typically
    inflexible. Consider a camera-LiDAR integration model where 3-channel LiDAR data
    is combined with 3-channel RGB data (3 RGB channels initially, followed by 3 LiDAR
    channels). If one sensor fails, the model will not function correctly because
    the integrated data’s dimension decreases from 6-channel to 3-channel, making
    it incompatible with downstream modules. Secondly, concatenation input sensors
    are not interchangeable. If the order of the two sensors is reversed, the model
    will not produce accurate results. As a result, concatenation struggles with varying
    input dimensions and is not ideal for integration systems that utilize interchangeable
    sensors.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 拼接是一个简单的操作，具有众多应用，特别是作为神经网络中的基本组合技术。拼接的输出格式与输入格式非常相似，尽管维度可能会改变（例如，通道数或向量长度的增加）。然而，拼接在实践中有一定的局限性，除了对空间大小的严格要求之外。首先，拼接的输入维度通常是不灵活的。考虑一个相机-激光雷达集成模型，其中3通道的激光雷达数据与3通道的RGB数据（最初是3个RGB通道，然后是3个激光雷达通道）进行组合。如果一个传感器发生故障，模型将无法正常工作，因为集成数据的维度从6通道减少到3通道，使其与下游模块不兼容。其次，拼接输入传感器不可互换。如果两个传感器的顺序被颠倒，模型将无法产生准确的结果。因此，拼接在面对变化的输入维度时表现不佳，并且不适合使用可互换传感器的集成系统。
- en: V-3 Addition-similar operations
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-3 相似操作的加法
- en: Addition, weighted sum, and average mean are mathematical operations used to
    combine two data sources by adding them together with pre-determined weights.
    These operations can be applied to values, feature vectors, or feature maps in
    an element-wise way. An example of integration with addition can be found in [[85](#bib.bib85)],
    where the authors add all features up in the multi-view fusion module. Element-wise
    summation is also implemented in [[137](#bib.bib137), [266](#bib.bib266)]. As
    shown in the residual fusion module in [[262](#bib.bib262)], multiple feature
    maps from LiDAR and camera are integrated together via element-wise mean. One
    disadvantage of addition and average mean is that they combine different data
    sources with equal importance. Weighted sum makes it possible to give different
    weights to different inputs. In the depth map refinement stage of the model in
    [[246](#bib.bib246)], the depth maps and a uncertainty map are fused according
    to a weighted scheme. In [[136](#bib.bib136)], the authors predict a pair of weights
    for each pair of the LiDAR and image features, thus the features can be integrated
    considering the reliability of the information. In [[100](#bib.bib100)], score
    maps are generated in merger blocks as weights when fusing feature maps and when
    fusing the reconstructed voxels from all view images.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 加法、加权和均值是用于通过将两个数据源相加并赋予预定权重的数学操作。这些操作可以以逐元素的方式应用于值、特征向量或特征图。一个使用加法的集成示例可以在[[85](#bib.bib85)]中找到，作者在多视图融合模块中将所有特征相加。逐元素求和也在[[137](#bib.bib137),
    [266](#bib.bib266)]中实现。如[[262](#bib.bib262)]中的残差融合模块所示，来自LiDAR和相机的多个特征图通过逐元素均值融合在一起。加法和均值的一个缺点是它们以相等的重要性组合不同的数据源。加权和使得可以为不同的输入赋予不同的权重。在[[246](#bib.bib246)]模型的深度图精化阶段，深度图和不确定性图根据加权方案融合。在[[136](#bib.bib136)]中，作者为每对LiDAR和图像特征预测一对权重，从而可以根据信息的可靠性进行集成。在[[100](#bib.bib100)]中，分数图在合并块中生成作为权重，用于融合特征图以及融合所有视图图像中的重建体素。
- en: 'Addition-similar operations are one of the simplest way for integrating features.
    While these operations are simple to implement, it is required that the two parties
    to be added to have the same spatial size and depth, i.e. to be in the exactly
    same format. The format of the output after these operations is also the same
    as the format of each input side. This is an even harder constraint comparing
    with concatenation: two feature maps can be concatenated if they can be added,
    otherwise not. This enables some researchers to compare addition and concatenation
    in their integration models [[121](#bib.bib121), [217](#bib.bib217), [222](#bib.bib222)].
    Another disadvantage of addition and average mean is that both parties to be combined
    have same weights, no matter how confident the respective data are. This may cause
    problems when one sensor fails to work or has low data quality. Also, as raised
    in [[14](#bib.bib14)], while addition and average mean help to achieve high average
    precision, the network may fail in corner cases thus have low robustness.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 加法相似操作是集成功能的最简单方法之一。虽然这些操作实现起来简单，但要求被加的两个部分具有相同的空间大小和深度，即必须完全相同的格式。这些操作后的输出格式也与每个输入侧的格式相同。这相比于拼接是一种更严格的约束：两个特征图可以拼接，如果它们可以相加，否则不可以。这使得一些研究人员能够在他们的集成模型中*比较*加法和拼接[[121](#bib.bib121),
    [217](#bib.bib217), [222](#bib.bib222)]。加法和均值的另一个缺点是被组合的两方具有相同的权重，无论各自数据的可靠性如何。这可能在某个传感器故障或数据质量低时导致问题。此外，正如[[14](#bib.bib14)]中提到的，虽然加法和均值有助于实现高平均精度，但网络在极端情况下可能表现不佳，因此*鲁棒性*较低。
- en: V-4 Probabilistic methods
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-4 概率方法
- en: Probabilistic integration methods incorporate uncertainties into the integration
    process in different ways. Berrio et al. [[118](#bib.bib118)] assign a vector
    to represent probabilities of different semantic classes to each LiDAR point when
    doing back-projecting. Another segmentation research [[267](#bib.bib267)] also
    uses a probabilistic method to help select the optimal segmentation result from
    independent outputs of image and LiDAR. [[89](#bib.bib89)] and [[71](#bib.bib71)]
    utilize Bayesian rules to obtain the confidence of object detection after integration.
    [[255](#bib.bib255), [111](#bib.bib111), [221](#bib.bib221)] leverage Joint Probabilistic
    Data Association (JPDA) filter based on Kalman Filter (KF)methods for system matching
    and state updating in tracking. Different from KF methods that are usually used
    to update the status of a single track given an observation, JPDA takes all measurements
    and tracks in the scene into account with joint probabilities in order to obtain
    a better estimation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 概率集成方法以不同方式将不确定性纳入集成过程。Berrio等人[[118](#bib.bib118)]在反向投影时为每个LiDAR点分配一个向量来表示不同语义类别的概率。另一项分割研究[[267](#bib.bib267)]也使用概率方法帮助从图像和LiDAR的独立输出中选择最佳分割结果。[[89](#bib.bib89)]和[[71](#bib.bib71)]利用贝叶斯规则在集成后获得对象检测的信心。[[255](#bib.bib255)、[111](#bib.bib111)、[221](#bib.bib221)]利用基于卡尔曼滤波器（KF）方法的联合概率数据关联（JPDA）滤波器进行系统匹配和状态更新。与通常用于根据观察更新单个轨迹状态的KF方法不同，JPDA考虑场景中的所有测量和轨迹，并以联合概率进行处理，以获得更好的估计。
- en: Incorporating uncertainties in the integration pipeline is essential for autonomous
    driving applications. This mechanism helps the system to assess risks and adjust
    confidences of choosing trustworthy sensory information for decision making. With
    this, the integration system can be more robust to different external environment
    and varying sensor data quality. However, they also have more parameters to be
    estimated and thus are usually applied to comparatively simple models.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成管道中引入不确定性对于自动驾驶应用至关重要。该机制帮助系统评估风险并调整选择可信感知信息以进行决策的信心。有了这些，集成系统可以对不同的外部环境和变化的传感器数据质量更具鲁棒性。然而，它们也需要估计更多的参数，因此通常应用于相对简单的模型中。
- en: V-5 Rule-based transaction
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-5 基于规则的事务
- en: There are a few researchers adopt handcrafted rules to guide the integration.
    [[248](#bib.bib248)] encodes each detected 2D and 3D bounding box pairs into a
    4-channel tensor, while the 4 elements of the tensor are chosen and designated
    carefully. [[144](#bib.bib144)] presents a method to integrate radar and LiDAR
    measurements/features by replacing low-quality LiDAR points affected by fog with
    radar data. To achieve this, the authors propose a set of manually-designed heuristic
    rules according to different conditions. Wang et al. [[147](#bib.bib147)] generate
    a 7-dimensional frustum by integrating RGB and LiDAR, and 8-dimensional points
    based on manually selected metrics of radar point clouds. Though these integration
    operations are proved to be effective in these papers, they may hardly be implemented
    in another scenario. Comparing with other integration operations, aforementioned
    rule-based ones are dedicatedly designed for one task or dataset, thus have poor
    generalizability.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员采用手工规则来指导集成。[[248](#bib.bib248)]将每个检测到的2D和3D边界框对编码为一个4通道张量，而张量的4个元素被精心选择和指定。[[144](#bib.bib144)]提出了一种通过用雷达数据替换受雾影响的低质量LiDAR点来集成雷达和LiDAR测量/特征的方法。为了实现这一点，作者根据不同条件提出了一组手动设计的启发式规则。王等人[[147](#bib.bib147)]通过集成RGB和LiDAR生成一个7维的圆锥体，并基于手动选择的雷达点云指标生成8维点。尽管这些集成操作在这些论文中被证明有效，但它们在其他场景中可能很难实现。与其他集成操作相比，上述基于规则的方法专门为一个任务或数据集设计，因此通用性较差。
- en: V-6 Temporal integration approaches
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-6 时间集成方法
- en: Non-deep-learning approaches for temporal information integration such as KF
    and relevant extensions (e.g., EKF, Unscented Kalman Filter) have long been the
    mainstream methods for multi-frame data association in localization and tracking
    [[216](#bib.bib216), [148](#bib.bib148), [174](#bib.bib174), [176](#bib.bib176),
    [175](#bib.bib175), [252](#bib.bib252), [111](#bib.bib111), [268](#bib.bib268),
    [154](#bib.bib154), [255](#bib.bib255), [224](#bib.bib224), [115](#bib.bib115),
    [223](#bib.bib223), [152](#bib.bib152), [269](#bib.bib269)]. In general, the pattern
    of tracking with KF methods is to first utilize feature extractor or detector
    to get the information of the object to track, and then employ KF models to integrate
    frames.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 非深度学习方法用于时间信息集成，例如KF及其相关扩展（例如EKF、无迹卡尔曼滤波器），长期以来一直是定位和跟踪中多帧数据关联的主流方法[[216](#bib.bib216)、[148](#bib.bib148)、[174](#bib.bib174)、[176](#bib.bib176)、[175](#bib.bib175)、[252](#bib.bib252)、[111](#bib.bib111)、[268](#bib.bib268)、[154](#bib.bib154)、[255](#bib.bib255)、[224](#bib.bib224)、[115](#bib.bib115)、[223](#bib.bib223)、[152](#bib.bib152)、[269](#bib.bib269)]。一般来说，使用KF方法进行跟踪的模式是首先利用特征提取器或检测器获取待跟踪对象的信息，然后使用KF模型来整合帧。
- en: 'Deep learning approaches with comparatively intuitive and interpretable structure,
    such as RNN family (e.g., RNN, LSTM/ConvLSTM, GRU/ConvGRU) and 3D CNN, are designed
    to leverage temporal information [[270](#bib.bib270), [159](#bib.bib159), [160](#bib.bib160),
    [271](#bib.bib271), [158](#bib.bib158), [113](#bib.bib113), [181](#bib.bib181)].
    Similar to KF, the pattern of temporal integration with RNN models also has two
    stages: first single-frame object detection with various methods, and then temporal
    integration with RNN family models. ODMD proposed by Griffin et al. [[270](#bib.bib270)]
    utilizes 2D object bounding boxes of a target in two frames and corresponding
    camera motion to estimate the target’s depth with LSTM. [[271](#bib.bib271), [159](#bib.bib159),
    [158](#bib.bib158)] integrate information of previous frames and generate depth
    maps with ConvLSTM. DeepVO uses RNN to fuse extracted features [[156](#bib.bib156)],
    and [[171](#bib.bib171), [157](#bib.bib157)] leverage LSTM to integrate optical
    flows or feature maps in time sequence. Besides, a deep learning Visual Inertial
    Odometry (VIO) method uses CNN and LSTM to get image feature map and atures respectively,
    and then conducts temporal integration with LSTM [[206](#bib.bib206)]. Object
    detection and BEV map generation in [[272](#bib.bib272)] employ LSTM to integrate
    temporal information. In [[195](#bib.bib195), [196](#bib.bib196)], BEV map grids
    are also temporally fused and updated with recurrent network designs. 3D CNN method
    is extended from traditional 2D CNN by stacking 2D feature maps at different timestamps
    together and utilizing a 3D convolution kernel to extract and integrate spatio-temporal
    information. Different from RNN models where the spatial and temporal feature
    extractions are conducted in two independent stages, 3D CNN kernels can extract
    and exchange spatial and temporal information simultaneously. This structure has
    been used for video image processing, recognition, and analysis in many different
    realms [[273](#bib.bib273), [274](#bib.bib274), [275](#bib.bib275), [276](#bib.bib276)].
    In autonomous driving area, Qian et al. [[113](#bib.bib113)] propose MVD-Net,
    a 3D CNN network combining LiDAR and radar signals, to detect vehicles via spatio-temporal
    integration. In BEVerse [[199](#bib.bib199)], BEV features generated at consecutive
    timestamps are warpped and aligned to form a stack of temporal block, and fused
    with 3D convolutions.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法，如RNN系列（例如，RNN、LSTM/ConvLSTM、GRU/ConvGRU）和3D CNN，具有相对直观和可解释的结构，旨在利用时间信息[[270](#bib.bib270),
    [159](#bib.bib159), [160](#bib.bib160), [271](#bib.bib271), [158](#bib.bib158),
    [113](#bib.bib113), [181](#bib.bib181)]。类似于KF，RNN模型的时间整合模式也分为两个阶段：首先使用各种方法进行单帧物体检测，然后使用RNN系列模型进行时间整合。Griffin等人提出的ODMD[[270](#bib.bib270)]利用两帧中的目标2D边界框和相应的相机运动，通过LSTM估计目标的深度。[[271](#bib.bib271),
    [159](#bib.bib159), [158](#bib.bib158)]将前帧的信息整合起来，并用ConvLSTM生成深度图。DeepVO使用RNN融合提取的特征[[156](#bib.bib156)]，[[171](#bib.bib171),
    [157](#bib.bib157)]则利用LSTM在时间序列中整合光流或特征图。此外，一种深度学习视觉惯性里程计（VIO）方法使用CNN和LSTM分别获取图像特征图和特征，然后通过LSTM进行时间整合[[206](#bib.bib206)]。在[[272](#bib.bib272)]中，物体检测和BEV地图生成使用LSTM来整合时间信息。在[[195](#bib.bib195),
    [196](#bib.bib196)]中，BEV地图网格也通过递归网络设计进行时间融合和更新。3D CNN方法通过将不同时间戳的2D特征图堆叠在一起，并利用3D卷积核提取和整合时空信息，扩展了传统2D
    CNN的应用。与在两个独立阶段进行空间和时间特征提取的RNN模型不同，3D CNN卷积核可以同时提取和交换空间与时间信息。这种结构已被广泛用于视频图像处理、识别和分析[[273](#bib.bib273),
    [274](#bib.bib274), [275](#bib.bib275), [276](#bib.bib276)]。在自动驾驶领域，Qian等人[[113](#bib.bib113)]提出了MVD-Net，这是一种结合LiDAR和雷达信号的3D
    CNN网络，通过时空整合来检测车辆。在BEVerse[[199](#bib.bib199)]中，生成于连续时间戳的BEV特征被变换和对齐以形成时间块堆栈，并通过3D卷积进行融合。
- en: KF and its variants, RNN families, and 3D CNN are commonly employed specifically
    for temporal data association and integration, yet they have their own limitations.
    KF methods heavily rely on the manually designed process model and the estimation
    of covariance matrix, which may not be appropriate or accurate. RNN methods tend
    to separate spatial and temporal integration into two independent stages. Though
    this order brings conveniences for model tuning, whether it is the optimal spatial-temporal
    integration paradigm still remains in doubt. Another disadvantage of RNN family
    models is that they usually have poor capability to track the long-term dependency
    and are difficult to train and converge. 3D CNN models integrate spatial-temporal
    information at the same time. However, dense 3D convolution networks usually have
    large number of parameters to be estimated, which may easily result in over-fitting
    when the training set is not large enough, or limited spatial and temporal reception
    fields if the kernel size is small.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: KF 及其变体、RNN 家族和 3D CNN 通常用于时间数据关联和集成，但它们也有各自的局限性。KF 方法严重依赖于手动设计的过程模型和协方差矩阵的估计，这可能不合适或不准确。RNN
    方法往往将空间和时间集成分成两个独立的阶段。虽然这种顺序为模型调优带来了便利，但是否是最佳的空间-时间集成范式仍然存疑。RNN 家族模型的另一个缺点是它们通常对长期依赖的跟踪能力较差，并且训练和收敛困难。3D
    CNN 模型同时集成了空间和时间信息。然而，密集的 3D 卷积网络通常需要估计大量的参数，这可能会导致在训练集不够大时过拟合，或者在核大小较小时空间和时间接收场有限。
- en: V-7 Encoder-decoder methods
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-7 编码器-解码器方法
- en: Many researchers exploit neural networks or encoder-decoder architecture models
    to integrate multi-view, multi-modality, and multi-frame sensory data. CNN network
    and its derivations are the most common neural networks used for feature extraction
    and integration. These methods are data driven, which means that they do not rely
    on handcraft-designed models. The kernels and convolution layers by nature can
    integrate spatial information of the input data. In many cases, these networks
    utilize well-designed combinations and procedure of multiple aforementioned integration
    operations and convolution layers [[137](#bib.bib137)]. In [[135](#bib.bib135)],
    the authors first use different sub-networks to extract features from image and
    proposal regions from multiple LiDAR view-representations and the original 3D
    point cloud. Then the proposals are projected into image tensors at multiple layers
    of their neural network. Caltagirone et al. [[117](#bib.bib117)] propose an innovative
    cross-fusion network architecture, in which the two input branches (2D RGB and
    LiDAR images) are connected by trainable scalar cross connections. Specifically,
    at each layer, the input feature tensors of LiDAR and camera are added together
    with learnable weights. [[146](#bib.bib146)] introduces Bayesian Neural Network
    along with concatenation operations in each layer to capture the uncertainties
    with the cost of almost doubled number of parameters to be estimated.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究人员利用神经网络或编码器-解码器架构模型来集成多视角、多模态和多帧传感数据。CNN 网络及其变体是最常用的神经网络，用于特征提取和集成。这些方法是数据驱动的，意味着它们不依赖于手工设计的模型。卷积核和卷积层本质上可以集成输入数据的空间信息。在许多情况下，这些网络利用了精心设计的组合和多次集成操作和卷积层的过程
    [[137](#bib.bib137)]。在 [[135](#bib.bib135)] 中，作者首先使用不同的子网络从多种 LiDAR 视图表示和原始 3D
    点云中提取图像和提议区域的特征。然后，这些提议被投影到其神经网络的多个层中的图像张量中。Caltagirone 等人 [[117](#bib.bib117)]
    提出了一个创新的交叉融合网络架构，其中两个输入分支（2D RGB 和 LiDAR 图像）通过可训练的标量交叉连接连接在一起。具体而言，在每一层中，LiDAR
    和相机的输入特征张量与可学习的权重相加。[[146](#bib.bib146)] 介绍了贝叶斯神经网络以及每层中的连接操作，以捕捉不确定性，但代价是几乎翻倍的参数估计数量。
- en: Though CNN models perform well in their respective fields, there are limitations
    for data integration. Firstly, they are also subject to the shortcomings of the
    integration operations included in the model structure. That is to say, for instance,
    a CNN model with concatenation operation in the network still suffers from the
    weakness of concatenation. Secondly, the mechanism of CNN and their limitations
    in the receptive field hinder its ability to perceive and integrate global information.
    Furthermore, the model procedure and structure are unchanged in application, which
    means that the integration is not adaptive to the changing environment.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CNN 模型在各自的领域表现良好，但在数据集成方面仍存在局限性。首先，它们也受到模型结构中集成操作缺陷的影响。换句话说，例如，具有连接操作的 CNN
    模型仍然会受到连接操作的弱点影响。其次，CNN 的机制及其在感受野中的局限性阻碍了其感知和集成全球信息的能力。此外，模型过程和结构在应用中保持不变，这意味着集成无法适应变化的环境。
- en: Besides CNNs, an increasing amount of researchers set their sights on attention-based
    models (e.g., Transformer) for feature combination. Chen et al. [[192](#bib.bib192)]
    integrates camera, LiDAR, and radar data with Transformer for 3D object detection.
    A set of queries encoding 3D locations can be projected to the corresponding input
    space of each sensor and get their features. DETR3D [[88](#bib.bib88)] proposes
    a 3D object detector utilizing Transformer to integrate multi-view image information.
    LoGoNet [[140](#bib.bib140)] integrate multi-modality data with attention blocks
    by setting LiDAR features as Q (query) and image features as K (key) and V (value).
    In the radar-guided fusion transformer (RGFT) module of MVFusion [[190](#bib.bib190)],
    cross-attention mechanism is introduced to fuse radar and image features. [[76](#bib.bib76)]
    leverages two Transformer decoder layers to produce 3D bounding boxes with queries
    that associate both LiDAR BEV features and image features. BEVSegFormer adopts
    BEV queries to combine multi-camera features in cross-attention module [[101](#bib.bib101)]
    to obtain BEV segmentation results. RI-Fusion [[132](#bib.bib132)] presents an
    attention module that merges range view feature derived from LiDAR data and RGB
    image feature. Specifically, the former is transformed into Q while the latter
    is converted to K and V. FrustumFormer [[201](#bib.bib201)] designs both scene
    queries and instance queries to transform multi-scale multi-view image features
    to a unified BEV feature with cross-attention layers.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 CNN 之外，越来越多的研究者将目光投向了基于注意力的模型（如 Transformer）用于特征组合。陈等人 [[192](#bib.bib192)]
    利用 Transformer 将相机、LiDAR 和雷达数据集成用于 3D 物体检测。一组编码 3D 位置的查询可以被投影到每个传感器的对应输入空间并获取其特征。DETR3D
    [[88](#bib.bib88)] 提出了一个利用 Transformer 集成多视角图像信息的 3D 物体检测器。LoGoNet [[140](#bib.bib140)]
    通过设置 LiDAR 特征为 Q（查询），图像特征为 K（键）和 V（值），使用注意力块集成多模态数据。在 MVFusion [[190](#bib.bib190)]
    的雷达引导融合 Transformer (RGFT) 模块中，引入了交叉注意力机制来融合雷达和图像特征。[[76](#bib.bib76)] 利用两个 Transformer
    解码器层，通过与 LiDAR BEV 特征和图像特征相关联的查询生成 3D 边界框。BEVSegFormer 采用 BEV 查询，通过交叉注意力模块 [[101](#bib.bib101)]
    将多相机特征结合以获得 BEV 分割结果。RI-Fusion [[132](#bib.bib132)] 提出了一个注意力模块，将源自 LiDAR 数据的范围视图特征与
    RGB 图像特征合并。具体来说，前者被转化为 Q，而后者被转换为 K 和 V。FrustumFormer [[201](#bib.bib201)] 设计了场景查询和实例查询，将多尺度多视角图像特征通过交叉注意力层转换为统一的
    BEV 特征。
- en: In addition, recent trials in various domains start focusing on Transformer
    to integrate spatial and temporal information [[277](#bib.bib277), [278](#bib.bib278),
    [279](#bib.bib279), [193](#bib.bib193), [220](#bib.bib220), [102](#bib.bib102),
    [280](#bib.bib280)]. Theoretically, Transformer have unlimited receptive fields
    and can obtain global information with the Q, K, and V structure. BEVFormer [[193](#bib.bib193)]
    integrate camera image multi-view and multi-frame information with Transformer
    architecture. Decoder of BEVFormer has a temporal self-attention layer where BEV
    queries can interact with historical BEV features, and a spatial cross-attention
    layer where BEV queries interact with features of other camera views. [[220](#bib.bib220)]
    is another example leveraging Transformer to integrate spatial-temporal information.
    The authors process camera and LiDAR features with cross-sensor point-wise attention,
    and integrate grid-wise features from BEV maps obtained at different timestamps
    as 4D tensors. [[161](#bib.bib161), [162](#bib.bib162), [163](#bib.bib163)] propose
    different designs of cross-frame query-key structures for multi-object tracking
    over time given image sequences as inputs. Study [[201](#bib.bib201)] utilize
    deformable cross-attention to aggregate information of history queries into current
    instance queries. Comparing with CNN, Transformer can has significantly larger,
    or global, receptive field. Its flexible model design also makes it possible to
    applied to a wide range of applications. Furthermore, Transformer is also scalable
    to model any data size. Though models utilizing Transformer structure tend to
    be larger and harder to train, its advantages of high structure design freedom
    and cross-domain information interaction architecture make it very suitable for
    data integration. We believe that more diverse Transformer-based spatial-temporal
    integration designs will spring up in the near future.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，近期在各领域的试验开始集中于Transformer，以整合空间和时间信息[[277](#bib.bib277), [278](#bib.bib278),
    [279](#bib.bib279), [193](#bib.bib193), [220](#bib.bib220), [102](#bib.bib102),
    [280](#bib.bib280)]。理论上，Transformer具有无限的感受野，并可以通过Q、K和V结构获取全局信息。BEVFormer[[193](#bib.bib193)]将摄像头图像的多视角和多帧信息与Transformer架构整合在一起。BEVFormer的解码器具有一个时间自注意力层，其中BEV查询可以与历史BEV特征进行交互，以及一个空间交叉注意力层，其中BEV查询与其他摄像头视角的特征进行交互。[[220](#bib.bib220)]是另一个利用Transformer整合空间-时间信息的例子。作者通过跨传感器点级注意力处理摄像头和LiDAR特征，并将来自不同时间戳的BEV地图中的网格特征整合为4D张量。[[161](#bib.bib161),
    [162](#bib.bib162), [163](#bib.bib163)]提出了不同的跨帧查询-键结构，用于在给定图像序列的情况下进行多目标跟踪。研究[[201](#bib.bib201)]利用可变形的交叉注意力将历史查询的信息聚合到当前实例查询中。与CNN相比，Transformer具有显著更大的或全局的感受野。其灵活的模型设计也使其可以应用于广泛的应用场景。此外，Transformer还具有对任意数据规模的可扩展性。尽管利用Transformer结构的模型往往更大且训练更困难，但其高结构设计自由度和跨领域信息交互架构的优势使其非常适合数据整合。我们相信，未来将会出现更多多样化的基于Transformer的空间-时间整合设计。
- en: VI Case Study
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 案例研究
- en: '| Camera | Number of sensors | Working range (meters) | Applicable scenarios
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 摄像头 | 传感器数量 | 工作范围（米） | 适用场景 |'
- en: '| Rearview | 1 | 50 | Objects in the back |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 后视 | 1 | 50 | 后方物体 |'
- en: '| Wide forward | 1 | 60 | Traffic lights, short-distance or crossing objects
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 宽前方 | 1 | 60 | 交通信号灯、短距离或穿越物体 |'
- en: '| Forward-looking side | 2 | 80 | Neighboring or crossing objects |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 前向侧面 | 2 | 80 | 邻近或穿越物体 |'
- en: '| Rearward-looking side | 2 | 100 | Objects in the neighboring lanes behind
    the car |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 后向侧面 | 2 | 100 | 车后邻近车道中的物体 |'
- en: '| Main forward | 1 | 150 | Most cases |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 主要前方 | 1 | 150 | 大多数情况 |'
- en: '| Narrow forward | 1 | 250 | Distant objects |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 狭窄前方 | 1 | 250 | 远距离物体 |'
- en: 'TABLE III: The cameras adopted by Tesla.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：特斯拉采用的摄像头。
- en: Due to the prosperity and rapid development of autonomous driving technology
    in recent years, many companies, such as Tesla, Waymo, and General Motors, have
    launched ADS-related products and services via different technical paths. In this
    section, we take Tesla’s vision structure as an example to illustrate data integration
    applications in real-world ADS systems with the “what-when-how” analysis structure
    we described previously.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于近年来自动驾驶技术的繁荣和快速发展，许多公司如特斯拉、Waymo和通用汽车，通过不同的技术路径推出了与ADS相关的产品和服务。在这一部分，我们以特斯拉的视觉结构为例，利用我们之前描述的“什么-何时-如何”分析结构，说明实际ADS系统中的数据整合应用。
- en: Tesla builds a pure camera-based system, which consists of eight cameras in
    total. Table [III](#S6.T3 "TABLE III ‣ VI Case Study ‣ A survey on deep learning
    approaches for data integration in autonomous driving system") displays the perception
    range of the mounted cameras. These cameras have the field of view overlapped
    with each other and together cover 360° around the car. The three cameras (main/narrow/wide
    forward) installed behind the windshield help to detect distant objects within
    a broad view angle. Specifically, the wide camera, equipping a 120-degree fish-eye
    lens, is helpful in downtown and crossroads where vehicles drive slow. The narrow
    camera is applicable on the highway. Besides the front sensors, there are also
    cameras observing both sides and the rear of vehicles. The forward-looking side
    cameras with a view angle of 90 degrees are mainly used to detect cars that change
    lanes and objects around crossroads. The rearward-looking side cameras assist
    lane changing. The rear-view camera avoids rear-end collisions and facilitates
    parking.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉建立了一个纯摄像头系统，总共有八个摄像头。表 [III](#S6.T3 "TABLE III ‣ VI Case Study ‣ A survey
    on deep learning approaches for data integration in autonomous driving system")
    显示了安装摄像头的感知范围。这些摄像头的视野互有重叠，共同覆盖车辆周围的360°。安装在挡风玻璃后方的三台摄像头（主/窄/宽前向）有助于在宽视角下检测远处物体。具体而言，配备120度鱼眼镜头的宽摄像头，在市区和交叉路口车辆缓慢行驶时特别有用。窄摄像头适用于高速公路。除了前方传感器，还有摄像头观察车辆的两侧和后方。前向侧摄像头的视角为90度，主要用于检测变道的车辆和交叉路口周围的物体。后向侧摄像头辅助变道。后视摄像头避免追尾碰撞并方便停车。
- en: All the cameras have different directions and are capturing images at certain
    frequencies, so that the integration of Tesla’s autonomous driving system can
    be regarded as multi-view and multi-frame integration.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 所有摄像头的方向各异，并以特定频率捕捉图像，因此特斯拉的自动驾驶系统的集成可以视为多视角和多帧集成。
- en: 'Similar to the approach we described earlier, Tesla splits multi-view multi-frame
    integration into two steps: first, multi-view spatial integration, and then multi-frame
    temporal integration. Integration between different views and frames is done at
    the feature level. After raw image calibration and multi-scale feature extraction,
    a transformer-like neural network is used to fuse features from different views.
    Specifically, the key and value of the transformer come from the features of the
    images, while the query comes from the position encoding of each point in the
    occupancy space. The occupancy space mentioned here is similar to the BEV space,
    which is a top-down view, but with an additional dimension of height. Each point
    in the occupancy space queries its most relevant corresponding object in other
    views through the attention mechanism. After aggregating the spatial information,
    the model aligns the positions of objects at different times using displacement
    information provided by the odometer for temporal fusion, with features from nearby
    time steps having higher weights. The spatiotemporal features are then passed
    through a set of deconvolutions to obtain high-resolution 3D features in the occupancy
    space for further decision-making.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前描述的方法类似，特斯拉将多视角多帧集成分为两步：首先是多视角空间集成，其次是多帧时间集成。不同视角和帧之间的集成在特征层面进行。在原始图像校准和多尺度特征提取后，使用类似变换器的神经网络融合来自不同视角的特征。具体来说，变换器的键和值来自图像的特征，而查询来自占据空间中每个点的位置编码。这里提到的占据空间类似于BEV空间，它是一种自上而下的视图，但具有额外的高度维度。占据空间中的每个点通过注意机制查询其在其他视角中最相关的对应物体。聚合空间信息后，模型利用里程计提供的位移信息对不同时间的物体位置进行对齐，以进行时间融合，附近时间步的特征具有更高的权重。然后，时空特征通过一组反卷积操作获得占据空间中的高分辨率3D特征，以便进一步决策。
- en: VII Conclusion and Discussion
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论与讨论
- en: 'In this paper, we review the latest deep learning-based data integration techniques
    for autonomous driving perception, focusing on three types of sensors: cameras,
    LiDAR, and radar. The need for data integration arises from the complementary
    nature of perception capabilities among these sensors. We present a unique perspective
    on integration techniques by examining “what, when, and how to integrate.” Under
    “what to integrate,” we propose a novel taxonomy that classifies data integration
    inputs into seven categories based on three dimensions (multi-view, multi-modality,
    and multi-frame). We observe that most existing research focuses on one-dimensional
    integration, while two-dimensional integration is slowly gaining interest. For
    “when to integrate,” we categorize data integration techniques based on data abstraction
    levels. In “how to integrate,” we not only discuss commonly used data/feature
    combination operations but also comprehensively outline their advantages and drawbacks,
    which is seldom addressed in previous studies. Furthermore, we demonstrate data
    integration techniques using Tesla’s ADS as an example.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了基于深度学习的自动驾驶感知数据集成技术，重点关注三种传感器：摄像头、激光雷达和雷达。数据集成的需求源于这些传感器之间感知能力的互补性。我们通过考察“集成什么、何时集成以及如何集成”提出了对集成技术的独特视角。在“集成什么”方面，我们提出了一种新颖的分类法，将数据集成输入按三个维度（多视角、多模态和多帧）划分为七类。我们观察到，大多数现有研究集中在一维集成上，而二维集成则逐渐受到关注。关于“何时集成”，我们根据数据抽象层次对数据集成技术进行分类。在“如何集成”方面，我们不仅讨论了常用的数据/特征组合操作，还全面概述了它们的优缺点，这在以往研究中较少涉及。此外，我们还以特斯拉的自动驾驶系统为例演示了数据集成技术。
- en: 'By examining related works published within the past five years, we identify
    several patterns or issues in current integration methodologies:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考察过去五年内发表的相关工作，我们识别出了当前集成方法中的若干模式或问题：
- en: '1) Spatial alignment based on explicit projection: As discussed in Section [V-1](#S5.SS0.SSS1
    "V-1 Projection ‣ V Data Integration: How to Integrate ‣ A survey on deep learning
    approaches for data integration in autonomous driving system"), projection and
    back-projection are two commonly used integration methods for spatially aligning
    data from different sensors. These methods rely on the geometric relationship
    between sensory measurements and intrinsic and extrinsic parameters for mutual
    projection. However, due to dimension and perspective differences between sensory
    measurements and estimation errors in the transformation matrix, these approaches
    can lead to information loss, redundancy, or cumulative error.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基于显式投影的空间对齐：如第[V-1](#S5.SS0.SSS1 "V-1 投影 ‣ V 数据集成：如何集成 ‣ 自动驾驶系统数据集成深度学习方法综述")节所述，投影和反投影是两种常用的空间对齐数据集成方法。这些方法依赖于感测测量值与内外参之间的几何关系进行相互投影。然而，由于感测测量值与转换矩阵之间的维度和视角差异以及估计误差，这些方法可能导致信息丢失、冗余或累积误差。
- en: '2) High specificity but low generalizability: Many existing data integration
    methods/algorithms are designed for specific scenarios, making them sensor-specific,
    data-representation-specific, and task-specific. While this high specificity contributes
    to improved perception performance in their respective domains, it also results
    in low generalizability, limiting the applicability of each integration technique.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 高特异性但低通用性：许多现有的数据集成方法/算法是为特定场景设计的，使它们在传感器、数据表示和任务上都具有特异性。虽然这种高特异性有助于提高各自领域的感知性能，但也导致了低通用性，限制了每种集成技术的适用性。
- en: '3) Fixed integration architecture with limited flexibility: Most existing integration
    techniques have fixed structures, which means that the weights, orders, dimensions,
    formats, and depths of the integration models usually do not change once a neural
    network is trained and ready for implementation. This property makes the model
    unable to handle some common practical situations, such as unavailability of data
    from one or several sensors (not sensor pluggable) or changes in the order of
    the input (not input permutation invariant). The unchangeable network is also
    difficult to scale from simple to complex applications, for example, extending
    from two-frame integration to multiple consecutive frames. Moreover, these integration
    architectures cannot automatically adapt to external changes and select the best
    integration method by adjusting their structure and weights.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 固定集成架构与有限灵活性：大多数现有的集成技术有固定的结构，这意味着一旦神经网络训练完成并准备实施后，集成模型的权重、顺序、维度、格式和深度通常不会改变。这一属性使得模型无法处理一些常见的实际情况，如数据缺失（不支持传感器插拔）或输入顺序的变化（不支持输入排列不变）。不可更改的网络也难以从简单的应用扩展到复杂的应用，例如，从两帧集成扩展到多个连续帧。此外，这些集成架构不能自动适应外部变化，通过调整结构和权重选择最佳集成方法。
- en: 'Comprehensive understanding of the aforementioned shortcomings sheds light
    on required properties of “ideal” approach for data integration:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对上述不足的全面理解揭示了数据集成“理想”方法所需的属性：
- en: '1) Task- and modality-agnostic: Developing high-specificity individual integration
    methods for every scenario and task can result in redundant computation. Therefore,
    integration models that can be generalized and applied to a wide range of application
    scenarios is a future direction of development. Though existing research have
    provided much experience of integration, their nature of task-, representation-,
    and modality-specificity may also bring limitations when generalizing to other
    applications.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 任务和模态无关：为每种场景和任务开发高特定性的单独集成方法可能会导致冗余计算。因此，可以广泛应用于各种应用场景的通用集成模型是未来的发展方向。尽管现有研究提供了许多集成经验，但它们任务、表示和模态的特定性质在推广到其他应用时可能也会带来限制。
- en: Developing Task- and modality-agnostic integration approaches, referring to
    data integration techniques without assumptions based on existing heuristic knowledge
    of sensors or different perception tasks, are essential for multi-view, multi-modality,
    and multi-frame integration. Specifically, in task- and modality-agnostic integration,
    the input data representations may not be described by one type of sensors or
    another, and the integration method may not be determined by one task.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 发展任务和模态无关的集成方法，即指不依赖于现有传感器或不同感知任务的启发式知识的数据集成技术，对于多视角、多模态和多帧集成至关重要。特别是在任务和模态无关的集成中，输入数据表示可能无法由某一种传感器描述，且集成方法可能不会由单一任务决定。
- en: '2) Sensor pluggable, permutation invariant, and spatial-temporal scalability:
    In practical applications, information missing may happen to a sensor for various
    reasons. Camera may not be able to take pictures if its lens is temporarily covered
    or contaminated. However, the perception module of the autonomous driving system
    can not fail due to this. An ideal integration approach is expected to provide
    comparatively high-quality integration outcomes even when a portion of information
    is lost. This also means that the data integration approach should be able to
    process data coming from different sensor configurations. In other words, the
    sensors are pluggable for the integration approach.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 传感器可插拔、排列不变和时空可扩展：在实际应用中，由于各种原因，传感器可能会发生信息丢失。如果相机镜头暂时被遮挡或污染，可能无法拍照。然而，自动驾驶系统的感知模块不能因此而失败。理想的集成方法应在部分信息丢失的情况下仍能提供较高质量的集成结果。这也意味着数据集成方法应能够处理来自不同传感器配置的数据。换句话说，传感器对集成方法是可插拔的。
- en: To be able to process data coming from different sensor combinations, the integration
    approach needs to be invariant to input permutation, which means that the integration
    and perception performance should not be affected by the order of the inputs (e.g.,
    not affected by whether LiDAR data or image data arrives first).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理来自不同传感器组合的数据，集成方法需要对输入排列不变，这意味着集成和感知性能不应受到输入顺序的影响（例如，不受LiDAR数据或图像数据先后到达的影响）。
- en: Spatial-temporal scalability describes the capability of the integration method
    to adapt to increased demand both in space and time. Specifically, this refers
    to the ability of integrating a larger amount of data from more sensors, and integrating
    temporal information from more frames in a wider time window. This capability
    enables self-driving vehicles to deal with complex driving scenarios, in which
    the demand for processing spatial-temporal data is high.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 空间-时间可扩展性描述了集成方法适应空间和时间需求增加的能力。具体而言，这指的是集成来自更多传感器的大量数据以及在更宽时间窗口中集成更多帧的时间信息的能力。这种能力使得自动驾驶车辆能够处理复杂的驾驶场景，其中对空间-时间数据的处理需求较高。
- en: '3) Adaptive: Traffic conditions are changing all the time with their own levels
    of complexity. For example, car following on highways with comparatively simple
    traffic scenes and limited road elements can be processed without complicated
    data integration, while driving through complex and crowded intersections in cities
    may require much more information to process for decision making. Handling perception
    demands of different scenes by integration data in the same way may lead to unnecessary
    computational consumption or errors due to under-computation. Thus, another important
    property of dynamic integration approach is adaptiveness.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 自适应：交通状况随着复杂程度不断变化。例如，高速公路上的车距跟随，交通场景相对简单且道路元素有限，可以在没有复杂数据集成的情况下处理，而在城市复杂且拥挤的交叉口行驶则可能需要处理更多信息以进行决策。以相同方式集成数据来处理不同场景的感知需求可能会导致不必要的计算消耗或由于计算不足而产生错误。因此，动态集成方法的另一个重要属性是自适应性。
- en: The ideal integration approach is expected to adaptively learn the demands of
    perception according to the complexity of the scene, and thus adjust its structure,
    i.e. activating different neurons, to fit the requirement of different driving
    environments. In low-complexity conditions where part of sensory measurements
    is not needed, these sensors can be “unplugged” from the integration processing
    (not integrated for simplicity). In temporal integration, the “ideal” integration
    approach can also select the appropriate fusion window or key frame according
    to the importance of the previous frames.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的集成方法应根据场景的复杂性自适应地学习感知需求，从而调整其结构，即激活不同的神经元，以适应不同驾驶环境的要求。在低复杂性条件下，某些传感器的测量可能是不需要的，这些传感器可以从集成处理中“断开”
    （为了简化起见不集成）。在时间集成中，“理想”的集成方法还可以根据前一帧的相对重要性选择合适的融合窗口或关键帧。
- en: Acknowledgments
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank Jiaheng Yang from Riemann Laboratory, Huawei
    Technologies, for his inspiring discussions. The authors also thank Qirui Wang
    from Parallel Distributed Computing Laboratory, Huawei Technologies, for collecting
    literature.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢华为技术有限公司Riemann实验室的Jiaheng Yang提供的启发性讨论。作者还感谢华为技术有限公司并行分布式计算实验室的Qirui Wang在文献收集方面的帮助。
- en: References
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Janai, F. Güney, A. Behl, A. Geiger *et al.*, “Computer vision for autonomous
    vehicles: Problems, datasets and state of the art,” *Foundations and Trends® in
    Computer Graphics and Vision*, vol. 12, no. 1–3, pp. 1–308, 2020.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Janai, F. Güney, A. Behl, A. Geiger *等*, “计算机视觉在自主车辆中的应用：问题、数据集和现状，”
    *计算机图形学与视觉基础及趋势®*, 卷 12, 号 1–3, 页 1–308, 2020。'
- en: '[2] J. Fayyad, M. A. Jaradat, D. Gruyer, and H. Najjaran, “Deep learning sensor
    fusion for autonomous vehicle perception and localization: A review,” *Sensors*,
    vol. 20, no. 15, p. 4220, 2020.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. Fayyad, M. A. Jaradat, D. Gruyer, 和 H. Najjaran, “深度学习传感器融合用于自主车辆感知和定位：综述，”
    *传感器*, 卷 20, 号 15, 页 4220, 2020。'
- en: '[3] S. Chen, Z. Jian, Y. Huang, Y. Chen, Z. Zhou, and N. Zheng, “Autonomous
    driving: cognitive construction and situation understanding,” *Science China Information
    Sciences*, vol. 62, no. 8, pp. 1–27, 2019.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] S. 陈, Z. 鉴, Y. 黄, Y. 陈, Z. 周, 和 N. 郑, “自主驾驶：认知构建与情境理解，” *中国科学信息科学*, 卷 62,
    号 8, 页 1–27, 2019。'
- en: '[4] K. Huang, B. Shi, X. Li, X. Li, S. Huang, and Y. Li, “Multi-modal sensor
    fusion for auto driving perception: A survey,” *arXiv preprint arXiv:2202.02703*,
    2022.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] K. Huang, B. Shi, X. Li, X. Li, S. Huang, 和 Y. Li，“自动驾驶感知的多模态传感器融合：综述”，*arXiv预印本
    arXiv:2202.02703*，2022年。'
- en: '[5] C. Chen, B. Wang, C. X. Lu, N. Trigoni, and A. Markham, “A survey on deep
    learning for localization and mapping: Towards the age of spatial machine intelligence,”
    *arXiv preprint arXiv:2006.12567*, 2020.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] C. Chen, B. Wang, C. X. Lu, N. Trigoni, 和 A. Markham，“深度学习在定位和地图构建中的综述：迈向空间机器智能的时代”，*arXiv预印本
    arXiv:2006.12567*，2020年。'
- en: '[6] S. Kuutti, S. Fallah, K. Katsaros, M. Dianati, F. Mccullough, and A. Mouzakitis,
    “A survey of the state-of-the-art localization techniques and their potentials
    for autonomous vehicle applications,” *IEEE Internet of Things Journal*, vol. 5,
    no. 2, pp. 829–846, 2018.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] S. Kuutti, S. Fallah, K. Katsaros, M. Dianati, F. Mccullough, 和 A. Mouzakitis，“前沿定位技术及其在自动驾驶车辆应用中的潜力综述”，*IEEE物联网杂志*，第5卷，第2期，第829–846页，2018年。'
- en: '[7] E. Marti, M. A. De Miguel, F. Garcia, and J. Perez, “A review of sensor
    technologies for perception in automated driving,” *IEEE Intelligent Transportation
    Systems Magazine*, vol. 11, no. 4, pp. 94–108, 2019.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] E. Marti, M. A. De Miguel, F. Garcia, 和 J. Perez，“自动驾驶感知的传感器技术综述”，*IEEE智能交通系统杂志*，第11卷，第4期，第94–108页，2019年。'
- en: '[8] H. A. Ignatious, M. Khan *et al.*, “An overview of sensors in autonomous
    vehicles,” *Procedia Computer Science*, vol. 198, pp. 736–741, 2022.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. A. Ignatious, M. Khan *等*，“自动驾驶车辆中传感器概述”，*Procedia计算机科学*，第198卷，第736–741页，2022年。'
- en: '[9] P. Kolar, P. Benavidez, and M. Jamshidi, “Survey of datafusion techniques
    for laser and vision based sensor integration for autonomous navigation,” *Sensors*,
    vol. 20, no. 8, p. 2180, 2020.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. Kolar, P. Benavidez, 和 M. Jamshidi，“激光和视觉传感器集成的数据信息融合技术综述”，*Sensors*，第20卷，第8期，第2180页，2020年。'
- en: '[10] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *2012 IEEE conference on computer vision
    and pattern recognition*.   IEEE, 2012, pp. 3354–3361.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Geiger, P. Lenz, 和 R. Urtasun，“我们准备好自动驾驶了吗？KITTI视觉基准套件”，在*2012 IEEE计算机视觉与模式识别会议*中。IEEE，2012年，第3354–3361页。'
- en: '[11] Z. Wang, Y. Wu, and Q. Niu, “Multi-sensor fusion in automated driving:
    A survey,” *Ieee Access*, vol. 8, pp. 2847–2868, 2019.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Z. Wang, Y. Wu, 和 Q. Niu，“自动驾驶中的多传感器融合：综述”，*IEEE Access*，第8卷，第2847–2868页，2019年。'
- en: '[12] D. J. Yeong, G. Velasco-Hernandez, J. Barry, J. Walsh *et al.*, “Sensor
    and sensor fusion technology in autonomous vehicles: A review,” *Sensors*, vol. 21,
    no. 6, p. 2140, 2021.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] D. J. Yeong, G. Velasco-Hernandez, J. Barry, J. Walsh *等*，“自动驾驶车辆中的传感器和传感器融合技术：综述”，*Sensors*，第21卷，第6期，第2140页，2021年。'
- en: '[13] T. Morkar, S. Sonawane, A. Mahajan, and S. Shinde, “Autonomous vehicles:
    A survey on sensor fusion, lane detection and drivable area segmentation,” in
    *Computational Intelligence in Data Mining: Proceedings of ICCIDM 2021*.   Springer,
    2022, pp. 695–709.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] T. Morkar, S. Sonawane, A. Mahajan, 和 S. Shinde，“自动驾驶车辆：传感器融合、车道检测和可驾驶区域分割综述”，在*数据挖掘中的计算智能：ICCIDM
    2021会议论文集*中。Springer，2022年，第695–709页。'
- en: '[14] D. Feng, C. Haase-Schütz, L. Rosenbaum, H. Hertlein, C. Glaeser, F. Timm,
    W. Wiesbeck, and K. Dietmayer, “Deep multi-modal object detection and semantic
    segmentation for autonomous driving: Datasets, methods, and challenges,” *IEEE
    Transactions on Intelligent Transportation Systems*, vol. 22, no. 3, pp. 1341–1360,
    2020.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] D. Feng, C. Haase-Schütz, L. Rosenbaum, H. Hertlein, C. Glaeser, F. Timm,
    W. Wiesbeck, 和 K. Dietmayer，“深度多模态目标检测和语义分割在自动驾驶中的应用：数据集、方法和挑战”，*IEEE智能交通系统学报*，第22卷，第3期，第1341–1360页，2020年。'
- en: '[15] L. Wang, X. Zhang, Z. Song, J. Bi, G. Zhang, H. Wei, L. Tang, L. Yang,
    J. Li, C. Jia *et al.*, “Multi-modal 3d object detection in autonomous driving:
    A survey and taxonomy,” *IEEE Transactions on Intelligent Vehicles*, 2023.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] L. Wang, X. Zhang, Z. Song, J. Bi, G. Zhang, H. Wei, L. Tang, L. Yang,
    J. Li, C. Jia *等*，“自动驾驶中的多模态3D目标检测：综述与分类”，*IEEE智能车辆学报*，2023年。'
- en: '[16] Q. Li, J. P. Queralta, T. N. Gia, Z. Zou, and T. Westerlund, “Multi-sensor
    fusion for navigation and mapping in autonomous vehicles: Accurate localization
    in urban environments,” *Unmanned Systems*, vol. 8, no. 03, pp. 229–237, 2020.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Q. Li, J. P. Queralta, T. N. Gia, Z. Zou, 和 T. Westerlund，“自动驾驶车辆中的多传感器融合：城市环境中的精确定位”，*Unmanned
    Systems*，第8卷，第03期，第229–237页，2020年。'
- en: '[17] L. Yu, Y. Duan, and K.-C. Li, “A real-world service mashup platform based
    on data integration, information synthesis, and knowledge fusion,” *Connection
    Science*, vol. 33, no. 3, pp. 463–481, 2021.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] L. Yu, Y. Duan, 和 K.-C. Li，“基于数据集成、信息综合和知识融合的真实世界服务整合平台”，*Connection Science*，第33卷，第3期，第463–481页，2021年。'
- en: '[18] T. Meng, X. Jing, Z. Yan, and W. Pedrycz, “A survey on machine learning
    for data fusion,” *Information Fusion*, vol. 57, pp. 115–129, 2020.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] T. Meng, X. Jing, Z. Yan, 和 W. Pedrycz, “关于数据融合的机器学习综述，” *信息融合*，第57卷，pp.
    115–129, 2020.'
- en: '[19] J. Bleiholder and F. Naumann, “Data fusion,” *ACM computing surveys (CSUR)*,
    vol. 41, no. 1, pp. 1–41, 2009.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Bleiholder 和 F. Naumann, “数据融合，” *ACM计算机调查（CSUR）*，第41卷，第1期，pp. 1–41,
    2009.'
- en: '[20] D. L. Hall and J. Llinas, “An introduction to multisensor data fusion,”
    *Proceedings of the IEEE*, vol. 85, no. 1, pp. 6–23, 1997.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] D. L. Hall 和 J. Llinas, “多传感器数据融合导论，” *IEEE汇刊*，第85卷，第1期，pp. 6–23, 1997.'
- en: '[21] B. Khaleghi, A. Khamis, F. O. Karray, and S. N. Razavi, “Multisensor data
    fusion: A review of the state-of-the-art,” *Information fusion*, vol. 14, no. 1,
    pp. 28–44, 2013.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] B. Khaleghi, A. Khamis, F. O. Karray, 和 S. N. Razavi, “多传感器数据融合：前沿综述，”
    *信息融合*，第14卷，第1期，pp. 28–44, 2013.'
- en: '[22] G. Velasco-Hernandez, J. Barry, J. Walsh *et al.*, “Autonomous driving
    architectures, perception and data fusion: A review,” in *2020 IEEE 16th International
    Conference on Intelligent Computer Communication and Processing (ICCP)*.   IEEE,
    2020, pp. 315–321.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] G. Velasco-Hernandez, J. Barry, J. Walsh *等*，“自动驾驶架构、感知与数据融合：综述，” 收录于
    *2020年IEEE第16届智能计算机通信与处理国际会议（ICCP）*。IEEE, 2020, pp. 315–321.'
- en: '[23] J. Zhou, X. Hong, and P. Jin, “Information fusion for multi-source material
    data: progress and challenges,” *Applied Sciences*, vol. 9, no. 17, p. 3473, 2019.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Zhou, X. Hong, 和 P. Jin, “多源材料数据的信息融合：进展与挑战，” *应用科学*，第9卷，第17期，p. 3473,
    2019.'
- en: '[24] S. Campbell, N. O’Mahony, L. Krpalcova, D. Riordan, J. Walsh, A. Murphy,
    and C. Ryan, “Sensor technology in autonomous vehicles: A review,” in *2018 29th
    Irish Signals and Systems Conference (ISSC)*.   IEEE, 2018, pp. 1–4.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Campbell, N. O’Mahony, L. Krpalcova, D. Riordan, J. Walsh, A. Murphy,
    和 C. Ryan, “自动驾驶汽车中的传感器技术：综述，” 收录于 *2018年第29届爱尔兰信号与系统会议（ISSC）*。IEEE, 2018, pp.
    1–4.'
- en: '[25] D. Gruyer, V. Magnier, K. Hamdi, L. Claussmann, O. Orfila, and A. Rakotonirainy,
    “Perception, information processing and modeling: Critical stages for autonomous
    driving applications,” *Annual Reviews in Control*, vol. 44, pp. 323–341, 2017.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] D. Gruyer, V. Magnier, K. Hamdi, L. Claussmann, O. Orfila, 和 A. Rakotonirainy,
    “感知、信息处理与建模：自动驾驶应用的关键阶段，” *控制年度评论*，第44卷，pp. 323–341, 2017.'
- en: '[26] J. Van Brummelen, M. O’Brien, D. Gruyer, and H. Najjaran, “Autonomous
    vehicle perception: The technology of today and tomorrow,” *Transportation research
    part C: emerging technologies*, vol. 89, pp. 384–406, 2018.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Van Brummelen, M. O’Brien, D. Gruyer, 和 H. Najjaran, “自动驾驶汽车感知：今天与明天的技术，”
    *交通研究C部分：新兴技术*，第89卷，pp. 384–406, 2018.'
- en: '[27] D. Feng, A. Harakeh, S. L. Waslander, and K. Dietmayer, “A review and
    comparative study on probabilistic object detection in autonomous driving,” *IEEE
    Transactions on Intelligent Transportation Systems*, 2021.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] D. Feng, A. Harakeh, S. L. Waslander, 和 K. Dietmayer, “自动驾驶中的概率对象检测：综述与比较研究，”
    *IEEE智能交通系统汇刊*，2021.'
- en: '[28] Y. Cui, R. Chen, W. Chu, L. Chen, D. Tian, Y. Li, and D. Cao, “Deep learning
    for image and point cloud fusion in autonomous driving: A review,” *IEEE Transactions
    on Intelligent Transportation Systems*, 2021.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Cui, R. Chen, W. Chu, L. Chen, D. Tian, Y. Li, 和 D. Cao, “自动驾驶中的图像和点云融合的深度学习：综述，”
    *IEEE智能交通系统汇刊*，2021.'
- en: '[29] Q. Cai, H. Wang, Z. Li, and X. Liu, “A survey on multimodal data-driven
    smart healthcare systems: approaches and applications,” *IEEE Access*, vol. 7,
    pp. 133 583–133 599, 2019.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Q. Cai, H. Wang, Z. Li, 和 X. Liu, “基于多模态数据驱动的智能医疗系统：方法与应用综述，” *IEEE Access*，第7卷，pp.
    133 583–133 599, 2019.'
- en: '[30] M. Z. Uddin, M. M. Hassan, A. Alsanad, and C. Savaglio, “A body sensor
    data fusion and deep recurrent neural network-based behavior recognition approach
    for robust healthcare,” *Information Fusion*, vol. 55, pp. 105–115, 2020.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] M. Z. Uddin, M. M. Hassan, A. Alsanad, 和 C. Savaglio, “基于体感传感器数据融合和深度递归神经网络的行为识别方法，用于稳健的医疗保健，”
    *信息融合*，第55卷，pp. 105–115, 2020.'
- en: '[31] W. Zhang, R. Sengupta, J. Fodero, and X. Li, “Deeppositioning: Intelligent
    fusion of pervasive magnetic field and wifi fingerprinting for smartphone indoor
    localization via deep learning,” in *2017 16th IEEE International Conference on
    Machine Learning and Applications (ICMLA)*.   IEEE, 2017, pp. 7–13.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] W. Zhang, R. Sengupta, J. Fodero, 和 X. Li, “深度定位：通过深度学习智能融合广泛存在的磁场和wifi指纹以实现智能手机室内定位，”
    收录于 *2017年第16届IEEE机器学习与应用国际会议（ICMLA）*。IEEE, 2017, pp. 7–13.'
- en: '[32] T. Liu, H. Lu, and Z. Wei, “Design and implementation of intelligent window
    control system based on multi-sensor fusion,” in *2019 IEEE 8th Data Driven Control
    and Learning Systems Conference (DDCLS)*.   IEEE, 2019, pp. 1368–1372.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] T. Liu, H. Lu, 和 Z. Wei，“基于多传感器融合的智能窗控系统的设计与实施，”在 *2019 IEEE第8届数据驱动控制与学习系统会议
    (DDCLS)*。 IEEE，2019年，第1368–1372页。'
- en: '[33] L. Wu, L. Chen, and X. Hao, “Multi-sensor data fusion algorithm for indoor
    fire early warning based on bp neural network,” *Information*, vol. 12, no. 2,
    p. 59, 2021.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] L. Wu, L. Chen, 和 X. Hao，“基于BP神经网络的室内火灾早期预警多传感器数据融合算法，” *信息*，第12卷，第2期，第59页，2021年。'
- en: '[34] Z. Guo, X. Li, H. Huang, N. Guo, and Q. Li, “Deep learning-based image
    segmentation on multimodal medical imaging,” *IEEE Transactions on Radiation and
    Plasma Medical Sciences*, vol. 3, no. 2, pp. 162–169, 2019.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Z. Guo, X. Li, H. Huang, N. Guo, 和 Q. Li，“基于深度学习的多模态医学影像图像分割，” *IEEE辐射与等离子体医学科学学报*，第3卷，第2期，第162–169页，2019年。'
- en: '[35] M. R. Shahrbabaki, A. A. Safavi, M. Papageorgiou, and I. Papamichail,
    “A data fusion approach for real-time traffic state estimation in urban signalized
    links,” *Transportation research part C: emerging technologies*, vol. 92, pp.
    525–548, 2018.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] M. R. Shahrbabaki, A. A. Safavi, M. Papageorgiou, 和 I. Papamichail，“一种用于城市信号化链路实时交通状态估计的数据融合方法，”
    *交通研究C部分：新兴技术*，第92卷，第525–548页，2018年。'
- en: '[36] S. Du, T. Li, X. Gong, and S.-J. Horng, “A hybrid method for traffic flow
    forecasting using multimodal deep learning,” *arXiv preprint arXiv:1803.02099*,
    2018.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Du, T. Li, X. Gong, 和 S.-J. Horng，“一种使用多模态深度学习的交通流预测混合方法，” *arXiv预印本
    arXiv:1803.02099*，2018年。'
- en: '[37] Z. Huang, X. Ling, P. Wang, F. Zhang, Y. Mao, T. Lin, and F.-Y. Wang,
    “Modeling real-time human mobility based on mobile phone and transportation data
    fusion,” *Transportation research part C: emerging technologies*, vol. 96, pp.
    251–269, 2018.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Z. Huang, X. Ling, P. Wang, F. Zhang, Y. Mao, T. Lin, 和 F.-Y. Wang，“基于手机和交通数据融合的实时人类移动建模，”
    *交通研究C部分：新兴技术*，第96卷，第251–269页，2018年。'
- en: '[38] S. Guo, B. Zhang, T. Yang, D. Lyu, and W. Gao, “Multitask convolutional
    neural network with information fusion for bearing fault diagnosis and localization,”
    *IEEE Transactions on Industrial Electronics*, vol. 67, no. 9, pp. 8005–8015,
    2019.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] S. Guo, B. Zhang, T. Yang, D. Lyu, 和 W. Gao，“用于轴承故障诊断和定位的信息融合多任务卷积神经网络，”
    *IEEE工业电子学报*，第67卷，第9期，第8005–8015页，2019年。'
- en: '[39] A. B. Torres, A. R. da Rocha, T. L. C. da Silva, J. N. de Souza, and R. S.
    Gondim, “Multilevel data fusion for the internet of things in smart agriculture,”
    *Computers and Electronics in Agriculture*, vol. 171, p. 105309, 2020.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] A. B. Torres, A. R. da Rocha, T. L. C. da Silva, J. N. de Souza, 和 R.
    S. Gondim，“智能农业中物联网的多层次数据融合，” *计算机与电子在农业*，第171卷，第105309页，2020年。'
- en: '[40] K. Beard, M. Kimble, J. Yuan, K. S. Evans, W. Liu, D. Brady, and S. Moore,
    “A method for heterogeneous spatio-temporal data integration in support of marine
    aquaculture site selection,” *Journal of Marine Science and Engineering*, vol. 8,
    no. 2, p. 96, 2020.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] K. Beard, M. Kimble, J. Yuan, K. S. Evans, W. Liu, D. Brady, 和 S. Moore，“支持海洋水产养殖场地选择的异质时空数据集成方法，”
    *海洋科学与工程学报*，第8卷，第2期，第96页，2020年。'
- en: '[41] Y. Guo, C. Yin, M. Li, X. Ren, and P. Liu, “Mobile e-commerce recommendation
    system based on multi-source information fusion for sustainable e-business,” *Sustainability*,
    vol. 10, no. 1, p. 147, 2018.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Y. Guo, C. Yin, M. Li, X. Ren, 和 P. Liu，“基于多源信息融合的移动电子商务推荐系统，支持可持续电子商务，”
    *可持续性*，第10卷，第1期，第147页，2018年。'
- en: '[42] D. Ayata, Y. Yaslan, and M. E. Kamasak, “Emotion based music recommendation
    system using wearable physiological sensors,” *IEEE transactions on consumer electronics*,
    vol. 64, no. 2, pp. 196–203, 2018.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] D. Ayata, Y. Yaslan, 和 M. E. Kamasak，“基于情感的音乐推荐系统，使用可穿戴生理传感器，” *IEEE消费电子学报*，第64卷，第2期，第196–203页，2018年。'
- en: '[43] W. Zhang, S. Yan, J. Li, X. Tian, and T. Yoshida, “Credit risk prediction
    of smes in supply chain finance by fusing demographic and behavioral data,” *Transportation
    Research Part E: Logistics and Transportation Review*, vol. 158, p. 102611, 2022.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] W. Zhang, S. Yan, J. Li, X. Tian, 和 T. Yoshida，“通过融合人口统计和行为数据预测供应链金融中中小企业的信用风险，”
    *交通研究E部分：物流与交通评论*，第158卷，第102611页，2022年。'
- en: '[44] B. P. L. Lau, S. H. Marakkalage, Y. Zhou, N. U. Hassan, C. Yuen, M. Zhang,
    and U.-X. Tan, “A survey of data fusion in smart city applications,” *Information
    Fusion*, vol. 52, pp. 357–374, 2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] B. P. L. Lau, S. H. Marakkalage, Y. Zhou, N. U. Hassan, C. Yuen, M. Zhang,
    和 U.-X. Tan，“智能城市应用中的数据融合调查，” *信息融合*，第52卷，第357–374页，2019年。'
- en: '[45] H. F. Nweke, Y. W. Teh, G. Mujtaba, and M. A. Al-Garadi, “Data fusion
    and multiple classifier systems for human activity detection and health monitoring:
    Review and open research directions,” *Information Fusion*, vol. 46, pp. 147–170,
    2019.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] H. F. Nweke, Y. W. Teh, G. Mujtaba 和 M. A. Al-Garadi，“用于人类活动检测和健康监测的数据融合和多分类器系统：综述与研究方向”，*Information
    Fusion*，第46卷，页码147–170，2019年。'
- en: '[46] J. Qi, P. Yang, L. Newcombe, X. Peng, Y. Yang, and Z. Zhao, “An overview
    of data fusion techniques for internet of things enabled physical activity recognition
    and measure,” *Information Fusion*, vol. 55, pp. 269–280, 2020.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Qi, P. Yang, L. Newcombe, X. Peng, Y. Yang 和 Z. Zhao，“物联网支持的身体活动识别与测量的数据融合技术概述”，*Information
    Fusion*，第55卷，页码269–280，2020年。'
- en: '[47] G. Muhammad, F. Alshehri, F. Karray, A. El Saddik, M. Alsulaiman, and
    T. H. Falk, “A comprehensive survey on multimodal medical signals fusion for smart
    healthcare systems,” *Information Fusion*, vol. 76, pp. 355–375, 2021.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] G. Muhammad, F. Alshehri, F. Karray, A. El Saddik, M. Alsulaiman 和 T.
    H. Falk，“多模态医疗信号融合的全面综述，用于智能医疗系统”，*Information Fusion*，第76卷，页码355–375，2021年。'
- en: '[48] Q.-V. Pham, K. Dev, P. K. R. Maddikunta, T. R. Gadekallu, T. Huynh-The
    *et al.*, “Fusion of federated learning and industrial internet of things: A survey,”
    *arXiv preprint arXiv:2101.00798*, 2021.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Q.-V. Pham, K. Dev, P. K. R. Maddikunta, T. R. Gadekallu, T. Huynh-The
    *等*，“联邦学习与工业物联网的融合：综述”，*arXiv 预印本 arXiv:2101.00798*，2021年。'
- en: '[49] F. Alam, R. Mehmood, I. Katib, N. N. Albogami, and A. Albeshri, “Data
    fusion and iot for smart ubiquitous environments: A survey,” *IEEE Access*, vol. 5,
    pp. 9533–9554, 2017.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] F. Alam, R. Mehmood, I. Katib, N. N. Albogami 和 A. Albeshri，“智能无处不在环境中的数据融合与物联网：综述”，*IEEE
    Access*，第5卷，页码9533–9554，2017年。'
- en: '[50] M. Singh, R. Singh, and A. Ross, “A comprehensive overview of biometric
    fusion,” *Information Fusion*, vol. 52, pp. 187–205, 2019.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] M. Singh, R. Singh 和 A. Ross，“生物特征融合的全面概述”，*Information Fusion*，第52卷，页码187–205，2019年。'
- en: '[51] R. Ravindran, M. J. Santora, and M. M. Jamali, “Multi-object detection
    and tracking, based on dnn, for autonomous vehicles: A review,” *IEEE Sensors
    Journal*, vol. 21, no. 5, pp. 5668–5677, 2020.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] R. Ravindran, M. J. Santora 和 M. M. Jamali，“基于DNN的多目标检测与跟踪，用于自动驾驶车辆：综述”，*IEEE
    Sensors Journal*，第21卷，第5期，页码5668–5677，2020年。'
- en: '[52] Y. Mo, Y. Wu, X. Yang, F. Liu, and Y. Liao, “Review the state-of-the-art
    technologies of semantic segmentation based on deep learning,” *Neurocomputing*,
    2022.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Mo, Y. Wu, X. Yang, F. Liu 和 Y. Liao，“基于深度学习的语义分割前沿技术综述”，*Neurocomputing*，2022年。'
- en: '[53] Y. Huang and Y. Chen, “Autonomous driving with deep learning: A survey
    of state-of-art technologies,” *arXiv preprint arXiv:2006.06091*, 2020.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Huang 和 Y. Chen，"基于深度学习的自动驾驶：前沿技术综述"，*arXiv 预印本 arXiv:2006.06091*，2020年。'
- en: '[54] A. Gupta, A. Anpalagan, L. Guan, and A. S. Khwaja, “Deep learning for
    object detection and scene perception in self-driving cars: Survey, challenges,
    and open issues,” *Array*, vol. 10, p. 100057, 2021.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Gupta, A. Anpalagan, L. Guan 和 A. S. Khwaja，“自驾车中的目标检测与场景感知的深度学习：综述、挑战与开放问题”，*Array*，第10卷，页码100057，2021年。'
- en: '[55] T. Zhou, M. Yang, K. Jiang, H. Wong, and D. Yang, “Mmw radar-based technologies
    in autonomous driving: A review,” *Sensors*, vol. 20, no. 24, p. 7283, 2020.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] T. Zhou, M. Yang, K. Jiang, H. Wong 和 D. Yang，“基于毫米波雷达的自动驾驶技术：综述”，*Sensors*，第20卷，第24期，页码7283，2020年。'
- en: '[56] Z. Wang, J. Zhan, C. Duan, X. Guan, P. Lu, and K. Yang, “A review of vehicle
    detection techniques for intelligent vehicles,” *IEEE Transactions on Neural Networks
    and Learning Systems*, 2022.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Z. Wang, J. Zhan, C. Duan, X. Guan, P. Lu 和 K. Yang，“智能车辆的车辆检测技术综述”，*IEEE
    Transactions on Neural Networks and Learning Systems*，2022年。'
- en: '[57] F. M. Barbosa and F. S. Osório, “Camera-radar perception for autonomous
    vehicles and adas: Concepts, datasets and metrics,” *arXiv preprint arXiv:2303.04302*,
    2023.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] F. M. Barbosa 和 F. S. Osório，“用于自动驾驶车辆和ADAS的相机-雷达感知：概念、数据集和指标”，*arXiv
    预印本 arXiv:2303.04302*，2023年。'
- en: '[58] Y. Wang, Q. Mao, H. Zhu, Y. Zhang, J. Ji, and Y. Zhang, “Multi-modal 3d
    object detection in autonomous driving: a survey,” *arXiv preprint arXiv:2106.12735*,
    2021.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. Wang, Q. Mao, H. Zhu, Y. Zhang, J. Ji 和 Y. Zhang，“自动驾驶中的多模态3D物体检测：综述”，*arXiv
    预印本 arXiv:2106.12735*，2021年。'
- en: '[59] Y. Zhou, P. Sun, Y. Zhang, D. Anguelov, J. Gao, T. Ouyang, J. Guo, J. Ngiam,
    and V. Vasudevan, “End-to-end multi-view fusion for 3d object detection in lidar
    point clouds,” in *Conference on Robot Learning*.   PMLR, 2020, pp. 923–932.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. Zhou, P. Sun, Y. Zhang, D. Anguelov, J. Gao, T. Ouyang, J. Guo, J.
    Ngiam 和 V. Vasudevan，“基于激光雷达点云的端到端多视图融合3D物体检测”，在*机器人学习会议*。 PMLR，2020年，页码923–932。'
- en: '[60] D. Xu, D. Anguelov, and A. Jain, “Pointfusion: Deep sensor fusion for
    3d bounding box estimation,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2018, pp. 244–253.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] D. Xu, D. Anguelov, 和 A. Jain, “Pointfusion: 用于三维边界框估计的深度传感器融合，” 收录于 *IEEE计算机视觉与模式识别会议论文集*，2018年，第244–253页。'
- en: '[61] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for
    3d object detection from rgb-d data,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2018, pp. 918–927.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] C. R. Qi, W. Liu, C. Wu, H. Su, 和 L. J. Guibas, “Frustum Pointnets用于从RGB-D数据中进行三维物体检测，”
    收录于 *IEEE计算机视觉与模式识别会议论文集*，2018年，第918–927页。'
- en: '[62] A. Paigwar, D. Sierra-Gonzalez, Ö. Erkent, and C. Laugier, “Frustum-pointpillars:
    A multi-stage approach for 3d object detection using rgb camera and lidar,” in
    *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 2021,
    pp. 2926–2933.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. Paigwar, D. Sierra-Gonzalez, Ö. Erkent, 和 C. Laugier, “Frustum-Pointpillars:
    一种多阶段方法用于使用RGB相机和激光雷达进行三维物体检测，” 收录于 *IEEE/CVF国际计算机视觉会议论文集*，2021年，第2926–2933页。'
- en: '[63] Y. Li, L. Ma, Z. Zhong, F. Liu, M. A. Chapman, D. Cao, and J. Li, “Deep
    learning for lidar point clouds in autonomous driving: A review,” *IEEE Transactions
    on Neural Networks and Learning Systems*, vol. 32, no. 8, pp. 3412–3432, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Y. Li, L. Ma, Z. Zhong, F. Liu, M. A. Chapman, D. Cao, 和 J. Li, “用于自动驾驶的激光雷达点云的深度学习：综述，”
    *IEEE神经网络与学习系统汇刊*，第32卷，第8期，第3412–3432页，2020年。'
- en: '[64] R. Qian, D. Garg, Y. Wang, Y. You, S. Belongie, B. Hariharan, M. Campbell,
    K. Q. Weinberger, and W.-L. Chao, “End-to-end pseudo-lidar for image-based 3d
    object detection,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2020, pp. 5881–5890.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] R. Qian, D. Garg, Y. Wang, Y. You, S. Belongie, B. Hariharan, M. Campbell,
    K. Q. Weinberger, 和 W.-L. Chao, “端到端伪激光雷达用于基于图像的三维物体检测，” 收录于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第5881–5890页。'
- en: '[65] M. Kutila, P. Pyykönen, H. Holzhüter, M. Colomb, and P. Duthon, “Automotive
    lidar performance verification in fog and rain,” in *2018 21st International Conference
    on Intelligent Transportation Systems (ITSC)*.   IEEE, 2018, pp. 1695–1701.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] M. Kutila, P. Pyykönen, H. Holzhüter, M. Colomb, 和 P. Duthon, “汽车激光雷达在雾和雨中的性能验证，”
    收录于 *2018年第21届智能交通系统国际会议（ITSC）*。IEEE，2018年，第1695–1701页。'
- en: '[66] A. Petrovskaya and S. Thrun, “Model based vehicle tracking for autonomous
    driving in urban environments,” *Proceedings of robotics: science and systems
    IV, Zurich, Switzerland*, vol. 34, 2008.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] A. Petrovskaya 和 S. Thrun, “基于模型的车辆跟踪用于城市环境中的自动驾驶，” *机器人学：科学与系统 IV, 苏黎世，瑞士会议论文集*，第34卷，2008年。'
- en: '[67] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 652–660.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas, “Pointnet: 基于点集的三维分类和分割的深度学习，”
    收录于 *IEEE计算机视觉与模式识别会议论文集*，2017年，第652–660页。'
- en: '[68] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
    feature learning on point sets in a metric space,” *Advances in neural information
    processing systems*, vol. 30, 2017.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] C. R. Qi, L. Yi, H. Su, 和 L. J. Guibas, “Pointnet++: 在度量空间中的点集深度层次特征学习，”
    *神经信息处理系统进展*，第30卷，2017年。'
- en: '[69] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “Pointcnn: Convolution
    on x-transformed points,” *Advances in neural information processing systems*,
    vol. 31, 2018.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, 和 B. Chen, “Pointcnn: 在x变换点上的卷积，”
    *神经信息处理系统进展*，第31卷，2018年。'
- en: '[70] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J.
    Guibas, “Kpconv: Flexible and deformable convolution for point clouds,” in *Proceedings
    of the IEEE/CVF international conference on computer vision*, 2019, pp. 6411–6420.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, 和 L.
    J. Guibas, “Kpconv: 灵活且可变形的点云卷积，” 收录于 *IEEE/CVF国际计算机视觉会议论文集*，2019年，第6411–6420页。'
- en: '[71] T. Wu, J. Hu, L. Ye, and K. Ding, “A pedestrian detection algorithm based
    on score fusion for multi-lidar systems,” *Sensors*, vol. 21, no. 4, p. 1159,
    2021.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] T. Wu, J. Hu, L. Ye, 和 K. Ding, “一种基于得分融合的多激光雷达系统行人检测算法，” *传感器*，第21卷，第4期，第1159页，2021年。'
- en: '[72] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d
    shapenets: A deep representation for volumetric shapes,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2015, pp. 1912–1920.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, 和 J. Xiao, “3D Shapenets:
    一种用于体积形状的深度表示，” 收录于 *IEEE计算机视觉与模式识别会议论文集*，2015年，第1912–1920页。'
- en: '[73] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud based
    3d object detection,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 4490–4499.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. 周 和 O. 图泽尔，“Voxelnet：基于点云的 3d 目标检测的端到端学习，” 在 *IEEE 计算机视觉与模式识别会议论文集*
    中，2018 年，第 4490–4499 页。'
- en: '[74] D. Maturana and S. Scherer, “Voxnet: A 3d convolutional neural network
    for real-time object recognition,” in *2015 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*.   IEEE, 2015, pp. 922–928.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] D. 马图拉纳 和 S. 谢雷尔，“Voxnet：一种用于实时物体识别的 3d 卷积神经网络，” 在 *2015 年 IEEE/RSJ 智能机器人与系统国际会议
    (IROS)* 中。 IEEE，2015 年，第 922–928 页。'
- en: '[75] I. Sobh, L. Amin, S. Abdelkarim, K. Elmadawy, M. Saeed, O. Abdeltawab,
    M. Gamal, and A. El Sallab, “End-to-end multi-modal sensors fusion system for
    urban automated driving,” 2018.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] I. 索布，L. 阿敏，S. 阿卜杜勒卡里姆，K. 艾尔马达维，M. 萨伊德，O. 阿卜杜勒塔瓦布，M. 贾马尔，和 A. 艾尔·萨拉布，“面向城市自动驾驶的端到端多模态传感器融合系统，”
    2018 年。'
- en: '[76] X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai, “Transfusion:
    Robust lidar-camera fusion for 3d object detection with transformers,” *arXiv
    preprint arXiv:2203.11496*, 2022.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] X. 白，Z. 胡，X. 朱，Q. 黄，Y. 陈，H. 傅，和 C.-L. 台，“Transfusion：基于变压器的稳健激光雷达-相机融合用于
    3d 目标检测，” *arXiv 预印本 arXiv:2203.11496*，2022 年。'
- en: '[77] B. Wang, V. Wu, B. Wu, and K. Keutzer, “Latte: accelerating lidar point
    cloud annotation via sensor fusion, one-click annotation, and tracking,” in *2019
    IEEE Intelligent Transportation Systems Conference (ITSC)*.   IEEE, 2019, pp.
    265–272.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] B. 王，V. 吴，B. 吴，和 K. 克伊泽特，“Latte：通过传感器融合、一键标注和跟踪加速激光雷达点云标注，” 在 *2019 年
    IEEE 智能交通系统会议 (ITSC)* 中。 IEEE，2019 年，第 265–272 页。'
- en: '[78] Z. Liu, H. Tang, Y. Lin, and S. Han, “Point-voxel cnn for efficient 3d
    deep learning,” *Advances in Neural Information Processing Systems*, vol. 32,
    2019.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Z. 刘，H. 唐，Y. 林，和 S. 韩，“点体素 cnn 用于高效的 3d 深度学习，” *神经信息处理系统进展*，第 32 卷，2019
    年。'
- en: '[79] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, “Pv-rcnn:
    Point-voxel feature set abstraction for 3d object detection,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020,
    pp. 10 529–10 538.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] S. 石，C. 郭，L. 姜，Z. 王，J. 石，X. 王，和 H. 李，“Pv-rcnn：用于 3d 目标检测的点体素特征集抽象，” 在
    *IEEE/CVF 计算机视觉与模式识别大会论文集* 中，2020 年，第 10 529–10 538 页。'
- en: '[80] T. Guan, J. Wang, S. Lan, R. Chandra, Z. Wu, L. Davis, and D. Manocha,
    “M3detr: Multi-representation, multi-scale, mutual-relation 3d object detection
    with transformers,” in *Proceedings of the IEEE/CVF Winter Conference on Applications
    of Computer Vision (WACV)*, January 2022, pp. 772–782.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] T. 关，J. 王，S. 兰，R. 昌德拉，Z. 吴，L. 戴维斯，和 D. 马诺查，“M3detr：多表示、多尺度、互相关系的 3d 目标检测与变压器，”
    在 *IEEE/CVF 冬季计算机视觉应用会议 (WACV)* 的会议论文中，2022 年 1 月，第 772–782 页。'
- en: '[81] R. Zhang and S. Cao, “Real-time human motion behavior detection via cnn
    using mmwave radar,” *IEEE Sensors Letters*, vol. 3, no. 2, pp. 1–4, 2018.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] R. 张 和 S. 曹，“通过 cnn 和 mmwave 雷达实现实时人类运动行为检测，” *IEEE 传感器快报*，第 3 卷，第 2 期，第
    1–4 页，2018 年。'
- en: '[82] Z. Wei, F. Zhang, S. Chang, Y. Liu, H. Wu, and Z. Feng, “Mmwave radar
    and vision fusion for object detection in autonomous driving: A review,” *Sensors*,
    vol. 22, no. 7, p. 2542, 2022.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Z. 韦，F. 张，S. 常，Y. 刘，H. 吴，和 Z. 冯，“mmwave 雷达与视觉融合在自动驾驶中的物体检测：综述，” *传感器*，第
    22 卷，第 7 期，第 2542 页，2022 年。'
- en: '[83] Y. Zhou, L. Liu, H. Zhao, M. López-Benítez, L. Yu, and Y. Yue, “Towards
    deep radar perception for autonomous driving: Datasets, methods, and challenges,”
    *Sensors*, vol. 22, no. 11, p. 4208, 2022.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. 周，L. 刘，H. 赵，M. 洛佩斯-贝尼特斯，L. 于，和 Y. 岳，“面向自动驾驶的深度雷达感知：数据集、方法和挑战，” *传感器*，第
    22 卷，第 11 期，第 4208 页，2022 年。'
- en: '[84] O. Schumann, M. Hahn, J. Dickmann, and C. Wöhler, “Semantic segmentation
    on radar point clouds,” in *2018 21st International Conference on Information
    Fusion (FUSION)*.   IEEE, 2018, pp. 2179–2186.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] O. 舒曼，M. 哈恩，J. 迪克曼，和 C. 维勒，“雷达点云上的语义分割，” 在 *2018 年第 21 届信息融合国际会议 (FUSION)*
    中。 IEEE，2018 年，第 2179–2186 页。'
- en: '[85] B. Pan, J. Sun, H. Y. T. Leung, A. Andonian, and B. Zhou, “Cross-view
    semantic segmentation for sensing surroundings,” *IEEE Robotics and Automation
    Letters*, vol. 5, no. 3, pp. 4867–4873, 2020.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] B. 潘，J. 孙，H. Y. T. 梁，A. 安东尼安，和 B. 周，“跨视图语义分割以感知环境，” *IEEE 机器人与自动化快报*，第
    5 卷，第 3 期，第 4867–4873 页，2020 年。'
- en: '[86] Q. Li, Y. Wang, Y. Wang, and H. Zhao, “Hdmapnet: A local semantic map
    learning and evaluation framework,” *arXiv preprint arXiv:2107.06307*, 2021.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Q. 李，Y. 王，Y. 王，和 H. 赵，“Hdmapnet：一种局部语义地图学习和评估框架，” *arXiv 预印本 arXiv:2107.06307*，2021
    年。'
- en: '[87] B. Zhou and P. Krähenbühl, “Cross-view transformers for real-time map-view
    semantic segmentation,” *arXiv preprint arXiv:2205.02833*, 2022.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] B. Zhou 和 P. Krähenbühl, “用于实时地图视图语义分割的跨视图变换器，” *arXiv 预印本 arXiv:2205.02833*，2022。'
- en: '[88] Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang, H. Zhao, and J. Solomon,
    “Detr3d: 3d object detection from multi-view images via 3d-to-2d queries,” in
    *Conference on Robot Learning*.   PMLR, 2022, pp. 180–191.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang, H. Zhao 和 J. Solomon, “Detr3d:
    通过 3D 到 2D 查询从多视图图像中进行 3D 目标检测，” 在 *机器人学习会议*。 PMLR, 2022, pp. 180–191。'
- en: '[89] S. Ramos, S. Gehrig, P. Pinggera, U. Franke, and C. Rother, “Detecting
    unexpected obstacles for self-driving cars: Fusing deep learning and geometric
    modeling,” in *2017 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 2017, pp.
    1025–1032.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] S. Ramos, S. Gehrig, P. Pinggera, U. Franke 和 C. Rother, “检测自动驾驶汽车的意外障碍物：融合深度学习和几何建模，”
    在 *2017 IEEE 智能车辆研讨会 (IV)*。 IEEE, 2017, pp. 1025–1032。'
- en: '[90] M. Cao and J. Wang, “Obstacle detection for autonomous driving vehicles
    with multi-lidar sensor fusion,” *Journal of Dynamic Systems, Measurement, and
    Control*, vol. 142, no. 2, p. 021007, 2020.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. Cao 和 J. Wang, “基于多激光雷达传感器融合的自动驾驶车辆障碍物检测，” *动态系统、测量与控制杂志*，第 142 卷，第
    2 期，第 021007 页，2020。'
- en: '[91] F. Roos, J. Bechter, C. Knill, B. Schweizer, and C. Waldschmidt, “Radar
    sensors for autonomous driving: Modulation schemes and interference mitigation,”
    *IEEE Microwave Magazine*, vol. 20, no. 9, pp. 58–72, 2019.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] F. Roos, J. Bechter, C. Knill, B. Schweizer 和 C. Waldschmidt, “自动驾驶的雷达传感器：调制方案和干扰缓解，”
    *IEEE 微波杂志*，第 20 卷，第 9 期，第 58–72 页，2019。'
- en: '[92] J. Dickmann, J. Klappstein, M. Hahn, N. Appenrodt, H.-L. Bloecher, K. Werber,
    and A. Sailer, “Automotive radar the key technology for autonomous driving: From
    detection and ranging to environmental understanding,” in *2016 IEEE Radar Conference
    (RadarConf)*.   IEEE, 2016, pp. 1–6.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Dickmann, J. Klappstein, M. Hahn, N. Appenrodt, H.-L. Bloecher, K.
    Werber 和 A. Sailer, “汽车雷达：自动驾驶的关键技术：从探测和测距到环境理解，” 在 *2016 IEEE 雷达会议 (RadarConf)*。
    IEEE, 2016, pp. 1–6。'
- en: '[93] G. L. Oliveira, N. Radwan, W. Burgard, and T. Brox, “Topometric localization
    with deep learning,” in *Robotics Research*.   Springer, 2020, pp. 505–520.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] G. L. Oliveira, N. Radwan, W. Burgard 和 T. Brox, “基于深度学习的拓扑定位，” 在 *机器人研究*。
    Springer, 2020, pp. 505–520。'
- en: '[94] L. Wang, Y. Gong, X. Ma, Q. Wang, K. Zhou, and L. Chen, “Is-mvsnet:importance
    sampling-based mvsnet,” in *Computer Vision – ECCV 2022*, S. Avidan, G. Brostow,
    M. Cissé, G. M. Farinella, and T. Hassner, Eds.   Cham: Springer Nature Switzerland,
    2022, pp. 668–683.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] L. Wang, Y. Gong, X. Ma, Q. Wang, K. Zhou, 和 L. Chen, “Is-mvsnet: 基于重要性采样的
    mvsnet，” 在 *计算机视觉 – ECCV 2022*，S. Avidan, G. Brostow, M. Cissé, G. M. Farinella
    和 T. Hassner 编辑。 Cham: Springer Nature Switzerland, 2022, pp. 668–683。'
- en: '[95] S. Song, K. G. Truong, D. Kim, and S. Jo, “Prior depth-based multi-view
    stereo network for online 3d model reconstruction,” *Pattern Recognition*, vol.
    136, p. 109198, 2023.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] S. Song, K. G. Truong, D. Kim 和 S. Jo, “基于先验深度的多视图立体网络用于在线 3D 模型重建，” *模式识别*，第
    136 卷，第 109198 页，2023。'
- en: '[96] J. Yu, W. Yin, Z. Hu, and Y. Liu, “3d reconstruction for multi-view objects,”
    *Computers and Electrical Engineering*, vol. 106, p. 108567, 2023.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J. Yu, W. Yin, Z. Hu 和 Y. Liu, “多视图物体的 3D 重建，” *计算机与电气工程*，第 106 卷，第 108567
    页，2023。'
- en: '[97] J. Lei, J. Song, B. Peng, W. Li, Z. Pan, and Q. Huang, “C2fnet: A coarse-to-fine
    network for multi-view 3d point cloud generation,” *IEEE Transactions on Image
    Processing*, vol. 31, pp. 6707–6718, 2022.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. Lei, J. Song, B. Peng, W. Li, Z. Pan 和 Q. Huang, “C2fnet: 用于多视图 3D
    点云生成的粗到细网络，” *IEEE 图像处理汇刊*，第 31 卷，第 6707–6718 页，2022。'
- en: '[98] Y. Sun, J. Hu, J. Yun, Y. Liu, D. Bai, X. Liu, G. Zhao, G. Jiang, J. Kong,
    and B. Chen, “Multi-objective location and mapping based on deep learning and
    visual slam,” *Sensors*, vol. 22, no. 19, p. 7576, 2022.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Sun, J. Hu, J. Yun, Y. Liu, D. Bai, X. Liu, G. Zhao, G. Jiang, J. Kong
    和 B. Chen, “基于深度学习和视觉 SLAM 的多目标定位与映射，” *传感器*，第 22 卷，第 19 期，第 7576 页，2022。'
- en: '[99] R. Tian, Y. Zhang, Y. Feng, L. Yang, Z. Cao, S. Coleman, and D. Kerr,
    “Accurate and robust object slam with 3d quadric landmark reconstruction in outdoor
    environment,” in *2022 IEEE International Conference on Robotics and Automation*,
    2022.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] R. Tian, Y. Zhang, Y. Feng, L. Yang, Z. Cao, S. Coleman 和 D. Kerr, “在户外环境中通过
    3D 二次标志物重建实现精确且鲁棒的目标 SLAM，” 在 *2022 IEEE 国际机器人与自动化会议*，2022。'
- en: '[100] Z. Zhu, L. Yang, X. Lin, C. Jiang, N. Li, L. Yang, and Y. Liang, “Garnet:
    Global-aware multi-view 3d reconstruction network and the cost-performance tradeoff,”
    *arXiv preprint arXiv:2211.02299*, 2022.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Z. Zhu, L. Yang, X. Lin, C. Jiang, N. Li, L. Yang 和 Y. Liang, “Garnet:
    全球感知的多视图 3D 重建网络及其成本性能权衡，” *arXiv 预印本 arXiv:2211.02299*，2022。'
- en: '[101] L. Peng, Z. Chen, Z. Fu, P. Liang, and E. Cheng, “Bevsegformer: Bird’s
    eye view semantic segmentation from arbitrary camera rigs,” in *Proceedings of
    the IEEE/CVF Winter Conference on Applications of Computer Vision*, 2023, pp.
    5935–5943.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] L. Peng, Z. Chen, Z. Fu, P. Liang 和 E. Cheng，“Bevsegformer: 从任意相机系统中进行鸟瞰视角语义分割”，收录于
    *IEEE/CVF 冬季计算机视觉应用会议论文集*，2023年，第5935–5943页。'
- en: '[102] Y. Jiang, L. Zhang, Z. Miao, X. Zhu, J. Gao, W. Hu, and Y.-G. Jiang,
    “Polarformer: Multi-camera 3d object detection with polar transformer,” 2022.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Y. Jiang, L. Zhang, Z. Miao, X. Zhu, J. Gao, W. Hu 和 Y.-G. Jiang，“Polarformer:
    基于极坐标变换器的多摄像头 3D 物体检测”，2022年。'
- en: '[103] R. Tao, W. Han, Z. Qiu, C.-z. Xu, and J. Shen, “Weakly supervised monocular
    3d object detection using multi-view projection and direction consistency,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2023, pp. 17 482–17 492.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] R. Tao, W. Han, Z. Qiu, C.-z. Xu 和 J. Shen，“使用多视角投影和方向一致性的弱监督单目 3D 物体检测”，收录于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023年，第17 482–17 492页。'
- en: '[104] H. J. Lee and Y. M. Ro, “Adversarially robust multi-sensor fusion model
    training via random feature fusion for semantic segmentation,” in *2021 IEEE International
    Conference on Image Processing (ICIP)*.   IEEE, 2021, pp. 339–343.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] H. J. Lee 和 Y. M. Ro，“通过随机特征融合进行对抗性鲁棒的多传感器融合模型训练用于语义分割”，收录于 *2021 IEEE
    图像处理国际会议（ICIP）*。IEEE，2021年，第339–343页。'
- en: '[105] S. Vachmanus, A. A. Ravankar, T. Emaru, and Y. Kobayashi, “Multi-modal
    sensor fusion-based semantic segmentation for snow driving scenarios,” *IEEE sensors
    journal*, vol. 21, no. 15, pp. 16 839–16 851, 2021.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] S. Vachmanus, A. A. Ravankar, T. Emaru 和 Y. Kobayashi，“基于多模态传感器融合的雪地驾驶场景语义分割”，*IEEE
    传感器学报*，第21卷，第15期，第16 839–16 851页，2021年。'
- en: '[106] Y. Sun, W. Zuo, P. Yun, H. Wang, and M. Liu, “Fuseseg: semantic segmentation
    of urban scenes based on rgb and thermal data fusion,” *IEEE Transactions on Automation
    Science and Engineering*, vol. 18, no. 3, pp. 1000–1011, 2020.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Y. Sun, W. Zuo, P. Yun, H. Wang 和 M. Liu，“Fuseseg: 基于 RGB 和热数据融合的城市场景语义分割”，*IEEE
    自动化科学与工程学报*，第18卷，第3期，第1000–1011页，2020年。'
- en: '[107] A. Ouaknine, A. Newson, P. Pérez, F. Tupin, and J. Rebut, “Multi-view
    radar semantic segmentation,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2021, pp. 15 671–15 680.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] A. Ouaknine, A. Newson, P. Pérez, F. Tupin 和 J. Rebut，“多视角雷达语义分割”，收录于
    *IEEE/CVF 国际计算机视觉大会论文集*，2021年，第15 671–15 680页。'
- en: '[108] S. Wu, S. Decker, P. Chang, T. Camus, and J. Eledath, “Collision sensing
    by stereo vision and radar sensor fusion,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 10, no. 4, pp. 606–614, 2009.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] S. Wu, S. Decker, P. Chang, T. Camus 和 J. Eledath，“通过立体视觉和雷达传感器融合进行碰撞检测”，*IEEE
    智能交通系统学报*，第10卷，第4期，第606–614页，2009年。'
- en: '[109] H. Jha, V. Lodhi, and D. Chakravarty, “Object detection and identification
    using vision and radar data fusion system for ground-based navigation,” in *2019
    6th International Conference on Signal Processing and Integrated Networks (SPIN)*.   IEEE,
    2019, pp. 590–593.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] H. Jha, V. Lodhi 和 D. Chakravarty，“基于视觉和雷达数据融合系统的物体检测与识别用于地面导航”，收录于 *2019年第六届信号处理与集成网络国际会议（SPIN）*。IEEE，2019年，第590–593页。'
- en: '[110] K. Kowol, M. Rottmann, S. Bracke, and H. Gottschalk, “Yodar: Uncertainty-based
    sensor fusion for vehicle detection with camera and radar sensors,” *arXiv preprint
    arXiv:2010.03320*, 2020.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] K. Kowol, M. Rottmann, S. Bracke 和 H. Gottschalk，“Yodar: 基于不确定性的传感器融合用于车辆检测的摄像头和雷达传感器”，*arXiv
    预印本 arXiv:2010.03320*，2020年。'
- en: '[111] Z. Liu, Y. Cai, H. Wang, L. Chen, H. Gao, Y. Jia, and Y. Li, “Robust
    target recognition and tracking of self-driving cars with radar and camera information
    fusion under severe weather conditions,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2021.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Z. Liu, Y. Cai, H. Wang, L. Chen, H. Gao, Y. Jia 和 Y. Li，“在恶劣天气条件下，基于雷达和摄像头信息融合的自驾车目标识别与跟踪”，*IEEE
    智能交通系统学报*，2021年。'
- en: '[112] R. Nabati and H. Qi, “Centerfusion: Center-based radar and camera fusion
    for 3d object detection,” in *Proceedings of the IEEE/CVF Winter Conference on
    Applications of Computer Vision*, 2021, pp. 1527–1536.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] R. Nabati 和 H. Qi，“Centerfusion: 基于中心的雷达与摄像头融合用于 3D 物体检测”，收录于 *IEEE/CVF
    冬季计算机视觉应用会议论文集*，2021年，第1527–1536页。'
- en: '[113] K. Qian, S. Zhu, X. Zhang, and L. E. Li, “Robust multimodal vehicle detection
    in foggy weather using complementary lidar and radar signals,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 444–453.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] K. Qian, S. Zhu, X. Zhang 和 L. E. Li，“在雾霾天气下使用互补的激光雷达和雷达信号进行鲁棒的多模态车辆检测”，收录于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，2021年，第444–453页。'
- en: '[114] P. Cai, S. Wang, Y. Sun, and M. Liu, “Probabilistic end-to-end vehicle
    navigation in complex dynamic environments with multimodal sensor fusion,” *IEEE
    Robotics and Automation Letters*, vol. 5, no. 3, pp. 4218–4224, 2020.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] P. Cai, S. Wang, Y. Sun 和 M. Liu，“复杂动态环境下的概率端到端车辆导航与多模态传感器融合，” *IEEE
    机器人与自动化快报*，第 5 卷，第 3 期，第 4218–4224 页，2020 年。'
- en: '[115] B. Shahian Jahromi, T. Tulabandhula, and S. Cetin, “Real-time hybrid
    multi-sensor fusion framework for perception in autonomous vehicles,” *Sensors*,
    vol. 19, no. 20, p. 4357, 2019.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] B. Shahian Jahromi, T. Tulabandhula 和 S. Cetin，“用于自动驾驶车辆感知的实时混合多传感器融合框架，”
    *传感器*，第 19 卷，第 20 期，第 4357 页，2019 年。'
- en: '[116] Y. Li, A. W. Yu, T. Meng, B. Caine, J. Ngiam, D. Peng, J. Shen, Y. Lu,
    D. Zhou, Q. V. Le *et al.*, “Deepfusion: Lidar-camera deep fusion for multi-modal
    3d object detection,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 17 182–17 191.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Y. Li, A. W. Yu, T. Meng, B. Caine, J. Ngiam, D. Peng, J. Shen, Y. Lu,
    D. Zhou, Q. V. Le *等*，“Deepfusion：用于多模态 3d 物体检测的激光雷达-相机深度融合，” 见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2022
    年，第 17 182–17 191 页。'
- en: '[117] L. Caltagirone, M. Bellone, L. Svensson, and M. Wahde, “Lidar–camera
    fusion for road detection using fully convolutional neural networks,” *Robotics
    and Autonomous Systems*, vol. 111, pp. 125–131, 2019.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] L. Caltagirone, M. Bellone, L. Svensson 和 M. Wahde，“使用全卷积神经网络的激光雷达-相机融合进行道路检测，”
    *机器人技术与自主系统*，第 111 卷，第 125–131 页，2019 年。'
- en: '[118] J. S. Berrio, M. Shan, S. Worrall, and E. Nebot, “Camera-lidar integration:
    Probabilistic sensor fusion for semantic mapping,” *IEEE Transactions on Intelligent
    Transportation Systems*, 2021.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] J. S. Berrio, M. Shan, S. Worrall 和 E. Nebot，“相机-激光雷达集成：用于语义映射的概率传感器融合，”
    *IEEE 智能交通系统汇刊*，2021 年。'
- en: '[119] T. Huang, Z. Liu, X. Chen, and X. Bai, “Epnet: Enhancing point features
    with image semantics for 3d object detection,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 35–52.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] T. Huang, Z. Liu, X. Chen 和 X. Bai，“Epnet：通过图像语义增强点特征以进行 3d 物体检测，” 见于
    *欧洲计算机视觉会议*。Springer，2020 年，第 35–52 页。'
- en: '[120] X. Cheng, Y. Zhong, Y. Dai, P. Ji, and H. Li, “Noise-aware unsupervised
    deep lidar-stereo fusion,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 6339–6348.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] X. Cheng, Y. Zhong, Y. Dai, P. Ji 和 H. Li，“噪声感知无监督深度激光雷达-立体融合，” 见于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2019 年，第 6339–6348 页。'
- en: '[121] M. Hu, S. Wang, B. Li, S. Ning, L. Fan, and X. Gong, “Penet: Towards
    precise and efficient image guided depth completion,” in *2021 IEEE International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2021, pp. 13 656–13 662.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Hu, S. Wang, B. Li, S. Ning, L. Fan 和 X. Gong，“Penet：向精确高效的图像引导深度补全迈进，”
    见于 *2021 IEEE 国际机器人与自动化会议 (ICRA)*。IEEE，2021 年，第 13 656–13 662 页。'
- en: '[122] Z. Yan, K. Wang, X. Li, Z. Zhang, B. Xu, J. Li, and J. Yang, “Rignet:
    Repetitive image guided network for depth completion,” *arXiv preprint arXiv:2107.13802*,
    2021.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Z. Yan, K. Wang, X. Li, Z. Zhang, B. Xu, J. Li 和 J. Yang，“Rignet：用于深度补全的重复图像引导网络，”
    *arXiv 预印本 arXiv:2107.13802*，2021 年。'
- en: '[123] C. Fu, C. Mertz, and J. M. Dolan, “Lidar and monocular camera fusion:
    On-road depth completion for autonomous driving,” in *2019 IEEE Intelligent Transportation
    Systems Conference (ITSC)*.   IEEE, 2019, pp. 273–278.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] C. Fu, C. Mertz 和 J. M. Dolan，“激光雷达与单目相机融合：用于自动驾驶的道路深度补全，” 见于 *2019 IEEE
    智能交通系统会议 (ITSC)*。IEEE，2019 年，第 273–278 页。'
- en: '[124] S. Vora, A. H. Lang, B. Helou, and O. Beijbom, “Pointpainting: Sequential
    fusion for 3d object detection,” in *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*, 2020, pp. 4604–4612.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] S. Vora, A. H. Lang, B. Helou 和 O. Beijbom，“Pointpainting：用于 3d 物体检测的序列融合，”
    见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020 年，第 4604–4612 页。'
- en: '[125] K. Geng, G. Dong, G. Yin, and J. Hu, “Deep dual-modal traffic objects
    instance segmentation method using camera and lidar data for autonomous driving,”
    *Remote Sensing*, vol. 12, no. 20, p. 3274, 2020.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] K. Geng, G. Dong, G. Yin 和 J. Hu，“基于相机和激光雷达数据的深度双模态交通物体实例分割方法用于自动驾驶，”
    *遥感*，第 12 卷，第 20 期，第 3274 页，2020 年。'
- en: '[126] Y. Liu, Z. Wang, K. Han, Z. Shou, P. Tiwari, and J. H. Hansen, “Sensor
    fusion of camera and cloud digital twin information for intelligent vehicles,”
    in *2020 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 2020, pp. 182–187.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Y. Liu, Z. Wang, K. Han, Z. Shou, P. Tiwari 和 J. H. Hansen，“相机与云数字双胞胎信息的传感器融合用于智能车辆，”
    见于 *2020 IEEE 智能车辆研讨会 (IV)*。IEEE，2020 年，第 182–187 页。'
- en: '[127] Z. Ouyang, C. Wang, Y. Liu, and J. Niu, “Multiview cnn model for sensor
    fusion based vehicle detection,” in *Pacific Rim Conference on Multimedia*.   Springer,
    2018, pp. 459–470.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Z. Ouyang, C. Wang, Y. Liu, 和 J. Niu， “基于传感器融合的多视角卷积神经网络模型用于车辆检测，” 在
    *太平洋地区多媒体会议*。 Springer，2018年，第459–470页。'
- en: '[128] A. Pfeuffer and K. Dietmayer, “Robust semantic segmentation in adverse
    weather conditions by means of sensor data fusion,” in *2019 22th International
    Conference on Information Fusion (FUSION)*.   IEEE, 2019, pp. 1–8.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Pfeuffer 和 K. Dietmayer， “在不利天气条件下通过传感器数据融合实现稳健的语义分割，” 在 *2019年第22届国际信息融合会议（FUSION）*。
    IEEE，2019年，第1–8页。'
- en: '[129] J. Mendez, M. Molina, N. Rodriguez, M. P. Cuellar, and D. P. Morales,
    “Camera-lidar multi-level sensor fusion for target detection at the network edge,”
    *Sensors*, vol. 21, no. 12, p. 3992, 2021.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] J. Mendez, M. Molina, N. Rodriguez, M. P. Cuellar, 和 D. P. Morales， “摄像头-激光雷达多层次传感器融合用于网络边缘目标检测，”
    *传感器*，第21卷，第12期，第3992页，2021年。'
- en: '[130] G. P. Meyer, J. Charland, D. Hegde, A. Laddha, and C. Vallespi-Gonzalez,
    “Sensor fusion for joint 3d object detection and semantic segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops*,
    2019, pp. 0–0.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] G. P. Meyer, J. Charland, D. Hegde, A. Laddha, 和 C. Vallespi-Gonzalez，
    “用于联合三维目标检测和语义分割的传感器融合，” 在 *IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第0–0页。'
- en: '[131] T.-L. Kim and T.-H. Park, “Camera-lidar fusion method with feature switch
    layer for object detection networks,” *Sensors*, vol. 22, no. 19, p. 7163, 2022.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] T.-L. Kim 和 T.-H. Park， “具有特征切换层的摄像头-激光雷达融合方法用于目标检测网络，” *传感器*，第22卷，第19期，第7163页，2022年。'
- en: '[132] X. Zhang, L. Wang, G. Zhang, T. Lan, H. Zhang, L. Zhao, J. Li, L. Zhu,
    and H. Liu, “Ri-fusion: 3d object detection using enhanced point features with
    range-image fusion for autonomous driving,” *IEEE Transactions on Instrumentation
    and Measurement*, 2022.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] X. Zhang, L. Wang, G. Zhang, T. Lan, H. Zhang, L. Zhao, J. Li, L. Zhu,
    和 H. Liu， “Ri-fusion：使用增强点特征与范围图像融合进行自动驾驶的三维目标检测，” *IEEE仪器与测量学报*，2022年。'
- en: '[133] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep continuous fusion for
    multi-sensor 3d object detection,” in *Proceedings of the European conference
    on computer vision (ECCV)*, 2018, pp. 641–656.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] M. Liang, B. Yang, S. Wang, 和 R. Urtasun， “深度连续融合用于多传感器三维目标检测，” 在 *欧洲计算机视觉会议（ECCV）论文集*，2018年，第641–656页。'
- en: '[134] E. Schröder, S. Braun, M. Mählisch, J. Vitay, and F. Hamker, “Feature
    map transformation for multi-sensor fusion in object detection networks for autonomous
    driving,” in *Science and Information Conference*.   Springer, 2019, pp. 118–131.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] E. Schröder, S. Braun, M. Mählisch, J. Vitay, 和 F. Hamker， “用于自动驾驶目标检测网络的多传感器融合中的特征图变换，”
    在 *科学与信息会议*。 Springer，2019年，第118–131页。'
- en: '[135] J. Zhao, X. N. Zhang, H. Gao, J. Yin, M. Zhou, and C. Tan, “Object detection
    based on hierarchical multi-view proposal network for autonomous driving,” in
    *2018 international joint conference on neural networks (IJCNN)*.   IEEE, 2018,
    pp. 1–6.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] J. Zhao, X. N. Zhang, H. Gao, J. Yin, M. Zhou, 和 C. Tan， “基于分层多视图提议网络的目标检测用于自动驾驶，”
    在 *2018年国际神经网络联合会议（IJCNN）*。 IEEE，2018年，第1–6页。'
- en: '[136] X. Wu, L. Peng, H. Yang, L. Xie, C. Huang, C. Deng, H. Liu, and D. Cai,
    “Sparse fuse dense: Towards high quality 3d detection with depth completion,”
    *arXiv preprint arXiv:2203.09780*, 2022.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] X. Wu, L. Peng, H. Yang, L. Xie, C. Huang, C. Deng, H. Liu, 和 D. Cai，
    “稀疏融合密集：朝向高质量三维检测与深度补全，” *arXiv预印本 arXiv:2203.09780*，2022年。'
- en: '[137] M. Liang, B. Yang, Y. Chen, R. Hu, and R. Urtasun, “Multi-task multi-sensor
    fusion for 3d object detection,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2019, pp. 7345–7353.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] M. Liang, B. Yang, Y. Chen, R. Hu, 和 R. Urtasun， “多任务多传感器融合用于三维目标检测，”
    在 *IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第7345–7353页。'
- en: '[138] A. Rövid and V. Remeli, “Towards raw sensor fusion in 3d object detection,”
    in *2019 IEEE 17th World Symposium on Applied Machine Intelligence and Informatics
    (SAMI)*.   IEEE, 2019, pp. 293–298.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] A. Rövid 和 V. Remeli， “朝向三维目标检测中的原始传感器融合，” 在 *2019 IEEE第17届应用机器智能与信息学世界研讨会（SAMI）*。
    IEEE，2019年， 第293–298页。'
- en: '[139] H. Zhu, J. Deng, Y. Zhang, J. Ji, Q. Mao, H. Li, and Y. Zhang, “Vpfnet:
    Improving 3d object detection with virtual point based lidar and stereo data fusion,”
    *arXiv preprint arXiv:2111.14382*, 2021.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] H. Zhu, J. Deng, Y. Zhang, J. Ji, Q. Mao, H. Li, 和 Y. Zhang， “Vpfnet：通过虚拟点基激光雷达和立体数据融合提高三维目标检测性能，”
    *arXiv预印本 arXiv:2111.14382*，2021年。'
- en: '[140] X. Li, T. Ma, Y. Hou, B. Shi, Y. Yang, Y. Liu, X. Wu, Q. Chen, Y. Li,
    Y. Qiao *et al.*, “Logonet: Towards accurate 3d object detection with local-to-global
    cross-modal fusion,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2023, pp. 17 524–17 534.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] X. Li, T. Ma, Y. Hou, B. Shi, Y. Yang, Y. Liu, X. Wu, Q. Chen, Y. Li,
    Y. Qiao *等*，“Logonet: 向精准的3D物体检测迈进，通过局部到全局的跨模态融合”，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2023年，第17,524–17,534页。'
- en: '[141] X. Dong, B. Zhuang, Y. Mao, and L. Liu, “Radar camera fusion via representation
    learning in autonomous driving,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2021, pp. 1672–1681.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] X. Dong, B. Zhuang, Y. Mao, 和 L. Liu，“通过表示学习进行雷达摄像头融合以实现自动驾驶”，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第1672–1681页。'
- en: '[142] C. Qi, C. Song, N. Zhang, S. Song, X. Wang, and F. Xiao, “Millimeter-wave
    radar and vision fusion target detection algorithm based on an extended network,”
    *Machines*, vol. 10, no. 8, p. 675, 2022.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] C. Qi, C. Song, N. Zhang, S. Song, X. Wang, 和 F. Xiao，“基于扩展网络的毫米波雷达与视觉融合目标检测算法”，*Machines*，第10卷，第8期，第675页，2022年。'
- en: '[143] Z. Li, M. Yan, W. Jiang, and P. Xu, “Vehicle object detection based on
    rgb-camera and radar sensor fusion,” in *2019 International Joint Conference on
    Information, Media and Engineering (IJCIME)*.   IEEE, 2019, pp. 164–169.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Z. Li, M. Yan, W. Jiang, 和 P. Xu，“基于RGB摄像头和雷达传感器融合的车辆物体检测”，在*2019国际信息、媒体与工程联合会议（IJCIME）*。IEEE，2019年，第164–169页。'
- en: '[144] P. Fritsche and B. Wagner, “Modeling structure and aerosol concentration
    with fused radar and lidar data in environments with changing visibility,” in
    *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2017, pp. 2685–2690.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] P. Fritsche 和 B. Wagner，“在能见度变化的环境中，通过融合雷达和激光雷达数据建模结构和气溶胶浓度”，在*2017 IEEE/RSJ国际智能机器人与系统会议（IROS）*。IEEE，2017年，第2685–2690页。'
- en: '[145] S. K. Kwon, E. Hyun, J.-H. Lee, J. Lee, and S. H. Son, “A low-complexity
    scheme for partially occluded pedestrian detection using lidar-radar sensor fusion,”
    in *2016 IEEE 22nd International Conference on Embedded and Real-Time Computing
    Systems and Applications (RTCSA)*.   IEEE, 2016, pp. 104–104.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] S. K. Kwon, E. Hyun, J.-H. Lee, J. Lee, 和 S. H. Son，“一种低复杂度的部分遮挡行人检测方案，使用激光雷达-雷达传感器融合”，在*2016
    IEEE第22届嵌入式和实时计算系统及应用国际会议（RTCSA）*。IEEE，2016年，第104–104页。'
- en: '[146] R. Ravindran, M. J. Santora, and M. M. Jamali, “Camera, lidar, and radar
    sensor fusion based on bayesian neural network (clr-bnn),” *IEEE Sensors Journal*,
    vol. 22, no. 7, pp. 6964–6974, 2022.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] R. Ravindran, M. J. Santora, 和 M. M. Jamali, “基于贝叶斯神经网络的摄像头、激光雷达和雷达传感器融合（clr-bnn）”，*IEEE
    Sensors Journal*，第22卷，第7期，第6964–6974页，2022年。'
- en: '[147] L. Wang, T. Chen, C. Anklam, and B. Goldluecke, “High dimensional frustum
    pointnet for 3d object detection from camera, lidar, and radar,” in *2020 IEEE
    Intelligent Vehicles Symposium (IV)*.   IEEE, 2020, pp. 1621–1628.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] L. Wang, T. Chen, C. Anklam, 和 B. Goldluecke，“用于从摄像头、激光雷达和雷达进行3D物体检测的高维度视锥体点网”，在*2020
    IEEE智能车辆研讨会（IV）*。IEEE，2020年，第1621–1628页。'
- en: '[148] J. Cao, X. Weng, R. Khirodkar, J. Pang, and K. Kitani, “Observation-centric
    sort: Rethinking sort for robust multi-object tracking,” *arXiv preprint arXiv:2203.14360*,
    2022.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] J. Cao, X. Weng, R. Khirodkar, J. Pang, 和 K. Kitani，“以观测为中心的SORT: 重新思考SORT以增强多目标跟踪的鲁棒性”，*arXiv预印本
    arXiv:2203.14360*，2022年。'
- en: '[149] J. Xu, Y. Cao, Z. Zhang, and H. Hu, “Spatial-temporal relation networks
    for multi-object tracking,” in *Proceedings of the IEEE/CVF international conference
    on computer vision*, 2019, pp. 3988–3998.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] J. Xu, Y. Cao, Z. Zhang, 和 H. Hu，“用于多目标跟踪的时空关系网络”，在*IEEE/CVF国际计算机视觉会议论文集*，2019年，第3988–3998页。'
- en: '[150] P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar, A. Geiger,
    and B. Leibe, “Mots: Multi-object tracking and segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 7942–7951.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar, A. Geiger,
    和 B. Leibe，“MOTS: 多目标跟踪与分割”，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第7942–7951页。'
- en: '[151] Y. Liu, I. E. Zulfikar, J. Luiten, A. Dave, D. Ramanan, B. Leibe, A. Ošep,
    and L. Leal-Taixé, “Opening up open world tracking,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2022, pp. 19 045–19 055.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Y. Liu, I. E. Zulfikar, J. Luiten, A. Dave, D. Ramanan, B. Leibe, A.
    Ošep, 和 L. Leal-Taixé，“开放世界跟踪的开启”，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第19,045–19,055页。'
- en: '[152] Y. Zhang and X. Li, “Multiple dynamic object tracking for visual slam,”
    in *2022 4th International Conference on Robotics and Computer Vision (ICRCV)*.   IEEE,
    2022, pp. 49–55.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Y. Zhang 和 X. Li，"视觉SLAM中的多目标动态跟踪"，见于*2022年第4届国际机器人与计算机视觉大会 (ICRCV)*。
    IEEE，2022年，第49–55页。'
- en: '[153] X. Zhou, V. Koltun, and P. Krähenbühl, “Tracking objects as points,”
    in *European Conference on Computer Vision*.   Springer, 2020, pp. 474–490.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] X. Zhou, V. Koltun, 和 P. Krähenbühl，"将物体跟踪为点"，见于*欧洲计算机视觉会议*。 Springer，2020年，第474–490页。'
- en: '[154] A. Osep, W. Mehner, M. Mathias, and B. Leibe, “Combined image-and world-space
    tracking in traffic scenes,” in *2017 IEEE International Conference on Robotics
    and Automation (ICRA)*.   IEEE, 2017, pp. 1988–1995.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] A. Osep, W. Mehner, M. Mathias, 和 B. Leibe，"交通场景中的结合图像和世界空间跟踪"，见于*2017
    IEEE 国际机器人与自动化会议 (ICRA)*。 IEEE，2017年，第1988–1995页。'
- en: '[155] U. Nguyen and C. Heipke, “3d pedestrian tracking using local structure
    constraints,” *ISPRS Journal of Photogrammetry and Remote Sensing*, vol. 166,
    pp. 347–358, 2020.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] U. Nguyen 和 C. Heipke，"使用局部结构约束进行3D行人跟踪"，*ISPRS 摄影测量与遥感杂志*，第166卷，第347–358页，2020年。'
- en: '[156] S. Wang, R. Clark, H. Wen, and N. Trigoni, “Deepvo: Towards end-to-end
    visual odometry with deep recurrent convolutional neural networks,” in *2017 IEEE
    international conference on robotics and automation (ICRA)*.   IEEE, 2017, pp.
    2043–2050.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] S. Wang, R. Clark, H. Wen, 和 N. Trigoni，"Deepvo: 迈向基于深度递归卷积神经网络的端到端视觉里程计"，见于*2017
    IEEE 国际机器人与自动化会议 (ICRA)*。 IEEE，2017年，第2043–2050页。'
- en: '[157] F. Xue, X. Wang, S. Li, Q. Wang, J. Wang, and H. Zha, “Beyond tracking:
    Selecting memory and refining poses for deep visual odometry,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 8575–8583.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] F. Xue, X. Wang, S. Li, Q. Wang, J. Wang, 和 H. Zha，"超越跟踪：为深度视觉里程计选择记忆和优化姿态"，见于*IEEE/CVF
    计算机视觉与模式识别会议论文集*，2019年，第8575–8583页。'
- en: '[158] A. CS Kumar, S. M. Bhandarkar, and M. Prasad, “Depthnet: A recurrent
    neural network architecture for monocular depth prediction,” in *Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition Workshops*, 2018,
    pp. 283–291.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] A. CS Kumar, S. M. Bhandarkar, 和 M. Prasad，"Depthnet: 一种用于单目深度预测的递归神经网络架构"，见于*IEEE
    计算机视觉与模式识别会议研讨会论文集*，2018年，第283–291页。'
- en: '[159] V. Patil, W. Van Gansbeke, D. Dai, and L. Van Gool, “Don’t forget the
    past: Recurrent depth estimation from monocular video,” *IEEE Robotics and Automation
    Letters*, vol. 5, no. 4, pp. 6813–6820, 2020.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] V. Patil, W. Van Gansbeke, D. Dai, 和 L. Van Gool，"不要忘记过去：基于单目视频的递归深度估计"，*IEEE
    机器人与自动化通讯*，第5卷，第4期，第6813–6820页，2020年。'
- en: '[160] H. Zhang, C. Shen, Y. Li, Y. Cao, Y. Liu, and Y. Yan, “Exploiting temporal
    consistency for real-time video depth estimation,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2019, pp. 1725–1734.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] H. Zhang, C. Shen, Y. Li, Y. Cao, Y. Liu, 和 Y. Yan，"利用时间一致性进行实时视频深度估计"，见于*IEEE/CVF
    国际计算机视觉大会论文集*，2019年，第1725–1734页。'
- en: '[161] P. Sun, J. Cao, Y. Jiang, R. Zhang, E. Xie, Z. Yuan, C. Wang, and P. Luo,
    “Transtrack: Multiple object tracking with transformer,” *arXiv preprint arXiv:2012.15460*,
    2020.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] P. Sun, J. Cao, Y. Jiang, R. Zhang, E. Xie, Z. Yuan, C. Wang, 和 P. Luo，"Transtrack:
    基于变换器的多目标跟踪"，*arXiv 预印本 arXiv:2012.15460*，2020年。'
- en: '[162] F. Zeng, B. Dong, T. Wang, X. Zhang, and Y. Wei, “Motr: End-to-end multiple-object
    tracking with transformer,” *arXiv preprint arXiv:2105.03247*, 2021.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] F. Zeng, B. Dong, T. Wang, X. Zhang, 和 Y. Wei，"Motr: 基于变换器的端到端多目标跟踪"，*arXiv
    预印本 arXiv:2105.03247*，2021年。'
- en: '[163] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer, “Trackformer:
    Multi-object tracking with transformers,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 8844–8854.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] T. Meinhardt, A. Kirillov, L. Leal-Taixe, 和 C. Feichtenhofer，"Trackformer:
    基于变换器的多目标跟踪"，见于*IEEE/CVF 计算机视觉与模式识别会议论文集*，2022年，第8844–8854页。'
- en: '[164] J. Watson, O. Mac Aodha, V. Prisacariu, G. Brostow, and M. Firman, “The
    temporal opportunist: Self-supervised multi-frame monocular depth,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 1164–1174.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] J. Watson, O. Mac Aodha, V. Prisacariu, G. Brostow, 和 M. Firman，"时间机会主义者：自监督的多帧单目深度"，见于*IEEE/CVF
    计算机视觉与模式识别会议论文集*，2021年，第1164–1174页。'
- en: '[165] C. Godard, O. Mac Aodha, M. Firman, and G. J. Brostow, “Digging into
    self-supervised monocular depth estimation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 3828–3838.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] C. Godard, O. Mac Aodha, M. Firman, 和 G. J. Brostow，"深入研究自监督的单目深度估计"，见于*IEEE/CVF
    国际计算机视觉大会论文集*，2019年，第3828–3838页。'
- en: '[166] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised learning
    of depth and ego-motion from video,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2017, pp. 1851–1858.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] T. Zhou, M. Brown, N. Snavely, 和 D. G. Lowe, “从视频中无监督学习深度和自我运动，” 在 *IEEE
    计算机视觉与模式识别会议论文集*，2017年，第1851–1858页。'
- en: '[167] Z. Yin and J. Shi, “Geonet: Unsupervised learning of dense depth, optical
    flow and camera pose,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 1983–1992.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Z. Yin 和 J. Shi, “GeoNet: 无监督学习密集深度、光流和相机姿态，” 在 *IEEE 计算机视觉与模式识别会议论文集*，2018年，第1983–1992页。'
- en: '[168] L. Wang, Y. Gong, Q. Wang, K. Zhou, and L. Chen, “Flora: dual-frequency
    loss-compensated real-time monocular 3d video reconstruction,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, 2023.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] L. Wang, Y. Gong, Q. Wang, K. Zhou, 和 L. Chen, “FLORA: 双频率损失补偿实时单目三维视频重建，”
    在 *AAAI 人工智能大会论文集*，2023年。'
- en: '[169] L. Chen, Z. Ling, Y. Gao, R. Sun, and S. Jin, “A real-time semantic visual
    slam for dynamic environment based on deep learning and dynamic probabilistic
    propagation,” *Complex & Intelligent Systems*, pp. 1–25, 2023.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] L. Chen, Z. Ling, Y. Gao, R. Sun, 和 S. Jin, “基于深度学习和动态概率传播的动态环境实时语义视觉SLAM，”
    *复杂与智能系统*，第1–25页，2023年。'
- en: '[170] H. Zhou, B. Ummenhofer, and T. Brox, “Deeptam: Deep tracking and mapping,”
    in *Proceedings of the European conference on computer vision (ECCV)*, 2018, pp.
    822–838.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] H. Zhou, B. Ummenhofer, 和 T. Brox, “DeepTAM: 深度跟踪与映射，” 在 *欧洲计算机视觉会议 (ECCV)
    论文集*，2018年，第822–838页。'
- en: '[171] S. Li, F. Xue, X. Wang, Z. Yan, and H. Zha, “Sequential adversarial learning
    for self-supervised deep visual odometry,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 2851–2860.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] S. Li, F. Xue, X. Wang, Z. Yan, 和 H. Zha, “自监督深度视觉里程计的序列对抗学习，” 在 *IEEE/CVF
    国际计算机视觉会议论文集*，2019年，第2851–2860页。'
- en: '[172] W. Wang, Y. Hu, and S. Scherer, “Tartanvo: A generalizable learning-based
    vo,” *arXiv preprint arXiv:2011.00359*, 2020.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] W. Wang, Y. Hu, 和 S. Scherer, “TartanVO: 一种可推广的基于学习的视觉里程计，” *arXiv 预印本
    arXiv:2011.00359*, 2020。'
- en: '[173] T. Yin, X. Zhou, and P. Krahenbuhl, “Center-based 3d object detection
    and tracking,” in *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*, 2021, pp. 11 784–11 793.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] T. Yin, X. Zhou, 和 P. Krahenbuhl, “基于中心的三维物体检测与跟踪，” 在 *IEEE/CVF 计算机视觉与模式识别大会论文集*，2021年，第11,784–11,793页。'
- en: '[174] W. Wang, X. Chang, J. Yang, and G. Xu, “Lidar-based dense pedestrian
    detection and tracking,” *Applied Sciences*, vol. 12, no. 4, p. 1799, 2022.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] W. Wang, X. Chang, J. Yang, 和 G. Xu, “基于激光雷达的密集行人检测与跟踪，” *应用科学*，第12卷，第4期，第1799页，2022年。'
- en: '[175] X. Weng, J. Wang, D. Held, and K. Kitani, “3d multi-object tracking:
    A baseline and new evaluation metrics,” in *2020 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*.   IEEE, 2020, pp. 10 359–10 366.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] X. Weng, J. Wang, D. Held, 和 K. Kitani, “三维多物体跟踪：基线与新评估指标，” 在 *2020 IEEE/RSJ
    智能机器人与系统国际会议 (IROS)*。 IEEE，2020年，第10,359–10,366页。'
- en: '[176] H.-k. Chiu, A. Prioletti, J. Li, and J. Bohg, “Probabilistic 3d multi-object
    tracking for autonomous driving,” *arXiv preprint arXiv:2001.05673*, 2020.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] H.-k. Chiu, A. Prioletti, J. Li, 和 J. Bohg, “用于自主驾驶的概率性三维多物体跟踪，” *arXiv
    预印本 arXiv:2001.05673*, 2020。'
- en: '[177] X. Chen, S. Shi, B. Zhu, K. C. Cheung, H. Xu, and H. Li, “Mppnet: Multi-frame
    feature intertwining with proxy points for 3d temporal object detection,” *arXiv
    preprint arXiv:2205.05979*, 2022.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] X. Chen, S. Shi, B. Zhu, K. C. Cheung, H. Xu, 和 H. Li, “MPPNet: 多帧特征与代理点交织的三维时间物体检测，”
    *arXiv 预印本 arXiv:2205.05979*, 2022。'
- en: '[178] J. Gao, X. Yan, W. Zhao, Z. Lyu, Y. Liao, and C. Zheng, “Spatio-temporal
    contextual learning for single object tracking on point clouds,” *IEEE Transactions
    on Neural Networks and Learning Systems*, 2023.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] J. Gao, X. Yan, W. Zhao, Z. Lyu, Y. Liao, 和 C. Zheng, “基于点云的单物体跟踪的时空上下文学习，”
    *IEEE 神经网络与学习系统汇刊*，2023年。'
- en: '[179] Y. Li, C. R. Qi, Y. Zhou, C. Liu, and D. Anguelov, “Modar: Using motion
    forecasting for 3d object detection in point cloud sequences,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023,
    pp. 9329–9339.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Y. Li, C. R. Qi, Y. Zhou, C. Liu, 和 D. Anguelov, “MODAR: 利用运动预测进行点云序列中的三维物体检测，”
    在 *IEEE/CVF 计算机视觉与模式识别大会论文集*，2023年，第9,329–9,339页。'
- en: '[180] W. Wang, X. You, J. Yang, M. Su, L. Zhang, Z. Yang, and Y. Kuang, “Lidar-based
    real-time panoptic segmentation via spatiotemporal sequential data fusion,” *Remote
    Sensing*, vol. 14, no. 8, p. 1775, 2022.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] W. Wang, X. You, J. Yang, M. Su, L. Zhang, Z. Yang, 和 Y. Kuang, “基于激光雷达的实时全景分割通过时空序列数据融合，”
    *遥感*，第14卷，第8期，第1775页，2022年。'
- en: '[181] F. Duerr, M. Pfaller, H. Weigel, and J. Beyerer, “Lidar-based recurrent
    3d semantic segmentation with temporal memory alignment,” in *2020 International
    Conference on 3D Vision (3DV)*.   IEEE, 2020, pp. 781–790.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] F. Duerr, M. Pfaller, H. Weigel, 和 J. Beyerer，“基于激光雷达的递归3d语义分割与时间记忆对齐”，见于
    *2020国际3D视觉会议 (3DV)*。IEEE, 2020, 第781–790页。'
- en: '[182] Q. Li, S. Chen, C. Wang, X. Li, C. Wen, M. Cheng, and J. Li, “Lo-net:
    Deep real-time lidar odometry,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2019, pp. 8473–8482.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Q. Li, S. Chen, C. Wang, X. Li, C. Wen, M. Cheng, 和 J. Li，“Lo-net: 深度实时激光雷达里程计”，见于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第8473–8482页。'
- en: '[183] J. Kümmerle, T. Kühner, and M. Lauer, “Automatic calibration of multiple
    cameras and depth sensors with a spherical target,” in *2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*.   IEEE, 2018, pp. 1–8.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] J. Kümmerle, T. Kühner, 和 M. Lauer，“使用球形目标的多相机和深度传感器的自动标定”，见于 *2018 IEEE/RSJ国际智能机器人与系统会议
    (IROS)*。IEEE, 2018, 第1–8页。'
- en: '[184] Y. Xiao, F. Codevilla, A. Gurram, O. Urfalioglu, and A. M. López, “Multimodal
    end-to-end autonomous driving,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2020.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Y. Xiao, F. Codevilla, A. Gurram, O. Urfalioglu, 和 A. M. López，“多模态端到端自动驾驶”，*IEEE智能交通系统汇刊*，2020年。'
- en: '[185] A. Dinesh Kumar, R. Karthika, and K. Soman, “Stereo camera and lidar
    sensor fusion-based collision warning system for autonomous vehicles,” in *Advances
    in Computational Intelligence Techniques*.   Springer, 2020, pp. 239–252.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] A. Dinesh Kumar, R. Karthika, 和 K. Soman，“基于立体相机和激光雷达传感器融合的自动驾驶车辆碰撞警告系统”，见于
    *计算智能技术进展*。Springer, 2020, 第239–252页。'
- en: '[186] O. Natan and J. Miura, “Towards compact autonomous driving perception
    with balanced learning and multi-sensor fusion,” *IEEE Transactions on Intelligent
    Transportation Systems*, 2022.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] O. Natan 和 J. Miura，“通过平衡学习和多传感器融合实现紧凑型自动驾驶感知”，*IEEE智能交通系统汇刊*，2022年。'
- en: '[187] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, and S. Han, “Bevfusion:
    Multi-task multi-sensor fusion with unified bird’s-eye view representation,” *arXiv
    preprint arXiv:2205.13542*, 2022.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, 和 S. Han，“Bevfusion:
    具有统一鸟瞰图表示的多任务多传感器融合”，*arXiv预印本 arXiv:2205.13542*，2022年。'
- en: '[188] S. Wu, A. Hadachi, D. Vivet, and Y. Prabhakar, “This is the way: Sensors
    auto-calibration approach based on deep learning for self-driving cars,” *IEEE
    Sensors Journal*, vol. 21, no. 24, pp. 27 779–27 788, 2021.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] S. Wu, A. Hadachi, D. Vivet, 和 Y. Prabhakar，“这就是方法：基于深度学习的传感器自动标定方法用于自动驾驶汽车”，*IEEE传感器期刊*，第21卷，第24期，第27 779–27 788页，2021年。'
- en: '[189] G. Wang, B. Tian, Y. Zhang, L. Chen, D. Cao, and J. Wu, “Multi-view adaptive
    fusion network for 3d object detection,” *arXiv preprint arXiv:2011.00652*, 2020.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] G. Wang, B. Tian, Y. Zhang, L. Chen, D. Cao, 和 J. Wu，“用于3d物体检测的多视角自适应融合网络”，*arXiv预印本
    arXiv:2011.00652*，2020年。'
- en: '[190] Z. Wu, G. Chen, Y. Gan, L. Wang, and J. Pu, “Mvfusion: Multi-view 3d
    object detection with semantic-aligned radar and camera fusion,” *arXiv preprint
    arXiv:2302.10511*, 2023.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Z. Wu, G. Chen, Y. Gan, L. Wang, 和 J. Pu，“Mvfusion: 具有语义对齐雷达和相机融合的多视角3d物体检测”，*arXiv预印本
    arXiv:2302.10511*，2023年。'
- en: '[191] L. Lou, Y. Li, Q. Zhang, and H. Wei, “Slam and 3d semantic reconstruction
    based on the fusion of lidar and monocular vision,” *Sensors*, vol. 23, no. 3,
    p. 1502, 2023.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] L. Lou, Y. Li, Q. Zhang, 和 H. Wei，“基于激光雷达和单目视觉融合的SLAM和3d语义重建”，*传感器*，第23卷，第3期，第1502页，2023年。'
- en: '[192] X. Chen, T. Zhang, Y. Wang, Y. Wang, and H. Zhao, “Futr3d: A unified
    sensor fusion framework for 3d detection,” *arXiv preprint arXiv:2203.10642*,
    2022.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] X. Chen, T. Zhang, Y. Wang, Y. Wang, 和 H. Zhao，“Futr3d: 一个统一的传感器融合框架用于3d检测”，*arXiv预印本
    arXiv:2203.10642*，2022年。'
- en: '[193] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Q. Yu, and J. Dai, “Bevformer:
    Learning bird’s-eye-view representation from multi-camera images via spatiotemporal
    transformers,” *arXiv preprint arXiv:2203.17270*, 2022.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Q. Yu, 和 J. Dai，“Bevformer:
    通过时空变换器从多相机图像中学习鸟瞰图表示”，*arXiv预印本 arXiv:2203.17270*，2022年。'
- en: '[194] Y. B. Can, A. Liniger, O. Unal, D. Paudel, and L. Van Gool, “Understanding
    bird’s-eye view semantic hd-maps using an onboard monocular camera,” *arXiv preprint
    arXiv:2012.03040*, 2020.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Y. B. Can, A. Liniger, O. Unal, D. Paudel, 和 L. Van Gool，“利用车载单目相机理解鸟瞰图语义高清地图”，*arXiv预印本
    arXiv:2012.03040*，2020年。'
- en: '[195] X. Xiong, Y. Liu, T. Yuan, Y. Wang, Y. Wang, and H. Zhao, “Neural map
    prior for autonomous driving,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2023, pp. 17 535–17 544.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] X. Xiong, Y. Liu, T. Yuan, Y. Wang, Y. Wang, 和 H. Zhao，“用于自动驾驶的神经地图先验，”发表于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023年，第17 535–17 544页。'
- en: '[196] X. Zhu, X. Cao, Z. Dong, C. Zhou, Q. Liu, W. Li, and Y. Wang, “Nemo:
    Neural map growing system for spatiotemporal fusion in bird’s-eye-view and bdd-map
    benchmark,” *arXiv preprint arXiv:2306.04540*, 2023.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] X. Zhu, X. Cao, Z. Dong, C. Zhou, Q. Liu, W. Li, 和 Y. Wang，“Nemo: 用于鸟瞰视角和BDD地图基准的时空融合的神经地图增长系统，”
    *arXiv 预印本 arXiv:2306.04540*，2023年。'
- en: '[197] Y. Liu, J. Yan, F. Jia, S. Li, Q. Gao, T. Wang, X. Zhang, and J. Sun,
    “Petrv2: A unified framework for 3d perception from multi-camera images,” *arXiv
    preprint arXiv:2206.01256*, 2022.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Y. Liu, J. Yan, F. Jia, S. Li, Q. Gao, T. Wang, X. Zhang, 和 J. Sun，“Petrv2:
    一个统一的3D感知框架用于多摄像头图像，” *arXiv 预印本 arXiv:2206.01256*，2022年。'
- en: '[198] J. Huang and G. Huang, “Bevdet4d: Exploit temporal cues in multi-camera
    3d object detection,” *arXiv preprint arXiv:2203.17054*, 2022.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] J. Huang 和 G. Huang，“Bevdet4d: 在多摄像头3D物体检测中利用时间线索，” *arXiv 预印本 arXiv:2203.17054*，2022年。'
- en: '[199] Y. Zhang, Z. Zhu, W. Zheng, J. Huang, G. Huang, J. Zhou, and J. Lu, “Beverse:
    Unified perception and prediction in birds-eye-view for vision-centric autonomous
    driving,” *arXiv preprint arXiv:2205.09743*, 2022.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Y. Zhang, Z. Zhu, W. Zheng, J. Huang, G. Huang, J. Zhou, 和 J. Lu，“Beverse:
    统一感知与预测在鸟瞰视角中的视觉中心自动驾驶，” *arXiv 预印本 arXiv:2205.09743*，2022年。'
- en: '[200] A. Hu, Z. Murez, N. Mohan, S. Dudas, J. Hawke, V. Badrinarayanan, R. Cipolla,
    and A. Kendall, “Fiery: Future instance prediction in bird’s-eye view from surround
    monocular cameras,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2021, pp. 15 273–15 282.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] A. Hu, Z. Murez, N. Mohan, S. Dudas, J. Hawke, V. Badrinarayanan, R.
    Cipolla, 和 A. Kendall，“Fiery: 从周围单目相机的鸟瞰视角进行未来实例预测，”发表于 *IEEE/CVF 国际计算机视觉会议论文集*，2021年，第15 273–15 282页。'
- en: '[201] Y. Wang, Y. Chen, and Z. Zhang, “Frustumformer: Adaptive instance-aware
    resampling for multi-view 3d detection,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2023, pp. 5096–5105.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Y. Wang, Y. Chen, 和 Z. Zhang，“Frustumformer: 自适应实例感知重采样用于多视角3D检测，”发表于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023年，第5096–5105页。'
- en: '[202] R. Li, S. Wang, Z. Long, and D. Gu, “Undeepvo: Monocular visual odometry
    through unsupervised deep learning,” in *2018 IEEE international conference on
    robotics and automation (ICRA)*.   IEEE, 2018, pp. 7286–7291.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] R. Li, S. Wang, Z. Long, 和 D. Gu，“Undeepvo: 通过无监督深度学习进行单目视觉测距，”发表于 *2018
    IEEE 国际机器人与自动化会议 (ICRA)*。 IEEE，2018年，第7286–7291页。'
- en: '[203] P. Li, J. Shi, and S. Shen, “Joint spatial-temporal optimization for
    stereo 3d object tracking,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 6877–6886.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] P. Li, J. Shi, 和 S. Shen，“立体3D物体跟踪的联合时空优化，”发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020年，第6877–6886页。'
- en: '[204] Q. Jia, Y. Pu, J. Chen, J. Cheng, C. Liao, and X. Yang, “D${}^{\mbox{2}}$vo:
    Monocular deep direct visual odometry,” in *IEEE/RSJ International Conference
    on Intelligent Robots and Systems, IROS 2020, Las Vegas, NV, USA, October 24,
    2020 - January 24, 2021*.   IEEE, 2020, pp. 10 158–10 165\. [Online]. Available:
    [https://doi.org/10.1109/IROS45743.2020.9341313](https://doi.org/10.1109/IROS45743.2020.9341313)'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Q. Jia, Y. Pu, J. Chen, J. Cheng, C. Liao, 和 X. Yang，“D${}^{\mbox{2}}$vo:
    单目深度直接视觉测距，”发表于 *IEEE/RSJ 国际机器人与系统会议，IROS 2020，拉斯维加斯，NV，美国，2020年10月24日 - 2021年1月24日*。
    IEEE，2020年，第10 158–10 165页。[在线]. 可用： [https://doi.org/10.1109/IROS45743.2020.9341313](https://doi.org/10.1109/IROS45743.2020.9341313)'
- en: '[205] C. Chen, S. Rosa, Y. Miao, C. X. Lu, W. Wu, A. Markham, and N. Trigoni,
    “Selective sensor fusion for neural visual-inertial odometry,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 10 542–10 551.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] C. Chen, S. Rosa, Y. Miao, C. X. Lu, W. Wu, A. Markham, 和 N. Trigoni，“用于神经视觉-惯性测距的选择性传感器融合，”发表于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，2019年，第10 542–10 551页。'
- en: '[206] R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, “Vinet: Visual-inertial
    odometry as a sequence-to-sequence learning problem,” in *Proceedings of the AAAI
    Conference on Artificial Intelligence*, vol. 31, no. 1, 2017.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] R. Clark, S. Wang, H. Wen, A. Markham, 和 N. Trigoni，“Vinet: 视觉-惯性测距作为序列到序列学习问题，”发表于
    *AAAI 人工智能会议论文集*，第31卷，第1期，2017年。'
- en: '[207] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, and I. D. Reid,
    “Unsupervised learning of monocular depth estimation and visual odometry with
    deep feature reconstruction,” in *2018 IEEE Conference on Computer Vision and
    Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*.   Computer
    Vision Foundation / IEEE Computer Society, 2018, pp. 340–349\. [Online]. Available:
    [http://openaccess.thecvf.com/content_cvpr_2018/html/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.html](http://openaccess.thecvf.com/content_cvpr_2018/html/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.html)'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, 和 I. D. Reid，“无监督学习单目深度估计和视觉里程计与深度特征重建”，见于
    *2018 IEEE 计算机视觉与模式识别大会（CVPR 2018），美国犹他州盐湖城，2018年6月18-22日*。计算机视觉基金会 / IEEE 计算机学会，2018，第340-349页。[在线]。可用链接：[http://openaccess.thecvf.com/content_cvpr_2018/html/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.html](http://openaccess.thecvf.com/content_cvpr_2018/html/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.html)'
- en: '[208] A. Düzçeker, S. Galliani, C. Vogel, P. Speciale, M. Dusmanu, and M. Pollefeys,
    “Deepvideomvs: Multi-view stereo on video with recurrent spatio-temporal fusion,”
    in *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual,
    June 19-25, 2021*.   Computer Vision Foundation / IEEE, 2021, pp. 15 324–15 333\.
    [Online]. Available: [https://openaccess.thecvf.com/content/CVPR2021/html/Duzceker_DeepVideoMVS_Multi-View_Stereo_on_Video_With_Recurrent_Spatio-Temporal_Fusion_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Duzceker_DeepVideoMVS_Multi-View_Stereo_on_Video_With_Recurrent_Spatio-Temporal_Fusion_CVPR_2021_paper.html)'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] A. Düzçeker, S. Galliani, C. Vogel, P. Speciale, M. Dusmanu, 和 M. Pollefeys，“Deepvideomvs：视频中的多视角立体视觉与递归时空融合”，见于
    *IEEE 计算机视觉与模式识别大会（CVPR 2021），线上，2021年6月19-25日*。计算机视觉基金会 / IEEE，2021，第15,324-15,333页。[在线]。可用链接：[https://openaccess.thecvf.com/content/CVPR2021/html/Duzceker_DeepVideoMVS_Multi-View_Stereo_on_Video_With_Recurrent_Spatio-Temporal_Fusion_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Duzceker_DeepVideoMVS_Multi-View_Stereo_on_Video_With_Recurrent_Spatio-Temporal_Fusion_CVPR_2021_paper.html)'
- en: '[209] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, “Neuralrecon: Real-time
    coherent 3d reconstruction from monocular video,” in *IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021*.   Computer
    Vision Foundation / IEEE, 2021, pp. 15 598–15 607\. [Online]. Available: [https://openaccess.thecvf.com/content/CVPR2021/html/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.html)'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] J. Sun, Y. Xie, L. Chen, X. Zhou, 和 H. Bao，“Neuralrecon：来自单目视频的实时一致性3D重建”，见于
    *IEEE 计算机视觉与模式识别大会（CVPR 2021），线上，2021年6月19-25日*。计算机视觉基金会 / IEEE，2021，第15,598-15,607页。[在线]。可用链接：[https://openaccess.thecvf.com/content/CVPR2021/html/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.html)'
- en: '[210] N. Stier, A. Rich, P. Sen, and T. Höllerer, “Vortx: Volumetric 3d reconstruction
    with transformers for voxelwise view selection and fusion,” in *2021 International
    Conference on 3D Vision (3DV)*.   IEEE, 2021, pp. 320–330.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] N. Stier, A. Rich, P. Sen, 和 T. Höllerer，“Vortx：使用变换器进行体积3D重建，以实现体素级视图选择和融合”，见于
    *2021 国际3D视觉会议（3DV）*。IEEE，2021，第320-330页。'
- en: '[211] K. Wang and S. Shen, “Mvdepthnet: Real-time multiview depth estimation
    neural network,” in *2018 International conference on 3d vision (3DV)*.   IEEE,
    2018, pp. 248–257.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] K. Wang 和 S. Shen，“Mvdepthnet：实时多视角深度估计神经网络”，见于 *2018 国际3D视觉会议（3DV）*。IEEE，2018，第248-257页。'
- en: '[212] X. Long, L. Liu, C. Theobalt, and W. Wang, “Occlusion-aware depth estimation
    with adaptive normal constraints,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 640–657.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] X. Long, L. Liu, C. Theobalt, 和 W. Wang，“考虑遮挡的深度估计与自适应法线约束”，见于 *欧洲计算机视觉会议*。Springer，2020，第640-657页。'
- en: '[213] X. Long, L. Liu, W. Li, C. Theobalt, and W. Wang, “Multi-view depth estimation
    using epipolar spatio-temporal networks,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 8258–8267.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] X. Long, L. Liu, W. Li, C. Theobalt, 和 W. Wang，“使用极线时空网络的多视角深度估计”，见于
    *IEEE/CVF 计算机视觉与模式识别大会论文集*，2021，第8258-8267页。'
- en: '[214] A. Rich, N. Stier, P. Sen, and T. Höllerer, “3dvnet: Multi-view depth
    prediction and volumetric refinement,” in *2021 International Conference on 3D
    Vision (3DV)*.   IEEE, 2021, pp. 700–709.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] A. Rich, N. Stier, P. Sen, 和 T. Höllerer，“3dvnet：多视角深度预测和体积细化”，见于 *2021
    国际3D视觉会议（3DV）*。IEEE，2021，第700-709页。'
- en: '[215] X. Han, H. Liu, Y. Ding, and L. Yang, “Ro-map: Real-time multi-object
    mapping with neural radiance fields,” arXiv:2304.05735, 2023.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] X. Han, H. Liu, Y. Ding 和 L. Yang，“Ro-map：使用神经辐射场的实时多目标映射”，*arXiv:2304.05735*，2023。'
- en: '[216] A. Kim, A. Ošep, and L. Leal-Taixé, “Eagermot: 3d multi-object tracking
    via sensor fusion,” in *2021 IEEE International Conference on Robotics and Automation
    (ICRA)*.   IEEE, 2021, pp. 11 315–11 321.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] A. Kim, A. Ošep 和 L. Leal-Taixé，“Eagermot：通过传感器融合进行 3D 多目标跟踪”，发表于 *2021
    IEEE 国际机器人与自动化会议 (ICRA)*。IEEE，2021，第 11 315–11 321 页。'
- en: '[217] X. Weng, Y. Wang, Y. Man, and K. Kitani, “Gnn3dmot: Graph neural network
    for 3d multi-object tracking with multi-feature learning,” *arXiv preprint arXiv:2006.07327*,
    2020.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] X. Weng, Y. Wang, Y. Man 和 K. Kitani，“Gnn3dmot：用于 3D 多目标跟踪的图神经网络与多特征学习”，*arXiv
    预印本 arXiv:2006.07327*，2020。'
- en: '[218] D. Frossard and R. Urtasun, “End-to-end learning of multi-sensor 3d tracking
    by detection,” in *2018 IEEE international conference on robotics and automation
    (ICRA)*.   IEEE, 2018, pp. 635–642.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] D. Frossard 和 R. Urtasun，“通过检测进行多传感器 3D 跟踪的端到端学习”，发表于 *2018 IEEE 国际机器人与自动化会议
    (ICRA)*。IEEE，2018，第 635–642 页。'
- en: '[219] L. Wang, X. Zhang, W. Qin, X. Li, J. Gao, L. Yang, Z. Li, J. Li, L. Zhu,
    H. Wang *et al.*, “Camo-mot: Combined appearance-motion optimization for 3d multi-object
    tracking with camera-lidar fusion,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2023.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] L. Wang, X. Zhang, W. Qin, X. Li, J. Gao, L. Yang, Z. Li, J. Li, L. Zhu,
    H. Wang *等*，“Camo-mot：通过摄像头-激光雷达融合的 3D 多目标跟踪的联合外观-运动优化”，*IEEE 智能交通系统汇刊*，2023。'
- en: '[220] Y. Zeng, D. Zhang, C. Wang, Z. Miao, T. Liu, X. Zhan, D. Hao, and C. Ma,
    “Lift: Learning 4d lidar image fusion transformer for 3d object detection,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 17 172–17 181.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Y. Zeng, D. Zhang, C. Wang, Z. Miao, T. Liu, X. Zhan, D. Hao 和 C. Ma，“Lift：用于
    3D 目标检测的 4D 激光雷达图像融合变换器”，发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2022，第 17 172–17 181 页。'
- en: '[221] A. Shenoi, M. Patel, J. Gwak, P. Goebel, A. Sadeghian, H. Rezatofighi,
    R. Martín-Martín, and S. Savarese, “Jrmot: A real-time 3d multi-object tracker
    and a new large-scale dataset,” in *2020 IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS)*.   IEEE, 2020, pp. 10 335–10 342.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] A. Shenoi, M. Patel, J. Gwak, P. Goebel, A. Sadeghian, H. Rezatofighi,
    R. Martín-Martín 和 S. Savarese，“Jrmot：一个实时 3D 多目标跟踪器和一个新的大规模数据集”，发表于 *2020 IEEE/RSJ
    国际智能机器人与系统会议 (IROS)*。IEEE，2020，第 10 335–10 342 页。'
- en: '[222] W. Zhang, H. Zhou, S. Sun, Z. Wang, J. Shi, and C. C. Loy, “Robust multi-modality
    multi-object tracking,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2019, pp. 2365–2374.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] W. Zhang, H. Zhou, S. Sun, Z. Wang, J. Shi 和 C. C. Loy，“鲁棒的多模态多目标跟踪”，发表于
    *IEEE/CVF 国际计算机视觉会议论文集*，2019，第 2365–2374 页。'
- en: '[223] T. Kim and T.-H. Park, “Extended kalman filter (ekf) design for vehicle
    position tracking using reliability function of radar and lidar,” *Sensors*, vol. 20,
    no. 15, p. 4126, 2020.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] T. Kim 和 T.-H. Park，“用于车辆位置跟踪的扩展卡尔曼滤波器 (EKF) 设计，利用雷达和激光雷达的可靠性函数”，*传感器*，第
    20 卷，第 15 期，第 4126 页，2020。'
- en: '[224] Y. Liang, S. Müller, D. Schwendner, D. Rolle, D. Ganesch, and I. Schaffer,
    “A scalable framework for robust vehicle state estimation with a fusion of a low-cost
    imu, the gnss, radar, a camera and lidar,” in *2020 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*.   IEEE, 2020, pp. 1661–1668.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Y. Liang, S. Müller, D. Schwendner, D. Rolle, D. Ganesch 和 I. Schaffer，“一个可扩展的框架，通过融合低成本
    IMU、GNSS、雷达、摄像头和激光雷达来实现稳健的车辆状态估计”，发表于 *2020 IEEE/RSJ 国际智能机器人与系统会议 (IROS)*。IEEE，2020，第
    1661–1668 页。'
- en: '[225] B. Moshiri, H. G. Garakani *et al.*, “Pedestrian detection using image
    fusion and stereo vision in autonomous vehicles,” in *2018 9th International Symposium
    on Telecommunications (IST)*.   IEEE, 2018, pp. 592–596.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] B. Moshiri, H. G. Garakani *等*，“在自主车辆中使用图像融合和立体视觉进行行人检测”，发表于 *2018 第九届国际电信研讨会
    (IST)*。IEEE，2018，第 592–596 页。'
- en: '[226] H. Liu, Y. Yao, Z. Sun, X. Li, K. Jia, and Z. Tang, “Road segmentation
    with image-lidar data fusion in deep neural network,” *Multimedia Tools and Applications*,
    vol. 79, no. 47, pp. 35 503–35 518, 2020.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] H. Liu, Y. Yao, Z. Sun, X. Li, K. Jia 和 Z. Tang，“在深度神经网络中使用图像-激光雷达数据融合进行道路分割”，*多媒体工具与应用*，第
    79 卷，第 47 期，第 35 503–35 518 页，2020。'
- en: '[227] Y. Wei, H. Liu, T. Xie, Q. Ke, and Y. Guo, “Spatial-temporal transformer
    for 3d point cloud sequences,” in *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*, 2022, pp. 1171–1180.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] Y. Wei, H. Liu, T. Xie, Q. Ke 和 Y. Guo，“用于 3D 点云序列的时空变换器”，发表于 *IEEE/CVF
    冬季计算机视觉应用会议论文集*，2022，第 1171–1180 页。'
- en: '[228] Z. Teed and J. Deng, “Droid-slam: Deep visual slam for monocular, stereo,
    and rgb-d cameras,” *Advances in Neural Information Processing Systems*, vol. 34,
    2021.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Z. Teed 和 J. Deng，“Droid-slam: 深度视觉slam用于单目、立体和rgb-d相机，” *神经信息处理系统进展*，第34卷，2021年。'
- en: '[229] Y. Almalioglu, M. Turan, A. E. Sari, M. R. U. Saputra, P. P. de Gusmão,
    A. Markham, and N. Trigoni, “Selfvio: Self-supervised deep monocular visual-inertial
    odometry and depth estimation,” *arXiv preprint arXiv:1911.09968*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] Y. Almalioglu, M. Turan, A. E. Sari, M. R. U. Saputra, P. P. de Gusmão,
    A. Markham, 和 N. Trigoni，“Selfvio: 自监督深度单目视觉-惯性里程计和深度估计，” *arXiv预印本 arXiv:1911.09968*，2019年。'
- en: '[230] J.-X. Zhong, K. Zhou, Q. Hu, B. Wang, N. Trigoni, and A. Markham, “No
    pain, big gain: Classify dynamic point cloud sequences with static models by fitting
    feature-level space-time surfaces,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 8510–8520.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] J.-X. Zhong, K. Zhou, Q. Hu, B. Wang, N. Trigoni, 和 A. Markham，“无痛，巨大的收益：通过拟合特征级空间-时间曲面使用静态模型对动态点云序列进行分类，”
    见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，页8510–8520。'
- en: '[231] M. Bijelic, T. Gruber, F. Mannan, F. Kraus, W. Ritter, K. Dietmayer,
    and F. Heide, “Seeing through fog without seeing fog: Deep multimodal sensor fusion
    in unseen adverse weather,” in *2020 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*.   Computer
    Vision Foundation / IEEE, 2020, pp. 11 679–11 689\. [Online]. Available: [https://openaccess.thecvf.com/content_CVPR_2020/html/Bijelic_Seeing_Through_Fog_Without_Seeing_Fog_Deep_Multimodal_Sensor_Fusion_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Bijelic_Seeing_Through_Fog_Without_Seeing_Fog_Deep_Multimodal_Sensor_Fusion_CVPR_2020_paper.html)'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] M. Bijelic, T. Gruber, F. Mannan, F. Kraus, W. Ritter, K. Dietmayer,
    和 F. Heide，“在看不到雾的情况下透视雾：在未知恶劣天气下的深度多模态传感器融合，” 见于*2020 IEEE/CVF计算机视觉与模式识别会议，CVPR
    2020，华盛顿州西雅图，2020年6月13-19日*。 计算机视觉基金会 / IEEE，2020年，页11 679–11 689。 [在线]。可用：[https://openaccess.thecvf.com/content_CVPR_2020/html/Bijelic_Seeing_Through_Fog_Without_Seeing_Fog_Deep_Multimodal_Sensor_Fusion_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Bijelic_Seeing_Through_Fog_Without_Seeing_Fog_Deep_Multimodal_Sensor_Fusion_CVPR_2020_paper.html)'
- en: '[232] S. Fadadu, S. Pandey, D. Hegde, Y. Shi, F.-C. Chou, N. Djuric, and C. Vallespi-Gonzalez,
    “Multi-view fusion of sensor data for improved perception and prediction in autonomous
    driving,” in *Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision*, 2022, pp. 2349–2357.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] S. Fadadu, S. Pandey, D. Hegde, Y. Shi, F.-C. Chou, N. Djuric, 和 C. Vallespi-Gonzalez，“多视角传感器数据融合以提高自主驾驶中的感知和预测，”
    见于*IEEE/CVF冬季计算机视觉应用会议论文集*，2022年，页2349–2357。'
- en: '[233]'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233]'
- en: '[234] Y. Liu, T. Wang, X. Zhang, and J. Sun, “Petr: Position embedding transformation
    for multi-view 3d object detection,” *arXiv preprint arXiv:2203.05625*, 2022.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] Y. Liu, T. Wang, X. Zhang, 和 J. Sun，“Petr: 多视角3d目标检测的位置嵌入变换，” *arXiv预印本
    arXiv:2203.05625*，2022年。'
- en: '[235] M. Yasuda, Y. Ohishi, S. Saito, and N. Harado, “Multi-view and multi-modal
    event detection utilizing transformer-based multi-sensor fusion,” in *ICASSP 2022-2022
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE,
    2022, pp. 4638–4642.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] M. Yasuda, Y. Ohishi, S. Saito, 和 N. Harado，“利用基于变换器的多传感器融合进行多视角和多模态事件检测，”
    见于*ICASSP 2022-2022 IEEE国际声学、语音与信号处理会议（ICASSP）*。 IEEE，2022年，页4638–4642。'
- en: '[236] J. Liu and Y. Gao, “A multi-frame lane detection method based on deep
    learning,” in *International Conference on Cognitive Systems and Signal Processing*.   Springer,
    2021, pp. 247–260.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] J. Liu 和 Y. Gao，“一种基于深度学习的多帧车道检测方法，” 见于*国际认知系统与信号处理会议*。 Springer，2021年，页247–260。'
- en: '[237] Y. Chen and Z. Xiang, “Lane mark detection with pre-aligned spatial-temporal
    attention,” *Sensors*, vol. 22, no. 3, p. 794, 2022.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] Y. Chen 和 Z. Xiang，“带有预对齐空间-时间注意力的车道标记检测，” *传感器*，第22卷，第3期，页794，2022年。'
- en: '[238] Z. Yuan, X. Song, L. Bai, Z. Wang, and W. Ouyang, “Temporal-channel transformer
    for 3d lidar-based video object detection for autonomous driving,” *IEEE Transactions
    on Circuits and Systems for Video Technology*, vol. 32, no. 4, pp. 2068–2078,
    2021.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Z. Yuan, X. Song, L. Bai, Z. Wang, 和 W. Ouyang，“基于时间通道的变换器用于3d lidar基础的视频目标检测，”
    *IEEE电路与系统视频技术汇刊*，第32卷，第4期，页2068–2078，2021年。'
- en: '[239] X. Long, L. Liu, W. Li, C. Theobalt, and W. Wang, “Multi-view depth estimation
    using epipolar spatio-temporal networks,” in *IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021*.   Computer Vision
    Foundation / IEEE, 2021, pp. 8258–8267\. [Online]. Available: [https://openaccess.thecvf.com/content/CVPR2021/html/Long_Multi-view_Depth_Estimation_using_Epipolar_Spatio-Temporal_Networks_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Long_Multi-view_Depth_Estimation_using_Epipolar_Spatio-Temporal_Networks_CVPR_2021_paper.html)'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] X. Long, L. Liu, W. Li, C. Theobalt, 和 W. Wang，“基于极线时空网络的多视角深度估计”，见于*IEEE计算机视觉与模式识别会议，CVPR
    2021，虚拟会议，2021年6月19-25日*。计算机视觉基金会 / IEEE，2021年，页码8258–8267。 [在线]. 可用: [https://openaccess.thecvf.com/content/CVPR2021/html/Long_Multi-view_Depth_Estimation_using_Epipolar_Spatio-Temporal_Networks_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Long_Multi-view_Depth_Estimation_using_Epipolar_Spatio-Temporal_Networks_CVPR_2021_paper.html)'
- en: '[240] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan, “Mvsnet: Depth inference
    for unstructured multi-view stereo,” in *Computer Vision - ECCV 2018 - 15th European
    Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VIII*, ser.
    Lecture Notes in Computer Science, V. Ferrari, M. Hebert, C. Sminchisescu, and
    Y. Weiss, Eds., vol. 11212.   Springer, 2018, pp. 785–801\. [Online]. Available:
    [https://doi.org/10.1007/978-3-030-01237-3_47](https://doi.org/10.1007/978-3-030-01237-3_47)'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Y. Yao, Z. Luo, S. Li, T. Fang, 和 L. Quan，“Mvsnet：用于无结构多视角立体的深度推断”，见于*计算机视觉
    - ECCV 2018 - 第15届欧洲会议，德国慕尼黑，2018年9月8-14日，会议论文集，第八部分*，系列: 计算机科学讲义笔记，V. Ferrari,
    M. Hebert, C. Sminchisescu, 和 Y. Weiss 编辑，第11212卷。Springer，2018年，页码785–801。 [在线].
    可用: [https://doi.org/10.1007/978-3-030-01237-3_47](https://doi.org/10.1007/978-3-030-01237-3_47)'
- en: '[241] X. Gu, Z. Fan, S. Zhu, Z. Dai, F. Tan, and P. Tan, “Cascade cost volume
    for high-resolution multi-view stereo and stereo matching,” in *2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,
    USA, June 13-19, 2020*.   Computer Vision Foundation / IEEE, 2020, pp. 2492–2501.
    [Online]. Available: [https://openaccess.thecvf.com/content_CVPR_2020/html/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.html)'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] X. Gu, Z. Fan, S. Zhu, Z. Dai, F. Tan, 和 P. Tan，“用于高分辨率多视角立体和立体匹配的级联代价体积”，见于*2020
    IEEE/CVF计算机视觉与模式识别会议，CVPR 2020，华盛顿州西雅图，2020年6月13-19日*。计算机视觉基金会 / IEEE，2020年，页码2492–2501。
    [在线]. 可用: [https://openaccess.thecvf.com/content_CVPR_2020/html/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.html)'
- en: '[242] X. Ma, Y. Gong, Q. Wang, J. Huang, L. Chen, and F. Yu, “Epp-mvsnet: Epipolar-assembling
    based depth prediction for multi-view stereo,” in *2021 IEEE/CVF International
    Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17,
    2021*.   IEEE, 2021, pp. 5712–5720\. [Online]. Available: [https://doi.org/10.1109/ICCV48922.2021.00568](https://doi.org/10.1109/ICCV48922.2021.00568)'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] X. Ma, Y. Gong, Q. Wang, J. Huang, L. Chen, 和 F. Yu，“Epp-mvsnet：基于极线组装的多视角立体深度预测”，见于*2021
    IEEE/CVF国际计算机视觉会议，ICCV 2021，加拿大蒙特利尔，2021年10月10-17日*。IEEE，2021年，页码5712–5720。 [在线].
    可用: [https://doi.org/10.1109/ICCV48922.2021.00568](https://doi.org/10.1109/ICCV48922.2021.00568)'
- en: '[243] Z. Murez, T. van As, J. Bartolozzi, A. Sinha, V. Badrinarayanan, and
    A. Rabinovich, “Atlas: End-to-end 3d scene reconstruction from posed images,”
    in *Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August
    23-28, 2020, Proceedings, Part VII*, ser. Lecture Notes in Computer Science, A. Vedaldi,
    H. Bischof, T. Brox, and J. Frahm, Eds., vol. 12352.   Springer, 2020, pp. 414–431\.
    [Online]. Available: [https://doi.org/10.1007/978-3-030-58571-6_25](https://doi.org/10.1007/978-3-030-58571-6_25)'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Z. Murez, T. van As, J. Bartolozzi, A. Sinha, V. Badrinarayanan, 和 A.
    Rabinovich，“Atlas：从姿态图像到端到端3D场景重建”，见于*计算机视觉 - ECCV 2020 - 第16届欧洲会议，英国格拉斯哥，2020年8月23-28日，会议论文集，第七部分*，系列:
    计算机科学讲义笔记，A. Vedaldi, H. Bischof, T. Brox, 和 J. Frahm 编辑，第12352卷。Springer，2020年，页码414–431。
    [在线]. 可用: [https://doi.org/10.1007/978-3-030-58571-6_25](https://doi.org/10.1007/978-3-030-58571-6_25)'
- en: '[244] J. Yang, W. Mao, J. M. Alvarez, and M. Liu, “Cost volume pyramid based
    depth inference for multi-view stereo,” in *2020 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*.   Computer
    Vision Foundation / IEEE, 2020, pp. 4876–4885\. [Online]. Available: [https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Cost_Volume_Pyramid_Based_Depth_Inference_for_Multi-View_Stereo_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Cost_Volume_Pyramid_Based_Depth_Inference_for_Multi-View_Stereo_CVPR_2020_paper.html)'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] J. Yang, W. Mao, J. M. Alvarez, 和 M. Liu, “基于成本体积金字塔的多视图立体深度推断，”在 *2020
    IEEE/CVF 计算机视觉与模式识别会议，CVPR 2020，美国华盛顿州西雅图，2020年6月13-19日*。 计算机视觉基金会 / IEEE，2020，第4876–4885页。
    [在线]. 可用: [https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Cost_Volume_Pyramid_Based_Depth_Inference_for_Multi-View_Stereo_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Cost_Volume_Pyramid_Based_Depth_Inference_for_Multi-View_Stereo_CVPR_2020_paper.html)'
- en: '[245] S. Cheng, Z. Xu, S. Zhu, Z. Li, L. E. Li, R. Ramamoorthi, and H. Su,
    “Deep stereo using adaptive thin volume representation with uncertainty awareness,”
    in *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR
    2020, Seattle, WA, USA, June 13-19, 2020*.   Computer Vision Foundation / IEEE,
    2020, pp. 2521–2531\. [Online]. Available: [https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Deep_Stereo_Using_Adaptive_Thin_Volume_Representation_With_Uncertainty_Awareness_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Deep_Stereo_Using_Adaptive_Thin_Volume_Representation_With_Uncertainty_Awareness_CVPR_2020_paper.html)'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] S. Cheng, Z. Xu, S. Zhu, Z. Li, L. E. Li, R. Ramamoorthi, 和 H. Su, “使用自适应薄体积表示与不确定性感知的深度立体，”在
    *2020 IEEE/CVF 计算机视觉与模式识别会议，CVPR 2020，美国华盛顿州西雅图，2020年6月13-19日*。 计算机视觉基金会 / IEEE，2020，第2521–2531页。
    [在线]. 可用: [https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Deep_Stereo_Using_Adaptive_Thin_Volume_Representation_With_Uncertainty_Awareness_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Deep_Stereo_Using_Adaptive_Thin_Volume_Representation_With_Uncertainty_Awareness_CVPR_2020_paper.html)'
- en: '[246] K. Tateno, F. Tombari, I. Laina, and N. Navab, “Cnn-slam: Real-time dense
    monocular slam with learned depth prediction,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 6243–6252.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] K. Tateno, F. Tombari, I. Laina, 和 N. Navab, “Cnn-slam: 实时密集单目 SLAM 与学习的深度预测，”在
    *IEEE 计算机视觉与模式识别会议论文集*，2017，第6243–6252页。'
- en: '[247] M. Obst, L. Hobert, and P. Reisdorf, “Multi-sensor data fusion for checking
    plausibility of v2v communications by vision-based multiple-object tracking,”
    in *2014 IEEE Vehicular Networking Conference (VNC)*.   IEEE, 2014, pp. 143–150.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] M. Obst, L. Hobert, 和 P. Reisdorf, “基于视觉的多目标跟踪检查 v2v 通信的可信度的多传感器数据融合，”在
    *2014 IEEE 车辆网络会议 (VNC)*。 IEEE，2014，第143–150页。'
- en: '[248] S. Pang, D. Morris, and H. Radha, “Clocs: Camera-lidar object candidates
    fusion for 3d object detection,” in *2020 IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS)*.   IEEE, 2020, pp. 10 386–10 393.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] S. Pang, D. Morris, 和 H. Radha, “Clocs: 摄像头-激光雷达物体候选融合用于 3D 物体检测，”在 *2020
    IEEE/RSJ 智能机器人与系统国际会议 (IROS)*。 IEEE，2020，第10 386–10 393页。'
- en: '[249] H. Pan, W. Sun, Q. Sun, and H. Gao, “Deep learning based data fusion
    for sensor fault diagnosis and tolerance in autonomous vehicles,” *Chinese Journal
    of Mechanical Engineering*, vol. 34, p. 72, 12 2021.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] H. Pan, W. Sun, Q. Sun, 和 H. Gao, “基于深度学习的数据融合用于传感器故障诊断和容错处理，” *中国机械工程学报*，第34卷，第72页，2021年12月。'
- en: '[250] T. Clunie, M. DeFilippo, M. Sacarny, and P. Robinette, “Development of
    a perception system for an autonomous surface vehicle using monocular camera,
    lidar, and marine radar,” in *2021 IEEE International Conference on Robotics and
    Automation (ICRA)*.   IEEE, 2021, pp. 14 112–14 119.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] T. Clunie, M. DeFilippo, M. Sacarny, 和 P. Robinette, “使用单目摄像头、激光雷达和海洋雷达开发自主水面车辆的感知系统，”在
    *2021 IEEE 国际机器人与自动化会议 (ICRA)*。 IEEE，2021，第14 112–14 119页。'
- en: '[251] S. Sharma, J. Ansari, K. Jatavallabhula, and M. Krishna, “Beyond pixels:
    Leveraging geometry and shape cues for online multi-object tracking,” 02 2018.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] S. Sharma, J. Ansari, K. Jatavallabhula, 和 M. Krishna, “超越像素：利用几何和形状线索进行在线多目标跟踪，”
    2018年2月。'
- en: '[252] Y. Zhang, B. Song, X. Du, and M. Guizani, “Vehicle tracking using surveillance
    with multimodal data fusion,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 19, no. 7, pp. 2353–2361, 2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] Y. Zhang, B. Song, X. Du, 和 M. Guizani, “利用多模态数据融合进行车辆跟踪，” *IEEE 智能交通系统汇刊*，第19卷，第7期，第2353–2361页，2018年。'
- en: '[253] J. Czarnowski, T. Laidlow, R. Clark, and A. Davison, “Deepfactors: Real-time
    probabilistic dense monocular slam,” *IEEE Robotics and Automation Letters*, vol. PP,
    pp. 1–1, 01 2020.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] J. Czarnowski, T. Laidlow, R. Clark, 和 A. Davison，“Deepfactors: 实时概率密集单目SLAM”，*IEEE机器人与自动化快报*，第PP卷，pp.
    1–1，2020年01月。'
- en: '[254] J. Xue, D. Wang, S. Du, D. Cui, Y. Huang, and N. Zheng, “A vision-centered
    multi-sensor fusing approach to self-localization and obstacle perception for
    robotic cars,” *Frontiers Inf. Technol. Electron. Eng.*, vol. 18, no. 1, pp. 122–138,
    2017\. [Online]. Available: [https://doi.org/10.1631/FITEE.1601873](https://doi.org/10.1631/FITEE.1601873)'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] J. Xue, D. Wang, S. Du, D. Cui, Y. Huang, 和 N. Zheng，“一种以视觉为中心的多传感器融合方法用于机器人车辆的自我定位和障碍物感知”，*Frontiers
    Inf. Technol. Electron. Eng.*，第18卷，第1期，pp. 122–138，2017年。[在线]. 可用：[https://doi.org/10.1631/FITEE.1601873](https://doi.org/10.1631/FITEE.1601873)'
- en: '[255] F. Garcia, D. Martin, A. De La Escalera, and J. M. Armingol, “Sensor
    fusion methodology for vehicle detection,” *IEEE Intelligent Transportation Systems
    Magazine*, vol. 9, no. 1, pp. 123–133, 2017.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] F. Garcia, D. Martin, A. De La Escalera, 和 J. M. Armingol，“车辆检测的传感器融合方法”，*IEEE智能交通系统杂志*，第9卷，第1期，pp.
    123–133，2017年。'
- en: '[256] T.-L. Kim, J.-S. Lee, and T.-H. Park, “Fusing lidar, radar, and camera
    using extended kalman filter for estimating the forward position of vehicles,”
    in *2019 IEEE International Conference on Cybernetics and Intelligent Systems
    (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)*.   IEEE,
    2019, pp. 374–379.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] T.-L. Kim, J.-S. Lee, 和 T.-H. Park，“利用扩展卡尔曼滤波器融合激光雷达、雷达和相机以估计车辆前方位置”，见于
    *2019 IEEE国际网络与智能系统会议（CIS）和IEEE机器人、自动化与机电一体化会议（RAM）*。IEEE，2019，pp. 374–379。'
- en: '[257] L. Caltagirone, M. Bellone, L. Svensson, and M. Wahde, “Lidar-camera
    fusion for road detection using fully convolutional neural networks,” *Robotics
    and Autonomous Systems*, vol. 111, 11 2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] L. Caltagirone, M. Bellone, L. Svensson, 和 M. Wahde，“使用全卷积神经网络进行道路检测的激光雷达-相机融合”，*机器人与自主系统*，第111卷，2018年11月。'
- en: '[258] R. Dheekonda, S. Panda, M. Khan, M. Hasan, and S. Anwar, “Object detection
    from a vehicle using deep learning network and future integration with multi-sensor
    fusion algorithm,” 03 2017.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] R. Dheekonda, S. Panda, M. Khan, M. Hasan, 和 S. Anwar，“利用深度学习网络的车辆物体检测及未来与多传感器融合算法的集成”，2017年03月。'
- en: '[259] J. Luiten, T. Fischer, and B. Leibe, “Track to reconstruct and reconstruct
    to track,” *IEEE Robotics Autom. Lett.*, vol. 5, no. 2, pp. 1803–1810, 2020\.
    [Online]. Available: [https://doi.org/10.1109/LRA.2020.2969183](https://doi.org/10.1109/LRA.2020.2969183)'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] J. Luiten, T. Fischer, 和 B. Leibe，“跟踪以重建和重建以跟踪”，*IEEE机器人与自动化快报*，第5卷，第2期，pp.
    1803–1810，2020年。[在线]. 可用：[https://doi.org/10.1109/LRA.2020.2969183](https://doi.org/10.1109/LRA.2020.2969183)'
- en: '[260] J. Fei, W. Chen, P. Heidenreich, S. Wirges, and C. Stiller, “Semanticvoxels:
    Sequential fusion for 3d pedestrian detection using lidar point cloud and semantic
    segmentation,” in *2020 IEEE International Conference on Multisensor Fusion and
    Integration for Intelligent Systems (MFI)*.   IEEE, 2020, pp. 185–190.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] J. Fei, W. Chen, P. Heidenreich, S. Wirges, 和 C. Stiller，“Semanticvoxels:
    利用激光雷达点云和语义分割进行3D行人检测的序列融合”，见于 *2020 IEEE国际多传感器融合与智能系统会议（MFI）*。IEEE，2020，pp. 185–190。'
- en: '[261] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “Ipod: Intensive point-based
    object detector for point cloud,” *arXiv preprint arXiv:1812.05276*, 2018.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] Z. Yang, Y. Sun, S. Liu, X. Shen, 和 J. Jia，“Ipod: 基于点的密集物体检测器”，*arXiv
    预印本 arXiv:1812.05276*，2018。'
- en: '[262] D. Yu, H. Xiong, Q. Xu, J. Wang, and K. Li, “Multi-stage residual fusion
    network for lidar-camera road detection,” in *2019 IEEE Intelligent Vehicles Symposium
    (IV)*.   IEEE, 2019, pp. 2323–2328.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] D. Yu, H. Xiong, Q. Xu, J. Wang, 和 K. Li，“用于激光雷达-相机道路检测的多阶段残差融合网络”，见于
    *2019 IEEE智能车辆研讨会（IV）*。IEEE，2019，pp. 2323–2328。'
- en: '[263] K. El Madawi, H. Rashed, A. El Sallab, O. Nasr, H. Kamel, and S. Yogamani,
    “Rgb and lidar fusion based 3d semantic segmentation for autonomous driving,”
    in *2019 IEEE Intelligent Transportation Systems Conference (ITSC)*.   IEEE, 2019,
    pp. 7–12.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] K. El Madawi, H. Rashed, A. El Sallab, O. Nasr, H. Kamel, 和 S. Yogamani，“基于RGB和激光雷达融合的3D语义分割用于自动驾驶”，见于
    *2019 IEEE智能交通系统会议（ITSC）*。IEEE，2019，pp. 7–12。'
- en: '[264] F. Farahnakian and J. Heikkonen, “Rgb-depth fusion framework for object
    detection in autonomous vehicles,” in *2020 14th International Conference on Signal
    Processing and Communication Systems (ICSPCS)*.   IEEE, 2020, pp. 1–6.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] F. Farahnakian 和 J. Heikkonen，“用于自动驾驶车辆物体检测的RGB-深度融合框架”，见于 *2020第14届国际信号处理与通信系统会议（ICSPCS）*。IEEE，2020，pp.
    1–6。'
- en: '[265] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习”，见于*IEEE计算机视觉与模式识别会议论文集*，2016年，第770–778页。'
- en: '[266] T. Lv, Y. Zhang, L. Luo, and X. Gao, “Maffnet: real-time multi-level
    attention feature fusion network with rgb-d semantic segmentation for autonomous
    driving,” *Applied Optics*, vol. 61, no. 9, pp. 2219–2229, 2022.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] T. Lv, Y. Zhang, L. Luo, 和 X. Gao，“Maffnet：用于自动驾驶的实时多级注意力特征融合网络与RGB-D语义分割”，*应用光学*，第61卷，第9期，第2219–2229页，2022年。'
- en: '[267] J. Park, H. Yoo, and Y. Wang, “Drivable dirt road region identification
    using image and point cloud semantic segmentation fusion,” *IEEE Transactions
    on Intelligent Transportation Systems*, 2021.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] J. Park, H. Yoo, 和 Y. Wang，“利用图像和点云语义分割融合识别可行的泥土道路区域”，*IEEE智能交通系统汇刊*，2021年。'
- en: '[268] D. Göhring, M. Wang, M. Schnürmacher, and T. Ganjineh, “Radar/lidar sensor
    fusion for car-following on highways,” in *The 5th International Conference on
    Automation, Robotics and Applications*.   IEEE, 2011, pp. 407–412.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] D. Göhring, M. Wang, M. Schnürmacher, 和 T. Ganjineh，“雷达/激光雷达传感器融合用于高速公路上的跟车”，见于*第5届国际自动化、机器人与应用会议*。IEEE，2011年，第407–412页。'
- en: '[269] F. Camarda, “Multi-sensor data fusion for lane boundaries detection applied
    to autonomous vehicle,” Ph.D. dissertation, Université de Technologie de Compiègne,
    2022.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] F. Camarda，“用于车道边界检测的多传感器数据融合应用于自主驾驶车辆”，博士论文，巴黎综合理工大学，2022年。'
- en: '[270] B. A. Griffin and J. J. Corso, “Depth from camera motion and object detection,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2021, pp. 1397–1406.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] B. A. Griffin 和 J. J. Corso，“通过相机运动和物体检测获取深度”，见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第1397–1406页。'
- en: '[271] Y. Liu, “Multi-scale spatio-temporal feature extraction and depth estimation
    from sequences by ordinal classification,” *Sensors*, vol. 20, no. 7, p. 1979,
    2020.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] Y. Liu，“通过序列的序数分类进行多尺度时空特征提取和深度估计”，*传感器*，第20卷，第7期，第1979页，2020年。'
- en: '[272] B. Liu, B. Zhuang, S. Schulter, P. Ji, and M. Chandraker, “Understanding
    road layout from videos as a whole,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 4414–4423.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] B. Liu, B. Zhuang, S. Schulter, P. Ji, 和 M. Chandraker，“从视频中全面理解道路布局”，见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第4414–4423页。'
- en: '[273] A. Diba, M. Fayyaz, V. Sharma, M. M. Arzani, R. Yousefzadeh, J. Gall,
    and L. Van Gool, “Spatio-temporal channel correlation networks for action classification,”
    in *Proceedings of the European Conference on Computer Vision (ECCV)*, 2018, pp.
    284–299.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] A. Diba, M. Fayyaz, V. Sharma, M. M. Arzani, R. Yousefzadeh, J. Gall,
    和 L. Van Gool，“用于动作分类的时空通道相关网络”，见于*欧洲计算机视觉会议（ECCV）论文集*，2018年，第284–299页。'
- en: '[274] S. Chen, K. Ma, and Y. Zheng, “Med3d: Transfer learning for 3d medical
    image analysis,” *arXiv preprint arXiv:1904.00625*, 2019.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] S. Chen, K. Ma, 和 Y. Zheng，“Med3d：用于3D医学图像分析的迁移学习”，*arXiv预印本 arXiv:1904.00625*，2019年。'
- en: '[275] K. Hara, H. Kataoka, and Y. Satoh, “Can spatiotemporal 3d cnns retrace
    the history of 2d cnns and imagenet?” in *Proceedings of the IEEE conference on
    Computer Vision and Pattern Recognition*, 2018, pp. 6546–6555.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] K. Hara, H. Kataoka, 和 Y. Satoh，“时空3D卷积神经网络能否重新追溯2D卷积神经网络和ImageNet的历史？”，见于*IEEE计算机视觉与模式识别会议论文集*，2018年，第6546–6555页。'
- en: '[276] C. Feichtenhofer, “X3d: Expanding architectures for efficient video recognition,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 203–213.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] C. Feichtenhofer，“X3d：扩展架构以提高视频识别效率”，见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第203–213页。'
- en: '[277] Q. Zhou, X. Li, L. He, Y. Yang, G. Cheng, Y. Tong, L. Ma, and D. Tao,
    “Transvod: End-to-end video object detection with spatial-temporal transformers,”
    *arXiv preprint arXiv:2201.05047*, 2022.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] Q. Zhou, X. Li, L. He, Y. Yang, G. Cheng, Y. Tong, L. Ma, 和 D. Tao，“Transvod：基于时空变换器的端到端视频目标检测”，*arXiv预印本
    arXiv:2201.05047*，2022年。'
- en: '[278] G. Bertasius, H. Wang, and L. Torresani, “Is space-time attention all
    you need for video understanding,” *arXiv preprint arXiv:2102.05095*, vol. 2,
    no. 3, p. 4, 2021.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] G. Bertasius, H. Wang, 和 L. Torresani，“时空注意力是否是视频理解所需的一切”，*arXiv预印本 arXiv:2102.05095*，第2卷，第3期，第4页，2021年。'
- en: '[279] K. Li, Y. Wang, P. Gao, G. Song, Y. Liu, H. Li, and Y. Qiao, “Uniformer:
    Unified transformer for efficient spatiotemporal representation learning,” *arXiv
    preprint arXiv:2201.04676*, 2022.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] K. Li, Y. Wang, P. Gao, G. Song, Y. Liu, H. Li, 和 Y. Qiao，“Uniformer：高效时空表示学习的统一变换器”，*arXiv预印本
    arXiv:2201.04676*，2022年。'
- en: '[280] Z. Pang, J. Li, P. Tokmakov, D. Chen, S. Zagoruyko, and Y.-X. Wang, “Standing
    between past and future: Spatio-temporal modeling for multi-camera 3d multi-object
    tracking,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, 2023, pp. 17 928–17 938.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] Z. Pang, J. Li, P. Tokmakov, D. Chen, S. Zagoruyko 和 Y.-X. Wang，“站在过去和未来之间：多摄像头
    3D 多目标跟踪的时空建模，”发表于*IEEE/CVF 计算机视觉与模式识别会议论文集*，2023，第17,928–17,938页。'
- en: Appendix A
  id: totrans-518
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A
- en: 'TABLE IV: Summary of Recent Reviews on Data Integration Techniques in ADS Perception
    with Deep Learning Approach.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：基于深度学习方法的自动驾驶系统感知数据融合技术最新综述总结。
- en: '| Reference | Sensors | Applications/Tasks | Fusion Taxonomy | Other Contents
    |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 传感器 | 应用/任务 | 融合分类 | 其他内容 |'
- en: '| Wang et al. [[11](#bib.bib11)] | Camera, LiDAR, MMW-radar, GPS, IMU, ultrasonic,
    V2X | Multi-target tracking and environment reconstruction (motion model and data
    association) | Discernible units (data level), feature complementarity (feature
    level), target attributes, decision making (decision level) | Present characteristics,
    advantages, and disadvantages of different sensors. |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 [[11](#bib.bib11)] | 相机、激光雷达、毫米波雷达、GPS、IMU、超声波、V2X | 多目标跟踪和环境重建（运动模型和数据关联）
    | 可辨别单元（数据级）、特征互补（特征级）、目标属性、决策制定（决策级） | 展示不同传感器的特点、优缺点。 |'
- en: '| Fayyad et al. [[2](#bib.bib2)] | Camera, LiDAR, MMW-radar, GPS, IMU, INS,
    map | Detection, ego-localization and mapping | Data level (early fusion), feature
    level (mid-level fusion), or decision level (late fusion) | A summary of deep
    learning algorithm architectures in the field of sensor fusion for autonomous
    vehicle systems. |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| Fayyad 等人 [[2](#bib.bib2)] | 相机、激光雷达、毫米波雷达、GPS、IMU、INS、地图 | 检测、自我定位和映射 |
    数据级（早期融合）、特征级（中级融合）或决策级（晚期融合） | 针对自主车辆系统中传感器融合领域的深度学习算法架构总结。 |'
- en: '| Cui et al. [[28](#bib.bib28)] | Camera, LiDAR | Depth completion, object
    detection, semantic segmentation, tracking and online cross-sensor calibration
    | Signal level, feature level, result level, and multi-level | A summary of deep
    learning fusion methods based on different sensor combinations and different input
    representations for each perception task. |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| Cui 等人 [[28](#bib.bib28)] | 相机、激光雷达 | 深度补全、目标检测、语义分割、跟踪和在线跨传感器标定 | 信号级、特征级、结果级和多级
    | 基于不同传感器组合和每个感知任务的不同输入表示的深度学习融合方法总结。 |'
- en: '| Yeong et al. [[12](#bib.bib12)] | Camera, LiDAR, radar | Obstacle detection
    | Low-, mid-, high-level fusion | 1\. Operating principles and characteristics
    of sensors, and a comparison of commercially hardware. 2\. Sensor calibration
    overview.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '| Yeong 等人 [[12](#bib.bib12)] | 相机、激光雷达、雷达 | 障碍物检测 | 低级、中级、高级融合 | 1\. 传感器的工作原理和特性，以及商用硬件的比较。
    2\. 传感器标定概述。 |'
- en: 3\. A summary of related data fusion reviews.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 相关数据融合综述的总结。
- en: 4\. A summary of recent studies on sensor fusion technologies. |
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 近期传感器融合技术研究的总结。
- en: '| Huang et al. [[4](#bib.bib4)] | Camera, LiDAR | BEV object detection, 3D
    object detection | New taxonomy of multi-modal fusion: two major classes (strong-
    and weak-fusion), and four minor classes in strong-fusion (early-, deep-, late-,
    and asymmetry-fusion) | 1\. Summary of data formats and representations of LiDAR
    and camera. 2\. Summary of commonly used open datasets. |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| Huang 等人 [[4](#bib.bib4)] | 相机、激光雷达 | BEV 目标检测、3D 目标检测 | 多模态融合的新分类法：两个主要类别（强融合和弱融合），以及强融合中的四个次要类别（早期融合、深度融合、晚期融合和不对称融合）
    | 1\. 激光雷达和相机的数据格式和表示总结。 2\. 常用开放数据集的总结。 |'
- en: '| Feng et al. [[14](#bib.bib14)] | Camera, LiDAR, radar | Object detection,
    semantic segmentation | Early-, late-, middle-fusion (fusion in one layer, deep
    fusion, short-cut fusion) | 1\. Discusses the fusion methodologies regarding “what
    to fuse”, “when to fuse” and “how to fuse”. 2\. Summary of multi-modal datasets
    and task-related algorithms in papers. |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| Feng 等人 [[14](#bib.bib14)] | 相机、激光雷达、雷达 | 目标检测、语义分割 | 早期融合、晚期融合、中级融合（单层融合、深度融合、快捷融合）
    | 1\. 讨论“融合什么”、“何时融合”和“如何融合”的融合方法。 2\. 文献中的多模态数据集和任务相关算法的总结。 |'
- en: '| Wang et al. [[58](#bib.bib58)] | Camera, LiDAR | 3D object detection | Feature
    fusion (granularity: RoI-wise, voxel-wise, point-wise, pixel-wise), decision fusion.
    | 1\. Summary of popular sensors in ADS, their data representations, and corresponding
    object detection deep learning networks. 2\. Datasets (and metrics) for multi-modal
    3D object detection. |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| 王等人 [[58](#bib.bib58)] | 摄像头, LiDAR | 3D 物体检测 | 特征融合（粒度：RoI-wise、体素级、点级、像素级）、决策融合。
    | 1\. ADS 中流行传感器的总结、数据表示及相应的物体检测深度学习网络。2\. 多模态 3D 物体检测的数据集（和指标）。 |'
- en: '| Wang et al. [[15](#bib.bib15)] | Camera, LiDAR, radar | 3D object detection
    | New taxonomy with aspects of representation, alignment, and fusion. Fusion methods
    are further divided into learning-agnostic based (element-wise operations and
    concatenation) and learning-based (attention mochanism) approaches. | 1\. Categorization
    of sensor data representation: unified representation (hybrid-based, stereoscopic-based,
    and BEV-based) and raw representation. 2\. Categorization of alignment methodology:
    projection-based (global projection and local projection) and model-based (cross-attention).'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '| 王等人 [[15](#bib.bib15)] | 摄像头, LiDAR, 雷达 | 3D 物体检测 | 新的分类法，涉及表示、对齐和融合方面。融合方法进一步分为基于学习无关（元素级操作和级联）和基于学习（注意力机制）的方法。
    | 1\. 传感器数据表示的分类：统一表示（混合型、立体型和 BEV 型）和原始表示。2\. 对齐方法的分类：基于投影（全局投影和局部投影）和基于模型（交叉注意力）。'
- en: 3\. Datasets (and metrics) for multi-modal 3D object detection. |
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 多模态 3D 物体检测的数据集（和指标）。 |
- en: Appendix B
  id: totrans-532
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B
- en: 'TABLE V: Summary of Integration Operations in ADS Perception with Deep Learning
    Approach.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：利用深度学习方法总结 ADS 感知中的集成操作。
- en: '| Operations / Methods | Advantages | Disadvantages | Output Format |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| 操作 / 方法 | 优势 | 劣势 | 输出格式 |'
- en: '| Projection | 1\. Can connect 2D space and 3D space. 2\. Based on the projection
    principle, which is easier to understand. | 1\. Heavily rely on the sensor extrinsic
    and intrinsic matrices, which may be vulnerable to errors. 2\. Need extra methods
    to deal with revolution inconsistency before projection. | Projected data with
    dimension reduction or dimension boosting. |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| 投影 | 1\. 可以连接 2D 空间和 3D 空间。 2\. 基于投影原理，更容易理解。 | 1\. 严重依赖传感器的外部和内部矩阵，可能容易出错。
    2\. 需要额外的方法处理投影前的旋转不一致性。 | 具有维度减少或维度增加的投影数据。 |'
- en: '| Concatenation | 1\. Easy to operate. 2\. Can integrate data from more than
    two sources. | 1\. Can only applied to data with same spatial size. 2\. Can not
    deal with missing dimensions. 3\. The input sensors are not permutable. | Concategated
    tensors with same spatial size but different dimension. |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| 级联 | 1\. 操作简单。 2\. 可以整合来自两个以上来源的数据。 | 1\. 仅适用于具有相同空间尺寸的数据。 2\. 不能处理缺失的维度。
    3\. 输入传感器不可互换。 | 级联的张量具有相同的空间尺寸但不同的维度。 |'
- en: '| Addition / Average mean / Weighted sum | Simple operation and easy to implement.
    Wide range of applications at different stages of the integration process. | Require
    the data to be integrated to have exactly the same format (both spatial size and
    dimension). | Added data with same data format (both spatial size and dimension)
    as the input. |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| 加法 / 平均值 / 加权和 | 操作简单且易于实施。 在集成过程的不同阶段具有广泛的应用。 | 需要集成的数据具有完全相同的格式（空间尺寸和维度）。
    | 添加格式相同的数据（空间尺寸和维度）与输入数据相同。 |'
- en: '| Probabilistic methods | Incorporate uncertainties into the integration process.
    | Usually have more parameters to be estimated thus currently are applied to comparatively
    simple models. | Depend on the input and implementation scenes. |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 概率方法 | 将不确定性纳入集成过程。 | 通常有更多参数需要估计，因此目前仅应用于相对简单的模型。 | 取决于输入和实施场景。 |'
- en: '| Rule-based transaction or integration | High specificity to deal with a certain
    scenario or dataset. | Low generalizability, and usually cannot be applied to
    other scenes. | Depend on the input and implementation scenes. |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 基于规则的事务或集成 | 高特异性以处理特定场景或数据集。 | 通用性低，通常不能应用于其他场景。 | 取决于输入和实施场景。 |'
- en: '| Kalman Filter and extensions | 1\. Easy to apply to integrate temporal information.
    2\. Do not acquire massive data to process and update. Can update system state
    with only a few data points. | 1\. Heavily rely on the manually designed process
    model and the covariance matrix, which may not be appropriate or accurate. 2\.
    Have short memory for temporal information. | Depend on the design of the system
    states. |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 卡尔曼滤波器及扩展 | 1\. 易于应用于集成时间信息。2\. 不需要大量数据进行处理和更新。可以仅用少量数据点更新系统状态。 | 1\. 强烈依赖手动设计的过程模型和协方差矩阵，这可能不适合或不准确。2\.
    对时间信息的记忆较短。 | 依赖于系统状态的设计。 |'
- en: '| RNN family | 1\. Designed for temporal integration by nature. 2\. Comparatively
    easy to understand and implement. | 1\. Have poor long-term memory. 2\. Difficult
    to train and converge. | Concatenated tensors. |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| RNN 家族 | 1\. 本质上设计用于时间集成。2\. 相对容易理解和实现。 | 1\. 长期记忆差。2\. 难以训练和收敛。 | 级联张量。
    |'
- en: '| CNN, 3D CNN, and variants | 1\. Data driven, and does not need manually designed
    system model. 2\. Kernels and convolution layers by nature can integrate spatial
    (CNN) and temporal (3D CNN) information. | 1\. Comparatively limited receptive
    field and memory. 2\. Subject to the shortcomings of the integration operations
    included in the model structure. 3\. Model procedure and structure are unchanged
    in application. | Concatenated tensors. |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| CNN、3D CNN 和变体 | 1\. 数据驱动，无需手动设计系统模型。2\. 核心和卷积层本质上可以整合空间（CNN）和时间（3D CNN）信息。
    | 1\. 相对有限的感受野和记忆。2\. 受限于模型结构中集成操作的缺陷。3\. 应用中的模型过程和结构不变。 | 级联张量。 |'
- en: '| Transformer | 1\. Can potentially integrate both spatial and temporal information.
    2\. Global receptive field. 3\. Flexible model design and scalable to model any
    data size. | 1\. Need well-designed Queries and positional encodings. 2\. Hard
    to train. | Tensor weighted summation. |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | 1\. 有潜力整合空间和时间信息。2\. 全局感受野。3\. 灵活的模型设计，能够扩展到任何数据大小。 | 1\. 需要精心设计的查询和位置编码。2\.
    难以训练。 | 张量加权求和。'
