- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:36:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:36:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2310.07745] Deep Reinforcement Learning for Autonomous Cyber Operations: A
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2310.07745] 自主网络安全操作的深度强化学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.07745](https://ar5iv.labs.arxiv.org/html/2310.07745)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.07745](https://ar5iv.labs.arxiv.org/html/2310.07745)
- en: \jyear
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \jyear
- en: '2023'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '2023'
- en: '[1]\fnmGregory \surPalmer'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]\fnmGregory \surPalmer'
- en: 1]\orgnameBAE Systems \orgdivApplied Intelligence Labs, Chelmsford Office &
    Technology Park, Great Baddow, Chelmsford, Essex, CM2 8HN
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 1]\orgnameBAE Systems \orgdiv应用智能实验室，Chelmsford 办公室及技术园区，Great Baddow，Chelmsford，Essex，CM2
    8HN
- en: 'Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自主网络安全操作的深度强化学习：综述
- en: '[gregory.palmer@baesystems.com](mailto:gregory.palmer@baesystems.com)    \fnmChris
    \surParry    \fnmDaniel \surHarrold    \fnmChris \surWillis ['
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[gregory.palmer@baesystems.com](mailto:gregory.palmer@baesystems.com)    \fnmChris
    \surParry    \fnmDaniel \surHarrold    \fnmChris \surWillis ['
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The rapid increase in the number of cyber-attacks in recent years raises the
    need for principled methods for defending networks against malicious actors. *Deep
    reinforcement learning* (DRL) has emerged as a promising approach for mitigating
    these attacks. However, while DRL has shown much potential for cyber defence,
    numerous challenges must be overcome before DRL can be applied to autonomous cyber
    operations (ACO) *at scale*. Principled methods are required for environments
    that confront learners with *very* high-dimensional state spaces, large multi-discrete
    action spaces, and adversarial learning. Recent works have reported success in
    solving these problems individually. There have also been impressive engineering
    efforts towards solving all three for real-time strategy games. However, applying
    DRL to the full ACO problem remains an open challenge. Here, we survey the relevant
    DRL literature and conceptualize an idealised ACO-DRL agent. We provide: i.) A
    summary of the domain properties that define the ACO problem; ii.) A comprehensive
    evaluation of the extent to which domains used for benchmarking DRL approaches
    are comparable to ACO; iii.) An overview of state-of-the-art approaches for scaling
    DRL to domains that confront learners with the curse of dimensionality, and; iv.)
    A survey and critique of current methods for limiting the exploitability of agents
    within adversarial settings from the perspective of ACO. We conclude with open
    research questions that we hope will motivate future directions for researchers
    and practitioners working on ACO.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来网络攻击数量的快速增加提高了对网络防御恶意攻击者的原则性方法的需求。*深度强化学习*（DRL）已经成为缓解这些攻击的有希望的方法。然而，尽管DRL在网络防御中显示了很大的潜力，但在DRL可以应用于大规模的自主网络安全操作（ACO）之前，还需要克服众多挑战。对于面临*非常*高维状态空间、大型多离散动作空间和对抗学习的环境，需要原则性的方法。近期的工作报告了成功解决这些问题的个别情况。也有针对实时战略游戏的三方面问题的令人印象深刻的工程努力。然而，将DRL应用于完整的ACO问题仍然是一个未解的挑战。在这里，我们回顾了相关的DRL文献，并构思了一个理想化的ACO-DRL代理。我们提供了：i.)
    定义ACO问题的领域属性总结；ii.) 对用于基准测试DRL方法的领域与ACO的可比性进行全面评估；iii.) 针对面临维度诅咒的领域，扩展DRL的最先进方法概述；iv.)
    从ACO的角度调查和批评当前限制代理在对抗设置中可利用性的方法。我们总结了希望能激励未来研究方向的开放研究问题。
- en: 'keywords:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Autonomous Cyber Operations, Multi-agent Deep Reinforcement Learning, Adversarial
    Learning
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自主网络安全操作、多智能体深度强化学习、对抗学习
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'The rapid increase in the number of cyber-attacks in recent years has raised
    the need for responsive, adaptive, and scalable *autonomous cyber operations*
    (ACO) [9596578, albahar2019cyber]. Adaptive solutions are desirable due to cyber-criminals
    increasingly showing an ability to evade conventional security systems, which
    often lack the ability to detect new types of attacks [9277523]. The ACO problem
    can be formulated as an adversarial game involving a Blue agent tasked with defending
    cyber resources from a Red attacker [baillie2020cyborg]. Deep reinforcement learning
    (DRL) has been identified as a suitable machine learning (ML) paradigm to apply
    to ACO [adawadkar2022cyber, li2019reinforcement, liu2020deep]. However, current
    \sayout of the box DRL solutions do not scale well to many real world scenarios.
    This is primarily due to ACO lying at the intersection of three open problem areas
    for DRL, namely: i.) The efficient processing and exploration of vast high-dimensional
    state spaces [abel2016exploratory]; ii.) Large combinatorial action spaces, and;
    iii.) Minimizing the exploitability of DRL agents in adversarial games [gleave2019adversarial]
    (see \autoreffig:aco_requirements).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来网络攻击数量的快速增加提高了对响应性、适应性和可扩展的*自主网络操作*（ACO）的需求 [9596578, albahar2019cyber]。由于网络犯罪分子越来越表现出规避传统安全系统的能力，适应性解决方案变得越来越重要，因为传统系统往往缺乏检测新型攻击的能力
    [9277523]。ACO问题可以被表述为一个对抗游戏，其中蓝方代理负责保护网络资源免受红方攻击者的侵害 [baillie2020cyborg]。深度强化学习（DRL）已被确定为应用于ACO的合适机器学习（ML）范式
    [adawadkar2022cyber, li2019reinforcement, liu2020deep]。然而，目前的DRL解决方案在许多现实场景中表现不佳。这主要是因为ACO位于DRL的三个开放问题领域的交集，即：i.)
    高维状态空间的有效处理和探索 [abel2016exploratory]；ii.) 大规模组合动作空间；iii.) 减少DRL代理在对抗游戏中的可利用性 [gleave2019adversarial]（参见
    \autoreffig:aco_requirements）。
- en: '![Refer to caption](img/af84a190a7892049475130afece1b362.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/af84a190a7892049475130afece1b362.png)'
- en: 'Figure 1: Three challenges that an idealised DRL-ACO agent must conquer.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一个理想化的DRL-ACO代理必须克服的三大挑战。
- en: The DRL literature features a plethora of efforts addressing the above challenges
    individually. Here, we survey these efforts and define an idealised DRL agent
    for ACO. This survey provides an overview of the current state of the field, defines
    long term objectives, and poses research questions for DRL practitioners and ACO
    researchers to dig their teeth into.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）文献中包含了大量分别解决上述挑战的努力。在这里，我们调查了这些努力，并为ACO定义了一个理想化的DRL代理。此调查提供了该领域的当前状态概述，定义了长期目标，并提出了DRL从业者和ACO研究人员需要深入探讨的研究问题。
- en: 'Our contributions can be summarised as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献可以总结如下：
- en: 1.) To enable an extensive evaluation of future DRL-ACO approaches, we provide
    an overview of ACO benchmarking environments, as well as environments found within
    the DRL literature that confront learners with comparable challenges.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 1.) 为了对未来的DRL-ACO方法进行全面评估，我们提供了ACO基准环境的概述，以及在DRL文献中发现的面对类似挑战的环境。
- en: 2.) We identify suitable methods for addressing the curse of dimensionality
    for ACO. This includes a summary of approaches for state-abstraction, efficient
    exploration and mitigating catastrophic forgetting, as well as a critical evaluation
    of high-dimensional action approaches.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2.) 我们确定了处理ACO维度诅咒的合适方法。这包括对状态抽象、高效探索和减轻灾难性遗忘的总结，以及对高维动作方法的批判性评估。
- en: 3.) We formally define the ACO problem from the perspective of adversarial learning.
    Even within \saysimple adversarial games, finding (near) optimal policies is non-trivial.
    We therefore review principled methods for limiting exploitability, and map out
    paths towards scaling these approaches to the full ACO challenge.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 3.) 我们从对抗学习的角度正式定义了ACO问题。即使在简单的对抗游戏中，寻找（近似）最优策略也并非易事。因此，我们回顾了限制可利用性的方法，并规划了将这些方法扩展到完整ACO挑战的路径。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: A number of surveys have been conducted in recent years that provide an overview
    of the different types of cyber attacks (e.g., intrusion, spam, and malware) and
    the ML methodologies that have been applied in response [9277523, li2018cyber,
    reda2021taxonomy, liu2019machine, thiyagarajan2020review]. Given that ML methods
    themselves are susceptible to adversarial attacks, there have also been efforts
    towards assessing the risk posed by adversarial learning techniques for cyber
    security [duddu2018survey, rosenberg2021adversarial]. However, while these works
    evaluate existing threats to ML models in general (e.g., white-box attacks [moosavi2016deepfool]
    and model poisoning attacks [kloft2010online]), our survey focuses on the adversarial
    learning process for DRL agents within ACO, desirable solution concepts, and a
    critical evaluation of existing techniques towards limiting exploitability.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来进行了一些调查，概述了不同类型的网络攻击（例如入侵、垃圾邮件和恶意软件）以及响应中应用的ML方法[9277523, li2018cyber, reda2021taxonomy,
    liu2019machine, thiyagarajan2020review]。鉴于ML方法本身容易受到对抗性攻击，也有评估对抗性学习技术对网络安全风险的努力[duddu2018survey,
    rosenberg2021adversarial]。然而，尽管这些研究评估了对ML模型的一般威胁（例如白盒攻击[moosavi2016deepfool]和模型中毒攻击[kloft2010online]），我们的调查专注于ACO中DRL智能体的对抗性学习过程、期望的解决概念，以及对现有技术在限制可利用性方面的关键评估。
- en: '[9596578] surveyed the DRL for cyber security literature, providing an overview
    of works where DRL-based security methods were applied to cyber–physical systems,
    autonomous intrusion detection techniques, and multiagent DRL-based game theory
    simulations for defence strategies against cyber-attacks. In contrast, our work
    focuses on generation after next solutions; we capture the challenges posed by
    the ACO problem at scale and survey the DRL literature for suitable methods designed
    to address these challenges separately, providing the building blocks for an idealised
    ACO-DRL agent. Such an agent will require a suitable evaluation environment. While
    there have been efforts towards evaluating cyber security datasets [8258167, kilincer2021machine,
    9277523], to the best of our knowledge we are the first to evaluate the extent
    to which cyber security and other benchmarking environments are representative
    of the full ACO problem.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[9596578] 调查了用于网络安全的DRL文献，概述了DRL基础的安全方法在网络物理系统、自主入侵检测技术以及多智能体DRL基础的博弈论模拟防御策略中的应用。相比之下，我们的工作侧重于下一代解决方案；我们捕捉了大规模ACO问题所带来的挑战，并调查了DRL文献中为应对这些挑战而设计的合适方法，为理想化的ACO-DRL智能体提供了构建块。这样的智能体将需要一个合适的评估环境。虽然已有评估网络安全数据集的努力[8258167,
    kilincer2021machine, 9277523]，但据我们所知，我们是首个评估网络安全和其他基准环境在多大程度上能代表完整ACO问题的研究。'
- en: 3 Background & Definitions
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 背景与定义
- en: Below we provide the definitions and notations that we will rely on throughout
    this survey. First, we will formally define the different types of models through
    which the interactions between RL agents and environments can be described. We
    will encounter each type in this survey (See \autoreffig:rl_problems_overview
    for an overview).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下我们提供本调查中依赖的定义和符号。首先，我们将正式定义描述RL智能体与环境交互的不同类型模型。我们将在本调查中遇到每种类型（参见\autoreffig:rl_problems_overview以获取概述）。
- en: '![Refer to caption](img/43637374482f4a3abf1a6f8f35a49577.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43637374482f4a3abf1a6f8f35a49577.png)'
- en: (a) MDP
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MDP
- en: '![Refer to caption](img/1d85724e79681b07e47c190596be0f40.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1d85724e79681b07e47c190596be0f40.png)'
- en: (b) POMDP
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (b) POMDP
- en: '![Refer to caption](img/ebbfd0fec31dc083e482f01ecefd0f6b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ebbfd0fec31dc083e482f01ecefd0f6b.png)'
- en: (c) Markov Game (MG)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 马尔可夫博弈（MG）
- en: '![Refer to caption](img/96522107a497e3c45731c89fcab76988.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/96522107a497e3c45731c89fcab76988.png)'
- en: (d) POMG
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (d) POMG
- en: '![Refer to caption](img/b18343832f415bba8b98ac74a92078ba.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b18343832f415bba8b98ac74a92078ba.png)'
- en: (e) Dec-POMDP
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Dec-POMDP
- en: '![Refer to caption](img/d813d6547cca65c329cdd945e30f6131.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d813d6547cca65c329cdd945e30f6131.png)'
- en: (f) Slate-MDP
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Slate-MDP
- en: '![Refer to caption](img/b53e130adb3901c29febbf37c546785d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b53e130adb3901c29febbf37c546785d.png)'
- en: (g) Parameterized Action MDP
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 参数化动作MDP
- en: 'Figure 2: An overview of the problem formulations discussed in this survey.
    Within these formulations we have the following variables: states $s$, rewards
    $r$, actions $a$, and observations $o$. For the Parameterized Action MDP we differentiate
    between discrete actions $a$ and continuous actions $u$.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：本调查讨论的问题公式的概述。在这些公式中，我们有以下变量：状态$s$，奖励$r$，动作$a$，和观察$o$。对于参数化动作MDP，我们区分离散动作$a$和连续动作$u$。
- en: 3.1 Markov Decision Process
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 马尔可夫决策过程
- en: 'Markov Decision Processes (MDPs) describe a class of problems – fully observable
    environments – that defines the field of RL, providing a suitable model to formulate
    interactions between reinforcement learners and their environment [sutton2018reinforcement].
    Formally: An MDP is a tuple $\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P}\rangle$,
    where: $\mathcal{S}$ is a finite set of states; for each state $s\in\mathcal{S}$
    there exists a finite set of possible actions $\mathcal{A}$; $\mathcal{R}$ is
    a real-valued payoff function $\mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}^{\prime}\rightarrow\mathbb{R}$,
    where $\mathcal{R}_{a}(s,s^{\prime})$ is the expected payoff following a state
    transition from $s$ to $s^{\prime}$ using action $a$; $\mathcal{P}$ is a state
    transition probability matrix $\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}^{\prime}\rightarrow[0,1]$,
    where $\mathcal{P}_{a}(s,s^{\prime})$ is the probability of state $s$ transitioning
    into state $s^{\prime}$ using action $a$. MDPs can have terminal (absorbing) states
    at which the episode ends.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDPs）描述了一类问题——完全可观测环境——这定义了强化学习（RL）领域，提供了一个合适的模型来制定强化学习者与其环境之间的交互[sutton2018reinforcement]。形式上：一个MDP是一个元组$\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P}\rangle$，其中：$\mathcal{S}$是有限的状态集；对于每个状态$s\in\mathcal{S}$，存在一个有限的可能动作集$\mathcal{A}$；$\mathcal{R}$是一个实值回报函数$\mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}^{\prime}\rightarrow\mathbb{R}$，其中$\mathcal{R}_{a}(s,s^{\prime})$是在状态从$s$转移到$s^{\prime}$时使用动作$a$后的期望回报；$\mathcal{P}$是一个状态转移概率矩阵$\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}^{\prime}\rightarrow[0,1]$，其中$\mathcal{P}_{a}(s,s^{\prime})$是状态$s$在使用动作$a$后转移到状态$s^{\prime}$的概率。MDPs可以具有终止（吸收）状态，在这些状态下，剧集结束。
- en: 3.2 Partially Observable Markov Decision Process
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 部分可观测马尔可夫决策过程
- en: 'Numerous environments lack the full observability property of MDPs [oliehoek2015concise].
    Here, a Partially Observable MDP (POMDP) extends an MDP $\mathcal{M}$ by adding
    $\langle\Omega,\mathcal{O}\rangle$, where: $\Omega$ is a finite set of observations;
    and $\mathcal{O}$ is an observation function defined as $\mathcal{O}:\mathcal{S}\times\mathcal{A}\times\Omega\rightarrow[0,1]$,
    where $\mathcal{O}(o\rvert s,a)$ is a distribution over observations $o$ that
    may occur in state $s$ after taking action $a$.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 许多环境缺乏MDP的完全可观测性属性[oliehoek2015concise]。在这里，部分可观测MDP（POMDP）通过添加$\langle\Omega,\mathcal{O}\rangle$来扩展MDP
    $\mathcal{M}$，其中：$\Omega$是有限的观察集；$\mathcal{O}$是定义为$\mathcal{O}:\mathcal{S}\times\mathcal{A}\times\Omega\rightarrow[0,1]$的观察函数，其中$\mathcal{O}(o\rvert
    s,a)$是可能在状态$s$中出现的观察$o$的分布，经过动作$a$后。
- en: 3.3 Markov Games
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 马尔可夫游戏
- en: 'Many environments, including the ones that are the focus of this survey, feature
    more than one learning agent. Here, game theory offers a solution via Markov games
    (also known as stochastic games [shapley1953stochastic]). A Markov game is defined
    as a tuple $(n,\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R})$, that has a finite
    state space $\mathcal{S}$; for each state $s\in\mathcal{S}$ a joint action space
    $(\mathcal{A}_{1}\times...\times\mathcal{A}_{n})$, with $\mathcal{A}_{p}$ being
    the number of actions available to player $p$; a state transition function $\mathcal{P}:\mathcal{S}_{t}\times\mathcal{A}_{1}\times...\times\mathcal{A}_{n}\times\mathcal{\mathcal{S}}_{t+1}\rightarrow[0,1]$,
    returning the probability of transitioning from a state $s_{t}$ to $s_{t+1}$ given
    an action profile $a_{1}\times...\times a_{n}$; and for each player $p$ a reward
    function: $\mathcal{\mathcal{R}}_{p}:\mathcal{\mathcal{S}}_{t}\times\mathcal{\mathcal{A}}_{1}\times...\times\mathcal{\mathcal{A}}_{n}\times\mathcal{\mathcal{S}}_{t+1}\rightarrow\mathbb{R}$
    [shapley1953stochastic]. We allow *terminal states* at which the game ends. Each
    state is fully-observable.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 许多环境，包括本调查的重点环境，具有多个学习代理。在这里，博弈论通过马尔可夫游戏（也称为随机游戏 [shapley1953stochastic]）提供了解决方案。马尔可夫游戏定义为一个元组$(n,\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R})$，具有有限状态空间$\mathcal{S}$；对于每个状态$s\in\mathcal{S}$，有一个联合动作空间$(\mathcal{A}_{1}\times...\times\mathcal{A}_{n})$，其中$\mathcal{A}_{p}$是玩家$p$可用的动作数；一个状态转移函数$\mathcal{P}:\mathcal{S}_{t}\times\mathcal{A}_{1}\times...\times\mathcal{A}_{n}\times\mathcal{\mathcal{S}}_{t+1}\rightarrow[0,1]$，返回从状态$s_{t}$到$s_{t+1}$的转移概率，给定一个动作配置$a_{1}\times...\times
    a_{n}$；对于每个玩家$p$，有一个奖励函数：$\mathcal{\mathcal{R}}_{p}:\mathcal{\mathcal{S}}_{t}\times\mathcal{\mathcal{A}}_{1}\times...\times\mathcal{\mathcal{A}}_{n}\times\mathcal{\mathcal{S}}_{t+1}\rightarrow\mathbb{R}$
    [shapley1953stochastic]。我们允许*终态*，即游戏结束的状态。每个状态都是完全可观测的。
- en: 3.4 Partially Observable Markov Games
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 部分可观测马尔可夫游戏
- en: As with POMDPs, we cannot assume the full observability property required by
    Markov games. A Partially Observable Markov Game (POMG) is an extension of Markov
    Games that includes $\langle\Omega,\mathcal{O}\rangle$ a set of joint observations
    $\Omega$; and an observation probability function defined as $\mathcal{O}_{p}:\mathcal{S}\times\mathcal{A}_{1}\times...\times\mathcal{A}_{n}\times\Omega\rightarrow[0,1]$.
    For each player $p$ the observation probability function $\mathcal{O}_{p}$ is
    a distribution over observations $o$ that may occur in state $s$, given an action
    profile $a_{1}\times...\times a_{n}$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与POMDPs一样，我们不能假设马尔可夫游戏所需的完全可观测性。部分可观测马尔可夫游戏（POMG）是马尔可夫游戏的扩展，它包括$\langle\Omega,\mathcal{O}\rangle$，即一组联合观测$\Omega$；以及一个观测概率函数定义为$\mathcal{O}_{p}:\mathcal{S}\times\mathcal{A}_{1}\times...\times\mathcal{A}_{n}\times\Omega\rightarrow[0,1]$。对于每个玩家$p$，观测概率函数$\mathcal{O}_{p}$是在状态$s$下，给定一个动作配置$a_{1}\times...\times
    a_{n}$可能出现的观测$o$的分布。
- en: 3.5 Decentralized-POMDPs
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 去中心化POMDPs
- en: A Decentralized-POMDP (Dec-POMDP) is a Partially Observable Markov Game where,
    at each step, all $n$ agents receive an identical reward [oliehoek2015concise].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 去中心化POMDP（Dec-POMDP）是一个部分可观测的马尔可夫游戏，在每一步，所有$n$个代理接收相同的奖励 [oliehoek2015concise]。
- en: 3.6 Slate Markov Decision Processes
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 铺设马尔可夫决策过程
- en: Some environments such as recommender systems require a custom model. Here,
    Slate-MDPs provide a solution [sunehag2015deep]. Given an underlying MDP $\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R}\rangle$,
    a Slate-MDP is a tuple $\langle\mathcal{S},\mathcal{A}^{l},\mathcal{P}^{\prime},\mathcal{R}^{\prime}\rangle$.
    Within this formulation $\mathcal{A}^{l}$ is a finite discrete action space $\mathcal{A}^{l}=\{\bm{a}_{1},\bm{a}_{2},...,\bm{a}_{N}\}$,
    representing the set of all possible slates to recommend given the current state
    $s$. Each slate can be formulated as $\bm{a}_{i}=\{a_{i}^{1},a_{i}^{2},...a_{i}^{K}\}$,
    with $K$ representing the size of the slate. Slate-MDPs assume an action selection
    function $\varphi:\mathcal{S}\times\mathcal{A}^{l}\rightarrow\mathcal{A}$. State
    transitions and rewards are, as a result, determined via functions $\mathcal{P}^{\prime}:\mathcal{S}\times\mathcal{A}^{l}\times\mathcal{S}^{\prime}\rightarrow[0,1]$
    and $\mathcal{R}^{\prime}:\mathcal{S}\times\mathcal{A}^{l}\times\mathcal{S}^{\prime}\rightarrow\mathbb{R}$
    respectively. Therefore, given an underlying MDP $\mathcal{M}$, we have $\mathcal{P}^{\prime}(s,\bm{a},s^{\prime})=\mathcal{P}(s,\varphi(\bm{a}),s^{\prime})$
    and $\mathcal{R}^{\prime}(s,\bm{a},s^{\prime})=\mathcal{R}(s,\varphi(\bm{a}),s^{\prime})$.
    Finally, there is an assumption that the most recently executed action can be
    derived from a state via a function $\psi:\mathcal{S}\rightarrow\mathcal{A}$.
    Note, there is no requirement that $\psi(s_{t+1})\in\bm{a}_{t}$, therefore, the
    action selected can also be outside the provided slate ¹¹1There are environments
    that treat $\psi(s_{t+1})\notin\bm{a}_{t}$ as a failure property, upon which an
    episode terminates [sunehag2015deep]..
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一些环境，如推荐系统，需要自定义模型。在这里，Slate-MDPs 提供了一种解决方案 [sunehag2015deep]。给定一个基础 MDP $\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R}\rangle$，Slate-MDP
    是一个元组 $\langle\mathcal{S},\mathcal{A}^{l},\mathcal{P}^{\prime},\mathcal{R}^{\prime}\rangle$。在这一表述中，$\mathcal{A}^{l}$
    是一个有限的离散动作空间 $\mathcal{A}^{l}=\{\bm{a}_{1},\bm{a}_{2},...,\bm{a}_{N}\}$，表示在当前状态
    $s$ 下可推荐的所有可能的板块集合。每个板块可以表示为 $\bm{a}_{i}=\{a_{i}^{1},a_{i}^{2},...a_{i}^{K}\}$，其中
    $K$ 代表板块的大小。Slate-MDPs 假设有一个动作选择函数 $\varphi:\mathcal{S}\times\mathcal{A}^{l}\rightarrow\mathcal{A}$。因此，状态转换和奖励通过函数
    $\mathcal{P}^{\prime}:\mathcal{S}\times\mathcal{A}^{l}\times\mathcal{S}^{\prime}\rightarrow[0,1]$
    和 $\mathcal{R}^{\prime}:\mathcal{S}\times\mathcal{A}^{l}\times\mathcal{S}^{\prime}\rightarrow\mathbb{R}$
    确定。因此，给定基础 MDP $\mathcal{M}$，我们有 $\mathcal{P}^{\prime}(s,\bm{a},s^{\prime})=\mathcal{P}(s,\varphi(\bm{a}),s^{\prime})$
    和 $\mathcal{R}^{\prime}(s,\bm{a},s^{\prime})=\mathcal{R}(s,\varphi(\bm{a}),s^{\prime})$。最后，有一个假设是，最近执行的动作可以通过函数
    $\psi:\mathcal{S}\rightarrow\mathcal{A}$ 从状态中推导出来。请注意，没有要求 $\psi(s_{t+1})\in\bm{a}_{t}$，因此，选择的动作也可以在提供的板块之外 ¹¹1有些环境将
    $\psi(s_{t+1})\notin\bm{a}_{t}$ 视为失败属性，导致一个回合终止 [sunehag2015deep]。
- en: 3.7 Parameterized Action MDPs
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 参数化动作MDPs
- en: Parameterized Action MDPs (PA-MDPs) are a generalization of MDPs where the agent
    must choose from a discrete set of parameterized actions [wei2018hierarchical,
    masson2016reinforcement]. More formally, PA-MDPs assume a finite discrete set
    of actions $\mathcal{A}_{d}=\{a_{1},a_{2},...,a_{n}\}$ and for each action $a\in\mathcal{A}_{d}$
    a set of continuous parameters $u_{a}\subseteq\mathbb{R}^{m_{a}}$, where $m_{a}$
    represents the dimensionality of action $a$. Therefore, an action is a tuple $(a,u)$
    in the joint action space, $\mathcal{A}=\bigcup_{a\in\mathcal{A}_{d}}\{(a,u)\rvert\
    \mathcal{U}_{a}\}$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化动作MDPs（PA-MDPs）是MDPs的一种推广，其中智能体必须从离散的参数化动作集中进行选择 [wei2018hierarchical, masson2016reinforcement]。更正式地，PA-MDPs
    假设有一个有限的离散动作集合 $\mathcal{A}_{d}=\{a_{1},a_{2},...,a_{n}\}$，对于每个动作 $a\in\mathcal{A}_{d}$，都有一组连续参数
    $u_{a}\subseteq\mathbb{R}^{m_{a}}$，其中 $m_{a}$ 代表动作 $a$ 的维度。因此，动作是联合动作空间中的一个元组
    $(a,u)$，即 $\mathcal{A}=\bigcup_{a\in\mathcal{A}_{d}}\{(a,u)\rvert\ \mathcal{U}_{a}\}$。
- en: 3.8 Types of Action Spaces
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 动作空间的类型
- en: 'From the above definitions we see that environments have different requirements
    with regard to their action spaces [9231687]. This survey will discuss approaches
    for:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述定义中，我们可以看到环境对其动作空间有不同的要求 [9231687]。本调查将讨论以下方法：
- en: Discrete actions $a\in\{0,1,...N\}$, with $N\in\mathcal{N}$ available actions
    in a given state.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 离散动作 $a\in\{0,1,...N\}$，在给定状态下有 $N\in\mathcal{N}$ 个可用动作。
- en: MultiDiscrete action vectors $\bm{a}$. Each $a_{i}$ is a discrete action with
    $N$ possibilities.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: MultiDiscrete 动作向量 $\bm{a}$。每个 $a_{i}$ 是一个具有 $N$ 种可能性的离散动作。
- en: Continuous actions, where an action $a\in\mathbb{R}$ is a real number, or a
    vector of real numbered actions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 连续动作，其中一个动作 $a\in\mathbb{R}$ 是一个实数，或者是一个实数向量。
- en: Slate actions $\bm{a}=\{a_{1},a_{2},...,a_{n}\}$ from which one can be selected.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以选择的 Slate 动作 $\bm{a}=\{a_{1},a_{2},...,a_{n}\}$。
- en: Parameterized Actions, mixed discrete-continuous, e.g., a tuple $(a,u)$ where
    $a$ is a discrete action, and $u$ is a continuous action.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化动作，混合离散-连续，例如，元组 $(a,u)$ 其中 $a$ 是离散动作，$u$ 是连续动作。
- en: 3.9 Reinforcement Learning
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.9 强化学习
- en: The goal of an RL algorithm is to learn a policy $\pi$ that maps states to a
    probability distribution over the actions $\pi:\mathcal{S}\rightarrow P(\mathcal{A})$,
    so as to maximize the expected return $\mathbb{E}_{\pi}[\sum^{H-1}_{t=0}\gamma^{t}r_{t}]$.
    Here, $H$ is the length of the horizon, and $\gamma$ is a discount factor $\gamma\in[\texttt{0},1]$
    weighting the value of future rewards. Many of the approaches discussed in this
    survey use the Q-learning algorithm introduced by Watkins [watkins1989learning,
    watkins1992q] as their foundation. Using a dynamic programming approach, the algorithm
    learns action-value estimates (Q-values) independent of the agent’s current policy.
    Q-values are estimates of the discounted sum of future rewards (the return) that
    can be obtained at time $t$ through selecting an action $a\in\mathcal{A}$ in a
    state $s_{t}$, providing the optimal policy is selected in each state that follows.
    Q-learning is an *off-policy* temporal-difference (TD) learning algorithm.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: RL 算法的目标是学习一个策略 $\pi$，该策略将状态映射到动作的概率分布 $\pi:\mathcal{S}\rightarrow P(\mathcal{A})$，以最大化期望回报
    $\mathbb{E}_{\pi}[\sum^{H-1}_{t=0}\gamma^{t}r_{t}]$。其中，$H$ 是时间范围的长度，$\gamma$ 是折扣因子
    $\gamma\in[\texttt{0},1]$，它对未来奖励的价值进行加权。本调查中讨论的许多方法以 Watkins [watkins1989learning,
    watkins1992q] 提出的 Q-learning 算法为基础。该算法使用动态规划方法，学习与代理当前策略无关的动作价值估计（Q值）。Q值是对未来奖励（回报）的折扣和的估计，这些奖励可以在时间
    $t$ 通过在状态 $s_{t}$ 中选择动作 $a\in\mathcal{A}$ 获得，前提是每个后续状态中选择了最优策略。Q-learning 是一种
    *离策略* 时间差分（TD）学习算法。
- en: 'In environments with a low-dimensional state space Q-values can be maintained
    using a Q-table. Upon choosing an action $a$ in state $s$ according to a policy
    $\pi$, the Q-table is updated by bootstrapping the immediate reward $r$ received
    in state $s^{\prime}$ plus the discounted expected future reward from the next
    state, using a discount factor $\gamma\in\left(\texttt{0},1\right]$ and scalar
    $\alpha$ to control the learning rate: $Q_{k+1}(s,a)\leftarrow Q_{k}(s,a)+\alpha\big{(}r+\gamma\max_{s\in\mathcal{S}}\left[Q_{k}\left(s^{\prime},a\right)-Q_{k}\left(s,a\right)\right]\big{)}$.
    Many sequential decision problems have a high-dimensional state space. Here, Q-values
    can be approximated using a function approximator, for instance using a neural
    network. A recap of popular DRL approaches is provided in \autorefsota_drl_approaches.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在低维状态空间的环境中，可以使用 Q-表来维护 Q 值。在根据策略 $\pi$ 在状态 $s$ 中选择动作 $a$ 后，通过引导在状态 $s^{\prime}$
    中获得的即时奖励 $r$ 和来自下一个状态的折扣期望未来奖励来更新 Q-表，使用折扣因子 $\gamma\in\left(\texttt{0},1\right]$
    和标量 $\alpha$ 来控制学习率：$Q_{k+1}(s,a)\leftarrow Q_{k}(s,a)+\alpha\big{(}r+\gamma\max_{s\in\mathcal{S}}\left[Q_{k}\left(s^{\prime},a\right)-Q_{k}\left(s,a\right)\right]\big{)}$。许多序列决策问题具有高维状态空间。在这种情况下，可以使用函数逼近器来近似
    Q 值，例如使用神经网络。关于流行的 DRL 方法的回顾见 \autorefsota_drl_approaches。
- en: 3.10 Joint Policies
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.10 联合策略
- en: 'Our domain of interest can feature multiple agents. For each agent $p$, the
    strategy $\pi_{p}$ represents a mapping from the state space to a probability
    distribution over actions: $\pi_{p}:\mathcal{S}_{p}\rightarrow\Delta(\mathcal{A}_{p})$.
    Transitions within Markov games are determined by a joint policy. The notation
    $\bm{\pi}$ refers to a joint policy of all agents. Joint policies excluding agent
    $p$ are defined as $\bm{\pi}_{-p}$. The notation $\langle\pi_{p},\bm{\pi}_{-p}\rangle$
    refers to a joint policy with agent $p$ following $\pi_{p}$ while the other agents
    follow $\bm{\pi}_{-p}$.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的领域可以包含多个代理。对于每个代理 $p$，策略 $\pi_{p}$ 表示从状态空间到动作的概率分布的映射：$\pi_{p}:\mathcal{S}_{p}\rightarrow\Delta(\mathcal{A}_{p})$。马尔可夫游戏中的转移由联合策略决定。符号
    $\bm{\pi}$ 指所有代理的联合策略。排除代理 $p$ 的联合策略定义为 $\bm{\pi}_{-p}$。符号 $\langle\pi_{p},\bm{\pi}_{-p}\rangle$
    指代理 $p$ 遵循 $\pi_{p}$ 的联合策略，而其他代理遵循 $\bm{\pi}_{-p}$。
- en: 4 Environments
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 环境
- en: A number of benchmarking environments have emerged in recent years that grant
    learning agents access to an abstracted version of the ACO problem, including
    the CybORG environment [cage_cyborg_2022] and YAWNING TITAN (YT) [YAWNING]. In
    ACO-gyms, the Red agent is tasked with moving through the network graph, compromising
    nodes in order to progress. The end goal in most cases is to reach and impact
    a high-value target node. In all ACO-gyms, the task of the Blue agent is conceptually
    identical – to identify and counter the Red agent’s intrusion and advances using
    the available actions. These actions differ significantly per ACO-gym, with actions
    including scanning/monitoring for Red activity; isolating, making safe, and reducing
    vulnerability of nodes; and deploying decoy nodes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来出现了一些基准测试环境，它们为学习代理提供了 ACO 问题的抽象版本，包括 CybORG 环境 [cage_cyborg_2022] 和 YAWNING
    TITAN (YT) [YAWNING]。在 ACO-gyms 中，红色代理的任务是通过网络图移动，破坏节点以推进。大多数情况下，最终目标是到达并影响一个高价值的目标节点。在所有
    ACO-gyms 中，蓝色代理的任务在概念上是相同的——使用可用的动作识别并阻止红色代理的入侵和进攻。这些动作在不同的 ACO-gyms 中差异很大，包括扫描/监控红色活动；隔离、使节点安全、降低节点脆弱性；以及部署诱饵节点。
- en: Since the specific observations and actions are ACO-gym dependant, it can be
    surmised that these are not particular to the ACO problem. Any other environment
    which shares core features in its internal dynamics, and structure of observation
    and action spaces, should provide a suitable platform for developing methodologies
    to tackle ACO challenges before the ACO-gyms themselves are able to fully represent
    the problem. We have identified a total of 14 desirable criteria to assess the
    suitability of environments from other domains for developing and assessing techniques
    for tackling challenges in the ACO domain (see \autoreftab:allenvs).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特定的观察和动作依赖于 ACO-gym，可以推测这些特征并不特定于 ACO 问题。任何其他具有核心特征的环境，包括内部动态和观察、动作空间结构，应提供一个适合开发方法的平台，以应对
    ACO 挑战，直到 ACO-gyms 能够完全表示问题。我们已经确定了总共 14 项期望标准，以评估其他领域环境在开发和评估处理 ACO 挑战的技术方面的适用性（参见
    \autoreftab:allenvs）。
- en: 'Code: The availability of code can significantly speed-up research efforts.
    We only list codeless environments that offer unique properties, or require minimal
    effort to implement.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 代码：代码的可用性可以显著加快研究进展。我们只列出提供独特属性或需要最少实现工作量的无代码环境。
- en: 'Adversarial: ACO is typically an adversarial game between a Blue and a Red
    agent.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性：ACO 通常是蓝色代理和红色代理之间的对抗游戏。
- en: 'General Sum & Team Games: Networks can span multiple geographic locations.
    Physical restraints, such as data transmission capacity and latency, can therefore
    necessitate a multi-agent reinforcement learning (MARL) approach.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通用和团队游戏：网络可以跨越多个地理位置。因此，物理限制，例如数据传输容量和延迟，可能需要采用多智能体强化学习（MARL）方法。
- en: 'Stochastic: ACO features stochastic factors, e.g., user activity, equipment
    failures and the likelihood of a Red attack succeeding. To meet this criteria
    stochastic state transitions are required. Random starting positions alone are
    insufficient.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 随机性：ACO 具有随机因素，例如用户活动、设备故障以及红色攻击成功的可能性。为了满足这一标准，需要进行随机状态转换。仅随机起始位置是不够的。
- en: 'Partially Observable: In all ACO-gyms, Blue must perform a scan/monitor action
    to observe Red activities. Future ACO-gyms are likely to further reduce observability,
    requiring more sophisticated Blue policies to detect Red agent actions.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 部分可观察：在所有 ACO-gyms 中，蓝色代理必须执行扫描/监控操作以观察红色活动。未来的 ACO-gyms 可能会进一步减少可观察性，需要更复杂的蓝色策略来检测红色代理的动作。
- en: 'Graph-Based: With ACO taking place on networks, ACO-gyms contain underlying
    dynamics unique to graph-based domains. Graph-based observation spaces are not
    necessarily required to fulfil this criteria. However, an environment being checked
    implies the presence of a graph structure.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图：由于 ACO 在网络上进行，ACO-gyms 包含独特的图形域的底层动态。并不一定要求基于图的观察空间来满足这一标准。然而，检查的环境暗示存在图结构。
- en: 'Multi-Discrete: Given the high-dimensionality challenge present in both the
    observation and action spaces of ACO-gyms – which increase in size with the number
    of nodes – the presence of multiple discrete dimensions in the observation and
    action spaces is highly desirable.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 多离散性：鉴于 ACO-gyms 的观察和动作空间中存在的高维挑战——这些空间随着节点数量的增加而增大——在观察和动作空间中存在多个离散维度是非常可取的。
- en: 'Extensible Dimension: A property of graph-based dynamics is a dimension that
    can be expanded to geometrically increase the size of the composite space, e.g.,
    the number of nodes in a network. Environments that meet this criteria allow methodologies
    to be tested for scalability to larger observation and action spaces.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展维度：基于图的动态特性之一是可以扩展的维度，这可以几何地增加复合空间的大小，例如，网络中的节点数量。符合此标准的环境允许方法被测试以扩展到更大的观察和动作空间。
- en: 'Continuous Dimension: As ACO-gyms scale up, a subset of action dimensions might
    need to be treated as continuous, e.g., networks featuring a large number of IP
    addresses. Additionally, continuous attributes may be present in the observation
    spaces, representing node vulnerability for example [YAWNING]. Therefore, environments
    with *mixed* input and output types are desirable.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 连续维度：随着 ACO-gyms 的扩展，某些动作维度可能需要被视为连续的，例如，具有大量 IP 地址的网络。此外，观察空间中可能存在连续属性，例如表示节点脆弱性的属性 [YAWNING]。因此，具有*混合*输入和输出类型的环境是理想的。
- en: 'Traditionally Intractable: By identifying environments where the observation
    or action space is intractable, we indicate one of two properties that pose a
    significant, if not impossible, challenge to traditional RL methodologies:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上不可处理：通过识别观察或动作空间不可处理的环境，我们表明这些环境具备两个特性之一，这对传统的 RL 方法构成了重大挑战，甚至是不可能的挑战：
- en: '1.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The multi-discrete space causes an exponential scaling of the size of the space
    when flattened, such that it quickly becomes computationally intractable.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多离散空间会导致空间大小的指数级扩展，平展后迅速变得计算上不可处理。
- en: '2.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The observation or action space changes in size as a function of the environment;
    standard DRL methods require a consistent shape and size.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 观察或动作空间的大小随着环境的变化而变化；标准的 DRL 方法需要一致的形状和大小。
- en: Environments that meet the above criteria pose a challenge to traditional DRL
    methodologies, and therefore necessitate novel algorithms or formulations. However,
    even current ACO-gyms do not meet all these criteria (see \autoreftab:allenvs).
    Below we provide a critical evaluation of current ACO-gyms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 满足上述标准的环境对传统 DRL 方法构成挑战，因此需要新颖的算法或模型。然而，即使是当前的 ACO-gyms 也未满足所有这些标准（见 \autoreftab:allenvs）。下面我们提供了对当前
    ACO-gyms 的关键评估。
- en: '| Environment Properties |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 环境特性 |'
- en: '| --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Categories | ACO | NBD | GD2D | DCC | RS | MG |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | ACO | NBD | GD2D | DCC | RS | MG |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| General \csvreader[head to column names, late after line= |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 一般 \csvreader[head to column names, late after line= |'
- en: '| , late after last line= |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| , late after last line= |'
- en: 'Table 1: An overview of the environments that we evaluated for this survey
    and their properties, with ✓and X indicating if the environment fulfills a criteria.
    The third symbol, $\nearrow$ indicates that the environment does not fulfil the
    criteria currently (due to enabling software features not being present, whether
    by design or due to incomplete implementations) but that the required features
    could be implemented with a modest amount of development effort, without the need
    for major extensions of environment scope. Along with ACO itself, other categories
    include: Gridworld Domains & 2D Games (GD2D), Discretised Continuous Control Games
    (DCC), Network-Based Domains (NBD), Recommender Systems (RS), and, Miscellaneous
    Games (MG). The latter is for unique environments which do not justify an additional
    category.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：我们为本次调查评估的环境及其属性概览，其中 ✓ 和 X 表示环境是否满足某项标准。第三个符号，$\nearrow$ 表示环境当前未满足标准（由于启用的软件功能未存在，无论是设计原因还是由于实现不完整），但所需的功能可以通过适度的开发工作来实现，无需对环境范围进行重大扩展。除了
    ACO 之外，其他类别包括：Gridworld Domains & 2D Games (GD2D)，Discretised Continuous Control
    Games (DCC)，Network-Based Domains (NBD)，Recommender Systems (RS)，以及 Miscellaneous
    Games (MG)。最后一类是指那些不值得单独设立类别的独特环境。
- en: 'CybORG: The Cyber Operations Research Gym (CybORG) [cyborg_acd_2021] is a framework
    for implementing a range of ACO scenarios. It provides a common interface at two
    levels of fidelity in the form of a finite state machine and an emulator. The
    latter provides for each node: an operating system; hardware architecture; users,
    and; passwords [cage_cyborg_2022]. CybORG allows for both the training of Blue
    and Red agents. Therefore, in principle, it provides an ACO environment for adversarial
    learning. However, in practice further modifications are required in order to
    support the loading of a DRL opponent. This is due to environment wrappers (for
    reducing the size of the observation space and mapping actions) only being available
    to the agent that is being trained. Currently, the opponent within the environment
    instance receives raw unprocessed dictionary observations. Furthermore, while
    CybORG does allow for customizable configuration files, in practice we find that
    specifying a custom network is non-trivial.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: CybORG：网络操作研究实验室（CybORG） [cyborg_acd_2021] 是一个用于实施各种 ACO 场景的框架。它提供了两级保真度的通用接口，分别是有限状态机和模拟器。后者为每个节点提供：操作系统；硬件架构；用户；以及密码 [cage_cyborg_2022]。CybORG
    允许同时训练蓝方和红方代理。因此，原则上，它提供了一个用于对抗学习的 ACO 环境。然而，实际上需要进一步的修改才能支持加载 DRL 对手。这是因为环境包装器（用于减少观察空间的大小和映射动作）仅对正在训练的代理可用。目前，环境实例中的对手接收到的是原始未处理的字典观察数据。此外，虽然
    CybORG 允许自定义配置文件，但实际上，我们发现指定自定义网络并不简单。
- en: 'Yawning Titan: The YT environment intentionally abstracts away the additional
    information used by CybORG’s emulator, facilitating the rapid integration and
    evaluation of new methods [YAWNING]. It is built to train ACO agents to defend
    arbitrary network topologies that can be specified using highly customisable configuration
    files. Each machine in the network has parameters that affect the extent to which
    they can be impacted by Blue and Red agent behaviour, including vulnerability
    scores on how easy it is for a node to be compromised. However, while YT is an
    easy to configure lightweight ACO environment, it currently lacks the ability
    to train Red agents, and does not support adversarial learning.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 打哈欠的泰坦（Yawning Titan）：YT 环境故意抽象化了 CybORG 的模拟器所使用的额外信息，促进了新方法的快速集成和评估 [YAWNING]。它旨在训练ACO代理以防御可以通过高度可定制的配置文件指定的任意网络拓扑。网络中的每台机器都有影响其受蓝方和红方代理行为影响程度的参数，包括节点被攻破的易受攻击分数。然而，虽然
    YT 是一个易于配置的轻量级 ACO 环境，但它目前缺乏训练红方代理的能力，并且不支持对抗学习。
- en: 'Network Attack Simulator (NASim): This ACO environment is a simulated computer
    network complete with vulnerabilities, scans and exploits designed to be used
    as a testing environment for AI agents and planning techniques applied to network
    penetration testing [schwartz2019autonomous]. NASim meets a number of our criteria,
    and supports scenarios that involve a large number of machines. However, [nguyen2020multiple]
    find that the probability of Red agent attacks succeeding in NASim are far greater
    than in reality. As a result the authors add additional rules to increase the
    task difficulty. NASim also does not support adversarial learning.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 网络攻击模拟器（NASim）：这个ACO环境是一个模拟计算机网络，具有漏洞、扫描和利用工具，旨在作为AI代理和应用于网络渗透测试的规划技术的测试环境 [schwartz2019autonomous]。NASim
    满足了我们的一些标准，并支持涉及大量机器的场景。然而，[nguyen2020multiple] 发现，红方代理攻击在 NASim 中成功的概率远高于现实。因此，作者增加了额外的规则以提高任务难度。NASim
    也不支持对抗学习。
- en: While ACO-gyms currently do not meet all the criteria that we have outlined
    above, it is likely that they will in the future. As the number of observation
    and action space dimensions grow with ACO-gyms maturing, so too will their scale;
    and since, in reality, the number of nodes on a computer network does not remain
    constant, neither shall the observation and action space sizes. Below we discuss
    a number of environments from several domains which could be used to develop and
    evaluate approaches designed to address a subset of the challenges identified.
    An overview of the environments that we evaluated is provided in \autoreftab:allenvs.
    Below we focus on four environments that meet a large number of ACO requirements.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ACO-gyms 当前未满足我们上述所有标准，但它们未来可能会满足这些标准。随着 ACO-gyms 成熟，观测和动作空间维度的数量增加，它们的规模也会增长；而且，实际上计算机网络上的节点数量并不保持不变，观测和动作空间的大小也不会保持不变。以下我们讨论了几个领域中的多个环境，这些环境可以用来开发和评估旨在解决识别出的挑战子集的方法。我们评估的环境概述见
    \autoreftab:allenvs。下面我们重点关注满足大量 ACO 要求的四个环境。
- en: MicroRTS [huang2021gym] is a simple Real-Time Strategy (RTS) game. It is designed
    to enable the training of RL agents on an environment which is similar to PySC2 [vinyals2017starcraft],
    the RL environment adaptation of the popular RTS game StarCraft II which possesses
    huge complexity. Therefore, MicroRTS is a lightweight version of PySC2, without
    the extensive (and expensive) computational requirements. The game features a
    gridworld-like observation space, where for a map of size $h\times w$, the observation
    space is of size $h\times w\times f$, where $f$ is a number of discrete features
    which may exist in any square. This observation space can be set to be partially
    observable. The action space is a large multi-discrete space, where each worker
    is commanded via a total of seven discrete actions. The number of workers will
    change during the game, making the core RTS action space intractable. In order
    to handle this, the authors of MicroRTS implemented action decomposition as part
    of the environment, and the action space is separated into the unit action space
    (of 7 discrete actions), and the player action space. While the authors discuss
    two formulations of this player action space, the one which is documented in code
    is the *GridNet* approach, whereby the agent predicts an action for each cell
    in the grid, with only the actions on cells containing player-owned units being
    carried out. This leads to a total action space size of $h\times w\times 7$. The
    challenge of varying numbers of agents is remarkably similar to the challenge
    of varying numbers of nodes in ACO. The MicroRTS environment, if action decomposition
    were to be stripped away, would be a strong candidate for handling large and variably-sized
    discrete action spaces.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: MicroRTS [huang2021gym] 是一个简单的实时战略（RTS）游戏。它旨在使 RL 代理能够在一个类似于 PySC2 [vinyals2017starcraft]
    的环境中进行训练，后者是对复杂度极高的热门 RTS 游戏《星际争霸 II》的 RL 环境适配。因此，MicroRTS 是 PySC2 的一个轻量级版本，没有大量（且昂贵的）计算需求。该游戏具有类似网格世界的观测空间，对于一个大小为
    $h\times w$ 的地图，观测空间的大小为 $h\times w\times f$，其中 $f$ 是可能存在于任意方格中的离散特征的数量。这个观测空间可以设置为部分可观察。动作空间是一个大型多离散空间，每个工人通过总共七个离散动作来指挥。游戏中的工人数量会发生变化，使核心
    RTS 动作空间变得难以处理。为了应对这一挑战，MicroRTS 的作者将动作分解作为环境的一部分实现，动作空间被分为单位动作空间（7 个离散动作）和玩家动作空间。尽管作者讨论了玩家动作空间的两种公式，但在代码中记录的是
    *GridNet* 方法，该方法使代理预测网格中每个单元的动作，仅对包含玩家控制单位的单元执行动作。这导致了总动作空间大小为 $h\times w\times
    7$。代理数量变化的挑战与 ACO 中节点数量变化的挑战非常相似。如果剥离动作分解，MicroRTS 环境将是处理大型和可变大小离散动作空间的强有力候选者。
- en: Nocturne [nocturne2022] is a 2D driving simulator, built from the Waymo Open
    Dataset [Sun_2020_CVPR]. Agents learn to drive in a partially observable environment
    without having to handle direct sensor inputs, e.g., video from imaging cameras.
    Nocturne is a general-sum game. Each agent is required to both coordinate its
    own actions to reach its own goal and cooperate with others to avoid congestion.
    It is partially observable, with objects in the environment (both static and moving)
    causing occlusion of objects (from the egocentric perspective of the agent). However,
    it is not stochastic, with the state-transition probabilities being deterministic.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Nocturne [nocturne2022] 是一个 2D 驾驶模拟器，基于 Waymo 开放数据集 [Sun_2020_CVPR] 构建。代理在部分可观测的环境中学习驾驶，无需处理直接传感器输入，例如，来自成像摄像头的视频。Nocturne
    是一个零和游戏。每个代理既需要协调自己的动作以实现目标，又需要与其他代理合作以避免拥堵。它是部分可观测的，环境中的物体（包括静态和移动的）会遮挡物体（从代理的自我中心视角）。然而，它不是随机的，状态转移概率是确定性的。
- en: SUMO [SUMO2018] is a traffic simulation tool which allows for the implementation
    of complex, distributed traffic light control scenarios. While neither Python-based
    nor intended for RL, there exists a third-party interface for fulfilling these
    criteria [sumorl]. There are two configurations for this environment; single-agent,
    and multi-agent. For this review, we consider the multi-agent configuration, but
    treat it as a single-agent problem since, while the multi-agent scenario introduces
    greater complexity with multiple junctions needing to be controlled, it does not
    necessitate the use of multiple agents. The goal of an agent in SUMO is to control
    traffic signals at junctions in order to minimise the delay of vehicles passing
    through. The observation space is a small, but extensible (with regard to the
    number of junctions) continuous space, containing information about the amount
    of traffic in incoming lanes. For the action space, each set of lights is controlled
    by a single discrete variable, each corresponding to a configuration of lights.
    For example, at a two-way single intersection there are four possible configurations.
    However, each intersection type has a different number of configurations, meaning
    that the action space is not only multi-discrete, but also uneven. Further, as
    the intersections are connected by roads in the simulation, the environment will
    inevitably contain graph-like dynamics, i.e., the actions at, and traffic through,
    each intersection will affect the state at connected junctions. As such, the action
    space and dynamics of SUMO are applicable to ACO. However, the small observation
    space and lack of adversarial or partially observable properties limit this applicability.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SUMO [SUMO2018] 是一个交通仿真工具，允许实现复杂的、分布式的交通信号控制场景。虽然它既不是基于 Python 的，也不打算用于 RL，但存在一个第三方接口来满足这些标准
    [sumorl]。该环境有两种配置：单代理和多代理。对于本次评审，我们考虑多代理配置，但将其视为单代理问题，因为尽管多代理场景引入了更多复杂性，涉及多个需要控制的交叉口，但并不需要使用多个代理。SUMO
    中的代理目标是控制交叉口的交通信号，以最小化通过车辆的延迟。观察空间是一个小而可扩展（关于交叉口数量）的连续空间，包含有关进入车道的交通量的信息。对于动作空间，每组信号灯由一个离散变量控制，每个变量对应于一种信号灯配置。例如，在一个双向单交叉口，有四种可能的配置。然而，每种交叉口类型的配置数量不同，这意味着动作空间不仅是多离散的，而且是不均匀的。此外，由于交叉口通过道路连接，环境不可避免地包含类似图的动态，即每个交叉口的动作和交通流量将影响连接交叉口的状态。因此，SUMO
    的动作空间和动态适用于 ACO。然而，小的观察空间和缺乏对抗性或部分可观测特性限制了其适用性。
- en: RecSim [ie2019recsim] is a configurable framework for creating RL environments
    for recommender systems. In RecSim, an agent observes multiple sets of features
    from both a user and a document database, and makes document recommendations based
    on these. These sets of features are comprised of features from the user, their
    responses to previous recommendations, and the available documents for recommendation.
    These observations fulfil all desirable criteria related to observation spaces.
    Not only are they stochastic (due to randomness in the user’s behaviour) and partially
    observable (due to incompleteness in the visible user features), but they are
    also an extensible mix of discrete and continuous features which scale exponentially
    with both the number of documents and the number of features extracted from them.
    As such, the observation space of RecSim poses a complex challenge similar to
    that presented in ACO. The action space, however, is less remarkable. The agent
    must choose a number of documents to recommend, which is a multi-discrete space
    which is potentially extensible as the number of recommendations and documents
    increases. However, this is unlikely to be intractable unless the number of documents
    becomes vast. Nevertheless, this large space complements the novel and complex
    observation space, making this a strong candidate for experimenting with approaches
    for ACO challenges.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: RecSim [ie2019recsim] 是一个用于创建推荐系统的 RL 环境的可配置框架。在 RecSim 中，代理观察到来自用户和文档数据库的多个特征集，并根据这些特征集进行文档推荐。这些特征集包括用户的特征、他们对之前推荐的响应以及可供推荐的文档。这些观察满足了与观察空间相关的所有期望标准。它们不仅是随机的（由于用户行为的随机性）和部分可观察的（由于可见用户特征的不完整性），而且还是一个可扩展的离散和连续特征的混合，这些特征随着文档数量和从中提取的特征数量的增加呈指数增长。因此，RecSim
    的观察空间带来了类似于 ACO 的复杂挑战。然而，动作空间却不那么显著。代理必须选择若干文档进行推荐，这是一个多离散空间，可能会随着推荐数量和文档数量的增加而扩展。然而，除非文档数量变得庞大，否则这不太可能变得不可处理。尽管如此，这个大空间与新颖复杂的观察空间相辅相成，使其成为实验
    ACO 挑战方法的有力候选者。
- en: In summary, in this section, we have reviewed multiple environments and domains
    which hold some relevance to the challenges presented by ACO. While none of the
    environments reviewed meet all of our desirable criteria. Each challenge is represented
    in at least one domain. The most scarce of these challenges, but one which is
    core to ACO, is the presence of graph-based dynamics. These could only be found
    in network-based domains, and even there, not all environments possess them. Therefore,
    if the ACO solution being investigated aims to tackle graph-based dynamics, one
    would be limited to environments in this domain. Should graph-based dynamics not
    be required for development, there are several options for the other desirable
    criteria. StarCraft II presents a particularly complex challenge, even in cut-down
    versions of the game such as SMAC [samvelyan19smac, ellis2022smacv2], which meets
    a large number of our requirements. However, it is a computationally demanding
    environment. Micro-RTS [huang2021gym], an environment specifically designed to
    provide a less computationally demanding version of the problems presented by
    StarCraft II, is an attractive alternative. While the observation space is a simplified
    version of StarCraft II’s, the action space holds much of the same complexity,
    with variable numbers of agents posing an issue of intractability.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在本节中，我们回顾了多个与 ACO 挑战相关的环境和领域。虽然没有一个回顾的环境完全符合我们所有的期望标准，但每个挑战在至少一个领域中有所体现。其中最稀缺的挑战是图形动态的存在，而这是
    ACO 的核心。这些挑战只能在基于网络的领域中找到，即使在那里，并非所有环境都具备它们。因此，如果正在研究的 ACO 解决方案旨在应对图形动态，那么将受到限于此领域中的环境。如果开发中不需要图形动态，则在其他期望标准中有几种选择。星际争霸
    II 即使在如 SMAC [samvelyan19smac, ellis2022smacv2] 这样的简化版本中也提供了特别复杂的挑战，并且满足了大量要求。然而，它是一个计算要求高的环境。Micro-RTS
    [huang2021gym] 是一个专门设计用于提供比星际争霸 II 更少计算需求的版本的环境，是一个有吸引力的替代方案。尽管观察空间是星际争霸 II 的简化版本，但动作空间保持了相同的复杂性，变量数量的代理构成了一个不可处理的问题。
- en: 5 Coping with Vast High-Dimensional Inputs
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 处理庞大的高维输入
- en: Due to an ever growing means through which large amounts of data can be harvested,
    the curse of dimensionality is a prevalent problem for ML and data mining tasks.
    This has implications for DRL. Learning efficiency is reduced due to unnecessary
    features contributing noise [zebari2020comprehensive], and the state space can
    grow exponentially with the size of the state representation [burden2021latent].
    Here, maintaining a sufficient sample spread over the state-action space becomes
    challenging [de2015importance].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可以收集大量数据的手段不断增加，维度诅咒是机器学习（ML）和数据挖掘任务中一个普遍存在的问题。这对深度强化学习（DRL）有影响。由于不必要的特征引入噪声，学习效率降低 [zebari2020comprehensive]，状态空间可能随着状态表示的大小呈指数增长 [burden2021latent]。在这里，维持状态-动作空间的足够样本分布变得具有挑战性 [de2015importance]。
- en: 'Dimensionality reduction techniques are a natural choice for dealing with unnecessary/noisy
    features. Benefits include: the elimination of irrelevant data and redundant features,
    while preserving the variance of the dataset; improved data quality; reduced processing
    time and memory requirements; improved accuracy; shorter training and inference
    times (i.e., reduced computing costs), and; improved performance [zebari2020comprehensive].
    Two popular approaches are *feature selection* and *feature extraction*. Feature
    selection aims to find the optimal sub-set of relevant features for training a
    model, which is an Non-deterministic Polynomial (NP)-hard problem [meiri2006using].
    In contrast, feature extraction involves creating linear combinations of the features,
    while preserving the original relative distances in the latent structures. The
    dimensionality is decreased without losing much of the initial information [zebari2020comprehensive].
    However, the resulting encodings are uninterpretable for humans.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 降维技术是处理不必要/噪声特征的自然选择。其好处包括：消除无关数据和冗余特征，同时保留数据集的方差；改善数据质量；减少处理时间和内存需求；提高准确性；缩短训练和推理时间（即，减少计算成本）；以及提升性能 [zebari2020comprehensive]。两种常用的方法是*特征选择*和*特征提取*。特征选择旨在找到适合训练模型的最优相关特征子集，这是一类非确定性多项式（NP）困难问题 [meiri2006using]。相对而言，特征提取则涉及创建特征的线性组合，同时保留潜在结构中的原始相对距离。降维时不会丢失过多初始信息 [zebari2020comprehensive]。然而，结果编码对人类来说不可解释。
- en: DRL uses Deep Neural Networks (DNNs) to directly extract features from high-dimensional
    data [mousavi2018deep, almasan2022deep]. Nevertheless, feature selection can provide
    a valuable pre-processing step for training DNNs. For example, anomaly detection
    DNNs for cyber security applications, tasked with differentiating benign from
    malicious packets, were found to perform better when trained on data where feature
    selection had been applied [9216403].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）使用深度神经网络（DNNs）直接从高维数据中提取特征 [mousavi2018deep, almasan2022deep]。然而，特征选择可以作为训练DNNs的有价值的预处理步骤。例如，用于网络安全应用的异常检测DNNs，负责区分良性和恶意数据包，发现当在应用了特征选择的数据上进行训练时，性能更佳 [9216403]。
- en: Below we will provide an overview of DNNs used in DRL for feature extraction.
    Then we shall discuss state of the art DRL approaches for further eliminating
    unnecessary information through learning *abstract* states and discuss advanced
    exploration strategies towards enabling the sufficient visitation of all *relevant*
    states to obtain accurate utility estimates [burden2021latent, pmlr-v134-perdomo21a].
    Finally, we shall take a closer look at approaches towards mitigating catastrophic
    forgetting, DNN’s tendency to unlearn previous knowledge [pmlr-v119-ota20a, de2015importance].
    A summary of the approaches discussed in this section is provided in \autoreftab:state_abstraction
    in \autorefappendix:states.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下，我们将提供用于特征提取的深度神经网络（DNNs）在深度强化学习（DRL）中的概述。接着，我们将讨论最先进的DRL方法，通过学习*抽象*状态进一步消除不必要的信息，并探讨先进的探索策略，以确保对所有*相关*状态的充分访问，从而获得准确的效用估计 [burden2021latent,
    pmlr-v134-perdomo21a]。最后，我们将详细查看减轻灾难性遗忘的策略，即DNNs倾向于遗忘先前知识 [pmlr-v119-ota20a, de2015importance]。本节讨论的方法总结见\autoreftab:state_abstraction和\autorefappendix:states。
- en: 5.1 Function Approximators
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 函数逼近器
- en: 'Many of the successes in RL over the past decade rely on the ability of DNNs
    to identify intricate structures and extract compact features from complex high-dimensional
    samples  [Goodfellow-et-al-2016, karpathy2014large, lecun2015deep]. These approaches
    work well for numerous domains that confront learners with the curse of dimensionality,
    such as the *Arcade Learning Environment* (ALE) [bellemare2013arcade], where DNNs
    are used to encode image observations. However, many of these domains are fully
    observable and contain state spaces with a dimensionality that is manageable for
    current DNNs. In contrast, this survey is focused on environments where the architectures
    used by standard DRL approaches cannot scale, necessitating innovative solutions.
    Here, considerations are required with respect to efficiently encoding an *overwhelmingly*
    large observation space to a low dimensional representation, while limiting concessions
    regarding performance. First, we will discuss two popular feature extraction techniques
    used by DRL: Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年中，强化学习的许多成功依赖于DNNs识别复杂结构和从复杂高维样本中提取紧凑特征的能力[Goodfellow-et-al-2016, karpathy2014large,
    lecun2015deep]。这些方法在面对具有维度诅咒的众多领域中表现良好，例如*街机学习环境*(ALE)[bellemare2013arcade]，其中DNNs用于编码图像观测。然而，这些领域中的许多是完全可观察的，并且包含的状态空间维度对于当前的DNNs是可管理的。相对而言，本调查关注于标准DRL方法所使用的架构无法扩展的环境，需要创新的解决方案。在这里，需要考虑如何高效地将*极其庞大的*观测空间编码为低维表示，同时尽量减少对性能的妥协。首先，我们将讨论DRL中使用的两种流行特征提取技术：卷积神经网络（CNNs）和图神经网络（GNNs）。
- en: 'Convolutional Neural Networks: CNNs can be optimized to extract features from
    high dimensional arrays and tensors via multiple stacked linear convolution and
    pooling layers, banks of filters which are convolved with an input to produce
    an output map [lecun2015deep, Goodfellow-et-al-2016]. The first layer may learn
    to extract edges, which can then be combined into corners and contours by the
    subsequent layers. These features can be combined to form the object parts that
    enable a classification, for instance through adding fully connected layers that
    precede the output layer [lecun2015deep, Goodfellow-et-al-2016].'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络：CNN可以通过多个堆叠的线性卷积层和池化层、与输入进行卷积的滤波器组来从高维数组和张量中提取特征，从而优化其性能[lecun2015deep,
    Goodfellow-et-al-2016]。第一层可能学习提取边缘，随后层可以将这些边缘组合成角点和轮廓。这些特征可以组合形成对象部分，从而实现分类，例如通过添加完全连接的层来预处理输出层[lecun2015deep,
    Goodfellow-et-al-2016]。
- en: 'CNNs have a large learning capacity and can be trained to implement complex
    functions that are sensitive towards minute details within inputs  [lecun2015deep,
    krizhevsky2012imagenet, wu2019wider]. DNNs can be trained end-to-end using stochastic
    gradient descent via the back-propagation procedure, providing that the network
    consists of smooth functions [Goodfellow-et-al-2016]. CNNs take advantage of assumptions
    regarding the location of pixel dependencies within images. This allows CNNs to
    reduce the number of weighted connections compared to fully-connected DNNs [krizhevsky2012imagenet].
    However, while CNNs are still considered a state-of-the-art (SOTA) approach for
    processing data in the form of arrays and tensors, there are formulations where
    CNNs are not directly applicable, for instance: graph-based representations.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: CNN具有较大的学习能力，可以被训练以实现对输入中微小细节敏感的复杂函数[lecun2015deep, krizhevsky2012imagenet,
    wu2019wider]。DNNs可以通过反向传播过程使用随机梯度下降进行端到端训练，前提是网络由平滑函数组成[Goodfellow-et-al-2016]。CNN利用了图像中像素依赖位置的假设，这使得CNN能够减少相对于完全连接的DNNs的加权连接数量[krizhevsky2012imagenet]。然而，尽管CNN仍被认为是处理数组和张量形式数据的*最先进*方法，但在某些情况下CNN并不直接适用，例如图基表示。
- en: 'Graph Neural Networks: Graph-based representations are a popular choice for
    many domains, such as traffic forecasting, drug discovery, and ACO [munikoti2022challenges].
    Conventional DNNs are not applicable to graphs, due to the graph’s uneven structure,
    irregular size of unordered nodes, and dynamic neighbourhood compositions [kipf2016semi].
    Here, GNNs have emerged as a powerful tool [JIANG2022117921]. GNNs have successfully
    been applied to graphs that systematically model relationships between node entities,
    and represent simplified versions of complex problems. Tasks include node classification,
    link prediction, community detection and graph classification [you2019position].
    GNNs model both graph structure and node attributes via a message passing scheme,
    propagating relevant feature information of nodes to their neighbours until a
    stable equilibrium is found [kipf2016semi]. GNNs are primarily applicable to environments
    where graphs can be used to capture relationships between entities [munikoti2022challenges],
    e.g., enabling an effective factorisations of value functions for Multi-Agent
    Reinforcement Learning (MARL) in the StarCraft Multi-Agent Challenge (SMAC) [kortvelesy2022qgnn],
    or for modelling relationships between agents and objects in multi-task DRL [hong2022structureaware].'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）：基于图的表示在许多领域中都非常流行，例如交通预测、药物发现和ACO [munikoti2022challenges]。由于图的结构不均匀、无序节点的大小不规则以及动态的邻域组成，传统的深度神经网络（DNNs）并不适用于图数据 [kipf2016semi]。在这里，GNNs作为一种强大的工具应运而生 [JIANG2022117921]。GNNs成功应用于系统性建模节点实体之间关系的图，并表示复杂问题的简化版本。任务包括节点分类、链接预测、社区检测和图分类 [you2019position]。GNNs通过消息传递机制建模图结构和节点属性，将节点的相关特征信息传播到其邻居，直到找到稳定的平衡 [kipf2016semi]。GNNs主要适用于可以通过图来捕捉实体之间关系的环境 [munikoti2022challenges]，例如，在StarCraft多智能体挑战（SMAC）中实现值函数的有效因式分解 [kortvelesy2022qgnn]，或者在多任务DRL中建模智能体和对象之间的关系 [hong2022structureaware]。
- en: '[munikoti2022challenges] recently conducted a survey on the opportunities and
    challenges for graph-based DRL, defining an idealised GNN for DRL as: i.) *dynamic*;
    ii.) *scalable*; iii.) providing *generalizability*, and; iv.) applicable to *multiagent*
    systems. Dynamic refers to DRL’s need for GNN approaches that can cope with time
    varying network configurations and parameters, e.g., the number of hosts varying
    over time. While GNN architectures have been proposed for dynamic graphs (e.g.,
    spatial-temporal GNNs [nicolicioiu2019recurrent]), tasks including node classification,
    link prediction, community detection, and graph classification could benefit from
    further improvements [munikoti2022challenges]. With regard to generalizability,
    there is a danger that a DRL agent can overfit on the graph structure(s) seen
    during training, and is unable to generalize across different graphs [munikoti2022challenges].'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[munikoti2022challenges] 最近对图基DRL的机会和挑战进行了调查，将理想的DRL GNN定义为：i.) *动态的*；ii.)
    *可扩展的*；iii.) 提供*泛化性*；iv.) 适用于*多智能体*系统。动态性指的是DRL需要GNN方法能够应对时间变化的网络配置和参数，例如，主机数量随时间变化。虽然已经提出了用于动态图的GNN架构（例如，时空GNNs [nicolicioiu2019recurrent]），但节点分类、链接预测、社区检测和图分类等任务仍可能受益于进一步改进 [munikoti2022challenges]。关于泛化性，存在DRL智能体可能在训练过程中对看到的图结构过拟合，并无法在不同图之间进行泛化的危险 [munikoti2022challenges]。'
- en: 'Three popular GNNs currently receiving attention from the DRL community are [munikoti2022challenges]:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 目前DRL社区关注的三种流行GNN是 [munikoti2022challenges]：
- en: 1.) *Graph Convolutional Networks (GCNs):* Using a semi-supervised learning
    approach, GCNs were the first GNNs to apply convolutional operations similar to
    those used by CNNs to learn from graph-structured data [kipf2016semi]. The model
    can learn encodings for local graph structures and the features of nodes. GCNs
    scale linearly in the number of graph edges. However, the entire graph adjacency
    matrix is required to learn these representations. Therefore, GCNs cannot generalize
    over graphs of different sizes [munikoti2022challenges]. This has implications
    for DRL, since it restricts the usage of GCNs to tasks with a static network configuration.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 1.) *图卷积网络（GCNs）：* 采用半监督学习方法，GCNs是首批将类似于CNN的卷积操作应用于图结构数据的GNN。该模型可以学习局部图结构和节点特征的编码。GCNs在图边数目上线性扩展。然而，需要整个图的邻接矩阵来学习这些表示。因此，GCNs不能对不同大小的图进行泛化 [munikoti2022challenges]。这对DRL有影响，因为它限制了GCNs在静态网络配置任务中的使用。
- en: 2.) *GraphSAGE* learns the topological node structure and the distribution of
    node features within a confined neighbourhood, computing a node’s local role in
    the graph along with global position [hamilton2017inductive]. Using an inductive
    learning approach, GraphSAGE samples node features in the local neighbourhood
    of each node and learns a functional mapping that aggregates the information received
    by each node. It is scalable to graphs of different sizes, and can be applied
    to different sub-graphs, thereby not requiring all the nodes to be present during
    training.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 2.) *GraphSAGE* 学习节点的拓扑结构以及节点特征在局部邻域内的分布，计算节点在图中的局部角色及全局位置 [hamilton2017inductive]。使用归纳学习方法，GraphSAGE
    在每个节点的局部邻域内采样节点特征，并学习一个功能映射，聚合每个节点接收到的信息。它可以扩展到不同大小的图，并可以应用于不同的子图，因此不需要在训练期间所有节点都存在。
- en: 3.) *Graph Attention Networks (GAT)* use masked self-attentional layers, allowing
    for an implicit specification of different weights for nodes in the neighbourhood,
    without the need for computationally expensive matrix operations or assuming knowledge
    of the graph structure upfront [GAT]. The approach selectively aggregates node
    contributions while suppressing minor structural details.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 3.) *图注意力网络 (GAT)* 使用掩码自注意力层，从而可以隐式地为邻域中的节点指定不同的权重，而无需计算开销大的矩阵运算或事先假设图的结构 [GAT]。这种方法选择性地聚合节点贡献，同时抑制较小的结构细节。
- en: The type of GNN selected for DRL depends on the properties of the environment.
    For large graphs GNN approaches are required that can be applied to sub-graphs,
    such as GraphSAGE [hamilton2017inductive]. Position-aware GNNs [you2019position]
    should be used when the position of a node provides critical information [munikoti2022challenges].
    Meanwhile, for dynamic graphs an appropriate approach would be to fuse GNNs with
    a Recurrent Neural Network (RNN) to capture a graph’s evolution over time, allowing
    the network to establish spatio-temporal dependencies [munikoti2022challenges].
    Indeed, even for non-graph-based environment representations we must consider
    the impact of partial observations $o$, or a trajectory of observations $\tau=\{o_{t-n},o_{t-n+1}...o_{t}\}$.
    Here RNN components are also utilized to retain relevant information [lample2017playing,
    li2021lstm, hochreiter1997long]. This allows the learner to encode and keep track
    of relevant objects, e.g., the location of other agents observed during previous
    time-steps [lample2017playing, kapturowski2019recurrent].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 选择用于深度强化学习 (DRL) 的 GNN 类型取决于环境的属性。对于大型图，要求使用能够应用于子图的 GNN 方法，例如 GraphSAGE [hamilton2017inductive]。当节点的位置提供关键信息时，应使用位置感知
    GNN [you2019position] [munikoti2022challenges]。与此同时，对于动态图，适当的方法是将 GNN 与递归神经网络
    (RNN) 融合，以捕捉图的演变，允许网络建立时空依赖关系 [munikoti2022challenges]。实际上，即使对于非图结构的环境表示，我们也必须考虑部分观测
    $o$ 的影响，或者观测的轨迹 $\tau=\{o_{t-n},o_{t-n+1}...o_{t}\}$。在这里，RNN 组件也被用于保留相关信息 [lample2017playing,
    li2021lstm, hochreiter1997long]。这使得学习者能够编码并跟踪相关对象，例如在前几个时间步中观察到的其他智能体的位置 [lample2017playing,
    kapturowski2019recurrent]。
- en: 5.2 State Abstraction
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 状态抽象
- en: The aim of state abstraction is to obtain a compressed model of an environment
    that retains all the useful information – enabling the efficient training and
    deployment of a DRL agent over an abstract formulation. Solving the abstract MDP
    is equivalent to solving the underlying MDP [pmlr-v108-abel20a, abel2019state,
    burden2018using, burden2021latent]. State abstraction groups together semantically
    similar states, abstracting the state space to a representation with lower dimensions [yu2018towards].
    A motivating example for the importance of state abstraction is the ALE game Pong [bellemare2013arcade],
    where success only requires access to the positions and velocities of the two
    paddles and the ball [pmlr-v97-gelada19a].
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 状态抽象的目标是获得一个压缩的环境模型，保留所有有用的信息——这使得可以在抽象形式上高效地训练和部署 DRL 代理。解决抽象的 MDP 等同于解决底层的
    MDP [pmlr-v108-abel20a, abel2019state, burden2018using, burden2021latent]。状态抽象将语义上相似的状态归为一组，将状态空间抽象为具有较低维度的表示
    [yu2018towards]。状态抽象重要性的一个激励性例子是 ALE 游戏 Pong [bellemare2013arcade]，其中成功只需访问两个挡板和球的位置及速度
    [pmlr-v97-gelada19a]。
- en: '[abel2019state] identify various types of abstraction discussed in the literature
    that can involve states and also actions, including: state abstraction [pmlr-v80-abel18a],
    temporal abstraction [precup2000temporal], state-action abstractions [pmlr-v108-abel20a],
    and hierarchical RL approaches [kulkarni2016hierarchical, dietterich2000overview].
    In this section we shall focus our attention on state abstraction. Formally, given
    a high-dimensional state space $\mathcal{S}$, the goal of state abstraction is
    to implement a mapping $\phi:\mathcal{S}\rightarrow\mathcal{S}_{\phi}$ from each
    state $s\in\mathcal{S}$ to an abstract state $s_{\phi}\in\mathcal{S}_{\phi}$,
    where $\rvert\mathcal{S}_{\phi}\rvert\ll\rvert\mathcal{S}\rvert$ [abel2019theory],
    and $\phi$ is an encoder [abel2019theory]. This expands the set of RL problem
    definitions defined in \autorefsec:background, introducing the notion of an Abstract
    MDP, as illustrated in \autoreffig:state_abstraction.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[abel2019state]确定文献中讨论的涉及状态和动作的各种抽象类型，包括：状态抽象[pmlr-v80-abel18a]、时间抽象[precup2000temporal]、状态-动作抽象[pmlr-v108-abel20a]和分层强化学习方法[kulkarni2016hierarchical,
    dietterich2000overview]。在本节中，我们将重点关注状态抽象。形式上，给定一个高维状态空间$\mathcal{S}$，状态抽象的目标是实现从每个状态$s\in\mathcal{S}$到抽象状态$s_{\phi}\in\mathcal{S}_{\phi}$的映射$\phi:\mathcal{S}\rightarrow\mathcal{S}_{\phi}$，其中$\rvert\mathcal{S}_{\phi}\rvert\ll\rvert\mathcal{S}\rvert$[abel2019theory]，而$\phi$是一个编码器[abel2019theory]。这扩展了\autorefsec:background中定义的RL问题集合，引入了抽象MDP的概念，如\autoreffig:state_abstraction所示。'
- en: '![Refer to caption](img/3a350571383245c1a950a5af06ea9f09.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/3a350571383245c1a950a5af06ea9f09.png)'
- en: 'Figure 3: Depiction of an Abstract MDP, that includes a mapping $\phi:\mathcal{S}\rightarrow\mathcal{S}_{\phi}$
    from the full state $s$ to an abstract state $s_{\phi}$.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：表示抽象MDP的描述，其中包括从完整状态$s$到抽象状态$s_{\phi}$的映射$\phi:\mathcal{S}\rightarrow\mathcal{S}_{\phi}$。
- en: In practice low dimensional representation are often obtained using Variational
    AutoEncoder (VAE) based architectures [kingma2013auto]. For example, [9287851]
    apply neural discrete representation learning, mapping high-dimensional raw video
    observations from an RL agent’s interactions with the environment to a low dimensional
    discrete latent representation, using a Vector Quantized AutoEncoder (VQ-AE) trained
    to reconstruct the raw video data. The benefits of the approach are demonstrated
    within a 3D navigation task in a maze environment constructed in Minecraft.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通常使用变分自动编码器（VAE）的基于体系结构的方法获得低维表示[kingma2013auto]。例如，[9287851]应用神经离散表示学习，使用经验马尔可夫决策过程（RL）代理与环境交互产生的高维原始视频观测映射到低维离散潜在表示中，使用训练用于重构原始视频数据的向量量化自动编码器（VQ-AE）。该方法在Minecraft中的三维导航任务中展示了其优点。
- en: The work from [9287851] and others [tang2017exploration, burden2018using, burden2021latent]
    demonstrates the ability of state abstraction to reduce noise from raw high-dimensional
    inputs. However, discarding too much information can result in the encoder failing
    to preserve essential features. Therefore, encoders must find a balance between
    appropriate degree of compression and adequate representational power [abel2016near].
    Using *apprenticeship learning*, where the availability of an expert demonstrator
    providing a policy $\pi_{E}$ is assumed, [abel2019state] seek to understand the
    role of information-theoretic compression in state abstraction for sequential
    decision making. The authors draw parallels between state-abstraction for RL and
    compression as understood in information theory. The work focuses on evaluating
    the extent to which an agent can perform on par with a demonstrator, while using
    as little (encoded) information as possible. Studying this property resulted in
    a novel objective function with which a VAE [kingma2013auto] can be optimized,
    enabling a convergent algorithm for computing latent embeddings with a trade-off
    between compression and value.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[9287851]及其他工作[tang2017exploration, burden2018using, burden2021latent]展示了状态抽象降低原始高维输入噪音的能力。然而，丢弃过多信息可能导致编码器无法保留关键特征。因此，编码器必须找到适当的压缩程度和足够的表示能力之间的平衡[abel2016near]。使用*学徒学习*假设有专家演示者提供策略$\pi_{E}$，[abel2019state]试图理解信息论压缩在顺序决策的状态抽象中的作用。作者在强化学习的状态抽象与信信息论中理解的压缩之间建立了类比关系。该工作集中于评估代理与演示者性能的程度，同时尽可能少地使用（编码）信息。研究这一特性导致了一个新的目标函数，通过该函数可以优化VAE[kingma2013auto]，实现压缩和价值之间的权衡。'
- en: '[pmlr-v97-gelada19a] and [zhanglearning] observe that encoder-decoder approaches
    are typically task agnostic – encodings represent all dynamic elements that they
    observe, even those which are not relevant. An idealised encoder, meanwhile, would
    learn a robust representation that maps two observations to the same point in
    the latent space while ignoring irrelevant objects that are of no consequence
    to our learning agent(s). Both works rely on the concept of bisimulation to avoid
    training a decoder. The intuition behind bisimulation is as follows.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[pmlr-v97-gelada19a] 和 [zhanglearning] 观察到编码器-解码器方法通常对任务没有特定的要求——编码表示所有它们观察到的动态元素，即使这些元素并不相关。而理想的编码器会学习一个稳健的表示，将两个观察映射到潜在空间中的同一点，同时忽略对我们学习代理无关的对象。这两个工作都依赖于双重模拟的概念，以避免训练解码器。双重模拟的直观理解如下。'
- en: Definition 5.1  (Bisimulation.).
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 5.1（双重模拟）。
- en: 'Given an MDP $\mathcal{M}$, an equivalence relation $B$ between states is a
    bisimulation relation if, for all states $s_{i},s_{j}\in\mathcal{S}$ that are
    equivalent under B (denoted $s_{i}\equiv_{B}s_{j}$ ) the following conditions
    hold:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个MDP $\mathcal{M}$，状态之间的等价关系$B$是一个双重模拟关系，如果对于所有在B下等价的状态 $s_{i},s_{j}\in\mathcal{S}$（记作
    $s_{i}\equiv_{B}s_{j}$），以下条件成立：
- en: '|  | $\mathcal{R}(s_{i},a)=\mathcal{R}(s_{j},a),\forall a\in\mathcal{A},$ |  |
    (1) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{R}(s_{i},a)=\mathcal{R}(s_{j},a),\forall a\in\mathcal{A},$ |  |
    (1) |'
- en: '|  | $\mathcal{P}(G\rvert s_{i},a)=\mathcal{P}(G\rvert s_{j},a),\forall a\in\mathcal{A},\forall
    G\in\mathcal{S}_{B},$ |  | (2) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{P}(G\rvert s_{i},a)=\mathcal{P}(G\rvert s_{j},a),\forall a\in\mathcal{A},\forall
    G\in\mathcal{S}_{B},$ |  | (2) |'
- en: where $\mathcal{S}_{B}$ is the partition of $\mathcal{S}$ under the relation
    $B$ (the set of all groups $G$ of equivalent states), and $\mathcal{P}(G\rvert
    s,a)=\sum_{s^{\prime}\in G}\mathcal{P}(s^{\prime}\rvert s,a)$.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{S}_{B}$ 是在关系 $B$ 下对 $\mathcal{S}$ 的划分（所有等价状态组 $G$ 的集合），而 $\mathcal{P}(G\rvert
    s,a)=\sum_{s^{\prime}\in G}\mathcal{P}(s^{\prime}\rvert s,a)$。
- en: '[zhanglearning] propose *deep bisimulation for control* (DBC) that learns directly
    on a bisimulation distance metric. This allows the learning of invariant representations
    that can be used effectively for downstream control policies, and are invariant
    with respect to task-irrelevant details. The encoders are trained in a manner
    such that distances in latent space equal bisimulation distances in the actual
    state space. The authors evaluated their approach on visual Multi-Joint dynamics
    with Contact (MuJoCo) tasks where control policies must be learnt from natural
    videos with moving distractors in the background. Exactly partitioning states
    with bisimulation is generally not feasible when dealing with a continuous state
    space, therefore a pseudometric space $(\mathcal{S},d)$ is utilized, where distance
    function $d:\mathcal{S}\times\mathcal{S}\rightarrow\mathbb{R}_{\geq 0}$ measures
    the similarity between two states. DBC significantly outperforms SAC and DeepMDP [gelada2019deepmdp]
    on CARLA, an open-source simulator for autonomous driving research ²²2\urlhttps://carla.org/.
    While DBC was applied to image data, in principle it could also be applied to
    observations from ACO domains.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[zhanglearning] 提出了 *用于控制的深度双重模拟*（DBC），直接在双重模拟距离度量上进行学习。这允许学习能够有效用于下游控制策略的不可变表示，并且与任务无关的细节保持不变。编码器的训练方式使得潜在空间中的距离等于实际状态空间中的双重模拟距离。作者在视觉多关节动力学与接触（MuJoCo）任务上评估了他们的方法，在这些任务中，控制策略必须从背景中移动干扰物的视频中学习。在处理连续状态空间时，精确地划分状态通常是不切实际的，因此使用了伪度量空间
    $(\mathcal{S},d)$，其中距离函数 $d:\mathcal{S}\times\mathcal{S}\rightarrow\mathbb{R}_{\geq
    0}$ 测量两个状态之间的相似性。DBC 在CARLA上显著优于SAC和DeepMDP [gelada2019deepmdp]，CARLA是一个用于自动驾驶研究的开源模拟器²²2\urlhttps://carla.org/。虽然DBC应用于图像数据，但原则上也可以应用于ACO领域的观察数据。'
- en: 5.3 Exploration
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 探索
- en: Successful state abstraction has numerous applications, including the scaling
    of principled exploration strategies to DRL [tang2017exploration] and potential-based
    reward shaping [burden2021latent, burden2018using]. However, learning an abstract
    state space that meets all the desired criteria remains a long-standing problem.
    RL agents often gradually unlock new abilities, that in turn result in new areas
    of the environment being visited. Many initial encodings may be learned before
    an agent has sufficiently explored the state space [pmlr-v119-misra20a]. In addition,
    exploration is intractable for domains suffering from the curse of dimensionality.
    Principled exploration strategies are required that enable the sufficient visitation
    of abstracted states  [burden2021latent, pmlr-v119-misra20a, wong2022deep]. To
    address this, [pmlr-v119-misra20a] introduce HOMER, a state abstraction approach
    that accounts for the fact that the learning of a compact representation for states
    requires comprehensive information from the environment - something that cannot
    be achieved via random exploration alone.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的状态抽象有许多应用，包括将原则性探索策略扩展到深度强化学习（DRL）[tang2017exploration]和基于潜在的奖励塑造[burden2021latent,
    burden2018using]。然而，学习一个符合所有期望标准的抽象状态空间仍然是一个长期存在的问题。强化学习（RL）代理通常会逐渐解锁新的能力，进而导致环境中新的区域被访问。在代理充分探索状态空间之前，可能会学习到许多初始编码[pmlr-v119-misra20a]。此外，对于受制于维度灾难的领域，探索是不可行的。需要原则性探索策略来确保对抽象状态的充分访问[burden2021latent,
    pmlr-v119-misra20a, wong2022deep]。为了解决这个问题，[pmlr-v119-misra20a]引入了HOMER，这是一种状态抽象方法，考虑到学习状态的紧凑表示需要来自环境的全面信息——这一点仅通过随机探索是无法实现的。
- en: HOMER is designed to learn a reward-free state abstraction termed *kinematic
    inseparability*, aggregating observations that share the same forward and backward
    dynamics. The approach iteratively explores the environment by training policies
    to visit each kinematically inseparable abstract state. Policies are constructed
    using contextual bandits and a synthetic reward function that incentifies agents
    to reach an abstract state. In addition, HOMER interleaves learning the state
    abstraction and the policies for reaching the new abstract states in an inductive
    manner, meaning policies reach new states, which are abstracted, and then new
    policies are learned, iteratively, until a *policy cover* has been obtained. This
    iterative learning approach is depicted in \autoreffig:homer. Once HOMER is trained
    a near-optimal policy can be found for any reward function. HOMER outperforms
    PPO and other baselines on an environment named the *diabolical combination lock*,
    a class of rich observation MDPs where the wrong choice leads to states from which
    an optimal return is impossible.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: HOMER被设计为学习一种称为*运动学不可分离性*的无奖励状态抽象，聚合具有相同前向和后向动态的观察。该方法通过训练策略来访问每个运动学不可分离的抽象状态，迭代地探索环境。策略的构建使用了上下文博弈和一个合成奖励函数，该函数激励代理到达抽象状态。此外，HOMER以归纳的方式交替学习状态抽象和到达新抽象状态的策略，这意味着策略会到达新的状态，这些状态会被抽象出来，然后再学习新的策略，迭代进行，直到获得一个*策略覆盖*。这种迭代学习方法在\autoreffig:homer中进行了描述。一旦HOMER被训练完成，就可以为任何奖励函数找到接近最优的策略。HOMER在一个名为*恶魔组合锁*的环境中优于PPO和其他基线，这是一类丰富观察的MDP，其中错误选择会导致无法获得最优回报的状态。
- en: 'Figure 4: HOMER (Adapted from [pmlr-v119-misra20a]).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：HOMER（改编自[pmlr-v119-misra20a]）。
- en: 'There are numerous *count-based* exploration approaches with strong convergence
    guarantees for tabular RL when applied to small discrete Markov decision processes [tang2017exploration].
    [ladosz2022exploration] define three desirable criteria for exploration methods,
    including: i.) determining the degree of exploration based on the agent’s learning;
    ii.) encouraging actions that are likely to result in new outcomes, and; iii.)
    rewarding the agent for exploring environments with sparse rewards. A popular
    approach towards encouraging exploration is via intrinsic rewards, where the reward
    signal consists of extrinsic and intrinsic components. When combined with state-abstraction,
    these tried and tested methods can be applied to environments suffering from the
    curse of dimensionality.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多*基于计数*的探索方法在应用于小型离散马尔可夫决策过程时，具有强大的收敛性保证 [tang2017exploration]。[ladosz2022exploration]定义了三种探索方法的理想标准，包括：i.)
    根据智能体的学习确定探索的程度；ii.) 鼓励可能导致新结果的动作；iii.) 奖励智能体探索具有稀疏奖励的环境。鼓励探索的一种流行方法是通过内在奖励，其中奖励信号包括外在和内在两个部分。与状态抽象结合时，这些经过验证的方法可以应用于受维度灾难困扰的环境。
- en: '[tang2017exploration] introduce a count-based exploration method through static
    hashing, using SimHash. The hash codes are obtained via a trained AutoEncoder,
    and provide a means through which to keep track of the number of times semantically
    similar observation-action pairs have been encountered. A count-based reward encourages
    the visitation of less frequently explored semantically similar observation-action
    pairs. [bellemare2016unifying] proposed using Pseudo-Counts, counting salient
    events derived from the log-probability improvement according to a *sequential
    density model* over the state space. In the limit this converges to the empirical
    count. [martin2017count] focus on counts within the feature representation space
    rather than for the raw inputs. Other approaches for computing intrinsic rewards
    are based on prediction errors  [pathak2017curiosity, stadie2015incentivizing,
    savinovepisodic, burdaexploration, bougie2021fast, ladosz2022exploration] and
    memory based methods [fu2017ex2, badianever], using models trained to distinguish
    states from one another, where easy to distinguish states are considered novel [ladosz2022exploration].'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[tang2017exploration] 通过静态哈希介绍了一种基于计数的探索方法，使用SimHash。哈希码通过训练好的AutoEncoder获得，提供了一种跟踪语义相似的观察-动作对出现次数的手段。基于计数的奖励鼓励访问较少探索的语义相似的观察-动作对。[bellemare2016unifying]
    提出了使用伪计数，根据*序列密度模型*计算从日志概率改善中得出的显著事件。在极限情况下，这将收敛于经验计数。[martin2017count] 关注于特征表示空间内的计数，而不是原始输入。其他计算内在奖励的方法基于预测误差
    [pathak2017curiosity, stadie2015incentivizing, savinovepisodic, burdaexploration,
    bougie2021fast, ladosz2022exploration] 和基于记忆的方法 [fu2017ex2, badianever]，使用训练好的模型来区分状态，其中易于区分的状态被视为新颖
    [ladosz2022exploration]。'
- en: '![Refer to caption](img/f343a8af1677054cf5abd69ef39882fd.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f343a8af1677054cf5abd69ef39882fd.png)'
- en: 'Figure 5: Approaches rewarding agents for visiting novel states (Adapted from [ladosz2022exploration]).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：奖励智能体访问新状态的方法（改编自 [ladosz2022exploration]）。
- en: 'Goal based exploration represents another class of methods, which [ladosz2022exploration]
    further divide into: meta-controllers, where a controller with a high-level overview
    of the environment provides goals for a worker agent [forestier2017intrinsically,
    colas2019curious, vezhnevets2017feudal, hester2013learning, kulkarni2016hierarchical];
    sub-goals, finding a sub-goal for agents to reach, e.g., bottlenecks in the environment [machado2017laplacian,
    machadoeigenoption, fangadaptive], and; goals in the region of highest uncertainty,
    where exploring uncertain states with respect to the rewards are the sub-goals [kovac2020grimgep].'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 基于目标的探索代表了另一类方法，[ladosz2022exploration] 进一步将其划分为：元控制器，其中具有环境高级概述的控制器为工作智能体提供目标
    [forestier2017intrinsically, colas2019curious, vezhnevets2017feudal, hester2013learning,
    kulkarni2016hierarchical]；子目标，为智能体找到需要达到的子目标，例如，环境中的瓶颈 [machado2017laplacian,
    machadoeigenoption, fangadaptive]；以及在最高不确定性区域的目标，在这些目标中，探索与奖励相关的不确定状态是子目标 [kovac2020grimgep]。
- en: Despite the above advances, learning over the entirety of the environment is
    neither feasible nor desirable when dealing with increased complexity. Here principled
    methods are required that can determine which parts of the state space are most
    relevant [pmlr-v134-perdomo21a]. However, this requirement in itself leads to
    the dilemma of how one can determine with minimal effort that an area of the state
    space is irrelevant, which is an open research question.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有上述进展，在处理复杂性增加时，学习整个环境既不可行也不可取。在这种情况下，需要有原则的方法来确定状态空间中哪些部分最为相关 [pmlr-v134-perdomo21a]。然而，这一要求本身带来了一个困境，即如何以最小的努力确定状态空间的某个区域是否无关，这是一个开放的研究问题。
- en: 5.4 Knowledge Retention
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 知识保留
- en: 'As with many online learning tasks, DRL agents are prone to *catastrophic forgetting*:
    the unlearning of previously acquired knowledge [atkinson2021pseudo, schak2019study].
    In order to be sample efficient, DRL approaches often resort to experience replay
    memories, which store experience transition tuples that are sampled during training [foerster2017stabilising].
    Samples are either stored long term, as in off-policy approaches such as DQN and
    DDPG, or short term, e.g., samples gathered using multiple workers for PPO. For
    the former, paying attention to the experience replay memory composition can mitigate
    catastrophic forgetting [de2015importance]. However, in practice, a large number
    of transitions are discarded, since there is a memory cost associated with storage.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多在线学习任务一样，DRL（深度强化学习）代理容易出现*灾难性遗忘*：即遗忘先前获得的知识 [atkinson2021pseudo, schak2019study]。为了提高样本效率，DRL
    方法通常采用经验重放记忆，这些记忆存储在训练过程中采样的经验转移元组 [foerster2017stabilising]。样本可以长期存储，如在 DQN 和
    DDPG 等离策略方法中，或短期存储，例如，通过多个工作者收集的 PPO 样本。对于前者，关注经验重放记忆的组成可以缓解灾难性遗忘 [de2015importance]。然而，在实践中，由于存储相关的内存成本，大量的转移数据会被丢弃。
- en: An additional challenge is that the stationarity of the environment, and one’s
    opponent(s), are a strong assumption. Samples stored inside a replay buffer can
    become deprecated, confronting learners with the same challenges seen in data
    streaming [hernandez2018multiagent]. Here DRL agents require continual learning,
    the ability to continuously learn and build on previously acquired knowledge [wolczyk2021continual].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是环境的静态性以及对手的静态性，这些都是强假设。存储在重放缓冲区中的样本可能会过时，使学习者面临与数据流处理相同的挑战 [hernandez2018multiagent]。在这种情况下，DRL
    代理需要持续学习，即能够不断学习并建立在先前获得的知识上 [wolczyk2021continual]。
- en: One approach to solve this problem is to utilize a *dual memory* where a freshly
    initialized DRL agent, a short-term agent (network), is trained on a new task,
    upon which knowledge is transferred to a DQN designed to retain long-term knowledge
    from previous tasks. A generative network is used to generate short sequences
    from previous tasks for the DQN to train on, in order to prevent catastrophic
    forgetting as the new task is learned [atkinson2021pseudo]. However, this approach
    relies on a stationary environment, as an additional mechanism would be required
    to determine the relevance of past knowledge, given drift in the state transition
    probabilities.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是利用*双重记忆*，即一个新初始化的 DRL 代理（短期代理（网络））在新任务上进行训练，然后将知识转移到设计用于保留先前任务长期知识的
    DQN 上。使用生成网络从先前任务生成短序列供 DQN 训练，以防止在学习新任务时发生灾难性遗忘 [atkinson2021pseudo]。然而，这种方法依赖于静态环境，因为需要额外的机制来确定过去知识的相关性，考虑到状态转移概率的漂移。
- en: '*Elastic Weight Consolidation* (EWC) is another popular approach for mitigating
    catastrophic forgetting for DNNs [kirkpatrick2017overcoming, huszar2018note, huszar2017quadratic].
    EWC has been applied to DRL using an additional loss term using the Fisher information
    matrix for the difference between the old and new parameters, and a hyperparameter
    $\lambda$ which can be used to specify how important older weights are [ribeiro2019multi,
    nguyen2017system, kessler2022same, wolczyk2021continual].'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*弹性权重固化*（EWC）是另一种用于减轻 DNN（深度神经网络）灾难性遗忘的热门方法 [kirkpatrick2017overcoming, huszar2018note,
    huszar2017quadratic]。EWC 已通过使用额外的损失项，即使用 Fisher 信息矩阵来衡量旧参数与新参数之间的差异，并通过超参数 $\lambda$
    指定旧权重的重要性 [ribeiro2019multi, nguyen2017system, kessler2022same, wolczyk2021continual]。'
- en: 6 Approaches for combinatorial action spaces
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 组合动作空间的方法
- en: Due to an explosion in the number of state-action pairs, traditional DRL approaches
    do not scale to high-dimensional combinatorial action spaces. Scalable methods
    will need to meet the following criteria.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于状态-动作对的数量激增，传统的深度强化学习（DRL）方法无法扩展到高维组合动作空间。可扩展的方法需要满足以下标准。
- en: 'Generalizability: For our target domains a sufficient visitation of all state-action
    pairs to obtain accurate value estimates is intractable. Formulations are required
    that allow for generalization over the action space [dulac2015deep].'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 可泛化性：对于我们的目标领域，所有状态-动作对的充分访问以获得准确的价值估计是不可行的。需要允许对动作空间进行泛化的公式[dulac2015deep]。
- en: Time-Varying Actions (TVA) can result in a policy being trained on a subset
    of actions $\mathcal{A}^{\prime}\subset\mathcal{A}$. This has implications when
    the agent is later asked to choose from a larger set of actions [9507301] or applying
    actions to previously unseen objects [chandak2020lifelong, fang2020learning, 9507301],
    e.g., a new host on a network for ACO.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 时间变化动作（TVA）可能导致策略仅在动作的子集$\mathcal{A}^{\prime}\subset\mathcal{A}$上进行训练。这在代理被要求从更大的动作集合中选择时有影响[9507301]，或者将动作应用于之前未见过的对象[chandak2020lifelong,
    fang2020learning, 9507301]，例如网络中的新主机用于ACO。
- en: 'Computational Complexity: An efficient formulation is to use a DNN with $\rvert\mathcal{A}\rvert$
    output nodes, requiring a single forward pass to compute an output for each action.
    However, this approach will not generalize well. Alternatively, for a value function
    with *a single output*, one could input observation-action pairs and estimate
    the utility of an arbitrary number of actions: $Q:\mathcal{O}\times\mathcal{A}\rightarrow\mathbb{R}$.
    However, this approach is intractable due to the computational cost growing linearly
    with $\rvert\mathcal{A}\rvert$. Instead, methods with sub-linear complexity are
    required.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 计算复杂性：一种有效的公式是使用具有$\rvert\mathcal{A}\rvert$输出节点的深度神经网络（DNN），每个动作需要单次前向传递来计算输出。然而，这种方法的泛化效果不好。作为替代，对于具有*单一输出*的价值函数，可以输入观察-动作对，并估计任意数量动作的效用：$Q:\mathcal{O}\times\mathcal{A}\rightarrow\mathbb{R}$。然而，由于计算成本线性增长，使用此方法是不切实际的。因此，需要具有子线性复杂度的方法。
- en: 'The criteria listed above provide the axes along which the suitability of approaches
    for our target domains can be measured. Through reviewing the literature on high-dimensional
    action spaces we were able to identify five categories that conveniently cluster
    the approaches: i.) *proto action* based approaches; ii.) *action decomposition*;
    iii.) *action elimination*; iv.) *hierarchical* approaches, and; v.) *curriculum
    learning*. \autoreffig:high_level_action_approaches_categories provides an illustrative
    example and short description for each category. In \autoreftab:high_dim_action_approaches
    in \autorefappendix:action_approaches we provide an overview of the literature
    and a short contributions summary.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述标准提供了评估方法对我们目标领域适用性的轴线。通过回顾高维动作空间的文献，我们能够识别出五种类别，这些类别方便地聚集了这些方法：i.) *原型动作*
    基于的方法；ii.) *动作分解*；iii.) *动作消除*；iv.) *层次化* 方法；v.) *课程学习*。\autoreffig:high_level_action_approaches_categories
    提供了每个类别的示例和简短描述。在 \autoreftab:high_dim_action_approaches 和 \autorefappendix:action_approaches
    中，我们提供了文献概述和简短的贡献总结。
- en: '![Refer to caption](img/f1d9e71cb4b4beee60dbe3444a6ca027.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f1d9e71cb4b4beee60dbe3444a6ca027.png)'
- en: (a) Proto Actions
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原型动作
- en: '![Refer to caption](img/414fef6f2db3c433ae0f69ea055e0d21.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/414fef6f2db3c433ae0f69ea055e0d21.png)'
- en: (b) Action Decomp.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 动作分解
- en: '![Refer to caption](img/c4c0d679b3c04289aa5c00e543e0a0d8.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c4c0d679b3c04289aa5c00e543e0a0d8.png)'
- en: (c) Action Elimination
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 动作消除
- en: '![Refer to caption](img/e4a97e078506f29a85659f0c00c52df2.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e4a97e078506f29a85659f0c00c52df2.png)'
- en: (d) Hierarchical
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 层次化
- en: '![Refer to caption](img/e22a6b44c63eafa970b4442eb89651af.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e22a6b44c63eafa970b4442eb89651af.png)'
- en: (e) Curriculum
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 课程
- en: 'Figure 6: Categories of DRL approaches for high-dimensional action spaces.
    \autoreffig:actions:proto: Proto actions leverage prior domain knowledge and embed
    actions into a continuous space, before applying $k$-NN to pick the closest discrete
    actions, which are passed to a critic. \autoreffig:actions:composition: Action
    Decomposition reformulates the single agent problem as a Dec-POMDP with a composite
    action space. \autoreffig:actions:elimination: Action elimination approaches use
    a module that determines which actions are redundant for a given observation.
    \autoreffig:actions:hierarchcial: Hierarchical, the action selected by a policy
    influences the action selected by a sub-policy. \autoreffig:actions:curriculum:
    Curriculum learning approaches for gradually increasing the number of available
    actions.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 高维动作空间的 DRL 方法类别。 \autoreffig:actions:proto: Proto 动作利用先验领域知识并将动作嵌入到连续空间中，然后应用
    $k$-NN 选择最接近的离散动作，这些动作会传递给评论员。 \autoreffig:actions:composition: 动作分解将单一智能体问题重新表述为具有复合动作空间的
    Dec-POMDP。 \autoreffig:actions:elimination: 动作消除方法使用一个模块来确定给定观察下哪些动作是多余的。 \autoreffig:actions:hierarchcial:
    层次化，策略选择的动作影响子策略选择的动作。 \autoreffig:actions:curriculum: 逐步增加可用动作数量的课程学习方法。'
- en: 6.1 Proto Action Approaches
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 Proto 动作方法
- en: '[dulac2015deep] proposed the first DRL approach to address the high dimensional
    action space problem, the Wolpertinger architecture, which embeds discrete actions
    into a continuous space $\mathbb{R}^{n}$. A continuous control policy $f_{\pi}:\mathcal{O}\rightarrow\mathbb{R}^{n}$,
    is trained to output a *proto* action. Given that a proto action $\hat{a}$ is
    unlikely to be a valid action ($\hat{a}\notin\mathcal{A}$), $k$-nearest-neighbours
    ($k$-NN) is used to map the proto action to the $k$ closest valid actions: $g_{k}(\hat{a})=\operatorname*{argmin}_{a\in\mathcal{A}}^{k}\rvert
    a-\hat{a}\rvert_{2}$. To avoid picking outlier actions, and to refine the action
    selection, the selected actions are passed to a critic $Q$, which then selects
    the $\operatorname*{argmax}$ (see Algorithm \autorefalg:WolpertingerPolicy).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[dulac2015deep] 提出了第一个 DRL 方法来解决高维动作空间问题，即 Wolpertinger 架构，该架构将离散动作嵌入到连续空间
    $\mathbb{R}^{n}$ 中。训练一个连续控制策略 $f_{\pi}:\mathcal{O}\rightarrow\mathbb{R}^{n}$，以输出
    *proto* 动作。由于 proto 动作 $\hat{a}$ 不太可能是有效动作（$\hat{a}\notin\mathcal{A}$），使用 $k$-最近邻（$k$-NN）将
    proto 动作映射到 $k$ 个最接近的有效动作：$g_{k}(\hat{a})=\operatorname*{argmin}_{a\in\mathcal{A}}^{k}\rvert
    a-\hat{a}\rvert_{2}$。为了避免选择离群动作，并改进动作选择，所选动作会传递给一个评论员 $Q$，然后由评论员选择 $\operatorname*{argmax}$（见算法
    \autorefalg:WolpertingerPolicy）。'
- en: Algorithm 1 Wolpertinger Policy [dulac2015deep]
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 Wolpertinger 策略 [dulac2015deep]
- en: 1:Receive an observation $o$ from the environment.2:$\hat{a}=f_{\pi}(o)$ $\triangleright$
    Obtain proto-action from the actor.3:$\mathcal{A}_{k}=g_{k}(\hat{a})$ $\triangleright$
    Get $k$ nearest neighbours.4:$a=\operatorname*{argmax}_{a_{j}\in\mathcal{A}_{k}}Q(o,a_{j})$
    $\triangleright$ Action refinement step.5:Apply $a$ to environment and receive
    $r$, $o^{\prime}$.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 从环境中接收观察 $o$。2: $\hat{a}=f_{\pi}(o)$ $\triangleright$ 从演员处获取 proto 动作。3:
    $\mathcal{A}_{k}=g_{k}(\hat{a})$ $\triangleright$ 获取 $k$ 个最近邻。4: $a=\operatorname*{argmax}_{a_{j}\in\mathcal{A}_{k}}Q(o,a_{j})$
    $\triangleright$ 动作改进步骤。5: 将 $a$ 应用到环境中，并接收 $r$ 和 $o^{\prime}$。'
- en: The Wolpertinger architecture meets many of the above requirements. While the
    time-complexity scales linearly with the number of actions $k$, the authors show
    both theoretically and in practice that there is a point at which increasing $k$
    delivers a marginal performance increase at best. Using 5-10% of the maximal number
    of actions was found to be sufficient, allowing agents to generalize over the
    set of actions with sub-linear complexity. Here, the action embedding space does
    require a logical ordering of the actions along each axis. Currently prior information
    about the action space is leveraged to construct the embedding space. However,
    [dulac2015deep] note that learning action representations during training could
    also provide a solution. The approach has also been criticised for instability
    during training due to the $k$-NN component preventing the gradients from propagating
    back to boost the training of the actor network [tran2022cascaded].
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Wolpertinger 架构满足了上述许多要求。尽管时间复杂度与动作数量 $k$ 线性相关，但作者们理论上和实践中都表明，增加 $k$ 至多只能带来边际性能提升。使用最大动作数的
    5-10% 被发现是足够的，这使得代理能够以亚线性复杂度对动作集合进行泛化。在这里，动作嵌入空间确实需要沿每个轴的逻辑排序。目前，利用了关于动作空间的先前信息来构建嵌入空间。然而，[dulac2015deep]
    指出，训练期间学习动作表示也可能提供解决方案。该方法还因 $k$-NN 组件阻止梯度传播回到演员网络的训练中而被批评为训练不稳定 [tran2022cascaded]。
- en: 'Wolpertinger has been applied to: caching on edge devices to reduce data traffic
    in next generation wireless networks  [zhong2018deep], voltage control for shunt
    compensations to enhance voltage stability [cao2020optimal], maintenance and rehabilitation
    optimization for multi-lane highway asphalt pavement [9750983], recommender systems
    (Slate-MDPs) [sunehag2015deep, 10.1145/3240323.3240374, DBLP:journals/corr/abs-1801-00209],
    and, penetration testing for ACO [nguyen2020multiple]. For the latter, [nguyen2020multiple]
    evaluate the ability of Wolpertinger to learn a policy that launches attacks on
    vulnerable services on a network. Wolpertinger is applied using an embedding space
    that consists of three levels (illustrated in \autoreffig:NASim_attack_embedding):
    i.) the action characteristics (*scan subnet*, *scan host* and *exploit services*);
    ii.) the subnet to target, and; iii.) services that are vulnerable towards attacks.
    The second dimension focuses on the destination of the action with respect to
    the subnet, e.g., selecting \sayscan subnet along axis $1$ and selecting the subnet
    on axis $2$. *Node2Vec* is used for expressing the network structure, and the
    authors also train a network to produce similar embeddings for correlated service
    vulnerabilities. Wolpertinger was shown to outperform DQN on the Network Attack
    Simulator environment [schwartz2019autonomous].'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Wolpertinger 已应用于：边缘设备的缓存以减少下一代无线网络中的数据流量 [zhong2018deep]，分流补偿的电压控制以增强电压稳定性
    [cao2020optimal]，多车道高速公路沥青路面的维护和修复优化 [9750983]，推荐系统（Slate-MDPs） [sunehag2015deep,
    10.1145/3240323.3240374, DBLP:journals/corr/abs-1801-00209]，以及 ACO 的渗透测试 [nguyen2020multiple]。对于后者，[nguyen2020multiple]
    评估了 Wolpertinger 学习对网络上易受攻击的服务发起攻击的策略的能力。Wolpertinger 采用了由三层组成的嵌入空间（如 \autoreffig:NASim_attack_embedding
    所示）：i. 动作特征（*扫描子网*，*扫描主机* 和 *利用服务*）；ii. 目标子网；iii. 易受攻击的服务。第二维度关注于与子网相关的动作目标，例如，选择
    \sayscan subnet 沿轴 $1$ 和选择轴 $2$ 上的子网。*Node2Vec* 用于表达网络结构，作者们还训练了一个网络，以产生类似的嵌入用于相关服务漏洞。Wolpertinger
    被证明在网络攻击模拟器环境 [schwartz2019autonomous] 上优于 DQN。
- en: '![Refer to caption](img/a6373a7e18cd7ce538026332fe8b69cd.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a6373a7e18cd7ce538026332fe8b69cd.png)'
- en: 'Figure 7: Wolpertinger attack embedding used by [nguyen2020multiple] on NASim [schwartz2019autonomous].'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: [nguyen2020multiple] 在 NASim 上使用的 Wolpertinger 攻击嵌入 [schwartz2019autonomous]。'
- en: 6.2 Action Decomposition Approaches
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 动作分解方法
- en: A popular approach towards scaling DRL to large combinatorial action spaces
    is to apply multi-agent deep reinforcement learning (MADRL). The action space
    is decomposed into actions provided by multiple agents, e.g., having each agent
    control an action dimension [tavakoli2018action], or via an algebraic formulation
    for combining the actions [tran2022cascaded]. However, learning an optimal policy
    requires the underlying agents to converge upon an *optimal joint-policy*. Therefore,
    approaches must be viewed through the lens of MADRL within a Dec-POMDP, using
    equilibrium concepts from multi-agent learning.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度强化学习（DRL）扩展到大型组合动作空间的一个常用方法是应用多智能体深度强化学习（MADRL）。动作空间被分解为由多个智能体提供的动作，例如，让每个智能体控制一个动作维度 [tavakoli2018action]，或通过代数形式来组合动作 [tran2022cascaded]。然而，学习一个*最优联合策略*需要基础智能体在此策略上达成一致。因此，方法必须从多智能体学习的均衡概念出发，通过
    MADRL 视角来分析 Dec-POMDP。
- en: 'Formally, given an expected gain, ${\mathcal{G}}_{i}(\bm{\pi})=\mathbb{E}_{\bm{\pi}}\{\sum_{k=0}^{\infty}\gamma^{k}r_{i,t+k+1}\rvert
    x_{t}=x\}$, the underlying policies must find a Pareto optimal solution, i.e.,
    a joint policy $\bm{\hat{\pi}}$ from which no agent $i$ can deviate without making
    at least one other agent worse off [matignon2012independent]:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，给定期望增益，${\mathcal{G}}_{i}(\bm{\pi})=\mathbb{E}_{\bm{\pi}}\{\sum_{k=0}^{\infty}\gamma^{k}r_{i,t+k+1}\rvert
    x_{t}=x\}$，基础策略必须找到一个帕累托最优解，即一个联合策略 $\bm{\hat{\pi}}$，使得没有智能体 $i$ 可以偏离而不使至少一个其他智能体变得更差 [matignon2012independent]。
- en: Definition 6.1  (Pareto Optimality).
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 6.1  (帕累托最优性)。
- en: 'A joint-strategy $\bm{\pi}$ is Pareto-dominated by $\bm{\hat{\pi}}$ *if and
    only if* (*iff*):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 联合策略 $\bm{\pi}$ *当且仅当*（*iff*）被 $\bm{\hat{\pi}}$ 帕累托支配：
- en: '|  | $\forall i,\forall s\in\mathcal{S},\mathcal{G}_{i,\bm{\hat{\pi}}}(s)\geq\mathcal{G}_{i,\bm{\pi}}(s)\
    and\ \exists j,\exists s\in\mathcal{S},\mathcal{G}_{j,\bm{\hat{\pi}}}(s)>\mathcal{G}_{j,\bm{\pi}}(s).$
    |  | (3) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\forall i,\forall s\in\mathcal{S},\mathcal{G}_{i,\bm{\hat{\pi}}}(s)\geq\mathcal{G}_{i,\bm{\pi}}(s)\
    and\ \exists j,\exists s\in\mathcal{S},\mathcal{G}_{j,\bm{\hat{\pi}}}(s)>\mathcal{G}_{j,\bm{\pi}}(s).$
    |  | (3) |'
- en: A joint policy $\bm{\hat{\pi}}^{*}$ is Pareto optimal if it is not Pareto-dominated
    by any other $\bm{\pi}$.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个联合策略 $\bm{\hat{\pi}}^{*}$ 不被任何其他策略 $\bm{\pi}$ 帕累托支配，则它是帕累托最优的。
- en: 'There are three categories of training schemes for cooperative MA(D)RL (illustrated
    in \autoreffig:marl_approaches_overview): independent learners (ILs), who treat
    each other as part of the environment; the centralized controller approach, which
    does not scale with the number of agents; and centralized training for decentralized
    execution (CTDE).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 合作 MA(D)RL 的训练方案分为三类（如 \autoreffig:marl_approaches_overview 所示）：独立学习者（ILs），他们将彼此视为环境的一部分；集中控制器方法，这种方法不随智能体数量的增加而扩展；以及集中训练用于分散执行（CTDE）。
- en: '![Refer to caption](img/c2c64afea7152c8a4399c8dbbca17049.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c2c64afea7152c8a4399c8dbbca17049.png)'
- en: (a) Independent Learners
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 独立学习者
- en: '![Refer to caption](img/1c46e9fd8bf129cdbabfc6a1fbf29700.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1c46e9fd8bf129cdbabfc6a1fbf29700.png)'
- en: (b) Centralized Training for Decenctralized Execution
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 集中训练用于分散执行
- en: '![Refer to caption](img/573cd6329b6b7c916e999c0c56ae7c10.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/573cd6329b6b7c916e999c0c56ae7c10.png)'
- en: (c) Centralized Controller
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 集中控制器
- en: 'Figure 8: An overview of multi-agent reinforcement learning training schemes.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：多智能体强化学习训练方案概述。
- en: 'Even within stateless two player matrix games with a small number of actions
    per agents, ILs fail to consistently converge upon Pareto optimal solutions [nowe2012game,
    claus1998dynamics, matignon2012independent, kapetanakis2002reinforcement, matignon2007hysteretic,
    busoniu2008comprehensive, panait2006lenient, panait2008theoretical]. However,
    ILs are frequently used as a baseline for action decomposition approaches [tavakoli2018action].
    Therefore, to better understand the challenges that confront action decomposition
    approaches we shall first briefly consider the multi-agent learning pathologies
    that learners must overcome to converge upon a Pareto optimal joint-policy from
    the perspective of ILs ³³3For a detailed recap please read [JMLR:v17:15-417, lauer2000algorithm,
    kapetanakis2002reinforcement, palmer2020independent].:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在每个智能体有少量动作的无状态双人矩阵游戏中，ILs 也未能一致地收敛到帕累托最优解 [nowe2012game, claus1998dynamics,
    matignon2012independent, kapetanakis2002reinforcement, matignon2007hysteretic,
    busoniu2008comprehensive, panait2006lenient, panait2008theoretical]。然而，ILs 经常被用作动作分解方法的基线 [tavakoli2018action]。因此，为了更好地理解面对动作分解方法的挑战，我们将首先简要考虑学习者必须克服的多智能体学习病态，以从
    ILs 的角度收敛到帕累托最优的联合策略³³3有关详细回顾，请参阅 [JMLR:v17:15-417, lauer2000algorithm, kapetanakis2002reinforcement,
    palmer2020independent]。
- en: 'Miscoordination occurs when there are two or more incompatible Pareto-optimal
    equilibria [claus1998dynamics, kapetanakis2002reinforcement, matignon2012independent].
    One agent choosing an action from an incompatible equilibria is sufficient to
    lower the gain. Formally: two equilibria $\bm{\pi}$ and $\bm{\hat{\pi}}$ are incompatible
    *iff* the gain received for pairing at least one agent using a policy $\pi$ with
    other agents using a policy $\hat{\pi}$ results in a lower gain compared to when
    all agents are using $\pi$: $\exists i,\bm{\pi}_{i}\neq\bm{\hat{\pi}}_{i},\mathcal{G}_{i,\langle\bm{\hat{\pi}}_{i},\bm{\pi}_{-i}\rangle}<\mathcal{G}_{i,\bm{\pi}}$.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 误协调发生在存在两个或更多不兼容的帕累托最优均衡时 [claus1998dynamics, kapetanakis2002reinforcement,
    matignon2012independent]。一个代理从不兼容的均衡中选择行动就足以降低收益。形式上：两个均衡 $\bm{\pi}$ 和 $\bm{\hat{\pi}}$
    是不兼容的 *当且仅当* 当至少一个代理使用策略 $\pi$ 与其他代理使用策略 $\hat{\pi}$ 配对时，获得的收益低于所有代理都使用 $\pi$
    的情况：$\exists i,\bm{\pi}_{i}\neq\bm{\hat{\pi}}_{i},\mathcal{G}_{i,\langle\bm{\hat{\pi}}_{i},\bm{\pi}_{-i}\rangle}<\mathcal{G}_{i,\bm{\pi}}$。
- en: 'Relative Overgeneralization: ILs are prone to being drawn to sub-optimal but
    wide peaks in the reward space, as there is a greater likelihood of achieving
    collaboration there [panait2006lenience]. Within these areas a sub-optimal policy
    yields a higher payoff on average when each selected action is paired with an
    arbitrary action chosen by the other agent [panait2006lenience, wiegand2003analysis,
    palmer2020independent].'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 相对过度泛化：ILs容易被吸引到奖励空间中的次优但宽广的峰值，因为在这些地方实现协作的可能性更大 [panait2006lenience]。在这些区域内，当每个选择的行动与其他代理选择的任意行动配对时，次优策略通常会带来更高的回报
    [panait2006lenience, wiegand2003analysis, palmer2020independent]。
- en: 'Stochasticity of Rewards and Transitions: Rewards and transitions can be stochastic,
    which has implications for approaches that use optimistic learning to overcome
    the relative overgeneralization pathology  [palmer2020independent, palmer2018negative,
    palmer2018lenient].'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励和转移的随机性：奖励和转移可能是随机的，这对采用乐观学习方法以克服相对过度泛化病理的方法具有影响 [palmer2020independent, palmer2018negative,
    palmer2018lenient]。
- en: 'The Alter-Exploration Problem: In MA(D)RL increasing the number of agents also
    increases *global exploration*, the probability of at least one of $n$ agents
    exploring: $1-(1-\epsilon)^{n}$. Here, each agent explores according to a probability
    $\epsilon$ [matignon2012independent].'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 改变探索问题：在MA(D)RL中，增加代理的数量也会增加 *全局探索* 的可能性，即至少一个 $n$ 个代理进行探索的概率：$1-(1-\epsilon)^{n}$。在这里，每个代理根据概率
    $\epsilon$ 进行探索 [matignon2012independent]。
- en: The Moving Target Problem is a result of agents updating their policies in parallel [bowling2002multiagent,
    sutton1998introduction, tuyls2012multiagent, tuyls2007evolutionary]. This pathology
    is amplified when using experience replay memories $\mathcal{D}_{i}$, due to transitions
    becoming deprecated [foerster2017stabilising, omidshafiei2017deep, palmer2018lenient].
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 移动目标问题是由于代理并行更新其策略 [bowling2002multiagent, sutton1998introduction, tuyls2012multiagent,
    tuyls2007evolutionary]。当使用经验重放记忆 $\mathcal{D}_{i}$ 时，这种病理被放大，因为转移变得过时 [foerster2017stabilising,
    omidshafiei2017deep, palmer2018lenient]。
- en: 'Deception: Deception occurs when utility values are calculated using rewards
    backed up from follow-on states from which pathologies such as miscoordination
    and relative overgeneralization can also be back-propagated [JMLR:v17:15-417].
    States with high local rewards can also represent a problem, drawing ILs away
    from optimal state-transition trajectories [JMLR:v17:15-417].'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 欺骗：当效用值通过从后续状态中回溯的奖励来计算时，会发生欺骗，此时病理如误协调和相对过度泛化也可能被回溯 [JMLR:v17:15-417]。高局部奖励的状态也可能代表一个问题，使得ILs偏离最优的状态转移轨迹
    [JMLR:v17:15-417]。
- en: Non-trivial approaches are required in order to consistently converge upon a
    Pareto optimal solution [JMLR:v17:15-417, lauer2000algorithm, kapetanakis2002reinforcement,
    palmer2020independent]. We shall now consider the different types of action decomposition
    approaches that can be found in the literature.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了始终如一地收敛到帕累托最优解，需要采用非平凡的方法 [JMLR:v17:15-417, lauer2000algorithm, kapetanakis2002reinforcement,
    palmer2020independent]。我们现在将考虑文献中存在的不同类型的行动分解方法。
- en: 6.2.1 Branching Dueling Q-Network
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 分支对抗Q网络
- en: Designed for environments where the action-space can be split into smaller action-spaces
    Branching Dueling Q-Network (BDQ) [tavakoli2018action] is a branching version
    of Dueling DDQN [wang2016dueling] ⁴⁴4 Dueling DDQNs consist of two separate estimators
    for the value and state-dependent action advantage function.. Each branch of the
    network is responsible for proposing a discrete action for an actuated joint.
    The approach features a *shared decision module*, allowing the agents to learn
    a common latent representation that is subsequently fed into each of the $n$ DNN
    branches, and can therefore be considered a CTDE approach. A conceptional illustration
    of the approach can be found in \autoreffig:BDQ.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 设计用于将行动空间拆分为更小行动空间的分支对抗Q网络（BDQ） [tavakoli2018action] 是对抗DDQN的分支版本 [wang2016dueling] ⁴⁴4
    对抗DDQN包含两个独立的估计器，用于价值和状态依赖的行动优势函数。网络的每个分支负责为一个执行关节提出一个离散动作。该方法具有*共享决策模块*，允许代理学习一个共同的潜在表示，然后将其输入到每个$n$
    DNN分支中，因此可以被认为是一种CTDE方法。该方法的概念性示意图可以在\autoreffig:BDQ中找到。
- en: BDQ has a linear increase of network outputs with regard to number of degrees
    of freedom, thereby allowing a level of independence for each individual action
    dimension. It does not suffer from the combinatorial growth of standard vanilla
    discrete action algorithms. However, the approach is designed for discretized
    continuous control domains. Therefore, BDQ’s scalability to the MultiDiscrete
    action spaces from ACO requires further investigation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: BDQ的网络输出随着自由度的增加而线性增长，从而为每个单独的动作维度提供一定的独立性。它不会遭受标准离散动作算法的组合增长问题。然而，该方法是为离散化的连续控制领域设计的。因此，BDQ对ACO的MultiDiscrete行动空间的可扩展性需要进一步研究。
- en: '![Refer to caption](img/d1706ab52f3804ce60c174f3d6d4bec5.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d1706ab52f3804ce60c174f3d6d4bec5.png)'
- en: 'Figure 9: An illustration Branching Dueling Q-Networks (adapted from [tavakoli2018action]).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：分支对抗Q网络的示意图（改编自 [tavakoli2018action]）。
- en: While BDQ achieves sub-linear complexity, the formulation is vulnerable towards
    the MA(D)RL pathologies outlined above. To evaluate the benefit of the shared
    decision module, BDQ is evaluated against Dueling-DQN, DDPG, and *independent
    Dueling DDQNs* (IDQ). The only mentioned distinction between BDQ and IDQ is that
    the first two layers were not shared among IDQ agents [tavakoli2018action]. BDQ
    uses a modified Prioritized Experience Replay memory [schaul2015prioritized],
    where transitions are prioritized based on the *aggregated distributed TD error*.
    In essence, a prioritized version of Concurrent Experience Replay Trajectories
    (CERTS) are being utilized, a method from the MADRL literature that has previously
    been shown to facilitate coordination [omidshafiei2017deep].
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管BDQ实现了次线性复杂度，但该公式易受到上述MA(D)RL病态的影响。为了评估共享决策模块的好处，BDQ与对抗-DQN、DDPG和*独立对抗DDQN*（IDQ）进行了比较。BDQ与IDQ之间唯一提到的区别是前两层在IDQ代理之间没有共享 [tavakoli2018action]。BDQ使用了修改版的优先经验回放内存 [schaul2015prioritized]，其中过渡根据*聚合的分布式TD误差*进行优先处理。本质上，使用了一种优先版本的并发经验回放轨迹（CERTS），这是一种以前已被证明可以促进协调的MADRL文献中的方法 [omidshafiei2017deep]。
- en: 'BDQ has been evaluated on numerous discretized MuJoCo domains [tavakoli2018action].
    The evaluation focused on two axes: granularity and degrees of freedom. BDQ’s
    benefits over Dueling-DQNs become noticeable as the number of degrees of freedom
    are increased. In addition, BDQ was able to solve granular, high degree of freedom
    domains for which Dueling-DDQNs was not applicable. On the majority of the domains
    DDPG still outperformed BDQ, with the exception of Humanoid-v1. Unfortunately
    a comparison of BDQ against the Wolpertinger architecture was not provided. For
    ACO we note that BDQ will probably not scale well if one of the branches is very
    large, e.g., has lot of nodes.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: BDQ在许多离散化的MuJoCo领域进行了评估 [tavakoli2018action]。评估集中在两个轴心：粒度和自由度。随着自由度的增加，BDQ相对于对抗-DQN的优势变得明显。此外，BDQ能够解决对抗-DDQN不适用的细粒度、高自由度领域。在大多数领域中，DDPG仍然优于BDQ，除了Humanoid-v1。遗憾的是，没有提供BDQ与Wolpertinger架构的比较。对于ACO，我们注意到如果其中一个分支非常大，例如，有很多节点，BDQ可能无法很好地扩展。
- en: 6.2.2 Cascading Reinforcement Learning Agents
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 级联强化学习代理
- en: 'For *cascading reinforcement learning agents* (CRLA) the action space $\mathcal{A}$
    is decomposed into smaller sets of actions $\ \mathcal{U}^{1},\ \mathcal{U}^{2},...,\
    \mathcal{U}^{L}$ [tran2022cascaded]. For each subset of actions $\mathcal{A}^{i}$,
    the size of the dimensionality is significantly smaller than that of $\mathcal{A}$,
    i.e., $\rvert\ \mathcal{U}^{i}\rvert\ll\ \rvert\mathcal{A}\rvert$ $(\forall i\in[1,L])$.
    In this formulation a primitive action $a_{t}$ at time step $t$ is given by a
    function over actions $u_{t}^{i}$ obtained from each respective subset $\ \mathcal{U}^{i}$:
    $u_{t}=f(u_{t}^{1},u_{t}^{2},...,u_{t}^{L})$. The action components $u_{t}^{i}$
    are chained together to algebraically build an integer identifier. This provides
    a formulation through which larger identifiers can be obtained using the smaller
    integer values provided by each action subset. A CTDE approach is used to facilitate
    the training of $n$ agents, where the joint action space $\bm{\mathcal{A}}$ is
    comprised of $\ \mathcal{U}^{1},\ \mathcal{U}^{2},...,\ \mathcal{U}^{n}$, with
    $\ \mathcal{U}^{i}$ representing the action space of an agent $i$. The concept
    is illustrated in \autoreffig:cascading.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *级联强化学习代理*（CRLA），动作空间 $\mathcal{A}$ 被分解为更小的动作集合 $\ \mathcal{U}^{1},\ \mathcal{U}^{2},...,\
    \mathcal{U}^{L}$ [tran2022cascaded]。对于每个动作子集 $\mathcal{A}^{i}$，维度大小显著小于 $\mathcal{A}$，即
    $\rvert\ \mathcal{U}^{i}\rvert\ll\ \rvert\mathcal{A}\rvert$ $(\forall i\in[1,L])。在这个公式中，时间步骤
    $t$ 的原始动作 $a_{t}$ 由动作 $u_{t}^{i}$ 的函数给出，这些动作是从各自的子集 $\ \mathcal{U}^{i}$ 中获得的：$u_{t}=f(u_{t}^{1},u_{t}^{2},...,u_{t}^{L})$。动作组件
    $u_{t}^{i}$ 被链式连接以代数方式构建整数标识符。这提供了一种通过使用每个动作子集提供的较小整数值来获得较大标识符的公式。使用 CTDE 方法来促进
    $n$ 个代理的训练，其中联合动作空间 $\bm{\mathcal{A}}$ 包含 $\ \mathcal{U}^{1},\ \mathcal{U}^{2},...,\
    \mathcal{U}^{n}$，其中 $\ \mathcal{U}^{i}$ 代表代理 $i$ 的动作空间。该概念在 \autoreffig:cascading
    中进行了说明。
- en: '![Refer to caption](img/58cdf0c3e1ec93dbbb6bf070f0372ed9.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/58cdf0c3e1ec93dbbb6bf070f0372ed9.png)'
- en: 'Figure 10: Action space composition. Action $u_{t}$ at time step $t$ is algebraically
    constructed using actions $a^{i}_{t}$ obtained from action subsets $\mathcal{A}^{i}$
    (adapted from  [tran2022cascaded]).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：动作空间组成。时间步骤 $t$ 的动作 $u_{t}$ 使用从动作子集 $\mathcal{A}^{i}$ 中获得的动作 $a^{i}_{t}$
    通过代数方法构造（改编自 [tran2022cascaded]）。
- en: 'CRLA yields a solution that allows for large combinatorial action spaces to
    be decomposed into a branching tree structure. Each node in the tree represents
    a decision by an agent regarding which child node to select next. Each node has
    its own identifier, and all nodes in the tree have the same branching factor,
    with a heuristic being used to determine the number of tree levels: $\rvert T\rvert=\log_{b}(\rvert\mathcal{A}\rvert)$.
    Instead of having an agent for each node, the authors propose to have a linear
    algebraic function that shifts the action component identifier values into their
    appropriate range: $u^{i+1}_{out}=f(u^{i}_{out})=u^{i}_{out}\times\beta^{i+1}+u^{i+1}$,
    with $\beta^{i+1}$ being the number of nodes at level $i+1$. More generally, given
    two actions $u^{i}$ and $u^{i+1}$ obtained from agents at levels $i$ and $i+1$,
    where for both actions we have identifiers in the range $[0,...,\rvert\ \mathcal{U}^{i}\rvert-1]$,
    and $[0,...,\rvert\ \mathcal{U}^{i+1}\rvert-1]$ respectively, then the action
    component identifier at level $i+1$ is $u^{i}\times\rvert\ \mathcal{U}^{i+1}\rvert+u^{i+1}$.
    The executive primitive action meanwhile will be computed via: $a=u^{L-1}\times\rvert\
    \mathcal{U}^{L}\rvert+u^{L}$. An illustration of this tree structure is provided
    in \autoreffig:cascading_selection_tree.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: CRLA 产生了一个解决方案，允许将大规模组合动作空间分解为分支树结构。树中的每个节点表示代理对选择下一个子节点的决策。每个节点都有自己的标识符，树中的所有节点具有相同的分支因子，并使用启发式方法来确定树的层数：$\rvert
    T\rvert=\log_{b}(\rvert\mathcal{A}\rvert)$。作者建议使用一个线性代数函数，将动作组件标识符值移到其适当范围：$u^{i+1}_{out}=f(u^{i}_{out})=u^{i}_{out}\times\beta^{i+1}+u^{i+1}$，其中$\beta^{i+1}$是第
    $i+1$ 层的节点数。更一般地，给定从第 $i$ 层和第 $i+1$ 层的代理获得的两个动作 $u^{i}$ 和 $u^{i+1}$，其中两个动作的标识符范围分别是
    $[0,...,\rvert\ \mathcal{U}^{i}\rvert-1]$ 和 $[0,...,\rvert\ \mathcal{U}^{i+1}\rvert-1]$，那么第
    $i+1$ 层的动作组件标识符是 $u^{i}\times\rvert\ \mathcal{U}^{i+1}\rvert+u^{i+1}$。同时，执行原始动作将通过以下方式计算：$a=u^{L-1}\times\rvert\
    \mathcal{U}^{L}\rvert+u^{L}$。此树结构的插图见 \autoreffig:cascading_selection_tree。
- en: '![Refer to caption](img/c8f32536bd7f3ae18734511f5026f444.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c8f32536bd7f3ae18734511f5026f444.png)'
- en: 'Figure 11: An illustration of the action selection process used by CRLA (adapted
    from [tran2022cascaded]). The leaf nodes represent the primitive actions from
    $\mathcal{A}$, while each internal node contains the action range for its children.
    Through using an algebraic formulation that makes use of an offset, only three
    agents with an action space $\rvert\ \mathcal{U}^{i}\rvert=3$ are needed to capture
    $\mathcal{A}$.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：CRLA使用的动作选择过程示意图（改编自 [tran2022cascaded]）。叶节点代表来自 $\mathcal{A}$ 的基本动作，而每个内部节点包含其子节点的动作范围。通过使用利用偏移量的代数公式，仅需三个动作空间为
    $\rvert\ \mathcal{U}^{i}\rvert=3$ 的代理就能捕捉到 $\mathcal{A}$。
- en: 'Cooperation among agents is facilitated via QMIX [rashid2018qmix], which uses
    a non-linear combination of the value estimates to compute the joint-action-value
    during training. The weights of the mixing network are produced using a hypernetwork [ha2016hypernetworks],
    conditioned on the state of the environment. In CRLA agents share a replay buffer.
    Therefore, as with BDQ, a synchronised sampling equivalent to CERTS [omidshafiei2017deep]
    is being used. The authors [tran2022cascaded] recommend limiting the size of the
    action sets to 10 – 15 actions, and to choose an $L$ that allows the approach
    to reconstruct the intended action set $\ \mathcal{U}$. CRLA-QMIX was evaluated
    on two environments against a version of CRLA using independent learners, and
    a single DDQN:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 通过QMIX [rashid2018qmix] 促进了代理之间的合作，QMIX 使用值估计的非线性组合来计算训练过程中的联合动作值。混合网络的权重由一个超网络
    [ha2016hypernetworks] 生成，并以环境状态为条件。在CRLA中，代理共享一个重放缓冲区。因此，与BDQ一样，使用了与CERTS [omidshafiei2017deep]
    相等的同步采样。作者 [tran2022cascaded] 推荐将动作集的大小限制为10到15个动作，并选择一个 $L$，以使方法能够重建预期的动作集 $\
    \mathcal{U}$。CRLA-QMIX在两个环境中与使用独立学习者的CRLA版本和单一DDQN进行了评估：
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A toy-maze scenario with a discretized action space, representing the directions
    in which the agent can move. The agent received a small negative reward for each
    step, and positive one upon completing the maze. An action size of 4096 was selected,
    with $n=12$ actuators.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个玩具迷宫场景，具有离散化的动作空间，代表代理可以移动的方向。代理每走一步都会获得小的负奖励，完成迷宫后会获得正奖励。选择了4096的动作大小，使用
    $n=12$ 个执行器。
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A partially observable CybORG capture the flag scenario from Red’s perspective.
    Upon finding a flag a large positive reward was received. A smaller reward was
    obtained for successfully hacking a host.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从Red的视角观察的部分可观测的CybORG夺旗场景。找到旗帜后会获得大量正奖励。成功黑进主机会获得较小的奖励。
- en: CRLA significantly outperformed DDQN on both the maze task and CybORG scenarios
    with more than 50 hosts. CRLA-QMIX had less variance and better stability than
    CRLA with ILs. However, in an evaluation scenario with 60 hosts the converged
    policies show similar rewards and steps per episode, potentially explained by
    the fact that CRLA-ILs also makes use of CERTs. The authors note that hyperparameter
    tuning for the QMIX hypernetwork was time consuming.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: CRLA在迷宫任务和CybORG场景中显著优于DDQN，特别是在主机数量超过50的情况下。CRLA-QMIX比带有ILs的CRLA具有更小的方差和更好的稳定性。然而，在一个有60个主机的评估场景中，收敛的策略显示出类似的奖励和每集步骤，这可能是由于CRLA-ILs也使用了CERTs。作者指出，QMIX超网络的超参数调优非常耗时。
- en: 6.2.3 Discrete Sequential Prediction of Continuous Actions
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 连续动作的离散序列预测
- en: '[metz2017discrete] propose a Sequential DQN (SDQN) for discretized continuous
    control. The original (*upper*) MDP with $N$ (actuators) times $D$ (dimensions)
    actions is transformed into a *lower* MDP with $1\times D$ actions. The lower
    MDP consists of the compositional action component, where $N$ actions are selected
    sequentially. Unlike BDQ actuators take turns selecting actions, and can observe
    the actions that have been selected by others (see \autoreffig:SDQN). Also, the
    action composition was obtained using a single DNN that learns to generalize across
    actuators. An LSTM [hochreiter1997long] was used to keep track of the selected
    actions. For stability, SDQN learns Q-values for both the upper and lower MDPs
    at the same time, performing a Bellman backup from the lower to the upper MDP
    for transitions where the Q-value should be equal. In addition, a zero discount
    is used for all steps except where the state of the upper MDP changes. Another
    requirement is the pre-specified ordering of actions. The authors hypothesise
    that this may negatively impact training on problems with a large number of actuators.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[metz2017discrete] 提出了用于离散化连续控制的顺序 DQN (SDQN)。原始的 (*上层*) MDP 具有 $N$（执行器）乘以
    $D$（维度）个动作，被转化为一个 *下层* MDP，具有 $1\times D$ 个动作。下层 MDP 包含组成动作组件，其中 $N$ 个动作是顺序选择的。与
    BDQ 执行器轮流选择动作，并能观察到其他人已选择的动作不同（参见 \autoreffig:SDQN）。此外，动作组合是使用一个学习跨执行器泛化的单一 DNN
    获得的。一个 LSTM [hochreiter1997long] 被用来跟踪已选择的动作。为了稳定性，SDQN 同时学习上层和下层 MDP 的 Q 值，对应于
    Q 值应相等的过渡，从下层 MDP 到上层 MDP 执行 Bellman 备份。此外，所有步骤使用零折扣，除了上层 MDP 状态变化的情况。另一个要求是动作的预先指定顺序。作者假设这可能对具有大量执行器的问题的训练产生负面影响。'
- en: The authors evaluate SDQN against DDPG on a number of continuous control tasks
    from the OpenAI gym [1606.01540], including Hopper ($N=3$), Swimmer ($N=2$), Half-Cheetah
    ($N=6$), Walker2d ($N=6$), and Humanoid($N=17$). SDQN outperformed DDPG on all
    domains except Walker2d. With respect to granularity SDQN required $D\geq 4$.
    The authors also evaluated 8 different action orderings at 3 points during training
    on Half Cheetah. All orderings achieved a similar performance.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在 OpenAI gym [1606.01540] 的多个连续控制任务中评估了 SDQN 与 DDPG 的表现，包括 Hopper ($N=3$)、Swimmer
    ($N=2$)、Half-Cheetah ($N=6$)、Walker2d ($N=6$) 和 Humanoid($N=17$)。SDQN 在所有领域中都优于
    DDPG，除了 Walker2d。在粒度方面，SDQN 需要 $D\geq 4$。作者还在 Half Cheetah 训练的 3 个时间点评估了 8 种不同的动作排序。所有排序的表现相似。
- en: '![Refer to caption](img/724ce78fdbc4df7adf2783ff055c70c7.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/724ce78fdbc4df7adf2783ff055c70c7.png)'
- en: 'Figure 12: Sequential DQN Architecture (adapted from [metz2017discrete]).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 顺序 DQN 架构（改编自 [metz2017discrete]）。'
- en: 6.2.4 Time-Varying Composite Action Spaces
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.4 时间变化的复合动作空间
- en: '[9507301] propose the *Structured Cooperation Reinforcement Learning (SCORE)*
    algorithm that accounts for dynamic time-varying action spaces, i.e., environments
    where a sub-set of actions become temporarily invalid [9507301]. SCORE can be
    applied to heterogeneous action spaces that contain continuous and discrete values.
    A series of DNNs model the composite action space. A centralized critic and decentralized
    actor (CCDA) approach [li2020f2a2] facilitates cooperation among the agents. In
    addition a Hierarchical Variational Autoencoder (HVAE) [edwards2017towards] maps
    the sub-action spaces of each agent to a common latent space. This is then fed
    to the critic, allowing the critic to model correlations between sub-actions,
    enabling the explicit modelling of dependencies between the agents’ action spaces.
    A graph attention network (GAT) [GAT] is used as the critic, in order to handle
    the varying numbers of agents (nodes). The HVAE and GAT are critical for SCORE
    to cope with varying numbers of heterogeneous actors. As a result, SCORE is a
    two stage framework, that must first learn an action space representation, before
    learning a robust and transferable policy. For the first phase a sufficient number
    of trajectories must be gathered for each sub-action space. The authors use a
    random policy to generate these transitions. Once the common latent action representation
    is acquired the training can switch to focusing on obtaining robust policies.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[9507301] 提出了*结构化合作强化学习（SCORE）*算法，该算法考虑了动态时间变化的动作空间，即某些动作子集在短时间内变得暂时无效 [9507301]。SCORE可以应用于包含连续值和离散值的异质动作空间。一系列DNNs对复合动作空间进行建模。一个集中式评论员和去中心化的演员（CCDA）方法
    [li2020f2a2] 促进了代理之间的合作。此外，一个层次变分自编码器（HVAE） [edwards2017towards] 将每个代理的子动作空间映射到一个共同的潜在空间。这些信息然后传递给评论员，使评论员能够建模子动作之间的相关性，从而明确建模代理动作空间之间的依赖关系。使用图注意网络（GAT）
    [GAT] 作为评论员，以处理变化的代理（节点）数量。HVAE和GAT对SCORE应对异质演员数量变化至关重要。因此，SCORE是一个两阶段的框架，必须首先学习动作空间表示，然后学习一个稳健且可迁移的策略。在第一阶段，需要为每个子动作空间收集足够的轨迹。作者使用随机策略生成这些过渡。一旦获得了共同的潜在动作表示，训练可以转向关注获得稳健的策略。'
- en: SCORE is evaluated on a proof-of-concept task – a Spider environment based on
    the MuJoCo Ant environment – and a Precision Agriculture Task, where the benefits
    of a mixed discrete-continuous action space comes into play. SCORE outperforms
    numerous baselines on both environments, including MADDPG [lowe2017multi], PPO [schulman2017proximal],
    SAC [haarnoja2018soft], H-PPO [fan2019hybrid], QMIX [rashid2018qmix] and MAAC [iqbal2019actor].
    However, the code for the environments and SCORE are not made publicly available.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: SCORE在一个概念验证任务上进行了评估——基于MuJoCo Ant环境的Spider环境——以及一个精准农业任务，其中混合离散-连续动作空间的优势得到了体现。SCORE在这两个环境中都优于众多基准模型，包括MADDPG
    [lowe2017multi]、PPO [schulman2017proximal]、SAC [haarnoja2018soft]、H-PPO [fan2019hybrid]、QMIX
    [rashid2018qmix]和MAAC [iqbal2019actor]。然而，这些环境和SCORE的代码并未公开提供。
- en: 6.2.5 Action Decomposition Approaches for Slate-MDPs
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.5 Slate-MDPs的动作分解方法
- en: Action decomposition approaches have also been applied to Slate-MDPs. Two noteworthy
    efforts are *Cascading Q-Networks* and *Slate Decomposition*.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 动作分解方法也已应用于Slate-MDPs。其中两个值得注意的努力是*级联Q网络*和*Slate分解*。
- en: 'Cascading Q-Networks (CDQNs): [pmlr-v97-chen19f] introduce a model-based RL
    approach for the recommender problem that utilizes Generative Adversarial Networks
    (GANs)[goodfellow2014generative] to imitate the user’s behaviour dynamics and
    reward function. The motivation for using GANs is to address the issue that a
    user’s interests can evolve over time, and the fact that the recommender system
    can have a significant impact on this evolution process. In contrast, most other
    works in this area use a manually designed reward function. A CDQN is used to
    address the large action space, through which a combinatorial recommendation policy
    is obtained. CDQNs consist of $k$ related Q-functions, where actions are passed
    on in a cascading fashion.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**级联Q网络（CDQNs）**: [pmlr-v97-chen19f] 提出了一个基于模型的强化学习方法，用于推荐问题，该方法利用生成对抗网络（GANs）[goodfellow2014generative]
    来模拟用户的行为动态和奖励函数。使用GAN的动机是解决用户兴趣可能随时间演变的问题，并且推荐系统可能对这一演变过程产生重大影响。相比之下，该领域的大多数其他工作使用的是手动设计的奖励函数。CDQN用于处理大规模动作空间，通过它可以获得组合推荐策略。CDQN由$k$个相关的Q函数组成，动作以级联的方式传递。'
- en: CDQNs were evaluated on six real-world recommendation datasets – *MovieLens*,
    *LastFM*, *Yelp*, *Taobao*, *YooChoose*, and *Ant Financial* – against a range
    of non-RL recommender approaches, including IKNN, S-RNN, SCKNNC, XGBOOST, DFM,
    W&D-LR, W&D-CCF, and a Vanilla DQN. On the majority of these datasets, the generative
    adversarial model is a better fit to user behaviour with respect to held-out likelihood
    and click prediction. With respect to the resulting model policies, better cumulative
    and long-term rewards were obtained. The approach took less time to adjust compared
    to approaches that did not make use of the GANs synthesized user. However, we
    caution that applying model-based RL approaches to complex asymmetrical adversarial
    games, such as ACO, requires further considerations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: CDQNs在六个真实世界推荐数据集上进行了评估——*MovieLens*、*LastFM*、*Yelp*、*淘宝*、*YooChoose* 和 *Ant
    Financial*——与包括IKNN、S-RNN、SCKNNC、XGBOOST、DFM、W&D-LR、W&D-CCF和Vanilla DQN在内的多种非RL推荐方法进行比较。在大多数这些数据集上，生成对抗模型在持出似然和点击预测方面更适合用户行为。就模型策略而言，获得了更好的累积和长期奖励。该方法调整的时间比未使用GAN合成用户的方法要少。然而，我们提醒，将基于模型的RL方法应用于复杂的非对称对抗性游戏（如ACO）需要进一步的考虑。
- en: 'Slate Decomposition: Slate decomposition, or *SlateQ*, is an approach where
    the Q-value estimate for a slate $\bm{a}$ can be decomposed into the item-wise
    Q-values of its constituent items $u_{i}$ [ie2019reinforcement]. Having a decomposition
    approach that can learn $\bar{Q}(s,a_{i})$ for an item $i$ mitigates the generalization
    and exploration challenges listed above. However, the ability to successfully
    factor the Q-value of a slate $\bm{a}$ relies on two assumptions: *Single Choice*
    (SC) and *Reward/Transition Dependence on Selection* (RTDS). The authors show
    theoretically that given the standard assumptions with respect to learning and
    exploration [sutton2018reinforcement], as well as SC and RTDS, SlateQ will converge
    to the true slate Q-function $Q_{\pi}(x,\bm{a})$. SlateQ is evaluated in a simulation,
    while validity and scalability were tested in live experiments on YouTube.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Slate分解：Slate分解，或称为*SlateQ*，是一种将Slate的Q值估计$\bm{a}$分解为其组成项$u_{i}$的逐项Q值的方法[ie2019reinforcement]。拥有一种能够学习项$i$的$\bar{Q}(s,a_{i})$的分解方法可以缓解上述的泛化和探索挑战。然而，成功分解Slate
    $\bm{a}$的Q值依赖于两个假设：*单一选择*（SC）和*选择的奖励/过渡依赖性*（RTDS）。作者在理论上证明，考虑到学习和探索的标准假设[sutton2018reinforcement]以及SC和RTDS，SlateQ将收敛到真实的Slate
    Q函数$Q_{\pi}(x,\bm{a})$。SlateQ在模拟中进行了评估，同时在YouTube上的实时实验中测试了其有效性和可扩展性。
- en: 6.3 Action Elimination Approaches
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 行动消除方法
- en: Learning with a large combinatorial action space is often challenging due to
    a large number of actions being either redundant or irrelevant within a given
    state [zahavy2018learn]. RL agents lack the ability to determine a sub-set of
    relevant actions. However, there have been efforts towards the state dependent
    elimination of actions. [zahavy2018learn] combine a DQN with an action-elimination
    network (AEN), which is trained via an elimination signal $e$, resulting in AE-DQN
    (\autoreffig:aedqn). After executing an action $a_{t}$, the environment will return
    a binary action elimination signal in $e(s_{t},a_{t})$ in addition to the new
    state and reward signal. The elimination signal $e$ is determined using domain-specific
    knowledge. A linear contextual bandit model is applied to the outputs of the AEN,
    that is tasked with eliminating irrelevant actions with a high probability, balancing
    out exploration/exploitation. Concurrent learning introduces the challenge that
    the learning process of both the DQN and AEN affect the state-action distribution
    of the other. However, the authors provide theoretical guarantees on the convergence
    of the approach using linear contextual bandits. While the AE-DQN was designed
    for text based games the authors note that the approach is applicable to any environment
    where an elimination signal can be obtained via a rule-based system.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模组合动作空间中学习通常具有挑战性，因为在给定状态下，大量的动作要么是冗余的，要么是无关的[zahavy2018learn]。RL代理缺乏确定相关动作子集的能力。然而，已有一些针对状态依赖的动作消除的努力。[zahavy2018learn]将DQN与一个动作消除网络（AEN）结合起来，AEN通过消除信号$e$进行训练，从而产生AE-DQN（\autoreffig:aedqn）。在执行动作$a_{t}$后，环境会返回一个二进制动作消除信号$e(s_{t},a_{t})$，以及新的状态和奖励信号。消除信号$e$是使用特定领域的知识确定的。线性上下文赌博模型应用于AEN的输出，该模型负责以高概率消除无关动作，平衡探索/利用。并行学习引入了DQN和AEN的学习过程互相影响状态-动作分布的挑战。然而，作者对使用线性上下文赌博模型的方法收敛性提供了理论保证。虽然AE-DQN是为基于文本的游戏设计的，但作者指出该方法适用于任何可以通过规则系统获得消除信号的环境。
- en: '![Refer to caption](img/8983e43ba5dbaee2876a3d01fa426911.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8983e43ba5dbaee2876a3d01fa426911.png)'
- en: 'Figure 13: Action Elimination DQN (adapted from [zahavy2018learn]). The agent
    selects an action $a_{t}$, and observes a reward $r_{t-1}$, the next observation
    $o_{t}$ and an elimination signal $e_{t-1}$. The agent uses this information to
    learn two function approximation deep networks: a DQN and an AEN. The AEN provides
    an admissible actions set $\mathcal{A}^{\prime}\subseteq\mathcal{A}$ to the DQN,
    from which the DQN can pick the next action $a_{t+1}$.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：行动消除DQN（改编自[zahavy2018learn]）。代理选择一个动作$a_{t}$，并观察到一个奖励$r_{t-1}$、下一个观察值$o_{t}$和一个消除信号$e_{t-1}$。代理利用这些信息来学习两个函数逼近深度网络：DQN和AEN。AEN向DQN提供一个可接受的动作集合$\mathcal{A}^{\prime}\subseteq\mathcal{A}$，DQN可以从中选择下一个动作$a_{t+1}$。
- en: AE-DQN is evaluated on both Zork and a $K$-Room Gridworld environment. For the
    $K$-rooms Gridworld environment a significant gain for the use of action elimination
    was observed as the number of categories $K$ was increased. Similar benefits were
    observed in the Zork environment, e.g., AE-DQN using 215 actions being able to
    match the performance of a DQN trained with a reduced action space of 35 actions,
    while significantly outperforming a DQN with 215 actions. However, questions remain
    regarding the extent to which the approach is applicable to *very* high dimensional
    action spaces, where additional considerations may be required as to how the AEN
    can generalize over actions.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: AE-DQN在Zork和$K$-Room Gridworld环境中进行了评估。在$K$-房间Gridworld环境中，随着类别$K$数量的增加，使用动作消除观察到了显著的增益。在Zork环境中也观察到了类似的好处，例如，AE-DQN使用215个动作能够与用35个动作的DQN训练的性能相匹配，同时显著超越了使用215个动作的DQN。然而，对于方法在*非常*高维动作空间中的适用范围仍然存在疑问，在这种情况下，可能需要额外的考虑以确定AEN如何在动作上进行泛化。
- en: Action elimination has also been applied to Slate-MDPs. [10.1145/3289600.3290999]
    adapted the REINFORCE algorithm into a top-$k$ neural candidate generator for
    large action spaces. The approach relies on data obtained through previous recommendation
    policies (behaviour policies $\beta$), which are utilized as a means to correct
    data biases via an importance sampling weight while training a new policy. Importance
    sampling is used due to the model being trained without access to a real-time
    environment. Instead the policy is trained on logged feedback of actions chosen
    by a historical mixture of policies, which will have a different distribution
    compared to the one that is being updated. A recurrent neural network is used
    to keep track of the evolving user interest.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 行动消除也已应用于 Slate-MDPs。[10.1145/3289600.3290999] 将 REINFORCE 算法改编为适用于大规模行动空间的
    top-$k$ 神经候选生成器。这种方法依赖于通过之前的推荐策略（行为策略 $\beta$）获得的数据，这些数据被用作通过重要性采样权重来纠正数据偏差的手段，同时训练新的策略。由于模型在训练时没有实时环境的访问，使用了重要性采样。相反，策略在由历史混合策略选择的行动的记录反馈上进行训练，这将与正在更新的分布有所不同。使用递归神经网络来跟踪不断变化的用户兴趣。
- en: With respect to sampling actions, instead of choosing the $k$ items that have
    the highest probability, the authors use a stochastic policy via Boltzmann exploration.
    However, computing the probabilities for all $N$ actions is computationally inefficient.
    Instead the authors chose the top $M$ items, select their logits, and then apply
    the softmax over this smaller set $M$ to normalize the probabilities and sample
    from this smaller distribution. The authors note that when $M\ll K$, one can still
    retrieve a reasonably sized probability mass, while limiting the risk of bad recommendations.
    Exploration and exploitation are balanced through returning the top $K^{\prime}$
    most probable items (with $K^{\prime}<k$), and sample $K-K^{\prime}$ items from
    the remaining $M-K^{\prime}$ items. The approach is evaluated in a production
    RNN candidate generation model in use at YouTube, and experiments are performed
    to validate the various design decisions.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 关于采样操作，作者们并不是选择概率最高的 $k$ 项，而是使用通过 Boltzmann 探索的随机策略。然而，计算所有 $N$ 个操作的概率在计算上是低效的。相反，作者选择了前
    $M$ 项，选择它们的 logits，然后对这个较小的集合 $M$ 应用 softmax，以规范化概率并从这个较小的分布中进行采样。作者指出，当 $M\ll
    K$ 时，仍然可以获取一个合理大小的概率质量，同时限制了差推荐的风险。通过返回最可能的前 $K^{\prime}$ 项（其中 $K^{\prime}<k$），并从剩余的
    $M-K^{\prime}$ 项中采样 $K-K^{\prime}$ 项，从而在探索和利用之间保持平衡。该方法在 YouTube 使用的生产 RNN 候选生成模型中进行了评估，并进行了实验以验证各种设计决策。
- en: 6.4 Hierarchical Reinforcement Learning
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 层次强化学习
- en: The RL literature features a number of hierarchical formulations. Feudal RL
    features Q-learning with a managerial hierarchy, where \saymanagers learn to set
    tasks for \saysub-managers until agents taking atomic actions at the lowest levels
    are reached [dayan1992feudal, vezhnevets2017feudal]. There have also been factored
    hierarchical approaches that decompose the value function of an MDP into smaller
    constituent MDPs [dietterich2000hierarchical, guestrin2003efficient].
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习文献中有多种层次化的表述。封建式强化学习特点是带有管理层次的 Q-learning，其中 **管理者** 学会为 **子管理者** 设置任务，直到到达在最低层执行原子操作的代理人
    [dayan1992feudal, vezhnevets2017feudal]。也有一些分解层次化的方法，将 MDP 的价值函数分解为更小的组成 MDP [dietterich2000hierarchical,
    guestrin2003efficient]。
- en: '[wei2018hierarchical] propose Parameterized Actions Trust Region Policy Optimization
    (TRPO) [schulman2015trust] and Parameterized Actions SVG(0), hierarchical RL approaches
    designed for Parameterized Action MDPs. The approaches consist of two policies
    implemented by neural networks. The first network is used by the discrete action
    policy $\pi_{\theta}(a\rvert s)$ to obtain an action $a$ in state $s$. The second
    network is for the parameter policy. It takes both the state and discrete action
    as inputs, and returns a continuous parameter (or a *set* of continuous parameters)
    $\pi_{\vartheta}(u\rvert s,a)$. Therefore, the joint action probability for $(a,u)$
    given a state $s$ is conditioned on both policies: $\pi(a,u\rvert s)\ =\pi_{\theta}(a\rvert
    s)\pi_{\vartheta}(u\rvert s,a)$. This formulation has the advantage that since
    the action $a$ is known before generating the parameters, there is no need to
    determine which action tuple $(a,u)$ has the highest Q-value for a state $s$.
    In order to optimize the above policies, methods are required that can back-propagate
    all the way back through the discrete action policy. Here the authors introduce
    a modified version of TRPO [schulman2015trust] that accounts for the above policy
    formulation, and a parameterized action stochastic value gradient approach that
    uses the Gumbel-Softmax trick for drawing an action $u$ and back-propagating through
    $[\theta,\vartheta]$. The approach is depicted in \autoreffig:pasvg. With respect
    to evaluation, PATRPO outperforms PASVG(0) and PADDPG within a Platform Jumping
    environment. PATRPO also outperforms PADDPG within the Half Field Offense soccer [HFO]
    environment with no goal keeper [hausknecht2015deep].'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[wei2018hierarchical] 提出了参数化动作信任区域策略优化（TRPO）[schulman2015trust] 和参数化动作 SVG(0)，这些是为参数化动作
    MDP 设计的层次化 RL 方法。该方法包括两个由神经网络实现的策略。第一个网络用于离散动作策略 $\pi_{\theta}(a\rvert s)$，以在状态
    $s$ 下获得动作 $a$。第二个网络用于参数策略。它将状态和离散动作作为输入，返回一个连续参数（或一个 *集合* 的连续参数）$\pi_{\vartheta}(u\rvert
    s,a)$。因此，给定状态 $s$ 的 $(a,u)$ 的联合动作概率依赖于两个策略：$\pi(a,u\rvert s)\ =\pi_{\theta}(a\rvert
    s)\pi_{\vartheta}(u\rvert s,a)$。这种表述的优点在于，由于动作 $a$ 在生成参数之前已知，因此无需确定哪个动作元组 $(a,u)$
    在状态 $s$ 下具有最高的 Q 值。为了优化上述策略，需要可以通过离散动作策略进行全程反向传播的方法。在这里，作者引入了一个修改版的 TRPO [schulman2015trust]，以适应上述策略表述，以及一种参数化动作随机值梯度方法，该方法使用
    Gumbel-Softmax 技巧来抽取动作 $u$ 并通过 $[\theta,\vartheta]$ 进行反向传播。该方法在 \autoreffig:pasvg
    中描述。关于评估，PATRPO 在平台跳跃环境中优于 PASVG(0) 和 PADDPG。PATRPO 在没有守门员的半场进攻足球 [HFO] 环境中也优于
    PADDPG [hausknecht2015deep]。'
- en: '![Refer to caption](img/fbc7a3e271395f858844ed13bf45add0.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fbc7a3e271395f858844ed13bf45add0.png)'
- en: 'Figure 14: PASVG(0) (adapted from [wei2018hierarchical]).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '图14: PASVG(0)（改编自 [wei2018hierarchical]）。'
- en: 6.5 Curriculum Learning
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 课程学习
- en: 'Curriculum learning (CL) approaches attempt to accelerate the learning process
    through initially subjecting the RL agent to a simplified version of the problem,
    and subsequently gradually increasing the task complexity, e.g., via a progression
    function  [bassich2019continuous, bassich2020curriculum]. This approach has also
    been applied to RL for combinatorial action spaces. [pmlr-v119-farquhar20a] introduce
    a CL approach using a growing action space (GAS). For an MDP with unrestricted
    action space $\mathcal{A}$ the authors define a set of $N$ action spaces $\mathcal{A}_{l},l\in\{0,...,N-1\}$.
    Each action space is a subset of the next level $l$: $\mathcal{A}_{0}\subset\mathcal{A}_{1}\subset...\subset\mathcal{A}_{N-1}\subset\mathcal{A}$.
    A policy restricted to an action space $\mathcal{A}_{l}$ is denoted as $\pi_{l}(a,s)$.
    The optimal policy for this restricted policy class is $\pi^{*}_{l}(u,x)$, and
    the corresponding action-value and value functions are: $Q^{*}_{l}(s,a)$ and $V^{*}_{l}(s)=\max_{a}Q^{*}_{l}(s,a)$.
    Domain knowledge is used to define a hierarchy of actions. For every action $a\in\mathcal{A}_{l}$
    where $l>0$ there is a parent action $\texttt{parent}_{l}(a)$ in the space of
    $\mathcal{A}_{l-1}$. Given that at each level, subsets of action spaces are subsets
    of larger action spaces, the actions available in $\mathcal{A}_{l-1}$ are their
    own parents in $\mathcal{A}_{l}$. The authors note that in many environments Euclidean
    distances are a valid measure for implementing a heuristic for defining a hierarchy
    over actions.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习（CL）方法试图通过最初将RL代理置于问题的简化版本中，并随后逐渐增加任务复杂度（例如，通过渐进函数 [bassich2019continuous,
    bassich2020curriculum]）来加速学习过程。这种方法也已应用于组合动作空间的RL。[pmlr-v119-farquhar20a]介绍了一种使用增长动作空间（GAS）的CL方法。对于具有无限制动作空间$\mathcal{A}$的MDP，作者定义了一组$N$个动作空间$\mathcal{A}_{l},l\in\{0,...,N-1\}$。每个动作空间是下一个级别$l$的子集：$\mathcal{A}_{0}\subset\mathcal{A}_{1}\subset...\subset\mathcal{A}_{N-1}\subset\mathcal{A}$。限制在动作空间$\mathcal{A}_{l}$上的策略表示为$\pi_{l}(a,s)$。该受限策略类的最优策略是$\pi^{*}_{l}(u,x)$，对应的动作值函数和价值函数为：$Q^{*}_{l}(s,a)$和$V^{*}_{l}(s)=\max_{a}Q^{*}_{l}(s,a)$。领域知识用于定义动作的层次结构。对于每个动作$a\in\mathcal{A}_{l}$，其中$l>0$，在$\mathcal{A}_{l-1}$空间中有一个父动作$\texttt{parent}_{l}(a)$。鉴于在每个级别，动作空间的子集是更大动作空间的子集，因此在$\mathcal{A}_{l-1}$中可用的动作在$\mathcal{A}_{l}$中是它们自己的父动作。作者指出，在许多环境中，欧几里得距离是实现定义动作层次结构的启发式的有效度量。
- en: An ablation study on discretized Acrobat and Mountaincart environments shows
    the value of efficiently using the data collected during training across levels.
    The authors also evaluate their approach on SMAC, using a far larger number of
    units compared to those usually used in MARL experiments – i.e., scenarios with
    50-100 instead of 20-30. In addition, the task difficulty is increased through
    having randomized starting positions, and scripted opponent logic that holds its
    position until any agent-controlled unit is in range. Subsequently the enemy focus-fires
    on its closest enemy. Having to locate the enemy first increases the exploration
    challenge. The authors demonstrate the advantage of their approach GAS(2) (GAS
    with 2 levels) against various ablations of their approach and methods that directly
    train on the full action space.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散化的Acrobat和Mountaincart环境进行的消融研究展示了在训练过程中高效利用各级数据的价值。作者还在SMAC上评估了他们的方法，使用的单位数量远远大于通常在MARL实验中使用的数量——即50-100个场景，而不是20-30个。此外，任务难度通过随机起始位置和在任何代理控制的单位进入范围之前保持其位置的脚本对手逻辑来增加。随后，敌人会集中火力攻击其最近的敌人。首先定位敌人增加了探索挑战。作者展示了他们的方法GAS(2)（具有2个级别的GAS）相对于各种方法的优势，包括对其方法的消融和直接在完整动作空间上训练的方法。
- en: '[yu4167820curriculum] take a similar approach to GAS [pmlr-v119-farquhar20a]
    for decision making in intelligent healthcare. A Progressive Action Space (PAS)
    approach allows the learner to master easier tasks first, via generalized actions,
    before gradually increasing the granularity, e.g., modifying the precise volume
    of a drug to be given to a patient. This work focuses on an offline RL setting,
    where traditional approaches lead to inaccurate state-action evaluation for those
    actions seldom applied by the physicians in the historical clinical dataset –
    a common problem for offline RL [fujimoto2019off]. Similar to GAS [pmlr-v119-farquhar20a],
    PAS also requires domain knowledge for defining $N$ abstracted action spaces $\{a_{1},a_{2},...,a_{N}\}$
    and a corresponding number of curricula $\{M_{1},M_{2},...,M_{N}\}$. Value and
    policy transfer approaches are used to transfer knowledge across levels.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[yu4167820curriculum] 采取了类似于GAS [pmlr-v119-farquhar20a] 的方法来进行智能医疗中的决策。渐进式行动空间（PAS）方法允许学习者首先掌握较容易的任务，通过一般化的行动，然后逐渐增加任务的细化程度，例如，调整给患者的药物的准确剂量。该工作关注于离线RL设置，其中传统方法会导致对那些在历史临床数据集中很少应用的行动的状态-行动评估不准确——这是离线RL
    [fujimoto2019off] 的一个常见问题。类似于GAS [pmlr-v119-farquhar20a]，PAS也需要领域知识来定义$N$个抽象行动空间$\{a_{1},a_{2},...,a_{N}\}$和相应数量的课程$\{M_{1},M_{2},...,M_{N}\}$。价值和策略转移方法用于跨层级转移知识。'
- en: 7 Adversarial Learning
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 对抗学习
- en: A number of approaches discussed in the previous sections have been applied
    to adversarial domains, including ACO (e.g., Wolpertinger [nguyen2020multiple]
    and CRLA [tran2022cascaded]). However, these approaches were trained against stationary
    opponents. In practice, the ACO problem is non-stationary, with Red and Blue adjusting
    their approach over time. In adversarial learning terminology, Red and Blue will
    attempt to compute *approximate best responses* (ABRs) to each others’ policies [oliehoek2018beyond].
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 前几节讨论的一些方法已应用于对抗领域，包括ACO（例如，Wolpertinger [nguyen2020multiple] 和CRLA [tran2022cascaded]）。然而，这些方法是在对抗性静态对手的情况下进行训练的。实际上，ACO问题是非静态的，Red和Blue会随时间调整他们的方法。在对抗学习术语中，Red和Blue将尝试计算*近似最优回应*（ABRs）以应对彼此的策略
    [oliehoek2018beyond]。
- en: 7.1 The Adversarial Learning Challenge
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 对抗学习挑战
- en: 'In this section we shall formally define the adversarial learning problem,
    desirable solution concepts, and the approaches designed to help learning agents
    converge upon policies that are hard to exploit. ACO environments are adversarial.
    However, they are not always *zero-sum games*. For instance, in CAGE Challenge
    2 Blue receives a penalty for restoring a compromised host due to the action having
    consequences for any of the Green agents currently working on this facility. Here,
    Red does not receive a corresponding reward $r_{Red}=-r_{Blue}$. Therefore, ACO
    lacks a key property from two player zero-sum games, where the gain of one player
    is equal to the loss of the other player. However, in this section we shall treat
    the ACO problem as a quasi zero-sum game, with the assumption that the consequences
    of Red winning outweigh other factors, shall assume: ${\mathcal{G}}_{1}({\pi,\mu})\approx-{\mathcal{G}}_{2}({\pi,\mu})$.
    An equilibrium concept commonly used in this class of games to define solutions
    is the Nash equilibrium [nash1951non]:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将正式定义对抗学习问题、期望的解决方案概念以及旨在帮助学习代理收敛到难以利用的策略的方法。ACO环境是对抗性的。然而，它们并不总是*零和游戏*。例如，在CAGE挑战2中，Blue因恢复受损主机而受到惩罚，因为该行动对当前在该设施上工作的任何Green代理都有影响。在这里，Red不会获得相应的奖励$r_{Red}=-r_{Blue}$。因此，ACO缺乏来自两个玩家零和游戏的一个关键特性，即一个玩家的收益等于另一个玩家的损失。然而，在本节中，我们将把ACO问题视为准零和游戏，假设Red获胜的后果超过其他因素，我们将假设：${\mathcal{G}}_{1}({\pi,\mu})\approx-{\mathcal{G}}_{2}({\pi,\mu})$。在这类游戏中常用的均衡概念是纳什均衡
    [nash1951non]：
- en: Definition 7.1  (Nash Equilibrium).
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 7.1 （纳什均衡）。
- en: 'A joint policy $\bm{\pi}^{*}$ is a Nash equilibrium *iff* no player $i$ can
    improve their gain through unilaterally deviating from $\bm{\pi}^{*}$:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一个联合策略$\bm{\pi}^{*}$是纳什均衡*当且仅当*没有玩家$i$能够通过单方面偏离$\bm{\pi}^{*}$来改善他们的收益：
- en: '|  | $\forall i,\forall\pi_{i}\in\Delta(\mathcal{S},\mathcal{A}_{i}),\forall
    s\in\mathcal{S},{\mathcal{G}}_{i}(\langle\pi^{*}_{i},\bm{\pi}^{*}_{-i}\rangle)\geq{\mathcal{G}}_{i}(\langle\pi_{i},\bm{\pi}^{*}_{-i}\rangle).$
    |  | (4) |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $\forall i,\forall\pi_{i}\in\Delta(\mathcal{S},\mathcal{A}_{i}),\forall
    s\in\mathcal{S},{\mathcal{G}}_{i}(\langle\pi^{*}_{i},\bm{\pi}^{*}_{-i}\rangle)\geq{\mathcal{G}}_{i}(\langle\pi_{i},\bm{\pi}^{*}_{-i}\rangle).$
    |  | (4) |'
- en: 'Our focus is on finite two-player (quasi) zero-sum games, where an equilibrium
    is referred to as a saddle point, representing the value of the game $v*$. Given
    two policies $\pi_{1}$, $\pi_{2}$, the equilibria of a finite zero-sum game is:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注的是有限的两玩家（准）零和游戏，其中的均衡被称为鞍点，代表游戏的价值 $v*$。给定两个策略 $\pi_{1}$，$\pi_{2}$，有限零和游戏的均衡是：
- en: Theorem 1  (Minmax Theorem).
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1（极小极大定理）。
- en: 'In a finite zero-sum game:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在有限零和游戏中：
- en: '|  | $max_{\pi_{1}}min_{\pi_{2}}{\mathcal{G}}_{i}(\langle\pi_{1},\pi_{2}\rangle)=min_{\pi_{2}}max_{\pi_{1}}{\mathcal{G}}_{i}(\langle\pi_{1},\pi_{2}\rangle)=v^{*}.$
    |  | (5) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $max_{\pi_{1}}min_{\pi_{2}}{\mathcal{G}}_{i}(\langle\pi_{1},\pi_{2}\rangle)=min_{\pi_{2}}max_{\pi_{1}}{\mathcal{G}}_{i}(\langle\pi_{1},\pi_{2}\rangle)=v^{*}.$
    |  | (5) |'
- en: Above is one of the fundamental theorems of game theory, which states that every
    finite, zero-sum, two-player game has optimal mixed strategies [v1928theorie].
    We shall discuss the implications of the above equations for ACO from the perspective
    of Blue. Given a joint-policy $\langle\pi^{*}_{Blue},\pi^{*}_{Red}\rangle$ where
    $\pi^{*}_{Blue}$ and $\pi^{*}_{Red}$ represent optimal mixed strategies, then
    by definition Red will be unable to learn a new best response $\pi_{Red}$ that
    improves on $\pi^{*}_{Red}$. Obtaining $\pi^{*}_{Blue}$ guarantees that Blue will
    perform well, even against a worst case opponent [perolat2022mastering]. This
    means that, even if the value of the game $v^{*}$ is in Red’s favour, assuming
    that Blue has found $\pi^{*}_{Blue}$, then Blue has found a policy that limits
    the extent to which that Red can *exploit* Blue.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 上述是博弈论中的基本定理之一，声明每个有限的、零和的、两玩家的游戏都有最优混合策略 [v1928theorie]。我们将从 Blue 的角度讨论上述方程对
    ACO 的影响。给定联合策略 $\langle\pi^{*}_{Blue},\pi^{*}_{Red}\rangle$，其中 $\pi^{*}_{Blue}$
    和 $\pi^{*}_{Red}$ 代表最优混合策略，则根据定义，Red 将无法学习到一个新的最佳响应 $\pi_{Red}$，从而改进 $\pi^{*}_{Red}$。获得
    $\pi^{*}_{Blue}$ 保证了 Blue 会表现良好，即使面对最坏情况的对手 [perolat2022mastering]。这意味着，即使游戏的价值
    $v^{*}$ 对 Red 有利，假设 Blue 找到了 $\pi^{*}_{Blue}$，那么 Blue 就找到了一个策略，可以限制 Red 对 Blue
    的 *利用* 程度。
- en: 'One of the long-term objectives of MARL is to limit the exploitability of agents
    deployed in competitive environments [lanctot2017unified, oliehoek2018beyond,
    heinrich2016deep]. While a number of methods for limiting exploitability exist
    that are underpinned by theoretical guarantees, in practice finding the value
    of the game is challenging even for simple games. This is due to DRL using function
    approximators that are unable to compute *exact* best responses to an opponent’s
    policy. The best a DRL agent can achieve is an ABR. In addition, for complex games
    finding a Nash equilibrium is intractable. Here the concept of an approximate
    Nash equilibrium ($\epsilon$-NE) is helpful [oliehoek2018beyond, lanctot2017unified]:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: MARL 的长期目标之一是限制在竞争环境中部署的代理的可利用性 [lanctot2017unified, oliehoek2018beyond, heinrich2016deep]。虽然存在许多具有理论保证的限制可利用性的方法，但在实践中，即使对于简单的游戏，找到游戏的价值也是具有挑战性的。这是因为
    DRL 使用的函数逼近器无法计算对手策略的 *精确* 最佳响应。DRL 代理所能达到的最佳状态是 ABR。此外，对于复杂的游戏，找到纳什均衡是不可行的。在这种情况下，近似纳什均衡
    ($\epsilon$-NE) 的概念是有帮助的 [oliehoek2018beyond, lanctot2017unified]：
- en: Definition 7.2  ($\epsilon$-Nash Equilibrium).
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 7.2  ($\epsilon$-纳什均衡)。
- en: 'The joint-policy $\bm{\pi}^{*}$ is an $\epsilon$-NE *iff*:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 联合策略 $\bm{\pi}^{*}$ 是一个 $\epsilon$-NE *当且仅当*：
- en: '|  | $\forall i,\forall\pi_{i}\in\Delta(\mathcal{S},\mathcal{A}_{i}),\forall
    s\in\mathcal{S},{\mathcal{G}}_{i}(\langle\pi^{*}_{i},\bm{\pi}^{*}_{-i}\rangle)\geq{\mathcal{G}}_{i}(\langle\pi_{i},\bm{\pi}^{*}_{-i}\rangle)-\epsilon.$
    |  | (6) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\forall i,\forall\pi_{i}\in\Delta(\mathcal{S},\mathcal{A}_{i}),\forall
    s\in\mathcal{S},{\mathcal{G}}_{i}(\langle\pi^{*}_{i},\bm{\pi}^{*}_{-i}\rangle)\geq{\mathcal{G}}_{i}(\langle\pi_{i},\bm{\pi}^{*}_{-i}\rangle)-\epsilon.$
    |  | (6) |'
- en: 7.2 Approaches Towards Limiting Exploitability
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 限制可利用性的方法
- en: 'The above raises the question: how can we *efficiently* limit the exploitability
    of ACO agents? A key insight here from the adversarial learning literature is
    that finding $\pi^{*}_{Blue}$ will require learning to best respond to principled
    Red agents, ideally through computing a best response against $\pi^{*}_{Red}$.
    However, similarly Red will need to face strong Blue agents to find $\pi^{*}_{Red}$.
    This raises the need for an iterative adversarial learning process where Blue
    and Red learn (A)BRs to each others’ latest policies. However, naïve independent
    learning approaches fail to generalize well due to a tendency to overfit on their
    opponents, also known as joint-policy correlation [lanctot2017unified]. Therefore,
    a more principled approach is required.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 上述情况引发了一个问题：我们如何*有效地*限制ACO代理的可被利用性？对抗学习文献中的一个关键见解是，找到$\pi^{*}_{Blue}$需要学习如何最好地响应有原则的Red代理，理想情况下，通过计算对$\pi^{*}_{Red}$的最佳响应。然而，Red同样需要面对强大的Blue代理来找到$\pi^{*}_{Red}$。这突显了一个迭代对抗学习过程的需求，在这个过程中，Blue和Red学习（A）BRs对方最新的策略。然而，天真的独立学习方法由于过度拟合对手，通常难以很好地推广，也称为联合策略相关性[lanctot2017unified]。因此，需要一种更有原则的方法。
- en: '[gleave2019adversarial] show that an adversary can learn simple attack policies
    that reliably win against a static opponent implemented with function approximators.
    Often random and uncoordinated behavior is sufficient to trigger sub-optimal actions ⁵⁵5The
    authors provide videos of the attack behaviours: \urlhttps://adversarialpolicies.github.io/.
    By adversarial attacks the study refers to attacks on the opponents observation
    space. This is quasi equivalent to adding perturbations to images for causing
    a misclassification in supervised learning, but doing so via taking actions within
    the environment. A worrying finding is that DNNs are more vulnerable towards high-dimensional
    adversarial samples [gilmer2018adversarial, khoury2018geometry, shafahi2018adversarial].
    The same applies for DRL. Empirical evaluations on MuJoCo show that the greater
    the dimensionality of the area of the observation space that Red can impact, the
    more vulnerable the victim is towards attacks [gleave2019adversarial].'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[gleave2019adversarial]展示了对手可以学习简单的攻击策略，这些策略可以可靠地战胜使用函数逼近器实现的静态对手。通常，随机且无协调的行为足以触发次优动作⁵⁵5作者提供了攻击行为的视频：\urlhttps://adversarialpolicies.github.io/。研究中的对抗攻击指的是对对手观察空间的攻击。这在某种程度上等同于通过在环境中采取行动对图像添加扰动以导致监督学习中的错误分类。一个令人担忧的发现是，DNNs对高维对抗样本更为脆弱[gilmer2018adversarial,
    khoury2018geometry, shafahi2018adversarial]。DRL也是如此。对MuJoCo的实证评估显示，Red可以影响的观察空间的维度越大，受害者对攻击的脆弱性就越大[gleave2019adversarial]。'
- en: 'The work by [gleave2019adversarial] shows that agents trained via simplistic
    training schemes – e.g., self-play – are very far from an $\epsilon$ bounded Nash
    equilibrium. This raises the need for training schemes designed to limit the exploitability
    of agents. [perolat2022mastering] identify three categories of approaches for
    reducing the exploitability of agents: regret minimization, regret policy gradient
    methods, and best response techniques.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[gleave2019adversarial]的研究表明，通过简单训练方案（例如自我对弈）训练的代理与$\epsilon$-界限的纳什均衡相距甚远。这突显了设计训练方案以限制代理可被利用性的需求。[perolat2022mastering]识别了减少代理可被利用性的方法的三大类：遗憾最小化、遗憾策略梯度方法和最佳响应技术。'
- en: The first category includes approaches that scale counterfactual regret minimization
    (CFR) using deep learning. Deep-CFR [brown2019deep] trains a regret DNN via an
    experience replay buffer containing counterfactual values. A limitation of this
    approach is the sampling method, in that it does not scale to games with a large
    branching factor [perolat2022mastering]. There are model-free regret-based approaches
    which use DNNs that scale to larger games, such as *deep regret minimization with
    advantage baselines and model-free learning* (DREAM) [steinberger2020dream] and
    the advantage regret-matching actor-critic (ARMAC) [gruslys2020advantage]. However,
    these approaches rely on an importance sampling term in order to remain unbiased.
    The importance weights can become very large in games with a long horizon [perolat2022mastering].
    To generalize, these techniques require the generation of an average strategy,
    necessitating either the complete retention of all strategies from previous iterations,
    or an error prone approximation, e.g., a DNN trained via supervised learning [perolat2022mastering].
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类方法包括利用深度学习来扩展反事实遗憾最小化（CFR）的策略。Deep-CFR [brown2019deep]通过包含反事实值的经验重放缓冲区训练一个遗憾DNN。这种方法的一个局限性是采样方法，它无法扩展到具有大分支因子的游戏中 [perolat2022mastering]。有些无模型的遗憾基础方法使用DNN可以扩展到更大的游戏中，如*带优势基线的深度遗憾最小化与无模型学习*（DREAM） [steinberger2020dream]和优势遗憾匹配演员-评论家（ARMAC） [gruslys2020advantage]。然而，这些方法依赖于一个重要性采样项以保持无偏。重要性权重在具有长时间范围的游戏中可能变得非常大 [perolat2022mastering]。一般来说，这些技术需要生成一个平均策略，这就需要完全保留所有来自之前迭代的策略，或是一个容易出错的近似，例如，通过监督学习训练的DNN [perolat2022mastering]。
- en: 'The second category of methods approximates CFR via a weighted policy gradient [srinivasan2018actor,
    perolat2022mastering]. However, the approach is not guaranteed to converge to
    a Nash equilibrium [perolat2022mastering]. In contrast Neural Replicator Dynamics
    (NeuRD) [hennes2019neural], an approach that approximates the Replicator Dynamics
    from evolutionary game theory with a policy gradient, is proven to converge to
    a Nash equilibrium. NeuRD has been applied to large-scale domains, as part of
    *DeepNash* [perolat2022mastering]. It is used to modify the loss function for
    optimizing the Q-function and the policy [perolat2022mastering]. DeepNash was
    recently introduced as a means of tackling the game of Stratego. However, its
    wider applicability is yet to be explored. In the remainder of this section we
    shall therefore focus on the third category of approaches: best response techniques.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类方法通过加权策略梯度来近似CFR [srinivasan2018actor, perolat2022mastering]。然而，这种方法并不能保证收敛到纳什均衡 [perolat2022mastering]。相比之下，神经复制者动态（NeuRD） [hennes2019neural]是一种通过策略梯度近似进化博弈理论中的复制者动态的方法，已被证明可以收敛到纳什均衡。NeuRD已应用于大规模领域，作为*DeepNash*的一部分 [perolat2022mastering]。它用于修改损失函数以优化Q函数和策略 [perolat2022mastering]。DeepNash最近被引入用于应对Stratego游戏。然而，它的更广泛应用仍需进一步探索。因此，在本节剩余部分，我们将重点关注第三类方法：最佳响应技术。
- en: 7.3 Best Response Techniques
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 最佳响应技术
- en: In recent years a number of principled approaches from the MARL literature –
    originally designed for competitive games with a low-dimensional state space –
    have been scaled to *deep* MARL. Population-based and game-theoretic training
    regimes have shown a significant amount of potential [li2023combining]. Early
    work in this area by [heinrich2016deep] scaled *Fictitious Self-Play* for domains
    suffering from the curse-of-dimensionality, resulting in *Neural Fictitious Self-Play*
    (NFSP). This approach approximates extensive-form fictitious play by progressively
    training a best response against the average of all past policies using off-policy
    DRL. A DNN is trained using supervised learning to imitate the average of the
    past best responses.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，来自MARL文献中的一些原则性方法——最初为低维状态空间的竞争游戏设计——已经扩展到*深度* MARL中。基于人群和博弈论的训练方案显示出显著的潜力 [li2023combining]。该领域的早期工作由[heinrich2016deep]提出，将*虚拟自我对弈*扩展到受维度诅咒影响的领域，
    resulting in *神经虚拟自我对弈*（NFSP）。这种方法通过逐步训练一个最佳响应来近似广泛形式的虚拟对弈，对抗所有过去策略的平均值，使用离策略DRL。一个DNN通过监督学习被训练以模仿过去最佳响应的平均值。
- en: '[lanctot2017unified] introduced Policy-Space Response Oracles (PSRO), a generalization
    of the Double-Oracle (DO) approach originally proposed by [mcmahan2003planning],
    a theoretically sound approach for finding a minimax equilibrium. Given the amount
    of interest that this approach has generated within the literature [berner2019dota,
    perolat2022mastering, vinyals2019grandmaster, lanctot2017unified, li2023combining,
    oliehoek2018beyond], we shall dedicate the remainder of this section to DO based
    approaches. First we will discuss the DO approach’s theoretical underpinnings,
    before providing an overview of the work that has been conducted in this area
    in recent years. We shall conclude the section with open challenges, in particular
    with respect to scaling this approach to ACO.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[lanctot2017unified] 引入了政策空间响应神谕（PSRO），这是对最初由 [mcmahan2003planning] 提出的双重神谕（DO）方法的推广，这是一种理论上可靠的寻找极小极大均衡的方法。鉴于该方法在文献中引起的关注
    [berner2019dota, perolat2022mastering, vinyals2019grandmaster, lanctot2017unified,
    li2023combining, oliehoek2018beyond]，我们将把本节剩余部分专门用于基于 DO 的方法。首先，我们将讨论 DO 方法的理论基础，然后概述近年来在该领域所进行的工作。最后，我们将以开放性挑战作为结尾，特别是关于将这种方法扩展到
    ACO 的问题。'
- en: 'The DO algorithm defines a two-player zero-sum normal-form game $\mathcal{N}$,
    where actions correspond to policies available to the players within an underlying
    stochastic game $\mathcal{M}$. Payoff entries within $\mathcal{N}$ are determined
    through computing the gain $\mathcal{G}$ for each policy pair within $\mathcal{M}$:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: DO 算法定义了一个两人零和常规形式游戏 $\mathcal{N}$，其中动作对应于在基础随机游戏 $\mathcal{M}$ 中可供玩家使用的策略。常规形式游戏
    $\mathcal{N}$ 中的回报项通过计算在 $\mathcal{M}$ 中每对策略的增益 $\mathcal{G}$ 来确定：
- en: '|  | $\mathcal{R}^{\mathcal{N}}_{i}(\langle a^{r}_{1},a^{c}_{2}\rangle)=\mathcal{G}^{\mathcal{M}}_{i}(\langle\pi^{r}_{1},\pi^{c}_{2}\rangle).$
    |  | (7) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{R}^{\mathcal{N}}_{i}(\langle a^{r}_{1},a^{c}_{2}\rangle)=\mathcal{G}^{\mathcal{M}}_{i}(\langle\pi^{r}_{1},\pi^{c}_{2}\rangle).$
    |  | (7) |'
- en: 'In Equation [7](#S7.E7 "In 7.3 Best Response Techniques ‣ 7 Adversarial Learning
    ‣ 6.5 Curriculum Learning ‣ 6 Approaches for combinatorial action spaces ‣ 5.4
    Knowledge Retention ‣ 5 Coping with Vast High-Dimensional Inputs ‣ 4 Environments
    ‣ Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey"), $r$
    and $c$ refer to the respective rows and columns inside the normal-form (bimatrix)
    game, and $\mathcal{M}$ is the game where the policies are being evaluated. The
    normal-form game $\mathcal{N}$ is subjected to a game-theoretic analysis, to find
    an optimal mixture over actions for each player. These mixtures represent a probability
    distribution over policies for the game $\mathcal{M}$. The DO algorithm assumes
    that both players have access to a *best response oracle*, returning a *best response* (BR)
    policy against the mixture played by the opponent. BRs are subsequently added
    to the list of available policies for each agent. As a result each player has
    an additional action that it can choose in the normal-form game $\mathcal{N}$.
    Therefore, $\mathcal{N}$ needs to be augmented through computing payoffs for the
    new row and column entries. Upon augmenting $\mathcal{N}$ another game theoretic
    analysis is conducted, and the steps described above are repeated. If no further
    BRs can be found, then the DO algorithm has converged upon a minimax equilibrium [mcmahan2003planning].'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 [7](#S7.E7 "在 7.3 最佳响应技术 ‣ 7 对抗学习 ‣ 6.5 课程学习 ‣ 6 组合动作空间的方法 ‣ 5.4 知识保留 ‣
    5 应对广泛的高维输入 ‣ 4 环境 ‣ 深度强化学习用于自主网络操作：调查") 中，$r$ 和 $c$ 指常规形式（双矩阵）游戏中的相应行和列，$\mathcal{M}$
    是评估策略的游戏。常规形式游戏 $\mathcal{N}$ 经过博弈论分析，以寻找每个玩家的最优行动混合。这些混合代表了游戏 $\mathcal{M}$ 中策略的概率分布。DO
    算法假设两个玩家都可以访问 *最佳响应神谕*，它返回一个对抗对手所玩混合的 *最佳响应*（BR）策略。随后，将 BR 添加到每个代理的可用策略列表中。结果是每个玩家在常规形式游戏
    $\mathcal{N}$ 中有一个额外的动作可以选择。因此，需要通过计算新行和列项的回报来扩展 $\mathcal{N}$。在扩展 $\mathcal{N}$
    后，再次进行博弈论分析，并重复上述步骤。如果找不到更多的 BR，那么 DO 算法已收敛到极小极大均衡 [mcmahan2003planning]。
- en: 'When applying the DO algorithm to MARL the oracles must compute Approximate
    Best Responses (ABRs) against a *mixture of policies*. There are a number of approaches
    for implementing a mixture of policies, e.g., sampling individual policies according
    to their respective mixture probabilities at the beginning of an episode [lanctot2017unified].
    Alternatively, the set of policies can be combined into a weighted-ensemble, where
    the outputs from each policy are weighted by their respective mixture probabilities
    prior to aggregation. For generality, we define a mixture of policies as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在将DO算法应用于MARL时，预言机必须计算对*策略混合*的近似最佳响应（ABRs）。实现策略混合的方法有很多，例如，在一个回合开始时，根据各自的混合概率采样个别策略[lanctot2017unified]。另外，策略集可以组合成一个加权集合，其中每个策略的输出在聚合之前按照各自的混合概率加权。为了通用性，我们将策略混合定义如下：
- en: Definition 7.3  (Mixture of Policies).
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 7.3（策略混合）。
- en: $\pi_{i}^{\mu}$ is a mixture of policies for a mixture $\mu_{i}$ and a corresponding
    set of policies $\Pi_{i}$ for agent $i$.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: $\pi_{i}^{\mu}$ 是混合 $\mu_{i}$ 的策略混合和相应的策略集 $\Pi_{i}$ 对于代理 $i$。
- en: 'Using an oracle to obtain an *exact* best response is often also intractable.
    In games that suffer from the curse-of-dimensionality an oracle can at best hope
    to find an ABR [oliehoek2018beyond]. Here we can apply *Approximate Double-Oracles*
    (ADO), which use linear programming to compute an *approximate mixed-strategy
    NE* $\langle{\mu}_{1},{\mu}_{2}\rangle$ for $\mathcal{N}$, where ${\mu}_{i}$ represents
    a *mixture* over policies $\pi_{i,1..n}$ for player $i$. We therefore require
    a function $O:\Pi_{i}^{\mu}\rightarrow\Pi_{i}$ that computes an ABR $\pi_{i}$
    to a mixture of policies $\pi_{i}^{\mu}$:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预言机获取*准确*的最佳响应通常也是不可行的。在受维度诅咒影响的游戏中，预言机至多只能希望找到一个ABR[oliehoek2018beyond]。在这里，我们可以应用*近似双重预言机*（ADO），它使用线性规划来计算*近似混合策略纳什均衡*
    $\langle{\mu}_{1},{\mu}_{2}\rangle$，其中${\mu}_{i}$表示玩家$i$的*策略混合* $\pi_{i,1..n}$。因此，我们需要一个函数$O:\Pi_{i}^{\mu}\rightarrow\Pi_{i}$，它计算一个针对策略混合$\pi_{i}^{\mu}$的ABR
    $\pi_{i}$：
- en: Definition 7.4  (Approximate Best Response).
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 7.4（近似最佳响应）。
- en: A policy $\pi_{i}\in\Pi_{i}$ of player $i$ is an approximate best response against
    a mixture of policies $\pi_{j}^{\mu}$, *iff*,
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家 $i$ 的策略 $\pi_{i}\in\Pi_{i}$ 是对策略混合 $\pi_{j}^{\mu}$ 的近似最佳响应，*当且仅当*，
- en: '|  | $\forall\pi^{\prime}_{i}\in\Pi_{i},{\mathcal{G}}_{i}(\langle\pi_{i},\pi^{\mu}_{j}\rangle)\geq{\mathcal{G}}_{i}(\langle\pi^{\prime}_{i},\pi_{j}^{\mu}\rangle).$
    |  | (8) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | $\forall\pi^{\prime}_{i}\in\Pi_{i},{\mathcal{G}}_{i}(\langle\pi_{i},\pi^{\mu}_{j}\rangle)\geq{\mathcal{G}}_{i}(\langle\pi^{\prime}_{i},\pi_{j}^{\mu}\rangle).$
    |  | (8) |'
- en: 'ABRs estimate the exploitability $\mathcal{G}_{E}$ of the current mixtures:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ABRs估计当前混合的可利用性$\mathcal{G}_{E}$：
- en: '|  | $\mathcal{G}_{E}\leftarrow{\mathcal{G}}_{i}(\langle{O}_{i}(\pi^{\mu}_{j}),\pi^{\mu}_{j}\rangle)+{\mathcal{G}}_{j}(\langle\pi^{\mu}_{i},{O}_{j}(\pi^{\mu}_{i})\rangle)$
    |  | (9) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{G}_{E}\leftarrow{\mathcal{G}}_{i}(\langle{O}_{i}(\pi^{\mu}_{j}),\pi^{\mu}_{j}\rangle)+{\mathcal{G}}_{j}(\langle\pi^{\mu}_{i},{O}_{j}(\pi^{\mu}_{i})\rangle)$
    |  | (9) |'
- en: 'If $\mathcal{G}_{E}\leq 0$, then the oracle has failed to find an ABR, and
    a *resource bounded Nash equilibrium* (RBNE) has been found [oliehoek2018beyond].
    Resources in this context refers to the amount of computational power available
    for obtaining an ABR:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$\mathcal{G}_{E}\leq 0$，那么预言机未能找到一个ABR，并且已经找到了一个*资源受限的纳什均衡*（RBNE）[oliehoek2018beyond]。在这种情况下，资源指的是用于获取ABR的计算能力：
- en: Definition 7.5  (Resource Bounded Nash Equilibrium).
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 7.5（资源受限的纳什均衡）。
- en: Two mixtures of policies $\langle\pi^{\mu}_{1},\pi^{\mu}_{2}\rangle$ are a resource-bounded
    Nash equilibrium (RBNE) *iff*,
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 两个策略混合 $\langle\pi^{\mu}_{1},\pi^{\mu}_{2}\rangle$ 是一个资源受限的纳什均衡（RBNE）*当且仅当*，
- en: '|  | $\forall_{i}{\mathcal{G}}_{i}(\langle\pi^{\mu}_{i},\pi^{\mu}_{j}\rangle)\geq{\mathcal{G}}_{i}(\langle{O}_{i}(\pi^{\mu}_{j}),\pi^{\mu}_{j}\rangle).$
    |  | (10) |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | $\forall_{i}{\mathcal{G}}_{i}(\langle\pi^{\mu}_{i},\pi^{\mu}_{j}\rangle)\geq{\mathcal{G}}_{i}(\langle{O}_{i}(\pi^{\mu}_{j}),\pi^{\mu}_{j}\rangle).$
    |  | (10) |'
- en: 'Each agents’ oracle is unable to find a response that outperforms the current
    mixture of policies against the opponent’s one. Therefore, an RBNE has been found.
    The ADO approach is outlined in Algorithm [2](#alg2 "Algorithm 2 ‣ 7.3 Best Response
    Techniques ‣ 7 Adversarial Learning ‣ 6.5 Curriculum Learning ‣ 6 Approaches for
    combinatorial action spaces ‣ 5.4 Knowledge Retention ‣ 5 Coping with Vast High-Dimensional
    Inputs ‣ 4 Environments ‣ Deep Reinforcement Learning for Autonomous Cyber Operations:
    A Survey") from the perspective of Blue and Red agents for ACO. The algorithm
    makes use of a number of functions, including i.) InitialStrategies, for providing
    an initial set of strategies for each agent, which can also include rules-based
    and other approaches; ii.) AugmentGame, that computes missing table entries for
    the new resource bounded best responses, and; iii.) SolveGame, for computing mixtures
    once the payoff table has been augmented [oliehoek2018beyond].'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 每个智能体的神谕无法找到比当前策略组合更优的回应。因此，已经找到一个 RBNE。ADO 方法在算法 [2](#alg2 "算法 2 ‣ 7.3 最佳回应技术
    ‣ 7 对抗学习 ‣ 6.5 课程学习 ‣ 6 组合动作空间的方法 ‣ 5.4 知识保留 ‣ 5 应对广泛的高维输入 ‣ 4 环境 ‣ 深度强化学习用于自主网络操作：综述")
    中从蓝方和红方智能体的角度进行了概述。该算法利用了多个函数，包括 i.) InitialStrategies，用于提供每个智能体的初始策略集，可能还包括基于规则和其他方法；ii.)
    AugmentGame，用于计算新资源有界最优回应的缺失表项；iii.) SolveGame，用于在收益表被扩充后计算混合策略 [oliehoek2018beyond]。
- en: Algorithm 2 Approximate Double Oracle Algorithm
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 近似双重神谕算法
- en: 1:$\langle{\pi}_{Blue},{\pi}_{Red}\rangle\leftarrow\textsc{InitialStrategies}()$2:$\langle{\mu}_{Blue},{\mu}_{Red}\rangle\leftarrow\langle\{{\pi}_{Blue}\},\{{\pi}_{Red}\}\rangle$  $\triangleright$
    set initial mixtures3:while True do4:    ${\pi}_{Blue}\leftarrow\textsc{RBBR}({\mu}_{Red})$  $\triangleright$
    get new res. bounded best resp.5:    ${\pi}_{Red}\leftarrow\textsc{RBBR}({\mu}_{Blue})$6:    $\mathcal{G}_{RBBRs}\leftarrow{\mathcal{G}}_{Blue}({\pi}_{Blue},{\mu}_{Red})+{\mathcal{G}}_{Red}({\mu}_{Blue},{\pi}_{Red})$  $\triangleright$
    Exploitability.7:    if $\mathcal{G}_{RBBRs}\leq\epsilon$ then8:         break  $\triangleright$
    found $\epsilon$-RBNE9:    end if10:    $SG\leftarrow\textsc{AugmentGame}(SG,{\pi}_{Blue},{\pi}_{Red})$11:    $\langle{\mu}_{Blue},{\mu}_{Red}\rangle\leftarrow\textsc{SolveGame}(SG)$12:end while
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 1:$\langle{\pi}_{Blue},{\pi}_{Red}\rangle\leftarrow\textsc{InitialStrategies}()$2:$\langle{\mu}_{Blue},{\mu}_{Red}\rangle\leftarrow\langle\{{\pi}_{Blue}\},\{{\pi}_{Red}\}\rangle$  $\triangleright$
    设置初始混合策略3:while True do4:    ${\pi}_{Blue}\leftarrow\textsc{RBBR}({\mu}_{Red})$  $\triangleright$
    获取新的有界最优回应5:    ${\pi}_{Red}\leftarrow\textsc{RBBR}({\mu}_{Blue})$6:    $\mathcal{G}_{RBBRs}\leftarrow{\mathcal{G}}_{Blue}({\pi}_{Blue},{\mu}_{Red})+{\mathcal{G}}_{Red}({\mu}_{Blue},{\pi}_{Red})$  $\triangleright$
    利用度。7:    if $\mathcal{G}_{RBBRs}\leq\epsilon$ then8:         break  $\triangleright$
    找到 $\epsilon$-RBNE9:    end if10:    $SG\leftarrow\textsc{AugmentGame}(SG,{\pi}_{Blue},{\pi}_{Red})$11:    $\langle{\mu}_{Blue},{\mu}_{Red}\rangle\leftarrow\textsc{SolveGame}(SG)$12:end while
- en: 7.4 Towards Scaling Adversarial Learning Approaches
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 向扩展对抗学习方法迈进
- en: 'Versions of ADO have been used at scale, e.g., [vinyals2019grandmaster] achieve
    a grandmaster level performance in StarCraft II, beating top human players using
    AlphaStar, an ADO inspired approach that computes best responses using a match-making
    mechanism that samples strong opponents with an increased probability. Imitation
    learning was utilized to obtain an initial pool of agents. OpenAI Five made use
    of similar mixture of self-play method and a dynamically-updated meta-distribution
    over past policies, beating top human players at Dota [berner2019dota, perolat2022mastering].
    Both achievements, although very impressive, come at a high cost with respect
    to engineering and training time. Indeed, despite their success, questions remain
    regarding the scalability of ADO to complex domains without making concessions
    at the cost of theoretical guarantees. ADO approaches are expensive due to:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ADO 的版本已在大规模应用中使用，例如，[vinyals2019grandmaster] 在《星际争霸 II》中实现了大宗师级别的表现，使用 AlphaStar
    这一受到 ADO 启发的方法，通过一种匹配机制计算最佳回应，并以更高的概率抽样强对手。模仿学习被用来获得初始的智能体池。OpenAI Five 使用了类似的自我对弈方法和动态更新的过去策略元分布，击败了顶级人类玩家
    [berner2019dota, perolat2022mastering]。尽管这两个成就非常令人印象深刻，但在工程和训练时间方面成本很高。确实，尽管取得了成功，但关于
    ADO 在复杂领域的可扩展性而不牺牲理论保证的问题仍然存在。ADO 方法由于以下原因而昂贵：
- en: Computing ABRs in non-trivial environments is a lengthy process [lanctot2017unified].
    Meanwhile, even for simple environments such as Kuhn poker, ADO can require over
    20 ABR iterations to approach the value of the game. For the slightly more complex
    Leduc this number grows to over 200, with further improvements still being obtainable [lanctot2017unified,
    li2023combining]. An even larger number of iterations would likely be necessary
    for ADO to approach $\pi^{*}_{Blue}$ for ACO. Computing ABRs from scratch in each
    iteration would therefore be wasteful, especially if the same skills are learnt
    from scratch in each iteration [liuneupl]. In addition, due to the ABRs being
    resource bounded [oliehoek2018beyond], each agent can end up with a population
    of under-trained policies [liuneupl]. An idealized ADO reuses *relevant* knowledge
    acquired over past iterations.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在非平凡环境中计算ABR是一个漫长的过程 [lanctot2017unified]。与此同时，即使是像库恩扑克这样的简单环境，ADO也可能需要超过20次ABR迭代才能接近游戏的价值。对于稍微复杂的Leduc，这个数字增加到200多次，仍然可以获得进一步的改进 [lanctot2017unified,
    li2023combining]。为了使ADO接近 $\pi^{*}_{Blue}$，可能需要更多的迭代。每次迭代从头计算ABR将是浪费的，特别是如果在每次迭代中都从头学习相同的技能 [liuneupl]。此外，由于ABR受资源限制 [oliehoek2018beyond]，每个代理可能会拥有一个训练不足的策略群体 [liuneupl]。理想的ADO会重用在过去迭代中获得的*相关*知识。
- en: Payoff Matrix Augmentation. The second challenge with respect to scalability
    is the growing normal-form game $\mathcal{N}$. Augmenting the payoff table with
    entries for new best responses takes exponential time with respect to the number
    of policies [lanctot2017unified]. Even for simple games, obtaining a good payoff
    estimate for each ${\mathcal{G}}_{i}(\langle\pi^{r}_{i},\pi^{c}_{j}\rangle)$ can
    require thousands of evaluation episodes. Principled strategies are required for
    keeping the payoff tables to a manageable size.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 收益矩阵扩展。第二个与扩展性相关的挑战是不断增长的标准形式游戏 $\mathcal{N}$。通过在收益表中添加新的最佳回应条目来进行扩展时，所需时间随着策略数量的增加呈指数增长 [lanctot2017unified]。即使是简单的游戏，获取每个
    ${\mathcal{G}}_{i}(\langle\pi^{r}_{i},\pi^{c}_{j}\rangle)$ 的良好收益估计也可能需要数千次评估。需要有原则的策略来保持收益表在可管理的范围内。
- en: Large Policy Supports. There is an overhead for having to store and utilize
    a large number of function approximators – one for each policy in the support [ijcai2019-66].
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模策略支持。存储和使用大量函数近似器的开销较大——每个策略都有一个函数近似器 [ijcai2019-66]。
- en: 'While the above challenges limit the scalability of ADO, there have been efforts
    towards remedying this approach without harming too many of the underlying principles.
    [liuneupl] propose Neural Population Learning (NeuPL) a method that deviates from
    the standard ADO algorithm PSRO in two specific ways: i.) all unique policies
    within the population are trained continuously, and; ii.) uses a single policy
    that contains the entire population of policies, conditioned on an opponent mixture
    identifier. This allows for learning to be transferred across policies without
    a loss of generality. The approach is capable of outperforming PSRO while maintaining
    a population of eight agents on running-with-scissors, which extends the rock-paper-scissors
    game to the spatio-temporal and partially-observed Markov game [vezhnevets2020options].'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述挑战限制了ADO的扩展性，但已有一些努力尝试在不损害太多基本原则的情况下解决这一方法的问题。[liuneupl] 提出了神经群体学习（NeuPL），这是一种在两个具体方面偏离标准ADO算法PSRO的方法：i.)
    群体中的所有独特策略都进行持续训练；ii.) 使用一个包含整个策略群体的单一策略，该策略依赖于对手混合标识符。这使得学习可以在策略之间转移而不会丧失通用性。这种方法能够在运行“剪刀石头布”游戏时超越PSRO，同时保持八个代理的群体，这将剪刀石头布游戏扩展到时空和部分观测马尔可夫游戏 [vezhnevets2020options]。
- en: It is also worth noting that the benefits of PSRO have been explored within
    single agent domains, specifically within the context of robust adversarial reinforcement
    learning (RARL) [pinto2017robust]. For RARL a single agent environment is converted
    into a zero-sum game between the standard agent, the protagonist, and an adversarial
    domain agent, that can manipulate the environment, e.g., through perturbing the
    protagonist’s actions. [yang2022game] recently applied a variant of PSRO to this
    problem, using model agnostic meta learning (MAML) [finn2017model] as the best-response
    oracle. The authors conducted an empirical evaluation on MuJoCo environments,
    finding that the proposed method outperforms state-of-the-art baselines such as
    standard MAML [yang2022game].
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，PSRO的好处已在单代理领域中得到了探索，特别是在鲁棒对抗强化学习（RARL）的背景下[pinto2017robust]。对于RARL，将单代理环境转换为一个零和游戏，其中标准代理（即主角）和一个可以操控环境的对抗域代理之间的零和游戏，例如，通过扰动主角的动作。[yang2022game]最近将PSRO的一个变体应用于这一问题，使用模型无关的元学习（MAML）[finn2017model]作为最佳响应oracle。作者在MuJoCo环境中进行了实证评估，发现所提出的方法优于标准MAML等最先进的基线方法[yang2022game]。
- en: ADO has also been applied to general-sum games, which can have more than one
    Nash equilibrium. This gives rise to the equilibrium selection problem. To remedy
    this, and enable scalable policy evaluation in general, [muller2020generalized]
    apply the $\alpha$-Rank solution concept, which does not face an equilibrium selection
    problem. The $\alpha$-Rank solution concept establishes an ordering over policies
    within the support of each player. Specifically, $\alpha$-Rank uses Markov-Conley
    Chains to identify the presence of cycles in game dynamics, and thereby rank policies.
    The approach attempts to compute stationary distributions by evaluating the strategy
    profiles of $N$ agents through an evolutionary process of mutation and selection.
    [yang2020alphaalpha] reviewed the claim of $\alpha$-ranks tractability. The authors
    find that instead of being a polynomial time implementation, (with respect to
    the total number of pure strategy profiles) solving $\alpha$-Rank is NP-hard.
    The authors introduce $\alpha^{\alpha}$-Rank, a stochastic implementation of $\alpha$-Rank
    that does not require an exponentially-large transitions matrix for ranking policies,
    and can terminate early.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ADO也被应用于一般和游戏中，这些游戏可能有多个纳什均衡。这引发了均衡选择问题。为了补救这一问题，并实现一般的可扩展策略评估，[muller2020generalized]应用了$\alpha$-Rank解概念，该概念不会面临均衡选择问题。$\alpha$-Rank解概念在每个玩家的支持中建立了策略的排序。具体而言，$\alpha$-Rank使用马尔科夫-康利链识别游戏动态中的周期，从而对策略进行排名。该方法通过对$N$个代理的策略配置进行变异和选择的进化过程来计算平稳分布。[yang2020alphaalpha]回顾了$\alpha$-rank的可解性问题。作者发现，与多项式时间实现（相对于纯策略配置的总数）不同，解决$\alpha$-Rank是NP困难的。作者介绍了$\alpha^{\alpha}$-Rank，这是一种$\alpha$-Rank的随机实现，不需要指数级大的转移矩阵来对策略进行排序，并且可以提前终止。
- en: '[feng2021neural] replace the computation of mixtures using linear-programming
    with a Neural Auto-Curricula (NAC), using meta-gradient descent to automate the
    discovery of the learning update rules (the mixtures) without explicit human design.
    The NAC is optimized via interaction with the game engine, where both players
    aim to minimise their exploitability. Even without human design, the discovered
    MARL algorithms achieve competitive or even better performance with the SOTA population-based
    game solvers on a number of benchmarking environments, including: Games of Skill,
    differentiable Lotto, non-transitive Mixture Games, Iterated Matching Pennies,
    and Kuhn Poker. However, the approach does not solve scalability issues with respect
    to the increase in time for augmenting the payoff tables, and the time required
    for computing ABRs.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[feng2021neural] 用神经自动课程（NAC）替代了使用线性规划计算混合物的方法，通过元梯度下降自动发现学习更新规则（即混合物），无需显式的人为设计。NAC通过与游戏引擎的互动进行优化，两个玩家都旨在最小化他们的可利用性。即使没有人为设计，所发现的MARL算法在多个基准环境中表现出与最先进的基于种群的游戏求解器相当甚至更好的性能，包括：技能游戏、可微分的彩票、非传递性混合游戏、迭代匹配便士和库恩扑克。然而，该方法未能解决随着支付表增大而引起的可扩展性问题，以及计算ABRs所需的时间。'
- en: 'Finally, we note that DO based methods have also been utilized for reducing
    the number of actions for an agent to choose from, given a state $s$ in two player
    games. [bakhtin2021no] propose an ADO based algorithm for action exploration and
    equilibrium approximation in games with combinatorial action spaces: Double Oracle
    Reinforcement learning for Action exploration (DORA). This algorithm simultaneously
    performs value iteration while learning a policy proposal network. An ADO step
    is used to explore additional actions to add to the policy proposals. The authors
    show that their approach achieves superhuman performance on a two-player variant
    of the board game Diplomacy.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们注意到 DO 基于的方法也被用于减少在两个玩家游戏中，给定状态 $s$ 时，代理可以选择的动作数量。[bakhtin2021no] 提出了一个基于
    ADO 的算法，用于在具有组合动作空间的游戏中进行动作探索和均衡逼近：动作探索的双重 oracle 强化学习 (DORA)。该算法同时执行值迭代，同时学习策略提议网络。ADO
    步骤用于探索额外的动作以添加到策略提议中。作者展示了他们的方法在两个玩家变体的棋盘游戏 Diplomacy 中达到了超人的表现。
- en: 8 Discussion and Open Questions
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论与未解问题
- en: The previous sections provide a comprehensive overview of the plethora of methods
    for addressing high-dimensional states, large combinatorial action spaces and
    reducing the exploitability of DRL agents. We shall now consider how the lessons
    learned from this process can help us formulate an idealised learner for ACO,
    and distill open research questions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的部分提供了有关处理高维状态、大组合动作空间和减少 DRL 代理可利用性的各种方法的全面概述。我们现在将考虑从这一过程中学到的经验如何帮助我们制定理想化的
    ACO 学习者，并提炼出未解的研究问题。
- en: 'The need for improved evaluation metrics: After reading \autorefsec:adv_learning,
    it should be evident that success achieved against a stationary opponent should
    be enjoyed with caution. For example, for the CAGE challenges [cage_challenge_announcement,
    cage_challenge_2_announcement, cage_challenge_3_announcement], the evaluation
    process to-date has consisted of evaluating the submitted approach against rules-based
    Red agents with different trial lengths. This formulation is concerning. Solutions
    that overfit on the provided Red agents are likely to perform best. To address
    this shortcoming we advocate for *exploitability* being used as a standard evaluation
    metric, given that it is widely used by the adversarial learning community [lanctot2017unified,
    oliehoek2018beyond, heinrich2016deep], raising the research question:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 对改进评估指标的需求：阅读了 \autorefsec:adv_learning 后，应该很明显地看到，对静态对手取得的成功应当谨慎对待。例如，对于 CAGE
    挑战 [cage_challenge_announcement, cage_challenge_2_announcement, cage_challenge_3_announcement]，迄今为止的评估过程是将提交的方法与基于规则的
    Red 代理进行对比，测试不同的试验长度。这种方案令人担忧。那些在提供的 Red 代理上过度拟合的解决方案可能表现最佳。为了解决这一缺陷，我们提倡将 *可利用性*
    作为标准评估指标，因为它被对抗学习社区广泛使用 [lanctot2017unified, oliehoek2018beyond, heinrich2016deep]，从而提出研究问题：
- en: 'Q1:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Q1：
- en: How exploitable are current approaches for autonomous cyber operations?
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的自主网络操作方法有多可利用？
- en: 'The need for improved evaluation environments: In \autorefsec:envs, it is identified
    that the challenges which would be present in an idealised ACO training environment
    are not currently implemented in CybORG or YAWNING TITAN. While environments are
    available to benchmark subsets of these challenges, no environment can be used
    to benchmark all of them. In particular, although *RecSim* and *Micro-RTS* are
    available to benchmark high-dimensional observation and action spaces, respectively,
    no environment is available to pose these challenges alongside graph-based dynamics.
    Practitioners aiming to develop methodologies which rely on graph constructs specifically
    have several avenues for environment development, with two open-source but difficult
    to modify ACO environments being available, alongside frameworks such as *Gym-ANM*
    which are designed to be built upon. As such, the following research (and development)
    questions arise:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对改进评估环境的需求：在 \autorefsec:envs 中指出，理想化 ACO 训练环境中存在的挑战在 CybORG 或 YAWNING TITAN
    中尚未实现。虽然有环境可以用来基准测试这些挑战的子集，但没有环境可以用来基准测试所有挑战。特别是，尽管 *RecSim* 和 *Micro-RTS* 可用于基准测试高维观察和行动空间，但没有环境可以在基于图的动态下提出这些挑战。那些希望开发依赖于图结构的方法的从业者有几种环境开发途径，包括两个开源但难以修改的
    ACO 环境，以及旨在进行扩展的框架如 *Gym-ANM*。因此，出现了以下研究（和开发）问题：
- en: 'Q2:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Q2：
- en: Will methodologies developed on environments such as *RecSim* and *Micro-RTS*
    translate well to ACO?
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在像*RecSim*和*Micro-RTS*这样的环境中开发的方法能否很好地迁移到ACO？
- en: 'Q3:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q3:'
- en: Where should environment developers seeking to build towards a more idealised
    graph-based ACO-like environment focus their efforts?
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 环境开发人员在致力于构建更理想化的图形化ACO（自适应控制优化）环境时，应该将精力集中在哪里？
- en: 'Limiting the exploitability of cyber defence agents: Handcrafting world-beating
    rules-based agents that find the value of the game is non-trivial, even for simple
    games [vincent2021top]. Therefore, our thoughts turn towards adversarial learning
    approaches for ACO. We have seen that best response techniques have a significant
    amount of potential. However, while computing an (approximate) best response to
    measure exploitability is a reasonable expenditure, best response techniques require
    this in *every* iteration. As a result the literature on adversarial learning
    often focuses on simple games for benchmarking such as Kuhn and Leduc poker, where
    the value of the game is known [lanctot2019openspiel, feng2021neural, li2023combining,
    lanctot2017unified]. Meanwhile, methods that we have identified as suitable for
    ACO can require lengthy training times. This raises the following research questions:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 限制网络防御代理的可利用性：手工制作能击败世界的基于规则的代理并找出游戏价值并非易事，即使是对于简单的游戏[vincent2021top]。因此，我们的思路转向了对抗性学习方法。我们已经看到最佳响应技术具有显著的潜力。然而，虽然计算（近似）最佳响应以衡量可利用性是合理的支出，但最佳响应技术需要在*每次*迭代中进行。因此，对抗性学习的文献通常集中在像Kuhn和Leduc扑克这样的简单游戏上进行基准测试，其中游戏的价值是已知的[lanctot2019openspiel,
    feng2021neural, li2023combining, lanctot2017unified]。与此同时，我们认为适合ACO的方法可能需要较长的训练时间。这引发了以下研究问题：
- en: 'Q4:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q4:'
- en: Can we implement an *efficient* best response framework for cyber defence?
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否为网络防御实现一个*高效*的最佳响应框架？
- en: 'Q5:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q5:'
- en: What is the value of the game for non-trivial cyber defence configurations?
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 非平凡的网络防御配置的游戏价值是什么？
- en: 'Efficient approximate best response oracles: While RQ4 focuses on mechanisms
    within the approximate best response framework to reduce training time (e.g.,
    through knowledge reuse [liuneupl]), the endeavour is underpinned by the need
    for methods that themselves can be trained efficiently. However, this is not a
    property one would associate with the methods discussed in sections [5](#S5 "5
    Coping with Vast High-Dimensional Inputs ‣ 4 Environments ‣ Deep Reinforcement
    Learning for Autonomous Cyber Operations: A Survey") and [6](#S6 "6 Approaches
    for combinatorial action spaces ‣ 5.4 Knowledge Retention ‣ 5 Coping with Vast
    High-Dimensional Inputs ‣ 4 Environments ‣ Deep Reinforcement Learning for Autonomous
    Cyber Operations: A Survey"), raising the question:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '高效的近似最佳响应预言机：虽然RQ4专注于在近似最佳响应框架内减少训练时间的机制（例如，通过知识重用[liuneupl]），但这一努力的基础在于需要可以高效训练的方法。然而，这不是我们在[5](#S5
    "5 Coping with Vast High-Dimensional Inputs ‣ 4 Environments ‣ Deep Reinforcement
    Learning for Autonomous Cyber Operations: A Survey")和[6](#S6 "6 Approaches for
    combinatorial action spaces ‣ 5.4 Knowledge Retention ‣ 5 Coping with Vast High-Dimensional
    Inputs ‣ 4 Environments ‣ Deep Reinforcement Learning for Autonomous Cyber Operations:
    A Survey")中讨论的方法所具备的特性，这引发了以下问题：'
- en: 'Q6:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q6:'
- en: Can we implement *wall time efficient* best response oracles for ACO?
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否为ACO实现*墙时高效*的最佳响应预言机？
- en: 9 Conclusions
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: In this work we surveyed the DRL literature to formulate an idealised learner
    for autonomous cyber operations. This idealised learner sits at the intersection
    of three active areas of research, namely environments that confront learners
    with the curse of dimensionality, with respect to both state and action spaces,
    and adversarial learning. While significant efforts have been made on each of
    these topics individually, each still remains an active area of research.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们调查了DRL（深度强化学习）文献，以制定一个用于自主网络操作的理想化学习者。这个理想化学习者处于三个活跃研究领域的交汇点，即那些在状态和动作空间中都面临维度诅咒的环境，以及对抗性学习。尽管在每个主题上都进行了大量研究，但每个领域仍然是一个活跃的研究方向。
- en: 'While SOTA DNNs have allowed RL approaches to be scaled to domains that were
    previously considered high-dimensional [mnih2013playing, mnih2015human], in this
    survey we provide an overview of solutions for environments that push standard
    DRL approaches to their limits. In sections [5](#S5 "5 Coping with Vast High-Dimensional
    Inputs ‣ 4 Environments ‣ Deep Reinforcement Learning for Autonomous Cyber Operations:
    A Survey") – [7](#S7 "7 Adversarial Learning ‣ 6.5 Curriculum Learning ‣ 6 Approaches
    for combinatorial action spaces ‣ 5.4 Knowledge Retention ‣ 5 Coping with Vast
    High-Dimensional Inputs ‣ 4 Environments ‣ Deep Reinforcement Learning for Autonomous
    Cyber Operations: A Survey") we identify components for the implementation and
    evaluation of an idealised learning agent for ACO, and in \autorefsec:challenges
    we discuss both theoretical and engineering challenges that need to be overcome
    in-order to: implement our idealised agent, and; train it at scale. We hope that
    this survey will raise awareness regarding issues that need to be solved in-order
    for DRL to be scalable to challenging cyber defence scenarios, and that our work
    will inspire readers to attempt to answer the research questions we have distilled
    from the literature.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管 SOTA DNNs 使 RL 方法能够扩展到以前被认为是高维的领域 [mnih2013playing, mnih2015human]，但在本综述中，我们提供了对那些将标准
    DRL 方法推向极限的环境解决方案的概述。在章节 [5](#S5 "5 Coping with Vast High-Dimensional Inputs ‣
    4 Environments ‣ Deep Reinforcement Learning for Autonomous Cyber Operations:
    A Survey") – [7](#S7 "7 Adversarial Learning ‣ 6.5 Curriculum Learning ‣ 6 Approaches
    for combinatorial action spaces ‣ 5.4 Knowledge Retention ‣ 5 Coping with Vast
    High-Dimensional Inputs ‣ 4 Environments ‣ Deep Reinforcement Learning for Autonomous
    Cyber Operations: A Survey") 中，我们识别了实现和评估 ACO 理想化学习代理的组件，并在 \autorefsec:challenges
    中讨论了需要克服的理论和工程挑战，以：实现我们的理想化代理，并；大规模训练它。我们希望这项综述能提高对需要解决问题的认识，以使 DRL 能够扩展到具有挑战性的网络防御场景，并希望我们的工作能激励读者尝试回答我们从文献中提炼出的研究问题。'
- en: 10 Acknowledgements
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 致谢
- en: Research funded by Frazer-Nash Consultancy Ltd. on behalf of the Defence Science
    and Technology Laboratory (Dstl) which is an executive agency of the UK Ministry
    of Defence providing world class expertise and delivering cutting-edge science
    and technology for the benefit of the nation and allies. The research supports
    the Autonomous Resilient Cyber Defence (ARCD) project within the Dstl Cyber Defence
    Enhancement programme.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究由 Frazer-Nash Consultancy Ltd. 代表国防科学与技术实验室（Dstl）资助，Dstl 是英国国防部的一个执行机构，提供世界级的专业知识，交付最前沿的科学和技术，以造福国家和盟友。该研究支持
    Dstl 网络防御增强计划中的自主弹性网络防御（ARCD）项目。
- en: 11 License
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11 许可证
- en: This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives
    4.0 International License. To view a copy of this license, visit https://creativecommons.org/licenses/by-nc-nd/4.0/
    or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作许可采用 Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 国际许可证。要查看此许可证的副本，请访问
    https://creativecommons.org/licenses/by-nc-nd/4.0/ 或将信函发送至 Creative Commons, PO
    Box 1866, Mountain View, CA 94042, USA。
- en: '{appendices}'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '{附录}'
- en: 12 Deep Reinforcement Learning
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12 深度强化学习
- en: 12.1 Deep Q-Learning
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1 深度 Q 学习
- en: In DRL multi-layer Deep Neural Networks (DNNs) are used to approximate value
    functions or, as we shall see below, even learn a parameterized policy. We shall
    assume a DNN with parameters $\theta$ and sufficient representational capacity
    to learn our required approximations. For Deep Q-learning (DQN), DNNs map a set
    of $n$-dimensional state variables to a set of $m$-dimensional Q-values $f:\mathbb{R}^{n}\to\mathbb{R}^{m}$ [mnih2013playing,
    mnih2015human]. Here, $m$ represents the number of actions available to the agent.
    The parameters $\theta$ are optimized via stochastic gradient descent, by randomly
    sampling past transitions stored within an experience replay memory $\mathcal{D}$
    [lin1992self, mnih2015human, schaul2015prioritized, mousavi2016deep]. The DNN
    is trained to minimize the time dependent loss function,
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DRL 中，多层深度神经网络（DNNs）用于近似价值函数，或者如下面所示，甚至学习一个参数化的策略。我们假设一个具有参数 $\theta$ 并具有足够表示能力的
    DNN，以学习我们所需的近似。对于深度 Q 学习（DQN），DNNs 将一组 $n$ 维状态变量映射到一组 $m$ 维 Q 值 $f:\mathbb{R}^{n}\to\mathbb{R}^{m}$
    [mnih2013playing, mnih2015human]。这里，$m$ 代表代理可以执行的动作数量。参数 $\theta$ 通过随机梯度下降进行优化，方法是随机抽取存储在经验回放记忆
    $\mathcal{D}$ 中的过去过渡 [lin1992self, mnih2015human, schaul2015prioritized, mousavi2016deep]。DNN
    被训练以最小化时间依赖的损失函数，
- en: '|  | $L_{k}\left(\theta_{k}\right)=\mathbb{E}_{(s,a,s^{\prime},r)\sim U(\mathcal{D})}\Big{[}\left(Y_{k}-Q\left(s,a;\theta_{k}\right)\right)^{2}\Big{]},$
    |  | (11) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{k}\left(\theta_{k}\right)=\mathbb{E}_{(s,a,s^{\prime},r)\sim U(\mathcal{D})}\Big{[}\left(Y_{k}-Q\left(s,a;\theta_{k}\right)\right)^{2}\Big{]},$
    |  | (11) |'
- en: 'where $(s,a,s^{\prime},r)\sim U(\mathcal{D})$ represents mini-batches of experiences
    drawn uniformly at random from $\mathcal{D}$, $t$ the current iteration, and $Y_{k}$
    is the target:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(s,a,s^{\prime},r)\sim U(\mathcal{D})$ 代表从 $\mathcal{D}$ 中均匀随机抽取的经验小批量，$t$
    是当前迭代，$Y_{k}$ 是目标：
- en: '|  | $Y_{k}\equiv r+\gamma Q(s^{\prime},\operatorname*{argmax}_{a\in\mathcal{U}}Q(s^{\prime},a;\theta_{k});\theta_{k}^{\prime}).$
    |  | (12) |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | $Y_{k}\equiv r+\gamma Q(s^{\prime},\operatorname*{argmax}_{a\in\mathcal{U}}Q(s^{\prime},a;\theta_{k});\theta_{k}^{\prime}).$
    |  | (12) |'
- en: 'Equation ([12](#S12.E12 "In 12.1 Deep Q-Learning ‣ 12 Deep Reinforcement Learning
    ‣ 11 License ‣ 10 Acknowledgements ‣ 9 Conclusions ‣ 8 Discussion and Open Questions
    ‣ 7.4 Towards Scaling Adversarial Learning Approaches ‣ 7 Adversarial Learning
    ‣ 6.5 Curriculum Learning ‣ 6 Approaches for combinatorial action spaces ‣ 5.4
    Knowledge Retention ‣ 5 Coping with Vast High-Dimensional Inputs ‣ 4 Environments
    ‣ Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey")) uses
    Double Deep Q-learning (DDQN) [hasselt2010double], where the target action is
    selected using weights $\theta$, while the target value is computed using weights
    $\theta^{\prime}$ from a target network. Using the current network rather than
    the target network for the target value estimation decouples action selection
    and evaluation for more stable updates. Weights are copied from the current to
    the target network after every $n$ transitions [van2016deep] or via a soft target
    update  [lillicrap2015continuous]. DDQNs have been shown to reduce overoptimistic
    value estimates [van2016deep].'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 ([12](#S12.E12 "在 12.1 深度 Q 学习 ‣ 12 深度强化学习 ‣ 11 许可 ‣ 10 致谢 ‣ 9 结论 ‣ 8 讨论与开放问题
    ‣ 7.4 向对抗学习方法的扩展 ‣ 7 对抗学习 ‣ 6.5 课程学习 ‣ 6 组合动作空间方法 ‣ 5.4 知识保留 ‣ 5 处理广泛高维输入 ‣ 4
    环境 ‣ 自主网络操作的深度强化学习：调查")) 使用了双重深度 Q 学习（DDQN）[hasselt2010double]，其中目标动作是使用权重 $\theta$
    选择的，而目标值则使用来自目标网络的权重 $\theta^{\prime}$ 计算。使用当前网络而不是目标网络进行目标值估计可以将动作选择与评估解耦，以实现更稳定的更新。权重在每
    $n$ 次过渡后从当前网络复制到目标网络 [van2016deep]，或者通过软目标更新 [lillicrap2015continuous]。DDQNs 已被证明能够减少过于乐观的价值估计
    [van2016deep]。
- en: 12.2 Proximal Policy Optimisation
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2 近端策略优化
- en: 'While laying important foundations, DQNs tend to be outperformed by SOTA policy
    gradient (PG) methods, such as *Proximal Policy Optimization* (PPO) [schulman2017proximal].
    PG approaches optimize a parameterized policy $\pi_{\theta}$ directly. The objective
    is to find an optimal policy $\pi^{*}_{\theta}$ with respect to the parameters
    $\theta$, which maximizes the expected return $J(\theta)=\mathbb{E}_{x_{i}\sim
    p_{\pi},a_{i}\sim\pi}[\mathcal{R}_{0}]$. PPO gathers $(s_{t},a_{t},r_{t},s_{t+1},a_{t+1},r_{t+1},...,s_{t+n},a_{t+n},r_{t+n})$
    trajectories by interacting with the environment, potentially over multiple instances
    of the environment using multiple actors synchronously or asynchronously [mnih2016asynchronous].
    The collected samples are then used to update the policy for a number of epochs,
    upon which the collected data is discarded. Then, a new iteration of collecting
    data for updating the policy begins, making the algorithm *on policy*. The policy
    is optimized through estimating the PG and subsequently running a stochastic gradient
    ascent algorithm:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在奠定重要基础的同时，DQNs 往往不如*最新的政策梯度*（PG）方法表现出色，例如*近端策略优化*（PPO）[schulman2017proximal]。PG
    方法直接优化参数化策略 $\pi_{\theta}$。其目标是找到一个关于参数 $\theta$ 的最优策略 $\pi^{*}_{\theta}$，从而最大化期望回报
    $J(\theta)=\mathbb{E}_{x_{i}\sim p_{\pi},a_{i}\sim\pi}[\mathcal{R}_{0}]$。PPO 通过与环境交互来收集
    $(s_{t},a_{t},r_{t},s_{t+1},a_{t+1},r_{t+1},...,s_{t+n},a_{t+n},r_{t+n})$ 轨迹，可能在多个环境实例上使用多个演员同步或异步地进行
    [mnih2016asynchronous]。收集到的样本随后用于在若干个迭代周期中更新策略，更新后数据会被丢弃。然后，开始新的数据收集和策略更新迭代，使算法变得*符合策略*。通过估计
    PG 并随后运行随机梯度上升算法来优化策略：
- en: '|  | $\hat{g}=\mathbb{\hat{E}}_{t}\Big{[}\nabla_{\theta}\log\pi_{\theta}(a_{t}\rvert
    s_{t})\hat{A}_{t}\Big{]}.$ |  | (13) |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{g}=\mathbb{\hat{E}}_{t}\Big{[}\nabla_{\theta}\log\pi_{\theta}(a_{t}\rvert
    s_{t})\hat{A}_{t}\Big{]}.$ |  | (13) |'
- en: 'In the above equation $\hat{A}_{t}$ is an estimate of the advantage function
    at time step $t$. The advantage function is computed using a learned state-value
    function $V(s)$, resulting in an actor-critic style architecture. The expectation
    $\mathbb{\hat{E}}_{t}[...]$ represents the fact that an empirical average over
    a finite batch of samples is used for computing the gradient. Therefore, as mentioned
    above, we assume an algorithm that alternates between sampling and updating the
    policy parameters $\theta$:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，$\hat{A}_{t}$ 是时间步$t$的优势函数估计。优势函数使用学习的状态值函数$V(s)$来计算，结果是一个演员-评论员风格的架构。期望值$\mathbb{\hat{E}}_{t}[...]$
    表示计算梯度时使用了有限样本批次的经验平均。因此，如上所述，我们假设一个在采样和更新策略参数$\theta$之间交替进行的算法：
- en: '|  | $L^{PG}(\theta)=\mathbb{\hat{E}}_{t}\Big{[}\log\pi_{\theta}(a_{t}\rvert
    s_{t})\hat{A}_{t}\Big{]}.$ |  | (14) |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  | $L^{PG}(\theta)=\mathbb{\hat{E}}_{t}\Big{[}\log\pi_{\theta}(a_{t}\rvert
    s_{t})\hat{A}_{t}\Big{]}.$ |  | (14) |'
- en: Performing multiple optimization steps on this loss, i.e., using the same trajectory,
    can lead to destructively large policy updates. To address this, PPO uses a a
    clipped surrogate loss function that effectively constrain the size of each update,
    allowing multiple epochs of mini-batch updates to be performed on gathered data.
    Given a probability ratio
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 对该损失进行多个优化步骤，即使用相同的轨迹，可能导致破坏性的大规模策略更新。为了解决这个问题，PPO使用了一个剪切的替代损失函数，这有效地限制了每次更新的大小，从而允许对收集到的数据进行多个小批量更新。给定概率比
- en: '|  | ${}_{t}(\theta)=\dfrac{\pi_{\theta}(a_{t}\rvert s_{t})}{\pi_{\theta_{old}}(a_{t}\rvert
    s_{t})},$ |  | (15) |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | ${}_{t}(\theta)=\dfrac{\pi_{\theta}(a_{t}\rvert s_{t})}{\pi_{\theta_{old}}(a_{t}\rvert
    s_{t})},$ |  | (15) |'
- en: 'PPO prevents $\theta$ from moving too far away from $\theta_{old}$:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: PPO防止$\theta$偏离$\theta_{old}$过远：
- en: '|  | $L^{CLIP}(\theta)=\mathbb{\hat{E}}_{t}\Big{[}min({}_{t}(\theta)\hat{A}_{t},clip({}_{t}(\theta),1-\varepsilon,1+\varepsilon)\hat{A}_{t})\Big{]}.$
    |  | (16) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  | $L^{CLIP}(\theta)=\mathbb{\hat{E}}_{t}\Big{[}min({}_{t}(\theta)\hat{A}_{t},clip({}_{t}(\theta),1-\varepsilon,1+\varepsilon)\hat{A}_{t})\Big{]}.$
    |  | (16) |'
- en: 12.3 Deep Deterministic Policy Gradients
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3 深度确定性策略梯度
- en: 'For continuous control problems, the policy can also be updated by taking the
    gradient of the expected return. A popular actor-critic method is Deep Deterministic
    Policy Gradients (DDPG) [lillicrap2015continuous] which uses many of the same
    fundamental principles of DQN but applied to continuous control problems. An actor
    $\mu(s)$ with network parameters $\phi$ is used for action selection, while the
    critic $Q(s,a)$ with network parameters $\theta$ provides the value function estimate.
    The critic is updated using temporal difference learning as with DQN in \autorefeq:DQNTarget,
    but the target value is estimated using the target actor and critic networks:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续控制问题，策略也可以通过计算期望回报的梯度来更新。一种流行的演员-评论员方法是深度确定性策略梯度（DDPG）[ lillicrap2015continuous
    ]，它使用了许多与DQN相同的基本原则，但应用于连续控制问题。一个具有网络参数$\phi$的演员$\mu(s)$用于动作选择，而一个具有网络参数$\theta$的评论员$Q(s,a)$提供值函数估计。评论员使用时间差分学习进行更新，类似于DQN中的\autorefeq:DQNTarget，但目标值是通过目标演员和评论员网络估计的：
- en: '|  | $Y_{k}=r+\gamma\hat{Q}_{\hat{\theta}}\left(s^{\prime},\hat{\mu}_{\hat{\phi}}(s^{\prime})\right)$
    |  | (17) |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  | $Y_{k}=r+\gamma\hat{Q}_{\hat{\theta}}\left(s^{\prime},\hat{\mu}_{\hat{\phi}}(s^{\prime})\right)$
    |  | (17) |'
- en: 'The actor network is then updated through gradient ascent using the deterministic
    policy gradient algorithm [silver2014deterministic]:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过使用确定性策略梯度算法[ silver2014deterministic ]对演员网络进行梯度上升更新：
- en: '|  | $\nabla_{\phi}J_{k}(\phi)=\mathbb{E}\left[\nabla_{a}Q_{\theta}(s,a)\rvert_{a=\mu(s)}\nabla_{\phi}\mu_{\phi}(s)\right]$
    |  | (18) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{\phi}J_{k}(\phi)=\mathbb{E}\left[\nabla_{a}Q_{\theta}(s,a)\rvert_{a=\mu(s)}\nabla_{\phi}\mu_{\phi}(s)\right]$
    |  | (18) |'
- en: The target actor and critic networks are then updated using soft target updates.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用软目标更新更新目标演员和评论员网络。
- en: 13 High-Dimensional State Space Approaches Overview
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13 高维状态空间方法概述
- en: '| Papers that are relevant with regard to the high-dimensional state spaces
    |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 与高维状态空间相关的论文 |'
- en: '| --- |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Work | Summary | Abstr. | Expl. | CF |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | 摘要 | 抽象 | 解释 | CF |'
- en: '| [pmlr-v80-abel18a] | Authors introduce two classes of abstractions: *transitive*
    and *PAC* state abstractions, and show that transitive PAC abstractions can be
    acquired efficiently, preserve near optimal-behavior, and experimentally reduce
    sample complexity in *simple domains*. | ✓ | X | X |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| [pmlr-v80-abel18a] | 作者引入了两类抽象：*传递*和*PAC*状态抽象，并展示了传递PAC抽象可以有效获取，保持接近最优行为，并在*简单领域*中实验性地减少样本复杂性。
    | ✓ | X | X |'
- en: '| [abel2016near] | Authors investigate approximate state abstractions, Present
    theoretical guarantees of the quality of behaviors derived from four types of
    approximate abstractions. Empirically demonstrate that approximate abstractions
    lead to reduction in task complexity and bounded loss of optimality of behavior
    in a variety of environments. | ✓ | ✓ | X |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| [abel2016near] | 作者研究了近似状态抽象，提出了从四种类型的近似抽象中获得行为质量的理论保证。通过实证证明，近似抽象能够减少任务复杂性，并在多种环境中限定行为的最优性损失。
    | ✓ | ✓ | X |'
- en: '| [abel2019state] | Seek to understand the role of information-theoretic compression
    in state abstraction for sequential decision making, resulting in a novel objective
    function. | ✓ | ✓ | X |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| [abel2019state] | 旨在理解信息论压缩在顺序决策中的状态抽象中的作用，结果得到了一种新的目标函数。 | ✓ | ✓ | X |'
- en: '| [pmlr-v108-abel20a] | Combine state abstractions and options to preserve
    the representation of near-optimal policies. | ✓ | ✓ | X |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| [pmlr-v108-abel20a] | 结合状态抽象和选项，以保留近似最优策略的表示。 | ✓ | ✓ | X |'
- en: '| [atkinson2021pseudo] | A generative network is used to generate short sequences
    from previous tasks for the DQN to train on, in order to prevent catastrophic
    forgetting as the new task is transferred. | X | X | ✓ |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| [atkinson2021pseudo] | 使用生成网络从先前的任务中生成短序列，以供 DQN 训练，从而防止新任务转移时的灾难性遗忘。 | X
    | X | ✓ |'
- en: '| [badianever] | Construct an episodic memory-based intrinsic reward using
    k-nearest neighbours over recent experiences. Encourage the agent to repeatedly
    revisit all states in its environment. | X | ✓ | ✓ |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| [badianever] | 使用 k 最近邻构建基于情节记忆的内在奖励，鼓励代理重复访问环境中的所有状态。 | X | ✓ | ✓ |'
- en: '| [bellemare2016unifying] | Use Pseudo-Counts to count salient events, derived
    from the log-probability improvement according to a *sequential density model*
    over the state space. | ✓ | ✓ | X |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| [bellemare2016unifying] | 使用伪计数来计数显著事件，基于状态空间上的*序列密度模型*从对数概率改进中获得。 | ✓ |
    ✓ | X |'
- en: '| [bougie2021fast] | Introduce the concept of fast and slow curiosity that
    aims to incentivise long-time horizon exploration. Method decomposes the curiosity
    bonus into a fast reward that deals with local exploration and a slow reward that
    encourages global exploration. | X | ✓ | X |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| [bougie2021fast] | 引入快速和慢速好奇心的概念，旨在激励长期的探索。该方法将好奇心奖励分解为处理局部探索的快速奖励和鼓励全局探索的慢速奖励。
    | X | ✓ | X |'
- en: '| [burdaexploration] | Introduce a method to flexibly combine intrinsic and
    extrinsic rewards via a *random network distillation* (RND) bonus enabling significant
    progress on several hard exploration Atari games. | X | ✓ | X |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| [burdaexploration] | 引入一种通过*随机网络蒸馏*（RND）奖金灵活结合内在和外在奖励的方法，从而在多个困难的 Atari 游戏中取得显著进展。
    | X | ✓ | X |'
- en: '| [burden2018using] | Automate the generation of Abstract Markov Decision Processes
    (AMDPs) using uniform state abstractions. Explores the effectiveness and efficiency
    of different resolutions of state abstractions. | ✓ | ✓ | X |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| [burden2018using] | 使用统一状态抽象自动生成抽象马尔可夫决策过程（AMDPs）。探讨不同分辨率的状态抽象的有效性和效率。 |
    ✓ | ✓ | X |'
- en: '| [burden2021latent] | Introduce Latent Property State Abstraction, for the
    full automation of creating and solving an AMDP, and apply potential function
    for potential based reward shaping. | ✓ | ✓ | X |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| [burden2021latent] | 引入潜在属性状态抽象，实现 AMDP 的完全自动化创建和解决，并应用潜在函数进行基于潜在的奖励塑造。 |
    ✓ | ✓ | X |'
- en: '| [pmlr-v97-gelada19a] | Introduce DeepMDP, where the $\ell_{2}$ distance represents
    an upper bound of the bisimulation distance, learning embeddings that ignore irrelevant
    objects that are of no consequence to the learning agent(s). | ✓ | ✓ | X |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| [pmlr-v97-gelada19a] | 引入 DeepMDP，其中 $\ell_{2}$ 距离表示双模拟距离的上界，学习嵌入以忽略对学习代理无关的对象。
    | ✓ | ✓ | X |'
- en: '| [colas2019curious] | Proposes CURIOUS, an algorithm that leverages a modular
    Universal Value Function Approximator with hindsight learning to achieve a diversity
    of goals of different kinds within a unique policy and an automated curriculum
    learning mechanism that biases the attention of the agent towards goals maximizing
    the absolute learning progress. Also focuses on goals that are being forgotten.
    | X | ✓ | ✓ |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| [colas2019curious] | 提出 CURIOUS，这是一种利用模块化的通用价值函数近似器和事后学习的算法，旨在在独特的策略中实现多样化的目标，并具备自动化课程学习机制，倾向于使代理的注意力集中在最大化绝对学习进展的目标上。同时关注被遗忘的目标。
    | X | ✓ | ✓ |'
- en: '| [de2015importance] | Study the extent to which experience replay memory composition
    can mitigate catastrophic forgetting. | X | X | ✓ |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| [de2015importance] | 研究经验重放记忆组成在多大程度上可以缓解灾难性遗忘。 | X | X | ✓ |'
- en: '| [precup2000temporal] | Introduction of the *options* framework, for prediction,
    control and learning at multiple timescales. ⁶⁶6A substantial amount of follow-on
    work exists from this work. | ✓ | ✓ | $\nearrow$ |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| [precup2000temporal] | 引入*选项*框架，用于在多个时间尺度下进行预测、控制和学习。⁶⁶6 从这项工作中存在大量后续研究。
    | ✓ | ✓ | $\nearrow$ |'
- en: '| [fangadaptive] | Introduce Adaptive Procedural Task Generation (APT-Gen),
    an approach to progressively generate a sequence of tasks as curricula to facilitate
    reinforcement learning in hard-exploration problems. | X | ✓ | X |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| [fangadaptive] | 介绍了自适应过程任务生成（APT-Gen），一种逐步生成任务序列作为课程的方法，以促进在困难探索问题中的强化学习。
    | X | ✓ | X |'
- en: '| [forestier2017intrinsically] | Introduce an intrinsically motivated goal
    exploration processes with automatic curriculum learning. | ✓ | ✓ | X |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| [forestier2017intrinsically] | 介绍了一种具有自动课程学习的内在动机目标探索过程。 | ✓ | ✓ | X |'
- en: '| [fu2017ex2] | Propose a novelty detection algorithm for exploration. Classifiers
    are trained to discriminate each visited state against all others, where novel
    states are easier to distinguish. | X | ✓ | X |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| [fu2017ex2] | 提出了一种用于探索的新颖性检测算法。训练分类器以区分每个访问过的状态与所有其他状态，其中新颖状态更易于区分。 | X
    | ✓ | X |'
- en: '| [9287851] | Map high-dim video to a low-dim discrete latent representation
    using a VQ-AE. | ✓ | ✓ | X |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| [9287851] | 使用 VQ-AE 将高维视频映射到低维离散潜在表示。 | ✓ | ✓ | X |'
- en: '| [hester2013learning] | Focus on learning exploration in model-based RL. |
    X | ✓ | X |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| [hester2013learning] | 专注于基于模型的强化学习中的探索学习。 | X | ✓ | X |'
- en: '| [kessler2022same] | Show that existing continual learning methods based on
    single neural network predictors with shared replay buffers fail in the presence
    of interference. Propose a factorized policy, using shared feature extraction
    layers, but separate heads, each specializing on a new task to prevent interference.
    | X | X | ✓ |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| [kessler2022same] | 显示现有的基于单一神经网络预测器和共享重放缓冲区的持续学习方法在存在干扰时会失败。提出了一种因子化策略，使用共享特征提取层，但分开头部，每个头部专注于新任务以防止干扰。
    | X | X | ✓ |'
- en: '| [kovac2020grimgep] | Set goals in the region of highest uncertainty. Exploring
    uncertain states with regard to the rewards are the sub-goals. | ✓ | ✓ | X |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| [kovac2020grimgep] | 在最高不确定性的区域设定目标。探索不确定状态时，奖励就是子目标。 | ✓ | ✓ | X |'
- en: '| [kulkarni2016hierarchical] | Introduce a hierarchical-DQN (h-DQN) operating
    at different temporal scales. | ✓ | ✓ | X |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| [kulkarni2016hierarchical] | 介绍了一种在不同时间尺度上运行的分层-DQN（h-DQN）。 | ✓ | ✓ | X |'
- en: '| [dietterich2000overview] | An overview of the MAXQ value function decomposition
    and its support for state and action abstraction. | ✓ | ✓ | X |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| [dietterich2000overview] | MAXQ 值函数分解及其对状态和动作抽象的支持概述。 | ✓ | ✓ | X |'
- en: '| [machado2017laplacian] | Introduce a Laplacian framework for option discovery.
    | ✓ | ✓ | X |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| [machado2017laplacian] | 引入一个拉普拉斯框架用于选项发现。 | ✓ | ✓ | X |'
- en: '| [machadoeigenoption] | Look at *Eigenoptions*, options obtained from representations
    that encode diffusive information flow in the environment. Authors extend the
    existing algorithms for Eigenoption discovery to settings with stochastic transitions
    and in which handcrafted features are not available. | ✓ | ✓ | X |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| [machadoeigenoption] | 关注*Eigenoptions*，这些选项从编码环境中扩散信息流的表示中获得。作者将现有的Eigenoption发现算法扩展到具有随机过渡的设置中，并且在这些设置中手工特征不可用。
    | ✓ | ✓ | X |'
- en: '| [pmlr-v119-misra20a] | Authors introduce HOMER, an iterative state abstraction
    approach that accounts for the fact that the learning of a compact representation
    for states requires comprehensive information from the environment. | ✓ | ✓ |
    X |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| [pmlr-v119-misra20a] | 作者介绍了HOMER，一种迭代状态抽象方法，该方法考虑到学习紧凑状态表示需要来自环境的全面信息。 |
    ✓ | ✓ | X |'
- en: '| [martin2017count] | Count-based exploration in feature space rather than
    for the raw inputs. | ✓ | ✓ | X |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| [martin2017count] | 在特征空间中进行基于计数的探索，而不是对原始输入进行探索。 | ✓ | ✓ | X |'
- en: '| [pathak2017curiosity] | Intrinsic rewards based method that formulates curiosity
    as the error in an agent’s ability to predict the consequence of its own actions
    in a visual feature space learned by a self-supervised inverse dynamics model.
    | ✓ | ✓ | X |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| [pathak2017curiosity] | 基于内在奖励的方法，将好奇心形式化为代理在通过自监督逆动态模型学习的视觉特征空间中预测自己行动结果的能力的误差。
    | ✓ | ✓ | X |'
- en: '| [savinovepisodic] | Propose a new curiosity method which uses episodic memory
    to form the novelty bonus. Current observation is compared with the observations
    in memory. | ✓ | ✓ | X |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| [savinovepisodic] | 提出了一种新的好奇心方法，该方法使用情景记忆来形成新颖性奖励。将当前观察与记忆中的观察进行比较。 | ✓
    | ✓ | X |'
- en: '| [stadie2015incentivizing] | Evaluate sophisticated exploration strategies,
    including Thompson sampling and Boltzman exploration, and propose a new exploration
    method based on assigning exploration bonuses from a concurrently learned model
    of the system dynamics. | ✓ | ✓ | X |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| [stadie2015incentivizing] | 评估复杂的探索策略，包括Thompson采样和Boltzman探索，并提出了一种基于从系统动态的并发学习模型中分配探索奖励的新探索方法。
    | ✓ | ✓ | X |'
- en: '| [ribeiro2019multi] | Train DRL on two similar tasks, augmented with EWC to
    mitigate catastrophic forgetting. | X | X | ✓ |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| [ribeiro2019multi] | 在两个相似任务上训练DRL，增加EWC以减轻灾难性遗忘。 | X | X | ✓ |'
- en: '| [tang2017exploration] | Use an auto-encoder and SimHash for to enable count
    based exploration. | ✓ | ✓ | X |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| [tang2017exploration] | 使用自编码器和SimHash来实现基于计数的探索。 | ✓ | ✓ | X |'
- en: '| [vezhnevets2017feudal] | Introduce FeUdal Networks (FuNs): a novel architecture
    for hierarchical RL. FuNs employs a Manager module and a Worker module. The Manager
    operates at a slower time scale and sets abstract goals which are conveyed to
    and enacted by the Worker. The Worker generates primitive actions at every tick
    of the environment. | ✓ | ✓ | X |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| [vezhnevets2017feudal] | 引入FeUdal Networks（FuNs）：一种用于分层强化学习的新颖架构。FuNs采用一个Manager模块和一个Worker模块。Manager在较慢的时间尺度上操作，并设定抽象目标，这些目标被传达给Worker并由其执行。Worker在环境的每一个时刻生成原始动作。
    | ✓ | ✓ | X |'
- en: '| [zhanglearning] | Propose *deep bisimulation for control* (DBC). DBC learns
    directly on this bisimulation distance metric. Allows the learning of invariant
    representations that can be used effectively for downstream control policies,
    and are invariant with respect to task-irrelevant details. | ✓ | ✓ | X |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| [zhanglearning] | 提出*深度双重仿真控制*（DBC）。DBC直接在这种双重仿真距离度量上进行学习。允许学习可以有效用于下游控制策略的恒定表示，并且对任务无关的细节保持不变。
    | ✓ | ✓ | X |'
- en: 'Table 2: An overview of approaches for addressing the curse of dimensionality
    with regard to states, and the extent to which works cover the topics of abstraction,
    *advanced* exploration strategies, and the mitigation of catastrophic forgetting.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：解决状态维度灾难的方法概述，以及这些方法涵盖抽象、*先进*探索策略和减轻灾难性遗忘的程度。
- en: Conversion to HTML had a Fatal error and exited abruptly. This document may
    be truncated or damaged.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为HTML时发生致命错误并突然退出。此文档可能已被截断或损坏。
