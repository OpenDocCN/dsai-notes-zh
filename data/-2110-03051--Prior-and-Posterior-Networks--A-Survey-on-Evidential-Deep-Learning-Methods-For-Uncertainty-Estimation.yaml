- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:51:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:51:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.03051] Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.03051] 先验和后验网络：关于不确定性估计的证据深度学习方法的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.03051](https://ar5iv.labs.arxiv.org/html/2110.03051)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.03051](https://ar5iv.labs.arxiv.org/html/2110.03051)
- en: 'Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods
    For Uncertainty Estimation'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先验和后验网络：关于不确定性估计的证据深度学习方法的调查
- en: Dennis Ulmer^(\faCompass) dennis.ulmer@mailbox.org Christian Hardmeier^(\faCompass)
    chrha@itu.dk Jes Frellsen^(\faRobot,\faCompressArrows*) jefr@dtu.dk
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 丹尼斯·乌尔默^(\faCompass) dennis.ulmer@mailbox.org 克里斯蒂安·哈德迈耶^(\faCompass) chrha@itu.dk
    赫斯·弗雷尔森^(\faRobot,\faCompressArrows*) jefr@dtu.dk
- en: ^(\faCompass)IT University of Copenhagen, ^(\faRobot)Technical University of
    Denmark, ^(\faCompressArrows*)Pioneer Centre for Artificial Intelligence
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^(\faCompass)哥本哈根IT大学，^(\faRobot)丹麦技术大学，^(\faCompressArrows*)人工智能先锋中心
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Popular approaches for quantifying predictive uncertainty in deep neural networks
    often involve distributions over weights or multiple models, for instance via
    Markov Chain sampling, ensembling, or Monte Carlo dropout. These techniques usually
    incur overhead by having to train multiple model instances or do not produce very
    diverse predictions. This comprehensive and extensive survey aims to familiarize
    the reader with an alternative class of models based on the concept of *Evidential
    Deep Learning*: For unfamiliar data, they aim to admit “what they don’t know”,
    and fall back onto a prior belief. Furthermore, they allow uncertainty estimation
    in a single model and forward pass by parameterizing *distributions over distributions*.
    This survey recapitulates existing works, focusing on the implementation in a
    classification setting, before surveying the application of the same paradigm
    to regression. We also reflect on the strengths and weaknesses compared to other
    existing methods and provide the most fundamental derivations using a unified
    notation to aid future research.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 定量评估深度神经网络中的预测不确定性的流行方法通常涉及权重的分布或多个模型，例如通过马尔可夫链采样、集成或蒙特卡洛丢弃。这些技术通常会产生额外开销，因为需要训练多个模型实例，或不能产生非常多样的预测。这项全面而广泛的调查旨在让读者熟悉基于*证据深度学习*概念的另一类模型：对于陌生数据，它们旨在承认“它们不知道的东西”，并回退到先验信念。此外，它们通过对*分布上的分布*进行参数化，允许在单一模型和前向传播中进行不确定性估计。本调查回顾了现有的工作，重点介绍了分类设置中的实现，然后调查了相同范式在回归中的应用。我们还反思了与其他现有方法相比的优缺点，并提供了最基本的推导，使用统一的符号来帮助未来的研究。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: '![Refer to caption](img/b257b25423bf826d0d0c7fb7d841b431.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b257b25423bf826d0d0c7fb7d841b431.png)'
- en: 'Figure 1: Taxonomy of surveyed approaches, divided into tractable parameterizations
    of the prior or posterior on one axis (see [Tables 1](#S3.T1 "In 3.4.1 Prior Networks
    ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") and [2](#S3.T2 "Table 2 ‣ 3.4.2
    Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") for an overview) and into approaches
    for classification and regression on the other. Regression methods are outlined
    in [Table 3](#S4.T3 "In 4 Evidential Deep Learning for Regression ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation").'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：调查方法的分类，分为一个轴上的可处理先验或后验参数化（参见[表1](#S3.T1 "在3.4.1 先验网络 ‣ 3.4 已存在的Dirichlet网络方法
    ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查") 和[2](#S3.T2 "表2 ‣ 3.4.2 后验网络 ‣
    3.4 已存在的Dirichlet网络方法 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")的概述）和分类与回归方法的另一个轴。回归方法在[表3](#S4.T3
    "在4 证据深度学习用于回归 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")中概述。
- en: 'Many existing methods for uncertainty estimation leverage the concept of Bayesian
    model averaging: These include ensembling (Lakshminarayanan et al., [2017](#bib.bib99);
    Wilson & Izmailov, [2020](#bib.bib178)), Markov chain Monte Carlo sampling (de Freitas,
    [2003](#bib.bib28); Andrieu et al., [2000](#bib.bib2)) as well as variational
    inference approaches (Mackay, [1992](#bib.bib114); MacKay, [1995](#bib.bib112);
    Hinton & Van Camp, [1993](#bib.bib67); Neal, [2012](#bib.bib134)), including approaches
    such as Monte Carlo (MC) dropout (Gal & Ghahramani, [2016](#bib.bib45)) and Bayes-by-backprop
    (Blundell et al., [2015](#bib.bib13)). Bayesian model averaging for neural networks
    usually involves the approximation of an otherwise infeasible integral using MC
    samples. This causes the following problems: Firstly, the quality of the MC approximation
    depends on the veracity and diversity of samples from the weight posterior. Secondly,
    the approach often involves increasing the number of parameters in a model or
    training more model instances altogether. Recently, a new class of models has
    been proposed to side-step this conundrum by using a different factorization of
    the posterior predictive distribution. This allows computing uncertainty in a
    single forward pass and with a single set of weights. These models are grounded
    in a concept coined *Evidential Deep Learning*: For out-of-distribution (OOD)
    inputs, they are encouraged to fall back onto a prior. This is often described
    as *knowing what they don’t know*.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现有的不确定性估计方法利用了贝叶斯模型平均的概念：这些方法包括集成方法（Lakshminarayanan et al., [2017](#bib.bib99);
    Wilson & Izmailov, [2020](#bib.bib178)），马尔科夫链蒙特卡洛采样（de Freitas, [2003](#bib.bib28);
    Andrieu et al., [2000](#bib.bib2)），以及变分推断方法（Mackay, [1992](#bib.bib114); MacKay,
    [1995](#bib.bib112); Hinton & Van Camp, [1993](#bib.bib67); Neal, [2012](#bib.bib134)），包括蒙特卡洛（MC）丢弃法（Gal
    & Ghahramani, [2016](#bib.bib45)）和贝叶斯反向传播（Blundell et al., [2015](#bib.bib13)）。神经网络的贝叶斯模型平均通常涉及使用MC样本来近似一个原本难以处理的积分。这会导致以下问题：首先，MC近似的质量依赖于来自权重后验的样本的真实性和多样性。其次，这种方法通常涉及增加模型中的参数数量或训练更多的模型实例。最近，提出了一类新模型，通过使用后验预测分布的不同因式分解来绕过这一困境。这允许在单次前向传递中计算不确定性，并使用一组权重。这些模型基于一个概念，称为*证据深度学习*：对于分布外（OOD）输入，它们被鼓励回退到先验。这通常被描述为*知道自己不知道什么*。
- en: 'In this paper, we summarize the existing literature and provide an overview
    of Evidential Deep Learning approaches. We give an overview over all discussed
    work in [Figure 1](#S1.F1 "In 1 Introduction ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), where
    we distinguish surveyed works for classification between models parameterizing
    a Dirichlet prior ([Section 3.4.1](#S3.SS4.SSS1 "3.4.1 Prior Networks ‣ 3.4 Existing
    Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")) or posterior ([Section 3.4.2](#S3.SS4.SSS2 "3.4.2 Posterior
    Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep
    Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")). We further discuss similar
    methods for regression problems ([Section 4](#S4 "4 Evidential Deep Learning for
    Regression ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation")). As we will see, obtaining well-behaving
    uncertainty estimates can be challenging in the Evidential Deep Learning framework;
    proposed solutions that are also reflected in [Figure 1](#S1.F1 "In 1 Introduction
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") are the usage of OOD examples during training (Malinin
    & Gales, [2018](#bib.bib115); [2019](#bib.bib116); Nandy et al., [2020](#bib.bib133);
    Shen et al., [2020](#bib.bib151); Chen et al., [2018](#bib.bib18); Zhao et al.,
    [2019](#bib.bib188); Hu et al., [2021](#bib.bib71); Sensoy et al., [2020](#bib.bib147)),
    knowledge distillation (Malinin et al., [2020b](#bib.bib118); [a](#bib.bib117))
    or the incorporation of density estimation (Charpentier et al., [2020](#bib.bib16);
    [2022](#bib.bib17); Stadler et al., [2021](#bib.bib155)), which we discuss in
    more detail in [Section 6](#S6 "6 Discussion ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). This
    survey aims to both serve as an accessible introduction to this model family to
    the unfamiliar reader as well as an informative overview, in order to promote
    a wider application outside the uncertainty quantification literature. We also
    provide a collection of the most important derivations for the Dirichlet distribution
    for Machine Learning, which plays a central role in many of the discussed approaches.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们总结了现有文献，并概述了证据深度学习方法。我们在[图 1](#S1.F1 "在 1 引言 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")中概述了所有讨论的工作，其中我们区分了用于分类的模型，根据模型是否对Dirichlet先验进行参数化（[第
    3.4.1 节](#S3.SS4.SSS1 "3.4.1 先验网络 ‣ 3.4 Dirichlet 网络的现有方法 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")）或后验（[第
    3.4.2 节](#S3.SS4.SSS2 "3.4.2 后验网络 ‣ 3.4 Dirichlet 网络的现有方法 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")）进行区分。我们进一步讨论了类似的回归问题方法（[第
    4 节](#S4 "4 证据深度学习用于回归 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")）。正如我们将看到的，在证据深度学习框架中获得良好的不确定性估计可能具有挑战性；提出的解决方案也在[图
    1](#S1.F1 "在 1 引言 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")中有所反映，包括在训练过程中使用OOD示例（Malinin &
    Gales，[2018](#bib.bib115)；[2019](#bib.bib116)；Nandy 等，[2020](#bib.bib133)；Shen
    等，[2020](#bib.bib151)；Chen 等，[2018](#bib.bib18)；Zhao 等，[2019](#bib.bib188)；Hu
    等，[2021](#bib.bib71)；Sensoy 等，[2020](#bib.bib147)），知识蒸馏（Malinin 等，[2020b](#bib.bib118)；[a](#bib.bib117)）或密度估计的纳入（Charpentier
    等，[2020](#bib.bib16)；[2022](#bib.bib17)；Stadler 等，[2021](#bib.bib155)），我们在[第 6
    节](#S6 "6 讨论 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")中更详细地讨论了这些方法。此综述旨在为不熟悉的读者提供一个易于访问的介绍，并作为一个信息丰富的概述，以促进其在不确定性量化文献之外的更广泛应用。我们还提供了机器学习中Dirichlet分布的最重要推导的集合，这在许多讨论的方法中扮演了核心角色。
- en: 2 Background
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 'We first introduce the central concepts for this survey, including Bayesian
    inference in [Section 2.1](#S2.SS1 "2.1 Bayesian Inference ‣ 2 Background ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"), Bayesian model averaging in [Section 2.2](#S2.SS2 "2.2 Predictive
    Uncertainty in Neural Networks ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation") and
    Evidential Deep Learning in [Section 2.3](#S2.SS3 "2.3 Evidential Deep Learning
    ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation").¹¹1Note that in the following we will use
    the suggested notation of the TMLR journal, e.g. by using $P$ for probability
    mass and $p$ for probability density functions.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先介绍本次调查的核心概念，包括[第2.1节](#S2.SS1 "2.1 Bayesian Inference ‣ 2 Background ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")中的贝叶斯推断、[第2.2节](#S2.SS2 "2.2 Predictive Uncertainty in
    Neural Networks ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")中的贝叶斯模型平均以及[第2.3节](#S2.SS3 "2.3
    Evidential Deep Learning ‣ 2 Background ‣ Prior and Posterior Networks: A Survey
    on Evidential Deep Learning Methods For Uncertainty Estimation")中的证据深度学习。¹¹1 请注意，在下面的内容中，我们将使用TMLR期刊建议的符号，例如用$P$表示概率质量，用$p$表示概率密度函数。'
- en: 2.1 Bayesian Inference
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 贝叶斯推断
- en: 'The foundation of the following sections is Bayesian inference: Given some
    prior belief $p(\bm{\theta})$ about parameters of interest $\bm{\theta}$, we use
    available observations $\mathbb{D}=\{(x_{i},y_{i})\}_{i=1}^{N}$ and their likelihood
    $p(\mathbb{D}|\bm{\theta})$ to obtain an updated belief in form of the posterior
    $p(\bm{\theta}|\mathbb{D})\propto p(\mathbb{D}|\bm{\theta})p(\bm{\theta})$. This
    update rule is derived from Bayes’ rule, namely'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节的基础是贝叶斯推断：给定对感兴趣参数$\bm{\theta}$的先验信念$p(\bm{\theta})$，我们利用可用的观察数据$\mathbb{D}=\{(x_{i},y_{i})\}_{i=1}^{N}$及其似然$p(\mathbb{D}|\bm{\theta})$来获得更新的信念，形式为后验$p(\bm{\theta}|\mathbb{D})\propto
    p(\mathbb{D}|\bm{\theta})p(\bm{\theta})$。该更新规则源于贝叶斯定理，即
- en: '|  | $p(\bm{\theta}&#124;\mathbb{D})=\frac{p(\mathbb{D}&#124;\bm{\theta})p(\bm{\theta})}{p(\mathbb{D})}=\frac{p(\mathbb{D}&#124;\bm{\theta})p(\bm{\theta})}{\int
    p(\mathbb{D}&#124;\bm{\theta})p(\bm{\theta})d\bm{\theta}},$ |  | (1) |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\bm{\theta}&#124;\mathbb{D})=\frac{p(\mathbb{D}&#124;\bm{\theta})p(\bm{\theta})}{p(\mathbb{D})}=\frac{p(\mathbb{D}&#124;\bm{\theta})p(\bm{\theta})}{\int
    p(\mathbb{D}&#124;\bm{\theta})p(\bm{\theta})d\bm{\theta}},$ |  | (1) |'
- en: where we often try to avoid computing the term in the denominator since marginalization
    over a large (continuous) parameter space of $\bm{\theta}$ is usually intractable.
    In order to perform a prediction $y$ for a new data point $\mathbf{x}$, we can
    now utilize the *posterior predictive distribution* defined as
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常尽量避免计算分母中的项，因为对$\bm{\theta}$的大（连续）参数空间进行边际化通常是不可处理的。为了对新数据点$\mathbf{x}$进行预测$y$，我们现在可以利用定义为*后验预测分布*的
- en: '|  | $P(y&#124;\mathbf{x},\mathbb{D})=\int P(y&#124;\mathbf{x},\bm{\theta})p(\bm{\theta}&#124;\mathbb{D})d\bm{\theta}.$
    |  | (2) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y&#124;\mathbf{x},\mathbb{D})=\int P(y&#124;\mathbf{x},\bm{\theta})p(\bm{\theta}&#124;\mathbb{D})d\bm{\theta}.$
    |  | (2) |'
- en: Since we integrate over the entire parameter space of $\bm{\theta}$, weighting
    each prediction by the posterior probability of its parameters to obtain the final
    result, this process is referred to as *Bayesian model averaging* (BMA). Here,
    predictions $P(y|\mathbf{x},\bm{\theta})$ stemming from parameters that are plausible
    given the observed data will receive a higher weight $p(\bm{\theta}|\mathbb{D})$
    in the final prediction $P(y|\mathbf{x},\mathbb{D})$. As we will see in the following
    section, this factorization of the predictive predictive distribution also has
    beneficial properties for analyzing the uncertainty of a model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在$\bm{\theta}$的整个参数空间上进行积分，通过后验概率加权每个预测以获得最终结果，这个过程被称为*贝叶斯模型平均*（BMA）。在这里，来自于观测数据下似乎合理的参数的预测$P(y|\mathbf{x},\bm{\theta})$将在最终预测$P(y|\mathbf{x},\mathbb{D})$中获得更高的权重$p(\bm{\theta}|\mathbb{D})$。正如我们将在以下部分看到的，这种预测分布的分解对于分析模型的不确定性也具有有益的特性。
- en: 2.2 Predictive Uncertainty in Neural Networks
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 神经网络中的预测不确定性
- en: 'In probabilistic modelling, uncertainty is commonly divided into aleatoric
    and epistemic uncertainty (Der Kiureghian & Ditlevsen, [2009](#bib.bib31); Kendall
    & Gal, [2017](#bib.bib85); Hüllermeier & Waegeman, [2021](#bib.bib75)). Aleatoric
    uncertainty refers to the uncertainty that is induced by the data-generating process,
    for instance noise or inherent overlap between observed instances of classes.
    Epistemic uncertainty is the type of uncertainty about the optimal model parameters
    (or even hypothesis class). It is reducible with an increasing amount of data,
    as fewer and fewer possible models become a plausible fit. These two notions resurface
    when formulating the posterior predictive distribution for a new data point $\mathbf{x}$:²²2Note
    that the predictive distribution in [Equation 2](#S2.E2 "In 2.1 Bayesian Inference
    ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation") generalizes the common case for a single
    network prediction where $P(y|\mathbf{x},\mathbb{D})\approx P(y|\mathbf{x},\hat{\bm{\theta}})$.
    Mathematically, this is expressed by replacing the posterior $p(\bm{\theta}|\mathbb{D})$
    by a Dirac delta distribution as in [Equation 5](#S2.E5 "In 2.3 Evidential Deep
    Learning ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), where all probability density
    rests on a single parameter configuration.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率建模中，不确定性通常分为**Aleatoric**不确定性和**Epistemic**不确定性（Der Kiureghian & Ditlevsen，[2009](#bib.bib31)；Kendall
    & Gal，[2017](#bib.bib85)；Hüllermeier & Waegeman，[2021](#bib.bib75)）。**Aleatoric**不确定性指的是由数据生成过程引起的不确定性，例如噪声或观察到的类别实例之间的固有重叠。**Epistemic**不确定性是对最优模型参数（甚至假设类）存在的不确定性。随着数据量的增加，这种不确定性是可以减少的，因为可能的模型变得越来越少，变得更为合理。这两个概念在为新的数据点$\mathbf{x}$制定后验预测分布时再次出现：²²2请注意，[方程2](#S2.E2
    "在2.1贝叶斯推断 ‣ 2 背景 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")中的预测分布概括了单个网络预测的常见情况，其中$P(y
    \mid \mathbf{x}, \mathbb{D}) \approx P(y \mid \mathbf{x}, \hat{\bm{\theta}})$。在数学上，这通过用Dirac
    delta分布替换后验$p(\bm{\theta} \mid \mathbb{D})$来表达，如[方程5](#S2.E5 "在2.3证据深度学习 ‣ 2 背景
    ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")中所示，其中所有的概率密度集中在单一的参数配置上。
- en: '|  | $P(y&#124;\mathbf{x},\mathbb{D})=\int\underbrace{P(y&#124;\mathbf{x},\bm{\theta})}_{\text{Aleatoric}}\underbrace{p(\bm{\theta}&#124;\mathbb{D})}_{\text{Epistemic}}d\bm{\theta}.$
    |  | (3) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y \mid \mathbf{x}, \mathbb{D}) = \int \underbrace{P(y \mid \mathbf{x},
    \bm{\theta})}_{\text{Aleatoric}} \underbrace{p(\bm{\theta} \mid \mathbb{D})}_{\text{Epistemic}}
    d\bm{\theta}.$ |  | (3) |'
- en: Here, the first factor captures the aleatoric uncertainty about the correct
    prediction, while the second one expresses uncertainty about the correct model
    parameters—the more data we observe, the more density of $p(\bm{\theta}|\mathbb{D})$
    should lie on reasonable parameter values for $\bm{\theta}$. For high-dimensional
    real-valued parameters $\bm{\theta}$ like in neural networks, this integral becomes
    intractable, and is usually approximated using Monte Carlo samples:³³3For easier
    distributions, the integral can often be evaluated analytically exploiting conjugacy.
    Another approach for more complex distributions can be the method of moments (see
    e.g. Duan, [2021](#bib.bib35)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第一个因素捕获了关于正确预测的**Aleatoric**不确定性，而第二个因素表达了关于正确模型参数的不确定性——我们观察到的数据越多，$p(\bm{\theta}
    \mid \mathbb{D})$的密度应当更多地集中在$\bm{\theta}$的合理参数值上。对于像神经网络中的高维实值参数$\bm{\theta}$，这个积分变得不可处理，通常使用Monte
    Carlo样本来近似：³³3对于更简单的分布，积分通常可以通过利用共轭性进行解析求解。另一种针对更复杂分布的方法是矩量法（见例如Duan，[2021](#bib.bib35)）。
- en: '|  | $P(y&#124;\mathbf{x},\mathbb{D})\approx\frac{1}{K}\sum_{k=1}^{K}P(y&#124;\mathbf{x},\bm{\theta}^{(k)});\quad\bm{\theta}^{(k)}\sim
    p(\bm{\theta}&#124;\mathbb{D})$ |  | (4) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y \mid \mathbf{x}, \mathbb{D}) \approx \frac{1}{K} \sum_{k=1}^{K} P(y
    \mid \mathbf{x}, \bm{\theta}^{(k)}); \quad \bm{\theta}^{(k)} \sim p(\bm{\theta}
    \mid \mathbb{D})$ |  | (4) |'
- en: based on $K$ different sets of parameters $\bm{\theta}^{(k)}$. Since this requires
    obtaining multiple versions of model parameters through some additional procedure,
    this however comes with the aforementioned problems of computational overhead
    and approximation errors, motivating the approaches discussed in this survey.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于$K$不同的参数集合$\bm{\theta}^{(k)}$。由于这需要通过一些额外的程序来获取多个版本的模型参数，因此伴随而来的是前述的计算开销和近似误差问题，这激发了本文讨论的方法。
- en: 2.3 Evidential Deep Learning
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 证据深度学习
- en: 'Since the traditional approach to predictive uncertainty estimation requires
    multiple parameter sets and can only approximate the predictive posterior, we
    can factorize [Equation 2](#S2.E2 "In 2.1 Bayesian Inference ‣ 2 Background ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") further to obtain a tractable form:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于传统的预测不确定性估计方法需要多个参数集，并且只能近似预测后验分布，我们可以进一步分解[方程 2](#S2.E2 "在 2.1 贝叶斯推断 ‣ 2
    背景 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")以获得一个可处理的形式：
- en: '|  | $\displaystyle p(y&#124;\mathbf{x},\mathbb{D})$ | $\displaystyle=\iint\underbrace{P(y&#124;\bm{\pi})}_{\vphantom{\big{[}}\text{Aleatoric}}\underbrace{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}_{\vphantom{\big{[}}\text{\
    Distributional\ }}\underbrace{p(\bm{\theta}&#124;\mathbb{D})}_{\vphantom{\big{[}}\text{Epistemic}}d\bm{\pi}d\bm{\theta}\approx\int
    P(y&#124;\bm{\pi})\underbrace{\vphantom{\big{[}}p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})}_{p(\bm{\theta}&#124;\mathbb{D})\approx\delta(\bm{\theta}-\hat{\bm{\theta}})}d\bm{\pi}.$
    |  | (5) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(y\mid\mathbf{x},\mathbb{D})$ | $\displaystyle=\iint\underbrace{P(y\mid\bm{\pi})}_{\vphantom{\big{[}}\text{随机}}\underbrace{p(\bm{\pi}\mid\mathbf{x},\bm{\theta})}_{\vphantom{\big{[}}\text{分布}}\underbrace{p(\bm{\theta}\mid\mathbb{D})}_{\vphantom{\big{[}}\text{认知}}d\bm{\pi}d\bm{\theta}\approx\int
    P(y\mid\bm{\pi})\underbrace{\vphantom{\big{[}}p(\bm{\pi}\mid\mathbf{x},\hat{\bm{\theta}})}_{p(\bm{\theta}\mid\mathbb{D})\approx\delta(\bm{\theta}-\hat{\bm{\theta}})}d\bm{\pi}.$
    |  | (5) |'
- en: 'This factorization contains another type of uncertainty, which Malinin & Gales
    ([2018](#bib.bib115)) call the *distributional* uncertainty, uncertainty caused
    by the mismatch of training and test data distributions. In the last step, Malinin
    & Gales ([2018](#bib.bib115)) replace $p(\bm{\theta}|\mathbb{D})$ by a point estimate
    $\hat{\bm{\theta}}$ using the Dirac delta function, i.e. a single trained neural
    network, to get rid of the intractable integral. Although another integral remains,
    retrieving the uncertainty from this predictive distribution actually has a closed-form
    analytical solution for the Dirichlet (see [Section 3.3](#S3.SS3 "3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")). The advantage of this approach is further that it allows
    us to distinguish uncertainty about a data point because it is ambiguous from
    points coming from an entirely different data distribution. As an example, consider
    a binary classification problem, in which the data manifold consists of two overlapping
    clusters. As we are classifying a new data point, we obtain a distribution $P(y|\mathbf{x},\bm{\theta})$
    which is uniform over the two classes. What does this mean? The model might either
    be confident that the point lies in the region of overlap and is inherently ambiguous,
    or that the model is uncertain about the correct class. Without further context,
    we cannot distinguish between these two cases (Bengs et al., [2022](#bib.bib8);
    Hüllermeier, [2022](#bib.bib74)). Compare that to instead predicting $p(\bm{\pi}|\mathbf{x},\bm{\theta})$:
    If the data point is ambiguous, the resulting distribution will be centered on
    $0.5$, if the model is generally uncertain, the distribution will be uniform,
    allowing this distinction. We will illustrate this principle further in the upcoming
    [Sections 2.4](#S2.SS4 "2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") and [3.3](#S3.SS3 "3.3 Uncertainty Estimation with Dirichlet
    Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '这种因式分解包含另一种不确定性，Malinin & Gales ([2018](#bib.bib115)) 称之为*分布性*不确定性，即由训练数据和测试数据分布不匹配引起的不确定性。在最后一步中，Malinin
    & Gales ([2018](#bib.bib115)) 用Dirac delta函数，即一个单一训练的神经网络，替换了$p(\bm{\theta}|\mathbb{D})$以消除难以处理的积分。尽管还有另一个积分存在，从这个预测分布中检索不确定性实际上对于Dirichlet分布有一个封闭形式的解析解（参见
    [第3.3节](#S3.SS3 "3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")）。这种方法的进一步优势在于它使我们能够区分数据点的不确定性，因为它与来自完全不同数据分布的点混淆。例如，考虑一个二分类问题，其中数据流形由两个重叠的簇组成。当我们对一个新的数据点进行分类时，我们会得到一个在两个类别之间均匀分布的$P(y|\mathbf{x},\bm{\theta})$。这意味着什么？模型可能会对该点位于重叠区域并固有模糊感到自信，或者模型对正确类别不确定。在没有更多上下文的情况下，我们无法区分这两种情况（Bengs
    等，[2022](#bib.bib8); Hüllermeier，[2022](#bib.bib74)）。将其与预测$p(\bm{\pi}|\mathbf{x},\bm{\theta})$进行比较：如果数据点模糊，结果分布将集中在$0.5$，如果模型普遍不确定，分布将是均匀的，从而允许这种区分。我们将在即将到来的
    [第2.4节](#S2.SS4 "2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") 和 [3.3节](#S3.SS3 "3.3 Uncertainty Estimation with Dirichlet
    Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    进一步说明这一原则。'
- en: 'In the neural network context in [Equation 5](#S2.E5 "In 2.3 Evidential Deep
    Learning ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), it should be noted that restricting
    oneself to a point estimate of the parameters prevent the estimation of epistemic
    uncertainty like in earlier works through the weight posterior $p(\bm{\theta}|\mathbb{D})$,
    as discussed in the next section. However, there are works like Haussmann et al.
    ([2019](#bib.bib58)); Zhao et al. ([2020](#bib.bib189)) that combine both approaches.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '在[方程5](#S2.E5 "In 2.3 Evidential Deep Learning ‣ 2 Background ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")中的神经网络背景下，需要注意的是，将自己限制在参数的点估计上会阻止像早期工作那样通过权重后验$p(\bm{\theta}|\mathbb{D})$进行的知识不确定性估计，如下一节所讨论的。然而，也有一些工作如Haussmann
    等 ([2019](#bib.bib58)); Zhao 等 ([2020](#bib.bib189)) 结合了这两种方法。'
- en: 'The term *Evidential Deep Learning* (EDL) originates from the work of Sensoy
    et al. ([2018](#bib.bib146)) and is based on the *Theory of Evidence* (Dempster,
    [1968](#bib.bib29); Audun, [2018](#bib.bib5)): Within the theory, belief mass
    is assigned to set of possible states, e.g. class labels, and can also express
    a lack of evidence, i.e. an “I don’t know”. We can for instance generalize the
    predicted output of a neural classifier using the Dirichlet distribution, allowing
    us to express a lack of evidence through a uniform Dirichlet. This is different
    from a uniform Categorical distribution, which does not distinguish an equal probability
    for all classes from the lack of evidence. For the purpose of this survey, we
    define Evidential Deep Learning as a family of approaches in which a neural network
    can fall back onto a uniform prior for unknown inputs. While neural networks usually
    parameterize likelihood functions, approaches in this survey parameterize prior
    or posterior distributions instead. The advantages of this methodology are now
    demonstrated using the example in the following section.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*证据深度学习*（EDL）这一术语来源于Sensoy等人的工作（[2018](#bib.bib146)），并基于*证据理论*（Dempster, [1968](#bib.bib29);
    Audun, [2018](#bib.bib5)）：在该理论中，信念质量被分配给一组可能的状态，例如类别标签，并且也可以表达证据的缺乏，即“我不知道”。例如，我们可以使用Dirichlet分布来推广神经分类器的预测输出，通过均匀的Dirichlet分布来表达证据的缺乏。这与均匀的Categorical分布不同，后者无法区分所有类别的相等概率与证据的缺乏。在本调查中，我们将证据深度学习定义为一种方法家族，其中神经网络可以在未知输入的情况下回退到均匀的先验分布。虽然神经网络通常对似然函数进行参数化，但本调查中的方法则对先验或后验分布进行参数化。以下部分的示例展示了这种方法的优点。'
- en: '2.4 An Illustrating Example: The Iris Dataset'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 一个说明性示例：鸢尾花数据集
- en: '![Refer to caption](img/e8634e0ef5fefc1e0e3894ddde313e44.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e8634e0ef5fefc1e0e3894ddde313e44.png)'
- en: (a) *Iris setosa*
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (a) *翡翠鸢尾*
- en: '![Refer to caption](img/37b1a1b0ee80c1db32c014efec6fec47.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/37b1a1b0ee80c1db32c014efec6fec47.png)'
- en: (b) *Iris versicolor*
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (b) *鸢尾花属*
- en: '![Refer to caption](img/85d09e49724a78779aaed213a2584675.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/85d09e49724a78779aaed213a2584675.png)'
- en: (c) *Iris virginica*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (c) *维吉尼亚鸢尾*
- en: '![Refer to caption](img/1c2c10915edf4368ab9f834c4d1ae644.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1c2c10915edf4368ab9f834c4d1ae644.png)'
- en: 'Figure 2: Illustration of different approaches to uncertainty quantifying on
    the Iris dataset, with examples for the classes given on the left ([Figures 2(a)](#S2.F2.sf1
    "In Figure 2 ‣ 2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation"), [2(b)](#S2.F2.sf2 "Figure 2(b) ‣ Figure 2 ‣ 2.4 An Illustrating
    Example: The Iris Dataset ‣ 2 Background ‣ Prior and Posterior Networks: A Survey
    on Evidential Deep Learning Methods For Uncertainty Estimation") and [2(c)](#S2.F2.sf3
    "Figure 2(c) ‣ Figure 2 ‣ 2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")). On the right, the data is plotted alongside some predictions
    of a prior network (lighter colors indicate higher density) and an ensemble and
    MC Dropout model on the probability simplex, with $50$ predictions each. Iris
    images were taken from Wikimedia Commons, [2022a](#bib.bib175); [b](#bib.bib176);
    [c](#bib.bib177).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在鸢尾花数据集上不确定性量化的不同方法的说明，左侧展示了各类别的示例（[图 2(a)](#S2.F2.sf1 "图 2 ‣ 2.4 一个说明性示例：鸢尾花数据集
    ‣ 2 背景 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查"), [2(b)](#S2.F2.sf2 "图 2(b) ‣ 图 2 ‣ 2.4
    一个说明性示例：鸢尾花数据集 ‣ 2 背景 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查") 和 [2(c)](#S2.F2.sf3 "图
    2(c) ‣ 图 2 ‣ 2.4 一个说明性示例：鸢尾花数据集 ‣ 2 背景 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")。右侧，数据与先验网络（较浅的颜色表示更高的密度）以及概率单纯形上的一个集成和MC
    Dropout模型的一些预测一起绘制，每个模型有$50$个预测。鸢尾花图像取自Wikimedia Commons， [2022a](#bib.bib175);
    [b](#bib.bib176); [c](#bib.bib177)。
- en: 'To illustrate the advantages of EDL, we choose a classification problem based
    on the Iris dataset (Fisher, [1936](#bib.bib40)). It contains measurements of
    three different species of iris flowers (shown in [Figures 2(a)](#S2.F2.sf1 "In
    Figure 2 ‣ 2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"), [2(b)](#S2.F2.sf2 "Figure 2(b) ‣ Figure 2 ‣ 2.4 An Illustrating
    Example: The Iris Dataset ‣ 2 Background ‣ Prior and Posterior Networks: A Survey
    on Evidential Deep Learning Methods For Uncertainty Estimation") and [2(c)](#S2.F2.sf3
    "Figure 2(c) ‣ Figure 2 ‣ 2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")). We use the dataset as made available through scikit-learn
    (Pedregosa et al., [2011](#bib.bib142)) and plot the relationship between the
    width and lengths measurements of the flowers’ petals in [Figure 2](#S2.F2 "In
    2.4 An Illustrating Example: The Iris Dataset ‣ 2 Background ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 EDL 的优势，我们选择了一个基于虹膜数据集（Fisher，[1936](#bib.bib40)）的分类问题。该数据集包含三种不同种类的虹膜花的测量数据（见[图
    2(a)](#S2.F2.sf1 "图 2 ‣ 2.4 一个示例：虹膜数据集 ‣ 2 背景 ‣ 先验和后验网络：关于证据深度学习方法的不确定性估计调查")，[2(b)](#S2.F2.sf2
    "图 2(b) ‣ 图 2 ‣ 2.4 一个示例：虹膜数据集 ‣ 2 背景 ‣ 先验和后验网络：关于证据深度学习方法的不确定性估计调查") 和[2(c)](#S2.F2.sf3
    "图 2(c) ‣ 图 2 ‣ 2.4 一个示例：虹膜数据集 ‣ 2 背景 ‣ 先验和后验网络：关于证据深度学习方法的不确定性估计调查")）。我们使用 scikit-learn
    提供的数据集（Pedregosa 等人，[2011](#bib.bib142)），并在[图 2](#S2.F2 "在 2.4 一个示例：虹膜数据集 ‣ 2
    背景 ‣ 先验和后验网络：关于证据深度学习方法的不确定性估计调查")中绘制花瓣宽度和长度测量之间的关系。
- en: 'We train an deep neural network ensemble (Lakshminarayanan et al., [2017](#bib.bib99))
    with $50$ model instances, a model with MC Dropout (Gal & Ghahramani, [2016](#bib.bib45))
    with $50$ predictions and a prior network (Sensoy et al., [2018](#bib.bib146)),
    an example of EDL, on all available data points, and plot their predictions on
    three test points on the 3-probability simplex in [Figure 2](#S2.F2 "In 2.4 An
    Illustrating Example: The Iris Dataset ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").⁴⁴4For
    information about training and model details, see [Section A.1](#A1.SS1 "A.1 Iris
    Example Training Details ‣ Appendix A Code Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). On
    these simplices, each point signifies a Categorical distribution, with the proximity
    to one of the corners indicating a higher probability for the corresponding class.
    EDL methods for classification do not predict a single output distribution, but
    an entire *density over output distributions*.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了一个深度神经网络集成（Lakshminarayanan 等人，[2017](#bib.bib99)），包括 $50$ 个模型实例，一个带有 MC
    Dropout 的模型（Gal & Ghahramani，[2016](#bib.bib45)），具有 $50$ 次预测，以及一个先验网络（Sensoy 等人，[2018](#bib.bib146)），这是
    EDL 的一个示例，使用所有可用的数据点，并在[图 2](#S2.F2 "在 2.4 一个示例：虹膜数据集 ‣ 2 背景 ‣ 先验和后验网络：关于证据深度学习方法的不确定性估计调查")上的三个测试点绘制它们的预测。在这些简单形上，每个点表示一个分类分布，与一个角落的接近程度表示对应类别的更高概率。用于分类的
    EDL 方法不会预测单一的输出分布，而是整个*输出分布的密度*。
- en: Test point <svg   height="14.16" overflow="visible" version="1.1" width="14.16"><g
    transform="translate(0,14.16) matrix(1 0 0 -1 0 0) translate(7.08,0) translate(0,7.08)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    lies in a region of overlap between instances of *Iris versicolor* and *Iris virginica*,
    thus inducing high aleatoric uncertainty. In this case, we can see that the prior
    network places all of its density on between these two classes, similar to most
    of the predictions of the ensemble and MC Dropout (bottom right). However, some
    of the latter predictions still land in the center of the simplex. The point <svg
    height="14.16" overflow="visible" version="1.1" width="14.16"><g transform="translate(0,14.16)
    matrix(1 0 0 -1 0 0) translate(7.08,0) translate(0,7.08)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg> is located
    in an area without training examples between instances of *Iris versicolor* and
    *setosa*, as well as close to a single *virginica* outlier. As shown in the top
    left, ensemble and MC Dropout predictions agree that the point belongs to either
    the *setosa* or *versicolor* class, with a slight preference for the former. The
    prior network concentrates its prediction on *versicolor*, but admits some uncertainty
    towards the two other choices. The last test point <svg height="14.16" overflow="visible"
    version="1.1" width="14.16"><g transform="translate(0,14.16) matrix(1 0 0 -1 0
    0) translate(7.08,0) translate(0,7.08)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
    is placed in an area of the feature space devoid of any data, roughly equidistant
    from the three clusters of flowers. Similar to the previous example, the ensemble
    and MC dropout predictions on the top right show a preference for *Iris setosa*
    and *versicolor*, albeit with higher uncertainy. The prior network however shows
    an almost uniform density, admitting distributional uncertainty about this particular
    input.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 测试点<svg   height="14.16" overflow="visible" version="1.1" width="14.16"><g transform="translate(0,14.16)
    matrix(1 0 0 -1 0 0) translate(7.08,0) translate(0,7.08)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>位于*Iris versicolor*和*Iris
    virginica*实例之间的重叠区域，因此引发了较高的随机不确定性。在这种情况下，我们可以看到先验网络将所有密度集中在这两个类别之间，这与集成预测和MC
    Dropout的预测（右下角）类似。然而，其中一些预测仍然落在简单形状的中心。点<svg height="14.16" overflow="visible"
    version="1.1" width="14.16"><g transform="translate(0,14.16) matrix(1 0 0 -1 0
    0) translate(7.08,0) translate(0,7.08)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>位于*Iris
    versicolor*和*setosa*实例之间没有训练样本的区域，同时接近一个*virginica*的离群点。如左上所示，集成和MC Dropout的预测一致认为该点属于*setosa*或*versicolor*类别，但稍微偏向前者。先验网络将其预测集中在*versicolor*上，但对其他两个选择表现出一些不确定性。最后一个测试点<svg
    height="14.16" overflow="visible" version="1.1" width="14.16"><g transform="translate(0,14.16)
    matrix(1 0 0 -1 0 0) translate(7.08,0) translate(0,7.08)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>位于特征空间中没有数据的区域，大致等距于三个花卉簇。与前面的例子类似，右上角的集成和MC
    Dropout预测显示出对*Iris setosa*和*versicolor*的偏好，但不确定性更高。然而，先验网络显示出几乎均匀的密度，对这一特定输入承认分布不确定性。
- en: 'This simple example provides some insights into the potential advantages of
    EDL: First of all, the prior network was able to provide reasonable uncertainty
    estimates in comparison with BMA methods. Secondly, the prior network is able
    to admit its lack of knowledge for the OOD data point by predicting an almost
    uniform prior, something that the other models are not able to. As laid out in
    [Section 3.3](#S3.SS3 "3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3
    Evidential Deep Learning for Classification ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), EDL actually
    allows the user to disentangle model uncertainty due to a simple lack of data
    and due to the input being out-of-distribution. Lastly, training the prior network
    only required a single model, which is a noticeable speed-up compared to MC Dropout
    and especially the training of ensembles.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '这个简单的例子提供了一些关于EDL潜在优势的见解：首先，先验网络能够提供相较于BMA方法的合理不确定性估计。其次，先验网络能够通过预测几乎均匀的先验来承认其对OOD数据点的知识不足，这是其他模型无法做到的。如在[第3.3节](#S3.SS3
    "3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")中所述，EDL实际上允许用户区分由于数据不足和由于输入超出分布导致的模型不确定性。最后，训练先验网络只需要一个模型，这比MC
    Dropout以及特别是训练集成方法明显加快了速度。'
- en: 3 Evidential Deep Learning for Classification
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 证据深度学习用于分类
- en: 'In order to introduce EDL methods for classification, we first give a brief
    introduction to the Dirichlet distribution and its role as a conjugate prior in
    Bayesian inference in [Section 3.1](#S3.SS1 "3.1 The Dirichlet distribution ‣
    3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). We
    then show in [Section 3.2](#S3.SS2 "3.2 Parameterization ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") how neural networks can parameterize
    Dirichlet distributions, while [Section 3.3](#S3.SS3 "3.3 Uncertainty Estimation
    with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation") reveals how such a parameterization can be exploited for efficient
    uncertainty estimation. The remaining sections enumerate different examples from
    the literature parameterizing either a prior ([Section 3.4.1](#S3.SS4.SSS1 "3.4.1
    Prior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")) or posterior Dirichlet distribution
    ([Section 3.4.2](#S3.SS4.SSS2 "3.4.2 Posterior Networks ‣ 3.4 Existing Approaches
    for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '为了介绍用于分类的EDL方法，我们首先在[第3.1节](#S3.SS1 "3.1 The Dirichlet distribution ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")中简要介绍了Dirichlet分布及其作为贝叶斯推断中的共轭先验的作用。然后在[第3.2节](#S3.SS2
    "3.2 Parameterization ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")中展示了神经网络如何对Dirichlet分布进行参数化，而[第3.3节](#S3.SS3 "3.3 Uncertainty Estimation
    with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")则揭示了如何利用这种参数化进行高效的不确定性估计。其余部分列举了来自文献中不同的例子，这些例子对先验分布 ([第3.4.1节](#S3.SS4.SSS1
    "3.4.1 Prior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")) 或后验Dirichlet分布 ([第3.4.2节](#S3.SS4.SSS2
    "3.4.2 Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3
    Evidential Deep Learning for Classification ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation")) 进行参数化。'
- en: 3.1 The Dirichlet distribution
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 Dirichlet分布
- en: '![Refer to caption](img/e3a701ff2a27eda0f281f65a2b314678.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e3a701ff2a27eda0f281f65a2b314678.png)'
- en: 'Figure 3: A prior Dirichlet distribution is updated with a vector of class
    observations. The posterior Dirichlet then shifts density towards the classes
    $k$ with more observed instances.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：一个先验Dirichlet分布通过一个类别观测向量进行更新。然后，后验Dirichlet将密度向具有更多观测实例的类别$k$转移。
- en: Modelling for instance a binary classification problem is commonly done using
    the Bernoulli likelihood. The Bernoulli likelihood has a single parameter $\pi$,
    indicating the probability of success (or of the positive class), and is given
    by
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，二分类问题的建模通常使用伯努利似然。伯努利似然有一个单一参数 $\pi$，表示成功（或正类）的概率，给定为
- en: '|  | $\text{Bernoulli}(y&#124;\pi)=\pi^{y}(1-\pi)^{(1-y)}.$ |  | (6) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Bernoulli}(y&#124;\pi)=\pi^{y}(1-\pi)^{(1-y)}.$ |  | (6) |'
- en: 'Within Bayesian inference as introduced in [Section 2](#S2 "2 Background ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation"), the Beta distribution is a commonly used prior for a
    Bernoulli likelihood. It defines a probability distribution over the parameter
    $\pi$, itself possessing two shape parameters $\alpha_{1}$ and $\alpha_{2}$:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '在[第2节](#S2 "2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")介绍的贝叶斯推断中，Beta 分布是伯努利似然的常用先验。它定义了一个关于参数
    $\pi$ 的概率分布，$\pi$ 本身具有两个形状参数 $\alpha_{1}$ 和 $\alpha_{2}$：'
- en: '|  | $\text{Beta}(\pi;\alpha_{1},\alpha_{2})=\frac{1}{B(\alpha_{1},\alpha_{2})}\pi^{\alpha_{1}-1}(1-\pi)^{\alpha_{2}-1};\quad
    B(\alpha_{1},\alpha_{2})=\frac{\Gamma(\alpha_{1})\Gamma(\alpha_{2})}{\Gamma(\alpha_{1}+\alpha_{2})};$
    |  | (7) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Beta}(\pi;\alpha_{1},\alpha_{2})=\frac{1}{B(\alpha_{1},\alpha_{2})}\pi^{\alpha_{1}-1}(1-\pi)^{\alpha_{2}-1};\quad
    B(\alpha_{1},\alpha_{2})=\frac{\Gamma(\alpha_{1})\Gamma(\alpha_{2})}{\Gamma(\alpha_{1}+\alpha_{2})};$
    |  | (7) |'
- en: 'where $\Gamma(\cdot)$ stands for the gamma function, a generalization of the
    factorial to the real numbers, and $B(\cdot)$ is called the Beta function (not
    to be confused with the distribution). When extending the classification problem
    from two to an arbitrary number of classes, we use a Categorical likelihood:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Gamma(\cdot)$ 代表伽玛函数，它是对实数的阶乘的推广，$B(\cdot)$ 被称为 Beta 函数（不要与分布混淆）。当将分类问题从两个类别扩展到任意数量的类别时，我们使用类别似然：
- en: '|  | $\text{Categorical}(y&#124;\bm{\pi})=\prod_{k=1}^{K}\pi_{k}^{\mathbf{1}_{y=k}},$
    |  | (8) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Categorical}(y&#124;\bm{\pi})=\prod_{k=1}^{K}\pi_{k}^{\mathbf{1}_{y=k}},$
    |  | (8) |'
- en: 'in which $K$ denotes the number of categories or classes, and the class probabilities
    are expressed using a vector $\bm{\pi}\in[0,1]^{K}$with $\sum_{k}\pi_{k}=1$, and
    $\mathbf{1}_{(\cdot)}$ is the indicator function. This distribution appears for
    instance in classification problems when using neural networks, since most neural
    networks for classification use a softmax function after their last layer to produce
    a Categorical distribution of classes s.t. $\pi_{k}\equiv P(y=k|x)$. In this setting,
    the Dirichlet distribution arises as a suitable prior and multivariate generalization
    of the Beta distribution (and is thus also called the *multivariate Beta distribution*):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $K$ 表示类别或类的数量，类别概率通过向量 $\bm{\pi}\in[0,1]^{K}$ 表达，其中 $\sum_{k}\pi_{k}=1$，$\mathbf{1}_{(\cdot)}$
    是指示函数。这种分布在使用神经网络的分类问题中出现，因为大多数用于分类的神经网络在最后一层之后使用 softmax 函数来生成类别的类别分布，使得 $\pi_{k}\equiv
    P(y=k|x)$。在这种情况下，Dirichlet 分布作为 Beta 分布的合适先验和多变量推广出现（因此也被称为*多变量 Beta 分布*）：
- en: '|  | $\text{Dir}(\bm{\pi};\bm{\alpha})=\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1};\quad
    B(\bm{\alpha})=\frac{\prod_{k=1}^{K}\Gamma(\alpha_{k})}{\Gamma(\alpha_{0})};\quad\alpha_{0}=\sum_{k=1}^{K}\alpha_{k};\quad\alpha_{k}\in\mathbb{R}^{+};$
    |  | (9) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Dir}(\bm{\pi};\bm{\alpha})=\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1};\quad
    B(\bm{\alpha})=\frac{\prod_{k=1}^{K}\Gamma(\alpha_{k})}{\Gamma(\alpha_{0})};\quad\alpha_{0}=\sum_{k=1}^{K}\alpha_{k};\quad\alpha_{k}\in\mathbb{R}^{+};$
    |  | (9) |'
- en: 'where the Beta function $B(\cdot)$ is now defined for $K$ shape parameters
    compared to [Equation 7](#S3.E7 "In 3.1 The Dirichlet distribution ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"). For notational convenience,
    we also define $\mathbb{K}=\{1,\ldots,K\}$ as the set of all classes. The distribution
    is characterized by its *concentration parameters* $\bm{\alpha}$, the sum of which,
    often denoted as $\alpha_{0}$, is called the *precision*.⁵⁵5The precision is analogous
    to the precision of a Gaussian, where a larger $\alpha_{0}$ signifies a sharper
    distribution. The Dirichlet is a *conjugate prior* for such a Categorical likelihood,
    meaning that according to Bayes’ rule in [Equation 1](#S2.E1 "In 2.1 Bayesian
    Inference ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), they produce a Dirichlet posterior
    with parameters $\bm{\beta}$, given a data set $\mathbb{D}=\{(x_{i},y_{i})\}_{i=1}^{N}$
    of $N$ observations with corresponding labels:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Beta 函数 $B(\cdot)$ 现在对于 $K$ 个形状参数进行了定义，这与 [公式 7](#S3.E7 "在 3.1 Dirichlet
    分布 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：一种针对不确定性估计的证据深度学习方法综述") 中的定义有所不同。为了符号的便利，我们还定义了 $\mathbb{K}=\{1,\ldots,K\}$
    作为所有类别的集合。该分布由其 *浓度参数* $\bm{\alpha}$ 表征，其总和通常表示为 $\alpha_{0}$，称为 *精度*。⁵⁵5 精度类似于高斯分布的精度，其中较大的
    $\alpha_{0}$ 表示更尖锐的分布。Dirichlet 是这种类别似然的 *共轭先验*，这意味着根据 [公式 1](#S2.E1 "在 2.1 贝叶斯推断
    ‣ 2 背景 ‣ 先验和后验网络：一种针对不确定性估计的证据深度学习方法综述") 的贝叶斯规则，给定数据集 $\mathbb{D}=\{(x_{i},y_{i})\}_{i=1}^{N}$
    及其对应标签，它们产生具有参数 $\bm{\beta}$ 的 Dirichlet 后验。
- en: '|  | $\displaystyle p(\bm{\pi}&#124;\mathbb{D},\bm{\alpha})$ | $\displaystyle\propto
    p\big{(}\{y_{i}\}_{i=1}^{N}&#124;\bm{\pi},\{x_{i}\}_{i=1}^{N}\big{)}p(\bm{\pi}&#124;\bm{\alpha})=\prod_{i=1}^{N}\prod_{k=1}^{K}\pi_{k}^{\mathbf{1}_{y_{i}=k}}\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1}$
    |  | (10) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(\bm{\pi}&#124;\mathbb{D},\bm{\alpha})$ | $\displaystyle\propto
    p\big{(}\{y_{i}\}_{i=1}^{N}&#124;\bm{\pi},\{x_{i}\}_{i=1}^{N}\big{)}p(\bm{\pi}&#124;\bm{\alpha})=\prod_{i=1}^{N}\prod_{k=1}^{K}\pi_{k}^{\mathbf{1}_{y_{i}=k}}\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1}$
    |  | (10) |'
- en: '|  |  | $\displaystyle=\prod_{k=1}^{K}\pi_{k}^{\big{(}\sum_{i=1}^{N}\mathbf{1}_{y_{i}=k}\big{)}}\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1}=\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{N_{k}+\alpha_{k}-1}\propto\text{Dir}(\bm{\pi};\bm{\beta}),$
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\prod_{k=1}^{K}\pi_{k}^{\big{(}\sum_{i=1}^{N}\mathbf{1}_{y_{i}=k}\big{)}}\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1}=\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{N_{k}+\alpha_{k}-1}\propto\text{Dir}(\bm{\pi};\bm{\beta}),$
    |  |'
- en: 'where $\bm{\beta}$ is a vector with $\beta_{k}=\alpha_{k}+N_{k}$, with $N_{k}$
    denoting the number of observations for class $k$. Intuitively, this implies that
    the prior belief encoded by the initial Dirichlet is updated using the actual
    data, sharpening the distribution for classes for which many instances have been
    observed. Similar to the Beta distribution in [Equation 7](#S3.E7 "In 3.1 The
    Dirichlet distribution ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"), the Dirichlet is a *distribution over Categorical distributions*
    on the $K-1$ probability simplex; we show an example with its concentration parameters
    and the Bayesian update in [Figure 3](#S3.F3 "In 3.1 The Dirichlet distribution
    ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{\beta}$ 是一个向量，$\beta_{k}=\alpha_{k}+N_{k}$，其中 $N_{k}$ 表示类别 $k$ 的观察次数。直观地说，这意味着由初始
    Dirichlet 编码的先验信念会使用实际数据进行更新，从而使观察到许多实例的类别的分布更加精准。类似于 [公式 7](#S3.E7 "在 3.1 Dirichlet
    分布 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：一种针对不确定性估计的证据深度学习方法综述") 中的 Beta 分布，Dirichlet 是 $K-1$
    概率单纯形上的 *类别分布的分布*；我们在 [图 3](#S3.F3 "在 3.1 Dirichlet 分布 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：一种针对不确定性估计的证据深度学习方法综述")
    中展示了一个带有其浓度参数和贝叶斯更新的示例。
- en: 3.2 Parameterization
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 参数化
- en: 'For a classification problem with $K$ classes, a neural classifier is usually
    realized as a function $f_{\bm{\theta}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{K}$,
    mapping an input $\mathbf{x}\in\mathbb{R}^{D}$ to *logits* for each class. Followed
    by a softmax function, this then defines a Categorical distribution over classes
    with a vector $\bm{\pi}$ with $\pi_{k}\equiv p(y=k|\mathbf{x},\bm{\theta})$. The
    same underlying architecture can be used without any major modification to instead
    parameterize a *Dirichlet* distribution, predicting a distribution *over Categorical
    distributions* $p(\bm{\pi}|\mathbf{x},\hat{\bm{\theta}})$ as in [Equation 9](#S3.E9
    "In 3.1 The Dirichlet distribution ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation").⁶⁶6The only thing to note here is that the every $\alpha_{k}$
    has to be strictly positive, which can for instance be enforced by using an additional
    softplus, exponential or ReLU function (Sensoy et al., [2018](#bib.bib146); Malinin
    & Gales, [2018](#bib.bib115); Sensoy et al., [2020](#bib.bib147)). In order to
    classify a data point $\mathbf{x}$, a Categorical distribution is created from
    the predicted concentration parameters of the Dirichlet as follows (this corresponds
    to the mean of the Dirichlet, see [Section C.1](#A3.SS1 "C.1 Expectation of a
    Dirichlet ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有 $K$ 类的分类问题，神经分类器通常被实现为函数 $f_{\bm{\theta}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{K}$，将输入
    $\mathbf{x}\in\mathbb{R}^{D}$ 映射为每个类别的 *logits*。然后通过 softmax 函数，这定义了一个在类别上的分类分布，其中向量
    $\bm{\pi}$ 的 $\pi_{k}\equiv p(y=k|\mathbf{x},\bm{\theta})$。相同的底层结构可以在不进行重大修改的情况下，用于参数化
    *Dirichlet* 分布，预测 *在分类分布上* 的分布 $p(\bm{\pi}|\mathbf{x},\hat{\bm{\theta}})$，如 [公式
    9](#S3.E9 "在 3.1 Dirichlet 分布 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：证据深度学习方法用于不确定性估计的综述")。⁶⁶6需要注意的是，每个
    $\alpha_{k}$ 必须严格为正，这可以通过使用额外的 softplus、指数或 ReLU 函数来强制执行（Sensoy 等， [2018](#bib.bib146)；
    Malinin 和 Gales，[2018](#bib.bib115)； Sensoy 等，[2020](#bib.bib147)）。为了对数据点 $\mathbf{x}$
    进行分类，从 Dirichlet 预测的浓度参数创建一个分类分布，如下所示（这对应于 Dirichlet 的均值，见 [第 C.1 节](#A3.SS1 "C.1
    Dirichlet 的期望 ‣ 附录 C 基本推导 ‣ 先验和后验网络：证据深度学习方法用于不确定性估计的综述")）：
- en: '|  | $\bm{\alpha}=\exp\big{(}f_{\bm{\theta}}(\mathbf{x})\big{)};\quad\pi_{k}=\frac{\alpha_{k}}{\alpha_{0}};\quad\hat{y}=\operatorname*{arg\,max}_{k\in\mathbb{K}}\
    \pi_{1},\ldots,\pi_{K}.$ |  | (11) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{\alpha}=\exp\big{(}f_{\bm{\theta}}(\mathbf{x})\big{)};\quad\pi_{k}=\frac{\alpha_{k}}{\alpha_{0}};\quad\hat{y}=\operatorname*{arg\,max}_{k\in\mathbb{K}}\
    \pi_{1},\ldots,\pi_{K}.$ |  | (11) |'
- en: 'Parameterizing a Dirichlet posterior distribution follows a similar logic,
    as we will discuss in [Section 3.4.2](#S3.SS4.SSS2 "3.4.2 Posterior Networks ‣
    3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning for
    Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation").'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Dirichlet 后验分布的参数化遵循类似的逻辑，我们将在 [第3.4.2节](#S3.SS4.SSS2 "3.4.2 后验网络 ‣ 3.4 Dirichlet
    网络的现有方法 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：证据深度学习方法用于不确定性估计的综述") 中讨论。
- en: 3.3 Uncertainty Estimation with Dirichlet Networks
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 使用 Dirichlet 网络进行不确定性估计
- en: '![Refer to caption](img/6cba51fe62adf4c3b92c6822e45e2a47.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6cba51fe62adf4c3b92c6822e45e2a47.png)'
- en: (a) Categorical distributions predicted by a neural ensemble on the probability
    simplex.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 神经网络集合在概率单纯形上预测的分类分布。
- en: '![Refer to caption](img/c6249a879b04f062c463de8c991d9215.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c6249a879b04f062c463de8c991d9215.png)'
- en: (b) Probability simplex for a confident prediction, for with the density concentrated
    in a single corner.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 对于自信预测的概率单纯形，其中密度集中在一个角落。
- en: '![Refer to caption](img/18146da21dde022402712deca8bbfc7f.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/18146da21dde022402712deca8bbfc7f.png)'
- en: (c) Dirichlet distribution for a case of data uncertainty, with the density
    concentrated in the center.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 对于数据不确定性的情况，Dirichlet 分布的密度集中在中心。
- en: '![Refer to caption](img/167bb1ad810dc8f055bc906dcf672fd6.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/167bb1ad810dc8f055bc906dcf672fd6.png)'
- en: (d) Dirichlet distribution for a case of model uncertainty, with the density
    spread out more.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 对于模型不确定性的情况，Dirichlet 分布的密度分布得更广。
- en: '![Refer to caption](img/7e84ff79feac78ba0ed80ca9874740f8.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7e84ff79feac78ba0ed80ca9874740f8.png)'
- en: (e) Dirichlet for a case of distributional uncertainty, with the density spread
    across the whole simplex.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 对于分布不确定性的情况，Dirichlet 分布的密度分布在整个单纯形上。
- en: '![Refer to caption](img/f6eeb1b7a56bd28a07a4622dd6eeff81.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f6eeb1b7a56bd28a07a4622dd6eeff81.png)'
- en: (f) Alternative approach to distributional uncertainty called representation
    gap, with density concentrated along the edges.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: （f）另一种称为表示间隙的分布不确定性方法，其密度集中在边缘。
- en: 'Figure 4: Examples of the probability simplex for a $K=3$ classification problem,
    where every corner corresponds to a class and every point to a Categorical distribution.
    Brighter colors correspond to higher density. (a) Predicted Categorical distributions
    by an ensemble of discriminators. (b) – (e) (Desired) Behavior of Dirichlet in
    different scenarios by Malinin & Gales ([2018](#bib.bib115)): (b) For a confident
    prediction, the density is concentrated in the corner of the simplex corresponding
    to the assumed class. (c) In the case of aleatoric uncertainty, the density is
    concentrated in the center, and thus uniform Categorical distributions are most
    likely. (d) In the case of model uncertainty, the density may still be concentrated
    in a corner, but more spread out, expressing the uncertainty about the right prediction.
    (e) In the case of an OOD input, a uniform Dirichlet expresses that any Categorical
    distribution is equally likely, since there is no evidence for any known class.
    (f) Representation gap by Nandy et al. ([2020](#bib.bib133)), proposed as an alternative
    behavior for OOD data. Here, the density is instead concentrated solely on the
    edges of the simplex.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：$K=3$分类问题的概率单纯形示例，其中每个角对应一个类别，每个点对应一个类别分布。较亮的颜色表示较高的密度。（a）通过一组判别器预测的类别分布。（b）–（e）Malinin
    & Gales（[2018](#bib.bib115)）中Dirichlet在不同场景下的（期望的）行为：（b）对于有信心的预测，密度集中在与假定类别对应的单纯形角落。（c）在随机不确定性的情况下，密度集中在中心，因此均匀类别分布最有可能。（d）在模型不确定性的情况下，密度可能仍集中在角落，但分布更广，表示对正确预测的不确定性。（e）在OOD输入的情况下，均匀Dirichlet表示任何类别分布的可能性相同，因为没有任何已知类别的证据。（f）Nandy等人（[2020](#bib.bib133)）提出的表示间隙，作为OOD数据的替代行为。在这里，密度则完全集中在单纯形的边缘。
- en: 'Let us now turn our attention to how to estimate the aleatoric, epistemic and
    distributional uncertainty as laid out in [Section 2.2](#S2.SS2 "2.2 Predictive
    Uncertainty in Neural Networks ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation") within
    the Dirichlet framework. In [Figure 4](#S3.F4 "In 3.3 Uncertainty Estimation with
    Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"), we show different shapes of a Dirichlet distribution parameterized
    by a neural network, corresponding to different cases of uncertainty, where each
    point on the simplex represents a Categorical distribution, with proximity to
    a corner indicating a high probability for the corresponding class. [Figure 4(a)](#S3.F4.sf1
    "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") displays the predictions of
    an ensemble of classifiers as a point cloud on the simplex. Using a Dirichlet,
    this finite set of distributions can be extended to a continuous density over
    the whole simplex. As we will see in the following sections, parameterizing a
    Dirichlet distribution with a neural network enables us to distinguish different
    scenarios using the shape of its density, as shown in [Figures 4(b)](#S3.F4.sf2
    "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), [4(c)](#S3.F4.sf3 "Figure
    4(c) ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), [4(d)](#S3.F4.sf4 "Figure
    4(d) ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), [4(e)](#S3.F4.sf5 "Figure
    4(e) ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") and [4(f)](#S3.F4.sf6 "Figure
    4(f) ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"), which we will discuss in more
    detail along the way.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '现在让我们关注如何在Dirichlet框架内估计**随机**、**认识**和**分布**不确定性，如在[第2.2节](#S2.SS2 "2.2 Predictive
    Uncertainty in Neural Networks ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")中所述。在[图4](#S3.F4
    "In 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")中，我们展示了由神经网络参数化的Dirichlet分布的不同形状，这些形状对应于不同的不确定性情况，其中单纯形上的每个点代表一个**类别分布**，靠近角落的点表示对应类别的高概率。[图4(a)](#S3.F4.sf1
    "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")显示了分类器集成的预测结果作为单纯形上的点云。利用Dirichlet，这些有限分布集可以扩展到整个单纯形上的连续密度。正如我们将在接下来的章节中看到的，通过神经网络参数化Dirichlet分布使我们能够使用其密度的形状来区分不同的场景，如[图4(b)](#S3.F4.sf2
    "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")、[4(c)](#S3.F4.sf3 "Figure 4(c)
    ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")、[4(d)](#S3.F4.sf4 "Figure 4(d)
    ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")、[4(e)](#S3.F4.sf5 "Figure 4(e)
    ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")和[4(f)](#S3.F4.sf6 "Figure 4(f)
    ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation")所示，我们将在过程中更详细地讨论。'
- en: However, since we do not want to inspect Dirichlets visually, we instead use
    closed form expression to quantify uncertainty, which we will discuss now. Although
    stated for the prior parameters $\bm{\alpha}$, the following methods can also
    be applied to the posterior parameters $\bm{\beta}$ without loss of generality.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们不希望直观地检查Dirichlets，我们改为使用封闭形式的表达式来量化不确定性，这一点我们将接下来讨论。尽管对先验参数 $\bm{\alpha}$
    进行了说明，但以下方法也可以同样应用于后验参数 $\bm{\beta}$，而不失一般性。
- en: Data (aleatoric) uncertainty
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据（随机）不确定性
- en: 'To obtain a measure of data uncertainty, we can evaluate the expected entropy
    of the data distribution $p(y|\bm{\pi})$ (similar to previous works like e.g.
    [Gal & Ghahramani](#bib.bib45), [2016](#bib.bib45)). As the entropy captures the
    “peakiness” of the output distribution, a lower entropy indicates that the model
    is concentrating most probability mass on a single class, while high entropy characterizes
    a more uniform distribution—the model is undecided about the right prediction.
    For Dirichlet networks, this quantity has a closed-form solution (for the full
    derivation, refer to [Section D.1](#A4.SS1 "D.1 Derivation of Expected Entropy
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation")):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '为了获得数据不确定性的度量，我们可以评估数据分布 $p(y|\bm{\pi})$ 的期望熵（类似于之前的研究，例如 [Gal & Ghahramani](#bib.bib45),
    [2016](#bib.bib45)）。由于熵捕捉了输出分布的“尖锐度”，较低的熵表明模型将大多数概率质量集中在单一类别上，而较高的熵则表征更均匀的分布——模型对正确预测没有明确的判断。对于Dirichlet网络，这个量有一个封闭形式的解（有关完整的推导，请参见
    [Section D.1](#A4.SS1 "D.1 Derivation of Expected Entropy ‣ Appendix D Additional
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")）：'
- en: '|  | $\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\psi(\alpha_{k}+1)-\psi(\alpha_{0}+1)\bigg{)}$
    |  | (12) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\psi(\alpha_{k}+1)-\psi(\alpha_{0}+1)\bigg{)}$
    |  | (12) |'
- en: where $\psi$ denotes the digamma function, defined as $\psi(x)=\frac{d}{dx}\log\Gamma(x)$,
    and $H$ the Shannon entropy.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\psi$ 表示 digamma 函数，定义为 $\psi(x)=\frac{d}{dx}\log\Gamma(x)$，而 $H$ 表示香农熵。
- en: Model (epistemic) uncertainty
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型（认知）不确定性
- en: 'As we saw in [Section 2.2](#S2.SS2 "2.2 Predictive Uncertainty in Neural Networks
    ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation"), most approaches in the Dirichlet framework
    avoid the intractable integral over network parameters $\bm{\theta}$ by using
    a point estimate $\hat{\bm{\theta}}$.⁷⁷7With exceptions such as Haussmann et al.
    ([2019](#bib.bib58)); Zhao et al. ([2020](#bib.bib189)). When the distribution
    over parameters in [Equation 5](#S2.E5 "In 2.3 Evidential Deep Learning ‣ 2 Background
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") is retained, alternate expressions of the aleatoric and
    epistemic uncertainty are derived by Woo ([2022](#bib.bib180)). This means that
    computing the model uncertainty via the weight posterior $p(\bm{\theta}|\mathbb{D})$
    like in Blundell et al. ([2015](#bib.bib13)); Gal & Ghahramani ([2016](#bib.bib45));
    Smith & Gal ([2018](#bib.bib153)) is not possible. Nevertheless, a key property
    of Dirichlet networks is that epistemic uncertainty is expressed in the spread
    of the Dirichlet distribution (for instance in [Figure 4](#S3.F4 "In 3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") (d) and (e)). Therefore, the epistemic uncertainty can
    be quantified considering the concentration parameters $\bm{\alpha}$ that shape
    this distribution: Charpentier et al. ([2020](#bib.bib16)) simply consider the
    maximum $\alpha_{k}$ as a score akin to the maximum probability score by Hendrycks
    & Gimpel ([2017](#bib.bib64)), while Sensoy et al. ([2018](#bib.bib146)) compute
    it by $K/\sum_{k=1}^{K}(\alpha_{k}+1)$ or simply $\alpha_{0}$ (Charpentier et al.,
    [2020](#bib.bib16)). In both cases, the underlying intuition is that larger $\alpha_{k}$
    produce a sharper density, and thus indicate increased confidence in a prediction.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第2.2节](#S2.SS2 "2.2 神经网络中的预测不确定性 ‣ 2 背景 ‣ 先验和后验网络：证据深度学习方法在不确定性估计中的调查")中看到的那样，大多数在Dirichlet框架下的方法通过使用点估计$\hat{\bm{\theta}}$来避免对网络参数$\bm{\theta}$的不可处理的积分。⁷⁷7一些例外，如Haussmann等人([2019](#bib.bib58))和Zhao等人([2020](#bib.bib189))。当在[方程5](#S2.E5
    "在2.3 证据深度学习 ‣ 2 背景 ‣ 先验和后验网络：证据深度学习方法在不确定性估计中的调查")中保持参数分布时，Woo ([2022](#bib.bib180))推导出了随机和认知不确定性的替代表达。这意味着像Blundell等人([2015](#bib.bib13))；Gal
    & Ghahramani ([2016](#bib.bib45))；Smith & Gal ([2018](#bib.bib153))那样通过权重后验$p(\bm{\theta}|\mathbb{D})$来计算模型不确定性是不可能的。然而，Dirichlet网络的一个关键特性是认知不确定性体现在Dirichlet分布的分散性中（例如在[图4](#S3.F4
    "在3.3 Dirichlet网络的不确定性估计 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：证据深度学习方法在不确定性估计中的调查") (d)和(e)中）。因此，可以考虑塑造这一分布的浓度参数$\bm{\alpha}$来量化认知不确定性：Charpentier等人([2020](#bib.bib16))简单地将最大$\alpha_{k}$视为类似于Hendrycks
    & Gimpel ([2017](#bib.bib64))的最大概率分数的评分，而Sensoy等人([2018](#bib.bib146))通过$K/\sum_{k=1}^{K}(\alpha_{k}+1)$或简单地$\alpha_{0}$
    (Charpentier等人，[2020](#bib.bib16))进行计算。在这两种情况下，其基本直觉是更大的$\alpha_{k}$产生更尖锐的密度，从而表示对预测的信心增加。
- en: Distributional uncertainty
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分布不确定性
- en: 'Another appealing property of this model family is being able to distinguish
    uncertainty due to model underspecification ([Figure 4(d)](#S3.F4.sf4 "In Figure
    4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")) from uncertainty due to unknown
    inputs ([Figure 4(e)](#S3.F4.sf5 "In Figure 4 ‣ 3.3 Uncertainty Estimation with
    Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")). In the Dirichlet framework, the distributional uncertainty can
    be quantified by computing the difference between the total amount of uncertainty
    and the data uncertainty, which can be expressed through the mutual information
    between the label $y$ and its Categorical distribution $\bm{\pi}$:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '这个模型家族的另一个吸引人的特性是能够区分由于模型不足指定（[Figure 4(d)](#S3.F4.sf4 "In Figure 4 ‣ 3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")）与由于未知输入（[Figure 4(e)](#S3.F4.sf5 "In Figure 4 ‣ 3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")）所造成的不确定性。在Dirichlet框架中，分布不确定性可以通过计算总不确定性与数据不确定性之间的差异来量化，这可以通过标签$y$与其类别分布$\bm{\pi}$之间的互信息来表达：'
- en: '|  | $I\Big{[}y,\bm{\pi}\Big{&#124;}\mathbf{x},\mathbb{D}\Big{]}=\underbrace{H\bigg{[}\mathbb{E}_{#1}\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}}_{\text{Total
    Uncertainty}}-\underbrace{\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}}_{\text{Data
    Uncertainty}}$ |  | (13) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $I\Big{[}y,\bm{\pi}\Big{&#124;}\mathbf{x},\mathbb{D}\Big{]}=\underbrace{H\bigg{[}\mathbb{E}_{#1}\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}}_{\text{总不确定性}}-\underbrace{\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}}_{\text{数据不确定性}}$
    |  | (13) |'
- en: 'This quantity expresses how much information we would receive about $\bm{\pi}$
    if we were given the label $y$, conditioned on the new input $\mathbf{x}$ and
    the training data $\mathbb{D}$. In regions in which the model is well-defined,
    receiving $y$ should not provide much new information about $\bm{\pi}$—and thus
    the mutual information would be low. Yet, such knowledge should be very informative
    in regions in which few data have been observed, and there this mutual information
    would indicate higher distributional uncertainty. Given that $\mathbb{E}_{#1}[\pi_{k}]=\frac{\alpha_{k}}{\alpha_{0}}$
    ([Section C.1](#A3.SS1 "C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")) and assuming the point estimate
    $p(\bm{\pi}|\mathbf{x},\mathbb{D})\approx p(\bm{\pi}|\mathbf{x},\hat{\bm{\theta}})$
    to be sufficient (Malinin & Gales, [2018](#bib.bib115)), we obtain an expression
    very similar to [Equation 12](#S3.E12 "In Data (aleatoric) uncertainty ‣ 3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation"):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '这个量度表示，如果给定标签$y$，在新的输入$\mathbf{x}$和训练数据$\mathbb{D}$的条件下，我们将获得多少关于$\bm{\pi}$的信息。在模型定义良好的区域，接收$y$不应提供太多关于$\bm{\pi}$的新信息——因此互信息会较低。然而，在观测到的数据很少的区域，这种知识应非常有用，这里的互信息会指示更高的分布不确定性。鉴于$\mathbb{E}_{#1}[\pi_{k}]=\frac{\alpha_{k}}{\alpha_{0}}$
    ([Section C.1](#A3.SS1 "C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"))并假设点估计$p(\bm{\pi}|\mathbf{x},\mathbb{D})\approx
    p(\bm{\pi}|\mathbf{x},\hat{\bm{\theta}})$足够（Malinin & Gales, [2018](#bib.bib115)），我们得到一个与[Equation
    12](#S3.E12 "In Data (aleatoric) uncertainty ‣ 3.3 Uncertainty Estimation with
    Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")非常相似的表达式：'
- en: '|  | $I\Big{[}y,\bm{\pi}\Big{&#124;}\mathbf{x},\mathbb{D}\Big{]}=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\log\frac{\alpha_{k}}{\alpha_{0}}-\psi(\alpha_{k}+1)+\psi(\alpha_{0}+1)\bigg{)}$
    |  | (14) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $I\Big{[}y,\bm{\pi}\Big{&#124;}\mathbf{x},\mathbb{D}\Big{]}=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\log\frac{\alpha_{k}}{\alpha_{0}}-\psi(\alpha_{k}+1)+\psi(\alpha_{0}+1)\bigg{)}$
    |  | (14) |'
- en: Note on epistemic uncertainty estimation
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于认识不确定性估计的说明
- en: 'The introduction of distributional uncertainty, a notion that is non-existent
    in the Bayesian Model Averaging framework, warrants a note on the estimation of
    epistemic uncertainty in general. Firstly, since we often use the point estimate
    $p(\bm{\theta}|\mathbb{D})\approx\delta(\bm{\theta}-\hat{\bm{\theta}})$ from [Equation 5](#S2.E5
    "In 2.3 Evidential Deep Learning ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation") in Evidential
    Deep Learning, model uncertainty usually is no longer estimated via the uncertainty
    in the weight posterior, but instead through the parameters of the prior or posterior
    distribution. Furthermore, even though they appear similar, distributional uncertainty
    is different from epistemic uncertainty, since it is the uncertainty in the distribution
    $p(\bm{\pi}|\mathbf{x},\bm{\theta})$. Distinguishing epistemic from distributional
    uncertainty also allows us to differentiate uncertainty due to underspecification
    from uncertainty due to a lack of evidence. In BMA, these notions are indistinguishable:
    In theory, model uncertainty on OOD data should be high since the model is underspecified
    on them, however theoretical and empirical work has shown this is not always the
    case (Ulmer et al., [2020](#bib.bib162); Ulmer & Cinà, [2021](#bib.bib161); Van Landeghem
    et al., [2022](#bib.bib168)). Even then, the additive decomposition of the mutual
    information has been critized since the model will also have a great deal of *uncertainty
    about its aleatoric uncertainty* in the beginning of the training process (Hüllermeier,
    [2022](#bib.bib74)), and thus this decomposition might not be accurate. Furthermore,
    even when we obtain the best possible model within its hypothesis class, using
    the discussed methods it is impossible to estimate uncertainty induced by a misspecified
    hypothesis class. This can motivate approaches in which a second, auxiliary model
    directly predicts model uncertainty of a target model (Lahlou et al., [2022](#bib.bib97);
    Zerva et al., [2022](#bib.bib187)).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 引入分布不确定性，这在贝叶斯模型平均（BMA）框架中并不存在，值得对一般的认知不确定性估计进行说明。首先，由于我们通常使用来自[方程 5](#S2.E5
    "在 2.3 证据深度学习 ‣ 2 背景 ‣ 先验与后验网络：关于不确定性估计的证据深度学习方法综述")的点估计 $p(\bm{\theta}|\mathbb{D})\approx\delta(\bm{\theta}-\hat{\bm{\theta}})$，在证据深度学习中，模型不确定性通常不再通过权重后验的不确定性来估计，而是通过先验或后验分布的参数来估计。此外，尽管它们看起来类似，但分布不确定性与认知不确定性不同，因为它是分布
    $p(\bm{\pi}|\mathbf{x},\bm{\theta})$ 中的不确定性。区分认知不确定性与分布不确定性也使我们能够区分由于规格不足造成的不确定性与由于证据不足造成的不确定性。在
    BMA 中，这些概念是无法区分的：理论上，OOD 数据上的模型不确定性应该很高，因为模型在这些数据上规格不足，但理论和实证工作已经表明这并非总是如此（Ulmer
    et al., [2020](#bib.bib162); Ulmer & Cinà, [2021](#bib.bib161); Van Landeghem
    et al., [2022](#bib.bib168)）。即便如此，互信息的加法分解也受到批评，因为模型在训练过程开始时对其*随机不确定性的极大不确定性*，因此这种分解可能并不准确。此外，即使我们在其假设类内获得了最佳模型，使用讨论的方法也无法估计由错误指定的假设类引起的不确定性。这可以激励一种方法，即第二个辅助模型直接预测目标模型的模型不确定性（Lahlou
    et al., [2022](#bib.bib97); Zerva et al., [2022](#bib.bib187)）。
- en: 3.4 Existing Approaches for Dirichlet Networks
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 现有的狄利克雷网络方法
- en: 'Being able to quantify aleatoric, epistemic and distributional uncertainty
    in a single forward pass and in closed form are desirable traits, as they simplify
    the process of obtaining different uncertainty scores. However, it is important
    to note that the behavior of the Dirichlet distributions in [Figure 4](#S3.F4
    "In 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") is idealized. In the usual way of
    training neural networks through empirical risk minimization, Dirichlet networks
    are not incentivized to behave in the depicted way. Thus, when comparing existing
    approaches for parameterizing Dirichlet priors in [Section 3.4.1](#S3.SS4.SSS1
    "3.4.1 Prior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") and posteriors in [Section 3.4.2](#S3.SS4.SSS2
    "3.4.2 Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3
    Evidential Deep Learning for Classification ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"),⁸⁸8Even
    though the term *prior* and *posterior network* were coined by Malinin & Gales
    ([2018](#bib.bib115)) and Charpentier et al. ([2020](#bib.bib16)) for their respective
    approaches, we use them in the following as an umbrella term for all methods targeting
    a prior or posterior distribution. we mainly focus on the different ways in which
    authors try to tackle this problem by means of loss functions and training procedures.
    We give an overview over the discussed works in [Tables 1](#S3.T1 "In 3.4.1 Prior
    Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep
    Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") and [2](#S3.T2 "Table 2 ‣ 3.4.2
    Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") in these respective sections.
    For additional details, we refer the reader to [Appendix C](#A3 "Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") for general derivations concerning
    the Dirichlet distribution. We dedicate [Appendix D](#A4 "Appendix D Additional
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") to derivations of the different
    loss functions and regularizers and give a detailed overview over their mathematical
    forms in [Appendix E](#A5 "Appendix E Overview over Loss Functions Appendix ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation"). Available code repositories for all works surveyed are
    listed in [Section A.2](#A1.SS2 "A.2 Code Availability ‣ Appendix A Code Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在单次前向传递和封闭形式中量化**随机性、不确定性**和**分布性不确定性**是期望的特征，因为这简化了获得不同不确定性评分的过程。然而，需要注意的是，[图4](#S3.F4
    "在3.3 使用Dirichlet网络进行不确定性估计 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：关于用于不确定性估计的证据深度学习方法的调查")中Dirichlet分布的行为是理想化的。在通过经验风险最小化训练神经网络的常规方法中，Dirichlet网络并没有激励其以所描绘的方式行为。因此，在比较[3.4.1节](#S3.SS4.SSS1
    "3.4.1 先验网络 ‣ 3.4 Dirichlet网络的现有方法 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：关于用于不确定性估计的证据深度学习方法的调查")中的Dirichlet先验和[3.4.2节](#S3.SS4.SSS2
    "3.4.2 后验网络 ‣ 3.4 Dirichlet网络的现有方法 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：关于用于不确定性估计的证据深度学习方法的调查")中的后验时，⁸⁸8尽管术语*先验*和*后验网络*是由Malinin
    & Gales ([2018](#bib.bib115)) 和 Charpentier et al. ([2020](#bib.bib16)) 为其各自的方法创造的，我们在以下内容中将其作为所有针对先验或后验分布的方法的统称。我们主要关注作者通过损失函数和训练过程尝试解决这一问题的不同方式。我们在[表1](#S3.T1
    "在3.4.1 先验网络 ‣ 3.4 Dirichlet网络的现有方法 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：关于用于不确定性估计的证据深度学习方法的调查")和[表2](#S3.T2
    "表2 ‣ 3.4.2 后验网络 ‣ 3.4 Dirichlet网络的现有方法 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：关于用于不确定性估计的证据深度学习方法的调查")中概述了讨论的工作。有关更多详细信息，请参阅[附录C](#A3
    "附录C 基本推导附录 ‣ 先验和后验网络：关于用于不确定性估计的证据深度学习方法的调查")，其中包含关于Dirichlet分布的一般推导。我们将[附录D](#A4
    "附录D 附加推导附录 ‣ 先验和后验网络：关于用于不确定性估计的证据深度学习方法的调查")专门用于不同损失函数和正则化器的推导，并在[附录E](#A5 "附录E
    损失函数概述附录 ‣ 先验和后验网络：关于用于不确定性估计的证据深度学习方法的调查")中详细概述其数学形式。所有被调查工作的可用代码库列在[第A.2节](#A1.SS2
    "A.2 代码可用性 ‣ 附录A 代码附录 ‣ 先验和后验网络：关于用于不确定性估计的证据深度学习方法的调查")中。
- en: 3.4.1 Prior Networks
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 先验网络
- en: 'Table 1: Overview over prior networks for classification. $(*)$ OOD samples
    were created inspired by the approach of Liang et al. ([2018](#bib.bib105)). ID:
    Using in-distribution data samples.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：分类用先验网络的概述。 $(*)$ OOD 样本是受到 Liang 等（[2018](#bib.bib105)）方法启发而创建的。ID：使用在分布内的数据样本。
- en: '| Method | Loss function | Architecture | OOD-free training? |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 损失函数 | 架构 | OOD-自由训练？ |'
- en: '| Prior network (Malinin & Gales, [2018](#bib.bib115)) | ID KL w.r.t smoothed
    label & OOD KL w.r.t. uniform prior | MLP / CNN | ✗ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 先验网络（Malinin & Gales，[2018](#bib.bib115)） | 相对于平滑标签的 ID KL & 相对于均匀先验的 OOD
    KL | MLP / CNN | ✗ |'
- en: '| Prior networks (Malinin & Gales, [2019](#bib.bib116)) | Reverse KL of Malinin
    & Gales ([2018](#bib.bib115)) | CNN | ✗ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 先验网络（Malinin & Gales，[2019](#bib.bib116)） | Malinin & Gales ([2018](#bib.bib115))
    的反向 KL | CNN | ✗ |'
- en: '| Information Robust Dirichlet Networks (Tsiligkaridis, [2019](#bib.bib159))
    | $l_{p}$ norm w.r.t one-hot label & Approx. Rényi divergence w.r.t. uniform prior
    | CNN | ✓ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 信息鲁棒 Dirichlet 网络（Tsiligkaridis，[2019](#bib.bib159)） | 相对于 one-hot 标签的 $l_{p}$
    范数 & 相对于均匀先验的近似 Rényi 发散 | CNN | ✓ |'
- en: '| Dirichlet via Function Decomposition (Biloš et al., [2019](#bib.bib11)) |
    Uncertainty Cross-entropy & mean & variance regularizer | RNN | ✓ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Dirichlet 通过函数分解（Biloš 等，[2019](#bib.bib11)） | 不确定性交叉熵 & 均值 & 方差正则化器 | RNN
    | ✓ |'
- en: '| Prior network with PAC Regularization (Haussmann et al., [2019](#bib.bib58))
    | Negative log-likelihood loss + PAC regularizer | BNN | ✓ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 带有 PAC 正则化的先验网络（Haussmann 等，[2019](#bib.bib58)） | 负对数似然损失 + PAC 正则化器 | BNN
    | ✓ |'
- en: '| Ensemble Distribution Distillation (Malinin et al., [2020b](#bib.bib118))
    | Knowledge distillation objective | MLP / CNN | ✓ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 集成分布蒸馏（Malinin 等，[2020b](#bib.bib118)） | 知识蒸馏目标 | MLP / CNN | ✓ |'
- en: '| Self-Distribution Distillation (Fathullah & Gales, [2022](#bib.bib39)) |
    Knowledge distillation objective | CNN | ✓ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 自我分布蒸馏（Fathullah & Gales，[2022](#bib.bib39)） | 知识蒸馏目标 | CNN | ✓ |'
- en: '| Prior networks with representation gap (Nandy et al., [2020](#bib.bib133))
    | ID & OOD Cross-entropy + precision regularizer | MLP / CNN | ✗ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 带有表示差距的先验网络（Nandy 等，[2020](#bib.bib133)） | ID & OOD 交叉熵 + 精度正则化器 | MLP /
    CNN | ✗ |'
- en: '| Prior RNN (Shen et al., [2020](#bib.bib151)) | Cross-entropy + entropy regularizer
    | RNN | (✗)^∗ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 先验 RNN（Shen 等，[2020](#bib.bib151)） | 交叉熵 + 熵正则化器 | RNN | (✗)^∗ |'
- en: '| Graph-based Kernel Dirichlet distribution estimation (GKDE) (Zhao et al.,
    [2020](#bib.bib189)) | $l_{2}$ norm w.r.t. one-hot label & KL reg. with node-level
    distance prior & Knowledge distillation objective | GNN | ✓ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 基于图的核 Dirichlet 分布估计（GKDE）（Zhao 等，[2020](#bib.bib189)） | 相对于 one-hot 标签的
    $l_{2}$ 范数 & 节点级距离先验的 KL 正则化 & 知识蒸馏目标 | GNN | ✓ |'
- en: 'The key challenge in training Dirichlet networks is to ensure both high classification
    performance and the intended behavior under OOD inputs. For this reason, most
    discussed works follow a loss function design using two parts: One optimizing
    for task accuracy to achieve the former goal, the other optimizing for a flat
    Dirichlet distribution, as flatness suggests a lack of evidence. To enforce flatness,
    the predicted Dirichlet is compared to a uniform distribution using some probabilistic
    divergence measure. We divide prior networks into two groups: Approaches using
    additional OOD data for this purpose (*OOD-dependent approaches*), and those which
    do not required OOD data (*OOD-free approaches*), as listed in [Table 1](#S3.T1
    "In 3.4.1 Prior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3
    Evidential Deep Learning for Classification ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 Dirichlet 网络的关键挑战在于确保高分类性能和在 OOD 输入下的预期行为。因此，大多数讨论的工作采用了使用两个部分的损失函数设计：一个用于优化任务准确性以实现前者目标，另一个用于优化平坦的
    Dirichlet 分布，因为平坦性表明缺乏证据。为了强制实现平坦性，预测的 Dirichlet 与均匀分布进行比较，使用某种概率发散度量。我们将先验网络分为两组：用于此目的的额外
    OOD 数据的方法（*依赖 OOD 的方法*），以及那些不需要 OOD 数据的方法（*无 OOD 的方法*），如 [表 1](#S3.T1 "在 3.4.1
    先验网络 ‣ 3.4 现有的 Dirichlet 网络方法 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：针对不确定性估计的证据深度学习方法综述") 中列出。
- en: OOD-free approaches
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: OOD-自由方法
- en: 'Apart from a standard negative log-likelihood loss (NLL) as used by Haussmann
    et al. ([2019](#bib.bib58)), one simple approach to optimizing the model is to
    impose a $l_{p}$-loss between the one-hot encoding $\mathbf{y}$ of the original
    label $y$ and the Categorical distribution $\bm{\pi}$. Tsiligkaridis ([2019](#bib.bib159))
    show that since the values of $\bm{\pi}$ depend directly on the predicted concentration
    parameters $\bm{\alpha}$, a generalized loss can be derived to be upper-bounded
    by the following expression (see the full derivation given in [Section D.3](#A4.SS3
    "D.3 𝑙_∞ Norm Derivation ‣ Appendix D Additional Derivations Appendix ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '除了 Haussmann 等人（[2019](#bib.bib58)）使用的标准负对数似然损失（NLL）之外，一种简单的优化模型的方法是对原始标签 $y$
    的独热编码 $\mathbf{y}$ 和分类分布 $\bm{\pi}$ 施加 $l_{p}$-损失。Tsiligkaridis（[2019](#bib.bib159)）展示了由于
    $\bm{\pi}$ 的值直接依赖于预测的集中参数 $\bm{\alpha}$，因此可以推导出一种广义的损失，其上界由以下表达式给出（详见[Section
    D.3](#A4.SS3 "D.3 𝑙_∞ Norm Derivation ‣ Appendix D Additional Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation)")）：'
- en: '|  | $\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{p}\big{]}\leq\bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bigg{)}^{\frac{1}{p}}\bBigg@{4}(\frac{\Gamma\Big{(}\sum_{k\neq
    y}\alpha_{k}+p\Big{)}}{\Gamma\Big{(}\sum_{k\neq y}\alpha_{k}\Big{)}}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})^{\frac{1}{p}}$ |  |
    (15) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{p}\big{]}\leq\bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bigg{)}^{\frac{1}{p}}\bBigg@{4}(\frac{\Gamma\Big{(}\sum_{k\neq
    y}\alpha_{k}+p\Big{)}}{\Gamma\Big{(}\sum_{k\neq y}\alpha_{k}\Big{)}}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})^{\frac{1}{p}}$ |  |
    (15) |'
- en: 'Since the sum over concentration parameters excludes the one corresponding
    to the true label, this loss can be seen as reducing the density on the areas
    of the probability simplex that do not correspond to the target class. Sensoy
    et al. ([2018](#bib.bib146)) specifically utilize the $l_{2}$ loss, which has
    the following form (see [Section D.4](#A4.SS4 "D.4 𝑙₂ Norm Loss Derivation ‣ Appendix
    D Additional Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation")):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '由于集中参数的和排除了对应于真实标签的那个，这种损失可以被视为减少了在概率单纯形的那些不对应于目标类别的区域上的密度。Sensoy 等人（[2018](#bib.bib146)）特别利用了
    $l_{2}$ 损失，其形式如下（详见[Section D.4](#A4.SS4 "D.4 𝑙₂ Norm Loss Derivation ‣ Appendix
    D Additional Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation)")）：'
- en: '|  | $\mathbb{E}_{#1}\Big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{2}^{2}\Big{]}=\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\alpha_{k}}{\alpha_{0}}\Big{)}^{2}+\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    |  | (16) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{#1}\Big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{2}^{2}\Big{]}=\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\alpha_{k}}{\alpha_{0}}\Big{)}^{2}+\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    |  | (16) |'
- en: 'where $\mathbf{1}_{(\cdot)}$ denotes the indicator function. Since $\alpha_{k}/\alpha_{0}\leq
    1$, we can see that the term with the indicator functions penalizes the network
    when the concentration parameter $\alpha_{k}$ corresponding to the correct label
    does not exceed the others. The remaining aspect lies in the regularization: To
    achieve reliable predictive uncertainty, the density associated with incorrect
    classes should be reduced. One such option is to decrease the Kullback-Leibler
    divergence from a uniform Dirichlet (see [Section C.3](#A3.SS3 "C.3 Kullback-Leibler
    Divergence between two Dirichlets ‣ Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{1}_{(\cdot)}$ 表示指示函数。由于 $\alpha_{k}/\alpha_{0}\leq 1$，我们可以看到，指示函数的项在集中参数
    $\alpha_{k}$ 对应于正确标签时没有超过其他参数时，会对网络施加惩罚。剩下的方面在于正则化：为了实现可靠的预测不确定性，应该减少与错误类别相关的密度。一种选择是减少与均匀Dirichlet分布的Kullback-Leibler散度（见[Section
    C.3](#A3.SS3 "C.3 Kullback-Leibler Divergence between two Dirichlets ‣ Appendix
    C Fundamental Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation)")）：'
- en: '|  | $\text{KL}\Big{[}p(\bm{\pi}&#124;\bm{\alpha})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{1})\Big{]}=\log\frac{\Gamma(K)}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-1)\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (17) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{KL}\Big{[}p(\bm{\pi}&#124;\bm{\alpha})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{1})\Big{]}=\log\frac{\Gamma(K)}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-1)\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (17) |'
- en: 'In the case of Zhao et al. ([2020](#bib.bib189)), who apply their model to
    graph structures, they do not decrease the divergence from a uniform Dirichlet,
    but incorporate information about the local graph neighborhood into the reference
    distribution by considering the distance from and label of close nodes.⁹⁹9They
    also add another knowledge distillation term (Hinton et al., [2015](#bib.bib66))
    to their loss, for which the model tries to imitate the predictions of a vanilla
    Graph Neural Network that functions as the teacher network. Nevertheless, the
    KL-divergence w.r.t. a uniform Dirichlet is used by many of the following works.
    Other divergence measures are also possible: Tsiligkaridis ([2019](#bib.bib159))
    instead use a local approximation of the Rényi divergence.^(10)^(10)10The Kullback-Leibler
    divergence can be seen as a special case of the Rényi divergence (van Erven &
    Harremoës, [2014](#bib.bib167)), where the latter has a stronger information-theoretic
    underpinning. First, the concentration parameter for the correct class $\alpha_{y}$
    is removed from the Dirichlet by creating $\tilde{\bm{\alpha}}=(1-\mathbf{y})\cdot\bm{\alpha}+\mathbf{y}$.
    Then, the remaining concentration parameters are pushed towards uniformity by
    the divergence measure, which can be derived to be'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在赵等人（[2020](#bib.bib189)）将他们的模型应用于图结构的情况下，他们没有减少与均匀Dirichlet的散度，而是通过考虑与标签相近的节点的距离和标签，将关于局部图邻域的信息纳入参考分布。他们还在损失函数中添加了另一个知识蒸馏项（Hinton
    等人，[2015](#bib.bib66)），模型试图模仿一个作为教师网络的普通图神经网络的预测。尽管如此，许多后续工作仍使用了与均匀Dirichlet相关的KL散度。其他散度度量也是可能的：Tsiligkaridis（[2019](#bib.bib159)）则使用了Rényi散度的局部近似。Rényi散度可以看作是Kullback-Leibler散度的特例（van
    Erven & Harremoës，[2014](#bib.bib167)），后者具有更强的信息论基础。首先，通过创建 $\tilde{\bm{\alpha}}=(1-\mathbf{y})\cdot\bm{\alpha}+\mathbf{y}$
    将正确类别的集中参数 $\alpha_{y}$ 从Dirichlet中移除。然后，通过散度度量将剩余的集中参数推向均匀性，这可以推导为：
- en: '|  | $\text{R\''{e}nyi}\Big{[}p(\bm{\pi}&#124;\tilde{\bm{\alpha}})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{1})\Big{]}\approx\frac{1}{2}\Big{[}\sum_{k\neq
    y}\big{(}\alpha_{k}-1\big{)}^{2}\big{(}\psi^{(1)}(\alpha_{j})-\psi^{(1)}(\tilde{\alpha}_{0})\big{)}-\psi^{(1)}(\tilde{\alpha}_{0})\sum_{\begin{subarray}{c}k\neq
    k^{\prime}\\ k\neq y,\ k^{\prime}\neq y\end{subarray}}\big{(}\alpha_{k}-1\big{)}\big{(}\alpha_{k^{\prime}}-1\big{)}\Big{]}$
    |  | (18) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{R\''{e}nyi}\Big{[}p(\bm{\pi}\mid\tilde{\bm{\alpha}})\Big{\,\mid\,}p(\bm{\pi}\mid\bm{1})\Big{]}\approx\frac{1}{2}\Big{[}\sum_{k\neq
    y}\big{(}\alpha_{k}-1\big{)}^{2}\big{(}\psi^{(1)}(\alpha_{j})-\psi^{(1)}(\tilde{\alpha}_{0})\big{)}-\psi^{(1)}(\tilde{\alpha}_{0})\sum_{\begin{subarray}{c}k\neq
    k^{\prime}\\ k\neq y,\ k^{\prime}\neq y\end{subarray}}\big{(}\alpha_{k}-1\big{)}\big{(}\alpha_{k^{\prime}}-1\big{)}\Big{]}$
    |  | (18) |'
- en: 'where $\psi^{(1)}$ denotes the first-order polygamma function, defined as $\psi^{(1)}(x)=\frac{d}{dx}\psi(x)$.
    Since the sums ignore the concentration parameter of the correct class, only the
    ones of the incorrect classes are penalized. Haussmann et al. ([2019](#bib.bib58))
    derive an entirely different regularizer using Probably Approximately Correct
    (PAC) bounds from learning theory, that together with the negative log-likelihood
    gives a proven bound to the expected true risk of the classifier. Setting a scalar
    $\delta$ allows one to set the desired risk, i.e. the model’s expected risk is
    guaranteed to be the same or less than the derived PAC bound with a probability
    of $1-\delta$. For a problem with $N$ available training data points, the following
    *upper bound* is presented:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\psi^{(1)}$ 表示一阶多伽玛函数，定义为 $\psi^{(1)}(x)=\frac{d}{dx}\psi(x)$。由于这些求和忽略了正确类别的集中参数，仅对错误类别进行惩罚。Haussmann
    等人（[2019](#bib.bib58)）使用学习理论中的**可能大致正确**（PAC）界限推导出了一个完全不同的正则化器，该正则化器与负对数似然函数结合，给出了分类器期望真实风险的证明界限。设置一个标量
    $\delta$ 可以确定所需的风险，即模型的期望风险保证与推导出的 PAC 界限相同或更小，其概率为 $1-\delta$。对于具有 $N$ 个可用训练数据点的问题，给出了以下*上界*：
- en: '|  | $\sqrt{\frac{\text{KL}\big{[}p(\bm{\pi}&#124;\bm{\alpha})\big{&#124;}\big{&#124;}p(\bm{\pi}&#124;\mathbf{1})\big{]}-\log\delta}{N}-1}.$
    |  | (19) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sqrt{\frac{\text{KL}\big{[}p(\bm{\pi}\mid\bm{\alpha})\big{\,\mid\,}p(\bm{\pi}\mid\mathbf{1})\big{]}-\log\delta}{N}-1}.$
    |  | (19) |'
- en: This upper bound is then used as the actual regularizer term in practice. We
    see that even from the learning-theoretic perspective, this method follows the
    intuition of the original KL regularizer in a shifted and scaled form. Haussmann
    et al. ([2019](#bib.bib58)) also admit that in this form, the regularizer does
    not allow for a direct PAC interpretation anymore, since its approximates only
    admits a loose bound on the risk. Yet, they demonstrate its usefulness in their
    experiments. Summarizing all of the presented approaches thus far, we can see
    that they try to force the model to concentrate the Dirichlet’s density solely
    on the parameter corresponding to the right label—expecting a more flat density
    for difficult or unknown inputs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个上界在实际中被用作实际的正则化项。即使从学习理论的角度来看，这种方法也遵循了原始 KL 正则化器的直觉，只不过是以一种偏移和缩放的形式。Haussmann
    等人（[2019](#bib.bib58)）也承认，这种形式下，正则化器不再允许直接的 PAC 解释，因为它的近似仅对风险提供了一个松散的界限。然而，他们在实验中展示了它的有效性。总结到目前为止呈现的所有方法，我们可以看到它们试图迫使模型将
    Dirichlet 的密度仅集中在对应于正确标签的参数上——期望对困难或未知输入有一个更平坦的密度。
- en: Knowledge distillation
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: 'A way to avoid the use of OOD examples while still using external information
    for regularization is to use *knowledge distillation* (Hinton et al., [2015](#bib.bib66)).
    Here, the core idea lies in a student model learning to imitate the predictions
    of a more complex teacher model. Malinin et al. ([2020b](#bib.bib118)) exploit
    this idea and show that prior networks can also be distilled using an ensemble
    of classifiers and their predicted Categorical distributions (akin to learning
    [Figure 4(e)](#S3.F4.sf5 "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet
    Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    from [Figure 4(a)](#S3.F4.sf1 "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet
    Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")),
    which does not require regularization at all, but comes at the cost of having
    to train an entire ensemble a priori. Trying to solve this shortcoming, Fathullah
    & Gales ([2022](#bib.bib39)) propose to use a shared feature extractor between
    the student and the teacher network. Instead of training an ensemble, diverse
    predictions are obtained from the teacher network through the use of Gaussian
    dropout, which are distilled into a Dirichlet distribution as in Malinin et al.
    ([2020b](#bib.bib118)).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一种在使用外部信息进行正则化的同时避免使用 OOD 示例的方法是使用*知识蒸馏*（Hinton 等人，[2015](#bib.bib66)）。这里的核心思想在于学生模型学习模仿更复杂的教师模型的预测。Malinin
    等人（[2020b](#bib.bib118)）利用了这一思想，并展示了先验网络也可以使用分类器集成及其预测的分类分布（类似于学习[图 4(e)](#S3.F4.sf5
    "图 4 ‣ 3.3 使用 Dirichlet 网络进行不确定性估计 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的综述")中的[图
    4(a)](#S3.F4.sf1 "图 4 ‣ 3.3 使用 Dirichlet 网络进行不确定性估计 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的综述")，这完全不需要正则化，但却需要事先训练一个完整的集成。为了克服这一不足，Fathullah
    和 Gales（[2022](#bib.bib39)）建议在学生和教师网络之间使用共享特征提取器。通过使用高斯 dropout 从教师网络中获得多样化的预测，然后将其蒸馏为
    Dirichlet 分布，就像 Malinin 等人（[2020b](#bib.bib118)）所做的一样。
- en: OOD-dependent approaches
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: OOD 相关的方法
- en: 'A uniform Dirichlet in the face of unknown inputs can also be achieved explicitly
    by training with OOD inputs and learning to be uncertain on them. We discuss a
    series of works utilizing this direction next. Malinin & Gales ([2018](#bib.bib115))
    simply minimize the KL divergence to a uniform Dirichlet on OOD data points. This
    way, the model is encouraged to be agnostic about its prediction in the face of
    unknown inputs. Further, instead of an $l_{p}$ norm, they utilize another KL term
    to train the model on predicting the correct label, minimizing the distance between
    the predicted concentration parameters and the true label. However, since only
    a gold *label* and not a gold *distribution* is available, they create one by
    re-distributing some of the density from the correct class onto the rest of the
    simplex (see [Appendix E](#A5 "Appendix E Overview over Loss Functions Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") for full form). In their follow-up work, Malinin & Gales
    ([2019](#bib.bib116)) argue that the asymmetry of the KL divergence as the main
    objective creates undesirable properties in producing the correct behavior of
    the predicted Dirichlet, since it creates a multi- instead of unimodal target
    distribution. They therefore propose to use the reverse KL instead (see [Section D.5](#A4.SS5
    "D.5 Derivation of Reverse KL loss ‣ Appendix D Additional Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") for the derivation), which enforces the desired unimodal
    target. Nandy et al. ([2020](#bib.bib133)) refine this idea further, stating that
    even with reverse KL training high epistemic and high distributional uncertainty
    ([Figures 4(d)](#S3.F4.sf4 "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet
    Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    and [4(e)](#S3.F4.sf5 "Figure 4(e) ‣ Figure 4 ‣ 3.3 Uncertainty Estimation with
    Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")) might be confused, and instead propose novel loss functions producing
    a *representation gap* ([Figure 4(f)](#S3.F4.sf6 "In Figure 4 ‣ 3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")), which aims to be more easily distinguishable. In this
    case, spread out densities signify epistemic uncertainty, whereas densities concentrated
    entirely on the edges of the simplex indicate distributional uncertainty. The
    way they achieve this goal is two-fold: In addition to minimizing the negative
    log-likelihood loss on in-domain and maximizing the entropy on OOD examples, they
    also penalize the precision of the Dirichlet (see [Appendix E](#A5 "Appendix E
    Overview over Loss Functions Appendix ‣ Prior and Posterior Networks: A Survey
    on Evidential Deep Learning Methods For Uncertainty Estimation") for full form).
    Maximizing the entropy on OOD examples hereby serves the same function as minimizing
    the KL w.r.t. to a uniform distribution, and can be implemented using the closed-form
    solution in [Section C.2](#A3.SS2 "C.2 Entropy of Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '面对未知输入，均匀的 Dirichlet 分布也可以通过用 OOD 输入进行训练并学会对其保持不确定性来显式实现。我们接下来讨论一系列利用这一方向的工作。Malinin
    和 Gales ([2018](#bib.bib115)) 仅通过最小化 KL 散度到 OOD 数据点上的均匀 Dirichlet 分布来实现这一点。这样，模型在面对未知输入时被鼓励对其预测保持不可知性。此外，他们使用另一个
    KL 项来训练模型预测正确标签，而不是 $l_{p}$ 范数，从而最小化预测浓度参数与真实标签之间的距离。然而，由于只有黄金*标签*而不是黄金*分布*可用，他们通过将正确类别的一部分密度重新分布到其余的简单形上来创建一个分布（参见
    [附录 E](#A5 "Appendix E Overview over Loss Functions Appendix ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    获取完整形式）。在其后续工作中，Malinin 和 Gales ([2019](#bib.bib116)) 认为，KL 散度作为主要目标的不对称性会在生成预测
    Dirichlet 的正确行为时产生不良特性，因为它产生了一个多模态而非单峰目标分布。因此，他们提议使用逆 KL （参见 [第 D.5 节](#A4.SS5
    "D.5 Derivation of Reverse KL loss ‣ Appendix D Additional Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") 获取推导），以强制实现所需的单峰目标。Nandy 等人 ([2020](#bib.bib133)) 进一步改进了这一想法，指出即使使用逆
    KL 训练也可能会产生高的知识不确定性和高的分布不确定性 ([图 4(d)](#S3.F4.sf4 "In Figure 4 ‣ 3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") 和 [4(e)](#S3.F4.sf5 "Figure 4(e) ‣ Figure 4 ‣ 3.3 Uncertainty
    Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")) 可能会产生混淆，并提出了新的损失函数来产生一个 *表示差距* ([图 4(f)](#S3.F4.sf6
    "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"))，旨在更容易区分。在这种情况下，分布广泛的密度表示知识不确定性，而完全集中在简单形边缘的密度表示分布不确定性。他们实现这一目标的方式有两方面：除了最小化领域内负对数似然损失和最大化
    OOD 示例上的熵外，他们还惩罚 Dirichlet 的精度（参见 [附录 E](#A5 "Appendix E Overview over Loss Functions
    Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation") 获取完整形式）。在 OOD 示例上最大化熵的作用与最小化相对于均匀分布的 KL 散度相同，并且可以使用
    [第 C.2 节](#A3.SS2 "C.2 Entropy of Dirichlet ‣ Appendix C Fundamental Derivations
    Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation") 中的封闭形式解决方案实现。'
- en: '|  | $H\big{[}p(\bm{\pi}&#124;\bm{\alpha})\big{]}=\log B(\bm{\alpha})+(\alpha_{0}-K)\psi(\alpha_{0})-\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{k})$
    |  | (20) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $H\big{[}p(\bm{\pi}&#124;\bm{\alpha})\big{]}=\log B(\bm{\alpha})+(\alpha_{0}-K)\psi(\alpha_{0})-\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{k})$
    |  | (20) |'
- en: Sequential models
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 顺序模型
- en: 'We also have identified two sequential applications of prior networks in the
    literature: For Natural Language Processing, Shen et al. ([2020](#bib.bib151))
    train a recurrent neural network for spoken language understanding using a simple
    cross-entropy loss. Instead of using OOD examples for training, they maximize
    the entropy of the model on data inputs given a learned, noisy version of the
    predicted concentration parameters. In comparison, Biloš et al. ([2019](#bib.bib11))
    apply their model to asynchronous event classification and note that the standard
    cross-entropy loss only involves a point estimate of a Categorical distribution,
    discarding all the information contained in the predicted Dirichlet. For this
    reason, they propose an *uncertainty-aware* cross-entropy (UCE) loss instead,
    which has a closed-form solution in the Dirichlet case (see [Section D.6](#A4.SS6
    "D.6 Uncertainty-aware Cross-Entropy Loss ‣ Appendix D Additional Derivations
    Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation"))'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在文献中发现了前置网络的两个连续应用：对于自然语言处理，沈等（[2020](#bib.bib151)）使用简单的交叉熵损失训练了一个递归神经网络用于口语理解。与使用
    OOD 示例进行训练不同，他们通过给定一个学习到的、带噪声的预测浓度参数版本，最大化模型在数据输入上的熵。相比之下，Biloš 等（[2019](#bib.bib11)）将他们的模型应用于异步事件分类，并指出标准的交叉熵损失仅涉及分类分布的点估计，忽略了预测的狄利克雷分布中包含的所有信息。因此，他们提出了一种*不确定性感知*的交叉熵（UCE）损失，这在狄利克雷情况下有一个封闭形式的解（见
    [D.6节](#A4.SS6 "D.6 不确定性感知交叉熵损失 ‣ 附录 D 附加推导附录 ‣ 前置和后置网络：不确定性估计的证据深度学习方法概述")）
- en: '|  | $\mathcal{L}_{\text{UCE}}=\psi(\alpha_{y})-\psi(\alpha_{0}),$ |  | (21)
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{UCE}}=\psi(\alpha_{y})-\psi(\alpha_{0}),$ |  | (21)
    |'
- en: with $\psi$ referring to the digamma function. By mimizing the difference between
    the digamma values of $\alpha_{y}$ and $\alpha_{0}$, the model learns to concentrate
    density on the correct class. Since their final concentration parameters are created
    using additional information from a class-specific Gaussian process, they further
    regularize the mean and variance for OOD data points using an extra loss term,
    incentivizing a loss mean and a variance corresponding to a pre-defined hyperparameter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\psi$ 指的是 digamma 函数。通过最小化 $\alpha_{y}$ 和 $\alpha_{0}$ 的 digamma 值之间的差异，模型学习将密度集中在正确的类别上。由于他们的最终浓度参数是利用来自特定类别的高斯过程的额外信息创建的，他们进一步对
    OOD 数据点的均值和方差进行正则化，使用额外的损失项来激励均值和方差与预定义的超参数相对应。
- en: 3.4.2 Posterior Networks
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 后置网络
- en: 'Table 2: Overview over posterior networks for classification. OOD data is created
    using $(\dagger)$ the fast-sign gradient method (Kurakin et al., [2017](#bib.bib96)),
    a $(\ddagger)$ Variational Auto-Encoder (VAE; [Kingma & Welling](#bib.bib88),
    [2014](#bib.bib88)) or $(\mathsection)$ a Wasserstein GAN (WGAN; [Arjovsky et al.](#bib.bib4),
    [2017](#bib.bib4)). NLL: Negative log-likelihood. CE: Cross-entropy.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：分类后置网络的概述。OOD 数据是使用 $(\dagger)$ 快速符号梯度方法（Kurakin 等， [2017](#bib.bib96)）、$(\ddagger)$
    变分自编码器（VAE；[Kingma & Welling](#bib.bib88)，[2014](#bib.bib88)）或 $(\mathsection)$
    Wasserstein GAN（WGAN；[Arjovsky 等](#bib.bib4)，[2017](#bib.bib4)）创建的。NLL：负对数似然。CE：交叉熵。
- en: '| Method | Loss function | Architecture | OOD-free training? |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 损失函数 | 体系结构 | 无 OOD 训练？ |'
- en: '| Evidential Deep Learning (Sensoy et al., [2018](#bib.bib146)) | $l_{2}$ norm
    w.r.t. one-hot label + KL w.r.t. uniform prior | CNN | ✓ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 证据深度学习 (Sensoy 等， [2018](#bib.bib146)) | 针对 one-hot 标签的 $l_{2}$ 范数 + 相对于均匀先验的
    KL | CNN | ✓ |'
- en: '| Regularized ENN (Zhao et al., [2019](#bib.bib188)) | $l_{2}$ norm w.r.t.
    one-hot label + Uncertainty regularizer on OOD/ difficult samples | MLP / CNN
    | ✗ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 正则化 ENN (赵等， [2019](#bib.bib188)) | 针对 one-hot 标签的 $l_{2}$ 范数 + 对 OOD/困难样本的不确定性正则化器
    | MLP / CNN | ✗ |'
- en: '| WGAN–ENN (Hu et al., [2021](#bib.bib71)) | $l_{2}$ norm w.r.t. one-hot label
    + Uncertainty regularizer on synth. OOD | MLP / CNN + WGAN | (✗)^§ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| WGAN–ENN (胡等， [2021](#bib.bib71)) | 针对 one-hot 标签的 $l_{2}$ 范数 + 对合成 OOD 的不确定性正则化器
    | MLP / CNN + WGAN | (✗)^§ |'
- en: '| Variational Dirichlet (Chen et al., [2018](#bib.bib18)) | ELBO + Contrastive
    Adversarial Loss | CNN | (✗)^† |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 变分狄利克雷 (陈等， [2018](#bib.bib18)) | ELBO + 对抗性对比损失 | CNN | (✗)^† |'
- en: '| Dirichlet Meta-Model (Shen et al., [2022](#bib.bib150)) | ELBO + KL w.r.t.
    uniform prior | CNN | ✓ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 狄利克雷元模型 (沈等， [2022](#bib.bib150)) | ELBO + 相对于均匀先验的 KL | CNN | ✓ |'
- en: '| Belief Matching (Joo et al., [2020](#bib.bib82)) | ELBO | CNN | ✓ |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 信念匹配（Joo 等，[2020](#bib.bib82)） | ELBO | CNN | ✓ |'
- en: '| Posterior Networks (Charpentier et al., [2020](#bib.bib16)) | Uncertainty
    CE (Biloš et al., [2019](#bib.bib11)) + Entropy regularizer | MLP / CNN + Norm.
    Flow | ✓ |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 后验网络（Charpentier 等，[2020](#bib.bib16)） | 不确定性 CE（Biloš 等，[2019](#bib.bib11)）+
    熵正则化器 | MLP / CNN + 规范流 | ✓ |'
- en: '| Graph Posterior Networks (Stadler et al., [2021](#bib.bib155)) | Same as
    Charpentier et al. ([2020](#bib.bib16)) | GNN | ✓ |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 图后验网络（Stadler 等，[2021](#bib.bib155)） | 同 Charpentier 等（[2020](#bib.bib16)）
    | GNN | ✓ |'
- en: '| Generative Evidential Neural Networks (Sensoy et al., [2020](#bib.bib147))
    | Contrastive NLL + KL between uniform & Dirichlet of wrong classes | CNN | (✗)^‡
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 生成证据神经网络（Sensoy 等，[2020](#bib.bib147)） | 对比均匀与错误类别的 Dirichlet 的 NLL + KL
    | CNN | (✗)^‡ |'
- en: 'As elaborated on in [Section 3.1](#S3.SS1 "3.1 The Dirichlet distribution ‣
    3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), choosing
    a Dirichlet prior, due to its conjugacy to the Categorical distribution, induces
    a Dirichlet posterior distribution. Like the prior before, surveyed works listed
    in [Table 2](#S3.T2 "In 3.4.2 Posterior Networks ‣ 3.4 Existing Approaches for
    Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation") parameterize the posterior with a neural network. The challenges
    hereby are two-fold: Accounting for the number of class observations $N_{k}$ that
    make up part of the posterior density parameters $\bm{\beta}$ ([Equation 10](#S3.E10
    "In 3.1 The Dirichlet distribution ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")), and, similarly to prior networks, ensuring the wanted
    behavior on the probability simplex for in- and out-of-distribution inputs. Sensoy
    et al. ([2018](#bib.bib146)) base their approach on the Dempster-Shafer theory
    of evidence (Yager & Liu, [2008](#bib.bib183); lending its name to the term “Evidential
    Deep Learning”) and its formalization via subjective logic (Audun, [2018](#bib.bib5)),
    where subjective beliefs about probabilities are expressed through Dirichlet distributions.
    In doing so, an agnostic belief in form of a uniform Dirichlet prior $\forall
    k:\alpha_{k}=1$ is updated using pseudo-counts $N_{k}$, which are predicted by
    a neural network. This is different from prior networks, where the prior concentration
    parameters $\bm{\alpha}$ are predicted instead. In both cases, this does not require
    any modification to a model’s architecture except for replacing the softmax output
    function by a ReLU (or similar). Sensoy et al. ([2018](#bib.bib146)) for instance
    train their model using the same techniques presented in the previous section:
    The main objective is the $l_{2}$ loss, penalizing the difference between the
    predicted Dirichlet and the one-hot encoded class label ([Section D.4](#A4.SS4
    "D.4 𝑙₂ Norm Loss Derivation ‣ Appendix D Additional Derivations Appendix ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")), and the KL divergence from a uniform Dirichlet is used for regularization.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第3.1节](#S3.SS1 "3.1 Dirichlet分布 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")中详细说明，由于Dirichlet先验与Categorical分布的共轭性，选择Dirichlet先验会引入Dirichlet后验分布。与之前的先验一样，[表2](#S3.T2
    "在3.4.2后验网络 ‣ 3.4 Dirichlet网络的现有方法 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")中列出的研究工作通过神经网络对后验进行参数化。这里的挑战有两个方面：考虑构成后验密度参数$\bm{\beta}$的类别观测数$N_{k}$（[方程10](#S3.E10
    "在3.1 Dirichlet分布 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")），以及类似于先验网络的，确保对输入的概率单纯形的期望行为。Sensoy等人（[2018](#bib.bib146)）基于Dempster-Shafer证据理论（Yager
    & Liu，[2008](#bib.bib183)；赋予“证据深度学习”这一术语名字）以及通过主观逻辑（Audun，[2018](#bib.bib5)）的形式化，在该理论中，关于概率的主观信念通过Dirichlet分布表示。这样，对形式为均匀Dirichlet先验的不可知信念$\forall
    k:\alpha_{k}=1$，使用神经网络预测的伪计数$N_{k}$进行更新。这不同于先验网络，其中先验浓度参数$\bm{\alpha}$被预测。在这两种情况下，这不需要对模型架构进行任何修改，只需将softmax输出函数替换为ReLU（或类似函数）。例如，Sensoy等人（[2018](#bib.bib146)）使用上一节中介绍的相同技术训练他们的模型：主要目标是$l_{2}$损失，惩罚预测的Dirichlet与one-hot编码类别标签之间的差异（[第D.4节](#A4.SS4
    "D.4 𝑙₂范数损失推导 ‣ 附录D 额外推导附录 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")），KL散度则用于正则化。
- en: Generating OOD samples using generative models
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用生成模型生成OOD样本
- en: Since OOD examples are not always readily available, several works try to create
    artificial samples using deep generative models. Hu et al. ([2021](#bib.bib71))
    train a Wasserstein GAN (Arjovsky et al., [2017](#bib.bib4)) to generate OOD samples,
    on which the network’s uncertainty is maximized. The uncertainty is given through
    *vacuity*, defined as $K/\sum_{k}\beta_{k}$. The vacuity compares a uniform prior
    belief against the amassed evidence $\sum_{k}\beta_{k}$, and thus is $1$ when
    there is no additonal evidence available. In a follow-up work, Sensoy et al. ([2020](#bib.bib147))
    similarly train a model using a contrastive loss with artificial OOD samples from
    a Variational Autoencoder (Kingma & Welling, [2014](#bib.bib88)), and a KL-based
    regularizer similar to that of Tsiligkaridis ([2019](#bib.bib159)), where the
    density for posterior concentration parameters $\beta_{k}$ that do not correspond
    to the true label are pushed to the uniform distribution.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OOD（域外）样本并不总是容易获得，一些研究尝试使用深度生成模型来创建人工样本。Hu 等人（[2021](#bib.bib71)）训练了一个 Wasserstein
    GAN（Arjovsky 等人，[2017](#bib.bib4)），以生成OOD样本，并最大化网络的不确定性。通过*vacuity*（空虚度）来表示不确定性，其定义为
    $K/\sum_{k}\beta_{k}$。空虚度比较了均匀先验信念与积累证据 $\sum_{k}\beta_{k}$，因此在没有额外证据可用时为空虚度 $1$。在后续的工作中，Sensoy
    等人（[2020](#bib.bib147)）类似地使用来自变分自编码器（Kingma & Welling，[2014](#bib.bib88)）的人工OOD样本，通过对比损失训练模型，并使用类似于
    Tsiligkaridis（[2019](#bib.bib159)）的基于KL的正则化器，其中与真实标签不对应的后验集中参数 $\beta_{k}$ 的密度被推向均匀分布。
- en: '![Refer to caption](img/aa47c0fbc42381a0abf6ccf494a3c00c.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/aa47c0fbc42381a0abf6ccf494a3c00c.png)'
- en: (a) Posterior Network (Charpentier et al., [2020](#bib.bib16)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 后验网络（Charpentier 等，[2020](#bib.bib16)）。
- en: '![Refer to caption](img/d6a86af5774681a556a67f9520ad1417.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d6a86af5774681a556a67f9520ad1417.png)'
- en: (b) Natural Posterior Network (Charpentier et al., [2022](#bib.bib17)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 自然后验网络（Charpentier 等人，[2022](#bib.bib17)）。
- en: 'Figure 5: Schematic of the Posterior Network and Natural Posterior Network,
    taken from Charpentier et al. ([2020](#bib.bib16); [2022](#bib.bib17)), respectively.
    In both cases, an encoder $f_{\bm{\theta}}$ maps inputs to a latent representation
    $\mathbf{z}$. NFs then model the latent densities, which are used together with
    the prior concentration to produce the posterior parameters. In (a), the latent
    representation of $\mathbf{x}^{(1)}$ lies right in the modelled density of the
    first class, and thus receives a confident prediction. The latent $\mathbf{z}^{(2)}$
    lies between densities, creating aleatoric uncertainty. $\mathbf{x}^{(3)}$ is
    an OOD input, is mapped to a low-density area of the latent space and thus produces
    an uncertain prediction. The differences in the two approaches is that the Posterior
    Network in (a) uses one NF per class, while only one NF is used in (b). Furthermore,
    (b) constitutes a generalization to different exponential family distributions,
    and is not restricted to classification problems (see main text for more detail).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：后验网络和自然后验网络的示意图，分别取自 Charpentier 等人（[2020](#bib.bib16)；[2022](#bib.bib17)）。在这两种情况下，编码器
    $f_{\bm{\theta}}$ 将输入映射到潜在表示 $\mathbf{z}$。NFs（归一化流）然后对潜在密度进行建模，这些密度与先验浓度一起用于生成后验参数。在
    (a) 中，$\mathbf{x}^{(1)}$ 的潜在表示正好位于第一个类别的建模密度中，因此获得了有信心的预测。潜在 $\mathbf{z}^{(2)}$
    位于密度之间，造成了偶然的不确定性。$\mathbf{x}^{(3)}$ 是一个OOD输入，映射到潜在空间的低密度区域，因此产生了不确定的预测。这两种方法的不同之处在于
    (a) 中的后验网络为每个类别使用一个NF，而 (b) 中只使用一个NF。此外，(b) 还推广到不同的指数族分布，并不局限于分类问题（有关更多细节，请参见正文）。
- en: Posterior networks via Normalizing Flows
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过归一化流的后验网络
- en: 'Charpentier et al. ([2020](#bib.bib16)) also set $\bm{\alpha}$ to a uniform
    prior, but obtain the pseudo-observations $N_{k}$ in a different way: Instead
    of a model predicting them directly, $N_{k}$ is determined by the number of examples
    of a certain class in the training set. This quantity is further modified in the
    following way: An encoder model $f_{\bm{\theta}}$ produces a latent representation
    $\mathbf{z}$ of some input. A (class-specific) normalizing flow^(11)^(11)11A NF
    is a generative model, estimating a density in the feature space by mapping it
    to a Gaussian in a latent space by a series of invertible, bijective transformations.
    The probability of an input can then be estimated by calculating the probability
    of its latent encoding under that Gaussian and applying the change-of-variable
    formula, traversing the flow in reverse. Instead of mapping from the feature space
    into latent space, the flows in Charpentier et al. ([2020](#bib.bib16)) map from
    the encoder latent space into a separate, second latent space. (NF; [Rezende &
    Mohamed](#bib.bib145), [2015](#bib.bib145)) with parameters $\bm{\phi}$ then assigns
    a probability to this latent representation, which is used to weight $N_{k}$:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Charpentier 等人 ([2020](#bib.bib16)) 也将 $\bm{\alpha}$ 设置为均匀先验，但以不同的方式获得伪观察值 $N_{k}$：不是通过模型直接预测它们，而是由训练集中某个类别的样本数量决定
    $N_{k}$。这个数量进一步以以下方式进行修改：一个编码模型 $f_{\bm{\theta}}$ 生成输入的潜在表示 $\mathbf{z}$。一个（特定类别的）归一化流^(11)^(11)11A
    NF 是一种生成模型，通过一系列可逆的双射变换，将特征空间映射到潜在空间中的高斯分布，从而估计特征空间中的密度。然后，通过计算潜在编码在该高斯下的概率并应用变量变换公式，反向遍历流，可以估计输入的概率。Charpentier
    等人 ([2020](#bib.bib16)) 中的流不是从特征空间映射到潜在空间，而是将编码器潜在空间映射到一个单独的第二潜在空间。具有参数 $\bm{\phi}$
    的 NF; [Rezende & Mohamed](#bib.bib145), [2015](#bib.bib145)) 然后为该潜在表示分配一个概率，用于加权
    $N_{k}$：
- en: '|  | $\beta_{k}=\alpha_{k}+N_{k}\cdot p(\mathbf{z}&#124;y=k,\bm{\phi});\quad\mathbf{z}=f_{\bm{\theta}}(\mathbf{x}).$
    |  | (22) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\beta_{k}=\alpha_{k}+N_{k}\cdot p(\mathbf{z}&#124;y=k,\bm{\phi});\quad\mathbf{z}=f_{\bm{\theta}}(\mathbf{x}).$
    |  | (22) |'
- en: 'This has the advantage of producing low probabilities for strange inputs like
    the noise as depicted in [Figure 5(a)](#S3.F5.sf1 "In Figure 5 ‣ Generating OOD
    samples using generative models ‣ 3.4.2 Posterior Networks ‣ 3.4 Existing Approaches
    for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"), which in turn translate to low concentration parameters of the posterior
    Dirichlet, as it falls back onto the uniform prior. The model is optimized using
    the same uncertainty-aware cross-entropy loss as in Biloš et al. ([2019](#bib.bib11))
    with an additional entropy regularizer, encouraging density only around the correct
    class. This scheme is also applied to Graph Neural Networks by Stadler et al.
    ([2021](#bib.bib155)): In order to take the neighborhood structure of the graph
    into account, the authors also use a Personalized Page Rank scheme to diffuse
    node-specific posterior parameters $\bm{\beta}$ between neighboring nodes. The
    Page Rank scores, reflecting the importance of a neighboring node to the current
    node, can be approximated using power iteration (Klicpera et al., [2019](#bib.bib89))
    and used to aggregate the originally predicted concentration parameters $\bm{\beta}$
    on a per-node basis.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的好处是对像 [图 5(a)](#S3.F5.sf1 "在图 5 ‣ 使用生成模型生成 OOD 样本 ‣ 3.4.2 后验网络 ‣ 3.4 Dirichlet
    网络的现有方法 ‣ 3 证据深度学习分类 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查") 中所示的噪声等奇怪输入产生低概率，从而转化为后验
    Dirichlet 的低浓度参数，因为它回退到均匀先验。该模型使用与 Biloš 等人 ([2019](#bib.bib11)) 相同的不确定性感知交叉熵损失进行优化，并增加了熵正则化器，鼓励仅在正确类别周围形成密度。这一方案也被
    Stadler 等人 ([2021](#bib.bib155)) 应用于图神经网络：为了考虑图的邻域结构，作者们还使用了个性化的 Page Rank 方案，将节点特定的后验参数
    $\bm{\beta}$ 在邻近节点之间进行扩散。Page Rank 分数反映了邻近节点对当前节点的重要性，可以通过幂迭代 (Klicpera 等人，[2019](#bib.bib89))
    近似，并用于在每个节点的基础上聚合最初预测的浓度参数 $\bm{\beta}$。
- en: A generalization of the posterior network method to exponential family distributions
    is given by Charpentier et al. ([2022](#bib.bib17)). Akin to the update for the
    posterior Dirichlet parameters, the authors formulate a general Bayesian update
    rule as
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Charpentier 等人 ([2022](#bib.bib17)) 对后验网络方法在指数族分布中的推广进行了阐述。类似于后验 Dirichlet 参数的更新，作者们制定了一般贝叶斯更新规则为
- en: '|  | $\bm{\chi}_{i}^{\text{post}}=\frac{n^{\text{prior}}\bm{\chi}^{\text{prior}}+n_{i}\bm{\chi}_{i}}{n^{\text{prior}}+n_{i}};\quad\mathbf{z}_{i}=f_{\bm{\theta}}(\mathbf{x}_{i});\quad
    n_{i}=N\cdot p(\mathbf{z}&#124;\bm{\phi});\quad\bm{\chi}_{i}=g_{\bm{\psi}}(\mathbf{x}_{i}).$
    |  | (23) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{\chi}_{i}^{\text{post}}=\frac{n^{\text{prior}}\bm{\chi}^{\text{prior}}+n_{i}\bm{\chi}_{i}}{n^{\text{prior}}+n_{i}};\quad\mathbf{z}_{i}=f_{\bm{\theta}}(\mathbf{x}_{i});\quad
    n_{i}=N\cdot p(\mathbf{z}|\bm{\phi});\quad\bm{\chi}_{i}=g_{\bm{\psi}}(\mathbf{x}_{i}).$
    |  | (23) |'
- en: '$\bm{\chi}$ here denotes the parameters of the exponential family distribution
    and $n$ the evidence. Thus, posterior parameters for a sample $\mathbf{x}_{i}$
    are obtained by updating the prior parameter and some prior evidence by some input-dependent
    pseudo-evidence $n_{i}$ and parameters $\bm{\chi}_{i}$: Again, given a latent
    representation by an encoder $\mathbf{z}$, a (this time single) normalizing flow
    predicts $n_{i}=N_{H}\cdot p(\mathbf{z}|\bm{\phi})$ based on some pre-defined
    certainty budget $N_{H}$.^(12)^(12)12The certainty budget can simply be set to
    the number of available datapoints, however Charpentier et al. ([2022](#bib.bib17))
    suggest to set it to $\log N_{H}=\frac{1}{2}\big{(}H\log(2\pi)+\log(H+1)\big{)}$
    to better scale with the dimensionality $H$ of the latent space. The update parameters
    $\bm{\chi}_{i}$ are predicted by an additional network $\bm{\chi}_{i}=g_{\bm{\psi}}(\mathbf{z})$,
    see [Figure 5(b)](#S3.F5.sf2 "In Figure 5 ‣ Generating OOD samples using generative
    models ‣ 3.4.2 Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks
    ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). For
    classification, $n^{\text{prior}}=1$ and $\bm{\chi}^{\text{prior}}$ corresponds
    to the uniform Dirichlet, while $\bm{\chi}_{i}$ are concentration parameters predicted
    by an output layer based on the latent encoding. For unfamiliar inputs, this method
    will again result in a small pseudo-evidence term $n_{i}$, reflecting high model
    uncertainty. Since the generalization to the exponential family implies the application
    of this scheme to normal distributions, we will discuss the same method applied
    to regression in the next section.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的$\bm{\chi}$表示指数族分布的参数，$n$表示证据。因此，样本$\mathbf{x}_{i}$的后验参数通过用一些输入相关的伪证据$n_{i}$和参数$\bm{\chi}_{i}$更新先验参数和一些先验证据获得：同样，给定一个由编码器$\mathbf{z}$表示的潜在表示，一个（这次是单一的）归一化流基于某个预定义的确定性预算$N_{H}$预测$n_{i}=N_{H}\cdot
    p(\mathbf{z}|\bm{\phi})$。^(12)^(12)12 确定性预算可以简单地设为可用数据点的数量，但Charpentier等人（[2022](#bib.bib17)）建议将其设置为$\log
    N_{H}=\frac{1}{2}\big{(}H\log(2\pi)+\log(H+1)\big{)}$以更好地与潜在空间的维度$H$进行尺度匹配。更新参数$\bm{\chi}_{i}$由额外的网络$\bm{\chi}_{i}=g_{\bm{\psi}}(\mathbf{z})$预测，见[图5(b)](#S3.F5.sf2
    "在图5中 ‣ 使用生成模型生成OOD样本 ‣ 3.4.2 后验网络 ‣ 3.4 Dirichlet网络的现有方法 ‣ 3 分类的证据深度学习 ‣ 先验和后验网络：对不确定性估计的证据深度学习方法的调查")。对于分类任务，$n^{\text{prior}}=1$，而$\bm{\chi}^{\text{prior}}$对应于均匀Dirichlet，而$\bm{\chi}_{i}$是基于潜在编码由输出层预测的浓度参数。对于不熟悉的输入，该方法将再次导致小的伪证据项$n_{i}$，反映出较高的模型不确定性。由于对指数族的推广意味着将该方案应用于正态分布，我们将在下一节讨论应用于回归的相同方法。
- en: Posterior networks via variational inference
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过变分推断的后验网络
- en: 'Another route lies in directly parameterizing the posterior parameters $\bm{\beta}$.
    Given a target distribution defined by a uniform Dirichlet prior plus the number
    of times an input is associated with a specific label, Chen et al. ([2018](#bib.bib18))
    optimize a distribution matching objective, i.e. the KL-divergence between the
    posterior parameters predicted by a neural network and the target distribution.
    Since this objective is intractable to optimize directly, this leaves us to instead
    model an *approximate posterior* using variational inference methods. As the KL
    divergence between the true and approximate posterior is infeasible to estimate,
    variational methods usually optimize the *evidence lower bound* (ELBO):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是直接对后验参数$\bm{\beta}$进行参数化。陈等人（[2018](#bib.bib18)）给定一个由均匀Dirichlet先验和输入与特定标签相关的次数定义的目标分布，通过优化分布匹配目标，即神经网络预测的后验参数与目标分布之间的KL散度。由于该目标难以直接优化，因此我们转而使用变分推断方法建模一个*近似后验*。由于真实后验与近似后验之间的KL散度难以估计，变分方法通常优化*证据下界*（ELBO）：
- en: '|  | $\mathcal{L}_{\text{ELBO}}=\underbrace{\psi(\beta_{y})-\psi(\beta_{0})}_{\text{UCE
    loss}}-\underbrace{\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}}_{\text{KL-divergence}}$
    |  | (24) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{ELBO}}=\underbrace{\psi(\beta_{y})-\psi(\beta_{0})}_{\text{UCE
    损失}}-\underbrace{\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}}_{\text{KL-散度}}$
    |  | (24) |'
- en: 'in which we can identify to consist of the uncertainty-aware cross-entropy
    (UCE) loss used by Biloš et al. ([2019](#bib.bib11)); Charpentier et al. ([2020](#bib.bib16);
    [2022](#bib.bib17)) and the KL-divergence between two Dirichlets ([Section C.3](#A3.SS3
    "C.3 Kullback-Leibler Divergence between two Dirichlets ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")). This approach is also employed
    by Joo et al. ([2020](#bib.bib82)), Chen et al. ([2018](#bib.bib18)) and Shen
    et al. ([2022](#bib.bib150)), while the latter predict posterior parameters based
    on the activations of different layers of a pre-trained feature extractor.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '在其中，我们可以识别出由 Biloš 等（ [2019](#bib.bib11)）使用的不确定性感知交叉熵（UCE）损失；Charpentier 等（
    [2020](#bib.bib16)； [2022](#bib.bib17)）和两个 Dirichlets 之间的KL-散度（ [Section C.3](#A3.SS3
    "C.3 Kullback-Leibler Divergence between two Dirichlets ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")）。这种方法也被 Joo 等（ [2020](#bib.bib82)）、Chen
    等（ [2018](#bib.bib18)）和 Shen 等（ [2022](#bib.bib150)）采用，其中后者基于预训练特征提取器的不同层的激活预测后验参数。'
- en: 4 Evidential Deep Learning for Regression
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 证据深度学习用于回归
- en: 'Table 3: Overview over Evidential Deep Learning methods for regression.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：回归的证据深度学习方法概述。
- en: '| Method | Parameterized distribution | Loss function | Model |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 参数化分布 | 损失函数 | 模型 |'
- en: '| Deep Evidential Regression (Amini et al., [2020](#bib.bib1)) | Normal-Inverse
    Gamma Prior | Negative log-likelihood loss + KL w.r.t. uniform prior | MLP / CNN
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 深度证据回归（Amini 等， [2020](#bib.bib1)） | 正态-逆伽马先验 | 负对数似然损失 + 相对于均匀先验的KL | MLP
    / CNN |'
- en: '| Deep Evidential Regression with Multi-task Learning (Oh & Shin, [2022](#bib.bib136))
    | Normal-Inverse Gamma Prior | Like Amini et al. ([2020](#bib.bib1)), with additional
    Lipschitz-modified MSE loss | MLP / CNN |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 多任务学习的深度证据回归（Oh & Shin， [2022](#bib.bib136)） | 正态-逆伽马先验 | 类似于 Amini 等（ [2020](#bib.bib1)），增加了
    Lipschitz 修改的均方误差损失 | MLP / CNN |'
- en: '| Multivariate Deep Evidential Regression (Meinert & Lavin, [2021](#bib.bib123))
    | Normal-Inverse Wishart Prior | Like Amini et al. ([2020](#bib.bib1)), but tying
    two predicted params. instead of using a regularizer | MLP |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 多变量深度证据回归（Meinert & Lavin， [2021](#bib.bib123)） | 正态-逆威沙特先验 | 类似于 Amini 等（
    [2020](#bib.bib1)），但通过将两个预测参数绑定在一起而不是使用正则化器 | MLP |'
- en: '| Regression Prior Network (Malinin et al., [2020a](#bib.bib117)) | Normal-Wishart
    Prior | Reverse KL (Malinin & Gales, [2019](#bib.bib116)) | MLP / CNN |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 回归先验网络（Malinin 等， [2020a](#bib.bib117)） | 正态-威沙特先验 | 逆KL（Malinin & Gales，
    [2019](#bib.bib116)） | MLP / CNN |'
- en: '| Natural Posterior Network (Charpentier et al., [2022](#bib.bib17)) | Inverse-$\chi^{2}$
    Posterior | Uncertainty Cross-entropy (Biloš et al., [2019](#bib.bib11)) + Entropy
    regularizer | MLP / CNN + Norm. Flow |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 自然后验网络（Charpentier 等， [2022](#bib.bib17)） | 逆-$\chi^{2}$ 后验 | 不确定性交叉熵（Biloš
    等， [2019](#bib.bib11)）+ 熵正则化器 | MLP / CNN |'
- en: 'Because the EDL framework provides convenient uncertainty estimation, the question
    naturally arises of whether it can be extended to regression problems as well.
    The answer is affirmative, although the Dirichlet distribution is not an appropriate
    choice in this case. It is very common to model a regression problem using a normal
    likelihood ([Bishop](#bib.bib12), [2006](#bib.bib12); Chapter 3.3). As such, there
    are multiple potential choices for a prior distribution. The methods listed in
    [Table 3](#S4.T3 "In 4 Evidential Deep Learning for Regression ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    either choose the Normal-Inverse Gamma distribution (Amini et al., [2020](#bib.bib1);
    Charpentier et al., [2022](#bib.bib17)), inducing a scaled inverse-$\chi^{2}$
    posterior (Gelman et al., [1995](#bib.bib49)),^(13)^(13)13The form of the Normal-Inverse
    Gamma posterior and the Normal Inverse-$\chi^{2}$ posterior are interchangable
    using some parameter substitutions (Murphy, [2007](#bib.bib129)). or a Normal-Wishart
    prior (Malinin et al., [2020a](#bib.bib117)). We will discuss these approaches
    in turn.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因为EDL框架提供了方便的不确定性估计，问题自然地出现了是否也可以扩展到回归问题。答案是肯定的，尽管在这种情况下Dirichlet分布并不是一个合适的选择。使用正态似然建模回归问题是很常见的（[Bishop](#bib.bib12),
    [2006](#bib.bib12); 第3.3章）。因此，先验分布有多种潜在选择。[表3](#S4.T3 "在4 证据深度学习中的回归 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")中列出的方法要么选择正态逆伽马分布（Amini等人，[2020](#bib.bib1);
    Charpentier等人，[2022](#bib.bib17)），引入缩放的逆$\chi^{2}$后验（Gelman等人，[1995](#bib.bib49)），^(13)^(13)13正态逆伽马后验和正态逆$\chi^{2}$后验的形式可以通过一些参数替换互换（Murphy，[2007](#bib.bib129)）。要么选择正态-威沙特先验（Malinin等人，[2020a](#bib.bib117)）。我们将依次讨论这些方法。
- en: '![Refer to caption](img/6f3c218cd63a240aa5448cf25e820961.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6f3c218cd63a240aa5448cf25e820961.png)'
- en: 'Figure 6: Example of an application of Evidential Deep Learning for regression,
    taken from Amini et al. ([2020](#bib.bib1)). The neural network predicts an Normal
    Inverse-Gamma prior, whose corresponding normal likelihoods display decreasing
    variance (and thus uncertainty) in the face of stronger evidence.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：回归中应用证据深度学习的示例，取自Amini等人（[2020](#bib.bib1)）。神经网络预测一个正态逆伽马先验，其对应的正态似然在面对更强证据时，显示出方差（从而不确定性）的减少。
- en: Univariate regression
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 单变量回归
- en: 'Amini et al. ([2020](#bib.bib1)) model the regression problem as a normal distribution
    with unknown mean and variance $\mathcal{N}(y;\pi,\sigma^{2})$, and use a normal
    prior for the mean with $\pi\sim\mathcal{N}(\gamma,\sigma^{2}\nu^{-1})$ and an
    inverse Gamma prior for the variance with $\sigma^{2}\sim\Gamma^{-1}(\alpha,\beta)$,
    resulting in a combined Inverse-Gamma prior with parameters $\gamma,\nu,\alpha,\beta$,
    shown in [Figure 6](#S4.F6 "In 4 Evidential Deep Learning for Regression ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"). These are predicted by different “heads” of a neural network. For
    predictions, the expectation of the mean corresponds to $\mathbb{E}_{#1}[\pi]=\gamma$,
    and aleatoric and epistemic uncertainty can then be estimated using the expected
    value of the variance as well as the variance of the mean, respectively, which
    have closed form solutions under this parameterization:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Amini等人（[2020](#bib.bib1)）将回归问题建模为具有未知均值和方差的正态分布$\mathcal{N}(y;\pi,\sigma^{2})$，并使用均值的正态先验，$\pi\sim\mathcal{N}(\gamma,\sigma^{2}\nu^{-1})$，以及方差的逆伽马先验，$\sigma^{2}\sim\Gamma^{-1}(\alpha,\beta)$，最终得到一个包含参数$\gamma,\nu,\alpha,\beta$的组合逆伽马先验，如[图6](#S4.F6
    "在4 证据深度学习中的回归 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")所示。这些由神经网络的不同“头”进行预测。对于预测，均值的期望对应于$\mathbb{E}_{#1}[\pi]=\gamma$，然后可以使用方差的期望值以及均值的方差来估计偶然和知识不确定性，这在这种参数化下具有封闭形式的解决方案：
- en: '|  | $\mathbb{E}_{#1}[\sigma^{2}]=\frac{\beta}{\alpha-1};\quad\text{Var}[\pi]=\frac{\beta}{\nu(\alpha-1)}$
    |  | (25) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{#1}[\sigma^{2}]=\frac{\beta}{\alpha-1};\quad\text{Var}[\pi]=\frac{\beta}{\nu(\alpha-1)}$
    |  | (25) |'
- en: 'By choosing to optimize using a negative log-likelihood objective, we can actually
    evaluate the loss function analytically, since the likelihood function corresponds
    to a Student’s t-distribution with $\gamma$ degrees of freedom, mean $\beta(1+\nu)/(\nu\alpha)$
    and $2\alpha$ variance:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择使用负对数似然目标进行优化，我们实际上可以从解析上评估损失函数，因为似然函数对应于具有$\gamma$自由度的Student t分布，均值为$\beta(1+\nu)/(\nu\alpha)$，方差为$2\alpha$：
- en: '|  | $\mathcal{L}_{\text{NLL}}=\frac{1}{2}\log\Big{(}\frac{\pi}{\nu}\Big{)}-\alpha\log\Omega+\Big{(}\alpha+\frac{1}{2}\Big{)}\log\Big{(}(y_{i}-\gamma)^{2}\nu+\Omega\Big{)}+\log\bigg{(}\frac{\Gamma(\alpha)}{\Gamma(\alpha+\frac{1}{2})}\bigg{)}$
    |  | (26) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{NLL}}=\frac{1}{2}\log\Big{(}\frac{\pi}{\nu}\Big{)}-\alpha\log\Omega+\Big{(}\alpha+\frac{1}{2}\Big{)}\log\Big{(}(y_{i}-\gamma)^{2}\nu+\Omega\Big{)}+\log\bigg{(}\frac{\Gamma(\alpha)}{\Gamma(\alpha+\frac{1}{2})}\bigg{)}$
    |  | (26) |'
- en: 'using $\Omega=2\beta(1+\nu)$. Akin to the entropy regularizer for Dirichlet
    networks, Amini et al. ([2020](#bib.bib1)) propose a regularization term that
    only allows for concentrating density on the correct prediction:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用$\Omega=2\beta(1+\nu)$。类似于Dirichlet网络的熵正则化器，Amini等人([2020](#bib.bib1))提出了一种正则化项，仅允许将密度集中在正确的预测上：
- en: '|  | $\mathcal{L}_{\text{reg}}=&#124;y_{i}-\gamma&#124;\cdot(2\nu+\alpha)$
    |  | (27) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{reg}}=&#124;y_{i}-\gamma&#124;\cdot(2\nu+\alpha)$
    |  | (27) |'
- en: 'Since $\mathbb{E}_{#1}[\pi]=\gamma$ is the prediction of the network, the second
    term in the product will be scaled by the degree to which the current prediction
    deviates from the target value. Since $\nu$ and $\alpha$ control the variance
    of the mean and the variance of the normal likelihood, this term encourages the
    network to decrease the evidence for mispredicted data samples. As Amini et al.
    ([2020](#bib.bib1)) point out, large amounts of evidence are not punished in cases
    where the prediction is close to the target. However, Oh & Shin ([2022](#bib.bib136))
    argue that this combination of objectives might create adverse incentives for
    the model during training: Since the difference between the prediction and target
    in [Equation 26](#S4.E26 "In Univariate regression ‣ 4 Evidential Deep Learning
    for Regression ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation") is scaled by $\nu$, the model could learn
    to increase the predictive uncertainty by decreasing $\nu$ instead of improving
    its prediction. They propose to ameliorate this issue by using a third loss term
    of the form'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$\mathbb{E}_{#1}[\pi]=\gamma$是网络的预测值，乘积中的第二项将按当前预测与目标值的偏离程度进行缩放。由于$\nu$和$\alpha$控制均值的方差和正常似然的方差，这一项鼓励网络减少对错误预测数据样本的证据。正如Amini等人([2020](#bib.bib1))指出的那样，在预测接近目标的情况下，大量的证据不会受到惩罚。然而，Oh
    & Shin ([2022](#bib.bib136))则认为，这种目标组合可能在训练过程中对模型产生不利的激励：由于[方程26](#S4.E26 "在单变量回归
    ‣ 4 证据深度学习用于回归 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法调查")中的预测与目标之间的差异按$\nu$缩放，模型可能会通过降低$\nu$来增加预测不确定性，而不是改善预测。他们建议通过使用第三个形式的损失项来缓解这个问题。
- en: '|  | $\mathcal{L}_{\text{MSE}}=\begin{cases}(y_{i}-\gamma)^{2}&amp;\quad\text{if
    }(y_{i}-\gamma)^{2}<U_{\nu,\alpha}\\ 2\sqrt{U_{\nu,\alpha}}&#124;y_{i}-\gamma&#124;-U_{\nu,\alpha}&amp;\quad\text{if
    }(y_{i}-\gamma)^{2}\geq U_{\nu,\alpha}\end{cases}$ |  | (28) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{MSE}}=\begin{cases}(y_{i}-\gamma)^{2}&amp;\quad\text{如果
    }(y_{i}-\gamma)^{2}<U_{\nu,\alpha}\\ 2\sqrt{U_{\nu,\alpha}}&#124;y_{i}-\gamma&#124;-U_{\nu,\alpha}&amp;\quad\text{如果
    }(y_{i}-\gamma)^{2}\geq U_{\nu,\alpha}\end{cases}$ |  | (28) |'
- en: where $U_{\nu,\alpha}=\min(U_{\nu},U_{\alpha})$ denotes the minimum value for
    the uncertainty thresholds for $\nu,\alpha$ given over a mini-batch, which are
    themselves defined as
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$U_{\nu,\alpha}=\min(U_{\nu},U_{\alpha})$表示在一个小批量上给定的$\nu,\alpha$的不确定性阈值的最小值，它们本身被定义为
- en: '|  | $U_{\nu}=\frac{\beta(\nu+1)}{\alpha\nu};\quad U_{\alpha}=\frac{2\beta(\nu+1)}{\nu}\Big{[}\exp\Big{(}\psi\Big{(}\alpha+\frac{1}{2}\Big{)}-\psi(\alpha))-1\Big{)}\Big{]}.\\
    $ |  | (29) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $U_{\nu}=\frac{\beta(\nu+1)}{\alpha\nu};\quad U_{\alpha}=\frac{2\beta(\nu+1)}{\nu}\Big{[}\exp\Big{(}\psi\Big{(}\alpha+\frac{1}{2}\Big{)}-\psi(\alpha))-1\Big{)}\Big{]}.\\
    $ |  | (29) |'
- en: 'These expression are obtained by taking the derivatives $\partial\mathcal{L}_{\text{NLL}}/\partial\nu$,
    $\partial\mathcal{L}_{\text{NLL}}/\partial\alpha$ and solving for the parameters,
    thus giving us the values for $\nu$ and $\alpha$ for which the loss gradients
    are maximal. In combination with [Equation 28](#S4.E28 "In Univariate regression
    ‣ 4 Evidential Deep Learning for Regression ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), [Equation 29](#S4.E29
    "In Univariate regression ‣ 4 Evidential Deep Learning for Regression ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation") ensures that, should the model error exceed $U_{\nu,\alpha}$, the
    error is rescaled. Thus, this rescaling bounds the Lipschitz constant of the loss
    function, motivating the model to ensure the correctness of its prediction, since
    its ability to increase uncertainty to decrease its loss is now limited.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表达式是通过对$\partial\mathcal{L}_{\text{NLL}}/\partial\nu$、$\partial\mathcal{L}_{\text{NLL}}/\partial\alpha$进行求导并解算参数得到的，从而给出使损失梯度最大化的$\nu$和$\alpha$值。结合[方程28](#S4.E28
    "在单变量回归 ‣ 4 证据深度学习回归 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")，[方程29](#S4.E29 "在单变量回归 ‣
    4 证据深度学习回归 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")确保如果模型误差超过$U_{\nu,\alpha}$，则误差会被重新缩放。因此，这种重新缩放限制了损失函数的Lipschitz常数，促使模型确保其预测的正确性，因为其通过增加不确定性来减少损失的能力现在已被限制。
- en: Posterior networks for regression
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 回归的后验网络
- en: 'Another approach for regression is the Natural Posterior Network by Charpentier
    et al. ([2022](#bib.bib17)), which was already discussed for classification in
    [Section 3.4.2](#S3.SS4.SSS2 "3.4.2 Posterior Networks ‣ 3.4 Existing Approaches
    for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation"). But since the proposed approach is a generalization for exponential
    family distributions, it can be applied to regression as well, using a Normal
    likelihood and Normal Inverse-Gamma prior. The Bayesian update rule in [Equation 23](#S3.E23
    "In Posterior networks via Normalizing Flows ‣ 3.4.2 Posterior Networks ‣ 3.4
    Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") is adapted as follows: $n$ is set to $n=\lambda=2\alpha$,
    and $\bm{\chi}=\big{[}\pi_{0}\ |\ \pi_{0}^{2}+2\beta/n\big{]}^{T}$. Feeding an
    input into the natural posterior network again first produces a latent encoding
    $\mathbf{z}$, from which a NF predicts $n_{i}=N_{H}\cdot p(\mathbf{z}|\bm{\phi})$,
    and an additional network produces $\bm{\chi}_{i}=g_{\bm{\psi}}(\mathbf{z})$,
    which are used in [Equation 23](#S3.E23 "In Posterior networks via Normalizing
    Flows ‣ 3.4.2 Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks
    ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation") to produce
    $\bm{\chi}^{\text{post}}$ and $n^{\text{post}}$, from which the parameters of
    the posterior Normal Inverse-Gamma can be derived. The authors also produce a
    general exponential family form of the UCE loss by Biloš et al. ([2019](#bib.bib11)),
    consisting of expected log-likelihood and an entropy regularizer, which they derive
    for the regression parameterization. Again, this approach relies on the density
    estimation capabilities of the NF to produce an agnostic belief about the right
    prediction for OOD examples (see [Figure 5(b)](#S3.F5.sf2 "In Figure 5 ‣ Generating
    OOD samples using generative models ‣ 3.4.2 Posterior Networks ‣ 3.4 Existing
    Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种回归方法是 Charpentier 等人提出的自然后验网络 ([2022](#bib.bib17))，该方法已经在[第3.4.2节](#S3.SS4.SSS2
    "3.4.2 Posterior Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3
    Evidential Deep Learning for Classification ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation") 中讨论过分类问题。但由于该方法是对指数族分布的推广，它同样适用于回归问题，使用正态似然和正态逆伽马先验。
    [方程23](#S3.E23 "In Posterior networks via Normalizing Flows ‣ 3.4.2 Posterior
    Networks ‣ 3.4 Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep
    Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") 中的贝叶斯更新规则被调整如下：$n$ 被设置为 $n=\lambda=2\alpha$，并且
    $\bm{\chi}=\big{[}\pi_{0}\ |\ \pi_{0}^{2}+2\beta/n\big{]}^{T}$。将输入数据再次输入自然后验网络，首先会生成一个潜在编码
    $\mathbf{z}$，从中 NF 预测 $n_{i}=N_{H}\cdot p(\mathbf{z}|\bm{\phi})$，并且额外的网络生成 $\bm{\chi}_{i}=g_{\bm{\psi}}(\mathbf{z})$，这些都被用于[方程23](#S3.E23
    "In Posterior networks via Normalizing Flows ‣ 3.4.2 Posterior Networks ‣ 3.4
    Existing Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") 以生成 $\bm{\chi}^{\text{post}}$ 和 $n^{\text{post}}$，从中可以推导出后验正态逆伽马的参数。作者们还由
    Biloš 等人 ([2019](#bib.bib11)) 提出了 UCE 损失的一般指数族形式，包括期望对数似然和熵正则化器，并且他们为回归参数化推导了这一形式。这个方法同样依赖于
    NF 的密度估计能力，以对 OOD 示例产生一种不具备先验知识的信念（见[图5(b)](#S3.F5.sf2 "In Figure 5 ‣ Generating
    OOD samples using generative models ‣ 3.4.2 Posterior Networks ‣ 3.4 Existing
    Approaches for Dirichlet Networks ‣ 3 Evidential Deep Learning for Classification
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")）。'
- en: Multivariate evidential regression
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多变量证据回归
- en: 'There are also some works offering solutions for multivariate regression problems:
    Malinin et al. ([2020a](#bib.bib117)) can be seen as a multivariate generalization
    of the work of Amini et al. ([2020](#bib.bib1)), where a combined Normal-Wishart
    prior is formed to fit the now Multivariate Normal likelihood. Again, the prior
    parameters are the output of a neural network, and uncertainty can be quantified
    in a similar way. For training purposes, they apply two different training objectives
    using the equivalent of the reverse KL objective of Malinin & Gales ([2019](#bib.bib116))
    as well as of the knowledge distillation objective of Malinin et al. ([2020b](#bib.bib118)),
    which does not require OOD data for regularization purposes. Meinert & Lavin ([2021](#bib.bib123))
    also provide a solution using a Normal Inverse-Wishart prior. In a similar vein
    to Oh & Shin ([2022](#bib.bib136)), they argue that the original objective proposed
    by Amini et al. ([2020](#bib.bib1)) can be minimized by increasing the network’s
    uncertainty instead of decreasing the mismatch of its prediction. As a solution,
    they simply propose to tie $\beta$ and $\nu$ via a hyperparameter.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些工作提供了多变量回归问题的解决方案：Malinin 等人（[2020a](#bib.bib117)）可以看作是 Amini 等人（[2020](#bib.bib1)）工作的多变量推广，其中形成了一个结合的正态-威沙特先验，以适应现在的多变量正态似然。同样，先验参数是神经网络的输出，不确定性可以以类似的方式量化。在训练过程中，他们使用了两种不同的训练目标，一个是
    Malinin & Gales（[2019](#bib.bib116)）的反向 KL 目标的等效目标，另一个是 Malinin 等人（[2020b](#bib.bib118)）的知识蒸馏目标，这不需要
    OOD 数据进行正则化。Meinert & Lavin（[2021](#bib.bib123)）也提供了使用正态逆威沙特先验的解决方案。他们与 Oh & Shin（[2022](#bib.bib136)）类似，认为可以通过增加网络的不确定性而不是减少预测的偏差来最小化
    Amini 等人（[2020](#bib.bib1)）提出的原始目标。作为解决方案，他们简单地建议通过超参数将 $\beta$ 和 $\nu$ 绑定在一起。
- en: 5 Related Work
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Other Approaches to Uncertainty Quantification
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不确定性量化的其他方法
- en: The need for the quantification of uncertainty in order to earn the trust of
    end-users and stakeholders has been a key driver for research (Bhatt et al., [2021](#bib.bib10);
    Jacovi et al., [2021](#bib.bib79); Liao & Sundar, [2022](#bib.bib106)). Existing
    methods can broadly be divided into frequentist and Bayesian methods, where the
    former judge the confidence of a model based on its predicted probabilities. Unfortunately,
    standard neural discriminator architectures have been proven to possess unwanted
    theoretical properties w.r.t. OOD inputs (Hein et al., [2019](#bib.bib61); Ulmer
    & Cinà, [2021](#bib.bib161)) and might therefore be unable to detect potentially
    risky inputs.^(14)^(14)14Pearce et al. ([2021](#bib.bib141)) argue that some insights
    might partially be misled by low-dimensional intuitions, and that empirically
    OOD data in higher dimensions tend to be mapped into regions of higher uncertainty.
    Further, a large line of research works has questioned the calibration of models
    (Guo et al., [2017](#bib.bib56); Nixon et al., [2019](#bib.bib135); Desai & Durrett,
    [2020](#bib.bib32); Minderer et al., [2021](#bib.bib126); Wang et al., [2021b](#bib.bib171)),
    i.e. to what extend the probability score of a class—also referred to as its confidence—corresponds
    to the chance of a correct prediction. Instead of relying on the confidence score
    alone, another way lies in constructing prediction sets consisting of the classes
    accumulating a certain share of the total predictive mass (Kompa et al., [2021](#bib.bib90);
    Ulmer et al., [2022](#bib.bib163)). By scoring a held-out population of data points
    to calibrate these prediction sets, we can also obtain frequentist guarantees
    in a procedure referred to a *conformal prediction* (Papadopoulos et al., [2002](#bib.bib138);
    Vovk et al., [2005](#bib.bib169); Lei & Wasserman, [2014](#bib.bib103); Angelopoulos
    & Bates, [2021](#bib.bib3)). This however still does not let us distinguish different
    notions of uncertainty. A popular *Bayesian* way to overcome this blemish by aggregating
    multiple predictions by networks in the Bayesian model averaging framework (Mackay,
    [1992](#bib.bib114); MacKay, [1995](#bib.bib112); Hinton & Van Camp, [1993](#bib.bib67);
    Neal, [2012](#bib.bib134); Jeffreys, [1998](#bib.bib80); Wilson & Izmailov, [2020](#bib.bib178);
    Kristiadi et al., [2020](#bib.bib92); Daxberger et al., [2021](#bib.bib27); Gal
    & Ghahramani, [2016](#bib.bib45); Blundell et al., [2015](#bib.bib13); Lakshminarayanan
    et al., [2017](#bib.bib99)). Nevertheless, many of these methods have been shown
    not to produce diverse predictions (Wilson & Izmailov, [2020](#bib.bib178); Fort
    et al., [2019](#bib.bib41)) and to deliver subpar performance and potentially
    misleading uncertainty estimates under distributional shift (Ovadia et al., [2019](#bib.bib137);
    Masegosa, [2020](#bib.bib120); Wenzel et al., [2020](#bib.bib174); Izmailov et al.,
    [2021a](#bib.bib77); [b](#bib.bib78)), raising doubts about their efficacy. The
    most robust method in this context is often given by an ensemble of neural predictors
    (Lakshminarayanan et al., [2017](#bib.bib99)), with multiple works exploring ways
    to make their training more efficient (Huang et al., [2017](#bib.bib72); Wilson
    & Izmailov, [2020](#bib.bib178); Wen et al., [2020](#bib.bib173); Turkoglu et al.,
    [2022](#bib.bib160)) or to provide theoretical guarantees (Pearce et al., [2020](#bib.bib140);
    Ciosek et al., [2020](#bib.bib19); He et al., [2020](#bib.bib60); D’Angelo & Fortuin,
    [2021](#bib.bib25)).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了赢得最终用户和利益相关者的信任，量化不确定性的需求成为研究的关键驱动因素（Bhatt et al.，[2021](#bib.bib10)；Jacovi
    et al.，[2021](#bib.bib79)；Liao & Sundar，[2022](#bib.bib106)）。现有方法可以广泛分为频率派和贝叶斯方法，前者根据模型的预测概率判断模型的置信度。不幸的是，已经证明标准的神经鉴别器架构在OOD输入（Hein
    et al.，[2019](#bib.bib61)；Ulmer & Cinà，[2021](#bib.bib161)）方面具有不希望的理论性质，因此可能无法检测出潜在的风险输入。^(14)^(14)14Pearce
    et al.（[2021](#bib.bib141)）认为，一些洞察力可能部分受到低维直觉的引导，经验证加入到更高维度的OOD数据往往会映射到更高的不确定性区域。此外，大量的研究工作质疑了模型的校准（Guo
    et al.，[2017](#bib.bib56)；Nixon et al.，[2019](#bib.bib135)；Desai & Durrett，[2020](#bib.bib32)；Minderer
    et al.，[2021](#bib.bib126)；Wang et al.，[2021b](#bib.bib171)），即类别的概率分数（也称为置信度）与正确预测的机会之间的相应程度。除了仅依赖置信度分数外，另一种方法是构建预测集，其中包含累积总预测量的一定份额的类别（Kompa
    et al.，[2021](#bib.bib90)；Ulmer et al.，[2022](#bib.bib163)）。通过对这些预测集进行校准的实际数据点进行评分，我们还可以在一个称为*符合预测*（Papadopoulos
    et al.，[2002](#bib.bib138)；Vovk et al.，[2005](#bib.bib169)；Lei & Wasserman，[2014](#bib.bib103)；Angelopoulos
    & Bates，[2021](#bib.bib3)）的过程中获得频率保证。然而，这仍然无法让我们区分不同的不确定性概念。一个流行的*贝叶斯*方法是通过在贝叶斯模型平均框架中聚合网络的多个预测来克服这个缺陷（Mackay，[1992](#bib.bib114)；MacKay，[1995](#bib.bib112)；Hinton
    & Van Camp，[1993](#bib.bib67)；Neal，[2012](#bib.bib134)；Jeffreys，[1998](#bib.bib80)；Wilson
    & Izmailov，[2020](#bib.bib178)；Kristiadi et al.，[2020](#bib.bib92)；Daxberger et al.，[2021](#bib.bib27)；Gal
    & Ghahramani，[2016](#bib.bib45)；Blundell et al.，[2015](#bib.bib13)；Lakshminarayanan
    et al.，[2017](#bib.bib99)）。然而，许多这些方法已经显示出不产生多样化的预测（Wilson & Izmailov，[2020](#bib.bib178)；Fort
    et al.，[2019](#bib.bib41)）以及在分布转移下提供次优的性能和潜在误导的不确定性估计（Ovadia et al.，[2019](#bib.bib137)；Masegosa，[2020](#bib.bib120)；Wenzel
    et al.，[2020](#bib.bib174)；Izmailov et al.，[2021a](#bib.bib77)；[b](#bib.bib78)），对它们的有效性产生了怀疑。在这种情况下，最强大的方法通常是通过多个神经预测器的集成（Lakshminarayanan
    et al.，[2017](#bib.bib99)），多个作品探索使其训练更高效的方法（Huang et al.，[2017](#bib.bib72)；Wilson
    & Izmailov，[2020](#bib.bib178)；Wen et al.，[2020](#bib.bib173)；Turkoglu et al.，[2022](#bib.bib160)）或者提供理论保证（Pearce
    et al.，[2020](#bib.bib140)；Ciosek et al.，[2020](#bib.bib19)；He et al.，[2020](#bib.bib60)；D’Angelo
    & Fortuin，[2021](#bib.bib25)）。
- en: Related Approaches to EDL
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与EDL相关的方法
- en: Kull et al. ([2019](#bib.bib94)) found an appealing use of the Dirichlet distribution
    as a post-training calibration map. Hobbhahn et al. ([2022](#bib.bib68)) use the
    Laplace bridge, a modified inverse based on an idea by MacKay ([1998](#bib.bib113)),
    to map from the model’s logit space to a Dirichlet distribution. The proposed
    Posterior Network (Charpentier et al., [2020](#bib.bib16); [2022](#bib.bib17))
    can furthermore be seen as related to another, competing approach, namely the
    combination of neural discriminators with density estimation methods, for instance
    in the form of energy-based models ([Grathwohl et al.,](#bib.bib54) ; Elflein
    et al., [2021](#bib.bib37)) or other hybrid architectures (Lee et al., [2018](#bib.bib102);
    Mukhoti et al., [2021](#bib.bib128)). Furthermore, there is a line of other single-pass
    uncertainty quantification approaches which do not originate from the evidential
    framework, for instance by taking inspiration from RBF networks (van Amersfoort
    et al., [2020b](#bib.bib165)) or via Gaussian Process output layers (Liu et al.,
    [2020](#bib.bib108); Fortuin et al., [2021](#bib.bib43); van Amersfoort et al.,
    [2021](#bib.bib166)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Kull et al. ([2019](#bib.bib94)) 发现了使用Dirichlet分布作为训练后校准映射的有趣方法。Hobbhahn et al.
    ([2022](#bib.bib68)) 使用Laplace桥，这是一种基于MacKay ([1998](#bib.bib113)) 提出的想法的修改逆向，来将模型的logit空间映射到Dirichlet分布。所提出的Posterior
    Network (Charpentier et al., [2020](#bib.bib16); [2022](#bib.bib17)) 还可以看作是与另一种竞争方法相关，即神经判别器与密度估计方法的结合，例如基于能量的模型
    ([Grathwohl et al.,](#bib.bib54) ; Elflein et al., [2021](#bib.bib37)) 或其他混合架构（Lee
    et al., [2018](#bib.bib102); Mukhoti et al., [2021](#bib.bib128)）。此外，还有一些其他的单次不确定性量化方法，这些方法并不源自证据框架，例如通过借鉴RBF网络（van
    Amersfoort et al., [2020b](#bib.bib165)）或通过高斯过程输出层（Liu et al., [2020](#bib.bib108);
    Fortuin et al., [2021](#bib.bib43); van Amersfoort et al., [2021](#bib.bib166)）。
- en: Applications of EDL
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: EDL的应用
- en: Some of the discussed models have already found a variety of applications, such
    as in autonomous driving (Capellier et al., [2019](#bib.bib15); Liu et al., [2021](#bib.bib110);
    Petek et al., [2022](#bib.bib143); Wang et al., [2021a](#bib.bib170)), remote
    sensing (Gawlikowski et al., [2022](#bib.bib48)), medical screening (Ghesu et al.,
    [2019](#bib.bib51); Gu et al., [2021](#bib.bib55); Li et al., [2022](#bib.bib104)),
    molecular analysis (Soleimany et al., [2021](#bib.bib154)), open set recognition
    (Bao et al., [2021](#bib.bib6)), active learning (Hemmer et al., [2022](#bib.bib62))
    and model selection (Radev et al., [2021](#bib.bib144)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一些讨论过的模型已经在多种应用中找到了用途，例如在自动驾驶（Capellier et al., [2019](#bib.bib15); Liu et al.,
    [2021](#bib.bib110); Petek et al., [2022](#bib.bib143); Wang et al., [2021a](#bib.bib170)）、遥感（Gawlikowski
    et al., [2022](#bib.bib48)）、医学筛查（Ghesu et al., [2019](#bib.bib51); Gu et al.,
    [2021](#bib.bib55); Li et al., [2022](#bib.bib104)）、分子分析（Soleimany et al., [2021](#bib.bib154)）、开放集识别（Bao
    et al., [2021](#bib.bib6)）、主动学习（Hemmer et al., [2022](#bib.bib62)）以及模型选择（Radev
    et al., [2021](#bib.bib144)）中。
- en: 6 Discussion
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: What is state-of-the-art?
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 什么是最先进的技术？
- en: 'As apparent from [Table 5](#A2.T5 "In Appendix B Datasets & Evaluation Techniques
    Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation"), evaluation methods and datasets can vary
    tremendously between different research works (for an overview, refer to [Appendix B](#A2
    "Appendix B Datasets & Evaluation Techniques Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"),). This
    can make it hard to accurately compare different approaches in a fair manner.
    Nevertheless, we try to draw some conclusion about the state-of-art in this research
    direction to the best extent possible: For image classification, the posterior
    (Charpentier et al., [2020](#bib.bib16)) and natural posterior network (Charpentier
    et al., [2022](#bib.bib17)) provide the best results on the tested benchmarks,
    both in terms of task performance and uncertainty quality. When the training an
    extra normalizing flow creates too much computational overhead, prior networks
    (Malinin & Gales, [2018](#bib.bib115)) with the PAC-based regularizer ([Haussmann
    et al.](#bib.bib58), [2019](#bib.bib58); see [Table 6](#A5.T6 "In Appendix E Overview
    over Loss Functions Appendix ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") for final form) or a simple
    entropy regularizer ([Section C.2](#A3.SS2 "C.2 Entropy of Dirichlet ‣ Appendix
    C Fundamental Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation")) can be used. In
    the case of regression problems, the natural posterior network (Stadler et al.,
    [2021](#bib.bib155)) performs better or on par with the evidential regression
    by Amini et al. ([2020](#bib.bib1)) or an ensemble Lakshminarayanan et al. ([2017](#bib.bib99))
    or MC Dropout (Gal & Ghahramani, [2016](#bib.bib45)). For graph neural networks,
    the graph posterior network (Stadler et al., [2021](#bib.bib155)) and a ensemble
    provide similar performance, but with the former displaying better uncertainty
    results. Again, this model requires training a NF, so a simpler fallback is provided
    by evidential regression (Amini et al., [2020](#bib.bib1)) with the improvement
    by Oh & Shin ([2022](#bib.bib136)). For NLP and count prediction, the works of
    Shen et al. ([2020](#bib.bib151)) and Charpentier et al. ([2022](#bib.bib17))
    are the only available instances from this model family, respectively. In the
    latter case, ensembles and the evidential regression framework (Amini et al.,
    [2020](#bib.bib1)) produce a lower root mean-squared error, but worse uncertainty
    estimates on OOD.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 从[表5](#A2.T5 "附录B 数据集与评估技术附录 ‣ 先验与后验网络：关于不确定性估计的证据深度学习方法的调查")中可以明显看出，不同研究工作中的评估方法和数据集差异巨大（有关概述，请参见[附录B](#A2
    "附录B 数据集与评估技术附录 ‣ 先验与后验网络：关于不确定性估计的证据深度学习方法的调查")）。这使得准确、公平地比较不同方法变得困难。然而，我们尽力对这一研究方向的最前沿状态进行最佳程度的总结：对于图像分类，后验网络（Charpentier
    et al., [2020](#bib.bib16)）和自然后验网络（Charpentier et al., [2022](#bib.bib17)）在测试基准上提供了最佳结果，无论是任务性能还是不确定性质量。当训练额外的归一化流带来过多计算开销时，可以使用具有PAC基础正则化器的先验网络（Malinin
    & Gales, [2018](#bib.bib115)）（[Haussmann et al.](#bib.bib58), [2019](#bib.bib58);
    参见[表6](#A5.T6 "附录E 损失函数概述附录 ‣ 先验与后验网络：关于不确定性估计的证据深度学习方法的调查")了解最终形式）或简单的熵正则化器（[C.2节](#A3.SS2
    "C.2 Dirichlet的熵 ‣ 附录C 基本推导附录 ‣ 先验与后验网络：关于不确定性估计的证据深度学习方法的调查")）。对于回归问题，自然后验网络（Stadler
    et al., [2021](#bib.bib155)）的表现优于或与Amini et al. ([2020](#bib.bib1))的证据回归或Lakshminarayanan
    et al. ([2017](#bib.bib99))或MC Dropout（Gal & Ghahramani, [2016](#bib.bib45)）相当。对于图神经网络，图后验网络（Stadler
    et al., [2021](#bib.bib155)）和一个集成体提供了类似的性能，但前者在不确定性结果上表现更好。同样，该模型需要训练NF，因此，证据回归（Amini
    et al., [2020](#bib.bib1)）和Oh & Shin（[2022](#bib.bib136)）的改进提供了更简单的备选方案。对于NLP和计数预测，Shen
    et al.（[2020](#bib.bib151)）和Charpentier et al.（[2022](#bib.bib17)）的工作是这一模型家族中仅有的实例。在后者的情况下，集成体和证据回归框架（Amini
    et al., [2020](#bib.bib1)）在OOD上产生了较低的均方根误差，但不确定性估计较差。
- en: Computational Cost
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算成本
- en: When it comes to the computational requirements, most of the proposed methods
    in this survey incur the same cost as a single deterministic network using a softmax
    output, since most of the architecture remains unchanged. Additional cost is mostly
    only produced when using knowledge distillation (Malinin et al., [2020b](#bib.bib118);
    Fathullah & Gales, [2022](#bib.bib39)), adding normalizing flow components like
    for posterior networks (Charpentier et al., [2020](#bib.bib16); [2022](#bib.bib17);
    Stadler et al., [2021](#bib.bib155)) or using generative models to produce synthetic
    OOD data (Chen et al., [2018](#bib.bib18); Sensoy et al., [2020](#bib.bib147);
    Hu et al., [2021](#bib.bib71)).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算需求方面，本调查中提出的大多数方法的成本与使用softmax输出的单一确定性网络相同，因为大部分架构保持不变。额外的成本主要在使用知识蒸馏（Malinin
    et al., [2020b](#bib.bib118); Fathullah & Gales, [2022](#bib.bib39)）、添加像后验网络的正则化流组件（Charpentier
    et al., [2020](#bib.bib16); [2022](#bib.bib17); Stadler et al., [2021](#bib.bib155)）或使用生成模型产生合成OOD数据（Chen
    et al., [2018](#bib.bib18); Sensoy et al., [2020](#bib.bib147); Hu et al., [2021](#bib.bib71)）时才会产生。
- en: Comparison to Other Approaches to Uncertainty Quantification
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与其他不确定性量化方法的比较
- en: 'As discussed in [Section 5](#S5 "5 Related Work ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), several
    existing approaches to uncertainty quantification equally suffer from shortcomings
    with respect to their reliability. One possible explanation for this behavior
    might lie in the insight that neural networks trained in the empirical risk minimization
    framework tend to learn spurious but highly predictive features (Ilyas et al.,
    [2019](#bib.bib76); Nagarajan et al., [2021](#bib.bib130)). This way, inputs stemming
    from the training distribution can be mapped to similar parts of the latent space
    as data points outside the distribution even though they display (from a human
    perspective) blatant semantic differences, simply because these semantic features
    were not useful to optimize for the training objective. This can result in ID
    and OOD points having assigned similar feature representations by a network, a
    phenomenon has been coined “feature collapse” (Nalisnick et al., [2019](#bib.bib131);
    van Amersfoort et al., [2021](#bib.bib166); Havtorn et al., [2021](#bib.bib59)).
    One strategy to mitigate (but not solve) this issue has been to enforce a constraint
    on the smoothness of the neural network function (Wei et al., [2018](#bib.bib172);
    van Amersfoort et al., [2020a](#bib.bib164); [2021](#bib.bib166); Liu et al.,
    [2020](#bib.bib108)), thereby maintaining both a sensitivity to semantic changes
    in the input and robustness against adversarial inputs (Yu et al., [2019](#bib.bib186)).
    Another approach lies in the usage of OOD data as well, sometimes dubbed “outlier
    exposure” (Fort et al., [2021](#bib.bib42)), but displaying the same shortcomings
    as in the EDL case. A generally promising strategy seems to seek functional diversity
    through ensembling: Juneja et al. ([2022](#bib.bib84)) show how model instances
    ending up in different low-loss modes correspond to distinct generalization strategies,
    indicating that combining diverse strategies may lead to better generalization
    and thus potentially more reliable uncertainty. Attaining different solutions
    still creates computational overhead, despite new methods to reduce it (Garipov
    et al., [2018](#bib.bib47); Dusenberry et al., [2020](#bib.bib36); Benton et al.,
    [2021](#bib.bib9)).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第5节](#S5 "5 相关工作 ‣ 先验和后验网络：证据深度学习方法在不确定性估计中的调查")中讨论的那样，现有的几种不确定性量化方法在可靠性方面都存在缺陷。这种现象的一个可能解释在于神经网络在经验风险最小化框架中训练时，往往会学习到虚假的但高度预测性的特征（Ilyas
    et al., [2019](#bib.bib76)；Nagarajan et al., [2021](#bib.bib130)）。这样，即使输入显示出明显的语义差异（从人类的角度来看），来自训练分布的输入也可以被映射到潜在空间中的类似部分，就像分布外的数据点一样，仅仅是因为这些语义特征对于优化训练目标没有用。这可能导致ID和OOD点被网络分配了相似的特征表示，这一现象被称为“特征崩溃”（Nalisnick
    et al., [2019](#bib.bib131)；van Amersfoort et al., [2021](#bib.bib166)；Havtorn
    et al., [2021](#bib.bib59)）。一种缓解（但不能解决）这一问题的策略是对神经网络函数的平滑性施加约束（Wei et al., [2018](#bib.bib172)；van
    Amersfoort et al., [2020a](#bib.bib164)；[2021](#bib.bib166)；Liu et al., [2020](#bib.bib108)），从而在输入的语义变化和对抗输入的鲁棒性之间保持敏感性（Yu
    et al., [2019](#bib.bib186)）。另一种方法是使用OOD数据，有时称为“异常值暴露”（Fort et al., [2021](#bib.bib42)），但显示出与EDL情况相同的缺陷。一个普遍有前景的策略似乎是通过集成寻求功能多样性：Juneja
    et al. ([2022](#bib.bib84))展示了模型实例在不同低损失模式下如何对应于不同的泛化策略，表明结合多样化的策略可能会导致更好的泛化，因此可能更可靠的不确定性。尽管有新的方法来减少计算开销（Garipov
    et al., [2018](#bib.bib47)；Dusenberry et al., [2020](#bib.bib36)；Benton et al.,
    [2021](#bib.bib9)），获得不同的解决方案仍然会产生计算开销。
- en: Bayesian model averaging
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贝叶斯模型平均
- en: 'One of the most fundamental differences between EDL and existing approaches
    is the sacrifice of Bayesian model averaging ([Equations 2](#S2.E2 "In 2.1 Bayesian
    Inference ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") and [5](#S2.E5 "Equation 5
    ‣ 2.3 Evidential Deep Learning ‣ 2 Background ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")): In
    principle, combining multiple parameter estimates is supposed to result in a lower
    predictive risk (Fragoso et al., [2018](#bib.bib44)). The Machine Learning community
    has ascribed further desiderata to this approach, such as better generalization
    and robustness to distributional shifts. Recent studies with exact Bayesian Neural
    Networks however have cast doubts on these assumptions (Izmailov et al., [2021a](#bib.bib77);
    [b](#bib.bib78)). Nevertheless, ensembles, that approximate [Equation 2](#S2.E2
    "In 2.1 Bayesian Inference ‣ 2 Background ‣ Prior and Posterior Networks: A Survey
    on Evidential Deep Learning Methods For Uncertainty Estimation") via Monte Carlo
    estimates, remain state-of-the-art on many uncertainty benchmarks. EDL abandons
    modelling epistemic uncertainty through the learnable parameters, and instead
    expresses it through the uncertainty in prior / posterior parameters. This loses
    functional diversity which could aid generalization, while sidestepping computational
    costs. Future research could therefore explore the combination of both paradigms,
    as proposed by Haussmann et al. ([2019](#bib.bib58)); Zhao et al. ([2020](#bib.bib189));
    Charpentier et al. ([2022](#bib.bib17)).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: EDL 和现有方法之间最根本的区别之一是牺牲了贝叶斯模型平均 ([Equations 2](#S2.E2 "在 2.1 贝叶斯推断 ‣ 2 背景 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")
    和 [5](#S2.E5 "公式 5 ‣ 2.3 证据深度学习 ‣ 2 背景 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查"))：原则上，结合多个参数估计应该会导致更低的预测风险（Fragoso
    等，[2018](#bib.bib44)）。机器学习社区还赋予这种方法其他期望，例如更好的泛化能力和对分布变化的鲁棒性。然而，最近对精确贝叶斯神经网络的研究对这些假设提出了质疑（Izmailov
    等，[2021a](#bib.bib77)；[b](#bib.bib78)）。尽管如此，利用蒙特卡洛估计来逼近 [Equation 2](#S2.E2 "在
    2.1 贝叶斯推断 ‣ 2 背景 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查") 的集合方法在许多不确定性基准测试中仍然是最先进的。EDL
    放弃了通过可学习参数建模认知不确定性，而是通过先验/后验参数的不确定性来表达。这失去了可能有助于泛化的功能多样性，同时回避了计算成本。因此，未来的研究可以探索这两种范式的结合，正如
    Haussmann 等人（[2019](#bib.bib58)）；Zhao 等人（[2020](#bib.bib189)）；Charpentier 等人（[2022](#bib.bib17)）所提议的那样。
- en: Challenges
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 挑战
- en: 'Despite their advantages, the last chapters have pointed out key weaknesses
    of Dirichlet networks as well: In order to achieve the right behavior of the distribution
    and thus guarantee sensible uncertainty estimates (since ground truth estimates
    are not available), the surveyed literature proposes a variety of loss functions.
    Bengs et al. ([2022](#bib.bib8)) show formally that many of the loss functions
    used so far are *not* appropriate and violate basic asymptotic assumptions about
    epistemic uncertainty: With increasing amount of data, epistemic uncertainty should
    vanish, but this is not guaranteed using the commonly used loss functions. Furthermore,
    some approaches (Malinin & Gales, [2018](#bib.bib115); [2019](#bib.bib116); Nandy
    et al., [2020](#bib.bib133); Malinin et al., [2020a](#bib.bib117)) require out-of-distribution
    data points during training. This comes with two problems: Such data is often
    not available or in the first place, or cannot guarantee robustness against *other*
    kinds of unseen OOD data, of which infinite types exist in a real-valued feature
    space.^(15)^(15)15The same applies to the artificial OOD data in Chen et al. ([2018](#bib.bib18));
    Shen et al. ([2020](#bib.bib151)); Sensoy et al. ([2020](#bib.bib147)). Indeed,
    Kopetzki et al. ([2021](#bib.bib91)) found OOD detection to deteriorate across
    a family of EDL models under adversarial perturbation and OOD data. Stadler et al.
    ([2021](#bib.bib155)) point out that much of the ability of posterior networks
    stems from the addition of a NF, which have been shown to also sometimes behave
    unreliably on OOD data (Nalisnick et al., [2019](#bib.bib131)). Although the NFs
    in posterior networks operate on the latent and not the feature space, they are
    also restricted to operate on features that the underlying network has learned
    to recognize. Recent work by Dietterich & Guyer ([2022](#bib.bib33)) has hinted
    at the fact that networks might identify OOD by the absence of known features,
    and not by the presence of new ones, providing a case in which posterior networks
    are likely to fail. Such evidence on OOD data and adversarial examples has indeed
    been identified by a study by Kopetzki et al. ([2021](#bib.bib91)).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有其优点，前几章也指出了Dirichlet网络的关键弱点：为了实现分布的正确行为，从而保证合理的不确定性估计（因为没有真实的估计），调查的文献提出了各种损失函数。Bengs等人（[2022](#bib.bib8)）正式表明，迄今为止使用的许多损失函数是*不*合适的，并且违反了关于认识不确定性的基本渐近假设：随着数据量的增加，认识不确定性应该消失，但使用常见的损失函数并不能保证这一点。此外，一些方法（Malinin
    & Gales，[2018](#bib.bib115)；[2019](#bib.bib116)；Nandy等人，[2020](#bib.bib133)；Malinin等人，[2020a](#bib.bib117)）在训练过程中需要超分布数据点。这带来了两个问题：这种数据通常不可用或根本不存在，或者不能保证对*其他*类型的未见OOD数据的鲁棒性，在实际特征空间中存在无限种类型。^(15)^(15)15同样适用于Chen等人（[2018](#bib.bib18)）；Shen等人（[2020](#bib.bib151)）；Sensoy等人（[2020](#bib.bib147)）的人工OOD数据。确实，Kopetzki等人（[2021](#bib.bib91)）发现，面对对抗性扰动和OOD数据时，EDL模型的OOD检测能力会下降。Stadler等人（[2021](#bib.bib155)）指出，后验网络的许多能力来自NF的添加，已经显示NF在OOD数据上有时也表现不可靠（Nalisnick等人，[2019](#bib.bib131)）。尽管后验网络中的NF作用于潜在空间而非特征空间，但它们也被限制在对底层网络已经学会识别的特征进行操作。Dietterich
    & Guyer（[2022](#bib.bib33)）最近的研究表明，网络可能通过已知特征的缺失而不是新特征的存在来识别OOD，这提供了一个后验网络可能失败的情况。Kopetzki等人（[2021](#bib.bib91)）的研究确实确认了在OOD数据和对抗样本上的这种证据。
- en: Future Research Directions
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 未来研究方向
- en: 'Overall, the following directions for future research on EDL crystallize from
    our previous reflections: *(1) Explicit epistemic uncertainty estimation:* Since
    we often employ the point estimate in [Equation 5](#S2.E5 "In 2.3 Evidential Deep
    Learning ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") to avoid the posterior $p(\bm{\theta}|\mathbb{D})$,
    explicit estimation of the epistemic uncertainty is not possible, and some summary
    statistic of the concentration parameters is used for classification problems
    instead ([Section 3.3](#S3.SS3 "3.3 Uncertainty Estimation with Dirichlet Networks
    ‣ 3 Evidential Deep Learning for Classification ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")). Estimating
    model uncertainty through modelling the (approximate) posterior $p(\bm{\theta}|\mathbb{D})$
    in Bayesian model averaging is a popular technique (Houlsby et al., [2011](#bib.bib69);
    Gal et al., [2016](#bib.bib46); Smith & Gal, [2018](#bib.bib153); Ulmer et al.,
    [2020](#bib.bib162)), but comes with the disadvantage of additional computational
    overhead. However, Sharma et al. ([2022](#bib.bib148)) recently showed that a
    Bayesian treatment of all model parameters may not be necessary, potentially allowing
    for a compromise. *(2) Robustness to diverse OOD data:* The emprical evidence
    compiled by Kopetzki et al. ([2021](#bib.bib91)) indicates that EDL classification
    models are not completely able to robustly classify and detect OOD and adversarial
    inputs. These findings hold both for prior networks trained with OOD data, or
    for posterior networks using density estimators. We speculate that through the
    information bottleneck principle (Tishby & Zaslavsky, [2015](#bib.bib157)), EDL
    models might not learn input features that are useful to indicate uncertainty
    in their prediction, or at best identify the absence of known features, but not
    the presence of new ones (Dietterich & Guyer, [2022](#bib.bib33)). Finding a way
    to have models identify unusual features could this help to mitigate this problem.
    *(3) Theoretical guarantees:* Even though some guarantees have been derived for
    EDL classifiers w.r.t. OOD data points (Charpentier et al., [2020](#bib.bib16);
    Stadler et al., [2021](#bib.bib155)), Bengs et al. ([2022](#bib.bib8)) point out
    the flaws of current training regimes for epistemic uncertainty in the limit of
    infinite limit. Furthermore, Hüllermeier & Waegeman ([2021](#bib.bib75)) argue
    that even uncertainty estimates are affected by uncertainty themselves, impacting
    their usefulness.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，从我们之前的反思中提炼出以下对未来EDL研究的方向：*(1) 明确的知识不确定性估计：* 由于我们经常使用[方程式 5](#S2.E5 "在
    2.3 证据深度学习 ‣ 2 背景 ‣ 先验与后验网络：关于不确定性估计的证据深度学习方法调查")中的点估计来避免后验 $p(\bm{\theta}|\mathbb{D})$，因此明确估计知识不确定性是不可能的，一些集中参数的总结统计量被用作分类问题的替代（[第
    3.3 节](#S3.SS3 "3.3 使用Dirichlet网络进行不确定性估计 ‣ 3 证据深度学习用于分类 ‣ 先验与后验网络：关于不确定性估计的证据深度学习方法调查")）。通过建模（近似）后验
    $p(\bm{\theta}|\mathbb{D})$ 在贝叶斯模型平均中估计模型不确定性是一种流行的技术（Houlsby 等，[2011](#bib.bib69)；Gal
    等，[2016](#bib.bib46)；Smith & Gal，[2018](#bib.bib153)；Ulmer 等，[2020](#bib.bib162)），但这带来了额外的计算开销。然而，Sharma
    等（[2022](#bib.bib148)）最近表明，所有模型参数的贝叶斯处理可能并非必要，可能允许某种妥协。*(2) 对各种OOD数据的鲁棒性：* Kopetzki
    等（[2021](#bib.bib91)）汇编的实证证据表明，EDL分类模型无法完全稳健地分类和检测OOD及对抗性输入。这些发现适用于使用OOD数据训练的先验网络或使用密度估计器的后验网络。我们推测，通过信息瓶颈原理（Tishby
    & Zaslavsky，[2015](#bib.bib157)），EDL模型可能不会学习对其预测不确定性有用的输入特征，或者最好的情况下只能识别已知特征的缺失，而不能识别新特征的存在（Dietterich
    & Guyer，[2022](#bib.bib33)）。找到一种使模型识别不寻常特征的方法可能有助于缓解这个问题。*(3) 理论保证：* 尽管已经为EDL分类器相对于OOD数据点推导出了一些保证（Charpentier
    等，[2020](#bib.bib16)；Stadler 等，[2021](#bib.bib155)），但Bengs 等（[2022](#bib.bib8)）指出了当前训练机制在无限极限下对知识不确定性的缺陷。此外，Hüllermeier
    & Waegeman（[2021](#bib.bib75)）认为，即使是不确定性估计也受到自身不确定性的影响，影响了其有效性。
- en: 7 Conclusion
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'This survey has given an overview over contemporary approaches for uncertainty
    estimation using neural networks to parameterize conjugate priors or the corresponding
    posteriors instead of likelihoods, called Evidential Deep Learning. We highlighted
    their appealing theoretical properties allowing for uncertainty estimation with
    minimal computational overhead, rendering them as a viable alternative to existing
    strategies. We also emphasized practical problems: In order to nudge models towards
    the desired behavior in the face of unseen or out-of-distribution samples, the
    design of the model architecture and loss function have to be carefully considered.
    Based on a summary and discussion of experimental findings in [Section 6](#S6
    "6 Discussion ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation"), the entropy regularizer seems to be a sensible
    choice in prior networks when OOD data is not available. Combining discriminators
    with generative models like normalizing flows as in Charpentier et al. ([2020](#bib.bib16);
    [2022](#bib.bib17)), embedded in a sturdy Bayesian framework, also appears as
    an exciting direction for practical applications. In summary, we believe that
    recent advances show promising results for Evidential Deep Learning, making it
    a viable option in uncertainty estimation to improve safety and trustworthiness
    in Machine Learning systems.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述概述了使用神经网络对共轭先验或相应后验进行不确定性估计的现代方法，称为证据深度学习。我们突出其有吸引力的理论特性，使其能够以最小的计算开销进行不确定性估计，从而成为现有策略的可行替代方案。我们还强调了实际问题：为了使模型在面对未见或分布外样本时趋向于期望行为，模型架构和损失函数的设计必须经过仔细考虑。根据[第
    6 节](#S6 "6 讨论 ‣ 先验与后验网络：不确定性估计的证据深度学习方法综述")中实验结果的总结和讨论，当没有 OOD 数据时，熵正则化器似乎是先验网络中的明智选择。将判别器与生成模型（如
    Charpentier 等（[2020](#bib.bib16)；[2022](#bib.bib17)）中的正则化流）结合，并嵌入稳健的贝叶斯框架中，也是一个令人兴奋的实际应用方向。总之，我们相信近期的进展展示了证据深度学习的有希望的结果，使其成为不确定性估计中改善机器学习系统安全性和可靠性的可行选项。
- en: Acknowledgements
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'We would like to thank Giovanni Cinà, Max Müller-Eberstein, Daniel Varab and
    Mike Zhang for reading early versions of this draft and providing tremendously
    useful feedback. Further, we would like to explicitly thank Mike Zhang for helping
    to improve [Figure 1](#S1.F1 "In 1 Introduction ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). We
    also would like to thank Alexander Amini for providing a long list of references
    that helped to further improve the coverage of this work and the anonymous reviewers
    for their suggestions. Lastly, we owe our gratitude to the anonymous reviewers
    that helped us such much to improve the different versions of this paper.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢 Giovanni Cinà、Max Müller-Eberstein、Daniel Varab 和 Mike Zhang 早期阅读本草稿并提供极其有用的反馈。此外，我们特别感谢
    Mike Zhang 帮助改进了[图 1](#S1.F1 "在 1 引言 ‣ 先验与后验网络：不确定性估计的证据深度学习方法综述")。我们还要感谢 Alexander
    Amini 提供了大量参考文献，这些参考文献帮助进一步改善了本工作的覆盖范围，以及匿名评审者提出的建议。最后，我们感谢匿名评审者，他们帮助我们改进了本文的不同版本。
- en: References
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Amini et al. (2020) Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela
    Rus. Deep Evidential Regression. In *Advances in Neural Information Processing
    Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
    2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amini 等（2020）Alexander Amini、Wilko Schwarting、Ava Soleimany 和 Daniela Rus。深度证据回归。在
    *神经信息处理系统 33：2020 年神经信息处理系统年会，NeurIPS 2020，2020 年 12 月 6 日 - 12 日，虚拟会议*，2020。
- en: 'Andrieu et al. (2000) Christophe Andrieu, Nando de Freitas, and Arnaud Doucet.
    Reversible Jump MCMC Simulated Annealing for Neural Networks. In Craig Boutilier
    and Moisés Goldszmidt (eds.), *UAI ’00: Proceedings of the 16th Conference in
    Uncertainty in Artificial Intelligence, Stanford University, Stanford, California,
    USA, June 30 - July 3, 2000*, pp.  11–18\. Morgan Kaufmann, 2000.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrieu 等（2000）Christophe Andrieu、Nando de Freitas 和 Arnaud Doucet。可逆跳跃 MCMC
    模拟退火用于神经网络。在 Craig Boutilier 和 Moisés Goldszmidt（编），*UAI ’00：第 16 届不确定性人工智能会议论文集，斯坦福大学，美国加州斯坦福，2000
    年 6 月 30 日 - 7 月 3 日*，第 11–18 页。Morgan Kaufmann，2000。
- en: Angelopoulos & Bates (2021) Anastasios N Angelopoulos and Stephen Bates. A Gentle
    Introduction to Conformal Prediction and Distribution-free Uncertainty Quantification.
    *arXiv preprint arXiv:2107.07511*, 2021.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Angelopoulos & Bates (2021) Anastasios N Angelopoulos 和 Stephen Bates. 温和介绍符合预测和无分布不确定性量化。*arXiv
    预印本 arXiv:2107.07511*，2021。
- en: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein
    Generative Adversarial Networks. In *International conference on machine learning*,
    pp. 214–223\. PMLR, 2017.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala 和 Léon Bottou. Wasserstein
    生成对抗网络。见于 *国际机器学习会议*，第214–223页。PMLR，2017。
- en: 'Audun (2018) Jsang Audun. *Subjective Logic: A Formalism for Reasoning under
    Uncertainty*. Springer, 2018.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Audun (2018) Jsang Audun. *主观逻辑：一种在不确定性下推理的形式*。Springer，2018。
- en: Bao et al. (2021) Wentao Bao, Qi Yu, and Yu Kong. Evidential Deep Learning for
    Open Set Action Recognition. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pp.  13349–13358, 2021.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao et al. (2021) Wentao Bao, Qi Yu 和 Yu Kong. 用于开放集动作识别的证据深度学习。见于 *IEEE/CVF
    国际计算机视觉会议论文集*，第13349–13358页，2021。
- en: Bastidas (2017) Alexei Bastidas. Tiny Imagenet Image Classification, 2017.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bastidas (2017) Alexei Bastidas. Tiny Imagenet 图像分类，2017。
- en: 'Bengs et al. (2022) Viktor Bengs, Eyke Hüllermeier, and Willem Waegeman. On
    the Difficulty of Epistemic Uncertainty Quantification in Machine Learning: The
    Case of Direct Uncertainty Estimation through Loss Minimisation. *arXiv preprint
    arXiv:2203.06102*, 2022.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengs et al. (2022) Viktor Bengs, Eyke Hüllermeier 和 Willem Waegeman. 机器学习中的认识不确定性量化难题：通过损失最小化进行直接不确定性估计的案例。*arXiv
    预印本 arXiv:2203.06102*，2022。
- en: Benton et al. (2021) Gregory W. Benton, Wesley J. Maddox, Sanae Lotfi, and Andrew Gordon
    Wilson. Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling.
    In *Proceedings of the 38th International Conference on Machine Learning, ICML
    2021, 18-24 July 2021, Virtual Event*, volume 139 of *Proceedings of Machine Learning
    Research*, pp.  769–779\. PMLR, 2021.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benton et al. (2021) Gregory W. Benton, Wesley J. Maddox, Sanae Lotfi 和 Andrew
    Gordon Wilson. 模式连接体积和快速集成的损失表面单形体。见于 *第38届国际机器学习会议论文集，ICML 2021，2021年7月18-24日，虚拟活动*，第139卷
    *机器学习研究论文集*，第769–779页。PMLR，2021。
- en: 'Bhatt et al. (2021) Umang Bhatt, Javier Antorán, Yunfeng Zhang, Q. Vera Liao,
    Prasanna Sattigeri, Riccardo Fogliato, Gabrielle Gauthier Melançon, Ranganath
    Krishnan, Jason Stanley, Omesh Tickoo, Lama Nachman, Rumi Chunara, Madhulika Srikumar,
    Adrian Weller, and Alice Xiang. Uncertainty as a Form of Transparency: Measuring,
    Communicating, and Using Uncertainty. In *AIES ’21: AAAI/ACM Conference on AI,
    Ethics, and Society, Virtual Event, USA, May 19-21, 2021*, pp.  401–413\. ACM,
    2021.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bhatt et al. (2021) Umang Bhatt, Javier Antorán, Yunfeng Zhang, Q. Vera Liao,
    Prasanna Sattigeri, Riccardo Fogliato, Gabrielle Gauthier Melançon, Ranganath
    Krishnan, Jason Stanley, Omesh Tickoo, Lama Nachman, Rumi Chunara, Madhulika Srikumar,
    Adrian Weller 和 Alice Xiang. 不确定性作为一种透明形式：测量、传达和使用不确定性。见于 *AIES ’21: AAAI/ACM
    人工智能、伦理与社会会议，虚拟活动，美国，2021年5月19-21日*，第401–413页。ACM，2021。'
- en: Biloš et al. (2019) Marin Biloš, Bertrand Charpentier, and Stephan Günnemann.
    Uncertainty on Asynchronous Time Event Prediction. In *Advances in Neural Information
    Processing Systems*, pp. 12851–12860, 2019.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biloš et al. (2019) Marin Biloš, Bertrand Charpentier 和 Stephan Günnemann. 异步时间事件预测中的不确定性。见于
    *神经信息处理系统进展*，第12851–12860页，2019。
- en: Bishop (2006) Christopher M Bishop. Pattern Recognition. *Machine learning*,
    128(9), 2006.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bishop (2006) Christopher M Bishop. 模式识别。*机器学习*，128(9)，2006。
- en: Blundell et al. (2015) Charles Blundell, Julien Cornebise, Koray Kavukcuoglu,
    and Daan Wierstra. Weight Uncertainty in Neural Networks. *arXiv preprint arXiv:1505.05424*,
    2015.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blundell et al. (2015) Charles Blundell, Julien Cornebise, Koray Kavukcuoglu
    和 Daan Wierstra. 神经网络中的权重不确定性。*arXiv 预印本 arXiv:1505.05424*，2015。
- en: 'Bulatov (2011) Yaroslav Bulatov. NotMNIST Dataset. *Google (Books/OCR), Tech.
    Rep.[Online]. Available: http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset.
    html*, 2, 2011.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bulatov (2011) Yaroslav Bulatov. NotMNIST 数据集。*Google (图书/OCR)，技术报告[在线]。可用：
    http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html*，2，2011。
- en: Capellier et al. (2019) Edouard Capellier, Franck Davoine, Véronique Cherfaoui,
    and You Li. Evidential Deep Learning for Arbitrary LIDAR Object Classification
    in the Context of Autonomous Driving. In *2019 IEEE Intelligent Vehicles Symposium,
    IV 2019, Paris, France, June 9-12, 2019*, pp.  1304–1311\. IEEE, 2019.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Capellier et al. (2019) Edouard Capellier, Franck Davoine, Véronique Cherfaoui
    和 You Li. 用于自主驾驶中任意 LIDAR 对象分类的证据深度学习。见于 *2019 IEEE 智能车辆研讨会，IV 2019，法国巴黎，2019年6月9-12日*，第1304–1311页。IEEE，2019。
- en: 'Charpentier et al. (2020) Bertrand Charpentier, Daniel Zügner, and Stephan
    Günnemann. Posterior network: Uncertainty estimation without ood samples via density-based
    pseudo-counts. *Advances in Neural Information Processing Systems*, 33:1356–1367,
    2020.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charpentier等（2020）贝特朗·沙尔潘捷、丹尼尔·茨根纳和斯特凡·居恩曼。后验网络：通过基于密度的伪计数进行没有OOD样本的不确定性估计。*神经信息处理系统进展*，33:1356–1367，2020年。
- en: 'Charpentier et al. (2022) Bertrand Charpentier, Oliver Borchert, Daniel Zügner,
    Simon Geisler, and Stephan Günnemann. Natural Posterior Network: Deep Bayesian
    Predictive Uncertainty for Exponential Family Distributions. In *The Tenth International
    Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,
    2022*. OpenReview.net, 2022.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charpentier等（2022）贝特朗·沙尔潘捷、奥利弗·博尔赫特、丹尼尔·茨根纳、西蒙·盖斯勒和斯特凡·居恩曼。自然后验网络：用于指数族分布的深度贝叶斯预测不确定性。发表于*第十届国际学习表征会议，ICLR
    2022，虚拟活动，2022年4月25-29日*。OpenReview.net，2022年。
- en: Chen et al. (2018) Wenhu Chen, Yilin Shen, Hongxia Jin, and William Wang. A
    Variational Dirichlet Framework for Out-Of-Distribution Detection. *arXiv preprint
    arXiv:1811.07308*, 2018.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2018）文虎·陈、伊琳·沈、洪霞·金和威廉·王。用于异常分布检测的变分狄利克雷框架。*arXiv 预印本 arXiv:1811.07308*，2018年。
- en: Ciosek et al. (2020) Kamil Ciosek, Vincent Fortuin, Ryota Tomioka, Katja Hofmann,
    and Richard Turner. Conservative Uncertainty Estimation by Fitting Prior Networks.
    In *International Conference on Learning Representations*, 2020.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciosek等（2020）卡米尔·乔塞克、文森特·福尔图因、龙田·富冈、卡佳·霍夫曼和理查德·特纳。通过拟合先验网络进行保守的不确定性估计。发表于*国际学习表征会议*，2020年。
- en: Clanuwat et al. (2018) Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto,
    Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep Learning for Classical Japanese
    Literature. *arXiv preprint arXiv:1812.01718*, 2018.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clanuwat等（2018）塔林·克拉努瓦特、米克尔·博伯-伊里萨尔、北本朝信、亚历克斯·兰姆、山本和明和大卫·哈。古典日本文学的深度学习。*arXiv
    预印本 arXiv:1812.01718*，2018年。
- en: 'Coraddu et al. (2016) Andrea Coraddu, Luca Oneto, Aessandro Ghio, Stefano Savio,
    Davide Anguita, and Massimo Figari. Machine Learning Approaches for Improving
    Condition-Based Maintenance of Naval Propulsion Plants. *Proceedings of the Institution
    of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment*,
    230(1):136–153, 2016.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coraddu等（2016）安德里亚·科拉杜、卢卡·奥内托、阿埃桑德罗·吉奥、斯特凡诺·萨维奥、大卫·安圭塔和马西莫·费加里。改进海军推进装置的条件基础维护的机器学习方法。*机械工程师学会会刊，M部分：海事环境工程期刊*，230(1):136–153，2016年。
- en: Corke (1996) Peter I Corke. A Robotics Toolbox for MATLAB. *IEEE Robotics &
    Automation Magazine*, 3(1):24–32, 1996.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Corke（1996）彼得·I·科克。MATLAB的机器人工具箱。*IEEE机器人与自动化杂志*，3(1):24–32，1996年。
- en: Cortez et al. (2009) Paulo Cortez, António Cerdeira, Fernando Almeida, Telmo
    Matos, and José Reis. Modeling Wine Preferences by Data Mining from Physicochemical
    Properties. *Decision support systems*, 47(4):547–553, 2009.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cortez等（2009）保罗·科尔特斯、安东尼奥·塞尔德拉、费尔南多·阿尔梅达、特尔莫·马托斯和若泽·雷斯。通过数据挖掘物理化学属性建模葡萄酒偏好。*决策支持系统*，47(4):547–553，2009年。
- en: 'Coucke et al. (2018) Alice Coucke, Alaa Saade, Adrien Ball, Théodore Bluche,
    Alexandre Caulier, David Leroy, Clément Doumouro, Thibault Gisselbrecht, Francesco
    Caltagirone, Thibaut Lavril, et al. Snips Voice Platform: An Embedded Spoken Language
    Understanding System for Private-by-Design Voice Interfaces. *arXiv preprint arXiv:1805.10190*,
    2018.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coucke等（2018）爱丽丝·库克、阿拉·萨阿德、阿德里安·巴尔、泰奥多尔·布吕歇、亚历山大·考利埃、戴维·勒鲁瓦、克莱门特·杜穆罗、蒂博·吉塞布雷赫、弗朗切斯科·卡尔塔基罗内、蒂博·拉夫里尔等。Snips语音平台：用于隐私设计语音接口的嵌入式语言理解系统。*arXiv
    预印本 arXiv:1805.10190*，2018年。
- en: D’Angelo & Fortuin (2021) Francesco D’Angelo and Vincent Fortuin. Repulsive
    deep ensembles are bayesian. *Advances in Neural Information Processing Systems*,
    34:3451–3465, 2021.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D’Angelo & Fortuin（2021）弗朗切斯科·D’安杰洛和文森特·福尔图因。排斥性深度集成方法是贝叶斯的。*神经信息处理系统进展*，34:3451–3465，2021年。
- en: Davis et al. (2011) Mindy I Davis, Jeremy P Hunt, Sanna Herrgard, Pietro Ciceri,
    Lisa M Wodicka, Gabriel Pallares, Michael Hocker, Daniel K Treiber, and Patrick P
    Zarrinkar. Comprehensive Analysis of Kinase Inhibitor Selectivity. *Nature biotechnology*,
    29(11):1046–1051, 2011.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 戴维斯等（2011）敏迪·I·戴维斯、杰里米·P·亨特、萨娜·赫尔加德、皮埃特罗·西切里、丽莎·M·沃迪卡、加布里埃尔·帕列雷斯、迈克尔·霍克、丹尼尔·K·特雷伯和帕特里克·P·扎林卡。激酶抑制剂选择性的综合分析。*自然生物技术*，29(11):1046–1051，2011年。
- en: 'Daxberger et al. (2021) Erik Daxberger, Agustinus Kristiadi, Alexander Immer,
    Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. Laplace Redux - Effortless
    Bayesian Deep Learning. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin,
    Percy Liang, and Jennifer Wortman Vaughan (eds.), *Advances in Neural Information
    Processing Systems 34: Annual Conference on Neural Information Processing Systems
    2021, NeurIPS 2021, December 6-14, 2021, virtual*, pp. 20089–20103, 2021.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daxberger et al. (2021) 埃里克·达克斯伯格，阿古斯丁努斯·克里斯蒂亚迪，亚历山大·伊默，鲁娜·艾申哈根，马蒂亚斯·鲍尔，以及菲利普·亨宁。拉普拉斯再现——轻松的贝叶斯深度学习。在马克·奥雷利奥·兰扎托，阿利娜·贝耶尔齐梅尔，扬·N·多芬，佩尔西·梁和詹妮弗·沃特曼·沃恩（编辑），*神经信息处理系统进展
    34：2021年神经信息处理系统年度会议，NeurIPS 2021，2021年12月6-14日，虚拟*，第20089–20103页，2021年。
- en: de Freitas (2003) João Ferdinando Gomes de Freitas. *Bayesian Methods for Neural
    Networks*. PhD thesis, University of Cambridge, 2003.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Freitas (2003) 若昂·费尔南多·戈梅斯·德·费雷塔斯。*神经网络的贝叶斯方法*。博士论文，剑桥大学，2003年。
- en: 'Dempster (1968) Arthur P Dempster. A Generalization of Bayesian Inference.
    *Journal of the Royal Statistical Society: Series B (Methodological)*, 30(2):205–232,
    1968.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dempster (1968) 阿瑟·P·邓普斯特。贝叶斯推断的推广。*皇家统计学会学报：B系列（方法论）*，30(2):205–232，1968年。
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. Imagenet: A Large-Scale Hierarchical Image Database. In *2009 IEEE
    conference on computer vision and pattern recognition*, pp.  248–255\. Ieee, 2009.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2009) 邓佳，魏东，理查德·索彻，李佳，凯·李和李飞飞。Imagenet：大规模分层图像数据库。在*2009 IEEE计算机视觉与模式识别会议*，第248–255页，IEEE，2009年。
- en: Der Kiureghian & Ditlevsen (2009) Armen Der Kiureghian and Ove Ditlevsen. Aleatory
    or Epistemic? Does it matter? *Structural safety*, 31(2):105–112, 2009.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Der Kiureghian & Ditlevsen (2009) 阿门·德尔·基乌雷吉安和奥夫·迪特尔维森。偶然性还是认知性？重要吗？*结构安全*，31(2):105–112，2009年。
- en: Desai & Durrett (2020) Shrey Desai and Greg Durrett. Calibration of Pre-trained
    Transformers. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2020, Online, November 16-20, 2020*, pp. 295–302\. Association for Computational
    Linguistics, 2020.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Desai & Durrett (2020) 谢雷·德赛和格雷格·杜雷特。预训练变换器的校准。在博妮·韦伯，特雷弗·科恩，余岚和杨柳（编辑），*2020年自然语言处理经验方法会议论文集，EMNLP
    2020，线上，2020年11月16-20日*，第295–302页，计算语言学协会，2020年。
- en: 'Dietterich & Guyer (2022) Thomas G. Dietterich and Alexander Guyer. The Familiarity
    Hypothesis: Explaining the Behavior of Deep Open Set Methods. *Pattern Recognit.*,
    132:108931, 2022.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dietterich & Guyer (2022) 托马斯·G·迪特里奇和亚历山大·盖耶。熟悉性假设：解释深度开放集方法的行为。*模式识别*，132:108931，2022年。
- en: Dua et al. (2017) Dheeru Dua, Casey Graff, et al. UCI Machine Learning Repository.
    2017.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dua et al. (2017) 德鲁·杜阿，凯西·格拉夫，等。UCI机器学习库。2017年。
- en: 'Duan (2021) Haonan Duan. Method of Moments in Approximate Bayesian Inference:
    From Theory to Practice. Master’s thesis, University of Waterloo, 2021.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan (2021) 华楠·段。近似贝叶斯推断中的矩方法：从理论到实践。硕士论文，滑铁卢大学，2021年。
- en: Dusenberry et al. (2020) Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-An
    Ma, Jasper Snoek, Katherine A. Heller, Balaji Lakshminarayanan, and Dustin Tran.
    Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors. In *Proceedings
    of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July
    2020, Virtual Event*, volume 119 of *Proceedings of Machine Learning Research*,
    pp.  2782–2792\. PMLR, 2020.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dusenberry et al. (2020) 迈克尔·杜森贝里，贾森·杰费尔，叶名文，马依安，贾斯珀·斯诺克，凯瑟琳·A·赫勒，巴拉吉·拉克什米纳拉亚南和达斯汀·特兰。具有秩-1因子的高效可扩展贝叶斯神经网络。在*第37届国际机器学习会议论文集，ICML
    2020，2020年7月13-18日，虚拟会议*，第119卷*机器学习研究论文集*，第2782–2792页，PMLR，2020年。
- en: Elflein et al. (2021) Sven Elflein, Bertrand Charpentier, Daniel Zügner, and
    Stephan Günnemann. On Out-of-distribution Detection with Energy-based Models.
    *arXiv preprint arXiv:2107.08785*, 2021.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elflein et al. (2021) 斯文·艾尔弗莱因，贝特朗·夏尔潘捷，丹尼尔·兹根纳和斯特凡·居内曼。基于能量的模型在分布外检测中的应用。*arXiv预印本
    arXiv:2107.08785*，2021年。
- en: Fanaee-T & Gama (2014) Hadi Fanaee-T and Joao Gama. Event Labeling Combining
    Ensemble Detectors and Background Knowledge. *Progress in Artificial Intelligence*,
    2(2):113–127, 2014.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fanaee-T & Gama (2014) 哈迪·法纳伊-特和若昂·加马。事件标记结合集成检测器和背景知识。*人工智能进展*，2(2):113–127，2014年。
- en: 'Fathullah & Gales (2022) Yassir Fathullah and Mark J. F. Gales. Self-distribution
    distillation: efficient uncertainty estimation. In James Cussens and Kun Zhang
    (eds.), *Uncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth
    Conference on Uncertainty in Artificial Intelligence, UAI 2022, 1-5 August 2022,
    Eindhoven, The Netherlands*, volume 180 of *Proceedings of Machine Learning Research*,
    pp.  663–673\. PMLR, 2022.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fathullah 和 Gales (2022) Yassir Fathullah 和 Mark J. F. Gales。《自我分布蒸馏：高效的不确定性估计》。在
    James Cussens 和 Kun Zhang (编辑) 编著的 *《人工智能中的不确定性，第三十八届人工智能不确定性会议论文集，UAI 2022，2022
    年 8 月 1-5 日，荷兰埃因霍温》* 中，第 180 卷 *《机器学习研究论文集》*，第 663–673 页。PMLR，2022 年。
- en: Fisher (1936) Ronald A Fisher. The Use of Multiple Measurements in Taxonomic
    Problems. *Annals of eugenics*, 7(2):179–188, 1936.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fisher (1936) Ronald A Fisher。《在分类学问题中的多重测量的使用》。*《优生学年刊》*，7(2)：179–188，1936
    年。
- en: 'Fort et al. (2019) Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep
    Ensembles: A Loss Landscape Perspective. *arXiv preprint arXiv:1912.02757*, 2019.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fort 等人 (2019) Stanislav Fort、Huiyi Hu 和 Balaji Lakshminarayanan。《深度集成：一种损失景观视角》。*arXiv
    预印本 arXiv:1912.02757*，2019 年。
- en: 'Fort et al. (2021) Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring
    the Limits of Out-of-Distribution Detection. In *Advances in Neural Information
    Processing Systems 34: Annual Conference on Neural Information Processing Systems
    2021, NeurIPS 2021, December 6-14, 2021, virtual*, pp.  7068–7081, 2021.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fort 等人 (2021) Stanislav Fort、Jie Ren 和 Balaji Lakshminarayanan。《探索分布外检测的极限》。发表于
    *《神经信息处理系统进展 34：2021 年神经信息处理系统年会，NeurIPS 2021，2021 年 12 月 6-14 日，虚拟》*，第 7068–7081
    页，2021 年。
- en: Fortuin et al. (2021) Vincent Fortuin, Mark Collier, Florian Wenzel, James Allingham,
    Jeremiah Liu, Dustin Tran, Balaji Lakshminarayanan, Jesse Berent, Rodolphe Jenatton,
    and Effrosyni Kokiopoulou. Deep Classifiers with Label Noise Modeling and Distance
    Awareness. *arXiv preprint arXiv:2110.02609*, 2021.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fortuin 等人 (2021) Vincent Fortuin、Mark Collier、Florian Wenzel、James Allingham、Jeremiah
    Liu、Dustin Tran、Balaji Lakshminarayanan、Jesse Berent、Rodolphe Jenatton 和 Effrosyni
    Kokiopoulou。《深度分类器的标签噪声建模与距离感知》。*arXiv 预印本 arXiv:2110.02609*，2021 年。
- en: 'Fragoso et al. (2018) Tiago M Fragoso, Wesley Bertoli, and Francisco Louzada.
    Bayesian Model Averaging: A Systematic Review and Conceptual Classification. *International
    Statistical Review*, 86(1):1–28, 2018.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fragoso 等人 (2018) Tiago M Fragoso、Wesley Bertoli 和 Francisco Louzada。《贝叶斯模型平均：系统评审与概念分类》。*《国际统计评论》*，86(1)：1–28，2018
    年。
- en: 'Gal & Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian
    Approximation: Representing Model Uncertainty in Deep Learning. In *International
    conference on Machine Learning*, pp. 1050–1059, 2016.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal 和 Ghahramani (2016) Yarin Gal 和 Zoubin Ghahramani。《Dropout 作为贝叶斯近似：在深度学习中表示模型不确定性》。发表于
    *《国际机器学习会议》*，第 1050–1059 页，2016 年。
- en: Gal et al. (2016) Yarin Gal et al. Uncertainty in Deep Learning. 2016.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal 等人 (2016) Yarin Gal 等人。《深度学习中的不确定性》。2016 年。
- en: 'Garipov et al. (2018) Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P.
    Vetrov, and Andrew Gordon Wilson. Loss Surfaces, Mode Connectivity, and Fast Ensembling
    of DNNs. In *Advances in Neural Information Processing Systems 31: Annual Conference
    on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, pp.  8803–8812, 2018.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garipov 等人 (2018) Timur Garipov、Pavel Izmailov、Dmitrii Podoprikhin、Dmitry P.
    Vetrov 和 Andrew Gordon Wilson。《损失曲面、模式连接性及 DNNs 的快速集成》。发表于 *《神经信息处理系统进展 31：2018
    年神经信息处理系统年会，NeurIPS 2018，2018 年 12 月 3-8 日，加拿大蒙特利尔》*，第 8803–8812 页，2018 年。
- en: Gawlikowski et al. (2022) Jakob Gawlikowski, Sudipan Saha, Anna M. Kruspe, and
    Xiao Xiang Zhu. An Advanced Dirichlet Prior Network for Out-of-Distribution Detection
    in Remote Sensing. *IEEE Trans. Geosci. Remote. Sens.*, 60:1–19, 2022.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gawlikowski 等人 (2022) Jakob Gawlikowski、Sudipan Saha、Anna M. Kruspe 和 Xiao Xiang
    Zhu。《用于遥感中分布外检测的先进 Dirichlet 先验网络》。*《IEEE 地球科学与遥感学报》*，60：1–19，2022 年。
- en: Gelman et al. (1995) Andrew Gelman, John B Carlin, Hal S Stern, and Donald B
    Rubin. *Bayesian Data Analysis*. Chapman and Hall/CRC, 1995.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gelman 等人 (1995) Andrew Gelman、John B Carlin、Hal S Stern 和 Donald B Rubin。《*贝叶斯数据分析*》。Chapman
    and Hall/CRC，1995 年。
- en: Gerritsma et al. (1981) J Gerritsma, R Onnink, and A Versluis. Geometry, Resistance
    and Stability of the Delft Systematic Yacht Hull Series. *International shipbuilding
    progress*, 28(328):276–297, 1981.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerritsma 等人 (1981) J Gerritsma、R Onnink 和 A Versluis。《代尔夫特系统化游艇船体系列的几何、阻力与稳定性》。*《国际造船进展》*，28(328)：276–297，1981
    年。
- en: Ghesu et al. (2019) Florin C. Ghesu, Bogdan Georgescu, Eli Gibson, Sebastian
    Gündel, Mannudeep K. Kalra, Ramandeep Singh, Subba R. Digumarthy, Sasa Grbic,
    and Dorin Comaniciu. Quantifying and Leveraging Classification Uncertainty for
    Chest Radiograph Assessment. In *Medical Image Computing and Computer Assisted
    Intervention - MICCAI 2019 - 22nd International Conference, Shenzhen, China, October
    13-17, 2019, Proceedings, Part VI*, volume 11769 of *Lecture Notes in Computer
    Science*, pp.  676–684\. Springer, 2019.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghesu et al. (2019) Florin C. Ghesu、Bogdan Georgescu、Eli Gibson、Sebastian Gündel、Mannudeep
    K. Kalra、Ramandeep Singh、Subba R. Digumarthy、Sasa Grbic 和 Dorin Comaniciu。量化和利用分类不确定性进行胸部
    X 光评估。在 *医学图像计算与计算机辅助手术 - MICCAI 2019 - 第 22 届国际会议，中国深圳，2019年10月13-17日，论文集，第 VI
    部分*，*Lecture Notes in Computer Science* 第 11769 卷，页码 676–684。Springer，2019。
- en: 'Giles et al. (1998) C Lee Giles, Kurt D Bollacker, and Steve Lawrence. CiteSeer:
    An Automatic Citation Indexing System. In *Proceedings of the third ACM conference
    on Digital libraries*, pp.  89–98, 1998.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giles et al. (1998) C Lee Giles、Kurt D Bollacker 和 Steve Lawrence。CiteSeer：自动文献引用索引系统。在
    *第三届 ACM 数字图书馆会议论文集*，页码 89–98，1998。
- en: Goodfellow et al. (2014) Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz,
    Sacha Arnoud, and Vinay D. Shet. Multi-Digit Number Recognition from Street View
    Imagery using Deep Convolutional Neural Networks. In *2nd International Conference
    on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
    Conference Track Proceedings*, 2014.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014) Ian J. Goodfellow、Yaroslav Bulatov、Julian Ibarz、Sacha
    Arnoud 和 Vinay D. Shet。利用深度卷积神经网络进行街景图像中的多位数字识别。在 *第 2 届国际学习表征会议，ICLR 2014，加拿大班夫，2014年4月14-16日，会议论文集*，2014。
- en: (54) Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud,
    Mohammad Norouzi, and Kevin Swersky. Your Classifier is Secretly an Energy-Based
    Model and You Should Treat It Like One. In *8th International Conference on Learning
    Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (54) Will Grathwohl、Kuan-Chieh Wang、Jörn-Henrik Jacobsen、David Duvenaud、Mohammad
    Norouzi 和 Kevin Swersky。你的分类器实际上是一个基于能量的模型，你应该像对待一个那样对待它。在 *第 8 届国际学习表征会议，ICLR
    2020，2020年4月26-30日，埃塞俄比亚亚的斯亚贝巴*。
- en: Gu et al. (2021) Ang Nan Gu, Christina Luong, Mohammad H. Jafari, Nathan Van
    Woudenberg, Hany Girgis, Purang Abolmaesumi, and Teresa Tsang. Efficient Echocardiogram
    View Classification with Sampling-Free Uncertainty Estimation. In *Simplifying
    Medical Ultrasound - Second International Workshop, ASMUS 2021, Held in Conjunction
    with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings*, volume
    12967 of *Lecture Notes in Computer Science*, pp.  139–148\. Springer, 2021.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2021) Ang Nan Gu、Christina Luong、Mohammad H. Jafari、Nathan Van Woudenberg、Hany
    Girgis、Purang Abolmaesumi 和 Teresa Tsang。无采样不确定性估计的高效超声图像视图分类。在 *简化医学超声 - 第二届国际研讨会，ASMUS
    2021，MICCAI 2021 联合举办，法国斯特拉斯堡，2021年9月27日，论文集*，*Lecture Notes in Computer Science*
    第 12967 卷，页码 139–148。Springer，2021。
- en: Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
    On Calibration of Modern Neural Networks. In *Proceedings of the 34th International
    Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August
    2017*, volume 70 of *Proceedings of Machine Learning Research*, pp.  1321–1330\.
    PMLR, 2017.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2017) Chuan Guo、Geoff Pleiss、Yu Sun 和 Kilian Q. Weinberger。现代神经网络的校准。在
    *第 34 届国际机器学习大会，ICML 2017，2017年8月6-11日，澳大利亚悉尼*，*Proceedings of Machine Learning
    Research* 第 70 卷，页码 1321–1330。PMLR，2017。
- en: Harrison Jr & Rubinfeld (1978) David Harrison Jr and Daniel L Rubinfeld. Hedonic
    Housing Prices and the Demand for Clean Air. *Journal of environmental economics
    and management*, 5(1):81–102, 1978.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harrison Jr & Rubinfeld (1978) David Harrison Jr 和 Daniel L Rubinfeld。享乐型住房价格与对清洁空气的需求。*环境经济与管理学杂志*，5(1):81–102，1978。
- en: Haussmann et al. (2019) Manuel Haussmann, Sebastian Gerwinn, and Melih Kandemir.
    Bayesian Evidential Deep Learning with PAC Regularization. *arXiv preprint arXiv:1906.00816*,
    2019.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haussmann et al. (2019) Manuel Haussmann、Sebastian Gerwinn 和 Melih Kandemir。贝叶斯证据深度学习与
    PAC 正则化。*arXiv 预印本 arXiv:1906.00816*，2019。
- en: Havtorn et al. (2021) Jakob Drachmann Havtorn, Jes Frellsen, Søren Hauberg,
    and Lars Maaløe. Hierarchical VAEs Know What They Don’t Know. In *Proceedings
    of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
    2021, Virtual Event*, volume 139 of *Proceedings of Machine Learning Research*,
    pp.  4117–4128\. PMLR, 2021.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Havtorn et al. (2021) Jakob Drachmann Havtorn、Jes Frellsen、Søren Hauberg 和 Lars
    Maaløe。分层 VAE 知道它们不知道什么。在 *第 38 届国际机器学习大会，ICML 2021，2021年7月18-24日，虚拟活动*，*Proceedings
    of Machine Learning Research* 第 139 卷，页码 4117–4128。PMLR，2021。
- en: He et al. (2020) Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian
    Deep Ensembles via the Neural Tangent Kernel. *Advances in neural information
    processing systems*, 33:1010–1022, 2020.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2020) Bobby He, Balaji Lakshminarayanan, 和 Yee Whye Teh. 基于神经切线核的贝叶斯深度集成方法。*神经信息处理系统进展*,
    33:1010–1022, 2020。
- en: Hein et al. (2019) Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf.
    Why ReLU Networks Yield High-Confidence Predictions Far Away From the Training
    Data and How to Mitigate the Problem. In *IEEE Conference on Computer Vision and
    Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*, pp. 41–50\.
    Computer Vision Foundation / IEEE, 2019.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hein et al. (2019) Matthias Hein, Maksym Andriushchenko, 和 Julian Bitterwolf.
    为什么 ReLU 网络在远离训练数据的地方会产生高置信度预测以及如何缓解这一问题。在 *IEEE 计算机视觉与模式识别大会，CVPR 2019，长滩，加州，美国，2019年6月16-20日*
    中，第 41–50 页。计算机视觉基金会 / IEEE, 2019。
- en: 'Hemmer et al. (2022) Patrick Hemmer, Niklas Kühl, and Jakob Schöffer. Deal:
    Deep Evidential Active Learning for Image Classification. In *Deep Learning Applications,
    Volume 3*, pp.  171–192. Springer, 2022.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hemmer et al. (2022) Patrick Hemmer, Niklas Kühl, 和 Jakob Schöffer. DEAL：用于图像分类的深度证据主动学习。在
    *深度学习应用，第 3 卷* 中，第 171–192 页。Springer, 2022。
- en: 'Hemphill et al. (1990) Charles T Hemphill, John J Godfrey, and George R Doddington.
    The ATIS Spoken Language Systems Pilot Corpus. In *Speech and Natural Language:
    Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990*,
    1990.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hemphill et al. (1990) Charles T Hemphill, John J Godfrey, 和 George R Doddington.
    ATIS 口语语言系统试点语料库。在 *语音和自然语言：在宾夕法尼亚州隐谷举行的研讨会论文集，1990年6月24-27日* 中，1990。
- en: Hendrycks & Gimpel (2017) Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting
    Misclassified and Out-of-Distribution Examples in Neural Networks. In *5th International
    Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
    2017, Conference Track Proceedings*, 2017.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks & Gimpel (2017) Dan Hendrycks 和 Kevin Gimpel. 检测神经网络中错误分类和分布外样本的基线。在
    *第五届国际学习表征会议，ICLR 2017，法国图卢兹，2017年4月24-26日，会议论文集* 中，2017。
- en: Hernández-Lobato & Adams (2015) José Miguel Hernández-Lobato and Ryan Adams.
    Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks.
    In *International conference on machine learning*, pp. 1861–1869\. PMLR, 2015.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernández-Lobato & Adams (2015) José Miguel Hernández-Lobato 和 Ryan Adams. 可扩展学习贝叶斯神经网络的概率反向传播。在
    *国际机器学习会议* 中，第 1861–1869 页。PMLR, 2015。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling
    the Knowledge in a Neural Network. *arXiv preprint arXiv:1503.02531*, 2(7), 2015.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, Jeff Dean 等。提取神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531*, 2(7), 2015。
- en: Hinton & Van Camp (1993) Geoffrey E Hinton and Drew Van Camp. Keeping the Neural
    Networks Simple by Minimizing the Description Length of the Weights. In *Proceedings
    of the sixth annual conference on Computational learning theory*, pp.  5–13, 1993.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton & Van Camp (1993) Geoffrey E Hinton 和 Drew Van Camp. 通过最小化权重描述长度来保持神经网络的简单性。在
    *第六届计算学习理论年会论文集* 中，第 5–13 页，1993。
- en: Hobbhahn et al. (2022) Marius Hobbhahn, Agustinus Kristiadi, and Philipp Hennig.
    Fast predictive uncertainty for classification with bayesian deep networks. In
    *Uncertainty in Artificial Intelligence*, pp.  822–832. PMLR, 2022.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hobbhahn et al. (2022) Marius Hobbhahn, Agustinus Kristiadi, 和 Philipp Hennig.
    使用贝叶斯深度网络进行分类的快速预测不确定性。在 *人工智能中的不确定性* 中，第 822–832 页。PMLR, 2022。
- en: Houlsby et al. (2011) Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté
    Lengyel. Bayesian Active Learning for Classification and Preference Learning.
    *arXiv preprint arXiv:1112.5745*, 2011.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. (2011) Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, 和 Máté
    Lengyel. 用于分类和偏好学习的贝叶斯主动学习。*arXiv 预印本 arXiv:1112.5745*, 2011。
- en: 'Hu et al. (2020) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open Graph Benchmark: Datasets
    for Machine Learning on Graphs. *Advances in neural information processing systems*,
    33:22118–22133, 2020.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2020) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, 和 Jure Leskovec. 开放图基准：用于图上机器学习的数据集。*神经信息处理系统进展*,
    33:22118–22133, 2020。
- en: Hu et al. (2021) Yibo Hu, Yuzhe Ou, Xujiang Zhao, Jin-Hee Cho, and Feng Chen.
    Multidimensional Uncertainty-Aware Evidential Neural Networks. In *Thirty-Fifth
    AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference
    on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual
    Event, February 2-9, 2021*, pp.  7815–7822\. AAAI Press, 2021.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2021) Yibo Hu, Yuzhe Ou, Xujiang Zhao, Jin-Hee Cho, 和 Feng Chen.
    多维不确定性感知证据神经网络。发表于 *第三十五届AAAI人工智能大会，AAAI 2021，第三十三届人工智能创新应用会议，IAAI 2021，第十一届人工智能教育进展研讨会，EAAI
    2021，虚拟会议，2021年2月2-9日*，第7815–7822页。AAAI出版社，2021年。
- en: 'Huang et al. (2017) Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E.
    Hopcroft, and Kilian Q. Weinberger. Snapshot Ensembles: Train 1, Get M for Free.
    In *5th International Conference on Learning Representations, ICLR 2017, Toulon,
    France, April 24-26, 2017, Conference Track Proceedings*, 2017.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2017) Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E.
    Hopcroft, 和 Kilian Q. Weinberger. 快照集成：训练一个，免费获得M个。发表于 *第五届国际学习表征会议，ICLR 2017，法国图盎，2017年4月24-26日，会议论文集*，2017年。
- en: Huang et al. (2018) Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao, Dingfu
    Zhou, Peng Wang, Yuanqing Lin, and Ruigang Yang. The Apolloscape Dataset for Autonomous
    Driving. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition workshops*, pp.  954–960, 2018.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2018) Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao, Dingfu
    Zhou, Peng Wang, Yuanqing Lin, 和 Ruigang Yang. Apolloscape 数据集用于自动驾驶。发表于 *IEEE计算机视觉与模式识别会议研讨会论文集*，第954–960页，2018年。
- en: 'Hüllermeier (2022) Eyke Hüllermeier. Quantifying Aleatoric and Epistemic Uncertainty
    in Machine Learning: Are Conditional Entropy and Mutual Information Appropriate
    Measures? *arXiv preprint arXiv:2209.03302*, 2022.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hüllermeier (2022) Eyke Hüllermeier. 量化机器学习中的随机性和认知不确定性：条件熵和互信息是否是合适的度量？ *arXiv预印本
    arXiv:2209.03302*，2022年。
- en: 'Hüllermeier & Waegeman (2021) Eyke Hüllermeier and Willem Waegeman. Aleatoric
    and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and
    Methods. *Mach. Learn.*, 110(3):457–506, 2021.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hüllermeier & Waegeman (2021) Eyke Hüllermeier 和 Willem Waegeman. 机器学习中的随机性和认知不确定性：概念和方法简介。*机器学习*，110(3):457–506，2021年。
- en: 'Ilyas et al. (2019) Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan
    Engstrom, Brandon Tran, and Aleksander Madry. Adversarial Examples Are Not Bugs,
    They Are Features. In *Advances in Neural Information Processing Systems 32: Annual
    Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
    8-14, 2019, Vancouver, BC, Canada*, pp.  125–136, 2019.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilyas et al. (2019) Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan
    Engstrom, Brandon Tran, 和 Aleksander Madry. 对抗样本不是错误，它们是特征。发表于 *神经信息处理系统进展 32：2019年神经信息处理系统年度会议，NeurIPS
    2019，2019年12月8-14日，加拿大不列颠哥伦比亚省温哥华*，第125–136页，2019年。
- en: Izmailov et al. (2021a) Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, and
    Andrew G Wilson. Dangers of Bayesian Model Averaging under Covariate Shift. *Advances
    in Neural Information Processing Systems*, 34:3309–3322, 2021a.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izmailov et al. (2021a) Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, 和 Andrew
    G Wilson. 协变量偏移下的贝叶斯模型平均的危险。*神经信息处理系统进展*，34:3309–3322，2021年。
- en: Izmailov et al. (2021b) Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, and
    Andrew Gordon Wilson. What Are Bayesian Neural Network Posteriors Really Like?
    In *Proceedings of the 38th International Conference on Machine Learning, ICML
    2021, 18-24 July 2021, Virtual Event*, volume 139 of *Proceedings of Machine Learning
    Research*, pp.  4629–4640\. PMLR, 2021b.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izmailov et al. (2021b) Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, 和
    Andrew Gordon Wilson. 贝叶斯神经网络后验到底是什么样的？发表于 *第38届国际机器学习大会论文集，ICML 2021，2021年7月18-24日，虚拟会议*，第139卷
    *机器学习研究论文集*，第4629–4640页。PMLR，2021b。
- en: 'Jacovi et al. (2021) Alon Jacovi, Ana Marasović, Tim Miller, and Yoav Goldberg.
    Formalizing trust in artificial intelligence: Prerequisites, Causes and Goals
    of Human Trust in AI. In *Proceedings of the 2021 ACM conference on fairness,
    accountability, and transparency*, pp.  624–635, 2021.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacovi et al. (2021) Alon Jacovi, Ana Marasović, Tim Miller, 和 Yoav Goldberg.
    正式化对人工智能的信任：人类对AI信任的前提、原因和目标。发表于 *2021年ACM公平性、问责制与透明度会议论文集*，第624–635页，2021年。
- en: Jeffreys (1998) Harold Jeffreys. *The Theory of Probability*. OUP Oxford, 1998.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeffreys (1998) Harold Jeffreys. *概率论*. 牛津大学出版社，1998。
- en: Jia et al. (2017) Robin Jia, Larry Heck, Dilek Hakkani-Tür, and Georgi Nikolov.
    Learning Concepts through Conversations in Spoken Dialogue Systems. In *2017 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    pp.  5725–5729\. IEEE, 2017.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等 (2017) Robin Jia, Larry Heck, Dilek Hakkani-Tür, 和 Georgi Nikolov. 通过对话系统学习概念。发表于*2017
    IEEE国际声学、语音与信号处理会议 (ICASSP)*，第 5725–5729 页。IEEE，2017 年。
- en: Joo et al. (2020) Taejong Joo, Uijung Chung, and Min-Gwan Seo. Being bayesian
    about categorical probability. In *International conference on machine learning*,
    pp. 4950–4961\. PMLR, 2020.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joo 等 (2020) Taejong Joo, Uijung Chung, 和 Min-Gwan Seo。对分类概率的贝叶斯思考。在*国际机器学习会议*，第
    4950–4961 页。PMLR，2020 年。
- en: Jordan et al. (1999) Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola,
    and Lawrence K Saul. An Introduction to Variational Methods for Graphical Models.
    *Machine learning*, 37(2):183–233, 1999.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jordan 等 (1999) Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, 和 Lawrence
    K Saul。图模型变分方法简介。*机器学习*，37(2):183–233，1999 年。
- en: Juneja et al. (2022) Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, João Sedoc,
    and Naomi Saphra. Linear Connectivity Reveals Generalization Strategies. *arXiv
    preprint arXiv:2205.12411*, 2022.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juneja 等 (2022) Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, João Sedoc, 和
    Naomi Saphra。线性连接揭示泛化策略。*arXiv 预印本 arXiv:2205.12411*，2022 年。
- en: Kendall & Gal (2017) Alex Kendall and Yarin Gal. What Uncertainties do We Need
    in Bayesian Deep Learning for Computer Vision? *Advances in neural information
    processing systems*, 30, 2017.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall & Gal (2017) Alex Kendall 和 Yarin Gal。计算机视觉中的贝叶斯深度学习需要哪些不确定性？*神经信息处理系统进展*，30，2017
    年。
- en: 'Kim et al. (2019) Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia
    He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al.
    PubChem 2019 Update: Improved Access to Chemical Data. *Nucleic acids research*,
    47(D1):D1102–D1109, 2019.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2019) Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian
    He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu 等。PubChem 2019
    更新：改进化学数据访问。*核酸研究*，47(D1):D1102–D1109，2019 年。
- en: 'Kingma & Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic
    Optimization. In Yoshua Bengio and Yann LeCun (eds.), *3rd International Conference
    on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference
    Track Proceedings*, 2015.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Ba (2015) Diederik P. Kingma 和 Jimmy Ba。Adam：一种随机优化方法。在 Yoshua Bengio
    和 Yann LeCun (编辑)，*第 3 届国际学习表征会议，ICLR 2015，加州圣地亚哥，美国，2015 年 5 月 7-9 日，会议论文集*，2015
    年。
- en: Kingma & Welling (2014) Diederik P. Kingma and Max Welling. Auto-Encoding Variational
    Bayes. In *2nd International Conference on Learning Representations, ICLR 2014,
    Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings*, 2014.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Welling (2014) Diederik P. Kingma 和 Max Welling。自动编码变分贝叶斯。在*第 2 届国际学习表征会议，ICLR
    2014，加拿大班夫，2014 年 4 月 14-16 日，会议论文集*，2014 年。
- en: 'Klicpera et al. (2019) Johannes Klicpera, Aleksandar Bojchevski, and Stephan
    Günnemann. Predict then Propagate: Graph Neural Networks meet Personalized PageRank.
    In *7th International Conference on Learning Representations, ICLR 2019, New Orleans,
    LA, USA, May 6-9, 2019*, 2019.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klicpera 等 (2019) Johannes Klicpera, Aleksandar Bojchevski, 和 Stephan Günnemann。预测然后传播：图神经网络与个性化
    PageRank 的碰撞。在*第 7 届国际学习表征会议，ICLR 2019，新奥尔良，美国，2019 年 5 月 6-9 日*，2019 年。
- en: Kompa et al. (2021) Benjamin Kompa, Jasper Snoek, and Andrew L. Beam. Empirical
    Frequentist Coverage of Deep Learning Uncertainty Quantification Procedures. *Entropy*,
    23(12):1608, 2021.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kompa 等 (2021) Benjamin Kompa, Jasper Snoek, 和 Andrew L. Beam。深度学习不确定性量化程序的经验频率覆盖。*熵*，23(12):1608，2021
    年。
- en: 'Kopetzki et al. (2021) Anna-Kathrin Kopetzki, Bertrand Charpentier, Daniel
    Zügner, Sandhya Giri, and Stephan Günnemann. Evaluating Robustness of Predictive
    Uncertainty Estimation: Are Dirichlet-based Models Reliable? In *Proceedings of
    the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
    Virtual Event*, volume 139 of *Proceedings of Machine Learning Research*, pp. 
    5707–5718\. PMLR, 2021.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kopetzki 等 (2021) Anna-Kathrin Kopetzki, Bertrand Charpentier, Daniel Zügner,
    Sandhya Giri, 和 Stephan Günnemann。评估预测不确定性估计的鲁棒性：基于 Dirichlet 的模型是否可靠？发表于*第 38
    届国际机器学习会议，ICML 2021，2021 年 7 月 18-24 日，虚拟会议*，*机器学习研究论文集*第 139 卷，第 5707–5718 页。PMLR，2021
    年。
- en: Kristiadi et al. (2020) Agustinus Kristiadi, Matthias Hein, and Philipp Hennig.
    Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks. In *Proceedings
    of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July
    2020, Virtual Event*, volume 119 of *Proceedings of Machine Learning Research*,
    pp.  5436–5446\. PMLR, 2020.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kristiadi et al. (2020) Agustinus Kristiadi、Matthias Hein 和 Philipp Hennig。即使只是稍微，贝叶斯方法也能修正
    ReLU 网络中的过度自信。见于 *第37届国际机器学习大会论文集，ICML 2020，2020年7月13-18日，虚拟会议*，第119卷 *机器学习研究论文集*，第5436–5446页。PMLR，2020年。
- en: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. Learning Multiple
    Layers of Features from Tiny Images. 2009.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky et al. (2009) Alex Krizhevsky、Geoffrey Hinton 等。 从小图像中学习多个层次的特征。2009年。
- en: 'Kull et al. (2019) Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo
    Silva Filho, Hao Song, and Peter Flach. Beyond Temperature Scaling: Obtaining
    Well-Calibrated Multi-Class Probabilities with Dirichlet Calibration. *Advances
    in neural information processing systems*, 32, 2019.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kull et al. (2019) Meelis Kull、Miquel Perello Nieto、Markus Kängsepp、Telmo Silva
    Filho、Hao Song 和 Peter Flach。超越温度缩放：使用狄利克雷校准获得良好校准的多类别概率。*神经信息处理系统进展*，第32卷，2019年。
- en: Kupperman (1964) Morton Kupperman. Probabilities of Hypotheses and Information-Statistics
    in Sampling from Exponential-Class Populations. *Selected Mathematical Papers*,
    29(2):57, 1964.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kupperman (1964) Morton Kupperman。假设的概率和从指数类总体中抽样的信息统计。*精选数学论文*，29(2)：57，1964年。
- en: Kurakin et al. (2017) Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial
    Examples in the Physical World. In *5th International Conference on Learning Representations,
    ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings*, 2017.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurakin et al. (2017) Alexey Kurakin、Ian J. Goodfellow 和 Samy Bengio。物理世界中的对抗样本。见于
    *第5届国际学习表示大会，ICLR 2017，法国图伦，2017年4月24-26日，研讨会论文集*，2017年。
- en: 'Lahlou et al. (2022) Salem Lahlou, Moksh Jain, Hadi Nekoei, Victor I Butoi,
    Paul Bertin, Jarrid Rector-Brooks, Maksym Korablyov, and Yoshua Bengio. DEUP:
    Direct Epistemic Uncertainty Prediction. *Transactions on Machine Learning Research*,
    2022. ISSN 2835-8856.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lahlou et al. (2022) Salem Lahlou、Moksh Jain、Hadi Nekoei、Victor I Butoi、Paul
    Bertin、Jarrid Rector-Brooks、Maksym Korablyov 和 Yoshua Bengio。DEUP：直接的知识不确定性预测。*机器学习研究交易*，2022年。ISSN
    2835-8856。
- en: Lake et al. (2015) Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum.
    Human-Level Concept Learning Through Probabilistic Program Induction. *Science*,
    350(6266):1332–1338, 2015.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lake et al. (2015) Brenden M Lake、Ruslan Salakhutdinov 和 Joshua B Tenenbaum。通过概率程序归纳进行人类水平的概念学习。*科学*，350(6266)：1332–1338，2015年。
- en: Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel, and
    Charles Blundell. Simple and Scalable Predictive Uncertainty Estimation using
    Deep Ensembles. In *Advances in neural information processing systems*, pp. 6402–6413,
    2017.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakshminarayanan et al. (2017) Balaji Lakshminarayanan、Alexander Pritzel 和 Charles
    Blundell。利用深度集成进行简单且可扩展的预测不确定性估计。见于 *神经信息处理系统进展*，第6402–6413页，2017年。
- en: LeCun (1998) Yann LeCun. The MNIST Database of Handwritten Digits, 1998. URL
    [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun (1998) Yann LeCun. 《手写数字的 MNIST 数据库》，1998年。网址 [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)。
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    Gradient-Based Learning Applied to Document Recognition. *Proceedings of the IEEE*,
    86(11):2278–2324, 1998.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1998) Yann LeCun、Léon Bottou、Yoshua Bengio 和 Patrick Haffner。基于梯度的学习应用于文档识别。*IEEE
    会议录*，86(11)：2278–2324，1998年。
- en: 'Lee et al. (2018) Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A Simple
    Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks.
    In *Advances in Neural Information Processing Systems 31: Annual Conference on
    Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, pp.  7167–7177, 2018.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2018) Kimin Lee、Kibok Lee、Honglak Lee 和 Jinwoo Shin。一个简单的统一框架用于检测分布外样本和对抗性攻击。见于
    *神经信息处理系统第31届年会：2018年神经信息处理系统年会 NeurIPS 2018，2018年12月3-8日，蒙特利尔，加拿大*，第7167–7177页，2018年。
- en: 'Lei & Wasserman (2014) Jing Lei and Larry Wasserman. Distribution-free Prediction
    Bands for Non-parametric Regression. *Journal of the Royal Statistical Society:
    Series B (Statistical Methodology)*, 76(1):71–96, 2014.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei & Wasserman (2014) Jing Lei 和 Larry Wasserman。用于非参数回归的分布无关预测带。*皇家统计学会杂志：B系列（统计方法学）*，76(1)：71–96，2014年。
- en: Li et al. (2022) Hao Li, Yang Nan, Javier Del Ser, and Guang Yang. Region-Based
    Evidential Deep Learning to Quantify Uncertainty and Improve Robustness of Brain
    Tumor Segmentation. *arXiv preprint arXiv:2208.06038*, 2022.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Hao Li、Yang Nan、Javier Del Ser 和 Guang Yang. 基于区域的证据深度学习以量化不确定性并提高脑肿瘤分割的鲁棒性。*arXiv预印本
    arXiv:2208.06038*，2022年。
- en: Liang et al. (2018) Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing The Reliability
    of Out-of-distribution Image Detection in Neural Networks. In *6th International
    Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
    30 - May 3, 2018, Conference Track Proceedings*, 2018.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2018) Shiyu Liang、Yixuan Li 和 R. Srikant. 提高神经网络中离群图像检测的可靠性。在
    *第6届国际表示学习会议，ICLR 2018，2018年4月30日-5月3日，加拿大温哥华，会议论文集*，2018年。
- en: 'Liao & Sundar (2022) Q. Vera Liao and S. Shyam Sundar. Designing for Responsible
    Trust in AI Systems: A Communication Perspective. In *FAccT ’22: 2022 ACM Conference
    on Fairness, Accountability, and Transparency, Seoul, Republic of Korea, June
    21 - 24, 2022*, pp.  1257–1268\. ACM, 2022.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao & Sundar (2022) Q. Vera Liao 和 S. Shyam Sundar. 从沟通角度设计负责任的AI系统信任。在 *FAccT
    ’22：2022年ACM公平性、问责制与透明度会议，韩国首尔，2022年6月21-24日*，第1257–1268页，ACM，2022年。
- en: Lin (2016) Jiayu Lin. On the Dirichlet Distribution. *Mater’s Report*, 2016.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin (2016) Jiayu Lin. 关于Dirichlet分布。*硕士报告*，2016年。
- en: 'Liu et al. (2020) Jeremiah Z. Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania
    Bedrax-Weiss, and Balaji Lakshminarayanan. Simple and Principled Uncertainty Estimation
    with Deterministic Deep Learning via Distance Awareness. In *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020) Jeremiah Z. Liu、Zi Lin、Shreyas Padhy、Dustin Tran、Tania Bedrax-Weiss
    和 Balaji Lakshminarayanan. 通过距离感知的确定性深度学习进行简单而原理性的 不确定性估计。在 *神经信息处理系统进展 33：2020年神经信息处理系统年会，NeurIPS
    2020，2020年12月6-12日，虚拟会议*，2020年。
- en: 'Liu et al. (2007) Tiqing Liu, Yuhmei Lin, Xin Wen, Robert N Jorissen, and Michael K
    Gilson. BindingDB: A Web-Accessible Database of Experimentally Determined Protein–Ligand
    Binding Affinities. *Nucleic acids research*, 35(suppl_1):D198–D201, 2007.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2007) Tiqing Liu、Yuhmei Lin、Xin Wen、Robert N Jorissen 和 Michael
    K Gilson. BindingDB：一个可以在线访问的实验确定蛋白质-配体结合亲和力数据库。*核酸研究*，35(suppl_1):D198–D201，2007年。
- en: Liu et al. (2021) Zhijian Liu, Alexander Amini, Sibo Zhu, Sertac Karaman, Song
    Han, and Daniela L. Rus. Efficient and Robust LiDAR-Based End-to-End Navigation.
    In *IEEE International Conference on Robotics and Automation, ICRA 2021, Xi’an,
    China, May 30 - June 5, 2021*, pp.  13247–13254. IEEE, 2021.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021) Zhijian Liu、Alexander Amini、Sibo Zhu、Sertac Karaman、Song Han
    和 Daniela L. Rus. 高效且鲁棒的基于LiDAR的端到端导航。在 *IEEE国际机器人与自动化会议，ICRA 2021，中国西安，2021年5月30日-6月5日*，第13247–13254页，IEEE，2021年。
- en: Liu et al. (2015) Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep
    Learning Face Attributes in the Wild. In *Proceedings of the IEEE international
    conference on computer vision*, pp.  3730–3738, 2015.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2015) Ziwei Liu、Ping Luo、Xiaogang Wang 和 Xiaoou Tang. 深度学习在自然环境中的人脸属性。在
    *IEEE国际计算机视觉会议论文集*，第3730–3738页，2015年。
- en: 'MacKay (1995) David JC MacKay. Developments in Probabilistic Modelling with
    Neural Networks—Ensemble Learning. In *Neural Networks: Artificial Intelligence
    and Industrial Applications*, pp.  191–198\. Springer, 1995.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacKay (1995) David JC MacKay. 基于神经网络的概率建模的发展——集成学习。在 *神经网络：人工智能与工业应用*，第191–198页，Springer，1995年。
- en: MacKay (1998) David JC MacKay. Choice of basis for Laplace approximation. *Machine
    learning*, 33:77–86, 1998.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacKay (1998) David JC MacKay. 拉普拉斯近似的基函数选择。*机器学习*，33:77–86，1998年。
- en: Mackay (1992) David John Cameron Mackay. *Bayesian Methods for Adaptive Models*.
    California Institute of Technology, 1992.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mackay (1992) David John Cameron Mackay. *自适应模型的贝叶斯方法*。加州理工学院，1992年。
- en: 'Malinin & Gales (2018) Andrey Malinin and Mark J. F. Gales. Predictive Uncertainty
    Estimation via Prior Networks. In *Advances in Neural Information Processing Systems
    31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
    3-8 December 2018, Montréal, Canada*, pp.  7047–7058, 2018.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinin & Gales (2018) Andrey Malinin 和 Mark J. F. Gales. 通过先验网络进行预测不确定性估计。在
    *神经信息处理系统进展 31：2018年神经信息处理系统年会，NeurIPS 2018，2018年12月3-8日，加拿大蒙特利尔*，第7047–7058页，2018年。
- en: 'Malinin & Gales (2019) Andrey Malinin and Mark J. F. Gales. Reverse KL-Divergence
    Training of Prior Networks: Improved Uncertainty and Adversarial Robustness. In
    *Advances in Neural Information Processing Systems 32: Annual Conference on Neural
    Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,
    BC, Canada*, pp.  14520–14531, 2019.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinin & Gales (2019) Andrey Malinin 和 Mark J. F. Gales. 反向KL散度训练先验网络：改进的不确定性和对抗鲁棒性。发表于
    *神经信息处理系统进展 32：2019年神经信息处理系统年会，NeurIPS 2019，2019年12月8-14日，加拿大温哥华*，第14520–14531页，2019年。
- en: Malinin et al. (2020a) Andrey Malinin, Sergey Chervontsev, Ivan Provilkov, and
    Mark Gales. Regression Prior Networks. *arXiv preprint arXiv:2006.11590*, 2020a.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinin et al. (2020a) Andrey Malinin, Sergey Chervontsev, Ivan Provilkov, 和
    Mark Gales. 回归先验网络。*arXiv预印本 arXiv:2006.11590*，2020年。
- en: Malinin et al. (2020b) Andrey Malinin, Bruno Mlodozeniec, and Mark J. F. Gales.
    Ensemble Distribution Distillation. In *8th International Conference on Learning
    Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*, 2020b.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinin et al. (2020b) Andrey Malinin, Bruno Mlodozeniec, 和 Mark J. F. Gales.
    集成分布蒸馏。发表于 *第8届国际学习表征会议，ICLR 2020，2020年4月26-30日，埃塞俄比亚亚的斯亚贝巴*，2020年。
- en: Mao (2019) Lei Mao. Introduction to Exponential Family, 2019. URL [https://zhiyzuo.github.io/Exponential-Family-Distributions/](https://zhiyzuo.github.io/Exponential-Family-Distributions/).
    Accessed April 2022.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao (2019) Lei Mao. 指数族介绍，2019年。网址 [https://zhiyzuo.github.io/Exponential-Family-Distributions/](https://zhiyzuo.github.io/Exponential-Family-Distributions/)。访问于2022年4月。
- en: 'Masegosa (2020) Andrés R. Masegosa. Learning under Model Misspecification:
    Applications to Variational and Ensemble methods. In *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masegosa (2020) Andrés R. Masegosa. 模型误设下的学习：应用于变分和集成方法。发表于 *神经信息处理系统进展 33：2020年神经信息处理系统年会，NeurIPS
    2020，2020年12月6-12日，虚拟会议*，2020年。
- en: McAuley et al. (2015) Julian McAuley, Christopher Targett, Qinfeng Shi, and
    Anton Van Den Hengel. Image-Based Recommendations on Styles and Substitutes. In
    *Proceedings of the 38th international ACM SIGIR conference on research and development
    in information retrieval*, pp.  43–52, 2015.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McAuley et al. (2015) Julian McAuley, Christopher Targett, Qinfeng Shi, 和 Anton
    Van Den Hengel. 基于图像的风格和替代品推荐。发表于 *第38届国际ACM SIGIR信息检索研究与开发会议论文集*，第43–52页，2015年。
- en: McCallum et al. (2000) Andrew Kachites McCallum, Kamal Nigam, Jason Rennie,
    and Kristie Seymore. Automating the Construction of Internet Portals with Machine
    Learning. *Information Retrieval*, 3(2):127–163, 2000.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCallum et al. (2000) Andrew Kachites McCallum, Kamal Nigam, Jason Rennie,
    和 Kristie Seymore. 利用机器学习自动构建互联网门户。*信息检索*，3(2):127–163，2000年。
- en: Meinert & Lavin (2021) Nis Meinert and Alexander Lavin. Multivariate Deep Evidential
    Regression. *arXiv preprint arXiv:2104.06135*, 2021.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meinert & Lavin (2021) Nis Meinert 和 Alexander Lavin. 多变量深度证据回归。*arXiv预印本 arXiv:2104.06135*，2021年。
- en: Menze & Geiger (2015) Moritz Menze and Andreas Geiger. Object Scene Flow for
    Autonomous Vehicles. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, pp.  3061–3070, 2015.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menze & Geiger (2015) Moritz Menze 和 Andreas Geiger. 自动驾驶车辆的对象场景流。发表于 *IEEE计算机视觉与模式识别会议论文集*，第3061–3070页，2015年。
- en: Miller (2011) Jeffrey W. Miller. (ML 7.7.A2) Expectation of a Dirichlet Random
    Variable, 2011. URL [https://www.youtube.com/watch?v=emnfq4txDuI](https://www.youtube.com/watch?v=emnfq4txDuI).
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller (2011) Jeffrey W. Miller. (ML 7.7.A2) 迪利克雷随机变量的期望，2011年。网址 [https://www.youtube.com/watch?v=emnfq4txDuI](https://www.youtube.com/watch?v=emnfq4txDuI)。
- en: Minderer et al. (2021) Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances
    Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the
    Calibration of Modern Neural Networks. *Advances in Neural Information Processing
    Systems*, 34:15682–15694, 2021.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minderer et al. (2021) Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances
    Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, 和 Mario Lucic. 重新审视现代神经网络的校准。*神经信息处理系统进展*，34:15682–15694，2021年。
- en: Moreno-Torres et al. (2012) Jose G Moreno-Torres, Troy Raeder, RocíO Alaiz-RodríGuez,
    Nitesh V Chawla, and Francisco Herrera. A Unifying View on Dataset Shift in Classification.
    *Pattern recognition*, 45(1):521–530, 2012.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moreno-Torres et al. (2012) Jose G Moreno-Torres, Troy Raeder, RocíO Alaiz-RodríGuez,
    Nitesh V Chawla, 和 Francisco Herrera. 分类中的数据集变化统一视角。*模式识别*，45(1):521–530，2012年。
- en: Mukhoti et al. (2021) Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort,
    Philip HS Torr, and Yarin Gal. Deterministic Neural Networks With Appropriate
    Inductive Biases Capture Epistemic and Aleatoric Uncertainty. *arXiv preprint
    arXiv:2102.11582*, 2021.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukhoti et al. (2021) Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort,
    Philip HS Torr 和 Yarin Gal. 具有适当归纳偏置的确定性神经网络捕捉到认知和随机不确定性。*arXiv 预印本 arXiv:2102.11582*，2021年。
- en: Murphy (2007) Kevin P Murphy. Conjugate Bayesian Analysis of the Gaussian Distribution.
    2007.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murphy (2007) Kevin P Murphy. 高斯分布的共轭贝叶斯分析。2007年。
- en: Nagarajan et al. (2021) Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur.
    Understanding the Failure Modes of Out-Of-Distribution Generalization. In *9th
    International Conference on Learning Representations, ICLR 2021, Virtual Event,
    Austria, May 3-7, 2021*, 2021.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagarajan et al. (2021) Vaishnavh Nagarajan, Anders Andreassen 和 Behnam Neyshabur.
    理解分布外泛化的失败模式。见 *第9届国际学习表征大会，ICLR 2021，虚拟会议，奥地利，2021年5月3-7日*，2021年。
- en: Nalisnick et al. (2019) Eric T. Nalisnick, Akihiro Matsukawa, Yee Whye Teh,
    Dilan Görür, and Balaji Lakshminarayanan. Do Deep Generative Models Know What
    They Don’t Know? In *7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*, 2019.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nalisnick et al. (2019) Eric T. Nalisnick, Akihiro Matsukawa, Yee Whye Teh,
    Dilan Görür 和 Balaji Lakshminarayanan. 深度生成模型是否知道它们不知道的东西？见 *第7届国际学习表征大会，ICLR
    2019，新奥尔良，美国，2019年5月6-9日*，2019年。
- en: Namata et al. (2012) Galileo Namata, Ben London, Lise Getoor, Bert Huang, and
    UMD EDU. Query-Driven Active Surveying for Collective Classification. In *10th
    International Workshop on Mining and Learning with Graphs*, volume 8, pp.  1,
    2012.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Namata et al. (2012) Galileo Namata, Ben London, Lise Getoor, Bert Huang 和 UMD
    EDU. 基于查询的主动调查以进行集体分类。见 *第10届国际图学习与挖掘研讨会*，第8卷，第1页，2012年。
- en: Nandy et al. (2020) Jay Nandy, Wynne Hsu, and Mong Li Lee. Towards Maximizing
    the Representation Gap between In-Domain & Out-of-Distribution Examples. *Advances
    in Neural Information Processing Systems*, 33, 2020.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nandy et al. (2020) Jay Nandy, Wynne Hsu 和 Mong Li Lee. 最大化领域内与分布外示例之间的表征差距。*神经信息处理系统进展*，33卷，2020年。
- en: Neal (2012) Radford M Neal. *Bayesian Learning for Neural Networks*, volume
    118. Springer Science & Business Media, 2012.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neal (2012) Radford M Neal. *《神经网络的贝叶斯学习》*，第118卷。Springer Science & Business
    Media，2012年。
- en: Nixon et al. (2019) Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen
    Jerfel, and Dustin Tran. Measuring Calibration in Deep Learning. In *CVPR Workshops*,
    volume 2, 2019.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nixon et al. (2019) Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen
    Jerfel 和 Dustin Tran. 测量深度学习中的校准。见 *CVPR 工作坊*，第2卷，2019年。
- en: Oh & Shin (2022) Dongpin Oh and Bonggun Shin. Improving Evidential Deep Learning
    via Multi-Task Learning. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 36, pp.  7895–7903, 2022.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh & Shin (2022) Dongpin Oh 和 Bonggun Shin. 通过多任务学习改进证据深度学习。见 *AAAI 人工智能大会论文集*，第36卷，第7895–7903页，2022年。
- en: Ovadia et al. (2019) Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David
    Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper
    Snoek. Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty
    under Dataset Shift. In *Advances in Neural Information Processing Systems*, pp. 13991–14002,
    2019.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ovadia et al. (2019) Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David
    Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan 和 Jasper Snoek.
    你能信任你模型的不确定性吗？评估数据集转移下的预测不确定性。见 *神经信息处理系统进展*，第13991–14002页，2019年。
- en: Papadopoulos et al. (2002) Harris Papadopoulos, Kostas Proedrou, Volodya Vovk,
    and Alex Gammerman. Inductive Confidence Machines for Regression. In *European
    Conference on Machine Learning*, pp.  345–356. Springer, 2002.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papadopoulos et al. (2002) Harris Papadopoulos, Kostas Proedrou, Volodya Vovk
    和 Alex Gammerman. 用于回归的归纳置信机器。见 *欧洲机器学习会议*，第345–356页。Springer，2002年。
- en: Paschke et al. (2013) Fabian Paschke, Christian Bayer, Martyna Bator, Uwe Mönks,
    Alexander Dicks, Olaf Enge-Rosenblatt, and Volker Lohweg. Sensorlose Zustandsüberwachung
    an Synchronmotoren. In *Proc*, pp.  211, 2013.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paschke et al. (2013) Fabian Paschke, Christian Bayer, Martyna Bator, Uwe Mönks,
    Alexander Dicks, Olaf Enge-Rosenblatt 和 Volker Lohweg. 无传感器同步电机状态监测。见 *Proc*，第211页，2013年。
- en: 'Pearce et al. (2020) Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty
    in Neural Networks: Approximately Bayesian Ensembling. In *International conference
    on artificial intelligence and statistics*, pp.  234–244\. PMLR, 2020.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearce et al. (2020) Tim Pearce, Felix Leibfried 和 Alexandra Brintrup. 神经网络中的不确定性：近似贝叶斯集成。见
    *国际人工智能与统计会议*，第234–244页。PMLR，2020年。
- en: Pearce et al. (2021) Tim Pearce, Alexandra Brintrup, and Jun Zhu. Understanding
    Softmax Confidence and Uncertainty. *arXiv preprint arXiv:2106.04972*, 2021.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearce 等人 (2021) Tim Pearce, Alexandra Brintrup 和 Jun Zhu. 理解Softmax置信度和不确定性。*arXiv预印本
    arXiv:2106.04972*，2021年。
- en: 'Pedregosa et al. (2011) F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
    B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas,
    A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
    Machine Learning in Python. *Journal of Machine Learning Research*, 12:2825–2830,
    2011.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pedregosa 等人 (2011) F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B.
    Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas,
    A. Passos, D. Cournapeau, M. Brucher, M. Perrot 和 E. Duchesnay. Scikit-learn:
    Python中的机器学习。*机器学习研究期刊*，12:2825–2830，2011年。'
- en: Petek et al. (2022) Kürsat Petek, Kshitij Sirohi, Daniel Büscher, and Wolfram
    Burgard. Robust Monocular Localization in Sparse HD Maps Leveraging Multi-Task
    Uncertainty Estimation. In *2022 International Conference on Robotics and Automation,
    ICRA 2022, Philadelphia, PA, USA, May 23-27, 2022*, pp.  4163–4169. IEEE, 2022.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petek 等人 (2022) Kürsat Petek, Kshitij Sirohi, Daniel Büscher 和 Wolfram Burgard.
    利用多任务不确定性估计进行稀疏高清地图中的稳健单目定位。发表于 *2022年国际机器人与自动化会议，ICRA 2022，费城，宾夕法尼亚，美国，2022年5月23-27日*，第4163–4169页。IEEE，2022年。
- en: Radev et al. (2021) Stefan T Radev, Marco D’Alessandro, Ulf K Mertens, Andreas
    Voss, Ullrich Köthe, and Paul-Christian Bürkner. Amortized Bayesian Model Comparison
    with Evidential Deep Learning. *IEEE Transactions on Neural Networks and Learning
    Systems*, 2021.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radev 等人 (2021) Stefan T Radev, Marco D’Alessandro, Ulf K Mertens, Andreas Voss,
    Ullrich Köthe 和 Paul-Christian Bürkner. 使用证据深度学习的均摊贝叶斯模型比较。*IEEE神经网络与学习系统汇刊*，2021年。
- en: Rezende & Mohamed (2015) Danilo Jimenez Rezende and Shakir Mohamed. Variational
    Inference with Normalizing Flows. In *Proceedings of the 32nd International Conference
    on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015*, volume 37 of *JMLR
    Workshop and Conference Proceedings*, pp.  1530–1538. JMLR.org, 2015.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezende & Mohamed (2015) Danilo Jimenez Rezende 和 Shakir Mohamed. 使用标准化流的变分推断。发表于
    *第32届国际机器学习会议，ICML 2015，法国里尔，2015年7月6-11日*，*JMLR研讨会与会议记录* 第37卷，第1530–1538页。JMLR.org，2015年。
- en: Sensoy et al. (2018) Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential
    Deep Learning to Quantify Classification Uncertainty. In *Advances in Neural Information
    Processing Systems*, pp. 3179–3189, 2018.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sensoy 等人 (2018) Murat Sensoy, Lance Kaplan 和 Melih Kandemir. 使用证据深度学习量化分类不确定性。发表于
    *神经信息处理系统进展*，第3179–3189页，2018年。
- en: Sensoy et al. (2020) Murat Sensoy, Lance M. Kaplan, Federico Cerutti, and Maryam
    Saleki. Uncertainty-Aware Deep Classifiers Using Generative Models. In *The Thirty-Fourth
    AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative
    Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
    NY, USA, February 7-12, 2020*, pp.  5620–5627\. AAAI Press, 2020.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sensoy 等人 (2020) Murat Sensoy, Lance M. Kaplan, Federico Cerutti 和 Maryam Saleki.
    使用生成模型的基于不确定性的深度分类器。发表于 *第34届AAAI人工智能会议，AAAI 2020，第32届人工智能创新应用会议，IAAI 2020，第10届AAAI教育进展研讨会，EAAI
    2020，纽约，纽约，美国，2020年2月7-12日*，第5620–5627页。AAAI出版社，2020年。
- en: Sharma et al. (2022) Mrinank Sharma, Sebastian Farquhar, Eric Nalisnick, and
    Tom Rainforth. Do Bayesian Neural Networks Need To Be Fully Stochastic? *arXiv
    preprint arXiv:2211.06291*, 2022.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人 (2022) Mrinank Sharma, Sebastian Farquhar, Eric Nalisnick 和 Tom Rainforth.
    贝叶斯神经网络是否需要完全随机？*arXiv预印本 arXiv:2211.06291*，2022年。
- en: Shchur et al. (2018) Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski,
    and Stephan Günnemann. Pitfalls of Graph Neural Network Evaluation. *arXiv preprint
    arXiv:1811.05868*, 2018.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shchur 等人 (2018) Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski 和
    Stephan Günnemann. 图神经网络评估中的陷阱。*arXiv预印本 arXiv:1811.05868*，2018年。
- en: Shen et al. (2022) Maohao Shen, Yuheng Bu, Prasanna Sattigeri, Soumya Ghosh,
    Subhro Das, and Gregory Wornell. Post-hoc Uncertainty Learning using a Dirichlet
    Meta-Model. *arXiv preprint arXiv:2212.07359*, 2022.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人 (2022) Maohao Shen, Yuheng Bu, Prasanna Sattigeri, Soumya Ghosh, Subhro
    Das 和 Gregory Wornell. 使用Dirichlet元模型的事后不确定性学习。*arXiv预印本 arXiv:2212.07359*，2022年。
- en: Shen et al. (2020) Yilin Shen, Wenhu Chen, and Hongxia Jin. Modeling Token-level
    Uncertainty to Learn Unknown Concepts in SLU via Calibrated Dirichlet Prior RNN.
    *CoRR*, abs/2010.08101, 2020.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人 (2020) Yilin Shen, Wenhu Chen 和 Hongxia Jin. 通过标定的Dirichlet先验RNN建模令牌级不确定性以学习SLU中的未知概念。*CoRR*，abs/2010.08101，2020年。
- en: Silberman et al. (2012) Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
    Fergus. Indoor Segmentation and Support Inference from RGBD Images. In *European
    conference on computer vision*, pp.  746–760. Springer, 2012.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silberman et al. (2012) Nathan Silberman, Derek Hoiem, Pushmeet Kohli, 和 Rob
    Fergus. 从RGBD图像中进行室内分割和支持推断。发表于*欧洲计算机视觉会议*，第746–760页。Springer，2012年。
- en: Smith & Gal (2018) Lewis Smith and Yarin Gal. Understanding Measures of Uncertainty
    for Adversarial Example Detection. In *Proceedings of the Thirty-Fourth Conference
    on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA,
    August 6-10, 2018*, pp.  560–569, 2018.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith & Gal (2018) Lewis Smith 和 Yarin Gal. 理解不确定性度量在对抗样本检测中的作用。发表于*第三十四届人工智能不确定性会议（UAI
    2018），美国加利福尼亚州蒙特雷，2018年8月6-10日*，第560–569页，2018年。
- en: Soleimany et al. (2021) Ava P Soleimany, Alexander Amini, Samuel Goldman, Daniela
    Rus, Sangeeta N Bhatia, and Connor W Coley. Evidential Deep Learning for Guided
    Molecular Property Prediction and Discovery. *ACS central science*, 7(8):1356–1367,
    2021.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soleimany et al. (2021) Ava P Soleimany, Alexander Amini, Samuel Goldman, Daniela
    Rus, Sangeeta N Bhatia, 和 Connor W Coley. 证据深度学习用于指导分子属性预测和发现。*ACS Central Science*，7(8):1356–1367，2021年。
- en: 'Stadler et al. (2021) Maximilian Stadler, Bertrand Charpentier, Simon Geisler,
    Daniel Zügner, and Stephan Günnemann. Graph Posterior Network: Bayesian Predictive
    Uncertainty for Node Classification. *Advances in Neural Information Processing
    Systems*, 34, 2021.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stadler et al. (2021) Maximilian Stadler, Bertrand Charpentier, Simon Geisler,
    Daniel Zügner, 和 Stephan Günnemann. 图后验网络：节点分类的贝叶斯预测不确定性。*神经信息处理系统进展*，34，2021年。
- en: 'Tang et al. (2014) Jing Tang, Agnieszka Szwajda, Sushil Shakyawar, Tao Xu,
    Petteri Hintsanen, Krister Wennerberg, and Tero Aittokallio. Making Sense of Large-Scale
    Kinase Inhibitor Bioactivity Data Sets: A Comparative and Integrative Analysis.
    *Journal of Chemical Information and Modeling*, 54(3):735–743, 2014.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al. (2014) Jing Tang, Agnieszka Szwajda, Sushil Shakyawar, Tao Xu, Petteri
    Hintsanen, Krister Wennerberg, 和 Tero Aittokallio. 理解大规模激酶抑制剂生物活性数据集：一种比较和综合分析。*Journal
    of Chemical Information and Modeling*，54(3):735–743，2014年。
- en: Tishby & Zaslavsky (2015) Naftali Tishby and Noga Zaslavsky. Deep Learning and
    the Information Bottleneck Principle. In *2015 ieee information theory workshop
    (itw)*, pp.  1–5. IEEE, 2015.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tishby & Zaslavsky (2015) Naftali Tishby 和 Noga Zaslavsky. 深度学习与信息瓶颈原理。发表于*2015
    IEEE 信息理论研讨会（ITW）*，第1–5页。IEEE，2015年。
- en: Tsanas & Xifara (2012) Athanasios Tsanas and Angeliki Xifara. Accurate Quantitative
    Estimation of Energy Performance of Residential Buildings using Statistical Machine
    Learning Tools. *Energy and buildings*, 49:560–567, 2012.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsanas & Xifara (2012) Athanasios Tsanas 和 Angeliki Xifara. 使用统计机器学习工具对住宅建筑的能源性能进行准确的定量估计。*Energy
    and Buildings*，49:560–567，2012年。
- en: Tsiligkaridis (2019) Theodoros Tsiligkaridis. Information Robust Dirichlet Networks
    for Predictive Uncertainty Estimation. *arXiv preprint arXiv:1910.04819*, 2019.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsiligkaridis (2019) Theodoros Tsiligkaridis. 信息稳健的Dirichlet网络用于预测不确定性估计。*arXiv
    预印本 arXiv:1910.04819*，2019年。
- en: 'Turkoglu et al. (2022) Mehmet Ozgur Turkoglu, Alexander Becker, Hüseyin Anil
    Gündüz, Mina Rezaei, Bernd Bischl, Rodrigo Caye Daudt, Stefano D’Aronco, Jan Dirk
    Wegner, and Konrad Schindler. FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise
    Linear Modulation. *arXiv preprint arXiv:2206.00050*, 2022.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turkoglu et al. (2022) Mehmet Ozgur Turkoglu, Alexander Becker, Hüseyin Anil
    Gündüz, Mina Rezaei, Bernd Bischl, Rodrigo Caye Daudt, Stefano D’Aronco, Jan Dirk
    Wegner, 和 Konrad Schindler. FiLM-Ensemble：通过特征线性调制的概率深度学习。*arXiv 预印本 arXiv:2206.00050*，2022年。
- en: 'Ulmer & Cinà (2021) Dennis Ulmer and Giovanni Cinà. Know Your Limits: Uncertainty
    Estimation with ReLU Classifiers Fails at Reliable OOD Detection. In *Uncertainty
    in Artificial Intelligence*, pp.  1766–1776. PMLR, 2021.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ulmer & Cinà (2021) Dennis Ulmer 和 Giovanni Cinà. 了解你的极限：带有ReLU分类器的不确定性估计在可靠的OOD检测中失败。发表于*人工智能中的不确定性*，第1766–1776页。PMLR，2021年。
- en: 'Ulmer et al. (2020) Dennis Ulmer, Lotta Meijerink, and Giovanni Cinà. Trust
    Issues: Uncertainty Estimation Does not Enable Reliable OOD Detection on Medical
    Tabular Data. In *Machine Learning for Health*, pp.  341–354\. PMLR, 2020.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ulmer et al. (2020) Dennis Ulmer, Lotta Meijerink, 和 Giovanni Cinà. 信任问题：不确定性估计无法在医疗表格数据上实现可靠的OOD检测。发表于*健康机器学习*，第341–354页。PMLR，2020年。
- en: 'Ulmer et al. (2022) Dennis Ulmer, Jes Frellsen, and Christian Hardmeier. "Exploring
    Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method
    & Data Scarcity". In *Findings of the Association for Computational Linguistics:
    EMNLP 2022*, pp.  2707–2735, Abu Dhabi, United Arab Emirates, December 2022\.
    Association for Computational Linguistics.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ulmer et al. (2022) Dennis Ulmer, Jes Frellsen, 和 Christian Hardmeier. “探索NLP中的预测不确定性和校准：方法和数据稀缺性的影响研究”。见于
    *计算语言学协会发现：EMNLP 2022*，pp. 2707–2735，阿布扎比，阿联酋，2022年12月。计算语言学协会。
- en: van Amersfoort et al. (2020a) Joost van Amersfoort, Lewis Smith, Yee Whye Teh,
    and Yarin Gal. Uncertainty Estimation Using a Single Deep Deterministic Neural
    Network. In *Proceedings of the 37th International Conference on Machine Learning,
    ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine
    Learning Research*, pp.  9690–9700\. PMLR, 2020a.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Amersfoort et al. (2020a) Joost van Amersfoort, Lewis Smith, Yee Whye Teh,
    和 Yarin Gal. 使用单一深度确定性神经网络的不确定性估计。见于 *第37届国际机器学习大会，ICML 2020，2020年7月13-18日，虚拟会议*，*机器学习研究文集*第119卷，pp.
    9690–9700. PMLR，2020a。
- en: van Amersfoort et al. (2020b) Joost van Amersfoort, Lewis Smith, Yee Whye Teh,
    and Yarin Gal. Uncertainty Estimation Using a Single Deep Deterministic Neural
    Network. In *Proceedings of the 37th International Conference on Machine Learning,
    ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine
    Learning Research*, pp.  9690–9700\. PMLR, 2020b.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Amersfoort et al. (2020b) Joost van Amersfoort, Lewis Smith, Yee Whye Teh,
    和 Yarin Gal. 使用单一深度确定性神经网络的不确定性估计。见于 *第37届国际机器学习大会，ICML 2020，2020年7月13-18日，虚拟会议*，*机器学习研究文集*第119卷，pp.
    9690–9700. PMLR，2020b。
- en: van Amersfoort et al. (2021) Joost van Amersfoort, Lewis Smith, Andrew Jesson,
    Oscar Key, and Yarin Gal. On Feature Collapse and Deep Kernel Learning for Single
    Forward Pass Uncertainty. *arXiv preprint arXiv:2102.11409*, 2021.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Amersfoort et al. (2021) Joost van Amersfoort, Lewis Smith, Andrew Jesson,
    Oscar Key, 和 Yarin Gal. 关于特征崩溃和深度内核学习用于单次前向传递不确定性。*arXiv预印本 arXiv:2102.11409*，2021。
- en: van Erven & Harremoës (2014) Tim van Erven and Peter Harremoës. Rényi Divergence
    and Kullback-Leibler Divergence. *IEEE Trans. Inf. Theory*, 60(7):3797–3820, 2014.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Erven & Harremoës (2014) Tim van Erven 和 Peter Harremoës. Rényi散度和Kullback-Leibler散度。*IEEE信息理论汇刊*，60(7):3797–3820，2014。
- en: Van Landeghem et al. (2022) Jordy Van Landeghem, Matthew Blaschko, Bertrand
    Anckaert, and Marie-Francine Moens. Benchmarking Scalable Predictive Uncertainty
    in Text Classification. *Ieee Access*, 10:43703–43737, 2022.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Landeghem et al. (2022) Jordy Van Landeghem, Matthew Blaschko, Bertrand
    Anckaert, 和 Marie-Francine Moens. 可扩展预测不确定性在文本分类中的基准测试。*IEEE Access*，10:43703–43737，2022。
- en: Vovk et al. (2005) Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. *Algorithmic
    lLarning in a Random World*. Springer Science & Business Media, 2005.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vovk et al. (2005) Vladimir Vovk, Alexander Gammerman, 和 Glenn Shafer. *算法学习在随机世界*。Springer
    Science & Business Media，2005。
- en: Wang et al. (2021a) Chen Wang, Xiang Wang, Jiawei Zhang, Liang Zhang, Xiao Bai,
    Xin Ning, Jun Zhou, and Edwin Hancock. Uncertainty Estimation for Stereo Matching
    Based on Evidential Deep Learning. *Pattern Recognition*, pp.  108498, 2021a.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021a) Chen Wang, Xiang Wang, Jiawei Zhang, Liang Zhang, Xiao Bai,
    Xin Ning, Jun Zhou, 和 Edwin Hancock. 基于证据深度学习的立体匹配不确定性估计。*模式识别*，pp. 108498，2021a。
- en: 'Wang et al. (2021b) Deng-Bao Wang, Lei Feng, and Min-Ling Zhang. Rethinking
    Calibration of Deep Neural Networks: Do not be Afraid of Overconfidence. *Advances
    in Neural Information Processing Systems*, 34:11809–11820, 2021b.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021b) Deng-Bao Wang, Lei Feng, 和 Min-Ling Zhang. 重新思考深度神经网络的校准：不要害怕过度自信。*神经信息处理系统进展*，34:11809–11820，2021b。
- en: 'Wei et al. (2018) Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang.
    Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its
    Dual Effect. In *6th International Conference on Learning Representations, ICLR
    2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*,
    2018.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2018) Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, 和 Liqiang Wang.
    改进Wasserstein GAN的改进训练：一致性项及其对偶效应。见于 *第6届国际学习表征会议，ICLR 2018，2018年4月30日 - 5月3日，温哥华，加拿大*，会议论文集，2018。
- en: 'Wen et al. (2020) Yeming Wen, Dustin Tran, and Jimmy Ba. BatchEnsemble: an
    Alternative Approach to Efficient Ensemble and Lifelong Learning. In *8th International
    Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
    26-30, 2020*, 2020.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen et al. (2020) Yeming Wen, Dustin Tran, 和 Jimmy Ba. BatchEnsemble：一种高效集成和终身学习的替代方法。见于
    *第8届国际学习表征会议，ICLR 2020，2020年4月26-30日，亚的斯亚贝巴，埃塞俄比亚*，2020。
- en: Wenzel et al. (2020) Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Światkowski,
    Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian
    Nowozin. How Good is the Bayes Posterior in Deep Neural Networks Really? *arXiv
    preprint arXiv:2002.02405*, 2020.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wenzel et al. (2020) Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Światkowski,
    Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton 和 Sebastian
    Nowozin. 贝叶斯后验在深度神经网络中到底有多好？*arXiv 预印本 arXiv:2002.02405*，2020 年。
- en: Wikimedia Commons (2022a) Wikimedia Commons. Iris setosa, 2022a. URL [{https://en.wikipedia.org/wiki/Iris_setosa}](%7Bhttps://en.wikipedia.org/wiki/Iris_setosa%7D).
    File:Irissetosa1.jpg.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wikimedia Commons (2022a) Wikimedia Commons. 矮鸢尾，2022a。网址 [{https://en.wikipedia.org/wiki/Iris_setosa}](%7Bhttps://en.wikipedia.org/wiki/Iris_setosa%7D)。文件:
    Irissetosa1.jpg。'
- en: Wikimedia Commons (2022b) Wikimedia Commons. Iris versicolor, 2022b. URL [{https://en.wikipedia.org/wiki/Iris_versicolor}](%7Bhttps://en.wikipedia.org/wiki/Iris_versicolor%7D).
    File:Blue_Flag,_Ottawa.jpg.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wikimedia Commons (2022b) Wikimedia Commons. 鸢尾花，2022b。网址 [{https://en.wikipedia.org/wiki/Iris_versicolor}](%7Bhttps://en.wikipedia.org/wiki/Iris_versicolor%7D)。文件:
    Blue_Flag,_Ottawa.jpg。'
- en: Wikimedia Commons (2022c) Wikimedia Commons. Iris virginica, 2022c. URL [{https://en.wikipedia.org/wiki/Iris_virginica#/media/File:Iris_virginica_2.jpg}](%7Bhttps://en.wikipedia.org/wiki/Iris_virginica#/media/File:Iris_virginica_2.jpg%7D).
    File:Iris_virginica_2.jpg.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wikimedia Commons (2022c) Wikimedia Commons. 弗吉尼亚鸢尾，2022c。网址 [{https://en.wikipedia.org/wiki/Iris_virginica#/media/File:Iris_virginica_2.jpg}](%7Bhttps://en.wikipedia.org/wiki/Iris_virginica#/media/File:Iris_virginica_2.jpg%7D)。文件:
    Iris_virginica_2.jpg。'
- en: 'Wilson & Izmailov (2020) Andrew Gordon Wilson and Pavel Izmailov. Bayesian
    Deep Learning and a Probabilistic Perspective of Generalization. In *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wilson & Izmailov (2020) Andrew Gordon Wilson 和 Pavel Izmailov. 贝叶斯深度学习及泛化的概率视角。发表于
    *《神经信息处理系统进展 33: 2020 年神经信息处理系统年会，NeurIPS 2020，2020 年 12 月 6-12 日，虚拟》*，2020 年。'
- en: Winn (2004) John Michael Winn. Variational Message Passing and Its Applications.
    2004.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Winn (2004) John Michael Winn. 变分消息传递及其应用。2004 年。
- en: Woo (2022) Jae Oh Woo. Analytic Mutual Information in Bayesian Neural Networks.
    In *IEEE International Symposium on Information Theory, ISIT 2022, Espoo, Finland,
    June 26 - July 1, 2022*, pp.  300–305\. IEEE, 2022.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Woo (2022) Jae Oh Woo. 贝叶斯神经网络中的解析互信息。发表于 *《IEEE 国际信息论研讨会，ISIT 2022，芬兰埃斯波，2022
    年 6 月 26 日 - 7 月 1 日》*，第 300–305 页。IEEE，2022 年。
- en: 'Xiao et al. (2017) Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST:
    A Novel Image Dataset for Benchmarking Machine Learning Algorithms. *arXiv preprint
    arXiv:1708.07747*, 2017.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2017) Han Xiao, Kashif Rasul 和 Roland Vollgraf. Fashion-MNIST:
    用于基准测试机器学习算法的新型图像数据集。*arXiv 预印本 arXiv:1708.07747*，2017 年。'
- en: 'Xiao et al. (2010) Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
    and Antonio Torralba. Sun Database: Large-scale Scene Recognition from Abbey to
    Zoo. In *2010 IEEE computer society conference on computer vision and pattern
    recognition*, pp.  3485–3492\. IEEE, 2010.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2010) Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva
    和 Antonio Torralba. 太阳数据库: 从修道院到动物园的大规模场景识别。发表于 *《2010 年 IEEE 计算机学会计算机视觉与模式识别会议》*，第
    3485–3492 页。IEEE，2010 年。'
- en: Yager & Liu (2008) Ronald R Yager and Liping Liu. *Classic Works of the Dempster-Shafer
    Theory of Belief Functions*, volume 219. Springer, 2008.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yager & Liu (2008) Ronald R Yager 和 Liping Liu. *《Dempster-Shafer 信念函数理论的经典著作》*，第
    219 卷。Springer，2008 年。
- en: Yeh (1998) I-C Yeh. Modeling of Strength of High-Performance Concrete using
    Artificial Neural Networks. *Cement and Concrete research*, 28(12):1797–1808,
    1998.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeh (1998) I-C Yeh. 使用人工神经网络建模高性能混凝土的强度。*水泥与混凝土研究*，28(12):1797–1808，1998 年。
- en: 'Yu et al. (2015) Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser,
    and Jianxiong Xiao. LSUN: Construction of a Large-Scale Image Dataset using Deep
    Learning with Humans in the Loop. *arXiv preprint arXiv:1506.03365*, 2015.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu et al. (2015) Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser
    和 Jianxiong Xiao. LSUN: 使用深度学习和人类参与构建的大规模图像数据集。*arXiv 预印本 arXiv:1506.03365*，2015
    年。'
- en: Yu et al. (2019) Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang,
    and Xiang Chen. Interpreting and Evaluating Neural Network Robustness. In *Proceedings
    of the Twenty-Eighth International Joint Conference on Artificial Intelligence,
    IJCAI 2019, Macao, China, August 10-16, 2019*, pp.  4199–4205\. ijcai.org, 2019.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2019) Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang
    和 Xiang Chen. 解释和评估神经网络的鲁棒性。发表于 *《第二十八届国际人工智能联合会议论文集，IJCAI 2019，澳门，中国，2019 年 8
    月 10-16 日》*，第 4199–4205 页。ijcai.org，2019 年。
- en: Zerva et al. (2022) Chrysoula Zerva, Taisiya Glushkova, Ricardo Rei, and André
    F. T. Martins. "Disentangling Uncertainty in Machine Translation Evaluation".
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pp.  8622–8641, Abu Dhabi, United Arab Emirates, December 2022\.
    Association for Computational Linguistics.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zerva等（2022）Chrysoula Zerva, Taisiya Glushkova, Ricardo Rei 和 André F. T. Martins。“在机器翻译评估中解开不确定性”。发表于*2022年自然语言处理实证方法会议论文集*，第8622–8641页，阿布扎比，阿拉伯联合酋长国，2022年12月。计算语言学协会。
- en: Zhao et al. (2019) Xujiang Zhao, Yuzhe Ou, Lance Kaplan, Feng Chen, and Jin-Hee
    Cho. Quantifying Classification Uncertainty Using Regularized Evidential Neural
    Networks. *arXiv preprint arXiv:1910.06864*, 2019.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等（2019）Xujiang Zhao, Yuzhe Ou, Lance Kaplan, Feng Chen 和 Jin-Hee Cho。使用正则化证据神经网络量化分类不确定性。*arXiv预印本arXiv:1910.06864*，2019年。
- en: 'Zhao et al. (2020) Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. Uncertainty
    Aware Semi-Supervised Learning on Graph Data. In *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等（2020）Xujiang Zhao, Feng Chen, Shu Hu 和 Jin-Hee Cho。在图数据上的不确定性感知半监督学习。发表于*神经信息处理系统进展33：2020年神经信息处理系统年度会议，NeurIPS
    2020，2020年12月6-12日，虚拟*，2020年。
- en: Appendix A Code Appendix
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 代码附录
- en: A.1 Iris Example Training Details
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 虹膜示例训练细节
- en: 'The code used to produce [Figure 2](#S2.F2 "In 2.4 An Illustrating Example:
    The Iris Dataset ‣ 2 Background ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation") is available online.^(16)^(16)16Code
    will be made available upon acceptance. All models use three layers with $100$
    hidden units and ReLU activations each. We furthermore optimized all of the models
    with a learning rate of $0.001$ using the Adam optimizer (Kingma & Ba, [2015](#bib.bib87))
    with its default parameter settings. We also regularize the ensemble and MC Dropout
    model with a dropout probability of $0.1$ each.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成[图2](#S2.F2 "在2.4中，一个说明示例：虹膜数据集 ‣ 2 背景 ‣ 先验和后验网络：不确定性估计的证据深度学习方法调查")的代码可在线获取。^(16)^(16)16代码将在接受后提供。所有模型使用三个层，每层有$100$个隐藏单元和ReLU激活函数。我们还使用Adam优化器（Kingma
    & Ba，[2015](#bib.bib87)）及其默认参数设置，以$0.001$的学习率优化所有模型。我们还对集成和MC Dropout模型进行正则化，每个模型的丢弃概率为$0.1$。
- en: Prior Network specifics
  id: totrans-411
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 先验网络细节
- en: We choose the expected $l_{2}$ loss by Sensoy et al. ([2018](#bib.bib146)) and
    regularize the network using the KL divergence w.r.t. to a uniform Dirichlet as
    in Sensoy et al. ([2018](#bib.bib146)). In the regularization term, we do not
    use the original concentration parameters $\bm{\alpha}$, but a version in which
    the concentration of the parameter $\alpha_{k}$ corresponding to the correct class
    is removed using a one-hot label encoding $\mathbf{y}$ by $\tilde{\bm{\alpha}}=(1-\bm{\alpha})\odot\bm{\alpha}+\mathbf{y}\odot\bm{\alpha}$,
    where $\odot$ denotes point-wise multiplication. The regularization term is added
    to the loss using a weighting factor of $0.05$.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了Sensoy等人（[2018](#bib.bib146)）提出的期望$l_{2}$损失，并使用KL散度对网络进行正则化，参考了Sensoy等人（[2018](#bib.bib146)）的方法。在正则化项中，我们不使用原始的浓度参数$\bm{\alpha}$，而是使用一种版本，其中将正确类别对应的参数$\alpha_{k}$的浓度通过一热标签编码$\mathbf{y}$移除，计算方式为$\tilde{\bm{\alpha}}=(1-\bm{\alpha})\odot\bm{\alpha}+\mathbf{y}\odot\bm{\alpha}$，其中$\odot$表示逐点乘法。正则化项使用$0.05$的权重因子添加到损失中。
- en: A.2 Code Availability
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 代码可用性
- en: 'Table 4: Overview over code repositories of surveyed works.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '表4: 调查工作代码库概述。'
- en: '| Paper | Code Repository |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 代码库 |'
- en: '| Prior network (Malinin & Gales, [2018](#bib.bib115)) | [https://github.com/KaosEngineer/PriorNetworks-OLD](https://github.com/KaosEngineer/PriorNetworks-OLD)
    |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 先验网络（Malinin & Gales, [2018](#bib.bib115)） | [https://github.com/KaosEngineer/PriorNetworks-OLD](https://github.com/KaosEngineer/PriorNetworks-OLD)
    |'
- en: '| Prior networks (Malinin & Gales, [2019](#bib.bib116)) | [https://github.com/KaosEngineer/PriorNetworks](https://github.com/KaosEngineer/PriorNetworks)
    |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 先验网络（Malinin & Gales, [2019](#bib.bib116)） | [https://github.com/KaosEngineer/PriorNetworks](https://github.com/KaosEngineer/PriorNetworks)
    |'
- en: '| Dirichlet via Function Decomposition (Biloš et al., [2019](#bib.bib11)) |
    [https://github.com/sharpenb/Uncertainty-Event-Prediction](https://github.com/sharpenb/Uncertainty-Event-Prediction)
    |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| Dirichlet通过函数分解（Biloš等， [2019](#bib.bib11)） | [https://github.com/sharpenb/Uncertainty-Event-Prediction](https://github.com/sharpenb/Uncertainty-Event-Prediction)
    |'
- en: '| Prior network with PAC Regularization (Haussmann et al., [2019](#bib.bib58))
    | [https://github.com/manuelhaussmann/bedl](https://github.com/manuelhaussmann/bedl)
    |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 带PAC正则化的先验网络（Haussmann 等人，[2019](#bib.bib58)） | [https://github.com/manuelhaussmann/bedl](https://github.com/manuelhaussmann/bedl)
    |'
- en: '| Prior networks with representation gap (Nandy et al., [2020](#bib.bib133))
    | [https://github.com/jayjaynandy/maximize-representation-gap](https://github.com/jayjaynandy/maximize-representation-gap)
    |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 带表示差距的先验网络（Nandy 等人，[2020](#bib.bib133)） | [https://github.com/jayjaynandy/maximize-representation-gap](https://github.com/jayjaynandy/maximize-representation-gap)
    |'
- en: '| Graph-based Kernel Dirichlet distribution estimation (GKDE) (Zhao et al.,
    [2020](#bib.bib189)) | [https://github.com/zxj32/uncertainty-GNN](https://github.com/zxj32/uncertainty-GNN)
    |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 基于图的核狄利克雷分布估计（GKDE）（Zhao 等人，[2020](#bib.bib189)） | [https://github.com/zxj32/uncertainty-GNN](https://github.com/zxj32/uncertainty-GNN)
    |'
- en: '| Evidential Deep Learning (Sensoy et al., [2018](#bib.bib146)) | [https://muratsensoy.github.io/uncertainty.html](https://muratsensoy.github.io/uncertainty.html)
    |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 证据深度学习（Sensoy 等人，[2018](#bib.bib146)） | [https://muratsensoy.github.io/uncertainty.html](https://muratsensoy.github.io/uncertainty.html)
    |'
- en: '| WGAN–ENN (Hu et al., [2021](#bib.bib71)) | [https://github.com/snowood1/wenn](https://github.com/snowood1/wenn)
    |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| WGAN–ENN（Hu 等人，[2021](#bib.bib71)） | [https://github.com/snowood1/wenn](https://github.com/snowood1/wenn)
    |'
- en: '| Belief Matching (Joo et al., [2020](#bib.bib82)) | [https://github.com/tjoo512/belief-matching-framework](https://github.com/tjoo512/belief-matching-framework)
    |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 信念匹配（Joo 等人，[2020](#bib.bib82)） | [https://github.com/tjoo512/belief-matching-framework](https://github.com/tjoo512/belief-matching-framework)
    |'
- en: '| Posterior Networks (Charpentier et al., [2020](#bib.bib16)) | [https://github.com/sharpenb/Posterior-Network](https://github.com/sharpenb/Posterior-Network)
    |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 后验网络（Charpentier 等人，[2020](#bib.bib16)） | [https://github.com/sharpenb/Posterior-Network](https://github.com/sharpenb/Posterior-Network)
    |'
- en: '| Graph Posterior Networks (Stadler et al., [2021](#bib.bib155)) | [https://github.com/stadlmax/Graph-Posterior-Network](https://github.com/stadlmax/Graph-Posterior-Network)
    |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 图后验网络（Stadler 等人，[2021](#bib.bib155)） | [https://github.com/stadlmax/Graph-Posterior-Network](https://github.com/stadlmax/Graph-Posterior-Network)
    |'
- en: '| Generative Evidential Neural Networks (Sensoy et al., [2020](#bib.bib147))
    | [https://muratsensoy.github.io/gen.html](https://muratsensoy.github.io/gen.html)
    |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 生成证据神经网络（Sensoy 等人，[2020](#bib.bib147)） | [https://muratsensoy.github.io/gen.html](https://muratsensoy.github.io/gen.html)
    |'
- en: '| Deep Evidential Regression with Multi-task Learning (Oh & Shin, [2022](#bib.bib136))
    | [https://github.com/deargen/MT-ENet](https://github.com/deargen/MT-ENet) |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 带多任务学习的深度证据回归（Oh & Shin，[2022](#bib.bib136)） | [https://github.com/deargen/MT-ENet](https://github.com/deargen/MT-ENet)
    |'
- en: '| Multivariate Deep Evidential Regression (Meinert & Lavin, [2021](#bib.bib123))
    | [https://github.com/avitase/mder/](https://github.com/avitase/mder/) |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 多变量深度证据回归（Meinert & Lavin，[2021](#bib.bib123)） | [https://github.com/avitase/mder/](https://github.com/avitase/mder/)
    |'
- en: '| Regression Prior Network (Malinin et al., [2020a](#bib.bib117)) | [https://github.com/JanRocketMan/regression-prior-networks](https://github.com/JanRocketMan/regression-prior-networks)
    |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 回归先验网络（Malinin 等人，[2020a](#bib.bib117)） | [https://github.com/JanRocketMan/regression-prior-networks](https://github.com/JanRocketMan/regression-prior-networks)
    |'
- en: '| Natural Posterior Network (Charpentier et al., [2022](#bib.bib17)) | [https://github.com/borchero/natural-posterior-network](https://github.com/borchero/natural-posterior-network)
    |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 自然后验网络（Charpentier 等人，[2022](#bib.bib17)） | [https://github.com/borchero/natural-posterior-network](https://github.com/borchero/natural-posterior-network)
    |'
- en: 'We list the available code repositories for surveyed works in [Table 4](#A1.T4
    "In A.2 Code Availability ‣ Appendix A Code Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"). Works
    for which no official implementation could be found are not listed.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表 4](#A1.T4 "在 A.2 代码可用性 ‣ 附录 A 代码附录 ‣ 先验和后验网络：深度学习方法用于不确定性估计的调查")中列出了调查作品的可用代码库。找不到官方实现的作品未列出。
- en: Appendix B Datasets & Evaluation Techniques Appendix
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 数据集与评估技术附录
- en: 'Table 5: Overview over uncertainty evaluation techniques and datasets. ^((∗))
    indicates that a dataset was used as an OOD dataset for evaluation purposes, while
    ^((⋄)) signifies that it was used as an in-distribution or out-of-distribution
    dataset. ^((†)) means that a dataset was modified to create ID and OOD splits
    (for instance by removing some classes for evaluation or corrupting samples with
    noise).'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不确定性评估技术和数据集概述。^((∗)) 表示数据集被用作OOD数据集进行评估，而^((⋄)) 表示数据集被用作分布内或分布外数据集。^((†))
    表示数据集被修改以创建ID和OOD划分（例如，通过去除一些类别用于评估或用噪声破坏样本）。
- en: '|  |  | Data Modality |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 数据模态 |'
- en: '| Method | Uncertainty Evaluation | Images | Tabular | Other |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 不确定性评估 | 图像 | 表格 | 其他 |'
- en: '| Prior network (Malinin & Gales, [2018](#bib.bib115)) | OOD Detection, Misclassification
    Detection | MNIST, CIFAR-10, Omniglot^((∗)), SVHN^((∗)), LSUN^((∗)), TIM^((∗))
    | ✗ | Clusters (Synthetic) |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 先验网络（Malinin & Gales, [2018](#bib.bib115)) | OOD 检测，误分类检测 | MNIST，CIFAR-10，Omniglot^((∗))，SVHN^((∗))，LSUN^((∗))，TIM^((∗))
    | ✗ | 聚类（合成） |'
- en: '| Prior networks (Malinin & Gales, [2019](#bib.bib116)) | OOD Detection, Adversarial
    Attack Detection | MNIST, CIFAR-10/100, SVHN^((∗)), LSUN^((∗)), TIM^((∗)) | ✗
    | Clusters (Synthetic) |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 先验网络（Malinin & Gales, [2019](#bib.bib116)) | OOD 检测，对抗攻击检测 | MNIST，CIFAR-10/100，SVHN^((∗))，LSUN^((∗))，TIM^((∗))
    | ✗ | 聚类（合成） |'
- en: '| Information Robust Dirichlet Networks (Tsiligkaridis, [2019](#bib.bib159))
    | OOD Detection, Adversarial Attack Detection | MNIST, FashionMNIST^((∗)) notMNIST^((∗)),
    Omniglot^((∗)) CIFAR-10, TIM^((∗)), SVHN^((∗)) | ✗ | ✗ |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 信息稳健狄利克雷网络（Tsiligkaridis, [2019](#bib.bib159)) | OOD 检测，对抗攻击检测 | MNIST，FashionMNIST^((∗))
    notMNIST^((∗))，Omniglot^((∗)) CIFAR-10，TIM^((∗))，SVHN^((∗)) | ✗ | ✗ |'
- en: '| Dirichlet via Function Decomposition (Biloš et al., [2019](#bib.bib11)) |
    OOD Detection | ✗ | Erdős-Rényi Graph (Synthetic), Stack Exchange, Smart Home,
    Car Indicators | ✗ |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 通过函数分解的狄利克雷（Biloš et al., [2019](#bib.bib11)) | OOD 检测 | ✗ | Erdős-Rényi
    图（合成），Stack Exchange，智能家居，汽车指示灯 | ✗ |'
- en: '| Prior network with PAC Regularization (Haussmann et al., [2019](#bib.bib58))
    | OOD Detection | MNIST, FashionMNIST^((∗)) CIFAR-10^((†)) | ✗ | ✗ |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 具有 PAC 正则化的先验网络（Haussmann et al., [2019](#bib.bib58)) | OOD 检测 | MNIST，FashionMNIST^((∗))
    CIFAR-10^((†)) | ✗ | ✗ |'
- en: '| Ensemble Distribution Distillation (Malinin et al., [2020b](#bib.bib118))
    | OOD Detection, Misclassification Detection, Calibration | CIFAR-10, CIFAR-100^((⋄))
    TIM^((⋄)), LSUN^((∗)) | ✗ | Spirals (Synthetic) |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 集成分布蒸馏（Malinin et al., [2020b](#bib.bib118)) | OOD 检测，误分类检测，校准 | CIFAR-10，CIFAR-100^((⋄))
    TIM^((⋄))，LSUN^((∗)) | ✗ | 螺旋（合成） |'
- en: '| Self-Distribution Distillation (Fathullah & Gales, [2022](#bib.bib39)) |
    OOD Detection, Calibration | CIFAR-100 SVHN^((∗)), LSUN^((∗)) | ✗ | ✗ |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 自我分布蒸馏（Fathullah & Gales, [2022](#bib.bib39)) | OOD 检测，校准 | CIFAR-100 SVHN^((∗))，LSUN^((∗))
    | ✗ | ✗ |'
- en: '| Prior networks with representation gap (Nandy et al., [2020](#bib.bib133))
    | OOD Detection | CIFAR-10^((⋄)), CIFAR-100^((⋄)) TIM, ImageNet^((∗)) | ✗ | Clusters
    (Synthetic) |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 具有表示差距的先验网络（Nandy et al., [2020](#bib.bib133)) | OOD 检测 | CIFAR-10^((⋄))，CIFAR-100^((⋄))
    TIM，ImageNet^((∗)) | ✗ | 聚类（合成） |'
- en: '| Prior RNN (Shen et al., [2020](#bib.bib151)) | New Concept Extraction | ✗
    | ✗ | Concept Learning^((⋄)), Snips^((⋄)), ATIS^((⋄)) (Language) |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 先验 RNN（Shen et al., [2020](#bib.bib151)) | 新概念提取 | ✗ | ✗ | 概念学习^((⋄))，Snips^((⋄))，ATIS^((⋄))（语言）
    |'
- en: '| Graph-based Kernel Dirichlet distribution estimation (GKDE) (Zhao et al.,
    [2020](#bib.bib189)) | OOD Detection Misclassification Detection | ✗ | ✗ | Coauthors
    Physics^((⋄)), Amazon Computer^((⋄)) Amazon Photo^((⋄)) (Graph) |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 基于图的内核狄利克雷分布估计（GKDE）（Zhao et al., [2020](#bib.bib189)) | OOD 检测 误分类检测 | ✗
    | ✗ | 合作者物理^((⋄))，亚马逊计算机^((⋄))，亚马逊照片^((⋄))（图） |'
- en: '| Evidential Deep Learning (Sensoy et al., [2018](#bib.bib146)) | OOD Detection,
    Adversarial Attack Detection | MNIST, notMNIST^((∗)), CIFAR-10^((†)) | ✗ | ✗ |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 证据深度学习（Sensoy et al., [2018](#bib.bib146)) | OOD 检测，对抗攻击检测 | MNIST，notMNIST^((∗))，CIFAR-10^((†))
    | ✗ | ✗ |'
- en: '| Regularized ENN Zhao et al. ([2019](#bib.bib188)) | OOD Detection | CIFAR-10^((†))
    | ✗ | Clusters (Synthetic) |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 正则化 ENN（Zhao et al., [2019](#bib.bib188)) | OOD 检测 | CIFAR-10^((†)) | ✗ |
    聚类（合成） |'
- en: '| WGAN–ENN (Hu et al., [2021](#bib.bib71)) | OOD Detection, Adversarial Attack
    Detection | MNIST, notMNIST^((∗)), CIFAR-10^((†)) | ✗ | Clusters (Synthetic) |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| WGAN–ENN（Hu et al., [2021](#bib.bib71)) | OOD 检测，对抗攻击检测 | MNIST，notMNIST^((∗))，CIFAR-10^((†))
    | ✗ | 聚类（合成） |'
- en: '| Variational Dirichlet (Chen et al., [2018](#bib.bib18)) | OOD Detection,
    Adversarial Attack Detection | MNIST, CIFAR-10/100, iSUN^((∗)), LSUN^((∗)), SVHN^((∗)),
    TIM^((∗)) | ✗ | ✗ |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 变分狄利克雷（Chen et al., [2018](#bib.bib18)) | OOD 检测，对抗攻击检测 | MNIST，CIFAR-10/100，iSUN^((∗))，LSUN^((∗))，SVHN^((∗))，TIM^((∗))
    | ✗ | ✗ |'
- en: '| Dirichlet Meta-Model (Shen et al., [2022](#bib.bib150)) | OOD Detection Misclassification
    Detection | MNIST^((⋄,†)), CIFAR-10^((⋄,†)), CIFAR-100^((⋄)), Omniglot^((∗)),
    FashionMNIST^((∗)), K-MNIST^((∗)), SVHN^((∗)), LSUN^((∗)), TIM^((∗)) | ✗ | ✗ |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 狄利克雷元模型（Shen et al., [2022](#bib.bib150)) | OOD 检测 误分类检测 | MNIST^((⋄,†))，CIFAR-10^((⋄,†))，CIFAR-100^((⋄))，Omniglot^((∗))，FashionMNIST^((∗))，K-MNIST^((∗))，SVHN^((∗))，LSUN^((∗))，TIM^((∗))
    | ✗ | ✗ |'
- en: '| Belief Matching (Joo et al., [2020](#bib.bib82)) | OOD Detection, Calibration
    | CIFAR-10/100, SVHN^((∗)) | ✗ | ✗ |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 信念匹配（Joo et al., [2020](#bib.bib82)) | OOD 检测，校准 | CIFAR-10/100，SVHN^((∗))
    | ✗ | ✗ |'
- en: '| Posterior Networks (Charpentier et al., [2020](#bib.bib16)) | OOD Detection,
    Misclassification Detection, Calibration | MNIST, FashionMNIST^((∗)), K-MNIST^((∗)),
    CIFAR-10, SVHN^((∗)) | Segment^((†)), Sensorless Drive^((†)) | Clusters (Synthetic)
    |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 后验网络 (Charpentier et al., [2020](#bib.bib16)) | OOD 检测、误分类检测、校准 | MNIST,
    FashionMNIST^((∗)), K-MNIST^((∗)), CIFAR-10, SVHN^((∗)) | Segment^((†)), Sensorless
    Drive^((†)) | 聚类（合成数据） |'
- en: '| Graph Posterior Networks (Stadler et al., [2021](#bib.bib155)) | OOD Detection,
    Misclassification Detection, Calibration | ✗ | ✗ | Amazon Computer^((⋄)), Amazon
    Photo^((⋄)) CoraML^((⋄)), CiteSeerCoraML^((⋄)), PubMed^((⋄)), Coauthors Physics^((⋄)),
    CoauthorsCS^((⋄)), OBGN Arxiv^((⋄)) (Graph) |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 图后验网络 (Stadler et al., [2021](#bib.bib155)) | OOD 检测、误分类检测、校准 | ✗ | ✗ | Amazon
    Computer^((⋄)), Amazon Photo^((⋄)) CoraML^((⋄)), CiteSeerCoraML^((⋄)), PubMed^((⋄)),
    Coauthors Physics^((⋄)), CoauthorsCS^((⋄)), OBGN Arxiv^((⋄))（图） |'
- en: '| Deep Evidential Regression (Amini et al., [2020](#bib.bib1)) | OOD Detection,
    Misclassification Detection, Adversarial Attack Detection Calibration | NYU Depth
    v2 ApolloScape^∗ (Depth Estimation) | UCI Regression Benchmark | Univariate Regression
    (Synthetic) |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 深度证据回归 (Amini et al., [2020](#bib.bib1)) | OOD 检测、误分类检测、对抗攻击检测、校准 | NYU Depth
    v2 ApolloScape^∗（深度估计） | UCI 回归基准 | 单变量回归（合成数据） |'
- en: '| Deep Evidential Regression with Multi-task Learning (Oh & Shin, [2022](#bib.bib136))
    | OOD Detection, Calibration | ✗ | Davis, Kiba^((†)), BindingDB, PubChem^((∗))
    (Drug discovery), UCI Regression Benchmark | Univariate Regression (Synthetic)
    |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| 具有多任务学习的深度证据回归 (Oh & Shin, [2022](#bib.bib136)) | OOD 检测、校准 | ✗ | Davis,
    Kiba^((†)), BindingDB, PubChem^((∗))（药物发现），UCI 回归基准 | 单变量回归（合成数据） |'
- en: '| Multivariate Deep Evidential Regression Meinert & Lavin ([2021](#bib.bib123))
    | Qualitative Evaluation | ✗ | ✗ | Multivariate Regression (Synthetic) |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 多变量深度证据回归 Meinert & Lavin ([2021](#bib.bib123)) | 质性评估 | ✗ | ✗ | 多变量回归（合成数据）
    |'
- en: '| Regression Prior Network (Malinin et al., [2020a](#bib.bib117)) | OOD Detection
    | NYU Depth v2^⋄, KITTI^⋄ (Depth Estimation) | UCI Regression Benchmark | Univariate
    Regression (Synthetic) |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| 回归先验网络 (Malinin et al., [2020a](#bib.bib117)) | OOD 检测 | NYU Depth v2^⋄,
    KITTI^⋄（深度估计） | UCI 回归基准 | 单变量回归（合成数据） |'
- en: '| Natural Posterior Network (Charpentier et al., [2022](#bib.bib17)) | OOD
    Detection, Calibration | NYU Depth v2, KITTI^∗, LSUN^((∗)) (Depth Estimation),
    MNIST, FashionMNIST^((∗)), K-MNIST^((∗)), CIFAR-10^((†)), SVHN^((∗)), CelebA^((∗))
    | UCI Regression Benchmark^((†)), Sensorless Drive^((†)), Bike Sharing^((†)) |
    Clusters (Synthetic), Univariate Regression (Synthetic)) |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| 自然后验网络 (Charpentier et al., [2022](#bib.bib17)) | OOD 检测、校准 | NYU Depth v2,
    KITTI^∗, LSUN^((∗))（深度估计），MNIST, FashionMNIST^((∗)), K-MNIST^((∗)), CIFAR-10^((†)),
    SVHN^((∗)), CelebA^((∗)) | UCI 回归基准^((†)), Sensorless Drive^((†)), Bike Sharing^((†))
    | 聚类（合成数据），单变量回归（合成数据） |'
- en: 'This section contains a discussion of the used datasets, methods to evaluate
    the quality of uncertainty evaluation, as well as a direct of available models
    based on the reported results to determine the most useful choices for practitioners.
    An overview over the differences between the surveyed works is given in [Table 5](#A2.T5
    "In Appendix B Datasets & Evaluation Techniques Appendix ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含对使用的数据集、评估不确定性评估质量的方法的讨论，以及基于报告结果的可用模型的直接比较，以确定对从业者最有用的选择。关于调查工作之间差异的概述见
    [表 5](#A2.T5 "附录 B 数据集与评估技术附录 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查")。
- en: Datasets
  id: totrans-461
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集
- en: 'Most models are applied to image classification problems, where popular choices
    involve the MNIST dataset (LeCun, [1998](#bib.bib100)), using as OOD datasets
    Fashion-MNIST (Xiao et al., [2017](#bib.bib181)), notMNIST (Bulatov, [2011](#bib.bib14))
    containing English letters, K-MNIST (Clanuwat et al., [2018](#bib.bib20)) with
    ancient Japanese Kuzushiji characters, and the Omniglot dataset (Lake et al.,
    [2015](#bib.bib98)), featuring handwritten characters from more than 50 alphabets.
    Other choices involve different versions of the CIFAR-10 object recognition dataset
    (LeCun et al., [1998](#bib.bib101); Krizhevsky et al., [2009](#bib.bib93)) for
    training purposes and SVHN (Goodfellow et al., [2014](#bib.bib53)), iSUN (Xiao
    et al., [2010](#bib.bib182)), LSUN (Yu et al., [2015](#bib.bib185)), CelebA (Liu
    et al., [2015](#bib.bib111)), ImageNet (Deng et al., [2009](#bib.bib30)) and TinyImagenet
    (Bastidas, [2017](#bib.bib7)) for OOD samples. Regression image datasets include
    for instance the NYU Depth Estimation v2 dataset (Silberman et al., [2012](#bib.bib152)),
    using ApolloScape (Huang et al., [2018](#bib.bib73)) or KITTI (Menze & Geiger,
    [2015](#bib.bib124)) as an OOD dataset. Many authors also illustrate model uncertainty
    on synthetic data, for instance by simulating clusters of data points using Gaussians
    (Malinin & Gales, [2018](#bib.bib115); [2019](#bib.bib116); Nandy et al., [2020](#bib.bib133);
    Zhao et al., [2019](#bib.bib188); Hu et al., [2020](#bib.bib70); Charpentier et al.,
    [2020](#bib.bib16); [2022](#bib.bib17)), spiral data (Malinin et al., [2020b](#bib.bib118))
    or polynomials for regression (Amini et al., [2020](#bib.bib1); Oh & Shin, [2022](#bib.bib136);
    Meinert & Lavin, [2021](#bib.bib123); Malinin et al., [2020a](#bib.bib117); Charpentier
    et al., [2022](#bib.bib17)). Tabular datasets include the Segment dataset, predicting
    image segments based on pixel features (Dua et al., [2017](#bib.bib34)), and the
    sensorless drive dataset (Dua et al., [2017](#bib.bib34); Paschke et al., [2013](#bib.bib139)),
    describing the maintenance state of electric current drives as well as popular
    regression datasets included in the UCI regression benchmark used by Hernández-Lobato
    & Adams ([2015](#bib.bib65)); Gal & Ghahramani ([2016](#bib.bib45)): Boston house
    prices (Harrison Jr & Rubinfeld, [1978](#bib.bib57)), concrete compression strength
    (Yeh, [1998](#bib.bib184)), energy efficiency of buildings (Tsanas & Xifara, [2012](#bib.bib158)),
    forward kinematics of an eight link robot arm (Corke, [1996](#bib.bib22)), maintenance
    of naval propulsion systems (Coraddu et al., [2016](#bib.bib21)), properties of
    protein tertiary stuctures, wine quality (Cortez et al., [2009](#bib.bib23)),
    and yacht hydrodynamics (Gerritsma et al., [1981](#bib.bib50)). Furthermore, Oh
    & Shin ([2022](#bib.bib136)) use a number of drug discovery datasets, such as
    Davis (Davis et al., [2011](#bib.bib26)), Kiba (Tang et al., [2014](#bib.bib156)),
    BindingDB (Liu et al., [2007](#bib.bib109)) and PubChem (Kim et al., [2019](#bib.bib86)).
    Biloš et al. ([2019](#bib.bib11)) are the only authors working on asynchronous
    time even prediction, and supply their own data in the form of processed stack
    exchange postings, smart home data, and car indicators. Shen et al. ([2020](#bib.bib151))
    provide the sole method on language data, and use three different concept learning
    datasets, i.e. Concept Learning (Jia et al., [2017](#bib.bib81)), Snips (Coucke
    et al., [2018](#bib.bib24)) and ATIS (Hemphill et al., [1990](#bib.bib63)), which
    contains new OOD concepts to be learned by design. For graph neural networks,
    Zhao et al. ([2020](#bib.bib189)) and Stadler et al. ([2021](#bib.bib155)) select
    data from the co-purchase datasets Amazon Computer, Amazon Photos (McAuley et al.,
    [2015](#bib.bib121)) and the CoraML (McCallum et al., [2000](#bib.bib122)), CiteSeer
    (Giles et al., [1998](#bib.bib52)) and PubMed (Namata et al., [2012](#bib.bib132)),
    Coauthors Physics (Shchur et al., [2018](#bib.bib149)), CoauthorCS (Namata et al.,
    [2012](#bib.bib132)) and OGBN Arxiv (Hu et al., [2020](#bib.bib70)) citation datasets.
    Lastly, Charpentier et al. ([2022](#bib.bib17)) use a single count prediction
    dataset concerned with predicting the number of bike rentals (Fanaee-T & Gama,
    [2014](#bib.bib38)).'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型应用于图像分类问题，其中常见的选择包括 MNIST 数据集（LeCun, [1998](#bib.bib100)），使用 Fashion-MNIST（Xiao
    et al., [2017](#bib.bib181)）、notMNIST（Bulatov, [2011](#bib.bib14)）包含英文字符，K-MNIST（Clanuwat
    et al., [2018](#bib.bib20)）包含古代日本的 Kuzushiji 字符，以及 Omniglot 数据集（Lake et al., [2015](#bib.bib98)），展示了来自
    50 多种字母表的手写字符。其他选择涉及 CIFAR-10 物体识别数据集的不同版本（LeCun et al., [1998](#bib.bib101);
    Krizhevsky et al., [2009](#bib.bib93)）用于训练，以及 SVHN（Goodfellow et al., [2014](#bib.bib53)）、iSUN（Xiao
    et al., [2010](#bib.bib182)）、LSUN（Yu et al., [2015](#bib.bib185)）、CelebA（Liu et
    al., [2015](#bib.bib111)）、ImageNet（Deng et al., [2009](#bib.bib30)）和 TinyImagenet（Bastidas,
    [2017](#bib.bib7)）用于 OOD 样本。回归图像数据集例如 NYU Depth Estimation v2 数据集（Silberman et
    al., [2012](#bib.bib152)），使用 ApolloScape（Huang et al., [2018](#bib.bib73)）或 KITTI（Menze
    & Geiger, [2015](#bib.bib124)）作为 OOD 数据集。许多作者还在合成数据上展示模型的不确定性，例如通过模拟高斯分布的数据点簇（Malinin
    & Gales, [2018](#bib.bib115); [2019](#bib.bib116); Nandy et al., [2020](#bib.bib133);
    Zhao et al., [2019](#bib.bib188); Hu et al., [2020](#bib.bib70); Charpentier et
    al., [2020](#bib.bib16); [2022](#bib.bib17)）、螺旋数据（Malinin et al., [2020b](#bib.bib118)）或回归用多项式（Amini
    et al., [2020](#bib.bib1); Oh & Shin, [2022](#bib.bib136); Meinert & Lavin, [2021](#bib.bib123);
    Malinin et al., [2020a](#bib.bib117); Charpentier et al., [2022](#bib.bib17)）。表格数据集包括
    Segment 数据集，根据像素特征预测图像分段（Dua et al., [2017](#bib.bib34)），以及无传感器驱动数据集（Dua et al.,
    [2017](#bib.bib34); Paschke et al., [2013](#bib.bib139)），描述电流驱动的维护状态，以及 Hernández-Lobato
    & Adams（[2015](#bib.bib65)）；Gal & Ghahramani（[2016](#bib.bib45)）使用的 UCI 回归基准中包含的流行回归数据集：波士顿房价（Harrison
    Jr & Rubinfeld, [1978](#bib.bib57)）、混凝土压缩强度（Yeh, [1998](#bib.bib184)）、建筑能效（Tsanas
    & Xifara, [2012](#bib.bib158)）、八链接机器人臂的前向运动学（Corke, [1996](#bib.bib22)）、海军推进系统的维护（Coraddu
    et al., [2016](#bib.bib21)）、蛋白质三级结构的属性、葡萄酒质量（Cortez et al., [2009](#bib.bib23)）以及游艇水动力学（Gerritsma
    et al., [1981](#bib.bib50)）。此外，Oh & Shin（[2022](#bib.bib136)）使用了多个药物发现数据集，如 Davis（Davis
    et al., [2011](#bib.bib26)）、Kiba（Tang et al., [2014](#bib.bib156)）、BindingDB（Liu
    et al., [2007](#bib.bib109)）和 PubChem（Kim et al., [2019](#bib.bib86)）。Biloš et
    al.（[2019](#bib.bib11)）是唯一研究异步时间预测的作者，并提供了处理后的堆叠交换帖子、智能家居数据和汽车指标的数据。Shen et al.（[2020](#bib.bib151)）提供了唯一的方法来处理语言数据，并使用了三个不同的概念学习数据集，即
    Concept Learning（Jia et al., [2017](#bib.bib81)）、Snips（Coucke et al., [2018](#bib.bib24)）和
    ATIS（Hemphill et al., [1990](#bib.bib63)），这些数据集包含了设计上要学习的新 OOD 概念。对于图神经网络，Zhao
    et al.（[2020](#bib.bib189)）和 Stadler et al.（[2021](#bib.bib155)）选择了来自共购数据集的数据，包括
    Amazon Computer、Amazon Photos（McAuley et al., [2015](#bib.bib121)）和 CoraML（McCallum
    et al., [2000](#bib.bib122)）、CiteSeer（Giles et al., [1998](#bib.bib52)）以及 PubMed（Namata
    et al., [2012](#bib.bib132)）、Coauthors Physics（Shchur et al., [2018](#bib.bib149)）、CoauthorCS（Namata
    et al., [2012](#bib.bib132)）和 OGBN Arxiv（Hu et al., [2020](#bib.bib70)）引用数据集。最后，Charpentier
    et al.（[2022](#bib.bib17)）使用了一个单一的计数预测数据集，关注预测自行车租赁数量（Fanaee-T & Gama, [2014](#bib.bib38)）。
- en: Uncertainty Evaluation Methods
  id: totrans-463
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不确定性评估方法
- en: 'There usually are no gold labels for uncertainty estimates, which is why the
    efficacy of proposed solutions has to be evaluated in a different way. One such
    way used by almost all the surveyed works is using uncertainty estimates in a
    proxy OOD detection task: Since the model is underspecified on unseen samples
    from another distribution, it should be more uncertain. By labelling OOD samples
    as the positive and ID inputs as the negative class, we can measure the performance
    of uncertainty estimates using the area under the receiver-operator characteristic
    (AUROC) or the area under the precision-recall curve (AUPR). We can thereby characterize
    the usage of data from another dataset as a form of covariate shift, while using
    left-out classes for testing can be seen as a kind of concept shift (Moreno-Torres
    et al., [2012](#bib.bib127)). Instead of using OOD data, another approach is to
    use adversarial examples (Malinin & Gales, [2019](#bib.bib116); Tsiligkaridis,
    [2019](#bib.bib159); Sensoy et al., [2018](#bib.bib146); Hu et al., [2021](#bib.bib71);
    Chen et al., [2018](#bib.bib18); Amini et al., [2020](#bib.bib1)), checking if
    they can be identified through uncertainty. In the case of Shen et al. ([2020](#bib.bib151)),
    OOD detection or new concept extraction is the actual and not a proxy task, and
    thus can be evaluated using classical metrics such as the $F_{1}$ score. Another
    way is misclassification detection: In general, we would desire the model to be
    more uncertain about inputs it incurs a higher loss on, i.e., what it is more
    wrong about. For this purpose, some works (Malinin & Gales, [2018](#bib.bib115);
    Zhao et al., [2020](#bib.bib189); Charpentier et al., [2020](#bib.bib16)) measure
    whether let missclassified inputs be the positive class in another binary proxy
    classification test, and again measure AUROC and AUPR. Alternatively, Malinin
    et al. ([2020b](#bib.bib118)); Stadler et al. ([2021](#bib.bib155)); Amini et al.
    ([2020](#bib.bib1)) show or measure the area under the prediction / rejection
    curve, graphing how task performance varies as predictions on increasingly uncertain
    inputs is suspended. Lastly, some authors look at a model’s calibration (Guo et al.,
    [2017](#bib.bib56)): While this does not allow to judge the quality of uncertainty
    estimates themselves, quantities like the expected calibration error quantify
    to what extent the output distribution of a classifier corresponds to the true
    label distribution, and thus whether aleatoric uncertainty is accurately reflected.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，对于不确定性估计没有金标准标签，这就是为什么必须以不同的方式评估所提出的解决方案的有效性。几乎所有调查的研究使用的一种方法是将不确定性估计应用于代理
    OOD 检测任务：由于模型在来自不同分布的未见样本上被指定不足，因此它应该更加不确定。通过将 OOD 样本标记为正类，将 ID 输入标记为负类，我们可以使用接收器操作特征曲线下面积（AUROC）或精确度-召回曲线下面积（AUPR）来衡量不确定性估计的性能。这样，我们可以将来自另一个数据集的数据使用情况描述为一种协变量转移，同时，使用被遗漏的类别进行测试可以视为一种概念转移（Moreno-Torres
    等，[2012](#bib.bib127)）。除了使用 OOD 数据，另一种方法是使用对抗样本（Malinin & Gales, [2019](#bib.bib116);
    Tsiligkaridis, [2019](#bib.bib159); Sensoy et al., [2018](#bib.bib146); Hu et
    al., [2021](#bib.bib71); Chen et al., [2018](#bib.bib18); Amini et al., [2020](#bib.bib1)），检查是否可以通过不确定性来识别它们。在
    Shen 等人（[2020](#bib.bib151)）的研究中，OOD 检测或新概念提取是实际任务而非代理任务，因此可以使用经典的指标，如 $F_{1}$
    分数进行评估。另一种方法是错误分类检测：一般来说，我们希望模型对其损失较高的输入表现出更大的不确定性，即对其更为错误的地方。为此，一些研究（Malinin
    & Gales, [2018](#bib.bib115); Zhao et al., [2020](#bib.bib189); Charpentier et
    al., [2020](#bib.bib16)）测量是否将错误分类的输入作为另一种二元代理分类测试中的正类，然后再次测量 AUROC 和 AUPR。或者，Malinin
    等人（[2020b](#bib.bib118)）；Stadler 等人（[2021](#bib.bib155)）；Amini 等人（[2020](#bib.bib1)）展示或测量预测/拒绝曲线下面积，绘制任务性能如何随对日益不确定输入的预测被暂停而变化。最后，一些作者关注模型的校准（Guo
    et al., [2017](#bib.bib56)）：虽然这不能判断不确定性估计本身的质量，但像期望校准误差这样的量化指标可以量化分类器输出分布与真实标签分布的对应程度，从而反映
    aleatoric 不确定性是否得到了准确的反映。
- en: Appendix C Fundamental Derivations Appendix
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 基础推导附录
- en: 'This appendix section walks the reader through generalized versions of recurring
    theoretical results using Dirichlet distributions in a Machine Learning context,
    such as their expectation in [Section C.1](#A3.SS1 "C.1 Expectation of a Dirichlet
    ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), their
    entropy in [Section C.2](#A3.SS2 "C.2 Entropy of Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation") and the Kullback-Leibler divergence
    between two Dirichlets in [Section D.3](#A4.SS3 "D.3 𝑙_∞ Norm Derivation ‣ Appendix
    D Additional Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation").'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录部分引导读者了解 Dirichlet 分布在机器学习背景下的理论结果的推广版本，例如它们在 [C.1 节](#A3.SS1 "C.1 Dirichlet
    分布的期望 ‣ 附录 C 基本推导附录 ‣ 先验与后验网络：证据深度学习方法用于不确定性估计的调查") 中的期望、在 [C.2 节](#A3.SS2 "C.2
    Dirichlet 分布的熵 ‣ 附录 C 基本推导附录 ‣ 先验与后验网络：证据深度学习方法用于不确定性估计的调查") 中的熵，以及在 [D.3 节](#A4.SS3
    "D.3 𝑙_∞ 范数推导 ‣ 附录 D 附加推导附录 ‣ 先验与后验网络：证据深度学习方法用于不确定性估计的调查") 中的两个 Dirichlet 之间的
    Kullback-Leibler 散度。
- en: C.1 Expectation of a Dirichlet
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 Dirichlet 分布的期望
- en: Here, we show results for the quantities $\mathbb{E}_{#1}[\pi_{k}]$ and $\mathbb{E}_{#1}[\log\pi_{k}]$.
    For the first, we follow the derivation by Miller ([2011](#bib.bib125)). Another
    proof is given by Lin ([2016](#bib.bib107)).
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了量 $\mathbb{E}_{#1}[\pi_{k}]$ 和 $\mathbb{E}_{#1}[\log\pi_{k}]$ 的结果。对于第一个，我们参考了
    Miller 的推导 ([2011](#bib.bib125))。另一个证明由 Lin 提供 ([2016](#bib.bib107))。
- en: '|  | $\displaystyle\mathbb{E}_{#1}[\pi_{k}]$ | $\displaystyle=\int\cdots\int\pi_{k}\frac{\Gamma(\alpha_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\alpha_{k}^{\prime})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\pi_{1}\ldots
    d\pi_{K}$ |  | (30) |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{#1}[\pi_{k}]$ | $\displaystyle=\int\cdots\int\pi_{k}\frac{\Gamma(\alpha_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\alpha_{k}^{\prime})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\pi_{1}\ldots
    d\pi_{K}$ |  | (30) |'
- en: '| Moving $\pi_{k}^{\alpha_{k}-1}$ out of the product: |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 将 $\pi_{k}^{\alpha_{k}-1}$ 从乘积中移出： |'
- en: '|  |  | $\displaystyle=\int\cdots\int\frac{\Gamma(\alpha_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\alpha_{k^{\prime}})}\pi_{k}^{\alpha_{k}-1+1}\prod_{k^{\prime}\neq
    k}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\pi_{1}\ldots d\pi_{K}$ |  | (31) |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\int\cdots\int\frac{\Gamma(\alpha_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\alpha_{k^{\prime}})}\pi_{k}^{\alpha_{k}-1+1}\prod_{k^{\prime}\neq
    k}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\pi_{1}\ldots d\pi_{K}$ |  | (31) |'
- en: '| For the next step, we define a new set of Dirichlet parameters with $\beta_{k}=\alpha_{k}+1$
    and $\forall k^{\prime}\neq k:\beta_{k^{\prime}}=\alpha_{k^{\prime}}$. For those
    new parameters, $\beta_{0}=\sum_{k}\beta_{k}=1+\alpha_{0}$. So by virtue of the
    Gamma function’s property that $\Gamma(\beta_{0})=\Gamma(\alpha_{0}+1)=\alpha_{0}\Gamma(\alpha_{0})$,
    replacing all terms in the normalization factor yields |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 在下一步中，我们定义一组新的 Dirichlet 参数，其中 $\beta_{k}=\alpha_{k}+1$ 且 $\forall k^{\prime}\neq
    k:\beta_{k^{\prime}}=\alpha_{k^{\prime}}$。对于这些新参数，$\beta_{0}=\sum_{k}\beta_{k}=1+\alpha_{0}$。由于
    Gamma 函数的性质 $\Gamma(\beta_{0})=\Gamma(\alpha_{0}+1)=\alpha_{0}\Gamma(\alpha_{0})$，替换所有标准化因子的项得
    |'
- en: '|  |  | $\displaystyle=\int\cdots\int\frac{\alpha_{k}}{\alpha_{0}}\frac{\Gamma(\beta_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\beta_{k^{\prime}})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\beta_{k^{\prime}}-1}d\pi_{1}\ldots
    d\pi_{K}=\frac{\alpha_{k}}{\alpha_{0}}$ |  | (32) |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\int\cdots\int\frac{\alpha_{k}}{\alpha_{0}}\frac{\Gamma(\beta_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\beta_{k^{\prime}})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\beta_{k^{\prime}}-1}d\pi_{1}\ldots
    d\pi_{K}=\frac{\alpha_{k}}{\alpha_{0}}$ |  | (32) |'
- en: where in the last step we obtain the final result, since the Dirichlet with
    new parameters $\beta_{k}$ must nevertheless integrate to $1$, and the integrals
    do not regard $\alpha_{k}$ or $\alpha_{0}$. For the expectation $\mathbb{E}_{#1}[\log\pi_{k}]$,
    we first rephrase the Dirichlet distribution in terms of the exponential family
    (Kupperman, [1964](#bib.bib95)). The exponential family encompasses many commonly-used
    distributions, such as the normal, exponential, Beta or Poisson, which all follow
    the form
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步中，我们获得最终结果，因为具有新参数 $\beta_{k}$ 的 Dirichlet 分布仍然必须积分为 $1$，且积分不涉及 $\alpha_{k}$
    或 $\alpha_{0}$。对于期望 $\mathbb{E}_{#1}[\log\pi_{k}]$，我们首先将 Dirichlet 分布用指数族来重新表述（Kupperman，[1964](#bib.bib95)）。指数族包含许多常用分布，如正态分布、指数分布、Beta
    分布或 Poisson 分布，它们都遵循形式
- en: '|  | $p(\mathbf{x};\bm{\eta})=h(\mathbf{x})\exp\big{(}\bm{\eta}^{T}u(\mathbf{x})-A(\bm{\eta})\big{)}$
    |  | (33) |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathbf{x};\bm{\eta})=h(\mathbf{x})\exp\big{(}\bm{\eta}^{T}u(\mathbf{x})-A(\bm{\eta})\big{)}$
    |  | (33) |'
- en: with *natural parameters* $\bm{\eta}$, *sufficient statistic* $u(\mathbf{x})$,
    and *log-partition function* $A(\bm{\eta})$. For the Dirichlet distribution, Winn
    ([2004](#bib.bib179)) provides the sufficient statistic as $u(\bm{\pi})=[\log\bm{\pi}_{1},\ldots,\bm{\pi}_{K}]^{T}$
    and the log-partition function
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*自然参数* $\bm{\eta}$、*充分统计量* $u(\mathbf{x})$ 和 *对数配分函数* $A(\bm{\eta})$。对于 Dirichlet
    分布，Winn ([2004](#bib.bib179)) 提供了充分统计量 $u(\bm{\pi})=[\log\bm{\pi}_{1},\ldots,\bm{\pi}_{K}]^{T}$
    及对数配分函数
- en: '|  | $A(\bm{\alpha})=\sum_{k=1}^{K}\log\Gamma(\alpha_{k})-\log\Gamma(\alpha_{o})$
    |  | (34) |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '|  | $A(\bm{\alpha})=\sum_{k=1}^{K}\log\Gamma(\alpha_{k})-\log\Gamma(\alpha_{o})$
    |  | (34) |'
- en: By Mao ([2019](#bib.bib119)), we also find that by the moment-generating function
    that for the sufficient statistic, its expectation can be derived by
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Mao ([2019](#bib.bib119))，我们还发现通过矩生成函数，对于充分统计量，其期望可以通过
- en: '|  | $\mathbb{E}_{#1}[u(\mathbf{x})_{k}]=\frac{\partial A(\bm{\eta})}{\partial\eta_{k}}$
    |  | (35) |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{#1}[u(\mathbf{x})_{k}]=\frac{\partial A(\bm{\eta})}{\partial\eta_{k}}$
    |  | (35) |'
- en: 'Therefore we can evaluate the expected value of $\log\pi_{k}$ (i.e. the sufficient
    statistic) by inserting the definition of the log-partition function in [Equation 34](#A3.E34
    "In C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") into [Equation 35](#A3.E35 "In C.1 Expectation of a Dirichlet
    ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation"):'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过将对数配分函数的定义插入 [方程 34](#A3.E34 "在 C.1 Dirichlet 的期望 ‣ 附录 C 基本推导附录 ‣ 先验和后验网络：针对不确定性估计的证据深度学习方法调查")
    到 [方程 35](#A3.E35 "在 C.1 Dirichlet 的期望 ‣ 附录 C 基本推导附录 ‣ 先验和后验网络：针对不确定性估计的证据深度学习方法调查")
    中来评估 $\log\pi_{k}$（即充分统计量）的期望值：
- en: '|  | $\displaystyle\mathbb{E}_{#1}[\log\pi_{k}]=\frac{\partial}{\partial\alpha_{k}}\sum_{k=1}^{K}\log\Gamma(\alpha_{k})-\log\Gamma(\alpha_{0})=\psi(\alpha_{k})-\psi(\alpha_{0})$
    |  | (36) |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{#1}[\log\pi_{k}]=\frac{\partial}{\partial\alpha_{k}}\sum_{k=1}^{K}\log\Gamma(\alpha_{k})-\log\Gamma(\alpha_{0})=\psi(\alpha_{k})-\psi(\alpha_{0})$
    |  | (36) |'
- en: which corresponds precisely to the definition of the digamma function as $\psi(x)=\frac{d}{dx}\log\Gamma(x)$.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 digamma 函数的定义完全一致，定义为 $\psi(x)=\frac{d}{dx}\log\Gamma(x)$。
- en: C.2 Entropy of Dirichlet
  id: totrans-483
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 Dirichlet 分布的熵
- en: The following derivation is adapted from Lin ([2016](#bib.bib107)), with the
    result stated in Charpentier et al. ([2020](#bib.bib16)) as well.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 以下推导改编自 Lin ([2016](#bib.bib107))，结果也在 Charpentier 等人 ([2020](#bib.bib16)) 中说明。
- en: '|  | $\displaystyle H[\bm{\pi}]$ | $\displaystyle=-\mathbb{E}_{#1}[\log p(\bm{\pi}&#124;\bm{\alpha})]$
    |  | (37) |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H[\bm{\pi}]$ | $\displaystyle=-\mathbb{E}_{#1}[\log p(\bm{\pi}&#124;\bm{\alpha})]$
    |  | (37) |'
- en: '|  |  | $\displaystyle=-\mathbb{E}_{#1}\bigg{[}\log\Big{(}\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1}\Big{)}\bigg{]}$
    |  | (38) |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\mathbb{E}_{#1}\bigg{[}\log\Big{(}\frac{1}{B(\bm{\alpha})}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1}\Big{)}\bigg{]}$
    |  | (38) |'
- en: '|  |  | $\displaystyle=-\mathbb{E}_{#1}\bigg{[}-\log B(\bm{\alpha})+\sum_{k=1}^{K}(\alpha_{k}-1)\log\pi_{k}\bigg{]}$
    |  | (39) |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\mathbb{E}_{#1}\bigg{[}-\log B(\bm{\alpha})+\sum_{k=1}^{K}(\alpha_{k}-1)\log\pi_{k}\bigg{]}$
    |  | (39) |'
- en: '|  |  | $\displaystyle=\log B(\bm{\alpha})-\sum_{k=1}^{K}(\alpha_{k}-1)\mathbb{E}_{#1}[\log\pi_{k}]$
    |  | (40) |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\log B(\bm{\alpha})-\sum_{k=1}^{K}(\alpha_{k}-1)\mathbb{E}_{#1}[\log\pi_{k}]$
    |  | (40) |'
- en: '| Using [Equation 36](#A3.E36 "In C.1 Expectation of a Dirichlet ‣ Appendix
    C Fundamental Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation"): |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| 使用 [方程 36](#A3.E36 "在 C.1 Dirichlet 的期望 ‣ 附录 C 基本推导附录 ‣ 先验和后验网络：针对不确定性估计的证据深度学习方法调查"):
    |'
- en: '|  |  | $\displaystyle=\log B(\bm{\alpha})-\sum_{k=1}^{K}(\alpha_{k}-1)\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (41) |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\log B(\bm{\alpha})-\sum_{k=1}^{K}(\alpha_{k}-1)\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (41) |'
- en: '|  |  | $\displaystyle=\log B(\bm{\alpha})+\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{0})-\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{k})$
    |  | (42) |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\log B(\bm{\alpha})+\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{0})-\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{k})$
    |  | (42) |'
- en: '|  |  | $\displaystyle=\log B(\bm{\alpha})+(\alpha_{0}-K)\psi(\alpha_{0})-\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{k})$
    |  | (43) |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\log B(\bm{\alpha})+(\alpha_{0}-K)\psi(\alpha_{0})-\sum_{k=1}^{K}(\alpha_{k}-1)\psi(\alpha_{k})$
    |  | (43) |'
- en: C.3 Kullback-Leibler Divergence between two Dirichlets
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 两个 Dirichlet 之间的 Kullback-Leibler 散度
- en: 'The following result is presented using an adapted derivation by Lin ([2016](#bib.bib107))
    and appears in Chen et al. ([2018](#bib.bib18)) and Joo et al. ([2020](#bib.bib82))
    as a starting point for their variational objective (see [Section D.7](#A4.SS7
    "D.7 Evidence-Lower Bound For Dirichlet Posterior Estimation ‣ Appendix D Additional
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")). In the following we use $\text{Dir}(\bm{\pi};\bm{\alpha})$
    to denote the optimized distribution, and $\text{Dir}(\bm{\pi};\bm{\gamma})$ the
    reference or target distribution.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '以下结果使用 Lin ([2016](#bib.bib107)) 的改编推导，并在 Chen 等 ([2018](#bib.bib18)) 和 Joo
    等 ([2020](#bib.bib82)) 的变分目标中作为起点（参见 [第 D.7 节](#A4.SS7 "D.7 Evidence-Lower Bound
    For Dirichlet Posterior Estimation ‣ 附录 D 附加推导 附录 ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")）。在以下内容中，我们用
    $\text{Dir}(\bm{\pi};\bm{\alpha})$ 来表示优化后的分布，用 $\text{Dir}(\bm{\pi};\bm{\gamma})$
    来表示参考或目标分布。'
- en: '|  | $\displaystyle\text{KL}\Big{[}p(\bm{\pi}&#124;\bm{\alpha})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{\gamma})\Big{]}=$
    | $\displaystyle\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\bm{\alpha})}{p(\bm{\pi}&#124;\bm{\gamma})}\bigg{]}=\mathbb{E}_{#1}\bigg{[}\log
    p(\bm{\pi}&#124;\bm{\alpha})\bigg{]}-\mathbb{E}_{#1}\bigg{[}\log p(\bm{\pi}&#124;\bm{\gamma})\bigg{]}$
    |  | (44) |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{KL}\Big{[}p(\bm{\pi}&#124;\bm{\alpha})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{\gamma})\Big{]}=$
    | $\displaystyle\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\bm{\alpha})}{p(\bm{\pi}&#124;\bm{\gamma})}\bigg{]}=\mathbb{E}_{#1}\bigg{[}\log
    p(\bm{\pi}&#124;\bm{\alpha})\bigg{]}-\mathbb{E}_{#1}\bigg{[}\log p(\bm{\pi}&#124;\bm{\gamma})\bigg{]}$
    |  | (44) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{#1}\bigg{[}-\log B(\bm{\alpha})+\sum_{k=1}^{K}(\alpha_{k}-1)\log\pi_{k}\bigg{]}$
    |  |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\mathbb{E}_{#1}\bigg{[}-\log B(\bm{\alpha})+\sum_{k=1}^{K}(\alpha_{k}-1)\log\pi_{k}\bigg{]}$
    |  |'
- en: '|  | $\displaystyle-$ | $\displaystyle\mathbb{E}_{#1}\bigg{[}-\log B(\bm{\gamma})+\sum_{k=1}^{K}(\gamma_{k}-1)\log\pi_{k}\bigg{]}$
    |  | (45) |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle-$ | $\displaystyle\mathbb{E}_{#1}\bigg{[}-\log B(\bm{\gamma})+\sum_{k=1}^{K}(\gamma_{k}-1)\log\pi_{k}\bigg{]}$
    |  | (45) |'
- en: '| Distributing and pulling out $B(\bm{\alpha})$ and $B(\bm{\gamma})$ out of
    the expectation (they don’t depend on $\bm{\pi}$): |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| 分配并将 $B(\bm{\alpha})$ 和 $B(\bm{\gamma})$ 从期望中提取出来（它们不依赖于 $\bm{\pi}$）： |'
- en: '|  | $\displaystyle=$ | $\displaystyle-\log\frac{B(\bm{\gamma})}{B(\bm{\alpha})}+\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}(\alpha_{k}-1)\log\pi_{k}-(\gamma_{k}-1)\log\pi_{k}\bigg{]}$
    |  | (46) |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-\log\frac{B(\bm{\gamma})}{B(\bm{\alpha})}+\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}(\alpha_{k}-1)\log\pi_{k}-(\gamma_{k}-1)\log\pi_{k}\bigg{]}$
    |  | (46) |'
- en: '|  | $\displaystyle=$ | $\displaystyle-\log\frac{B(\bm{\gamma})}{B(\bm{\alpha})}+\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}(\alpha_{k}-\gamma_{k})\log\pi_{k}\bigg{]}$
    |  | (47) |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-\log\frac{B(\bm{\gamma})}{B(\bm{\alpha})}+\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}(\alpha_{k}-\gamma_{k})\log\pi_{k}\bigg{]}$
    |  | (47) |'
- en: '| Moving the expectation inward and using the identity $\mathbb{E}_{#1}[\pi_{k}]=\psi(\alpha_{k})-\psi(\alpha_{0})$
    from [Section C.1](#A3.SS1 "C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"): |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| 将期望向内移动，并使用 [第 C.1 节](#A3.SS1 "C.1 Expectation of a Dirichlet ‣ 附录 C 基本推导
    附录 ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods
    For Uncertainty Estimation") 中的恒等式 $\mathbb{E}_{#1}[\pi_{k}]=\psi(\alpha_{k})-\psi(\alpha_{0})$：
    |'
- en: '|  | $\displaystyle=$ | $\displaystyle-\log\frac{B(\bm{\gamma})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\gamma_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (48) |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle-\log\frac{B(\bm{\gamma})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\gamma_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (48) |'
- en: The KL divergence is also used by some works as regularizer by penalizing the
    distance to a uniform Dirichlet with $\bm{\gamma}=\mathbf{1}$ (Sensoy et al.,
    [2018](#bib.bib146)). In this case, the result above can be derived to be
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度也被一些研究作为正则化器，通过惩罚到均匀Dirichlet分布的距离，其中$\bm{\gamma}=\mathbf{1}$（Sensoy等，[2018](#bib.bib146)）。在这种情况下，上述结果可以推导为
- en: '|  | $\text{KL}\Big{[}p(\bm{\pi}&#124;\bm{\alpha})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{1})\Big{]}=\log\frac{\Gamma(K)}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-1)\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (49) |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{KL}\Big{[}p(\bm{\pi}&#124;\bm{\alpha})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\bm{1})\Big{]}=\log\frac{\Gamma(K)}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-1)\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    |  | (49) |'
- en: where the $\log\Gamma(K)$ term can also be omitted for optimization purposes,
    since it does not depend on $\bm{\alpha}$.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$\log\Gamma(K)$项不依赖于$\bm{\alpha}$，因此在优化时可以省略。
- en: Appendix D Additional Derivations Appendix
  id: totrans-506
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 附加推导 附录
- en: 'In this appendix we present relevant results in a Machine Learning context,
    including from some of the surveyed works, featuring as unified notation and annotated
    derivation steps. These include derivations of expected entropy ([Section D.1](#A4.SS1
    "D.1 Derivation of Expected Entropy ‣ Appendix D Additional Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")) and mutual information ([Section D.2](#A4.SS2 "D.2 Derivation
    of Mutual Information ‣ Appendix D Additional Derivations Appendix ‣ Prior and
    Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")) as uncertainty metrics for Dirichlet networks. Also, we derive a
    multitude of loss functions, including the $l_{\infty}$ norm loss of a Dirichlet
    w.r.t. a one-hot encoded class label in [Section D.3](#A4.SS3 "D.3 𝑙_∞ Norm Derivation
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation"), the $l_{2}$
    norm loss in [Section D.4](#A4.SS4 "D.4 𝑙₂ Norm Loss Derivation ‣ Appendix D Additional
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"), as well as the reverse KL loss
    by Malinin & Gales ([2019](#bib.bib116)), the UCE objective Biloš et al. ([2019](#bib.bib11));
    Charpentier et al. ([2020](#bib.bib16)) and ELBO Shen et al. ([2020](#bib.bib151));
    Chen et al. ([2018](#bib.bib18)) as training objectives ([Sections D.5](#A4.SS5
    "D.5 Derivation of Reverse KL loss ‣ Appendix D Additional Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation"), [D.6](#A4.SS6 "D.6 Uncertainty-aware Cross-Entropy Loss
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation") and [D.7](#A4.SS7
    "D.7 Evidence-Lower Bound For Dirichlet Posterior Estimation ‣ Appendix D Additional
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")).'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '在本附录中，我们展示了在机器学习背景下相关的结果，包括一些调查过的工作，采用统一的符号和注释的推导步骤。这些包括对期望熵的推导（[第D.1节](#A4.SS1
    "D.1 Derivation of Expected Entropy ‣ Appendix D Additional Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")）和互信息的推导（[第D.2节](#A4.SS2 "D.2 Derivation of Mutual Information
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation")），作为Dirichlet网络的**不确定性度量**。此外，我们推导了多种损失函数，包括[第D.3节](#A4.SS3
    "D.3 𝑙_∞ Norm Derivation ‣ Appendix D Additional Derivations Appendix ‣ Prior
    and Posterior Networks: A Survey on Evidential Deep Learning Methods For Uncertainty
    Estimation")中Dirichlet相对于一个独热编码类标签的$l_{\infty}$范数损失，[第D.4节](#A4.SS4 "D.4 𝑙₂ Norm
    Loss Derivation ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")中的$l_{2}$范数损失，以及Malinin
    & Gales（[2019](#bib.bib116)）、UCE目标Biloš et al.（[2019](#bib.bib11)）、Charpentier
    et al.（[2020](#bib.bib16)）和ELBO Shen et al.（[2020](#bib.bib151)）、Chen et al.（[2018](#bib.bib18)）作为训练目标的反向KL损失（[第D.5节](#A4.SS5
    "D.5 Derivation of Reverse KL loss ‣ Appendix D Additional Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")）、[第D.6节](#A4.SS6 "D.6 Uncertainty-aware Cross-Entropy
    Loss ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")和[第D.7节](#A4.SS7
    "D.7 Evidence-Lower Bound For Dirichlet Posterior Estimation ‣ Appendix D Additional
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")）。'
- en: D.1 Derivation of Expected Entropy
  id: totrans-508
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 期望熵的推导
- en: 'The following derivation is adapted from Malinin & Gales ([2018](#bib.bib115))
    appendix section C.4\. In the following, we assume that $\forall k\in\mathbb{K}:\pi_{k}>0$:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 以下推导来自Malinin & Gales（[2018](#bib.bib115)）附录第C.4节。在下文中，我们假设$\forall k\in\mathbb{K}:\pi_{k}>0$：
- en: '|  |  | $\displaystyle\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}=\int
    p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})\bigg{(}-\sum_{k=1}^{K}\pi_{k}\log\pi_{k}\bigg{)}d\bm{\pi}$
    |  | (50) |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y\mid\bm{\pi})\Big{]}\bigg{]}=\int
    p(\bm{\pi}\mid\mathbf{x},\hat{\bm{\theta}})\bigg{(}-\sum_{k=1}^{K}\pi_{k}\log\pi_{k}\bigg{)}d\bm{\pi}$
    |  | (50) |'
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\int p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})\Big{(}\pi_{k}\log\pi_{k}\Big{)}d\bm{\pi}$
    |  | (51) |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\int p(\bm{\pi}\mid\mathbf{x},\hat{\bm{\theta}})\Big{(}\pi_{k}\log\pi_{k}\Big{)}d\bm{\pi}$
    |  | (51) |'
- en: '| Inserting the definition of $p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})\approx
    p(\bm{\pi}&#124;\mathbf{x},\mathbb{D})$: |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| 插入$p(\bm{\pi}\mid\mathbf{x},\hat{\bm{\theta}})\approx p(\bm{\pi}\mid\mathbf{x},\mathbb{D})$的定义：
    |'
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\Bigg{(}\frac{\Gamma(\alpha_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\alpha_{k^{\prime}})}\int\pi_{k}\log\pi_{k}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\bm{\pi}\Bigg{)}$
    |  | (52) |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\Bigg{(}\frac{\Gamma(\alpha_{0})}{\prod_{k^{\prime}=1}^{K}\Gamma(\alpha_{k^{\prime}})}\int\pi_{k}\log\pi_{k}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\bm{\pi}\Bigg{)}$
    |  | (52) |'
- en: '| Singling out the factor $\pi_{k}$: |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| 单独考虑因子 $\pi_{k}$: |'
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\Bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{k})\prod_{k^{\prime}\neq
    k}\Gamma(\alpha_{k^{\prime}})}\pi_{k}^{\alpha_{k}-1}\int\pi_{k}\log\pi_{k}\prod_{k^{\prime}\neq
    k}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\bm{\pi}\Bigg{)}$ |  | (53) |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\Bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{k})\prod_{k^{\prime}\neq
    k}\Gamma(\alpha_{k^{\prime}})}\pi_{k}^{\alpha_{k}-1}\int\pi_{k}\log\pi_{k}\prod_{k^{\prime}\neq
    k}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\bm{\pi}\Bigg{)}$ |  | (53) |'
- en: '| Adjusting the normalizing constant (this is the same trick used in [Section C.1](#A3.SS1
    "C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")): |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 调整归一化常数（这与 [第 C.1 节](#A3.SS1 "C.1 期望的 Dirichlet ‣ 附录 C 基本推导 附录 ‣ 先验和后验网络：对不确定性估计的证据深度学习方法的调查")
    中使用的技巧相同）： |'
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\Bigg{(}\frac{\alpha_{k}}{\alpha_{0}}\int\frac{\Gamma(\alpha_{0}+1)}{\Gamma(\alpha_{k}+1)\prod_{k^{\prime}\neq
    k}\Gamma(\alpha_{k^{\prime}})}\pi_{k}^{\alpha_{k}-1}\log\pi_{k}\prod_{k^{\prime}\neq
    k}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\bm{\pi}\Bigg{)}$ |  | (54) |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\Bigg{(}\frac{\alpha_{k}}{\alpha_{0}}\int\frac{\Gamma(\alpha_{0}+1)}{\Gamma(\alpha_{k}+1)\prod_{k^{\prime}\neq
    k}\Gamma(\alpha_{k^{\prime}})}\pi_{k}^{\alpha_{k}-1}\log\pi_{k}\prod_{k^{\prime}\neq
    k}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}d\bm{\pi}\Bigg{)}$ |  | (54) |'
- en: '| Using the identity $\mathbb{E}_{#1}[\log\pi_{k}]=\psi(\alpha_{k})-\psi(\alpha_{0})$
    ([Equation 36](#A3.E36 "In C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental
    Derivations Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation")). Since the expectation here is
    w.r.t to a Dirichlet with concentration parameters $\alpha_{k}+1$, we obtain |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: 使用恒等式 $\mathbb{E}_{#1}[\log\pi_{k}]=\psi(\alpha_{k})-\psi(\alpha_{0})$ ([公式 36](#A3.E36
    "在 C.1 期望的 Dirichlet ‣ 附录 C 基本推导 附录 ‣ 先验和后验网络：对不确定性估计的证据深度学习方法的调查"))。由于这里的期望是相对于具有集中参数
    $\alpha_{k}+1$ 的 Dirichlet，我们得到 |
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\psi(\alpha_{k}+1)-\psi(\alpha_{0}+1)\bigg{)}$
    |  | (55) |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\psi(\alpha_{k}+1)-\psi(\alpha_{0}+1)\bigg{)}$
    |  | (55) |'
- en: D.2 Derivation of Mutual Information
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 互信息的推导
- en: 'We start from the expression in [Equation 13](#S3.E13 "In Distributional uncertainty
    ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential Deep Learning
    for Classification ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"):'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 [公式 13](#S3.E13 "在分布不确定性 ‣ 3.3 使用 Dirichlet 网络的不确定性估计 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：对不确定性估计的证据深度学习方法的调查")
    中的表达式开始：
- en: '|  | $\displaystyle I\Big{[}y,\bm{\pi}\Big{&#124;}\mathbf{x},\mathbb{D}\Big{]}$
    | $\displaystyle=H\bigg{[}\mathbb{E}_{#1}\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}-\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}$
    |  | (56) |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I\Big{[}y,\bm{\pi}\Big{&#124;}\mathbf{x},\mathbb{D}\Big{]}$
    | $\displaystyle=H\bigg{[}\mathbb{E}_{#1}\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}-\mathbb{E}_{#1}\bigg{[}H\Big{[}P(y&#124;\bm{\pi})\Big{]}\bigg{]}$
    |  | (56) |'
- en: '| Given that $\mathbb{E}_{#1}[\pi_{k}]=\frac{\alpha_{k}}{\alpha_{0}}$ ([Section C.1](#A3.SS1
    "C.1 Expectation of a Dirichlet ‣ Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation")) and assuming that point estimate $p(\bm{\pi}&#124;\mathbf{x},\mathbb{D})\approx
    p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})$ is sufficient (Malinin & Gales,
    [2018](#bib.bib115)), we can identify the first term as the Shannon entropy $-\sum_{k=1}^{K}\pi_{k}\log\pi_{k}=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\log\frac{\alpha_{k}}{\alpha_{0}}$.
    Furthermore, the second part we already derived in [Section D.1](#A4.SS1 "D.1
    Derivation of Expected Entropy ‣ Appendix D Additional Derivations Appendix ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") and thus we obtain: |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| 由于 $\mathbb{E}_{#1}[\pi_{k}]=\frac{\alpha_{k}}{\alpha_{0}}$ ([第 C.1 节](#A3.SS1
    "C.1 Expectation of a Dirichlet ‣ 附录 C 基本推导附录 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述"))，并且假设点估计
    $p(\bm{\pi}&#124;\mathbf{x},\mathbb{D})\approx p(\bm{\pi}&#124;\mathbf{x},\hat{\bm{\theta}})$
    是足够的（Malinin & Gales, [2018](#bib.bib115)），我们可以将第一个项识别为 Shannon 熵 $-\sum_{k=1}^{K}\pi_{k}\log\pi_{k}=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\log\frac{\alpha_{k}}{\alpha_{0}}$。此外，第二部分我们已在
    [第 D.1 节](#A4.SS1 "D.1 Derivation of Expected Entropy ‣ 附录 D 额外推导附录 ‣ 先验和后验网络：不确定性估计的证据深度学习方法综述")
    中推导过，因此我们得到：'
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\log\frac{\alpha_{k}}{\alpha_{0}}+\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\psi(\alpha_{k}+1)-\psi(\alpha_{0}+1)\bigg{)}$
    |  | (57) |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\log\frac{\alpha_{k}}{\alpha_{0}}+\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\psi(\alpha_{k}+1)-\psi(\alpha_{0}+1)\bigg{)}$
    |  | (57) |'
- en: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\log\frac{\alpha_{k}}{\alpha_{0}}-\psi(\alpha_{k}+1)+\psi(\alpha_{0}+1)\bigg{)}$
    |  | (58) |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\sum_{k=1}^{K}\frac{\alpha_{k}}{\alpha_{0}}\bigg{(}\log\frac{\alpha_{k}}{\alpha_{0}}-\psi(\alpha_{k}+1)+\psi(\alpha_{0}+1)\bigg{)}$
    |  | (58) |'
- en: D.3 $l_{\infty}$ Norm Derivation
  id: totrans-526
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 $l_{\infty}$ 范数推导
- en: In this section we elaborate on the derivation of Tsiligkaridis ([2019](#bib.bib159))
    deriving a generalized $l_{p}$ loss, upper-bounding the $l_{\infty}$ loss. This
    in turn allows us to easily derive the $l_{2}$ loss used by Sensoy et al. ([2018](#bib.bib146));
    Zhao et al. ([2020](#bib.bib189)). Here we assume the classification target $y$
    is provided in the form of a one-hot encoded label $\mathbf{y}=[\mathbf{1}_{y=1},\ldots,\mathbf{1}_{y=K}]^{T}$.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们详细阐述 Tsiligkaridis ([2019](#bib.bib159)) 推导的广义 $l_{p}$ 损失，进而上界 $l_{\infty}$
    损失。这使得我们可以轻松推导 Sensoy 等人 ([2018](#bib.bib146)) 和 Zhao 等人 ([2020](#bib.bib189))
    使用的 $l_{2}$ 损失。这里我们假设分类目标 $y$ 以 one-hot 编码标签 $\mathbf{y}=[\mathbf{1}_{y=1},\ldots,\mathbf{1}_{y=K}]^{T}$
    的形式提供。
- en: '|  | $\displaystyle\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{\infty}\big{]}$
    | $\displaystyle\leq\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{p}\big{]}$
    |  | (59) |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{\infty}\big{]}$
    | $\displaystyle\leq\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{p}\big{]}$
    |  | (59) |'
- en: '| Using Jensen’s inequality |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Jensen 不等式'
- en: '|  |  | $\displaystyle\leq\Big{(}\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{p}^{p}\big{]}\Big{)}^{1/p}$
    |  | (60) |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\Big{(}\mathbb{E}_{#1}\big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{p}^{p}\big{]}\Big{)}^{1/p}$
    |  | (60) |'
- en: '| Evaluating the expression with $\forall k\neq y:\mathbf{y}_{k}=0$: |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| 在 $\forall k\neq y:\mathbf{y}_{k}=0$ 的情况下评估该表达式：'
- en: '|  |  | $\displaystyle=\Big{(}\mathbb{E}_{#1}[(1-\pi_{y})^{p}]+\sum_{k\neq
    y}\mathbb{E}_{#1}[\pi_{k}^{p}]\Big{)}^{1/p}$ |  | (61) |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\Big{(}\mathbb{E}_{#1}[(1-\pi_{y})^{p}]+\sum_{k\neq
    y}\mathbb{E}_{#1}[\pi_{k}^{p}]\Big{)}^{1/p}$ |  | (61) |'
- en: 'In order to compute the expression above, we first realize that all components
    of $\pi$ are distributed according to a Beta distribution $\text{Beta}(\alpha,\beta)$
    (since the Dirichlet is a multivariate generalization of the beta distribution)
    for which the moment-generating function is given as follows:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算上述表达式，我们首先要意识到，$\pi$ 的所有分量都按照 Beta 分布 $\text{Beta}(\alpha,\beta)$ 分布（因为
    Dirichlet 是 Beta 分布的多变量推广），其矩生成函数如下所示：
- en: '|  | $\mathbb{E}_{#1}[\pi^{p}]=\frac{\Gamma(\alpha+p)\Gamma(\beta)\Gamma(\alpha+\beta)}{\Gamma(\alpha+p+\beta)\Gamma(\alpha)\Gamma(\beta)}=\frac{\Gamma(\alpha+p)\Gamma(\alpha+\beta)}{\Gamma(\alpha+p+\beta)\Gamma(\alpha)}$
    |  | (62) |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{#1}[\pi^{p}]=\frac{\Gamma(\alpha+p)\Gamma(\beta)\Gamma(\alpha+\beta)}{\Gamma(\alpha+p+\beta)\Gamma(\alpha)\Gamma(\beta)}=\frac{\Gamma(\alpha+p)\Gamma(\alpha+\beta)}{\Gamma(\alpha+p+\beta)\Gamma(\alpha)}$
    |  | (62) |'
- en: 'Given that the first term in [Equation 59](#A4.E59 "In D.3 𝑙_∞ Norm Derivation
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation") is characterized
    by $\text{Beta}(\alpha_{0}-\alpha_{y},\alpha_{y})$ and the second one by $\text{Beta}(\alpha_{k},\alpha_{0}-\alpha_{k})$,
    we can evaluate the result in [Equation 59](#A4.E59 "In D.3 𝑙_∞ Norm Derivation
    ‣ Appendix D Additional Derivations Appendix ‣ Prior and Posterior Networks: A
    Survey on Evidential Deep Learning Methods For Uncertainty Estimation") using
    the moment generating function:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于[方程59](#A4.E59 "在D.3 𝑙_∞ 范数推导 ‣ 附录D 附加推导 附录 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的综述")中的第一个项由$\text{Beta}(\alpha_{0}-\alpha_{y},\alpha_{y})$特征化，第二个项由$\text{Beta}(\alpha_{k},\alpha_{0}-\alpha_{k})$特征化，我们可以使用矩生成函数评估[方程59](#A4.E59
    "在D.3 𝑙_∞ 范数推导 ‣ 附录D 附加推导 附录 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的综述")中的结果。
- en: '|  | $\displaystyle\mathbb{E}_{#1}\Big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{\infty}\Big{]}$
    | $\displaystyle\leq\Bigg{(}\frac{\Gamma(\alpha_{0}-\alpha_{y}+p)\Gamma(\alpha_{0}-\cancel{\alpha_{y}}+\cancel{\alpha_{y}})}{\Gamma(\alpha_{0}-\cancel{\alpha_{y}}+p+\cancel{\alpha_{y})}\Gamma(\alpha_{0}-\alpha_{y})}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)\Gamma(\cancel{\alpha_{k}}+\alpha_{0}-\cancel{\alpha_{k}})}{\Gamma(\cancel{\alpha_{k}}+p+\alpha_{0}-\cancel{\alpha_{k}})\Gamma(\alpha_{k})}\Bigg{)}^{\frac{1}{p}}$
    |  | (63) |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{#1}\Big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{\infty}\Big{]}$
    | $\displaystyle\leq\Bigg{(}\frac{\Gamma(\alpha_{0}-\alpha_{y}+p)\Gamma(\alpha_{0}-\cancel{\alpha_{y}}+\cancel{\alpha_{y}})}{\Gamma(\alpha_{0}-\cancel{\alpha_{y}}+p+\cancel{\alpha_{y})}\Gamma(\alpha_{0}-\alpha_{y})}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)\Gamma(\cancel{\alpha_{k}}+\alpha_{0}-\cancel{\alpha_{k}})}{\Gamma(\cancel{\alpha_{k}}+p+\alpha_{0}-\cancel{\alpha_{k}})\Gamma(\alpha_{k})}\Bigg{)}^{\frac{1}{p}}$
    |  | (63) |'
- en: '|  |  | $\displaystyle=\Bigg{(}\frac{\Gamma(\alpha_{0}-\alpha_{y}+p)\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)\Gamma(\alpha_{0}-\alpha_{y})}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)\Gamma(\alpha_{0})}{\Gamma(p+\alpha_{0})\Gamma(\alpha_{k})}\Bigg{)}^{\frac{1}{p}}$
    |  | (64) |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\Bigg{(}\frac{\Gamma(\alpha_{0}-\alpha_{y}+p)\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)\Gamma(\alpha_{0}-\alpha_{y})}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)\Gamma(\alpha_{0})}{\Gamma(p+\alpha_{0})\Gamma(\alpha_{k})}\Bigg{)}^{\frac{1}{p}}$
    |  | (64) |'
- en: '| Factoring out common terms: |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 提取公共项: |'
- en: '|  |  | $\displaystyle=\bBigg@{4}(\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bBigg@{4}(\frac{\Gamma(\alpha_{0}-\alpha_{y}+p)}{\Gamma(\alpha_{0}-\alpha_{y})}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})\bBigg@{4})^{\frac{1}{p}}$
    |  | (65) |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\bBigg@{4}(\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bBigg@{4}(\frac{\Gamma(\alpha_{0}-\alpha_{y}+p)}{\Gamma(\alpha_{0}-\alpha_{y})}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})\bBigg@{4})^{\frac{1}{p}}$
    |  | (65) |'
- en: '| Expressing $\alpha_{0}-\alpha_{k}=\sum_{k\neq y}\alpha_{k}$: |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 表示 $\alpha_{0}-\alpha_{k}=\sum_{k\neq y}\alpha_{k}$: |'
- en: '|  |  | $\displaystyle=\bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bigg{)}^{\frac{1}{p}}\bBigg@{4}(\frac{\Gamma\Big{(}\sum_{k\neq
    y}\alpha_{k}+p\Big{)}}{\Gamma\Big{(}\sum_{k\neq y}\alpha_{k}\Big{)}}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})^{\frac{1}{p}}$ |  |
    (66) |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bigg{)}^{\frac{1}{p}}\bBigg@{4}(\frac{\Gamma\Big{(}\sum_{k\neq
    y}\alpha_{k}+p\Big{)}}{\Gamma\Big{(}\sum_{k\neq y}\alpha_{k}\Big{)}}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})^{\frac{1}{p}}$ |  |
    (66) |'
- en: D.4 $l_{2}$ Norm Loss Derivation
  id: totrans-542
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.4 $l_{2}$ 范数损失推导
- en: Here we present an adapted derivation by Sensoy et al. ([2018](#bib.bib146))
    for the $l_{2}$-norm loss to train Dirichlet networks. Here we again use a one-hot
    vector for a label with $\mathbf{y}=[\mathbf{1}_{y=1},\ldots,\mathbf{1}_{y=K}]^{T}$.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们展示了Sensoy等人（[2018](#bib.bib146)）为训练Dirichlet网络而调整的$l_{2}$-范数损失推导。这里我们再次使用一-hot向量表示标签，$\mathbf{y}=[\mathbf{1}_{y=1},\ldots,\mathbf{1}_{y=K}]^{T}$。
- en: '|  | $\displaystyle\mathbb{E}_{#1}\Big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{2}^{2}\Big{]}$
    | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}(\mathbf{1}_{y=k}-\pi_{k})^{2}\bigg{]}$
    |  | (67) |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{#1}\Big{[}&#124;&#124;\mathbf{y}-\bm{\pi}&#124;&#124;_{2}^{2}\Big{]}$
    | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}(\mathbf{1}_{y=k}-\pi_{k})^{2}\bigg{]}$
    |  | (67) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}\mathbf{1}_{y=k}^{2}-2\pi_{k}\mathbf{1}_{y=k}+\pi_{k}^{2}\bigg{]}$
    |  | (68) |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}\mathbf{1}_{y=k}^{2}-2\pi_{k}\mathbf{1}_{y=k}+\pi_{k}^{2}\bigg{]}$
    |  | (68) |'
- en: '|  |  | $\displaystyle=\sum_{k=1}^{K}\mathbf{1}_{y=k}^{2}-2\mathbb{E}_{#1}[\pi_{k}]\mathbf{1}_{y=k}+\mathbb{E}_{#1}[\pi_{k}^{2}]$
    |  | (69) |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{k=1}^{K}\mathbf{1}_{y=k}^{2}-2\mathbb{E}_{#1}[\pi_{k}]\mathbf{1}_{y=k}+\mathbb{E}_{#1}[\pi_{k}^{2}]$
    |  | (69) |'
- en: '| Using the identity that $\mathbb{E}_{#1}[\pi_{k}^{2}]=\mathbb{E}_{#1}[\pi_{k}]^{2}+\text{Var}(\pi_{k})$:
    |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| 使用恒等式 $\mathbb{E}_{#1}[\pi_{k}^{2}]=\mathbb{E}_{#1}[\pi_{k}]^{2}+\text{Var}(\pi_{k})$:
    |'
- en: '|  |  | $\displaystyle=\sum_{k=1}^{K}\mathbf{1}_{y=k}^{2}-2\mathbb{E}_{#1}[\pi_{k}]\mathbf{1}_{y=k}+\mathbb{E}_{#1}[\pi_{k}]^{2}+\text{Var}(\pi_{k})$
    |  | (70) |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{k=1}^{K}\mathbf{1}_{y=k}^{2}-2\mathbb{E}_{#1}[\pi_{k}]\mathbf{1}_{y=k}+\mathbb{E}_{#1}[\pi_{k}]^{2}+\text{Var}(\pi_{k})$
    |  | (70) |'
- en: '|  |  | $\displaystyle=\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\mathbb{E}_{#1}[\pi_{k}]\Big{)}^{2}+\text{Var}(\pi_{k})$
    |  | (71) |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\mathbb{E}_{#1}[\pi_{k}]\Big{)}^{2}+\text{Var}(\pi_{k})$
    |  | (71) |'
- en: '| Finally, we use the result from [Section C.1](#A3.SS1 "C.1 Expectation of
    a Dirichlet ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    and the result that $\displaystyle\text{Var}(\pi_{k})=\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    (see [Lin](#bib.bib107), [2016](#bib.bib107)): |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 最后，我们使用了[第C.1节](#A3.SS1 "C.1 Dirichlet的期望值 ‣ 附录C 基本推导 附录 ‣ 先验和后验网络：一种用于不确定性估计的证据深度学习方法综述")中的结果和$\displaystyle\text{Var}(\pi_{k})=\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$的结果（见[Lin](#bib.bib107)，[2016](#bib.bib107)）：
    |'
- en: '|  |  | $\displaystyle=\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\alpha_{k}}{\alpha_{0}}\Big{)}^{2}+\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    |  | (72) |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\alpha_{k}}{\alpha_{0}}\Big{)}^{2}+\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    |  | (72) |'
- en: D.5 Derivation of Reverse KL loss
  id: totrans-552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5 反向KL损失的推导
- en: Here we re-state and annotate the derivation of reverse KL loss by Malinin &
    Gales ([2019](#bib.bib116)) in more detail, starting from the forward KL loss
    by Malinin & Gales ([2018](#bib.bib115)). Note that here, $\hat{\bm{\alpha}}$
    contains a dependence on $k$, since Malinin & Gales ([2018](#bib.bib115)) let
    $\hat{\alpha}_{k}=\hat{\pi_{k}}\hat{\alpha}_{0}$ with $\hat{\alpha}_{0}$ being
    a hyperparameter and $\hat{\pi}_{k}=\mathbf{1}_{k=y}+(-\mathbf{1}_{k=y}K+1)\varepsilon$
    and $\varepsilon$ being a small number.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们重新陈述并注释了Malinin & Gales ([2019](#bib.bib116))的反向KL损失的推导，从Malinin & Gales
    ([2018](#bib.bib115))的前向KL损失开始。注意，这里$\hat{\bm{\alpha}}$包含了对$k$的依赖，因为Malinin &
    Gales ([2018](#bib.bib115))设置$\hat{\alpha}_{k}=\hat{\pi_{k}}\hat{\alpha}_{0}$，其中$\hat{\alpha}_{0}$是一个超参数，而$\hat{\pi}_{k}=\mathbf{1}_{k=y}+(-\mathbf{1}_{k=y}K+1)\varepsilon$，$\varepsilon$是一个小数。
- en: '|  |  | $\displaystyle\quad\ \mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}\mathbf{1}_{y=k}\text{KL}\Big{[}p(\bm{\pi}&#124;\hat{\bm{\alpha}})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\Big{]}\bigg{]}$
    |  | (73) |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad\ \mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}\mathbf{1}_{y=k}\text{KL}\Big{[}p(\bm{\pi}\mid\hat{\bm{\alpha}})\Big{\,\mid\,}p(\bm{\pi}\mid\mathbf{x},\bm{\theta})\Big{]}\bigg{]}$
    |  | (73) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}d\bm{\pi}\bigg{]}$
    |  | (74) |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}\mid\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}\mid\hat{\bm{\alpha}})}{p(\bm{\pi}\mid\mathbf{x},\bm{\theta})}d\bm{\pi}\bigg{]}$
    |  | (74) |'
- en: '| Writing the expectation explicitly: |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| 显式写出期望值： |'
- en: '|  |  | $\displaystyle=\int\sum_{k=1}^{K}p(y=k,\mathbf{x})\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}d\bm{\pi}d\mathbf{x}$
    |  | (75) |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\int\sum_{k=1}^{K}p(y=k,\mathbf{x})\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}\mid\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}\mid\hat{\bm{\alpha}})}{p(\bm{\pi}\mid\mathbf{x},\bm{\theta})}d\bm{\pi}d\mathbf{x}$
    |  | (75) |'
- en: '|  |  | $\displaystyle=\int\sum_{k=1}^{K}p(\mathbf{x})P(y=k&#124;\mathbf{x})\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}d\bm{\pi}d\mathbf{x}$
    |  | (76) |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\int\sum_{k=1}^{K}p(\mathbf{x})P(y=k\mid\mathbf{x})\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}\mid\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}\mid\hat{\bm{\alpha}})}{p(\bm{\pi}\mid\mathbf{x},\bm{\theta})}d\bm{\pi}d\mathbf{x}$
    |  | (76) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}d\bm{\pi}\Bigg{]}$
    |  | (77) |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\sum_{k=1}^{K}P(y=k\mid\mathbf{x})\sum_{k=1}^{K}\mathbf{1}_{y=k}\int
    p(\bm{\pi}\mid\hat{\bm{\alpha}})\log\frac{p(\bm{\pi}\mid\hat{\bm{\alpha}})}{p(\bm{\pi}\mid\mathbf{x},\bm{\theta})}d\bm{\pi}\Bigg{]}$
    |  | (77) |'
- en: '| Adding factor in log, collapsing double sum: |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| 在对数中添加因子，合并双重求和： |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\Bigg{(}\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})}\Bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (78) |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\int
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\log\Bigg{(}\frac{p(\bm{\pi}&#124;\hat{\bm{\alpha}})\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})}\Bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (78) |'
- en: '| Reordering, separating constant factor from log: |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| 重新排序，将常量因子与对数分开：|'
- en: '|  |  | $\displaystyle=\mathbb{E}_{p(\mathbf{x})}\Bigg{[}\int\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})p(\bm{\pi}&#124;\hat{\bm{\alpha}})\Bigg{(}\log\bigg{(}\frac{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}\bigg{)}$
    |  | (79) |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{p(\mathbf{x})}\Bigg{[}\int\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})p(\bm{\pi}&#124;\hat{\bm{\alpha}})\Bigg{(}\log\bigg{(}\frac{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})p(\bm{\pi}&#124;\hat{\bm{\alpha}})}{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}\bigg{)}$
    |  | (79) |'
- en: '|  |  | $\displaystyle-\underbrace{\log\Big{(}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\Big{)}}_{=0}\Bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (80) |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\underbrace{\log\Big{(}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\Big{)}}_{=0}\Bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (80) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\text{KL}\bigg{[}\underbrace{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})p(\bm{\pi}&#124;\hat{\alpha})}_{\text{Mixture
    of }K\text{ Dirichlets}}\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{]}\Bigg{]}$
    |  | (81) |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\text{KL}\bigg{[}\underbrace{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})p(\bm{\pi}&#124;\hat{\alpha})}_{\text{K个Dirichlets的混合}}\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{]}\Bigg{]}$
    |  | (81) |'
- en: 'where we can see that this objective actually tries to minimizes the divergence
    towards a mixture of $K$ Dirichlet distributions. In the case of high data uncertainty,
    this is claimed incentivize the model to distribute mass around each of the corners
    of the simplex, instead of the desired behavior shown in [Figure 4(c)](#S3.F4.sf3
    "In Figure 4 ‣ 3.3 Uncertainty Estimation with Dirichlet Networks ‣ 3 Evidential
    Deep Learning for Classification ‣ Prior and Posterior Networks: A Survey on Evidential
    Deep Learning Methods For Uncertainty Estimation"). Therefore, Malinin & Gales
    ([2019](#bib.bib116)) propose to swap the order of arguments in the KL-divergence,
    resulting in the following:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这个目标实际上试图最小化相对于$K$个Dirichlet分布的混合的散度。在数据不确定性高的情况下，这被声称会促使模型将质量分布在单纯形的每个角落，而不是如[图4(c)](#S3.F4.sf3
    "在图4 ‣ 3.3 使用Dirichlet网络的不确定性估计 ‣ 3 证据深度学习用于分类 ‣ 先验和后验网络：关于不确定性估计方法的证据深度学习方法的调查")中所示的期望行为。因此，Malinin
    & Gales ([2019](#bib.bib116)) 提议交换KL散度中参数的顺序，得到如下结果：
- en: '|  |  | $\displaystyle\quad\ \mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\cdot\text{KL}\Big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\hat{\bm{\alpha}})\Big{]}\bigg{]}$
    |  | (82) |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad\ \mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\cdot\text{KL}\Big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\Big{&#124;}\Big{&#124;}p(\bm{\pi}&#124;\hat{\bm{\alpha}})\Big{]}\bigg{]}$
    |  | (82) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\cdot\int
    p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}d\bm{\pi}\bigg{]}$
    |  | (83) |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\cdot\int
    p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}d\bm{\pi}\bigg{]}$
    |  | (83) |'
- en: '| Reordering: |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| 重新排序：|'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}d\bm{\pi}\bigg{]}$
    |  | (84) |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi}&#124;\hat{\bm{\alpha}})}d\bm{\pi}\bigg{]}$
    |  | (84) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\log
    p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})-\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\log
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\bigg{]}\bigg{]}$ |  | (85) |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\mathbb{E}_{#1}\bigg{[}\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\log
    p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})-\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\log
    p(\bm{\pi}&#124;\hat{\bm{\alpha}})\bigg{]}\bigg{]}$ |  | (85) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\bigg{(}\prod_{k=1}^{K}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})^{P(y=k&#124;\mathbf{x})}\bigg{)}-\log\bigg{(}\prod_{k=1}^{K}p(\bm{\pi}&#124;\hat{\bm{\alpha}})^{P(y=k&#124;\mathbf{x})}\bigg{)}\bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (86) |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\bigg{(}\prod_{k=1}^{K}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})^{P(y=k&#124;\mathbf{x})}\bigg{)}-\log\bigg{(}\prod_{k=1}^{K}p(\bm{\pi}&#124;\hat{\bm{\alpha}})^{P(y=k&#124;\mathbf{x})}\bigg{)}\bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (86) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{p(\mathbf{x})}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\bigg{(}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})^{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})}\bigg{)}-\log\bigg{(}\prod_{k=1}^{K}\Big{(}\frac{1}{B(\bm{\alpha})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}\Big{)}^{p(y=k&#124;\mathbf{x})}\bigg{)}\bigg{)}d\bm{\pi}\bigg{]}$
    |  | (87) |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{p(\mathbf{x})}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\bigg{(}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})^{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})}\bigg{)}-\log\bigg{(}\prod_{k=1}^{K}\Big{(}\frac{1}{B(\bm{\alpha})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}\Big{)}^{p(y=k&#124;\mathbf{x})}\bigg{)}\bigg{)}d\bm{\pi}\bigg{]}$
    |  | (87) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\big{(}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{)}-\log\bigg{(}\prod_{k=1}^{K}\Big{(}\frac{1}{B(\bm{\alpha})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}\Big{)}^{P(y=k&#124;\mathbf{x})}\bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (88) |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\big{(}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{)}-\log\bigg{(}\prod_{k=1}^{K}\Big{(}\frac{1}{B(\bm{\alpha})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\alpha_{k^{\prime}}-1}\Big{)}^{P(y=k&#124;\mathbf{x})}\bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (88) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\big{(}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{)}-\log\bigg{(}\frac{1}{B(\bm{\alpha})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\alpha_{k^{\prime}}-1}\bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (89) |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\int p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\bigg{(}\log\big{(}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{)}-\log\bigg{(}\frac{1}{B(\bm{\alpha})}\prod_{k^{\prime}=1}^{K}\pi_{k^{\prime}}^{\sum_{k=1}^{K}P(y=k&#124;\mathbf{x})\alpha_{k^{\prime}}-1}\bigg{)}d\bm{\pi}\Bigg{]}$
    |  | (89) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\text{KL}\Big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})&#124;&#124;p(\bm{\pi}&#124;\bar{\bm{\alpha}})\Big{]}\Bigg{]}\quad\text{where}\quad\bar{\bm{\alpha}}=\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\alpha_{k^{\prime}}$
    |  | (90) |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\Bigg{[}\text{KL}\Big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})&#124;&#124;p(\bm{\pi}&#124;\bar{\bm{\alpha}})\Big{]}\Bigg{]}\quad\text{where}\quad\bar{\bm{\alpha}}=\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\alpha_{k^{\prime}}$
    |  | (90) |'
- en: Therefore, instead of a mixture of Dirichlet distribution, we obtain a single
    distribution whose *parameters are a mixture* of the concentrations of each class.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到一个单一的分布，其*参数是一个混合体*，而不是 Dirichlet 分布的混合体。
- en: D.6 Uncertainty-aware Cross-Entropy Loss
  id: totrans-578
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6 关注不确定性的交叉熵损失
- en: The uncertainty-aware cross-entropy loss in Biloš et al. ([2019](#bib.bib11));
    Charpentier et al. ([2020](#bib.bib16)) has the form
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: Biloš 等人 ([2019](#bib.bib11)); Charpentier 等人 ([2020](#bib.bib16)) 提出的关注不确定性的交叉熵损失形式为
- en: '|  | $\mathcal{L}_{\text{UCE}}=\mathbb{E}_{#1}[\log p(y&#124;\bm{\pi})]=\mathbb{E}_{#1}[\log\pi_{y}]=\psi(\alpha_{y})-\psi(\alpha_{0})$
    |  | (91) |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{UCE}}=\mathbb{E}_{#1}[\log p(y&#124;\bm{\pi})]=\mathbb{E}_{#1}[\log\pi_{y}]=\psi(\alpha_{y})-\psi(\alpha_{0})$
    |  | (91) |'
- en: 'as $p(y|\bm{\pi})$ is given by the true label in form of a delta distribution,
    we can apply the result from [Section C.1](#A3.SS1 "C.1 Expectation of a Dirichlet
    ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior Networks:
    A Survey on Evidential Deep Learning Methods For Uncertainty Estimation").'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 $p(y|\bm{\pi})$ 由形式为 delta 分布的真实标签给出，我们可以应用 [第 C.1 节](#A3.SS1 "C.1 Expectation
    of a Dirichlet ‣ Appendix C Fundamental Derivations Appendix ‣ Prior and Posterior
    Networks: A Survey on Evidential Deep Learning Methods For Uncertainty Estimation")
    中的结果。'
- en: D.7 Evidence-Lower Bound For Dirichlet Posterior Estimation
  id: totrans-582
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.7 Dirichlet 后验估计的证据下界
- en: The evidence lower bound is a well-known objective to optimize the KL-divergence
    between an approximate proposal and target distribution (Jordan et al., [1999](#bib.bib83);
    Kingma & Welling, [2014](#bib.bib88)). We derive it based on Chen et al. ([2018](#bib.bib18))
    in the following for the Dirichlet case with a proposal distribution $p(\bm{\pi}|\mathbf{x},\bm{\theta})$
    to the target distribution $p(\bm{\pi}|y)$. For the first part of the derivation,
    we omit the dependence on $\bm{\beta}$ for clarity.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 证据下界是一个众所周知的优化目标，用于优化近似提案和目标分布之间的 KL 散度 (Jordan et al., [1999](#bib.bib83);
    Kingma & Welling, [2014](#bib.bib88))。我们基于 Chen et al. ([2018](#bib.bib18)) 的结果，针对狄利克雷分布情况，使用提案分布
    $p(\bm{\pi}|\mathbf{x},\bm{\theta})$ 对目标分布 $p(\bm{\pi}|y)$。对于推导的第一部分，我们为了清晰起见省略了对
    $\bm{\beta}$ 的依赖。
- en: '|  | $\displaystyle\text{KL}\big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{&#124;}\big{&#124;}p(\bm{\pi}&#124;y)\big{]}$
    | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi}&#124;y)}\bigg{]}=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})p(y)}{p(\bm{\pi},y)}\bigg{]}$
    |  | (92) |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{KL}\big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{&#124;}\big{&#124;}p(\bm{\pi}&#124;y)\big{]}$
    | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi}&#124;y)}\bigg{]}=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})p(y)}{p(\bm{\pi},y)}\bigg{]}$
    |  | (92) |'
- en: '| Factorizing $p(\bm{\pi},y)=P(y&#124;\bm{\pi})p(\bm{\pi})$, pulling out $p(y)$
    as it doesn’t depend on $\pi$: |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| 因式分解 $p(\bm{\pi},y)=P(y&#124;\bm{\pi})p(\bm{\pi})$，提取出 $p(y)$，因为它与 $\pi$
    无关： |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{P(y&#124;\bm{\pi})p(\bm{\pi})}\bigg{]}+p(y)$
    |  | (93) |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{P(y&#124;\bm{\pi})p(\bm{\pi})}\bigg{]}+p(y)$
    |  | (93) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi})}\bigg{]}-\mathbb{E}_{#1}\big{[}\log
    P(y&#124;\bm{\pi})\big{]}+p(y)$ |  | (94) |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{#1}\bigg{[}\log\frac{p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})}{p(\bm{\pi})}\bigg{]}-\mathbb{E}_{#1}\big{[}\log
    P(y&#124;\bm{\pi})\big{]}+p(y)$ |  | (94) |'
- en: '|  |  | $\displaystyle\leq\text{KL}\big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{&#124;}\big{&#124;}p(\bm{\pi})\big{]}-\mathbb{E}_{#1}\big{[}\log
    P(y&#124;\bm{\pi})\big{]}$ |  | (95) |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\text{KL}\big{[}p(\bm{\pi}&#124;\mathbf{x},\bm{\theta})\big{&#124;}\big{&#124;}p(\bm{\pi})\big{]}-\mathbb{E}_{#1}\big{[}\log
    P(y&#124;\bm{\pi})\big{]}$ |  | (95) |'
- en: 'Now note that the second part of the result is the uncertainty-aware cross-entropy
    loss from [Section D.6](#A4.SS6 "D.6 Uncertainty-aware Cross-Entropy Loss ‣ Appendix
    D Additional Derivations Appendix ‣ Prior and Posterior Networks: A Survey on
    Evidential Deep Learning Methods For Uncertainty Estimation") and re-adding the
    dependence of $p(\pi)$ on $\bm{\gamma}$, we can re-use our result regarding the
    KL-divergence between two Dirichlets in [Section C.3](#A3.SS3 "C.3 Kullback-Leibler
    Divergence between two Dirichlets ‣ Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") and thus obtain:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 现在注意到结果的第二部分是来自[第 D.6 节](#A4.SS6 "D.6 不确定性感知交叉熵损失 ‣ 附录 D 附加推导 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法综述")的不确定性感知交叉熵损失，并重新加入
    $p(\pi)$ 对 $\bm{\gamma}$ 的依赖，我们可以重用在[第 C.3 节](#A3.SS3 "C.3 两个狄利克雷分布之间的 Kullback-Leibler
    散度 ‣ 附录 C 基本推导 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法综述")中关于两个狄利克雷分布之间 KL 散度的结果，从而得到：
- en: '|  | $\displaystyle\mathcal{L}_{\text{ELBO}}$ | $\displaystyle=\psi(\beta_{y})-\psi(\beta_{0})-\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}$
    |  | (96) |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{ELBO}}$ | $\displaystyle=\psi(\beta_{y})-\psi(\beta_{0})-\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}$
    |  | (96) |'
- en: which is exactly the solution obtained by both Chen et al. ([2018](#bib.bib18))
    and Joo et al. ([2020](#bib.bib82)).
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是 Chen et al. ([2018](#bib.bib18)) 和 Joo et al. ([2020](#bib.bib82)) 获得的解决方案。
- en: Appendix E Overview over Loss Functions Appendix
  id: totrans-592
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 关于损失函数的概述
- en: 'In [Tables 6](#A5.T6 "In Appendix E Overview over Loss Functions Appendix ‣
    Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") and [7](#A5.T7 "Table 7 ‣ Appendix E Overview over Loss
    Functions Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep
    Learning Methods For Uncertainty Estimation"), we compare the forms of the loss
    function used by Evidential Deep Learning methods for classification, using the
    consistent notation from the paper. Most of the presented results can be found
    in the previous [Appendix C](#A3 "Appendix C Fundamental Derivations Appendix
    ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning Methods For
    Uncertainty Estimation") and [Appendix D](#A4 "Appendix D Additional Derivations
    Appendix ‣ Prior and Posterior Networks: A Survey on Evidential Deep Learning
    Methods For Uncertainty Estimation"). We refer to the original work for details
    about the objective of Nandy et al. ([2020](#bib.bib133)).'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表 6](#A5.T6 "附录 E 关于损失函数的概述 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查") 和 [7](#A5.T7
    "表 7 ‣ 附录 E 关于损失函数的概述 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查") 中，我们比较了证据深度学习方法用于分类的损失函数形式，使用了论文中的一致符号。大多数呈现的结果可以在之前的
    [附录 C](#A3 "附录 C 基本推导 ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查") 和 [附录 D](#A4 "附录 D 附加推导
    ‣ 先验和后验网络：关于不确定性估计的证据深度学习方法的调查") 中找到。关于 Nandy 等人的目标，详见原始工作 ([2020](#bib.bib133))。
- en: 'Table 6: Overview over objectives used by prior networks for classification.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：先前网络用于分类的目标概述。
- en: '| Method | Loss function | Regularizer | Comment |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 损失函数 | 正则项 | 备注 |'
- en: '| Prior networks (Malinin & Gales, [2018](#bib.bib115)) | $\log\frac{B(\hat{\bm{\alpha}})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\hat{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | $-\log\frac{\Gamma(K)}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-1)(\psi(\alpha_{k})-\psi(\alpha_{0}))$
    | Target concentration parameters $\hat{\bm{\alpha}}$ are created using a label
    smoothing approach, i.e. $\hat{\pi}_{k}=\begin{cases}1-(K-1)\varepsilon&amp;\quad\text{if
    }y=k\\ \varepsilon&amp;\quad\text{if }y\neq k\end{cases}$. Together with setting
    $\hat{\alpha}_{0}$ as a hyperparameter, $\hat{\alpha_{k}}=\hat{\pi_{k}}\hat{\alpha_{0}}$
    |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| 之前的网络 (Malinin & Gales, [2018](#bib.bib115)) | $\log\frac{B(\hat{\bm{\alpha}})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\hat{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | $-\log\frac{\Gamma(K)}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-1)(\psi(\alpha_{k})-\psi(\alpha_{0}))$
    | 目标集中参数 $\hat{\bm{\alpha}}$ 是使用标签平滑方法创建的，即 $\hat{\pi}_{k}=\begin{cases}1-(K-1)\varepsilon&\quad\text{如果
    }y=k\\ \varepsilon&\quad\text{如果 }y\neq k\end{cases}$。与设置 $\hat{\alpha}_{0}$ 作为超参数一起，$\hat{\alpha_{k}}=\hat{\pi_{k}}\hat{\alpha_{0}}$
    |'
- en: '| Prior networks (Malinin & Gales, [2019](#bib.bib116)) | $\log\frac{B(\hat{\bm{\alpha}})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\hat{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | $\log\frac{B(\bar{\bm{\alpha}})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\bar{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | Similar to above, $\hat{\alpha}_{c}^{(k)}=\mathbf{1}_{c=k}\alpha_{\text{in}}+1$
    for in-distribution and $\bar{\alpha}_{c}^{(k)}=\mathbf{1}_{c=k}\alpha_{\text{out}}+1$
    where we have hyperparameters set to $\alpha_{\text{in}}=0.01$ and $\alpha_{\text{out}}=0$.
    Then finally, $\hat{\bm{\alpha}}=\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\hat{\alpha}_{k}$
    and $\bar{\bm{\alpha}}=\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\bar{\alpha}_{k}$.
    |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| 之前的网络 (Malinin & Gales, [2019](#bib.bib116)) | $\log\frac{B(\hat{\bm{\alpha}})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\hat{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | $\log\frac{B(\bar{\bm{\alpha}})}{B(\bm{\alpha})}+\sum_{k=1}^{K}(\alpha_{k}-\bar{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | 与上述类似，对于内部分布，$\hat{\alpha}_{c}^{(k)}=\mathbf{1}_{c=k}\alpha_{\text{in}}+1$ 和对于外部分布，$\bar{\alpha}_{c}^{(k)}=\mathbf{1}_{c=k}\alpha_{\text{out}}+1$，其中超参数设置为
    $\alpha_{\text{in}}=0.01$ 和 $\alpha_{\text{out}}=0$。最终，$\hat{\bm{\alpha}}=\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\hat{\alpha}_{k}$
    和 $\bar{\bm{\alpha}}=\sum_{k=1}^{K}p(y=k&#124;\mathbf{x})\bar{\alpha}_{k}$。 |'
- en: '| Information Robust Dirichlet Networks (Tsiligkaridis, [2019](#bib.bib159))
    | $\bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bigg{)}^{\frac{1}{p}}\bBigg@{4}(\frac{\Gamma\Big{(}\sum_{k\neq
    y}\alpha_{k}+p\Big{)}}{\Gamma\Big{(}\sum_{k\neq y}\alpha_{k}\Big{)}}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\bBigg@{4})^{\frac{1}{p}}$ |
    $\frac{1}{2}\sum_{k\neq y}(\alpha_{k}-1)^{2}(\psi^{(1)}(\alpha_{k})-\psi^{(1)})(\alpha_{0}))$
    | $\psi^{(1)}$ is the polygamma function defined as $\psi^{(1)}(x)=\frac{d}{dx}\psi(x)$.
    |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| 信息稳健的狄利克雷网络 (Tsiligkaridis，[2019](#bib.bib159)) | $\bigg{(}\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{0}+p)}\bigg{)}^{\frac{1}{p}}\Bigg{(}\frac{\Gamma\Big{(}\sum_{k\neq
    y}\alpha_{k}+p\Big{)}}{\Gamma\Big{(}\sum_{k\neq y}\alpha_{k}\Big{)}}+\sum_{k\neq
    y}\frac{\Gamma(\alpha_{k}+p)}{\Gamma(\alpha_{k})}\Bigg{)}^{\frac{1}{p}}$ | $\frac{1}{2}\sum_{k\neq
    y}(\alpha_{k}-1)^{2}(\psi^{(1)}(\alpha_{k})-\psi^{(1)}(\alpha_{0}))$ | $\psi^{(1)}$
    是多伽马函数，定义为 $\psi^{(1)}(x)=\frac{d}{dx}\psi(x)$。 |'
- en: '| Dirichlet via Function Decomposition (Biloš et al., [2019](#bib.bib11)) |
    $\psi(\alpha_{y})-\psi(\alpha_{0})$ | $\lambda_{1}\int_{0}^{T}\pi_{k}(\tau)^{2}d\tau+\lambda_{2}\int_{0}^{T}(\nu-\sigma^{2}(\tau))^{2}d\tau$
    | Factors $\lambda_{1}$ and $\lambda_{2}$ that are treated as hyperparameters
    that weigh first term pushing the for logit $k$ to zero, while pushing the variance
    in the first term to $\nu$. |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| 通过函数分解的狄利克雷 (Biloš 等，[2019](#bib.bib11)) | $\psi(\alpha_{y})-\psi(\alpha_{0})$
    | $\lambda_{1}\int_{0}^{T}\pi_{k}(\tau)^{2}d\tau+\lambda_{2}\int_{0}^{T}(\nu-\sigma^{2}(\tau))^{2}d\tau$
    | $\lambda_{1}$ 和 $\lambda_{2}$ 被视为超参数，前者权衡第一个项，促使对数值 $k$ 接近零，而后者将第一个项的方差推向 $\nu$。
    |'
- en: '| Prior network with PAC Reg. (Haussmann et al., [2019](#bib.bib58)) | $-\log\mathbb{E}_{#1}\bigg{[}\prod_{k=1}^{K}\bigg{(}\frac{\alpha_{k}}{\alpha_{0}}\bigg{)}^{\mathbf{1}_{k=y}}\bigg{]}$
    | $\sqrt{\frac{\text{KL}\big{[}p(\bm{\pi}&#124;\bm{\alpha})\big{&#124;}\big{&#124;}p(\bm{\pi}&#124;\mathbf{1})\big{]}-\log\delta}{N}-1}$
    | The expectation in the loss function is evaluated using parameter samples from
    a weight distribution. $\delta\in[0,1]$. |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| 带 PAC 正则化的先验网络 (Haussmann 等，[2019](#bib.bib58)) | $-\log\mathbb{E}_{#1}\bigg{[}\prod_{k=1}^{K}\bigg{(}\frac{\alpha_{k}}{\alpha_{0}}\bigg{)}^{\mathbf{1}_{k=y}}\bigg{]}$
    | $\sqrt{\frac{\text{KL}\big{[}p(\bm{\pi}\mid\bm{\alpha})\big{\,\middle|\,}\big{\,\middle|\,}p(\bm{\pi}\mid\mathbf{1})\big{]}-\log\delta}{N}-1}$
    | 损失函数中的期望值使用来自权重分布的参数样本进行评估。$\delta\in[0,1]$。 |'
- en: '| Ensemble Distribution Distillation (Malinin et al., [2020b](#bib.bib118))
    | $\psi(\alpha_{0})-\sum_{k=1}^{K}\psi(\alpha_{k})+\frac{1}{M}\sum_{m=1}^{M}\sum_{k=1}^{K}(\alpha_{k}-1)$
    $\log p(y=k&#124;\mathbf{x},\bm{\theta}^{(m)})$ | - | The objective uses predictions
    from a trained ensemble with parameters $\bm{\theta}_{1},\ldots,\bm{\theta}_{M}$.
    |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| 集成分布蒸馏 (Malinin 等，[2020b](#bib.bib118)) | $\psi(\alpha_{0})-\sum_{k=1}^{K}\psi(\alpha_{k})+\frac{1}{M}\sum_{m=1}^{M}\sum_{k=1}^{K}(\alpha_{k}-1)$
    $\log p(y=k\mid\mathbf{x},\bm{\theta}^{(m)})$ | - | 目标函数使用了来自训练集成模型的预测，模型参数为 $\bm{\theta}_{1},\ldots,\bm{\theta}_{M}$。
    |'
- en: '| Prior networks with representation gap (Nandy et al., [2020](#bib.bib133))
    | $-\log\pi_{y}-\frac{\lambda_{\text{in}}}{K}\sum_{k=1}^{K}\sigma(\alpha_{k})$
    | $-\sum_{k=1}\frac{1}{K}\log\pi_{k}-\frac{\lambda_{\text{out}}}{K}\sum_{k=1}^{K}\sigma(\alpha_{k})$
    | The main objective is being optimized on in-distribution, the regularizer on
    out-of-distribution data. $\lambda_{\text{in}}$ and $\lambda_{\text{out}}$ weighing
    terms and $\sigma$ denotes the sigmoid function. |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| 带表示差距的先验网络 (Nandy 等，[2020](#bib.bib133)) | $-\log\pi_{y}-\frac{\lambda_{\text{in}}}{K}\sum_{k=1}^{K}\sigma(\alpha_{k})$
    | $-\sum_{k=1}\frac{1}{K}\log\pi_{k}-\frac{\lambda_{\text{out}}}{K}\sum_{k=1}^{K}\sigma(\alpha_{k})$
    | 主要目标是在分布内优化，正则化项用于分布外数据。$\lambda_{\text{in}}$ 和 $\lambda_{\text{out}}$ 影响项的权重，$\sigma$
    表示 sigmoid 函数。 |'
- en: '| Prior RNN (Shen et al., [2020](#bib.bib151)) | $\sum_{k=1}\mathbf{1}_{k=y}\log\pi_{k}$
    | $-\log B(\tilde{\bm{\alpha})}+(\hat{\alpha}_{0}-K)\psi(\hat{\alpha}_{0})-\sum_{k=1}^{K}(\hat{\alpha}_{k}-1)\psi(\hat{\alpha}_{k})$
    | Here, the entropy regularizer operates on a scaled version of the concentration
    parameters $\tilde{\bm{\alpha}}=(\mathbf{I}_{K}-\mathbf{W})\bm{\alpha}$, where
    $\mathbf{W}$ is learned. |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| 先验 RNN (Shen 等，[2020](#bib.bib151)) | $\sum_{k=1}\mathbf{1}_{k=y}\log\pi_{k}$
    | $-\log B(\tilde{\bm{\alpha})}+(\hat{\alpha}_{0}-K)\psi(\hat{\alpha}_{0})-\sum_{k=1}^{K}(\hat{\alpha}_{k}-1)\psi(\hat{\alpha}_{k})$
    | 这里，熵正则化器作用于浓度参数的缩放版本 $\tilde{\bm{\alpha}}=(\mathbf{I}_{K}-\mathbf{W})\bm{\alpha}$，其中
    $\mathbf{W}$ 是学习得到的。 |'
- en: '| Graph-based Kernel Dirichlet dist. est. (GKDE) (Zhao et al., [2020](#bib.bib189))
    | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\alpha_{k}}{\alpha_{0}}\Big{)}^{2}+\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    | $-\log\frac{B(\bm{\alpha})}{B(\hat{\bm{\alpha}})}+\sum_{k=1}^{K}(\alpha_{k}-\hat{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | $\hat{\bm{\alpha}}$ here corresponds to a uniform prior including some information
    about the local graph structure. The authors also use an additional knowledge
    distillation objective, which was omitted here since it doesn’t related to the
    Dirichlet. |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| 基于图的核狄利克雷分布估计 (GKDE) (Zhao et al., [2020](#bib.bib189)) | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\alpha_{k}}{\alpha_{0}}\Big{)}^{2}+\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
    | $-\log\frac{B(\bm{\alpha})}{B(\hat{\bm{\alpha}})}+\sum_{k=1}^{K}(\alpha_{k}-\hat{\alpha}_{k})\big{(}\psi(\alpha_{k})-\psi(\alpha_{0})\big{)}$
    | $\hat{\bm{\alpha}}$ 这里对应一个包含一些关于局部图结构信息的均匀先验。作者还使用了额外的知识蒸馏目标，由于与狄利克雷无关，这里省略了。
    |'
- en: 'Table 7: Overview over objectives used by posterior networks for classification.'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：分类后验网络使用的目标概述。
- en: '| Method | Loss function | Regularizer | Comment |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 损失函数 | 正则项 | 评论 |'
- en: '| Evidential Deep Learning (Sensoy et al., [2018](#bib.bib146)) | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\beta_{k}}{\beta_{0}}\Big{)}^{2}+\frac{\beta_{k}(\beta_{0}-\beta_{k})}{\beta_{0}^{2}(\beta_{0}+1)}$
    | $-\log\frac{\Gamma(K)}{B(\bm{\beta})}+\sum_{k=1}^{K}(\beta_{k}-1)(\psi(\beta_{k})-\psi(\beta_{0}))$
    |  |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| 证据深度学习 (Sensoy et al., [2018](#bib.bib146)) | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\beta_{k}}{\beta_{0}}\Big{)}^{2}+\frac{\beta_{k}(\beta_{0}-\beta_{k})}{\beta_{0}^{2}(\beta_{0}+1)}$
    | $-\log\frac{\Gamma(K)}{B(\bm{\beta})}+\sum_{k=1}^{K}(\beta_{k}-1)(\psi(\beta_{k})-\psi(\beta_{0}))$
    |  |'
- en: '| Variational Dirichlet (Chen et al., [2018](#bib.bib18)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}$
    |  |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| 变分狄利克雷 (Chen et al., [2018](#bib.bib18)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}$
    |  |'
- en: '| Regularized ENN Zhao et al. ([2019](#bib.bib188)) | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\beta_{k}}{\beta_{0}}\Big{)}^{2}+\frac{\beta_{k}(\beta_{0}-\beta_{k})}{\beta_{0}^{2}(\beta_{0}+1)}$
    | $-\lambda_{1}\mathbb{E}_{#1}\Big{[}\frac{\alpha_{y}}{\alpha_{0}}\Big{]}-\lambda_{2}\mathbb{E}_{#1}\Bigg{[}\sum_{k=1}^{K}\bigg{(}\frac{\beta_{k}\sum_{k^{\prime}\neq
    k}\beta_{k^{\prime}}\big{(}1-\frac{&#124;\beta_{k^{\prime}}-\beta_{k}&#124;}{\beta_{k^{\prime}}+\beta_{k}}\big{)}}{\sum_{k^{\prime}\neq
    k}\beta_{k^{\prime}}}\bigg{)}\Bigg{]}$ | The first term represents *vacuity*,
    i.e. the lack of evidence and is optimized using OOD examples. The second term
    stands for *dissonance*, and is computed using points with neighborhoods with
    different classes from their own. $\lambda_{1},\lambda_{2}$ are hyperparameters.
    |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| 正则化的 ENN Zhao et al. ([2019](#bib.bib188)) | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\beta_{k}}{\beta_{0}}\Big{)}^{2}+\frac{\beta_{k}(\beta_{0}-\beta_{k})}{\beta_{0}^{2}(\beta_{0}+1)}$
    | $-\lambda_{1}\mathbb{E}_{#1}\Big{[}\frac{\alpha_{y}}{\alpha_{0}}\Big{]}-\lambda_{2}\mathbb{E}_{#1}\Bigg{[}\sum_{k=1}^{K}\bigg{(}\frac{\beta_{k}\sum_{k^{\prime}\neq
    k}\beta_{k^{\prime}}\big{(}1-\frac{&#124;\beta_{k^{\prime}}-\beta_{k}&#124;}{\beta_{k^{\prime}}+\beta_{k}}\big{)}}{\sum_{k^{\prime}\neq
    k}\beta_{k^{\prime}}}\bigg{)}\Bigg{]}$ | 第一项表示*空洞性*，即缺乏证据，通过使用OOD示例进行优化。第二项表示*不一致性*，通过计算与邻近不同类别的点来得出。$\lambda_{1},\lambda_{2}$
    是超参数。 |'
- en: '| WGAN–ENN (Hu et al., [2021](#bib.bib71)) | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\beta_{k}}{\beta_{0}}\Big{)}^{2}+\frac{\beta_{k}(\beta_{0}-\beta_{k})}{\beta_{0}^{2}(\beta_{0}+1)}$
    | $-\lambda\mathbb{E}_{#1}\Big{[}\frac{\alpha_{y}}{\alpha_{0}}\Big{]}$ |  |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| WGAN–ENN (Hu et al., [2021](#bib.bib71)) | $\sum_{k=1}^{K}\Big{(}\mathbf{1}_{y=k}-\frac{\beta_{k}}{\beta_{0}}\Big{)}^{2}+\frac{\beta_{k}(\beta_{0}-\beta_{k})}{\beta_{0}^{2}(\beta_{0}+1)}$
    | $-\lambda\mathbb{E}_{#1}\Big{[}\frac{\alpha_{y}}{\alpha_{0}}\Big{]}$ |  |'
- en: '| Belief Matching (Joo et al., [2020](#bib.bib82)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}$
    |  |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| 信念匹配 (Joo et al., [2020](#bib.bib82)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log\frac{B(\bm{\beta})}{B(\bm{\gamma})}+\sum_{k=1}^{K}(\beta_{k}-\gamma_{k})\big{(}\psi(\beta_{k})-\psi(\beta_{0})\big{)}$
    |  |'
- en: '| Posterior networks (Charpentier et al., [2020](#bib.bib16)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log B(\bm{\beta})+(\beta_{0}-K)\psi(\beta_{0})-\sum_{k=1}^{K}(\beta_{k}-1)\psi(\beta_{k})$
    |  |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| 后验网络 (Charpentier et al., [2020](#bib.bib16)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log B(\bm{\beta})+(\beta_{0}-K)\psi(\beta_{0})-\sum_{k=1}^{K}(\beta_{k}-1)\psi(\beta_{k})$
    |  |'
- en: '| Graph Posterior Networks (Stadler et al., [2021](#bib.bib155)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log B(\bm{\beta})+(\beta_{0}-K)\psi(\beta_{0})-\sum_{k=1}^{K}(\beta_{k}-1)\psi(\beta_{k})$
    |  |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| 图模型后验网络 (Stadler et al., [2021](#bib.bib155)) | $\psi(\beta_{y})-\psi(\beta_{0})$
    | $-\log B(\bm{\beta})+(\beta_{0}-K)\psi(\beta_{0})-\sum_{k=1}^{K}(\beta_{k}-1)\psi(\beta_{k})$
    |  |'
- en: '| Generative Evidential Neural Network (Sensoy et al., [2020](#bib.bib147))
    | $-\sum_{k=1}^{K}\bigg{(}\mathbb{E}_{#1}\big{[}\log(\sigma(f_{\bm{\theta}}(\mathbf{x})))\big{]}+\mathbb{E}_{#1}\big{[}\log(1-\sigma(f_{\bm{\theta}}(\mathbf{x})))\big{]}\bigg{)}$
    | $-\log\frac{\Gamma(K)}{B(\bm{\beta}_{-y})}+\sum_{k\neq y}(\beta_{k}-1)(\psi(\beta_{k})-\psi(\beta_{0}))$
    | The main loss is a discriminative loss using ID and OOD samples, generated by
    a VAE. The regularizer is taken over all classes *excluding* the true class $y$
    (also indicated by $\bm{\beta}_{-y}$). |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| 生成证据神经网络 (Sensoy et al., [2020](#bib.bib147)) | $-\sum_{k=1}^{K}\bigg{(}\mathbb{E}_{#1}\big{[}\log(\sigma(f_{\bm{\theta}}(\mathbf{x})))\big{]}+\mathbb{E}_{#1}\big{[}\log(1-\sigma(f_{\bm{\theta}}(\mathbf{x})))\big{]}\bigg{)}$
    | $-\log\frac{\Gamma(K)}{B(\bm{\beta}_{-y})}+\sum_{k\neq y}(\beta_{k}-1)(\psi(\beta_{k})-\psi(\beta_{0}))$
    | 主要损失是使用ID和OOD样本的区分损失，由VAE生成。正则化项考虑了所有类*不包括*真实类别$y$（也由$\bm{\beta}_{-y}$表示）。'
