- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:40:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:40:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2304.10031] Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2304.10031] 拓扑深度学习架构：消息传递拓扑神经网络综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2304.10031](https://ar5iv.labs.arxiv.org/html/2304.10031)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2304.10031](https://ar5iv.labs.arxiv.org/html/2304.10031)
- en: 'Architectures of Topological Deep Learning:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拓扑深度学习架构：
- en: A Survey of Message-Passing Topological Neural Networks
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递拓扑神经网络综述
- en: Mathilde Papillon^(∗1) ¹Department of Physics
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Mathilde Papillon^(∗1) ¹物理系
- en: ²Department of Electrical and Computer Engineering
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²电气与计算机工程系
- en: University of California, Santa Barbara
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚大学圣塔芭芭拉分校
- en: ³ Department of Data Science
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 数据科学系
- en: University of San Francisco
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 旧金山大学
- en: '^∗Corresponding Author: papillon@ucsb.edu Sophia Sanborn² ¹Department of Physics'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗通讯作者：papillon@ucsb.edu Sophia Sanborn² ¹物理系
- en: ²Department of Electrical and Computer Engineering
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ²电气与计算机工程系
- en: University of California, Santa Barbara
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚大学圣塔芭芭拉分校
- en: ³ Department of Data Science
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 数据科学系
- en: University of San Francisco
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 旧金山大学
- en: '^∗Corresponding Author: papillon@ucsb.edu Mustafa Hajij³ ¹Department of Physics'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗通讯作者：papillon@ucsb.edu Mustafa Hajij³ ¹物理系
- en: ²Department of Electrical and Computer Engineering
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ²电气与计算机工程系
- en: University of California, Santa Barbara
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚大学圣塔芭芭拉分校
- en: ³ Department of Data Science
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 数据科学系
- en: University of San Francisco
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 旧金山大学
- en: '^∗Corresponding Author: papillon@ucsb.edu Nina Miolane² ¹Department of Physics'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗通讯作者：papillon@ucsb.edu Nina Miolane² ¹物理系
- en: ²Department of Electrical and Computer Engineering
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ²电气与计算机工程系
- en: University of California, Santa Barbara
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚大学圣塔芭芭拉分校
- en: ³ Department of Data Science
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 数据科学系
- en: University of San Francisco
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 旧金山大学
- en: '^∗Corresponding Author: papillon@ucsb.edu'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗通讯作者：papillon@ucsb.edu
- en: Abstract
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The natural world is full of complex systems characterized by intricate relations
    between their components: from social interactions between individuals in a social
    network to electrostatic interactions between atoms in a protein. Topological
    Deep Learning (TDL) provides a comprehensive framework to process and extract
    knowledge from data associated with these systems, such as predicting the social
    community to which an individual belongs or predicting whether a protein can be
    a reasonable target for drug development. TDL has demonstrated theoretical and
    practical advantages that hold the promise of breaking ground in the applied sciences
    and beyond. However, the rapid growth of the TDL literature for relational systems
    has also led to a lack of unification in notation and language across message-passing
    Topological Neural Network (TNN) architectures. This presents a real obstacle
    for building upon existing works and for deploying message-passing TNNs to new
    real-world problems. To address this issue, we provide an accessible introduction
    to TDL for relational systems, and compare the recently published message-passing
    TNNs using a unified mathematical and graphical notation. Through an intuitive
    and critical review of the emerging field of TDL, we extract valuable insights
    into current challenges and exciting opportunities for future development.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自然界充满了由其组件之间复杂关系所特征的复杂系统：从社交网络中个体之间的社会互动，到蛋白质中原子之间的静电相互作用。拓扑深度学习（TDL）提供了一个全面的框架来处理和提取与这些系统相关的数据中的知识，例如预测个体所属的社会群体，或预测蛋白质是否是药物开发的合理目标。TDL展示了理论和实践上的优势，有望在应用科学及其他领域开辟新天地。然而，TDL文献的快速增长也导致了在消息传递拓扑神经网络（TNN）架构中符号和语言的缺乏统一，这为在现有工作基础上进一步研究以及将消息传递TNN应用于新的现实问题带来了实际障碍。为了解决这个问题，我们提供了一个关于关系系统的TDL的易于理解的介绍，并使用统一的数学和图形符号比较了最近发布的消息传递TNN。通过对新兴的TDL领域进行直观和批判性的审查，我们提取了当前挑战和未来发展令人兴奋的机会的宝贵见解。
- en: 'Index Terms:'
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: 'Deep learning, topology, message passing, graph, hypergraph, simplicial complex,
    cellular complex, combinatorial complex^†^†publicationid: pubid: 0000–0000/00$00.00 © 2023
    IEEE'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习，拓扑学，消息传递，图，超图，简并复形，细胞复形，组合复形^†^†出版编号：pubid: 0000–0000/00$00.00 © 2023
    IEEE'
- en: I Introduction
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Many natural systems as diverse as social networks [[1](#bib.bib1)] and proteins
    [[2](#bib.bib2)] are characterized by relational structure. This is the structure
    of interactions between components in the system, such as social interactions
    between individuals or electrostatic interactions between atoms. In Geometric
    Deep Learning [[3](#bib.bib3)], Graph Neural Networks (GNNs) [[4](#bib.bib4)]
    have demonstrated remarkable achievements in processing relational data using
    graphs—mathematical objects commonly used to encode pairwise relations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 许多自然系统，如社交网络[[1](#bib.bib1)]和蛋白质[[2](#bib.bib2)]，具有关系结构。这是系统中组件之间的交互结构，如个体之间的社交互动或原子之间的电静力学互动。在几何深度学习[[3](#bib.bib3)]中，图神经网络（GNNs）[[4](#bib.bib4)]
    在使用图处理关系数据方面取得了显著成就——图是编码成对关系的常用数学对象。
- en: However, the pairwise structure of graphs is limiting. Social interactions can
    involve more than two individuals, and electrostatic interactions more than two
    atoms. Topological Deep Learning (TDL) [[5](#bib.bib5), [6](#bib.bib6)] leverages
    more general abstractions to process data with higher-order relational structure.
    The theoretical guarantees [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)] of
    its models, Topological Neural Networks (TNNs), lead to state-of-the-art performance
    on many machine learning tasks [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)]—and reveal high potential for the applied sciences and beyond.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，图的成对结构存在局限性。社会互动可能涉及两个以上的个体，电静力学互动也可能涉及两个以上的原子。拓扑深度学习（TDL）[[5](#bib.bib5),
    [6](#bib.bib6)] 利用更一般的抽象来处理具有更高阶关系结构的数据。其模型拓扑神经网络（TNNs）的理论保障[[7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)] 在许多机器学习任务上表现出最先进的性能[[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)]，并且在应用科学及其他领域展现了极大的潜力。
- en: 'However, the abstraction and fragmentation of mathematical notation across
    the TDL literature significantly limits the field’s accessibility, while complicating
    model comparison and obscuring opportunities for innovation. To address this,
    we present an intuitive and systematic comparison of published message-passing
    TNN architectures, heretofore referred to as TNNs. We contribute:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，TDL文献中数学符号的抽象和碎片化显著限制了该领域的可访问性，同时使模型比较复杂化并掩盖了创新机会。为解决这一问题，我们提供了对已发布的消息传递TNN架构的直观和系统化比较，之前称之为TNNs。我们的贡献包括：
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A pedagogical resource accessible to newcomers interested in applying TNNs to
    real-world problems.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个面向新手的教学资源，帮助他们将TNNs应用于现实世界的问题。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A comprehensive and critical review of TNNs, their implementations and practical
    applications, with equations rewritten in our notation available at https://github.com/awesome-tnns.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对TNN的全面而批判性的综述，包括其实现和实际应用，方程式已按照我们的符号重新编写，网址为[https://github.com/awesome-tnns](https://github.com/awesome-tnns)。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A summary of open research questions, challenges, and opportunities for innovation.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对未解研究问题、挑战和创新机会的总结。
- en: By establishing a common and accessible language in the field, we hope to provide
    newcomers and experienced practitioners alike with a solid foundation for cutting-edge
    research in TDL.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在该领域建立一个共同且易于理解的语言，我们希望为新手和经验丰富的从业者提供坚实的基础，以便在TDL领域进行前沿研究。
- en: Other literature reviews at the intersection of topology and machine learning
    have focused on data representation [[14](#bib.bib14)] and physics-inspired models [[15](#bib.bib15)].
    Message-passing TNNs are part of a broader spectrum of machine learning architectures
    leveraging topology. First surveyed in [[16](#bib.bib16)], this spectrum encompasses
    additional methods such as topological data analysis for machine learning. In
    such cases, features computed from techniques such as persistent homology are
    used to enhance data representation or model selection.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其他在拓扑学和机器学习交叉点的文献综述集中于数据表示[[14](#bib.bib14)]和物理启发的模型[[15](#bib.bib15)]。消息传递TNNs是利用拓扑的广泛机器学习架构的一部分。首次调查见于[[16](#bib.bib16)]，该光谱涵盖了如拓扑数据分析等额外的方法，这些方法计算出的特征用于增强数据表示或模型选择。
- en: II Topological Neural Networks
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 拓扑神经网络
- en: 'Topological Neural Networks (TNNs) are deep learning architectures that extract
    knowledge from data associated with topologically rich systems such as protein
    structures, city traffic maps, or citation networks. A TNN, like a GNN, is comprised
    of stacked layers that transform data into a series of features (Figure [1](#S2.F1
    "Figure 1 ‣ II Topological Neural Networks ‣ Architectures of Topological Deep
    Learning: A Survey of Message-Passing Topological Neural Networks")). Each layer
    leverages the fundamental concepts of data and computational domains, neighborhoods,
    and message passing—presented in this section.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑神经网络（TNNs）是深度学习架构，用于从与拓扑丰富的系统（如蛋白质结构、城市交通图或引文网络）相关的数据中提取知识。TNN 像 GNN 一样，由一系列层组成，这些层将数据转换为一系列特征（图
    [1](#S2.F1 "图 1 ‣ II 拓扑神经网络 ‣ 拓扑深度学习的架构：消息传递拓扑神经网络综述")）。每一层利用数据和计算领域、邻域以及消息传递的基本概念，这些概念将在本节中介绍。
- en: '![Refer to caption](img/26ffcc604898b6af1afb2555d47cab91.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/26ffcc604898b6af1afb2555d47cab91.png)'
- en: 'Figure 1: Topological Neural Network: Data associated with a complex system
    are features defined on a data domain, which is preprocessed into a computational
    domain that encodes interactions between the system’s components with neighborhoods.
    The TNN’s layers use message passing to successively update features and yield
    an output, e.g. a categorical label in classification or a quantitative value
    in regression. The output represents new knowledge extracted from the input data.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：拓扑神经网络：与复杂系统相关的数据是定义在数据领域上的特征，该领域经过预处理成计算领域，编码系统组件之间的交互和邻域。TNN 的层使用消息传递来连续更新特征并生成输出，例如分类中的类别标签或回归中的定量值。输出代表从输入数据中提取的新知识。
- en: II-A Domains
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 领域
- en: 'In Topological Deep Learning (TDL), data are features defined on discrete domains
    [[5](#bib.bib5), [6](#bib.bib6)]. Traditional examples of discrete domains include
    sets and graphs (Figure [2](#S2.F2 "Figure 2 ‣ II-A Domains ‣ II Topological Neural
    Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks"), Left). A set is a collection of points called nodes
    without any additional structure. A graph is a set with edges that encode pairwise
    relations between nodes, representing either geometric proximity or more abstract
    relationships. For example, a graph may represent a protein, with nodes encoding
    its atoms and edges encoding the pairwise bonds between them. Alternatively, a
    graph may represent a social network, where nodes represent individuals and edges
    denote social relationships. The domains of TDL generalize the pairwise relations
    of graphs to part-whole and set-types relations that permit the representation
    of more complex relational structure (Figure [2](#S2.F2 "Figure 2 ‣ II-A Domains
    ‣ II Topological Neural Networks ‣ Architectures of Topological Deep Learning:
    A Survey of Message-Passing Topological Neural Networks"), Right) [[5](#bib.bib5)].
    Here, we describe the key attributes of each domain and highlight their suitability
    for different data types. We refer the reader to [[14](#bib.bib14)] and [[5](#bib.bib5)]
    for more extensive discussions.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在拓扑深度学习（TDL）中，数据是定义在离散领域上的特征 [[5](#bib.bib5), [6](#bib.bib6)]。离散领域的传统示例包括集合和图（图
    [2](#S2.F2 "图 2 ‣ II-A 领域 ‣ II 拓扑神经网络 ‣ 拓扑深度学习的架构：消息传递拓扑神经网络综述")，左）。集合是一组称为节点的点，没有任何额外的结构。图是一个具有边的集合，这些边编码节点之间的成对关系，代表几何邻近关系或更抽象的关系。例如，图可以表示一个蛋白质，其中节点编码其原子，边编码它们之间的成对键。或者，图可以表示一个社交网络，其中节点代表个人，边表示社会关系。TDL
    的领域将图的成对关系推广到部分-整体和集合类型关系，从而允许表示更复杂的关系结构（图 [2](#S2.F2 "图 2 ‣ II-A 领域 ‣ II 拓扑神经网络
    ‣ 拓扑深度学习的架构：消息传递拓扑神经网络综述")，右） [[5](#bib.bib5)]。在这里，我们描述了每个领域的关键属性，并突出它们对不同数据类型的适用性。我们建议读者参考
    [[14](#bib.bib14)] 和 [[5](#bib.bib5)] 以获取更详细的讨论。
- en: '<svg   height="221.28" overflow="visible" version="1.1" width="600"><g transform="translate(0,221.28)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="193.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Beyond Graphs: The Domains of Topological Deep Learning Set +
    Pairwise Relations Graph: A set of points (nodes) connected with edges that denote
    pairwise relationships. Set + Part-Whole Relations Simplicial Complex (SC): A
    generalization of a graph in which three edges can form a triangular face, four
    triangles can form a tetrahedral volume, and so on. Edges only connect pairs of
    nodes. Cellular Complex (CC): A generalization of an SC in which faces, volumes,
    etc are not restricted to be triangles or tetrahedrons but may instead take any
    shape. Still, edges only connect pairs of nodes. Set + Set-Type Relations Hypergraph
    (HG): A generalization of a graph, in which higher-order edges called hyperedges
    can connect arbitrary sets of two or more nodes. Set + Part-Whole and Set-Type
    Relations Combinatorial Complex (CCC): A structure that combines features of HGs
    and CCs. Like an HG, edges may connect any number of nodes. Like a CC, cells can
    be combined to form higher-ranked structures.![Refer to caption](img/02ef42885fd7cd5fa4fc5fd3d9a25828.png)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="221.28" overflow="visible" version="1.1" width="600"><g transform="translate(0,221.28)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="193.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">超越图：拓扑深度学习的领域 集合 + 成对关系图：一组用边连接的点（节点），这些边表示成对的关系。集合 + 部分-整体关系
    简单复形（SC）：图的推广，其中三条边可以形成一个三角面，四个三角形可以形成一个四面体体积，等等。边只连接节点对。细胞复形（CC）：SC的推广，其中面、体积等不局限于三角形或四面体，而可以是任何形状。尽管如此，边仍然只连接节点对。集合
    + 集合类型关系 超图（HG）：图的推广，其中称为超边的高阶边可以连接任意两个或更多节点的集合。集合 + 部分-整体和集合类型关系 组合复形（CCC）：结合了HG和CC特征的结构。类似于HG，边可以连接任意数量的节点。类似于CC，单元可以组合形成更高等级的结构。![参考说明](img/02ef42885fd7cd5fa4fc5fd3d9a25828.png)
- en: 'Figure 2: Domains: Nodes in blue, (hyper)edges in pink, and faces in dark red.
    Figure adapted from [[11](#bib.bib11)].'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：领域：蓝色的节点，粉色的（超）边，深红色的面。图示改编自[[11](#bib.bib11)]。
- en: 'Simplicial complexes (SCs) generalize graphs to incorporate hierarchical part-whole
    relations through the multi-scale construction of cells. Nodes are rank 0 cells
    that can be combined to form edges (rank 1 cells). Edges are, in turn, combined
    to form faces (rank 2 cells), which are combined to form volumes (rank 3 cells),
    and so on. As such, an SC’s faces must be triangles, volumes must be tetrahedrons,
    and so forth. SCs are commonly used to encode discrete representations of 3D geometric
    surfaces represented with triangular meshes (Figure [3](#S2.F3 "Figure 3 ‣ II-A
    Domains ‣ II Topological Neural Networks ‣ Architectures of Topological Deep Learning:
    A Survey of Message-Passing Topological Neural Networks")). They may also be used
    to represent more abstract relations; however, there is a risk of introducing
    spurious connections if the strict geometric constraints of an SC are not respected
    by the data—a point we elaborate on in Section [II-A2](#S2.SS1.SSS2 "II-A2 Limitations
    ‣ II-A Domains ‣ II Topological Neural Networks ‣ Architectures of Topological
    Deep Learning: A Survey of Message-Passing Topological Neural Networks").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 简单复形（SCs）将图的概念推广到通过多尺度的单元构造来包含层次的部分-整体关系。节点是等级0的单元，可以组合形成边（等级1的单元）。边反过来又组合形成面（等级2的单元），面组合形成体积（等级3的单元），依此类推。因此，SC的面必须是三角形，体积必须是四面体，等等。SC通常用于编码离散的3D几何表面表示，这些表面用三角网格表示（图[3](#S2.F3
    "图 3 ‣ II-A 领域 ‣ II 拓扑神经网络 ‣ 拓扑深度学习架构：基于消息传递的拓扑神经网络综述")）。它们也可以用于表示更抽象的关系；然而，如果数据不遵守SC的严格几何约束，可能会引入虚假的连接——这一点我们在第[II-A2](#S2.SS1.SSS2
    "II-A2 局限性 ‣ II-A 领域 ‣ II 拓扑神经网络 ‣ 拓扑深度学习架构：基于消息传递的拓扑神经网络综述")节中详细阐述。
- en: 'Cellular complexes (CCs) generalize SCs such that cells are not limited to
    simplexes: faces can involve more than three nodes, volumes more than four faces,
    and so on. This flexibility endows CCs with greater expressivity than SCs [[8](#bib.bib8)].
    A practitioner should consider employing this domain when studying a system that
    features part-whole interactions between more than three nodes, such as a molecule
    with benzene rings (Figure [3](#S2.F3 "Figure 3 ‣ II-A Domains ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks")).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞复杂体（CCs）将简单复形（SCs）推广到细胞不限于单纯形：面可以涉及超过三个节点，体积可以涉及超过四个面，等等。这种灵活性赋予了CCs比SCs更大的表现力
    [[8](#bib.bib8)]。实践者在研究具有三个以上节点的部分-整体交互的系统时，应考虑使用这一领域，例如具有苯环的分子（图 [3](#S2.F3 "图
    3 ‣ II-A 领域 ‣ II 拓扑神经网络 ‣ 拓扑深度学习架构：信息传递拓扑神经网络的调查")）。
- en: 'Hypergraphs (HGs) extend graphs in that their edges, called hyperedges, can
    connect more than two nodes. Connections in HGs represent set-type relationships,
    in which participation in an interaction is not implied by any other relation
    in the system. This makes HGs an ideal choice for data with abstract and arbitrarily
    large interactions of equal importance, such as semantic text and citation networks.
    Protein interaction networks (Figure [3](#S2.F3 "Figure 3 ‣ II-A Domains ‣ II
    Topological Neural Networks ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks")) also exhibit this property:
    an interaction between proteins requires a precise set of molecules—no more and
    no less. The interaction of Proteins A, B, and C does not imply an interaction
    between A and B on their own.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 超图（HGs）通过其边，称为超边，能够连接超过两个节点，从而扩展了图的概念。在HGs中，连接代表集合型关系，其中参与一个交互的状态并不由系统中的其他关系所暗示。这使得HGs成为处理抽象且任意大且同等重要的交互的数据的理想选择，如语义文本和引用网络。蛋白质相互作用网络（图
    [3](#S2.F3 "图 3 ‣ II-A 领域 ‣ II 拓扑神经网络 ‣ 拓扑深度学习架构：信息传递拓扑神经网络的调查")）也展现了这一特性：蛋白质之间的相互作用需要一组精确的分子——不多也不少。蛋白质A、B和C之间的相互作用并不暗示A和B之间的相互作用。
- en: 'Combinatorial complexes (CCCs) generalize CCs and HGs to incorporate both part-whole
    and set-type relationships [[11](#bib.bib11), [5](#bib.bib5)]. The benefit of
    this can be observed in the example of molecular representation. The strict geometric
    constraints of simplicial and cellular complexes are too rigid for capturing much
    of hierarchical structure observed in molecules. By contrast, the flexible but
    hierarchically ranked hyperedges of a combinatorial complex can capture the full
    richness of molecular structure, as depicted in Figure [3](#S2.F3 "Figure 3 ‣
    II-A Domains ‣ II Topological Neural Networks ‣ Architectures of Topological Deep
    Learning: A Survey of Message-Passing Topological Neural Networks"). This is the
    most recent and most general topological domain, introduced in 2022 by [[11](#bib.bib11)]
    and further theoretically established in [[5](#bib.bib5)].'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 组合复杂体（CCCs）将组合复杂体（CCs）和超图（HGs）推广到既包括部分-整体关系也包括集合型关系 [[11](#bib.bib11), [5](#bib.bib5)]。这点可以通过分子表示的例子观察到。简单复形和细胞复形的严格几何约束对于捕捉分子中观察到的层级结构过于僵化。相比之下，组合复杂体的灵活但按层级排序的超边能够捕捉到分子结构的全部丰富性，如图
    [3](#S2.F3 "图 3 ‣ II-A 领域 ‣ II 拓扑神经网络 ‣ 拓扑深度学习架构：信息传递拓扑神经网络的调查") 所示。这是最新的、最通用的拓扑领域，由
    [[11](#bib.bib11)] 在2022年引入，并在 [[5](#bib.bib5)] 中进一步理论化。
- en: '![Refer to caption](img/d753fa0319e8ed403f32e90779c50ea4.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d753fa0319e8ed403f32e90779c50ea4.png)'
- en: 'Figure 3: Examples of Data on Topological Domains. (a) Higher-order interactions
    in protein networks. (b) Limited molecular representation: rings can only contain
    three atoms. (c) Triangular mesh of a protein surface. (d) More flexible molecular
    representation, permitting the representation of any ring-shaped functional group.
    (e) Flexible mesh which includes arbitrarily shaped faces. (f) Fully flexible
    molecular representation, permitting the representation of the complex nested
    hierarchical structure characteristic of molecules and other natural systems.
    (g) Hierarchical higher-order interactions in protein networks.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：拓扑领域中的数据示例。(a) 蛋白质网络中的高阶相互作用。(b) 有限的分子表示：环只能包含三个原子。(c) 蛋白质表面的三角网格。(d) 更灵活的分子表示，允许表示任何环状官能团。(e)
    包括任意形状面片的灵活网格。(f) 完全灵活的分子表示，允许表示分子及其他自然系统特有的复杂嵌套层级结构。(g) 蛋白质网络中的层次高阶相互作用。
- en: II-A1 Terminology
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 术语
- en: 'Across discrete domains, we use the term cell to denote any node or relation
    between nodes such as (hyper)edges, faces, or volumes. Cells possess two attributes:
    size—the number of cells it contains—and rank—where nodes are said to have rank
    0, edges and hyperedges rank 1, faces rank 2, and so on. The part-whole relationships
    of simplicial and cellular complexes impose a relationship between the rank of
    a cell and its size: cells of rank $r$ contains exactly (resp. at least) $r+1$
    cells of rank $r-1$: faces ($r=2$) contain exactly (resp. at least) three edges
    ($r-1=1$). By contrast, hypergraph cells do not encode part-whole relations and
    hyperedges may have any size. However, hypergraph cells are limited to ranks 0
    and 1\. A combinatorial complex is unrestricted in both rank and size: nodes have
    rank 0 and cells of any size $>$ 1 can have any rank.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散领域中，我们使用术语“单元”（cell）来表示任何节点或节点之间的关系，例如（超）边、面或体。单元具有两个属性：大小——包含的单元数量——和秩——其中节点的秩为0，边和超边的秩为1，面为2，依此类推。简单和细胞复合体的部分-整体关系对单元的秩和大小之间施加了关系：秩为$r$的单元包含正好（或至少）$r+1$个秩为$r-1$的单元：面（$r=2$）包含正好（或至少）三个边（$r-1=1$）。相比之下，超图单元不编码部分-整体关系，超边的大小可以是任意的。然而，超图单元的秩仅限于0和1。组合复合体在秩和大小上没有限制：节点的秩为0，任何大小$>$
    1的单元可以具有任何秩。
- en: 'There is an important distinction between the inherent domain of the data (the
    data domain) and the domain in which the data will be processed within a TNN:
    the computational domain. Data defined on a graph, for example, may be “lifted”
    (Figure [4](#S2.F4 "Figure 4 ‣ II-A1 Terminology ‣ II-A Domains ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks")) to an alternative domain through a pre-processing
    stage (Figure [1](#S2.F1 "Figure 1 ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks")).
    For instance, a protein originally given as the graph of its atoms (nodes) and
    covalent bounds (edges) may be lifted into a CC computational domain that explicitly
    represents its rings (faces). In this review, domain refers to the computational
    domain. Additionally, the computational domain may be dynamic, changing from layer
    to layer in a TNN.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的固有领域（数据领域）与在TNN中处理数据的领域之间存在重要区别：计算领域。例如，定义在图上的数据可能会通过预处理阶段（图 [4](#S2.F4 "图
    4 ‣ II-A1 术语 ‣ II-A 领域 ‣ II 拓扑神经网络 ‣ 拓扑深度学习的体系结构：消息传递拓扑神经网络的综述")）被“提升”到另一个领域（图
    [1](#S2.F1 "图 1 ‣ II 拓扑神经网络 ‣ 拓扑深度学习的体系结构：消息传递拓扑神经网络的综述")）。例如，最初作为原子（节点）和共价键（边）的图表示的蛋白质，可能会提升到一个明确表示其环（面）的CC计算领域。在本综述中，领域指的是计算领域。此外，计算领域可能是动态的，在TNN中的不同层之间变化。
- en: '<svg   height="82.73" overflow="visible" version="1.1" width="600"><g transform="translate(0,82.73)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="55.18" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Dynamic Domains Static vs. Dynamic: In a TNN, a static domain
    is identical for each layer. For example, all three layers in Figure [1](#S2.F1
    "Figure 1 ‣ II Topological Neural Networks ‣ Architectures of Topological Deep
    Learning: A Survey of Message-Passing Topological Neural Networks") operate on
    the same CCC, only features evolve across layers. A dynamic domain changes from
    layer to layer. Nodes can be added or removed, edges can be rewired, and so on.</foreignobject></g></g></svg>![Refer
    to caption](img/3af0cbe7ad2785ff22b1d939656055ee.png)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg   height="82.73" overflow="visible" version="1.1" width="600"><g transform="translate(0,82.73)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="55.18" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">动态域 静态与动态：在TNN中，静态域对每一层都是相同的。例如，图中的所有三层[1](#S2.F1 "Figure 1 ‣
    II Topological Neural Networks ‣ Architectures of Topological Deep Learning: A
    Survey of Message-Passing Topological Neural Networks")都作用于相同的CCC，只有特征在层间演变。动态域则在层与层之间发生变化。节点可以被添加或移除，边缘可以重新布线，等等。</foreignobject></g></g></svg>![参见标题](img/3af0cbe7ad2785ff22b1d939656055ee.png)'
- en: 'Figure 4: Lifting Topological Domains. (a) A graph is lifted to a hypergraph
    by adding hyperedges that connect groups of nodes. (b) A graph can be lifted to
    a cellular complex by adding faces of any shape. (c) Hyperedges can be added to
    a cellular complex to lift the structure to a combinatorial complex. Figure adopted
    from [[5](#bib.bib5)].'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 提升拓扑域。 (a) 通过添加连接节点组的超边，将图提升为超图。 (b) 通过添加任意形状的面，将图提升为细胞复合体。 (c) 可以向细胞复合体中添加超边，将结构提升为组合复合体。
    图源自 [[5](#bib.bib5)]。'
- en: II-A2 Limitations
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 局限性
- en: An important limitation of both SCs and CCs is that faces (and analogous higher-order
    structures) can only form rings; the nodes on the boundary of the face must be
    connected in pairs. In many cases, this requirement is too stringent and can introduce
    artificial connections to the domain [[17](#bib.bib17)]. For instance, lifting
    a citation network into an SC necessarily requires that any set of three co-authors
    having written a paper together (A, B, C) are also pairwise connected (A and B,
    A and C, B and C), even if no paper was ever exclusively authored by authors A
    and B, authors A and C, or authors B and C. [[17](#bib.bib17)] propose a “relaxed”
    definition of the SC that remedies this. They show how training a TNN on such
    a modified domain increases performance. We note that even with artificial connections,
    SCs and CCs allow TNNs to leverage richer topological structure and avoid computational
    problems faced by GNNs [[18](#bib.bib18)]. We further note that any topological
    domain is mathematically equivalent to a (possibly larger) graph [[19](#bib.bib19)].
    We choose to express domains in their form above in order to provide better intuition
    to newcomers and reflect the widely adopted approaches in the literature.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: SC和CC的一个重要局限性是面（以及类似的高阶结构）只能形成环；面边界上的节点必须成对连接。在许多情况下，这一要求过于严格，可能会在域中引入人工连接 [[17](#bib.bib17)]。例如，将引文网络提升为SC必然要求任何一组三位合著者（A,
    B, C）也必须成对连接（A和B，A和C，B和C），即使A和B、A和C或B和C没有独立编写的论文。 [[17](#bib.bib17)] 提出了一个“放宽”的SC定义来解决这个问题。他们展示了在这样的修改域上训练TNN如何提高性能。我们注意到，即使有了人工连接，SC和CC仍允许TNN利用更丰富的拓扑结构，避免GNN所面临的计算问题
    [[18](#bib.bib18)]。我们进一步指出，任何拓扑域在数学上都等价于一个（可能更大的）图 [[19](#bib.bib19)]。我们选择以上述形式表达域，以便为新手提供更好的直观理解，并反映文献中广泛采用的方法。
- en: II-A3 Features on a Domain
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 域上的特征
- en: Consider a domain, denoted $\mathcal{X}$, encoding relationships between components
    of a system. Data on the domain are represented as features supported on the domain’s
    cells. Typically, features are vectors in $\mathbb{R}^{d}$ that encode attributes
    of each cell. For example, features may encode the atom (node), bond (edge), and
    functional group (face) types in a molecule. A feature associated with the interaction
    between a set of drugs (hyperedge) could indicate the probability of adverse reaction.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个领域，记作 $\mathcal{X}$，它编码了系统组件之间的关系。领域上的数据以支撑在领域单元格上的特征来表示。通常，特征是 $\mathbb{R}^{d}$
    中的向量，编码每个单元格的属性。例如，特征可能编码分子中的原子（节点）、键（边）和功能团（面）的类型。与一组药物（超边）之间的相互作用相关联的特征可以指示不良反应的概率。
- en: 'We denote with $\mathbf{h}_{x}^{t,(r)}$ a feature supported on the cell $x\in\mathcal{X}$
    at layer $t$ of the TNN, with $r$ indicating the rank of $x$ (Figure [5](#S2.F5
    "Figure 5 ‣ II-A3 Features on a Domain ‣ II-A Domains ‣ II Topological Neural
    Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks")). The domain is decomposed into ranks, with $X^{(r)}$,
    or $r$-skeleton, referring to all cells of rank $r$. Features can be categorical
    or quantitative. If the feature dimension varies across skeletons, the domain
    is heterogeneous.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 $\mathbf{h}_{x}^{t,(r)}$ 表示在 TNN 层 $t$ 上支撑在单元格 $x\in\mathcal{X}$ 的特征，$r$
    表示 $x$ 的等级（图 [5](#S2.F5 "图 5 ‣ II-A3 领域上的特征 ‣ II-A 领域 ‣ II 拓扑神经网络 ‣ 拓扑深度学习架构：基于消息传递的拓扑神经网络综述")）。领域被分解为等级，$X^{(r)}$
    或 $r$-骨架，指所有等级为 $r$ 的单元格。特征可以是类别的或定量的。如果特征维度在骨架间变化，领域就是异质的。
- en: '<svg   height="71.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,71.84)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="44.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Heterogeneous Domains Homogeneity vs. Heterogeneity: In a heterogeneous
    domain, the dimension $d_{r}$ of a feature $\mathbf{h}_{x}^{(r)}$ depends on the
    rank $r$ of the cell $x$ supporting it. A homogeneous domain uses the same dimensionality
    $d$ for all ranks.</foreignobject></g></g></svg>![Refer to caption](img/089915783e08a51f6ef042e9e4b92b68.png)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="71.84" overflow="visible" version="1.1" width="600"><g transform="translate(0,71.84)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="44.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">异质领域的同质性与异质性：在异质领域中，特征 $\mathbf{h}_{x}^{(r)}$ 的维度 $d_{r}$ 依赖于支持它的单元格
    $x$ 的等级 $r$。同质领域对所有等级使用相同的维度 $d$。</foreignobject></g></g></svg>![参考说明](img/089915783e08a51f6ef042e9e4b92b68.png)
- en: 'Figure 5: Features on a Domain. Left: Features onto three cells—$x$, $y$, and
    $z$. Right: Skeletons for the entire complex: $X^{(0)}$ contains node features,
    $X^{(1)}$ contains edge features, and so on.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：领域上的特征。左侧：三个单元格上的特征——$x$、$y$ 和 $z$。右侧：整个复杂体的骨架：$X^{(0)}$ 包含节点特征，$X^{(1)}$
    包含边特征，以此类推。
- en: 'The features assigned to each cell may come directly from the data or be hand-designed
    by the practitioner. Alternatively, features can be assigned in a pre-processing
    stage using embedding methods, which compute cell feature vectors that encode
    the local structure of the space. For graphs, methods such as DeepWalk [[20](#bib.bib20)]
    and Node2Vec [[21](#bib.bib21)] are commonly used to embed nodes. Recent works
    have generalized these approaches to topological domains: Hyperedge2Vec [[22](#bib.bib22)]
    and Deep Hyperedge [[23](#bib.bib23)] for hypergraphs, Simplex2Vec [[24](#bib.bib24)]
    and k-Simplex2Vec [[25](#bib.bib25)] for simplicial complexes, and Cell2Vec [[26](#bib.bib26)]
    for cellular complexes.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给每个单元格的特征可能直接来自数据或由从业人员手动设计。或者，可以在预处理阶段使用嵌入方法分配特征，这些方法计算编码空间局部结构的单元格特征向量。对于图，常用的嵌入方法包括
    DeepWalk [[20](#bib.bib20)] 和 Node2Vec [[21](#bib.bib21)]。最近的研究将这些方法推广到拓扑领域：Hyperedge2Vec
    [[22](#bib.bib22)] 和 Deep Hyperedge [[23](#bib.bib23)] 用于超图，Simplex2Vec [[24](#bib.bib24)]
    和 k-Simplex2Vec [[25](#bib.bib25)] 用于单纯形复形，以及 Cell2Vec [[26](#bib.bib26)] 用于细胞复形。
- en: II-B Neighborhood Structure
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 邻域结构
- en: '![Refer to caption](img/5ec96a538ca7319942af6443439dbe65.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5ec96a538ca7319942af6443439dbe65.png)'
- en: 'Figure 6: Incidence Matrices. Examples of an SC, a CC, an HG, and a CCC with
    their corresponding boundary matrices $B_{1}$ which map from 1-cells to 0-cells.
    The SC and CC maps are signed to encode edge orientation: the node appearing first
    in the arbitrary ordering (a,b,c,d) is always assigned -1.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：关联矩阵。展示了一个 SC、一个 CC、一个 HG 和一个 CCC 以及它们对应的边界矩阵 $B_{1}$，这些矩阵将 1-单元映射到 0-单元。SC
    和 CC 映射被签名以编码边的方向：在任意排序（a,b,c,d）中首先出现的节点总是被分配 -1。
- en: 'A TNN successively updates cell features throughout its layers by using a notion
    of nearness between cells: the neighborhood structure (Figure [1](#S2.F1 "Figure
    1 ‣ II Topological Neural Networks ‣ Architectures of Topological Deep Learning:
    A Survey of Message-Passing Topological Neural Networks")). Neighborhood structures
    are defined by boundary relations, which describe how cells of different ranks
    relate to each other. A cell $y$ of rank $r$ is said to be on the boundary of
    cell $x$ of rank $R$ if it is connected to $x$ and rank $r<R$. This relation is
    expressed as $y\prec x$. For example, a node connected to an edge is said to be
    on the boundary of that edge.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 'TNN 通过使用单元之间的邻近概念，逐层更新单元特征：邻域结构（图 [1](#S2.F1 "Figure 1 ‣ II Topological Neural
    Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks")）。邻域结构由边界关系定义，描述了不同排名的单元如何相互关联。一个排名为 $r$ 的单元 $y$
    被称为在排名为 $R$ 的单元 $x$ 的边界上，如果它与 $x$ 连接且 $r<R$。这种关系表示为 $y\prec x$。例如，一个与边连接的节点被称为在该边的边界上。'
- en: 'Boundary relations are encoded in incidence matrices. Specifically, we denote
    with $B_{r}$ the matrix that records which (regular) cells of rank $r-1$ bound
    which cells of rank $r$ (Figure [6](#S2.F6 "Figure 6 ‣ II-B Neighborhood Structure
    ‣ II Topological Neural Networks ‣ Architectures of Topological Deep Learning:
    A Survey of Message-Passing Topological Neural Networks")). Formally, $B_{r}$
    is a matrix of size $n_{r-1}\crossproduct n_{r}$, with $n_{r}$ denoting the number
    of cells of rank $r\geq 1$, defined:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '边界关系在关联矩阵中被编码。具体地，我们用 $B_{r}$ 来表示记录哪些（常规）排名为 $r-1$ 的单元与哪些排名为 $r$ 的单元相绑定的矩阵（图
    [6](#S2.F6 "Figure 6 ‣ II-B Neighborhood Structure ‣ II Topological Neural Networks
    ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing Topological
    Neural Networks")）。形式上，$B_{r}$ 是一个大小为 $n_{r-1}\crossproduct n_{r}$ 的矩阵，其中 $n_{r}$
    表示排名为 $r\geq 1$ 的单元数量。'
- en: '|  | $(B_{r})_{i,j}=\begin{cases}\pm 1&amp;x_{i}^{(r-1)}\prec x_{j}^{(r)}\\
    0&amp;\text{otherwise,}\end{cases}$ |  | (1) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $(B_{r})_{i,j}=\begin{cases}\pm 1&amp;x_{i}^{(r-1)}\prec x_{j}^{(r)}\\
    0&amp;\text{其他情况,}\end{cases}$ |  | (1) |'
- en: where $x_{i}^{(r-1)}$, $x_{j}^{(r)}$ are two cells of ranks $r-1$ and $r$ respectively.
    The $\pm 1$ sign encodes a notion of orientation required for SCs and CCs [[27](#bib.bib27),
    [28](#bib.bib28)], and is always $+1$ for HGs and CCCs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{i}^{(r-1)}$ 和 $x_{j}^{(r)}$ 是分别具有 $r-1$ 和 $r$ 排名的两个单元。$\pm 1$ 的符号编码了
    SCs 和 CCs 所需的方向概念 [[27](#bib.bib27), [28](#bib.bib28)]，对于 HGs 和 CCCs 始终为 $+1$。
- en: 'Incidence matrices can be used to encode the four most common neighborhood
    structures used in the literature, which we illustrate in Fig. [7](#S2.F7 "Figure
    7 ‣ II-C1 The Steps of Message Passing ‣ II-C Message Passing ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks") and define below. Here, $L_{\uparrow,0}$ denotes
    the typical graph Laplacian. Its higher order generalization, the $r$-Hodge Laplacian,
    is $H_{r}=L_{\downarrow,r}+L_{\uparrow,r}$ [[12](#bib.bib12), [29](#bib.bib29)].
    $D_{r}\in\mathbb{N}^{n_{r}\crossproduct n_{r}}$ denotes the degree matrix, a diagonal
    matrix representing the number of connections of $r$-cells with $(r+1)$-cells.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '关联矩阵可以用于编码文献中最常见的四种邻域结构，我们在图 [7](#S2.F7 "Figure 7 ‣ II-C1 The Steps of Message
    Passing ‣ II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks")
    中进行了说明，并在下文中定义。这里，$L_{\uparrow,0}$ 表示典型的图拉普拉斯算子。它的高阶推广，即 $r$-Hodge 拉普拉斯算子，记作 $H_{r}=L_{\downarrow,r}+L_{\uparrow,r}$
    [[12](#bib.bib12), [29](#bib.bib29)]。$D_{r}\in\mathbb{N}^{n_{r}\crossproduct n_{r}}$
    表示度矩阵，是一个对角矩阵，表示 $r$-单元与 $(r+1)$-单元的连接数量。'
- en: '<svg   height="287.69" overflow="visible" version="1.1" width="600"><g transform="translate(0,287.69)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="260.14" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">Neighborhood Structures Boundary Adjacent Neighborhood $\mathcal{B}(y)=\{x\mid
    x\prec y\}$:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="287.69" overflow="visible" version="1.1" width="600"><g transform="translate(0,287.69)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="260.14" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">邻域结构 边界相邻邻域 $\mathcal{B}(y)=\{x\mid x\prec y\}$：
- en: 'The set of $y$-connected $x$ cells of next lower rank. The neighborhood is
    specified with the boundary matrix $B_{r}$. Example: The set of nodes $x$ connected
    to edge $y$. Co-Boundary Adjacent Neighborhood $\mathcal{C}(y)=\{x\mid y\prec
    x\}$:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下一级的 $y$-连接 $x$ 单元集。邻域由边界矩阵 $B_{r}$ 指定。例如：与边缘 $y$ 连接的节点集 $x$。共同边界相邻邻域 $\mathcal{C}(y)=\{x\mid
    y\prec x\}$：
- en: 'The set of $y$-connected $x$ cells of next higher rank. The neighborhood is
    specified with the co-boundary matrix $B^{T}_{r}$. Example: The set of edges $x$
    connected to node $y$. Lower Adjacent Neighborhood $\mathcal{L}_{\downarrow}(y)=\{x\mid\exists
    z$ s.t. $z\prec y$ and $z\prec x\}$:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上一级的 $y$-连接 $x$ 单元集。邻域由共同边界矩阵 $B^{T}_{r}$ 指定。例如：与节点 $y$ 连接的边缘集 $x$。下相邻邻域 $\mathcal{L}_{\downarrow}(y)=\{x\mid\exists
    z$ 使得 $z\prec y$ 且 $z\prec x\}$：
- en: 'The set of $x$ cells that share a boundary $z$ with $y$. The neighborhood is
    specified with either the lower Laplacian matrix $L_{\downarrow,r}=B_{r}B_{r}^{T}$
    or the lower adjacency matrix $A_{\downarrow,r}=D_{r}-L_{\downarrow,r}$. Example:
    the set of edges $x$ that connect to any of the nodes $z$ that touch edge $y$.
    Upper Adjacent Neighborhood $\mathcal{L}_{\uparrow}(y)=\{x\mid\exists z$ s.t.
    $y\prec z$ and $x\prec z\}$:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与 $y$ 共享边界 $z$ 的 $x$ 单元集。邻域由下拉普拉斯矩阵 $L_{\downarrow,r}=B_{r}B_{r}^{T}$ 或下邻接矩阵
    $A_{\downarrow,r}=D_{r}-L_{\downarrow,r}$ 指定。例如：连接到触及边缘 $y$ 的任何节点 $z$ 的边缘集 $x$。上相邻邻域
    $\mathcal{L}_{\uparrow}(y)=\{x\mid\exists z$ 使得 $y\prec z$ 且 $x\prec z\}$：
- en: The set of $x$ cells that share a co-boundary $z$ with $y$. The neighborhood
    is specified with either the upper Laplacian matrix $L_{\uparrow,r}=B_{r+1}^{T}B_{r+1}$
    or the upper adjacency matrix $A_{\uparrow,r}=D_{r}-L_{\uparrow,r}$.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与 $y$ 共享共同边界 $z$ 的 $x$ 单元集。邻域由上拉普拉斯矩阵 $L_{\uparrow,r}=B_{r+1}^{T}B_{r+1}$ 或上邻接矩阵
    $A_{\uparrow,r}=D_{r}-L_{\uparrow,r}$ 指定。
- en: 'Example: The set of nodes $x$ that touch any of the edges $z$ that touch node
    $y$.</foreignobject></g></g></svg>'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：与触及节点 $y$ 的任何边缘 $z$ 触及的节点 $x$ 集合。</foreignobject></g></g></svg>
- en: II-C Message Passing
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 消息传递
- en: 'Message passing defines the computation performed by a single layer $t$ of
    the TNN. During message passing, each cell’s feature $\mathbf{h_{x}^{t,(r)}}$
    is updated to incorporate: (1) the features associated with cells in its neighborhood
    and (2) the layer’s learnable parameters denoted $\Theta^{t}$. The term “message
    passing” reflects that a signal is “traveling” through the network, passing between
    cells on paths laid out by the neighborhood structure. The output $\mathbf{h}^{t+1}$
    of layer $t$ becomes the input to layer $t+1$. In this way, deeper layers incorporate
    information from more distant cells, as information diffuses through the network.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递定义了 TNN 单层 $t$ 执行的计算。在消息传递过程中，每个单元的特征 $\mathbf{h_{x}^{t,(r)}}$ 会更新，以包含：（1）其邻域中单元的特征，以及（2）层的可学习参数
    $\Theta^{t}$。术语“消息传递”反映了信号在网络中“旅行”，在由邻域结构布置的路径上在单元之间传递。层 $t$ 的输出 $\mathbf{h}^{t+1}$
    成为层 $t+1$ 的输入。通过这种方式，较深的层将信息从更远的单元中整合进来，随着信息在网络中扩散。
- en: II-C1 The Steps of Message Passing
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 消息传递的步骤
- en: 'We decompose message passing into four steps, adopted from the framework of
    [[5](#bib.bib5)]. Each step is represented with a different color—red, orange,
    green, or blue—illustrated in Figure [8](#S2.F8 "Figure 8 ‣ II-C1 The Steps of
    Message Passing ‣ II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks").'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将消息传递分解为四个步骤，这些步骤采用了 [[5](#bib.bib5)] 的框架。每个步骤用不同的颜色表示——红色、橙色、绿色或蓝色——如图 [8](#S2.F8
    "图 8 ‣ II-C1 消息传递的步骤 ‣ II-C 消息传递 ‣ II 拓扑神经网络 ‣ 拓扑深度学习架构：消息传递拓扑神经网络的综述") 所示。
- en: '![Refer to caption](img/c44c9a046dccb95a60c0e6f6c8fbfcba.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c44c9a046dccb95a60c0e6f6c8fbfcba.png)'
- en: 'Figure 7: Neighborhood Structures: their neighborhood matrices and illustrations
    for a cell $x$ in the neighborhood of a cell $y$.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：邻域结构：单元 $x$ 在单元 $y$ 的邻域中的邻域矩阵和示意图。
- en: '![Refer to caption](img/43e0ef81dbb27691bbd2b0c0d738017e.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43e0ef81dbb27691bbd2b0c0d738017e.png)'
- en: 'Figure 8: Message passing steps: 1: Message (red), 2: Within-neighborhood aggregation
    (orange), 3: Between-neighborhood aggregation (green), 4: Update (blue). The scheme
    updates a feature $\mathbf{h}_{x}^{t,(r)}$ on a $r$-cell $x$ at layer $t$ (left
    column) into a new feature $\mathbf{h}_{x}^{t+1,(r)}$ on that same cell at the
    next layer $t+1$ (right column). Here, the scheme uses four neighborhood structures
    $\mathcal{N}_{k}$ for $k\in\{1,2,3,4\}$ (middle column). Figure adapted from [[5](#bib.bib5)].'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：信息传递步骤：1：信息（红色），2：邻域内聚合（橙色），3：邻域间聚合（绿色），4：更新（蓝色）。该方案将第 $t$ 层 $r$-单元 $x$
    上的特征 $\mathbf{h}_{x}^{t,(r)}$（左列）更新为下一个层次 $t+1$ 上相同单元的特征 $\mathbf{h}_{x}^{t+1,(r)}$（右列）。这里，该方案使用了四种邻域结构
    $\mathcal{N}_{k}$，其中 $k\in\{1,2,3,4\}$（中间列）。图源自 [[5](#bib.bib5)]。
- en: '<svg   height="346.18" overflow="visible" version="1.1" width="600"><g transform="translate(0,346.18)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="318.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">The Steps of Message Passing 1\. Message (red): First, a message
    $m_{y\rightarrow x}^{\left(r^{\prime}\rightarrow r\right)}$ travels from a $r^{\prime}$-cell
    $y$ to a $r$-cell $x$ through a neighborhood $k$ of $x$ denoted $\mathcal{N}_{k}(x)$:
    $\displaystyle{m_{y\rightarrow x}^{\left(r^{\prime}\rightarrow r\right)}}$ $\displaystyle=M_{\mathcal{N}_{k}}\left(\mathbf{h}_{x}^{t,(r)},\mathbf{h}_{y}^{t,(r^{\prime})},\Theta^{t}\right).$
    (2) via the function $M_{\mathcal{N}_{k}}$ depicted in red in Figure [8](#S2.F8
    "Figure 8 ‣ II-C1 The Steps of Message Passing ‣ II-C Message Passing ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks"). Here, $\textbf{h}_{x}^{t,(r)}$ and $\textbf{h}_{y}^{t,(r^{\prime})}$
    are features of dimension $d_{r}$ and $d_{r^{\prime}}$ on cells $y$ and $x$ respectively,
    and $\Theta^{t}$ are learnable parameters. In the simplest case, this step looks
    like a neighborhood matrix $M$ propagating a feature $\mathbf{h}_{y}^{t,(r^{\prime})}$
    on $r^{\prime}$-cell $y$ to $r$-cell $x$ as: $\displaystyle m_{y\rightarrow x}^{\left(r^{\prime}\rightarrow
    r\right)}$ $\displaystyle=M_{xy}\cdot\textbf{h}_{y}^{t,(r^{\prime})}\cdot\Theta^{t},$
    (3) where $M_{xy}$ is the scalar entry of matrix $M$ at the row corresponding
    to cell $x$ and column corresponding to cell $y$ and $m_{y\rightarrow x}^{\left(r^{\prime}\rightarrow
    r\right)}$ and $\Theta$ is a $d_{r^{\prime}}\times d_{r}$ matrix. If $y$ is not
    in the neighborhood structure of $x$, then $M_{xy}$ will be 0, and $x$ cannot
    receive any message from $y$. 2\. Within-Neighborhood Aggregation (orange): Next,
    messages are aggregated across all cells $y$ belonging to the neighborhood $\mathcal{N}_{k}(x)$:
    $\displaystyle{m_{x}^{\left(r^{\prime}\rightarrow r\right)}}$ $\displaystyle=AGG_{y\in\mathcal{N}_{k}(x)}m_{y\rightarrow
    x}^{\left(r^{\prime}\rightarrow r\right)},$ (4) resulting in the within-neighborhood
    aggregated message  $m_{x}^{\left(r^{\prime}\rightarrow r\right)}$. Here, $AGG$
    is an aggregation function, depicted in orange in Figure [8](#S2.F8 "Figure 8
    ‣ II-C1 The Steps of Message Passing ‣ II-C Message Passing ‣ II Topological Neural
    Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks"), analogous to pooling in standard convolutional
    networks. 3\. Between-Neighborhood Aggregation (green): Then, messages are aggregated
    across neighborhoods in a neighborhood set $\mathcal{N}$: $\displaystyle{m_{x}^{(r)}}$
    $\displaystyle=AGG_{\mathcal{N}_{k}\in\mathcal{N}}m_{x}^{\left(r^{\prime}\rightarrow
    r\right)},$ (5) where AGG is a (potentially different) aggregation function depicted
    in green in Figure [8](#S2.F8 "Figure 8 ‣ II-C1 The Steps of Message Passing ‣
    II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures of Topological
    Deep Learning: A Survey of Message-Passing Topological Neural Networks"), and
    $m_{x}^{(r)}$ is the message received by cell $x$ that triggers the update of
    its feature. 4\. Update (blue): Finally, the feature on cell $x$ is updated via
    a function $U$ depicted in blue in Figure [8](#S2.F8 "Figure 8 ‣ II-C1 The Steps
    of Message Passing ‣ II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks"),
    which may depend on the previous feature $\textbf{h}_{x}^{t,(r)}$ on cell $x$:
    $\displaystyle{\textbf{h}_{x}^{t+1,(r)}}$ $\displaystyle=U\left(\textbf{h}_{x}^{t,(r)},m_{x}^{(r)}\right),$
    (6) The result $\textbf{h}_{x}^{t+1,(r)}$ is the updated feature on cell $x$ that
    is input to layer $t+1$.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg   height="346.18" overflow="visible" version="1.1" width="600"><g transform="translate(0,346.18)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    width="556.69" height="318.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#000000">消息传递的步骤 1\. 消息（红色）：首先，一条消息 $m_{y\rightarrow x}^{\left(r^{\prime}\rightarrow
    r\right)}$ 从 $r^{\prime}$-单元 $y$ 通过 $x$ 的邻域 $k$ 传递到 $r$-单元 $x$，邻域 $k$ 表示为 $\mathcal{N}_{k}(x)$：$\displaystyle{m_{y\rightarrow
    x}^{\left(r^{\prime}\rightarrow r\right)}}$ $\displaystyle=M_{\mathcal{N}_{k}}\left(\mathbf{h}_{x}^{t,(r)},\mathbf{h}_{y}^{t,(r^{\prime})},\Theta^{t}\right).$（2）通过函数
    $M_{\mathcal{N}_{k}}$，该函数在图 [8](#S2.F8 "Figure 8 ‣ II-C1 The Steps of Message
    Passing ‣ II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks")
    中用红色表示。在这里，$\textbf{h}_{x}^{t,(r)}$ 和 $\textbf{h}_{y}^{t,(r^{\prime})}$ 是单元 $y$
    和 $x$ 上的维度为 $d_{r}$ 和 $d_{r^{\prime}}$ 的特征，而 $\Theta^{t}$ 是可学习的参数。在最简单的情况下，这一步看起来像一个邻域矩阵
    $M$ 将特征 $\mathbf{h}_{y}^{t,(r^{\prime})}$ 从 $r^{\prime}$-单元 $y$ 传播到 $r$-单元 $x$，公式为：$\displaystyle
    m_{y\rightarrow x}^{\left(r^{\prime}\rightarrow r\right)}$ $\displaystyle=M_{xy}\cdot\textbf{h}_{y}^{t,(r^{\prime})}\cdot\Theta^{t},$（3）其中
    $M_{xy}$ 是矩阵 $M$ 中对应单元 $x$ 的行和单元 $y$ 的列的标量条目，而 $m_{y\rightarrow x}^{\left(r^{\prime}\rightarrow
    r\right)}$ 和 $\Theta$ 是一个 $d_{r^{\prime}}\times d_{r}$ 的矩阵。如果 $y$ 不在 $x$ 的邻域结构中，则
    $M_{xy}$ 将为 0，$x$ 无法从 $y$ 接收任何消息。2\. 内部邻域聚合（橙色）：接下来，消息在所有属于邻域 $\mathcal{N}_{k}(x)$
    的单元 $y$ 中被聚合：$\displaystyle{m_{x}^{\left(r^{\prime}\rightarrow r\right)}}$ $\displaystyle=AGG_{y\in\mathcal{N}_{k}(x)}m_{y\rightarrow
    x}^{\left(r^{\prime}\rightarrow r\right)},$（4）生成内部邻域聚合的消息 $m_{x}^{\left(r^{\prime}\rightarrow
    r\right)}$。这里，$AGG$ 是一个聚合函数，在图 [8](#S2.F8 "Figure 8 ‣ II-C1 The Steps of Message
    Passing ‣ II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks")
    中用橙色表示，类似于标准卷积网络中的池化。3\. 邻域间聚合（绿色）：然后，消息在邻域集 $\mathcal{N}$ 中的邻域间聚合：$\displaystyle{m_{x}^{(r)}}$
    $\displaystyle=AGG_{\mathcal{N}_{k}\in\mathcal{N}}m_{x}^{\left(r^{\prime}\rightarrow
    r\right)},$（5）其中 AGG 是一个（可能不同的）聚合函数，在图 [8](#S2.F8 "Figure 8 ‣ II-C1 The Steps
    of Message Passing ‣ II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks")
    中用绿色表示，而 $m_{x}^{(r)}$ 是单元 $x$ 接收到的消息，触发其特征的更新。4\. 更新（蓝色）：最后，通过函数 $U$ 更新单元 $x$
    上的特征，该函数在图 [8](#S2.F8 "Figure 8 ‣ II-C1 The Steps of Message Passing ‣ II-C Message
    Passing ‣ II Topological Neural Networks ‣ Architectures of Topological Deep Learning:
    A Survey of Message-Passing Topological Neural Networks") 中用蓝色表示，可能依赖于单元 $x$ 上的先前特征
    $\textbf{h}_{x}^{t,(r)}$：$\displaystyle{\textbf{h}_{x}^{t+1,(r)}}$ $\displaystyle=U\left(\textbf{h}_{x}^{t,(r)},m_{x}^{(r)}\right),$（6）结果
    $\textbf{h}_{x}^{t+1,(r)}$ 是更新后的单元 $x$ 特征，将作为 $t+1$ 层的输入。'
- en: In this review, we decompose the structure of TNN architectures proposed in
    the literature into these four message passing steps—a unified notational framework
    adopted from [[5](#bib.bib5)] that allows us to contrast existing approaches.
    Many architectures repeat steps and/or modify their order. We note that this conceptualization
    of message passing as a local, cell-specific operation is called the spatial approach
    [[30](#bib.bib30)]. In GNNs and TNNs alike, message passing can alternatively
    be expressed in its dual spectral form, using global Fourier analysis over the
    domain. For this review, we choose to write all equations in spatial form for
    intuitiveness and generality [[7](#bib.bib7), [26](#bib.bib26), [31](#bib.bib31)].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本综述中，我们将文献中提出的TNN架构的结构分解为这四个消息传递步骤——这是一个统一的符号框架，采用了[[5](#bib.bib5)]的方法，使我们能够对比现有的方法。许多架构重复步骤和/或修改其顺序。我们注意到，将消息传递概念化为局部的、单元特定的操作被称为空间方法[[30](#bib.bib30)]。在GNN和TNN中，消息传递也可以用其对偶谱形式表达，通过对领域进行全局傅里叶分析。在本综述中，我们选择将所有方程写成空间形式，以便于理解和通用性[[7](#bib.bib7),
    [26](#bib.bib26), [31](#bib.bib31)]。
- en: II-C2 Tensor Diagrams
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 张量图
- en: 'We visually represent message passing schemes with an adapted version of the
    tensor diagram introduced in [[11](#bib.bib11)] and further developed in [[5](#bib.bib5)].
    A tensor diagram provides a graphical representation of a TNN architecture. Figure
    [9](#S2.F9 "Figure 9 ‣ II-C2 Tensor Diagrams ‣ II-C Message Passing ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks") explains the recipe for constructing a tensor diagram
    from message passing steps.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '我们用一种改编版的张量图来直观地表示消息传递方案，这种张量图在[[11](#bib.bib11)]中介绍，并在[[5](#bib.bib5)]中进一步发展。张量图提供了TNN架构的图形化表示。图[9](#S2.F9
    "Figure 9 ‣ II-C2 Tensor Diagrams ‣ II-C Message Passing ‣ II Topological Neural
    Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks")解释了从消息传递步骤构建张量图的过程。'
- en: '![Refer to caption](img/8374ef7b8b5cf9e22a82977aaf9fdd1a.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8374ef7b8b5cf9e22a82977aaf9fdd1a.png)'
- en: 'Figure 9: Tensor Diagrams: a graphical notation for the four steps of a message
    passing scheme. A diagram depicts how a feature on cell $y$ at layer $t$, $\textbf{h}_{y}^{(t)}$,
    becomes a feature on cell $x$ at layer $t+1$, $\textbf{h}_{x}^{(t+1)}$.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：张量图：消息传递方案的四个步骤的图形符号表示。图示展示了如何将层$t$上的单元$y$的特征$\textbf{h}_{y}^{(t)}$转化为层$t+1$上的单元$x$的特征$\textbf{h}_{x}^{(t+1)}$。
- en: II-C3 Types of Message Passing Functions
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 消息传递函数的类型
- en: 'The message passing function $M_{\mathcal{N}_{k}}$ employed in Step 1 is defined
    by the practitioner. There are three kinds of functions commonly used in the literature,
    as outlined in Figure [10](#S2.F10 "Figure 10 ‣ II-C3 Types of Message Passing
    Functions ‣ II-C Message Passing ‣ II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks")
    [[32](#bib.bib32)]. The variety used determines how layer parameters weight each
    incoming message from cell $y$ to cell $x$. The standard convolutional case multiplies
    each message by some learned scalar. The attentional convolutional case weights
    this multiplication depending on the features of the cells involved. The general
    case implements a potentially non-linear function that may or may not incorporate
    attention. Some schemes also make use of fixed, non-learned weights to assign
    different levels of importance to higher-order cells. Figure [10](#S2.F10 "Figure
    10 ‣ II-C3 Types of Message Passing Functions ‣ II-C Message Passing ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks") illustrates each type with tensor diagrams.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤1中使用的消息传递函数$M_{\mathcal{N}_{k}}$由从业者定义。文献中常用三种函数，如图[10](#S2.F10 "Figure 10
    ‣ II-C3 Types of Message Passing Functions ‣ II-C Message Passing ‣ II Topological
    Neural Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks") [[32](#bib.bib32)]所述。所用的变种决定了层参数如何加权每个来自单元$y$到单元$x$的消息。标准卷积情况将每条消息乘以某个学习的标量。注意力卷积情况根据相关单元的特征加权这个乘法。一般情况实现了一个可能的非线性函数，可能会或可能不会包含注意力。有些方案还使用固定的、非学习的权重来分配更高阶单元的重要性。图[10](#S2.F10
    "Figure 10 ‣ II-C3 Types of Message Passing Functions ‣ II-C Message Passing ‣
    II Topological Neural Networks ‣ Architectures of Topological Deep Learning: A
    Survey of Message-Passing Topological Neural Networks")用张量图说明了每种类型。'
- en: '![Refer to caption](img/04fe99c745afd218e78ae5489d0e3f7d.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04fe99c745afd218e78ae5489d0e3f7d.png)'
- en: 'Figure 10: Types of Message Passing Functions. In each case, a cell $x_{i}$
    (an edge) receives information from its various neighbors, cells $y_{j}$ (two
    nodes, an edge, and a face). The message received by cell $x_{i}$ from cell $y_{j}$
    is determined by a specific function $c(x_{i},y_{j})$, $a(x_{i},y_{j})$, or $g(x_{i},y_{j})$.
    Top: Each neighborhood cell $y_{j}$ sends a message to cell $x_{i}$. (Inspired
    by P. Veličković and [[32](#bib.bib32)]). Bottom: Illustration of the message-passing
    scheme above using tensor diagrams [[5](#bib.bib5)].'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '图10: 消息传递函数的类型。在每种情况下，一个单元 $x_{i}$（一个边）从其各种邻居单元 $y_{j}$（两个节点，一个边和一个面）接收信息。单元
    $x_{i}$ 从单元 $y_{j}$ 接收到的消息由特定函数 $c(x_{i},y_{j})$、$a(x_{i},y_{j})$ 或 $g(x_{i},y_{j})$
    决定。顶部: 每个邻域单元 $y_{j}$ 向单元 $x_{i}$ 发送消息。（灵感来自 P. Veličković 和 [[32](#bib.bib32)]）。底部:
    使用张量图示的上述消息传递方案的说明 [[5](#bib.bib5)]。'
- en: III Literature Review
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 文献综述
- en: 'We now review the literature on topological neural networks (TNNs) over hypergraphs,
    simplicial complexes, cellular complexes, and combinatorial complexes, using the
    conceptual framework of Section [II](#S2 "II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks").
    We summarize and compare the TNNs in terms of their architectures (Section [III-A](#S3.SS1
    "III-A Architectures ‣ III Literature Review ‣ Architectures of Topological Deep
    Learning: A Survey of Message-Passing Topological Neural Networks")), the machine
    learning tasks to which they have been applied (Section [III-B](#S3.SS2 "III-B
    Tasks ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A
    Survey of Message-Passing Topological Neural Networks")), and their geometric
    properties (Section [III-C](#S3.SS3 "III-C Symmetries and Geometric Properties
    ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks")).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在回顾了关于超图、简单复形、细胞复形和组合复形上拓扑神经网络（TNNs）的文献，使用了[II](#S2 "II Topological Neural
    Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks")节的概念框架。我们总结并比较了TNNs的架构（[III-A](#S3.SS1 "III-A Architectures
    ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks")节）、它们应用的机器学习任务（[III-B](#S3.SS2
    "III-B Tasks ‣ III Literature Review ‣ Architectures of Topological Deep Learning:
    A Survey of Message-Passing Topological Neural Networks")节）以及它们的几何属性（[III-C](#S3.SS3
    "III-C Symmetries and Geometric Properties ‣ III Literature Review ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks")节）。'
- en: III-A Architectures
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 架构
- en: 'Figure [11](#S3.F11 "Figure 11 ‣ III-A Architectures ‣ III Literature Review
    ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing Topological
    Neural Networks") summarizes TNN architectures according to the fundamental concepts
    introduced in Section [II](#S2 "II Topological Neural Networks ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks"),
    with the domain on the vertical axis, the message passing type on the horizontal
    axis, neighborhood structures and message passing equations visually represented
    with tensor diagrams. We share complete message passing equations for each architecture—decomposed
    according to the four steps introduced in Section [II-C1](#S2.SS3.SSS1 "II-C1
    The Steps of Message Passing ‣ II-C Message Passing ‣ II Topological Neural Networks
    ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing Topological
    Neural Networks") and rewritten in unifying notations —at github.com/awesome-tnns.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](#S3.F11 "Figure 11 ‣ III-A Architectures ‣ III Literature Review ‣ Architectures
    of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks")
    总结了根据[II](#S2 "II Topological Neural Networks ‣ Architectures of Topological Deep
    Learning: A Survey of Message-Passing Topological Neural Networks")节介绍的基本概念的TNN架构，纵轴为领域，横轴为消息传递类型，邻域结构和消息传递方程以张量图示的方式呈现。我们在github.com/awesome-tnns分享了每种架构的完整消息传递方程——按照[II-C1](#S2.SS3.SSS1
    "II-C1 The Steps of Message Passing ‣ II-C Message Passing ‣ II Topological Neural
    Networks ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks")节介绍的四个步骤进行分解，并使用统一符号重新编写。'
- en: '![Refer to caption](img/2be5c7aecb4f969a76371d30e7bd452e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2be5c7aecb4f969a76371d30e7bd452e.png)'
- en: 'Figure 11: Topological Neural Networks (TNNs): A Graphical Literature Review.
    We organize TNNs according to the domain (rows) and the message passing type (columns).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '图11: 拓扑神经网络（TNNs）：图形文献综述。我们根据领域（行）和消息传递类型（列）组织TNNs。'
- en: III-A1 Hypergraphs
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 超图
- en: Of the domains considered here, hypergraph neural networks have been most extensively
    researched, and have been surveyed previously [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]. Many papers in the early
    literature do not use hypergraphs as the computational domain. Rather, algorithms
    like clique-expansion [[38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)] are
    used to reduce hypergraphs to graphs, which are then processed by the model. This
    reduction adversely affects performance, as structural information is lost [[41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)]. Many such graph-based models—including HGNN
    [[44](#bib.bib44)], HyperConv[[45](#bib.bib45)], HyperGCN [[46](#bib.bib46)],
    and HNHN [[10](#bib.bib10)]—are used as benchmarks for more recent models that
    do computationally operate on hypergraphs. Here, we focus on models that preserve
    hypergraph structure during learning.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里考虑的领域中，超图神经网络已经得到了最广泛的研究，并且以前已有综述 [[33](#bib.bib33)、[34](#bib.bib34)、[35](#bib.bib35)、[36](#bib.bib36)、[37](#bib.bib37)]。早期文献中的许多论文并未使用超图作为计算领域。而是使用如
    clique-expansion [[38](#bib.bib38)、[39](#bib.bib39)、[40](#bib.bib40)] 等算法将超图简化为图，然后由模型处理。这种简化对性能产生了不利影响，因为结构信息丢失了
    [[41](#bib.bib41)、[42](#bib.bib42)、[43](#bib.bib43)]。许多这样的基于图的模型——包括 HGNN [[44](#bib.bib44)]、HyperConv
    [[45](#bib.bib45)]、HyperGCN [[46](#bib.bib46)] 和 HNHN [[10](#bib.bib10)]——被用作更近期模型的基准，这些模型在计算上直接操作超图。在这里，我们关注的是在学习过程中保持超图结构的模型。
- en: 'Many hypergraph models use a message passing scheme comprised of two phases,
    with information flowing from nodes to their hyperedges and then back to the nodes.
    We call this the two-phase scheme. The scheme appears in many tensor diagrams
    of Figure [11](#S3.F11 "Figure 11 ‣ III-A Architectures ‣ III Literature Review
    ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing Topological
    Neural Networks") where information flows from blue to pink (phase 1) and then
    from pink to blue (phase 2). The scheme is used in models with both standard and
    attentional message passing.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '许多超图模型使用由两个阶段组成的消息传递方案，其中信息从节点流向其超边，然后再流回节点。我们称之为两阶段方案。该方案出现在图 [11](#S3.F11
    "Figure 11 ‣ III-A Architectures ‣ III Literature Review ‣ Architectures of Topological
    Deep Learning: A Survey of Message-Passing Topological Neural Networks") 的许多张量图中，其中信息从蓝色流向粉色（阶段
    1），然后从粉色流向蓝色（阶段 2）。该方案用于具有标准和注意力消息传递的模型中。'
- en: Standard
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 标准
- en: 'Of those using a standard message passing, the models from [[47](#bib.bib47)],
    [[48](#bib.bib48)], [[49](#bib.bib49)], and [[9](#bib.bib9)] use the two-phase
    scheme. [[48](#bib.bib48)] is unique in using a learnable weight matrix in the
    first phase of message passing. On the second phase, [[49](#bib.bib49)] and the
    UniGCN model from [[9](#bib.bib9)] are unique in using a fixed weight matrix on
    top of learnable weights. In [[50](#bib.bib50)], [[48](#bib.bib48)], and the UniGNN,
    UniSAGE, and UniGCNII models from [[9](#bib.bib9)], the initial feature on each
    node is recurrently used to update each incoming message—denoted with a looped
    black arrow in Figure [11](#S3.F11 "Figure 11 ‣ III-A Architectures ‣ III Literature
    Review ‣ Architectures of Topological Deep Learning: A Survey of Message-Passing
    Topological Neural Networks"). We note that [[9](#bib.bib9)] systematically generalizes
    some of the most popular GNN architectures to hypergraphs with its unifying framework:
    UniGNN.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '在使用标准消息传递的模型中，[[47](#bib.bib47)]、[[48](#bib.bib48)]、[[49](#bib.bib49)] 和 [[9](#bib.bib9)]
    的模型使用了两阶段方案。[[48](#bib.bib48)] 在消息传递的第一阶段中独特地使用了可学习的权重矩阵。在第二阶段中，[[49](#bib.bib49)]
    和 [[9](#bib.bib9)] 的 UniGCN 模型在可学习的权重之上独特地使用了固定权重矩阵。在 [[50](#bib.bib50)]、[[48](#bib.bib48)]
    和 [[9](#bib.bib9)] 的 UniGNN、UniSAGE 和 UniGCNII 模型中，节点上的初始特征被反复用来更新每个传入消息——在图 [11](#S3.F11
    "Figure 11 ‣ III-A Architectures ‣ III Literature Review ‣ Architectures of Topological
    Deep Learning: A Survey of Message-Passing Topological Neural Networks") 中用循环黑色箭头表示。我们注意到，[[9](#bib.bib9)]
    系统性地将一些最流行的 GNN 架构推广到超图上，通过其统一框架：UniGNN。'
- en: 'In [[10](#bib.bib10)], fixed weights are used on both the node to hyperedge
    and hyperedge to node phases. The paper AllSet [[51](#bib.bib51)] uses a similar
    structure while incorporating fully learnable multi-set functions for neighborhood
    aggregation, which imbues its TNNs with high expressivity and generality. EHNN
    [[52](#bib.bib52)] (excluded from Figure [11](#S3.F11 "Figure 11 ‣ III-A Architectures
    ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks") for its complexity; see written
    equations) proposes a maximally expressive model using sparse symmetric tensors
    to process data on hypergraphs with uniformly sized hyperedges.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[10](#bib.bib10)]中，节点到超边和超边到节点的阶段都使用了固定权重。论文《AllSet》[[51](#bib.bib51)]采用了类似的结构，同时结合了完全可学习的多集合函数用于邻域聚合，这赋予了其TNNs高度的表达力和通用性。EHNN
    [[52](#bib.bib52)]（由于其复杂性被排除在图 [11](#S3.F11 "图 11 ‣ III-A 架构 ‣ III 文献综述 ‣ 拓扑深度学习架构：消息传递拓扑神经网络的综述")
    之外；见书面方程式）提出了一种最大表达能力的模型，使用稀疏对称张量处理具有均匀大小超边的超图数据。
- en: Attentional / General
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力 / 通用
- en: The models from [[53](#bib.bib53)], [[54](#bib.bib54)], [[48](#bib.bib48)],
    [[36](#bib.bib36)], and the UniGAT model from [[9](#bib.bib9)] employ the two-phase
    scheme in concert with attentional message passing. The architectures of [[55](#bib.bib55)]
    and [[56](#bib.bib56)] apply multi-head attention. [[31](#bib.bib31)] adapts the
    two-phase scheme in order to update node features through two parallel paths.
    [[51](#bib.bib51)] and [[52](#bib.bib52)] offer transformer-based variants of
    their standard architectures, concurrently with [[56](#bib.bib56)].
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[[53](#bib.bib53)]、[[54](#bib.bib54)]、[[48](#bib.bib48)]、[[36](#bib.bib36)]
    以及来自 [[9](#bib.bib9)] 的 UniGAT 模型采用了与注意力消息传递相结合的两阶段方案。[[55](#bib.bib55)] 和 [[56](#bib.bib56)]
    的架构应用了多头注意力。[[31](#bib.bib31)] 采用了两阶段方案，通过两个并行路径更新节点特征。[[51](#bib.bib51)] 和 [[52](#bib.bib52)]
    提供了其标准架构的基于变换器的变体，同时与 [[56](#bib.bib56)] 并行。'
- en: III-A2 Simplicial Complexes
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 单纯形复合体
- en: Simplicial complexes were first explored from a signal processing perspective
    [[57](#bib.bib57)], with initial focus on edge flows [[58](#bib.bib58), [59](#bib.bib59)],
    Hodge Laplacians [[12](#bib.bib12), [60](#bib.bib60)], and convolution [[61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63)]. As a precursor to deep learning, [[64](#bib.bib64)]
    introduced $\mathcal{L}_{\downarrow,1}$ in HodgeNet to learn convolutions on edge
    features on graphs. This contrasts with former GNN approaches processing node
    features.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从信号处理的角度首次探索了单纯形复合体[[57](#bib.bib57)]，初步关注于边流[[58](#bib.bib58), [59](#bib.bib59)]、霍奇拉普拉斯算子[[12](#bib.bib12),
    [60](#bib.bib60)] 和卷积[[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63)]。作为深度学习的前身，[[64](#bib.bib64)]
    在 HodgeNet 中引入了 $\mathcal{L}_{\downarrow,1}$ 以学习图上边特征的卷积。这与以前的 GNN 方法处理节点特征形成对比。
- en: Standard
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 标准
- en: '[[65](#bib.bib65)] (SNN) and [[66](#bib.bib66)] (SCCONV) first generalized
    the convolutional approach of [[64](#bib.bib64)] to features supported on faces
    and cells of higher ranks. Unlike HodgeNet, SNN and SCCONV use both $\mathcal{L}_{\downarrow,1}$
    and $\mathcal{L}_{\uparrow,1}$. In SNN [[65](#bib.bib65)] messages are not passed
    between adjacent ranks. By contrast, SCCONV uses independent boundary and co-boundary
    neighborhoods, hence incorporating features from adjacent ranks. [[67](#bib.bib67)]
    also makes use of this multi-neighborhood scheme for updating and classifying
    edge features. They also propose a single neighborhood scheme with an update including
    the initial cell’s feature. [[68](#bib.bib68)] and [[69](#bib.bib69)] devise schemes
    where messages coming from $\mathcal{L}_{\downarrow,1}$ and $\mathcal{L}_{\uparrow,1}$
    are weighted separately, providing greater learning flexibility. [[69](#bib.bib69)]
    allows features to travel multiple hops through the domain by using a polynomial
    form of the neighborhood structures, leveraging the simplicial convolutional filter
    from[[62](#bib.bib62)]. [[70](#bib.bib70)] extend this multiple-hop model with
    additional neighborhood structures. [[71](#bib.bib71)] used a modified version
    of $\mathcal{L}_{\downarrow}$ to find signals coiled around holes in the complex.
    BSCNet [[13](#bib.bib13)] combines node and edge-level shifting to predict links
    between nodes. This was the first model to pass messages between arbitrary ranks,
    leveraging a pseudo Hodge Laplacian.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[[65](#bib.bib65)] (SNN) 和 [[66](#bib.bib66)] (SCCONV) 首次将 [[64](#bib.bib64)]
    的卷积方法推广到支持于更高等级的面和单元上的特征。与 HodgeNet 不同，SNN 和 SCCONV 使用了 $\mathcal{L}_{\downarrow,1}$
    和 $\mathcal{L}_{\uparrow,1}$。在 SNN [[65](#bib.bib65)] 中，消息不会在相邻等级之间传递。相比之下，SCCONV
    使用独立的边界和共边界邻域，从而整合了来自相邻等级的特征。[[67](#bib.bib67)] 还利用了这种多邻域方案来更新和分类边缘特征。他们还提出了一种单邻域方案，包括初始单元的特征。[[68](#bib.bib68)]
    和 [[69](#bib.bib69)] 设计了方案，其中来自 $\mathcal{L}_{\downarrow,1}$ 和 $\mathcal{L}_{\uparrow,1}$
    的消息被分别加权，从而提供了更大的学习灵活性。[[69](#bib.bib69)] 通过使用多项式形式的邻域结构，允许特征在领域中经过多次跳跃，利用了来自
    [[62](#bib.bib62)] 的简约卷积滤波器。[[70](#bib.bib70)] 扩展了这个多跳跃模型，增加了额外的邻域结构。[[71](#bib.bib71)]
    使用了 $\mathcal{L}_{\downarrow}$ 的修改版本来寻找在复杂体中的孔周围缠绕的信号。BSCNet [[13](#bib.bib13)]
    结合了节点和边级别的移动来预测节点之间的连接。这是第一个在任意等级之间传递消息的模型，利用了伪 Hodge 拉普拉斯算子。'
- en: MPSN [[7](#bib.bib7)] explicitly details their message-passing scheme in the
    spatial domain, subsuming previous models described from a spectral approach.
    [[72](#bib.bib72)] introduces High Skip Networks (HSNs), in which each layer updates
    features through multiple sequential higher-order message passing steps, “skipping”
    it through higher ranks as a generalization of skip-connections in conventional
    neural networks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: MPSN [[7](#bib.bib7)] 在空间域中明确详细地描述了他们的消息传递方案，涵盖了之前从频谱方法描述的模型。[[72](#bib.bib72)]
    介绍了高跳跃网络（HSNs），其中每一层通过多个连续的高阶消息传递步骤来更新特征，将其“跳跃”到更高的等级，作为传统神经网络中跳跃连接的一种泛化。
- en: Attentional / General
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制 / 通用
- en: SAN [[73](#bib.bib73)], SAT [[74](#bib.bib74)], and SGAT [[75](#bib.bib75)]
    concurrently introduced attentional message passing networks on the simplicial
    domain. Each model makes use of a unique set of neighborhood structures and attention
    coefficients. SGAT is the only model as of yet developed for heterogeneous simplicial
    complexes of general rank. [[76](#bib.bib76)] introduces a variety of general
    message passing schemes with two neighborhood structures. [[7](#bib.bib7)] uses
    all four neighborhood structures, endowing each with a separate learnable matrix
    and general aggregation function.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: SAN [[73](#bib.bib73)]、SAT [[74](#bib.bib74)] 和 SGAT [[75](#bib.bib75)] 同时引入了在简约域上的注意力消息传递网络。每个模型都利用了一组独特的邻域结构和注意力系数。SGAT
    是目前唯一为一般等级的异质简约复合体开发的模型。[[76](#bib.bib76)] 介绍了多种通用消息传递方案，具有两种邻域结构。[[7](#bib.bib7)]
    使用了所有四种邻域结构，为每种结构提供了一个独立的可学习矩阵和通用聚合函数。
- en: III-A3 Cellular Complexes
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 细胞复杂体
- en: Just as for simplicial complexes, cellular complex networks have been significantly
    influenced by work in signal processing [[12](#bib.bib12), [77](#bib.bib77), [78](#bib.bib78)].
    These works demonstrated that representing data in the CC domain yields substantially
    better results than the more rigid SC domain.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 与简约复合体类似，细胞复杂网络也受到了信号处理领域工作的显著影响 [[12](#bib.bib12), [77](#bib.bib77), [78](#bib.bib78)]。这些研究表明，在
    CC 域中表示数据比在更为僵化的 SC 域中获得的结果要好得多。
- en: Standard
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 标准
- en: '[[78](#bib.bib78)] proposes theoretically possible message passing schemes
    for CCs inspired by works in the SC domain. As of yet, these models have not been
    implemented.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[[78](#bib.bib78)] 提出了理论上可能的消息传递方案，用于受SC领域研究启发的CC。迄今为止，这些模型尚未实现。'
- en: Attentional / General
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力/通用
- en: '[[26](#bib.bib26)] introduces the first TNNs to be theoretically defined on
    the CC domain. [[8](#bib.bib8)] was the first to implement and evaluate such a
    model, and demonstrated that TNNs on CCs outperform state-of-the-art graph-based
    models in expressivity and classification tests. The CAN model from [[79](#bib.bib79)]
    adapts a modified version of the message passing scheme from [[73](#bib.bib73)]
    onto the CC domain.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[[26](#bib.bib26)] 介绍了首个在CC领域理论上定义的TNN。[[8](#bib.bib8)] 是第一个实现并评估该模型的，并展示了TNN在CC上在表现力和分类测试中超越了最先进的基于图的模型。[[79](#bib.bib79)]
    的CAN模型将 [[73](#bib.bib73)] 的消息传递方案的修改版本适应于CC领域。'
- en: III-A4 Combinatorial Complexes
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 组合复杂体
- en: The combinatorial complex domain was only recently mathematically defined by
    [[11](#bib.bib11)]. This work introduces four attentional message passing schemes
    for CCCs tailored to mesh and graph classification. A more extensive analysis
    is needed to quantify the advantages of this domain over other topological domains.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 组合复杂体领域最近才由 [[11](#bib.bib11)] 在数学上定义。这项工作介绍了针对网格和图分类的四种注意力消息传递方案。需要更广泛的分析来量化该领域相对于其他拓扑领域的优势。
- en: III-B Tasks
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 任务
- en: 'Table [I](#S3.T1 "TABLE I ‣ Hypergraphs. ‣ III-C Symmetries and Geometric Properties
    ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks") reviews the tasks studied by
    each paper proposing TNNs. Tasks are first categorized into: node-level tasks
    assigning labels to nodes, as in node classification, regression or clustering;
    edge-level tasks assigning labels to edges, as in edge classification or link
    prediction; and complex-level tasks assigning labels to each complex as a whole,
    as in hypergraph classification. Tasks are additionally labeled according to their
    purpose (e.g. classification, regression, prediction). We also indicate the extent
    of benchmarking performed on each model and code availability.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [I](#S3.T1 "表 I ‣ 超图。 ‣ III-C 对称性和几何性质 ‣ III 文献综述 ‣ 拓扑深度学习架构：消息传递拓扑神经网络的综述")
    回顾了每篇提出TNN的论文研究的任务。任务首先被分类为：节点级任务，将标签分配给节点，如节点分类、回归或聚类；边级任务，将标签分配给边，如边分类或链接预测；以及复杂体级任务，将标签分配给每个复杂体作为整体，如超图分类。任务还根据其目的（例如分类、回归、预测）标记。我们还指明了每个模型的基准测试范围和代码可用性。
- en: III-C Symmetries and Geometric Properties
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 对称性和几何性质
- en: Topological domains possess symmetries and other geometric properties that should
    be respected to ensure the quality of the features learned by a TNN [[80](#bib.bib80)].
    Here, we outline such properties harnessed by models in the literature.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑领域具有对称性和其他几何性质，应予以尊重，以确保TNN学习特征的质量 [[80](#bib.bib80)]。在这里，我们概述了文献中模型利用的这些性质。
- en: Hypergraphs.
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超图。
- en: 'On hypergraphs, the following symmetries are desirable:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于超图，以下对称性是期望的：
- en: '1.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Permutation Invariance: Relabeling the nodes and applying the TNN yields an
    output that is identical to the original output obtained without relabeling. This
    requires the aggregation functions to be permutation invariant, such as a mean
    or a sum [[47](#bib.bib47), [52](#bib.bib52), [51](#bib.bib51), [10](#bib.bib10),
    [71](#bib.bib71)]. This is also called hypergraph isomorphism invariance.'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 排列不变性：重新标记节点并应用TNN，得到的输出与未重新标记时获得的原始输出相同。这要求聚合函数具有排列不变性，如均值或总和 [[47](#bib.bib47),
    [52](#bib.bib52), [51](#bib.bib51), [10](#bib.bib10), [71](#bib.bib71)]。这也称为超图同构不变性。
- en: 'TABLE I: Applications of Topological Neural Networks (TNNs). We organize papers
    according to domain and task level, task purpose, and extent of benchmark testing
    (Graph: compared to graph-based models, GNN SOTA: compared to GNN state-of-the-art,
    TNN SOTA: compared to state-of-the-art on topological domain). We exclude papers
    without implementation, and use * to indicate that an implementation has not been
    shared.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：拓扑神经网络（TNNs）的应用。我们根据领域和任务级别、任务目的以及基准测试的范围组织论文（图：与基于图的模型相比，GNN SOTA：与GNN前沿技术相比，TNN
    SOTA：与拓扑领域前沿技术相比）。我们排除了没有实现的论文，并使用*表示未共享实现。
- en: '| Domain | Model | Task Level | Task Purpose | Comparisons |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 模型 | 任务级别 | 任务目的 | 比较 |'
- en: '|  |  |  Node  |  Edge  |  Complex  |  |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 节点 | 边 | 复杂体 |  |  |'
- en: '| HG | HyperSage [[47](#bib.bib47)] | ✓ |  |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| HG | HyperSage [[47](#bib.bib47)] | ✓ |  |  |'
- en: '&#124; Classification (Inductive + Transductive) &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类（归纳 + 演绎） &#124;'
- en: '| GNN SOTA |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| GNN SOTA |'
- en: '|  | AllSet [[51](#bib.bib51)] | ✓ |  |  | Classification | TNN SOTA |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | AllSet [[51](#bib.bib51)] | ✓ |  |  | 分类 | TNN SOTA |'
- en: '|  | HyperGat [[54](#bib.bib54)] | ✓ |  |  | Classification | GNN SOTA |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | HyperGat [[54](#bib.bib54)] | ✓ |  |  | 分类 | GNN SOTA |'
- en: '|  | HNHN [[10](#bib.bib10)] | ✓ | ✓ |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | HNHN [[10](#bib.bib10)] | ✓ | ✓ |  |'
- en: '&#124; Classification, Dimensionality Reduction &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类、降维 &#124;'
- en: '| GNN SOTA |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| GNN SOTA |'
- en: '|  | HMPNN* [[31](#bib.bib31)] | ✓ |  |  | Classification | TNN SOTA |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | HMPNN* [[31](#bib.bib31)] | ✓ |  |  | 分类 | TNN SOTA |'
- en: '|  | UniGNN [[9](#bib.bib9)] | ✓ |  |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | UniGNN [[9](#bib.bib9)] | ✓ |  |  |'
- en: '&#124; Classification (Inductive + Transductive) &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类（归纳 + 演绎） &#124;'
- en: '| TNN SOTA |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| TNN SOTA |'
- en: '|  | DHGNN [[53](#bib.bib53)] | ✓ |  |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | DHGNN [[53](#bib.bib53)] | ✓ |  |  |'
- en: '&#124; Classification (Multimodal) &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类（多模态） &#124;'
- en: '| GNN SOTA |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| GNN SOTA |'
- en: '|  | EHNN [[52](#bib.bib52)] | ✓ |  |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | EHNN [[52](#bib.bib52)] | ✓ |  |  |'
- en: '&#124; Classification, Keypoint Matching &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类、关键点匹配 &#124;'
- en: '| TNN SOTA |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| TNN SOTA |'
- en: '|  | HHNN [[55](#bib.bib55)] | ✓ |  |  | Link prediction | TNN SOTA |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | HHNN [[55](#bib.bib55)] | ✓ |  |  | 链接预测 | TNN SOTA |'
- en: '|  | HTNN [[56](#bib.bib56)] | ✓ |  |  | Classification | TNN SOTA |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | HTNN [[56](#bib.bib56)] | ✓ |  |  | 分类 | TNN SOTA |'
- en: '|  | SHARE* [[36](#bib.bib36)] | ✓ |  |  | Prediction | GNN SOTA |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | SHARE* [[36](#bib.bib36)] | ✓ |  |  | 预测 | GNN SOTA |'
- en: '|  | DHGCN* [[49](#bib.bib49)] |  |  | ✓ | Classification | GNN SOTA |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | DHGCN* [[49](#bib.bib49)] |  |  | ✓ | 分类 | GNN SOTA |'
- en: '|  | HGC-RNN* [[48](#bib.bib48)] | ✓ |  |  | Prediction | GNN SOTA |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | HGC-RNN* [[48](#bib.bib48)] | ✓ |  |  | 预测 | GNN SOTA |'
- en: '| SC | MPSN [[7](#bib.bib7)] |  | ✓ | ✓ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| SC | MPSN [[7](#bib.bib7)] |  | ✓ | ✓ |'
- en: '&#124; Classification, Trajectory Classification &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类、轨迹分类 &#124;'
- en: '| GNN SOTA |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| GNN SOTA |'
- en: '|  | SCCONV [[66](#bib.bib66)] |  |  | ✓ | Classification | Graph |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | SCCONV [[66](#bib.bib66)] |  |  | ✓ | 分类 | 图 |'
- en: '|  | BScNet [[13](#bib.bib13)] |  | ✓ |  | Link prediction | GNN SOTA |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | BScNet [[13](#bib.bib13)] |  | ✓ |  | 链接预测 | GNN SOTA |'
- en: '|  | SNN [[65](#bib.bib65)] |  | ✓ |  | Imputation | None |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | SNN [[65](#bib.bib65)] |  | ✓ |  | 插补 | 无 |'
- en: '|  | SAN [[73](#bib.bib73)] |  | ✓ |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | SAN [[73](#bib.bib73)] |  | ✓ |  |'
- en: '&#124; Classification, Trajectory Classification &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类、轨迹分类 &#124;'
- en: '| TNN SOTA |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| TNN SOTA |'
- en: '|  | SAT [[74](#bib.bib74)] |  | ✓ | ✓ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | SAT [[74](#bib.bib74)] |  | ✓ | ✓ |'
- en: '&#124; Classification, Trajectory Classification &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类、轨迹分类 &#124;'
- en: '| TNN SOTA |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| TNN SOTA |'
- en: '|  | HSN* [[72](#bib.bib72)] | ✓ | ✓ | ✓ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | HSN* [[72](#bib.bib72)] | ✓ | ✓ | ✓ |'
- en: '&#124; Classification, Link prediction, Vector embedding &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类、链接预测、向量嵌入 &#124;'
- en: '| Graph |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 图 |'
- en: '|  | SCA* [[76](#bib.bib76)] |  |  | ✓ | Clustering | Graph |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | SCA* [[76](#bib.bib76)] |  |  | ✓ | 聚类 | 图 |'
- en: '|  | Dist2Cycle [[71](#bib.bib71)] |  | ✓ |  | Homology Localization | GNN
    SOTA |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | Dist2Cycle [[71](#bib.bib71)] |  | ✓ |  | 同源定位 | GNN SOTA |'
- en: '|  | SGAT [[75](#bib.bib75)] | ✓ |  |  | Classification | GNN SOTA |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | SGAT [[75](#bib.bib75)] | ✓ |  |  | 分类 | GNN SOTA |'
- en: '|  | SCoNe [[68](#bib.bib68)] |  | ✓ |  | Trajectory Classification | TNN SOTA
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | SCoNe [[68](#bib.bib68)] |  | ✓ |  | 轨迹分类 | TNN SOTA |'
- en: '|  | SCNN* [[62](#bib.bib62)] |  | ✓ |  | Imputation | TNN SOTA |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | SCNN* [[62](#bib.bib62)] |  | ✓ |  | 插补 | TNN SOTA |'
- en: '|  | SCCNN [[70](#bib.bib70)] |  | ✓ |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | SCCNN [[70](#bib.bib70)] |  | ✓ |  |'
- en: '&#124; Link prediction, Trajectory Classification &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 链接预测、轨迹分类 &#124;'
- en: '| TNN SOTA |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| TNN SOTA |'
- en: '|  | SCN [[67](#bib.bib67)] |  | ✓ |  | Classification | TNN SOTA |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | SCN [[67](#bib.bib67)] |  | ✓ |  | 分类 | TNN SOTA |'
- en: '| CC | CWN [[8](#bib.bib8)] |  | ✓ | ✓ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| CC | CWN [[8](#bib.bib8)] |  | ✓ | ✓ |'
- en: '&#124; Classification, prediction, regression &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类、预测、回归 &#124;'
- en: '| GNN SOTA |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| GNN SOTA |'
- en: '|  | CAN [[79](#bib.bib79)] |  |  | ✓ | Classification | GNN SOTA |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | CAN [[79](#bib.bib79)] |  |  | ✓ | 分类 | GNN SOTA |'
- en: '| CCC | HOAN* [[11](#bib.bib11)] |  | ✓ | ✓ | Classification | GNN SOTA |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| CCC | HOAN* [[11](#bib.bib11)] |  | ✓ | ✓ | 分类 | GNN SOTA |'
- en: '2.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Global Neighborhood Invariance: The network’s representation of a node is invariant
    to hyperedge cardinality: a hyperedge connecting many nodes is weighted the same
    as a hyperedge connecting less nodes [[47](#bib.bib47)].'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全球邻域不变性：网络对节点的表示对超边的基数是不变的：连接多个节点的超边与连接较少节点的超边具有相同的权重 [[47](#bib.bib47)]。
- en: Simplicial Complex.
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Simplicial Complex.
- en: 'For simplicial complexes, the following symmetries have been considered:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单纯形复形，考虑了以下对称性：
- en: '1.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Permutation Invariance: Invariance to node relabeling; the same as for HGs.
    [[29](#bib.bib29), [68](#bib.bib68), [7](#bib.bib7)]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 排列不变性：对节点重新标记的不变性；与HGs相同。[[29](#bib.bib29), [68](#bib.bib68), [7](#bib.bib7)]
- en: '2.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Orientation Equivariance: Changing the orientation of the simplicial complex
    (i.e. flipping the signs in the incidence matrix) re-orients the output of that
    network accordingly [[29](#bib.bib29), [68](#bib.bib68), [7](#bib.bib7)].'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定向等变性：改变单纯形复形的定向（即在关联矩阵中翻转符号）会相应地重新定向该网络的输出[[29](#bib.bib29), [68](#bib.bib68),
    [7](#bib.bib7)]。
- en: '3.'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Simplicial Locality (geometric property): In each layer, messages are only
    passed between $r$-cells and $(r\pm 1)$-cells [[29](#bib.bib29)]. If that property
    is not verified, and messages can pass between any $r$- and $r^{\prime}$-cells,
    then the network has extended simplicial locality.'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单纯形局部性（几何属性）：在每一层中，消息仅在$r$-单元和$(r\pm 1)$-单元之间传递[[29](#bib.bib29)]。如果该属性未得到验证，并且消息可以在任何$r$-单元和$r^{\prime}$-单元之间传递，则网络具有扩展的单纯形局部性。
- en: In addition, simplicial awareness can be imposed, such that message passing
    on a simplicial complex with maximum cell rank $r$ depends on every rank $r^{\prime}\leq
    r$ [[68](#bib.bib68)].
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以施加单纯形意识，使得在最大单元秩$r$的单纯形复形上的消息传递依赖于每一个秩$r^{\prime}\leq r$[[68](#bib.bib68)]。
- en: Cellular Complex and Combinatorial Complex.
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 细胞复形和组合复形。
- en: Permutation invariance is defined for CCs [[8](#bib.bib8)] and CCCs [[11](#bib.bib11)]
    just as for SCs and HGs. Beyond generalizing global neighborhood invariance to
    CCC, more research is required to understand the symmetries that can equip this
    general topological domain.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 排列不变性被定义为CCs[[8](#bib.bib8)]和CCCs[[11](#bib.bib11)]，与SCs和HGs相同。除了将全局邻域不变性推广到CCC之外，还需要更多的研究来理解能够为这一通用拓扑领域提供装备的对称性。
- en: IV Discussion
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四部分 讨论
- en: Our literature review has revealed the diversity of TNN architectures as well
    as their main axes of comparison. Looking to the future, we highlight four salient
    opportunities for development.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文献综述揭示了TNN架构的多样性及其主要比较轴。展望未来，我们突出了四个显著的发展机会。
- en: Within-Domain and Between-Domain Benchmarking.
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 域内和域间基准测试。
- en: 'Table [I](#S3.T1 "TABLE I ‣ Hypergraphs. ‣ III-C Symmetries and Geometric Properties
    ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks") shows that the domain choice
    strongly correlates with a TNN’s task level. This necessarily makes within-domain
    comparisons difficult, regardless of code sharing. We also emphasize that many
    TNNs are only benchmarked against graph-based models or early models in their
    respective domain, which makes between-domain comparisons equally difficult. As
    the field grows, improving within and between-domain benchmarking mechanisms will
    be critical to better informing model selection and quantifying progress.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '表[Ⅰ](#S3.T1 "TABLE I ‣ Hypergraphs. ‣ III-C Symmetries and Geometric Properties
    ‣ III Literature Review ‣ Architectures of Topological Deep Learning: A Survey
    of Message-Passing Topological Neural Networks") 显示了域选择与TNN的任务水平之间的强相关性。这使得域内比较变得困难，无论是否共享代码。我们还强调，许多TNN仅与基于图的模型或其各自领域中的早期模型进行基准测试，这使得域间比较同样困难。随着领域的发展，改善域内和域间基准测试机制将对更好地指导模型选择和量化进展至关重要。'
- en: TNN Architectures on General Domains.
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: TNN架构在一般领域中的应用。
- en: The diversity of implementations on HGs and SCs point to a strong potential
    for similar development in the cellular and combinatorial domains. For instance,
    only one attentional CC model has been proposed [[79](#bib.bib79)]. Moreover,
    any previously developed HG/SC/CC model can be reproduced in the CCC domain and,
    if desirable, improved with greater flexibility. Evaluating the impact of this
    added flexibility will directly characterize utility of richer topological structure
    in deep learning.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: HGs和SCs上的实现多样性表明细胞和组合领域有很大的类似发展潜力。例如，目前仅提出了一个注意力CC模型[[79](#bib.bib79)]。此外，任何之前开发的HG/SC/CC模型都可以在CCC领域中重现，并且如果需要，可以通过更大的灵活性进行改进。评估这种附加灵活性的影响将直接表征在深度学习中更丰富的拓扑结构的实用性。
- en: Connecting to the Graph Literature.
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 连接到图论文献。
- en: The HG field’s ties to the graph community has led to GNN-based advancements
    not yet propagated to other domains. A first example are dynamic domains, successful
    with HGs for tasks like pose estimation [[81](#bib.bib81)], rail transit modeling
    [[82](#bib.bib82)], and co-authorship prediction [[53](#bib.bib53)]. No work in
    other discrete domains has explored dynamism. In addition, outside of the HG domain,
    TNNs are largely implemented as homogeneous networks. This leaves room for heterogeneous
    and non-Euclidean generalizations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: HG领域与图形社区的联系导致了基于GNN的进展尚未扩展到其他领域。第一个例子是动态领域，HG在姿态估计[[81](#bib.bib81)]、轨道交通建模[[82](#bib.bib82)]和合著预测[[53](#bib.bib53)]等任务上取得了成功。其他离散领域尚未探索动态性。此外，在HG领域之外，TNNs主要作为同质网络实现。这为异质和非欧几里得的推广留出了空间。
- en: Going Deeper.
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更深入地探讨。
- en: Over-smoothing occurs when a network is too effective at aggregating signal
    over multiple layers. This leads to very similar features across cells and poor
    performance on the downstream learning task. While this issue draws attention
    in the graph community [[83](#bib.bib83), [84](#bib.bib84), [18](#bib.bib18)],
    little of this work has been generalized to TNNs, causing them to remain mostly
    shallow. UniGCNII [[9](#bib.bib9)] achieves a 64-layer deep TNN by generalizing
    over-smoothing solutions from GNNs [[85](#bib.bib85)] to the HG domain. HSNs [[72](#bib.bib72)]
    generalize skip connections to allow signal to propagate further, but are still
    implemented as shallow networks.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 过度平滑发生在网络在多个层次上过于有效地聚合信号时。这导致细胞之间特征非常相似，并且在下游学习任务上表现不佳。虽然这个问题在图形社区引起了关注[[83](#bib.bib83),
    [84](#bib.bib84), [18](#bib.bib18)]，但这些工作很少被推广到TNNs，导致它们仍然主要是浅层的。UniGCNII [[9](#bib.bib9)]通过将GNNs
    [[85](#bib.bib85)]的过度平滑解决方案推广到HG领域，实现了64层深的TNN。HSNs [[72](#bib.bib72)]将跳跃连接推广到允许信号传播更远，但仍然作为浅层网络实现。
- en: V Conclusion
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: In this work, we have provided a comprehensive, intuitive and critical view
    of the advances in TNNs through unifying notations and graphical illustrations.
    We have characterized each neural network by its choice of data domain and its
    model, which we further specify through choice of neighboring structure(s) and
    message-passing scheme. We hope that this review will make this rich body of work
    more accessible to practitioners whose fields would benefit from topology-sensitive
    deep learning.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们通过统一的符号和图示提供了对TNNs进展的全面、直观和批判性视角。我们通过数据领域和模型的选择来表征每个神经网络，并进一步通过选择邻接结构和消息传递方案来加以说明。我们希望这篇综述能使这些丰富的工作更容易被那些领域受益于拓扑敏感深度学习的从业者所接受。
- en: Acknowledgments
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the National Science Foundation Grant Number 2134241.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作得到了国家科学基金会资助，资助号2134241。
- en: VI References Section
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 参考文献部分
- en: References
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] D. Knoke and S. Yang, *Social network analysis*.   SAGE publications, 2019.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] D. Knoke 和 S. Yang，*社会网络分析*。SAGE 出版社，2019年。'
- en: '[2] K. Jha, S. Saha, and H. Singh, “Prediction of protein–protein interaction
    using graph neural networks,” *Scientific Reports*, vol. 12, no. 1, pp. 1–12,
    2022.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] K. Jha, S. Saha, 和 H. Singh，“使用图神经网络预测蛋白质–蛋白质相互作用”，*Scientific Reports*，第12卷，第1期，页码1–12，2022年。'
- en: '[3] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veličković, “Geometric deep
    learning: Grids, groups, graphs, geodesics, and gauges,” *arXiv preprint arXiv:2104.13478*,
    2021.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. M. Bronstein, J. Bruna, T. Cohen, 和 P. Veličković，“几何深度学习：网格、群体、图形、测地线和量规”，*arXiv
    预印本 arXiv:2104.13478*，2021年。'
- en: '[4] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and
    M. Sun, “Graph neural networks: A review of methods and applications,” *AI Open*,
    vol. 1, pp. 57–81, 2020.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, 和 M.
    Sun，“图神经网络：方法和应用综述”，*AI Open*，第1卷，页码57–81，2020年。'
- en: '[5] M. Hajij, G. Zamzmi, T. Papamarkou, N. Miolane, A. Guzmán-Sáenz, K. N.
    Ramamurthy, T. Birdal, T. Dey, S. Mukherjee, S. Samaga, N. Livesay, R. Walters,
    P. Rosen, and M. Schaub, “Topological deep learning: Going beyond graph data,”
    *arXiv preprint arXiv:1906.09068 (v3)*, 2023.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Hajij, G. Zamzmi, T. Papamarkou, N. Miolane, A. Guzmán-Sáenz, K. N.
    Ramamurthy, T. Birdal, T. Dey, S. Mukherjee, S. Samaga, N. Livesay, R. Walters,
    P. Rosen, 和 M. Schaub，“拓扑深度学习：超越图数据”，*arXiv 预印本 arXiv:1906.09068 (v3)*，2023年。'
- en: '[6] C. Bodnar, “Topological deep learning: Graphs, complexes, sheaves,” Ph.D.
    dissertation, Apollo - University of Cambridge Repository, 2022\. [Online]. Available:
    https://www.repository.cam.ac.uk/handle/1810/350982'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C. Bodnar，“拓扑深度学习：图形、复形、层叠”，博士论文，Apollo - 剑桥大学存储库，2022年。[在线]. 可用: https://www.repository.cam.ac.uk/handle/1810/350982'
- en: '[7] C. Bodnar, F. Frasca, Y. Wang, N. Otter, G. F. Montufar, P. Lio, and M. Bronstein,
    “Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks,” in
    *International Conference on Machine Learning*.   PMLR, 2021, pp. 1026–1037.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. Bodnar, F. Frasca, Y. Wang, N. Otter, G. F. Montufar, P. Lio 和 M. Bronstein，“Weisfeiler和Lehman走向拓扑：消息传递简单复形网络，”在*国际机器学习会议*。PMLR，2021年，页码1026–1037。'
- en: '[8] C. Bodnar, F. Frasca, N. Otter, Y. Wang, P. Lio, G. F. Montufar, and M. Bronstein,
    “Weisfeiler and Lehman Go Cellular: CW Networks,” *Advances in Neural Information
    Processing Systems*, vol. 34, pp. 2625–2640, 2021.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. Bodnar, F. Frasca, N. Otter, Y. Wang, P. Lio, G. F. Montufar 和 M. Bronstein，“Weisfeiler和Lehman走向细胞：CW网络，”*神经信息处理系统进展*，第34卷，页码2625–2640，2021年。'
- en: '[9] J. Huang and J. Yang, “Unignn: a unified framework for graph and hypergraph
    neural networks,” in *Proceedings of the Thirtieth International Joint Conference
    on Artificial Intelligence, IJCAI-21*, 2021.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] J. Huang 和 J. Yang，“Unignn: 图神经网络与超图神经网络的统一框架，”在*第三十届国际联合人工智能会议论文集，IJCAI-21*，2021年。'
- en: '[10] Y. Dong, W. Sawin, and Y. Bengio, “Hnhn: Hypergraph networks with hyperedge
    neurons,” *ICML Graph Representation Learning and Beyond Workshop*, 2020\. [Online].
    Available: https://arxiv.org/abs/2006.12278'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Dong, W. Sawin 和 Y. Bengio，“Hnhn: 带有超边神经元的超图网络，”*ICML图表示学习及其他研讨会*，2020年。[在线]
    可用：https://arxiv.org/abs/2006.12278'
- en: '[11] M. Hajij, G. Zamzmi, T. Papamarkou, N. Miolane, A. Guzmán-Sáenz, and K. N.
    Ramamurthy, “Higher-order attention networks,” *arXiv preprint arXiv:2206.00606
    (v1)*, 2022.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Hajij, G. Zamzmi, T. Papamarkou, N. Miolane, A. Guzmán-Sáenz 和 K. N.
    Ramamurthy，“高阶注意力网络，”*arXiv预印本arXiv:2206.00606 (v1)*，2022年。'
- en: '[12] S. Barbarossa and S. Sardellitti, “Topological signal processing over
    simplicial complexes,” *IEEE Transactions on Signal Processing*, vol. 68, pp.
    2992–3007, 2020.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. Barbarossa 和 S. Sardellitti，“简单复形上的拓扑信号处理，”*IEEE信号处理学报*，第68卷，页码2992–3007，2020年。'
- en: '[13] Y. Chen, Y. R. Gel, and H. V. Poor, “Bscnets: Block simplicial complex
    neural networks,” in *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 36, 2022, pp. 6333–6341.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Chen, Y. R. Gel 和 H. V. Poor，“Bscnets: 块简单复形神经网络，”在*AAAI人工智能会议论文集*，第36卷，2022年，页码6333–6341。'
- en: '[14] L. Torres, A. S. Blevins, D. Bassett, and T. Eliassi-Rad, “The why, how,
    and when of representations for complex systems,” *SIAM Review*, vol. 63, no. 3,
    pp. 435–485, 2021.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] L. Torres, A. S. Blevins, D. Bassett 和 T. Eliassi-Rad，“复杂系统表示的原因、方法和时机，”*SIAM评论*，第63卷，第3期，页码435–485，2021年。'
- en: '[15] F. Battiston, E. Amico, A. Barrat, G. Bianconi, G. Ferraz de Arruda, B. Franceschiello,
    I. Iacopini, S. Kéfi, V. Latora, Y. Moreno *et al.*, “The physics of higher-order
    interactions in complex systems,” *Nature Physics*, vol. 17, no. 10, pp. 1093–1098,
    2021.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] F. Battiston, E. Amico, A. Barrat, G. Bianconi, G. Ferraz de Arruda, B.
    Franceschiello, I. Iacopini, S. Kéfi, V. Latora, Y. Moreno *等*，“复杂系统中高阶交互的物理学，”*自然物理学*，第17卷，第10期，页码1093–1098，2021年。'
- en: '[16] F. Hensel, M. Moor, and B. Rieck, “A survey of topological machine learning
    methods,” *Frontiers in Artificial Intelligence*, vol. 4, 2021\. [Online]. Available:
    https://www.frontiersin.org/articles/10.3389/frai.2021.681108'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] F. Hensel, M. Moor 和 B. Rieck，“拓扑机器学习方法综述，”*人工智能前沿*，第4卷，2021年。[在线] 可用：https://www.frontiersin.org/articles/10.3389/frai.2021.681108'
- en: '[17] R. Yang, F. Sala, and P. Bogdan, “Efficient representation learning for
    higher-order data with simplicial complexes,” in *Proceedings of the First Learning
    on Graphs Conference*, ser. Proceedings of Machine Learning Research, B. Rieck
    and R. Pascanu, Eds., vol. 198.   PMLR, 09–12 Dec 2022, pp. 13:1–13:21.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] R. Yang, F. Sala 和 P. Bogdan，“高阶数据的高效表示学习与简单复形，”在*首届图学习会议论文集*，机器学习研究论文集，B.
    Rieck 和 R. Pascanu 编，卷198。PMLR，2022年12月09–12日，页码13:1–13:21。'
- en: '[18] T. K. Rusch, M. M. Bronstein, and S. Mishra, “A survey on oversmoothing
    in graph neural networks,” 2023.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] T. K. Rusch, M. M. Bronstein 和 S. Mishra，“图神经网络中的过度平滑调查，”2023年。'
- en: '[19] P. Veličković, “Message passing all the way up,” *arXiv preprint arXiv:2202.11097*,
    2022.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] P. Veličković，“消息传递一路到底，”*arXiv预印本arXiv:2202.11097*，2022年。'
- en: '[20] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social
    representations,” in *Proceedings of the 20th ACM SIGKDD international conference
    on Knowledge discovery and data mining*, 2014, pp. 701–710.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] B. Perozzi, R. Al-Rfou 和 S. Skiena，“Deepwalk: 社会表征的在线学习，”在*第20届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，2014年，页码701–710。'
- en: '[21] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,”
    in *Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery
    and data mining*, 2016, pp. 855–864.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Grover 和 J. Leskovec, “node2vec: 可扩展的网络特征学习方法，” 出自 *第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集*,
    2016, 页码 855–864.'
- en: '[22] A. Sharma, S. Joty, H. Kharkwal, and J. Srivastava, “Hyperedge2vec: Distributed
    representations for hyperedges,” 2018.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Sharma, S. Joty, H. Kharkwal, 和 J. Srivastava, “Hyperedge2vec: 超边分布式表示方法,”
    2018.'
- en: '[23] J. Payne, “Deep hyperedges: a framework for transductive and inductive
    learning on hypergraphs,” *arXiv preprint arXiv:1910.02633*, 2019.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Payne, “深度超边：超图的跨式学习和感知学习框架,” *arXiv预印本 arXiv:1910.02633*, 2019.'
- en: '[24] J. C. W. Billings, M. Hu, G. Lerda, A. N. Medvedev, F. Mottes, A. Onicas,
    A. Santoro, and G. Petri, “Simplex2vec embeddings for community detection in simplicial
    complexes,” *arXiv preprint arXiv:1906.09068*, 2019.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. C. W. Billings, M. Hu, G. Lerda, A. N. Medvedev, F. Mottes, A. Onicas,
    A. Santoro, 和 G. Petri, “用于单纯形复合体社区检测的Simplex2vec嵌入方法,” *arXiv预印本 arXiv:1906.09068*,
    2019.'
- en: '[25] C. Hacker, “k-simplex2vec: a simplicial extension of node2vec,” *arXiv
    preprint arXiv:2010.05636*, 2020.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] C. Hacker, “k-simplex2vec: node2vec的单纯形扩展方法,” *arXiv预印本 arXiv:2010.05636*,
    2020.'
- en: '[26] M. Hajij, K. Istvan, and G. Zamzmi, “Cell Complex Neural Networks,” *NeurIPS
    2020 Workshop TDA and Beyond*, 2020.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Hajij, K. Istvan, 和 G. Zamzmi, “单纯形复合体神经网络,” *NeurIPS 2020 研讨会 TDA
    and Beyond*, 2020.'
- en: '[27] M. Aschbacher, “Combinatorial cell complexes,” in *Progress in Algebraic
    Combinatorics*.   Mathematical Society of Japan, 1996, pp. 1–80.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. Aschbacher, “组合单纯形复合体,” 出自 *代数组合学进展*.   日本数学学会, 1996, 页码 1–80.'
- en: '[28] R. Klette, “Cell complexes through time,” in *Vision Geometry IX*, vol.
    4117.   SPIE, 2000, pp. 134–145.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. Klette, “时空中的单纯形复合体,” 出自 *视觉几何学IX*, vol. 4117.   SPIE, 2000, 页码 134–145.'
- en: '[29] M. T. Schaub, Y. Zhu, J.-B. Seby, T. M. Roddenberry, and S. Segarra, “Signal
    processing on higher-order networks: Livin’on the edge… and beyond,” *Signal Processing*,
    vol. 187, p. 108149, 2021.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. T. Schaub, Y. Zhu, J.-B. Seby, T. M. Roddenberry, 和 S. Segarra, “高阶网络信号处理：生活在边缘并超越”，*信号处理*,
    vol. 187, 页码 108149, 2021.'
- en: '[30] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
    “Neural message passing for quantum chemistry,” in *International conference on
    machine learning*.   PMLR, 2017, pp. 1263–1272.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, 和 G. E. Dahl, “量子化学的神经信息传递方法”，
    出自 *国际机器学习会议*.   PMLR, 2017, 页码 1263–1272.'
- en: '[31] S. Heydari and L. Livi, “Message passing neural networks for hypergraphs,”
    in *Proceedings of 31st International Conference on Artificial Neural Networks,
    Part II*, ser. Lecture Notes in Computer Science, E. Pimenidis, P. P. Angelov,
    C. Jayne, A. Papaleonidas, and M. Aydin, Eds., vol. 13530.   Springer, 2022, pp.
    583–592.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Heydari 和 L. Livi, “超图的消息传递神经网络,” 出自 *第31届国际人工神经网络会议第II部分*, 编者 E. Pimenidis,
    P. P. Angelov, C. Jayne, A. Papaleonidas, 和 M. Aydin, vol. 13530.   Springer,
    2022, 页码 583–592.'
- en: '[32] M. Bronstein, “Beyond message passing: a physics-inspired paradigm for
    graph neural networks,” May 2022\. [Online]. Available: https://thegradient.pub/graph-neural-networks-beyond-message-passing-and-weisfeiler-lehman/'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. Bronstein, “超越信息传递：图神经网络的物理启发范式”， 2022年5月。[在线]. 可访问：https://thegradient.pub/graph-neural-networks-beyond-message-passing-and-weisfeiler-lehman/'
- en: '[33] T. Ling, Z. Jinchuan, Z. Jinhao, Z. Wangtao, and Z. Xue, “A review of
    knowledge graphs: Representation, construction, reasoning and knowledge hypergraph
    theory [j],” *Computer Applications*, vol. 41, no. 08, pp. 2161–2186, 2021.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] T. Ling, Z. Jinchuan, Z. Jinhao, Z. Wangtao, 和 Z. Xue, “知识图谱综述：表示、构建、推理和知识超图理论
    [j],” *计算机应用*, vol. 41, no. 08, 页码 2161–2186, 2021.'
- en: '[34] Y. Gao, Z. Zhang, H. Lin, X. Zhao, S. Du, and C. Zou, “Hypergraph learning:
    Methods and practices,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 44, no. 5, pp. 2548–2566, 2022.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Y. Gao, Z. Zhang, H. Lin, X. Zhao, S. Du, 和 C. Zou, “超图学习：方法与实践,” *IEEE模式分析与机器智能交易*,
    vol. 44, no. 5, 页码 2548–2566, 2022.'
- en: '[35] B.-D. Hu, X.-G. Wang, X.-Y. Wang, M.-L. Song, and C. Chen, “Survey on
    hypergraph learning: Algorithm classification and application analysis,” *Journal
    of Software*, vol. 33, no. 2, pp. 498–523, 2021.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] B.-D. Hu, X.-G. Wang, X.-Y. Wang, M.-L. Song, 和 C. Chen, “超图学习综述：算法分类与应用分析,”
    *软件杂志*, vol. 33, no. 2, 页码 498–523, 2021.'
- en: '[36] J. Wang, K. Ding, Z. Zhu, and J. Caverlee, “Session-based recommendation
    with hypergraph attention networks,” in *Proceedings of the 2021 SIAM International
    Conference on Data Mining (SDM)*.   SIAM, 2021, pp. 82–90.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Wang, K. Ding, Z. Zhu, 和 J. Caverlee, “基于超图注意力网络的基于会话的推荐方法,” 出自 *2021
    SIAM国际数据挖掘会议论文集 (SDM)*.   SIAM, 2021, 页码 82–90.'
- en: '[37] M. T. Fischer, A. Frings, D. A. Keim, and D. Seebacher, “Towards a survey
    on static and dynamic hypergraph visualizations,” in *2021 IEEE visualization
    conference (VIS)*.   IEEE, 2021, pp. 81–85.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] M. T. Fischer, A. Frings, D. A. Keim, 和 D. Seebacher, “静态和动态超图可视化调查，”
    在 *2021 IEEE 可视化会议（VIS）* 中。 IEEE，2021，第 81–85 页。'
- en: '[38] J. Y. Zien, M. D. Schlag, and P. K. Chan, “Multilevel spectral hypergraph
    partitioning with arbitrary vertex sizes,” *IEEE Transactions on computer-aided
    design of integrated circuits and systems*, vol. 18, no. 9, pp. 1389–1399, 1999.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. Y. Zien, M. D. Schlag, 和 P. K. Chan, “具有任意顶点大小的多级光谱超图分区，” *IEEE 计算机辅助集成电路与系统设计学报*，第
    18 卷，第 9 期，第 1389–1399 页，1999。'
- en: '[39] S. Agarwal, J. Lim, L. Zelnik-Manor, P. Perona, D. Kriegman, and S. Belongie,
    “Beyond pairwise clustering,” in *2005 IEEE Computer Society Conference on Computer
    Vision and Pattern Recognition (CVPR’05)*, vol. 2.   IEEE, 2005, pp. 838–845.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Agarwal, J. Lim, L. Zelnik-Manor, P. Perona, D. Kriegman, 和 S. Belongie,
    “超越成对聚类，” 在 *2005 IEEE 计算机学会计算机视觉与模式识别会议（CVPR’05）* 中，第 2 卷。 IEEE，2005，第 838–845
    页。'
- en: '[40] D. Zhou, J. Huang, and B. Schölkopf, “Learning with hypergraphs: Clustering,
    classification, and embedding,” *Advances in neural information processing systems*,
    vol. 19, 2006.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] D. Zhou, J. Huang, 和 B. Schölkopf, “超图学习：聚类、分类和嵌入，” *神经信息处理系统进展*，第 19
    卷，2006。'
- en: '[41] M. Hein, S. Setzer, L. Jost, and S. S. Rangapuram, “The total variation
    on hypergraphs-learning on hypergraphs revisited,” *Advances in Neural Information
    Processing Systems*, vol. 26, 2013.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. Hein, S. Setzer, L. Jost, 和 S. S. Rangapuram, “超图上的总变差——超图学习再探，” *神经信息处理系统进展*，第
    26 卷，2013。'
- en: '[42] G. Li, L. Qi, and G. Yu, “The z-eigenvalues of a symmetric tensor and
    its application to spectral hypergraph theory,” *Numerical Linear Algebra with
    Applications*, vol. 20, no. 6, pp. 1001–1029, 2013.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] G. Li, L. Qi, 和 G. Yu, “对称张量的 z-eigenvalues 及其在光谱超图理论中的应用，” *Numerical
    Linear Algebra with Applications*，第 20 卷，第 6 期，第 1001–1029 页，2013。'
- en: '[43] I. E. Chien, H. Zhou, and P. Li, “$hs2$: Active learning over hypergraphs
    with pointwise and pairwise queries,” in *The 22nd International Conference on
    Artificial Intelligence and Statistics*.   PMLR, 2019, pp. 2466–2475.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] I. E. Chien, H. Zhou, 和 P. Li, “$hs2$: 基于超图的主动学习，涉及逐点和成对查询，” 在 *第 22 届人工智能与统计国际会议*
    中。 PMLR，2019，第 2466–2475 页。'
- en: '[44] Y. Feng, H. You, Z. Zhang, R. Ji, and Y. Gao, “Hypergraph neural networks,”
    in *Proceedings of the AAAI conference on artificial intelligence*, vol. 33, 2019,
    pp. 3558–3565.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Feng, H. You, Z. Zhang, R. Ji, 和 Y. Gao, “超图神经网络，” 在 *AAAI 人工智能会议论文集*
    中，第 33 卷，2019，第 3558–3565 页。'
- en: '[45] S. Bai, F. Zhang, and P. H. Torr, “Hypergraph convolution and hypergraph
    attention,” *Pattern Recognition*, vol. 110, p. 107637, 2021.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] S. Bai, F. Zhang, 和 P. H. Torr, “超图卷积和超图注意力，” *模式识别*，第 110 卷，第 107637
    页，2021。'
- en: '[46] N. Yadati, M. Nimishakavi, P. Yadav, V. Nitin, A. Louis, and P. Talukdar,
    “Hypergcn: A new method for training graph convolutional networks on hypergraphs,”
    *Advances in neural information processing systems*, vol. 32, 2019.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] N. Yadati, M. Nimishakavi, P. Yadav, V. Nitin, A. Louis, 和 P. Talukdar,
    “Hypergcn：一种用于超图的图卷积网络训练的新方法，” *神经信息处理系统进展*，第 32 卷，2019。'
- en: '[47] D. Arya, D. K. Gupta, S. Rudinac, and M. Worring, “Hypersage: Generalizing
    inductive representation learning on hypergraphs,” *arXiv preprint arXiv:2010.04558*,
    2020.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] D. Arya, D. K. Gupta, S. Rudinac, 和 M. Worring, “Hypersage：在超图上的归纳表示学习的推广，”
    *arXiv 预印本 arXiv:2010.04558*，2020。'
- en: '[48] J. Yi and J. Park, “Hypergraph convolutional recurrent neural network,”
    in *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*, 2020, pp. 3366–3376.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. Yi 和 J. Park, “超图卷积递归神经网络，” 在 *第 26 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*
    中，2020，第 3366–3376 页。'
- en: '[49] J. Wei, Y. Wang, M. Guo, P. Lv, X. Yang, and M. Xu, “Dynamic hypergraph
    convolutional networks for skeleton-based action recognition,” *arXiv preprint
    arXiv:2112.10570*, 2021.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] J. Wei, Y. Wang, M. Guo, P. Lv, X. Yang, 和 M. Xu, “基于骨架的动作识别的动态超图卷积网络，”
    *arXiv 预印本 arXiv:2112.10570*，2021。'
- en: '[50] D. Arya, S. Rudinac, and M. Worring, “Hyperlearn: a distributed approach
    for representation learning in datasets with many modalities,” in *Proceedings
    of the 27th ACM International Conference on Multimedia*, 2019, pp. 2245–2253.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] D. Arya, S. Rudinac, 和 M. Worring, “Hyperlearn：一种用于多模态数据集的分布式表示学习方法，”
    在 *第 27 届 ACM 国际多媒体会议论文集* 中，2019，第 2245–2253 页。'
- en: '[51] E. Chien, C. Pan, J. Peng, and O. Milenkovic, “You are allset: A multiset
    function framework for hypergraph neural networks,” in *International Conference
    on Learning Representations*, 2022\. [Online]. Available: https://openreview.net/forum?id=hpBTIv2uy_E'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] E. Chien, C. Pan, J. Peng, 和 O. Milenkovic，“你已经准备好了：一个用于超图神经网络的多集函数框架，”
    收录于 *国际学习表征会议*，2022年。 [在线]. 可用: https://openreview.net/forum?id=hpBTIv2uy_E'
- en: '[52] J. Kim, S. Oh, S. Cho, and S. Hong, “Equivariant hypergraph neural networks,”
    in *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October
    23–27, 2022, Proceedings, Part XXI*.   Springer, 2022, pp. 86–103.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Kim, S. Oh, S. Cho, 和 S. Hong，“等变超图神经网络，” 收录于 *计算机视觉–ECCV 2022: 第17届欧洲会议，特拉维夫，以色列，2022年10月23–27日，论文集，第XXI部分*。Springer，2022年，第86–103页。'
- en: '[53] J. Jiang, Y. Wei, Y. Feng, J. Cao, and Y. Gao, “Dynamic hypergraph neural
    networks,” in *Proceedings of International Joint Conferences on Artificial Intelligence*,
    2019.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Jiang, Y. Wei, Y. Feng, J. Cao, 和 Y. Gao，“动态超图神经网络，” 收录于 *国际人工智能联合会议论文集*，2019年。'
- en: '[54] K. Ding, J. Wang, J. Li, D. Li, and H. Liu, “Be more with less: Hypergraph
    attention networks for inductive text classification,” in *Proceedings of the
    2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.   Online:
    Association for Computational Linguistics, Nov. 2020, pp. 4927–4936\. [Online].
    Available: https://aclanthology.org/2020.emnlp-main.399'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] K. Ding, J. Wang, J. Li, D. Li, 和 H. Liu，“以少胜多：用于归纳文本分类的超图注意力网络，” 收录于
    *2020年自然语言处理实证方法会议（EMNLP）论文集*。在线：计算语言学协会，2020年11月，第4927–4936页。[在线]. 可用: https://aclanthology.org/2020.emnlp-main.399'
- en: '[55] Y. Li, Z. Fan, J. Zhang, D. Shi, T. Xu, D. Yin, J. Deng, and X. Song,
    “Heterogeneous hypergraph neural network for friend recommendation with human
    mobility,” in *Proceedings of the 31st ACM International Conference on Information
    & Knowledge Management*, 2022, pp. 4209–4213.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Li, Z. Fan, J. Zhang, D. Shi, T. Xu, D. Yin, J. Deng, 和 X. Song，“用于好友推荐的异质超图神经网络与人类迁徙，”
    收录于 *第31届ACM国际信息与知识管理会议论文集*，2022年，第4209–4213页。'
- en: '[56] M. Li, Y. Zhang, X. Li, Y. Zhang, and B. Yin, “Hypergraph transformer
    neural networks,” *ACM Transactions on Knowledge Discovery from Data (TKDD)*,
    2022.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] M. Li, Y. Zhang, X. Li, Y. Zhang, 和 B. Yin，“超图变换器神经网络，” *ACM数据知识发现学报（TKDD）*，2022年。'
- en: '[57] F. Battiston, G. Cencetti, I. Iacopini, V. Latora, M. Lucas, A. Patania,
    J.-G. Young, and G. Petri, “Networks beyond pairwise interactions: structure and
    dynamics,” *Physics Reports*, vol. 874, pp. 1–92, 2020.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] F. Battiston, G. Cencetti, I. Iacopini, V. Latora, M. Lucas, A. Patania,
    J.-G. Young, 和 G. Petri，“超越配对交互的网络：结构与动态，” *物理学报告*，第874卷，第1–92页，2020年。'
- en: '[58] X. Jiang, L.-H. Lim, Y. Yao, and Y. Ye, “Statistical ranking and combinatorial
    hodge theory,” *Mathematical Programming*, vol. 127, no. 1, pp. 203–244, 2011.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] X. Jiang, L.-H. Lim, Y. Yao, 和 Y. Ye，“统计排序和组合Hodge理论，” *数学编程*，第127卷，第1期，第203–244页，2011年。'
- en: '[59] M. T. Schaub and S. Segarra, “Flow smoothing and denoising: Graph signal
    processing in the edge-space,” in *2018 IEEE Global Conference on Signal and Information
    Processing (GlobalSIP)*.   IEEE, 2018, pp. 735–739.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. T. Schaub 和 S. Segarra，“流量平滑和去噪：边缘空间中的图信号处理，” 收录于 *2018年IEEE全球信号与信息处理大会（GlobalSIP）*。IEEE，2018年，第735–739页。'
- en: '[60] M. T. Schaub, A. R. Benson, P. Horn, G. Lippner, and A. Jadbabaie, “Random
    walks on simplicial complexes and the normalized hodge 1-laplacian,” *SIAM Review*,
    vol. 62, no. 2, pp. 353–391, 2020.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] M. T. Schaub, A. R. Benson, P. Horn, G. Lippner, 和 A. Jadbabaie，“简化复形上的随机游走与标准化Hodge
    1-Laplacian，” *SIAM评论*，第62卷，第2期，第353–391页，2020年。'
- en: '[61] M. Yang, E. Isufi, M. T. Schaub, and G. Leus, “Finite impulse response
    filters for simplicial complexes,” in *2021 29th European Signal Processing Conference
    (EUSIPCO)*.   IEEE, 2021, pp. 2005–2009.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] M. Yang, E. Isufi, M. T. Schaub, 和 G. Leus，“用于简化复形的有限冲激响应滤波器，” 收录于 *2021年第29届欧洲信号处理会议（EUSIPCO）*。IEEE，2021年，第2005–2009页。'
- en: '[62] ——, “Simplicial convolutional filters,” *IEEE Transactions on Signal Processing*,
    vol. 70, pp. 4633–4648, 2022.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] ——, “简化卷积滤波器，” *IEEE信号处理学报*，第70卷，第4633–4648页，2022年。'
- en: '[63] E. Isufi and M. Yang, “Convolutional filtering in simplicial complexes,”
    in *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*.   IEEE, 2022, pp. 5578–5582.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] E. Isufi 和 M. Yang，“简化复形中的卷积滤波，” 收录于 *ICASSP 2022-2022 IEEE国际声学、语音与信号处理会议（ICASSP）*。IEEE，2022年，第5578–5582页。'
- en: '[64] T. M. Roddenberry and S. Segarra, “Hodgenet: Graph neural networks for
    edge data,” in *2019 53rd Asilomar Conference on Signals, Systems, and Computers*.   IEEE,
    2019, pp. 220–224.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] T. M. Roddenberry 和 S. Segarra, “Hodgenet：用于边缘数据的图神经网络，” 见于 *2019年第53届Asilomar信号、系统与计算会议*。
    IEEE，2019年，第220–224页。'
- en: '[65] S. Ebli, M. Defferrard, and G. Spreemann, “Simplicial neural networks,”
    in *TDA & Beyond*, 2020.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. Ebli, M. Defferrard, 和 G. Spreemann, “简化神经网络，” 见于 *TDA & Beyond*，2020年。'
- en: '[66] E. Bunch, Q. You, G. Fung, and V. Singh, “Simplicial 2-complex convolutional
    neural networks,” in *TDA & Beyond*, 2020.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] E. Bunch, Q. You, G. Fung, 和 V. Singh, “简化2-复形卷积神经网络，” 见于 *TDA & Beyond*，2020年。'
- en: '[67] R. Yang, F. Sala, and P. Bogdan, “Efficient representation learning for
    higher-order data with simplicial complexes,” in *Learning on Graphs Conference*.   PMLR,
    2022, pp. 13–1.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] R. Yang, F. Sala, 和 P. Bogdan, “针对高阶数据的高效表示学习与简化复形，” 见于 *图学习会议*。 PMLR，2022年，第13–1页。'
- en: '[68] T. M. Roddenberry, N. Glaze, and S. Segarra, “Principled simplicial neural
    networks for trajectory prediction,” in *International Conference on Machine Learning*.   PMLR,
    2021, pp. 9020–9029.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] T. M. Roddenberry, N. Glaze, 和 S. Segarra, “用于轨迹预测的有原则的简化神经网络，” 见于 *国际机器学习会议*。
    PMLR，2021年，第9020–9029页。'
- en: '[69] M. Yang, E. Isufi, and G. Leus, “Simplicial convolutional neural networks,”
    in *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*.   IEEE, 2022, pp. 8847–8851.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M. Yang, E. Isufi, 和 G. Leus, “简化卷积神经网络，” 见于 *ICASSP 2022-2022 IEEE国际声学、语音与信号处理会议（ICASSP）*。
    IEEE，2022年，第8847–8851页。'
- en: '[70] M. Yang and E. Isufi, “Convolutional learning on simplicial complexes,”
    *arXiv preprint arXiv:2301.11163*, 2023.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] M. Yang 和 E. Isufi, “在简化复形上的卷积学习，” *arXiv 预印本 arXiv:2301.11163*，2023年。'
- en: '[71] A. D. Keros, V. Nanda, and K. Subr, “Dist2cycle: A simplicial neural network
    for homology localization,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 36, 2022, pp. 7133–7142.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] A. D. Keros, V. Nanda, 和 K. Subr, “Dist2cycle: 一种用于同调定位的简化神经网络，” 见于 *AAAI人工智能会议论文集*，第36卷，2022年，第7133–7142页。'
- en: '[72] M. Hajij, K. N. Ramamurthy, A. Guzmán-Sáenz, and G. Za, “High Skip Networks:
    A Higher Order Generalization of Skip Connections,” in *ICLR 2022 Workshop on
    Geometrical and Topological Representation Learning*, 2022.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. Hajij, K. N. Ramamurthy, A. Guzmán-Sáenz, 和 G. Za, “高跳跃网络：跳跃连接的高阶推广，”
    见于 *ICLR 2022几何和拓扑表示学习工作坊*，2022年。'
- en: '[73] L. Giusti, C. Battiloro, P. Di Lorenzo, S. Sardellitti, and S. Barbarossa,
    “Simplicial attention networks,” *arXiv preprint arXiv:2203.07485*, 2022.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] L. Giusti, C. Battiloro, P. Di Lorenzo, S. Sardellitti, 和 S. Barbarossa,
    “简化注意力网络，” *arXiv 预印本 arXiv:2203.07485*，2022年。'
- en: '[74] C. W. J. Goh, C. Bodnar, and P. Lio, “Simplicial attention networks,”
    *arXiv preprint arXiv:2204.09455*, 2022.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] C. W. J. Goh, C. Bodnar, 和 P. Lio, “简化注意力网络，” *arXiv 预印本 arXiv:2204.09455*，2022年。'
- en: '[75] S. H. Lee, F. Ji, and W. P. Tay, “Sgat: Simplicial graph attention network,”
    in *International Joint Conference on Artificial Intelligence*, 2022.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] S. H. Lee, F. Ji, 和 W. P. Tay, “Sgat: 简化图注意力网络，” 见于 *国际人工智能联合会议*，2022年。'
- en: '[76] M. Hajij, G. Zamzmi, T. Papamarkou, V. Maroulas, and X. Cai, “Simplicial
    complex representation learning,” in *Machine Learning on Graphs (MLoG) Workshop
    at 15th ACM International WSDM (2022) Conference, WSDM2022-MLoG ; Conference date:
    21-02-2022 Through 25-02-2022*, Jan. 2022.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] M. Hajij, G. Zamzmi, T. Papamarkou, V. Maroulas, 和 X. Cai, “简化复形表示学习，”
    见于 *第15届ACM国际WSDM（2022）会议MLoG工作坊，WSDM2022-MLoG；会议日期：2022年2月21日至2022年2月25日*，2022年1月。'
- en: '[77] S. Sardellitti, S. Barbarossa, and L. Testa, “Topological signal processing
    over cell complexes,” in *2021 55th Asilomar Conference on Signals, Systems, and
    Computers*.   IEEE, 2021, pp. 1558–1562.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S. Sardellitti, S. Barbarossa, 和 L. Testa, “在单元复形上的拓扑信号处理，” 见于 *2021年第55届Asilomar信号、系统与计算会议*。
    IEEE，2021年，第1558–1562页。'
- en: '[78] T. M. Roddenberry, M. T. Schaub, and M. Hajij, “Signal processing on cell
    complexes,” in *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP)*.   IEEE, 2022, pp. 8852–8856.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] T. M. Roddenberry, M. T. Schaub, 和 M. Hajij, “在单元复形上的信号处理，” 见于 *ICASSP
    2022-2022 IEEE国际声学、语音与信号处理会议（ICASSP）*。 IEEE，2022年，第8852–8856页。'
- en: '[79] L. Giusti, C. Battiloro, L. Testa, P. Di Lorenzo, S. Sardellitti, and
    S. Barbarossa, “Cell attention networks,” *arXiv preprint arXiv:2209.08179*, 2022.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] L. Giusti, C. Battiloro, L. Testa, P. Di Lorenzo, S. Sardellitti, 和 S.
    Barbarossa, “单元注意力网络，” *arXiv 预印本 arXiv:2209.08179*，2022年。'
- en: '[80] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, “Geometric
    deep learning: going beyond euclidean data,” *IEEE Signal Processing Magazine*,
    vol. 34, no. 4, pp. 18–42, 2017.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, 和 P. Vandergheynst，“几何深度学习：超越欧几里得数据，”
    *IEEE 信号处理杂志*，第34卷，第4期，页码18–42，2017年。'
- en: '[81] S. Liu, P. Lv, Y. Zhang, J. Fu, J. Cheng, W. Li, B. Zhou, and M. Xu, “Semi-dynamic
    hypergraph neural network for 3d pose estimation.” in *IJCAI*, 2020, pp. 782–788.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] S. Liu, P. Lv, Y. Zhang, J. Fu, J. Cheng, W. Li, B. Zhou, 和 M. Xu，“用于三维姿态估计的半动态超图神经网络。”
    在*IJCAI*，2020年，页码782–788。'
- en: '[82] J. Wang, Y. Zhang, Y. Wei, Y. Hu, X. Piao, and B. Yin, “Metro passenger
    flow prediction via dynamic hypergraph convolution networks,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 22, no. 12, pp. 7891–7903, 2021.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Wang, Y. Zhang, Y. Wei, Y. Hu, X. Piao, 和 B. Yin， “通过动态超图卷积网络进行地铁客流预测，”
    *IEEE 智能交通系统汇刊*，第22卷，第12期，页码7891–7903，2021年。'
- en: '[83] D. Chen, Y. Lin, W. Li, P. Li, J. Zhou, and X. Sun, “Measuring and relieving
    the over-smoothing problem for graph neural networks from the topological view,”
    in *Proceedings of the AAAI conference on artificial intelligence*, vol. 34, 2020,
    pp. 3438–3445.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] D. Chen, Y. Lin, W. Li, P. Li, J. Zhou, 和 X. Sun，“从拓扑视角测量和缓解图神经网络的过平滑问题，”
    在*AAAI 人工智能会议论文集*，第34卷，2020年，页码3438–3445。'
- en: '[84] K. Oono and T. Suzuki, “Graph neural networks exponentially lose expressive
    power for node classification,” in *International Conference on Learning Representations*,
    2020\. [Online]. Available: https://openreview.net/forum?id=S1ldO2EFPr'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] K. Oono 和 T. Suzuki，“图神经网络在节点分类中指数性地丧失表达能力，” 在*国际学习表征会议*，2020年。[在线]. 可用：
    https://openreview.net/forum?id=S1ldO2EFPr'
- en: '[85] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li, “Simple and deep graph
    convolutional networks,” in *International conference on machine learning*.   PMLR,
    2020, pp. 1725–1735.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] M. Chen, Z. Wei, Z. Huang, B. Ding, 和 Y. Li，“简单而深度的图卷积网络，” 在*国际机器学习大会*，PMLR，2020年，页码1725–1735。'
