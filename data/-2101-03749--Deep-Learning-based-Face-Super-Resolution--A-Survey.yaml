- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:57:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:57:20'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2101.03749] Deep Learning-based Face Super-Resolution: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2101.03749] 基于深度学习的人脸超分辨率：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2101.03749](https://ar5iv.labs.arxiv.org/html/2101.03749)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2101.03749](https://ar5iv.labs.arxiv.org/html/2101.03749)
- en: 'Deep Learning-based Face Super-Resolution: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的人脸超分辨率：综述
- en: Junjun Jiang School of Computer Science and Technology, Harbin Institute of
    TechnologyHarbinChina [jiangjunjun@hit.edu.cn](mailto:jiangjunjun@hit.edu.cn)
    ,  Chenyang Wang School of Computer Science and Technology, Harbin Institute of
    TechnologyHarbinChina [wangchy02@hit.edu.cn](mailto:wangchy02@hit.edu.cn) ,  Xianming
    Liu School of Computer Science and Technology, Harbin Institute of TechnologyHarbinChina
    [csxm@hit.edu.cn](mailto:csxm@hit.edu.cn)  and  Jiayi Ma Electronic Information
    School, Wuhan UniversityWuhanChina(2021)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Junjun Jiang 哈尔滨工业大学计算机科学与技术学院 哈尔滨 中国 [jiangjunjun@hit.edu.cn](mailto:jiangjunjun@hit.edu.cn)
    ,  Chenyang Wang 哈尔滨工业大学计算机科学与技术学院 哈尔滨 中国 [wangchy02@hit.edu.cn](mailto:wangchy02@hit.edu.cn)
    ,  Xianming Liu 哈尔滨工业大学计算机科学与技术学院 哈尔滨 中国 [csxm@hit.edu.cn](mailto:csxm@hit.edu.cn)  和
    Jiayi Ma 武汉大学电子信息学院 武汉 中国(2021)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Face super-resolution (FSR), also known as face hallucination, which is aimed
    at enhancing the resolution of low-resolution (LR) face images to generate high-resolution
    (HR) face images, is a domain-specific image super-resolution problem. Recently,
    FSR has received considerable attention and witnessed dazzling advances with the
    development of deep learning techniques. To date, few summaries of the studies
    on the deep learning-based FSR are available. In this survey, we present a comprehensive
    review of deep learning-based FSR methods in a systematic manner¹¹1A curated list
    of papers and resources to face super-resolution at [https://github.com/junjun-jiang/Face-Hallucination-Benchmark](https://github.com/junjun-jiang/Face-Hallucination-Benchmark)..
    First, we summarize the problem formulation of FSR and introduce popular assessment
    metrics and loss functions. Second, we elaborate on the facial characteristics
    and popular datasets used in FSR. Third, we roughly categorize existing methods
    according to the utilization of facial characteristics. In each category, we start
    with a general description of design principles, then present an overview of representative
    approaches, and then discuss the pros and cons among them. Fourth, we evaluate
    the performance of some state-of-the-art methods. Fifth, joint FSR and other tasks,
    and FSR-related applications are roughly introduced. Finally, we envision the
    prospects of further technological advancement in this field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸超分辨率（FSR），也称为人脸幻觉，旨在提高低分辨率（LR）人脸图像的分辨率，以生成高分辨率（HR）人脸图像，是一个领域特定的图像超分辨率问题。近年来，随着深度学习技术的发展，FSR受到了相当大的关注，并取得了令人瞩目的进展。迄今为止，关于深度学习基础上的FSR的总结资料还很少。在这项综述中，我们以系统的方式呈现了基于深度学习的FSR方法的全面回顾¹¹1人脸超分辨率的精选论文和资源列表见[https://github.com/junjun-jiang/Face-Hallucination-Benchmark](https://github.com/junjun-jiang/Face-Hallucination-Benchmark)。首先，我们总结了FSR的问题表述，并介绍了流行的评估指标和损失函数。其次，我们详细说明了FSR中使用的面部特征和流行的数据集。第三，我们根据面部特征的利用方式大致分类了现有的方法。在每个类别中，我们首先介绍设计原则的一般描述，然后概述代表性的方法，最后讨论它们之间的优缺点。第四，我们评估了一些最先进方法的性能。第五，我们粗略介绍了联合FSR及其他任务以及与FSR相关的应用。最后，我们展望了该领域进一步技术发展的前景。
- en: 'Face super-resolution and Face hallucination and Deep learning and Convolution
    neural networkface super-resolution, deep learning, survey, facial characteristics^†^†copyright:
    acmcopyright^†^†journalyear: 2021^†^†journal: CSUR^†^†journalvolume: 0^†^†journalnumber:
    0^†^†article: 0^†^†publicationmonth: 0^†^†ccs: General and reference Reference
    works^†^†ccs: General and reference Surveys and overviews'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸超分辨率、人脸幻觉、深度学习和卷积神经网络人脸超分辨率、深度学习、综述、面部特征^†^†版权：acmcopyright^†^†期刊年份：2021^†^†期刊：CSUR^†^†期刊卷号：0^†^†期刊号：0^†^†文章：0^†^†出版月份：0^†^†ccs：一般和参考
    文献^†^†ccs：一般和参考 综述和概述
- en: The research is supported by the National Natural Science Foundation of China
    (61971165, 61922027, 61773295), and the Fundamental Research Funds for the Central
    Universities.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 研究得到了中国国家自然科学基金（61971165、61922027、61773295）和中央高校基本科研业务费的资助。
- en: 1\. Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Face super-resolution (FSR), a domain-specific image super-resolution problem,
    refers to the technique of recovering high-resolution (HR) face images from low-resolution
    (LR) face images. It can increase the resolution of an LR face image of low quality
    and recover the details. In many real-world scenarios, limited by physical imaging
    systems and imaging conditions, the face images are always low quality. Thus,
    with a wide range of applications and notable advantages, FSR has always been
    a hot topic since its birth in image processing and computer vision.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 面部超分辨率（FSR），一个特定领域的图像超分辨率问题，指的是从低分辨率（LR）面部图像中恢复高分辨率（HR）面部图像的技术。它可以提高低质量LR面部图像的分辨率并恢复细节。在许多实际场景中，由于物理成像系统和成像条件的限制，面部图像通常质量较低。因此，FSR具有广泛的应用范围和显著的优势，自其在图像处理和计算机视觉领域诞生以来，一直是一个热门话题。
- en: Table 1\. Summary of face super-resolution surveys since 2010.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 自2010年以来面部超分辨率调查总结。
- en: '| No. | Survey title | Year | Venue |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 编号 | 调查标题 | 年份 | 出版机构 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  |  |  |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | A survey of face hallucination [[1](#bib.bib1)] | 2012 | CCBR |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 面部幻像的调查 [[1](#bib.bib1)] | 2012 | CCBR |'
- en: '| 2 | A comprehensive survey to face hallucination [[2](#bib.bib2)] | 2014
    | IJCV |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 面部幻像的全面调查 [[2](#bib.bib2)] | 2014 | IJCV |'
- en: '| 3 | A review of various approaches to face hallucination [[3](#bib.bib3)]
    | 2015 | ICACTA |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 面部幻像各种方法的综述 [[3](#bib.bib3)] | 2015 | ICACTA |'
- en: '| 4 | Face super resolution: a survey  [[4](#bib.bib4)] | 2017 | IJIGSP |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 面部超分辨率：调查 [[4](#bib.bib4)] | 2017 | IJIGSP |'
- en: '| 5 | Super-resolution for biometrics: a comprehensive survey [[5](#bib.bib5)]
    | 2018 | PR |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 生物特征超分辨率：全面调查 [[5](#bib.bib5)] | 2018 | PR |'
- en: '| 6 | Face hallucination techniques: a survey [[6](#bib.bib6)] | 2018 | CICT
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 面部幻像技术：调查 [[6](#bib.bib6)] | 2018 | CICT |'
- en: '| 7 | Survey on GAN-based face hallucination with its model development [[7](#bib.bib7)]
    | 2019 | IET |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 基于GAN的面部幻像调查及其模型发展 [[7](#bib.bib7)] | 2019 | IET |'
- en: 'The concept of FSR was first proposed in 2000 by Baker and Kanade [[8](#bib.bib8)],
    who are the pioneers of the FSR technique. They develop a multi-level learning
    and prediction model based on the Gaussian image pyramid to improve the resolution
    of an LR face image. Liu *et al*. [[9](#bib.bib9)] propose to integrate a global
    parametric principal component analysis (PCA) model with a local nonparametric
    Markov random field (MRF) model for FSR. Since then, a number of innovative methods
    have been proposed, and FSR has become the subject of active research efforts.
    Researchers super-resolve the LR face images by means of global face statistical
    models [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)], local patch-based representation
    methods [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)], or hybrid ones [[28](#bib.bib28), [29](#bib.bib29)].
    These methods have achieved good performance, however, have trouble when meeting
    requirements in practice. With the rapid development of deep learning technique,
    attractive advantages over previous attempts have been obtained and have been
    applied into image or video super-resolution. Many comprehensive surveys have
    reviewed recent achievements in these fields, *i.e.*, general image super-resolution
    surveys [[30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)], and video super-resolution
    survey [[33](#bib.bib33)]. Towards FSR, a domain-specific image super-resolution,
    a few surveys are listed in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ Deep
    Learning-based Face Super-Resolution: A Survey"). In the early stage of research,
     [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6)] provide a comprehensive review of traditional FSR methods (mainly
    including patch-based super-resolution, PCA-based methods, *etc*.), while Liu
    *et al*. [[7](#bib.bib7)] offer a generative adversarial network (GAN) based FSR
    survey. However, so far no literature review is available on deep learning super-resolution
    specifically for human faces. In this paper, we present a comparative study of
    different deep learning-based FSR methods.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 'FSR的概念首次由Baker和Kanade于2000年提出[[8](#bib.bib8)]，他们是FSR技术的开创者。他们基于高斯图像金字塔开发了一个多层次学习和预测模型，以提高LR人脸图像的分辨率。刘*等人*[[9](#bib.bib9)]建议将全局参数主成分分析（PCA）模型与局部非参数马尔可夫随机场（MRF）模型集成用于FSR。从那时起，提出了许多创新方法，FSR成为了活跃的研究主题。研究人员通过全球人脸统计模型[[10](#bib.bib10)、[11](#bib.bib11)、[12](#bib.bib12)、[13](#bib.bib13)、[14](#bib.bib14)、[15](#bib.bib15)、[16](#bib.bib16)]、局部基于补丁的表示方法[[17](#bib.bib17)、[18](#bib.bib18)、[19](#bib.bib19)、[20](#bib.bib20)、[21](#bib.bib21)、[22](#bib.bib22)、[23](#bib.bib23)、[24](#bib.bib24)、[25](#bib.bib25)、[26](#bib.bib26)、[27](#bib.bib27)]或混合方法[[28](#bib.bib28)、[29](#bib.bib29)]对LR人脸图像进行超分辨率处理。这些方法已经取得了良好的性能，但在实际应用中遇到了一些困难。随着深度学习技术的快速发展，相较于以前的尝试，取得了显著的优势，并已应用于图像或视频超分辨率。许多全面的调查回顾了这些领域的最新成就，即，一般图像超分辨率调查[[30](#bib.bib30)、[31](#bib.bib31)、[32](#bib.bib32)]，和视频超分辨率调查[[33](#bib.bib33)]。针对FSR，作为一个特定领域的图像超分辨率，表[1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ Deep Learning-based Face Super-Resolution: A Survey")中列出了少数调查。在研究的早期阶段，[[1](#bib.bib1)、[2](#bib.bib2)、[3](#bib.bib3)、[4](#bib.bib4)、[5](#bib.bib5)、[6](#bib.bib6)]提供了对传统FSR方法（主要包括基于补丁的超分辨率、基于PCA的方法，*等*）的全面回顾，而刘*等人*[[7](#bib.bib7)]提供了基于生成对抗网络（GAN）的FSR调查。然而，到目前为止，没有关于专门针对人脸的深度学习超分辨率的文献综述。本文对不同的基于深度学习的FSR方法进行了比较研究。'
- en: 'The main contributions of this survey are as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的主要贡献如下：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The survey provides a comprehensive review of recent techniques for FSR, including
    problem definition, commonly used evaluation metrics and loss functions, the characteristics
    of FSR, benchmark datasets, deep learning-based FSR methods, performance comparison
    of state-of-the-arts, methods that jointly perform FSR and other tasks, and FSR-related
    applications.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这份调查提供了对最近FSR技术的全面回顾，包括问题定义、常用评价指标和损失函数、FSR的特征、基准数据集、基于深度学习的FSR方法、最先进技术的性能比较、同时执行FSR和其他任务的方法，以及与FSR相关的应用。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The survey summarizes how existing deep learning-based FSR methods explore the
    potential of network architecture and take advantage of the characteristics of
    face images, as well as compare the similarities and differences among these methods.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这项综述总结了现有基于深度学习的FSR方法如何探索网络架构的潜力，利用面部图像的特征，以及比较这些方法之间的相似性和差异。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The survey discusses the challenges and envisions the prospects of future research
    in the FSR field.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这项综述讨论了挑战，并展望了FSR领域未来研究的前景。
- en: 'In the following, we will cover the existing deep learning-based FSR methods
    and Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Deep Learning-based Face Super-Resolution:
    A Survey") shows the taxonomy of FSR. Section [2](#S2 "2\. Background ‣ Deep Learning-based
    Face Super-Resolution: A Survey") introduces the problem definition of FSR, and
    commonly used assessment metrics and loss functions. Section [3](#S3 "3\. Characteristics
    of Face Images ‣ Deep Learning-based Face Super-Resolution: A Survey") presents
    the facial characteristics (*i.e.*, prior information, attribute information,
    and identity information) and reviews some mainstream face datasets. In Section [4](#S4
    "4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey"), we discuss
    FSR methods. To avoid exhaustive enumeration and take facial characteristics into
    consideration, FSR methods are categorized according to facial characteristics
    used. In Section [4](#S4 "4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey"), five major categories are presented: general FSR methods, prior-guided
    FSR methods, attribute-constrained FSR methods, identity-preserving FSR methods,
    and reference FSR methods. Depending on the network architecture or the utilization
    of facial characteristics, every category is further divided into several subcategories.
    Moreover, Section [4](#S4 "4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey") compares the performance of some state-of-the-art methods. Besides,
    Section [4](#S4 "4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey") also reviews some methods dealing with joint tasks and FSR-related
    applications. Section [5](#S5 "5\. Conclusion and Future Directions ‣ Deep Learning-based
    Face Super-Resolution: A Survey") concludes the FSR and further discusses the
    limitations as well as envisions the prospects of further technological advancement.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们将涵盖现有的基于深度学习的面部超分辨率（FSR）方法，图[1](#S1.F1 "图 1 ‣ 1\. 介绍 ‣ 基于深度学习的面部超分辨率：综述")展示了FSR的分类法。第[2](#S2
    "2\. 背景 ‣ 基于深度学习的面部超分辨率：综述")节介绍了FSR的问题定义，以及常用的评估指标和损失函数。第[3](#S3 "3\. 面部图像的特征
    ‣ 基于深度学习的面部超分辨率：综述")节展示了面部特征（*即*，先验信息、属性信息和身份信息），并回顾了一些主流面部数据集。在第[4](#S4 "4\.
    FSR 方法 ‣ 基于深度学习的面部超分辨率：综述")节中，我们讨论了FSR方法。为了避免详尽的列举，并考虑到面部特征，FSR方法根据使用的面部特征进行分类。在第[4](#S4
    "4\. FSR 方法 ‣ 基于深度学习的面部超分辨率：综述")节中，提出了五大类：一般FSR方法、先验引导的FSR方法、属性约束的FSR方法、身份保留的FSR方法和参考FSR方法。根据网络架构或面部特征的利用，每个类别进一步细分为几个子类别。此外，第[4](#S4
    "4\. FSR 方法 ‣ 基于深度学习的面部超分辨率：综述")节还比较了一些最先进方法的性能。此外，第[4](#S4 "4\. FSR 方法 ‣ 基于深度学习的面部超分辨率：综述")节还回顾了一些处理联合任务和FSR相关应用的方法。第[5](#S5
    "5\. 结论与未来方向 ‣ 基于深度学习的面部超分辨率：综述")节总结了FSR，并进一步讨论了其局限性以及展望了未来技术发展的前景。
- en: '![Refer to caption](img/124ebc3936fda5dea4db133213352ab3.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/124ebc3936fda5dea4db133213352ab3.png)'
- en: Figure 1\. The taxonomy of face super-resolution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 面部超分辨率的分类。
- en: 2\. Background
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景
- en: 2.1\. Problem Definition
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 问题定义
- en: 'FSR focuses on recovering the corresponding HR face image from an observed
    LR face image. The image degradation model $\Phi$ can be mathematically written
    as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: FSR 关注于从观察到的低分辨率（LR）面部图像中恢复相应的高分辨率（HR）面部图像。图像退化模型 $\Phi$ 可以用数学公式表示为：
- en: '| (1) |  | $I_{\text{LR}}=\Phi(I_{\text{HR}},\theta),$ |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $I_{\text{LR}}=\Phi(I_{\text{HR}},\theta),$ |  |'
- en: 'where $\theta$ represents the model parameters including blurring kernel, downsampling
    operation, and noise, $I_{\text{LR}}$ is the observed LR face image, and $I_{\text{HR}}$
    is the original HR face image. FSR is devoted to simulating the inverse process
    of the degradation model and recovers the $I_{\text{SR}}$ from $I_{\text{LR}}$,
    which can be expressed as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\theta$ 代表模型参数，包括模糊核、下采样操作和噪声，$I_{\text{LR}}$ 是观察到的 LR 面部图像，$I_{\text{HR}}$
    是原始 HR 面部图像。FSR 致力于模拟退化模型的逆过程，并从 $I_{\text{LR}}$ 中恢复 $I_{\text{SR}}$，可以表示为：
- en: '| (2) |  | $I_{\text{SR}}=\Phi^{\text{-1}}(I_{\text{LR}},\delta)=F(I_{\text{LR}},\delta),$
    |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $I_{\text{SR}}=\Phi^{\text{-1}}(I_{\text{LR}},\delta)=F(I_{\text{LR}},\delta),$
    |  |'
- en: 'where $F$ is the super-resolution model (inverse degradation model), $\delta$
    represents the parameters of $F$, and $I_{\text{SR}}$ represents the super-resolved
    result. The optimization of $\delta$ can be defined as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$F$ 是超分辨率模型（逆退化模型），$\delta$ 代表 $F$ 的参数，$I_{\text{SR}}$ 代表超分辨率结果。$\delta$
    的优化可以定义为：
- en: '| (3) |  | $\hat{\delta}=\underset{\delta}{\text{argmin}}\,\,\mathcal{L}(I_{\text{SR}},I_{\text{HR}}),$
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\hat{\delta}=\underset{\delta}{\text{argmin}}\,\,\mathcal{L}(I_{\text{SR}},I_{\text{HR}}),$
    |  |'
- en: 'where $\mathcal{L}$ represents the loss between $I_{\text{SR}}$ and $I_{\text{HR}}$
    and $\hat{\delta}$ is the optimal parameter of the trained model. In FSR, MSE
    loss and $\mathcal{L}_{1}$ loss are the most popular loss functions, and some
    models tend to use a combination of multiple loss functions, which will be reviewed
    in Section [2.2](#S2.SS2 "2.2\. Assessment Metrics and Loss Functions ‣ 2\. Background
    ‣ Deep Learning-based Face Super-Resolution: A Survey").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{L}$ 代表 $I_{\text{SR}}$ 和 $I_{\text{HR}}$ 之间的损失，$\hat{\delta}$ 是训练模型的最优参数。在
    FSR 中，MSE 损失和 $\mathcal{L}_{1}$ 损失是最常用的损失函数，一些模型倾向于使用多个损失函数的组合，这将在第 [2.2](#S2.SS2
    "2.2\. 评估指标与损失函数 ‣ 2\. 背景 ‣ 基于深度学习的面部超分辨率：综述") 节中进行回顾。
- en: The degradation model and parameters are all unavailable in a real-world environment,
    and $I_{\text{LR}}$ is the only given information. To simulate the image degradation
    process, researchers tend to use mathematical models to generate some LR and HR
    pairs to train the model. The simplest mathematical model is
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实环境中，退化模型和参数都是不可用的，$I_{\text{LR}}$ 是唯一已知的信息。为了模拟图像退化过程，研究人员倾向于使用数学模型生成一些 LR
    和 HR 对以训练模型。最简单的数学模型是
- en: '| (4) |  | $I_{\text{LR}}=(I_{\text{HR}})\downarrow_{s},$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $I_{\text{LR}}=(I_{\text{HR}})\downarrow_{s},$ |  |'
- en: 'where $\downarrow$ denotes the downsampling operation, and $s$ is the scaling
    factor. However, this pattern is too simple to match the real-world degradation
    process. To better mimic the real degradation process, researchers design a degradation
    process with the combination of many operations (*e.g.*, downsampling, blur, noise,
    and compression) as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\downarrow$ 表示下采样操作，$s$ 是缩放因子。然而，这种模式过于简单，无法匹配真实世界的退化过程。为了更好地模拟真实的退化过程，研究人员设计了一个包含多种操作（*例如*，下采样、模糊、噪声和压缩）的退化过程，如下所示：
- en: '| (5) |  | $I_{\text{LR}}=J((I_{\text{HR}}\otimes k)\downarrow_{s}+\,n),$ |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $I_{\text{LR}}=J((I_{\text{HR}}\otimes k)\downarrow_{s}+\,n),$ |  |'
- en: where $k$ is the blurring kernel, $\otimes$ represents the convolutional operation,
    $n$ denotes the noise, and $J$ denotes the image compression. Various combinations
    of different operations are used in FSR. They include the widely used bicubic
    model [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)], as well as the general
    degradation model used for blind FSR [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)].
    However, they are not introduced in detail in this survey.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$k$ 是模糊核，$\otimes$ 代表卷积操作，$n$ 代表噪声，$J$ 代表图像压缩。FSR 中使用了多种不同操作的组合，包括广泛使用的双三次模型
    [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)]，以及用于盲目 FSR 的通用退化模型 [[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39)]。然而，这些模型在本综述中没有详细介绍。
- en: 2.2\. Assessment Metrics and Loss Functions
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 评估指标与损失函数
- en: In deep learning-based FSR methods, the loss function which measures the difference
    between $I_{\text{HR}}$ and $I_{\text{SR}}$ plays an important role in guiding
    the network training. Upon acquiring the trained network, the reconstruction performance
    of these methods can be evaluated by the assessment metrics. The preferences of
    different loss functions are different. For example, $\mathcal{L}_{2}$ loss tends
    to produce the result that is faithful to the original image (high PSNR value),
    and the perceptual and adversarial losses will generate subjectively pleasing
    results (low FID [[40](#bib.bib40)] and LPIPS [[41](#bib.bib41)] values). In practice,
    we can choose the appropriate loss function according to the needs. Considering
    the relationship between loss functions and assessment metrics, we introduce them
    together in this section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于深度学习的超分辨率方法中，用于测量 $I_{\text{HR}}$ 和 $I_{\text{SR}}$ 之间差异的损失函数在指导网络训练中起着重要作用。在获得训练好的网络后，这些方法的重建性能可以通过评估指标来评估。不同损失函数的偏好不同。例如，$\mathcal{L}_{2}$
    损失倾向于产生忠实于原始图像的结果（高 PSNR 值），而感知损失和对抗性损失会生成主观上令人满意的结果（低 FID [[40](#bib.bib40)]
    和 LPIPS [[41](#bib.bib41)] 值）。在实际应用中，我们可以根据需求选择合适的损失函数。考虑到损失函数与评估指标之间的关系，我们在本节中一并介绍它们。
- en: 2.2.1\. Image Quality Assessment
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 图像质量评估
- en: Generally, two main methods of quality evaluation are subjective and objective
    evaluation. Subjective evaluation relies on the judgement of humans, and tends
    to invite readers or interviewers to see and assess the quality of the generated
    images, leading to results always consistent with human perception but time-assuming,
    inconvenient and expensive. In contrast, the objective evaluation mainly utilizes
    statistical data to reflect the quality of the generated images. In general, the
    objective evaluation methods usually produce different results from subjective
    evaluation metrics, because the starting point of objective evaluation methods
    is mathematics instead of human visual perception, which leaves the assessment
    image quality in dispute. Here, we introduce some popular assessment metrics.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，质量评估有两种主要方法：主观评估和客观评估。主观评估依赖于人的判断，通常邀请读者或评审来查看和评估生成图像的质量，从而得到与人类感知一致的结果，但这种方法费时、麻烦且昂贵。相比之下，客观评估主要利用统计数据来反映生成图像的质量。一般而言，客观评估方法通常会产生与主观评估指标不同的结果，因为客观评估方法的出发点是数学而非人类视觉感知，这使得图像质量的评估存在争议。在这里，我们介绍一些流行的评估指标。
- en: 'Peak Signal-to-Noise Ratio (PSNR): PSNR is a commonly used objective assessment
    metric in FSR. Given $I_{\text{HR}}$ and $I_{\text{SR}}$, the mean square error
    (MSE) between them is firstly calculated, then the PSNR is obtained,'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 峰值信噪比（PSNR）：PSNR 是一种在超分辨率（FSR）中常用的客观评估指标。给定 $I_{\text{HR}}$ 和 $I_{\text{SR}}$，首先计算它们之间的均方误差（MSE），然后得到
    PSNR，
- en: '| (6) |  | $\text{MSE}=\frac{\text{1}}{hwc}\left\&#124;I_{\text{SR}}-I_{\text{HR}}\right\&#124;^{2}_{2},$
    |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\text{MSE}=\frac{\text{1}}{hwc}\left\&#124;I_{\text{SR}}-I_{\text{HR}}\right\&#124;^{2}_{2},$
    |  |'
- en: '| (7) |  | $\text{PSNR}=10\,log_{\text{10}}(\frac{\text{M}^{2}}{\text{MSE}}),$
    |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\text{PSNR}=10\,log_{\text{10}}(\frac{\text{M}^{2}}{\text{MSE}}),$
    |  |'
- en: where $h$, $w$, and $c$ denote the height, width, and channel of the image,
    and M is the maximum possible pixel value (*i.e.*, 255 for 8-bit images). The
    smaller the pixel-wise difference of the two images, the higher the PSNR. In this
    pattern, PSNR focuses on the distance between every pair of pixels in two images,
    which is inconsistent with human perception, resulting in poor performance when
    human perception is more important.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h$、$w$ 和 $c$ 分别表示图像的高度、宽度和通道，M 是最大可能的像素值（*即*，8 位图像的值为 255）。两幅图像的像素级差异越小，PSNR
    越高。在这种模式下，PSNR 关注的是两幅图像中每对像素之间的距离，这与人类感知不一致，因此当人类感知更重要时，表现较差。
- en: 'Structural Similarity Index (SSIM): SSIM [[42](#bib.bib42)] is also a popular
    objective assessment metric that measures the structural similarity between two
    images. To be specific, SSIM measures similarity from three aspects: luminance,
    contrast, and structure. Given $I_{\text{HR}}$ and $I_{\text{SR}}$, SSIM is obtained
    by'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 结构相似性指数（SSIM）：SSIM [[42](#bib.bib42)] 也是一种流行的客观评估指标，用于衡量两幅图像之间的结构相似性。具体来说，SSIM
    从亮度、对比度和结构三个方面来衡量相似性。给定 $I_{\text{HR}}$ 和 $I_{\text{SR}}$，SSIM 通过以下方式获得
- en: '| (8) |  | $\text{SSIM}=l(I_{\text{HR}},I_{\text{SR}})*C(I_{\text{HR}},I_{\text{SR}})*S(I_{\text{HR}},I_{\text{SR}}),$
    |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\text{SSIM}=l(I_{\text{HR}},I_{\text{SR}})*C(I_{\text{HR}},I_{\text{SR}})*S(I_{\text{HR}},I_{\text{SR}}),$
    |  |'
- en: where $l(I_{\text{HR}},I_{\text{SR}})$, $C(I_{\text{HR}},I_{\text{SR}})$ and
    $S(I_{\text{HR}},I_{\text{SR}})$ denote the similarity of the luminance, contrast
    and structure. SSIM varies from 0 to 1\. The higher the structural similarity
    of the two images, the larger the SSIM. Considering the uneven distribution of
    the image, SSIM is not reliable enough. Thus, multi-scale structural similarity
    index measure (MS-SSIM) [[43](#bib.bib43)] is proposed, which divides the image
    into multiple windows, first assesses SSIM for every window separately, and then
    converges them to obtain MS-SSIM.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l(I_{\text{HR}},I_{\text{SR}})$、$C(I_{\text{HR}},I_{\text{SR}})$ 和 $S(I_{\text{HR}},I_{\text{SR}})$
    表示亮度、对比度和结构的相似性。SSIM 从 0 到 1 变化。两张图像的结构相似性越高，SSIM 越大。考虑到图像的分布不均匀，SSIM 并不总是可靠。因此，提出了多尺度结构相似性指数度量（MS-SSIM） [[43](#bib.bib43)]，它将图像划分为多个窗口，首先分别评估每个窗口的
    SSIM，然后汇总这些结果以获得 MS-SSIM。
- en: 'Learned Perceptual Image Patch Similarity (LPIPS): LPIPS [[41](#bib.bib41)]
    measures the distance between two images in a deep feature space. LPIPS is more
    in line with human judgement than PSNR and SSIM. The more similar the two images,
    the smaller the LPIPS.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的感知图像块相似度（LPIPS）：LPIPS [[41](#bib.bib41)] 衡量两张图像在深层特征空间中的距离。LPIPS 比 PSNR
    和 SSIM 更符合人类判断。两张图像越相似，LPIPS 越小。
- en: 'Fréchet Inception Distance (FID): In contrast to PSNR and SSIM, FID [[40](#bib.bib40)]
    focuses on the difference between $I_{\text{HR}}$ and $I_{\text{SR}}$ in a distribution-wise
    manner, and it is always applied to assess the visual quality of face images.
    The better the visual quality, the smaller the FID.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Fréchet Inception Distance（FID）：与 PSNR 和 SSIM 相比，FID [[40](#bib.bib40)] 专注于
    $I_{\text{HR}}$ 和 $I_{\text{SR}}$ 之间的分布差异，它通常用于评估面部图像的视觉质量。视觉质量越好，FID 越小。
- en: 'Natural Image Quality Evaluator (NIQE): NIQE [[44](#bib.bib44)] is a no-reference
    metric that measures the distance between two multivariate Gaussian models fitting
    natural images and the evaluated images without ground truth images. Specifically,
    the fitting of multivariate Gaussian model is based on the quality-aware features
    derived from the natural scene statistic model. The better the visual quality,
    the smaller the NIQE.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 自然图像质量评估器（NIQE）：NIQE [[44](#bib.bib44)] 是一种无参考指标，测量两种多变量高斯模型（拟合自然图像和评估图像）之间的距离，无需真实图像。具体而言，多变量高斯模型的拟合基于从自然场景统计模型中提取的质量感知特征。视觉质量越好，NIQE
    越小。
- en: 'Mean Opinion Score (MOS): MOS is a commonly used subjective assessment metric,
    in contrast to the above objective quantitative metrics. To obtain the MOS, human
    raters are asked to assign perceptual quality scores to the tested images. Finally,
    MOS is obtained by calculating the arithmetic mean ratings assigned by human raters.
    When the number of human raters is small, MOS would be biased while MOS would
    be faithful enough when the number of human raters is large.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 平均意见分数（MOS）：MOS 是一种常用的主观评估指标，与上述客观定量指标不同。为了获得 MOS，人类评分者被要求对测试图像分配感知质量分数。最后，通过计算人类评分者分配的算术平均评分来获得
    MOS。当评分者数量较少时，MOS 可能会有偏差，而当评分者数量较多时，MOS 足够真实。
- en: 2.2.2\. Loss Functions
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. 损失函数
- en: Initially, pixel-wise $\mathcal{L}_{2}$ loss (also known as MSE loss) is popular,
    however, researchers then find that models based on $\mathcal{L}_{2}$ loss tend
    to generate smooth results. Then many kinds of loss functions are employed, such
    as pixel-wise $\mathcal{L}_{1}$ loss, SSIM loss, perceptual loss, adversarial
    loss, *etc*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，像素级的 $\mathcal{L}_{2}$ 损失（也称为均方误差损失）很受欢迎，但研究人员发现，基于 $\mathcal{L}_{2}$ 损失的模型往往生成平滑的结果。于是，许多种损失函数被采用，例如像素级的
    $\mathcal{L}_{1}$ 损失、SSIM 损失、感知损失、对抗性损失，*等等*。
- en: 'Pixel-wise Loss: Pixel-wise loss measures the distance between the two images
    at pixel level, including $\mathcal{L}_{1}$ loss that calculates the mean absolute
    error, $\mathcal{L}_{2}$ loss that calculates the mean square error, Huber loss [[45](#bib.bib45)]
    and Carbonnier penalty function [[46](#bib.bib46)]. With the constrain of the
    pixel-wise loss, the obtained $I_{\text{SR}}$ can be close enough to the $I_{\text{HR}}$
    on the pixel value. From the definition, $\mathcal{L}_{2}$ loss is sensitive to
    large errors but indifferent to small errors, while $\mathcal{L}_{1}$ loss treats
    them equally. Therefore, $\mathcal{L}_{1}$ loss has advantages in improving the
    performance and convergence over $\mathcal{L}_{2}$ loss. Overall, pixel-wise loss
    can force the model to improve PSNR, but the generated images are always over-smooth
    and lack high-frequency details.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 像素级损失：像素级损失衡量两张图像在像素级别的距离，包括计算均值绝对误差的$\mathcal{L}_{1}$损失、计算均方误差的$\mathcal{L}_{2}$损失、Huber损失[[45](#bib.bib45)]和Carbonnier惩罚函数[[46](#bib.bib46)]。在像素级损失的约束下，获得的$I_{\text{SR}}$可以与$I_{\text{HR}}$在像素值上足够接近。从定义来看，$\mathcal{L}_{2}$损失对大误差敏感，但对小误差漠不关心，而$\mathcal{L}_{1}$损失对它们一视同仁。因此，$\mathcal{L}_{1}$损失在提高性能和收敛方面相对于$\mathcal{L}_{2}$损失具有优势。总体而言，像素级损失可以迫使模型提高PSNR，但生成的图像总是过于平滑，缺乏高频细节。
- en: 'SSIM Loss: Similar to pixel-wise loss, SSIM loss is designed to improve the
    structure similarity between super-resolved image and the original HR one:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: SSIM损失：类似于像素级损失，SSIM损失旨在改善超分辨率图像与原始HR图像之间的结构相似性：
- en: '| (9) |  | $\mathcal{L}_{\text{SSIM}}(I_{\text{HR}},I_{\text{SR}})=\frac{1}{2}\left(1-F_{\text{SSIM}}(I_{\text{HR}},I_{\text{SR}})\right),$
    |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\mathcal{L}_{\text{SSIM}}(I_{\text{HR}},I_{\text{SR}})=\frac{1}{2}\left(1-F_{\text{SSIM}}(I_{\text{HR}},I_{\text{SR}})\right),$
    |  |'
- en: where $F_{\text{SSIM}}$ denotes the function of SSIM. Except for SSIM loss,
    multi-scale SSIM loss can calculate SSIM loss at different scales.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$F_{\text{SSIM}}$表示SSIM的函数。除了SSIM损失，多尺度SSIM损失可以在不同尺度上计算SSIM损失。
- en: 'Perceptual Loss: To improve the perceptual quality, one solution is to minimize
    the perceptual loss:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 感知损失：为了提高感知质量，一种解决方案是最小化感知损失：
- en: '| (10) |  | $\mathcal{L}_{\text{Perceptual}}(I_{\text{HR}},I_{\text{SR}},\Psi,l)=\left\&#124;\Psi^{l}(I_{\text{HR}})-\Psi^{l}(I_{\text{SR}})\right\&#124;_{2},$
    |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\mathcal{L}_{\text{Perceptual}}(I_{\text{HR}},I_{\text{SR}},\Psi,l)=\left\&#124;\Psi^{l}(I_{\text{HR}})-\Psi^{l}(I_{\text{SR}})\right\&#124;_{2},$
    |  |'
- en: where $\Psi$ is the pretrained network and $l$ is the $l$-th layer. In essence,
    the perceptual loss measures the distance between the features extracted from
    $\Psi$ (*e.g.*, VGG [[47](#bib.bib47)]), and it can evaluate the difference at
    the semantic level. Perceptual loss encourages the network to generate $I_{\text{SR}}$
    that is more perceptually similar to $I_{\text{HR}}$. The $I_{\text{SR}}$ predicted
    by the model with perceptual loss always looks more pleasant but usually has lower
    PSNR than those pixel-wise loss-based methods.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Psi$是预训练网络，$l$是第$l$层。本质上，感知损失衡量从$\Psi$（*例如*，VGG[[47](#bib.bib47)]）提取的特征之间的距离，它可以评估语义层面的差异。感知损失鼓励网络生成与$I_{\text{HR}}$在感知上更相似的$I_{\text{SR}}$。模型预测的感知损失的$I_{\text{SR}}$总是看起来更愉悦，但通常比那些基于像素级损失的方法有更低的PSNR。
- en: 'Adversarial Loss: Adversarial loss, proposed in generative adversarial network
    (GAN) [[48](#bib.bib48)], is also widely used in FSR. For details, GAN is comprised
    of two models: a generator (G) and a discriminator (D). In FSR, GAN can be described
    as follows: G is the super-resolution model which generates the super-resolved
    face with an LR face image as input, and D discriminates whether the output result
    is generated or real. In the training phase, G and D are trained alternatively.
    Early methods  [[34](#bib.bib34), [49](#bib.bib49)] use cross entropy-based adversarial
    loss expressed as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗损失：对抗损失是生成对抗网络（GAN）[[48](#bib.bib48)]中提出的，也广泛应用于FSR。详细来说，GAN由两个模型组成：生成器（G）和鉴别器（D）。在FSR中，GAN可以描述为：G是超分辨率模型，它以LR图像作为输入生成超分辨率人脸，D则判断输出结果是否生成或真实。在训练阶段，G和D交替训练。早期方法[[34](#bib.bib34),
    [49](#bib.bib49)]使用基于交叉熵的对抗损失，表达如下：
- en: '| (11) |  | $\mathcal{L}_{\text{G}}(I_{\text{SR}})=-\text{log}(\mathcal{D}(I_{\text{SR}})),$
    |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\mathcal{L}_{\text{G}}(I_{\text{SR}})=-\text{log}(\mathcal{D}(I_{\text{SR}})),$
    |  |'
- en: '| (12) |  | $\mathcal{L}_{\text{D}}(I_{\text{HR}},I_{\text{SR}})=-\text{log}(\mathcal{D}(I_{\text{HR}}))-\text{log}(1-\mathcal{D}(I_{\text{SR}})),$
    |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\mathcal{L}_{\text{D}}(I_{\text{HR}},I_{\text{SR}})=-\text{log}(\mathcal{D}(I_{\text{HR}}))-\text{log}(1-\mathcal{D}(I_{\text{SR}})),$
    |  |'
- en: where $\mathcal{L}_{\text{D}}$ and $\mathcal{L}_{\text{G}}$ denotes the loss
    function of D and G, respectively, $\mathcal{D}$ denotes the function of D, and
    $I_{\text{HR}}$ is randomly sampled from HR training samples. However, the model
    trained with this adversarial loss is always unstable and may cause model collapse.
    Therefore, Wasserstein GAN [[50](#bib.bib50)] and WGAN-GP [[51](#bib.bib51)] are
    proposed to alleviate the training difficulties. The model trained with adversarial
    loss tends to introduce artificial details, leading to worse PSNR and SSIM but
    pleasing visual quality with smaller FID.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{\text{D}}$ 和 $\mathcal{L}_{\text{G}}$ 分别表示 D 和 G 的损失函数，$\mathcal{D}$
    表示 D 的函数，$I_{\text{HR}}$ 是从 HR 训练样本中随机采样的。然而，使用这种对抗损失训练的模型通常不稳定，可能导致模型崩溃。因此，提出了
    Wasserstein GAN [[50](#bib.bib50)] 和 WGAN-GP [[51](#bib.bib51)] 来缓解训练困难。使用对抗损失训练的模型往往会引入人工细节，导致
    PSNR 和 SSIM 较差，但视觉效果令人愉悦且 FID 较小。
- en: 'Cycle Consistency Loss: Cycle consistency loss is proposed by CycleGAN [[52](#bib.bib52)].
    In CycleGAN-based FSR, two cooperated models are used: a super-resolution model
    super-resolves the $I_{\text{LR}}$ to recover the $I_{\text{SR}}$, and a degradation
    model downsamples the $I_{\text{SR}}$ back to $I_{\text{LR}^{{}^{\prime}}}$. In
    turn, the degradation model downsamples the HR face image to obtain $I_{\text{HLR}}$,
    and then the super-resolution model recovers the $I_{\text{HLR}}$ to generate
    $I_{\text{HR}^{{}^{\prime}}}$. The cycle consistent loss is aimed to keep the
    consistency between $I_{\text{LR}}$ ($I_{\text{LR}^{{}^{\prime}}}$) and $I_{\text{HR}}$
    ($I_{\text{HR}^{{}^{\prime}}}$),'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 循环一致性损失：循环一致性损失由 CycleGAN [[52](#bib.bib52)] 提出。在基于 CycleGAN 的 FSR 中，使用两个合作的模型：一个超分辨率模型将
    $I_{\text{LR}}$ 超分辨率到 $I_{\text{SR}}$，另一个降解模型将 $I_{\text{SR}}$ 下采样回 $I_{\text{LR}^{{}^{\prime}}}$。反过来，降解模型将
    HR 人脸图像下采样以获得 $I_{\text{HLR}}$，然后超分辨率模型恢复 $I_{\text{HLR}}$ 以生成 $I_{\text{HR}^{{}^{\prime}}}$。循环一致性损失旨在保持
    $I_{\text{LR}}$ ($I_{\text{LR}^{{}^{\prime}}}$) 和 $I_{\text{HR}}$ ($I_{\text{HR}^{{}^{\prime}}}$)
    之间的一致性，
- en: '| (13) |  | $\mathcal{L}_{\text{Cycle}}(I_{\text{LR}},I_{\text{LR}^{{}^{\prime}}},I_{\text{HR}},I_{\text{HR}^{{}^{\prime}}})=\left\&#124;I_{\text{LR}}-I_{\text{LR}^{{}^{\prime}}}\right\&#124;_{2}+\left\&#124;I_{\text{HR}}-I_{\text{HR}^{{}^{\prime}}}\right\&#124;_{2}.$
    |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\mathcal{L}_{\text{Cycle}}(I_{\text{LR}},I_{\text{LR}^{{}^{\prime}}},I_{\text{HR}},I_{\text{HR}^{{}^{\prime}}})=\left\&#124;I_{\text{LR}}-I_{\text{LR}^{{}^{\prime}}}\right\&#124;_{2}+\left\&#124;I_{\text{HR}}-I_{\text{HR}^{{}^{\prime}}}\right\&#124;_{2}.$
    |  |'
- en: In addition to the above loss functions, many other loss functions are also
    used in FSR, including style loss [[53](#bib.bib53)], feature match loss [[54](#bib.bib54)],
    *etc*. Due to the limitation of space, we do not introduce them in detail.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述损失函数外，FSR 中还使用了许多其他损失函数，包括风格损失 [[53](#bib.bib53)]、特征匹配损失 [[54](#bib.bib54)]，*等等*。由于篇幅限制，我们不再详细介绍。
- en: 3\. Characteristics of Face Images
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 面部图像特征
- en: '![Refer to caption](img/0d1a67af7fc49c3631849dce7ba91d91.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0d1a67af7fc49c3631849dce7ba91d91.png)'
- en: Figure 2\. Facial characteristics.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 面部特征。
- en: Human face is a highly structured object with its own unique characteristics,
    which can be explored and utilized in FSR task. In this section, we simply introduce
    these facial characteristics.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸是一个高度结构化的对象，具有其独特的特征，这些特征可以在 FSR 任务中被探索和利用。在本节中，我们简单介绍这些面部特征。
- en: 3.1\. Prior Information
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 先验信息
- en: 'As shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3\. Characteristics of Face Images
    ‣ Deep Learning-based Face Super-Resolution: A Survey"), structural priors can
    be found in face images, such as facial landmarks, facial heatmaps and facial
    paring maps.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S3.F2 "图 2 ‣ 3\. 面部图像特征 ‣ 基于深度学习的面部超分辨率：综述") 所示，面部图像中可以找到结构性先验，例如面部关键点、面部热图和面部配对图。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Facial landmarks*: These locate the key points of facial components. The number
    of landmarks varies in different datasets, such as CelebA [[55](#bib.bib55)],
    which provides five landmarks while Helen [[56](#bib.bib56)] offers 194 landmarks.'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*面部关键点*：这些点标识面部组件的关键位置。不同的数据集中关键点的数量不同，例如 CelebA [[55](#bib.bib55)] 提供了五个关键点，而
    Helen [[56](#bib.bib56)] 提供了 194 个关键点。'
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Facial heatmaps*: These are generated from facial landmarks. Facial landmarks
    give accurate points of the facial components, while heatmaps give the probability
    of the point being a facial landmark. To generate the heatmaps, every landmark
    is represented by a Gaussian kernel centered on the location of the landmark.'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*面部热图*：这些热图是从面部关键点生成的。面部关键点给出面部组件的准确位置，而热图则给出该点是面部关键点的概率。为了生成热图，每个关键点由一个以关键点位置为中心的高斯核表示。'
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Facial parsing maps*: These are semantic segmentation maps of face images
    separating the facial components from face images, including eyes, nose, mouth,
    skin, ears, hair, and others.'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*面部解析图*：这些是面部图像的语义分割图，将面部组件从面部图像中分离出来，包括眼睛、鼻子、嘴巴、皮肤、耳朵、头发等。'
- en: These face structure prior information can provide the location of facial components
    and facial structure information. We can expect to recover more reasonable target
    face images if we incorporate these prior knowledge to regularize or guide the
    FSR models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些面部结构的先验信息可以提供面部组件的位置和面部结构信息。如果我们将这些先验知识用于规范化或引导FSR模型，我们可以期待恢复出更合理的目标面部图像。
- en: Table 2\. Summary of public face image datasets for FSR.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. FSR 的公共面部图像数据集总结。
- en: '| Dataset | Number | #Attributes | #Landmarks | Parsing maps | Identity |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 数量 | #属性 | #标志点 | 解析图 | 身份 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  |  |  |  |  |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| CelebA [[55](#bib.bib55)] | 202,599 | 40 | 5 | $\times$ | ✓ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| CelebA [[55](#bib.bib55)] | 202,599 | 40 | 5 | $\times$ | ✓ |'
- en: '| CelebAMask-HQ [[57](#bib.bib57)] | 30,000 | $\times$ | $\times$ | ✓ | $\times$
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| CelebAMask-HQ [[57](#bib.bib57)] | 30,000 | $\times$ | $\times$ | ✓ | $\times$
    |'
- en: '| Helen [[56](#bib.bib56)] | 2,330 | $\times$ | 194 | ✓ | $\times$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Helen [[56](#bib.bib56)] | 2,330 | $\times$ | 194 | ✓ | $\times$ |'
- en: '| FFHQ [[58](#bib.bib58)] | 70,000 | $\times$ | 68 | $\times$ | $\times$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| FFHQ [[58](#bib.bib58)] | 70,000 | $\times$ | 68 | $\times$ | $\times$ |'
- en: '| AFLW [[59](#bib.bib59)] | 25,993 | $\times$ | 21 | $\times$ | $\times$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| AFLW [[59](#bib.bib59)] | 25,993 | $\times$ | 21 | $\times$ | $\times$ |'
- en: '| 300W [[60](#bib.bib60)] | 3,837 | $\times$ | 68 | $\times$ | $\times$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 300W [[60](#bib.bib60)] | 3,837 | $\times$ | 68 | $\times$ | $\times$ |'
- en: '| LS3D-W [[61](#bib.bib61)] | 230,000 | $\times$ | 68 | $\times$ | $\times$
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LS3D-W [[61](#bib.bib61)] | 230,000 | $\times$ | 68 | $\times$ | $\times$
    |'
- en: '| Menpo [[62](#bib.bib62)] | 9,000 | $\times$ | 68 | $\times$ | $\times$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Menpo [[62](#bib.bib62)] | 9,000 | $\times$ | 68 | $\times$ | $\times$ |'
- en: '| LFW [[63](#bib.bib63)] | 13,233 | 73 | $\times$ | $\times$ | ✓ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LFW [[63](#bib.bib63)] | 13,233 | 73 | $\times$ | $\times$ | ✓ |'
- en: '| LFWA [[64](#bib.bib64)] | 13,233 | 40 | $\times$ | $\times$ | ✓ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LFWA [[64](#bib.bib64)] | 13,233 | 40 | $\times$ | $\times$ | ✓ |'
- en: '| VGGFace [[65](#bib.bib65)] | 3,310,000 | $\times$ | $\times$ | $\times$ |
    ✓ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| VGGFace [[65](#bib.bib65)] | 3,310,000 | $\times$ | $\times$ | $\times$ |
    ✓ |'
- en: 3.2\. Attribute Information
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 属性信息
- en: 'Second, the attributes, such as gender, hair color, and others, are the affiliated
    features of face images and can be seen as semantic-level information. In FSR,
    because of one-to-many maps from LR images to HR ones, the recovered face image
    may contain artifacts and even wrong attributes. For example, the face in the
    recovered result does not wear but the ground truth wears eyeglasses. At this
    time, attribute information can remind the network which attribute should be covered
    in the result. From a different perspective, attribute information also contains
    facial details. Taking eyeglasses as an example, the attribute of wearing eyeglasses
    provides the details of the facial eyes. We provide a concise example of attribute
    information in Fig. [2](#S3.F2 "Figure 2 ‣ 3\. Characteristics of Face Images
    ‣ Deep Learning-based Face Super-Resolution: A Survey"). Moreover, these attributes
    are always binary in the face dataset, 1 denotes that the face image has the attribute,
    while 0 means there is no such information.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，属性，如性别、发色等，是面部图像的附属特征，可以视为语义级别的信息。在FSR中，由于从LR图像到HR图像的一对多映射，恢复的面部图像可能包含伪影甚至错误的属性。例如，恢复结果中的面部没有佩戴眼镜，而实际情况中却佩戴了眼镜。这时，属性信息可以提醒网络在结果中应覆盖哪个属性。从不同的角度来看，属性信息也包含面部细节。以眼镜为例，佩戴眼镜的属性提供了面部眼睛的细节。我们在图 [2](#S3.F2
    "图 2 ‣ 3\. 面部图像的特点 ‣ 基于深度学习的面部超分辨率：综述")中提供了属性信息的简要示例。此外，这些属性在面部数据集中通常是二元的，1表示面部图像具有该属性，而0表示没有此信息。
- en: 3.3\. Identity Information
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 身份信息
- en: Third, every face image corresponds to a person, which is enabled by identity
    information. This type of information is always used for keeping the identity
    consistency between the super-resolved result and the ground truth. On the one
    hand, the person should not be changed after super-resolution visually. On the
    other hand, FSR should facilitate the performance of face recognition. Similar
    to attribute information, identity also offers high-level constraints to the FSR
    task and is beneficial to face restoration.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，每个面部图像对应一个人，这得益于身份信息。这类信息总是用于保持超分辨率结果与真实图像之间的身份一致性。一方面，经过超分辨率处理后，视觉上不应更改人的身份。另一方面，FSR应促进面部识别性能。类似于属性信息，身份信息也为FSR任务提供了高级约束，并有利于面部恢复。
- en: 3.4\. Datasets for FSR
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. FSR数据集
- en: 'In recent years, many face image datasets are used for FSR, which differ in
    many aspects, *e.g.*, the number of samples, facial characteristics contained
    and others. In Table [2](#S3.T2 "Table 2 ‣ 3.1\. Prior Information ‣ 3\. Characteristics
    of Face Images ‣ Deep Learning-based Face Super-Resolution: A Survey"), we list
    a number of commonly used face image datasets and simply indicate their amount
    and the facial characteristics offered. For parsing maps and identity, we only
    present whether they are provided or not, while for attributes and landmarks,
    we offer the specific amount. Aside from these datasets, many other face datasets
    are used in FSR, including CACD200 [[66](#bib.bib66)], VGGFace2 [[67](#bib.bib67)],
    UMDFaces [[68](#bib.bib68)], CASIA-WebFace [[69](#bib.bib69)], and others. It
    is worth noting that all above-mentioned datasets only provide HR face images.
    If we want to use them for training and evaluating any super-resolution model,
    we need to generate the corresponding LR face images using the degradation model
    introduced in Section  [2](#S2 "2\. Background ‣ Deep Learning-based Face Super-Resolution:
    A Survey").'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，许多面部图像数据集被用于FSR，这些数据集在许多方面存在差异，例如，样本数量、包含的面部特征等。在表[2](#S3.T2 "表 2 ‣ 3.1\.
    先验信息 ‣ 3\. 面部图像特征 ‣ 基于深度学习的面部超分辨率：综述")中，我们列出了许多常用的面部图像数据集，并简单说明了它们的数量和提供的面部特征。对于解析图和身份，我们只列出是否提供，而对于属性和标记，我们提供了具体数量。除了这些数据集，还有许多其他面部数据集被用于FSR，包括CACD200
    [[66](#bib.bib66)]、VGGFace2 [[67](#bib.bib67)]、UMDFaces [[68](#bib.bib68)]、CASIA-WebFace
    [[69](#bib.bib69)]等。值得注意的是，上述所有数据集仅提供高分辨率面部图像。如果我们想用它们来训练和评估任何超分辨率模型，我们需要使用第[2](#S2
    "2\. 背景 ‣ 基于深度学习的面部超分辨率：综述")节中介绍的退化模型生成相应的低分辨率面部图像。
- en: 4\. FSR Methods
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. FSR方法
- en: 'At present, various deep learning FSR methods have been proposed. On the one
    hand, these methods tap the potential of the efficient network for FSR regardless
    of facial characteristics, *i.e.*, developing a basic convolution neural network
    (CNN) or generative adversarial network (GAN) for face reconstruction. On the
    other hand, some approaches focus on the utilization of facial characteristics,
    *e.g.*, using structure prior information to facilitate face restoration and so
    on. Furthermore, some recently proposed models introduce additional high-quality
    reference face images to assist the restoration. Here, according to the type of
    face image special information used, we divide FSR methods into five categories:
    general FSR, prior-guided FSR, attribute-constrained FSR, identity-preserving
    FSR, and reference FSR. In this section, we concentrate on every kind of FSR method
    and introduce each category in detail.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，已经提出了各种深度学习的面部超分辨率（FSR）方法。一方面，这些方法挖掘了高效网络在面部超分辨率中的潜力，而不考虑面部特征，即，开发基础的卷积神经网络（CNN）或生成对抗网络（GAN）进行面部重建。另一方面，一些方法则专注于面部特征的利用，例如，使用结构先验信息来促进面部恢复等。此外，一些最近提出的模型引入了额外的高质量参考面部图像来协助恢复。在这里，根据使用的面部图像特殊信息类型，我们将FSR方法分为五类：通用FSR、先验引导FSR、属性约束FSR、身份保留FSR和参考FSR。在本节中，我们集中介绍每种FSR方法，并详细介绍每一类别。
- en: 4.1\. General FSR
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 通用FSR
- en: 'General FSR methods mainly focus on designing an efficient network and exploit
    the potential of efficient network structure for FSR without any facial characteristics.
    In the early days, most of these methods are based on CNN and incorporate various
    advanced architectures (including back projection, residual network, spatial or
    channel attention, *etc*.), to improve the representation ability of the network.
    Since then, many FSR methods by using advanced networks have been proposed. We
    divide general FSR methods into four categories: basic CNN-based methods, GAN-based
    methods, reinforcement learning-based methods, and ensemble learning-based methods.
    Aiming to present a clear and concise overview, we summarize the general FSR methods
    in Fig. [3](#S4.F3 "Figure 3 ‣ 4.1\. General FSR ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey").'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '通用FSR方法主要关注设计高效的网络，并利用高效网络结构在没有任何面部特征的情况下进行FSR。在早期，大多数这些方法基于CNN，并结合了各种先进的架构（包括反向投影、残差网络、空间或通道注意力，*等*），以提高网络的表示能力。从那时起，许多利用先进网络的FSR方法被提出。我们将通用FSR方法分为四类：基本的基于CNN的方法、基于GAN的方法、基于强化学习的方法和基于集成学习的方法。为了呈现一个清晰简洁的概述，我们在图[3](#S4.F3
    "Figure 3 ‣ 4.1\. General FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey")中总结了通用FSR方法。'
- en: '![Refer to caption](img/e9e6428b57e8ddd7a2ebcda28b396f59.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e9e6428b57e8ddd7a2ebcda28b396f59.png)'
- en: Figure 3\. Overview of general FSR methods.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 通用FSR方法概述。
- en: 4.1.1\. Basic CNN-based Methods
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 基本CNN方法
- en: 'Inspired by the pioneer deep learning general image super-resolution method
    [[70](#bib.bib70)], some researchers also propose to incorporate the CNN network
    into the FSR task. Depending on whether they consider the global information and
    local differences, we can further divide the basic CNN-based methods into three
    categories: global methods that feed the entire face into the network and recover
    face images globally, local methods that divide face images into different components
    and then recover them, and mixed methods that recover face images locally and
    globally.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 受到开创性深度学习通用图像超分辨率方法[[70](#bib.bib70)]的启发，一些研究者也提出将CNN网络引入FSR任务。根据是否考虑全球信息和局部差异，我们可以将基本的基于CNN的方法进一步分为三类：将整个面部图像输入网络并全局恢复面部图像的全局方法，将面部图像划分为不同组件然后恢复的局部方法，以及局部和全局恢复面部图像的混合方法。
- en: 'Global Methods: In the early years, researchers treat a face image as a whole
    and recover it globally. Inspired by the strong representative ability of CNN,
    bi-channel convolutional neural network (BCCNN) [[71](#bib.bib71)] and  [[72](#bib.bib72)]
    directly learn a mapping from LR face images to HR ones. Then, benefiting from
    the performance gain of iterative back projection (IBP) in general image super-resolution,
    Huang *et al*. [[73](#bib.bib73)] introduce IBP to FSR as an extra post-processing
    step, developing the SRCNN-IBP method. After that, the thought of back projection
    is generally used in FSR [[74](#bib.bib74), [75](#bib.bib75)]. Later on, channel
    and spatial attention mechanisms greatly improve the general image super-resolution
    methods, which inspires researchers to explore their utilization in FSR. Thus,
    a number of innovative methods integrating the attention mechanism are proposed [[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78)]. In these works, two representative methods
    are E-ComSupResNet [[77](#bib.bib77)] that introduces a channel attention mechanism
    and SPARNet [[78](#bib.bib78)] which has a well-designed spatial attention for
    FSR. Besides that, many researchers design the cascaded model and exploit multi-scale
    information to improve the restoration performance [[79](#bib.bib79), [80](#bib.bib80),
    [81](#bib.bib81)].'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 全局方法：在早期，研究人员将面部图像视为一个整体并进行全局恢复。受到CNN强大代表能力的启发，双通道卷积神经网络（BCCNN）[[71](#bib.bib71)]
    和[[72](#bib.bib72)] 直接学习从LR面部图像到HR面部图像的映射。随后，受益于迭代反向投影（IBP）在通用图像超分辨率中的性能提升，黄*等*[[73](#bib.bib73)]
    将IBP引入FSR作为额外的后处理步骤，开发了SRCNN-IBP方法。之后，反向投影的思想在FSR中普遍应用[[74](#bib.bib74), [75](#bib.bib75)]。随后，通道和空间注意力机制极大地提升了通用图像超分辨率方法，这激发了研究人员探索其在FSR中的应用。因此，提出了许多集成注意力机制的创新方法[[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78)]。在这些工作中，有两个代表性的方法是引入通道注意力机制的E-ComSupResNet[[77](#bib.bib77)]和为FSR设计了精妙空间注意力的SPARNet[[78](#bib.bib78)]。此外，许多研究人员设计了级联模型并利用多尺度信息来提高恢复性能[[79](#bib.bib79),
    [80](#bib.bib80), [81](#bib.bib81)]。
- en: It is observed that super-resolution in the image domain produces smooth results
    without high-frequency detail. Considering that wavelet transform can represent
    the textural and contextual information of the images, WaSRNet [[82](#bib.bib82)]
    and [[83](#bib.bib83)] transform face images into wavelet coefficients and super-resolve
    the face images in the wavelet coefficient domain to avoid over-smooth results.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到图像域中的超分辨率生成平滑结果而没有高频细节。考虑到小波变换可以表示图像的纹理和上下文信息，WaSRNet [[82](#bib.bib82)]
    和 [[83](#bib.bib83)] 将面部图像转换为小波系数，并在小波系数域中进行超分辨率处理，以避免过度平滑的结果。
- en: 'Local Methods: Global methods can capture global information but cannot well
    recover the face details. Thus, local methods are developed to recover different
    parts of a face image differently. Super-resolution technique based on de?nition-scalable
    inference (SRDSI) [[84](#bib.bib84)] decomposes the face into a basic face with
    low-frequency and a compensation face with high-frequency through PCA. Then, SRDSI
    recovers the basic face and the compensation face with very deep convolutional
    network (VDSR) [[85](#bib.bib85)] and sparse representation respectively. Finally,
    the two recovered faces are fused. After that, many patch-based methods have been
    proposed [[86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88)], all of which
    divide face images into several patches and train models for recovering the corresponding
    patches.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 局部方法：全局方法可以捕捉全局信息，但不能很好地恢复面部细节。因此，局部方法被开发出来以不同的方式恢复面部图像的不同部分。基于定义可缩放推断（SRDSI）[[84](#bib.bib84)]
    的超分辨率技术通过PCA将面部图像分解为低频的基本面部和高频的补偿面部。然后，SRDSI 分别使用非常深的卷积网络（VDSR）[[85](#bib.bib85)]
    和稀疏表示来恢复基本面部和补偿面部。最后，融合这两个恢复的面部。之后，许多基于补丁的方法被提出 [[86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88)]，这些方法都将面部图像分成若干补丁，并训练模型以恢复对应的补丁。
- en: 'Mixed Methods: Considering that global methods can capture global structure
    but ignore local details while local methods focus on local details but lose global
    structure, a line of research naturally combines global and local methods for
    capturing global structure and recovering local details simultaneously. At first,
    global-local network [[89](#bib.bib89), [90](#bib.bib90)] develop a global upsampling
    network to model global constraints and a local enhancement network to learn face-specific
    details. To simultaneously capture global clues and recover local details, dual-path
    deep fusion network (DPDFN) [[91](#bib.bib91)] constructs two individual branches
    for learning global facial contours and local facial component details, and then
    fuses the result of the two branches to generate the final SR result.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法：考虑到全局方法可以捕捉全局结构但忽略局部细节，而局部方法则关注局部细节但失去全局结构，因此，一系列研究自然地结合全局和局部方法，以同时捕捉全局结构并恢复局部细节。最初，global-local
    network [[89](#bib.bib89), [90](#bib.bib90)] 开发了一个全局上采样网络来建模全局约束和一个局部增强网络来学习面部特定的细节。为了同时捕捉全局线索和恢复局部细节，dual-path
    deep fusion network (DPDFN) [[91](#bib.bib91)] 构建了两个独立的分支来学习全局面部轮廓和局部面部组件细节，然后融合这两个分支的结果以生成最终的SR结果。
- en: 4.1.2\. GAN-based Methods
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2. 基于GAN的方法
- en: Compared with CNN-based methods that utilize pixel-wise loss and generate smooth
    face images, GAN, first proposed by Goodfellow *et al*. [[48](#bib.bib48)], which
    can be applied to generate realistic-looking face images with more details, inspires
    researchers to design GAN-based methods. At first, researchers focus on designing
    various GANs to learn from paired or unpaired data. In recent years, how to utilize
    a pretrained generative model to boost FSR has attracted increasing attention.
    Therefore, GAN-based methods can be divided into general GAN-based methods and
    generative prior-based methods.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用像素级损失并生成平滑面部图像的基于CNN的方法相比，**Goodfellow** *等人* 提出的GAN [[48](#bib.bib48)] 可以生成更具细节的逼真面部图像，这激发了研究人员设计基于GAN的方法。最初，研究人员专注于设计各种GAN来从配对或非配对数据中学习。近年来，如何利用预训练的生成模型来提升FSR引起了越来越多的关注。因此，基于GAN的方法可以分为一般的基于GAN的方法和基于生成先验的方法。
- en: 'General GAN-based Methods: In the early stage, Yu *et al*. [[34](#bib.bib34)]
    develop ultra-resolving face images by discriminative generative networks (URDGN),
    which consists of two subnetworks: a discriminative model to distinguish a real
    HR face image or an arti?cially super-resolved output, and a generative model
    to generate SR face images to fool the discriminative model and match the distribution
    of HR face images. MLGE [[92](#bib.bib92)] not only designs discriminators to
    distinguish face images but also applies edge maps of the face images to reconstruct
    HR face images. Recently, HiFaceGAN [[93](#bib.bib93)] and the works of [[94](#bib.bib94),
    [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97)] also super-resolve face
    images with generative models. Instead of directly feeding the whole face images
    into the discriminator, PCA-SRGAN [[98](#bib.bib98)] decomposes face images into
    components by PCA and progressively feeds increasing components of the face images
    into the discriminator to reduce the learning difficulty of the discriminator.
    The commonality of these types of GAN is that the discriminator outputs a single
    probability value to characterize whether the result is a real face image. However,
    Zhang *et al*. [[99](#bib.bib99)] assume that a single probability value is too
    fragile to represent a whole image, thus they design a supervised pixel-wise GAN
    (SPGAN) whose discriminator outputs a discriminative matrix with the same resolution
    as the input images, and design a supervised pixel-wise adversarial loss, thus
    recovering more photo-realistic face images.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通用 GAN 基础方法：在早期，Yu *et al*. [[34](#bib.bib34)] 开发了通过判别生成网络（URDGN）来超分辨率面部图像，该网络由两个子网络组成：一个判别模型用于区分真实的
    HR 面部图像或人工超分辨率输出，另一个生成模型用于生成 SR 面部图像以欺骗判别模型，并匹配 HR 面部图像的分布。MLGE [[92](#bib.bib92)]
    不仅设计了判别器来区分面部图像，还应用了面部图像的边缘图来重建 HR 面部图像。最近，HiFaceGAN [[93](#bib.bib93)] 和 [[94](#bib.bib94),
    [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97)] 的研究也使用生成模型进行面部图像的超分辨率。PCA-SRGAN
    [[98](#bib.bib98)] 不直接将整个面部图像输入判别器，而是通过 PCA 将面部图像分解为组件，并逐步将越来越多的组件输入判别器，以减少判别器的学习难度。这些类型的
    GAN 的共同点是判别器输出一个单一的概率值来表征结果是否为真实的面部图像。然而，Zhang *et al*. [[99](#bib.bib99)] 假设单一的概率值过于脆弱，无法代表整个图像，因此设计了一种监督像素级
    GAN（SPGAN），其判别器输出与输入图像分辨率相同的判别矩阵，并设计了一种监督像素级对抗损失，从而恢复出更多逼真的面部图像。
- en: The above methods rely on the artificial LR and HR pairs generated by a known
    degradation. However, the quality of the real-world LR image is affected by a
    wide range of factors such as the imaging conditions and the imaging system, leading
    to the complicated unknown degradation of real LR images. The gap between real
    LR images and artificial LR ones is large and will inevitably decrease the performance
    when applying methods trained on the artificial pairs to real LR images [[100](#bib.bib100)].
    To settle this problem, real-world super-resolution (RWSR) [[101](#bib.bib101)]
    first estimates the parameters from real LR faces, such as the blur kernel, noise,
    and compression, and then generates the LR and HR face image pairs with estimated
    parameters for the training of the model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法依赖于由已知退化生成的人工 LR 和 HR 对。然而，真实世界中的 LR 图像质量受到诸多因素的影响，如成像条件和成像系统，这导致真实 LR 图像的退化复杂且未知。真实
    LR 图像与人工 LR 图像之间的差距较大，必然会降低将基于人工对训练的方法应用于真实 LR 图像的性能 [[100](#bib.bib100)]。为了解决这个问题，真实世界超分辨率（RWSR）
    [[101](#bib.bib101)] 首先从真实的 LR 面部图像中估计参数，如模糊核、噪声和压缩，然后生成具有估计参数的 LR 和 HR 面部图像对，以用于模型的训练。
- en: LRGAN [[102](#bib.bib102)] proposes to learn the degradation before super-resolution
    from unpaired data. It designs a high-to-low GAN to learn the real degradation
    process from unpaired LR and HR face images and create paired LR and HR face images
    for training low-to-high GAN. Specifically, with HR face images as input, the
    high-to-low GAN generates LR face images (GLR) that should belong to the real
    LR distribution and be close to the corresponding downsampled HR face images.
    Then, for low-to-high GAN, GLRs are fed into the generator to recover the SR results
    which have to be close to HR face images and match the real HR distribution. Goswami
    *et al*. [[103](#bib.bib103)] further develop a robust FSR method and Zheng *et
    al*.  [[104](#bib.bib104)] utilize semi-dual optimal transport to guide model
    learning and develop semi-dual optimal transport CycleGAN. Considering that discrepancies
    between GLRs in the training phase and real LR face images in the testing phase
    still exist, researchers introduce the concept of characteristic regularization
    (CR) [[105](#bib.bib105)]. Different from LRGAN, CR transforms the real LR face
    images into artificial LR ones and then conducts super-resolution reconstruction
    in the artificial LR space. Based on CycleGAN, CR learns the mapping between real
    LR face images and artificial LR ones. Then, it uses the artificial LR face images
    generated from real LR ones to fine-tune the super-resolution model, which is
    pretrained by the artificial pairs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: LRGAN [[102](#bib.bib102)] 提出了从未配对数据中学习超分辨率之前的降解。它设计了一个高到低的 GAN 来从未配对的 LR 和
    HR 面部图像中学习真实的降解过程，并创建配对的 LR 和 HR 面部图像以训练低到高的 GAN。具体而言，以 HR 面部图像作为输入，高到低的 GAN 生成的
    LR 面部图像（GLR）应属于真实的 LR 分布，并接近相应的降采样 HR 面部图像。然后，对于低到高的 GAN，GLR 被输入生成器中以恢复 SR 结果，这些结果必须接近
    HR 面部图像并匹配真实的 HR 分布。Goswami *et al*. [[103](#bib.bib103)] 进一步开发了一种鲁棒的 FSR 方法，而
    Zheng *et al*. [[104](#bib.bib104)] 利用半对偶最优传输来指导模型学习，并开发了半对偶最优传输 CycleGAN。考虑到训练阶段
    GLR 和测试阶段真实 LR 面部图像之间仍存在差异，研究人员引入了特征正则化（CR）[[105](#bib.bib105)] 的概念。不同于 LRGAN，CR
    将真实的 LR 面部图像转换为人工 LR 图像，然后在人工 LR 空间中进行超分辨率重建。基于 CycleGAN，CR 学习真实 LR 面部图像与人工 LR
    图像之间的映射。然后，它使用由真实 LR 图像生成的人工 LR 面部图像来微调超分辨率模型，该模型由人工配对进行预训练。
- en: 'Generative prior-based methods: Recently, many face generation models, such
    as popular StyleGAN [[58](#bib.bib58)], StyleGAN v2 [[106](#bib.bib106)], ProGAN [[107](#bib.bib107)],
    StarGAN [[108](#bib.bib108)], *etc.*, have been proposed and they are capable
    of generating faithful faces with a high degree of variability. Thus, more and
    more researchers explore the generative prior of pretrained GAN.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生成先验的方法：最近，许多面部生成模型，如流行的 StyleGAN [[58](#bib.bib58)]、StyleGAN v2 [[106](#bib.bib106)]、ProGAN
    [[107](#bib.bib107)]、StarGAN [[108](#bib.bib108)]，*等*，已经被提出，它们能够生成具有高度变异性的真实面孔。因此，越来越多的研究人员探索预训练
    GAN 的生成先验。
- en: 'The first generative prior-based FSR method is PULSE [[109](#bib.bib109)].
    It formulates FSR as a generation problem to generate high-quality SR face image
    so that the downsampled SR result is close to LR face image. Mathematically, the
    problem can be expressed as:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种基于生成先验的 FSR 方法是 PULSE [[109](#bib.bib109)]。它将 FSR 表述为一个生成问题，以生成高质量的 SR 面部图像，使得降采样后的
    SR 结果接近 LR 面部图像。在数学上，这个问题可以表示为：
- en: '| (14) |  | $min_{G}\left\&#124;G(z)\downarrow_{s}-I_{\text{LR}}\right\&#124;,$
    |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $min_{G}\left\&#124;G(z)\downarrow_{s}-I_{\text{LR}}\right\&#124;,$
    |  |'
- en: where $z$ is a randomly sampled latent vector and the input of the pretrained
    StyleGAN [[58](#bib.bib58)], $\downarrow$ is the downsampling operation, $s$ is
    the downsampling factor, and $G$ denotes the function of the generator. PULSE
    solves FSR from a new perspective and this inspires many other works.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $z$ 是一个随机抽样的潜在向量，并且是预训练 StyleGAN [[58](#bib.bib58)] 的输入，$\downarrow$ 是降采样操作，$s$
    是降采样因子，$G$ 表示生成器的函数。PULSE 从新的角度解决了 FSR 问题，这也激发了许多其他研究。
- en: However, the latent code $z$ in PULSE is randomly sampled and in low dimension,
    making the generated images lose important spatial information. To overcome this
    problem, GLEAN [[110](#bib.bib110)], CFP-GAN [[111](#bib.bib111)] and GPEN [[112](#bib.bib112)]
    are developed. Rather than directly employing the pretrained StyleGAN [[58](#bib.bib58)],
    they develop their own networks and embed the pretrained generation network of
    StyleGAN [[58](#bib.bib58)] into their own networks to incorporate the generative
    prior. To maintain faithful information, they not only obtain latent code by encoding
    LR face images instead of randomly sampling, but also extract multi-scale features
    from LR face images and fuse the features into the generation network. In this
    way, the generative prior provided by the pretrained StyleGAN can be fully utilized
    and the important spatial information can be well maintained.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，PULSE中的潜在代码$z$是随机采样且维度较低的，这使得生成的图像丧失了重要的空间信息。为了解决这个问题，开发了GLEAN [[110](#bib.bib110)]、CFP-GAN [[111](#bib.bib111)]和GPEN [[112](#bib.bib112)]。它们不是直接使用预训练的StyleGAN [[58](#bib.bib58)]，而是开发了自己的网络，并将预训练的StyleGAN [[58](#bib.bib58)]生成网络嵌入到自己的网络中，以融合生成先验。为了保持真实信息，它们不仅通过编码LR面部图像来获取潜在代码，而不是随机采样，还从LR面部图像中提取多尺度特征，并将这些特征融合到生成网络中。通过这种方式，预训练的StyleGAN提供的生成先验可以得到充分利用，重要的空间信息也可以很好地保持。
- en: 4.1.3\. Reinforcement Learning-based Methods
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 基于强化学习的方法
- en: 'Deep learning-based FSR methods learn the mapping from LR face images to HR
    ones, but ignore the contextual dependencies among the facial parts. Cao *et al*.
    propose to recurrently discover facial parts and enhance them by fully exploiting
    the global inter-dependency of the image, then attention-aware face hallucination
    via deep reinforcement learning (Attention-FH) is proposed [[113](#bib.bib113)].
    Specifically, Attention-FH has two subnetworks: a policy network that locates
    the region that needs to be enhanced in the current step, and a local enhancement
    network that enhances the selected region.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的FSR方法学习从LR面部图像到HR面部图像的映射，但忽略了面部部分之间的上下文依赖。Cao *等人* 提出了通过充分利用图像的全局互依性来递归发现面部部分并增强它们的方案，随后提出了通过深度强化学习的注意力感知面部幻觉（Attention-FH）[[113](#bib.bib113)]。具体来说，Attention-FH有两个子网络：一个策略网络用于定位当前步骤中需要增强的区域，另一个局部增强网络则用于增强选定的区域。
- en: 4.1.4\. Ensemble Learning-based Methods
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. 基于集成学习的方法
- en: CNN-based methods utilize pixel-wise loss to recover face images with higher
    PSNR and smoother details while GAN-based methods can generate face images with
    lower PSNR but more high-frequency details. To combine the advantages of different
    types of methods, ensemble learning is used in adaptive threshold-based multi-model
    fusion network (ATFMN) [[114](#bib.bib114)]. Specifically, ATFMN uses three models
    (CNN-based, GAN-based, and RNN-based) to generate candidate SR faces, and then
    fuses all candidate SR faces to reconstruct the final SR result. In contrast to
    previous approaches, ATFMN exploits the potential of ensemble learning for FSR
    instead of focusing on a single model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN的方法利用逐像素损失来恢复具有更高PSNR和更平滑细节的面部图像，而基于GAN的方法则可以生成具有较低PSNR但更多高频细节的面部图像。为了结合不同方法的优点，集成学习被用于自适应阈值多模型融合网络（ATFMN）[[114](#bib.bib114)]。具体来说，ATFMN使用三个模型（基于CNN、基于GAN和基于RNN）生成候选SR面部图像，然后融合所有候选SR面部图像以重建最终的SR结果。与之前的方法相比，ATFMN利用集成学习的潜力进行FSR，而不是专注于单一模型。
- en: 4.1.5\. Discussion
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5\. 讨论
- en: Here we discuss the pros and cons among these sub-categories in general FSR
    methods. From a global perspective, the difference between CNN-based and GAN-based
    methods relies on adversarial learning. CNN-based methods tend to utilize pixel-wise
    loss, leading to higher PSNR and smoother results, while GAN-based methods might
    recover visually pleasing face images with more details but lower PSNR. Each of
    them has its own merits. Compared with them, ensemble learning-based method can
    combine their advantages to make up their deficiencies by integrating multiple
    models. However, ensemble learning inevitably results in the increase of memory,
    computation and parameters. Reinforcement learning-based methods recover the attentional
    local regions by sequentially searching, and consider the contextual dependency
    of patches from a global perspective, which brings improvement of performance
    but needs much more training time and computational cost.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论这些子类别在一般FSR方法中的优缺点。从整体来看，基于CNN的方法和基于GAN的方法的区别在于对抗学习。基于CNN的方法倾向于使用逐像素损失，从而导致更高的PSNR和更平滑的结果，而基于GAN的方法可能恢复出视觉上更令人愉悦、细节更多的面部图像，但PSNR较低。它们各有优点。与之相比，基于集成学习的方法可以通过整合多个模型来结合它们的优点，弥补不足。然而，集成学习不可避免地会导致内存、计算和参数的增加。基于强化学习的方法通过顺序搜索恢复注意局部区域，并从全局角度考虑补丁的上下文依赖性，这带来了性能的提高，但需要更多的训练时间和计算成本。
- en: '![Refer to caption](img/43492a3ffd0f576609289c5bf2abea63.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43492a3ffd0f576609289c5bf2abea63.png)'
- en: (a) Framework of pre-prior methods.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 预先先验方法的框架。
- en: '![Refer to caption](img/17873604e5799e6b84b85fe88e923f1f.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17873604e5799e6b84b85fe88e923f1f.png)'
- en: (b) Framework of parallel-prior methods.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 并行先验方法的框架。
- en: '![Refer to caption](img/2631825854efc7a6a422886dbd016867.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2631825854efc7a6a422886dbd016867.png)'
- en: (c) Framework of in-prior methods.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 内部先验方法的框架。
- en: '![Refer to caption](img/a514b776a4087baec8120b0e8a2b24d4.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a514b776a4087baec8120b0e8a2b24d4.png)'
- en: (d) Framework of post-prior methods.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 后先验方法的框架。
- en: Figure 4\. Four frameworks of prior-guided FSR methods. PEN is prior estimation
    network, SRN is super-resolution network, FEN is a feature extraction network,
    and P is prior information.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 基于先验的FSR方法的四种框架。PEN是先验估计网络，SRN是超分辨率网络，FEN是特征提取网络，P是先验信息。
- en: 4.2\. Prior-guided FSR
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 基于先验的FSR
- en: General FSR methods aim to design efficient networks. Nevertheless, as a highly
    structured object, human face has some specific characteristics, such as prior
    information (including facial landmarks, facial parsing maps, and facial heatmaps),
    which are ignored by general FSR methods. Therefore, to recover facial images
    with a much clearer facial structure, researchers begin to develop prior-guided
    FSR methods.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的FSR方法旨在设计高效的网络。然而，作为一个高度结构化的对象，人脸具有一些特定的特征，如先验信息（包括面部标志、面部解析图和面部热图），这些特征被一般FSR方法忽略。因此，为了恢复具有更清晰面部结构的面部图像，研究人员开始开发基于先验的FSR方法。
- en: '![Refer to caption](img/7616b0f754e60f0b976f39601851e5a7.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7616b0f754e60f0b976f39601851e5a7.png)'
- en: Figure 5\. Milestones of prior-guided FSR methods. We simply list their names
    and venues.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 基于先验的FSR方法的里程碑。我们简单列出了它们的名称和场所。
- en: 'Prior-guided FSR methods refer to extracting facial prior information and utilizing
    it to facilitate face reconstruction. Considering the order of prior information
    extraction and FSR, we further divide the prior-guided FSR methods into four parts:
    i) pre-prior methods that extract prior information followed by FSR; ii) parallel-prior
    methods that extract prior information and FSR simultaneously; iii) in-prior methods
    that extract prior information from the intermediate results or features at the
    middle stag, and iv) post-prior methods that extract prior information from FSR
    results. We illustrate the main frameworks of the four categories in Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1.5\. Discussion ‣ 4.1\. General FSR ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey"), outline the development of prior-guided FSR
    methods in Fig. [5](#S4.F5 "Figure 5 ‣ 4.2\. Prior-guided FSR ‣ 4\. FSR Methods
    ‣ Deep Learning-based Face Super-Resolution: A Survey") and compare them on several
    key features in Table [3](#S4.T3 "Table 3 ‣ 4.2.1\. Pre-prior Methods ‣ 4.2\.
    Prior-guided FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey").'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '先验引导的FSR方法是指提取面部先验信息并利用它来促进面部重建。考虑到先验信息提取和FSR的顺序，我们进一步将先验引导的FSR方法分为四类：i) 预先先验方法，即先提取先验信息再进行FSR；ii)
    并行先验方法，即同时提取先验信息和进行FSR；iii) 内部先验方法，即从中间结果或中间阶段的特征中提取先验信息；iv) 后期先验方法，即从FSR结果中提取先验信息。我们在图
    [4](#S4.F4 "Figure 4 ‣ 4.1.5\. Discussion ‣ 4.1\. General FSR ‣ 4\. FSR Methods
    ‣ Deep Learning-based Face Super-Resolution: A Survey") 中说明了这四类方法的主要框架，在图 [5](#S4.F5
    "Figure 5 ‣ 4.2\. Prior-guided FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face
    Super-Resolution: A Survey") 中概述了先验引导的FSR方法的发展，并在表 [3](#S4.T3 "Table 3 ‣ 4.2.1\.
    Pre-prior Methods ‣ 4.2\. Prior-guided FSR ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey") 中对它们的若干关键特征进行了比较。'
- en: 4.2.1\. Pre-prior Methods
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 预先先验方法
- en: 'These methods first extract face structure prior information and then feed
    the prior information to the beginning of FSR model. That is, they always extract
    prior information from LR face images by an extraction network which can be a
    pretrained network or a subnetwork associated with the FSR model, then take advantage
    of the prior information to facilitate FSR. To extract the accurate face structure
    prior, prior-based loss is always used in these methods to train their prior extraction
    network, which is defined as:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法首先提取面部结构的先验信息，然后将先验信息输入到FSR模型的开始阶段。也就是说，它们总是通过一个提取网络从低分辨率面部图像中提取先验信息，该提取网络可以是预训练网络或与FSR模型关联的子网络，然后利用先验信息来促进FSR。为了提取准确的面部结构先验，这些方法总是使用基于先验的损失来训练它们的先验提取网络，该损失定义为：
- en: '| (15) |  | $\mathcal{L}_{\text{Prior}}=\left\&#124;P_{\text{HR}}-P\right\&#124;_{F},$
    |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $\mathcal{L}_{\text{Prior}}=\left\|P_{\text{HR}}-P\right\|_{F},$
    |  |'
- en: where $P_{\text{HR}}$ is the ground truth prior, $P$ is extracted prior from
    the super-resolved face image, $F$ can be 1 or 2, and the prior can be heatmap,
    landmark and parsing maps in different methods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P_{\text{HR}}$ 是真实的先验，$P$ 是从超分辨面部图像中提取的先验，$F$ 可以是1或2，而先验可以是热图、地标和不同方法中的解析图。
- en: In the early years, both LCGE [[115](#bib.bib115)] and MNCEFH [[116](#bib.bib116)]
    extract landmarks from LR face images to crop the faces into different components,
    and then predict high-frequency details for different components. However, accurate
    landmarks are unavailable especially when LR face images are tiny (*i.e.*, 16$\times$16).
    Thus, researchers turn to facial parsing maps [[117](#bib.bib117), [118](#bib.bib118),
    [45](#bib.bib45), [119](#bib.bib119)]. PSFR-GAN [[117](#bib.bib117)], SeRNet [[118](#bib.bib118)]
    and CAGFace [[45](#bib.bib45)] all pretrain a face structure prior extraction
    network to extract facial parsing maps. Then all of them except SeRNet directly
    concatenate the prior and LR face images as the input of the super-resolution
    model while SeRNet designs its improved residual block (IRB) to fuse the prior
    and features from LR face images. In addition, PSFR-GAN designs a semantic aware
    style loss to calculate the gram matrix loss for each semantic region separately.
    Later on, super-resolution guided by 3D facial priors (FSRG3DFP) [[120](#bib.bib120)]
    estimates 3D priors instead of 2D priors to learn 3D facial details and capture
    facial component information by the spatial feature transform block (SFT).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，LCGE [[115](#bib.bib115)] 和 MNCEFH [[116](#bib.bib116)] 都从低分辨率人脸图像中提取地标，将人脸裁剪成不同的组件，然后为不同组件预测高频细节。然而，当低分辨率人脸图像特别小（*即*，16$\times$16）时，准确的地标不可用。因此，研究人员转向人脸解析图
    [[117](#bib.bib117), [118](#bib.bib118), [45](#bib.bib45), [119](#bib.bib119)]。PSFR-GAN
    [[117](#bib.bib117)]、SeRNet [[118](#bib.bib118)] 和 CAGFace [[45](#bib.bib45)]
    都预训练了一个面部结构先验提取网络来提取人脸解析图。然后，除了 SeRNet 之外，所有方法都直接将先验和低分辨率人脸图像拼接作为超分辨率模型的输入，而 SeRNet
    设计了改进的残差块（IRB）来融合先验和来自低分辨率人脸图像的特征。此外，PSFR-GAN 设计了一种语义感知风格损失来单独计算每个语义区域的 Gram 矩阵损失。后来，3D
    人脸先验指导的超分辨率（FSRG3DFP） [[120](#bib.bib120)] 估计 3D 先验而不是 2D 先验，以学习 3D 人脸细节，并通过空间特征变换块（SFT）捕获人脸组件信息。
- en: Table 3\. Comparison of prior-guided FSR methods. To be short, we use *Pre*,
    *Parallel*, *In*, and *Post* to denote different prior-guided methods.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3. 先验指导的 FSR 方法比较。为了简洁，我们使用 *Pre*、*Parallel*、*In* 和 *Post* 来表示不同的先验指导方法。
- en: '|  | Methods | Prior | Extraction | Fusion Strategies |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 先验 | 提取 | 融合策略 |'
- en: '|  |  |  |  |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |'
- en: '| Pre | LCGE [[115](#bib.bib115)] | Landmark | Pretrained | Crop |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 预 | LCGE [[115](#bib.bib115)] | 地标 | 预训练 | 裁剪 |'
- en: '|  | MNCEFH [[116](#bib.bib116)] | Landmark | Pretrained | Crop |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | MNCEFH [[116](#bib.bib116)] | 地标 | 预训练 | 裁剪 |'
- en: '|  | PSFR-GAN [[117](#bib.bib117)] | Parsing map | Pretrained | Concatenation
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | PSFR-GAN [[117](#bib.bib117)] | 解析图 | 预训练 | 拼接 |'
- en: '|  | CAGFace [[45](#bib.bib45)] | Parsing map | Pretrained | Concatenation
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | CAGFace [[45](#bib.bib45)] | 解析图 | 预训练 | 拼接 |'
- en: '|  | FSRG3DFP [[120](#bib.bib120)] | 3D prior | Joint | SFT |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | FSRG3DFP [[120](#bib.bib120)] | 3D 先验 | 联合 | SFT |'
- en: '|  | SeRNet [[118](#bib.bib118)] | Parsing map | Pretrained | IRB |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | SeRNet [[118](#bib.bib118)] | 解析图 | 预训练 | IRB |'
- en: '| Parallel | CBN [[121](#bib.bib121)] | Dense correspondence field | Joint
    | Concatenation |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | CBN [[121](#bib.bib121)] | 密集对应场 | 联合 | 拼接 |'
- en: '|  | KPEFH [[122](#bib.bib122)] | Parsing map | Joint | $\mathcal{L}_{\text{Parsing}}$
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | KPEFH [[122](#bib.bib122)] | 解析图 | 联合 | $\mathcal{L}_{\text{Parsing}}$
    |'
- en: '|  | JASRNet [[123](#bib.bib123)] | Heatmap | Joint | $\mathcal{L}_{\text{Heatmap}}$
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | JASRNet [[123](#bib.bib123)] | 热图 | 联合 | $\mathcal{L}_{\text{Heatmap}}$
    |'
- en: '|  | ATSENet [[124](#bib.bib124)] | Facial boundary heatmap | Joint | FFU |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | ATSENet [[124](#bib.bib124)] | 人脸边界热图 | 联合 | FFU |'
- en: '| In | FSRNet [[35](#bib.bib35)] | Landmark, parsing map, heatmap | Joint |
    Concatenation |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 在 | FSRNet [[35](#bib.bib35)] | 地标、解析图、热图 | 联合 | 拼接 |'
- en: '|  | FSRGFCH [[125](#bib.bib125)] | Heatmap | Joint | Concatenation |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | FSRGFCH [[125](#bib.bib125)] | 热图 | 联合 | 拼接 |'
- en: '|  | DIC [[36](#bib.bib36)] | Heatmap | Joint | $\mathcal{L}_{\text{Heatmap}}$,
    AFM |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | DIC [[36](#bib.bib36)] | 热图 | 联合 | $\mathcal{L}_{\text{Heatmap}}$，AFM
    |'
- en: '| Post | Super-FAN [[126](#bib.bib126)] | Heatmap | Joint | $\mathcal{L}_{\text{Heatmap}}$
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 后 | Super-FAN [[126](#bib.bib126)] | 热图 | 联合 | $\mathcal{L}_{\text{Heatmap}}$
    |'
- en: '|  | PFSRNet [[127](#bib.bib127)] | Heatmap | Pretrained | $\mathcal{L}_{\text{Heatmap}}$,
    $\mathcal{L}_{\text{Attention}}$ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | PFSRNet [[127](#bib.bib127)] | 热图 | 预训练 | $\mathcal{L}_{\text{Heatmap}}$，$\mathcal{L}_{\text{Attention}}$
    |'
- en: 4.2.2\. Parallel-prior Methods
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2. 并行先验方法
- en: 'The above methods ignore the correlation between face structure prior estimation
    and FSR task: face prior estimation benefits from the enhancement of FSR and vice
    versa. Thus, parallel-prior methods that perform prior estimation and super-resolution
    in parallel are proposed, including cascaded bi-network (CBN) [[121](#bib.bib121)],
    KPEFH [[122](#bib.bib122)], JASRNet [[123](#bib.bib123)], SAAN [[128](#bib.bib128)],
    HaPFSR [[129](#bib.bib129)], OBC-FSR [[130](#bib.bib130)] and ATSENet [[124](#bib.bib124)].
    They train the prior estimation and super-resolution networks jointly and require
    ground truth prior to calculate prior-based loss like Eq. ([15](#S4.E15 "In 4.2.1\.
    Pre-prior Methods ‣ 4.2\. Prior-guided FSR ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey")).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '以上方法忽略了面部结构先验估计与 FSR 任务之间的相关性：面部先验估计受益于 FSR 的增强，反之亦然。因此，提出了并行先验方法，这些方法在并行进行先验估计和超分辨率，包括级联双网络（CBN）[[121](#bib.bib121)]，KPEFH
    [[122](#bib.bib122)]，JASRNet [[123](#bib.bib123)]，SAAN [[128](#bib.bib128)]，HaPFSR
    [[129](#bib.bib129)]，OBC-FSR [[130](#bib.bib130)] 和 ATSENet [[124](#bib.bib124)]。它们联合训练先验估计和超分辨率网络，并且需要真实数据来计算基于先验的损失，如
    Eq. ([15](#S4.E15 "In 4.2.1\. Pre-prior Methods ‣ 4.2\. Prior-guided FSR ‣ 4\.
    FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey"))。'
- en: One of the most representative parallel-prior methods is JASRNet. Specifically,
    JASRNet utilizes a shared encoder to extract features for super-resolution and
    prior estimation simultaneously. Through this design, the shared encoder can extract
    the most expressive information for both tasks. In contrast to JASRNet, ATSENet
    not only extracts shared features for the two tasks, but also feeds features from
    the prior estimation branch into the feature fusion unit (FFU) in the super-resolution
    branch.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最具代表性的并行先验方法之一是 JASRNet。具体来说，JASRNet 使用一个共享编码器同时提取超分辨率和先验估计的特征。通过这种设计，共享编码器能够提取出两项任务所需的最具表现力的信息。与
    JASRNet 相比，ATSENet 不仅为这两项任务提取共享特征，还将来自先验估计分支的特征输入到超分辨率分支中的特征融合单元（FFU）。
- en: 4.2.3\. In-prior Methods
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 先验方法
- en: Pre- and parallel-prior methods directly extract structure prior information
    from LR face images. Due to the low-quality of LR face images, extracting accurate
    prior information is challenging. To reduce the difficulty and improve the accuracy
    of prior estimation, researchers first coarsely recover LR face images and then
    extract prior information from the enhanced results of LR face images, including
    FSRNet [[35](#bib.bib35)], FSR guided by facial component heatmaps (FSRGFCH) [[125](#bib.bib125)],
    HCFR [[131](#bib.bib131)], deep-iterative-collaboration (DIC) [[36](#bib.bib36)]
    and [[132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)]. Similar to parallel-prior methods, in-prior methods always
    jointly optimize the networks for two tasks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 预先和并行先验方法直接从 LR 面部图像中提取结构先验信息。由于 LR 面部图像的低质量，提取准确的先验信息具有挑战性。为了减少难度并提高先验估计的准确性，研究人员首先粗略恢复
    LR 面部图像，然后从增强后的 LR 面部图像结果中提取先验信息，包括 FSRNet [[35](#bib.bib35)]，由面部组件热图指导的 FSR (FSRGFCH)
    [[125](#bib.bib125)]，HCFR [[131](#bib.bib131)]，深度迭代协作 (DIC) [[36](#bib.bib36)]
    和 [[132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)]。与并行先验方法类似，先验方法总是联合优化两项任务的网络。
- en: Specifically, FSRNet [[35](#bib.bib35)], FSRGFCH [[125](#bib.bib125)] and HCFR [[131](#bib.bib131)]
    first upsample the LR face images to obtain intermediate results, then extract
    face structure prior from the intermediate results, and finally make use of the
    prior and intermediate results to recover the final results. FSRNet and FSRGFCH
    concatenate the intermediate results and the prior and feed the concatenated results
    into the following network to recover final SR results while HCFR utilizes the
    prior to segment the intermediate results and recovers final SR results by random
    forests. Considering that FSR and prior extraction should facilitate each other,
    DIC [[36](#bib.bib36)] proposes to iteratively perform super-resolution and prior
    extraction tasks. In the first iteration, DIC recovers a face $I_{\text{SR}^{\text{1}}}$
    with super-resolution model and extracts prior (heatmaps) $P_{\text{1}}$ from
    $I_{\text{SR}^{\text{1}}}$. In the $i$-th iteration, both the LR face image and
    $P_{i-1}$ are fed into the super-resolution model to obtain $I_{\text{SR}^{i}}$,
    and then $P_{i}$ can be extracted. In this way, the two tasks can promote each
    other. Moreover, DIC builds an attention fusion module (AFM) to fuse facial prior
    and the LR face image efficiently.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，FSRNet [[35](#bib.bib35)]、FSRGFCH [[125](#bib.bib125)] 和 HCFR [[131](#bib.bib131)]
    首先将低分辨率人脸图像上采样以获得中间结果，然后从中间结果中提取人脸结构先验，最后利用先验和中间结果来恢复最终结果。FSRNet 和 FSRGFCH 将中间结果和先验连接起来，并将连接后的结果输入到后续网络中以恢复最终的超分辨率结果，而
    HCFR 则利用先验对中间结果进行分割，并通过随机森林恢复最终的超分辨率结果。考虑到 FS 和先验提取应相互促进，DIC [[36](#bib.bib36)]
    提出迭代执行超分辨率和先验提取任务。在第一次迭代中，DIC 使用超分辨率模型恢复人脸 $I_{\text{SR}^{\text{1}}}$ 并从 $I_{\text{SR}^{\text{1}}}$
    中提取先验（热图） $P_{\text{1}}$。在第 $i$ 次迭代中，将低分辨率人脸图像和 $P_{i-1}$ 输入超分辨率模型以获得 $I_{\text{SR}^{i}}$，然后可以提取
    $P_{i}$。这样，两个任务可以相互促进。此外，DIC 构建了一个注意力融合模块（AFM），以高效地融合人脸先验和低分辨率人脸图像。
- en: 4.2.4\. Post-prior Methods
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 后处理先验方法
- en: In contrast to the above methods, post-prior methods extract the face structure
    prior from SR result rather than LR face image or intermediate result, and utilize
    the prior to design loss functions, including Super-FAN [[126](#bib.bib126)],
    progressive FSR network (PFSRNet) [[127](#bib.bib127)], and [[137](#bib.bib137)].
    Super-FAN [[126](#bib.bib126)] and PFSRNet [[127](#bib.bib127)] first super-resolve
    LR face images and obtain SR results, and then develops a prior estimation network
    to extract the heatmaps of SR face images and HR ones, and constrains the heatmaps
    of SR face images and HR ones to be close. PFSRNet further generates multi-scale
    super-resolved results and applies prior-based loss at every scale. In addition,
    PFSRNet utilizes heatmaps to generate a mask and calculates facial attention loss
    $\mathcal{L}_{\text{Attention}}$ based on the masked SR and HR face images. Compared
    with the above methods, post-prior methods do not require prior extraction during
    the inference.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述方法相比，后处理先验方法从超分辨率结果中提取人脸结构先验，而不是从低分辨率人脸图像或中间结果中提取，并利用先验设计损失函数，包括 Super-FAN
    [[126](#bib.bib126)]、渐进式 FSR 网络（PFSRNet） [[127](#bib.bib127)] 和 [[137](#bib.bib137)]。Super-FAN
    [[126](#bib.bib126)] 和 PFSRNet [[127](#bib.bib127)] 首先对低分辨率人脸图像进行超分辨率处理并获得超分辨率结果，然后开发先验估计网络以提取超分辨率人脸图像和高分辨率人脸图像的热图，并约束超分辨率人脸图像和高分辨率人脸图像的热图接近。PFSRNet
    进一步生成多尺度超分辨率结果，并在每个尺度上应用基于先验的损失。此外，PFSRNet 利用热图生成掩码，并基于掩码的超分辨率和高分辨率人脸图像计算人脸注意力损失
    $\mathcal{L}_{\text{Attention}}$。与上述方法相比，后处理先验方法在推理过程中不需要先验提取。
- en: 4.2.5\. Discussion
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. 讨论
- en: All prior-guided FSR methods need the ground truth of the face structure prior
    to calculate loss in the training phase. During the testing phase, all prior-guided
    FSR methods except post-prior methods need to estimate the prior. Due to the loss
    of information caused by image degradation, LR face images increase the difficulty
    and limit the accuracy of prior extraction in pre-prior methods, further limiting
    the super-resolution performance. Although parallel-prior methods can facilitate
    prior extraction and super-resolution simultaneously by sharing feature extraction,
    the improvement is still limited. In-prior methods extract prior from the intermediate
    result, which can improve the performance but increase the memory and computation
    cost caused by iterative super-resolution procedure especially in the iterative
    method (DIC [[36](#bib.bib36)]). In post-prior methods, the prior only plays the
    role of the supervisor during training, while not participating in inference,
    and they cannot make full use of the specific prior of the input LR face image.
    Thus, a method that can exploit the prior fully without increasing additional
    memory or computation cost is on demand.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 所有基于先验的FSR方法在训练阶段需要面部结构的真实数据来计算损失。在测试阶段，所有基于先验的FSR方法，除了后先验方法，都需要估计先验。由于图像降解造成的信息丢失，低分辨率面部图像增加了先验提取的难度，限制了预先先验方法的准确性，进一步限制了超分辨率性能。尽管并行先验方法通过共享特征提取可以同时促进先验提取和超分辨率，但改进仍然有限。先验方法从中间结果中提取先验，这可以提高性能，但增加了由于迭代超分辨率过程而导致的内存和计算成本，特别是在迭代方法中（DIC
    [[36](#bib.bib36)]）。在后先验方法中，先验仅在训练期间充当监督者，而不参与推断，并且无法充分利用输入低分辨率面部图像的特定先验。因此，需要一种能够充分利用先验而不增加额外内存或计算成本的方法。
- en: 4.3\. Attribute-constrained FSR
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 属性约束的FSR
- en: Facial attribute is also usually exploited in FSR, and they are called attribute-constrained
    FSR. As a kind of semantic information, facial attribute provides semantic knowledge,
    *e.g.*, whether people wear glasses, which is useful for FSR. In the following,
    we will introduce some attribute-constrained FSR methods.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 面部属性通常也被用于FSR，这些方法称为属性约束的FSR。作为一种语义信息，面部属性提供了语义知识，*例如*，人们是否佩戴眼镜，这对FSR有用。接下来，我们将介绍一些属性约束的FSR方法。
- en: '![Refer to caption](img/1592a5b13d72e72a043a673fb6fef8ee.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1592a5b13d72e72a043a673fb6fef8ee.png)'
- en: Figure 6\. Milestones of attribute-constrained FSR methods. Their names and
    venues are listed.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 属性约束的FSR方法的里程碑。它们的名称和场所列出。
- en: 'Different from face structure prior information of which acquisition relies
    on the image itself, attribute information can be available without LR face images,
    such as in criminal cases where attribute information may not be clear in LR face
    images but accurately known by witnesses. Thus, some researchers construct networks
    on the condition that attribute information is given, while others relax this
    by estimating attributes. According to this concept, attribute-constrained FSR
    methods can be divided into two frameworks: given attribute methods and estimated
    attribute methods. The overview is provided in Fig. [6](#S4.F6 "Figure 6 ‣ 4.3\.
    Attribute-constrained FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey") and Table [4](#S4.T4 "Table 4 ‣ 4.3.1\. Given Attribute Methods ‣ 4.3\.
    Attribute-constrained FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey").'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与面部结构先验信息的获取依赖于图像本身不同，属性信息可以在没有低分辨率面部图像的情况下获得，例如在刑事案件中，属性信息可能在低分辨率面部图像中不清晰，但由目击者准确知道。因此，一些研究者在属性信息给定的条件下构建网络，而其他人则通过估计属性来放宽这一条件。根据这一概念，属性约束的FSR方法可以分为两种框架：给定属性方法和估计属性方法。概述见图[6](#S4.F6
    "图6 ‣ 4.3\. 属性约束的FSR ‣ 4\. FSR方法 ‣ 基于深度学习的面部超分辨率：调查")和表[4](#S4.T4 "表4 ‣ 4.3.1\.
    给定属性方法 ‣ 4.3\. 属性约束的FSR ‣ 4\. FSR方法 ‣ 基于深度学习的面部超分辨率：调查")。
- en: 4.3.1\. Given Attribute Methods
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 给定属性方法
- en: 'Given the attribute information, how to integrate it into the super-resolution
    model is the key. For this problem, attribute-guided conditional CycleGAN (AGCycleGAN) [[138](#bib.bib138)],
    FSR with supplementary attributes (FSRSA) [[139](#bib.bib139)], expansive FSR
    with supplementary attributes (EFSRSA), attribute transfer network (ATNet) [[140](#bib.bib140)]
    and ATSENet [[124](#bib.bib124)] all directly concatenate attribute information
    and LR face image (or features extracted from LR face image). AGCycleGAN and FSRSA
    also feed the attribute into their discriminators to force the super-resolution
    model to notice the attribute information and develop attribute-based loss to
    achieve attribute matching, which is defined as:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 给定属性信息，如何将其整合到超分辨率模型中是关键。针对这一问题，属性引导条件CycleGAN (AGCycleGAN) [[138](#bib.bib138)]、具有补充属性的FSR
    (FSRSA) [[139](#bib.bib139)]、具有补充属性的扩展FSR (EFSRSA)、属性迁移网络 (ATNet) [[140](#bib.bib140)]
    和 ATSENet [[124](#bib.bib124)] 都直接将属性信息与LR人脸图像（或从LR人脸图像中提取的特征）级联。AGCycleGAN 和
    FSRSA 还将属性输入到其判别器中，以强制超分辨率模型注意到属性信息，并发展基于属性的损失以实现属性匹配，定义为：
- en: '| (16) |  | $\mathcal{L}_{\text{Attribute}_{D}}=-\log D(I_{\text{HR}},A)-\log\left(1-D(I_{\text{SR}},A)\right)-\log\left(1-D(I_{\text{HR}},\tilde{A})\right),$
    |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\mathcal{L}_{\text{Attribute}_{D}}=-\log D(I_{\text{HR}},A)-\log\left(1-D(I_{\text{SR}},A)\right)-\log\left(1-D(I_{\text{HR}},\tilde{A})\right),$
    |  |'
- en: where $A$ is attribute matched with $I_{\text{HR}}$ while $\tilde{A}$ is the
    mismatched one. ATSENet feeds the super-resolved result into an attribute analysis
    network to calculate attribute prediction loss,
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$A$是与$I_{\text{HR}}$匹配的属性，而$\tilde{A}$是不匹配的属性。ATSENet将超分辨结果输入到属性分析网络中以计算属性预测损失，
- en: '| (17) |  | $\mathcal{L}_{\text{Attribute}}=\left\&#124;A_{\text{P}}-A_{\text{HR}}\right\&#124;_{F},$
    |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $\mathcal{L}_{\text{Attribute}}=\left\|A_{\text{P}}-A_{\text{HR}}\right\|_{F},$
    |  |'
- en: where $A_{\text{P}}$ is the predicted attribute of the network and $A_{\text{HR}}$
    is the ground truth attribute. However, Lee *et al*. [[141](#bib.bib141)] hold
    that LR face image and attributes belong to different domains, and direct concatenation
    is unsuitable and may decrease the performance. With regard to this view, Lee
    *et al*. construct an attribute augmented convolutional neural network (AACNN) [[141](#bib.bib141)],
    which extracts features from the attribute to boost face super-resolution.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$A_{\text{P}}$是网络预测的属性，$A_{\text{HR}}$是真实的属性。然而，Lee *等* [[141](#bib.bib141)]认为LR人脸图像和属性属于不同领域，直接级联不适用，可能会降低性能。针对这一观点，Lee
    *等* 构建了一个属性增强卷积神经网络 (AACNN) [[141](#bib.bib141)]，该网络从属性中提取特征以提升人脸超分辨率。
- en: Table 4\. Comparison of attribute-constrained FSR methods. ”NG” denotes that
    the information is not given.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表4\. 属性约束的FSR方法比较。“NG”表示信息未提供。
- en: '|  | Methods | #Attribute | Attribute embedding methods |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | #属性 | 属性嵌入方法 |'
- en: '|  |  |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| Given | FSRSA [[139](#bib.bib139)] | 18 | Concatenation and $\mathcal{L}_{\text{Attribute}_{D}}$
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 给定 | FSRSA [[139](#bib.bib139)] | 18 | 级联和$\mathcal{L}_{\text{Attribute}_{D}}$
    |'
- en: '|  | EFSRSA [[142](#bib.bib142)] | 18 | Concatenation and $\mathcal{L}_{\text{Attribute}_{D}}$
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | EFSRSA [[142](#bib.bib142)] | 18 | 级联和$\mathcal{L}_{\text{Attribute}_{D}}$
    |'
- en: '|  | AGCycleGAN [[138](#bib.bib138)] | 18 | Concatenation and $\mathcal{L}_{\text{Attribute}_{D}}$
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | AGCycleGAN [[138](#bib.bib138)] | 18 | 级联和$\mathcal{L}_{\text{Attribute}_{D}}$
    |'
- en: '|  | AACNN [[141](#bib.bib141)] | 38 | Concatenation |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | AACNN [[141](#bib.bib141)] | 38 | 级联 |'
- en: '|  | ATNet [[140](#bib.bib140)] | NG | Concatenation and $\mathcal{L}_{\text{Attribute}}$
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | ATNet [[140](#bib.bib140)] | NG | 级联和$\mathcal{L}_{\text{Attribute}}$
    |'
- en: '|  | ATSENet [[124](#bib.bib124)] | NG | Concatenation and $\mathcal{L}_{\text{Attribute}}$
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | ATSENet [[124](#bib.bib124)] | NG | 级联和$\mathcal{L}_{\text{Attribute}}$
    |'
- en: '| Estimated | RAAN [[143](#bib.bib143)] | NG | Attribute channel attention
    and $\mathcal{L}_{\text{Attribute}}$ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 预计 | RAAN [[143](#bib.bib143)] | NG | 属性通道注意力和$\mathcal{L}_{\text{Attribute}}$
    |'
- en: '|  | FACN [[144](#bib.bib144)] | 18 | Attribute attention mask and $\mathcal{L}_{\text{Attribute}}$
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | FACN [[144](#bib.bib144)] | 18 | 属性注意力掩码和$\mathcal{L}_{\text{Attribute}}$
    |'
- en: 4.3.2\. Estimated Attribute Methods
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 预计属性方法
- en: 'The above-mentioned given attribute methods work on the condition that all
    attributes are given, making them limited in real-world scenes where some attributes
    are missing. Although the missed attributes can be set as unknown, such as 0 or
    random values, the performance may drop sharply. To this end, researchers build
    modules to estimate attribute information for FSR. In estimated attribute methods,
    attribute-based loss forces the network to predict attribute information correctly,
    which is similar to Eq. ([17](#S4.E17 "In 4.3.1\. Given Attribute Methods ‣ 4.3\.
    Attribute-constrained FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey")). Estimated attribute methods include residual attribute attention
    network (RAAN) [[143](#bib.bib143)] and facial attribute capsule network (FACN) [[144](#bib.bib144)].
    RAAN is based on cascaded residual attribute attention blocks (RAAB). RAAB builds
    three branches to generate shape, texture, and attribute information, respectively,
    and introduces two attribute channel attention applied to shape and texture information.
    In contrast, FACN [[144](#bib.bib144)] integrates attributes in capsules. Specifically,
    FACN encodes LR face image into encoded features, and the features are fed into
    a capsule generation block that produces semantic capsules, probabilistic capsules,
    and facial attributes. Then, the attribute is viewed as a kind of mask to refine
    other features by multiplication or summation. With the combination of three information
    as input, the decoder of FACN can well recover the final SR results.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '上述提到的给定属性方法在所有属性都已给出的条件下有效，因此在实际场景中某些属性缺失的情况下它们是有限的。虽然缺失的属性可以设置为未知，例如0或随机值，但性能可能会急剧下降。为此，研究人员建立了模块来估计FSR的属性信息。在估计属性方法中，基于属性的损失强制网络正确预测属性信息，这类似于等式
    ([17](#S4.E17 "In 4.3.1\. Given Attribute Methods ‣ 4.3\. Attribute-constrained
    FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey"))。估计属性方法包括残差属性注意网络
    (RAAN) [[143](#bib.bib143)] 和面部属性胶囊网络 (FACN) [[144](#bib.bib144)]。RAAN 基于级联残差属性注意块
    (RAAB)。RAAB 构建了三个分支分别生成形状、纹理和属性信息，并引入了两个应用于形状和纹理信息的属性通道注意力。相比之下，FACN [[144](#bib.bib144)]
    在胶囊中集成属性。具体而言，FACN 将LR人脸图像编码为编码特征，这些特征输入到一个胶囊生成块中，生成语义胶囊、概率胶囊和面部属性。然后，属性被视为一种掩码，通过乘法或加法来细化其他特征。通过将三种信息组合为输入，FACN
    的解码器可以很好地恢复最终的SR结果。'
- en: 4.3.3\. Discussion
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 讨论
- en: Given attribute methods require attribute information, making them only applicable
    in some restricted scenes. Although the attribute can be set as unknown in these
    methods, the performance may drop sharply. Towards the estimated attribute methods,
    they need to estimate the attribute and then utilize the attribute. Compared with
    given attribute methods, they have a wider range of applications but the accuracy
    of attribute estimation is difficult to guarantee in practice.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 给定属性方法需要属性信息，使得它们仅适用于一些受限的场景。虽然在这些方法中可以将属性设置为未知，但性能可能会急剧下降。相较于给定属性方法，估计属性方法需要估计属性然后利用属性。与给定属性方法相比，它们具有更广泛的应用范围，但在实际操作中属性估计的准确性难以保证。
- en: 4.4\. Identity-preserving FSR
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 身份保留 FSR
- en: 'Compared with face structure prior and attribute information, identity information
    containing identity-aware details is essential and identity-preserving FSR methods
    have received an increasing amount of attention in recent years. They aim to maintain
    the identity consistency between SR face image and LR one and improve the performance
    of down-stream face recognition. We show the overview and comparison of some representative
    methods in Fig. [7](#S4.F7 "Figure 7 ‣ 4.4\. Identity-preserving FSR ‣ 4\. FSR
    Methods ‣ Deep Learning-based Face Super-Resolution: A Survey") and Table [5](#S4.T5
    "Table 5 ‣ 4.4.1\. Face Recognition-based Methods: ‣ 4.4\. Identity-preserving
    FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey").'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '与面部结构先验和属性信息相比，包含身份感知细节的身份信息至关重要，身份保留的FSR方法近年来受到了越来越多的关注。它们旨在保持SR人脸图像与LR人脸图像之间的身份一致性，并提高下游人脸识别的性能。我们在图[7](#S4.F7
    "Figure 7 ‣ 4.4\. Identity-preserving FSR ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey")和表[5](#S4.T5 "Table 5 ‣ 4.4.1\. Face Recognition-based
    Methods: ‣ 4.4\. Identity-preserving FSR ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey")中展示了一些代表性方法的概述和比较。'
- en: '![Refer to caption](img/a8dfbc19ff6d6ac701be84cf8629f90c.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a8dfbc19ff6d6ac701be84cf8629f90c.png)'
- en: Figure 7\. Milestones of identity-preserving FSR methods. Their names and venues
    are listed.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7 身份保持 FSR 方法的里程碑。它们的名称和场所列出。
- en: '4.4.1\. Face Recognition-based Methods:'
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 面部识别基础的方法：
- en: 'To maintain identity consistency between $I_{\text{SR}}$ and $I_{\text{HR}}$,
    in the training phase, a commonly used design is utilizing face recognition network
    to define identity loss, *e.g.*, super-identity convolutional neural network (SICNN)
    [[145](#bib.bib145)], face hallucination generative adversarial network (FH-GAN) [[146](#bib.bib146)],
    WaSRGAN [[147](#bib.bib147)],  [[148](#bib.bib148)], identity preserving face
    hallucination (IPFH) [[149](#bib.bib149)], cascaded super-resolution and identity
    priors (C-SRIP) [[150](#bib.bib150)], [[151](#bib.bib151), [152](#bib.bib152),
    [153](#bib.bib153), [154](#bib.bib154)] and ATSENet [[124](#bib.bib124)]. The
    framework of these methods consists of two main components: a super-resolution
    model, and a pretrained face recognition network (FRN), probably an additional
    discriminator. The super-resolution model super-resolves the input LR face image,
    generating $I_{\text{SR}}$ which is fed into FRN to obtain its identity features.
    Simultaneously, $I_{\text{HR}}$ is also fed into FRN, obtaining its identity features.
    The identity loss is calculated by'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持 $I_{\text{SR}}$ 和 $I_{\text{HR}}$ 之间的身份一致性，在训练阶段，常用的设计是利用面部识别网络定义身份损失，例如，超身份卷积神经网络
    (SICNN) [[145](#bib.bib145)]，面部幻觉生成对抗网络 (FH-GAN) [[146](#bib.bib146)]，WaSRGAN
    [[147](#bib.bib147)]， [[148](#bib.bib148)]，身份保持面部幻觉 (IPFH) [[149](#bib.bib149)]，级联超分辨率和身份先验
    (C-SRIP) [[150](#bib.bib150)]，[[151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153),
    [154](#bib.bib154)] 和 ATSENet [[124](#bib.bib124)]。这些方法的框架包括两个主要组件：一个超分辨率模型和一个预训练的面部识别网络（FRN），可能还有一个额外的判别器。超分辨率模型对输入的低分辨率面部图像进行超分辨率处理，生成
    $I_{\text{SR}}$ 并将其输入到 FRN 中以获取其身份特征。同时，$I_{\text{HR}}$ 也被输入到 FRN 中，获得其身份特征。身份损失的计算方式为：
- en: '| (18) |  | $\mathcal{L}_{\text{Identity}}=\left\&#124;\text{FR}(I_{\text{HR}})-\text{FR}(I_{\text{SR}})\right\&#124;_{F},$
    |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $\mathcal{L}_{\text{Identity}}=\left\&#124;\text{FR}(I_{\text{HR}})-\text{FR}(I_{\text{SR}})\right\&#124;_{F},$
    |  |'
- en: where FR is the function of FRN. $F$ is 1 in WaSRGAN [[147](#bib.bib147)] and
    2 in FH-GAN [[146](#bib.bib146)] and [[151](#bib.bib151)]. Some methods calculate
    the loss on normalized features [[145](#bib.bib145), [155](#bib.bib155)], and
    some use A-softmax loss [[156](#bib.bib156), [149](#bib.bib149)]. Rather than
    directly extracting identity features from $I_{\text{SR}}$ and $I_{\text{HR}}$,
    C-SRIP [[150](#bib.bib150)] feeds residual maps between $I_{\text{HR}}$ (or $I_{\text{SR}}$)
    and $I_{\text{LR}}^{\uparrow_{s}}$ (upsampled by bicubic interpolation), respectively,
    into FRN, and applies cross-entropy loss on them. Moreover, C-SRIP generates multi-scale
    face images which are fed into different scale face recognition networks.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 FR 是 FRN 的函数。$F$ 在 WaSRGAN [[147](#bib.bib147)] 中为 1，在 FH-GAN [[146](#bib.bib146)]
    和 [[151](#bib.bib151)] 中为 2。一些方法在归一化特征上计算损失 [[145](#bib.bib145), [155](#bib.bib155)]，而一些方法使用
    A-softmax 损失 [[156](#bib.bib156), [149](#bib.bib149)]。C-SRIP [[150](#bib.bib150)]
    并不是直接从 $I_{\text{SR}}$ 和 $I_{\text{HR}}$ 中提取身份特征，而是将 $I_{\text{HR}}$（或 $I_{\text{SR}}$）与
    $I_{\text{LR}}^{\uparrow_{s}}$（通过双三次插值上采样）之间的残差图分别输入到 FRN 中，并在它们上应用交叉熵损失。此外，C-SRIP
    生成多尺度的面部图像，并将其输入到不同尺度的面部识别网络中。
- en: To fully explore the identity prior, SPGAN [[99](#bib.bib99)] feeds identity
    information extracted by the pretrained FRN to the discriminator at different
    scales, and designs attention-based identity loss. Firstly, SPGAN generates two
    attention maps $M_{\text{G}}$ and $M_{\text{D}}$,
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分探索身份先验，SPGAN [[99](#bib.bib99)] 将预训练的 FRN 提取的身份信息输入到不同尺度的判别器中，并设计基于注意力的身份损失。首先，SPGAN
    生成两个注意力图 $M_{\text{G}}$ 和 $M_{\text{D}}$，
- en: '| (19) |  | $E=\mathcal{D}(I_{\text{LR}},I_{\text{HR}})-\mathcal{D}(I_{\text{LR}},I_{\text{SR}}),$
    |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $E=\mathcal{D}(I_{\text{LR}},I_{\text{HR}})-\mathcal{D}(I_{\text{LR}},I_{\text{SR}}),$
    |  |'
- en: '| (20) |  | $M_{\text{D}}=-\min\left(0,E-\left\&#124;I_{\text{HR}}-I_{\text{SR}}\right\&#124;_{2}\right),$
    |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $M_{\text{D}}=-\min\left(0,E-\left\&#124;I_{\text{HR}}-I_{\text{SR}}\right\&#124;_{2}\right),$
    |  |'
- en: '| (21) |  | $M_{\text{G}}=\alpha*E+b,$ |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $M_{\text{G}}=\alpha*E+b,$ |  |'
- en: where $E$ denotes the difference, $*$ denotes the element-wise multiplication,
    $b$ is identity matrix, and $\alpha$ is a 0-1 matrix. At $i$-th row and $j$-th
    column, $\alpha_{i,j}$ is 0 when $E_{i,j}$ is negative, otherwise $\alpha_{i,j}$
    is 1\. Then two attention maps are applied to the identity loss $\mathcal{L}_{\text{Identity}}$,
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E$ 表示差异，$*$ 表示逐元素乘法，$b$ 是单位矩阵，$\alpha$ 是一个 0-1 矩阵。在第 $i$ 行和第 $j$ 列，$\alpha_{i,j}$
    为 0 当 $E_{i,j}$ 为负值时，否则 $\alpha_{i,j}$ 为 1。然后将两个注意力图应用于身份损失 $\mathcal{L}_{\text{Identity}}$。
- en: '| (22) |  | $\mathcal{L}_{\text{Identity}_{\text{SP}}}=\mathcal{L}_{\text{Identity}}*M_{\text{G}}+\mathcal{L}_{\text{Identity}}*M_{\text{D}},$
    |  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $\mathcal{L}_{\text{Identity}_{\text{SP}}}=\mathcal{L}_{\text{Identity}}*M_{\text{G}}+\mathcal{L}_{\text{Identity}}*M_{\text{D}},$
    |  |'
- en: where $\mathcal{L}_{\text{Identity}_{\text{SP}}}$ is the identity loss of SPGAN.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{\text{Identity}_{\text{SP}}}$ 是 SPGAN 的身份损失。
- en: Table 5\. Comparison of identity-preserving FSR methods. Notably, $I_{\text{RH}}$
    ($I_{\text{RS}}$) is the residual map between $I_{\text{HR}}$ ($I_{\text{SR}}$)
    and $I_{\text{LR}}^{\uparrow_{s}}$.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 表5\. 识别保留的 FSR 方法比较。特别地，$I_{\text{RH}}$ ($I_{\text{RS}}$) 是 $I_{\text{HR}}$
    ($I_{\text{SR}}$) 和 $I_{\text{LR}}^{\uparrow_{s}}$ 之间的残差图。
- en: '|  | Methods | Loss Functions |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 损失函数 |'
- en: '|  |  |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| Face Recognition-based | SICNN [[145](#bib.bib145)] | MSE loss on normalized
    $\text{FR}(I_{\text{SR}})$ and $\text{FR}(I_{\text{HR}})$ |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 基于面部识别的 | SICNN [[145](#bib.bib145)] | 在归一化的 $\text{FR}(I_{\text{SR}})$ 和
    $\text{FR}(I_{\text{HR}})$ 上的 MSE 损失 |'
- en: '|  | FH-GAN [[146](#bib.bib146)] | MSE loss on $\text{FR}(I_{\text{SR}})$ and
    $\text{FR}(I_{\text{HR}})$ |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | FH-GAN [[146](#bib.bib146)] | 在 $\text{FR}(I_{\text{SR}})$ 和 $\text{FR}(I_{\text{HR}})$
    上的 MSE 损失 |'
- en: '|  | WaSRGAN [[147](#bib.bib147)] | $\mathcal{L}_{1}$ loss on $\text{FR}(I_{\text{SR}})$
    and $\text{FR}(I_{\text{HR}})$ |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | WaSRGAN [[147](#bib.bib147)] | 在 $\text{FR}(I_{\text{SR}})$ 和 $\text{FR}(I_{\text{HR}})$
    上的 $\mathcal{L}_{1}$ 损失 |'
- en: '|  | C-SRIP [[150](#bib.bib150)] | Cross entropy loss on $\text{FR}(I_{\text{RS}})$
    and $\text{FR}(I_{\text{RH}})$ |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | C-SRIP [[150](#bib.bib150)] | 在 $\text{FR}(I_{\text{RS}})$ 和 $\text{FR}(I_{\text{RH}})$
    上的交叉熵损失 |'
- en: '|  | IPFH [[149](#bib.bib149)] | A-softmax loss on $\text{FR}(I_{\text{SR}})$
    and $\text{FR}(I_{\text{HR}})$ |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | IPFH [[149](#bib.bib149)] | 在 $\text{FR}(I_{\text{SR}})$ 和 $\text{FR}(I_{\text{HR}})$
    上的 A-softmax 损失 |'
- en: '|  | SPGAN [[99](#bib.bib99)] | Attention-based loss $\mathcal{L}_{\text{Identity}_{\text{SP}}}$
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | SPGAN [[99](#bib.bib99)] | 基于注意力的损失 $\mathcal{L}_{\text{Identity}_{\text{SP}}}$
    |'
- en: '| Pairwise Data-based | SiGAN [[157](#bib.bib157)] | Pair contrastive loss
    $\mathcal{L}_{\text{Contrastive}}$ |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 基于配对数据的 | SiGAN [[157](#bib.bib157)] | 配对对比损失 $\mathcal{L}_{\text{Contrastive}}$
    |'
- en: '|  | IADFH [[158](#bib.bib158)] | Adversarial face veri?cation loss $\mathcal{L}_{\text{AFVL}}$
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | IADFH [[158](#bib.bib158)] | 对抗面部验证损失 $\mathcal{L}_{\text{AFVL}}$ |'
- en: '4.4.2\. Pairwise Data-based Methods:'
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 基于配对数据的方法：
- en: The training of FRN needs well-labeled datasets. However, a large well-labeled
    dataset is very costly. One solution is based only on the weakly-labeled datasets.
    In consideration of this, siamese generative adversarial network (SiGAN) [[157](#bib.bib157)]
    takes advantage of the weak pairwise label (in which different LR face images
    correspond to different identities) to achieve identity preservation. Specifically,
    SiGAN has twin GANs ($G_{1}$ and $G_{2}$) that share the same architecture but
    super-resolve different LR face images ($I_{\text{LR}}^{1}$ and $I_{\text{LR}}^{2}$)
    at the same time. As the identities of different LR face images are different,
    the identities of SR results corresponding to LR face images are also varied.
    Based on this observation, SiGAN designs an identity-preserving contrastive loss
    that minimizes the difference between same-identity pairs and maximizes the difference
    between different-identity pairs,
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: FRN 的训练需要标注良好的数据集。然而，大型标注良好的数据集非常昂贵。一种解决方案是仅基于弱标注数据集。考虑到这一点，孪生生成对抗网络 (SiGAN) [[157](#bib.bib157)]
    利用弱配对标签（其中不同的 LR 面部图像对应不同的身份）来实现身份保留。具体而言，SiGAN 有两个共享相同架构的生成对抗网络 ($G_{1}$ 和 $G_{2}$)，但同时对不同的
    LR 面部图像 ($I_{\text{LR}}^{1}$ 和 $I_{\text{LR}}^{2}$) 进行超分辨率处理。由于不同的 LR 面部图像的身份不同，因此与
    LR 面部图像对应的 SR 结果的身份也会有所不同。基于这一观察，SiGAN 设计了一种身份保留对比损失，最小化同一身份配对之间的差异，并最大化不同身份配对之间的差异，
- en: '| (23) |  | $\mathcal{L}_{\text{Contrastive}}=(1-y)\frac{1}{2}\left[\text{max}(0,0.5-E_{\text{w}})\right]^{2}+y\frac{1}{2}(E_{\text{w}})^{2},$
    |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| (23) |  | $\mathcal{L}_{\text{Contrastive}}=(1-y)\frac{1}{2}\left[\text{max}(0,0.5-E_{\text{w}})\right]^{2}+y\frac{1}{2}(E_{\text{w}})^{2},$
    |  |'
- en: '| (24) |  | $E_{\text{w}}=\left\&#124;F_{\text{E}}(I_{\text{LR}}^{\text{1}}),F_{\text{E}}(I_{\text{LR}}^{\text{2}})\right\&#124;_{1},$
    |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $E_{\text{w}}=\left\&#124;F_{\text{E}}(I_{\text{LR}}^{\text{1}}),F_{\text{E}}(I_{\text{LR}}^{\text{2}})\right\&#124;_{1},$
    |  |'
- en: where $F_{\text{E}}$ is a function used to extract features from the intermediate
    layers of the generators, $E_{\text{w}}$ measures the distance between the features
    of $I_{\text{LR}}^{1}$ and $I_{\text{LR}}^{2}$, $y$ is 1 when two LR face images
    belong to the same identity, and $y$ is 0 when LR face images belong to different
    identities.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $F_{\text{E}}$ 是用于从生成器中间层提取特征的函数，$E_{\text{w}}$ 测量 $I_{\text{LR}}^{1}$ 和
    $I_{\text{LR}}^{2}$ 特征之间的距离，当两个 LR 面部图像属于同一身份时 $y$ 为 1，当 LR 面部图像属于不同身份时 $y$ 为
    0。
- en: 'Instead of feeding the pair data into twin generators, identity-aware deep
    face hallucination (IADFH) [[158](#bib.bib158)] feeds pair data into the discriminator.
    Its discriminator is a three-way classifier that generates fake, genuine and imposter:
    i) HR and SR face images with the same or different identities ($\text{pair}_{\text{1}}$
    or $\text{pair}_{\text{2}}$) correspond to the fake, which forces the discriminator
    to distinguish $I_{\text{HR}}$ and $I_{\text{SR}}$; ii) two different HR face
    images of the same identity ($\text{pair}_{\text{3}}$) correspond to the genuine;
    iii) two HR face images with different identities ($\text{pair}_{\text{4}}$) correspond
    to the imposter. The last two pairs force the discriminator to capture the identity
    feature. In this pattern, the generator can incorporate the identity information.
    The loss is called adversarial face veri?cation loss (AFVL),'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 与将配对数据输入双生成器不同，身份感知深度人脸幻影（IADFH）[[158](#bib.bib158)] 将配对数据输入判别器。其判别器是一个三分类器，生成假、真和冒名顶替者：i)
    相同或不同身份的高分辨率（HR）和超分辨率（SR）人脸图像（$\text{pair}_{\text{1}}$ 或 $\text{pair}_{\text{2}}$）对应假图像，迫使判别器区分
    $I_{\text{HR}}$ 和 $I_{\text{SR}}$；ii) 相同身份的两个不同HR人脸图像（$\text{pair}_{\text{3}}$）对应真图像；iii)
    不同身份的两个HR人脸图像（$\text{pair}_{\text{4}}$）对应冒名顶替者。最后两个配对强迫判别器捕捉身份特征。在这种模式下，生成器可以结合身份信息。损失函数称为对抗人脸验证损失（AFVL），
- en: '| (25) |  | $\mathcal{L}_{\text{AFVL(D)}}=\log d_{f}(\text{pair}_{1})+\log
    d_{f}(\text{pair}_{2})\\ +\log d_{\text{gen}}(\text{pair}_{3})+\log d_{\text{imp}}(\text{pair}_{4}),$
    |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $\mathcal{L}_{\text{AFVL(D)}}=\log d_{f}(\text{pair}_{1})+\log
    d_{f}(\text{pair}_{2})\\ +\log d_{\text{gen}}(\text{pair}_{3})+\log d_{\text{imp}}(\text{pair}_{4}),$
    |  |'
- en: '| (26) |  | $\mathcal{L}_{\text{AFVL(G)}}=\log d_{\text{gen}}(\text{pair}_{1})+\log
    d_{\text{imp}}(\text{pair}_{2}),$ |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $\mathcal{L}_{\text{AFVL(G)}}=\log d_{\text{gen}}(\text{pair}_{1})+\log
    d_{\text{imp}}(\text{pair}_{2}),$ |  |'
- en: where $\mathcal{L}_{\text{AFVL(D)}}$ ($\mathcal{L}_{\text{AFVL(G)}}$) is the
    loss function of the discriminator (generator), and $d_{\text{f}},d_{\text{gen}},d_{\text{imp}}$
    (can be -1, 1, 0) are the outputs of the discriminator for fake, genuine and imposter
    pairs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{\text{AFVL(D)}}$ ($\mathcal{L}_{\text{AFVL(G)}}$) 是判别器（生成器）的损失函数，而
    $d_{\text{f}},d_{\text{gen}},d_{\text{imp}}$（可以是 -1、1、0）是判别器对假图像、真图像和冒名顶替者配对的输出。
- en: 4.4.3\. Discussion
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 讨论
- en: Face recognition-based methods design identity loss based on face recognition
    network which is always pretrained. The training of a face recognition network
    requires well-labeled datasets which are costly. Instead, pairwise data-based
    methods take advantage of the contrast between different identities and the similarity
    between the same identity to maintain identity consistency without well-labeled
    datasets, which has a wider range of applications.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 基于人脸识别的方法设计身份损失，这些方法通常需要预训练的人脸识别网络。训练人脸识别网络需要高质量标注的数据集，这非常昂贵。而配对数据方法利用不同身份之间的对比以及相同身份之间的相似性，以在没有高质量标注数据集的情况下保持身份一致性，具有更广泛的应用范围。
- en: 4.5\. Reference FSR
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 参考 FSR
- en: 'The FSR networks discussed all exploit only the input LR face itself. In some
    conditions, we may obtain the high-quality face image of the same identity of
    the LR face image, for example, the person of the LR face image may have other
    high-quality face images. These high-quality face images can provide identity-aware
    face details for FSR. Thus, reference FSR methods utilize high-quality face image(s)
    as reference (R) to boost face restoration. Obviously, the reference face image
    can be only one image or multiple images. According to the number of R, a guided
    framework can be partitioned into single-face guided, multi-face guided, and dictionary-guided
    methods. An overview of reference FSR methods is shown in Fig. [8](#S4.F8 "Figure
    8 ‣ 4.5\. Reference FSR ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey") and the comparison of them is shown in Table [6](#S4.T6 "Table 6 ‣
    4.5.1\. Single-face Guided Methods ‣ 4.5\. Reference FSR ‣ 4\. FSR Methods ‣ Deep
    Learning-based Face Super-Resolution: A Survey").'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '所讨论的 FSR 网络都仅利用输入的低分辨率（LR）人脸图像。在某些情况下，我们可能会获得相同身份的高质量人脸图像，例如，LR人脸图像的人员可能有其他高质量的脸部图像。这些高质量的人脸图像可以为FSR提供身份感知的面部细节。因此，参考
    FSR 方法利用高质量人脸图像作为参考（R）来增强人脸恢复。显然，参考人脸图像可以是一个图像或多个图像。根据 R 的数量，指导框架可以分为单人脸指导、多人人脸指导和字典指导方法。参考
    FSR 方法的概述见图 [8](#S4.F8 "Figure 8 ‣ 4.5\. Reference FSR ‣ 4\. FSR Methods ‣ Deep
    Learning-based Face Super-Resolution: A Survey")，它们的比较见表 [6](#S4.T6 "Table 6 ‣
    4.5.1\. Single-face Guided Methods ‣ 4.5\. Reference FSR ‣ 4\. FSR Methods ‣ Deep
    Learning-based Face Super-Resolution: A Survey")。'
- en: '![Refer to caption](img/cb498b7d215a31300c2c21cec593b8fb.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cb498b7d215a31300c2c21cec593b8fb.png)'
- en: Figure 8\. Milestones of reference FSR methods. We simply list their names and
    venues.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 参考 FSR 方法的里程碑。我们简单列出了它们的名称和场所。
- en: 4.5.1\. Single-face Guided Methods
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1\. 单面引导方法
- en: 'At first, a high-quality face image which shares the same identity with the
    LR face image serves as R, such as guided face restoration network (GFRNet) [[39](#bib.bib39)],
    GWAInet [[159](#bib.bib159)]. Since the reference face image and LR face image
    may have different poses and expressions, which may hinder the recovery of face
    images, single-face guided methods tend to perform the alignment between the reference
    face image and the LR face image. After alignment, both the LR face image and
    aligned reference face image (we name it $I_{\text{w}}$) are fed into a reconstruction
    network to recover the SR result. The differences between GFRNet and GWAInet include
    two aspects: i) GFRNet employs landmarks while GWAInet employs flow field to carry
    out the alignment; ii) in the reconstruction network, GFRNet directly concatenates
    the LR face image and $I_{\text{w}}$ as the input. Nevertheless, GWAInet builds
    a GFENet to extract features from $I_{w}$ and transferring useful features of
    $I_{w}$ to the reconstruction network to recover SR results.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，一个高质量的面部图像，其身份与 LR 面部图像相同，作为 R，例如引导面部修复网络（GFRNet） [[39](#bib.bib39)]、GWAInet [[159](#bib.bib159)]。由于参考面部图像和
    LR 面部图像可能具有不同的姿势和表情，这可能会妨碍面部图像的恢复，单面引导方法倾向于对参考面部图像和 LR 面部图像进行对齐。在对齐之后，将 LR 面部图像和对齐后的参考面部图像（我们称之为
    $I_{\text{w}}$）输入重建网络以恢复 SR 结果。GFRNet 和 GWAInet 之间的差异包括两个方面：i) GFRNet 使用地标，而 GWAInet
    使用流场进行对齐；ii) 在重建网络中，GFRNet 直接将 LR 面部图像和 $I_{\text{w}}$ 拼接作为输入。然而，GWAInet 构建了一个
    GFENet 从 $I_{w}$ 中提取特征，并将 $I_{w}$ 的有用特征转移到重建网络中以恢复 SR 结果。
- en: Table 6\. Comparison of reference FSR methods. ”-” denotes that the method does
    not contain the procedure.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表6\. 参考 FSR 方法的比较。 “-” 表示该方法不包含该程序。
- en: '|  | Methods | Same identity | Alignment | Utilization of R |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 相同身份 | 对齐 | R 的利用 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  |  |  |  |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |'
- en: '| Single-face guided | GFRNet [[39](#bib.bib39)] | ✓ | Landmark | Concatenation
    |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 单面引导 | GFRNet [[39](#bib.bib39)] | ✓ | 地标 | 拼接 |'
- en: '|  | GWAInet [[159](#bib.bib159)] | ✓ | Flow field | GFENet |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | GWAInet [[159](#bib.bib159)] | ✓ | 流场 | GFENet |'
- en: '| Multi-face guided | ASFFNet [[37](#bib.bib37)] | ✓ | Moving least-square
    | AFFB |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 多面引导 | ASFFNet [[37](#bib.bib37)] | ✓ | 移动最小二乘 | AFFB |'
- en: '|  | MEFSR [[160](#bib.bib160)] | ✓ | - | PWAve |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | MEFSR [[160](#bib.bib160)] | ✓ | - | PWAve |'
- en: '| Dictionary-guided | JSRFC [[161](#bib.bib161)] | $\times$ | Landmark | Concatenation
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 字典引导 | JSRFC [[161](#bib.bib161)] | $\times$ | 地标 | 拼接 |'
- en: '|  | DFDNet [[38](#bib.bib38)] | $\times$ | - | DFT |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | DFDNet [[38](#bib.bib38)] | $\times$ | - | DFT |'
- en: 4.5.2\. Multi-face Guided Methods
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.2\. 多面引导方法
- en: Single-face guided methods set the problem as an LR face image only has one
    high-quality reference face image, but in some applications many high-quality
    face images are available, and they can further provide more complementary information
    for FSR. Adaptive spatial feature fusion network (ASFFNet) [[37](#bib.bib37)]
    is the first to explore multi-face guided FSR. Given multiple reference images,
    ASFFNet first selects the best reference image which should have the most similar
    pose and expression with LR face image by guidance selection module. However,
    misalignment and illumination differences still exist in the reference face image
    and the LR face image. Thus, ASFFNet applies weighted least-square alignment [[162](#bib.bib162)]
    and AdaIN [[163](#bib.bib163)] to cope with these two problems. Finally, they
    design an adaptive feature fusion block (AFFB) to generate an attention mask that
    is used to complement the information from LR face image and R. Multiple exemplar
    FSR (MEFSR) [[160](#bib.bib160)] directly feed all reference faces into weighted
    pixel average (PWAve) module to extract information for face restoration.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 单面引导方法将问题设置为LR面部图像仅有一个高质量的参考面部图像，但在某些应用中，有多个高质量的面部图像可用，它们可以进一步提供更多补充信息用于FSR。自适应空间特征融合网络（ASFFNet）[[37](#bib.bib37)]首次探索了多面引导FSR。给定多个参考图像，ASFFNet首先通过引导选择模块选择最合适的参考图像，该图像应与LR面部图像具有最相似的姿势和表情。然而，参考面部图像和LR面部图像之间仍存在错位和光照差异。因此，ASFFNet应用加权最小二乘对齐[[162](#bib.bib162)]和AdaIN[[163](#bib.bib163)]来应对这两个问题。最后，他们设计了一个自适应特征融合块（AFFB）来生成一个注意力掩码，用于补充LR面部图像和R的信息。多例FSR（MEFSR）[[160](#bib.bib160)]直接将所有参考面孔输入加权像素平均（PWAve）模块，以提取面部恢复所需的信息。
- en: 4.5.3\. Dictionary-guided Methods
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.3 字典引导方法
- en: It is observed that different people may have similar facial components. According
    to this observation, dictionary-guided methods are proposed, including joint super-resolution
    and face composite (JSRFC) [[161](#bib.bib161)] and deep face dictionary network
    (DFDNet) [[38](#bib.bib38)]. Dictionary-guided methods do not require the identity
    consistency between the reference face image and the LR face image, but build
    a component dictionary to boost face restoration. For example, JSRFC selects reference
    images which have similar components with the LR face image (every reference face
    image is labeled with a vector to indicate which components are similar.). Then,
    it aligns LR face image with the reference face image and extracts the corresponding
    components as a component dictionary. Finally, the dictionary components are used
    for the following face restoration. Different from JSRFC, Li *et al*. [[38](#bib.bib38)]
    build multi-scale component dictionaries based on features of the entire dataset.
    They use pretrained VGGFace [[67](#bib.bib67)] to extract features in different
    scales from high-quality faces, and then crop and resample four components with
    landmarks, and then cluster obtain K classes for every component by K-means. Given
    component dictionaries, they first select the most similar atoms for every component
    by the inner product, and then transfer the features from dictionary to the LR
    face image by dictionary feature transfer (DFT).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到不同的人可能具有相似的面部组件。根据这一观察，提出了字典引导方法，包括联合超分辨率和面部合成（JSRFC）[[161](#bib.bib161)]和深度面部字典网络（DFDNet）[[38](#bib.bib38)]。字典引导方法不要求参考面部图像和LR面部图像之间具有身份一致性，而是建立一个组件字典来促进面部恢复。例如，JSRFC选择与LR面部图像具有相似组件的参考图像（每个参考面部图像都标记有一个向量，以指示哪些组件是相似的）。然后，它对齐LR面部图像与参考面部图像，并提取相应的组件作为组件字典。最后，这些字典组件用于后续的面部恢复。不同于JSRFC，Li
    *et al*. [[38](#bib.bib38)]基于整个数据集的特征建立多尺度组件字典。他们使用预训练的VGGFace[[67](#bib.bib67)]从高质量面部中提取不同尺度的特征，然后裁剪和重采样四个组件，并通过K均值聚类获得每个组件的K类。给定组件字典后，他们首先通过内积选择每个组件的最相似原子，然后通过字典特征转移（DFT）将特征从字典转移到LR面部图像。
- en: 4.5.4\. Discussion
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.4 讨论
- en: Single-face and multi-face guided FSR methods require one or multiple additional
    high-quality face image(s) with the same identity as the LR face image, which
    facilitates face restoration but limits their application since the reference
    image may not exist. In addition, the alignment between low-quality LR face image
    and high-quality reference face image is also challenging in the reference FSR.
    Dictionary-guided methods break the restriction of the same identity, broadening
    the application but increasing the difficulty of face reconstruction.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 单面和多面引导的面部超级分辨率（FSR）方法需要与低分辨率（LR）面部图像具有相同身份的一张或多张高质量面部图像，这有助于面部修复，但限制了其应用，因为参考图像可能不存在。此外，低质量LR面部图像与高质量参考面部图像之间的对齐也是参考FSR中的一个挑战。字典引导的方法打破了相同身份的限制，拓宽了应用范围，但增加了面部重建的难度。
- en: 4.6\. Experiments and Analysis
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6\. 实验与分析
- en: To have a clear view of deep learning-based FSR methods, we compare the PSNR,
    SSIM and LPIPS performance of the state-of-the-art algorithms on commonly used
    benchmark datasets (including CelebA [[55](#bib.bib55)], VGGFace2 [[67](#bib.bib67)]
    and CASIA-WebFace [[69](#bib.bib69)]) with upscale $\times$4, $\times$8 and $\times$16\.
    Considering that the reference FSR methods are different from other FSR methods,
    we compare other FSR methods and reference FSR methods individually.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰了解基于深度学习的FSR方法，我们比较了在常用基准数据集（包括CelebA [[55](#bib.bib55)]、VGGFace2 [[67](#bib.bib67)]和CASIA-WebFace [[69](#bib.bib69)]）上，最先进算法的PSNR、SSIM和LPIPS性能，放大倍数为$\times$4、$\times$8和$\times$16。考虑到参考FSR方法与其他FSR方法的不同，我们分别比较了其他FSR方法和参考FSR方法。
- en: 4.6.1\. Comparison Results of FSR Methods
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.1\. FSR方法的比较结果
- en: We first introduce the experimental settings and analyze the results of FSR
    methods.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍实验设置，并分析FSR方法的结果。
- en: 'Experimental Setting: For CelebA [[55](#bib.bib55)] dataset, 168,854 images
    are used for training and 1,000 images for testing following DIC [[36](#bib.bib36)].
    All the images are cropped and resized into 128$\times$128 as $I_{\text{HR}}$.
    We apply the degradation model in Eq. ([4](#S2.E4 "In 2.1\. Problem Definition
    ‣ 2\. Background ‣ Deep Learning-based Face Super-Resolution: A Survey")) to generate
    $I_{\text{LR}}$. Facial landmarks are detected by [[164](#bib.bib164), [165](#bib.bib165),
    [166](#bib.bib166)] and heatmaps are generated according to the landmarks. For
    facial parsing map, we adopt pretrained BiSeNet [[167](#bib.bib167)] to extract
    the parsing map from $I_{\text{HR}}$. For quality evaluation, PSNR and SSIM are
    introduced and both of them are computed on the Y channel of YCbCr space, which
    also follows DIC [[36](#bib.bib36)]. In addition, we further introduce the LPIPS
    to evaluate the performance of all comparison approaches. For the optimizer and
    learning rate when retraining different methods, we follow the setting in their
    original papers.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '实验设置：对于CelebA [[55](#bib.bib55)]数据集，使用168,854张图像进行训练，1,000张图像进行测试，遵循DIC [[36](#bib.bib36)]。所有图像都裁剪并调整为128$\times$128作为$I_{\text{HR}}$。我们应用方程（[4](#S2.E4
    "In 2.1\. Problem Definition ‣ 2\. Background ‣ Deep Learning-based Face Super-Resolution:
    A Survey")）中的退化模型生成$I_{\text{LR}}$。面部特征点由[[164](#bib.bib164), [165](#bib.bib165),
    [166](#bib.bib166)]检测，并根据特征点生成热图。对于面部解析图，我们采用预训练的BiSeNet [[167](#bib.bib167)]从$I_{\text{HR}}$中提取解析图。质量评估方面，引入了PSNR和SSIM，并且它们都在YCbCr空间的Y通道上计算，这也遵循DIC [[36](#bib.bib36)]。此外，我们进一步引入LPIPS来评估所有对比方法的性能。对于重新训练不同方法时的优化器和学习率，我们遵循其原始论文中的设置。'
- en: 'Experimental Results: We list and compare the results of some representative
    FSR methods in Table [7](#S4.T7 "Table 7 ‣ 4.6.1\. Comparison Results of FSR Methods
    ‣ 4.6\. Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face
    Super-Resolution: A Survey"), including four general image super-resolution methods:
    super-resolution using deep convolutional networks (SRCNN) [[70](#bib.bib70)],
    VDSR [[85](#bib.bib85)], residual channel attention network (RCAN) [[168](#bib.bib168)],
    non-local sparse network (NLSN) [[169](#bib.bib169)], three general FSR methods:
    URDGN [[34](#bib.bib34)], WaSRNet [[82](#bib.bib82)], SPARNet [[78](#bib.bib78)],
    three prior-guided FSR methods: FSRNet [[35](#bib.bib35)], Super-FAN [[126](#bib.bib126)],
    DIC [[36](#bib.bib36)], two attribute-constrained FSR methods: FSRSA [[142](#bib.bib142)],
    AACNN [[141](#bib.bib141)], and three identity-preserving FSR methods: SICNN [[145](#bib.bib145)],
    SiGAN [[157](#bib.bib157)], and WaSRGAN [[147](#bib.bib147)]. Except that, we
    also report the parameters and FLOPs of these methods in the last two columns
    of Table [7](#S4.T7 "Table 7 ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\.
    Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey"). Note that the parameter and FLOPs are associated with the model with
    upscale $\times$8\. In addition, we also present the visual comparisons between
    a few state-of-the-art algorithms in Fig.[9](#S4.F9 "Figure 9 ‣ 4.6.1\. Comparison
    Results of FSR Methods ‣ 4.6\. Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep
    Learning-based Face Super-Resolution: A Survey"), Fig.[10](#S4.F10 "Figure 10
    ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\. Experiments and Analysis ‣
    4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey") and Fig.[11](#S4.F11
    "Figure 11 ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\. Experiments and
    Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey").'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果：我们在表[7](#S4.T7 "Table 7 ‣ 4.6.1\. Comparison Results of FSR Methods ‣
    4.6\. Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey")中列出了并比较了一些代表性的FSR方法，包括四种通用图像超分辨率方法：使用深度卷积网络的超分辨率（SRCNN）[[70](#bib.bib70)]、VDSR[[85](#bib.bib85)]、残差通道注意网络（RCAN）[[168](#bib.bib168)]、非局部稀疏网络（NLSN）[[169](#bib.bib169)]，三种通用FSR方法：URDGN[[34](#bib.bib34)]、WaSRNet[[82](#bib.bib82)]、SPARNet[[78](#bib.bib78)]，三种先验引导的FSR方法：FSRNet[[35](#bib.bib35)]、Super-FAN[[126](#bib.bib126)]、DIC[[36](#bib.bib36)]，两种属性约束的FSR方法：FSRSA[[142](#bib.bib142)]、AACNN[[141](#bib.bib141)]，以及三种身份保持的FSR方法：SICNN[[145](#bib.bib145)]、SiGAN[[157](#bib.bib157)]和WaSRGAN[[147](#bib.bib147)]。此外，我们还在表[7](#S4.T7
    "Table 7 ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\. Experiments and Analysis
    ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey")的最后两列中报告了这些方法的参数和FLOPs。请注意，参数和FLOPs与放大倍数$\times$8的模型相关。此外，我们还在图[9](#S4.F9
    "Figure 9 ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\. Experiments and
    Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey")、图[10](#S4.F10
    "Figure 10 ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\. Experiments and
    Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey")和图[11](#S4.F11
    "Figure 11 ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\. Experiments and
    Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A Survey")中展示了一些最新算法的视觉比较。'
- en: 'From these objective metrics and visual comparison results, we have the following
    observations:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些客观指标和视觉比较结果中，我们有以下观察：
- en: (i) The retrained state-of-the-art (SOTA) general image super-resolution methods,
    such as RCAN and NLSN, are very competitive and even outperform the best FSR methods
    in terms of PSNR and SSIM. Meanwhile, as a general FSR method, SPARNet obtains
    the best performance among all the FSR methods. RCAN, NLSN, and SPARNet all do
    not explicitly incorporate the prior knowledge of face image, but they have obtained
    outstanding results. It shows that the design and optimization of the network
    is very important, and a well-designed network will have stronger fitting capabilities
    (less reconstruction errors). This observation will enlighten us that when we
    are designing a FSR deep network, it should be based on a strong backbone network.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: （i）经过重新训练的最先进（SOTA）通用图像超分辨率方法，如RCAN和NLSN，在PSNR和SSIM方面非常有竞争力，甚至超过了最佳FSR方法。同时，作为一种通用FSR方法，SPARNet在所有FSR方法中表现最好。RCAN、NLSN和SPARNet都没有明确地融合人脸图像的先验知识，但它们取得了出色的结果。这表明网络的设计和优化非常重要，设计良好的网络将具有更强的拟合能力（重建误差更小）。这一观察将启发我们，在设计FSR深度网络时，应基于一个强大的骨干网络。
- en: '(ii) The terms of RCAN* and NLSN* in Table [7](#S4.T7 "Table 7 ‣ 4.6.1\. Comparison
    Results of FSR Methods ‣ 4.6\. Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep
    Learning-based Face Super-Resolution: A Survey") represent the pretrained models
    on general training images, and we directly download these models from the authors’
    pages. Note that the pretrained results under certain magnification factors are
    not given (indicated as ‘-’ in the table) because these methods are not trained
    under these magnification factors. RCAN and NLSN achieve better performance than
    RCAN* and NLSN*. This demonstrates that models trained by general images are not
    suitable for FSR but general image super-resolution methods trained by face images
    may perform well (sometimes even better than FSR methods on face images). Therefore,
    if we want to know and compare the performance of a newly proposed general image
    super-resolution on the task of FSR, we cannot directly use the pretrained model
    released by the authors, but should retrain the model on the face image dataset.
    It should be noted that the objective results of these GAN-based FSR methods (*e.g.*,
    URDGN, FSRSA, SiGAN and WaSRGAN) are worse than those of NLSN*. This is mainly
    because that they often cannot get a better MSE due to the introduction of adversarial
    losses, which tend to allow the models to obtain perceptually better SR results
    but large reconstruction errors.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '(ii) 表格 [7](#S4.T7 "Table 7 ‣ 4.6.1\. Comparison Results of FSR Methods ‣ 4.6\.
    Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey") 中的 RCAN* 和 NLSN* 代表在通用训练图像上预训练的模型，我们直接从作者的页面下载了这些模型。需要注意的是，在某些放大因子下的预训练结果未提供（在表中标记为‘-’），因为这些方法未在这些放大因子下进行训练。RCAN
    和 NLSN 的表现优于 RCAN* 和 NLSN*。这表明，在通用图像上训练的模型不适用于 FSR，但在面部图像上训练的通用图像超分辨率方法可能表现良好（有时甚至优于面部图像上的
    FSR 方法）。因此，如果我们想了解和比较新提出的通用图像超分辨率在 FSR 任务中的性能，我们不能直接使用作者发布的预训练模型，而应该在面部图像数据集上重新训练模型。需要注意的是，这些基于
    GAN 的 FSR 方法（*例如*，URDGN、FSRSA、SiGAN 和 WaSRGAN）的客观结果逊色于 NLSN*。这主要是因为引入对抗损失后，它们通常无法获得更好的
    MSE，对抗损失倾向于使模型获得感知上更好的 SR 结果，但重建误差较大。'
- en: (iii) Compared with general image super-resolution methods and general FSR methods,
    these methods that incorporate facial characteristics do not perform well in terms
    of PSNR and SSIM. Nevertheless, we cannot conclude that it is meaningless to develop
    FSR methods that use facial characteristics. This is mainly because PSNR and SSIM
    may be not good assessment metrics for the task of image super-resolution [[41](#bib.bib41)],
    let alone for the task of FSR, in which human perception will be more important.
    To further exploit the super-resolution reconstruction capacity, we also introduce
    another assessment metric, LPIPS, which is more in line with human judgement.
    From the LPIPS results, we learn that these methods with low PSNR and SSIM may
    produce very good performance in terms of LPIPS, please refer to Super-FAN and
    SiGAN. This indicates that these methods that introduce facial characteristics
    can well represent the face image and recover the face contours and discriminant
    details.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 与通用图像超分辨率方法和通用 FSR 方法相比，这些结合面部特征的方法在 PSNR 和 SSIM 指标上表现不佳。然而，我们不能得出开发使用面部特征的
    FSR 方法毫无意义的结论。这主要是因为 PSNR 和 SSIM 可能不是图像超分辨率任务的良好评估指标 [[41](#bib.bib41)]，更不用说 FSR
    任务，在其中人类感知将更加重要。为了进一步利用超分辨率重建能力，我们还引入了另一个评估指标 LPIPS，它更符合人类的判断。从 LPIPS 结果中，我们了解到这些
    PSNR 和 SSIM 较低的方法在 LPIPS 方面可能表现非常好，请参考 Super-FAN 和 SiGAN。这表明，这些引入面部特征的方法能够很好地表示面部图像，并恢复面部轮廓和判别细节。
- en: (iv) When we compare FSR methods that use different facial characteristics,
    such as face structure prior, attributes, and identity, it is difficult to say
    which type of characteristic is more effective for FSR. Because these methods
    often use different backbone networks, and it is difficult to determine whether
    their performance changes are caused by the difference in the backbone network
    itself or because of the introduction of different facial characteristics. In
    practice, we can first develop a strong backbone and then incorporate facial characteristics
    to boost FSR.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: (iv) 当我们比较使用不同面部特征的 FSR 方法时，例如面部结构先验、属性和身份，很难判断哪种特征对 FSR 更有效。因为这些方法通常使用不同的骨干网络，很难确定它们性能的变化是由于骨干网络本身的差异，还是因为引入了不同的面部特征。在实践中，我们可以首先开发一个强大的骨干网络，然后结合面部特征来提升
    FSR。
- en: Table 7\. Quantitative evaluation of various FSR methods on CelebA, in terms
    of PSNR, SSIM and LPIPS for $\times$4, $\times$8 and $\times$16\. The best, the
    second-best and the third-best results are emphasized with red, blue and underscore
    respectively. Note that Params and FLOPs are calculated for $\times$8 super-resolution
    model.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 各种 FSR 方法在 CelebA 上的定量评估，分别按 $\times$4、$\times$8 和 $\times$16 计算 PSNR、SSIM
    和 LPIPS。最佳、第二最佳和第三最佳结果分别用红色、蓝色和下划线突出显示。请注意，Params 和 FLOPs 是针对 $\times$8 超分辨率模型计算的。
- en: '| Methods | $\times$4 | $\times$8 | $\times$16 | Params | FLOPs |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $\times$4 | $\times$8 | $\times$16 | Params | FLOPs |'
- en: '|  | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ |
    SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$
    |  |  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ |
    SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$
    |  |  |'
- en: '| General Image Super-Resolution Methods |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 通用图像超分辨率方法 |'
- en: '| SRCNN [[70](#bib.bib70)] | 28.04 | 0.837 | 0.160 | 23.93 | 0.635 | 0.256
    | 20.54 | 0.467 | 0.291 | 0.01M | 0.3G |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| SRCNN [[70](#bib.bib70)] | 28.04 | 0.837 | 0.160 | 23.93 | 0.635 | 0.256
    | 20.54 | 0.467 | 0.291 | 0.01M | 0.3G |'
- en: '| VDSR [[85](#bib.bib85)] | 31.25 | 0.906 | 0.055 | 26.36 | 0.761 | 0.112 |
    22.42 | 0.594 | 0.186 | 0.6M | 11.0G |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| VDSR [[85](#bib.bib85)] | 31.25 | 0.906 | 0.055 | 26.36 | 0.761 | 0.112 |
    22.42 | 0.594 | 0.186 | 0.6M | 11.0G |'
- en: '| RCAN [[168](#bib.bib168)] | 31.69 | 0.913 | 0.051 | 27.30 | 0.799 | 0.100
    | 23.32 | 0.641 | 0.204 | 15.0M | 4.7G |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| RCAN [[168](#bib.bib168)] | 31.69 | 0.913 | 0.051 | 27.30 | 0.799 | 0.100
    | 23.32 | 0.641 | 0.204 | 15.0M | 4.7G |'
- en: '| RCAN* [[168](#bib.bib168)] | 26.30 | 0.769 | 0.177 | 22.17 | 0.521 | 0.265
    | - | - | - | 15.0M | 4.7G |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| RCAN* [[168](#bib.bib168)] | 26.30 | 0.769 | 0.177 | 22.17 | 0.521 | 0.265
    | - | - | - | 15.0M | 4.7G |'
- en: '| NLSN [[169](#bib.bib169)] | 32.08 | 0.919 | 0.044 | 27.45 | 0.804 | 0.091
    | 23.69 | 0.671 | 0.154 | 43.4M | 22.9G |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| NLSN [[169](#bib.bib169)] | 32.08 | 0.919 | 0.044 | 27.45 | 0.804 | 0.091
    | 23.69 | 0.671 | 0.154 | 43.4M | 22.9G |'
- en: '| NLSN* [[169](#bib.bib169)] | 30.82 | 0.899 | 0.065 | - | - | - | - | - |
    - | 43.4M | 22.9G |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| NLSN* [[169](#bib.bib169)] | 30.82 | 0.899 | 0.065 | - | - | - | - | - |
    - | 43.4M | 22.9G |'
- en: '| General FSR Methods |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 通用 FSR 方法 |'
- en: '| URDGN [[34](#bib.bib34)] | 30.11 | 0.884 | 0.075 | 25.62 | 0.726 | 0.148
    | 22.29 | 0.579 | 0.185 | 1.0M | 14.6G |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| URDGN [[34](#bib.bib34)] | 30.11 | 0.884 | 0.075 | 25.62 | 0.726 | 0.148
    | 22.29 | 0.579 | 0.185 | 1.0M | 14.6G |'
- en: '| WaSRNet [[82](#bib.bib82)] | 30.92 | 0.908 | 0.051 | 26.83 | 0.787 | 0.089
    | 23.13 | 0.634 | 0.160 | 71.5M | 19.2G |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| WaSRNet [[82](#bib.bib82)] | 30.92 | 0.908 | 0.051 | 26.83 | 0.787 | 0.089
    | 23.13 | 0.634 | 0.160 | 71.5M | 19.2G |'
- en: '| SPARNet [[78](#bib.bib78)] | 31.71 | 0.913 | 0.048 | 27.44 | 0.804 | 0.089
    | 23.68 | 0.674 | 0.139 | 10.0M | 7.2G |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| SPARNet [[78](#bib.bib78)] | 31.71 | 0.913 | 0.048 | 27.44 | 0.804 | 0.089
    | 23.68 | 0.674 | 0.139 | 10.0M | 7.2G |'
- en: '| Prior-guided FSR Methods |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 先验指导的 FSR 方法 |'
- en: '| FSRNet [[35](#bib.bib35)] | 31.46 | 0.908 | 0.052 | 26.66 | 0.771 | 0.110
    | 23.04 | 0.629 | 0.175 | 3.1M | 39.0G |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| FSRNet [[35](#bib.bib35)] | 31.46 | 0.908 | 0.052 | 26.66 | 0.771 | 0.110
    | 23.04 | 0.629 | 0.175 | 3.1M | 39.0G |'
- en: '| Super-FAN [[126](#bib.bib126)] | 31.17 | 0.905 | 0.040 | 27.08 | 0.788 |
    0.058 | 23.42 | 0.652 | 0.125 | 1.3M | 1.1G |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Super-FAN [[126](#bib.bib126)] | 31.17 | 0.905 | 0.040 | 27.08 | 0.788 |
    0.058 | 23.42 | 0.652 | 0.125 | 1.3M | 1.1G |'
- en: '| DIC [[36](#bib.bib36)] | 31.44 | 0.909 | 0.053 | 27.41 | 0.802 | 0.092 |
    23.47 | 0.657 | 0.160 | 20.8M | 14.8G |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| DIC [[36](#bib.bib36)] | 31.44 | 0.909 | 0.053 | 27.41 | 0.802 | 0.092 |
    23.47 | 0.657 | 0.160 | 20.8M | 14.8G |'
- en: '| Attribute-constrained FSR Methods |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 属性约束的 FSR 方法 |'
- en: '| FSRSA [[142](#bib.bib142)] | 30.80 | 0.898 | 0.058 | 26.19 | 0.757 | 0.111
    | 22.84 | 0.630 | 0.153 | 76.9M | 0.9G |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| FSRSA [[142](#bib.bib142)] | 30.80 | 0.898 | 0.058 | 26.19 | 0.757 | 0.111
    | 22.84 | 0.630 | 0.153 | 76.9M | 0.9G |'
- en: '| AACNN [[141](#bib.bib141)] | 31.30 | 0.907 | 0.052 | 26.68 | 0.773 | 0.100
    | 22.98 | 0.626 | 0.171 | 3.3M | 0.2G |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| AACNN [[141](#bib.bib141)] | 31.30 | 0.907 | 0.052 | 26.68 | 0.773 | 0.100
    | 22.98 | 0.626 | 0.171 | 3.3M | 0.2G |'
- en: '| Identity-preserving FSR Methods |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 保持身份的 FSR 方法 |'
- en: '| SICNN [[145](#bib.bib145)] | 31.59 | 0.911 | 0.050 | 27.18 | 0.793 | 0.095
    | 23.50 | 0.662 | 0.152 | 4.9M | 5.4G |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| SICNN [[145](#bib.bib145)] | 31.59 | 0.911 | 0.050 | 27.18 | 0.793 | 0.095
    | 23.50 | 0.662 | 0.152 | 4.9M | 5.4G |'
- en: '| SiGAN [[157](#bib.bib157)] | 30.68 | 0.892 | 0.034 | 25.63 | 0.740 | 0.062
    | 22.18 | 0.596 | 0.099 | 19.5M | 5.7G |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| SiGAN [[157](#bib.bib157)] | 30.68 | 0.892 | 0.034 | 25.63 | 0.740 | 0.062
    | 22.18 | 0.596 | 0.099 | 19.5M | 5.7G |'
- en: '| WaSRGAN [[147](#bib.bib147)] | 30.72 | 0.907 | 0.045 | 25.55 | 0.765 | 0.092
    | 22.78 | 0.625 | 0.148 | 71.5M | 19.2G |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| WaSRGAN [[147](#bib.bib147)] | 30.72 | 0.907 | 0.045 | 25.55 | 0.765 | 0.092
    | 22.78 | 0.625 | 0.148 | 71.5M | 19.2G |'
- en: '![Refer to caption](img/d715c86058c0d434d22517981bc3234c.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d715c86058c0d434d22517981bc3234c.png)'
- en: Figure 9\. Qualitative comparison of different FSR approaches for $\times$4
    super-resolution reconstruction.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 不同 FSR 方法在 $\times$4 超分辨率重建中的定性比较。
- en: '![Refer to caption](img/62334897cdd32103280f99262d900873.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/62334897cdd32103280f99262d900873.png)'
- en: Figure 10\. Qualitative comparison of different FSR approaches for $\times$8
    super-resolution reconstruction.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 不同 FSR 方法在 $\times$8 超分辨率重建上的定性比较。
- en: '![Refer to caption](img/2fc65fa725a110f0eccb37f3d1dbe89e.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2fc65fa725a110f0eccb37f3d1dbe89e.png)'
- en: Figure 11\. Qualitative comparison of different FSR approaches for $\times$16
    super-resolution reconstruction.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 不同 FSR 方法在 $\times$16 超分辨率重建上的定性比较。
- en: 4.6.2\. Comparison Results of Reference FSR Methods
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.2\. 参考 FSR 方法的比较结果
- en: The above FSR methods only require LR face images as input, while the reference
    FSR methods require LR face images and reference images. It is unfair to directly
    compare with these methods that do not use auxiliary high-resolution face images.
    Therefore, we compare the performance of the reference FSR methods individually.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 FSR 方法只需输入 LR 面部图像，而参考 FSR 方法则需要 LR 面部图像和参考图像。直接与这些不使用辅助高分辨率面部图像的方法进行比较是不公平的。因此，我们对参考
    FSR 方法的性能进行单独比较。
- en: 'Experimental Setting: Following ASFFNet [[37](#bib.bib37)], VGGFace2 [[67](#bib.bib67)]
    is reorganized into 106,000 groups and every group has 3-10 high-quality face
    images of the same identity, in which 10,000 groups are used for training set,
    4,000 groups are for validation set and the remaining are testing set. In addition,
    two testing sets based on CelebA [[55](#bib.bib55)] and CASIA-WebFace [[69](#bib.bib69)]
    are also used, and each set contains 2,000 groups with 3-10 high-quality face
    images. We utilize facial landmarks to crop and resize all images into 256$\times$256
    as high-quality face images. To generate $I_{\text{LR}}$, the degradation model
    Eq. ([5](#S2.E5 "In 2.1\. Problem Definition ‣ 2\. Background ‣ Deep Learning-based
    Face Super-Resolution: A Survey")), where $J$ and $\downarrow$ are embodied as
    JPEG compression with quality $q$ and bicubic interpolation respectively, is applied
    to the high-quality images. We consider two types of blur kernels, *i.e.*, Gaussian
    blur and motion blur kernels, and randomly sample the scale $s$ from {1:0.1:8},
    the noise level from {0:1:15}, and the compression quality factor $q$ from {10
    : 1 : 60} [[37](#bib.bib37)]. PSNR, SSIM and LPIPS [[41](#bib.bib41)] are used
    as metrics.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '实验设置：按照 ASFFNet [[37](#bib.bib37)]，VGGFace2 [[67](#bib.bib67)] 被重新组织成 106,000
    个组，每个组包含 3-10 张相同身份的高质量面部图像，其中 10,000 个组用于训练集，4,000 个组用于验证集，其余的用于测试集。此外，还使用了基于
    CelebA [[55](#bib.bib55)] 和 CASIA-WebFace [[69](#bib.bib69)] 的两个测试集，每个集包含 2,000
    个组，每组有 3-10 张高质量面部图像。我们利用面部标志点裁剪并调整所有图像为 256$\times$256 的高质量面部图像。为了生成 $I_{\text{LR}}$，应用退化模型
    Eq. ([5](#S2.E5 "在 2.1\. 问题定义 ‣ 2\. 背景 ‣ 基于深度学习的面部超分辨率：综述"))，其中 $J$ 和 $\downarrow$
    分别表现为 JPEG 压缩质量 $q$ 和双三次插值，对高质量图像进行处理。我们考虑两种模糊核，*即* 高斯模糊和运动模糊核，并从 {1:0.1:8} 中随机抽样缩放
    $s$，从 {0:1:15} 中随机抽样噪声水平，并从 {10 : 1 : 60} 中随机抽样压缩质量因子 $q$ [[37](#bib.bib37)]。使用
    PSNR、SSIM 和 LPIPS [[41](#bib.bib41)] 作为评估指标。'
- en: 'Experimental Results: The experimental results are shown in Table [8](#S4.T8
    "Table 8 ‣ 4.6.2\. Comparison Results of Reference FSR Methods ‣ 4.6\. Experiments
    and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution: A
    Survey"). To be specific, we list the results of GFRNet [[39](#bib.bib39)], GWAInet [[159](#bib.bib159)]
    and the latest proposed ASFFNet [[37](#bib.bib37)] on CelebA [[55](#bib.bib55)],
    VGGFace2 [[67](#bib.bib67)] and CASIA-WebFace [[69](#bib.bib69)] with upscale
    $\times$8\. Note that all the results are copied from the paper [[37](#bib.bib37)]
    since we have difficulty in reproducing these methods. Note that GFRNet and GWAInet
    are single-face guided methods while ASFFNet is multi-face guided method. To be
    fair, the reference image of GFRNet and GWAInet is the same as the selected image
    in ASFFNet. From Table [8](#S4.T8 "Table 8 ‣ 4.6.2\. Comparison Results of Reference
    FSR Methods ‣ 4.6\. Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey"), it is obvious that multi-face guided method
    ASFFNet performs better than single-face guided methods (GWAInet and GFRNet).
    ASFFNet considers the illumination difference between the reference face image
    and the LR face image, which is ignored by GFRNet and GWAInet, and builds a well-designed
    AFFB instead of simple concatenation to adaptively the features of the reference
    face image and the LR face image. These two points contribute to the excellent
    performance of ASFFNet. Thus, difference (*i.e.*, misalignment, illumination difference,
    *etc*.) elimination and effective information fusion of the reference face image
    and the LR face image are both important in reference FSR methods.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果：实验结果见表 [8](#S4.T8 "Table 8 ‣ 4.6.2\. Comparison Results of Reference FSR
    Methods ‣ 4.6\. Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based
    Face Super-Resolution: A Survey")。具体来说，我们列出了 GFRNet [[39](#bib.bib39)]、GWAInet [[159](#bib.bib159)]
    和最新提出的 ASFFNet [[37](#bib.bib37)] 在 CelebA [[55](#bib.bib55)]、VGGFace2 [[67](#bib.bib67)]
    和 CASIA-WebFace [[69](#bib.bib69)] 上的结果，放大倍数为 $\times$8\. 注意，所有结果均来自论文 [[37](#bib.bib37)]，因为我们在重现这些方法时遇到了困难。请注意，GFRNet
    和 GWAInet 是单脸引导方法，而 ASFFNet 是多脸引导方法。公平起见，GFRNet 和 GWAInet 的参考图像与 ASFFNet 选择的图像相同。从表
    [8](#S4.T8 "Table 8 ‣ 4.6.2\. Comparison Results of Reference FSR Methods ‣ 4.6\.
    Experiments and Analysis ‣ 4\. FSR Methods ‣ Deep Learning-based Face Super-Resolution:
    A Survey") 可以明显看出，多脸引导方法 ASFFNet 的表现优于单脸引导方法（GWAInet 和 GFRNet）。ASFFNet 考虑了参考面部图像和低分辨率面部图像之间的光照差异，而
    GFRNet 和 GWAInet 忽略了这一点，并且构建了一个精心设计的 AFFB，而不是简单的拼接，以自适应地处理参考面部图像和低分辨率面部图像的特征。这两点都促成了
    ASFFNet 的出色表现。因此，参考 FSR 方法中参考面部图像和低分辨率面部图像的差异（*例如*，错位、光照差异、*等*）消除和有效信息融合都很重要。'
- en: Table 8\. Quantitative evaluation of various reference FSR methods on CelebA [[55](#bib.bib55)],
    VGGFace2 [[67](#bib.bib67)] and CASIA-WebFace [[69](#bib.bib69)], in terms of
    PSNR, SSIM and LPIPS for $\times$8\. The best, the second-best and the third-best
    results are emphasized with red, blue and underscore respectively.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8\. 各种参考 FSR 方法在 CelebA [[55](#bib.bib55)]、VGGFace2 [[67](#bib.bib67)] 和 CASIA-WebFace [[69](#bib.bib69)]
    上的定量评估，基于 PSNR、SSIM 和 LPIPS 的 $\times$8\. 最佳、第二最佳和第三最佳结果分别用红色、蓝色和下划线突出显示。
- en: '| Methods | CelebA [[55](#bib.bib55)] | VGGFace2 [[67](#bib.bib67)] | CASIA-WebFace [[69](#bib.bib69)]
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | CelebA [[55](#bib.bib55)] | VGGFace2 [[67](#bib.bib67)] | CASIA-WebFace [[69](#bib.bib69)]
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ |
    SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$
    |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ |
    SSIM$\uparrow$ | LPIPS$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GFRNet [[39](#bib.bib39)] | 25.93 | 0.901 | 0.227 | 23.85 | 0.879 | 0.263
    | 27.19 | 0.912 | 0.307 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| GFRNet [[39](#bib.bib39)] | 25.93 | 0.901 | 0.227 | 23.85 | 0.879 | 0.263
    | 27.19 | 0.912 | 0.307 |'
- en: '| GWAInet [[159](#bib.bib159)] | 25.77 | 0.901 | 0.210 | 23.87 | 0.879 | 0.261
    | 27.18 | 0.910 | 0.250 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| GWAInet [[159](#bib.bib159)] | 25.77 | 0.901 | 0.210 | 23.87 | 0.879 | 0.261
    | 27.18 | 0.910 | 0.250 |'
- en: '| ASFFNet [[37](#bib.bib37)] | 26.39 | 0.905 | 0.185 | 24.34 | 0.881 | 0.238
    | 27.69 | 0.921 | 0.219 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| ASFFNet [[37](#bib.bib37)] | 26.39 | 0.905 | 0.185 | 24.34 | 0.881 | 0.238
    | 27.69 | 0.921 | 0.219 |'
- en: 4.7\. Joint FSR and Other Tasks
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7\. 联合 FSR 和其他任务
- en: Although the above FSR methods have achieved a breakthrough, FSR is still challenging
    and complex since the input face images are often affected by many factors, including
    shadow, occlusion, blur, abnormal illumination, *etc*. To recover these face images
    efficiently, some work is proposed to consider degradation caused by low-quality
    and other factors together. Moreover, researchers also jointly perform FSR and
    other tasks. In the following, we will review these joint FSR and other tasks
    methods.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述 FSR 方法已取得突破，但由于输入面部图像通常受到许多因素的影响，包括阴影、遮挡、模糊、异常光照，*等*，FSR 仍然具有挑战性和复杂性。为了有效恢复这些面部图像，提出了一些工作考虑低质量和其他因素造成的退化。此外，研究人员还将
    FSR 与其他任务联合进行。以下，我们将回顾这些联合 FSR 和其他任务的方法。
- en: 4.7.1\. Joint Face Completion and Super-Resolution
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.1\. 联合面部完成与超分辨率
- en: Both low-resolution and occlusion or shadowing always coexist in the real-world
    face images. Thus, the restoration of faces degraded by these two factors is important.
    The simplest way is to first complete the occluded part and then super-resolve
    the completed LR face images [[170](#bib.bib170)]. However, the results always
    contain large artifacts due to the accumulation of errors. Cai *et al*. [[171](#bib.bib171)]
    propose the FCSR-GAN method which pretrains a face completion model (FCM), and
    combines FCM with super-resolution model (SRM), then trains SRM with the fixed
    FCM, and finally finetunes the whole network. Then, Liu *et al*. [[172](#bib.bib172)]
    propose a graph convolution pyramid blocks, which only needs one step to be trained
    rather than multiple steps of FCSR-GAN. In contrast, Pro-UIGAN [[173](#bib.bib173)]
    utilizes facial landmark to capture facial geometric prior and recovers occluded
    LR face images progressively.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的面部图像中，低分辨率和遮挡或阴影总是共存的。因此，恢复这两种因素导致的面部退化非常重要。最简单的方法是首先完成遮挡部分，然后对完成的低分辨率面部图像进行超分辨率
    [[170](#bib.bib170)]。然而，由于误差的积累，结果总是包含较大的伪影。Cai *et al*. [[171](#bib.bib171)]
    提出了 FCSR-GAN 方法，该方法首先预训练一个面部完成模型（FCM），然后将 FCM 与超分辨率模型（SRM）结合，再用固定的 FCM 训练 SRM，最后微调整个网络。然后，Liu
    *et al*. [[172](#bib.bib172)] 提出了图卷积金字塔块，该方法只需要一步训练，而不是像 FCSR-GAN 那样的多步骤训练。相比之下，Pro-UIGAN
    [[173](#bib.bib173)] 利用面部标志捕捉面部几何先验，并逐步恢复遮挡的低分辨率面部图像。
- en: 4.7.2\. Joint Face Deblurring and Super-Resolution
  id: totrans-335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.2\. 联合面部去模糊与超分辨率
- en: Blurry LR face images always arise in real surveillance and sports videos, which
    cannot be recovered effectively by a single task model, *e.g.*, super-resolution
    or deblurring model. In the literature, Yu *et al*. [[174](#bib.bib174)] develop
    SCGAN to deblur and super-resolve the input jointly. Then, Song *et al*. [[175](#bib.bib175)]
    find that the previous methods ignore the utilization of facial prior information
    and the recovered face image are lack of high-frequency details. Thus, they first
    utilize a parsing map and LR face image to recover a basic result, and then feed
    the basic result into detail enhancement module to compensate high-frequency details
    from the high-quality exemplar. Later on, DGFAN [[176](#bib.bib176)] develops
    two feature extraction modules for different tasks to extract features, and imports
    them into well-designed gated fusion modules to generate deblurred high-quality
    results. Xu *et al*. [[177](#bib.bib177)] incorporate face recognition network
    with face restoration to improve the identi?ability of the recovered face images.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊的低分辨率面部图像经常出现在实际监控和体育视频中，单任务模型，如超分辨率或去模糊模型，无法有效恢复这些图像。在文献中，Yu *et al*. [[174](#bib.bib174)]
    开发了 SCGAN 来联合去模糊和超分辨率输入图像。随后，Song *et al*. [[175](#bib.bib175)] 发现之前的方法忽略了面部先验信息的利用，恢复的面部图像缺乏高频细节。因此，他们首先利用解析图和低分辨率面部图像恢复基本结果，然后将基本结果输入到细节增强模块中，从高质量示例中补偿高频细节。后来，DGFAN
    [[176](#bib.bib176)] 开发了两个特征提取模块用于不同任务，提取特征，并将其输入到精心设计的门控融合模块中，以生成去模糊的高质量结果。Xu
    *et al*. [[177](#bib.bib177)] 将面部识别网络与面部恢复结合，以提高恢复面部图像的可识别性。
- en: 4.7.3\. Joint Illumination Compensation and FSR
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.3\. 联合光照补偿与 FSR
- en: Abnormal illumination FSR has also attracted the attention of many scholars.
    SeLENet [[178](#bib.bib178)] decomposes a face image into a normal face, an albedo
    map and a lighting coef?cient, then replaces the lighting coef?cient with the
    standard ambient white light coef?cient, and then reconstructs the corresponding
    neutral light face image. Ding *et al*. [[179](#bib.bib179)] build a pipeline
    of face detection, and then recover the detected faces with landmarks. Zhang *et
    al*. [[180](#bib.bib180)] utilize a normal illumination external HR guidance to
    guide abnormal illumination LR face images for illumination compensation. They
    develop a copy-and-paste GAN (CPGAN), including an internal copy-and-paste network
    to utilize face intern information for reconstruction, and an external copy-and-paste
    network is applied to compensate illumination. Based on CPGAN, they further improve
    the external copy-and-paste network by introducing recursive learning and incorporating
    landmark estimation and develop the recursive CPGAN [[181](#bib.bib181)]. In contrast,
    Yasarla *et al*. [[182](#bib.bib182)] introduce network architecture search into
    face enhancement to design efficient network and extract identity information
    from HR guidance to restore face images.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 异常照明 FSR 也引起了许多学者的关注。SeLENet [[178](#bib.bib178)] 将面部图像分解为正常面部、反射率图和光照系数，然后用标准环境白光系数替换光照系数，再重建相应的中性光照面部图像。Ding
    *等人* [[179](#bib.bib179)] 构建了一个面部检测管道，然后用地标恢复检测到的面部。Zhang *等人* [[180](#bib.bib180)]
    利用正常光照外部 HR 指导来指导异常光照的 LR 面部图像进行光照补偿。他们开发了一个复制粘贴 GAN（CPGAN），包括一个内部复制粘贴网络，用于利用面部内部信息进行重建，以及一个外部复制粘贴网络，用于补偿光照。基于
    CPGAN，他们通过引入递归学习和结合地标估计进一步改进了外部复制粘贴网络，并开发了递归 CPGAN [[181](#bib.bib181)]。相比之下，Yasarla
    *等人* [[182](#bib.bib182)] 将网络架构搜索引入面部增强中，以设计高效的网络并从 HR 指导中提取身份信息以恢复面部图像。
- en: 4.7.4\. Joint Face Alignment and Super-Resolution
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.4\. 联合面部对齐与超分辨率
- en: The above FSR methods require the all the HR training sample to be aligned.
    Thus, the misalignment of the input LR face image to the training face images
    often leads to sharp performance decrease and artifacts. Therefore, a set of joint
    face alignment and super-resolution methods are developed. Yu *et al*. [[49](#bib.bib49)]
    insert multiple spatial transformer networks (STN) [[183](#bib.bib183)] into the
    generator to achieve face alignment, and develop TDN and MTDN [[184](#bib.bib184)].
    As LR face images can be noisy and unaligned, Yu *et al*. build the TDAE method [[185](#bib.bib185)].
    TDAE first upsamples and coarsely aligns LR face images to produce $I_{\text{CSR}}$,
    then downsamples $I_{\text{CSR}}$ and obtains $I_{\text{CLR}}$ to reduce noise,
    and then upsamples $I_{\text{CLR}}$ for the final reconstruction.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 FSR 方法要求所有 HR 训练样本必须对齐。因此，输入的 LR 面部图像与训练面部图像的不对齐常常导致性能显著下降和伪影。因此，开发了一组联合面部对齐和超分辨率的方法。Yu
    *等人* [[49](#bib.bib49)] 在生成器中插入多个空间变换网络（STN）[[183](#bib.bib183)] 以实现面部对齐，并开发了
    TDN 和 MTDN [[184](#bib.bib184)]。由于 LR 面部图像可能存在噪声和未对齐，Yu *等人* 构建了 TDAE 方法 [[185](#bib.bib185)]。TDAE
    首先对 LR 面部图像进行上采样并粗略对齐以生成 $I_{\text{CSR}}$，然后对 $I_{\text{CSR}}$ 进行下采样并获得 $I_{\text{CLR}}$
    以减少噪声，接着对 $I_{\text{CLR}}$ 进行上采样以进行最终重建。
- en: 4.7.5\. Joint Face Frontalization and Super-Resolution
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.5\. 联合面部正面化与超分辨率
- en: Faces in the real world have various poses, and some of them may not be frontal.
    When existing FSR methods are applied to non-frontal faces, the reconstruction
    performance drops sharply and has poor visual quality. Artifacts exist even when
    FSR and face frontalization are performed in sequence or inverse order. To alleviate
    this problem, the method in [[186](#bib.bib186)] first takes advantage of STN
    and CNN to coarsely frontalize and hallucinate the faces, and then designs a fine
    upsampling network for refining face details. Yu *et al*. [[187](#bib.bib187)]
    propose a transformative adversarial neural network for joint face frontalization
    and hallucination. The method builds a transformer network to encode non-frontal
    LR face images and frontal LR ones into the latent space and requires the non-frontal
    one to be close to the frontal one, and then the encoded latent representations
    are imported into the upsampling network to recover the final results. Tu *et
    al*. [[188](#bib.bib188)] first train face restoration network and face frontalization
    network separately, and then propose task-integrated training strategy to merge
    two networks into a unified network for face frontalization and super-resolution.
    Note that face alignment aims to generate SR face images with the same pose as
    HR ones while face frontalization is to recover frontal SR faces from non-frontal
    LR faces.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的面部姿态各异，有些可能不是正面的。当现有的 FSR 方法应用于非正面面部时，重建性能急剧下降，视觉质量较差。即使在顺序或逆序进行 FSR 和面部正面化时，也会存在伪影。为缓解这一问题，文献[[186](#bib.bib186)]中提出的方法首先利用
    STN 和 CNN 粗略地将面部转化为正面并进行虚拟重建，然后设计了一个精细的上采样网络来细化面部细节。Yu *等*[[187](#bib.bib187)]
    提出了一个变换对抗神经网络用于联合面部正面化和虚拟重建。该方法建立了一个变换器网络，将非正面 LR 面部图像和正面 LR 面部图像编码到潜在空间中，并要求非正面图像接近正面图像，然后将编码的潜在表示输入到上采样网络中以恢复最终结果。Tu
    *等*[[188](#bib.bib188)] 首先分别训练面部修复网络和面部正面化网络，然后提出任务集成训练策略，将两个网络合并为一个统一的网络用于面部正面化和超分辨率。需要注意的是，面部对齐旨在生成与
    HR 图像姿态相同的 SR 面部图像，而面部正面化则是从非正面 LR 面部图像恢复正面的 SR 面部图像。
- en: 4.8\. Related Applications
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8\. 相关应用
- en: Except the above-mentioned FSR methods and joint methods, a large number of
    new methods related to FSR have emerged in recent years, including face video
    super-resolution, old photo restoration, audio-guided FSR, 3D FSR, *etc*., which
    are introduced in the following.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述提到的 FSR 方法和联合方法外，近年来出现了大量与 FSR 相关的新方法，包括面部视频超分辨率、旧照片修复、音频引导的 FSR、3D FSR、*等*，这些将在下面介绍。
- en: 4.8.1\. Face Video Super-Resolution
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.8.1\. 面部视频超分辨率
- en: Faces usually appear in LR video sequences, such as surveillance. The correlation
    between frames can provide more complementary details, which benefit the face
    reconstruction. One direct solution is to fuse multi-frame information and exploit
    inter-frame dependency [[189](#bib.bib189)]. The approach of [[190](#bib.bib190)]
    employs a generator to generate the SR results for every frame, and a fusion module
    is applied to estimate the central frame. Considering that the aforementioned
    methods cannot model the complex temporal dependency, Xin *et al*. [[191](#bib.bib191)]
    propose a motion-adaptive feedback cell which captures inter-frame motion information
    and updates the current frames adaptively. In [[192](#bib.bib192)], based on the
    assumption that multiple super-resolved frames are crucial for the reconstruction
    of the subsequent frame, and thus it designs a recurrence strategy to make better
    use of inter-frame information. Inspired by the powerful transformer, the work
    of [[193](#bib.bib193)] develops the first pure transformer-based face video hallucination
    model. MDVDNet [[194](#bib.bib194)] incorporates multiple priors from the video,
    including speech, semantic elements and facial landmarks to enhance the capability
    of deep learning-based method.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 面部通常出现在 LR 视频序列中，如监控视频。帧之间的相关性可以提供更多互补细节，有助于面部重建。一个直接的解决方案是融合多帧信息并利用帧间依赖性[[189](#bib.bib189)]。文献[[190](#bib.bib190)]的方法采用生成器为每一帧生成
    SR 结果，并应用融合模块来估计中央帧。考虑到上述方法无法建模复杂的时间依赖性，Xin *等*[[191](#bib.bib191)] 提出了一个运动自适应反馈单元，该单元捕获帧间运动信息并自适应地更新当前帧。在
    [[192](#bib.bib192)] 中，基于多个超分辨率帧对后续帧重建至关重要的假设，设计了一种递归策略来更好地利用帧间信息。受到强大变换器的启发，[[193](#bib.bib193)]
    的工作开发了第一个纯变换器基础的面部视频虚拟重建模型。MDVDNet [[194](#bib.bib194)] 综合了来自视频的多种先验，包括语音、语义元素和面部标志，以增强基于深度学习的方法的能力。
- en: 4.8.2\. Old Photo Restoration
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.8.2\. 旧照片修复
- en: Restoration of old pictures is vital and difficult in the real world since the
    degradation is too complex to be stimulated. Naturally, one solution is to learn
    the mapping from a real LR face image (regarding real old images as real LR face
    images) to an artificial LR face images, and then apply the existing FSR methods
    to the generated artificial LR face image. BOPBL [[195](#bib.bib195)] proposes
    to transform images at latent space rather than image space. Specifically, BOPBL
    first encodes real and artificial LR face images into the same latent space $S_{1}$,
    and encodes HR face images into another latent space $S_{2}$, and then maps $S_{1}$
    into $S_{2}$ by a mapping network.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 旧照片的修复在现实世界中至关重要且困难，因为退化情况过于复杂而难以模拟。自然地，一种解决方案是学习从真实LR人脸图像（将真实旧图像视为真实LR人脸图像）到人工LR人脸图像的映射，然后将现有的FSR方法应用于生成的人工LR人脸图像。BOPBL
    [[195](#bib.bib195)] 提出了在潜在空间中而不是图像空间中转换图像。具体而言，BOPBL首先将真实和人工LR人脸图像编码到相同的潜在空间
    $S_{1}$，并将HR人脸图像编码到另一个潜在空间 $S_{2}$，然后通过映射网络将 $S_{1}$ 映射到 $S_{2}$。
- en: 4.8.3\. Audio-guided FSR
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.8.3\. 音频引导的FSR
- en: Considering that audio carries face-related information [[196](#bib.bib196)],
    Meishvili *et al*. [[197](#bib.bib197)] develop the first audio-guided FSR method.
    Due to the difference of multi-modal, they build two encoders to encode image
    and audio information. Then the encoded representations of images and the audio
    are fused, and the fused results are fed into the generator to recover the final
    SR results. The introduction of the audio in FSR is novel and inspires researchers
    to exploit cross modal information, but is challenging due to the differences
    between different modalities.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到音频包含与人脸相关的信息[[196](#bib.bib196)]，Meishvili *等人* [[197](#bib.bib197)] 开发了首个音频引导的FSR方法。由于多模态之间的差异，他们建立了两个编码器来编码图像和音频信息。然后将图像和音频的编码表示融合，融合结果输入到生成器中以恢复最终的SR结果。在FSR中引入音频是创新的，并激发了研究人员利用跨模态信息的灵感，但由于不同模态之间的差异，这也充满挑战。
- en: 4.8.4\. 3D FSR
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.8.4\. 3D FSR
- en: Human face is the most concerned object in the field of computer vision. With
    the development of 2D technology, a large number of 3D methods are often proposed
    because they can provide more useful features for face reconstruction and recognition.
    In the FSR society, the early 3D FSR approach is proposed by Pan *et al.* [[198](#bib.bib198)].
    In [[199](#bib.bib199)], Berretti *et al.* propose a superface model from a sequence
    of low-resolution 3D scans. The approach of [[200](#bib.bib200)] takes only the
    rough, noisy, and low-resolution depth image as input, and predicts the corresponding
    high-quality 3D face mesh. By establishing the correspondence between the input
    LR face and 3D textures, Qu *et al.* present a patch-based 3D FSR on the mesh
    [[201](#bib.bib201)]. Benefiting from the development of deep learning technology,
    most recently, a 3D face point cloud super-resolution network approach is developed
    to infer the high-resolution data from low-resolution 3D face point cloud data [[202](#bib.bib202)].
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸是计算机视觉领域中最受关注的对象。随着2D技术的发展，通常会提出大量的3D方法，因为它们能够提供更多有用的特征用于人脸重建和识别。在FSR领域，早期的3D
    FSR方法由Pan *等人* 提出[[198](#bib.bib198)]。在[[199](#bib.bib199)]中，Berretti *等人* 从一系列低分辨率的3D扫描中提出了一种超脸模型。[[200](#bib.bib200)]的方法仅以粗糙、噪声多且低分辨率的深度图像作为输入，并预测对应的高质量3D人脸网格。通过建立输入LR人脸与3D纹理之间的对应关系，Qu
    *等人* 在网格上提出了一种基于补丁的3D FSR[[201](#bib.bib201)]。得益于深度学习技术的发展，最近，开发了一种3D人脸点云超分辨率网络方法，用于从低分辨率的3D人脸点云数据中推断高分辨率数据[[202](#bib.bib202)]。
- en: 5\. Conclusion and Future Directions
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论与未来方向
- en: 'In this review, we have presented a taxonomy of deep learning-based FSR methods.
    According to facial characteristics, this field can be divided into five categories:
    general FSR methods, prior-guided FSR methods, attribute-constrained FSR methods,
    identity-preserving FSR methods, and reference FSR methods. Then, every category
    is further divided into some subcategories depending on the design of the network
    architecture or the specific utilization of facial characteristics. In particular,
    general FSR methods are further divided into basic CNN-based methods, GAN-based
    methods, reinforcement learning-based methods, and ensemble learning-based methods.
    Besides, other methods combining facial characteristics are categorized according
    to the specific utilization pattern of facial characteristics. We also compare
    the performance of state-of-the-arts and give some deep analysis. Of course, FSR
    technique is not limited to the methods we presented, and a panoramic view of
    this fast-expanding field is rather challenging, thereby resulting in possible
    omissions. Therefore, this review serves as a pedagogical tool, providing researchers
    with insights into typical methods of FSR. In practice, researchers could use
    these general guidelines to develop the most suitable technique for their specific
    studies.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在本综述中，我们展示了基于深度学习的 FSR 方法的分类。根据面部特征，这个领域可以分为五类：通用 FSR 方法、先验引导 FSR 方法、属性约束 FSR
    方法、身份保持 FSR 方法和参考 FSR 方法。然后，每个类别根据网络架构的设计或面部特征的具体利用方式进一步细分为一些子类别。特别地，通用 FSR 方法进一步分为基本
    CNN 方法、GAN 方法、强化学习方法和集成学习方法。此外，其他结合面部特征的方法根据面部特征的具体利用模式进行分类。我们还比较了最新技术的性能并进行了深入分析。当然，FSR
    技术不限于我们展示的方法，对这个快速扩展领域的全景视图是相当具有挑战性的，因此可能存在遗漏。因此，本综述作为一种教学工具，为研究人员提供了 FSR 典型方法的见解。在实践中，研究人员可以使用这些通用指南来开发最适合其特定研究的技术。
- en: Despite great breakthroughs, FSR still presents many challenges and is expected
    to continue its rapid growth. In the following, we simply provide an outlook on
    the problems to be solved and trends to expect in the future.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了重大突破，FSR 仍然面临许多挑战，并预计将继续快速增长。接下来，我们简单地展望了需要解决的问题和未来的趋势。
- en: Design of Network. From the comparison results with the SOTA general image super-resolution
    methods, we learn that the backbone network has a crucial impact on the performance,
    especially in terms of PSNR and SSIM. Therefore, we can learn from the general
    image super-resolution task, in which many well-designed network structures have
    been continuously proposed (IPT [[203](#bib.bib203)] and SwinIR [[204](#bib.bib204)]),
    and design an effective deep network that is more suitable for FSR task. In addition
    to the effectiveness, an efficient network is also needed in practice, where the
    large model (with a mass of parameters and high computation costs) is very difficult
    to be deployed in real-world applications. Hence, developing models with lighter
    structure and lower computational taxing is still a major challenge.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 网络设计。通过与当前最先进的通用图像超分辨率方法的比较结果，我们了解到主干网络对性能有重要影响，特别是在 PSNR 和 SSIM 方面。因此，我们可以借鉴通用图像超分辨率任务中的经验，其中许多经过精心设计的网络结构（例如
    IPT [[203](#bib.bib203)] 和 SwinIR [[204](#bib.bib204)]）不断被提出，从而设计出更适合 FSR 任务的有效深度网络。除了有效性，实践中还需要高效的网络，因为大型模型（具有大量参数和高计算成本）在实际应用中很难部署。因此，开发具有更轻结构和较低计算负担的模型仍然是一个主要挑战。
- en: Exploitation of Facial Prior. As a domain-specific super-resolution technique,
    FSR can be used to recover the facial details which are lost in the observed LR
    face images. The key to the success of FSR is to effectively exploit the prior
    knowledge of human faces, from 1D vector (identity and attributes), to 2D images
    (facial landmarks, facial heatmaps and parsing maps), and to 3D models. Therefore,
    discovering new prior knowledge of human face, how to model or represent these
    prior knowledge, and how to integrate this information organically into the end-to-end
    training framework are worthy of further discussion. In addition to these explicit
    prior knowledge, how to model and utilize the implicit prior that is learned from
    the data (such as the GAN prior [[58](#bib.bib58), [106](#bib.bib106)]), may be
    another direction.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 面部先验的利用。作为一种特定领域的超分辨率技术，FSR可以用于恢复在观察到的低分辨率面部图像中丢失的面部细节。FSR成功的关键在于有效利用人脸的先验知识，从1D向量（身份和属性）、到2D图像（面部标志点、面部热图和解析图），再到3D模型。因此，发现人脸的新先验知识，如何建模或表示这些先验知识，以及如何将这些信息有机地整合到端到端训练框架中，都是值得进一步讨论的方向。除了这些显式的先验知识外，如何建模和利用从数据中学习到的隐式先验（如GAN先验[[58](#bib.bib58),
    [106](#bib.bib106)]），可能是另一个方向。
- en: Metrics and Loss Functions. As we know, the pixel-wise $\mathcal{L}_{1}$ loss
    or $\mathcal{L}_{2}$ loss tend to produce the super-resolution results with high
    PSNR and SSIM values, while perceptual loss and adversarial loss are in favor
    of letting the model produce some visually pleasant results, *i.e.*, good performance
    in terms of LPIPS and FID. Therefore, the assessment metric plays an important
    role in guiding the model optimization and affecting the final results. If we
    want to obtain a trustable result (in criminal investigation application), PSNR
    and SSIM may be better metrics. In contrast, if we just want some visually pleasant
    results, employing LPIPS and FID metrics may be a good choice. As a result, there
    is no universal assessment metric that can make the best of both worlds. Therefore,
    assessment metrics for FSR need more exploration in the future.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标和损失函数。正如我们所知，像素级的$\mathcal{L}_{1}$损失或$\mathcal{L}_{2}$损失往往产生具有高PSNR和SSIM值的超分辨率结果，而感知损失和对抗损失则更倾向于让模型产生一些视觉上令人愉悦的结果，*即*在LPIPS和FID方面表现良好。因此，评估指标在指导模型优化和影响最终结果方面发挥着重要作用。如果我们想要获得可靠的结果（在刑事调查应用中），PSNR和SSIM可能是更好的指标。相反，如果我们只是希望获得一些视觉上令人愉悦的结果，采用LPIPS和FID指标可能是一个不错的选择。因此，没有一个通用的评估指标可以兼顾两者的优点。因此，FSR的评估指标在未来需要更多探索。
- en: Discriminate FSR. In most situations, our goal is not only to reconstruct a
    visually pleasing HR face image. Actually, we hope that the super-resolved results
    can improve the face recognition task by human or computer. Therefore, it would
    be beneficial to recover a discriminated HR face image (for human) or discriminated
    feature (for computers) from an LR face image. To enhance the discriminant of
    super-resolved face images, we can use the weakly-supervised information (paired
    positive or negative samples) of the training sample to force the model to be
    able to reconstruct a discriminative face image.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 区分FSR。在大多数情况下，我们的目标不仅仅是重建一个视觉上令人愉悦的高分辨率面部图像。实际上，我们希望超分辨率的结果能够提高人类或计算机的面部识别任务。因此，从低分辨率面部图像中恢复一个具有区分性的高分辨率面部图像（对人类）或区分特征（对计算机）将是有益的。为了增强超分辨率面部图像的区分性，我们可以使用训练样本的弱监督信息（配对的正样本或负样本），迫使模型能够重建一个具有区分性的面部图像。
- en: Real-world FSR. The degradation process in the real world is too complex to
    be simulated, which results in a large gap between the synthesized LR and HR pairs
    and real-world data. When applying models trained by synthesized pairs to real-world
    LR face images, their performance drops dramatically. Given the HR training face
    images and the unpaired real-world LR face images, some methods [[102](#bib.bib102),
    [205](#bib.bib205), [206](#bib.bib206)] have been proposed to learn the real image
    degradation to create the sample pairs of synthesis LR face images and HR face
    images. These methods achieve better performance than previous approaches trained
    with the data produced by bicubic degradation. These methods actually have a potential
    assumption that all real-world LR face images share the same degradation, *i.e.*,
    captured from the same camera. However, the obtained real-world LR face images
    are very different, and their degradation processes are different. Therefore,
    designing a more robust real-world FSR method is one of the problem has to be
    settled urgently.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 实际世界的 FSR。实际世界中的降解过程过于复杂，无法模拟，这导致合成的 LR 和 HR 配对与实际数据之间存在较大差距。当将通过合成配对训练的模型应用于实际的
    LR 人脸图像时，其性能会急剧下降。考虑到 HR 训练人脸图像和未配对的实际 LR 人脸图像，一些方法 [[102](#bib.bib102), [205](#bib.bib205),
    [206](#bib.bib206)] 被提出以学习实际图像降解，创建合成的 LR 人脸图像和 HR 人脸图像的样本对。这些方法的表现优于之前使用双三次降解生成的数据训练的方法。这些方法实际上有一个潜在的假设，即所有实际世界的
    LR 人脸图像都共享相同的降解，*即*，来自相同的相机。然而，获得的实际世界 LR 人脸图像差异很大，其降解过程也不同。因此，设计一种更稳健的实际世界 FSR
    方法是当前急需解决的问题之一。
- en: Multi-modal FSR. Due to the rapid development of sensing technology, multiple
    sensors in the same system, such as autonomous driving and robots, are becoming
    more and more common. The utilization of multi-modal information (including audio,
    depth, near infrared) will be increasingly promoted. Evidently, different modalities
    provide different clues. In this field, researchers always explore image-related
    information, such as attribute, identity, and others. Nevertheless, the emergence
    of audio-guided FSR [[197](#bib.bib197)] and hyperspectral FSR [[207](#bib.bib207)]
    inspire us to take advantage of information belonging to different modalities.
    This trend will undoubtedly continue and diffuse into every category in this field.
    The introduction of multi-modal information will also spur the development of
    FSR.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态 FSR。由于传感技术的快速发展，相同系统中的多个传感器，如自动驾驶和机器人，变得越来越普遍。多模态信息（包括音频、深度、近红外）的利用将越来越受到推动。显然，不同的模态提供不同的线索。在这个领域，研究人员总是探索图像相关的信息，如属性、身份等。然而，音频引导的
    FSR [[197](#bib.bib197)] 和高光谱 FSR [[207](#bib.bib207)] 的出现激发了我们利用不同模态的信息。这一趋势无疑将继续并扩散到该领域的每一个类别。多模态信息的引入也将促进
    FSR 的发展。
- en: References
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. Liang, J. H. Lai, W. S. Zheng, and Z. Cai. A survey of face hallucination.
    In CCBR, pages 83–93, 2012.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. Liang, J. H. Lai, W. S. Zheng, 和 Z. Cai. 人脸幻觉的调查。在 CCBR，第 83–93 页，2012
    年。'
- en: '[2] N. Wang, D. Tao, X. Gao, X. Li, and J. Li. A comprehensive survey to face
    hallucination. International Journal of Computer Vision, 106(1):9–30, 2014.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] N. Wang, D. Tao, X. Gao, X. Li, 和 J. Li. 人脸幻觉的全面调查。国际计算机视觉期刊, 106(1):9–30,
    2014 年。'
- en: '[3] M. P. Autee, M. S. Mehta, M. S. Desai, V. Sawant, and A. Nagare. A review
    of various approaches to face hallucination. Procedia Computer Science, 45:361–369,
    2015.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. P. Autee, M. S. Mehta, M. S. Desai, V. Sawant, 和 A. Nagare. 各种人脸幻觉方法的综述。Procedia
    Computer Science, 45:361–369, 2015 年。'
- en: '[4] S. Kanakaraj, V. K. Govindan, and S. Kalady. Face super resolution: A survey.
    International Journal of Image, Graphics and Signal Processing, 9:54–67, 05 2017.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Kanakaraj, V. K. Govindan, 和 S. Kalady. 人脸超分辨率：一项调查。国际图像、图形和信号处理期刊,
    9:54–67, 2017 年 5 月。'
- en: '[5] K. Nguyen, C. Fookes, S. Sridharan, M. Tistarelli, and M. Nixon. Super-resolution
    for biometrics: A comprehensive survey. Pattern Recognition, 78:23–42, 2018.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] K. Nguyen, C. Fookes, S. Sridharan, M. Tistarelli, 和 M. Nixon. 生物特征识别的超分辨率：一项全面调查。Pattern
    Recognition, 78:23–42, 2018 年。'
- en: '[6] S. S. Rajput, K. V. Arya, V. Singh, and V. K. Bohat. Face hallucination
    techniques: A survey. In IC-ICTES, pages 1–6, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] S. S. Rajput, K. V. Arya, V. Singh, 和 V. K. Bohat. 人脸幻觉技术：一项调查。在 IC-ICTES，第
    1–6 页，2018 年。'
- en: '[7] H. Liu, X. Zheng, J. Han, Y. Chu, and T. Tao. Survey on GAN-based face
    hallucination with its model development. IET Image Processing, 13(14):2662–2672,
    2019.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] H. Liu, X. Zheng, J. Han, Y. Chu, 和 T. Tao. 基于 GAN 的人脸幻觉及其模型发展的调查。IET Image
    Processing, 13(14):2662–2672, 2019 年。'
- en: '[8] S. Baker and T. Kanade. Hallucinating faces. In FG, pages 83–88, 2000.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. Baker 和 T. Kanade. 幻像面孔。在FG会议，第83–88页，2000年。'
- en: '[9] C. Liu, H. Y. Shum, and C. S. Zhang. A two-step approach to hallucinating
    faces: global parametric model and local nonparametric model. In CVPR, volume 1,
    pages I–I, 2001.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] C. Liu, H. Y. Shum, 和 C. S. Zhang. 面部幻像的两步法：全局参数模型和局部非参数模型。在CVPR会议，卷1，第I–I页，2001年。'
- en: '[10] B. K. Gunturk, A. U. Batur, Y. Altunbasak, M. H. Hayes, and R. M. Mersereau.
    Eigenface-domain super-resolution for face recognition. IEEE Transactions on Image
    Processing, 12(5):597–606, 2003.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] B. K. Gunturk, A. U. Batur, Y. Altunbasak, M. H. Hayes, 和 R. M. Mersereau.
    用于面部识别的特征脸域超分辨率。IEEE Transactions on Image Processing, 12(5):597–606, 2003.'
- en: '[11] X. Wang and X. Tang. Hallucinating face by eigentransformation. IEEE Transactions
    on Systems, Man, and Cybernetics, Part C, 35(3):425–434, 2005.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] X. Wang 和 X. Tang. 通过特征变换实现面部幻像。IEEE Transactions on Systems, Man, and
    Cybernetics, Part C, 35(3):425–434, 2005.'
- en: '[12] A. Chakrabarti, A. N. Rajagopalan, and R. Chellappa. Super-resolution
    of face images using kernel PCA-based prior. IEEE Transactions on Multimedia,
    9(4):888–892, 2007.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Chakrabarti, A. N. Rajagopalan, 和 R. Chellappa. 使用基于核PCA的先验实现面部图像超分辨率。IEEE
    Transactions on Multimedia, 9(4):888–892, 2007.'
- en: '[13] J. Park and S. Lee. An example-based face hallucination method for single-frame,
    low-resolution facial images. IEEE Transactions on Image Processing, 17(10):1806–1816,
    2008.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Park 和 S. Lee. 基于实例的单帧低分辨率面部图像幻像方法。IEEE Transactions on Image Processing,
    17(10):1806–1816, 2008.'
- en: '[14] P. Innerhofer and T. PockInnerhofer. A convex approach for image hallucination.
    In AAPRW, 2013.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] P. Innerhofer 和 T. PockInnerhofer. 图像幻像的凸优化方法。在AAPRW会议，2013年。'
- en: '[15] Y. Liang, X. Xie, and J. H. Lai. Face hallucination based on morphological
    component analysis. Signal Processing, 93(2):445–458, 2013.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Liang, X. Xie, 和 J. H. Lai. 基于形态学成分分析的面部幻像。Signal Processing, 93(2):445–458,
    2013.'
- en: '[16] C. Y. Yang, S. Liu, and M. H. Yang. Structured face hallucination. In
    CVPR, pages 1099–1106, 2013.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] C. Y. Yang, S. Liu, 和 M. H. Yang. 结构化面部幻像。在CVPR会议，第1099–1106页，2013年。'
- en: '[17] H. Chang, D. Y. Yeung, and Y. Xiong. Super-resolution through neighbor
    embedding. In CVPR, volume 1, pages I–I, 2004.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] H. Chang, D. Y. Yeung, 和 Y. Xiong. 通过邻域嵌入实现超分辨率。在CVPR会议，卷1，第I–I页，2004年。'
- en: '[18] X. Ma, J. Zhang, and C. Qi. Hallucinating face by position-patch. Pattern
    Recognition, 43(6):2224–2236, 2010.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] X. Ma, J. Zhang, 和 C. Qi. 通过位置补丁实现面部幻像。Pattern Recognition, 43(6):2224–2236,
    2010.'
- en: '[19] C. Jung, L. Jiao, B. Liu, and M. Gong. Position-patch based face hallucination
    using convex optimization. IEEE Signal Processing Letters, 18(6):367–370, 2011.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] C. Jung, L. Jiao, B. Liu, 和 M. Gong. 基于位置补丁的面部幻像使用凸优化。IEEE Signal Processing
    Letters, 18(6):367–370, 2011.'
- en: '[20] J. Jiang, R. Hu, Z. Wang, and Z. Han. Noise robust face hallucination
    via locality-constrained representation. IEEE Transactions on Multimedia, 16(5):1268–1281,
    2014.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] J. Jiang, R. Hu, Z. Wang, 和 Z. Han. 通过局部约束表示实现噪声鲁棒的面部幻像。IEEE Transactions
    on Multimedia, 16(5):1268–1281, 2014.'
- en: '[21] R. A. Farrugia and C. Guillemot. Face hallucination using linear models
    of coupled sparse support. IEEE Transactions on Image Processing, 26(9):4562–4577,
    2017.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] R. A. Farrugia 和 C. Guillemot. 使用耦合稀疏支持的线性模型实现面部幻像。IEEE Transactions on
    Image Processing, 26(9):4562–4577, 2017.'
- en: '[22] J. Jiang, Y. Yu, S. Tang, J. Ma, A. Aizawa, and K. Aizawa. Context-patch
    face hallucination based on thresholding locality-constrained representation and
    reproducing learning. IEEE transactions on cybernetics, 50(1):324–337, 2018.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Jiang, Y. Yu, S. Tang, J. Ma, A. Aizawa, 和 K. Aizawa. 基于阈值局部约束表示和重现学习的上下文补全面部幻像。IEEE
    Transactions on Cybernetics, 50(1):324–337, 2018.'
- en: '[23] J. Shi, X. Liu, Y. Zong, C. Qi, and G. Zhao. Hallucinating face image
    by regularization models in high-resolution feature space. IEEE Transactions on
    Image Processing, 27(6):2980–2995, 2018.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Shi, X. Liu, Y. Zong, C. Qi, 和 G. Zhao. 通过高分辨率特征空间中的正则化模型实现面部图像幻像。IEEE
    Transactions on Image Processing, 27(6):2980–2995, 2018.'
- en: '[24] L. Chen, J. Pan, and Q. Li. Robust face image super-resolution via joint
    learning of subdivided contextual model. IEEE Transactions on Image Processing,
    28(12):5897–5909, 2019.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] L. Chen, J. Pan, 和 Q. Li. 通过联合学习细分上下文模型实现鲁棒的面部图像超分辨率。IEEE Transactions
    on Image Processing, 28(12):5897–5909, 2019.'
- en: '[25] J. Shi and G. Zhao. Face hallucination via coarse-to-fine recursive kernel
    regression structure. IEEE Transactions on Multimedia, 21(9):2223–2236, 2019.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J. Shi 和 G. Zhao. 通过粗到细递归核回归结构实现面部幻像。IEEE Transactions on Multimedia,
    21(9):2223–2236, 2019.'
- en: '[26] L. Liu, C. P. Chen, and S. Li. Hallucinating color face image by learning
    graph representation in quaternion space. IEEE transactions on cybernetics, pages
    1–13, 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] L. Liu, C. P. Chen, 和 S. Li. 通过在四元数空间中学习图形表示实现彩色面部图像幻像。IEEE Transactions
    on Cybernetics, 页1–13, 2020.'
- en: '[27] L. Chen, J. Pan, J. Jiang, J. Zhang, Z. Han, and L. Bao. Multi-stage degradation
    homogenization for super-resolution of face images with extreme degradations.
    IEEE Transactions on Image Processing, 30:5600–5612, 2021.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] L. Chen, J. Pan, J. Jiang, J. Zhang, Z. Han, 和 L. Bao. 极端退化下面孔图像超分辨率的多阶段退化均化。IEEE图像处理汇刊，30:5600–5612，2021年。'
- en: '[28] Y. Zhuang, J. Zhang, and F. Wu. Hallucinating faces: LPH super-resolution
    and neighbor reconstruction for residue compensation. Pattern Recognitionm, 40(11):3178–3194,
    2007.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Zhuang, J. Zhang, 和 F. Wu. 幻觉面孔：LPH超分辨率与邻域重建用于残差补偿。模式识别，40(11):3178–3194，2007年。'
- en: '[29] H. Huang, H. He, X. Fan, and J. Zhang. Super-resolution of human face
    image using canonical correlation analysis. Pattern Recognition, 43(7):2532–2543,
    2010.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] H. Huang, H. He, X. Fan, 和 J. Zhang. 使用典型相关分析的人脸图像超分辨率。模式识别，43(7):2532–2543，2010年。'
- en: '[30] Z. Wang, J. Chen, and S. C. H. Hoi. Deep learning for image super-resolution:
    A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Z. Wang, J. Chen, 和 S. C. H. Hoi. 图像超分辨率的深度学习：综述。IEEE模式分析与机器智能汇刊，2020年。'
- en: '[31] S. Anwar, S. Khan, and N. Barnes. A deep journey into super-resolution:
    A survey. ACM Computing Surveys, 53(3):1–34, 2020.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Anwar, S. Khan, 和 N. Barnes. 超分辨率的深度之旅：综述。ACM计算调查，53(3):1–34，2020年。'
- en: '[32] W. Yang, X. Zhang, Y. Tian, W. Wang, J. H. Xue, and Q. Liao. Deep learning
    for single image super-resolution: A brief review. IEEE Transactions on Multimedia,
    21(12):3106–3121, 2019.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] W. Yang, X. Zhang, Y. Tian, W. Wang, J. H. Xue, 和 Q. Liao. 单图像超分辨率的深度学习：简要回顾。IEEE多媒体汇刊，21(12):3106–3121，2019年。'
- en: '[33] H. Liu, Z. Ruan, P. Zhao, F. Shang, L. Yang, and Y. Liu. Video super resolution
    based on deep learning: A comprehensive survey. arXiv preprint arXiv:2007.12928,
    2020.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] H. Liu, Z. Ruan, P. Zhao, F. Shang, L. Yang, 和 Y. Liu. 基于深度学习的视频超分辨率：全面综述。arXiv预印本arXiv:2007.12928，2020年。'
- en: '[34] X. Yu and F. Porikli. Ultra-resolving face images by discriminative generative
    networks. In ECCV, pages 318–333, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] X. Yu 和 F. Porikli. 通过判别生成网络超分辨率面孔图像。发表于ECCV，页码318–333，2016年。'
- en: '[35] Y. Chen, Y. Tai, X. Liu, C. Shen, and J. Yang. FSRNet: End-to-end learning
    face super-resolution with facial priors. In CVPR, pages 2492–2501, 2018.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Chen, Y. Tai, X. Liu, C. Shen, 和 J. Yang. FSRNet：端到端学习面孔超分辨率与面部先验。发表于CVPR，页码2492–2501，2018年。'
- en: '[36] C. Ma, Z. Jiang, Y. Rao, J. Lu, and J. Zhou. Deep face super-resolution
    with iterative collaboration between attentive recovery and landmark estimation.
    In CVPR, pages 5569–5578, 2020.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] C. Ma, Z. Jiang, Y. Rao, J. Lu, 和 J. Zhou. 通过注意力恢复与地标估计之间的迭代协作实现深度面孔超分辨率。发表于CVPR，页码5569–5578，2020年。'
- en: '[37] X. Li, W. Li, D. Ren, H. Zhang, M. Wang, and W. Zuo. Enhanced blind face
    restoration with multi-exemplar images and adaptive spatial feature fusion. In
    CVPR, pages 2706–2715, 2020.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] X. Li, W. Li, D. Ren, H. Zhang, M. Wang, 和 W. Zuo. 结合多样本图像与自适应空间特征融合的增强盲人面孔修复。发表于CVPR，页码2706–2715，2020年。'
- en: '[38] X. Li, C. Chen, S. Zhou, X. Lin, W. Zuo, and L. Zhang. Blind face restoration
    via deep multi-scale component dictionaries. In ECCV, pages 399–415, 2020.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] X. Li, C. Chen, S. Zhou, X. Lin, W. Zuo, 和 L. Zhang. 通过深度多尺度组件字典进行盲人面孔修复。发表于ECCV，页码399–415，2020年。'
- en: '[39] X. Li, M. Liu, Y. Ye, W. Zuo, L. Lin, and R. Yang. Learning warped guidance
    for blind face restoration. In ECCV, pages 272–289, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] X. Li, M. Liu, Y. Ye, W. Zuo, L. Lin, 和 R. Yang. 学习变形引导进行盲人面孔修复。发表于ECCV，页码272–289，2018年。'
- en: '[40] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter.
    Gans trained by a two time-scale update rule converge to a local nash equilibrium.
    In NIPS, pages 6626–6637, 2017.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, 和 S. Hochreiter. 通过双时间尺度更新规则训练的GAN收敛于局部纳什均衡。发表于NIPS，页码6626–6637，2017年。'
- en: '[41] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable
    effectiveness of deep features as a perceptual metric. In CVPR, pages 586–595,
    2018.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, 和 O. Wang. 深度特征作为感知度量的非理性有效性。发表于CVPR，页码586–595，2018年。'
- en: '[42] W. Zhou and A. C. Bovik. A universal image quality index. IEEE Signal
    Processing Letters, 9(3):81–84, 2002.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] W. Zhou 和 A. C. Bovik. 通用图像质量指数。IEEE信号处理快报，9(3):81–84，2002年。'
- en: '[43] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale structural similarity
    for image quality assessment. In ACSSC, volume 2, pages 1398–1402, 2003.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Z. Wang, E. P. Simoncelli, 和 A. C. Bovik. 图像质量评估的多尺度结构相似性。发表于ACSSC，第2卷，页码1398–1402，2003年。'
- en: '[44] A. Mittal, R. Soundararajan, and A. C. Bovik. Making a ¡°completely blind¡±
    image quality analyzer. IEEE Signal Processing Letters, 20(3):209–212, 2013.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Mittal, R. Soundararajan, 和 A. C. Bovik. 制作“完全盲”的图像质量分析仪。IEEE信号处理快报，20(3):209–212，2013年。'
- en: '[45] R. Kalarot, T. Li, and F. Porikli. Component attention guided face super-resolution
    network: CAGFace. In WACV, pages 359–369, 2020.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] R. Kalarot, T. Li, 和 F. Porikli. 组件注意力引导的人脸超分辨率网络：CAGFace。发表在WACV，第359–369页，2020年。'
- en: '[46] W. S. Lai, J. B. Huang, N. Ahuja, and M. H. Yang. Deep laplacian pyramid
    networks for fast and accurate super-resolution. In CVPR, pages 624–632, 2017.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] W. S. Lai, J. B. Huang, N. Ahuja, 和 M. H. Yang. 用于快速准确超分辨率的深度拉普拉斯金字塔网络。发表在CVPR，第624–632页，2017年。'
- en: '[47] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale
    image recognition. CoRR, abs/1409.1556, 2015.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] K. Simonyan 和 A. Zisserman. 用于大规模图像识别的非常深卷积网络。CoRR，abs/1409.1556，2015年。'
- en: '[48] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, X. Bing, and Y. Bengio.
    Generative adversarial nets. In NIPS, volume 2, pages 2672–2680, 2014.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, X. Bing, 和 Y. Bengio. 生成对抗网络。发表在NIPS，第2卷，第2672–2680页，2014年。'
- en: '[49] X. Yu and F. Porikli. Face hallucination with tiny unaligned images by
    transformative discriminative neural networks. In AAAI, pages 4327–4333, 2017.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] X. Yu 和 F. Porikli. 通过变换性判别神经网络对齐微小未对齐图像的人脸幻觉。发表在AAAI，第4327–4333页，2017年。'
- en: '[50] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial
    networks. In PMLR, volume 70, pages 214–223, 2017.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] M. Arjovsky, S. Chintala, 和 L. Bottou. Wasserstein生成对抗网络。发表在PMLR，第70卷，第214–223页，2017年。'
- en: '[51] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved
    training of wasserstein GANs. In NIPS, pages 5767–5777, 2017.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, 和 A. Courville. 改进的Wasserstein
    GAN训练。发表在NIPS，第5767–5777页，2017年。'
- en: '[52] J. Y. Zhu, T. Park, P P. Isola, and A. A. Efros. Unpaired image-to-image
    translation using cycle-consistent adversarial networks. In ICCV, pages 2223–2232,
    2017.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Y. Zhu, T. Park, P. P. Isola, 和 A. A. Efros. 使用循环一致对抗网络的无配对图像到图像转换。发表在ICCV，第2223–2232页，2017年。'
- en: '[53] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional
    neural networks. In CVPR, pages 2414–2423, 2016.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] L. A. Gatys, A. S. Ecker, 和 M. Bethge. 使用卷积神经网络的图像风格迁移。发表在CVPR，第2414–2423页，2016年。'
- en: '[54] T. C. Wang, M. Y. Liu, J. Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro.
    High-resolution image synthesis and semantic manipulation with conditional GANs.
    In CVPR, pages 8798–8807, 2018.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] T. C. Wang, M. Y. Liu, J. Y. Zhu, A. Tao, J. Kautz, 和 B. Catanzaro. 使用条件GAN的高分辨率图像合成和语义操控。发表在CVPR，第8798–8807页，2018年。'
- en: '[55] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in
    the wild. In ICCV, pages 3730–3738, 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Z. Liu, P. Luo, X. Wang, 和 X. Tang. 深度学习面部属性。发表在ICCV，第3730–3738页，2016年。'
- en: '[56] V. Le, J. Brandt, L. Zhe, L. D. Bourdev, and T. S. Huang. Interactive
    facial feature localization. In ECCV, pages 679–692, 2012.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] V. Le, J. Brandt, L. Zhe, L. D. Bourdev, 和 T. S. Huang. 交互式面部特征定位。发表在ECCV，第679–692页，2012年。'
- en: '[57] C. H. Lee, Z. Liu, L. Wu, and P. Luo. MaskGAN: Towards diverse and interactive
    facial image manipulation. In CVPR, pages 5549–5558, 2020.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] C. H. Lee, Z. Liu, L. Wu, 和 P. Luo. MaskGAN: 迈向多样化和交互式面部图像操控。发表在CVPR，第5549–5558页，2020年。'
- en: '[58] T. Karras, S. Laine, and T. Aila. A style-based generator architecture
    for generative adversarial networks. In CVPR, pages 4396–4405, 2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] T. Karras, S. Laine, 和 T. Aila. 一种基于风格的生成对抗网络生成器架构。发表在CVPR，第4396–4405页，2019年。'
- en: '[59] M. Koestinger, P. Wohlhart, P. M. Roth, and H. Bischof. Annotated facial
    landmarks in the wild: A large-scale, real-world database for facial landmark
    localization. In ICCVW, pages 2144–2151, 2011.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. Koestinger, P. Wohlhart, P. M. Roth, 和 H. Bischof. 野外标注的人脸关键点：一个大规模的真实世界数据库用于面部关键点定位。发表在ICCVW，第2144–2151页，2011年。'
- en: '[60] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. A semi-automatic
    methodology for facial landmark annotation. In CVPRW, pages 896–903, 2013.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, 和 M. Pantic. 一种半自动面部关键点标注方法。发表在CVPRW，第896–903页，2013年。'
- en: '[61] A. Bulat and G. Tzimiropoulos. How far are we from solving the 2D & 3D
    face alignment problem? (and a dataset of 230,000 3D facial landmarks). In ICCV,
    pages 1021–1030, 2017.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A. Bulat 和 G. Tzimiropoulos. 我们距离解决2D和3D人脸对齐问题还有多远？（以及一个包含230,000个3D面部关键点的数据集）。发表在ICCV，第1021–1030页，2017年。'
- en: '[62] S. Zafeiriou, G. Trigeorgis, G. Chrysos, J. Deng, and J. Shen. The menpo
    facial landmark localisation challenge: A step towards the solution. In CVPRW,
    pages 170–179, 2017.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. Zafeiriou, G. Trigeorgis, G. Chrysos, J. Deng, 和 J. Shen. Menpo人脸关键点定位挑战：迈向解决方案的一步。发表在CVPRW，第170–179页，2017年。'
- en: '[63] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller. Labeled faces
    in the wild: A database for studying face recognition in unconstrained environments.
    Technical Report 07-49, University of Massachusetts, Amherst, 2007.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] G. B. Huang, M. Mattar, T. Berg, 和 E. Learned-Miller. 野外标记的人脸：用于研究无约束环境下人脸识别的数据库。技术报告07-49，马萨诸塞大学，安姆斯特，2007年。'
- en: '[64] L. Wolf, T. Hassner, and Y. Taigman. Effective unconstrained face recognition
    by combining multiple descriptors and learned background statistics. IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 33(10):1978–1990, 2011.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] L. Wolf, T. Hassner, 和 Y. Taigman. 通过结合多个描述符和学习的背景统计实现有效的非约束人脸识别。IEEE
    Transactions on Pattern Analysis and Machine Intelligence, 33(10):1978–1990, 2011。'
- en: '[65] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In
    BMVC, pages 1–12, 2015.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] O. M. Parkhi, A. Vedaldi, 和 A. Zisserman. 深度人脸识别。在 BMVC, 页码 1–12, 2015。'
- en: '[66] B. Chen, C. Chen, and W. H. Hsu. Face recognition and retrieval using
    cross-age reference coding with cross-age celebrity dataset. IEEE Transactions
    on Multimedia, 17(6):804–815, 2015.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] B. Chen, C. Chen, 和 W. H. Hsu. 使用跨年龄参考编码与跨年龄名人数据集进行人脸识别和检索。IEEE Transactions
    on Multimedia, 17(6):804–815, 2015。'
- en: '[67] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. VGGFace2: A dataset
    for recognising faces across pose and age. In FG, pages 67–74, 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, 和 A. Zisserman. VGGFace2: 一个用于识别跨姿态和年龄的人脸的数据集。在
    FG, 页码 67–74, 2018。'
- en: '[68] A. Bansal, A. Nanduri, C. Castillo, R. Ranjan, and R. Chellappa. UMDFaces:
    An annotated face dataset for training deep networks. CoRR, abs/1611.01484, 2016.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Bansal, A. Nanduri, C. Castillo, R. Ranjan, 和 R. Chellappa. UMDFaces:
    用于训练深度网络的注释人脸数据集。CoRR, abs/1611.01484, 2016。'
- en: '[69] Y. Dong, L. Zhen, S. Liao, and S. Z. Li. Learning face representation
    from scratch. CoRR, abs/1411.7923, 2014.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Dong, L. Zhen, S. Liao, 和 S. Z. Li. 从头学习人脸表示。CoRR, abs/1411.7923, 2014。'
- en: '[70] C. Dong, C. C. Loy, K. He, and X. Tang. Image super-resolution using deep
    convolutional networks. IEEE Transactions on Pattern Analysis and Machine Intelligence,
    38(2):295–307, 2016.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] C. Dong, C. C. Loy, K. He, 和 X. Tang. 使用深度卷积网络进行图像超分辨率。IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 38(2):295–307, 2016。'
- en: '[71] E. Zhou, H. Fan, Z. Cao, Y. Jiang, and Q. Yin. Learning face hallucination
    in the wild. In AAAI, pages 3871–3877, 2015.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] E. Zhou, H. Fan, Z. Cao, Y. Jiang, 和 Q. Yin. 在野外学习人脸幻觉。在 AAAI, 页码 3871–3877,
    2015。'
- en: '[72] W. Huang, Y. Chen, M. Li, and Y. Hui. Super-resolution reconstruction
    of face image based on convolution network. In AISC, pages 288–294, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] W. Huang, Y. Chen, M. Li, 和 Y. Hui. 基于卷积网络的人脸图像超分辨率重建。在 AISC, 页码 288–294,
    2018。'
- en: '[73] D. Huang and H. Liu. Face hallucination using convolutional neural network
    with iterative back projection. In CCBR, pages 167–175, 2016.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] D. Huang 和 H. Liu. 使用卷积神经网络和迭代反投影进行人脸幻觉。在 CCBR, 页码 167–175, 2016。'
- en: '[74] X. Chen, X. Wang, Y. Lu, W. Li, Z. Wang, and Z. Huang. RBPNET: An asymptotic
    residual back-projection network for super-resolution of very low-resolution face
    image. Neurocomputing, 376:119–127, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] X. Chen, X. Wang, Y. Lu, W. Li, Z. Wang, 和 Z. Huang. RBPNET: 一种用于极低分辨率人脸图像超分辨率的渐近残差反投影网络。Neurocomputing,
    376:119–127, 2020。'
- en: '[75] X. Chen and Y. Wu. Efficient face super-resolution based on separable
    convolution projection networks. In CRC, pages 92–97, 2020.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] X. Chen 和 Y. Wu. 基于可分离卷积投影网络的高效人脸超分辨率。在 CRC, 页码 92–97, 2020。'
- en: '[76] Y. Liu, Z. Dong, K. Pang Lim, and N. Ling. A densely connected face super-resolution
    network based on attention mechanism. In ICIEA, pages 148–152, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. Liu, Z. Dong, K. Pang Lim, 和 N. Ling. 基于注意力机制的密集连接人脸超分辨率网络。在 ICIEA,
    页码 148–152, 2020。'
- en: '[77] V. Chudasama, K. Nighania, K. Upla, K. Raja, R. Ramachandra, and C. Busch.
    E-ComSupResNet: Enhanced face super-resolution through compact network. IEEE Transactions
    on Biometrics, Behavior, and Identity Science, 3(2):166–179, 2021.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] V. Chudasama, K. Nighania, K. Upla, K. Raja, R. Ramachandra, 和 C. Busch.
    E-ComSupResNet: 通过紧凑网络增强的人脸超分辨率。IEEE Transactions on Biometrics, Behavior, and
    Identity Science, 3(2):166–179, 2021。'
- en: '[78] C. Chen, D. Gong, H. Wang, Z. Li, and K. Y. K. Wong. Learning spatial
    attention for face super-resolution. IEEE Transactions on Image Processing, 30:1219–1231,
    2021.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] C. Chen, D. Gong, H. Wang, Z. Li, 和 K. Y. K. Wong. 学习用于人脸超分辨率的空间注意力。IEEE
    Transactions on Image Processing, 30:1219–1231, 2021。'
- en: '[79] L. Han, H. Zhen, G. Jin, and D. Xin. A noise robust face hallucination
    framework via cascaded model of deep convolutional networks and manifold learning.
    In ICME, pages 1–6, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] L. Han, H. Zhen, G. Jin, 和 D. Xin. 通过深度卷积网络和流形学习级联模型的噪声鲁棒人脸幻觉框架。在 ICME,
    页码 1–6, 2018。'
- en: '[80] H. Nie, Y. Lu, and J. Ikram. Face hallucination via convolution neural
    network. In ICTAI, pages 485–489, 2016.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] H. Nie, Y. Lu, 和 J. Ikram. 通过卷积神经网络进行人脸幻觉。在 ICTAI, 页码 485–489, 2016。'
- en: '[81] Z. Chen, J. Lin, T. Zhou, and F. Wu. Sequential gating ensemble network
    for noise robust multiscale face restoration. IEEE Transactions on Cybernetics,
    pages 1–11, 2019.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Z. Chen, J. Lin, T. Zhou, 和 F. Wu. 一种用于噪声鲁棒的多尺度人脸恢复的顺序门控集成网络。IEEE Transactions
    on Cybernetics, 页码 1–11, 2019。'
- en: '[82] H. Huang, R. He, Z. Sun, and T. Tan. Wavelet-SRNet: A wavelet-based CNN
    for multi-scale face super resolution. In ICCV, pages 1689–1697, 2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] H. Huang, R. He, Z. Sun, 和 T. Tan. 小波-SRNet: 一种基于小波的卷积神经网络用于多尺度面部超分辨率。发表于
    ICCV，页码 1689–1697, 2017。'
- en: '[83] Y. Liu, D. Sun, F. Wang, L. K. Pang, and Y. Lai. Learning wavelet coefficients
    for face super-resolution. The Visual Computer, (3):1613–1622, 2020.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Liu, D. Sun, F. Wang, L. K. Pang, 和 Y. Lai. 学习小波系数进行面部超分辨率。视觉计算，(3):1613–1622,
    2020。'
- en: '[84] X. Hu, P. Ma, Z. Mai, S. Peng, Z. Yang, and L. Wang. Face hallucination
    from low quality images using definition-scalable inference. Pattern Recognition,
    94:110–121, 2019.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] X. Hu, P. Ma, Z. Mai, S. Peng, Z. Yang, 和 L. Wang. 使用定义可扩展推断进行低质量图像的面部幻觉生成。模式识别，94:110–121,
    2019。'
- en: '[85] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-resolution using
    very deep convolutional networks. In CVPR, pages 1646–1654, 2016.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Kim, J. K. Lee, 和 K. M. Lee. 使用非常深的卷积网络进行准确的图像超分辨率。发表于 CVPR，页码 1646–1654,
    2016。'
- en: '[86] W. Ko and S. Chien. Patch-based face hallucination with multitask deep
    neural network. In ICME, pages 1–6, 2016.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] W. Ko 和 S. Chien. 基于补丁的面部幻觉生成与多任务深度神经网络。发表于 ICME，页码 1–6, 2016。'
- en: '[87] Z. Feng, J. Lai, X. Xie, D. Yang, and M. Ling. Face hallucination by deep
    traversal network. In ICPR, pages 3276–3281, 2016.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Z. Feng, J. Lai, X. Xie, D. Yang, 和 M. Ling. 通过深度遍历网络生成面部幻觉。发表于 ICPR，页码
    3276–3281, 2016。'
- en: '[88] T. Lu, H. Wang, Z. Xiong, J. Jiang, Y. Zhang, H. Zhou, and Z. Wang. Face
    hallucination using region-based deep convolutional networks. In ICIP, pages 1657–1661,
    2017.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] T. Lu, H. Wang, Z. Xiong, J. Jiang, Y. Zhang, H. Zhou, 和 Z. Wang. 使用基于区域的深度卷积网络生成面部幻觉。发表于
    ICIP，页码 1657–1661, 2017。'
- en: '[89] O. Tuzel, Y. Taguchi, and J. R. Hershey. Global-local face upsampling
    network. ArXiv, abs/1603.07235, 2016.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] O. Tuzel, Y. Taguchi, 和 J. R. Hershey. 全局-局部面部上采样网络。ArXiv, abs/1603.07235,
    2016。'
- en: '[90] T. Lu, J. Wang, J. Jiang, and Y. Zhang. Global-local fusion network for
    face super-resolution. Neurocomputing, 387:309–320, 2020.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] T. Lu, J. Wang, J. Jiang, 和 Y. Zhang. 用于面部超分辨率的全局-局部融合网络。神经计算，387:309–320,
    2020。'
- en: '[91] K. Jiang, Z. Wang, P. Yi, T. Lu, J. Jiang, and Z. Xiong. Dual-path deep
    fusion network for face image hallucination. IEEE Transactions on Neural Networks
    and Learning Systems, pages 1–14, 2020.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] K. Jiang, Z. Wang, P. Yi, T. Lu, J. Jiang, 和 Z. Xiong. 双路径深度融合网络用于面部图像的幻觉生成。IEEE
    神经网络与学习系统汇刊，页码 1–14, 2020。'
- en: '[92] S. Ko and B. R. Dai. Multi-laplacian GAN with edge enhancement for face
    super resolution. In ICPR, pages 3505–3512, 2021.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. Ko 和 B. R. Dai. 具有边缘增强的多拉普拉斯生成对抗网络用于面部超分辨率。发表于 ICPR，页码 3505–3512, 2021。'
- en: '[93] L. Yang, P. Wang, Z. Gao, S. Wang, P. Ren, S. Ma, and W. Gao. Implicit
    subspace prior learning for dual-blind face restoration. arXiv preprint arXiv:2010.05508,
    2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] L. Yang, P. Wang, Z. Gao, S. Wang, P. Ren, S. Ma, 和 W. Gao. 隐式子空间先验学习用于双盲面部修复。arXiv
    预印本 arXiv:2010.05508, 2020。'
- en: '[94] Y. Luo and K. Huang. Super-resolving tiny faces with face feature vectors.
    In ICIST, pages 145–152, 2020.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. Luo 和 K. Huang. 使用面部特征向量超分辨率微小面部。发表于 ICIST，页码 145–152, 2020。'
- en: '[95] S. D. Indradi, A. Arifianto, and K. N. Ramadhani. Face image super-resolution
    using inception residual network and GAN framework. In ICOICT, pages 1–6, 2019.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] S. D. Indradi, A. Arifianto, 和 K. N. Ramadhani. 使用 inception 残差网络和生成对抗网络框架进行面部图像超分辨率。发表于
    ICOICT，页码 1–6, 2019。'
- en: '[96] Z. Chen and Y. Tong. Face super-resolution through wasserstein GANs. ArXiv,
    abs/1705.02438, 2017.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Z. Chen 和 Y. Tong. 通过 wasserstein 生成对抗网络进行面部超分辨率。ArXiv, abs/1705.02438,
    2017。'
- en: '[97] B. Huang, W. Chen, X. Wu, and C. L. Lin. High-quality face image SR using
    conditional generative adversarial networks. ARXiv, abs/1707.00737, 2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] B. Huang, W. Chen, X. Wu, 和 C. L. Lin. 使用条件生成对抗网络进行高质量面部图像超分辨率。ARXiv,
    abs/1707.00737, 2017。'
- en: '[98] H. Dou, C. Chen, X. Hu, Z. Xuan, Z. Hu, and S. Peng. PCA-SRGAN: Incremental
    orthogonal projection discrimination for face super-resolution. In ACM MM, pages
    1891–1899, 2020.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] H. Dou, C. Chen, X. Hu, Z. Xuan, Z. Hu, 和 S. Peng. PCA-SRGAN: 用于面部超分辨率的增量正交投影判别。发表于
    ACM MM，页码 1891–1899, 2020。'
- en: '[99] M. Zhang and Q. Ling. Supervised pixel-wise GAN for face super-resolution.
    IEEE Transactions on Multimedia, 2020.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] M. Zhang 和 Q. Ling. 监督的像素级生成对抗网络用于面部超分辨率。IEEE 多媒体汇刊，2020。'
- en: '[100] K. Grm, M. Pernus, L. Cluzel, W. Scheirer, S. Dobrisek, and V. Struc.
    Face hallucination revisited: An exploratory study on dataset bias. In CVPRW,
    pages 2405–2413, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] K. Grm, M. Pernus, L. Cluzel, W. Scheirer, S. Dobrisek, 和 V. Struc. 面部幻觉再访：关于数据集偏差的探索性研究。发表于
    CVPRW，页码 2405–2413, 2019。'
- en: '[101] A. Aakerberg, K. Nasrollahi, and T. B. Moeslund. Real-world super-resolution
    of face-images from surveillance cameras. arXiv preprint arXiv:2102.03113, 2021.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] A. Aakerberg, K. Nasrollahi, 和 T. B. Moeslund. 真实世界中的面部图像超分辨率来自监控摄像头。arXiv
    预印本 arXiv:2102.03113, 2021。'
- en: '[102] A. Bulat, Y. Jing, and G. Tzimiropoulos. To learn image super-resolution,
    use a GAN to learn how to do image degradation first. In ECCV, pages 187–202,
    2018.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] A. Bulat, Y. Jing, 和 G. Tzimiropoulos. 为了学习图像超分辨率，使用 GAN 学习如何进行图像降质。发表于
    ECCV，第 187–202 页，2018 年。'
- en: '[103] S. Goswami, Aakanksha, and A. N. Rajagopalan. Robust super-resolution
    of real faces using smooth features. In ECCV Workshop, pages I–I, 11 2020.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] S. Goswami, Aakanksha, 和 A. N. Rajagopalan. 使用平滑特征进行真实面孔的鲁棒超分辨率。发表于 ECCV
    研讨会，第 I–I 页，2020 年 11 月。'
- en: '[104] W. Zheng, L. Yan, W. Zhang, C. Gou, and F. Wang. Guided cyclegan via
    semi-dual optimal transport for photo-realistic face super-resolution. In ICIP,
    pages 2851–2855, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] W. Zheng, L. Yan, W. Zhang, C. Gou, 和 F. Wang. 通过半对偶最优传输的引导式 CycleGAN
    实现照片级真实脸部超分辨率。发表于 ICIP，第 2851–2855 页，2019 年。'
- en: '[105] Z. Cheng, X. Zhu, and S. Gong. Characteristic regularisation for super-resolving
    face images. In WACV, pages 2424–2433, 2020.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Z. Cheng, X. Zhu, 和 S. Gong. 超分辨率人脸图像的特征正则化。发表于 WACV，第 2424–2433 页，2020
    年。'
- en: '[106] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila.
    Analyzing and improving the image quality of stylegan. In CVPR, pages 8107–8116,
    2020.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, 和 T. Aila.
    分析和改进 StyleGAN 的图像质量。发表于 CVPR，第 8107–8116 页，2020 年。'
- en: '[107] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of
    GANs for improved quality, stability, and variation. In ICLR, 2018.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] T. Karras, T. Aila, S. Laine, 和 J. Lehtinen. 为了提高质量、稳定性和变化，逐步增长的 GAN。发表于
    ICLR，2018 年。'
- en: '[108] Y. Choi, M. Choi, M. Kim, J. W. Ha, S. Kim, and J. Choo. Stargan: Unified
    generative adversarial networks for multi-domain image-to-image translation. In
    CVPR, pages 8789–8797, 2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Y. Choi, M. Choi, M. Kim, J. W. Ha, S. Kim, 和 J. Choo. StarGAN：用于多域图像到图像转换的统一生成对抗网络。发表于
    CVPR，第 8789–8797 页，2018 年。'
- en: '[109] S. Menon, A. Damian, S. Hu, N. Ravi, and C. Rudin. PULSE: Self-supervised
    photo upsampling via latent space exploration of generative models. In CVPR, pages
    2223–2232, 2020.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] S. Menon, A. Damian, S. Hu, N. Ravi, 和 C. Rudin. PULSE：通过生成模型的潜在空间探索自监督照片上采样。发表于
    CVPR，第 2223–2232 页，2020 年。'
- en: '[110] K. C. K. Chan, X. Wang, X. Xu, J. Gu, and C. C. Loy. GLEAN: Generative
    latent bank for large-factor image super-resolution. In CVPR, pages 14245–14254,
    2021.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] K. C. K. Chan, X. Wang, X. Xu, J. Gu, 和 C. C. Loy. GLEAN：用于大倍数图像超分辨率的生成潜在库。发表于
    CVPR，第 14245–14254 页，2021 年。'
- en: '[111] X. Wang, Y. Li, H. Zhang, and Y. Shan. Towards real-world blind face
    restoration with generative facial prior. In CVPR, pages 9168–9178, 2021.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] X. Wang, Y. Li, H. Zhang, 和 Y. Shan. 通过生成面部先验实现真实世界盲人脸修复。发表于 CVPR，第 9168–9178
    页，2021 年。'
- en: '[112] T. Yang, P. Ren, X. Xie, and L. Zhang. GAN prior embedded network for
    blind face restoration in the wild. In CVPR, pages 672–681, 2021.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] T. Yang, P. Ren, X. Xie, 和 L. Zhang. 嵌入 GAN 先验的网络用于野外盲人脸修复。发表于 CVPR，第
    672–681 页，2021 年。'
- en: '[113] Y. Shi, G. Li, Q. Cao, K. Wang, and L. Lin. Face hallucination by attentive
    sequence optimization with reinforcement learning. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, 42(11):2809–2824, 2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Y. Shi, G. Li, Q. Cao, K. Wang, 和 L. Lin. 通过注意序列优化和强化学习实现人脸幻觉。IEEE 图案分析与机器智能学报，42(11)：2809–2824，2020
    年。'
- en: '[114] K. Jiang, Z. Wang, P. Yi, G. Wang, K. Gu, and J. Jiang. ATMFN: Adaptive-threshold-based
    multi-model fusion network for compressed face hallucination. IEEE Transactions
    on Multimedia, 22(10):2734–2747, 2020.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] K. Jiang, Z. Wang, P. Yi, G. Wang, K. Gu, 和 J. Jiang. ATMFN：基于自适应阈值的多模型融合网络，用于压缩人脸幻觉。IEEE
    多媒体学报，22(10)：2734–2747，2020 年。'
- en: '[115] Y. Song, J. Zhang, S. He, L. Bao, and Q. Yang. Learning to hallucinate
    face images via component generation and enhancement. In IJCAI, pages 4537–4543,
    2017.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Y. Song, J. Zhang, S. He, L. Bao, 和 Q. Yang. 通过组件生成和增强学习人脸图像的幻觉。发表于 IJCAI，第
    4537–4543 页，2017 年。'
- en: '[116] J. Jiang, Y. Yu, J. Hu, S. Tang, and J. Ma. Deep CNN denoiser and multi-layer
    neighbor component embedding for face hallucination. In IJCAI, pages 771¨C–778,
    2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] J. Jiang, Y. Yu, J. Hu, S. Tang, 和 J. Ma. 深度 CNN 去噪器和多层邻域组件嵌入用于人脸幻觉。发表于
    IJCAI，第 771–778 页，2018 年。'
- en: '[117] C. Chen, X. Li, L. Yang, X. Lin, and K. Wong. Progressive semantic-aware
    style transformation for blind face restoration. In CVPR, pages 11896–11905, 2021.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] C. Chen, X. Li, L. Yang, X. Lin, 和 K. Wong. 盲人脸修复的渐进语义感知风格转换。发表于 CVPR，第
    11896–11905 页，2021 年。'
- en: '[118] X. Yu, L. Zhang, and W. Xie. Semantic-driven face hallucination based
    on residual network. IEEE Transactions on Biometrics, Behavior, and Identity Science,
    3(2):214–228, 2021.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] X. Yu, L. Zhang, 和 W. Xie. 基于残差网络的语义驱动人脸幻觉。IEEE 生物识别、行为和身份科学学报，3(2)：214–228，2021
    年。'
- en: '[119] C. Wang, Z. Zhong, J. Jiang, D. Zhai, and X. Liu. Parsing map guided
    multi-scale attention network for face hallucination. In ICASSP, pages 2518–2522,
    2020.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] C. Wang, Z. Zhong, J. Jiang, D. Zhai, 和 X. Liu. 基于地图指导的多尺度注意力网络用于面部幻觉。发表于
    ICASSP，页码 2518–2522，2020。'
- en: '[120] X. Hu, W. Ren, J. Lamaster, X. Cao, X. Li, Z. Li, B. Menze, and W. Liu.
    Face super-resolution guided by 3D facial priors. In ECCV, pages 763–780, 2020.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] X. Hu, W. Ren, J. Lamaster, X. Cao, X. Li, Z. Li, B. Menze, 和 W. Liu.
    由 3D 面部先验指导的面部超分辨率。发表于 ECCV，页码 763–780，2020。'
- en: '[121] S. Zhu, S. Liu, C. L. Chen, and X. Tang. Deep cascaded bi-network for
    face hallucination. In ECCV, volume 9909, pages 614–630, 2016.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] S. Zhu, S. Liu, C. L. Chen, 和 X. Tang. 用于面部幻觉的深度级联双网络。发表于 ECCV，卷号 9909，页码
    614–630，2016。'
- en: '[122] K. Li, B. Bare, B. Yan, B. Feng, and C. Yao. Face hallucination based
    on key parts enhancement. In ICASSP, volume 30, pages 1378–1382, 2018.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] K. Li, B. Bare, B. Yan, B. Feng, 和 C. Yao. 基于关键部位增强的面部幻觉。发表于 ICASSP，卷号
    30，页码 1378–1382，2018。'
- en: '[123] Y. Yin, J. P. Robinson, Y. Zhang, and Y. Fu. Joint super-resolution and
    alignment of tiny faces. In AAAI, pages 2693–12700, 2019.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Y. Yin, J. P. Robinson, Y. Zhang, 和 Y. Fu. 微小面部的联合超分辨率和对齐。发表于 AAAI，页码
    2693–12700，2019。'
- en: '[124] M. Li, Z. Zhang, J. Yu, and C. W. Chen. Learning face image super-resolution
    through facial semantic attribute transformation and self-attentive structure
    enhancement. IEEE Transactions on Multimedia, 23:468–483, 2020.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] M. Li, Z. Zhang, J. Yu, 和 C. W. Chen. 通过面部语义属性变换和自注意力结构增强学习面部图像超分辨率。IEEE
    Transactions on Multimedia, 23:468–483，2020。'
- en: '[125] X. Yu, B. Fernando, B. Ghanem, F. Porikli, and R. Hartley. Face super-resolution
    guided by facial component heatmaps. In ECCV, pages 217–233, September 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] X. Yu, B. Fernando, B. Ghanem, F. Porikli, 和 R. Hartley. 由面部组件热图指导的面部超分辨率。发表于
    ECCV，页码 217–233，2018年9月。'
- en: '[126] A. Bulat and G. Tzimiropoulos. Super-FAN: Integrated facial landmark
    localization and super-resolution of real-world low resolution faces in arbitrary
    poses with GANs. In CVPR, pages 109–117, 2018.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] A. Bulat 和 G. Tzimiropoulos. Super-FAN: 集成面部地标定位和真实世界低分辨率面部超分辨率的 GAN。发表于
    CVPR，页码 109–117，2018。'
- en: '[127] D. Kim, M. Kim, G. Kwon, and D. S. Kim. Progressive face super-resolution
    via attention to facial landmark. In BMCV, pages I–I, 2019.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] D. Kim, M. Kim, G. Kwon, 和 D. S. Kim. 通过关注面部地标的渐进式面部超分辨率。发表于 BMCV，页码
    I–I，2019。'
- en: '[128] T. Zhao and C. Zhang. Saan: Semantic attention adaptation network for
    face super-resolution. In ICME, pages 1–6, 2020.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] T. Zhao 和 C. Zhang. Saan: 用于面部超分辨率的语义注意力自适应网络。发表于 ICME，页码 1–6，2020。'
- en: '[129] C. Wang, J. Jiang, and X. Liu. Heatmap-aware pyramid face hallucination.
    In ICME, pages 1–6, 2021.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] C. Wang, J. Jiang, 和 X. Liu. 热图感知金字塔面部幻觉。发表于 ICME，页码 1–6，2021。'
- en: '[130] J. Li, B. Bare, S. Zhou, B. Yan, and K. Li. Organ-branched cnn for robust
    face super-resolution. In ICME, pages 1–6, 2021.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] J. Li, B. Bare, S. Zhou, B. Yan, 和 K. Li. 用于鲁棒面部超分辨率的器官分支 CNN。发表于 ICME，页码
    1–6，2021。'
- en: '[131] Z. S. Liu, W. C. Siu, and Y. L. Chan. Features guided face super-resolution
    via hybrid model of deep learning and random forests. IEEE Transactions on Image
    Processing, 30:4157–4170, 2021.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Z. S. Liu, W. C. Siu, 和 Y. L. Chan. 通过深度学习和随机森林的混合模型引导的特征面部超分辨率。IEEE
    Transactions on Image Processing, 30:4157–4170, 2021。'
- en: '[132] M. Li, Y. Sun, Z. Zhang, and J. Yu. A coarse-to-fine face hallucination
    method by exploiting facial prior knowledge. In ICIP, pages 61–65, 2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] M. Li, Y. Sun, Z. Zhang, 和 J. Yu. 通过利用面部先验知识的粗到细面部幻觉方法。发表于 ICIP，页码 61–65，2018。'
- en: '[133] Y. Zhang, Y. Wu, and L. Chen. MSFSR: A multi-stage face super-resolution
    with accurate facial representation via enhanced facial boundaries. In CVPRW,
    pages 2120–2129, 2020.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Y. Zhang, Y. Wu, 和 L. Chen. MSFSR: 通过增强面部边界实现精确面部表示的多阶段面部超分辨率。发表于 CVPRW，页码
    2120–2129，2020。'
- en: '[134] H. Wang, Q. Hu, C. Wu, J. Chi, X. Yu, and H. Wu. DCLNet: Dual closed-loop
    networks for face super-resolution. Knowledge-Based Systems, 222:106987, 2021.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] H. Wang, Q. Hu, C. Wu, J. Chi, X. Yu, 和 H. Wu. DCLNet: 用于面部超分辨率的双闭环网络。Knowledge-Based
    Systems, 222:106987, 2021。'
- en: '[135] S. Liu, C. Xiong, X. Shi, and Z. Gao. Progressive face super-resolution
    with cascaded recurrent convolutional network. Neurocomputing, 449:357–367, 2021.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] S. Liu, C. Xiong, X. Shi, 和 Z. Gao. 使用级联递归卷积网络的渐进式面部超分辨率。Neurocomputing,
    449:357–367, 2021。'
- en: '[136] S. Liu, C. Xiong, and Z. Gao. Face super-resolution network with incremental
    enhancement of facial parsing information. In ICPR, pages 7537–7543, 2021.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] S. Liu, C. Xiong, 和 Z. Gao. 面部超分辨率网络，通过增量增强面部解析信息。发表于 ICPR，页码 7537–7543，2021。'
- en: '[137] L. Li, J. Tang, Z. Ye, B. Sheng, L. Mao, and L. Ma. Unsupervised face
    super-resolution via gradient enhancement and semantic guidance. The Visual Computer,
    pages 1–13, 2021.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] L. Li, J. Tang, Z. Ye, B. Sheng, L. Mao 和 L. Ma. 通过梯度增强和语义引导进行无监督人脸超分辨率。The
    Visual Computer, 页码 1–13, 2021。'
- en: '[138] Y. Lu, Y. W. Tai, and C. K. Tang. Attribute-guided face generation using
    conditional CycleGAN. In ECCV, pages 282–297, 2018.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Y. Lu, Y. W. Tai 和 C. K. Tang. 使用条件 CycleGAN 的属性引导人脸生成。发表于 ECCV, 页码 282–297,
    2018。'
- en: '[139] X. Yu, B. Fernando, R. Hartley, and F. Porikli. Super-resolving very
    low-resolution face images with supplementary attributes. In CVPR, pages 908–917,
    2018.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] X. Yu, B. Fernando, R. Hartley 和 F. Porikli. 通过补充属性超分辨率非常低分辨率的人脸图像。发表于
    CVPR, 页码 908–917, 2018。'
- en: '[140] M. Li, Y. Sun, Z. Zhang, H. Xie, and J. Yu. Deep learning face hallucination
    via attributes transfer and enhancement. In ICME, pages 604–609, 2019.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] M. Li, Y. Sun, Z. Zhang, H. Xie 和 J. Yu. 通过属性转移和增强的深度学习人脸幻觉。发表于 ICME,
    页码 604–609, 2019。'
- en: '[141] C. H. Lee, K. Zhang, H. C. Lee, C. W. Cheng, and W. Hsu. Attribute augmented
    convolutional neural network for face hallucination. In CVPRW, pages 721–729,
    2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] C. H. Lee, K. Zhang, H. C. Lee, C. W. Cheng 和 W. Hsu. 用于人脸幻觉的属性增强卷积神经网络。发表于
    CVPRW, 页码 721–729, 2018。'
- en: '[142] X. Yu, B. Fernando, R. Hartley, and F. Porikli. Semantic face hallucination:
    Super-resolving very low-resolution face images with supplementary attributes.
    IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(11):2926–2943,
    2020.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] X. Yu, B. Fernando, R. Hartley 和 F. Porikli. 语义人脸幻觉：通过补充属性超分辨率非常低分辨率的人脸图像。IEEE
    Transactions on Pattern Analysis and Machine Intelligence, 42(11):2926–2943, 2020。'
- en: '[143] J. Xin, N. Wang, X. Gao, and J. Li. Residual attribute attention network
    for face image super-resolution. In AAAI, pages 9054–9061, 2019.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] J. Xin, N. Wang, X. Gao 和 J. Li. 用于人脸图像超分辨率的残差属性注意力网络。发表于 AAAI, 页码 9054–9061,
    2019。'
- en: '[144] J. Xin, N. Wang, X. Jiang, J. Li, X. Gao, and Z. Li. Facial attribute
    capsules for noise face super resolution. In AAAI, volume 34, pages 12476–12483,
    2020.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. Xin, N. Wang, X. Jiang, J. Li, X. Gao 和 Z. Li. 噪声人脸超分辨率的面部属性胶囊。发表于
    AAAI, 卷 34, 页码 12476–12483, 2020。'
- en: '[145] K. Zhang, Z. Zhang, C. W. Cheng, W. H. Hsu, Y. Qiao, W. Liu, and T. Zhang.
    Super-identity convolutional neural network for face hallucination. In ECCV, pages
    183–198, 2018.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] K. Zhang, Z. Zhang, C. W. Cheng, W. H. Hsu, Y. Qiao, W. Liu 和 T. Zhang.
    用于人脸幻觉的超身份卷积神经网络。发表于 ECCV, 页码 183–198, 2018。'
- en: '[146] B. Bayramli, U. Ali, T. Qi, and H. Lu. FH-GAN: Face hallucination and
    recognition using generative adversarial network. In ICONIP, pages 3–15, 2019.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] B. Bayramli, U. Ali, T. Qi 和 H. Lu. FH-GAN: 使用生成对抗网络进行人脸幻觉和识别。发表于 ICONIP,
    页码 3–15, 2019。'
- en: '[147] H. Huang, R. He, Z. Sun, and T. Tan. Wavelet domain generative adversarial
    network for multi-scale face hallucination. International Journal of Computer
    Vision, 127(6-7):763–784, 2019.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] H. Huang, R. He, Z. Sun 和 T. Tan. 多尺度人脸幻觉的小波域生成对抗网络。International Journal
    of Computer Vision, 127(6-7):763–784, 2019。'
- en: '[148] S. Lai, C. He, and K. Lam. Low-resolution face recognition based on identity-preserved
    face hallucination. In ICIP, pages 1173–1177, 2019.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] S. Lai, C. He 和 K. Lam. 基于身份保持的人脸幻觉的低分辨率人脸识别。发表于 ICIP, 页码 1173–1177,
    2019。'
- en: '[149] X. Cheng, J. Lu, B. Yuan, and J. Zhou. Identity-preserving face hallucination
    via deep reinforcement learning. IEEE Transactions on Circuits and Systems for
    Video Technology, pages 4796–4809, 2020.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] X. Cheng, J. Lu, B. Yuan 和 J. Zhou. 通过深度强化学习进行身份保持的人脸幻觉。IEEE Transactions
    on Circuits and Systems for Video Technology, 页码 4796–4809, 2020。'
- en: '[150] K. Grm, W. J. Scheirer, and V. Štruc. Face hallucination using cascaded
    super-resolution and identity priors. IEEE Transactions on Image Processing, 29:2150–2165,
    2020.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] K. Grm, W. J. Scheirer 和 V. Štruc. 使用级联超分辨率和身份先验的人脸幻觉。IEEE Transactions
    on Image Processing, 29:2150–2165, 2020。'
- en: '[151] A. A. Abello and R. Hirata. Optimizing super resolution for face recognition.
    In SIBGRAPI, pages 194–201, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] A. A. Abello 和 R. Hirata. 优化人脸识别的超分辨率。发表于 SIBGRAPI, 页码 194–201, 2019。'
- en: '[152] F. Cheng, T. Lu, Y. Wang, and Y. Zhang. Face super-resolution through
    dual-identity constraint. In ICME, pages 1–6, 2021.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] F. Cheng, T. Lu, Y. Wang 和 Y. Zhang. 通过双重身份约束进行人脸超分辨率。发表于 ICME, 页码 1–6,
    2021。'
- en: '[153] J. Kim, G. Li, I. Yun, C. Jung, and J. Kim. Edge and identity preserving
    network for face super-resolution. Neurocomputing, 446:11–22, 2021.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] J. Kim, G. Li, I. Yun, C. Jung 和 J. Kim. 边缘和身份保持网络用于人脸超分辨率。Neurocomputing,
    446:11–22, 2021。'
- en: '[154] E. Ataer-Cansizoglu, M. Jones, Z. Zhang, and A. Sullivan. Verification
    of very low-resolution faces using an identity-preserving deep face super-resolution
    network. ArXiv, abs/1903.10974, 2019.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] E. Ataer-Cansizoglu, M. Jones, Z. Zhang 和 A. Sullivan. 使用保持身份的深度人脸超分辨率网络进行非常低分辨率人脸的验证。ArXiv,
    abs/1903.10974, 2019。'
- en: '[155] J. Chen, J. Chen, Z. Wang, C. Liang, and C. W. Lin. Identity-aware face
    super-resolution for low-resolution face recognition. IEEE Signal Processing Letters,
    27:645–649, 2020.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] J. Chen, J. Chen, Z. Wang, C. Liang, 和 C. W. Lin. 用于低分辨率人脸识别的身份感知人脸超分辨率。IEEE信号处理快报，27:645–649，2020年。'
- en: '[156] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. SphereFace: Deep hypersphere
    embedding for face recognition. In CVPR, pages 212–220, 2017.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, 和 L. Song. SphereFace: 用于人脸识别的深度超球面嵌入。发表于CVPR，页码212–220，2017年。'
- en: '[157] C. Hsu, C. Lin, W. Su, and G. Cheung. SiGAN: Siamese generative adversarial
    network for identity-preserving face hallucination. IEEE Transactions on Image
    Processing, 28(12):6225–6236, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] C. Hsu, C. Lin, W. Su, 和 G. Cheung. SiGAN: 用于身份保持的人脸生成对抗网络。IEEE图像处理杂志，28(12):6225–6236，2019年。'
- en: '[158] H. Kazemi, F. Taherkhani, and N. M. Nasrabadi. Identity-aware deep face
    hallucination via adversarial face verification. In BTAS, pages 1–10, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] H. Kazemi, F. Taherkhani, 和 N. M. Nasrabadi. 通过对抗性人脸验证的身份感知深度人脸幻觉。发表于BTAS，页码1–10，2019年。'
- en: '[159] B. Dogan, S. Gu, and R. Timofte. Exemplar guided face image super-resolution
    without facial landmarks. In CVPRW, pages 1814–1823, 2019.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] B. Dogan, S. Gu, 和 R. Timofte. 无需面部特征点的示例指导人脸图像超分辨率。发表于CVPRW，页码1814–1823，2019年。'
- en: '[160] K. Wang, J. Oramas, and T. Tuytelaars. Multiple exemplars-based hallucination
    for face super-resolution and editing. In ACCV, November 2020.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] K. Wang, J. Oramas, 和 T. Tuytelaars. 基于多个示例的幻觉用于人脸超分辨率和编辑。发表于ACCV，2020年11月。'
- en: '[161] X. Li, G. Duan, Z. Wang, J. Ren, Y. Zhang, J. Zhang, and K. Song. Recovering
    extremely degraded faces by joint super-resolution and facial composite. In ICTAI,
    pages 524–530, 2019.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] X. Li, G. Duan, Z. Wang, J. Ren, Y. Zhang, J. Zhang, 和 K. Song. 通过联合超分辨率和人脸合成恢复极度退化的人脸。发表于ICTAI，页码524–530，2019年。'
- en: '[162] S. Schaefer, T. Mcphail, and J. Warren. Image deformation using moving
    least squares. In ACM SIGGRAPH, page 533¨C540, 2006.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] S. Schaefer, T. Mcphail, 和 J. Warren. 使用移动最小二乘法的图像变形。发表于ACM SIGGRAPH，页码533–540，2006年。'
- en: '[163] X. Huang and S. Belongie. Arbitrary style transfer in real-time with
    adaptive instance normalization. In ICCV, pages 1501–1510, 2017.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] X. Huang 和 S. Belongie. 使用自适应实例归一化的实时任意风格转移。发表于ICCV，页码1501–1510，2017年。'
- en: '[164] T. Baltrusaitis, P. Robinson, and L. P. Morency. Constrained local neural
    fields for robust facial landmark detection in the wild. In ICCVW, pages 354–361,
    2013.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] T. Baltrusaitis, P. Robinson, 和 L. P. Morency. 用于在自然环境中稳健的人脸标志检测的约束局部神经场。发表于ICCVW，页码354–361，2013年。'
- en: '[165] T. Baltrusaitis, A. Zadeh, Y. C. Lim, and L. P. Morency. OpenFace 2.0:
    Facial behavior analysis toolkit. In FG, pages 59–66, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] T. Baltrusaitis, A. Zadeh, Y. C. Lim, 和 L. P. Morency. OpenFace 2.0:
    人脸行为分析工具包。发表于FG，页码59–66，2018年。'
- en: '[166] A. Zadeh, C. L. Yao, T. Baltruaitis, and L. P. Morency. Convolutional
    experts constrained local model for 3D facial landmark detection. In ICCVW, pages
    2519–2528, 2017.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] A. Zadeh, C. L. Yao, T. Baltruaitis, 和 L. P. Morency. 用于3D人脸标志检测的卷积专家约束局部模型。发表于ICCVW，页码2519–2528，2017年。'
- en: '[167] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang. BiSeNet: Bilateral
    segmentation network for real-time semantic segmentation. In ECCV, pages 325–341,
    2018.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, 和 N. Sang. BiSeNet: 用于实时语义分割的双边分割网络。发表于ECCV，页码325–341，2018年。'
- en: '[168] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu. Image super-resolution
    using very deep residual channel attention networks. In ECCV, pages 286–301, 2018.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, 和 Y. Fu. 使用非常深的残差通道注意力网络的图像超分辨率。发表于ECCV，页码286–301，2018年。'
- en: '[169] Y. Mei, Y. Fan, and Y. Zhou. Image super-resolution with non-local sparse
    attention. In CVPR, pages 3517–3526, 2021.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Y. Mei, Y. Fan, 和 Y. Zhou. 使用非局部稀疏注意力的图像超分辨率。发表于CVPR，页码3517–3526，2021年。'
- en: '[170] L. Yang, B. Shao, T. Sun, S. Ding, and X. Zhang. Hallucinating very low-resolution
    and obscured face images. CoRR, abs/1811.04645, 2018.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] L. Yang, B. Shao, T. Sun, S. Ding, 和 X. Zhang. 幻觉非常低分辨率和遮挡的人脸图像。CoRR，abs/1811.04645，2018年。'
- en: '[171] J. Cai, H. Hu, S. Shan, and X. Chen. FCSR-GAN: Joint face completion
    and super-resolution via multi-task learning. IEEE Transactions on Biometrics,
    Behavior, and Identity Science, 2:109–121, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] J. Cai, H. Hu, S. Shan, 和 X. Chen. FCSR-GAN: 通过多任务学习实现人脸补全与超分辨率。IEEE生物识别、行为与身份科学杂志，2:109–121，2020年。'
- en: '[172] Z. Liu, Y. Wu, L. Li, C. Zhang, and B. Wu. Joint face completion and
    super-resolution using multi-scale feature relation learning. arXiv preprint arXiv:2003.00255,
    2020.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Z. Liu, Y. Wu, L. Li, C. Zhang, 和 B. Wu. 使用多尺度特征关系学习的联合人脸补全与超分辨率。arXiv预印本
    arXiv:2003.00255，2020年。'
- en: '[173] Y. Zhang, X. Yu, X. Lu, and P. Liu. Pro-uigan: Progressive face hallucination
    from occluded thumbnails. arXiv preprint arXiv:2108.00602, 2021.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Y. Zhang, X. Yu, X. Lu, 和 P. Liu. Pro-uigan：从遮挡的缩略图中渐进式人脸虚拟化。arXiv 预印本
    arXiv:2108.00602，2021年。'
- en: '[174] X. Xu, D. Sun, J. Pan, Y. Zhang, H. Pfister, and M. H. Yang. Learning
    to super-resolve blurry face and text images. In ICCV, pages 251–260, 2017.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] X. Xu, D. Sun, J. Pan, Y. Zhang, H. Pfister, 和 M. H. Yang. 学习超分辨率模糊的人脸和文本图像。
    In ICCV, 页码 251–260，2017年。'
- en: '[175] Y. Song, J. Zhang, L. Gong, S. He, L. Bao, J. Pan, Q. Yang, and M. H.
    Yang. Joint face hallucination and deblurring via structure generation and detail
    enhancement. International Journal of Computer Vision, 127(6-7):785–800, 2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Y. Song, J. Zhang, L. Gong, S. He, L. Bao, J. Pan, Q. Yang, 和 M. H. Yang.
    通过结构生成和细节增强进行联合人脸虚拟化和去模糊处理。国际计算机视觉杂志，127(6-7):785–800，2019年。'
- en: '[176] C. H. Yang and L. W. Chang. Deblurring and super-resolution using deep
    gated fusion attention networks for face images. In ICASSP, pages 1623–1627, 2020.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] C. H. Yang 和 L. W. Chang. 使用深度门控融合注意力网络进行人脸图像的去模糊和超分辨率。 In ICASSP, 页码
    1623–1627，2020年。'
- en: '[177] Y. Xu, H. Zou, Y. Huang, L. Jin, and H. Ling. Super-resolving blurry
    face images with identity preservation. Pattern Recognition Letters, 146:158–164,
    2021.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Y. Xu, H. Zou, Y. Huang, L. Jin, 和 H. Ling. 在保留身份的情况下超分辨率模糊人脸图像。模式识别快报，146:158–164，2021年。'
- en: '[178] H. A. Le and I. A. Kakadiaris. SeLENet: A semi-supervised low light face
    enhancement method for mobile face unlock. In ICB, pages 1–8, 2019.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] H. A. Le 和 I. A. Kakadiaris. SeLENet：一种用于移动人脸解锁的半监督低光人脸增强方法。 In ICB,
    页码 1–8，2019年。'
- en: '[179] X. Ding and R. Hu. Learning to see faces in the dark. In ICME, pages
    1–6, 2020.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] X. Ding 和 R. Hu. 学习在黑暗中看清人脸。 In ICME, 页码 1–6，2020年。'
- en: '[180] Y. Zhang, T. Tsang, Y. Luo, C. Hu, X. Lu, and X. Yu. Copy and paste gan:
    Face hallucination from shaded thumbnails. In CVPR, pages 7355–7364, 2020.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Y. Zhang, T. Tsang, Y. Luo, C. Hu, X. Lu, 和 X. Yu. 复制和粘贴 GAN：从阴影缩略图中进行人脸虚拟化。
    In CVPR, 页码 7355–7364，2020年。'
- en: '[181] Y. Zhang, I. Tsang, Y. Luo, C. Hu, X. Lu, and X. Yu. Recursive copy and
    paste gan: Face hallucination from shaded thumbnails. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, pages 1–1, 2021.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Y. Zhang, I. Tsang, Y. Luo, C. Hu, X. Lu, 和 X. Yu. 递归复制和粘贴 GAN：从阴影缩略图中进行人脸虚拟化。IEEE
    模式分析与机器智能学报，页码 1–1，2021年。'
- en: '[182] R. Yasarla, H. Joze, and V. M. Patel. Network architecture search for
    face enhancement. arXiv preprint arXiv:2105.06528, 2021.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] R. Yasarla, H. Joze, 和 V. M. Patel. 人脸增强的网络架构搜索。arXiv 预印本 arXiv:2105.06528，2021年。'
- en: '[183] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. Spatial
    transformer networks. In NIPS, pages 2017–2025, 2015.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] M. Jaderberg, K. Simonyan, A. Zisserman, 和 K. Kavukcuoglu. 空间变换网络。 In
    NIPS, 页码 2017–2025，2015年。'
- en: '[184] X. Yu, F. Porikli, B. Fernando, and R. Hartley. Hallucinating unaligned
    face images by multiscale transformative discriminative networks. International
    Journal of Computer Vision, 128(2):500–526, 2020.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] X. Yu, F. Porikli, B. Fernando, 和 R. Hartley. 通过多尺度变换判别网络虚拟化未对齐的人脸图像。国际计算机视觉杂志，128(2):500–526，2020年。'
- en: '[185] X. Yu and F. Porikli. Hallucinating very low-resolution unaligned and
    noisy face images by transformative discriminative autoencoders. In CVPR, pages
    3760–3768, 2017.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] X. Yu 和 F. Porikli. 通过变换判别自编码器虚拟化非常低分辨率未对齐和噪声人脸图像。 In CVPR, 页码 3760–3768，2017年。'
- en: '[186] Y. Zhang, I. W. Tsang, J. Li, P. Liu, X. Lu, and X. Yu. Face hallucination
    with finishing touches. IEEE Transactions on Image Processing, 30:1728–1743, 2021.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Y. Zhang, I. W. Tsang, J. Li, P. Liu, X. Lu, 和 X. Yu. 完美的人脸虚拟化。IEEE 图像处理学报，30:1728–1743，2021年。'
- en: '[187] X. Yu, F. Shiri, B. Ghanem, and F. Porikli. Can we see more? joint frontalization
    and hallucination of unaligned tiny faces. IEEE Transactions on Pattern Analysis
    and Machine Intelligence, 42(9):2148–2164, 2020.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] X. Yu, F. Shiri, B. Ghanem, 和 F. Porikli. 我们能看到更多吗？未对齐的小人脸的联合正面化和虚拟化。IEEE
    模式分析与机器智能学报，42(9):2148–2164，2020年。'
- en: '[188] X. Tu, J. Zhao, Q. Liu, W. Ai, G. Guo, Z. Li, W. Liu, and J. Feng. Joint
    face image restoration and frontalization for recognition. IEEE Transactions on
    Circuits and Systems for Video Technology, pages 1–1, 2021.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] X. Tu, J. Zhao, Q. Liu, W. Ai, G. Guo, Z. Li, W. Liu, 和 J. Feng. 联合人脸图像恢复和正面化以进行识别。IEEE
    视频技术电路与系统学报，页码 1–1，2021年。'
- en: '[189] D. Li and Z. Wang. Face video super-resolution with identity guided generative
    adversarial networks. In CCCV, pages 357–369, 2017.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] D. Li 和 Z. Wang. 使用身份引导生成对抗网络进行人脸视频超分辨率。 In CCCV, 页码 357–369，2017年。'
- en: '[190] E. Ataer-Cansizoglu and M. Jones. Super-resolution of very low-resolution
    faces from videos. In BMCV, 2018.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] E. Ataer-Cansizoglu 和 M. Jones. 从视频中超分辨率极低分辨率人脸。 In BMCV, 2018年。'
- en: '[191] J. Xin, N. Wang, J. Li, X. Gao, and Z. Li. Video face super-resolution
    with motion-adaptive feedback cell. AAAI, 34(7):12468–12475, 2020.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] J. Xin, N. Wang, J. Li, X. Gao, 和 Z. Li. 视频面部超分辨率与运动自适应反馈单元. AAAI, 34(7):12468–12475,
    2020.'
- en: '[192] C. Fang, G. Li, X. Han, and Y. Yu. Self-enhanced convolutional network
    for facial video hallucination. IEEE Transactions on Image Processing, 29:3078–3090,
    2020.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] C. Fang, G. Li, X. Han, 和 Y. Yu. 自增强卷积网络用于面部视频幻觉. IEEE 图像处理汇刊, 29:3078–3090,
    2020.'
- en: '[193] Y. Gan, Y. Luo, X. Yu, B. Zhang, and Y. Yang. Vidface: A full-transformer
    solver for video facehallucination with unaligned tiny snapshots. ArXiv, abs/2105.14954,
    2021.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Y. Gan, Y. Luo, X. Yu, B. Zhang, 和 Y. Yang. Vidface: 一个全变换器求解器用于视频面部幻觉与未对齐的小快照.
    ArXiv, abs/2105.14954, 2021.'
- en: '[194] X. Zhang and X. Wu. Multi-modality deep restoration of extremely compressed
    face videos. ArXiv, abs/2107.05548, 2021.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] X. Zhang 和 X. Wu. 极度压缩面部视频的多模态深度恢复. ArXiv, abs/2107.05548, 2021.'
- en: '[195] Z. Wan, B. Zhang, D. Chen, P. Zhang, D. Chen, J. Liao, and F. Wen. Bringing
    old photos back to life. In CVPR, pages 2747–2757, 2020.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Z. Wan, B. Zhang, D. Chen, P. Zhang, D. Chen, J. Liao, 和 F. Wen. 让老照片重焕生机.
    见于 CVPR, 页码 2747–2757, 2020.'
- en: '[196] T. H. Oh, T. Dekel, C. Kim, I. Mosseri, W. T. Freeman, M. Rubinstein,
    and W. Matusik. Speech2face: Learning the face behind a voice. In CVPR, pages
    7539–7548, 2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] T. H. Oh, T. Dekel, C. Kim, I. Mosseri, W. T. Freeman, M. Rubinstein,
    和 W. Matusik. Speech2face: 学习声音背后的面孔. 见于 CVPR, 页码 7539–7548, 2019.'
- en: '[197] G. Meishvili, S. Jenni, and P. Favaro. Learning to have an ear for face
    super-resolution. In CVPR, pages 1364–1374, June 2020.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] G. Meishvili, S. Jenni, 和 P. Favaro. 学习用耳朵识别面部超分辨率. 见于 CVPR, 页码 1364–1374,
    2020年6月.'
- en: '[198] G. Pan, S. Han, Z. Wu, and Y. Wang. Super-resolution of 3D face. In ECCV,
    pages 389–401\. Springer, 2006.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] G. Pan, S. Han, Z. Wu, 和 Y. Wang. 3D面部的超分辨率. 见于 ECCV, 页码 389–401. Springer,
    2006.'
- en: '[199] S. Berretti, A. Del Bimbo, and P. Pala. Superfaces: A super-resolution
    model for 3D faces. In ECCV, pages 73–82\. Springer, 2012.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] S. Berretti, A. Del Bimbo, 和 P. Pala. Superfaces: 3D面部的超分辨率模型. 见于 ECCV,
    页码 73–82. Springer, 2012.'
- en: '[200] L. Shu, I. Kemelmacher-Shlizerman, and L. G. Shapiro. 3D face hallucination
    from a single depth frame. pages 31–38, 2014.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] L. Shu, I. Kemelmacher-Shlizerman, 和 L. G. Shapiro. 从单个深度帧生成3D面部幻觉. 页码
    31–38, 2014.'
- en: '[201] C. Qu, C. Herrmann, E. Monari, T. Schuchert, and J. Beyerer. Robust 3D
    patch-based face hallucination. In WACV, pages 1105–1114\. IEEE, 2017.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] C. Qu, C. Herrmann, E. Monari, T. Schuchert, 和 J. Beyerer. 稳健的基于3D补丁的面部幻觉.
    见于 WACV, 页码 1105–1114. IEEE, 2017.'
- en: '[202] J. Li, F. Zhu, X. Yang, and Q. Zhao. 3D face point cloud super-resolution
    network. In IJCB, pages 1–8\. IEEE, 2021.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] J. Li, F. Zhu, X. Yang, 和 Q. Zhao. 3D面部点云超分辨率网络. 见于 IJCB, 页码 1–8. IEEE,
    2021.'
- en: '[203] H Chen, Y Wang, T Guo, C Xu, Y Deng, Z Liu, S Ma, C Xu, C Xu, and W Gao.
    Pre-trained image processing transformer. In CVPR, pages 12299–12310, 2021.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] H Chen, Y Wang, T Guo, C Xu, Y Deng, Z Liu, S Ma, C Xu, C Xu, 和 W Gao.
    预训练的图像处理变换器. 见于 CVPR, 页码 12299–12310, 2021.'
- en: '[204] J. Liang, J. Cao, G. Sun, K. Zhang, Van G.L., and R. Timofte. SwinIR:
    Image restoration using swin transformer. ArXiv, abs/2108.10257, 2021.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] J. Liang, J. Cao, G. Sun, K. Zhang, Van G.L., 和 R. Timofte. SwinIR: 使用swin变换器的图像恢复.
    ArXiv, abs/2108.10257, 2021.'
- en: '[205] S. Goswami and A. N. Rajagopalan. Robust super-resolution of real faces
    using smooth features. In ECCV, pages 169–185\. Springer, 2020.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] S. Goswami 和 A. N. Rajagopalan. 使用平滑特征的真实面孔稳健超分辨率. 见于 ECCV, 页码 169–185.
    Springer, 2020.'
- en: '[206] A. Aakerberg, K. Nasrollahi, and T.B. Moeslund. Real-world super-resolution
    of face-images from surveillance cameras. ArXiv, abs/2102.03113, 2021.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] A. Aakerberg, K. Nasrollahi, 和 T.B. Moeslund. 实际世界中的面部图像超分辨率来自监控摄像头.
    ArXiv, abs/2102.03113, 2021.'
- en: '[207] J. Jiang, C. Wang, X. Liu, and J. Ma. Spectral splitting and aggregation
    network for hyperspectral face super-resolution. ArXiv, abs/2108.13584, 2021.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] J. Jiang, C. Wang, X. Liu, 和 J. Ma. 用于高光谱面部超分辨率的光谱分割和聚合网络. ArXiv, abs/2108.13584,
    2021.'
