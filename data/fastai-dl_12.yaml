- en: 'Deep Learning 2: Part 2 Lesson 12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习 2：第 2 部分 第 12 课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94)
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自* [*fast.ai 课程*](http://www.fast.ai/)* 的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*
    [*Jeremy*](https://twitter.com/jeremyphoward) *和* [*Rachel*](https://twitter.com/math_rachel)
    *给了我这个学习的机会。*'
- en: Generative Adversarial Networks (GANs)
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）
- en: '[Video](https://youtu.be/ondivPiwQho) / [Forum](http://forums.fast.ai/t/part-2-lesson-12-in-class/15023)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[视频](https://youtu.be/ondivPiwQho) / [论坛](http://forums.fast.ai/t/part-2-lesson-12-in-class/15023)'
- en: Very hot technology but definitely deserving to be in the cutting edge deep
    learning part of the course because they are not quite proven to be necessarily
    useful for anything but they are nearly there and will definitely get there. We
    are going to focus on the things where they are definitely going to be useful
    in practice and there is a number of areas where they may turn out to be useful
    but we don’t know yet. So I think the area that they are definitely going to be
    useful in practice is the kind of thing you see on the left of the slide — which
    is for example turning drawing into rendered pictures. This comes from [a paper
    that just came out 2 days ago](https://arxiv.org/abs/1804.04732), so there’s a
    very active research going on right now.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 非常炙手可热的技术，但绝对值得成为课程中前沿深度学习部分的一部分，因为它们并不完全被证明对任何事情都有用，但它们几乎到了那个地步，肯定会成功。我们将专注于那些在实践中肯定会有用的事情，有许多领域可能会被证明有用，但我们还不知道。所以我认为它们在实践中肯定会有用的领域是你在幻灯片左侧看到的那种东西
    —— 例如将绘画转化为渲染图片。这来自于[两天前刚刚发布的一篇论文](https://arxiv.org/abs/1804.04732)，所以目前正在进行非常活跃的研究。
- en: '**From the last lecture [**[**1:04**](https://youtu.be/ondivPiwQho?t=1m4s)**]:**
    One of our diversity fellows Christine Payne has a master’s in medicine from Stanford
    and so she had an interest in thinking what it would look like if we built a language
    model of medicine. One of the things we briefly touched on back in lesson 4 but
    didn’t really talk much about last time is this idea that you can actually seed
    a generative language model which means you’ve trained a language model on some
    corpus and then you are going to generate some text from that language model.
    You can start off by feeding it a few words to say “here is the first few words
    to create the hidden state in the language model and generate from there please.
    Christine did something clever which was to seed it with a question and repeat
    the question three times and let it generate from there. She fed a language model
    lots of different medical texts and fed in questions as you see below:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**从上一堂课 [**[**1:04**](https://youtu.be/ondivPiwQho?t=1m4s)**]：** 我们的多样性研究员
    Christine Payne 拥有斯坦福大学的医学硕士学位，因此她对构建医学语言模型感兴趣。我们在第四课中简要提到过的一件事，但上次并没有详细讨论的是，你实际上可以种子一个生成式语言模型，这意味着你已经在某个语料库上训练了一个语言模型，然后你将从该语言模型生成一些文本。你可以通过输入一些词来开始，告诉它“这是用来创建语言模型中隐藏状态的前几个词，请从这里生成”。Christine
    做了一些聪明的事情，她用一个问题作为种子，重复这个问题三次，然后让它从那里生成。她向语言模型输入了许多不同的医学文本，并输入了下面看到的问题：'
- en: What Jeremy found interesting about this is it’s pretty close to being a believable
    answer to the question for people without master’s in medicine. But it has no
    bearing on reality whatsoever. He thinks it is an interesting kind of ethical
    and user experience quandary. Jeremy is involved in a company called doc.ai that’s
    trying to doing a number of things but in the end provide an app for doctors and
    patients which can help create a conversational user interface around helping
    them with their medical issues. He’s been continually saying to the software engineers
    on that team please don’t try to create a generative model using LSTM or something
    because they are going to be really good at creating bad advice that sounds impressive
    — kind of like political pundits or tenured professor who can say bullcrap with
    great authority. So he thought it was really interesting experiment. If you’ve
    done some interesting experiments, share them in the forum, blog, Twitter. Let
    people know about it and get noticed by awesome people.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy 发现这个有趣的地方是，对于没有医学硕士学位的人来说，这几乎是一个可信的答案。但它与现实完全没有关系。他认为这是一种有趣的伦理和用户体验困境。Jeremy
    参与了一个名为 doc.ai 的公司，该公司试图做很多事情，但最终提供一个应用程序供医生和患者使用，可以帮助他们解决医疗问题。他一直在对团队中的软件工程师说，请不要尝试使用
    LSTM 或其他东西创建生成模型，因为它们会擅长创造听起来令人印象深刻但实际上是错误建议的东西 —— 就像政治评论员或终身教授可以以极大的权威说废话一样。所以他认为这是一个非常有趣的实验。如果你做了一些有趣的实验，请在论坛、博客、Twitter
    上分享。让人们知道并受到了了不起的人的关注。
- en: CIFAR10 [[5:26](https://youtu.be/ondivPiwQho?t=5m26s)]
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CIFAR10 [[5:26](https://youtu.be/ondivPiwQho?t=5m26s)]
- en: Let’s talk about CIFAR10 and the reason is that we are going to be looking at
    some more bare-bones PyTorch stuff today to build these generative adversarial
    models. There is no fastai support to speak up at all for GANs at the moment —
    there will be soon enough but currently there isn’t so we are going to be building
    a lot of models from scratch. It’s been a while since we’ve done much serious
    model building. We looked at CIFAR10 in the part 1 of the course and we built
    something which was getting about 85% accuracy and took a couple hours to train.
    Interestingly, there is a competition going on now to see who can actually train
    CIFAR10 the fastest ([DAWN](https://dawn.cs.stanford.edu/benchmark/#cifar10-train-time)),
    and the goal is to get it to train to 94% accuracy. It would be interesting to
    see if we can build an architecture that can get to 94% accuracy because that
    is a lot better than our previous attempt. Hopefully in doing so we will learn
    something about creating good architectures, that will be then useful for looking
    at GANs today. Also it is useful because Jeremy has been looking much more deeply
    into the last few years’ papers about different kinds of CNN architectures and
    realizes that a lot of the insights in those papers are not being widely leveraged
    and clearly not widely understood. So he wants to show you what happens if we
    can leverage so me of that understanding.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈CIFAR10，原因是今天我们将看一些更基础的PyTorch内容，以构建这些生成对抗模型。目前没有关于GAN的fastai支持 - 很快就会有，但目前还没有，所以我们将从头开始构建许多模型。我们已经有一段时间没有进行严肃的模型构建了。在课程的第一部分中，我们看了CIFAR10，并构建了一个准确率约为85%的模型，训练时间约为几个小时。有趣的是，现在正在进行一项竞赛，看看谁能最快地训练CIFAR10（[DAWN](https://dawn.cs.stanford.edu/benchmark/#cifar10-train-time)），目标是将准确率提高到94%。看看我们是否能构建一个能够达到94%准确率的架构，因为这比我们之前的尝试要好得多。希望通过这样做，我们将学到一些关于创建良好架构的东西，这对于今天研究GANs将会很有用。此外，这也很有用，因为Jeremy在过去几年深入研究了关于不同类型CNN架构的论文，并意识到这些论文中的许多见解并没有被广泛利用，显然也没有被广泛理解。因此，他想向您展示如果我们能利用其中一些理解会发生什么。
- en: '[cifar10-darknet.ipynb](https://github.com/fastai/fastai/blob/master/courses/dl2/cifar10-darknet.ipynb)
    [[7:17](https://youtu.be/ondivPiwQho?t=7m17s)]'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[cifar10-darknet.ipynb](https://github.com/fastai/fastai/blob/master/courses/dl2/cifar10-darknet.ipynb)
    [[7:17](https://youtu.be/ondivPiwQho?t=7m17s)]'
- en: The notebook is called [darknet](https://pjreddie.com/darknet/) because the
    particular architecture we are going to look at is very close to the darknet architecture.
    But you will see in the process that the darknet architecture as in not the whole
    YOLO v3 end-to-end thing but just the part of it that they pre-trained on ImageNet
    to do classification. It’s almost like the most generic simple architecture you
    could come up with, so it’s a really great starting point for experiments. So
    we will call it “darknet” but it’s not quite that and you can fiddle around with
    it to create things that definitely aren’t darknet. It’s really just the basis
    of nearly any modern ResNet based architecture.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个笔记本被称为[darknet](https://pjreddie.com/darknet/)，因为我们将要查看的特定架构与darknet架构非常接近。但在这个过程中，您会发现darknet架构并不是整个YOLO
    v3端到端的东西，而只是他们在ImageNet上预训练用于分类的部分。这几乎就像您可以想到的最通用的简单架构，因此它是实验的一个很好的起点。因此，我们将其称为“darknet”，但它并不完全是那样，您可以对其进行调整以创建绝对不是darknet的东西。它实际上只是几乎任何现代基于ResNet的架构的基础。
- en: 'CIFAR10 is a fairly small dataset [[8:06](https://youtu.be/ondivPiwQho?t=8m6s)].
    The images are only 32 by 32 in size, and it’s a great dataset to work with because:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR10是一个相当小的数据集[[8:06](https://youtu.be/ondivPiwQho?t=8m6s)]。图像大小仅为32x32，这是一个很好的数据集，因为：
- en: You can train it relatively quickly unlike ImageNet
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与ImageNet不同，您可以相对快速地对其进行训练
- en: A relatively small amount of data
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对较少的数据
- en: Actually quite hard to recognize the images because 32 by 32 is too small to
    easily see what’s going on.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际上很难识别这些图像，因为32x32太小了，很难看清楚发生了什么。
- en: It is an under-appreciated dataset because it’s old. Who wants to work with
    small old dataset when they could use their entire server room to process something
    much bigger. But it’s is a really great dataset to focus on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个被低估的数据集，因为它很老。谁愿意使用小而古老的数据集，当他们可以利用整个服务器房间来处理更大的数据时。但这是一个非常好的数据集，值得关注。
- en: Go ahead and import our usual stuff and we are going to try and build a network
    from scratch to train this with[[8:58](https://youtu.be/ondivPiwQho?t=8m58s)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 继续导入我们通常使用的东西，我们将尝试从头开始构建一个网络来训练这个[[8:58](https://youtu.be/ondivPiwQho?t=8m58s)]。
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A really good exercise for anybody who is not 100% confident with their broadcasting
    and PyTorch basic skill is figure out how Jeremy came up with these `stats` numbers.
    These numbers are the averages and standard deviations for each channel in CIFAR10\.
    Try and make sure you can recreate those numbers and see if you can do it with
    no more than a couple of lines of code (no loops!).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对广播和PyTorch基本技能不是100%自信的人来说，一个非常好的练习是弄清楚Jeremy是如何得出这些`stats`数字的。这些数字是CIFAR10中每个通道的平均值和标准差。尝试确保您可以重新创建这些数字，并查看是否可以在不超过几行代码的情况下完成（不使用循环！）。
- en: Because these are fairly small, we can use a larger batch size than usual and
    the size of these images is 32 [[9:46](https://youtu.be/ondivPiwQho?t=9m46s)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些数据相当小，我们可以使用比通常更大的批量大小，并且这些图像的大小为32[[9:46](https://youtu.be/ondivPiwQho?t=9m46s)]。
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Transformations [[9:57](https://youtu.be/ondivPiwQho?t=9m57s)], normally we
    have this standard set of side_on transformations we use for photos of normal
    objects. We are not going to use that here because these images are so small that
    trying to rotate a 32 by 32 image a bit is going to introduce a lot of blocky
    distortions. So the standard transformations that people tend to use is a random
    horizontal flip and then we add 4 pixels (size divided by 8) of padding on each
    side. One thing which works really well is by default fastai does not add black
    padding which many other libraries do. Fastai takes the last 4 pixels of the existing
    photo and flip it and reflect it, and we find that we get much better results
    by using reflection padding by default. Now that we have 40 by 40 image, this
    set of transforms in training will randomly pick a 32 by 32 crops, so we get a
    little bit of variation but not heaps. Wo we can use the normal `from_paths` to
    grab our data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 变换[[9:57](https://youtu.be/ondivPiwQho?t=9m57s)]，通常我们有这一套标准的side_on变换，用于普通物体的照片。我们不会在这里使用这个，因为这些图像太小了，尝试将一个32x32的图像稍微旋转会引入很多块状失真。人们倾向于使用的标准变换是随机水平翻转，然后我们在每一侧添加4个像素（尺寸除以8）的填充。一个非常有效的方法是，默认情况下fastai不会添加黑色填充，而许多其他库会这样做。Fastai会取现有照片的最后4个像素，翻转并反射它，我们发现使用反射填充会得到更好的结果。现在我们有了40x40的图像，在训练中，这组变换将随机选择32x32的裁剪，所以我们会有一点变化但不会太多。因此我们可以使用正常的`from_paths`来获取我们的数据。
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we need an architecture and we are going to create one which fits in one
    screen [[11:07](https://youtu.be/ondivPiwQho?t=11m7s)]. This is from scratch.
    We are using predefined `Conv2d`, `BatchNorm2d`, `LeakyReLU` modules but we are
    not using any blocks or anything. The entire thing is in one screen so if you
    are ever wondering can I understand a modern good quality architecture, absolutely!
    Let’s study this one.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一个架构，我们将创建一个适合在一个屏幕上显示的架构[[11:07](https://youtu.be/ondivPiwQho?t=11m7s)]。这是从头开始的。我们正在使用预定义的`Conv2d`，`BatchNorm2d`，`LeakyReLU`模块，但我们没有使用任何块或其他东西。整个东西都在一个屏幕上，所以如果你曾经想知道我是否能理解一个现代的高质量架构，绝对可以！让我们来学习这个。
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The basic starting point with an architecture is to say it’s a stacked bunch
    of layers and generally speaking there is going to be some kind of hierarchy of
    layers [[11:51](https://youtu.be/ondivPiwQho?t=11m51s)]. At the very bottom level,
    there is things like a convolutional layer and a batch norm layer, but any time
    you have a convolution, you are probably going to have some standard sequence.
    Normally it’s going to be:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 架构的基本起点是说它是一堆堆叠的层，一般来说会有一种层次结构[[11:51](https://youtu.be/ondivPiwQho?t=11m51s)]。在最底层，有像卷积层和批量归一化层这样的东西，但任何时候你有一个卷积，你可能会有一些标准的顺序。通常会是：
- en: conv
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积
- en: batch norm
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量归一化
- en: a nonlinear activation (e.g. ReLU)
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个非线性激活（例如ReLU）
- en: We will start by determining what our basic unit is going to be and define it
    in a function (`conv_layer`) so we don’t have to worry about trying to keep everything
    consistent and it will make everything a lot simpler.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从确定我们的基本单元是什么开始，并在一个函数（`conv_layer`）中定义它，这样我们就不必担心保持一致性，这将使一切变得更简单。
- en: '**Leaky Relu** [[12:43](https://youtu.be/ondivPiwQho?t=12m43s)]:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**Leaky Relu** [[12:43](https://youtu.be/ondivPiwQho?t=12m43s)]：'
- en: The gradient of Leaky ReLU (where *x* < 0) varies but something about 0.1 or
    0.01 is common. The idea behind it is that when you are in the negative zone,
    you don’t end up with a zero gradient which makes it very hard to update it. In
    practice, people have found Leaky ReLU more useful on smaller datasets and less
    useful in big datasets. But it is interesting that for the [YOLO v3](https://pjreddie.com/media/files/papers/YOLOv3.pdf)
    paper, they used Leaky ReLU and got great performance from it. It rarely makes
    things worse and it often makes things better. So it’s probably not bad if you
    need to create your own architecture to make that your default go-to is to use
    Leaky ReLU.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU的梯度（其中*x* < 0）会有所变化，但通常是0.1或0.01左右。其背后的想法是，当你处于负区域时，你不会得到一个零梯度，这会使更新变得非常困难。实践中，人们发现Leaky
    ReLU在较小的数据集上更有用，在大数据集上不太有用。但有趣的是，在[YOLO v3](https://pjreddie.com/media/files/papers/YOLOv3.pdf)论文中，他们使用了Leaky
    ReLU，并从中获得了很好的性能。它很少会使事情变得更糟，通常会使事情变得更好。所以如果你需要创建自己的架构，Leaky ReLU可能不错作为默认选择。
- en: You’ll notice that we don’t define PyTorch module in `conv_layer`, we just do
    `nn.Sequential` [[14:07](https://youtu.be/ondivPiwQho?t=14m7s)]. This is something
    if you read other people’s PyTorch code, it’s really underutilized. People tend
    to write everything as a PyTorch module with `__init__` and `forward`, but if
    the thing you want is just a sequence of things one after the other, it’s much
    more concise and easy to understand to make it a `Sequential`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们在`conv_layer`中没有定义PyTorch模块，我们只是使用`nn.Sequential`[[14:07](https://youtu.be/ondivPiwQho?t=14m7s)]。如果你阅读其他人的PyTorch代码，你会发现这是一个被低估的东西。人们倾向于将一切都写成PyTorch模块，带有`__init__`和`forward`，但如果你想要的只是一系列按顺序排列的东西，将其作为`Sequential`会更简洁易懂。
- en: '**Residual block** [[14:40](https://youtu.be/ondivPiwQho?t=14m40s)]: As mentioned
    before that there is generally a number of hierarchies of units in most modern
    networks, and we know now that the next level in this unit hierarchy for ResNet
    is the ResBlock or residual block (see `ResLayer`). Back when we last did CIFAR10,
    we oversimplified this (cheated a little bit). We had `x` coming in and we put
    that through a `conv`, then we added it back up to `x` to go out. In the real
    ResBlock, there are two of them. When we say “conv” we are using it as a shortcut
    for our `conv_layer` (conv, batch norm, ReLU).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**残差块** [[14:40](https://youtu.be/ondivPiwQho?t=14m40s)]：如前所述，大多数现代网络中通常有多个层次的单元，我们现在知道ResNet中这个单元层次结构的下一个级别是ResBlock或残差块（参见`ResLayer`）。回顾我们上次做CIFAR10时，我们过于简化了（有点作弊）。我们将`x`输入，经过一个`conv`，然后将其加回到`x`中输出。在真正的ResBlock中，有两个这样的块。当我们说“conv”时，我们将其作为`conv_layer`的快捷方式（卷积，批量归一化，ReLU）。'
- en: One interesting insight here is the number of channels in these convolutions
    [[16:47](https://youtu.be/ondivPiwQho?t=16m47s)]. We have some `ni` coming in
    (some number of input channels/filters). The way the darknet folks set things
    up is they make every one of these Res layers spit out the same number of channels
    that came in, and Jeremy liked that and that’s why he used it in `ResLayer` because
    it makes life simpler. The first conv halves the number of channels, and then
    second conv doubles it again. So you have this funneling effect where 64 channels
    coming in, squished down with a first conv down to 32 channels, and then taken
    back up again to 64 channels coming out.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的观点是这些卷积中的通道数量。我们有一些ni进来（一些输入通道/滤波器的数量）。Darknet团队设置的方式是，他们让每一个这些Res层输出与进来的相同数量的通道，Jeremy喜欢这样做，这就是为什么他在`ResLayer`中使用它，因为这样会让生活更简单。第一个卷积将通道数量减半，然后第二个卷积再将其加倍。所以你有一个漏斗效应，64个通道进来，通过第一个卷积压缩到32个通道，然后再次提升到64个通道输出。
- en: '**Question:** Why is `inplace=True` in the `LeakyReLU` [[17:54](https://youtu.be/ondivPiwQho?t=17m54s)]?
    Thanks for asking! A lot of people forget this or don’t know about it, but this
    is a really important memory technique. If you think about it, this `conv_layer`,
    it’s the lowest level thing, so pretty much everything in our ResNet once it’s
    all put together is going to be many `conv_layer`’s. If you do not have `inplace=True`,
    it’s going to create a whole separate piece of memory for the output of the ReLU
    so it’s going to allocate a whole bunch of memory that is totally unnecessary.
    Another example is that the original `forward` in `ResLayer` looked like:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：为什么`LeakyReLU`中要使用`inplace=True`？谢谢你的提问！很多人忘记了这一点或者不知道这一点，但这是一个非常重要的内存技巧。如果你想一下，这个`conv_layer`，它是最底层的东西，所以基本上我们的ResNet一旦全部组装起来，就会有很多`conv_layer`。如果你没有`inplace=True`，它会为ReLU的输出创建一个完全独立的内存块，这样就会分配一大堆完全不必要的内存。另一个例子是`ResLayer`中的原始`forward`看起来像这样：'
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Hopefully some of you might remember that in PyTorch pretty much every every
    function has an underscore suffix version which tells it to do it in-place. `+`
    is equivalent to `add` and in-place version of `add` is `add_` so this will reduce
    memory usage:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你们中的一些人记得在PyTorch中几乎每个函数都有一个下划线后缀版本，告诉它在原地执行。`+`等同于`add`，`add`的原地版本是`add_`，这样可以减少内存使用量：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These are really handy little tricks. Jeremy forgot the `inplace=True` at first
    but he was having to decrease the batch size to much lower amounts and it was
    driving him crazy — then he realized that that was missing. You can also do that
    with dropout if you have dropout. Here are what to look out for:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是非常方便的小技巧。Jeremy一开始忘记了`inplace=True`，但他不得不将批量大小降低到非常低的数量，这让他发疯了——然后他意识到那个部分缺失了。如果你使用了dropout，你也可以这样做。以下是需要注意的事项：
- en: Dropout
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: All the activation functions
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有激活函数
- en: Any arithmetic operation
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何算术操作
- en: '**Question**: In ResNet, why is bias usually set to False in conv_layer [[19:53](https://youtu.be/ondivPiwQho?t=19m53s)]?
    Immediately after the `Conv`, there is a `BatchNorm`. Remember, `BatchNorm` has
    2 learnable parameters for each activation — the thing you multiply by and the
    thing you add. If we had bias in `Conv` and then add another thing in `BatchNorm`,
    we would be adding two things which is totally pointless — that’s two weights
    where one would do. So if you have a BatchNorm after a `Conv`, you can either
    tell `BatchNorm` not to include the add bit or easier is to tell `Conv` not to
    include the bias. There is no particular harm, but again, it’s going to take more
    memory because that is more gradients that it has to keep track of, so best to
    avoid.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：在ResNet中，为什么`conv_layer`中的偏置通常设置为False？在`Conv`之后，紧接着是`BatchNorm`。记住，`BatchNorm`对于每个激活有2个可学习参数——你要乘以的东西和你要添加的东西。如果我们在`Conv`中有偏置，然后在`BatchNorm`中再添加另一件事，那就是在添加两件事，这完全没有意义——这是两个权重，一个就够了。所以如果在`Conv`之后有一个`BatchNorm`，你可以告诉`BatchNorm`不要包括添加部分，或者更简单的方法是告诉`Conv`不要包括偏置。这没有特别的危害，但是会占用更多内存，因为它需要跟踪更多的梯度，所以最好避免。'
- en: Also another little trick is, most people’s `conv_layer`’s have padding as a
    parameter [[21:11](https://youtu.be/ondivPiwQho?t=21m11s)]. But generally speaking,
    you should be able to calculate the padding easily enough. If you have a kernel
    size of 3, then obviously that is going to overlap by one unit on each side, so
    we want padding of 1\. Or else, if it’s kernel size of 1, then we don’t need any
    padding. So in general, padding of kernel size “integer divided” by 2 is what
    you need. There’re some tweaks sometimes but in this case, this works perfectly
    well. Again, trying to simplify my code by having the computer calculate stuff
    for me rather than me having to do it myself.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个小技巧是，大多数人的`conv_layer`都有填充作为参数。但一般来说，你应该能够很容易地计算填充。如果卷积核大小为3，那么显然每边会有一个单位的重叠，所以我们需要填充1。或者，如果卷积核大小为1，那么我们就不需要任何填充。所以一般来说，卷积核大小“整数除以2”就是你需要的填充。有时会有一些调整，但在这种情况下，这个方法非常有效。再次尝试简化我的代码，让计算机为我计算东西，而不是我自己去做。
- en: 'Another thing with the two `conv_layer`’s [[22:14](https://youtu.be/ondivPiwQho?t=22m14s)]:
    We had this idea of bottleneck (reducing the channels and then increase them again),
    there is also what kernel size to use. The first one has 1 by 1 `Conv`. What actually
    happen in 1 by 1 conv? If we have 4 by 4 grid with 32 filters/channels and we
    will do 1 by 1 conv, the kernel for the conv looks like the one in the middle.
    When we talk about the kernel size, we never mention the last piece — but let’s
    say it’s 1 by 1 by 32 because that’s the part of the filters in and filters out.
    The kernel gets placed on the first cell in yellow and we get a dot product these
    32 deep bits which gives us our first output. We then move it to the second cell
    and get the second output. So there will be bunch of dot products for each point
    in the grid. It is allowing us to change the dimensionality in whatever way we
    want in the channel dimension. We are creating `ni//2` filters and we will have
    `ni//2` dot products which are basically different weighted averages of the input
    channels. With very little computation, it lets us add this additional step of
    calculations and nonlinearities. It is a cool trick to take advantage of these
    1 by 1 convs, creating this bottleneck, and then pulling it out again with 3 by
    3 convs — which will take advantage of the 2D nature of the input properly. Or
    else, 1 by 1 conv doesn’t take advantage of that at all.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关于这两个`conv_layer`的事情：我们有这个瓶颈的想法（减少通道然后再增加），还有要使用的卷积核大小。第一个有1乘1的`Conv`。1乘1卷积实际上发生了什么？如果我们有一个4乘4的网格，有32个滤波器/通道，我们将进行1乘1卷积，卷积的核看起来像中间的那个。当我们谈论卷积核大小时，我们从来没有提到最后一部分——但假设它是1乘1乘32，因为这是输入和输出的滤波器的一部分。卷积核被放在黄色的第一个单元上，我们得到这32个深度位的点积，这给了我们第一个输出。然后我们将其移动到第二个单元并得到第二个输出。所以对于网格中的每个点，都会有一堆点积。这使我们能够以任何我们想要的方式改变通道维度。我们创建了`ni//2`个滤波器，我们将有`ni//2`个点积，基本上是输入通道的不同加权平均值。通过非常少的计算，它让我们添加了这个额外的计算和非线性步骤。这是一个很酷的技巧，利用这些1乘1卷积，创建这个瓶颈，然后再用3乘3卷积拉出来——这将充分利用输入的2D特性。否则，1乘1卷积根本不利用这一点。
- en: These two lines of code, there is not much in it, but it’s a really great test
    of your understanding and intuition about what is going on [[25:17](https://youtu.be/ondivPiwQho?t=25m17s)]
    — why does it work? why do the tensor ranks line up? why do the dimensions all
    line up nicely? why is it a good idea? what is it really doing? It’s a really
    good thing to fiddle around with. Maybe create some small ones in Jupyter Notebook,
    run them yourself, see what inputs and outputs come in and out. Really get a feel
    for that. Once you’ve done so, you can then play around with different things.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这两行代码，里面没有太多内容，但这是一个对你对正在发生的事情的理解和直觉的很好的测试——为什么它有效？为什么张量秩是对齐的？为什么维度都很好地对齐？为什么这是一个好主意？它到底在做什么？这是一个很好的东西来调整。也许在Jupyter
    Notebook中创建一些小的实例，自己运行一下，看看输入和输出是什么。真正感受一下。一旦你这样做了，你就可以尝试不同的东西。
- en: 'One of the really unappreciated papers is this one [[26:09](https://youtu.be/ondivPiwQho?t=26m9s)]
    — [Wide Residual Networks](https://arxiv.org/abs/1605.07146). It’s really quite
    simple paper but what they do is they fiddle around with these two lines of code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇真正被低估的论文是这篇[Wide Residual Networks](https://arxiv.org/abs/1605.07146)。这篇论文非常简单，但他们做的是围绕这两行代码进行调整：
- en: What is we did `ni*2` instead of `ni//2`?
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们用`ni*2`代替`ni//2`会怎样？
- en: What if we added `conv3`?
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们添加`conv3`呢？
- en: 'They come up with this kind of simple notation for defining what the two lines
    of code can look like and they show lots of experiments. What they show is that
    this approach of a bottlenecking of decreasing the number of channels which is
    almost universal in ResNet is probably not a good idea. In fact, from the experiments,
    definitely not a good idea. Because what happens is it lets you create really
    deep networks. The guys who created ResNet got particularly famous for creating
    1001 layer network. But the thing about 1001 layers is you can’t calculate layer
    2 until you are finished layer 1\. You can’t calculate layer 3 until you finish
    calculating layer 2\. So it’s sequential. GPUs don’t like sequential. So what
    they showed is that if you have less layers but with more calculations per layer
    — so one easy way to do that would be to remove `//2`, no other changes:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出了一种简单的符号表示来定义这两行代码可能的样子，并展示了许多实验。他们展示的是，在ResNet中普遍采用的减少通道数量的瓶颈方法可能不是一个好主意。实际上，根据实验结果，绝对不是一个好主意。因为这样可以创建非常深的网络。创建ResNet的人因为创建了1001层网络而变得特别有名。但是1001层的问题在于，你无法在完成第1层之前计算第2层。你无法在完成计算第2层之前计算第3层。所以是顺序的。GPU不喜欢顺序。所以他们展示的是，如果层数较少但每层计算量更大——一个简单的方法是去掉`//2`，没有其他改变：
- en: Try this at home. Try running CIFAR and see what happens. Even multiply by 2
    or fiddle around. That lets your GPU do more work and it’s very interesting because
    the vast majority of papers that talk about performance of different architectures
    never actually time how long it takes to run a batch through it. They say “this
    one takes X number of floating-point operations per batch” but they never actually
    bother to run it like a proper experimentalists and find out whether it’s faster
    or slower. A lot of the architectures that are really famous now turn out to be
    slow as molasses and take crap loads of memory and just totally useless because
    the researchers never actually bothered to see whether they are fast and to actually
    see whether they fit in RAM with normal batch sizes. So Wide ResNet paper is unusual
    in that it actually times how long it takes as does the YOLO v3 paper which made
    the same insight. They might have missed the Wide ResNet paper because the YOLO
    v3 paper came to a lot of the same conclusions but Jeremy is not sure they sited
    the Wide ResNet paper so they might not be aware that all that work has been done.
    It’s great to see people are actually timing things and noticing what actually
    makes sense.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在家里试试吧。尝试运行CIFAR看看会发生什么。甚至乘以2或者摆弄一下。这样可以让你的GPU做更多的工作，这非常有趣，因为绝大多数关于不同架构性能的论文实际上从来没有计算运行一个批次需要多长时间。他们说“这个需要每批次X个浮点运算”，但他们从来没有真正费心像一个合格的实验者那样运行它，找出它是快还是慢。现在很有名的很多架构结果都很慢，占用大量内存，完全没用，因为研究人员从来没有费心看看它们是否快，实际上看看它们是否适合正常批次大小的内存。所以Wide
    ResNet论文之所以不同在于它实际上计算了运行所需的时间，YOLO v3论文也做了同样的发现。他们可能错过了Wide ResNet论文，因为YOLO v3论文得出了很多相同的结论，但Jeremy不确定他们是否引用了Wide
    ResNet论文，所以他们可能不知道所有这些工作已经完成。看到人们实际上在计时并注意到什么是有意义的是很好的。
- en: '**Question**: What is your opinion on SELU (scaled exponential linear units)?
    [[29:44](https://youtu.be/ondivPiwQho?t=29m44s)] SELU is largely for fully connected
    layers which allows you to get rid of batch norm and the basic idea is that if
    you use this different activation function, it’s self normalizing. Self normalizing
    means it will always remain at a unit standard deviation and zero mean and therefore
    you don’t need batch norm. It hasn’t really gone anywhere and the reason is because
    it’s incredibly finicky — you have to use a very specific initialization otherwise
    it doesn’t start with exactly the right standard deviation and mean. Very hard
    to use it with things like embeddings, if you do then you have to use a particular
    kind of embedding initialization which doesn’t make sense for embeddings. And
    you do all this work, very hard to get it right, and if you do finally get it
    right, what’s the point? Well, you’ve managed to get rid of some batch norm layers
    which weren’t really hurting you anyway. It’s interesting because the SELU paper
    — the main reason people noticed it was because it was created by the inventor
    of LSTM and also it had a huge mathematical appendix. So people thought “lots
    of maths from a famous guy — it must be great!” but in practice, Jeremy doesn’t
    see anybody using it to get any state-of-the-art results or win any competitions.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：你对SELU（缩放指数线性单元）有什么看法？[[29:44](https://youtu.be/ondivPiwQho?t=29m44s)]
    SELU主要用于全连接层，它允许你摆脱批量归一化，基本思想是，如果你使用这种不同的激活函数，它是自归一化的。自归一化意味着它将始终保持单位标准差和零均值，因此你不需要批量归一化。它实际上并没有取得什么进展，原因是因为它非常挑剔
    — 你必须使用非常特定的初始化，否则它就不会以完全正确的标准差和均值开始。很难将其用于诸如嵌入之类的东西，如果你这样做，那么你必须使用一种特定类型的嵌入初始化，这对嵌入来说是没有意义的。你做了所有这些工作，很难搞对，最终如果你搞对了，有什么意义呢？好吧，你成功摆脱了一些并没有真正伤害你的批量归一化层。有趣的是SELU论文
    — 人们注意到它的主要原因是因为它是由LSTM的发明者创建的，而且它有一个巨大的数学附录。所以人们认为“一个名人的大量数学 — 必定很棒！”但实际上，Jeremy没有看到任何人使用它来获得任何最先进的结果或赢得任何比赛。'
- en: '`Darknet.make_group_layer` contains a bunch of `ResLayer` [[31:28](https://youtu.be/ondivPiwQho?t=31m28s)].
    `group_layer` is going to have some number of channels/filters coming in. We will
    double the number of channels coming in by just using the standard `conv_layer`.
    Optionally, we will halve the grid size by using a stride of 2\. Then we are going
    to do a whole bunch of ResLayers — we can pick how many (2, 3, 8 etc) because
    remember ResLayers do not change the grid size and they don’t change the number
    of channels, so you can add as many as you like without causing any problems.
    This is going to use more computation and more RAM but there is no reason other
    than that you can’t add as many as you like. `group_layer`, therefore, is going
    to end up doubling the number of channels because the initial convolution doubles
    the number of channels and depending on what we pass in a `stride`, it may also
    halve the grid size if we put `stride=2`. And then we can do a whole bunch of
    Res block computations as many as we like.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`Darknet.make_group_layer`包含一堆`ResLayer`[[31:28](https://youtu.be/ondivPiwQho?t=31m28s)]。`group_layer`将会有一些通道/滤波器进入。我们将通过使用标准的`conv_layer`来使进入的通道数量加倍。可选地，我们将通过使用步幅为2来减半网格大小。然后我们将做一系列的ResLayers
    — 我们可以选择多少个（2、3、8等），因为记住ResLayers不会改变网格大小，也不会改变通道数量，所以你可以添加任意数量而不会造成任何问题。这将使用更多的计算和内存，但除此之外你可以添加任意数量。因此，`group_layer`最终将使通道数量加倍，因为初始卷积使通道数量加倍，取决于我们传入的`stride`，如果我们设置`stride=2`，它也可能减半网格大小。然后我们可以做一系列Res块的计算，任意数量。'
- en: 'To define our `Darknet`, we are going to pass in something that looks like
    this [[33:13](https://youtu.be/ondivPiwQho?t=33m13s)]:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们的`Darknet`，我们将传入类似这样的东西[[33:13](https://youtu.be/ondivPiwQho?t=33m13s)]：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'What this says is create five group layers: the first one will contain 1 extra
    ResLayer, the second will contain 2, then 4, 6, 3 and we want to start with 32
    filters. The first one of ResLayers will contain 32 filters, and there’ll just
    be one extra ResLayer. The second one, it’s going to double the number of filters
    because that’s what we do each time we have a new group layer. So the second one
    will have 64, and then 128, 256, 512 and that’ll be it. Nearly all of the network
    is going to be those bunches of layers and remember, every one of those group
    layers also has one convolution at the start. So then all we have is before that
    all happens, we are going to have one convolutional layer at the very start, and
    at the very end we are going to do our standard adaptive average pooling, flatten,
    and a linear layer to create the number of classes out at the end. To summarize
    [[34:44](https://youtu.be/ondivPiwQho?t=34m44s)], one convolution at one end,
    adaptive pooling and one linear layer at the other end, and in the middle, these
    group layers each one consisting of a convolutional layer followed by `n` number
    of ResLayers.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着创建五个组层：第一个将包含1个额外的ResLayer，第二个将包含2个，然后是4个，6个，3个，我们希望从32个滤波器开始。第一个ResLayers将包含32个滤波器，只会有一个额外的ResLayer。第二个将会使滤波器数量翻倍，因为每次有一个新的组层时我们都会这样做。所以第二个将有64个，然后128个，256个，512个，就这样。几乎整个网络将由这些层组成，记住，每个组层在开始时也有一个卷积。所以在这之前，我们将在一开始有一个卷积层，在最后我们将执行标准的自适应平均池化，展平，并在最后创建一个线性层来生成最终的类别数量。总结一下，一个端有一个卷积，自适应池化和另一个端有一个线性层，中间是这些组层，每个组层由一个卷积层和`n`个ResLayers组成。
- en: '**Adaptive average pooling** [[35:02](https://youtu.be/ondivPiwQho?t=35m2s)]:
    Jeremy’s mentioned this a few times, but he’s yet to see any code out there, any
    example, anything anywhere, that uses adaptive average pooling. Every one he’s
    seen writes it like `nn.AvgPool2d(n)` where `n` is a particular number — this
    means that it’s now tied to a particular image size which definitely isn’t what
    you want. So most people are still under the impression that a specific architecture
    is tied to a specific size. That’s a huge problem when people think that because
    it really limits their ability to use smaller sizes to kick-start their modeling
    or to use smaller size for doing experiments.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应平均池化**：Jeremy多次提到过这个，但他还没有看到任何代码，任何示例，任何地方使用自适应平均池化。他看到的每一个都像`nn.AvgPool2d(n)`这样写，其中`n`是一个特定的数字-这意味着它现在与特定的图像大小绑定在一起，这绝对不是您想要的。所以大多数人仍然认为特定的架构与特定的大小绑定在一起。当人们认为这样时，这是一个巨大的问题，因为这会严重限制他们使用更小的尺寸来启动建模或使用更小的尺寸进行实验的能力。'
- en: '**Sequential** [[35:53](https://youtu.be/ondivPiwQho?t=35m53s)]: A nice way
    to create architectures is to start out by creating a list, in this case this
    is this is a list with just one `conv_layer` in, and `make_group_layer` returns
    another list. Then we can append that list to the previous list with `+=` and
    do the same for another list containing `AdaptiveAvnPool2d`. Finally we will call
    `nn.Sequential` of all those layers. Now the `forward` is just `self.layers(x)`.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sequential**：创建架构的一个好方法是首先创建一个列表，在这种情况下，这是一个只有一个`conv_layer`的列表，然后`make_group_layer`返回另一个列表。然后我们可以用`+=`将该列表附加到前一个列表中，并对包含`AdaptiveAvnPool2d`的另一个列表执行相同操作。最后，我们将调用所有这些层的`nn.Sequential`。现在`forward`只是`self.layers(x)`。'
- en: This is a nice picture of how to make your architectures as simple as possible.
    There are a lot you can fiddle around with. You can parameterize the divider of
    `ni` to make it a number that you pass in to pass in different numbers- maybe
    do times 2 instead. You can also pass in things that change the kernel size, or
    change the number of convolutional layers. Jeremy has a version of this which
    he is going to run for you which implements all of the different parameters that
    were in the Wide ResNet paper, so he could fiddle around to see what worked well.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何使您的架构尽可能简单的好方法。有很多可以摆弄的地方。您可以将`ni`的除数参数化，使其成为您传入的数字，以传入不同的数字-也许是乘以2。您还可以传入一些可以改变内核大小或改变卷积层数量的参数。Jeremy有一个版本，他将为您运行，其中实现了Wide
    ResNet论文中的所有不同参数，因此他可以摆弄看看哪些效果好。
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once we’ve got that, we can use `ConvLearner.from_model_data` to take our PyTorch
    module and a model data object, and turn them into a learner [[37:08](https://youtu.be/ondivPiwQho?t=37m8s)].
    Give it a criterion, add a metrics if we like, and then we can fit and away we
    go.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个，我们可以使用`ConvLearner.from_model_data`来获取我们的PyTorch模块和模型数据对象，并将它们转换为一个学习器。给它一个标准，如果我们喜欢，可以添加一个指标，然后我们可以拟合并开始。
- en: '**Question**: Could you please explain adaptive average pooling? How does setting
    to 1 work [[37:25](https://youtu.be/ondivPiwQho?t=37m25s)]? Sure. Normally when
    we are doing average pooling, let’s say we have 4x4 and we did `avgpool((2, 2))`
    [[40:35](https://youtu.be/ondivPiwQho?t=40m35s)]. That creates 2x2 area (blue
    in the below) and takes the average of those four. If we pass in `stride=1`, the
    next one is 2x2 shown in green and take the average. So this is what a normal
    2x2 average pooling would be. If we didn’t have any padding, that would spit out
    3x3\. If we wanted 4x4, we can add padding.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：您能解释一下自适应平均池化吗？将其设置为1是如何工作的？当我们进行平均池化时，通常情况下，假设我们有4x4，然后进行`avgpool((2,
    2))`。这将创建一个2x2的区域（下方的蓝色），并取这四个的平均值。如果我们传入`stride=1`，下一个是2x2（绿色），然后取平均值。这就是正常的2x2平均池化。如果我们没有填充，那么输出将是3x3。如果我们想要4x4，我们可以添加填充。'
- en: What if we wanted 1x1? Then we could say `avgpool((4,4), stride=1)` that would
    do 4x4 in yellow and average the whole lot which results in 1x1\. But that’s just
    one way to do it. Rather than saying the size of the pooling filter, why don’t
    we instead say “I don’t care what the size of the input grid is. I always want
    one by one”. That’s where you say `adap_avgpool(1)`. In this case, you don’t say
    what’s the size of the pooling filter, you instead say what the size of the output
    we want. We want something that’s one by one. If you put a single integer `n`,
    it assumes you mean `n` by `n`. In this case, adaptive average pooling 1 with
    a 4x4 grid coming in is the same as average pooling (4, 4). If it was 7x7 grid
    coming in, it would be the same as average pooling (7, 7). It is the same operation,
    it’s just expressing it in a way that regardless of the input, we want something
    of that sized output.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要1x1呢？那么我们可以说`avgpool((4,4), stride=1)`，这将在黄色中进行4x4并对整体进行平均，结果为1x1。但这只是一种方法。与其说池化滤波器的大小，为什么不说“我不在乎输入网格的大小。我总是想要一个一个”。这就是你说`adap_avgpool(1)`的地方。在这种情况下，你不说池化滤波器的大小，而是说我们想要的输出大小。我们想要的是一个一个。如果你放一个单独的整数`n`，它会假设你的意思是`n`乘以`n`。在这种情况下，一个4x4网格的自适应平均池化与平均池化(4,4)相同。如果是一个7x7的网格进来，它将与平均池化(7,7)相同。这是相同的操作，只是以一种方式表达，无论输入是什么，我们都希望得到那个大小的输出。
- en: '**DAWNBench** [[37:43](https://youtu.be/ondivPiwQho?t=37m43s)]: Let’s see how
    we go with our simple network against these state-of-the-art results. Jeremy has
    the command ready to go. We’ve taken all that stuff and put it into a simple Python
    script, and he modified some of the parameters he mentioned to create something
    he called `wrn_22` network which doesn’t officially exist but it has a bunch of
    changes to the parameters we talked about based on Jeremy’s experiments. It has
    bunch of cool stuff like:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**DAWNBench**：让我们看看我们的简单网络与这些最新技术结果相比如何。Jeremy已经准备好命令了。我们已经将所有这些东西放入一个简单的Python脚本中，他修改了一些他提到的参数，创建了一个他称之为`wrn_22`网络，它并不存在，但根据Jeremy的实验，它对我们讨论的参数进行了一些改变。它有一堆很酷的东西，比如：'
- en: Leslie Smith’s one cycle
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 莱斯利·史密斯的一个周期
- en: Half-precision floating-point implementation
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半精度浮点实现
- en: This is going to run on AWS p3 which has 8 GPUs and Volta architecture GPUs
    which have special support for half-precision floating-point. Fastai is the first
    library to actually integrate the Volta optimized half-precision floating-point
    into the library, so you can just do `learn.half()` and get that support automatically.
    And it’s also the first to integrate one cycle.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在AWS p3上运行，它有8个GPU和Volta架构的GPU，这些GPU对半精度浮点有特殊支持。Fastai是第一个实际将Volta优化的半精度浮点集成到库中的库，所以你只需`learn.half()`就可以自动获得支持。它也是第一个集成一个周期的库。
- en: What this actually does is it’s using PyTorch’s multi-GPU support [[39:35](https://youtu.be/ondivPiwQho?t=39m35s)].
    Since there are eight GPUs, it is actually going to fire off eight separate Python
    processors and each one is going to train on a little bit and then at the end
    it’s going to pass the gradient updates back to the master process that is going
    to integrate them all together. So you will see lots of progress bars pop up together.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这是使用PyTorch的多GPU支持。由于有八个GPU，它实际上会启动八个单独的Python处理器，每个处理器都会训练一点，然后最后将梯度更新传回主进程，主进程将把它们全部整合在一起。所以你会看到很多进度条一起弹出。
- en: You can see it’s training three or four seconds when you do it this way. Where
    else, when Jeremy was training earlier, he was getting 30 seconds per epoch. So
    doing it this way, we can train things ~10 times faster which is pretty cool.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这种方式训练三到四秒。而在之前，当Jeremy早些时候训练时，他每个时代要花30秒。所以用这种方式，我们可以训练东西大约快10倍，这很酷。
- en: '**Checking on the status** [[43:19](https://youtu.be/ondivPiwQho?t=43m19s)]:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**检查状态**：'
- en: It’s done! We got to 94% and it took 3 minutes and 11 seconds. Previous state-of-the-art
    was 1 hour 7 minutes. Was it worth fiddling around with those parameters and learning
    a little bit about how these architectures actually work and not just using what
    came out of the box? Well, holy crap. We just used a publicly available instance
    (we used a spot instance so it costs us $8 per hour — for 3 minutes 40 cents)
    to train this from scratch 20 times faster than anybody has ever done it before.
    So that is one of the craziest state-of-the-art result. We’ve seen many but this
    one just blew it out of the water. This is partly thanks to fiddling around with
    those parameters of the architecture, mainly frankly about using Leslie Smith’s
    one cycle. Reminder of what it is doing [[44:35](https://youtu.be/ondivPiwQho?t=44m35s)],
    for learning rate, it creates upward path that is equally long as the downward
    path so it’s true triangular cyclical learning rate (CLR). As per usual, you can
    pick the ratio of x and y (i.e. starting LR / peak LR). In
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！我们达到了94%，用时3分11秒。之前的最新技术是1小时7分钟。折腾这些参数，学习这些架构实际上是如何工作的，而不仅仅是使用开箱即用的东西，值得吗？哇哦。我们刚刚使用了一个公开可用的实例（我们使用了一个spot实例，所以花费了我们每小时8美元——3分钟40美分）来从头开始训练，比以往任何人都要快20倍。所以这是最疯狂的最新技术结果之一。我们看到了很多，但这个结果真的让人大吃一惊。这在很大程度上要归功于调整这些架构参数，主要是关于使用莱斯利·史密斯的一个周期。提醒一下它在做什么，对于学习率，它创建了一个向上的路径，与向下的路径一样长，所以它是真正的三角形循环学习率（CLR）。像往常一样，你可以选择x和y的比例（即起始LR/峰值LR）。在
- en: In this case, we picked 50 for the ratio. So we started out with much smaller
    learning rate. Then it has this cool idea where you get to say what percentage
    of your epochs is spent going from the bottom of the triangle all the way down
    pretty much to zero — that is the second number. So 15% of the batches are spent
    going from the bottom of our triangle even further.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们选择了50作为比率。所以我们从更小的学习率开始。然后它有一个很酷的想法，你可以说你的epochs的百分之几是从三角形底部一直下降到几乎为零
    - 这是第二个数字。所以15%的批次花在从我们的三角形底部进一步下降。
- en: That is not the only thing one cycle does, we also have momentum. Momentum goes
    from .95 to .85\. In other words, when learning rate is really low, we use a lot
    of momentum and when the learning rate is really high, we use very little momentum
    which makes a lot of sense but until Leslie Smith showed this in the paper, Jeremy
    has never seen anybody do it before. It’s a really cool trick. You can now use
    that by using `use-clr-beta` parameter in fastai ([forum post by Sylvain](http://forums.fast.ai/t/using-use-clr-beta-and-new-plotting-tools/14702))
    and you should be able to replicate the state-of-the-art result. You can use it
    on your own computer or your paper space, the only thing you won’t get is the
    multi-GPU piece, but that makes it a bit easier to train anyway.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个周期所做的唯一事情，我们还有动量。动量从0.95到0.85。换句话说，当学习率很低时，我们使用很大的动量，当学习率很高时，我们使用很少的动量，这很有道理，但在Leslie
    Smith在论文中展示之前，Jeremy从未见过有人这样做。这是一个非常酷的技巧。你现在可以通过在fastai中使用`use-clr-beta`参数来使用它（Sylvain的论坛帖子），你应该能够复制最先进的结果。你可以在自己的计算机上或者paper
    space上使用它，唯一得不到的是多GPU部分，但这样训练会更容易一些。
- en: '**Question**: `make_group_layer` contains stride equals 2, so this means stride
    is one for layer one and two for everything else. What is the logic behind it?
    Usually the strides I have seen are odd [[46:52](https://youtu.be/ondivPiwQho?t=46m52s)].
    Strides are either one or two. I think you are thinking of kernel sizes. So stride=2
    means that I jump two across which means that you halve your grid size. So I think
    you might have got confused between stride and kernel size there. If you have
    a stride of one, the grid size does not change. If you have a stride of two, then
    it does. In this case, because this is CIFAR10, 32 by 32 is small and we don’t
    get to halve the grid size very often because pretty quickly we are going to run
    out of cells. So that is why the first layer has a stride of one so we don’t decrease
    the grid size straight away. It is kind of a nice way of doing it because that’s
    why we have a low number at first `Darknet([1, 2, 4, 6, 3], …)` . We can start
    out with not too much computation on the big grid, and then we can gradually doing
    more and more computation as the grids get smaller and smaller because the smaller
    grid the computation will take less time'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：`make_group_layer`包含步幅等于2，这意味着第一层的步幅为1，其他所有层的步幅为2。背后的逻辑是什么？通常我见过的步幅是奇数。步幅要么是1，要么是2。我认为你在考虑卷积核大小。所以步幅=2意味着我跨越两个，这意味着你的网格大小减半。所以我认为你可能在步幅和卷积核大小之间混淆了。如果步幅为1，网格大小不会改变。如果步幅为2，那么会改变。在这种情况下，因为这是CIFAR10，32x32很小，我们不会经常减半网格大小，因为很快我们就会用完单元格。这就是为什么第一层的步幅为1，这样我们不会立即减小网格大小。这是一种很好的做法，因为这就是为什么我们一开始在大网格上没有太多计算`Darknet([1,
    2, 4, 6, 3], …)`。我们可以从大网格上开始，然后随着网格变小，逐渐增加更多的计算，因为网格越小，计算所需的时间就越少。'
- en: Generative Adversarial Networks (GAN) [[48:49](https://youtu.be/ondivPiwQho?t=48m49s)]
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）[[48:49](https://youtu.be/ondivPiwQho?t=48m49s)]
- en: '[Wasserstein GAN](https://arxiv.org/abs/1701.07875)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[沃瑟斯坦GAN](https://arxiv.org/abs/1701.07875)'
- en: '[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial
    Networks](https://arxiv.org/abs/1511.06434)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用深度卷积生成对抗网络进行无监督表示学习](https://arxiv.org/abs/1511.06434)'
- en: 'We are going to talk about generative adversarial networks also known as GANs
    and specifically we are going to focus on Wasserstein GAN paper which included
    Soumith Chintala who went on to create PyTorch. Wasserstein GAN (WGAN) was heavily
    influenced by the deep convolutional generative adversarial network paper which
    also Soumith was involved with. It is a really interesting paper to read. A lot
    of it looks like this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论生成对抗网络，也称为GAN，具体来说，我们将专注于沃瑟斯坦GAN论文，其中包括后来创建PyTorch的Soumith Chintala。沃瑟斯坦GAN（WGAN）受到了深度卷积生成对抗网络论文的重大影响，Soumith也参与其中。这是一篇非常有趣的论文。很多内容看起来像这样：
- en: 'The good news is you can skip those bits because there is also a bit that looks
    like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是你可以跳过那些部分，因为还有一个看起来像这样的部分：
- en: A lot of papers have a theoretical section which seems to be there entirely
    to get past the reviewer’s need for theory. That’s not true with WGAN paper. The
    theory bit is actually interesting — you don’t need to know it to use it, but
    if you want to learn about some cool ideas and see the thinking behind why this
    particular algorithm, it’s absolutely fascinating. Before this paper came out,
    Jeremy knew nobody who studied the math it’s based on, so everybody had to learn
    the math. The paper does a pretty good job of laying out all the pieces (you have
    to do a bunch of reading yourself). So if you are interested in digging into the
    deeper math behind some paper to see what it’s like to study it, I would pick
    this one because at the end of that theory section, you’ll come away saying “I
    can see now why they made this algorithm the way it is.”
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 很多论文都有一个理论部分，似乎完全是为了满足审稿人对理论的需求。但WGAN论文并非如此。理论部分实际上很有趣 - 你不需要了解它就能使用它，但如果你想了解一些很酷的想法，并看到为什么选择这种特定算法的思考过程，那绝对是迷人的。在这篇论文出来之前，Jeremy不认识任何研究其基础数学的人，所以每个人都必须学习这些数学知识。这篇论文做了很好的工作，列出了所有的要点（你需要自己阅读一些内容）。所以如果你对深入研究某篇论文背后更深层次的数学感兴趣，想看看学习它是什么感觉，我会选择这篇，因为在那个理论部分结束时，你会说“我现在明白他们为什么要设计这种算法了。”
- en: 'The basic idea of GAN is it’s a generative model[[51:23](https://youtu.be/ondivPiwQho?t=51m23s)].
    It is something that is going to create sentences, create images, or generate
    something. It is going to try and create thing which is very hard to tell the
    difference between generated stuff and real stuff. So generative model could be
    used to face-swap a video — a very controversial thing of deep fakes and fake
    pornography happening at the moment. It could be used to fake somebody’s voice.
    It could be used to fake the answer to a medical question — but in that case,
    it’s not really a fake, it could be a generative answer to a medical question
    that is actually a good answer so you are generating language. You could generate
    a caption to an image, for example. So generative models have lots of interesting
    applications. But generally speaking, they need to be good enough that for example
    if you are using it to automatically create a new scene for Carrie Fisher in the
    next Star Wars movie and she is not around to play that part anymore, you want
    to try and generate an image of her that looks the same then it has to fool the
    Star Wars audience into thinking “okay, that doesn’t look like some weird Carrie
    Fisher — that looks like the real Carrie Fisher. Or if you are trying to generate
    an answer to a medical question, you want to generate English that reads nicely
    and clearly, and sounds authoritative and meaningful. The idea of generative adversarial
    network is we are going to create not just a generative model to create the generated
    image, but a second model that’s going to try to pick which ones are real and
    which ones are generated (we will call them “fake”). So we have a generator that
    is going to create our fake content and a discriminator that’s going to try to
    get good at recognizing which ones are real and which ones are fake. So there
    are going to be two models and they are going to be adversarial meaning the generator
    is going to try to keep getting better at fooling the discriminator into thinking
    that fake is real, and the discriminator is going to try to keep getting better
    at discriminating between the real and the fake. So they are going to go head
    to head. It is basically as easy as Jeremy just described [[54:14](https://youtu.be/ondivPiwQho?t=54m14s)]:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的基本思想是它是一个生成模型[[51:23](https://youtu.be/ondivPiwQho?t=51m23s)]。它将创建句子、创建图像或生成一些东西。它将尝试创建一些很难区分生成的东西和真实的东西的东西。因此，生成模型可以用于换脸视频——目前发生的深度伪造和虚假色情非常有争议。它可以用来伪造某人的声音。它可以用来伪造对医学问题的回答——但在这种情况下，它并不是真正的伪造，它可以是对医学问题的生成回答，实际上是一个好的回答，因此你在生成语言。例如，你可以为图像生成标题。因此，生成模型有许多有趣的应用。但一般来说，它们需要足够好，例如，如果你要用它自动为凯丽·费舍尔在下一部星球大战电影中的新场景而她已经不在了，你想尝试生成一个看起来一样的图像，那么它必须欺骗星球大战的观众，让他们认为“好吧，那看起来不像奇怪的凯丽·费舍尔——那看起来像真正的凯丽·费舍尔。或者如果你试图生成对医学问题的回答，你希望生成的英语读起来流畅清晰，并且听起来有权威和意义。生成对抗网络的思想是我们不仅要创建一个生成模型来创建生成的图像，还要创建一个第二个模型，它将尝试挑选哪些是真实的，哪些是生成的（我们将称之为“假的”）。因此，我们有一个生成器，它将创建我们的虚假内容，还有一个鉴别器，它将努力变得擅长识别哪些是真实的，哪些是假的。因此，将有两个模型，它们将是对抗性的，意味着生成器将努力不断提高欺骗鉴别器认为假的是真实的能力，而鉴别器将努力不断提高区分真实和虚假的能力。因此，它们将正面交锋。这基本上就像Jeremy刚刚描述的那样[[54:14](https://youtu.be/ondivPiwQho?t=54m14s)]：
- en: We are going to build two models in PyTorch
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在PyTorch中构建两个模型
- en: We are going to create a training loop that first of all says the loss function
    for the discriminator is “can you tell the difference between real and fake, then
    update the weights of that.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将创建一个训练循环，首先说鉴别器的损失函数是“你能分辨真实和虚假吗，然后更新那个的权重。
- en: We are going to create a loss function for the generator which is “can you generate
    something which fools the discriminator and update the weights from that loss.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将为生成器创建一个损失函数，即“你能生成一些能欺骗鉴别器的东西并从中更新权重。
- en: And we are going to loop through that a few times and see what happens.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将循环几次并看看会发生什么。
- en: Looking at the code [[54:52](https://youtu.be/ondivPiwQho?t=54m52s)]
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看代码[[54:52](https://youtu.be/ondivPiwQho?t=54m52s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/wgan.ipynb)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/wgan.ipynb)'
- en: There is a lot of different things you can do with GANS. We are going to do
    something that is kind of boring but easy to understand and it’s kind of cool
    that it’s even possible which is we are going to generate some pictures from nothing.
    We are just going to get it to draw some pictures. Specifically, we are going
    to get it to draw pictures of bedrooms. Hopefully you get a chance to play around
    with this during the week with your own datasets. If you pick a dataset that’s
    very varied like ImageNet and then get a GAN to try and create ImageNet pictures,
    it tends not to do so well because it’s not clear enough what you want a picture
    of. So it’s better to give it, for example, there is a dataset called [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)
    which is pictures of celebrities’ faces that works great with GANs. You create
    really clear celebrity faces that don’t actually exist. The bedroom dataset is
    also a good one — pictures of the same kind of thing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: GAN有很多不同的用途。我们将做一些有点无聊但易于理解的事情，而且甚至可能的是我们将从无中生成一些图片。我们只是让它画一些图片。具体来说，我们将让它画卧室的图片。希望你有机会在这一周内使用自己的数据集玩耍。如果你选择一个非常多样化的数据集，比如ImageNet，然后让GAN尝试创建ImageNet的图片，它往往做得不太好，因为你想要的图片不够清晰。所以最好给它，例如，有一个名为[CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)的数据集，其中包含名人的脸部图片，这对GAN非常有效。你可以生成真实但实际上不存在的名人脸。卧室数据集也是一个不错的选择——同一种类型的图片。
- en: There is something called LSUN scene classification dataset [[55:55](https://youtu.be/ondivPiwQho?t=55m55s)].
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个叫做LSUN场景分类数据集的东西。
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Download the LSUN scene classification dataset bedroom category, unzip it,
    and convert it to jpg files (the scripts folder is here in the `dl2` folder):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下载LSUN场景分类数据集卧室类别，解压缩它，并将其转换为jpg文件（脚本文件夹在`dl2`文件夹中）：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This isn't tested on Windows - if it doesn't work, you could use a Linux box
    to convert the files, then copy them over. Alternatively, you can download [this
    20% sample](https://www.kaggle.com/jhoward/lsun_bedroom) from Kaggle datasets.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这在Windows上没有经过测试 - 如果不起作用，您可以使用Linux框来转换文件，然后复制它们。或者，您可以从Kaggle数据集中下载这个20%的样本。
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this case, it is much easier to go the CSV route when it comes to handling
    our data. So we generate a CSV with the list of files that we want, and a fake
    label “0” because we don’t really have labels for these at all. One CSV file contains
    everything in that bedroom dataset, and another one contains random 10%. It is
    nice to do that because then we can most of the time use the sample when we are
    experimenting because there is well over a million files even just reading in
    the list takes a while.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理我们的数据时，通过CSV路线会更容易。因此，我们生成一个包含我们想要的文件列表和一个虚假标签“0”的CSV，因为我们实际上根本没有这些标签。一个CSV文件包含卧室数据集中的所有内容，另一个包含随机的10%。这样做很好，因为这样我们在实验时大多数时间可以使用样本，因为即使只是读取列表也需要很长时间，因为有超过一百万个文件。
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will look pretty familiar [[57:10](https://youtu.be/ondivPiwQho?t=57m10s)].
    This is before Jeremy realized that sequential models are much better. So if you
    compare this to the previous conv block with a sequential model, there is a lot
    more lines of code here — but it does the same thing of conv, ReLU, batch norm.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来非常熟悉。这是在Jeremy意识到顺序模型更好之前。因此，如果将这与以前的顺序模型的卷积块进行比较，这里有更多的代码行数——但它做的事情是一样的，卷积，ReLU，批量归一化。
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The first thing we are going to do is to build a discriminator [[57:47](https://youtu.be/ondivPiwQho?t=57m47s)].
    A discriminator is going to receive an image as an input, and it’s going to spit
    out a number. The number is meant to be lower if it thinks this image is real.
    Of course “what does it do for a lower number” thing does not appear in the architecture,
    that will be in the loss function. So all we have to do is to create something
    that takes an image and spits out a number. A lot of this code is borrowed from
    the original authors of this paper, so some of the naming scheme is different
    to what we are used to. But it looks similar to what we had before. We start out
    with a convolution (conv, ReLU, batch norm). Then we have a bunch of extra conv
    layers — this is not going to use a residual so it looks very similar to before
    a bunch of extra layers but these are going to be conv layers rather than res
    layers. At the end, we need to append enough stride 2 conv layers that we decrease
    the grid size down to no bigger than 4x4\. So it’s going to keep using stride
    2, divide the size by 2, and repeat till our grid size is no bigger than 4\. This
    is quite a nice way of creating as many layers as you need in a network to handle
    arbitrary sized images and turn them into a fixed known grid size.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是构建一个鉴别器。鉴别器将接收一幅图像作为输入，并输出一个数字。如果它认为这幅图像是真实的，那么这个数字应该更低。当然，“它为什么输出一个更低的数字”这个问题不会出现在架构中，这将在损失函数中。所以我们所要做的就是创建一个接收图像并输出数字的东西。这些代码的很多部分都是从这篇论文的原始作者那里借来的，所以一些命名方案与我们习惯的不同。但它看起来与我们之前的很相似。我们从卷积（conv，ReLU，批量归一化）开始。然后我们有一堆额外的卷积层——这不会使用残差，所以它看起来与之前非常相似，有一堆额外的层，但这些将是卷积层而不是残差层。最后，我们需要添加足够的步幅为2的卷积层，使网格大小减小到不大于4x4。所以它将继续使用步幅2，将大小除以2，并重复直到我们的网格大小不大于4。这是一个非常好的方法，可以创建网络中所需的任意数量的层，以处理任意大小的图像并将它们转换为固定的已知网格大小。
- en: '**Question**: Does GAN need a lot more data than say dogs vs. cats or NLP?
    Or is it comparable [[59:48](https://youtu.be/ondivPiwQho?t=59m48s)]? Honestly,
    I am kind of embarrassed to say I am not an expert practitioner in GANs. The stuff
    I teach in part one is things I am happy to say I know the best way to do these
    things and so I can show you state-of-the-art results like we just did with CIFAR10
    with the help of some of the students. I am not there at all with GANs so I am
    not quite sure how much you need. In general, it seems it needs quite a lot but
    remember the only reason we didn’t need too much in dogs and cats is because we
    had a pre-trained model and could we leverage pre-trained GAN models and fine
    tune them, probably. I don’t think anybody has done it as far as I know. That
    could be really interesting thing for people to think about and experiment with.
    Maybe people have done it and there is some literature there we haven’t come across.
    I’m somewhat familiar with the main pieces of literature in GANs but I don’t know
    all of it, so maybe I’ve missed something about transfer learning in GANs. But
    that would be the trick to not needing too much data.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：GAN是否需要比狗和猫或NLP等更多的数据？还是可以相提并论？老实说，我有点尴尬地说我不是GAN的专家从业者。我在第一部分教授的东西是我很高兴地说我知道如何做这些事情的最佳方式，所以我可以展示像我们刚刚在CIFAR10中所做的那样的最新结果，有一些学生的帮助。我在GAN方面一点也不行，所以我不太确定你需要多少。总的来说，似乎需要相当多，但请记住我们在狗和猫方面不需要太多的原因是因为我们有一个预训练模型，我们可以利用预训练的GAN模型并微调它们，可能。据我所知，我认为没有人这样做过。这可能是人们考虑和实验的一个非常有趣的事情。也许人们已经这样做了，有一些文献我们还没有接触到。我对GAN的主要文献有一些了解，但并不是全部，所以也许我错过了关于GAN中迁移学习的一些内容。但这可能是不需要太多数据的诀窍。'
- en: '**Question**: So the huge speed-up a combination of one cycle learning rate
    and momentum annealing plus the eight GPU parallel training in the half precision?
    Is that only possible to do the half precision calculation with consumer GPU?
    Another question, why is the calculation 8 times faster from single to half precision,
    while from double the single is only 2 times faster [[1:01:09](https://youtu.be/ondivPiwQho?t=1h1m9s)]?
    Okay, so the CIFAR10 result, it’s not 8 times faster from single to half. It’s
    about 2 or 3 times as fast from single to half. NVIDIA claims about the flops
    performance of the tensor cores, academically correct, but in practice meaningless
    because it really depends on what calls you need for what piece — so about 2 or
    3x improvement for half. So the half precision helps a bit, the extra GPUs helps
    a bit, the one cycle helps an enormous amount, then another key piece was the
    playing around with the parameters that I told you about. So reading the wide
    ResNet paper carefully, identifying the kinds of things that they found there,
    and then writing a version of the architecture you just saw that made it really
    easy for us to fiddle around with parameters, staying up all night trying every
    possible combination of different kernel sizes, numbers of kernels, number of
    layer groups, size of layer groups. And remember, we did a bottleneck but actually
    we tended to focus instead on widening so we increase the size and then decrease
    it because it takes better advantage of the GPU. So all those things combined
    together, I’d say the one cycle was perhaps the most critical but every one of
    those resulted in a big speed-up. That’s why we were able to get this 30x improvement
    over the state-of-the-art CIFAR10\. We have some ideas for other things — after
    this DAWN bench finishes, maybe we’ll try and go even further to see if we can
    beat one minute one day. That’ll be fun.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：是单周期学习率和动量退火加上八个GPU并行训练在半精度下的巨大加速？只有消费级GPU才能进行半精度计算吗？另一个问题，为什么从单精度到半精度的计算速度提高了8倍，而从双精度到单精度只提高了2倍？好的，所以CIFAR10的结果，从单精度到半精度并不是提高了8倍。从单精度到半精度大约快了2到3倍。NVIDIA声称张量核心的flops性能，在学术上是正确的，但在实践中是没有意义的，因为这真的取决于你需要什么调用来做什么事情——所以半精度大约提高了2到3倍。所以半精度有所帮助，额外的GPU有所帮助，单周期有很大帮助，然后另一个关键部分是我告诉你的参数调整。所以仔细阅读Wide
    ResNet论文，识别他们在那里发现的东西的类型，然后编写一个你刚刚看到的架构的版本，使我们可以轻松地调整参数，整夜不眠地尝试每种可能的不同核大小、核数、层组数、层组大小的组合。记住，我们做了一个瓶颈，但实际上我们更倾向于扩大，所以我们增加了大小，然后减小了，因为这更好地利用了GPU。所以所有这些结合在一起，我会说单周期也许是最关键的，但每一个都导致了巨大的加速。这就是为什么我们能够在CIFAR10的最新技术上取得30倍的改进。我们对其他事情有一些想法——在这个DAWN基准完成之后，也许我们会尝试更进一步，看看是否可以在某一天打破一分钟。那将很有趣。
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So here is our discriminator [[1:03:37](https://youtu.be/ondivPiwQho?t=1h3m37s)].The
    important thing to remember about an architecture is it doesn’t do anything rather
    than have some input tensor size and rank, and some output tensor size and rank.
    As you see the last conv has one channel. This is different from what we are used
    to because normally our last thing is a linear block. But our last layer here
    is a conv block. It only has one channel but it has a grid size of something around
    4x4 (no more than 4x4). So we are going to spit out (let’s say it’s 4x4), 4 by
    4 by 1 tensor. What we then do is we then take the mean of that. So it goes from
    4x4x1 to a scalar. This is kind of like the ultimate adaptive average pooling
    because we have something with just one channel and we take the mean. So this
    is a bit different — normally we first do average pooling and then we put it through
    a fully connected layer to get our one thing out. But this is getting one channel
    out and then taking the mean of that. Jeremy suspects that it would work better
    if we did the normal way, but he hasn’t tried it yet and he doesn’t really have
    a good enough intuition to know whether he is missing something — but it will
    be an interesting experiment to try if somebody wants to stick an adaptive average
    pooling layer and a fully connected layer afterwards with a single output.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是我们的鉴别器。关于架构需要记住的重要事情是它除了有一些输入张量大小和秩，以及一些输出张量大小和秩之外，什么也不做。正如你所看到的，最后一个卷积层只有一个通道。这与我们通常的做法不同，因为通常我们的最后一层是一个线性块。但我们这里的最后一层是一个卷积块。它只有一个通道，但它的网格大小大约是4x4（不超过4x4）。所以我们将输出（假设是4x4），4x4x1张量。然后我们计算平均值。所以它从4x4x1变成一个标量。这有点像最终的自适应平均池化，因为我们有一个通道，我们取平均值。这有点不同——通常我们首先进行平均池化，然后通过一个全连接层来得到我们的输出。但这里是得到一个通道，然后取平均值。Jeremy怀疑如果我们按照正常方式做会更好，但他还没有尝试过，他也没有足够好的直觉来知道是否漏掉了什么——但如果有人想要尝试在自适应平均池化层和一个具有单个输出的全连接层之后添加一个，那将是一个有趣的实验。
- en: So that’s a discriminator. Let’s assume we already have a generator — somebody
    says “okay, here is a generator which generates bedrooms. I want you to build
    a model that can figure out which ones are real and which ones aren’t”. We are
    going to take the dataset and label bunch of images which are fake bedrooms from
    the generator, and a bunch of images of real bedrooms from LSUN dataset to stick
    a 1 or a 0 on each one. Then we’ll try to get the discriminator to tell the difference.
    So that is going to be simple enough. But we haven’t been given a generator. We
    need to build one. We haven’t talked about the loss function yet — we are going
    to assume that there’s some loss function that does this thing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一个鉴别器。假设我们已经有了一个生成器——有人说“好的，这里有一个生成卧室的生成器。我希望你建立一个模型，可以找出哪些是真实的，哪些是假的”。我们将拿取数据集，并标记一堆来自生成器的假卧室图像，以及LSUN数据集中真实卧室的一堆图像，然后在每个图像上贴上1或0。然后我们将尝试让鉴别器区分出差异。所以这将是足够简单的。但我们还没有得到一个生成器。我们需要建立一个。我们还没有讨论损失函数——我们将假设有一个损失函数可以做到这一点。
- en: '**Generator** [[1:06:15](https://youtu.be/ondivPiwQho?t=1h6m15s)]'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成器
- en: A generator is also an architecture which doesn’t do anything by itself until
    we have a loss function and data. But what are the ranks and sizes of the tensors?
    The input to the generator is going to be a vector of random numbers. In the paper,
    they call that the “prior.” How big? We don’t know. The idea is that a different
    bunch of random numbers will generate a different bedroom. So our generator has
    to take as input a vector, stick it through sequential models, and turn it into
    a rank 4 tensor (rank 3 without the batch dimension) — height by width by 3\.
    So in the final step, `nc` (number of channel) is going to have to end up being
    3 because it’s going to create a 3 channel image of some size.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器也是一种架构，本身不会做任何事情，直到我们有损失函数和数据。但张量的秩和大小是什么？生成器的输入将是一个随机数向量。在论文中，他们称之为“先验”。有多大？我们不知道。这个想法是不同的一堆随机数将生成一个不同的卧室。因此，我们的生成器必须将一个向量作为输入，通过顺序模型，将其转换为一个秩为4的张量（没有批量维度的秩为3）-高度乘以宽度乘以3。因此，在最后一步，`nc`（通道数）最终将变为3，因为它将创建一个大小为3的通道图像。
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Question**: In ConvBlock, is there a reason why batch norm comes after ReLU
    (i.e. `self.bn(self.relu(…))`) [[1:07:50](https://youtu.be/ondivPiwQho?t=1h7m50s)]?
    I would normally expect to go ReLU then batch norm [[1:08:23](https://youtu.be/ondivPiwQho?t=1h8m23s)]
    that this is actually the order that makes sense to Jeremy. The order we had in
    the darknet was what they used in the darknet paper, so everybody seems to have
    a different order of these things. In fact, most people for CIFAR10 have a different
    order again which is batch norm → ReLU → conv which is a quirky way of thinking
    about it, but it turns out that often for residual blocks that works better. That
    is called a “pre-activation ResNet.” There is a few blog posts out there where
    people have experimented with different order of those things and it seems to
    depend a lot on what specific dataset it is and what you are doing with — although
    the difference in performance is small enough that you won’t care unless it’s
    for a competition.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：在ConvBlock中，为什么批量归一化在ReLU之后（即`self.bn(self.relu(...))`）？我通常期望先进行ReLU，然后批量归一化，这实际上是Jeremy认为有意义的顺序。我们在darknet中使用的顺序是darknet论文中使用的顺序，所以每个人似乎对这些事情有不同的顺序。事实上，大多数人对CIFAR10有一个不同的顺序，即批量归一化→ReLU→卷积，这是一种奇特的思考方式，但事实证明，对于残差块来说，这通常效果更好。这被称为“预激活ResNet”。有一些博客文章中，人们已经尝试了不同顺序的事物，似乎这很大程度上取决于特定数据集以及您正在处理的内容，尽管性能差异很小，除非是为了比赛，否则您不会在意。
- en: Deconvolution [[1:09:36](https://youtu.be/ondivPiwQho?t=1h9m36s)]
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反卷积
- en: So the generator needs to start with a vector and end up with a rank 3 tensor.
    We don’t really know how to do that yet. We need to use something called a “deconvolution”
    and PyTorch calls it transposed convolution — same thing, different name. Deconvolution
    is something which rather than decreasing the grid size, it increases the grid
    size. So as with all things, it’s easiest to see in an Excel spreadsheet.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，生成器需要从一个向量开始，最终得到一个秩为3的张量。我们还不知道如何做到这一点。我们需要使用一种称为“反卷积”的东西，PyTorch称之为转置卷积-相同的东西，不同的名称。反卷积是一种增加网格大小而不是减小网格大小的东西。因此，像所有事物一样，在Excel电子表格中最容易看到。
- en: Here is a convolution. We start, let’s say, with a 4 by 4 grid cell with a single
    channel. Let’s put it through a 3 by 3 kernel with a single output filter. So
    we have a single channel in, a single filter kernel, so if we don’t add any padding,
    we are going to end up with 2 by 2\. Remember, the convolution is just the sum
    of the product of the kernel and the appropriate grid cell [[1:11:09](https://youtu.be/ondivPiwQho?t=1h11m9s)].
    So there is our standard 3 by 3 conv one channel one filter.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个卷积。我们开始，假设有一个单通道的4x4网格单元。让我们通过一个单输出滤波器的3x3核心。所以我们有一个输入通道，一个滤波器核心，如果我们不添加任何填充，最终会得到2x2。记住，卷积只是核心和适当网格单元的乘积的总和。所以这是我们标准的3x3卷积一个通道一个滤波器。
- en: So the idea now is we want to go the opposite direction [[1:11:25](https://youtu.be/ondivPiwQho?t=1h11m25s)].
    We want to start with our 2 by 2 and we want to create a 4 by 4\. Specifically
    we want to create the same 4 by 4 that we started with. And we want to do that
    by using a convolution. How would we do that?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的想法是我们想要朝相反的方向发展。我们想要从我们的2x2开始，我们想要创建一个4x4。具体来说，我们想要创建与我们开始的相同的4x4。我们想通过使用卷积来实现这一点。我们如何做到这一点？
- en: 'If we have a 3 by 3 convolution, then if we want to create a 4 by 4 output,
    we are going to need to create this much padding:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个3x3卷积，那么如果我们想要创建一个4x4输出，我们将需要创建这么多填充：
- en: 'Because with this much padding, we are going to end up with 4 by 4\. So let’s
    say our convolutional filter was just a bunch of zeros then we can calculate our
    error for each cell just by taking this subtraction:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因为有这么多填充，我们最终会得到4x4。所以假设我们的卷积滤波器只是一堆零，那么我们可以通过进行这个减法来计算每个单元格的错误：
- en: 'Then we can get the sum of absolute values (L1 loss) by summing up the absolute
    values of those errors:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过对这些错误的绝对值求和来获得绝对值之和（L1损失）：
- en: So now we could use optimization, in Excel it’s called “solver” to do a gradient
    descent. So we will set the Total cell equal to minimum and we’ll try and reduce
    our loss by changing our filter. You can see it’s come up with a filter such that
    Result is almost like Data. It’s not perfect, and in general, you can’t assume
    that a deconvolution can exactly create the same exact thing you want because
    there is just not enough. Because there is 9 things in the filter and 16 things
    in the result. But it’s made a pretty good attempt. So this is what a deconvolution
    looks like — a stride 1, 3x3 deconvolution on a 2x2 grid cell input.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用优化，在Excel中称为“求解器”来进行梯度下降。所以我们将设置总单元格等于最小值，然后尝试通过改变我们的滤波器来减少我们的损失。你可以看到它提出了一个滤波器，使得结果几乎像数据一样。它并不完美，一般来说，你不能假设反卷积可以完全创建出你想要的完全相同的东西，因为这里没有足够的。因为滤波器中有9个东西，结果中有16个东西。但它做出了一个相当不错的尝试。所以这就是反卷积的样子
    - 在一个2x2的网格单元上进行步长为1的3x3反卷积。
- en: '**Question**: How difficult is it to create a discriminator to identify fake
    news vs. real news [[1:13:43](https://youtu.be/ondivPiwQho?t=1h13m43s)]? You don’t
    need anything special — that’s just a classifier. So you would just use the NLP
    classifier from previous class and lesson 4\. In that case, there is no generative
    piece, so you just need a dataset that says these are the things that we believe
    are fake news and these are the things we consider to be real news and it should
    actually work very well. To the best of our knowledge, if you try it you should
    get as good a result as anybody else has got — whether it’s good enough to be
    useful in practice, Jeremy doesn’t know. The best thing you could do at this stage
    would be to generate a kind of a triage that says these things look pretty sketchy
    based on how they are written and then some human could go in and fact check them.
    NLP classifier and RNN can’t fact-check things but it could recognize that these
    are written in that kind of highly popularized style which often fake news is
    written in so maybe these ones are worth paying attention to. That would probably
    be the best you could hope for without drawing on some kind of external data sources.
    But it’s important to remember the discriminator is basically just a classifier
    and you don’t need any special techniques beyond what we’ve already learned to
    do NLP classification.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**: 创建一个鉴别器来识别假新闻和真实新闻有多难？你不需要任何特殊的东西 - 那只是一个分类器。所以你可以使用之前课程和第4课的NLP分类器。在这种情况下，没有生成部分，所以你只需要一个数据集，其中说这些是我们认为是假新闻的东西，这些是我们认为是真实新闻的东西，它应该工作得非常好。据我们所知，如果你尝试，你应该得到和其他人一样好的结果
    - 它是否足够实用，Jeremy不知道。在这个阶段，你能做的最好的事情可能是生成一种分类，说这些东西看起来相当可疑，基于它们的写作方式，然后一些人可以去核实它们。NLP分类器和RNN不能核实事实，但它可以识别这些是以那种高度通俗的风格写成的，通常假新闻就是这样写的，所以也许这些值得关注。这可能是你在不依赖某种外部数据源的情况下所能希望的最好的结果。但重要的是要记住，鉴别器基本上只是一个分类器，你不需要任何特殊的技术，超出我们已经学会的NLP分类的范围。'
- en: ConvTranspose2d [[1:16:00](https://youtu.be/ondivPiwQho?t=1h16m)]
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'ConvTranspose2d '
- en: 'To do deconvolution in PyTorch, just say:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中进行反卷积，只需说：
- en: '`nn.ConvTranspose2d(ni, no, ks, stride, padding=pad, bias=False)`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.ConvTranspose2d(ni, no, ks, stride, padding=pad, bias=False)`'
- en: '`ni` : number of input channels'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ni`: 输入通道的数量'
- en: '`no`: number of ourput channels'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no`: 输出通道的数量'
- en: '`ks`: kernel size'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ks`: 卷积核大小'
- en: The reason it’s called a ConvTranspose is because it turns out that this is
    the same as the calculation of the gradient of convolution. That’s why they call
    it that.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为ConvTranspose的原因是因为事实证明这与卷积的梯度计算是相同的。这就是为什么他们这样称呼它。
- en: '**Visualizing** [[1:16:33](https://youtu.be/ondivPiwQho?t=1h16m33s)]'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**可视化** '
- en: '[http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)'
- en: One on the left is what we just saw of doing a 2x2 deconvolution. If there is
    a stride 2, then you don’t just have padding around the outside, but you actually
    have to put padding in the middle as well. They are not actually quite implemented
    this way because this is slow to do. In practice, you’ll implement them in a different
    way but it all happens behind the scene, so you don’t have to worry about it.
    We’ve talked about this [convolution arithmetic tutorial](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)
    before and if you are still not comfortable with convolutions and in order to
    get comfortable with deconvolutions, this is a great site to go to. If you want
    to see the paper, it is [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 左边的是我们刚刚看到的进行2x2反卷积。如果有一个步长为2，那么你不仅在外面周围有填充，而且你实际上还需要在中间放填充。它们实际上并不是这样实现的，因为这样做很慢。在实践中，你会以不同的方式实现它们，但所有这些都是在幕后发生的，所以你不必担心。我们之前已经讨论过这个[卷积算术教程](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)，如果你对卷积仍然不熟悉，并且想要熟悉反卷积，这是一个很好的网站。如果你想看这篇论文，它是[A
    guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)。
- en: '`DeconvBlock` looks identical to a `ConvBlock` except it has the word `Transpose`
    [[1:17:49](https://youtu.be/ondivPiwQho?t=1h17m49s)]. We just go conv → relu →
    batch norm as before, and it has input filters and output filters. The only difference
    is taht stride 2 means that the grid size will double rather than half.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`DeconvBlock` 看起来与 `ConvBlock` 几乎相同，只是多了一个 `Transpose`。我们像以前一样进行卷积 → relu →
    批量归一化，它有输入滤波器和输出滤波器。唯一的区别是步长为2意味着网格大小会加倍而不是减半。'
- en: 'Question: Both `nn.ConvTranspose2d` and `nn.Upsample` seem to do the same thing,
    i.e. expand grid-size (height and width) from previous layer. Can we say `nn.ConvTranspose2d`
    is always better than `nn.Upsample`, since `nn.Upsample` is merely resize and
    fill unknowns by zero’s or interpolation [[1:18:10](https://youtu.be/ondivPiwQho?t=1h18m10s)]?
    No, you can’t. There is a fantastic interactive paper on distill.pub called [Deconvolution
    and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/) which
    points out that what we are doing right now is extremely suboptimal but the good
    news is everybody else does it.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：`nn.ConvTranspose2d` 和 `nn.Upsample` 似乎做着相同的事情，即从上一层扩展网格大小（高度和宽度）。我们可以说 `nn.ConvTranspose2d`
    总是优于 `nn.Upsample` 吗，因为 `nn.Upsample` 仅仅是调整大小并用零或插值填充未知部分吗？不，不能。在 distill.pub
    上有一篇名为 [反卷积和棋盘伪影](https://distill.pub/2016/deconv-checkerboard/) 的出色互动论文指出，我们现在正在做的事情极其不理想，但好消息是其他人都在这样做。
- en: Have a look here, could you see these checkerboard artifacts? These are all
    from actual papers and basically they noticed every one of these papers with generative
    models have these checkerboard artifacts and what they realized is it’s because
    when you have a stride 2 convolution of size three kernel, they overlap. So some
    grid cells gets twice as much activation,
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下这里，你能看到这些棋盘伪影吗？这些都来自实际论文，基本上他们注意到每一篇关于生成模型的论文都有这些棋盘伪影，他们意识到这是因为当您使用大小为三的内核的步幅
    2 卷积时，它们会重叠。因此，一些网格单元会获得两倍的激活。
- en: So even if you start with random weights, you end up with a checkerboard artifacts.
    So deeper you get, the worse it gets. Their advice is less direct than it ought
    to be, Jeremy found that for most generative models, upsampling is better. If
    you `nn.Upsample`, it’s basically doing the opposite of pooling — it says let’s
    replace this one grid cell with four (2x2). There is a number of ways to upsample
    — one is just to copy it all across to those four, and other is to use bilinear
    or bicubic interpolation. There are various techniques to try and create a smooth
    upsampled version and you can choose any of them in PyTorch. If you do a 2 x 2
    upsample and then regular stride one 3 x 3 convolution, that is another way of
    doing the same kind of thing as a ConvTranspose — it’s doubling the grid size
    and doing some convolutional arithmetic on it. For generative models, it pretty
    much always works better. In that distil.pub publication, they indicate that maybe
    that’s a good approach but they don’t just come out and say just do this whereas
    Jeremy would just say just do this. Having said that, for GANS, he hasn’t had
    that much success with it yet and he thinks it probably requires some tweaking
    to get it to work, The issue is that in the early stages, it doesn’t create enough
    noise. He had a version where he tried to do it with an upsample and you could
    kind of see that the noise didn’t look very noisy. Next week when we look at style
    transfer and super-resolution, you will see `nn.Upsample` really comes into its
    own.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使您从随机权重开始，最终也会得到一个棋盘状的伪影。所以你越深入，情况就越糟。他们的建议没有那么直接，Jeremy 发现对于大多数生成模型，上采样更好。如果你使用
    `nn.Upsample`，基本上是在做池化的相反操作 —— 它说让我们用四个（2x2）网格单元替换这一个。有许多方法可以进行上采样 —— 一种方法是将所有内容复制到这四个单元格中，另一种方法是使用双线性或双三次插值。有各种技术可以尝试创建平滑的上采样版本，您可以在
    PyTorch 中选择任何一种。如果您进行了 2x2 的上采样，然后正常的 3x3 卷积，这是另一种与 ConvTranspose 相同的操作方式 —— 它将网格大小加倍，并对其进行一些卷积运算。对于生成模型，这几乎总是效果更好。在
    distil.pub 的出版物中，他们指出也许这是一个好方法，但他们没有直接说出来，而 Jeremy 会直接说出来。话虽如此，对于 GANS，他还没有取得太大的成功，他认为可能需要一些调整才能使其正常工作。问题在于在早期阶段，它没有产生足够的噪音。他尝试过使用上采样的版本，您可以看到噪音看起来并不是很嘈杂。下周当我们研究风格转移和超分辨率时，您将看到
    `nn.Upsample` 真正发挥作用。
- en: The generator, we can now start with the vector [[1:22:04](https://youtu.be/ondivPiwQho?t=1h22m04s)].
    We can decide and say okay let’s not think of it as a vector but actually it’s
    1x1 grid cell, and then we can turn it into a 4x4 then 8x8 and so forth. That
    is why we have to make sure it’s a suitable multiple so that we can create something
    of the right size. As you can see, it’s doing the exact opposite as before. It’s
    making the cell size bigger and bigger by 2 at a time as long as it can until
    it gets to half the size that we want, and then finally we add `n` more on at
    the end with stride 1\. Then we add one more ConvTranspose to finally get to the
    size that we wanted and we are done. Finally we put that through a `tanh` and
    that will force us to be in the zero to one range because of course we don’t want
    to spit out arbitrary size pixel values. So we have a generator architecture which
    spits out an image of some given size with the correct number of channels with
    values between zero and one.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器，我们现在可以从向量开始。我们可以决定并说好，让我们不把它看作一个向量，而实际上是一个 1x1 的网格单元，然后我们可以将其转换为 4x4，然后是
    8x8 等等。这就是为什么我们必须确保它是一个合适的倍数，以便我们可以创建出正确大小的东西。正如您所看到的，它正在做与之前完全相反的事情。它每次使单元格大小增加
    2，直到达到我们想要的一半大小，然后最后我们再添加 `n` 个，步幅为 1。然后我们再添加一个 ConvTranspose 最终得到我们想要的大小，然后我们完成了。最后我们通过一个
    `tanh`，这将强制我们处于零到一的范围内，因为当然我们不希望输出任意大小的像素值。因此，我们有一个生成器架构，它输出一个给定大小的图像，具有正确数量的通道，值在零到一之间。
- en: At this point, we can now create our model data object [[1:23:38](https://youtu.be/ondivPiwQho?t=1h23m38s)].
    These things take a while to train, so we made it 128 by 128 (just a convenient
    way to make it a little bit faster). So that is going to be the size of the input,
    but then we are going to use transformation to turn it into 64 by 64.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们现在可以创建我们的模型数据对象。这些东西需要一段时间来训练，所以我们将其设置为 128x128（只是一个更快的便利方式）。因此，这将是输入的大小，但然后我们将使用转换将其转换为
    64x64。
- en: There’s been more recent advances which have attempted to really increase this
    up to high resolution sizes but they still tend to require either a batch size
    of 1 or lots and lots of GPUs [[1:24:05](https://youtu.be/ondivPiwQho?t=1h24m5s)].
    So we are trying to do things that we can do with a single consumer GPU. Here
    is an example of one of the 64 by 64 bedrooms.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最近有更多的进展，试图将其提高到高分辨率大小，但它们仍然倾向于要求批量大小为1或大量的GPU。所以我们试图做一些可以用单个消费者GPU完成的事情。这是一个64x64卧室的例子。
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Putting them all together [[1:24:30](https://youtu.be/ondivPiwQho?t=1h24m30s)]
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将它们全部放在一起
- en: We are going to do pretty much everything manually so let’s go ahead and create
    our two models — our generator and discriminator and as you can see they are DCGAN,
    so in other words, they are the same modules that appeared in [this paper](https://arxiv.org/abs/1511.06434).
    It is well worth going back and looking at the DCGAN paper to see what these architectures
    are because it’s assumed that when you read the Wasserstein GAN paper that you
    already know that.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将几乎所有事情都手动完成，所以让我们继续创建我们的两个模型 - 我们的生成器和鉴别器，正如你所看到的它们是DCGAN，换句话说，它们是出现在这篇论文中的相同模块。值得回头看一下DCGAN论文，看看这些架构是什么，因为假定当你阅读Wasserstein
    GAN论文时，你已经知道这一点。
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Question**: Shouldn’t we use a sigmoid if we want values between 0 and 1
    [[1:25:06](https://youtu.be/ondivPiwQho?t=1h25m6s)]? As usual, our images have
    been normalized to have a range from -1 to 1, so their pixel values don’t go between
    0 and 1 anymore. This is why we want values going from -1 to 1 otherwise we wouldn’t
    give a correct input for the discriminator (via [this post](http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/140)).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：如果我们想要在0到1之间的值，我们不应该使用sigmoid吗？像往常一样，我们的图像已经被归一化为范围从-1到1，因此它们的像素值不再在0到1之间。这就是为什么我们希望值从-1到1，否则我们将无法为鉴别器提供正确的输入。
- en: So we have a generator and a discriminator, and we need a function that returns
    a “prior” vector (i.e. a bunch of noise)[[1:25:49](https://youtu.be/ondivPiwQho?t=1h25m49s)].
    We do that by creating a bunch of zeros. `nz` is the size of `z` — very often
    in our code, if you see a mysterious letter, it’s because that’s the letter they
    used in the paper. Here, `z` is the size of our noise vector. We then use normal
    distribution to generate random numbers between 0 and 1\. And that needs to be
    a variable because it’s going to be participating in the gradient updates.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有一个生成器和一个鉴别器，我们需要一个返回“先验”向量（即一堆噪音）的函数。我们通过创建一堆零来实现这一点。`nz`是`z`的大小 - 在我们的代码中经常看到一个神秘的字母，那是因为那是他们在论文中使用的字母。这里，`z`是我们噪音向量的大小。然后我们使用正态分布生成0到1之间的随机数。这需要是一个变量，因为它将参与梯度更新。
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So here is an example of creating some noise and resulting four different pieces
    of noise.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是创建一些噪音并生成四个不同噪音片段的示例。
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We need an optimizer in order to update our gradients [[1:26:41](https://youtu.be/ondivPiwQho?t=1h26m41s)].
    In the Wasserstein GAN paper, they told us to use RMSProp:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个优化器来更新我们的梯度。在Wasserstein GAN论文中，他们告诉我们使用RMSProp：
- en: 'We can easily do that in PyTorch:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地在PyTorch中做到这一点：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the paper, they suggested a learning rate of 0.00005 (`5e-5`), we found `1e-4`
    seem to work, so we made it a little bit bigger.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，他们建议使用学习率为0.00005（`5e-5`），我们发现`1e-4`似乎有效，所以我们将其增加了一点。
- en: 'Now we need a training loop [[1:27:14](https://youtu.be/ondivPiwQho?t=1h27m14s)]:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一个训练循环：
- en: For easier reading
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易阅读
- en: 'A training loop will go through some number of epochs that we get to pick (so
    that’s going to be a parameter). Remember, when you do everything manually, you’ve
    got to remember all the manual steps to do:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环将经过我们选择的一些时代（这将是一个参数）。记住，当你手动完成所有事情时，你必须记住所有手动步骤：
- en: You have to set your modules into training mode when you are training them and
    into evaluation mode when you are evaluating because in training mode batch norm
    updates happen and dropout happens, in evaluation mode, those two things gets
    turned off.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你训练模型时，你必须将模块设置为训练模式，并在评估时将其设置为评估模式，因为在训练模式下，批量归一化更新会发生，丢失会发生，在评估模式下，这两个事情会被关闭。
- en: We are going to grab an iterator from our training data loader
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从我们的训练数据加载器中获取一个迭代器
- en: We are going to see how many steps we have to go through and then we will use
    `tqdm` to give us a progress bar, and we are going to go through that many steps.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将看看我们需要经过多少步，然后我们将使用`tqdm`给我们提供一个进度条，然后我们将经过那么多步。
- en: The first step of the algorithm in the paper is to update the discriminator
    (in the paper, they call discriminator a “critic” and `w` is the weights of the
    critic). So the first step is to train our critic a little bit, and then we are
    going to train our generator a little bit, and we will go back to the top of the
    loop. The inner `for` loop in the paper correspond to the second `while` loop
    in our code.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中算法的第一步是更新鉴别器（在论文中，他们称鉴别器为“评论家”，`w`是评论家的权重）。所以第一步是训练我们的评论家一点点，然后我们将训练我们的生成器一点点，然后我们将回到循环的顶部。论文中的内部`for`循环对应于我们代码中的第二个`while`循环。
- en: 'What we are going to do now is we have a generator that is random at the moment
    [[1:29:06](https://youtu.be/ondivPiwQho?t=1h29m6s)]. So our generator will generate
    something that looks like the noise. First of all, we need to teach our discriminator
    to tell the difference between the noise and a bedroom — which shouldn’t be too
    hard you would hope. So we just do it in the usual way but there is a few little
    tweaks:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要做的是我们现在有一个随机的生成器。所以我们的生成器将生成看起来像噪音的东西。首先，我们需要教我们的鉴别器区分噪音和卧室之间的区别 - 你希望这不会太难。所以我们只是按照通常的方式做，但有一些小调整：
- en: We are going to grab a mini batch of real bedroom photos so we can just grab
    the next batch from our iterator, turn it into a variable.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将获取一小批真实卧室照片，这样我们就可以从迭代器中获取下一批，将其转换为变量。
- en: Then we are going to calculate the loss for that — so this is going to be how
    much the discriminator thinks this looks fake (“does the real one look fake?”).
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将计算损失——这将是鉴别器认为这看起来假的程度（“真实的看起来假吗？”）。
- en: Then we are going to create some fake images and to do that we will create some
    random noise, and we will stick it through our generator which at this stage is
    just a bunch of random weights. That will create a mini batch of fake images.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将创建一些假图像，为此我们将创建一些随机噪音，并将其通过我们的生成器，这个阶段它只是一堆随机权重。这将创建一个小批量的假图像。
- en: Then we will put that through the same discriminator module as before to get
    the loss for that (“how fake does the fake one look?”). Remember, when you do
    everything manually, you have to zero the gradients (`netD.zero_grad()`) in your
    loop. If you have forgotten about that, go back to the part 1 lesson where we
    do everything from scratch.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将通过与之前相同的鉴别器模块来获取该损失（“假的看起来有多假？”）。记住，当你手动做所有事情时，你必须在循环中将梯度归零（`netD.zero_grad()`）。如果你忘记了这一点，请回到第1部分课程，我们从头开始做所有事情。
- en: Finally, the total discriminator loss is equal to the real loss minus the fake
    loss.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，总鉴别器损失等于真实损失减去假损失。
- en: 'So you can see that here [[1:30:58](https://youtu.be/ondivPiwQho?t=1h30m58s)]:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以在这里看到：
- en: They don’t talk about the loss, they actually just talk about one of the gradient
    updates.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 他们没有谈论损失，实际上他们只谈论了一个梯度更新。
- en: 'In PyTorch, we don’t have to worry about getting the gradients, we can just
    specify the loss and call `loss.backward()` then discriminator’s `optimizer.step()`[[1:34:27](https://youtu.be/ondivPiwQho?t=1h34m27s)].
    There is one key step which is that we have to keep all of our weights which are
    the parameters in PyTorch module in the small range of -0.01 and 0.01\. Why? Because
    the mathematical assumptions that make this algorithm work only apply in a small
    ball. It is interesting to understand the math of why that is the case, but it’s
    very specific to this one paper and understanding it won’t help you understand
    any other paper, so only study it if you are interested. It is nicely explained
    and Jeremy thinks it’s fun but it won’t be information that you will reuse elsewhere
    unless you get super into GANs. He also mentioned that after the came out and
    improved Wasserstein GAN came out that said there are better ways to ensure that
    your weight space is in this tight ball which was to penalize gradients that are
    too high, so nowadays there are slightly different ways to do this. But this line
    of code is the key contribution and it is what makes it Wasserstein GAN:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，我们不必担心获取梯度，我们只需指定损失并调用`loss.backward()`，然后鉴别器的`optimizer.step()`。有一个关键步骤，即我们必须将PyTorch模块中的所有权重（参数）保持在-0.01和0.01的小范围内。为什么？因为使该算法工作的数学假设仅适用于一个小球。了解为什么这样是有趣的数学是有趣的，但这与这篇论文非常相关，了解它不会帮助你理解其他论文，所以只有在你感兴趣的情况下才去学习。Jeremy认为这很有趣，但除非你对GANs非常感兴趣，否则这不会是你在其他地方会重复使用的信息。他还提到，在改进的Wasserstein
    GAN出现后，有更好的方法来确保你的权重空间在这个紧密球内，即惩罚梯度过高，所以现在有稍微不同的方法来做这个。但这行代码是关键贡献，它是使Wasserstein
    GAN成功的关键：
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: At the end of this, we have a discriminator that can recognize real bedrooms
    and our totally random crappy generated images [[1:36:20](https://youtu.be/ondivPiwQho?t=1h36m20s)].
    Let’s now try and create some better images. So now set trainable discriminator
    to false, set trainable generator to true, zero out the gradients of the generator.
    Our loss again is `fw` (discriminator) of the generator applied to some more random
    noise. So it’s exactly the same as before where we did generator on the noise
    and then pass that to a discriminator, but this time, the thing that’s trainable
    is the generator, not the discriminator. In other words, in the pseudo code, the
    thing they update is Ɵ which is the generator’s parameters. So it takes noise,
    generate some images, try and figure out if they are fake or real, and use that
    to get gradients with respect to the generator, as opposed to earlier we got them
    with respect to the discriminator, and use that to update our weights with RMSProp
    with an alpha learning rate [[1:38:21](https://youtu.be/ondivPiwQho?t=1h38m21s)].
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这之后，我们有一个可以识别真实卧室和完全随机糟糕生成的图像的鉴别器。现在让我们尝试创建一些更好的图像。所以现在将可训练的鉴别器设置为false，将可训练的生成器设置为true，将生成器的梯度归零。我们的损失再次是生成器的“fw”（鉴别器）应用于一些更多的随机噪音。所以这与之前完全相同，我们对噪音进行生成，然后将其传递给鉴别器，但这次，可训练的是生成器，而不是鉴别器。换句话说，在伪代码中，更新的是θ，即生成器的参数。它接受噪音，生成一些图像，尝试弄清楚它们是假的还是真实的，并使用这些梯度来更新生成器的权重，而不是之前我们是根据鉴别器来获取梯度，并使用RMSProp和alpha学习率来更新我们的权重。
- en: '[PRE21]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You’ll see that it’s unfair that the discriminator is getting trained *ncritic*
    times (`d_iters` in above code) which they set to 5 for every time we train the
    generator once. And the paper talks a bit about this but the basic idea is there
    is no point making the generator better if the discriminator doesn’t know how
    to discriminate yet. So that’s why we have the second while loop. And here is
    that 5:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现鉴别器被训练*ncritic*次（上面代码中的d_iters），他们将其设置为5，每次我们训练生成器一次。论文中谈到了这一点，但基本思想是如果鉴别器还不知道如何区分，那么让生成器变得更好是没有意义的。这就是为什么我们有第二个while循环。这里是5：
- en: '[PRE22]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Actually something which was added in the later paper or maybe supplementary
    material is the idea that from time to time and a bunch of times at the start,
    you should do more steps at the discriminator to make sure that the discriminator
    is capable.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，稍后的论文中添加的内容或者可能是补充材料是，不时地在开始时，您应该在鉴别器上执行更多步骤，以确保鉴别器是有能力的。
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s train that for one epoch:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为一个时代进行训练：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Then let’s create some noise so we can generate some examples.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们创建一些噪音，这样我们就可以生成一些示例。
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'But before that, reduce the learning rate by 10 and do one more pass:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 但在此之前，将学习率降低10倍，并再进行一次训练：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then let’s use the noise to pass it to our generator, then put it through our
    denormalization to turn it back into something we can see, and then plot it:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们使用噪音传递给我们的生成器，然后通过我们的反标准化将其转换回我们可以看到的东西，然后绘制它：
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: And we have some bedrooms. These are not real bedrooms, and some of them don’t
    look particularly like bedrooms, but some of them look a lot like bedrooms, so
    that’s the idea. That’s GAN. The best way to think about GAN is it is like an
    underlying technology that you will probably never use like this, but you will
    use in lots of interesting ways. For example, we are going to use it to create
    a cycle GAN.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一些卧室。这些不是真实的卧室，有些看起来并不像卧室，但有些看起来很像卧室，这就是想法。这就是GAN。最好的方法是将GAN视为一种基础技术，你可能永远不会像这样使用它，但你会以许多有趣的方式使用它。例如，我们将使用它来创建一个循环GAN。
- en: '**Question**: Is there any reason for using RMSProp specifically as the optimizer
    as opposed to Adam etc. [[1:41:38](https://youtu.be/ondivPiwQho?t=1h41m38s)]?
    I don’t remember it being explicitly discussed in the paper. I don’t know if it’s
    just experimental or the theoretical reason. Have a look in the paper and see
    what it says.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：为什么要特别使用RMSProp作为优化器，而不是Adam等等？我不记得论文中有明确讨论过这个问题。我不知道这是实验性的还是理论上的原因。看看论文中是怎么说的。'
- en: '[From the forum](http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/211)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[来自论坛](http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/211)'
- en: From experimenting I figured that Adam and WGANs not just work worse — it causes
    to completely fail to train meaningful generator.
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过实验，我发现Adam和WGAN不仅效果更差 - 它导致生成器训练失败。
- en: ''
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'from WGAN paper:'
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来自WGAN论文：
- en: ''
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Finally, as a negative result, we report that WGAN training becomes unstable
    at times when one uses a momentum based optimizer such as Adam [8] (with β1>0)
    on the critic, or when one uses high learning rates. Since the loss for the critic
    is nonstationary, momentum based methods seemed to perform worse. We identified
    momentum as a potential cause because, as the loss blew up and samples got worse,
    the cosine between the Adam step and the gradient usually turned negative. The
    only places where this cosine was negative was in these situations of instability.
    We therefore switched to RMSProp [21] which is known to perform well even on very
    nonstationary problems*'
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*最后，作为一个负面结果，我们报告说当使用基于动量的优化器（如Adam [8]（具有β1>0））对评论者进行训练时，WGAN训练有时会变得不稳定，或者当使用高学习率时。由于评论者的损失是非平稳的，基于动量的方法似乎表现更差。我们确定动量可能是一个潜在原因，因为随着损失的增加和样本变得更糟，Adam步骤和梯度之间的余弦通常变为负值。这种余弦为负值的唯一情况是在这些不稳定的情况下。因此，我们转而使用RMSProp
    [21]，它被认为在非平稳问题上表现良好*'
- en: '**Question**: Which could be a reasonable way of detecting overfitting while
    training? Or of evaluating the performance of one of these GAN models once we
    are done training? In other words, how does the notion of train/val/test sets
    translate to GANs [[1:41:57](https://youtu.be/ondivPiwQho?t=1h41m57s)]? That is
    an awesome question, and there’s a lot of people who make jokes about how GANs
    is the one field where you don’t need a test set and people take advantage of
    that by making stuff up and saying it looks great. There are some famous problems
    with GANs, one of them is called Mode Collapse. Mode collapse happens where you
    look at your bedrooms and it turns out that there’s only three kinds of bedrooms
    that every possible noise vector maps to. You look at your gallery and it turns
    out they are all just the same thing or just three different things. Mode collapse
    is easy to see if you collapse down to a small number of modes, like 3 or 4\.
    But what if you have a mode collapse down to 10,000 modes? So there are only 10,000
    possible bedrooms that all of your noise vectors collapse to. You wouldn’t be
    able to see in the gallery view we just saw because it’s unlikely you would have
    two identical bedrooms out of 10,000\. Or what if every one of these bedrooms
    is basically a direct copy of one of the input — it basically memorized some input.
    Could that be happening? And the truth is, most papers don’t do a good job or
    sometimes any job of checking those things. So the question of how do we evaluate
    GANS and even the point of maybe we should actually evaluate GANs properly is
    something that is not widely enough understood even now. Some people are trying
    to really push. Ian Goodfellow was the first author on the most famous deep learning
    book and is the inventor of GANs and he’s been sending continuous stream of tweets
    reminding people about the importance of testing GANs properly. If you see a paper
    that claims exceptional GAN results, then this is definitely something to look
    at. Have they talked about mode collapse? Have they talked about memorization?
    And so forth.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**: 在训练过程中，检测过拟合的一个合理方法是什么？或者在训练结束后评估这些 GAN 模型的性能的一个方法是什么？换句话说，训练/验证/测试集的概念如何转化为
    GANs [[1:41:57](https://youtu.be/ondivPiwQho?t=1h41m57s)]？这是一个很棒的问题，很多人开玩笑说 GANs
    是唯一不需要测试集的领域，人们利用这一点编造东西并说看起来很棒。GANs 存在一些著名的问题，其中之一被称为模式崩溃。模式崩溃发生在你查看卧室时，结果发现只有三种卧室，每个可能的噪声向量都映射到这三种卧室中的一种。你查看画廊，结果发现它们都是相同的东西或者只有三种不同的东西。模式崩溃很容易看到，如果崩溃到一个很小的模式数量，比如
    3 或 4。但如果模式崩溃到 10,000 种模式怎么办？因此，只有 10,000 种可能的卧室，所有的噪声向量都崩溃到这些卧室。你不太可能在我们刚刚看到的画廊视图中看到，因为在
    10,000 种卧室中很少会有两个相同的卧室。或者如果每个卧室基本上是输入的直接副本 —— 它基本上记住了一些输入。这可能正在发生吗？事实是，大多数论文在检查这些问题方面做得不好，有时甚至根本不检查。因此，我们如何评估
    GANs 甚至也许我们应该真正正确地评估 GANs 是一个现在还不够广泛理解的问题。一些人正在努力推动。Ian Goodfellow 是最著名的深度学习书籍的第一作者，也是
    GANs 的发明者，他一直在发送持续的推文提醒人们测试 GANs 的重要性。如果你看到一篇声称有异常 GAN 结果的论文，那么这绝对值得关注。他们是否谈到了模式崩溃？他们是否谈到了记忆化？等等。'
- en: '**Question**: Can GANs be used for data augmentation [[1:45:33](https://youtu.be/ondivPiwQho?t=1h45m33s)]?
    Yeah, absolutely you can use GAN for data augmentation. Should you? I don’t know.
    There are some papers that try to do semi-supervised learning with GANs. I haven’t
    found any that are particularly compelling showing state-of-the-art results on
    really interesting datasets that have been widely studied. I’m a little skeptical
    and the reason I’m a little skeptical is because in my experience, if you train
    a model with synthetic data, the neural net will become fantastically good at
    recognizing the specific problems of your synthetic data and that’ll end up what
    it’s learning from. There are lots of other ways of doing semi-supervised models
    which do work well. There are some places that can work. For example, you might
    remember Otavio Good created that fantastic visualization in part 1 of the zooming
    conv net where it showed letter going through MNIST, he, at least at that time,
    was the number one in autonomous remote control car competitions, and he trained
    his model using synthetically augmented data where he basically took real videos
    of a car driving around the circuit and added fake people and fake other cars.
    I think that worked well because A. he is kind of a genius and B. because I think
    he had a well defined little subset that he had to work in. But in general, it’s
    really really hard to use synthetic data. I’ve tried using synthetic data and
    models for decades now (obviously not GANs because they’re pretty new) but in
    general it’s very hard to do. Very interesting research question.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**: GANs 可以用于数据增强吗 [[1:45:33](https://youtu.be/ondivPiwQho?t=1h45m33s)]？是的，绝对可以使用
    GAN 进行数据增强。你应该吗？我不知道。有一些论文尝试使用 GANs 进行半监督学习。我还没有找到任何特别引人注目的论文，在广泛研究的真正有趣的数据集上展示出最先进的结果。我有点怀疑，原因是在我的经验中，如果用合成数据训练模型，神经网络将变得极其擅长识别你合成数据的具体问题，并且最终学到的将是这些问题。还有很多其他方法可以做半监督模型，效果很好。有一些地方可以工作。例如，你可能还记得
    Otavio Good 在第一部分的缩放卷积网络中创建的那个奇妙的可视化，其中显示了字母通过 MNIST，他，至少在那个时候，是自动遥控汽车比赛中的第一名，他使用合成增强数据训练了他的模型，基本上是拿真实的汽车绕着赛道行驶的视频，然后添加了虚假的人和虚假的其他汽车。我认为这样做效果很好，因为
    A. 他有点天才，B. 因为我认为他有一个明确定义的小子集需要处理。但总的来说，使用合成数据真的很难。我尝试过几十年使用合成数据和模型（显然不包括 GANs，因为它们是相当新的），但总的来说，这很难做到。非常有趣的研究问题。'
- en: Cycle GAN [[1:41:08](https://youtu.be/ondivPiwQho?t=1h41m8s)]
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cycle GAN [[1:41:08](https://youtu.be/ondivPiwQho?t=1h41m8s)]
- en: '[Paper](https://arxiv.org/abs/1703.10593) / [Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/cyclegan.ipynb)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[Paper](https://arxiv.org/abs/1703.10593) / [Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/cyclegan.ipynb)'
- en: We are going to use cycle GAN to turn horses into zebras. You can also use it
    to turn Monet prints into photos or to turn photos of Yosemite in summer into
    winter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用cycle GAN将马变成斑马。您也可以使用它将莫奈的印刷品转变为照片，或将优胜美地夏季的照片转变为冬季。
- en: This is going to be really straight forward because it’s just a neural net [[1:44:46](https://youtu.be/ondivPiwQho?t=1h44m46s)].
    All we are going to do is we are going to create an input containing lots of zebra
    photos and with each one we’ll pair it with an equivalent horse photo and we’ll
    just train a neural net that goes from one to the other. Or you could do the same
    thing for every Monet painting — create a dataset containing the photo of the
    place …oh wait, that’s not possible because the places that Monet painted aren’t
    there anymore and there aren’t exact zebra versions of horses …how the heck is
    this going to work? This seems to break everything we know about what neural nets
    can do and how they do them.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这将非常简单，因为它只是一个神经网络。我们要做的就是创建一个包含大量斑马照片的输入，并将每个照片与等价的马照片配对，然后训练一个从一个到另一个的神经网络。或者您可以对每幅莫奈的画做同样的事情——创建一个包含该地点照片的数据集……哦等等，这不可能，因为莫奈绘制的地方已经不存在了，也没有确切的斑马版本的马……这将如何运作？这似乎违背了我们对神经网络能做什么以及它们如何做的一切认知。
- en: So somehow these folks at Berkeley cerated a model that can turn a horse into
    a zebra despite not having any photos. Unless they went out there and painted
    horses and took before-and-after shots but I believe they didn’t [[1:47:51](https://youtu.be/ondivPiwQho?t=1h47m51s)].
    So how the heck did they do this? It’s kind of genius.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 所以某种方式，这些伯克利的人创造了一个模型，可以将马变成斑马，尽管没有任何照片。除非他们出去画马并拍摄前后照片，但我相信他们没有。那么他们是如何做到的呢？这有点天才。
- en: The person I know who is doing the most interesting practice of cycle GAN right
    now is one of our students Helena Sarin [**@**glagolista](https://twitter.com/glagolista).
    She is the only artist I know of who is a cycle GAN artist.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道目前正在进行最有趣的cycle GAN实践的人是我们的学生Helena Sarin。她是我所知道的唯一一位cycle GAN艺术家。
- en: Here are some more of her amazing works and I think it’s really interesting.
    I mentioned at the start of this class that GANs are in the category of stuff
    that is not there yet, but it’s nearly there. And in this case, there is at least
    one person in the world who is creating beautiful and extraordinary artworks using
    GANs (specifically cycle GANs). At least a dozen people I know of who are just
    doing interesting creative work with neural nets more generally. And the field
    of creative AI is going to expand dramatically.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是她更多令人惊叹的作品，我觉得非常有趣。我在这堂课开始时提到，GANs属于尚未出现的东西，但它们几乎已经到位了。在这种情况下，世界上至少有一个人正在使用GANs（具体来说是cycle
    GANs）创作美丽而非凡的艺术作品。至少我知道有十几个人正在用神经网络进行有趣的创意工作。创意人工智能领域将会大幅扩展。
- en: Here is the basic trick [[1:50:11](https://youtu.be/ondivPiwQho?t=1h50m11s)].
    This is from the cycle GAN paper. We are going to have two images (assuming we
    are doing this with images). The key thing is they are not paired images, so we
    don’t have a dataset of horses and the equivalent zebras. We have bunch of horses,
    and bunch of zebras. Grab one horse *X*, grab one zebra *Y*. We are going to train
    a generator (what they call here a “mapping function”) that turns horse into zebra.
    We’ll call that mapping function *G* and we’ll create one mapping function (a.k.a.
    generator) that turns a zebra into a horse and we will call that *F.* We will
    create a discriminator just like we did before which is going to get as good as
    possible at recognizing real from fake horses so that will be *Dx.* Another discriminator
    which is going to be as good as possible at recognizing real from fake zebras,
    we will call that *Dy*. That is our starting point.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基本的技巧。这是来自cycle GAN论文。我们将有两幅图像（假设我们正在处理图像）。关键是它们不是配对的图像，所以我们没有一组马和等价斑马的数据集。我们有一堆马，一堆斑马。拿一匹马*X*，拿一匹斑马*Y*。我们将训练一个生成器（他们在这里称之为“映射函数”），将马变成斑马。我们将称之为映射函数*G*，并创建一个将斑马变成马的映射函数（也称为生成器），我们将称之为*F*。我们将创建一个鉴别器，就像以前一样，它将尽可能地识别真假马，我们将称之为*Dx*。另一个鉴别器，它将尽可能地识别真假斑马，我们将称之为*Dy*。这是我们的起点。
- en: The key thing to making this work [[1:51:27](https://youtu.be/ondivPiwQho?t=1h51m27s)]—
    so we are generating a loss function here (*Dx* and *Dy*). We are going to create
    something called **cycle-consistency loss** which says after you turn your horse
    into a zebra with your generator, and check whether or not I can recognize that
    it’s a real. We turn our horse into a zebra and then going to try and turn that
    zebra back into the same horse that we started with. Then we are going to have
    another function that is going to check whether this horse which are generated
    knowing nothing about *x* — generated entirely from this zebra *Y* is similar
    to the original horse or not. So the idea would be if your generated zebra doesn’t
    look anything like your original horse, you’ve got no chance of turning it back
    into the original horse. So a loss which compares *x-hat* to *x* is going to be
    really bad unless you can go into *Y* and back out again and you’re probably going
    to be able to do that if you’re able to create a zebra that looks like the original
    horse so that you know what the original horse looked like. And vice versa — take
    your zebra, turn it into a fake horse, and check that you can recognize that and
    then try and turn it back into the original zebra and check that it looks like
    the original.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使这个工作的关键[[1:51:27](https://youtu.be/ondivPiwQho?t=1h51m27s)] - 所以我们在这里生成一个损失函数(*Dx*和*Dy*)。我们将创建一个叫做**循环一致性损失**的东西，它说当你用生成器将你的马变成斑马后，检查我是否能识别它是真实的。我们将我们的马变成斑马，然后尝试将那只斑马再变回我们开始的同一匹马。然后我们将有另一个函数，它将检查这匹马是否与原始马相似，这匹马是完全由这只斑马*Y*生成的，不知道*x*的任何信息。所以想法是，如果你生成的斑马看起来一点也不像原始马，你就没有机会将其变回原始马。因此，将*x-hat*与*x*进行比较的损失会非常糟糕，除非你能进入*Y*再出来，如果你能够创建一个看起来像原始马的斑马，那么你可能能够做到这一点。反之亦然
    - 将你的斑马变成一个假马，检查你是否能识别它，然后尝试将其变回原始斑马并检查它是否看起来像原始的。
- en: So notice *F* (zebra to horse) and *G* (horse to zebra) are doing two things
    [[1:53:09](https://youtu.be/ondivPiwQho?t=1h53m9s)]. They are both turning the
    original horse into the zebra, and then turning the zebra back into the original
    horse. So there are only two generators. There isn’t a separate generator for
    the reverse mapping. You have to use the same generator that was used for the
    original mapping. So this is the cycle-consistency loss. I think this is genius.
    The idea that this is a thing that could even be possible. Honestly when this
    came out, it just never occurred to me as a thing that I could even try and solve.
    It seems so obviously impossible and then the idea that you can solve it like
    this — I just think it’s so darn smart.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意*F*（斑马到马）和*G*（马到斑马）正在做两件事。它们都将原始马变成斑马，然后将斑马再变回原始马。所以只有两个生成器。没有一个单独的生成器用于反向映射。你必须使用用于原始映射的相同生成器。这就是循环一致性损失。我认为这是天才。这种事情甚至可能存在的想法。老实说，当这一点出现时，我从未想过我甚至可以尝试解决这个问题。它似乎如此明显地不可能，然后你可以像这样解决它的想法
    - 我只是觉得这太聪明了。
- en: It’s good to look at the equations in this paper because they are good examples
    — they are written pretty simply and it’s not like some of the Wasserstein GAN
    paper which is lots of theoretical proofs and whatever else [[1:54:05](https://youtu.be/ondivPiwQho?t=1h54m5s)].
    In this case, they are just equations that lay out what’s going on. You really
    want to get to a point where you can read them and understand them.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 看这篇论文中的方程式是很好的，因为它们是很好的例子 - 它们写得相当简单，不像一些瓦瑟斯坦GAN论文那样，那些是很多理论证明和其他东西。在这种情况下，它们只是列出了正在发生的事情的方程式。你真的想要达到一个可以阅读并理解它们的程度。
- en: So we’ve got a horse *X* and a zebra *Y*[[1:54:34](https://youtu.be/ondivPiwQho?t=1h54m34s)].
    For some mapping function *G* which is our horse to zebra mapping function then
    there is a GAN loss which is a bit we are already familiar with it says we have
    a horse, a zebra, a fake zebra recognizer, and a horse-zebra generator. The loss
    is what we saw before — it’s our ability to draw one zebra out of our zebras and
    recognize whether it is real or fake. Then take a horse and turn it into a zebra
    and recognize whether that’s real or fake. You then do one minus the other (in
    this case, they have a log in there but the log is not terribly important). So
    this is the thing we just saw. That is why we did Wasserstein GAN first. This
    is just a standard GAN loss in math form.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有一匹马*X*和一只斑马*Y*。对于一些映射函数*G*，这是我们的马到斑马映射函数，然后有一个GAN损失，这是我们已经熟悉的一部分，它说我们有一匹马，一只斑马，一个假斑马识别器和一个马斑马生成器。损失就是我们之前看到的
    - 我们能够从我们的斑马中画出一只斑马并识别它是真实的还是假的。然后拿一匹马变成一只斑马并识别它是真实的还是假的。然后做一减另一个（在这种情况下，它们里面有一个对数，但对数并不是非常重要）。这就是我们刚刚看到的东西。这就是为什么我们先做了瓦瑟斯坦GAN。这只是一个标准的数学形式的GAN损失。
- en: '**Question**: All of this sounds awfully like translating in one language to
    another then back to the original. Have GANs or any equivalent been tried in translation
    [[1:55:54](https://youtu.be/ondivPiwQho?t=1h55m54s)]? [Paper from the forum](https://arxiv.org/abs/1711.00043).
    Back up to what I do know — normally with translation you require this kind of
    paired input (i.e. parallel text — “this is the French translation of this English
    sentence”). There has been a couple of recent papers that show the ability to
    create good quality translation models without paired data. I haven’t implemented
    them and I don’t understand anything I haven’t implemented, but they may well
    be doing the same basic idea. We’ll look at it during the week and get back to
    you.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所有这些听起来很像将一种语言翻译成另一种语言，然后再翻译回原来的语言。GANs 或任何等效物已经尝试过翻译吗？来自论坛的论文。回到我所知道的 —
    通常在翻译中，你需要这种配对的输入（即平行文本 — “这是这个英语句子的法语翻译”）。最近有几篇论文显示了在没有配对数据的情况下创建高质量翻译模型的能力。我还没有实施它们，我不理解我没有实施的任何东西，但它们很可能在做同样的基本想法。我们将在本周内研究一下，并回复您。
- en: '**Cycle-consistency loss** [[1:57:14](https://youtu.be/ondivPiwQho?t=1h57m14s)]:
    So we’ve got a GAN loss and the next piece is the cycle-consistency loss. So the
    basic idea here is that we start with our horse, use our zebra generator on that
    to create a zebra, use our horse generator on that to create a horse and compare
    that to the original horse. This double lines with the 1 is the L1 loss — sum
    of the absolute value of differences [[1:57:35](https://youtu.be/ondivPiwQho?t=1h57m35s)].
    Where else if this was 2, it would be the L2 loss so the 2-norm which would be
    the sum of the squared differences.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 循环一致性损失：所以我们有一个 GAN 损失，接下来是循环一致性损失。基本思想是我们从我们的马开始，使用我们的斑马生成器创建一匹斑马，然后使用我们的马生成器创建一匹马，并将其与原始马进行比较。这个双线与
    1 是 L1 损失 — 差异的绝对值的和。否则，如果这是 2，那么它将是 L2 损失，即平方差的和。
- en: 'We now know this squiggle idea which is from our horses grab a horse. This
    is what we mean by sample from a distribution. There’s all kinds of distributions
    but most commonly in these papers we’re using an empirical distribution, in other
    words we’ve got some rows of data, grab a row. So here, it is saying grab something
    from the data and we are going to call that thing *x*. To recapture:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道这个波浪线的想法是从我们的马抓取一匹马。这就是我们所说的从分布中取样。有各种各样的分布，但在这些论文中，我们最常用的是经验分布，换句话说，我们有一些数据行，抓取一行。所以这里，它是说从数据中抓取一些东西，我们将称那个东西为*x*。为了重新概括：
- en: From our horse pictures, grab a horse
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的马图片中，抓取一匹马
- en: Turn it into a zebra
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其变成斑马
- en: Turn it back into a horse
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其转换回马
- en: Compare it to the original and sum of the absolute values
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其与原始图像进行比较并求绝对值的和
- en: Do it for zebra to horse as well
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 也对斑马进行同样的操作
- en: And add the two together
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将两者相加
- en: That is our cycle-consistency loss.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的循环一致性损失。
- en: '**Full objective** [[1:58:54](https://youtu.be/ondivPiwQho?t=1h58m54s)]'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 完整目标
- en: 'Now we get our loss function and the whole loss function depends on:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了我们的损失函数，整个损失函数取决于：
- en: our horse generator
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的马生成器
- en: a zebra generator
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个斑马生成器
- en: our horse recognizer
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的马识别器
- en: our zebra recognizer (a.k.a. discriminator)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的斑马识别器（又名鉴别器）
- en: 'We are going to add up :'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加起来：
- en: the GAN loss for recognizing horses
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于识别马的 GAN 损失
- en: GAN loss for recognizing zebras
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于识别斑马的 GAN 损失
- en: the cycle-consistency loss for our two generators
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们两个生成器的循环一致性损失
- en: We have a lambda here which hopefully we are kind of used to this idea now that
    is when you have two different kinds of loss, you chuck in a parameter there you
    can multiply them by so they are about the same scale [[1:59:23](https://youtu.be/ondivPiwQho?t=1h59m23s)].
    We did a similar thing with our bounding box loss compared to our classifier loss
    when we did the localization.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里有一个 lambda，希望我们现在对这个想法有点习惯了，当你有两种不同的损失时，你可以加入一个参数，这样你可以将它们乘以一个相同的比例。我们在定位时也对我们的边界框损失与分类器损失做了类似的事情。
- en: Then for this loss function, we are going to try to maximize the capability
    of the discriminators to discriminate, whilst minimizing that for the generators.
    So the generators and the discriminators are going to be facing off against each
    other. When you see this *min max* thing in papers, it basically means this idea
    that in your training loop, one thing is trying to make something better, the
    other is trying to make something worse, and there’re lots of ways to do it but
    most commonly, you’ll alternate between the two. You will often see this just
    referred to in math papers as min-max. So when you see min-max, you should immediately
    think **adversarial training**.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对于这个损失函数，我们将尝试最大化鉴别器的辨别能力，同时最小化生成器的辨别能力。因此，生成器和鉴别器将面对面地对抗。当你在论文中看到这个 *min
    max* 时，基本上意味着在你的训练循环中，一个东西试图让某事变得更好，另一个东西试图让某事变得更糟，有很多方法可以做到，但最常见的是你会在两者之间交替。你经常会在数学论文中看到这个被简称为
    min-max。所以当你看到 min-max 时，你应该立即想到对抗训练。
- en: Implementing cycle GAN [[2:00:41](https://youtu.be/ondivPiwQho?t=2h41s)]
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施循环 GAN
- en: Let’s look at the code. We are going to do something almost unheard of which
    is I started looking at somebody else’s code and I was not so disgusted that I
    threw the whole thing away and did it myself. I actually said I quite like this,
    I like it enough I’m going to show it to my students. [This](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)
    is where the code came from, and this is one of the people that created the original
    code for cycle GANs and they created a PyTorch version. I had to clean it up a
    little bit but it’s actually pretty darn good. The cool thing about this is that
    you are now going to get to see almost all the bits of fast.ai or all the relevant
    bits of fast.ai written in a different way by somebody else. So you’re going to
    get to see how they do datasets, data loaders, models, training loops, and so
    forth.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码。我们将要做一些几乎闻所未闻的事情，那就是我开始查看别人的代码，但并没有对整个东西感到恶心，然后自己重新做。我实际上说我相当喜欢这个，我喜欢它到足以向我的学生展示。这是代码的来源，这是一个为循环GAN创建原始代码的人之一，他们创建了一个PyTorch版本。我不得不稍微整理一下，但实际上它还是相当不错的。这个酷的地方是，你现在将看到几乎所有fast.ai的部分，或者其他相关的fast.ai部分，是由其他人以不同的方式编写的。所以你将看到他们如何处理数据集、数据加载器、模型、训练循环等等。
- en: You’ll find there is a `cgan` directory [[2:02:12](https://youtu.be/ondivPiwQho?t=2h2m12s)]
    which is basically nearly the original with some cleanups which I hope to submit
    as a PR sometime . It was written in a way that unfortunately made it a bit over
    connected to how they were using it as a script, so I cleaned it up a little bit
    so I could use it as a module. But other than that, it’s pretty similar.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现有一个`cgan`目录，这基本上几乎是原始的，只是做了一些清理，我希望有一天能提交为PR。它是以一种不幸地使它与他们作为脚本使用的方式过于连接的方式编写的，所以我稍微整理了一下，以便我可以将其用作模块。但除此之外，它还是相当相似的。
- en: '[PRE28]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: So `cgan` is their code copied from their github repo with some minor changes.
    The way `cgan` mini library has been set up is that the configuration options,
    they are assuming, are being passed into like a script. So they have `TrainOptions().parse`
    method and I’m basically passing in an array of script options (where’s my data,
    how many threads, do I want to dropout, how many iterations, what am I going to
    call this model, which GPU do I want run it on). That gives us an `opt` object
    which you can see what it contains. You’ll see that it contains some things we
    didn’t mention that is because it has defaults for everything else that we didn’t
    mention.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 所以`cgan`是他们从github仓库复制的代码，做了一些小的改动。`cgan`迷你库的设置方式是，它假设配置选项是被传递到像脚本一样。所以他们有`TrainOptions().parse`方法，我基本上传入一个脚本选项的数组（我的数据在哪里，有多少线程，我想要丢弃吗，我要迭代多少次，我要怎么称呼这个模型，我要在哪个GPU上运行）。这给我们一个`opt`对象，你可以看到它包含了什么。你会看到它包含了一些我们没有提到的东西，那是因为它对我们没有提到的其他所有东西都有默认值。
- en: '[PRE29]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: So rather than using fast.ai stuff, we are going to largely use cgan stuff.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们不再使用fast.ai的东西，我们将主要使用cgan的东西。
- en: '[PRE30]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The first thing we are going to need is a data loader. So this is also a great
    opportunity for you again to practice your ability to navigate through code with
    your editor or IDE of choice. We are going to start with `CreateDataLoader`. You
    should be able to go find symbol or in vim tag to jump straight to `CreateDataLoader`
    and we can see that’s creating a `CustomDatasetDataLoader`. Then we can see `CustomDatasetDataLoader`
    is a `BaseDataLoader`. We can see that it’s going to use a standard PyTorch DataLoader,
    so that’s good. We know if you are going to use a standard PyTorch DataLoader,
    you have pass it a dataset, and we know that a dataset is something that contains
    a length and an indexer so presumably when we look at `CreateDataset` it’s going
    to do that.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要的是一个数据加载器。这也是一个很好的机会，让你再次练习使用你选择的编辑器或IDE浏览代码的能力。我们将从`CreateDataLoader`开始。你应该能够找到符号或在vim标签中直接跳转到`CreateDataLoader`，我们可以看到它创建了一个`CustomDatasetDataLoader`。然后我们可以看到`CustomDatasetDataLoader`是一个`BaseDataLoader`。我们可以看到它将使用标准的PyTorch
    DataLoader，这很好。我们知道如果要使用标准的PyTorch DataLoader，你需要传递一个数据集，我们知道数据集是包含长度和索引器的东西，所以当我们查看`CreateDataset`时，它应该会这样做。
- en: Here is `CreateDataset` and this library does more than just cycle GAN — it
    handles both aligned and unaligned image pairs [[2:04:46](https://youtu.be/ondivPiwQho?t=2h4m46s)].
    We know that our image pairs are unaligned so we are going to `UnalignedDataset`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`CreateDataset`，这个库不仅仅是循环GAN - 它处理对齐和不对齐的图像对。我们知道我们的图像对是不对齐的，所以我们要使用`UnalignedDataset`。
- en: 'As expected, it has `__getitem__` and `__len__`. For length, A and B are our
    horses and zebras, we got two sets, so whichever one is longer is the length of
    the `DataLoader`. `__getitem__` is going to:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，它有`__getitem__`和`__len__`。对于长度，A和B是我们的马和斑马，我们有两组，所以较长的那个将是`DataLoader`的长度。`__getitem__`将会：
- en: Randomly grab something from each of our two horses and zebras
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机从我们的两匹马和斑马中抓取一些东西
- en: Open them up with pillow (PIL)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用Pillow（PIL）打开它们
- en: Run them through some transformations
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一些转换运行它们
- en: Then we could either be turning horses into zebras or zebras into horses, so
    there’s some direction
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们可以把马变成斑马，或者把斑马变成马，所以有一些方向
- en: Return our horse, zebra, a path to the horse, and a path of zebra
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回我们的马、斑马、马的路径和斑马的路径
- en: Hopefully you can kind of see that this is looking pretty similar to the kind
    of things fast.ai does. Fast.ai obviously does quite a lot more when it comes
    to transforms and performance, but remember, this is research code for this one
    thing and it’s pretty cool that they did all this work.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能看到这看起来与fast.ai所做的事情非常相似。当涉及到转换和性能时，fast.ai显然做了更多，但请记住，这是为这个特定事情的研究代码，他们做了这么多工作，这是相当酷的。
- en: '[PRE31]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We’ve got a data loader so we can go and load our data into it [[2:06:17](https://youtu.be/ondivPiwQho?t=2h6m17s)].
    That will tell us how many mini-batches are in it (that’s the length of the data
    loader in PyTorch).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个数据加载器，所以我们可以将我们的数据加载到其中[[2:06:17](https://youtu.be/ondivPiwQho?t=2h6m17s)]。这将告诉我们其中有多少个小批次（这是PyTorch数据加载器的长度）。
- en: Next step is to create a model. Same idea, we’ve got different kind of models
    and we’re going to be doing a cycle GAN.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个模型。同样的想法，我们有不同类型的模型，我们将要做一个循环GAN。
- en: Here is our `CycleGANModel`. There is quite a lot of stuff in `CycleGANModel`,
    so let’s go through and find out what’s going to be used. At this stage, we’ve
    just called initializer so when we initialize it, it’s going to go through and
    define two generators which is not surprising a generator for our horses and a
    generator for zebras. There is some way for it to generate a pool of fake data
    and then we’re going to grab our GAN loss, and as we talked about our cycle-consistency
    loss is an L1 loss. They are going to use Adam, so obviously for cycle GANS they
    found Adam works pretty well. Then we are going to have an optimizer for our horse
    discriminator, an optimizer for our zebra discriminator, and an optimizer for
    our generator. The optimizer for the generator is going to contain the parameters
    both for the horse generator and the zebra generator all in one place.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的`CycleGANModel`。`CycleGANModel`中有相当多的内容，所以让我们逐步找出将要使用的内容。在这个阶段，我们只是调用了初始化器，所以当我们初始化它时，它将会定义两个生成器，一个用于我们的马，一个用于斑马。它有一种方法来生成一组假数据，然后我们将获取我们的GAN损失，正如我们所讨论的，我们的循环一致性损失是一个L1损失。他们将使用Adam，显然对于循环GAN，他们发现Adam效果很好。然后我们将为我们的马判别器、斑马判别器和生成器各自创建一个优化器。生成器的优化器将包含马生成器和斑马生成器的参数，所有这些都在一个地方。
- en: So the initializer is going to set up all of the different networks and loss
    functions we need and they are going to be stored inside this `model` [[2:08:14](https://youtu.be/ondivPiwQho?t=2h8m14s)].
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，初始化器将设置我们需要的所有不同网络和损失函数，并将它们存储在这个`model`中[[2:08:14](https://youtu.be/ondivPiwQho?t=2h8m14s)]。
- en: '[PRE32]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: It then prints out and shows us exactly the PyTorch model we have. It’s interesting
    to see that they are using ResNets and so you can see the ResNets look pretty
    familiar, so we have conv, batch norm, Relu. `InstanceNorm` is just the same as
    batch norm basically but it applies to one image at a time and the difference
    isn’t particularly important. And you can see they are doing reflection padding
    just like we are. You can kind of see when you try to build everything from scratch
    like this, it is a lot of work and you can forget the nice little things that
    fast.ai does automatically for you. You have to do all of them by hand and only
    you end up with a subset of them. So over time, hopefully soon, we’ll get all
    of this GAN stuff into fast.ai and it’ll be nice and easy.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然后打印出并向我们展示我们拥有的PyTorch模型。看到他们正在使用ResNets，你会发现ResNets看起来非常熟悉，所以我们有卷积、批量归一化、Relu。`InstanceNorm`基本上与批量归一化相同，但它是针对一幅图像应用的，区别并不特别重要。你可以看到他们正在做反射填充，就像我们一样。当你尝试像这样从头开始构建所有东西时，这是很多工作，你可能会忘记fast.ai自动为你做的一些好事。你必须手动完成所有这些工作，最终只能得到其中的一部分。所以随着时间的推移，希望很快，我们将把所有这些GAN内容整合到fast.ai中，这将变得简单而容易。
- en: We’ve got our model and remember the model contains the loss functions, generators,
    discriminators, all in one convenient place [[2:09:32](https://youtu.be/ondivPiwQho?t=2h9m32s)].
    I’ve gone ahead and copied and pasted and slightly refactored the training loop
    from their code so that we can run it inside the notebook. So this one should
    look a lot familiar. A loop to go through each epoch and a loop to go through
    the data. Before we did this, we set up `dataset`. This is actually not a PyTorch
    dataset, I think this is what they used slightly confusingly to talk about their
    combined what we would call a model data object — all the data that they need.
    Loop through that with `tqdm` to get a progress bar, and so now we can go through
    and see what happens in the model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有我们的模型，记住模型包含损失函数、生成器、判别器，所有这些都在一个方便的地方[[2:09:32](https://youtu.be/ondivPiwQho?t=2h9m32s)]。我已经复制、粘贴并稍微重构了他们代码中的训练循环，这样我们就可以在笔记本中运行它。所以这个应该看起来很熟悉。一个循环用于遍历每个epoch，一个循环用于遍历数据。在这之前，我们设置了`dataset`。实际上这不是一个PyTorch数据集，我认为这是他们稍微令人困惑地用来谈论他们的组合数据，我们称之为模型数据对象——他们需要的所有数据。用`tqdm`循环遍历它，以获得进度条，这样我们就可以看看模型中发生了什么。
- en: '[PRE33]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`set_input` [[2:10:32](https://youtu.be/ondivPiwQho?t=2h10m32s)]: It’s a different
    approach to what we do in fast.ai. This is kind of neat, it’s quite specific to
    cycle GANs but basically internally inside this model is this idea that we are
    going to go into our data and grab the appropriate one. We are either going horse
    to zebra or zebra to horse, depending on which way we go, `A` is either horse
    or zebra, and vice versa. If necessary put it on the appropriate GPU, then grab
    the appropriate paths. So the model now has a mini-batch of horses and a mini-batch
    of zebras.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`set_input`[[2:10:32](https://youtu.be/ondivPiwQho?t=2h10m32s)]：这是一种与fast.ai中所做的不同方法。这很巧妙，它相当特定于循环GAN，但基本上在这个模型内部的想法是，我们将进入我们的数据并获取适当的数据。我们要么将马转换为斑马，要么将斑马转换为马，取决于我们选择的方式，`A`要么是马要么是斑马，反之亦然。如果需要，将其放在适当的GPU上，然后获取适当的路径。因此，模型现在有一批马和一批斑马。'
- en: Now we optimize the parameters [[2:11:19](https://youtu.be/ondivPiwQho?t=2h11m19s)].
    It’s kind of nice to see it like this. You can see each step. First of all, try
    to optimize the generators, then try to optimize the horse discriminators, then
    try to optimize the zebra discriminator. `zero_grad()` is a part of PyTorch, as
    well as `step()`. So the interesting bit is the actual thing that does the back
    propagation on the generator.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们优化参数[[2:11:19](https://youtu.be/ondivPiwQho?t=2h11m19s)]。这样看起来很好。你可以看到每一步。首先，尝试优化生成器，然后尝试优化马判别器，然后尝试优化斑马判别器。`zero_grad()`是PyTorch的一部分，以及`step()`。因此，有趣的部分是实际执行生成器反向传播的部分。
- en: Here it is [[2:12:04](https://youtu.be/ondivPiwQho?t=2h12m4s)]. Let’s jump to
    the key pieces. There’s all the formula that we just saw in the paper. Let’s take
    a horse and generate a zebra. Let’s now use the discriminator to see if we can
    tell whether it’s fake or not (`pred_fake`). Then let’s pop that into our loss
    function which we set up earlier to get a GAN loss based on that prediction. Let’s
    do the same thing going the opposite direction using the opposite discriminator
    then put that through the loss function again. Then let’s do the cycle consistency
    loss. Again, we take our fake which we created and try and turn it back again
    into the original. Let’s use the cycle consistency loss function we created earlier
    to compare it to the real original. And here is that lambda — so there’s some
    weight that we used and that would set up actually we just use the default that
    they suggested in their options. Then do the same for the opposite direction and
    then add them all together. We then do the backward step. That’s it.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是[2:12:04]。让我们跳到关键部分。这里有我们刚刚在论文中看到的所有公式。让我们拿一匹马生成一只斑马。现在让我们使用鉴别器来看看我们是否能够判断它是假的还是真的（`pred_fake`）。然后让我们将其放入我们之前设置的损失函数中，以基于该预测获得GAN损失。然后让我们以相反的方向做同样的事情，使用相反的鉴别器，然后再次通过损失函数。然后让我们做循环一致性损失。再次，我们拿我们创建的假的东西，尝试将其转回原始状态。让我们使用之前创建的循环一致性损失函数将其与真实原始状态进行比较。这里是那个lambda
    - 所以有一些权重我们使用了，实际上我们只是使用了他们在选项中建议的默认值。然后对相反的方向做同样的事情，然后将它们全部加在一起。然后进行反向步骤。就是这样。
- en: So we can do the same thing for the first discriminator [[2:13:50](https://youtu.be/ondivPiwQho?t=2h13m50s)].
    Since basically all the work has been done now, there’s much less to do here.
    There that is. We won’t step all through it but it’s basically the same basic
    stuff that we’ve already seen.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以为第一个鉴别器做同样的事情[2:13:50]。因为基本上所有的工作现在都已经完成了，这里要做的事情就少得多了。就是这样。我们不会一步步走过来，但基本上是我们已经看到的相同的基本东西。
- en: So `optimize_parameters()` is calculating the losses and doing the optimizer
    step. From time to time, save and print out some results. Then from time to time,
    update the learning rate so they’ve got some learning rate annealing built in
    here as well. Kind of like fast.ai, they’ve got this idea of schedulers which
    you can then use to update your learning rates.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 所以`optimize_parameters()`正在计算损失并执行优化器步骤。不时保存并打印一些结果。然后不时更新学习率，所以他们在这里也有一些学习率退火的机制。有点像fast.ai，他们有这个调度器的概念，你可以用它来更新你的学习率。
- en: For those of you are interested in better understanding deep learning APIs,
    contributing more to fast.ai, or creating your own version of some of this stuff
    in some different back-end, it’s cool to look at a second API that covers some
    subset of some of the similar things to get a sense for how they are solving some
    of these problems and what the similarities/differences are.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对更好地理解深度学习API、更多地为fast.ai做贡献，或者在一些不同的后端中创建自己版本的一些东西感兴趣的人，看看第二个API是很酷的，它涵盖了一些类似的东西的一些子集，以便了解他们是如何解决这些问题的，以及相似之处/不同之处是什么。
- en: '[PRE34]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We train that for a little while and then we can just grab a few examples and
    here we have them [[2:15:29](https://youtu.be/ondivPiwQho?t=2h15m29s)]. Here are
    horses, zebras, and back again as horses.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了一段时间，然后我们可以随便拿几个例子，这里有它们[2:15:29]。这里有马、斑马，然后再变回马。
- en: It took me like 24 hours to train it even that far so it’s kind of slow [[2:16:39](https://youtu.be/ondivPiwQho?t=2h16m39s)].
    I know Helena is constantly complaining on Twitter about how long these things
    take. I don’t know how she’s so productive with them.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我花了大约24小时来训练它，所以它有点慢[2:16:39]。我知道Helena经常在Twitter上抱怨这些事情花费的时间有多长。我不知道她是如何在这些事情上如此高效的。
- en: '[PRE35]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'I will mention one more thing that just came out yesterday [[2:16:54](https://youtu.be/ondivPiwQho?t=2h16m54s)]:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我还要提到昨天刚出来的另一件事[2:16:54]：
- en: '[Multimodal Unsupervised Image-to-Image Translation](https://arxiv.org/abs/1804.04732)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[多模态无监督图像到图像翻译](https://arxiv.org/abs/1804.04732)'
- en: There is now a multi-modal image to image translation of unpaired. So you can
    basically now create different cats for instance from this dog.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有一种多模态的无监督图像到图像的翻译。所以你现在基本上可以从这只狗创建不同的猫。
- en: This is basically not just creating one example of the output that you want,
    but creating multiple ones. This came out yesterday or the day before. I think
    it’s pretty amazing. So you can kind of see how this technology is developing
    and I think there’s so many opportunities to maybe do this with music, speech,
    writing, or to create kind of tools for artists.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是创建你想要的输出的一个例子，而是创建多个例子。这是昨天或前天才出来的。我觉得这很惊人。所以你可以看到这项技术是如何发展的，我认为在音乐、语音、写作方面，或者为艺术家创造工具方面，可能有很多机会。
