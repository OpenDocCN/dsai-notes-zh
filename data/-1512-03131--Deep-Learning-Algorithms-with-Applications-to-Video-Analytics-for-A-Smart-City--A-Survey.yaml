- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 20:09:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:09:23'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1512.03131] Deep Learning Algorithms with Applications to Video Analytics
    for A Smart City: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1512.03131] 深度学习算法在智能城市视频分析中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1512.03131](https://ar5iv.labs.arxiv.org/html/1512.03131)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1512.03131](https://ar5iv.labs.arxiv.org/html/1512.03131)
- en: 'Deep Learning Algorithms with Applications to Video Analytics for A Smart City:
    A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习算法在智能城市视频分析中的应用：综述
- en: 'Li Wang,  and Dennis Sng L. Wang and D. Sng are with the Rapid-Rich Object
    Search (ROSE) Lab at the Nanyang Technological University, Singapore 637553 (e-mail:
    wa0002li@e.ntu.edu.sg; dennis.sng@ntu.edu.sg).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 李旺和丹尼斯·尚（Dennis Sng L. Wang 和 D. Sng）均来自新加坡南洋理工大学的快速富有对象搜索（ROSE）实验室，地址637553（电子邮件：wa0002li@e.ntu.edu.sg;
    dennis.sng@ntu.edu.sg）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep learning has recently achieved very promising results in a wide range
    of areas such as computer vision, speech recognition and natural language processing.
    It aims to learn hierarchical representations of data by using deep architecture
    models. In a smart city, a lot of data (e.g. videos captured from many distributed
    sensors) need to be automatically processed and analyzed. In this paper, we review
    the deep learning algorithms applied to video analytics of smart city in terms
    of different research topics: object detection, object tracking, face recognition,
    image classification and scene labeling.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习最近在计算机视觉、语音识别和自然语言处理等众多领域取得了非常有前景的成果。它旨在通过使用深层架构模型来学习数据的层次表示。在智能城市中，许多数据（例如来自多个分布式传感器的录像）需要自动处理和分析。本文综述了应用于智能城市视频分析的深度学习算法，涵盖了不同的研究主题：目标检测、目标跟踪、人脸识别、图像分类和场景标注。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep learning, smart city, video analytics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，智能城市，视频分析。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Asmart city aims to improve quality and performance of urban services by using
    digital technologies or information and communication technologies. Data analytics
    plays an important role in smart cities. Many sensors are installed in a smart
    city to capture a huge volume of data such as surveillance videos, environment
    and transportation data. To capture useful information from such big data, machine
    learning algorithms are often used and have achieved very promising results in
    a wide range of applications, e.g. video analytics. Therefore, leveraging on machine
    learning can facilitate smart city development.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 智能城市旨在通过使用数字技术或信息与通信技术来提高城市服务的质量和性能。数据分析在智能城市中扮演着重要角色。许多传感器被安装在智能城市中，以捕捉大量的数据，例如监控视频、环境和交通数据。为了从这些大数据中获取有用的信息，通常会使用机器学习算法，这些算法在视频分析等广泛应用中取得了非常有前景的成果。因此，利用机器学习可以促进智能城市的发展。
- en: 'Machine learning aims to develop the computer algorithms which can learn experience
    from example inputs and make data-driven predictions on unknown test data. Such
    algorithms can be divided into two categories: supervised learning (e.g. [[1](#bib.bib1)]
    [[2](#bib.bib2)]) and unsupervised learning (e.g. [[3](#bib.bib3)] [[4](#bib.bib4)]).
    Given labeled input and output pairs, supervised learning aims to find a mapping
    rule for predicting outputs of unknown inputs. In contrast, unsupervised learning
    focuses on exploring intrinsic characteristics of inputs. Supervised learning
    and unsupervised learning are complementary to each other. Since supervised learning
    leverages labels of inputs which are meaningful to human, it is easy to apply
    this kind of learning algorithms to pattern classification and data regression
    problems. However, supervised learning relies on labeled data which could cost
    a lot of manual works. Moreover, there are uncertainties and ambiguities in labels.
    In other words, the label for an object is not unique. To mitigate these problems,
    unsupervised learning can be used to handle intra-class variation as it does not
    require labels of data. In the past decades, machine learning methods have been
    applied to a wide range of applications such as bioinformatics, computer vision,
    medical diagnosis, natural language processing, robotics, sentiment analysis,
    speech recognition and stock market analysis, etc. In this paper, we focus on
    reviewing the recent achievements of deep learning which is a subfield of machine
    learning.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习旨在开发可以从示例输入中学习经验并对未知测试数据进行数据驱动预测的计算机算法。这些算法可以分为两类：监督学习（例如[[1](#bib.bib1)]
    [[2](#bib.bib2)]）和无监督学习（例如[[3](#bib.bib3)] [[4](#bib.bib4)]）。监督学习在给定标记输入和输出对的情况下，旨在找到一个映射规则，以预测未知输入的输出。相对而言，无监督学习则侧重于探索输入的内在特征。监督学习和无监督学习是互补的。由于监督学习利用对人类有意义的输入标签，它可以很容易地应用于模式分类和数据回归问题。然而，监督学习依赖于标记数据，这可能需要大量的手工工作。此外，标签中存在不确定性和模糊性。换句话说，一个对象的标签不是唯一的。为了缓解这些问题，可以使用无监督学习来处理类内变异，因为它不需要数据标签。在过去几十年中，机器学习方法已应用于广泛的领域，如生物信息学、计算机视觉、医学诊断、自然语言处理、机器人技术、情感分析、语音识别和股票市场分析等。本文重点回顾了深度学习这一机器学习子领域的最新成就。
- en: In contrast with shallow learning algorithms, deep learning aims to extract
    hierarchical representations from large-scale data (e.g. images and videos) by
    using deep architecture models with multiple layers of non-linear transformations.
    With such learned feature representations, it becomes easier to achieve better
    performance than using raw pixel values or hand-crafted features. The principle
    behind this success is that deep learning is able to disentangle different levels
    of abstractions embedded in observed data by elaborately designing the layer depth
    and width, and properly selecting features that are beneficial for learning tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与浅层学习算法相比，深度学习旨在通过使用具有多层非线性变换的深度架构模型，从大规模数据（例如图像和视频）中提取层次化表示。通过这种学习到的特征表示，相较于使用原始像素值或手工设计的特征，获得更好性能变得更加容易。成功的原理在于，深度学习能够通过精心设计的层深度和宽度，以及适当选择对学习任务有利的特征，解开嵌入在观察数据中的不同抽象层次。
- en: In fact, the history of deep learning starts at least from 1980, when Neocognitron
    [[5](#bib.bib5)] is proposed by Fukushima. In 1989, LeCun et al. [[6](#bib.bib6)]
    propose to apply backpropagation onto a deep neural network for handwritten ZIP
    code recognition. However, the training time on the network was too long for practical
    use. Also, deep neural networks have been studied in speech recognition for many
    years, but can hardly surpass the shallow generative models. It is due to the
    fact that deep learning architectures require large training data which was scarce
    in those early days. Hinton et al. [[7](#bib.bib7)] review these difficulties
    and claim their confidence of solving these issues for applying deep learning
    to speech recognition, since Hinton [[8](#bib.bib8)] has achieved breakthroughs
    on training multi-layer neural networks by pre-training one layer at a time as
    an unsupervised restricted Boltzmann machine and then using supervised backpropagation
    for fine-tuning. Since the breakthrough of deep learning, it has been applied
    to many other research areas besides speech recognition.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，深度学习的历史至少可以追溯到1980年，当时Fukushima提出了Neocognitron [[5](#bib.bib5)]。1989年，LeCun等人
    [[6](#bib.bib6)] 提出了将反向传播应用于深度神经网络用于手写ZIP码识别。然而，网络的训练时间过长，实际应用受限。此外，深度神经网络在语音识别领域研究多年，但很难超越浅层生成模型。这是因为深度学习架构需要大量的训练数据，而在早期这些数据非常稀缺。Hinton等人
    [[7](#bib.bib7)] 回顾了这些困难，并表示对解决这些问题以将深度学习应用于语音识别充满信心，因为Hinton [[8](#bib.bib8)]
    通过逐层预训练作为无监督限制玻尔兹曼机并使用监督反向传播进行微调，取得了训练多层神经网络的突破。自深度学习突破以来，它已被应用于语音识别之外的许多其他研究领域。
- en: '![Refer to caption](img/d809928c723385a5ea3f50975ca46d19.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d809928c723385a5ea3f50975ca46d19.png)'
- en: 'Figure 1: Illustration of the CNN proposed by LeCun et al. [[9](#bib.bib9)]
    for digits recognition.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LeCun等人 [[9](#bib.bib9)] 提出的用于数字识别的CNN示意图。
- en: 'Deep learning architectures have different variants such as Deep Belief Networks
    (DBN) [[10](#bib.bib10)], Convolutional Neural Networks (CNN) [[11](#bib.bib11)],
    Deep Boltzmann Machines (DBM) [[12](#bib.bib12)] and Stacked Denoising Auto-Encoders
    (SDAE) [[13](#bib.bib13)], etc. The most attractive model is Convolutional Neural
    Networks which have achieved very promising results in both computer vision and
    speech recognition. An illustration of the CNN proposed by LeCun et al. [[9](#bib.bib9)]
    for digits recognition is presented in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Deep Learning Algorithms with Applications to Video Analytics for A Smart City:
    A Survey"). As shown in the figure, a CNN usually consists of convolutional layers,
    pooling layers and fully connected layers. With loss layers on the top of the
    CNN, the whole network can be trained end-to-end by using the backpropagation
    algorithm. Compared to other deep feed-forward neural networks, a CNN is easier
    to train as it has fewer parameters to estimate. As a result, CNN has a wide range
    of applications such as image classification [[14](#bib.bib14)], face recognition
    [[15](#bib.bib15)], object tracking [[16](#bib.bib16)], pedestrian detection [[17](#bib.bib17)],
    attribute prediction [[18](#bib.bib18)], scene labeling [[19](#bib.bib19)], person
    re-identification [[20](#bib.bib20)], RGB-D object recognition [[21](#bib.bib21)],
    image labeling [[22](#bib.bib22)], scene image classification [[23](#bib.bib23)],
    speech recognition [[24](#bib.bib24)] and natural language processing [[25](#bib.bib25)],
    etc.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习架构有不同的变体，如深度信念网络（DBN） [[10](#bib.bib10)]、卷积神经网络（CNN） [[11](#bib.bib11)]、深度玻尔兹曼机（DBM）
    [[12](#bib.bib12)] 和堆叠去噪自编码器（SDAE） [[13](#bib.bib13)] 等。其中最吸引人的模型是卷积神经网络，它在计算机视觉和语音识别中都取得了非常有前景的结果。图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Learning Algorithms with Applications to Video
    Analytics for A Smart City: A Survey")展示了LeCun等人 [[9](#bib.bib9)] 提出的用于数字识别的CNN示意图。如图所示，CNN通常包括卷积层、池化层和全连接层。通过在CNN顶部添加损失层，整个网络可以通过使用反向传播算法进行端到端训练。与其他深度前馈神经网络相比，CNN更容易训练，因为它需要估计的参数较少。因此，CNN具有广泛的应用，如图像分类
    [[14](#bib.bib14)]、人脸识别 [[15](#bib.bib15)]、目标跟踪 [[16](#bib.bib16)]、行人检测 [[17](#bib.bib17)]、属性预测
    [[18](#bib.bib18)]、场景标注 [[19](#bib.bib19)]、人员再识别 [[20](#bib.bib20)]、RGB-D物体识别
    [[21](#bib.bib21)]、图像标注 [[22](#bib.bib22)]、场景图像分类 [[23](#bib.bib23)]、语音识别 [[24](#bib.bib24)]
    和自然语言处理 [[25](#bib.bib25)] 等。'
- en: Deep learning has received much attention from not only academic but also industry.
    For example, Geoff Hinton and Li Deng started their collaborations from 2009 in
    the focus of applying deep learning to large-scale speech recognition, in which
    the performance is significantly improved against the traditional generative models
    by using big training data and the correspondingly designed deep neural networks.
    Another exciting example is that Andrew Ng and Jeff Dean from the Google Brain
    team successfully extract object-level semantics (e.g. cats) from unlabeled YouTube
    videos by using a neural network with the self-taught capacity. In future, deep
    learning will have more and more real applications in industry.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习不仅受到学术界的关注，也引起了工业界的广泛关注。例如，Geoff Hinton 和 Li Deng 从 2009 年开始合作，专注于将深度学习应用于大规模语音识别，通过使用大规模训练数据和相应设计的深度神经网络显著提升了性能，相比传统生成模型有了显著改进。另一个令人兴奋的例子是，Google
    Brain 团队的 Andrew Ng 和 Jeff Dean 成功地从未标记的 YouTube 视频中提取了对象级别的语义（例如猫），使用了具有自我学习能力的神经网络。未来，深度学习将在工业界有越来越多的实际应用。
- en: '![Refer to caption](img/4bf2f626a7684c8a3dcd3c06a73e7006.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4bf2f626a7684c8a3dcd3c06a73e7006.png)'
- en: 'Figure 2: Illustration of the parallel structure of the Nvidia GeForce GTX
    980.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Nvidia GeForce GTX 980 的并行结构示意图。
- en: 'Besides methodology breakthroughs and available big training data, the recent
    success for deep learning is also due to advances in hardware. Specifically, an
    electronic circuit called Graphics Processor Unit (GPU) is designed for accelerating
    the algorithms that need to process a large number of blocks of data, and is characteristic
    of its highly parallel structure. For example, Nvidia’s latest GPU, the GTX $980$,
    is based on their $10$th generation GPU architecture, called Maxwell, which delivers
    double the performance per watt compared to the previous generation. The GM $204$
    chip is composed of an array of $4$ Graphics Processing Clusters (GPCs), $16$
    Streaming Multiprocessors (SMs), and $4$ memory controllers. The GeForce GTX $980$
    uses the full complement of these architectural components. Its parallel structure
    is illustrated in Figure [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep Learning
    Algorithms with Applications to Video Analytics for A Smart City: A Survey").
    It is necessary to mention that the Nvidia company has made a lot of contributions
    for popularizing GPUs, e.g. Nvidia marketed the GeForce 256 as the world’s first
    GPU. Recently, GPUs are very popular in machine learning. For a general instance,
    Raina et al. [[26](#bib.bib26)] propose to accelerate the sparse coding algorithm
    [[27](#bib.bib27)] by using GPUs and finally speed up the previous method using
    a dual-core CPU up to $15$ times. In particular, Raina et al. [[26](#bib.bib26)]
    also report that the GPU speedup on learning Deep Belief Networks (DBNs) achieves
    up to $70$ times against a dual-core CPU implementation. For learning a four-layer
    DBN with $100$ million parameters, using GPU rather than CPU is able to reduce
    the required time from several weeks to around a single day.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '除了方法学上的突破和丰富的大规模训练数据，深度学习最近的成功还得益于硬件的进步。具体来说，一种名为图形处理单元（GPU）的电子电路被设计用来加速需要处理大量数据块的算法，并且具有高度并行的结构。例如，Nvidia
    最新的 GPU，GTX $980$，基于其第 $10$ 代 GPU 架构，称为 Maxwell，相比上一代在每瓦特的性能上提升了两倍。GM $204$ 芯片由
    $4$ 个图形处理集群（GPCs）、$16$ 个流式多处理器（SMs）和 $4$ 个内存控制器组成。GeForce GTX $980$ 使用了这些架构组件的全部配置。其并行结构如图[2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ Deep Learning Algorithms with Applications to Video
    Analytics for A Smart City: A Survey")所示。需要提到的是，Nvidia 公司在普及 GPU 方面做出了很多贡献，例如
    Nvidia 推出了 GeForce 256 作为世界上第一个 GPU。最近，GPU 在机器学习中非常流行。以 Raina 等人的研究为例 [[26](#bib.bib26)]
    提出通过使用 GPU 加速稀疏编码算法 [[27](#bib.bib27)]，最终使得使用双核 CPU 的先前方法加速了 $15$ 倍。特别地，Raina
    等人 [[26](#bib.bib26)] 还报告说，在深度信念网络（DBNs）的学习中，GPU 的加速效果相比于双核 CPU 实现可达到 $70$ 倍。对于学习一个具有
    $1$ 亿参数的四层 DBN，使用 GPU 而非 CPU 能将所需时间从几周减少到大约一天。'
- en: Smart city is so-called “smart” as it has the capability of computing and analyzing
    urban data collected from e.g. monitoring systems, government agencies, commercial
    companies and social networking websites. Since deep learning is suitable for
    handling large-scale data, it can be used to process and analyze millions of video
    data captured from the distributed sensors in a smart city. Regarding such data,
    there are many active research topics such as object detection, object tracking,
    face recognition, image classification and scene labeling. In the following sections,
    we review the state-of-the-art deep learning algorithms in these application areas.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 智能城市之所以被称为“智能”，是因为它具有计算和分析来自监控系统、政府机构、商业公司和社交网络网站等城市数据的能力。由于深度学习适合处理大规模数据，它可以用来处理和分析来自智能城市分布式传感器的数百万条视频数据。对于这些数据，存在许多活跃的研究主题，如对象检测、对象跟踪、人脸识别、图像分类和场景标注。在以下章节中，我们回顾了这些应用领域的最先进深度学习算法。
- en: II Object Detection
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 对象检测
- en: Object detection aims to precisely locate interested objects in video frames.
    Many interesting works have been proposed for object detection by using deep learning
    algorithms. We review some representative works as follows.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检测旨在精确定位视频帧中的感兴趣对象。许多有趣的工作已经通过深度学习算法提出用于对象检测。我们回顾了一些具有代表性的工作。
- en: '![Refer to caption](img/ef02a52fe53b35b9079bfb8f91e50cdd.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ef02a52fe53b35b9079bfb8f91e50cdd.png)'
- en: 'Figure 3: Illustration of object detection as DNN-based regression proposed
    by Szegedy et al. [[28](#bib.bib28)].'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: Szegedy 等人 [[28](#bib.bib28)] 提出的基于 DNN 的回归对象检测示意图。'
- en: '![Refer to caption](img/11d01beb5533ef0ecedfc4407689fdbf.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/11d01beb5533ef0ecedfc4407689fdbf.png)'
- en: 'Figure 4: Illustration of multi-scale strategy for refining detection precision
    proposed by Szegedy et al. [[28](#bib.bib28)].'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: Szegedy 等人 [[28](#bib.bib28)] 提出的用于提高检测精度的多尺度策略示意图。'
- en: 'Szegedy et al. [[28](#bib.bib28)] modify deep convolutional neural networks
    [[11](#bib.bib11)] by replacing the last layer with a regression layer to produce
    a binary mask of the object bounding box as illustrated in Figure [3](#S2.F3 "Figure
    3 ‣ II Object Detection ‣ Deep Learning Algorithms with Applications to Video
    Analytics for A Smart City: A Survey"). Moreover, a multi-scale strategy (see
    Figure [4](#S2.F4 "Figure 4 ‣ II Object Detection ‣ Deep Learning Algorithms with
    Applications to Video Analytics for A Smart City: A Survey")) is proposed for
    the DNN mask generation to refine detection precision. As a result, the average
    precision $0.305$ over $20$ classes on Pascal Visual Object Challenge (VOC) $2007$
    can be achieved by applying the proposed network a few dozen times per input image.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Szegedy 等人 [[28](#bib.bib28)] 通过将最后一层替换为回归层，从而生成对象边界框的二进制掩码，对深度卷积神经网络 [[11](#bib.bib11)]
    进行了修改，如图 [3](#S2.F3 "图 3 ‣ II 对象检测 ‣ 深度学习算法在智能城市视频分析中的应用：综述") 所示。此外，提出了一种多尺度策略（见图
    [4](#S2.F4 "图 4 ‣ II 对象检测 ‣ 深度学习算法在智能城市视频分析中的应用：综述")），用于 DNN 掩码生成，以提高检测精度。因此，通过对每个输入图像应用该网络几十次，可以在
    Pascal Visual Object Challenge (VOC) $2007$ 上实现 $20$ 类的平均精度 $0.305$。
- en: '![Refer to caption](img/136ed650480b43c16b90512ea52ab3c4.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/136ed650480b43c16b90512ea52ab3c4.png)'
- en: 'Figure 5: Illustration of R-CNN: Regions with CNN features proposed by Girshick
    et al. [[29](#bib.bib29)].'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: Girshick 等人 [[29](#bib.bib29)] 提出的 R-CNN: CNN 特征区域示意图。'
- en: 'Different from the Szegedy’s method [[28](#bib.bib28)], Girshick et al. [[29](#bib.bib29)]
    propose a bottom-up region proposal based deep model for object detection. Figure [5](#S2.F5
    "Figure 5 ‣ II Object Detection ‣ Deep Learning Algorithms with Applications to
    Video Analytics for A Smart City: A Survey") presents an overview of the method.
    First, around $2000$ region proposals are generated within the input image. For
    each proposal, a large convolutional neural network is used to extract features.
    Finally, each region is classified by using class-specific linear SVMs. It is
    reported that using this method can significantly improve detection performance
    on $VOC~{}2012$ by more than $30\%$ in terms of mean average precision (mAP).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Szegedy 的方法 [[28](#bib.bib28)] 不同，Girshick 等人 [[29](#bib.bib29)] 提出了一个自下而上的区域提议深度模型用于对象检测。图
    [5](#S2.F5 "图 5 ‣ II 对象检测 ‣ 深度学习算法在智能城市视频分析中的应用：综述") 展示了该方法的概述。首先，在输入图像中生成约 $2000$
    个区域提议。对于每个提议，使用一个大型卷积神经网络提取特征。最后，使用特定类别的线性 SVM 对每个区域进行分类。据报道，使用该方法可以显著提高 $VOC~{}2012$
    上的检测性能，mAP 提高超过 $30\%$。
- en: Similarly, Erhan et al. [[30](#bib.bib30)] propose a saliency-inspired deep
    neural networks to detect any object of interest. However, a small number of bounding
    boxes are generated as object candidates by using a deep neural network in a class
    agnostic manner. In this work, object detection is defined as a regression problem
    to the coordinates of bounding boxes. For the training part, an assignment problem
    between predictions and groundtruth boxes is solved to update box coordinates,
    their confidences and the learned features by using backpropagation. In summary,
    a deep neural network is customized towards object localization problem.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Erhan等人[[30](#bib.bib30)]提出了一种受显著性启发的深度神经网络来检测任何感兴趣的对象。然而，使用深度神经网络以类无关的方式生成了少量的边界框作为对象候选。在这项工作中，对象检测被定义为一个回归问题，回归到边界框的坐标。在训练部分，通过反向传播解决预测与真实框之间的分配问题，以更新框坐标、置信度和学习到的特征。总之，深度神经网络被定制用于对象定位问题。
- en: Object detection has a wide range of smart city applications, such as pedestrian
    detection, on-road vehicle detection, unattended object detection. Deep learning
    algorithms are able to handle large variations of different objects. As a result,
    the smart city systems using deep models are more robust to large-scale real data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检测在智能城市应用中有广泛的应用，如行人检测、道路车辆检测、无人看管的物体检测。深度学习算法能够处理不同对象的大范围变化。因此，使用深度模型的智能城市系统对大规模真实数据更加稳健。
- en: III Object Tracking
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 对象跟踪
- en: Object tracking is intended to locate a target object in a video sequence given
    its location in the first frame. Recently, some deep learning based tracking algorithms
    have achieved very promising results. We review some representative works as follows.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对象跟踪旨在根据视频序列中首帧的位置信息来定位目标对象。最近，一些基于深度学习的跟踪算法取得了非常有前景的结果。我们回顾了一些代表性的工作如下。
- en: '![Refer to caption](img/c407e4861935ad5a12f4900372581a75.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c407e4861935ad5a12f4900372581a75.png)'
- en: 'Figure 6: Illustration of the network architecture proposed by Wang and Yeung
    [[29](#bib.bib29)]: (a) denoising autoencoder; (b) stacked denoising autoencoder;
    (c) network for online tracking.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: Wang和Yeung[[29](#bib.bib29)]提出的网络架构说明：（a）去噪自编码器；（b）堆叠去噪自编码器；（c）在线跟踪网络。'
- en: 'Wang and Yeung [[31](#bib.bib31)] propose to learn deep and compact features
    for visual tracking by using stacked denoising autoencoder [[32](#bib.bib32)].
    The network architecture is illustrated in Figure [6](#S3.F6 "Figure 6 ‣ III Object
    Tracking ‣ Deep Learning Algorithms with Applications to Video Analytics for A
    Smart City: A Survey"). It is reported that using the learned deep features can
    outperform other $7$ state-of-the-art trackers on $10$ sequences in terms of two
    tracking measurements: average center error ($7.3$ pixels) and average success
    rate ($85.5$%). Additionally, the proposed tracker with deep features achieved
    an average frame rate of $15$fps which is suitable for real-time applications.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '王和杨[[31](#bib.bib31)]建议通过使用堆叠去噪自编码器[[32](#bib.bib32)]来学习深层且紧凑的特征用于视觉跟踪。网络架构如图[6](#S3.F6
    "Figure 6 ‣ III Object Tracking ‣ Deep Learning Algorithms with Applications to
    Video Analytics for A Smart City: A Survey")所示。报告称，使用学习到的深层特征在两个跟踪测量指标上优于其他$7$种最先进的跟踪器，分别是平均中心误差（$7.3$像素）和平均成功率（$85.5$%）。此外，提出的深层特征跟踪器实现了$15$fps的平均帧率，适用于实时应用。'
- en: '![Refer to caption](img/a73ae341b284ad5cf6e140faf9a2f07b.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a73ae341b284ad5cf6e140faf9a2f07b.png)'
- en: 'Figure 7: Overview of the network architecture proposed by Li et al. [[16](#bib.bib16)].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: Li等人[[16](#bib.bib16)]提出的网络架构概述。'
- en: 'Li et al. [[16](#bib.bib16)] propose to learn discriminative feature representations
    for visual tracking by using convolutional neural networks. An overview of the
    method is illustrated in Figure [7](#S3.F7 "Figure 7 ‣ III Object Tracking ‣ Deep
    Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey").
    It can be observed that a pool of multiple convolutional neural networks are utilized
    to maintain different kernels regarding all possible low-level cues, which aim
    to discriminate object patches from their surrounding background. Given a frame,
    the most prospective convolutional neural network in the pool is used to predict
    the new location of the target object. Meanwhile, the selected network is retrained
    using a warm-start backpropagation scheme. Also, we can observe from Figure [7](#S3.F7
    "Figure 7 ‣ III Object Tracking ‣ Deep Learning Algorithms with Applications to
    Video Analytics for A Smart City: A Survey") that a class-specific convolutional
    neural network is involved in the whole architecture, which is helpful for tracking
    a certain class of objects (e.g. a person’s face).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '李等人[[16](#bib.bib16)] 提出了使用卷积神经网络学习用于视觉跟踪的判别特征表示的方法。该方法的概述见图 [7](#S3.F7 "Figure
    7 ‣ III Object Tracking ‣ Deep Learning Algorithms with Applications to Video
    Analytics for A Smart City: A Survey")。可以观察到，使用了多个卷积神经网络池来维持所有可能低级线索的不同核，这些核旨在区分目标补丁和其周围背景。给定一帧图像，池中最有前景的卷积神经网络被用来预测目标物体的新位置。同时，选定的网络使用热启动反向传播方案进行再训练。同时，从图 [7](#S3.F7
    "Figure 7 ‣ III Object Tracking ‣ Deep Learning Algorithms with Applications to
    Video Analytics for A Smart City: A Survey") 可以观察到，整个架构中涉及一个类特定的卷积神经网络，这对跟踪某一类物体（例如，一个人的面部）非常有帮助。'
- en: '![Refer to caption](img/921a65affdaf18fcea4e24017fcedc48.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/921a65affdaf18fcea4e24017fcedc48.png)'
- en: 'Figure 8: Illustration of the stacked architecture and the adaptation module
    proposed by Wang et al. [[33](#bib.bib33)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：王等人[[33](#bib.bib33)] 提出的堆叠架构和适应模块的示意图。
- en: 'Wang et al. [[33](#bib.bib33)] propose to learn hierarchical features robust
    to both complicated motion transformations and target appearance changes. The
    stacked architecture and the adaptation module are illustrated in Figure [8](#S3.F8
    "Figure 8 ‣ III Object Tracking ‣ Deep Learning Algorithms with Applications to
    Video Analytics for A Smart City: A Survey"). First, the generic features robust
    to complicated motion transformations are offline learned from auxiliary video
    data by using a two-layer neural networks under the temporal constraints [[34](#bib.bib34)].
    Then, the pre-learned features are online adapted according to the specific target
    object sequence. As a result, the adapted features are able to capture appearance
    changes of target objects. For example, the proposed tracker can handle not only
    non-rigid deformation of a basketball player’s body but also the specific appearance
    changes. It is reported that using the feature learning algorithm can significantly
    improve tracking performance, especially on the sequences with complex motion
    transformations.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '王等人[[33](#bib.bib33)] 提出了一种学习对复杂运动变换和目标外观变化都具有鲁棒性的层次特征的方法。图 [8](#S3.F8 "Figure
    8 ‣ III Object Tracking ‣ Deep Learning Algorithms with Applications to Video
    Analytics for A Smart City: A Survey") 展示了堆叠架构和适应模块。首先，通过使用具有时间约束的双层神经网络，从辅助视频数据中离线学习到对复杂运动变换具有鲁棒性的通用特征[[34](#bib.bib34)]。然后，根据特定目标物体序列对预先学习的特征进行在线适应。因此，适应后的特征能够捕捉目标物体的外观变化。例如，所提出的跟踪器不仅可以处理篮球运动员身体的非刚性变形，还可以处理特定的外观变化。报告指出，使用特征学习算法可以显著提高跟踪性能，特别是在具有复杂运动变换的序列中。'
- en: Object tracking can be applied to surveillance systems of smart cities. It is
    important to automatically track suspected people or target vehicles for safety
    monitoring and urban flow management. Deep learning can leverage smart city big
    data to train deep models which are more robust to visual variations of target
    objects than traditional models. Therefore, smart city tracking systems can be
    enhanced by using deep learning algorithms for handling large amount of video
    data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 物体跟踪可以应用于智能城市的监控系统。自动跟踪可疑人员或目标车辆对于安全监控和城市流量管理至关重要。深度学习可以利用智能城市的大数据来训练深度模型，这些模型比传统模型对目标物体的视觉变化更具鲁棒性。因此，智能城市的跟踪系统可以通过使用深度学习算法来处理大量视频数据，从而得到增强。
- en: IV Face Recognition
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 人脸识别
- en: 'Face recognition consists of two main tasks: face verification and face identification.
    The former aims to determine whether the given two faces belong to the same person.
    The latter is intended to find the identities of the given faces from the known
    face set. Recently, many deep learning based algorithms have achieved very promising
    results in these two face recognition tasks. We review some of them as follows.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 面部识别包括两个主要任务：面部验证和面部识别。前者旨在确定给定的两张面孔是否属于同一个人。后者旨在从已知面孔集找到给定面孔的身份。近年来，许多基于深度学习的算法在这两个面部识别任务中取得了非常有前景的结果。我们将回顾其中的一些。
- en: '![Refer to caption](img/a72acf866831b65cc12ccaa1ba252073.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a72acf866831b65cc12ccaa1ba252073.png)'
- en: 'Figure 9: Illustration of the convolutional restricted Boltzmann machine used
    in Huang et al. [[35](#bib.bib35)].'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：黄等人使用的卷积限制玻尔兹曼机的示意图[[35](#bib.bib35)]。
- en: 'Huang et al. [[35](#bib.bib35)] propose to learn hierarchical features for
    face verification by using convolutional deep belief networks. The main contributions
    of this work are as follows: i) a local convolutional restricted Boltzmann machine
    is developed to adapt to the global structure in an object class (e.g. face);
    ii) deep learning is applied to local binary pattern representation [[36](#bib.bib36)]
    rather than raw pixel values to capture more complex characteristics of hand-crafted
    features; iii) learning the network architecture parameters is evaluated to be
    necessary for enhancing the multi-layer networks. The convolutional restricted
    Boltzmann machine used in the proposed method is illustrated in Figure [9](#S4.F9
    "Figure 9 ‣ IV Face Recognition ‣ Deep Learning Algorithms with Applications to
    Video Analytics for A Smart City: A Survey"). It is reported that using the learned
    representations can achieve comparable performance with state-of-the-art methods
    using hand-crafted features. Actually, the subsequent works have shown that deep
    features outperform hand-crafted features significantly.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 黄等人[[35](#bib.bib35)]提出通过使用卷积深度置信网络来学习面部验证的层次特征。这项工作的主要贡献如下：i) 开发了一个局部卷积限制玻尔兹曼机，以适应对象类别（例如面孔）的全局结构；ii)
    深度学习被应用于局部二值模式表示[[36](#bib.bib36)]而非原始像素值，以捕捉手工特征的更复杂特性；iii) 学习网络架构参数被评估为提升多层网络所必需的。图[9](#S4.F9
    "图 9 ‣ IV 面部识别 ‣ 应用于智能城市视频分析的深度学习算法：综述")中展示了所提出方法中使用的卷积限制玻尔兹曼机。报告指出，使用学习到的表示可以达到与使用手工特征的最先进方法相媲美的性能。实际上，后续的研究表明，深度特征显著优于手工特征。
- en: '![Refer to caption](img/99d512147ea1b86f7227d42ff367bf76.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/99d512147ea1b86f7227d42ff367bf76.png)'
- en: 'Figure 10: Overview of the nine-layer deep neural network used in Taigman et
    al. [[15](#bib.bib15)].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：Taigman 等人使用的九层深度神经网络概述[[15](#bib.bib15)]。
- en: 'Taigman et al. [[15](#bib.bib15)] propose a $3$D face model based face alignment
    algorithm and a face representation learned from a nine-layer deep neural network.
    An overview of the architecture of the deep network is illustrated in Figure [10](#S4.F10
    "Figure 10 ‣ IV Face Recognition ‣ Deep Learning Algorithms with Applications
    to Video Analytics for A Smart City: A Survey"). The first three convolutional
    layers are used to extract low-level features (e.g. edges and textures). The next
    three layers are locally connected to learn a different set of filters for each
    location of a face image since different regions have different local statistics.
    The top two layers are fully connected to capture correlations between features
    captured in different parts of a face image. At last, the output of the last layer
    is fed to a K-way softmax which predicts class labels. The objective of training
    is to maximize the probability of correct class by minimizing the cross-entropy
    loss for each training sample. It is shown that using the learned representations
    can achieve the near-human performance on the Labeled Faces in the Wild benchmark
    (LFW).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Taigman 等人 [[15](#bib.bib15)] 提出了基于 $3$D 面部模型的面部对齐算法和从九层深度神经网络中学习的面部表示。深度网络架构的概述见图
    [10](#S4.F10 "图 10 ‣ IV 面部识别 ‣ 深度学习算法在智能城市视频分析中的应用：综述")。前三个卷积层用于提取低级特征（如边缘和纹理）。接下来的三层是局部连接的，以学习每个面部图像位置的不同滤波器，因为不同区域具有不同的局部统计特性。顶部的两层是全连接的，用于捕捉面部图像不同部分之间的特征相关性。最后，最后一层的输出被送入一个
    K-way softmax，用于预测类别标签。训练的目标是通过最小化每个训练样本的交叉熵损失来最大化正确类别的概率。结果表明，使用学习到的表示可以在 Labeled
    Faces in the Wild 基准（LFW）上实现接近人类的表现。
- en: '![Refer to caption](img/30fbf569ba86117ca7ab4ad69b2c2c04.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/30fbf569ba86117ca7ab4ad69b2c2c04.png)'
- en: 'Figure 11: Illustration of the feature extraction process proposed in Sun et
    al. [[37](#bib.bib37)].'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：Sun 等人 [[37](#bib.bib37)] 提出的特征提取过程的示意图。
- en: 'Sun et al. [[37](#bib.bib37)] propose to learn so-called Deep hidden IDentity
    features (DeepID) for face verification. Figure [11](#S4.F11 "Figure 11 ‣ IV Face
    Recognition ‣ Deep Learning Algorithms with Applications to Video Analytics for
    A Smart City: A Survey") illustrates the feature extraction process. First, the
    local low-level features of an input face patch are extracted and fed into a ConvNet
    [[9](#bib.bib9)]. Then, the feature dimension gradually decreases to $160$ through
    several feed-forward layers, during which more global and high-level features
    are learned. Last, the identity class (among $10,000$ classes) of the face patch
    is predicted directly by using the $160$-dimensional DeepID. Rather than training
    a binary classifier for each face class, Sun et al. simultaneously classify all
    ConvNets regarding $10,000$ face identities. The advantages of this manipulation
    are as follows: i) effective features are extracted for face recognition by using
    the super learning capacity of neural networks; ii) the hidden features among
    all identities are shared by adding a strong regularization to ConvNets. It is
    reported that using the learned DeepID can achieve the near-human performance
    on the LFW dataset although only weakly aligned faces are used.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Sun 等人 [[37](#bib.bib37)] 提出了为面部验证学习所谓的深度隐含身份特征（DeepID）。图 [11](#S4.F11 "图 11
    ‣ IV 面部识别 ‣ 深度学习算法在智能城市视频分析中的应用：综述") 说明了特征提取过程。首先，提取输入面部补丁的局部低级特征，并将其输入到 ConvNet
    [[9](#bib.bib9)] 中。然后，通过几个前馈层，特征维度逐渐减少到 $160$，在此过程中学习到更多的全局和高级特征。最后，直接使用 $160$
    维 DeepID 预测面部补丁的身份类别（在 $10,000$ 个类别中）。Sun 等人并非为每个面部类别训练一个二分类器，而是同时对 $10,000$ 个面部身份进行所有
    ConvNet 的分类。这种操作的优势如下：i) 利用神经网络的超强学习能力提取有效的面部识别特征；ii) 通过对 ConvNets 添加强正则化来共享所有身份之间的隐藏特征。报告显示，虽然仅使用了弱对齐的面孔，但使用学习到的
    DeepID 在 LFW 数据集上可以实现接近人类的表现。
- en: '![Refer to caption](img/a0bb5dadd9207f28dbee760d9b63ad0c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a0bb5dadd9207f28dbee760d9b63ad0c.png)'
- en: 'Figure 12: Illustration of the basic idea of the approach [[38](#bib.bib38)].'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：方法基本思想的示意图 [[38](#bib.bib38)]。
- en: 'Lu et al. [[38](#bib.bib38)] develop a joint feature learning approach to automatically
    learn hierarchical representation from raw pixels for face recognition. Figure [12](#S4.F12
    "Figure 12 ‣ IV Face Recognition ‣ Deep Learning Algorithms with Applications
    to Video Analytics for A Smart City: A Survey") illustrates the basic idea of
    the proposed method. First, each face image is divided into several non-overlapping
    regions and feature weighting matrices are jointly learned. Then, the learned
    features in each region are pooled and represented as local histogram feature
    descriptors. Lastly, these local features are combined and concatenated into a
    longer feature vector for face representation. Moreover, the joint learning model
    is stacked into a deep architecture exploiting hierarchical information. As a
    result, the effectiveness of the proposed approach is demonstrated on five widely
    used face datasets.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'Lu 等人 [[38](#bib.bib38)] 发展了一种联合特征学习方法，用于自动从原始像素中学习面部识别的层次表示。图 [12](#S4.F12
    "Figure 12 ‣ IV Face Recognition ‣ Deep Learning Algorithms with Applications
    to Video Analytics for A Smart City: A Survey") 说明了该方法的基本思路。首先，将每张面部图像分成几个不重叠的区域，并联合学习特征加权矩阵。接着，将每个区域中学习到的特征进行汇聚，并表示为局部直方图特征描述符。最后，这些局部特征被组合并连接成一个更长的特征向量，用于面部表示。此外，联合学习模型被堆叠成一个深层架构，以利用层次信息。因此，所提方法在五个广泛使用的面部数据集上证明了其有效性。'
- en: Face recognition has been widely used in security systems and human-machine
    interaction systems. It is still a challenge for computer to automatically identify
    or verify a person due to large variations, e.g. illumination, pose and expression.
    Deep learning can utilize big data for training deep architecture models so as
    to obtain more powerful features for representing faces. In future, face recognition
    systems in smart cities will largely rely on hierarchical features learned from
    deep models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 面部识别已广泛应用于安全系统和人机交互系统。由于光照、姿态和表情等因素的巨大变化，计算机自动识别或验证一个人仍然是一个挑战。深度学习可以利用大数据来训练深层架构模型，从而获得更强大的特征以表示面部。未来，智能城市中的面部识别系统将主要依赖于从深度模型中学习的层次特征。
- en: V Image Classification
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 图像分类
- en: Image classification has been an active research topic in the past few decades.
    Many methods have been proposed to achieve very promising results by using Bag-of-Words
    (BoW) representation [[39](#bib.bib39)], spatial pyramid matching [[40](#bib.bib40)],
    topic models [[41](#bib.bib41)], part-based models [[42](#bib.bib42)] and sparse
    coding [[43](#bib.bib43)], etc. However, these methods utilize raw pixel values
    or hand-crafted features which cannot capture data-driven representations for
    specific input data. Recently, deep learning has achieved very promising results
    in image classification. We review some representative works as follows.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类在过去几十年里一直是一个活跃的研究主题。许多方法已经被提出，通过使用词袋（BoW）表示 [[39](#bib.bib39)]、空间金字塔匹配 [[40](#bib.bib40)]、主题模型
    [[41](#bib.bib41)]、基于部件的模型 [[42](#bib.bib42)] 和稀疏编码 [[43](#bib.bib43)] 等，取得了非常有前景的结果。然而，这些方法利用的是原始像素值或手工制作的特征，无法捕捉特定输入数据的基于数据的表示。最近，深度学习在图像分类中取得了非常有前景的结果。我们回顾了一些具有代表性的工作，如下所示。
- en: '![Refer to caption](img/8e454c97eeaf2a7038ae8281c9ad4e2b.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8e454c97eeaf2a7038ae8281c9ad4e2b.png)'
- en: 'Figure 13: Illustration of the architecture of the deep CNN proposed by Krizhevsky
    et al. [[11](#bib.bib11)]. It consists of five convolutional layers, some of which
    are followed by max-pooling layers, and three fully connected layers with a $1000$-way
    softmax.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：Krizhevsky 等人提出的深度卷积神经网络架构的示意图 [[11](#bib.bib11)]。该网络包含五个卷积层，其中一些后面接有最大池化层，还有三个全连接层和一个
    $1000$-维 softmax 层。
- en: 'Krizhevsky et al. [[11](#bib.bib11)] develop a deep convolutional neural network
    which has $60$ million parameters and $650,000$ neurons. The deep model includes
    five convolutional layers followed by max-pooling layers, and three fully-connected
    layers with a final $1000$-way softmax. The architecture of the deep convolutional
    neural network proposed by Krizhevsky et al. [[11](#bib.bib11)] is illustrated
    in Figure [13](#S5.F13 "Figure 13 ‣ V Image Classification ‣ Deep Learning Algorithms
    with Applications to Video Analytics for A Smart City: A Survey"). The deep model
    achieved very impressive results on the ImageNet LSVRC-$2010$ contest by reducing
    the error rates down to $8$% against the then-state-of-the-art. The dataset includes
    $1.2$ million high-resolution images in $1000$ different classes. This competition
    has become one of the largest and most challenging computer vision challenges
    and is held annually. The challenge has attracted not only academic research groups
    but also industry companies. For example, Google has won the classification challenge
    in the ImageNet $2014$ with a error rate $6.66$%. Nowadays, high performance computing
    plays a very important role in deep learning. Recently, the Chinese search engine
    company Baidu has obtained a $5.98$% error rate on the ImageNet classification
    dataset by using a supercomputer called Minwa, which consists of $36$ server nodes,
    each with $4$ Nvidia Tesla K$40$m GPUs. Baidu also claims that Minwa can handle
    higher-resolution images and the larger training dataset ( $2$ billion images)
    which is generated by distorting, flipping and changing colors of the original
    $1.2$ million images. As a result, the system is able to work on real-world photos.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Krizhevsky 等人 [[11](#bib.bib11)] 开发了一种深度卷积神经网络，该网络具有 $60$ 百万个参数和 $650,000$ 个神经元。该深度模型包括五个卷积层，随后是最大池化层，以及三个全连接层，最后是
    $1000$ 路软最大层。Krizhevsky 等人 [[11](#bib.bib11)] 提出的深度卷积神经网络的结构如图 [13](#S5.F13 "图
    13 ‣ V 图像分类 ‣ 深度学习算法在智能城市视频分析中的应用：综述") 所示。该深度模型在 ImageNet LSVRC-$2010$ 比赛中取得了非常令人印象深刻的结果，将错误率降低到
    $8$% 对比当时最先进的技术。数据集包括 $1.2$ 百万张高分辨率图像，分为 $1000$ 个不同的类别。这个比赛已成为最大的、最具挑战性的计算机视觉挑战之一，并且每年举行。这一挑战不仅吸引了学术研究团队，还吸引了工业公司。例如，谷歌在
    ImageNet $2014$ 比赛中获得了 $6.66$% 的错误率。如今，高性能计算在深度学习中发挥着非常重要的作用。最近，中国搜索引擎公司百度通过使用名为
    Minwa 的超级计算机获得了 $5.98$% 的错误率，该计算机由 $36$ 个服务器节点组成，每个节点配备 $4$ 块 Nvidia Tesla K$40$m
    GPU。百度还声称 Minwa 能处理更高分辨率的图像和更大的训练数据集（$2$ 亿图像），这些图像是通过扭曲、翻转和改变原始 $1.2$ 百万张图像的颜色生成的。因此，该系统能够处理现实世界中的照片。
- en: '![Refer to caption](img/9c3d46648b7e1cf48eef0a8bd79abfe2.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9c3d46648b7e1cf48eef0a8bd79abfe2.png)'
- en: 'Figure 14: Illustration of the basic idea of the method [[44](#bib.bib44)].'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: 方法基本思想的示意图 [[44](#bib.bib44)]。'
- en: Lu et al. [[44](#bib.bib44)] present a multi-manifold deep metric learning (MMDML)
    method for image set classification. First, each image set is modeled as a manifold
    which is passed into multiple layers of deep neural networks and mapped into another
    feature space. Specifically, the deep network is class-specific so that different
    classes have different parameters in their networks. Then, the maximal manifold
    margin criterion is used to learn the parameters of these manifold. In the testing
    stage, these class-specific deep networks are applied to compute the similarity
    between the testing image set and all training classes. Finally, the smallest
    distance is used for classification. As a result, the proposed method can achieve
    the state-of-the-art performance on five widely used datasets by exploiting both
    discriminative and class-specific information.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Lu 等人 [[44](#bib.bib44)] 提出了一个用于图像集分类的多流形深度度量学习（MMDML）方法。首先，每个图像集被建模为一个流形，该流形传递到多个深度神经网络层中，并映射到另一个特征空间中。具体而言，深度网络是特定于类别的，因此不同类别在其网络中具有不同的参数。然后，使用最大流形间隔准则来学习这些流形的参数。在测试阶段，这些特定于类别的深度网络被应用于计算测试图像集与所有训练类别之间的相似性。最后，使用最小距离进行分类。因此，该方法通过利用区分性和特定类别的信息，能够在五个广泛使用的数据集上实现最先进的性能。
- en: '![Refer to caption](img/574d77975cded5ebcbfcd91bb59286c6.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/574d77975cded5ebcbfcd91bb59286c6.png)'
- en: 'Figure 15: Illustration of the overall framework of C-HRNNs [[14](#bib.bib14)].'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: C-HRNNs [[14](#bib.bib14)] 整体框架的示意图。'
- en: 'Zuo et al. [[14](#bib.bib14)] develop an end-to-end Convolutional Hierarchical
    Recurrent Neural Networks (C-HRNNs) to explore contextual dependencies for image
    classification. Figure [15](#S5.F15 "Figure 15 ‣ V Image Classification ‣ Deep
    Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey")
    illustrates the overall framework of C-HRNNs. First, mid-level representations
    for image regions are extracted by using five layers of Convolutional Neural Networks
    (CNNs). Then, the output of the fifth CNN layer is pooled into multiple scales.
    For each scale, spatial dependencies are captured by direct or indirect connections
    between each region and its surrounding neighbors. For different scales, scale
    dependencies are encoded by transferring information from the higher level scales
    to corresponding areas at the lower level scales. Finally, different scale HRNN
    outputs are collected and put into two fully connected layers. C-HRNNs not only
    make use of the representation power of CNNs, but also efficiently encodes spatial
    and scale dependencies among different image regions. As a result, the proposed
    model achieves state-of-the-art performance on four challenging image classification
    benchmarks.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zuo 等人 [[14](#bib.bib14)] 开发了一种端到端的卷积层次递归神经网络 (C-HRNNs) 来探索图像分类的上下文依赖性。图 [15](#S5.F15
    "Figure 15 ‣ V Image Classification ‣ Deep Learning Algorithms with Applications
    to Video Analytics for A Smart City: A Survey") 展示了 C-HRNNs 的整体框架。首先，通过使用五层卷积神经网络
    (CNNs) 提取图像区域的中级表示。然后，第五层 CNN 的输出被汇聚到多个尺度。对于每个尺度，通过直接或间接连接捕获每个区域及其周围邻域之间的空间依赖性。对于不同尺度，尺度依赖性通过将信息从高层尺度转移到低层尺度的对应区域来编码。最后，将不同尺度的
    HRNN 输出收集并输入到两个全连接层。C-HRNNs 不仅利用了 CNNs 的表示能力，还有效地编码了不同图像区域之间的空间和尺度依赖性。因此，所提模型在四个具有挑战性的图像分类基准测试中实现了最先进的性能。'
- en: VI Scene Labeling
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 场景标记
- en: Scene labeling aims to assign one of a set of semantic labels to each pixel
    of a scene image. It is very challenging due to the fact that some classes may
    be indistinguishable in a close-up view. Generally, “thing” pixels (cars, person,
    etc) in real world images can be quite different due to their scale, illumination
    and pose variation. Recently, deep learning based methods have achieved very promising
    results for scene labeling. We review some of them as follows.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 场景标记的目标是为场景图像的每个像素分配一组语义标签中的一个。由于某些类别在特写视图中可能无法区分，这是一项非常具有挑战性的任务。一般来说，现实世界图像中的“事物”像素（如汽车、人物等）由于尺度、光照和姿态变化，可能会有很大差异。最近，基于深度学习的方法在场景标记方面取得了非常有前景的结果。我们将回顾其中的一些方法。
- en: '![Refer to caption](img/95d48431f53f80d1e4fd71216af6b64a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/95d48431f53f80d1e4fd71216af6b64a.png)'
- en: 'Figure 16: Illustration of the framework of the approach [[19](#bib.bib19)].'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：所提方法框架的示意图 [[19](#bib.bib19)]。
- en: 'Shuai et al. [[19](#bib.bib19)] propose to adopt Convolutional Neural Networks
    (CNNs) as a parametric model to learn discriminative features and classifier for
    scene labeling. Figure [16](#S6.F16 "Figure 16 ‣ VI Scene Labeling ‣ Deep Learning
    Algorithms with Applications to Video Analytics for A Smart City: A Survey") illustrates
    the framework of the proposed method. First, global scene semantics are used to
    remove the ambiguity of local context by transferring class dependencies and priors
    from similar exemplars. Then, the global potential is decoupled to the aggregation
    of global beliefs over pixels. The labeling result can be obtained by integrating
    the local and global beliefs. Finally, a large margin based metric learning is
    introduced to make the estimation of global belief more accurate. As a result,
    the proposed model is able to achieve state-of-the-art results on the SiftFlow
    benchmark and very competitive results on the Stanford Background dataset.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'Shuai 等人 [[19](#bib.bib19)] 提出了采用卷积神经网络 (CNNs) 作为参数模型来学习判别特征和用于场景标记的分类器。图 [16](#S6.F16
    "Figure 16 ‣ VI Scene Labeling ‣ Deep Learning Algorithms with Applications to
    Video Analytics for A Smart City: A Survey") 展示了所提方法的框架。首先，使用全局场景语义通过从相似样本中转移类别依赖性和先验来消除局部上下文的模糊性。然后，全局潜力被解耦为像素上的全局信念的聚合。通过整合局部和全局信念，可以获得标记结果。最后，引入基于大间隔的度量学习以提高全局信念的估计精度。因此，所提模型能够在
    SiftFlow 基准测试上实现最先进的结果，并在斯坦福背景数据集上获得非常有竞争力的结果。'
- en: '![Refer to caption](img/366fef821442f56447cf031abeea1ae0.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/366fef821442f56447cf031abeea1ae0.png)'
- en: 'Figure 17: Illustration of the architecture of the full labeling network in
    the approach [[45](#bib.bib45)].'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：方法[[45](#bib.bib45)]中完整标注网络的架构示意图。
- en: 'Shuai et al. [[45](#bib.bib45)] present a directed acyclic graph RNNs (DAG-RNNs)
    for scene labeling to model long-range semantic dependencies among image units.
    Figure [17](#S6.F17 "Figure 17 ‣ VI Scene Labeling ‣ Deep Learning Algorithms
    with Applications to Video Analytics for A Smart City: A Survey") illustrates
    the architecture of the full labeling network in the proposed method. First, undirected
    cyclic graphs (UCG) are adopted to model interactions among image units. Due to
    the loopy property of UCGs, RNNs are not directly applicable to UCG-structured
    images. Thus, the UCG is decomposed to several directed acyclic graphs (DAGs).
    Then, each hidden layer is generated independently through applying DAG-RNNs to
    the corresponding DAG-structured image, and they are integrated to produce the
    context-aware feature maps. As a result, the local representations are able to
    embed the abstract gist of the image, so their discriminative power are enhanced
    remarkably. It is reported that the DAG-RNNs achieve new state-of-the-art results
    on the challenging SiftFlow, CamVid and Barcelona benchmarks.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 帅等人[[45](#bib.bib45)]提出了一种用于场景标注的有向无环图RNN（DAG-RNN），以建模图像单元之间的长距离语义依赖。图[17](#S6.F17
    "图 17 ‣ VI 场景标注 ‣ 深度学习算法在智能城市视频分析中的应用：综述")展示了该方法中完整标注网络的架构。首先，采用无向循环图（UCG）来建模图像单元之间的交互。由于UCG的循环特性，RNN无法直接应用于UCG结构的图像。因此，将UCG分解为多个有向无环图（DAG）。然后，通过将DAG-RNN应用于相应的DAG结构图像，独立生成每个隐藏层，并将它们整合以产生上下文感知特征图。因此，本地表示能够嵌入图像的抽象要点，从而显著提高其判别能力。据报道，DAG-RNN在具有挑战性的SiftFlow、CamVid和Barcelona基准数据集上取得了新的最先进结果。
- en: '![Refer to caption](img/570e789283597e03f0998e4bc37a608d.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/570e789283597e03f0998e4bc37a608d.png)'
- en: 'Figure 18: Illustration of the framework for RGB-D indoor scene labeling in
    the approach [[46](#bib.bib46)].'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：方法[[46](#bib.bib46)]中RGB-D室内场景标注框架的示意图。
- en: 'Wang et al. [[46](#bib.bib46)] develop an unsupervised joint feature learning
    and encoding (JFLE) framework for RGB-D scene labeling. Figure [18](#S6.F18 "Figure
    18 ‣ VI Scene Labeling ‣ Deep Learning Algorithms with Applications to Video Analytics
    for A Smart City: A Survey") illustrates the framework for RGB-D indoor scene
    labeling in the proposed method. First, feature learning and encoding are jointly
    performed in a two-layer stacked structure, called joint feature learning and
    encoding framework (JFLE). To further extend the JFLE framework to a more general
    framework called joint deep feature learning and encoding (JDFLE), a deep model
    is used with stacked nonlinear layers to model the input data. The input to the
    learning structure (either JFLE or JDFLE) is a set of patches densely sampled
    from RGB-D images, and the learning output is the set of corresponding path features,
    which are then combined to generate superpixel features. Finally, linear SVMs
    are trained to map superpixel features to scene labels. It is reported that the
    proposed feature learning framework can achieve competitive performance on the
    benchmark NYU depth dataset.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人[[46](#bib.bib46)]开发了一个无监督的联合特征学习与编码（JFLE）框架，用于RGB-D场景标注。图[18](#S6.F18 "图
    18 ‣ VI 场景标注 ‣ 深度学习算法在智能城市视频分析中的应用：综述")展示了该方法中RGB-D室内场景标注的框架。首先，在一个名为联合特征学习与编码框架（JFLE）的两层堆叠结构中，特征学习与编码被共同执行。为了将JFLE框架扩展到一个更通用的框架——联合深度特征学习与编码（JDFLE），使用了一个具有堆叠非线性层的深度模型来建模输入数据。学习结构（无论是JFLE还是JDFLE）的输入是一组从RGB-D图像中密集采样的补丁，学习输出是相应路径特征的集合，然后这些特征被组合生成超像素特征。最后，训练线性支持向量机（SVM）以将超像素特征映射到场景标签。据报道，所提出的特征学习框架在基准NYU深度数据集上表现出色。
- en: Scene labeling can be used to understand urban images captured from surveillance
    cameras. It is a very important component of smart city, in which city elements
    such as “road” and “building” are required to be recognized. Deep models can leverage
    on big data from smart city to learn hierarchical features for labeling each pixel
    of scene images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 场景标注可以用来理解从监控摄像头捕捉到的城市图像。它是智能城市中的一个重要组成部分，其中“道路”和“建筑”等城市元素需要被识别。深度模型可以利用来自智能城市的大数据来学习层次特征，从而对场景图像的每个像素进行标注。
- en: VII Conclusions
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: In this paper, we have reviewed the recent progress of deep learning in object
    detection, object tracking, face recognition, image classification and scene labeling.
    The deep models have significantly improved the performance in these areas, often
    approaching human capabilities. The reasons for this success are two-folded. First,
    big training data are becoming increasingly available (e.g. data streams from
    a multitude of smart city sensors) for building up large deep neural networks.
    Second, new advanced hardware (e.g. GPU) has largely reduced the training time
    for deep networks. We believe that deep learning will have a more prospective
    future in a wide range of applications.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了深度学习在目标检测、目标跟踪、人脸识别、图像分类和场景标注方面的最新进展。深度模型在这些领域显著提升了性能，通常接近人类能力。这一成功有两个原因。首先，大量训练数据变得越来越可用（例如，来自众多智能城市传感器的数据流），用于构建大型深度神经网络。其次，新型先进硬件（例如GPU）大大减少了深度网络的训练时间。我们相信，深度学习将在广泛应用领域中拥有更具前景的未来。
- en: Acknowledgment
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research was carried out at the Rapid-Rich Object Search (ROSE) Lab at
    the Nanyang Technological University, Singapore. The ROSE Lab is supported by
    the National Research Foundation, Prime Minister’s Office, Singapore, under its
    IDM Futures Funding Initiative and administered by the Interactive and Digital
    Media Programme Office.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究在新加坡南洋理工大学的快速富对象搜索（ROSE）实验室进行。ROSE实验室由新加坡总理办公室国家研究基金会支持，通过其IDM未来资金计划资助，并由互动与数字媒体计划办公室管理。
- en: References
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] C. Cortes and V. Vapnik, “Support-vector networks,” *Machine Learning*,
    vol. 20, no. 3, pp. 273–297, 1995.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] C. Cortes 和 V. Vapnik, “支持向量网络，” *机器学习*，第20卷，第3期，第273–297页，1995年。'
- en: '[2] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations
    by back-propagating errors,” *Cognitive modeling*, vol. 5, 1988.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. E. Rumelhart, G. E. Hinton 和 R. J. Williams, “通过反向传播错误学习表示，” *认知建模*，第5卷，1988年。'
- en: '[3] T. Kohonen, “Self-organized formation of topologically correct feature
    maps,” *Biological cybernetics*, vol. 43, no. 1, pp. 59–69, 1982.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] T. Kohonen, “自组织形成拓扑正确的特征图，” *生物控制论*，第43卷，第1期，第59–69页，1982年。'
- en: '[4] H. Hotelling, “Analysis of a complex of statistical variables into principal
    components.” *Journal of educational psychology*, vol. 24, no. 6, p. 417, 1933.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] H. Hotelling, “将统计变量的复杂性分析为主成分。” *教育心理学杂志*，第24卷，第6期，第417页，1933年。'
- en: '[5] K. Fukushima, “Neocognitron: A self-organizing neural network model for
    a mechanism of pattern recognition unaffected by shift in position,” *Biological
    cybernetics*, vol. 36, no. 4, pp. 193–202, 1980.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] K. Fukushima, “Neocognitron：一种自组织神经网络模型，用于模式识别机制，不受位置变化的影响，” *生物控制论*，第36卷，第4期，第193–202页，1980年。'
- en: '[6] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E.
    Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,”
    *Neural Computation*, vol. 1, no. 4, pp. 541–551, 1989.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E.
    Hubbard 和 L. D. Jackel, “反向传播应用于手写邮政编码识别，” *神经计算*，第1卷，第4期，第541–551页，1989年。'
- en: '[7] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *et al.*, “Deep neural networks for acoustic
    modeling in speech recognition: The shared views of four research groups,” *Signal
    Processing Magazine, IEEE*, vol. 29, no. 6, pp. 82–97, 2012.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *等*，“用于语音识别的深度神经网络：四个研究小组的共同观点，” *IEEE信号处理杂志*，第29卷，第6期，第82–97页，2012年。'
- en: '[8] G. E. Hinton, “Learning multiple layers of representation,” *Trends in
    cognitive sciences*, vol. 11, no. 10, pp. 428–434, 2007.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] G. E. Hinton, “学习多层次表示，” *认知科学趋势*，第11卷，第10期，第428–434页，2007年。'
- en: '[9] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” *Proceedings of the IEEE*, vol. 86, no. 11,
    pp. 2278–2324, 1998.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Y. LeCun, L. Bottou, Y. Bengio 和 P. Haffner, “基于梯度的学习应用于文档识别，” *IEEE汇刊*，第86卷，第11期，第2278–2324页，1998年。'
- en: '[10] G. E. Hinton, “Deep belief networks,” *Scholarpedia*, vol. 4, no. 5, p.
    5947, 2009.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] G. E. Hinton, “深度信念网络，” *学者百科*，第4卷，第5期，第5947页，2009年。'
- en: '[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Annual Conference on Neural Information
    Processing Systems*, 2012, pp. 1106–1114.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Krizhevsky, I. Sutskever 和 G. E. Hinton, “使用深度卷积神经网络进行ImageNet分类，”
    在 *神经信息处理系统年会*，2012年，第1106–1114页。'
- en: '[12] R. Salakhutdinov and G. E. Hinton, “Deep boltzmann machines,” in *International
    Conference on Artificial Intelligence and Statistics*, 2009, pp. 448–455.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] R. Salakhutdinov 和 G. E. Hinton, “深度玻尔兹曼机”，发表于 *国际人工智能与统计会议*，2009年，页码448–455。'
- en: '[13] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, “Stacked
    denoising autoencoders: Learning useful representations in a deep network with
    a local denoising criterion,” *The Journal of Machine Learning Research*, vol. 11,
    pp. 3371–3408, 2010.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio 和 P.-A. Manzagol, “堆叠去噪自编码器：通过具有局部去噪标准的深度网络学习有用的表示”，*机器学习研究杂志*，第11卷，页码3371–3408，2010年。'
- en: '[14] Z. Zuo, B. Shuai, G. Wang, X. Liu, X. Wang, and B. Wang, “Learning contextual
    dependencies with convolutional hierarchical recurrent neural networks,” *CoRR*,
    vol. abs/1509.03877, 2015.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Z. Zuo, B. Shuai, G. Wang, X. Liu, X. Wang 和 B. Wang, “通过卷积层次递归神经网络学习上下文依赖”，*CoRR*，第abs/1509.03877卷，2015年。'
- en: '[15] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing the gap
    to human-level performance in face verification,” in *IEEE Conference on Computer
    Vision and Pattern Recognition*, 2014, pp. 1701–1708.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Taigman, M. Yang, M. Ranzato 和 L. Wolf, “Deepface: 缩小人类水平人脸验证性能的差距”，发表于
    *IEEE计算机视觉与模式识别会议*，2014年，页码1701–1708。'
- en: '[16] H. Li, Y. Li, and F. Porikli, “Deeptrack: Learning discriminative feature
    representations by convolutional neural networks for visual tracking,” in *British
    Machine Vision Conference*, 2014.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] H. Li, Y. Li 和 F. Porikli, “Deeptrack：通过卷积神经网络学习用于视觉跟踪的判别特征表示”，发表于 *英国机器视觉会议*，2014年。'
- en: '[17] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun, “Pedestrian detection
    with unsupervised multi-stage feature learning,” in *IEEE Conference on Computer
    Vision and Pattern Recognition*, 2013, pp. 3626–3633.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] P. Sermanet, K. Kavukcuoglu, S. Chintala 和 Y. LeCun, “通过无监督的多阶段特征学习进行行人检测”，发表于
    *IEEE计算机视觉与模式识别会议*，2013年，页码3626–3633。'
- en: '[18] A. H. Abdulnabi, G. Wang, J. Lu, and K. Jia, “Multi-task CNN model for
    attribute prediction,” *IEEE Transactions on Multimedia*, vol. 17, no. 11, pp.
    1949–1959, 2015.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. H. Abdulnabi, G. Wang, J. Lu 和 K. Jia, “用于属性预测的多任务CNN模型”，*IEEE多媒体学报*，第17卷，第11期，页码1949–1959，2015年。'
- en: '[19] B. Shuai, G. Wang, Z. Zuo, B. Wang, and L. Zhao, “Integrating parametric
    and non-parametric models for scene labeling,” in *IEEE Conference on Computer
    Vision and Pattern Recognition*, 2015, pp. 4249–4258.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] B. Shuai, G. Wang, Z. Zuo, B. Wang 和 L. Zhao, “集成参数化和非参数化模型进行场景标注”，发表于
    *IEEE计算机视觉与模式识别会议*，2015年，页码4249–4258。'
- en: '[20] R. R. Varior, G. Wang, and J. Lu, “Learning invariant color features for
    person re-identification,” *CoRR*, vol. abs/1410.1035, 2014.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] R. R. Varior, G. Wang 和 J. Lu, “学习不变的颜色特征以进行人物重新识别”，*CoRR*，第abs/1410.1035卷，2014年。'
- en: '[21] A. Wang, J. Lu, J. Cai, T. Cham, and G. Wang, “Large-margin multi-modal
    deep learning for RGB-D object recognition,” *IEEE Transactions on Multimedia*,
    vol. 17, no. 11, pp. 1887–1898, 2015.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Wang, J. Lu, J. Cai, T. Cham 和 G. Wang, “用于RGB-D物体识别的大间隔多模态深度学习”，*IEEE多媒体学报*，第17卷，第11期，页码1887–1898，2015年。'
- en: '[22] B. Shuai, Z. Zuo, and G. Wang, “Quaddirectional 2d-recurrent neural networks
    for image labeling,” *IEEE Signal Process. Lett.*, vol. 22, no. 11, pp. 1990–1994,
    2015.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] B. Shuai, Z. Zuo 和 G. Wang, “用于图像标注的四向2D递归神经网络”，*IEEE信号处理快报*，第22卷，第11期，页码1990–1994，2015年。'
- en: '[23] Z. Zuo, G. Wang, B. Shuai, L. Zhao, and Q. Yang, “Exemplar based deep
    discriminative and shareable feature learning for scene image classification,”
    *Pattern Recognition*, vol. 48, no. 10, pp. 3004–3015, 2015.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Z. Zuo, G. Wang, B. Shuai, L. Zhao 和 Q. Yang, “基于示例的深度判别和可共享特征学习用于场景图像分类”，*模式识别*，第48卷，第10期，页码3004–3015，2015年。'
- en: '[24] L. Deng, O. Abdel-Hamid, and D. Yu, “A deep convolutional neural network
    using heterogeneous pooling for trading acoustic invariance with phonetic confusion,”
    in *IEEE International Conference on Acoustics, Speech and Signal Processing*,
    2013, pp. 6669–6673.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] L. Deng, O. Abdel-Hamid 和 D. Yu, “一种使用异构池化的深度卷积神经网络，用于在音素混淆中交易声学不变性”，发表于
    *IEEE国际声学、语音与信号处理会议*，2013年，页码6669–6673。'
- en: '[25] R. Collobert and J. Weston, “A unified architecture for natural language
    processing: deep neural networks with multitask learning,” in *International Conference
    on Machine Learning*, 2008, pp. 160–167.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] R. Collobert 和 J. Weston, “自然语言处理的统一架构：带有多任务学习的深度神经网络”，发表于 *国际机器学习会议*，2008年，页码160–167。'
- en: '[26] R. Raina, A. Madhavan, and A. Y. Ng, “Large-scale deep unsupervised learning
    using graphics processors,” in *International Conference on Machine Learning*,
    2009, pp. 873–880.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] R. Raina, A. Madhavan 和 A. Y. Ng, “使用图形处理器的大规模深度无监督学习”，发表于 *国际机器学习会议*，2009年，页码873–880。'
- en: '[27] H. Lee, A. Battle, R. Raina, and A. Y. Ng, “Efficient sparse coding algorithms,”
    in *Annual Conference on Neural Information Processing Systems*, 2006, pp. 801–808.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] H. Lee, A. Battle, R. Raina 和 A. Y. Ng，“高效的稀疏编码算法，”发表于 *神经信息处理系统年会*，2006年，页码801–808。'
- en: '[28] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object
    detection,” in *Annual Conference on Neural Information Processing Systems*, 2013,
    pp. 2553–2561.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] C. Szegedy, A. Toshev 和 D. Erhan，“用于对象检测的深度神经网络，”发表于 *神经信息处理系统年会*，2013年，页码2553–2561。'
- en: '[29] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in *IEEE Conference
    on Computer Vision and Pattern Recognition*, 2014, pp. 580–587.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] R. B. Girshick, J. Donahue, T. Darrell 和 J. Malik，“用于准确对象检测和语义分割的丰富特征层次，”发表于
    *IEEE计算机视觉与模式识别会议*，2014年，页码580–587。'
- en: '[30] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable object detection
    using deep neural networks,” in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2014, pp. 2155–2162.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] D. Erhan, C. Szegedy, A. Toshev 和 D. Anguelov，“使用深度神经网络的可扩展对象检测，”发表于 *IEEE计算机视觉与模式识别会议*，2014年，页码2155–2162。'
- en: '[31] N. Wang and D. Yeung, “Learning a deep compact image representation for
    visual tracking,” in *Annual Conference on Neural Information Processing Systems*,
    2013, pp. 809–817.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] N. Wang 和 D. Yeung，“用于视觉跟踪的深度紧凑图像表示学习，”发表于 *神经信息处理系统年会*，2013年，页码809–817。'
- en: '[32] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol, “Stacked
    denoising autoencoders: Learning useful representations in a deep network with
    a local denoising criterion,” *Journal of Machine Learning Research*, vol. 11,
    pp. 3371–3408, 2010.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio 和 P. Manzagol，“堆叠去噪自编码器：通过局部去噪标准在深度网络中学习有用的表示，”
    *机器学习研究杂志*，第11卷，页码3371–3408，2010年。'
- en: '[33] L. Wang, T. Liu, G. Wang, K. L. Chan, and Q. Yang, “Video tracking using
    learned hierarchical features,” *IEEE Transactions on Image Processing*, vol. 24,
    no. 4, pp. 1424–1435, 2015.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] L. Wang, T. Liu, G. Wang, K. L. Chan 和 Q. Yang，“使用学习的层次特征进行视频跟踪，” *IEEE图像处理学报*，第24卷，第4期，页码1424–1435，2015年。'
- en: '[34] W. Y. Zou, A. Y. Ng, S. Zhu, and K. Yu, “Deep learning of invariant features
    via simulated fixations in video,” in *Annual Conference on Neural Information
    Processing Systems*, 2012, pp. 3212–3220.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] W. Y. Zou, A. Y. Ng, S. Zhu 和 K. Yu，“通过模拟注视在视频中学习不变特征，”发表于 *神经信息处理系统年会*，2012年，页码3212–3220。'
- en: '[35] G. B. Huang, H. Lee, and E. G. Learned-Miller, “Learning hierarchical
    representations for face verification with convolutional deep belief networks,”
    in *IEEE Conference on Computer Vision and Pattern Recognition*, 2012, pp. 2518–2525.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] G. B. Huang, H. Lee 和 E. G. Learned-Miller，“利用卷积深度信念网络学习面部验证的层次表示，”发表于
    *IEEE计算机视觉与模式识别会议*，2012年，页码2518–2525。'
- en: '[36] T. Ojala, M. Pietikäinen, and D. Harwood, “A comparative study of texture
    measures with classification based on featured distributions,” *Pattern Recognition*,
    vol. 29, no. 1, pp. 51–59, 1996.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T. Ojala, M. Pietikäinen 和 D. Harwood，“基于特征分布的分类的纹理度量比较研究，” *模式识别*，第29卷，第1期，页码51–59，1996年。'
- en: '[37] Y. Sun, X. Wang, and X. Tang, “Deep learning face representation from
    predicting 10, 000 classes,” in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2014, pp. 1891–1898.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Y. Sun, X. Wang 和 X. Tang，“从预测10,000个类别中学习深度面部表示，”发表于 *IEEE计算机视觉与模式识别会议*，2014年，页码1891–1898。'
- en: '[38] J. Lu, V. E. Liong, G. Wang, and P. Moulin, “Joint feature learning for
    face recognition,” *IEEE Transactions on Information Forensics and Security*,
    vol. 10, no. 7, pp. 1371–1383, 2015.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. Lu, V. E. Liong, G. Wang 和 P. Moulin，“面部识别的联合特征学习，” *IEEE信息取证与安全学报*，第10卷，第7期，页码1371–1383，2015年。'
- en: '[39] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray, “Visual categorization
    with bags of keypoints,” in *Workshop on statistical learning in computer vision,
    ECCV*, vol. 1, no. 1-22.   Prague, 2004, pp. 1–2.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] G. Csurka, C. Dance, L. Fan, J. Willamowski 和 C. Bray，“使用特征点袋的视觉分类，”发表于
    *计算机视觉统计学习研讨会，ECCV*，第1卷，第1-22期。布拉格，2004年，页码1–2。'
- en: '[40] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial
    pyramid matching for recognizing natural scene categories,” in *IEEE Conference
    on Computer Vision and Pattern Recognition*, 2006, pp. 2169–2178.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. Lazebnik, C. Schmid 和 J. Ponce，“超越特征袋：用于识别自然场景类别的空间金字塔匹配，”发表于 *IEEE计算机视觉与模式识别会议*，2006年，页码2169–2178。'
- en: '[41] F. Li and P. Perona, “A bayesian hierarchical model for learning natural
    scene categories,” in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2005, pp. 524–531.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] F. Li 和 P. Perona，“用于学习自然场景类别的贝叶斯层次模型，”发表于*IEEE计算机视觉与模式识别会议*，2005年，第524–531页。'
- en: '[42] R. Fergus, P. Perona, and A. Zisserman, “Object class recognition by unsupervised
    scale-invariant learning,” in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2003, pp. 264–271.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] R. Fergus、P. Perona 和 A. Zisserman，“通过无监督尺度不变学习进行目标类别识别，”发表于*IEEE计算机视觉与模式识别会议*，2003年，第264–271页。'
- en: '[43] J. Yang, K. Yu, Y. Gong, and T. S. Huang, “Linear spatial pyramid matching
    using sparse coding for image classification,” in *IEEE Conference on Computer
    Vision and Pattern Recognition*, 2009, pp. 1794–1801.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Yang、K. Yu、Y. Gong 和 T. S. Huang，“使用稀疏编码的线性空间金字塔匹配用于图像分类，”发表于*IEEE计算机视觉与模式识别会议*，2009年，第1794–1801页。'
- en: '[44] J. Lu, G. Wang, W. Deng, P. Moulin, and J. Zhou, “Multi-manifold deep
    metric learning for image set classification,” in *IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015*,
    2015, pp. 1137–1145.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] J. Lu、G. Wang、W. Deng、P. Moulin 和 J. Zhou，“用于图像集分类的多流形深度度量学习，”发表于*IEEE计算机视觉与模式识别会议，CVPR
    2015，马萨诸塞州波士顿，美国，2015年6月7-12日*，2015年，第1137–1145页。'
- en: '[45] B. Shuai, Z. Zuo, G. Wang, and B. Wang, “Dag-recurrent neural networks
    for scene labeling,” *CoRR*, vol. abs/1509.00552, 2015.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] B. Shuai、Z. Zuo、G. Wang 和 B. Wang，“用于场景标注的DAG-递归神经网络，”*CoRR*，第abs/1509.00552卷，2015年。'
- en: '[46] A. Wang, J. Lu, J. Cai, G. Wang, and T. Cham, “Unsupervised joint feature
    learning and encoding for RGB-D scene labeling,” *IEEE Transactions on Image Processing*,
    vol. 24, no. 11, pp. 4459–4473, 2015.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Wang、J. Lu、J. Cai、G. Wang 和 T. Cham，“无监督的联合特征学习与编码用于RGB-D场景标注，”*IEEE图像处理学报*，第24卷，第11期，第4459–4473页，2015年。'
