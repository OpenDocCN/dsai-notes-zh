- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:30:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:30:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2408.01287] Deep Learning based Visually Rich Document Content Understanding:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2408.01287] 基于深度学习的视觉丰富文档内容理解：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.01287](https://ar5iv.labs.arxiv.org/html/2408.01287)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.01287](https://ar5iv.labs.arxiv.org/html/2408.01287)
- en: 'Deep Learning based Visually Rich Document Content Understanding: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的视觉丰富文档内容理解：综述
- en: Yihao Ding [0000-0001-5065-6911](https://orcid.org/0000-0001-5065-6911 "ORCID
    identifier") The University of Melbourne, The University of SydneyAustralia , 
    Jean Lee [0000-0002-7457-028X](https://orcid.org/0000-0002-7457-028X "ORCID identifier")
    The University of SydneyAustralia  and  Soyeon Caren Han [0000-0002-1948-6819](https://orcid.org/0000-0002-1948-6819
    "ORCID identifier") The University of Melbourne, The University of SydneyAustralia(2024)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 丁一豪 [0000-0001-5065-6911](https://orcid.org/0000-0001-5065-6911 "ORCID identifier")
    墨尔本大学，悉尼大学 澳大利亚，  让·李 [0000-0002-7457-028X](https://orcid.org/0000-0002-7457-028X
    "ORCID identifier") 悉尼大学 澳大利亚 和  韩素妍 [0000-0002-1948-6819](https://orcid.org/0000-0002-1948-6819
    "ORCID identifier") 墨尔本大学，悉尼大学 澳大利亚 (2024)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Visually Rich Documents (VRDs) are essential in academia, finance, medical fields,
    and marketing due to their multimodal information content. Traditional methods
    for extracting information from VRDs depend on expert knowledge and manual labor,
    making them costly and inefficient. The advent of deep learning has revolutionized
    this process, introducing models that leverage multimodal information—vision,
    text, and layout—along with pretraining tasks to develop comprehensive document
    representations. These models have achieved state-of-the-art performance across
    various downstream tasks, significantly enhancing the efficiency and accuracy
    of information extraction from VRDs. In response to the growing demands and rapid
    developments in Visually Rich Document Understanding (VRDU), this paper provides
    a comprehensive review of deep learning-based VRDU frameworks. We systematically
    survey and analyze existing methods and benchmark datasets, categorizing them
    based on adopted strategies and downstream tasks. Furthermore, we compare different
    techniques used in VRDU models, focusing on feature representation and fusion,
    model architecture, and pretraining methods, while highlighting their strengths,
    limitations, and appropriate scenarios. Finally, we identify emerging trends and
    challenges in VRDU, offering insights into future research directions and practical
    applications. This survey aims to provide a thorough understanding of VRDU advancements,
    benefiting both academic and industrial sectors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉丰富文档（VRDs）因其多模态信息内容在学术界、金融、医疗和营销领域中至关重要。传统的从VRDs中提取信息的方法依赖于专家知识和人工劳动，使得这些方法成本高且效率低下。深度学习的出现彻底改变了这一过程，引入了利用多模态信息——视觉、文本和布局——以及预训练任务来发展全面文档表示的模型。这些模型在各种下游任务中取得了最先进的性能，显著提高了从VRDs中提取信息的效率和准确性。为了响应视觉丰富文档理解（VRDU）日益增长的需求和快速发展的情况，本文对基于深度学习的VRDU框架进行了全面综述。我们系统地调查和分析了现有的方法和基准数据集，并根据采用的策略和下游任务进行分类。此外，我们比较了VRDU模型中使用的不同技术，重点关注特征表示和融合、模型架构以及预训练方法，同时强调它们的优缺点和适用场景。最后，我们识别了VRDU中的新兴趋势和挑战，为未来的研究方向和实际应用提供了见解。该综述旨在提供对VRDU进展的全面了解，惠及学术界和工业界。
- en: 'Visually Rich Document Understanding, Key Information Extraction, Question
    Answering, Entity Linking, Multimodal^†^†copyright: acmcopyright^†^†journalyear:
    2024^†^†doi: XXXXXXX.XXXXXXX^†^†ccs: Information systems Information extraction'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉丰富文档理解，关键信息提取，问答，实体链接，多模态^†^†版权：acmcopyright^†^†期刊年份：2024^†^†doi：XXXXXXX.XXXXXXX^†^†ccs：信息系统
    信息提取
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 1.1\. Backgrounds
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 背景
- en: Visually Rich Documents (VRDs) are documents that contain a diverse mix of visual
    and textual elements designed to convey information in a comprehensive and visually
    engaging manner, which are ubiquitous in our daily lives and appear in domains
    such as finance, medicine, and academia. These documents integrate various types
    of visually rich elements, including paragraphs, tables, charts, diagrams, and
    photos. Both textual elements (such as paragraphs, lists, and captions) and visually
    rich elements (such as tables and figures), collectively referred to as document
    semantic entities, are essential for illustrating, explaining, and summarizing
    information. Common formats for VRDs include PDF (Portable Document Format), DOC/DOCX
    (Microsoft Word Document), and image files (JPEG, PNG). These documents are typically
    structured in a semi-structured or even unstructured manner, making their understanding
    and information extraction challenging.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉丰富文档（VRD）是包含多种视觉和文本元素的文档，旨在以全面且视觉吸引的方式传达信息，这些文档在我们的日常生活中随处可见，应用于金融、医学和学术等领域。这些文档整合了各种类型的视觉丰富元素，包括段落、表格、图表、图示和照片。文本元素（如段落、列表和标题）和视觉丰富元素（如表格和图形），统称为文档语义实体，对于说明、解释和总结信息至关重要。VRD
    的常见格式包括 PDF（便携式文档格式）、DOC/DOCX（Microsoft Word 文档）和图像文件（JPEG、PNG）。这些文档通常以半结构化或甚至非结构化的方式组织，使其理解和信息提取具有挑战性。
- en: 'The VRD Understanding (VRDU) field is dedicated to comprehending the structure
    of VRDs and extracting pertinent information, thus transforming unstructured or
    semi-structured content into a machine-readable format. VRDU mainly encompasses
    two tasks: Key Information Extraction (KIE) and Question Answering (QA). Key Information
    Extraction (KIE) aims to identify and extract values based on predefined keys.
    Depending on the model used, KIE can be approached as an entity retrieval task
    (KIE Task Formulation 1) or a sequence tagging task (Task Formulation 2), as shown
    in Figure [1](#S2.F1 "Figure 1 ‣ 2.1\. Task Definition ‣ 2\. Background ‣ Deep
    Learning based Visually Rich Document Content Understanding: A Survey"). Visually
    Rich Document Question Answering (VRD-QA) involves answering questions in natural
    language based on contextual information in VRDs. As explained in Figure [1](#S2.F1
    "Figure 1 ‣ 2.1\. Task Definition ‣ 2\. Background ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey"), the model must locate the answer
    within the document to respond to the input question. Beyond extractive methods,
    KIE and VRD-QA can also be formulated as generative tasks, where the model generates
    the desired value or answer auto-regressively based on the query or question and
    the document images. In addition to the above-mentioned tasks, another content
    understanding task is Entity Linking (EL), which is proposed to identify the semantic
    correlations between document entities (parent-child relations).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: VRD 理解（VRDU）领域致力于理解 VRD 的结构并提取相关信息，从而将非结构化或半结构化内容转化为机器可读格式。VRDU 主要包括两个任务：关键**信息提取**（KIE）和**问题回答**（QA）。关键**信息提取**（KIE）的目标是基于预定义的关键字识别和提取值。根据所用模型的不同，KIE
    可以作为实体检索任务（KIE 任务形式 1）或序列标注任务（任务形式 2）来处理，如图 [1](#S2.F1 "图 1 ‣ 2.1\. 任务定义 ‣ 2\.
    背景 ‣ 基于深度学习的视觉丰富文档内容理解：综述")所示。视觉丰富文档**问题回答**（VRD-QA）涉及基于 VRD 中的上下文信息用自然语言回答问题。如图
    [1](#S2.F1 "图 1 ‣ 2.1\. 任务定义 ‣ 2\. 背景 ‣ 基于深度学习的视觉丰富文档内容理解：综述")所示，模型必须在文档中定位答案以回应输入的问题。除了提取方法，KIE
    和 VRD-QA 还可以被制定为生成任务，其中模型根据查询或问题和文档图像自回归生成所需的值或答案。除了上述任务外，另一个内容理解任务是**实体链接**（EL），该任务旨在识别文档实体之间的语义关联（父子关系）。
- en: To address visually rich document understanding tasks, heuristic methods (Watanabe
    et al., [1995](#bib.bib133); Seki et al., [2007](#bib.bib106); Rusinol et al.,
    [2013](#bib.bib104)) and statistical machine learning techniques (Oliveira and
    Viana, [2017](#bib.bib89)) have been used in domain-specific document applications,
    which require expert customization and are challenging to update. Recent advances
    in deep learning have introduced promising alternatives. LSTM and CNN-based models
    (Katti et al., [2018](#bib.bib51); Denk and Reisswig, [2019](#bib.bib22); Zhao
    et al., [2019](#bib.bib148)), feature-driven approaches (Yu et al., [2021](#bib.bib142);
    Zhang et al., [2020](#bib.bib145); Wang et al., [2021a](#bib.bib124)), and layout-aware
    pretrained frameworks (Xu et al., [2020a](#bib.bib136); Wang et al., [2022b](#bib.bib123);
    Hong et al., [2021](#bib.bib41)), along with visual-integrated pretrained frameworks
    (Huang et al., [2022](#bib.bib43)), have significantly improved document representation
    and achieved state-of-the-art performance in various downstream tasks. However,
    these models primarily focus on fine-grained features, such as grids and word/word
    pieces (textual tokens), and often struggle with computational complexity and
    the ability to capture global logical and layout correlations. In contrast, coarse-grained
    models, which operate at the entity level, address some limitations of fine-grained
    models but may omit detailed information, resulting in less comprehensive representations.
    To bridge these gaps, joint-grained frameworks (Li et al., [2021c](#bib.bib68);
    Yu et al., [2022](#bib.bib143); Bai et al., [2022](#bib.bib4)) and LLM-based frameworks
    (Luo et al., [2024](#bib.bib79); Liu et al., [2024b](#bib.bib71)) have been developed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对视觉丰富文档理解任务，启发式方法（Watanabe et al., [1995](#bib.bib133); Seki et al., [2007](#bib.bib106);
    Rusinol et al., [2013](#bib.bib104)）和统计机器学习技术（Oliveira and Viana, [2017](#bib.bib89)）已被用于领域特定的文档应用，这些应用需要专家定制且难以更新。深度学习的最新进展提供了有前景的替代方案。基于LSTM和CNN的模型（Katti
    et al., [2018](#bib.bib51); Denk and Reisswig, [2019](#bib.bib22); Zhao et al.,
    [2019](#bib.bib148)）、特征驱动的方法（Yu et al., [2021](#bib.bib142); Zhang et al., [2020](#bib.bib145);
    Wang et al., [2021a](#bib.bib124)）和布局感知的预训练框架（Xu et al., [2020a](#bib.bib136);
    Wang et al., [2022b](#bib.bib123); Hong et al., [2021](#bib.bib41)），以及视觉集成的预训练框架（Huang
    et al., [2022](#bib.bib43)），显著改善了文档表示，并在各种下游任务中达到了最先进的性能。然而，这些模型主要关注细粒度特征，如网格和单词/词片（文本标记），并且通常面临计算复杂性和捕捉全局逻辑及布局相关性的能力的挑战。相比之下，粗粒度模型在实体级别操作，解决了细粒度模型的一些局限性，但可能会遗漏详细信息，导致表示不够全面。为了弥补这些差距，已经开发了联合粒度框架（Li
    et al., [2021c](#bib.bib68); Yu et al., [2022](#bib.bib143); Bai et al., [2022](#bib.bib4)）和基于LLM的框架（Luo
    et al., [2024](#bib.bib79); Liu et al., [2024b](#bib.bib71)）。
- en: With the rapid development and increasing demands in Visually Rich Document
    Understanding (VRDU), various model architectures, multimodal learning methods,
    and pretraining techniques have been introduced to consistently enhance the performance
    of specific or multiple VRDU tasks. A comprehensive review of VRDU frameworks
    based on deep learning is provided, systematically surveying and analyzing existing
    methods and benchmark datasets, which are categorized based on adopted strategies
    and downstream tasks. Additionally, different techniques used in VRDU models are
    compared, focusing on feature representation and fusion, model architecture, and
    pretraining methods, with their strengths, limitations, and appropriate scenarios
    highlighted. Finally, emerging trends and challenges in visually rich document
    content understanding are identified, offering insights into future research directions
    and practical applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着视觉丰富文档理解（VRDU）的快速发展和需求增加，各种模型架构、多模态学习方法和预训练技术已经被引入，以持续提升特定或多个VRDU任务的性能。本文提供了基于深度学习的VRDU框架的全面回顾，系统地调查和分析现有方法和基准数据集，这些方法和数据集根据采用的策略和下游任务进行分类。此外，比较了VRDU模型中使用的不同技术，重点关注特征表示与融合、模型架构和预训练方法，并突出了它们的优势、局限性和适用场景。最后，识别了视觉丰富文档内容理解中的新兴趋势和挑战，并提供了未来研究方向和实际应用的见解。
- en: 1.2\. Scope
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 范围
- en: This paper is devoted to surveying and analysing existing work on understanding
    deep learning-based document content. Papers on the following topics will be reviewed
    and summarized in this paper.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文致力于调查和分析基于深度学习的文档内容理解的现有工作。将对以下主题的论文进行回顾和总结。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper focuses on document content understanding tasks that will be included
    in this paper, mainly including document key information extraction, visual question
    answering and entity linking. This paper will not summarise the models and datasets
    that only focus on document structure understanding tasks.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文重点介绍了将包括在本文中的文档内容理解任务，主要包括文档关键信息提取、视觉问答和实体链接。本文不会总结仅关注文档结构理解任务的模型和数据集。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A paper proposed a novel deep-learning VRDU model for addressing one or more
    Visually Rich Document Content Understanding (VRD-CU) tasks. Section [2.2.1](#S2.SS2.SSS1
    "2.2.1\. Traditional Approaches ‣ 2.2\. Development of Document Understanding
    ‣ 2\. Background ‣ Deep Learning based Visually Rich Document Content Understanding:
    A Survey") will briefly describe typical document content understanding approaches
    in a heuristically or traditional machine learning way.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一篇论文提出了一种新颖的深度学习VRDU模型，用于解决一个或多个视觉丰富文档内容理解（VRD-CU）任务。第[2.2.1](#S2.SS2.SSS1 "2.2.1\.
    传统方法 ‣ 2.2\. 文档理解的发展 ‣ 2\. 背景 ‣ 基于深度学习的视觉丰富文档内容理解：综述")节将简要描述在启发式或传统机器学习方式下的典型文档内容理解方法。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This paper compiles widely used VRDU benchmark datasets and recently proposed
    VRDU datasets from top-tier conferences and journals since 2019\. All included
    datasets are publicly available.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文汇编了自2019年以来广泛使用的VRDU基准数据集和最近提出的VRDU数据集，这些数据集来自顶级会议和期刊。所有包含的数据集均为公开可用。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This paper focuses on the multimodal capabilities for achieving a holistic understanding
    of entire, multi-page documents, in contrast to previous document understanding
    surveys that primarily focus on isolated subtasks.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文专注于实现对整个多页文档的整体理解的多模态能力，这与以往主要关注孤立子任务的文档理解调查有所不同。
- en: It should be noted that this article only investigates models and datasets designed
    for the entire document or the document page rather than the specific document
    components such as table, chart. Therefore, models and datasets proposed for table
    detection, table structure recognition, and chart or plot question answering will
    not be summarized in the main body of this survey.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，本文只调查了针对整个文档或文档页面设计的模型和数据集，而非具体的文档组件，如表格、图表。因此，对于表格检测、表格结构识别和图表或图形问答的模型和数据集，将不会在本文的主体部分总结。
- en: 1.3\. Related Surveys
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 相关调查
- en: Several surveys offer a comprehensive overview of general document understanding
    tasks. These surveys primarily focus on document layout analysis (Subramani et al.,
    [2020](#bib.bib113)), table extraction (Liu et al., [2023](#bib.bib73)), named
    entity recognition (Ehrmann et al., [2023](#bib.bib31)), and document image analysis
    (Lombardi and Marinai, [2020](#bib.bib77)) across diverse document types, including
    invoices (Saout et al., [2024](#bib.bib105)) and historical document (Ehrmann
    et al., [2023](#bib.bib31); Lombardi and Marinai, [2020](#bib.bib77)). In particular,
    computer vision-based research focuses on scanned document analysis and structure
    understanding. While these studies have advanced document image analysis, they
    often focus on fragmented subtasks and fall short of providing a holistic understanding
    of an entire document on multiple pages.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一些调查提供了对一般文档理解任务的全面概述。这些调查主要关注文档布局分析（Subramani 等，[2020](#bib.bib113)）、表格提取（Liu
    等，[2023](#bib.bib73)）、命名实体识别（Ehrmann 等，[2023](#bib.bib31)）和文档图像分析（Lombardi 和 Marinai，[2020](#bib.bib77)），涵盖了各种文档类型，包括发票（Saout
    等，[2024](#bib.bib105)）和历史文档（Ehrmann 等，[2023](#bib.bib31)；Lombardi 和 Marinai，[2020](#bib.bib77)）。特别是，基于计算机视觉的研究专注于扫描文档分析和结构理解。虽然这些研究推动了文档图像分析的发展，但它们通常集中于碎片化的子任务，未能提供对多页文档的整体理解。
- en: Recent advancements in deep learning have fueled the emergence of VRDU tasks,
    which demand complex document content understanding capabilities. These tasks
    include key information extraction, question answering, and document entity linking.
    However, existing surveys have not adequately addressed the unique challenges
    and opportunities presented by VRDU, with a specific emphasis on deep learning-based
    multimodal approaches (Cui et al., [2021](#bib.bib20)). To bridge this gap, this
    survey aims to provide a comprehensive overview of VRDU frameworks and datasets,
    including multimodal feature extractions and fusions in both mono and multi-task
    VRD models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的最新进展促进了VRDU任务的出现，这些任务要求复杂的文档内容理解能力。这些任务包括关键信息提取、问答和文档实体链接。然而，现有的调查并没有充分解决VRDU所带来的独特挑战和机遇，尤其是基于深度学习的多模态方法（Cui
    et al., [2021](#bib.bib20)）。为弥补这一空白，本调查旨在提供VRDU框架和数据集的全面概述，包括单任务和多任务VRD模型中的多模态特征提取和融合。
- en: 1.4\. Contributions
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4\. 贡献
- en: 'The main contributions of this paper can be summarized as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献可以总结如下：
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper provides a detailed review and systematic categorization of VRDU frameworks
    and benchmark datasets, organized based on adopted strategies and downstream tasks.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该论文提供了对VRDU框架和基准数据集的详细回顾和系统分类，基于采用的策略和下游任务进行组织。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It critically examines and compares different techniques used in VRDU models,
    focusing on feature representation and fusion, model architecture, and pretraining
    methods, highlighting their strengths, limitations, and appropriate scenarios.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它批判性地审查和比较了VRDU模型中使用的不同技术，重点关注特征表示和融合、模型架构和预训练方法，突出了它们的优点、局限性和适用场景。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper identifies emerging trends and challenges in visually rich document
    content understanding, offering insights into future research directions and practical
    applications.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文识别了视觉丰富文档内容理解中的新兴趋势和挑战，提供了对未来研究方向和实际应用的见解。
- en: 1.5\. Survey Structure
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5\. 调查结构
- en: 'Section [1](#S1 "1\. Introduction ‣ Deep Learning based Visually Rich Document
    Content Understanding: A Survey") in this survey discusses the background of visually
    rich document understanding and emphasizes the aim, scope, and contributions of
    this paper. Section [2](#S2 "2\. Background ‣ Deep Learning based Visually Rich
    Document Content Understanding: A Survey") provides additional background knowledge,
    including definitions of the VRDU tasks covered in this research and the development
    of document understanding. Sections [3](#S3 "3\. Mono-Task Document Understanding
    Frameworks ‣ Deep Learning based Visually Rich Document Content Understanding:
    A Survey") and [3](#S4.F3 "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models
    ‣ Deep Learning based Visually Rich Document Content Understanding: A Survey")
    review significant frameworks for mono-task and multi-task VRDU, respectively.
    Section [5](#S5 "5\. Visually Rich Document Content Understanding Datasets ‣ Deep
    Learning based Visually Rich Document Content Understanding: A Survey") compiles
    benchmark datasets for three VRDU subtasks: scanned receipts, forms, and single
    and multiple-page documents from diverse domains. Section [6](#S6 "6\. Critical
    Discussion ‣ Deep Learning based Visually Rich Document Content Understanding:
    A Survey") critically examines and compares the techniques adopted by various
    models, highlighting their strengths and limitations. Finally, Section [7](#S7
    "7\. Conclusion ‣ Deep Learning based Visually Rich Document Content Understanding:
    A Survey") summarizes the trends and future directions in this field.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的[第1节](#S1 "1\. 介绍 ‣ 基于深度学习的视觉丰富文档内容理解：综述")讨论了视觉丰富文档理解的背景，并强调了本文的目的、范围和贡献。[第2节](#S2
    "2\. 背景 ‣ 基于深度学习的视觉丰富文档内容理解：综述")提供了额外的背景知识，包括本研究中涵盖的VRDU任务定义和文档理解的发展。[第3节](#S3
    "3\. 单任务文档理解框架 ‣ 基于深度学习的视觉丰富文档内容理解：综述")和[第4节](#S4.F3 "图3 ‣ 4\. 多任务VRD理解模型 ‣ 基于深度学习的视觉丰富文档内容理解：综述")分别回顾了单任务和多任务VRDU的重要框架。[第5节](#S5
    "5\. 视觉丰富文档内容理解数据集 ‣ 基于深度学习的视觉丰富文档内容理解：综述")汇编了三个VRDU子任务的基准数据集：扫描收据、表单以及来自不同领域的单页和多页文档。[第6节](#S6
    "6\. 关键讨论 ‣ 基于深度学习的视觉丰富文档内容理解：综述")批判性地审查和比较了各种模型采用的技术，突出了它们的优缺点。最后，[第7节](#S7 "7\.
    结论 ‣ 基于深度学习的视觉丰富文档内容理解：综述")总结了该领域的趋势和未来方向。
- en: 2\. Background
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景
- en: 2.1\. Task Definition
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 任务定义
- en: Based on the differences in the purpose and application scenarios, visually
    rich document understanding tasks are classified into three categories, including
    Key Information Extraction, Question Answering and Entity Linking.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 根据目的和应用场景的不同，视觉丰富的文档理解任务被分为三个类别，包括关键信息提取、问答和实体链接。
- en: '![Refer to caption](img/13198d7fb8081e85dcc98e2000bc4af0.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/13198d7fb8081e85dcc98e2000bc4af0.png)'
- en: Figure 1\. Visually rich document content understanding task clarifications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 视觉丰富的文档内容理解任务说明。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Key Information Extraction: refers to identifying and extracting the relevant
    information based on the given text queries. Distinct pre-defined queries can
    be defined based on the domain of targeting documents and practical demands. For
    example, the crucial information of scanned receipts contains ”Store Name”, ”Address”,
    ”Item” and ”Price”, while for the financial reports, ”Company Name”, ”Share Holder
    Name”, ”Number of Interests” may be the critical information need to be extracted.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关键信息提取：指根据给定的文本查询识别和提取相关信息。可以根据目标文档的领域和实际需求定义不同的预定义查询。例如，扫描收据的关键信息包括“商店名称”、“地址”、“商品”和“价格”，而对于财务报告，“公司名称”、“股东名称”、“利息数量”可能是需要提取的关键信息。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Question Answering: is a task of answering questions about a VRD by using natural
    languages. Based on the answer types, it can be divided into extractive VQA and
    abstractive VQA. The answer from extractive VQA is directly extracted from the
    target document, while abstractive VQA requires generative answers based on comprehensively
    understanding questions and related VRDs.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问答任务：是通过使用自然语言回答关于 VRD 的问题的任务。根据答案类型，它可以分为抽取式 VQA 和 摘要式 VQA。抽取式 VQA 的答案直接从目标文档中提取，而摘要式
    VQA 需要基于对问题和相关 VRD 的全面理解生成答案。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Document Entity Linking: refers to identifying the semantic relations between
    document entities to construct the logical structure of the input document image.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文档实体链接：指识别文档实体之间的语义关系，以构建输入文档图像的逻辑结构。
- en: 2.2\. Development of Document Understanding
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 文档理解的发展
- en: 2.2.1\. Traditional Approaches
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 传统方法
- en: 'Rule-based methods, as highlighted in several studies (Watanabe et al., [1995](#bib.bib133);
    O’Gorman, [1993](#bib.bib88); Seki et al., [2007](#bib.bib106); Rusinol et al.,
    [2013](#bib.bib104)), have demonstrated high precision in domain-specific applications.
    However, these methods have several drawbacks: they are manually intensive, costly,
    and require expert intervention for tailored customization. Additionally, they
    are inflexible, often necessitating frequent manual updates, even for minor modifications.
    In response to these limitations, machine learning-based approaches have been
    proposed for document understanding. For instance, SVM-based methods have been
    utilized for layout understanding (Oliveira and Viana, [2017](#bib.bib89)), and
    TF-IDF techniques combined with hand-crafted features have been applied for extracting
    information from invoices. Furthermore, rule-based and statistical models have
    been used for entity extraction. Despite these advancements, machine learning
    methods rely heavily on human intervention and domain-specific expertise. They
    are time-consuming and often deliver suboptimal performance. Moreover, the majority
    of these methods typically depend on single-modality data, restricting them to
    either layout, text, or visual inputs.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如几项研究中所强调的规则基础方法（Watanabe 等，[1995](#bib.bib133)；O’Gorman，[1993](#bib.bib88)；Seki
    等，[2007](#bib.bib106)；Rusinol 等，[2013](#bib.bib104)），在特定领域应用中表现出高精度。然而，这些方法也有几个缺点：它们需要大量人工操作，成本高，并且需要专家干预进行定制化。此外，这些方法不够灵活，通常需要频繁的人工更新，即使是微小的修改也需要重新调整。为了应对这些局限性，已经提出了基于机器学习的方法来进行文档理解。例如，SVM
    基础的方法已被用于布局理解（Oliveira 和 Viana，[2017](#bib.bib89)），而 TF-IDF 技术与手工特征结合已被应用于发票信息提取。此外，规则基础和统计模型已被用于实体提取。尽管有这些进展，机器学习方法仍然严重依赖人工干预和领域特定的专业知识。它们耗时且通常表现不佳。此外，这些方法大多数依赖于单一模态数据，限制了它们对布局、文本或视觉输入的处理。
- en: 2.2.2\. Single Modality-based Approaches
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. 单一模态方法
- en: With the advancement of deep learning, deeper model architectures such as CNNs
    (Katti et al., [2018](#bib.bib51); Yang et al., [2017](#bib.bib139)) have emerged,
    and pretrained language (Devlin, [2018](#bib.bib23); Liu et al., [2019b](#bib.bib75))
    and vision models (Ren et al., [2015](#bib.bib103); He et al., [2017](#bib.bib39))
    are now commonly used as robust baselines for understanding VRD content and structure.
    Due to the multimodal nature of VRDs, which involves the integration of text,
    vision, and layout, researchers are increasingly focusing on leveraging this combined
    information to achieve significant improvements in various downstream VRDU tasks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的进步，出现了更深层次的模型架构，如 CNN（Katti 等，[2018](#bib.bib51)；Yang 等，[2017](#bib.bib139)），预训练的语言模型（Devlin，[2018](#bib.bib23)；Liu
    等，[2019b](#bib.bib75)）和视觉模型（Ren 等，[2015](#bib.bib103)；He 等，[2017](#bib.bib39)）现在被广泛用作理解
    VRD 内容和结构的强大基线。由于 VRD 的多模态特性，涉及文本、视觉和布局的整合，研究人员越来越专注于利用这些综合信息，以在各种下游 VRDU 任务中实现显著的改进。
- en: 2.2.3\. Cross Modality based Approaches
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. 跨模态方法
- en: Considering the multimodal nature of VRDs, many frameworks propose various methods
    to encode multimodal information, including text, vision, and layout, and fuse
    them effectively. Text and vision information is normally encoded by pretrained
    backbones such as BERT (Devlin, [2018](#bib.bib23)) or RoBERTa (Liu et al., [2019b](#bib.bib75))
    for textual features, Faster-RCNN (Ren et al., [2015](#bib.bib103)) or Mask-RCNN
    (He et al., [2017](#bib.bib39)) for visual features. For layout information, different
    encoding methods are introduced, including linear projection (Wang et al., [2020b](#bib.bib130)),
    2D positional encoding (Xu et al., [2020a](#bib.bib136)), and attention bias to
    allow the proposed models to be layout-sensitive. Different feature fusion methods
    are introduced including summing up (Yu et al., [2021](#bib.bib142)), concatenation
    (Lee et al., [2022b](#bib.bib57)), attention-based contextual learning (Majumder
    et al., [2020a](#bib.bib81)), and prompting (He et al., [2023](#bib.bib38)). However,
    most of those frameworks leverage implicit knowledge from pretrained backbones
    with a task-orientated shadow adapter for specific VRDU downstream tasks such
    as KIE (Lee et al., [2023](#bib.bib58); Wang and Shang, [2022](#bib.bib128); Chen
    et al., [2023](#bib.bib15); Cao et al., [2023b](#bib.bib12)) or EL (Zhang et al.,
    [2021](#bib.bib146); Hu et al., [2023](#bib.bib42); Carbonell et al., [2021](#bib.bib13)).
    Those frameworks tend to achieve delicate performance on specific tasks or document
    formats instead of acquiring a generalised model to represent documents comprehensively.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 VRD 的多模态特性，许多框架提出了各种方法来编码多模态信息，包括文本、视觉和布局，并有效地融合这些信息。文本和视觉信息通常由预训练的骨干网络编码，例如
    BERT（Devlin，[2018](#bib.bib23)）或 RoBERTa（Liu 等，[2019b](#bib.bib75)）用于文本特征，Faster-RCNN（Ren
    等，[2015](#bib.bib103)）或 Mask-RCNN（He 等，[2017](#bib.bib39)）用于视觉特征。对于布局信息，提出了不同的编码方法，包括线性投影（Wang
    等，[2020b](#bib.bib130)）、2D 位置编码（Xu 等，[2020a](#bib.bib136)）和注意力偏置，以使所提出的模型对布局敏感。引入了不同的特征融合方法，包括加和（Yu
    等，[2021](#bib.bib142)）、拼接（Lee 等，[2022b](#bib.bib57)）、基于注意力的上下文学习（Majumder 等，[2020a](#bib.bib81)）和提示（He
    等，[2023](#bib.bib38)）。然而，大多数这些框架依赖于来自预训练骨干网络的隐式知识，并为特定的 VRDU 下游任务（如 KIE（Lee 等，[2023](#bib.bib58)；Wang
    和 Shang，[2022](#bib.bib128)；Chen 等，[2023](#bib.bib15)；Cao 等，[2023b](#bib.bib12)）或
    EL（Zhang 等，[2021](#bib.bib146)；Hu 等，[2023](#bib.bib42)；Carbonell 等，[2021](#bib.bib13)）使用任务导向的影子适配器。这些框架往往在特定任务或文档格式上取得精细的表现，而不是获得一个全面表示文档的通用模型。
- en: 2.2.4\. Multimodal Pre-training Approches
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4\. 多模态预训练方法
- en: Inspired by the success of BERT-style models (Devlin, [2018](#bib.bib23); Liu
    et al., [2019b](#bib.bib75)) in acquiring knowledge through self-supervised learning,
    pretrained document understanding models have emerged to harness self-supervised
    or supervised pretraining tasks from extensive document collections. LayoutLM
    (Xu et al., [2020a](#bib.bib136)), the first encoder-only VRDU model, utilizes
    self-supervised tasks, such as masked vision-language modelling, with text and
    layout information. Subsequent models have expanded on this by integrating layout
    information (Wang et al., [2022b](#bib.bib123); Tu et al., [2023](#bib.bib119);
    Li et al., [2021c](#bib.bib68)) and visual cues (Xu et al., [2020b](#bib.bib138);
    Huang et al., [2022](#bib.bib43)) through multimodal transformers. While encoder-only
    models have shown significant improvements on various benchmark datasets (Jaume
    et al., [2019](#bib.bib47); Park et al., [2019](#bib.bib93); Huang et al., [2019](#bib.bib44);
    Mathew et al., [2021](#bib.bib84); Harley et al., [2015](#bib.bib37)), they often
    require detailed annotations and are limited by fixed input lengths. To address
    these limitations, encoder-decoder frameworks (Tang et al., [2022](#bib.bib116);
    Kim et al., [2022](#bib.bib52); Davis et al., [2022](#bib.bib21)) and prompt-based
    methods for LLMs/MLLMs (Luo et al., [2024](#bib.bib79); Liu et al., [2024b](#bib.bib71))
    have been developed, enhancing layout awareness and performance in VRDU tasks.
    However, a significant gap remains in effectively applying these models in real-world
    scenarios with zero shot.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 受到BERT风格模型（Devlin, [2018](#bib.bib23); Liu et al., [2019b](#bib.bib75)）通过自监督学习获取知识成功的启发，预训练的文档理解模型应运而生，以利用来自大量文档集合的自监督或监督预训练任务。LayoutLM
    (Xu et al., [2020a](#bib.bib136))，第一个仅编码器的VRDU模型，利用了自监督任务，如遮蔽视觉语言建模，结合文本和布局信息。
    随后的模型通过集成布局信息（Wang et al., [2022b](#bib.bib123); Tu et al., [2023](#bib.bib119);
    Li et al., [2021c](#bib.bib68)）和视觉线索（Xu et al., [2020b](#bib.bib138); Huang et
    al., [2022](#bib.bib43)）扩展了这一点，采用了多模态变换器。 尽管仅编码器模型在各种基准数据集上显示了显著改进（Jaume et al.,
    [2019](#bib.bib47); Park et al., [2019](#bib.bib93); Huang et al., [2019](#bib.bib44);
    Mathew et al., [2021](#bib.bib84); Harley et al., [2015](#bib.bib37)），但它们通常需要详细的注释，并且受限于固定的输入长度。
    为了应对这些限制，已开发了编码器-解码器框架（Tang et al., [2022](#bib.bib116); Kim et al., [2022](#bib.bib52);
    Davis et al., [2022](#bib.bib21)）和针对LLMs/MLLMs的提示方法（Luo et al., [2024](#bib.bib79);
    Liu et al., [2024b](#bib.bib71)），从而增强了布局意识和VRDU任务中的性能。 然而，在有效地将这些模型应用于实际场景时，仍存在显著差距。
- en: 3\. Mono-Task Document Understanding Frameworks
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 单任务文档理解框架
- en: Visually rich document content understanding encompasses several independent
    downstream tasks tailored to different application scenarios and user demands.
    This section introduces methods focused on specific document understanding tasks.
    Many of these models aim to provide comprehensive document representation and
    integrate target-oriented modules or techniques to improve performance and efficiency
    across various VRDU tasks. The models for three VRDU downstream tasks—Key Information
    Extraction (KIE), Entity Linking (EL), and Visual Question Answering (VQA)—are
    introduced and summarized with insights into current trends.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉丰富的文档内容理解涵盖了几个独立的下游任务，这些任务针对不同的应用场景和用户需求。 本节介绍了专注于特定文档理解任务的方法。 许多这些模型旨在提供全面的文档表示，并整合面向目标的模块或技术，以提高在各种VRDU任务中的性能和效率。
    介绍和总结了三种VRDU下游任务的模型——关键信息提取（KIE）、实体链接（EL）和视觉问答（VQA），并对当前趋势进行了深入分析。
- en: '{forest}'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: 'for tree= forked edges, grow’=0, draw, rounded corners, node options=align=center,,
    text width=3cm, s sep=6pt, calign=child edge, calign child=(n_children()+1)/2
    [Mono-task based models (Content Understanding), for tree=fill=lime!60 [Key Information
    Extraction, for tree=fill=cyan!40 [Feature-driven Models, for tree=fill=cyan!25
    [Chargrid(Katti et al., [2018](#bib.bib51)); CUTIE(Zhao et al., [2019](#bib.bib148));BERTgrid(Denk
    and Reisswig, [2019](#bib.bib22)); ACP(Palm et al., [2019](#bib.bib92));XYLayoutLM(Gu
    et al., [2022](#bib.bib34)), for tree=fill=cyan!10] ] [Joint-learning Frameworks,
    for tree=fill=cyan!25 [TRIE(Zhang et al., [2020](#bib.bib145)); VIES(Wang et al.,
    [2021a](#bib.bib124)), for tree=fill=cyan!10] ] [Relation-aware Models, for tree=fill=cyan!25
    [Majumder et al. (Majumder et al., [2020a](#bib.bib81)); Liu et al. (Liu et al.,
    [2019a](#bib.bib74)); PICK(Yu et al., [2021](#bib.bib142)); FormNet(Lee et al.,
    [2022b](#bib.bib57)); FormNetv2(Lee et al., [2023](#bib.bib58)), for tree=fill=cyan!10]
    ] [Few/Zero-shot Learning Models, for tree=fill=cyan!25 [LASER(Wang and Shang,
    [2022](#bib.bib128)); Chen et al.(Chen et al., [2023](#bib.bib15)); Cheng et al.(Cheng
    et al., [2020](#bib.bib18)); QueryForm(Wang et al., [2023a](#bib.bib131)), for
    tree=fill=cyan!10] ] [Prompt-based Frameworks, for tree=fill=cyan!25 [GenKIE(Cao
    et al., [2023b](#bib.bib12)); ICL-D3IE(He et al., [2023](#bib.bib38)); LMDX(Perot
    et al., [2023](#bib.bib95)), for tree=fill=cyan!10] ] ] [Entity Linking, for tree=fill=blue!40
    [Entity-level Linking, for tree=fill=blue!20 [DocStruct(Wang et al., [2020b](#bib.bib130));
    Zhang et al.(Zhang et al., [2021](#bib.bib146)); KVPFormer.(Hu et al., [2023](#bib.bib42)),
    for tree=fill=blue!10] ] [Token-level Linking, for tree=fill=blue!20 [Carbonell
    et al. (Carbonell et al., [2021](#bib.bib13)); SPADE (Hwang et al., [2021](#bib.bib45));DocTR(Liao
    et al., [2023](#bib.bib69)), for tree=fill=blue!10] ] ] [Visual Question Answering,
    for tree=fill=teal!40 [Single Page Frameworks, for tree=fill=teal!25 [Please refer
    Section [3](#S4.F3 "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep
    Learning based Visually Rich Document Content Understanding: A Survey"). , for
    tree=fill=teal!10] ] [Multi-Page Frameworks, for tree=fill=teal!25 [Hi-VT5(Tito
    et al., [2023](#bib.bib117)); GRAM(Blau et al., [2024](#bib.bib9)); Kang et al.
    (Kang et al., [2024](#bib.bib50)), for tree=fill=teal!10] ] ] ]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 tree= forked edges, grow’=0, draw, rounded corners, node options=align=center,,
    text width=3cm, s sep=6pt, calign=child edge, calign child=(n_children()+1)/2
    [单任务模型（内容理解），对于 tree=fill=lime!60 [关键信息提取，对于 tree=fill=cyan!40 [特征驱动模型，对于 tree=fill=cyan!25
    [Chargrid(Katti et al., [2018](#bib.bib51)); CUTIE(Zhao et al., [2019](#bib.bib148));BERTgrid(Denk
    and Reisswig, [2019](#bib.bib22)); ACP(Palm et al., [2019](#bib.bib92));XYLayoutLM(Gu
    et al., [2022](#bib.bib34))，对于 tree=fill=cyan!10] ] [联合学习框架，对于 tree=fill=cyan!25
    [TRIE(Zhang et al., [2020](#bib.bib145)); VIES(Wang et al., [2021a](#bib.bib124))，对于
    tree=fill=cyan!10] ] [关系感知模型，对于 tree=fill=cyan!25 [Majumder et al. (Majumder et
    al., [2020a](#bib.bib81)); Liu et al. (Liu et al., [2019a](#bib.bib74)); PICK(Yu
    et al., [2021](#bib.bib142)); FormNet(Lee et al., [2022b](#bib.bib57)); FormNetv2(Lee
    et al., [2023](#bib.bib58))，对于 tree=fill=cyan!10] ] [少样本/零样本学习模型，对于 tree=fill=cyan!25
    [LASER(Wang and Shang, [2022](#bib.bib128)); Chen et al.(Chen et al., [2023](#bib.bib15));
    Cheng et al.(Cheng et al., [2020](#bib.bib18)); QueryForm(Wang et al., [2023a](#bib.bib131))，对于
    tree=fill=cyan!10] ] [基于提示的框架，对于 tree=fill=cyan!25 [GenKIE(Cao et al., [2023b](#bib.bib12));
    ICL-D3IE(He et al., [2023](#bib.bib38)); LMDX(Perot et al., [2023](#bib.bib95))，对于
    tree=fill=cyan!10] ] ] [实体链接，对于 tree=fill=blue!40 [实体级链接，对于 tree=fill=blue!20
    [DocStruct(Wang et al., [2020b](#bib.bib130)); Zhang et al.(Zhang et al., [2021](#bib.bib146));
    KVPFormer.(Hu et al., [2023](#bib.bib42))，对于 tree=fill=blue!10] ] [标记级链接，对于 tree=fill=blue!20
    [Carbonell et al. (Carbonell et al., [2021](#bib.bib13)); SPADE (Hwang et al.,
    [2021](#bib.bib45));DocTR(Liao et al., [2023](#bib.bib69))，对于 tree=fill=blue!10]
    ] ] [视觉问答，对于 tree=fill=teal!40 [单页框架，对于 tree=fill=teal!25 [请参考第 [3](#S4.F3 "图
    3 ‣ 4\. 多任务 VRD 理解模型 ‣ 基于深度学习的视觉丰富文档内容理解：综述") 节。对于 tree=fill=teal!10] ] [多页框架，对于
    tree=fill=teal!25 [Hi-VT5(Tito et al., [2023](#bib.bib117)); GRAM(Blau et al.,
    [2024](#bib.bib9)); Kang et al. (Kang et al., [2024](#bib.bib50))，对于 tree=fill=teal!10]
    ] ] ]
- en: Figure 2\. Mono-task visually rich document understanding models
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 单任务视觉丰富文档理解模型
- en: 3.1\. Key Information Extraction
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 关键信息提取
- en: 'Key Information Extraction (KIE), a typical natural language processing task,
    refers to the task of identifying and extracting crucial pieces of information
    from textual data. Unlike typical name entity recognition methods for addressing
    plain text, VRDs contain visually rich entities like tables and charts, as well
    as spatial and logical layout arrangements to enhance the challenge of extracting
    crucial information. Although plain-text pretrained language models, such as BERT
    (Devlin, [2018](#bib.bib23)), RoBERTa (Liu et al., [2019b](#bib.bib75)), and ALBERT
    (Lan et al., [2019](#bib.bib56)), are widely used as solid baselines on many benchmark
    datasets, more recent works introduce layout-aware pretrained models such as LayoutLM
    families (Xu et al., [2020a](#bib.bib136), [b](#bib.bib138); Huang et al., [2022](#bib.bib43)),
    LiLT (Wang et al., [2022b](#bib.bib123)), Bros (Hong et al., [2021](#bib.bib41))
    to enhance the document representation by leveraging visual and layout information
    and achieving SoTA performance on several downstream tasks (see Section [3](#S4.F3
    "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey")). This section will mainly focus
    on models specifically proposed for the document KIE models or only evaluated
    on KIE benchmark datasets including FUNSD (Jaume et al., [2019](#bib.bib47)),
    CORD (Park et al., [2019](#bib.bib93)), SROIE (Huang et al., [2019](#bib.bib44)),
    XFUND (Xu et al., [2021](#bib.bib137)), etc. Based on innovative aspects, we categorise
    KIE frameworks into five types: Feature-driven models use multimodal cues for
    rich feature representation. Joint Learning frameworks integrate auxiliary tasks
    to enhance document representation. Relation-aware models leverage spatial or
    logical relations via graphs or masked attention mechanisms. Few/Zero-shot learning
    frameworks explore methods for extracting key information with minimal labelled
    data, often using transfer learning. Prompt-based frameworks use structured prompts
    to guide specific information extraction from pretrained models or LLM/MLLMs.
    These categories represent diverse approaches to improving Key Information Extraction
    (KIE) by leveraging specific model designs and learning strategies tailored to
    document understanding challenges.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '关键信息提取（KIE），一种典型的自然语言处理任务，指的是从文本数据中识别和提取关键的信息。与典型的名称实体识别方法不同，视觉丰富的数据（VRDs）包含表格和图表等视觉丰富的实体，以及空间和逻辑布局安排，增加了提取关键信息的挑战。尽管纯文本预训练语言模型，如BERT（Devlin,
    [2018](#bib.bib23)）、RoBERTa（Liu et al., [2019b](#bib.bib75)）和ALBERT（Lan et al.,
    [2019](#bib.bib56)），在许多基准数据集上作为坚实的基准被广泛使用，但最近的工作引入了布局感知的预训练模型，如LayoutLM系列（Xu et
    al., [2020a](#bib.bib136)，[b](#bib.bib138)；Huang et al., [2022](#bib.bib43)）、LiLT（Wang
    et al., [2022b](#bib.bib123)）、Bros（Hong et al., [2021](#bib.bib41)）以通过利用视觉和布局信息来增强文档表示，并在几个下游任务上实现了最先进的性能（参见第[3](#S4.F3
    "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey")节）。本节将主要关注专门为文档KIE模型提出或仅在KIE基准数据集上评估的模型，包括FUNSD（Jaume
    et al., [2019](#bib.bib47)）、CORD（Park et al., [2019](#bib.bib93)）、SROIE（Huang
    et al., [2019](#bib.bib44)）、XFUND（Xu et al., [2021](#bib.bib137)）等。基于创新方面，我们将KIE框架分为五类：特征驱动模型使用多模态线索进行丰富的特征表示。联合学习框架整合辅助任务以增强文档表示。关系感知模型通过图或掩蔽注意机制利用空间或逻辑关系。少样本/零样本学习框架探索使用最少标记数据提取关键信息的方法，通常使用迁移学习。基于提示的框架使用结构化提示来指导从预训练模型或LLM/MLLM中提取特定信息。这些类别代表了通过利用特定模型设计和学习策略来提高关键信息提取（KIE）的多样化方法，以应对文档理解挑战。'
- en: 3.1.1\. Feature-driven Models
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1. 特征驱动模型
- en: In the initial phase, certain Recurrent Neural Network (RNN)-based models (Lample
    et al., [2016](#bib.bib55)) were introduced, primarily focusing on key information
    extraction tasks from plain text. However, these approaches overlook the importance
    of visual cues and layout information. Hence, several multimodal frameworks with
    feature-driven designs have been proposed to generate more representative document
    representations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在初期阶段，某些基于递归神经网络（RNN）的模型（Lample et al., [2016](#bib.bib55)）被引入，主要集中于从纯文本中提取关键信息。然而，这些方法忽略了视觉线索和布局信息的重要性。因此，提出了几种具有特征驱动设计的多模态框架，以生成更具代表性的文档表示。
- en: Chargrid (Katti et al., [2018](#bib.bib51)) first mentioned the significance
    of 2D structure for document KIE and designed a character-box-based chargrid to
    convert the textual and 2D layout structure into coloured visual cues feed into
    CNN. Instead of using fine-grained character information, CUTIE (Zhao et al.,
    [2019](#bib.bib148)) and BERTgrid (Denk and Reisswig, [2019](#bib.bib22)) utilise
    various word embedding methods with bounding box coordinates to persevere the
    layout structure. ACP (Palm et al., [2019](#bib.bib92)) leverages attention mechanism
    and multi-aspect features, including visual, semantic (both character and word)
    and spatial, into dilated CNN for capturing both short and long-term dependencies
    of each word piece.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Chargrid（Katti et al., [2018](#bib.bib51)）首次提到2D结构在文档KIE中的重要性，并设计了基于字符框的chargrid，将文本和2D布局结构转换为输入CNN的彩色视觉提示。与使用细粒度字符信息不同，CUTIE（Zhao
    et al., [2019](#bib.bib148)）和BERTgrid（Denk and Reisswig, [2019](#bib.bib22)）利用各种词嵌入方法和边界框坐标来保持布局结构。ACP（Palm
    et al., [2019](#bib.bib92)）利用注意力机制和多方面特征，包括视觉、语义（字符和单词）和空间特征，结合扩张CNN捕捉每个单词片段的短期和长期依赖关系。
- en: Joint learning KIE frameworks are proposed to leverage multi-level features
    to mitigate the information gap between various focused tasks. TRIE (Zhang et al.,
    [2020](#bib.bib145)) firstly offers an end-to-end framework for simultaneously
    conducting Optical Character Recognition (OCR) and KIE. OCR module will generate
    multi-aspect features, including positional, visual and textual aspects. Adaptively
    trainable weighting mechanisms are adopted to generate the fused embedding, followed
    by a Bi-LSTM-based Entity Extraction Module to conduct the final prediction. VIES
    (Wang et al., [2021a](#bib.bib124)) is introduced to use the multi-level cues
    of vision, position and text to generate more comprehensive representations. Both
    token and segment (entity) level positional and visual features are acquired from
    text detection modules, and dual-level textual features are gathered by text recognition
    brunch, fused by a self-attention-based fusion module for sequence labelling.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 联合学习KIE框架被提出用于利用多层特征来减轻各种聚焦任务之间的信息差距。TRIE（Zhang et al., [2020](#bib.bib145)）首次提供了一个端到端的框架，以同时进行光学字符识别（OCR）和KIE。OCR模块将生成多方面的特征，包括位置、视觉和文本方面。采用适应性可训练的加权机制来生成融合的嵌入，随后通过基于Bi-LSTM的实体提取模块进行最终预测。VIES（Wang
    et al., [2021a](#bib.bib124)）被介绍用于使用视觉、位置和文本的多层提示生成更全面的表示。从文本检测模块中获取标记和片段（实体）级别的位置和视觉特征，从文本识别分支中收集双级文本特征，通过基于自注意力的融合模块融合进行序列标注。
- en: 3.1.2\. Relation-aware Models
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2. 关系感知模型
- en: Instead of only leveraging multimodal information, leveraging the inherent spatial
    or logical relations between multi-level components may lead to more robust and
    comprehensive document representations. Majumder et al. (Majumder et al., [2020a](#bib.bib81))
    propose a model which starts by generating candidates for each field using type-specific
    detectors and key phrases. The neural model scores these candidates by learning
    dense representations that consider both textual content and spatial positioning,
    allowing for accurate information extraction across different document templates
    by focusing on the candidates’ relevance to the fields. Graph-based frameworks
    are increasingly favoured for modelling document elements’ spatial and logical
    relationships. This trend involves meticulously defining distinct graph structures
    and employing graph convolution techniques to incorporate these relationships
    into feature representations seamlessly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅仅利用多模态信息不同，利用多层组件之间固有的空间或逻辑关系可能会导致更稳健和全面的文档表示。Majumder 等（Majumder et al.,
    [2020a](#bib.bib81)）提出了一种模型，该模型通过类型特定的检测器和关键短语为每个字段生成候选项。神经网络模型通过学习密集的表示来对这些候选项进行评分，这些表示考虑了文本内容和空间位置，从而通过关注候选项与字段的相关性来准确提取不同文档模板中的信息。基于图的框架越来越受到青睐，用于建模文档元素的空间和逻辑关系。这一趋势涉及精确地定义不同的图结构，并采用图卷积技术将这些关系无缝地融入特征表示中。
- en: Liu et al. (Liu et al., [2019a](#bib.bib74)) first utilise a graph convolution
    module on top of the BiLSTM-CRF for key information extraction. Each document
    is a fully connected graph where the node is the textual representation $T$ of
    the document entity $E_{i}\in\mathbb{E}$, and the edge is related to the spatial
    relation between $E_{i}$ and another entity $E_{j}$. It is defined as $e_{ij}=[x_{ij},y_{ij},\frac{w_{i}}{h_{i}},\frac{h_{j}}{h_{i}},\frac{w_{j}}{h_{i}}]$,
    where $x_{ij}$ and $y_{ij}$ are horizontal and vertical distance between $E_{i}$
    and $E_{j}$. The self-attention-based graph convolution is performed on node-edge-node
    triplets (concatenated nodes and edge embeddings) to learn contextually with all
    other nodes. PICK (Yu et al., [2021](#bib.bib142)) adopts a similar way to construct
    the document graph but utilising multimodal node representations, including transformer-encoded
    textual embedding and CNN-encoded visual embedding, as well as the edge embedding
    is updated to $e_{ij}=[x_{ij},y_{ij},\frac{w_{i}}{h_{i}},\frac{h_{j}}{h_{i}},\frac{w_{j}}{h_{i}},\frac{S_{j}}{S_{i}}]$
    where $S$ is the sentence length of corresponding entity. Additionally, to get
    the node embedding for the downstream tagging task, a soft adjacent matrix-based
    graph learning layer is applied to obtain the task-specific node representations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Liu 等人（Liu et al., [2019a](#bib.bib74)）首次在 BiLSTM-CRF 的基础上利用图卷积模块进行关键信息提取。每个文档都是一个完全连接的图，其中节点是文档实体
    $E_{i}\in\mathbb{E}$ 的文本表示 $T$，边缘与 $E_{i}$ 和另一个实体 $E_{j}$ 之间的空间关系有关。定义为 $e_{ij}=[x_{ij},y_{ij},\frac{w_{i}}{h_{i}},\frac{h_{j}}{h_{i}},\frac{w_{j}}{h_{i}}]$，其中
    $x_{ij}$ 和 $y_{ij}$ 是 $E_{i}$ 和 $E_{j}$ 之间的水平和垂直距离。基于自注意力的图卷积在节点-边-节点三元组（连接的节点和边嵌入）上执行，以便与所有其他节点进行上下文学习。PICK（Yu
    et al., [2021](#bib.bib142)）采用了类似的方式来构建文档图，但利用了多模态节点表示，包括变换器编码的文本嵌入和 CNN 编码的视觉嵌入，同时边嵌入更新为
    $e_{ij}=[x_{ij},y_{ij},\frac{w_{i}}{h_{i}},\frac{h_{j}}{h_{i}},\frac{w_{j}}{h_{i}},\frac{S_{j}}{S_{i}}]$，其中
    $S$ 是对应实体的句子长度。此外，为了获得下游标注任务的节点嵌入，应用了一种基于软邻接矩阵的图学习层，以获得任务特定的节点表示。
- en: 'FormNet (Lee et al., [2022b](#bib.bib57)) introduces an end-to-end framework
    with a new attention mechanism, Rich Attention, and graph framework to make an
    order/distance sensitive long sequence transformer. In Rich Attention, the model
    introduces two order-based and pixel distance scores along the x/y axis. These
    are summed up with usual self-attention scores to enable the model order/distance
    awareness. Additionally, a Graph is applied to contextually learn the neighbouring
    token embedding before serialising the token sequence feeding into the transformer
    encoders. The graph nodes are text embedding of tokens, and the edges are relative
    positions between nodes. This could mitigate the imperfect serialization issue,
    enabling token representation capturing in more contexts. FormNetv2 (Lee et al.,
    [2023](#bib.bib58)) integrates image modality as the additional Graph edge feature
    to capture more visual cues ¹¹1Please refer to Section [6.1.2](#S6.SS1.SSS2 "6.1.2\.
    Visual Representation ‣ 6.1\. Feature Representation ‣ 6\. Critical Discussion
    ‣ Deep Learning based Visually Rich Document Content Understanding: A Survey")
    to get more details. To use contrastive loss to learn the multimodal graph representation,
    they first perform stochastic graph corruption to sample topology-corrupted and
    feature-corrupted graphs. Topology corruption randomly removes edges in the original
    graph, while feature corruption drops all modality features from nodes and edges.
    Then, the contrastive objective is to maximize agreement between a pair of tokens
    between corrupted and original graphs under a standard normalized temperature-scaled
    cross-entropy (Sohn, [2016](#bib.bib110)) loss.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: FormNet (Lee et al., [2022b](#bib.bib57)) 引入了一种端到端的框架，结合了新的注意力机制——Rich Attention，以及图形框架，旨在创建一个对顺序/距离敏感的长序列变换器。在
    Rich Attention 中，模型引入了两个基于顺序和像素距离的评分，沿 x/y 轴分布。这些评分与常规自注意力评分相加，以实现模型的顺序/距离感知。此外，应用了一个图来在将令牌序列传入变换器编码器之前，上下文地学习相邻令牌的嵌入。图节点是令牌的文本嵌入，边缘是节点之间的相对位置。这可以缓解不完美序列化的问题，使令牌表示能够捕捉到更多上下文。FormNetv2
    (Lee et al., [2023](#bib.bib58)) 将图像模态整合为额外的图边缘特征，以捕获更多视觉线索¹¹1有关更多细节，请参见第[6.1.2](#S6.SS1.SSS2
    "6.1.2\. 视觉表征 ‣ 6.1\. 特征表征 ‣ 6\. 关键讨论 ‣ 基于深度学习的视觉丰富文档内容理解：综述")节。为了利用对比损失学习多模态图表征，他们首先执行随机图腐蚀，以采样拓扑腐蚀和特征腐蚀图。拓扑腐蚀随机移除原始图中的边缘，而特征腐蚀则从节点和边缘中去除所有模态特征。然后，对比目标是最大化在标准归一化温度缩放交叉熵
    (Sohn, [2016](#bib.bib110)) 损失下，腐蚀图与原始图之间的一对令牌的相符程度。
- en: 3.1.3\. Few-shot Learning Frameworks
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 小样本学习框架
- en: Extracting key information from VRDs based on deep learning frameworks typically
    involves more manual work to annotate the training data. However, acquiring large-scale,
    high-quality annotations in urgent and labouring-limited application scenarios
    is challenging. Thus, few-shot or one-shot frameworks emerged to extract the key
    information with less labouring annotations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习框架从 VRD 中提取关键信息通常需要更多的手动工作来标注训练数据。然而，在紧急和劳动限制的应用场景中，获取大规模、高质量的标注是具有挑战性的。因此，出现了小样本或一-shot
    框架，以减少标注劳动量来提取关键信息。
- en: 'LASER (Wang and Shang, [2022](#bib.bib128)) leverage the architecture from
    LayoutReader (Wang et al., [2021b](#bib.bib129)) to reformulate the entity recognition
    task to sequence-labelling to generative model by embedding the entity type information
    (named label surface name) into the target sequence to enable the model label
    semantic-aware. LASER depends on a ”partially triangle” mask to use one encoder
    to encode source text and generate the prediction. The $n$ source text sequence,
    $\{t_{1},...,t_{n}\}$, is the text sequence by summing the relative word, spatial,
    and positional embedding, feeding into the $\mathcal{E}$ with full self-attention.
    The target sequence only attends previous tokens. The generative formulation is
    defined as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: LASER (Wang and Shang, [2022](#bib.bib128)) 利用 LayoutReader (Wang et al., [2021b](#bib.bib129))
    的架构，将实体识别任务重新表述为序列标注，以生成模型通过将实体类型信息（命名标签表面名称）嵌入目标序列，从而使模型能够进行标签语义感知。LASER 依赖于一个“部分三角”掩码，使用一个编码器来编码源文本并生成预测。$n$
    源文本序列，$\{t_{1},...,t_{n}\}$，是通过求和相对词、空间和位置嵌入的文本序列，传入 $\mathcal{E}$ 进行全自注意力处理。目标序列仅关注前面的令牌。生成公式定义为：
- en: '| (1) |  | $\displaystyle t_{i-1},[B],t_{i},...,t_{j},[E],e_{1},...,e_{k},[T],t_{j+1}$
    |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle t_{i-1},[B],t_{i},...,t_{j},[E],e_{1},...,e_{k},[T],t_{j+1}$
    |  |'
- en: 'where $[B]$ and $[E]$ denote start and end of entity, $e_{i}$ are the label
    surface name; $[T]$ denotes the end of the label surface name. As label surface
    names and functional tokens do not exist, the learnable embeddings are applied
    to them to ensure the generative-progress layout-aware. A binary classification
    module is applied to classify the next generated token type of current generated
    token hidden states: from source or not, effectively controlling the generative
    series.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $[B]$ 和 $[E]$ 表示实体的开始和结束，$e_{i}$ 是标签表面名称；$[T]$ 表示标签表面名称的结束。由于标签表面名称和功能性标记不存在，因此应用可学习的嵌入来确保生成进度布局的感知。应用一个二分类模块来分类当前生成的标记隐藏状态的下一个生成标记类型：是来自源的还是不是，从而有效地控制生成系列。
- en: Chen et al. (Chen et al., [2023](#bib.bib15)) introduce a novel framework for
    entity-level, N-way soft-K-shot extracting key information from VRDs, focusing
    on the challenges of extracting rare or unseen entities from documents with few
    examples. It leverages a meta-learning approach (Snell et al., [2017](#bib.bib109);
    Oreshkin et al., [2018](#bib.bib91)), utilizing a hierarchical decoder and contrastive
    learning (ContrastProtoNet) for task personalization and improved adaptation to
    new entity types. Additionally, it introduces FewVEX, a dataset tailored for entity-level
    few-shot VDER, to facilitate research and benchmarking in this area. The framework’s
    effectiveness is demonstrated through significant improvements in robustness and
    performance over existing meta-learning baselines.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人（Chen et al., [2023](#bib.bib15)）引入了一个新颖的框架，用于从 VRDs 中提取实体级 N-way 软-K-shot
    关键信息，关注从仅有少数示例的文档中提取稀有或未见实体的挑战。它利用了一种元学习方法（Snell et al., [2017](#bib.bib109);
    Oreshkin et al., [2018](#bib.bib91)），结合层次解码器和对比学习（ContrastProtoNet）来实现任务个性化和改进对新实体类型的适应。此外，提出了
    FewVEX 数据集，这是一个针对实体级少样本 VDER 的数据集，用于促进该领域的研究和基准测试。通过显著提升鲁棒性和性能，该框架的有效性得到了证明。
- en: In the domain of one-shot scenarios, Cheng et al. (Cheng et al., [2020](#bib.bib18))
    presents a novel application of graphs. They utilize attention mechanisms to seamlessly
    transfer spatial relationships between static text regions ( key/landmark) and
    dynamic text regions (value/field) from support documents to query documents.
    This transfer enables the acquisition of label probability distributions for files.
    Additionally, a self-attention-based module is integrated to exploit relationships
    between field entities, facilitating the derivation of label transition probability
    distributions. Finally, belief propagation is harnessed for inference within a
    pairwise Conditional Random Field (CRF), resulting in an assessable end-to-end
    trainable pipeline.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在一次性场景领域，Cheng 等人（Cheng et al., [2020](#bib.bib18)）提出了一种图的创新应用。他们利用注意力机制将支持文档中的静态文本区域（关键/标志）和动态文本区域（值/字段）之间的空间关系无缝转移到查询文档中。这种转移使得获取文件的标签概率分布成为可能。此外，集成了基于自注意力的模块来利用字段实体之间的关系，从而促进标签转移概率分布的推导。最后，利用信念传播进行推断，在一对条件随机场（CRF）中，形成一个可评估的端到端可训练管道。
- en: 3.1.4\. Prompt-learning Frameworks
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. Prompt-learning 框架
- en: 'QueryForm (Wang et al., [2023a](#bib.bib131)) introduce a query-based framework
    for zero-shot document key information extraction. A dual prompting-based mechanism,
    entity query (E-prompt) and schema query (S-prompt), is introduced for pre-training
    and transferring knowledge from large-scale weakly annotated pre-trained webpages
    to target domains. The pretraining target is highly aligned with fine-tuning to
    ensure the proposed framework can consistently make query-conditional predictions
    at both stages. The HTML tags acquire the E-prompt ($\bm{e_{p}}$) during pretraining,
    and the S-prompt ($\bm{\tilde{s}_{p}}$) is generated from webpage domains, while
    during the fine-tuning, $\bm{e_{p}}$ is predefined and S-prompt $\bm{s_{p}}$ is
    learnable vectors. Supposing $\bm{t}$ is the serialised text of the input document,
    the pre-training and fine-tuning target $\bm{\hat{y}}$ and $\bm{y}$ can be represented
    as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: QueryForm（Wang et al., [2023a](#bib.bib131)）引入了一个基于查询的零样本文档关键信息提取框架。引入了一种双重提示机制，即实体查询（E-prompt）和模式查询（S-prompt），用于预训练和将从大规模弱标注预训练网页中获得的知识迁移到目标领域。预训练目标与微调高度对齐，以确保所提框架在两个阶段都能一致地进行查询条件预测。在预训练期间，HTML
    标签获取 E-prompt ($\bm{e_{p}}$)，S-prompt ($\bm{\tilde{s}_{p}}$) 从网页域中生成，而在微调期间，$\bm{e_{p}}$
    是预定义的，S-prompt $\bm{s_{p}}$ 是可学习的向量。假设 $\bm{t}$ 是输入文档的序列化文本，预训练和微调目标 $\bm{\hat{y}}$
    和 $\bm{y}$ 可以表示为：
- en: '| (2) |  | $\displaystyle\bm{\hat{y}}=\mathcal{F}([\bm{s_{p}};\mathcal{E}[\bm{e_{p}},\bm{x}]]),$
    |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle\bm{\hat{y}}=\mathcal{F}([\bm{s_{p}};\mathcal{E}[\bm{e_{p}},\bm{x}]]),$
    |  |'
- en: '| (3) |  | $\displaystyle\bm{y}=\mathcal{F}([\mathcal{E}[\bm{\tilde{s}_{p}};\bm{e_{p}},\bm{x}]])$
    |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle\bm{y}=\mathcal{F}([\mathcal{E}[\bm{\tilde{s}_{p}};\bm{e_{p}},\bm{x}]])$
    |  |'
- en: where $\mathcal{E}$ and $\mathcal{F}$ in here are the feature encoder and the
    rest of the language model, respectively. The training objective is to minimise
    the cross-entropy loss between $\bm{\hat{y}}$ and $\bm{y}$.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{E}$ 和 $\mathcal{F}$ 分别是特征编码器和语言模型的其余部分。训练目标是最小化 $\bm{\hat{y}}$
    和 $\bm{y}$ 之间的交叉熵损失。
- en: Prompt learning is a technique in natural language processing where models are
    guided by specific prompts to produce or interpret targeted responses. With the
    rise of large-scale models, prompt learning can leverage contextual representation
    and implicit knowledge to enhance performance on targeted tasks. Since most LLMs
    (OpenAI, [2023](#bib.bib90); Touvron et al., [2023](#bib.bib118)) and MLLMs (Liu
    et al., [2024a](#bib.bib72)) are trained on plain text or natural images, layout-aware
    prompting and in-context learning (Brown et al., [2020](#bib.bib10)) methods have
    been introduced to improve understanding in visually rich documents (VRD)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 提示学习是一种自然语言处理技术，在这种技术中，模型通过特定的提示来产生或解释目标响应。随着大规模模型的兴起，提示学习可以利用上下文表示和隐性知识来提高在特定任务上的表现。由于大多数LLM（OpenAI，[2023](#bib.bib90)；Touvron等，[2023](#bib.bib118)）和MLLM（Liu等，[2024a](#bib.bib72)）是基于纯文本或自然图像进行训练的，因此引入了布局感知提示和上下文学习（Brown等，[2020](#bib.bib10)）方法，以提高对视觉丰富文档（VRD）的理解。
- en: GenKIE (Cao et al., [2023b](#bib.bib12)) proposes an encoder-decoder-based multimodal
    KIE framework to leverage prompt to adapt various datasets and better leverage
    multimodal information. Following (Xu et al., [2020a](#bib.bib136), [b](#bib.bib138)),
    different encoding methods are adopted to acquire textual, layout and visual embeddings.
    Byte Pair Encoding is used as language pretrained backbones, and the OCR extracted
    document content is concatenated with predefined prompts split by the ”[SEP]”
    token between OCR tokens and each prompt. The 2D-positional encoding introduced
    by (Xu et al., [2020a](#bib.bib136)) is used to acquire the layout embedding of
    each OCR extracted token, and ResNet (He et al., [2016](#bib.bib40)) is used to
    extract the visual representations following (Wang et al., [2022c](#bib.bib125)).
    The multimodal representations are fed into an encoder to learn interactively
    between modalities. Prompts, inserted at the end of the encoder’s textual inputs,
    are either template-style or question-style. For the entity extraction task, the
    prompt specifies the target entity type, and the decoder generates the entity
    value (e.g., for ”Company is?”, the decoder outputs the company name). For the
    entity-labeling task, the prompt includes the value, and the decoder provides
    the entity type (e.g., for ”Es Kopi Rupa is [SEP]”, the decoder identifies the
    entity type).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: GenKIE（Cao等，[2023b](#bib.bib12)）提出了一种基于编码器-解码器的多模态KIE框架，以利用提示来适应各种数据集并更好地利用多模态信息。根据（Xu等，[2020a](#bib.bib136)，[b](#bib.bib138)），采用了不同的编码方法来获取文本、布局和视觉嵌入。使用字节对编码作为语言预训练骨干，OCR提取的文档内容与通过“[SEP]”标记分隔的预定义提示连接在一起。采用（Xu等，[2020a](#bib.bib136)）提出的二维位置编码来获取每个OCR提取的标记的布局嵌入，并使用ResNet（He等，[2016](#bib.bib40)）来提取视觉表示，方法参见（Wang等，[2022c](#bib.bib125)）。将多模态表示输入到编码器中，以便在模态之间进行交互学习。提示插入在编码器的文本输入的末尾，既可以是模板风格的，也可以是问题风格的。对于实体提取任务，提示指定目标实体类型，解码器生成实体值（例如，对于“公司是？”，解码器输出公司名称）。对于实体标注任务，提示包括值，解码器提供实体类型（例如，对于“Es
    Kopi Rupa 是 [SEP]”，解码器识别实体类型）。
- en: ICL-D3IE (He et al., [2023](#bib.bib38)) is the first framework to employ LLMs
    with in-context learning to extract key information from VRDs using the iterative
    updated diverse demonstrates. Before designing the initial diverse demonstrations,
    the most similar $n$ training documents to the $n$ test samples need to be selected
    by calculating the cosine-similarity of document representations encoded by Sentence-BERT
    (Reimers and Gurevych, [2019](#bib.bib102)). Then, different types of demonstrations
    are introduced to integrate multiple-view context information into LLMs. Hard
    Demonstrations highlight the most challenging cases, are initially designed based
    on the incorrectly predicted cases from GPT-3 (Brown et al., [2020](#bib.bib10))
    predictions and are updated based on prediction results during the training process.
    Layout-aware demonstrations are created by selecting the adjacent hard segments
    to understand the positional relations. Formatting demonstrations are designed
    to guide LLMs in formatting the outputs for easy post-processing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ICL-D3IE（He 等人，[2023](#bib.bib38)）是第一个采用 LLM 和上下文学习从 VRDs 中提取关键信息的框架，使用了迭代更新的多样化示例。在设计初始的多样化示例之前，需要通过计算由
    Sentence-BERT（Reimers 和 Gurevych，[2019](#bib.bib102)）编码的文档表示的余弦相似度来选择与 $n$ 个测试样本最相似的
    $n$ 个训练文档。然后，引入不同类型的示例，将多视角的上下文信息整合到 LLM 中。困难示例突出了最具挑战性的案例，最初基于 GPT-3（Brown 等人，[2020](#bib.bib10)）预测中的错误预测案例进行设计，并在训练过程中根据预测结果进行更新。布局感知示例通过选择相邻的困难段落来理解位置关系。格式化示例旨在指导
    LLM 格式化输出，以便于后续处理。
- en: LMDX (Perot et al., [2023](#bib.bib95)) designs a pipeline to use arbitrary
    LLMs to extract singular, repeated and hierarchical entities from VRDs. The document
    images are first fed into off-the-shelf OCR tools and are divided into smaller
    document chunks to be processed by the LLMs with accessible input length. Then,
    prompts are generated under XML-like tags to control the LLM’s responses and mitigate
    hallucination. Document Representation is a prompt contains the chunk content
    with the coordinates of OCR lines to bring layout modality to LLMs, where a text
    segment extracted by OCR tools is represented ¡text¿ [$x_{centre}$, $y_{centre}$].
    After that, the task description and scheme representation prompts are designed
    to explain the task to accomplish and determine the output format. During inference,
    $N$ prompts with $K$ LLMs completions are generated to sample the correct answers.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: LMDX（Perot 等人，[2023](#bib.bib95)）设计了一种管道，利用任意的 LLM 从 VRDs 中提取单一、重复和层次结构的实体。文档图像首先被输入到现成的
    OCR 工具中，然后被划分为较小的文档块，以便 LLM 处理，并满足输入长度的要求。接着，生成 XML 类似标签的提示，以控制 LLM 的响应并减轻幻觉现象。文档表示是一个包含块内容及
    OCR 线坐标的提示，将布局模态引入 LLM，其中由 OCR 工具提取的文本片段表示为 ¡text¿ [$x_{centre}$, $y_{centre}$]。随后，任务描述和方案表示提示被设计用来解释需要完成的任务，并确定输出格式。在推理过程中，生成
    $N$ 个提示和 $K$ 个 LLM 完成的样本以获取正确答案。
- en: 3.1.5\. Summary of Key Information Extraction
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5\. 关键信息提取总结
- en: Several models like Chargrid (Katti et al., [2018](#bib.bib51)) and ACP (Palm
    et al., [2019](#bib.bib92)) have enhanced VRDU by integrating visual and textual
    information. Additionally, auxiliary tasks such as OCR, utilized by (Zhang et al.,
    [2020](#bib.bib145); Wang et al., [2021a](#bib.bib124)), aid in improving multimodal
    feature representations through joint training. However, these frameworks often
    rely on smaller, randomly initialized models, which produce less representative
    features compared to those generated by large-scale pre-trained models like LayoutLM
    (Xu et al., [2020a](#bib.bib136)) and SelfDoc (Li et al., [2021b](#bib.bib66)).
    Documents typically exhibit specific layouts and logical structures, which has
    prompted many models (Yu et al., [2021](#bib.bib142); Lee et al., [2022b](#bib.bib57),
    [2023](#bib.bib58)) to adopt graph-based approaches. These methods capture spatial
    and logical correlations among document elements, such as key-value pairs, leading
    to a more comprehensive document representation. While these frameworks have achieved
    improvements in document representation, their effectiveness hinges on having
    sufficient well-annotated training samples, which are time-consuming to acquire.
    This limitation has escalated the demand for few-shot (Wang and Shang, [2022](#bib.bib128))
    and zero-shot (Wang et al., [2023a](#bib.bib131)) frameworks, which leverage contrastive
    learning and innovative attention mechanisms. Additionally, prompt learning has
    been applied to distill implicit knowledge from large-scale layout-aware pre-trained
    models (Hong et al., [2021](#bib.bib41); Tu et al., [2023](#bib.bib119)) and large
    language models (LLMs/MLLMs) (He et al., [2023](#bib.bib38); Perot et al., [2023](#bib.bib95)).
    Despite these advances, a performance gap remains between well-fine-tuned models
    and few/zero-shot frameworks, highlighting the ongoing challenges in VRDU optimization.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型如Chargrid (Katti et al., [2018](#bib.bib51))和ACP (Palm et al., [2019](#bib.bib92))通过整合视觉和文本信息增强了VRDU。此外，辅助任务如OCR（由Zhang
    et al., [2020](#bib.bib145); Wang et al., [2021a](#bib.bib124)）有助于通过联合训练提高多模态特征表示。然而，这些框架通常依赖较小的随机初始化模型，与由大规模预训练模型如LayoutLM
    (Xu et al., [2020a](#bib.bib136))和SelfDoc (Li et al., [2021b](#bib.bib66))生成的特征相比，生成的特征代表性较差。文档通常展现出特定的布局和逻辑结构，这促使许多模型（Yu
    et al., [2021](#bib.bib142); Lee et al., [2022b](#bib.bib57), [2023](#bib.bib58)）采用基于图的方法。这些方法捕捉了文档元素之间的空间和逻辑关联，如键值对，从而实现更全面的文档表示。虽然这些框架在文档表示上取得了改进，但其有效性依赖于足够的高质量标注训练样本，这些样本的获取耗时。这一限制加剧了对少样本（Wang
    and Shang, [2022](#bib.bib128)）和零样本（Wang et al., [2023a](#bib.bib131)）框架的需求，这些框架利用对比学习和创新的注意力机制。此外，提示学习已被应用于从大规模布局感知预训练模型（Hong
    et al., [2021](#bib.bib41); Tu et al., [2023](#bib.bib119)）和大型语言模型（LLMs/MLLMs）（He
    et al., [2023](#bib.bib38); Perot et al., [2023](#bib.bib95)）中提炼隐性知识。尽管有这些进展，但在精细调整模型和少/零样本框架之间仍存在性能差距，突显了VRDU优化中的持续挑战。
- en: 3.2\. Document Entity Linking
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2. 文档实体链接
- en: Documents are normally structured hierarchically, where parent-child relations
    always exist in various documents, such as key-value pairs in forms and section
    paragraphs in reports or papers. Unlike most VRD key information extraction models,
    which focus on recognising the semantic entity categories in a sequence tagging
    task ignoring the relation between entities, linking the logical associations
    between document semantic entity pairs has recently been of greater interest.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 文档通常是层次结构的，其中父子关系在各种文档中始终存在，例如表单中的键值对和报告或论文中的章节段落。与大多数VRD关键数据提取模型不同，这些模型专注于在序列标记任务中识别语义实体类别而忽略实体之间的关系，最近的研究更关注于连接文档语义实体对之间的逻辑关联。
- en: 3.2.1\. Entity-Level Entity Linking
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1. 实体级实体链接
- en: Document entity linking aims to identify the relation between document entities.
    Some frameworks use the known entity bounding boxes, ignoring the entity recognition
    step and mainly focusing on exploring the relation between input document entities.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 文档实体链接的目标是识别文档实体之间的关系。一些框架利用已知的实体边界框，忽略实体识别步骤，主要集中在探索输入文档实体之间的关系。
- en: 'DocStruct (Wang et al., [2020b](#bib.bib130)) is the first entity linking framework
    by leveraging multimodal features of known document entities to predict the hierarchical
    structure between them. [CLS] token from a BERT-like pretrained language model
    is extracted for entity-level textual representation $T_{e}$, and an RNN is applied
    to encode the sequential RoI visual cues $V_{e}$ along the width of the ResNet-50
    feature maps. Then, $T_{e}$ is concatenated with layout feature $P_{e}$, which
    is a linear projected vector of entity bounding box coordinates, $[x1,y1,x2,y2,x3,y3,x4,y4]$.
    The final entity representation $E$ can be represented:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DocStruct（Wang et al., [2020b](#bib.bib130)）是第一个利用已知文档实体的多模态特征来预测它们之间层次结构的实体链接框架。从
    BERT 类的预训练语言模型中提取 [CLS] token 以获得实体级别的文本表示 $T_{e}$，并使用 RNN 对沿 ResNet-50 特征图宽度的序列
    RoI 视觉提示 $V_{e}$ 进行编码。然后，将 $T_{e}$ 与布局特征 $P_{e}$ 连接，$P_{e}$ 是实体边界框坐标的线性投影向量，$[x1,y1,x2,y2,x3,y3,x4,y4]$。最终的实体表示
    $E$ 可以表示为：
- en: '| (4) |  | $\displaystyle\alpha=Sigmoid(W[T_{e},P_{e},V_{e}]+b),$ |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle\alpha=Sigmoid(W[T_{e},P_{e},V_{e}]+b),$ |  |'
- en: '| (5) |  | $\displaystyle E=[T_{e};P_{e}]+\alpha V_{e},$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\displaystyle E=[T_{e};P_{e}]+\alpha V_{e},$ |  |'
- en: where $\alpha$ is used as a gate to control the influence of visual cues $V_{e}$
    following (Wang et al., [2019](#bib.bib126)). Then, for pair of entities $E_{i}$
    and $E_{j}$, the probability of an existing parent-child relationship is represented
    as $P_{i\rightarrow j}=E_{i}ME_{j}$ where M is the matrix of asymmetric parameters
    to ensure the asymmetric relation between $E_{i}$ and $E_{j}$. Negative sampling
    (Mikolov et al., [2013b](#bib.bib86)) is applied during training to handle data
    sparsity and balance problems.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 用作门控来控制视觉提示 $V_{e}$ 的影响，参考（Wang et al., [2019](#bib.bib126)）。然后，对于实体对
    $E_{i}$ 和 $E_{j}$，存在父子关系的概率表示为 $P_{i\rightarrow j}=E_{i}ME_{j}$，其中 M 是非对称参数矩阵，以确保
    $E_{i}$ 和 $E_{j}$ 之间的非对称关系。在训练过程中应用负采样（Mikolov et al., [2013b](#bib.bib86)）以处理数据稀疏性和不平衡问题。
- en: 'Zhang et al. (Zhang et al., [2021](#bib.bib146)) propose an entity-linking
    framework, SERA (Semantic Entity Relation extraction As dependency parsing) by
    formulating the entity relation prediction as a dependency parsing problem. LayoutLM
    (Xu et al., [2020a](#bib.bib136)) is used to obtain textual representations at
    the entity level $T_{e}$ that are concatenated with the embedding of the linear
    projected entity label $L_{e}$ to obtain the final entity representation $E$,
    which could be formulated as $E=[T_{e},L_{e}]$. Various encoders, such as Vanilla
    Transformer, BiLSTM, and Graph, are adopted to learn the following contextual
    representations. Then, a Biaffine parser is adopted to calculate the score between
    $E_{i}$ and $E_{j}$. The biaffine parser score $p_{b}$ can be calculated by:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人（Zhang et al., [2021](#bib.bib146)）提出了一种实体链接框架 SERA（Semantic Entity Relation
    extraction As dependency parsing），通过将实体关系预测公式化为依赖解析问题。LayoutLM（Xu et al., [2020a](#bib.bib136)）用于获取在实体级别的文本表示
    $T_{e}$，这些表示与线性投影实体标签 $L_{e}$ 的嵌入连接，以获得最终的实体表示 $E$，公式化为 $E=[T_{e},L_{e}]$。采用各种编码器，如
    Vanilla Transformer、BiLSTM 和 Graph，来学习以下上下文表示。然后，采用 Biaffine 解析器来计算 $E_{i}$ 和
    $E_{j}$ 之间的分数。biaffine 解析器分数 $p_{b}$ 可以通过以下公式计算：
- en: '| (6) |  | $\displaystyle h_{i}^{key}=\sigma(W^{key}E_{i}+b^{key}),$ |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle h_{i}^{key}=\sigma(W^{key}E_{i}+b^{key}),$ |  |'
- en: '| (7) |  | $\displaystyle h_{j}^{value}=\sigma(W^{value}E_{j}+b^{value}),$
    |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\displaystyle h_{j}^{value}=\sigma(W^{value}E_{j}+b^{value}),$
    |  |'
- en: '| (8) |  | $\displaystyle p_{b}=h_{i}^{key}W_{b1}h_{j}^{value}+h_{i}^{key}W_{b2}.$
    |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\displaystyle p_{b}=h_{i}^{key}W_{b1}h_{j}^{value}+h_{i}^{key}W_{b2}.$
    |  |'
- en: where $h_{i}^{key}$ and $h_{j}^{value}$ are the hidden states after linear projection.
    To leverage the layout information, a two-dim layout vector $l_{ij}$ is used to
    get the layout feature score $p_{l}=W_{l}l_{ij}+b_{l}$, where $l_{ij}$ is the
    minimum distance between two entities along with width and height direction respectively.
    Then, the final score $p=p_{b}+p_{l}$ is used to calculate loss using binary or
    multi-label classification.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{i}^{key}$ 和 $h_{j}^{value}$ 是线性投影后的隐藏状态。为了利用布局信息，使用二维布局向量 $l_{ij}$ 来获得布局特征分数
    $p_{l}=W_{l}l_{ij}+b_{l}$，其中 $l_{ij}$ 是两个实体在宽度和高度方向上的最小距离。然后，最终的分数 $p=p_{b}+p_{l}$
    用于通过二分类或多标签分类计算损失。
- en: KVPFormer (Hu et al., [2023](#bib.bib42)) formulate the entity linking to a
    question-answer problem by introducing a transformer-based encoder-decoder structure
    to leverage joint-grained information (token and entity) to predict the entity
    association. The textual representation $T$ of each document entity, $E$, is encoded
    by averaging the inner-token embedding from pretrained document understanding
    models, concatenating with the linear projected entity label embedding $l$, represented
    as $E=[T;l]$. Then, the entity representations will be fed into a Transformer
    encoder, which has a spatial compatibility attention bias into vanilla self-attention
    mechanism ²²2Please refer to . A binary classifier determines which entity is
    a key entity or not. All detected key entities will be treated as question representation
    and fed into a DETR-style (Carion et al., [2020](#bib.bib14)) decoder to predict
    corresponding answers in parallel. For each input question (key query), Top K
    answer candidates will be selected based on the $Sigmoid$ score and fed into a
    softmax layer to get the final prediction, named a coarse-to-fine answer prediction.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: KVPFormer（Hu et al., [2023](#bib.bib42)）将实体链接公式化为一个问答问题，通过引入基于 Transformer 的编码器-解码器结构来利用联合细粒度信息（token
    和实体）以预测实体关联。每个文档实体 $E$ 的文本表示 $T$ 通过从预训练文档理解模型中平均内部 token 嵌入来编码，并与线性投影的实体标签嵌入 $l$
    连接，表示为 $E=[T;l]$。然后，实体表示将被送入一个 Transformer 编码器，该编码器具有空间兼容的注意力偏置到普通自注意力机制中²²2Please
    refer to。一个二分类器确定哪个实体是关键实体。所有检测到的关键实体将被视为问题表示，并送入一个 DETR 风格（Carion et al., [2020](#bib.bib14)）的解码器以并行预测相应的答案。对于每个输入问题（关键查询），将基于
    $Sigmoid$ 分数选择前 K 个答案候选，并送入一个 softmax 层以获得最终预测，称为粗到细的答案预测。
- en: 3.2.2\. Token-level Entity Linking
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2. Token-level 实体链接
- en: As acquiring entity information needs prior knowledge from manual annotation
    or layout analysing models, some works utilise serialised OCR-extracted tokens
    as inputs to extract the structured relations. However, as the logical relation
    links semantic entities, token-level frameworks must group tokens into entities
    before exploring their association.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于获取实体信息需要来自人工标注或布局分析模型的先验知识，一些研究利用序列化的 OCR 提取的 tokens 作为输入来提取结构化关系。然而，由于逻辑关系链接语义实体，token-level
    框架必须在探索其关联之前将 tokens 分组为实体。
- en: Carbonell et al. (Carbonell et al., [2021](#bib.bib13)) introduces a framework
    comprising three modules for sequentially conducting token grouping, entity labelling
    and relation prediction. Firstly, each text token $t$ is represented by $[L_{t};T_{t}]$
    where $L_{t}=[x,y,w,h]$ is the coco format bounding box coordinates of $t$ and
    $T_{t}$ is the work/representations. All tokens are fed into a token grouping
    GNN, $\mathcal{G}_{group}$, as a node where the edge between nodes is determined
    by k-NN to avoid high consumption of fully connected GNN. The $\mathcal{G}_{group}$
    is trained on a link prediction task to predict the edge score between two nodes
    to group words, of which scores larger than a predefined threshold $\rho$. Then,
    the grouped words are fed into a Graph Attention Network (GAT) to use multi-head
    attention to aggregate words into entities and follow an MLP to conduct node classification
    to predict the category of each document entity. At last, another link prediction
    GNN, $\mathcal{G}_{link}$, is trained on edge classification based on aggregated
    entities.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Carbonell 等人（Carbonell et al., [2021](#bib.bib13)）介绍了一个由三个模块组成的框架，用于依次进行 token
    分组、实体标注和关系预测。首先，每个文本 token $t$ 用 $[L_{t};T_{t}]$ 表示，其中 $L_{t}=[x,y,w,h]$ 是 $t$
    的 coco 格式边界框坐标，$T_{t}$ 是工作/表示。所有 tokens 被送入一个 token 分组 GNN，$\mathcal{G}_{group}$，作为一个节点，其中节点之间的边由
    k-NN 确定，以避免完全连接 GNN 的高耗能。$\mathcal{G}_{group}$ 在一个链接预测任务上进行训练，以预测两个节点之间的边分数来分组单词，分数大于预定义的阈值
    $\rho$。然后，分组后的单词被送入一个图注意力网络（GAT），利用多头注意力将单词聚合为实体，并跟随一个 MLP 进行节点分类，以预测每个文档实体的类别。最后，另一个链接预测
    GNN，$\mathcal{G}_{link}$，在基于聚合实体的边分类上进行训练。
- en: 'SPADE (Hwang et al., [2021](#bib.bib45)) formulate the token-level entity linking
    as a spatial dependency parsing task for serializing (ordering and grouping) tokens
    and predicting inter-group relation between grouped tokens. Firstly, a spatial
    text encoder is designed to make spatial-aware attention by introducing a relative
    spatial vector considering relative, physical and angle aspects. During this task,
    two binary matrices must be predicted $M_{g}$ for token grouping and inter-group
    linking $M_{l}$. The vertices comprised by a entity types $\mathbb{V}$ and sequence
    of tokens $\mathbb{T}$ the encoded entity category and token are represented as
    $c$ and $t$, respectively, and the relation score between vertices $v_{i}\rightarrow
    v_{j}$ can be calculated by:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: SPADE（Hwang 等，[2021](#bib.bib45)）将令牌级实体链接公式化为空间依赖解析任务，用于对令牌进行序列化（排序和分组）并预测分组令牌之间的组间关系。首先，设计了一个空间文本编码器，通过引入相对空间向量来实现空间感知注意力，考虑相对、物理和角度方面。在此任务中，必须预测两个二进制矩阵$M_{g}$（用于令牌分组）和$M_{l}$（用于组间链接）。由实体类型$\mathbb{V}$和令牌序列$\mathbb{T}$组成的顶点中，编码的实体类别和令牌分别表示为$c$和$t$，顶点$v_{i}\rightarrow
    v_{j}$之间的关系得分可以通过以下公式计算：
- en: '| (9) |  | $\displaystyle h_{i}=\begin{cases}c_{v_{i}},&amp;\text{for }v_{i}\in\mathbb{V},\\
    W_{h}t_{v_{i}},&amp;\text{otherwise}\end{cases},$ |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\displaystyle h_{i}=\begin{cases}c_{v_{i}},&\text{对于 }v_{i}\in\mathbb{V},\\
    W_{h}t_{v_{i}},&\text{否则}\end{cases},$ |  |'
- en: '| (10) |  | $\displaystyle d=W_{d}v_{j},$ |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\displaystyle d=W_{d}v_{j},$ |  |'
- en: '| (11) |  | $\displaystyle s_{0}=h_{i}^{T}W_{0}d,\ s_{1}=h_{i}^{T}W_{1}d.$
    |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\displaystyle s_{0}=h_{i}^{T}W_{0}d,\ s_{1}=h_{i}^{T}W_{1}d.$
    |  |'
- en: The probability is acquired by $p_{ij}=\frac{\exp(s_{0,j})}{\exp(s_{0,j})+\exp(s_{1,j})}$.
    An adjustable threshold is set to construct the $M_{g}$ or $M_{l}$.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 概率通过$p_{ij}=\frac{\exp(s_{0,j})}{\exp(s_{0,j})+\exp(s_{1,j})}$获得。设置一个可调阈值来构建$M_{g}$或$M_{l}$。
- en: DocTR (Liao et al., [2023](#bib.bib69)) formulate the entity linking as an anchor
    word-based entity detection and association problem. Each document entity is represented
    by the anchor word to convert the entity extraction and linking to token-level
    tasks. It contains a Deformable DETR-based vision encoder to extract multi-scale
    visual feature extraction. A LayoutLM based language encoder is applied to encode
    token-level textual representations. The outputs from vision/language encoders
    are fed into the vision-language decoder with the language-conditional queries
    to conduct entity extraction and linking. The decoder queries are one-to-one mapping
    with language encoder inputs. The entity extraction task aims to predict whether
    the query underlying token level input is an anchor word and corresponding categories,
    while entity linking is acquired by
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: DocTR（Liao 等，[2023](#bib.bib69)）将实体链接公式化为基于锚词的实体检测和关联问题。每个文档实体由锚词表示，将实体提取和链接转换为令牌级任务。它包含一个基于
    Deformable DETR 的视觉编码器，用于提取多尺度视觉特征。应用基于 LayoutLM 的语言编码器来编码令牌级的文本表示。来自视觉/语言编码器的输出被输入到视觉语言解码器中，结合语言条件查询进行实体提取和链接。解码器查询与语言编码器输入一一对应。实体提取任务旨在预测查询基础令牌级输入是否为锚词及其对应类别，而实体链接则通过以下方式获得：
- en: 3.2.3\. Summary
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 总结
- en: Several models like Chargrid (Katti et al., [2018](#bib.bib51)) and ACP (Palm
    et al., [2019](#bib.bib92)) have enhanced VRDU by integrating visual and textual
    information. Additionally, auxiliary tasks such as OCR, utilised by (Zhang et al.,
    [2020](#bib.bib145); Wang et al., [2021a](#bib.bib124)), aid in improving multimodal
    feature representations through joint training. However, these frameworks often
    rely on smaller, randomly initialised models, which produce less representative
    features compared to those generated by large-scale pretrained models like LayoutLM
    (Xu et al., [2020a](#bib.bib136)) and SelfDoc (Li et al., [2021b](#bib.bib66)).
    Documents typically exhibit specific layouts and logical structures, which has
    prompted many models (Yu et al., [2021](#bib.bib142); Lee et al., [2022b](#bib.bib57),
    [2023](#bib.bib58)) to adopt graph-based approaches. These methods capture spatial
    and logical correlations among document elements, such as key-value pairs, leading
    to a more comprehensive document representation. While these frameworks have achieved
    improvements in document representation, their effectiveness hinges on having
    sufficient well-annotated training samples, which are time-consuming to acquire.
    This limitation has escalated the demand for few-shot (Wang and Shang, [2022](#bib.bib128))
    and zero-shot (Wang et al., [2023a](#bib.bib131)) frameworks, which leverage contrastive
    learning and innovative attention mechanisms. Furthermore, prompt learning has
    been applied to distil implicit knowledge from large-scale layout-aware pretrained
    models (Hong et al., [2021](#bib.bib41); Tu et al., [2023](#bib.bib119)) and large
    language models (LLMs/MLLMs) (He et al., [2023](#bib.bib38); Perot et al., [2023](#bib.bib95)).
    Despite these advances, a performance gap remains between well-fine-tuned models
    and few/zero-shot frameworks, highlighting the ongoing challenges in VRDU optimization.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如 Chargrid（Katti et al., [2018](#bib.bib51)）和 ACP（Palm et al., [2019](#bib.bib92)）等多个模型通过整合视觉和文本信息增强了
    VRDU。此外，辅助任务如 OCR，由（Zhang et al., [2020](#bib.bib145); Wang et al., [2021a](#bib.bib124)）利用，有助于通过联合训练改进多模态特征表示。然而，这些框架通常依赖较小、随机初始化的模型，这些模型产生的特征相较于大规模预训练模型如
    LayoutLM（Xu et al., [2020a](#bib.bib136)）和 SelfDoc（Li et al., [2021b](#bib.bib66)）生成的特征不够具有代表性。文档通常展示特定的布局和逻辑结构，这促使许多模型（Yu
    et al., [2021](#bib.bib142); Lee et al., [2022b](#bib.bib57), [2023](#bib.bib58)）采用基于图的方法。这些方法捕捉文档元素（如键值对）之间的空间和逻辑关联，从而获得更全面的文档表示。尽管这些框架在文档表示方面取得了进展，但它们的有效性依赖于充足的标注训练样本，这些样本获取起来费时费力。这一限制加剧了对少样本（Wang
    and Shang, [2022](#bib.bib128)）和零样本（Wang et al., [2023a](#bib.bib131)）框架的需求，这些框架利用对比学习和创新的注意机制。此外，提示学习已被应用于从大规模布局感知预训练模型（Hong
    et al., [2021](#bib.bib41); Tu et al., [2023](#bib.bib119)）和大语言模型（LLMs/MLLMs）（He
    et al., [2023](#bib.bib38); Perot et al., [2023](#bib.bib95)）中提炼隐性知识。尽管取得了这些进展，但精细调优模型与少样本/零样本框架之间仍存在性能差距，突显了
    VRDU 优化中持续存在的挑战。
- en: 3.3\. VRD Question Answering
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. VRD 问答
- en: Unlike key information extraction, which targets specific details within document
    images, answering natural language questions involves interpreting more complex
    intentions and requires models to facilitate interactive understanding between
    the queries and document representations (Ding et al., [2022](#bib.bib24)). The
    introduction of DocVQA (Mathew et al., [2021](#bib.bib84)) marked a significant
    shift in focus from natural scene images to text-dense, layout-aware single-page
    document images, establishing a benchmark in the field. As advancements have continued,
    demands have recently emerged for models capable of addressing more complex, multi-page
    scenarios (Tito et al., [2023](#bib.bib117); Ding et al., [2023b](#bib.bib26),
    [2024a](#bib.bib27)). These emerging requirements highlight the need for models
    to process multimodal inputs and navigate through extensive documents, reflecting
    user inquiries’ evolving complexity and naturalness in document-based question-answering
    systems. This section will briefly review the SoTAs in single-page document VQA
    models and introduce some recently proposed multi-page document understanding
    solutions (Tito et al., [2023](#bib.bib117); Blau et al., [2024](#bib.bib9)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与针对文档图像中具体细节的关键信息提取不同，回答自然语言问题涉及解释更复杂的意图，并要求模型在查询和文档表示之间促进互动理解（Ding 等，[2022](#bib.bib24)）。DocVQA
    的引入（Mathew 等，[2021](#bib.bib84)）标志着重点从自然场景图像转向文本密集、布局感知的单页文档图像，这在该领域建立了一个基准。随着技术的进步，最近对能够处理更复杂的多页场景的模型的需求开始出现（Tito
    等，[2023](#bib.bib117)；Ding 等，[2023b](#bib.bib26)，[2024a](#bib.bib27)）。这些新兴需求突显了模型处理多模态输入和在广泛文档中导航的必要性，反映了基于文档的问题回答系统中用户询问的复杂性和自然性的演变。本节将简要回顾单页文档
    VQA 模型中的 SoTA，并介绍一些最近提出的多页文档理解解决方案（Tito 等，[2023](#bib.bib117)；Blau 等，[2024](#bib.bib9)）。
- en: 3.3.1\. Single-page VRD-QA
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 单页 VRD-QA
- en: 'Similar to key information extraction from visually rich documents, single-page
    question answering (QA) often utilises classical pretrained language models (Devlin,
    [2018](#bib.bib23); Liu et al., [2019b](#bib.bib75)) as baselines. These models
    conduct span-based question answering to extract sequences of text tokens. Additionally,
    general domain visual language pretrained models (Li et al., [2019](#bib.bib65);
    Tan and Bansal, [2019](#bib.bib114); Kim et al., [2021](#bib.bib53)) are employed
    to identify the target document’s semantic entities (Ding et al., [2023b](#bib.bib26)).
    Beyond these plain text or general domain vision language, numerous layout-aware
    models specifically pretrained in the document domain (see Section [3](#S4.F3
    "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey")) have achieved state-of-the-art
    (SoTA) performance in single-page document tasks. This approach underscores the
    importance of integrating layout awareness in the preprocessing stages to enhance
    performance on these specific document-based tasks.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于从视觉丰富的文档中提取关键信息，单页问答（QA）通常利用经典的预训练语言模型（Devlin，[2018](#bib.bib23)；Liu 等，[2019b](#bib.bib75)）作为基线。这些模型进行基于跨度的问答以提取文本令牌序列。此外，通用领域的视觉语言预训练模型（Li
    等，[2019](#bib.bib65)；Tan 和 Bansal，[2019](#bib.bib114)；Kim 等，[2021](#bib.bib53)）被用于识别目标文档的语义实体（Ding
    等，[2023b](#bib.bib26)）。除了这些纯文本或通用领域视觉语言模型外，许多在文档领域专门预训练的布局感知模型（见第 [3](#S4.F3 "Figure
    3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep Learning based Visually Rich
    Document Content Understanding: A Survey") 节）在单页文档任务中达到了最先进（SoTA）的性能。这种方法强调了在预处理阶段整合布局感知的重要性，以提升在这些特定文档任务中的表现。'
- en: 3.3.2\. Multi-page VRD-QA
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 多页 VRD-QA
- en: As demand increases for retrieving answers from multi-page documents (Tito et al.,
    [2023](#bib.bib117); Ding et al., [2024a](#bib.bib27)), most SoTA models (Xu et al.,
    [2020a](#bib.bib136); Wang et al., [2022b](#bib.bib123); Hong et al., [2021](#bib.bib41)),
    which are typically designed for single-page inputs, face a significant challenge
    due to their input length limitation of 512 tokens. To overcome this limitation,
    recent innovations have introduced solutions such as transformers capable of handling
    longer sequences and page-locating modules. These advancements are specifically
    designed to address the requirements of multipage document question answering
    (QA), enabling more efficient processing.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对从多页文档中检索答案的需求增加（Tito 等，[2023](#bib.bib117)；Ding 等，[2024a](#bib.bib27)），大多数当前最先进（SoTA）模型（Xu
    等，[2020a](#bib.bib136)；Wang 等，[2022b](#bib.bib123)；Hong 等，[2021](#bib.bib41)）通常设计用于单页输入，由于其输入长度限制为
    512 个标记，面临显著挑战。为克服这一限制，近期的创新提出了如处理更长序列的变压器和页面定位模块等解决方案。这些进展专门针对多页文档问答（QA）的需求，提升了处理效率。
- en: Hi-VT5 (Tito et al., [2023](#bib.bib117)) proposes a multimodal hierarchical
    encoder-decoder architecture for multi-page generative question-answering. A T5-based
    multimodal transformer is applied to encode the single-page level information,
    including questions, OCR-extracted page content, image patches, and a set of page
    tokens. The question and OCR extracted token sequence is the following (Biten
    et al., [2022](#bib.bib6)) to acquire the layout-aware initial textual representations.
    Document Image Trasnfoer (DIT) (Li et al., [2022b](#bib.bib64)) is leveraged to
    acquire initial patch representations. Then, the concatenated question ($Q$) and
    page OCR tokens ($T$), image patch tokens ($I$), and randomly initialised page
    tokens ($P$) are fed into T5-based page encoder to enhance the multi-level and
    multimodal contextual learning contextually. The enhanced page token embeddings
    $P^{\prime}$ of each input document image are fed into a T5-based decoder to generate
    the predicted answer, as well auto-regressively, and the page located on the target
    answer must output the page number based on $P^{\prime}$. As T5 is not a layout-aware
    language model, masked language modelling is applied to predicted masked tokens
    by leveraging non-based visual and layout information to boost multimodal understanding.
    As T5 can accept input sequence lengths up to 20,480 tokens, it dramatically increases
    the standard VRDU pretrained models that apply Hi-VT5 to multi-page scenarios.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Hi-VT5（Tito 等，[2023](#bib.bib117)）提出了一种多模态层次编码器-解码器架构，用于多页生成问答。采用基于 T5 的多模态变压器对单页级别信息进行编码，包括问题、OCR
    提取的页面内容、图像块和一组页面标记。问题和 OCR 提取的标记序列如下（Biten 等，[2022](#bib.bib6)），以获取布局感知的初始文本表示。文档图像变换器（DIT）（Li
    等，[2022b](#bib.bib64)）用于获取初始图块表示。然后，将连接的问答 ($Q$) 和页面 OCR 标记 ($T$)、图像块标记 ($I$)
    以及随机初始化的页面标记 ($P$) 输入到基于 T5 的页面编码器中，以增强多层次和多模态上下文学习。每个输入文档图像的增强页面标记嵌入 $P^{\prime}$
    被送入基于 T5 的解码器中以生成预测答案，同时自动回归，目标答案所在的页面必须基于 $P^{\prime}$ 输出页面编号。由于 T5 不是一个布局感知的语言模型，应用了掩码语言建模，通过利用非基于视觉和布局信息来提升多模态理解。由于
    T5 能够接受最长 20,480 个标记的输入序列，这大大提高了标准 VRDU 预训练模型在多页场景下应用 Hi-VT5 的能力。
- en: 'GRAM (Blau et al., [2024](#bib.bib9)): proposes a framework to extend single-page
    models to tackle multi-page document VQA scenarios. Following the way of pretrained
    DocFormerv2 (Appalaraju et al., [2024](#bib.bib3)), the framework encodes single-page
    inputs, including questions, OCR-extracted content, and visual features. For multi-page
    scenarios, a slim global encoder follows each single-page encoder layer, allowing
    the new learnable page tokens to interact with other pages contextually. This
    enables page tokens to capture page-level information through self-attention and
    enhance document-level understanding with sparse attention. As the global layer
    is a new stream applied to the pretrained model, an attention bias is applied
    following ALiBi (Press et al., [2021](#bib.bib97)) to prevent the model from possibly
    disregarding the page tokens, enabling them to capture more fine-grained information
    from pretrained weights. During the decoding stage, unlike Hi-VT5, which only
    feeds page tokens to the decoder, GRAM uses all fine-trained information for the
    decoder. C-Former (Raffel et al., [2020](#bib.bib98)) is applied to alleviate
    the high computation consumption, which could revise the information across all
    pages and distil only important details.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: GRAM（Blau et al., [2024](#bib.bib9)）提出了一个框架，用于扩展单页模型以处理多页面文档 VQA 场景。遵循预训练 DocFormerv2（Appalaraju
    et al., [2024](#bib.bib3)）的方法，该框架对单页输入进行编码，包括问题、OCR 提取的内容和视觉特征。对于多页面场景，一个精简的全局编码器跟随每个单页编码器层，允许新的可学习页面标记与其他页面在上下文中交互。这使得页面标记能够通过自注意力捕捉页面级信息，并通过稀疏注意力增强文档级理解。由于全局层是应用于预训练模型的新流，因此应用了类似
    ALiBi（Press et al., [2021](#bib.bib97)）的方法来防止模型可能忽略页面标记，使其能够从预训练权重中捕捉到更细粒度的信息。在解码阶段，与仅将页面标记输入解码器的
    Hi-VT5 不同，GRAM 使用所有微调的信息供解码器使用。C-Former（Raffel et al., [2020](#bib.bib98)）被应用于减轻高计算消耗，它可以跨所有页面修订信息并提炼出重要细节。
- en: Kang et al. (Kang et al., [2024](#bib.bib50)) introduce a multi-page document
    VQA framework to use a scoring self-attention mechanism to select and identify
    the related pages for generating the answer to the input question. The training
    processes include single-page Document VQA training and then training a self-attention
    scoring module based on the frozen trained encoder to feed the most relevant page
    information into the decoder for answer generating. Pix2Struct (Lee et al., [2022a](#bib.bib59))
    is used as the single-page model fine-tuned on the DocVQA (Mathew et al., [2021](#bib.bib84))
    dataset. The output from the Pix2Struct is fed into the self-attention scoring
    module to extract the first token for predicting a question-page matching score.
    The page with the highest matching score will be fed into the decoder for answer
    generation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Kang 等人（Kang et al., [2024](#bib.bib50)）提出了一种多页面文档 VQA 框架，使用评分自注意力机制来选择和识别与生成输入问题答案相关的页面。训练过程包括单页文档
    VQA 训练，然后训练一个基于冻结训练编码器的自注意力评分模块，将最相关的页面信息传递到解码器中以生成答案。Pix2Struct（Lee et al., [2022a](#bib.bib59)）作为单页模型，在
    DocVQA（Mathew et al., [2021](#bib.bib84)）数据集上进行了微调。Pix2Struct 的输出被输入到自注意力评分模块中以提取第一个标记，用于预测问题-页面匹配评分。具有最高匹配评分的页面将被输入到解码器中以生成答案。
- en: 3.3.3\. Summary
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. 总结
- en: Document Visual Question Answering (Document VQA) is a relatively new field,
    pioneered by DocVQA (Mathew et al., [2021](#bib.bib84)), which aims to generate
    or extract answers to natural language questions based on document images. This
    differs from key information extraction, which focuses on recognizing or extracting
    predefined key-value pairs. Document VQA requires a more comprehensive representation
    of the document and an understanding of correlations between the document and
    questions. Pretrained VRDU models (Xu et al., [2020b](#bib.bib138); Huang et al.,
    [2022](#bib.bib43)) demonstrate robust performance in single-page document understanding
    tasks. However, these models encounter challenges when applied to more typical
    and natural multi-page scenarios due to input length limitations. Recent solutions,
    such as (Tito et al., [2023](#bib.bib117); Blau et al., [2024](#bib.bib9)), mainly
    address these challenges by identifying the page where a possible answer may be
    located and then applying SoTA single-page techniques to retrieve the answer.
    Despite these advancements, real-world applications often present more complex
    situations, such as long-term dependencies and cross-page entity relationships,
    which still need further exploration in the document VQA field.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 文档视觉问答（Document VQA）是一个相对较新的领域，由 DocVQA（Mathew 等人，[2021](#bib.bib84)）率先开创，旨在根据文档图像生成或提取自然语言问题的答案。这与关键情报提取不同，后者侧重于识别或提取预定义的键值对。文档
    VQA 需要对文档进行更全面的表征，并理解文档与问题之间的关联。预训练的 VRDU 模型（Xu 等人，[2020b](#bib.bib138)；Huang
    等人，[2022](#bib.bib43)）在单页文档理解任务中表现出强大的性能。然而，由于输入长度限制，这些模型在应用于更典型和自然的多页场景时遇到挑战。最近的解决方案，如
    (Tito 等人，[2023](#bib.bib117)；Blau 等人，[2024](#bib.bib9))，主要通过识别可能包含答案的页面来解决这些挑战，然后应用
    SoTA 单页技术来检索答案。尽管取得了这些进展，但现实世界的应用通常呈现出更复杂的情况，例如长期依赖和跨页实体关系，这仍然需要在文档 VQA 领域进一步探索。
- en: 4\. Multi-Task VRD Understanding Models
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 多任务 VRD 理解模型
- en: '{forest}'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: for tree= forked edges, grow’=0, draw, rounded corners, node options=align=center,,
    text width=2.7cm, s sep=6pt, calign=child edge, calign child=(n_children()+1)/2
    [Multi-task based models, for tree=fill=brown!45 [Fine-grained Pretrained Models
    (Encoder-only), for tree=fill=violet!40 [Layout-aware Pretrained Models, for tree=fill=violet!20
    [LayoutLM (Xu et al., [2020a](#bib.bib136)); BROS(Hong et al., [2021](#bib.bib41));
    LiLT (Wang et al., [2022b](#bib.bib123)); XDoc (Chen et al., [2022](#bib.bib16));
    LayoutMask (Tu et al., [2023](#bib.bib119)); StructuralLM (Xu et al., [2020a](#bib.bib136)),
    for tree=fill=violet!10] ] [Visual-integrated Pretrained Models, for tree=fill=violet!20
    [LayoutLMv2(Xu et al., [2020b](#bib.bib138)); LayoutXLM(Xu et al., [2021](#bib.bib137));
    DocFormer(Appalaraju et al., [2021](#bib.bib2)); LayoutLMv3(Huang et al., [2022](#bib.bib43)),
    for tree=fill=violet!10] ] ] [Coarse and Joint-grained Pretrained Models (Encoder-only),
    for tree=fill=pink!40 [Coarse-grained Models, for tree=fill=pink!20 [SelfDoc(Li
    et al., [2021b](#bib.bib66)); UniDoc(Gu et al., [2021](#bib.bib33))], for tree=fill=pink!10
    ] [Joint-grained Models, for tree=fill=pink!20 [StrucText(Li et al., [2021c](#bib.bib68));
    Fast-StrucText(Zhai et al., [2023](#bib.bib144)); MGDoc (Wang et al., [2022a](#bib.bib127));
    WUKONG-READER (Bai et al., [2022](#bib.bib4)); GeoLayoutLM (Luo et al., [2023](#bib.bib78)),
    for tree=fill=pink!10] ] ] [Encoder-Decoder Pretrained Frameworks, for tree=fill=red!40
    [OCR-dependent Encoder-Decoder Frameworks, for tree=fill=red!20 [TiLT(Li et al.,
    [2021b](#bib.bib66)); UDOP(Tang et al., [2022](#bib.bib116)); DocFormerv2(Appalaraju
    et al., [2024](#bib.bib3)); ViTLP(Mao et al., [2024](#bib.bib83)), for tree=fill=red!10]
    ] [OCR-independent Frameworks, for tree=fill=red!20 [Donut(Kim et al., [2022](#bib.bib52));
    Dessurt(Davis et al., [2022](#bib.bib21)); ReRum (Cao et al., [2023a](#bib.bib11));
    StructTextV2 (Yu et al., [2022](#bib.bib143)), for tree=fill=red!10] ] [LLM-based
    Frameworks, for tree=fill=red!20 [HRVDA(Liu et al., [2024b](#bib.bib71)); LayoutLLM(Luo
    et al., [2024](#bib.bib79)), for tree=fill=red!10] ] ] [Non-Pretrained Frameworks,
    for tree=fill=yellow!40 [CALM(Du et al., [2022](#bib.bib30)); LayoutGCN(Shi et al.,
    [2023](#bib.bib107)),for tree=fill=yellow!20 ] ] ]
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树形结构=分叉边缘，grow’=0，绘制，圆角，节点选项=对齐=center，文本宽度=2.7cm，s sep=6pt，calign=子边，calign
    child=(n_children()+1)/2 [基于多任务的模型，对于树形结构=填充=brown!45 [细粒度预训练模型（仅编码器），对于树形结构=填充=violet!40
    [布局感知预训练模型，对于树形结构=填充=violet!20 [LayoutLM (Xu et al., [2020a](#bib.bib136)); BROS(Hong
    et al., [2021](#bib.bib41)); LiLT (Wang et al., [2022b](#bib.bib123)); XDoc (Chen
    et al., [2022](#bib.bib16)); LayoutMask (Tu et al., [2023](#bib.bib119)); StructuralLM
    (Xu et al., [2020a](#bib.bib136)), 对于树形结构=填充=violet!10] ] [视觉集成预训练模型，对于树形结构=填充=violet!20
    [LayoutLMv2(Xu et al., [2020b](#bib.bib138)); LayoutXLM(Xu et al., [2021](#bib.bib137));
    DocFormer(Appalaraju et al., [2021](#bib.bib2)); LayoutLMv3(Huang et al., [2022](#bib.bib43)),
    对于树形结构=填充=violet!10] ] ] [粗粒度和联合粒度预训练模型（仅编码器），对于树形结构=填充=pink!40 [粗粒度模型，对于树形结构=填充=pink!20
    [SelfDoc(Li et al., [2021b](#bib.bib66)); UniDoc(Gu et al., [2021](#bib.bib33))],
    对于树形结构=填充=pink!10 ] [联合粒度模型，对于树形结构=填充=pink!20 [StrucText(Li et al., [2021c](#bib.bib68));
    Fast-StrucText(Zhai et al., [2023](#bib.bib144)); MGDoc (Wang et al., [2022a](#bib.bib127));
    WUKONG-READER (Bai et al., [2022](#bib.bib4)); GeoLayoutLM (Luo et al., [2023](#bib.bib78)),
    对于树形结构=填充=pink!10] ] ] [编码器-解码器预训练框架，对于树形结构=填充=red!40 [OCR依赖的编码器-解码器框架，对于树形结构=填充=red!20
    [TiLT(Li et al., [2021b](#bib.bib66)); UDOP(Tang et al., [2022](#bib.bib116));
    DocFormerv2(Appalaraju et al., [2024](#bib.bib3)); ViTLP(Mao et al., [2024](#bib.bib83)),
    对于树形结构=填充=red!10] ] [OCR独立框架，对于树形结构=填充=red!20 [Donut(Kim et al., [2022](#bib.bib52));
    Dessurt(Davis et al., [2022](#bib.bib21)); ReRum (Cao et al., [2023a](#bib.bib11));
    StructTextV2 (Yu et al., [2022](#bib.bib143)), 对于树形结构=填充=red!10] ] [基于LLM的框架，对于树形结构=填充=red!20
    [HRVDA(Liu et al., [2024b](#bib.bib71)); LayoutLLM(Luo et al., [2024](#bib.bib79)),
    对于树形结构=填充=red!10] ] ] [非预训练框架，对于树形结构=填充=yellow!40 [CALM(Du et al., [2022](#bib.bib30));
    LayoutGCN(Shi et al., [2023](#bib.bib107)), 对于树形结构=填充=yellow!20 ] ] ]
- en: Figure 3\. Multi-task visually rich document understanding frameworks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 多任务视觉丰富的文档理解框架。
- en: Models designed for specific VRDU tasks often incorporate task-oriented techniques.
    Drawing inspiration from pretrained models in the vision (Dosovitskiy et al.,
    [2020](#bib.bib29); Ren et al., [2015](#bib.bib103)) and language (Devlin, [2018](#bib.bib23);
    Liu et al., [2019b](#bib.bib75)) domains, enhancing document representations may
    significantly improve the performance of various downstream tasks. Consequently,
    a range of models have been developed to extract knowledge from extensive document
    collections. These models employ different pretrained tasks based on their architecture,
    the modalities they process, and the granularity of the information they extract
    from documents. Moreover, some models introduce specialised techniques to boost
    the effectiveness of pretrained model representations, achieving better performance
    without heavy pretraining. This section explores various document understanding
    frameworks focused on enhancing document representation for robustness and comprehensiveness
    in addressing multiple VRDU downstream tasks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为特定VRDU任务设计的模型通常会结合任务导向的技术。借鉴于视觉（Dosovitskiy et al., [2020](#bib.bib29); Ren
    et al., [2015](#bib.bib103)）和语言（Devlin, [2018](#bib.bib23); Liu et al., [2019b](#bib.bib75)）领域的预训练模型，增强文档表示可能显著提升各种下游任务的性能。因此，已经开发出一系列模型来从大量文档集合中提取知识。这些模型根据其架构、处理的模态以及从文档中提取信息的粒度，采用不同的预训练任务。此外，一些模型引入了专门的技术来提升预训练模型表示的有效性，在没有大量预训练的情况下实现更好的性能。本节探讨了各种文档理解框架，重点是增强文档表示，以应对多个VRDU下游任务的鲁棒性和全面性。
- en: 4.1\. Fine-grained Pretrained Models
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1. 精细化的预训练模型
- en: Inspired by BERT-style pretrained models, many researchers have proposed effective
    methods to integrate layout and visual information into models, aiming to enhance
    the comprehensiveness of textual token representations.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 受到BERT风格预训练模型的启发，许多研究人员提出了将布局和视觉信息集成到模型中的有效方法，旨在提升文本标记表示的全面性。
- en: 4.1.1\. Layout-aware Pretrained Language Modeling
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1. 布局感知的预训练语言建模
- en: Understanding layout structure and spatial correlations between textual tokens
    can yield a more comprehensive representation of documents, going beyond what
    plain-text input offers. To this end, various methods have been proposed to encode
    layout features. These methods, combined with tailored pretraining tasks, enable
    models to capture layout-aware information better and effectively fuse textual
    and layout features.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 理解布局结构和文本标记之间的空间相关性可以提供比纯文本输入更全面的文档表示。为此，提出了各种方法来编码布局特征。这些方法结合了定制的预训练任务，使模型能够更好地捕捉布局感知的信息，并有效地融合文本和布局特征。
- en: LayoutLM (Xu et al., [2020a](#bib.bib136)) is the first pretrained document
    understanding model by leveraging textual and layout information in the pretraining
    stage. BERT architecture is the backbone and 2-D positional embedding ³³3Please
    refer to Section xx for more detailed information on 2-D positional encoding with
    textual information is used to pretrain on IIT-CDIP Test Collection 1.0\. Two
    specific pretraining tasks are first introduced, named Masked Visual-Language
    Model (MVLM) and Multi-label Document Classification, to generate layout-aware
    textual representation and more comprehensive document representation, respectively.
    Like Masked Language Modeling adopted by most pretrained language models, MVLM
    randomly masks some input tokens but keeps the corresponding 2-D position embeddings
    to predict masked tokens to ensure the pretrained model is aware of the spatial
    relations between input tokens. MDC is a supervised pretraining task to predict
    the input document types (e.g. forms, exam papers, academic papers) that generate
    more comprehensive document-level representations. The fine-tuned LayoutLM could
    perform much better than textual-only frameworks on key information extraction
    (Jaume et al., [2019](#bib.bib47); Huang et al., [2019](#bib.bib44)) and document
    classification (Harley et al., [2015](#bib.bib37)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLM（Xu 等，[2020a](#bib.bib136)）是首个通过在预训练阶段利用文本和布局信息的预训练文档理解模型。BERT 架构是骨干，并使用
    2-D 位置嵌入 ³³3 请参见第 xx 节以获得有关带文本信息的 2-D 位置编码的更多详细信息，以在 IIT-CDIP Test Collection
    1.0 上进行预训练。首先引入了两个特定的预训练任务，分别称为掩码视觉-语言模型（MVLM）和多标签文档分类，以生成布局感知的文本表示和更全面的文档表示。与大多数预训练语言模型采用的掩码语言建模类似，MVLM
    随机掩盖一些输入 token，但保留相应的 2-D 位置嵌入以预测被掩盖的 token，从而确保预训练模型意识到输入 token 之间的空间关系。MDC 是一个有监督的预训练任务，用于预测输入文档类型（例如表单、考试试卷、学术论文），以生成更全面的文档级表示。经过微调的
    LayoutLM 在关键信息提取（Jaume 等，[2019](#bib.bib47); Huang 等，[2019](#bib.bib44)）和文档分类（Harley
    等，[2015](#bib.bib37)）方面表现远优于仅使用文本的框架。
- en: BROS (Hong et al., [2021](#bib.bib41)) aims to propose a pretrained VRDU model
    to represent the continuous property of 2D space by introducing a new 2-D positional
    encoding and a textual-spatial correlation aware attention score to replace the
    vanilla self-attention. Supposing $[x1,y1,x2,y2,x3,y3,x4,y4]$ is the bounding
    box coordinates of input token $t$, $p^{1}=[\mathcal{F}_{sin}(x1)\oplus\mathcal{F}_{sin}(y1)],p^{1}\rightarrow\mathbb{R}^{d_{pos}}$.
    The final positional representation of $t$ is $pos_{t}=W_{p1}p_{1}+W_{p2}p_{2}+W_{p3}p_{3}+W_{p4}p_{4}$,
    where all $W_{p}\in\mathbb{R}^{2d_{pos}\times d}$. The new 2-D positional encoding
    intends to provide a more natural way to encode the continuous 2-D coordinates.
    Additionally, instead of simply summing textual and positional representations,
    a novel attention score is introduced to consider both textual and spatial features
    and their correlations. To calculate the attention score $\alpha_{ij}$ between
    $t_{i}$ and $t_{j}$, they consider correlation intra and inter-correlation between
    textual and positional modalities.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: BROS（Hong 等，[2021](#bib.bib41)）旨在提出一个预训练的 VRDU 模型，通过引入新的 2-D 位置编码和一个文本-空间相关注意力评分来替代传统的自注意力，以表示
    2D 空间的连续属性。假设 $[x1,y1,x2,y2,x3,y3,x4,y4]$ 是输入 token $t$ 的边界框坐标，$p^{1}=[\mathcal{F}_{sin}(x1)\oplus\mathcal{F}_{sin}(y1)],p^{1}\rightarrow\mathbb{R}^{d_{pos}}$。$t$
    的最终位置表示为 $pos_{t}=W_{p1}p_{1}+W_{p2}p_{2}+W_{p3}p_{3}+W_{p4}p_{4}$，其中所有 $W_{p}\in\mathbb{R}^{2d_{pos}\times
    d}$。新的 2-D 位置编码旨在提供一种更自然的方式来编码连续的 2-D 坐标。此外，除了简单地将文本和位置表示相加外，引入了一种新型的注意力评分来考虑文本和空间特征及其相关性。为了计算
    $t_{i}$ 和 $t_{j}$ 之间的注意力评分 $\alpha_{ij}$，他们考虑了文本和位置模态之间的内部和外部相关性。
- en: '| (12) |  | $\displaystyle\alpha_{ij}=(W^{q_{t_{i}}}T_{i})^{\top}(W^{q_{t_{j}}}T_{j})+(W^{q_{t_{i}}}T_{i}\circ
    W^{pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{pos_{t_{j}}}pos_{t_{j}})+(W^{\prime pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{\prime
    pos_{t_{j}}}pos_{t_{j}})$ |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\displaystyle\alpha_{ij}=(W^{q_{t_{i}}}T_{i})^{\top}(W^{q_{t_{j}}}T_{j})+(W^{q_{t_{i}}}T_{i}\circ
    W^{pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{pos_{t_{j}}}pos_{t_{j}})+(W^{\prime pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{\prime
    pos_{t_{j}}}pos_{t_{j}})$ |  |'
- en: where $(W^{q_{t_{i}}}T_{i})^{\top}(W^{q_{t_{j}}}T_{j})$ and $(W^{\prime pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{\prime
    pos_{t_{j}}}pos_{t_{j}})$ is the intra-modality attention scores of textual and
    positional modalities between two tokens. $(W^{q_{t_{i}}}T_{i}\circ W^{pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{pos_{t_{j}}}pos_{t_{j}})$
    is used to formulate the spatial dependency given the source semantic representation.
    Additionally, inspired by SpanBERT (Joshi et al., [2020](#bib.bib49)), they use
    an area-masked language model to mask tokens by random-sized rectangle regions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(W^{q_{t_{i}}}T_{i})^{\top}(W^{q_{t_{j}}}T_{j})$ 和 $(W^{\prime pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{\prime
    pos_{t_{j}}}pos_{t_{j}})$ 是两个标记之间文本和位置模态的内部模态注意力分数。$(W^{q_{t_{i}}}T_{i}\circ W^{pos_{t_{i}}}pos_{t_{i}})^{\top}(W^{pos_{t_{j}}}pos_{t_{j}})$
    用于制定给定源语义表示的空间依赖性。此外，受 SpanBERT（Joshi 等，[2020](#bib.bib49)）的启发，他们使用区域掩模语言模型通过随机大小的矩形区域掩蔽标记。
- en: StructuralLM (Li et al., [2021a](#bib.bib62)) is the first VRUD model using
    image patches (named ”cells” in the paper) to group input tokens for conducting
    various pretraining tasks. They use BERT as the backbone and generate multimodal
    representations $P$ of each patch, which will be used for two pretraining tasks,
    MVLM and Cell (Patch) Position Classification (CPC). The bounding box $(x0,y0,x1,y1)$
    of an image patch is encoded by the 2-D positional encoding introduced by LayoutLM
    and the $n$ tokens inside $p$ are represented as $\{t_{1},t_{2},\dots,t_{n}\}$
    which share the same 2-D positional encoding as for tokens belongs to one patch.
    A token $t_{i}$ could be represented as summing up token representation $T_{i}$,
    2-D positional encoding $pos_{t_{i}}^{2d}$ and 1-D position embedding $pos_{i}$.
    Two pretraining tasks are used to understand the patch-level spatial correlations,
    including MVLM and CPC. MVLM is similar to LayoutLM but uses patch-level layout
    embeddings instead of token-level. Another novel cell position classification
    task is applied to predict the area index of randomly selected tokens from $N$
    evenly split areas. Two pretraining tasks are performed simultaneously to capture
    the patch-level spatial dependencies between input tokens.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: StructuralLM（Li 等，[2021a](#bib.bib62)）是第一个使用图像块（文中称为“单元”）来对输入标记进行分组以执行各种预训练任务的
    VRUD 模型。他们使用 BERT 作为骨干，并生成每个块的多模态表示 $P$，这些表示将用于两个预训练任务，即 MVLM 和单元（块）位置分类（CPC）。图像块的边界框
    $(x0,y0,x1,y1)$ 通过 LayoutLM 引入的 2-D 位置信息编码，而 $p$ 内的 $n$ 个标记表示为 $\{t_{1},t_{2},\dots,t_{n}\}$，这些标记共享与属于一个块的标记相同的
    2-D 位置信息。标记 $t_{i}$ 可以表示为标记表示 $T_{i}$、2-D 位置信息 $pos_{t_{i}}^{2d}$ 和 1-D 位置嵌入 $pos_{i}$
    的和。两个预训练任务用于理解块级空间相关性，包括 MVLM 和 CPC。MVLM 类似于 LayoutLM，但使用块级布局嵌入代替标记级别的嵌入。另一个新颖的单元位置分类任务用于预测从
    $N$ 个均匀分割区域中随机选择的标记的区域索引。两个预训练任务同时进行，以捕捉输入标记之间的块级空间依赖关系。
- en: 'LiLT (Wang et al., [2022b](#bib.bib123)) introduces a language-independent
    layout Transformer for mono/multi-lingual document understanding. The text and
    layout information are first separately encoded and jointly learned during pretraining.
    In the fine-tuning stage, two modality representations are concatenated to perform
    downstream tasks. The input token representations follow BERT, where $j$-th token
    is $T_{j}=T_{j}+pos_{t_{j}}+pos_{t_{j}}^{2d}$. The layout representations are
    slightly different from LayoutLM, normalising the bbox coordinates in the $[0,1000]$
    and four linear layers encode the x-axis, y-axis, width and height features. The
    normalised bbox of a token $t$ is $[x0,y0,x1,y1,w,h]$ is encoded to get the final
    layout representation $L$:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: LiLT（Wang 等，[2022b](#bib.bib123)）引入了一种语言无关的布局 Transformer 用于单语言/多语言文档理解。文本和布局信息首先分别编码，并在预训练期间联合学习。在微调阶段，两个模态表示被串联以执行下游任务。输入标记表示遵循
    BERT，其中第 $j$ 个标记为 $T_{j}=T_{j}+pos_{t_{j}}+pos_{t_{j}}^{2d}$。布局表示与 LayoutLM 略有不同，bbox
    坐标在 $[0,1000]$ 范围内进行归一化，四个线性层编码 x 轴、y 轴、宽度和高度特征。标记 $t$ 的归一化 bbox 为 $[x0,y0,x1,y1,w,h]$
    被编码以获取最终的布局表示 $L$：
- en: '| (13) |  | $\displaystyle L=W_{L}(W_{x}x0\oplus W_{y}x0\oplus W_{x}x1\oplus
    W_{y}y1\oplus W_{w}w\oplus W_{h}h)+pos_{L}$ |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\displaystyle L=W_{L}(W_{x}x0\oplus W_{y}x0\oplus W_{x}x1\oplus
    W_{y}y1\oplus W_{w}w\oplus W_{h}h)+pos_{L}$ |  |'
- en: where $W_{L}\in\mathbb{R}^{6d_{L}^{\prime}\times d_{L}}$ and $W_{x},W_{y},W_{w},W_{h}\in\mathbb{R}^{1\times
    d_{L}}$. The text and layout embedding are fed into two sub-models to generate
    high-level representations. A bi-directional attention complementation mechanism
    (BiACM) is proposed to augment the cross-modality interaction ⁴⁴4We provide a
    detailed explanation of BiACM in Section X. Three self-supervised learning methods
    are proposed to enable the model to understand the multimodal document representation,
    including MVLM and Key Point Location, which is similar to the CPC proposed by
    StructureLM to predict the area index of masked tokens based on the layout features.
    Additionally, a text and layout alignment task is proposed to enhance further
    the cross-modality representation to predict whether each pair is aligned. LiLT
    could achieve promising performance on multi-lingual document understanding benchmarks
    (Wang et al., [2021a](#bib.bib124); Xu et al., [2021](#bib.bib137)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{L}\in\mathbb{R}^{6d_{L}^{\prime}\times d_{L}}$ 和 $W_{x},W_{y},W_{w},W_{h}\in\mathbb{R}^{1\times
    d_{L}}$。文本和布局嵌入被输入到两个子模型中以生成高级表示。提出了一种双向注意力补充机制（BiACM）来增强跨模态交互 ⁴⁴4我们在第 X 节中详细解释了
    BiACM。提出了三种自监督学习方法，以使模型理解多模态文档表示，包括 MVLM 和 Key Point Location，这类似于 StructureLM
    提出的 CPC，旨在根据布局特征预测掩码令牌的区域索引。此外，提出了文本和布局对齐任务，以进一步增强跨模态表示，以预测每对是否对齐。LiLT 在多语言文档理解基准测试
    (Wang et al., [2021a](#bib.bib124); Xu et al., [2021](#bib.bib137)) 上表现出色。
- en: XDoc (Chen et al., [2022](#bib.bib16)) propose a unified framework to deal with
    text inputs from multi-format inputs, including plain text, document and web text.
    Various encoding methods are proposed to tackle diverse text formats. For plain
    text token $t_{pln}$, the token representation follows BERT to get $T_{pln}$.
    For document text token $t_{doc}$, they adopted similar strategies (Xu et al.,
    [2020a](#bib.bib136); Hong et al., [2021](#bib.bib41)) to get $T_{doc}$ by summing
    up initial token representation $E_{t_{doc}}$, 1-D position embedding $pos_{t_{doc}}$
    and 2-D position embedding $pos_{t_{doc}}^{2d}$. $pos_{t_{doc}}^{2d}$ uses different
    bbox formats $[x_{0},x_{1},y_{0},y_{1},w,h]$ following a Linear-ReLu-Linear adaptor
    to make the document format text embedding more representative. To encode web
    text inputs, they follow the way introduced by MarkupLM to encode the source file
    tags and subscript information to get the XPath embedding. The same structured
    adaptor is leveraged for better pretraining. All three text formats are pretrained
    by MLM and fine-tuned on plain text (Rajpurkar et al., [2016a](#bib.bib99); Wang
    et al., [2018](#bib.bib121)), document (Jaume et al., [2019](#bib.bib47); Mathew
    et al., [2021](#bib.bib84)), and web text (Chen et al., [2021](#bib.bib17)) benchmark
    datasets, respectively.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: XDoc (Chen et al., [2022](#bib.bib16)) 提出了一个统一的框架来处理来自多种格式的文本输入，包括纯文本、文档和网页文本。提出了各种编码方法来应对不同的文本格式。对于纯文本令牌
    $t_{pln}$，令牌表示遵循 BERT 以获得 $T_{pln}$。对于文档文本令牌 $t_{doc}$，他们采用了类似的策略 (Xu et al.,
    [2020a](#bib.bib136); Hong et al., [2021](#bib.bib41)) 来通过将初始令牌表示 $E_{t_{doc}}$、1-D
    位置嵌入 $pos_{t_{doc}}$ 和 2-D 位置嵌入 $pos_{t_{doc}}^{2d}$ 相加来获得 $T_{doc}$。$pos_{t_{doc}}^{2d}$
    使用不同的 bbox 格式 $[x_{0},x_{1},y_{0},y_{1},w,h]$，遵循 Linear-ReLu-Linear 适配器，使文档格式的文本嵌入更具代表性。为了编码网页文本输入，他们遵循
    MarkupLM 介绍的方法来编码源文件标签和下标信息，以获得 XPath 嵌入。相同的结构化适配器被用于更好的预训练。所有三种文本格式均通过 MLM 进行预训练，并在纯文本
    (Rajpurkar et al., [2016a](#bib.bib99); Wang et al., [2018](#bib.bib121))、文档 (Jaume
    et al., [2019](#bib.bib47); Mathew et al., [2021](#bib.bib84)) 和网页文本 (Chen et
    al., [2021](#bib.bib17)) 基准数据集上进行微调。
- en: LayoutMask (Tu et al., [2023](#bib.bib119)) aims to improve the text-layout
    interactions by using local 1D positional encoding and new pretraining tasks.
    OCR-dependent document understanding frameworks are suffered from improper reading
    orders. It uses LayoutLMv2 as the backbone but removes the visual module, and
    the local 1D positional encoding is adopted to replace the global 1D positional
    encoding, which only encodes the token ordered within each segment detected by
    OCR tools and always restarts with 1 from each individual segment. Moreover, the
    2D positional encoding is also using segment bbox instead of individual words.
    The first task pertains to traditional masked language modelling, but two different
    masking strategies are used. Firstly, they set the mask at the word level instead
    of masking the token itself to enable the model to capture more contextual information
    to predict the masked word. Additionally, to boost cross-segment understanding,
    the probability of masking the first and last word of a segment is higher than
    that of others. Another Masked Position Modelling is proposed to ask the model
    to predict the masked word-level 2D positions to promote the layout information
    representation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutMask (Tu 等，[2023](#bib.bib119)) 旨在通过使用局部 1D 位置编码和新的预训练任务来改善文本-布局交互。依赖
    OCR 的文档理解框架受到不正确阅读顺序的困扰。它使用 LayoutLMv2 作为骨干，但移除了视觉模块，并采用局部 1D 位置编码替代全局 1D 位置编码，该编码仅对
    OCR 工具检测到的每个段落内的标记进行编码，并且每个段落总是从 1 重新开始。此外，2D 位置编码也使用段框而不是单个词。第一个任务涉及传统的掩蔽语言建模，但使用了两种不同的掩蔽策略。首先，他们在词汇级别设置掩蔽，而不是掩蔽标记本身，以便模型捕捉更多的上下文信息来预测被掩蔽的词。此外，为了增强跨段理解，对段落首词和末词的掩蔽概率高于其他词。还提出了一种掩蔽位置建模，要求模型预测掩蔽词汇级别的
    2D 位置，以促进布局信息的表示。
- en: 4.1.2\. Visual Integrated Models
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 视觉集成模型
- en: 'Integrating visual cues alongside text and layout information during the pretraining
    stage can significantly enhance model capabilities, capturing more comprehensive
    document insights than text and layout alone. Various frameworks and pretraining
    tasks focusing solely on text and layout have been expanded to include visual-text
    matching tasks to strengthen cross-modality alignment. This integration enables
    models to interpret better the complex interplay of visual, textual, and layout
    features within documents. Existing models that utilise these comprehensive inputs
    can be categorised based on their approach to visual feature acquisition: feature
    map-based models, which generate comprehensive visual representations, and patch
    pixel-based models, which focus on granular visual details. This classification
    helps understand how different models leverage visual information to enhance document
    understanding.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段，将视觉线索与文本和布局信息集成可以显著增强模型的能力，捕捉比单独的文本和布局更全面的文档洞察。各种专注于文本和布局的框架和预训练任务已扩展为包括视觉-文本匹配任务，以增强跨模态对齐。这种集成使得模型能够更好地解读文档中视觉、文本和布局特征的复杂相互作用。利用这些综合输入的现有模型可以根据其视觉特征获取的方法进行分类：基于特征图的模型生成全面的视觉表示，而基于补丁像素的模型则专注于细粒度的视觉细节。这种分类有助于理解不同模型如何利用视觉信息来增强文档理解。
- en: LayoutLMv2 (Xu et al., [2020b](#bib.bib138)) is the first pretrained model integrating
    text, layout and visual aspects during the pretraining stage. A single framework
    multimodal transformer is applied to get the visual, text and layout features
    simultaneously, where each input textual and visual tokens are assigned a segment
    ID to distinguish the modality or semantic type. The textual and layout representations
    generally follow LayoutLM, except the linear projected segment ID, $seg_{t}$,
    embeddings are addicted to original textual embeddings. The input document image
    is firstly fed into a trainable visual encoder (based on ResNeXt-FPN), of which
    output is evenly split into $m$ flattened visual tokens, $v_{token}$ following
    a linear layer to project into the same dimension as textual embedding. The $j$-th
    visual token embedding is $V_{j}=W_{v_{j}}+pos_{j}+W_{seg_{v}}$. Moreover, to
    capture the relative position information of inter/intra-modality features, spatial-aware
    self-attention is introduced by adding bias terms of 1-D ($b^{1D}$) and 2-D ($b^{2D}$)
    relative position. The 1-D relative positional bias between input visual or textual
    tokens $t_{i}$ and $t_{j}$ is $b^{1D}=W_{b_{1D}}(j-i)$ and $b^{2D}=b^{2D}_{x}+b^{2D}_{y}$,
    where $b^{2D}_{x}=W_{b^{2D}_{x}}(x0_{i}-x0_{j})$, $b^{2D}_{y}=W_{b^{2D}_{y}}(y0_{i}-y0_{j})$.
    LayoutLMv2 is pretrained on MVLM and the other two new proposed tasks, Text-Image
    Alignment (TIA) and Text-Image Matching (TIM) ⁵⁵5Please refer to Section to check
    more detailed information about those two pretraining tasks. to capture more cross-modality
    alignment.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv2（Xu 等人，[2020b](#bib.bib138)）是第一个在预训练阶段整合文本、布局和视觉方面的预训练模型。采用单一框架的多模态变换器，同时获取视觉、文本和布局特征，其中每个输入的文本和视觉标记被分配一个段落
    ID，以区分模态或语义类型。文本和布局表示通常遵循 LayoutLM，除了线性投影的段落 ID，$seg_{t}$，嵌入被添加到原始文本嵌入中。输入的文档图像首先被输入到一个可训练的视觉编码器（基于
    ResNeXt-FPN），其输出被均匀地拆分成 $m$ 个扁平化的视觉标记，$v_{token}$，经过一个线性层投影到与文本嵌入相同的维度。第 $j$ 个视觉标记嵌入为
    $V_{j}=W_{v_{j}}+pos_{j}+W_{seg_{v}}$。此外，为了捕捉模态间/模态内特征的相对位置信息，引入了空间感知自注意力，通过添加
    1-D ($b^{1D}$) 和 2-D ($b^{2D}$) 相对位置的偏置项来实现。输入视觉或文本标记 $t_{i}$ 和 $t_{j}$ 之间的 1-D
    相对位置偏置为 $b^{1D}=W_{b_{1D}}(j-i)$ 和 $b^{2D}=b^{2D}_{x}+b^{2D}_{y}$，其中 $b^{2D}_{x}=W_{b^{2D}_{x}}(x0_{i}-x0_{j})$，$b^{2D}_{y}=W_{b^{2D}_{y}}(y0_{i}-y0_{j})$。LayoutLMv2
    在 MVLM 上进行预训练，并增加了两个新的任务，文本-图像对齐（TIA）和文本-图像匹配（TIM）⁵⁵5 请参阅相关章节以获取这两个预训练任务的详细信息，以捕捉更多的跨模态对齐。
- en: LayoutXLM (Xu et al., [2021](#bib.bib137)) extends the LayoutLMv2 architecture
    to a multilingual setup where the MVLM is extended to Multilingual Mased Visual-Language
    Modeling pretrained on 22 million self-collected digital-born PDF files and 8
    million scanned English documents from IIT-CDIP processed by off-the-shelf PDF
    parsers.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutXLM（Xu 等人，[2021](#bib.bib137)）将 LayoutLMv2 架构扩展到多语言设置，其中 MVLM 被扩展为多语言视觉语言建模，预训练数据包括
    2200 万个自收集的数字出生 PDF 文件和 800 万个来自 IIT-CDIP 的扫描英文文档，这些文档经过了现成 PDF 解析器的处理。
- en: DocFormer (Appalaraju et al., [2021](#bib.bib2)) is a multimodal transformer
    encoder-based architecture which also uses a trainable CNN-backbone (ResNet50)
    to extract the visual cues of input document image to get visual representations
    $\mathbb{V}\in\mathbb{R}^{(d\times N)}$, where $d=768$ is the transformer hidden
    state and $N=512$ is the number of visual tokens. The initial textual representations
    $\mathbb{T}$ are acquired by feeding the OCR extracted text with bbox into LayoutLM
    where $\mathbb{T}\in\mathbb{R}^{(d\times N)}$. 2D positional encoding is also
    used by using $W^{x}_{v},W^{y}_{v}$ and $W^{x}_{t},W^{y}_{t}$ to encode $x,y$
    axis, as well as, visual and textual aspects to acquire $pos^{2d}_{v}$ and $pos^{2d}_{t}$.
    More positional features are adopted such as width, height and relative distance
    between neighbours. Moreover, certain inductive biases are applied to get a new
    self-attention score $\alpha$ for paying more attention to local features. Supposing
    $\alpha_{ij}^{v}$ is attention score between visual feature $V_{i}$ and $V_{j}$,
    we could have $\alpha_{ij}^{v}=(W^{K}_{v}V_{j})^{\top}(W^{Q}_{v}V_{i})+(pos_{ij})^{\top}W^{Q}_{v}V_{i}+(pos_{ij})^{\top}W^{K}_{v}V_{j}+(W^{Q}_{s}pos_{ij}^{2d})^{\top}(W^{K}_{s}pos_{ij}^{2d})$,
    where $pos_{ij}$ is the 1D relative positional encoding. The same procedures are
    applied to textual features as well, and two modalities share the same $W^{Q}_{s}$
    and $W^{K}_{s}$ to help the model correlate features across modalities. The multi-modal
    token representations $\mathbb{M}$ of $l$ encoder layer is $\mathbb{M}_{l}=\mathbb{T}_{l}+\mathbb{V}_{l}$.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: DocFormer (Appalaraju 等, [2021](#bib.bib2)) 是一种基于多模态变换器编码器的架构，它还使用一个可训练的 CNN
    主干（ResNet50）来提取输入文档图像的视觉线索，以获得视觉表示 $\mathbb{V}\in\mathbb{R}^{(d\times N)}$，其中
    $d=768$ 是变换器隐藏状态的维度，$N=512$ 是视觉标记的数量。初始的文本表示 $\mathbb{T}$ 通过将 OCR 提取的文本和 bbox
    输入 LayoutLM 获得，其中 $\mathbb{T}\in\mathbb{R}^{(d\times N)}$。2D 位置编码也通过使用 $W^{x}_{v},W^{y}_{v}$
    和 $W^{x}_{t},W^{y}_{t}$ 来对 $x,y$ 轴、视觉和文本方面进行编码，从而获得 $pos^{2d}_{v}$ 和 $pos^{2d}_{t}$。还采用了更多的位置信息特征，如宽度、高度和邻近之间的相对距离。此外，应用了某些归纳偏置以获取新的自注意力分数
    $\alpha$，以更多地关注局部特征。假设 $\alpha_{ij}^{v}$ 是视觉特征 $V_{i}$ 和 $V_{j}$ 之间的注意力分数，我们可以得到
    $\alpha_{ij}^{v}=(W^{K}_{v}V_{j})^{\top}(W^{Q}_{v}V_{i})+(pos_{ij})^{\top}W^{Q}_{v}V_{i}+(pos_{ij})^{\top}W^{K}_{v}V_{j}+(W^{Q}_{s}pos_{ij}^{2d})^{\top}(W^{K}_{s}pos_{ij}^{2d})$，其中
    $pos_{ij}$ 是 1D 相对位置编码。相同的过程也适用于文本特征，两个模态共享相同的 $W^{Q}_{s}$ 和 $W^{K}_{s}$ 以帮助模型跨模态关联特征。多模态标记表示
    $\mathbb{M}$ 在第 $l$ 层编码器中的表示为 $\mathbb{M}_{l}=\mathbb{T}_{l}+\mathbb{V}_{l}$。
- en: LayoutLMv3 (Huang et al., [2022](#bib.bib43)) is the first pretrained VRDU model
    to encode visual features without using heavy CNN-backbones. LayoutLMv3 uses the
    exact same way as LayoutLMv2 to encode the textual and layout information except
    replacing the word-level bbox to segment-level for 2-D positional embedding. For
    visual feature representation, they follow ViT and ViLT to linearly project the
    evenly split image patches with learnable 1-D positional encoding to the transformer
    encoder. For the pretraining setting, they use masked language modelling to mask
    30% tokens drawn from a Poisson distribution. To augment the visual representation
    by contextually learning with multimodal features, a Masked Image Modelling (MIM)
    (Bao et al., [2021](#bib.bib5)) task is used to mask 40% image tokens with clockwise
    masking randomly. Each image token will be converted into discrete tokens following
    (Ramesh et al., [2021](#bib.bib101)), and a cross-entropy loss is adopted to reconstruct
    the masked image tokens. Moreover, to explicitly learn the correlation between
    text and image modalities, a Word-Patch Alignment (WPA) pretraining objective
    is applied. WPA is a binary classification task to predict whether the unmasked
    token-patch pair is ”aligned” or ”unaligned”.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv3 (Huang 等, [2022](#bib.bib43)) 是第一个预训练的 VRDU 模型，它在编码视觉特征时没有使用重型 CNN
    主干。LayoutLMv3 使用与 LayoutLMv2 完全相同的方法来编码文本和布局信息，只是将词级 bbox 替换为分段级 bbox，以进行 2-D
    位置嵌入。在视觉特征表示方面，它们遵循 ViT 和 ViLT 将均匀分割的图像补丁通过可学习的 1-D 位置编码线性投影到变换器编码器中。在预训练设置中，它们使用掩码语言建模来掩盖从泊松分布中抽取的
    30% 标记。为了通过与多模态特征进行上下文学习来增强视觉表示，采用了掩码图像建模 (MIM) (Bao 等, [2021](#bib.bib5)) 任务，随机掩盖
    40% 的图像标记。每个图像标记将按照 (Ramesh 等, [2021](#bib.bib101)) 转换为离散标记，并采用交叉熵损失来重建掩盖的图像标记。此外，为了明确学习文本和图像模态之间的相关性，应用了
    Word-Patch Alignment (WPA) 预训练目标。WPA 是一个二分类任务，用于预测未掩盖的标记-补丁对是否为“对齐”或“未对齐”。
- en: 4.2\. Coarse and Joint-grained Pretrained Models
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 粗略和联合粒度的预训练模型
- en: Fine-grained models achieve state-of-the-art performance on many downstream
    tasks but face challenges with input length limitations and capturing document
    image layout and logical arrangement. To address these issues, coarse-grained
    or joint-grained frameworks have been introduced. To mitigate these limitations,
    these frameworks leverage multimodal information from document semantic entities
    such as paragraphs, tables, and textlines.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度模型在许多下游任务中实现了最先进的性能，但面临输入长度限制以及捕捉文档图像布局和逻辑排列的挑战。为了解决这些问题，已经引入了粗粒度或联合粒度框架。为了缓解这些限制，这些框架利用了来自文档语义实体的多模态信息，如段落、表格和文本行。
- en: 4.2.1\. Coarse-grained Frameworks
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 粗粒度框架
- en: SelfDoc (Li et al., [2021b](#bib.bib66)) is the first pretrained VRDU model
    to leverage coarse-grained document elements for various document understanding
    tasks. Unlike fine-grained models that rely on OCR-extracted text sequences, SelfDoc
    uses Faster-RCNN to extract Regions of Interest (RoIs) from document entities.
    This approach reduces the input sequence length for text-dense and long documents
    with improved time and space complexities. To acquire the initial multimodal representations,
    Faster-RCNN extracts visual embeddings from these RoIs, while Sentence-BERT (Reimers
    and Gurevych, [2019](#bib.bib102)) provides textual embeddings based on the OCR-extracted
    text of each entity. The textual and visual representations are fed into two single-modality
    BERT-like encoders to learn the intra-modality correlations between entities contextually.
    A cross-modality encoder is introduced to enhance inter-modality learning, mirroring
    the structure of single-modality encoders but incorporating cross-attention layers.
    It randomly masks entities in document images within either language or vision
    branches during pretraining. Furthermore, a modality-adaptive attention mechanism
    dynamically adjusts weights for visual and textual embeddings based on the input
    and task, creating robust entity representations for fine-tuning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: SelfDoc（Li 等人，[2021b](#bib.bib66)）是首个预训练的 VRDU 模型，利用粗粒度文档元素进行各种文档理解任务。与依赖 OCR
    提取的文本序列的细粒度模型不同，SelfDoc 使用 Faster-RCNN 从文档实体中提取兴趣区域（RoIs）。这种方法减少了文本密集和长文档的输入序列长度，并改善了时间和空间复杂度。为了获取初始的多模态表示，Faster-RCNN
    从这些 RoIs 中提取视觉嵌入，而 Sentence-BERT（Reimers 和 Gurevych，[2019](#bib.bib102)）则基于每个实体的
    OCR 提取文本提供文本嵌入。文本和视觉表示被输入到两个单模态 BERT 类似的编码器中，以在上下文中学习实体之间的模态内相关性。引入了一个跨模态编码器来增强跨模态学习，其结构类似于单模态编码器，但加入了跨注意力层。在预训练过程中，它随机屏蔽文档图像中的实体，分布在语言或视觉分支中。此外，模态自适应注意力机制根据输入和任务动态调整视觉和文本嵌入的权重，创建出强大的实体表示以进行微调。
- en: UniDoc (Gu et al., [2021](#bib.bib33)) is an entity-level document understanding
    model which contains a trainable image encoder with RoI-Align (He et al., [2017](#bib.bib39))
    to extract entity visual representation and novel cross-attention mechanisms to
    fuse multimodal information. The initial sentence and textual embeddings are acquired
    by RoI features and averaged word embeddings summing up with the linear projected
    bbox coordinates. To enable the trainable visual representation to predict meaningful
    visual cues effectively, product quantization (Jegou et al., [2010](#bib.bib48))
    is applied to discretise a RoI feature into a finite set of visual representations
    and mapped to a new embedding. Additionally, a novel Gated Cross-Attention is
    introduced to improve the interactive learning between various modalities. After
    a multi-head cross-attention to get the enhanced visual and textual representations,
    a gating mechanism is applied to dynamically weight the visual ($V$) and textual
    features ($T$) by feeding concatenated $[V:T]$ to a non-linear network to acquire
    a modality-aware attention bias $\beta_{v},\beta_{t}$ for visual and textual respectively.
    There are three pretraining tasks are conducted simultaneously to enhance multimodal
    feature representatives. Masked Sentence Modelling and Visual Contrastive Learning
    are required to predict the masked sentence representation and quantize visual
    representation, respectively. Another Vision-Language Alignment introduced by
    LayoutLMv2 (Xu et al., [2020b](#bib.bib138)) to enforce contextual learning between
    multi-modalities. But instead of using split image regions, UniDoc aligns the
    image and text to belonging to the same entity.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: UniDoc (Gu et al., [2021](#bib.bib33)) 是一个实体级文档理解模型，其中包含一个可训练的图像编码器，该编码器使用 RoI-Align
    (He et al., [2017](#bib.bib39)) 提取实体的视觉表示，并使用新颖的跨注意力机制融合多模态信息。初始句子和文本嵌入通过 RoI
    特征和平均词嵌入获取，并与线性投影的 bbox 坐标相加。为了使可训练的视觉表示能够有效地预测有意义的视觉线索，应用了产品量化 (Jegou et al.,
    [2010](#bib.bib48)) 将 RoI 特征离散化为有限的视觉表示集，并映射到新的嵌入。此外，引入了一种新颖的门控跨注意力机制，以提高不同模态之间的交互学习。经过多头跨注意力获取增强的视觉和文本表示后，应用了一个门控机制，通过将连接的
    $[V:T]$ 输入到一个非线性网络中，动态地加权视觉 ($V$) 和文本特征 ($T$)，以获得视觉和文本的模态感知注意力偏置 $\beta_{v},\beta_{t}$。同时进行三个预训练任务以增强多模态特征表示。需要进行掩蔽句子建模和视觉对比学习，分别用于预测掩蔽句子的表示和量化视觉表示。另一个由
    LayoutLMv2 (Xu et al., [2020b](#bib.bib138)) 引入的视觉-语言对齐方法用于强化多模态之间的上下文学习。但与使用拆分图像区域不同，UniDoc
    将图像和文本对齐到属于同一实体。
- en: 4.2.2\. Joint-grained Frameworks
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 细粒度框架
- en: 'StrucText (Li et al., [2021c](#bib.bib68)) is a multimodal pretrained VRDU
    model using fine-grained textual and coarse-grained vision information to capture
    richer geometric and semantic information from different levels and modalities.
    The layout embedding of each text token and segment is encoded by $L=W_{l}[x0,y0,x1,y1,w,h],W_{1}\in\mathbb{R}^{6\times
    N}$. They follow token-level models to encode the sequence of tokens and use pretrained
    ResNet50 with FPN to extract the entity visual features. A segment-ID embedding
    is allocated to each token’s textual and entity visual representations to boost
    the alignment learning. Three self-supervised tasks are introduced to enhance
    inter-modality learning: MVLM, Segment length Prediction (SLP), and Paired Box
    Direction (PBD). As a new self-supervised learning task, SLP asks the model to
    predict the entity’s length to leverage the entity’s visual embeddings and textual
    information from the same segment ID. Another self-supervised learning task aims
    to learn more comprehensive pair-wise spatial correlations between entities by
    clarifying their spatial correlation.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: StrucText (Li et al., [2021c](#bib.bib68)) 是一个多模态预训练的 VRDU 模型，利用细粒度的文本信息和粗粒度的视觉信息从不同层次和模态中捕获更丰富的几何和语义信息。每个文本标记和段落的布局嵌入由
    $L=W_{l}[x0,y0,x1,y1,w,h],W_{1}\in\mathbb{R}^{6\times N}$ 编码。它们遵循标记级模型来编码标记序列，并使用预训练的
    ResNet50 与 FPN 提取实体的视觉特征。为每个标记的文本和实体视觉表示分配了一个段-ID 嵌入，以提高对齐学习。引入了三个自监督任务来增强模态间学习：MVLM、段长预测
    (SLP) 和配对框方向 (PBD)。作为一个新的自监督学习任务，SLP 要求模型预测实体的长度，以利用来自同一段-ID 的实体视觉嵌入和文本信息。另一个自监督学习任务旨在通过明确实体之间的空间相关性，学习更全面的配对空间关联。
- en: Fast-StructText (Zhai et al., [2023](#bib.bib144)) is built on StrucText to
    improve model efficiency and enhance the feature expressiveness by introducing
    an hourglass transformer and Symmetry Cross-Attention mechanisms (SCA). Similar
    feature encoding methods to StructText are adopted, but the bbox formats of layout
    encoding are $[x0,y0,x1,y1]$. To encode the module and progressively reduce the
    redundancy tokens, an hourglass transformer is proposed consisting of several
    Merging blocks and Extension blocks to down-sampling and up-sampling the number
    of input tokens. Merging blocks suggest merging neighbouring $k$ tokens with the
    weighted 1D average pooling to shorten the sequence length. The extension block
    is required to transform the shortened sequence back to the entire length by simply
    applying a repeat up-sampling method. In addition, to enhance the interaction
    between textual and vision modalities, SCA consists of two dual cross-attention
    to handle text and visual features. Different self-supervised tasks are conducted
    simultaneously, including MVLM (Xu et al., [2020b](#bib.bib138); Li et al., [2021c](#bib.bib68)),
    Graph-based Token Relation (GTR), Sentence Order Prediction (SOP) and Text-Image
    Alignment (Huang et al., [2022](#bib.bib43)). For the two new proposed tasks,
    GTR is a task similar to Paired-Box Direction to predict the positional relations
    between paired entity visual features and SOP is used to predict whether two sentence
    pairs are normal-order adjacent to learning semantic knowledge.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Fast-StructText (Zhai et al., [2023](#bib.bib144)) 基于 StrucText 构建，以通过引入沙漏形变换器和对称交叉注意力机制（SCA）来提高模型效率并增强特征表现力。采用与
    StructText 相似的特征编码方法，但布局编码的 bbox 格式为 $[x0,y0,x1,y1]$。为了对模块进行编码并逐步减少冗余标记，提出了一种由多个合并块和扩展块组成的沙漏形变换器，用于对输入标记的数量进行下采样和上采样。合并块建议使用加权的一维平均池化来合并相邻的
    $k$ 个标记，以缩短序列长度。扩展块需要通过简单地应用重复上采样方法将缩短的序列转换回完整长度。此外，为了增强文本和视觉模态之间的互动，SCA 由两个双重交叉注意力机制组成，以处理文本和视觉特征。与此同时，进行不同的自监督任务，包括
    MVLM (Xu et al., [2020b](#bib.bib138); Li et al., [2021c](#bib.bib68))、基于图的标记关系（GTR）、句子顺序预测（SOP）和文本-图像对齐（Huang
    et al., [2022](#bib.bib43)）。对于两个新提出的任务，GTR 是一个类似于配对框方向的任务，用于预测配对实体视觉特征之间的位置关系，而
    SOP 用于预测两个句子对是否为正常顺序相邻，以学习语义知识。
- en: MGDoc (Wang et al., [2022a](#bib.bib127)) is the first multimodal multi-granular
    pretrained framework which introduces multi-granular attention and cross-modal
    attention to increase the inter-grained learning and better cross-modality fusion.
    For acquiring initial features of different modalities, pretrained language models
    (Reimers and Gurevych, [2019](#bib.bib102)) and vision encoders (He et al., [2016](#bib.bib40))
    are used to encode different-grained inputs from word to whole page. The encoded
    modalities are associated with positional features (Zhai et al., [2023](#bib.bib144))
    and modality-type embeddings (Li et al., [2021c](#bib.bib68)) to acquire the final
    input representation of various modalities. To encode the hierarchical relation
    between multi-grained features, including pages, entities and words, two attention
    biases are added to the original self-attention weights. A hierarchical bias is
    a binary value of 0 or 1 to examine the inside or outside relation between two
    inputs, while the relation bias is the relative positive between two bboxes. Apart
    from multi-grained learning, cross-attention is applied to fuse cross-modality
    information better. Three tasks are proposed during pretraining, including Mask
    Text Modeling (MTM), Mask Vision Modeling (MVM) and Multi-Granularity Modeling.
    MTM and MVM randomly mask the multi-grained textual and visual representations
    to predict the masked features under the mean absolute error. To boost the interactive
    understanding of multi-granularity, a token-entity linking prediction is proposed
    by computing the dot-product between token and entity features.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: MGDoc（Wang 等， [2022a](#bib.bib127)）是第一个多模态多粒度预训练框架，引入了多粒度注意力和跨模态注意力，以增加不同粒度的学习并更好地融合跨模态。为了获取不同模态的初始特征，使用预训练语言模型（Reimers
    和 Gurevych，[2019](#bib.bib102)）和视觉编码器（He 等，[2016](#bib.bib40)）对从单词到整页的不同粒度输入进行编码。编码后的模态与位置特征（Zhai
    等，[2023](#bib.bib144)）和模态类型嵌入（Li 等，[2021c](#bib.bib68)）相关联，以获得各种模态的最终输入表示。为了编码多粒度特征之间的层次关系，包括页面、实体和单词，在原始自注意力权重中添加了两个注意力偏置。层次偏置是0或1的二进制值，用于检查两个输入之间的内部或外部关系，而关系偏置是两个边界框之间的相对正值。除了多粒度学习外，跨注意力也被应用于更好地融合跨模态信息。在预训练期间提出了三个任务，包括掩码文本建模（MTM）、掩码视觉建模（MVM）和多粒度建模。MTM
    和 MVM 随机掩盖多粒度的文本和视觉表示，以在平均绝对误差下预测被掩盖的特征。为了提升对多粒度的交互理解，通过计算令牌和实体特征之间的点积，提出了一种令牌-实体链接预测。
- en: WUKONG-READER (Bai et al., [2022](#bib.bib4)) uses fine-grained level inputs
    but leverages coarse-grained level self-supervised tasks to enhance the fine-grained
    level information. The model inputs contain a document image and OCR extracted
    sequence of tokens with their bboxes. A Mask-RCNN is used as a visual backbone
    to acquire several visual tokens and extract visual features of textlines from
    a RoIHead layer. For textual information encoding, the first is layers of RoBERTa(Wei
    et al., [2020](#bib.bib134)) are applied to extract the token representation and
    sum up with additional features following (Xu et al., [2020b](#bib.bib138)). The
    concatenated visual and textual features are fed into a multimodal modal encoder
    based on the rest six layers of RoBERTa. Different pretraining tasks are proposed,
    including MLM, Textline-Region Contrastive Learning (TRC), Masked Region Modeling
    (MRM) and Textline Grid Matching (TCM). TRC aims to enhance cross-modality interactive
    learning at the entity (textline) level by following a text-vision-aligned contrastive
    learning approach (Yao et al., [2021](#bib.bib140)). To enhance the visual representation,
    MRM is applied to mask 15% of textlines randomly and to predict the masked visual
    embeddings following (Li et al., [2021b](#bib.bib66)). A text-grid alignment task
    is introduced to enhance layout understanding by dividing the document image into
    N-grids and predicting the tokens in 15% selected textlines belonging to which
    grid. The scaling parameters are applied to each loss of pretraining tasks.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: WUKONG-READER (Bai et al., [2022](#bib.bib4)) 使用精细粒度的输入，但利用粗粒度自监督任务来增强精细粒度信息。模型输入包含文档图像和带有其边框的
    OCR 提取的标记序列。使用 Mask-RCNN 作为视觉骨干网络来获取多个视觉标记，并从 RoIHead 层提取文本行的视觉特征。对于文本信息编码，首先应用
    RoBERTa(Wei et al., [2020](#bib.bib134)) 的层来提取标记表示，并与额外特征进行汇总（Xu et al., [2020b](#bib.bib138)）。将拼接的视觉和文本特征输入到基于
    RoBERTa 其余六层的多模态编码器中。提出了不同的预训练任务，包括 MLM、文本行-区域对比学习 (TRC)、遮蔽区域建模 (MRM) 和文本行网格匹配
    (TCM)。TRC 旨在通过遵循文本-视觉对齐的对比学习方法来增强实体（文本行）级别的跨模态交互学习 (Yao et al., [2021](#bib.bib140))。为了增强视觉表示，应用
    MRM 随机遮蔽 15% 的文本行，并预测遮蔽的视觉嵌入 (Li et al., [2021b](#bib.bib66))。引入了文本网格对齐任务，通过将文档图像划分为
    N 网格，并预测 15% 选择的文本行属于哪个网格，从而增强布局理解。对每个预训练任务的损失应用缩放参数。
- en: GeoLayoutLM (Luo et al., [2023](#bib.bib78)) is a sophisticated multimodal framework
    that distinctively incorporates geometric information through specialised pretraining
    tasks and the development of innovative relation heads. Inspired by the dual-stream
    structure of METER and SelfDoc (Li et al., [2021b](#bib.bib66)), GeoLayoutLM features
    separate vision and text-layout modules coupled with interactive co-attention
    layers that enhance the integration of visual and textual data. The model introduces
    two advanced relation heads—the Coarse Relation Prediction (CRP) head and the
    Relation Feature Enhancement (RFE) head—which refine relation feature representation
    crucial for both pretraining and fine-tuning phases. The pretraining regimen includes
    tasks designed to understand geometric relationships, such as GeoPair, GeoMPair,
    and GeoTriplet, aiding the model in grasping the complex dynamics of document
    layouts. During fine-tuning, the model utilises pretrained parameters to optimise
    both semantic entity recognition and relation extraction tasks, employing a novel
    inference technique that enhances relation pair selection accuracy by focusing
    on the most probable relationships and minimizing variance among potential options.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: GeoLayoutLM (Luo et al., [2023](#bib.bib78)) 是一个复杂的多模态框架，独特地通过专业的预训练任务和创新的关系头来融入几何信息。受
    METER 和 SelfDoc (Li et al., [2021b](#bib.bib66)) 的双流结构启发，GeoLayoutLM 具有独立的视觉和文本布局模块，并配备了交互式共同注意层，增强了视觉和文本数据的整合。该模型引入了两个先进的关系头——粗略关系预测
    (CRP) 头和关系特征增强 (RFE) 头——这些头精炼了关系特征表示，对于预训练和微调阶段至关重要。预训练方案包括理解几何关系的任务，如 GeoPair、GeoMPair
    和 GeoTriplet，帮助模型掌握文档布局的复杂动态。在微调过程中，模型利用预训练参数优化语义实体识别和关系提取任务，采用一种新颖的推理技术，通过关注最可能的关系并最小化潜在选项之间的方差，来提高关系对选择的准确性。
- en: 4.3\. Encoder-Decoder Pretrained Frameworks
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 编码器-解码器预训练框架
- en: In addition to the above encoder-only frameworks, researchers have proposed
    encoder-decoder pretrained models that often approach tasks like Key Information
    Extraction (KIE) or Visual Question Answering (VQA) in a generative style. Addressing
    limitations of OCR-dependent frameworks, such as accumulated OCR errors and incorrect
    reading orders, OCR-independent models have been introduced for end-to-end VRDU.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述仅编码器框架外，研究人员还提出了编码器-解码器预训练模型，这些模型通常以生成风格处理关键资料提取（KIE）或视觉问答（VQA）等任务。为了解决OCR依赖框架的局限性，如累积的OCR错误和错误的阅读顺序，已经引入了无OCR的模型以实现端到端的VRDU。
- en: 4.3.1\. OCR-dependent Encoder-Decoder Frameworks
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 依赖OCR的编码器-解码器框架
- en: TILT (Powalski et al., [2021](#bib.bib96)) is a T5-based transformer encoder-decoder
    architecture enhanced with a relative spatial bias in the self-attention mechanism
    to acquire fine-grained token representations. It incorporates encoding methods
    that apply relative sequence input bias and capture horizontal and vertical distance
    biases in the attention scores. A U-Net-based framework is applied to extract
    the fixed-size feature maps fed into the encoder together. They follow T5 pretraining
    strategies on RVL-CIDP dataset (Harley et al., [2015](#bib.bib37)) but use a salient
    span masking scheme adopted by (Raffel et al., [2020](#bib.bib98); Guu et al.,
    [2020](#bib.bib36)). As the first encoder-decoder framework, TILT requires off-the-shelf
    OCR tools to acquire the textual token sequence. To conduct VRD content understanding
    end-to-end, some OCR-free frameworks are proposed to solve the limitations of
    OCR-dependent models.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: TILT（Powalski et al., [2021](#bib.bib96)）是一种基于T5的变换器编码器-解码器架构，通过在自注意力机制中增强相对空间偏差，以获得细粒度的标记表示。它结合了应用相对序列输入偏差的编码方法，并捕捉注意力得分中的水平和垂直距离偏差。一个基于U-Net的框架被用来提取固定大小的特征图，并一起输入到编码器中。它们在RVL-CIDP数据集（Harley
    et al., [2015](#bib.bib37)）上遵循T5的预训练策略，但使用了(Raffel et al., [2020](#bib.bib98);
    Guu et al., [2020](#bib.bib36))采用的显著跨度遮蔽方案。作为第一个编码器-解码器框架，TILT需要现成的OCR工具来获取文本标记序列。为了实现端到端的VRD内容理解，一些无OCR框架被提出以解决依赖OCR模型的限制。
- en: UDOP (Tang et al., [2022](#bib.bib116)) proposes an encoder-decoder pre-trained
    document understanding framework that leverages a ViT-based model (Dosovitskiy
    et al., [2020](#bib.bib29)), following the principles of LayoutLMv3 (Huang et al.,
    [2022](#bib.bib43)), to process multimodal information. To enhance the comprehensiveness
    of textual token embeddings, the framework sums the text token embeddings with
    token-aligned image patch embeddings when the centre of the bounding box falls
    within the image patch. Additionally, the positional biases introduced by TILT
    are added, but ID positional encoding is not used. The decoding stage is designed
    to generate all vision, text and layout modalities consisting of a bi-directional
    Transformer text-layout decoder and an MAE vision decoder. Two decoders are cross-attended
    with each other. The pertaining tasks contain self-supervised on unlabeled documents
    to learn robust document representation and supervised pertaining tasks for fine-grained
    model supervision. Masked text layout and image reconstruction are used to predict
    the masked information using unmasked multimodal information. A layout moulding
    task is applied to predict the positions of groups of text tokens, and a visual
    text recognition task is proposed to identify text at the given location in the
    image. Supervised tasks utilise a training set of publicly available benchmark
    datasets of different downstream tasks, including document classification (Harley
    et al., [2015](#bib.bib37)), KIE (Stanisławek et al., [2021](#bib.bib112)), VQA
    (Tanaka et al., [2021](#bib.bib115); Mathew et al., [2021](#bib.bib84)) and layout
    analysing (Zhong et al., [2019](#bib.bib150)).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: UDOP（Tang 等， [2022](#bib.bib116)）提出了一种编码器-解码器预训练的文档理解框架，该框架利用基于 ViT 的模型（Dosovitskiy
    等，[2020](#bib.bib29)），遵循 LayoutLMv3（Huang 等，[2022](#bib.bib43)）的原则，以处理多模态信息。为了增强文本令牌嵌入的全面性，该框架在边界框中心位于图像补丁内时，将文本令牌嵌入与令牌对齐的图像补丁嵌入相加。此外，引入了
    TILT 提出的定位偏差，但未使用 ID 定位编码。解码阶段旨在生成所有视觉、文本和布局模态，包括双向 Transformer 文本-布局解码器和 MAE
    视觉解码器。两个解码器彼此进行交叉注意。相关任务包括在未标记文档上的自监督学习，以学习鲁棒的文档表示，并进行有监督的相关任务，以实现细粒度模型监督。使用掩码文本布局和图像重建来预测掩码信息，使用未掩码的多模态信息。应用布局模具任务来预测文本令牌组的位置，并提出了一个视觉文本识别任务，以识别图像中给定位置的文本。有监督的任务利用不同下游任务的公开基准数据集的训练集，包括文档分类（Harley
    等，[2015](#bib.bib37)）、KIE（Stanisławek 等，[2021](#bib.bib112)）、VQA（Tanaka 等，[2021](#bib.bib115)；Mathew
    等，[2021](#bib.bib84)）和布局分析（Zhong 等，[2019](#bib.bib150)）。
- en: DocFormerv2 (Appalaraju et al., [2024](#bib.bib3)) is an encoder-decoder transformer
    architecture that uses multimodal (visual, textual and positional) to enhance
    the multimodal understanding and layout-aware language decoder to predict the
    predictions. The patched image pixels are fed into the convolutional and linear
    layers to get down-sampled patch embedding. The textual embeddings are acquired
    by linear projected token one-hot encoding. Both visual and textual embeddings
    are summed with the 2D-positional encoding (Xu et al., [2020a](#bib.bib136)) of
    Patches and linear project bbox embedding ($[x0,y0,x1,y1]$) (Wang et al., [2022b](#bib.bib123)),
    respectively. Two encoder-based Token to Line (T2L), Token to Grid (T2G), and
    one decoder-based self-supervised learning task, MLM, are proposed to enable multimodal
    feature interactive learning. T2L aims to improve the relative position understanding
    between tokens by predicting the number of textlines between two randomly selected
    tokens. For improving the layout and structure, understanding needs to split the
    image into $m\times n$ grids to predict the located grid number of each OCR token.
    For the decoder MLM, the spatial feature of each masked text token is masked as
    well, and the other setup follows T5 (Raffel et al., [2020](#bib.bib98)).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: DocFormerv2（Appalaraju 等，[2024](#bib.bib3)）是一个编码器-解码器 Transformer 架构，它使用多模态（视觉、文本和位置）来增强多模态理解，并使用布局感知语言解码器进行预测。将图像像素送入卷积层和线性层以获得下采样的补丁嵌入。文本嵌入通过线性投影的令牌独热编码获得。视觉和文本嵌入分别与补丁的
    2D 定位编码（Xu 等，[2020a](#bib.bib136)）和线性投影的边界框嵌入（$[x0,y0,x1,y1]$）（Wang 等，[2022b](#bib.bib123)）相加。提出了两个基于编码器的任务：Token
    to Line（T2L）、Token to Grid（T2G），以及一个基于解码器的自监督学习任务 MLM，以实现多模态特征的交互学习。T2L 旨在通过预测两个随机选择的令牌之间的文本行数来提高令牌之间的相对位置理解。为了提高布局和结构理解，需要将图像拆分为
    $m\times n$ 网格，以预测每个 OCR 令牌所在的网格位置。对于解码器 MLM，每个掩码文本令牌的空间特征也被掩码，其余设置遵循 T5（Raffel
    等，[2020](#bib.bib98)）。
- en: 'ViTLP (Mao et al., [2024](#bib.bib83)) introduces an encoder-decoder architecture
    for OCR and document understanding using a ViT-based vision encoder to obtain
    image patch representations, which are fed into a decoder to generate text and
    layout sequences autoregressively. A special ”[LOC]” token, encoding bounding
    box coordinates $[x0,y0,x1,y1]$, reduces the layout token sequence length. To
    control generation flow, ”[BOS]” and ”[CONT]” tokens are added, representing input
    sequences of two tokens, t1,t2, as ”[BOS], t1, [LOC], t2, [LOC]”. The decoder
    has hierarchical heads: the text head uses all tokens to generate the next text
    token, while the layout head uses ”[LOC]” tokens to predict bounding box coordinates.
    The ”[CONT]” token handles sequences of arbitrary length by continuing generation
    until ”[END]”, based on the prefix token ratio.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ViTLP (Mao et al., [2024](#bib.bib83)) 介绍了一种用于 OCR 和文档理解的编码器-解码器架构，使用基于 ViT
    的视觉编码器来获取图像补丁表示，这些表示被输入到解码器中，以自回归方式生成文本和布局序列。一个特殊的”[LOC]”标记，编码边界框坐标 $[x0,y0,x1,y1]$，减少了布局标记序列的长度。为了控制生成流程，添加了”[BOS]”
    和 ”[CONT]” 标记，表示由两个标记 t1 和 t2 组成的输入序列，形式为”[BOS], t1, [LOC], t2, [LOC]”。解码器具有层次化头部：文本头使用所有标记生成下一个文本标记，而布局头使用”[LOC]”标记预测边界框坐标。
    ”[CONT]” 标记通过继续生成直到 ”[END]” 来处理任意长度的序列，基于前缀标记的比例。
- en: 4.3.2\. OCR-free Pretrained Frameworks
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 无需 OCR 的预训练框架
- en: Donut (Kim et al., [2022](#bib.bib52)) is the first OCR-free VRD understanding
    model to understand and extract key information from input document images. Donut
    contains a Swin Transformer-based visual encoder to encode the input document
    image into image patches, which are then fed into a BART-based (Lewis et al.,
    [2020](#bib.bib61)) decoder pretrained on multi-lingual scenarios. During model
    training, teacher forcing is applied, and in the test stage, inspired by GPT-3
    (Brown et al., [2020](#bib.bib10)), prompts with special identify tokens are fed
    into the model for different downstream tasks. The output token sequence contains
    the special tokens $<START\_*>$ and $<END\_*>$ to identify the type of tasks and
    struct predict entities. The wrongly structured entity will be treated as an empty
    prediction. The model is pretrained on next-token prediction on the IIT-CDIP dataset
    and a Synthetic Dataset, which can be interpreted as a pseudo-OCR task. Similar
    to Donut, Dessurt (Davis et al., [2022](#bib.bib21)) also proposes an encoder-decoder
    architecture but a different decoding process. Instead of using BART, the cross-attention
    used in Dessurt attends to all visual, query and previously generated textual
    information and is pretrained on more synthetic datasets with different font sizes
    and handwritten content.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Donut (Kim et al., [2022](#bib.bib52)) 是首个无需 OCR 的 VRD 理解模型，旨在理解和提取输入文档图像中的关键信息。Donut
    包含一个基于 Swin Transformer 的视觉编码器，将输入文档图像编码为图像补丁，然后输入到一个基于 BART 的 (Lewis et al.,
    [2020](#bib.bib61)) 解码器，该解码器在多语言场景下进行了预训练。在模型训练过程中应用了教师强制，而在测试阶段，受 GPT-3 (Brown
    et al., [2020](#bib.bib10)) 启发，将具有特殊标识符的提示输入到模型中用于不同的下游任务。输出标记序列包含特殊标记 $<START\_*>$
    和 $<END\_*>$，以识别任务类型并结构化预测实体。结构错误的实体将被视为空预测。该模型在 IIT-CDIP 数据集和一个合成数据集上进行了下一个标记预测的预训练，可以解释为伪
    OCR 任务。与 Donut 相似，Dessurt (Davis et al., [2022](#bib.bib21)) 也提出了一种编码器-解码器架构，但采用了不同的解码过程。与
    BART 不同，Dessurt 中使用的交叉注意力关注所有视觉、查询和先前生成的文本信息，并在具有不同字体大小和手写内容的更多合成数据集上进行了预训练。
- en: 'ReRum (Cao et al., [2023a](#bib.bib11)) introduce an end-to-end architecture
    in which the decoding process focuses on local interest and visual cues. Like
    other OCR-free frameworks, a Swin-Transformer extracts image patch representations.
    The extracted visual features are fed into a transformer-based Query-Decoder to
    acquire the vision-enhanced query representation. The enhanced query representation
    with visual features is fed into a Text Decoder to generate the predicted text
    tokens auto-regressively. Notably, a Content-aware Token Merge technique is proposed
    to dynamically focus on more relevant foreground parts of visual features, which
    selects top-K visual tokens based on the averaged correlation scores between query
    and visual token representations. Additionally, the unselected visual tokens (called
    Background Area) contain rich global features that could be used to enhance the
    Top-K foreground visual tokens through basic attention mechanisms. Three pretraining
    tasks are applied: query to segmentation (Q2S), text to segmentation (T2S), and
    segmentation to text (S2T). Q2S follows DETR setup for a token generation but
    alters to an instance segmentation task to predict $N$ mask for the target text
    area to improve text detection ability. T2S acquire the Text Decoder output to
    conduct another instance segmentation task after feeding the image text snippet
    into the Text Decoder to boost the layout-aware textual information understanding.
    The last S2T is to use the outputs from the Text Decoder to generate the token
    like an OCR task auto-regressively.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ReRum（Cao et al., [2023a](#bib.bib11)）引入了一个端到端的架构，其中解码过程专注于局部兴趣和视觉线索。像其他无OCR框架一样，Swin-Transformer提取图像补丁表示。提取的视觉特征被输入到基于Transformer的查询-解码器中，以获得视觉增强的查询表示。带有视觉特征的增强查询表示被输入到文本解码器中，以自回归地生成预测的文本令牌。值得注意的是，提出了一种内容感知令牌合并技术，以动态地关注视觉特征中更相关的前景部分，这种技术基于查询和视觉令牌表示之间的平均相关性分数选择前K个视觉令牌。此外，未被选择的视觉令牌（称为背景区域）包含丰富的全局特征，可以通过基本的注意机制来增强前K个前景视觉令牌。应用了三个预训练任务：查询到分割（Q2S）、文本到分割（T2S）和分割到文本（S2T）。Q2S遵循DETR设置进行令牌生成，但更改为实例分割任务，以预测$N$个掩码用于目标文本区域，以提高文本检测能力。T2S获取文本解码器输出，在将图像文本片段输入到文本解码器后，进行另一个实例分割任务，以提升布局感知文本信息理解。最后的S2T是使用来自文本解码器的输出，自回归地生成类似OCR任务的令牌。
- en: 'StrucTextV2 (Yu et al., [2022](#bib.bib143)) is an end-to-end structure that
    uses image-only input to conduct several downstream tasks. It contains a CNN-based
    visual extractor with FPN strategies (Lin et al., [2017](#bib.bib70)) and follows
    ViT (Dosovitskiy et al., [2020](#bib.bib29)) to get linear projected flattened
    patch-level representations. The patch token embeddings serve as the input to
    the Transformer encoder to enhance the contextually semantic representations.
    Then, the lightweight fusion network is applied to generate the final representations
    and fed into two branches during pretraining: made language Modeling (MLM) and
    Masked Image Modeling (MIM). Instead of using text inputs when MLM is used by
    other models (Devlin, [2018](#bib.bib23)), a portion of the text regions are masked
    with RGB values $[255,255,255]$ randomly with a 2-layer MLP decoder to predict
    the masked token. MIM masks the rectangular text regions and predicts the RGB
    values of the missing pixels to improve the document representations. Except for
    the global average pooled FPN fused visual representations, the MLM-generated
    hidden state of each text region is concatenated and fed into a Fully Convolutional
    New York to get the regressed masked missing pixel values.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: StrucTextV2（Yu et al., [2022](#bib.bib143)）是一个端到端的结构，使用仅图像输入来进行多个下游任务。它包含一个基于CNN的视觉提取器，采用FPN策略（Lin
    et al., [2017](#bib.bib70)），并遵循ViT（Dosovitskiy et al., [2020](#bib.bib29)）来获取线性投影的扁平补丁级表示。补丁令牌嵌入作为Transformer编码器的输入，以增强上下文语义表示。然后，应用轻量级融合网络生成最终表示，并在预训练期间输入两个分支：掩码语言建模（MLM）和掩码图像建模（MIM）。与其他模型在使用MLM时使用文本输入不同（Devlin,
    [2018](#bib.bib23)），一部分文本区域会随机用RGB值$[255,255,255]$进行掩码，并通过2层MLP解码器预测掩码令牌。MIM掩盖矩形文本区域，并预测缺失像素的RGB值，以改善文档表示。除了全局平均池化的FPN融合视觉表示外，每个文本区域的MLM生成的隐藏状态被串联起来，并输入到一个全卷积纽约中，以获取回归的掩码缺失像素值。
- en: 4.3.3\. LLM-based Frameworks
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 基于LLM的框架
- en: HRVDA (Liu et al., [2024b](#bib.bib71)) aims to propose a MLLM accepting high-resolution
    image inputs to conduct fine-grained information extraction from VRDs. A swin-transformer
    (Liu et al., [2021](#bib.bib76)) is used to encode document images into image
    patch tokens. A pluggable content detector then identifies visual tokens that
    contain relevant document content information. Following this, a content filtering
    mechanism performs token pruning to remove irrelevant tokens. The remaining encoded
    visual tokens are processed through an MLP to ensure consistency with the LLM
    embedding space dimensions. These pruned tokens are then fused with instruction
    features, allowing further filtering of tokens irrelevant to the instructions.
    The final streamlined set of visual tokens and instructions is fed into the LLM,
    which generates the corresponding responses.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: HRVDA (Liu et al., [2024b](#bib.bib71)) 旨在提出一个接受高分辨率图像输入的MLLM，以从VRDs中进行细粒度信息提取。使用了swin-transformer
    (Liu et al., [2021](#bib.bib76)) 来将文档图像编码成图像块令牌。然后，一个可插拔的内容检测器识别包含相关文档内容信息的视觉令牌。随后，内容过滤机制执行令牌修剪，以去除不相关的令牌。剩余的编码视觉令牌通过MLP进行处理，以确保与LLM嵌入空间维度的一致性。这些修剪过的令牌随后与指令特征融合，从而进一步过滤与指令无关的令牌。最终精简的视觉令牌和指令集被输入LLM，生成相应的响应。
- en: 'LayoutLLM (Luo et al., [2024](#bib.bib79)) introduces an LLM/MLLM-based approach
    integrated with a pretrained document understanding model to address the challenges
    of applying LLMs to zero-shot document understanding tasks. The input document’s
    visual, textual, and layout information and any question text are encoded by a
    pretrained LayoutLMv3 (Huang et al., [2022](#bib.bib43)) encoder and projected
    into the same embedding space as the adopted LLM, Vicuna-7B-v1.5 (Zheng et al.,
    [2024](#bib.bib149)). The method incorporates layout-aware pretraining tasks at
    three levels: document-level (e.g., document summarization), region-level (e.g.,
    layout analysis), and segment-level (e.g., MVLM). These tasks enable the model
    to achieve comprehensive document understanding. Additionally, a novel module
    called LayoutCoT is designed to help LayoutLLM focus on question-relevant regions
    and generate accurate answers through intermediate steps. GPT-3.5-turbo (OpenAI,
    [2023](#bib.bib90)) is used to prepare the dataset for document summarization
    training and to construct LayoutCoT training data.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLLM (Luo et al., [2024](#bib.bib79)) 引入了一种基于LLM/MLLM的方法，并结合了预训练的文档理解模型，以应对将LLM应用于零-shot文档理解任务的挑战。输入文档的视觉、文本和布局信息以及任何问题文本都由预训练的LayoutLMv3
    (Huang et al., [2022](#bib.bib43)) 编码器进行编码，并映射到与采用的LLM Vicuna-7B-v1.5 (Zheng et
    al., [2024](#bib.bib149)) 相同的嵌入空间。该方法在三个层级中融入了布局感知的预训练任务：文档级（例如，文档摘要），区域级（例如，布局分析），和分段级（例如，MVLM）。这些任务使得模型能够实现全面的文档理解。此外，一个名为LayoutCoT的新模块被设计出来，以帮助LayoutLLM专注于与问题相关的区域，并通过中间步骤生成准确的答案。GPT-3.5-turbo
    (OpenAI, [2023](#bib.bib90)) 被用来准备文档摘要训练的数据集，并构建LayoutCoT训练数据。
- en: 4.4\. Non-Pretrained Frameworks
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4. 非预训练框架
- en: CALM (Du et al., [2022](#bib.bib30)) introduces a common-sense augment document
    understanding framework to understand the query and extrapolate answers not contained
    in the context of the input document image. They follow LayoutLMv2 (Xu et al.,
    [2020b](#bib.bib138)) to encode input document multimodal representations. The
    textual token embeddings are fed into a Document Purifier component to merge the
    tokens $\{t_{1},\dots,t_{n}\}$ belonging to one entity type $N$ to one Upper Layer
    token $\hat{c}$ by applying average pooling of $\hat{c}=AvePool(t_{1},\dots,t_{n})$.
    Each Upper Layer token is concatenated with the commence augmented on ConceptNet
    NumberBatch (Speer et al., [2017](#bib.bib111)) entity word vector $c^{\prime}$
    to get the final entity representation $c=concat(\hat{c},c^{\prime})$. A similar
    Question-Purifier is applied to use common-sense knowledge to enhance the question
    representation. Then, with the assistance of ConceptNet, relevant common-sense
    knowledge is recalled based on the common-sense representation of both documents
    and queries. By considering the predicted question-answer relationship, a final
    self-attentive graph convolutional network following (Zhu et al., [2021b](#bib.bib153))
    is proposed to address document reasoning tasks more effectively.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: CALM (Du et al., [2022](#bib.bib30)) 引入了一个常识增强的文档理解框架，用于理解查询并推断不在输入文档图像上下文中的答案。他们遵循
    LayoutLMv2 (Xu et al., [2020b](#bib.bib138)) 对输入文档的多模态表示进行编码。文本标记嵌入被送入一个文档净化器组件，通过对
    $\{t_{1},\dots,t_{n}\}$ 的平均池化 $\hat{c}=AvePool(t_{1},\dots,t_{n})$，将属于同一实体类型 $N$
    的标记合并为一个上层标记 $\hat{c}$。每个上层标记与基于 ConceptNet NumberBatch (Speer et al., [2017](#bib.bib111))
    实体词向量 $c^{\prime}$ 进行拼接，以获得最终的实体表示 $c=concat(\hat{c},c^{\prime})$。类似的 Question-Purifier
    被应用于利用常识知识增强问题表示。然后，在 ConceptNet 的协助下，基于文档和查询的常识表示召回相关的常识知识。通过考虑预测的问题-答案关系，提出了一个最终的自注意图卷积网络，遵循
    (Zhu et al., [2021b](#bib.bib153))，以更有效地解决文档推理任务。
- en: LayoutGCN (Shi et al., [2023](#bib.bib107)) proposes a lightweight and effective
    model which contains a fully connected graph where text blocks are nodes and edges
    connect every two blocks. The model architecture includes a TextCNN-based (Kim,
    [2014](#bib.bib54)) encoder to encode N-gram textual embeddings, a linear trainable
    layout encoder to project the normalised bbox coordinates into hyperspace following
    other layout-aware models, and a visual encoder (CSP-Darknet (Wang et al., [2020a](#bib.bib122))
    for document image features). These features are integrated using a Graph Convolution
    Network (GCN) to capture relationships between nodes. The final node representation
    combines text, layout, and visual information, benefiting various VRDU tasks.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutGCN (Shi et al., [2023](#bib.bib107)) 提出了一个轻量且高效的模型，该模型包含一个完全连接的图，其中文本块为节点，边连接每两个块。模型架构包括一个基于
    TextCNN (Kim, [2014](#bib.bib54)) 的编码器，用于编码 N-gram 文本嵌入，一个线性可训练布局编码器，将标准化的 bbox
    坐标投射到超空间中，遵循其他布局感知模型，以及一个视觉编码器（CSP-Darknet (Wang et al., [2020a](#bib.bib122))
    用于文档图像特征）。这些特征通过图卷积网络 (GCN) 进行整合，以捕捉节点之间的关系。最终的节点表示结合了文本、布局和视觉信息，适用于各种 VRDU 任务。
- en: 'XYLayoutLM (Gu et al., [2022](#bib.bib34)) introduces an Augmented XY Cut module
    to correct the improper reading order generated by OCR engines and a Dialted Conditional
    Position Encoding module to handle the variable lengths of input text and image
    tokens. LayoutXLM (Xu et al., [2021](#bib.bib137)) ⁶⁶6Please refer to Section [4.1.2](#S4.SS1.SSS2
    "4.1.2\. Visual Integrated Models ‣ 4.1\. Fine-grained Pretrained Models ‣ 4\.
    Multi-Task VRD Understanding Models ‣ Deep Learning based Visually Rich Document
    Content Understanding: A Survey") to see a detailed description of the LayoutXLM
    model. is adopted as a basic framework to process multimodal inputs. The Augmented
    XY Cut module enhances traditional XY Cut (Nagy and Seth, [1984](#bib.bib87))
    by incorporating thresholds ($\lambda_{x},\lambda_{y}$) and a shift factor ($\theta$)
    to adjust box positions on the x and y axis based on randomly generated values.
    It recursively divides token boxes using valleys in horizontal and vertical projection
    profiles, forming clusters in descending order. Each cluster’s reading order is
    determined recursively, prioritizing divisions until no significant valleys remain,
    ensuring a proper reading order is derived from an XY Tree structure. DCPE (Dilated
    Conditional Position Encoding) addresses limitations of CPE (Chu et al., [2022](#bib.bib19))
    in multimodal tasks by separately processing textual and visual features. It employs
    1D convolutions for textual tokens to extract 1D local layouts, accommodating
    their inherent 1D relationships. Additionally, DCPE utilizes dilated convolutions
    (Yu and Koltun, [2015](#bib.bib141)) to capture long-range dependencies effectively
    without increasing model complexity, thereby enhancing performance in multimodal
    document understanding tasks.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: XYLayoutLM (Gu et al., [2022](#bib.bib34)) 引入了增强型 XY 切割模块，以纠正 OCR 引擎生成的不正确阅读顺序，以及扩张条件位置编码模块，以处理输入文本和图像令牌的可变长度。LayoutXLM
    (Xu et al., [2021](#bib.bib137)) ⁶⁶6请参考第 [4.1.2](#S4.SS1.SSS2 "4.1.2\. 视觉集成模型
    ‣ 4.1\. 细粒度预训练模型 ‣ 4\. 多任务 VRD 理解模型 ‣ 基于深度学习的视觉丰富文档内容理解：综述") 节以查看 LayoutXLM 模型的详细描述。被采用为处理多模态输入的基础框架。增强型
    XY 切割模块通过引入阈值 ($\lambda_{x},\lambda_{y}$) 和一个平移因子 ($\theta$) 来调整 x 轴和 y 轴上的框位置，基于随机生成的值来改进传统的
    XY 切割 (Nagy and Seth, [1984](#bib.bib87))。它通过水平和垂直投影轮廓中的谷值递归地划分令牌框，形成按降序排列的聚类。每个聚类的阅读顺序通过递归确定，优先进行分割直到没有显著的谷值，从而确保从
    XY 树结构中得出正确的阅读顺序。DCPE（扩张条件位置编码）通过分别处理文本和视觉特征，解决了 CPE（Chu et al., [2022](#bib.bib19)）在多模态任务中的局限性。它使用
    1D 卷积提取文本令牌的 1D 局部布局，以适应其固有的 1D 关系。此外，DCPE 利用扩张卷积 (Yu and Koltun, [2015](#bib.bib141))
    有效捕捉长距离依赖关系而不增加模型复杂性，从而提升了多模态文档理解任务中的性能。
- en: 4.5\. Summary of Multi-Task Frameworks
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 多任务框架总结
- en: Various models are proposed to enhance document representations for VRDU tasks
    by leveraging pretrained language models to enrich text token sequences with layout
    information through positional encoding (Xu et al., [2020a](#bib.bib136)), attention
    mechanisms (Wang et al., [2022b](#bib.bib123)), and layout-aware tasks (Tu et al.,
    [2023](#bib.bib119)). However, VRDs contain rich visual details like font, texture,
    and colour and visually complex entities such as tables, charts, and photos. Many
    models (Xu et al., [2020b](#bib.bib138); Huang et al., [2022](#bib.bib43); Appalaraju
    et al., [2021](#bib.bib2)) integrate visual cues to enhance fine-grained document
    features, but their quadratic time and space complexity pose challenges for handling
    long sequences in multi-page document understanding (Ding et al., [2024a](#bib.bib27)).
    Fine-grained models excel but struggle with capturing layout and structural details
    from document images. Coarse-grained frameworks (Li et al., [2021b](#bib.bib66);
    Gu et al., [2021](#bib.bib33)) mitigate fine-grained limitations by leveraging
    entity-level multimodal information, yet compressing diverse entity aspects into
    a single dense vector risks losing information (Ding et al., [2024b](#bib.bib28)).
    Joint-grained frameworks (Li et al., [2021c](#bib.bib68); Zhai et al., [2023](#bib.bib144);
    Wang et al., [2022a](#bib.bib127); Bai et al., [2022](#bib.bib4); Luo et al.,
    [2023](#bib.bib78)) integrate multi-grained information to produce comprehensive
    representations. Non-pretrained models leverage external knowledge (Du et al.,
    [2022](#bib.bib30)) or lightweight networks (Shi et al., [2023](#bib.bib107))
    to rival large-scale pretrained frameworks’ performance. Most document understanding
    models (Xu et al., [2020a](#bib.bib136); Huang et al., [2022](#bib.bib43); Wang
    et al., [2022b](#bib.bib123); Li et al., [2021b](#bib.bib66)) rely on off-the-shelf
    OCR tools for text extraction, which can be susceptible to OCR quality issues
    and incorrect reading orders. OCR-free frameworks directly process document images
    to mitigate these limitations; However, these frameworks may exhibit sub-optimal
    performance compared to methods using established OCR tools with additional resource
    consumption.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 各种模型被提出，以通过利用预训练语言模型，利用位置编码（Xu et al., [2020a](#bib.bib136)）、注意机制（Wang et al.,
    [2022b](#bib.bib123)）和布局感知任务（Tu et al., [2023](#bib.bib119)）来增强 VRDU 任务的文档表示。然而，VRD
    包含丰富的视觉细节，如字体、纹理和颜色，以及视觉上复杂的实体，如表格、图表和照片。许多模型（Xu et al., [2020b](#bib.bib138)；Huang
    et al., [2022](#bib.bib43)；Appalaraju et al., [2021](#bib.bib2)）结合了视觉线索，以增强细粒度文档特征，但其二次时间和空间复杂性给处理多页文档理解中的长序列带来了挑战（Ding
    et al., [2024a](#bib.bib27)）。细粒度模型表现出色，但在捕捉文档图像的布局和结构细节方面存在困难。粗粒度框架（Li et al.,
    [2021b](#bib.bib66)；Gu et al., [2021](#bib.bib33)）通过利用实体级多模态信息来缓解细粒度的局限性，但将多样化的实体方面压缩成单一的密集向量存在信息丢失的风险（Ding
    et al., [2024b](#bib.bib28)）。联合粒度框架（Li et al., [2021c](#bib.bib68)；Zhai et al.,
    [2023](#bib.bib144)；Wang et al., [2022a](#bib.bib127)；Bai et al., [2022](#bib.bib4)；Luo
    et al., [2023](#bib.bib78)）整合多粒度信息，以生成综合表示。非预训练模型利用外部知识（Du et al., [2022](#bib.bib30)）或轻量级网络（Shi
    et al., [2023](#bib.bib107)）来与大规模预训练框架的性能相抗衡。大多数文档理解模型（Xu et al., [2020a](#bib.bib136)；Huang
    et al., [2022](#bib.bib43)；Wang et al., [2022b](#bib.bib123)；Li et al., [2021b](#bib.bib66)）依赖现成的
    OCR 工具进行文本提取，这可能受到 OCR 质量问题和读取顺序不正确的影响。无 OCR 框架直接处理文档图像以减轻这些限制；然而，与使用已建立 OCR 工具的方法相比，这些框架可能表现出亚最优性能，并且会额外消耗资源。
- en: 5\. Visually Rich Document Content Understanding Datasets
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 视觉丰富文档内容理解数据集
- en: 5.1\. Key Information Extraction and Entity Linking
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 关键信息提取和实体链接
- en: 'Name Conf./J. Year Domain # Docs # Images # Keys MP. Language Metrics Format
    FUNSD ICDAR-w 2019 Multi-source N/A 199 4 N English F1 P.& H. SROIE ICDAR-c 2019
    Scanned Receipts N/A 973 4 N English F1* P. CORD Neurips-w 2019 Scanned Receipts
    N/A 1,000 54 N English F1 P. Payment-Invoice ACL 2020 Invoice Form N/A 14,237+595
    7 N English F1 D. Payment-Receipts ACL 2020 Scanned Receipts N/A 478 2 N English
    F1 P. Kleister-NDA ICDAR 2021 Private Agreements 540 3,229 4 Y English F1 D. Kleister-Charity
    ICDAR 2021 AFR 2,778 61,643 8 Y English F1 D.& P. EPHOIE AAAI 2021 Exam Paper
    N/A 1,494 10 N Chinese F1 P.& H. XFUND ACL 2022 Synthetic Forms N/A 1,393 4 N
    Multilingual F1 D.& P.& H. Form-NLU SIGIR 2023 Financial Form N/A 857 12 N English
    F1 D.& P.& H. VRDU-Regist. Form KDD 2023 Registration Form N/A 1,915 6 N English
    F1 D. VRDU-Ad-buy Form KDD 2023 Political Invoice Form N/A 641 9+1(5) N English
    F1 D.&P. DocILE ICDAR 2023 Invoice Form 6,680 106,680 55 Y English AP, CLEval
    D.&P.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 'Name Conf./J. Year Domain # Docs # Images # Keys MP. Language Metrics Format
    FUNSD ICDAR-w 2019 多源数据集 N/A 199 4 N 英语 F1 P.& H. SROIE ICDAR-c 2019 扫描收据 N/A
    973 4 N 英语 F1* P. CORD Neurips-w 2019 扫描收据 N/A 1,000 54 N 英语 F1 P. Payment-Invoice
    ACL 2020 发票表单 N/A 14,237+595 7 N 英语 F1 D. Payment-Receipts ACL 2020 扫描收据 N/A 478
    2 N 英语 F1 P. Kleister-NDA ICDAR 2021 私人协议 540 3,229 4 Y 英语 F1 D. Kleister-Charity
    ICDAR 2021 AFR 2,778 61,643 8 Y 英语 F1 D.& P. EPHOIE AAAI 2021 考试试卷 N/A 1,494 10
    N 中文 F1 P.& H. XFUND ACL 2022 合成表单 N/A 1,393 4 N 多语言 F1 D.& P.& H. Form-NLU SIGIR
    2023 财务表单 N/A 857 12 N 英语 F1 D.& P.& H. VRDU-Regist. Form KDD 2023 注册表单 N/A 1,915
    6 N 英语 F1 D. VRDU-Ad-buy Form KDD 2023 政治发票表单 N/A 641 9+1(5) N 英语 F1 D.&P. DocILE
    ICDAR 2023 发票表单 6,680 106,680 55 Y 英语 AP, CLEval D.&P.'
- en: Table 1\. Summary of visually rich document key information extraction datasets.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 视觉丰富文档关键信息提取数据集总结。
- en: 5.1.1\. Scanned Receipt Datasets
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 扫描收据数据集
- en: 'SROIE (Huang et al., [2019](#bib.bib44)) is a widely used dataset for text
    localization, OCR, and key information extraction from scanned receipts, introduced
    in the ICDAR 2019 Challenge on ”Scanned Receipts OCR and Key Information Extraction”
    The Key Information Extraction (KIE) task focuses on four key types: Address,
    Date, Company, and Total, with corresponding values provided in the annotation
    file for each receipt. The F1 score for this task is calculated based on Mean
    Average Precision (MAP) and recall. Note that entity-level annotations are not
    provided in the official dataset, requiring the use of external tools to obtain
    the information for coarse-grained or joint-grained models.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: SROIE（Huang 等，[2019](#bib.bib44)）是一个广泛使用的数据集，用于文本定位、OCR和从扫描收据中提取关键信息，在ICDAR
    2019挑战赛“扫描收据OCR和关键信息提取”中引入。关键数据提取（KIE）任务关注四种关键类型：地址、日期、公司和总计，并在每个收据的注释文件中提供相应的值。此任务的F1分数基于平均精度均值（MAP）和召回率计算。请注意，官方数据集中未提供实体级注释，因此需要使用外部工具来获取粗粒度或联合粒度模型所需的信息。
- en: Payment-Receipts (Majumder et al., [2020b](#bib.bib82)) is a subset of SROIE,
    created by sampling up to 5 documents from each template in the original SROIE
    dataset. The template of each receipt is decided by the Company annotation. The
    target schema focuses on extracting only Date and Total. This subset is used to
    evaluate the model’s ability to handle unseen templates.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Payment-Receipts（Majumder 等，[2020b](#bib.bib82)）是SROIE的一个子集，通过从原始SROIE数据集中每个模板中采样最多5个文档创建。每个收据的模板由公司注释决定。目标模式专注于提取仅日期和总计。该子集用于评估模型处理未见过的模板的能力。
- en: CORD (Park et al., [2019](#bib.bib93)) is a widely used dataset for post-OCR
    receipt understanding, featuring two-level labels annotated by crowdsourcing workers.
    It includes eight superclasses, such as Store, Payment, Menu, Subtotal, and Total,
    each with several subclasses. For example, Store contains subclasses like Name,
    Address, and Telephone. CORD provides both textline-level and word-level annotations
    for both fine-grained and coarse-grained frameworks, with some sensitive information
    blurred. All models are evaluated on the released first 1,000 samples.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: CORD（Park 等，[2019](#bib.bib93)）是一个广泛使用的数据集，用于后OCR收据理解，具有由众包工作者注释的两级标签。它包括八个超级类，如商店、支付、菜单、小计和总计，每个超级类都有几个子类。例如，商店包含名称、地址和电话等子类。CORD为细粒度和粗粒度框架提供了文本行级和单词级注释，其中一些敏感信息已被模糊处理。所有模型都在发布的前1,000个样本上进行评估。
- en: 5.1.2\. Form-style Datasets
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 表单样式数据集
- en: FUNSD (Jaume et al., [2019](#bib.bib47)) is derived from the RVL-CDIP dataset
    (Harley et al., [2015](#bib.bib37)) by manually selecting 199 readable and diverse
    template form images. The dataset is annotated using the GuiZero library to provide
    both entity and word-level annotations, including manual text recognition. Semantic
    links indicate relationships between entities, such as Question-Answer or Header-Question
    pairs. Consequently, FUNSD supports key information extraction, OCR, and entity
    linking tasks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: FUNSD (Jaume et al., [2019](#bib.bib47)) 来源于 RVL-CDIP 数据集 (Harley et al., [2015](#bib.bib37))，通过手动选择
    199 张可读且多样的模板表单图像。该数据集使用 GuiZero 库进行标注，提供了实体级和词级的标注，包括手动文本识别。语义链接表示实体之间的关系，例如问题-答案或标题-问题对。因此，FUNSD
    支持关键数据提取、OCR 和实体链接任务。
- en: XFUND (Xu et al., [2021](#bib.bib137)) is the first multilingual dataset following
    the FUNSD format. It collects form templates in seven languages (Chinese, Japanese,
    Spanish, French, Italian, German, and Portuguese) from the internet. Human annotators
    fill these templates with synthetic information by typing or handwriting, ensuring
    each template is used only once. The filled forms are then scanned into document
    images, processed with OCR, and annotated with key-value pairs. Each language
    has 199 annotated forms, supporting multilingual key information extraction and
    entity linking tasks.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: XFUND (Xu et al., [2021](#bib.bib137)) 是首个遵循 FUNSD 格式的多语言数据集。它从互联网上收集了七种语言（中文、日文、西班牙文、法文、意大利文、德文和葡萄牙文）的表单模板。人工标注员通过输入或手写的方式填充这些模板，确保每个模板仅使用一次。填写完的表单随后被扫描成文档图像，经过
    OCR 处理，并标注了键值对。每种语言都有 199 个标注过的表单，支持多语言关键数据提取和实体链接任务。
- en: Payment-Invoice (Majumder et al., [2020b](#bib.bib82)) contains two corpora
    of invoices from different sources. The first corpus, Invoice 1, includes 14,273
    invoices from various vendors with different template styles, used for training
    and validation. The second corpus, Invoice 2, comprises 595 documents with distinct
    templates not found in Invoice 1, serving as the test set. Human annotators extract
    six required keys from each single-page invoice, such as Invoice Date, Total Amount,
    and Tax Amount. This dataset is suitable for evaluating generative-style models
    and fine-grained sequence labeling models. For coarse-grained models, additional
    text line or entity-level information can be extracted using off-the-shelf tools.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Payment-Invoice (Majumder et al., [2020b](#bib.bib82)) 包含来自不同来源的两种发票语料库。第一个语料库，Invoice
    1，包含来自各种供应商的 14,273 张具有不同模板样式的发票，用于训练和验证。第二个语料库，Invoice 2，由 595 份具有与 Invoice 1
    不同的模板的文档组成，作为测试集。人工标注员从每张单页发票中提取六个必要的键，如发票日期、总金额和税额。该数据集适用于评估生成式模型和细粒度序列标注模型。对于粗粒度模型，可以使用现成工具提取额外的文本行或实体级信息。
- en: 'VRDU-Registration Form (Wang et al., [2023b](#bib.bib132)) is a dataset of
    registration forms about foreign agents registering with the US government collected
    from the Federal Communications Commission. Commercial OCR tools extract the text
    content of the forms. Annotators draw bounding boxes around six unrepeated entities
    (each entity appears only once per document) per document: File Date, Foreign
    Principal Name, Registrant Name, Registration ID, Signer Name, and Signer Title.
    The dataset provides entity-level annotations, which can be easily preprocessed
    to acquire token-level annotations, supporting any granularity of Key Information
    Extraction (KIE) models.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: VRDU-Registration Form (Wang et al., [2023b](#bib.bib132)) 是一个有关外国代理人向美国政府注册的注册表单数据集，收集自联邦通信委员会。商业
    OCR 工具提取表单的文本内容。标注员在每个文档中绘制六个不重复实体的边界框（每个实体在文档中仅出现一次）：文件日期、外国主要负责人名称、注册人名称、注册
    ID、签署人名称和签署人职称。该数据集提供了实体级标注，可以很容易地预处理为 token 级标注，支持任何粒度的关键数据提取（KIE）模型。
- en: VRDU-Ad-buy Form (Wang et al., [2023b](#bib.bib132)) consists of 641 invoices
    or receipts signed between TV stations and campaign groups for political advertisements.
    It follows the same annotation procedure as the VRDU-Registration Form but involves
    a more complex schema. This includes nine unique entities (e.g., Advertiser, Agency,
    Contract ID), four repeated entities (e.g., Item Description, Sub Prices), and
    hierarchical entities (e.g., Line Item). Repeated entities may contain different
    values within a single document, while hierarchical entities comprise several
    repeated entities as components.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: VRDU-Ad-buy 表单（Wang 等，[2023b](#bib.bib132)）由641份电视台与竞选团体之间签署的政治广告发票或收据组成。它遵循与
    VRDU-Registration Form 相同的注释程序，但涉及更复杂的架构。这包括九个独特的实体（如**广告商**、**代理商**、**合同编号**）、四个重复的实体（如**项目描述**、**子价格**）以及层级实体（如**行项目**）。重复的实体可能在单一文档中包含不同的值，而层级实体则由多个重复的实体作为组件组成。
- en: Form-NLU (Ding et al., [2023a](#bib.bib25)) is a visual-linguistics dataset
    designed to support researchers in interpreting specific designer intentions amidst
    various types of noise from different form carriers, including digital, printed,
    and handwritten forms. Fine-grained key-value pairs, such as Company Name, Previous
    Notice Date, and Previous Shares, are manually annotated. The training and validation
    set comprises 535 digital-born forms, with 76 reserved for validation. Additionally,
    three test sets are provided, containing 146 digital, 50 printed, and 50 handwritten
    form images, respectively. Form-NLU can be used to evaluate form layout analysis
    and Key Information Extraction (KIE) models of any granularity. With proper processing,
    it can also be used to evaluate entity linking frameworks, thanks to the well-annotated
    key-value pairs.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Form-NLU（Ding 等，[2023a](#bib.bib25)）是一个视觉语言数据集，旨在支持研究人员在各种表单承载体（包括数字、打印和手写表单）中解读特定设计师意图。精细的关键值对，如**公司名称**、**之前通知日期**和**之前股份**，经过手动注释。训练和验证集包括535份数字原生表单，其中76份保留用于验证。此外，还提供了三个测试集，分别包含146份数字表单、50份打印表单和50份手写表单图像。Form-NLU
    可以用于评估任何粒度的表单布局分析和**关键信息提取（KIE）**模型。通过适当处理，它也可以用于评估实体链接框架，得益于精细标注的关键值对。
- en: 5.1.3\. Multi-page Datasets
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 多页数据集
- en: Kleister-NDA (Stanisławek et al., [2021](#bib.bib112)) is a dataset collected
    from the Electronic Data Gathering, Analysis, and Retrieval System (EDGAR) focusing
    on Non-disclosure Agreements (NDAs). During preprocessing, the collected 540 HTML
    files are converted into digital multi-page PDF files (totalling 3,229 pages)
    using the Puppeteer library. Four key items, Effective Date, Party, Jurisdiction,
    and Term, are manually annotated by three annotators to extract the corresponding
    values. The NDA dataset is widely used by fine-grained level models but may require
    additional processing for frameworks with limited sequence length due to the multi-page
    inputs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Kleister-NDA（Stanisławek 等，[2021](#bib.bib112)）是从电子数据收集、分析和检索系统（EDGAR）收集的数据集，重点关注**保密协议（NDAs）**。在预处理过程中，收集的540个
    HTML 文件使用 Puppeteer 库转换为数字多页 PDF 文件（总计3,229页）。四个关键项目，即**生效日期**、**方**、**司法管辖区**和**期限**，由三名标注者手动注释以提取相应的值。NDA
    数据集被细粒度模型广泛使用，但由于多页输入，可能需要额外处理以适应有限序列长度的框架。
- en: Kleister-Charity (Stanisławek et al., [2021](#bib.bib112)) contains 2,778 annual
    financial reports from the Charity Commission, which lack strict formatting rules.
    The Charity Commission website provides eight key pieces of information, such
    as Postcode, Charity Name, and Report Date. Annotators manually correct minor
    errors to ensure accuracy. Compared to Kleister-NDA, the Charity dataset has longer
    document inputs, totalling 61,643 pages, requiring models to handle long sequence
    outputs. Both Charity and NDA datasets provide only key-value pair annotations,
    making them suitable for the generation and fine-grained sequence labelling tasks
    but requiring additional processing to acquire entity-level annotations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Kleister-Charity（Stanisławek 等，[2021](#bib.bib112)）包含来自慈善委员会的2,778份年度财务报告，这些报告缺乏严格的格式化规则。慈善委员会网站提供了八个关键信息，如**邮政编码**、**慈善名称**和**报告日期**。标注者手动修正轻微错误以确保准确性。与
    Kleister-NDA 相比，慈善数据集的文档输入更长，总计61,643页，要求模型处理长序列输出。慈善和 NDA 数据集仅提供关键值对注释，使其适用于生成和精细序列标注任务，但需要额外处理以获取实体级注释。
- en: 'DocILE (Šimsa et al., [2023](#bib.bib108)) comprises three subsets: an annotated
    set of 6,680 real business documents, an unlabeled set of 932,000 real business
    documents for unsupervised pretraining, and a synthetic set of 100,000 documents
    generated with full task labels. Documents come from public sources like the UCSF
    Industry Documents Library and Public Inspection Files, with annotations for Key
    Information Localization and Extraction and Line Item Recognition. Synthetic documents
    were created using annotated templates and a rule-based synthesizer. DocILE provides
    entity-level annotations that can be easily post-processed to acquire token-level
    annotations.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: DocILE (Šimsa et al., [2023](#bib.bib108)) 包含三个子集：一个标注的6,680个真实商业文档的集合，一个未标记的932,000个真实商业文档的集合用于无监督预训练，以及一个100,000个生成文档的合成集合，具有完整的任务标签。文档来自公共来源，如UCSF行业文档库和公共检查文件，带有关键内容定位和提取及行项识别的注释。合成文档使用标注模板和基于规则的合成器创建。DocILE
    提供了可以轻松后处理以获得标记级别注释的实体级注释。
- en: 5.2\. Visual Question Answering
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 视觉问答
- en: '| Name | Conf./J. | Year | Domain | Lang. | # Docs | # Images | # Questions
    | Answer Type | MP | Format | Metrics | Annotation |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 会议/期刊 | 年份 | 领域 | 语言 | 文档数量 | 图像数量 | 问题数量 | 答案类型 | MP | 格式 | 评价指标 |
    注释 |'
- en: '| DocVQA | WACV | 2021 | Industrial Reports | English | N/A | 12,767 | 50,000
    | Text | N | D./P./H. | ANLS | Human |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| DocVQA | WACV | 2021 | 工业报告 | 英语 | N/A | 12,767 | 50,000 | 文本 | N | D./P./H.
    | ANLS | 人工 |'
- en: '| VisualMRC | AAAI | 2021 | Website | English | N/A | 10,197 | 30,562 | Text
    | N | D. | BLUE, etc | Human |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| VisualMRC | AAAI | 2021 | 网站 | 英语 | N/A | 10,197 | 30,562 | 文本 | N | D. |
    BLUE 等 | 人工 |'
- en: '| TAT-DQA | MM | 2022 | Financial Reports | English | 2,758 | 3,067 | 16,558
    | Text/RS-Gen. | Y | D. | EM, F1 | Human |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| TAT-DQA | MM | 2022 | 财务报告 | 英语 | 2,758 | 3,067 | 16,558 | 文本/RS-生成 | Y |
    D. | EM, F1 | 人工 |'
- en: '| RDVQA | MM | 2022 | Data Analysis Report | N/A | 8,362 | 8,514 | 41,378 |
    Text | N | D. | ANLS, ACC | Human |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| RDVQA | MM | 2022 | 数据分析报告 | N/A | 8,362 | 8,514 | 41,378 | 文本 | N | D. |
    ANLS, ACC | 人工 |'
- en: '| CS-DVQA | MM | 2022 | Industry Documents | English | N/A | 600 | 1,000 |
    Text and Nodes | N | D./P./H. | ANLS | Human |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| CS-DVQA | MM | 2022 | 行业文档 | 英语 | N/A | 600 | 1,000 | 文本和节点 | N | D./P./H.
    | ANLS | 人工 |'
- en: '| PDFVQA-Task A | ECML-PKDD | 2023 | Academic Paper | English | N/A | 12,337
    | 81,085 | Num or Yes/No | N | D. | F1 | Template |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| PDFVQA-Task A | ECML-PKDD | 2023 | 学术论文 | 英语 | N/A | 12,337 | 81,085 | 数值或是/否
    | N | D. | F1 | 模板 |'
- en: '| PDFVQA-Task B | ECML-PKDD | 2023 | Academic Paper | English | N/A | 12,337
    | 53,872 | Entity | N | D. | F1 | Template |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| PDFVQA-Task B | ECML-PKDD | 2023 | 学术论文 | 英语 | N/A | 12,337 | 53,872 | 实体
    | N | D. | F1 | 模板 |'
- en: '| PDFVQA-Task C | ECML-PKDD | 2023 | Academic Paper | English | 1,147 | 12,337
    | 5,653 | Entity | Y | D. | EM | Template |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| PDFVQA-Task C | ECML-PKDD | 2023 | 学术论文 | 英语 | 1,147 | 12,337 | 5,653 | 实体
    | Y | D. | EM | 模板 |'
- en: '| MPDocVQA | PR | 2023 | Industrial Reports | English | 6,000 | 48,000 | 46,000
    | Text | Y | D./P./H. | ANLS | Human |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| MPDocVQA | PR | 2023 | 工业报告 | 英语 | 6,000 | 48,000 | 46,000 | 文本 | Y | D./P./H.
    | ANLS | 人工 |'
- en: '| DUDE | ICCV | 2023 | Cross-domain | English | 5,019 | 28,709 | 41,541 | Text,
    Yes/No | Y | D. | ANLS | Human |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| DUDE | ICCV | 2023 | 跨领域 | 英语 | 5,019 | 28,709 | 41,541 | 文本，是/否 | Y | D.
    | ANLS | 人工 |'
- en: '| MMVQA | IJCAI | 2024 | Academic Paper | English | 3,146 | 30,239 | 262,928
    | Entity | Y | D. | EM, PM, MR | LLM + Human |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| MMVQA | IJCAI | 2024 | 学术论文 | 英语 | 3,146 | 30,239 | 262,928 | 实体 | Y | D.
    | EM, PM, MR | LLM + 人工 |'
- en: Table 2\. Summary of visually rich document question answering datasets
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 视觉丰富文档问答数据集汇总
- en: 5.2.1\. Single Page VRD-QA Datasets
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 单页 VRD-QA 数据集
- en: DocVQA (Mathew et al., [2021](#bib.bib84)) is a pioneering dataset in document-based
    Visual Question Answering (VQA), sourced from the UCSF Industry Document Library.
    It comprises 50,000 manually generated questions framed on 12,767 document images,
    encompassing digital, printed, and handwritten formats. The dataset follows an
    extractive-style QA format similar to benchmarks like SQuAD (Rajpurkar et al.,
    [2016b](#bib.bib100)) and VQA (Biten et al., [2019b](#bib.bib8)). Evaluation typically
    involves fine-grained models such as LayoutLM variants (Xu et al., [2020a](#bib.bib136),
    [b](#bib.bib138); Huang et al., [2022](#bib.bib43)), LiLT (Wang et al., [2022b](#bib.bib123)),
    and generative models (Tang et al., [2022](#bib.bib116); Kim et al., [2022](#bib.bib52)),
    using metrics like Average Normalized Levenshtein Similarity (ANLS) (Biten et al.,
    [2019a](#bib.bib7)). However, it requires additional processing for coarse-grained
    models like SelfDoc (Li et al., [2021b](#bib.bib66)). CS-DVQA (Du et al., [2022](#bib.bib30))
    builds upon DocVQA by enhancing QA pairs to better reflect real-world requirements.
    It extracts 600 images from the DocVQA dataset and generates 1,000 QA pairs under
    human supervision. During question generation, it incorporates common-sense knowledge
    from real life, expanding answers beyond extractive in-line text to include question-related
    nodes (Nodes) sourced from ConceptNet (Speer et al., [2017](#bib.bib111)).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: DocVQA（Mathew 等，[2021](#bib.bib84)）是文档基础的视觉问答（VQA）领域的开创性数据集，来源于UCSF Industry
    Document Library。它包含50,000个手动生成的问题，基于12,767张文档图像，涵盖数字格式、打印格式和手写格式。该数据集遵循类似于SQuAD（Rajpurkar
    等，[2016b](#bib.bib100)）和VQA（Biten 等，[2019b](#bib.bib8)）的抽取式问答格式。评估通常涉及细粒度模型，如LayoutLM变体（Xu
    等，[2020a](#bib.bib136)，[b](#bib.bib138)；Huang 等，[2022](#bib.bib43)）、LiLT（Wang
    等，[2022b](#bib.bib123)）和生成模型（Tang 等，[2022](#bib.bib116)；Kim 等，[2022](#bib.bib52)），使用如平均标准化Levenshtein相似度（ANLS）（Biten
    等，[2019a](#bib.bib7)）等指标。然而，对于粗粒度模型，如SelfDoc（Li 等，[2021b](#bib.bib66)），则需要额外处理。CS-DVQA（Du
    等，[2022](#bib.bib30)）在DocVQA基础上进行改进，通过增强问答对以更好地反映现实世界需求。它从DocVQA数据集中提取了600张图像，并在人工监督下生成了1,000个问答对。在问题生成过程中，它结合了现实生活中的常识知识，将答案扩展到除了提取的内联文本外，还包括从ConceptNet（Speer
    等，[2017](#bib.bib111)）获取的与问题相关的节点（Nodes）。
- en: VisualMRC (Tanaka et al., [2021](#bib.bib115)) is compiled from website screenshots
    across 35 domains, carefully selected to exclude pages with handwritten content
    and to prefer pages containing short text (no more than 2 to 3 paragraphs). Unlike
    other datasets that might only provide question-answer annotations (Mathew et al.,
    [2021](#bib.bib84)) or automatically acquire document semantic entities (Ding
    et al., [2023b](#bib.bib26)), VisualMRC includes manually annotated layout structures
    with fine-grained semantic entity types such as Heading, Paragraph, Subtitle,
    Picture, and Caption. Question-answer pairs are generated through crowdsourcing.
    Consequently, VisualMRC is well-suited for evaluating both fine-grained and coarse-grained-based
    QA frameworks, providing a rich resource for assessing the effectiveness of models
    in understanding and interpreting detailed document layouts and semantic entities.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: VisualMRC（Tanaka 等，[2021](#bib.bib115)）是从35个领域的网页截图中编制而成，经过精心挑选，以排除手写内容的页面，并优先选择包含简短文本（不超过2到3段）的页面。与其他数据集可能仅提供问答注释（Mathew
    等，[2021](#bib.bib84)）或自动获取文档语义实体（Ding 等，[2023b](#bib.bib26)）不同，VisualMRC包括手动标注的布局结构，具有如标题、段落、副标题、图片和说明等细粒度语义实体类型。问答对通过众包生成。因此，VisualMRC非常适合评估基于细粒度和粗粒度的QA框架，提供了一个丰富的资源用于评估模型在理解和解释详细文档布局和语义实体方面的效果。
- en: 'PDFVQA-Task A and Task B (Ding et al., [2023b](#bib.bib26)) form part of the
    first document VQA dataset from PubMed Central, focusing on content and structural
    understanding. This dataset includes three tasks: two for single-page documents
    (Tasks A and B) and one for multi-page documents (Task C). Task A evaluates the
    structural and spatial relationships within document images, with answers being
    either counts or Yes/No. Task B focuses on extracting document entities based
    on their logical and spatial configurations. The PDFVQA dataset provides only
    coarse-grained, entity-level annotations, necessitating further processing for
    models that require fine-grained analysis. This setup is ideal for testing models’
    capabilities in understanding the logical and spatial structures of document images.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: PDFVQA-任务 A 和任务 B（Ding 等，[2023b](#bib.bib26)）是来自 PubMed Central 的第一个文档 VQA 数据集的一部分，侧重于内容和结构理解。该数据集包括三个任务：两个针对单页文档（任务
    A 和 B），一个针对多页文档（任务 C）。任务 A 评估文档图像中的结构和空间关系，答案为计数或是/否。任务 B 侧重于根据文档实体的逻辑和空间配置进行提取。PDFVQA
    数据集仅提供粗粒度的实体级注释，需要进一步处理以适用于需要细粒度分析的模型。这种设置非常适合测试模型理解文档图像的逻辑和空间结构的能力。
- en: 5.2.2\. Multi-Page VRD-QA Datasets
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 多页 VRD-QA 数据集
- en: TAT-DQA (Zhu et al., [2022](#bib.bib151)), an extension of the TAT-QA (Zhu et al.,
    [2021a](#bib.bib152)) dataset, is developed with more complex natural document
    structures and an expanded set of manually corrected and generated question-answer
    pairs derived from business financial reports. Unlike other datasets that primarily
    focus on extractive or simple abstractive answers (such as counting or yes/no),
    TAT-DQA includes questions requiring arithmetic reasoning, where values must be
    extracted from tabular data and textual content for discrete calculations. This
    dataset adopts the evaluation metrics of TAT-QA, including Exact Matching and
    a numeracy-focused F1 score. These metrics are particularly tailored to assess
    the accuracy of arithmetic reasoning and data extraction capabilities of the models
    tested with TAT-DQA.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: TAT-DQA（Zhu 等，[2022](#bib.bib151)）是 TAT-QA（Zhu 等，[2021a](#bib.bib152)）数据集的扩展，开发了更复杂的自然文档结构，并扩展了手动修正和生成的问题-答案对，这些问题-答案对源于商业财务报告。与其他主要关注提取性或简单抽象性答案（如计数或是/否）的数据集不同，TAT-DQA
    包含需要算术推理的问题，其中值必须从表格数据和文本内容中提取，以进行离散计算。该数据集采用了 TAT-QA 的评估指标，包括精确匹配和以数值为重点的 F1
    分数。这些指标特别适用于评估使用 TAT-DQA 测试模型的算术推理和数据提取能力。
- en: RDVQA dataset (Wu et al., [2022](#bib.bib135)) compiles a large collection of
    conversational chats and associated images from an E-commerce platform. It employs
    standard OCR and Named Entity Recognition (NER) techniques to extract text and
    redact sensitive information, ensuring privacy protection through masking. The
    dataset includes question-answer pairs within the images, which are manually verified
    to confirm image clarity and the presence of at least one question-answer pair
    per image. Although some documents span multiple pages, this dataset is structured
    such that it can be processed relatively easily by single-page VRD-QA models.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: RDVQA 数据集（Wu 等，[2022](#bib.bib135)）汇集了大量来自电子商务平台的对话聊天和相关图像。它采用标准的 OCR 和命名实体识别（NER）技术提取文本并遮蔽敏感信息，确保通过屏蔽保护隐私。数据集中包括图像中的问题-答案对，这些对经过人工验证，以确认图像的清晰度和每张图像至少包含一个问题-答案对。尽管一些文档跨越多个页面，但该数据集的结构使得它可以相对容易地被单页
    VRD-QA 模型处理。
- en: PDFVQA-Task C (Ding et al., [2023b](#bib.bib26)) is a distinct sub-task within
    the PDFVQA dataset that expands document VQA to encompass entire long documents,
    moving beyond the single-page focus of Tasks A and B. In Task C, to answer questions,
    the model often needs to retrieve information from multiple document entities.
    Thus, Task C employs Exact Matching for its ground truth annotations. Similar
    to Tasks A and B, additional processing is required for evaluating models at a
    fine-grained level.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: PDFVQA-任务 C（Ding 等，[2023b](#bib.bib26)）是 PDFVQA 数据集中的一个独特子任务，将文档 VQA 扩展到涵盖整个长文档，超越了任务
    A 和 B 的单页焦点。在任务 C 中，为了回答问题，模型通常需要从多个文档实体中检索信息。因此，任务 C 采用精确匹配作为其地面真实标注。与任务 A 和
    B 类似，评估模型时需要额外的处理以实现细粒度级别。
- en: MP-DocVQA (Tito et al., [2023](#bib.bib117)) extends the original DocVQA (Mathew
    et al., [2021](#bib.bib84)) dataset to accommodate multi-page document analysis.
    This version includes adjacent pages from the same documents, expanding the dataset
    from 12,767 to 64,057 document images. In adapting to a multi-page format, some
    questions inappropriate were removed. However, it’s important to note that while
    the dataset allows for questions across multiple pages, the answers remain confined
    to individual pages; there are no cross-page answers in the MP-DocVQA dataset.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: MP-DocVQA (Tito 等，[2023](#bib.bib117)) 扩展了原始的 DocVQA (Mathew 等，[2021](#bib.bib84))
    数据集，以适应多页文档分析。此版本包括来自相同文档的相邻页面，将数据集从 12,767 扩展到 64,057 张文档图像。在适应多页格式时，一些不合适的问题被移除。然而，值得注意的是，虽然数据集允许跨多页提问，但答案仍限制在单页内；在
    MP-DocVQA 数据集中没有跨页答案。
- en: 'DUDE (Van Landeghem et al., [2023](#bib.bib120)) is the first cross-domain,
    multi-page document VQA dataset, featuring a diverse collection of documents from
    various fields such as medical, legal, technical, and financial, and different
    document types including CVs, reports, and papers. It comprises 5,019 documents,
    28,709 document pages, and 41,541 manually annotated questions. Question types
    vary from extractive in-line text and Yes/No answers to multi-hop reasoning and
    structural understanding, similar to those in PDFVQA (Ding et al., [2023b](#bib.bib26)).
    To evaluate model performance, DUDE uses the ANLS metric (Mathew et al., [2021](#bib.bib84))
    for assessing answer prediction accuracy. Additionally, it employs two other metrics:
    Expected Calibration Error (Guo et al., [2017](#bib.bib35)) and Area-Under-Risk-Coverage-Curve
    (CURC) (Geifman and El-Yaniv, [2017](#bib.bib32); Jaeger et al., [2022](#bib.bib46))
    to gauge the overconfidence and miscalibration in document understanding models.
    These features make DUDE a comprehensive tool for evaluating cross-domain document
    understanding models.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: DUDE (Van Landeghem 等，[2023](#bib.bib120)) 是第一个跨领域的多页文档 VQA 数据集，包含来自医疗、法律、技术和金融等各种领域的文档集合，以及简历、报告和论文等不同类型的文档。它包括
    5,019 篇文档、28,709 页文档和 41,541 个人工标注的问题。问题类型从提取的内嵌文本和是/否回答到多跳推理和结构理解，与 PDFVQA (Ding
    等，[2023b](#bib.bib26)) 中的类似。为了评估模型性能，DUDE 使用 ANLS 指标 (Mathew 等，[2021](#bib.bib84))
    来评估答案预测的准确性。此外，它还采用了两个其他指标：期望校准误差 (Guo 等，[2017](#bib.bib35)) 和风险覆盖曲线下的面积 (CURC)
    (Geifman 和 El-Yaniv，[2017](#bib.bib32); Jaeger 等，[2022](#bib.bib46)) 来衡量文档理解模型中的过度自信和误校准。这些特性使
    DUDE 成为评估跨领域文档理解模型的全面工具。
- en: 'MMVQA (Ding et al., [2024a](#bib.bib27)) is a dataset sourced from PubMed Central,
    designed for the retrieval of multimodal semantic entities from multi-page documents.
    The questions are generated using ChatGPT (OpenAI, [2023](#bib.bib90)) and subsequently
    verified manually. Unlike other datasets that focus solely on in-line text or
    text-dense entities, MMVQA also considers entire tables and figures as potential
    answers to the given questions. This dataset introduces various evaluation metrics
    to cater to different application scenarios: Exact Matching and Partial Matching
    Accuracy assess the precision of responses, while Multi-label Recall evaluates
    how well the model identifies all relevant answers across the document. This diverse
    set of metrics makes MMVQA suitable for comprehensive performance evaluation in
    complex, multimodal document understanding tasks.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: MMVQA (Ding 等，[2024a](#bib.bib27)) 是一个来自 PubMed Central 的数据集，旨在从多页文档中检索多模态语义实体。问题是使用
    ChatGPT (OpenAI，[2023](#bib.bib90)) 生成的，并随后进行人工验证。与其他仅关注内嵌文本或文本密集实体的数据集不同，MMVQA
    还考虑将整个表格和图形作为给定问题的潜在答案。该数据集引入了各种评估指标以适应不同的应用场景：精确匹配和部分匹配准确率评估响应的准确性，而多标签召回则评估模型在文档中识别所有相关答案的能力。这些多样化的指标使得
    MMVQA 适合于复杂的多模态文档理解任务中的全面性能评估。
- en: 5.3\. Summary of VRDU Datasets
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. VRDU 数据集总结
- en: 'The common characteristics of KIE and VRD-QA datasets are represented in Table [1](#S5.T1
    "Table 1 ‣ 5.1\. Key Information Extraction and Entity Linking ‣ 5\. Visually
    Rich Document Content Understanding Datasets ‣ Deep Learning based Visually Rich
    Document Content Understanding: A Survey") and Table [2](#S5.T2 "Table 2 ‣ 5.2\.
    Visual Question Answering ‣ 5\. Visually Rich Document Content Understanding Datasets
    ‣ Deep Learning based Visually Rich Document Content Understanding: A Survey").
    Most existing benchmark datasets for key information extraction are designed for
    single-page scenarios and predominantly cater to English documents. However, real-world
    applications often involve more complex multi-page forms. Even forms typically
    require input from multiple parties and contain multiple languages, such as customs
    or import/export declaration forms, presenting unique challenges not addressed
    by current datasets. Similarly, in the domain of VRD-QA, while multi-page VRD-QA
    has gained increased attention, the integration and exploration of multimodal
    information remain insufficiently developed. Specifically, structure and relation-aware
    VRD-QA, which is crucial for interpreting and understanding cross-page relationships,
    is still a largely unexplored area.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 'KIE 和 VRD-QA 数据集的共同特征见表 [1](#S5.T1 "Table 1 ‣ 5.1\. Key Information Extraction
    and Entity Linking ‣ 5\. Visually Rich Document Content Understanding Datasets
    ‣ Deep Learning based Visually Rich Document Content Understanding: A Survey")
    和表 [2](#S5.T2 "Table 2 ‣ 5.2\. Visual Question Answering ‣ 5\. Visually Rich Document
    Content Understanding Datasets ‣ Deep Learning based Visually Rich Document Content
    Understanding: A Survey")。大多数现有的关键数据提取基准数据集是为单页场景设计的，主要针对英文文档。然而，实际应用中往往涉及更复杂的多页表单。甚至表单通常需要来自多个方的输入，并包含多种语言，例如海关或进出口申报单，这些都呈现出当前数据集未能解决的独特挑战。同样，在
    VRD-QA 领域，虽然多页 VRD-QA 获得了更多关注，但多模态信息的整合和探索仍然不足。具体而言，结构和关系感知的 VRD-QA 对于解读和理解跨页关系至关重要，但仍然是一个未被充分探索的领域。'
- en: 6\. Critical Discussion
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 关键讨论
- en: 'In Section [3](#S3 "3\. Mono-Task Document Understanding Frameworks ‣ Deep
    Learning based Visually Rich Document Content Understanding: A Survey") and Section [3](#S4.F3
    "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey"), the models proposed for handling
    both mono-task and multi-task Visually Rich Document Understanding (VRDU) are
    introduced. This section aims to provide a summary and analysis of relevant techniques,
    including their application scenarios, advantages, and disadvantages. Several
    key topics are covered: Feature Engineering, Cross-Modality Fusion, Model Architecture,
    and Pre-training Mechanisms. Additionally, with the rapid development of Large
    Language Models (LLMs), LLM-based frameworks and emerging trends in applying LLMs
    to VRDU are discussed.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '在第 [3](#S3 "3\. Mono-Task Document Understanding Frameworks ‣ Deep Learning
    based Visually Rich Document Content Understanding: A Survey") 节和第 [3](#S4.F3
    "Figure 3 ‣ 4\. Multi-Task VRD Understanding Models ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey") 节中，介绍了处理单任务和多任务视觉丰富文档理解 (VRDU)
    的模型。本节旨在总结和分析相关技术，包括其应用场景、优缺点。涵盖了几个关键主题：特征工程、跨模态融合、模型架构和预训练机制。此外，随着大语言模型 (LLMs)
    的快速发展，还讨论了基于 LLM 的框架以及将 LLM 应用于 VRDU 的新兴趋势。'
- en: 6.1\. Feature Representation
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 特征表示
- en: 6.1.1\. Textual Representation
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. 文本表示
- en: Text in VRDs provides essential semantic context, crucial for understanding
    content and conducting various downstream tasks. Depending on the information
    granularity required by the framework and application scenarios, textual representation
    methods can generally be categorized at the word or entity level.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: VRDs 中的文本提供了基本的语义上下文，这对于理解内容和进行各种下游任务至关重要。根据框架和应用场景所需的信息粒度，文本表示方法一般可以分为词汇级或实体级。
- en: '![Refer to caption](img/3615efd6ebdfd0e27c83fb988e2ebb51.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3615efd6ebdfd0e27c83fb988e2ebb51.png)'
- en: Figure 4\. Fine-grained (word-level) and coarse-grained (entity-level) textual
    information encoding for VRDU frameworks.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. VRDU 框架中的细粒度（词汇级）和粗粒度（实体级）文本信息编码。
- en: Word-level Representations In Visually Rich Documents (VRDs), text sequences
    extracted by off-the-shelf OCR tools or PDF parsers (e.g., PDFMiner) can be encoded
    using word embedding methods such as Word2Vec (Mikolov et al., [2013a](#bib.bib85))
    and Glove (Pennington et al., [2014](#bib.bib94)). For more comprehensive textual
    embeddings, various BERT-style bi-directional pretrained transformer models like
    BERT (Devlin, [2018](#bib.bib23)) and RoBERTa (Liu et al., [2019b](#bib.bib75))
    are employed to generate context-aware word representations. As the visually rich
    and structurally complex nature of VRDs, layout-aware and visual-integrated fine-grained
    models have been developed, such as LayoutLM families (Xu et al., [2020a](#bib.bib136),
    [b](#bib.bib138); Huang et al., [2022](#bib.bib43); Xu et al., [2021](#bib.bib137)),
    LiLT (Wang et al., [2022b](#bib.bib123)). These models generate word representations
    that integrate multimodal information, combining text, visual cues, and layout
    structure to achieve SoTA performance on several downstream tasks.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉丰富文档（VRDs）中，使用现成的OCR工具或PDF解析器（例如PDFMiner）提取的文本序列可以通过诸如Word2Vec（Mikolov et
    al., [2013a](#bib.bib85)）和Glove（Pennington et al., [2014](#bib.bib94)）等词嵌入方法进行编码。为了获得更全面的文本嵌入，采用各种BERT风格的双向预训练变换器模型，如BERT（Devlin,
    [2018](#bib.bib23)）和RoBERTa（Liu et al., [2019b](#bib.bib75)），生成上下文感知的词表示。由于VRDs的视觉丰富和结构复杂的特性，已经开发了布局感知和视觉集成的细粒度模型，如LayoutLM家族（Xu
    et al., [2020a](#bib.bib136), [b](#bib.bib138)；Huang et al., [2022](#bib.bib43)；Xu
    et al., [2021](#bib.bib137)），LiLT（Wang et al., [2022b](#bib.bib123)）。这些模型生成结合了多模态信息的词表示，结合了文本、视觉线索和布局结构，以在多个下游任务中实现SoTA表现。
- en: Entity-level Representations To acquire a dense representation of a text sequence
    for performing entity-level VRDU tasks, various approaches are adopted. These
    include averaging word embeddings specific to an entity or leveraging the $<CLS>$
    token to encapsulate the entire sequence, including averaging the word embeddings
    belonging to an entity or using $<CLS>$ token to represent an entire sequence.
    SentenceBERT (Reimers and Gurevych, [2019](#bib.bib102)) is also often adopted
    to encode text sequences within document entities. However, a standardized approach
    for acquiring textual representations of entities is yet to be established, necessitating
    preliminary testing and validation.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 实体级表示 为了获得用于执行实体级VRDU任务的文本序列的密集表示，采用了各种方法。这些方法包括对特定实体的词嵌入进行平均，或利用$<CLS>$标记来封装整个序列，包括对属于一个实体的词嵌入进行平均，或使用$<CLS>$标记表示整个序列。SentenceBERT（Reimers和Gurevych,
    [2019](#bib.bib102)）也常用于编码文档实体中的文本序列。然而，获取实体文本表示的标准化方法尚未建立，需要进行初步测试和验证。
- en: 6.1.2\. Visual Representation
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. 视觉表示
- en: '![Refer to caption](img/3f0d9ed8e847bf4ebd3dae4af349868f.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3f0d9ed8e847bf4ebd3dae4af349868f.png)'
- en: Figure 5\. Commonly adopted visual information encoding approaches.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 常见的视觉信息编码方法。
- en: 'Visual information provides layout, structural insights, and rich contextual
    clues, making it easier for humans to interpret and prioritize content and resulting
    in a more comprehensive reading experience. Based on the methods used for encoding
    visual information, we categorize them into two main types: CNN-based and Vision
    Transformer-based approaches.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉信息提供了布局、结构性洞察和丰富的上下文线索，使人们更容易解读和优先考虑内容，从而提供更全面的阅读体验。根据用于编码视觉信息的方法，我们将其分为两种主要类型：基于CNN的方法和基于Vision
    Transformer的方法。
- en: CNN-based Vision Encoding methods involve first acquiring Region of Interest
    (RoI) bounding boxes and then applying RoI-pooling and RoI-Align on pretrained
    CNN backbones (e.g., Faster-RCNN or Mask-RCNN) to extract the region features.
    Many frameworks (Xu et al., [2020a](#bib.bib136); Li et al., [2021b](#bib.bib66);
    Luo et al., [2022](#bib.bib80); Ding et al., [2024a](#bib.bib27); Zhai et al.,
    [2023](#bib.bib144); Ding et al., [2023b](#bib.bib26); Gu et al., [2021](#bib.bib33))
    utilize word or entity-level RoIs to effectively extract visual features of target
    regions and leverage the implicit knowledge embedded in pretrained backbones.
    However, acquiring the bounding boxes of words or entities incurs additional costs.
    Therefore, several frameworks (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137);
    Bai et al., [2022](#bib.bib4); Appalaraju et al., [2021](#bib.bib2)) directly
    use image patch bounding RoIs to extract visual features and learn contextually
    with other modalities.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 CNN 的视觉编码方法首先涉及获取感兴趣区域（RoI）边界框，然后在预训练的 CNN 主干（如 Faster-RCNN 或 Mask-RCNN）上应用
    RoI-pooling 和 RoI-Align 以提取区域特征。许多框架 (Xu et al., [2020a](#bib.bib136); Li et al.,
    [2021b](#bib.bib66); Luo et al., [2022](#bib.bib80); Ding et al., [2024a](#bib.bib27);
    Zhai et al., [2023](#bib.bib144); Ding et al., [2023b](#bib.bib26); Gu et al.,
    [2021](#bib.bib33)) 利用词汇或实体级别的 RoI 来有效提取目标区域的视觉特征，并利用预训练主干中嵌入的隐性知识。然而，获取词汇或实体的边界框会产生额外的成本。因此，一些框架
    (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137); Bai et al., [2022](#bib.bib4);
    Appalaraju et al., [2021](#bib.bib2)) 直接使用图像补丁边界 RoI 来提取视觉特征，并与其他模态进行上下文学习。
- en: After acquiring the visual features, they are typically fed into a transformer
    framework to fuse multimodal information, which can create significant computational
    bottlenecks. Additionally, acquiring high-quality RoIs of words or entities requires
    supervised training. To address these challenges, LayoutLMv3, inspired by ViT
    (Dosovitskiy et al., [2020](#bib.bib29)), introduces a transformer-only framework.
    This approach applies a linear layer to project flattened patch pixels, which
    are then fed into a multimodal transformer to contextually learn with other modalities.
    This method reduces the number of parameters and simplifies the preprocessing
    steps, making it more efficient and adopted by many recent frameworks (Tang et al.,
    [2022](#bib.bib116); Appalaraju et al., [2024](#bib.bib3); Mao et al., [2024](#bib.bib83)).
    However, this encoding method cannot take advantage of implicit knowledge in pre-trained
    frameworks and typically requires extensive pre-trained.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取视觉特征后，通常将其输入到转换器框架中以融合多模态信息，这可能会造成显著的计算瓶颈。此外，获取高质量的词汇或实体的区域需要监督训练。为了解决这些挑战，LayoutLMv3
    受 ViT (Dosovitskiy et al., [2020](#bib.bib29)) 启发，引入了一个仅基于转换器的框架。这种方法应用线性层来投影展平的补丁像素，然后将其输入到多模态转换器中，以与其他模态进行上下文学习。这种方法减少了参数数量，简化了预处理步骤，使其更加高效，并被许多最近的框架所采用
    (Tang et al., [2022](#bib.bib116); Appalaraju et al., [2024](#bib.bib3); Mao et al.,
    [2024](#bib.bib83))。然而，这种编码方法无法利用预训练框架中的隐性知识，并且通常需要广泛的预训练。
- en: 6.1.3\. Layout Representation
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3\. 布局表示
- en: '![Refer to caption](img/b91737df9f5662d1e13f9a0bb111e77e.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b91737df9f5662d1e13f9a0bb111e77e.png)'
- en: Figure 6\. Commonly adopted layout information encoding approaches.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 常用的布局信息编码方法。
- en: Layout information is crucial for understanding document elements’ spatial arrangement,
    including words and entities. Enhance document representation by clarifying the
    spatial relationships between these elements, thereby aiding in the comprehension
    of the overall document structure. The coordinates of the bounding box (bbox)
    of the document elements serve as initial layout information. This layout information
    can then be encoded using methods such as positional encoding, linear projection,
    and spatial-aware attention bias.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 布局信息对于理解文档元素的空间排列至关重要，包括词汇和实体。通过阐明这些元素之间的空间关系来增强文档表示，从而帮助理解整个文档结构。文档元素的边界框 (bbox)
    坐标作为初始布局信息。然后可以使用位置编码、线性投影和空间感知注意力偏置等方法对这些布局信息进行编码。
- en: 2D positional encoding, first introduced by LayoutLM (Xu et al., [2020a](#bib.bib136)),
    and widely adopted by many visually rich document understanding (VRDU) models,
    allows the model to be aware of the relative spatial positions within a document.
    In this approach, document elements are normalized and discretized into integer
    ranges, and two separate embedding layers are used to encode the x and y coordinates,
    respectively. Despite its widespread use in models like (Xu et al., [2020a](#bib.bib136),
    [b](#bib.bib138), [2021](#bib.bib137); Huang et al., [2022](#bib.bib43); Tu et al.,
    [2023](#bib.bib119); Wang et al., [2022b](#bib.bib123); Bai et al., [2022](#bib.bib4);
    Liao et al., [2023](#bib.bib69))this method encodes x and y coordinates individually,
    making it challenging to represent continuous 2D space and capture special correlations
    between document elements. Some models (Ding et al., [2023a](#bib.bib25); Wang
    et al., [2020b](#bib.bib130)) that follow the approach of LXMERT (Tan and Bansal,
    [2019](#bib.bib114)) utilize linear projection to update the x and y coordinates
    of the normalized bounding box coordinates simultaneously. To address the limitations
    of absolute positional encoding and incorporate relative positional correlations,
    other models introduce spatial-aware attention mechanisms (Hong et al., [2021](#bib.bib41);
    Appalaraju et al., [2021](#bib.bib2); Wang et al., [2022b](#bib.bib123)). These
    mechanisms enable vanilla self-attention to learn spatial dependencies effectively.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 2D位置编码，首次由LayoutLM (Xu et al., [2020a](#bib.bib136))提出，并被许多视觉丰富文档理解（VRDU）模型广泛采用，允许模型感知文档内的相对空间位置。在这种方法中，文档元素被标准化并离散化为整数范围，并使用两个单独的嵌入层分别对x和y坐标进行编码。尽管在像(Xu
    et al., [2020a](#bib.bib136), [b](#bib.bib138), [2021](#bib.bib137); Huang et
    al., [2022](#bib.bib43); Tu et al., [2023](#bib.bib119); Wang et al., [2022b](#bib.bib123);
    Bai et al., [2022](#bib.bib4); Liao et al., [2023](#bib.bib69))这样的模型中广泛使用，这种方法将x和y坐标分别编码，使得表示连续2D空间和捕捉文档元素之间的特殊相关性变得具有挑战性。一些模型（Ding
    et al., [2023a](#bib.bib25); Wang et al., [2020b](#bib.bib130)）遵循LXMERT (Tan and
    Bansal, [2019](#bib.bib114))的方法，利用线性投影同时更新标准化边界框坐标的x和y坐标。为了应对绝对位置编码的局限性并融入相对位置相关性，其他模型引入了空间感知注意力机制（Hong
    et al., [2021](#bib.bib41); Appalaraju et al., [2021](#bib.bib2); Wang et al.,
    [2022b](#bib.bib123)）。这些机制使得普通自注意力能够有效地学习空间依赖性。
- en: 6.2\. Multi-modality Fusion
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 多模态融合
- en: '![Refer to caption](img/f75b57ef7fd56929ca92731438775b3d.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f75b57ef7fd56929ca92731438775b3d.png)'
- en: Figure 7\. Commonly adopted multi-modality fusion methods.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. 常见的多模态融合方法。
- en: 'After acquiring multimodal representations, it is important to explore effective
    fusion methods to integrate textual, visual, and layout information. This integration
    improves document understanding and boosts performance on downstream tasks. The
    straightforward integration methods, as shown in Figure [7](#S6.F7 "Figure 7 ‣
    6.2\. Multi-modality Fusion ‣ 6\. Critical Discussion ‣ Deep Learning based Visually
    Rich Document Content Understanding: A Survey"), include the additive and concatenation
    of the feature vectors. For example, additive integration sums layout information
    with corresponding textual or visual token representations (Xu et al., [2020a](#bib.bib136),
    [b](#bib.bib138); Huang et al., [2022](#bib.bib43); Li et al., [2021b](#bib.bib66)),
    while concatenation merges visual and textual features of document entities (e.g.
    tables) (Ding et al., [2024a](#bib.bib27), [2023b](#bib.bib26)). However, these
    methods require one-to-one correlations and alternative approaches are needed
    when such correlations are not available. Consequently, self-attention and cross-attention
    mechanisms are widely adopted to enhance each modality by learning inter- and
    intramodality contexts. These mechanisms are commonly used in frameworks (Xu et al.,
    [2020b](#bib.bib138); Huang et al., [2022](#bib.bib43); Tu et al., [2023](#bib.bib119);
    Bai et al., [2022](#bib.bib4); Xu et al., [2021](#bib.bib137)) that integrate
    patch-level visual embeddings with textual features for contextual learning. Novel
    self-attention (Appalaraju et al., [2021](#bib.bib2); Wang et al., [2022b](#bib.bib123))
    and cross-attention (Gu et al., [2021](#bib.bib33); Li et al., [2021b](#bib.bib66);
    Zhai et al., [2023](#bib.bib144)) methods have been proposed to fuse multimodal
    information more effectively. Apart from model-based fusion approaches, self-supervised
    and joint learning tasks are also effective for integrating multi-aspect features.
    Self-supervised pretraining tasks such as Masked Visual-Language Modeling (Wang
    et al., [2022b](#bib.bib123); Xu et al., [2020a](#bib.bib136), [b](#bib.bib138);
    Li et al., [2021a](#bib.bib62); Huang et al., [2022](#bib.bib43); Zhai et al.,
    [2023](#bib.bib144)), Text-Image Alignment (Appalaraju et al., [2021](#bib.bib2)),
    Text-Layout Pairing (Wang et al., [2022b](#bib.bib123)), and Text-Image Matching
    (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137)) can significantly enhance
    multimodal information fusion. These methods require large-scale pretraining to
    learn cross-modality semantic correlations. Joint learning methods, often used
    in OCR-free frameworks (Wang et al., [2021a](#bib.bib124); Kim et al., [2022](#bib.bib52);
    Davis et al., [2022](#bib.bib21); Yu et al., [2022](#bib.bib143)), design auxiliary
    text detection or recognition tasks to fuse textual and visual information. This
    approach reduces pre-processing during inference and addresses mis-ordering sequence
    issues. However, these methods generally underperform compared to OCR-dependent
    models and involve additional training costs.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '在获取了多模态表示之后，探索有效的融合方法以整合文本、视觉和布局信息是非常重要的。这种整合可以改善文档理解，并提升下游任务的表现。简单的融合方法，如图[7](#S6.F7
    "Figure 7 ‣ 6.2\. Multi-modality Fusion ‣ 6\. Critical Discussion ‣ Deep Learning
    based Visually Rich Document Content Understanding: A Survey")所示，包括特征向量的加法和拼接。例如，加法融合将布局信息与相应的文本或视觉标记表示相加（Xu
    et al., [2020a](#bib.bib136), [b](#bib.bib138); Huang et al., [2022](#bib.bib43);
    Li et al., [2021b](#bib.bib66)），而拼接则将文档实体（如表格）的视觉和文本特征合并（Ding et al., [2024a](#bib.bib27),
    [2023b](#bib.bib26)）。然而，这些方法需要一对一的关联，当这种关联不可用时，需要替代方法。因此，自注意力和交叉注意力机制被广泛采用，以通过学习模态间和模态内的上下文来增强每种模态。这些机制通常用于将补丁级别的视觉嵌入与文本特征结合的框架（Xu
    et al., [2020b](#bib.bib138); Huang et al., [2022](#bib.bib43); Tu et al., [2023](#bib.bib119);
    Bai et al., [2022](#bib.bib4); Xu et al., [2021](#bib.bib137)）。新颖的自注意力（Appalaraju
    et al., [2021](#bib.bib2); Wang et al., [2022b](#bib.bib123)）和交叉注意力（Gu et al.,
    [2021](#bib.bib33); Li et al., [2021b](#bib.bib66); Zhai et al., [2023](#bib.bib144)）方法已被提出，以更有效地融合多模态信息。除了基于模型的融合方法，自监督和联合学习任务在整合多方面特征方面也很有效。自监督预训练任务如掩码视觉语言建模（Wang
    et al., [2022b](#bib.bib123); Xu et al., [2020a](#bib.bib136), [b](#bib.bib138);
    Li et al., [2021a](#bib.bib62); Huang et al., [2022](#bib.bib43); Zhai et al.,
    [2023](#bib.bib144)），文本图像对齐（Appalaraju et al., [2021](#bib.bib2)），文本布局配对（Wang
    et al., [2022b](#bib.bib123)），以及文本图像匹配（Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137)）可以显著提升多模态信息融合。这些方法需要大规模的预训练来学习跨模态的语义关联。联合学习方法，通常用于无OCR框架（Wang
    et al., [2021a](#bib.bib124); Kim et al., [2022](#bib.bib52); Davis et al., [2022](#bib.bib21);
    Yu et al., [2022](#bib.bib143)），设计辅助文本检测或识别任务以融合文本和视觉信息。这种方法减少了推理过程中的预处理，并解决了顺序错乱问题。然而，这些方法通常表现不如依赖OCR的模型，并涉及额外的训练成本。'
- en: 6.3\. Model Architecture
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 模型架构
- en: 6.3.1\. Transformer in VRDU
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1\. VRDU中的Transformer
- en: 'Referring to the models introduced in Section [3](#S3 "3\. Mono-Task Document
    Understanding Frameworks ‣ Deep Learning based Visually Rich Document Content
    Understanding: A Survey") and Section [3](#S4.F3 "Figure 3 ‣ 4\. Multi-Task VRD
    Understanding Models ‣ Deep Learning based Visually Rich Document Content Understanding:
    A Survey"), transformers have become extensively utilized in VRDU tasks, attaining
    state-of-the-art performance due to several key advantages. Firstly, the attention
    mechanism effectively captures long-range dependencies within the multimodal information,
    including text, vision, and layout. Furthermore, the inherent scalability of transformers
    improves self-supervised learning on large-scale datasets (e.g. IIT-CDIP (Lewis
    et al., [2006](#bib.bib60))), allowing them to handle diverse types and formats
    of documents, capturing more intricate and comprehensive features of the documents.
    Based on the transformer architecture used, VRDU models can be divided into two
    categories: encoder-only and encoder-decoder-based models. The first encoder-only
    model, LayoutLM (Xu et al., [2020a](#bib.bib136)), was inspired by BERT (Devlin,
    [2018](#bib.bib23)) and uses various pretraining tasks to allow the bidirectional
    transformer encoder to capture more textual and layout information. Following
    LayoutLM, more pretrained VRDU models with encoder-only, layout-aware (Wang et al.,
    [2022b](#bib.bib123); Hong et al., [2021](#bib.bib41); Chen et al., [2022](#bib.bib16);
    Wang et al., [2022b](#bib.bib123); Tu et al., [2023](#bib.bib119); Li et al.,
    [2021a](#bib.bib62)) or visual integrated (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137);
    Appalaraju et al., [2021](#bib.bib2); Huang et al., [2022](#bib.bib43)) have been
    proposed. These models are pretrained on various tasks to enhance their understanding
    of document structures. Encoder-only models demonstrate remarkable performance
    in sequence tagging and document classification tasks. However, they face challenges
    related to heavy annotation requirements and low readability, and they struggle
    with generative tasks such as abstractive question answering. Additionally, OCR
    errors can complicate the extraction of accurate information from input text.
    Furthermore, the fixed maximum input length of encoder-only frameworks limits
    their ability to handle long document inputs effectively. To overcome the limitations
    of encoder-only frameworks in generative QA and KIE, several encoder-decoder models
    (Powalski et al., [2021](#bib.bib96); Tang et al., [2022](#bib.bib116); Appalaraju
    et al., [2024](#bib.bib3); Mao et al., [2024](#bib.bib83)) have been developed,
    but they still depend on costly OCR tools which can introduce errors affecting
    performance. OCR-free frameworks (Kim et al., [2022](#bib.bib52); Davis et al.,
    [2022](#bib.bib21); Cao et al., [2023a](#bib.bib11); Yu et al., [2022](#bib.bib143))
    address this issue by using vision encoders and text decoders for end-to-end processing.
    For long documents, T5-based encoder-decoder models (Raffel et al., [2020](#bib.bib98))
    have been proposed to effectively handle multipage contexts.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 参考第[3节](#S3 "3\. 单任务文档理解框架 ‣ 基于深度学习的视觉丰富文档内容理解：综述")和第[3节](#S4.F3 "图3 ‣ 4\. 多任务视觉丰富文档理解模型
    ‣ 基于深度学习的视觉丰富文档内容理解：综述")中介绍的模型，transformers 在 VRDU 任务中得到广泛应用，凭借几个关键优势取得了最先进的性能。首先，注意力机制有效捕捉了多模态信息中的长距离依赖，包括文本、视觉和布局。此外，transformers
    的固有可扩展性改善了在大规模数据集（如 IIT-CDIP (Lewis et al., [2006](#bib.bib60))）上的自监督学习，使其能够处理多种类型和格式的文档，捕捉文档的更复杂和全面的特征。根据所使用的
    transformer 架构，VRDU 模型可以分为两类：仅编码器模型和编码器-解码器模型。第一个仅编码器模型 LayoutLM (Xu et al., [2020a](#bib.bib136))
    受到 BERT (Devlin, [2018](#bib.bib23)) 的启发，使用各种预训练任务使双向 transformer 编码器能够捕捉更多的文本和布局信息。在
    LayoutLM 之后，提出了更多的预训练 VRDU 模型，这些模型具有仅编码器、布局感知 (Wang et al., [2022b](#bib.bib123);
    Hong et al., [2021](#bib.bib41); Chen et al., [2022](#bib.bib16); Wang et al.,
    [2022b](#bib.bib123); Tu et al., [2023](#bib.bib119); Li et al., [2021a](#bib.bib62))
    或视觉集成 (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137); Appalaraju et al.,
    [2021](#bib.bib2); Huang et al., [2022](#bib.bib43))。这些模型在各种任务上进行了预训练，以增强对文档结构的理解。仅编码器模型在序列标注和文档分类任务中表现出色。然而，它们面临着与大量标注需求和低可读性相关的挑战，并且在生成任务（如抽象问答）中表现不佳。此外，OCR
    错误可能会使从输入文本中提取准确的信息变得复杂。此外，仅编码器框架的固定最大输入长度限制了它们有效处理长文档输入的能力。为了克服仅编码器框架在生成 QA 和
    KIE 中的局限性，开发了几种编码器-解码器模型 (Powalski et al., [2021](#bib.bib96); Tang et al., [2022](#bib.bib116);
    Appalaraju et al., [2024](#bib.bib3); Mao et al., [2024](#bib.bib83))，但它们仍然依赖于昂贵的
    OCR 工具，这可能引入影响性能的错误。无 OCR 框架 (Kim et al., [2022](#bib.bib52); Davis et al., [2022](#bib.bib21);
    Cao et al., [2023a](#bib.bib11); Yu et al., [2022](#bib.bib143)) 通过使用视觉编码器和文本解码器进行端到端处理来解决此问题。对于长文档，已经提出了基于
    T5 的编码器-解码器模型 (Raffel et al., [2020](#bib.bib98)) 来有效处理多页上下文。
- en: 6.3.2\. CNNs in VRDU
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2\. CNNs在VRDU中的应用
- en: In VRDU, CNNs are employed as the core framework for extracting feature maps
    from document images (Yang et al., [2017](#bib.bib139); Palm et al., [2019](#bib.bib92))
    and character grids (Katti et al., [2018](#bib.bib51)), benefiting from their
    strong local feature extraction capabilities. Joint-learning frameworks (Wang
    et al., [2021a](#bib.bib124), [a](#bib.bib124)) use CNNs as a backbone to integrate
    OCR and KIE tasks, combining visual and textual information through auxiliary
    tasks. Some OCR-free pretrained frameworks (Davis et al., [2022](#bib.bib21);
    Yu et al., [2022](#bib.bib143)) also utilize CNNs as vision encoders to extract
    visual feature maps. However, CNNs struggle to capture long-range dependencies
    due to their localized receptive fields. To address this, StrucTexTv2 (Yu et al.,
    [2022](#bib.bib143)) combines CNN-extracted feature maps with a transformer to
    capture global contextual information. Additionally, CNNs are commonly used to
    extract visual features from regions of interest (RoI) with RoI Align(Xu et al.,
    [2020a](#bib.bib136), [b](#bib.bib138), [2021](#bib.bib137); Huang et al., [2022](#bib.bib43);
    Tu et al., [2023](#bib.bib119); Wang et al., [2022b](#bib.bib123); Bai et al.,
    [2022](#bib.bib4); Liao et al., [2023](#bib.bib69)). Although CNNs can accurately
    capture region-specific visual cues and leverage pre-trained knowledge from general
    domains, they require extra processing to obtain RoI bounding boxes, unlike vision
    transformers (Huang et al., [2022](#bib.bib43); Tang et al., [2022](#bib.bib116);
    Appalaraju et al., [2024](#bib.bib3); Mao et al., [2024](#bib.bib83)), which operate
    directly on patch pixel values.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在VRDU中，CNNs被作为核心框架用于从文档图像（Yang et al., [2017](#bib.bib139); Palm et al., [2019](#bib.bib92)）和字符网格（Katti
    et al., [2018](#bib.bib51)）中提取特征图，利用其强大的局部特征提取能力。联合学习框架（Wang et al., [2021a](#bib.bib124),
    [a](#bib.bib124)）使用CNNs作为主干，将OCR和KIE任务结合，通过辅助任务融合视觉和文本信息。一些无OCR预训练框架（Davis et al.,
    [2022](#bib.bib21); Yu et al., [2022](#bib.bib143)）也利用CNNs作为视觉编码器来提取视觉特征图。然而，由于CNNs的局部接收场，它们难以捕捉长距离依赖。为了解决这个问题，StrucTexTv2（Yu
    et al., [2022](#bib.bib143)）将CNN提取的特征图与transformer结合，以捕捉全球上下文信息。此外，CNNs通常用于从感兴趣区域（RoI）中提取视觉特征，并通过RoI
    Align（Xu et al., [2020a](#bib.bib136), [b](#bib.bib138), [2021](#bib.bib137);
    Huang et al., [2022](#bib.bib43); Tu et al., [2023](#bib.bib119); Wang et al.,
    [2022b](#bib.bib123); Bai et al., [2022](#bib.bib4); Liao et al., [2023](#bib.bib69)）。虽然CNNs可以准确捕捉区域特定的视觉线索并利用来自通用领域的预训练知识，但它们需要额外的处理来获得RoI边界框，而视觉transformers（Huang
    et al., [2022](#bib.bib43); Tang et al., [2022](#bib.bib116); Appalaraju et al.,
    [2024](#bib.bib3); Mao et al., [2024](#bib.bib83)）则直接对patch像素值进行操作。
- en: 6.3.3\. Graphs in VRDU
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3\. 图形在VRDU中的应用
- en: VRDs feature complex spatial and logical structures. The spatial structure shows
    the layout and positional relationships, such as a Title above a Paragraph and
    a Caption near a Figure or Table. The logical structure denotes semantic and hierarchical
    connections, like a Title being the parent of a Paragraph and a Caption describing
    a related Table or Figure. Graph-based frameworks explicitly encode these relationships
    using node and edge representations; therefore, Graph Neural Networks (GNNs) are
    widely used in VRDU models (Lee et al., [2022b](#bib.bib57); Shi et al., [2023](#bib.bib107);
    Luo et al., [2022](#bib.bib80); Zhang et al., [2021](#bib.bib146)) to encode the
    spatial and logical representations. Although GNNs effectively capture domain-specific
    knowledge, they struggle with scalability and general domain knowledge pretraining.
    To address this, some frameworks (Li et al., [2023](#bib.bib67); Zhang et al.,
    [2022](#bib.bib147)) use attention masks or biases to mimic relationships between
    document elements, blending attention mechanisms with explicit relational modelling.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: VRDs具有复杂的空间和逻辑结构。空间结构展示了布局和位置关系，例如标题在段落上方，以及图表或表格附近的标题。逻辑结构表示语义和层次关系，例如标题是段落的父级，而标题描述了相关的表格或图形。基于图形的框架通过节点和边的表示显式编码这些关系，因此图神经网络（GNNs）被广泛应用于VRDU模型（Lee
    et al., [2022b](#bib.bib57); Shi et al., [2023](#bib.bib107); Luo et al., [2022](#bib.bib80);
    Zhang et al., [2021](#bib.bib146)）来编码空间和逻辑表示。尽管GNNs有效地捕捉了领域特定的知识，但它们在可扩展性和通用领域知识预训练方面存在困难。为了解决这一问题，一些框架（Li
    et al., [2023](#bib.bib67); Zhang et al., [2022](#bib.bib147)）使用注意力掩码或偏置来模拟文档元素之间的关系，将注意力机制与显式关系建模相结合。
- en: 6.4\. Pretraining Mechanisms
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 预训练机制
- en: By performing various pretraining tasks, a model can enhance its generalization
    ability through extensive datasets and prior training. Inspired by advances in
    pre-training language (Devlin, [2018](#bib.bib23); Brown et al., [2020](#bib.bib10))
    and vision models (He et al., [2016](#bib.bib40); Dosovitskiy et al., [2020](#bib.bib29)),
    numerous pretraining tasks for VRDU have been developed that are typically trained
    in large-scale document collections. This section will summarize the pretraining
    techniques and datasets commonly used for VRDU pretraining.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行各种预训练任务，模型可以通过广泛的数据集和先前的训练来增强其泛化能力。受到预训练语言（Devlin, [2018](#bib.bib23); Brown
    et al., [2020](#bib.bib10)）和视觉模型（He et al., [2016](#bib.bib40); Dosovitskiy et
    al., [2020](#bib.bib29)）进展的启发，已经开发出许多针对VRDU的预训练任务，这些任务通常在大规模文档集合中进行训练。本节将总结常用于VRDU预训练的技术和数据集。
- en: 6.4.1\. Pretraining Tasks
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1. 预训练任务
- en: Based on the purpose and pretraining targets, the pretraining methods can be
    categorised into Masked Information Modeling, Cross-modality Learning, Mono-modality
    Augmentation, and Contrastive Learning.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 根据目的和预训练目标，预训练方法可以分为遮蔽信息建模、跨模态学习、单模态增强和对比学习。
- en: Masked Information Modelling (MIM) is first introduced by Masked Language Modelling
    in BERT (Devlin, [2018](#bib.bib23)), which randomly masks 15% workpiece tokens
    and requires the model to predict the masked tokens. Some models use multi-source
    inputs, such as XDoc (Chen et al., [2022](#bib.bib16)) and MarkupLM (Li et al.,
    [2022a](#bib.bib63)), which directly adopts Masked Language Modeling as a pretraining
    task on plain text or markdown text subsets. Some methods optimise MLM by changing
    masking wordpieces to whole words (Whole Word Masking by LayoutMask (Tu et al.,
    [2023](#bib.bib119))) or all tokens belong to one randomly generated text block,
    named Area-Masked Language Modelling (Hong et al., [2021](#bib.bib41)). Similar
    strategies can also be applied to mask entity-level textual representation, e.g.
    Masked Sentence Modelling (Gu et al., [2021](#bib.bib33)). Those language-targeted
    masked information modelling to improve the language understanding ability of
    VRDU models. Except for language-focused masking strategies, vision (Gu et al.,
    [2021](#bib.bib33); Yu et al., [2022](#bib.bib143)) and layout-focused (Tu et al.,
    [2023](#bib.bib119); Wang et al., [2022b](#bib.bib123)) strategies are also adopted.
    Moreover, masked information modelling is an effective method to boost cross-modality
    understanding. LayoutLM (Xu et al., [2020a](#bib.bib136)) introduces a Masked
    Visual-Language Modelling which allows using kept visual/layout information and
    contextual text content to predict the masked word-pieces, adopted by many VRDU
    pretrained models (Wang et al., [2022b](#bib.bib123); Xu et al., [2020b](#bib.bib138);
    Li et al., [2021a](#bib.bib62); Huang et al., [2022](#bib.bib43); Zhai et al.,
    [2023](#bib.bib144); Appalaraju et al., [2021](#bib.bib2)). Similarly, some visual
    token masked models leverage multimodal information to reconstruct the view tokens,
    e.g. Learn to Reconstruct (Appalaraju et al., [2021](#bib.bib2)), Masked Image
    Modeling (Huang et al., [2022](#bib.bib43)). Additionally, some models mask multimodal
    features simultaneously to conduct a cross-modality masking (Li et al., [2021b](#bib.bib66);
    Liao et al., [2023](#bib.bib69)). Masking Information Modelling may have limitations
    on bias in masking strategies, thus some frameworks (Tu et al., [2023](#bib.bib119))
    may try different masking ratios or strategies to improve the training effectiveness.
    Additionally, other common concerns about MIM also include training efficiency
    and lack of structural information. Thus, other pretraining methods are introduced
    to mitigate the limitations.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码信息建模（MIM）最初由BERT中的掩码语言建模（Devlin，[2018](#bib.bib23)）引入，该方法随机掩盖15%的词片段，并要求模型预测被掩盖的词片段。一些模型使用多源输入，如XDoc（Chen等，[2022](#bib.bib16)）和MarkupLM（Li等，[2022a](#bib.bib63)），这些模型直接将掩码语言建模作为普通文本或markdown文本子集上的预训练任务。一些方法通过将掩盖的词片段更改为整个单词（通过LayoutMask的全词掩码（Tu等，[2023](#bib.bib119)））或将所有词片段归为一个随机生成的文本块（称为区域掩码语言建模（Hong等，[2021](#bib.bib41)））来优化MLM。类似的策略也可以应用于掩盖实体级文本表示，例如掩码句子建模（Gu等，[2021](#bib.bib33)）。这些以语言为目标的掩码信息建模旨在提高VRDU模型的语言理解能力。除了以语言为中心的掩码策略外，还采用了视觉（Gu等，[2021](#bib.bib33)；Yu等，[2022](#bib.bib143)）和布局（Tu等，[2023](#bib.bib119)；Wang等，[2022b](#bib.bib123)）为重点的策略。此外，掩码信息建模是提升跨模态理解的有效方法。LayoutLM（Xu等，[2020a](#bib.bib136)）引入了一种掩码视觉-语言建模，它允许使用保留的视觉/布局信息和上下文文本内容来预测被掩盖的词片段，这一方法被许多VRDU预训练模型（Wang等，[2022b](#bib.bib123)；Xu等，[2020b](#bib.bib138)；Li等，[2021a](#bib.bib62)；Huang等，[2022](#bib.bib43)；Zhai等，[2023](#bib.bib144)；Appalaraju等，[2021](#bib.bib2)）采纳。同样，一些视觉标记掩码模型利用多模态信息来重建视图标记，例如Learn
    to Reconstruct（Appalaraju等，[2021](#bib.bib2)），掩码图像建模（Huang等，[2022](#bib.bib43)）。此外，一些模型同时掩盖多模态特征以进行跨模态掩盖（Li等，[2021b](#bib.bib66)；Liao等，[2023](#bib.bib69)）。掩码信息建模可能在掩码策略的偏差方面存在局限，因此一些框架（Tu等，[2023](#bib.bib119)）可能尝试不同的掩盖比率或策略以提高训练效果。此外，关于MIM的其他常见问题还包括训练效率和缺乏结构信息。因此，引入了其他预训练方法以缓解这些局限。
- en: Cross-modality Aligning Although some Masked Information Modelling methods could
    effectively boost the cross-modality understanding, implicit contextual learning
    is limited to capturing explicit alignment between different modalities. Thus,
    few cross-modality aligning methods are introduced to enhance the modality interaction.
    To enhance the text-image interactive learning, Text-Image Alignment is adopted
    (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137)) which covers the image
    region of token lines to predict whether the image region of the target token
    line is covered or not. LayoutLMv3 (Huang et al., [2022](#bib.bib43)) expands
    from covering the image region only to image/text tokens to further enhance interactive
    learning. Vision-language alignment (Gu et al., [2021](#bib.bib33)) and Text-Image
    Matching (Xu et al., [2020b](#bib.bib138), [2021](#bib.bib137)) target to predict
    whether image-text features belong to the same region or not. DocFormer (Appalaraju
    et al., [2021](#bib.bib2)) tends to predict the text content of the paired images.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 跨模态对齐：虽然一些掩蔽信息建模方法能够有效提高跨模态理解，但隐式上下文学习在捕捉不同模态之间的显式对齐方面有限。因此，少量的跨模态对齐方法被引入以增强模态交互。为了提升文本-图像的互动学习，采用了文本-图像对齐（Xu
    et al., [2020b](#bib.bib138), [2021](#bib.bib137)），该方法覆盖了目标令牌行的图像区域，以预测该图像区域是否被覆盖。LayoutLMv3（Huang
    et al., [2022](#bib.bib43)）从仅覆盖图像区域扩展到图像/文本令牌，以进一步增强互动学习。视觉-语言对齐（Gu et al., [2021](#bib.bib33)）和文本-图像匹配（Xu
    et al., [2020b](#bib.bib138), [2021](#bib.bib137)）旨在预测图像-文本特征是否属于同一区域。DocFormer（Appalaraju
    et al., [2021](#bib.bib2)）倾向于预测配对图像的文本内容。
- en: 'Other Pretraining Techniques: Some pretraining tasks are introduced to further
    enhance the understanding of specific modalities. To enhance layout information
    understanding, StructuralLM (Li et al., [2021a](#bib.bib62)) and WOKONG-READER
    (Bai et al., [2022](#bib.bib4)) introduce Cell Position Classification and Textline
    Grid Matching to predict which located grids are of each cell or textline. Fast-StrucText
    (Zhai et al., [2023](#bib.bib144)) introduces a graph-based token relation method
    to predict a spatial correlation between token pairs. MarkupLM (Li et al., [2022a](#bib.bib63))
    leverage the benefits from markup files to predict the logical parent-child relation
    between nodes by introducing a Node Relation Prediction. Contrastive learning
    based strategies are adopted to conduct single (Gu et al., [2021](#bib.bib33))
    or cross-modality contrastive (Textline-Region Contrastive Learning) (Bai et al.,
    [2022](#bib.bib4)) learning.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其他预训练技术：一些预训练任务被引入以进一步增强对特定模态的理解。为了增强布局信息的理解，StructuralLM（Li et al., [2021a](#bib.bib62)）和WOKONG-READER（Bai
    et al., [2022](#bib.bib4)）引入了单元位置分类和文本行网格匹配，以预测每个单元或文本行的定位网格。Fast-StrucText（Zhai
    et al., [2023](#bib.bib144)）引入了基于图的令牌关系方法，以预测令牌对之间的空间相关性。MarkupLM（Li et al., [2022a](#bib.bib63)）利用标记文件的优势，通过引入节点关系预测来预测节点之间的逻辑父子关系。对比学习策略被用于进行单模态（Gu
    et al., [2021](#bib.bib33)）或跨模态对比（文本行-区域对比学习）（Bai et al., [2022](#bib.bib4)）学习。
- en: 6.4.2\. Pretraining Datasets
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2\. 预训练数据集
- en: To perform the aforementioned tasks, large-scale document collections are essential
    for conducting self-supervised learning. Different pretraining datasets are adopted
    by various models. The most widely used pretraining dataset is the IIT-CDIP Test
    Collection 1.0 (Lewis et al., [2006](#bib.bib60)), which contains more than 6
    million documents with over 11 million scanned document images. Since it contains
    a cross-domain and large number of unannotated documents, it is used by the majority
    of models (Li et al., [2022b](#bib.bib64); Hong et al., [2021](#bib.bib41); Kim
    et al., [2022](#bib.bib52); Xu et al., [2020a](#bib.bib136), [b](#bib.bib138);
    Huang et al., [2022](#bib.bib43); Xu et al., [2021](#bib.bib137); Bai et al.,
    [2022](#bib.bib4); Appalaraju et al., [2021](#bib.bib2); Liao et al., [2023](#bib.bib69);
    Wang et al., [2022b](#bib.bib123); Gu et al., [2021](#bib.bib33); Zhai et al.,
    [2023](#bib.bib144); Li et al., [2021a](#bib.bib62); Luo et al., [2023](#bib.bib78);
    Chen et al., [2022](#bib.bib16); Appalaraju et al., [2024](#bib.bib3)). As the
    original IIT-CDIP dataset provides the text content without layout information,
    off-the-shelf OCR tools are normally used to acquire the bounding box information
    of each document. Some models (Li et al., [2021c](#bib.bib68), [b](#bib.bib66);
    Wang et al., [2022a](#bib.bib127)) use relatively smaller pretraining datasets
    like RVL-CDIP (Harley et al., [2015](#bib.bib37)), which contains 400,000 evenly
    distributed documents in 16 types, to reduce the cost of pretraining. The multi-source
    model XDoc (Chen et al., [2022](#bib.bib16)) also leverages many plain text corpora
    for pretraining, such as BookCORPUS, CC-NEWS, OPENWEBTEXT, STORIES and HTML-sourced
    CommonCrawl datasets. To address multilingual scenarios, both LiLT (Wang et al.,
    [2022b](#bib.bib123)) and LayoutXLM (Xu et al., [2021](#bib.bib137)) follow the
    principles and policies of Common Crawl to gather large amounts of multilingual
    digitally-born PDF documents.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行上述任务，大规模文档集合对于进行自监督学习至关重要。不同的预训练数据集被各种模型采用。最广泛使用的预训练数据集是IIT-CDIP Test Collection
    1.0（Lewis et al., [2006](#bib.bib60)），它包含超过600万份文档以及超过1100万份扫描文档图像。由于它包含跨领域的大量未标注文档，绝大多数模型都在使用它（Li
    et al., [2022b](#bib.bib64)；Hong et al., [2021](#bib.bib41)；Kim et al., [2022](#bib.bib52)；Xu
    et al., [2020a](#bib.bib136)，[b](#bib.bib138)；Huang et al., [2022](#bib.bib43)；Xu
    et al., [2021](#bib.bib137)；Bai et al., [2022](#bib.bib4)；Appalaraju et al., [2021](#bib.bib2)；Liao
    et al., [2023](#bib.bib69)；Wang et al., [2022b](#bib.bib123)；Gu et al., [2021](#bib.bib33)；Zhai
    et al., [2023](#bib.bib144)；Li et al., [2021a](#bib.bib62)；Luo et al., [2023](#bib.bib78)；Chen
    et al., [2022](#bib.bib16)；Appalaraju et al., [2024](#bib.bib3)）。由于原始IIT-CDIP数据集提供了文本内容而没有布局信息，因此通常使用现成的OCR工具来获取每个文档的边界框信息。一些模型（Li
    et al., [2021c](#bib.bib68)，[b](#bib.bib66)；Wang et al., [2022a](#bib.bib127)）使用相对较小的预训练数据集，如RVL-CDIP（Harley
    et al., [2015](#bib.bib37)），该数据集包含40万份均匀分布的16种类型的文档，以降低预训练成本。多源模型XDoc（Chen et
    al., [2022](#bib.bib16)）还利用了许多纯文本语料库进行预训练，如BookCORPUS、CC-NEWS、OPENWEBTEXT、STORIES和HTML来源的CommonCrawl数据集。为了应对多语言场景，LiLT（Wang
    et al., [2022b](#bib.bib123)）和LayoutXLM（Xu et al., [2021](#bib.bib137)）遵循Common
    Crawl的原则和政策，收集大量多语言数字出生的PDF文档。
- en: 7\. Conclusion
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7. 结论
- en: This paper comprehensively reviews deep learning-based models for visually rich
    document content understanding, encompassing both mono-task frameworks designed
    for specific VRDU downstream tasks and multi-task frameworks that support multiple
    VRDU downstream tasks. Beyond introducing the novelties of each model, the limitations
    of these frameworks are summarized at the end of each section, offering a thorough
    trend analysis. Additionally, this paper summarizes existing VRD content understanding
    datasets, pointing out future trends and demands for VRDU. To provide a systematic
    review, we critically discuss various techniques, highlighting their strengths
    and limitations. We believe that this survey offers a comprehensive overview of
    the development of visually rich document content understanding, catering to the
    needs of both the academic and industrial sectors.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 本文全面回顾了基于深度学习的视觉丰富文档内容理解模型，包括针对特定VRDU下游任务设计的单任务框架和支持多个VRDU下游任务的多任务框架。除了介绍每个模型的创新点外，本文还总结了这些框架的局限性，并在每节的末尾提供了全面的趋势分析。此外，本文总结了现有的VRD内容理解数据集，指出了VRDU的未来趋势和需求。为了提供系统的回顾，我们批判性地讨论了各种技术，突出其优缺点。我们相信，本次调查提供了视觉丰富文档内容理解发展的全面概述，以满足学术界和工业界的需求。
- en: References
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Appalaraju et al. (2021) Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota,
    Yusheng Xie, and R Manmatha. 2021. Docformer: End-to-end transformer for document
    understanding. In *Proceedings of the IEEE/CVF international conference on computer
    vision*. 993–1003.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Appalaraju 等人（2021）Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng
    Xie 和 R Manmatha。2021年。Docformer: 用于文档理解的端到端转换器。在 *IEEE/CVF 国际计算机视觉会议论文集*。993–1003。'
- en: 'Appalaraju et al. (2024) Srikar Appalaraju, Peng Tang, Qi Dong, Nishant Sankaran,
    Yichu Zhou, and R Manmatha. 2024. Docformerv2: Local features for document understanding.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 38\.
    709–718.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Appalaraju 等人（2024）Srikar Appalaraju, Peng Tang, Qi Dong, Nishant Sankaran,
    Yichu Zhou 和 R Manmatha。2024年。Docformerv2: 用于文档理解的局部特征。在 *AAAI 人工智能会议论文集*，第38卷。709–718。'
- en: 'Bai et al. (2022) Haoli Bai, Zhiguang Liu, Xiaojun Meng, Wentao Li, Shuang
    Liu, Nian Xie, Rongfu Zheng, Liangwei Wang, Lu Hou, Jiansheng Wei, et al. 2022.
    Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding.
    *arXiv preprint arXiv:2212.09621* (2022).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等人（2022）Haoli Bai, Zhiguang Liu, Xiaojun Meng, Wentao Li, Shuang Liu, Nian
    Xie, Rongfu Zheng, Liangwei Wang, Lu Hou, Jiansheng Wei 等。2022年。Wukong-Reader:
    用于细粒度视觉文档理解的多模态预训练。*arXiv 预印本 arXiv:2212.09621*（2022）。'
- en: 'Bao et al. (2021) Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. 2021. BEiT:
    BERT Pre-Training of Image Transformers. In *International Conference on Learning
    Representations*.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bao 等人（2021）Hangbo Bao, Li Dong, Songhao Piao 和 Furu Wei。2021年。BEiT: BERT 预训练的图像转换器。在
    *国际学习表征会议*。'
- en: 'Biten et al. (2022) Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju,
    and R Manmatha. 2022. Latr: Layout-aware transformer for scene-text vqa. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*. 16548–16558.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Biten 等人（2022）Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju
    和 R Manmatha。2022年。Latr: 布局感知转换器用于场景文本 VQA。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*。16548–16558。'
- en: Biten et al. (2019a) Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,
    Marçal Rusinol, Minesh Mathew, CV Jawahar, Ernest Valveny, and Dimosthenis Karatzas.
    2019a. Icdar 2019 competition on scene text visual question answering. In *2019
    International Conference on Document Analysis and Recognition (ICDAR)*. IEEE,
    1563–1570.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biten 等人（2019a）Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal
    Rusinol, Minesh Mathew, CV Jawahar, Ernest Valveny 和 Dimosthenis Karatzas。2019a年。ICDAR
    2019 场景文本视觉问答竞赛。在 *2019 国际文档分析与识别会议（ICDAR）*。IEEE，1563–1570。
- en: Biten et al. (2019b) Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,
    Marçal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. 2019b. Scene
    text visual question answering. In *Proceedings of the IEEE/CVF international
    conference on computer vision*. 4291–4301.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biten 等人（2019b）Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal
    Rusinol, Ernest Valveny, CV Jawahar 和 Dimosthenis Karatzas。2019b年。场景文本视觉问答。在 *IEEE/CVF
    国际计算机视觉会议论文集*。4291–4301。
- en: 'Blau et al. (2024) Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz,
    Elad Ben Avraham, Aviad Aberdam, Shahar Tsiper, and Ron Litman. 2024. GRAM: Global
    Reasoning for Multi-Page VQA. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 15598–15607.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Blau 等人（2024）Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz, Elad
    Ben Avraham, Aviad Aberdam, Shahar Tsiper 和 Ron Litman。2024年。GRAM: 多页 VQA 的全局推理。在
    *IEEE/CVF 计算机视觉与模式识别会议论文集*。15598–15607。'
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等。2020年。语言模型是少样本学习者。*神经信息处理系统进展* 33（2020），1877–1901。
- en: 'Cao et al. (2023a) Haoyu Cao, Changcun Bao, Chaohu Liu, Huang Chen, Kun Yin,
    Hao Liu, Yinsong Liu, Deqiang Jiang, and Xing Sun. 2023a. Attention Where It Matters:
    Rethinking Visual Document Understanding with Selective Region Concentration.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*.
    19517–19527.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等人（2023a）Haoyu Cao, Changcun Bao, Chaohu Liu, Huang Chen, Kun Yin, Hao Liu,
    Yinsong Liu, Deqiang Jiang 和 Xing Sun。2023a年。关注重要区域：重新思考视觉文档理解中的选择性区域集中。在 *IEEE/CVF
    国际计算机视觉会议论文集*。19517–19527。
- en: 'Cao et al. (2023b) Panfeng Cao, Ye Wang, Qiang Zhang, and Zaiqiao Meng. 2023b.
    GenKIE: Robust Generative Multimodal Document Key Information Extraction. In *Findings
    of the Association for Computational Linguistics: EMNLP 2023*. 14702–14713.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao et al. (2023b) **曹盼峰**、**王烨**、**张强** 和 **孟在桥**。2023b。《GenKIE: 强大的生成多模态文档关键信息提取》。载于
    *计算语言学协会年会：EMNLP 2023*。14702–14713。'
- en: Carbonell et al. (2021) Manuel Carbonell, Pau Riba, Mauricio Villegas, Alicia
    Fornés, and Josep Lladós. 2021. Named entity recognition and relation extraction
    with graph neural networks in semi structured documents. In *2020 25th International
    Conference on Pattern Recognition (ICPR)*. IEEE, 9622–9627.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carbonell et al. (2021) **曼努埃尔·卡博内尔**、**保罗·里巴**、**毛里西奥·维列加斯**、**艾丽西亚·福尔内斯**
    和 **约瑟普·拉多斯**。2021。《在半结构化文档中使用图神经网络进行命名实体识别和关系提取》。载于 *2020年第25届国际模式识别大会 (ICPR)*。IEEE,
    9622–9627。
- en: Carion et al. (2020) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
    Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection
    with transformers. In *European conference on computer vision*. Springer, 213–229.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carion et al. (2020) **尼古拉斯·卡里昂**、**弗朗西斯科·马萨**、**加布里埃尔·辛纳夫**、**尼古拉斯·乌苏尼耶**、**亚历山大·基里洛夫**
    和 **谢尔盖·扎戈鲁伊科**。2020。《使用变换器进行端到端对象检测》。载于 *欧洲计算机视觉会议*。Springer, 213–229。
- en: 'Chen et al. (2023) Jiayi Chen, Hanjun Dai, Bo Dai, Aidong Zhang, and Wei Wei.
    2023. On Task-personalized Multimodal Few-shot Learning for Visually-rich Document
    Entity Retrieval. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*. 9006–9025.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) **陈佳一**、**戴汉俊**、**戴博**、**张爱东** 和 **魏伟**。2023。《针对视觉丰富文档实体检索的任务个性化多模态少样本学习》。载于
    *计算语言学协会年会：EMNLP 2023*。9006–9025。
- en: 'Chen et al. (2022) Jingye Chen, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei.
    2022. XDoc: Unified Pre-training for Cross-Format Document Understanding. *arXiv
    preprint arXiv:2210.02849* (2022).'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2022) **陈晶烨**、**吕腾超**、**雷崔**、**张岔** 和 **魏复儒**。2022。《XDoc: 跨格式文档理解的统一预训练》。*arXiv
    预印本 arXiv:2210.02849* (2022)。'
- en: 'Chen et al. (2021) Xingyu Chen, Zihan Zhao, Lu Chen, Jiabao Ji, Danyang Zhang,
    Ao Luo, Yuxuan Xiong, and Kai Yu. 2021. WebSRC: A Dataset for Web-Based Structural
    Reading Comprehension. In *Proceedings of the 2021 Conference on Empirical Methods
    in Natural Language Processing*. 4173–4185.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2021) **陈兴宇**、**赵子涵**、**陈璐**、**季家宝**、**张丹扬**、**罗奥**、**熊宇轩** 和
    **余凯**。2021。《WebSRC: 一个基于网页的结构化阅读理解数据集》。载于 *2021年自然语言处理实证方法会议论文集*。4173–4185。'
- en: Cheng et al. (2020) Mengli Cheng, Minghui Qiu, Xing Shi, Jun Huang, and Wei
    Lin. 2020. One-shot text field labeling using attention and belief propagation
    for structure information extraction. In *Proceedings of the 28th ACM International
    Conference on Multimedia*. 340–348.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2020) **程梦丽**、**邱明辉**、**史星**、**黄俊** 和 **林伟**。2020。《使用注意力和信念传播进行结构信息提取的单次文本字段标注》。载于
    *第28届ACM国际多媒体会议论文集*。340–348。
- en: Chu et al. (2022) Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua
    Shen. 2022. Conditional Positional Encodings for Vision Transformers. In *The
    Eleventh International Conference on Learning Representations*.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu et al. (2022) **楚向向**、**田智**、**张博**、**王鑫龙** 和 **沈春华**。2022。《视觉变换器的条件位置编码》。载于
    *第十一届国际学习表征会议*。
- en: 'Cui et al. (2021) Lei Cui, Yiheng Xu, Tengchao Lv, and Furu Wei. 2021. Document
    ai: Benchmarks, models and applications. *arXiv preprint arXiv:2111.08609* (2021).'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cui et al. (2021) **雷崔**、**徐一恒**、**吕腾超** 和 **魏复儒**。2021。《Document ai: Benchmarks,
    models and applications》。*arXiv 预印本 arXiv:2111.08609* (2021)。'
- en: Davis et al. (2022) Brian Davis, Bryan Morse, Brian Price, Chris Tensmeyer,
    Curtis Wigington, and Vlad Morariu. 2022. End-to-end document recognition and
    understanding with dessurt. In *European Conference on Computer Vision*. Springer,
    280–296.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davis et al. (2022) **布莱恩·戴维斯**、**布莱恩·摩尔斯**、**布赖恩·普莱斯**、**克里斯·滕斯迈耶**、**柯蒂斯·威金顿**
    和 **弗拉德·莫拉里乌**。2022。《使用dessurt进行端到端文档识别和理解》。载于 *欧洲计算机视觉会议*。Springer, 280–296。
- en: 'Denk and Reisswig (2019) Timo I Denk and Christian Reisswig. 2019. Bertgrid:
    Contextualized embedding for 2d document representation and understanding. *arXiv
    preprint arXiv:1909.04948* (2019).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Denk and Reisswig (2019) **蒂莫·I·邓克** 和 **克里斯蒂安·赖斯维格**。2019。《Bertgrid: 用于2D文档表示和理解的上下文化嵌入》。*arXiv
    预印本 arXiv:1909.04948* (2019)。'
- en: 'Devlin (2018) J Devlin. 2018. BERT: Pre-training of deep bidirectional transformers
    for language understanding.. In *Proceedings of NAACL-HLT*, Vol. 2019\. 4171.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin (2018) **J Devlin**。2018。《BERT: 语言理解的深度双向变换器预训练》。载于 *NAACL-HLT 会议论文集*，第2019卷。4171。'
- en: 'Ding et al. (2022) Yihao Ding, Zhe Huang, Runlin Wang, YanHang Zhang, Xianru
    Chen, Yuzhong Ma, Hyunsuk Chung, and Soyeon Caren Han. 2022. V-Doc: Visual questions
    answers with Documents. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 21492–21498.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding 等（2022）Yihao Ding, Zhe Huang, Runlin Wang, YanHang Zhang, Xianru Chen,
    Yuzhong Ma, Hyunsuk Chung 和 Soyeon Caren Han。2022年。V-Doc: 具有文档的视觉问题回答。发表于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*。21492–21498。'
- en: 'Ding et al. (2023a) Yihao Ding, Siqu Long, Jiabin Huang, Kaixuan Ren, Xingxiang
    Luo, Hyunsuk Chung, and Soyeon Caren Han. 2023a. Form-NLU: Dataset for the Form
    Natural Language Understanding. In *Proceedings of the 46th International ACM
    SIGIR Conference on Research and Development in Information Retrieval*. 2807–2816.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding 等（2023a）Yihao Ding, Siqu Long, Jiabin Huang, Kaixuan Ren, Xingxiang Luo,
    Hyunsuk Chung 和 Soyeon Caren Han。2023a年。Form-NLU: 表单自然语言理解数据集。发表于 *第46届国际ACM SIGIR信息检索研究与发展会议论文集*。2807–2816。'
- en: 'Ding et al. (2023b) Yihao Ding, Siwen Luo, Hyunsuk Chung, and Soyeon Caren
    Han. 2023b. VQA: A New Dataset for Real-World VQA on PDF Documents. In *Joint
    European Conference on Machine Learning and Knowledge Discovery in Databases*.
    Springer, 585–601.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding 等（2023b）Yihao Ding, Siwen Luo, Hyunsuk Chung 和 Soyeon Caren Han。2023b年。VQA:
    针对PDF文档的现实世界VQA的新数据集。发表于 *欧洲机器学习与数据库知识发现联合会议*。Springer，585–601。'
- en: 'Ding et al. (2024a) Yihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo, and Soyeon Caren
    Han. 2024a. MVQA: A Dataset for Multimodal Information Retrieval in PDF-based
    Visual Question Answering. *arXiv preprint arXiv:2404.12720* (2024).'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding 等（2024a）Yihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo 和 Soyeon Caren
    Han。2024a年。MVQA: 用于基于PDF的视觉问题回答的多模态信息检索数据集。 *arXiv 预印本 arXiv:2404.12720*（2024）。'
- en: 'Ding et al. (2024b) Yihao Ding, Lorenzo Vaiani, Caren Han, Jean Lee, Paolo
    Garza, Josiah Poon, and Luca Cagliero. 2024b. M3-VRD: Multimodal Multi-task Multi-teacher
    Visually-Rich Form Document Understanding. *arXiv preprint arXiv:2402.17983* (2024).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding 等（2024b）Yihao Ding, Lorenzo Vaiani, Caren Han, Jean Lee, Paolo Garza,
    Josiah Poon 和 Luca Cagliero。2024b年。M3-VRD: 多模态多任务多教师视觉丰富表单文档理解。 *arXiv 预印本 arXiv:2402.17983*（2024）。'
- en: 'Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An Image is Worth 16x16 Words:
    Transformers for Image Recognition at Scale. In *International Conference on Learning
    Representations*.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等（2020）Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk
    Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,
    Georg Heigold, Sylvain Gelly 等。2020年。一张图像胜过 16x16 个词：用于大规模图像识别的变换器。发表于 *国际学习表示会议*。
- en: 'Du et al. (2022) Qinyi Du, Qingqing Wang, Keqian Li, Jidong Tian, Liqiang Xiao,
    and Yaohui Jin. 2022. CALM: commen-sense knowledge augmentation for document image
    understanding. In *Proceedings of the 30th ACM International Conference on Multimedia*.
    3282–3290.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等（2022）Qinyi Du, Qingqing Wang, Keqian Li, Jidong Tian, Liqiang Xiao 和 Yaohui
    Jin。2022年。CALM: 用于文档图像理解的常识知识增强。发表于 *第30届ACM国际多媒体会议论文集*。3282–3290。'
- en: 'Ehrmann et al. (2023) Maud Ehrmann, Ahmed Hamdi, Elvys Linhares Pontes, Matteo
    Romanello, and Antoine Doucet. 2023. Named entity recognition and classification
    in historical documents: A survey. *Comput. Surveys* 56, 2 (2023), 1–47.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ehrmann 等（2023）Maud Ehrmann, Ahmed Hamdi, Elvys Linhares Pontes, Matteo Romanello
    和 Antoine Doucet。2023年。历史文档中的命名实体识别与分类：一项调查。 *计算机调查* 56，2（2023），1–47。
- en: Geifman and El-Yaniv (2017) Yonatan Geifman and Ran El-Yaniv. 2017. Selective
    classification for deep neural networks. *Advances in neural information processing
    systems* 30 (2017).
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geifman 和 El-Yaniv（2017）Yonatan Geifman 和 Ran El-Yaniv。2017年。深度神经网络的选择性分类。 *神经信息处理系统进展*
    30（2017）。
- en: 'Gu et al. (2021) Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Rajiv
    Jain, Nikolaos Barmpalios, Ani Nenkova, and Tong Sun. 2021. Unidoc: Unified pretraining
    framework for document understanding. *Advances in Neural Information Processing
    Systems* 34 (2021), 39–50.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu 等（2021）Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Rajiv Jain,
    Nikolaos Barmpalios, Ani Nenkova 和 Tong Sun。2021年。Unidoc: 用于文档理解的统一预训练框架。 *神经信息处理系统进展*
    34（2021），39–50。'
- en: 'Gu et al. (2022) Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan, Weiqiang Wang,
    Ming Gu, and Liqing Zhang. 2022. Xylayoutlm: Towards layout-aware multimodal networks
    for visually-rich document understanding. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 4583–4592.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu 等（2022）Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan, Weiqiang Wang, Ming
    Gu 和 Liqing Zhang。2022年。Xylayoutlm: 迈向布局感知的多模态网络用于视觉丰富的文档理解。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*。4583–4592。'
- en: Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
    2017. On calibration of modern neural networks. In *International conference on
    machine learning*. PMLR, 1321–1330.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人（2017）Chuan Guo, Geoff Pleiss, Yu Sun, 和 Kilian Q Weinberger. 2017. 现代神经网络的校准问题。在
    *国际机器学习会议* 中。PMLR, 1321–1330。
- en: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. Retrieval augmented language model pre-training. In *International
    conference on machine learning*. PMLR, 3929–3938.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu 等人（2020）Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, 和 Mingwei Chang.
    2020. 检索增强语言模型预训练。在 *国际机器学习会议* 中。PMLR, 3929–3938。
- en: Harley et al. (2015) Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis.
    2015. Evaluation of deep convolutional nets for document image classification
    and retrieval. In *2015 13th International Conference on Document Analysis and
    Recognition (ICDAR)*. IEEE, 991–995.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harley 等人（2015）Adam W Harley, Alex Ufkes, 和 Konstantinos G Derpanis. 2015. 深度卷积网络在文档图像分类和检索中的评估。在
    *2015年第13届国际文档分析与识别会议（ICDAR）* 中。IEEE, 991–995。
- en: 'He et al. (2023) Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, Xing Xu, and
    Heng Tao Shen. 2023. ICL-D3IE: In-context learning with diverse demonstrations
    updating for document information extraction. In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*. 19485–19494.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2023）Jiabang He, Lei Wang, Yi Hu, Ning Liu, Hui Liu, Xing Xu, 和 Heng Tao
    Shen. 2023. ICL-D3IE：用于文档信息提取的多样化示例更新的上下文学习。在 *IEEE/CVF国际计算机视觉会议论文集* 中。19485–19494。
- en: He et al. (2017) Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
    2017. Mask r-cnn. In *Proceedings of the IEEE international conference on computer
    vision*. 2961–2969.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2017）Kaiming He, Georgia Gkioxari, Piotr Dollár, 和 Ross Girshick. 2017.
    Mask R-CNN。在 *IEEE国际计算机视觉会议论文集* 中。2961–2969。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2016）Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 2016. 用于图像识别的深度残差学习。在
    *IEEE计算机视觉与模式识别会议论文集* 中。770–778。
- en: 'Hong et al. (2021) Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok Hwang, Daehyun
    Nam, and Sungrae Park. 2021. BROS: A pre-trained language model for understanding
    texts in document. (2021).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人（2021）Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam,
    和 Sungrae Park. 2021. BROS：用于理解文档中文本的预训练语言模型。（2021）。
- en: Hu et al. (2023) Kai Hu, Zhuoyuan Wu, Zhuoyao Zhong, Weihong Lin, Lei Sun, and
    Qiang Huo. 2023. A question-answering approach to key value pair extraction from
    form-like document images. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 37\. 12899–12906.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2023）Kai Hu, Zhuoyuan Wu, Zhuoyao Zhong, Weihong Lin, Lei Sun, 和 Qiang
    Huo. 2023. 一种从类似表单的文档图像中提取键值对的问答方法。在 *AAAI人工智能会议论文集* 中，第37卷。12899–12906。
- en: 'Huang et al. (2022) Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu
    Wei. 2022. Layoutlmv3: Pre-training for document ai with unified text and image
    masking. In *Proceedings of the 30th ACM International Conference on Multimedia*.
    4083–4091.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2022）Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, 和 Furu Wei. 2022.
    Layoutlmv3：用于文档AI的统一文本和图像掩蔽的预训练。在 *第30届ACM国际多媒体会议论文集* 中。4083–4091。
- en: Huang et al. (2019) Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis
    Karatzas, Shijian Lu, and CV Jawahar. 2019. Icdar2019 competition on scanned receipt
    ocr and information extraction. In *2019 International Conference on Document
    Analysis and Recognition (ICDAR)*. IEEE, 1516–1520.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2019）Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas,
    Shijian Lu, 和 CV Jawahar. 2019. Icdar2019扫描收据OCR和信息提取竞赛。在 *2019国际文档分析与识别会议（ICDAR）*
    中。IEEE, 1516–1520。
- en: 'Hwang et al. (2021) Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang,
    and Minjoon Seo. 2021. Spatial Dependency Parsing for Semi-Structured Document
    Information Extraction. In *Findings of the Association for Computational Linguistics:
    ACL-IJCNLP 2021*. 330–343.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hwang 等人（2021）Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, 和 Minjoon
    Seo. 2021. 半结构化文档信息提取的空间依赖解析。在 *计算语言学协会会议论文集：ACL-IJCNLP 2021* 中。330–343。
- en: Jaeger et al. (2022) Paul F Jaeger, Carsten Tim Lüth, Lukas Klein, and Till J
    Bungert. 2022. A Call to Reflect on Evaluation Practices for Failure Detection
    in Image Classification. In *The Eleventh International Conference on Learning
    Representations*.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaeger 等人（2022）Paul F Jaeger, Carsten Tim Lüth, Lukas Klein, 和 Till J Bungert.
    2022. 呼吁反思图像分类中失败检测的评估实践。在 *第十一届国际学习表示会议* 中。
- en: 'Jaume et al. (2019) Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe
    Thiran. 2019. Funsd: A dataset for form understanding in noisy scanned documents.
    In *2019 International Conference on Document Analysis and Recognition Workshops
    (ICDARW)*, Vol. 2\. IEEE, 1–6.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jaume et al. (2019) Guillaume Jaume, Hazim Kemal Ekenel, 和 Jean-Philippe Thiran.
    2019. Funsd: 用于嘈杂扫描文档中的表单理解的数据集。发表于 *2019 年国际文档分析与识别研讨会（ICDARW）*，第 2 卷。IEEE，1–6。'
- en: Jegou et al. (2010) Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010.
    Product quantization for nearest neighbor search. *IEEE transactions on pattern
    analysis and machine intelligence* 33, 1 (2010), 117–128.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jegou et al. (2010) Herve Jegou, Matthijs Douze, 和 Cordelia Schmid. 2010. 用于最近邻搜索的产品量化。*IEEE
    交易模式分析与机器智能* 33, 1 (2010), 117–128。
- en: 'Joshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke
    Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing
    and predicting spans. *Transactions of the association for computational linguistics*
    8 (2020), 64–77.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Joshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke
    Zettlemoyer, 和 Omer Levy. 2020. Spanbert: 通过表示和预测跨度来改进预训练。*计算语言学协会交易* 8 (2020)，64–77。'
- en: Kang et al. (2024) Lei Kang, Rubèn Tito, Ernest Valveny, and Dimosthenis Karatzas.
    2024. Multi-Page Document Visual Question Answering using Self-Attention Scoring
    Mechanism. *arXiv preprint arXiv:2404.19024* (2024).
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang et al. (2024) Lei Kang, Rubèn Tito, Ernest Valveny, 和 Dimosthenis Karatzas.
    2024. 使用自注意力评分机制的多页文档视觉问答。*arXiv 预印本 arXiv:2404.19024* (2024)。
- en: 'Katti et al. (2018) Anoop R Katti, Christian Reisswig, Cordula Guder, Sebastian
    Brarda, Steffen Bickel, Johannes Höhne, and Jean Baptiste Faddoul. 2018. Chargrid:
    Towards Understanding 2D Documents. In *Proceedings of the 2018 Conference on
    Empirical Methods in Natural Language Processing*. 4459–4469.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Katti et al. (2018) Anoop R Katti, Christian Reisswig, Cordula Guder, Sebastian
    Brarda, Steffen Bickel, Johannes Höhne, 和 Jean Baptiste Faddoul. 2018. Chargrid:
    向理解二维文档迈进。发表于 *2018 年自然语言处理经验方法会议论文集*。4459–4469。'
- en: 'Kim et al. (2022) Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung
    Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.
    2022. Ocr-free document understanding transformer. In *Computer Vision–ECCV 2022:
    17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXVIII*. Springer, 498–517.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2022) Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung
    Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, 和 Seunghyun Park.
    2022. 无 OCR 的文档理解变换器。发表于 *计算机视觉–ECCV 2022：第 17 届欧洲会议，特拉维夫，以色列，2022 年 10 月 23-27
    日，论文集，第二十八部分*。Springer，498–517。
- en: 'Kim et al. (2021) Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language
    transformer without convolution or region supervision. In *International conference
    on machine learning*. PMLR, 5583–5594.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim et al. (2021) Wonjae Kim, Bokyung Son, 和 Ildoo Kim. 2021. Vilt: 无卷积或区域监督的视觉-语言变换器。发表于
    *国际机器学习会议*。PMLR，5583–5594。'
- en: Kim (2014) Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, Alessandro Moschitti, Bo Pang, and Walter Daelemans (Eds.).
    Association for Computational Linguistics, Doha, Qatar, 1746–1751. [https://doi.org/10.3115/v1/D14-1181](https://doi.org/10.3115/v1/D14-1181)
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim (2014) Yoon Kim. 2014. 用于句子分类的卷积神经网络。发表于 *2014 年自然语言处理经验方法会议（EMNLP）论文集*，Alessandro
    Moschitti, Bo Pang, 和 Walter Daelemans (Eds.)。计算语言学协会，卡塔尔多哈，1746–1751。 [https://doi.org/10.3115/v1/D14-1181](https://doi.org/10.3115/v1/D14-1181)
- en: 'Lample et al. (2016) Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian,
    Kazuya Kawakami, and Chris Dyer. 2016. Neural Architectures for Named Entity Recognition.
    In *Proceedings of the 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*. 260–270.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample et al. (2016) Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian,
    Kazuya Kawakami, 和 Chris Dyer. 2016. 用于命名实体识别的神经网络架构。发表于 *2016 年北美计算语言学协会年会：人类语言技术会议论文集*。260–270。
- en: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised
    learning of language representations. *arXiv preprint arXiv:1909.11942* (2019).'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, 和 Radu Soricut. 2019. Albert: 一种用于自监督语言表示学习的轻量级 BERT。*arXiv 预印本
    arXiv:1909.11942* (2019)。'
- en: 'Lee et al. (2022b) Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot,
    Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, and Tomas Pfister.
    2022b. FormNet: Structural Encoding beyond Sequential Modeling in Form Document
    Information Extraction. In *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*. 3735–3754.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等人 (2022b) Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong
    Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii 和 Tomas Pfister. 2022b.
    FormNet: 超越序列建模的表单文档信息提取结构编码。在 *第60届计算语言学协会年会论文集（第1卷：长篇论文）* 中。3735–3754。'
- en: 'Lee et al. (2023) Chen-Yu Lee, Chun-Liang Li, Hao Zhang, Timothy Dozat, Vincent
    Perot, Guolong Su, Xiang Zhang, Kihyuk Sohn, Nikolai Glushnev, Renshen Wang, et al.
    2023. FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information
    Extraction. *arXiv preprint arXiv:2305.02549* (2023).'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等人 (2023) Chen-Yu Lee, Chun-Liang Li, Hao Zhang, Timothy Dozat, Vincent
    Perot, Guolong Su, Xiang Zhang, Kihyuk Sohn, Nikolai Glushnev, Renshen Wang 等.
    2023. FormNetV2: 用于表单文档信息提取的多模态图对比学习。*arXiv 预印本 arXiv:2305.02549* (2023)。'
- en: 'Lee et al. (2022a) Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu
    Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina
    Toutanova. 2022a. Pix2Struct: Screenshot parsing as pretraining for visual language
    understanding. *arXiv preprint arXiv:2210.03347* (2022).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等人 (2022a) Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu,
    Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang 和 Kristina
    Toutanova. 2022a. Pix2Struct: 截图解析作为视觉语言理解的预训练。*arXiv 预印本 arXiv:2210.03347* (2022)。'
- en: Lewis et al. (2006) David Lewis, Gady Agam, Shlomo Argamon, Ophir Frieder, David
    Grossman, and Jefferson Heard. 2006. Building a test collection for complex document
    information processing. In *Proceedings of the 29th annual international ACM SIGIR
    conference on Research and development in information retrieval*. 665–666.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人 (2006) David Lewis, Gady Agam, Shlomo Argamon, Ophir Frieder, David
    Grossman 和 Jefferson Heard. 2006. 建立一个用于复杂文档信息处理的测试集。在 *第29届国际ACM SIGIR研究与开发信息检索会议论文集*
    中。665–666。
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
    BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
    Translation, and Comprehension. In *Proceedings of the 58th Annual Meeting of
    the Association for Computational Linguistics*. 7871–7880.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis 等人 (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov 和 Luke Zettlemoyer. 2020. BART:
    用于自然语言生成、翻译和理解的去噪序列到序列预训练。在 *第58届计算语言学协会年会论文集* 中。7871–7880。'
- en: 'Li et al. (2021a) Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang,
    Fei Huang, and Luo Si. 2021a. StructuralLM: Structural Pre-training for Form Understanding.
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*. 6309–6318.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2021a) Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei
    Huang 和 Luo Si. 2021a. StructuralLM: 用于表单理解的结构预训练。在 *第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）*
    中。6309–6318。'
- en: 'Li et al. (2022a) Junlong Li, Yiheng Xu, Lei Cui, and Furu Wei. 2022a. MarkupLM:
    Pre-training of Text and Markup Language for Visually Rich Document Understanding.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*. 6078–6087.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2022a) Junlong Li, Yiheng Xu, Lei Cui 和 Furu Wei. 2022a. MarkupLM: 用于视觉丰富文档理解的文本和标记语言预训练。在
    *第60届计算语言学协会年会论文集（第1卷：长篇论文）* 中。6078–6087。'
- en: 'Li et al. (2022b) Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and
    Furu Wei. 2022b. Dit: Self-supervised pre-training for document image transformer.
    In *Proceedings of the 30th ACM International Conference on Multimedia*. 3530–3539.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2022b) Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang 和 Furu
    Wei. 2022b. Dit: 用于文档图像变换器的自监督预训练。在 *第30届ACM国际多媒体会议论文集* 中。3530–3539。'
- en: 'Li et al. (2019) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and
    Kai-Wei Chang. 2019. Visualbert: A simple and performant baseline for vision and
    language. *arXiv preprint arXiv:1908.03557* (2019).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2019) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh 和 Kai-Wei
    Chang. 2019. Visualbert: 一种简单且高效的视觉与语言基线。*arXiv 预印本 arXiv:1908.03557* (2019)。'
- en: 'Li et al. (2021b) Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong
    Zhao, Rajiv Jain, Varun Manjunatha, and Hongfu Liu. 2021b. Selfdoc: Self-supervised
    document representation learning. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*. 5652–5660.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2021b）Peizhao Li、Jiuxiang Gu、Jason Kuen、Vlad I Morariu、Handong Zhao、Rajiv
    Jain、Varun Manjunatha和Hongfu Liu。2021b。Selfdoc：自监督文档表示学习。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*。5652–5660。
- en: Li et al. (2023) Qiwei Li, Zuchao Li, Xiantao Cai, Bo Du, and Hai Zhao. 2023.
    Enhancing Visually-Rich Document Understanding via Layout Structure Modeling.
    In *Proceedings of the 31st ACM International Conference on Multimedia*. 4513–4523.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2023）Qiwei Li、Zuchao Li、Xiantao Cai、Bo Du和Hai Zhao。2023。通过布局结构建模提升视觉丰富文档理解。发表于*第31届ACM国际多媒体会议论文集*。4513–4523。
- en: 'Li et al. (2021c) Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan Zhang,
    Yan Liu, Kun Yao, Junyu Han, Jingtuo Liu, and Errui Ding. 2021c. Structext: Structured
    text understanding with multi-modal transformers. In *Proceedings of the 29th
    ACM International Conference on Multimedia*. 1912–1920.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2021c）Yulin Li、Yuxi Qian、Yuechen Yu、Xiameng Qin、Chengquan Zhang、Yan Liu、Kun
    Yao、Junyu Han、Jingtuo Liu和Errui Ding。2021c。Structext：利用多模态变换器的结构化文本理解。发表于*第29届ACM国际多媒体会议论文集*。1912–1920。
- en: 'Liao et al. (2023) Haofu Liao, Aruni RoyChowdhury, Weijian Li, Ankan Bansal,
    Yuting Zhang, Zhuowen Tu, Ravi Kumar Satzoda, R Manmatha, and Vijay Mahadevan.
    2023. Doctr: Document transformer for structured information extraction in documents.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*.
    19584–19594.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao等（2023）Haofu Liao、Aruni RoyChowdhury、Weijian Li、Ankan Bansal、Yuting Zhang、Zhuowen
    Tu、Ravi Kumar Satzoda、R Manmatha和Vijay Mahadevan。2023。Doctr：用于文档中结构化信息提取的文档变换器。发表于*IEEE/CVF国际计算机视觉会议论文集*。19584–19594。
- en: Lin et al. (2017) Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath
    Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    2117–2125.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2017）Tsung-Yi Lin、Piotr Dollár、Ross Girshick、Kaiming He、Bharath Hariharan和Serge
    Belongie。2017。用于目标检测的特征金字塔网络。发表于*IEEE计算机视觉与模式识别会议论文集*。2117–2125。
- en: 'Liu et al. (2024b) Chaohu Liu, Kun Yin, Haoyu Cao, Xinghua Jiang, Xin Li, Yinsong
    Liu, Deqiang Jiang, Xing Sun, and Linli Xu. 2024b. Hrvda: High-resolution visual
    document assistant. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 15534–15545.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2024b）Chaohu Liu、Kun Yin、Haoyu Cao、Xinghua Jiang、Xin Li、Yinsong Liu、Deqiang
    Jiang、Xing Sun和Linli Xu。2024b。Hrvda：高分辨率视觉文档助手。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*。15534–15545。
- en: Liu et al. (2024a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    2024a. Visual instruction tuning. *Advances in neural information processing systems*
    36 (2024).
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2024a）Haotian Liu、Chunyuan Li、Qingyang Wu和Yong Jae Lee。2024a。视觉指令调优。*神经信息处理系统进展*
    36（2024）。
- en: 'Liu et al. (2023) Jixiong Liu, Yoan Chabot, Raphaël Troncy, Viet-Phi Huynh,
    Thomas Labbé, and Pierre Monnin. 2023. From tabular data to knowledge graphs:
    A survey of semantic table interpretation tasks and methods. *Journal of Web Semantics*
    76 (2023), 100761.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2023）Jixiong Liu、Yoan Chabot、Raphaël Troncy、Viet-Phi Huynh、Thomas Labbé和Pierre
    Monnin。2023。从表格数据到知识图谱：语义表解释任务和方法综述。*网络语义学杂志* 76（2023），100761。
- en: 'Liu et al. (2019a) Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao. 2019a.
    Graph Convolution for Multimodal Information Extraction from Visually Rich Documents.
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 2 (Industry
    Papers)*. 32–39.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2019a）Xiaojing Liu、Feiyu Gao、Qiong Zhang和Huasha Zhao。2019a。图卷积用于从视觉丰富文档中提取多模态信息。发表于*2019年北美计算语言学协会年会：人类语言技术会议论文集，第2卷（行业论文）*。32–39。
- en: 'Liu et al. (2019b) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*
    (2019).'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2019b）Yinhan Liu、Myle Ott、Naman Goyal、Jingfei Du、Mandar Joshi、Danqi Chen、Omer
    Levy、Mike Lewis、Luke Zettlemoyer和Veselin Stoyanov。2019b。Roberta：一种鲁棒优化的BERT预训练方法。*arXiv预印本
    arXiv:1907.11692*（2019）。
- en: 'Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,
    Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer
    using shifted windows. In *Proceedings of the IEEE/CVF international conference
    on computer vision*. 10012–10022.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,
    Stephen Lin, 和 Baining Guo. 2021. Swin transformer: 使用偏移窗口的分层视觉变换器。发表于 *IEEE/CVF国际计算机视觉会议论文集*。10012–10022。'
- en: Lombardi and Marinai (2020) Francesco Lombardi and Simone Marinai. 2020. Deep
    learning for historical document analysis and recognition—a survey. *Journal of
    Imaging* 6, 10 (2020), 110.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lombardi and Marinai (2020) Francesco Lombardi 和 Simone Marinai. 2020. 深度学习在历史文档分析和识别中的应用——一项综述。*图像学杂志*
    6, 10 (2020), 110。
- en: 'Luo et al. (2023) Chuwei Luo, Changxu Cheng, Qi Zheng, and Cong Yao. 2023.
    Geolayoutlm: Geometric pre-training for visual information extraction. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 7092–7101.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. (2023) Chuwei Luo, Changxu Cheng, Qi Zheng, 和 Cong Yao. 2023. Geolayoutlm:
    视觉信息提取的几何预训练。发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*。7092–7101。'
- en: 'Luo et al. (2024) Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, and
    Cong Yao. 2024. LayoutLLM: Layout Instruction Tuning with Large Language Models
    for Document Understanding. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 15630–15640.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. (2024) Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, 和
    Cong Yao. 2024. LayoutLLM: 利用大语言模型进行文档理解的布局指令调整。发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*。15630–15640。'
- en: 'Luo et al. (2022) Siwen Luo, Yihao Ding, Siqu Long, Josiah Poon, and Soyeon Caren
    Han. 2022. Doc-GCN: Heterogeneous Graph Convolutional Networks for Document Layout
    Analysis. In *Proceedings of the 29th International Conference on Computational
    Linguistics*. 2906–2916.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. (2022) Siwen Luo, Yihao Ding, Siqu Long, Josiah Poon, 和 Soyeon Caren
    Han. 2022. Doc-GCN: 用于文档布局分析的异质图卷积网络。发表于 *第29届国际计算语言学会议论文集*。2906–2916。'
- en: Majumder et al. (2020a) Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep
    Tata, James Bradley Wendt, Qi Zhao, and Marc Najork. 2020a. Representation learning
    for information extraction from form-like documents. In *proceedings of the 58th
    annual meeting of the Association for Computational Linguistics*. 6495–6504.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majumder et al. (2020a) Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep
    Tata, James Bradley Wendt, Qi Zhao, 和 Marc Najork. 2020a. 表示学习用于从表单类文档中提取信息。发表于
    *第58届计算语言学协会年会论文集*。6495–6504。
- en: Majumder et al. (2020b) Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep
    Tata, James Bradley Wendt, Qi Zhao, and Marc Najork. 2020b. Representation learning
    for information extraction from form-like documents. In *proceedings of the 58th
    annual meeting of the Association for Computational Linguistics*. 6495–6504.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majumder et al. (2020b) Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep
    Tata, James Bradley Wendt, Qi Zhao, 和 Marc Najork. 2020b. 表示学习用于从表单类文档中提取信息。发表于
    *第58届计算语言学协会年会论文集*。6495–6504。
- en: 'Mao et al. (2024) Zhiming Mao, Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang,
    Qun Liu, and Kam-Fai Wong. 2024. Visually Guided Generative Text-Layout Pre-training
    for Document Intelligence. In *Proceedings of the 2024 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies (Volume 1: Long Papers)*. 4713–4730.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao et al. (2024) Zhiming Mao, Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Qun
    Liu, 和 Kam-Fai Wong. 2024. 视觉引导生成文本-布局预训练用于文档智能。发表于 *2024年北美计算语言学协会年会：人类语言技术（第1卷：长篇论文）会议论文集*。4713–4730。
- en: 'Mathew et al. (2021) Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021.
    Docvqa: A dataset for vqa on document images. In *Proceedings of the IEEE/CVF
    winter conference on applications of computer vision*. 2200–2209.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mathew et al. (2021) Minesh Mathew, Dimosthenis Karatzas, 和 CV Jawahar. 2021.
    Docvqa: 一个用于文档图像的视觉问答数据集。发表于 *IEEE/CVF冬季计算机视觉应用会议论文集*。2200–2209。'
- en: Mikolov et al. (2013a) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013a. Efficient estimation of word representations in vector space. *arXiv preprint
    arXiv:1301.3781* (2013).
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov et al. (2013a) Tomas Mikolov, Kai Chen, Greg Corrado, 和 Jeffrey Dean.
    2013a. 向量空间中词表示的高效估计。*arXiv预印本 arXiv:1301.3781* (2013)。
- en: Mikolov et al. (2013b) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013b. Distributed representations of words and phrases and their
    compositionality. *Advances in neural information processing systems* 26 (2013).
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov et al. (2013b) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    和 Jeff Dean. 2013b. 单词和短语的分布式表示及其组成性。*神经信息处理系统进展* 26 (2013)。
- en: Nagy and Seth (1984) George Nagy and Sharad C Seth. 1984. Hierarchical representation
    of optically scanned documents. (1984).
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagy and Seth (1984) George Nagy and Sharad C Seth. 1984. 光学扫描文档的分层表示。（1984）。
- en: O’Gorman (1993) Lawrence O’Gorman. 1993. The document spectrum for page layout
    analysis. *IEEE Transactions on pattern analysis and machine intelligence* 15,
    11 (1993), 1162–1173.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Gorman (1993) Lawrence O’Gorman. 1993. 用于页面布局分析的文档光谱。*IEEE模式分析和机器智能交易* 15,
    11 (1993), 1162–1173.
- en: Oliveira and Viana (2017) Dario Augusto Borges Oliveira and Matheus Palhares
    Viana. 2017. Fast CNN-based document layout analysis. In *2017 IEEE International
    Conference on Computer Vision Workshops (ICCVW)*. IEEE, 1173–1180.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oliveira and Viana (2017) Dario Augusto Borges Oliveira and Matheus Palhares
    Viana. 2017. 基于快速CNN的文档布局分析。在*2017年IEEE计算机视觉研讨会(ICCVW)*。IEEE, 1173–1180.
- en: 'OpenAI (2023) OpenAI. 2023. ChatGPT: A conversational agent. [https://www.openai.com/chatgpt](https://www.openai.com/chatgpt)'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI (2023) OpenAI. 2023. ChatGPT: 一款对话代理。[https://www.openai.com/chatgpt](https://www.openai.com/chatgpt)'
- en: 'Oreshkin et al. (2018) Boris Oreshkin, Pau Rodríguez López, and Alexandre Lacoste.
    2018. Tadam: Task dependent adaptive metric for improved few-shot learning. *Advances
    in neural information processing systems* 31 (2018).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Oreshkin et al. (2018) Boris Oreshkin, Pau Rodríguez López, and Alexandre Lacoste.
    2018. Tadam: 任务相关自适应度量以改善少样本学习。*神经信息处理系统进展* 31 (2018).'
- en: Palm et al. (2019) Rasmus Berg Palm, Florian Laws, and Ole Winther. 2019. Attend,
    copy, parse end-to-end information extraction from documents. In *2019 International
    Conference on Document Analysis and Recognition (ICDAR)*. IEEE, 329–336.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Palm et al. (2019) Rasmus Berg Palm, Florian Laws, and Ole Winther. 2019. 参与、复制、解析端到端文档信息提取。在*2019年国际文档分析与识别会议(ICDAR)*。IEEE,
    329–336.
- en: 'Park et al. (2019) Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung
    Surh, Minjoon Seo, and Hwalsuk Lee. 2019. CORD: a consolidated receipt dataset
    for post-OCR parsing. In *Workshop on Document Intelligence at NeurIPS 2019*.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park et al. (2019) Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung
    Surh, Minjoon Seo, and Hwalsuk Lee. 2019. CORD: 用于后光学字符识别解析的合并收据数据集。在*NeurIPS
    2019上的文档智能研讨会*。'
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D
    Manning. 2014. Glove: Global vectors for word representation. In *Proceedings
    of the 2014 conference on empirical methods in natural language processing (EMNLP)*.
    1532–1543.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D
    Manning. 2014. Glove：词表示的全局向量。在*2014年经验方法在自然语言处理会议上的论文集(EMNLP)*。1532–1543。
- en: 'Perot et al. (2023) Vincent Perot, Kai Kang, Florian Luisier, Guolong Su, Xiaoyu
    Sun, Ramya Sree Boppana, Zilong Wang, Jiaqi Mu, Hao Zhang, and Nan Hua. 2023.
    LMDX: Language Model-based Document Information Extraction and Localization. *arXiv
    preprint arXiv:2309.10952* (2023).'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perot et al. (2023) Vincent Perot, Kai Kang, Florian Luisier, Guolong Su, Xiaoyu
    Sun, Ramya Sree Boppana, Zilong Wang, Jiaqi Mu, Hao Zhang, and Nan Hua. 2023.
    LMDX：基于语言模型的文档信息提取和定位。*arXiv预印本arXiv:2309.10952* (2023).
- en: 'Powalski et al. (2021) Rafał Powalski, Łukasz Borchmann, Dawid Jurkiewicz,
    Tomasz Dwojak, Michał Pietruszka, and Gabriela Pałka. 2021. Going full-tilt boogie
    on document understanding with text-image-layout transformer. In *Document Analysis
    and Recognition–ICDAR 2021: 16th International Conference, Lausanne, Switzerland,
    September 5–10, 2021, Proceedings, Part II 16*. Springer, 732–747.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Powalski et al. (2021) Rafał Powalski, Łukasz Borchmann, Dawid Jurkiewicz,
    Tomasz Dwojak, Michał Pietruszka, and Gabriela Pałka. 2021. 使用文本-图像-布局变换器在文档理解方面全力以赴。在*文档分析与识别-ICDAR
    2021: 第16届国际会议, 2021年9月5日-10日, 瑞士洛桑, 第16部分会议论文集*. Springer, 732–747.'
- en: 'Press et al. (2021) Ofir Press, Noah Smith, and Mike Lewis. 2021. Train Short,
    Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In
    *International Conference on Learning Representations*.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press et al. (2021) Ofir Press, Noah Smith, and Mike Lewis. 2021. 训练短，测试长：线性偏差的注意力使输入长度外推。在*国际学习表示会议*。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research* 21, 140 (2020), 1–67.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. 使用统一的文本到文本变换器探索迁移学习的极限。*机器学习研究杂志*
    21, 140 (2020), 1–67.
- en: 'Rajpurkar et al. (2016a) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
    and Percy Liang. 2016a. SQuAD: 100,000+ Questions for Machine Comprehension of
    Text. In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*. 2383–2392.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar 等（2016a）Pranav Rajpurkar、Jian Zhang、Konstantin Lopyrev 和 Percy Liang。2016a年。《SQuAD：100,000+
    机器理解文本的问题》。在 *2016年自然语言处理实证方法会议* 上。2383–2392页。
- en: 'Rajpurkar et al. (2016b) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
    and Percy Liang. 2016b. SQuAD: 100,000+ Questions for Machine Comprehension of
    Text. In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*. 2383–2392.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar 等（2016b）Pranav Rajpurkar、Jian Zhang、Konstantin Lopyrev 和 Percy Liang。2016b年。《SQuAD：100,000+
    机器理解文本的问题》。在 *2016年自然语言处理实证方法会议* 上。2383–2392页。
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image
    generation. In *International conference on machine learning*. Pmlr, 8821–8831.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh 等（2021）Aditya Ramesh、Mikhail Pavlov、Gabriel Goh、Scott Gray、Chelsea Voss、Alec
    Radford、Mark Chen 和 Ilya Sutskever。2021年。《零-shot 文本到图像生成》。在 *国际机器学习会议* 上。Pmlr，8821–8831页。
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*. 3982–3992.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers 和 Gurevych（2019）Nils Reimers 和 Iryna Gurevych。2019年。《Sentence-BERT：使用
    Siamese BERT 网络的句子嵌入》。在 *2019年自然语言处理实证方法会议和第9届国际联合自然语言处理会议（EMNLP-IJCNLP）* 上。3982–3992页。
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.
    Faster r-cnn: Towards real-time object detection with region proposal networks.
    *Advances in neural information processing systems* 28 (2015).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等（2015）Shaoqing Ren、Kaiming He、Ross Girshick 和 Jian Sun。2015年。《Faster r-cnn：基于区域提议网络的实时目标检测》。*神经信息处理系统进展*
    28（2015年）。
- en: Rusinol et al. (2013) Marçal Rusinol, Tayeb Benkhelfallah, and Vincent Poulain dAndecy.
    2013. Field extraction from administrative documents by incremental structural
    templates. In *2013 12th International Conference on Document Analysis and Recognition*.
    IEEE, 1100–1104.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rusinol 等（2013）Marçal Rusinol、Tayeb Benkhelfallah 和 Vincent Poulain dAndecy。2013年。《通过增量结构模板从行政文档中提取字段》。在
    *2013年第12届国际文档分析与识别会议* 上。IEEE，1100–1104页。
- en: Saout et al. (2024) Thomas Saout, Frédéric Lardeux, and Frédéric Saubion. 2024.
    An Overview of Data Extraction From Invoices. *IEEE Access* (2024).
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saout 等（2024）Thomas Saout、Frédéric Lardeux 和 Frédéric Saubion。2024年。《发票数据提取概述》。*IEEE
    Access*（2024年）。
- en: Seki et al. (2007) Minenobu Seki, Masakazu Fujio, Takeshi Nagasaki, Hiroshi
    Shinjo, and Katsumi Marukawa. 2007. Information management system using structure
    analysis of paper/electronic documents and its applications. In *Ninth International
    Conference on Document Analysis and Recognition (ICDAR 2007)*, Vol. 2\. IEEE,
    689–693.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seki 等（2007）Minenobu Seki、Masakazu Fujio、Takeshi Nagasaki、Hiroshi Shinjo 和 Katsumi
    Marukawa。2007年。《使用纸质/电子文档结构分析的信息管理系统及其应用》。在 *第九届国际文档分析与识别会议（ICDAR 2007）* 上，第2卷。IEEE，689–693页。
- en: 'Shi et al. (2023) Dengliang Shi, Siliang Liu, Jintao Du, and Huijia Zhu. 2023.
    Layoutgcn: A lightweight architecture for visually rich document understanding.
    In *International Conference on Document Analysis and Recognition*. Springer,
    149–165.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等（2023）Dengliang Shi、Siliang Liu、Jintao Du 和 Huijia Zhu。2023年。《Layoutgcn：用于视觉丰富文档理解的轻量级架构》。在
    *国际文档分析与识别会议* 上。Springer，149–165页。
- en: Šimsa et al. (2023) Štěpán Šimsa, Milan Šulc, Michal Uřičář, Yash Patel, Ahmed
    Hamdi, Matěj Kocián, Matyáš Skalickỳ, Jiří Matas, Antoine Doucet, Mickaël Coustaty,
    et al. 2023. Docile benchmark for document information localization and extraction.
    In *International Conference on Document Analysis and Recognition*. Springer,
    147–166.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Šimsa 等（2023）Štěpán Šimsa、Milan Šulc、Michal Uřičář、Yash Patel、Ahmed Hamdi、Matěj
    Kocián、Matyáš Skalickỳ、Jiří Matas、Antoine Doucet、Mickaël Coustaty 等。2023年。《用于文档信息定位和提取的
    Docile 基准》。在 *国际文档分析与识别会议* 上。Springer，147–166页。
- en: Snell et al. (2017) Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical
    networks for few-shot learning. *Advances in neural information processing systems*
    30 (2017).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snell 等（2017）Jake Snell、Kevin Swersky 和 Richard Zemel。2017年。《用于少样本学习的原型网络》。*神经信息处理系统进展*
    30（2017年）。
- en: Sohn (2016) Kihyuk Sohn. 2016. Improved deep metric learning with multi-class
    n-pair loss objective. *Advances in neural information processing systems* 29
    (2016).
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohn（2016）Kihyuk Sohn。2016年。《具有多类别 n-对损失目标的改进深度度量学习》。*神经信息处理系统进展* 29（2016年）。
- en: 'Speer et al. (2017) Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet
    5.5: An open multilingual graph of general knowledge. In *Proceedings of the AAAI
    conference on artificial intelligence*, Vol. 31.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Speer 等 (2017) Robyn Speer、Joshua Chin 和 Catherine Havasi。2017。Conceptnet 5.5：一个开放的多语言一般知识图谱。在
    *AAAI 人工智能会议论文集*，第 31 卷。
- en: 'Stanisławek et al. (2021) Tomasz Stanisławek, Filip Graliński, Anna Wróblewska,
    Dawid Lipiński, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, and Przemysław
    Biecek. 2021. Kleister: key information extraction datasets involving long documents
    with complex layouts. In *Document Analysis and Recognition–ICDAR 2021: 16th International
    Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part I*.
    Springer, 564–579.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stanisławek 等 (2021) Tomasz Stanisławek、Filip Graliński、Anna Wróblewska、Dawid
    Lipiński、Agnieszka Kaliska、Paulina Rosalska、Bartosz Topolski 和 Przemysław Biecek。2021。Kleister：涉及复杂布局长文档的关键信息提取数据集。在
    *文档分析与识别–ICDAR 2021：第16届国际会议，瑞士洛桑，2021年9月5–10日，会议录，第I部分*。Springer，564–579。
- en: Subramani et al. (2020) Nishant Subramani, Alexandre Matton, Malcolm Greaves,
    and Adrian Lam. 2020. A survey of deep learning approaches for ocr and document
    understanding. *arXiv preprint arXiv:2011.13534* (2020).
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subramani 等 (2020) Nishant Subramani、Alexandre Matton、Malcolm Greaves 和 Adrian
    Lam。2020。关于 OCR 和文档理解的深度学习方法综述。*arXiv 预印本 arXiv:2011.13534* (2020)。
- en: 'Tan and Bansal (2019) Hao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality
    Encoder Representations from Transformers. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*. 5100–5111.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 和 Bansal (2019) Hao Tan 和 Mohit Bansal。2019。LXMERT：从变换器中学习跨模态编码表示。在 *2019
    年自然语言处理经验方法会议暨第九届国际联合自然语言处理会议 (EMNLP-IJCNLP)*。5100–5111。
- en: 'Tanaka et al. (2021) Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. 2021.
    Visualmrc: Machine reading comprehension on document images. In *Proceedings of
    the AAAI Conference on Artificial Intelligence*, Vol. 35\. 13878–13888.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanaka 等 (2021) Ryota Tanaka、Kyosuke Nishida 和 Sen Yoshida。2021。Visualmrc：文档图像上的机器阅读理解。在
    *AAAI 人工智能会议论文集*，第 35 卷。13878–13888。
- en: Tang et al. (2022) Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu,
    Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. 2022. Unifying Vision,
    Text, and Layout for Universal Document Processing. *arXiv preprint arXiv:2212.02623*
    (2022).
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等 (2022) Zineng Tang、Ziyi Yang、Guoxin Wang、Yuwei Fang、Yang Liu、Chenguang
    Zhu、Michael Zeng、Cha Zhang 和 Mohit Bansal。2022。统一视觉、文本和布局以进行通用文档处理。*arXiv 预印本
    arXiv:2212.02623* (2022)。
- en: Tito et al. (2023) Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. 2023.
    Hierarchical multimodal transformers for Multipage DocVQA. *Pattern Recognition*
    144 (2023), 109834.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tito 等 (2023) Rubèn Tito、Dimosthenis Karatzas 和 Ernest Valveny。2023。用于多页文档问答的层次化多模态变换器。*模式识别*
    144 (2023)，109834。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2023) Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。2023。Llama：开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971* (2023)。
- en: 'Tu et al. (2023) Yi Tu, Ya Guo, Huan Chen, and Jinyang Tang. 2023. LayoutMask:
    Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding.
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*. 15200–15212.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 等 (2023) Yi Tu、Ya Guo、Huan Chen 和 Jinyang Tang。2023。LayoutMask：增强多模态预训练中的文本-布局交互以理解文档。在
    *第61届计算语言学协会年会（第1卷：长篇论文）*。15200–15212。
- en: Van Landeghem et al. (2023) Jordy Van Landeghem, Rubèn Tito, Łukasz Borchmann,
    Michał Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, Mickaël Coustaty,
    Bertrand Anckaert, Ernest Valveny, et al. 2023. Document understanding dataset
    and evaluation (DUDE). In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*. 19528–19540.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Landeghem 等 (2023) Jordy Van Landeghem、Rubèn Tito、Łukasz Borchmann、Michał
    Pietruszka、Pawel Joziak、Rafal Powalski、Dawid Jurkiewicz、Mickaël Coustaty、Bertrand
    Anckaert、Ernest Valveny 等。2023。文档理解数据集和评估 (DUDE)。在 *IEEE/CVF 国际计算机视觉会议论文集*。19528–19540。
- en: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis
    Platform for Natural Language Understanding. In *International Conference on Learning
    Representations*.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, 和 Samuel R Bowman. 2018. GLUE: 一种多任务基准和自然语言理解分析平台。见于 *国际学习表征会议*。'
- en: 'Wang et al. (2020a) Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang
    Chen, Jun-Wei Hsieh, and I-Hau Yeh. 2020a. CSPNet: A new backbone that can enhance
    learning capability of CNN. In *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition workshops*. 390–391.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2020a) Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang
    Chen, Jun-Wei Hsieh, 和 I-Hau Yeh. 2020a. CSPNet: 一种新的骨干网络，可以增强CNN的学习能力。见于 *IEEE/CVF计算机视觉与模式识别会议论文集*。390–391。'
- en: 'Wang et al. (2022b) Jiapeng Wang, Lianwen Jin, and Kai Ding. 2022b. LiLT: A
    Simple yet Effective Language-Independent Layout Transformer for Structured Document
    Understanding. In *Proceedings of the 60th Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*. 7747–7757.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2022b) Jiapeng Wang, Lianwen Jin, 和 Kai Ding. 2022b. LiLT: 一种简单而有效的语言无关布局变换器，用于结构化文档理解。见于
    *第60届计算语言学协会年会论文集（第1卷：长篇论文）*。7747–7757。'
- en: 'Wang et al. (2021a) Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin
    Zhang, Shuaitao Zhang, Qianying Wang, Yaqiang Wu, and Mingxiang Cai. 2021a. Towards
    robust visual information extraction in real world: new dataset and novel solution.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 35\.
    2738–2745.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021a) Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin
    Zhang, Shuaitao Zhang, Qianying Wang, Yaqiang Wu, 和 Mingxiang Cai. 2021a. 朝着现实世界中鲁棒的视觉信息提取：新数据集和新解决方案。见于
    *AAAI人工智能会议论文集*，第35卷。2738–2745。
- en: 'Wang et al. (2022c) Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang
    Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022c. Ofa: Unifying
    architectures, tasks, and modalities through a simple sequence-to-sequence learning
    framework. In *International Conference on Machine Learning*. PMLR, 23318–23340.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2022c) Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang
    Li, Jianxin Ma, Chang Zhou, Jingren Zhou, 和 Hongxia Yang. 2022c. Ofa: 通过简单的序列到序列学习框架统一架构、任务和模态。见于
    *国际机器学习会议*。PMLR, 23318–23340。'
- en: 'Wang et al. (2019) Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh,
    and Louis-Philippe Morency. 2019. Words can shift: Dynamically adjusting word
    representations using nonverbal behaviors. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 33\. 7216–7223.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019) Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh,
    和 Louis-Philippe Morency. 2019. 词语可以转移：使用非语言行为动态调整词语表示。见于 *AAAI人工智能会议论文集*，第33卷。7216–7223。
- en: 'Wang et al. (2022a) Zilong Wang, Jiuxiang Gu, Chris Tensmeyer, Nikolaos Barmpalios,
    Ani Nenkova, Tong Sun, Jingbo Shang, and Vlad Morariu. 2022a. MGDoc: Pre-training
    with Multi-granular Hierarchy for Document Image Understanding. In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing*. 3984–3993.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2022a) Zilong Wang, Jiuxiang Gu, Chris Tensmeyer, Nikolaos Barmpalios,
    Ani Nenkova, Tong Sun, Jingbo Shang, 和 Vlad Morariu. 2022a. MGDoc: 通过多粒度层次预训练进行文档图像理解。见于
    *2022年自然语言处理经验方法会议论文集*。3984–3993。'
- en: 'Wang and Shang (2022) Zilong Wang and Jingbo Shang. 2022. Towards Few-shot
    Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework.
    In *Findings of the Association for Computational Linguistics: ACL 2022*. 4174–4186.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Shang (2022) Zilong Wang 和 Jingbo Shang. 2022. 朝着文档图像中的少样本实体识别：一种标签感知序列到序列框架。见于
    *计算语言学协会发现：ACL 2022*。4174–4186。
- en: 'Wang et al. (2021b) Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, and Furu
    Wei. 2021b. LayoutReader: Pre-training of Text and Layout for Reading Order Detection.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*. 4735–4744.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2021b) Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, 和 Furu Wei.
    2021b. LayoutReader: 预训练文本和布局以检测阅读顺序。见于 *2021年自然语言处理经验方法会议论文集*。4735–4744。'
- en: 'Wang et al. (2020b) Zilong Wang, Mingjie Zhan, Xuebo Liu, and Ding Liang. 2020b.
    Docstruct: A multimodal method to extract hierarchy structure in document for
    general form understanding. *arXiv preprint arXiv:2010.11685* (2020).'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2020b) Zilong Wang, Mingjie Zhan, Xuebo Liu, 和 Ding Liang. 2020b.
    Docstruct: 一种多模态方法，用于提取文档中的层次结构以实现通用形式理解。*arXiv预印本 arXiv:2010.11685* (2020)。'
- en: 'Wang et al. (2023a) Zifeng Wang, Zizhao Zhang, Jacob Devlin, Chen-Yu Lee, Guolong
    Su, Hao Zhang, Jennifer Dy, Vincent Perot, and Tomas Pfister. 2023a. QueryForm:
    A Simple Zero-shot Form Entity Query Framework. In *Findings of the Association
    for Computational Linguistics: ACL 2023*. 4146–4159.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) 王自峰、张子钊、雅各布·德夫林、陈宇李、郭龙苏、郝张、詹妮弗·戴、文森特·佩罗和托马斯·菲斯特。2023a年。QueryForm：一个简单的零样本表单实体查询框架。发表于*计算语言学协会会议发现：ACL
    2023*。4146–4159页。
- en: 'Wang et al. (2023b) Zilong Wang, Yichao Zhou, Wei Wei, Chen-Yu Lee, and Sandeep
    Tata. 2023b. Vrdu: A benchmark for visually-rich document understanding. In *Proceedings
    of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. 5184–5193.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) 王子龙、周一超、魏伟、陈宇李和桑迪普·塔塔。2023b年。Vrdu：视觉丰富文档理解基准。发表于*第29届ACM
    SIGKDD知识发现与数据挖掘会议论文集*。5184–5193页。
- en: Watanabe et al. (1995) Toyohide Watanabe, Qin Luo, and Noboru Sugie. 1995. Layout
    recognition of multi-kinds of table-form documents. *IEEE Transactions on Pattern
    Analysis and Machine Intelligence* 17, 4 (1995), 432–445.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watanabe et al. (1995) 渡边丰秀、洛秦和杉江诺博。1995年。多种表格形式文档的布局识别。*IEEE模式分析与机器智能学报* 17卷，4期（1995年），432–445页。
- en: Wei et al. (2020) Mengxi Wei, Yifan He, and Qiong Zhang. 2020. Robust layout-aware
    IE for visually rich documents with pre-trained language models. In *Proceedings
    of the 43rd International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 2367–2376.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2020) 魏孟熙、赫一凡和张琼。2020年。利用预训练语言模型的视觉丰富文档鲁棒布局感知信息抽取。发表于*第43届国际ACM
    SIGIR信息检索研究与发展会议论文集*。2367–2376页。
- en: Wu et al. (2022) Xinya Wu, Duo Zheng, Ruonan Wang, Jiashen Sun, Minzhen Hu,
    Fangxiang Feng, Xiaojie Wang, Huixing Jiang, and Fan Yang. 2022. A region-based
    document VQA. In *Proceedings of the 30th ACM International Conference on Multimedia*.
    4909–4920.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2022) 吴欣雅、郑多、王若楠、孙家深、敏珍胡、芳香冯、肖杰王、辉兴姜和范杨。2022年。基于区域的文档VQA。发表于*第30届ACM国际多媒体会议论文集*。4909–4920页。
- en: 'Xu et al. (2020a) Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei,
    and Ming Zhou. 2020a. Layoutlm: Pre-training of text and layout for document image
    understanding. In *Proceedings of the 26th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*. 1192–1200.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2020a) 许恒徐、明浩李、雷崔、少涵黄、富如魏和明周。2020a年。Layoutlm：文本与布局的预训练用于文档图像理解。发表于*第26届ACM
    SIGKDD国际知识发现与数据挖掘大会论文集*。1192–1200页。
- en: 'Xu et al. (2021) Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei
    Florencio, Cha Zhang, and Furu Wei. 2021. Layoutxlm: Multimodal pre-training for
    multilingual visually-rich document understanding. *arXiv preprint arXiv:2104.08836*
    (2021).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2021) 许恒徐、腾超吕、雷崔、国信王、怡娟陆、迪内·弗洛伦西奥、查张和富如魏。2021年。Layoutxlm：多模态预训练用于多语言视觉丰富文档理解。*arXiv
    预印本 arXiv:2104.08836*（2021年）。
- en: 'Xu et al. (2020b) Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin
    Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, et al. 2020b. Layoutlmv2:
    Multi-modal pre-training for visually-rich document understanding. *arXiv preprint
    arXiv:2012.14740* (2020).'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2020b) 杨徐、许恒徐、腾超吕、雷崔、富如魏、国信王、怡娟陆、迪内·弗洛伦西奥、查张、万象车等。2020b年。Layoutlmv2：多模态预训练用于视觉丰富文档理解。*arXiv
    预印本 arXiv:2012.14740*（2020年）。
- en: Yang et al. (2017) Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel
    Kifer, and C Lee Giles. 2017. Learning to extract semantic structure from documents
    using multimodal fully convolutional neural networks. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 5315–5324.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2017) 杨晓、厄尔辛·尤梅尔、保罗·阿森特、迈克·克拉利、丹尼尔·基弗和C·李·吉尔斯。2017年。利用多模态全卷积神经网络从文档中提取语义结构。发表于*IEEE计算机视觉与模式识别会议论文集*。5315–5324页。
- en: 'Yao et al. (2021) Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu,
    Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. FILIP: Fine-grained
    Interactive Language-Image Pre-Training. In *International Conference on Learning
    Representations*.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2021) 乐伟姚、润辉黄、卢厚、关松陆、敏哲牛、航徐、晓丹梁、正国李、新江和春晶徐。2021年。FILIP：细粒度交互式语言-图像预训练。发表于*国际学习表征会议*。
- en: Yu and Koltun (2015) Fisher Yu and Vladlen Koltun. 2015. Multi-scale context
    aggregation by dilated convolutions. *arXiv preprint arXiv:1511.07122* (2015).
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu and Koltun (2015) 渔夫·余和弗拉德伦·科尔顿。2015年。通过膨胀卷积的多尺度上下文聚合。*arXiv 预印本 arXiv:1511.07122*（2015年）。
- en: 'Yu et al. (2021) Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, and Rong Xiao.
    2021. PICK: processing key information extraction from documents using improved
    graph learning-convolutional networks. In *2020 25th International Conference
    on Pattern Recognition (ICPR)*. IEEE, 4363–4370.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2021) 文文·余、宁·陆、献标·齐、平·龚、和荣·肖。2021。PICK：利用改进的图学习-卷积网络处理文档中的关键信息提取。发表于*2020年第25届国际模式识别会议（ICPR）*。IEEE，4363–4370。
- en: 'Yu et al. (2022) Yuechen Yu, Yulin Li, Chengquan Zhang, Xiaoqiang Zhang, Zengyuan
    Guo, Xiameng Qin, Kun Yao, Junyu Han, Errui Ding, and Jingdong Wang. 2022. StrucTexTv2:
    Masked Visual-Textual Prediction for Document Image Pre-training. In *The Eleventh
    International Conference on Learning Representations*.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2022) 越臣·余、雨林·李、承全·张、晓强·张、增源·郭、霞梦·秦、昆尧·阮、俊宇·韩、尔瑞·丁、和京东·王。2022。StrucTexTv2：用于文档图像预训练的遮罩视觉-文本预测。发表于*第十一届国际学习表示会议*。
- en: 'Zhai et al. (2023) Mingliang Zhai, Yulin Li, Xiameng Qin, Chen Yi, Qunyi Xie,
    Chengquan Zhang, Kun Yao, Yuwei Wu, and Yunde Jia. 2023. Fast-StrucTexT: an efficient
    hourglass transformer with modality-guided dynamic token merge for document understanding.
    In *Proceedings of the Thirty-Second International Joint Conference on Artificial
    Intelligence*. 5269–5277.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai et al. (2023) 明亮·翟、雨林·李、霞梦·秦、晨意·陈、群义·谢、承全·张、昆尧·阮、宇伟·吴、和云德·贾。2023。Fast-StrucTexT：一种高效的沙漏变换器，结合模态引导的动态标记合并用于文档理解。发表于*第三十二届国际人工智能联合会议论文集*。5269–5277。
- en: 'Zhang et al. (2020) Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing
    Lu, Liang Qiao, Yi Niu, and Fei Wu. 2020. TRIE: end-to-end text reading and information
    extraction for document understanding. In *Proceedings of the 28th ACM International
    Conference on Multimedia*. 1413–1422.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020) 鹏·张、云璐·徐、展展·程、士亮·浦、静·卢、亮·乔、易·牛、和飞·吴。2020。TRIE：端到端文本阅读和信息提取用于文档理解。发表于*第28届ACM国际多媒体会议论文集*。1413–1422。
- en: Zhang et al. (2021) Yue Zhang, Zhang Bo, Rui Wang, Junjie Cao, Chen Li, and
    Zuyi Bao. 2021. Entity Relation Extraction as Dependency Parsing in Visually Rich
    Documents. In *Proceedings of the 2021 Conference on Empirical Methods in Natural
    Language Processing*. 2759–2768.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021) 越·张、张博、锐·王、俊杰·曹、晨·李、和祖义·包。2021。视觉丰富文档中的实体关系提取作为依赖解析。发表于*2021年自然语言处理实证方法会议论文集*。2759–2768。
- en: Zhang et al. (2022) Zhenrong Zhang, Jiefeng Ma, Jun Du, Licheng Wang, and Jianshu
    Zhang. 2022. Multimodal pre-training based on graph attention network for document
    understanding. *IEEE Transactions on Multimedia* (2022).
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) 振荣·张、杰峰·马、君·杜、力成·王、和建书·张。2022。基于图注意力网络的多模态预训练用于文档理解。*IEEE多媒体学报*（2022）。
- en: 'Zhao et al. (2019) Xiaohui Zhao, Endi Niu, Zhuo Wu, and Xiaoguang Wang. 2019.
    Cutie: Learning to understand documents with convolutional universal text information
    extractor. *arXiv preprint arXiv:1903.12363* (2019).'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2019) 小辉·赵、恩地·牛、卓·吴、和小光·王。2019。Cutie：通过卷积通用文本信息提取器学习理解文档。*arXiv预印本
    arXiv:1903.12363*（2019）。
- en: Zheng et al. (2024) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2024. Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural
    Information Processing Systems* 36 (2024).
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2024) 连敏·郑、伟霖·蒋、颖胜·邢、思远·庄、张浩·吴、永浩·庄、紫林·李、卓涵·李、大成·李、埃里克·邢等。2024。通过mt-bench和聊天机器人竞技场评判llm-as-a-judge。*神经信息处理系统进展*
    36（2024）。
- en: 'Zhong et al. (2019) Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019.
    Publaynet: largest dataset ever for document layout analysis. In *2019 International
    Conference on Document Analysis and Recognition (ICDAR)*. IEEE, 1015–1022.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong et al. (2019) 旭·钟、建斌·唐、和安东尼奥·吉梅诺·耶佩斯。2019。Publaynet：有史以来最大的数据集用于文档布局分析。发表于*2019年国际文档分析与识别会议（ICDAR）*。IEEE，1015–1022。
- en: Zhu et al. (2022) Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang,
    and Tat-Seng Chua. 2022. Towards complex document understanding by discrete reasoning.
    In *Proceedings of the 30th ACM International Conference on Multimedia*. 4857–4866.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2022) 风斌·朱、文强·雷、富力·冯、超·王、浩洲·张、和达成·蔡。2022。通过离散推理迈向复杂文档理解。发表于*第30届ACM国际多媒体会议论文集*。4857–4866。
- en: 'Zhu et al. (2021a) Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo
    Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021a. TAT-QA: A Question Answering
    Benchmark on a Hybrid of Tabular and Textual Content in Finance. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers)*. 3277–3287.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2021a) Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo
    Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021a. TAT-QA: 融合表格与文本内容的金融领域问答基准。在*第59届计算语言学协会年会及第11届国际自然语言处理联合会议（第1卷：长论文）*中。3277–3287。'
- en: 'Zhu et al. (2021b) Zihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu, and
    Qi Wu. 2021b. Mucko: multi-layer cross-modal knowledge reasoning for fact-based
    visual question answering. In *Proceedings of the Twenty-Ninth International Conference
    on International Joint Conferences on Artificial Intelligence*. 1097–1103.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2021b) Zihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu, and
    Qi Wu. 2021b. Mucko: 多层次跨模态知识推理用于基于事实的视觉问答。在*第29届国际人工智能联合会议*中。1097–1103。'
