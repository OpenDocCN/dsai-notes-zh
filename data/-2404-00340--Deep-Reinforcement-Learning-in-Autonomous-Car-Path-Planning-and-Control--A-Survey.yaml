- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:33:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:33:33'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2404.00340] Deep Reinforcement Learning in Autonomous Car Path Planning and
    Control: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2404.00340] 深度强化学习在自动驾驶车辆路径规划和控制中的应用综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00340](https://ar5iv.labs.arxiv.org/html/2404.00340)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00340](https://ar5iv.labs.arxiv.org/html/2404.00340)
- en: 'Deep Reinforcement Learning in Autonomous Car Path Planning and Control: A
    Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在自动驾驶车辆路径规划和控制中的应用综述
- en: Yiyang Chen¹², Chao Ji¹³, Yunrui Cai¹², Tong Yan¹, Bo Su^(1∗)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 陈一扬¹²，贾超¹³，蔡云瑞¹²，闵通¹，苏博^(1∗)
- en: ¹China North Artificial Intelligence and Innovation Research Institute, Beijing,
    100072, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹中国北方人工智能与创新研究院，北京，100072，中国
- en: ²Department of Precision Instruments, Tsinghua University, Beijing, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²清华大学精密仪器系，北京，中国
- en: ³Department of Automation, Tsinghua University, Beijing, China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³清华大学自动化系，北京，中国
- en: Email:chenyy22@mails.tsinghua.edu.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'Email: chenyy22@mails.tsinghua.edu.cn'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Combining data-driven applications with control systems plays a key role in
    recent Autonomous Car research. This thesis offers a structured review of the
    latest literature on Deep Reinforcement Learning (DRL) within the realm of autonomous
    vehicle Path Planning and Control. It collects a series of DRL methodologies and
    algorithms and their applications in the field, focusing notably on their roles
    in trajectory planning and dynamic control. In this review, we delve into the
    application outcomes of DRL technologies in this domain. By summarizing these
    literatures, we highlight potential challenges, aiming to offer insights that
    might aid researchers engaged in related fields.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动应用与控制系统的结合在近期的自动驾驶汽车研究中发挥了关键作用。本文提供了有关深度强化学习（DRL）在自动驾驶车辆路径规划和控制领域的最新文献的结构化综述。它汇集了一系列DRL方法和算法及其在该领域的应用，特别关注它们在轨迹规划和动态控制中的作用。在此综述中，我们*深入探讨*了DRL技术在该领域的应用成果。通过总结这些文献，我们突出潜在挑战，旨在提供对从事相关领域研究的学者可能有帮助的见解。
- en: '*K*eywords Deep Reinforcement Learning (DRL)  $\cdot$ Autonomous Vehicles  $\cdot$
    Path Planning  $\cdot$ Automotive Control  $\cdot$ path tracking'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*eywords 深度强化学习（DRL）  $\cdot$ 自动驾驶车辆  $\cdot$ 路径规划  $\cdot$ 汽车控制  $\cdot$
    路径跟踪'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: As autonomous driving technology rapidly advances, its potential to relieve
    drivers, enhance traffic efficiency, reduce energy consumption, and improve road
    safety is increasingly being recognized[[1](#bib.bib1)]. At present, advancements
    in autonomous vehicle control technologies are chiefly derived from the integration
    of Advanced Driver Assistance Systems (ADAS), including Adaptive Cruise Control
    (ACC), Lane Keeping Assistance Systems, and Lane Departure Warning technologies,
    which have been implemented in a variety of commercial electric vehicles. Projects
    such as Google’s Waymo and Baidu’s Apollo have advanced towards commercial operations,
    achieving autonomous driving capabilities and launching unmanned vehicle rental
    services in designated areas.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自动驾驶技术的迅速进步，其在减轻驾驶员负担、提高交通效率、降低能耗和改善道路安全方面的潜力越来越被认可[[1](#bib.bib1)]。目前，自动驾驶车辆控制技术的进步主要来自于先进驾驶辅助系统（ADAS）的整合，包括自适应巡航控制（ACC）、车道保持辅助系统和车道偏离警告技术，这些技术已在各种商用电动汽车中得到应用。诸如谷歌的Waymo和百度的Apollo等项目已向商业运营迈进，实现了自动驾驶能力，并在指定区域推出了无人驾驶车辆租赁服务。
- en: 'The control framework of autonomous vehicles fundamentally encompasses three
    tiers: perception, planning, and control, with Figure 1 [[2](#bib.bib2)] depicting
    the comprehensive architecture of autonomous driving systems. The perception layer
    is tasked with the accurate perception and processing of measurement data to produce
    dependable state estimates essential for precise localization and environmental
    recognition. The planning layer is dedicated to task planning, behavioral planning,
    and path planning, ensuring the vehicle is capable of making judicious decisions
    within intricate environments. The control layer concentrates on sustaining vehicle
    stability and adhering precisely to the pre-determined trajectory. The success
    in perception and prediction is largely attributable to substantial advancements
    in the field of machine vision in recent years. However, the planning and control
    segments depend on developers setting parameters for the hierarchical controller
    and conducting fine-tuning through simulations and on-site testing, a traditional
    methodology that presents distinct limitations, particularly in parameter adjustment
    and adaptation to new settings, proving to be both time-consuming and inefficient.
    Moreover, given the highly nonlinear characteristics of the driving process, control
    strategies dependent on the linearization of vehicle models or other algebraic
    solutions encounter challenges regarding implementation and scalability[[3](#bib.bib3)].'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自主车辆的控制框架从根本上包括三个层级：感知、规划和控制，图1 [[2](#bib.bib2)] 展示了自主驾驶系统的全面架构。感知层负责准确感知和处理测量数据，以产生可靠的状态估计，这对于精确定位和环境识别至关重要。规划层专注于任务规划、行为规划和路径规划，确保车辆能够在复杂环境中做出明智的决策。控制层则集中于保持车辆稳定性，并严格遵循预定轨迹。感知和预测的成功在很大程度上归功于近年来机器视觉领域的重大进展。然而，规划和控制部分依赖于开发者为层级控制器设定参数，并通过仿真和现场测试进行微调，这是一种传统方法，存在明显的局限性，尤其是在参数调整和适应新环境方面，表现为既耗时又低效。此外，由于驾驶过程的高度非线性特征，依赖于车辆模型线性化或其他代数解决方案的控制策略在实施和扩展性方面面临挑战[[3](#bib.bib3)]。
- en: '![Refer to caption](img/a9b924cb3830684c014c7cdf46d64884.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a9b924cb3830684c014c7cdf46d64884.png)'
- en: 'Figure 1: Control architecture for automated vehicles[[2](#bib.bib2)].'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：自动化车辆的控制架构[[2](#bib.bib2)]。
- en: Reinforcement Learning (RL) is founded upon the Markov Decision Process (MDP),
    enabling the continual execution of actions in response to a system of rewards
    and punishments. The integration of reinforcement learning with deep learning,
    termed Deep Reinforcement Learning (DRL), signifies the cutting edge in learning
    frameworks for control systems. Deep Reinforcement Learning (DRL) is adept at
    approximating highly non-linear functions from complex datasets, addressing the
    complex control challenges within various subfields of autonomous driving, such
    as behavioral decision-making, energy management, and traffic flow control. Within
    the realm of behavioral decision-making, advanced planning and behavioral prediction
    employ Deep Reinforcement Learning (DRL) and Deep Inverse Reinforcement Learning
    to navigate and make decisions within dynamic and uncertain environments[[4](#bib.bib4)].
    Furthermore, the application of multi-agent DRL in the management and optimization
    of traffic flows presents a promising avenue for addressing and mitigating traffic
    congestion issues, thus enhancing overall traffic efficiency. This innovative
    application of DRL not only underscores its versatility but also demonstrates
    its potential in orchestrating complex traffic dynamics, paving the way for the
    creation of more streamlined and efficient transportation systems[[5](#bib.bib5)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）基于马尔可夫决策过程（MDP），使得可以在奖励和惩罚系统的响应下持续执行动作。将强化学习与深度学习结合，称为深度强化学习（DRL），标志着控制系统学习框架的前沿。深度强化学习（DRL）擅长从复杂的数据集中近似高度非线性函数，解决了自主驾驶各个子领域中的复杂控制挑战，如行为决策、能源管理和交通流量控制。在行为决策领域，高级规划和行为预测采用深度强化学习（DRL）和深度逆向强化学习来在动态和不确定的环境中进行导航和决策[[4](#bib.bib4)]。此外，将多智能体DRL应用于交通流量的管理和优化，为解决和缓解交通拥堵问题提供了一个有前景的途径，从而提高了整体交通效率。这种DRL的创新应用不仅凸显了其多样性，还展示了其在协调复杂交通动态中的潜力，为创建更高效的交通系统铺平了道路[[5](#bib.bib5)]。
- en: In the realm of motion planning and control, the dynamic and uncertain nature
    of driving environments necessitates sophisticated strategies capable of adapting
    to real-time changes while ensuring optimal path selection. The integration of
    Deep Reinforcement Learning (DRL) has been demonstrated to effectively navigate
    these complexities, providing evolving solutions through continual environmental
    learning[[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]. For instance, in the
    context of autonomous vehicle planning problems grounded in stochastic MDPs, DRL
    facilitates the simulation of interactions between vehicles and their environment,
    learning optimal strategies like overtaking and tailgating. This approach simultaneously
    takes into account the vehicle’s status, destination, and potential obstacles,
    thus ensuring the accuracy of path planning and passenger comfort. While Deep
    Reinforcement Learning (DRL) presents significant advantages, the challenges of
    effectively learning from sparse, high-dimensional sensory inputs and ensuring
    safety and robustness in actual driving scenarios persist.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在运动规划和控制领域，驾驶环境的动态性和不确定性要求采取复杂的策略，这些策略能够适应实时变化，同时确保最佳路径选择。深度强化学习（DRL）的整合已被证明能够有效应对这些复杂性，通过持续的环境学习提供不断演变的解决方案[[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)]。例如，在基于随机马尔可夫决策过程（MDP）的自动驾驶车辆规划问题中，DRL促进了车辆与环境之间互动的模拟，学习超车和跟车等最佳策略。这种方法同时考虑了车辆的状态、目的地和潜在障碍，从而确保路径规划的准确性和乘客的舒适度。尽管深度强化学习（DRL）具有显著优势，但从稀疏的高维传感器输入中有效学习并确保实际驾驶场景中的安全性和鲁棒性仍然面临挑战。
- en: This paper centers on local path planning and motion control within autonomous
    driving, examining the merits and current landscape of Reinforcement Learning
    (RL) and Deep Reinforcement Learning (DRL) by gathering, assessing, and synthesizing
    pertinent literature to underscore significant accomplishments and burgeoning
    research avenues. Concurrently, through meticulous analysis, this work aims to
    illuminate the pivotal role of RL in the advancement of autonomous driving technologies,
    particularly in the realm of precise trajectory planning and control, while also
    identifying unresolved challenges and prospective research directions within the
    current domain.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点讨论自动驾驶中的局部路径规划和运动控制，通过收集、评估和综合相关文献，检视强化学习（RL）和深度强化学习（DRL）的优点及现状，以突出重要成就和新兴研究方向。同时，通过细致分析，本研究旨在阐明RL在自动驾驶技术进步中的关键作用，特别是在精确轨迹规划和控制领域，同时识别当前领域内未解决的问题和潜在的研究方向。
- en: To ascertain the pivotal articles for this investigation, a systematic review
    procedure as illustrated in Figure 2 was utilized. Initially, through keyword-based
    topic searches on the Web of Science database, LLM tools, and word embedding techniques,
    a preliminary filtration was performed, retaining about 500 articles that integrate
    Reinforcement Learning (RL) with autonomous driving trajectory control. Subsequently,
    a preliminary categorization of the research domains was conducted based on the
    abstracts of each article, retaining those pertinent to trajectory planning and
    motion control, thus reducing the count to 180 articles. Within the selected cohort
    of 180 articles, a comprehensive review of the full texts was undertaken, selecting
    45 of the most recent and pertinent articles. In this review article, a recapitulation
    of the latest developments in the application of RL to trajectory planning and
    control is provided, based on these 45 articles. Furthermore, an additional 50+
    reference articles were incorporated to encompass descriptions of the most advanced
    RL methodologies and delineate issues related to autonomous driving trajectory
    planning and control.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定本研究的关键文献，使用了如图2所示的系统评价程序。最初，通过在Web of Science数据库、LLM工具和词嵌入技术上的关键词主题搜索，进行了初步筛选，保留了大约500篇将强化学习（RL）与自动驾驶轨迹控制相结合的文章。随后，基于每篇文章的摘要进行了初步分类，保留了与轨迹规划和运动控制相关的文章，将数量减少到180篇。在选定的180篇文章中，进行了全面的全文审查，选出了45篇最新且相关的文章。在本综述文章中，根据这45篇文章总结了RL在轨迹规划和控制应用中的最新进展。此外，还纳入了50篇以上的参考文献，以涵盖最先进的RL方法并描述与自动驾驶轨迹规划和控制相关的问题。
- en: '![Refer to caption](img/8aee673fec3982fa562eb6f5210ef2c9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8aee673fec3982fa562eb6f5210ef2c9.png)'
- en: 'Figure 2: Methodology of screening and selecting papers in this review paper.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：本综述论文中筛选和选择文献的方法。
- en: Chapter 2 of this paper will succinctly introduce mainstream reinforcement learning
    methods and their applicability across a spectrum of autonomous driving challenges.
    Chapter 3 delves into trajectory planning issues, encompassing the challenges
    of established methodologies and detailing the application of reinforcement learning
    methods tailored to specific scenarios. Chapter 4 explores lateral and longitudinal
    control challenges, along with the implementation of reinforcement learning in
    addressing these issues. Chapter 5 presents the recently popular end-to-end approaches,
    wherein reinforcement learning agents integrate path planning with motion control.
    Finally, a summary and discussion of the current literature on DRL methods will
    be provided, highlighting the challenges and potential future research directions
    for DRL’s further application in the sub-domains of trajectory planning and motion
    control.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的第二章将简明扼要地介绍主流的强化学习方法及其在各种自动驾驶挑战中的适用性。第三章将深入探讨轨迹规划问题，包括现有方法的挑战，并详细说明针对特定场景的强化学习方法的应用。第四章探讨横向和纵向控制挑战，以及强化学习在解决这些问题中的实施。第五章介绍了最近流行的端到端方法，其中强化学习智能体将路径规划与运动控制整合在一起。最后，将提供对当前文献中DRL方法的总结和讨论，突出DRL在轨迹规划和运动控制子领域进一步应用的挑战和潜在未来研究方向。
- en: 2 Overview of DRL in Autonomous Driving
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 自动驾驶中的DRL概述
- en: 2.1 Introduction to Reinforcement Learning Algorithms
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 强化学习算法简介
- en: RL is a subset of machine learning in which an agent learns to make decisions
    through the execution of actions within an environment to attain a specified objective.
    The core of RL lies in the interaction between the agent and its environment,
    facilitated through a trial-and-error process. The agent receives feedback via
    rewards, directing it towards a strategy that optimizes the accumulation of rewards
    over time. This learning paradigm is distinguished by its focus on learning the
    optimal action selection strategy, without requiring a pre-defined environmental
    model, rendering it particularly apt for complex and dynamically uncertain applications
    like autonomous driving.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: RL是机器学习的一个子集，其中智能体通过在环境中执行行动来学习决策，以实现指定目标。RL的核心在于智能体与其环境之间的互动，这一过程通过试错法来实现。智能体通过奖励获得反馈，引导其采取优化长期奖励积累的策略。这种学习范式的特点在于其专注于学习最佳行动选择策略，而不需要预定义的环境模型，使其特别适合于复杂和动态不确定的应用，如自动驾驶。
- en: Within the context of autonomous driving, particularly regarding motion planning
    and control, the Markov Decision Process (MDP) offers a foundational framework
    for modeling the decision-making process. The MDP is delineated by its states,
    actions, transition probabilities, and rewards. States can encompass a broad array
    of information, including the vehicle’s current speed, position, direction, and
    the surrounding environmental conditions, like the proximity to other vehicles.
    Actions denote the comprehensive set of operations a vehicle can execute, such
    as accelerating, braking, or turning at various angles. The MDP framework presupposes
    that future states are contingent solely upon the current state and action, adhering
    to the Markov property. This assumption simplifies the complexity involved in
    forecasting future scenarios based on the entirety of historical data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶的背景下，特别是关于运动规划和控制方面，马尔可夫决策过程（MDP）提供了一个建模决策过程的基础框架。MDP由其状态、行动、转移概率和奖励组成。状态可以包含广泛的信息，包括车辆的当前速度、位置、方向以及周围环境条件，如与其他车辆的距离。行动指的是车辆可以执行的全面操作集合，如加速、制动或在不同角度转弯。MDP框架假设未来状态仅依赖于当前状态和行动，遵循马尔可夫性质。这一假设简化了基于历史数据整体预测未来场景的复杂性。
- en: '![Refer to caption](img/cc81eac983f501d524a74a5b065c64d0.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cc81eac983f501d524a74a5b065c64d0.png)'
- en: 'Figure 3: MDP for RL loop[[5](#bib.bib5)].'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：RL循环中的MDP[[5](#bib.bib5)]。
- en: The reward function plays a pivotal role in Reinforcement Learning (RL), as
    it directly influences the agent’s behavior by incentivizing actions that guide
    towards desired outcomes. Within the context of autonomous driving, the reward
    function can be crafted to prioritize aspects such as safety, efficiency, and
    compliance with traffic regulations. For instance, rewards can be allocated for
    behaviors such as maintaining a safe distance from other vehicles, reaching the
    destination within a designated time frame, or minimizing fuel consumption. Conversely,
    penalties might be imposed for actions deemed dangerous, such as speeding or deviating
    from the planned route. The state transition function, or the environment within
    the MDP, is not entirely unknown in the realm of autonomous driving applications.
    This entails the modeling of the vehicle’s kinematics and dynamics, as well as
    the surrounding road conditions. Given the unpredictability of actual driving
    conditions, this can be exceedingly complex, leading many RL methodologies to
    opt for not considering the specific vehicle model, thus adopting a model-free
    approach.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数在强化学习（RL）中发挥着关键作用，因为它通过激励那些引导达到期望结果的行为直接影响代理的行为。在自动驾驶的背景下，奖励函数可以被设计为优先考虑安全、效率以及遵守交通规则等方面。例如，可以为保持与其他车辆的安全距离、在规定时间内到达目的地或减少燃料消耗等行为分配奖励。相反，对于如超速或偏离计划路线等被认为危险的行为，可能会施加惩罚。在自主驾驶应用领域，状态转移函数或MDP中的环境并非完全陌生。这涉及到对车辆动力学和运动学以及周围道路条件的建模。鉴于实际驾驶条件的不可预测性，这可能非常复杂，导致许多RL方法选择不考虑具体的车辆模型，从而采用无模型的方法。
- en: Approximate Dynamic Programming (ADP) is a concept fundamentally recognized
    in Reinforcement Learning (RL), where exploration is integrated into Dynamic Programming
    (DP) to identify an approximate optimal strategy for addressing problems with
    large state or action spaces, wherein an exact solution is computationally unviable.
    Through the approximation of value functions or policies, ADP methods can furnish
    scalable solutions to the complex challenges encountered in autonomous driving.
    These methodologies are particularly invaluable in refining control strategies
    and motion planning algorithms, especially when precise state transition dynamics
    are either unknown or too complex for explicit modeling.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 近似动态规划（ADP）是强化学习（RL）中一个基本概念，其中探索被整合到动态规划（DP）中，以识别解决大状态或动作空间问题的近似最优策略，其中精确解法在计算上不可行。通过对价值函数或策略的近似，ADP方法可以为自动驾驶中遇到的复杂挑战提供可扩展的解决方案。这些方法在优化控制策略和运动规划算法时尤为宝贵，特别是在精确的状态转移动态未知或过于复杂以至于无法显式建模的情况下。
- en: The most prevalent categorization within model-free Deep Reinforcement Learning
    (DRL) distinguishes between value-based and policy-based learning approaches.
    Deep Q-Learning (DQN) integrates deep learning with Q-learning, employing deep
    neural networks to approximate the Q-function, assessing the value of disparate
    actions within the current state. This approach enables DQN to manage environments
    characterized by high-dimensional state spaces, including video games and autonomous
    driving simulators. The implementation of DQN involves the collection and utilization
    of data from environmental interactions to train neural networks, where experience
    replay and target networks serve as crucial stabilizing elements. DQN’s capacity
    to handle complex decision-making processes renders it highly suitable for applications
    within dynamic and visually rich environments.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在无模型深度强化学习（DRL）中，最常见的分类方法区分了基于价值的方法和基于策略的方法。深度Q学习（DQN）将深度学习与Q学习结合，采用深度神经网络来近似Q函数，评估当前状态下不同行为的价值。这种方法使得DQN能够管理具有高维状态空间的环境，包括视频游戏和自动驾驶模拟器。DQN的实现涉及从环境交互中收集和利用数据来训练神经网络，其中经验回放和目标网络作为关键的稳定元素。DQN处理复杂决策过程的能力使其非常适合在动态和视觉丰富的环境中应用。
- en: 'Enhancements to the foundational DQN architecture, including Double DQN, Dueling
    DQN, and the adoption of target networks, have ameliorated challenges associated
    with training convergence. Three prevalent refinements exist: Double DQN reduces
    overestimation bias by decoupling action selection from Q-value evaluation. Dueling
    DQN augments action evaluation by distinguishing between state value and action
    advantage estimations, furnishing a more detailed comprehension of the impacts
    of actions. Target networks enhance training stability by moderating the pace
    of Q-value updates, thereby mitigating fluctuations in learning dynamics.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对基础 DQN 架构的增强，包括 Double DQN、Dueling DQN 和目标网络的采用，已经改善了与训练收敛相关的挑战。三种常见的改进如下：Double
    DQN 通过将动作选择与 Q 值评估解耦来减少过高估计偏差。Dueling DQN 通过区分状态值和动作优势估计来增强动作评估，从而提供对动作影响的更详细理解。目标网络通过调节
    Q 值更新的速度来提高训练的稳定性，从而减少学习动态中的波动。
- en: Policy learning approaches, by facilitating a direct mapping from states to
    actions, allow agents to ascertain a probability distribution over actions for
    a given state, diverging from value-based learning approaches which indirectly
    choose actions to maximize expected returns. Policy Gradient (PG) represents a
    foundational method within policy learning, aiming to enhance expected returns
    through the optimization of policy network parameters. This approach is particularly
    apt for scenarios characterized by continuous action spaces or high dimensions,
    as seen in robot control and complex decision-making tasks. Policy Gradient facilitates
    learning by directly modifying policy parameters to amplify the likelihood of
    beneficial actions and diminish the probability of detrimental actions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 策略学习方法通过实现状态到动作的直接映射，使智能体能够为给定状态确定动作的概率分布，这与基于价值的学习方法通过间接选择动作以最大化预期回报的方式不同。策略梯度（PG）是策略学习中的一种基础方法，旨在通过优化策略网络参数来提高预期回报。这种方法特别适用于连续动作空间或高维场景，例如机器人控制和复杂决策任务。策略梯度通过直接修改策略参数来促进学习，以增加有益动作的概率并减少有害动作的概率。
- en: Advanced policy learning algorithms, such as Proximal Policy Optimization (PPO)[[9](#bib.bib9)]
    and Trust Region Policy Optimization (TRPO)[[10](#bib.bib10)], are designed to
    tackle the stability and efficiency challenges faced by policy gradient methods
    in practical implementations. PPO enhances the training process’s stability by
    constraining the magnitude of policy updates, whereas TRPO guarantees updates
    do not diverge excessively by incorporating a trust region within the optimization
    process. These techniques offer means to effectively update policies while preserving
    learning stability, particularly apt for application scenarios necessitating meticulous
    control over policy update steps.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 高级策略学习算法，如近端策略优化（PPO）[[9](#bib.bib9)] 和信赖域策略优化（TRPO）[[10](#bib.bib10)]，旨在解决策略梯度方法在实际应用中面临的稳定性和效率挑战。PPO
    通过限制策略更新的幅度来增强训练过程的稳定性，而 TRPO 通过在优化过程中引入信赖域来保证更新不会过度发散。这些技术提供了有效更新策略的手段，同时保持学习的稳定性，特别适用于需要对策略更新步骤进行细致控制的应用场景。
- en: 'Value-based approaches face challenges in effectively generating actions within
    large action spaces, and the training of policy networks is not readily amenable
    to temporal difference and experience replay methods, resulting in convergence
    difficulties. The Actor-Critic methodology merges policy and value learning, comprising
    two models: the Actor, responsible for generating actions, and the Critic, tasked
    with evaluating the value of these actions. This architecture is designed to harness
    the advantages of value learning to guide policy learning, enhancing the directionality
    and efficiency of the learning process. The Actor-Critic approach exhibits distinct
    advantages when addressing issues involving partially observable or high-dimensional
    state spaces.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的方法在大动作空间内生成有效动作时面临挑战，并且策略网络的训练不易适应时序差分和经验重放方法，导致收敛困难。Actor-Critic 方法结合了策略学习和价值学习，由两个模型组成：Actor
    负责生成动作，Critic 负责评估这些动作的价值。该架构旨在利用价值学习的优势来指导策略学习，从而提高学习过程的方向性和效率。Actor-Critic 方法在处理部分可观察或高维状态空间的问题时表现出明显的优势。
- en: '![Refer to caption](img/bae7aa5f283369eaf8b9c9cb8de35445.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bae7aa5f283369eaf8b9c9cb8de35445.png)'
- en: 'Figure 4: Actor Critic method[[5](#bib.bib5)].'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Actor Critic 方法[[5](#bib.bib5)]。
- en: Enhancements to the Actor-Critic methodology encompass Deep Deterministic Policy
    Gradient (DDPG)[[11](#bib.bib11)], Twin Delayed DDPG (TD3)[[12](#bib.bib12)],
    and Soft Actor-Critic (SAC). DDPG is apt for continuous action spaces, melding
    the merits of deep learning and policy gradient approaches. The employment of
    a "deterministic" policy markedly enhances sampling efficiency throughout training,
    more effectively directing the network’s gradient update trajectory. TD3 mitigates
    overestimation bias via the integration of a twin network structure and delayed
    update mechanisms, whereas SAC fosters broader exploration of state spaces among
    agents through entropy regularization, thereby enhancing the stability and robustness
    of learning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Actor-Critic 方法的增强包括深度确定性策略梯度（DDPG）[[11](#bib.bib11)]、双延迟 DDPG（TD3）[[12](#bib.bib12)]
    和软 Actor-Critic（SAC）。DDPG 适用于连续动作空间，结合了深度学习和策略梯度方法的优点。“确定性”策略的应用显著提高了训练过程中的采样效率，更有效地引导了网络的梯度更新轨迹。TD3
    通过整合双重网络结构和延迟更新机制来减轻过度估计偏差，而 SAC 通过熵正则化促进代理在状态空间的更广泛探索，从而提高学习的稳定性和鲁棒性。
- en: In the domain of continuous control, reinforcement learning encounters the significant
    challenge of high-quality sample scarcity, markedly elevating the complexity of
    training. An efficacious strategy is the adoption of an experience replay mechanism[[13](#bib.bib13)],
    enabling the algorithm to store and subsequently reuse encountered transitions
    (encompassing states, actions, rewards, and new states) multiple times throughout
    training. This approach significantly boosts sample utilization efficiency, diminishes
    learning process variance, and permits batch updating of non-continuous experiences.
    Building upon this, Deep q-learning from demonstrations (DQfD) [[14](#bib.bib14)]
    further broadens the application scope of experience replay, not merely reutilizing
    experiences generated by the agent itself but also specifically incorporating
    mechanisms for learning from expert demonstrations. DQfD significantly hastens
    the learning pace by amalgamating temporal difference updates with the supervised
    classification of expert actions, even when based on limited demonstration data.
    Furthermore, through the prioritized replay mechanism, DQfD can adaptively modulate
    the usage ratio between demonstration data and self-generated data throughout
    the learning process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在连续控制领域，强化学习面临着高质量样本稀缺的重大挑战，显著增加了训练的复杂性。一种有效的策略是采用经验回放机制[[13](#bib.bib13)]，使算法能够在训练过程中多次存储和重用遇到的转移（包括状态、动作、奖励和新状态）。这种方法显著提升了样本利用效率，减少了学习过程的方差，并允许对非连续经验进行批量更新。在此基础上，基于演示的深度
    Q 学习（DQfD）[[14](#bib.bib14)] 进一步扩展了经验回放的应用范围，不仅重新利用由代理自身生成的经验，还特别引入了从专家演示中学习的机制。DQfD
    通过将时间差分更新与专家动作的监督分类相结合，显著加快了学习速度，即使在演示数据有限的情况下也能有效学习。此外，通过优先回放机制，DQfD 能够在学习过程中自适应地调整演示数据与自生成数据之间的使用比例。
- en: An innovative approach for accelerating training entails initially constructing
    a policy based on existing data through the supervised learning of the policy
    network, followed by further optimization and enhancement of this policy via reinforcement
    learning[[15](#bib.bib15)]. This approach leverages supervised learning for a
    rapid initiation of the learning process and employs reinforcement learning for
    policy refinement, thereby enhancing the efficiency of the learning trajectory.
    It is particularly well-suited for contexts with ample labeled data, facilitating
    swift attainment of superior performance levels.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一种加速训练的创新方法是最初基于现有数据构建一个策略，通过对策略网络的监督学习进行，然后通过强化学习进一步优化和提升这个策略[[15](#bib.bib15)]。这种方法利用监督学习快速启动学习过程，并通过强化学习进行策略优化，从而提高学习轨迹的效率。它特别适合于数据标注充足的情境，能够迅速达到更高的性能水平。
- en: Beyond model-free RL, Model-Based RL considerably enhances training efficiency[[16](#bib.bib16)].
    Model-Based RL merges the viewpoints of control theory and reinforcement learning,
    utilizing introduced or fitted environmental models to forecast state transitions,
    thus facilitating more precise network parameter updates within simulation environments.
    From the perspective of control, neural networks are employed to fit dynamic models
    (unknown model calibration), acquiring state transition functions and supplying
    nominal control strategies for intricate control tasks. From the reinforcement
    learning viewpoint, policy updates are conducted directly through the fitted environmental
    model, optimizing objectives with simulator-generated data to boost solution efficiency
    and diminish the necessity for real-environment interaction. The principal challenge
    encountered by model-driven RL is that model inaccuracies may induce data biases,
    particularly over extended periods, where cumulative errors can substantially
    diminish performance. To tackle this issue, current improvements like Model Value
    Expansion (MVE) have been developed, limiting the step length of model predictions
    to curtail cumulative errors[[17](#bib.bib17)]. Moreover, strategies incorporating
    Model-Free RL methods are under investigation to exploit their benefit of direct
    learning from environmental feedback, alleviating the impact of model inaccuracies
    on the learning process and enhancing the stability and efficiency of model-driven
    RL[[18](#bib.bib18)].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 超越无模型强化学习，基于模型的强化学习显著提高了训练效率[[16](#bib.bib16)]。基于模型的强化学习融合了控制理论和强化学习的观点，利用引入或拟合的环境模型来预测状态转移，从而在仿真环境中实现更精确的网络参数更新。从控制的角度看，神经网络用于拟合动态模型（未知模型校准），获取状态转移函数，并为复杂的控制任务提供名义控制策略。从强化学习的角度看，政策更新是通过拟合的环境模型直接进行的，利用模拟器生成的数据优化目标，以提高解决效率并减少对真实环境交互的需求。模型驱动的强化学习面临的主要挑战是模型不准确可能会引入数据偏差，尤其是随着时间的推移，累积误差可能会显著降低性能。为解决这一问题，当前开发了如模型价值扩展（MVE）等改进方法，通过限制模型预测的步长来减少累积误差[[17](#bib.bib17)]。此外，还在研究结合无模型强化学习方法的策略，以利用其直接从环境反馈中学习的优势，缓解模型不准确对学习过程的影响，提高模型驱动的强化学习的稳定性和效率[[18](#bib.bib18)]。
- en: '![Refer to caption](img/05e82ca8ba0de161ce3010718ab6b548.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/05e82ca8ba0de161ce3010718ab6b548.png)'
- en: 'Figure 5: Model-based RL'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 基于模型的强化学习'
- en: 2.2 Review of RL-based Autonomous Driving Research
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基于强化学习的自动驾驶研究综述
- en: '![Refer to caption](img/9a175e7fcad8161ffc9a7cf819b5b079.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9a175e7fcad8161ffc9a7cf819b5b079.png)'
- en: 'Figure 6: RL-based works in Autonomous Car.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 基于强化学习的自动驾驶工作。'
- en: In the domain of autonomous driving, the deployment of Reinforcement Learning
    (RL) is rapidly proliferating, encompassing realms from high-level decision-making
    to specific motion control. An analysis of literature post-2016 reveals that RL
    has achieved notable research advancements across various subdomains of autonomous
    driving. Notably, in the realms of behavioral decision-making (139 papers), traffic
    efficiency (128 papers), motion planning (comprising 74 papers on motion planning,
    12 on trajectory optimization, and 9 on dynamic obstacle avoidance), vehicle-to-everything
    (V2X) communications (59 papers), and control (encompassing 20 papers on lateral
    control, 18 on longitudinal control, and 19 on energy management), the volume
    of papers underscores the research focus and significance of these areas. This
    illustrates the pivotal role of reinforcement learning techniques in deciphering
    and enhancing the complex interactions, decision-making processes, and control
    strategies within autonomous driving systems.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶领域，强化学习（RL）的应用正在迅速扩展，涵盖从高层决策到具体运动控制的多个领域。对2016年后的文献进行分析表明，强化学习在自动驾驶的各个子领域取得了显著的研究进展。特别是在行为决策（139篇论文）、交通效率（128篇论文）、运动规划（包括74篇关于运动规划的论文、12篇关于轨迹优化的论文和9篇关于动态障碍物避让的论文）、车联网（V2X）通信（59篇论文）和控制（包括20篇关于横向控制的论文、18篇关于纵向控制的论文和19篇关于能量管理的论文）领域，论文数量突显了这些领域的研究重点和重要性。这说明了强化学习技术在解读和提升自动驾驶系统中复杂交互、决策过程和控制策略中的关键作用。
- en: Reinforcement learning is capable of learning optimal strategies through exploration
    and exploitation mechanisms within unknown or dynamically changing environments,
    with certain off-policy methods permitting online updates. Given the potential
    for instantaneous changes in road conditions, traffic flow, and the behavior of
    surrounding vehicles, autonomous driving systems must be capable of continuously
    learning from and adapting to new situations. For instance, within the domains
    of behavioral decision-making and traffic efficiency, RL can aid autonomous driving
    systems in assessing various potential actions within complex scenarios, selecting
    the optimal course of action to augment safety and fluidity. In the context of
    multi-agent systems applications, reinforcement learning has unveiled new perspectives
    for research within areas like vehicle-to-everything (V2X) communications and
    traffic flow control. Through multi-agent reinforcement learning, the communication
    and coordination among vehicles can be optimized, thereby enhancing the efficiency
    and safety of the entire traffic system. This capability is particularly crucial
    in highly interactive scenarios, such as intersections and congested roads, where
    multiple vehicles must make joint decisions to avert conflicts and optimize traffic
    flow.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习能够通过探索和利用机制在未知或动态变化的环境中学习最佳策略，某些脱离策略的方法允许在线更新。鉴于道路条件、交通流量和周围车辆行为的瞬时变化，自主驾驶系统必须能够持续学习和适应新情况。例如，在行为决策和交通效率领域，强化学习可以帮助自主驾驶系统评估复杂情境中的各种潜在行动，选择最佳行动方案以提升安全性和流畅性。在多智能体系统应用的背景下，强化学习为如车联网（V2X）通信和交通流量控制等领域的研究揭示了新的视角。通过多智能体强化学习，车辆之间的通信和协调可以得到优化，从而提高整个交通系统的效率和安全性。这种能力在高度互动的场景中尤为重要，如交叉口和拥挤的道路，在这些情况下，多辆车必须做出联合决策以避免冲突并优化交通流量。
- en: Following the analysis of 45 selected recent papers, it has been observed that
    RL algorithms are extensively applied across diverse subdomains of autonomous
    driving. Within the context of path planning challenges, algorithms such as SAC,
    PPO, TD3, DQN, and DDPG are frequently employed, showcasing their robust adaptability
    in this subdomain. Regarding control challenges, particularly in lateral, longitudinal,
    and integrated lateral-longitudinal control tasks, the application of the DDPG
    algorithm is notably pervasive. Simultaneously, model-based DRL and ADP also exhibit
    their efficacy in these contexts. In the arena of end-to-end control, the application
    of algorithms tends to be more scattered, attributable to the fact that end-to-end
    control does not rely exclusively on RL. This reflects a demand for multiple algorithms
    within complex systems integrating perception with action output.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在对45篇近期选定论文的分析中，观察到强化学习算法在自主驾驶的各个子领域中被广泛应用。在路径规划挑战中，SAC、PPO、TD3、DQN和DDPG等算法被频繁使用，展示了它们在这一子领域的强大适应性。在控制挑战方面，特别是在横向、纵向和综合横向-纵向控制任务中，DDPG算法的应用尤为广泛。同时，基于模型的深度强化学习和自适应动态规划在这些背景下也显示出了其有效性。在端到端控制领域，由于端到端控制并不完全依赖于强化学习，算法的应用较为分散，这反映了在将感知与行动输出整合的复杂系统中对多种算法的需求。
- en: '![Refer to caption](img/30071995a51281d8f6891b9f5a52bc40.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/30071995a51281d8f6891b9f5a52bc40.png)'
- en: 'Figure 7: Application of specific RL algorithms to trajectory planning, lateral
    control, longitudinal control and end-to-end control subproblems.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：特定强化学习算法在轨迹规划、横向控制、纵向控制和端到端控制子问题中的应用。
- en: Autonomous driving systems encounter a diverse array of complex challenges across
    various subdomains, with the nature of these challenges dictating the selection
    diversity of RL algorithms. Within path planning, the pronounced attributes of
    dynamism and uncertainty render SAC and PPO algorithms, capable of balancing exploration
    with stability, particularly crucial. These algorithms can effectively navigate
    the optimal path across fluctuating road conditions while maintaining stability
    in the learning process. DDPG, by employing its deterministic policy, delivers
    explicit gradient signals for continuous control issues and diminishes variance
    throughout the training phase, thus showcasing outstanding performance in continuous
    control challenges, including smooth acceleration and precise steering. The challenge
    within end-to-end control revolves around effectively converting high-dimensional
    perceptual data into immediate action decisions. This necessitates algorithms
    capable of not only efficiently learning policies but also managing substantial
    inputs from diverse sensors and devising intricate action sequences.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶系统在各种子领域中遇到多种复杂挑战，这些挑战的性质决定了RL算法的选择多样性。在路径规划中，动态性和不确定性的显著特征使得能够平衡探索与稳定性的SAC和PPO算法尤为重要。这些算法能够有效地在变化的道路条件下导航最佳路径，同时保持学习过程中的稳定性。DDPG通过其确定性策略，为连续控制问题提供明确的梯度信号，并在训练阶段减少方差，从而在平滑加速和精确转向等连续控制挑战中展现出卓越的性能。端到端控制中的挑战在于有效地将高维感知数据转换为即时的行动决策。这需要不仅能有效学习策略的算法，还能处理来自不同传感器的大量输入并制定复杂的行动序列。
- en: 3 DRL for Trajectory Planning
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 DRL用于轨迹规划
- en: Within autonomous driving systems, trajectory planning is crucial, guaranteeing
    that vehicles can transition safely and efficiently from their present location
    to their destination. This chapter will delve into the trajectory planning issue,
    with a particular emphasis on the application of RL methods in addressing this
    challenge.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶系统中，轨迹规划至关重要，确保车辆可以安全、高效地从当前位置过渡到目的地。本章将**深入探讨**轨迹规划问题，特别是**强调**RL方法在解决这一挑战中的应用。
- en: 3.1 Challenges in Trajectory Planning
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 轨迹规划中的挑战
- en: The paramount challenge in trajectory planning within autonomous driving systems
    lies in generating smooth paths that adhere to initial and final conditions as
    well as an array of constraints. Additionally, algorithms must navigate static
    and dynamic obstacle avoidance tasks, foresee future vehicle trajectories, and
    sidestep collisions while tracking arbitrary targets within dynamic environments.
    Spanning from global to local path planning within intricate environments, prevailing
    methodologies encompass random search and sampling, curve interpolation, and numerical
    optimization[[19](#bib.bib19)]. Random search and sampling approaches, notably
    the Rapidly-exploring Random Tree (RRT) algorithm, furnish an efficacious means
    for identifying feasible paths within complex terrains. By engaging in random
    sampling of the configuration or state spaces and examining their connectivity,
    these strategies facilitate swift planning within high-dimensional spaces.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶系统中的轨迹规划面临的首要挑战是生成平滑的路径，这些路径必须遵守初始和最终条件以及一系列约束。此外，算法还必须应对静态和动态障碍物避免任务，预测未来车辆轨迹，并在动态环境中追踪任意目标时避免碰撞。涵盖从复杂环境中的全局到局部路径规划的主流方法包括随机搜索和采样、曲线插值以及数值优化[[19](#bib.bib19)]。随机搜索和采样方法，特别是快速探索随机树（RRT）算法，提供了一种在复杂地形中识别可行路径的有效手段。通过对配置或状态空间进行随机采样并检查其连通性，这些策略促进了在高维空间中的快速规划。
- en: Paths yielded by RRT might be discontinuous and somewhat rudimentary; however,
    the RRT* algorithm substantially enhances path optimality via temporal framework
    optimization, swiftly offering resolutions for intricate driving conditions. To
    address the discontinuity in paths engendered by sampling approaches, curve interpolation
    techniques are employed to forge smooth trajectories. Accounting for variables
    like vehicle dynamics, feasibility, and comfort, these techniques adapt to roadway
    conditions, generating new datapoints via predefined nodes to guarantee trajectory
    continuity and satisfy motion constraints. Helical, polynomial, and Bézier curves
    represent commonly utilized methods capable of consistently delivering high-quality
    solutions. However, within the context of global path planning, despite the ability
    to optimize paths and account for road and vehicle limitations, real-time obstacle
    processing may prove to be considerably time-intensive.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: RRT生成的路径可能会不连续且略显粗糙；然而，RRT*算法通过时间框架优化显著提升路径的最优性，迅速为复杂驾驶条件提供解决方案。为了应对采样方法生成路径的不连续性，采用曲线插值技术来生成平滑轨迹。考虑到诸如车辆动力学、可行性和舒适性等变量，这些技术根据道路条件进行适应，通过预定义节点生成新的数据点，以确保轨迹连续性并满足运动约束。螺旋曲线、多项式曲线和贝塞尔曲线是常用的方法，能够持续提供高质量的解决方案。然而，在全局路径规划的背景下，尽管可以优化路径并考虑道路和车辆的限制，但实时障碍物处理可能会变得相当耗时。
- en: Under dynamically changing environments and constraints, numerical optimization
    methods manifest their distinct advantages. Through the design of an objective
    function and its subsequent minimization or maximization, this approach holistically
    considers vehicle dynamics, environmental data, and constraints in the pursuit
    of optimal control strategies and paths. The meticulous design of the objective
    function is crucial for the success of numerical optimization, and within the
    context of reinforcement learning, it is translated into a reward function, steering
    the algorithm towards learning the optimal path planning strategy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态变化的环境和约束条件下，数值优化方法展现出其独特的优势。通过设计目标函数并进行最小化或最大化，这种方法全面考虑了车辆动力学、环境数据和约束，以寻求最优的控制策略和路径。目标函数的精细设计对数值优化的成功至关重要，在强化学习的背景下，这被转化为奖励函数，引导算法学习最优的路径规划策略。
- en: Dynamic Programming (DP) and Quadratic Programming (QP) represent two prevalent
    solution approaches within the domain of numerical optimization. DP utilizes the
    Bellman optimality principle, iteratively working backwards from the end point
    to ascertain the optimal path, theoretically capable of identifying the global
    optimum. However, its computational complexity escalates substantially with the
    problem size. Quadratic Programming, employed within Model Predictive Control
    (MPC), involves setting the objective function as the sum of squares of state
    equations across a series of forecasting periods, underscoring the real-time nature
    and efficiency of the solution. This renders it more suitable for problems in
    continuous spaces and dynamic environments. This approach is principally solved
    via the interior point or active set methods, effectively managing constraints
    and ensuring that control strategies can be updated in real-time to accommodate
    dynamic shifts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划（DP）和二次规划（QP）是数值优化领域内的两种常见解决方法。DP利用贝尔曼最优性原则，从终点向回迭代工作以确定最优路径，理论上能够识别全局最优解。然而，其计算复杂度随着问题规模的增加而显著上升。二次规划在模型预测控制（MPC）中应用，将目标函数设为一系列预测期内状态方程的平方和，强调了解决方案的实时性和效率。这使其更适用于连续空间和动态环境中的问题。此方法主要通过内点法或活动集方法进行求解，有效管理约束并确保控制策略能够实时更新，以适应动态变化。
- en: 'The flexibility and adaptability of numerical optimization methods in generating
    trajectories enable path planning to directly respond to real-time vehicle states
    and environmental changes. This allows for dynamic adjustments in path and control
    decisions, taking into account constraints pertaining to roads, vehicles, and
    other road users. Consequently, the selection of suitable optimization strategies
    and algorithms, along with the thoughtful design of objective functions, is crucial
    for enhancing the performance of autonomous driving systems. The two principal
    challenges confronting current mainstream solution approaches are: first, the
    vehicle model may be unknown or imprecise, impacting the accuracy of solutions;
    and second, computational efficiency issues. Particularly, the DP method experiences
    a drastic increase in computational load when addressing continuous or high-resolution
    spaces, rendering the solution process exceedingly time-consuming. While MPC mitigates
    complexity by configuring the objective function as a quadratic form and employing
    QP, in scenarios necessitating real-time path planning with control loop frequencies
    demanding up to 100Hz or beyond, QP still necessitates highly optimized algorithms
    and hardware acceleration to satisfy real-time criteria.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数值优化方法在生成轨迹方面的灵活性和适应性使得路径规划能够直接响应实时的车辆状态和环境变化。这允许在路径和控制决策中进行动态调整，同时考虑到与道路、车辆和其他道路使用者相关的约束。因此，选择合适的优化策略和算法，以及精心设计目标函数，对于提升自动驾驶系统的性能至关重要。目前主流解决方案面临的两个主要挑战是：首先，车辆模型可能未知或不准确，影响解决方案的准确性；其次，计算效率问题。特别是，当处理连续或高分辨率空间时，DP
    方法的计算负荷会急剧增加，导致解决过程异常耗时。虽然 MPC 通过将目标函数配置为二次形式并采用 QP 来减轻复杂性，但在需要高达 100Hz 或更高频率的实时路径规划场景中，QP
    仍然需要高度优化的算法和硬件加速以满足实时标准。
- en: 3.2 Implementing DRL for Trajectory Planning
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实施 DRL 进行轨迹规划
- en: In the realm of autonomous driving trajectory planning, while Model Predictive
    Control (MPC) and Dynamic Programming (DP) methodologies have demonstrated their
    efficiency and accuracy across numerous scenarios, their adaptability and flexibility
    are constrained in the face of environmental uncertainties, model inaccuracies,
    and computational resource limitations. In contrast, Deep Reinforcement Learning
    (DRL) significantly enhances planning flexibility through its exceptional adaptability
    to the environment and dynamic learning traits. It generates trajectories via
    value or policy networks, eschewing reliance on predetermined waypoints in favor
    of dynamic decision-making based on environmental states and objectives. Concerning
    trajectory planning challenges, DRL emerges as a promising alternative. We’ve
    collated research on DRL’s specific applications in this arena into Table 1, spotlighting
    the scenarios addressed, RL methodologies employed, and experimental approaches
    undertaken.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶轨迹规划领域，尽管模型预测控制（MPC）和动态规划（DP）方法在众多场景中展示了其效率和准确性，但它们在面对环境不确定性、模型不准确性和计算资源限制时，其适应性和灵活性受到限制。相比之下，深度强化学习（DRL）通过其出色的环境适应性和动态学习特性显著增强了规划灵活性。它通过价值或策略网络生成轨迹，摒弃了对预定路径点的依赖，而是基于环境状态和目标进行动态决策。关于轨迹规划挑战，DRL
    作为一种有前景的替代方案出现。我们将 DRL 在这一领域的具体应用研究汇总在表 1 中，突出显示了所涉及的场景、使用的 RL 方法和所采取的实验方法。
- en: In the application of RL to autonomous driving trajectory generation, the design
    of appropriate states (State) and actions (Action) is crucial, enabling the algorithm
    to effectively learn and produce optimized trajectories. The survey by Leurent
    et al.[[20](#bib.bib20)] synthesizes various representation methods of states
    and actions within autonomous driving research. The design of states should encompass
    the vehicle’s current motion status (e.g., position, speed, acceleration, and
    heading angle), environmental information (e.g., road conditions, traffic signs,
    and the status of surrounding vehicles), and goal information (e.g., the destination
    location or planned trajectory), to thoroughly depict the vehicle’s interaction
    with the environment. Additionally, considerations should include lane information,
    path curvature, the vehicle’s past and future trajectories, longitudinal information
    such as Time to Collision (TTC), and scene information like traffic regulations
    and the positions of traffic lights. Raw sensor data (e.g., camera images, LiDAR,
    radar) offers more detailed contextual information, whereas simplified abstract
    data (e.g., 2D bird’s eye views ) reduces the complexity of the state space.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）应用于自主驾驶轨迹生成中，设计适当的状态（State）和动作（Action）至关重要，这使得算法能够有效地学习并生成优化的轨迹。Leurent等人的调查[[20](#bib.bib20)]综合了自主驾驶研究中各种状态和动作的表示方法。状态的设计应包括车辆当前的运动状态（例如位置、速度、加速度和航向角）、环境信息（例如道路状况、交通标志和周围车辆的状态），以及目标信息（例如目的地位置或计划轨迹），以全面描述车辆与环境的互动。此外，还应考虑车道信息、路径曲率、车辆的过去和未来轨迹、纵向信息如碰撞时间（TTC），以及场景信息如交通规则和交通灯的位置。原始传感器数据（例如相机图像、激光雷达、雷达）提供了更详细的上下文信息，而简化的抽象数据（例如2D鸟瞰图）则减少了状态空间的复杂性。
- en: Action design may encompass vehicle control maneuvers, such as steering and
    speed adjustments, aimed at trajectory generation, or it could involve simplifying
    the trajectory into an articulation of a sequence of parameters, with actions
    being those parameters that delineate the trajectory. Prudent design of states
    and actions must not only encapsulate sufficient information to inform decisions
    but also sustain efficiency and operability, ensuring that RL algorithms are capable
    of producing safe and efficient trajectories in real-time and with precision within
    intricate autonomous driving contexts.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 动作设计可能包括车辆控制操作，如转向和速度调整，旨在生成轨迹，或者将轨迹简化为一系列参数的表达，其中动作是那些定义轨迹的参数。状态和动作的设计必须不仅封装足够的信息以指导决策，还需保持效率和可操作性，确保RL算法能够实时且精确地在复杂的自主驾驶环境中生成安全高效的轨迹。
- en: Crafting effective reward functions is vital for the successful deployment of
    Reinforcement Learning (RL) within autonomous driving trajectory planning. The
    reward function must comprehensively account for multiple aspects, including the
    trajectory’s safety, smoothness, and speed consistency, encompassing factors such
    as the distance traveled towards the destination, vehicle speed, maintaining stillness,
    avoiding collisions with other road users or scene objects, refraining from infractions
    on sidewalks, remaining within lanes, sustaining comfort and stability amidst
    avoiding extreme acceleration, braking, or turning, and compliance with traffic
    laws. The design of reward functions also encompasses advanced techniques like
    reward shaping[[21](#bib.bib21)], encouraging the optimization process to evolve
    towards the optimal strategy by furnishing the agent with additional, well-crafted
    rewards. Rewards can also be deduced based on expert demonstrations through Inverse
    Reinforcement Learning (IRL)[[22](#bib.bib22)]. In the absence of explicit reward
    shaping and expert demonstrations, agents may utilize intrinsic rewards or motivations
    to evaluate the quality of their actions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 制定有效的奖励函数对于成功应用强化学习（RL）于自动驾驶轨迹规划至关重要。奖励函数必须全面考虑多个方面，包括轨迹的安全性、平稳性和速度一致性，涵盖诸如向目标的行驶距离、车辆速度、保持静止、避免与其他道路用户或场景物体碰撞、避免在人行道上违规、保持车道内、在避免极端加速、刹车或转弯时维持舒适性和稳定性，以及遵守交通法规。奖励函数的设计还包括高级技术，如奖励塑形[[21](#bib.bib21)]，通过提供额外的、精心设计的奖励，鼓励优化过程朝着最佳策略演变。奖励还可以通过逆强化学习（IRL）[[22](#bib.bib22)]基于专家演示来推导。在没有明确的奖励塑形和专家演示的情况下，代理可以利用内在奖励或动机来评估其动作的质量。
- en: In configuring reward functions, it’s crucial to ensure that rewards aren’t
    overly sparse, allowing the agent to derive useful feedback from each action.
    Appropriately setting the weights of reward elements to balance trajectory planning
    objectives, including safety, efficiency, and comfort, is equally crucial. For
    scenarios involving urban road networks and intersections, the reward function
    should account for vehicle behavior within complex traffic settings, encouraging
    the agent to adhere to traffic regulations and optimize paths through intersections.
    In emergency avoidance scenarios, there’s a particular emphasis required on maintaining
    safe distances and avoiding collisions, motivating the agent to implement preventive
    measures. Furthermore, the design of reward functions must contemplate the dynamic
    shifts in the environment and interactions among multiple vehicles, ensuring the
    agent can make adaptive choices in fluctuating settings, thereby significantly
    boosting the robustness and flexibility of trajectory planning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置奖励函数时，确保奖励不会过于稀疏是至关重要的，这样可以让代理从每个动作中获得有用的反馈。适当地设置奖励元素的权重，以平衡轨迹规划目标，包括安全性、效率和舒适性，同样重要。对于涉及城市道路网络和交叉口的场景，奖励函数应考虑复杂交通环境中的车辆行为，鼓励代理遵守交通规则，并优化通过交叉口的路径。在紧急避险场景中，特别需要强调保持安全距离和避免碰撞，激励代理采取预防措施。此外，奖励函数的设计必须考虑环境的动态变化和多辆车之间的互动，确保代理能够在变化的环境中做出适应性选择，从而显著提升轨迹规划的鲁棒性和灵活性。
- en: While Reinforcement Learning (RL) proposes innovative solutions for trajectory
    planning in autonomous driving, its application encounters numerous challenges.
    Initially, unlike traditional Model Predictive Control (MPC), RL does not depend
    on precise environmental models, which proves particularly advantageous when models
    are inaccurate or entirely absent. However, RL must navigate the constraints of
    computational resources and the challenges posed by environmental uncertainties
    and dynamic changes, necessitating algorithms with the capacity for real-time
    adaptation and updates. Additionally, one of the primary challenges in applying
    RL to autonomous driving trajectory planning lies in designing a reward structure
    that can effectively balance multiple objectives and ensuring the solutions’ generalizability
    and adaptability. Specifically, the limited ability to manage emergency scenarios,
    where most approaches favor pursuing solutions that meet specific rules at the
    expense of adequately responding to emergencies; the requirement for data preprocessing
    and dependency on substantial volumes of data exacerbate the challenge of facilitating
    real-time responses, particularly in emergency situations. These challenges necessitate
    overcoming through innovative RL methodologies and strategies to realize efficient
    and safe trajectory planning within complex traffic scenarios.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然强化学习（RL）为自主驾驶中的轨迹规划提出了创新的解决方案，但其应用面临许多挑战。最初，与传统的模型预测控制（MPC）不同，RL 不依赖于精确的环境模型，这在模型不准确或完全缺失时尤为有利。然而，RL
    必须克服计算资源的限制以及环境不确定性和动态变化带来的挑战，因此需要具备实时适应和更新能力的算法。此外，将 RL 应用于自主驾驶轨迹规划的主要挑战之一在于设计一种能有效平衡多个目标的奖励结构，并确保解决方案的通用性和适应性。具体而言，处理紧急情况的能力有限，大多数方法倾向于追求符合特定规则的解决方案，而未能充分应对紧急情况；数据预处理和对大量数据的依赖加剧了实时响应的挑战，尤其是在紧急情况下。这些挑战需要通过创新的
    RL 方法和策略来克服，以实现复杂交通场景中的高效和安全的轨迹规划。
- en: 3.3 Recent DRL Applications in Trajectory Planning
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 近期深度强化学习在轨迹规划中的应用
- en: 'Table 1: A Comparison of DRL-based Trajectory Planning'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：基于深度强化学习的轨迹规划比较
- en: '| Application Scenario | Work | Algorithm | Experiments | Pros |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 应用场景 | 工作 | 算法 | 实验 | 优势 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Intersections | [[23](#bib.bib23)] | SAC | Custom-built Simulator | Integration
    of Reinforcement Learning and Computer Vision |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 十字路口 | [[23](#bib.bib23)] | SAC | 定制模拟器 | 强化学习与计算机视觉的结合 |'
- en: '| Intersections | [[24](#bib.bib24)] | PPO | Flow framework | Fully Autonomous
    Traffic Improvement at Intersections |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 十字路口 | [[24](#bib.bib24)] | PPO | Flow 框架 | 完全自主的交叉口交通改善'
- en: '| Intersections | [[25](#bib.bib25)] | TD3 | SUMO and CARLA | Safety and Efficiency
    in Multi-Task Intersection Navigation |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 十字路口 | [[25](#bib.bib25)] | TD3 | SUMO 和 CARLA | 多任务交叉口导航的安全性和效率 |'
- en: '| Intersections | [[26](#bib.bib26)] | DQN | Custom-built Simulator | Reduction
    in Fuel Consumption |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 十字路口 | [[26](#bib.bib26)] | DQN | 定制模拟器 | 燃料消耗减少 |'
- en: '| Urban road | [[27](#bib.bib27)] | DDPG | CARLA | Success in Unprotected Left
    Turns |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 城市道路 | [[27](#bib.bib27)] | DDPG | CARLA | 无保护左转的成功 |'
- en: '| Urban road | [[28](#bib.bib28)] | DQN | CARLA | Short Training Time |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 城市道路 | [[28](#bib.bib28)] | DQN | CARLA | 短训练时间 |'
- en: '| Urban road | [[29](#bib.bib29)] | TD3 | CARLA and ROS | Improved Convergence
    and Stability |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 城市道路 | [[29](#bib.bib29)] | TD3 | CARLA 和 ROS | 改进的收敛性和稳定性 |'
- en: '| Urban road | 297 | TD3 | CARLA | Hierarchical Method for Object Avoidance
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 城市道路 | 297 | TD3 | CARLA | 面向对象避让的层次方法 |'
- en: '| Urban road | [[30](#bib.bib30)] | Double DQN | Custom-built Simulator | Energy
    Consumption Reduction in EVs |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 城市道路 | [[30](#bib.bib30)] | 双重 DQN | 定制模拟器 | 电动车能耗降低 |'
- en: '| Narrow lane | [[31](#bib.bib31)] | TD3 | CARLA | Instantaneous Solution from
    Pre-trained Network |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 窄车道 | [[31](#bib.bib31)] | TD3 | CARLA | 预训练网络的即时解决方案 |'
- en: '| Narrow lane | [[32](#bib.bib32)] | Actor critic | Custom-built Simulator
    And real world in a campus | Negotiation-Aware Motion Planning |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 窄车道 | [[32](#bib.bib32)] | Actor Critic | 定制模拟器及校园中的真实世界 | 具有协商意识的运动规划 |'
- en: '| Highway | [[33](#bib.bib33)] | SAC | MATLAB | Increased Security and Efficiency
    on Highways |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 高速公路 | [[33](#bib.bib33)] | SAC | MATLAB | 高速公路上的安全性和效率提升 |'
- en: '| Racing | [[34](#bib.bib34)] | Fuzzy DRL | DeepRacer | Explainable Fuzzy Deep
    Reinforcement Learning |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 赛车 | [[34](#bib.bib34)] | 模糊深度强化学习 | DeepRacer | 可解释的模糊深度强化学习 |'
- en: '| Racing | [[35](#bib.bib35)] | TRPO&PPO | F1Tenth | Residual Policy Learning
    for High-Speed Racing |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 赛车 | [[35](#bib.bib35)] | TRPO&PPO | F1Tenth | 高速赛车的残差策略学习 |'
- en: '| Racing | [[36](#bib.bib36)] | SAC | CARLA | Combining Control Methods for
    Handling Limits |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 赛车 | [[36](#bib.bib36)] | SAC | CARLA | 处理极限情况的控制方法结合 |'
- en: '| No specific scenario | [[37](#bib.bib37)] | DDPG | Custom-built Simulator
    And Real World in a Campus | Real-time NN-based Motion Planning |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 无特定场景 | [[37](#bib.bib37)] | DDPG | 定制模拟器与校园中的实际世界 | 基于神经网络的实时运动规划 |'
- en: '| No specific scenario | [[38](#bib.bib38)] | DDPG | real world cases on the
    ZalaZone | Optimal Vehicle Trajectory Learning |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 无特定场景 | [[38](#bib.bib38)] | DDPG | ZalaZone上的实际案例 | 最优车辆轨迹学习 |'
- en: '| No specific scenario | [[39](#bib.bib39)] | DDPG | numerical verification
    | Stability Evaluation with Lyapunov Function |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 无特定场景 | [[39](#bib.bib39)] | DDPG | 数值验证 | 使用Lyapunov函数的稳定性评估 |'
- en: '| Unknown Dynamic Environment | [[40](#bib.bib40)] | Dueling Double DQN | ROS
    and Gazebo | APF-D3QNPER for Superior Generalization |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 未知动态环境 | [[40](#bib.bib40)] | 对抗双重DQN | ROS和Gazebo | APF-D3QNPER以实现更好的泛化
    |'
- en: '| Off-road environments | [[41](#bib.bib41)] | DDPG | ARL Unity | Covert Navigation
    in Off-road Environments |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 越野环境 | [[41](#bib.bib41)] | DDPG | ARL Unity | 越野环境中的隐蔽导航 |'
- en: DRL has already showcased its ability to offer solutions within applications
    targeted at specific scenarios. Path planning challenges are intricately linked
    to specific application contexts, such as intersections, areas dense with pedestrians,
    and urban environments with complex traffic flows. The unique attributes of these
    scenarios are vital for the design of reward functions and experimental validation.
    Concentrating on these scenarios allows researchers to more precisely simulate
    real-world challenges and, in turn, formulate effective DRL strategies.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: DRL已经展示了它在针对特定场景的应用中提供解决方案的能力。路径规划挑战与特定应用环境密切相关，例如交叉口、人流密集区域以及复杂交通流的城市环境。这些场景的独特特性对奖励函数的设计和实验验证至关重要。专注于这些场景使研究人员能够更准确地模拟现实世界的挑战，从而制定有效的DRL策略。
- en: Within the context of intersections, deep reinforcement learning has demonstrated
    substantial potential in addressing path planning challenges. The complexity inherent
    in intersection scenarios primarily stems from dynamic traffic flows, multi-vehicle
    interactions, and the presence of pedestrians, all of which significantly compound
    the challenges of path planning. In response, multiple studies have employed diverse
    DRL strategies for mitigation. For instance, Yudin et al.[[23](#bib.bib23)] trained
    intelligent agents for simulated autonomous vehicles through a novel method integrating
    reinforcement learning with computer vision. Utilizing comprehensive visual information
    on intersections obtained from aerial photography, they automatically detected
    the relative positions of all road users and explored the feasibility of estimating
    vehicle orientation angles via convolutional neural networks. Through the application
    of modern and efficacious reinforcement learning methodologies like Soft Actor
    Critic and Rainbow, the convergence of the learning process was expedited by harnessing
    the acquired additional features. Liu Yuqi et al.[[25](#bib.bib25)] proposed a
    multi-task safety reinforcement learning framework equipped with a social attention
    module, aimed at enhancing safety and efficiency during interactions with other
    traffic participants. The social attention module focuses on the statuses of negotiating
    vehicles, with a safety layer incorporated into the multi-task reinforcement learning
    framework to ensure safe negotiations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉口的背景下，深度强化学习在解决路径规划挑战方面展示了显著潜力。交叉口场景中固有的复杂性主要来源于动态交通流、多车互动以及行人的存在，这些都显著增加了路径规划的挑战。对此，多个研究采用了不同的DRL策略来缓解。例如，Yudin等人[[23](#bib.bib23)]通过将强化学习与计算机视觉结合的创新方法，训练了用于模拟自动驾驶车辆的智能代理。利用从空中摄影获得的交叉口的全面视觉信息，他们自动检测了所有道路使用者的相对位置，并探索了通过卷积神经网络估计车辆方向角的可行性。通过应用现代和有效的强化学习方法，如Soft
    Actor Critic和Rainbow，学习过程的收敛通过利用获得的附加特征得到了加速。刘宇琪等人[[25](#bib.bib25)]提出了一个配备社会注意模块的多任务安全强化学习框架，旨在提高与其他交通参与者互动时的安全性和效率。社会注意模块关注于协商车辆的状态，安全层被纳入多任务强化学习框架中，以确保安全协商。
- en: In urban road network contexts, the complexity is heightened by sudden changes
    in traffic conditions and the presence of dynamic obstacles, presenting substantial
    challenges to vehicular safety. Zhou Weitao et al. [124] concentrate on the notably
    challenging task of unprotected left turns within urban road networks. Utilizing
    the DDPG algorithm and the CARLA simulator, they introduce an innovative method
    for trajectory planning. This method not only accounts for dynamically generated
    trajectory actions but also enhances adaptability and safety through a weighted
    amalgamation with fixed-strategy trajectory actions. This approach exhibited commendable
    performance in random and challenging test scenarios, achieving a 94.4 percent
    success rate in autonomously navigating through 250 intersections. Wang Zhitao
    et al. [[32](#bib.bib32)] utilized an actor-critic methodology to develop a negotiation-aware
    motion planning framework within a proprietary simulator. This framework, while
    ensuring vehicular safety and fluidity, takes into account the needs of other
    traffic participants, adeptly adapting to the complexities of urban traffic, and
    has undergone real-vehicle validation on campus roads.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在城市道路网络中，复杂性由于交通条件的突然变化和动态障碍物的存在而加剧，这给车辆安全带来了重大挑战。Zhou Weitao等人[124]集中于城市道路网络中未受保护的左转这一极具挑战性的任务。利用DDPG算法和CARLA模拟器，他们引入了一种创新的轨迹规划方法。该方法不仅考虑了动态生成的轨迹动作，还通过与固定策略轨迹动作的加权融合提高了适应性和安全性。这种方法在随机和具有挑战性的测试场景中表现出色，实现了在250个交叉口自动导航的94.4%的成功率。Wang
    Zhitao等人[[32](#bib.bib32)]采用演员-评论家方法，在专有模拟器中开发了一个考虑协商的运动规划框架。该框架在确保车辆安全和流畅的同时，考虑了其他交通参与者的需求，巧妙地适应了城市交通的复杂性，并在校园道路上进行了真实车辆验证。
- en: Moreover, the most significant challenge posed to reinforcement learning by
    complex environments is the difficulty associated with achieving training convergence.
    Clemmons J. and their team[[28](#bib.bib28)] significantly reduced training time
    and enhanced learning efficiency through the application of various optimized
    DQN algorithms, marking a substantial improvement over most extant image-based
    approaches. Gao L. et al.[[29](#bib.bib29)] utilized the Twin Delayed DDPG (TD3)
    algorithm to improve the training efficacy of the DDPG network, substantiating
    the model’s convergence speed and stability within CARLA and ROS settings. Cheng
    Yanqiu et al.[51]] concentrated on longitudinal trajectory planning within mixed
    traffic flows, employing the Adam optimizer to hasten training. By segmenting
    vehicle trajectories into portions featuring constant acceleration/deceleration
    while minimizing assumptions, they circumvented the enumeration of states and
    actions in value networks within intricate solution spaces, thus boosting the
    efficiency of model training and inference. Meanwhile, Zhang Ruiqi et al.[[35](#bib.bib35)]
    leveraged TRPO and PPO algorithms within the F1Tenth environment to demonstrate
    the efficiency benefits conferred upon network training by the residual policy
    learning method in the context of high-speed autonomous racing scenarios.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，复杂环境对强化学习提出的最大挑战是实现训练收敛的难度。Clemmons J.及其团队[[28](#bib.bib28)]通过应用各种优化的DQN算法显著减少了训练时间，提高了学习效率，较现有的大多数基于图像的方法有了显著改进。Gao
    L.等人[[29](#bib.bib29)]利用Twin Delayed DDPG (TD3)算法提高了DDPG网络的训练效果，证实了该模型在CARLA和ROS环境中的收敛速度和稳定性。Cheng
    Yanqiu等人[51]集中于混合交通流中的纵向轨迹规划，采用Adam优化器加速训练。通过将车辆轨迹分割为具有恒定加速/减速的部分，同时最小化假设，他们避免了在复杂解空间中对状态和动作的枚举，从而提高了模型训练和推理的效率。同时，Zhang
    Ruiqi等人[[35](#bib.bib35)]在F1Tenth环境中利用TRPO和PPO算法，展示了在高速自动驾驶场景中，残差策略学习方法对网络训练的效率提升。
- en: '![Refer to caption](img/5e2b11a78e78c6a4c7889cc07a79ba42.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5e2b11a78e78c6a4c7889cc07a79ba42.png)'
- en: 'Figure 8: The framework of the Value Estimation Guild (VEG) optimal trajectory
    planner, a methodology integrating RL with outcomes from rule-based controllers[[27](#bib.bib27)]'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：价值估计公会（VEG）最优轨迹规划器的框架，这是一种将RL与基于规则的控制器结果相结合的方法[[27](#bib.bib27)]
- en: In scenarios characterized by lane divisions, high-speed driving, and lane-changing
    maneuvers, the immediacy of decision-making is increasingly crucial. Feher Arpad
    et al.[[31](#bib.bib31)] utilized the Twin Delayed DDPG algorithm to develop a
    pre-trained neural network and trajectory generation algorithm within the CARLA
    simulator, capable of delivering solutions within 1 millisecond. This significantly
    surpasses traditional dynamics models and rule-based methodologies like DP or
    MPC, making it particularly apt for dynamic trajectory generation tasks. By breaking
    down the curve of the overtaking process into a sequence of line segments, they
    substantially simplified the complexity of the issue, enhancing both the real-time
    nature and applicability of the solution. However, ensuring the stability of the
    output results amidst varying road conditions and on wet or slippery surfaces
    continues to pose a challenge.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有车道分隔、高速驾驶和车道变换操作的场景中，决策的即时性变得越来越关键。Feher Arpad 等人[[31](#bib.bib31)] 利用 Twin
    Delayed DDPG 算法开发了一个预训练神经网络和轨迹生成算法，在 CARLA 仿真器内能够在 `1 millisecond` 内提供解决方案。这显著超越了传统的动态模型和基于规则的方法如
    DP 或 MPC，使其特别适用于动态轨迹生成任务。通过将超车过程的曲线分解为一系列线段，他们大大简化了问题的复杂性，提高了解决方案的实时性和适用性。然而，确保在不同道路条件和湿滑表面下输出结果的稳定性仍然是一个挑战。
- en: '![Refer to caption](img/662a5638206e4fcbe1b704adf362db11.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/662a5638206e4fcbe1b704adf362db11.png)'
- en: 'Figure 9: The training and control loop employed in the agent’s design. The
    curve is segmented into eight parts, enabling rapid trajectory generation from
    a single action output by the agent[[31](#bib.bib31)].'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：代理设计中使用的训练和控制循环。曲线被分成八部分，使得代理能够从单个动作输出中快速生成轨迹[[31](#bib.bib31)]。
- en: Currently, established methods for trajectory generation are predicated on dynamics
    models and rules. Thus, integrating reinforcement learning into a segment of these
    traditional methodologies to augment performance constitutes an effective strategy.
    Zhang Mei et al.[[33](#bib.bib33)] employed the SAC algorithm to realize a safer
    and more efficient path planning solution within the MATLAB environment. By optimizing
    crucial parameters within the polynomial curve interpolation process, their approach
    maintained the highest success rates in three and four-lane scenarios, demonstrating
    how to enhance safety by suitably adjusting driving strategies amidst increasingly
    complex traffic conditions. Bautista-Montesano et al.[[34](#bib.bib34)] enhanced
    fuzzy logic control utilizing reinforcement learning techniques on the DeepRacer
    platform, instituting an interpretable Fuzzy Deep Reinforcement Learning methodology,
    offering an innovative solution for autonomous vehicles. Hou et al.[[36](#bib.bib36)]
    proposed a novel controller amalgamating traditional model-based control, model-free
    reinforcement learning, and expert knowledge. This controller demonstrated superior
    strategies and adaptability under extreme vehicle maneuverability boundary conditions,
    substantiating its excellence and extensibility across diverse complex track conditions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，现有的轨迹生成方法基于动态模型和规则。因此，将强化学习集成到这些传统方法的一部分以增强性能是一个有效的策略。Zhang Mei 等人[[33](#bib.bib33)]
    使用 SAC 算法在 MATLAB 环境中实现了更安全、更高效的路径规划解决方案。通过优化多项式曲线插值过程中的关键参数，他们的方法在三车道和四车道场景中保持了最高的成功率，展示了如何通过适当调整驾驶策略来提高安全性，面对越来越复杂的交通条件。Bautista-Montesano
    等人[[34](#bib.bib34)] 利用强化学习技术在 DeepRacer 平台上增强了模糊逻辑控制，建立了一种可解释的模糊深度强化学习方法，为自动驾驶车辆提供了创新的解决方案。Hou
    等人[[36](#bib.bib36)] 提出了一个新型控制器，结合了传统的基于模型的控制、无模型的强化学习和专家知识。该控制器在极端车辆操控边界条件下展示了卓越的策略和适应性，证实了其在各种复杂轨道条件下的优越性和可扩展性。
- en: '![Refer to caption](img/e100185e35a76e176faff20eff1c9eff.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e100185e35a76e176faff20eff1c9eff.png)'
- en: 'Figure 10: A quintessential approach of incorporating a modicum of ML into
    traditional solutions, where the agent is employed to generate some key parameters
    of the trajectory via the curve interpolation method[[33](#bib.bib33)].'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：将少量机器学习融入传统解决方案的典型方法，其中代理通过曲线插值方法生成轨迹的一些关键参数[[33](#bib.bib33)]。
- en: Moreover, within the realm of autonomous driving path planning research, the
    environmental exploration attribute of deep reinforcement learning facilitates
    its application beyond specific driving scenarios. Existing studies span a broad
    spectrum, from real-world driving settings to unknown dynamic environments and
    even off-road conditions. In two studies, Feher Arpad et al.[[37](#bib.bib37)][[38](#bib.bib38)]
    developed a real-time motion planner based on neural networks through the DDPG
    algorithm, and transitioned it to real-world testing in campus environments and
    the ZalaZone automobile testing field, underscoring DRL’s potential in crafting
    safe and dependable trajectories. Cabezas-Olivenza Mireya et al.[[39](#bib.bib39)]
    performed numerical validation with the DDPG algorithm, evaluating the navigation
    stability of agents trained by DDPG through the Lyapunov function, exploring DRL’s
    effectiveness in sustaining navigational stability. Hu Hui et al.[[40](#bib.bib40)]
    in ROS and Gazebo environments, validated the exceptional generalization capabilities
    of a singular network produced by Dueling Double DQN across diverse scenarios,
    alongside its superior performance regarding convergence speed, loss values, and
    path planning duration. In off-road settings, Hossain J. et al.[[41](#bib.bib41)]
    employed DRL to ensure covert navigation while accomplishing low-cost trajectories,
    demonstrating DRL’s potential to tackle complex terrains and unknown obstacles.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在自动驾驶路径规划研究领域，深度强化学习的环境探索属性使其应用超越了特定的驾驶场景。现有研究涵盖了广泛的范围，从真实世界的驾驶环境到未知的动态环境，甚至是越野条件。在两项研究中，Feher
    Arpad 等人[[37](#bib.bib37)][[38](#bib.bib38)] 通过DDPG算法开发了一个基于神经网络的实时运动规划器，并将其过渡到校园环境和ZalaZone汽车测试场的现实测试中，突显了DRL在制定安全可靠轨迹中的潜力。Cabezas-Olivenza
    Mireya 等人[[39](#bib.bib39)] 进行了数值验证，使用DDPG算法通过Lyapunov函数评估了DDPG训练的代理的导航稳定性，探讨了DRL在维持导航稳定性方面的有效性。Hu
    Hui 等人[[40](#bib.bib40)] 在ROS和Gazebo环境中验证了由Dueling Double DQN生成的单一网络在不同场景下的卓越泛化能力，以及其在收敛速度、损失值和路径规划持续时间方面的优越性能。在越野环境中，Hossain
    J. 等人[[41](#bib.bib41)] 采用DRL确保隐蔽导航，同时实现低成本轨迹，展示了DRL在应对复杂地形和未知障碍物方面的潜力。
- en: 4 DRL for Vehicle Control
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 DRL在车辆控制中的应用
- en: The output of vehicle control is directly connected to specific actuators, constituting
    the final connection to the physical world within the autonomous driving framework.
    This chapter will delve into motion control challenges and elucidate the application
    of RL methods in addressing these issues.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆控制的输出直接连接到特定的执行器，构成了自动驾驶框架中与物理世界的最终连接。本章将深入探讨运动控制中的挑战，并阐明RL方法在解决这些问题中的应用。
- en: 4.1 Challenges in Path Tracking and Speed Control
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 路径追踪和速度控制中的挑战
- en: 'In autonomous driving systems, accurate motion control is essential for achieving
    safe and dependable navigation. Vehicle motion control can generally be categorized
    into two principal components: lateral control and longitudinal control. The central
    objective of lateral control is path tracking, which seeks to ensure that the
    vehicle precisely adheres to the pre-established path, regardless of whether these
    paths are straight lines or complex curves. The conventional approach to accomplishing
    this goal entails initially identifying and modeling the dynamic behavior of the
    vehicle, selecting essential dynamic variables, and formulating corresponding
    state equations based on physical laws and real-vehicle testing data. Building
    upon these foundations, controllers are designed to dynamically adjust the vehicle’s
    driving state in real time, ensuring that the key dynamic variables remain within
    a specified range[[42](#bib.bib42)], thus following the target path.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶系统中，精确的运动控制对于实现安全可靠的导航至关重要。车辆运动控制通常可以分为两个主要组成部分：横向控制和纵向控制。横向控制的核心目标是路径追踪，旨在确保车辆准确遵循预设的路径，无论这些路径是直线还是复杂曲线。实现这一目标的传统方法包括首先识别和建模车辆的动态行为，选择重要的动态变量，并根据物理法则和真实车辆测试数据制定相应的状态方程。在这些基础上，设计控制器以实时动态调整车辆的驾驶状态，确保关键动态变量保持在指定范围内[[42](#bib.bib42)]，从而跟随目标路径。
- en: For instance, the yaw angle discrepancy between the current vehicle orientation
    and the desired orientation could serve as the dynamic variable for tracking,
    given that we can regulate the yaw angle by manipulating wheel steering. Utilizing
    methods such as look-ahead, corrected look-ahead, or future behavior prediction
    based on dynamic models, we can ascertain the difference between the current and
    desired yaw angles. Subsequently, trajectory tracking can be accomplished by controlling
    steering, traction braking, and other aspects of control quality.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当前车辆方向与期望方向之间的偏航角差异可以作为跟踪的动态变量，因为我们可以通过操控车轮转向来调节偏航角。利用诸如前视、修正前视或基于动态模型的未来行为预测等方法，我们可以确定当前和期望偏航角之间的差异。随后，可以通过控制转向、牵引制动和其他控制质量方面来实现轨迹跟踪。
- en: '![Refer to caption](img/474b638a34bae0db6cfc5e19c9b5ea75.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/474b638a34bae0db6cfc5e19c9b5ea75.png)'
- en: 'Figure 11: Achieving trajectory tracking by following the yaw angle $\Psi$
    [[42](#bib.bib42)] based on look-ahead error, (b) based on modified look-ahead
    error, and (c) through the derivation of a dynamical model.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：通过跟随偏航角$\Psi$ [[42](#bib.bib42)] 实现轨迹跟踪，（b）基于修正后的前视误差，（c）通过动态模型推导。
- en: In the realm of autonomous vehicle control, speed tracking poses a significant
    challenge, aiming to maintain or modify vehicle speed to align with a predetermined
    speed curve. This task is critical for minimizing travel time and ensuring the
    dynamic stability of the vehicle, aiding in the prevention of rollovers or skidding
    resulting from excessive speed. The complexity in precisely controlling speed
    arises from the vehicle’s dynamic behaviors, encompassing factors such as the
    nonlinear interaction between the tires and the road surface and dynamic shifts
    prompted by alterations in speed and steering angle.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶车辆控制领域，速度跟踪是一项重大挑战，旨在保持或调整车辆速度以符合预定的速度曲线。这一任务对于最小化行程时间和确保车辆动态稳定性至关重要，有助于防止因速度过快而导致的翻车或滑移。精确控制速度的复杂性源于车辆的动态行为，包括轮胎与路面之间的非线性互动以及由于速度和转向角度变化而引起的动态变化。
- en: 'According to a review[[2](#bib.bib2)], between 2015 and 2021, Model Predictive
    Control (MPC) emerged as the predominant method in the domain of path tracking
    control research, constituting approximately 50 percent of the published papers.
    MPC technology is capable of addressing problems with multiple variables, taking
    into account constraints on states and control actions, and forecasting future
    behavior of the system. Other approaches, such as the Linear Quadratic Regulator
    (LQR), Model-Free Control (MFC), and Nonlinear Model Predictive Control (NLMPC),
    tackle path tracking and speed control challenges through the adoption of varied
    strategies. Nevertheless, these methodologies share a common challenge: reconciling
    the discrepancy between model simplification and the complexities of the real
    world while ensuring control accuracy. The design and implementation of control
    strategies become especially intricate in scenarios of high-speed driving and
    complex road conditions, necessitating the use of more sophisticated models and
    algorithms to accommodate these variations.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据一项综述[[2](#bib.bib2)]，在2015年至2021年间，模型预测控制（MPC）成为路径跟踪控制研究领域的主导方法，占已发表论文的约50%。MPC技术能够解决多变量问题，考虑状态和控制动作的约束，并预测系统的未来行为。其他方法，如线性二次调节器（LQR）、无模型控制（MFC）和非线性模型预测控制（NLMPC），通过采用不同的策略来应对路径跟踪和速度控制挑战。然而，这些方法面临一个共同挑战：在确保控制精度的同时，调和模型简化与现实世界复杂性之间的差异。在高速驾驶和复杂道路条件下，控制策略的设计和实施尤为复杂，需要使用更复杂的模型和算法来适应这些变化。
- en: Constructing an accurate vehicle dynamics model is an exceedingly complex endeavor
    that necessitates the integrated consideration of various factors. These include
    the intricate interplay between the road surface and tires, alterations in dynamic
    characteristics with changes in vehicle speed, and the interdependence between
    lateral and longitudinal dynamics. To simplify this issue, while preserving a
    certain level of accuracy, simplified models, including kinematic models and dynamic
    bicycle models, are commonly employed. These models undergo linearization, facilitating
    easier analysis and implementation during the design of control strategies. However,
    when vehicles operate at high speeds or encounter complex road conditions, these
    simplified models may not adequately capture the vehicle’s authentic behavior.
    In such instances, more sophisticated models, like Pacejka’s magic formula tire
    model, are required to enhance the precision of control strategies.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个准确的车辆动态模型是一个极其复杂的工作，需要综合考虑各种因素。这些因素包括路面与轮胎之间的复杂相互作用、车辆速度变化引起的动态特性变化以及横向和纵向动态之间的相互依赖。为了简化这个问题，同时保持一定的准确性，通常采用简化模型，包括运动学模型和动态自行车模型。这些模型会进行线性化，便于在设计控制策略时进行更简单的分析和实施。然而，当车辆以高速度行驶或遇到复杂的道路条件时，这些简化模型可能无法充分捕捉车辆的真实行为。在这种情况下，需要更复杂的模型，如Pacejka的魔术公式轮胎模型，以提高控制策略的精度。
- en: 'Current control strategies predicated on dynamic models confront several significant
    challenges: firstly, they necessitate precise system models, but as different
    vehicles possess disparate parameters, this demands substantial efforts in parameter
    calibration. Secondly, in certain specific road segments, such as large curves
    necessitating higher lateral acceleration, the system may falter. Furthermore,
    the decision-making within these systems typically relies on rule-based definitions,
    leading to complications in development, the introduction of subjective human
    factors, and challenges in ensuring comprehensive coverage.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 目前基于动态模型的控制策略面临几个重大挑战：首先，它们需要精确的系统模型，但由于不同的车辆具有不同的参数，这要求进行大量的参数校准。其次，在某些特定的道路段，如需要较高横向加速度的大弯道，系统可能会出现问题。此外，这些系统中的决策通常依赖于基于规则的定义，导致开发中的复杂性、主观人为因素的引入以及确保全面覆盖的挑战。
- en: To tackle these intricate challenges, model-free, data-driven methodologies
    present a novel solution. Contrary to traditional control strategies grounded
    in precisely defined vehicle dynamics models, data-driven approaches empower vehicles
    to learn and refine their control strategies based on actual driving data. The
    application of model-based DRL likewise reveals vast potential. This approach
    efficaciously narrows the divide between theoretical models and real-world vehicle
    dynamics. It encompasses not just the theoretical models of vehicle dynamics but
    also assimilates the dynamic behavior exhibited by vehicles in actual driving
    scenarios, behavior that is frequently subject to the influence of numerous unpredictable
    environmental factors.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些复杂的挑战，无模型的、数据驱动的方法提供了一个新解决方案。与传统的基于精确定义车辆动态模型的控制策略不同，数据驱动的方法使车辆能够根据实际驾驶数据学习和优化其控制策略。基于模型的DRL应用同样展示了巨大的潜力。这种方法有效地缩小了理论模型与实际车辆动态之间的差距。它不仅涵盖了车辆动态的理论模型，还融合了车辆在实际驾驶场景中表现出的动态行为，这些行为通常会受到许多不可预测的环境因素的影响。
- en: 4.2 Implementing DRL for Enhanced Vehicle Control
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实施DRL以增强车辆控制
- en: Implementing augmented vehicle control utilizing DRL enables the RL algorithm
    to progressively master strategies for adjusting vehicle steering, acceleration,
    and braking through an ongoing training regimen. This method’s advantage lies
    in its independence from labeled datasets, which facilitates the agent’s ability
    to exhibit robust generalization capabilities across novel scenarios. The training
    goal is to attain near-optimal trajectory and speed tracking across diverse driving
    paths, concurrently sustaining the vehicle’s dynamic stability. Throughout the
    learning process, the vehicle dynamically adjusts its state to follow the designated
    trajectory, optimizing transit time while ensuring not to surpass the boundaries
    of dynamic stability.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 实施使用DRL的增强车辆控制，使得RL算法能够通过持续的训练过程逐步掌握调整车辆转向、加速和刹车的策略。这种方法的优势在于它不依赖标记数据集，从而促进了代理在新场景中的强泛化能力。训练目标是实现近乎最佳的轨迹和速度跟踪，同时保持车辆的动态稳定性。在学习过程中，车辆会动态调整其状态以跟随指定轨迹，优化过境时间，同时确保不超出动态稳定性的边界。
- en: In modeling vehicle motion control issues within a MDP, the state space mirrors
    that utilized in trajectory planning, encompassing critical factors like vehicle
    position, speed, acceleration, wheel steering angle, environmental data, and the
    destination location or planned trajectory. The distinction lies in the control’s
    heightened demand for state dynamism, hence this information is predominantly
    employed in a simplified and abstracted form.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDP中建模车辆运动控制问题时，状态空间类似于轨迹规划中使用的状态空间，涵盖了车辆位置、速度、加速度、车轮转角、环境数据以及目的地位置或规划轨迹等关键因素。区别在于控制对状态动态性的需求更高，因此这些信息主要以简化和抽象的形式使用。
- en: The action space of an agent consists of control command outputs, typically
    encompassing modifications to the wheel steering angle, adjustments in acceleration
    or deceleration, braking, and gear shifting among discrete actuators. The majority
    of these control commands are continuous values, resulting in a substantially
    large dimension of the action space. To reduce complexity, the use of DRL algorithms
    intended solely for discrete action spaces (like DQN) is permissible. Here, the
    action space can be discretized by segmenting the continuous actuator range into
    equal-sized intervals[[43](#bib.bib43)]. The selection of the number of actuator
    intervals involves a trade-off aimed at balancing smooth control against the cost
    of action selection. Employing DRL algorithms to directly learn strategies for
    managing continuous value actuators (like DDPG) or simplifying the action selection
    process through a temporal abstraction options framework is viable.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的动作空间由控制命令输出组成，通常包括对车轮转角的修改、加速或减速调整、刹车和离合器换挡等离散执行器。大多数控制命令是连续值，因此动作空间的维度非常大。为了减少复杂性，可以使用专门针对离散动作空间的DRL算法（如DQN）。在这里，可以通过将连续执行器范围分段为相等的区间来离散化动作空间[[43](#bib.bib43)]。选择执行器区间的数量涉及在平滑控制与动作选择成本之间的权衡。使用DRL算法直接学习管理连续值执行器的策略（如DDPG），或通过时间抽象选项框架简化动作选择过程也是可行的。
- en: The design of the reward function must encompass multiple objectives, including
    vehicle stability, efficient driving, and safe obstacle avoidance. Positive rewards
    are allocated for reaching the target location or maintaining stability, whereas
    negative rewards are imposed for behaviors like collisions or deviation from the
    intended path. RL encounters challenges related to computational resource constraints,
    environmental uncertainty, and dynamic shifts, necessitating real-time adjustments
    and updates. Although reinforcement learning offers innovative solutions for autonomous
    vehicle control, its application process is fraught with numerous challenges.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数的设计必须包含多个目标，包括车辆稳定性、高效驾驶和安全避障。到达目标位置或维持稳定性会获得正奖励，而碰撞或偏离预定路径等行为则会受到负奖励。RL面临计算资源限制、环境不确定性和动态变化等挑战，需要实时调整和更新。虽然强化学习为自动驾驶控制提供了创新的解决方案，但其应用过程充满了许多挑战。
- en: RL does not depend on precise environmental models, an advantage when models
    are imprecise or absent. Yet, when capable of accurate modeling and necessitating
    interpretable and readily modifiable control implementations, RL falls short compared
    to traditional methodologies. A method to enhance the modifiability of RL involves
    primarily utilizing the existing framework while integrating RL’s adaptive and
    dynamic updating capabilities[[34](#bib.bib34), [44](#bib.bib44)], enabling operation
    independent of RL’s output actions in situations of problems or when identifying
    emergency constraints. In terms of interpretability, one could endeavor to use
    a network or model inversely to ascertain the relationship between RL’s output
    actions and input states, thereby attempting to rationalize unforeseen actions
    undertaken by the agent[[45](#bib.bib45)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: RL 不依赖于精确的环境模型，这在模型不精确或缺失时是一大优势。然而，当能够进行准确建模且需要可解释和易于修改的控制实现时，RL 不如传统方法。提高 RL
    可修改性的一种方法是主要利用现有框架，同时整合 RL 的适应性和动态更新能力[[34](#bib.bib34), [44](#bib.bib44)]，使得在问题发生或识别紧急约束的情况下能够独立于
    RL 的输出动作进行操作。在可解释性方面，可以尝试使用网络或模型逆向确定 RL 输出动作和输入状态之间的关系，从而尝试合理化代理所采取的意外行动[[45](#bib.bib45)]。
- en: 4.3 Recent DRL Applications in Vehicle Control
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 最近的 DRL 在车辆控制中的应用
- en: 'Table 2: A Comparison of DRL-based Vehicle Control'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 基于 DRL 的车辆控制比较'
- en: '| Problem | Work | Algorithm | Experiments | Pros |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 工作 | 算法 | 实验 | 优势 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Path tracking | [[46](#bib.bib46)] | DQN and DDPG | CARLA | Realization Process
    Clarity and DDPG Performance |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 路径跟踪 | [[46](#bib.bib46)] | DQN 和 DDPG | CARLA | 实现过程的清晰度和 DDPG 性能 |'
- en: '| Path tracking | [[47](#bib.bib47)] | DQN | Paramics | Travel Time Consistency
    with Increased Bus Volume |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 路径跟踪 | [[47](#bib.bib47)] | DQN | Paramics | 公交车量增加下的旅行时间一致性 |'
- en: '| Path tracking | [[44](#bib.bib44)] | Actor Critic | Driving simulator hardware
    | Adaptive PID Weight Adjustment |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 路径跟踪 | [[44](#bib.bib44)] | Actor Critic | 驾驶模拟器硬件 | 自适应 PID 权重调整 |'
- en: '| Path tracking | [[48](#bib.bib48)] | Actor Critic | Driving simulator hardware
    | SRL-TR2 for Trajectory Tracking |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 路径跟踪 | [[48](#bib.bib48)] | Actor Critic | 驾驶模拟器硬件 | SRL-TR2 用于轨迹跟踪 |'
- en: '| Path tracking | [[49](#bib.bib49)] | PPO | Numerical Simulation | PPO2-Stanley
    for Vehicle Tracking and Safety |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 路径跟踪 | [[49](#bib.bib49)] | PPO | 数值仿真 | PPO2-Stanley 用于车辆跟踪和安全 |'
- en: '| Path tracking | [[50](#bib.bib50)] | DDPG | CARLA | DCN-DDPG for Path-Tracking
    Efficiency |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 路径跟踪 | [[50](#bib.bib50)] | DDPG | CARLA | 路径跟踪效率的 DCN-DDPG |'
- en: '| Lane following | [[51](#bib.bib51)] | DDPG | TORCS | DCPER-DDPG for Lane
    Following |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 车道跟踪 | [[51](#bib.bib51)] | DDPG | TORCS | 车道跟踪的 DCPER-DDPG |'
- en: '| Lateral Control | [[52](#bib.bib52)] | Model-based DRL | Numerical Simulation
    | Model-Based DRL for Lateral Control |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 横向控制 | [[52](#bib.bib52)] | 基于模型的 DRL | 数值仿真 | 基于模型的 DRL 用于横向控制 |'
- en: '| Lateral Control | [[53](#bib.bib53)] | PG | Custom-built Simulator | Monte
    Carlo Tree Search to reduce complexity of Policy Network |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 横向控制 | [[53](#bib.bib53)] | PG | 自定义模拟器 | 蒙特卡洛树搜索以减少策略网络的复杂性 |'
- en: '| Speed Control | [[54](#bib.bib54)] | DDPG | MATLAB | DDPG for Multi-Target:
    Enhanced Safety, Efficiency, and Comfort, VS MPC |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 速度控制 | [[54](#bib.bib54)] | DDPG | MATLAB | DDPG 多目标：增强的安全性、效率和舒适性，与 MPC
    对比 |'
- en: '| Speed Control | [[55](#bib.bib55)] | Model-based DDPG | Unity | Model-Based
    DDPG for High-Speed Autonomy |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 速度控制 | [[55](#bib.bib55)] | 基于模型的 DDPG | Unity | 基于模型的 DDPG 用于高速自主 |'
- en: '| Longitudinal Control | [[45](#bib.bib45)] | DDPG | Numerical Simulation |
    Explainable RL for Decision-Making Clarity |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 纵向控制 | [[45](#bib.bib45)] | DDPG | 数值仿真 | 可解释的 RL 以提高决策清晰度 |'
- en: '| Integrated Control | [[56](#bib.bib56)] | ADP | Numerical Simulation | Two-Phase
    Data-Driven Policy Iteration |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 集成控制 | [[56](#bib.bib56)] | ADP | 数值仿真 | 两阶段数据驱动策略迭代 |'
- en: '| Integrated Control | [[57](#bib.bib57)] | CNN self attention and RL methods
    | SUMO | Joint Optimization with CNN Self Attention |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 集成控制 | [[57](#bib.bib57)] | CNN 自注意力和 RL 方法 | SUMO | 与 CNN 自注意力的联合优化 |'
- en: '| Integrated Control | [[58](#bib.bib58)] | DDPG | Numerical Simulation | DDPG-Enhanced
    LADRC for 3-DOF Vehicles |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 集成控制 | [[58](#bib.bib58)] | DDPG | 数值仿真 | DDPG 增强的 LADRC 用于 3 自由度车辆 |'
- en: In tackling path tracking and lane-keeping challenges, recent research has made
    notable advancements across several experimental platforms (e.g., CARLA, Paramics,
    and TORCS) by employing a range of RL algorithms, including DQN, DDPG, Actor Critic,
    and PPO. These research findings not only underscore the efficiency and adaptability
    of RL algorithms in managing complex dynamic systems but also offer fresh perspectives
    and solutions for enhancing the performance optimization and safety of autonomous
    vehicles.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决路径跟踪和车道保持挑战方面，最近的研究通过在多个实验平台（如CARLA、Paramics和TORCS）上采用多种RL算法（包括DQN、DDPG、Actor
    Critic和PPO）取得了显著进展。这些研究结果不仅强调了RL算法在管理复杂动态系统中的效率和适应性，还为提升自主车辆的性能优化和安全性提供了新的视角和解决方案。
- en: Enhancing the generalization capability of models across varied scenarios, especially
    for effective control within intricate urban traffic contexts, necessitates the
    comprehensive application of diversified simulation environments, safe exploration
    mechanisms, model-agnostic control strategies, and real-time adaptive adjustment
    methods. Through extensive experimentation on simulation platforms like CARLA
    and Paramics, Perez-Gil et al. [[46](#bib.bib46)] and Gao et al. [[47](#bib.bib47)]
    showcased strategies for mitigating risks in practical applications and augmenting
    the adaptability of algorithms. Hu, Fu, and Wen [[52](#bib.bib52)] integrated
    Dyna-style algorithms with actions derived from Robust Control Barrier Functions
    (CBF) and employed Gaussian Process (GP) models to boost sample efficiency, facilitating
    effective learning in uncertain environments while ensuring safety. Wang, Zheng,
    Sun [[48](#bib.bib48)], by combining ADRC with DDPG, not only exploited ADRC’s
    model-agnostic property for estimating and compensating for uncertainties and
    external disturbances but also facilitated real-time adaptive adjustment of control
    parameters via DDPG-optimized strategies, thereby further improving generalization
    capabilities and execution efficiency in variable environments.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 提升模型在各种场景下的泛化能力，特别是在复杂城市交通环境中的有效控制，需要全面应用多样化的模拟环境、安全探索机制、模型无关的控制策略以及实时自适应调整方法。通过在像CARLA和Paramics这样的模拟平台上进行广泛实验，Perez-Gil等人[[46](#bib.bib46)]和Gao等人[[47](#bib.bib47)]展示了在实际应用中降低风险和增强算法适应性的策略。Hu、Fu和Wen[[52](#bib.bib52)]将Dyna风格算法与来自鲁棒控制屏障函数（CBF）的动作相结合，并使用高斯过程（GP）模型来提高样本效率，促进在不确定环境中的有效学习，同时确保安全。Wang、Zheng、Sun[[48](#bib.bib48)]通过将ADRC与DDPG相结合，不仅利用了ADRC的模型无关特性来估计和补偿不确定性和外部干扰，还通过DDPG优化的策略实现了控制参数的实时自适应调整，从而进一步提高了在变化环境中的泛化能力和执行效率。
- en: '![Refer to caption](img/5c06773c842656007212f4acdd61b1e3.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5c06773c842656007212f4acdd61b1e3.png)'
- en: 'Figure 12: DDPG-based DRL controller architecture[[46](#bib.bib46)]'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：基于DDPG的DRL控制器架构[[46](#bib.bib46)]
- en: For tasks requiring higher precision control, without compromising safety and
    stability, the efficacy of model-free approaches seldom exceeds that of methods
    grounded on precise models, suggesting that model-based RL might hold greater
    advantages. Chen et al. [[54](#bib.bib54)] determined speed control rewards on
    rough surfaces by configuring a combination of rewards for safety, efficiency,
    and comfort, subsequently employing a DDPG-based trained speed control model to
    foster safe, efficient, and comfortable vehicle following behaviors. They reported
    that the trained network exhibited lower overall metrics for speed, acceleration,
    and clearance distance compared to MPC control. However, their MPC methodology
    did not disclose the time step, and the objective function merely assigned all
    weights as 1, suggesting room for refinement. Moreover, the model employed was
    inferred from a kinematic model without detailing the kinematic process[[59](#bib.bib59)].
    Despite comparative studies, the upper limit of control precision achievable by
    model-free RL approaches remains an aspect warranting further contemplation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要更高精度控制的任务，且不妨碍安全和稳定性的情况下，无模型方法的效果很少超越基于精确模型的方法，这表明基于模型的 RL 可能具有更大的优势。Chen
    等人[[54](#bib.bib54)] 通过配置安全、效率和舒适性的奖励组合来确定粗糙表面的速度控制奖励，随后使用基于 DDPG 训练的速度控制模型来促进安全、高效和舒适的车辆跟随行为。他们报告称，经过训练的网络在速度、加速度和间隙距离方面的整体指标低于
    MPC 控制。然而，他们的 MPC 方法没有公开时间步长，且目标函数仅将所有权重分配为1，表明还有改进空间。此外，所使用的模型是从运动学模型推断的，而没有详细说明运动学过程[[59](#bib.bib59)]。尽管有比较研究，但无模型
    RL 方法可实现的控制精度上限仍是一个需要进一步探讨的方面。
- en: On another note, Hartmann et al.[[55](#bib.bib55)] devised a model-based DRL
    approach targeted at the time-optimal speed control problem, formulating time-optimal
    speed control strategies and employing numerical solutions to anticipate and mitigate
    scenarios potentially causing vehicle instability. This methodology was validated
    within the Unity environment, substantiating its adept fit to the dynamics model,
    outperforming singular model predictions after a training duration of five minutes.
    Moreover, it achieved superior control outcomes compared to model-free approaches.
    Similarly, the work of Cui et al.[[56](#bib.bib56)], employing a data-driven approach,
    enhanced the deductive process of dynamic planning grounded in dynamic models.
    The implementation of Adaptive Dynamic Programming (ADP) elevated the precision
    of trajectory tracking across diverse road surfaces.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Hartmann 等人[[55](#bib.bib55)] 设计了一种基于模型的 DRL 方法，针对时间最优速度控制问题，制定了时间最优速度控制策略，并使用数值解法来预测和缓解可能导致车辆不稳定的情况。这一方法在
    Unity 环境中得到了验证，证明其与动力学模型的契合度高，在经过五分钟的训练后超越了单一模型的预测。此外，与无模型方法相比，它取得了更优的控制结果。同样，Cui
    等人[[56](#bib.bib56)] 采用数据驱动的方法，增强了基于动态模型的动态规划的推导过程。自适应动态规划（ADP）的实施提高了不同路面上的轨迹跟踪精度。
- en: '![Refer to caption](img/c67faf2d3b493d01c36ab6ea9853a251.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c67faf2d3b493d01c36ab6ea9853a251.png)'
- en: 'Figure 13: Utilize a moderately simple network to approximate the state transition
    function[[55](#bib.bib55)].'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：利用一个适度简单的网络来近似状态转移函数[[55](#bib.bib55)]。
- en: In high-curvature sections or emergency obstacle avoidance scenarios, significant
    variations in path curvature present substantial challenges to maintaining path
    tracking accuracy. He et al.[[51](#bib.bib51)]’s methodology, through refinements
    to the Deep Deterministic Policy Gradient (DDPG) algorithm involving the integration
    of a dual critic network and a prioritized experience replay mechanism, has enhanced
    the algorithm’s stability and accuracy in navigating high-curvature and complex
    road conditions. Incorporating extensive high-curvature and complex road scenarios
    within the simulation environment enables this approach to not only preclude trajectory
    tracking failures but also to ensure elevated tracking precision. The enhanced
    DDPG algorithm not only improves lane-tracking performance but also bolsters the
    model’s adaptability to a variety of driving conditions. Luo et al. [[49](#bib.bib49)]
    implemented a hybrid control strategy integrating the robotic Stanley trajectory
    tracking algorithm with Deep Reinforcement Learning (DRL) technology. Leveraging
    the efficient path tracking capability of the Stanley algorithm along with the
    self-learning and adaptability features of DRL, this strategy not only enhances
    tracking accuracy but also seamlessly integrates collision avoidance capabilities,
    thus facilitating safe control in emergency situations.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在高曲率段或紧急障碍物回避场景中，路径曲率的显著变化对保持路径跟踪精度提出了重大挑战。He等人[[51](#bib.bib51)]的方法通过对深度确定性策略梯度（DDPG）算法的改进，包括集成双重评论网络和优先经验回放机制，提高了算法在高曲率和复杂道路条件下的稳定性和准确性。在模拟环境中融入大量高曲率和复杂道路场景，使得这种方法不仅可以预防轨迹跟踪失败，还可以确保较高的跟踪精度。改进的DDPG算法不仅提高了车道跟踪性能，还增强了模型对各种驾驶条件的适应性。Luo等人[[49](#bib.bib49)]实现了一种混合控制策略，将机器人Stanley轨迹跟踪算法与深度强化学习（DRL）技术结合。利用Stanley算法高效的路径跟踪能力以及DRL的自学习和适应性特征，这种策略不仅提高了跟踪准确性，还无缝集成了避碰能力，从而在紧急情况下实现安全控制。
- en: In the context of vehicle control, action spaces are typically continuous, necessitating
    more complex networks for finer control, a factor that frequently complicates
    training convergence. Numerous studies have focused on accelerating convergence
    speed and enhancing convergence stability during training. Yao et al. [[50](#bib.bib50)]
    and He et al.[[51](#bib.bib51)] enhanced the DDPG algorithm by incorporating a
    dual critic network and a prioritized experience replay mechanism, addressing
    numerous deficiencies inherent to the original algorithm and thereby improving
    training efficiency and accuracy; Kovari et al. [[53](#bib.bib53)], in a method
    analogous to that of Alpha Go, leveraged the combination of Monte Carlo tree search
    with short-term reward strategy networks. This approach necessitates focusing
    solely on immediate rewards during the training of the strategy network, substantially
    diminishing the network’s complexity and computational requirements, thereby facilitating
    rapid convergence and real-time operation. Addressing the dichotomy between Q-learning
    training complexity and the granularity of action spaces, [[60](#bib.bib60)] applied
    the Double DQN methodology atop a framework of finely discretized action spaces,
    incorporating real-world driving data. This approach simulates the real-world
    commuting experience, facilitating the validation of the devised vehicle speed
    control system within a simulated environment.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在车辆控制的背景下，动作空间通常是连续的，这需要更复杂的网络来实现更精细的控制，这一因素常常使训练收敛变得复杂。许多研究集中于加速收敛速度和提高训练过程中的收敛稳定性。Yao等人[[50](#bib.bib50)]和He等人[[51](#bib.bib51)]通过引入双重评论网络和优先经验回放机制改进了DDPG算法，解决了原始算法中的许多缺陷，从而提高了训练效率和准确性；Kovari等人[[53](#bib.bib53)]，采用类似Alpha
    Go的方法，结合了蒙特卡罗树搜索与短期奖励策略网络。这种方法需要在策略网络的训练过程中仅关注即时奖励，从而大大减少了网络的复杂性和计算需求，促进了快速收敛和实时操作。为了解决Q学习训练复杂性与动作空间粒度之间的对立，[[60](#bib.bib60)]在细粒度离散动作空间的框架上应用了Double
    DQN方法，并融入了真实世界的驾驶数据。这种方法模拟了真实世界的通勤体验，便于在模拟环境中验证设计的车辆速度控制系统。
- en: As outlined in the preceding discourse, faced with the complexity inherent to
    managing multiple chassis control subsystems and potential subsystem inter-conflicts,
    Deep Reinforcement Learning (DRL) is poised to facilitate integrated motion control.
    This entails orchestrating steering, traction, and braking actuators’ outputs
    in unison to address lateral, longitudinal, and postural control issues effectively.
    Cui et al. [[56](#bib.bib56)] introduced a bi-phase data-driven policy iteration
    algorithm designed for achieving optimal longitudinal and lateral control in autonomous
    vehicles. This algorithm ensures stable convergence towards a suboptimal controller
    solution, notably independent of trailing vehicle dynamics, a claim substantiated
    through numerical simulations. Hu et al. [[52](#bib.bib52)] utilized model-based
    Deep Reinforcement Learning (DRL) to facilitate learning from scratch in real-world
    conditions, leading to tangible deployment. This illustrates DRL’s practical applicability
    and remarkable adaptability in autonomous vehicle control scenarios. Chen et al.
    [[57](#bib.bib57)] leveraged CNNs, self-attention networks, and deep reinforcement
    learning approaches to realize the joint optimization of perception, decision-making,
    and motion control. Validated within the SUMO simulation environment, this research
    underscores the potential of integrated optimization control strategies. Wang
    et al. [[58](#bib.bib58)] demonstrated a DDPG-augmented Linear Active Disturbance
    Rejection Control (LADRC) controller applied to three degrees of freedom (3-DOF)
    autonomous vehicles, evidencing significant adaptability to uncertainties and
    enhanced control performance through real-time parameter tuning facilitated by
    deep reinforcement learning. While Reinforcement Learning (RL) has seen some application
    in integrating control across various sub-problems, it is crucial to acknowledge
    that for essential safety-critical control subsystems, such as ABS and ESP, RL’s
    near-term replacement remains challenging due to insufficient temporal and empirical
    validation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前文所述，面对管理多个底盘控制子系统及其潜在子系统之间的冲突所固有的复杂性，深度强化学习（DRL）有望促进集成运动控制。这包括协调转向、牵引和制动执行器的输出，以有效解决横向、纵向和姿态控制问题。Cui等人[[56](#bib.bib56)]
    引入了一种双相数据驱动策略迭代算法，旨在实现自主车辆的最佳纵向和横向控制。该算法确保了稳定收敛到一个次优控制器解决方案，特别是独立于跟随车辆动力学，这一声明通过数值模拟得到了证实。Hu等人[[52](#bib.bib52)]
    利用基于模型的深度强化学习（DRL）来实现从零开始的实际条件下的学习，从而实现了实际部署。这显示了DRL在自主车辆控制场景中的实际适用性和显著适应性。Chen等人[[57](#bib.bib57)]
    利用CNN、 自注意力网络和深度强化学习方法实现了感知、决策和运动控制的联合优化。在SUMO模拟环境中验证了该研究，强调了集成优化控制策略的潜力。Wang等人[[58](#bib.bib58)]
    展示了一种DDPG增强的线性主动扰动拒绝控制（LADRC）控制器，应用于三自由度（3-DOF）自主车辆，证明了其对不确定性的显著适应性以及通过深度强化学习实现的实时参数调整所带来的增强控制性能。尽管强化学习（RL）在整合控制各子问题方面已有一定应用，但必须承认，对于诸如ABS和ESP等关键安全控制子系统，RL的近期替代仍然具有挑战性，因为其时间和经验验证不足。
- en: Considering the transition of models trained within simulated environments to
    real-world contexts, the direct linkage of vehicle motion control to the physical
    realm necessitates a cautious approach when employing data-driven methods with
    limited interpretability. Wang et al. [[48](#bib.bib48)] devised a practical framework
    utilizing an actor-critic model. Under the auspices of safety constraints, they
    developed an RL-based trajectory tracking system and subsequently implemented
    this system as a lateral controller for full-sized vehicles. The crux of their
    approach was the employment of a lightweight adapter to forge a mapping between
    simulated environments and the real world. This strategy underwent validation
    on driving simulation hardware, affirmatively testing the model’s efficacy in
    actual driving situations, thereby facilitating a smoother transition from simulated
    frameworks to tangible applications.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到从模拟环境中训练的模型向实际环境的过渡，将车辆运动控制直接关联到物理领域，需要在使用数据驱动方法时采取谨慎的方法，因为这些方法的可解释性有限。王等人[[48](#bib.bib48)]
    设计了一个实用的框架，利用了一个演员-评论家模型。在安全约束的支持下，他们开发了一个基于RL的轨迹跟踪系统，并随后将该系统实施为全尺寸车辆的横向控制器。他们的方法的核心是使用一个轻量级适配器来建立模拟环境和现实世界之间的映射。这一策略在驾驶模拟硬件上进行了验证，积极测试了模型在实际驾驶情况下的有效性，从而促进了从模拟框架到实际应用的平滑过渡。
- en: 5 Integrated Motion Planning and Control via DRL
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 通过深度强化学习的集成运动规划与控制
- en: The process of human driving is inherently an end-to-end system. While drivers’
    brains may process intermediate outcomes like predicting the actions of other
    road users, selecting driving maneuvers, and local trajectory planning, these
    processes lack the explicit interfaces characteristic of layered decision-making
    controls. Humans are capable of navigating vehicles almost solely with visual
    cues. End-to-end supervised learning approaches strive to emulate human visual-dependent
    driving by directly mapping input sensor data to vehicular control commands.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 人类驾驶的过程本质上是一个端到端的系统。虽然驾驶员的大脑可能会处理一些中间结果，例如预测其他道路使用者的行为、选择驾驶操作以及局部轨迹规划，但这些过程缺乏分层决策控制的显式接口。人类几乎完全依靠视觉线索来驾驶车辆。端到端的监督学习方法旨在通过将输入传感器数据直接映射到车辆控制命令，来模拟人类视觉依赖的驾驶方式。
- en: 5.1 End-to-End DRL Methods for Autonomous Driving
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 自主驾驶的端到端深度强化学习方法
- en: End-to-end applications within the vehicle control sector have showcased their
    proficiency in effectively realizing well-defined goals. [[61](#bib.bib61)] employed
    Reinforcement Learning (RL) for drone control, utilizing a policy network comprised
    of two layers of multilayer perceptrons for local path planning and motion control.
    This method concentrates directly on enabling the drone to traverse specific points,
    bypassing the generation of detailed path trajectories. This approach facilitated
    ultra-high-speed control, outperforming the premier human contestants in aerial
    races. Additionally, this study explored drone control via optimal control techniques;
    however, the pivotal reason for RL’s superiority over optimal control lies in
    its ability to "offer improved optimization objectives," essentially direct effect
    realization.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在车辆控制领域的端到端应用展示了它们在有效实现明确定义目标方面的能力。[[61](#bib.bib61)] 使用了强化学习（RL）进行无人机控制，利用由两层多层感知器构成的策略网络进行局部路径规划和运动控制。这种方法直接集中于使无人机穿越特定点，跳过了详细路径轨迹的生成。这种方法实现了超高速控制，超越了顶级人类参赛者在空中竞赛中的表现。此外，这项研究还通过最优控制技术探索了无人机控制；然而，RL优于最优控制的关键原因在于其能够“提供更好的优化目标”，即直接效果的实现。
- en: '![Refer to caption](img/ba4184efc5f5013ef06f6110749ff5ab.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ba4184efc5f5013ef06f6110749ff5ab.png)'
- en: 'Figure 14: End-to-end control approaches in drone racing[[61](#bib.bib61)].'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：无人机竞赛中的端到端控制方法[[61](#bib.bib61)]。
- en: Beyond goals directly tied to control mechanisms, extant studies have elucidated
    how the intricate interplay between decision-making, planning, and execution facilitates
    enhanced control performance. Chen et al.[[57](#bib.bib57)] engaged in the joint
    learning and optimization of three critical components of autonomous vehicles—perception,
    decision-making, and motion control—a facet frequently overlooked in prevailing
    autonomous driving research. They utilized a Deep Reinforcement Learning (DRL)
    methodology, particularly through the development of a novel state representation
    mechanism. This mechanism processes sensory data via attention and convolutional
    neural network (CNN) layers, thereby augmenting the overarching efficacy of the
    autonomous driving strategy. Sensory data is initially funneled through attention
    layers, focusing on the extraction of pivotal local information, subsequently
    processed by CNN layers to incorporate a comprehensive view of global information,
    thus achieving a superior representation. The collaborative learning of decision-making
    and motion control contemplates the symbiotic relationship between these modules
    to forge a more efficacious autonomous driving strategy, a claim substantiated
    within the SUMO simulation environment.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接与控制机制相关的目标之外，现有研究已阐明决策、规划和执行之间复杂的相互作用如何促进增强的控制性能。陈等人[[57](#bib.bib57)] 参与了自主车辆三个关键组成部分——感知、决策和运动控制的联合学习与优化，这是当前自主驾驶研究中常被忽视的一个方面。他们采用了深度强化学习（DRL）方法，特别是通过开发一种新颖的状态表示机制。该机制通过注意力和卷积神经网络（CNN）层处理传感器数据，从而提升了自主驾驶策略的整体效能。传感器数据最初通过注意力层进行处理，关注于提取关键的局部信息，随后通过CNN层处理，以纳入全球信息的全面视角，从而实现了更优的表示。决策制定和运动控制的协同学习考虑了这些模块之间的共生关系，以制定更有效的自主驾驶策略，这一主张在SUMO模拟环境中得到了验证。
- en: '![Refer to caption](img/e62e3cfce1f63aac67dcd11eb3c4aa07.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e62e3cfce1f63aac67dcd11eb3c4aa07.png)'
- en: 'Figure 15: The overall system architecture to tactfully processes the sensing
    data for improvement of control [[57](#bib.bib57)].'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：整体系统架构巧妙地处理感知数据以改善控制[[57](#bib.bib57)]。
- en: A strategy for realizing end-to-end control involves leveraging supervised learning,
    capitalizing on extensive sensor and control output data. Long Short-Term Memory
    networks (LSTMs) are particularly apt for intricate sequence prediction tasks,
    attributed to their efficacy in managing extended sequence data, discerning long-term
    dependencies, and circumventing gradient vanishing or exploding issues, a capability
    reflected across a myriad of end-to-end applications. L. Chen et al. introduced
    an innovative motion planning framework dubbed "Parallel Planning," integrating
    artificial traffic scenarios with deep learning models. This framework empowers
    autonomous vehicles to assimilate environmental cues and adeptly navigate emergencies
    in a manner akin to human drivers, consequently elevating autonomous driving’s
    safety and sophistication. S, N.F. et al. unveiled an LSTM-based end-to-end model
    tailored for applications involving autonomous vehicles merging onto highways.
    Employing driving simulator hardware for both training and evaluation phases,
    this research harnessed expert driver data to instruct the LSTM network in executing
    high-speed merging maneuvers both accurately and securely.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 实现端到端控制的策略涉及利用监督学习，利用大量传感器和控制输出数据。长短期记忆网络（LSTM）特别适用于复杂的序列预测任务，因为它们在处理长序列数据、识别长期依赖关系以及避免梯度消失或爆炸问题方面表现出色，这一能力在众多端到端应用中得到了体现。L.
    Chen等人引入了一种创新的运动规划框架，称为“并行规划”，将人工交通场景与深度学习模型相结合。该框架使自主车辆能够 assimilate 环境提示，并以类似于人类驾驶员的方式灵活应对紧急情况，从而提高了自主驾驶的安全性和复杂性。S,
    N.F.等人揭示了一种基于LSTM的端到端模型，专门用于自主车辆并入高速公路的应用。该研究采用驾驶模拟器硬件进行训练和评估阶段，利用专家驾驶员数据指导LSTM网络在高速度合并操作中准确且安全地执行任务。
- en: The advent of large-scale models has prompted researchers to favor the development
    of increasingly complex models trained on extensive datasets. In online applications,
    end-to-end networks are capable of directly deriving necessary driving maneuvers
    from sensor outputs, thereby enhancing the coherence and immediacy of decision-making
    control. Nonetheless, this methodology is not without its significant shortcomings.
    Primarily, the vast dimensionality of state spaces in autonomous driving tasks
    necessitates data in exponential volumes, culminating in a profound need for extensive
    datasets. Presently available open-source datasets, including Waymo Open [[62](#bib.bib62)],
    Oxford Robotcar [[63](#bib.bib63)], ApolloScape [[64](#bib.bib64)], Udacity [[65](#bib.bib65)],
    and ETH Pedestrian [[66](#bib.bib66)], predominantly furnish data related to environmental
    perception, object detection, and tracking but do not directly supply driving
    behavior annotations. Additionally, given this strategy’s intrinsic reliance on
    neural networks’ capacity to generalize across data points, its safety assurances
    become challenging to validate in the face of novel or infrequent data instances.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模模型的出现促使研究人员更倾向于开发在广泛数据集上训练的日益复杂的模型。在在线应用中，端到端网络能够直接从传感器输出中推导出必要的驾驶动作，从而提高决策控制的连贯性和即时性。然而，这种方法并非没有显著缺点。首先，自动驾驶任务中状态空间的巨大维度需要以指数级的体量来获取数据，从而对大规模数据集的需求十分迫切。目前可用的开源数据集，包括Waymo
    Open [[62](#bib.bib62)]、Oxford Robotcar [[63](#bib.bib63)]、ApolloScape [[64](#bib.bib64)]、Udacity
    [[65](#bib.bib65)] 和 ETH Pedestrian [[66](#bib.bib66)]，主要提供与环境感知、物体检测和跟踪相关的数据，但并未直接提供驾驶行为注释。此外，鉴于这一策略固有地依赖于神经网络在数据点上的泛化能力，其安全保障在面对新颖或少见的数据实例时变得难以验证。
- en: 5.2 Implementations of End-to-End DRL in Driving Automation
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 驾驶自动化中端到端DRL的实现
- en: 'Table 3: Deep Reinforcement Learning Applications in Autonomous Driving'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 深度强化学习在自动驾驶中的应用'
- en: '| Work | RL Algorithm | Experiments | Pros |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | RL 算法 | 实验 | 优点 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| [[67](#bib.bib67)] | PPO | CARLA | PPO with GAE, wild environment |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| [[67](#bib.bib67)] | PPO | CARLA | PPO与GAE，野外环境 |'
- en: '| [[68](#bib.bib68)] | PG | Real-World | VISTA for Real-world Policy Transfer
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| [[68](#bib.bib68)] | PG | 现实世界 | 现实世界政策转移的VISTA |'
- en: '| [[69](#bib.bib69)] | SAC | Real-World | SESR for Enhanced Interpretability
    in Sim2Real |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| [[69](#bib.bib69)] | SAC | 现实世界 | SESR 提升Sim2Real的可解释性 |'
- en: '| [[70](#bib.bib70)] | LSTM | Driving simulator hardware | LSTM for Highway
    Merging |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| [[70](#bib.bib70)] | LSTM | 驾驶模拟器硬件 | LSTM用于高速公路合流 |'
- en: '| [[71](#bib.bib71)] | A3C | Custom-built Simulator And real word in a campus
    | A3C for Automated Lane Changing |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| [[71](#bib.bib71)] | A3C | 自定义模拟器和校园内现实世界 | A3C用于自动车道变换 |'
- en: '| [[72](#bib.bib72)] | Model-based RL | Custom-built Simulator And real word
    | IDC Framework, real-world, Optimized Control Excellence |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| [[72](#bib.bib72)] | 基于模型的RL | 自定义模拟器和现实世界 | IDC框架，现实世界，优化控制卓越 |'
- en: Contrasting with end-to-end controls predicated on supervised learning, Reinforcement
    Learning (RL) boasts the capability to gather data within simulated environments,
    facilitating expedited training across various contexts. The studies conducted
    by Amini et al.[[68](#bib.bib68)] and Chung et al.[[69](#bib.bib69)] have illuminated
    the efficacy of end-to-end RL techniques within the realm of autonomous driving
    applications. Utilizing the VISTA data-driven simulator, Amini et al. adeptly
    transitioned policies honed through RL to authentic driving contexts, navigating
    challenges presented by unfamiliar roads and intricate near-collision scenarios,
    thereby showcasing the bolstered robustness of end-to-end RL strategies in intricate
    navigation tasks. This substantiates the premise that strategies cultivated within
    simulators can efficaciously generalize to real-world thoroughfares, adeptly managing
    unprecedented scenes and complex predicaments, thus unveiling the potential of
    employing RL for effective perception and robust operations. Concurrently, the
    SESR method introduced by Chung et al., leveraging category-decoupled latent encoding,
    not only augmented the interpretability of end-to-end autonomous driving systems
    but also significantly alleviated the simulation-to-reality (Sim2Real) distribution
    shift issue, further validating the successful deployment of end-to-end RL approaches
    in genuine environments and their contribution to the refinement of control maneuvers.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于监督学习的端到端控制方法相比，强化学习 (RL) 具有在模拟环境中收集数据的能力，从而在各种环境中加速训练。Amini等人[[68](#bib.bib68)]
    和 Chung等人[[69](#bib.bib69)]的研究揭示了端到端RL技术在自动驾驶应用中的有效性。利用VISTA数据驱动的模拟器，Amini等人巧妙地将通过RL优化的策略转移到真实驾驶环境中，成功应对了陌生道路和复杂的近碰撞情境，展示了端到端RL策略在复杂导航任务中的增强鲁棒性。这证实了在模拟器中培养的策略能够有效地推广到真实世界的道路，能够处理前所未见的场景和复杂的困境，从而揭示了使用RL进行有效感知和稳健操作的潜力。同时，Chung等人提出的SESR方法，通过类别解耦的潜在编码，不仅增强了端到端自动驾驶系统的可解释性，还显著缓解了模拟到现实
    (Sim2Real) 分布偏移问题，进一步验证了端到端RL方法在真实环境中的成功应用及其对控制策略改进的贡献。
- en: '![Refer to caption](img/d6f728c7dc5b18955a63d4a02261b788.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d6f728c7dc5b18955a63d4a02261b788.png)'
- en: 'Figure 16: The process of deploying an end-to-end method. (A) Initiates with
    pre-training via supervised learning, subsequently undergoing RL optimization
    within the VISTA simulator (B) Transitions to deployment in the real world[[68](#bib.bib68)].'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：端到端方法的部署过程。 (A) 以通过监督学习进行预训练开始，随后在VISTA模拟器中经历RL优化 (B) 转到真实世界中的部署[[68](#bib.bib68)]。
- en: Analogous to trajectory planning endeavors, contemporary end-to-end Reinforcement
    Learning (RL) techniques are predominantly confined to particular scenarios. This
    limitation chiefly arises from the dependency on state and reward function designs
    intricately linked with specific scenarios, a condition that, even within singular
    tasks, presents substantial complexity and necessitates extensive manual fine-tuning.
    Addressing lane-changing dilemmas, Zhou et al.[[71](#bib.bib71)] experimented
    with an end-to-end Asynchronous Advantage Actor-Critic (A3C)-based RL framework,
    boosting exploration efficacy via a multi-threaded setting, and secured stable
    convergence through weighted averaging methodologies. Wang et al.[[67](#bib.bib67)]
    executed a lane-keeping feature on the CARLA simulation platform, utilizing images
    for input and translating these into steering maneuvers and acceleration outputs.
    A pioneering end-to-end deep reinforcement learning model, predicated on the Proximal
    Policy Optimization (PPO) algorithm, was devised expressly for the autonomous
    navigation of off-road Unmanned Ground Vehicles (UGVs). This model amalgamates
    various functionalities facilitated by PPO, encompassing gradient computation,
    objective functions, and amendment mechanisms, tailored to particular application
    scenarios and demonstrated to surpass the Soft Actor-Critic (SAC) approach in
    simulated settings. Furthermore, this model utilizes the Generalized Advantage
    Estimation (GAE) algorithm to diminish training variability and streamline the
    hyperparameter optimization process, rendering it optimally suited for scenarios
    characterized by high-dimensional states.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于轨迹规划的努力，现代端到端强化学习（RL）技术通常局限于特定场景。这一限制主要源于状态和奖励函数设计依赖于特定场景，即使在单一任务中也存在复杂性，并需要大量手动微调。针对车道变换问题，Zhou等人[[71](#bib.bib71)]尝试了一种基于异步优势演员-评论家（A3C）的端到端RL框架，通过多线程设置提高了探索效率，并通过加权平均方法确保了稳定收敛。Wang等人[[67](#bib.bib67)]在CARLA模拟平台上实施了车道保持功能，利用图像作为输入并将其转换为转向动作和加速输出。一种基于近端策略优化（PPO）算法的开创性端到端深度强化学习模型，专门用于自主导航越野无人地面车辆（UGV）。该模型融合了PPO提供的各种功能，包括梯度计算、目标函数和修正机制，针对特定应用场景进行了调整，并在模拟环境中优于软演员-评论家（SAC）方法。此外，该模型利用广义优势估计（GAE）算法减少训练变异性，简化超参数优化过程，使其在高维状态特征的场景中表现出色。
- en: In another investigation, the Integrated Decision and Control (IDC) framework
    introduced by Y Guan exhibited a model-based reinforcement learning approach that
    synergizes seamlessly with optimal control techniques. This methodology adeptly
    navigates the real-world application challenges of securing precise models and
    managing high computational complexities, which traditional strategies often encounter.
    Building upon a meticulously trained vehicle fitting model, this technique bifurcates
    the driving task into static path planning and dynamic optimal tracking. In contexts
    where computational speed demands are moderate, it employs optimal control strategies
    to forge statically viable paths, whereas actual execution relies on policy networks
    for path selection and adherence. This approach was validated within simulated
    environments featuring four-lane highways and intersections, followed by the agent’s
    transition to real-world implementations. It showcased remarkable online computational
    efficiency and extensive applicability.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项调查中，由Y Guan引入的集成决策与控制（IDC）框架展示了一种基于模型的强化学习方法，该方法与最优控制技术无缝协同。这种方法巧妙地解决了传统策略常遇到的精准模型获取和高计算复杂度的现实应用挑战。在精心训练的车辆拟合模型基础上，该技术将驾驶任务分为静态路径规划和动态最优跟踪。在计算速度需求适中的情况下，它采用最优控制策略来制定静态可行路径，而实际执行则依赖于策略网络进行路径选择和遵循。该方法在模拟环境中经过验证，涵盖了四车道高速公路和交叉口，之后代理转向现实世界的应用，展现了卓越的在线计算效率和广泛的适用性。
- en: '![Refer to caption](img/d5ab139d837025f42368ba5d0c546ee1.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d5ab139d837025f42368ba5d0c546ee1.png)'
- en: 'Figure 17: The real-world testing of the devised IDC framework. (a) Halting
    and awaiting the green traffic light. (b) Accelerating towards and entering the
    intersection. (c) Slowing down to prevent a collision and altering the route.
    (d) Accelerating to advance first upon selecting a safer pathway. (e) Establishing
    a set route and following it. (f) Successfully navigating through the intersection.[[72](#bib.bib72)]'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：设计的IDC框架的实际测试。(a) 停车等待绿灯。(b) 加速进入交叉口。(c) 减速以防碰撞并改变路线。(d) 加速以选择更安全的路径。(e)
    确定并沿设定路线行驶。(f) 成功通过交叉口。[[72](#bib.bib72)]
- en: 6 Challenges and Future directions
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 挑战与未来方向
- en: 6.1 Enhancing Training Stability and Convergence
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 提升训练的稳定性和收敛性
- en: 'Within the domain of autonomous vehicle planning and control, the stability
    and convergence of reinforcement learning model training represent a pivotal challenge.
    Elevating model complexity is intended to bolster adaptability to intricate scenarios
    and generalization capacity; however, it concurrently escalates the requisite
    for substantial volumes of high-quality training samples, amplifying the complexity
    inherent in the learning trajectory. Enhancements to the training regimen primarily
    concentrate on two dimensions: sample quality and algorithmic refinement.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在自主车辆规划和控制领域，强化学习模型训练的稳定性和收敛性是一个关键挑战。提高模型复杂性旨在增强对复杂场景的适应能力和泛化能力；然而，这同时也提高了对大量高质量训练样本的需求，增加了学习过程中的复杂性。训练方案的改进主要集中在两个方面：样本质量和算法优化。
- en: Within domains like autonomous driving and robotic control, amassing high-quality
    training samples grapples with hurdles related to latency, reward scarcity, and
    the uneven dispersion of observational outcomes across vast state spaces. Notably,
    when accruing valuable insights is either prohibitively expensive or fraught with
    risk, sample efficiency emerges as a pronounced difficulty. At present, numerous
    studies concentrate on refining methods for the acquisition of training samples.
    This includes investigations into the enhancement of training quality through
    the introduction of noise into training samples[[73](#bib.bib73)], alongside research
    aimed at boosting sample efficiency via the definition of the Bellman Eluder dimension[[74](#bib.bib74)].
    Collectively, these approaches strive to optimize sample usage to bolster training
    efficiency.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在自主驾驶和机器人控制等领域，积累高质量训练样本面临着与延迟、奖励稀缺以及在广阔状态空间中观察结果分布不均等相关的难题。特别是当获得宝贵的见解成本过高或充满风险时，样本效率成为一个显著问题。目前，许多研究集中在改进训练样本获取的方法上。这包括通过向训练样本中引入噪声来提升训练质量的研究[[73](#bib.bib73)]，以及通过定义Bellman
    Eluder维度来提高样本效率的研究[[74](#bib.bib74)]。这些方法共同致力于优化样本使用，以提高训练效率。
- en: Concurrently, numerous enhancements to reinforcement learning algorithms are
    designed to augment the stability and hasten the convergence rate of the training
    regimen. For instance, the Soft Actor-Critic algorithm incorporates entropy regularization
    into policy optimization to mediate between exploration and exploitation. Meanwhile,
    the Twin Delayed Deep Deterministic policy gradient (TD3) algorithm mitigates
    estimation biases and overestimation concerns by integrating dual value networks
    alongside policy smoothing methodologies. Moreover, model-based RL approaches
    enhance the stability and acceleration of training through the development of
    environmental models and the utilization of forecasts regarding dynamic environmental
    shifts.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，许多强化学习算法的改进旨在增强稳定性并加快训练的收敛速度。例如，Soft Actor-Critic算法在策略优化中引入了熵正则化，以调节探索与利用之间的平衡。与此同时，Twin
    Delayed Deep Deterministic policy gradient（TD3）算法通过结合双重价值网络和策略平滑方法来减轻估计偏差和过度估计问题。此外，基于模型的RL方法通过开发环境模型和利用关于动态环境变化的预测来提高训练的稳定性和加速训练过程。
- en: Utilizing pre-training and transfer learning strategies can significantly alleviate
    the challenges and costs associated with obtaining real-world road data, while
    expediting the convergence of models. Moreover, meticulously crafted reward functions
    can mitigate issues related to sparse rewards, recalibrate reward magnitudes,
    and augment the coherence and explicability of rewards, furnishing stable and
    efficacious direction throughout the training phase. Recent studies have explored
    the utilization of offline reinforcement learning for the pre-training of generic
    agents. Notably, approaches that expand offline reinforcement learning to employ
    extant static datasets for the training of value functions have showcased the
    capability for swiftly adapting to novel tasks and hastening the online learning
    of new task variations[[75](#bib.bib75)].
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 利用预训练和迁移学习策略可以显著减轻获取真实世界道路数据相关的挑战和成本，同时加快模型的收敛速度。此外，精心设计的奖励函数可以缓解与稀疏奖励相关的问题，重新校准奖励的大小，并增强奖励的一致性和可解释性，为训练阶段提供稳定而有效的方向。最近的研究探讨了离线强化学习在通用代理预训练中的应用。值得注意的是，将离线强化学习扩展到利用现有静态数据集训练价值函数的方法，展示了其快速适应新任务和加速新任务变体在线学习的能力[[75](#bib.bib75)]。
- en: 6.2 Addressing Comparability Issues
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 解决可比性问题
- en: Within the sphere of DRL application research, conducting comparisons across
    disparate methodologies and their performance outcomes in varied simulation settings
    constitutes a crucial aspect of the investigative process. Nevertheless, DRL is
    significantly reliant on the caliber of code execution and meticulous adjustments
    of hyperparameters. Should there be deficiencies in the code or inappropriate
    hyperparameter configurations, algorithms that are theoretically efficacious may
    falter in practical deployment scenarios. This article straightforwardly discusses
    the adjustment of hyperparameters, code refinement, and the amalgamation of various
    optimization strategies, culminating in performance that substantially outstrips
    that of other DQN methodologies in Atari games.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度强化学习应用研究领域，比较不同方法及其在各种仿真环境中的性能结果是研究过程中的一个关键方面。然而，深度强化学习在很大程度上依赖于代码执行的质量和超参数的精细调整。如果代码存在缺陷或超参数配置不当，理论上有效的算法可能会在实际部署场景中失败。本文直接讨论了超参数调整、代码优化和各种优化策略的结合，最终在Atari游戏中实现了远超其他DQN方法的性能。
- en: The DRL training regimen is subject to numerous uncertainties, encompassing
    environmental variability, methods of initialization, mechanisms for action selection,
    and strategies for experience replay. Variations in random seeds can also significantly
    impact the quality of results and the efficacy of convergence. Owing to these
    intrinsic attributes of DRL, directly comparing disparate models poses substantial
    challenges. Moreover, the comparison of model performances across varied testing
    platforms is profoundly influenced by the inherent properties of the data, including
    distribution heterogeneity, scale, and quality. In the absence of unified evaluation
    criteria and benchmarking tests, such comparisons may culminate in deceptive conclusions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习的训练过程面临许多不确定性，包括环境变化、初始化方法、行动选择机制和经验回放策略。随机种子的变化也会显著影响结果的质量和收敛的效果。由于这些深度强化学习的固有属性，直接比较不同模型面临着巨大的挑战。此外，模型在不同测试平台上的表现比较深受数据的固有属性影响，包括分布异质性、规模和质量。在缺乏统一评估标准和基准测试的情况下，此类比较可能会导致误导性的结论。
- en: Systematized approaches to hyperparameter searching, including grid search,
    random search, and Bayesian optimization, alongside parameterization strategies
    that sustain activation scale consistency throughout training, exemplified by
    µTransfer technology, significantly curtail the arbitrariness and subjectivity
    associated with manual tuning. This, in turn, facilitates the standardization
    of hyperparameter optimization and network architecture formulation. In their
    work on end-to-end off-road driving tasks, Wang et al.[[67](#bib.bib67)] utilized
    the Generalized Advantage Estimation (GAE) algorithm to diminish the volatility
    of PPO training and streamline the hyperparameter tuning process, achieving a
    superior overall return relative to outcomes yielded by the SAC methodology. They
    further hypothesized that this advantage is likely attributable to the enhanced
    efficacy of GAE in the context of hyperparameter optimization for PPO.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 系统化的超参数搜索方法，包括网格搜索、随机搜索和贝叶斯优化，以及在训练过程中保持激活规模一致性的参数化策略，如µTransfer技术，显著减少了手动调整中的随意性和主观性。这反过来又促进了超参数优化和网络架构制定的标准化。在对端到端越野驾驶任务的研究中，Wang等人[[67](#bib.bib67)]利用广义优势估计（GAE）算法来减少PPO训练的波动性，并简化超参数调优过程，相较于SAC方法获得了更优的整体回报。他们进一步假设，这一优势可能归因于GAE在PPO超参数优化中的更高效能。
- en: Investigating training and validation frameworks for RL facilitates equitable
    comparisons among diverse methodologies. The integration of standardized evaluation
    procedures and performance metrics, coupled with benchmark testing grounded in
    open-source scenario repositories, guarantees that varied approaches are juxtaposed
    under identical conditions, thereby cultivating a harmonized assessment framework.
    The disclosure of comprehensive details regarding experimental configurations,
    code implementations, and hyperparameter settings, alongside the employment of
    a standardized data reporting format, aids fellow researchers in replicating and
    corroborating experimental findings, thereby augmenting the transparency and reliability
    of comparative analyses.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 研究强化学习（RL）的训练和验证框架有助于在不同方法之间进行公平的比较。整合标准化的评估程序和性能指标，并在开源场景库中进行基准测试，确保了不同方法在相同条件下进行对比，从而培养了统一的评估框架。公开实验配置、代码实现和超参数设置的详细信息，以及采用标准化的数据报告格式，有助于其他研究人员复制和验证实验结果，从而增强比较分析的透明性和可靠性。
- en: Beyond juxtaposing various DRL algorithms, numerous articles benchmark their
    network outcomes against established control methodologies like Dynamic Programming
    (DP) and Model Predictive Control (MPC). These comparisons endeavor to showcase
    DRL’s capacity to outperform conventional control strategies in designated tasks,
    furnishing pivotal substantiation for DRL’s broader deployment in real-world application
    scenarios. Nonetheless, it’s critical to acknowledge that within the realm of
    vehicle control, devising and executing methodologies such as MPC presents considerable
    challenges. Several investigations, despite employing numerical simulations or
    developing simulated environments, have not thoroughly validated the potential
    of established control strategies. Comparisons of this nature frequently overlook
    variances in the implementation conditions of diverse methods, potentially culminating
    in inequitable assessment outcomes. Consequently, it is advised that such research
    endeavors should enhance the quality of mature control method implementations
    within benchmarks, or solicit participation from vehicle control sector practitioners,
    to guarantee the fairness and precision of these comparisons.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了并列各种深度强化学习（DRL）算法外，许多文章还将其网络结果与已建立的控制方法进行基准对比，如动态规划（DP）和模型预测控制（MPC）。这些比较旨在展示DRL在特定任务中超越传统控制策略的能力，为DRL在现实世界应用场景中的广泛部署提供关键证据。然而，必须承认，在车辆控制领域，设计和执行如MPC这样的控制方法面临着相当大的挑战。尽管一些研究通过数值模拟或开发模拟环境进行探索，但尚未彻底验证已建立控制策略的潜力。这类比较经常忽视不同方法实施条件的差异，可能导致不公平的评估结果。因此，建议此类研究应提升成熟控制方法在基准测试中的实施质量，或寻求车辆控制领域从业者的参与，以确保这些比较的公平性和准确性。
- en: 6.3 Bridging the Simulation-to-Real-World Gap
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3  缩小仿真与现实世界的差距
- en: Transitioning from simulated to real-world environments represents a dynamic
    area of inquiry within Deep Reinforcement Learning (DRL), driven by simulations
    serving as extensive, accurately annotated, and economically viable data reservoirs.
    Via domain adaptation on both feature and pixel dimensions, OpenAI[[76](#bib.bib76)]
    adeptly trained a robotic arm within the GYM environment, enabling it to execute
    grasping maneuvers in the real world sans supplemental real-world training. This
    endeavor hinges on the Domain Randomization strategy, which entails integrating
    randomness into the simulation training milieu to encompass potential real-world
    scenarios, thus narrowing the divide between simulated and actual environments.
    Within autonomous driving, research has involved training A3C agents in environments
    featuring simulation-to-real[[77](#bib.bib77)] converted imagery, subsequently
    assessing their performance against real-world driving datasets.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从模拟到现实世界环境的过渡是深度强化学习（DRL）中的一个动态研究领域，模拟作为广泛、准确标注和经济可行的数据库。通过对特征和像素维度的领域适应，OpenAI[[76](#bib.bib76)]在GYM环境中熟练地训练了一个机器人臂，使其能够在现实世界中执行抓取操作，而无需额外的现实世界训练。这一努力依赖于领域随机化策略，即在模拟训练环境中引入随机性，以涵盖潜在的现实世界场景，从而缩小模拟环境与实际环境之间的差距。在自动驾驶领域，研究涉及在具有模拟到现实[[77](#bib.bib77)]转换图像的环境中训练A3C代理，随后评估其在现实世界驾驶数据集上的表现。
- en: Yet, transitioning from simulated environments to the real world constitutes
    one of the foremost challenges encountered by DRL within the sphere of autonomous
    driving applications. Although simulated settings offer a safe and controllable
    learning context, the real world’s complexity and unpredictability vastly surpass
    those of their simulated counterparts. To guarantee a model’s performance and
    stability under authentic road conditions, an array of targeted technological
    approaches and strategies must be employed. The crux involves enhancing the fidelity
    of simulation models, necessitating the creation of simulated environments grounded
    on accurate GIS and real-time traffic data, in conjunction with deploying sophisticated
    sensor data for the meticulous modeling of environmental dynamics.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从模拟环境过渡到现实世界构成了DRL在自动驾驶应用领域面临的主要挑战之一。虽然模拟环境提供了一个安全且可控的学习背景，但现实世界的复杂性和不可预测性远远超过了模拟环境。为了确保模型在真实道路条件下的性能和稳定性，必须采用一系列有针对性的技术方法和策略。关键在于提高模拟模型的真实度，需要基于准确的GIS和实时交通数据创建模拟环境，并结合使用复杂的传感器数据对环境动态进行细致建模。
- en: The efficacy of transfer strategies is profoundly contingent upon the caliber
    and methodologies employed in data processing. Optimal training datasets ought
    to encompass a broad spectrum of driving scenarios, such as severe weather conditions,
    diverse traffic configurations, and myriad emergency situations, to affirm the
    model’s adaptability to the real world’s intricacies. Concurrently, navigating
    the uncertainty of data and harmonizing datasets from varied origins and frequencies
    are critical for affirming the efficacy of simulation training.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 转移策略的有效性深刻依赖于数据处理所采用的质量和方法。最佳训练数据集应涵盖广泛的驾驶场景，如恶劣天气条件、不同交通配置和各种紧急情况，以确认模型对现实世界复杂性的适应能力。同时，处理数据的不确定性以及协调来自不同来源和频率的数据集对于确认模拟训练的有效性至关重要。
- en: Conversely, while our present inquiries into generalization predominantly focus
    on the adaptability to diverse scenarios, attention must also be granted to agents’
    capacity for interfacing with varying hardware, thereby enhancing an agent’s integration
    with its "body". The application of sophisticated agents within the actual physical
    realm offers numerous referential methodologies within the domain of embodied
    intelligence. This applicability extends not solely to autonomous vehicles but
    also to realms encompassing industrial unmanned vehicles and robots equipped with
    mobile platforms, representing a research direction of considerable value.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，虽然我们当前对泛化的研究主要集中于对不同场景的适应能力，但还必须关注代理与不同硬件的接口能力，从而增强代理与其“身体”的整合。将复杂的代理应用于实际物理领域为体现智能领域提供了许多参考方法。这种适用性不仅限于自动驾驶车辆，还扩展到包括工业无人车辆和配备移动平台的机器人等领域，代表了一个具有重要价值的研究方向。
- en: Beyond the preparation of models and data, actual deployment necessitates considerations
    of ethics, safety, and compliance with regulations, rendering real-world testing
    a complex endeavor. Augmenting the validation capabilities of simulation environments
    presents a viable solution. Recent investigations [[78](#bib.bib78)] have navigated
    the bottlenecks of safety verification via Dense Deep Reinforcement Learning (D2RL)
    techniques. The adoption of intelligent testing environments and the study of
    adversarial maneuvers have enhanced the resilience and dependability of autonomous
    driving technologies against the complexities of the real world.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型和数据的准备，实际部署还需要考虑伦理、安全和法规合规性，使得真实世界测试成为一个复杂的任务。增强仿真环境的验证能力是一个可行的解决方案。最近的研究[[78](#bib.bib78)]通过密集深度强化学习（D2RL）技术解决了安全验证的瓶颈。智能测试环境的采用和对抗性操作的研究提高了自动驾驶技术在复杂现实世界中的韧性和可靠性。
- en: 6.4 Improving Computational Efficiency and Real-time Processing
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 提高计算效率和实时处理能力
- en: During the model evaluation stage, assessing the model’s accuracy is crucial,
    but evaluating its real-time performance—specifically, whether the model’s response
    time and processing speed satisfy the real-time demands of autonomous driving—is
    equally important, particularly upon incorporating complex architectures like
    the Transformer. To surmount this challenge, the employment of model optimization
    techniques—such as model pruning, quantization, and knowledge distillation—serves
    as a pivotal strategy for enhancing computational efficiency and real-time processing
    prowess. By diminishing the complexity and computational requisites of the model,
    these techniques facilitate an expedited inference process. Additionally, the
    design of lightweight network structures and the utilization of hardware acceleration
    technologies—like GPUs and TPUs—represent effective measures to bolster the real-time
    performance of autonomous driving systems.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型评估阶段，评估模型的准确性至关重要，但同样重要的是评估其实时性能——特别是模型的响应时间和处理速度是否满足自动驾驶的实时需求，尤其是在引入像Transformer这样复杂的架构时。为了克服这一挑战，模型优化技术的应用——如模型剪枝、量化和知识蒸馏——是提升计算效率和实时处理能力的关键策略。通过降低模型的复杂性和计算需求，这些技术能够加速推理过程。此外，轻量级网络结构的设计和硬件加速技术的利用——如GPU和TPU——也是提升自动驾驶系统实时性能的有效措施。
- en: Model deployment represents a pivotal phase, entailing challenges in transitioning
    models from development settings (like Python) to real-world applications (such
    as C++ inference frameworks). Prominent inference frameworks, including TensorRT,
    OpenVINO, and TVM, are designed to optimize model execution efficiency on specified
    hardware platforms, facilitating efficient model conversion and deployment. While
    these frameworks aid in the efficient operation of models on vehicular computing
    platforms, the deployment process must still navigate challenges related to performance
    disparities, environmental compatibility, real-time demands, and constraints on
    resources. A holistic strategy is requisite to guarantee that models not only
    adhere to the stringent real-time criteria of autonomous driving but also maintain
    stable operation within environments constrained by resources.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署是一个关键阶段，涉及将模型从开发环境（如Python）转移到实际应用（如C++推理框架）的挑战。包括TensorRT、OpenVINO和TVM在内的主流推理框架旨在优化模型在特定硬件平台上的执行效率，促进模型转换和部署。尽管这些框架有助于在车辆计算平台上高效运行模型，但部署过程仍需应对性能差异、环境兼容性、实时需求和资源限制等挑战。必须采用全面的策略，以确保模型不仅符合自动驾驶的严格实时标准，还能在资源受限的环境中稳定运行。
- en: 6.5 Safety
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 安全
- en: 'In [[79](#bib.bib79)], two methodologies for training safety-compliant agents
    were outlined: incorporating a safety layer into the decision-making process or
    modifying the optimization criteria. Within the realm of DRL’s application to
    autonomous driving, a safety layer ensures operational safety by real-time monitoring
    of potential risks and intervening when potential violations of safety constraints
    are detected. The safety layer, as an integral part of the system, not only perpetually
    monitors and assesses potential safety risks but also corrects or substitutes
    strategies as needed to avert dangerous scenarios. As an instance, Gu et al.[[80](#bib.bib80)]
    detailed a state-based method for augmenting safety in autonomous driving, which
    notably enhances both performance and safety within intricate highway contexts
    by amalgamating dynamic goal setting with adaptive safety constraints via hierarchical
    reinforcement learning.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[79](#bib.bib79)]中，概述了两种训练符合安全标准的智能体的方法：在决策过程中加入安全层或修改优化标准。在DRL应用于自动驾驶的领域中，安全层通过实时监控潜在风险并在检测到可能违反安全约束时进行干预，从而确保操作安全。安全层作为系统的一个组成部分，不仅持续监控和评估潜在的安全风险，还根据需要纠正或替代策略以避免危险场景。例如，Gu等人[[80](#bib.bib80)]详细介绍了一种基于状态的方法来增强自动驾驶的安全性，通过将动态目标设定与通过分层强化学习的自适应安全约束结合，显著提高了复杂高速公路环境中的性能和安全性。
- en: In supervised learning, meticulously adjusting input data serves as an efficacious
    method to enhance the precision of model outputs. Within reinforcement learning,
    implementing hard constraints to curtail agents from undertaking perilous actions
    could precipitate sparse rewards issues, complicating the convergence of the learning
    process. In reaction, an indirect restriction on agents’ outputs through the preprocessing
    of input data might be viable, as opposed to directly limiting their action selection.
    State elements that precipitate hazardous actions can be pinpointed through feature
    engineering and state filtering techniques. The fundamental principle behind this
    strategy is to optimize input data in a manner that steers agents away from potentially
    hazardous or superfluous actions, without detracting from the agent’s thorough
    and profound exploration of the environment. It’s pertinent to acknowledge that
    excessively constraining the input could diminish the agent’s exploratory capabilities.
    Consequently, an approach of indirectly restricting outputs could be employed
    either during the pre-training stage of the agent or upon the training attaining
    a certain threshold. This entails preprocessing input data to navigate agents
    away from perilous or undesired behaviors, thus harmonizing the imperatives of
    safety and exploratory capacity.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，细致地调整输入数据是提高模型输出精度的有效方法。在强化学习中，实施硬性约束以限制智能体进行危险行为可能会导致奖励稀疏问题，从而使学习过程的收敛变得复杂。对此，通过对输入数据进行预处理间接限制智能体的输出可能是一种可行的方法，而不是直接限制其行动选择。通过特征工程和状态过滤技术可以识别出导致危险行为的状态元素。这一策略的基本原则是优化输入数据，以引导智能体远离潜在的危险或多余的行为，同时不影响智能体对环境的深入探索。需要注意的是，过度限制输入可能会减少智能体的探索能力。因此，可以在智能体的预训练阶段或当训练达到一定阈值时采用间接限制输出的方法。这涉及预处理输入数据以引导智能体避免危险或不希望出现的行为，从而协调安全性和探索能力的需求。
- en: 7 Conclusions
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This article provides a thorough review of recent literature, exploring the
    application of deep reinforcement learning in unmanned vehicle path planning and
    control. The document encapsulates the application of DRL across specific contexts,
    including intersection management, urban road navigation, and highway motoring,
    whilst elaborately discussing how these technologies tackle pivotal autonomous
    driving challenges like dynamic obstacle detection, adaptability to traffic conditions,
    and environmental generalization. Despite the presence of established examples
    within these domains, the incorporation of DRL has notably enhanced the operational
    efficacy and decision-making optimization of autonomous driving systems.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了对近期文献的全面回顾，探讨了深度强化学习在无人驾驶车辆路径规划和控制中的应用。文档概述了DRL在交叉口管理、城市道路导航和高速公路驾驶等具体情境中的应用，并详细讨论了这些技术如何应对关键的自动驾驶挑战，如动态障碍物检测、对交通条件的适应性以及环境泛化。尽管这些领域内已有成熟的例子，但DRL的引入显著提升了自动驾驶系统的操作效率和决策优化。
- en: An analysis was conducted on the application of diverse DRL methodologies within
    pivotal subdomains of autonomous driving, encompassing path planning, vehicle
    control, and end-to-end control. Within the realm of path planning tasks, SAC,
    PPO, TD3, DQN, and DDPG exhibited exceptional prowess, notably in scenarios necessitating
    dynamic adaptation and intricate decision-making processes. For control tasks,
    DDPG has been extensively utilized owing to its explicit gradient signaling and
    elevated sample efficiency in addressing continuous control challenges. Model-based
    DRL and ADP have manifested their worth in long-haul control endeavors that demand
    accurate dynamic modeling and control. End-to-end control is marked by its complexity
    and is frequently employed alongside self-attention mechanisms, optimal control
    strategies, and model-based RL, to name a few. Such integrative technologies proficiently
    navigate the hurdles associated with end-to-end learning.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对各种DRL方法在自动驾驶关键子领域的应用进行了分析，包括路径规划、车辆控制和端到端控制。在路径规划任务中，SAC、PPO、TD3、DQN 和 DDPG
    展现了卓越的能力，特别是在需要动态适应和复杂决策过程的场景中。对于控制任务，由于其明确的梯度信号和在处理连续控制挑战中的高样本效率，DDPG 已被广泛使用。基于模型的DRL和ADP在需要准确动态建模和控制的长途控制任务中展现了其价值。端到端控制以其复杂性为特征，通常与自注意力机制、最优控制策略和基于模型的RL等技术一起使用。这些综合技术有效应对了端到端学习相关的挑战。
- en: As hardware computational capabilities enhance and algorithms are refined, the
    deployment of DRL in autonomous driving is set to broaden, accompanied by increasingly
    complex neural network architectures. The discourse underscored the challenges
    associated with enhancing the comparability, stability, generalization capacity,
    and post-deployment vehicular inference efficiency, alongside the imperative of
    facilitating transitions to real-world applications and ensuring safety during
    unforeseen circumstances. To aptly showcase DRL’s efficacy within autonomous driving,
    the development of more efficient and stable training methodologies, more precise
    environmental simulation techniques, and stricter safety norms is requisite. The
    amalgamation of these technological strides and practical implementations harbors
    the potential to realize autonomous vehicles that are smarter, safer, and more
    efficient.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 随着硬件计算能力的提升和算法的优化，DRL在自动驾驶中的应用将会扩大，同时神经网络架构也会变得越来越复杂。讨论中强调了提高可比性、稳定性、泛化能力以及部署后车辆推断效率的挑战，并指出了在实际应用中的过渡和在突发情况下确保安全的重要性。为了充分展示DRL在自动驾驶中的效能，需要开发更高效、稳定的训练方法、更精确的环境模拟技术和更严格的安全规范。这些技术进步和实际应用的结合有潜力实现更智能、更安全、更高效的自动驾驶车辆。
- en: Acknowledgments
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is xxxx. This research was partially supported by the xxxxx.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作是 xxxx。此研究部分得到 xxxxx 的资助。
- en: References
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] E. Yurtsever, J. Lambert, A. Carballo, et al. A survey of autonomous driving:
    Common practices and emerging technologies. IEEE Access, 8:58443–58469, 2020.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] E. Yurtsever, J. Lambert, A. Carballo, 等. 自动驾驶调查：常见实践和新兴技术。IEEE Access,
    8:58443–58469, 2020.'
- en: '[2] Pietro Stano, Umberto Montanaro, Davide Tavernini, Manuela Tufo, Giovanni
    Fiengo, Luigi Novella, and Aldo Sorniotti. Model predictive path tracking control
    for automated road vehicles: A review. Annual reviews in control, 55:194–236,
    2023.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Pietro Stano, Umberto Montanaro, Davide Tavernini, Manuela Tufo, Giovanni
    Fiengo, Luigi Novella, 和 Aldo Sorniotti. 自动化道路车辆的模型预测路径跟踪控制：综述。控制年鉴，55:194–236，2023年。'
- en: '[3] Antonio Artuñedo, Marcos Moreno-Gonzalez, and Jorge Villagra. Lateral control
    for autonomous vehicles: A comparative evaluation. Annual Reviews in Control,
    57:100910, 2024.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Antonio Artuñedo, Marcos Moreno-Gonzalez, 和 Jorge Villagra. 自主车辆的横向控制：比较评估。控制年鉴，57:100910，2024年。'
- en: '[4] Changxi You, Jianbo Lu, Dimitar Filev, and Panagiotis Tsiotras. Advanced
    planning for autonomous vehicles using reinforcement learning and deep inverse
    reinforcement learning. Robotics and Autonomous Systems, 114:1–18, 2019.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Changxi You, Jianbo Lu, Dimitar Filev, 和 Panagiotis Tsiotras. 使用强化学习和深度逆强化学习进行自主车辆的高级规划。机器人与自主系统，114:1–18，2019年。'
- en: '[5] Ammar Haydari and Yasin Yılmaz. Deep reinforcement learning for intelligent
    transportation systems: A survey. IEEE Transactions on Intelligent Transportation
    Systems, 23(1):11–32, 2020.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ammar Haydari 和 Yasin Yılmaz. 智能交通系统中的深度强化学习：综述。IEEE 智能交通系统汇刊，23(1):11–32，2020年。'
- en: '[6] S. Aradi. Survey of deep reinforcement learning for motion planning of
    autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems,
    23(2):740–759, 2020.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] S. Aradi. 深度强化学习在自主车辆运动规划中的综述。IEEE 智能交通系统汇刊，23(2):740–759，2020年。'
- en: '[7] Laurene Claussmann, Marc Revilloud, Dominique Gruyer, and Sébastien Glaser.
    A review of motion planning for highway autonomous driving. IEEE Transactions
    on Intelligent Transportation Systems, 21(5):1826–1848, 2019.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Laurene Claussmann, Marc Revilloud, Dominique Gruyer, 和 Sébastien Glaser.
    高速公路自主驾驶的运动规划综述。IEEE 智能交通系统汇刊，21(5):1826–1848，2019年。'
- en: '[8] Sampo Kuutti, Richard Bowden, Yaochu Jin, Phil Barber, and Saber Fallah.
    A survey of deep learning applications to autonomous vehicle control. IEEE Transactions
    on Intelligent Transportation Systems, 22(2):712–733, 2020.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Sampo Kuutti, Richard Bowden, Yaochu Jin, Phil Barber, 和 Saber Fallah.
    深度学习在自主车辆控制中的应用综述。IEEE 智能交通系统汇刊，22(2):712–733，2020年。'
- en: '[9] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, 和 Oleg Klimov.
    近端策略优化算法。arXiv 预印本 arXiv:1707.06347，2017年。'
- en: '[10] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
    Moritz. Trust region policy optimization. In International conference on machine
    learning, pages 1889–1897\. PMLR, 2015.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, 和 Philipp
    Moritz. 信任区域策略优化。在国际机器学习会议上，页码1889–1897。PMLR，2015年。'
- en: '[11] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, 和 Daan Wierstra. 使用深度强化学习进行连续控制。arXiv 预印本
    arXiv:1509.02971，2015年。'
- en: '[12] Stephen Dankwa and Wenfeng Zheng. Twin-delayed ddpg: A deep reinforcement
    learning technique to model a continuous movement of an intelligent robot agent.
    In Proceedings of the 3rd international conference on vision, image and signal
    processing, pages 1–5, 2019.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Stephen Dankwa 和 Wenfeng Zheng. 双重延迟 DDPG：一种用于建模智能机器人代理连续运动的深度强化学习技术。在第三届国际视觉、图像和信号处理会议论文集中，页码1–5，2019年。'
- en: '[13] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized
    experience replay. arXiv preprint arXiv:1511.05952, 2015.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Tom Schaul, John Quan, Ioannis Antonoglou, 和 David Silver. 优先经验回放。arXiv
    预印本 arXiv:1511.05952，2015年。'
- en: '[14] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul,
    Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et al. Deep
    q-learning from demonstrations. In Proceedings of the AAAI conference on artificial
    intelligence, volume 32, 2018.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul,
    Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband 等. 从演示中学习深度 Q
    学习。在 AAAI 人工智能会议论文集中，卷32，2018年。'
- en: '[15] Xiaoqin Zhang and Huimin Ma. Pretraining deep actor-critic reinforcement
    learning algorithms with expert demonstrations. arXiv preprint arXiv:1801.10459,
    2018.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Xiaoqin Zhang 和 Huimin Ma. 通过专家演示进行深度演员-评论家强化学习算法的预训练。arXiv 预印本 arXiv:1801.10459，2018年。'
- en: '[16] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H
    Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey
    Levine, et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374,
    2019.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 卢卡斯·凯瑟尔、穆罕默德·巴巴伊扎德、皮奥特·米洛斯、布拉泽·奥辛斯基、罗伊·H·坎贝尔、孔拉德·切赫科夫斯基、杜米特鲁·厄尔汉、切尔西·芬、皮奥特·科扎科夫斯基、谢尔盖·莱文等。基于模型的强化学习用于雅达利。arXiv
    预印本 arXiv:1903.00374，2019年。'
- en: '[17] Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez,
    and Sergey Levine. Model-based value estimation for efficient model-free reinforcement
    learning. arXiv preprint arXiv:1803.00101, 2018.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 弗拉基米尔·费因贝格、阿尔文·万、伊昂·斯托伊卡、迈克尔·I·乔丹、约瑟夫·E·冈萨雷斯和谢尔盖·莱文。基于模型的价值估计用于高效的无模型强化学习。arXiv
    预印本 arXiv:1803.00101，2018年。'
- en: '[18] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous
    deep q-learning with model-based acceleration. In International conference on
    machine learning, pages 2829–2838\. PMLR, 2016.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 石翔顾、蒂莫西·利利克拉普、伊利亚·苏茨克维尔和谢尔盖·莱文。带模型加速的连续深度Q学习。在国际机器学习会议论文集中，页2829–2838。PMLR，2016年。'
- en: '[19] David González, Joshué Pérez, Vicente Milanés, and Fawzi Nashashibi. A
    review of motion planning techniques for automated vehicles. IEEE Transactions
    on intelligent transportation systems, 17(4):1135–1145, 2015.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 大卫·冈萨雷斯、约书亚·佩雷斯、维森特·米拉内斯和法兹·纳沙希比。自动驾驶车辆的运动规划技术综述。IEEE 智能交通系统学报，17(4):1135–1145，2015年。'
- en: '[20] Edouard Leurent. A survey of state-action representations for autonomous
    driving. 2018.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 爱德华·勒朗。自主驾驶的状态-动作表示综述。2018年。'
- en: '[21] Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under
    reward transformations: Theory and application to reward shaping. In Icml, volume 99,
    pages 278–287, 1999.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 安德鲁·Y·吴、大石原田和斯图尔特·拉塞尔。奖励变换下的策略不变性：理论及其在奖励塑造中的应用。在Icml，卷99，页278–287，1999年。'
- en: '[22] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement
    learning. In Proceedings of the twenty-first international conference on Machine
    learning, page 1, 2004.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 皮特·阿贝尔和安德鲁·Y·吴。通过逆向强化学习的学徒学习。在第21届国际机器学习会议论文集中，页1，2004年。'
- en: '[23] D. A. Yudin, A. Skrynnik, A. Krishtopik, I. Belkin, and A. I. Panov. Object
    detection with deep neural networks for reinforcement learning in the task of
    autonomous vehicles path planning at the intersection. Optical Memory and Neural
    Networks, 28(4):283–295, 2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] D. A. 尤丁、A. 斯克林尼克、A. 克里什托皮克、I. 贝尔金和A. I. 帕诺夫。用于自动驾驶车辆路径规划任务中的深度神经网络物体检测。光学记忆与神经网络，28(4):283–295，2019年。'
- en: '[24] Duy Quang Tran and Sang-Hoon Bae. Proximal policy optimization through
    a deep reinforcement learning framework for multiple autonomous vehicles at a
    non-signalized intersection. Applied Sciences-Basel, 10(16), 2020.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 杜奎昂·陈和尚勋·贝。通过深度强化学习框架的近端策略优化，用于非信号交叉口的多个自主车辆。应用科学-巴塞尔，10(16)，2020年。'
- en: '[25] Yuqi Liu, Yinfeng Gao, Qichao Zhang, Dawei Ding, and Dongbin Zhao. Multi-task
    safe reinforcement learning for navigating intersections in dense traffic. Journal
    of the Franklin Institute-Engineering and Applied Mathematics, 360(17):13737–13760,
    2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 刘玉琪、高银凤、张奇超、丁大伟和赵东宾。用于密集交通中交叉口导航的多任务安全强化学习。富兰克林学报-工程与应用数学，360(17):13737–13760，2023年。'
- en: '[26] Yanqiu Cheng, Xianbiao Hu, Kuanmin Chen, Xinlian Yu, and Yulong Luo. Online
    longitudinal trajectory planning for connected and autonomous vehicles in mixed
    traffic flow with deep reinforcement learning approach. Journal of Intelligent
    Transportation Systems, 27(3):396–410, 2023.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 陈艳秋、胡献标、陈宽敏、余新联和罗玉龙。基于深度强化学习方法的混合交通流中连接与自动驾驶车辆的在线纵向轨迹规划。智能交通系统学报，27(3):396–410，2023年。'
- en: '[27] Weitao Zhou, Kun Jiang, Zhong Cao, Nanshan Deng, and Diange Yang. Integrating
    deep reinforcement learning with optimal trajectory planner for automated driving.
    In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems
    (ITSC), 2020.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 周伟涛、姜坤、曹中、邓南山和杨垫格。将深度强化学习与最优轨迹规划器结合用于自动驾驶。在2020年IEEE第23届国际智能交通系统大会（ITSC）中，2020年。'
- en: '[28] J. Clemmons and Y.-F. Jin. Reinforcement learning-based guidance of autonomous
    vehicles. In 2023 24th International Symposium on Quality Electronic Design (ISQED),
    pages 1–6, 2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. 克莱蒙斯和Y.-F. 金。基于强化学习的自主车辆引导。在2023年第24届质量电子设计国际研讨会（ISQED）中，页1–6，2023年。'
- en: '[29] L. Gao, Y. Wu, L. Wang, L. Wang, J. Zhang, and K. Li. End-to-end autonomous
    vehicle navigation control method guided by the dynamic window approach. In 2023
    IEEE 6th International Electrical and Energy Conference (CIEEC), pages 4472–4476,
    2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] L. Gao, Y. Wu, L. Wang, L. Wang, J. Zhang, 和 K. Li. 基于动态窗口方法的端到端自动驾驶车辆导航控制方法。在2023年IEEE第六届国际电气与能源会议（CIEEC）中，第4472–4476页，2023年。'
- en: '[30] Tawfiq M. Aljohani, Ahmed Ebrahim, and Osama Mohammed. Real-time metadata-driven
    routing optimization for electric vehicle energy consumption minimization using
    deep reinforcement learning and markov chain model. Electric Power Systems Research,
    192, 2021.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Tawfiq M. Aljohani, Ahmed Ebrahim, 和 Osama Mohammed. 基于深度强化学习和马尔可夫链模型的电动汽车能耗最小化的实时元数据驱动路由优化。电力系统研究，192，2021年。'
- en: '[31] Arpad Feher, Szilard Aradi, and Tamas Becsi. Hierarchical evasive path
    planning using reinforcement learning and model predictive control. IEEE Access,
    8:187470–187482, 2020.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Arpad Feher, Szilard Aradi, 和 Tamas Becsi. 使用强化学习和模型预测控制的层次化规避路径规划。IEEE
    Access，8:187470–187482，2020年。'
- en: '[32] Zhitao Wang, Yuzheng Zhuang, Qiang Gu, Dong Chen, Hongbo Zhang, and Wulong
    Liu. Reinforcement learning based negotiation-aware motion planning of autonomous
    vehicles. In 2021 IEEE/RSJ International Conference on Intelligent Robots and
    Systems (IROS), pages 4532–4537, 2021.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Zhitao Wang, Yuzheng Zhuang, Qiang Gu, Dong Chen, Hongbo Zhang, 和 Wulong
    Liu. 基于强化学习的自适应协商运动规划用于自动驾驶车辆。在2021年IEEE/RSJ国际智能机器人与系统会议（IROS）中，第4532–4537页，2021年。'
- en: '[33] Mei Zhang, Kai Chen, and Jinhui Zhu. An efficient planning method based
    on deep reinforcement learning with hybrid actions for autonomous driving on highway.
    International Journal of Machine Learning and Cybernetics, 14(10):3483–3499, 2023.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Mei Zhang, Kai Chen, 和 Jinhui Zhu. 一种基于深度强化学习与混合动作的高效规划方法，用于高速公路自动驾驶。国际机器学习与网络期刊，14(10):3483–3499，2023年。'
- en: '[34] Rolando Bautista-Montesano, Rogelio Bustamante-Bello, and Ricardo A. Ramirez-Mendoza.
    Explainable navigation system using fuzzy reinforcement learning. International
    Journal of Interactive Design and Manufacturing - IJIDeM, 14(4):1411–1428, 2020.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Rolando Bautista-Montesano, Rogelio Bustamante-Bello, 和 Ricardo A. Ramirez-Mendoza.
    使用模糊强化学习的可解释导航系统。国际互动设计与制造期刊 - IJIDeM，14(4):1411–1428，2020年。'
- en: '[35] Ruiqi Zhang, Jing Hou, Guang Chen, Zhijun Li, Jianxiao Chen, and Alois
    Knoll. Residual policy learning facilitates efficient model-free autonomous racing.
    IEEE Robotics and Automation Letters, 7(4):11625–11632, 2022.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ruiqi Zhang, Jing Hou, Guang Chen, Zhijun Li, Jianxiao Chen, 和 Alois Knoll.
    残差策略学习促进了高效的无模型自主赛车。IEEE机器人与自动化快报，7(4):11625–11632，2022年。'
- en: '[36] Xiaohui Hou, Junzhi Zhang, Chengkun He, Yuan Ji, Junfeng Zhang, and Jinheng
    Han. Autonomous driving at the handling limit using residual reinforcement learning.
    Advanced Engineering Informatics, 54, 2022.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Xiaohui Hou, Junzhi Zhang, Chengkun He, Yuan Ji, Junfeng Zhang, 和 Jinheng
    Han. 使用残差强化学习进行的处理极限下的自主驾驶。先进工程信息学，54，2022年。'
- en: '[37] Arpad Feher, Szilard Aradi, Ferenc Hegedus, Tamas Becsi, and Peter Gaspar.
    Hybrid ddpg approach for vehicle motion planning. In ICINCO: Proceedings of the
    16th International Conference on Informatics in Control, Automation and Robotics,
    Vol 1, pages 422–429, 2019.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Arpad Feher, Szilard Aradi, Ferenc Hegedus, Tamas Becsi, 和 Peter Gaspar.
    用于车辆运动规划的混合ddpg方法。在ICINCO: 第16届控制、自动化和机器人信息学国际会议论文集中，第1卷，第422–429页，2019年。'
- en: '[38] Arpad Feher, Szilard Aradi, Tamas Becsi, Peter Gaspar, and Zsolt Szalay.
    Proving ground test of a ddpg-based vehicle trajectory planner. In 2020 European
    Control Conference (ECC 2020), pages 332–337, 2020.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Arpad Feher, Szilard Aradi, Tamas Becsi, Peter Gaspar, 和 Zsolt Szalay.
    基于ddpg的车辆轨迹规划器的验证性测试。在2020年欧洲控制会议（ECC 2020）中，第332–337页，2020年。'
- en: '[39] Mireya Cabezas-Olivenza, Ekaitz Zulueta, Ander Sanchez-Chica, Unai Fernandez-Gamiz,
    and Adrian Teso-Fz-Betono. Stability analysis for autonomous vehicle navigation
    trained over deep deterministic policy gradient. Mathematics, 11(1), 2023.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Mireya Cabezas-Olivenza, Ekaitz Zulueta, Ander Sanchez-Chica, Unai Fernandez-Gamiz,
    和 Adrian Teso-Fz-Betono. 基于深度确定性策略梯度的自动驾驶车辆导航稳定性分析。数学，11(1)，2023年。'
- en: '[40] Hui Hu, Yuge Wang, Wenjie Tong, Jiao Zhao, and Yulei Gu. Path planning
    for autonomous vehicles in unknown dynamic environment based on deep reinforcement
    learning. Applied Sciences-Basel, 13(18), 2023.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Hui Hu, Yuge Wang, Wenjie Tong, Jiao Zhao, 和 Yulei Gu. 基于深度强化学习的未知动态环境中的自动驾驶路径规划。应用科学-巴塞尔，13(18)，2023年。'
- en: '[41] J. Hossain, A.-Z. Faridee, N. Roy, A. Basak, and D.E. Asher. Covernav:
    Cover following navigation planning in unstructured outdoor environment with deep
    reinforcement learning. In 2023 IEEE International Conference on Autonomic Computing
    and Self-Organizing Systems (ACSOS), pages 127–132, 2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Hossain, A.-Z. Faridee, N. Roy, A. Basak, 和 D.E. Asher. Covernav：基于深度强化学习的非结构化户外环境中的跟随导航规划。发表于2023
    IEEE国际自主计算与自组织系统会议（ACSOS），第127–132页，2023年。'
- en: '[42] Victor Mazzilli, Stefano De Pinto, Leonardo Pascali, Michele Contrino,
    Francesco Bottiglione, Giacomo Mantriota, Patrick Gruber, and Aldo Sorniotti.
    Integrated chassis control: Classification, analysis and future trends. Annual
    Reviews in Control, 51:172–205, 2021.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Victor Mazzilli, Stefano De Pinto, Leonardo Pascali, Michele Contrino,
    Francesco Bottiglione, Giacomo Mantriota, Patrick Gruber, 和 Aldo Sorniotti. 集成底盘控制：分类、分析和未来趋势。《控制年鉴》，51:172–205，2021年。'
- en: '[43] Charles Desjardins and Brahim Chaib-Draa. Cooperative adaptive cruise
    control: A reinforcement learning approach. IEEE Transactions on intelligent transportation
    systems, 12(4):1248–1260, 2011.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Charles Desjardins 和 Brahim Chaib-Draa. 合作自适应巡航控制：一种强化学习方法。《IEEE智能交通系统汇刊》，12(4):1248–1260，2011年。'
- en: '[44] Jichang Ma, Hui Xie, Kang Song, and Hao Liu. Self-optimizing path tracking
    controller for intelligent vehicles based on reinforcement learning. Symmetry-Basel,
    14(1), 2022.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Jichang Ma, Hui Xie, Kang Song, 和 Hao Liu. 基于强化学习的智能车辆自优化路径跟踪控制器。《对称性-巴塞尔》，14(1)，2022年。'
- en: '[45] Roman Liessner, Jan Dohmen, and Marco Wiering. Explainable reinforcement
    learning for longitudinal control. In ICAART: Proceedings of the 13th International
    Conference on Agents and Artificial Intelligence - Vol 2, pages 874–881, 2021.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Roman Liessner, Jan Dohmen, 和 Marco Wiering. 可解释的强化学习用于纵向控制。发表于ICAART：第13届国际代理与人工智能会议论文集
    - 第2卷，第874–881页，2021年。'
- en: '[46] Oscar Perez-Gil, Rafael Barea, Elena Lopez-Guillen, Luis M. Bergasa, Carlos
    Gomez-Huelamo, Rodrigo Gutierrez, and Alejandro Diaz-Diaz. Deep reinforcement
    learning based control for autonomous vehicles in carla. Multimedia Tools and
    Applications, 81(3):3553–3576, 2022.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Oscar Perez-Gil, Rafael Barea, Elena Lopez-Guillen, Luis M. Bergasa, Carlos
    Gomez-Huelamo, Rodrigo Gutierrez, 和 Alejandro Diaz-Diaz. 基于深度强化学习的自动驾驶车辆控制在carla中的应用。《多媒体工具与应用》，81(3):3553–3576，2022年。'
- en: '[47] Weinan Gao, Jingqin Gao, Kaan Ozbay, and Zhong-Ping Jiang. Reinforcement-learning-based
    cooperative adaptive cruise control of buses in the lincoln tunnel corridor with
    time-varying topology. IEEE Transactions on Intelligent Transportation Systems,
    20(10):3796–3805, 2019.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Weinan Gao, Jingqin Gao, Kaan Ozbay, 和 Zhong-Ping Jiang. 基于强化学习的合作自适应巡航控制在林肯隧道走廊中的应用，具有时间变化的拓扑。《IEEE智能交通系统汇刊》，20(10):3796–3805，2019年。'
- en: '[48] Chengyu Wang, Luhan Wang, Zhaoming Lu, Xinghe Chu, Zhengrui Shi, Jiayin
    Deng, Tianyang Su, Guochu Shou, and Xiangming Wen. A safe reinforcement learning
    based trajectory tracker framework. IEEE Transactions on Intelligent Transportation
    Systems, 24(6):5765–5780, 2023.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Chengyu Wang, Luhan Wang, Zhaoming Lu, Xinghe Chu, Zhengrui Shi, Jiayin
    Deng, Tianyang Su, Guochu Shou, 和 Xiangming Wen. 基于安全强化学习的轨迹跟踪器框架。《IEEE智能交通系统汇刊》，24(6):5765–5780，2023年。'
- en: '[49] Z. Luo, J. Zhou, and G. Wen. Deep reinforcement learning based tracking
    control of unmanned vehicle with safety guarantee. In 2022 13th Asian Control
    Conference (ASCC), pages 1893–1898, 2022.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Z. Luo, J. Zhou, 和 G. Wen. 基于深度强化学习的无人驾驶车辆跟踪控制与安全保障。发表于2022年第13届亚洲控制会议（ASCC），第1893–1898页，2022年。'
- en: '[50] Jialing Yao and Zhen Ge. Path-tracking control strategy of unmanned vehicle
    based on ddpg algorithm. Sensors, 22(20), 2022.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Jialing Yao 和 Zhen Ge. 基于DDPG算法的无人驾驶车辆路径跟踪控制策略。《传感器》，22(20)，2022年。'
- en: '[51] Rui He, Haipeng Lv, Sumin Zhang, Dong Zhang, and Hang Zhang. Lane following
    method based on improved ddpg algorithm. Sensors, 21(14), 2021.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Rui He, Haipeng Lv, Sumin Zhang, Dong Zhang, 和 Hang Zhang. 基于改进DDPG算法的车道跟随方法。《传感器》，21(14)，2021年。'
- en: '[52] Yifan Hu, Junjie Fu, and Guanghui Wen. Safe reinforcement learning for
    model-reference trajectory tracking of uncertain autonomous vehicles with model-based
    acceleration. IEEE Transactions on Intelligent Vehicles, 8(3):2332–2344, 2023.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Yifan Hu, Junjie Fu, 和 Guanghui Wen. 基于模型加速的安全强化学习在不确定自动驾驶车辆的模型参考轨迹跟踪中的应用。《IEEE智能车辆汇刊》，8(3):2332–2344，2023年。'
- en: '[53] Balint Kovari, Ferenc Hegedus, and Tamas Becsi. Design of a reinforcement
    learning-based lane keeping planning agent for automated vehicles. Applied Sciences-Basel,
    10(20), 2020.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Balint Kovari, Ferenc Hegedus, 和 Tamas Becsi. 设计一种基于强化学习的车道保持规划代理，用于自动驾驶车辆。《应用科学-巴塞尔》，10(20)，2020年。'
- en: '[54] Jing Chen, Cong Zhao, Shengchuan Jiang, Xinyuan Zhang, Zhongxin Li, and
    Yuchuan Du. Safe, efficient, and comfortable autonomous driving based on cooperative
    vehicle infrastructure system. International Journal of Environmental Research
    and Public Health, 20(1), 2023.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Jing Chen, Cong Zhao, Shengchuan Jiang, Xinyuan Zhang, Zhongxin Li, 和
    Yuchuan Du. 基于合作车辆基础设施系统的安全、高效、舒适的自动驾驶。《国际环境研究与公共卫生期刊》，20(1)，2023年。'
- en: '[55] Gabriel Hartmann, Zvi Shiller, and Amos Azaria. Model-based reinforcement
    learning for time-optimal velocity control. IEEE Robotics and Automation Letters,
    5(4):6185–6192, 2020.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Gabriel Hartmann, Zvi Shiller, 和 Amos Azaria. 基于模型的强化学习用于时间最优速度控制。IEEE
    机器人与自动化快报，5(4):6185–6192，2020年。'
- en: '[56] Leilei Cui, Kaan Ozbay, and Zhong-Ping Jiang. Combined longitudinal and
    lateral control of autonomous vehicles based on reinforcement learning. In 2021
    American Control Conference (ACC), pages 1929–1934, 2021.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Leilei Cui, Kaan Ozbay, 和 Zhong-Ping Jiang. 基于强化学习的自动驾驶车辆纵向与横向控制的结合。在
    2021 美国控制会议（ACC），页码 1929–1934，2021年。'
- en: '[57] Longquan Chen, Ying He, Qiang Wang, Weike Pan, and Zhong Ming. Joint optimization
    of sensing, decision-making and motion-controlling for autonomous vehicles: A
    deep reinforcement learning approach. IEEE Transactions on Vehicular Technology,
    71(5):4642–4654, 2022.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Longquan Chen, Ying He, Qiang Wang, Weike Pan, 和 Zhong Ming. 自动驾驶车辆的感知、决策和运动控制的联合优化：一种深度强化学习方法。IEEE
    车辆技术汇刊，71(5):4642–4654，2022年。'
- en: '[58] Y. Wang, C. Zheng, M. Sun, Z. Chen, and Q. Sun. Reinforcement-learning-aided
    adaptive control for autonomous driving with combined lateral and longitudinal
    dynamics. In 2023 IEEE 12th Data Driven Control and Learning Systems Conference
    (DDCLS), pages 840–845, 2023.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. Wang, C. Zheng, M. Sun, Z. Chen, 和 Q. Sun. 基于强化学习的自适应控制用于结合横向和纵向动态的自动驾驶。在
    2023 IEEE 第十二届数据驱动控制与学习系统会议（DDCLS），页码 840–845，2023年。'
- en: '[59] Meixin Zhu, Yinhai Wang, Ziyuan Pu, Jingyun Hu, Xuesong Wang, and Ruimin
    Ke. Safe, efficient, and comfortable velocity control based on reinforcement learning
    for autonomous driving. Transportation Research Part C: Emerging Technologies,
    117:102662, 2020.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Meixin Zhu, Yinhai Wang, Ziyuan Pu, Jingyun Hu, Xuesong Wang, 和 Ruimin
    Ke. 基于强化学习的安全、高效、舒适的速度控制用于自动驾驶。运输研究C部分：新兴技术，117:102662，2020年。'
- en: '[60] Yi Zhang, Ping Sun, Yuhan Yin, Lin Lin, and Xuesong Wang. Human-like autonomous
    vehicle speed control by deep reinforcement learning with double q-learning. In
    2018 IEEE intelligent vehicles symposium (IV), pages 1251–1256\. IEEE, 2018.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Yi Zhang, Ping Sun, Yuhan Yin, Lin Lin, 和 Xuesong Wang. 基于深度强化学习的类人自动驾驶车辆速度控制与双重
    Q 学习。在 2018 IEEE 智能车辆研讨会（IV），页码 1251–1256。IEEE，2018年。'
- en: '[61] Yunlong Song, Angel Romero, Matthias Müller, Vladlen Koltun, and Davide
    Scaramuzza. Reaching the limit in autonomous racing: Optimal control versus reinforcement
    learning. Science Robotics, 8(82):eadg1462, 2023.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Yunlong Song, Angel Romero, Matthias Müller, Vladlen Koltun, 和 Davide
    Scaramuzza. 在自动驾驶竞速中达到极限：最优控制与强化学习的比较。科学机器人，8(82):eadg1462，2023年。'
- en: '[62] W LLC. Waymo open dataset: An autonomous driving dataset, 2019.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] W LLC. Waymo 开放数据集：一个自动驾驶数据集，2019年。'
- en: '[63] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year,
    1000 km: The oxford robotcar dataset. The International Journal of Robotics Research,
    36(1):3–15, 2017.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Will Maddern, Geoffrey Pascoe, Chris Linegar, 和 Paul Newman. 1年，1000公里：牛津机器人车数据集。《国际机器人研究期刊》，36(1):3–15，2017年。'
- en: '[64] Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, and
    Ruigang Yang. The apolloscape open dataset for autonomous driving and its application.
    IEEE transactions on pattern analysis and machine intelligence, 42(10):2702–2719,
    2019.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, 和 Ruigang
    Yang. 用于自动驾驶的 Apolloscape 开放数据集及其应用。IEEE 模式分析与机器智能汇刊，42(10):2702–2719，2019年。'
- en: '[65] Ry Rivard. Udacity project on «pause». Inside Higher Ed, 18, 2013.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Ry Rivard. Udacity 项目处于“暂停”状态。《高等教育内部》，18，2013年。'
- en: '[66] Shuyang Du, Haoli Guo, and Andrew Simpson. Self-driving car steering angle
    prediction based on image recognition. arXiv preprint arXiv:1912.05440, 2019.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Shuyang Du, Haoli Guo, 和 Andrew Simpson. 基于图像识别的自驾车转向角预测。arXiv 预印本 arXiv:1912.05440，2019年。'
- en: '[67] Yiquan Wang, Jingguo Wang, Yu Yang, Zhaodong Li, and Xijun Zhao. An end-to-end
    deep reinforcement learning model based on proximal policy optimization algorithm
    for autonomous driving of off-road vehicle. In International Conference on Autonomous
    Unmanned Systems, pages 2692–2704\. Springer, 2022.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Yiquan Wang, Jingguo Wang, Yu Yang, Zhaodong Li, 和 Xijun Zhao. 基于近端策略优化算法的端到端深度强化学习模型用于越野车的自动驾驶。在国际无人系统大会，页码
    2692–2704。Springer，2022年。'
- en: '[68] Alexander Amini, Igor Gilitschenski, Jacob Phillips, Julia Moseyko, Rohan
    Banerjee, Sertac Karaman, and Daniela Rus. Learning robust control policies for
    end-to-end autonomous driving from data-driven simulation. IEEE Robotics and Automation
    Letters, 5(2):1143–1150, 2020.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Alexander Amini, Igor Gilitschenski, Jacob Phillips, Julia Moseyko, Rohan
    Banerjee, Sertac Karaman 和 Daniela Rus. 从数据驱动的仿真中学习稳健的控制策略以实现端到端的自动驾驶。《IEEE机器人与自动化快报》，5(2):1143–1150，2020年。'
- en: '[69] Seung-Hwan Chung, Seung-Hyun Kong, Sangjae Cho, and I. Made Aswin Nahrendra.
    Segmented encoding for sim2real of rl-based end-to-end autonomous driving. In
    2022 IEEE Intelligent Vehicles Symposium (IV), pages 1290–1296, 2022.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Seung-Hwan Chung, Seung-Hyun Kong, Sangjae Cho 和 I. Made Aswin Nahrendra.
    基于RL的端到端自动驾驶的分段编码。在2022年IEEE智能车辆研讨会（IV）上，第1290–1296页，2022年。'
- en: '[70] N.F. S, S. Naeemi, S.H. Shamchi, and A. Nahvi. Autonomous merging onto
    the highway using lstm neural network. In 2023 11th RSI International Conference
    on Robotics and Mechatronics (ICRoM), pages 574–579, 2023.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] N.F. S, S. Naeemi, S.H. Shamchi 和 A. Nahvi. 使用LSTM神经网络的自动高速公路并入。在2023年第11届RSI国际机器人与机电一体化会议（ICRoM）上，第574–579页，2023年。'
- en: '[71] C. Zhou, M. Liao, L. Jiao, and F. Tao. Lane change decision control of
    autonomous vehicle based on a3c algorithm. In Cognitive Systems and Information
    Processing: 8th International Conference, ICCSIP 2023, Revised Selected Papers.
    Communications in Computer and Information Science, volume 1918, pages 217–229,
    2024.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] C. Zhou, M. Liao, L. Jiao 和 F. Tao. 基于A3C算法的自动驾驶车辆车道变换决策控制。在认知系统与信息处理：第八届国际会议，ICCSIP
    2023，修订精选论文中。《计算机与信息科学通讯》，第1918卷，第217–229页，2024年。'
- en: '[72] Yang Guan, Yangang Ren, Qi Sun, Shengbo Eben Li, Haitong Ma, Jingliang
    Duan, Yifan Dai, and Bo Cheng. Integrated decision and control: toward interpretable
    and computationally efficient driving intelligence. IEEE transactions on cybernetics,
    53(2):859–873, 2022.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Yang Guan, Yangang Ren, Qi Sun, Shengbo Eben Li, Haitong Ma, Jingliang
    Duan, Yifan Dai 和 Bo Cheng. 综合决策与控制：迈向可解释和计算高效的驾驶智能。《IEEE网络学报》，53(2):859–873，2022年。'
- en: '[73] Onno Eberhard, Jakob Hollenstein, Cristina Pinneri, and Georg Martius.
    Pink noise is all you need: Colored noise exploration in deep reinforcement learning.
    In The Eleventh International Conference on Learning Representations, 2022.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Onno Eberhard, Jakob Hollenstein, Cristina Pinneri 和 Georg Martius. 粉红噪声就够了：深度强化学习中的有色噪声探索。在第十一届国际学习表征会议上，2022年。'
- en: '[74] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension:
    New rich classes of rl problems, and sample-efficient algorithms. Advances in
    neural information processing systems, 34:13406–13418, 2021.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Chi Jin, Qinghua Liu 和 Sobhan Miryoosefi. Bellman逃逸维度：新的丰富RL问题类别和样本高效算法。《神经信息处理系统进展》，34:13406–13418，2021年。'
- en: '[75] Girolamo Macaluso, Alessandro Sestini, and Andrew D Bagdanov. Small dataset,
    big gains: Enhancing reinforcement learning by offline pre-training with model-based
    augmentation. In Computer Sciences & Mathematics Forum, volume 9, page 4\. MDPI,
    2024.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Girolamo Macaluso, Alessandro Sestini 和 Andrew D Bagdanov. 小数据集，大收益：通过基于模型的增强离线预训练来提升强化学习。在计算机科学与数学论坛，第9卷，第4页。MDPI，2024年。'
- en: '[76] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz,
    Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex
    Ray, et al. Learning dexterous in-hand manipulation. The International Journal
    of Robotics Research, 39(1):3–20, 2020.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz,
    Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex
    Ray 等人. 学习灵巧的手部操作。《国际机器人研究期刊》，39(1):3–20，2020年。'
- en: '[77] Xinlei Pan, Yurong You, Ziyan Wang, and Cewu Lu. Virtual to real reinforcement
    learning for autonomous driving. arXiv preprint arXiv:1704.03952, 2017.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Xinlei Pan, Yurong You, Ziyan Wang 和 Cewu Lu. 从虚拟到现实的强化学习用于自动驾驶。arXiv预印本
    arXiv:1704.03952，2017年。'
- en: '[78] Shuo Feng, Haowei Sun, Xintao Yan, Haojie Zhu, Zhengxia Zou, Shengyin
    Shen, and Henry X Liu. Dense reinforcement learning for safety validation of autonomous
    vehicles. Nature, 615(7953):620–627, 2023.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Shuo Feng, Haowei Sun, Xintao Yan, Haojie Zhu, Zhengxia Zou, Shengyin
    Shen 和 Henry X Liu. 用于自动驾驶车辆安全验证的密集强化学习。《自然》，615(7953):620–627，2023年。'
- en: '[79] Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement
    learning. Journal of Machine Learning Research, 16(1):1437–1480, 2015.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Javier Garcıa 和 Fernando Fernández. 关于安全强化学习的综合调查。《机器学习研究期刊》，16(1):1437–1480，2015年。'
- en: '[80] Ziqing Gu, Lingping Gao, Haitong Ma, Shengbo Eben Li, Sifa Zheng, Wei
    Jing, and Junbo Chen. Safe-state enhancement method for autonomous driving via
    direct hierarchical reinforcement learning. IEEE Transactions on Intelligent Transportation
    Systems, 24(9):9966–9983, 2023.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] 祖青·顾、灵平·高、海通·马、胜博·李、斯法·郑、魏静和君博·陈。通过直接层次强化学习提高自动驾驶的安全状态方法。IEEE 智能交通系统学报，24(9)：9966–9983，2023年。'
