- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:33:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:33:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2404.09939] A Survey on Deep Learning for Theorem Proving'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2404.09939] 关于定理证明的深度学习调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.09939](https://ar5iv.labs.arxiv.org/html/2404.09939)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.09939](https://ar5iv.labs.arxiv.org/html/2404.09939)
- en: \forestset
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \forestset
- en: direction switch/.style= forked edges, for tree= calign=last, edge+=thick, font=,
    , where level=1minimum width=13em, where level¡=2draw=red, where level¿=1folder,
    grow’=0, ,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: direction switch/.style= forked edges, for tree= calign=last, edge+=thick, font=,
    , where level=1minimum width=13em, where level¡=2draw=red, where level¿=1folder,
    grow’=0, ,
- en: A Survey on Deep Learning for Theorem Proving
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于定理证明的深度学习调查
- en: Zhaoyu Li¹, Jialiang Sun¹, Logan Murphy¹, Qidong Su¹, Zenan Li², Xian Zhang³
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Zhaoyu Li¹, Jialiang Sun¹, Logan Murphy¹, Qidong Su¹, Zenan Li², Xian Zhang³
- en: Kaiyu Yang⁴, Xujie Si¹
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Kaiyu Yang⁴, Xujie Si¹
- en: ¹University of Toronto, ²Nanjing University, ³Microsoft Research Asia, ⁴Caltech
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ¹多伦多大学, ²南京大学, ³微软亚洲研究院, ⁴加州理工学院
- en: '[https://github.com/zhaoyu-li/DL4TP](https://github.com/zhaoyu-li/DL4TP)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/zhaoyu-li/DL4TP](https://github.com/zhaoyu-li/DL4TP)'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Theorem proving is a fundamental aspect of mathematics, spanning from informal
    reasoning in mathematical language to rigorous derivations in formal systems.
    In recent years, the advancement of deep learning, especially the emergence of
    large language models, has sparked a notable surge of research exploring these
    techniques to enhance the process of theorem proving. This paper presents a pioneering
    comprehensive survey of deep learning for theorem proving by offering i) a thorough
    review of existing approaches across various tasks such as autoformalization,
    premise selection, proofstep generation, and proof search; ii) a meticulous summary
    of available datasets and strategies for data generation; iii) a detailed analysis
    of evaluation metrics and the performance of state-of-the-art; and iv) a critical
    discussion on the persistent challenges and the promising avenues for future exploration.
    Our survey aims to serve as a foundational reference for deep learning approaches
    in theorem proving, seeking to catalyze further research endeavors in this rapidly
    growing field.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 定理证明是数学的一个基本方面，涵盖了从数学语言中的非正式推理到正式系统中的严格推导。近年来，深度学习的发展，特别是大语言模型的出现，引发了大量研究探索这些技术以提升定理证明过程。本文呈现了一项开创性的关于定理证明的深度学习的综合调查，提供了
    i) 对现有方法的全面回顾，包括自动形式化、前提选择、证明步骤生成和证明搜索等任务；ii) 对现有数据集和数据生成策略的细致总结；iii) 对评估指标和最先进技术性能的详细分析；iv)
    对持续挑战和未来研究方向的批判性讨论。我们的调查旨在作为定理证明中深度学习方法的基础参考，力图推动这一快速发展的领域的进一步研究。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Proving theorems is a cornerstone of mathematics. Since the era of Euclid around
    300 B.C., people have crafted theorems and proofs using a blend of natural language
    and mathematical symbols, meticulously evaluating their correctness through manual
    inspection. In the 1950s, a paradigm shift occurred with the exploration of computer-assisted
    proofs (Davis, [1957](#bib.bib37); Davis & Putnam, [1960](#bib.bib38)), wherein
    a machine automatically applies deduction rules to prove assertions. These innovations
    laid the groundwork for the subsequent development of interactive theorem provers (Bruijn,
    de, [1970](#bib.bib19); Milner, [1972](#bib.bib120)), enabling people to construct
    more intricate theorems and proofs by interacting with these systems. Building
    upon these advancements, later research extended the scope of theorem proving
    beyond mathematics, applying it to various practical applications such as software
    verification (Schumann, [2001](#bib.bib154)) and hardware design (Kern & Greenstreet,
    [1999](#bib.bib90)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 证明定理是数学的基石。自公元前 300 年左右欧几里得时代以来，人们通过自然语言和数学符号的结合来构建定理和证明，并通过手动检查仔细评估其正确性。在 1950
    年代，随着计算机辅助证明的探索（Davis, [1957](#bib.bib37); Davis & Putnam, [1960](#bib.bib38)），发生了一次范式转变，其中机器自动应用推理规则来证明断言。这些创新为后来的交互式定理证明器（Bruijn,
    de, [1970](#bib.bib19); Milner, [1972](#bib.bib120)）的发展奠定了基础，使人们能够通过与这些系统交互来构建更复杂的定理和证明。在这些进展的基础上，后来的研究将定理证明的范围扩展到了数学之外，将其应用于软件验证（Schumann,
    [2001](#bib.bib154)）和硬件设计（Kern & Greenstreet, [1999](#bib.bib90)）等各种实际应用。
- en: '![Refer to caption](img/bf3da018792cc74d150230faa8f411d3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bf3da018792cc74d150230faa8f411d3.png)'
- en: 'Figure 1: Papers on deep learning for theorem proving over the years.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：近年来关于定理证明的深度学习论文。
- en: Exploring learning-based approaches for theorem proving has been a long-standing
    research focus, dating back to the 1990s (Suttner & Ertel, [1990](#bib.bib166);
    Denzinger et al., [1999](#bib.bib39)). The recent development of deep learning,
    especially with the evolution of large language models (LLMs), has ignited a wave
    of research interest in this area again. As shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Theorem Proving"), the volume
    of papers on deep learning for theorem proving has grown approximately from 2
    in 2016 to 50 in 2023\. However, despite such remarkable growth, this domain is
    characterized by a wide range of tasks, methods, datasets, and evaluations, which
    lack a cohesive framework to comprehend the true extent of progress and indicate
    the underlying challenges and potential future work.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 探索基于学习的方法进行定理证明一直是一个长期的研究重点，追溯到1990年代（Suttner & Ertel，[1990](#bib.bib166)；Denzinger
    et al.，[1999](#bib.bib39)）。近年来深度学习的发展，特别是大型语言模型（LLMs）的演变，再次激发了对这一领域的研究兴趣。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Theorem Proving")所示，关于定理证明的深度学习论文的数量从2016年的约2篇增长到2023年的50篇。然而，尽管增长如此显著，该领域特征上任务、方法、数据集和评估范围广泛，缺乏一个统一的框架来理解真正的进展程度，并指出潜在的挑战和未来的工作方向。
- en: In this paper, we provide a comprehensive survey of more than 170 research papers
    in deep learning for theorem proving, aiming to map out the current research landscape
    and highlight key advancements systematically. We begin with the background for
    informal and formal settings of theorem proving (§[2](#S2 "2 Background ‣ A Survey
    on Deep Learning for Theorem Proving")). Subsequently, we delve into the details
    of the tasks and methods within this domain (§[3](#S3 "3 Tasks and Methods ‣ A
    Survey on Deep Learning for Theorem Proving")), which include autoformalization,
    premise selection, proofstep generation, proof search, and other tasks. We also
    review the datasets for theorem proving (§[4](#S4 "4 Datasets ‣ A Survey on Deep
    Learning for Theorem Proving")), including manually curated and synthetically
    generated ones. Moreover, we evaluate the metrics used in existing methods and
    assess their performance (§[5](#S5 "5 Evaluations ‣ A Survey on Deep Learning
    for Theorem Proving")). Following this, we discuss the prevailing challenges and
    conclude with future directions (§[6](#S6 "6 Discussions ‣ A Survey on Deep Learning
    for Theorem Proving")).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对超过170篇关于定理证明的深度学习研究论文进行了全面的综述，旨在系统地描绘当前的研究现状，并突出关键进展。我们首先介绍定理证明的非正式和正式设置的背景（§[2](#S2
    "2 Background ‣ A Survey on Deep Learning for Theorem Proving")）。随后，我们深入探讨这一领域的任务和方法的细节（§[3](#S3
    "3 Tasks and Methods ‣ A Survey on Deep Learning for Theorem Proving")），包括自动形式化、前提选择、证明步骤生成、证明搜索等任务。我们还回顾了定理证明的数据集（§[4](#S4
    "4 Datasets ‣ A Survey on Deep Learning for Theorem Proving")），包括手动策划的和合成生成的数据集。此外，我们评估了现有方法中使用的指标并评估其性能（§[5](#S5
    "5 Evaluations ‣ A Survey on Deep Learning for Theorem Proving")）。接下来，我们讨论了当前面临的挑战，并总结未来的方向（§[6](#S6
    "6 Discussions ‣ A Survey on Deep Learning for Theorem Proving")）。
- en: 2 Background
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: In this section, we recall the fundamental concepts of informal and formal to
    theorem proving. Figure [2](#S2.F2 "Figure 2 ‣ 2 Background ‣ A Survey on Deep
    Learning for Theorem Proving") shows an illustrative example of these two settings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们回顾非正式和正式定理证明的基本概念。图[2](#S2.F2 "Figure 2 ‣ 2 Background ‣ A Survey on
    Deep Learning for Theorem Proving")展示了这两种设置的示例。
- en: '![Refer to caption](img/90e87befdcf63c2659a94d9baebb21cc.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/90e87befdcf63c2659a94d9baebb21cc.png)'
- en: 'Figure 2: Top: The informal statement and proof of the Fundamental Theorem
    of Arithmetic in [ProofWiki](https://proofwiki.org/wiki/Fundamental_Theorem_of_Arithmetic).
    Bottom Left: The formal statement and proof of the same theorem in the mathlib
    library (mathlib Community, [2020](#bib.bib117)) of Lean 4\. Bottom Right: The
    corresponding proof tree illustrating the formal proof process in Lean 4\. The
    references and premises used in the informal and formal proof are highlighted
    by underlines and colors respectively.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：上：在[ProofWiki](https://proofwiki.org/wiki/Fundamental_Theorem_of_Arithmetic)中对算术基本定理的非正式声明和证明。下左：在Lean
    4的mathlib库（mathlib Community，[2020](#bib.bib117)）中对同一定理的正式声明和证明。下右：在Lean 4中说明正式证明过程的相应证明树。非正式和正式证明中使用的参考文献和前提分别用下划线和颜色标出。
- en: 2.1 Informal Theorem Proving
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 非正式定理证明
- en: Informal theorem proving involves establishing the truth of mathematical statements
    building on existing knowledge via intuitive reasoning and natural language explanations.
    This mirrors how people learn and prove theorems in everyday mathematics. For
    instance, to prove the Fundamental Theorem of Arithmetic in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Background ‣ A Survey on Deep Learning for Theorem Proving") (Top),
    one needs to comprehend basic concepts like primes and might apply established
    results to draw a conclusion. Despite the ubiquity of informal theorem proving,
    as mathematics evolves, the theories and proofs tend to be more intricate, making
    verifying their correctness increasingly difficult.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 非正式定理证明涉及通过直观推理和自然语言解释，在现有知识的基础上建立数学声明的真理。这类似于人们在日常数学中学习和证明定理的方式。例如，为了证明图[2](#S2.F2
    "Figure 2 ‣ 2 Background ‣ A Survey on Deep Learning for Theorem Proving")（顶部）中的算术基本定理，需要理解基本概念如质数，并可能应用已建立的结果来得出结论。尽管非正式定理证明普遍存在，但随着数学的发展，理论和证明趋向于更加复杂，使得验证其正确性变得越来越困难。
- en: 2.2 Formal Theorem Proving
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 正式定理证明
- en: 'Formal theorem proving represents theorems and proofs in a machine-verifiable
    format, ensuring their correctness using rigorous logical rules. This field can
    be broadly classified into two paradigms: automated theorem proving (ATP) and
    interactive theorem proving (ITP).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正式定理证明以机器可验证的格式表示定理和证明，通过严格的逻辑规则确保其正确性。该领域大致可以分为两种范式：自动定理证明（ATP）和互动定理证明（ITP）。
- en: ATP aims to verify formal statements fully automatically. Saturation-based theorem
    provers, including E (Schulz, [2002](#bib.bib153)) and Vampire (Kovács & Voronkov,
    [2013](#bib.bib93)), mainly operate on first-order logic (FOL) to autonomously
    generate logical consequences from a set of axioms until a proof or refutation
    is derived, or computational limits are reached. Similarly, geometric ATP systems
    such as GEX (Chou et al., [2000](#bib.bib26)) prove geometry problems by iteratively
    applying deduction rules. Other approaches, such as tableau-based methods like
    leanCoP (Otten & Bibel, [2003](#bib.bib126)) and instantiation-based methods like
    iProver (Korovin, [2008](#bib.bib92)), use other forms of proof calculi for proof
    construction. Despite the sophisticated designs of these ATP systems, the inherent
    vast search space often limits their practicality in more complex problems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ATP旨在完全自动地验证正式声明。基于饱和度的定理证明器，包括E（Schulz，[2002](#bib.bib153)）和Vampire（Kovács
    & Voronkov，[2013](#bib.bib93)），主要在一阶逻辑（FOL）上运作，从一组公理中自主生成逻辑结果，直到得出证明或反驳，或达到计算极限。同样，几何ATP系统如GEX（Chou
    et al.，[2000](#bib.bib26)）通过迭代应用推理规则来证明几何问题。其他方法，如基于表格的方法，如leanCoP（Otten & Bibel，[2003](#bib.bib126)）和基于实例化的方法，如iProver（Korovin，[2008](#bib.bib92)），使用其他形式的证明演算来进行证明构建。尽管这些ATP系统设计精巧，但固有的广泛搜索空间往往限制了它们在更复杂问题中的实用性。
- en: In ITP, humans collaboratively prove theorems by interacting with *proof assistants*,
    such as Isabelle (Paulson, [1994](#bib.bib131)), HOL Light (Harrison, [1996](#bib.bib66)),
    Coq (Barras et al., [1999](#bib.bib12)), and Lean (Moura & Ullrich, [2021](#bib.bib123)).
    These proof assistants typically enable users to formalize theorems in higher-order
    logic and provide a language for users to build verifiable proofs. As shown in
    Figure [2](#S2.F2 "Figure 2 ‣ 2 Background ‣ A Survey on Deep Learning for Theorem
    Proving") (Bottom Left), to prove a theorem (initial goal) in Lean, one can use
    *tactics* like refine’ and rw as the proof steps. Applying a tactic either finishes
    the goal or decomposes it into simpler sub-goals, and the proof is complete when
    no further goals remain. When proving the current goal, one can apply assumptions
    in the local context and previously proven premises in the environment as tactic
    arguments. For example, the premise perm_of_prod_eq_prod is used as the argument
    of refine’. The proving process can be modeled as a proof tree, where each node
    is a proof state with a goal and its local context, and each edge is a tactic,
    as shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 Background ‣ A Survey on Deep Learning
    for Theorem Proving") (Bottom Right). Using proof assistants, researchers have
    successfully formalized and proved landmark theorems like the Four Color Theorem (Gonthier,
    [2008](#bib.bib62)) and the Kepler Conjecture (Hales et al., [2017](#bib.bib63)),
    and verified the correctness of critical software such as the seL4 microkernel (Klein
    et al., [2009](#bib.bib91)) and the CompCert C compiler (Leroy et al., [2016](#bib.bib102)).
    However, it is worth noting that these projects took several Ph.D. years to complete,
    requiring substantial labor and expertise.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在交互式定理证明（ITP）中，人类通过与*证明助手*（如Isabelle（Paulson, [1994](#bib.bib131)）、HOL Light（Harrison,
    [1996](#bib.bib66)）、Coq（Barras et al., [1999](#bib.bib12)）和Lean（Moura & Ullrich,
    [2021](#bib.bib123)）等）互动来协作证明定理。这些证明助手通常使用户能够在高阶逻辑中形式化定理，并提供一种语言来构建可验证的证明。如图[2](#S2.F2
    "图 2 ‣ 2 背景 ‣ 关于定理证明的深度学习调查")（左下角）所示，在Lean中证明一个定理（初始目标）时，可以使用*策略*如refine’和rw作为证明步骤。应用一个策略要么完成目标，要么将其分解为更简单的子目标，当没有进一步的目标时，证明即为完成。在证明当前目标时，可以将局部上下文中的假设和环境中之前证明的前提作为策略参数。例如，前提perm_of_prod_eq_prod被用作refine’的参数。证明过程可以建模为证明树，每个节点是一个带有目标及其局部上下文的证明状态，每个边是一个策略，如图[2](#S2.F2
    "图 2 ‣ 2 背景 ‣ 关于定理证明的深度学习调查")（右下角）所示。利用证明助手，研究人员成功形式化并证明了标志性的定理，如四色定理（Gonthier,
    [2008](#bib.bib62)）和开普勒猜想（Hales et al., [2017](#bib.bib63)），并验证了关键软件的正确性，如seL4微内核（Klein
    et al., [2009](#bib.bib91)）和CompCert C编译器（Leroy et al., [2016](#bib.bib102)）。然而，值得注意的是，这些项目花费了数年的博士研究时间，需要大量的劳动和专业知识。
- en: 3 Tasks and Methods
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 任务与方法
- en: 'The emergence of deep learning has opened new avenues for the landscape of
    theorem proving, either enhancing or substituting traditional components involved
    in the process. This section categorizes and summarizes existing deep learning
    approaches into 5 tasks: autoformalization, premise selection, proofstep generation,
    proof search, and others.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的出现为定理证明的领域开辟了新的途径，无论是增强还是替代了过程中涉及的传统组件。本节将现有的深度学习方法分类和总结为5个任务：自动形式化、前提选择、证明步骤生成、证明搜索以及其他。
- en: 3.1 Autoformalization
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 自动形式化
- en: Autoformalization aims to convert informal theorems and proofs into machine-verifiable
    formats automatically. This task is notoriously challenging, requiring a profound
    understanding of semantics across informal and formal mathematics (Kaliszyk et al.,
    [2014](#bib.bib83); [2015](#bib.bib84)). Nonetheless, the success of autoformalization
    promises to facilitate the verification of mathematical papers and pave the way
    for general-purpose reasoning engines (Szegedy, [2020](#bib.bib168)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自动形式化旨在将非正式的定理和证明自动转换为机器可验证的格式。这个任务因其挑战性而闻名，要求对非正式和正式数学的语义有深刻理解（Kaliszyk et
    al., [2014](#bib.bib83); [2015](#bib.bib84)）。尽管如此，自动形式化的成功有望促进数学论文的验证，并为通用推理引擎铺平道路（Szegedy,
    [2020](#bib.bib168)）。
- en: Wang et al. ([2018](#bib.bib185); [2020](#bib.bib186)) first explore using deep
    learning models in autoformalization. Inspired by the sequence-to-sequence models
    in neural machine translation (Sutskever et al., [2014](#bib.bib165); Cho et al.,
    [2014](#bib.bib25)), they experiment various encoder-decoder frameworks (Luong
    et al., [2017](#bib.bib116); Lample et al., [2018](#bib.bib99); Lample & Conneau,
    [2019](#bib.bib98)) to convert LaTeX-written mathematical texts to the Mizar language (Rudnicki,
    [1992](#bib.bib148)). Subsequent studies (Bansal & Szegedy, [2020](#bib.bib10);
    Cunningham et al., [2022](#bib.bib34)) utilize similar neural architectures for
    HOL Light and Coq.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等（[2018](#bib.bib185)；[2020](#bib.bib186)）首次探索了使用深度学习模型进行自动形式化。他们受到神经机器翻译中的序列到序列模型（Sutskever
    等，[2014](#bib.bib165)；Cho 等，[2014](#bib.bib25)）的启发，实验了各种编码器-解码器框架（Luong 等，[2017](#bib.bib116)；Lample
    等，[2018](#bib.bib99)；Lample & Conneau，[2019](#bib.bib98)）以将 LaTeX 编写的数学文本转换为 Mizar
    语言（Rudnicki，[1992](#bib.bib148)）。后续研究（Bansal & Szegedy，[2020](#bib.bib10)；Cunningham
    等，[2022](#bib.bib34)）利用类似的神经架构处理 HOL Light 和 Coq。
- en: 'The recent development of LLMs and their in-context learning capabilities (Brown
    et al., [2020](#bib.bib18)) have provided new opportunities for autoformalization.
    Wu et al. ([2022](#bib.bib196)); Agrawal et al. ([2022](#bib.bib4)); Gadgil et al.
    ([2022](#bib.bib54)) study the prospects of autoformalization using PaLM (Chowdhery
    et al., [2023](#bib.bib27)) and Codex (Chen et al., [2021](#bib.bib23)) with few-shot
    prompting to translate mathematical problems into Isabelle and Lean. Some researchers (Jiang
    et al., [2023b](#bib.bib77); Patel et al., [2023](#bib.bib130); Zhao et al., [2023](#bib.bib211);
    Xin et al., [2024](#bib.bib197)) propose more structured approaches to autoformalization:
    For example, DSP (Jiang et al., [2023b](#bib.bib77)) utilizes Minerva (Lewkowycz
    et al., [2022](#bib.bib103)) to draft informal proofs and map them into formal
    sketches, with ATP systems employed to fill in the missing details in the proof
    sketch. Zhao et al. ([2023](#bib.bib211)) improves informal proofs and formal
    sketches in DSP with sub-goal proofs and prompt selection respectively. Additionally,
    a line of research (Azerbayev et al., [2023](#bib.bib8); Jiang et al., [2023a](#bib.bib76);
    Azerbayev et al., [2024](#bib.bib9); Shao et al., [2024](#bib.bib155); Ying et al.,
    [2024](#bib.bib205)) focuses on training LLMs on large-scale datasets with both
    informal and formal mathematical data to evaluate their performance on autoformalization.
    Recent studies (Liu et al., [2023](#bib.bib110); Ye et al., [2023](#bib.bib203);
    Zhou et al., [2024a](#bib.bib214); Huang et al., [2024](#bib.bib70)) also investigate
    autoformalization in some downstream tasks: For instance, SAT-LM (Ye et al., [2023](#bib.bib203))
    uses LLMs to formalize natural language problems and solve them with ATP systems
    on several reasoning tasks. DTV (Zhou et al., [2024a](#bib.bib214)) leverages
    autoformalization to ground LLM reasoning, formalizing LLM-generated answers and
    verifying them with ATP tools. Besides these efforts, Wu et al. ([2022](#bib.bib196));
    Azerbayev et al. ([2023](#bib.bib8)); Jiang et al. ([2023a](#bib.bib76)) explore
    advanced LLMs like Codex and GPT-4 (Achiam et al., [2023](#bib.bib3)) for informalization,
    i.e., the translation of formal statements to natural language.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）及其上下文学习能力（Brown 等，[2020](#bib.bib18)）为自动形式化提供了新的机遇。Wu 等（[2022](#bib.bib196)）；Agrawal
    等（[2022](#bib.bib4)）；Gadgil 等（[2022](#bib.bib54)）研究了使用 PaLM（Chowdhery 等，[2023](#bib.bib27)）和
    Codex（Chen 等，[2021](#bib.bib23)）通过少量示例提示将数学问题翻译成 Isabelle 和 Lean 的自动形式化前景。一些研究人员（Jiang
    等，[2023b](#bib.bib77)；Patel 等，[2023](#bib.bib130)；Zhao 等，[2023](#bib.bib211)；Xin
    等，[2024](#bib.bib197)）提出了更结构化的自动形式化方法：例如，DSP（Jiang 等，[2023b](#bib.bib77)）利用 Minerva（Lewkowycz
    等，[2022](#bib.bib103)）起草非正式证明并将其映射到正式草图中，使用 ATP 系统填补证明草图中的缺失细节。Zhao 等（[2023](#bib.bib211)）通过子目标证明和提示选择分别改进了
    DSP 中的非正式证明和正式草图。此外，一些研究（Azerbayev 等，[2023](#bib.bib8)；Jiang 等，[2023a](#bib.bib76)；Azerbayev
    等，[2024](#bib.bib9)；Shao 等，[2024](#bib.bib155)；Ying 等，[2024](#bib.bib205)）集中于在大规模数据集上训练
    LLMs，包括非正式和正式数学数据，以评估其在自动形式化中的表现。近期研究（Liu 等，[2023](#bib.bib110)；Ye 等，[2023](#bib.bib203)；Zhou
    等，[2024a](#bib.bib214)；Huang 等，[2024](#bib.bib70)）还探讨了某些下游任务中的自动形式化：例如，SAT-LM（Ye
    等，[2023](#bib.bib203)）使用 LLMs 对自然语言问题进行形式化，并通过 ATP 系统解决多个推理任务。DTV（Zhou 等，[2024a](#bib.bib214)）利用自动形式化来巩固
    LLM 推理，形式化 LLM 生成的答案，并使用 ATP 工具进行验证。除了这些努力，Wu 等（[2022](#bib.bib196)）；Azerbayev
    等（[2023](#bib.bib8)）；Jiang 等（[2023a](#bib.bib76)）探索了像 Codex 和 GPT-4（Achiam 等，[2023](#bib.bib3)）这样的高级
    LLMs 用于非正式化，即将正式声明翻译成自然语言。
- en: 3.2 Premise Selection
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 前提选择
- en: Given a large collection of previously proven lemmas, premise selection is to
    retrieve the helpful lemmas that can contribute to a successful proof. It is an
    enduring challenge in both mathematical research and ATP/ITP systems (Kühlwein
    et al., [2012](#bib.bib96); Alama et al., [2014](#bib.bib5)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定大量之前证明过的引理的情况下，前提选择是检索那些可以对成功证明作出贡献的有用引理。这在数学研究和 ATP/ITP 系统中是一个持久的挑战（Kühlwein
    等，[2012](#bib.bib96)；Alama 等，[2014](#bib.bib5)）。
- en: The seminal works (Irving et al., [2016](#bib.bib71); Kaliszyk et al., [2017](#bib.bib85))
    model premise selection as a binary classification task, embedding theorems and
    premises with a variety of neural networks including convolutional neural networks
    (CNNs), recurrent neural networks (RNNs), and hybrid models. These embeddings
    are then combined to feed into a logit layer to predict their relevance. Follow-up
    works (Kucik & Korovin, [2018](#bib.bib95); Bansal et al., [2019](#bib.bib11);
    Piotrowski & Urban, [2020a](#bib.bib136); Proroković et al., [2021](#bib.bib141);
    Szegedy et al., [2021](#bib.bib169)) extend previous frameworks by using a better
    representation of features or more sophisticated architectures like Wavenet (Van
    Den Oord et al., [2016](#bib.bib178)) and Transformer (Vaswani et al., [2017](#bib.bib179)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的研究成果（Irving 等，[2016](#bib.bib71)；Kaliszyk 等，[2017](#bib.bib85)）将前提选择建模为二分类任务，使用包括卷积神经网络（CNNs）、递归神经网络（RNNs）和混合模型在内的各种神经网络对定理和前提进行嵌入。这些嵌入随后被组合以输入到一个逻辑层，以预测它们的相关性。后续的研究（Kucik
    & Korovin，[2018](#bib.bib95)；Bansal 等，[2019](#bib.bib11)；Piotrowski & Urban，[2020a](#bib.bib136)；Proroković
    等，[2021](#bib.bib141)；Szegedy 等，[2021](#bib.bib169)）通过使用更好的特征表示或更复杂的架构如 Wavenet（Van
    Den Oord 等，[2016](#bib.bib178)）和 Transformer（Vaswani 等，[2017](#bib.bib179)）扩展了之前的框架。
- en: Given the inherent structured nature of mathematical formulas, a stream of research (Wang
    et al., [2017](#bib.bib184); Peng & Ma, [2017](#bib.bib132); Olšák et al., [2019](#bib.bib124);
    Goertzel & Urban, [2019](#bib.bib59); Crouse et al., [2019](#bib.bib32); Paliwal
    et al., [2020](#bib.bib127); Rawson & Reger, [2020](#bib.bib144); Liu et al.,
    [2022a](#bib.bib111); Goertzel et al., [2022](#bib.bib61); Holden & Korovin, [2023](#bib.bib68);
    Jakubüv et al., [2023](#bib.bib75)) parses formal statements into trees or directed
    acyclic graphs and leverages tree-structured neural networks (Tai et al., [2015](#bib.bib170))
    or graph neural networks (GNNs) (Duvenaud et al., [2015](#bib.bib41); Veličković
    et al., [2018](#bib.bib180); Xu et al., [2019](#bib.bib199)) for encoding. For
    example, FormulaNet (Wang et al., [2017](#bib.bib184)) proposes a graph embedding
    method that preserves the information of edge ordering. Olšák et al. ([2019](#bib.bib124))
    introduces a GNN framework that captures several logical invariances in FOL formulas.
    Paliwal et al. ([2020](#bib.bib127)) conducts comprehensive experiments to evaluate
    various designs for the graph representations of formulas in HOL Light. Subsequent
    works (Li et al., [2021b](#bib.bib106); Lin et al., [2021](#bib.bib109)) explore
    graph contrastive learning (Oord et al., [2018](#bib.bib125); Chen et al., [2020](#bib.bib24))
    to train GNNs for premise selection. Moreover, Ferreira & Freitas ([2020b](#bib.bib44));
    Bauer et al. ([2023](#bib.bib14)) construct a dependency graph over a large corpus
    by representing theorems and premises as nodes and their dependencies as edges,
    leveraging GNNs to predict the link between nodes for premise selection.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于数学公式固有的结构化特性，一系列研究（Wang 等，[2017](#bib.bib184)；Peng & Ma，[2017](#bib.bib132)；Olšák
    等，[2019](#bib.bib124)；Goertzel & Urban，[2019](#bib.bib59)；Crouse 等，[2019](#bib.bib32)；Paliwal
    等，[2020](#bib.bib127)；Rawson & Reger，[2020](#bib.bib144)；Liu 等，[2022a](#bib.bib111)；Goertzel
    等，[2022](#bib.bib61)；Holden & Korovin，[2023](#bib.bib68)；Jakubüv 等，[2023](#bib.bib75)）将形式声明解析为树或有向无环图，并利用树结构神经网络（Tai
    等，[2015](#bib.bib170)）或图神经网络（GNNs）（Duvenaud 等，[2015](#bib.bib41)；Veličković 等，[2018](#bib.bib180)；Xu
    等，[2019](#bib.bib199)）进行编码。例如，FormulaNet（Wang 等，[2017](#bib.bib184)）提出了一种保留边顺序信息的图嵌入方法。Olšák
    等（[2019](#bib.bib124)）介绍了一种 GNN 框架，捕捉了 FOL 公式中的多个逻辑不变性。Paliwal 等（[2020](#bib.bib127)）进行了综合实验，以评估
    HOL Light 中公式图表示的各种设计。后续工作（Li 等，[2021b](#bib.bib106)；Lin 等，[2021](#bib.bib109)）探索了图对比学习（Oord
    等，[2018](#bib.bib125)；Chen 等，[2020](#bib.bib24)），以训练 GNNs 进行前提选择。此外，Ferreira &
    Freitas（[2020b](#bib.bib44)）；Bauer 等（[2023](#bib.bib14)）通过将定理和前提表示为节点，将它们的依赖关系表示为边，构建了一个大语料库上的依赖图，利用
    GNNs 预测节点之间的链接以进行前提选择。
- en: With the advancement of pre-trained language models, some efforts (Ferreira
    & Freitas, [2020a](#bib.bib43); Welleck et al., [2021](#bib.bib189)) fine-tune
    BERT (Devlin et al., [2019](#bib.bib40))-like models to embed natural language
    statements into vectors and select premises using a linear classifier layer. Later
    works (Ferreira & Freitas, [2021](#bib.bib45); Tran et al., [2022](#bib.bib172);
    Trust et al., [2022](#bib.bib174); Dastgheib & Asgari, [2022](#bib.bib36); Yeh
    et al., [2023](#bib.bib204); Yang et al., [2023](#bib.bib202)) leverage different
    pre-trained models (Liu et al., [2019](#bib.bib112); Song et al., [2020](#bib.bib158);
    Xue et al., [2022](#bib.bib200)) for encoding and retrieve informal/formal premises
    based on several similarity metrics. Han et al. ([2021](#bib.bib64)) also explores
    fine-tuning over large informal mathematical corpora using the contrastive objective (Oord
    et al., [2018](#bib.bib125)), while PACT (Han et al., [2022](#bib.bib65)) uses
    the auto-regressive objective for formal premise selection. Additionally, several
    research (Kovriguina et al., [2022](#bib.bib94); Tworkowski et al., [2022](#bib.bib175);
    Mikuła et al., [2024](#bib.bib119)) design a second phase to re-rank the selected
    subset of premises, enabling a more accurate selection.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随着预训练语言模型的进步，一些研究 (Ferreira & Freitas, [2020a](#bib.bib43); Welleck et al.,
    [2021](#bib.bib189)) 对 BERT (Devlin et al., [2019](#bib.bib40))-类模型进行了微调，以将自然语言陈述嵌入向量中，并使用线性分类器层选择前提。后来的工作 (Ferreira
    & Freitas, [2021](#bib.bib45); Tran et al., [2022](#bib.bib172); Trust et al.,
    [2022](#bib.bib174); Dastgheib & Asgari, [2022](#bib.bib36); Yeh et al., [2023](#bib.bib204);
    Yang et al., [2023](#bib.bib202)) 利用不同的预训练模型 (Liu et al., [2019](#bib.bib112);
    Song et al., [2020](#bib.bib158); Xue et al., [2022](#bib.bib200)) 进行编码，并基于多个相似性度量来检索非正式/正式前提。Han
    et al. ([2021](#bib.bib64)) 还探索了使用对比目标 (Oord et al., [2018](#bib.bib125)) 在大型非正式数学语料库上进行微调，而
    PACT (Han et al., [2022](#bib.bib65)) 则使用自回归目标进行正式前提选择。此外，一些研究 (Kovriguina et al.,
    [2022](#bib.bib94); Tworkowski et al., [2022](#bib.bib175); Mikuła et al., [2024](#bib.bib119))
    设计了第二阶段来重新排序选定的前提子集，从而实现更准确的选择。
- en: 3.3 Proofstep Generation
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 证明步骤生成
- en: Proofstep generation is the core problem for theorem proving, which aims to
    predict one or more steps to build the proof of a theorem. This task also refers
    to tactic prediction in ITP, which has been widely studied in tactic-based ATP
    systems (hammers) (Böhme & Nipkow, [2010](#bib.bib16); Blanchette et al., [2016](#bib.bib15);
    Czajka & Kaliszyk, [2018](#bib.bib35)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 证明步骤生成是定理证明的核心问题，其目标是预测一个或多个步骤以建立定理的证明。这个任务也指的是 ITP 中的策略预测，已在基于策略的 ATP 系统 (hammers) (Böhme
    & Nipkow, [2010](#bib.bib16); Blanchette et al., [2016](#bib.bib15); Czajka &
    Kaliszyk, [2018](#bib.bib35)) 中得到了广泛研究。
- en: A stream of research (Whalen, [2016](#bib.bib191); Huang et al., [2019](#bib.bib69);
    Bansal et al., [2019](#bib.bib11); Paliwal et al., [2020](#bib.bib127); Sanchez-Stern
    et al., [2020](#bib.bib150); Wu et al., [2021b](#bib.bib194); Rute et al., [2024](#bib.bib149))
    treats tactic prediction as a classification problem and uses separate neural
    networks to predict the tactic and its arguments. For example, Gamepad (Huang
    et al., [2019](#bib.bib69)) employs TreeLSTM (Tai et al., [2015](#bib.bib170))
    to encode the proof states and two distinct linear layers for tactic and argument
    prediction. Proverbot9001 (Sanchez-Stern et al., [2020](#bib.bib150)) uses a feed-forward
    neural network and an RNN to predict the tactic and its arguments respectively.
    Besides these works, ASTactic (Yang & Deng, [2019](#bib.bib201)) proposes a decoder
    that generates the tactic as a program, using an RNN to control the generation
    based on a predefined context-free grammar. Later studies (First et al., [2020](#bib.bib48);
    First & Brun, [2022](#bib.bib47); Sanchez-Stern et al., [2023](#bib.bib151)) improve
    ASTactic by incorporating prior proof scripts, combining varied models, and modeling
    identifiers of theorems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列研究 (Whalen, [2016](#bib.bib191); Huang et al., [2019](#bib.bib69); Bansal
    et al., [2019](#bib.bib11); Paliwal et al., [2020](#bib.bib127); Sanchez-Stern
    et al., [2020](#bib.bib150); Wu et al., [2021b](#bib.bib194); Rute et al., [2024](#bib.bib149))
    将策略预测视为分类问题，并使用不同的神经网络来预测策略及其参数。例如，Gamepad (Huang et al., [2019](#bib.bib69))
    使用 TreeLSTM (Tai et al., [2015](#bib.bib170)) 来编码证明状态，并为策略和参数预测使用两个不同的线性层。Proverbot9001 (Sanchez-Stern
    et al., [2020](#bib.bib150)) 分别使用前馈神经网络和 RNN 来预测策略及其参数。除了这些工作之外，ASTactic (Yang
    & Deng, [2019](#bib.bib201)) 提出了一个解码器，该解码器将策略生成作为一个程序，并使用 RNN 基于预定义的上下文无关文法来控制生成。后来的研究 (First
    et al., [2020](#bib.bib48); First & Brun, [2022](#bib.bib47); Sanchez-Stern et al.,
    [2023](#bib.bib151)) 通过结合先前的证明脚本、组合多种模型以及建模定理标识符来改进 ASTactic。
- en: Subsequent advancements (Polu & Sutskever, [2020](#bib.bib138); Polu et al.,
    [2023](#bib.bib139); Han et al., [2022](#bib.bib65); Jiang et al., [2021](#bib.bib78);
    Zhang et al., [2023a](#bib.bib207); Yeh et al., [2023](#bib.bib204); Xiong et al.,
    [2023](#bib.bib198); Welleck & Saha, [2023](#bib.bib188); Vishwakarma & Mishra,
    [2023](#bib.bib181); Gloeckle et al., [2023](#bib.bib58); First et al., [2023](#bib.bib49))
    formulate tactic prediction as language modeling. Specifically, GPT-$f$ (Polu
    & Sutskever, [2020](#bib.bib138)) first apply a conditional language modeling
    objective to train decoder-only Transformers to generate a proof step in the format
    of GOAL <GOAL> PROOFSTEP <PROOFSTEP><EOT>. Baldur (First et al., [2023](#bib.bib49))
    applies a similar objective to generate or repair the whole proof at once. Several
    studies (Szegedy et al., [2021](#bib.bib169); Tworkowski et al., [2022](#bib.bib175);
    Welleck et al., [2022](#bib.bib190); Jiang et al., [2022](#bib.bib79); Yang et al.,
    [2023](#bib.bib202)) also jointly train tactic prediction with premise selection.
    For instance, NaturalProver (Welleck et al., [2022](#bib.bib190)) trains GPT-3 (Brown
    et al., [2020](#bib.bib18)) with constrained decoding to encourage using retrieved
    references in the proof steps. Thor (Jiang et al., [2022](#bib.bib79)) adds a
    <hammer> token to learn when to invoke an ATP tool (Böhme & Nipkow, [2010](#bib.bib16))
    for premise selection to simplify the proof. In the geometry domain, Chen et al.
    ([2022](#bib.bib22)); Liang et al. ([2023a](#bib.bib107)); Trinh et al. ([2024](#bib.bib173));
    He et al. ([2024](#bib.bib67)) follow the same paradigm, auto-regressively generating
    the proof sequence at each step. Notably, AlphaGeometry (Trinh et al., [2024](#bib.bib173))
    trains a decoder-only Transformer to predict the auxiliary constructions in the
    proofs of International Mathematical Olympiad (IMO) geometry problems. Besides
    training on proof data, Azerbayev et al. ([2024](#bib.bib9)); Shao et al. ([2024](#bib.bib155));
    Ying et al. ([2024](#bib.bib205)) train LLMs on extensive general mathematical
    corpora and evaluate their abilities for generating formal proofs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 后续的进展（Polu & Sutskever, [2020](#bib.bib138); Polu et al., [2023](#bib.bib139);
    Han et al., [2022](#bib.bib65); Jiang et al., [2021](#bib.bib78); Zhang et al.,
    [2023a](#bib.bib207); Yeh et al., [2023](#bib.bib204); Xiong et al., [2023](#bib.bib198);
    Welleck & Saha, [2023](#bib.bib188); Vishwakarma & Mishra, [2023](#bib.bib181);
    Gloeckle et al., [2023](#bib.bib58); First et al., [2023](#bib.bib49)) 将战术预测形式化为语言建模。具体来说，GPT-$f$（Polu
    & Sutskever, [2020](#bib.bib138)）首先应用条件语言建模目标来训练仅解码器的Transformers，以生成格式为GOAL <GOAL>
    PROOFSTEP <PROOFSTEP><EOT>的证明步骤。Baldur（First et al., [2023](#bib.bib49)）应用类似的目标来一次性生成或修复整个证明。一些研究（Szegedy
    et al., [2021](#bib.bib169); Tworkowski et al., [2022](#bib.bib175); Welleck et
    al., [2022](#bib.bib190); Jiang et al., [2022](#bib.bib79); Yang et al., [2023](#bib.bib202)）还将战术预测与前提选择联合训练。例如，NaturalProver（Welleck
    et al., [2022](#bib.bib190)）训练GPT-3（Brown et al., [2020](#bib.bib18)）进行受限解码，以鼓励在证明步骤中使用检索的参考资料。Thor（Jiang
    et al., [2022](#bib.bib79)）添加了一个<hammer>标记以学习何时调用ATP工具（Böhme & Nipkow, [2010](#bib.bib16)）进行前提选择，以简化证明。在几何领域，Chen
    et al.（[2022](#bib.bib22)）；Liang et al.（[2023a](#bib.bib107)）；Trinh et al.（[2024](#bib.bib173)）；He
    et al.（[2024](#bib.bib67)）遵循相同的范式，自回归地生成每一步的证明序列。值得注意的是，AlphaGeometry（Trinh et
    al., [2024](#bib.bib173)）训练仅解码器的Transformer来预测国际数学奥林匹克（IMO）几何问题的辅助构造。除了在证明数据上进行训练外，Azerbayev
    et al.（[2024](#bib.bib9)）；Shao et al.（[2024](#bib.bib155)）；Ying et al.（[2024](#bib.bib205)）还在广泛的数学通用语料库上训练LLMs，并评估它们生成形式证明的能力。
- en: Recent explorations (Zhang et al., [2023b](#bib.bib209); Yousefzadeh & Cao,
    [2023](#bib.bib206); Scheidt, [2023](#bib.bib152); Frieder et al., [2023a](#bib.bib51);
    [c](#bib.bib53); [b](#bib.bib52); Zhang et al., [2024](#bib.bib208); Poulsen et al.,
    [2024](#bib.bib140)) also investigate advanced LLMs with several prompting methods
    for proof generation across various domains. Alternative works (Jiang et al.,
    [2023b](#bib.bib77); Zhao et al., [2023](#bib.bib211); Zheng et al., [2023](#bib.bib212);
    Xin et al., [2024](#bib.bib197)) leverage autoformalization to generate the formal
    proof of an informal problem in structured pipelines.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的探索（Zhang et al., [2023b](#bib.bib209); Yousefzadeh & Cao, [2023](#bib.bib206);
    Scheidt, [2023](#bib.bib152); Frieder et al., [2023a](#bib.bib51); [c](#bib.bib53);
    [b](#bib.bib52); Zhang et al., [2024](#bib.bib208); Poulsen et al., [2024](#bib.bib140)）还研究了几种提示方法在各个领域的高级LLMs的证明生成。替代性工作（Jiang
    et al., [2023b](#bib.bib77); Zhao et al., [2023](#bib.bib211); Zheng et al., [2023](#bib.bib212);
    Xin et al., [2024](#bib.bib197)）利用自动形式化在结构化管道中生成非正式问题的形式证明。
- en: 3.4 Proof Search
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 证明搜索
- en: Proof search seeks to systematically traverse the vast landscape of potential
    proof paths to construct a valid proof tree for a given theorem in formal systems.
    It is not only a long-standing research focus in ATP (Urban et al., [2011](#bib.bib177);
    Kaliszyk & Urban, [2015a](#bib.bib81); Jakubüv & Urban, [2017](#bib.bib72)) but
    also a vital process for tactic-based models to complete the proof in ITP.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 证明搜索旨在系统地遍历潜在证明路径的广阔领域，为给定定理在形式系统中构建有效的证明树。这不仅是ATP中的一个长期研究重点（Urban等，[2011](#bib.bib177)；Kaliszyk
    & Urban，[2015a](#bib.bib81)；Jakubüv & Urban，[2017](#bib.bib72)），而且是基于策略的模型在ITP中完成证明的关键过程。
- en: A thread of research trains deep learning models on existing successful proof
    paths in a supervised fashion to guide the search in ATP systems. Specifically,
    a large body of works (Loos et al., [2017](#bib.bib114); Chvalovskỳ et al., [2019](#bib.bib28);
    Jakubüv & Urban, [2019](#bib.bib73); Aygün et al., [2020](#bib.bib6); Jakubüv
    et al., [2020](#bib.bib74); Suda, [2021b](#bib.bib162); Chvalovskỳ et al., [2021](#bib.bib29);
    Goertzel et al., [2021](#bib.bib60); Suda, [2021a](#bib.bib161); Firoiu et al.,
    [2021](#bib.bib46); Aygün et al., [2022](#bib.bib7); Goertzel et al., [2022](#bib.bib61);
    Jakubüv et al., [2023](#bib.bib75); Bártek & Suda, [2023](#bib.bib13)) exploit
    RNNs, GNNs, or hybrid models for the clause selection in saturation-based provers.
    Piepenbrock et al. ([2022b](#bib.bib135)); Chvalovskỳ et al. ([2023](#bib.bib30))
    and Piotrowski & Urban ([2020b](#bib.bib137)) focus on guiding instantiation and
    connection tableau respectively. Some works (Rawson & Reger, [2019](#bib.bib143);
    Olšák et al., [2019](#bib.bib124); Rawson & Reger, [2021](#bib.bib145); Zombori
    et al., [2021b](#bib.bib217)) further combine the supervised trained models as
    the policy or value networks to guide the Monte Carlo Tree Search (MCTS) or A*
    search across various ATP systems. Additionally, another direction of research (Kusumoto
    et al., [2018](#bib.bib97); Fawzi et al., [2019](#bib.bib42); Abdelaziz et al.,
    [2020](#bib.bib1); Zombori et al., [2021a](#bib.bib216); Crouse et al., [2021](#bib.bib33);
    Piepenbrock et al., [2021](#bib.bib133); [2022a](#bib.bib134); Liu et al., [2022b](#bib.bib113);
    Abdelaziz et al., [2022](#bib.bib2); Fokoue et al., [2023](#bib.bib50); McKeown
    & Sutcliffe, [2023](#bib.bib118); Shminke, [2023](#bib.bib156)) models proof search
    as a Markov decision process and applies deep reinforcement learning to train
    and guide the proof search. For instance, TRAIL (Crouse et al., [2021](#bib.bib33))
    uses the policy gradient (Sutton et al., [1999](#bib.bib167)) to train an attention-based
    action policy in saturation-based provers, and Fawzi et al. ([2019](#bib.bib42))
    applies deep Q-learning (Mnih et al., [2013](#bib.bib121)) to guide the choice
    of inference rules in a semi-algebraic proof system (Lovász & Schrijver, [1991](#bib.bib115))
    for polynomial inequalities.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一项研究线程在现有成功的证明路径上以监督的方式训练深度学习模型，以指导ATP系统中的搜索。具体而言，大量的研究工作（Loos等，[2017](#bib.bib114)；Chvalovskỳ等，[2019](#bib.bib28)；Jakubüv
    & Urban，[2019](#bib.bib73)；Aygün等，[2020](#bib.bib6)；Jakubüv等，[2020](#bib.bib74)；Suda，[2021b](#bib.bib162)；Chvalovskỳ等，[2021](#bib.bib29)；Goertzel等，[2021](#bib.bib60)；Suda，[2021a](#bib.bib161)；Firoiu等，[2021](#bib.bib46)；Aygün等，[2022](#bib.bib7)；Goertzel等，[2022](#bib.bib61)；Jakubüv等，[2023](#bib.bib75)；Bártek
    & Suda，[2023](#bib.bib13)）利用RNNs、GNNs或混合模型进行饱和证明器中的子句选择。Piepenbrock等（[2022b](#bib.bib135)）；Chvalovskỳ等（[2023](#bib.bib30)）和Piotrowski
    & Urban（[2020b](#bib.bib137)）分别集中于指导实例化和连接表。某些研究（Rawson & Reger，[2019](#bib.bib143)；Olšák等，[2019](#bib.bib124)；Rawson
    & Reger，[2021](#bib.bib145)；Zombori等，[2021b](#bib.bib217)）进一步将监督训练的模型作为策略或价值网络来指导蒙特卡洛树搜索（MCTS）或A*搜索，涵盖各种ATP系统。此外，另一方向的研究（Kusumoto等，[2018](#bib.bib97)；Fawzi等，[2019](#bib.bib42)；Abdelaziz等，[2020](#bib.bib1)；Zombori等，[2021a](#bib.bib216)；Crouse等，[2021](#bib.bib33)；Piepenbrock等，[2021](#bib.bib133)；[2022a](#bib.bib134)；Liu等，[2022b](#bib.bib113)；Abdelaziz等，[2022](#bib.bib2)；Fokoue等，[2023](#bib.bib50)；McKeown
    & Sutcliffe，[2023](#bib.bib118)；Shminke，[2023](#bib.bib156)）将证明搜索建模为马尔可夫决策过程，并应用深度强化学习来训练和指导证明搜索。例如，TRAIL（Crouse等，[2021](#bib.bib33)）使用策略梯度（Sutton等，[1999](#bib.bib167)）在饱和证明器中训练基于注意力的动作策略，而Fawzi等（[2019](#bib.bib42)）应用深度Q学习（Mnih等，[2013](#bib.bib121)）来指导半代数证明系统（Lovász
    & Schrijver，[1991](#bib.bib115)）中推理规则的选择。
- en: Most tactic-based models in ITPs use beam search to sample multiple tactic predictions
    per step with breadth-first (Bansal et al., [2019](#bib.bib11)), depth-first (Yang
    & Deng, [2019](#bib.bib201)), or best-first heuristics (Polu & Sutskever, [2020](#bib.bib138))
    to traverse the search space. In particular, GPT-$f$ (Polu & Sutskever, [2020](#bib.bib138))
    and FMSCL (Polu et al., [2023](#bib.bib139)) train language models with outcome
    and proof size objectives as value functions to perform the best-first search.
    Besides these methods, Whalen ([2016](#bib.bib191)); Mo et al. ([2020](#bib.bib122));
    Gauthier ([2020](#bib.bib55)); Wu et al. ([2021a](#bib.bib193)); Gauthier ([2021](#bib.bib56));
    Lample et al. ([2022](#bib.bib100)); Wang et al. ([2023a](#bib.bib182)); Brandfonbrener
    et al. ([2024](#bib.bib17)) combine MCTS or use reinforcement learning to train
    and guide the search procedure. For example, HTPS (Lample et al., [2022](#bib.bib100))
    adopts an AlphaZero (Silver et al., [2018](#bib.bib157))-like approach with online
    training, and DT-Solver (Wang et al., [2023a](#bib.bib182)) improves MCTS with
    dynamic tree sampling and proof-level value function. Additionally, COPRA (Thakur
    et al., [2023](#bib.bib171)) implements a language-agent method that leverages
    GPT-4 to perform a backtracking search based on the proof history.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于策略的模型在ITPs中使用束搜索，通过广度优先（Bansal 等，[2019](#bib.bib11)）、深度优先（Yang & Deng，[2019](#bib.bib201)）或最佳优先启发式（Polu
    & Sutskever，[2020](#bib.bib138)）来对每一步进行多个策略预测，遍历搜索空间。特别地，GPT-$f$（Polu & Sutskever，[2020](#bib.bib138)）和FMSCL（Polu
    等，[2023](#bib.bib139)）训练语言模型，以结果和证明规模目标作为价值函数，执行最佳优先搜索。除了这些方法，Whalen（[2016](#bib.bib191)）；Mo
    等（[2020](#bib.bib122)）；Gauthier（[2020](#bib.bib55)）；Wu 等（[2021a](#bib.bib193)）；Gauthier（[2021](#bib.bib56)）；Lample
    等（[2022](#bib.bib100)）；Wang 等（[2023a](#bib.bib182)）；Brandfonbrener 等（[2024](#bib.bib17)）结合了MCTS或使用强化学习来训练和指导搜索过程。例如，HTPS（Lample
    等，[2022](#bib.bib100)）采用了类似AlphaZero（Silver 等，[2018](#bib.bib157)）的方法进行在线训练，而DT-Solver（Wang
    等，[2023a](#bib.bib182)）通过动态树采样和证明级别价值函数改进了MCTS。此外，COPRA（Thakur 等，[2023](#bib.bib171)）实现了一种语言代理方法，利用GPT-4基于证明历史进行回溯搜索。
- en: 3.5 Other Tasks
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 其他任务
- en: In addition to the primary tasks outlined previously, we briefly list other
    prediction tasks related or helpful to theorem proving. A line of research (Urban
    & Jakubüv, [2020](#bib.bib176); Piotrowski & Urban, [2020b](#bib.bib137); Rabe
    et al., [2021](#bib.bib142); Johansson & Smallbone, [2023](#bib.bib80)) explore
    automated conjecturing. Lee et al. ([2020](#bib.bib101)); Wu & Wu ([2021](#bib.bib192))
    investigate proving theorems in the latent space. IsarStep (Li et al., [2021a](#bib.bib105))
    predicts the intermediate proposition given surrounding proofs. Skip-tree (Rabe
    et al., [2021](#bib.bib142)) and PACT (Han et al., [2022](#bib.bib65)) propose
    several self-supervised tasks by masking various proof terms for training language
    models. LIME (Wu et al., [2021c](#bib.bib195)) constructs three reasoning tasks
    for deduction, abduction, and induction. Recently, Li et al. ([2023](#bib.bib104))
    proposes to match the proofs with theorem statements from a large database and
    REFACTOR (Zhou et al., [2024b](#bib.bib215)) predicts to extract theorems from
    proofs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前概述的主要任务外，我们简要列出了一些与定理证明相关或有助于定理证明的其他预测任务。一些研究（Urban & Jakubüv，[2020](#bib.bib176)；Piotrowski
    & Urban，[2020b](#bib.bib137)；Rabe 等，[2021](#bib.bib142)；Johansson & Smallbone，[2023](#bib.bib80)）探讨了自动猜想。Lee
    等（[2020](#bib.bib101)）；Wu & Wu（[2021](#bib.bib192)）研究了在潜在空间中证明定理。IsarStep（Li 等，[2021a](#bib.bib105)）在给定周围证明的情况下预测中间命题。Skip-tree（Rabe
    等，[2021](#bib.bib142)）和PACT（Han 等，[2022](#bib.bib65)）通过掩盖各种证明项提出了几个自监督任务，用于训练语言模型。LIME（Wu
    等，[2021c](#bib.bib195)）构建了三个推理任务，用于演绎、归纳和推测。最近，Li 等（[2023](#bib.bib104)）提出了将证明与来自大数据库的定理声明进行匹配，REFACTOR（Zhou
    等，[2024b](#bib.bib215)）预测从证明中提取定理。
- en: 4 Datasets
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据集
- en: 'This section classifies and summarizes datasets for theorem proving into 2
    categories: i) datasets extracted from existing corpora or manually curated and
    ii) those using synthetic generation or augmentation methods. The overview of
    these datasets is shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Data Collection ‣
    4 Datasets ‣ A Survey on Deep Learning for Theorem Proving").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将定理证明的数据集分类和总结为两类：i) 从现有语料库中提取或手动策划的数据集，以及ii) 使用合成生成或增强方法的数据集。这些数据集的概述见图[3](#S4.F3
    "图 3 ‣ 4.1 数据收集 ‣ 4 数据集 ‣ 定理证明深度学习的调查")。
- en: 4.1 Data Collection
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据收集
- en: We begin with the review of informal datasets. NL-PS (Ferreira & Freitas, [2020a](#bib.bib43))
    first builds a natural language premise selection dataset source from [ProofWiki](https://proofwiki.org).
    Similarly, NaturalProofs (Welleck et al., [2021](#bib.bib189)) further incorporates
    data from [Stacks](https://stacks.math.columbia.edu/) and textbooks, resulting
    in a dataset with roughly 25k examples. Adapted from it, NaturalProofs-Gen (Welleck
    et al., [2022](#bib.bib190)) contains around 14.5k theorems for informal proof
    generation. Moreover, MATcH (Li et al., [2023](#bib.bib104)) constructs over 180k
    statement-proof pairs for matching using the [MREC corpus](https://mir.fi.muni.cz/MREC/).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先回顾非正式数据集。NL-PS (Ferreira & Freitas, [2020a](#bib.bib43)) 首先构建了一个来自 [ProofWiki](https://proofwiki.org)
    的自然语言前提选择数据集。类似地，NaturalProofs (Welleck et al., [2021](#bib.bib189)) 进一步融合了来自
    [Stacks](https://stacks.math.columbia.edu/) 和教科书的数据，形成了一个大约包含 25k 示例的数据集。根据此数据集改编，NaturalProofs-Gen (Welleck
    et al., [2022](#bib.bib190)) 包含大约 14.5k 个用于非正式证明生成的定理。此外，MATcH (Li et al., [2023](#bib.bib104))
    构建了超过 180k 个陈述-证明对用于匹配，使用了 [MREC corpus](https://mir.fi.muni.cz/MREC/)。
- en: For formal datasets, a line of efforts focuses on extracting and cleaning theorems
    and proofs from established formal libraries and verification projects. Notable
    datasets for Coq include Gamepad (Huang et al., [2019](#bib.bib69)), CoqGym (Yang
    & Deng, [2019](#bib.bib201)), and PRISM (Reichel et al., [2023](#bib.bib146)),
    with CoqGym constructing a dataset from 123 projects encompassing 71k proofs.
    For Isabelle, datasets like IsarStep (Li et al., [2021a](#bib.bib105)), PISA (Jiang
    et al., [2021](#bib.bib78)), and Magnushammer (Mikuła et al., [2024](#bib.bib119))
    are built on the [Archive of Formal Proofs](https://www.isa-afp.org/) and [Isabelle
    Standard Library](https://isabelle.in.tum.de/library/), where PISA extracts 183K
    theorems and 2.16M proof steps. LeanStep (Han et al., [2022](#bib.bib65)), LeanDojo (Yang
    et al., [2023](#bib.bib202)), and MLFMF (Bauer et al., [2023](#bib.bib14)) leverage
    the mathlib library (mathlib Community, [2020](#bib.bib117)) in Lean. In particular,
    LeanDojo extracts over 98k theorems and proofs with 130k premises from mathlib.
    Datasets for other proof assistants include HolStep (Kaliszyk et al., [2017](#bib.bib85))
    and HOList (Bansal et al., [2019](#bib.bib11)) for HOL Light, MPTP2078 (Alama
    et al., [2014](#bib.bib5)), Mizar40 (Kaliszyk & Urban, [2015b](#bib.bib82)), and
    M2K (Kaliszyk et al., [2018](#bib.bib86)) for Mizar, etc. Besides extracting data
    from existing projects, several works manually annotate or formalize the problems
    in natural language. Notably, MiniF2F (Zheng et al., [2022](#bib.bib213)) manually
    formalizes 488 Olympiad-level problems across 4 proof systems and equally splits
    them into a validation set and a test set. FIMO (Liu et al., [2023](#bib.bib110))
    and ProofNet (Azerbayev et al., [2023](#bib.bib8)) formalize the theorem statements
    of IMO and undergraduate-level problems in Lean. For other domains, TRIGO (Xiong
    et al., [2023](#bib.bib198)) targets formalizing the trigonometric reduction problem,
    while UniGeo (Chen et al., [2022](#bib.bib22)) and FormalGeo (Zhang et al., [2023c](#bib.bib210))
    annotate the proof steps for geometry proving problems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正式数据集，一系列努力集中在从既有的正式库和验证项目中提取和清理定理和证明。 Coq 的著名数据集包括 Gamepad (Huang et al.,
    [2019](#bib.bib69))、CoqGym (Yang & Deng, [2019](#bib.bib201)) 和 PRISM (Reichel
    et al., [2023](#bib.bib146))，其中 CoqGym 从 123 个项目中构建了一个包含 71k 证明的数据集。对于 Isabelle，像
    IsarStep (Li et al., [2021a](#bib.bib105))、PISA (Jiang et al., [2021](#bib.bib78))
    和 Magnushammer (Mikuła et al., [2024](#bib.bib119)) 的数据集是基于 [Archive of Formal
    Proofs](https://www.isa-afp.org/) 和 [Isabelle Standard Library](https://isabelle.in.tum.de/library/)，其中
    PISA 提取了 183K 个定理和 2.16M 个证明步骤。LeanStep (Han et al., [2022](#bib.bib65))、LeanDojo (Yang
    et al., [2023](#bib.bib202)) 和 MLFMF (Bauer et al., [2023](#bib.bib14)) 在 Lean
    中利用了 mathlib 库 (mathlib Community, [2020](#bib.bib117))。特别是，LeanDojo 从 mathlib
    中提取了超过 98k 个定理和证明及 130k 个前提。其他证明助手的数据集包括 HolStep (Kaliszyk et al., [2017](#bib.bib85))
    和 HOList (Bansal et al., [2019](#bib.bib11)) 用于 HOL Light，MPTP2078 (Alama et al.,
    [2014](#bib.bib5))、Mizar40 (Kaliszyk & Urban, [2015b](#bib.bib82)) 和 M2K (Kaliszyk
    et al., [2018](#bib.bib86)) 用于 Mizar 等。除了从现有项目中提取数据之外，还有一些工作手动标注或形式化自然语言中的问题。特别是，MiniF2F (Zheng
    et al., [2022](#bib.bib213)) 手动形式化了 488 个奥林匹克级问题，涵盖 4 个证明系统，并将它们均分为验证集和测试集。FIMO (Liu
    et al., [2023](#bib.bib110)) 和 ProofNet (Azerbayev et al., [2023](#bib.bib8))
    在 Lean 中形式化了 IMO 和本科级问题的定理陈述。对于其他领域，TRIGO (Xiong et al., [2023](#bib.bib198))
    旨在形式化三角形化简问题，而 UniGeo (Chen et al., [2022](#bib.bib22)) 和 FormalGeo (Zhang et al.,
    [2023c](#bib.bib210)) 注释了几何证明问题的证明步骤。
- en: On the other hand, a large body of recent studies leverages large-scale online
    corpora with billions of tokens from informal and formal mathematical data to
    build the pre-training datasets that could aid in theorem proving. These datasets
    include WebMath (Han et al., [2022](#bib.bib65)), Proof-Pile (Azerbayev et al.,
    [2023](#bib.bib8)), InternLM-Math (Ying et al., [2024](#bib.bib205)), etc.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，大量近期研究利用包含数十亿标记的规模庞大的在线语料库，这些语料库包括非正式和正式的数学数据，以构建可能有助于定理证明的预训练数据集。这些数据集包括WebMath (Han
    et al., [2022](#bib.bib65))，Proof-Pile (Azerbayev et al., [2023](#bib.bib8))，InternLM-Math (Ying
    et al., [2024](#bib.bib205))等。
- en: '{forest}'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '{森林}'
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=black, rounded corners, align=left,
    minimum width=2.5em, edge+=black, line width=0.5pt, s sep=3pt, inner xsep=1.9pt,
    inner ysep=1.9pt, line width=0.5pt, ver/.style=rotate=90, child anchor=north,
    parent anchor=south, anchor=center, , where level=0text width=2.2em,font=,, where
    level=1text width=4.4em,font=,, where level=2text width=3.8em,font=,, where level=3text
    width=38.6em,font=,, [ Datasets [Data Collection [Natural
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 分叉边缘，树=生长=东，反向=true，锚点=基线西，父锚点=东，子锚点=西，基点=左，字体=，矩形，描边=黑色，圆角，左对齐，最小宽度=2.5em，边缘+=黑色，线宽=0.5pt，s
    sep=3pt，内部 xsep=1.9pt，内部 ysep=1.9pt，线宽=0.5pt，ver/.style=旋转=90，子锚点=北，父锚点=南，锚点=中心，，当级别=0时文本宽度=2.2em，字体=，，当级别=1时文本宽度=4.4em，字体=，，当级别=2时文本宽度=3.8em，字体=，，当级别=3时文本宽度=38.6em，字体=，，
    [数据集 [数据收集 [自然
- en: Language[ E.g., NL-PS (Ferreira & Freitas, [2020a](#bib.bib43)), NaturalProofs (Welleck
    et al., [2021](#bib.bib189)), NaturalProofs-Gen (Welleck et al., [2022](#bib.bib190)),
    MATcH (Li et al., [2023](#bib.bib104))]] [Coq [ E.g., GamePad (Huang et al., [2019](#bib.bib69)),
    CoqGym (Yang & Deng, [2019](#bib.bib201)), PRISM (Reichel et al., [2023](#bib.bib146))]]
    [Isabelle [ E.g., IsarStep (Li et al., [2021a](#bib.bib105)), PISA (Jiang et al.,
    [2021](#bib.bib78)), MiniF2F (Zheng et al., [2022](#bib.bib213)), Magnushammer (Mikuła
    et al., [2024](#bib.bib119))]] [Lean [ E.g., LeanStep (Han et al., [2022](#bib.bib65)),
    MiniF2F (Zheng et al., [2022](#bib.bib213)), FIMO (Liu et al., [2023](#bib.bib110)),
    TRIGO (Xiong et al., [2023](#bib.bib198)),
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 语言 [例如，NL-PS (Ferreira & Freitas, [2020a](#bib.bib43))，NaturalProofs (Welleck
    et al., [2021](#bib.bib189))，NaturalProofs-Gen (Welleck et al., [2022](#bib.bib190))，MATcH (Li
    et al., [2023](#bib.bib104))]] [Coq [例如，GamePad (Huang et al., [2019](#bib.bib69))，CoqGym (Yang
    & Deng, [2019](#bib.bib201))，PRISM (Reichel et al., [2023](#bib.bib146))]] [Isabelle
    [例如，IsarStep (Li et al., [2021a](#bib.bib105))，PISA (Jiang et al., [2021](#bib.bib78))，MiniF2F (Zheng
    et al., [2022](#bib.bib213))，Magnushammer (Mikuła et al., [2024](#bib.bib119))]]
    [Lean [例如，LeanStep (Han et al., [2022](#bib.bib65))，MiniF2F (Zheng et al., [2022](#bib.bib213))，FIMO (Liu
    et al., [2023](#bib.bib110))，TRIGO (Xiong et al., [2023](#bib.bib198)),
- en: ProofNet (Azerbayev et al., [2023](#bib.bib8)), LeanDojo (Yang et al., [2023](#bib.bib202)),
    MLFMF (Bauer et al., [2023](#bib.bib14))]] [HOL Light [ E.g., HolStep (Kaliszyk
    et al., [2017](#bib.bib85)), HOList (Bansal et al., [2019](#bib.bib11)), MiniF2F (Zheng
    et al., [2022](#bib.bib213))]] [Mizar [ E.g., MPTP2078 (Alama et al., [2014](#bib.bib5)),
    Mizar40 (Kaliszyk & Urban, [2015b](#bib.bib82)), M2K (Kaliszyk et al., [2018](#bib.bib86))]]
    [Geometry [ E.g., UniGeo (Chen et al., [2022](#bib.bib22)), FormalGeo (Zhang et al.,
    [2023c](#bib.bib210))]] [Other
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ProofNet (Azerbayev et al., [2023](#bib.bib8))，LeanDojo (Yang et al., [2023](#bib.bib202))，MLFMF (Bauer
    et al., [2023](#bib.bib14))]] [HOL Light [例如，HolStep (Kaliszyk et al., [2017](#bib.bib85))，HOList (Bansal
    et al., [2019](#bib.bib11))，MiniF2F (Zheng et al., [2022](#bib.bib213))]] [Mizar
    [例如，MPTP2078 (Alama et al., [2014](#bib.bib5))，Mizar40 (Kaliszyk & Urban, [2015b](#bib.bib82))，M2K (Kaliszyk
    et al., [2018](#bib.bib86))]] [几何 [例如，UniGeo (Chen et al., [2022](#bib.bib22))，FormalGeo (Zhang
    et al., [2023c](#bib.bib210))]] [其他
- en: Languages [ E.g., Holophrasm (Whalen, [2016](#bib.bib191)), TPTP (Sutcliffe,
    [2017](#bib.bib164)), TacticToe (Gauthier et al., [2020](#bib.bib57)), MLFMF (Bauer
    et al., [2023](#bib.bib14))]] [Pre-training
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 语言 [例如，Holophrasm (Whalen, [2016](#bib.bib191))，TPTP (Sutcliffe, [2017](#bib.bib164))，TacticToe (Gauthier
    et al., [2020](#bib.bib57))，MLFMF (Bauer et al., [2023](#bib.bib14))]] [预训练
- en: Corpus [ E.g., WebMath (Polu & Sutskever, [2020](#bib.bib138)), Proof-Pile (Azerbayev
    et al., [2023](#bib.bib8)), MathPile (Wang et al., [2023b](#bib.bib187)), OpenWebMath (Paster
    et al., [2024](#bib.bib129)),
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库 [例如，WebMath (Polu & Sutskever, [2020](#bib.bib138))，Proof-Pile (Azerbayev
    et al., [2023](#bib.bib8))，MathPile (Wang et al., [2023b](#bib.bib187))，OpenWebMath (Paster
    et al., [2024](#bib.bib129))，
- en: Proof-Pile-v2 (Azerbayev et al., [2024](#bib.bib9)), DeepSeekMath (Shao et al.,
    [2024](#bib.bib155)), InternLM-Math (Ying et al., [2024](#bib.bib205))]] ] [Data
    Generation [Rule-based
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Proof-Pile-v2 (Azerbayev et al., [2024](#bib.bib9))，DeepSeekMath (Shao et al.,
    [2024](#bib.bib155))，InternLM-Math (Ying et al., [2024](#bib.bib205))]] ] [数据生成
    [基于规则
- en: Generator [ E.g., INT (Wu et al., [2021b](#bib.bib194)), MetaGen (Wang & Deng,
    [2020](#bib.bib183)), LIME (Wu et al., [2021c](#bib.bib195)), FwdP (Firoiu et al.,
    [2021](#bib.bib46))
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器 [例如，INT (Wu et al., [2021b](#bib.bib194)), MetaGen (Wang & Deng, [2020](#bib.bib183)),
    LIME (Wu et al., [2021c](#bib.bib195)), FwdP (Firoiu et al., [2021](#bib.bib46))
- en: AutoTrig (Liu et al., [2022b](#bib.bib113)), TRIGO-gen (Xiong et al., [2023](#bib.bib198)),
    GeomVerse (Kazemi et al., [2023](#bib.bib89)), AlphaGeometry (Trinh et al., [2024](#bib.bib173))]]
    [Iterative
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: AutoTrig (Liu et al., [2022b](#bib.bib113)), TRIGO-gen (Xiong et al., [2023](#bib.bib198)),
    GeomVerse (Kazemi et al., [2023](#bib.bib89)), AlphaGeometry (Trinh et al., [2024](#bib.bib173))]]
    [迭代
- en: Augmentation [ E.g., DeepHOL (Bansal et al., [2019](#bib.bib11)), GPT-$f$ (Polu
    & Sutskever, [2020](#bib.bib138)), FMSCL (Polu et al., [2023](#bib.bib139)), HER (Aygün
    et al., [2022](#bib.bib7))]] [Lemma
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 增强 [例如，DeepHOL (Bansal et al., [2019](#bib.bib11)), GPT-$f$ (Polu & Sutskever,
    [2020](#bib.bib138)), FMSCL (Polu et al., [2023](#bib.bib139)), HER (Aygün et
    al., [2022](#bib.bib7))]] [引理
- en: Discovery [ E.g., LEGO-Prover (Xin et al., [2024](#bib.bib197)), REFACTOR (Zhou
    et al., [2024b](#bib.bib215))]] [Other
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 [例如，LEGO-Prover (Xin et al., [2024](#bib.bib197)), REFACTOR (Zhou et al.,
    [2024b](#bib.bib215))]] [其他
- en: Methods [ E.g., MMA (Jiang et al., [2023a](#bib.bib76)), MUSTARD (Huang et al.,
    [2024](#bib.bib70))]] ] ]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 [例如，MMA (Jiang et al., [2023a](#bib.bib76)), MUSTARD (Huang et al., [2024](#bib.bib70))]]
    ] ]
- en: 'Figure 3: The taxonomy of datasets in theorem proving.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：定理证明中的数据集分类。
- en: 4.2 Data Generation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据生成
- en: Beyond utilizing existing projects, researchers also study the generation of
    new proof data. A line of work (Wu et al., [2021b](#bib.bib194); [c](#bib.bib195);
    Firoiu et al., [2021](#bib.bib46); Liu et al., [2022b](#bib.bib113); Xiong et al.,
    [2023](#bib.bib198); Kazemi et al., [2023](#bib.bib89); Trinh et al., [2024](#bib.bib173))
    develops rule-based generators to generate both theorems and proofs by iteratively
    sampling inference rules and axioms from a pre-defined set to form a new theorem.
    Such a generation process enables manual control of difficulty. For example, INT (Wu
    et al., [2021b](#bib.bib194)) synthesizes theorems for inequalities with different
    proof lengths while GeomVerse (Kazemi et al., [2023](#bib.bib89)) targets geometry
    problems with 5 dimensions of difficulty design. MetaGen (Wang & Deng, [2020](#bib.bib183))
    further trains a neural generator to synthesize theorems similar to human-write
    ones.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用现有项目，研究人员还研究了新证明数据的生成。一系列工作 (Wu et al., [2021b](#bib.bib194); [c](#bib.bib195);
    Firoiu et al., [2021](#bib.bib46); Liu et al., [2022b](#bib.bib113); Xiong et
    al., [2023](#bib.bib198); Kazemi et al., [2023](#bib.bib89); Trinh et al., [2024](#bib.bib173))
    开发了基于规则的生成器，通过从预定义集合中迭代采样推理规则和公理来生成定理和证明。这种生成过程使得可以手动控制难度。例如，INT (Wu et al., [2021b](#bib.bib194))
    生成具有不同证明长度的不等式定理，而 GeomVerse (Kazemi et al., [2023](#bib.bib89)) 针对具有 5 个难度维度的几何问题。MetaGen
    (Wang & Deng, [2020](#bib.bib183)) 进一步训练了一个神经生成器来合成类似于人类编写的定理。
- en: Alternative approaches turn to iteratively augment the training dataset with
    fixed theorems but newly generated proofs. A line of work (Bansal et al., [2019](#bib.bib11);
    Polu & Sutskever, [2020](#bib.bib138); Polu et al., [2023](#bib.bib139)) adopts
    the idea of expert iteration (Silver et al., [2018](#bib.bib157)), which repeatedly
    applies the trained prover on existing theorems and adds the successful proof
    paths as new data points to train the prover further. Aygün et al. ([2022](#bib.bib7))
    further proposes to adapt hindsight experience replay (Aygün et al., [2022](#bib.bib7))
    to FOL provers, which leverages previously unsuccessful proof trajectories by
    viewing their final states as the desired ones. Meanwhile, a line of reinforcement
    learning methods can also be viewed in this category.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 替代方法转向通过固定定理但新生成的证明来迭代增强训练数据集。一系列工作 (Bansal et al., [2019](#bib.bib11); Polu
    & Sutskever, [2020](#bib.bib138); Polu et al., [2023](#bib.bib139)) 采用了专家迭代的理念
    (Silver et al., [2018](#bib.bib157))，该方法重复应用训练过的证明器于现有定理，并将成功的证明路径作为新的数据点来进一步训练证明器。Aygün
    et al. ([2022](#bib.bib7)) 进一步提出将事后经验重放 (Aygün et al., [2022](#bib.bib7)) 适应于
    FOL 证明器，这通过将先前未成功的证明轨迹视为期望的结果来利用这些轨迹。同时，一系列强化学习方法也可以被视为这一类别。
- en: Additionally, some works aim to generate intermediate helpful lemmas. REFACTOR (Zhou
    et al., [2024b](#bib.bib215)) trains a GNN to extract lemmas from proofs, which
    can be used to streamline the proofs of other theorems. LEGO-Prover (Xin et al.,
    [2024](#bib.bib197)) prompts GPT-4 to generate sub-goal lemmas for the proof of
    a theorem and finalize it by proving or retrieving these lemmas. Newly proven
    lemmas are added to a growing library for future use. Works on conjecturing are
    also relevant to this field despite the generated conjectures may be incorrect.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，一些研究致力于生成中间的有用引理。REFACTOR（Zhou et al., [2024b](#bib.bib215)）训练 GNN 从证明中提取引理，这些引理可以用来简化其他定理的证明。LEGO-Prover（Xin
    et al., [2024](#bib.bib197)）提示 GPT-4 生成定理证明的子目标引理，并通过证明或检索这些引理来完成定理的证明。新证明的引理会被添加到一个不断增长的库中以供将来使用。尽管生成的猜想可能不正确，关于猜想的研究仍然与这一领域相关。
- en: Moreover, several recent approaches leverage auto(in)formalization for data
    generation. Using GPT-4, MMA (Jiang et al., [2023a](#bib.bib76)) informalizes
    all theorem statements in Archive of Formal Proofs and mathlib, and MUSTARD (Huang
    et al., [2024](#bib.bib70)) synthesizes problems from seed concepts, crafts informal
    proofs, and translates them into Lean to verify the correctness.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些近期的方法利用自动（非）形式化进行数据生成。使用 GPT-4，MMA（Jiang et al., [2023a](#bib.bib76)）将《形式化证明档案》和
    mathlib 中的所有定理语句进行非形式化，而 MUSTARD（Huang et al., [2024](#bib.bib70)）则从种子概念中合成问题，制作非形式化证明，并将其翻译成
    Lean 以验证正确性。
- en: 5 Evaluations
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: In this section, we focus on the evaluations of deep learning approaches in
    theorem proving, analyzing the key metrics and state-of-the-art performance for
    each task.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们重点关注深度学习方法在定理证明中的评估，分析每个任务的关键指标和最先进的性能。
- en: Autoformalization. The assessment of autoformalization mainly relies on manually
    checking the equivalence between informal and formalized statements. Through such
    evaluation, recent studies (Wu et al., [2022](#bib.bib196); Azerbayev et al.,
    [2023](#bib.bib8)) show that state-of-the-art LLMs with few-shot prompting can
    correctly formalize 25% and 13% high-school and undergraduate-level problems,
    demonstrating the challenge in autoformalization. In addition, both studies reveal
    that LLMs exhibit considerably higher efficacy in informalization, achieving around
    76% and 62% accuracies. Despite the modest success in autoformalizing statements,
    subsequent research (Jiang et al., [2023b](#bib.bib77); Zhao et al., [2023](#bib.bib211);
    Xin et al., [2024](#bib.bib197)) reveals the value of autoformalizing informal
    intermediate goals to prove theorems in modularized pipelines.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 自动形式化。自动形式化的评估主要依赖于手动检查非形式化和形式化语句之间的等价性。通过这种评估，近期的研究（Wu et al., [2022](#bib.bib196);
    Azerbayev et al., [2023](#bib.bib8)）表明，最先进的 LLMs 在少量提示下能够正确形式化 25% 和 13% 的高中和本科水平的问题，显示了自动形式化的挑战。此外，两项研究还揭示
    LLMs 在非形式化中的效果明显更高，准确率分别达到约 76% 和 62%。尽管在形式化语句上取得了适度成功，但后续研究（Jiang et al., [2023b](#bib.bib77);
    Zhao et al., [2023](#bib.bib211); Xin et al., [2024](#bib.bib197)）揭示了在模块化流程中自动形式化非形式化中间目标以证明定理的价值。
- en: Premise selection. Retrieval metrics are widely used as the evaluation metric
    for premise selection. For example, recall at k (R@k) measures the ratio of correctly
    used premises in ground-truth proof within the top-k selections, and the mean
    reciprocal rank (MRR) computes the average reciprocal rank of the first correctly
    selected premise. Dense passage retrieval (DPR) (Karpukhin et al., [2020](#bib.bib88))
    with a Transformer encoder has significantly improved upon traditional methods,
    demonstrating a remarkable generalization ability to unseen data for premise selection.
    For example, ReProver (Yang et al., [2023](#bib.bib202)) achieves 27.6% for R@10
    and 0.24 for MRR on the LeanDojo benchmark for retrieving unseen premises in training,
    while BM25 (Robertson et al., [2009](#bib.bib147)) achieves 15.5% for R@10 and
    0.14 for MRR. Furthermore, a DPR-based retriever, Magnushammer (Mikuła et al.,
    [2024](#bib.bib119)), outperforms a symbolic method, Sledgehammer (Böhme & Nipkow,
    [2010](#bib.bib16)), when used to select premises for the Thor prover (Jiang et al.,
    [2022](#bib.bib79)), improving the theorem proving success rate from 57% to 71%
    on the PISA dataset and from 28.3% to 36.9% on the MiniF2F-valid dataset.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 前提选择。检索指标被广泛用作前提选择的评估指标。例如，k 处的召回率（R@k）衡量的是在前 k 个选择中正确使用的前提的比例，而平均倒排排名（MRR）计算的是第一个正确选择的前提的平均倒排排名。基于
    Transformer 编码器的密集段落检索（DPR）（Karpukhin 等，[2020](#bib.bib88)）显著提高了传统方法的性能，展示了对未见数据的卓越泛化能力。例如，ReProver（Yang
    等，[2023](#bib.bib202)）在 LeanDojo 基准上对训练中未见前提的检索达到 R@10 的 27.6% 和 MRR 的 0.24，而
    BM25（Robertson 等，[2009](#bib.bib147)）则达到 R@10 的 15.5% 和 MRR 的 0.14。此外，基于 DPR 的检索器
    Magnushammer（Mikuła 等，[2024](#bib.bib119)）在用于选择 Thor 证明器（Jiang 等，[2022](#bib.bib79)）的前提时优于符号方法
    Sledgehammer（Böhme & Nipkow，[2010](#bib.bib16)），在 PISA 数据集上的定理证明成功率从 57% 提升到 71%，在
    MiniF2F-valid 数据集上的成功率从 28.3% 提升到 36.9%。
- en: Theorem proving. The effectiveness of proofstep generation, proof search, and
    support from autoformalization and premise selection can be collectively evaluated
    by their success rate in proving theorems within a test set. Recent research (Zheng
    et al., [2023](#bib.bib212); Xin et al., [2024](#bib.bib197)) shows impressive
    performance increases using state-of-the-art LLMs like GPT-4 in structured frameworks
    than tactic-based models with the best-first search. For instance, LEGO-Prover (Xin
    et al., [2024](#bib.bib197)) achieves 57.0% accuracy and 50.0% accuracy on the
    valid and test set of MiniF2F, while the previous best model Thor (Jiang et al.,
    [2022](#bib.bib79)) with expert iteration training (Wu et al., [2022](#bib.bib196))
    achieves 37.3% and 35.2%. Moreover, deep learning-based proof searches could further
    improve the performance of ITP/ATP systems. HTPS (Lample et al., [2022](#bib.bib100))
    achieves an accumulative successful rate of 58.6% on MiniF2F-valid through online
    training and a 41.0% success rate with 64 search attempts on MiniF2F-test. Besides,
    the reinforcement learning-based ATP prover NIAGRA (Fokoue et al., [2023](#bib.bib50))
    outperforms both E (Schulz, [2002](#bib.bib153)) and Vampire (Kovács & Voronkov,
    [2013](#bib.bib93)) on the MPTP2078 dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 定理证明。证明步骤生成、证明搜索以及自动形式化和前提选择的支持效果可以通过它们在测试集中的定理证明成功率来综合评估。最近的研究（Zheng 等，[2023](#bib.bib212)；Xin
    等，[2024](#bib.bib197)）表明，使用像 GPT-4 这样的最先进 LLM 在结构化框架中的表现比使用最佳优先搜索的基于策略的模型更为出色。例如，LEGO-Prover（Xin
    等，[2024](#bib.bib197)）在 MiniF2F 的有效集和测试集上的准确率分别为 57.0% 和 50.0%，而之前最佳的模型 Thor（Jiang
    等，[2022](#bib.bib79)）通过专家迭代训练（Wu 等，[2022](#bib.bib196)）的准确率为 37.3% 和 35.2%。此外，基于深度学习的证明搜索可以进一步提高
    ITP/ATP 系统的性能。HTPS（Lample 等，[2022](#bib.bib100)）通过在线训练在 MiniF2F-valid 上达到了 58.6%
    的累积成功率，并在 MiniF2F-test 上通过 64 次搜索尝试达到了 41.0% 的成功率。此外，基于强化学习的 ATP 证明器 NIAGRA（Fokoue
    等，[2023](#bib.bib50)）在 MPTP2078 数据集上超越了 E（Schulz，[2002](#bib.bib153)）和 Vampire（Kovács
    & Voronkov，[2013](#bib.bib93)）。
- en: Caveats. Tasks related to theorem proving can be tricky to evaluate. For autoformalization,
    manual evaluation is costly, whereas automated metrics are inaccurate. For example,
    the compilation rate evaluates only syntactic correctness, and the BLEU score (Papineni
    et al., [2002](#bib.bib128)) struggles with formalizations semantically similar
    but not logically equivalent to the ground truth. For premise selection, metrics
    rely on the premises that are used in ground-truth proofs, so they may neglect
    other valid premises for alternative correct proofs different from the ground-truth
    ones, causing false negatives. Lastly, the evaluation of theorem proving is complicated
    by various experimental setups (§ [6.1](#S6.SS1 "6.1 Challenges ‣ 6 Discussions
    ‣ A Survey on Deep Learning for Theorem Proving")).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项。与定理证明相关的任务可能难以评估。对于自动形式化，人工评估成本高，而自动化指标不准确。例如，编译率只评估语法正确性，而BLEU分数（Papineni等，[2002](#bib.bib128)）在形式化过程中处理语义相似但与真实情况不完全一致的内容时表现不佳。对于前提选择，指标依赖于在真实证明中使用的前提，因此可能忽略其他有效的前提，这些前提可以用于与真实证明不同的替代正确证明，从而导致假阴性。最后，定理证明的评估因各种实验设置而复杂化（§
    [6.1](#S6.SS1 "6.1 Challenges ‣ 6 Discussions ‣ A Survey on Deep Learning for
    Theorem Proving")）。
- en: 6 Discussions
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: 6.1 Challenges
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 挑战
- en: Despite significant progress, deep learning for theorem proving still faces
    many challenges, including data scarcity, disunified evaluation protocols, and
    human-AI interaction.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了显著进展，但深度学习在定理证明方面仍面临许多挑战，包括数据稀缺、评估协议不统一和人机互动。
- en: Data scarcity. The amount of formal proof data is growing but is still far behind
    other domains where LLMs are successful, e.g., code generation. The largest corpora
    of Isabelle proofs, Archive of Formal Proofs, currently contains 250K proofs.
    Lean’s mathlib contains 140K proofs. This amount of data is decent for small models
    (e.g., billions of parameters) but insufficient for models with hundreds of billions
    of parameters. Although the use of rule-based generators could offer some assistance,
    the complexity and quality of the generated data often do not match that of human-written
    ones. Furthermore, autoformalization is even more data-scarce, due to the difficulty
    in obtaining aligned informal-formal pairs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据稀缺。形式证明数据量虽然在增长，但仍远远落后于其他LLM成功的领域，例如代码生成。最大的Isabelle证明语料库，Formal Proofs Archive，目前包含250K个证明。Lean的mathlib包含140K个证明。这些数据量对于小型模型（例如，数十亿参数）来说是相当的，但对于拥有数百亿参数的模型则不够。虽然基于规则的生成器可以提供一些帮助，但生成数据的复杂性和质量通常无法与人工编写的匹配。此外，由于难以获得对齐的非正式-正式对，自动形式化的数据更加稀缺。
- en: 'Evaluation. Compared to traditional deep learning tasks such as classification,
    it is much harder to evaluate the performance of theorem provers comprehensively.
    First, results across different proof assistants are not comparable. Even though
    MiniF2F is available across multiple proof assistants, the impact due to proof
    automation (e.g., Isabelle has Sledgehammer (Böhme & Nipkow, [2010](#bib.bib16))
    whereas Lean does not) often outweighs the differences due to deep learning models.
    Second, the resource constraints (e.g., time, #attempts) during evaluation may
    impact the performance and the relative ranking of different methods. Compared
    to time constraints, #attempts favors bigger models such as LLMs. In contrast,
    a tight time constraint arising from real-time applications may favor simple but
    fast models. Without a specific application as the context, it is unclear what
    evaluation setting makes the most sense. We as a community still lack a systematic
    understanding of the trade-off, despite nascent efforts (Rute et al., [2024](#bib.bib149)).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 评估。与传统的深度学习任务（如分类）相比，全面评估定理证明者的性能要困难得多。首先，不同证明助手之间的结果不可比较。即使MiniF2F在多个证明助手中可用，由于证明自动化（例如，Isabelle有Sledgehammer（Böhme
    & Nipkow，[2010](#bib.bib16)），而Lean没有）的影响往往超过了深度学习模型之间的差异。其次，评估过程中的资源限制（例如，时间、尝试次数）可能会影响性能和不同方法的相对排名。与时间限制相比，尝试次数更有利于较大的模型，如LLMs。相比之下，实时应用中的紧迫时间限制可能更倾向于简单但快速的模型。在没有特定应用背景的情况下，不清楚哪个评估设置最合理。尽管已有初步努力（Rute等，[2024](#bib.bib149)），但我们作为一个社区仍然缺乏对这种权衡的系统性理解。
- en: 'Human-AI interaction. One motivation for theorem proving is to assist human
    mathematicians (Castelvecchi, [2021](#bib.bib21); Sørensen et al., [2021](#bib.bib160)).
    However, existing research has led to surprisingly few tools useful for them.
    Current methods are evaluated following standard deep learning protocols: running
    neural networks or querying LLMs in a Python program, testing the prover on a
    dataset, and calculating the percentage of successfully proved ones. This practice
    is misaligned with the needs of mathematicians. First, mathematicians need a tool
    that can be called easily in a proof assistant. Second, the tool must run on consumer
    CPUs with low latency. Third, instead of a performance measure, mathematicians
    care more about whether the prover can help with the specific theorems they are
    working on. These theorems are often out of the training distribution and pose
    a challenge for the prover to generalize to other domains. Although initial efforts
    have been made to develop user-oriented tools (Song et al., [2023](#bib.bib159);
    Welleck & Saha, [2023](#bib.bib188)), there is abundant room in improving the
    user experience and exploring other forms of interaction, which requires close
    collaboration between deep learning researchers and mathematicians (Collins et al.,
    [2023](#bib.bib31)).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能与人类的互动。定理证明的一个动机是协助人类数学家（Castelvecchi, [2021](#bib.bib21)；Sørensen et al.,
    [2021](#bib.bib160)）。然而，现有的研究却意外地导致了很少有对他们有用的工具。目前的方法是按照标准深度学习协议进行评估：在Python程序中运行神经网络或查询大语言模型，测试证明器在数据集上的表现，并计算成功证明的百分比。这种做法与数学家的需求不匹配。首先，数学家需要一个可以在证明助手中轻松调用的工具。其次，该工具必须在消费级CPU上低延迟运行。第三，数学家更关心的是证明器是否能帮助他们解决具体的定理，而不是性能指标。这些定理通常超出训练分布，并对证明器在其他领域的推广提出挑战。尽管已经做出初步努力开发面向用户的工具（Song
    et al., [2023](#bib.bib159)；Welleck & Saha, [2023](#bib.bib188)），但在改善用户体验和探索其他形式的互动方面仍有大量空间，这需要深度学习研究人员和数学家之间的密切合作（Collins
    et al., [2023](#bib.bib31)）。
- en: 6.2 Future Directions
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 未来方向
- en: 'Combining deep learning, especially LLMs, with formal mathematics provides
    a promising avenue for enhancing the math capabilities of AI and may significantly
    impact various disciplines. We conclude our survey paper by listing a few future
    directions we are particularly excited about, envisioning significant strides
    in these burgeoning domains:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度学习，特别是大语言模型（LLMs），与形式数学结合，为提升人工智能的数学能力提供了一个有前景的途径，并可能对各个学科产生重大影响。我们通过列举一些我们特别感兴趣的未来方向来结束我们的调查论文，展望这些新兴领域的重大进展：
- en: Conjecturing. Beyond merely proving theorems, mathematicians would always explore
    theories in a domain, identify underlying problem structures, and formulate new
    conjectures. These explorative activities around conjecturing are indispensable
    for mathematicians but are currently limited in AI (Urban & Jakubüv, [2020](#bib.bib176);
    Johansson & Smallbone, [2023](#bib.bib80)). By enabling AI to conjecture, it can
    explore the space of mathematics autonomously. Furthermore, when combined with
    theorem proving, such exploration can be used to discover new math knowledge.
    A direct application of conjecturing is to generate useful theorems and proofs,
    which could mitigate the data scarcity inherent in theorem proving.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 推测。除了仅仅证明定理之外，数学家们总是会探索某个领域中的理论，识别潜在的问题结构，并提出新的猜想。这些围绕猜想的探索活动对数学家来说是不可或缺的，但目前在人工智能中仍然有限（Urban
    & Jakubüv, [2020](#bib.bib176)；Johansson & Smallbone, [2023](#bib.bib80)）。通过使人工智能能够进行推测，它可以自主探索数学空间。此外，当与定理证明结合时，这种探索可以用于发现新的数学知识。推测的直接应用是生成有用的定理和证明，这可以缓解定理证明中固有的数据稀缺问题。
- en: Verified code generation. As AI coding assistants such as GitHub Copilot become
    prevalent, it is increasingly important to be able to verify LLM-generated code.
    Proof assistants, especially Coq, have been widely used for software verification (Leroy
    et al., [2016](#bib.bib102)). Therefore, methods for theorem proving surveyed
    in this paper may play a role in generating verified code. There is a plethora
    of problems to explore in this space. For example, one can prompt LLMs to synthesize
    inductive loop invariants (Kamath et al., [2023](#bib.bib87)) or generate programs
    in verification-friendly languages such as Dafny (Sun et al., [2023](#bib.bib163)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 验证代码生成。随着AI编码助手如GitHub Copilot的普及，验证LLM生成的代码变得越来越重要。证明助手，特别是Coq，已被广泛用于软件验证（Leroy
    et al.，[2016](#bib.bib102)）。因此，本文调查的定理证明方法可能在生成验证代码中发挥作用。在这个领域有大量问题值得探索。例如，可以提示LLMs合成归纳循环不变式（Kamath
    et al.，[2023](#bib.bib87)）或生成如Dafny（Sun et al.，[2023](#bib.bib163)）等适合验证的语言中的程序。
- en: Math education. A roadblock to democratizing math education is the lack of qualified
    tutors to provide feedback to students (Liang et al., [2023b](#bib.bib108)). Formal
    mathematics can potentially mitigate the problem by providing an environment for
    students to explore and receive automatic and reliable feedback. AI has demonstrated
    promise in guiding students in this process. For example, Kevin Buzzard reported
    that Lean Copilot could help prove a number of theorems in his undergraduate course
    and answer questions on Lean Zulip (Buzzard, [2024](#bib.bib20)). To further integrate
    AI-driven formal tutoring into mainstream education, informalization is also essential
    to make these formal proofs accessible to students who are not familiar with formal
    languages. Looking ahead, we believe that LLMs and theorem proving could lead
    to intelligent tutors in a variety of STEM classes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数学教育。民主化数学教育的一个障碍是缺乏合格的导师来为学生提供反馈（Liang et al.，[2023b](#bib.bib108)）。正式数学可以通过提供一个环境来减轻这个问题，使学生能够探索并获得自动且可靠的反馈。AI在引导学生这一过程中展现了潜力。例如，Kevin
    Buzzard报告称，Lean Copilot可以帮助证明他本科课程中的多个定理，并回答关于Lean Zulip的问题（Buzzard，[2024](#bib.bib20)）。为了进一步将AI驱动的正式辅导整合到主流教育中，非正式化也是必要的，以使这些正式证明对不熟悉正式语言的学生可访问。展望未来，我们相信LLMs和定理证明可以在各种STEM课程中带来智能辅导员。
- en: References
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abdelaziz et al. (2020) Ibrahim Abdelaziz, Veronika Thost, Maxwell Crouse, and
    Achille Fokoue. An experimental study of formula embeddings for automated theorem
    proving in first-order logic. *arXiv preprint arXiv:2002.00423*, 2020.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelaziz et al. (2020) Ibrahim Abdelaziz, Veronika Thost, Maxwell Crouse, 和
    Achille Fokoue。第一阶逻辑中公式嵌入的实验研究。*arXiv预印本 arXiv:2002.00423*，2020年。
- en: Abdelaziz et al. (2022) Ibrahim Abdelaziz, Maxwell Crouse, Bassem Makni, Vernon
    Austel, Cristina Cornelio, Shajith Ikbal, Pavan Kapanipathi, Ndivhuwo Makondo,
    Kavitha Srinivas, Michael Witbrock, et al. Learning to guide a saturation-based
    theorem prover. *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2022.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelaziz et al. (2022) Ibrahim Abdelaziz, Maxwell Crouse, Bassem Makni, Vernon
    Austel, Cristina Cornelio, Shajith Ikbal, Pavan Kapanipathi, Ndivhuwo Makondo,
    Kavitha Srinivas, Michael Witbrock, 等人。学习引导基于饱和的定理证明器。*IEEE模式分析与机器智能学报*，2022年。
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. GPT-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, 等人。GPT-4技术报告。*arXiv预印本 arXiv:2303.08774*，2023年。
- en: Agrawal et al. (2022) Ayush Agrawal, Siddhartha Gadgil, Navin Goyal, Ashvni
    Narayanan, and Anand Tadipatri. Towards a mathematics formalisation assistant
    using large language models. *arXiv preprint arXiv:2211.07524*, 2022.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agrawal et al. (2022) Ayush Agrawal, Siddhartha Gadgil, Navin Goyal, Ashvni
    Narayanan, 和 Anand Tadipatri。使用大型语言模型的数学形式化助手。*arXiv预印本 arXiv:2211.07524*，2022年。
- en: Alama et al. (2014) Jesse Alama, Tom Heskes, Daniel Kühlwein, Evgeni Tsivtsivadze,
    and Josef Urban. Premise selection for mathematics by corpus analysis and kernel
    methods. *Journal of Automated Reasoning*, 2014.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alama et al. (2014) Jesse Alama, Tom Heskes, Daniel Kühlwein, Evgeni Tsivtsivadze,
    和 Josef Urban。通过语料库分析和核方法进行数学前提选择。*自动推理学报*，2014年。
- en: Aygün et al. (2020) Eser Aygün, Zafarali Ahmed, Ankit Anand, Vlad Firoiu, Xavier
    Glorot, Laurent Orseau, Doina Precup, and Shibl Mourad. Learning to prove from
    synthetic theorems. *arXiv preprint arXiv:2006.11259*, 2020.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aygün et al. (2020) 埃塞尔·阿尤根、扎法拉利·艾哈迈德、安基特·安纳德、弗拉德·菲罗伊、泽维尔·格洛罗、洛朗·奥尔索、多伊娜·普雷库普和希布尔·穆拉德。从合成定理中学习证明。*arXiv
    预印本 arXiv:2006.11259*，2020年。
- en: Aygün et al. (2022) Eser Aygün, Ankit Anand, Laurent Orseau, Xavier Glorot,
    Stephen M Mcaleer, Vlad Firoiu, Lei M Zhang, Doina Precup, and Shibl Mourad. Proving
    theorems using incremental learning and hindsight experience replay. In *Proceedings
    of the 39th International Conference on Machine Learning*, 2022.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aygün et al. (2022) 埃塞尔·阿尤根、安基特·安纳德、洛朗·奥尔索、泽维尔·格洛罗、斯蒂芬·M·麦卡利尔、弗拉德·菲罗伊、雷·M·张、多伊娜·普雷库普和希布尔·穆拉德。使用增量学习和后见经验回放证明定理。见于
    *第39届国际机器学习大会论文集*，2022年。
- en: 'Azerbayev et al. (2023) Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf,
    Edward W Ayers, Dragomir Radev, and Jeremy Avigad. ProofNet: Autoformalizing and
    formally proving undergraduate-level mathematics. *arXiv preprint arXiv:2302.12433*,
    2023.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Azerbayev et al. (2023) 张吉尔·阿泽尔巴耶夫、巴尔托什·皮奥特罗夫斯基、海莉·舍尔科普夫、爱德华·W·艾尔斯、德拉戈米尔·拉德夫和杰里米·阿维戈德。ProofNet:
    自动形式化和形式证明本科水平的数学。*arXiv 预印本 arXiv:2302.12433*，2023年。'
- en: 'Azerbayev et al. (2024) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster,
    Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman,
    and Sean Welleck. Llemma: An open language model for mathematics. In *The Twelfth
    International Conference on Learning Representations*, 2024.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Azerbayev et al. (2024) 张吉尔·阿泽尔巴耶夫、海莉·舍尔科普夫、基兰·帕斯特、马尔科·多斯·桑托斯、斯蒂芬·麦卡利尔、阿尔伯特·Q·姜、贾·邓、斯特拉·比德曼和肖恩·维莱克。Llemma:
    一个开放的数学语言模型。见于 *第十二届国际学习表征会议*，2024年。'
- en: Bansal & Szegedy (2020) Kshitij Bansal and Christian Szegedy. Learning alignment
    between formal & informal mathematics. In *5th Conference on Artificial Intelligence
    and Theorem Proving*, 2020.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bansal & Szegedy (2020) 克什提吉·班萨尔和克里斯蒂安·塞格迪。学习形式数学与非形式数学之间的对齐。见于 *第五届人工智能与定理证明会议*，2020年。
- en: 'Bansal et al. (2019) Kshitij Bansal, Sarah M. Loos, Markus Norman Rabe, Christian
    Szegedy, and Stewart Wilcox. HOList: An environment for machine learning of higher
    order logic theorem proving. In *Proceedings of the 36th International Conference
    on Machine Learning*, 2019.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bansal et al. (2019) 克什提吉·班萨尔、萨拉赫·M·卢斯、马库斯·诺曼·拉比、克里斯蒂安·塞格迪和斯图尔特·威尔科克斯。HOList:
    用于高阶逻辑定理证明的机器学习环境。见于 *第36届国际机器学习大会论文集*，2019年。'
- en: Barras et al. (1999) Bruno Barras, Samuel Boutin, Cristina Cornes, Judicaël
    Courant, Yann Coscoy, David Delahaye, Daniel de Rauglaudre, Jean-Christophe Filliâtre,
    Eduardo Giménez, Hugo Herbelin, et al. The Coq proof assistant reference manual.
    *INRIA*, 1999.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barras et al. (1999) 布鲁诺·巴拉斯、塞缪尔·布廷、克里斯蒂娜·科尔内斯、朱迪凯尔·库朗、扬·科斯科伊、大卫·德拉耶、丹尼尔·德·罗格劳德、让-克里斯托夫·菲利亚特、爱德华多·吉门内斯、雨果·赫尔贝林等。Coq证明助手参考手册。*INRIA*，1999年。
- en: Bártek & Suda (2023) Filip Bártek and Martin Suda. How much should this symbol
    weigh? a gnn-advised clause selection. In *Proceedings of 24th International Conference
    on Logic for Programming, Artificial Intelligence and Reasoning*, 2023.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bártek & Suda (2023) 菲利普·巴尔特克和马丁·苏达。这个符号应该重多少？一个GNN建议的子句选择。见于 *第24届国际编程逻辑、人工智能和推理会议论文集*，2023年。
- en: 'Bauer et al. (2023) Andrej Bauer, Matej Petković, and Ljupco Todorovski. MLFMF:
    Data sets for machine learning for mathematical formalization. In *Thirty-seventh
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track*,
    2023.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bauer et al. (2023) 安德烈·鲍尔、马特杰·佩特科维奇和柳普科·托多罗夫斯基。MLFMF: 用于数学形式化的机器学习数据集。见于 *第37届神经信息处理系统大会数据集与基准跟踪*，2023年。'
- en: Blanchette et al. (2016) Jasmin Christian Blanchette, Cezary Kaliszyk, Lawrence C
    Paulson, and Josef Urban. Hammering towards QED. *Journal of Formalized Reasoning*,
    2016.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blanchette et al. (2016) 贾斯敏·克里斯蒂安·布兰切特、切扎里·卡利斯齐克、劳伦斯·C·保尔森和约瑟夫·厄尔班。朝着QED前进。*形式化推理期刊*，2016年。
- en: 'Böhme & Nipkow (2010) Sascha Böhme and Tobias Nipkow. Sledgehammer: Judgement
    day. In *Proceedings of the 5th International Joint Conference on Automated Reasoning*,
    2010.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Böhme & Nipkow (2010) 萨沙·贝默和托比亚斯·尼普科。Sledgehammer: 最终审判。见于 *第五届国际联合自动推理会议论文集*，2010年。'
- en: Brandfonbrener et al. (2024) David Brandfonbrener, Sibi Raja, Tarun Prasad,
    Chloe Loughridge, Jianang Yang, Simon Henniger, William E Byrd, Robert Zinkov,
    and Nada Amin. Verified multi-step synthesis using large language models and monte
    carlo tree search. *arXiv preprint arXiv:2402.08147*, 2024.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brandfonbrener et al. (2024) 大卫·布兰德丰布伦、锡比·拉贾、塔伦·普拉萨德、克洛伊·劳里奇、贾南·杨、西蒙·亨尼格、威廉·E·伯德、罗伯特·津科夫和纳达·阿敏。使用大型语言模型和蒙特卡洛树搜索进行经过验证的多步骤合成。*arXiv
    预印本 arXiv:2402.08147*，2024年。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. In *Proceedings of the 34th
    International Conference on Neural Information Processing Systems*, 2020.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等人。语言模型是少样本学习者。见于 *第34届国际神经信息处理系统大会论文集*，2020。
- en: Bruijn, de (1970) N.G. Bruijn, de. The mathematical language AUTOMATH, its usage
    and some of its extensions. In *Proceedings Symposium on Automatic Demonstration*,
    1970.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruijn, de (1970) N.G. Bruijn, de。数学语言AUTOMATH，它的使用及其一些扩展。见于 *自动证明研讨会论文集*，1970。
- en: Buzzard (2024) Kevin Buzzard. Lean in 2024. [https://xenaproject.wordpress.com/2024/01/20/lean-in-2024/](https://xenaproject.wordpress.com/2024/01/20/lean-in-2024/),
    2024.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buzzard (2024) Kevin Buzzard。2024年的Lean。 [https://xenaproject.wordpress.com/2024/01/20/lean-in-2024/](https://xenaproject.wordpress.com/2024/01/20/lean-in-2024/)，2024。
- en: Castelvecchi (2021) Davide Castelvecchi. Mathematicians welcome computer-assisted
    proof in ‘grand unification’ theory. *Nature*, 2021.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castelvecchi (2021) Davide Castelvecchi。数学家欢迎计算机辅助证明的“宏大统一”理论。*自然*，2021。
- en: 'Chen et al. (2022) Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu
    Chen, and Xiaodan Liang. UniGeo: Unifying geometry logical reasoning via reformulating
    mathematical expression. In *Proceedings of the 2022 Conference on Empirical Methods
    in Natural Language Processing*, 2022.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2022) Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu
    Chen 和 Xiaodan Liang。UniGeo：通过重新表述数学表达统一几何逻辑推理。见于 *2022年自然语言处理经验方法会议论文集*，2022。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman 等人。评估基于代码训练的大型语言模型。*arXiv 预印本 arXiv:2107.03374*，2021。
- en: Chen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey
    Hinton. A simple framework for contrastive learning of visual representations.
    In *Proceedings of the 37th International Conference on Machine Learning*, 2020.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi 和 Geoffrey Hinton。视觉表示对比学习的简单框架。见于
    *第37届国际机器学习大会论文集*，2020。
- en: Cho et al. (2014) Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using RNN encoder–decoder for statistical machine translation. In *Proceedings
    of the 2014 Conference on Empirical Methods in Natural Language Processing*, 2014.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等人 (2014) Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk 和 Yoshua Bengio。使用 RNN 编码器-解码器学习短语表示用于统计机器翻译。见于
    *2014年自然语言处理经验方法会议论文集*，2014。
- en: Chou et al. (2000) Shang-Ching Chou, Xiao-Shan Gao, and Jing-Zhong Zhang. A
    deductive database approach to automated geometry theorem proving and discovering.
    *Journal of Automated Reasoning*, 2000.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chou 等人 (2000) Shang-Ching Chou, Xiao-Shan Gao 和 Jing-Zhong Zhang。基于演绎数据库的方法进行自动几何定理证明和发现。*自动推理期刊*，2000。
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. *Journal
    of Machine Learning Research*, 2023.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等人 (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann 等人。PaLM：通过路径扩展语言建模。*机器学习研究期刊*，2023。
- en: 'Chvalovskỳ et al. (2019) Karel Chvalovskỳ, Jan Jakubüv, Martin Suda, and Josef
    Urban. ENIGMA-NG: Efficient neural and gradient-boosted inference guidance for
    E. In *27th International Conference on Automated Deduction*, 2019.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chvalovskỳ 等人 (2019) Karel Chvalovskỳ, Jan Jakubüv, Martin Suda 和 Josef Urban。ENIGMA-NG：高效的神经和梯度提升推理指导。见于
    *第27届自动推理国际会议*，2019。
- en: Chvalovskỳ et al. (2021) Karel Chvalovskỳ, Jan Jakubüv, Miroslav Olšák, and
    Josef Urban. Learning theorem proving components. In *30th International Conference
    on Automated Reasoning with Analytic Tableaux and Related Methods*, 2021.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chvalovskỳ 等人 (2021) Karel Chvalovskỳ, Jan Jakubüv, Miroslav Olšák 和 Josef Urban。学习定理证明组件。见于
    *第30届自动推理与解析表格及相关方法国际会议*，2021。
- en: Chvalovskỳ et al. (2023) Karel Chvalovskỳ, Konstantin Korovin, Jelle Piepenbrock,
    and Josef Urban. Guiding an instantiation prover with graph neural networks. In
    *Proceedings of 24th International Conference on Logic for Programming, Artificial
    Intelligence and Reasoning*, 2023.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chvalovskỳ et al. (2023) 卡雷尔·赫瓦洛夫斯基、康斯坦丁·科罗文、杰勒·皮彭布罗克 和 约瑟夫·厄尔班。通过图神经网络引导实例化证明器。在
    *第 24 届逻辑编程、人工智能与推理国际会议论文集*，2023 年。
- en: Collins et al. (2023) Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel
    Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum,
    William Hart, et al. Evaluating language models for mathematics through interactions.
    *arXiv preprint arXiv:2306.01694*, 2023.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collins et al. (2023) 凯瑟琳·M·柯林斯、阿尔伯特·Q·姜、西蒙·弗里德、利昂内尔·黄、米里·齐尔卡、乌曼·巴特、托马斯·卢卡谢维奇、余怀·吴、乔舒亚·B·特南鲍姆、威廉·哈特
    等人。通过交互评估语言模型在数学领域的表现。*arXiv 预印本 arXiv:2306.01694*，2023 年。
- en: Crouse et al. (2019) Maxwell Crouse, Ibrahim Abdelaziz, Cristina Cornelio, Veronika
    Thost, Lingfei Wu, Kenneth Forbus, and Achille Fokoue. Improving graph neural
    network representations of logical formulae with subgraph pooling. *arXiv preprint
    arXiv:1911.06904*, 2019.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Crouse et al. (2019) 麦克斯韦·克劳斯、易卜拉欣·阿卜杜拉齐兹、克里斯蒂娜·科尔内利奥、维罗尼卡·托斯特、林飞·吴、肯尼斯·福布斯
    和 阿基尔·福库。通过子图池化改进图神经网络对逻辑公式的表示。*arXiv 预印本 arXiv:1911.06904*，2019 年。
- en: Crouse et al. (2021) Maxwell Crouse, Ibrahim Abdelaziz, Bassem Makni, Spencer
    Whitehead, Cristina Cornelio, Pavan Kapanipathi, Kavitha Srinivas, Veronika Thost,
    Michael Witbrock, and Achille Fokoue. A deep reinforcement learning approach to
    first-order logic theorem proving. In *Proceedings of the Thirty-Fifth AAAI Conference
    on Artificial Intelligence*, 2021.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Crouse et al. (2021) 麦克斯韦·克劳斯、易卜拉欣·阿卜杜拉齐兹、巴塞姆·马克尼、斯宾塞·怀特黑德、克里斯蒂娜·科尔内利奥、帕万·卡帕尼帕西、卡维塔·斯里尼瓦斯、维罗尼卡·托斯特、迈克尔·维特布罗克
    和 阿基尔·福库。一种深度强化学习方法用于一阶逻辑定理证明。在 *第三十五届 AAAI 人工智能大会论文集*，2021 年。
- en: 'Cunningham et al. (2022) Garett Cunningham, Razvan C Bunescu, and David Juedes.
    Towards autoformalization of mathematics and code correctness: Experiments with
    elementary proofs. In *Proceedings of the 1st Workshop on Mathematical Natural
    Language Processing*, 2022.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cunningham et al. (2022) 加雷特·坎宁安、拉兹万·C·布内斯库 和 大卫·朱德斯。迈向数学和代码正确性的自动形式化：初步证明的实验。在
    *第 1 届数学自然语言处理研讨会* 论文集，2022 年。
- en: 'Czajka & Kaliszyk (2018) Łukasz Czajka and Cezary Kaliszyk. Hammer for Coq:
    Automation for dependent type theory. *Journal of automated reasoning*, 2018.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czajka & Kaliszyk (2018) 卢卡什·察伊卡 和 塞扎里·卡利希克。Coq 的 Hammer：依赖类型理论的自动化。*自动推理期刊*，2018
    年。
- en: 'Dastgheib & Asgari (2022) Doratossadat Dastgheib and Ehsaneddin Asgari. Keyword-based
    natural language premise selection for an automatic mathematical statement proving.
    In *Proceedings of TextGraphs-16: Graph-based Methods for Natural Language Processing*,
    2022.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dastgheib & Asgari (2022) 多拉托萨达特·达斯特赫伊布 和 埃赫桑丁·阿斯加里。基于关键词的自然语言前提选择用于自动数学陈述证明。在
    *TextGraphs-16: 基于图的方法在自然语言处理中的应用* 论文集，2022 年。'
- en: Davis (1957) Martin Davis. A computer program for Presburger’s algorithm. *Symbolic
    Computation Automation of Reasoning 1*, 1957.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davis (1957) 马丁·戴维斯。一个用于 Presburger 算法的计算机程序。*符号计算 1*，1957 年。
- en: Davis & Putnam (1960) Martin Davis and Hilary Putnam. A computing procedure
    for quantification theory. *Journal of the ACM*, 1960.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davis & Putnam (1960) 马丁·戴维斯 和 希拉里·普特南。量化理论的计算程序。*ACM 期刊*，1960 年。
- en: 'Denzinger et al. (1999) Jörg Denzinger, Matthias Fuchs, Christoph Goller, and
    Stephan Schulz. Learning from previous proof experience: A survey. *Technical
    Report*, 1999.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denzinger et al. (1999) 约尔格·登青格、马蒂亚斯·福克斯、克里斯托夫·戈勒 和 斯特凡·舒尔茨。从以前的证明经验中学习：综述。*技术报告*，1999
    年。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies*,
    2019.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2019) 雅各布·德夫林、明伟·张、肯顿·李 和 克里斯蒂娜·托塔诺瓦。BERT：用于语言理解的深度双向变换器的预训练。在
    *2019 年北美计算语言学协会会议：人类语言技术* 论文集，2019 年。
- en: Duvenaud et al. (2015) David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre,
    Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional
    networks on graphs for learning molecular fingerprints. In *Proceedings of the
    28th International Conference on Neural Information Processing Systems*, 2015.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duvenaud et al. (2015) 大卫·K·杜文诺德、道格·麦克劳林、豪尔赫·伊帕拉吉雷、拉斐尔·博姆巴雷尔、蒂莫西·赫尔泽尔、阿兰·阿斯普鲁-古兹克
    和 瑞安·P·亚当斯。用于学习分子指纹的图上的卷积网络。在 *第 28 届国际神经信息处理系统大会论文集*，2015 年。
- en: Fawzi et al. (2019) Alhussein Fawzi, Mateusz Malinowski, Hamza Fawzi, and Omar
    Fawzi. Learning dynamic polynomial proofs. In *Proceedings of the 33rd International
    Conference on Neural Information Processing Systems*, 2019.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fawzi 等人 (2019) **Alhussein Fawzi**、**Mateusz Malinowski**、**Hamza Fawzi** 和
    **Omar Fawzi**。学习动态多项式证明。发表于 *第33届国际神经信息处理系统会议论文集*，2019。
- en: 'Ferreira & Freitas (2020a) Deborah Ferreira and André Freitas. Natural language
    premise selection: Finding supporting statements for mathematical text. In *Proceedings
    of the Twelfth Language Resources and Evaluation Conference*, 2020a.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira & Freitas (2020a) **Deborah Ferreira** 和 **André Freitas**。自然语言前提选择：为数学文本寻找支持陈述。发表于
    *第十二届语言资源与评估会议论文集*，2020a。
- en: Ferreira & Freitas (2020b) Deborah Ferreira and André Freitas. Premise selection
    in natural language mathematical texts. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics*, 2020b.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira & Freitas (2020b) **Deborah Ferreira** 和 **André Freitas**。自然语言数学文本中的前提选择。发表于
    *第58届计算语言学协会年会论文集*，2020b。
- en: 'Ferreira & Freitas (2021) Deborah Ferreira and André Freitas. STAR: Cross-modal
    [STA]tement [R]epresentation for selecting relevant mathematical premises. In
    *Proceedings of the 16th Conference of the European Chapter of the Association
    for Computational Linguistic*, 2021.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira & Freitas (2021) **Deborah Ferreira** 和 **André Freitas**。STAR：跨模态
    [STA]tement [R]epresentation 用于选择相关数学前提。发表于 *第16届欧洲计算语言学协会会议论文集*，2021。
- en: Firoiu et al. (2021) Vlad Firoiu, Eser Aygun, Ankit Anand, Zafarali Ahmed, Xavier
    Glorot, Laurent Orseau, Lei Zhang, Doina Precup, and Shibl Mourad. Training a
    first-order theorem prover from synthetic data. In *9th International Conference
    on Learning Representations Workshop on Mathematical Reasoning in General Artificial
    Intelligence*, 2021.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Firoiu 等人 (2021) **Vlad Firoiu**、**Eser Aygun**、**Ankit Anand**、**Zafarali Ahmed**、**Xavier
    Glorot**、**Laurent Orseau**、**Lei Zhang**、**Doina Precup** 和 **Shibl Mourad**。从合成数据中训练一阶定理证明器。发表于
    *第9届国际学习表征会议数学推理研讨会*，2021。
- en: First & Brun (2022) Emily First and Yuriy Brun. Diversity-driven automated formal
    verification. In *Proceedings of the 44th International Conference on Software
    Engineering*, 2022.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: First & Brun (2022) **Emily First** 和 **Yuriy Brun**。基于多样性驱动的自动形式验证。发表于 *第44届国际软件工程会议论文集*，2022。
- en: 'First et al. (2020) Emily First, Yuriy Brun, and Arjun Guha. TacTok: Semantics-aware
    proof synthesis. *Proceedings of the ACM on Programming Languages*, 2020.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: First 等人 (2020) **Emily First**、**Yuriy Brun** 和 **Arjun Guha**。TacTok：语义感知的证明合成。*ACM
    编程语言期刊论文集*，2020。
- en: 'First et al. (2023) Emily First, Markus Rabe, Talia Ringer, and Yuriy Brun.
    Baldur: Whole-proof generation and repair with large language models. In *Proceedings
    of the 31st ACM Joint European Software Engineering Conference and Symposium on
    the Foundations of Software Engineering*, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: First 等人 (2023) **Emily First**、**Markus Rabe**、**Talia Ringer** 和 **Yuriy Brun**。Baldur：基于大型语言模型的全证明生成与修复。发表于
    *第31届 ACM 欧洲联合软件工程会议与软件工程基础研讨会*，2023。
- en: Fokoue et al. (2023) Achille Fokoue, Ibrahim Abdelaziz, Maxwell Crouse, Shajith
    Ikbal, Akihiro Kishimoto, Guilherme Lima, Ndivhuwo Makondo, and Radu Marinescu.
    An ensemble approach for automated theorem proving based on efficient name invariant
    graph neural representations. In *Proceedings of the Thirty-Second International
    Joint Conference on Artificial Intelligence*, 2023.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fokoue 等人 (2023) **Achille Fokoue**、**Ibrahim Abdelaziz**、**Maxwell Crouse**、**Shajith
    Ikbal**、**Akihiro Kishimoto**、**Guilherme Lima**、**Ndivhuwo Makondo** 和 **Radu
    Marinescu**。一种基于高效名称不变图神经表示的自动定理证明集成方法。发表于 *第三十二届国际人工智能联合会议论文集*，2023。
- en: Frieder et al. (2023a) Simon Frieder, Julius Berner, Philipp Petersen, and Thomas
    Lukasiewicz. Large language models for mathematicians. *arXiv preprint arXiv:2312.04556*,
    2023a.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frieder 等人 (2023a) **Simon Frieder**、**Julius Berner**、**Philipp Petersen**
    和 **Thomas Lukasiewicz**。为数学家设计的大型语言模型。*arXiv 预印本 arXiv:2312.04556*，2023a。
- en: Frieder et al. (2023b) Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys
    Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen,
    and Julius Berner. Mathematical capabilities of ChatGPT. In *Thirty-seventh Conference
    on Neural Information Processing Systems Datasets and Benchmarks Track*, 2023b.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frieder 等人 (2023b) **Simon Frieder**、**Luca Pinchetti**、**Alexis Chevalier**、**Ryan-Rhys
    Griffiths**、**Tommaso Salvatori**、**Thomas Lukasiewicz**、**Philipp Christian Petersen**
    和 **Julius Berner**。ChatGPT 的数学能力。发表于 *第三十七届神经信息处理系统会议数据集与基准轨道*，2023b。
- en: Frieder et al. (2023c) Simon Frieder, Martin Trimmel, Rashid Alawadhi, and Klaus
    Gy. LLM vs ITP. In *37th Conference on Neural Information Processing Systems Workshop
    on MATH-AI*, 2023c.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frieder et al. (2023c) Simon Frieder, Martin Trimmel, Rashid Alawadhi 和 Klaus
    Gy。LLM 与 ITP。在 *第三十七届神经信息处理系统会议 MATH-AI 研讨会*，2023年。
- en: Gadgil et al. (2022) Siddhartha Gadgil, Anand Rao Tadipatri, Ayush Agrawal,
    Ashvni Narayanan, and Navin Goyal. Towards automating formalisation of theorem
    statements using large language models. In *36th Conference on Neural Information
    Processing Systems Workshop on MATH-AI*, 2022.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gadgil et al. (2022) Siddhartha Gadgil, Anand Rao Tadipatri, Ayush Agrawal,
    Ashvni Narayanan 和 Navin Goyal。利用大型语言模型自动化定理陈述的形式化。在 *第三十六届神经信息处理系统会议 MATH-AI
    研讨会*，2022年。
- en: Gauthier (2020) Thibault Gauthier. Deep reinforcement learning for synthesizing
    functions in higher-order logic. In *Proceedings of 23rd International Conference
    on Logic for Programming, Artificial Intelligence and Reasoning*, 2020.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gauthier (2020) Thibault Gauthier。用于高阶逻辑中函数合成的深度强化学习。在 *第23届国际编程逻辑、人工智能与推理会议论文集*，2020年。
- en: Gauthier (2021) Thibault Gauthier. Learned provability likelihood for tactical
    search. In *Proceedings of the 9th International Symposium on Symbolic Computation
    in Software Science*, 2021.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gauthier (2021) Thibault Gauthier。战术搜索的可学习证明可能性。在 *第九届国际符号计算与软件科学研讨会论文集*，2021年。
- en: 'Gauthier et al. (2020) Thibault Gauthier, Cezary Kaliszyk, and Josef Urban.
    TacticToe: Learning to prove with tactics. *Journal of Automated Reasoning*, 2020.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gauthier et al. (2020) Thibault Gauthier, Cezary Kaliszyk 和 Josef Urban。TacticToe：通过策略学习证明。
    *自动推理杂志*，2020年。
- en: Gloeckle et al. (2023) Fabian Gloeckle, Baptiste Roziere, Amaury Hayat, and
    Gabriel Synnaeve. Temperature-scaled large language models for Lean proofstep
    prediction. In *37th Conference on Neural Information Processing Systems Workshop
    on MATH-AI*, 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gloeckle et al. (2023) Fabian Gloeckle, Baptiste Roziere, Amaury Hayat 和 Gabriel
    Synnaeve。温度缩放的大型语言模型用于 Lean 证明步骤预测。在 *第三十七届神经信息处理系统会议 MATH-AI 研讨会*，2023年。
- en: Goertzel & Urban (2019) Zarathustra Goertzel and Josef Urban. Usefulness of
    lemmas via graph neural networks. In *4th Conference on Artificial Intelligence
    and Theorem Proving*, 2019.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goertzel & Urban (2019) Zarathustra Goertzel 和 Josef Urban。通过图神经网络的引理有效性。在 *第四届人工智能与定理证明会议*，2019年。
- en: Goertzel et al. (2021) Zarathustra A Goertzel, Karel Chvalovskỳ, Jan Jakubüv,
    Miroslav Olšák, and Josef Urban. Fast and slow ENIGMAs and parental guidance.
    In *13th International Symposium on Frontiers of Combining Systems*, 2021.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goertzel et al. (2021) Zarathustra A Goertzel, Karel Chvalovskỳ, Jan Jakubüv,
    Miroslav Olšák 和 Josef Urban。快速与慢速的 ENIGMAs 以及父母指导。在 *第十三届结合系统前沿国际研讨会*，2021年。
- en: Goertzel et al. (2022) Zarathustra A Goertzel, Jan Jakubüv, Cezary Kaliszyk,
    Miroslav Olšák, Jelle Piepenbrock, and Josef Urban. The Isabelle ENIGMA. In *13th
    International Conference on Interactive Theorem Proving*, 2022.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goertzel et al. (2022) Zarathustra A Goertzel, Jan Jakubüv, Cezary Kaliszyk,
    Miroslav Olšák, Jelle Piepenbrock 和 Josef Urban。Isabelle ENIGMA。在 *第十三届国际互动定理证明会议*，2022年。
- en: 'Gonthier (2008) Georges Gonthier. The four colour theorem: Engineering of a
    formal proof. In *8th Asian Symposium on Computer Mathematics*, 2008.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gonthier (2008) Georges Gonthier。四色定理：形式化证明的工程。在 *第八届亚洲计算数学研讨会*，2008年。
- en: Hales et al. (2017) Thomas Hales, Mark Adams, Gertrud Bauer, Tat Dat Dang, John
    Harrison, Hoang Le Truong, Cezary Kaliszyk, Victor Magron, Sean McLaughlin, Tat Thang
    Nguyen, et al. A formal proof of the Kepler conjecture. In *Forum of Mathematics,
    Pi*, 2017.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hales et al. (2017) Thomas Hales, Mark Adams, Gertrud Bauer, Tat Dat Dang, John
    Harrison, Hoang Le Truong, Cezary Kaliszyk, Victor Magron, Sean McLaughlin, Tat
    Thang Nguyen 等。开普勒猜想的形式化证明。在 *数学论坛，Pi*，2017年。
- en: Han et al. (2021) Jesse Michael Han, Tao Xu, Stanislas Polu, Arvind Neelakantan,
    and Alec Radford. Contrastive finetuning of generative language models for informal
    premise selection. In *6th Conference on Artificial Intelligence and Theorem Proving*,
    2021.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2021) Jesse Michael Han, Tao Xu, Stanislas Polu, Arvind Neelakantan
    和 Alec Radford。生成语言模型的对比微调用于非正式前提选择。在 *第六届人工智能与定理证明会议*，2021年。
- en: Han et al. (2022) Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers,
    and Stanislas Polu. Proof artifact co-training for theorem proving with language
    models. In *The Tenth International Conference on Learning Representations*, 2022.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2022) Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers 和
    Stanislas Polu。用于定理证明的语言模型的证明文档协同训练。在 *第十届国际学习表示会议*，2022年。
- en: 'Harrison (1996) John Harrison. HOL Light: A tutorial introduction. In *International
    Conference on Formal Methods in Computer-Aided Design*, 1996.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harrison (1996) John Harrison。HOL Light：教程介绍。在 *国际计算机辅助设计中的形式化方法会议*，1996年。
- en: 'He et al. (2024) Yiming He, Jia Zou, Xiaokai Zhang, Na Zhu, and Tuo Leng. FGeo-TP:
    A language model-enhanced solver for geometry problems. *arXiv preprint arXiv:2402.09047*,
    2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等（2024）Yiming He, Jia Zou, Xiaokai Zhang, Na Zhu, 和 Tuo Leng. **FGeo-TP**：一种增强语言模型的几何问题求解器。*arXiv预印本
    arXiv:2402.09047*，2024年。
- en: Holden & Korovin (2023) Edvard K Holden and Konstantin Korovin. Graph sequence
    learning for premise selection. In *8th Conference on Artificial Intelligence
    and Theorem Proving*, 2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holden & Korovin（2023）Edvard K Holden和Konstantin Korovin. 图序列学习用于前提选择。见于*第8届人工智能与定理证明会议*，2023年。
- en: 'Huang et al. (2019) Daniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever.
    GamePad: A learning environment for theorem proving. In *The Seventh International
    Conference on Learning Representations*, 2019.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等（2019）Daniel Huang, Prafulla Dhariwal, Dawn Song, 和 Ilya Sutskever. **GamePad**：一个用于定理证明的学习环境。见于*第七届国际学习表征会议*，2019年。
- en: 'Huang et al. (2024) Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao,
    Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, and Xiaodan Liang. MUSTARD:
    Mastering uniform synthesis of theorem and proof data. In *The Twelfth International
    Conference on Learning Representations*, 2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等（2024）Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin,
    Haiming Wang, Zhenguo Li, Linqi Song, 和 Xiaodan Liang. **MUSTARD**：掌握定理与证明数据的一致合成。见于*第十二届国际学习表征会议*，2024年。
- en: Irving et al. (2016) Geoffrey Irving, Christian Szegedy, Alexander A Alemi,
    Niklas Eén, François Chollet, and Josef Urban. DeepMath - deep sequence models
    for premise selection. In *Proceedings of the 30th International Conference on
    Neural Information Processing Systems*, 2016.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Irving等（2016）Geoffrey Irving, Christian Szegedy, Alexander A Alemi, Niklas Eén,
    François Chollet, 和 Josef Urban. **DeepMath** - 深度序列模型用于前提选择。见于*第30届国际神经信息处理系统会议论文集*，2016年。
- en: 'Jakubüv & Urban (2017) Jan Jakubüv and Josef Urban. ENIGMA: efficient learning-based
    inference guiding machine. In *10th International Conference on Intelligent Computer
    Mathematics*, 2017.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jakubüv & Urban（2017）Jan Jakubüv和Josef Urban. **ENIGMA**：基于学习的高效推理引导机器。见于*第10届国际智能计算数学会议*，2017年。
- en: Jakubüv & Urban (2019) Jan Jakubüv and Josef Urban. Hammering Mizar by learning
    clause guidance. In *10th International Conference on Interactive Theorem Proving*,
    2019.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jakubüv & Urban（2019）Jan Jakubüv和Josef Urban. 通过学习子句引导来**敲击**Mizar。见于*第10届国际交互定理证明会议*，2019年。
- en: 'Jakubüv et al. (2020) Jan Jakubüv, Karel Chvalovskỳ, Miroslav Olšák, Bartosz
    Piotrowski, Martin Suda, and Josef Urban. ENIGMA anonymous: Symbol-independent
    inference guiding machine (system description). In *Proceedings of the 10th International
    Joint Conference on Automated Reasoning*, 2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jakubüv等（2020）Jan Jakubüv, Karel Chvalovskỳ, Miroslav Olšák, Bartosz Piotrowski,
    Martin Suda, 和 Josef Urban. **ENIGMA**匿名：符号独立推理引导机器（系统描述）。见于*第10届国际自动推理联合会议论文集*，2020年。
- en: Jakubüv et al. (2023) Jan Jakubüv, Karel Chvalovskỳ, Zarathustra Goertzel, Cezary
    Kaliszyk, Mirek Olšák, Bartosz Piotrowski, Stephan Schulz, Martin Suda, and Josef
    Urban. MizAR 60 for Mizar 50. In *14th International Conference on Interactive
    Theorem Proving*, 2023.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jakubüv等（2023）Jan Jakubüv, Karel Chvalovskỳ, Zarathustra Goertzel, Cezary Kaliszyk,
    Mirek Olšák, Bartosz Piotrowski, Stephan Schulz, Martin Suda, 和 Josef Urban. **MizAR
    60** for **Mizar 50**。见于*第14届国际交互定理证明会议*，2023年。
- en: Jiang et al. (2023a) Albert Q Jiang, Wenda Li, and Mateja Jamnik. Multilingual
    mathematical autoformalization. *arXiv preprint arXiv:2311.03755*, 2023a.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等（2023a）Albert Q Jiang, Wenda Li, 和 Mateja Jamnik. 多语言数学自动形式化。*arXiv预印本
    arXiv:2311.03755*，2023年a。
- en: 'Jiang et al. (2023b) Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li,
    Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, and Guillaume Lample.
    Draft, sketch, and prove: Guiding formal theorem provers with informal proofs.
    In *The Eleventh International Conference on Learning Representations*, 2023b.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等（2023b）Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng
    Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, 和 Guillaume Lample. 起草、草图与证明：通过非正式证明指导形式化定理证明者。见于*第十一届国际学习表征会议*，2023年b。
- en: 'Jiang et al. (2021) Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, and
    Yuhuai Wu. LISA: Language models of Isabelle proofs. In *6th Conference on Artificial
    Intelligence and Theorem Proving*, 2021.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等（2021）Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, 和 Yuhuai Wu.
    **LISA**：Isabelle证明的语言模型。见于*第6届人工智能与定理证明会议*，2021年。
- en: 'Jiang et al. (2022) Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad
    Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu, and Mateja Jamnik. Thor:
    Wielding hammers to integrate language models and automated theorem provers. In
    *Proceedings of the 36th International Conference on Neural Information Processing
    Systems*, 2022.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人（2022）**Albert Qiaochu Jiang**、**Wenda Li**、**Szymon Tworkowski**、**Konrad
    Czechowski**、**Tomasz Odrzygóźdź**、**Piotr Miłoś**、**Yuhuai Wu** 和 **Mateja Jamnik**。Thor：运用锤子整合语言模型和自动定理证明器。在
    *第36届神经信息处理系统国际会议论文集*，2022年。
- en: Johansson & Smallbone (2023) Moa Johansson and Nicholas Smallbone. Exploring
    mathematical conjecturing with large language models. In *17th International Workshop
    on Neural-Symbolic Learning and Reasoning*, 2023.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johansson & Smallbone（2023）**Moa Johansson** 和 **Nicholas Smallbone**。使用大型语言模型探索数学猜想。在
    *第17届神经-符号学习与推理国际研讨会*，2023年。
- en: 'Kaliszyk & Urban (2015a) Cezary Kaliszyk and Josef Urban. FEMaLeCoP: Fairly
    efficient machine learning connection prover. In *Proceedings of 20th International
    Conference on Logic for Programming, Artificial Intelligence and Reasoning*, 2015a.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliszyk & Urban（2015a）**Cezary Kaliszyk** 和 **Josef Urban**。FEMaLeCoP：相当高效的机器学习连接证明器。在
    *第20届逻辑编程、人工智能与推理国际会议论文集*，2015a年。
- en: Kaliszyk & Urban (2015b) Cezary Kaliszyk and Josef Urban. MizAR 40 for Mizar
    40. *Journal of Automated Reasoning*, 2015b.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliszyk & Urban（2015b）**Cezary Kaliszyk** 和 **Josef Urban**。MizAR 40 for Mizar
    40。*自动推理杂志*，2015b年。
- en: 'Kaliszyk et al. (2014) Cezary Kaliszyk, Josef Urban, Jiří Vyskočil, and Herman
    Geuvers. Developing corpus-based translation methods between informal and formal
    mathematics: Project description. In *International Conference on Intelligent
    Computer Mathematics*, 2014.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliszyk 等人（2014）**Cezary Kaliszyk**、**Josef Urban**、**Jiří Vyskočil** 和 **Herman
    Geuvers**。开发基于语料库的非正式与正式数学之间的翻译方法：项目描述。在 *国际智能计算数学会议*，2014年。
- en: Kaliszyk et al. (2015) Cezary Kaliszyk, Josef Urban, and Jiří Vyskočil. Learning
    to parse on aligned corpora (rough diamond). In *6th International Conference
    on Interactive Theorem Proving*, 2015.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliszyk 等人（2015）**Cezary Kaliszyk**、**Josef Urban** 和 **Jiří Vyskočil**。在对齐语料库上学习解析（粗糙的钻石）。在
    *第六届国际交互定理证明会议*，2015年。
- en: 'Kaliszyk et al. (2017) Cezary Kaliszyk, François Chollet, and Christian Szegedy.
    HolStep: A machine learning dataset for higher-order logic theorem proving. In
    *The Fifth International Conference on Learning Representations*, 2017.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliszyk 等人（2017）**Cezary Kaliszyk**、**François Chollet** 和 **Christian Szegedy**。HolStep：用于高阶逻辑定理证明的机器学习数据集。在
    *第五届国际学习表征会议*，2017年。
- en: Kaliszyk et al. (2018) Cezary Kaliszyk, Josef Urban, Henryk Michalewski, and
    Miroslav Olšák. Reinforcement learning of theorem proving. In *Proceedings of
    the 32nd International Conference on Neural Information Processing Systems*, 2018.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliszyk 等人（2018）**Cezary Kaliszyk**、**Josef Urban**、**Henryk Michalewski**
    和 **Miroslav Olšák**。定理证明的强化学习。在 *第32届神经信息处理系统国际会议论文集*，2018年。
- en: Kamath et al. (2023) Adharsh Kamath, Aditya Senthilnathan, Saikat Chakraborty,
    Pantazis Deligiannis, Shuvendu K Lahiri, Akash Lal, Aseem Rastogi, Subhajit Roy,
    and Rahul Sharma. Finding inductive loop invariants using large language models.
    *arXiv preprint arXiv:2311.07948*, 2023.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamath 等人（2023）**Adharsh Kamath**、**Aditya Senthilnathan**、**Saikat Chakraborty**、**Pantazis
    Deligiannis**、**Shuvendu K Lahiri**、**Akash Lal**、**Aseem Rastogi**、**Subhajit
    Roy** 和 **Rahul Sharma**。使用大型语言模型寻找归纳循环不变式。*arXiv 预印本 arXiv:2311.07948*，2023年。
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval
    for open-domain question answering. In *Proceedings of the 2020 Conference on
    Empirical Methods in Natural Language Processing*, 2020.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin 等人（2020）**Vladimir Karpukhin**、**Barlas Oğuz**、**Sewon Min**、**Patrick
    Lewis**、**Ledell Wu**、**Sergey Edunov**、**Danqi Chen** 和 **Wen-tau Yih**。开放领域问答的密集段落检索。在
    *2020年自然语言处理实证方法会议论文集*，2020年。
- en: 'Kazemi et al. (2023) Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu,
    Xi Chen, and Radu Soricut. GeomVerse: A systematic evaluation of large models
    for geometric reasoning. *arXiv preprint arXiv:2312.12241*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kazemi 等人（2023）**Mehran Kazemi**、**Hamidreza Alvari**、**Ankit Anand**、**Jialin
    Wu**、**Xi Chen** 和 **Radu Soricut**。GeomVerse：大型模型在几何推理中的系统评估。*arXiv 预印本 arXiv:2312.12241*，2023年。
- en: 'Kern & Greenstreet (1999) Christoph Kern and Mark R Greenstreet. Formal verification
    in hardware design: A survey. *ACM Transactions on Design Automation of Electronic
    Systems*, 1999.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kern & Greenstreet（1999）**Christoph Kern** 和 **Mark R Greenstreet**。硬件设计中的形式化验证：综述。*ACM
    电子系统设计自动化期刊*，1999年。
- en: 'Klein et al. (2009) Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick,
    David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski,
    Michael Norrish, et al. seL4: Formal verification of an os kernel. In *Proceedings
    of the ACM SIGOPS 22nd Symposium on Operating Systems Principles*, 2009.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Klein et al. (2009) Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick,
    David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski,
    Michael Norrish 等。seL4: 操作系统内核的形式化验证。在 *ACM SIGOPS 第22届操作系统原理研讨会论文集*，2009年。'
- en: Korovin (2008) Konstantin Korovin. iProver–an instantiation-based theorem prover
    for first-order logic (system description). In *Proceedings of the 4th International
    Joint Conference on Automated Reasoning*, 2008.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korovin (2008) Konstantin Korovin。iProver–一个基于实例的命题逻辑定理证明器（系统描述）。在 *第四届国际自动推理联合会议论文集*，2008年。
- en: Kovács & Voronkov (2013) Laura Kovács and Andrei Voronkov. First-order theorem
    proving and Vampire. In *International Conference on Computer Aided Verification*,
    2013.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kovács & Voronkov (2013) Laura Kovács 和 Andrei Voronkov。一阶定理证明和Vampire。在 *计算机辅助验证国际会议*，2013年。
- en: 'Kovriguina et al. (2022) Liubov Kovriguina, Roman Teucher, and Robert Wardenga.
    TextGraphs-16 natural language premise selection task: Zero-shot premise selection
    with prompting generative language models. In *Proceedings of TextGraphs-16: Graph-based
    Methods for Natural Language Processing*, 2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kovriguina et al. (2022) Liubov Kovriguina, Roman Teucher 和 Robert Wardenga。TextGraphs-16
    自然语言前提选择任务：使用生成语言模型进行零样本前提选择。在 *TextGraphs-16: 基于图的方法在自然语言处理中的应用论文集*，2022年。'
- en: Kucik & Korovin (2018) Andrzej Stanisław Kucik and Konstantin Korovin. Premise
    selection with neural networks and distributed representation of features. *arXiv
    preprint arXiv:1807.10268*, 2018.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kucik & Korovin (2018) Andrzej Stanisław Kucik 和 Konstantin Korovin。神经网络和特征分布表示的前提选择。*arXiv
    预印本 arXiv:1807.10268*，2018年。
- en: Kühlwein et al. (2012) Daniel Kühlwein, Twan van Laarhoven, Evgeni Tsivtsivadze,
    Josef Urban, and Tom Heskes. Overview and evaluation of premise selection techniques
    for large theory mathematics. In *Proceedings of the 6th International Joint Conference
    on Automated Reasoning*, 2012.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kühlwein et al. (2012) Daniel Kühlwein, Twan van Laarhoven, Evgeni Tsivtsivadze,
    Josef Urban 和 Tom Heskes。大规模理论数学的前提选择技术概述和评估。在 *第六届国际自动推理联合会议论文集*，2012年。
- en: Kusumoto et al. (2018) Mitsuru Kusumoto, Keisuke Yahata, and Masahiro Sakai.
    Automated theorem proving in intuitionistic propositional logic by deep reinforcement
    learning. *arXiv preprint arXiv:1811.00796*, 2018.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kusumoto et al. (2018) Mitsuru Kusumoto, Keisuke Yahata 和 Masahiro Sakai。通过深度强化学习进行直觉主义命题逻辑的自动定理证明。*arXiv
    预印本 arXiv:1811.00796*，2018年。
- en: Lample & Conneau (2019) Guillaume Lample and Alexis Conneau. Cross-lingual language
    model pretraining. In *Proceedings of the 33rd International Conference on Neural
    Information Processing Systems*, 2019.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample & Conneau (2019) Guillaume Lample 和 Alexis Conneau。跨语言模型预训练。在 *第33届国际神经信息处理系统会议论文集*，2019年。
- en: Lample et al. (2018) Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer,
    and Marc’Aurelio Ranzato. Phrase-based & neural unsupervised machine translation.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, 2018.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample et al. (2018) Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer
    和 Marc’Aurelio Ranzato。基于短语和神经网络的无监督机器翻译。在 *2018年自然语言处理经验方法会议论文集*，2018年。
- en: Lample et al. (2022) Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux,
    Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.
    Hypertree proof search for neural theorem proving. In *Proceedings of the 36th
    International Conference on Neural Information Processing Systems*, 2022.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample et al. (2022) Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux,
    Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner 和 Xavier Martinet。用于神经定理证明的超树证明搜索。在
    *第36届国际神经信息处理系统会议论文集*，2022年。
- en: Lee et al. (2020) Dennis Lee, Christian Szegedy, Markus N Rabe, Sarah M Loos,
    and Kshitij Bansal. Mathematical reasoning in latent space. In *The Eighth International
    Conference on Learning Representations*, 2020.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2020) Dennis Lee, Christian Szegedy, Markus N Rabe, Sarah M Loos
    和 Kshitij Bansal。潜在空间中的数学推理。在 *第八届国际学习表征会议*，2020年。
- en: Leroy et al. (2016) Xavier Leroy, Sandrine Blazy, Daniel Kästner, Bernhard Schommer,
    Markus Pister, and Christian Ferdinand. CompCert–a formally verified optimizing
    compiler. In *Proceeding of the 8th European Congress on Embedded Real Time Software
    and Systems*, 2016.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leroy et al. (2016) Xavier Leroy, Sandrine Blazy, Daniel Kästner, Bernhard Schommer,
    Markus Pister 和 Christian Ferdinand。CompCert–一个形式化验证的优化编译器。在 *第八届欧洲嵌入式实时软件与系统大会论文集*，2016年。
- en: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language
    models. *Proceedings of the 36th International Conference on Neural Information
    Processing Systems*, 2022.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo 等。利用语言模型解决定量推理问题。*第36届国际神经信息处理系统会议论文集*，2022年。
- en: 'Li et al. (2023) Weixian Waylon Li, Yftah Ziser, Maximin Coavoux, and Shay B
    Cohen. BERT is not the count: Learning to match mathematical statements with proofs.
    In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Weixian Waylon Li, Yftah Ziser, Maximin Coavoux, 和 Shay B Cohen。BERT
    不是计数器：学习匹配数学陈述与证明。发表于*第17届欧洲计算语言学协会会议论文集*，2023年。
- en: 'Li et al. (2021a) Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C Paulson. IsarStep:
    A benchmark for high-level mathematical reasoning. In *The Ninth International
    Conference on Learning Representations*, 2021a.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021a) Wenda Li, Lei Yu, Yuhuai Wu, 和 Lawrence C Paulson。IsarStep：一个高水平数学推理基准。发表于*第九届国际表示学习会议*，2021a。
- en: Li et al. (2021b) Zhaoyu Li, Binghong Chen, and Xujie Si. Graph contrastive
    pre-training for effective theorem reasoning. *38th International Conference on
    Machine Learning Workshop on Self-Supervised Learning for Reasoning and Perception*,
    2021b.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021b) Zhaoyu Li, Binghong Chen, 和 Xujie Si。图对比预训练以有效的定理推理。*第38届国际机器学习会议自监督学习与推理与感知研讨会*，2021b。
- en: 'Liang et al. (2023a) Zhenwen Liang, Tianyu Yang, Jipeng Zhang, and Xiangliang
    Zhang. UniMath: A foundational and multimodal mathematical reasoner. In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, 2023a.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2023a) Zhenwen Liang, Tianyu Yang, Jipeng Zhang, 和 Xiangliang
    Zhang。UniMath：一个基础且多模态的数学推理器。发表于*2023年自然语言处理实证方法会议论文集*，2023a。
- en: 'Liang et al. (2023b) Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark,
    Xiangliang Zhang, and Ashwin Kalyan. Let GPT be a math tutor: Teaching math word
    problem solvers with customized exercise generation. In *The 2023 Conference on
    Empirical Methods in Natural Language Processing*, 2023b.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2023b) Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark,
    Xiangliang Zhang, 和 Ashwin Kalyan。让 GPT 成为数学辅导员：通过定制化练习生成教学数学问题求解器。发表于*2023年自然语言处理实证方法会议论文集*，2023b。
- en: Lin et al. (2021) Qika Lin, Jun Liu, Lingling Zhang, Yudai Pan, Xin Hu, Fangzhi
    Xu, and Hongwei Zeng. Contrastive graph representations for logical formulas embedding.
    *IEEE Transactions on Knowledge and Data Engineering*, 2021.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2021) Qika Lin, Jun Liu, Lingling Zhang, Yudai Pan, Xin Hu, Fangzhi
    Xu, 和 Hongwei Zeng。用于逻辑公式嵌入的对比图表示。*IEEE 知识与数据工程交易*，2021年。
- en: 'Liu et al. (2023) Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan,
    Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, et al. FIMO: A challenge
    formal dataset for automated theorem proving. *arXiv preprint arXiv:2309.04295*,
    2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye
    Yuan, Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li 等。FIMO：一个用于自动定理证明的挑战性正式数据集。*arXiv
    预印本 arXiv:2309.04295*，2023年。
- en: Liu et al. (2022a) Qinghua Liu, Yang Xu, and Xingxing He. Attention recurrent
    cross-graph neural network for selecting premises. *International Journal of Machine
    Learning and Cybernetics*, 2022a.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022a) Qinghua Liu, Yang Xu, 和 Xingxing He。注意力递归跨图神经网络用于选择前提。*国际机器学习与网络学期刊*，2022a。
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa:
    A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*,
    2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, 和 Veselin Stoyanov。RoBERTa：一种强健优化的
    BERT 预训练方法。*arXiv 预印本 arXiv:1907.11692*，2019年。
- en: Liu et al. (2022b) Zhou Liu, Yujun Li, Zhengying Liu, Lin Li, and Zhenguo Li.
    Learning to prove trigonometric identities. *arXiv preprint arXiv:2207.06679*,
    2022b.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022b) Zhou Liu, Yujun Li, Zhengying Liu, Lin Li, 和 Zhenguo Li。学习证明三角恒等式。*arXiv
    预印本 arXiv:2207.06679*，2022b。
- en: Loos et al. (2017) Sarah Loos, Geoffrey Irving, Christian Szegedy, and Cezary
    Kaliszyk. Deep network guided proof search. In *Proceedings of 21st International
    Conference on Logic for Programming, Artificial Intelligence and Reasoning*, 2017.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loos et al. (2017) Sarah Loos, Geoffrey Irving, Christian Szegedy, 和 Cezary
    Kaliszyk。深度网络引导的证明搜索。发表于*第21届国际编程逻辑、人工智能和推理会议论文集*，2017年。
- en: Lovász & Schrijver (1991) László Lovász and Alexander Schrijver. Cones of matrices
    and set-functions and 0–1 optimization. *SIAM journal on optimization*, 1991.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lovász & Schrijver (1991) László Lovász 和 Alexander Schrijver。矩阵和集合函数的锥体及 0–1
    优化。*SIAM 优化期刊*，1991年。
- en: Luong et al. (2017) Minh-Thang Luong, Eugene Brevdo, and Rui Zhao. Neural machine
    translation (seq2seq) tutorial. *https://github.com/tensorflow/nmt*, 2017.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luong et al. (2017) Minh-Thang Luong, Eugene Brevdo 和 Rui Zhao。神经机器翻译（seq2seq）教程。*https://github.com/tensorflow/nmt*，2017年。
- en: mathlib Community (2020) The mathlib Community. The Lean mathematical library.
    In *Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs
    and Proofs*, 2020.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mathlib Community (2020) mathlib 社区。Lean 数学库。在*第九届 ACM SIGPLAN 国际认证程序与证明会议*，2020年。
- en: McKeown & Sutcliffe (2023) Jack McKeown and Geoff Sutcliffe. Reinforcement learning
    for guiding the E theorem prover. In *Proceedings of the Thirty-Sixth International
    Florida Artificial Intelligence Research Society Conference*, 2023.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McKeown & Sutcliffe (2023) Jack McKeown 和 Geoff Sutcliffe。用于引导 E 定理证明器的强化学习。在*第36届国际佛罗里达人工智能研究学会会议论文集*，2023年。
- en: 'Mikuła et al. (2024) Maciej Mikuła, Szymon Antoniak, Szymon Tworkowski, Albert Qiaochu
    Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, and Yuhuai
    Wu. Magnushammer: A transformer-based approach to premise selection. In *The Twelfth
    International Conference on Learning Representations*, 2024.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mikuła et al. (2024) Maciej Mikuła, Szymon Antoniak, Szymon Tworkowski, Albert
    Qiaochu Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś
    和 Yuhuai Wu。Magnushammer: 基于 Transformer 的前提选择方法。在*第十二届国际学习表征会议*，2024年。'
- en: Milner (1972) Robin Milner. Implementation and applications of Scott’s logic
    for computable functions. In *Proceedings of ACM Conference on Proving Assertions
    about Programs*, 1972.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Milner (1972) Robin Milner。Scott 逻辑在可计算函数中的实现和应用。在*ACM 会议关于程序断言证明*，1972年。
- en: Mnih et al. (2013) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep
    reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih et al. (2013) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra 和 Martin Riedmiller。使用深度强化学习玩 Atari 游戏。*arXiv
    预印本 arXiv:1312.5602*，2013年。
- en: Mo et al. (2020) Guangshuai Mo, Yan Xiong, Wenchao Huang, and Lu Ma. Automated
    theorem proving via interacting with proof assistants by dynamic strategies. In
    *6th International Conference on Big Data Computing and Communications*, 2020.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mo et al. (2020) Guangshuai Mo, Yan Xiong, Wenchao Huang 和 Lu Ma。通过与证明助手的动态策略互动进行自动定理证明。在*第六届国际大数据计算与通信会议*，2020年。
- en: Moura & Ullrich (2021) Leonardo de Moura and Sebastian Ullrich. The Lean 4 theorem
    prover and programming language. In *28th International Conference on Automated
    Deduction*, 2021.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moura & Ullrich (2021) Leonardo de Moura 和 Sebastian Ullrich。Lean 4 定理证明器和编程语言。在*第28届国际自动推理会议*，2021年。
- en: Olšák et al. (2019) Miroslav Olšák, Cezary Kaliszyk, and Josef Urban. Property
    invariant embedding for automated reasoning. In *24th European Conference on Artificial
    Intelligence*, 2019.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olšák et al. (2019) Miroslav Olšák, Cezary Kaliszyk 和 Josef Urban。用于自动推理的属性不变嵌入。在*第24届欧洲人工智能会议*，2019年。
- en: Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation
    learning with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*,
    2018.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oord et al. (2018) Aaron van den Oord, Yazhe Li 和 Oriol Vinyals。使用对比预测编码进行表征学习。*arXiv
    预印本 arXiv:1807.03748*，2018年。
- en: 'Otten & Bibel (2003) Jens Otten and Wolfgang Bibel. leanCoP: Lean connection-based
    theorem proving. *Journal of Symbolic Computation*, 2003.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Otten & Bibel (2003) Jens Otten 和 Wolfgang Bibel。leanCoP: 基于连接的 Lean 定理证明。在*符号计算期刊*，2003年。'
- en: Paliwal et al. (2020) Aditya Paliwal, Sarah Loos, Markus Rabe, Kshitij Bansal,
    and Christian Szegedy. Graph representations for higher-order logic and theorem
    proving. In *Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence*,
    2020.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paliwal et al. (2020) Aditya Paliwal, Sarah Loos, Markus Rabe, Kshitij Bansal
    和 Christian Szegedy。高阶逻辑和定理证明的图表示。在*第三十四届 AAAI 人工智能会议论文集*，2020年。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. BLEU: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics*,
    2002.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward 和 Wei-Jing
    Zhu。BLEU: 一种机器翻译自动评价方法。在*第40届计算语言学协会年会论文集*，2002年。'
- en: 'Paster et al. (2024) Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and
    Jimmy Ba. OpenWebMath: An open dataset of high-quality mathematical web text.
    In *The Twelfth International Conference on Learning Representations*, 2024.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paster等（2024）Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, 和 Jimmy Ba。OpenWebMath：一个高质量数学网页文本的开放数据集。在*第十二届国际学习表征会议*，2024。
- en: Patel et al. (2023) Nilay Patel, Jeffrey Flanigan, and Rahul Saha. A new approach
    towards autoformalization. *arXiv preprint arXiv:2310.07957*, 2023.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patel等（2023）Nilay Patel, Jeffrey Flanigan, 和 Rahul Saha。自动形式化的新方法。*arXiv预印本
    arXiv:2310.07957*，2023。
- en: 'Paulson (1994) Lawrence C Paulson. *Isabelle: A Generic Theorem Prover*. Springer,
    1994.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paulson（1994）Lawrence C Paulson。*Isabelle: A Generic Theorem Prover*。Springer，1994。'
- en: Peng & Ma (2017) Kebin Peng and Dianfu Ma. Tree-structure cnn for automated
    theorem proving. In *24th International Conference on Neural Information Processing*,
    2017.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng & Ma（2017）Kebin Peng 和 Dianfu Ma。用于自动定理证明的树状结构cnn。在*第24届神经信息处理国际会议*，2017。
- en: Piepenbrock et al. (2021) Jelle Piepenbrock, Tom Heskes, Mikoláš Janota, and
    Josef Urban. Learning equational theorem proving. In *6th Conference on Artificial
    Intelligence and Theorem Proving*, 2021.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piepenbrock等（2021）Jelle Piepenbrock, Tom Heskes, Mikoláš Janota, 和 Josef Urban。学习方程定理证明。在*第六届人工智能与定理证明会议*，2021。
- en: Piepenbrock et al. (2022a) Jelle Piepenbrock, Tom Heskes, Mikoláš Janota, and
    Josef Urban. Guiding an automated theorem prover with neural rewriting. In *Proceedings
    of the 11th International Joint Conference on Automated Reasoning*, 2022a.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piepenbrock等（2022a）Jelle Piepenbrock, Tom Heskes, Mikoláš Janota, 和 Josef Urban。利用神经重写指导自动定理证明。在*第十一届国际自动推理联合会议论文集*，2022a。
- en: Piepenbrock et al. (2022b) Jelle Piepenbrock, Josef Urban, Konstantin Korovin,
    Miroslav Olšák, Tom Heskes, and Mikolaš Janota. Machine learning meets the herbrand
    universe. *arXiv preprint arXiv:2210.03590*, 2022b.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piepenbrock等（2022b）Jelle Piepenbrock, Josef Urban, Konstantin Korovin, Miroslav
    Olšák, Tom Heskes, 和 Mikolaš Janota。机器学习与Herbrand宇宙的结合。*arXiv预印本 arXiv:2210.03590*，2022b。
- en: Piotrowski & Urban (2020a) Bartosz Piotrowski and Josef Urban. Stateful premise
    selection by recurrent neural networks. In *Proceedings of 23rd International
    Conference on Logic for Programming, Artificial Intelligence and Reasoning*, 2020a.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piotrowski & Urban（2020a）Bartosz Piotrowski 和 Josef Urban。通过递归神经网络进行有状态前提选择。在*第23届国际逻辑编程、人工智能和推理会议论文集*，2020a。
- en: Piotrowski & Urban (2020b) Bartosz Piotrowski and Josef Urban. Guiding inferences
    in connection tableau by recurrent neural networks. In *13th International Conference
    on Intelligent Computer Mathematics*, 2020b.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piotrowski & Urban（2020b）Bartosz Piotrowski 和 Josef Urban。通过递归神经网络指导连接表中的推理。在*第十三届智能计算数学国际会议*，2020b。
- en: Polu & Sutskever (2020) Stanislas Polu and Ilya Sutskever. Generative language
    modeling for automated theorem proving. *arXiv preprint arXiv:2009.03393*, 2020.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polu & Sutskever（2020）Stanislas Polu 和 Ilya Sutskever。用于自动定理证明的生成语言建模。*arXiv预印本
    arXiv:2009.03393*，2020。
- en: Polu et al. (2023) Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys,
    Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning.
    In *The Eleventh International Conference on Learning Representations*, 2023.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polu等（2023）Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor
    Babuschkin, 和 Ilya Sutskever。形式化数学陈述课程学习。在*第十一届国际学习表征会议*，2023。
- en: Poulsen et al. (2024) Seth Poulsen, Sami Sarsa, James Prather, Juho Leinonen,
    Brett A Becker, Arto Hellas, Paul Denny, and Brent N Reeves. Solving proof block
    problems using large language models. In *Proceedings of the 55th ACM Technical
    Symposium on Computer Science Education*, 2024.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poulsen等（2024）Seth Poulsen, Sami Sarsa, James Prather, Juho Leinonen, Brett
    A Becker, Arto Hellas, Paul Denny, 和 Brent N Reeves。使用大型语言模型解决证明块问题。在*第55届ACM计算机科学教育技术研讨会论文集*，2024。
- en: Proroković et al. (2021) Krsto Proroković, Michael Wand, and Jürgen Schmidhuber.
    Improving stateful premise selection with transformers. In *14th International
    Conference on Intelligent Computer Mathematics*, 2021.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Proroković等（2021）Krsto Proroković, Michael Wand, 和 Jürgen Schmidhuber。利用变换器改进有状态前提选择。在*第十四届智能计算数学国际会议*，2021。
- en: Rabe et al. (2021) Markus N Rabe, Dennis Lee, Kshitij Bansal, and Christian
    Szegedy. Mathematical reasoning via self-supervised skip-tree training. In *The
    Ninth International Conference on Learning Representations*, 2021.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabe等（2021）Markus N Rabe, Dennis Lee, Kshitij Bansal, 和 Christian Szegedy。通过自监督跳树训练进行数学推理。在*第九届国际学习表征会议*，2021。
- en: Rawson & Reger (2019) Michael Rawson and Giles Reger. A neurally-guided, parallel
    theorem prover. In *12th International Symposium on Frontiers of Combining Systems*,
    2019.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rawson & Reger (2019) Michael Rawson 和 Giles Reger. 一种神经引导的并行定理证明器。发表于*第12届结合系统前沿国际研讨会*，2019年。
- en: Rawson & Reger (2020) Michael Rawson and Giles Reger. Directed graph networks
    for logical reasoning (extended abstract). In *Joint Proceedings of the 7th Workshop
    on Practical Aspects of Automated Reasoning (PAAR) and the 5th Satisfiability
    Checking and Symbolic Computation Workshop (SC-Square) Workshop*, 2020.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rawson & Reger (2020) Michael Rawson 和 Giles Reger. 用于逻辑推理的有向图网络（扩展摘要）。发表于*第7届自动推理实践方面研讨会（PAAR）与第5届可满足性检查与符号计算研讨会（SC-Square）联合会议论文集*，2020年。
- en: 'Rawson & Reger (2021) Michael Rawson and Giles Reger. lazyCoP: Lazy paramodulation
    meets neurally guided search. In *30th International Conference on Automated Reasoning
    with Analytic Tableaux and Related Methods*, 2021.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rawson & Reger (2021) Michael Rawson 和 Giles Reger. lazyCoP：懒惰的参数化模遇上神经引导的搜索。发表于*第30届自动推理与分析表格及相关方法国际会议*，2021年。
- en: 'Reichel et al. (2023) Tom Reichel, R Henderson, Andrew Touchet, Andrew Gardner,
    and Talia Ringer. Proof repair infrastructure for supervised models: Building
    a large proof repair dataset. In *14th International Conference on Interactive
    Theorem Proving*, 2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reichel et al. (2023) Tom Reichel, R Henderson, Andrew Touchet, Andrew Gardner
    和 Talia Ringer. 针对监督模型的证明修复基础设施：构建一个大型证明修复数据集。发表于*第14届交互式定理证明国际会议*，2023年。
- en: 'Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza, et al. The probabilistic
    relevance framework: BM25 and beyond. *Foundations and Trends® in Information
    Retrieval*, 2009.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza 等。概率相关框架：BM25 及其扩展。*信息检索基础与趋势®*，2009年。
- en: Rudnicki (1992) Piotr Rudnicki. An overview of the Mizar project. In *Proceedings
    of the 1992 Workshop on Types for Proofs and Programs*, 1992.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rudnicki (1992) Piotr Rudnicki. Mizar 项目概述。发表于*1992年证明与程序类型研讨会论文集*，1992年。
- en: 'Rute et al. (2024) Jason Rute, Miroslav Olšák, Lasse Blaauwbroek, Fidel Ivan Schaposnik
    Massolo, Jelle Piepenbrock, and Vasily Pestun. Graph2Tac: Learning hierarchical
    representations of math concepts in theorem proving. *arXiv preprint arXiv:2401.02949*,
    2024.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rute et al. (2024) Jason Rute, Miroslav Olšák, Lasse Blaauwbroek, Fidel Ivan
    Schaposnik Massolo, Jelle Piepenbrock 和 Vasily Pestun. Graph2Tac：在定理证明中学习数学概念的层次表示。*arXiv
    预印本 arXiv:2401.02949*，2024年。
- en: Sanchez-Stern et al. (2020) Alex Sanchez-Stern, Yousef Alhessi, Lawrence Saul,
    and Sorin Lerner. Generating correctness proofs with neural networks. In *Proceedings
    of the 4th ACM SIGPLAN International Workshop on Machine Learning and Programming
    Languages*, 2020.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanchez-Stern et al. (2020) Alex Sanchez-Stern, Yousef Alhessi, Lawrence Saul
    和 Sorin Lerner. 使用神经网络生成正确性证明。发表于*第4届 ACM SIGPLAN 国际机器学习与编程语言研讨会论文集*，2020年。
- en: 'Sanchez-Stern et al. (2023) Alex Sanchez-Stern, Emily First, Timothy Zhou,
    Zhanna Kaufman, Yuriy Brun, and Talia Ringer. Passport: Improving automated formal
    verification using identifiers. *ACM Transactions on Programming Languages and
    Systems*, 2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanchez-Stern et al. (2023) Alex Sanchez-Stern, Emily First, Timothy Zhou, Zhanna
    Kaufman, Yuriy Brun 和 Talia Ringer. Passport：使用标识符改进自动化形式验证。*ACM 编程语言与系统交易*，2023年。
- en: Scheidt (2023) Gregor vom Scheidt. Experimental results from applying GPT-4
    to an unpublished formal language. *arXiv preprint arXiv:2305.12196*, 2023.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scheidt (2023) Gregor vom Scheidt. 将 GPT-4 应用于未公开形式语言的实验结果。*arXiv 预印本 arXiv:2305.12196*，2023年。
- en: Schulz (2002) Stephan Schulz. E–a brainiac theorem prover. *AI Communications*,
    2002.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulz (2002) Stephan Schulz. E–一个天才定理证明器。*AI 通讯*，2002年。
- en: Schumann (2001) Johann M Schumann. *Automated Theorem Proving in Software Engineering*.
    Springer Science & Business Media, 2001.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schumann (2001) Johann M Schumann. *软件工程中的自动定理证明*。Springer Science & Business
    Media，2001年。
- en: 'Shao et al. (2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao
    Song, Mingchuan Zhang, YK Li, Y Wu, and Daya Guo. DeepSeekMath: Pushing the limits
    of mathematical reasoning in open language models. *arXiv preprint arXiv:2402.03300*,
    2024.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao et al. (2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song,
    Mingchuan Zhang, YK Li, Y Wu 和 Daya Guo. DeepSeekMath：推动开放语言模型中的数学推理极限。*arXiv
    预印本 arXiv:2402.03300*，2024年。
- en: 'Shminke (2023) Boris Shminke. gym-saturation: Gymnasium environments for saturation
    provers (system description). In *32nd International Conference on Automated Reasoning
    with Analytic Tableaux and Related Methods*, 2023.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shminke (2023) Boris Shminke. gym-saturation：用于饱和证明器的 Gymnasium 环境（系统描述）。发表于*第32届自动推理与分析表格及相关方法国际会议*，2023年。
- en: Silver et al. (2018) David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis
    Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran,
    Thore Graepel, et al. A general reinforcement learning algorithm that masters
    chess, shogi, and Go through self-play. *Science*, 2018.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver 等（2018）David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,
    Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
    Graepel 等。一个通用的强化学习算法，通过自我对弈掌握国际象棋、将棋和围棋。*科学*，2018年。
- en: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
    MPNet: Masked and permuted pre-training for language understanding. In *Proceedings
    of the 34th International Conference on Neural Information Processing Systems*,
    2020.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song 等（2020）Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu 和 Tie-Yan Liu。MPNet:
    用于语言理解的掩码和排列预训练。在 *第34届国际神经信息处理系统大会论文集*，2020年。'
- en: Song et al. (2023) Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Towards large
    language models as copilots for theorem proving in Lean. In *37th Conference on
    Neural Information Processing Systems Workshop on MATH-AI*, 2023.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等（2023）Peiyang Song, Kaiyu Yang 和 Anima Anandkumar。朝着将大型语言模型作为 Lean 定理证明的助手。在
    *第37届神经信息处理系统大会 MATH-AI 研讨会*，2023年。
- en: Sørensen et al. (2021) Henrik Kragh Sørensen, Mikkel Willum Johansen, Renee
    Hoekzema, and Hester Breman. Augmenting the human mathematician. In *9th International
    Conference on Learning Representations Workshop on Mathematical Reasoning in General
    Artificial Intelligence*, 2021.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sørensen 等（2021）Henrik Kragh Sørensen, Mikkel Willum Johansen, Renee Hoekzema
    和 Hester Breman。增强人类数学家。在 *第9届国际学习表征会议关于通用人工智能中的数学推理工作坊*，2021年。
- en: Suda (2021a) Martin Suda. Vampire with a brain is a good ITP hammer. In *13th
    International Symposium on Frontiers of Combining Systems*, 2021a.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suda（2021a）Martin Suda。具有大脑的 Vampire 是一个优秀的 ITP 锤子。在 *第13届结合系统前沿国际研讨会*，2021年。
- en: Suda (2021b) Martin Suda. Improving ENIGMA-style clause selection while learning
    from history. In *28th International Conference on Automated Deduction*, 2021b.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suda（2021b）Martin Suda。改善 ENIGMA 风格的子句选择，同时学习历史。在 *第28届国际自动推理会议*，2021年。
- en: 'Sun et al. (2023) Chuyue Sun, Ying Sheng, Oded Padon, and Clark Barrett. Clover:
    Closed-loop verifiable code generation. *arXiv preprint arXiv:2310.17807*, 2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等（2023）Chuyue Sun, Ying Sheng, Oded Padon 和 Clark Barrett。Clover: 闭环可验证代码生成。*arXiv
    预印本 arXiv:2310.17807*，2023年。'
- en: Sutcliffe (2017) Geoff Sutcliffe. The TPTP problem library and associated infrastructure.
    *Journal of Automated Reasoning*, 2017.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutcliffe（2017）Geoff Sutcliffe。TPTP 问题库及相关基础设施。*自动推理杂志*，2017年。
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence
    to sequence learning with neural networks. In *Proceedings of the 27th International
    Conference on Neural Information Processing Systems*, 2014.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等（2014）Ilya Sutskever, Oriol Vinyals 和 Quoc V Le。基于神经网络的序列到序列学习。在
    *第27届国际神经信息处理系统大会论文集*，2014年。
- en: Suttner & Ertel (1990) Christian Suttner and Wolfgang Ertel. Automatic acquisition
    of search guiding heuristics. In *10th International Conference on Automated Deduction*,
    1990.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suttner & Ertel（1990）Christian Suttner 和 Wolfgang Ertel。自动获取搜索指导启发式。在 *第10届国际自动推理会议*，1990年。
- en: Sutton et al. (1999) Richard S Sutton, David McAllester, Satinder Singh, and
    Yishay Mansour. Policy gradient methods for reinforcement learning with function
    approximation. In *Proceedings of the 12th International Conference on Neural
    Information Processing Systems*, 1999.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 等（1999）Richard S Sutton, David McAllester, Satinder Singh 和 Yishay Mansour。用于函数逼近的强化学习策略梯度方法。在
    *第12届国际神经信息处理系统大会论文集*，1999年。
- en: Szegedy (2020) Christian Szegedy. A promising path towards autoformalization
    and general artificial intelligence. In *13th International Conference on Intelligent
    Computer Mathematics*, 2020.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy（2020）Christian Szegedy。通向自动形式化和通用人工智能的有前景的路径。在 *第13届国际智能计算数学会议*，2020年。
- en: Szegedy et al. (2021) Christian Szegedy, Markus Rabe, and Henryk Michalewski.
    Retrieval-augmented proof step synthesis. In *6th Conference on Artificial Intelligence
    and Theorem Proving*, 2021.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等（2021）Christian Szegedy, Markus Rabe 和 Henryk Michalewski。检索增强的证明步骤合成。在
    *第6届人工智能与定理证明会议*，2021年。
- en: Tai et al. (2015) Kai Sheng Tai, Richard Socher, and Christopher D Manning.
    Improved semantic representations from tree-structured long short-term memory
    networks. In *Proceedings of the 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing*,
    2015.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tai et al. (2015) Kai Sheng Tai, Richard Socher 和 Christopher D Manning。来自树结构长短期记忆网络的改进语义表示。在
    *第53届计算语言学协会年会和第七届国际自然语言处理联合会议论文集*，2015年。
- en: Thakur et al. (2023) Amitayush Thakur, Yeming Wen, and Swarat Chaudhuri. An
    in-context learning agent for formal theorem-proving. *arXiv preprint arXiv:2310.04353*,
    2023.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thakur et al. (2023) Amitayush Thakur, Yeming Wen 和 Swarat Chaudhuri。一个用于形式定理证明的上下文学习代理。*arXiv
    预印本 arXiv:2310.04353*，2023年。
- en: 'Tran et al. (2022) Thi Hong Hanh Tran, Matej Martinc, Antoine Doucet, and Senja
    Pollak. IJS at TextGraphs-16 natural language premise selection task: Will contextual
    information improve natural language premise selection? In *Proceedings of TextGraphs-16:
    Graph-based Methods for Natural Language Processing*, 2022.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran et al. (2022) Thi Hong Hanh Tran, Matej Martinc, Antoine Doucet 和 Senja
    Pollak。IJS 在 TextGraphs-16 自然语言前提选择任务中：上下文信息是否能改善自然语言前提选择？在 *TextGraphs-16：基于图的方法自然语言处理论文集*，2022年。
- en: Trinh et al. (2024) Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong.
    Solving olympiad geometry without human demonstrations. *Nature*, 2024.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trinh et al. (2024) Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He 和 Thang Luong。无需人工演示解决奥林匹克几何问题。*Nature*，2024年。
- en: 'Trust et al. (2022) Paul Trust, Provia Kadusabe, Haseeb Younis, Rosane Minghim,
    Evangelos Milios, and Ahmed Zahran. UNLPS at TextGraphs 2022 shared task: Unsupervised
    natural language premise selection in mathematical texts using sentence-MPNet.
    In *Proceedings of TextGraphs-16: Graph-based Methods for Natural Language Processing*,
    2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trust et al. (2022) Paul Trust, Provia Kadusabe, Haseeb Younis, Rosane Minghim,
    Evangelos Milios 和 Ahmed Zahran。UNLPS 在 TextGraphs 2022 共享任务中：使用 sentence-MPNet
    在数学文本中进行无监督自然语言前提选择。在 *TextGraphs-16：基于图的方法自然语言处理论文集*，2022年。
- en: Tworkowski et al. (2022) Szymon Tworkowski, Maciej Mikuła, Tomasz Odrzygóźdź,
    Konrad Czechowski, Szymon Antoniak, Albert Jiang, Christian Szegedy, Łukasz Kuciński,
    Piotr Miłoś, and Yuhuai Wu. Formal premise selection with language models. In
    *7th Conference on Artificial Intelligence and Theorem Proving*, 2022.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tworkowski et al. (2022) Szymon Tworkowski, Maciej Mikuła, Tomasz Odrzygóźdź,
    Konrad Czechowski, Szymon Antoniak, Albert Jiang, Christian Szegedy, Łukasz Kuciński,
    Piotr Miłoś 和 Yuhuai Wu。使用语言模型进行正式前提选择。在 *第七届人工智能与定理证明会议*，2022年。
- en: Urban & Jakubüv (2020) Josef Urban and Jan Jakubüv. First neural conjecturing
    datasets and experiments. In *13th International Conference on Intelligent Computer
    Mathematics*, 2020.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Urban & Jakubüv (2020) Josef Urban 和 Jan Jakubüv。首批神经猜测数据集和实验。在 *第13届智能计算数学国际会议*，2020年。
- en: Urban et al. (2011) Josef Urban, Jiří Vyskočil, and Petr Štěpánek. MaLeCoP machine
    learning connection prover. In *20th International Conference on Automated Reasoning
    with Analytic Tableaux and Related Methods*, 2011.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Urban et al. (2011) Josef Urban, Jiří Vyskočil 和 Petr Štěpánek。MaLeCoP 机器学习连接证明器。在
    *第20届国际自动推理与解析表格及相关方法会议*，2011年。
- en: 'Van Den Oord et al. (2016) Aaron Van Den Oord, Sander Dieleman, Heiga Zen,
    Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray
    Kavukcuoglu, et al. WaveNet: A generative model for raw audio. *arXiv preprint
    arXiv:1609.03499*, 2016.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Den Oord et al. (2016) Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen
    Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu
    等。WaveNet：一种原始音频的生成模型。*arXiv 预印本 arXiv:1609.03499*，2016年。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *Proceedings of the 31st International Conference on Neural Information
    Processing Systems*, 2017.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser 和 Illia Polosukhin。注意力即你所需。在 *第31届神经信息处理系统国际会议论文集*，2017年。
- en: Veličković et al. (2018) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In *The
    Sixth International Conference on Learning Representations*, 2018.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veličković et al. (2018) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio 和 Yoshua Bengio。图注意力网络。在 *第六届国际学习表示会议*，2018年。
- en: Vishwakarma & Mishra (2023) Rahul Vishwakarma and Subhankar Mishra. Enhancing
    neural theorem proving through data augmentation and dynamic sampling method.
    *arXiv preprint arXiv:2312.14188*, 2023.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vishwakarma & Mishra (2023) Rahul Vishwakarma 和 Subhankar Mishra。通过数据增强和动态采样方法提升神经定理证明。*arXiv
    预印本 arXiv:2312.14188*，2023年。
- en: 'Wang et al. (2023a) Haiming Wang, Ye Yuan, Zhengying Liu, Jianhao Shen, Yichun
    Yin, Jing Xiong, Enze Xie, Han Shi, Yujun Li, Lin Li, et al. DT-Solver: Automated
    theorem proving with dynamic-tree sampling guided by proof-level value function.
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics*, 2023a.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2023a) Haiming Wang, Ye Yuan, Zhengying Liu, Jianhao Shen, Yichun Yin,
    Jing Xiong, Enze Xie, Han Shi, Yujun Li, Lin Li 等。DT-Solver：通过证明级值函数引导的动态树采样进行自动定理证明。发表于
    *第61届计算语言学协会年会论文集*，2023a年。
- en: Wang & Deng (2020) Mingzhe Wang and Jia Deng. Learning to prove theorems by
    learning to generate theorems. In *Proceedings of the 34th International Conference
    on Neural Information Processing Systems*, 2020.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang & Deng (2020) Mingzhe Wang 和 Jia Deng. 通过学习生成定理来学习证明定理。发表于 *第34届神经信息处理系统国际会议论文集*，2020年。
- en: Wang et al. (2017) Mingzhe Wang, Yihe Tang, Jian Wang, and Jia Deng. Premise
    selection for theorem proving by deep graph embedding. In *Proceedings of the
    31st International Conference on Neural Information Processing Systems*, 2017.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2017) Mingzhe Wang, Yihe Tang, Jian Wang 和 Jia Deng. 通过深度图嵌入进行定理证明的前提选择。发表于
    *第31届神经信息处理系统国际会议论文集*，2017年。
- en: Wang et al. (2018) Qingxiang Wang, Cezary Kaliszyk, and Josef Urban. First experiments
    with neural translation of informal to formal mathematics. In *11th International
    Conference on Intelligent Computer Mathematics*, 2018.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2018) Qingxiang Wang, Cezary Kaliszyk 和 Josef Urban. 初步实验：将非正式数学翻译为正式数学的神经网络方法。发表于
    *第11届智能计算数学国际会议*，2018年。
- en: Wang et al. (2020) Qingxiang Wang, Chad Brown, Cezary Kaliszyk, and Josef Urban.
    Exploration of neural machine translation in autoformalization of mathematics
    in Mizar. In *Proceedings of the 9th ACM SIGPLAN International Conference on Certified
    Programs and Proofs*, 2020.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2020) Qingxiang Wang, Chad Brown, Cezary Kaliszyk 和 Josef Urban. 探索神经机器翻译在
    Mizar 中的数学自动形式化应用。发表于 *第9届 ACM SIGPLAN 认证程序与证明国际会议论文集*，2020年。
- en: 'Wang et al. (2023b) Zengzhi Wang, Rui Xia, and Pengfei Liu. Generative ai for
    math: Part i–MathPile: A billion-token-scale pretraining corpus for math. *arXiv
    preprint arXiv:2312.17120*, 2023b.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2023b) Zengzhi Wang, Rui Xia 和 Pengfei Liu. 数学生成 AI：第一部分—MathPile：一个十亿标记规模的数学预训练语料库。*arXiv
    预印本 arXiv:2312.17120*，2023b年。
- en: 'Welleck & Saha (2023) Sean Welleck and Rahul Saha. LLMSTEP: LLM proofstep suggestions
    in Lean. In *37th Conference on Neural Information Processing Systems Workshop
    on MATH-AI*, 2023.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welleck & Saha (2023) Sean Welleck 和 Rahul Saha. LLMSTEP：在 Lean 中的 LLM 证明步骤建议。发表于
    *第37届神经信息处理系统会议 MATH-AI 研讨会*，2023年。
- en: 'Welleck et al. (2021) Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi,
    Yejin Choi, and Kyunghyun Cho. NaturalProofs: Mathematical theorem proving in
    natural language. In *Thirty-fifth Conference on Neural Information Processing
    Systems Datasets and Benchmarks Track*, 2021.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welleck 等 (2021) Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi,
    Yejin Choi 和 Kyunghyun Cho. NaturalProofs：自然语言中的数学定理证明。发表于 *第35届神经信息处理系统会议数据集和基准测试分会*，2021年。
- en: 'Welleck et al. (2022) Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi,
    and Yejin Choi. NaturalProver: Grounded mathematical proof generation with language
    models. In *Proceedings of the 36th International Conference on Neural Information
    Processing Systems*, 2022.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welleck 等 (2022) Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi
    和 Yejin Choi. NaturalProver：基于语言模型的实际数学证明生成。发表于 *第36届神经信息处理系统国际会议论文集*，2022年。
- en: 'Whalen (2016) Daniel Whalen. Holophrasm: A neural automated theorem prover
    for higher-order logic. *arXiv preprint arXiv:1608.02644*, 2016.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whalen (2016) Daniel Whalen. Holophrasm：用于高阶逻辑的神经自动定理证明器。*arXiv 预印本 arXiv:1608.02644*，2016年。
- en: Wu & Wu (2021) Minchao Wu and Yuhuai Wu. Latent action space for efficient planning
    in theorem proving. In *6th Conference on Artificial Intelligence and Theorem
    Proving*, 2021.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu & Wu (2021) Minchao Wu 和 Yuhuai Wu. 高效定理证明中的潜在动作空间。发表于 *第6届人工智能与定理证明会议*，2021年。
- en: 'Wu et al. (2021a) Minchao Wu, Michael Norrish, Christian Walder, and Amir Dezfouli.
    TacticZero: Learning to prove theorems from scratch with deep reinforcement learning.
    *Proceedings of the 35th International Conference on Neural Information Processing
    Systems*, 2021a.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2021a) Minchao Wu, Michael Norrish, Christian Walder 和 Amir Dezfouli.
    TacticZero：通过深度强化学习从头学习证明定理。发表于 *第35届神经信息处理系统国际会议论文集*，2021a年。
- en: 'Wu et al. (2021b) Yuhuai Wu, Albert Qiaochu Jiang, Jimmy Ba, and Roger Grosse.
    INT: An inequality benchmark for evaluating generalization in theorem proving.
    In *The Ninth International Conference on Learning Representations*, 2021b.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2021b) Yuhuai Wu, Albert Qiaochu Jiang, Jimmy Ba, 和 Roger Grosse.
    INT: 用于评估定理证明中的泛化能力的指标基准。在 *第九届国际表示学习会议* 中，2021年。'
- en: 'Wu et al. (2021c) Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse,
    and Christian Szegedy. LIME: Learning inductive bias for primitives of mathematical
    reasoning. In *Proceedings of the 38th International Conference on Machine Learning*,
    2021c.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2021c) Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse,
    和 Christian Szegedy. LIME: 学习数学推理的归纳偏置。在 *第38届国际机器学习会议论文集* 中，2021年。'
- en: Wu et al. (2022) Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles
    Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language
    models. In *Proceedings of the 36th International Conference on Neural Information
    Processing Systems*, 2022.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2022) Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles
    Staats, Mateja Jamnik, 和 Christian Szegedy. 大型语言模型的自动形式化。在 *第36届国际神经信息处理系统会议论文集*
    中，2022年。
- en: 'Xin et al. (2024) Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying
    Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, et al. LEGO-prover:
    Neural theorem proving with growing libraries. In *The Twelfth International Conference
    on Learning Representations*, 2024.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xin et al. (2024) Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying
    Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, et al. LEGO-prover:
    具有不断增长库的神经定理证明。在 *第十二届国际表示学习会议* 中，2024年。'
- en: 'Xiong et al. (2023) Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun
    Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, et al. TRIGO:
    Benchmarking formal mathematical proof reduction for generative language models.
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, 2023.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiong et al. (2023) Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun
    Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, et al. TRIGO:
    基于生成语言模型的形式数学证明简化基准。在 *2023年自然语言处理经验方法会议论文集* 中，2023年。'
- en: Xu et al. (2019) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
    How powerful are graph neural networks? In *The Seventh International Conference
    on Learning Representations*, 2019.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2019) Keyulu Xu, Weihua Hu, Jure Leskovec, 和 Stefanie Jegelka. 图神经网络的能力有多强？在
    *第七届国际表示学习会议* 中，2019年。
- en: 'Xue et al. (2022) Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan
    Narang, Mihir Kale, Adam Roberts, and Colin Raffel. ByT5: Towards a token-free
    future with pre-trained byte-to-byte models. *Transactions of the Association
    for Computational Linguistics*, 2022.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xue et al. (2022) Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan
    Narang, Mihir Kale, Adam Roberts, 和 Colin Raffel. ByT5: 迈向无令牌的未来与预训练的字节到字节模型。*计算语言学学会会刊*，2022年。'
- en: Yang & Deng (2019) Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting
    with proof assistants. In *Proceedings of the 36th International Conference on
    Machine Learning*, 2019.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang & Deng (2019) Kaiyu Yang 和 Jia Deng. 通过与证明助手互动学习证明定理。在 *第36届国际机器学习会议论文集*
    中，2019年。
- en: 'Yang et al. (2023) Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang
    Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. LeanDojo: Theorem
    proving with retrieval-augmented language models. In *Proceedings of the 37th
    International Conference on Neural Information Processing Systems*, 2023.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2023) Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang
    Song, Shixing Yu, Saad Godil, Ryan Prenger, 和 Anima Anandkumar. LeanDojo: 结合检索增强语言模型的定理证明。在
    *第37届国际神经信息处理系统会议论文集* 中，2023年。'
- en: 'Ye et al. (2023) Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. SATLM:
    Satisfiability-aided language models using declarative prompting. In *Proceedings
    of the 37th International Conference on Neural Information Processing Systems*,
    2023.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye et al. (2023) Xi Ye, Qiaochu Chen, Isil Dillig, 和 Greg Durrett. SATLM: 使用声明性提示的可满足性辅助语言模型。在
    *第37届国际神经信息处理系统会议论文集* 中，2023年。'
- en: 'Yeh et al. (2023) Eric Yeh, Briland Hitaj, Sam Owre, Maena Quemener, and Natarajan
    Shankar. CoProver: A recommender system for proof construction. In *16th International
    Conference on Intelligent Computer Mathematics*, 2023.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yeh et al. (2023) Eric Yeh, Briland Hitaj, Sam Owre, Maena Quemener, 和 Natarajan
    Shankar. CoProver: 一种用于证明构建的推荐系统。在 *第16届国际智能计算数学会议* 中，2023年。'
- en: 'Ying et al. (2024) Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan
    Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. InternLM-Math:
    Open math large language models toward verifiable reasoning. *arXiv preprint arXiv:2402.06332*,
    2024.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ying et al. (2024) Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan
    Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang 等人。InternLM-Math：朝向可验证推理的开放数学大型语言模型。*arXiv
    预印本 arXiv:2402.06332*，2024年。
- en: 'Yousefzadeh & Cao (2023) Roozbeh Yousefzadeh and Xuenan Cao. Large language
    models’ understanding of math: Source criticism and extrapolation. *arXiv preprint
    arXiv:2311.07618*, 2023.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yousefzadeh & Cao (2023) Roozbeh Yousefzadeh 和 Xuenan Cao. 大型语言模型对数学的理解：来源批评与推断。*arXiv
    预印本 arXiv:2311.07618*，2023年。
- en: Zhang et al. (2023a) Liao Zhang, Lasse Blaauwbroek, Cezary Kaliszyk, and Josef
    Urban. Learning proof transformations and its applications in interactive theorem
    proving. In *International Symposium on Frontiers of Combining Systems*, 2023a.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023a) Liao Zhang, Lasse Blaauwbroek, Cezary Kaliszyk, 和 Josef
    Urban. 学习证明转换及其在互动定理证明中的应用。在*国际组合系统前沿研讨会*，2023年。
- en: 'Zhang et al. (2024) Lichen Zhang, Shuai Lu, and Nan Duan. Selene: Pioneering
    automated proof in software verification. *arXiv preprint arXiv:2401.07663*, 2024.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024) Lichen Zhang, Shuai Lu, 和 Nan Duan. Selene：在软件验证中开创自动证明的先河。*arXiv
    预印本 arXiv:2401.07663*，2024年。
- en: Zhang et al. (2023b) Shizhuo Dylan Zhang, Talia Ringer, and Emily First. Getting
    more out of large language models for proofs. In *8th Conference on Artificial
    Intelligence and Theorem Proving*, 2023b.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023b) Shizhuo Dylan Zhang, Talia Ringer, 和 Emily First. 从大型语言模型中获取更多的证明信息。在*第8届人工智能与定理证明会议*，2023年。
- en: 'Zhang et al. (2023c) Xiaokai Zhang, Na Zhu, Yiming He, Jia Zou, Qike Huang,
    Xiaoxiao Jin, Yanjun Guo, Chenyang Mao, Zhe Zhu, Dengfeng Yue, et al. FormalGeo:
    The first step toward human-like imo-level geometric automated reasoning. *arXiv
    preprint arXiv:2310.18021*, 2023c.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023c) Xiaokai Zhang, Na Zhu, Yiming He, Jia Zou, Qike Huang,
    Xiaoxiao Jin, Yanjun Guo, Chenyang Mao, Zhe Zhu, Dengfeng Yue 等人。FormalGeo：迈向类人级几何自动推理的第一步。*arXiv
    预印本 arXiv:2310.18021*，2023年。
- en: 'Zhao et al. (2023) Xueliang Zhao, Wenda Li, and Lingpeng Kong. Decomposing
    the enigma: Subgoal-based demonstration learning for formal theorem proving. *arXiv
    preprint arXiv:2305.16366*, 2023.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2023) Xueliang Zhao, Wenda Li, 和 Lingpeng Kong. 解开谜团：基于子目标的演示学习用于形式化定理证明。*arXiv
    预印本 arXiv:2305.16366*，2023年。
- en: 'Zheng et al. (2023) Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu,
    Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, and Yu Li. Lyra: Orchestrating
    dual correction in automated theorem proving. *arXiv preprint arXiv:2309.15806*,
    2023.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu,
    Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, 和 Yu Li. Lyra：在自动定理证明中协调双重校正。*arXiv
    预印本 arXiv:2309.15806*，2023年。
- en: 'Zheng et al. (2022) Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. MiniF2F:
    A cross-system benchmark for formal olympiad-level mathematics. In *The Tenth
    International Conference on Learning Representations*, 2022.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2022) Kunhao Zheng, Jesse Michael Han, 和 Stanislas Polu. MiniF2F：一个跨系统的正式奥林匹克级数学基准测试。在*第十届国际学习表征会议*，2022年。
- en: 'Zhou et al. (2024a) Jin Peng Zhou, Charles E Staats, Wenda Li, Christian Szegedy,
    Kilian Q Weinberger, and Yuhuai Wu. Don’t trust: Verify–grounding llm quantitative
    reasoning with autoformalization. In *The Twelfth International Conference on
    Learning Representations*, 2024a.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2024a) Jin Peng Zhou, Charles E Staats, Wenda Li, Christian Szegedy,
    Kilian Q Weinberger, 和 Yuhuai Wu. 不要盲目相信：验证–用自动形式化来验证LLM的定量推理。在*第十二届国际学习表征会议*，2024年。
- en: 'Zhou et al. (2024b) Jin Peng Zhou, Yuhuai Wu, Qiyang Li, and Roger Grosse.
    REFACTOR: Learning to extract theorems from proofs. In *The Twelfth International
    Conference on Learning Representations*, 2024b.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2024b) Jin Peng Zhou, Yuhuai Wu, Qiyang Li, 和 Roger Grosse. REFACTOR：学习从证明中提取定理。在*第十二届国际学习表征会议*，2024年。
- en: Zombori et al. (2021a) Zsolt Zombori, Adrián Csiszárik, Henryk Michalewski,
    Cezary Kaliszyk, and Josef Urban. Towards finding longer proofs. In *30th International
    Conference on Automated Reasoning with Analytic Tableaux and Related Methods*,
    2021a.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zombori et al. (2021a) Zsolt Zombori, Adrián Csiszárik, Henryk Michalewski,
    Cezary Kaliszyk, 和 Josef Urban. 朝着寻找更长证明的方向前进。在*第30届国际自动推理与分析表格及相关方法会议*，2021年。
- en: Zombori et al. (2021b) Zsolt Zombori, Josef Urban, and Miroslav Olšák. The role
    of entropy in guiding a connection prover. In *30th International Conference on
    Automated Reasoning with Analytic Tableaux and Related Methods*, 2021b.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zombori et al. (2021b) Zsolt Zombori, Josef Urban, 和 Miroslav Olšák. 熵在引导连接证明器中的作用。在*第30届国际自动推理与分析表格及相关方法会议*，2021年。
