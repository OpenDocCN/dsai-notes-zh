- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:57:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:57:47'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2012.08044] Deep Bayesian Active Learning, A Brief Survey on Recent Advances'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2012.08044] 深度贝叶斯主动学习，近期进展的简要调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2012.08044](https://ar5iv.labs.arxiv.org/html/2012.08044)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2012.08044](https://ar5iv.labs.arxiv.org/html/2012.08044)
- en: Deep Bayesian Active Learning, A Brief Survey on Recent Advances
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度贝叶斯主动学习，近期进展的简要调查
- en: Salman Mohamadi Computer Science and Electrical Engineering
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 萨尔曼·穆罕默迪 计算机科学与电气工程
- en: West Virginia University Morgantown, WV, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 西弗吉尼亚大学 摩根敦, WV, 美国
- en: Sm0224@mix.wvu.edu    Hamidreza Amindavar Electrical Engineering
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Sm0224@mix.wvu.edu    哈米德雷扎·阿敏达瓦尔 电气工程
- en: Amirkabir University of Technology Tehran, Iran
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 阿米尔卡比尔科技大学 德黑兰, 伊朗
- en: hamidami@aut.ac.ir
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: hamidami@aut.ac.ir
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Active learning frameworks offer efficient data annotation without remarkable
    accuracy degradation. In other words, active learning starts training the model
    with a small size of labeled data while exploring the space of unlabeled data
    in order to select most informative samples to be labeled. Generally speaking,
    representing the uncertainty is crucial in any active learning framework, however,
    deep learning methods are not capable of either representing or manipulating model
    uncertainty. On the other hand, from the real world application perspective, uncertainty
    representation is getting more and more attention in the machine learning community.
    Deep Bayesian active learning frameworks and generally any Bayesian active learning
    settings, provide practical consideration in the model which allows training with
    small data while representing the model uncertainty for further efficient training.
    In this paper, we briefly survey recent advances in Bayesian active learning and
    in particular deep Bayesian active learning frameworks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习框架提供了高效的数据标注，同时不会显著降低准确性。换句话说，主动学习从小规模的标记数据开始训练模型，同时探索未标记数据的空间，以选择最有信息量的样本进行标注。一般来说，表示不确定性在任何主动学习框架中都至关重要，然而，深度学习方法既不能表示也不能操控模型的不确定性。另一方面，从实际应用的角度来看，不确定性表示在机器学习社区中越来越受到关注。深度贝叶斯主动学习框架以及一般的贝叶斯主动学习设置，提供了实际的考虑，使得在小数据量下训练模型的同时，能够表示模型的不确定性，从而实现更高效的训练。本文简要回顾了贝叶斯主动学习的最新进展，特别是深度贝叶斯主动学习框架的进展。
- en: 'Index Terms:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Bayesian Active Learning, Deep learning, Posterior estimation, Bayesian inference,
    Semi-supervised learning
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯主动学习, 深度学习, 后验估计, 贝叶斯推断, 半监督学习
- en: I Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: In real life application, while data collection may not be as costly and laborious
    as it was a few decades ago, there are still a lot of considerations that make
    the data annotation process costly and inefficient for actual deployment of many
    machine learning algorithms. Therefore, experiment design and particularly, incorporating
    active learning setting into many of machine problem domains are increasingly
    gaining attention. Active learning is a framework in the area of machine learning
    in which the model starts training by small amount of labeled data and then, in
    a sequential process asks for more data samples from a pool of unlabeled data
    to label and incorporate in the training process. In fact, the key idea behind
    this framework is to achieve desired accuracy while lowering the cost of labeling
    by efficiently asking for more labeling most informative data samples. Therefore,
    compared to many other relevant frameworks, active learning tries to incorporate
    the uncertainty representation to achieve the same or higher accuracy by using
    smaller amount of labeled data. Other than the uncertainty coming from noisy data
    samples, there are two major types of uncertainty, the uncertainty associated
    with the best model parameters, and the uncertainty associated with best network
    structure [[1](#bib.bib1), [2](#bib.bib2)]. In fact we might find multiple models
    and/or different structures which provide well representation for the data, however,
    we may face uncertainty on selecting the one with highest performance on further
    predictions or generalization. In contrast, similar frameworks such as semi-supervised
    learning addresses relatively similar problem domains, however, there are differences
    between their learning paradigm. In more detain, semi-supervised learning frameworks
    use the unlabeled data for feature representations in order to better model the
    labeled data [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]. Classical methods
    and tools in signal processing area, with an emphasis on parametric modeling,
    have been used in many areas with different type of data [[7](#bib.bib7), [9](#bib.bib9),
    [8](#bib.bib8)]. However recent advances in machine learning and in particular,
    artificial neural network, have shown that non-parametric models, are capable
    of almost modeling any type of data at the cost of higher complexity. In this
    line, deep learning could be incorporated with classical tools and frameworks
    of machine learning such as active learning in order to address a wider range
    of problems while improving the performance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，虽然数据收集可能不像几十年前那样昂贵和费力，但数据标注过程仍然存在许多考虑因素，这使得在实际部署许多机器学习算法时成本高且效率低。因此，实验设计，尤其是将主动学习设置纳入许多机器问题领域，正越来越受到关注。主动学习是机器学习领域中的一种框架，其中模型开始时用少量标记数据进行训练，然后在一个顺序过程中从未标记数据池中请求更多数据样本进行标注，并将其纳入训练过程。实际上，该框架背后的关键思想是通过高效地请求更多最具信息性的标注数据样本来实现期望的准确性，同时降低标注成本。因此，与许多其他相关框架相比，主动学习尝试通过使用更少的标记数据来结合不确定性表示，以实现相同或更高的准确性。除了来自噪声数据样本的不确定性外，还有两种主要的不确定性类型：与最佳模型参数相关的不确定性，以及与最佳网络结构相关的不确定性[[1](#bib.bib1),
    [2](#bib.bib2)]。实际上，我们可能会发现多个模型和/或不同的结构能够很好地表示数据，但我们可能会面临选择在进一步预测或泛化中性能最高的那个模型的不确定性。相比之下，类似的框架，如半监督学习，解决的是相对类似的问题领域，但它们的学习范式之间存在差异。更详细地说，半监督学习框架利用未标记数据进行特征表示，以更好地建模标记数据[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]。在信号处理领域，强调参数建模的经典方法和工具已在不同类型的数据领域得到应用[[7](#bib.bib7),
    [9](#bib.bib9), [8](#bib.bib8)]。然而，最近在机器学习特别是人工神经网络方面的进展表明，非参数模型几乎能够建模任何类型的数据，尽管这需要更高的复杂性。在这方面，深度学习可以与机器学习的经典工具和框架如主动学习结合，以解决更广泛的问题，同时提升性能。
- en: On the other side, representing the uncertainty of either embedding space or
    output probability space is challenging where we are going to use deep learning
    tools and concepts. It would show up in multiple scenarios in which we need to
    measure the model uncertainty such as problems addressed by classical active learning
    or its various versions with the similar learning paradigm. In fact, exploiting
    model uncertainty is essential for several problem domains concerning with learning
    from small amount of data. For instance, it is necessary to develop a desired
    output probability space in a typical active learning framework, which necessitates
    the model uncertainty measurement and representation[[28](#bib.bib28)]. Bayesian
    methods, here can play an important role to capture the underlying model uncertainty.
    Original idea of incorporating active learning with neural networks has been proposed
    and assessed few decades ago [[25](#bib.bib25)], however in recent years, more
    specific efforts were performed to introduce deep learning tools to active learning
    framework. In this regard, in order to better understand the addressed problem
    domain, it is crucial to ponder upon the associated difficulties. Authors of [[6](#bib.bib6)],
    in their work as a pioneered effort, discussed that bringing deep learning tools
    into active learning setting poses two major problem; uncertainty representation
    and the amount of data possibly needed to train the model. In next two sections,
    in order to have a taste of the basic concepts and some theoretical definitions,
    a brief overview of the learning paradigm of active learning and Bayesian convolutional
    and recurrent neural networks will be presented.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，表示嵌入空间或输出概率空间的不确定性是具有挑战性的，我们将使用深度学习工具和概念。这种情况会出现在多个场景中，我们需要测量模型的不确定性，例如经典主动学习或其各种版本中解决的问题。事实上，利用模型不确定性对涉及从少量数据中学习的多个问题领域至关重要。例如，在典型的主动学习框架中，开发期望的输出概率空间是必要的，这需要对模型不确定性的测量和表示[[28](#bib.bib28)]。贝叶斯方法在捕捉潜在模型不确定性方面可以发挥重要作用。将主动学习与神经网络结合的最初想法在几十年前就已提出并评估[[25](#bib.bib25)]，然而近年来，更多的具体努力被用来将深度学习工具引入主动学习框架。在这方面，为了更好地理解所涉及的问题领域，思考相关的困难至关重要。[[6](#bib.bib6)]的作者在他们的开创性工作中讨论了将深度学习工具引入主动学习环境中面临的两个主要问题：不确定性表示和可能需要的数据量。在接下来的两个部分中，为了了解基本概念和一些理论定义，将简要概述主动学习和贝叶斯卷积神经网络及递归神经网络的学习范式。
- en: '![Refer to caption](img/8a4deb329459e56202f3bd2b43625929.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8a4deb329459e56202f3bd2b43625929.png)'
- en: 'Figure 1: Learning paradigm of active learning; as it is shown, at every iteration
    the training starts from scratch on modified training data.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：主动学习的学习范式；如图所示，在每次迭代中，训练从修改后的训练数据中重新开始。
- en: II Learning Paradigm in Active Learning
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 主动学习中的学习范式
- en: Simply put, the goal of active learning is to minimize the cost of data annotation
    or labeling by efficiently selecting the unlabeled data to be labeled. In more
    detail, in every iteration of active learning, a new labeled data sample (or even
    batch of data) will be added to the training data, and the training process starts
    from scratch. This sequential training will continue until either the accuracy
    reaches to the desired level, or the entire labeling budget be used.The overview
    of the learning cycle of active learning is shown in Fig.[1](#S1.F1 "Figure 1
    ‣ I Introduction ‣ Deep Bayesian Active Learning, A Brief Survey on Recent Advances").
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，主动学习的目标是通过高效选择待标记的未标记数据来最小化数据注释或标记的成本。更详细地说，在每次主动学习的迭代中，会将一个新的标记数据样本（或甚至是数据批次）添加到训练数据中，并且训练过程从头开始。这个顺序训练将继续进行，直到准确率达到期望水平，或者整个标记预算用完。主动学习的学习周期概述如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Bayesian Active Learning, A Brief Survey on
    Recent Advances")所示。
- en: In each iteration, all of the unlabeled data samples will be evaluated to select
    the most informative sample. Selection of these samples is performed by a functions
    named acquisition function which deals with classification uncertainty. The uncertainty
    presented in classification could be measured in terms of predictive entropy,
    variation ration and mutual information [[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)].
    Generally speaking, we make use of acquisition function in active learning framework
    either for regression or classification purposes. In case of regression, sample
    variance is the criterion for building and acquisition function. However in case
    of classification, acquisition functions could be designed based on, maximum predictive
    entropy-based acquisition,acquisition based on maximum mutual information for
    predictions and model posterior, maximum variation ratios acquisition or simply
    random acquisition as a baseline [[28](#bib.bib28), [29](#bib.bib29)]. Hence,
    we could think of acquisition functions as a function performing uncertainty sampling,
    or diversity sampling or both of them. For the task of image classification, most
    popular functions are presented and approximately formulated in [[6](#bib.bib6)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，所有未标记的数据样本将被评估以选择最有信息量的样本。这些样本的选择由一个称为获取函数的函数来执行，该函数处理分类的不确定性。分类中的不确定性可以通过预测熵、变异比率和互信息来度量
    [[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)]。一般来说，我们在主动学习框架中使用获取函数，无论是用于回归还是分类目的。在回归的情况下，样本方差是构建和获取函数的标准。然而，在分类的情况下，获取函数可以基于最大预测熵的获取、基于最大互信息的获取、最大变异比率的获取或简单的随机获取作为基线来设计
    [[28](#bib.bib28), [29](#bib.bib29)]。因此，我们可以将获取函数视为执行不确定性采样、或多样性采样，或两者兼而有之的函数。在图像分类任务中，最受欢迎的函数在
    [[6](#bib.bib6)] 中进行了介绍和大致公式化。
- en: III Projecting Uncertainty in the Models
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 模型中的不确定性投影
- en: III-A Convolutional Neural Networks with Bayesian Prior
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 带有贝叶斯先验的卷积神经网络
- en: Nowadays, deep learning algorithms mostly rely on training convolutional neural
    networks (CNNs). In fact with the recent advancement of CNNs, one of the main
    advantages of CNNs is that they enable capturing spatial information of the image
    data [[10](#bib.bib10)]. However, Bayesian learning concepts initially were introduced
    to simple versions of neural networks. In fact first attempt on developing a special
    type of neural networks (NNs) named Bayesian NNs, dates back to a work presented
    by reference [[30](#bib.bib30)] more than three decades ago. Their goal was to
    put a prior distribution over the weights of NN in order to develop a mapping
    framework between each specific setting of weights and its corresponding sets
    of outputs. This idea gradually evolved into more complex learning frameworks
    such as Bayesian CNNs. In essence, practical implementation of Bayesian CNNs requires
    the measurement of the model predictive posterior on the test data. Theoretically
    speaking, first each kernel of CNN is set to follow a prior distribution and next,
    a Bernoulli variational distribution is applied to the resultant kernel-patch
    pair of the image; in other words the product of Bernoulli random variables and
    weight matrix is applied to each patch of the image individually. Therefore, some
    of the patches of image could be multiplied by kernels set to zero. In terms of
    practical implementation though, it is approximately implemented by performing
    dropout after every convolutional and fully-connected layer [[28](#bib.bib28),
    [11](#bib.bib11)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，深度学习算法大多依赖于训练卷积神经网络（CNNs）。事实上，随着CNNs的最新进展，CNNs的主要优势之一是能够捕捉图像数据的空间信息 [[10](#bib.bib10)]。然而，贝叶斯学习概念最初是引入到简单版本的神经网络中。实际上，开发一种特殊类型的神经网络（NNs）——贝叶斯NNs的第一次尝试可以追溯到三十多年前的参考文献
    [[30](#bib.bib30)]。他们的目标是对神经网络的权重施加先验分布，以便在每种特定的权重设置与其对应的输出集之间建立映射框架。这个想法逐渐演变为更复杂的学习框架，例如贝叶斯CNNs。从本质上讲，贝叶斯CNNs的实际实施需要在测试数据上测量模型的预测后验。理论上，首先，每个CNN的卷积核都设置为遵循先验分布，然后，将伯努利变分分布应用于图像的卷积核-图像块对；换句话说，伯努利随机变量与权重矩阵的乘积被应用于图像的每个图像块。因此，图像的一些图像块可能会与设置为零的卷积核相乘。不过，从实际实现的角度来看，这通常是通过在每个卷积层和全连接层后执行丢弃操作来近似实现的
    [[28](#bib.bib28), [11](#bib.bib11)]。
- en: 'Based on above discussion, authors of [[11](#bib.bib11)] proposed a new version
    of CNNs with Bayesian prior on a set of weights,i.e., Gaussian prior ${p(w);w}$
    is ${\{W_{1},W_{2},...W_{N}\}}$. In their work, Bayesian CNNs for classification
    tasks with a softmax layer is formulated with a likelihood model as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述讨论，[[11](#bib.bib11)] 的作者提出了一个新的 CNN 版本，其中对一组权重应用 Bayesian 先验，即 Gaussian
    先验 ${p(w);w}$ 是 ${\{W_{1},W_{2},...W_{N}\}}$。在他们的工作中，为分类任务制定的 Bayesian CNN 带有
    softmax 层，其似然模型如下：
- en: '|  | $\displaystyle p(y=c&#124;x,w)=softmax(f^{w}(x)).$ |  | (1) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(y=c\mid x,w)=softmax(f^{w}(x)).$ |  | (1) |'
- en: 'As above discussed briefly, in order to practically implement such models,
    the Gal et.al [[6](#bib.bib6)] suggest approximate inference using stochastic
    regularization techniques such as dropout or multiplicative Guassian noise. They
    performed it by utilizing dropout during the training as well as the test process
    to estimating the posterior over the test process. In more detail, such work is
    feasible by finding a distribution, namely ${q_{\theta}^{*}(w)}$ which given a
    set of training data ${D}$, minimizes the Kullback-Leibler (KL) divergence between
    estimated posterior and exact posterior ${p(w|D)}$. Finally, as it is common,
    using Mont Carlo integration for such variational inference, we will have:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，为了实际实现这些模型，Gal 等人 [[6](#bib.bib6)] 建议使用随机正则化技术，如 dropout 或乘法 Gaussian 噪声进行近似推断。他们通过在训练和测试过程中使用
    dropout 来估计测试过程中的后验分布。更详细地说，通过找到一个分布，即 ${q_{\theta}^{*}(w)}$，在给定一组训练数据 ${D}$ 的情况下，最小化估计后验和精确后验
    ${p(w|D)}$ 之间的 Kullback-Leibler (KL) 散度。这项工作通常通过蒙特卡罗积分来进行变分推断，我们得到：
- en: '|  | $\displaystyle p(y=c&#124;x,D)=\int p(y=c&#124;x,w)p(w&#124;D)dw$ |  |
    (2) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(y=c\mid x,D)=\int p(y=c\mid x,w)p(w\mid D)dw$ |  | (2)
    |'
- en: '|  | $\displaystyle\approx\int p(y=c&#124;x,w)q_{\theta}^{*}(w)dw$ |  | (3)
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\approx\int p(y=c\mid x,w)q_{\theta}^{*}(w)dw$ |  | (3)
    |'
- en: '|  | $\displaystyle\approx\frac{1}{T}\sum_{t=1}^{T}p(y=c&#124;x,\hat{w}_{t}),$
    |  | (4) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\approx\frac{1}{T}\sum_{t=1}^{T}p(y=c\mid x,\hat{w}_{t}),$
    |  | (4) |'
- en: with ${q_{\theta}(w)}$ as dropout distribution and ${\hat{w}_{t}}$ as estimation
    of ${q_{\theta}^{*}}$ [[6](#bib.bib6)].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${q_{\theta}(w)}$ 是 dropout 分布，${\hat{w}_{t}}$ 是 ${q_{\theta}^{*}}$ 的估计 [[6](#bib.bib6)]。
- en: III-B Recurrent Neural Networks with Bayesian Prior
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 带有 Bayesian 先验的递归神经网络
- en: Similar to what is performed on CNN to make it into Bayesian CNN, recurrent
    neural networks (RNNs) could be modified in terms of model uncertainty.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于将 CNN 转变为 Bayesian CNN 的方法，递归神经网络（RNNs）也可以在模型不确定性方面进行修改。
- en: IV Review on Recent advances
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 最近进展回顾
- en: Bayesian inference methods allow the introduction of probabilistic framework
    to machine learning and deep learning. The notion behind the introduction of these
    kind of frameworks to machine learning is that learning from data would be treated
    as inferring optimal or near optimal models for data representation, such as automatic
    model discovery. In this sense, Bayesian methods and here, specifically Bayesian
    active learning methods gain attention due to their ability for uncertainty representation
    and even better generalization on small amount of data [[20](#bib.bib20)]. One
    of the main work on introduction of model uncertainty measurement and manipulation
    to active learning is done by Gal and Ghahramani [[6](#bib.bib6)]. In fact the
    major contribution of this paper is special introduction of Bayesian uncertainty
    estimation to active learning in order to form a deep active learning framework.
    In more detail, deep learning tools are data hungry while active learning tends
    to use small amount of data, moreover, generally speaking deep learning is no
    suitable for uncertainty representation while active learning relies on model
    uncertainty measurement or even manipulation. Understanding these big natural
    differences, authors of this paper found the Bayesian approach to be the solution.
    In fact they refine the active learning general framework, which usually work
    with SVM and small amount of data, to be well scaled to high dimensional data
    such as images in the case of big data. in contrast to small data. It practice,
    the authors put a Bayesian prior on the kernels of a convolutional neural network
    as the training engine of active learning framework. They refer to their previous
    work [[21](#bib.bib21)] suggesting that in order to have a practical Bayesian
    CNN, the Bayesian inference could be done through approximate inference in the
    Bayesian CNN, which makes the solution computationally tractable. The interesting
    point is that they empirically showed that dropout is a Bayesian approximation
    which can be used as a way to introduce uncertainty to deep learning [[22](#bib.bib22)].
    Here the point is that dropout is not only used in the training process, i.e.,
    they do inference applying dropout before every weight layer during training,
    and also during test to sample from the approximate posterior. This framework
    compared to other active learning methods addressing big data for image, such
    as those using RBF, performs better.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯推断方法允许将概率框架引入机器学习和深度学习。这种框架引入机器学习的概念在于将从数据中学习视为推断数据表示的最佳或近似最佳模型，例如自动模型发现。从这个意义上说，贝叶斯方法，特别是贝叶斯主动学习方法，由于其不确定性表示能力和在小数据量上更好的泛化能力而受到关注[[20](#bib.bib20)]。在将模型不确定性测量和操控引入主动学习方面的主要工作是由Gal和Ghahramani完成的[[6](#bib.bib6)]。实际上，本文的主要贡献是将贝叶斯不确定性估计特别引入主动学习，以形成一个深度主动学习框架。更详细地说，深度学习工具需要大量数据，而主动学习倾向于使用少量数据。此外，通常来说，深度学习不适合不确定性表示，而主动学习依赖于模型不确定性测量甚至操控。理解这些显著的自然差异，本文的作者发现贝叶斯方法是解决方案。实际上，他们将主动学习的通用框架进行精炼，这个框架通常与SVM和少量数据一起工作，优化为适合处理高维数据，如大数据中的图像，与小数据对比。在实践中，作者将贝叶斯先验应用于卷积神经网络的核函数，作为主动学习框架的训练引擎。他们提到他们之前的工作[[21](#bib.bib21)]，建议为了拥有一个实际的贝叶斯CNN，可以通过贝叶斯CNN中的近似推断来进行贝叶斯推断，这使得解决方案在计算上是可处理的。有趣的是，他们通过实验证明，dropout是一种贝叶斯近似，可以用作将不确定性引入深度学习的方式[[22](#bib.bib22)]。这里的要点是，dropout不仅用于训练过程，即在训练期间每个权重层之前应用dropout进行推断，还用于测试期间从近似后验中进行采样。与其他针对大数据图像的主动学习方法（如使用RBF的方法）相比，这种框架表现更好。
- en: In this line, Jedoui et. al. [[23](#bib.bib23)] even go further in the level
    of uncertainty of model by assuming that the output space is no longer mutually
    exclusive, for instance we have more that one output for a single input. They
    empirically show that classical uncertainty sampling does not perform better than
    random sampling at these sort of tasks such as Visual Question Answering, therefore
    they refer to [[6](#bib.bib6), [18](#bib.bib18), [21](#bib.bib21), [19](#bib.bib19)]
    take a similar strategy by using Bayesian uncertainty in a semantically structured
    embedding space rather than modeling uncertainty of the output probability space.
    Referring to Gal and Ghahramani’s works, they mention that dropout can be interpreted
    as a variational Bayesian approximation [[21](#bib.bib21), [22](#bib.bib22)],
    where the approximating distribution is a mixture of two Gaussians with small
    variances which the mean of one of the Gaussians is zero. The prediction uncertainty
    caused by uncertainty in the weights which could be measured by approximate posterior
    using Monte Carlo integration.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一行中，Jedoui 等人 [[23](#bib.bib23)] 进一步探讨了模型不确定性的层面，他们假设输出空间不再是互斥的，例如，对于单个输入我们可能有多个输出。他们实证表明，经典的不确定性采样在诸如视觉问答这类任务中表现不如随机采样，因此他们提到
    [[6](#bib.bib6)、[18](#bib.bib18)、[21](#bib.bib21)、[19](#bib.bib19)] 采用了类似的策略，通过在语义结构化的嵌入空间中使用贝叶斯不确定性，而不是对输出概率空间的不确定性进行建模。参考
    Gal 和 Ghahramani 的研究，他们提到 dropout 可以被解释为一种变分贝叶斯近似 [[21](#bib.bib21)、[22](#bib.bib22)]，其中近似分布是两个方差较小的高斯分布的混合，其中一个高斯分布的均值为零。由权重的不确定性引起的预测不确定性可以通过使用蒙特卡罗积分的近似后验来衡量。
- en: Authors of reference [[24](#bib.bib24)] poses another similar problem by introducing
    deep learning with relatively very large amount of data and big network into active
    learning; and suggesting the necessity of systematic request for labeling in the
    form of batch active learning (batch rather than sample in each active learning
    iteration). They offer batch active learning in order to address the problem that
    existing greedy algorithms become computationally costly and sensitive to the
    model slightest changes. The authors propose a model aimed at efficiently scaled
    active learning by well estimating data posterior. They suggest scenarios in which
    more efficiency comes with one batch rather than one data sample at each iteration.
    In this paper, authors take multiple active learning methods, different acquisition
    functions, into account for their objective of efficient batch selection in the
    sense of sparsity, or sparse subset approximation. Moreover, they claim that based
    on their experiments, that reference [[6](#bib.bib6)], as a Bayesian approach,
    outperforms others in many problem setting. More specifically, with the same Bayesian
    active learning framework proposed by [[6](#bib.bib6)] for capturing uncertainty,
    they target the most optimum batch selection by finding data posterior, however
    as active learning setting does not provide access to the labels before querying
    the pool set, they take expectation w.r.t. the current predictive posterior distribution.
    This work represents a closed-form solution consistent with basic theoretical
    setting of reference [[6](#bib.bib6)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献 [[24](#bib.bib24)] 的作者提出了另一个类似的问题，即将深度学习与相对大量的数据和大网络引入到主动学习中，并建议以批量主动学习的形式系统性地请求标注（即每次主动学习迭代中不是样本而是批量）。他们提供了批量主动学习，以解决现有贪婪算法在计算上变得昂贵且对模型微小变化敏感的问题。作者提出了一种旨在通过良好估计数据后验来高效扩展主动学习的模型。他们建议，在每次迭代中用一个批量而不是一个数据样本可以带来更高的效率。在本文中，作者考虑了多种主动学习方法和不同的获取函数，以实现稀疏性或稀疏子集逼近的高效批量选择。
    此外，他们声称，根据他们的实验，参考文献 [[6](#bib.bib6)] 作为一种贝叶斯方法，在许多问题设置中优于其他方法。更具体地说，利用 [[6](#bib.bib6)]
    提出的相同贝叶斯主动学习框架来捕捉不确定性，他们通过寻找数据后验来针对最优批量选择，然而由于主动学习设置在查询池集之前无法访问标签，他们基于当前预测后验分布进行期望。这项工作代表了与参考文献
    [[6](#bib.bib6)] 基本理论设置一致的封闭形式解。
- en: Gal and Ghahramani [[11](#bib.bib11)] suggest a Bernoulli approximate variational
    inference method, which prevents from CNNs over-fitting, i.e., by considering
    a Bayesian prior on the weights of the network, it will become capable of learning
    from small data, with no over-fitting or higher computational complexity. This
    work can be considered as one of the basics of developing deep Bayesian active
    learning. Authors of [[12](#bib.bib12)] with an emphasis on the fact that the
    nature of active learning does not allows thorough comparison of models and acquisition
    functions, explore more than 4 experiments of different models and acquisition
    functions for multiple tasks of natural language processing, and finally show
    that deep Bayesian active learning consistently provides the best performance.
    Kandasamy et. al[[13](#bib.bib13)] underscore the fact that classical methods
    for posterior estimation are query inefficient in the sense of estimating likelihood.
    They suggest that a query efficient approach would be posterior approximation
    using Bayesian active learning framework. Considering a Gaussian prior, a utility
    function as a measure of divergence between probabilities (here densities)is formed
    and at each time step, the estimated most informative query would be sent to an
    oracle. Then the posterior will be updated. Their experiment confirms the query
    efficiency of the approach. Reference [[14](#bib.bib14)] suggests that since most
    of the methods of distance metric learning are sensitive to the size of the data
    and randomly select the training pairs, they could not return satisfactory results
    in many real world problems. The authors address this problem by firstly introducing
    Bayesian approach to distance metric learning, and then developing this framework
    by uncertainty modeling, which ends up in a Bayesian framework for active distance
    metric learning. This framework enables efficient pair selection for training
    as well as posterior probability approximation. Reference [[15](#bib.bib15)] tries
    to combine the benefits of Bayesian active learning and semi-supervised learning
    by introducing a active expectation maximization framework with pseudo-labels.
    Authors use Mont Carlo dropout introduce in [[6](#bib.bib6)] to compute the probability
    outputs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Gal 和 Ghahramani [[11](#bib.bib11)] 提出了伯努利近似变分推断方法，这可以防止 CNN 过拟合，即通过在网络权重上考虑贝叶斯先验，它将能够从小数据中学习，而不会出现过拟合或更高的计算复杂性。这项工作可以被视为开发深度贝叶斯主动学习的基础之一。[[12](#bib.bib12)]
    的作者强调，主动学习的本质不允许彻底比较模型和获取函数，探索了用于多个自然语言处理任务的不同模型和获取函数的四个以上实验，并最终表明深度贝叶斯主动学习始终提供最佳性能。Kandasamy
    等人[[13](#bib.bib13)] 强调了经典的后验估计方法在估计可能性时的查询效率问题。他们建议查询效率高的方法是使用贝叶斯主动学习框架进行后验近似。考虑到高斯先验，形成一个作为概率（此处为密度）之间的差异度量的效用函数，并在每个时间步骤，将估计的最有信息量的查询发送到一个oracle。然后更新后验。实验确认了该方法的查询效率。参考文献
    [[14](#bib.bib14)] 指出，由于大多数距离度量学习方法对数据的大小敏感并随机选择训练对，因此在许多实际问题中无法返回令人满意的结果。作者通过首先引入贝叶斯距离度量学习方法，然后通过不确定性建模来开发这个框架，最终形成了一个用于主动距离度量学习的贝叶斯框架。该框架实现了有效的训练对选择以及后验概率近似。参考文献
    [[15](#bib.bib15)] 尝试通过引入带有伪标签的主动期望最大化框架，结合贝叶斯主动学习和半监督学习的优点。作者使用在 [[6](#bib.bib6)]
    中引入的 Mont Carlo dropout 来计算概率输出。
- en: '[[32](#bib.bib32), [31](#bib.bib31)] present innovative strategies for addressing
    the issues of sampling and visualizing high-dimensional unbalanced datasets to
    reduce computing complexity and memory utilization while preserving accuracy.
    This could be used as an auxiliary technique for deep active learning frameworks.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[[32](#bib.bib32), [31](#bib.bib31)] 提出了针对高维不平衡数据集的采样和可视化问题的创新策略，以减少计算复杂性和内存使用，同时保持准确性。这可以作为深度主动学习框架的辅助技术。'
- en: Zeng et. al [[16](#bib.bib16)] address the question that is it possible to measure
    the model uncertainty without fully Bayesian CNNs. Their results on several Bayesian
    CNNs confirm that in order to represent the model uncertainty, one needs to apply
    the Bayesian prior on only a few last layer before the output. With this setting,
    the model would enjoy the benefits of both deterministic and Bayesian CNNs. Lewenberg
    et. al [[17](#bib.bib17)] address the problem of active surveying using a Bayesian
    active learner. In fact they use dimensionality reduction in an active learning
    framework by applying Bayesian prior in order to design a system to predict the
    answers to unasked question using a limited sequential active question asking.
    Their framework outperform several state of-the-art frameworks based on enhanced
    linear regression in terms of prediction accuracy and response to missing data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Zeng 等人 [[16](#bib.bib16)] 探讨了是否可以在没有完全贝叶斯 CNN 的情况下测量模型不确定性。他们对几种贝叶斯 CNN 的结果证实，为了表示模型的不确定性，需要将贝叶斯先验应用于输出之前的最后几层。通过这种设置，模型可以享受确定性和贝叶斯
    CNN 的双重好处。Lewenberg 等人 [[17](#bib.bib17)] 解决了使用贝叶斯主动学习者进行主动调查的问题。实际上，他们在主动学习框架中通过应用贝叶斯先验来进行降维，以设计一个系统来预测未提问的问题的答案，使用有限的顺序主动提问。他们的框架在预测准确性和对缺失数据的响应方面优于基于增强线性回归的几种最先进框架。
- en: Houlsby et. al.[[18](#bib.bib18)] propose a measurement of predictive entropy
    which later is used in a classification framework based on Gaussian Process. Their
    method performs well compared to several similar classification frameworks while
    the computational complexity is not greater than other methods. Finally they develop
    their framework to a Gaussian process preference learning by extension of binary
    preference learning to classification setting. One of the main advantages of this
    method is that it provides desired accuracy at a relatively low computational
    complexity cost.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Houlsby 等人 [[18](#bib.bib18)] 提出了预测熵的测量方法，随后在基于高斯过程的分类框架中使用。他们的方法在计算复杂度不高于其他方法的情况下，与几种类似的分类框架相比表现良好。最后，他们通过将二元偏好学习扩展到分类设置，将其框架发展为高斯过程偏好学习。该方法的主要优势之一是，它在相对较低的计算复杂度成本下提供了所需的准确性。
- en: V Future trends
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 未来趋势
- en: VI Conclusion
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: In this paper, we surveyed recent advances in Bayesian active learning, with
    an emphasis on deep Bayesian active learning. Our main focus in on the works contributing
    to the theory of this problem domain, however some interesting works on the application
    of Bayesian active learning are surveyed. Bayesian inference approaches hold very
    important place in machine learning and recently, many attentions have shifted
    toward data, model and even network structure uncertainty representation using
    these approaches. Bayesian active learning and its intersection with deep learning
    concepts provide very interesting frameworks in terms of theroy and application.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们调查了贝叶斯主动学习的最新进展，重点关注深度贝叶斯主动学习。我们的主要关注点是对该问题领域理论的贡献工作，但也调查了一些关于贝叶斯主动学习应用的有趣工作。贝叶斯推断方法在机器学习中占据了非常重要的地位，最近，许多关注已转向使用这些方法表示数据、模型甚至网络结构的不确定性。贝叶斯主动学习及其与深度学习概念的交叉提供了非常有趣的理论和应用框架。
- en: References
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Settles, B. (2009). Active learning literature survey. University of Wisconsin-Madison
    Department of Computer Sciences.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Settles, B. (2009). 主动学习文献综述。威斯康星大学麦迪逊分校计算机科学系。'
- en: '[2] Sener, O., & Savarese, S. (2017). Active learning for convolutional neural
    networks: A core-set approach. arXiv preprint arXiv:1708.00489.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Sener, O., & Savarese, S. (2017). 卷积神经网络的主动学习：核心集方法。arXiv 预印本 arXiv:1708.00489。'
- en: '[3] Taherkhani, Fariborz, Nasser M. Nasrabadi, and Jeremy Dawson. ”A deep face
    identification network enhanced by facial attributes prediction.” In Proceedings
    of the IEEE conference on computer vision and pattern recognition workshops, pp.
    553-560\. 2018.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Taherkhani, Fariborz, Nasser M. Nasrabadi, 和 Jeremy Dawson. “通过面部属性预测增强的深度面部识别网络。”
    发表在 IEEE 计算机视觉与模式识别会议论文集中，第 553-560 页，2018年。'
- en: '[4] Taherkhani, Fariborz, Hadi Kazemi, and Nasser M. Nasrabadi. ”Matrix completion
    for graph-based deep semi-supervised learning.” In Proceedings of the AAAI Conference
    on Artificial Intelligence, vol. 33, pp. 5058-5065\. 2019.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Taherkhani, Fariborz, Hadi Kazemi, 和 Nasser M. Nasrabadi. “基于图的深度半监督学习的矩阵补全。”
    发表在 AAAI 人工智能会议论文集中，第 33 卷，第 5058-5065 页，2019年。'
- en: '[5] Taherkhani, Fariborz, Hadi Kazemi, Ali Dabouei, Jeremy Dawson, and Nasser
    M. Nasrabadi. ”A weakly supervised fine label classifier enhanced by coarse supervision.”
    In Proceedings of the IEEE International Conference on Computer Vision, pp. 6459-6468\.
    2019.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Taherkhani, Fariborz, Hadi Kazemi, Ali Dabouei, Jeremy Dawson, 和 Nasser
    M. Nasrabadi. ”通过粗略监督增强的弱监督细标签分类器。” 见于IEEE国际计算机视觉会议论文集，第6459-6468页。2019.'
- en: '[6] Gal, Yarin, Riashat Islam, and Zoubin Ghahramani. ”Deep bayesian active
    learning with image data.” arXiv preprint arXiv:1703.02910 (2017).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Gal, Yarin, Riashat Islam, 和 Zoubin Ghahramani. ”基于图像数据的深度贝叶斯主动学习。” arXiv预印本
    arXiv:1703.02910 (2017).'
- en: '[7] Mohamadi, Salman, Hamidreza Amindavar, and SM Ali Tayaranian Hosseini.
    ”Arima-garch modeling for epileptic seizure prediction.” In 2017 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 994-998\.
    IEEE, 2017.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Mohamadi, Salman, Hamidreza Amindavar, 和 SM Ali Tayaranian Hosseini. ”用于癫痫发作预测的Arima-garch建模。”
    见于2017 IEEE国际声学、语音与信号处理会议（ICASSP），第994-998页。IEEE, 2017.'
- en: '[8] Mohamadi, Salman, Farhang Yeganegi, and Nasser M. Nasrabadi. ”Detection
    and Statistical Modeling of Birth-Death Anomaly.” arXiv preprint arXiv:1906.11788
    (2019).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Mohamadi, Salman, Farhang Yeganegi, 和 Nasser M. Nasrabadi. ”出生-死亡异常的检测与统计建模。”
    arXiv预印本 arXiv:1906.11788 (2019).'
- en: '[9] Mohamadi, Salman, Farhang Yeganegi, and Hamidreza Amindavar. ”A New Framework
    For Spatial Modeling And Synthesis of Genome Sequence.” arXiv preprint arXiv:1908.03342
    (2019).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Mohamadi, Salman, Farhang Yeganegi, 和 Hamidreza Amindavar. ”用于基因组序列空间建模与合成的新框架。”
    arXiv预印本 arXiv:1908.03342 (2019).'
- en: '[10] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ”Imagenet classification
    with deep convolutional neural networks.” Communications of the ACM 60, no. 6
    (2017): 84-90.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Krizhevsky, Alex, Ilya Sutskever, 和 Geoffrey E. Hinton. ”使用深度卷积神经网络进行ImageNet分类。”
    ACM通讯 60, no. 6 (2017): 84-90.'
- en: '[11] Gal, Yarin, and Zoubin Ghahramani. ”Bayesian convolutional neural networks
    with Bernoulli approximate variational inference.” arXiv preprint arXiv:1506.02158
    (2015).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Gal, Yarin, 和 Zoubin Ghahramani. ”带有伯努利近似变分推断的贝叶斯卷积神经网络。” arXiv预印本 arXiv:1506.02158
    (2015).'
- en: '[12] Siddhant, Aditya, and Zachary C. Lipton. ”Deep bayesian active learning
    for natural language processing: Results of a large-scale empirical study.” arXiv
    preprint arXiv:1808.05697 (2018).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Siddhant, Aditya, 和 Zachary C. Lipton. ”自然语言处理中的深度贝叶斯主动学习：大规模实证研究结果。”
    arXiv预印本 arXiv:1808.05697 (2018).'
- en: '[13] Kandasamy, Kirthevasan, Jeff Schneider, and Barnabás Póczos. ”Bayesian
    active learning for posterior estimation.” In Proceedings of the 24th International
    Conference on Artificial Intelligence, pp. 3605-3611\. 2015.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Kandasamy, Kirthevasan, Jeff Schneider, 和 Barnabás Póczos. ”贝叶斯主动学习用于后验估计。”
    见于第24届国际人工智能会议论文集，第3605-3611页。2015.'
- en: '[14] Yang, Liu, Rong Jin, and Rahul Sukthankar. ”Bayesian active distance metric
    learning.” arXiv preprint arXiv:1206.5283 (2012).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Yang, Liu, Rong Jin, 和 Rahul Sukthankar. ”贝叶斯主动距离度量学习。” arXiv预印本 arXiv:1206.5283
    (2012).'
- en: '[15] Matthias, Rottmann, Kahl Karsten, and Gottschalk Hanno. ”Deep bayesian
    active semi-supervised learning.” In 2018 17th IEEE International Conference on
    Machine Learning and Applications (ICMLA), pp. 158-164\. IEEE, 2018.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Matthias, Rottmann, Kahl Karsten, 和 Gottschalk Hanno. ”深度贝叶斯主动半监督学习。”
    见于2018年第17届IEEE机器学习与应用国际会议（ICMLA），第158-164页。IEEE, 2018.'
- en: '[16] Zeng, Jiaming, Adam Lesnikowski, and Jose M. Alvarez. ”The relevance of
    Bayesian layer positioning to model uncertainty in deep Bayesian active learning.”
    arXiv preprint arXiv:1811.12535 (2018).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Zeng, Jiaming, Adam Lesnikowski, 和 Jose M. Alvarez. ”贝叶斯层定位对深度贝叶斯主动学习中模型不确定性的相关性。”
    arXiv预印本 arXiv:1811.12535 (2018).'
- en: '[17] Lewenberg, Yoad, Yoram Bachrach, Ulrich Paquet, and Jeffrey S. Rosenschein.
    ”Knowing What to Ask: A Bayesian Active Learning Approach to the Surveying Problem.”
    In AAAI, pp. 1396-1402\. 2017.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Lewenberg, Yoad, Yoram Bachrach, Ulrich Paquet, 和 Jeffrey S. Rosenschein.
    ”知道该问什么：一种贝叶斯主动学习方法解决调查问题。” 见于AAAI，第1396-1402页。2017.'
- en: '[18] Houlsby, Neil, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. ”Bayesian
    active learning for classification and preference learning.” arXiv preprint arXiv:1112.5745
    (2011).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Houlsby, Neil, Ferenc Huszár, Zoubin Ghahramani, 和 Máté Lengyel. ”用于分类和偏好学习的贝叶斯主动学习。”
    arXiv预印本 arXiv:1112.5745 (2011).'
- en: '[19] Karpathy, Andrej, Armand Joulin, and Li F. Fei-Fei. ”Deep fragment embeddings
    for bidirectional image sentence mapping.” In Advances in neural information processing
    systems, pp. 1889-1897\. 2014.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Karpathy, Andrej, Armand Joulin, 和 Li F. Fei-Fei. ”用于双向图像句子映射的深度片段嵌入。”
    见于神经信息处理系统进展，第1889-1897页。2014.'
- en: '[20] Ghahramani, Zoubin. ”Probabilistic machine learning and artificial intelligence.”
    Nature 521, no. 7553 (2015): 452-459.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Ghahramani, Zoubin. “概率机器学习与人工智能。” 《自然》 521, 无. 7553 (2015): 452-459。'
- en: '[21] Gal, Yarin, and Zoubin Ghahramani. ”Dropout as a bayesian approximation:
    Insights and applications.” In Deep Learning Workshop, ICML, vol. 1, p. 2\. 2015.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Gal, Yarin, 和 Zoubin Ghahramani. “丢弃法作为贝叶斯近似：洞察与应用。” 见于深度学习研讨会，ICML，第1卷，第2页。2015。'
- en: '[22] Gal, Yarin, and Zoubin Ghahramani. ”Dropout as a bayesian approximation:
    Representing model uncertainty in deep learning.” In international conference
    on machine learning, pp. 1050-1059\. 2016.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Gal, Yarin, 和 Zoubin Ghahramani. “丢弃法作为贝叶斯近似：在深度学习中表示模型不确定性。” 见于国际机器学习会议，pp.
    1050-1059。2016。'
- en: '[23] Jedoui, Khaled, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. ”Deep
    Bayesian Active Learning for Multiple Correct Outputs.” arXiv preprint arXiv:1912.01119
    (2019).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Jedoui, Khaled, Ranjay Krishna, Michael Bernstein, 和 Li Fei-Fei. “多重正确输出的深度贝叶斯主动学习。”
    arXiv 预印本 arXiv:1912.01119 (2019)。'
- en: '[24] Pinsler, Robert, Jonathan Gordon, Eric Nalisnick, and José Miguel Hernández-Lobato.
    ”Bayesian batch active learning as sparse subset approximation.” In Advances in
    Neural Information Processing Systems, pp. 6359-6370\. 2019.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Pinsler, Robert, Jonathan Gordon, Eric Nalisnick, 和 José Miguel Hernández-Lobato.
    “贝叶斯批量主动学习作为稀疏子集逼近。” 见于神经信息处理系统进展，pp. 6359-6370。2019。'
- en: '[25] Cohn, David A., Zoubin Ghahramani, and Michael I. Jordan. Active Learning
    with Statistical Models. MASSACHUSETTS INST OF TECH CAMBRIDGE ARTIFICIAL INTELLIGENCE
    LAB, 1995.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Cohn, David A., Zoubin Ghahramani, 和 Michael I. Jordan. 《统计模型的主动学习》。麻省理工学院剑桥人工智能实验室，1995。'
- en: '[26] Freeman, Linton C., and Linton C. Freeman. Elementary applied statistics:
    for students in behavioral science. New York: Wiley, 1965.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Freeman, Linton C., 和 Linton C. Freeman. 《初级应用统计：针对行为科学学生》。纽约：Wiley，1965。'
- en: '[27] Shannon, Claude E. ”A mathematical theory of communication.” The Bell
    system technical journal 27, no. 3 (1948): 379-423.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Shannon, Claude E. “通信的数学理论。” 《贝尔系统技术期刊》 27, 无. 3 (1948): 379-423。'
- en: '[28] Gal, Yarin. ”Uncertainty in deep learning.” University of Cambridge 1,
    no. 3 (2016): 4.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Gal, Yarin. “深度学习中的不确定性。” 剑桥大学 1, 无. 3 (2016): 4。'
- en: '[29] Houlsby, Neil, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. ”Bayesian
    active learning for classification and preference learning.” arXiv preprint arXiv:1112.5745
    (2011).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Houlsby, Neil, Ferenc Huszár, Zoubin Ghahramani, 和 Máté Lengyel. “分类和偏好学习的贝叶斯主动学习。”
    arXiv 预印本 arXiv:1112.5745 (2011)。'
- en: '[30] Denker, John, Daniel Schwartz, Ben Wittner, Sara Solla, Richard Howard,
    Lawrence Jackel, and John Hopfield. ”Large automatic learning, rule extraction,
    and generalization.” Complex systems 1, no. 5 (1987): 877-922.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Denker, John, Daniel Schwartz, Ben Wittner, Sara Solla, Richard Howard,
    Lawrence Jackel, 和 John Hopfield. “大规模自动学习、规则提取与泛化。” 《复杂系统》 1, 无. 5 (1987): 877-922。'
- en: '[31] Hajibabaee, Parisa, Farhad Pourkamali-Anaraki, and Mohammad Amin Hariri-Ardebili.
    ”Kernel matrix approximation on class-imbalanced data with an application to scientific
    simulation.” IEEE Access 9 (2021): 83579-83591.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Hajibabaee, Parisa, Farhad Pourkamali-Anaraki, 和 Mohammad Amin Hariri-Ardebili.
    “在类别不平衡数据上进行核矩阵逼近及其在科学仿真中的应用。” 《IEEE Access》 9 (2021): 83579-83591。'
- en: '[32] Hajibabaee, Parisa, Farhad Pourkamali-Anaraki, and Mohammad Amin Hariri-Ardebili.
    ”An empirical evaluation of the t-sne algorithm for data visualization in structural
    engineering.” In 2021 20th IEEE International Conference on Machine Learning and
    Applications (ICMLA), pp. 1674-1680\. IEEE, 2021.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Hajibabaee, Parisa, Farhad Pourkamali-Anaraki, 和 Mohammad Amin Hariri-Ardebili.
    “对 t-sne 算法在结构工程数据可视化中的实证评估。” 见于 2021年第20届IEEE国际机器学习与应用会议（ICMLA），第1674-1680页。IEEE，2021。'
