- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:58:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:58:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2012.00641] A Decade Survey of Content Based Image Retrieval using Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2012.00641] 基于深度学习的内容图像检索的十年综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2012.00641](https://ar5iv.labs.arxiv.org/html/2012.00641)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2012.00641](https://ar5iv.labs.arxiv.org/html/2012.00641)
- en: A Decade Survey of Content Based Image Retrieval using Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的内容图像检索的十年综述
- en: 'Shiv Ram Dubey S.R. Dubey is with the Computer Vision Group, Indian Institute
    of Information Technology, Sri City, Chittoor, Andhra Pradesh-517646, India (e-mail:
    shivram1987@gmail.com, srdubey@iiits.in).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Shiv Ram Dubey S.R. Dubey 现任印度信息技术学院计算机视觉组成员，位于印度安得拉邦奇图尔Sri City，邮政编码517646（电子邮件：shivram1987@gmail.com,
    srdubey@iiits.in）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The content based image retrieval aims to find the similar images from a large
    scale dataset against a query image. Generally, the similarity between the representative
    features of the query image and dataset images is used to rank the images for
    retrieval. In early days, various hand designed feature descriptors have been
    investigated based on the visual cues such as color, texture, shape, etc. that
    represent the images. However, the deep learning has emerged as a dominating alternative
    of hand-designed feature engineering from a decade. It learns the features automatically
    from the data. This paper presents a comprehensive survey of deep learning based
    developments in the past decade for content based image retrieval. The categorization
    of existing state-of-the-art methods from different perspectives is also performed
    for greater understanding of the progress. The taxonomy used in this survey covers
    different supervision, different networks, different descriptor type and different
    retrieval type. A performance analysis is also performed using the state-of-the-art
    methods. The insights are also presented for the benefit of the researchers to
    observe the progress and to make the best choices. The survey presented in this
    paper will help in further research progress in image retrieval using deep learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内容的图像检索旨在从大规模数据集中找到与查询图像相似的图像。通常，查询图像和数据集图像的代表性特征之间的相似性用于对图像进行排名以进行检索。在早期，研究了基于颜色、纹理、形状等视觉线索的各种手工设计特征描述符，这些特征用于表示图像。然而，深度学习在过去十年中成为了手工设计特征工程的主导替代方案。它能够自动从数据中学习特征。本文提供了过去十年中基于深度学习的内容图像检索发展的全面综述。同时，也对现有的先进方法从不同角度进行了分类，以便更好地理解进展。本综述中使用的分类法涵盖了不同的监督方式、不同的网络、不同的描述符类型和不同的检索类型。还使用最先进的方法进行了性能分析，并提供了有助于研究人员观察进展并做出最佳选择的见解。本文的综述将有助于进一步研究基于深度学习的图像检索进展。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Content Based Image Retrieval; Deep Learning; CNNs; Survey; Supervised and Unsupervised
    Learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内容的图像检索；深度学习；卷积神经网络（CNNs）；综述；监督学习和无监督学习。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Image retrieval is a well studied problem of image matching where the similar
    images are retrieved from a database w.r.t. a given query image [[1](#bib.bib1)],
    [[2](#bib.bib2)]. Basically, the similarity between the query image and the database
    images is used to rank the database images in decreasing order of similarity [[3](#bib.bib3)].
    Thus, the performance of any image retrieval method depends upon the similarity
    computation between images. Ideally, the similarity score computation method between
    two images should be discriminative, robust and efficient.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图像检索是一个研究广泛的问题，旨在从数据库中检索与给定查询图像相似的图像[[1](#bib.bib1)], [[2](#bib.bib2)]。基本上，查询图像与数据库图像之间的相似性用于对数据库图像进行降序排名[[3](#bib.bib3)]。因此，任何图像检索方法的性能都依赖于图像之间的相似性计算。理想情况下，两幅图像之间的相似性分数计算方法应该是有区分度的、稳健的和高效的。
- en: I-A Hand-crafted Descriptor based Image Retrieval
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 手工设计描述符基础的图像检索
- en: In order to make the retrieval robust to geometric and photometric changes,
    the similarity between images is computed based on the content of images. Basically,
    the content of the images (i.e., the visual appearance) in terms of the color,
    texture, shape, gradient, etc. are represented in the form of a feature descriptor
    [[4](#bib.bib4)]. The similarity between the feature vectors of the corresponding
    images is treated as the similarity between the images. Thus, the performance
    of any content based image retrieval (CBIR) method heavily depends upon the feature
    descriptor representation of the image. Any feature descriptor representation
    method is expected to have the discriminating ability, robustness and low dimensionality.
    Various feature descriptor representation methods have been investigated to compute
    the similarity between the two images for content based image retrieval. The feature
    descriptor representation utilizes the visual cues of the images selected manually
    based on the need [[5](#bib.bib5)], [[6](#bib.bib6)], [[7](#bib.bib7)], [[8](#bib.bib8)],
    [[9](#bib.bib9)], [[10](#bib.bib10)], [[11](#bib.bib11)], [[12](#bib.bib12)],
    [[13](#bib.bib13)], [[14](#bib.bib14)], [[15](#bib.bib15)], [[16](#bib.bib16)].
    These approaches are also termed as the hand-designed or hand-engineered feature
    description. Moreover, generally these methods are unsupervised as they do not
    need the data to design the feature representation method. Various survey has
    been also conducted time to time to present the progress in content based image
    retrieval, including [[17](#bib.bib17)] in 2008, [[18](#bib.bib18)] in 2014 and
    [[19](#bib.bib19)] in 2017. The hand-engineering feature for image retrieval was
    a very active research area. However, its performance was limited as the hand-engineered
    features are not able to represent the image characteristics in an accurate manner.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使检索对几何和光度变化具有鲁棒性，图像之间的相似度是基于图像内容计算的。基本上，图像的内容（即视觉外观）在颜色、纹理、形状、梯度等方面被表示为特征描述符[[4](#bib.bib4)]。相应图像的特征向量之间的相似度被视为图像之间的相似度。因此，任何基于内容的图像检索（CBIR）方法的性能在很大程度上取决于图像的特征描述符表示。任何特征描述符表示方法都应具备区分能力、鲁棒性和低维度。已研究各种特征描述符表示方法来计算两个图像之间的相似度，以用于基于内容的图像检索。特征描述符表示利用了根据需要手动选择的图像视觉线索[[5](#bib.bib5)],
    [[6](#bib.bib6)], [[7](#bib.bib7)], [[8](#bib.bib8)], [[9](#bib.bib9)], [[10](#bib.bib10)],
    [[11](#bib.bib11)], [[12](#bib.bib12)], [[13](#bib.bib13)], [[14](#bib.bib14)],
    [[15](#bib.bib15)], [[16](#bib.bib16)]。这些方法也被称为手工设计或手工工程的特征描述。此外，通常这些方法是无监督的，因为它们在设计特征表示方法时不需要数据。各种调查也不时进行，以展示基于内容的图像检索的进展，包括2008年的[[17](#bib.bib17)]，2014年的[[18](#bib.bib18)]以及2017年的[[19](#bib.bib19)]。手工工程特征用于图像检索曾是一个非常活跃的研究领域。然而，由于手工设计的特征无法准确表示图像特性，其性能受到限制。
- en: '![Refer to caption](img/7c480ae0a6fe9059438b91485dbef341.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7c480ae0a6fe9059438b91485dbef341.png)'
- en: 'Figure 1: The pipeline of state-of-the-art feature representation is replaced
    by the CNN based feature representation.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：先进的特征表示管道被基于 CNN 的特征表示所替代。
- en: I-B Distance Metric Learning based Image Retrieval
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 距离度量学习基础的图像检索
- en: The distance metric learning has been also used very extensively for feature
    vectors representation [[20](#bib.bib20)]. It is also explored well for image
    retrieval [[21](#bib.bib21)]. Some notable deep metric learning based image retrieval
    approaches include Contextual constraints distance metric learning [[22](#bib.bib22)],
    Kernel-based distance metric learning [[23](#bib.bib23)], [[24](#bib.bib24)],
    Visuality-preserving distance metric learning [[25](#bib.bib25)], Rank-based distance
    metric learning [[26](#bib.bib26)], Semi-supervised distance metric learning [[27](#bib.bib27)],
    Hamming distance metric learning [[28](#bib.bib28)], [[29](#bib.bib29)], and Rank
    based metric learning [[30](#bib.bib30)], [[31](#bib.bib31)]. Generally, the deep
    metric learning based approaches have shown the promising retrieval performance
    compared to hand-crafted approaches. However, most of the existing deep metric
    learning based methods rely on the linear distance functions which limits its
    discriminative ability and robustness to represent the non-linear data for image
    retrieval. Moreover, it is also not able to handle the multi-modal retrieval effectively.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 距离度量学习在特征向量表示中也被广泛使用[[20](#bib.bib20)]。它在图像检索中也得到了很好的探索[[21](#bib.bib21)]。一些显著的基于深度度量学习的图像检索方法包括上下文约束距离度量学习[[22](#bib.bib22)]、基于核的距离度量学习[[23](#bib.bib23)]、[[24](#bib.bib24)]、视觉保留距离度量学习[[25](#bib.bib25)]、基于排序的距离度量学习[[26](#bib.bib26)]、半监督距离度量学习[[27](#bib.bib27)]、汉明距离度量学习[[28](#bib.bib28)]、[[29](#bib.bib29)]和基于排序的度量学习[[30](#bib.bib30)]、[[31](#bib.bib31)]。总体来说，与手工制作的方法相比，基于深度度量学习的方法在检索性能上显示出了良好的前景。然而，大多数现有的基于深度度量学习的方法依赖于线性距离函数，这限制了其对非线性数据进行图像检索的判别能力和鲁棒性。此外，它也无法有效处理多模态检索。
- en: I-C Deep Learning based Image Retrieval
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-C 基于深度学习的图像检索
- en: 'From a decade, a shift has been observed in feature representation from hand-engineering
    to learning-based after the emergence of deep learning [[32](#bib.bib32)], [[33](#bib.bib33)].
    This transition is depicted in Fig. [1](#S1.F1 "Figure 1 ‣ I-A Hand-crafted Descriptor
    based Image Retrieval ‣ I Introduction ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning") where the convoltional neural networks based feature
    learning replaces the state-of-the-art pipeline of traditional hand-engineered
    feature representation. The deep learning is a hierarchical feature representation
    technique to learn the abstract features from data which are important for that
    dataset and application [[34](#bib.bib34)]. Based on the type of data to be processed,
    different architectures came into existence such as Artificial Neural Network
    (ANN)/ Multilayer Perceptron (MLP) for 1-D data [[35](#bib.bib35)], [[36](#bib.bib36)],
    Convolutional Neural Networks (CNN) for image data [[37](#bib.bib37)], [[38](#bib.bib38)],
    and Reurrent Neural Networks (RNN) for time-series data [[39](#bib.bib39)], [[40](#bib.bib40)].
    A huge progress has been made in this decade to utilize the power of deep learning
    for content based image retrieval [[32](#bib.bib32)], [[41](#bib.bib41)], [[42](#bib.bib42)],
    [[43](#bib.bib43)], [[44](#bib.bib44)]. Thus, this survey mainly focuses over
    the progress in state-of-the-art deep learning based models and features for content
    based image retrieval from its inception. A taxonomy for the same is portrayed
    in Fig. [2](#S1.F2 "Figure 2 ‣ I-C Deep Learning based Image Retrieval ‣ I Introduction
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning"). The
    major contributions of this survey can be outlined as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从十年前开始，随着深度学习的出现，特征表示从手工工程转向了基于学习的方法[[32](#bib.bib32)]、[[33](#bib.bib33)]。这一过渡在图
    Fig. [1](#S1.F1 "Figure 1 ‣ I-A Hand-crafted Descriptor based Image Retrieval
    ‣ I Introduction ‣ A Decade Survey of Content Based Image Retrieval using Deep
    Learning")中有所体现，其中卷积神经网络基于特征学习取代了传统手工工程特征表示的最先进管道。深度学习是一种分层特征表示技术，用于从数据中学习对数据集和应用程序重要的抽象特征[[34](#bib.bib34)]。根据处理的数据类型，出现了不同的架构，例如用于
    1-D 数据的人工神经网络 (ANN)/多层感知器 (MLP) [[35](#bib.bib35)]、[[36](#bib.bib36)]，用于图像数据的卷积神经网络
    (CNN) [[37](#bib.bib37)]、[[38](#bib.bib38)]，以及用于时间序列数据的递归神经网络 (RNN) [[39](#bib.bib39)]、[[40](#bib.bib40)]。在过去十年中，在利用深度学习进行基于内容的图像检索方面取得了巨大的进展[[32](#bib.bib32)]、[[41](#bib.bib41)]、[[42](#bib.bib42)]、[[43](#bib.bib43)]、[[44](#bib.bib44)]。因此，本调查主要集中在从其起源到目前为止最先进的深度学习模型和特征在基于内容的图像检索中的进展。相关的分类法在图
    Fig. [2](#S1.F2 "Figure 2 ‣ I-C Deep Learning based Image Retrieval ‣ I Introduction
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning")中描绘。此调查的主要贡献可以概括如下：
- en: '![Refer to caption](img/df274ca1464ce6210a97d971ece38beb.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/df274ca1464ce6210a97d971ece38beb.png)'
- en: 'Figure 2: Taxonomy used in this survey to categorize the existing deep learning
    based image retrieval approaches.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：本调查中用于分类现有基于深度学习的图像检索方法的分类法。
- en: '1.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: This survey covers the deep learning based image retrieval approaches very comprehensively
    in terms of evolution of image retrieval using deep learning, different supervision
    type, network type, descriptor type, retrieval type and other aspects.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查在基于深度学习的图像检索方法方面进行了非常全面的覆盖，涉及图像检索的演变、不同的监督类型、网络类型、描述符类型、检索类型及其他方面。
- en: '2.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: In contrast to the recent reviews [[42](#bib.bib42)], [[21](#bib.bib21)], [[43](#bib.bib43)],
    this survey specifically covers the progress in image retrieval using deep learning
    in 2011-2020 decade. An informative taxonomy is provided with wide coverage of
    existing deep learning based image retrieval approaches as compared to the recent
    survey [[44](#bib.bib44)].
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与最近的综述[[42](#bib.bib42)]、[[21](#bib.bib21)]、[[43](#bib.bib43)]相比，本调查专门涵盖了2011-2020年十年间基于深度学习的图像检索的进展。提供了一个信息丰富的分类法，与最近的综述[[44](#bib.bib44)]相比，覆盖了现有的基于深度学习的图像检索方法。
- en: '3.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: This survey enriches the reader with the state-of-the-art image retrieval using
    deep learning methods with analysis from various perspectives.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查通过从不同角度分析，丰富了读者对基于深度学习的最先进图像检索技术的了解。
- en: '4.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: This paper also presents the brief highlights and important discussions along
    with the comprehensive comparisons on benchmark datasets using the state-of-the-art
    deep learning based image retrieval approaches.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文还简要介绍了重点内容和重要讨论，并对使用最先进的基于深度学习的图像检索方法在基准数据集上的综合比较进行了描述。
- en: 'This survey is organized as follows: the background is presented in Section
    [II](#S2 "II Background ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning") the evolution of deep learning based image retrieval is compiled
    in Section [III](#S3 "III Evolution of Deep Learning for Content Based Image Retrieval
    (CBIR) ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning");
    the categorization of existing approaches based on the supervision type, network
    type, descriptor type, and retrieval type are discussed in Section [IV](#S4 "IV
    Different Supervision Categorization ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning"), [V](#S5 "V Network Types For Image Retrieval
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning"), [VI](#S6
    "VI Type of Descriptors for Image Retrieval ‣ A Decade Survey of Content Based
    Image Retrieval using Deep Learning"), and [VII](#S7 "VII Retrieval Type ‣ A Decade
    Survey of Content Based Image Retrieval using Deep Learning"), respectively; some
    other aspects are highlighted in Section [VIII](#S8 "VIII Miscellaneous ‣ A Decade
    Survey of Content Based Image Retrieval using Deep Learning"); the performance
    comparison of the popular methods is performed in Section [IX](#S9 "IX Performance
    Comparison ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning");
    conclusions and future directions are presented in Section [X](#S10 "X Conclusion
    and Future Directives ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning").'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的组织结构如下：背景介绍见第[II](#S2 "II Background ‣ A Decade Survey of Content Based
    Image Retrieval using Deep Learning")节，基于深度学习的图像检索的演变编纂见第[III](#S3 "III Evolution
    of Deep Learning for Content Based Image Retrieval (CBIR) ‣ A Decade Survey of
    Content Based Image Retrieval using Deep Learning")节；基于监督类型、网络类型、描述符类型和检索类型的现有方法分类分别在第[IV](#S4
    "IV Different Supervision Categorization ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning")、[V](#S5 "V Network Types For Image Retrieval ‣
    A Decade Survey of Content Based Image Retrieval using Deep Learning")、[VI](#S6
    "VI Type of Descriptors for Image Retrieval ‣ A Decade Survey of Content Based
    Image Retrieval using Deep Learning")和第[VII](#S7 "VII Retrieval Type ‣ A Decade
    Survey of Content Based Image Retrieval using Deep Learning")节中讨论；第[VIII](#S8
    "VIII Miscellaneous ‣ A Decade Survey of Content Based Image Retrieval using Deep
    Learning")节突出了其他一些方面；第[IX](#S9 "IX Performance Comparison ‣ A Decade Survey of
    Content Based Image Retrieval using Deep Learning")节对流行方法的性能进行了比较；结论和未来方向见第[X](#S10
    "X Conclusion and Future Directives ‣ A Decade Survey of Content Based Image Retrieval
    using Deep Learning")节。
- en: II Background
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: In this section the background is presented in terms of the commonly used evaluation
    metrics and benchmark datasets.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了背景，包括常用的评估指标和基准数据集。
- en: II-A Retrieval Evaluation Measures
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 检索评估指标
- en: In order to judge the performance of image retrieval approaches, precision,
    recall and f-score are the common evaluation metrics. The mean average precision
    ($mAP$) is very commonly used in the literature. The precision is defined as the
    percentage of correctly retrieved images out of the total number of retrieved
    images. The recall is another performance measure being used for image retrieval
    by computing the percentage of correctly retrieved images out of the total number
    of relevant images present in the dataset. The f-score is computed from the harmonic
    mean of precision and recall.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估图像检索方法的性能，**精度**、**召回率**和**f-分数**是常用的评估指标。均值平均精度（$mAP$）在文献中被广泛使用。精度定义为从总检索图像中正确检索出的图像的百分比。召回率是通过计算正确检索的图像占数据集中所有相关图像总数的百分比来评估图像检索的另一种性能指标。f-分数是从精度和召回率的调和均值计算得出的。
- en: 'TABLE I: The summary of large-scale datasets for deep learning based image
    retrieval.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 深度学习图像检索的大规模数据集总结。'
- en: 'Dataset Year #Classes Training Test Image Type CIFAR-10 [[45](#bib.bib45)]
    2009 10 50,000 10,000 Object Category Images NUS-WIDE [[46](#bib.bib46)] 2009
    21 97,214 65,075 Scene Images MNIST [[47](#bib.bib47)] 1998 10 60,000 10,000 Handwritten
    Digit Images SVHN [[48](#bib.bib48)] 2011 10 73,257 26,032 House Number Images
    SUN397 [[49](#bib.bib49)] 2010 397 100,754 8,000 Scene Images UT-ZAP50K [[50](#bib.bib50)]
    2014 4 42,025 8,000 Shoes Images Yahoo-1M [[51](#bib.bib51)] 2015 116 1,011,723
    112,363 Clothing Images ILSVRC2012 [[52](#bib.bib52)] 2012 1,000 $\sim$1.2 M 50,000
    Object Category Images MS COCO [[53](#bib.bib53)] 2015 80 82,783 40,504 Common
    Object Images MIRFlicker-1M [[54](#bib.bib54)] 2010 - 1 M - Scene Images Google
    Landmarks [[55](#bib.bib55)] 2017 15 K $\sim$1 M - Landmark Images Google Landmarks
    v2 [[56](#bib.bib56)] 2020 200 K 5 M - Landmark Images Clickture [[57](#bib.bib57)]
    2013 73.6 M 40 M - Search Log'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 年份 类别数 训练 测试 图像类型 CIFAR-10 [[45](#bib.bib45)] 2009 10 50,000 10,000 对象类别图像
    NUS-WIDE [[46](#bib.bib46)] 2009 21 97,214 65,075 场景图像 MNIST [[47](#bib.bib47)]
    1998 10 60,000 10,000 手写数字图像 SVHN [[48](#bib.bib48)] 2011 10 73,257 26,032 房屋号码图像
    SUN397 [[49](#bib.bib49)] 2010 397 100,754 8,000 场景图像 UT-ZAP50K [[50](#bib.bib50)]
    2014 4 42,025 8,000 鞋子图像 Yahoo-1M [[51](#bib.bib51)] 2015 116 1,011,723 112,363
    服装图像 ILSVRC2012 [[52](#bib.bib52)] 2012 1,000 $\sim$1.2 M 50,000 对象类别图像 MS COCO
    [[53](#bib.bib53)] 2015 80 82,783 40,504 常见对象图像 MIRFlicker-1M [[54](#bib.bib54)]
    2010 - 1 M - 场景图像 Google Landmarks [[55](#bib.bib55)] 2017 15 K $\sim$1 M - 地标图像
    Google Landmarks v2 [[56](#bib.bib56)] 2020 200 K 5 M - 地标图像 Clickture [[57](#bib.bib57)]
    2013 73.6 M 40 M - 搜索日志
- en: \startchronology
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \startchronology
- en: '[startyear=1,stopyear=43, startdate=false, color=blue!100, height=0.2cm, stopdate=false,
    arrow=true, arrowwidth=0.6cm, arrowheight=0.45cm] \setupchronoeventtextstyle=,datestyle=
    \chronoevent[markdepth=-15pt, year=false]12011 \chronoevent[markdepth=15pt,year=false,textwidth=1.5cm]2Deep
    Autoencoder [[58](#bib.bib58)] \chronoevent[markdepth=-15pt, year=false]32012
    \chronoevent[markdepth=-30pt,year=false,textwidth=1.5cm]4Deep Multi-View Hashing
    (DMVH) [[59](#bib.bib59)] \chronoevent[markdepth=-15pt, year=false]52013 \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]6Online
    Multimodal Deep Similarity Learning (OMDSL) [[60](#bib.bib60)] \chronoevent[markdepth=-15pt,
    year=false]72014 \chronoevent[markdepth=-50pt,year=false,textwidth=1.5cm]8Neural
    Codes [[61](#bib.bib61)] \chronoevent[markdepth=15pt,year=false,textwidth=1cm]9Deep
    Ranking Model [[62](#bib.bib62)] \chronoevent[markdepth=-15pt, year=false]102015
    \chronoevent[markdepth=-25pt,year=false,textwidth=1.5cm]11Stack of Conv Layers
    [[63](#bib.bib63)] \chronoevent[markdepth=35pt,year=false,textwidth=1cm]12Triplet
    Ranking Loss [[63](#bib.bib63)] \chronoevent[markdepth=-50pt,year=false,textwidth=1cm]13DRSCH
    [[64](#bib.bib64)] \chronoevent[markdepth=-15pt, year=false]142016 \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]15Region
    based CNN Descriptor [[65](#bib.bib65)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.6cm]16Lifted
    Structure Embedding [[66](#bib.bib66)] \chronoevent[markdepth=40pt,year=false,textwidth=1.5cm]17Deep
    Hashing Network (DHN) [[67](#bib.bib67)] \chronoevent[markdepth=-55pt,year=false,textwidth=1.5cm]18Deep
    Quantization Network (DQN) [[68](#bib.bib68)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]19Feature
    Aggregation [[69](#bib.bib69)] \chronoevent[markdepth=-30pt,year=false,textwidth=1.3cm]20Sigmoid
    based Feature [[70](#bib.bib70)] \chronoevent[markdepth=-15pt, year=false]212017
    \chronoevent[markdepth=40pt,year=false,textwidth=1cm]22HashNet [[71](#bib.bib71)]
    \chronoevent[markdepth=-60pt,year=false,textwidth=2cm]23 Siamese Network [[72](#bib.bib72)]
    \chronoevent[markdepth=10pt,year=false,textwidth=1.3cm]24Mask based Feature [[73](#bib.bib73)]
    \chronoevent[markdepth=-30pt,year=false,textwidth=1.3cm]25Bilinear Network [[74](#bib.bib74)]
    \chronoevent[markdepth=-15pt, year=false]262018 \chronoevent[markdepth=30pt,year=false,textwidth=1.3cm]27Deep
    Cauchy Hashing (DCH) [[75](#bib.bib75)] \chronoevent[markdepth=-60pt,year=false,textwidth=1.3cm]28GreedyHash
    [[76](#bib.bib76)] \chronoevent[markdepth=5pt,year=false,textwidth=1.3cm]29Policy
    Gradient [[77](#bib.bib77)] \chronoevent[markdepth=-20pt,year=false,textwidth=1.3cm]30Relaxation
    Approach [[78](#bib.bib78)] \chronoevent[markdepth=40pt,year=false,textwidth=1.3cm]31Deep
    Index-Compatible Hashing (DICH) [[79](#bib.bib79)] \chronoevent[markdepth=-15pt,
    year=false]322019 \chronoevent[markdepth=-45pt,year=false,textwidth=1.3cm]33Deep
    Incremental Hashing Network (DIHN) [[80](#bib.bib80)] \chronoevent[markdepth=7pt,year=false,textwidth=1.3cm]34Deep
    Spherical Quantization (DSQ) [[81](#bib.bib81)] \chronoevent[markdepth=-30pt,year=false,textwidth=1.3cm]35DistillHash
    [[82](#bib.bib82)] \chronoevent[markdepth=45pt,year=false,textwidth=2cm]36Deep
    Progressive Hashing (DPH) [[83](#bib.bib83)] \chronoevent[markdepth=-20pt,year=false,textwidth=1.3cm]37DHA
    [[84](#bib.bib84)] \chronoevent[markdepth=5pt,year=false,textwidth=1.3cm]38Just-Maximizing-Likelihood
    Hashing (JMLH) [[85](#bib.bib85)] \chronoevent[markdepth=-45pt,year=false,textwidth=1.1cm]39Deep
    Variational Binaries (DVB) [[86](#bib.bib86)] \chronoevent[markdepth=-15pt, year=false]402020
    \chronoevent[markdepth=55pt,year=false,textwidth=2.2cm]41Twin-Bottleneck Hashing
    (TBH) [[87](#bib.bib87)] \chronoevent[markdepth=-22pt,year=false,textwidth=1.1cm]42Co-occurrences
    from Deep Conv Features [[88](#bib.bib88)] \chronoevent[markdepth=10pt,year=false,textwidth=1.45cm]43Deep
    Position-Aware Hashing (DPAH) [[89](#bib.bib89)] \stopchronology'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[startyear=1,stopyear=43, startdate=false, color=blue!100, height=0.2cm, stopdate=false,
    arrow=true, arrowwidth=0.6cm, arrowheight=0.45cm] \setupchronoeventtextstyle=,datestyle=
    \chronoevent[markdepth=-15pt, year=false]12011 \chronoevent[markdepth=15pt,year=false,textwidth=1.5cm]2深度自编码器
    [[58](#bib.bib58)] \chronoevent[markdepth=-15pt, year=false]32012 \chronoevent[markdepth=-30pt,year=false,textwidth=1.5cm]4深度多视图哈希
    (DMVH) [[59](#bib.bib59)] \chronoevent[markdepth=-15pt, year=false]52013 \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]6在线多模态深度相似性学习
    (OMDSL) [[60](#bib.bib60)] \chronoevent[markdepth=-15pt, year=false]72014 \chronoevent[markdepth=-50pt,year=false,textwidth=1.5cm]8神经编码
    [[61](#bib.bib61)] \chronoevent[markdepth=15pt,year=false,textwidth=1cm]9深度排序模型
    [[62](#bib.bib62)] \chronoevent[markdepth=-15pt, year=false]102015 \chronoevent[markdepth=-25pt,year=false,textwidth=1.5cm]11卷积层堆叠
    [[63](#bib.bib63)] \chronoevent[markdepth=35pt,year=false,textwidth=1cm]12三元组排序损失
    [[63](#bib.bib63)] \chronoevent[markdepth=-50pt,year=false,textwidth=1cm]13DRSCH
    [[64](#bib.bib64)] \chronoevent[markdepth=-15pt, year=false]142016 \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]15基于区域的
    CNN 描述符 [[65](#bib.bib65)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.6cm]16提升结构嵌入
    [[66](#bib.bib66)] \chronoevent[markdepth=40pt,year=false,textwidth=1.5cm]17深度哈希网络
    (DHN) [[67](#bib.bib67)] \chronoevent[markdepth=-55pt,year=false,textwidth=1.5cm]18深度量化网络
    (DQN) [[68](#bib.bib68)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]19特征聚合
    [[69](#bib.bib69)] \chronoevent[markdepth=-30pt,year=false,textwidth=1.3cm]20基于
    sigmoid 的特征 [[70](#bib.bib70)] \chronoevent[markdepth=-15pt, year=false]212017
    \chronoevent[markdepth=40pt,year=false,textwidth=1cm]22哈希网络 (HashNet) [[71](#bib.bib71)]
    \chronoevent[markdepth=-60pt,year=false,textwidth=2cm]23 孪生网络 [[72](#bib.bib72)]
    \chronoevent[markdepth=10pt,year=false,textwidth=1.3cm]24基于掩码的特征 [[73](#bib.bib73)]
    \chronoevent[markdepth=-30pt,year=false,textwidth=1.3cm]25双线性网络 [[74](#bib.bib74)]
    \chronoevent[markdepth=-15pt, year=false]262018 \chronoevent[markdepth=30pt,year=false,textwidth=1.3cm]27深度柯西哈希
    (DCH) [[75](#bib.bib75)] \chronoevent[markdepth=-60pt,year=false,textwidth=1.3cm]28贪婪哈希
    [[76](#bib.bib76)] \chronoevent[markdepth=5pt,year=false,textwidth=1.3cm]29策略梯度
    [[77](#bib.bib77)] \chronoevent[markdepth=-20pt,year=false,textwidth=1.3cm]30放松方法
    [[78](#bib.bib78)] \chronoevent[markdepth=40pt,year=false,textwidth=1.3cm]31深度索引兼容哈希
    (DICH) [[79](#bib.bib79)] \chronoevent[markdepth=-15pt, year=false]322019 \chronoevent[markdepth=-45pt,year=false,textwidth=1.3cm]33深度增量哈希网络
    (DIHN) [[80](#bib.bib80)] \chronoevent[markdepth=7pt,year=false,textwidth=1.3cm]34深度球面量化
    (DSQ) [[81](#bib.bib81)] \chronoevent[markdepth=-30pt,year=false,textwidth=1.3cm]35DistillHash
    [[82](#bib.bib82)] \chronoevent[markdepth=45pt,year=false,textwidth=2cm]36深度渐进哈希
    (DPH) [[83](#bib.bib83)] \chronoevent[markdepth=-20pt,year=false,textwidth=1.3cm]37DHA
    [[84](#bib.bib84)] \chronoevent[markdepth=5pt,year=false,textwidth=1.3cm]38仅最大化似然哈希
    (JMLH) [[85](#bib.bib85)] \chronoevent[markdepth=-45pt,year=false,textwidth=1.1cm]39深度变分二进制
    (DVB) [[86](#bib.bib86)] \chronoevent[markdepth=-15pt, year=false]402020 \chronoevent[markdepth=55pt,year=false,textwidth=2.2cm]41双瓶颈哈希
    (TBH) [[87](#bib.bib87)] \chronoevent[markdepth=-22pt,year=false,textwidth=1.1cm]42来自深度卷积特征的共现
    [[88](#bib.bib88)] \chronoevent[markdepth=10pt,year=false,textwidth=1.45cm]43深度位置感知哈希
    (DPAH) [[89](#bib.bib89)] \stopchronology'
- en: 'Figure 3: A chronological view of deep learning based image retrieval methods
    depicting its evolution from 2011 to 2020.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：基于深度学习的图像检索方法的时间序列视图，展示了其从 2011 年到 2020 年的演变。
- en: II-B Datasets
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 数据集
- en: With the inception of deep learning models, various large-scale datasets have
    been created to facilitate the research in image recognition and retrieval. The
    details of large-scale datasets are summarized in Table [I](#S2.T1 "TABLE I ‣
    II-A Retrieval Evaluation Measures ‣ II Background ‣ A Decade Survey of Content
    Based Image Retrieval using Deep Learning"). Datasets having various types of
    images are available to test the deep learning based approaches such as object
    category datasets [[45](#bib.bib45)], [[52](#bib.bib52)], [[53](#bib.bib53)],
    scene datasets [[46](#bib.bib46)], [[49](#bib.bib49)], [[90](#bib.bib90)], digit
    datasets [[47](#bib.bib47)], [[48](#bib.bib48)], apparel datasets [[50](#bib.bib50)],
    [[51](#bib.bib51)], landmark datasets [[55](#bib.bib55)], [[56](#bib.bib56)],
    etc. The CIFAR-10 dataset is very widely used object category datset [[45](#bib.bib45)].
    The ImageNet (ILSVRC2012), a large-scale dataset, is also an object category dataset
    with more than a million number of images [[52](#bib.bib52)]. The MS COCO dataset
    [[53](#bib.bib53)] created for common object detection is also utilized for image
    retrieval purpose. Among scene image datasets commonly used for retrieval purpose,
    the NUS-WIDE dataset is from National University of Singapore [[46](#bib.bib46)];
    the Sun397 is a scene understanding dataset from 397 categories with more than
    one lakh images [[49](#bib.bib49)], [[91](#bib.bib91)]; and the MIRFlicker-1M
    [[90](#bib.bib90)] dataset consists of a million images downloaded from the social
    photography site Flickr. The MNIST dataset is one of the old and large-scale digit
    image datasets [[47](#bib.bib47)] consisting of optical characters. The SVHN is
    another digit dataset [[48](#bib.bib48)] from the street view house number images
    which is more complex than MNIST dataset. The shoes apparel dataset, namely UT-ZAP50K
    [[50](#bib.bib50)], consists of roughly 50K images. The Yahoo-1M is another apparel
    large-scale dataset used in [[51](#bib.bib51)] for image retrieval. The Google
    landmarks dataset is having around a million landmark images [[55](#bib.bib55)].
    The extended version of Google landmarks (i.e., v2) [[56](#bib.bib56)] contains
    around 5 million landmark images. There are more datasets used for retrieval in
    the literature, such as Corel, Oxford, Paris, etc., however, these are not the
    large-scale datasets. The CIFAR-10, MNIST, SVHN and ImageNet are the widely used
    datasets in majority of the research. Clickture is a common dataset for search
    log based on the queries of users [[57](#bib.bib57)]. The click property has been
    utilized for different applications, such as cross-view learning for image search
    [[92](#bib.bib92)], distance metric learning for image ranking [[93](#bib.bib93)]
    and deep structure-preserving embeddings with visual attention [[94](#bib.bib94)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习模型的出现，创建了各种大规模数据集以促进图像识别和检索的研究。大规模数据集的详细信息总结在表格 [I](#S2.T1 "TABLE I ‣
    II-A Retrieval Evaluation Measures ‣ II Background ‣ A Decade Survey of Content
    Based Image Retrieval using Deep Learning") 中。可以用于测试深度学习方法的各种图像类型的数据集包括物体类别数据集
    [[45](#bib.bib45)], [[52](#bib.bib52)], [[53](#bib.bib53)], 场景数据集 [[46](#bib.bib46)],
    [[49](#bib.bib49)], [[90](#bib.bib90)], 数字数据集 [[47](#bib.bib47)], [[48](#bib.bib48)],
    服装数据集 [[50](#bib.bib50)], [[51](#bib.bib51)], 地标数据集 [[55](#bib.bib55)], [[56](#bib.bib56)]
    等。CIFAR-10 数据集是非常广泛使用的物体类别数据集 [[45](#bib.bib45)]。ImageNet（ILSVRC2012）是一个大规模数据集，也是一种物体类别数据集，包含超过一百万张图像
    [[52](#bib.bib52)]。MS COCO 数据集 [[53](#bib.bib53)] 为通用物体检测而创建，也被用于图像检索。常用于检索目的的场景图像数据集包括来自新加坡国立大学的
    NUS-WIDE 数据集 [[46](#bib.bib46)]; Sun397 是一个场景理解数据集，包含来自 397 个类别的超过十万张图像 [[49](#bib.bib49)],
    [[91](#bib.bib91)]; MIRFlicker-1M [[90](#bib.bib90)] 数据集包含从社交摄影网站 Flickr 下载的一百万张图像。MNIST
    数据集是旧的且大规模的数字图像数据集 [[47](#bib.bib47)]，包含光学字符。SVHN 是另一个数字数据集 [[48](#bib.bib48)]，来源于街景房号图像，复杂程度高于
    MNIST 数据集。鞋类服装数据集 UT-ZAP50K [[50](#bib.bib50)] 包含大约 50K 张图像。Yahoo-1M 是另一个在 [[51](#bib.bib51)]
    中用于图像检索的大规模服装数据集。Google 地标数据集包含大约一百万张地标图像 [[55](#bib.bib55)]。Google 地标的扩展版本（即
    v2） [[56](#bib.bib56)] 包含大约 500 万张地标图像。文献中还有更多用于检索的数据集，如 Corel、Oxford、Paris 等，但这些不是大规模数据集。CIFAR-10、MNIST、SVHN
    和 ImageNet 是大多数研究中广泛使用的数据集。Clickture 是一个基于用户查询的搜索日志常用数据集 [[57](#bib.bib57)]。点击属性已被用于不同的应用，如基于图像搜索的交叉视图学习
    [[92](#bib.bib92)]、图像排名的距离度量学习 [[93](#bib.bib93)] 和具有视觉注意力的深度结构保留嵌入 [[94](#bib.bib94)]。
- en: Note that only CIFAR-10 and MNIST datasets contain the same number of samples
    in each category. Other datasets are created generally in unconstrained environment
    with huge number of samples, thus the classes are not well balanced. The choice
    of dataset can be dependent upon the scenario where image retrieval models need
    to be used, such as object category and scene datasets for unconstrained environment,
    apparel datasets for e-commerce applications, and landmark datasets for driving
    applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只有CIFAR-10和MNIST数据集在每个类别中包含相同数量的样本。其他数据集通常是在非约束环境下创建的，样本数量巨大，因此类别不均衡。数据集的选择可能依赖于需要使用图像检索模型的场景，例如用于非约束环境的对象类别和场景数据集，用于电子商务应用的服装数据集，以及用于驾驶应用的地标数据集。
- en: III Evolution of Deep Learning for Content Based Image Retrieval (CBIR)
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基于内容的图像检索（CBIR）的深度学习发展
- en: The deep learning based generation of descriptors or hash codes is the recent
    trends large-scale content based image retrieval, due to its computational efficiency
    and retrieval quality [[21](#bib.bib21)]. In this section, a journey of deep learning
    models for image retrieval from 2011 to 2020 is presented as a chronological overview
    in Fig. [3](#S2.F3 "Figure 3 ‣ II-A Retrieval Evaluation Measures ‣ II Background
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning").
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的描述符或哈希码生成是大规模基于内容的图像检索的最新趋势，由于其计算效率和检索质量[[21](#bib.bib21)]。在本节中，图[3](#S2.F3
    "图3 ‣ II-A 检索评估指标 ‣ II 背景 ‣ 基于深度学习的内容检索的十年调查")展示了从2011年到2020年的深度学习模型图像检索的历程。
- en: III-1 2011-2013
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-1 2011-2013
- en: Among the initial attempts, in 2011, Krizhevsky and Hinton have used a deep
    autoencoder to map the images to short binary codes for content based image retrieval
    (CBIR) [[58](#bib.bib58)]. Kang et al. (2012) have proposed a deep multi-view
    hashing to generate the code for CBIR from multiple views of data by modeling
    the layers with view-specific and shared hidden nodes [[59](#bib.bib59)]. In 2013,
    Wu et al. have considered the multiple pretrained stacked denoising autoencoders
    over low features of the images [[60](#bib.bib60)]. They also fine tune the multiple
    deep networks on the output of the pretrained autoencoders.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的尝试中，2011年，Krizhevsky和Hinton使用了深度自编码器将图像映射到短二进制代码以进行基于内容的图像检索（CBIR）[[58](#bib.bib58)]。Kang等人（2012）提出了一种深度多视角哈希方法，通过建模具有视角特定和共享隐藏节点的层，从多个视角的数据中生成CBIR代码[[59](#bib.bib59)]。2013年，Wu等人考虑了多个预训练的堆叠去噪自编码器用于图像的低特征[[60](#bib.bib60)]。他们还在预训练自编码器的输出上微调了多个深度网络。
- en: III-2 2014
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-2 2014
- en: In an outstanding work, the activations of the top layers of a large convolutional
    neural network (CNN) are utilized as the descriptors (neural codes) for image
    retrieval [[61](#bib.bib61)] as depicted in Fig. [4](#S3.F4 "Figure 4 ‣ III-2
    2014 ‣ III Evolution of Deep Learning for Content Based Image Retrieval (CBIR)
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning"). A very
    promising performance has been recorded using the neural codes for image retrieval
    even if the model is trained on un-related data. The neural code is compressed
    using principal component analysis (PCA) to generate the compact descriptor. In
    2014, deep ranking model is investigated by learning the similarity metric directly
    from images [[62](#bib.bib62)]. Basically, the triplets are employed to capture
    the inter-class and intra-class image differences.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在一项杰出的工作中，大型卷积神经网络（CNN）顶层的激活被用作图像检索的描述符（神经编码）[[61](#bib.bib61)]，如图[4](#S3.F4
    "图4 ‣ III-2 2014 ‣ III 基于内容的图像检索（CBIR）深度学习的发展 ‣ 基于深度学习的内容检索的十年调查")所示。即使模型在不相关数据上训练，使用神经编码进行图像检索也记录了非常有前景的性能。神经编码通过主成分分析（PCA）压缩以生成紧凑的描述符。2014年，通过直接从图像中学习相似性度量，深度排序模型得到了研究[[62](#bib.bib62)]。基本上，三元组用于捕捉类间和类内图像差异。
- en: '![Refer to caption](img/406de49a8c2833d5e19a0a41343711f0.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/406de49a8c2833d5e19a0a41343711f0.png)'
- en: 'Figure 4: The illustration of the neural code generation from a convolutional
    neural network (CNN) [[61](#bib.bib61)].'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：卷积神经网络（CNN）中神经编码生成的说明[[61](#bib.bib61)]。
- en: III-3 2015
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-3 2015
- en: In 2015, a deep architecture is developed which consists of a stack of convolution
    layers to produce the intermediate image features [[63](#bib.bib63)] which are
    used to generate the hash bits. The triplet ranking loss is also utilized to incorporate
    the inter-class and intra-class differences in [[63](#bib.bib63)] for image retrieval.
    Zhang et al. (2015) have developed a deep regularized similarity comparison hashing
    (DRSCH) by training a deep CNN model to simultaneously optimize the discriminative
    image features and hash functions [[64](#bib.bib64)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年，开发了一种深度架构，由一系列卷积层组成，以生成中间图像特征[[63](#bib.bib63)]，这些特征用于生成哈希位。在[[63](#bib.bib63)]中也使用了三元组排序损失来结合类间和类内差异进行图像检索。Zhang等人（2015）开发了一种深度正则化相似性比较哈希（DRSCH），通过训练深度CNN模型来同时优化区分性图像特征和哈希函数[[64](#bib.bib64)]。
- en: III-4 2016
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-4 2016
- en: In 2016, Gordo et al. have pooled the relevant regions to form the descriptor
    with the help of a region proposal network to prioritize the important object
    regions [[65](#bib.bib65)]. Song et al. (2016) have computed the lifted structure
    loss between the CNN and the original features [[66](#bib.bib66)]. Supervised
    deep hashing network (DHN) learns the important image representation by controlling
    the quantization error [[67](#bib.bib67)]. At the same time, Cao et al. have introduced
    a deep quantization network (DQN) which is very similar to the DHN model [[68](#bib.bib68)].
    The CNN based features are aggregated in [[69](#bib.bib69)] with the help of rank-aware
    multi-assignment and direction based combination. A sigmoid layer is added before
    the loss layer of a CNN to learn the binary code for CBIR [[70](#bib.bib70)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年，Gordo等人利用区域建议网络汇总相关区域，以形成描述符，从而优先考虑重要的物体区域[[65](#bib.bib65)]。Song等人（2016）计算了CNN与原始特征之间的提升结构损失[[66](#bib.bib66)]。监督深度哈希网络（DHN）通过控制量化误差来学习重要的图像表示[[67](#bib.bib67)]。与此同时，Cao等人引入了一个与DHN模型非常相似的深度量化网络（DQN）[[68](#bib.bib68)]。基于CNN的特征在[[69](#bib.bib69)]中通过排名感知的多分配和基于方向的组合进行汇总。在CNN的损失层之前添加了一个Sigmoid层，以学习用于CBIR的二进制码[[70](#bib.bib70)]。
- en: III-5 2017
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-5 2017
- en: In 2017, Cao et al. have proposed HashNet deep architecture to generate the
    hash code by a continuation method [[71](#bib.bib71)]. It learns the non-smooth
    binary activations using the continuation method to generate the binary hash codes
    from imbalanced similarity data. Gordo et al. (2017) have shown that the noisy
    training data, inappropriate deep architecture and suboptimal training procedure
    are the main hurdle to utilize the deep learning for image retrieval [[72](#bib.bib72)].
    Different masking schemes are used in [[73](#bib.bib73)] to select the prominent
    CNN features for image retrieval. A bilinear network with two parallel CNNs is
    also used as a feature extractors [[74](#bib.bib74)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，Cao等人提出了HashNet深度架构，通过续接方法生成哈希码[[71](#bib.bib71)]。它利用续接方法学习不光滑的二进制激活，从不平衡的相似性数据中生成二进制哈希码。Gordo等人（2017）表明，噪声训练数据、不适当的深度架构和次优的训练程序是利用深度学习进行图像检索的主要障碍[[72](#bib.bib72)]。不同的掩码方案在[[73](#bib.bib73)]中被用来选择突出的CNN特征进行图像检索。[[74](#bib.bib74)]中也使用了一个具有两个并行CNN的双线性网络作为特征提取器。
- en: III-6 2018
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-6 2018
- en: In 2018, Cao et al. have investigated a deep cauchy hashing (DCH) model for
    binary hash code with the help of a pairwise cross-entropy loss based on Cauchy
    distribution [[75](#bib.bib75)]. Su et al. have employed the greedy hash by transmitting
    the gradient as intact during the backpropagation for hash coding layer which
    uses the sign function in forward propagation [[76](#bib.bib76)]. Different approaches
    such as policy gradient [[77](#bib.bib77)] and series expansion [[78](#bib.bib78)]
    are also utilized to train the models. Deep index-compatible hashing (DICH) method
    [[79](#bib.bib79)] is investigated by minimizing the number of similar bits between
    the binary codes of inter-class images.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年，Cao等人研究了一种基于Cauchy分布的深度Cauchy哈希（DCH）模型，用于二进制哈希码，使用了基于成对交叉熵损失的帮助[[75](#bib.bib75)]。Su等人采用了贪婪哈希，通过在反向传播过程中保持梯度不变来进行哈希编码层的计算，该层在前向传播中使用符号函数[[76](#bib.bib76)]。不同的方法如策略梯度[[77](#bib.bib77)]和级数展开[[78](#bib.bib78)]也被用来训练模型。通过最小化类间图像二进制码的相似位数，研究了深度索引兼容哈希（DICH）方法[[79](#bib.bib79)]。
- en: III-7 2019
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-7 2019
- en: In 2019, a deep incremental hashing network (DIHN) is proposed in [[80](#bib.bib80)]
    to directly learn the hash codes corresponding to the new class coming images,
    while retaining the hash codes of existing class images. A supervised quantization
    based points representation on a unit hypersphere is used in deep spherical quantization
    (DSQ) model [[81](#bib.bib81)]. DistillHash [[82](#bib.bib82)] distills data pairs
    and learns deep hash functions from the distilled data set by employing the Bayesian
    learning framework. A deep progressive hashing (DPH) model is developed to generate
    a sequence of binary codes by utilizing the progressively expanded salient regions
    [[83](#bib.bib83)]. Adaptive loss function based deep hashing [[84](#bib.bib84)],
    just-maximizing-likelihood hashing (JMLH) [[85](#bib.bib85)] and deep variational
    binaries (DVB) [[86](#bib.bib86)] are other approaches discovered in 2019.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，提出了一种深度增量哈希网络（DIHN）[[80](#bib.bib80)]，以直接学习与新类别图像对应的哈希码，同时保留现有类别图像的哈希码。在深度球面量化（DSQ）模型[[81](#bib.bib81)]中使用了基于监督量化的单位超球面点表示。DistillHash[[82](#bib.bib82)]通过贝叶斯学习框架从蒸馏数据集中提炼数据对，并学习深度哈希函数。深度渐进哈希（DPH）模型[[83](#bib.bib83)]利用逐步扩展的显著区域生成一系列二进制代码。自适应损失函数基础的深度哈希[[84](#bib.bib84)]、最大似然哈希（JMLH）[[85](#bib.bib85)]和深度变分二进制（DVB）[[86](#bib.bib86)]是2019年发现的其他方法。
- en: III-8 2020
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-8 2020
- en: Recently, in 2020, Shen et al. have come up with a twin-bottleneck hashing (TBH)
    model between encoder and decoder networks [[87](#bib.bib87)]. They have employed
    the binary and continuous bottlenecks as the latent variables in a collaborative
    manner. Forcen et al. (2020) have utilized the last convolution layer of CNN representation
    by modeling the co-occurrences from deep convolutional features [[88](#bib.bib88)].
    A deep position-aware hashing (DPAH) model is proposed in 2020 [[89](#bib.bib89)]
    which constraints the distance between data samples and class centers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在2020年，Shen等人提出了一个编码器和解码器网络之间的双瓶颈哈希（TBH）模型[[87](#bib.bib87)]。他们以协作方式使用了二进制和连续瓶颈作为潜在变量。Forcen等人（2020）通过对深度卷积特征的共现建模，利用了CNN表示的最后卷积层[[88](#bib.bib88)]。2020年提出了一种深度位置感知哈希（DPAH）模型[[89](#bib.bib89)]，该模型约束数据样本与类别中心之间的距离。
- en: Most of the methods developed between 2011 and 2015 use the features learnt
    by the autoencoders and convolutional neural networks. However, these methods
    face the issues in terms of the less discriminative ability as the models are
    generally trained for the classification problem and the information loss due
    to the quantization of features. Image retrieval using deep learning has witnessed
    a huge growth in between 2016 and 2020\. As the image retrieval application needs
    feature learning for matching, different types of networks have been utilized
    to do so. The recent methods have designed the several objective functions which
    lead to the high inter class separation and high intra class condensation in feature
    space. Moreover, the development in different network architectures has also led
    to the growth in the image retrieval area. The key issue being addressed by deep
    learning methods is to learn very discriminative, robust and compact features
    for image retrieval.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数在2011年至2015年间开发的方法使用了由自编码器和卷积神经网络学习到的特征。然而，这些方法面临的问题是模型通常为分类问题而训练，导致区分能力较差以及由于特征量化造成的信息丢失。深度学习在2016年至2020年间的图像检索领域经历了巨大的增长。由于图像检索应用需要特征学习来进行匹配，因此采用了不同类型的网络。近期的方法设计了多个目标函数，导致特征空间中类间分离度高和类内凝聚度高。此外，不同网络架构的发展也推动了图像检索领域的增长。深度学习方法正在解决的关键问题是学习具有很强区分性、鲁棒性和紧凑性的图像检索特征。
- en: IV Different Supervision Categorization
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 不同监督分类
- en: This section covers the image retrieval methods in terms of the different supervision
    types. Basically, supervised, unsupervised, semi-supervised, weakly-supervised,
    pseudo-supervised and self-supervised approaches are included.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了不同监督类型下的图像检索方法。基本上，包括有监督、无监督、半监督、弱监督、伪监督和自监督方法。
- en: IV-A Supervised Approaches
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 有监督方法
- en: The supervised deep learning models are used by researchers very heavily to
    learn the class specific and discriminative features for image retrieval. In 2014,
    Xia et al. have used a CNN to learn the representation of images which is used
    to generate a hash code and class labels [[95](#bib.bib95)]. The promising performance
    is reported over MNIST, CIFAR-10 and NUS-WIDE datasets. Shen et al. (2015) [[96](#bib.bib96)]
    have proposed the supervised discrete hashing (SDH) based generation of image
    description with the help of the discrete cyclic coordinate descent for retrieval.
    Liu et al. (2016) have done the revolutionary work by introducing a deep supervised
    hashing (DSH) method to learn the binary codes from the similar/dissimilar pairs
    of images [[97](#bib.bib97)]. A similar work is also presented in deep pairwise-supervised
    hashing (DPSH) method for image retrieval [[98](#bib.bib98)]. The pair-wise labels
    are extended to the triplet labels (i.e., query, positive and negative images)
    to train a shared deep CNN model for feature learning [[99](#bib.bib99)]. An independent
    layer-wise local updates are performed in [[100](#bib.bib100)] to efficiently
    train a very deep supervised hashing (VDSH) model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员非常频繁地使用监督深度学习模型来学习类特定和辨别特征以进行图像检索。在2014年，Xia等人使用CNN学习图像的表示，用于生成哈希码和类别标签[[95](#bib.bib95)]。在MNIST，CIFAR-10和NUS-WIDE数据集上报告了有前景的性能。Shen等人（2015）[[96](#bib.bib96)]提出了基于监督离散哈希（SDH）的图像描述生成，借助离散循环坐标下降进行检索。Liu等人（2016）通过引入深度监督哈希（DSH）方法，进行了一项革命性工作，以从相似/不相似的图像对中学习二进制代码[[97](#bib.bib97)]。在深度成对监督哈希（DPSH）方法中也提出了类似的工作，用于图像检索[[98](#bib.bib98)]。将成对标签扩展为三元组标签（即查询，正图像和负图像），以训练共享的深度CNN模型进行特征学习[[99](#bib.bib99)]。在[[100](#bib.bib100)]中执行了独立的层级局部更新，以高效训练非常深的监督哈希（VDSH）模型。
- en: 'In 2017, Li et al. have used the classification information and the pairwise
    label information in a single framework for the learning of the deep supervised
    discrete hashing (DSDH) codes [[101](#bib.bib101)]. The supervised semantics-preserving
    deep hashing (SSDH) model integrates the retrieval and classification characteristics
    in feature learning [[102](#bib.bib102)]. The scalable image search is performed
    in [[103](#bib.bib103)] by introducing the following three characteristics: 1)
    minimizing the loss between the real-valued code and equivalent converted binary
    code, 2) ensuring the even distribution among each bit in the binary codes, and
    3) decreasing the redundancy of a bit in the binary code.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，Li等人将分类信息和成对标签信息在一个单一框架中用于深度监督离散哈希（DSDH）码的学习[[101](#bib.bib101)]。监督语义保留深度哈希（SSDH）模型整合了特征学习中的检索和分类特征[[102](#bib.bib102)]。通过引入以下三种特性，在[[103](#bib.bib103)]中执行了可扩展的图像搜索：1）最小化真实值代码与等效转换的二进制代码之间的损失，2）确保二进制代码中每个位的均匀分布，3）减少二进制代码中一个位的冗余。
- en: The supervised training has been also the choice in asymmetric hashing [[104](#bib.bib104)].
    A deep product quantization (DPQ) model is followed in supervised learning mode
    for image search and retrieval [[105](#bib.bib105)]. The supervised deep feature
    embedding is also used with the hand crafted features [[106](#bib.bib106)]. A
    very recently, a multi-Level hashing of deep features is performed by Ng et al.
    (2020) [[107](#bib.bib107)]. An angular hashing loss function is used to train
    the network in the supervised fashion [[108](#bib.bib108)]. A supervised hashing
    is also used for the multi-deep ranking [[109](#bib.bib109)] to improve the retrieval
    efficiency. Other supervised approaches are deep binary hash codes [[51](#bib.bib51)],
    deep hashing network [[67](#bib.bib67)], deep spherical quantization [[81](#bib.bib81)],
    and adaptive loss based supervised deep learning to hash [[84](#bib.bib84)].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 监督训练也被选择用于非对称哈希[[104](#bib.bib104)]。在图像搜索和检索的监督学习模式中，采用了深度产品量化（DPQ）模型[[105](#bib.bib105)]。监督深度特征嵌入也用于与手工特征[[106](#bib.bib106)]。最近，Ng等人（2020）进行了深度特征的多级哈希[[107](#bib.bib107)]。使用角度哈希损失函数以监督方式训练网络[[108](#bib.bib108)]。监督哈希也用于多深度排名[[109](#bib.bib109)]以提高检索效率。其他监督方法包括深度二进制哈希码[[51](#bib.bib51)]，深度哈希网络[[67](#bib.bib67)]，深度球形量化[[81](#bib.bib81)]，以及基于自适应损失的监督深度学习哈希[[84](#bib.bib84)]。
- en: IV-B Unsupervised Approaches
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 无监督方法
- en: Though the supervised models have shown promising performance for retrieval,
    it is difficult to get the labelled large-scale data always. Thus, several unsupervised
    models have been also investigated which do not require the class labels. The
    unsupervised models generally enforce the constraints on hash code and/or generated
    output to learn the features.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有监督模型在检索中表现出色，但始终获得标注的大规模数据并不容易。因此，还研究了几种无需类别标签的无监督模型。无监督模型通常对哈希码和/或生成的输出施加约束以学习特征。
- en: Erin et al. (2015) [[110](#bib.bib110)] have used the deep networks in an unsupervised
    manner to learn the hash code with the help of the constraints like quantization
    loss, balanced bits and independent bits. Huang et al. (2016) [[111](#bib.bib111)]
    have utilized the CNN coupled with unsupervised discriminative clustering. In
    an outstanding work, DeepBit utilizes the constraints like minimal quantization
    loss, evenly distributed codes and uncorrelated bits for unsupervised image retrieval
    [[112](#bib.bib112)], [[113](#bib.bib113)]. In order to improve the robustness
    of DeepBit, a rotation data augmentation based fine tuning is also performed.
    However, the DeepBit model suffers with the severe quantization loss due to the
    rigid binarization of data using sign function without considering its distribution
    property. Deep binary descriptor with multiquantization (DBD-MQ) [[114](#bib.bib114)]
    tackles the quantization problem of DeepBit by jointly learning the parameters
    and the binarization functions using a K-AutoEncoders (KAEs).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Erin 等人（2015）[[110](#bib.bib110)] 使用深度网络在无监督模式下，通过量化损失、平衡位和独立位等约束来学习哈希码。Huang
    等人（2016）[[111](#bib.bib111)] 则利用了结合无监督判别聚类的卷积神经网络（CNN）。在一项出色的工作中，DeepBit 利用最小量化损失、均匀分布的编码和不相关位等约束进行无监督图像检索
    [[112](#bib.bib112)]，[[113](#bib.bib113)]。为了提高 DeepBit 的鲁棒性，还进行了基于旋转数据增强的微调。然而，由于使用符号函数对数据进行严格二值化而未考虑其分布特性，DeepBit
    模型遭遇了严重的量化损失。带有多量化的深度二值描述符（DBD-MQ）[[114](#bib.bib114)] 通过使用 K-自动编码器（KAEs）共同学习参数和二值化函数来解决
    DeepBit 的量化问题。
- en: It is observed in [[115](#bib.bib115)] that unsupervised CNN can learn more
    distinctive features if fine tuned with hard positive and hard negative examples.
    The patch representation using a patch convolutional kernel network is also adapted
    for patch retrieval [[116](#bib.bib116)]. An anchor image, a rotated image and
    a random image based triplets are used in unsupervised triplet hashing (UTH) to
    learn the binary codes for image retrieval [[117](#bib.bib117)]. The UTH objective
    function uses the combination of discriminative loss, quantization loss and entropy
    loss. An unsupervised similarity-adaptive deep hashing (SADH) is proposed in [[118](#bib.bib118)]
    by updating a similarity graph and optimizing the binary codes. Xu et al. (2018)
    [[119](#bib.bib119)] have proposed a semantic-aware part weighted aggregation
    using part-based detectors for CBIR systems. Unsupervised generative adversarial
    networks [[120](#bib.bib120)], [[121](#bib.bib121)], [[122](#bib.bib122)] are
    also investigated for image retrieval. The distill data pairs [[82](#bib.bib82)]
    and deep variational networks [[86](#bib.bib86)] are also used for unsupervised
    image retrieval. The pseudo triplets based unsupervised deep triplet hashing (UDTH)
    technique [[123](#bib.bib123)] is introduced for scalable image retrieval. Very
    recently unsupervised deep transfer learning has been exploited by Liu et al.
    (2020) [[124](#bib.bib124)] for retrieval in remote sensing images.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[115](#bib.bib115)] 中观察到，如果用困难的正样本和负样本进行微调，无监督卷积神经网络（CNN）可以学习到更具区分性的特征。使用补丁卷积核网络的补丁表示法也被适用于补丁检索
    [[116](#bib.bib116)]。无监督三元组哈希（UTH）使用锚图像、旋转图像和随机图像组成的三元组来学习用于图像检索的二进制码 [[117](#bib.bib117)]。UTH
    目标函数结合了判别损失、量化损失和熵损失。[[118](#bib.bib118)] 提出了无监督相似性自适应深度哈希（SADH），通过更新相似性图并优化二进制码。Xu
    等人（2018）[[119](#bib.bib119)] 提出了使用基于部分的检测器的语义感知部分加权聚合方法用于内容基图像检索（CBIR）系统。无监督生成对抗网络
    [[120](#bib.bib120)]，[[121](#bib.bib121)]，[[122](#bib.bib122)] 也被研究用于图像检索。还使用了蒸馏数据对
    [[82](#bib.bib82)] 和深度变分网络 [[86](#bib.bib86)] 进行无监督图像检索。伪三元组基于的无监督深度三元组哈希（UDTH）技术
    [[123](#bib.bib123)] 被引入用于可扩展的图像检索。最近，Liu 等人（2020）[[124](#bib.bib124)] 也利用了无监督深度迁移学习来进行遥感图像检索。
- en: Though the unsupervised models do not need labelled data, its performance is
    generally lower than the supervised approaches. Thus, researchers have explored
    the models between supervised and unsupervised, such as semi-supervised, weakly-supervised,
    pseudo-supervised and self-supervised.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无监督模型不需要标记数据，但其性能通常低于监督方法。因此，研究人员探索了监督和无监督之间的模型，例如半监督、弱监督、伪监督和自监督。
- en: \startchronology
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: \startchronology
- en: '[startyear=1,stopyear=50, startdate=false, color=blue!100, stopdate=false,
    arrow=true, arrowwidth=0.8cm, arrowheight=0.5cm] \setupchronoeventtextstyle=,datestyle=
    \chronoevent[markdepth=-15pt, year=false]12011 \chronoevent[markdepth=-30pt,year=false,textwidth=1cm]2Deep
    Autoencoder [[58](#bib.bib58)] \chronoevent[markdepth=-15pt, year=false]32013
    \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]4Denoising Autoencoder [[60](#bib.bib60)]'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[startyear=1,stopyear=50, startdate=false, color=blue!100, stopdate=false,
    arrow=true, arrowwidth=0.8cm, arrowheight=0.5cm] \setupchronoeventtextstyle=,datestyle=
    \chronoevent[markdepth=-15pt, year=false]12011 \chronoevent[markdepth=-30pt,year=false,textwidth=1cm]2深度自编码器
    [[58](#bib.bib58)] \chronoevent[markdepth=-15pt, year=false]32013 \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]4去噪自编码器
    [[60](#bib.bib60)]'
- en: \chronoevent
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=-15pt, year=false]52014 \chronoevent[markdepth=-30pt,year=false,textwidth=1cm]6Corres-pondance
    Autoencoder [[125](#bib.bib125)] \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]8Siamese
    [[62](#bib.bib62)]'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=-15pt, year=false]52014 \chronoevent[markdepth=-30pt,year=false,textwidth=1cm]6对应自编码器
    [[125](#bib.bib125)] \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]8Siamese
    [[62](#bib.bib62)]'
- en: \chronoevent
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=65pt,year=false,textwidth=2cm]7CNN Features [[95](#bib.bib95)],
    [[126](#bib.bib126)] \chronoevent[markdepth=40pt,year=false,textwidth=2cm]7Neural
    Code [[61](#bib.bib61)] \chronoevent[markdepth=5pt,year=false,textwidth=2cm]7CNN
    off-the-shelf [[32](#bib.bib32)]'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=65pt,year=false,textwidth=2cm]7CNN 特征 [[95](#bib.bib95)], [[126](#bib.bib126)]
    \chronoevent[markdepth=40pt,year=false,textwidth=2cm]7神经编码 [[61](#bib.bib61)]
    \chronoevent[markdepth=5pt,year=false,textwidth=2cm]7CNN 即用 [[32](#bib.bib32)]'
- en: \chronoevent
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=-15pt, year=false]92015 \chronoevent[markdepth=-30pt,year=false,textwidth=1cm]10Binary
    Autoencoder [[127](#bib.bib127)] \chronoevent[markdepth=-70pt,year=false,textwidth=1cm]11Patch
    + Siamese [[128](#bib.bib128)], [[129](#bib.bib129)] \chronoevent[markdepth=-105pt,year=false,textwidth=1cm]13Adjacency
    Consistency Triplet [[64](#bib.bib64)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]13Triplet
    Ranking [[63](#bib.bib63)]'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=-15pt, year=false]92015 \chronoevent[markdepth=-30pt,year=false,textwidth=1cm]10二进制自编码器
    [[127](#bib.bib127)] \chronoevent[markdepth=-70pt,year=false,textwidth=1cm]11补丁
    + Siamese [[128](#bib.bib128)], [[129](#bib.bib129)] \chronoevent[markdepth=-105pt,year=false,textwidth=1cm]13邻接一致性三元组
    [[64](#bib.bib64)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]13三元组排序
    [[63](#bib.bib63)]'
- en: \chronoevent
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=-15pt, year=false]142016 \chronoevent[markdepth=-78pt,year=false,textwidth=1cm]16Triplet
    Based [[130](#bib.bib130)], [[131](#bib.bib131)], [[132](#bib.bib132)], [[99](#bib.bib99)],
    [[133](#bib.bib133)] \chronoevent[markdepth=-45pt,year=false,textwidth=1cm]16Image
    Pair [[97](#bib.bib97)]'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=-15pt, year=false]142016 \chronoevent[markdepth=-78pt,year=false,textwidth=1cm]16基于三元组
    [[130](#bib.bib130)], [[131](#bib.bib131)], [[132](#bib.bib132)], [[99](#bib.bib99)],
    [[133](#bib.bib133)] \chronoevent[markdepth=-45pt,year=false,textwidth=1cm]16图像对
    [[97](#bib.bib97)]'
- en: \chronoevent
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=70pt,year=false,textwidth=2cm]15Pairwise Labels [[97](#bib.bib97)],
    [[98](#bib.bib98)] \chronoevent[markdepth=38pt,year=false,textwidth=2cm]15Different
    Losses [[65](#bib.bib65)], [[68](#bib.bib68)], [[71](#bib.bib71)] \chronoevent[markdepth=5pt,year=false,textwidth=2cm]15CNN
    Features [[112](#bib.bib112)], [[97](#bib.bib97)], [[134](#bib.bib134)]'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=70pt,year=false,textwidth=2cm]15成对标签 [[97](#bib.bib97)], [[98](#bib.bib98)]
    \chronoevent[markdepth=38pt,year=false,textwidth=2cm]15不同损失 [[65](#bib.bib65)],
    [[68](#bib.bib68)], [[71](#bib.bib71)] \chronoevent[markdepth=5pt,year=false,textwidth=2cm]15CNN
    特征 [[112](#bib.bib112)], [[97](#bib.bib97)], [[134](#bib.bib134)]'
- en: \chronoevent
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=-15pt, year=false]172017 \chronoevent[markdepth=-120pt,year=false,textwidth=1cm]19Triplet
    + Siamese [[72](#bib.bib72)] \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]19Triplet
    Quantization Loss [[135](#bib.bib135)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]19Triplet
    Hashing [[117](#bib.bib117)] \chronoevent[markdepth=-50pt,year=false,textwidth=1cm]21Fisher
    + Siamese [[136](#bib.bib136)] \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]24Attention
    Based [[55](#bib.bib55)], [[137](#bib.bib137)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]24Two-stream
    Attention [[138](#bib.bib138)] \chronoevent[markdepth=-85pt,year=false,textwidth=1cm]27LSTM
    Based [[139](#bib.bib139)], [[139](#bib.bib139)] \chronoevent[markdepth=-35pt,year=false,textwidth=1cm]27Recurrent
    Neural Hashing [[140](#bib.bib140)]'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=-15pt, year=false]172017 \chronoevent[markdepth=-120pt,year=false,textwidth=1cm]19Triplet
    + Siamese [[72](#bib.bib72)] \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]19Triplet
    Quantization Loss [[135](#bib.bib135)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]19Triplet
    Hashing [[117](#bib.bib117)] \chronoevent[markdepth=-50pt,year=false,textwidth=1cm]21Fisher
    + Siamese [[136](#bib.bib136)] \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]24Attention
    Based [[55](#bib.bib55)], [[137](#bib.bib137)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]24Two-stream
    Attention [[138](#bib.bib138)] \chronoevent[markdepth=-85pt,year=false,textwidth=1cm]27LSTM
    Based [[139](#bib.bib139)], [[139](#bib.bib139)] \chronoevent[markdepth=-35pt,year=false,textwidth=1cm]27Recurrent
    Neural Hashing [[140](#bib.bib140)]'
- en: \chronoevent
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=55pt,year=false,textwidth=1.5cm]21HashNet [[71](#bib.bib71)] \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]21CNN
    Features [[114](#bib.bib114)], [[103](#bib.bib103)], [[141](#bib.bib141)], [[142](#bib.bib142)]
    \chronoevent[markdepth=75pt,year=false,textwidth=1.5cm]24GAN Based [[143](#bib.bib143)]'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=55pt,year=false,textwidth=1.5cm]21HashNet [[71](#bib.bib71)] \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]21CNN
    Features [[114](#bib.bib114)], [[103](#bib.bib103)], [[141](#bib.bib141)], [[142](#bib.bib142)]
    \chronoevent[markdepth=75pt,year=false,textwidth=1.5cm]24GAN Based [[143](#bib.bib143)]'
- en: \chronoevent
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=-15pt, year=false]282018 \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]30Binary
    Siamese [[144](#bib.bib144)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]32Attention
    GAN [[145](#bib.bib145)]'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=-15pt, year=false]282018 \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]30Binary
    Siamese [[144](#bib.bib144)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]32Attention
    GAN [[145](#bib.bib145)]'
- en: \chronoevent
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=72pt,year=false,textwidth=2cm]29SSAH [[146](#bib.bib146)], SSGAH
    [[147](#bib.bib147)], GDH [[148](#bib.bib148)] \chronoevent[markdepth=50pt,year=false,textwidth=2.5cm]29HashGAN
    [[120](#bib.bib120)], [[149](#bib.bib149)] \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]29Binary
    GAN [[121](#bib.bib121)], Regularized GAN [[150](#bib.bib150)] \chronoevent[markdepth=5pt,year=false,textwidth=1cm]33CNN
    Features [[118](#bib.bib118)], [[113](#bib.bib113)], [[151](#bib.bib151)] \chronoevent[markdepth=67pt,year=false,textwidth=1.5cm]34Reinforcement
    Learning Based [[77](#bib.bib77)]'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=72pt,year=false,textwidth=2cm]29SSAH [[146](#bib.bib146)], SSGAH
    [[147](#bib.bib147)], GDH [[148](#bib.bib148)] \chronoevent[markdepth=50pt,year=false,textwidth=2.5cm]29HashGAN
    [[120](#bib.bib120)], [[149](#bib.bib149)] \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]29Binary
    GAN [[121](#bib.bib121)], Regularized GAN [[150](#bib.bib150)] \chronoevent[markdepth=5pt,year=false,textwidth=1cm]33CNN
    Features [[118](#bib.bib118)], [[113](#bib.bib113)], [[151](#bib.bib151)] \chronoevent[markdepth=67pt,year=false,textwidth=1.5cm]34Reinforcement
    Learning Based [[77](#bib.bib77)]'
- en: \chronoevent
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=-15pt, year=false]352019 \chronoevent[markdepth=-115pt,year=false,textwidth=1.2cm]36Deep
    Variational Binaries [[86](#bib.bib86)] \chronoevent[markdepth=-75pt,year=false,textwidth=1.5cm]36Unsupervised
    Autoencoder [[123](#bib.bib123)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.5cm]36Relaxed
    Binary Autoencoder [[152](#bib.bib152)] \chronoevent[markdepth=-100pt,year=false,textwidth=1.2cm]39Triplet
    Based [[153](#bib.bib153)], [[123](#bib.bib123)] \chronoevent[markdepth=-125pt,year=false,textwidth=1cm]41Gradient
    Attention [[154](#bib.bib154)] \chronoevent[markdepth=-72pt,year=false,textwidth=1cm]41Spatial
    Attention [[155](#bib.bib155)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]41Two-stream
    Attention [[156](#bib.bib156)] \chronoevent[markdepth=-100pt,year=false,textwidth=1.2cm]43LSTM
    Based [[83](#bib.bib83)], [[157](#bib.bib157)]'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=-15pt, year=false]352019 \chronoevent[markdepth=-115pt,year=false,textwidth=1.2cm]36Deep
    Variational Binaries [[86](#bib.bib86)] \chronoevent[markdepth=-75pt,year=false,textwidth=1.5cm]36Unsupervised
    Autoencoder [[123](#bib.bib123)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.5cm]36Relaxed
    Binary Autoencoder [[152](#bib.bib152)] \chronoevent[markdepth=-100pt,year=false,textwidth=1.2cm]39Triplet
    Based [[153](#bib.bib153)], [[123](#bib.bib123)] \chronoevent[markdepth=-125pt,year=false,textwidth=1cm]41Gradient
    Attention [[154](#bib.bib154)] \chronoevent[markdepth=-72pt,year=false,textwidth=1cm]41Spatial
    Attention [[155](#bib.bib155)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]41Two-stream
    Attention [[156](#bib.bib156)] \chronoevent[markdepth=-100pt,year=false,textwidth=1.2cm]43LSTM
    Based [[83](#bib.bib83)], [[157](#bib.bib157)]'
- en: \chronoevent
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=5pt,year=false,textwidth=1cm]39CNN Features [[106](#bib.bib106)],
    [[158](#bib.bib158)] \chronoevent[markdepth=70pt,year=false,textwidth=1.5cm]42GAN
    Based [[159](#bib.bib159)], [[160](#bib.bib160)] \chronoevent[markdepth=38pt,year=false,textwidth=1.5cm]42Unsupervised
    GAN [[122](#bib.bib122)] \chronoevent[markdepth=5pt,year=false,textwidth=1cm]42SSAH
    [[161](#bib.bib161)]'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=5pt,year=false,textwidth=1cm]39CNN 特征 [[106](#bib.bib106)], [[158](#bib.bib158)]
    \chronoevent[markdepth=70pt,year=false,textwidth=1.5cm]42基于 GAN [[159](#bib.bib159)],
    [[160](#bib.bib160)] \chronoevent[markdepth=38pt,year=false,textwidth=1.5cm]42无监督
    GAN [[122](#bib.bib122)] \chronoevent[markdepth=5pt,year=false,textwidth=1cm]42SSAH
    [[161](#bib.bib161)]'
- en: \chronoevent
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=-15pt, year=false]442020 \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]45Bottleneck
    Autoencoder [[87](#bib.bib87)] \chronoevent[markdepth=-125pt,year=false,textwidth=1cm]48SBS-CNN
    [[124](#bib.bib124)] \chronoevent[markdepth=-95pt,year=false,textwidth=1cm]48Pairwise
    [[162](#bib.bib162)] \chronoevent[markdepth=-63pt,year=false,textwidth=1.5cm]48Adversarial
    Siamese [[163](#bib.bib163)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]50Second-Order
    Similarity [[164](#bib.bib164)]'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=-15pt, year=false]442020 \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]45瓶颈自编码器
    [[87](#bib.bib87)] \chronoevent[markdepth=-125pt,year=false,textwidth=1cm]48SBS-CNN
    [[124](#bib.bib124)] \chronoevent[markdepth=-95pt,year=false,textwidth=1cm]48成对
    [[162](#bib.bib162)] \chronoevent[markdepth=-63pt,year=false,textwidth=1.5cm]48对抗性孪生
    [[163](#bib.bib163)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]50二阶相似性
    [[164](#bib.bib164)]'
- en: \chronoevent
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: \chronoevent
- en: '[markdepth=5pt,year=false,textwidth=1cm]45CNN Features [[165](#bib.bib165)]
    \chronoevent[markdepth=75pt,year=false,textwidth=1.5cm]47BGAN+ [[166](#bib.bib166)]
    \chronoevent[markdepth=35pt,year=false,textwidth=1.5cm]47Stack GAN [[163](#bib.bib163)]
    \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]50Reinforcement Learning
    Based [[167](#bib.bib167)]'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[markdepth=5pt,year=false,textwidth=1cm]45CNN 特征 [[165](#bib.bib165)] \chronoevent[markdepth=75pt,year=false,textwidth=1.5cm]47BGAN+
    [[166](#bib.bib166)] \chronoevent[markdepth=35pt,year=false,textwidth=1.5cm]47Stack
    GAN [[163](#bib.bib163)] \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]50基于强化学习
    [[167](#bib.bib167)]'
- en: \stopchronology
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: \stopchronology
- en: 'Figure 5: A chronological view of deep learning based image retrieval methods
    depicting the different type of neural networks used from 2011 to 2020\. The convolutional
    neural network, autoencoder network, siamese & triplet network, recurrent neural
    network, generative adversarial network, attention network and reinforcement learning
    network based deep learning approches for image retrieval are depicted in Red,
    Cyan, Magenta, Black, Blue, Green, and Yellow colors, respectively.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：深度学习基于图像检索方法的时间顺序视图，描绘了从 2011 年到 2020 年使用的不同类型的神经网络。卷积神经网络、自编码器网络、孪生网络与三元组网络、递归神经网络、生成对抗网络、注意力网络和强化学习网络基于图像检索的深度学习方法分别用红色、青色、洋红色、黑色、蓝色、绿色和黄色表示。
- en: IV-C Semi, Weakly, Pseudo and Self -supervised Approaches
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 半监督、弱监督、伪监督和自监督方法
- en: The semi-supervised approaches generally use a combination of labelled and unlabelled
    data for feature learning [[168](#bib.bib168)], [[169](#bib.bib169)]. Semi-supervised
    deep hashing (SSDH) [[141](#bib.bib141)] uses labelled data for the empirical
    error minimization and both labelled and unlabelled data for embedding error minimization.
    The generative adversarial learning has been also utilized extensively in semi-supervised
    image retrieval [[147](#bib.bib147)], [[161](#bib.bib161)], [[170](#bib.bib170)].
    A teacher-student based semi-supervised image retrieval [[171](#bib.bib171)] uses
    the pairwise information learnt by the teacher network as the guidance to train
    the student network.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督方法通常使用标注数据和未标注数据的组合进行特征学习 [[168](#bib.bib168)], [[169](#bib.bib169)]。半监督深度哈希
    (SSDH) [[141](#bib.bib141)] 使用标注数据进行经验误差最小化，并利用标注和未标注数据进行嵌入误差最小化。生成对抗学习也被广泛应用于半监督图像检索
    [[147](#bib.bib147)], [[161](#bib.bib161)], [[170](#bib.bib170)]。基于教师-学生的半监督图像检索
    [[171](#bib.bib171)] 利用教师网络学到的成对信息作为指导，训练学生网络。
- en: Weakly-supervised approaches have been also explored for the image retrieval.
    Tang et al. (2017) have put forward a weakly-supervised multimodal hashing (WMH)
    by utilizing the local discriminative and geometric structures in the visual space
    [[172](#bib.bib172)]. Guan et al. (2018) [[173](#bib.bib173)] have performed the
    pre-training in weakly-supervised mode and fine-tuning in supervised mode. A weakly
    supervised deep hashing using tag embeddings (WDHT) [[174](#bib.bib174)] utilizes
    the word2vec semantic embeddings. A semantic guided hashing (SGH) [[175](#bib.bib175)]
    is used for image retrieval by simultaneously employing the weakly-supervised
    tag information and the inherent data relations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督方法也被探索用于图像检索。Tang等（2017）提出了一种利用视觉空间中的局部区分和几何结构的弱监督多模态哈希（WMH）[[172](#bib.bib172)]。Guan等（2018）[[173](#bib.bib173)]在弱监督模式下进行了预训练，并在监督模式下进行了微调。利用tag嵌入的弱监督深度哈希（WDHT）[[174](#bib.bib174)]使用了word2vec语义嵌入。语义引导哈希（SGH）[[175](#bib.bib175)]通过同时利用弱监督标签信息和固有的数据关系来进行图像检索。
- en: The pseudo suervised networks have been also developed for image retrieval.
    The pseudo triplets are utilized in [[123](#bib.bib123)] for unsupervised image
    retrieval. K-means clustering based pseudo labels are generated and used for the
    training of a deep hashing network [[176](#bib.bib176)], [[177](#bib.bib177)].
    An appealing performance has been observed using pseudo labels over CIFAR-10 and
    Flickr datasets for image retrieval.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 伪监督网络也被开发用于图像检索。伪三元组在[[123](#bib.bib123)]中用于无监督图像检索。基于K-means聚类的伪标签被生成并用于训练深度哈希网络[[176](#bib.bib176)]，[[177](#bib.bib177)]。在CIFAR-10和Flickr数据集上使用伪标签进行图像检索时，观察到了令人满意的性能。
- en: The self-supervision is another way of supervision used in some research works
    for image retrieval. For example, Li et al. (2018) [[146](#bib.bib146)] have used
    the adversarial networks in self-supervision mode by utilizing the multi-label
    annotations. Zhang et al. (2016) [[178](#bib.bib178)] have introduced a self-supervised
    temporal hashing (SSTH) for video retrieval.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督是一些研究工作中用于图像检索的另一种监督方式。例如，Li等（2018）[[146](#bib.bib146)]通过利用多标签注释，在自监督模式下使用对抗网络。Zhang等（2016）[[178](#bib.bib178)]为视频检索引入了一种自监督时间哈希（SSTH）。
- en: IV-D Summary
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 总结
- en: 'Following are the take aways from the above discussion on deep learning based
    models from the supervision perspective:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从监督角度对深度学习模型的讨论中的要点：
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The supervised approaches utilize the class-specific semantic information through
    the classification error apart from the other objectives related to the hash code
    generation. Generally, the performance of supervised models is better than other
    models due to learning of the fine-grained and class specific information.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 监督方法通过分类误差利用类特定的语义信息，除了与哈希码生成相关的其他目标。通常，由于学习了细粒度和类特定的信息，监督模型的性能优于其他模型。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The unsupervised models make use of the unsupervised constraints on hash code
    (i.e., quantization loss, independent bits, etc.) and/or data reconstruction (i.e.,
    using an autoencoder type of networks) to learn the features.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无监督模型利用对哈希码（即量化损失、独立位等）和/或数据重构（即使用自编码器类型的网络）施加的无监督约束来学习特征。
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The semi-supervised approaches exploit the labelled and un-labelled data for
    the feature learning using deep networks. These approaches generally utilize the
    information from different modalities using different networks.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半监督方法利用标记和未标记数据进行深度网络特征学习。这些方法通常利用来自不同模态的信息，通过不同的网络进行处理。
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The pseudo-supervised approaches generate the pseudo labels using some other
    methods to facilitate the training using generated labels. The self-supervised
    methods generate the temporal or generative information to learn the models over
    the training epochs.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伪监督方法使用其他方法生成伪标签，以利用生成的标签促进训练。自监督方法生成时间性或生成性信息，以便在训练周期中学习模型。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The minimal quantization error, independent bits, low dimensional feature, and
    discriminative code are the common objectives for most of the retrieval methods.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最小化量化误差、独立位、低维特征和区分性编码是大多数检索方法的共同目标。
- en: V Network Types For Image Retrieval
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 图像检索的网络类型
- en: In this section, deep learning based image retrieval approaches are presented
    in terms of the different architectures. A chronological overview from 2011 to
    2020 is illustrated in Fig. [5](#S4.F5 "Figure 5 ‣ IV-B Unsupervised Approaches
    ‣ IV Different Supervision Categorization ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning") for different type of networks for image retrieval.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，基于深度学习的图像检索方法将从不同架构的角度进行介绍。图 [5](#S4.F5 "Figure 5 ‣ IV-B Unsupervised Approaches
    ‣ IV Different Supervision Categorization ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning") 展示了从2011年到2020年的按时间顺序的概述，涵盖了不同类型的图像检索网络。
- en: V-A Convolutional Neural Networks for Image Retrieval
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 卷积神经网络用于图像检索
- en: Convolutional neural networks (CNN) based feature learning has been utilized
    extensively for image retrieval as shown in Fig. [4](#S3.F4 "Figure 4 ‣ III-2
    2014 ‣ III Evolution of Deep Learning for Content Based Image Retrieval (CBIR)
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning"). In 2014,
    CNN features off-the-shelf have shown a tremendous performance gain for image
    recognition and retrieval as compared to the hand-crafted features [[32](#bib.bib32)].
    At the same time the activations of trained CNN has been also explored as the
    neural code for retrieval [[61](#bib.bib61)]. An image representation learning
    has been also performed using the CNN model to generate the descriptor for image
    retrieval [[95](#bib.bib95)]. In 2016, pairwise labels are exploited to learn
    the CNN feature for image retrieval [[97](#bib.bib97)], [[98](#bib.bib98)]. The
    CNN activations are heavily used to generate the hash codes for efficient image
    retrieval by employing the different losses [[65](#bib.bib65)], [[68](#bib.bib68)],
    [[71](#bib.bib71)]. The abstract features of CNN are learnt for the image retrieval
    in different modes, such as unsupervised image retrieval [[112](#bib.bib112)],
    [[114](#bib.bib114)], [[118](#bib.bib118)], [[113](#bib.bib113)], supervised image
    retrieval [[95](#bib.bib95)], [[97](#bib.bib97)], [[103](#bib.bib103)], [[106](#bib.bib106)],
    semi-supervised image retrieval [[141](#bib.bib141)], cross-modal retrieval [[134](#bib.bib134)],
    [[142](#bib.bib142)], sketch based image retrieval [[151](#bib.bib151)], [[158](#bib.bib158)],
    and object retrieval [[126](#bib.bib126)], [[165](#bib.bib165)].
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 基于卷积神经网络（CNN）的特征学习已被广泛应用于图像检索，如图 [4](#S3.F4 "Figure 4 ‣ III-2 2014 ‣ III Evolution
    of Deep Learning for Content Based Image Retrieval (CBIR) ‣ A Decade Survey of
    Content Based Image Retrieval using Deep Learning") 所示。2014年，相比于手工设计的特征，现成的CNN特征在图像识别和检索中表现出显著的性能提升[[32](#bib.bib32)]。同时，训练后的CNN激活也被探索作为检索的神经编码[[61](#bib.bib61)]。使用CNN模型进行图像表示学习以生成图像检索描述符也有所实现[[95](#bib.bib95)]。2016年，利用成对标签学习CNN特征进行图像检索[[97](#bib.bib97)],
    [[98](#bib.bib98)]。CNN激活被广泛用于生成哈希码以提高图像检索效率，采用了不同的损失函数[[65](#bib.bib65)], [[68](#bib.bib68)],
    [[71](#bib.bib71)]。CNN的抽象特征用于在不同模式下的图像检索，例如无监督图像检索[[112](#bib.bib112)], [[114](#bib.bib114)],
    [[118](#bib.bib118)], [[113](#bib.bib113)]，有监督图像检索[[95](#bib.bib95)], [[97](#bib.bib97)],
    [[103](#bib.bib103)], [[106](#bib.bib106)]，半监督图像检索[[141](#bib.bib141)]，跨模态检索[[134](#bib.bib134)],
    [[142](#bib.bib142)]，基于草图的图像检索[[151](#bib.bib151)], [[158](#bib.bib158)]，以及物体检索[[126](#bib.bib126)],
    [[165](#bib.bib165)]。
- en: '![Refer to caption](img/a354b05a602ae4a63b7bbf4baf89eafe.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a354b05a602ae4a63b7bbf4baf89eafe.png)'
- en: 'Figure 6: A typical Autoencoder network consisting of an Encoder and a Decoder
    network. Generally, the encoder is a CNN and the decoder is an up-CNN. The output
    of the encoder is a latent space which is used to generate the hash codes.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：一个典型的自编码器网络由编码器和解码器网络组成。通常，编码器是卷积神经网络（CNN），解码器是上卷积神经网络（up-CNN）。编码器的输出是一个潜在空间，用于生成哈希码。
- en: V-B Autoencoder Networks based Image Retrieval
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 自编码器网络基于图像检索
- en: Autoencoder ($AE$) is a type of unsupervised neural network which can be used
    to reconstruct the input image from the latent space as portrayed in Fig. [6](#S5.F6
    "Figure 6 ‣ V-A Convolutional Neural Networks for Image Retrieval ‣ V Network
    Types For Image Retrieval ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning"). Basically, it consists of two networks, namely encoder ($En$)
    and decoder ($De$). The encoder network transforms the input ($I$) into latent
    feature space ($z$) as $En:I\rightarrow z$. Whereas, the decoder network tries
    to reconstruct the original image ($I^{\prime}$) from latent feature space as
    $De:z\rightarrow I^{\prime}$. The model is trained by minimizing the reconstruction
    error between original image ($I$) and reconstructed image ($I^{\prime}$) using
    $L_{1}$ or $L_{2}$ loss function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（$AE$）是一种无监督神经网络，可以从潜在空间重建输入图像，如图[6](#S5.F6 "Figure 6 ‣ V-A Convolutional
    Neural Networks for Image Retrieval ‣ V Network Types For Image Retrieval ‣ A
    Decade Survey of Content Based Image Retrieval using Deep Learning")所示。基本上，它由两个网络组成，即编码器（$En$）和解码器（$De$）。编码器网络将输入（$I$）转换为潜在特征空间（$z$），表示为$En:I\rightarrow
    z$。而解码器网络试图从潜在特征空间重建原始图像（$I^{\prime}$），表示为$De:z\rightarrow I^{\prime}$。该模型通过最小化原始图像（$I$）和重建图像（$I^{\prime}$）之间的重建误差来训练，使用$L_{1}$或$L_{2}$损失函数。
- en: The autoencoders have been very intensively used to learn the features as the
    latent space for image retrieval. In the initial attempts, the deep autoencoder
    was used for image retrieval in 2011 [[58](#bib.bib58)]. A stacked denoising autoencoder
    is used to train the multiple deep neural networks for retrieval task [[60](#bib.bib60)].
    Feng et al. (2014) have utilized the correspondence autoencoder (Corr-AE) for
    cross-modal retrieval [[125](#bib.bib125)]. A binary autoencoder is used to learn
    the binary code for fast image retrieval by reconstructing the image from that
    binary code function [[127](#bib.bib127)]. The use of autoencoder in image retrieval
    has witnessed a huge progressed in recent years, such as Deep variational binaries
    (DVB) using the variational Bayesian networks [[86](#bib.bib86)]; Autoencoder
    over the triplet [[123](#bib.bib123)]; and Relaxed binary autoencoder (RBA) [[152](#bib.bib152)]
    are investigated in 2019. In a recent work, double latent bottlenecks is used
    in autoencoder [[87](#bib.bib87)]. It includes binary latent variable and continuous
    latent variable. The latent variable bottleneck exchanges crucial information
    collaboratively and the binary codes bottleneck uses a code-driven graph to capture
    the intrinsic data structure.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器已被广泛用于学习作为图像检索的潜在空间的特征。在最初的尝试中，2011年使用了深度自编码器进行图像检索[[58](#bib.bib58)]。堆叠去噪自编码器用于训练多个深度神经网络以进行检索任务[[60](#bib.bib60)]。冯等人（2014）利用了对应自编码器（Corr-AE）进行跨模态检索[[125](#bib.bib125)]。使用二进制自编码器通过从二进制代码函数中重建图像来学习二进制代码以实现快速图像检索[[127](#bib.bib127)]。近年来，自编码器在图像检索中的使用取得了巨大进展，例如使用变分贝叶斯网络的深度变分二进制（DVB）[[86](#bib.bib86)]；三元组上的自编码器[[123](#bib.bib123)]；以及放松二进制自编码器（RBA）[[152](#bib.bib152)]在2019年进行了研究。在最近的工作中，双潜在瓶颈被应用于自编码器[[87](#bib.bib87)]。它包括二进制潜变量和连续潜变量。潜变量瓶颈协同交换关键信息，二进制代码瓶颈使用基于代码的图来捕捉数据的内在结构。
- en: '![Refer to caption](img/6e2a15e08c101a88ae6d2911ef796123.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6e2a15e08c101a88ae6d2911ef796123.png)'
- en: 'Figure 7: (a) Siamese network computes the similarity between image pairs.
    (b) Triplet network minimizes the distance between the anchor and positive and
    maximizes the distance between the anchor and negative in feature space.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7： (a) 孪生网络计算图像对之间的相似性。 (b) 三元组网络在特征空间中最小化锚点与正样本之间的距离，同时最大化锚点与负样本之间的距离。
- en: V-C Siamese and Triplet Networks for Image Retrieval
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 图像检索中的孪生网络和三元组网络
- en: V-C1 Siamese Network
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C1 孪生网络
- en: The siamese is type of neural network that exploits the distance between features
    of image pairs as depicted in Fig. [7](#S5.F7 "Figure 7 ‣ V-B Autoencoder Networks
    based Image Retrieval ‣ V Network Types For Image Retrieval ‣ A Decade Survey
    of Content Based Image Retrieval using Deep Learning")(a). The siamese network
    based learnt features have shown very promising performance for fine-grained image
    retrieval [[62](#bib.bib62)]. A pair of similar or dissimilar images is jointly
    processed by Liu et al. [[97](#bib.bib97)] to produce 1 or -1 output by CNN to
    learn the feature for image retrieval. Ong et al. (2017) have used the fisher
    vector computed on top of the CNN feature in autoencoder network to generate the
    discriminating feature descriptor for image retrieval [[136](#bib.bib136)]. The
    siamese network is also used to develop the light weight models for efficient
    image retrieval [[144](#bib.bib144)], [[124](#bib.bib124)]. A pairwise similarity-preserving
    quantization loss is employed in [[162](#bib.bib162)]. The siamese network is
    used with the stacked adversarial network in [[163](#bib.bib163)]. The siamese
    network is also used for patch based image matching [[128](#bib.bib128)], [[129](#bib.bib129)].
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese 网络是一种利用图像对特征之间距离的神经网络，如图[7](#S5.F7 "Figure 7 ‣ V-B Autoencoder Networks
    based Image Retrieval ‣ V Network Types For Image Retrieval ‣ A Decade Survey
    of Content Based Image Retrieval using Deep Learning")(a)所示。基于Siamese 网络学习的特征在细粒度图像检索中表现出了非常有前景的性能[[62](#bib.bib62)]。刘等人[[97](#bib.bib97)]联合处理一对相似或不相似的图像，通过CNN输出1或-1以学习图像检索的特征。Ong等人（2017）在自编码器网络中使用CNN特征上计算的Fisher向量来生成用于图像检索的判别特征描述符[[136](#bib.bib136)]。Siamese
    网络还被用来开发轻量级模型以实现高效的图像检索[[144](#bib.bib144)], [[124](#bib.bib124)]。在[[162](#bib.bib162)]中采用了成对相似度保持的量化损失。Siamese
    网络与堆叠对抗网络在[[163](#bib.bib163)]中结合使用。Siamese 网络还用于基于补丁的图像匹配[[128](#bib.bib128)],
    [[129](#bib.bib129)]。
- en: V-C2 Triplet Network
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C2 三元组网络
- en: A triplet network is a variation of siamese network which utilizes a triplet
    of images, including an anchor, a positive and a negative image as shown in Fig.
    [7](#S5.F7 "Figure 7 ‣ V-B Autoencoder Networks based Image Retrieval ‣ V Network
    Types For Image Retrieval ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning")(b). The triplet network minimizes the distance between the features
    of anchor and positive image and maximizes the distance between the features of
    anchor and negative image, simultaneously. In 2015, a triplet ranking loss is
    utilized on top of the shared CNN features to learn the network for computation
    of binary descriptors for image retrieval [[63](#bib.bib63)]. An adjacency consistency
    based regularization term is introduced in the triplet network to enforce the
    discriminative ability of the CNN feature description [[64](#bib.bib64)]. Zhuang
    et al. (2016) [[130](#bib.bib130)] have used triplet to learn the hash code by
    employing the relation weights matrix and graph cuts optimization. The triplet
    ranking loss, orthogonality constraint and softmax loss are minimized jointly
    in [[131](#bib.bib131)]. Triplet based siamese networks are als used for image
    rerieval [[132](#bib.bib132)], [[72](#bib.bib72)]. Triplet quantization based
    objective function minimizes the information loss [[135](#bib.bib135)], [[179](#bib.bib179)].
    The triplet based feature learning has been also exploited for sketch based image
    retrieval [[153](#bib.bib153)]. Triplets are also exploited for supervised hashing
    [[99](#bib.bib99)] and unsupervised hashing [[117](#bib.bib117)], [[123](#bib.bib123)]
    for image retrieval.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组网络是Siamese 网络的一种变体，它利用一个包含锚点图像、正样本图像和负样本图像的三元组，如图[7](#S5.F7 "Figure 7 ‣ V-B
    Autoencoder Networks based Image Retrieval ‣ V Network Types For Image Retrieval
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning")(b)所示。三元组网络最小化锚点和正样本图像特征之间的距离，同时最大化锚点和负样本图像特征之间的距离。2015年，在共享CNN特征上利用了三元组排序损失，以学习网络用于计算图像检索的二进制描述符[[63](#bib.bib63)]。在三元组网络中引入了基于邻接一致性的正则化项，以增强CNN特征描述的判别能力[[64](#bib.bib64)]。Zhuang等人（2016）[[130](#bib.bib130)]通过采用关系权重矩阵和图割优化来使用三元组学习哈希码。在[[131](#bib.bib131)]中，三元组排序损失、正交性约束和Softmax损失被联合最小化。基于三元组的Siamese
    网络也用于图像检索[[132](#bib.bib132)], [[72](#bib.bib72)]。基于三元组的量化目标函数最小化信息损失[[135](#bib.bib135)],
    [[179](#bib.bib179)]。三元组特征学习也被用于基于草图的图像检索[[153](#bib.bib153)]。三元组还被用于监督哈希[[99](#bib.bib99)]和无监督哈希[[117](#bib.bib117)],
    [[123](#bib.bib123)]的图像检索。
- en: V-D Generative Adversarial Networks based Retrieval
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 生成对抗网络基础的检索
- en: The generative adversarial network (GAN) uses two networks, i.e., generator
    and discriminator. The generator network generates the new samples in the training
    set from the random vector. Whereas, the discriminator network distinguishes between
    generated image and original image. In 2018, Song et al. have introduced a binary
    generative adversarial network (BGAN) for generating the representational binary
    codes for image retrieval [[121](#bib.bib121)]. At the same time, a regularized
    GAN is used to introduce the BinGAN model [[150](#bib.bib150)] to learn the compact
    binary patterns. The BinGAN uses two regularizers, including a distance matching
    regularizer and a binarization representation entropy (BRE) regularizer. In 2018,
    the generative networks are also utilized in [[120](#bib.bib120)] to develop HashGAN
    in an unsupervised manner to generate the hash code for image retrieval. At the
    same time another HashGAN is developed by employing the paired conditional Wasserstein
    GAN for image retrieval [[149](#bib.bib149)]. GAN has been also used for cross-modal
    retrieval [[143](#bib.bib143)], [[145](#bib.bib145)], [[146](#bib.bib146)], [[160](#bib.bib160)],
    semi-supervised hashing [[147](#bib.bib147)], [[161](#bib.bib161)], sketch based
    image retrieval [[148](#bib.bib148)], [[159](#bib.bib159)], [[163](#bib.bib163)]
    and unsupervised adversarial hashing [[122](#bib.bib122)]. In 2020, binary generative
    adversarial networks based unified BGAN+ framework [[166](#bib.bib166)] is developed
    for image retrieval.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）使用两个网络，即生成器和判别器。生成器网络从随机向量中生成训练集中的新样本。而判别器网络则区分生成的图像和原始图像。2018年，Song等人介绍了一种用于图像检索的二进制生成对抗网络（BGAN），用于生成表征二进制码[[121](#bib.bib121)]。同时，使用正则化GAN引入了BinGAN模型[[150](#bib.bib150)]以学习紧凑的二进制模式。BinGAN使用了两个正则化器，包括距离匹配正则化器和二值化表示熵（BRE）正则化器。2018年，生成网络也被应用于[[120](#bib.bib120)]以无监督方式开发HashGAN，用于生成图像检索的哈希码。同时，另一种HashGAN通过使用配对条件Wasserstein
    GAN来进行图像检索[[149](#bib.bib149)]。GAN还被用于跨模态检索[[143](#bib.bib143)]，[[145](#bib.bib145)]，[[146](#bib.bib146)]，[[160](#bib.bib160)]，半监督哈希[[147](#bib.bib147)]，[[161](#bib.bib161)]，基于草图的图像检索[[148](#bib.bib148)]，[[159](#bib.bib159)]，[[163](#bib.bib163)]和无监督对抗哈希[[122](#bib.bib122)]。2020年，基于二进制生成对抗网络的统一BGAN+框架[[166](#bib.bib166)]被开发用于图像检索。
- en: V-E Attention Networks for Image Retrieval
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E注意力网络用于图像检索
- en: The attention has been observed as a very effective way of modelling the saliency
    information into the feature space to avoid the effect of background. In 2017,
    Noh et al. have used the attention-based keypoints to select the important deep
    local features [[55](#bib.bib55)]. Yang et al. (2017) have introduced a two-stream
    attentive CNNs by fusing a Main and an Auxiliary CNN (MAC) for image retrieval
    [[138](#bib.bib138)]. The main CNN focuses over the discriminative visual features
    for semantic information, whereas the auxiliary CNN focuses over the part of features
    for attentive information. Similarily, two sub-networks are employed in [[155](#bib.bib155)]
    for spatial attention and global features, respectively. Recently, Ng et al. [[164](#bib.bib164)]
    have computed the second-order similarity (SOS) loss over the attention based
    selected regions of the input image for image retrieval. The attention based models
    are developed for cross-modal retrieval [[145](#bib.bib145)] and fine-grained
    sketch-based image retrieval [[137](#bib.bib137)]. The gradient attention network
    based deep hashing [[154](#bib.bib154)] enforces the CNN binary features of a
    pair to minimize the distances between them, irrespective of their signs or directions.
    In order to localize the important image region for the feature description, an
    attentional heterogeneous bilinear network is employed in [[180](#bib.bib180)]
    for fashion image retrieval.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制已被观察到是一种非常有效的方法，它将显著性信息建模到特征空间中，以避免背景的影响。2017年，Noh等人使用了基于注意力的关键点来选择重要的深度局部特征[[55](#bib.bib55)]。Yang等人（2017年）引入了一种双流注意力卷积神经网络（CNN），通过融合主CNN和辅助CNN（MAC）来进行图像检索[[138](#bib.bib138)]。主CNN侧重于语义信息的区分视觉特征，而辅助CNN则关注于注意力信息的部分特征。同样，[[155](#bib.bib155)]中也使用了两个子网络，分别用于空间注意力和全局特征。最近，Ng等人[[164](#bib.bib164)]计算了基于注意力选择区域的输入图像的二阶相似性（SOS）损失用于图像检索。基于注意力的模型已经开发用于跨模态检索[[145](#bib.bib145)]和细粒度草图图像检索[[137](#bib.bib137)]。基于梯度注意力网络的深度哈希[[154](#bib.bib154)]强制一对CNN二进制特征最小化它们之间的距离，而不管它们的符号或方向。为了定位用于特征描述的重要图像区域，[[180](#bib.bib180)]中使用了一个注意力异质双线性网络用于时尚图像检索。
- en: \startchronology
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: \startchronology
- en: '[startyear=1,stopyear=49, startdate=false, color=blue!100, stopdate=false,
    arrow=true, arrowwidth=0.8cm, arrowheight=0.5cm] \setupchronoeventtextstyle=,datestyle=
    \chronoevent[markdepth=-15pt, year=false]12011 \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]2Deep
    Autoencoder [[58](#bib.bib58)] \chronoevent[markdepth=-15pt, year=false]32015
    \chronoevent[markdepth=-75pt,year=false,textwidth=1.5cm]4CNN [[110](#bib.bib110)]
    \chronoevent[markdepth=-50pt,year=false,textwidth=1.5cm]4DNN [[110](#bib.bib110)]
    \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]6Autoencoder [[127](#bib.bib127)]
    \chronoevent[markdepth=-55pt,year=false,textwidth=1cm]8Binary Code [[181](#bib.bib181)],
    [[96](#bib.bib96)] \chronoevent[markdepth=10pt,year=false,textwidth=1cm]4CNN Feature
    Aggregation [[182](#bib.bib182)] \chronoevent[markdepth=65pt,year=false,textwidth=2cm]6Multi
    CNN Fusion [[183](#bib.bib183)] \chronoevent[markdepth=42pt,year=false,textwidth=1.3cm]8CNN-VLAD
    [[184](#bib.bib184)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]8CNN-BoF
    [[185](#bib.bib185)] \chronoevent[markdepth=-15pt, year=false]92016 \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]10Binary
    DNN [[186](#bib.bib186)] \chronoevent[markdepth=-50pt,year=false,textwidth=1.5cm]12Binary
    Code [[97](#bib.bib97)], [[112](#bib.bib112)], [[178](#bib.bib178)], [[97](#bib.bib97)],
    [[112](#bib.bib112)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]14Siamese
    + Triplet [[132](#bib.bib132)] \chronoevent[markdepth=50pt,year=false,textwidth=1cm]11CNN
    Feature Aggregation [[69](#bib.bib69)] \chronoevent[markdepth=10pt,year=false,textwidth=1.3cm]13Bag
    of CNN Feature [[187](#bib.bib187)] \chronoevent[markdepth=-15pt, year=false]162017
    \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]17Siamese Network [[144](#bib.bib144)]
    \chronoevent[markdepth=-50pt,year=false,textwidth=1cm]19Binary Hashing [[101](#bib.bib101)],
    [[101](#bib.bib101)] \chronoevent[markdepth=-65pt,year=false,textwidth=1.2cm]22Real-valued
    Descriptor [[188](#bib.bib188)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]22Siamese
    Network [[136](#bib.bib136)], [[72](#bib.bib72)] \chronoevent[markdepth=60pt,year=false,textwidth=1.5cm]17Selective
    Conv Descriptor Aggregation (SCDA) [[189](#bib.bib189)] \chronoevent[markdepth=10pt,year=false,textwidth=1.5cm]17CNN
    High-Low Layer Fusion [[190](#bib.bib190)] \chronoevent[markdepth=60pt,year=false,textwidth=1.5cm]23Joint
    feature aggregation and hashing [[191](#bib.bib191)] \chronoevent[markdepth=10pt,year=false,textwidth=1.5cm]23Attention
    based aggregation [[138](#bib.bib138)] \chronoevent[markdepth=10pt,year=false,textwidth=0.7cm]20CNN
    + Hand-crafted Fusion [[192](#bib.bib192)] \chronoevent[markdepth=-15pt, year=false]242018
    \chronoevent[markdepth=-70pt,year=false,textwidth=1.5cm]26Binary Quantization
    [[75](#bib.bib75)], [[76](#bib.bib76)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]26Binary
    Hashing [[113](#bib.bib113)], [[120](#bib.bib120)], [[113](#bib.bib113)] \chronoevent[markdepth=-35pt,year=false,textwidth=1cm]29GAN
    [[121](#bib.bib121)], [[150](#bib.bib150)], [[120](#bib.bib120)] \chronoevent[markdepth=-70pt,year=false,textwidth=1.5cm]31Part-based
    CNN [[119](#bib.bib119)] \chronoevent[markdepth=60pt,year=false,textwidth=1.2cm]27Part-based
    weighting [[119](#bib.bib119)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]27Visual
    words learning [[193](#bib.bib193)] \chronoevent[markdepth=10pt,year=false,textwidth=1cm]30Multi-layer
    fusion of CNN features [[151](#bib.bib151)] \chronoevent[markdepth=-15pt, year=false]322019
    \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]33Triplet Network [[123](#bib.bib123)]
    \chronoevent[markdepth=-65pt,year=false,textwidth=1.5cm]35Deep Variational Network
    [[86](#bib.bib86)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.3cm]38Binary
    Features [[194](#bib.bib194)], [[152](#bib.bib152)], [[195](#bib.bib195)], [[196](#bib.bib196)]
    \chronoevent[markdepth=-60pt,year=false,textwidth=1.2cm]41Real-valued Descriptors
    [[197](#bib.bib197)], [[198](#bib.bib198)] \chronoevent[markdepth=60pt,year=false,textwidth=1.5cm]33Joint
    feature aggregation and hashing [[152](#bib.bib152)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]35Selective
    CNN feature aggregation [[195](#bib.bib195)] \chronoevent[markdepth=50pt,year=false,textwidth=1.2cm]37Attention
    based feature aggregation [[156](#bib.bib156)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]39Multi-network
    fusion [[155](#bib.bib155)] \chronoevent[markdepth=10pt,year=false,textwidth=1.25cm]42Co-weighting
    based CNN feature fusion [[199](#bib.bib199)] \chronoevent[markdepth=50pt,year=false,textwidth=1.2cm]40CNN
    + Hand-crafted fusion [[106](#bib.bib106)] \chronoevent[markdepth=-15pt, year=false]432019
    \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]44GAN [[166](#bib.bib166)]
    \chronoevent[markdepth=-38pt,year=false,textwidth=1.3cm]46Pairwise Correlation
    Discrete Hashing (PCDH) [[200](#bib.bib200)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]49Double
    bottleneck hashing [[87](#bib.bib87)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]46Co-occurrence
    + feature map fusion [[88](#bib.bib88)] \stopchronology'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[startyear=1,stopyear=49, startdate=false, color=blue!100, stopdate=false,
    arrow=true, arrowwidth=0.8cm, arrowheight=0.5cm] \setupchronoeventtextstyle=,datestyle=
    \chronoevent[markdepth=-15pt, year=false]12011 \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]2深度自编码器
    [[58](#bib.bib58)] \chronoevent[markdepth=-15pt, year=false]32015 \chronoevent[markdepth=-75pt,year=false,textwidth=1.5cm]4卷积神经网络
    [[110](#bib.bib110)] \chronoevent[markdepth=-50pt,year=false,textwidth=1.5cm]4深度神经网络
    [[110](#bib.bib110)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]6自编码器
    [[127](#bib.bib127)] \chronoevent[markdepth=-55pt,year=false,textwidth=1cm]8二进制编码
    [[181](#bib.bib181)], [[96](#bib.bib96)] \chronoevent[markdepth=10pt,year=false,textwidth=1cm]4卷积神经网络特征聚合
    [[182](#bib.bib182)] \chronoevent[markdepth=65pt,year=false,textwidth=2cm]6多卷积神经网络融合
    [[183](#bib.bib183)] \chronoevent[markdepth=42pt,year=false,textwidth=1.3cm]8卷积神经网络-VLAD
    [[184](#bib.bib184)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]8卷积神经网络-BoF
    [[185](#bib.bib185)] \chronoevent[markdepth=-15pt, year=false]92016 \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]10二进制深度神经网络
    [[186](#bib.bib186)] \chronoevent[markdepth=-50pt,year=false,textwidth=1.5cm]12二进制编码
    [[97](#bib.bib97)], [[112](#bib.bib112)], [[178](#bib.bib178)], [[97](#bib.bib97)],
    [[112](#bib.bib112)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]14Siamese
    + Triplet [[132](#bib.bib132)] \chronoevent[markdepth=50pt,year=false,textwidth=1cm]11卷积神经网络特征聚合
    [[69](#bib.bib69)] \chronoevent[markdepth=10pt,year=false,textwidth=1.3cm]13卷积神经网络特征包
    [[187](#bib.bib187)] \chronoevent[markdepth=-15pt, year=false]162017 \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]17Siamese网络
    [[144](#bib.bib144)] \chronoevent[markdepth=-50pt,year=false,textwidth=1cm]19二进制哈希
    [[101](#bib.bib101)], [[101](#bib.bib101)] \chronoevent[markdepth=-65pt,year=false,textwidth=1.2cm]22实值描述符
    [[188](#bib.bib188)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]22Siamese网络
    [[136](#bib.bib136)], [[72](#bib.bib72)] \chronoevent[markdepth=60pt,year=false,textwidth=1.5cm]17选择性卷积描述符聚合
    (SCDA) [[189](#bib.bib189)] \chronoevent[markdepth=10pt,year=false,textwidth=1.5cm]17卷积神经网络高低层融合
    [[190](#bib.bib190)] \chronoevent[markdepth=60pt,year=false,textwidth=1.5cm]23联合特征聚合与哈希
    [[191](#bib.bib191)] \chronoevent[markdepth=10pt,year=false,textwidth=1.5cm]23基于注意力的聚合
    [[138](#bib.bib138)] \chronoevent[markdepth=10pt,year=false,textwidth=0.7cm]20卷积神经网络
    + 手工设计融合 [[192](#bib.bib192)] \chronoevent[markdepth=-15pt, year=false]242018
    \chronoevent[markdepth=-70pt,year=false,textwidth=1.5cm]26二进制量化 [[75](#bib.bib75)],
    [[76](#bib.bib76)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]26二进制哈希
    [[113](#bib.bib113)], [[120](#bib.bib120)], [[113](#bib.bib113)] \chronoevent[markdepth=-35pt,year=false,textwidth=1cm]29生成对抗网络
    [[121](#bib.bib121)], [[150](#bib.bib150)], [[120](#bib.bib120)] \chronoevent[markdepth=-70pt,year=false,textwidth=1.5cm]31基于部分的卷积神经网络
    [[119](#bib.bib119)] \chronoevent[markdepth=60pt,year=false,textwidth=1.2cm]27基于部分的加权
    [[119](#bib.bib119)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]27视觉词学习
    [[193](#bib.bib193)] \chronoevent[markdepth=10pt,year=false,textwidth=1cm]30卷积神经网络特征的多层融合
    [[151](#bib.bib151)] \chronoevent[markdepth=-15pt, year=false]322019 \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]33Triplet网络
    [[123](#bib.bib123)] \chronoevent[markdepth=-65pt,year=false,textwidth=1.5cm]35深度变分网络
    [[86](#bib.bib86)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.3cm]38二进制特征
    [[194](#bib.bib194)], [[152](#bib.bib152)], [[195](#bib.bib195)], [[196](#bib.bib196)]
    \chronoevent[markdepth=-60pt,year=false,textwidth=1.2cm]41实值描述符 [[197](#bib.bib197)],
    [[198](#bib.bib198)] \chronoevent[markdepth=60pt,year=false,textwidth=1.5cm]33联合特征聚合与哈希
    [[152](#bib.bib152)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]35选择性卷积神经网络特征聚合
    [[195](#bib.bib195)] \chronoevent[markdepth=50pt,year=false,textwidth=1.2cm]37基于注意力的特征聚合
    [[156](#bib.bib156)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]39多网络融合
    [[155](#bib.bib155)] \chronoevent[markdepth=10pt,year=false,textwidth=1.25cm]42基于共同加权的卷积神经网络特征融合
    [[199](#bib.bib199)] \chronoevent[markdepth=50pt,year=false,textwidth=1.2cm]40卷积神经网络
    + 手工设计融合 [[106](#bib.bib106)] \chronoevent[markdepth=-15pt, year=false]432019
    \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]44生成对抗网络 [[166](#bib.bib166)]
    \chronoevent[markdepth=-38pt,year=false,textwidth=1.3cm]46成对相关离散哈希 (PCDH) [[200](#bib.bib200)]
    \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]49双瓶颈哈希 [[87](#bib.bib87)]
    \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]46共现 + 特征图融合 [[88](#bib.bib88)]
    \stopchronology'
- en: 'Figure 8: A chronological view of deep learning based image retrieval methods
    depicting the different type of descriptors. The binary and real-valued feature
    vector based models are presented in Red and Blue colors, respectively. The feature
    aggregation based models are presented in Cyan color.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：展示不同类型描述符的深度学习基础图像检索方法的时间顺序视图。基于二进制和实值特征向量的模型分别用红色和蓝色表示。基于特征聚合的模型用青色表示。
- en: V-F Recurrent Neural Networks for Image Retrieval
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-F 递归神经网络用于图像检索
- en: In 2018, Lu et al. have utilized the recurrent neural network (RNN) concept
    to perform a hierarchical recurrent neural hashing (HRNH) to produce the effective
    hash codes for image retrieval [[140](#bib.bib140)]. In 2017, Shen et al. have
    used the region-based convolutional networks with long short-term memory (LSTM)
    modules for textual-visual cross retrieval [[139](#bib.bib139)]. Bai et al. (2019)
    have also employed the LSTM based recurrent deep network in the triplet hashing
    framework to naturally inherit the useful information for image retrieval [[83](#bib.bib83)].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，陆等人利用了递归神经网络（RNN）概念来执行层次递归神经哈希（HRNH），以生成有效的图像检索哈希码[[140](#bib.bib140)]。2017年，沈等人使用了基于区域的卷积网络与长短期记忆（LSTM）模块进行文本视觉跨检索[[139](#bib.bib139)]。白等人（2019）还在三元组哈希框架中应用了基于LSTM的递归深度网络，以自然地继承用于图像检索的有用信息[[83](#bib.bib83)]。
- en: V-G Reinforcement Learning Networks based Retrieval
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-G 强化学习网络基础的检索
- en: In 2018, Yuan et al. have exploited the reinforcement learning for image retrieval
    [[77](#bib.bib77)]. They have used a relaxation free method through policy gradient
    to generate the hash codes for image retrieval. The similarity preservation via
    the generated binary codes is used as the reward function. In 2020, Yang et al.
    [[167](#bib.bib167)] have utilized the deep reinforcement learning to perform
    the de-redundancy in hash bits to get rid of redundant and/or harmful bits, which
    reduces the ambiguity in the similarity computation for image retrieval.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，袁等人利用了强化学习进行图像检索[[77](#bib.bib77)]。他们通过策略梯度采用了一种无松弛的方法来生成图像检索的哈希码。生成的二进制码通过相似性保持作为奖励函数。在2020年，杨等人[[167](#bib.bib167)]利用深度强化学习来执行哈希位的去冗余，以去除冗余和/或有害的位，从而减少图像检索中相似性计算的模糊性。
- en: V-H Summary
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-H 总结
- en: 'The summary of the different network driven deep learning based image retrieval
    approaches is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 不同网络驱动的深度学习基础图像检索方法的总结如下：
- en: •
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The convolutional neural network features are exploited for the hash code and
    descriptor learning by employing the various constraints like classification error,
    quantization error, independent bits, etc.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过采用各种约束（如分类误差、量化误差、独立位等），卷积神经网络特征被用于哈希码和描述符的学习。
- en: •
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In order to make the features more representative of the image, the autoencoder
    networks are used which enforces the learning based on the reconstruction loss.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了使特征更具代表性，使用了自编码器网络，这些网络通过重建损失来强制学习。
- en: •
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The discriminative power of descriptive hash code is enhanced by exploiting
    the siamese and triplet networks. Different constraints are used on the hash code
    to make it discriminative and compact.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过利用孪生网络和三元组网络来增强描述性哈希码的判别能力。对哈希码施加不同的约束以使其具有判别性和紧凑性。
- en: •
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The generative adversarial network based approaches have been highly utilized
    to improve the discriminative ability and robustness of the learnt features by
    encoder network guided through the discriminator network.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于生成对抗网络的方法被广泛用于提高通过编码器网络学习的特征的判别能力和鲁棒性，编码器网络通过鉴别网络进行指导。
- en: •
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The automatic important feature selection is performed using attention module
    to control the redundancy in the feature space. The recurrent neural network and
    reinforcement learning network have been also shown very effective for the image
    retrieval.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动重要特征选择使用注意力模块来控制特征空间中的冗余。递归神经网络和强化学习网络在图像检索中也显示出非常有效。
- en: VI Type of Descriptors for Image Retrieval
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 图像检索的描述符类型
- en: This section covers the binary hash codes for efficient image retrieval, real-valued
    descriptors and feature aggregation for discriminative image retrieval as depicted
    in Fig. [8](#S5.F8 "Figure 8 ‣ V-E Attention Networks for Image Retrieval ‣ V
    Network Types For Image Retrieval ‣ A Decade Survey of Content Based Image Retrieval
    using Deep Learning").
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了高效图像检索的二进制哈希码、实值描述符以及用于判别性图像检索的特征聚合，如图[8](#S5.F8 "Figure 8 ‣ V-E Attention
    Networks for Image Retrieval ‣ V Network Types For Image Retrieval ‣ A Decade
    Survey of Content Based Image Retrieval using Deep Learning")所示。
- en: VI-A Binary Descriptors
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 二进制描述符
- en: Different types of networks are used to learn the binary description such as
    deep neural networks [[110](#bib.bib110)], convolutional neural networks [[51](#bib.bib51)],
    autoencoder networks [[127](#bib.bib127)], siamese networks [[144](#bib.bib144)],
    triplet networks [[123](#bib.bib123)], generative adversarial networks, [[166](#bib.bib166)],
    and variational networks [[86](#bib.bib86)]. In 2015, Liong et al. [[110](#bib.bib110)]
    have introduced a supervised deep hashing (SDH). The SDH method uses the quantization
    loss, balanced bits and independent bits constraints. Binary hash code is also
    learnt through a latent layer in a supervised manner in [[51](#bib.bib51)]. A
    binary autoencoder [[58](#bib.bib58)], [[127](#bib.bib127)] and a siamese network
    [[144](#bib.bib144)] are used to learn the binary features for efficient image
    retrieval. A binary deep neural network (BDNN) is proposed by converting a hidden
    layer output to binary code [[186](#bib.bib186)], [[194](#bib.bib194)]. The binary
    code is jointly learnt with feature aggregation in [[152](#bib.bib152)]. A masking
    technique over the convolutional features is used to generate the binary description
    for image retrieval [[195](#bib.bib195)]. A ranking optimization discrete hashing
    (RODH) approach is used in [[196](#bib.bib196)] by generating the discrete hash
    codes (+1 or -1) by employing the ranking information. A cauchy quantization loss
    is used in [[75](#bib.bib75)] to improve the discriminative power of binary descriptors.
    An iterative quantization approach is used to convert the features into binary
    codes to avoid the quantization loss [[76](#bib.bib76)]. Binary hash code is also
    used for clothing image retrieval [[181](#bib.bib181)]. The binary description
    is learnt through the supervised [[96](#bib.bib96)], [[97](#bib.bib97)], [[101](#bib.bib101)],
    unsupervised [[112](#bib.bib112)], [[113](#bib.bib113)], [[120](#bib.bib120)]
    and self-supervised [[178](#bib.bib178)] deep learning techniques. Among the generative
    approaches, a binary generative adversarial network (BGAN) is used to learn the
    binary code [[121](#bib.bib121)]. At the same time a regularized GAN is used by
    maximizing the entropy of binarized layer for image retrieval [[150](#bib.bib150)].
    The GAN is trained in unsupervised mode [[120](#bib.bib120)] to learn the binary
    codes for image retrieval. In 2020, the binary GAN [[166](#bib.bib166)] is used
    for image retrieval and compression jointly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的网络用于学习二进制描述，如深度神经网络[[110](#bib.bib110)]、卷积神经网络[[51](#bib.bib51)]、自编码器网络[[127](#bib.bib127)]、孪生网络[[144](#bib.bib144)]、三元组网络[[123](#bib.bib123)]、生成对抗网络[[166](#bib.bib166)]和变分网络[[86](#bib.bib86)]。2015年，Liong等人[[110](#bib.bib110)]提出了一种监督深度哈希（SDH）。SDH方法使用量化损失、平衡位和独立位约束。二进制哈希码也通过监督方式在[[51](#bib.bib51)]中的潜层中学习。二进制自编码器[[58](#bib.bib58)]、[[127](#bib.bib127)]和孪生网络[[144](#bib.bib144)]用于学习高效图像检索的二进制特征。通过将隐藏层输出转换为二进制代码[[186](#bib.bib186)]、[[194](#bib.bib194)]，提出了二进制深度神经网络（BDNN）。二进制代码与特征聚合在[[152](#bib.bib152)]中共同学习。通过对卷积特征进行掩蔽技术，生成用于图像检索的二进制描述[[195](#bib.bib195)]。在[[196](#bib.bib196)]中使用了排名优化离散哈希（RODH）方法，通过利用排名信息生成离散哈希码（+1或-1）。在[[75](#bib.bib75)]中使用了Cauchy量化损失，以提高二进制描述符的判别能力。使用迭代量化方法将特征转换为二进制代码，以避免量化损失[[76](#bib.bib76)]。二进制哈希码也用于服装图像检索[[181](#bib.bib181)]。二进制描述通过监督[[96](#bib.bib96)]、[[97](#bib.bib97)]、[[101](#bib.bib101)]、无监督[[112](#bib.bib112)]、[[113](#bib.bib113)]、[[120](#bib.bib120)]和自监督[[178](#bib.bib178)]深度学习技术进行学习。在生成方法中，使用二进制生成对抗网络（BGAN）来学习二进制代码[[121](#bib.bib121)]。同时，通过最大化二值化层的熵来使用正则化GAN进行图像检索[[150](#bib.bib150)]。GAN以无监督模式[[120](#bib.bib120)]训练，以学习图像检索的二进制代码。在2020年，二进制GAN[[166](#bib.bib166)]被用于图像检索和压缩。
- en: VI-B Real-Valued Descriptors
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 实值描述符
- en: The binary hashing approaches have the obvious shortcomings. First, it is difficult
    to represent the fine-grained similarity using binary code. Second, the generation
    of similar binary codes is common even for different images. Thus, researchers
    have also used the real-valued features to represent the images for the retrieval.
    The siamese networks have been extensively used to learn the real-valued feature
    descriptor for image retrieval [[132](#bib.bib132)], [[136](#bib.bib136)], [[72](#bib.bib72)].
    In 2018, part-based CNN features are utilized to extract a non-binary hash code
    [[119](#bib.bib119)]. The real-valued descriptors generated using CNNs are used
    for medical image retrieval [[188](#bib.bib188)], [[197](#bib.bib197)] and cross-modal
    retrieval [[198](#bib.bib198)]. Chen et al. (2020) [[200](#bib.bib200)] have developed
    a pairwise correlation discrete hashing (PCDH) by exploiting the pairwise correlation
    of deep features for image retrieval. Shen et al. (2020) [[87](#bib.bib87)] have
    also used the real-valued descriptors with the help of double bottleneck hashing
    approach for image retrieval.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制哈希方法存在明显的不足。首先，使用二进制代码难以表示细粒度的相似性。其次，即使是不同的图像，生成相似的二进制代码也很常见。因此，研究人员还使用真实值特征来表示图像以进行检索。孪生网络被广泛用于学习用于图像检索的真实值特征描述符
    [[132](#bib.bib132)]，[[136](#bib.bib136)]，[[72](#bib.bib72)]。在2018年，基于部分的CNN特征被用来提取非二进制哈希码
    [[119](#bib.bib119)]。使用CNN生成的真实值描述符用于医学图像检索 [[188](#bib.bib188)]，[[197](#bib.bib197)]
    和跨模态检索 [[198](#bib.bib198)]。Chen等人（2020） [[200](#bib.bib200)] 通过利用深度特征的成对相关性开发了一种成对相关离散哈希（PCDH）方法用于图像检索。Shen等人（2020）
    [[87](#bib.bib87)] 也使用了真实值描述符，并通过双瓶颈哈希方法进行图像检索。
- en: VI-C Aggregation of Descriptors
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 描述符的聚合
- en: Several researchers have also tried to combine/fuse the feature at different
    stages of the network or multiple networks to generate the aggregation of descriptors
    for image retrieval [[69](#bib.bib69)], [[182](#bib.bib182)], [[185](#bib.bib185)].
    Different strategies have been excercised for aggregation of features, such as
    vector locally aggregated descriptors (VLAD) [[184](#bib.bib184)] on the features
    extracted from different layers; bag of local convolutional features [[187](#bib.bib187)];
    selective convolutional descriptor aggregation [[189](#bib.bib189)], [[195](#bib.bib195)];
    fusion of multi-layer features [[190](#bib.bib190)], [[151](#bib.bib151)]; part-based
    weighting aggregation [[119](#bib.bib119)]; joint training of feature aggregation
    and hashing [[191](#bib.bib191)]; learning of feature aggregation and hash function
    in a joint manner [[152](#bib.bib152)]; and co-weighting based CNN feature fusion
    [[199](#bib.bib199)]. The features from different CNNs are also integrated for
    image retrieval [[183](#bib.bib183)], [[138](#bib.bib138)], [[156](#bib.bib156)].
    One main sub-network and other attention-based sub-network are also fused at the
    last fully connected layer in [[155](#bib.bib155)]. The hand-designed features
    are fused with CNNs [[106](#bib.bib106)], [[192](#bib.bib192)]. Recently, Forcen
    et al. (2020) [[88](#bib.bib88)] have generated the image representation by combining
    a co-occurrence map with the feature map for image retrieval.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员还尝试在网络的不同阶段或多个网络之间进行特征的组合/融合，以生成用于图像检索的描述符聚合 [[69](#bib.bib69)]，[[182](#bib.bib182)]，[[185](#bib.bib185)]。特征聚合的不同策略包括对从不同层提取的特征使用局部聚合描述符（VLAD）
    [[184](#bib.bib184)]；局部卷积特征的袋 [[187](#bib.bib187)]；选择性卷积描述符聚合 [[189](#bib.bib189)]，[[195](#bib.bib195)]；多层特征的融合
    [[190](#bib.bib190)]，[[151](#bib.bib151)]；基于部分的加权聚合 [[119](#bib.bib119)]；特征聚合与哈希的联合训练
    [[191](#bib.bib191)]；特征聚合和哈希函数的联合学习 [[152](#bib.bib152)]；以及基于共同加权的CNN特征融合 [[199](#bib.bib199)]。来自不同CNN的特征也被整合用于图像检索
    [[183](#bib.bib183)]，[[138](#bib.bib138)]，[[156](#bib.bib156)]。在 [[155](#bib.bib155)]
    中，主要子网络和其他基于注意力的子网络在最后一个全连接层处也进行了融合。手工设计的特征与CNN [[106](#bib.bib106)]，[[192](#bib.bib192)]
    进行了融合。最近，Forcen等人（2020） [[88](#bib.bib88)] 通过将共现图与特征图结合来生成图像表示以进行图像检索。
- en: VI-D Summary
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 总结
- en: 'The followings are the summary of deep learning based approaches from the perspective
    of the type of feature descriptor:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从特征描述符类型的角度总结的深度学习方法：
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In order to facilitate the large-scale image retrieval, the compact and binary
    hash codes are generated using different networks. Different methods try to improve
    the discriminative ability, lower redundancy among bits, generalization of the
    binary hash code, etc. in different supervision modes.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了方便大规模图像检索，使用不同的网络生成紧凑且二进制的哈希码。不同的方法尝试在不同的监督模式下提高区分能力，降低位之间的冗余性，改善二进制哈希码的泛化能力等。
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The real-valued descriptors concentrate over the discriminative ability of the
    learnt features for image retrieval at the cost of increased computational complexity
    for feature matching. Such methods try to increase the robustness and reduce the
    dimensionality of the descriptors.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实值描述符专注于提高图像检索中学习到的特征的区分能力，但代价是增加了特征匹配的计算复杂度。这些方法试图提高鲁棒性并减少描述符的维度。
- en: •
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Feature aggregation approaches try to utilize the complementary information
    between the features of different networks, the features of different sub-network,
    and the features of different layers of same network to improve the image retrieval
    performance.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征聚合方法试图利用不同网络、不同子网络以及同一网络不同层之间的互补信息，以提高图像检索性能。
- en: VII Retrieval Type
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 检索类型
- en: Various retrieval types have been explored using deep learning approaches based
    on the nature of the problem and data as discussed in this section.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题和数据的性质，已经探索了各种基于深度学习的方法进行检索类型。
- en: VII-A Cross-modal Retrieval
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 跨模态检索
- en: The cross-modal retrieval refers to the image retrieval involving more than
    one modality by measuring the similarity between heterogeneous data objects. Feng
    et al. (2014) have introduced a correspondence autoencoder (Corr-AE) network for
    cross-modal retrieval [[125](#bib.bib125)]. In 2016 [[201](#bib.bib201)], a deep
    visual-semantic hashing (DVSH) network is developed for sentence and image based
    cross-modal retrieval by jointly learning the embeddings for images and sentences.
    Textual-visual deep binaries (TVDB) model represents the long descriptive sentences
    along with its corresponding informative images [[139](#bib.bib139)]. The CNN
    visual features have been also exploited for cross-modal retrieval, such as CNN
    off-the-shelf features for labelled annotation [[134](#bib.bib134)], CNN features
    with bi-directional hinge loss [[202](#bib.bib202)], and pairwise constraints
    based deep hashing network [[142](#bib.bib142)]. The adversarial neural network
    is also employed for cross-modal retrieval, such as adversarial cross-modal retrieval
    (ACMR) [[143](#bib.bib143)], self-supervised adversarial hashing (SSAH) [[146](#bib.bib146)],
    attention-aware deep adversarial hashing (ADAH) [[145](#bib.bib145)], adversary
    guided asymmetric hashing (AGAH) [[160](#bib.bib160)], deep multi-level semantic
    hashing (DMSH) [[198](#bib.bib198)], and teacher-student learning [[203](#bib.bib203)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 跨模态检索指的是通过测量异质数据对象之间的相似性来进行的图像检索。Feng等人（2014）引入了一种用于跨模态检索的对应自编码器（Corr-AE）网络[[125](#bib.bib125)]。在2016年[[201](#bib.bib201)]，开发了一种深度视觉-语义哈希（DVSH）网络，通过联合学习图像和句子的嵌入来实现基于句子和图像的跨模态检索。文本-视觉深度二进制（TVDB）模型表示长描述句子及其对应的有信息量的图像[[139](#bib.bib139)]。CNN视觉特征也被用于跨模态检索，如用于标注的CNN现成特征[[134](#bib.bib134)]，带有双向铰链损失的CNN特征[[202](#bib.bib202)]，以及基于成对约束的深度哈希网络[[142](#bib.bib142)]。对抗神经网络也被用于跨模态检索，例如对抗性跨模态检索（ACMR）[[143](#bib.bib143)]，自监督对抗哈希（SSAH）[[146](#bib.bib146)]，注意力感知深度对抗哈希（ADAH）[[145](#bib.bib145)]，对抗引导的非对称哈希（AGAH）[[160](#bib.bib160)]，深度多层语义哈希（DMSH）[[198](#bib.bib198)]，以及师生学习[[203](#bib.bib203)]。
- en: VII-B Sketch Based Image Retrieval
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 基于草图的图像检索
- en: Sketch based image retrieval (SBIR) is a special case of cross-modal retrieval
    where the query image is in the sketch domain the retrieval has to be performed
    in the image domain [[204](#bib.bib204)]. In 2017, a fine-grained SBIR (FG-SBIR)
    [[137](#bib.bib137)] is explored with the help of attention module and higher-order
    learnable energy function loss. Liu et al. (2017) [[205](#bib.bib205)] have introduced
    a semi-heterogeneous deep sketch hashing (DSH) model for SBIR by utilizing the
    representation of free-hand sketches. The sketches and natural photos are mapped
    in multiple layers in a deep CNN framework in [[151](#bib.bib151)] for SBIR. A
    zero-shot SBIR (ZS-SBIR) is proposed for retrieval of photos from unseen categories
    [[153](#bib.bib153)]. Wang et al. (2019) [[158](#bib.bib158)] have proposed a
    CNN based SBIR re-ranking approach to refine the retrieval results. The generative
    adversarial networks have been also exploited extensively for SBIR, such as generative
    domain-migration hashing (GDH) using cycle consistency loss [[148](#bib.bib148)],
    class sketch conditioned generative model [[159](#bib.bib159)], semantically aligned
    paired cycle-consistent generative model [[206](#bib.bib206)], and stacked adversarial
    network [[163](#bib.bib163)].
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基于草图的图像检索（SBIR）是跨模态检索的一个特殊情况，其中查询图像在草图领域，检索必须在图像领域进行 [[204](#bib.bib204)]。在2017年，探讨了一种细粒度SBIR（FG-SBIR）
    [[137](#bib.bib137)]，借助注意力模块和高阶可学习能量函数损失。刘等（2017年） [[205](#bib.bib205)] 通过利用手绘草图的表示，提出了一种半异质深度草图哈希（DSH）模型用于SBIR。在
    [[151](#bib.bib151)] 中，草图和自然照片在深度CNN框架中进行多层映射以进行SBIR。提出了一种零样本SBIR（ZS-SBIR）用于检索来自未见类别的照片
    [[153](#bib.bib153)]。王等（2019年） [[158](#bib.bib158)] 提出了一种基于CNN的SBIR重新排序方法以优化检索结果。生成对抗网络也被广泛用于SBIR，例如使用循环一致性损失的生成领域迁移哈希（GDH）
    [[148](#bib.bib148)]，基于类别草图的生成模型 [[159](#bib.bib159)]，语义对齐的成对循环一致生成模型 [[206](#bib.bib206)]，以及堆叠对抗网络
    [[163](#bib.bib163)]。
- en: VII-C Multi-label Image Retrieval
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 多标签图像检索
- en: Multi-label retrieval involves multiple categorical labels while generating
    the image representations for image retrieval. Several deep learning approaches
    have been investigated for multi-label image retrieval using different strategies,
    such as multilevel similarity information [[207](#bib.bib207)], multilevel semantic
    similarity preserving hashing [[208](#bib.bib208)], multi-label annotations [[146](#bib.bib146)],
    category-aware object based hashing [[209](#bib.bib209)], [[210](#bib.bib210)],
    and fine-grained features for multilevel similarity hashing [[211](#bib.bib211)].
    Readers may refer to the survey of multi-label image retrieval [[43](#bib.bib43)]
    published in 2020 for wider aspects and developments.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签检索涉及在生成图像检索的图像表示时使用多个分类标签。已经研究了几种深度学习方法用于多标签图像检索，采用不同策略，例如多层次相似性信息 [[207](#bib.bib207)]，多层次语义相似性保持哈希
    [[208](#bib.bib208)]，多标签注释 [[146](#bib.bib146)]，类别感知的基于对象的哈希 [[209](#bib.bib209)]，[[210](#bib.bib210)]，以及用于多层次相似性哈希的细粒度特征
    [[211](#bib.bib211)]。读者可以参考2020年发布的多标签图像检索综述 [[43](#bib.bib43)]，以获取更广泛的视角和发展情况。
- en: VII-D Instance Retrieval
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D 实例检索
- en: In 2015, Razavian et al. have developed a baseline for deep CNN based visual
    instance retrieval [[212](#bib.bib212)]. An instance-aware image representations
    for multi-label image data by modeling the features of one category in a group
    is proposed in [[209](#bib.bib209)]. Other approaches for image instance retrieval
    includes bags of local convolutional features [[187](#bib.bib187)], learning global
    representations [[65](#bib.bib65)], and group invariant deep representation [[213](#bib.bib213)].
    In 2020, Chen et al. have proposed a deep multiple-instance ranking based hashing
    (DMIRH) model for multi-label image retrieval by employing the category-aware
    bag of feature [[210](#bib.bib210)]. More details about image instance retrieval
    can be found in the survey compiled in [[42](#bib.bib42)].
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年，Razavian等人开发了一种基于深度CNN的视觉实例检索基线 [[212](#bib.bib212)]。在 [[209](#bib.bib209)]
    中提出了一种通过对一个类别的特征建模来实现多标签图像数据的实例感知图像表示的方法。图像实例检索的其他方法包括局部卷积特征的袋子 [[187](#bib.bib187)]，学习全局表示
    [[65](#bib.bib65)]，以及群体不变的深度表示 [[213](#bib.bib213)]。在2020年，陈等人提出了一种深度多实例排名基础哈希（DMIRH）模型用于多标签图像检索，采用了类别感知的特征袋
    [[210](#bib.bib210)]。有关图像实例检索的更多细节可以在 [[42](#bib.bib42)] 中找到。
- en: VII-E Object Retrieval
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-E 对象检索
- en: The object retrieval aims to perform the retrieval based on the features derived
    from the specific objects in the image. In 2014, Sun et al. have extracted the
    CNN features from the region of interest detected through object detection technique
    for object based retrieval [[126](#bib.bib126)]. Several deep learning models
    have been investigated for object retrieval, such as integral image driven max-pooling
    on CNN activations [[214](#bib.bib214)], pooling of the relevant features based
    on the region proposal network [[65](#bib.bib65)], replicator equation based simultaneous
    selection and weighting of the primitive deep CNN features [[215](#bib.bib215)],
    co-weighting based aggregation of the semantic CNN features [[199](#bib.bib199)],
    and consideration of spatial and channel contribution to improve the region detection
    [[216](#bib.bib216)]. Gao et al. (2020) [[165](#bib.bib165)] have performed the
    3D object retrieval with the help of a multi-view discrimination and pairwise
    CNN (MDPCNN) network.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检索旨在基于图像中特定对象的特征进行检索。在2014年，Sun等人通过对象检测技术提取了感兴趣区域的CNN特征用于基于对象的检索[[126](#bib.bib126)]。已经研究了多种深度学习模型用于对象检索，例如基于CNN激活的积分图驱动最大池化[[214](#bib.bib214)]，基于区域提议网络的相关特征池化[[65](#bib.bib65)]，基于复制方程的原始深度CNN特征的同时选择和加权[[215](#bib.bib215)]，基于共加权的语义CNN特征聚合[[199](#bib.bib199)]，以及考虑空间和通道贡献以提高区域检测[[216](#bib.bib216)]。Gao等人（2020年）[[165](#bib.bib165)]通过多视图判别和成对CNN（MDPCNN）网络进行了3D对象检索。
- en: VII-F Semantic Retrieval
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-F 语义检索
- en: In 2016, Yao et al. [[131](#bib.bib131)] have introduced a deep semantic preserving
    and ranking-based hashing (DSRH) method by exploiting the hash and classification
    losses. Similar losses are also used in [[217](#bib.bib217)]. A deep visual-semantic
    quantization (DVSQ) [[218](#bib.bib218)] is used by jointly learning the visual-semantic
    embeddings and quantizers. An adaptive Gaussian filter based aggregation of CNN
    features is used in [[199](#bib.bib199)] to exploit the semantic information.
    Semantic hashing has been also extensively performed for sketch based image retrieval
    [[137](#bib.bib137)], [[153](#bib.bib153)], [[206](#bib.bib206)], [[158](#bib.bib158)],
    cross-modal retrieval [[198](#bib.bib198)], [[201](#bib.bib201)], [[139](#bib.bib139)].
    Other notable deep learning based works that model the semantic information include
    Multi-label retrieval [[207](#bib.bib207)], unsupervised image retrieval [[219](#bib.bib219)],
    supervised image retrieval [[102](#bib.bib102)], and semi-supervised image retrieval
    [[141](#bib.bib141)]. Semantic similarity in Hamming space based deep position-aware
    hashing (DPAH) [[89](#bib.bib89)] and semantic affinity deep semantic reconstruction
    hashing (DSRH) [[162](#bib.bib162)] are the recent methods for semantic retrieval.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年，Yao等人[[131](#bib.bib131)]提出了一种深度语义保持和基于排序的哈希（DSRH）方法，通过利用哈希和分类损失。类似的损失也被用于[[217](#bib.bib217)]。深度视觉-语义量化（DVSQ）[[218](#bib.bib218)]通过联合学习视觉-语义嵌入和量化器来实现。自适应高斯滤波器基于CNN特征的聚合在[[199](#bib.bib199)]中用于利用语义信息。语义哈希也被广泛应用于基于草图的图像检索[[137](#bib.bib137)]，[[153](#bib.bib153)]，[[206](#bib.bib206)]，[[158](#bib.bib158)]，跨模态检索[[198](#bib.bib198)]，[[201](#bib.bib201)]，[[139](#bib.bib139)]。其他显著的基于深度学习的语义信息建模工作包括多标签检索[[207](#bib.bib207)]，无监督图像检索[[219](#bib.bib219)]，有监督图像检索[[102](#bib.bib102)]，以及半监督图像检索[[141](#bib.bib141)]。在Hamming空间中的语义相似性基于深度位置感知哈希（DPAH）[[89](#bib.bib89)]和语义亲和深度语义重构哈希（DSRH）[[162](#bib.bib162)]是近期的语义检索方法。
- en: VII-G Fine-Grained Image Retrieval
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-G 细粒度图像检索
- en: In order to increase the discriminative ability of the deep learnt descriptors,
    many researchers have utilized the fine-grained constraints in deep networks.
    Different works have incorporated the fine-grained property using different approaches,
    such as capturing the inter-class and intra-class image similarities using a siamese
    network [[62](#bib.bib62)], attention modules based incorporation of the spatial-semantic
    information [[137](#bib.bib137)], using selective CNN features [[189](#bib.bib189)],
    fine-grained ranking using the weighted Hamming distance [[220](#bib.bib220)],
    using the multilevel semantic similarity between multi-label image pairs [[211](#bib.bib211)],
    and using a piecewise cross entropy loss [[221](#bib.bib221)].
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高深度学习描述符的区分能力，许多研究人员在深度网络中利用了细粒度约束。不同的研究通过不同的方法融入了细粒度特性，例如使用孪生网络捕捉类间和类内图像相似性[[62](#bib.bib62)]，基于注意力模块的空间-语义信息融合[[137](#bib.bib137)]，使用选择性CNN特征[[189](#bib.bib189)]，使用加权汉明距离的细粒度排序[[220](#bib.bib220)]，使用多标签图像对之间的多层次语义相似性[[211](#bib.bib211)]，以及使用分段交叉熵损失[[221](#bib.bib221)]。
- en: VII-H Asymmetric Quantization based Retrieval
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-H 非对称量化基础的检索
- en: In 2017, Wu et al. have performed the online asymmetric similarity learning
    to preserve the similarity between heterogeneous data [[202](#bib.bib202)]. An
    asymmetric deep supervised hashing (ADSH) is used by learning the deep hash function
    only for query images, while the hash codes for gallery images are directly learned
    [[104](#bib.bib104)]. In 2019, Yang et al. have investigated an asymmetric deep
    semantic quantization (ADSQ) using three stream networks to model the heterogeneous
    data [[222](#bib.bib222)]. A similarity preserving deep asymmetric quantization
    (SPDAQ) is proposed by exploiting the image subset and the label information of
    all the database items [[223](#bib.bib223)]. An adversary guided asymmetric hashing
    (AGAH) is introduced in [[160](#bib.bib160)] with the help of adversarial learning
    guided multi-label attention module for cross-modal image retrieval.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，Wu等人进行了在线非对称相似性学习以保持异质数据之间的相似性[[202](#bib.bib202)]。非对称深度监督哈希（ADSH）用于仅对查询图像学习深度哈希函数，而对库图像的哈希码直接学习[[104](#bib.bib104)]。2019年，Yang等人研究了使用三流网络建模异质数据的非对称深度语义量化（ADSQ）[[222](#bib.bib222)]。通过利用图像子集和所有数据库项的标签信息提出了相似性保持深度非对称量化（SPDAQ）[[223](#bib.bib223)]。在[[160](#bib.bib160)]中引入了一种对抗指导的非对称哈希（AGAH），该方法借助对抗学习指导的多标签注意力模块用于跨模态图像检索。
- en: VII-I Summary
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-I 总结
- en: 'Based on the progress in image retrieval using deep learning methods for different
    retrieval types, following are the outlines drawn from this section:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习方法在不同检索类型的图像检索进展，以下是本节的概要：
- en: •
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The cross-modal retrieval approaches learn the joint features for multiple modality
    using different networks. The recent methods utilize of the adversarial network
    for cross-modal retrieval. The similar observation and trend has been also witnessed
    for sketch based image retrieval.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨模态检索方法使用不同的网络学习多个模态的联合特征。近期的方法利用对抗网络进行跨模态检索。类似的观察和趋势也出现在基于草图的图像检索中。
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The multi-label and instance retrieval approaches are generally useful where
    more than one type of visual scenarios is present in the image. The deep learning
    based approaches are able to handle such retrieval by facilitating the feature
    learning through different type of networks.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多标签和实例检索方法在图像中存在多种视觉场景时通常很有用。基于深度学习的方法能够通过不同类型的网络促进特征学习，从而处理这种检索。
- en: •
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The region proposal network based feature selection has been employed by the
    existing deep learning methods for the object retrieval.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 区域提议网络基础的特征选择已被现有的深度学习方法用于对象检索。
- en: •
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The semantic information of the image has been used by different networks through
    abstract features to enhance the semantic image retrieval. The reconstruction
    based network is more suitable for semantic preserving hashing.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同的网络通过抽象特征使用图像的语义信息来增强语义图像检索。基于重建的网络更适合于语义保持哈希。
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Different feature selection and aggregation based networks have been utilized
    for fine-grained image retrieval.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同的特征选择和聚合基础网络已被用于细粒度图像检索。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The asymmetric hashing has also shown the suitability of deep learning models
    by processing the query and gallery images with different networks.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非对称哈希也展示了深度学习模型的适用性，通过用不同网络处理查询和图库图像。
- en: VIII Miscellaneous
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 杂项
- en: This section covers the deep learning models for retrieval in terms of the different
    losses, applications and other aspects.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了深度学习模型在检索中的进展，包括不同的损失函数、应用及其他方面。
- en: VIII-A Progress in Retrieval Loss
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A 检索损失的进展
- en: A siamese based loss function is used in [[132](#bib.bib132)] by Kumar et al.
    (2016) for minimizing the global loss leading to discriminative feature learning.
    Zhou et al. (2017) have used the triplet quantization loss for deep hashing, which
    is based on the similarity between the anchor-positive pairs and anchor-negative
    pairs [[135](#bib.bib135)]. A listwise loss has been employed by Revaud et al.
    in 2019 [[224](#bib.bib224)] to directly optimize the global mean average precision
    in end-to-end deep learning. In 2020, a piecewise cross entropy loss function
    is used in [[221](#bib.bib221)] for fine-grained image retrieval. Several innovative
    losses have been used by the different feature learning approaches such as a lifted
    structured loss [[66](#bib.bib66)] and ranking loss [[147](#bib.bib147)].
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Kumar等人（2016）在[[132](#bib.bib132)]中使用了基于孪生网络的损失函数，以最小化全局损失，从而实现判别特征学习。Zhou等人（2017）使用了三元组量化损失进行深度哈希，该损失基于锚点-正样本对和锚点-负样本对之间的相似性[[135](#bib.bib135)]。Revaud等人（2019）采用了列表损失[[224](#bib.bib224)]，直接优化端到端深度学习中的全局均值平均精度。在2020年，[[221](#bib.bib221)]中使用了分段交叉熵损失函数用于细粒度图像检索。不同的特征学习方法使用了几种创新损失，例如提升结构化损失[[66](#bib.bib66)]和排序损失[[147](#bib.bib147)]。
- en: 'TABLE II: Mean Average Precision (mAP) with 5000 retrieved images (mAP@5000)
    in % for different deep learning based image retrieval approaches over NUS-WIDE,
    MS COCO and CIFAR-10 datasets. Note that $2^{nd}$ column list the reference from
    where the results of corresponding approach are considered. Followings are the
    used acronyms for different network types in the results: DNN - Deep Neural Network,
    CNN - Convolutional Neural Network, SN - Siamese Network, TN - Triplet Network,
    GAN - Generative Adversarial Network, DQN - Deep Q Network, PTN - Parametric Transformation
    Network, DVN - Deep Variational Networks, and AE - Autoencoder.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：不同基于深度学习的图像检索方法在NUS-WIDE、MS COCO和CIFAR-10数据集上以5000张检索图像的均值平均精度（mAP@5000）百分比表示的结果。注意第$2^{nd}$列列出了结果的来源。以下是结果中不同网络类型的缩写：DNN
    - 深度神经网络，CNN - 卷积神经网络，SN - 孪生网络，TN - 三元组网络，GAN - 生成对抗网络，DQN - 深度Q网络，PTN - 参数变换网络，DVN
    - 深度变分网络，AE - 自编码器。
- en: '|  |  |  | NUS-WIDE | MS COCO |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | NUS-WIDE | MS COCO |'
- en: '| Method Name | Net. Type | Result Source | 16 Bits | 32 Bits | 64 Bits | 16
    Bits | 32 Bits | 64 Bits |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 方法名称 | 网络类型 | 结果来源 | 16位 | 32位 | 64位 | 16位 | 32位 | 64位 |'
- en: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[71](#bib.bib71)] | 57.0 | 58.3 | 60.0
    | 56.4 | 57.4 | 56.7 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[71](#bib.bib71)] | 57.0 | 58.3 | 60.0
    | 56.4 | 57.4 | 56.7 |'
- en: '| SDH’15 [[96](#bib.bib96)] | PTN | [[71](#bib.bib71)] | 47.6 | 55.5 | 58.1
    | 55.5 | 56.4 | 58.0 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| SDH’15 [[96](#bib.bib96)] | PTN | [[71](#bib.bib71)] | 47.6 | 55.5 | 58.1
    | 55.5 | 56.4 | 58.0 |'
- en: '| DNNH’15 [[63](#bib.bib63)] | DNN | [[71](#bib.bib71)] | 59.8 | 61.6 | 63.9
    | 59.3 | 60.3 | 61.0 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| DNNH’15 [[63](#bib.bib63)] | DNN | [[71](#bib.bib71)] | 59.8 | 61.6 | 63.9
    | 59.3 | 60.3 | 61.0 |'
- en: '| DHN’16 [[67](#bib.bib67)] | CNN | [[71](#bib.bib71)] | 63.7 | 66.4 | 67.1
    | 67.7 | 70.1 | 69.4 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| DHN’16 [[67](#bib.bib67)] | CNN | [[71](#bib.bib71)] | 63.7 | 66.4 | 67.1
    | 67.7 | 70.1 | 69.4 |'
- en: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[71](#bib.bib71)] | 66.2 | 69.9 |
    71.6 | 68.7 | 71.8 | 73.6 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[71](#bib.bib71)] | 66.2 | 69.9 |
    71.6 | 68.7 | 71.8 | 73.6 |'
- en: '| DeepBit’16 [[112](#bib.bib112)] | CNN | [[87](#bib.bib87)] | 39.2 | 40.3
    | 42.9 | 40.7 | 41.9 | 43.0 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| DeepBit’16 [[112](#bib.bib112)] | CNN | [[87](#bib.bib87)] | 39.2 | 40.3
    | 42.9 | 40.7 | 41.9 | 43.0 |'
- en: '| BGAN’18 [[121](#bib.bib121)] | GAN | [[87](#bib.bib87)] | 68.4 | 71.4 | 73.0
    | 64.5 | 68.2 | 70.7 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| BGAN’18 [[121](#bib.bib121)] | GAN | [[87](#bib.bib87)] | 68.4 | 71.4 | 73.0
    | 64.5 | 68.2 | 70.7 |'
- en: '| GreedyHash’18 [[76](#bib.bib76)] | CNN | [[87](#bib.bib87)] | 63.3 | 69.1
    | 73.1 | 58.2 | 66.8 | 71.0 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| GreedyHash’18 [[76](#bib.bib76)] | CNN | [[87](#bib.bib87)] | 63.3 | 69.1
    | 73.1 | 58.2 | 66.8 | 71.0 |'
- en: '| BinGAN’18 [[150](#bib.bib150)] | GAN | [[87](#bib.bib87)] | 65.4 | 70.9 |
    71.3 | 65.1 | 67.3 | 69.6 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| BinGAN’18 [[150](#bib.bib150)] | GAN | [[87](#bib.bib87)] | 65.4 | 70.9 |
    71.3 | 65.1 | 67.3 | 69.6 |'
- en: '| DVB’19 [[86](#bib.bib86)] | DVN | [[87](#bib.bib87)] | 60.4 | 63.2 | 66.5
    | 57.0 | 62.9 | 62.3 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| DVB’19 [[86](#bib.bib86)] | DVN | [[87](#bib.bib87)] | 60.4 | 63.2 | 66.5
    | 57.0 | 62.9 | 62.3 |'
- en: '| DistillHash’19 [[82](#bib.bib82)] | SN | [[87](#bib.bib87)] | 66.7 | 67.5
    | 67.7 | - | - | - |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| DistillHash’19 [[82](#bib.bib82)] | SN | [[87](#bib.bib87)] | 66.7 | 67.5
    | 67.7 | - | - | - |'
- en: '| TBH’20 [[87](#bib.bib87)] | AE | [[87](#bib.bib87)] | 71.7 | 72.5 | 73.5
    | 70.6 | 73.5 | 72.2 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| TBH’20 [[87](#bib.bib87)] | AE | [[87](#bib.bib87)] | 71.7 | 72.5 | 73.5
    | 70.6 | 73.5 | 72.2 |'
- en: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[84](#bib.bib84)] | 57.0 | 58.3 | 60.0
    | 56.4 | 57.4 | 56.7 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[84](#bib.bib84)] | 57.0 | 58.3 | 60.0
    | 56.4 | 57.4 | 56.7 |'
- en: '| DNNH’15 [[63](#bib.bib63)] | DNN | [[84](#bib.bib84)] | 59.8 | 61.6 | 63.9
    | 59.3 | 60.3 | 61.0 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| DNNH’15 [[63](#bib.bib63)] | DNN | [[84](#bib.bib84)] | 59.8 | 61.6 | 63.9
    | 59.3 | 60.3 | 61.0 |'
- en: '| DHN’16 [[67](#bib.bib67)] | CNN | [[84](#bib.bib84)] | 63.7 | 66.4 | 67.1
    | 67.7 | 70.1 | 69.4 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| DHN’16 [[67](#bib.bib67)] | CNN | [[84](#bib.bib84)] | 63.7 | 66.4 | 67.1
    | 67.7 | 70.1 | 69.4 |'
- en: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[84](#bib.bib84)] | 66.3 | 69.9 |
    71.6 | 68.7 | 71.8 | 73.6 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[84](#bib.bib84)] | 66.3 | 69.9 |
    71.6 | 68.7 | 71.8 | 73.6 |'
- en: '| DHA’19 [[84](#bib.bib84)] | CNN | [[84](#bib.bib84)] | 66.9 | 70.6 | 72.7
    | 70.8 | 73.1 | 75.2 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| DHA’19 [[84](#bib.bib84)] | CNN | [[84](#bib.bib84)] | 66.9 | 70.6 | 72.7
    | 70.8 | 73.1 | 75.2 |'
- en: '| HashGAN’18 [[120](#bib.bib120)] | GAN | [[120](#bib.bib120)] | 71.5 | 73.7
    | 74.8 | 69.7 | 72.5 | 74.4 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| HashGAN’18 [[120](#bib.bib120)] | GAN | [[120](#bib.bib120)] | 71.5 | 73.7
    | 74.8 | 69.7 | 72.5 | 74.4 |'
- en: '| UH-BDNN’16 [[186](#bib.bib186)] | DNN | [[123](#bib.bib123)] | 59.2 | 59.0
    | 61.0 | - | - | - |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| UH-BDNN’16 [[186](#bib.bib186)] | DNN | [[123](#bib.bib123)] | 59.2 | 59.0
    | 61.0 | - | - | - |'
- en: '| UTH’17 [[117](#bib.bib117)] | TN | [[123](#bib.bib123)] | 54.3 | 53.7 | 54.7
    | - | - | - |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| UTH’17 [[117](#bib.bib117)] | TN | [[123](#bib.bib123)] | 54.3 | 53.7 | 54.7
    | - | - | - |'
- en: '| UDTH’19 [[123](#bib.bib123)] | TN | [[123](#bib.bib123)] | 64.4 | 67.7 |
    69.6 | - | - | - |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| UDTH’19 [[123](#bib.bib123)] | TN | [[123](#bib.bib123)] | 64.4 | 67.7 |
    69.6 | - | - | - |'
- en: '| SSDH’17 [[102](#bib.bib102)] | CNN | [[89](#bib.bib89)] | - | - | - | 69.7
    | 72.5 | 74.4 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| SSDH’17 [[102](#bib.bib102)] | CNN | [[89](#bib.bib89)] | - | - | - | 69.7
    | 72.5 | 74.4 |'
- en: '| DPAH’20 [[89](#bib.bib89)] | PTN | [[89](#bib.bib89)] | - | - | - | 73.3
    | 76.8 | 78.2 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| DPAH’20 [[89](#bib.bib89)] | PTN | [[89](#bib.bib89)] | - | - | - | 73.3
    | 76.8 | 78.2 |'
- en: '| DRDH’20 [[167](#bib.bib167)] | DQN | [[167](#bib.bib167)] | 80.5 | 81.7 |
    81.8 | 71.5 | 74.8 | 76.1 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| DRDH’20 [[167](#bib.bib167)] | DQN | [[167](#bib.bib167)] | 80.5 | 81.7 |
    81.8 | 71.5 | 74.8 | 76.1 |'
- en: '| DVSQ’17 [[218](#bib.bib218)] | CNN | [[223](#bib.bib223)] | 79.0 | 79.7 |
    - | 71.2 | 72.0 | - |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| DVSQ’17 [[218](#bib.bib218)] | CNN | [[223](#bib.bib223)] | 79.0 | 79.7 |
    - | 71.2 | 72.0 | - |'
- en: '| DTQ’18 [[179](#bib.bib179)] | TN | [[223](#bib.bib223)] | 79.8 | 80.1 | -
    | 76.0 | 76.7 | - |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| DTQ’18 [[179](#bib.bib179)] | TN | [[223](#bib.bib223)] | 79.8 | 80.1 | -
    | 76.0 | 76.7 | - |'
- en: '| SPDAQ’19 [[223](#bib.bib223)] | CNN | [[223](#bib.bib223)] | 84.2 | 85.1
    | - | 84.4 | 84.7 | - |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| SPDAQ’19 [[223](#bib.bib223)] | CNN | [[223](#bib.bib223)] | 84.2 | 85.1
    | - | 84.4 | 84.7 | - |'
- en: '| DSQ’19 [[81](#bib.bib81)] | CNN | [[81](#bib.bib81)] | 77.9 | 79.0 | 79.9
    | - | - | - |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| DSQ’19 [[81](#bib.bib81)] | CNN | [[81](#bib.bib81)] | 77.9 | 79.0 | 79.9
    | - | - | - |'
- en: '|  |  |  | CIFAR-10 Dataset |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CIFAR-10 数据集 |'
- en: '|  |  |  | - | 12 Bits | 24 Bits | 32 Bits | 48 Bits | - |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | - | 12 位 | 24 位 | 32 位 | 48 位 | - |'
- en: '| SDH’15 [[96](#bib.bib96)] | PTN | [[225](#bib.bib225)] | - | 45.4 | 63.3
    | 65.1 | 66.0 | - |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| SDH’15 [[96](#bib.bib96)] | PTN | [[225](#bib.bib225)] | - | 45.4 | 63.3
    | 65.1 | 66.0 | - |'
- en: '| DSH’16 [[97](#bib.bib97)] | CNN | [[225](#bib.bib225)] | - | 64.4 | 74.2
    | 77.0 | 79.9 | - |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| DSH’16 [[97](#bib.bib97)] | CNN | [[225](#bib.bib225)] | - | 64.4 | 74.2
    | 77.0 | 79.9 | - |'
- en: '| DHN’16 [[67](#bib.bib67)] | CNN | [[225](#bib.bib225)] | - | 68.1 | 72.1
    | 72.3 | 73.3 | - |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| DHN’16 [[67](#bib.bib67)] | CNN | [[225](#bib.bib225)] | - | 68.1 | 72.1
    | 72.3 | 73.3 | - |'
- en: '| DPSH’16 [[98](#bib.bib98)] | SN | [[225](#bib.bib225)] | - | 68.2 | 72.0
    | 73.4 | 74.6 | - |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| DPSH’16 [[98](#bib.bib98)] | SN | [[225](#bib.bib225)] | - | 68.2 | 72.0
    | 73.4 | 74.6 | - |'
- en: '| DQN’16 [[68](#bib.bib68)] | CNN | [[225](#bib.bib225)] | - | 55.4 | 55.8
    | 56.4 | 58.0 | - |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| DQN’16 [[68](#bib.bib68)] | CNN | [[225](#bib.bib225)] | - | 55.4 | 55.8
    | 56.4 | 58.0 | - |'
- en: '| DSDH’17 [[101](#bib.bib101)] | CNN | [[225](#bib.bib225)] | - | 74.0 | 78.6
    | 80.1 | 82.0 | - |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| DSDH’17 [[101](#bib.bib101)] | CNN | [[225](#bib.bib225)] | - | 74.0 | 78.6
    | 80.1 | 82.0 | - |'
- en: '| ADSH’18 [[104](#bib.bib104)] | CNN | [[225](#bib.bib225)] | - | 89.0 | 92.8
    | 93.1 | 93.9 | - |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| ADSH’18 [[104](#bib.bib104)] | CNN | [[225](#bib.bib225)] | - | 89.0 | 92.8
    | 93.1 | 93.9 | - |'
- en: '| DIHN2+ADSH’19 [[80](#bib.bib80)] | CNN | [[225](#bib.bib225)] | - | 89.8
    | 92.9 | 92.9 | 93.9 | - |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| DIHN2+ADSH’19 [[80](#bib.bib80)] | CNN | [[225](#bib.bib225)] | - | 89.8
    | 92.9 | 92.9 | 93.9 | - |'
- en: '| DTH’20 [[225](#bib.bib225)] | CNN | [[225](#bib.bib225)] | - | 92.1 | 93.3
    | 93.7 | 94.9 | - |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| DTH’20 [[225](#bib.bib225)] | CNN | [[225](#bib.bib225)] | - | 92.1 | 93.3
    | 93.7 | 94.9 | - |'
- en: VIII-B Applications
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 应用
- en: The deep learning based approaches have been utilized for image retrieval pertaining
    to different applications such as cloth retrieval [[181](#bib.bib181)], biomedical
    image retrieval [[197](#bib.bib197)], face retrieval [[226](#bib.bib226)], [[227](#bib.bib227)],
    remote sensing image retrieval [[124](#bib.bib124)], landmark retrieval [[228](#bib.bib228)],
    social image retrieval [[229](#bib.bib229)], and video retrieval [[178](#bib.bib178)].
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的方法已被应用于不同领域的图像检索，包括衣物检索 [[181](#bib.bib181)]、生物医学图像检索 [[197](#bib.bib197)]、面部检索
    [[226](#bib.bib226)]、[[227](#bib.bib227)]、遥感图像检索 [[124](#bib.bib124)]、地标检索 [[228](#bib.bib228)]、社交图像检索
    [[229](#bib.bib229)] 和视频检索 [[178](#bib.bib178)]。
- en: 'TABLE III: Mean Average Precision (mAP) with 1000 retrieved images (mAP@1000)
    in % for different deep learning based image retrieval methods over ImageNet,
    CIFAR-10 and MNIST datasets.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：不同深度学习基于图像检索方法在 ImageNet、CIFAR-10 和 MNIST 数据集上的 1000 张检索图像的平均精度（mAP@1000）百分比。
- en: '| Method | Network Type | Result Source | 16 Bits | 32 Bits | 48 Bits | 64
    Bits |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 网络类型 | 结果来源 | 16 位 | 32 位 | 48 位 | 64 位 |'
- en: '|  |  |  | ImageNet Dataset |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | ImageNet 数据集 |'
- en: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[71](#bib.bib71)] | 28.1 | 45.0 | 52.5
    | 55.4 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[71](#bib.bib71)] | 28.1 | 45.0 | 52.5
    | 55.4 |'
- en: '| SDH’15 [[96](#bib.bib96)] | PTN | [[71](#bib.bib71)] | 29.9 | 45.5 | 55.5
    | 58.5 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| SDH’15 [[96](#bib.bib96)] | PTN | [[71](#bib.bib71)] | 29.9 | 45.5 | 55.5
    | 58.5 |'
- en: '| DNNH’15 [[63](#bib.bib63)] | DNN | [[71](#bib.bib71)] | 29.0 | 46.1 | 53.0
    | 56.5 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| DNNH’15 [[63](#bib.bib63)] | DNN | [[71](#bib.bib71)] | 29.0 | 46.1 | 53.0
    | 56.5 |'
- en: '| DHN’16 [[67](#bib.bib67)] | CNN | [[71](#bib.bib71)] | 31.1 | 47.2 | 54.2
    | 57.3 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| DHN’16 [[67](#bib.bib67)] | CNN | [[71](#bib.bib71)] | 31.1 | 47.2 | 54.2
    | 57.3 |'
- en: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[71](#bib.bib71)] | 50.6 | 63.1 |
    66.3 | 68.4 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[71](#bib.bib71)] | 50.6 | 63.1 |
    66.3 | 68.4 |'
- en: '| SSDH’17 [[102](#bib.bib102)] | CNN | [[89](#bib.bib89)] | 63.4 | 69.2 | 70.1
    | 70.7 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| SSDH’17 [[102](#bib.bib102)] | CNN | [[89](#bib.bib89)] | 63.4 | 69.2 | 70.1
    | 70.7 |'
- en: '| DSQ’19 [[81](#bib.bib81)] | CNN | [[81](#bib.bib81)] | 57.8 | 65.4 | 68.0
    | 69.4 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| DSQ’19 [[81](#bib.bib81)] | CNN | [[81](#bib.bib81)] | 57.8 | 65.4 | 68.0
    | 69.4 |'
- en: '| DPAH’20 [[89](#bib.bib89)] | PTN | [[89](#bib.bib89)] | 65.2 | 70.0 | 71.5
    | 71.4 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| DPAH’20 [[89](#bib.bib89)] | PTN | [[89](#bib.bib89)] | 65.2 | 70.0 | 71.5
    | 71.4 |'
- en: '|  |  |  | CIFAR-10 Dataset |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CIFAR-10 数据集 |'
- en: '| BGAN’18 [[121](#bib.bib121)] | GAN | [[87](#bib.bib87)] | 52.5 | 53.1 | -
    | 56.2 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| BGAN’18 [[121](#bib.bib121)] | GAN | [[87](#bib.bib87)] | 52.5 | 53.1 | -
    | 56.2 |'
- en: '| GreedyHash’18 [[76](#bib.bib76)] | CNN | [[87](#bib.bib87)] | 44.8 | 47.3
    | - | 50.1 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| GreedyHash’18 [[76](#bib.bib76)] | CNN | [[87](#bib.bib87)] | 44.8 | 47.3
    | - | 50.1 |'
- en: '| BinGAN’18 [[150](#bib.bib150)] | GAN | [[87](#bib.bib87)] | 47.6 | 51.2 |
    - | 52.0 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| BinGAN’18 [[150](#bib.bib150)] | GAN | [[87](#bib.bib87)] | 47.6 | 51.2 |
    - | 52.0 |'
- en: '| HashGAN’18 [[120](#bib.bib120)] | GAN | [[87](#bib.bib87)] | 44.7 | 46.3
    | - | 48.1 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| HashGAN’18 [[120](#bib.bib120)] | GAN | [[87](#bib.bib87)] | 44.7 | 46.3
    | - | 48.1 |'
- en: '| DVB’19 [[86](#bib.bib86)] | DVN | [[87](#bib.bib87)] | 40.3 | 42.2 | - |
    44.6 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| DVB’19 [[86](#bib.bib86)] | DVN | [[87](#bib.bib87)] | 40.3 | 42.2 | - |
    44.6 |'
- en: '| DistillHash’19 [[82](#bib.bib82)] | SN | [[87](#bib.bib87)] | 28.4 | 28.5
    | - | 28.8 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| DistillHash’19 [[82](#bib.bib82)] | SN | [[87](#bib.bib87)] | 28.4 | 28.5
    | - | 28.8 |'
- en: '| TBH’20 [[87](#bib.bib87)] | AE | [[87](#bib.bib87)] | 53.2 | 57.3 | - | 57.8
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| TBH’20 [[87](#bib.bib87)] | AE | [[87](#bib.bib87)] | 53.2 | 57.3 | - | 57.8
    |'
- en: '| SDH’15 [[110](#bib.bib110)] | PTN | [[110](#bib.bib110)] | 18.8 | 20.8 |
    - | 22.5 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| SDH’15 [[110](#bib.bib110)] | PTN | [[110](#bib.bib110)] | 18.8 | 20.8 |
    - | 22.5 |'
- en: '| DAR’16 [[111](#bib.bib111)] | TN | [[111](#bib.bib111)] | 16.8 | 17.0 | -
    | 17.2 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| DAR’16 [[111](#bib.bib111)] | TN | [[111](#bib.bib111)] | 16.8 | 17.0 | -
    | 17.2 |'
- en: '| DH’15 [[110](#bib.bib110)] | DNN | [[117](#bib.bib117)] | 16.2 | 16.6 | -
    | 17.0 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| DH’15 [[110](#bib.bib110)] | DNN | [[117](#bib.bib117)] | 16.2 | 16.6 | -
    | 17.0 |'
- en: '| DeepBit’16 [[112](#bib.bib112)] | CNN | [[117](#bib.bib117)] | 19.4 | 24.9
    | - | 27.7 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| DeepBit’16 [[112](#bib.bib112)] | CNN | [[117](#bib.bib117)] | 19.4 | 24.9
    | - | 27.7 |'
- en: '| UTH’17 [[117](#bib.bib117)] | TN | [[117](#bib.bib117)] | 28.7 | 30.7 | -
    | 32.4 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| UTH’17 [[117](#bib.bib117)] | TN | [[117](#bib.bib117)] | 28.7 | 30.7 | -
    | 32.4 |'
- en: '| DBD-MQ’17 [[114](#bib.bib114)] | CNN | [[114](#bib.bib114)] | 21.5 | 26.5
    | - | 31.9 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| DBD-MQ’17 [[114](#bib.bib114)] | CNN | [[114](#bib.bib114)] | 21.5 | 26.5
    | - | 31.9 |'
- en: '| UCBD’18 [[113](#bib.bib113)] | CNN | [[113](#bib.bib113)] | 26.4 | 27.9 |
    - | 34.1 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| UCBD’18 [[113](#bib.bib113)] | CNN | [[113](#bib.bib113)] | 26.4 | 27.9 |
    - | 34.1 |'
- en: '| UH-BDNN’16 [[186](#bib.bib186)] | DNN | [[123](#bib.bib123)] | 30.1 | 30.9
    | - | 31.2 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| UH-BDNN’16 [[186](#bib.bib186)] | DNN | [[123](#bib.bib123)] | 30.1 | 30.9
    | - | 31.2 |'
- en: '| UDTH’19 [[123](#bib.bib123)] | TN | [[123](#bib.bib123)] | 46.1 | 50.4 |
    - | 54.3 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| UDTH’19 [[123](#bib.bib123)] | TN | [[123](#bib.bib123)] | 46.1 | 50.4 |
    - | 54.3 |'
- en: '|  |  |  | MNIST Dataset |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | MNIST 数据集 |'
- en: '| SDH’15 [[110](#bib.bib110)] | PTN | [[110](#bib.bib110)] | 46.8 | 51.0 |
    - | 52.5 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| SDH’15 [[110](#bib.bib110)] | PTN | [[110](#bib.bib110)] | 46.8 | 51.0 |
    - | 52.5 |'
- en: '| DH’15 [[110](#bib.bib110)] | DNN | [[117](#bib.bib117)] | 43.1 | 45.0 | -
    | 46.7 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| DH’15 [[110](#bib.bib110)] | DNN | [[117](#bib.bib117)] | 43.1 | 45.0 | -
    | 46.7 |'
- en: '| DeepBit’16 [[112](#bib.bib112)] | CNN | [[117](#bib.bib117)] | 28.2 | 32.0
    | - | 44.5 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| DeepBit’16 [[112](#bib.bib112)] | CNN | [[117](#bib.bib117)] | 28.2 | 32.0
    | - | 44.5 |'
- en: '| UTH’17 [[117](#bib.bib117)] | TN | [[117](#bib.bib117)] | 43.2 | 46.6 | -
    | 49.9 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| UTH’17 [[117](#bib.bib117)] | TN | [[117](#bib.bib117)] | 43.2 | 46.6 | -
    | 49.9 |'
- en: VIII-C Others
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-C 其他
- en: The hashing difficulty is also increased by generating the harder samples in
    a self-paced manner [[161](#bib.bib161)] to make the network training as reasoning
    oriented. In the initial work, the pre-trained CNN features have also very promising
    retrieval performance [[61](#bib.bib61)]. Recently, the transfer learning has
    been also utilized in [[225](#bib.bib225)] for deep transfer hashing.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希难度也通过以自我节奏的方式生成更难的样本[[161](#bib.bib161)]来增加，以使网络训练更具推理导向。在早期的工作中，预训练的 CNN
    特征也表现出了非常有前景的检索性能[[61](#bib.bib61)]。最近，转移学习也被用于[[225](#bib.bib225)]中的深度转移哈希。
- en: VIII-D Summary
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-D 总结
- en: Researchers have come up with various loss functions to facilitate the discriminative
    learning of features by the networks for image retrieval. The losses constraint
    and guide the training of the deep learning models. The image retrieval has shown
    a great utilization with its application to solve the real-life problems. Researchers
    have also tried to understand what works and what not for deep learning based
    image retrieval. The transfer learning has been also utilized for retrieval.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员提出了各种损失函数，以促进网络在图像检索中的特征辨别学习。这些损失函数限制并引导深度学习模型的训练。图像检索在解决现实生活问题方面显示了极大的应用价值。研究人员还尝试理解深度学习图像检索中有效和无效的部分。转移学习也被用于检索。
- en: 'TABLE IV: mAP@54000 and mAP@All in % for state-of-the-art and recent image
    retrieval methods over the CIFAR-10 dataset.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：在 CIFAR-10 数据集上，最新图像检索方法的 mAP@54000 和 mAP@All 的百分比。
- en: '|  |  |  | CIFAR-10 Dataset |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CIFAR-10 数据集 |'
- en: '| Method Name | Network Type | Result Source | 16 Bits | 24 Bits | 32 Bits
    | 48 Bits | 64 Bits |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 方法名称 | 网络类型 | 结果来源 | 16 位 | 24 位 | 32 位 | 48 位 | 64 位 |'
- en: '|  |  |  | mAP@54000 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | mAP@54000 |'
- en: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[84](#bib.bib84)] | 47.6 | - | 47.2 |
    48.9 | 50.1 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[84](#bib.bib84)] | 47.6 | - | 47.2 |
    48.9 | 50.1 |'
- en: '| DNNH’15 [[63](#bib.bib63)] | TN | [[84](#bib.bib84)] | 55.9 | - | 55.8 |
    58.1 | 58.3 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| DNNH’15 [[63](#bib.bib63)] | TN | [[84](#bib.bib84)] | 55.9 | - | 55.8 |
    58.1 | 58.3 |'
- en: '| SDH’15 [[96](#bib.bib96)] | PTN | [[84](#bib.bib84)] | 46.1 | - | 52.0 |
    55.3 | 56.8 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| SDH’15 [[96](#bib.bib96)] | PTN | [[84](#bib.bib84)] | 46.1 | - | 52.0 |
    55.3 | 56.8 |'
- en: '| DHN’16 [[67](#bib.bib67)] | CNN | [[84](#bib.bib84)] | 56.8 |  | 60.3 | 62.1
    | 63.5 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| DHN’16 [[67](#bib.bib67)] | CNN | [[84](#bib.bib84)] | 56.8 |  | 60.3 | 62.1
    | 63.5 |'
- en: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[84](#bib.bib84)] | 64.3 | - | 66.7
    | 67.5 | 68.7 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[84](#bib.bib84)] | 64.3 | - | 66.7
    | 67.5 | 68.7 |'
- en: '| DHA’14 [[84](#bib.bib84)] | CNN | [[84](#bib.bib84)] | 65.2 |  | 68.1 | 69.0
    | 69.9 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| DHA’14 [[84](#bib.bib84)] | CNN | [[84](#bib.bib84)] | 65.2 |  | 68.1 | 69.0
    | 69.9 |'
- en: '| HashGAN’18 [[120](#bib.bib120)] | GAN | [[120](#bib.bib120)] | 66.8 | - |
    73.1 | 73.5 | 74.9 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| HashGAN’18 [[120](#bib.bib120)] | GAN | [[120](#bib.bib120)] | 66.8 | - |
    73.1 | 73.5 | 74.9 |'
- en: '| DTQ’18 [[179](#bib.bib179)] | TN | [[179](#bib.bib179)] | 78.9 | - | 79.2
    | - | - |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| DTQ’18 [[179](#bib.bib179)] | TN | [[179](#bib.bib179)] | 78.9 | - | 79.2
    | - | - |'
- en: '| DRDH’20 [[167](#bib.bib167)] | DQN | [[167](#bib.bib167)] | 78.7 | - | 80.5
    | 80.6 | 80.3 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| DRDH’20 [[167](#bib.bib167)] | DQN | [[167](#bib.bib167)] | 78.7 | - | 80.5
    | 80.6 | 80.3 |'
- en: '|  |  |  | mAP@All |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | mAP@All |'
- en: '| DQN’16 [[68](#bib.bib68)] | CNN | [[223](#bib.bib223)] | - | 55.8 | 56.4
    | 58.0 | - |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| DQN’16 [[68](#bib.bib68)] | CNN | [[223](#bib.bib223)] | - | 55.8 | 56.4
    | 58.0 | - |'
- en: '| DPSH’16 [[98](#bib.bib98)] | SN | [[223](#bib.bib223)] | - | 72.7 | 74.4
    | 75.7 | - |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| DPSH’16 [[98](#bib.bib98)] | SN | [[223](#bib.bib223)] | - | 72.7 | 74.4
    | 75.7 | - |'
- en: '| DSDH’17 [[101](#bib.bib101)] | CNN | [[223](#bib.bib223)] | - | 78.6 | 80.1
    | 82.0 | - |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| DSDH’17 [[101](#bib.bib101)] | CNN | [[223](#bib.bib223)] | - | 78.6 | 80.1
    | 82.0 | - |'
- en: '| DTQ’18 [[179](#bib.bib179)] | DQN | [[223](#bib.bib223)] | - | 79.0 | 79.2
    | - | - |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| DTQ’18 [[179](#bib.bib179)] | DQN | [[223](#bib.bib223)] | - | 79.0 | 79.2
    | - | - |'
- en: '| DVSQ’17 [[218](#bib.bib218)] | CNN | [[223](#bib.bib223)] | - | 80.3 | 80.8
    | 81.1 | - |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| DVSQ’17 [[218](#bib.bib218)] | CNN | [[223](#bib.bib223)] | - | 80.3 | 80.8
    | 81.1 | - |'
- en: '| SPDAQ’19 [[223](#bib.bib223)] | CNN | [[223](#bib.bib223)] | - | 88.4 | 89.1
    | 89.3 | - |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| SPDAQ’19 [[223](#bib.bib223)] | CNN | [[223](#bib.bib223)] | - | 88.4 | 89.1
    | 89.3 | - |'
- en: '| SSAH’19 [[161](#bib.bib161)] | GAN | [[161](#bib.bib161)] | - | 87.8 | -
    | 88.6 | - |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| SSAH’19 [[161](#bib.bib161)] | GAN | [[161](#bib.bib161)] | - | 87.8 | -
    | 88.6 | - |'
- en: '| DeepBit’19 [[112](#bib.bib112)] | CNN | [[122](#bib.bib122)] | 22.0 | - |
    24.1 | - | 29.0 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| DeepBit’19 [[112](#bib.bib112)] | CNN | [[122](#bib.bib122)] | 22.0 | - |
    24.1 | - | 29.0 |'
- en: '| BGAN’19 [[121](#bib.bib121)] | GAN | [[122](#bib.bib122)] | 49.7 | - | 47.0
    | - | 50.7 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| BGAN’19 [[121](#bib.bib121)] | GAN | [[122](#bib.bib122)] | 49.7 | - | 47.0
    | - | 50.7 |'
- en: '| UADH’19 [[122](#bib.bib122)] | GAN | [[122](#bib.bib122)] | 67.7 | - | 68.9
    | - | 69.6 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| UADH’19 [[122](#bib.bib122)] | GAN | [[122](#bib.bib122)] | 67.7 | - | 68.9
    | - | 69.6 |'
- en: '| DSAH’19 [[155](#bib.bib155)] | CNN | [[155](#bib.bib155)] | - | 84.1 | 84.5
    | 84.9 | - |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| DSAH’19 [[155](#bib.bib155)] | CNN | [[155](#bib.bib155)] | - | 84.1 | 84.5
    | 84.9 | - |'
- en: IX Performance Comparison
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 性能比较
- en: 'This survey also presents a performance analysis for the state-of-the-art deep
    learning based image retrieval approaches. The Mean Average Precision (mAP) reported
    for the different image retrieval approaches is summarized in Table [II](#S8.T2
    "TABLE II ‣ VIII-A Progress in Retrieval Loss ‣ VIII Miscellaneous ‣ A Decade
    Survey of Content Based Image Retrieval using Deep Learning"), [III](#S8.T3 "TABLE
    III ‣ VIII-B Applications ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based
    Image Retrieval using Deep Learning"), and [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary
    ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning"). The mAP@5000 (i.e., 5000 retrieved images) using various existing
    deep learning approaches is summarized in Table [II](#S8.T2 "TABLE II ‣ VIII-A
    Progress in Retrieval Loss ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based
    Image Retrieval using Deep Learning") over CIFAR-10, NUS-WIDE and MS COCO datasets.
    The results over CIFAR-10, ImageNet and MNIST datasets using different state-of-the-art
    deep learning based image retrieval methods are compiled in Table [III](#S8.T3
    "TABLE III ‣ VIII-B Applications ‣ VIII Miscellaneous ‣ A Decade Survey of Content
    Based Image Retrieval using Deep Learning") in terms of the mAP@1000\. The mAP@54000
    using few methods is reported in Table [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary
    ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning") over the CIFAR-10 dataset. The standard mAP is also depicted in
    Table [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary ‣ VIII Miscellaneous ‣ A Decade Survey
    of Content Based Image Retrieval using Deep Learning") by considering all the
    retrieved images for CIFAR-10 dataset using some of the available literature.
    Note that 2nd column in Table [II](#S8.T2 "TABLE II ‣ VIII-A Progress in Retrieval
    Loss ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning"), [III](#S8.T3 "TABLE III ‣ VIII-B Applications ‣ VIII Miscellaneous
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning"), and
    [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary ‣ VIII Miscellaneous ‣ A Decade Survey
    of Content Based Image Retrieval using Deep Learning") list the source reference
    of the corresponding method reported results. Following are the observations out
    of these results by deep learning methods:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查还呈现了最先进的基于深度学习的图像检索方法的性能分析。不同图像检索方法的平均精度均值（mAP）总结在表 [II](#S8.T2 "TABLE II
    ‣ VIII-A Progress in Retrieval Loss ‣ VIII Miscellaneous ‣ A Decade Survey of
    Content Based Image Retrieval using Deep Learning")、[III](#S8.T3 "TABLE III ‣
    VIII-B Applications ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning") 和 [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary ‣ VIII
    Miscellaneous ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning")
    中。使用各种现有深度学习方法的mAP@5000（即，5000张检索图像）在表 [II](#S8.T2 "TABLE II ‣ VIII-A Progress
    in Retrieval Loss ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning") 中总结，数据集包括CIFAR-10、NUS-WIDE 和 MS COCO。表 [III](#S8.T3
    "TABLE III ‣ VIII-B Applications ‣ VIII Miscellaneous ‣ A Decade Survey of Content
    Based Image Retrieval using Deep Learning") 中汇总了不同最先进深度学习方法在CIFAR-10、ImageNet
    和 MNIST 数据集上的 mAP@1000 的结果。表 [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary ‣ VIII Miscellaneous
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning") 中报告了使用少数方法的
    mAP@54000，数据集为 CIFAR-10。表 [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary ‣ VIII Miscellaneous
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning") 还展示了标准
    mAP，通过考虑 CIFAR-10 数据集中所有检索图像以及一些现有文献。注意，表 [II](#S8.T2 "TABLE II ‣ VIII-A Progress
    in Retrieval Loss ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning")、[III](#S8.T3 "TABLE III ‣ VIII-B Applications
    ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning") 和 [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary ‣ VIII Miscellaneous
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning") 中的第2列列出了对应方法报告结果的来源参考。以下是这些深度学习方法的观察结果：
- en: •
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recently proposed Deep Transfer Hashing (DTH) by Zhai et al. (2020) [[225](#bib.bib225)]
    have shown outstanding performance over CIFAR-10 and NUS-WIDE datasets in terms
    of the mAP@5000\. Other promising methods include Deep Spatial Attention Hashing
    (DSAH) by Ge et al. (2019) [[155](#bib.bib155)], Similarity Preserving Deep Asymmetric
    Quantization (SPDAQ) by Chen et al. (2019) [[223](#bib.bib223)], Deep Position-Aware
    Hashing (DPAH) by Wang et al. (2020) [[89](#bib.bib89)] and Deep Reinforcement
    De-Redundancy Hashing (DRDH) by Yang et al. (2020) [[167](#bib.bib167)].
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近，Zhai等（2020年）提出的深度迁移哈希（DTH）[[225](#bib.bib225)]在CIFAR-10和NUS-WIDE数据集上的mAP@5000方面表现出色。其他有前景的方法包括Ge等（2019年）的深度空间注意力哈希（DSAH）[[155](#bib.bib155)]、Chen等（2019年）的相似性保留深度非对称量化（SPDAQ）[[223](#bib.bib223)]、Wang等（2020年）的深度位置感知哈希（DPAH）[[89](#bib.bib89)]和Yang等（2020年）的深度强化去冗余哈希（DRDH）[[167](#bib.bib167)]。
- en: •
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Twin-Bottleneck Hashing (TBH) introduced by Shen et al. (2020) [[87](#bib.bib87)]
    is also observed as an appealing method using autoencoder having a double bottleneck
    over the CIFAR-10 dataset in terms of the mAP@1000\. However, the Deep Position-Aware
    Hashing (DPAH) investigated by Wang et al. (2020) [[89](#bib.bib89)] have outperformed
    the other approaches over ImageNet dataset. Supervised Deep Hashing (SDH) by Erin
    et al. (2015) [[110](#bib.bib110)] has depicted appealing performance over the
    MNIST dataset.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Shen等（2020年）提出的双瓶颈哈希（TBH）[[87](#bib.bib87)]也被认为是一种有吸引力的方法，使用具有双瓶颈的自动编码器在CIFAR-10数据集上的mAP@1000方面表现出色。然而，Wang等（2020年）研究的深度位置感知哈希（DPAH）[[89](#bib.bib89)]在ImageNet数据集上的表现超过了其他方法。Erin等（2015年）的监督深度哈希（SDH）[[110](#bib.bib110)]在MNIST数据集上展现了良好的性能。
- en: •
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The deep reinforcement learning based image retrieval model, namely Deep Reinforcement
    De-Redundancy Hashing (DRDH) by Yang et al. (2020) [[167](#bib.bib167)], is one
    of recent breakthrough as supported by superlative mAP@54000 over the CIFAR-10
    dataset. The Deep Triplet Quantization [[179](#bib.bib179)] is also one of the
    favourable model for feature learning.
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由Yang等（2020年）提出的深度强化学习基于图像检索模型，即深度强化去冗余哈希（DRDH）[[167](#bib.bib167)]，是最近的一项突破，在CIFAR-10数据集上的mAP@54000表现出色。深度三元组量化[[179](#bib.bib179)]也是特征学习的一个优选模型。
- en: •
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Similarity Preserving Deep Asymmetric Quantization (SPDAQ) by Chen et al.
    (2019) [[223](#bib.bib223)] and Unsupervised ADversarial Hashing (UADH) by Deng
    et al. (2019) [[122](#bib.bib122)] methods have been also identified as very encouraging
    based on the mAP by considering all the retrieved images over the CIFAR-10 dataset.
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Chen等（2019年）的相似性保留深度非对称量化（SPDAQ）[[223](#bib.bib223)]和Deng等（2019年）的无监督对抗哈希（UADH）[[122](#bib.bib122)]方法在CIFAR-10数据集上考虑所有检索图像的mAP也被认为非常鼓舞人心。
- en: X Conclusion and Future Directives
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: X 结论与未来方向
- en: X-A Conclusion and Trend
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-A 结论与趋势
- en: This paper presents a comprehensive survey of deep learning methods for content
    based image retrieval. As most of the deep learning based developments are recent,
    this survey majorly focuses over the image retrieval methods using deep learning
    in a decade from 2011 to 2020\. A detailed taxonomy is presented in terms of different
    supervision type, different networks used, different data type of descriptors,
    different retrieval type and other aspects. The detailed discussion under each
    section is also presented with the further categorization. A chronological summarization
    is presented to show the evolution of the deep learning models for image retrieval.
    Moreover, the chronological overview is also portrayed under each category to
    showcase the growth of image retrieval approaches. A summary of large-scale common
    datasets used for image retrieval is also compiled in this survey. A performance
    analysis of the state-of-the-art deep learning based image retrieval methods is
    also conducted in terms of the mean average precision for different no. of retrieved
    images.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了针对基于内容的图像检索的深度学习方法的全面调查。由于大多数基于深度学习的开发都是近期出现的，本调查主要集中在2011年至2020年间使用深度学习的图像检索方法。提供了一个详细的分类，包括不同的监督类型、不同的网络使用、不同的数据描述符类型、不同的检索类型及其他方面。每个部分下的详细讨论也进行了进一步分类。提供了一个按时间顺序的总结，以展示深度学习模型在图像检索中的演变。此外，各类别下的按时间顺序概述也展示了图像检索方法的增长。本调查还汇编了用于图像检索的大规模公共数据集的总结。同时，还对最先进的深度学习图像检索方法进行了性能分析，依据不同检索图像数量的平均精度。
- en: The research trend in image retrieval suggests that the deep learning based
    models are driving the progress. The recently developed models such as generative
    adversarial networks, autoencoder networks and reinforcement learning networks
    have shown the superior performance for image retrieval. The discovery of better
    objective functions has been also the trend in order to constrain the learning
    of the hash code for discriminative, robust and efficient image retrieval. The
    semantic preserving class-specific feature learning using different networks and
    different quantization techniques is also the recent trend for image retrieval.
    Other trends include utilization of attention module, transfer learning, etc.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图像检索领域的研究趋势表明，基于深度学习的模型正在推动进展。最近开发的模型如生成对抗网络、自动编码器网络和强化学习网络在图像检索中表现优越。发现更好的目标函数也是一种趋势，目的是约束哈希码的学习，以实现具有判别性、鲁棒性和高效性的图像检索。使用不同网络和量化技术进行语义保留的类别特征学习也是图像检索中的一种新趋势。其他趋势还包括注意力模块的利用、迁移学习等。
- en: X-B Future Directions
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-B 未来方向
- en: The future work in image retrieval using deep learning can include exploration
    of improved deep learning models, more relevant objective functions, minimum loss
    based quantization techniques, semantic preserving feature learning, and attention
    focused feature learning. The future direction in the image retrieval might be
    driven from the basic goal of the expected solution. There are three important
    aspects of any retrieval system, which include the discriminative ability, robustness
    capability and fast image search. In order to achieve the discriminative ability,
    the features corresponding to the samples of different class should be as far
    as possible. Thus, different approaches such as triplet based objective function,
    consideration of class distribution, incorporation of distance between class centroids,
    etc. can be exploited. In order to maintain the robustness property, various data
    augmentation, layer manipulation, siamese loss based objective functions, feature
    normalization, incorporation of class distribution and majority voting in the
    feature representation, etc. can be explored. In order to perform the faster image
    search, the learnt feature or hash code should be as low dimensional and compact
    as possible. Thus, better strategy for feature quantization and maximizing the
    relevant information into feature space in a compact way can be seen as one of
    the future directions. The self-supervised learning has shown very promising performance
    for different down-stream tasks and has potential to learn the important features
    in compact form. Thus, in future, the self-supervised learning can boost the performance
    of image retrieval models significantly.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习进行图像检索的未来工作可以包括改进深度学习模型、更相关的目标函数、基于最小损失的量化技术、语义保留特征学习以及关注注意力的特征学习。图像检索的未来方向可能会受到预期解决方案基本目标的驱动。任何检索系统有三个重要方面，包括判别能力、鲁棒性和快速图像搜索。为了实现判别能力，不同类别样本对应的特征应该尽可能远离。因此，可以利用三元组目标函数、类别分布的考虑、类别中心距离的引入等不同方法。为了保持鲁棒性，可以探索各种数据增强、层操作、基于孪生网络损失的目标函数、特征归一化、类别分布的引入和特征表示中的多数投票等方法。为了实现更快的图像搜索，学习到的特征或哈希码应该尽可能低维且紧凑。因此，更好的特征量化策略和将相关信息以紧凑方式最大化到特征空间的策略可以视为未来的方向之一。自监督学习在不同下游任务中表现出了非常有前景的性能，并有潜力以紧凑的形式学习重要特征。因此，在未来，自监督学习可以显著提升图像检索模型的性能。
- en: Acknowledgement
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by Global Innovation & Technology Alliance (GITA) on
    behalf of Department of Science and Technology (DST), Govt. of India through project
    no. GITA/DST/TWN/P-83/2019.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究由全球创新与技术联盟（GITA）资助，代表印度政府科学与技术部（DST），项目编号为 GITA/DST/TWN/P-83/2019。
- en: References
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani,
    J. Hafner, D. Lee, D. Petkovic *et al.*, “Query by image and video content: The
    qbic system,” *Computer*, vol. 28, no. 9, pp. 23–32, 1995.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani,
    J. Hafner, D. Lee, D. Petkovic *等*，“通过图像和视频内容查询：qbic系统，” *计算机*，第28卷，第9期，页码23–32，1995年。'
- en: '[2] A. W. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain, “Content-based
    image retrieval at the end of the early years,” *IEEE TPAMI*, vol. 22, no. 12,
    pp. 1349–1380, 2000.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. W. Smeulders, M. Worring, S. Santini, A. Gupta 和 R. Jain，"早期几年结束时的基于内容的图像检索"，*IEEE
    TPAMI*，第 22 卷，第 12 期，页码 1349–1380，2000 年。'
- en: '[3] H. Müller, W. Müller, D. M. Squire, S. Marchand-Maillet, and T. Pun, “Performance
    evaluation in content-based image retrieval: overview and proposals,” *Pattern
    Recog. Letters*, vol. 22, no. 5, pp. 593–601, 2001.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] H. Müller, W. Müller, D. M. Squire, S. Marchand-Maillet 和 T. Pun，"基于内容的图像检索性能评估：概述与建议"，*模式识别信函*，第
    22 卷，第 5 期，页码 593–601，2001 年。'
- en: '[4] T. Deselaers, D. Keysers, and H. Ney, “Features for image retrieval: an
    experimental comparison,” *Information Retrieval*, vol. 11, no. 2, pp. 77–107,
    2008.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. Deselaers, D. Keysers 和 H. Ney，"图像检索的特征：实验比较"，*信息检索*，第 11 卷，第 2 期，页码
    77–107，2008 年。'
- en: '[5] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    *IJCV*, vol. 60, no. 2, pp. 91–110, 2004.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. G. Lowe，"来自尺度不变关键点的独特图像特征"，*国际计算机视觉*，第 60 卷，第 2 期，页码 91–110，2004 年。'
- en: '[6] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale and
    rotation invariant texture classification with local binary patterns,” *IEEE TPAMI*,
    vol. 24, no. 7, pp. 971–987, 2002.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. Ojala, M. Pietikainen 和 T. Maenpaa，"利用局部二进制模式进行多分辨率灰度和旋转不变纹理分类"，*IEEE
    TPAMI*，第 24 卷，第 7 期，页码 971–987，2002 年。'
- en: '[7] S. R. Dubey, S. K. Singh, and R. K. Singh, “Rotation and illumination invariant
    interleaved intensity order-based local descriptor,” *IEEE TIP*, vol. 23, no. 12,
    pp. 5323–5333, 2014.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] S. R. Dubey, S. K. Singh 和 R. K. Singh，"旋转和光照不变的交错强度顺序局部描述符"，*IEEE TIP*，第
    23 卷，第 12 期，页码 5323–5333，2014 年。'
- en: '[8] I. J. Jacob, K. Srinivasagan, and K. Jayapriya, “Local oppugnant color
    texture pattern for image retrieval system,” *Pattern Recog. Letters*, vol. 42,
    pp. 72–78, 2014.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] I. J. Jacob, K. Srinivasagan 和 K. Jayapriya，"用于图像检索系统的局部对抗颜色纹理模式"，*模式识别信函*，第
    42 卷，页码 72–78，2014 年。'
- en: '[9] D. Song and D. Tao, “Biologically inspired feature manifold for scene classification,”
    *IEEE TIP*, vol. 19, no. 1, pp. 174–184, 2009.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] D. Song 和 D. Tao，"生物启发的特征流形用于场景分类"，*IEEE TIP*，第 19 卷，第 1 期，页码 174–184，2009
    年。'
- en: '[10] H. Jegou, F. Perronnin, M. Douze, J. Sánchez, P. Perez, and C. Schmid,
    “Aggregating local image descriptors into compact codes,” *IEEE TPAMI*, vol. 34,
    no. 9, pp. 1704–1716, 2011.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] H. Jegou, F. Perronnin, M. Douze, J. Sánchez, P. Perez 和 C. Schmid，"将局部图像描述符聚合成紧凑编码"，*IEEE
    TPAMI*，第 34 卷，第 9 期，页码 1704–1716，2011 年。'
- en: '[11] S. Murala, R. Maheshwari, and R. Balasubramanian, “Local tetra patterns:
    a new feature descriptor for content-based image retrieval,” *IEEE TIP*, vol. 21,
    no. 5, pp. 2874–2886, 2012.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Murala, R. Maheshwari 和 R. Balasubramanian，"局部四面体模式：用于基于内容的图像检索的新特征描述符"，*IEEE
    TIP*，第 21 卷，第 5 期，页码 2874–2886，2012 年。'
- en: '[12] S. R. Dubey, S. K. Singh, and R. K. Singh, “Local wavelet pattern: a new
    feature descriptor for image retrieval in medical ct databases,” *IEEE TIP*, vol. 24,
    no. 12, pp. 5892–5903, 2015.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. R. Dubey, S. K. Singh 和 R. K. Singh，"局部小波模式：用于医学 CT 数据库图像检索的新特征描述符"，*IEEE
    TIP*，第 24 卷，第 12 期，页码 5892–5903，2015 年。'
- en: '[13] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin, “Iterative quantization:
    A procrustean approach to learning binary codes for large-scale image retrieval,”
    *IEEE TPAMI*, vol. 35, no. 12, pp. 2916–2929, 2012.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Gong, S. Lazebnik, A. Gordo 和 F. Perronnin，"迭代量化：用于大规模图像检索的 Procrustes
    方法学习二进制编码"，*IEEE TPAMI*，第 35 卷，第 12 期，页码 2916–2929，2012 年。'
- en: '[14] K.-C. Fan and T.-Y. Hung, “A novel local pattern descriptor—local vector
    pattern in high-order derivative space for face recognition,” *IEEE TIP*, vol. 23,
    no. 7, pp. 2877–2891, 2014.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] K.-C. Fan 和 T.-Y. Hung，"一种新颖的局部模式描述符——高阶导数空间中的局部向量模式用于人脸识别"，*IEEE TIP*，第
    23 卷，第 7 期，页码 2877–2891，2014 年。'
- en: '[15] S. R. Dubey, S. K. Singh, and R. K. Singh, “Multichannel decoded local
    binary patterns for content-based image retrieval,” *IEEE TIP*, vol. 25, no. 9,
    pp. 4018–4032, 2016.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. R. Dubey, S. K. Singh 和 R. K. Singh，"多通道解码局部二进制模式用于基于内容的图像检索"，*IEEE
    TIP*，第 25 卷，第 9 期，页码 4018–4032，2016 年。'
- en: '[16] S. Chakraborty, S. K. Singh, and P. Chakraborty, “Local gradient hexa
    pattern: A descriptor for face recognition and retrieval,” *IEEE TCSVT*, vol. 28,
    no. 1, pp. 171–180, 2016.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Chakraborty, S. K. Singh 和 P. Chakraborty，"局部梯度六边形模式：用于人脸识别和检索的描述符"，*IEEE
    TCSVT*，第 28 卷，第 1 期，页码 171–180，2016 年。'
- en: '[17] R. Datta, D. Joshi, J. Li, and J. Z. Wang, “Image retrieval: Ideas, influences,
    and trends of the new age,” *ACM Computing Surveys*, vol. 40, no. 2, pp. 1–60,
    2008.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] R. Datta, D. Joshi, J. Li 和 J. Z. Wang，"图像检索：新时代的理念、影响与趋势"，*ACM 计算机调查*，第
    40 卷，第 2 期，页码 1–60，2008 年。'
- en: '[18] T. Mei, Y. Rui, S. Li, and Q. Tian, “Multimedia search reranking: A literature
    survey,” *ACM Computing Surveys*, vol. 46, no. 3, pp. 1–38, 2014.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] T. Mei, Y. Rui, S. Li, 和 Q. Tian，“多媒体搜索重排：文献综述，” *ACM计算机调查*，第46卷，第3期，第1–38页，2014年。'
- en: '[19] W. Zhou, H. Li, and Q. Tian, “Recent advance in content-based image retrieval:
    A literature survey,” *arXiv preprint arXiv:1706.06064*, 2017.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] W. Zhou, H. Li, 和 Q. Tian，“基于内容的图像检索的最新进展：文献综述，” *arXiv预印本 arXiv:1706.06064*，2017年。'
- en: '[20] A. Bellet, A. Habrard, and M. Sebban, “A survey on metric learning for
    feature vectors and structured data,” *arXiv preprint arXiv:1306.6709*, 2013.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Bellet, A. Habrard, 和 M. Sebban，“关于特征向量和结构化数据的度量学习综述，” *arXiv预印本 arXiv:1306.6709*，2013年。'
- en: '[21] J. Wang, T. Zhang, N. Sebe, H. T. Shen *et al.*, “A survey on learning
    to hash,” *IEEE TPAMI*, vol. 40, no. 4, pp. 769–790, 2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Wang, T. Zhang, N. Sebe, H. T. Shen *等*，“学习哈希的综述，” *IEEE TPAMI*，第40卷，第4期，第769–790页，2017年。'
- en: '[22] S. C. Hoi, W. Liu, M. R. Lyu, and W.-Y. Ma, “Learning distance metrics
    with contextual constraints for image retrieval,” in *CVPR*, vol. 2, 2006, pp.
    2072–2078.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] S. C. Hoi, W. Liu, M. R. Lyu, 和 W.-Y. Ma，“带上下文约束的距离度量学习用于图像检索，” 见于 *CVPR*，第2卷，2006年，第2072–2078页。'
- en: '[23] H. Chang and D.-Y. Yeung, “Kernel-based distance metric learning for content-based
    image retrieval,” *Image and Vision Computing*, vol. 25, no. 5, pp. 695–703, 2007.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] H. Chang 和 D.-Y. Yeung，“基于核的距离度量学习用于基于内容的图像检索，” *图像与视觉计算*，第25卷，第5期，第695–703页，2007年。'
- en: '[24] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang, “Supervised hashing
    with kernels,” in *IEEE CVPR*.   IEEE, 2012, pp. 2074–2081.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, 和 S.-F. Chang，“带核的监督哈希，” 见于 *IEEE
    CVPR*。 IEEE，2012，第2074–2081页。'
- en: '[25] L. Yang, R. Jin, L. Mummert, R. Sukthankar, A. Goode, B. Zheng, S. C.
    Hoi, and M. Satyanarayanan, “A boosting framework for visuality-preserving distance
    metric learning and its application to medical image retrieval,” *IEEE TPAMI*,
    vol. 32, no. 1, pp. 30–44, 2008.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] L. Yang, R. Jin, L. Mummert, R. Sukthankar, A. Goode, B. Zheng, S. C.
    Hoi, 和 M. Satyanarayanan，“一种用于视觉保留距离度量学习的提升框架及其在医学图像检索中的应用，” *IEEE TPAMI*，第32卷，第1期，第30–44页，2008年。'
- en: '[26] J.-E. Lee, R. Jin, and A. K. Jain, “Rank-based distance metric learning:
    An application to image retrieval,” in *CVPR*, 2008, pp. 1–8.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J.-E. Lee, R. Jin, 和 A. K. Jain，“基于排名的距离度量学习：应用于图像检索，” 见于 *CVPR*，2008年，第1–8页。'
- en: '[27] S. C. Hoi, W. Liu, and S.-F. Chang, “Semi-supervised distance metric learning
    for collaborative image retrieval and clustering,” *ACM TOMM*, vol. 6, no. 3,
    pp. 1–26, 2010.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. C. Hoi, W. Liu, 和 S.-F. Chang，“用于协作图像检索和聚类的半监督距离度量学习，” *ACM TOMM*，第6卷，第3期，第1–26页，2010年。'
- en: '[28] M. Norouzi, D. J. Fleet, and R. R. Salakhutdinov, “Hamming distance metric
    learning,” in *NIPS*, 2012, pp. 1061–1069.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] M. Norouzi, D. J. Fleet, 和 R. R. Salakhutdinov，“汉明距离度量学习，” 见于 *NIPS*，2012年，第1061–1069页。'
- en: '[29] D. Song, W. Liu, and D. A. Meyer, “Fast structural binary coding.” in
    *IJCAI*, 2016, pp. 2018–2024.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] D. Song, W. Liu, 和 D. A. Meyer，“快速结构化二进制编码。” 见于 *IJCAI*，2016，第2018–2024页。'
- en: '[30] D. Song, W. Liu, D. A. Meyer, D. Tao, and R. Ji, “Rank preserving hashing
    for rapid image search,” in *Data Compression Conference*.   IEEE, 2015, pp. 353–362.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] D. Song, W. Liu, D. A. Meyer, D. Tao, 和 R. Ji，“用于快速图像搜索的排名保留哈希，” 见于 *数据压缩会议*。
    IEEE，2015年，第353–362页。'
- en: '[31] D. Song, W. Liu, R. Ji, D. A. Meyer, and J. R. Smith, “Top rank supervised
    binary coding for visual search,” in *IEEE ICCV*, 2015, pp. 1922–1930.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. Song, W. Liu, R. Ji, D. A. Meyer, 和 J. R. Smith，“用于视觉搜索的顶级排名监督二进制编码，”
    见于 *IEEE ICCV*，2015年，第1922–1930页。'
- en: '[32] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “Cnn features
    off-the-shelf: an astounding baseline for recognition,” in *CVPR workshops*, 2014,
    pp. 806–813.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Sharif Razavian, H. Azizpour, J. Sullivan, 和 S. Carlsson，“现成的CNN特征：一个惊人的识别基线，”
    见于 *CVPR workshops*，2014年，第806–813页。'
- en: '[33] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE TPAMI*, 2020.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] L. Jing 和 Y. Tian，“基于深度神经网络的自监督视觉特征学习：综述，” *IEEE TPAMI*，2020年。'
- en: '[34] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Y. LeCun, Y. Bengio, 和 G. Hinton，“深度学习，” *自然*，第521卷，第7553期，第436–444页，2015年。'
- en: '[35] C. H. Dagli, *Artificial neural networks for intelligent manufacturing*.   Springer
    Science & Business Media, 2012.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] C. H. Dagli，*智能制造中的人工神经网络*。 Springer Science & Business Media，2012年。'
- en: '[36] F. Amato, A. López, E. M. Peña-Méndez, P. Vaňhara, A. Hampl, and J. Havel,
    “Artificial neural networks in medical diagnosis,” *J Appl Biomed*, vol. 11, pp.
    47–58, 2013.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] F. Amato, A. López, E. M. Peña-Méndez, P. Vaňhara, A. Hampl, 和 J. Havel，“医学诊断中的人工神经网络，”
    *J Appl Biomed*，第11卷，第47–58页，2013年。'
- en: '[37] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NIPS*, 2012, pp. 1097–1105.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行Imagenet分类，”在
    *NIPS*，2012年，页码1097–1105。'
- en: '[38] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] K. He, X. Zhang, S. Ren, 和 J. Sun, “用于图像识别的深度残差学习，”在 *CVPR*，2016年，页码770–778。'
- en: '[39] M. Sundermeyer, R. Schlüter, and H. Ney, “Lstm neural networks for language
    modeling,” in *Conf. of the Int. Speech Communication Association*, 2012.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. Sundermeyer, R. Schlüter, 和 H. Ney, “用于语言建模的LSTM神经网络，”在 *国际语音通信协会会议*，2012年。'
- en: '[40] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Gated feedback recurrent
    neural networks,” in *ICML*, 2015, pp. 2067–2075.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Chung, C. Gulcehre, K. Cho, 和 Y. Bengio, “门控反馈递归神经网络，”在 *ICML*，2015年，页码2067–2075。'
- en: '[41] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y. Zhang, and J. Li, “Deep
    learning for content-based image retrieval: A comprehensive study,” in *Proceedings
    of the 22nd ACMMM*, 2014, pp. 157–166.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y. Zhang, 和 J. Li, “基于内容的图像检索的深度学习：一项综合研究，”在
    *第22届ACMMM会议录*，2014年，页码157–166。'
- en: '[42] L. Zheng, Y. Yang, and Q. Tian, “Sift meets cnn: A decade survey of instance
    retrieval,” *IEEE TPAMI*, vol. 40, no. 5, pp. 1224–1244, 2017.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] L. Zheng, Y. Yang, 和 Q. Tian, “Sift遇上cnn：实例检索的十年综述，” *IEEE TPAMI*，第40卷，第5期，页码1224–1244，2017年。'
- en: '[43] J. Rodrigues, M. Cristo, and J. G. Colonna, “Deep hashing for multi-label
    image retrieval: a survey,” *Artificial Intel. Review*, pp. 1–47, 2020.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Rodrigues, M. Cristo, 和 J. G. Colonna, “多标签图像检索的深度哈希：一项综述，” *人工智能评论*，页码1–47，2020年。'
- en: '[44] X. Luo, C. Chen, H. Zhong, H. Zhang, M. Deng, J. Huang, and X. Hua, “A
    survey on deep hashing methods,” *arXiv preprint arXiv:2003.03369*, 2020.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] X. Luo, C. Chen, H. Zhong, H. Zhang, M. Deng, J. Huang, 和 X. Hua, “深度哈希方法的综述，”
    *arXiv预印本arXiv:2003.03369*，2020年。'
- en: '[45] A. Krizhevsky, “Learning multiple layers of features from tiny images,”
    *Master’s thesis, University of Tront*, 2009.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Krizhevsky, “从微小图像中学习多个层次的特征，” *硕士论文，特隆特大学*，2009年。'
- en: '[46] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng, “Nus-wide:
    a real-world web image database from national university of singapore,” in *ACM
    Int. Conf. on Image and Video Retrieval*, 2009, pp. 1–9.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, 和 Y. Zheng, “nus-wide：来自新加坡国立大学的真实世界网页图像数据库，”在
    *ACM国际图像和视频检索会议*，2009年，页码1–9。'
- en: '[47] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” *Proceedings of the IEEE*, vol. 86, no. 11,
    pp. 2278–2324, 1998.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Y. LeCun, L. Bottou, Y. Bengio, 和 P. Haffner, “基于梯度的学习应用于文档识别，” *IEEE会议录*，第86卷，第11期，页码2278–2324，1998年。'
- en: '[48] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading
    digits in natural images with unsupervised feature learning,” in *NIPS*, 2011.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, 和 A. Y. Ng, “通过无监督特征学习在自然图像中读取数字，”在
    *NIPS*，2011年。'
- en: '[49] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, “Sun database:
    Large-scale scene recognition from abbey to zoo,” in *CVPR*, 2010, pp. 3485–3492.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, 和 A. Torralba, “Sun数据库：从修道院到动物园的大规模场景识别，”在
    *CVPR*，2010年，页码3485–3492。'
- en: '[50] A. Yu and K. Grauman, “Fine-grained visual comparisons with local learning,”
    in *CVPR*, 2014, pp. 192–199.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] A. Yu 和 K. Grauman, “通过局部学习进行细粒度视觉比较，”在 *CVPR*，2014年，页码192–199。'
- en: '[51] K. Lin, H.-F. Yang, J.-H. Hsiao, and C.-S. Chen, “Deep learning of binary
    hash codes for fast image retrieval,” in *CVPR workshops*, 2015, pp. 27–35.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] K. Lin, H.-F. Yang, J.-H. Hsiao, 和 C.-S. Chen, “用于快速图像检索的二进制哈希码深度学习，”在
    *CVPR 研讨会*，2015年，页码27–35。'
- en: '[52] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein *et al.*, “Imagenet large scale visual recognition
    challenge,” *IJCV*, vol. 115, no. 3, pp. 211–252, 2015.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein *等*，“Imagenet大规模视觉识别挑战，” *IJCV*，第115卷，第3期，页码211–252，2015年。'
- en: '[53] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *ECCV*, 2014,
    pp. 740–755.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    和 C. L. Zitnick, “微软coco：背景中的常见对象，”在 *ECCV*，2014年，页码740–755。'
- en: '[54] M. J. Huiskes, B. Thomee, and M. S. Lew, “New trends and ideas in visual
    concept detection: the mir flickr retrieval evaluation initiative,” in *ICMIR*,
    2010, pp. 527–536.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] M. J. Huiskes, B. Thomee, 和 M. S. Lew, “视觉概念检测的新趋势和想法：mir flickr检索评估计划，”在
    *ICMIR*，2010年，页码527–536。'
- en: '[55] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han, “Large-scale image retrieval
    with attentive deep local features,” in *ICCV*, 2017, pp. 3456–3465.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] H. Noh, A. Araujo, J. Sim, T. Weyand 和 B. Han，“具有关注型深度局部特征的大规模图像检索，”发表于*ICCV*，2017年，第3456–3465页。'
- en: '[56] T. Weyand, A. Araujo, B. Cao, and J. Sim, “Google landmarks dataset v2-a
    large-scale benchmark for instance-level recognition and retrieval,” in *CVPR*,
    2020, pp. 2575–2584.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] T. Weyand, A. Araujo, B. Cao 和 J. Sim，“Google地标数据集v2—用于实例级识别和检索的大规模基准测试，”发表于*CVPR*，2020年，第2575–2584页。'
- en: '[57] X.-S. Hua, L. Yang, J. Wang, J. Wang, M. Ye, K. Wang, Y. Rui, and J. Li,
    “Clickage: Towards bridging semantic and intent gaps via mining click logs of
    search engines,” in *Proceedings of the 21st ACM international conference on Multimedia*,
    2013, pp. 243–252.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] X.-S. Hua, L. Yang, J. Wang, J. Wang, M. Ye, K. Wang, Y. Rui 和 J. Li，“Clickage：通过挖掘搜索引擎的点击日志缩小语义和意图差距，”发表于*第21届ACM国际多媒体会议论文集*，2013年，第243–252页。'
- en: '[58] A. Krizhevsky and G. E. Hinton, “Using very deep autoencoders for content-based
    image retrieval.” in *ESANN*, vol. 1, 2011, p. 2.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] A. Krizhevsky 和 G. E. Hinton，“使用非常深的自编码器进行基于内容的图像检索。”发表于*ESANN*，第1卷，2011年，第2页。'
- en: '[59] Y. Kang, S. Kim, and S. Choi, “Deep learning to hash with multiple representations,”
    in *ICDM*, 2012, pp. 930–935.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. Kang, S. Kim 和 S. Choi，“使用多重表示的深度学习哈希，”发表于*ICDM*，2012年，第930–935页。'
- en: '[60] P. Wu, S. C. Hoi, H. Xia, P. Zhao, D. Wang, and C. Miao, “Online multimodal
    deep similarity learning with application to image retrieval,” in *ACMMM*, 2013,
    pp. 153–162.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] P. Wu, S. C. Hoi, H. Xia, P. Zhao, D. Wang 和 C. Miao，“用于图像检索的在线多模态深度相似性学习，”发表于*ACMMM*，2013年，第153–162页。'
- en: '[61] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, “Neural codes
    for image retrieval,” in *ECCV*, 2014, pp. 584–599.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A. Babenko, A. Slesarev, A. Chigorin 和 V. Lempitsky，“图像检索的神经编码，”发表于*ECCV*，2014年，第584–599页。'
- en: '[62] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen,
    and Y. Wu, “Learning fine-grained image similarity with deep ranking,” in *CVPR*,
    2014, pp. 1386–1393.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen
    和 Y. Wu，“通过深度排序学习细粒度图像相似性，”发表于*CVPR*，2014年，第1386–1393页。'
- en: '[63] H. Lai, Y. Pan, Y. Liu, and S. Yan, “Simultaneous feature learning and
    hash coding with deep neural networks,” in *CVPR*, 2015, pp. 3270–3278.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] H. Lai, Y. Pan, Y. Liu 和 S. Yan，“利用深度神经网络进行特征学习和哈希编码，”发表于*CVPR*，2015年，第3270–3278页。'
- en: '[64] R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang, “Bit-scalable deep hashing
    with regularized similarity learning for image retrieval and person re-identification,”
    *IEEE TIP*, vol. 24, no. 12, pp. 4766–4779, 2015.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] R. Zhang, L. Lin, R. Zhang, W. Zuo 和 L. Zhang，“具有正则化相似性学习的位扩展深度哈希，用于图像检索和人物重识别，”*IEEE
    TIP*，第24卷，第12期，第4766–4779页，2015年。'
- en: '[65] A. Gordo, J. Almazán, J. Revaud, and D. Larlus, “Deep image retrieval:
    Learning global representations for image search,” in *ECCV*, 2016, pp. 241–257.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] A. Gordo, J. Almazán, J. Revaud 和 D. Larlus，“深度图像检索：学习图像搜索的全局表示，”发表于*ECCV*，2016年，第241–257页。'
- en: '[66] H. Oh Song, Y. Xiang, S. Jegelka, and S. Savarese, “Deep metric learning
    via lifted structured feature embedding,” in *CVPR*, 2016, pp. 4004–4012.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] H. Oh Song, Y. Xiang, S. Jegelka 和 S. Savarese，“通过提升结构化特征嵌入进行深度度量学习，”发表于*CVPR*，2016年，第4004–4012页。'
- en: '[67] H. Zhu, M. Long, J. Wang, and Y. Cao, “Deep hashing network for efficient
    similarity retrieval,” in *AAAI*, 2016.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] H. Zhu, M. Long, J. Wang 和 Y. Cao，“用于高效相似性检索的深度哈希网络，”发表于*AAAI*，2016年。'
- en: '[68] Y. Cao, M. Long, J. Wang, H. Zhu, and Q. Wen, “Deep quantization network
    for efficient image retrieval,” in *AAAI*, 2016.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Y. Cao, M. Long, J. Wang, H. Zhu 和 Q. Wen，“用于高效图像检索的深度量化网络，”发表于*AAAI*，2016年。'
- en: '[69] S. S. Husain and M. Bober, “Improving large-scale image retrieval through
    robust aggregation of local descriptors,” *IEEE TPAMI*, vol. 39, no. 9, pp. 1783–1796,
    2016.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] S. S. Husain 和 M. Bober，“通过鲁棒聚合局部描述符改善大规模图像检索，”*IEEE TPAMI*，第39卷，第9期，第1783–1796页，2016年。'
- en: '[70] G. Zhong, H. Xu, P. Yang, S. Wang, and J. Dong, “Deep hashing learning
    networks,” in *IJCNN*, 2016, pp. 2236–2243.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] G. Zhong, H. Xu, P. Yang, S. Wang 和 J. Dong，“深度哈希学习网络，”发表于*IJCNN*，2016年，第2236–2243页。'
- en: '[71] Z. Cao, M. Long, J. Wang, and P. S. Yu, “Hashnet: Deep learning to hash
    by continuation,” in *ICCV*, 2017, pp. 5608–5617.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Z. Cao, M. Long, J. Wang 和 P. S. Yu，“Hashnet：通过继续学习进行深度哈希，”发表于*ICCV*，2017年，第5608–5617页。'
- en: '[72] A. Gordo, J. Almazan, J. Revaud, and D. Larlus, “End-to-end learning of
    deep visual representations for image retrieval,” *IJCV*, vol. 124, no. 2, pp.
    237–254, 2017.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Gordo, J. Almazan, J. Revaud 和 D. Larlus，“端到端学习深度视觉表示用于图像检索，”*IJCV*，第124卷，第2期，第237–254页，2017年。'
- en: '[73] T. Hoang, T.-T. Do, D.-K. Le Tan, and N.-M. Cheung, “Selective deep convolutional
    features for image retrieval,” in *ACMMM*, 2017, pp. 1600–1608.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] T. Hoang, T.-T. Do, D.-K. Le Tan 和 N.-M. Cheung，“用于图像检索的选择性深度卷积特征，”发表于*ACMMM*，2017年，第1600–1608页。'
- en: '[74] A. Alzu’bi, A. Amira, and N. Ramzan, “Content-based image retrieval with
    compact deep convolutional features,” *Neurocomputing*, vol. 249, pp. 95–105,
    2017.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] A. Alzu’bi, A. Amira, 和 N. Ramzan，“基于内容的图像检索与紧凑深度卷积特征，” *Neurocomputing*，第
    249 卷，第 95–105 页，2017 年。'
- en: '[75] Y. Cao, M. Long, B. Liu, and J. Wang, “Deep cauchy hashing for hamming
    space retrieval,” in *CVPR*, 2018, pp. 1229–1237.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. Cao, M. Long, B. Liu, 和 J. Wang，“用于哈明空间检索的深度 Cauchy 哈希，” 在 *CVPR*，2018
    年，第 1229–1237 页。'
- en: '[76] S. Su, C. Zhang, K. Han, and Y. Tian, “Greedy hash: Towards fast optimization
    for accurate hash coding in cnn,” in *NIPS*, 2018, pp. 798–807.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S. Su, C. Zhang, K. Han, 和 Y. Tian，“贪婪哈希：为了在 CNN 中准确的哈希编码进行快速优化，” 在 *NIPS*，2018
    年，第 798–807 页。'
- en: '[77] X. Yuan, L. Ren, J. Lu, and J. Zhou, “Relaxation-free deep hashing via
    policy gradient,” in *ECCV*, 2018, pp. 134–150.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] X. Yuan, L. Ren, J. Lu, 和 J. Zhou，“通过策略梯度进行无松弛深度哈希，” 在 *ECCV*，2018 年，第
    134–150 页。'
- en: '[78] Z. Chen, X. Yuan, J. Lu, Q. Tian, and J. Zhou, “Deep hashing via discrepancy
    minimization,” in *CVPR*, 2018, pp. 6838–6847.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Z. Chen, X. Yuan, J. Lu, Q. Tian, 和 J. Zhou，“通过差异最小化进行深度哈希，” 在 *CVPR*，2018
    年，第 6838–6847 页。'
- en: '[79] D. Wu, J. Liu, B. Li, and W. Wang, “Deep index-compatible hashing for
    fast image retrieval,” in *ICME*, 2018, pp. 1–6.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] D. Wu, J. Liu, B. Li, 和 W. Wang，“用于快速图像检索的深度索引兼容哈希，” 在 *ICME*，2018 年，第
    1–6 页。'
- en: '[80] D. Wu, Q. Dai, J. Liu, B. Li, and W. Wang, “Deep incremental hashing network
    for efficient image retrieval,” in *CVPR*, 2019, pp. 9069–9077.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] D. Wu, Q. Dai, J. Liu, B. Li, 和 W. Wang，“用于高效图像检索的深度增量哈希网络，” 在 *CVPR*，2019
    年，第 9069–9077 页。'
- en: '[81] S. Eghbali and L. Tahvildari, “Deep spherical quantization for image search,”
    in *CVPR*, 2019, pp. 11 690–11 699.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] S. Eghbali 和 L. Tahvildari，“用于图像搜索的深度球形量化，” 在 *CVPR*，2019 年，第 11 690–11 699
    页。'
- en: '[82] E. Yang, T. Liu, C. Deng, W. Liu, and D. Tao, “Distillhash: Unsupervised
    deep hashing by distilling data pairs,” in *CVPR*, 2019, pp. 2946–2955.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] E. Yang, T. Liu, C. Deng, W. Liu, 和 D. Tao，“Distillhash：通过提炼数据对进行无监督深度哈希，”
    在 *CVPR*，2019 年，第 2946–2955 页。'
- en: '[83] J. Bai, B. Ni, M. Wang, Z. Li, S. Cheng, X. Yang, C. Hu, and W. Gao, “Deep
    progressive hashing for image retrieval,” *IEEE TMM*, vol. 21, no. 12, pp. 3178–3193,
    2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. Bai, B. Ni, M. Wang, Z. Li, S. Cheng, X. Yang, C. Hu, 和 W. Gao，“深度渐进哈希用于图像检索，”
    *IEEE TMM*，第 21 卷，第 12 期，第 3178–3193 页，2019 年。'
- en: '[84] J. Xu, C. Guo, Q. Liu, J. Qin, Y. Wang, and L. Liu, “Dha: Supervised deep
    learning to hash with an adaptive loss function,” in *ICCV Workshops*, 2019.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] J. Xu, C. Guo, Q. Liu, J. Qin, Y. Wang, 和 L. Liu，“DHA：使用自适应损失函数的监督深度学习哈希，”
    在 *ICCV 工作坊*，2019 年。'
- en: '[85] Y. Shen, J. Qin, J. Chen, L. Liu, F. Zhu, and Z. Shen, “Embarrassingly
    simple binary representation learning,” in *ICCV Workshops*, 2019.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Y. Shen, J. Qin, J. Chen, L. Liu, F. Zhu, 和 Z. Shen，“令人尴尬的简单二进制表示学习，”
    在 *ICCV 工作坊*，2019 年。'
- en: '[86] Y. Shen, L. Liu, and L. Shao, “Unsupervised binary representation learning
    with deep variational networks,” *IJCV*, vol. 127, no. 11-12, pp. 1614–1628, 2019.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Y. Shen, L. Liu, 和 L. Shao，“基于深度变分网络的无监督二进制表示学习，” *IJCV*，第 127 卷，第 11-12
    期，第 1614–1628 页，2019 年。'
- en: '[87] Y. Shen, J. Qin, J. Chen, M. Yu, L. Liu, F. Zhu, F. Shen, and L. Shao,
    “Auto-encoding twin-bottleneck hashing,” in *CVPR*, 2020, pp. 2818–2827.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Y. Shen, J. Qin, J. Chen, M. Yu, L. Liu, F. Zhu, F. Shen, 和 L. Shao，“自编码双瓶颈哈希，”
    在 *CVPR*，2020 年，第 2818–2827 页。'
- en: '[88] J. I. Forcen, M. Pagola, E. Barrenechea, and H. Bustince, “Co-occurrence
    of deep convolutional features for image search,” *Image and Vision Computing*,
    p. 103909, 2020.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] J. I. Forcen, M. Pagola, E. Barrenechea, 和 H. Bustince，“深度卷积特征的共现用于图像搜索，”
    *图像与视觉计算*，第 103909 页，2020 年。'
- en: '[89] R. Wang, R. Wang, S. Qiao, S. Shan, and X. Chen, “Deep position-aware
    hashing for semantic continuous image retrieval,” in *WACV*, 2020, pp. 2493–2502.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] R. Wang, R. Wang, S. Qiao, S. Shan, 和 X. Chen，“用于语义连续图像检索的深度位置感知哈希，” 在
    *WACV*，2020 年，第 2493–2502 页。'
- en: '[90] M. J. Huiskes and M. S. Lew, “The mir flickr retrieval evaluation,” in
    *ACMMM Information Retrieval*, 2008, pp. 39–43.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. J. Huiskes 和 M. S. Lew，“Mir Flickr 检索评估，” 在 *ACMMM 信息检索*，2008 年，第 39–43
    页。'
- en: '[91] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, and A. Oliva, “Sun database:
    Exploring a large collection of scene categories,” *IJCV*, vol. 119, no. 1, pp.
    3–22, 2016.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, 和 A. Oliva，“Sun 数据库：探索大规模场景类别集合，”
    *IJCV*，第 119 卷，第 1 期，第 3–22 页，2016 年。'
- en: '[92] Y. Pan, T. Yao, T. Mei, H. Li, C.-W. Ngo, and Y. Rui, “Click-through-based
    cross-view learning for image search,” in *Proceedings of the 37th international
    ACM SIGIR conference on Research & development in information retrieval*, 2014,
    pp. 717–726.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Y. Pan, T. Yao, T. Mei, H. Li, C.-W. Ngo, 和 Y. Rui，“基于点击率的跨视图学习用于图像搜索，”
    在 *第 37 届国际 ACM SIGIR 信息检索研究与开发会议论文集*，2014 年，第 717–726 页。'
- en: '[93] J. Yu, X. Yang, F. Gao, and D. Tao, “Deep multimodal distance metric learning
    using click constraints for image ranking,” *IEEE Transactions on Cybernetics*,
    vol. 47, no. 12, pp. 4014–4024, 2016.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Yu, X. Yang, F. Gao, 和 D. Tao, “使用点击约束的深度多模态距离度量学习用于图像排序”，*IEEE Transactions
    on Cybernetics*, 第47卷，第12期，第4014–4024页，2016年。'
- en: '[94] Y. Li, Y. Pan, T. Yao, H. Chao, Y. Rui, and T. Mei, “Learning click-based
    deep structure-preserving embeddings with visual attention,” *ACM Transactions
    on Multimedia Computing, Communications, and Applications (TOMM)*, vol. 15, no. 3,
    pp. 1–19, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. Li, Y. Pan, T. Yao, H. Chao, Y. Rui, 和 T. Mei, “基于点击的深度结构保留嵌入学习与视觉注意”，*ACM
    Transactions on Multimedia Computing, Communications, and Applications (TOMM)*,
    第15卷，第3期，第1–19页，2019年。'
- en: '[95] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan, “Supervised hashing for image
    retrieval via image representation learning,” in *AAAI*, 2014.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] R. Xia, Y. Pan, H. Lai, C. Liu, 和 S. Yan, “通过图像表示学习的图像检索监督哈希”，在*AAAI*，2014年。'
- en: '[96] F. Shen, C. Shen, W. Liu, and H. Tao Shen, “Supervised discrete hashing,”
    in *CVPR*, 2015, pp. 37–45.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] F. Shen, C. Shen, W. Liu, 和 H. Tao Shen, “监督离散哈希”，在*CVPR*，2015年，第37–45页。'
- en: '[97] H. Liu, R. Wang, S. Shan, and X. Chen, “Deep supervised hashing for fast
    image retrieval,” in *CVPR*, 2016, pp. 2064–2072.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] H. Liu, R. Wang, S. Shan, 和 X. Chen, “用于快速图像检索的深度监督哈希”，在*CVPR*，2016年，第2064–2072页。'
- en: '[98] W.-J. Li, S. Wang, and W.-C. Kang, “Feature learning based deep supervised
    hashing with pairwise labels,” in *IJCAI*, 2016, pp. 1711–1717.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] W.-J. Li, S. Wang, 和 W.-C. Kang, “基于特征学习的深度监督哈希与成对标签”，在*IJCAI*，2016年，第1711–1717页。'
- en: '[99] X. Wang, Y. Shi, and K. M. Kitani, “Deep supervised hashing with triplet
    labels,” in *ACCV*, 2016, pp. 70–84.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] X. Wang, Y. Shi, 和 K. M. Kitani, “具有三元组标签的深度监督哈希”，在*ACCV*，2016年，第70–84页。'
- en: '[100] Z. Zhang, Y. Chen, and V. Saligrama, “Efficient training of very deep
    neural networks for supervised hashing,” in *CVPR*, 2016, pp. 1487–1495.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Z. Zhang, Y. Chen, 和 V. Saligrama, “针对监督哈希的非常深层神经网络的高效训练”，在*CVPR*，2016年，第1487–1495页。'
- en: '[101] Q. Li, Z. Sun, R. He, and T. Tan, “Deep supervised discrete hashing,”
    in *NIPS*, 2017, pp. 2482–2491.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Q. Li, Z. Sun, R. He, 和 T. Tan, “深度监督离散哈希”，在*NIPS*，2017年，第2482–2491页。'
- en: '[102] H.-F. Yang, K. Lin, and C.-S. Chen, “Supervised learning of semantics-preserving
    hash via deep convolutional neural networks,” *IEEE TPAMI*, vol. 40, no. 2, pp.
    437–451, 2017.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] H.-F. Yang, K. Lin, 和 C.-S. Chen, “通过深度卷积神经网络监督学习语义保留哈希”，*IEEE TPAMI*,
    第40卷，第2期，第437–451页，2017年。'
- en: '[103] J. Lu, V. E. Liong, and J. Zhou, “Deep hashing for scalable image search,”
    *IEEE TIP*, vol. 26, no. 5, pp. 2352–2367, 2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. Lu, V. E. Liong, 和 J. Zhou, “用于可扩展图像搜索的深度哈希”，*IEEE TIP*, 第26卷，第5期，第2352–2367页，2017年。'
- en: '[104] Q.-Y. Jiang and W.-J. Li, “Asymmetric deep supervised hashing,” in *AAAI*,
    2018.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Q.-Y. Jiang 和 W.-J. Li, “非对称深度监督哈希”，在*AAAI*，2018年。'
- en: '[105] B. Klein and L. Wolf, “End-to-end supervised product quantization for
    image search and retrieval,” in *CVPR*, 2019, pp. 5041–5050.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] B. Klein 和 L. Wolf, “用于图像搜索和检索的端到端监督产品量化”，在*CVPR*，2019年，第5041–5050页。'
- en: '[106] S. Kan, Y. Cen, Z. He, Z. Zhang, L. Zhang, and Y. Wang, “Supervised deep
    feature embedding with handcrafted feature,” *IEEE TIP*, vol. 28, no. 12, pp.
    5809–5823, 2019.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] S. Kan, Y. Cen, Z. He, Z. Zhang, L. Zhang, 和 Y. Wang, “具有手工特征的监督深度特征嵌入”，*IEEE
    TIP*, 第28卷，第12期，第5809–5823页，2019年。'
- en: '[107] W. W. Ng, J. Li, X. Tian, H. Wang, S. Kwong, and J. Wallace, “Multi-level
    supervised hashing with deep features for efficient image retrieval,” *Neurocomputing*,
    2020.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] W. W. Ng, J. Li, X. Tian, H. Wang, S. Kwong, 和 J. Wallace, “具有深度特征的多层次监督哈希用于高效图像检索”，*Neurocomputing*,
    2020年。'
- en: '[108] C. Zhou, L.-M. Po, W. Y. Yuen, K. W. Cheung, X. Xu, K. W. Lau, Y. Zhao,
    M. Liu, and P. H. Wong, “Angular deep supervised hashing for image retrieval,”
    *IEEE Access*, vol. 7, pp. 127 521–127 532, 2019.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] C. Zhou, L.-M. Po, W. Y. Yuen, K. W. Cheung, X. Xu, K. W. Lau, Y. Zhao,
    M. Liu, 和 P. H. Wong, “用于图像检索的角度深度监督哈希”，*IEEE Access*, 第7卷，第127 521–127 532页，2019年。'
- en: '[109] J. Li, W. W. Ng, X. Tian, S. Kwong, and H. Wang, “Weighted multi-deep
    ranking supervised hashing for efficient image retrieval,” *Int. Journal of Machine
    Learning and Cybernetics*, pp. 1–15, 2019.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] J. Li, W. W. Ng, X. Tian, S. Kwong, 和 H. Wang, “加权多深层排序监督哈希用于高效图像检索”，*Int.
    Journal of Machine Learning and Cybernetics*, 第1–15页，2019年。'
- en: '[110] V. Erin Liong, J. Lu, G. Wang, P. Moulin, and J. Zhou, “Deep hashing
    for compact binary codes learning,” in *CVPR*, 2015, pp. 2475–2483.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] V. Erin Liong, J. Lu, G. Wang, P. Moulin, 和 J. Zhou, “紧凑二进制码学习的深度哈希”，在*CVPR*，2015年，第2475–2483页。'
- en: '[111] C. Huang, C. Change Loy, and X. Tang, “Unsupervised learning of discriminative
    attributes and visual representations,” in *CVPR*, 2016, pp. 5175–5184.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] C. Huang, C. Change Loy, 和 X. Tang, “无监督学习区分属性和视觉表示”，在*CVPR*，2016年，第5175–5184页。'
- en: '[112] K. Lin, J. Lu, C.-S. Chen, and J. Zhou, “Learning compact binary descriptors
    with unsupervised deep neural networks,” in *CVPR*, 2016, pp. 1183–1192.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] K. Lin, J. Lu, C.-S. Chen, 和 J. Zhou，“通过无监督深度神经网络学习紧凑的二进制描述符，” 见 *CVPR*，2016年，第1183–1192页。'
- en: '[113] K. Lin, J. Lu, C.-S. Chen, J. Zhou, and M.-T. Sun, “Unsupervised deep
    learning of compact binary descriptors,” *IEEE TPAMI*, vol. 41, no. 6, pp. 1501–1514,
    2018.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] K. Lin, J. Lu, C.-S. Chen, J. Zhou, 和 M.-T. Sun，“无监督深度学习紧凑的二进制描述符，” *IEEE
    TPAMI*，第41卷，第6期，第1501–1514页，2018年。'
- en: '[114] Y. Duan, J. Lu, Z. Wang, J. Feng, and J. Zhou, “Learning deep binary
    descriptor with multi-quantization,” in *CVPR*, 2017, pp. 1183–1192.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Y. Duan, J. Lu, Z. Wang, J. Feng, 和 J. Zhou，“带有多量化的深度二进制描述符学习，” 见 *CVPR*，2017年，第1183–1192页。'
- en: '[115] F. Radenović, G. Tolias, and O. Chum, “Cnn image retrieval learns from
    bow: Unsupervised fine-tuning with hard examples,” in *ECCV*, 2016, pp. 3–20.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] F. Radenović, G. Tolias, 和 O. Chum，“CNN图像检索从词袋学习：使用困难样本的无监督微调，” 见 *ECCV*，2016年，第3–20页。'
- en: '[116] M. Paulin, J. Mairal, M. Douze, Z. Harchaoui, F. Perronnin, and C. Schmid,
    “Convolutional patch representations for image retrieval: an unsupervised approach,”
    *IJCV*, vol. 121, no. 1, pp. 149–168, 2017.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] M. Paulin, J. Mairal, M. Douze, Z. Harchaoui, F. Perronnin, 和 C. Schmid，“用于图像检索的卷积补丁表示：一种无监督的方法，”
    *IJCV*，第121卷，第1期，第149–168页，2017年。'
- en: '[117] S. Huang, Y. Xiong, Y. Zhang, and J. Wang, “Unsupervised triplet hashing
    for fast image retrieval,” in *Thematic Workshops of ACM Multimedia*, 2017, pp.
    84–92.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. Huang, Y. Xiong, Y. Zhang, 和 J. Wang，“无监督三元组哈希用于快速图像检索，” 见 *Thematic
    Workshops of ACM Multimedia*，2017年，第84–92页。'
- en: '[118] F. Shen, Y. Xu, L. Liu, Y. Yang, Z. Huang, and H. T. Shen, “Unsupervised
    deep hashing with similarity-adaptive and discrete optimization,” *IEEE TPAMI*,
    vol. 40, no. 12, pp. 3034–3044, 2018.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] F. Shen, Y. Xu, L. Liu, Y. Yang, Z. Huang, 和 H. T. Shen，“带有相似性自适应和离散优化的无监督深度哈希，”
    *IEEE TPAMI*，第40卷，第12期，第3034–3044页，2018年。'
- en: '[119] J. Xu, C. Shi, C. Qi, C. Wang, and B. Xiao, “Unsupervised part-based
    weighting aggregation of deep convolutional features for image retrieval,” in
    *AAAI*, 2018.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] J. Xu, C. Shi, C. Qi, C. Wang, 和 B. Xiao，“用于图像检索的无监督部分基于加权的深度卷积特征聚合，”
    见 *AAAI*，2018年。'
- en: '[120] K. Ghasedi Dizaji, F. Zheng, N. Sadoughi, Y. Yang, C. Deng, and H. Huang,
    “Unsupervised deep generative adversarial hashing network,” in *CVPR*, 2018, pp.
    3664–3673.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] K. Ghasedi Dizaji, F. Zheng, N. Sadoughi, Y. Yang, C. Deng, 和 H. Huang，“无监督深度生成对抗哈希网络，”
    见 *CVPR*，2018年，第3664–3673页。'
- en: '[121] J. Song, T. He, L. Gao, X. Xu, A. Hanjalic, and H. T. Shen, “Binary generative
    adversarial networks for image retrieval,” in *AAAI*, 2018.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] J. Song, T. He, L. Gao, X. Xu, A. Hanjalic, 和 H. T. Shen，“用于图像检索的二进制生成对抗网络，”
    见 *AAAI*，2018年。'
- en: '[122] C. Deng, E. Yang, T. Liu, J. Li, W. Liu, and D. Tao, “Unsupervised semantic-preserving
    adversarial hashing for image search,” *IEEE TIP*, vol. 28, no. 8, pp. 4032–4044,
    2019.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] C. Deng, E. Yang, T. Liu, J. Li, W. Liu, 和 D. Tao，“无监督语义保留对抗哈希用于图像搜索，”
    *IEEE TIP*，第28卷，第8期，第4032–4044页，2019年。'
- en: '[123] Y. Gu, H. Zhang, Z. Zhang, and Q. Ye, “Unsupervised deep triplet hashing
    with pseudo triplets for scalable image retrieval,” *Multimedia Tools and Applications*,
    pp. 1–22, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Y. Gu, H. Zhang, Z. Zhang, 和 Q. Ye，“带有伪三元组的无监督深度三元组哈希，用于可扩展图像检索，” *Multimedia
    Tools and Applications*，第1–22页，2019年。'
- en: '[124] Y. Liu, L. Ding, C. Chen, and Y. Liu, “Similarity-based unsupervised
    deep transfer learning for remote sensing image retrieval,” *IEEE TGRS*, 2020.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Y. Liu, L. Ding, C. Chen, 和 Y. Liu，“基于相似性的无监督深度迁移学习用于遥感图像检索，” *IEEE TGRS*，2020年。'
- en: '[125] F. Feng, X. Wang, and R. Li, “Cross-modal retrieval with correspondence
    autoencoder,” in *ACMMM*, 2014, pp. 7–16.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] F. Feng, X. Wang, 和 R. Li，“使用对应自编码器的跨模态检索，” 见 *ACMMM*，2014年，第7–16页。'
- en: '[126] S. Sun, W. Zhou, H. Li, and Q. Tian, “Search by detection: Object-level
    feature for image retrieval,” in *Int. conf. on Internet Multimedia Computing
    and Service*, 2014, pp. 46–49.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] S. Sun, W. Zhou, H. Li, 和 Q. Tian，“通过检测进行搜索：图像检索的对象级特征，” 见 *Int. conf.
    on Internet Multimedia Computing and Service*，2014年，第46–49页。'
- en: '[127] M. A. Carreira-Perpinán and R. Raziperchikolaei, “Hashing with binary
    autoencoders,” in *CVPR*, 2015, pp. 557–566.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] M. A. Carreira-Perpinán 和 R. Raziperchikolaei，“带有二进制自编码器的哈希，” 见 *CVPR*，2015年，第557–566页。'
- en: '[128] S. Zagoruyko and N. Komodakis, “Learning to compare image patches via
    convolutional neural networks,” in *CVPR*, 2015, pp. 4353–4361.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] S. Zagoruyko 和 N. Komodakis，“通过卷积神经网络学习比较图像补丁，” 见 *CVPR*，2015年，第4353–4361页。'
- en: '[129] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg, “Matchnet: Unifying
    feature and metric learning for patch-based matching,” in *CVPR*, 2015, pp. 3279–3286.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] X. Han, T. Leung, Y. Jia, R. Sukthankar, 和 A. C. Berg，“Matchnet：统一的特征和度量学习用于补丁匹配，”
    见 *CVPR*，2015年，第3279–3286页。'
- en: '[130] B. Zhuang, G. Lin, C. Shen, and I. Reid, “Fast training of triplet-based
    deep binary embedding networks,” in *CVPR*, 2016, pp. 5955–5964.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] B. Zhuang, G. Lin, C. Shen, 和 I. Reid，“基于三元组的深度二进制嵌入网络的快速训练，” 见 *CVPR*，2016
    年，第 5955–5964 页。'
- en: '[131] T. Yao, F. Long, T. Mei, and Y. Rui, “Deep semantic-preserving and ranking-based
    hashing for image retrieval,” in *IJCAI*, vol. 1, 2016, p. 4.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] T. Yao, F. Long, T. Mei, 和 Y. Rui，“深度语义保留和基于排名的哈希用于图像检索，” 见 *IJCAI*，第
    1 卷，2016 年，第 4 页。'
- en: '[132] V. Kumar BG, G. Carneiro, and I. Reid, “Learning local image descriptors
    with deep siamese and triplet convolutional networks by minimising global loss
    functions,” in *CVPR*, 2016, pp. 5385–5394.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] V. Kumar BG, G. Carneiro, 和 I. Reid，“通过最小化全局损失函数学习局部图像描述符，使用深度孪生和三元组卷积网络，”
    见 *CVPR*，2016 年，第 5385–5394 页。'
- en: '[133] J. Lin, O. Morere, J. Petta, V. Chandrasekhar, and A. Veillard, “Tiny
    descriptors for image retrieval with unsupervised triplet hashing,” in *DCC*,
    2016, pp. 397–406.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. Lin, O. Morere, J. Petta, V. Chandrasekhar, 和 A. Veillard，“用于图像检索的无监督三元组哈希的小型描述符，”
    见 *DCC*，2016 年，第 397–406 页。'
- en: '[134] Y. Wei, Y. Zhao, C. Lu, S. Wei, L. Liu, Z. Zhu, and S. Yan, “Cross-modal
    retrieval with cnn visual features: A new baseline,” *IEEE Transactions on Cybernetics*,
    vol. 47, no. 2, pp. 449–460, 2016.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Y. Wei, Y. Zhao, C. Lu, S. Wei, L. Liu, Z. Zhu, 和 S. Yan，“使用 CNN 视觉特征的跨模态检索：一个新的基准，”
    *IEEE Transactions on Cybernetics*，第 47 卷，第 2 期，第 449–460 页，2016 年。'
- en: '[135] Y. Zhou, S. Huang, Y. Zhang, and Y. Wang, “Deep hashing with triplet
    quantization loss,” in *VCIP*, 2017, pp. 1–4.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Y. Zhou, S. Huang, Y. Zhang, 和 Y. Wang，“具有三元组量化损失的深度哈希，” 见 *VCIP*，2017
    年，第 1–4 页。'
- en: '[136] E.-J. Ong, S. Husain, and M. Bober, “Siamese network of deep fisher-vector
    descriptors for image retrieval,” *arXiv preprint arXiv:1702.00338*, 2017.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] E.-J. Ong, S. Husain, 和 M. Bober，“用于图像检索的深度 Fisher 向量描述符的孪生网络，” *arXiv
    preprint arXiv:1702.00338*，2017 年。'
- en: '[137] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Deep spatial-semantic
    attention for fine-grained sketch-based image retrieval,” in *ICCV*, 2017, pp.
    5551–5560.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“用于细粒度草图基图像检索的深度空间-语义注意力，”
    见 *ICCV*，2017 年，第 5551–5560 页。'
- en: '[138] F. Yang, J. Li, S. Wei, Q. Zheng, T. Liu, and Y. Zhao, “Two-stream attentive
    cnns for image retrieval,” in *ACMMM*, 2017, pp. 1513–1521.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] F. Yang, J. Li, S. Wei, Q. Zheng, T. Liu, 和 Y. Zhao，“用于图像检索的双流注意力 CNNs，”
    见 *ACMMM*，2017 年，第 1513–1521 页。'
- en: '[139] Y. Shen, L. Liu, L. Shao, and J. Song, “Deep binaries: Encoding semantic-rich
    cues for efficient textual-visual cross retrieval,” in *ICCV*, 2017, pp. 4097–4106.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Y. Shen, L. Liu, L. Shao, 和 J. Song，“深度二进制：编码丰富语义线索以实现高效的文本-视觉跨检索，” 见
    *ICCV*，2017 年，第 4097–4106 页。'
- en: '[140] X. Lu, Y. Chen, and X. Li, “Hierarchical recurrent neural hashing for
    image retrieval with hierarchical convolutional features,” *IEEE TIP*, vol. 27,
    no. 1, pp. 106–120, 2017.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] X. Lu, Y. Chen, 和 X. Li，“用于图像检索的层次递归神经哈希与层次卷积特征，” *IEEE TIP*，第 27 卷，第
    1 期，第 106–120 页，2017 年。'
- en: '[141] J. Zhang and Y. Peng, “Ssdh: semi-supervised deep hashing for large scale
    image retrieval,” *IEEE TCSVT*, vol. 29, no. 1, pp. 212–225, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] J. Zhang 和 Y. Peng，“SSDH：用于大规模图像检索的半监督深度哈希，” *IEEE TCSVT*，第 29 卷，第 1
    期，第 212–225 页，2017 年。'
- en: '[142] E. Yang, C. Deng, W. Liu, X. Liu, D. Tao, and X. Gao, “Pairwise relationship
    guided deep hashing for cross-modal retrieval,” in *AAAI*, 2017.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] E. Yang, C. Deng, W. Liu, X. Liu, D. Tao, 和 X. Gao，“基于对的关系引导的深度哈希用于跨模态检索，”
    见 *AAAI*，2017 年。'
- en: '[143] B. Wang, Y. Yang, X. Xu, A. Hanjalic, and H. T. Shen, “Adversarial cross-modal
    retrieval,” in *ACMMM*, 2017, pp. 154–162.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] B. Wang, Y. Yang, X. Xu, A. Hanjalic, 和 H. T. Shen，“对抗性跨模态检索，” 见 *ACMMM*，2017
    年，第 154–162 页。'
- en: '[144] A. Jose, S. Yan, and I. Heisterklaus, “Binary hashing using siamese neural
    networks,” in *ICIP*, 2017, pp. 2916–2920.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] A. Jose, S. Yan, 和 I. Heisterklaus，“使用孪生神经网络的二进制哈希，” 见 *ICIP*，2017 年，第
    2916–2920 页。'
- en: '[145] X. Zhang, H. Lai, and J. Feng, “Attention-aware deep adversarial hashing
    for cross-modal retrieval,” in *ECCV*, 2018, pp. 591–606.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] X. Zhang, H. Lai, 和 J. Feng，“关注度感知的深度对抗性哈希用于跨模态检索，” 见 *ECCV*，2018 年，第
    591–606 页。'
- en: '[146] C. Li, C. Deng, N. Li, W. Liu, X. Gao, and D. Tao, “Self-supervised adversarial
    hashing networks for cross-modal retrieval,” in *CVPR*, 2018, pp. 4242–4251.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] C. Li, C. Deng, N. Li, W. Liu, X. Gao, 和 D. Tao，“自监督对抗性哈希网络用于跨模态检索，”
    见 *CVPR*，2018 年，第 4242–4251 页。'
- en: '[147] G. Wang, Q. Hu, J. Cheng, and Z. Hou, “Semi-supervised generative adversarial
    hashing for image retrieval,” in *ECCV*, 2018, pp. 469–485.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] G. Wang, Q. Hu, J. Cheng, 和 Z. Hou，“用于图像检索的半监督生成对抗哈希，” 见 *ECCV*，2018
    年，第 469–485 页。'
- en: '[148] J. Zhang, F. Shen, L. Liu, F. Zhu, M. Yu, L. Shao, H. Tao Shen, and L. Van Gool,
    “Generative domain-migration hashing for sketch-to-image retrieval,” in *ECCV*,
    2018, pp. 297–314.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] J. Zhang, F. Shen, L. Liu, F. Zhu, M. Yu, L. Shao, H. Tao Shen, 和 L.
    Van Gool，“用于草图到图像检索的生成领域迁移哈希，” 见 *ECCV*，2018 年，第 297–314 页。'
- en: '[149] Y. Cao, B. Liu, M. Long, and J. Wang, “Hashgan: Deep learning to hash
    with pair conditional wasserstein gan,” in *CVPR*, 2018, pp. 1287–1296.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Cao, B. Liu, M. Long, 和 J. Wang, “Hashgan: 通过对条件 Wasserstein 生成对抗网络进行深度学习以实现哈希”，发表于
    *CVPR*，2018年，第1287–1296页。'
- en: '[150] M. Zieba, P. Semberecki, T. El-Gaaly, and T. Trzcinski, “Bingan: Learning
    compact binary descriptors with a regularized gan,” in *NIPS*, 2018, pp. 3608–3618.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] M. Zieba, P. Semberecki, T. El-Gaaly, 和 T. Trzcinski, “Bingan: 通过正则化的生成对抗网络学习紧凑的二进制描述符”，发表于
    *NIPS*，2018年，第3608–3618页。'
- en: '[151] D. Yu, Y. Liu, Y. Pang, Z. Li, and H. Li, “A multi-layer deep fusion
    convolutional neural network for sketch based image retrieval,” *Neurocomputing*,
    vol. 296, pp. 23–32, 2018.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] D. Yu, Y. Liu, Y. Pang, Z. Li, 和 H. Li, “一种多层深度融合卷积神经网络用于基于草图的图像检索”，*Neurocomputing*，第296卷，第23–32页，2018年。'
- en: '[152] T.-T. Do, K. Le, T. Hoang, H. Le, T. V. Nguyen, and N.-M. Cheung, “Simultaneous
    feature aggregating and hashing for compact binary code learning,” *IEEE TIP*,
    vol. 28, no. 10, pp. 4954–4969, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] T.-T. Do, K. Le, T. Hoang, H. Le, T. V. Nguyen, 和 N.-M. Cheung, “同时特征聚合和哈希用于紧凑二进制代码学习”，*IEEE
    TIP*，第28卷，第10期，第4954–4969页，2019年。'
- en: '[153] S. Dey, P. Riba, A. Dutta, J. Llados, and Y.-Z. Song, “Doodle to search:
    Practical zero-shot sketch-based image retrieval,” in *CVPR*, 2019, pp. 2179–2188.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] S. Dey, P. Riba, A. Dutta, J. Llados, 和 Y.-Z. Song, “涂鸦搜索：实用的零样本基于草图的图像检索”，发表于
    *CVPR*，2019年，第2179–2188页。'
- en: '[154] L.-K. Huang, J. Chen, and S. J. Pan, “Accelerate learning of deep hashing
    with gradient attention,” in *ICCV*, 2019, pp. 5271–5280.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] L.-K. Huang, J. Chen, 和 S. J. Pan, “通过梯度注意力加速深度哈希学习”，发表于 *ICCV*，2019年，第5271–5280页。'
- en: '[155] L.-W. Ge, J. Zhang, Y. Xia, P. Chen, B. Wang, and C.-H. Zheng, “Deep
    spatial attention hashing network for image retrieval,” *JVCIR*, vol. 63, p. 102577,
    2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] L.-W. Ge, J. Zhang, Y. Xia, P. Chen, B. Wang, 和 C.-H. Zheng, “用于图像检索的深度空间注意力哈希网络”，*JVCIR*，第63卷，第102577页，2019年。'
- en: '[156] S. Wei, L. Liao, J. Li, Q. Zheng, F. Yang, and Y. Zhao, “Saliency inside:
    Learning attentive cnns for content-based image retrieval,” *IEEE TIP*, vol. 28,
    no. 9, pp. 4580–4593, 2019.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] S. Wei, L. Liao, J. Li, Q. Zheng, F. Yang, 和 Y. Zhao, “内在显著性：学习专注的卷积神经网络用于基于内容的图像检索”，*IEEE
    TIP*，第28卷，第9期，第4580–4593页，2019年。'
- en: '[157] Z. Chen, J. Lin, Z. Wang, V. Chandrasekhar, and W. Lin, “Beyond ranking
    loss: Deep holographic networks for multi-label video search,” in *ICIP*, 2019,
    pp. 879–883.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Z. Chen, J. Lin, Z. Wang, V. Chandrasekhar, 和 W. Lin, “超越排序损失：用于多标签视频搜索的深度全息网络”，发表于
    *ICIP*，2019年，第879–883页。'
- en: '[158] L. Wang, X. Qian, Y. Zhang, J. Shen, and X. Cao, “Enhancing sketch-based
    image retrieval by cnn semantic re-ranking,” *IEEE Transactions on Cybernetics*,
    2019.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] L. Wang, X. Qian, Y. Zhang, J. Shen, 和 X. Cao, “通过卷积神经网络语义重新排序提升基于草图的图像检索”，*IEEE
    Transactions on Cybernetics*，2019年。'
- en: '[159] V. Kumar Verma, A. Mishra, A. Mishra, and P. Rai, “Generative model for
    zero-shot sketch-based image retrieval,” in *CVPR Workshops*, 2019.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] V. Kumar Verma, A. Mishra, A. Mishra, 和 P. Rai, “用于零样本基于草图的图像检索的生成模型”，发表于
    *CVPR Workshops*，2019年。'
- en: '[160] W. Gu, X. Gu, J. Gu, B. Li, Z. Xiong, and W. Wang, “Adversary guided
    asymmetric hashing for cross-modal retrieval,” in *ICMR*, 2019, pp. 159–167.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] W. Gu, X. Gu, J. Gu, B. Li, Z. Xiong, 和 W. Wang, “用于跨模态检索的对抗性引导不对称哈希”，发表于
    *ICMR*，2019年，第159–167页。'
- en: '[161] S. Jin, S. Zhou, Y. Liu, C. Chen, X. Sun, H. Yao, and X. Hua, “Ssah:
    Semi-supervised adversarial deep hashing with self-paced hard sample generation,”
    *arXiv preprint arXiv:1911.08688*, 2019.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] S. Jin, S. Zhou, Y. Liu, C. Chen, X. Sun, H. Yao, 和 X. Hua, “Ssah: 半监督对抗深度哈希与自适应困难样本生成”，*arXiv预印本
    arXiv:1911.08688*，2019年。'
- en: '[162] Y. Wang, X. Ou, J. Liang, and Z. Sun, “Deep semantic reconstruction hashing
    for similarity retrieval,” *IEEE TCSVT*, 2020.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Y. Wang, X. Ou, J. Liang, 和 Z. Sun, “用于相似性检索的深度语义重建哈希”，*IEEE TCSVT*，2020年。'
- en: '[163] A. Pandey, A. Mishra, V. K. Verma, A. Mittal, and H. Murthy, “Stacked
    adversarial network for zero-shot sketch based image retrieval,” in *WACV*, 2020,
    pp. 2540–2549.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] A. Pandey, A. Mishra, V. K. Verma, A. Mittal, 和 H. Murthy, “用于零样本草图图像检索的堆叠对抗网络”，发表于
    *WACV*，2020年，第2540–2549页。'
- en: '[164] T. Ng, V. Balntas, Y. Tian, and K. Mikolajczyk, “Solar: Second-order
    loss and attention for image retrieval,” *arXiv preprint arXiv:2001.08972*, 2020.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] T. Ng, V. Balntas, Y. Tian, 和 K. Mikolajczyk, “Solar: 用于图像检索的二阶损失和注意力”，*arXiv预印本
    arXiv:2001.08972*，2020年。'
- en: '[165] Z. Gao, H. Xue, and S. Wan, “Multiple discrimination and pairwise cnn
    for view-based 3d object retrieval,” *Neural Networks*, 2020.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Z. Gao, H. Xue, 和 S. Wan, “用于基于视图的3D物体检索的多重判别和配对卷积神经网络”，*Neural Networks*，2020年。'
- en: '[166] J. Song, T. He, L. Gao, X. Xu, A. Hanjalic, and H. T. Shen, “Unified
    binary generative adversarial network for image retrieval and compression,” *IJCV*,
    pp. 1–22, 2020.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] J. Song, T. He, L. Gao, X. Xu, A. Hanjalic, 和 H. T. Shen, “用于图像检索和压缩的统一二进制生成对抗网络”，*IJCV*，第1–22页，2020年。'
- en: '[167] J. Yang, Y. Zhang, R. Feng, T. Zhang, and W. Fan, “Deep reinforcement
    hashing with redundancy elimination for effective image retrieval,” *Pattern Recognition*,
    vol. 100, p. 107116, 2020.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. Yang, Y. Zhang, R. Feng, T. Zhang, 和 W. Fan, “带有冗余消除的深度强化哈希用于有效图像检索，”
    *Pattern Recognition*, 第 100 卷，第 107116 页, 2020。'
- en: '[168] Y. Pan, T. Yao, H. Li, C.-W. Ngo, and T. Mei, “Semi-supervised hashing
    with semantic confidence for large scale visual search,” in *Proceedings of the
    38th International ACM SIGIR Conference on Research and Development in Information
    Retrieval*, 2015, pp. 53–62.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Pan, T. Yao, H. Li, C.-W. Ngo, 和 T. Mei, “带有语义置信度的半监督哈希用于大规模视觉搜索，”
    在 *第 38 届国际 ACM SIGIR 信息检索研究与开发会议论文集*, 2015, 第 53–62 页。'
- en: '[169] X. Yan, L. Zhang, and W.-J. Li, “Semi-supervised deep hashing with a
    bipartite graph.” in *IJCAI*, 2017, pp. 3238–3244.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] X. Yan, L. Zhang, 和 W.-J. Li, “带有二分图的半监督深度哈希。” 在 *IJCAI*, 2017, 第 3238–3244
    页。'
- en: '[170] Z. Qiu, Y. Pan, T. Yao, and T. Mei, “Deep semantic hashing with generative
    adversarial networks,” in *Proceedings of the 40th International ACM SIGIR Conference
    on Research and Development in Information Retrieval*, 2017, pp. 225–234.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Z. Qiu, Y. Pan, T. Yao, 和 T. Mei, “使用生成对抗网络的深度语义哈希，” 在 *第 40 届国际 ACM
    SIGIR 信息检索研究与开发会议论文集*, 2017, 第 225–234 页。'
- en: '[171] S. Zhang, J. Li, and B. Zhang, “Pairwise teacher-student network for
    semi-supervised hashing,” in *CVPR Workshops*, 2019.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] S. Zhang, J. Li, 和 B. Zhang, “用于半监督哈希的对偶教师-学生网络，” 在 *CVPR Workshops*,
    2019。'
- en: '[172] J. Tang and Z. Li, “Weakly supervised multimodal hashing for scalable
    social image retrieval,” *IEEE TCSVT*, vol. 28, no. 10, pp. 2730–2741, 2017.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] J. Tang 和 Z. Li, “用于可扩展社交图像检索的弱监督多模态哈希，” *IEEE TCSVT*, 第 28 卷，第 10 期，第
    2730–2741 页, 2017。'
- en: '[173] Z. Guan, F. Xie, W. Zhao, X. Wang, L. Chen, W. Zhao, and J. Peng, “Tag-based
    weakly-supervised hashing for image retrieval.” in *IJCAI*, 2018, pp. 3776–3782.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Z. Guan, F. Xie, W. Zhao, X. Wang, L. Chen, W. Zhao, 和 J. Peng, “基于标签的弱监督哈希用于图像检索。”
    在 *IJCAI*, 2018, 第 3776–3782 页。'
- en: '[174] V. Gattupalli, Y. Zhuo, and B. Li, “Weakly supervised deep image hashing
    through tag embeddings,” in *CVPR*, 2019, pp. 10 375–10 384.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] V. Gattupalli, Y. Zhuo, 和 B. Li, “通过标签嵌入的弱监督深度图像哈希，” 在 *CVPR*, 2019,
    第 10 375–10 384 页。'
- en: '[175] Z. Li, J. Tang, L. Zhang, and J. Yang, “Weakly-supervised semantic guided
    hashing for social image retrieval,” *IJCV*, 2020.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Z. Li, J. Tang, L. Zhang, 和 J. Yang, “弱监督语义引导哈希用于社交图像检索，” *IJCV*, 2020。'
- en: '[176] Q. Hu, J. Wu, J. Cheng, L. Wu, and H. Lu, “Pseudo label based unsupervised
    deep discriminative hashing for image retrieval,” in *ACMMM*, 2017, pp. 1584–1590.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Q. Hu, J. Wu, J. Cheng, L. Wu, 和 H. Lu, “基于伪标签的无监督深度判别哈希用于图像检索，” 在 *ACMMM*,
    2017, 第 1584–1590 页。'
- en: '[177] X. Dong, L. Liu, L. Zhu, Z. Cheng, and H. Zhang, “Unsupervised deep k-means
    hashing for efficient image retrieval and clustering,” *IEEE TCSVT*, 2020.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] X. Dong, L. Liu, L. Zhu, Z. Cheng, 和 H. Zhang, “无监督深度 k-means 哈希用于高效的图像检索和聚类，”
    *IEEE TCSVT*, 2020。'
- en: '[178] H. Zhang, M. Wang, R. Hong, and T.-S. Chua, “Play and rewind: Optimizing
    binary representations of videos by self-supervised temporal hashing,” in *ACMMM*,
    2016, pp. 781–790.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] H. Zhang, M. Wang, R. Hong, 和 T.-S. Chua, “播放和回放：通过自监督时间哈希优化视频的二进制表示，”
    在 *ACMMM*, 2016, 第 781–790 页。'
- en: '[179] B. Liu, Y. Cao, M. Long, J. Wang, and J. Wang, “Deep triplet quantization,”
    in *ACMMM*, 2018, pp. 755–763.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] B. Liu, Y. Cao, M. Long, J. Wang, 和 J. Wang, “深度三元组量化，” 在 *ACMMM*, 2018,
    第 755–763 页。'
- en: '[180] H. Su, P. Wang, L. Liu, H. Li, Z. Li, and Y. Zhang, “Where to look and
    how to describe: Fashion image retrieval with an attentional heterogeneous bilinear
    network,” *IEEE TCSVT*, 2020.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] H. Su, P. Wang, L. Liu, H. Li, Z. Li, 和 Y. Zhang, “如何观察和描述：通过注意力异构双线性网络进行时尚图像检索，”
    *IEEE TCSVT*, 2020。'
- en: '[181] K. Lin, H.-F. Yang, K.-H. Liu, J.-H. Hsiao, and C.-S. Chen, “Rapid clothing
    retrieval via deep learning of binary codes and hierarchical search,” in *ICMR*,
    2015, pp. 499–502.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] K. Lin, H.-F. Yang, K.-H. Liu, J.-H. Hsiao, 和 C.-S. Chen, “通过深度学习二进制代码和层次搜索实现快速服装检索，”
    在 *ICMR*, 2015, 第 499–502 页。'
- en: '[182] A. Babenko and V. Lempitsky, “Aggregating local deep features for image
    retrieval,” in *ICCV*, 2015, pp. 1269–1277.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] A. Babenko 和 V. Lempitsky, “聚合局部深度特征用于图像检索，” 在 *ICCV*, 2015, 第 1269–1277
    页。'
- en: '[183] Y. Liu, Y. Guo, S. Wu, and M. S. Lew, “Deepindex for accurate and efficient
    image retrieval,” in *ICMR*, 2015, pp. 43–50.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Y. Liu, Y. Guo, S. Wu, 和 M. S. Lew, “用于准确和高效图像检索的 Deepindex，” 在 *ICMR*,
    2015, 第 43–50 页。'
- en: '[184] J. Yue-Hei Ng, F. Yang, and L. S. Davis, “Exploiting local features from
    deep networks for image retrieval,” in *CVPR workshops*, 2015, pp. 53–61.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] J. Yue-Hei Ng, F. Yang, 和 L. S. Davis, “利用深度网络的局部特征进行图像检索，” 在 *CVPR workshops*,
    2015, 第 53–61 页。'
- en: '[185] T. Uricchio, M. Bertini, L. Seidenari, and A. Bimbo, “Fisher encoded
    convolutional bag-of-windows for efficient image retrieval and social image tagging,”
    in *ICCV Workshops*, 2015, pp. 9–15.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] T. Uricchio, M. Bertini, L. Seidenari, 和 A. Bimbo, “用于高效图像检索和社交图像标记的
    Fisher 编码卷积窗口包，” 见 *ICCV Workshops*，2015年，第 9–15 页。'
- en: '[186] T.-T. Do, A.-D. Doan, and N.-M. Cheung, “Learning to hash with binary
    deep neural network,” in *ECCV*, 2016, pp. 219–234.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] T.-T. Do, A.-D. Doan, 和 N.-M. Cheung, “使用二进制深度神经网络学习哈希，” 见 *ECCV*，2016年，第
    219–234 页。'
- en: '[187] E. Mohedano, K. McGuinness, N. E. O’Connor, A. Salvador, F. Marques,
    and X. Giro-i Nieto, “Bags of local convolutional features for scalable instance
    search,” in *ICMR*, 2016, pp. 327–331.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] E. Mohedano, K. McGuinness, N. E. O’Connor, A. Salvador, F. Marques,
    和 X. Giro-i Nieto, “用于可扩展实例检索的局部卷积特征包，” 见 *ICMR*，2016年，第 327–331 页。'
- en: '[188] A. Qayyum, S. M. Anwar, M. Awais, and M. Majid, “Medical image retrieval
    using deep convolutional neural network,” *Neurocomputing*, vol. 266, pp. 8–20,
    2017.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] A. Qayyum, S. M. Anwar, M. Awais, 和 M. Majid, “使用深度卷积神经网络的医学图像检索，” *Neurocomputing*，第
    266 卷，第 8–20 页，2017年。'
- en: '[189] X.-S. Wei, J.-H. Luo, J. Wu, and Z.-H. Zhou, “Selective convolutional
    descriptor aggregation for fine-grained image retrieval,” *IEEE TIP*, vol. 26,
    no. 6, pp. 2868–2881, 2017.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] X.-S. Wei, J.-H. Luo, J. Wu, 和 Z.-H. Zhou, “用于精细图像检索的选择性卷积描述符聚合，” *IEEE
    TIP*，第 26 卷，第 6 期，第 2868–2881 页，2017年。'
- en: '[190] W. Yu, K. Yang, H. Yao, X. Sun, and P. Xu, “Exploiting the complementary
    strengths of multi-layer cnn features for image retrieval,” *Neurocomputing*,
    vol. 237, pp. 235–241, 2017.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] W. Yu, K. Yang, H. Yao, X. Sun, 和 P. Xu, “利用多层 CNN 特征的互补优势进行图像检索，” *Neurocomputing*，第
    237 卷，第 235–241 页，2017年。'
- en: '[191] T.-T. Do, D.-K. Le Tan, T. T. Pham, and N.-M. Cheung, “Simultaneous feature
    aggregating and hashing for large-scale image search,” in *CVPR*, 2017, pp. 6618–6627.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] T.-T. Do, D.-K. Le Tan, T. T. Pham, 和 N.-M. Cheung, “大规模图像检索的特征聚合与哈希同步，”
    见 *CVPR*，2017年，第 6618–6627 页。'
- en: '[192] W. Zhou, H. Li, J. Sun, and Q. Tian, “Collaborative index embedding for
    image retrieval,” *IEEE TPAMI*, vol. 40, no. 5, pp. 1154–1166, 2017.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] W. Zhou, H. Li, J. Sun, 和 Q. Tian, “用于图像检索的协作索引嵌入，” *IEEE TPAMI*，第 40
    卷，第 5 期，第 1154–1166 页，2017年。'
- en: '[193] X. Liu, S. Zhang, T. Huang, and Q. Tian, “E2bows: An end-to-end bag-of-words
    model via deep convolutional neural network for image retrieval,” *Neurocomputing*,
    2019.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] X. Liu, S. Zhang, T. Huang, 和 Q. Tian, “E2bows: 基于深度卷积神经网络的端到端词袋模型用于图像检索，”
    *Neurocomputing*，2019年。'
- en: '[194] T.-T. Do, T. Hoang, D.-K. Le Tan, A.-D. Doan, and N.-M. Cheung, “Compact
    hash code learning with binary deep neural network,” *IEEE TMM*, vol. 22, no. 4,
    pp. 992–1004, 2019.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] T.-T. Do, T. Hoang, D.-K. Le Tan, A.-D. Doan, 和 N.-M. Cheung, “使用二进制深度神经网络的紧凑哈希码学习，”
    *IEEE TMM*，第 22 卷，第 4 期，第 992–1004 页，2019年。'
- en: '[195] T.-T. Do, T. Hoang, D.-K. L. Tan, H. Le, T. V. Nguyen, and N.-M. Cheung,
    “From selective deep convolutional features to compact binary representations
    for image retrieval,” *ACM TOMM*, vol. 15, no. 2, pp. 1–22, 2019.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] T.-T. Do, T. Hoang, D.-K. L. Tan, H. Le, T. V. Nguyen, 和 N.-M. Cheung,
    “从选择性深度卷积特征到紧凑的二进制表示用于图像检索，” *ACM TOMM*，第 15 卷，第 2 期，第 1–22 页，2019年。'
- en: '[196] X. Lu, Y. Chen, and X. Li, “Discrete deep hashing with ranking optimization
    for image retrieval,” *IEEE TNNLS*, 2019.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] X. Lu, Y. Chen, 和 X. Li, “用于图像检索的离散深度哈希与排序优化，” *IEEE TNNLS*，2019年。'
- en: '[197] S. R. Dubey, S. K. Roy, S. Chakraborty, S. Mukherjee, and B. B. Chaudhuri,
    “Local bit-plane decoded convolutional neural network features for biomedical
    image retrieval,” *NCAA*, pp. 1–13, 2019.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] S. R. Dubey, S. K. Roy, S. Chakraborty, S. Mukherjee, 和 B. B. Chaudhuri,
    “用于生物医学图像检索的局部比特平面解码卷积神经网络特征，” *NCAA*，第 1–13 页，2019年。'
- en: '[198] Z. Ji, W. Yao, W. Wei, H. Song, and H. Pi, “Deep multi-level semantic
    hashing for cross-modal retrieval,” *IEEE Access*, vol. 7, pp. 23 667–23 674,
    2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] Z. Ji, W. Yao, W. Wei, H. Song, 和 H. Pi, “用于跨模态检索的深度多级语义哈希，” *IEEE Access*，第
    7 卷，第 23,667–23,674 页，2019年。'
- en: '[199] J. Zhu, J. Wang, S. Pang, W. Guan, Z. Li, Y. Li, and X. Qian, “Co-weighting
    semantic convolutional features for object retrieval,” *JVCIR*, vol. 62, pp. 368–380,
    2019.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] J. Zhu, J. Wang, S. Pang, W. Guan, Z. Li, Y. Li, 和 X. Qian, “用于对象检索的语义卷积特征协同加权，”
    *JVCIR*，第 62 卷，第 368–380 页，2019年。'
- en: '[200] Y. Chen and X. Lu, “Deep discrete hashing with pairwise correlation learning,”
    *Neurocomputing*, vol. 385, pp. 111–121, 2020.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Y. Chen 和 X. Lu, “深度离散哈希与成对相关学习，” *Neurocomputing*，第 385 卷，第 111–121
    页，2020年。'
- en: '[201] Y. Cao, M. Long, J. Wang, Q. Yang, and P. S. Yu, “Deep visual-semantic
    hashing for cross-modal retrieval,” in *ACM ICKDDM*, 2016, pp. 1445–1454.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Y. Cao, M. Long, J. Wang, Q. Yang, 和 P. S. Yu, “用于跨模态检索的深度视觉-语义哈希，” 见
    *ACM ICKDDM*，2016年，第 1445–1454 页。'
- en: '[202] Y. Wu, S. Wang, and Q. Huang, “Online asymmetric similarity learning
    for cross-modal retrieval,” in *CVPR*, 2017, pp. 4269–4278.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Y. Wu, S. Wang, 和 Q. Huang，“在线非对称相似性学习用于跨模态检索，” 见 *CVPR*，2017 年，第 4269–4278
    页。'
- en: '[203] J. Liu, M. Yang, C. Li, and R. Xu, “Improving cross-modal image-text
    retrieval with teacher-student learning,” *IEEE TCSVT*, 2020.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] J. Liu, M. Yang, C. Li, 和 R. Xu，“通过师生学习提高跨模态图像-文本检索，” *IEEE TCSVT*，2020
    年。'
- en: '[204] J. Lei, Y. Song, B. Peng, Z. Ma, L. Shao, and Y.-Z. Song, “Semi-heterogeneous
    three-way joint embedding network for sketch-based image retrieval,” *IEEE TCSVT*,
    vol. 30, no. 9, pp. 3226–3237, 2019.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] J. Lei, Y. Song, B. Peng, Z. Ma, L. Shao, 和 Y.-Z. Song，“用于基于草图的图像检索的半异质三路联合嵌入网络，”
    *IEEE TCSVT*，第 30 卷，第 9 期，第 3226–3237 页，2019 年。'
- en: '[205] L. Liu, F. Shen, Y. Shen, X. Liu, and L. Shao, “Deep sketch hashing:
    Fast free-hand sketch-based image retrieval,” in *CVPR*, 2017, pp. 2862–2871.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] L. Liu, F. Shen, Y. Shen, X. Liu, 和 L. Shao，“深度草图哈希：快速自由手绘草图图像检索，” 见
    *CVPR*，2017 年，第 2862–2871 页。'
- en: '[206] A. Dutta and Z. Akata, “Semantically tied paired cycle consistency for
    zero-shot sketch-based image retrieval,” in *CVPR*, 2019, pp. 5089–5098.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] A. Dutta 和 Z. Akata，“用于零样本草图基础图像检索的语义绑定成对循环一致性，” 见 *CVPR*，2019 年，第 5089–5098
    页。'
- en: '[207] F. Zhao, Y. Huang, L. Wang, and T. Tan, “Deep semantic ranking based
    hashing for multi-label image retrieval,” in *CVPR*, 2015, pp. 1556–1564.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] F. Zhao, Y. Huang, L. Wang, 和 T. Tan，“基于深度语义排名的哈希用于多标签图像检索，” 见 *CVPR*，2015
    年，第 1556–1564 页。'
- en: '[208] D. Wu, Z. Lin, B. Li, M. Ye, and W. Wang, “Deep supervised hashing for
    multi-label and large-scale image retrieval,” in *ICMR*, 2017, pp. 150–158.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] D. Wu, Z. Lin, B. Li, M. Ye, 和 W. Wang，“用于多标签和大规模图像检索的深度监督哈希，” 见 *ICMR*，2017
    年，第 150–158 页。'
- en: '[209] H. Lai, P. Yan, X. Shu, Y. Wei, and S. Yan, “Instance-aware hashing for
    multi-label image retrieval,” *IEEE TIP*, vol. 25, no. 6, pp. 2469–2479, 2016.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] H. Lai, P. Yan, X. Shu, Y. Wei, 和 S. Yan，“面向实例的哈希用于多标签图像检索，” *IEEE TIP*，第
    25 卷，第 6 期，第 2469–2479 页，2016 年。'
- en: '[210] G. Chen, X. Cheng, S. Su, and C. Tang, “Multiple-instance ranking based
    deep hashing for multi-label image retrieval,” *Neurocomp.*, 2020.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] G. Chen, X. Cheng, S. Su, 和 C. Tang，“基于多实例排序的深度哈希用于多标签图像检索，” *Neurocomp.*，2020
    年。'
- en: '[211] Q. Qin, L. Huang, and Z. Wei, “Deep multilevel similarity hashing with
    fine-grained features for multi-label image retrieval,” *Neurocomp.*, 2020.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Q. Qin, L. Huang, 和 Z. Wei，“深度多级相似性哈希与细粒度特征用于多标签图像检索，” *Neurocomp.*，2020
    年。'
- en: '[212] A. Sharif Razavian, J. Sullivan, A. Maki, and S. Carlsson, “A baseline
    for visual instance retrieval with deep convolutional networks,” in *ICLR*, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] A. Sharif Razavian, J. Sullivan, A. Maki, 和 S. Carlsson，“使用深度卷积网络进行视觉实例检索的基准，”
    见 *ICLR*，2015 年。'
- en: '[213] O. Morère, A. Veillard, L. Jie, J. Petta, V. Chandrasekhar, and T. Poggio,
    “Group invariant deep representations for image instance retrieval,” in *AAAI
    Spring Symposium Series*, 2017.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] O. Morère, A. Veillard, L. Jie, J. Petta, V. Chandrasekhar, 和 T. Poggio，“用于图像实例检索的群体不变深度表示，”
    见 *AAAI Spring Symposium Series*，2017 年。'
- en: '[214] G. Tolias, R. Sicre, and H. Jégou, “Particular object retrieval with
    integral max-pooling of cnn activations,” *arXiv preprint arXiv:1511.05879*, 2015.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] G. Tolias, R. Sicre, 和 H. Jégou，“通过 CNN 激活的积分最大池化进行特定对象检索，” *arXiv 预印本
    arXiv:1511.05879*，2015 年。'
- en: '[215] S. Pang, J. Zhu, J. Wang, V. Ordonez, and J. Xue, “Building discriminative
    cnn image representations for object retrieval using the replicator equation,”
    *Pattern Recognition*, vol. 83, pp. 150–160, 2018.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] S. Pang, J. Zhu, J. Wang, V. Ordonez, 和 J. Xue，“利用复制方程构建区分性 CNN 图像表示用于对象检索，”
    *Pattern Recognition*，第 83 卷，第 150–160 页，2018 年。'
- en: '[216] X. Shi and X. Qian, “Exploring spatial and channel contribution for object
    based image retrieval,” *Knowledge-Based Systems*, vol. 186, p. 104955, 2019.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] X. Shi 和 X. Qian，“探索对象基础图像检索的空间和通道贡献，” *Knowledge-Based Systems*，第 186
    卷，第 104955 页，2019 年。'
- en: '[217] J. Guo, S. Zhang, and J. Li, “Hash learning with convolutional neural
    networks for semantic based image retrieval,” in *KDDM*, 2016, pp. 227–238.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] J. Guo, S. Zhang, 和 J. Li，“用于语义基础图像检索的卷积神经网络哈希学习，” 见 *KDDM*，2016 年，第
    227–238 页。'
- en: '[218] Y. Cao, M. Long, J. Wang, and S. Liu, “Deep visual-semantic quantization
    for efficient image retrieval,” in *CVPR*, 2017, pp. 1328–1337.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] Y. Cao, M. Long, J. Wang, 和 S. Liu，“用于高效图像检索的深度视觉-语义量化，” 见 *CVPR*，2017
    年，第 1328–1337 页。'
- en: '[219] Q. Qin, L. Huang, Z. Wei, K. Xie, and W. Zhang, “Unsupervised deep multi-similarity
    hashing with semantic structure for image retrieval,” *IEEE TCSVT*, 2020.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Q. Qin, L. Huang, Z. Wei, K. Xie, 和 W. Zhang，“用于图像检索的无监督深度多相似性哈希与语义结构，”
    *IEEE TCSVT*，2020 年。'
- en: '[220] J. Zhang and Y. Peng, “Query-adaptive image retrieval by deep-weighted
    hashing,” *IEEE TMM*, vol. 20, no. 9, pp. 2400–2414, 2018.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] J. Zhang 和 Y. Peng, “通过深度加权哈希进行查询自适应图像检索，” *IEEE TMM*，第 20 卷，第 9 期，第
    2400–2414 页，2018 年。'
- en: '[221] X. Zeng, Y. Zhang, X. Wang, K. Chen, D. Li, and W. Yang, “Fine-grained
    image retrieval via piecewise cross entropy loss,” *Image and Vision Computing*,
    vol. 93, p. 103820, 2020.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] X. Zeng, Y. Zhang, X. Wang, K. Chen, D. Li, 和 W. Yang, “通过分段交叉熵损失进行细粒度图像检索，”
    *图像与视觉计算*，第 93 卷，第 103820 页，2020 年。'
- en: '[222] Z. Yang, O. I. Raymond, W. Sun, and J. Long, “Asymmetric deep semantic
    quantization for image retrieval,” *IEEE Access*, vol. 7, pp. 72 684–72 695, 2019.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Z. Yang, O. I. Raymond, W. Sun, 和 J. Long, “用于图像检索的非对称深度语义量化，” *IEEE
    Access*，第 7 卷，第 72 684–72 695 页，2019 年。'
- en: '[223] J. Chen and W. K. Cheung, “Similarity preserving deep asymmetric quantization
    for image retrieval,” in *AAAI*, vol. 33, 2019, pp. 8183–8190.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] J. Chen 和 W. K. Cheung, “用于图像检索的相似性保持深度非对称量化，” 见 *AAAI*，第 33 卷，2019 年，第
    8183–8190 页。'
- en: '[224] J. Revaud, J. Almazán, R. S. Rezende, and C. R. d. Souza, “Learning with
    average precision: Training image retrieval with a listwise loss,” in *ICCV*,
    2019, pp. 5107–5116.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] J. Revaud, J. Almazán, R. S. Rezende, 和 C. R. d. Souza, “使用平均精度学习：用列表损失训练图像检索，”
    见 *ICCV*，2019 年，第 5107–5116 页。'
- en: '[225] H. Zhai, S. Lai, H. Jin, X. Qian, and T. Mei, “Deep transfer hashing
    for image retrieval,” *IEEE TCSVT*, 2020.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] H. Zhai, S. Lai, H. Jin, X. Qian, 和 T. Mei, “深度迁移哈希用于图像检索，” *IEEE TCSVT*，2020
    年。'
- en: '[226] Z. Dong, C. Jing, M. Pei, and Y. Jia, “Deep cnn based binary hash video
    representations for face retrieval,” *Pattern Recognition*, vol. 81, pp. 357–369,
    2018.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] Z. Dong, C. Jing, M. Pei, 和 Y. Jia, “基于深度 CNN 的二进制哈希视频表示用于人脸检索，” *模式识别*，第
    81 卷，第 357–369 页，2018 年。'
- en: '[227] S. R. Dubey and S. Chakraborty, “Average biased relu based cnn descriptor
    for improved face retrieval,” *arXiv preprint arXiv:1804.02051*, 2018.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] S. R. Dubey 和 S. Chakraborty, “基于平均偏置 Relu 的 CNN 描述符用于改进的人脸检索，” *arXiv
    预印本 arXiv:1804.02051*，2018 年。'
- en: '[228] T.-Y. Yang, D. Kien Nguyen, H. Heijnen, and V. Balntas, “Dame web: Dynamic
    mean with whitening ensemble binarization for landmark retrieval without human
    annotation,” in *ICCV Workshops*, 2019.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] T.-Y. Yang, D. Kien Nguyen, H. Heijnen, 和 V. Balntas, “Dame web：动态均值与白化集成二值化用于无人工注释的地标检索，”
    见 *ICCV Workshops*，2019 年。'
- en: '[229] L. Zhu, H. Cui, Z. Cheng, J. Li, and Z. Zhang, “Dual-level semantic transfer
    deep hashing for efficient social image retrieval,” *IEEE TCSVT*, 2020.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] L. Zhu, H. Cui, Z. Cheng, J. Li, 和 Z. Zhang, “双层语义转移深度哈希用于高效社交图像检索，”
    *IEEE TCSVT*，2020 年。'
- en: '| ![[Uncaptioned image]](img/2edc107d25d332d63b13c724d00c95ff.png) | Shiv Ram
    Dubey has been with the Indian Institute of Information Technology (IIIT), Sri
    City since June 2016, where he is currently the Assistant Professor of Computer
    Science and Engineering. He received the Ph.D. degree in Computer Vision and Image
    Processing from Indian Institute of Information Technology, Allahabad (IIIT Allahabad)
    in 2016\. Before that, from August 2012-Feb 2013, he was a Project Officer in
    the Computer Science and Engineering Department at Indian Institute of Technology,
    Madras (IIT Madras). He was a recipient of several awards, including the Best
    PhD Award in PhD Symposium, IEEE-CICT2017 at IIITM Gwalior and NVIDIA GPU Grant
    Award Twice from NVIDIA. His research interest includes Computer Vision, Deep
    Learning, Image Feature Description, and Content Based Image Retrieval. |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/2edc107d25d332d63b13c724d00c95ff.png) | Shiv Ram Dubey 自 2016
    年 6 月以来一直在印度信息技术学院（IIIT），斯里城担任计算机科学与工程助理教授。他于 2016 年在印度信息技术学院，阿拉哈巴德（IIIT Allahabad）获得计算机视觉和图像处理的博士学位。在此之前，从
    2012 年 8 月到 2013 年 2 月，他在印度理工学院，马德拉斯（IIT Madras）计算机科学与工程系担任项目官员。他曾获得多个奖项，包括 IIITM
    Gwalior 的 PhD Symposium 最佳博士奖和 NVIDIA 的两次 GPU 奖学金。他的研究兴趣包括计算机视觉、深度学习、图像特征描述和基于内容的图像检索。'
