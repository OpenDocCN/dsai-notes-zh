- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:42:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:42:48
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2212.00253] Distributed Deep Reinforcement Learning: A Survey and A Multi-Player
    Multi-Agent Learning Toolbox'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2212.00253] 分布式深度强化学习：综述与多玩家多代理学习工具箱'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2212.00253](https://ar5iv.labs.arxiv.org/html/2212.00253)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2212.00253](https://ar5iv.labs.arxiv.org/html/2212.00253)
- en: 'Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent
    Learning Toolbox'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式深度强化学习：综述与多玩家多代理学习工具箱
- en: 'Qiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang, Meijing Zhao, Kaiqi Huang,
    Bin Liang, Liang Wang Qiyue Yin (qyyin@nlpr.ia.ac.cn), Tongtong Yu, Shengqi Shen,
    Meijing Zhao, Kaiqi Huang and Liang Wang are with Institute of Automation, Chinese
    Academy of Sciences, Beijing, China, 100190.Jun Yang and Bin Liang are with the
    Department of Automation, Tsinghua University, Beijing, China, 100084.Corresponding
    authors: kqhuang@nlpr.ia.ac.cn (Kaiqi Huang); yangjun603@tsinghua.edu.cn (Jun
    Yang)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 祁悦银，佟佟余，盛琪申，杨俊，梅晶赵，黄凯琪，梁斌，王亮 祁悦银 (qyyin@nlpr.ia.ac.cn)，佟佟余，盛琪申，梅晶赵，黄凯琪和王亮均来自中国科学院自动化研究所，北京，中国，100190。杨俊和梁斌来自清华大学自动化系，北京，中国，100084。通讯作者：kqhuang@nlpr.ia.ac.cn（黄凯琪）；yangjun603@tsinghua.edu.cn（杨俊）
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized
    technique for solving sequential decision-making problems. Despite its reputation,
    data inefficiency caused by its trial and error learning mechanism makes deep
    reinforcement learning hard to be practical in a wide range of areas. Plenty of
    methods have been developed for sample efficient deep reinforcement learning,
    such as environment modeling, experience transfer, and distributed modifications,
    amongst which, distributed deep reinforcement learning has shown its potential
    in various applications, such as human-computer gaming, and intelligent transportation.
    In this paper, we conclude the state of this exciting field, by comparing the
    classical distributed deep reinforcement learning methods, and studying important
    components to achieve efficient distributed learning, covering single player single
    agent distributed deep reinforcement learning to the most complex multiple players
    multiple agents distributed deep reinforcement learning. Furthermore, we review
    recently released toolboxes that help to realize distributed deep reinforcement
    learning without many modifications of their non-distributed versions. By analyzing
    their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement
    learning toolbox is developed and released, which is further validated on Wargame,
    a complex environment, showing usability of the proposed toolbox for multiple
    players and multiple agents distributed deep reinforcement learning under complex
    games. Finally, we try to point out challenges and future trends, hoping this
    brief review can provide a guide or a spark for researchers who are interested
    in distributed deep reinforcement learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 AlphaGo 的突破，深度强化学习成为解决序列决策问题的公认技术。尽管它享有盛誉，但由于其试错学习机制导致的数据低效，使得深度强化学习在广泛领域中难以实际应用。为提高样本效率，已经开发了大量方法，如环境建模、经验迁移和分布式改进，其中分布式深度强化学习在多个应用中展现了其潜力，如人机对战和智能交通。本文通过比较经典的分布式深度强化学习方法，总结了这一激动人心领域的现状，并研究了实现高效分布式学习的重要组件，从单玩家单代理分布式深度强化学习到最复杂的多玩家多代理分布式深度强化学习。进一步地，我们回顾了最近发布的工具箱，这些工具箱在无需对其非分布式版本进行太多修改的情况下，有助于实现分布式深度强化学习。通过分析这些工具箱的优缺点，我们开发并发布了一个多玩家多代理的分布式深度强化学习工具箱，并在复杂环境
    Wargame 上进行了验证，展示了所提工具箱在复杂游戏中的多玩家多代理分布式深度强化学习的可用性。最后，我们尝试指出挑战和未来趋势，希望这篇简要的综述能为对分布式深度强化学习感兴趣的研究人员提供指导或启发。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep reinforcement learning, distributed machine learning, self-play, population-play,
    toolbox.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习，分布式机器学习，自我博弈，群体博弈，工具箱。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the breakthrough of AlphaGo [[1](#bib.bib1), [2](#bib.bib2)], an agent
    that wins plenty of professional Go players in human-computer gaming, deep reinforcement
    learning (DRL) comes to most researchers’ attention, which becomes a recognized
    technique for solving sequential decision making problems. Plenty of algorithms
    are developed to solve challenging issues that lie between DRL and real world
    applications, such as exploration and exploitation dilemma, data inefficiency,
    multi-agent cooperation and competition. Among all these challenges, data inefficiency
    is the most criticized due to the trial and error learning mechanism of DRL, which
    requires a huge amount of interactive data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AlphaGo [[1](#bib.bib1), [2](#bib.bib2)] 的突破，这一在棋类游戏中击败大量职业围棋玩家的代理，引起了大多数研究者对深度强化学习（DRL）的关注，这已成为解决序列决策问题的公认技术。为解决DRL与现实世界应用之间的挑战问题，如探索与开发困境、数据低效、多代理合作与竞争，开发了许多算法。在所有这些挑战中，数据低效问题最受批评，因为DRL的试错学习机制需要大量交互数据。
- en: To alleviate the data inefficiency problem, several research directions are
    developed [[3](#bib.bib3)]. For example, model based deep reinforcement learning
    constructs environment model for generating imaginary trajectories to help reduce
    times of interaction with the environment. Transfer reinforcement learning mines
    shared skills, roles, or patterns from source tasks, and then uses the learned
    knowledge to accelerate reinforcement learning in the target task. Inspired from
    distributed machine learning techniques, which has been successfully utilized
    in computer vision and natural language processing [[4](#bib.bib4)], distributed
    deep reinforcement learning (DDRL) is developed, which has shown its potential
    to train very successful agents, e.g., Suphx [[5](#bib.bib5)], OpenAI Five[[6](#bib.bib6)],
    and AlphaStar[[7](#bib.bib7)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解数据低效问题，开发了几种研究方向[[3](#bib.bib3)]。例如，基于模型的深度强化学习构建环境模型以生成虚拟轨迹，从而减少与环境的交互次数。迁移强化学习从源任务中挖掘共享技能、角色或模式，然后利用学到的知识加速目标任务中的强化学习。受到已成功应用于计算机视觉和自然语言处理的分布式机器学习技术的启发[[4](#bib.bib4)]，开发了分布式深度强化学习（DDRL），它展示了训练非常成功的代理的潜力，例如Suphx
    [[5](#bib.bib5)]、OpenAI Five[[6](#bib.bib6)] 和AlphaStar[[7](#bib.bib7)]。
- en: Generally, training DRL agents consists of two main parts, i.e., pulling policy
    network parameters to generate data by interacting with the environment, and updating
    policy network parameters by consuming data. Such a structured pattern makes distributed
    modifications of DRL feasible, and plenty of DDRL algorithms have been developed.
    For example, the general reinforcement learning architecture [[8](#bib.bib8)],
    likely the first DDRL architecture, divides the training system into four components,
    i.e., parameter server, learners, actors and replay buffer, which inspires successive
    more data efficient DDRL architectures. The recently proposed SEED RL [[9](#bib.bib9)],
    an improved version of IMPALA [[10](#bib.bib10)], is claimed to be able to produce
    and consume millions of frames per second, based on which, AlphaStar is successfully
    trained within 44 days (192 v3 + 12 128 core TPUs, 1800 CPUs) for beating professional
    human players.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，训练深度强化学习（DRL）代理包括两个主要部分，即通过与环境交互来生成数据的策略网络参数拉取，以及通过消耗数据来更新策略网络参数。这种结构化模式使得分布式修改DRL成为可能，因此开发了许多深度分布式强化学习（DDRL）算法。例如，一般的强化学习架构[[8](#bib.bib8)]，可能是第一个DDRL架构，将训练系统划分为四个组件，即参数服务器、学习者、演员和重放缓冲区，这启发了更多数据高效的DDRL架构。最近提出的SEED
    RL [[9](#bib.bib9)]，作为IMPALA [[10](#bib.bib10)] 的改进版，声称能够每秒生成和消耗数百万帧，在此基础上，AlphaStar在44天内（192个v3
    + 12个128核心TPU，1800个CPU）成功训练以击败专业人类玩家。
- en: To make distributed modifications of DRL be able to use multiple machines, several
    engineering problems should be solved such as machines communication and distributed
    storage. Fortunately, several useful toolboxes have been developed and released,
    and revising codes of DRL to a distributed version usually requires a small amount
    of code modification, which largely promotes the development of DDRL. For example,
    Horovod [[11](#bib.bib11)], released by Uber, makes full use of ring allreduce
    technique, and can properly use multiple GPUs for training acceleration by just
    adding a few lines of codes compared with the single GPU version. Ray [[12](#bib.bib12)],
    a distributed framework of machine learning released by UC Berkeley RISELab, provides
    a RLlib [[13](#bib.bib13)] for efficient DDRL, which is easy to be used due to
    its reinforcement learning abstraction and algorithm library.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为使 DRL 的分布式修改能够使用多台机器，必须解决若干工程问题，如机器通信和分布式存储。幸运的是，已经开发和发布了多个有用的工具箱，将 DRL 代码修改为分布式版本通常只需少量代码修改，这在很大程度上推动了
    DDRL 的发展。例如，Horovod [[11](#bib.bib11)] 由 Uber 发布，充分利用了环形全归约技术，通过仅添加几行代码即可适当地使用多
    GPU 进行训练加速，相比于单 GPU 版本。Ray [[12](#bib.bib12)] 是由 UC Berkeley RISELab 发布的分布式机器学习框架，为高效
    DDRL 提供了 RLlib [[13](#bib.bib13)]，由于其强化学习抽象和算法库，使用起来非常方便。
- en: Considering the big progress of DDRL, it is emergent to comb out the course
    of DDRL techniques, challenges and opportunities, so as to provide clues for future
    research. Recently, Samsami and Alimadad [[14](#bib.bib14)] gave a brief review
    of DDRL, but their aim is single player single agent distributed reinforcement
    learning frameworks, and more challenging multiple agents and multiple players
    DDRL is absent. Czech [[15](#bib.bib15)] made a short survey on distributed methods
    for reinforcement learning, but only several specific algorithms are categorized
    and no key techniques, comparison and challenges are discussed. Different from
    previous summary, this paper shows a more comprehensive survey by comparing the
    classical distributed deep reinforcement learning methods, and studying important
    components to achieve efficient distributed learning, covering single player single
    agent distributed deep reinforcement learning to the most complex multiple players
    multiple agents distributed deep reinforcement learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 DDRL 的重大进展，亟需梳理 DDRL 技术的发展历程、挑战和机遇，以便为未来的研究提供线索。最近，Samsami 和 Alimadad [[14](#bib.bib14)]
    对 DDRL 进行了简要回顾，但他们的目标是单玩家单代理分布式强化学习框架，而更具挑战性的多代理和多玩家 DDRL 并未涉及。Czech [[15](#bib.bib15)]
    对分布式强化学习方法进行了简短调查，但只对几个具体算法进行了分类，并没有讨论关键技术、比较和挑战。与之前的总结不同，本文通过比较经典的分布式深度强化学习方法，并研究实现高效分布式学习的重要组成部分，展示了更全面的调查，涵盖了从单玩家单代理分布式深度强化学习到最复杂的多玩家多代理分布式深度强化学习。
- en: The rest of the paper is organized as follows. In Section 2, we briefly describe
    background of DRL, distributed learning, and typical testbeds for DDRL. In section
    3, we elaborate on taxonomy of DDRL, by dividing current algorithms based on the
    learning frameworks and players and agents participating in. In Section 4, we
    compare current DDRL toolboxes, which help achieve efficient DDRL a lot. In Section
    5, we introduce a new multi-player multi-agent DDRL toolbox, which provides a
    useful DDRL tool for complex games. In Section 6, we summarize the main challenges
    and opportunities for DDRL, hoping to inspire future research. Finally, we conclude
    the paper in Section 7.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 论文其余部分的组织结构如下。在第 2 节中，我们简要描述了 DRL 的背景、分布式学习以及 DDRL 的典型测试平台。在第 3 节中，我们详细阐述了 DDRL
    的分类，通过基于学习框架和参与的玩家和代理对当前算法进行分类。在第 4 节中，我们比较了当前的 DDRL 工具箱，这些工具箱在实现高效 DDRL 方面发挥了重要作用。在第
    5 节中，我们介绍了一种新的多玩家多代理 DDRL 工具箱，为复杂游戏提供了有用的 DDRL 工具。在第 6 节中，我们总结了 DDRL 的主要挑战和机遇，希望能激发未来的研究。最后，在第
    7 节中，我们对论文进行了总结。
- en: 2 Background
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Deep Reinforcement Learning
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 深度强化学习
- en: Reinforcement learning is a typical kind of machine learning paradigm, the essence
    of which is learning via interaction. In a general reinforcement learning method,
    an agent interacts with an environment by posing actions to drive the environment
    dynamics, and receiving rewards to improve its policy for chasing long-term outcomes.
    To learn a good agent that can make sequential decisions, there are two typical
    kinds of algorithms, i.e., model-free methods that use no environment models,
    and model-based approaches that use the pre-given or learned environment models.
    Plenty of algorithms have been proposed, and readers can refer to [[16](#bib.bib16),
    [17](#bib.bib17)] for a more thorough review.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种典型的机器学习范式，其本质是通过互动进行学习。在一般的强化学习方法中，智能体通过采取行动与环境互动，以驱动环境动态，并通过奖励来改善其策略，以追求长期结果。为了学习一个能够进行顺序决策的优秀智能体，有两种典型的算法，即无模型方法和基于模型的方法。大量算法已被提出，读者可以参考[[16](#bib.bib16),
    [17](#bib.bib17)]以获取更全面的综述。
- en: In reality, applications naturally involve the participation of multiple agents,
    making multi-agent reinforcement learning a hot topic. Generally, multi-agent
    reinforcement learning is modeled as a stochastic game, and obeys similar learning
    paradigm with conventional reinforcement learning. Based on the game setting,
    agents can be fully cooperative, competitive and a mix of the two, requiring reinforcement
    learning agents to emerge abilities that can match the goal. Various key problems
    of multi-agent reinforcement learning have been raised, e.g., communication and
    credit assignment. Readers can refer to [[18](#bib.bib18), [19](#bib.bib19)] for
    a detailed introduction.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，应用自然涉及多个智能体的参与，使得多智能体强化学习成为一个热门话题。一般而言，多智能体强化学习被建模为一个随机游戏，并遵循与传统强化学习类似的学习范式。根据游戏设置，智能体可以是完全合作的、竞争的或两者的混合，这要求强化学习智能体具备匹配目标的能力。多智能体强化学习的各种关键问题已被提出，例如通信和信用分配。读者可以参考[[18](#bib.bib18),
    [19](#bib.bib19)]以获取详细介绍。
- en: With the breakthrough of deep learning, deep reinforcement learning becomes
    a strong learning paradigm by combining representation learning ability of deep
    learning and decision making ability of reinforcement learning, and several successful
    deep reinforcement learning agents have been proposed. For example, AlphaGo [[1](#bib.bib1),
    [2](#bib.bib2)], a Go agent that can beat professional human players, is based
    on single agent deep reinforcement learning. OpenAI Five, a dota2 agent that wins
    champion players in an e-sport for the first time, relies on multi-agent deep
    reinforcement learning. In the following, unless otherwise stated, we do not distinguish
    single agent or multiple agents deep reinforcement learning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的突破，深度强化学习通过结合深度学习的表示学习能力和强化学习的决策能力，成为一个强大的学习范式，并提出了几个成功的深度强化学习智能体。例如，AlphaGo
    [[1](#bib.bib1), [2](#bib.bib2)]是一个能够击败职业人类玩家的围棋智能体，基于单智能体深度强化学习。OpenAI Five是一个在电子竞技中首次击败冠军玩家的dota2智能体，依赖于多智能体深度强化学习。接下来，除非另有说明，否则我们不区分单智能体或多智能体深度强化学习。
- en: 2.2 Distributed Learning
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 分布式学习
- en: The success of deep learning is inseparable from huge data and computing power,
    which leads to huge demand of distributed learning that can handle data intensive
    and compute intensive computing. Due to the structured computation pattern of
    deep learning algorithms, some successful distributed learning methods are proposed
    for parallelism in deep learning [[20](#bib.bib20), [21](#bib.bib21)]. An early
    popular distributed deep learning framework is DistBelief [[22](#bib.bib22)],
    designed by Google, where concepts of parameter server and A-SGD are proposed.
    Based on DistBelief, Google released the second generation of distributed deep
    learning framework, Tensorflow [[23](#bib.bib23)], which becomes a widely used
    tool. Other typical distributed deep learning frameworks, such as PyTorch, MXNet,
    and Caffe2 are also developed and used by the research and industrial communities.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的成功离不开海量的数据和计算能力，这导致了对分布式学习的巨大需求，以处理数据密集型和计算密集型的计算。由于深度学习算法的结构化计算模式，提出了一些成功的分布式学习方法，以实现深度学习中的并行性
    [[20](#bib.bib20), [21](#bib.bib21)]。早期流行的分布式深度学习框架是 DistBelief [[22](#bib.bib22)]，由
    Google 设计，其中提出了参数服务器和 A-SGD 的概念。基于 DistBelief，Google 发布了第二代分布式深度学习框架 Tensorflow
    [[23](#bib.bib23)]，这成为了广泛使用的工具。其他典型的分布式深度学习框架，如 PyTorch、MXNet 和 Caffe2，也由研究界和工业界开发和使用。
- en: Ben-Nun and Hoefler[[24](#bib.bib24)] gave an in-depth concurrency analysis
    of parallel and distributed deep learning. In the survey, the authors gave different
    types of concurrency for deep neural networks, covering the bottom level operators,
    and key factors such as network inference and training. Finally, several important
    topics such as asynchronous stochastic optimization, distributed system architectures,
    communication schemes are discussed, providing clues for future directions of
    distributed deep learning. Nowadays, distributed learning is widely used in various
    fields, such as wireless networks [[25](#bib.bib25)], AIoT service platform [[26](#bib.bib26)]
    and human-computer gaming [[27](#bib.bib27)]. In short, DDRL is a special type
    of distributed deep learning. Instead of focusing on data parallelism and model
    parallelism in conventional deep learning, DDRL aims at improving data throughput
    due to the characteristics of reinforcement learning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Ben-Nun 和 Hoefler[[24](#bib.bib24)] 对并行和分布式深度学习进行了深入的并发分析。在这项调查中，作者对深度神经网络的不同类型并发进行了阐述，涵盖了底层操作符以及网络推理和训练等关键因素。最后，讨论了异步随机优化、分布式系统架构、通信方案等几个重要话题，为分布式深度学习的未来方向提供了线索。如今，分布式学习在各种领域得到了广泛应用，如无线网络
    [[25](#bib.bib25)]、AIoT 服务平台 [[26](#bib.bib26)] 和人机游戏 [[27](#bib.bib27)]。简言之，DDRL
    是一种特殊类型的分布式深度学习。与传统深度学习关注数据并行和模型并行不同，DDRL 旨在提高数据吞吐量，因为强化学习的特点。
- en: 2.3 Testing Environments
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 测试环境
- en: With the huge success of AlphaGo [[1](#bib.bib1)], DDRL is widely used in games,
    especially human-computer gaming. Those games provide an ideal testbeds for development
    of DDRL algorithms or frameworks, from single player single agent DDRL to multiple
    players multiple agents DDRL.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 AlphaGo [[1](#bib.bib1)] 的巨大成功，DDRL 在游戏中得到了广泛应用，特别是在人人机游戏中。这些游戏为 DDRL 算法或框架的开发提供了理想的测试环境，从单玩家单代理
    DDRL 到多玩家多代理 DDRL。
- en: Atari is a popular reinforcement learning testbed because it has the similar
    high dimensional visual input compared to human [[28](#bib.bib28)]. Besides, several
    environments confront challenging issues such as long time horizon and sparse
    rewards [[29](#bib.bib29)]. Plenty of DDRL algorithms are compared in Atari games,
    showing training acceleration against DRL without parallelism. However, typical
    Atari games are designed for single player single agent problems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Atari 是一个流行的强化学习测试环境，因为它具有类似于人类的高维视觉输入 [[28](#bib.bib28)]。此外，几个环境面临挑战性问题，如长时间的时间范围和稀疏的奖励
    [[29](#bib.bib29)]。许多 DDRL 算法在 Atari 游戏中进行了比较，显示出与没有并行性的 DRL 相比的训练加速。然而，典型的 Atari
    游戏是为单玩家单代理问题设计的。
- en: With the emerging of multi-agent reinforcement learning in multi-agent games,
    StarCraft Multi-Agent Challenge (SMAC) [[30](#bib.bib30)] becomes a recognized
    testbed for single player multi-agent reinforcement learning. Specifically, SMAC
    is a sub-task of StarCraft by focusing on micromanagement challenges, where a
    team of units is controlled to fight against build-in opponents. Several typical
    multi-agent reinforcement learning algorithms are released along with SMAC, which
    support parallel data collection in reinforcement learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 随着多智能体强化学习在多智能体游戏中的兴起，StarCraft多智能体挑战（SMAC）[[30](#bib.bib30)]成为了单玩家多智能体强化学习的公认测试平台。具体而言，SMAC是StarCraft的一个子任务，专注于微观管理挑战，其中一组单位被控制与内建对手作战。与SMAC一起发布了几种典型的多智能体强化学习算法，这些算法支持强化学习中的并行数据收集。
- en: Apart from the above single player single agent and single player multiple agents
    testing environments, there are a few multiple players environments for deep reinforcement
    learning algorithms [[31](#bib.bib31)]. Even though huge success has been made
    for games like Go, StarCraft, dota2 and honor of kings, those multiple players
    environments are used for a few researchers due to the huge game complexity. Overall,
    those multiple player single agent and multiple agents environments largely promote
    the development of DDRL.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述单玩家单智能体和单玩家多智能体测试环境，还有一些多玩家环境用于深度强化学习算法[[31](#bib.bib31)]。尽管在围棋、StarCraft、dota2和王者荣耀等游戏中取得了巨大成功，这些多玩家环境仅被少数研究者使用，因为游戏复杂性极高。总体而言，这些多玩家单智能体和多智能体环境在很大程度上促进了DDRL的发展。
- en: 3 Taxonomy of Distributed Deep Reinforcement Learning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 分布式深度强化学习的分类
- en: 3.1 Taxonomic Basis
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 分类基础
- en: 'Plenty of DDRL algorithms or frameworks are developed with representatives
    such as GORILA [[8](#bib.bib8)], A3C [[32](#bib.bib32)], APE-X [[33](#bib.bib33)],
    IMPALA [[10](#bib.bib10)], Distributed PPO [[34](#bib.bib34)], R2D2 [[35](#bib.bib35)]
    and Seed RL [[9](#bib.bib9)], based on which, we can draw the key components of
    a DDRL, as shown in Fig. [1](#S3.F1 "Figure 1 ‣ 3.1 Taxonomic Basis ‣ 3 Taxonomy
    of Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement Learning:
    A Survey and A Multi-Player Multi-Agent Learning Toolbox"). We sometimes use the
    frameworks instead of algorithms or methods because these frameworks are not targeted
    to a specific reinforcement learning algorithm, and they are more like a distributed
    framework for various reinforcement learning methods. Generally, there are mainly
    three parts for a basic DDRL algorithm, which forms a single player single agent
    DDRL method:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 许多DDRL算法或框架已经开发，代表性如GORILA [[8](#bib.bib8)]，A3C [[32](#bib.bib32)]，APE-X [[33](#bib.bib33)]，IMPALA
    [[10](#bib.bib10)]，分布式PPO [[34](#bib.bib34)]，R2D2 [[35](#bib.bib35)]和Seed RL [[9](#bib.bib9)]，基于这些，我们可以绘制出DDRL的关键组成部分，如图[1](#S3.F1
    "图 1 ‣ 3.1 分类基础 ‣ 3 分布式深度强化学习分类 ‣ 分布式深度强化学习：综述及多玩家多智能体学习工具箱")所示。我们有时使用框架而不是算法或方法，因为这些框架并不针对特定的强化学习算法，更像是用于各种强化学习方法的分布式框架。一般而言，一个基本的DDRL算法主要由三个部分组成，形成单玩家单智能体DDRL方法：
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Actors: produce data (trajectories or gradients) by interacting with the environment.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 演员：通过与环境互动产生数据（轨迹或梯度）。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Learners: consume data (trajectories or gradients) to perform neural network
    parameters updating.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习者：消耗数据（轨迹或梯度）以执行神经网络参数更新。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Coordinators: coordinate data (parameters or trajectories) to control the communication
    between learners and actors.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 协调者：协调数据（参数或轨迹）以控制学习者与演员之间的通信。
- en: '![Refer to caption](img/43170d0698873fc512872343d93df0ae.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/43170d0698873fc512872343d93df0ae.png)'
- en: 'Figure 1: Basic framework of DDRL.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：DDRL的基本框架。
- en: Actors pull neural network parameters from the learners, receive states from
    the environments, and perform inference to obtain actions, which drive the dynamics
    of environments to the next states. By repeating the above process with more than
    one actor, data throughput can be increased and enough data can be collected.
    Learners pull data from actors, perform gradients calculation or post-processing,
    and update the network parameters. More than one learner can alleviate the limited
    storage of a GPU by utilizing multiple GPUs with tools such as ring allreduce
    or parameter-server [[11](#bib.bib11)]. By repeating above process, the final
    reinforcement learning agent can be obtained.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 演员从学习者那里拉取神经网络参数，从环境中接收状态，并进行推理以获得动作，这些动作推动环境的动态到下一个状态。通过重复以上过程，数据吞吐量可以增加，并收集到足够的数据。学习者从演员那里拉取数据，进行梯度计算或后处理，并更新网络参数。多个学习者可以通过利用多个GPU和工具（如ring
    allreduce或parameter-server [[11](#bib.bib11)]）缓解GPU存储的限制。通过重复上述过程，可以获得最终的强化学习代理。
- en: Coordinators are important for the DDRL algorithms, which control the communication
    between learners and actors. For example, when the coordinators are used to synchronize
    the parameters updating and pulling (by actors), the DDRL algorithm is synchronous.
    When the parameters updating and pulling (by actors) are not strictly coordinated,
    the DDRL algorithm is asynchronous. So a basic classification of DDRL algorithms
    can be based on the coordinators types.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 协调器对于DDRL算法非常重要，它们控制学习者和演员之间的通信。例如，当协调器用于同步参数更新和拉取（由演员进行）时，DDRL算法是同步的。当参数更新和拉取（由演员进行）没有严格协调时，DDRL算法是异步的。因此，DDRL算法的基本分类可以基于协调器类型。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Synchronous: global policy parameters updating is synchronized, and pulling
    policy parameters (by actors) is synchronous, i.e., different actors share the
    same latest global policy.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同步：全局策略参数更新是同步的，演员拉取策略参数也是同步的，即不同的演员共享相同的最新全局策略。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Asynchronous: Updating the global policy parameters is asynchronous, or policy
    updating (by learners) and pulling (by actors) are asynchronous, i.e., actors
    and learners usually have different policy parameters.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 异步：全局策略参数的更新是异步的，或者策略更新（由学习者进行）和拉取（由演员进行）是异步的，即演员和学习者通常具有不同的策略参数。
- en: 'With the above basic framework, a single player single agent DDRL algorithm
    can be designed. However, when facing multiple agents or multiple players, the
    basic framework is unable to train usable RL agents. Based on current DDRL algorithms
    that support large system level AI such as AlphaStar [[7](#bib.bib7)], OpenAI
    Five [[6](#bib.bib6)] and JueWU [[36](#bib.bib36)], two key components are essential
    to build multiple players and multiple agents DDRL, i.e., agents cooperation and
    players evolution, as shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3.1 Taxonomic Basis
    ‣ 3 Taxonomy of Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement
    Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '在上述基本框架的基础上，可以设计单玩家单代理的DDRL算法。然而，当面对多个代理或多个玩家时，基本框架无法训练出可用的RL代理。基于当前支持大型系统级AI的DDRL算法，如AlphaStar
    [[7](#bib.bib7)]、OpenAI Five [[6](#bib.bib6)] 和JueWU [[36](#bib.bib36)]，构建多个玩家和多个代理的DDRL需要两个关键组件，即代理合作和玩家进化，如图[2](#S3.F2
    "Figure 2 ‣ 3.1 Taxonomic Basis ‣ 3 Taxonomy of Distributed Deep Reinforcement
    Learning ‣ Distributed Deep Reinforcement Learning: A Survey and A Multi-Player
    Multi-Agent Learning Toolbox")所示。'
- en: '![Refer to caption](img/34376b5163a408a7bc3cd330c2b62080.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/34376b5163a408a7bc3cd330c2b62080.png)'
- en: 'Figure 2: Single player single agent DDRL to multiple players multiple agents
    DDRL.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：从单玩家单代理DDRL到多个玩家多个代理DDRL。
- en: '![Refer to caption](img/9bae8982aa7f3b78fb03de29068afeaf.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9bae8982aa7f3b78fb03de29068afeaf.png)'
- en: 'Figure 3: The taxonomy of distributed deep reinforcement learning.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：分布式深度强化学习的分类。
- en: Module of agents cooperation is used to train multiple agents based on multi-agent
    reinforcement learning algorithms [[18](#bib.bib18)]. Generally, multi-agent reinforcement
    learning can be classified into two categories, i.e., independent training and
    joint training, based on how to perform agents relationship modeling.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 代理合作模块用于基于多代理强化学习算法训练多个代理 [[18](#bib.bib18)]。通常，多代理强化学习可以根据代理关系建模的方式分为两类，即独立训练和联合训练。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Independent training: train each agent independently by considering other learning
    agents as part of the environment.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 独立训练：通过将其他学习代理视为环境的一部分来独立训练每个代理。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Joint training: train all the agents as a whole, considering factors such as
    agents communication, reward assignment, and centralized training with distributed
    execution.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联合训练：整体训练所有代理，考虑诸如代理通信、奖励分配和集中式训练与分布式执行等因素。
- en: 'Module of players evolution is designed for agents iteration for each player,
    where agents of other players are learning at the same time, leading to more than
    one generation of agents to be learned for each player like in AlphaStar, and
    OpenAI Five. Based on current mainstream players evolution techniques, players
    evolution can be divided into two types:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家进化模块设计用于每个玩家的代理迭代，其中其他玩家的代理同时学习，导致每个玩家需要学习多个代的代理，如AlphaStar和OpenAI Five。根据当前主流的玩家进化技术，玩家进化可以分为两种类型：
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Self-play based: different players share the same policy networks, and the
    player updates the current generation of policy by confronting its past versions.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自我对抗基于：不同玩家共享相同的策略网络，玩家通过与其过去版本对抗来更新当前代的策略。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Population-play based: different players have different policy networks, or
    called populations, and a player updates its current generation of policy by confronting
    other players or/and its past versions.'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于种群对抗：不同玩家拥有不同的策略网络，或称为种群，玩家通过与其他玩家或/和其过去版本对抗来更新当前代的策略。
- en: 'Finally, based on the above key components for DDRL, the taxonomy of DDRL is
    shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1 Taxonomic Basis ‣ 3 Taxonomy of Distributed
    Deep Reinforcement Learning ‣ Distributed Deep Reinforcement Learning: A Survey
    and A Multi-Player Multi-Agent Learning Toolbox"). In the following, we will summarize
    and compare representative methods based on their main characteristics.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '最终，基于以上DDRL的关键组成部分，DDRL的分类如图[3](#S3.F3 "Figure 3 ‣ 3.1 Taxonomic Basis ‣ 3
    Taxonomy of Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement
    Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox")所示。接下来，我们将总结并比较基于主要特征的代表性方法。'
- en: 3.2 Coordinators Types
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 协调器类型
- en: 'Based on the coordinators types, DDRL algorithms can be divided into asynchronous
    based and synchronous based. For a asynchronous based DDRL method, there are two
    cases: the updating of global policy parameters is asynchronous; the global policy
    parameters updating (by learners) and pulling (by actors) are asynchronous. For
    a synchronous based DDRL method, global policy parameters updating is synchronized,
    and pulling policy parameters (by actors) is synchronous.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据协调器类型，DDRL算法可以分为基于异步和基于同步的。对于异步基于的DDRL方法，有两种情况：全局策略参数的更新是异步的；全局策略参数更新（由学习者）和拉取（由演员）是异步的。对于同步基于的DDRL方法，全局策略参数更新是同步的，拉取策略参数（由演员）是同步的。
- en: 3.2.1 Asynchronous based
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 基于异步的
- en: 'Nair et al. [[8](#bib.bib8)] proposed probably the first massively distributed
    architecture for deep reinforcement learning, Gorila, which builds the basis of
    the succeeding DDRL algorithms. As shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.2.1
    Asynchronous based ‣ 3.2 Coordinators Types ‣ 3 Taxonomy of Distributed Deep Reinforcement
    Learning ‣ Distributed Deep Reinforcement Learning: A Survey and A Multi-Player
    Multi-Agent Learning Toolbox"), a distributed deep Q-Network (DQN) algorithm is
    implemented. Apart from the basic DQN algorithm that mains a Q network and a target
    Q network, the distribution lies in: parallel actors to generate trajectories
    and send them to the Q network and target Q network of the learners, and learners
    to calculate gradients for parameters updating based on a parameter server tool
    that can store a distributed neural network. The algorithm is asynchronous because
    neural network parameters updating of learners and trajectories collecting of
    actors are asynchronously performed without waiting. In their paper, the implemented
    distributed DQN reduces the wall-time required to achieve compared or super results
    by an order of magnitude on most 49 games in Atari compared to non-distributed
    DQN.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Nair等人[[8](#bib.bib8)]提出了可能是第一个用于深度强化学习的广泛分布式架构Gorila，为后续的DDRL算法奠定了基础。如图[4](#S3.F4
    "图4 ‣ 3.2.1 异步基础 ‣ 3.2 协调器类型 ‣ 3 深度分布式强化学习分类 ‣ 深度分布式强化学习：调查与多玩家多代理学习工具箱")所示，实现了一个分布式深度Q网络（DQN）算法。除了基本的DQN算法，主要包括一个Q网络和一个目标Q网络外，分布在于：并行的演员生成轨迹并将其发送到学习者的Q网络和目标Q网络，而学习者则基于能够存储分布式神经网络的参数服务器工具计算梯度以更新参数。该算法是异步的，因为学习者的神经网络参数更新和演员的轨迹收集是异步进行的，无需等待。在他们的论文中，实现的分布式DQN将大多数49个Atari游戏所需的墙时间与非分布式DQN相比减少了一个数量级。
- en: '![Refer to caption](img/9bb677a52e7144a0e9834eb78d50467e.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9bb677a52e7144a0e9834eb78d50467e.png)'
- en: 'Figure 4: Basic framework of Gorila.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Gorila的基本框架。
- en: Similar with [[8](#bib.bib8)], Horgan et al. [[33](#bib.bib33)] introduced distributed
    prioritized experience replay, i.e., APE-X, to enhance the Q-learning based distributed
    reinforcement learning. Specifically, prioritized experience replay is used to
    sample the most important trajectories, which are generated by all the actors.
    Accordingly, a shared experience replay memory should be introduced to store all
    the generated trajectories. In the experiments, a fraction of the wall-clock training
    time is achieved on the Arcade Learning Environment. To further enhance [[33](#bib.bib33)],
    Kapturowski et al. [[35](#bib.bib35)] proposed recurrent experience replay in
    distributed reinforcement learning, i.e., R2D2, by introducing RNN-based reinforcement
    learning agents. The authors investigate the effects of parameter lag and recurrent
    state staleness problems on the performance, obtaining the first agent to exceed
    human-level performance in 52 of the 57 Atari games with the designed training
    strategy.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[8](#bib.bib8)]类似，Horgan等人[[33](#bib.bib33)]引入了分布式优先经验回放，即APE-X，以增强基于Q学习的分布式强化学习。具体而言，优先经验回放用于采样由所有演员生成的最重要的轨迹。因此，应引入共享经验回放记忆以存储所有生成的轨迹。在实验中，在Arcade
    Learning Environment上实现了一部分实际训练时间。为了进一步增强[[33](#bib.bib33)]，Kapturowski等人[[35](#bib.bib35)]提出了分布式强化学习中的递归经验回放，即R2D2，采用了基于RNN的强化学习代理。作者研究了参数滞后和递归状态过时问题对性能的影响，获得了在57个Atari游戏中有52个超越人类水平的第一个代理，并采用了设计的训练策略。
- en: 'Mnih et. al [[32](#bib.bib32)] proposed Asynchronous Advantage Actor-Critic
    (A3C) framework, which can make full use of the multi-core CPU instead of the
    GPU, leading to cheap distribution of reinforcement learning algorithm. As shown
    in Fig. [5](#S3.F5 "Figure 5 ‣ 3.2.1 Asynchronous based ‣ 3.2 Coordinators Types
    ‣ 3 Taxonomy of Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement
    Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox"), each actor
    calculates gradient of the samples (mainly states, actions and rewards used for
    regular reinforcement learning algorithms), send them to the learners, and then
    update the global policy. The updating is asynchronous without synchronization
    among gradients from different actors. Besides, parameters (maybe not the latest
    version) are pulled by each actor to generate data with environments. In their
    paper, four specific reinforcement learning algorithms are established, i.e.,
    asynchronous one-step Q-learning, asynchronous one-step Sarsa, asynchronous n-step
    Q-learning and asynchronous advantage actor-critic. Experiments show that half
    the time on a single multi-core CPU instead of a GPU is obtained on the Atari
    domain.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'Mnih等人[[32](#bib.bib32)]提出了异步优势演员-评论家（A3C）框架，该框架可以充分利用多核CPU而不是GPU，从而实现对强化学习算法的廉价分布。如[图5](#S3.F5
    "Figure 5 ‣ 3.2.1 Asynchronous based ‣ 3.2 Coordinators Types ‣ 3 Taxonomy of
    Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement Learning:
    A Survey and A Multi-Player Multi-Agent Learning Toolbox")所示，每个演员都计算样本的梯度（主要是用于常规强化学习算法的状态、动作和奖励），然后将它们发送给学习者，然后更新全局策略。更新是异步进行的，不需要对来自不同演员的梯度进行同步。此外，每个演员还会拉取参数（可能不是最新版本）以生成与环境相关的数据。在他们的论文中，他们建立了四种具体的强化学习算法，即异步一步Q-learning、异步一步Sarsa、异步n步Q-learning和异步优势演员-评论家算法。实验表明，在Atari领域中，与GPU相比，单个多核CPU上的时间减少了一半。'
- en: '![Refer to caption](img/9ba0d2546889bdc577068a587238f14a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/9ba0d2546889bdc577068a587238f14a.png)'
- en: 'Figure 5: Basic framework of A3C.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：A3C的基本架构。
- en: 'To make use of the GPU’s computational power instead of just the multi-core
    CPU as in A3C, Babaeizadeh et al. [[37](#bib.bib37)] proposed asynchronous advantage
    actor-critic on a gpu, i.e., GA3C, which is a hybrid CPU/GPU version of the A3C.
    As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.2.1 Asynchronous based ‣ 3.2 Coordinators
    Types ‣ 3 Taxonomy of Distributed Deep Reinforcement Learning ‣ Distributed Deep
    Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox"),
    the learner consists of three parts: predictor to dequeue prediction requests
    and obtain actions by the inference, trainer to dequeue batches of trajectories
    for the agent model, and the agent model to update the parameters with the trajectories.
    Noted that the threads of predictor and trainer are asynchronously executed. With
    the above multi-process, multi-thread CPU for environments rollout and a GPU,
    GA3C achieves a significant speed up compared to A3C.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '为了充分利用GPU的计算能力而不仅仅是像A3C中那样使用多核CPU，Babaeizadeh等人[[37](#bib.bib37)]提出了异步优势演员-评论家GPU版本，即GA3C，它是A3C的混合CPU/GPU版本。如[图6](#S3.F6
    "Figure 6 ‣ 3.2.1 Asynchronous based ‣ 3.2 Coordinators Types ‣ 3 Taxonomy of
    Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement Learning:
    A Survey and A Multi-Player Multi-Agent Learning Toolbox")所示，学习者包括三个部分：预测者负责出队预测请求，并通过推理得到动作；训练者负责出队路径批次以供代理模型使用；而代理模型用于使用路径更新参数。需要注意的是，预测者和训练者的线程是异步执行的。通过上述多进程、多线程CPU执行环境卷积和一个GPU的结合，与A3C相比，GA3C实现了显著的加速。'
- en: '![Refer to caption](img/00c8f0c864cc14f36d0c7e5842dd01bf.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/00c8f0c864cc14f36d0c7e5842dd01bf.png)'
- en: 'Figure 6: Basic framework of GA3C.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：GA3C的基本架构。
- en: 'Placing gradient calculation in the actor side will limit the data throughput
    of the whole DDRL system, i.e., trajectories collected per time unit, so Espeholt
    et al. [[10](#bib.bib10)] proposed Importance Weighted Actor-Learner Architecture
    (IMPALA) to alleviate this problem. As shown in [7](#S3.F7 "Figure 7 ‣ 3.2.1 Asynchronous
    based ‣ 3.2 Coordinators Types ‣ 3 Taxonomy of Distributed Deep Reinforcement
    Learning ‣ Distributed Deep Reinforcement Learning: A Survey and A Multi-Player
    Multi-Agent Learning Toolbox"), parallel actors communicate with environments,
    collect trajectories, and send them to the learners for parameters updating. Since
    gradients calculation is put in the learners side, which can be accelerated with
    GPUs, the framework is claimed to scale to thousands of machines without sacrificing
    data efficiency. Considering that the local policy used to generate trajectories
    are behind the global policy in the learners due to the asynchrony between learner
    and actors, a V-trace off-policy actor-critic algorithm is introduced to correct
    the harmful discrepancy. Experiments on DMLab-30 and Atari-57 show that IMPALA
    can achieve better performance with less data compared with previous agents.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '将梯度计算放在演员端会限制整个 DDRL 系统的数据吞吐量，即每时间单位收集的轨迹数量，因此 Espeholt 等人 [[10](#bib.bib10)]
    提出了重要性加权演员-学习者架构（IMPALA）以缓解这一问题。如图 [7](#S3.F7 "Figure 7 ‣ 3.2.1 Asynchronous based
    ‣ 3.2 Coordinators Types ‣ 3 Taxonomy of Distributed Deep Reinforcement Learning
    ‣ Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent
    Learning Toolbox") 所示，多个并行演员与环境进行通信，收集轨迹，并将其发送给学习者以更新参数。由于梯度计算放在学习者端，可以通过 GPU
    加速，因此该框架被称为可以扩展到数千台机器而不牺牲数据效率。考虑到生成轨迹所用的本地策略由于学习者和演员之间的异步性而落后于学习者中的全局策略，引入了一种
    V-trace 离策略演员-评论员算法来修正有害的差异。在 DMLab-30 和 Atari-57 上的实验表明，IMPALA 能够在数据较少的情况下实现比以前的智能体更好的性能。'
- en: '![Refer to caption](img/a6530023dd09adebc4f018cb28206875.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/a6530023dd09adebc4f018cb28206875.png)'
- en: 'Figure 7: Basic framework of impala.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：IMPALA 的基本框架。
- en: 'By using synchronized sampling strategy for actors instead of the independent
    sampling of IMPALA, Stooke and Abbeel [[38](#bib.bib38)] proposed a novel accelerated
    method, which consists of two main parts, i.e, synchronized sampling and synchronous/asynchronous
    multi-GPU optimization. As shown in [8](#S3.F8 "Figure 8 ‣ 3.2.1 Asynchronous
    based ‣ 3.2 Coordinators Types ‣ 3 Taxonomy of Distributed Deep Reinforcement
    Learning ‣ Distributed Deep Reinforcement Learning: A Survey and A Multi-Player
    Multi-Agent Learning Toolbox"), individual observations of each environment are
    gathered into a batch for inference, which largely reduce the inference times
    compared with approaches that generate trajectories for each environment independently.
    However, such synchronized sampling may suffer from slowdown when different environments
    in different processes have large execution differences, which is alleviated by
    tricks such as allocating available CPU cores used for environments evenly. As
    for the learners, they server as a parameter server, whose parameters are pushed
    by actors, and then updated asynchronously among other actors. The implemented
    asynchronous version of PPO, i.e., APPO, learn successful policies in Acari games
    in mere minutes.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '通过使用同步采样策略代替 IMPALA 的独立采样，Stooke 和 Abbeel [[38](#bib.bib38)] 提出了一个新颖的加速方法，该方法包括两个主要部分，即同步采样和同步/异步多
    GPU 优化。如图 [8](#S3.F8 "Figure 8 ‣ 3.2.1 Asynchronous based ‣ 3.2 Coordinators Types
    ‣ 3 Taxonomy of Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement
    Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox") 所示，将每个环境的个体观察收集到一个批次中进行推断，与为每个环境独立生成轨迹的方法相比，这大大减少了推断次数。然而，当不同进程中的不同环境具有较大的执行差异时，这种同步采样可能会受到减速影响，通过均匀分配用于环境的可用
    CPU 核心等技巧可以缓解这一问题。对于学习者来说，它们充当参数服务器，其参数由演员推送，然后在其他演员之间异步更新。实现的 PPO 异步版本，即 APPO，在
    Acari 游戏中仅需几分钟即可学习成功的策略。'
- en: '![Refer to caption](img/4fc45ac60fcafdddcc2a504c09b03ad6.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/4fc45ac60fcafdddcc2a504c09b03ad6.png)'
- en: 'Figure 8: Basic framework of APPO.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：APPO 的基本框架。
- en: 'With the above synchronized sampling in [[38](#bib.bib38)], inference times
    will be largely reduced, but the communication burden between learners and actors
    will be a big problem when the networks are huge. Espeholt et al. [[9](#bib.bib9)]
    proposed Scalable, Efficient, Deep-RL (SEEDRL), which features centralized inference
    and an optimized communication layer called gRPC. As shown in Fig. [9](#S3.F9
    "Figure 9 ‣ 3.2.1 Asynchronous based ‣ 3.2 Coordinators Types ‣ 3 Taxonomy of
    Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement Learning:
    A Survey and A Multi-Player Multi-Agent Learning Toolbox"), the communication
    between learners and actors are mere states and actions, which will reduce latency
    with the proposed high performance RPC library gRPC. The authors implemented policy
    gradients and Q-learning based algorithms and tested them on the Atari-57, DeepMind
    Lab and Google Research Football environments, and a 40% to 80% cost reduction
    is obtained, showing great improvements.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '通过[[38](#bib.bib38)]中的同步采样，推理时间将大大减少，但当网络规模巨大时，学习者和执行者之间的通信负担将成为一个大问题。Espeholt
    等人[[9](#bib.bib9)]提出了可扩展的、高效的深度强化学习（SEEDRL），其特点是集中推理和名为 gRPC 的优化通信层。如图 [9](#S3.F9
    "Figure 9 ‣ 3.2.1 Asynchronous based ‣ 3.2 Coordinators Types ‣ 3 Taxonomy of
    Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement Learning:
    A Survey and A Multi-Player Multi-Agent Learning Toolbox") 所示，学习者和执行者之间的通信仅为状态和动作，这将通过提出的高性能
    RPC 库 gRPC 减少延迟。作者实现了基于策略梯度和 Q 学习的算法，并在 Atari-57、DeepMind Lab 和 Google Research
    Football 环境中进行了测试，获得了 40% 到 80% 的成本降低，显示了显著的改进。'
- en: '![Refer to caption](img/f1341e47b150981bdd29806d610ac8e4.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f1341e47b150981bdd29806d610ac8e4.png)'
- en: 'Figure 9: Basic framework of SEEDRL.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：SEEDRL 的基本框架。
- en: 3.2.2 Synchronous based
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 基于同步
- en: 'As an alternative to asynchronous advantage actor-critic (A3C), Clemente et
    al. [[39](#bib.bib39)] found that a synchronous version, i.e., advantage actor-critic
    (A2C), can better use the GPU resources, which should perform well with more actors.
    In the implementation of A2C, i.e., PAAC, a coordinator is utilized to wait for
    all gradients of the actors before optimizing the global network. As shown in
    Fig. [10](#S3.F10 "Figure 10 ‣ 3.2.2 Synchronous based ‣ 3.2 Coordinators Types
    ‣ 3 Taxonomy of Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement
    Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox"), learners
    update the policy parameters before all the trajectories are collected, i.e.,
    the job of actors is done, and when the learners are updating the policy, the
    trajectory sampling is stopped. As a result, all actors are coordinated to obtain
    the same global network to interact with environments in the following steps.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '作为异步优势演员-评论家（A3C）的替代方案，Clemente 等人[[39](#bib.bib39)]发现同步版本，即优势演员-评论家（A2C），可以更好地利用
    GPU 资源，这在有更多演员的情况下表现良好。在 A2C 的实现中，即 PAAC，使用协调器等待所有演员的梯度，然后优化全局网络。如图 [10](#S3.F10
    "Figure 10 ‣ 3.2.2 Synchronous based ‣ 3.2 Coordinators Types ‣ 3 Taxonomy of
    Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement Learning:
    A Survey and A Multi-Player Multi-Agent Learning Toolbox") 所示，学习者在所有轨迹被收集之前更新策略参数，即演员的工作完成了，当学习者更新策略时，轨迹采样被停止。因此，所有演员被协调以获得相同的全局网络，在接下来的步骤中与环境进行交互。'
- en: '![Refer to caption](img/b705c8f0075d5c889a28cdb0c5fbe4d7.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b705c8f0075d5c889a28cdb0c5fbe4d7.png)'
- en: 'Figure 10: Basic framework of PAAC.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：PAAC 的基本框架。
- en: 'As an alternative of advantage actor-critic (A2C) algorithm in handling continuous
    action space, PPO algorithm [[40](#bib.bib40)] shows great potential due to its
    trust region constraint. Heess et al. [[34](#bib.bib34)] proposed large scale
    reinforcement learning with distributed PPO, i.e., DPPO, which has both synchronous
    and asynchronous versions, and shows better performance with the synchronous update.
    As shown in Fig. [11](#S3.F11 "Figure 11 ‣ 3.2.2 Synchronous based ‣ 3.2 Coordinators
    Types ‣ 3 Taxonomy of Distributed Deep Reinforcement Learning ‣ Distributed Deep
    Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox"),
    implementation of DPPO is similar to A3C but with synchronization when updating
    the policy neural network. However, the synchronization will limit the throughout
    of the whole system due to different rhythm of the actors. The authors use a threshold
    for the number of actors whose gradients must be available for the learners, which
    makes the algorithm scale to large number of actors.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作为处理连续动作空间的优势演员-评论员（A2C）算法的替代方案，PPO 算法 [[40](#bib.bib40)] 由于其信任区域约束展现出巨大潜力。Heess
    等人 [[34](#bib.bib34)] 提出了分布式 PPO 的大规模强化学习，即 DPPO，该方法有同步和异步两个版本，并且在同步更新时表现更佳。如图
    [11](#S3.F11 "图 11 ‣ 3.2.2 基于同步 ‣ 3.2 协调器类型 ‣ 3 分布式深度强化学习的分类 ‣ 分布式深度强化学习：调查与多玩家多智能体学习工具箱")
    所示，DPPO 的实现类似于 A3C，但在更新策略神经网络时进行了同步。然而，由于演员的节奏不同，同步会限制整个系统的吞吐量。作者为必须对学习者可用的演员梯度数量设定了阈值，这使得算法能够扩展到大量演员。
- en: '![Refer to caption](img/6c53c15b403ea095b6e5731e0383cb75.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6c53c15b403ea095b6e5731e0383cb75.png)'
- en: 'Figure 11: Basic framework of DPPO.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：DPPO 的基本框架。
- en: 'Different from DPPO algorithm that a server is applied for neural networks
    updating, Wijmans et al. [[41](#bib.bib41)] further proposed a decentralized DPPO
    framework, i.e., DDPPO, which exhibits near-liner scaling to the GPUs. As shown
    in Fig. [12](#S3.F12 "Figure 12 ‣ 3.2.2 Synchronous based ‣ 3.2 Coordinators Types
    ‣ 3 Taxonomy of Distributed Deep Reinforcement Learning ‣ Distributed Deep Reinforcement
    Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox"), a learner
    and an actor are bundled together, as a unit, to perform trajectories collection
    and gradients calculation. Then gradients from all the units are gathered together
    through some reduce operations, e.g., ring allreduce, to update the neural networks,
    which make sure that parameters are the same for all the units. Noted that to
    alleviate the synchronization overhead when performing trajectories collection
    in parallel units, similar strategies like in DPPO is used to discard certain
    percentages of trajectories in several units. Experiments show a speedup of 107x
    on 128 GPUs over a serial implementation.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于 DPPO 算法在神经网络更新中使用服务器，Wijmans 等人 [[41](#bib.bib41)] 进一步提出了一种去中心化的 DPPO 框架，即
    DDPPO，该框架表现出接近线性的 GPU 扩展性。如图 [12](#S3.F12 "图 12 ‣ 3.2.2 基于同步 ‣ 3.2 协调器类型 ‣ 3 分布式深度强化学习的分类
    ‣ 分布式深度强化学习：调查与多玩家多智能体学习工具箱") 所示，学习者和演员被捆绑在一起，作为一个单元，进行轨迹收集和梯度计算。然后，通过一些归约操作（例如环形全规约）将所有单元的梯度汇总在一起，以更新神经网络，确保所有单元的参数相同。需要注意的是，为了减轻在并行单元中进行轨迹收集时的同步开销，使用了类似于
    DPPO 的策略，以丢弃几个单元中某些百分比的轨迹。实验表明，在 128 个 GPU 上相较于串行实现，速度提高了 107 倍。
- en: '![Refer to caption](img/71489aea857d3d728f518911716bc241.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/71489aea857d3d728f518911716bc241.png)'
- en: 'Figure 12: Basic framework of DDPPO.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：DDPPO 的基本框架。
- en: 3.2.3 Discussion
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 讨论
- en: Single machine or multiple machines. In the beginning of developing DDRL algorithms,
    researchers make previous non-distributed DRL methods distributed using one machine.
    For example, the parallel of several typical actor-critic algorithms are designed
    to use the multi-process of CPUs, e.g., A3C [[32](#bib.bib32)], and PAAC [[39](#bib.bib39)].
    Lately, researchers aim at improving data throughput of the whole DDRL system,
    e.g., IMPALA [[10](#bib.bib10)], and SEEDRL [[9](#bib.bib9)], which serves as
    a basic infrastructure for training complex games AI like AlphaStar and OpenAI
    Five. These systems usually can make use of multiple machines. However, early
    DDRL algorithms designed for a single machine can also be deployed in multiple
    machines when communications between machines are solved, which is relatively
    simple by using open soured tools (will be introduced in Section 4).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**单机**或**多机**。在开发 DDRL 算法的初期，研究人员将以前的非分布式 DRL 方法使用单台机器进行分布式处理。例如，几个典型的演员-评论家算法的并行设计使用了
    CPU 的多进程，例如 A3C [[32](#bib.bib32)] 和 PAAC [[39](#bib.bib39)]。最近，研究人员致力于提高整个 DDRL
    系统的数据吞吐量，例如 IMPALA [[10](#bib.bib10)] 和 SEEDRL [[9](#bib.bib9)]，这些系统作为训练复杂游戏 AI（如
    AlphaStar 和 OpenAI Five）的基础设施。这些系统通常可以利用多台机器。然而，早期设计为单台机器的 DDRL 算法也可以在多台机器上部署，当机器之间的通信问题解决时，这可以通过使用开源工具相对简单地实现（将在第
    4 节介绍）。'
- en: Exchange trajectories or gradients. Learners and actors serve as basic components
    for DDRL algorithms, and the data communicated between them can be trajectories
    or gradients based on whether to put the gradient calculation on the actor or
    learner side. For example, actors of A3C [[32](#bib.bib32)] are in charge of trajectories
    collection and gradients calculation, and the gradients are then sent to learners
    for policy update, which just make simple operations such as sum operation. Since
    gradients calculation is time-consuming, especially the policy model is large,
    the calculating load between the learners and actors will be unbalanced. Accordingly,
    more and more DDRL algorithms put gradients calculation in the learners side by
    using more suitable devices, i.e., GPUs. For example, in the higher data throughput
    DDRL frameworks like IMPALA [[10](#bib.bib10)], learners are in charge of policy
    updating and actors are in charge of trajectories collection.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**交换**轨迹或**梯度**。学习者和演员是 DDRL 算法的基本组件，它们之间传输的数据可以是轨迹或梯度，这取决于梯度计算是在演员端还是学习者端进行。例如，A3C
    [[32](#bib.bib32)] 的演员负责轨迹收集和梯度计算，然后将梯度发送给学习者以进行策略更新，这只是执行简单的操作，例如求和。由于梯度计算是时间密集型的，尤其是在策略模型很大的情况下，学习者和演员之间的计算负载会不平衡。因此，越来越多的
    DDRL 算法将梯度计算放在学习者端，使用更适合的设备，即 GPU。例如，在像 IMPALA [[10](#bib.bib10)] 这样的高数据吞吐量 DDRL
    框架中，学习者负责策略更新，演员则负责轨迹收集。'
- en: synchronized or independent inference. When actors are collecting trajectories
    by interacting with the environments, actions should be inferred. Basically, when
    performing a step on an environment, there should be one times inference. Previous
    DDRL methods usually maintain an environment for an actor, where action inference
    is performed independently from other actors and environments. With the increasing
    number of environments to collect trajectories, it is resource consuming especially
    only CPUs are used in the actor side. By putting the inference in the GPU side,
    the resources are also largely wasted because the batch size of the inference
    is one. To cope with above problems, plenty of DDRL frameworks use an actor to
    manage several environments, and perform actions inference synchronized. For example,
    APPO [[38](#bib.bib38)] and SEEDRL [[9](#bib.bib9)] introduce synchronization
    to collect states and distribute actions obtained by environments and actor policy,
    respectively.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**同步**或**独立**推断。当演员通过与环境交互来收集轨迹时，应该推断出动作。基本上，在环境上执行一步时，应进行一次推断。以前的 DDRL 方法通常为每个演员维护一个环境，其中的动作推断与其他演员和环境是独立进行的。随着收集轨迹的环境数量增加，这会消耗大量资源，特别是在演员端仅使用
    CPU 的情况下。将推断放在 GPU 端，资源也会大量浪费，因为推断的批量大小是 1。为了解决上述问题，大多数 DDRL 框架使用一个演员管理多个环境，并进行同步的动作推断。例如，APPO
    [[38](#bib.bib38)] 和 SEEDRL [[9](#bib.bib9)] 引入了同步机制，以收集状态并分别分配由环境和演员策略获得的动作。'
- en: Asynchronous or synchronous DDRL. In regarding synchronous based and asynchronous
    based DDRL algorithms, different methods share advantages and disadvantages. For
    asynchronous DDRL algorithms, the global policy usually does not need to wait
    all the trajectories or gradients, and data collection conventionally does not
    need to wait the latest policy parameters. Accordingly, the data throughput of
    the whole DDRL system will be large. However, there exists a lag between the global
    policy and behavior policy, and such a lag is usually a trouble for on-policy
    based reinforcement learning algorithms. DDRL frameworks such as IMPALA [[10](#bib.bib10)]
    introduces V-trace, and GA3C [[37](#bib.bib37)] brings in small term $\varepsilon$
    to alleviate the problem, but those kinds of methods will be unstable when the
    lags are large. For synchronous DDRL algorithms, synchronization among trajectories
    or gradients is required before updating the policy. Accordingly, waiting time
    is wasted for actors or learners when one side is working. However, synchronization
    makes the training stable, and it is easier to be implemented such as DPPO [[34](#bib.bib34)]
    and DDPPO [[41](#bib.bib41)].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 异步或同步的 DDRL。在基于同步和基于异步的 DDRL 算法中，不同的方法各有优缺点。对于异步 DDRL 算法，全球策略通常不需要等待所有的轨迹或梯度，数据收集通常也不需要等待最新的策略参数。因此，整个
    DDRL 系统的数据吞吐量会很大。然而，全球策略和行为策略之间存在延迟，这种延迟通常对基于策略的强化学习算法构成问题。像 IMPALA [[10](#bib.bib10)]
    这样的 DDRL 框架引入了 V-trace，而 GA3C [[37](#bib.bib37)] 引入了小的 $\varepsilon$ 来缓解这个问题，但当延迟较大时，这些方法会变得不稳定。对于同步
    DDRL 算法，在更新策略之前需要在轨迹或梯度之间进行同步。因此，当一方在工作时，演员或学习者的等待时间会被浪费。然而，同步使训练更加稳定，并且实现起来更容易，例如
    DPPO [[34](#bib.bib34)] 和 DDPPO [[41](#bib.bib41)]。
- en: Others. Usually, multiple actors can be implemented with no data exchange, because
    their jobs, i.e., trajectory collection, can be independent. As for learners,
    most methods only maintain one learner, which will be enough due to limited model
    size and especially the limited trajectory batch size. However, large batch size
    is claimed to be important for complex games [[6](#bib.bib6)], and accordingly
    multiple learners become necessary. In the multiple learners case, usually a synchronization
    should be performed before updating the global policy network. Generally, a sum
    operation can handle the synchronization, but it is time consuming when the learners
    are distributed in multiple machines. An optimal choice is proposed in [[41](#bib.bib41)],
    where ring allreduce operation can nicely deal with the synchronization problem,
    and an implementation of [[41](#bib.bib41)] is easy by using toolbox such as Horovod
    [[11](#bib.bib11)]. On the other hand, when the model size is large and a GPU
    can not load the whole model, a parameter-server framework [[8](#bib.bib8), [33](#bib.bib33)]
    based learner can be a choice, which may be combined with the ring allreduce operation
    to handle the large model size and large batch size challenge.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其他。通常，多名演员可以在没有数据交换的情况下实现，因为他们的工作，即轨迹收集，可以是独立的。至于学习者，大多数方法仅维持一个学习者，这通常足够，因为模型大小有限，尤其是轨迹批量大小有限。然而，大批量大小被认为对复杂游戏
    [[6](#bib.bib6)] 很重要，因此多个学习者变得必要。在多个学习者的情况下，通常需要在更新全球策略网络之前进行同步。一般来说，求和操作可以处理同步问题，但当学习者分布在多台机器上时，这会消耗时间。[[41](#bib.bib41)]
    中提出了一种最佳选择，其中环形 allreduce 操作可以很好地解决同步问题，并且通过使用如 Horovod [[11](#bib.bib11)] 这样的工具箱来实现
    [[41](#bib.bib41)] 非常容易。另一方面，当模型大小很大而 GPU 无法加载整个模型时，基于参数服务器框架 [[8](#bib.bib8),
    [33](#bib.bib33)] 的学习者可以成为一种选择，这可以与环形 allreduce 操作结合使用，以应对大模型大小和大批量大小的挑战。
- en: Brief summary. Finally, when a DDRL algorithm is required, how to select a proper
    or efficient method largely rely on the computing resources can be used, the policy
    model resource occupancy, and the environment resource occupancy. If there is
    only one machine with multiple CPU cores and GPUs, no extra communication is required
    except for the data exchange between CPU and GPUs. But, if there are multiple
    machines, the data exchange should be considered, which may be the bottleneck
    of the whole system. When the policy model is large, exchange of model between
    machines is time consuming, so methods such as SEEDRL [[9](#bib.bib9)] is proper
    due to only states and actions being exchanged. However, if the policy model is
    small, frequently exchange the trajectories will be time consuming, and methods
    such as DDPPO [[41](#bib.bib41)] will be a choice. When the environment resource
    occupancy is large, massive resources will be used to start-up environments, and
    limited GPUs maybe competent at the policy updating. Accordingly, DDRL methods
    such as IMPALA [[10](#bib.bib10)] will be suitable because a high data throughput
    can be obtained. Finally, there may be no best DDRL methods for any learning environments,
    and researchers can choose one that best suits their tasks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 简要总结。最后，当需要一个 DDRL 算法时，如何选择一个合适或高效的方法在很大程度上依赖于可用的计算资源、策略模型的资源占用以及环境资源的占用。如果只有一台拥有多个
    CPU 核心和 GPU 的机器，除了 CPU 和 GPU 之间的数据交换外，不需要额外的通信。但如果有多台机器，则需要考虑数据交换，这可能是整个系统的瓶颈。当策略模型很大时，机器之间的模型交换是耗时的，因此诸如
    SEEDRL [[9](#bib.bib9)] 的方法是合适的，因为只交换状态和动作。然而，如果策略模型较小，频繁交换轨迹会耗时，像 DDPPO [[41](#bib.bib41)]
    这样的算法将是一个选择。当环境资源占用很大时，启动环境需要大量资源，而有限的 GPU 可能足以进行策略更新。因此，像 IMPALA [[10](#bib.bib10)]
    这样的 DDRL 方法将是合适的，因为可以获得高数据吞吐量。最后，可能没有适用于任何学习环境的最佳 DDRL 方法，研究人员可以选择最适合其任务的方法。
- en: 3.3 Agents Cooperation Types
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 代理合作类型
- en: 'When confronting single agent reinforcement learning, previous DDRL algorithms
    can be easily used. But, when there are multiple agents, distributed multi-agent
    reinforcement learning algorithms are required to train multiple agents simultaneously.
    Accordingly, previous DDRL algorithms may need to be revised to handle the multiple
    agents case. Based on current training paradigms for multi-agent reinforcement
    learning, agents cooperation types can be classified into two categories, i.e.,
    independent training and joint training, as shown in Fig. [13](#S3.F13 "Figure
    13 ‣ 3.3 Agents Cooperation Types ‣ 3 Taxonomy of Distributed Deep Reinforcement
    Learning ‣ Distributed Deep Reinforcement Learning: A Survey and A Multi-Player
    Multi-Agent Learning Toolbox"). Usually, an agents manager is added to control
    all the agents in a game. Independent training trains each agent by considering
    other learning agents as part of the environment, and joint training trains all
    the agents as a whole by using typical multi-agent reinforcement learning algorithms.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 面对单一代理强化学习时，可以轻松使用之前的 DDRL 算法。但当有多个代理时，需要分布式多代理强化学习算法来同时训练多个代理。因此，之前的 DDRL 算法可能需要修订以处理多代理的情况。基于当前的多代理强化学习训练范式，代理合作类型可以分为两类，即独立训练和联合训练，如图
    [13](#S3.F13 "图 13 ‣ 3.3 代理合作类型 ‣ 3 分布式深度强化学习的分类 ‣ 分布式深度强化学习：调查与多玩家多代理学习工具箱")
    所示。通常，游戏中会添加一个代理管理器来控制所有代理。独立训练通过将其他学习代理视为环境的一部分来训练每个代理，而联合训练则通过使用典型的多代理强化学习算法来整体训练所有代理。
- en: '![Refer to caption](img/02f739e9b985b5c429bb2131d3e9ac73.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/02f739e9b985b5c429bb2131d3e9ac73.png)'
- en: 'Figure 13: Basic framework of agents training.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：代理训练的基本框架。
- en: 3.3.1 Independent training
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 独立训练
- en: Independent training makes a $n$ agents training as a training of $n$ independent
    training, and accordingly previous DDRL algorithms can be used with a few modifications.
    The agents manager is mainly used to bring other agents’ information, e.g., actions,
    into current DDRL training of an agent, because dynamic of an environment should
    be driven by all the agents. Considering the requirement of agents cooperation,
    independent training makes more contribution on how to promote cooperation among
    independent agents.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 独立训练将$n$个智能体的训练视为$n$次独立训练的训练，因此，之前的DDRL算法可以通过少量修改后继续使用。智能体管理器主要用于将其他智能体的信息（例如，动作）引入当前智能体的DDRL训练中，因为环境的动态应该由所有智能体驱动。考虑到智能体合作的要求，独立训练在促进独立智能体之间的合作方面贡献更多。
- en: Jaderberg et al. [[42](#bib.bib42)] proposed FTW agents for Quake III Arena
    in Capture the Flag (CTF) mode, where several agents cooperate to fight another
    camp. To train scalable agents that can cooperate with any other agents even for
    unseen agents, the authors train agents independently, where a population of independent
    agents are trained concurrently, with each participating thousands of parallel
    matches. To handle thousands of parallel environments, an IMPALA [[10](#bib.bib10)]
    based framework is used¹¹1Mainly based on their codes released.. As for the cooperation
    problem, the authors design rewards based on several marks between the agents
    cooperated, so as to promote the emergence of cooperation. More specifically,
    All the agents share the same final global reward, i.e., win or lose. Besides,
    intermediate rewards are learned based on several events that considering teammates’
    action such as teammates capturing the flag, and teammate picking up the flag.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Jaderberg等人[[42](#bib.bib42)] 为《Quake III Arena》的夺旗（CTF）模式提出了FTW智能体，其中几个智能体合作对抗另一个阵营。为了训练可以与任何其他智能体（即使是未见过的智能体）合作的可扩展智能体，作者独立训练智能体，其中一群独立智能体同时训练，每个智能体参与数千场并行比赛。为了处理数千个并行环境，使用了基于IMPALA
    [[10](#bib.bib10)] 的框架¹¹主要基于他们发布的代码。关于合作问题，作者基于智能体之间的几个标记设计了奖励，以促进合作的出现。更具体地说，所有智能体共享相同的最终全球奖励，即赢或输。此外，基于几个考虑队友动作的事件，如队友抓取旗帜和队友捡起旗帜，学习中间奖励。
- en: Berner et al. [[6](#bib.bib6)] proposed OpenAI Five for Dota2, where five heroes
    cooperate together to fight another cooperated five heroes. In their AI, each
    hero is modeled as an agent and trained independently. To deal with large parallel
    environments so as to generate a batch size of more than a million of time steps,
    a SEEDRL [[9](#bib.bib9)] framework is used. Unlike [[42](#bib.bib42)] utilizing
    different policy networks for different agents, OpenAI Five uses the same policy
    for different agents, which may promote the emergence of cooperation. The actions
    differences lie in the features designing, where different agents in Dota2 share
    almost the same features but with specific features such as hero ID. Finally,
    similar with [[42](#bib.bib42)] that designs rewards to promote cooperation, the
    authors use a weighted sum of individual and team rewards, which are given by
    following experience of human players, e.g., gaining resources, and killing enemies.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Berner等人[[6](#bib.bib6)] 提出了OpenAI Five用于Dota2，其中五个英雄合作对抗另五个合作的英雄。在他们的AI中，每个英雄被建模为一个智能体，并且是独立训练的。为了处理大型并行环境并生成超过一百万个时间步的批量大小，使用了SEEDRL
    [[9](#bib.bib9)] 框架。与[[42](#bib.bib42)] 使用不同策略网络用于不同智能体不同，OpenAI Five为不同智能体使用相同策略，这可能促进合作的出现。动作的差异在于特征设计，其中Dota2中的不同智能体几乎共享相同的特征，但具有特定的特征，如英雄ID。最后，与[[42](#bib.bib42)]
    设计奖励以促进合作类似，作者使用了个人奖励和团队奖励的加权和，这些奖励是通过遵循人类玩家的经验而给出的，例如，获得资源和击杀敌人。
- en: Ye et al. [[36](#bib.bib36)] proposed JueWu²²2A recognized name. for Honor of
    Kings, which is a similar game compared to Dota2 but played in mobile devices
    instead of computer devices. Like in [[6](#bib.bib6)], a SEEDRL [[9](#bib.bib9)]
    framework is adopted. Besides, the authors also use the same policy for different
    agents as in [[6](#bib.bib6)]. The policy network is kind of different, where
    five value heads are used due to a deeper consideration of the game characteristics.
    Key difference between [[6](#bib.bib6)] is the training paradigm used to scale
    to a large number of heroes, which is not the main scope of this paper, and researchers
    can refer to the original paper for more details.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Ye 等人 [[36](#bib.bib36)] 提出了 JueWu²²2A，荣誉之王中的一个识别名称，这是一款与 Dota2 类似但在移动设备上进行的游戏。与
    [[6](#bib.bib6)] 类似，采用了 SEEDRL [[9](#bib.bib9)] 框架。此外，作者还使用了与 [[6](#bib.bib6)]
    相同的策略用于不同的智能体。策略网络有所不同，由于对游戏特征的深入考虑，使用了五个价值头。与 [[6](#bib.bib6)] 的主要区别在于训练范式，用于扩展到大量的英雄，这不是本文的主要范围，研究人员可以参考原始论文以获取更多细节。
- en: Zha et al. [[43](#bib.bib43)] proposed DouZero for DouDiZhu, where a Landlord
    agent and two Peasant agents are confronting for a win. Three agents using three
    policy networks are trained independently like in [[42](#bib.bib42)]. A Gorila
    [[8](#bib.bib8)] based DDRL algorithm is used for the three agents learning in
    a single server. Cooperation between the Peasants agents emerges with the increasing
    of training epochs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Zha 等人 [[43](#bib.bib43)] 提出了 DouZero，用于斗地主，其中一个地主智能体和两个农民智能体进行对抗。三个智能体使用三个策略网络独立训练，如
    [[42](#bib.bib42)] 所示。使用基于 Gorila [[8](#bib.bib8)] 的 DDRL 算法在单个服务器上进行三个智能体的学习。随着训练轮次的增加，农民智能体之间的合作逐渐显现。
- en: Baker et al. [[44](#bib.bib44)] proposed multi-agent autocurricula for game
    hide-and-seek to study the emergent tool use. Like in [[6](#bib.bib6)], a SEEDRL
    [[9](#bib.bib9)] framework is used, and the same policy for different agents are
    used for training. Besides, the authors test using distinct policies for different
    agents, showing similar results but reduced sample efficiency.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Baker 等人 [[44](#bib.bib44)] 提出了用于游戏捉迷藏的多智能体自适应课程，以研究工具的涌现使用。与 [[6](#bib.bib6)]
    类似，采用了 SEEDRL [[9](#bib.bib9)] 框架，并对不同的智能体使用相同的策略进行训练。此外，作者测试了对不同智能体使用不同策略的效果，结果显示相似但样本效率有所降低。
- en: 3.3.2 Joint training
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 联合训练
- en: Joint training trains all the agents as a whole using typical multi-agent reinforcement
    learning algorithms like a single agent. The difference is the trajectories collected,
    which have all the agents’ data instead of just an agent. The agents manager can
    be designed to handle multi-agent issues, such as communication, and coordination,
    to further accelerate training. However, current multi-agent DDRL algorithms only
    consider a simple way, i.e., actor parallelization to collect enough trajectories.
    Accordingly, most previous DDRL algorithms can be easily implemented.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 联合训练将所有智能体作为一个整体使用典型的多智能体强化学习算法进行训练，就像单智能体训练一样。不同之处在于收集的轨迹，其中包含所有智能体的数据而不仅仅是一个智能体。智能体管理器可以设计成处理多智能体问题，例如通信和协调，以进一步加速训练。然而，当前的多智能体
    DDRL 算法仅考虑一种简单的方法，即演员并行化以收集足够的轨迹。因此，大多数先前的 DDRL 算法可以很容易地实现。
- en: The implementation of QMIX [[45](#bib.bib45)], a popular Q value factorisation
    based multi-agent reinforcement learning algorithm, is implemented using multi-processing
    to interact with the environments [[46](#bib.bib46)]. Another example is the RLlib
    [[13](#bib.bib13)], a part of the open source Ray project [[12](#bib.bib12)],
    which makes abstractions for DDRL and implements several jointly trained multi-agent
    reinforcement learning algorithms, e.g., QMIX and PPO with centralized critic.
    Generally speaking, joint training is similar with single agent training in the
    field of DDRL, but consideration of parallelized training for issues such as communication
    and coordination among agents, the training speed may further accelerated.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: QMIX [[45](#bib.bib45)] 的实现是一个流行的基于 Q 值分解的多智能体强化学习算法，通过多处理实现与环境的交互 [[46](#bib.bib46)]。另一个例子是
    RLlib [[13](#bib.bib13)]，这是开源 Ray 项目 [[12](#bib.bib12)] 的一部分，它为 DDRL 提供了抽象，并实现了多个联合训练的多智能体强化学习算法，例如带有集中式评论员的
    QMIX 和 PPO。一般来说，联合训练在 DDRL 领域与单智能体训练类似，但考虑到并行化训练的问题，如智能体间的通信和协调，训练速度可能会进一步加快。
- en: 3.3.3 Discussion
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 讨论
- en: As for independent training, even though different agents are trained independently,
    different methods take into account problems such as the feature engineering,
    and reward reshaping, so as to promote cooperation. Since different agents are
    trained by making other agents as part of the environment, conventional DDRL algorithms
    can be used without many modifications. From the successful agents such as OpenAI
    Five and JueWu, we can see that SeedRL or its revised versions are a good choice.
    As for joint training, it is far from satisfactory, because there is a huge room
    to improve parallelism among agents by properly considering the multi-agent issues
    such as communication when designing actors and learners.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关于独立训练，即使不同的代理被独立训练，不同的方法也会考虑特征工程、奖励重塑等问题，以促进合作。由于不同的代理通过将其他代理作为环境的一部分进行训练，因此可以在没有太多修改的情况下使用传统的DDR算法。从成功的代理，如OpenAI
    Five和JueWu，我们可以看到SeedRL或其修订版本是一个不错的选择。至于联合训练，目前还远未令人满意，因为在设计演员和学习者时，需要适当地考虑多代理问题，例如通信，以提高代理之间的并行性。
- en: 3.4 Player Evolution Types
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 玩家进化类型
- en: 'In most case, we have no opponents to drive the capacity growth for a player³³3Here
    player means the a side for a game, which may controls one agent like Go or multiple
    agents like Dota2. To handle such a problem, the player usually fights against
    itself to increase its ability, such as AlphaGo [[1](#bib.bib1)], which uses DDRL
    and self-play for superhuman AI learning. Based on current learning paradigms
    for players evolution, current methods can be classified into two categories,
    i.e., self-play and population-play, as shown in Fig [14](#S3.F14 "Figure 14 ‣
    3.4 Player Evolution Types ‣ 3 Taxonomy of Distributed Deep Reinforcement Learning
    ‣ Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent
    Learning Toolbox"). To maintain the players for evolution, a players manager is
    required for the DDRL algorithms for one or multiple players. Self-play maintains
    a player and its past versions, whereas, population-play maintains several distinct
    players and their past versions.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '在大多数情况下，我们没有对手来推动玩家的能力增长³³3这里的玩家指的是游戏的一方，可能控制一个像围棋这样的代理，或者多个像Dota2这样的代理。为了解决这个问题，玩家通常会与自己对抗以提升能力，例如AlphaGo
    [[1](#bib.bib1)]，它利用DDR和自我对弈进行超人类AI学习。基于当前玩家进化的学习范式，当前的方法可以分为两类，即自我对弈和群体对弈，如图
    [14](#S3.F14 "Figure 14 ‣ 3.4 Player Evolution Types ‣ 3 Taxonomy of Distributed
    Deep Reinforcement Learning ‣ Distributed Deep Reinforcement Learning: A Survey
    and A Multi-Player Multi-Agent Learning Toolbox") 所示。为了维护玩家的进化，需要一个玩家管理器来管理一个或多个玩家的DDR算法。自我对弈维护一个玩家及其过去的版本，而群体对弈则维护多个不同的玩家及其过去的版本。'
- en: '![Refer to caption](img/2887a315882c50ca9ce12f4ba8c72497.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2887a315882c50ca9ce12f4ba8c72497.png)'
- en: 'Figure 14: Basic framework of player iteration.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：玩家迭代的基本框架。
- en: 3.4.1 Self-play based evolution
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 基于自我对弈的进化
- en: Self-play becomes a popular tool since the success of AlphaGo series [[1](#bib.bib1),
    [2](#bib.bib2), [47](#bib.bib47)], which train a player by fighting against itself.
    In an iteration or called generation of the player, the current version is trained
    based on previous DDRL algorithms by using one or some of its previous versions
    as opponents. The players manager decides which previous versions are used as
    the opponents.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 自我对弈成为一种流行工具，自AlphaGo系列成功以来 [[1](#bib.bib1), [2](#bib.bib2), [47](#bib.bib47)]，它通过与自己对抗来训练一个玩家。在玩家的一个迭代或称为世代中，当前版本是基于以前的DDR算法进行训练的，使用一个或一些以前的版本作为对手。玩家管理器决定使用哪些以前的版本作为对手。
- en: In JueWu [[36](#bib.bib36)] developed for Honor of Kings, a naive self-play
    is used for two players (each controls five agents) using the same policy. A SEEDRL
    [[9](#bib.bib9)] DDRL algorithm is used, and the self-play is used in the fixed
    lineup and random lineup stages for handling large hero pool size. Players trained
    for hide-and-seek [[44](#bib.bib44)] is similar with JueWu [[36](#bib.bib36)],
    where a SEEDRL [[9](#bib.bib9)] DDRL algorithm and a naive self-play are used
    to prove the multi-agent auto-curricula. Another similar example is Suphx [[5](#bib.bib5)]
    proposed for Mahjong, which uses self-play for a player to confront the other
    three players (use the same policy). As for the DDRL algorithm, an IMPALA [[10](#bib.bib10)]
    framework is applied for training each generation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在为《王者荣耀》开发的 JueWu [[36](#bib.bib36)] 中，使用了相同策略的两个玩家（每个玩家控制五个代理）的简单自我对弈。使用了 SEEDRL
    [[9](#bib.bib9)] DDRL 算法，自我对弈用于固定阵容和随机阵容阶段，以处理大规模英雄池。用于捉迷藏 [[44](#bib.bib44)]
    的玩家与 JueWu [[36](#bib.bib36)] 类似，使用 SEEDRL [[9](#bib.bib9)] DDRL 算法和简单自我对弈来证明多代理自动课程。另一个类似的例子是为麻将提出的
    Suphx [[5](#bib.bib5)]，它使用自我对弈让一个玩家对抗其他三个玩家（使用相同策略）。至于 DDRL 算法，每一代的训练都应用了 IMPALA
    [[10](#bib.bib10)] 框架。
- en: In OpenAI Five [[6](#bib.bib6)] designed for Dota2, a more complex self-play
    is used for two players (each controls five agents) using the same policy. In
    each generation, instead of fighting against the current generation like naive
    self-play, the player trains the current policy against itself for 80% of games,
    and against its previous versions for 20% of games. Specifically, a dynamic sampling
    system is designed to select the past versions based on their dynamically generated
    quality score, which claims to alleviate cyclic strategies problem. As for the
    basic DDRL algorithm, a SEEDRL [[9](#bib.bib9)] framework is used for all the
    generations of players training.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在为 Dota2 设计的 OpenAI Five [[6](#bib.bib6)] 中，使用了更复杂的自我对弈方式，其中两个玩家（每个玩家控制五个代理）使用相同的策略。在每一代中，玩家不是像传统自我对弈那样与当前代进行对抗，而是将当前策略在
    80% 的游戏中与自身对抗，在 20% 的游戏中与之前的版本对抗。具体而言，设计了一个动态抽样系统，根据动态生成的质量评分选择过去的版本，这被认为可以缓解循环策略的问题。至于基本的
    DDRL 算法，所有玩家训练的每一代都使用 SEEDRL [[9](#bib.bib9)] 框架。
- en: 3.4.2 Population-play based evolution
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 基于群体对弈的进化
- en: 'Population-play can be seen as an advanced self-play, where more than one player
    and their past generations should be maintained for players evolution. It can
    be used for several cases: the policy used is different for different players
    (e.g., a Landlord and two Peasant players in DouZero); some auxiliary players
    are introduced for the target player to overcome game-theoretic challenges (e.g.,
    main exploiter and league exploiter players in AlphaStar); parallel players are
    used with consistent roles to support concurrent training and to alleviate unstabitily
    of self-play (e.g., populations in FTW).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 群体对弈可以被视为一种高级自我对弈，其中需要维护多个玩家及其过去的版本以实现玩家进化。它可以用于多种情况：不同玩家使用不同的策略（例如 DouZero
    中的一个地主和两个农民玩家）；引入一些辅助玩家以帮助目标玩家克服博弈理论挑战（例如 AlphaStar 中的主要掠夺者和联盟掠夺者玩家）；使用具有一致角色的并行玩家以支持并发训练并减轻自我对弈的不稳定性（例如
    FTW 中的群体）。
- en: In DouZero [[43](#bib.bib43)] designed for DouDiZhu, a Landlord and two Peasant
    players are trained simultaneously, where their current generations fight against
    each other to collect trajectories and to train the players. The basic DDRL algorithm
    is Gorila [[8](#bib.bib8)] running on a single machine, based on which, all the
    three players are trained asynchronously.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在为斗地主设计的 DouZero [[43](#bib.bib43)] 中，同时训练一个地主和两个农民玩家，这些玩家的当前代相互对抗以收集轨迹并训练玩家。基本的
    DDRL 算法是运行在单台机器上的 Gorila [[8](#bib.bib8)]，基于此，所有三个玩家都异步训练。
- en: In AlphaStar [[7](#bib.bib7)] developed for StarCraft, the players manager maintains
    three main players for three different races, i.e., Protoss, Terran, and Zerg.
    Besides, for each race, several auxiliary players are designed, i.e., one main
    exploiter player and two league exploiter players. Those auxiliary players help
    the main player find out weaknesses and help all the players find systemic weaknesses.
    The authors claim that using such a population addresses the complexity and game-theoretic
    challenges of the StarCraft. As for the DDRL algorithm, SEEDRL [[9](#bib.bib9)]
    is utilized to support large system training. Commander [[48](#bib.bib48)] is
    similar with [[7](#bib.bib7)], and more exploiter players are used.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在为星际争霸开发的AlphaStar [[7](#bib.bib7)] 中，玩家管理器维护三名主要玩家，分别对应三种不同的种族，即 Protoss、Terran
    和 Zerg。此外，为每个种族设计了几个辅助玩家，即一个主要的利用玩家和两个联赛利用玩家。这些辅助玩家帮助主要玩家发现弱点，并帮助所有玩家找出系统性弱点。作者声称，使用这样的群体解决了星际争霸中的复杂性和博弈论挑战。至于DDR算法，利用了SEEDRL
    [[9](#bib.bib9)] 来支持大规模系统训练。Commander [[48](#bib.bib48)] 与 [[7](#bib.bib7)] 类似，并使用了更多的利用玩家。
- en: In FTW [[42](#bib.bib42)] designed for CTF, the players manager maintains a
    population of players, who cooperate and confront with each other to learn scalable
    bots. The positions of all the players are the same, and a population-based training
    method is designed to adjust players with worse performance, so as to improve
    the ability of all the players. As for the basic DDRL algorithm, an IMPALA [[10](#bib.bib10)]
    method is used to have large data throughput to train tens of players.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在FTW [[42](#bib.bib42)] 中为CTF设计，玩家管理器维持一个玩家群体，这些玩家相互合作和对抗，以学习可扩展的机器人。所有玩家的位置相同，并且设计了一种基于人口的训练方法，以调整表现较差的玩家，从而提升所有玩家的能力。至于基本的DDR算法，使用了IMPALA
    [[10](#bib.bib10)] 方法，以实现大量数据的处理，训练数十名玩家。
- en: 3.4.3 Discussion
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3 讨论
- en: Self-play has a long history in multi-agent settings, where early work explored
    it in genetic algorithms [[49](#bib.bib49)]. It becomes very popular since the
    success of AlphaGo series [[1](#bib.bib1), [2](#bib.bib2)] and then be used for
    AI systems such as Libratus [[50](#bib.bib50)], DeepStack [[51](#bib.bib51)] and
    OpenAI Five [[6](#bib.bib6)]. Combining DDRL, it can be used to solve very complex
    games. On the other side, population-play can be seen as an advanced self-play,
    which maintains more players to achieve ability improvement. Current works use
    population-play to accelerate training, overcome game-theoretic challenges, or
    just handle the problem that requires distinct players. Compared with self-play,
    population-play is more flexible, and can handle diverse situations, whereas,
    self-play is easy to be implemented, and has proved its potential in complex games.
    So, there is no conclusion which one is better, and researchers can select self-play
    or population-play DDRL based on their request.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 自我对弈在多智能体设置中有着悠久的历史，早期的研究在遗传算法中探索了这一方法 [[49](#bib.bib49)]。自AlphaGo系列取得成功以来，它变得非常流行
    [[1](#bib.bib1), [2](#bib.bib2)]，并被用于像Libratus [[50](#bib.bib50)]、DeepStack [[51](#bib.bib51)]
    和OpenAI Five [[6](#bib.bib6)] 这样的AI系统。结合DDR， self-play 可以用于解决非常复杂的游戏。另一方面，人口对弈可以看作是高级的自我对弈，它保持更多的玩家以实现能力的提升。目前的工作使用人口对弈来加速训练，克服博弈论挑战，或者处理需要不同玩家的问题。与自我对弈相比，人口对弈更具灵活性，可以处理多样的情况，而自我对弈易于实现，并已在复杂游戏中证明了其潜力。因此，目前尚无结论说明哪种方法更好，研究人员可以根据需求选择自我对弈或人口对弈的DDR。
- en: 4 Typical Distributed deep Reinforcement Learning Toolboxes
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 典型的分布式深度强化学习工具箱
- en: DDRL is important for complex problems using reinforcement learning as solvers,
    and several useful toolboxes have been released to help researchers reduce development
    costs. In this section, we analysis several typical toolboxes, hoping to give
    a clue when researchers are making a selection among them.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: DDR对使用强化学习作为求解器的复杂问题非常重要，已经发布了若干有用的工具箱，帮助研究人员降低开发成本。在这一部分，我们分析了几个典型的工具箱，希望能为研究人员在选择时提供一些线索。
- en: 4.1 Typical Toolboxes
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 典型工具箱
- en: Ray [[12](#bib.bib12)] is a distributed framework consisting of two main parts,
    i.e., a system layer to implement tasks scheduling and data management, and an
    application layer to provide high-level API for various applications. Using Ray,
    researchers can easily implement a DDRL method without considering the nodes/machines
    communications and how to schedule different calculations. The API is user-friendly,
    and by adding @ray.remote, users can obtain a remote function that can be executed
    in parallel. A RLLib [[13](#bib.bib13)] package is specifically introduced to
    handle reinforcement learning problems such as A3C, APEX and IMPALA. Furthermore,
    several built-in multi-agent DDRL algorithms are provided such as QMIX [[45](#bib.bib45)]
    and MADDPG [[52](#bib.bib52)].
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Ray [[12](#bib.bib12)] 是一个分布式框架，包括两个主要部分，即系统层用于任务调度和数据管理，以及应用层提供各种应用的高级 API。使用
    Ray，研究人员可以轻松实现 DDRL 方法，而无需考虑节点/机器之间的通信以及如何调度不同的计算。API 友好，通过添加 @ray.remote，用户可以获得一个可以并行执行的远程函数。专门引入了
    RLLib [[13](#bib.bib13)] 包来处理强化学习问题，如 A3C、APEX 和 IMPALA。此外，还提供了几个内置的多智能体 DDRL
    算法，如 QMIX [[45](#bib.bib45)] 和 MADDPG [[52](#bib.bib52)]。
- en: Acme [[53](#bib.bib53)] is designed to enable distributed reinforcement learning
    to promote development of novel RL agents and their applications. It involves
    many separate (parallel) acting, learning, as well as diagnostic and helper processes,
    which are key building blocks for a DDRL system. One of the main contributions
    is the in-memory storage system, called Reverb, which is a high-throughput data
    system that are suitable for experience replay based reinforcement learning algorithms.
    With the aim of supporting agents at various scales of execution, plenty of mainstream
    DDRL algorithms are implemented, i.e., online reinforcement learning algorithms
    such as Deep Q-Networks [[28](#bib.bib28)], R2D2 [[35](#bib.bib35)] and IMPALA
    [[10](#bib.bib10)], offline reinforcement learning such as behavior cloning and
    TD3 [[54](#bib.bib54)], imitation learning such as adversarial imitation learning
    [[55](#bib.bib55)] and soft Q imitation learning [[56](#bib.bib56)].
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Acme [[53](#bib.bib53)] 旨在促进分布式强化学习的发展，以支持新型 RL 代理及其应用。它涉及许多独立的（并行的）动作、学习，以及诊断和辅助进程，这些都是
    DDRL 系统的关键构建块。主要贡献之一是内存存储系统 Reverb，这是一个适用于经验回放的高吞吐量数据系统。为了支持各种规模的代理，实施了大量主流 DDRL
    算法，即在线强化学习算法，如 Deep Q-Networks [[28](#bib.bib28)]、R2D2 [[35](#bib.bib35)] 和 IMPALA
    [[10](#bib.bib10)]，离线强化学习如行为克隆和 TD3 [[54](#bib.bib54)]，模仿学习如对抗模仿学习 [[55](#bib.bib55)]
    和软 Q 模仿学习 [[56](#bib.bib56)]。
- en: Tianshou [[57](#bib.bib57)] is a highly modularized Python library that uses
    PyTorch for DDRL. Its main characteristic is the design of building blocks that
    support more than 20 classic reinforcement learning algorithms with distributed
    version through a unified interface. Since Tianshou focuses on small-to medium-scale
    applications of DDRL with only parallel sampling, it is a lightweight platform
    that is research-friendly. It is claimed that Tianshou is easy to install, and
    users can apply Pip or Conda to accomplish installation on platforms covering
    Windows, macOS and Linux.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Tianshou [[57](#bib.bib57)] 是一个高度模块化的 Python 库，使用 PyTorch 实现 DDRL。其主要特点是设计了构建模块，支持通过统一接口的分布式版本的
    20 多种经典强化学习算法。由于 Tianshou 专注于小到中规模的 DDRL 应用，仅进行并行采样，因此它是一个轻量级的、适合研究的平台。声称 Tianshou
    安装简便，用户可以通过 Pip 或 Conda 在 Windows、macOS 和 Linux 平台上完成安装。
- en: TorchBeast [[58](#bib.bib58)] is another DDRL toolbox that bases on Pytorch
    to support for fast, asynchronous and parallel training of reinforcement learning
    agents. The authors provide two versions, i.e., a pure-Python MonoBeast and a
    multi-machine high-performance PolyBeast with several parts being implemented
    with C++. Users only require Python and Pytorch to implement DDRL algorithms.
    In the toolbox, IMPALA is supported and tested with the classic Atari suite.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: TorchBeast [[58](#bib.bib58)] 是另一个基于 Pytorch 的 DDRL 工具箱，支持快速、异步和并行训练强化学习代理。作者提供了两个版本，即纯
    Python 的 MonoBeast 和一个多机高性能的 PolyBeast，其中几个部分使用 C++ 实现。用户只需 Python 和 Pytorch 即可实现
    DDRL 算法。该工具箱支持 IMPALA，并经过经典 Atari 套件的测试。
- en: MALib [[59](#bib.bib59)] is a scalable and efficient computing framework for
    population-based multi-agent reinforcement learning algorithms. Using a centralized
    task dispatching model, it supports self-generated tasks and heterogeneous policy
    combinations. Besides, by abstracting DDRL algorithms using Actor-Evaluator-Learner,
    a higher parallelism for learning and sampling is achieved. The authors also claimed
    to have an efficient code reuse and flexible deployments due to the higher-level
    abstractions of multi-agent reinforcement learning. In the released code, several
    popular reinforcement learning environments such as Google Research Football and
    SMAC are supported and typical population based algorithms such as policy space
    response oracle (PSRO) [[60](#bib.bib60)] and Pipeline-PSRO [[61](#bib.bib61)]
    are implemented.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: MALib [[59](#bib.bib59)] 是一个可扩展且高效的计算框架，专为基于种群的多代理强化学习算法设计。采用集中式任务调度模型，它支持自生成任务和异质策略组合。此外，通过使用
    Actor-Evaluator-Learner 抽象 DDRL 算法，实现了更高的学习和采样并行性。作者还声称，由于多代理强化学习的高层次抽象，具有高效的代码重用和灵活的部署。在发布的代码中，支持了
    Google Research Football 和 SMAC 等几个流行的强化学习环境，并实现了典型的基于种群的算法，如政策空间响应预言机 (PSRO)
    [[60](#bib.bib60)] 和 Pipeline-PSRO [[61](#bib.bib61)]。
- en: SeedRL [[9](#bib.bib9)] is a scalable and efficient deep reinforcement learning
    toolbox, as described in Section 3.2.1. Generally, it is verified on the tensor
    processing unit (TPU) device, which is a special chip customized by Google for
    machine learning. Typical DDRL algorithms are implemented, e.g., IMPALA [[10](#bib.bib10)],
    and R2D2 [[35](#bib.bib35)], which are tested on four classical environments,
    i.e., Arati, DeepMind lab, Google research football and Mujoco. Distributed training
    is supported using cloud machine learning engine of Google.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: SeedRL [[9](#bib.bib9)] 是一个可扩展且高效的深度强化学习工具箱，如 3.2.1 节所述。一般来说，它在张量处理单元 (TPU)
    设备上经过验证，这是一种 Google 为机器学习定制的特殊芯片。实现了典型的 DDRL 算法，例如 IMPALA [[10](#bib.bib10)] 和
    R2D2 [[35](#bib.bib35)]，并在四个经典环境中进行了测试，即 Arati、DeepMind lab、Google research football
    和 Mujoco。支持使用 Google 的云机器学习引擎进行分布式训练。
- en: '![Refer to caption](img/f933a606d112e66bbb55d4c94fea03b0.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f933a606d112e66bbb55d4c94fea03b0.png)'
- en: 'Figure 15: Basic framework of the proposed multi-player multi-agent reinforcement
    learning toolbox M2RL.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：所提出的多玩家多代理强化学习工具箱 M2RL 的基本框架。
- en: 4.2 Discussions
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 讨论
- en: Before comparing different kinks of toolboxes, we want to claim that there are
    no best DDRL toolboxes for any requirements, but the most suitable one depending
    on specific goals.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较不同类型的工具箱之前，我们想声明，没有适用于所有需求的最佳 DDRL 工具箱，只有根据特定目标选择的最合适的工具箱。
- en: Tianshou and TorchBeast are lightweight platforms that support several typical
    DDRL algorithms. Users can easily use and revise the released codes for developing
    reinforcement learning algorithms with the PyTorch deep learning library. The
    user-friendly features make these toolboxes popular. However, even though those
    toolboxes are highly modularized, the scalability to large number of machines
    for performing large learner parallel and actor parallel are not tested, and bottleneck
    may appear with increasing number of machines.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Tianshou 和 TorchBeast 是支持几种典型 DDRL 算法的轻量级平台。用户可以轻松使用和修改发布的代码来开发基于 PyTorch 深度学习库的强化学习算法。用户友好的特性使这些工具箱很受欢迎。然而，尽管这些工具箱高度模块化，但对大规模机器的可扩展性以进行大规模学习者并行和演员并行尚未经过测试，随着机器数量的增加，可能会出现瓶颈。
- en: Ray, Acme and SeedRL are relatively large toolboxes that can theoretically support
    any DDRL algorithms with certain modifications. Using their open projects, users
    can utilize multiple machines to implement high data throughput DDRL algorithms.
    Moreover, multiple agents training, and multiple players evolution can be achieved,
    such as for AlphaStar. However, the modifications are not easy when revising the
    DDRL algorithms due to the code nesting, especially for complex functions such
    as self-play and population-play.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Ray、Acme 和 SeedRL 是相对较大的工具箱，理论上可以支持任何需要某些修改的 DDRL 算法。利用它们的开放项目，用户可以使用多台机器实现高数据吞吐量的
    DDRL 算法。此外，可以实现多个代理训练和多个玩家演化，如 AlphaStar。然而，由于代码嵌套，修改 DDRL 算法并不容易，尤其是对于自我对弈和种群对弈等复杂函数。
- en: MALib is similar with Ray, Acme and SeedRL, which is a specially designed DDRL
    toolbox for population-based multi-agent reinforcement learning. With their APIs,
    users may easily implement population based multi-agent reinforcement learning
    algorithms such as fictitious self-play [[62](#bib.bib62)] and PSRO. Even though
    experiments for large number of machines are not tested, this toolbox is fully
    functional (APIs provided) for various requirements of DDRL algorithms from single
    player single agent DDRL to multiple players multiple agents DDRL.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: MALib 与 Ray、Acme 和 SeedRL 相似，是一个专门为基于人群的多智能体强化学习设计的 DDRL 工具箱。通过它们的 API，用户可以轻松实现基于人群的多智能体强化学习算法，如虚拟自我对战
    [[62](#bib.bib62)] 和 PSRO。尽管未对大量机器进行实验，这个工具箱对于 DDRL 算法的各种需求，从单玩家单智能体 DDRL 到多玩家多智能体
    DDRL，都是完全功能性的（提供了 API）。
- en: In summary, current DDRL toolboxes provide a good support for DDRL algorithms,
    and several typical testing environments are applied for performance validation.
    However, those DDRL toolboxes are either lightweight or heavy, and not tested
    for complex games. In the following, we will design a new toolbox, which focuses
    on multiple players and multiple agents DDRL training on complex games.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，当前的 DDRL 工具箱为 DDRL 算法提供了良好的支持，并且应用了几个典型的测试环境来进行性能验证。然而，这些 DDRL 工具箱要么较轻量，要么较重，且未对复杂游戏进行测试。在接下来的内容中，我们将设计一个新的工具箱，专注于在复杂游戏中进行多玩家和多智能体的
    DDRL 训练。
- en: '![Refer to caption](img/a605f21e7373d00b32b8130b09bcb454.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a605f21e7373d00b32b8130b09bcb454.png)'
- en: 'Figure 16: Specific details of the proposed multi-player multi-agent reinforcement
    learning toolbox M2RL.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：所提议的多玩家多智能体强化学习工具箱 M2RL 的具体细节。
- en: 5 A Multi-Player Multi-Agent Reinforcement Learning Toolbox
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 多玩家多智能体强化学习工具箱
- en: In this section, we open a multi-player multi-agent reinforcement learning toolbox,
    M2RL, to support populations of players (with each may control several agents)
    for complex games, e.g., Wargame [[63](#bib.bib63)]. Noted that this project is
    on going, so the main purpose is a preliminary introduction, and we will continue
    to improve this project. Hyperlink of the project is [m2rl-V1.0](http://turingai.ia.ac.cn/ai_center/show/14).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们推出了一个多玩家多智能体强化学习工具箱 M2RL，以支持复杂游戏中的玩家群体（每个玩家可以控制多个智能体），例如，战争游戏 [[63](#bib.bib63)]。请注意，该项目正在进行中，因此主要目的是进行初步介绍，我们将继续改进此项目。项目的超链接是
    [m2rl-V1.0](http://turingai.ia.ac.cn/ai_center/show/14)。
- en: 5.1 Overall Framework
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 总体框架
- en: 'The overall framework is shown in Fig [15](#S4.F15 "Figure 15 ‣ 4.1 Typical
    Toolboxes ‣ 4 Typical Distributed deep Reinforcement Learning Toolboxes ‣ Distributed
    Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning
    Toolbox"). Each player, consisting one or multiple agents, has three key components:
    learner, actor and experience buffer. The multiple concurrently executed actors
    produce data for learners, which use the current player and other players as opponents
    based on the choice of players manager. The experience buffer is used to store
    trajectories of the player to support asynchronous or synchronous training. The
    learner for each player is used to update parameters of the player and send parameters
    to the actors. Apart from the above basic factors, Players manager maintains self-play
    and population-play, which has two key parts: evaluating players and choosing
    opponent players for each player.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '总体框架如图 [15](#S4.F15 "Figure 15 ‣ 4.1 Typical Toolboxes ‣ 4 Typical Distributed
    deep Reinforcement Learning Toolboxes ‣ Distributed Deep Reinforcement Learning:
    A Survey and A Multi-Player Multi-Agent Learning Toolbox") 所示。每个玩家，包括一个或多个智能体，具有三个关键组件：学习者、演员和经验缓冲区。多个同时执行的演员为学习者生成数据，学习者根据玩家管理者的选择使用当前玩家和其他玩家作为对手。经验缓冲区用于存储玩家的轨迹，以支持异步或同步训练。每个玩家的学习者用于更新玩家的参数并将参数发送给演员。除了上述基本因素外，玩家管理者维持自我对战和群体对战，其中包含两个关键部分：评估玩家和为每个玩家选择对手玩家。'
- en: '![Refer to caption](img/a134c5a166d8e8c18d61d3c34e0a4721.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a134c5a166d8e8c18d61d3c34e0a4721.png)'
- en: 'Figure 17: Elo results of the trained AI bots (red and blue players) based
    on M2RL. Knowledge_1, Knowledge_2 and Knowledge_3 are three professional level
    AI bots. Demo is an AI with a strategy to select the highest priority action when
    it is possible.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：基于 M2RL 的训练 AI 机器人（红色和蓝色玩家）的 Elo 结果。Knowledge_1、Knowledge_2 和 Knowledge_3
    是三个专业水平的 AI 机器人。Demo 是一个具有在可能的情况下选择最高优先级行动策略的 AI。
- en: 'More specific details of M2RL is shown in Fig [16](#S4.F16 "Figure 16 ‣ 4.2
    Discussions ‣ 4 Typical Distributed deep Reinforcement Learning Toolboxes ‣ Distributed
    Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning
    Toolbox"). To make M2RL easy to be used for complex games, we design each parts
    in a relatively flexible manner.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 'M2RL的更多具体细节见图[16](#S4.F16 "Figure 16 ‣ 4.2 Discussions ‣ 4 Typical Distributed
    deep Reinforcement Learning Toolboxes ‣ Distributed Deep Reinforcement Learning:
    A Survey and A Multi-Player Multi-Agent Learning Toolbox")。为了使M2RL易于用于复杂游戏，我们以相对灵活的方式设计了各个部分。'
- en: •
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The players manager evaluates all saved players (including their past versions)
    using their confrontation results, based on which, various opponents selection
    methods can be implemented to promote players evolution, e.g., revised self-play
    in OpenAI Five [[6](#bib.bib6)], and prioritized fictitious self-play in AlphaStar
    [[7](#bib.bib7)].
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 玩家管理器通过对抗结果评估所有保存的玩家（包括他们的过去版本），基于此，可以实现各种对手选择方法以促进玩家的进化，例如OpenAI Five中的修订自我对战[[6](#bib.bib6)]，以及AlphaStar中的优先虚拟自我对战[[7](#bib.bib7)]。
- en: •
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Each player maintains its own learner, actor and experience buffer, making distinct
    players training possible, e.g., red and blue players in Wargame [[63](#bib.bib63)].
    Considering that the game is complex with different observation and action spaces
    compared to OpenAI gym, feature engineering and mask engineering are used in the
    framework. Besides, experience buffer is revised to change unfinished buffer to
    finished buffer, which is very useful for asynchronous multi-agent cooperation
    [[64](#bib.bib64)].
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个玩家都维护自己的学习者、行动者和经验缓冲区，这使得不同玩家的训练成为可能，例如，《战争游戏》中的红色和蓝色玩家[[63](#bib.bib63)]。考虑到游戏的复杂性以及与OpenAI
    gym相比，观察和行动空间的不同，框架中使用了特征工程和掩码工程。此外，经验缓冲区被修订为将未完成的缓冲区更改为已完成的缓冲区，这对异步多智能体协作非常有用[[64](#bib.bib64)]。
- en: •
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All the codes are based on the user-friendly framework Ray, which is easy to
    be deployed, revised and used. More specifically, we can make full use of computing
    resources by segmenting a GPU to several parts and assigning each part to different
    tasks, which is important for complex games under limited computing resources.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有代码都基于用户友好的框架Ray，该框架易于部署、修订和使用。更具体地说，我们可以通过将一个GPU划分为多个部分，并将每个部分分配给不同的任务，充分利用计算资源，这对于在有限计算资源下的复杂游戏非常重要。
- en: 5.2 A Case
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 一个案例
- en: Wargame, a complex game like Dota2 and StarCraft, is a popular testing environment
    for verifying artificial intelligence [[27](#bib.bib27)]. In a Wargame map⁴⁴4wargame.ia.ac.cn,
    ID=2010431153, the red player controls several fighting units to confront the
    blue player who also controls several units. The game is asymmetric because players
    have distinct strategies space, and usually the blue player has more forces, while
    the red player has vision advantage. Please refer to [[63](#bib.bib63)] for more
    details of the Wargame.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 《战争游戏》是类似于Dota2和StarCraft的复杂游戏，是验证人工智能的热门测试环境[[27](#bib.bib27)]。在Wargame地图⁴⁴4wargame.ia.ac.cn，ID=2010431153中，红色玩家控制几个战斗单位来对抗控制几个单位的蓝色玩家。由于玩家拥有不同的策略空间，游戏是不对称的，通常蓝色玩家拥有更多的力量，而红色玩家则具有视野优势。有关《战争游戏》的更多细节，请参见[[63](#bib.bib63)]。
- en: 'We can naturally model Wargame as a two players multiple agents problem, where
    each fighting unit is regarded as an agent. To train two AI bots for the red and
    blue players, respectively, we use several widely adopted settings like in OpenAI
    Five [[6](#bib.bib6)], JueWu [[36](#bib.bib36)] and AlphaStar [[7](#bib.bib7)],
    e.g., shared PPO policy for each agent, dual-clip for the PPO, and prioritized
    fictitious self-play. Each player trains its bot using about 200,000 games, and
    uses 9,500 games for the players manager to evaluate each generation of the player.
    The computing resources used here are: 2$\times$Intel(R) Xeon(R) Gold 6240R CPU
    @ 2.40GHz, 4$\times$ NVIDIA GeForce RTX 2080 Ti, and 500GB memory. With above
    resources, the training lasts for five days, and we finally obtain 20 generations
    for each player. To evaluate the performance of these bots, we use the build-in
    demo agent as baseline, and bring in three professional level AI bots designed
    by teams who have studied Wargame for several years, represented as Knowledge_1,
    Knowledge_2, and Knowledge_3, respectively. It should be noted that those professional
    AI bots do not participated in training. Similar with the evaluation for AlphaGo
    [[1](#bib.bib1)] and AlphaStar [[7](#bib.bib7)], we use Elo as metrics. The results
    are shown in Fig [17](#S5.F17 "Figure 17 ‣ 5.1 Overall Framework ‣ 5 A Multi-Player
    Multi-Agent Reinforcement Learning Toolbox ‣ Distributed Deep Reinforcement Learning:
    A Survey and A Multi-Player Multi-Agent Learning Toolbox").'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以自然地将 Wargame 模型化为一个双玩家多个代理的问题，其中每个作战单位被视为一个代理。为了分别训练红蓝玩家的两个 AI 机器人，我们使用了多个广泛采用的设置，如
    OpenAI Five [[6](#bib.bib6)]、JueWu [[36](#bib.bib36)] 和 AlphaStar [[7](#bib.bib7)]，例如，每个代理的共享
    PPO 策略、PPO 的双剪辑以及优先虚拟自我对抗。每个玩家使用约 200,000 场游戏来训练其机器人，并用 9,500 场游戏来评估每一代玩家。使用的计算资源包括：2$\times$Intel(R)
    Xeon(R) Gold 6240R CPU @ 2.40GHz、4$\times$ NVIDIA GeForce RTX 2080 Ti 和 500GB
    内存。在上述资源支持下，训练持续了五天，最终为每个玩家获得了 20 代。为了评估这些机器人的表现，我们使用内置的演示代理作为基线，并引入了由研究 Wargame
    几年的团队设计的三款专业级 AI 机器人，分别表示为 Knowledge_1、Knowledge_2 和 Knowledge_3。需要注意的是，这些专业 AI
    机器人没有参与训练。与 AlphaGo [[1](#bib.bib1)] 和 AlphaStar [[7](#bib.bib7)] 的评估类似，我们使用 Elo
    作为指标。结果如图 [17](#S5.F17 "Figure 17 ‣ 5.1 Overall Framework ‣ 5 A Multi-Player Multi-Agent
    Reinforcement Learning Toolbox ‣ Distributed Deep Reinforcement Learning: A Survey
    and A Multi-Player Multi-Agent Learning Toolbox") 所示。'
- en: 'From Fig [17](#S5.F17 "Figure 17 ‣ 5.1 Overall Framework ‣ 5 A Multi-Player
    Multi-Agent Reinforcement Learning Toolbox ‣ Distributed Deep Reinforcement Learning:
    A Survey and A Multi-Player Multi-Agent Learning Toolbox"), it can be seen that
    with the increasing of players evolution, the learned policy for each player is
    becoming stronger. Since Wargame is a complex game and previous toolboxes are
    not specifically designed for complex games, the comparison is not performed due
    to a hard transfer on these toolboxes. Overall, the results show the ability of
    the proposed M2RL to some extent. Since this project is an on going item, so the
    main purpose of this part is an introduction, and we will continue improve this
    project ([m2rl-V1.0](http://turingai.ia.ac.cn/ai_center/show/14)).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '从图 [17](#S5.F17 "Figure 17 ‣ 5.1 Overall Framework ‣ 5 A Multi-Player Multi-Agent
    Reinforcement Learning Toolbox ‣ Distributed Deep Reinforcement Learning: A Survey
    and A Multi-Player Multi-Agent Learning Toolbox") 可以看出，随着玩家演化的增加，每个玩家学习到的策略变得越来越强。由于
    Wargame 是一个复杂的游戏，且之前的工具箱并未专门为复杂游戏设计，因此未进行比较，因为这些工具箱的迁移难度较大。总体而言，结果在某种程度上展示了提出的
    M2RL 的能力。由于这个项目仍在进行中，所以这一部分的主要目的是介绍，我们将继续改进该项目 ([m2rl-V1.0](http://turingai.ia.ac.cn/ai_center/show/14))。'
- en: 6 Challenges and Opportunities
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 个挑战与机遇
- en: Plenty of DDRL algorithms and toolboxes are proposed, which largely promote
    the study of reinforcement learning and its applications. We think current methods
    still suffer several challenges, which may be the future directions. Firstly,
    current methods rarely consider accelerating complex reinforcement learning algorithms,
    such as those studying exploration, communication and generalization problems.
    Secondly, current approaches mainly use ring allreduce or parameter and server
    for learners, which seldom handle large model size and batch size situations simultaneously.
    Thirdly, self-play or population-play are important methods for multiple players
    and multiple agents training, which are also flexible without strict restrictions,
    but deeper study is deficient. Fourthly, several famous DDRL toolboxes are developed,
    but none of them is verified with large scale training, e.g., tens of machines
    for complex games.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了大量的 DDRL 算法和工具箱，这在很大程度上促进了强化学习及其应用的研究。我们认为当前的方法仍然面临几个挑战，这些挑战可能是未来的研究方向。首先，当前的方法很少考虑加速复杂的强化学习算法，例如那些研究探索、通信和泛化问题的算法。其次，当前的方法主要使用环形全减少或参数和服务器进行学习，这些方法很少同时处理大模型尺寸和批量尺寸的情况。第三，自我对弈或群体对弈是多玩家和多代理训练的重要方法，这些方法也具有灵活性而没有严格限制，但对其的深入研究仍然不足。第四，虽然开发了一些著名的
    DDRL 工具箱，但没有一个经过大规模训练的验证，例如复杂游戏中的数十台机器。
- en: DDRL with advanced reinforcement learning algorithms. The research and application
    of reinforcement learning show explosive growth since the success of AlphaGo.
    New topics emerge such as hierarchical deep reinforcement learning, model based
    reinforcement learning, multi-agent reinforcement learning, off-line reinforcement
    learning, meta reinforcement learning [[16](#bib.bib16), [18](#bib.bib18)], but
    DDRL methods rarely consider those new research area. Distributed implementation
    is kind of engineering but not naive. For example, when considering information
    communication for a multi-agent reinforcement learning algorithm, agents manager
    should reasonably parallelize agents communication calculation so as to improve
    data throughput. Accordingly, how to accelerate advanced reinforcement learning
    algorithms with distributed implementation is an important direction.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: DDRL 与先进的强化学习算法。自从 AlphaGo 成功以来，强化学习的研究和应用显示出爆炸式增长。新话题不断出现，例如分层深度强化学习、基于模型的强化学习、多智能体强化学习、离线强化学习、元强化学习
    [[16](#bib.bib16), [18](#bib.bib18)]，但 DDRL 方法很少考虑这些新的研究领域。分布式实现是一种工程技术，但不是简单的操作。例如，在考虑多智能体强化学习算法的信息通信时，代理管理者应合理并行化代理的通信计算，以提高数据吞吐量。因此，如何通过分布式实现来加速先进的强化学习算法是一个重要的方向。
- en: DDRL with big model size and batch size. With the success of foundation models
    in the field of computer vision and natural language processing, big model in
    reinforcement learning will be a direction [[27](#bib.bib27)]. This requires that
    the DDRL methods can handle big model size and batch size situations simultaneously.
    Currently, the learners in DDRL are based on techniques such as ring allreduce
    or parameter-server, with each has its advantages. For example, parameter-server
    can store big model in different GPUs, and ring allreduce can quickly exchange
    gradients between different GPUs. However, none of them are applied for big model
    size and batch size in reinforcement learning. Accordingly, how to combine these
    techniques to fit DDRL algorithms for efficient training is a future direction.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: DDRL 使用大模型尺寸和批量尺寸。随着基础模型在计算机视觉和自然语言处理领域的成功，大模型在强化学习中将是一个方向 [[27](#bib.bib27)]。这要求
    DDRL 方法能够同时处理大模型尺寸和批量尺寸的情况。目前，DDRL 的学习器基于诸如环形全减少或参数服务器等技术，每种技术都有其优点。例如，参数服务器可以将大模型存储在不同的
    GPU 中，而环形全减少可以在不同的 GPU 之间快速交换梯度。然而，这些技术尚未应用于强化学习中的大模型尺寸和批量尺寸。因此，如何将这些技术结合起来以适应
    DDRL 算法进行高效训练是未来的方向。
- en: Self-play and population-play based DDRL methods. Self-play and population-play
    are mainstream reinforcement learning agents evolution methods, which are widely
    adopted in current professional human-level AI systems, e.g., OpenAI Five [[6](#bib.bib6)]
    and AlphaStar [[7](#bib.bib7)]. Generally, self-play and population-play have
    no strict restrictions on the players, which means a player can fight against
    any past versions for the same player or different players. Those heuristic design
    makes exploring the best configuration a hard question, which also makes designing
    templates for a toolbox a tricky problem. In the future, self-play and population-play
    based DDRL methods are worthy of further study, e.g., adaptively finding out the
    best configuration.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 基于自我对弈和群体对弈的DDRL方法。自我对弈和群体对弈是主流的强化学习代理演化方法，广泛应用于当前的专业人类级AI系统，例如OpenAI Five [[6](#bib.bib6)]和AlphaStar
    [[7](#bib.bib7)]。通常，自我对弈和群体对弈对玩家没有严格的限制，这意味着一个玩家可以与任何过去版本的相同玩家或不同玩家对抗。这些启发式设计使得探索最佳配置成为一个困难的问题，这也使得为工具箱设计模板变得棘手。未来，基于自我对弈和群体对弈的DDRL方法值得进一步研究，例如自适应地找出最佳配置。
- en: Toolboxes construction and validation. Several famous scientific research institutions
    such as DeepMind, OpenAI, and UC Berkeley have released toolboxes to support DDRL
    methods. Most of them use gym to test the performance, such as data throughput,
    and linearity. However, environments in gym are relatively small compared with
    environments in real world applications. On the other hand, most of the testing
    uses one or two nodes/machines with limited numbers of CPU and GPU devices, making
    the testing insufficient to discover bottleneck of the toolboxes. Accordingly,
    even though most current DDRL toolboxes are highly modularized, the scalability
    to large number of machines for performing large learner parallel and actor parallel
    for complex environments are not fulled tested. Future bottlenecks of the toolboxes
    may be discovered with large testing.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 工具箱的构建和验证。许多著名的科学研究机构，如DeepMind、OpenAI和加州大学伯克利分校，已经发布了支持DDRL方法的工具箱。它们大多数使用gym来测试性能，例如数据吞吐量和线性度。然而，与实际应用中的环境相比，gym中的环境相对较小。另一方面，大多数测试使用一个或两个节点/机器，并且CPU和GPU设备数量有限，这使得测试不足以发现工具箱的瓶颈。因此，尽管目前大多数DDRL工具箱高度模块化，但对于在复杂环境中执行大规模学习器并行和演员并行的大量机器的可扩展性尚未充分测试。工具箱的未来瓶颈可能会通过大规模测试发现。
- en: 7 Conclusion
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we have surveyed representative distributed deep reinforcement
    learning methods. By summarizing key components to form a distributed deep reinforcement
    learning system, single player single agent distributed deep reinforcement learning
    methods are compared based on different types of coordinators. Furthermore, by
    introducing agents cooperation and players evolution, multiple players multiple
    agents distributed deep reinforcement learning approaches are elaborated. To support
    easy codes implementation, some popular distributed deep reinforcement learning
    toolboxes are introduced and discussed, based on which, a new multiple players
    and multiple agents learning toolbox is developed, hoping to assist learning for
    complex games. Finally, we discuss the challenges and opportunities of this exciting
    filed. Through this paper, we hope it becomes a reference for researchers and
    engineers when they are exploring novel reinforcement learning algorithms and
    solving practical reinforcement learning problems.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对代表性的分布式深度强化学习方法进行了调查。通过总结关键组件形成分布式深度强化学习系统，基于不同类型的协调器比较了单玩家单代理的分布式深度强化学习方法。此外，通过介绍代理合作和玩家演化，详细阐述了多玩家多代理的分布式深度强化学习方法。为了支持简单的代码实现，介绍和讨论了一些流行的分布式深度强化学习工具箱，并基于这些工具箱开发了一个新的多玩家多代理学习工具箱，希望能帮助复杂游戏的学习。最后，我们讨论了这一激动人心领域的挑战和机遇。通过本文，我们希望它能成为研究人员和工程师在探索新型强化学习算法和解决实际强化学习问题时的参考。
- en: References
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre *et al.*, “Mastering
    the game of go with deep neural networks and tree search,” *Nature*, vol. 529,
    pp. 484–489, 2016.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre *et al.*, “使用深度神经网络和树搜索掌握围棋游戏，”
    *自然*，第529卷，第484–489页，2016年。'
- en: '[2] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang *et al.*,
    “Mastering the game of go without human knowledge,” *Nature*, vol. 550, pp. 354–359,
    2017.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang *等*，“在没有人类知识的情况下掌握围棋，”
    *自然*，第 550 卷，第 354–359 页，2017。'
- en: '[3] Y. Yu, “Towards sample efficient reinforcement learning,” in *International
    Joint Conference on Artificial Intelligence*, 2018, pp. 5739–5743.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Yu，“朝着样本高效的强化学习前进，” 在 *国际人工智能联合大会*，2018，第 5739–5743 页。'
- en: '[4] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai *et al.*, “Pre-trained models for
    natural language processing: A survey,” *Science China Technological Sciences*,
    pp. 1–26, 2020.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai *等*，“自然语言处理的预训练模型：综述，” *科学中国技术科学*，第
    1–26 页，2020。'
- en: '[5] J. Li, S. Koyamada, Q. Ye, G. Liu, C. Wang *et al.*, “Suphx: mastering
    mahjong with deep reinforcement learning,” *arXiv:2003.13590v2*, 2020.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Li, S. Koyamada, Q. Ye, G. Liu, C. Wang *等*， “Suphx：使用深度强化学习掌握麻将，” *arXiv:2003.13590v2*，2020。'
- en: '[6] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak *et al.*, “Dota 2
    with large scale deep reinforcement learning,” *arXiv:1912.06680v1*, 2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak *等*，“使用大规模深度强化学习的
    Dota 2，” *arXiv:1912.06680v1*，2019。'
- en: '[7] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik *et al.*,
    “Grandmaster level in starcraft ii using multi-agent reinforcement learning,”
    *Nature*, vol. 575, pp. 350–354, 2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik *等*，“使用多智能体强化学习在《星际争霸
    II》中达到大师级别，” *自然*，第 575 卷，第 350–354 页，2019。'
- en: '[8] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon *et al.*, “Massively
    parallel methods for deep reinforcement learning,” in *Deep Learning Workshop,
    International Conference on Machine Learning*, 2015.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon *等*，“用于深度强化学习的大规模并行方法，”
    在 *深度学习研讨会，国际机器学习大会*，2015。'
- en: '[9] L. Espeholt, R. Marinier, P. Stanczyk, K. Wang *et al.*, “Seed rl: Scalable
    and efficient deep-rl with accelerated central inference,” in *International Conference
    on Learning Representations*, 2020.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] L. Espeholt, R. Marinier, P. Stanczyk, K. Wang *等*，“Seed RL：可扩展和高效的深度 RL
    与加速的中央推断，” 在 *国际学习表征大会*，2020。'
- en: '[10] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih *et al.*, “Impala:
    scalable distributed deep-rl with importance weighted actor-learner architectures,”
    *arXiv:1802.01561*, 2018.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih *等*，“Impala：可扩展的分布式深度
    RL 与重要性加权的演员-学习者架构，” *arXiv:1802.01561*，2018。'
- en: '[11] A. Sergeev and M. Del Balso, “Horovod: fast and easy distributed deep
    learning in tensorflow,” *arXiv:1802.05799*, 2018.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Sergeev 和 M. Del Balso，“Horovod：在 TensorFlow 中快速和简单的分布式深度学习，” *arXiv:1802.05799*，2018。'
- en: '[12] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw *et al.*, “Ray:
    A distributed framework for emerging $\{$AI$\}$ applications,” in *13th $\{$USENIX$\}$
    Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 18)*, 2018,
    pp. 561–577.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw *等*，“Ray：一个用于新兴
    $\{$AI$\}$ 应用的分布式框架，” 在 *第 13 届 $\{$USENIX$\}$ 操作系统设计与实现研讨会 ($\{$OSDI$\}$ 18)*，2018，第
    561–577 页。'
- en: '[13] E. Liang, R. Liaw, P. Moritz, R. Nishihara, R. Fox *et al.*, “Rllib: Abstractions
    for distributed reinforcement learning,” in *International Conference on Machine
    Learning*, 2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] E. Liang, R. Liaw, P. Moritz, R. Nishihara, R. Fox *等*，“Rllib：用于分布式强化学习的抽象，”
    在 *国际机器学习大会*，2018。'
- en: '[14] M. R. Samsami and H. Alimadad, “Distributed deep reinforcement learning:
    An overview,” in *Reinforcement Learning Algorithms: Analysis and Applications*,
    2021.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. R. Samsami 和 H. Alimadad，“分布式深度强化学习：概述，” 在 *强化学习算法：分析与应用*，2021。'
- en: '[15] J. Czech, “Distributed methods for reinforcement learning survey,” in
    *Reinforcement Learning Algorithms: Analysis and Applications*, 2021.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] J. Czech，“强化学习方法综述，” 在 *强化学习算法：分析与应用*，2021。'
- en: '[16] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
    reinforcement learning: A brief survey,” *IEEE Signal Processing Magazine*, vol. 34,
    no. 6, pp. 26–38, 2017.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] K. Arulkumaran, M. P. Deisenroth, M. Brundage 和 A. A. Bharath，“深度强化学习：简要综述，”
    *IEEE 信号处理杂志*，第 34 卷，第 6 期，第 26–38 页，2017。'
- en: '[17] T. M. Moerland, J. Broekens, and C. M. Jonker, “Model-based reinforcement
    learning: A survey,” *arXiv:2006.16712*, 2020.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] T. M. Moerland, J. Broekens 和 C. M. Jonker，“基于模型的强化学习：综述，” *arXiv:2006.16712*，2020。'
- en: '[18] S. Gronauer and K. Diepold, “Multi-agent deep reinforcement learning:
    a survey,” *Artificial Intelligence Review*, vol. 55, pp. 895–943, 2022.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Gronauer 和 K. Diepold，“多智能体深度强化学习：综述，” *人工智能评论*，第 55 卷，第 895–943 页，2022。'
- en: '[19] Y. Yang and J. Wang, “Multi-agent deep reinforcement learning: a survey,”
    *arXiv:2011.00583v3*, 2021.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Y. Yang 和 J. Wang，“多智能体深度强化学习：综述”，*arXiv:2011.00583v3*，2021年。'
- en: '[20] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Demystifying
    parallel and distributed deep learning: An in-depth concurrency analysis,” *Ben-Num,
    Tai and Torsten, Hoefler*, vol. 52, no. 4, pp. 1–43, 2020.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Arulkumaran, M. P. Deisenroth, M. Brundage, 和 A. A. Bharath，“揭开并行与分布式深度学习的神秘面纱：深入的并发分析”，*Ben-Num,
    Tai 和 Torsten, Hoefler*，第52卷，第4期，第1–43页，2020年。'
- en: '[21] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang *et al.*, “Terngrad: Ternary gradients
    to reduce communication in distributed deep learning,” in *Advances in Neural
    Information Processing Systems*, 2017, pp. 1509–1519.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang *等人*，“Terngrad：三值梯度以减少分布式深度学习中的通信”，在*神经信息处理系统进展*中，2017年，第1509–1519页。'
- en: '[22] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin *et al.*, “Large scale
    distributed deep networks,” in *Advances in Neural Information Processing Systems*,
    2012, pp. 1232–1240.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin *等人*，“大规模分布式深度网络”，在*神经信息处理系统进展*中，2012年，第1232–1240页。'
- en: '[23] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen *et al.*, “Tensorflow:
    Large-scale machine learning on heterogeneous distributed systems,” *https://arxiv.org/abs/1603.04467*,
    2016.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen *等人*，“Tensorflow：大规模异构分布式系统上的机器学习”，*https://arxiv.org/abs/1603.04467*，2016年。'
- en: '[24] T. Ben-Nun and T. Hoefler, “Demystifying parallel and distributed deep
    learning: An in-depth concurrency analysis,” *ACM Computing Surveys*, vol. 52,
    no. 4, pp. 1–43, 2020.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. Ben-Nun 和 T. Hoefler，“揭开并行与分布式深度学习的神秘面纱：深入的并发分析”，*ACM计算调查*，第52卷，第4期，第1–43页，2020年。'
- en: '[25] J. Park, S. Samarakoon, A. Elgabli, J. Kim, M. Bennis *et al.*, “Communication-efficient
    and distributed learning over wireless networks: principles and applications,”
    *Proceedings of the IEEE*, vol. 109, no. 5, pp. 796–819, 2021.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J. Park, S. Samarakoon, A. Elgabli, J. Kim, M. Bennis *等人*，“无线网络上的通信高效分布式学习：原理与应用”，*IEEE汇刊*，第109卷，第5期，第796–819页，2021年。'
- en: '[26] T.-C. Chiu, Y.-Y. Shih, A.-C. Pang, C.-S. Wang, W. Weng *et al.*, “Semisupervised
    distributed learning with non-iid data for aiot service platform,” *IEEE Internet
    of Things Journal*, vol. 7, no. 10, pp. 9266–9277, 2020.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] T.-C. Chiu, Y.-Y. Shih, A.-C. Pang, C.-S. Wang, W. Weng *等人*，“针对aiot服务平台的半监督分布式学习与非iid数据”，*IEEE物联网期刊*，第7卷，第10期，第9266–9277页，2020年。'
- en: '[27] Q. Yin, J. Yang, K. Huang, M. Zhao, W. Ni *et al.*, “Ai in human-computer
    gaming: techniques, challenges and opportunities,” *arXiv:2111.07631v2*, 2022.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Q. Yin, J. Yang, K. Huang, M. Zhao, W. Ni *等人*，“人机游戏中的ai：技术、挑战与机会”，*arXiv:2111.07631v2*，2022年。'
- en: '[28] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, and J. Veness, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, pp. 529–533,
    2015.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, 和 J. Veness，“通过深度强化学习实现人类级别的控制”，*自然*，第518卷，第529–533页，2015年。'
- en: '[29] Y. Burda, H. Edwards, A. Storkey, and O. Klimov, “Exploration by random
    network distillation,” *https://doi.org/10.48550/arXiv.1810.12894*, 2018.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Y. Burda, H. Edwards, A. Storkey, 和 O. Klimov，“通过随机网络蒸馏进行探索”，*https://doi.org/10.48550/arXiv.1810.12894*，2018年。'
- en: '[30] M. Samvelyan, T. Rashid, C. S. d. Witt, G. Farquhar, N. Nardelli *et al.*,
    “The starcraft multi-agent challenge,” *https://doi.org/10.48550/arXiv.1902.04043*,
    2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] M. Samvelyan, T. Rashid, C. S. d. Witt, G. Farquhar, N. Nardelli *等人*，“星际争霸多智能体挑战”，*https://doi.org/10.48550/arXiv.1902.04043*，2019年。'
- en: '[31] M. Lanctot, E. Lockhart, J.-B. Lespiau, V. Zambaldi, S. Upadhyay *et al.*,
    “Openspiel: A framework for reinforcement learning in games,” *arXiv:1908.09453v6*,
    2020.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] M. Lanctot, E. Lockhart, J.-B. Lespiau, V. Zambaldi, S. Upadhyay *等人*，“Openspiel：游戏中的强化学习框架”，*arXiv:1908.09453v6*，2020年。'
- en: '[32] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap *et al.*, “Asynchronous
    methods for deep reinforcement learning,” in *International Conference on Machine
    Learning*, 2016, pp. 1928–1937.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap *等人*，“深度强化学习的异步方法”，在*国际机器学习会议*中，2016年，第1928–1937页。'
- en: '[33] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel *et al.*, “Distributed
    prioritized experience replay,” in *International Conference on Learning Representations*,
    2018.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel *等人*，“分布式优先经验回放”，在*国际学习表征会议*中，2018年。'
- en: '[34] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel *et al.*, “Ai in human-computer
    gaming: techniques, challenges and opportunities,” *arXiv:1707.02286*, 2017.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel *等人*，“人机游戏中的ai：技术、挑战与机会”，*arXiv:1707.02286*，2017年。'
- en: '[35] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney, “Recurrent
    experience replay in distributed reinforcement learning,” in *International Conference
    on Learning Representations*, 2019.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, 和 W. Dabney，“分布式强化学习中的递归经验回放，”在
    *国际学习表征会议*，2019 年。'
- en: '[36] D. Ye, G. Chen, W. Zhang, S. Chen, B. Yuan *et al.*, “Towards playing
    full moba games with deep reinforcement learning,” in *Neural Information Processing
    Systems*, 2020.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] D. Ye, G. Chen, W. Zhang, S. Chen, B. Yuan *等*，“通过深度强化学习玩完整 MOBA 游戏，”在
    *神经信息处理系统*，2020 年。'
- en: '[37] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz, “Reinforcement
    learning through asynchronous advantage actor-critic on a gpu,” in *International
    Conference on Learning Representations*, 2017.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, 和 J. Kautz，“通过 GPU 上的异步优势演员-评论员进行强化学习，”在
    *国际学习表征会议*，2017 年。'
- en: '[38] A. Stooke and P. Abbeel, “Accelerated methods for deep reinforcement learning,”
    *arXiv:1803.02811v2*, 2019.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Stooke 和 P. Abbeel，“加速的深度强化学习方法，”*arXiv:1803.02811v2*，2019 年。'
- en: '[39] A. V. Clemente, H. N. Castej´on, and A. Chandra, “Efficient parallel methods
    for deep reinforcement learning,” *arXiv:1705.04862v2*, 2017.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] A. V. Clemente, H. N. Castejón, 和 A. Chandra，“深度强化学习的高效并行方法，”*arXiv:1705.04862v2*，2017
    年。'
- en: '[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv:1707.06347*, 2017.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, 和 O. Klimov，“近端策略优化算法，”*arXiv:1707.06347*，2017
    年。'
- en: '[41] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa *et al.*, “Dd-ppo: Learning
    near-perfect pointgoal navigators from 2.5 billion frames,” in *International
    Conference on Learning Representations*, 2020.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa *等*，“Dd-ppo：从 25 亿帧中学习近乎完美的点目标导航器，”在
    *国际学习表征会议*，2020 年。'
- en: '[42] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever *et al.*,
    “Human-level performance in 3d multiplayer games with populationbased reinforcement
    learning,” *Science*, vol. 364, pp. 859–865, 2019.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever *等*，“基于人群的强化学习在
    3D 多人游戏中的人类水平表现，”*Science*，卷 364，页 859–865，2019 年。'
- en: '[43] D. Zha, J. Xie, W. Ma, S. Zhang, X. Lian, X. Hu, and J. Liu, “Douzero:
    Mastering doudizhu with self-play deep reinforcement learning,” in *International
    Conference on Machine Learning*, 2021.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] D. Zha, J. Xie, W. Ma, S. Zhang, X. Lian, X. Hu, 和 J. Liu，“Douzero：通过自我博弈深度强化学习掌握斗地主，”在
    *国际机器学习会议*，2021 年。'
- en: '[44] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell *et al.*, “Emergent
    tool use from multi-agent autocurricula,” *arXiv:1909.07528*, 2019.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell *等*，“多智能体自适应课程中涌现的工具使用，”*arXiv:1909.07528*，2019
    年。'
- en: '[45] T. Rashid, M. Samvelyan, C. S. d. Witt, G. Farquhar, J. N. Foerster, and
    S. Whiteson, “Qmix: Monotonic value function factorisation for deep multi-agent
    reinforcement learning,” in *International Conference on Machine Learning*, 2018,
    pp. 4292–4301.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] T. Rashid, M. Samvelyan, C. S. d. Witt, G. Farquhar, J. N. Foerster, 和
    S. Whiteson，“Qmix：用于深度多智能体强化学习的单调价值函数分解，”在 *国际机器学习会议*，2018 年，页 4292–4301。'
- en: '[46] M. Samvelyan, T. Rashid, C. S. d. Witt, G. Farquhar, N. Nardelli *et al.*,
    “The starcraft multi-agent challenge,” *arXiv:1902.04043v5*, 2019.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] M. Samvelyan, T. Rashid, C. S. d. Witt, G. Farquhar, N. Nardelli *等*，“星际争霸多智能体挑战，”*arXiv:1902.04043v5*，2019
    年。'
- en: '[47] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai *et al.*,
    “A general reinforcement learning algorithm that masters chess, shogi, and go
    through self-play,” *Science*, vol. 362, pp. 1140–1144, 2018.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai *等*，“一种通用强化学习算法，通过自我博弈掌握国际象棋、将棋和围棋，”*Science*，卷
    362，页 1140–1144，2018 年。'
- en: '[48] X. Wang, J. Song, P. Qi, P. Peng, Z. Tang *et al.*, “Scc: an efficient
    deep reinforcement learning agent mastering the game of starcraft ii,” in *International
    Conference on Machine Learning*, 2021.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] X. Wang, J. Song, P. Qi, P. Peng, Z. Tang *等*，“SCC：一个高效的深度强化学习智能体，掌握星际争霸
    II 游戏，”在 *国际机器学习会议*，2021 年。'
- en: '[49] J. Paredis, “Coevolutionary computation,” *Artificial life*, vol. 2, no. 4,
    pp. 355–375, 1995.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] J. Paredis，“协同进化计算，”*人工生命*，卷 2，第 4 期，页 355–375，1995 年。'
- en: '[50] N. Brown and T. Sandholm, “Superhuman ai for heads-up no-limit poker:
    Libratus beats top professionals,” *Science*, vol. 359, pp. 418–424, 2018.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] N. Brown 和 T. Sandholm，“超人类 AI 在 Heads-up 无限制扑克中的表现：Libratus 击败顶级职业玩家，”*Science*，卷
    359，页 418–424，2018 年。'
- en: '[51] M. Moravčík, M. Schmid, N. Burch, V. Lisý, D. Morrill *et al.*, “Deepstack:
    Expert-level artificial intelligence in heads-up no-limit poker,” *Science*, vol.
    356, pp. 508–513, 2017.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] M. Moravčík, M. Schmid, N. Burch, V. Lisý, D. Morrill *等*，“Deepstack:
    高水平的头对头无限注扑克人工智能，” *Science*，第356卷，第508–513页，2017年。'
- en: '[52] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi-agent
    actor-critic for mixed cooperative-competitive environments,” *arXiv:1706.02275v4*,
    2017.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel 和 I. Mordatch，“混合合作-竞争环境中的多智能体演员-评论家方法，”
    *arXiv:1706.02275v4*，2017年。'
- en: '[53] M. W. Hoffman, B. Shahriari, J. Aslanides, G. Barth-Maron, N. Momchev
    *et al.*, “Acme: A research framework for distributed reinforcement learning,”
    *arXiv:2006.00979v2*, 2020.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. W. Hoffman, B. Shahriari, J. Aslanides, G. Barth-Maron, N. Momchev
    *等*，“Acme: 一个用于分布式强化学习的研究框架，” *arXiv:2006.00979v2*，2020年。'
- en: '[54] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approximation
    error in actor-critic methods,” in *International Conference on Machine Learning*,
    2018, pp. 1587–1596.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Fujimoto, H. Hoof 和 D. Meger，“解决演员-评论家方法中的函数逼近误差，” 收录于 *International
    Conference on Machine Learning*，2018年，第1587–1596页。'
- en: '[55] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in *Advances
    in Neural Information Processing Systems*, 2016.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. Ho 和 S. Ermon，“生成对抗模仿学习，” 收录于 *Advances in Neural Information Processing
    Systems*，2016年。'
- en: '[56] S. Reddy, A. D. Dragan, and S. Levine, “Sqil: Imitation learning via reinforcement
    learning with sparse rewards,” *https://doi.org/10.48550/arXiv.1905.11108*, 2019.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] S. Reddy, A. D. Dragan 和 S. Levine，“Sqil: 通过稀疏奖励的强化学习进行模仿学习，” *https://doi.org/10.48550/arXiv.1905.11108*，2019年。'
- en: '[57] J. Weng, H. Chen, D. Yan, K. You, A. Duburcq *et al.*, “Tianshou: A highly
    modularized deep reinforcement learning library,” *Journal of Machine Learning
    Research*, vol. 23, no. 267, pp. 1–6, 2022.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] J. Weng, H. Chen, D. Yan, K. You, A. Duburcq *等*，“Tianshou: 一个高度模块化的深度强化学习库，”
    *Journal of Machine Learning Research*，第23卷，第267号，第1–6页，2022年。'
- en: '[58] H. Küttler, N. Nardelli, T. Lavril, M. Selvatici, V. Sivakumar *et al.*,
    “Torchbeast: A pytorch platform for distributed rl,” *arXiv:1910.03552v1*, 2019.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] H. Küttler, N. Nardelli, T. Lavril, M. Selvatici, V. Sivakumar *等*，“Torchbeast:
    一个用于分布式RL的pytorch平台，” *arXiv:1910.03552v1*，2019年。'
- en: '[59] M. Zhou, Z. Wan, H. Wang, M. Wen, R. Wu *et al.*, “Malib: A parallel framework
    for population-based multi-agent reinforcement learning,” *arXiv:2106.07551*,
    2021.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. Zhou, Z. Wan, H. Wang, M. Wen, R. Wu *等*，“Malib: 一个用于基于人群的多智能体强化学习的并行框架，”
    *arXiv:2106.07551*，2021年。'
- en: '[60] P. Muller, S. Omidshafiei, M. Rowland, K. Tuyls, J. Perolat *et al.*,
    “A generalized training approach for multiagent learning,” *https://doi.org/10.48550/arXiv.1909.12823*,
    2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] P. Muller, S. Omidshafiei, M. Rowland, K. Tuyls, J. Perolat *等*，“一种用于多智能体学习的通用训练方法，”
    *https://doi.org/10.48550/arXiv.1909.12823*，2019年。'
- en: '[61] S. Mcaleer, J. Lanier, R. Fox, and P. Baldi, “Pipeline psro: A scalable
    approach for finding approximate nash equilibria in large games,” in *Advances
    in Neural Information Processing Systems*, 2020.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] S. Mcaleer, J. Lanier, R. Fox 和 P. Baldi，“Pipeline psro: 一种用于在大型游戏中找到近似纳什均衡的可扩展方法，”
    收录于 *Advances in Neural Information Processing Systems*，2020年。'
- en: '[62] J. Heinrich, M. Lanctot, and D. Silver, “Fictitious self-play in extensive-form
    games,” in *International Conference on Machine Learning*, 2015, pp. 805–813.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] J. Heinrich, M. Lanctot 和 D. Silver，“广泛形式游戏中的虚假自我对抗，” 收录于 *International
    Conference on Machine Learning*，2015年，第805–813页。'
- en: '[63] Q. Yin, M. Zhao, W. Ni, J. Zhang, and K. Huang, “Intelligent decision
    making technology and challenge of wargame,” *Acta Automatica Sinica*, vol. 47,
    2021.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Q. Yin, M. Zhao, W. Ni, J. Zhang 和 K. Huang，“智能决策技术与战争游戏挑战，” *Acta Automatica
    Sinica*，第47卷，2021年。'
- en: '[64] H. Jia, Y. Hu, Y. Chen, C. Ren, T. Lv *et al.*, “Fever basketball: A complex,
    flexible, and asynchronized sports game environment for multi-agent reinforcement
    learning,” *arXiv:2012.03204*, 2020.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] H. Jia, Y. Hu, Y. Chen, C. Ren, T. Lv *等*，“Fever basketball: 一个复杂、灵活且异步的体育游戏环境，用于多智能体强化学习，”
    *arXiv:2012.03204*，2020年。'
