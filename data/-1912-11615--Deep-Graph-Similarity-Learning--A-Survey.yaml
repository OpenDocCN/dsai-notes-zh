- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:03:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 20:03:25'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1912.11615] Deep Graph Similarity Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1912.11615] 深度图相似度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.11615](https://ar5iv.labs.arxiv.org/html/1912.11615)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1912.11615](https://ar5iv.labs.arxiv.org/html/1912.11615)
- en: ∎
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '¹¹institutetext: G. Ma ²²institutetext: Intel Labs, Intel Corporation, Hillsboro,
    OR 97124, USA'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹institutetext: G. Ma ²²institutetext: Intel Labs, Intel Corporation, Hillsboro,
    OR 97124, USA'
- en: '²²email: guixiang.ma@intel.com ³³institutetext: N. K. Ahmed ⁴⁴institutetext:
    Intel Labs, Intel Corporation, Santa Clara, CA 95054, USA'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '²²email: guixiang.ma@intel.com ³³institutetext: N. K. Ahmed ⁴⁴institutetext:
    Intel Labs, Intel Corporation, Santa Clara, CA 95054, USA'
- en: '⁴⁴email: nesreen.k.ahmed@intel.com ⁵⁵institutetext: T. Willke ⁶⁶institutetext:
    Intel Labs, Intel Corporation, Hillsboro, OR 97124, USA'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '⁴⁴email: nesreen.k.ahmed@intel.com ⁵⁵institutetext: T. Willke ⁶⁶institutetext:
    Intel Labs, Intel Corporation, Hillsboro, OR 97124, USA'
- en: '⁶⁶email: ted.willke@intel.com ⁷⁷institutetext: P. Yu ⁸⁸institutetext: Department
    of Computer Science, University of Illinois at Chicago, IL 60607, USA'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '⁶⁶email: ted.willke@intel.com ⁷⁷institutetext: P. Yu ⁸⁸institutetext: Department
    of Computer Science, University of Illinois at Chicago, IL 60607, USA'
- en: '⁸⁸email: psyu@uic.edu'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '⁸⁸email: psyu@uic.edu'
- en: 'Deep Graph Similarity Learning: A Survey'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度图相似度学习：综述
- en: Guixiang Ma    Nesreen K. Ahmed    Theodore L. Willke    Philip S. Yu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Guixiang Ma    Nesreen K. Ahmed    Theodore L. Willke    Philip S. Yu
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In many domains where data are represented as graphs, learning a similarity
    metric among graphs is considered a key problem, which can further facilitate
    various learning tasks, such as classification, clustering, and similarity search.
    Recently, there has been an increasing interest in deep graph similarity learning,
    where the key idea is to learn a deep learning model that maps input graphs to
    a target space such that the distance in the target space approximates the structural
    distance in the input space. Here, we provide a comprehensive review of the existing
    literature of deep graph similarity learning. We propose a systematic taxonomy
    for the methods and applications. Finally, we discuss the challenges and future
    directions for this problem.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多将数据表示为图的领域中，学习图之间的相似度度量被认为是一个关键问题，这可以进一步促进各种学习任务，如分类、聚类和相似性搜索。最近，对深度图相似度学习的兴趣日益增加，其中关键思想是学习一个深度学习模型，将输入图映射到目标空间，使目标空间中的距离接近输入空间中的结构距离。在这里，我们提供了对现有深度图相似度学习文献的全面回顾。我们提出了一种系统的分类方法来对这些方法和应用进行分类。最后，我们讨论了这个问题的挑战和未来方向。
- en: 'Keywords:'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Metric learning, Similarity learning, Graph Neural Networks, Graph Convolutional
    Networks, Higher-order Networks, Graph Similarity, Structural Similarity, Graph
    Matching, Deep graph similarity learning
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Metric learning, Similarity learning, Graph Neural Networks, Graph Convolutional
    Networks, Higher-order Networks, Graph Similarity, Structural Similarity, Graph
    Matching, Deep graph similarity learning
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Learning an adequate similarity measure on a feature space can significantly
    determine the performance of machine learning methods. Learning such measures
    automatically from data is the primary aim of similarity learning. Similarity/Metric
    learning refers to learning a function to measure the distance or similarity between
    objects, which is a critical step in many machine learning problems, such as classification,
    clustering, ranking, etc. For example, in k-Nearest Neighbor (kNN) classification
    [cover1967nearest](#bib.bib30) , a metric is needed for measuring the distance
    between data points and identifying the nearest neighbors; in many clustering
    algorithms, similarity measurements between data points are used to determine
    the clusters. Although there are some general metrics like Euclidean distance
    that can be used for getting similarity measure between objects represented as
    vectors, these metrics often fail to capture the specific characteristics of the
    data being studied, especially for structured data. Therefore, it is essential
    to find or learn a metric for measuring the similarity of data points involved
    in the specific task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征空间上学习适当的相似度度量可以显著决定机器学习方法的性能。自动从数据中学习这种度量是相似度学习的主要目标。相似度/度量学习指的是学习一个函数来测量对象之间的距离或相似度，这在许多机器学习问题中是一个关键步骤，如分类、聚类、排序等。例如，在k-最近邻（kNN）分类中[cover1967nearest](#bib.bib30)，需要一种度量来测量数据点之间的距离并识别最近邻；在许多聚类算法中，数据点之间的相似度测量用于确定聚类。虽然有一些通用的度量如欧几里得距离可以用于获取对象之间的相似度，但这些度量通常不能捕捉到被研究数据的特定特征，尤其是对于结构化数据。因此，寻找或学习一种度量来测量特定任务中数据点的相似度是至关重要的。
- en: Metric learning has been widely studied in many fields on various data types.
    For instance, in computer vision, metric learning has been explored on images
    or videos for image classification, object recognition, visual tracking, and other
    learning tasks [mensink2012metric](#bib.bib85) ; [guillaumin2009you](#bib.bib52)
    ; [jiang2012order](#bib.bib61) . In information retrieval, such as in search engines,
    metric learning has been used to determine the ranking of relevant documents to
    a given query [lee2008rank](#bib.bib71) ; [lim2013robust](#bib.bib74) . In this
    paper, we survey the existing work in similarity learning for graphs, which encode
    relational structures and are ubiquitous in various domains.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 度量学习在各种数据类型的许多领域中得到了广泛研究。例如，在计算机视觉中，度量学习已经在图像或视频上进行了探索，用于图像分类、物体识别、视觉跟踪以及其他学习任务
    [mensink2012metric](#bib.bib85) ; [guillaumin2009you](#bib.bib52) ; [jiang2012order](#bib.bib61)
    。在信息检索中，例如在搜索引擎中，度量学习被用于确定与给定查询相关文档的排名 [lee2008rank](#bib.bib71) ; [lim2013robust](#bib.bib74)
    。在本文中，我们调查了图形相似性学习的现有工作，这些图形编码了关系结构，并在各个领域中广泛存在。
- en: Similarity learning for graphs has been studied for many real applications,
    such as molecular graph classification in chemoinformatics [horvath2004cyclic](#bib.bib56)
    ; [frohlich2006kernel](#bib.bib42) , protein-protein interaction network analysis
    for disease prediction [borgwardt2007graph](#bib.bib22) , binary function similarity
    search in computer security [li2019graph](#bib.bib73) , multi-subject brain network
    similarity learning for neurological disorder analysis [ktena2018metric](#bib.bib67)
    , etc. In many of these application scenarios, the number of training samples
    available is often very limited, making it a difficult problem to directly train
    a classification or prediction model. With graph similarity learning strategies,
    these applications benefit from pairwise learning that utilizes every pair of
    training samples to learn a metric for mapping the input data to the target space,
    which further facilitates the specific learning task.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图形相似性学习已被用于许多实际应用，例如化学信息学中的分子图分类 [horvath2004cyclic](#bib.bib56) ; [frohlich2006kernel](#bib.bib42)
    、疾病预测中的蛋白质-蛋白质相互作用网络分析 [borgwardt2007graph](#bib.bib22) 、计算机安全中的二元函数相似性搜索 [li2019graph](#bib.bib73)
    、用于神经系统疾病分析的多主体脑网络相似性学习 [ktena2018metric](#bib.bib67) 等。在许多这些应用场景中，可用的训练样本数量通常非常有限，这使得直接训练分类或预测模型成为一个困难的问题。通过图形相似性学习策略，这些应用受益于成对学习，利用每一对训练样本来学习将输入数据映射到目标空间的度量，从而进一步促进特定学习任务的进行。
- en: In the past few decades, many techniques have emerged for studying the similarity
    of graphs. Early on, multiple graph similarity metrics were defined, such as the
    Graph Edit Distance [bunke1983inexact](#bib.bib26) , Maximum Common Subgraph [bunke1998graph](#bib.bib27)
    ; [wallis2001graph](#bib.bib119) , and Graph Isomorphism [dijkman2009graph](#bib.bib36)
    ; [berretti2001efficient](#bib.bib19) , to address the problem of graph similarity
    search and graph matching. However, the computation of these metrics is an NP-complete
    problem in general [zeng2009comparing](#bib.bib135) . Although some pruning strategies
    and heuristic methods have been proposed to approximate the values and speed up
    the computation, it is difficult to analyze the computational complexities of
    the above heuristic algorithms and the sub-optimal solutions provided by them
    are also unbounded [zeng2009comparing](#bib.bib135) . Therefore, these approaches
    are feasible only for graphs of relatively small size and in practical applications
    where these metrics are of primary interest. Thus it is hard to adapt these methods
    to new tasks. In addition, for other methods that are relatively more efficient
    like the Weisfeiler-Lehman method in [douglas2011weisfeiler](#bib.bib38) , since
    it is developed specifically for isomorphism testing without mapping functions,
    it cannot be applied for general graph similarity learning. More recently, researchers
    have formulated similarity estimation as a learning problem where the goal is
    to learn a model that maps a pair of graphs to a similarity score based on the
    graph representations. For example, graph kernels, such as path-based kernels
    [borgwardt2005shortest](#bib.bib21) and the subgraph matching kernel [yan2005substructure](#bib.bib129)
    ; [yoshida2019learning](#bib.bib132) , were proposed for graph similarity learning.
    Traditional graph embedding techniques, such as geometric embedding, are also
    leveraged for graph similarity learning [johansson2015learning](#bib.bib62) .
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几十年里，许多技术已出现用于研究图的相似性。早期，定义了多种图相似性度量标准，例如图编辑距离 [bunke1983inexact](#bib.bib26)、最大公共子图
    [bunke1998graph](#bib.bib27)；[wallis2001graph](#bib.bib119) 和图同构 [dijkman2009graph](#bib.bib36)；[berretti2001efficient](#bib.bib19)，以解决图相似性搜索和图匹配问题。然而，这些度量标准的计算通常是一个
    NP 完全问题 [zeng2009comparing](#bib.bib135)。虽然提出了一些修剪策略和启发式方法来近似这些值并加速计算，但分析上述启发式算法的计算复杂度是困难的，它们提供的次优解也是无界的
    [zeng2009comparing](#bib.bib135)。因此，这些方法仅对相对较小的图和在这些度量标准为主要关注点的实际应用中是可行的。因此，很难将这些方法适应到新任务中。此外，对于其他相对高效的方法，如
    [douglas2011weisfeiler](#bib.bib38) 中的 Weisfeiler-Lehman 方法，由于其专门为同构测试而开发且没有映射函数，因此无法应用于一般的图相似性学习。最近，研究人员将相似性估计形式化为一个学习问题，其目标是学习一个模型，将一对图映射到基于图表示的相似性分数。例如，图核，例如基于路径的核
    [borgwardt2005shortest](#bib.bib21) 和子图匹配核 [yan2005substructure](#bib.bib129)；[yoshida2019learning](#bib.bib132)，被提出用于图相似性学习。传统的图嵌入技术，例如几何嵌入，也被用于图相似性学习
    [johansson2015learning](#bib.bib62)。
- en: With the emergence of deep learning techniques, graph neural networks (GNNs)
    have become a powerful new tool for learning representations on graphs with various
    structures for various tasks. The main distinction between GNNs and the traditional
    graph embedding is that GNNs address graph-related tasks in an end-to-end manner,
    where the representation learning and the target learning task are conducted jointly
    [wu2020comprehensive](#bib.bib126) , while the graph embedding generally learns
    graph representations in an isolated stage and the learned representations are
    then used for the target task. Therefore, the GNN deep models can better leverage
    the graph features for the specific learning task compared to the graph embedding
    methods. Moreover, GNNs are easily adapted and extended for various graph related
    tasks, including deep graph similarity learning tasks in different domains. For
    instance, in brain connectivity network analysis in neuroscience, community structure
    among the nodes (i.e. brain regions) within the brain network is an important
    factor that should be considered when learning node representations for cross-subject
    similarity analysis. However, none of the traditional graph embedding methods
    are able to capture such special structure and jointly leverage the learned node
    representations for similarity learning on brain networks. In [ma2019similarity](#bib.bib78)
    , a higher-order GNN model is developed to encode the community-structure of brain
    networks during the representation learning and leverage it for the similarity
    learning task on these brain networks. Some more examples from other domains include
    the GNN-based graph similarity predictive models introduced for chemical compound
    queries in computational chemistry [bai2019simgnn](#bib.bib15) , and the deep
    graph matching networks proposed for binary function similarity search and malware
    detection in computer security [li2019graph](#bib.bib73) ; [ijcai2019-522](#bib.bib122)
    .
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习技术的出现，图神经网络（GNNs）已成为一种强大的新工具，用于学习具有各种结构的图的表示，以应对各种任务。GNNs与传统的图嵌入的主要区别在于，GNNs以端到端的方式处理图相关任务，其中表示学习和目标学习任务是共同进行的[wu2020comprehensive](#bib.bib126)，而图嵌入通常在孤立的阶段学习图表示，之后再将学习到的表示用于目标任务。因此，与图嵌入方法相比，GNN深度模型能够更好地利用图特征来完成特定的学习任务。此外，GNNs可以轻松地适应和扩展到各种图相关任务，包括不同领域的深度图相似性学习任务。例如，在神经科学中的脑连接网络分析中，脑网络中节点（即脑区）之间的社区结构是学习节点表示以进行跨受试者相似性分析时应考虑的一个重要因素。然而，传统的图嵌入方法都无法捕捉这种特殊结构，并且无法共同利用学习到的节点表示来进行脑网络的相似性学习。在[ma2019similarity](#bib.bib78)中，开发了一种高阶GNN模型，用于在表示学习过程中编码脑网络的社区结构，并将其用于这些脑网络的相似性学习任务。其他领域的一些例子包括用于计算化学中的化学化合物查询的GNN-based图相似性预测模型[bai2019simgnn](#bib.bib15)以及用于计算机安全中的二元函数相似性搜索和恶意软件检测的深度图匹配网络[li2019graph](#bib.bib73)；[ijcai2019-522](#bib.bib122)。
- en: 'In this survey paper, we provide a systematic review of the existing work in
    deep graph similarity learning. Based on the different graph representation learning
    strategies and how they are leveraged for the deep graph similarity learning task,
    we propose to categorize deep graph similarity learning models into three groups:
    Graph Embedding based-methods, GNN-based methods, and Deep Graph Kernel-based
    methods. Additionally, we sub-categorize the models based on their properties.
    Table  [2](#S3.T2 "Table 2 ‣ 3 Taxonomy of Models ‣ Deep Graph Similarity Learning:
    A Survey") shows our proposed taxonomy, with some example models for each category
    as well as the relevant applications. In this survey, we will illustrate how these
    different categories of models approach the graph similarity learning problem.
    We will also discuss the loss functions used for the graph similarity learning
    task.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '在这篇综述论文中，我们提供了对现有深度图相似性学习工作的系统性回顾。根据不同的图表示学习策略及其在深度图相似性学习任务中的应用，我们建议将深度图相似性学习模型分为三类：基于图嵌入的方法、基于GNN的方法以及基于深度图核的方法。此外，我们根据模型的属性对这些模型进行细分。表[2](#S3.T2
    "Table 2 ‣ 3 Taxonomy of Models ‣ Deep Graph Similarity Learning: A Survey")展示了我们提出的分类法，其中包括每个类别的一些示例模型及相关应用。在这项综述中，我们将阐明这些不同类别的模型如何处理图相似性学习问题。我们还将讨论用于图相似性学习任务的损失函数。'
- en: Scope and Contributions.
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 范围与贡献。
- en: 'This paper is focused on surveying the recently emerged deep models for graph
    similarity learning, where the goal is to use deep strategies on graphs for learning
    the similarity of given pairs of graphs, instead of computing similarity scores
    based on predefined measures. We emphasize that this paper does not attempt to
    survey the extensive literature on graph representation learning, graph neural
    networks, and graph embedding. Prior work has focused on these topics (see [cai2018comprehensive](#bib.bib28)
    ; [goyal2018graph](#bib.bib49) ; [lee2019attention](#bib.bib70) ; [wu2019comprehensive](#bib.bib127)
    ; [rossi2019community](#bib.bib101) ; [cui2018survey](#bib.bib31) ; [zhang2018network](#bib.bib136)
    for examples). Here instead, we focus on deep graph representation learning methods
    that explicitly focus on modeling graph similarity. To the best of our knowledge,
    this is the first survey paper on this problem. We summarize the main contributions
    of this paper as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本文聚焦于对新近出现的图相似性学习深度模型进行调查，目标是使用深度策略在图上学习给定图对的相似性，而不是基于预定义的度量计算相似性分数。我们强调本文不尝试调查图表示学习、图神经网络和图嵌入的大量文献。先前的工作集中于这些主题（参见[cai2018comprehensive](#bib.bib28)
    ; [goyal2018graph](#bib.bib49) ; [lee2019attention](#bib.bib70) ; [wu2019comprehensive](#bib.bib127)
    ; [rossi2019community](#bib.bib101) ; [cui2018survey](#bib.bib31) ; [zhang2018network](#bib.bib136)）。相反，我们专注于那些明确关注图相似性建模的深度图表示学习方法。根据我们所知，这是第一个关于这个问题的调查论文。我们总结了本文的主要贡献如下：
- en: $\circ$
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\circ$
- en: Two comprehensive taxonomies to categorize the literature of the emerging field
    of deep graph similarity learning, based on the type of models and the type of
    features adopted by the existing methods, respectively.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两种全面的分类法，用于根据模型类型和现有方法采用的特征类型，对新兴领域深度图相似性学习的文献进行分类。
- en: $\circ$
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\circ$
- en: Summary and discussion of the key techniques and building blocks of the models
    in each category.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对每个类别模型的关键技术和构建块进行总结和讨论。
- en: $\circ$
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\circ$
- en: Summary and comparison of the different deep graph similarity learning models
    across the taxonomy.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对不同的深度图相似性学习模型进行总结和比较，按照分类法进行组织。
- en: $\circ$
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\circ$
- en: Summary and discussion of the real-world applications that can benefit from
    deep graph similarity learning in a variety of domains.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对深度图相似性学习在各个领域中能够带来的实际应用进行总结和讨论。
- en: $\circ$
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\circ$
- en: Summary and discussion of the major challenges for deep graph similarity learning,
    the future directions, and the open problems.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对深度图相似性学习的主要挑战、未来方向和未解决的问题进行总结和讨论。
- en: Organization.
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 组织结构。
- en: 'The rest of the paper is organized as follows. In Section [2](#S2 "2 Notation
    and Preliminaries ‣ Deep Graph Similarity Learning: A Survey"), we introduce notation,
    preliminary concepts, and define the graph similarity learning problem. In Section
    3, we introduce the taxonomy with detailed illustrations of the existing deep
    models. In Section 4, we summarize the datasets and evaluations adopted in the
    existing works. In Section [5](#S5 "5 Applications ‣ Deep Graph Similarity Learning:
    A Survey"), we present the applications of deep graph similarity learning in various
    domains. In Section [6](#S6 "6 Challenges ‣ Deep Graph Similarity Learning: A
    Survey"), we discuss the remaining challenges in this area and highlight future
    directions. Finally, we conclude in Section [7](#S7 "7 Conclusion ‣ Deep Graph
    Similarity Learning: A Survey").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分组织如下。在第[2](#S2 "2 Notation and Preliminaries ‣ Deep Graph Similarity
    Learning: A Survey")节中，我们介绍了符号、初步概念，并定义了图相似性学习问题。在第 3 节中，我们介绍了具有详细插图的现有深度模型分类。在第
    4 节中，我们总结了现有工作中采用的数据集和评估方法。在第[5](#S5 "5 Applications ‣ Deep Graph Similarity Learning:
    A Survey")节中，我们展示了深度图相似性学习在各个领域的应用。在第[6](#S6 "6 Challenges ‣ Deep Graph Similarity
    Learning: A Survey")节中，我们讨论了该领域的剩余挑战并强调未来方向。最后，在第[7](#S7 "7 Conclusion ‣ Deep
    Graph Similarity Learning: A Survey")节中，我们做出总结。'
- en: 2 Notation and Preliminaries
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 符号和初步概念
- en: 'In this section, we provide the necessary notation and definitions of the fundamental
    concepts pertaining to the graph similarity problem that will be used throughout
    this survey. The notation is summarized in Table [1](#S2.T1 "Table 1 ‣ 2 Notation
    and Preliminaries ‣ Deep Graph Similarity Learning: A Survey").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提供了必要的符号和图相似性问题的基本概念定义，这些定义将在本调查中使用。符号总结见表格[1](#S2.T1 "Table 1 ‣ 2 Notation
    and Preliminaries ‣ Deep Graph Similarity Learning: A Survey")。'
- en: 'Table 1: Summary of Notation'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 1: 符号总结'
- en: '| $G$ |  |  |  | Input graph |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| $G$ |  |  |  | 输入图 |'
- en: '| $V$ |  |  |  | The set of nodes in a graph $G$ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| $V$ |  |  |  | 图$G$中的节点集 |'
- en: '| $E$ |  |  |  | The set of edges in a graph $G$ |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| $E$ |  |  |  | 图$G$中的边集 |'
- en: '| $a,\mathbf{a},\mathbf{A}$ |  |  |  | Scalar, vector, matrix |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| $a,\mathbf{a},\mathbf{A}$ |  |  |  | 标量，向量，矩阵 |'
- en: '| $\mathcal{G}$ |  |  |  | Graph set $\mathcal{G}=\{G_{1},G_{2},\cdots,G_{n}\}$
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{G}$ |  |  |  | 图集$\mathcal{G}=\{G_{1},G_{2},\cdots,G_{n}\}$ |'
- en: '| $\mathcal{M}$ |  |  |  | Similarity function |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}$ |  |  |  | 相似性函数 |'
- en: '| $s_{ij}$ |  |  |  | Similarity score between two graphs $G_{i},G_{j}\in\mathcal{G}$
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| $s_{ij}$ |  |  |  | 两个图$G_{i},G_{j}\in\mathcal{G}$之间的相似性评分 |'
- en: '| $\mathbb{R}^{m\times m}$ |  |  |  | $m-$dimensional Euclidean space |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbb{R}^{m\times m}$ |  |  |  | $m$维欧几里得空间 |'
- en: '| $\mathbf{I}_{m}$ |  |  |  | Identity matrix of dimension $m$ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{I}_{m}$ |  |  |  | $m$维单位矩阵 |'
- en: '| $\mathbf{A}^{T}$ |  |  |  | Matrix transpose |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{A}^{T}$ |  |  |  | 矩阵转置 |'
- en: '| $\mathbf{L}$ |  |  |  | Laplacian matrix |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{L}$ |  |  |  | 拉普拉斯矩阵 |'
- en: '| $g_{\theta}*\mathbf{x}$ |  |  |  | Convolution of $g_{\theta}$ and $\mathbf{x}$
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| $g_{\theta}*\mathbf{x}$ |  |  |  | $g_{\theta}$和$\mathbf{x}$的卷积 |'
- en: Let $G=(V,E,\mathbf{A})$ denote a graph, where $V$ is the set of nodes, $E\subseteq
    V\times V$ is the set of edges, and $\mathbf{A}\in\mathbb{R}^{|V|\times|V|}$ is
    the adjacency matrix of the graph. This is a general notation for graphs that
    covers different types of graphs, including unweighted/weighted graphs, undirected/directed
    graphs, and attributed/non-attributed graphs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 设$G=(V,E,\mathbf{A})$表示一个图，其中$V$是节点集，$E\subseteq V\times V$是边集，$\mathbf{A}\in\mathbb{R}^{|V|\times|V|}$是图的邻接矩阵。这是一种图的通用表示法，涵盖了不同类型的图，包括无权图/有权图、无向图/有向图，以及有属性图/无属性图。
- en: We are also assuming a set of graphs as input, $\mathcal{G}=\{G_{1},G_{2},\dots,G_{n}\}$,
    and the goal is measure/model their pairwise similarity. This relates to the classical
    problem of graph isomorphism and its variants. In graph isomorphism [miller1979graph](#bib.bib87)
    , two graphs $G=(V_{G},E_{G})$ and $H=(V_{H},E_{H})$ are isomorphic (i.e., $G\cong
    H$), if there is a mapping function $\pi\mathrel{\mathop{\mathchar 58\relax}}V_{G}\rightarrow
    V_{H}$, such that $(u,v)\in E_{G}$ iff $(\pi(u),\pi(v))\in E_{H}$. The graph isomorphism
    is an NP problem, and no efficient algorithms are known for it. Subgraph isomorphism
    is a generalization of the graph isomorphism problem. In subgraph isomorphism,
    the goal is to answer for two input graphs $G$ and $H$, if there is a subgraph
    of $G$ ($G^{\prime}\subset G$) such that $G^{\prime}$ is isomorphic to $H$ (i.e.,
    $G^{\prime}\cong H$). This is suitable in a setting in which the two graphs have
    different sizes. The subgraph isomorphism problem has been proven to be NP-complete
    (unlike the graph isomorphism problem) [Garey1978ComputersAI](#bib.bib47) . The
    maximum common subgraph problem is another less-restrictive measure of graph similarity,
    in which the similarity between two graphs is defined based on the size of the
    largest common subgraph in the two input graphs. However, this problem is also
    NP-complete [Garey1978ComputersAI](#bib.bib47) .
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设输入为一组图$\mathcal{G}=\{G_{1},G_{2},\dots,G_{n}\}$，目标是测量/建模它们之间的成对相似性。这涉及到图同构及其变体的经典问题。在图同构问题中，[miller1979graph](#bib.bib87)两个图$G=(V_{G},E_{G})$和$H=(V_{H},E_{H})$是同构的（即，$G\cong
    H$），如果存在一个映射函数$\pi\mathrel{\mathop{\mathchar 58\relax}}V_{G}\rightarrow V_{H}$，使得$(u,v)\in
    E_{G}$当且仅当$(\pi(u),\pi(v))\in E_{H}$。图同构是一个NP问题，目前尚无有效算法。子图同构是图同构问题的推广。在子图同构问题中，目标是对于两个输入图$G$和$H$，判断是否存在$G$的一个子图($G^{\prime}\subset
    G$)使得$G^{\prime}$与$H$同构（即，$G^{\prime}\cong H$）。这适用于两个图具有不同尺寸的情况。子图同构问题已被证明是NP完全的（与图同构问题不同）[Garey1978ComputersAI](#bib.bib47)。最大公共子图问题是衡量图相似性的另一种不太严格的度量，其中两个图之间的相似性是基于两个输入图中最大公共子图的大小定义的。然而，这个问题也是NP完全的[Garey1978ComputersAI](#bib.bib47)。
- en: Definition 1 (Graph Similarity Learning)
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义1（图相似性学习）
- en: 'Let $\mathcal{G}$ be an input set of graphs, $\mathcal{G}=\{G_{1},G_{2},\cdots,G_{n}\}$
    where $G_{i}=(V_{i},E_{i},\mathbf{A}_{i})$. Let $\mathcal{M}$ denote a learnable
    similarity function, such that $\mathcal{M}\mathrel{\mathop{\mathchar 58\relax}}(G_{i},G_{j})\rightarrow\mathbb{R}$,
    for any pair of graphs $G_{i},G_{j}\in\mathcal{G}$. Assume $s_{ij}\in\mathbb{R}$
    denote the similarity score computed using $\mathcal{M}$ between pairs $G_{i}$
    and $G_{j}$. Then $\mathcal{M}$ is symmetric if and only if $s_{ij}=s_{ji}$ for
    any pair of graphs $G_{i},G_{j}\in\mathcal{G}$. $\mathcal{M}$ should satisfy the
    property that: $s_{ii}>=s_{ij}$ for any pair of graphs $G_{i},G_{j}\in\mathcal{G}$.
    And, $s_{ij}$ is minimum if $G_{i}$ is the complement of $G_{j}$, i.e, $G_{i}=\bar{G_{j}}$,
    for any graph $G_{j}\in\mathcal{G}$.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\mathcal{G}$为输入图集，$\mathcal{G}=\{G_{1},G_{2},\cdots,G_{n}\}$，其中$G_{i}=(V_{i},E_{i},\mathbf{A}_{i})$。设$\mathcal{M}$表示一个可学习的相似性函数，使得$\mathcal{M}\mathrel{\mathop{\mathchar
    58\relax}}(G_{i},G_{j})\rightarrow\mathbb{R}$，对于任意一对图$G_{i},G_{j}\in\mathcal{G}$。假设$s_{ij}\in\mathbb{R}$表示使用$\mathcal{M}$计算的图对$G_{i}$和$G_{j}$之间的相似性分数。那么，$\mathcal{M}$是对称的，当且仅当$s_{ij}=s_{ji}$对于任意一对图$G_{i},G_{j}\in\mathcal{G}$。$\mathcal{M}$应该满足以下属性：$s_{ii}>=s_{ij}$对于任意一对图$G_{i},G_{j}\in\mathcal{G}$。并且，当$G_{i}$是$G_{j}$的补图，即$G_{i}=\bar{G_{j}}$时，$s_{ij}$达到最小，对于任意图$G_{j}\in\mathcal{G}$。
- en: 'Clearly, graph isomorphism and its related variants (e.g., subgraph isomorphism,
    maximum common subgraphs, etc.) are focused on measuring the topological equivalence
    of graphs, which gives rise to a binary similarity measure that outputs $1$ if
    two graphs are isomorphic and $0$ otherwise. While these methods may sound intuitive,
    they are actually more restrictive and difficult to compute for large graphs.
    Here instead, we focus on a relaxed notion of graph similarity that can be measured
    using machine learning models, where the goal is to learn a model that quantifies
    the degree of structural similarity and relatedness between two graphs. This is
    slightly similar to the work done on modeling the structural similarity between
    nodes in the same graph [ahmedrole2020](#bib.bib9) ; [rossi2014role](#bib.bib98)
    ; [ahmed2018learning](#bib.bib10) . We formally state the definition of graph
    similarity learning (GSL) in Definition [1](#Thmmydef1 "Definition 1 (Graph Similarity
    Learning) ‣ 2 Notation and Preliminaries ‣ Deep Graph Similarity Learning: A Survey").
    Note that in the case of deep graph similarity learning, the similarity function
    $\mathcal{M}$ is a neural network model that can be trained in an end-to-end fashion.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，图同构及其相关变体（例如，子图同构、最大公共子图等）侧重于测量图的拓扑等价性，这导致了一种二进制相似性度量，如果两个图是同构的则输出$1$，否则输出$0$。尽管这些方法听起来很直观，但对于大图来说，它们实际上更具限制性且计算难度较大。这里，我们关注的是一种放宽的图相似性概念，该概念可以通过机器学习模型进行测量，目标是学习一个模型来量化两个图之间的结构相似性和相关性。这与对同一图中节点之间结构相似性的建模工作略有相似 [ahmedrole2020](#bib.bib9)
    ; [rossi2014role](#bib.bib98) ; [ahmed2018learning](#bib.bib10) 。我们在定义 [1](#Thmmydef1
    "定义 1 (图相似性学习) ‣ 2 符号和预备知识 ‣ 深度图相似性学习：综述")中正式说明了图相似性学习（GSL）的定义。请注意，在深度图相似性学习的情况下，相似性函数$\mathcal{M}$是一个可以进行端到端训练的神经网络模型。
- en: 3 Taxonomy of Models
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 模型分类
- en: '![Refer to caption](img/5419d39f6508d8370340273c8cfb91e2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5419d39f6508d8370340273c8cfb91e2.png)'
- en: (a)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/a210df13ffbacf67e730fc349912dc14.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a210df13ffbacf67e730fc349912dc14.png)'
- en: (b)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 1: Proposed taxonomy for categorizing the literature of deep graph similarity
    learning based on (a) Model architecture, (b) Type of features.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '图1: 基于(a) 模型架构，(b) 特征类型的深度图相似性学习文献分类建议。'
- en: 'Table 2: A Taxonomy of Deep Graph Similarity Learning Methods'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '表2: 深度图相似性学习方法的分类'
- en: '| Category |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 分类 |'
- en: '&#124; Methods &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法 &#124;'
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  Weighted graphs  &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  加权图  &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  Heterogeneous graphs  &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  异质图  &#124;'
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  Attributed graphs  &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  属性图  &#124;'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  Feature propagation  &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  特征传播  &#124;'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  Cross-graph interaction  &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  跨图交互  &#124;'
- en: '| Applications |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 应用 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Graph Embedding based GSL | Node-level Embedding | [tixier2019graph](#bib.bib111)
    | ✗ | ✗ | ✓ | ✗ | ✗ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 基于图嵌入的GSL | 节点级嵌入 | [tixier2019graph](#bib.bib111) | ✗ | ✗ | ✓ | ✗ | ✗ |'
- en: '&#124; Social Network Analysis &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 社交网络分析 &#124;'
- en: '&#124; Bioinformatics &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生物信息学 &#124;'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [nikolentzos2017matching](#bib.bib92) | ✗ | ✓ | ✗ | ✗ | ✗ | Chemoinformatics
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| [nikolentzos2017matching](#bib.bib92) | ✗ | ✓ | ✗ | ✗ | ✗ | 化学信息学 |'
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Graph-level Embedding &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图级嵌入 &#124;'
- en: '| [narayanan2017graph2vec](#bib.bib88) ; [atamna2019spi](#bib.bib13) ; [wu2018dgcnn](#bib.bib125)
    | ✗ | ✓ | ✗ | ✗ | ✗ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| [narayanan2017graph2vec](#bib.bib88) ; [atamna2019spi](#bib.bib13) ; [wu2018dgcnn](#bib.bib125)
    | ✗ | ✓ | ✗ | ✗ | ✗ |'
- en: '&#124; Chemoinformatics &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 化学信息学 &#124;'
- en: '&#124; Bioinformatics &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生物信息学 &#124;'
- en: '|'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | [anonymous2019_inductive](#bib.bib120) | ✗ | ✓ | ✗ | ✗ | ✗ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | [anonymous2019_inductive](#bib.bib120) | ✗ | ✓ | ✗ | ✗ | ✗ |'
- en: '&#124; Social Network Analysis &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 社会网络分析 &#124;'
- en: '&#124; Chemoinformatics &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 化学信息学 &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  | [xu2017neural](#bib.bib128) | ✗ | ✗ | ✓ | ✗ | ✗ | Binary Code Similarity
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | [xu2017neural](#bib.bib128) | ✗ | ✗ | ✓ | ✗ | ✗ | 二进制代码相似性 |'
- en: '|  |  | [liu2019n](#bib.bib77) | ✗ | ✗ | ✓ | ✗ | ✗ | Chemoinformatics |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [liu2019n](#bib.bib77) | ✗ | ✗ | ✓ | ✗ | ✗ | 化学信息学 |'
- en: '| GNN-based GSL | GNN-CNN Models | [bai2018convolutional](#bib.bib16) | ✗ |
    ✓ | ✓ | ✓ | ✗ | Chemoinformatics |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 基于 GNN 的 GSL | GNN-CNN 模型 | [bai2018convolutional](#bib.bib16) | ✗ | ✓ |
    ✓ | ✓ | ✗ | 化学信息学 |'
- en: '| [bai2019simgnn](#bib.bib15) | ✗ | ✓ | ✓ | ✓ | ✗ | Chemoinformatics |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [bai2019simgnn](#bib.bib15) | ✗ | ✓ | ✓ | ✓ | ✗ | 化学信息学 |'
- en: '| Siamese GNNs | [ktena2018metric](#bib.bib67) ; [ma2019similarity](#bib.bib78)
    ; [liu2019community](#bib.bib76) | ✓ | ✗ | ✓ | ✓ | ✗ | Brain Network Analysis
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Siamese GNNs | [ktena2018metric](#bib.bib67) ; [ma2019similarity](#bib.bib78)
    ; [liu2019community](#bib.bib76) | ✓ | ✗ | ✓ | ✓ | ✗ | 大脑网络分析 |'
- en: '| [ijcai2019-522](#bib.bib122) | ✗ | ✓ | ✓ | ✓ | ✗ | Malware Detection |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [ijcai2019-522](#bib.bib122) | ✗ | ✓ | ✓ | ✓ | ✗ | 恶意软件检测 |'
- en: '|  | [chaudhuri2019siamese](#bib.bib29) | ✓ | ✗ | ✓ | ✓ | ✗ | Image Retrieval
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | [chaudhuri2019siamese](#bib.bib29) | ✓ | ✗ | ✓ | ✓ | ✗ | 图像检索 |'
- en: '| GNN-based Graph Matching Networks | [li2019graph](#bib.bib73) ; [anonymous2019](#bib.bib75)
    | ✗ | ✗ | ✓ | ✓ | ✓ | Binary Code Similarity |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 基于 GNN 的图匹配网络 | [li2019graph](#bib.bib73) ; [anonymous2019](#bib.bib75) |
    ✗ | ✗ | ✓ | ✓ | ✓ | 二进制代码相似性 |'
- en: '| [anonymous2019_mcs](#bib.bib17) | ✗ | ✓ | ✓ | ✓ | ✓ | Chemoinformatics |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| [anonymous2019_mcs](#bib.bib17) | ✗ | ✓ | ✓ | ✓ | ✓ | 化学信息学 |'
- en: '|  |  | [wang2019learning](#bib.bib121) ; [jiang2019glmnet](#bib.bib60) | ✗
    | ✗ | ✓ | ✓ | ✓ | Image Matching |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [wang2019learning](#bib.bib121) ; [jiang2019glmnet](#bib.bib60) | ✗
    | ✗ | ✓ | ✓ | ✓ | 图像匹配 |'
- en: '|  |  | [guo2018neural](#bib.bib53) | ✓ | ✓ | ✓ | ✓ | ✗ | 3D Action Recognition
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [guo2018neural](#bib.bib53) | ✓ | ✓ | ✓ | ✓ | ✗ | 3D 动作识别 |'
- en: '| Deep Graph Kernel based GSL |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 深度图核基于 GSL |'
- en: '&#124; Sub-structure based &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于子结构的方法 &#124;'
- en: '&#124; Deep Kernels &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度核方法 &#124;'
- en: '| [yanardag2015deep](#bib.bib130) | ✗ | ✓ | ✗ | ✗ | ✗ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| [yanardag2015deep](#bib.bib130) | ✗ | ✓ | ✗ | ✗ | ✗ |'
- en: '&#124; Chemoinformatics &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 化学信息学 &#124;'
- en: '&#124; Bioinformatics &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生物信息学 &#124;'
- en: '&#124; Social Network Analysis &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 社会网络分析 &#124;'
- en: '|'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Deep Neural Network &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度神经网络 &#124;'
- en: '&#124; based Kernels &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于核的方法 &#124;'
- en: '| [al2019ddgk](#bib.bib11) | ✗ | ✓ | ✓ | ✗ | ✓ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| [al2019ddgk](#bib.bib11) | ✗ | ✓ | ✓ | ✗ | ✓ |'
- en: '&#124; Chemoinformatics &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 化学信息学 &#124;'
- en: '&#124; Bioinformatics &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生物信息学 &#124;'
- en: '|'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  |  | [du2019graph](#bib.bib39) | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [du2019graph](#bib.bib39) | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '&#124; Social Network Analysis &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 社会网络分析 &#124;'
- en: '&#124; Chemoinformatics &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 化学信息学 &#124;'
- en: '|'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'In this section, we describe the taxonomy for the literature of deep graph
    similarity learning. As shown in Fig.  [1](#S3.F1 "Figure 1 ‣ 3 Taxonomy of Models
    ‣ Deep Graph Similarity Learning: A Survey"), we propose two intuitive taxonomies
    for categorizing the various deep graph similarity learning methods based on the
    model architecture and the type of features used in these methods.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了深度图相似性学习文献的分类法。如图 [1](#S3.F1 "图 1 ‣ 3 模型分类 ‣ 深度图相似性学习：综述") 所示，我们提出了两种直观的分类法，用于根据模型架构和这些方法中使用的特征类型对各种深度图相似性学习方法进行分类。
- en: 'First, we start by discussing the categorization based on which model architecture
    has been used. There are three main categories of deep graph similarity learning
    methods (see Fig. [1(a)](#S3.F1.sf1 "In Figure 1 ‣ 3 Taxonomy of Models ‣ Deep
    Graph Similarity Learning: A Survey")): (1) graph embedding based methods, which
    apply graph embedding techniques to obtain node-level or graph-level representations
    and further use the representations for similarity learning [tixier2019graph](#bib.bib111)
    ; [nikolentzos2017matching](#bib.bib92) ; [narayanan2017graph2vec](#bib.bib88)
    ; [atamna2019spi](#bib.bib13) ; [wu2018dgcnn](#bib.bib125) ; [anonymous2019_inductive](#bib.bib120)
    ; [xu2017neural](#bib.bib128) ; [liu2019n](#bib.bib77) ; (2) graph neural network
    (GNN) based models, which are based on using GNNs for similarity learning, including
    GNN-CNNs [bai2018convolutional](#bib.bib16) ; [bai2019simgnn](#bib.bib15) , Siamese
    GNNs [ktena2018metric](#bib.bib67) ; [ma2019similarity](#bib.bib78) ; [liu2019community](#bib.bib76)
    ; [ijcai2019-522](#bib.bib122) ; [chaudhuri2019siamese](#bib.bib29) and GNN-based
    graph matching networks [li2019graph](#bib.bib73) ; [anonymous2019](#bib.bib75)
    ; [anonymous2019_mcs](#bib.bib17) ; [wang2019learning](#bib.bib121) ; [jiang2019glmnet](#bib.bib60)
    ; [guo2018neural](#bib.bib53) ; and (3) deep graph kernels that first map graphs
    into a new feature space, where kernel functions are defined for similarity learning
    on graph pairs, including sub-structure based deep kernels [yanardag2015deep](#bib.bib130)
    and deep neural network based kernels [al2019ddgk](#bib.bib11) ; [du2019graph](#bib.bib39)
    . In the meantime, different methods may use different types of features in the
    learning process.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，我们从讨论基于使用哪种模型架构的分类开始。深度图相似性学习方法主要分为三类（见图[1(a)](#S3.F1.sf1 "In Figure 1 ‣
    3 Taxonomy of Models ‣ Deep Graph Similarity Learning: A Survey")）：（1）基于图嵌入的方法，这些方法应用图嵌入技术来获得节点级或图级表示，并进一步利用这些表示进行相似性学习
    [tixier2019graph](#bib.bib111)； [nikolentzos2017matching](#bib.bib92)； [narayanan2017graph2vec](#bib.bib88)；
    [atamna2019spi](#bib.bib13)； [wu2018dgcnn](#bib.bib125)； [anonymous2019_inductive](#bib.bib120)；
    [xu2017neural](#bib.bib128)； [liu2019n](#bib.bib77)；（2）基于图神经网络（GNN）的模型，这些模型基于使用GNN进行相似性学习，包括GNN-CNNs
    [bai2018convolutional](#bib.bib16)； [bai2019simgnn](#bib.bib15)，Siamese GNNs [ktena2018metric](#bib.bib67)；
    [ma2019similarity](#bib.bib78)； [liu2019community](#bib.bib76)； [ijcai2019-522](#bib.bib122)；
    [chaudhuri2019siamese](#bib.bib29)和基于GNN的图匹配网络 [li2019graph](#bib.bib73)； [anonymous2019](#bib.bib75)；
    [anonymous2019_mcs](#bib.bib17)； [wang2019learning](#bib.bib121)； [jiang2019glmnet](#bib.bib60)；
    [guo2018neural](#bib.bib53)；（3）深度图核，这些方法首先将图映射到一个新的特征空间，其中定义了用于图对之间相似性学习的核函数，包括基于子结构的深度核
    [yanardag2015deep](#bib.bib130) 和基于深度神经网络的核 [al2019ddgk](#bib.bib11)； [du2019graph](#bib.bib39)。同时，不同的方法在学习过程中可能使用不同类型的特征。'
- en: 'Second, we discuss the categorization of methods based on the type of features
    used in them. Existing GSL approaches can be generally grouped into two categories
    (see Fig. [1(b)](#S3.F1.sf2 "In Figure 1 ‣ 3 Taxonomy of Models ‣ Deep Graph Similarity
    Learning: A Survey")): (1) methods that uses single-graph features  [ktena2018metric](#bib.bib67)
    ; [ma2019similarity](#bib.bib78) ; [liu2019community](#bib.bib76) ; [ijcai2019-522](#bib.bib122)
    ; [chaudhuri2019siamese](#bib.bib29) ; (2) methods that uses cross-graph features
    for similarity learning [li2019graph](#bib.bib73) ; [anonymous2019](#bib.bib75)
    ; [anonymous2019_mcs](#bib.bib17) ; [al2019ddgk](#bib.bib11) ; [wang2019learning](#bib.bib121)
    ; [anonymous2019_mcs](#bib.bib17) . The main difference between these two categories
    of methods is that for methods using single-graph features, the representation
    of each graph is learned individually, while those methods that use cross-graph
    features allow graphs to learn and propagate features from each other and the
    cross-graph interaction is leveraged for pairs of graphs. The single-graph features
    mainly includes graph embeddings at different granularity (i.e.,node-level, graph-level,
    and subgraph-level), while the cross-graph features includes the cross-graph node-level
    features and cross-graph graph-level features, which are usually obtained by node-level
    attention and graph-level attention across the two graphs in each pair.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们讨论了基于所使用特征类型的方法的分类。现有的 GSL 方法大致可以分为两类（见图 [1(b)](#S3.F1.sf2 "在图 1 ‣ 3 模型分类
    ‣ 深度图相似性学习：综述")）：（1）使用单图特征的方法 [ktena2018metric](#bib.bib67) ; [ma2019similarity](#bib.bib78)
    ; [liu2019community](#bib.bib76) ; [ijcai2019-522](#bib.bib122) ; [chaudhuri2019siamese](#bib.bib29)
    ; （2）使用跨图特征进行相似性学习的方法 [li2019graph](#bib.bib73) ; [anonymous2019](#bib.bib75)
    ; [anonymous2019_mcs](#bib.bib17) ; [al2019ddgk](#bib.bib11) ; [wang2019learning](#bib.bib121)
    ; [anonymous2019_mcs](#bib.bib17) 。这两类方法的主要区别在于，对于使用单图特征的方法，每个图的表示是单独学习的，而那些使用跨图特征的方法允许图之间相互学习和传播特征，并且跨图交互被利用于图对。单图特征主要包括不同粒度的图嵌入（即节点级别、图级别和子图级别），而跨图特征包括跨图节点级别特征和跨图图级别特征，这些特征通常通过对每对图进行节点级别注意力和图级别注意力来获得。
- en: 'Next, we detail the description of the methods based on the taxonomy in Figures [1(a)](#S3.F1.sf1
    "In Figure 1 ‣ 3 Taxonomy of Models ‣ Deep Graph Similarity Learning: A Survey")
    and [1(b)](#S3.F1.sf2 "In Figure 1 ‣ 3 Taxonomy of Models ‣ Deep Graph Similarity
    Learning: A Survey"). We summarize the general characteristics and applications
    of all the methods in Table [2](#S3.T2 "Table 2 ‣ 3 Taxonomy of Models ‣ Deep
    Graph Similarity Learning: A Survey"), including the type of graphs they are developed
    for, the type of features, and the domains/applications where they could be applied.
    We describe these methods in the following order:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们详细描述了基于图 [1(a)](#S3.F1.sf1 "在图 1 ‣ 3 模型分类 ‣ 深度图相似性学习：综述") 和 [1(b)](#S3.F1.sf2
    "在图 1 ‣ 3 模型分类 ‣ 深度图相似性学习：综述") 中分类法的的方法。我们在表 [2](#S3.T2 "表 2 ‣ 3 模型分类 ‣ 深度图相似性学习：综述")
    中总结了所有方法的一般特征和应用，包括它们所开发的图类型、特征类型以及可以应用的领域/应用。我们按照以下顺序描述这些方法：
- en: '1.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Graph embedding based GSL
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于图嵌入的 GSL
- en: '2.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Graph Neural Network based GSL
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于图神经网络的 GSL
- en: '3.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Deep graph kernel based GSL
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于深度图核的 GSL
- en: 3.1 Graph Embedding based Graph Similarity Learning
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于图嵌入的图相似性学习
- en: Graph embedding has received considerable attention in the past decade [cui2018survey](#bib.bib31)
    ; [zhang2018network](#bib.bib136) , and a variety of deep graph embedding models
    have been proposed in recent years [huang2019learning](#bib.bib58) ; [narayanan2017graph2vec](#bib.bib88)
    ; [gao2019graph1](#bib.bib45) , for example the popular DeepWalk model proposed
    in [perozzi2014deepwalk](#bib.bib94) and the node2vec model from [grover2016node2vec](#bib.bib51)
    . Similarity learning methods based on graph embedding seek to utilize node-level
    or graph-level representations learned by these graph embedding techniques for
    defining similarity functions or predicting similarity scores [tsitsulin2018verse](#bib.bib114)
    ; [tixier2019graph](#bib.bib111) ; [narayanan2017graph2vec](#bib.bib88) . Given
    a collection of graphs, these works first aim to convert each graph $G$ into a
    $d-$dimensional space $(d\ll\|V\|)$, where the graph is represented as either
    a set of $d-$dimensional vectors with each vector representing the embedding of
    one node (i.e.,node-level embedding) or a $d-$dimensional vector for the whole
    graph as the graph-level embedding [cai2018comprehensive](#bib.bib28) . The graph
    embeddings are usually learned in an unsupervised manner in a separate stage prior
    to the similarity learning stage, where the graph embeddings obtained are used
    for estimating or predicting the similarity score between each pair of graphs.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图嵌入在过去十年中受到了相当多的关注 [cui2018survey](#bib.bib31) ; [zhang2018network](#bib.bib136)
    ，近年来提出了多种深度图嵌入模型 [huang2019learning](#bib.bib58) ; [narayanan2017graph2vec](#bib.bib88)
    ; [gao2019graph1](#bib.bib45) ，例如在 [perozzi2014deepwalk](#bib.bib94) 中提出的流行的DeepWalk模型和
    [grover2016node2vec](#bib.bib51) 中的node2vec模型。基于图嵌入的相似性学习方法旨在利用这些图嵌入技术学习的节点级或图级表示来定义相似性函数或预测相似性分数
    [tsitsulin2018verse](#bib.bib114) ; [tixier2019graph](#bib.bib111) ; [narayanan2017graph2vec](#bib.bib88)
    。给定一组图，这些工作首先旨在将每个图 $G$ 转换为一个 $d-$维空间 $(d\ll\|V\|)$，其中图被表示为一组 $d-$维向量，每个向量表示一个节点的嵌入（即节点级嵌入）或整个图的
    $d-$维向量作为图级嵌入 [cai2018comprehensive](#bib.bib28) 。图嵌入通常在相似性学习阶段之前的单独阶段以无监督的方式学习，在此阶段获得的图嵌入用于估计或预测每对图之间的相似性分数。
- en: 3.1.1 Node-level Embedding based Methods
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 基于节点级嵌入的方法
- en: Node-level embedding based methods compare graphs using the node-level representations
    learned from the graphs. The similarity scores obtained by these methods mainly
    capture the similarity between the corresponding nodes in two graphs. Therefore
    they focus on the local node-level information on graphs during the learning process.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基于节点级嵌入的方法使用从图中学习的节点级表示来比较图。这些方法获得的相似性分数主要捕捉两个图中对应节点之间的相似性。因此，它们在学习过程中专注于图中的局部节点级信息。
- en: node2vec-PCA.
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: node2vec-PCA。
- en: In [tixier2019graph](#bib.bib111) , the node2vec approach [grover2016node2vec](#bib.bib51)
    is employed for obtaining the node-level embeddings of graphs. To make the embeddings
    of all the graphs in the given collection comparable, they apply the principal
    component analysis (PCA) on the embeddings to retain the first $d\ll D$ principal
    components (where $D$ is the dimensionality of the original node embedding space).
    Afterwards, the embedding matrix of each graph is split into $d/2$ 2D slices.
    Suppose there are $n$ nodes in each graph $G$ and the embedding matrix for graph
    $G$ is $F\in\mathbb{R}^{n\times d}$, then $d/2$ 2D slices each with $\mathbb{R}^{n\times
    2}$ will be obtained, which are viewed as $d/2$ channels. Then each 2D slice from
    the embedding space is turned into regular grids by discretizing them into a fixed
    number of equallly-sized bins, where the value associate with each bin is the
    count of the number of nodes falling into that bin. These bins can be viewed as
    pixels. Then, the graph is represented as a stack of 2D histograms of its node
    embeddings. The graphs are then compared in the grid space and input into a 2D
    CNN as multi-channel image-like structures for a graph classification task.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [tixier2019graph](#bib.bib111) 中，使用了 node2vec 方法 [grover2016node2vec](#bib.bib51)
    来获取图的节点级嵌入。为了使给定集合中所有图的嵌入可比，他们对嵌入应用了主成分分析（PCA），以保留前 $d\ll D$ 主成分（其中 $D$ 是原始节点嵌入空间的维度）。随后，每个图的嵌入矩阵被分割成
    $d/2$ 个 2D 切片。假设每个图 $G$ 中有 $n$ 个节点，并且图 $G$ 的嵌入矩阵为 $F\in\mathbb{R}^{n\times d}$，那么将获得
    $d/2$ 个 $\mathbb{R}^{n\times 2}$ 的 2D 切片，这些切片被视为 $d/2$ 个通道。然后，通过将每个 2D 切片离散化为固定数量的相等大小的箱子，将其转换为规则网格，其中每个箱子关联的值是落入该箱子的节点数量。这些箱子可以视为像素。然后，图被表示为其节点嵌入的
    2D 直方图堆叠。然后在网格空间中比较图，并将其输入到作为多通道图像结构的 2D CNN 中，用于图分类任务。
- en: Bag-of-Vectors.
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 向量袋。
- en: In [nikolentzos2017matching](#bib.bib92) , the nodes of the graphs are first
    embedded in the Euclidean space using the eigenvectors of the adjacency matrices
    of the graphs, and each graph is then represented as a bag-of-vectors. The similarity
    between two graphs is then measured by computing a matching based on the Earth
    Mover’s Distance [rubner2000earth](#bib.bib102) between the two sets of embeddings.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [nikolentzos2017matching](#bib.bib92) 中，首先使用图的邻接矩阵的特征向量将图的节点嵌入到欧几里得空间中，然后每个图被表示为一个向量袋。两个图之间的相似性通过计算基于地球移动者距离
    [rubner2000earth](#bib.bib102) 的匹配来衡量。
- en: Although node embedding based graph similarity learning methods have been extensively
    developed, a common problem with these methods is that, since the comparison is
    based on node-level representations, the global structure of the graphs tends
    to be ignored, which actually is very important for comparing two graphs in terms
    of their structural patterns.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于节点嵌入的图相似性学习方法已经得到了广泛的发展，但这些方法的一个共同问题是，由于比较是基于节点级别的表示，图的全球结构往往被忽视，而这实际上对于比较两个图的结构模式非常重要。
- en: 3.1.2 Graph-level Embedding based Methods
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 图级嵌入基于的方法
- en: The graph-level embedding based methods aim to learn a vector representation
    for each graph and then learn the similarity score between graphs based on their
    vector representations.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图级别嵌入的方法旨在为每个图学习一个向量表示，然后根据这些向量表示学习图之间的相似性分数。
- en: (1) graph2vec.
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (1) graph2vec。
- en: 'In [narayanan2017graph2vec](#bib.bib88) , a graph2vec was proposed to learn
    distributed representations of graphs, similar to Doc2vec [le2014distributed](#bib.bib68)
    in natural language processing. In graph2vec each graph is viewed as a document
    and the rooted subgraphs around every node in the graph are viewed as words that
    compose the document. There are two main components in this method: first, a procedure
    to extract rooted subgraphs around every node in a given graph following the Weisfeiler-Lehman
    relabeling process and second, the procedure to learn embeddings of the given
    graphs by skip-gram with negative sampling. The Weisfeiler-Lehman relabeling algorithm
    takes the root node of the given graph and degree of the intended subgraph $d$
    as inputs, and returns the intended subgraph. In the negative sampling phase,
    given a graph and a set of rooted subgraphs in its context, a set of randomly
    chosen subgraphs are selected as negative samples and only the embeddings of the
    negative samples are updated in the training. After the graph embedding is obtained
    for each graph, the similarity or distance between graphs are computed in the
    embedding space for downstream prediction tasks (e.g., graph classification, clustering,
    etc.).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [narayanan2017graph2vec](#bib.bib88) 中，提出了一种graph2vec方法，用于学习图的分布表示，类似于自然语言处理中的Doc2vec
    [le2014distributed](#bib.bib68)。在graph2vec中，每个图被视为一个文档，而图中每个节点周围的根子图被视为组成文档的单词。该方法有两个主要组成部分：首先是提取给定图中每个节点周围的根子图，按照Weisfeiler-Lehman重新标记过程进行；其次是通过带有负采样的skip-gram学习给定图的嵌入。Weisfeiler-Lehman重新标记算法将给定图的根节点和目标子图的度
    $d$ 作为输入，返回目标子图。在负采样阶段，给定图和其上下文中的根子图集合，随机选择一组子图作为负样本，只有负样本的嵌入在训练中被更新。获得每个图的图嵌入后，在嵌入空间中计算图之间的相似性或距离，以进行下游预测任务（例如，图分类、聚类等）。
- en: (2) Neural Networks with Structure2vec.
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2) 使用Structure2vec的神经网络。
- en: In [xu2017neural](#bib.bib128) , a deep graph embedding approach is proposed
    for cross-platform binary code similarity detection. A Siamese architecture is
    applied to enable the pair-wise similarity learning, and the graph embedding network
    based on Structure2vec [dai2016discriminative](#bib.bib32) is used for learning
    graph representations in the twin networks, which share weights with each other.
    The Structure2vec is a neural network approach inspired by graphical model inference
    algorithms where node-specific features are aggregated recursively according to
    graph topology. After a few steps of recursion, the network will produce a new
    feature representation for each node which considers both graph characteristics
    and long-range interaction between node features. Given is a set of $K$ pairs
    of graphs $<G_{i},{G_{i}}^{\prime}>$, with ground truth pair label $y_{i}\in\{+1,-1\}$,
    where $y_{i}=+1$ indicates that $G_{i}$ and ${G_{i}}^{\prime}$ are similar, and
    $y_{i}=-1$ indicates they are dissimilar. With the Structure2vec embedding output
    for $G_{i}$ and ${G_{i}}^{\prime}$, represented as $\mathbf{f}_{i}$ and ${\mathbf{f}_{i}}^{\prime}$
    respectively, they define the Siamese network output for each pair as
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [xu2017neural](#bib.bib128) 中，提出了一种深度图嵌入方法用于跨平台二进制代码相似性检测。应用了Siamese架构以实现对偶相似性学习，并使用基于Structure2vec
    [dai2016discriminative](#bib.bib32) 的图嵌入网络来学习双网络中的图表示，这两个网络共享权重。Structure2vec
    是一种受图模型推理算法启发的神经网络方法，其中节点特定特征根据图拓扑递归地聚合。在几次递归步骤后，网络将为每个节点生成一个新的特征表示，考虑了图特征和节点特征之间的远程交互。给定一组
    $K$ 对图 $<G_{i},{G_{i}}^{\prime}>$，其真实对偶标签为 $y_{i}\in\{+1,-1\}$，其中 $y_{i}=+1$ 表示
    $G_{i}$ 和 ${G_{i}}^{\prime}$ 相似，$y_{i}=-1$ 表示它们不相似。使用Structure2vec嵌入输出 $G_{i}$
    和 ${G_{i}}^{\prime}$，分别表示为 $\mathbf{f}_{i}$ 和 ${\mathbf{f}_{i}}^{\prime}$，它们将Siamese网络对每对图的输出定义为
- en: '|  | $\displaystyle Sim(G_{i},{G_{i}}^{\prime})=\cos(\mathbf{f}_{i},{\mathbf{f}_{i}}^{\prime})=\frac{\langle\mathbf{f}_{i},{\mathbf{f}_{i}}^{\prime}\rangle}{\&#124;\mathbf{f}_{i}\&#124;\cdot\&#124;{\mathbf{f}_{i}}^{\prime}\&#124;}$
    |  | (1) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Sim(G_{i},{G_{i}}^{\prime})=\cos(\mathbf{f}_{i},{\mathbf{f}_{i}}^{\prime})=\frac{\langle\mathbf{f}_{i},{\mathbf{f}_{i}}^{\prime}\rangle}{\&#124;\mathbf{f}_{i}\&#124;\cdot\&#124;{\mathbf{f}_{i}}^{\prime}\&#124;}$
    |  | (1) |'
- en: and the following loss function is used for training the model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下损失函数用于训练模型。
- en: '|  | $\displaystyle L=\sum_{i=1}^{K}(Sim(G_{i},{G_{i}}^{\prime})-y_{i})^{2}$
    |  | (2) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L=\sum_{i=1}^{K}(Sim(G_{i},{G_{i}}^{\prime})-y_{i})^{2}$
    |  | (2) |'
- en: (3) Simple Permutation-Invariant GCN.
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3) 简单排列不变GCN。
- en: In [atamna2019spi](#bib.bib13) , a graph representation learning method based
    on a simple permutation-invariant graph convolutional network is proposed for
    the graph similarity and graph classification problem. A graph convolution module
    is used to encode local graph structure and node features, after which a sum-pooling
    layer is used to transform the substructure feature matrix computed by the graph
    convolutions into a single feature vector representation of the input graphs.
    The vector representation is then used as features for each graph, based on which
    the graph similarity or graph classification task can be performed.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [atamna2019spi](#bib.bib13) 中，提出了一种基于简单置换不变图卷积网络的图表示学习方法，用于图相似性和图分类问题。该方法使用图卷积模块对局部图结构和节点特征进行编码，然后使用求和池化层将通过图卷积计算的子结构特征矩阵转换为输入图的单一特征向量表示。然后将向量表示用作每个图的特征，基于此可以执行图相似性或图分类任务。
- en: '(4) SEED: Sampling, Encoding, and Embedding Distributions.'
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '(4) SEED: 采样、编码和嵌入分布。'
- en: 'In [anonymous2019_inductive](#bib.bib120) , an inductive and unsupervised graph
    representation learning approach called SEED is proposed for graph similarity
    learning. The proposed framework consists of three components: sampling, encoding,
    and embedding distribution. In the sampling stage, a number of subgraphs called
    WEAVE are sampled based on the random walk with earliest visit time. Then in the
    encoding stage, an autoencoder [hinton2006reducing](#bib.bib55) is used to encode
    the subgraphs into dense low-dimensional vectors. Given a set of k sampled WEAVEs
    $\{X_{1},X_{2},X_{3},\cdots,X_{k}\}$, for each subgraph $X_{i}$ the autoencoder
    works as follows.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [anonymous2019_inductive](#bib.bib120) 中，提出了一种称为 SEED 的归纳性和无监督图表示学习方法，用于图相似性学习。该框架由三个组件组成：采样、编码和嵌入分布。在采样阶段，根据最早访问时间的随机游走采样出若干个称为
    WEAVE 的子图。然后在编码阶段，使用自编码器 [hinton2006reducing](#bib.bib55) 将子图编码为密集的低维向量。给定一组 $k$
    个采样的 WEAVE $\{X_{1},X_{2},X_{3},\cdots,X_{k}\}$，对于每个子图 $X_{i}$ 自编码器的工作如下。
- en: '|  | $\displaystyle\mathbf{z}_{i}=f(X_{i};{\theta}_{e}),\quad\hat{X_{i}}=g(\mathbf{z}_{i};{\theta}_{d}),$
    |  | (3) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{z}_{i}=f(X_{i};{\theta}_{e}),\quad\hat{X_{i}}=g(\mathbf{z}_{i};{\theta}_{d}),$
    |  | (3) |'
- en: 'where $\mathbf{z}_{i}$ is the dense low-dimensional representation for the
    input WEAVE subgraph $X_{i}$, $f(\cdot)$ is the encoding function implemented
    with an Multi-layer Perceptron (MLP) with parameters ${\theta}_{e}$, and $g(\cdot)$
    is the decoding function implemented by another MLP with parameters ${\theta}_{d}$.
    A reconstruction loss is used to train the autoencoder:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{z}_{i}$ 是输入 WEAVE 子图 $X_{i}$ 的密集低维表示，$f(\cdot)$ 是用具有参数 ${\theta}_{e}$
    的多层感知机（MLP）实现的编码函数，$g(\cdot)$ 是用具有参数 ${\theta}_{d}$ 的另一个 MLP 实现的解码函数。重构损失用于训练自编码器：
- en: '|  | $\displaystyle L=\mathinner{\!\left\lVert X-\hat{X}\right\rVert}_{2}^{2}$
    |  | (4) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L=\mathinner{\!\left\lVert X-\hat{X}\right\rVert}_{2}^{2}$
    |  | (4) |'
- en: 'After the autoencoder is well trained, the final subgraph embedding vectors
    ${\mathbf{z}_{1},\mathbf{z}_{2},\mathbf{z}_{3},\cdots,}$ and $\mathbf{z}_{k}$
    can be obtained for each graph. Finally, in the embedding distribution stage,
    the distance between the subgraph distributions of two input graphs $G$ and $H$
    is evaluated using the maximum mean discrepancy (MMD) [gretton2012kernel](#bib.bib50)
    on the embeddings. Assume the $k$ subgraphs sampled from $G$ are encoded into
    embeddings ${\mathbf{z}_{1},\mathbf{z}_{2},\cdots,\mathbf{z}_{k}}$, and the $k$
    subgraphs of $H$ are encoded into embeddings ${\mathbf{h}_{1},\mathbf{h}_{2},\cdots,\mathbf{h}_{k}}$,
    the MMD distance between $G$ and $H$ is:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器经过充分训练后，可以为每个图获得最终的子图嵌入向量 ${\mathbf{z}_{1},\mathbf{z}_{2},\mathbf{z}_{3},\cdots,}$
    和 $\mathbf{z}_{k}$。最后，在嵌入分布阶段，通过在嵌入上使用最大均值差异（MMD）[gretton2012kernel](#bib.bib50)
    来评估两个输入图 $G$ 和 $H$ 的子图分布之间的距离。假设从 $G$ 中采样的 $k$ 个子图被编码为嵌入 ${\mathbf{z}_{1},\mathbf{z}_{2},\cdots,\mathbf{z}_{k}}$，而
    $H$ 的 $k$ 个子图被编码为嵌入 ${\mathbf{h}_{1},\mathbf{h}_{2},\cdots,\mathbf{h}_{k}}$，则
    $G$ 和 $H$ 之间的 MMD 距离为：
- en: '|  | $\displaystyle\widehat{\text{MMD}}(G,H)=\mathinner{\!\left\lVert\hat{\mu}_{G}-\hat{\mu}_{H}\right\rVert}_{2}^{2}$
    |  | (5) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\text{MMD}}(G,H)=\mathinner{\!\left\lVert\hat{\mu}_{G}-\hat{\mu}_{H}\right\rVert}_{2}^{2}$
    |  | (5) |'
- en: 'where $\hat{\mu}_{G}$ and $\hat{\mu}_{H}$ are empirical kernel embeddings of
    the two distributions, which are defined as:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{\mu}_{G}$ 和 $\hat{\mu}_{H}$ 是两个分布的经验核嵌入，它们定义为：
- en: '|  | $\displaystyle\hat{\mu}_{G}=\frac{1}{k}\sum_{i=1}^{k}\phi(\mathbf{z}_{i}),\quad\hat{\mu}_{H}=\frac{1}{k}\sum_{i=1}^{k}\phi(\mathbf{h}_{i})$
    |  | (6) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\mu}_{G}=\frac{1}{k}\sum_{i=1}^{k}\phi(\mathbf{z}_{i}),\quad\hat{\mu}_{H}=\frac{1}{k}\sum_{i=1}^{k}\phi(\mathbf{h}_{i})$
    |  | (6) |'
- en: where $\phi(\cdot)$ is the feature mapping function used for the kernel function
    for graph similarity evaluation. An identity kernel is applied in this work.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi(\cdot)$ 是用于图相似性评估的核函数的特征映射函数。在本工作中应用了单位核。
- en: '(5) DGCNN: Disordered Graph CNN.'
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '(5) DGCNN: 无序图 CNN。'
- en: In [wu2018dgcnn](#bib.bib125) , another graph-level representation learning
    approach called DGCNN is introduced based on graph CNN and mixed Gaussian model,
    where a set of key nodes are selected from each graph. Specifically, to ensure
    the number of neighborhoods of the nodes in each graph is consistent, the same
    number of key nodes are sampled for each graph in a key node selection stage.
    Then a convolution operation is performed over the kernel parameter matrix and
    the nodes in the neighborhood of the selected key nodes, after which the graph
    CNN takes the output of the convolutional layer as the input data of the overall
    connection layer. Finally, the output of the dense hidden layer is used as the
    feature vector for each graph in the graph similarity retrieval task.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [wu2018dgcnn](#bib.bib125) 中，介绍了一种名为 DGCNN 的图级表示学习方法，该方法基于图 CNN 和混合高斯模型，其中从每个图中选择一组关键节点。具体而言，为了确保每个图中节点的邻域数量一致，在关键节点选择阶段，为每个图采样相同数量的关键节点。然后，在所选关键节点的邻域节点上执行卷积操作，之后图
    CNN 将卷积层的输出作为整体连接层的输入数据。最后，密集隐藏层的输出用作图相似性检索任务中每个图的特征向量。
- en: (6) N-Gram Graph Embedding.
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (6) N-Gram 图嵌入。
- en: In [liu2019n](#bib.bib77) , an unsupervised graph representation based method
    called $N$-gram is proposed for similarity learning on molecule graphs. It first
    views each node in the graph as one token and applies an analog of the CBOW (continuous
    bag of words) [mikolov2013distributed](#bib.bib86) strategy and trains a neural
    network to learn the node embeddings for each graph. Then it enumerates the walks
    of length $n$ in each graph, where each walk is called an $n$-gram, and obtains
    the embedding for each $n$-gram by assembling the embeddings of the nodes in the
    $n$-gram using element-wise product. The embedding for the n-gram walk set is
    defined as the sum of the embeddings for all n-grams. The final n-gram graph-level
    representation up to lenght $T$ is then constructed by concatenating the embeddings
    of all the $n$-gram sets for $n\in\{1,2,\cdots,T\}$ in the graph. Finally, the
    graph-level embeddings are used for the similarity prediction or graph classification
    task for molecule analysis.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [liu2019n](#bib.bib77) 中，提出了一种基于无监督图表示的方法，称为 $N$-gram，用于分子图上的相似性学习。它首先将图中的每个节点视为一个标记，并应用类似
    CBOW（连续词袋） [mikolov2013distributed](#bib.bib86) 的策略，训练神经网络以学习每个图的节点嵌入。然后，它枚举每个图中长度为
    $n$ 的游走，其中每个游走称为 $n$-gram，并通过使用元素级乘积组合 $n$-gram 中节点的嵌入来获得每个 $n$-gram 的嵌入。n-gram
    游走集合的嵌入定义为所有 n-grams 嵌入的总和。最终的 n-gram 图级表示，长度为 $T$，通过将图中所有 $n$-gram 集合的嵌入 $n\in\{1,2,\cdots,T\}$
    连接起来构建。最后，图级嵌入用于分子分析的相似性预测或图分类任务。
- en: By summarizing the embedding based methods, we find the main advantage of these
    methods is their speed and scalability, due to the fact that the graph representations
    learned through these factorized models are developed on each single graph where
    there is no feature interactions across graphs. This property makes these methods
    a great option for graph similarity learning applications such as graph retrieval,
    where similarity search becomes a nearest neighbor search in a database of the
    precomputed graph representations by these factorized methods. Moreover, these
    embedding based methods provide a variety of perspectives and strategies for learning
    representations from graphs and demonstrate that these representations can be
    used for graph similarity learning. However, there are also shortcomings in these
    solutions, a common one being that the embeddings are learned independently on
    the individual graphs in a separate stage from the similarity learning, therefore
    the graph-graph proximity is not considered or utilized in the graph representation
    learning process, and the representations learned by these models may not be suitable
    for graph-graph similarity prediction compared to the methods that integrate the
    similarity learning with the graph representation learning in an end-to-end framework.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通过总结基于嵌入的方法，我们发现这些方法的主要优势在于其速度和可扩展性，因为通过这些因子化模型学习的图表示是基于每个单独图的，其中不存在跨图特征交互。这一特性使得这些方法成为图相似性学习应用的绝佳选择，例如图检索，其中相似性搜索变成了对这些因子化方法预计算的图表示数据库中的最近邻搜索。此外，这些基于嵌入的方法提供了多种视角和策略来学习图表示，并展示了这些表示可以用于图相似性学习。然而，这些解决方案也存在不足之处，一个常见的问题是嵌入是在与相似性学习分开的阶段独立学习的，因此在图表示学习过程中没有考虑或利用图与图之间的接近性，与那些将相似性学习与图表示学习整合在端到端框架中的方法相比，这些模型学习的表示可能不适合图与图的相似性预测。
- en: 3.2 GNN-based Graph Similarity Learning
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于GNN的图相似性学习
- en: 'The similarity learning methods based on Graph Neural Networks (GNNs) seek
    to learn graph representations by GNNs while doing the similarity learning task
    in an end-to-end fashion. Fig. [2](#S3.F2 "Figure 2 ‣ GNN Preliminaries. ‣ 3.2
    GNN-based Graph Similarity Learning ‣ 3 Taxonomy of Models ‣ Deep Graph Similarity
    Learning: A Survey") illustrates a general workflow of GNN-based graph similarity
    learning models. Given pairs of input graphs $<G_{i},G_{j},y_{ij}>$, where $y_{ij}$
    denotes the ground-truth similarity label or score of $<G_{i},G_{j}>$, the GNN-based
    GSL methods first employ multi-layer GNNs with weights $W$ to learn the representations
    for $G_{i}$ and $G_{j}$ in the encoding space, where the learning on each graph
    in a pair could influence each other by some mechanisms such as weight sharing
    and cross-graph interactions between the GNNs for the two graphs. A matrix or
    vector representation will be output for each graph by the GNN layers, after which
    a dot product layer or fully connected layers can be added to produce or predict
    the similarity scores between two graphs. Finally, the similarity estimates for
    all pairs of graphs and their ground-truth labels are used in a loss function
    for training the model $M$ with parameters $W$.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '基于图神经网络（GNN）的相似性学习方法旨在通过GNN学习图表示，同时以端到端的方式进行相似性学习任务。图示[2](#S3.F2 "Figure 2
    ‣ GNN Preliminaries. ‣ 3.2 GNN-based Graph Similarity Learning ‣ 3 Taxonomy of
    Models ‣ Deep Graph Similarity Learning: A Survey")展示了基于GNN的图相似性学习模型的一般工作流程。给定输入图对$<G_{i},G_{j},y_{ij}>$，其中$y_{ij}$表示$<G_{i},G_{j}>$的真实相似性标签或分数，基于GNN的GSL方法首先使用具有权重$W$的多层GNNs来学习$G_{i}$和$G_{j}$在编码空间中的表示，其中一对中的每个图的学习可以通过一些机制如权重共享和图间交互相互影响。GNN层将为每个图输出一个矩阵或向量表示，之后可以添加点积层或全连接层来生成或预测两个图之间的相似性分数。最后，所有图对的相似性估计和其真实标签用于损失函数，以训练具有参数$W$的模型$M$。'
- en: Before introducing the methods in this category, we provide the necessary background
    on GNNs.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍本类别的方法之前，我们提供了关于GNN的必要背景。
- en: GNN Preliminaries.
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GNN基础知识。
- en: Graph neural networks (GNNs) were first formulated in [gori2005new](#bib.bib48)
    , which proposed to use a propagation process to learn node representations for
    graphs. It has then been further extended by [scarselli2008graph](#bib.bib103)
    ; [gallicchio2010graph](#bib.bib43) . Later, graph convolutional networks were
    proposed which compute node updates by aggregating information in local neighborhoods
    [bruna2013spectral](#bib.bib25) ; [defferrard2016convolutional](#bib.bib34) ;
    [kipf2016semi](#bib.bib64) , and they have become the most popular graph neural
    networks, which are widely used and extended for graph representation learning
    in various domains [zhou2018graph](#bib.bib139) ; [zhang2018graph](#bib.bib137)
    ; [gao2018large](#bib.bib46) ; [gao2019graph](#bib.bib44) ; [gao2019graph1](#bib.bib45)
    .
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）最早在 [gori2005new](#bib.bib48) 中提出，该文提出使用传播过程来学习图的节点表示。随后，[scarselli2008graph](#bib.bib103)
    和 [gallicchio2010graph](#bib.bib43) 对其进行了进一步扩展。后来，提出了图卷积网络，这些网络通过聚合局部邻域中的信息来计算节点更新
    [bruna2013spectral](#bib.bib25)；[defferrard2016convolutional](#bib.bib34)；[kipf2016semi](#bib.bib64)，它们已成为最受欢迎的图神经网络，并在各个领域的图表示学习中得到了广泛应用和扩展
    [zhou2018graph](#bib.bib139)；[zhang2018graph](#bib.bib137)；[gao2018large](#bib.bib46)；[gao2019graph](#bib.bib44)；[gao2019graph1](#bib.bib45)。
- en: With the development of graph neural networks, researchers began to build graph
    similarity learning models based on GNNs. In this section, we will first introduce
    the workflow of GCNs with the spectral GCN [shuman2013emerging](#bib.bib106) as
    an example, and then describe the GNN-based graph similarity learning methods
    covering three main categories.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图神经网络的发展，研究人员开始构建基于 GNN 的图相似性学习模型。在本节中，我们将首先介绍以谱GCN [shuman2013emerging](#bib.bib106)
    为例的 GCN 工作流程，然后描述基于 GNN 的图相似性学习方法，包括三大类方法。
- en: Given a graph $G=(V,E,\mathbf{A})$, where $V$ is the set of vertices, $E\subset
    V\times V$ is the set of edges, and $\mathbf{A}\in\mathbb{R}^{m\times m}$ is the
    adjacency matrix, the diagonal degree matrix $\mathbf{D}$ will have elements $\mathbf{D}_{ii}=\sum_{j}\mathbf{A}_{ij}$.
    The graph Laplacian matrix is $\mathbf{L}=\mathbf{D}-\mathbf{A}$, which can be
    normalized as $\mathbf{L}=\mathbf{I}_{m}-\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$,
    where $\mathbf{I}_{m}$ is the identity matrix. Assume the orthonormal eigenvectors
    of $\mathbf{L}$ are represented as $\{u_{l}\}_{l=0}^{m-1}\in\mathbb{R}^{m\times
    m}$, and their associated eigenvalues are $\{\lambda_{l}\}_{l=0}^{m-1}$, the Laplacian
    is diagonalized by the Fourier basis $[u_{0},\cdots,u_{m-1}](=\mathbf{U})\in\mathbb{R}^{m\times
    m}$ and $\mathbf{L}=\mathbf{U\Lambda U^{T}}$ where $\mathbf{\Lambda}=diag([\lambda_{0},\cdots,\lambda_{m-1}])\in\mathbb{R}^{m\times
    m}$. The graph Fourier transform of a signal $x\in\mathbb{R}^{m}$ can then be
    defined as $\hat{x}=\mathbf{U^{T}}x\in\mathbb{R}^{m}$[shuman2013emerging](#bib.bib106)
    . Suppose a signal vector $\mathbf{x}\mathrel{\mathop{\mathchar 58\relax}}V\rightarrow\mathbb{R}$
    is defined on the nodes of graph $G$, where $\mathbf{x}_{i}$ is the value of $\mathbf{x}$
    at the $i^{th}$ node. Then the signal $\mathbf{x}$ can be filtered by $g_{\theta}$
    as
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个图 $G=(V,E,\mathbf{A})$，其中 $V$ 是顶点集合，$E\subset V\times V$ 是边集合，$\mathbf{A}\in\mathbb{R}^{m\times
    m}$ 是邻接矩阵，对角度矩阵 $\mathbf{D}$ 的元素 $\mathbf{D}_{ii}=\sum_{j}\mathbf{A}_{ij}$。图拉普拉斯矩阵为
    $\mathbf{L}=\mathbf{D}-\mathbf{A}$，可以被标准化为 $\mathbf{L}=\mathbf{I}_{m}-\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$，其中
    $\mathbf{I}_{m}$ 是单位矩阵。假设 $\mathbf{L}$ 的正交特征向量表示为 $\{u_{l}\}_{l=0}^{m-1}\in\mathbb{R}^{m\times
    m}$，它们的特征值为 $\{\lambda_{l}\}_{l=0}^{m-1}$，拉普拉斯矩阵通过傅里叶基底 $[u_{0},\cdots,u_{m-1}](=\mathbf{U})\in\mathbb{R}^{m\times
    m}$ 对角化，$\mathbf{L}=\mathbf{U\Lambda U^{T}}$ 其中 $\mathbf{\Lambda}=diag([\lambda_{0},\cdots,\lambda_{m-1}])\in\mathbb{R}^{m\times
    m}$。信号 $x\in\mathbb{R}^{m}$ 的图傅里叶变换可以定义为 $\hat{x}=\mathbf{U^{T}}x\in\mathbb{R}^{m}$[shuman2013emerging](#bib.bib106)
    。假设一个信号向量 $\mathbf{x}\mathrel{\mathop{\mathchar 58\relax}}V\rightarrow\mathbb{R}$
    定义在图 $G$ 的节点上，其中 $\mathbf{x}_{i}$ 是第 $i^{th}$ 个节点上 $\mathbf{x}$ 的值。然后，信号 $\mathbf{x}$
    可以通过 $g_{\theta}$ 进行滤波。
- en: '|  | $\displaystyle y=g_{\theta}*\mathbf{x}=g_{\theta}(\mathbf{L})\mathbf{x}=g_{\theta}(\mathbf{U{\Lambda}U^{T}})\mathbf{x}=\mathbf{U}g_{\theta}(\Lambda)\mathbf{U^{T}}\mathbf{x}$
    |  | (7) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y=g_{\theta}*\mathbf{x}=g_{\theta}(\mathbf{L})\mathbf{x}=g_{\theta}(\mathbf{U{\Lambda}U^{T}})\mathbf{x}=\mathbf{U}g_{\theta}(\Lambda)\mathbf{U^{T}}\mathbf{x}$
    |  | (7) |'
- en: 'where the filter $g_{\theta}(\Lambda)$ can be defined as $g_{\theta}(\Lambda)=\sum_{k=0}^{K-1}{\theta_{k}}{\Lambda^{k}}$,
    and the parameter $\theta\in{\mathbb{R}}^{K}$ is a vector of polynomial coefficients
    [defferrard2016convolutional](#bib.bib34) . GCNs can be constructed by stacking
    multiple convolutional layers in the form of Equation ([7](#S3.E7 "In GNN Preliminaries.
    ‣ 3.2 GNN-based Graph Similarity Learning ‣ 3 Taxonomy of Models ‣ Deep Graph
    Similarity Learning: A Survey")), with a non-linearity activation (ReLU) following
    each layer.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其中滤波器 $g_{\theta}(\Lambda)$ 可以定义为 $g_{\theta}(\Lambda)=\sum_{k=0}^{K-1}{\theta_{k}}{\Lambda^{k}}$，参数
    $\theta\in{\mathbb{R}}^{K}$ 是多项式系数的向量 [defferrard2016convolutional](#bib.bib34)。GCNs
    可以通过堆叠多个卷积层的形式构建，如方程 ([7](#S3.E7 "在 GNN 基础知识中。 ‣ 3.2 基于 GNN 的图相似度学习 ‣ 3 模型分类 ‣
    深度图相似度学习：综述"))，每层后面跟着一个非线性激活函数（ReLU）。
- en: '![Refer to caption](img/980ab3e3039251bba75b6834775c81b1.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/980ab3e3039251bba75b6834775c81b1.png)'
- en: 'Figure 2: Illustration of GNN-based Graph Similarity Learning.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：基于 GNN 的图相似度学习示意图。
- en: 'Based on how graph-graph similarity/proximity is leveraged in the learning,
    we summarize the existing GNN-based graph similarity learning work into three
    main categories: 1) GNN-CNN mixed models for graph similarity prediction, 2) Siamese
    GNNs for graph similarity prediction, and 3) GNN-based graph matching networks.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 基于在学习中如何利用图-图相似度/接近度，我们将现有的基于 GNN 的图相似度学习工作总结为三大类：1) GNN-CNN 混合模型用于图相似度预测，2)
    用于图相似度预测的孪生 GNN，3) 基于 GNN 的图匹配网络。
- en: 3.2.1 GNN-CNN Models for Graph Similarity Prediction
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 GNN-CNN 模型用于图相似度预测
- en: The works that use GNN-CNN mixed networks for graph similarity prediction mainly
    employ GNNs to learn graph representations and leverage the learned representations
    into CNNs for predicting similarity scores, which is approached as a classification
    or regression problem. Fully connected layers are often added for the similarity
    score prediction in an end-to-end learning framework.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GNN-CNN 混合网络进行图相似度预测的研究主要利用 GNN 来学习图表示，并将学习到的表示输入到 CNN 中以预测相似度分数，这被视为分类或回归问题。在端到端学习框架中，通常会添加全连接层来进行相似度分数预测。
- en: (1) GSimCNN.
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (1) GSimCNN。
- en: In [bai2018convolutional](#bib.bib16) , a method called GSimCNN is proposed
    for pairwise graph similarity prediction, which consists of three stages. In Stage
    1, node representations are first generated by multi-layer GCNs, where each layer
    is defined as
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [bai2018convolutional](#bib.bib16) 中，提出了一种名为 GSimCNN 的方法用于成对图相似度预测，该方法包括三个阶段。在第
    1 阶段，首先通过多层 GCN 生成节点表示，其中每一层定义为
- en: '|  | $\displaystyle conv(\mathbf{x}_{i})=ReLU(\sum_{j\in N(i)}\frac{1}{\sqrt{d_{i}d_{j}}}\mathbf{x}_{j}\mathbf{W}^{(l)}+\mathbf{b}^{(l)})$
    |  | (8) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle conv(\mathbf{x}_{i})=ReLU(\sum_{j\in N(i)}\frac{1}{\sqrt{d_{i}d_{j}}}\mathbf{x}_{j}\mathbf{W}^{(l)}+\mathbf{b}^{(l)})$
    |  | (8) |'
- en: where $N(i)$ is the set of first-order neighbors of node $i$ plus node $i$ itself,
    $d_{i}$ is the degree of node $i$ plus $1$, $\mathbf{W}^{(l)}$ is the weight matrix
    for the $l-$th GCN layer, $\mathbf{b}^{(l)}$ is the bias, and $ReLU(x)=max(0,x)$
    is the activation function. In Stage 2, the inner products between all possible
    pairs of node embeddings between two graphs from different GCN layers are calculated,
    which results in multiple similarity matrices. Finally, the similarity matrices
    from different layers are processed by multiple independent CNNs, where the output
    of the CNNs are concatenated and fed into fully connected layers for predicting
    the final similarity score $s_{ij}$ for each pair of graphs $G_{i}$ and $G_{j}$.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N(i)$ 是节点 $i$ 的一阶邻居集合加上节点 $i$ 本身，$d_{i}$ 是节点 $i$ 的度加 $1$，$\mathbf{W}^{(l)}$
    是第 $l$ 层 GCN 的权重矩阵，$\mathbf{b}^{(l)}$ 是偏置项，$ReLU(x)=max(0,x)$ 是激活函数。在第 2 阶段，计算两个图中来自不同
    GCN 层的所有可能的节点嵌入对之间的内积，这将产生多个相似度矩阵。最后，将来自不同层的相似度矩阵由多个独立的 CNN 处理，CNN 的输出被拼接并输入到全连接层中，以预测每对图
    $G_{i}$ 和 $G_{j}$ 的最终相似度分数 $s_{ij}$。
- en: (2) SimGNN.
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2) SimGNN。
- en: In [bai2019simgnn](#bib.bib15) , a SimGNN model is introduced based on the GSimCNN
    from [bai2018convolutional](#bib.bib16) . In addition to pairwise node comparison
    with node-level embeddings from the GCN output, neural tensor networks (NTN) [socher2013reasoning](#bib.bib107)
    are utilized to model the relation between the graph-level embeddings of two input
    graphs, whereas the graph embedding for each graph is generated via a weighted
    sum of node embeddings, and a global context-aware attention is applied on each
    node, such that nodes similar to the global context receive higher attention weights.
    Finally, both the comparison between node-level embeddings and graph-level embeddings
    are considered for the similarity score prediction in the CNN fully connected
    layers.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在[bai2019simgnn](#bib.bib15)中，引入了一种基于[bai2018convolutional](#bib.bib16)中的GSimCNN的SimGNN模型。除了使用GCN输出的节点级嵌入进行成对节点比较外，还利用神经张量网络（NTN）[socher2013reasoning](#bib.bib107)来建模两个输入图的图级嵌入之间的关系，而每个图的图嵌入通过节点嵌入的加权和生成，并且在每个节点上应用全局上下文感知注意力，以便与全局上下文相似的节点接收更高的注意力权重。最后，在CNN全连接层中同时考虑节点级嵌入和图级嵌入之间的比较，以预测相似性分数。
- en: 3.2.2 Siamese GNN models for Graph Similarity Learning
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 用于图相似性学习的Siamese GNN模型
- en: 'This category of works uses the Siamese network architecture with GNNs as twin
    networks to simultaneously learn representations from two graphs, and then obtain
    a similarity estimate based on the output representations of the GNNs. Fig. [3](#S3.F3
    "Figure 3 ‣ 3.2.2 Siamese GNN models for Graph Similarity Learning ‣ 3.2 GNN-based
    Graph Similarity Learning ‣ 3 Taxonomy of Models ‣ Deep Graph Similarity Learning:
    A Survey") shows an example of Siamese architecture with GCNs in the twin networks,
    where the weights of the networks are shared with each other. The similarity estimate
    is typically leveraged in a loss function for training the network.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这类工作使用带有GNN的Siamese网络架构作为双胞胎网络，同时从两个图中学习表示，然后基于GNN的输出表示获得相似性估计。图[3](#S3.F3 "图
    3 ‣ 3.2.2 用于图相似性学习的Siamese GNN模型 ‣ 3.2 基于GNN的图相似性学习 ‣ 3 模型分类 ‣ 深度图相似性学习：综述")展示了带有GCNs的Siamese架构示例，其中网络的权重相互共享。相似性估计通常被用于训练网络的损失函数中。
- en: '![Refer to caption](img/66ed3c9a6680edbe172b34074186f4d1.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/66ed3c9a6680edbe172b34074186f4d1.png)'
- en: 'Figure 3: Siamese Architecture with Graph Convolutional Networks.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：带有图卷积网络的Siamese架构。
- en: (1) Siamese GCN.
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (1) Siamese GCN。
- en: The work in [ktena2018metric](#bib.bib67) proposes to learn a graph similarity
    metric using the Siamese graph convolutional neural network (S-GCN) in a supervised
    setting. The S-GCN takes a pair of graphs as inputs and employs spectral GCN to
    get graph embedding for each input graph, after which a dot product layer followed
    by a fully connected layer is used to produce the similarity estimate between
    the two graphs in the spectral domain.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[ktena2018metric](#bib.bib67)中的工作提出使用Siamese图卷积神经网络（S-GCN）在监督设置中学习图相似性度量。S-GCN接受一对图作为输入，并采用谱GCN为每个输入图获取图嵌入，然后使用点积层和全连接层来生成两个图之间的谱域相似性估计。'
- en: (2) Higher-order Siamese GCN.
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2) 高阶Siamese GCN。
- en: 'Higher-order Siamese GCN (HS-GCN) is proposed in [ma2019similarity](#bib.bib78)
    , which incorporates higher-order node-level proximity into graph convolutional
    networks so as to perform higher-order convolutions on each of the input graphs
    for the graph similarity learning task. A Siamese framework is employed with the
    proposed higher-order GCN in each of the twin networks. Specifically, random walk
    is used for capturing higher-order proximity from graphs and refining the graph
    representations used in graph convolutions. Both this work and the S-GCN [ktena2018metric](#bib.bib67)
    introduced above use the Hinge loss for training the Siamese similarity learning
    models:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ma2019similarity](#bib.bib78)中提出了高阶Siamese GCN（HS-GCN），它将高阶节点级别的邻近性引入图卷积网络，以便对每个输入图进行高阶卷积，用于图相似性学习任务。在每个双胞胎网络中使用了Siamese框架与所提出的高阶GCN。具体而言，使用随机游走来捕捉图中的高阶邻近性并改进用于图卷积的图表示。这项工作和上面提到的S-GCN
    [ktena2018metric](#bib.bib67)都使用了Hinge损失来训练Siamese相似性学习模型：
- en: '|  | $\displaystyle L_{Hinge}=\frac{1}{K}\sum_{i=1}^{N}\sum_{j=i+1}^{N}max(0,1-{y_{ij}}{s_{ij}}),$
    |  | (9) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{Hinge}=\frac{1}{K}\sum_{i=1}^{N}\sum_{j=i+1}^{N}max(0,1-{y_{ij}}{s_{ij}}),$
    |  | (9) |'
- en: where $N$ is the total number of graphs in the training set, $K=N(N-1)/2$ is
    the total number of pairs from the training set, $y_{ij}$ is the ground-truth
    label for the pair of graphs $G_{i}$ and $G_{j}$ where $y_{ij}=1$ for similar
    pairs and $y_{ij}=-1$ for dissimilar pairs, and $s_{ij}$ is the similarity score
    estimated by the model. More general forms of higher-order information (e.g.,
    motifs [ahmed2015efficient](#bib.bib7) ; [ahmed2017graphlet](#bib.bib8) ) have
    been used for learning graph representations [rossi2018higher](#bib.bib99) ; [rossi2020structural](#bib.bib100)
    and would likely benefit the learning.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 是训练集中的图总数，$K=N(N-1)/2$ 是训练集中对的总数，$y_{ij}$ 是图对 $G_{i}$ 和 $G_{j}$ 的真实标签，其中
    $y_{ij}=1$ 表示相似对，$y_{ij}=-1$ 表示不相似对，$s_{ij}$ 是模型估计的相似度分数。更一般的高阶信息形式（例如，motifs
    [ahmed2015efficient](#bib.bib7) ; [ahmed2017graphlet](#bib.bib8) ）已用于学习图表示 [rossi2018higher](#bib.bib99)
    ; [rossi2020structural](#bib.bib100) 并且可能会对学习有所帮助。
- en: (3) Community-preserving Siamese GCN.
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3) 社区保留的Siamese GCN。
- en: 'In [liu2019community](#bib.bib76) , another Siamese GCN based model called
    SCP-GCN is proposed for the similarity learning in functional and structural joint
    analysis of brain networks, where the graph structure used in the GCN is defined
    from the structural connectivity network while the node features come from the
    functional brain network. The contrastive loss (Equation ([10](#S3.E10 "In (3)
    Community-preserving Siamese GCN. ‣ 3.2.2 Siamese GNN models for Graph Similarity
    Learning ‣ 3.2 GNN-based Graph Similarity Learning ‣ 3 Taxonomy of Models ‣ Deep
    Graph Similarity Learning: A Survey"))) along with a newly proposed community-preserving
    loss (Equation ([11](#S3.E11 "In (3) Community-preserving Siamese GCN. ‣ 3.2.2
    Siamese GNN models for Graph Similarity Learning ‣ 3.2 GNN-based Graph Similarity
    Learning ‣ 3 Taxonomy of Models ‣ Deep Graph Similarity Learning: A Survey")))
    is used for training the model.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [liu2019community](#bib.bib76) 中，提出了一种基于 Siamese GCN 的模型称为 SCP-GCN，用于功能和结构的脑网络联合分析中的相似性学习，其中
    GCN 中使用的图结构来自结构连接网络，而节点特征来自功能脑网络。对比损失（方程 ([10](#S3.E10 "在 (3) 社区保留的 Siamese GCN.
    ‣ 3.2.2 Siamese GNN 模型用于图相似性学习 ‣ 3.2 GNN 基于图的相似性学习 ‣ 3 模型分类 ‣ 深度图相似性学习：综述"))）以及新提出的社区保留损失（方程
    ([11](#S3.E11 "在 (3) 社区保留的 Siamese GCN. ‣ 3.2.2 Siamese GNN 模型用于图相似性学习 ‣ 3.2 GNN
    基于图的相似性学习 ‣ 3 模型分类 ‣ 深度图相似性学习：综述"))）用于训练模型。
- en: '|  | $L_{Contrastive}=\frac{y_{ij}}{2}\&#124;\mathbf{g}_{i}-\mathbf{g}_{j}\&#124;_{2}^{2}+(1-y_{ij})\frac{1}{2}\{max(0,m-\&#124;\mathbf{g}_{i}-\mathbf{g}_{j}\&#124;_{2})\}^{2}$
    |  | (10) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{Contrastive}=\frac{y_{ij}}{2}\&#124;\mathbf{g}_{i}-\mathbf{g}_{j}\&#124;_{2}^{2}+(1-y_{ij})\frac{1}{2}\{max(0,m-\&#124;\mathbf{g}_{i}-\mathbf{g}_{j}\&#124;_{2})\}^{2}$
    |  | (10) |'
- en: where $\mathbf{g}_{i}$ and $\mathbf{g}_{j}$ are the graph embeddings of graph
    $G_{i}$ and graph $G_{j}$ computed from the GCN, $m$ is a margin value which is
    greater than $0$. $y_{ij}=1$ if $G_{i}$ and $G_{j}$ are from the same class and
    $y_{ij}=0$ if they are from different classes. By minimizing the contrastive loss,
    the Euclidean distance between two graph embedding vectors will be minimized when
    the two graphs are from the same class, and maximized when they belong to different
    classes. The community-preserving loss is defined as follows.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{g}_{i}$ 和 $\mathbf{g}_{j}$ 是从 GCN 计算出的图 $G_{i}$ 和图 $G_{j}$ 的图嵌入，$m$
    是一个大于 $0$ 的边际值。如果 $G_{i}$ 和 $G_{j}$ 来自同一类，则 $y_{ij}=1$，如果来自不同类，则 $y_{ij}=0$。通过最小化对比损失，当两个图来自同一类时，它们的图嵌入向量之间的欧几里得距离将被最小化，而当它们来自不同类时将被最大化。社区保留的损失定义如下。
- en: '|  | $L_{CP}=\alpha(\sum_{c}\frac{1}{&#124;S_{c}&#124;}\sum_{i\in S_{c}}\&#124;\mathbf{z}_{i}-\hat{\mathbf{z}}_{c}\&#124;_{2}^{2})-\beta\sum_{c,c^{\prime}}\&#124;\hat{\mathbf{z}}_{c}-\hat{\mathbf{z}}_{c^{\prime}}\&#124;_{2}^{2}$
    |  | (11) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{CP}=\alpha(\sum_{c}\frac{1}{&#124;S_{c}&#124;}\sum_{i\in S_{c}}\&#124;\mathbf{z}_{i}-\hat{\mathbf{z}}_{c}\&#124;_{2}^{2})-\beta\sum_{c,c^{\prime}}\&#124;\hat{\mathbf{z}}_{c}-\hat{\mathbf{z}}_{c^{\prime}}\&#124;_{2}^{2}$
    |  | (11) |'
- en: where $S_{c}$ contains the indexes of nodes belonging to community $c$, $\hat{\mathbf{z}}_{c}=\frac{1}{|S_{c}|}\sum_{i\in
    S_{c}}\mathbf{z}_{i}$ is the community center embedding for each community $c$,
    where $\mathbf{z}_{i}$ is the embedding of the $i^{th}$ node, i.e., the $i^{th}$
    row in the node embedding $\mathbf{Z}$ of the GCN output, and $\alpha$ and $\beta$
    are the weights balancing the intra/inter-community loss.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S_{c}$ 包含属于社区 $c$ 的节点索引，$\hat{\mathbf{z}}_{c}=\frac{1}{|S_{c}|}\sum_{i\in
    S_{c}}\mathbf{z}_{i}$ 是每个社区 $c$ 的社区中心嵌入，其中 $\mathbf{z}_{i}$ 是第 $i$ 个节点的嵌入，即 GCN
    输出中节点嵌入 $\mathbf{Z}$ 的第 $i$ 行，$\alpha$ 和 $\beta$ 是平衡社区内部/间社区损失的权重。
- en: (4) Hierarchical Siamese GNN.
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (4) 层次化的Siamese GNN。
- en: 'In [ijcai2019-522](#bib.bib122) , a Siamese network with two hierarchical GNN
    models is introduced for the similarity learning of heterogeneous graphs for unknown
    malware detection. Specifically, they consider the path-relevant sets of neighbors
    according to meta-paths and generate node embeddings by selectively aggregating
    the entities in each path-relevant neighbor set. The loss function in Equation ([2](#S3.E2
    "In (2) Neural Networks with Structure2vec. ‣ 3.1.2 Graph-level Embedding based
    Methods ‣ 3.1 Graph Embedding based Graph Similarity Learning ‣ 3 Taxonomy of
    Models ‣ Deep Graph Similarity Learning: A Survey")) is used for training the
    model.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [ijcai2019-522](#bib.bib122) 中，引入了一种具有两个层次化GNN模型的Siamese网络，用于异构图的相似性学习，以检测未知恶意软件。具体而言，他们根据元路径考虑路径相关的邻居集，并通过选择性地聚合每个路径相关邻居集中的实体来生成节点嵌入。模型训练中使用了方程
    ([2](#S3.E2 "在(2)中 神经网络与Structure2vec. ‣ 3.1.2 基于图级嵌入的方法 ‣ 3.1 基于图嵌入的图相似性学习 ‣
    3 模型分类 ‣ 深度图相似性学习：综述")) 中的损失函数。
- en: (5) Siamese GCN for Image Retrieval.
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5) 用于图像检索的Siamese GCN。
- en: In [chaudhuri2019siamese](#bib.bib29) , Siamese GCNs are used for content based
    remote sensing image retrieval, where each image is converted to a region adjacency
    graph in which each node represents a region segmented from the image. The goal
    is to learn an embedding space that pulls semantically coherent images closer
    while pushing dissimilar samples far apart. Contrastive loss is used in the model
    training.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [chaudhuri2019siamese](#bib.bib29) 中，Siamese GCNs 用于基于内容的遥感图像检索，其中每张图像被转换为一个区域邻接图，在这个图中，每个节点代表从图像中分割出的一个区域。目标是学习一个嵌入空间，使得语义上相关的图像更接近，同时将不相似的样本分开。模型训练中使用对比损失。
- en: Since the twin GNNs in the Siamese network share the same weights, an advantage
    of the Siamese GNN models is that the two input graphs are guaranteed to be processed
    in the same manner by the networks. As such, similar input graphs would be embedded
    similarly in the latent space. Therefore, the Siamese GNNs are good for differentiating
    the two input graphs in the latent space or measuring the similarity between them.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Siamese网络中的双胞胎GNN共享相同的权重，Siamese GNN模型的一个优势是保证两个输入图在网络中以相同的方式处理。因此，相似的输入图会在潜在空间中以相似的方式嵌入。因此，Siamese
    GNNs 非常适合区分潜在空间中的两个输入图或测量它们之间的相似性。
- en: In addition to choosing the appropriate GNN models in the twin networks, one
    needs to choose a proper loss function. Another widely used loss function for
    Siamese network is the triplet loss [schroff2015facenet](#bib.bib104) . For a
    triplet $(G_{i},G_{p},G_{n})$, $G_{p}$ is from the same class as $G_{i}$, while
    $G_{n}$ is from a different class from $G_{i}$. The triplet loss is defined as
    follows.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在双胞胎网络中选择合适的GNN模型外，还需要选择适当的损失函数。Siamese网络中另一种广泛使用的损失函数是三元组损失 [schroff2015facenet](#bib.bib104)。对于一个三元组
    $(G_{i},G_{p},G_{n})$，$G_{p}$ 与 $G_{i}$ 同属一个类别，而 $G_{n}$ 则与 $G_{i}$ 不同类别。三元组损失定义如下。
- en: '|  | $L_{Triplet}=\frac{1}{K}\sum_{K}max(d_{ip}-d_{in}+m,0)$ |  | (12) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{Triplet}=\frac{1}{K}\sum_{K}max(d_{ip}-d_{in}+m,0)$ |  | (12) |'
- en: where $K$ is the number of triplets used in the training, $d_{ip}$ represents
    the distance between $G_{i}$ and $G_{p}$, $d_{in}$ represents the distance between
    $G_{i}$ and $G_{n}$, and $m$ is a margin value which is greater than 0\. By minimizing
    the triplet loss, the distance between graphs from same class (i.e., $d_{ip}$)
    will be pushed to $0$, and the distance between graphs from different classes
    (i.e.,$d_{in}$ will be pushed to be greater than $d_{ip}+m$.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $K$ 是训练中使用的三元组数量，$d_{ip}$ 表示 $G_{i}$ 和 $G_{p}$ 之间的距离，$d_{in}$ 表示 $G_{i}$
    和 $G_{n}$ 之间的距离，$m$ 是一个大于0的边际值。通过最小化三元组损失，同类别图（即 $d_{ip}$）之间的距离将被推向 $0$，而不同类别图（即
    $d_{in}$）之间的距离将被推向大于 $d_{ip}+m$。
- en: It is important to consider which loss function would be suitable for the targeted
    problem when applying these Siamese GNN models for the graph similarity learning
    task in practice.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用这些Siamese GNN模型进行图相似性学习任务时，考虑哪种损失函数适合目标问题是很重要的。
- en: 3.2.3 GNN-based Graph Matching Networks
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 基于GNN的图匹配网络
- en: '![Refer to caption](img/ad0b721705d044fb79a27348e83b410a.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad0b721705d044fb79a27348e83b410a.png)'
- en: (a) Siamese GNN
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Siamese GNN
- en: '![Refer to caption](img/728b21a66d0eeb107dbeb491b5120547.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/728b21a66d0eeb107dbeb491b5120547.png)'
- en: (b) GNN-based Graph Matching Network
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 基于GNN的图匹配网络
- en: 'Figure 4: Comparison of the Learning Process of Siamese GNN and GNN-based Graph
    Matching Network'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Siamese GNN与基于GNN的图匹配网络学习过程的比较
- en: 'The work in this category adapts Siamese GNNs by incorporating matching mechanisms
    during the learning with GNNs, and cross-graph interactions are considered in
    the graph representation learning process. Fig. [4](#S3.F4 "Figure 4 ‣ 3.2.3 GNN-based
    Graph Matching Networks ‣ 3.2 GNN-based Graph Similarity Learning ‣ 3 Taxonomy
    of Models ‣ Deep Graph Similarity Learning: A Survey") shows this difference between
    the Siamese GNNs and the GNN-based graph matching networks.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的工作通过在 GNN 学习过程中融入匹配机制来改进 Siamese GNNs，并在图表示学习过程中考虑跨图交互。图 [4](#S3.F4 "图
    4 ‣ 3.2.3 基于 GNN 的图匹配网络 ‣ 3.2 基于 GNN 的图相似性学习 ‣ 3 模型分类 ‣ 深度图相似性学习：综述") 展示了 Siamese
    GNNs 和基于 GNN 的图匹配网络之间的差异。
- en: '(1) GMN: Graph Matching Network.'
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (1) GMN：图匹配网络。
- en: In [li2019graph](#bib.bib73) , a GNN based architecture called Graph Matching
    Network (GMN) is proposed, where the node update module in each propagation layer
    takes into account both the aggregated messages on the edges for each graph and
    a cross-graph matching vector which measures how well a node in one graph can
    be matched to the nodes in the other graph. Given a pair of graphs as input, the
    GMN jointly learns graph representations for the pair through the cross-graph
    attention-based matching mechanism, which propagates node representations by using
    both the neighborhood information within the same graph and cross-graph node information.
    A similarity score between the two input graphs is computed in the latent vector
    space.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [li2019graph](#bib.bib73) 中，提出了一种基于 GNN 的架构称为图匹配网络（GMN），其中每个传播层中的节点更新模块同时考虑了每个图上的边缘聚合消息和一个跨图匹配向量，该向量衡量一个图中的节点与另一个图中的节点的匹配程度。给定一对图作为输入，GMN
    通过基于跨图注意力的匹配机制联合学习这对图的表示，该机制通过使用图内的邻域信息和跨图节点信息传播节点表示。计算两个输入图之间的相似性分数在潜在向量空间中。
- en: '(2) NeuralMCS: Neural Maximum Common Subgraph GMN.'
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (2) NeuralMCS：神经最大公共子图 GMN。
- en: Based on the graph matching network in [li2019graph](#bib.bib73) , [anonymous2019_mcs](#bib.bib17)
    proposes a neural maximum common subgraph (MCS) detection approach for learning
    graph similarity. The graph matching network is adapted to learn node representations
    for two input graphs $G_{1}$ and $G_{2}$, after which a likelihood of matching
    each node in $G_{1}$ to each node in $G_{2}$ is computed by a normalized dot product
    between the node embeddings. The likelihood indicates which node pair is most
    likely to be in the MCS, and the likelihood for all pairs of nodes constitutes
    the matching matrix $\mathbf{Y}$ for $G_{1}$ and $G_{2}$. Then a guided subgraph
    extraction process is applied, which starts by finding the most likely pair and
    iteratively expands the extracted subgraphs by selecting one more pair at a time
    until adding more pairs would lead to non-isomorphic subgraphs. To check the subgraph
    isomorphism, subgraph-level embeddings are computed by aggregating the node embeddings
    of the neighboring nodes that are included in the MCS, and Euclidean distance
    between the subgraph embeddings are computed. Finally, a similarity/match score
    is obtained based on the subgraphs extracted from $G_{1}$ and $G_{2}$.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 [li2019graph](#bib.bib73) 中的图匹配网络，[anonymous2019_mcs](#bib.bib17) 提出了一个神经最大公共子图（MCS）检测方法用于学习图相似性。图匹配网络被调整以学习两个输入图
    $G_{1}$ 和 $G_{2}$ 的节点表示，之后通过节点嵌入之间的归一化点积计算每个 $G_{1}$ 中的节点与每个 $G_{2}$ 中的节点匹配的可能性。该可能性表示哪个节点对最有可能在
    MCS 中，并且所有节点对的可能性构成了 $G_{1}$ 和 $G_{2}$ 的匹配矩阵 $\mathbf{Y}$。然后应用引导子图提取过程，开始时找到最可能的节点对，并通过一次选择一个更多的节点对，迭代扩展提取的子图，直到添加更多的节点对会导致非同构子图。为了检查子图同构性，通过汇总包含在
    MCS 中的邻近节点的节点嵌入来计算子图级嵌入，并计算子图嵌入之间的欧几里得距离。最后，根据从 $G_{1}$ 和 $G_{2}$ 中提取的子图获得相似性/匹配分数。
- en: (3) Hierarchical Graph Matching Network.
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (3) 分层图匹配网络。
- en: In [anonymous2019](#bib.bib75) , a hierarchical graph matching network is proposed
    for graph similarity learning, which consists of a Siamese GNN for learning global-level
    interactions between two graphs and a multi-perspective node-graph matching network
    for learning the cross-level node-graph interactions between parts of one graph
    and one whole graph. Given two graphs $G_{1}$ and $G_{2}$ as inputs, a three-layer
    GCN is utilized to generate embeddings for them, and aggregation layers are added
    to generate the graph embedding vector for each graph. In particular, cross-graph
    attention coefficients are calculated between each node in $G_{1}$ and all the
    nodes in $G_{2}$, and between each node in $G_{2}$ and all the nodes in $G_{1}$.
    Then the attentive graph-level embeddings are generated using the weighted average
    of node embeddings of the other graph, and a multi-perspective matching function
    is defined to compare the node embeddings of one graph with the attentive graph-level
    embeddings of the other graph. Finally, the BiLSTM model [schuster1997bidirectional](#bib.bib105)
    is used to aggregate the cross-level interaction feature matrix from the node-graph
    matching layer, followed by the final prediction layers for the similarity score
    learning.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [anonymous2019](#bib.bib75) 中，提出了一种用于图相似性学习的分层图匹配网络，该网络包括一个Siamese GNN用于学习两个图之间的全局级别交互和一个多视角节点-图匹配网络用于学习一个图的部分和整个图之间的跨级别节点-图交互。给定两个图
    $G_{1}$ 和 $G_{2}$ 作为输入，利用三层GCN生成它们的嵌入，并添加聚合层生成每个图的图嵌入向量。特别地，计算 $G_{1}$ 中每个节点与
    $G_{2}$ 中所有节点之间的跨图注意力系数，以及 $G_{2}$ 中每个节点与 $G_{1}$ 中所有节点之间的跨图注意力系数。然后，通过对另一个图的节点嵌入进行加权平均生成关注图级别的嵌入，并定义多视角匹配函数以比较一个图的节点嵌入与另一个图的关注图级别嵌入。最后，使用
    BiLSTM 模型 [schuster1997bidirectional](#bib.bib105) 聚合来自节点-图匹配层的跨级别交互特征矩阵，然后进行相似度得分学习的最终预测层。
- en: '(4) NCMN: Neural Graph Matching Network.'
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '(4) NCMN: 神经图匹配网络。'
- en: In [guo2018neural](#bib.bib53) , a Neural Graph Matching Network (NGMN) is proposed
    for few-shot 3D action recognition, where 3D data are represented as interaction
    graphs. A GCN is applied for updating node features in the graphs and an MLP is
    employed for updating the edge strength. A graph matching metric is then defined
    based on both node matching features and edge matching features. In the proposed
    NGMN, edge generation and graph matching metric are learned jointly for the few-shot
    learning task.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [guo2018neural](#bib.bib53) 中，提出了一种神经图匹配网络（NGMN）用于少样本3D动作识别，其中3D数据被表示为交互图。GCN被应用于更新图中的节点特征，MLP则用于更新边的强度。然后，基于节点匹配特征和边匹配特征定义图匹配度量。在提出的NGMN中，边生成和图匹配度量是为少样本学习任务共同学习的。
- en: Recently, deep graph matching networks were introduced for the graph matching
    problem for image matching [anonymous2019_matching](#bib.bib41) ; [zanfir2018deep](#bib.bib134)
    ; [jiang2019glmnet](#bib.bib60) ; [wang2019learning](#bib.bib121) . Graph matching
    aims to find node correspondence between graphs, such that the corresponding node
    and edge’s affinity is maximized. Although the problem of graph matching is different
    from the graph similarity learning problem we focus on in this survey and is beyond
    the scope of this survey, some work on deep graph matching networks involves graph
    similarity learning and thus we review some of this work below to provide some
    insights into how deep similarity learning may be leveraged for graph matching
    applications, such as image matching.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度图匹配网络被引入用于图匹配问题，用于图像匹配 [anonymous2019_matching](#bib.bib41)；[zanfir2018deep](#bib.bib134)；[jiang2019glmnet](#bib.bib60)；[wang2019learning](#bib.bib121)。图匹配的目标是找到图之间的节点对应关系，以最大化对应节点和边的相似度。尽管图匹配问题与我们在本综述中关注的图相似性学习问题不同，并且超出了本综述的范围，但一些深度图匹配网络的研究涉及图相似性学习，因此我们在下文中回顾了一些相关工作，以提供深度相似性学习如何应用于图匹配应用（如图像匹配）的见解。
- en: (5) GMNs for Image Matching.
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5) 用于图像匹配的GMNs。
- en: In [jiang2019glmnet](#bib.bib60) , a Graph Learning-Matching Network is proposed
    for image matching. A CNN is first utilized to extract feature descriptors of
    all feature points for the input images, and graphs are then constructed based
    on the features. Then the GCNs are used for learning node embeddings from the
    graphs, in which both intra-graph convolutions and cross-graph convolutions are
    conducted. The final matching prediction is formulated as node-to-node affinity
    metric learning in the embedding space, and the constraint regularized loss along
    with cross-entropy loss is used for the metric learning and the matching prediction.
    In [wang2019learning](#bib.bib121) , another GNN-based graph matching network
    is proposed for the image matching problem, which consists of a CNN image feature
    extractor, a GNN-based graph embedding component, an affinity metric function
    and a permutation prediction component, as an end-to-end learnable framework.
    Specifically, GCNs are used to learn node-wise embeddings for intra-graph affinity,
    where a cross-graph aggregation step is introduced to aggregate features of nodes
    in the other graph for incorporating cross-graph affinity into the node embeddings.
    The node embeddings are then used for building an affinity matrix which contains
    the similarity scores at the node level between two graphs, and the affinity matrix
    is further used for the matching prediction. The cross-entropy loss is used to
    train the model end-to-end.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [jiang2019glmnet](#bib.bib60) 中，提出了一种用于图像匹配的图学习-匹配网络。首先使用 CNN 提取输入图像中所有特征点的特征描述符，然后基于这些特征构建图。接着，使用
    GCN 从图中学习节点嵌入，其中进行图内卷积和图间卷积。最终的匹配预测被公式化为嵌入空间中的节点对节点亲和度度量学习，并使用约束正则化损失和交叉熵损失进行度量学习和匹配预测。在
    [wang2019learning](#bib.bib121) 中，提出了另一种基于 GNN 的图匹配网络来解决图像匹配问题，该网络由 CNN 图像特征提取器、GNN
    基础的图嵌入组件、亲和度度量函数和置换预测组件组成，形成了一个端到端可学习的框架。具体而言，GCNs 被用于学习图内亲和度的节点级嵌入，其中引入了图间聚合步骤，以将另一个图中节点的特征聚合到节点嵌入中，从而将图间亲和度融入节点嵌入。然后使用节点嵌入构建亲和度矩阵，该矩阵包含两个图之间节点级的相似性分数，并进一步用于匹配预测。交叉熵损失用于端到端训练模型。
- en: 3.3 Deep Graph Kernels
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 深度图核
- en: Graph kernels have become a standard tool for capturing the similarity between
    graphs for tasks such as graph classification [vishwanathan2010graph](#bib.bib116)
    . Given a collection of graphs, possibly with node or edge attributes, the work
    in graph kernel aim to learn a kernel function that can capture the similarity
    between any two graphs. Traditional graph kernels, such as random walk kernels,
    subtree kernels, and shortest-path kernels have been widely used in the graph
    classification task [giannis2019](#bib.bib93) . Recently, deep graph kernel models
    have also emerged, which build kernels based on the graph representations learned
    via deep neural networks.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图核已经成为捕捉图之间相似性的标准工具，用于图分类等任务 [vishwanathan2010graph](#bib.bib116)。给定一组图，可能具有节点或边属性，图核工作的目标是学习一个能够捕捉任何两个图之间相似性的核函数。传统的图核，如随机游走核、子树核和最短路径核，已广泛应用于图分类任务
    [giannis2019](#bib.bib93)。最近，深度图核模型也出现了，这些模型基于通过深度神经网络学习的图表示来构建核。
- en: 3.3.1 Deep Graph Kernels
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 深度图核
- en: In [yanardag2015deep](#bib.bib130) , a Deep Graph Kernel approach is proposed.
    For a given set of graphs, each graph is decomposed into its sub-structures. Then
    the sub-structures are viewed as words and neural language models in the form
    of CBOW (continuous bag-of-words) and Skip-gram are used to learn latent representations
    of sub-structures from the graphs, where corpora are generated for the Shortest-path
    graph and Weisfeiler-Lehman kernels in order to measure the co-occurrence relationship
    between substructures. Finally, the kernel between two graphs is defined based
    on the similarity of the sub-structure space.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [yanardag2015deep](#bib.bib130) 中，提出了一种深度图核方法。对于给定的一组图，每个图被分解为其子结构。然后将这些子结构视为词语，并使用
    CBOW（连续词袋模型）和 Skip-gram 形式的神经语言模型来学习图中子结构的潜在表示，其中为最短路径图和 Weisfeiler-Lehman 核生成语料库，以测量子结构之间的共现关系。最后，基于子结构空间的相似性定义两个图之间的核。
- en: '![Refer to caption](img/1c4dc7138d4af5b18aec60709f8337dc.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1c4dc7138d4af5b18aec60709f8337dc.png)'
- en: 'Figure 5: The Graph Representation Learning in the Deep Divergence Graph Kernels
    [al2019ddgk](#bib.bib11) .'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：深度发散图核中的图表示学习 [al2019ddgk](#bib.bib11)。
- en: 3.3.2 Deep Divergence Graph Kernels
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 深度发散图核
- en: 'In [al2019ddgk](#bib.bib11) , a model called Deep Divergence Graph Kernels
    (DDGK) is introduced to learn kernel functions for graph pairs. Given two graphs
    $G_{1}$ and $G_{2}$, they aim to learn an embedding based kernel function $k()$
    as a similarity metric for graph pairs, defined as:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在[al2019ddgk](#bib.bib11)中，介绍了一种称为深度发散图核（DDGK）的模型，用于学习图对的核函数。给定两个图$G_{1}$和$G_{2}$，它们旨在学习一种基于嵌入的核函数$k()$作为图对的相似度度量，其定义为：
- en: '|  | $\displaystyle k(G_{1},G_{2})=\&#124;\Psi(G_{1})-\Psi(G_{2})\&#124;^{2}$
    |  | (13) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle k(G_{1},G_{2})=\&#124;\Psi(G_{1})-\Psi(G_{2})\&#124;^{2}$
    |  | (13) |'
- en: 'where $\Psi(G_{i})$ is a representation learned for $G_{i}$. This work proposes
    to learn graph representation by measuring the divergence of the target graph
    across a population of source graph encoders. Given a source graph collection
    $\{G_{1},G_{2},$ $\cdots,G_{n}\}$, a graph encoder is first trained to learn the
    structure of each graph in the source collection. Then, for a target graph $G_{T}$,
    the divergence of $G_{T}$ from each source graph is measured, after which the
    divergence scores are used to compose the vector representation of the target
    graph $G_{T}$. Fig. [5](#S3.F5 "Figure 5 ‣ 3.3.1 Deep Graph Kernels ‣ 3.3 Deep
    Graph Kernels ‣ 3 Taxonomy of Models ‣ Deep Graph Similarity Learning: A Survey")
    illustrates the above graph representation learning process. Specifically, the
    divergence score between a target graph $G_{T}=(V_{T},E_{T})$ and a source graph
    $G_{S}=(V_{S},E_{S})$ is computed as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$\Psi(G_{i})$是为$G_{i}$学习到的表示。本文提出通过测量目标图在一组源图编码器中的发散来学习图表示。给定一个源图集合$\{G_{1},G_{2},$
    $\cdots,G_{n}\}$，首先训练一个图编码器来学习源集合中每个图的结构。然后，对于目标图$G_{T}$，测量$G_{T}$与每个源图之间的发散，之后使用发散分数来组成目标图$G_{T}$的向量表示。图 [5](#S3.F5
    "Figure 5 ‣ 3.3.1 Deep Graph Kernels ‣ 3.3 Deep Graph Kernels ‣ 3 Taxonomy of
    Models ‣ Deep Graph Similarity Learning: A Survey") 说明了上述图表示学习过程。具体来说，目标图$G_{T}=(V_{T},E_{T})$与源图$G_{S}=(V_{S},E_{S})$之间的发散分数计算如下：'
- en: '|  | $\displaystyle\mathcal{D}^{\prime}(G_{T}\&#124;G_{S})=\sum_{v_{i}\in V_{T}}\sum_{\begin{subarray}{c}j\\
    {e_{ij}\in E_{T}}\end{subarray}}-log\text{Pr}(v_{j}&#124;v_{i},H_{S})$ |  | (14)
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{D}^{\prime}(G_{T}\&#124;G_{S})=\sum_{v_{i}\in V_{T}}\sum_{\begin{subarray}{c}j\\
    {e_{ij}\in E_{T}}\end{subarray}}-log\text{Pr}(v_{j}&#124;v_{i},H_{S})$ |  | (14)
    |'
- en: where $H_{S}$ is the encoder trained on graph $S$.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$H_{S}$是训练在图$S$上的编码器。
- en: 3.3.3 Graph Neural Tangent Kernel
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 图神经切线核
- en: In [du2019graph](#bib.bib39) , a Graph Neural Tangent Kernel (GNTK) is proposed
    for fusing GNNs with the neural tangent kernel, which is originally formulated
    for fully-connected neural networks in [jacot2018neural](#bib.bib59) and later
    introduced to CNNs in [arora2019exact](#bib.bib12) . Given a pair of graphs $<G,G^{\prime}>$,
    they first apply GNNs on the graphs. Let $f(\theta,G)\in\mathbb{R}$ be the output
    of the GNN under parameters $\theta\in\mathbb{R}^{m}$ on input Graph $G$, where
    $m$ is the dimension of the parameters. To get the corresponding GNTK value, they
    calculate the expected value of
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在[du2019graph](#bib.bib39)中，提出了一种图神经切线核（GNTK），用于将GNN与神经切线核融合，后者最初为[Jacot et
    al.](#bib.bib59)中全连接神经网络制定，并在[arora2019exact](#bib.bib12)中引入到CNN中。给定一对图$<G,G^{\prime}>$，它们首先在图上应用GNN。设$f(\theta,G)\in\mathbb{R}$是GNN在参数$\theta\in\mathbb{R}^{m}$下对输入图$G$的输出，其中$m$是参数的维度。为了得到相应的GNTK值，它们计算期望值
- en: '|  | $\displaystyle\Bigg{\langle}\frac{\partial f(\theta,G)}{\partial\theta},\frac{\partial
    f(\theta,G^{\prime})}{\partial\theta}\bigg{\rangle}$ |  | (15) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Bigg{\langle}\frac{\partial f(\theta,G)}{\partial\theta},\frac{\partial
    f(\theta,G^{\prime})}{\partial\theta}\bigg{\rangle}$ |  | (15) |'
- en: in the limit that $m\rightarrow\infty$ and $\theta$ are all Gaussian random
    variables.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当$m\rightarrow\infty$且$\theta$都是高斯随机变量时。
- en: Meanwhile, there are also some deep graph kernels proposed for the node representation
    learning on graphs for node classification and node similarity learning. For instance,
    in [tian2019rethinking](#bib.bib110) , a learnable kernel-based framework is proposed
    for node classification, where the kernel function is decoupled into a feature
    mapping function and a base kernel. An encoder-decoder function is introduced
    to project each node into the embedding space and reconstructs pairwise similarity
    measurements from the node embeddings. Since we focus on the similarity learning
    between graphs in this survey, we will not discuss this work further.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，也提出了一些深度图核用于图中的节点表示学习，尤其是节点分类和节点相似性学习。例如，在[tian2019rethinking](#bib.bib110)中，提出了一种基于可学习核的框架用于节点分类，其中核函数被解耦为特征映射函数和基本核。引入了一个编码器-解码器函数，将每个节点投影到嵌入空间，并从节点嵌入中重建成对相似性测量。由于我们在本调查中专注于图之间的相似性学习，我们将不再进一步讨论这项工作。
- en: 4 Datasets and Evaluation
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据集与评估
- en: 'Table 3: Summary of Benchmark Datasets that are Frequently Used in Deep Graph
    Similarity Learning.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 深度图相似性学习中常用的基准数据集总结。'
- en: '| Graph Type | Datasets | Source |  Number of Graphs  |  Number of Classes  |  Avg.
    Number of Nodes  |  Avg. Number of Edges  | References |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 图类型 | 数据集 | 来源 | 图的数量 | 类别数量 | 平均节点数 | 平均边数 | 参考文献 |'
- en: '| Social Networks | COLLAB | [yanardag2015deep](#bib.bib130) | 5000 | 3 | 74.49
    | 2457.78 | [yanardag2015deep](#bib.bib130) ; [tixier2019graph](#bib.bib111) ;
    [wu2018dgcnn](#bib.bib125) ; [anonymous2019_inductive](#bib.bib120) ; [du2019graph](#bib.bib39)
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 社交网络 | COLLAB | [yanardag2015deep](#bib.bib130) | 5000 | 3 | 74.49 | 2457.78
    | [yanardag2015deep](#bib.bib130) ; [tixier2019graph](#bib.bib111) ; [wu2018dgcnn](#bib.bib125)
    ; [anonymous2019_inductive](#bib.bib120) ; [du2019graph](#bib.bib39) |'
- en: '| IMDB-BINARY | [yanardag2015deep](#bib.bib130) | 1000 | 2 | 19.77 | 96.53
    | [yanardag2015deep](#bib.bib130) ; [atamna2019spi](#bib.bib13) ; [anonymous2019_inductive](#bib.bib120)
    ; [du2019graph](#bib.bib39) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-BINARY | [yanardag2015deep](#bib.bib130) | 1000 | 2 | 19.77 | 96.53
    | [yanardag2015deep](#bib.bib130) ; [atamna2019spi](#bib.bib13) ; [anonymous2019_inductive](#bib.bib120)
    ; [du2019graph](#bib.bib39) |'
- en: '| IMDB-MULTI | [yanardag2015deep](#bib.bib130) | 1500 | 3 | 13.00 | 65.94 |
    [yanardag2015deep](#bib.bib130) ; [atamna2019spi](#bib.bib13) ; [anonymous2019_inductive](#bib.bib120)
    ; [bai2018convolutional](#bib.bib16) ; [bai2019simgnn](#bib.bib15) ; [du2019graph](#bib.bib39)
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-MULTI | [yanardag2015deep](#bib.bib130) | 1500 | 3 | 13.00 | 65.94 |
    [yanardag2015deep](#bib.bib130) ; [atamna2019spi](#bib.bib13) ; [anonymous2019_inductive](#bib.bib120)
    ; [bai2018convolutional](#bib.bib16) ; [bai2019simgnn](#bib.bib15) ; [du2019graph](#bib.bib39)
    |'
- en: '| REDDIT-BINARY | [yanardag2015deep](#bib.bib130) | 2000 | 2 | 429.63 | 497.75
    | [yanardag2015deep](#bib.bib130) ; [tixier2019graph](#bib.bib111) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| REDDIT-BINARY | [yanardag2015deep](#bib.bib130) | 2000 | 2 | 429.63 | 497.75
    | [yanardag2015deep](#bib.bib130) ; [tixier2019graph](#bib.bib111) |'
- en: '| REDDIT-MULTI-5K | [yanardag2015deep](#bib.bib130) | 4999 | 5 | 508.52 | 594.87
    | [yanardag2015deep](#bib.bib130) ; [tixier2019graph](#bib.bib111) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| REDDIT-MULTI-5K | [yanardag2015deep](#bib.bib130) | 4999 | 5 | 508.52 | 594.87
    | [yanardag2015deep](#bib.bib130) ; [tixier2019graph](#bib.bib111) |'
- en: '| REDDIT-MULTI-12K | [yanardag2015deep](#bib.bib130) | 11929 | 11 | 391.41
    | 456.89 | [yanardag2015deep](#bib.bib130) ; [tixier2019graph](#bib.bib111) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| REDDIT-MULTI-12K | [yanardag2015deep](#bib.bib130) | 11929 | 11 | 391.41
    | 456.89 | [yanardag2015deep](#bib.bib130) ; [tixier2019graph](#bib.bib111) |'
- en: '| Bioinformatics | D&D | [dobson2003distinguishing](#bib.bib37) | 1178 | 2
    | 284.32 | 715.66 | [al2019ddgk](#bib.bib11) ; [nikolentzos2017matching](#bib.bib92)
    ; [wu2018dgcnn](#bib.bib125) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 生物信息学 | D&D | [dobson2003distinguishing](#bib.bib37) | 1178 | 2 | 284.32
    | 715.66 | [al2019ddgk](#bib.bib11) ; [nikolentzos2017matching](#bib.bib92) ;
    [wu2018dgcnn](#bib.bib125) |'
- en: '| ENZYMES | [borgwardt2005protein](#bib.bib23) | 600 | 6 | 32.63 | 62.14 |
    [atamna2019spi](#bib.bib13) ; [nikolentzos2017matching](#bib.bib92) ; [yanardag2015deep](#bib.bib130)
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 酶 | [borgwardt2005protein](#bib.bib23) | 600 | 6 | 32.63 | 62.14 | [atamna2019spi](#bib.bib13)
    ; [nikolentzos2017matching](#bib.bib92) ; [yanardag2015deep](#bib.bib130) |'
- en: '| PROTEINS | [borgwardt2005protein](#bib.bib23) | 1113 | 2 | 39.06 | 72.82
    | [du2019graph](#bib.bib39) ; [narayanan2017graph2vec](#bib.bib88) ; [nikolentzos2017matching](#bib.bib92)
    ; [yanardag2015deep](#bib.bib130) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 蛋白质 | [borgwardt2005protein](#bib.bib23) | 1113 | 2 | 39.06 | 72.82 | [du2019graph](#bib.bib39)
    ; [narayanan2017graph2vec](#bib.bib88) ; [nikolentzos2017matching](#bib.bib92)
    ; [yanardag2015deep](#bib.bib130) |'
- en: '| Chemoinformatics | AIDS | [riesen2008iam](#bib.bib95) | 2000 | 2 | 15.69
    | 16.20 | [bai2018convolutional](#bib.bib16) ; [bai2019simgnn](#bib.bib15) ; [anonymous2019_matching](#bib.bib41)
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 化学信息学 | 艾滋病 | [riesen2008iam](#bib.bib95) | 2000 | 2 | 15.69 | 16.20 | [bai2018convolutional](#bib.bib16)
    ; [bai2019simgnn](#bib.bib15) ; [anonymous2019_matching](#bib.bib41) |'
- en: '| MUTAG | [debnath1991structure](#bib.bib33) | 188 | 2 | 17.93 | 19.79 | [al2019ddgk](#bib.bib11)
    ; [du2019graph](#bib.bib39) ; [atamna2019spi](#bib.bib13) ; [narayanan2017graph2vec](#bib.bib88)
    ; [nikolentzos2017matching](#bib.bib92) ; [anonymous2019_inductive](#bib.bib120)
    ; [yanardag2015deep](#bib.bib130) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| MUTAG | [debnath1991structure](#bib.bib33) | 188 | 2 | 17.93 | 19.79 | [al2019ddgk](#bib.bib11)
    ; [du2019graph](#bib.bib39) ; [atamna2019spi](#bib.bib13) ; [narayanan2017graph2vec](#bib.bib88)
    ; [nikolentzos2017matching](#bib.bib92) ; [anonymous2019_inductive](#bib.bib120)
    ; [yanardag2015deep](#bib.bib130) |'
- en: '| NCI1 | [wale2008comparison](#bib.bib118) | 4110 | 2 | 29.87 | 32.30 | [al2019ddgk](#bib.bib11)
    ; [atamna2019spi](#bib.bib13) ; [du2019graph](#bib.bib39) ; [nikolentzos2017matching](#bib.bib92)
    ; [anonymous2019_inductive](#bib.bib120) ; [yanardag2015deep](#bib.bib130) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| NCI1 | [wale2008comparison](#bib.bib118) | 4110 | 2 | 29.87 | 32.30 | [al2019ddgk](#bib.bib11)
    ; [atamna2019spi](#bib.bib13) ; [du2019graph](#bib.bib39) ; [nikolentzos2017matching](#bib.bib92)
    ; [anonymous2019_inductive](#bib.bib120) ; [yanardag2015deep](#bib.bib130) |'
- en: '| NCI109 | [wale2008comparison](#bib.bib118) | 4127 | 2 | 29.68 | 32.13 | [narayanan2017graph2vec](#bib.bib88)
    ; [nikolentzos2017matching](#bib.bib92) ; [yanardag2015deep](#bib.bib130) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| NCI109 | [wale2008comparison](#bib.bib118) | 4127 | 2 | 29.68 | 32.13 | [narayanan2017graph2vec](#bib.bib88)
    ; [nikolentzos2017matching](#bib.bib92) ; [yanardag2015deep](#bib.bib130) |'
- en: '| PTC_MR | [helma2001predictive](#bib.bib54) | 344 | 2 | 14.29 | 14.69 | [narayanan2017graph2vec](#bib.bib88)
    ; [nikolentzos2017matching](#bib.bib92) ; [yanardag2015deep](#bib.bib130) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| PTC_MR | [helma2001predictive](#bib.bib54) | 344 | 2 | 14.29 | 14.69 | [narayanan2017graph2vec](#bib.bib88)
    ; [nikolentzos2017matching](#bib.bib92) ; [yanardag2015deep](#bib.bib130) |'
- en: '| Brain Networks | ABIDE | [di2014autism](#bib.bib35) | 871 | 2 | 110 | - |
    [ktena2018metric](#bib.bib67) ; [ma2019similarity](#bib.bib78) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 大脑网络 | ABIDE | [di2014autism](#bib.bib35) | 871 | 2 | 110 | - | [ktena2018metric](#bib.bib67)
    ; [ma2019similarity](#bib.bib78) |'
- en: '| UK Biobank | [biobank2014uk](#bib.bib20) | 2500 | 2 | 55 | - | [ktena2018metric](#bib.bib67)
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 英国生物银行 | [biobank2014uk](#bib.bib20) | 2500 | 2 | 55 | - | [ktena2018metric](#bib.bib67)
    |'
- en: '| HCP | [van2012human](#bib.bib115) | 1200 | 2 | 360 | - | [ma2019similarity](#bib.bib78)
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| HCP | [van2012human](#bib.bib115) | 1200 | 2 | 360 | - | [ma2019similarity](#bib.bib78)
    |'
- en: '| Image Graphs | COIL-DEL | [riesen2008iam](#bib.bib95) | 3900 | 100 | 21.54
    | 54.24 | [li2019graph](#bib.bib73) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 图像图 | COIL-DEL | [riesen2008iam](#bib.bib95) | 3900 | 100 | 21.54 | 54.24
    | [li2019graph](#bib.bib73) |'
- en: In this section, we summarize the characteristics of the datasets that are frequently
    used in deep graph similarity learning methods and the experimental evaluation
    adopted by these methods.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了在深度图相似性学习方法中经常使用的数据集的特征以及这些方法所采用的实验评估。
- en: 4.1 Datasets
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: 'Graph data from various domains have been used to evaluate graph similarity
    learning methods [rossi2015network](#bib.bib97) , for example, protein-protein
    graphs from bioinformatics, chemical compound graphs from chemoinformatics, and
    brain networks from neuroscience, etc. We summarize the benchmark datasets that
    are frequently used in deep graph similarity learning methods in Table [3](#S4.T3
    "Table 3 ‣ 4 Datasets and Evaluation ‣ Deep Graph Similarity Learning: A Survey").'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '来自不同领域的图数据已被用于评估图相似性学习方法 [rossi2015network](#bib.bib97)，例如，生物信息学中的蛋白质-蛋白质图、化学信息学中的化学化合物图以及神经科学中的大脑网络等。我们在表格
    [3](#S4.T3 "Table 3 ‣ 4 Datasets and Evaluation ‣ Deep Graph Similarity Learning:
    A Survey") 中总结了在深度图相似性学习方法中经常使用的基准数据集。'
- en: In addition to these datasets, synthetic graph datasets or other domain-specific
    datasets are also widely used in some graph similarity learning works. For example,
    in [li2019graph](#bib.bib73) and [anonymous2019_matching](#bib.bib41) , control
    flow graphs of binary functions are generated and used to evaluate graph matching
    networks for binary code similarity search. In [ijcai2019-522](#bib.bib122) ,
    attacks are conducted on testing machines to generate malware data, which are
    then merged with normal data to evaluate the Siamese GNN model for malware detection.
    In [jiang2019glmnet](#bib.bib60) , images are collected from multiple categories
    and keypoints are annotated in the images to evaluate the proposed model for graph
    matching.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些数据集，合成图数据集或其他领域特定的数据集在一些图相似性学习工作中也被广泛使用。例如，在 [li2019graph](#bib.bib73) 和
    [anonymous2019_matching](#bib.bib41) 中，生成了二进制函数的控制流图，并用来评估图匹配网络在二进制代码相似性搜索中的表现。在
    [ijcai2019-522](#bib.bib122) 中，对测试机器进行攻击以生成恶意软件数据，然后将其与正常数据合并，以评估Siamese GNN模型在恶意软件检测中的表现。在
    [jiang2019glmnet](#bib.bib60) 中，收集了多个类别的图像，并在图像中标注关键点，以评估所提出的图匹配模型。
- en: 4.2 Evaluation
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估
- en: During evaluation, most GSL methods take pairs or triplets of graphs as input
    during training with various objective functions used for various graph similarity
    tasks. The existing evaluation tasks mainly include pair classification  [xu2017neural](#bib.bib128)
    ; [ktena2018metric](#bib.bib67) ; [ma2019similarity](#bib.bib78) ; [li2019graph](#bib.bib73)
    ; [anonymous2019_matching](#bib.bib41) , graph classification [tixier2019graph](#bib.bib111)
    ; [nikolentzos2017matching](#bib.bib92) ; [narayanan2017graph2vec](#bib.bib88)
    ; [atamna2019spi](#bib.bib13) ; [wu2018dgcnn](#bib.bib125) ; [anonymous2019_inductive](#bib.bib120)
    ; [liu2019n](#bib.bib77) ; [yanardag2015deep](#bib.bib130) ; [al2019ddgk](#bib.bib11)
    ; [du2019graph](#bib.bib39) , graph clustering [anonymous2019_inductive](#bib.bib120)
    , graph distance prediction [bai2018convolutional](#bib.bib16) ; [bai2019simgnn](#bib.bib15)
    ; [anonymous2019_matching](#bib.bib41) , and graph similarity search [ijcai2019-522](#bib.bib122)
    . Classification AUC (i.e., Area Under the ROC Curve) or accuracy are used as
    the most popular metric for the evaluation of graph-pair classification or graph
    classification task [ma2019similarity](#bib.bib78) ; [li2019graph](#bib.bib73)
    . Mean squared error (MSE) is used as evaluation metric for the regression task
    in graph distance prediction [bai2018convolutional](#bib.bib16) ; [bai2019simgnn](#bib.bib15)
    .
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估过程中，大多数 GSL 方法在训练时使用图的对或三元组作为输入，并且对于各种图相似性任务使用不同的目标函数。现有的评估任务主要包括对分类 [xu2017neural](#bib.bib128)
    ; [ktena2018metric](#bib.bib67) ; [ma2019similarity](#bib.bib78) ; [li2019graph](#bib.bib73)
    ; [anonymous2019_matching](#bib.bib41) ，图分类 [tixier2019graph](#bib.bib111) ; [nikolentzos2017matching](#bib.bib92)
    ; [narayanan2017graph2vec](#bib.bib88) ; [atamna2019spi](#bib.bib13) ; [wu2018dgcnn](#bib.bib125)
    ; [anonymous2019_inductive](#bib.bib120) ; [liu2019n](#bib.bib77) ; [yanardag2015deep](#bib.bib130)
    ; [al2019ddgk](#bib.bib11) ; [du2019graph](#bib.bib39) ，图聚类 [anonymous2019_inductive](#bib.bib120)
    ，图距离预测 [bai2018convolutional](#bib.bib16) ; [bai2019simgnn](#bib.bib15) ; [anonymous2019_matching](#bib.bib41)
    ，以及图相似性搜索 [ijcai2019-522](#bib.bib122) 。分类 AUC（即 ROC 曲线下面积）或准确率是评估图对分类或图分类任务的最常用指标
    [ma2019similarity](#bib.bib78) ; [li2019graph](#bib.bib73) 。均方误差（MSE）则被用作图距离预测回归任务的评估指标 [bai2018convolutional](#bib.bib16)
    ; [bai2019simgnn](#bib.bib15) 。
- en: According to the evaluation results reported in the above works, the deep graph
    similarity learning methods tend to outperform the traditional methods. For example,
    [al2019ddgk](#bib.bib11) shows that the deep divergence graph kernel approach
    achieves higher classification accuracy scores compared to traditional graph kernels
    such as the shortest-path kernel [borgwardt2005shortest](#bib.bib21) and Weisfeiler-Lehman
    kernel [kriege2016valid](#bib.bib66) in most cases for the graph classification
    task. Meanwhile, among the deep methods, methods that allow for cross-graph feature
    interaction tend to achieve a better performance compared to the factorized methods
    that relies only on single graph features. For instance, the experimental evaluations
    in [li2019graph](#bib.bib73) and [anonymous2019_matching](#bib.bib41) have demonstrated
    that the GNN-based graph matching networks have a superior performance than the
    Siamese GNNs in pair classification and graph edit distance prediction tasks.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述工作的评估结果，深度图相似性学习方法通常优于传统方法。例如，[al2019ddgk](#bib.bib11) 表明，深度发散图核方法在大多数情况下在图分类任务中相较于传统图核（如最短路径核
    [borgwardt2005shortest](#bib.bib21) 和 Weisfeiler-Lehman 核 [kriege2016valid](#bib.bib66)）实现了更高的分类准确率。同时，在深度方法中，允许跨图特征交互的方法往往比仅依赖单一图特征的因子分解方法表现更好。例如，[li2019graph](#bib.bib73)
    和 [anonymous2019_matching](#bib.bib41) 中的实验评估表明，基于 GNN 的图匹配网络在对分类和图编辑距离预测任务中优于
    Siamese GNNs。
- en: 'The efficiency of different methods are also analyzed and evaluated in some
    of these works. In [bai2019simgnn](#bib.bib15) , some evaluations have been done
    for comparing the efficiency of the GNN based graph similarity learning approach
    SimGNN with traditional GED approximation methods including A*-Beamsearch [neuhaus2006fast](#bib.bib89)
    , Hungarian [riesen2009approximate](#bib.bib96) and VJ [fankhauser2011speeding](#bib.bib40)
    , where the core operation for GED approximation may take polynomial or sub-exponential
    to the number of nodes in the graphs. For the GNN based model like SimGNN, to
    compute similarity score for pairs of graphs, the time complexity mainly involves
    two parts: (1) the node-level and graph-level embedding computation stages, where
    the time complexity is $O(|E|)$, and $|E|$ is the number of edges of the graph
    [kipf2016semi](#bib.bib64) ; and (2) the similarity score computation stage, where
    the time complexity is $O(D^{2}K)$ ($D$ is the dimension of the graph-level embedding,
    and $K$ is the feature map dimension used in the graph-graph interaction stage)
    for the strategy of using graph-level embedding interaction, and the time complexity
    is $O(DN^{2})$ ($N$ is the number of nodes in the larger graph). The experimental
    evaluations in [bai2019simgnn](#bib.bib15) show that the GNN based models consistently
    achieve the best results in efficiency and effectiveness for the pairwise GED
    computation [bai2019simgnn](#bib.bib15) on multiple graph datasets, demonstrating
    the benefit of using these deep models for the similarity learning tasks.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些工作中，还分析和评估了不同方法的效率。在 [bai2019simgnn](#bib.bib15) 中，对基于 GNN 的图相似性学习方法 SimGNN
    与传统的 GED 近似方法（包括 A*-Beamsearch [neuhaus2006fast](#bib.bib89) 、Hungarian [riesen2009approximate](#bib.bib96)
    和 VJ [fankhauser2011speeding](#bib.bib40) ）的效率进行了比较评估，其中 GED 近似的核心操作可能需要多项式时间或次指数时间来处理图中的节点数量。对于像
    SimGNN 这样的 GNN 基模型，计算图对之间的相似性分数时，时间复杂度主要涉及两个部分：（1）节点级和图级嵌入计算阶段，其中时间复杂度为 $O(|E|)$，$|E|$
    是图中的边数 [kipf2016semi](#bib.bib64) ；（2）相似性分数计算阶段，对于使用图级嵌入交互的策略，时间复杂度为 $O(D^{2}K)$（$D$
    是图级嵌入的维度，$K$ 是图-图交互阶段使用的特征图维度），而对于较大图中的节点数量 $N$，时间复杂度为 $O(DN^{2})$。在 [bai2019simgnn](#bib.bib15)
    中的实验评估表明，基于 GNN 的模型在多个图数据集上的成对 GED 计算中一致地取得了最佳的效率和效果，展示了使用这些深度模型进行相似性学习任务的好处。
- en: 5 Applications
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 应用
- en: Graph similarity learning is a fundamental problem in domains where data are
    represented as graph structures, and it has various applications in the real world.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图相似性学习是数据以图结构表示的领域中的一个基础问题，并且在现实世界中具有各种应用。
- en: 5.1 Computational Chemistry and Biology
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 计算化学与生物学
- en: An important application of graph similarity learning in the chemistry and biology
    domain is to learn the chemical similarity, which aims to learn the similarity
    of chemical elements, molecules or chemical compounds with respect to their effect
    on reaction partners in inorganic or biological settings [brown2009chemoinformatics](#bib.bib24)
    . An example is the compounds query for in-silico drug screening, where searching
    for similar compounds in a database is the key process.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图相似性学习在化学和生物领域的一个重要应用是学习化学相似性，其目的是学习化学元素、分子或化学化合物在无机或生物环境中对反应伙伴的影响的相似性 [brown2009chemoinformatics](#bib.bib24)
    。一个例子是用于计算机药物筛选的化合物查询，其中在数据库中搜索相似化合物是关键过程。
- en: In the literature of graph similarity learning, quite a number of models have
    been proposed and applied to similarity learning for chemical compounds or molecules.
    Among these work, the traditional models mainly employ sub-graph based search
    strategies or graph kernels to solve the problem [zheng2013graph](#bib.bib138)
    ; [zeng2009comparing](#bib.bib135) ; [swamidass2005kernels](#bib.bib108) ; [mahe2009graph](#bib.bib82)
    . However, these methods tend to have high computational complexity and strongly
    rely on the sub-graph or kernels defined, making it difficult to use them in real
    applications. Recently, a deep graph similarity learning model SimGNN is proposed
    in [bai2019simgnn](#bib.bib15) which also aims to learn similarity for chemical
    compounds as one of the tasks. Instead of using sub-graphs or other explicit features,
    the model adopts GCNs to learn node-level embeddings, which are fed into an attention
    module after multiple layers of GCNs to generate the graph-level embeddings. Then
    a neural tensor network (NTN) [socher2013reasoning](#bib.bib107) is used to model
    the relation between two graph-level embeddings, and the output of the NTN is
    used together with the pairwise node embedding comparison output in the fully
    connected layers for predicting the graph edit distance between the two graphs.
    This work has shown that the proposed deep learning model outperforms the traditional
    methods for graph edit distance computation in prediction accuracy and with much
    less running time, which indicates the promising application of the deep graph
    similarity learning models in the chemo-informatics and bio-informatics.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在图相似性学习的文献中，已经提出并应用了许多模型用于化学化合物或分子的相似性学习。在这些工作中，传统模型主要采用基于子图的搜索策略或图核来解决问题 [zheng2013graph](#bib.bib138)
    ; [zeng2009comparing](#bib.bib135) ; [swamidass2005kernels](#bib.bib108) ; [mahe2009graph](#bib.bib82)
    。然而，这些方法往往具有较高的计算复杂度，并且强烈依赖于定义的子图或核，使其在实际应用中难以使用。最近，在 [bai2019simgnn](#bib.bib15)
    中提出了一种深度图相似性学习模型 SimGNN，该模型也旨在学习化学化合物的相似性作为任务之一。该模型没有使用子图或其他显式特征，而是采用了GCNs来学习节点级嵌入，这些嵌入经过多层GCNs后输入到注意力模块中生成图级嵌入。然后，使用神经张量网络
    (NTN) [socher2013reasoning](#bib.bib107) 来建模两个图级嵌入之间的关系，并将NTN的输出与全连接层中的成对节点嵌入比较输出一起用于预测两个图之间的图编辑距离。该研究表明，提出的深度学习模型在预测精度和运行时间方面均优于传统方法，这表明深度图相似性学习模型在化学信息学和生物信息学中的应用前景广阔。
- en: 5.2 Neuroscience
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 神经科学
- en: Many neuroscience studies have shown that structural and functional connectivity
    of the human brain reflects the brain activity patterns that could be indicators
    of the brain health status or cognitive ability level [badhwar2017resting](#bib.bib14)
    ; [ma2017multi](#bib.bib80) ; [ma2017multib](#bib.bib81) . For example, the functional
    brain connectivity networks derived from fMRI neuroimaging data can reflect the
    functional activity across different brain regions, and people with brain disorder
    like Alzheimer’s disease or bipolar disorder tend to have functional activity
    patterns that differ from those of healthy people [badhwar2017resting](#bib.bib14)
    ; [syan2018resting](#bib.bib109) ; [ma2016multi](#bib.bib79) . To investigate
    the difference in brain connectivity patterns for these neuroscience problems,
    researchers have started to study the similarity of brain networks among multiple
    subjects with graph similarity learning methods [lee2020deep](#bib.bib69) ; [ktena2018metric](#bib.bib67)
    ; [ma2019similarity](#bib.bib78) .
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 许多神经科学研究表明，人脑的结构和功能连通性反映了大脑活动模式，这些模式可能是大脑健康状态或认知能力水平的指标 [badhwar2017resting](#bib.bib14)
    ; [ma2017multi](#bib.bib80) ; [ma2017multib](#bib.bib81) 。例如，从 fMRI 神经影像数据中获得的功能性大脑连通网络可以反映不同脑区之间的功能活动，患有阿尔茨海默病或躁郁症等大脑疾病的人往往有不同于健康人的功能活动模式
    [badhwar2017resting](#bib.bib14) ; [syan2018resting](#bib.bib109) ; [ma2016multi](#bib.bib79)
    。为了探究这些神经科学问题中的大脑连通模式的差异，研究人员已开始利用图相似性学习方法研究多个受试者之间的大脑网络相似性 [lee2020deep](#bib.bib69)
    ; [ktena2018metric](#bib.bib67) ; [ma2019similarity](#bib.bib78) 。
- en: The organization of functional brain networks is complicated and usually constrained
    by various factors, such as the underlying brain anatomical network, which plays
    an important role in shaping the activity across the brain. These constraints
    make it a challenging task to characterize the structure and organization of brain
    networks while performing similarity learning on them. Recent work in [ktena2018metric](#bib.bib67)
    , [ma2019similarity](#bib.bib78) and [liu2019community](#bib.bib76) have shown
    that the deep graph models based on graph convolutional networks have a superior
    ability to capture brain connectivity features for the similarity analysis compared
    to the traditional graph embedding based approaches. In particular, [ma2019similarity](#bib.bib78)
    proposes a higher-order Siamese GCN framework that leverages higher-order connectivity
    structure of functional brain networks for the similarity learning of brain networks.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 功能脑网络的组织复杂，通常受到多种因素的制约，例如底层脑解剖网络在塑造脑部活动中扮演重要角色。这些制约使得在进行相似性学习时，表征脑网络的结构和组织成为一项具有挑战性的任务。最近的研究表明，基于图卷积网络的深度图模型在捕捉脑连接特征方面，比传统的图嵌入方法具有更好的能力[ktina2018metric](#bib.bib67)、[ma2019similarity](#bib.bib78)和[liu2019community](#bib.bib76)。特别是，[ma2019similarity](#bib.bib78)提出了一种高阶Siamese
    GCN框架，利用功能脑网络的高阶连接结构进行脑网络的相似性学习。
- en: In view of the work introduced above and the trending research problems in the
    field of neuroscience, we believe that deep graph similarity learning will benefit
    the clinical investigation of many brain diseases and other neuroscience applications.
    Promising research directions include, but are not limited to, deep similarity
    learning on resting-state or task-related fMRI brain networks for multi-subject
    analysis with respect to brain health status or cognitive abilities, deep similarity
    learning on the temporal or multi-task fMRI brain networks of individual subjects
    for within-subject contrastive analysis over time or across tasks for neurological
    disorder detection. Some example fMRI brain network datasets that can be used
    for such analysis have been introduced in Table 3.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于上述工作的介绍以及神经科学领域中的研究热点问题，我们相信深度图相似性学习将有利于许多脑部疾病的临床研究和其他神经科学应用。值得关注的研究方向包括但不限于，在静息态或任务相关fMRI脑网络上进行深度相似性学习，以多主体分析脑健康状况或认知能力；在单个受试者的时间或多任务fMRI脑网络上进行深度相似性学习，以实现对神经系统疾病检测的时间内或任务间的对比分析。一些可以用于这种分析的fMRI脑网络数据集已在表3中介绍。
- en: 5.3 Computer Security
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 计算机安全
- en: In the field of computer security, graph similarity has also been studied for
    various application scenarios, such as the hardware security problem [marc2019](#bib.bib84)
    , the malware indexing problem based on function-call graphs [hu2009large](#bib.bib57)
    , and the binary function similarity search for identifying vulnerable functions
    [li2019graph](#bib.bib73) .
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机安全领域，图相似性也已被研究用于各种应用场景，例如硬件安全问题[marc2019](#bib.bib84)、基于函数调用图的恶意软件索引问题[hu2009large](#bib.bib57)以及用于识别易受攻击函数的二进制函数相似性搜索[li2019graph](#bib.bib73)。
- en: In [marc2019](#bib.bib84) , a graph similarity heuristic is proposed based on
    spectral analysis of adjacency matrices for the hardware security problem, where
    evaluations are done for three tasks, including gate-level netlist reverse engineering,
    Trojan detection, and obfuscation assessment. The proposed method outperforms
    the graph edit distance approximation algorithm proposed in [hu2009large](#bib.bib57)
    and the neighbor matching approach [vujovsevic2013software](#bib.bib117) , which
    matches neighboring vertices based on graph topology. [li2019graph](#bib.bib73)
    is the work that introduced GNN-based deep graph similarity learning models to
    the security field to solve the binary function similarity search problem. Compared
    to previous models, the proposed deep model computes similarity scores jointly
    on pairs of graphs rather than first independently mapping each graph to a vector,
    and the node representation update process uses an attention-based module which
    considers both within-graph and cross-graph information. Empirical evaluations
    demonstrate the superior performance of the proposed deep graph matching networks
    compared to the Google’s open source function similarity search tool [functionsim](#bib.bib1)
    , the basic GNN models, and the Siamese GNNs.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在[marc2019](#bib.bib84)中，提出了一种基于邻接矩阵谱分析的图相似性启发式方法，用于硬件安全问题，对三个任务进行评估，包括门级网表逆向工程、木马检测和混淆评估。所提方法优于[hu2009large](#bib.bib57)中提出的图编辑距离近似算法和邻居匹配方法[vujovsevic2013software](#bib.bib117)，该方法基于图拓扑匹配邻近顶点。[li2019graph](#bib.bib73)的工作将基于GNN的深度图相似性学习模型引入安全领域，以解决二进制函数相似性搜索问题。与之前的模型相比，所提深度模型在图对上联合计算相似性分数，而不是先独立地将每个图映射到一个向量，节点表示更新过程使用基于注意力的模块，考虑了图内和跨图的信息。实证评估表明，所提的深度图匹配网络在性能上优于谷歌开源的函数相似性搜索工具[functionsim](#bib.bib1)、基本GNN模型和Siamese
    GNNs。
- en: 5.4 Computer Vision
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 计算机视觉
- en: Graph similarity learning has also been explored for applications in computer
    vision. In [wu2014human](#bib.bib124) , context-dependent graph kernels are proposed
    to measure the similarity between graphs for human action recognition in video
    sequences. Two directed and attributed graphs are constructed to describe the
    local features with intra-frame relationships and inter-frame relationships, respectively.
    The graphs are decomposed into a number of primary walk groups with different
    walk lengths, and a generalized multiple kernel learning algorithm is applied
    to combine all the context-dependent graph kernels, which further facilitates
    human action classification. In [guo2018neural](#bib.bib53) , a deep model called
    Neural Graph Matching Network is first introduced for the 3D action recognition
    problem in the few-shot learning setting. Interaction graphs are constructed from
    the 3D scenes, where the nodes represent physical entities in the scene and edges
    represent interactions between the entities. The proposed NGM Networks jointly
    learn a graph generator and a graph matching metric function in an end-to-end
    fashion to directly optimize the few-shot learning objective. It has been shown
    to significantly improve the few-shot 3D action recognition over the holistic
    baselines.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图相似性学习也被探索用于计算机视觉应用。在[wu2014human](#bib.bib124)中，提出了基于上下文的图核来测量图之间的相似性，以用于视频序列中的人类动作识别。构建了两个有向和带属性的图，分别描述了局部特征的帧内关系和帧间关系。图被分解成多个具有不同步长的主要步态组，并应用了一种广义多核学习算法来结合所有上下文相关的图核，这进一步促进了人类动作分类。在[guo2018neural](#bib.bib53)中，首次引入了一种名为神经图匹配网络的深度模型，用于少样本学习环境中的3D动作识别。从3D场景中构建交互图，其中节点代表场景中的物理实体，边代表实体之间的交互。所提的NGM网络以端到端的方式联合学习图生成器和图匹配度量函数，以直接优化少样本学习目标。已证明该方法在少样本3D动作识别方面显著优于整体基线。
- en: Another emerging application of graph similarity learning in computer vision
    is the image matching problem, where the goal is to find consistent correspondences
    between the sets of features in two images. As introduced at the end of Section
    3.2, recently some deep graph matching networks have been developed for the image
    matching task [jiang2019glmnet](#bib.bib60) ; [wang2019learning](#bib.bib121)
    , where images are first converted to graphs and the image matching problem is
    then solved as a graph matching problem. In the graph converted from an image,
    the nodes represent the unary descriptors of annotated feature points in images,
    and edges encode the pairwise relationships among different feature points in
    that image. Based on the new graph representation, the feature matching can be
    reformulated as graph matching problem. However, it is worth noting that, this
    graph matching is actually the graph node matching, as the goal is to match the
    nodes between graphs instead of two entire graphs. Therefore, the graph based
    image matching problem is a special case or a sub-problem of the general graph
    matching problem.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉中图相似性学习的另一个新兴应用是图像匹配问题，其目标是在两幅图像中找到一致的对应特征集。正如在第3.2节末尾介绍的，最近一些深度图匹配网络已经被开发用于图像匹配的任务[jiang2019glmnet](#bib.bib60);[wang2019learning](#bib.bib121)，其中图像首先被转换为图形式，然后图像匹配问题被解决为图匹配问题。在从图像转换的图中，节点表示图像中注释特征点的单个描述符，边编码了图像中不同特征点之间的成对关系。基于新的图表示，特征匹配可以重新表述为图匹配问题。但是，值得注意的是，这种图匹配实际上是图节点匹配，因为目标是匹配两个图之间的节点而不是整个图。因此，基于图的图像匹配问题是一般图匹配问题的特例或子问题。
- en: The two application problems discussed above are both promising directions of
    applying deep graph similarity learning models for the practical learning tasks
    in computer vision. A key advice we provide on applying graph similarity learning
    methods for these image applications is to first find an appropriate mapping for
    converting the images to graphs, so that the learning tasks on images can be formulated
    as the graph similarity learning based tasks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 上面讨论的两个应用问题都是在计算机视觉的实际学习任务中应用深度图相似性学习模型的有希望的方向。我们对在图像应用中应用图相似性学习方法提供的一个关键建议是，首先找到一个合适的映射，将图像转换为图形式，这样就可以将图像上的学习任务表述为基于图相似性学习的任务。
- en: 6 Challenges
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 挑战
- en: 6.1 Various Graph Types
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 各种图形类型
- en: In most of the work discussed above, the graphs involved consist of unlabeled
    nodes/edges and undirected edges. However, there are many variants of graphs in
    real world applications. How to build deep graph similarity learning models for
    these various graph types is a challenging problem.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面讨论的大部分工作中，涉及的图形由无标记的节点/边和无定向的边组成。然而，在实际应用中有许多变种的图。如何为这些不同类型的图构建深度图相似性学习模型是一个具有挑战性的问题。
- en: Directed Graphs.
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 有向图。
- en: In some application scenarios, the graphs are directed, which means all the
    edges in the graph are directed from one vertex to another. For instance, in a
    knowledge graph, edges go from one entity to another, where the relationship is
    directed. In such cases, we should treat the information propagation process differently
    according to the direction of the edge. Recently some GCN based graph models have
    suggested some strategies for dealing with such directed graphs. In [kampffmeyer2019rethinking](#bib.bib63)
    , a dense graph propagation strategy is proposed for the propagation on knowledge
    graphs, where two kinds of weight matrices are introduced for the propagation
    based on a node’s relationship to its ancestors and descendants respectively.
    However, to the best of our knowledge, no work has been done on deep similarity
    learning specifically for directed graphs, which arises as a challenging problem
    for this community.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些应用场景中，图是有向的，这意味着图中的所有边都是从一个顶点指向另一个顶点的。例如，在知识图中，边从一个实体指向另一个实体，这种关系是有向的。在这种情况下，我们应该根据边的方向不同来处理信息传播过程。最近，一些基于GCN的图模型提出了一些处理这种有向图的策略。在[kampffmeyer2019rethinking](#bib.bib63)中，提出了一种稠密图传播策略，用于知识图的传播，引入了两种权重矩阵，根据节点与其祖先和后代的关系进行传播。然而，据我们所知，还没有针对有向图的深度相似性学习做出过的工作，这对这个领域来说是一个具有挑战性的问题。
- en: Labeled Graphs.
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标记图。
- en: Labeled graphs are graphs where vertices or edges have labels. For example,
    in chemical compound graphs where vertices denote the atoms and the edges represent
    the chemical bonds between the atoms, each node and edge have labels representing
    the atom type and bond type, respectively. These labels are important for characterizing
    the node-node relationship in the graphs, therefore it is important to leverage
    these label information for the similarity learning. In [bai2019simgnn](#bib.bib15)
    ; [ahmed2018learning](#bib.bib10) , the node label information are used as the
    initial node representations encoded by a one-hot vector and used in the node
    embedding stage. In this case, the nodes with same type share the same one-hot
    encoding vector. This should guarantee that even if the node ids are permuted,
    the aggregation results would be the same. However, the label information is only
    used for the node embedding process within each graph, and the comparison of the
    node or edge labels across graphs is not considered during the similarity learning
    stage. In [al2019ddgk](#bib.bib11) , both node labels and edge labels in the chemo-
    and bio-informatic graphs have been used as attributes for learning better alignment
    across graphs, which has been shown to lead to a better performance. Therefore,
    how to leverage the node / edge attributes of the labeled graphs into the similarity
    learning process is a critical problem.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 标记图是指那些顶点或边带有标签的图。例如，在化学化合物图中，顶点表示原子，边表示原子之间的化学键，每个节点和边都有表示原子类型和键类型的标签。这些标签对于表征图中的节点间关系至关重要，因此利用这些标签信息进行相似性学习非常重要。在[bai2019simgnn](#bib.bib15)和[ahmed2018learning](#bib.bib10)中，节点标签信息被用作初始节点表示，通过一-hot向量进行编码，并在节点嵌入阶段使用。在这种情况下，相同类型的节点共享相同的一-hot编码向量。这应该可以保证，即使节点ID被重新排列，聚合结果也会相同。然而，标签信息仅用于每个图中的节点嵌入过程，而图之间的节点或边标签的比较在相似性学习阶段并未考虑。在[al2019ddgk](#bib.bib11)中，化学和生物信息图中的节点标签和边标签都被用作学习图之间更好对齐的属性，这已被证明能带来更好的性能。因此，如何将标记图的节点/边属性融入相似性学习过程是一个关键问题。
- en: Dynamic and Streaming Graphs.
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动态和流图。
- en: 'Another type of graphs is the dynamic graph, which has a static graph structure
    and dynamic input signals/features. For example, the 3D human action or motion
    data can be represented as graphs where the entities are represented as nodes
    and the actions as edges connecting the entities. Then similarity learning on
    these graphs is an important problem for action and motion recognition. Moreover,
    another type of graph is the streaming graph, where both the structure and/or
    features are continuously changing [ahmed2019temporal](#bib.bib4) ; [ahmed2019network](#bib.bib2)
    . For example, online social networks [ahmed2017sampling](#bib.bib5) ; [ahmed2014graph](#bib.bib3)
    ; [ahmed2014network](#bib.bib6) . The similarity learning would be important for
    change/anomaly detection, link prediction, relationship strength prediction, etc.
    Although some work has proposed variants of GNN models for spatio-temporal graphs
    [yu2017spatio](#bib.bib133) ; [manessi2020dynamic](#bib.bib83) , and other learning
    methods for dynamic graphs [nguyen2018continuous](#bib.bib91) ; [nguyen2018dynamic](#bib.bib90)
    ; [tong2008colibri](#bib.bib112) ; [li2017attributed](#bib.bib72) , the similarity
    learning problem on dynamic and streaming graphs has not been well studied. For
    example, in the multi-subject analysis of task-related fMRI brain networks as
    mentioned in Section [5.2](#S5.SS2 "5.2 Neuroscience ‣ 5 Applications ‣ Deep Graph
    Similarity Learning: A Survey"), for each subject, a set of brain connectivity
    networks can be collected for a give time period, which forms a spatio-temporal
    graph. It would be interesting to conduct similarity learning on the spatio-temporal
    graphs of different subjects to analyze their similarity in cognitive abilities,
    which is an important problem in the neuroscience field. However, to the best
    of our knowledge, none of the existing similarity learning methods is able to
    deal with such spatio-temporal graphs. The main challenge in such problems is
    how to leverage the temporal updates of the node-level representations and the
    interactions between the nodes on these graphs while modeling their similarity.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种类型的图是动态图，它具有静态图结构和动态输入信号/特征。例如，3D 人体动作或运动数据可以表示为图，其中实体表示为节点，动作表示为连接这些实体的边。然后，对这些图进行相似性学习是动作和运动识别中的一个重要问题。此外，还有一种图是流图，其中结构和/或特征不断变化
    [ahmed2019temporal](#bib.bib4) ; [ahmed2019network](#bib.bib2) 。例如，在线社交网络 [ahmed2017sampling](#bib.bib5)
    ; [ahmed2014graph](#bib.bib3) ; [ahmed2014network](#bib.bib6) 。相似性学习对于变化/异常检测、链接预测、关系强度预测等都非常重要。尽管一些工作已经提出了用于时空图的
    GNN 模型变体 [yu2017spatio](#bib.bib133) ; [manessi2020dynamic](#bib.bib83) ，以及用于动态图的其他学习方法
    [nguyen2018continuous](#bib.bib91) ; [nguyen2018dynamic](#bib.bib90) ; [tong2008colibri](#bib.bib112)
    ; [li2017attributed](#bib.bib72) ，但在动态和流图上的相似性学习问题仍未得到很好研究。例如，在第 [5.2](#S5.SS2
    "5.2 Neuroscience ‣ 5 Applications ‣ Deep Graph Similarity Learning: A Survey")
    节提到的任务相关 fMRI 大脑网络的多主体分析中，对于每个受试者，可以收集一组大脑连接网络，用于给定的时间段，从而形成一个时空图。在不同受试者的时空图上进行相似性学习，以分析其认知能力的相似性，这在神经科学领域是一个重要问题。然而，尽我们所知，目前没有现有的相似性学习方法能够处理这种时空图。这类问题的主要挑战在于如何利用节点级表示的时间更新以及这些图中节点之间的交互，同时建模它们的相似性。'
- en: 6.2 Interpretability
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 可解释性
- en: The deep graph models, such as GNNs, combine node feature information with graph
    structure by recursively passing neural messages along edges of the graph, which
    is a complex process and makes it challenging to explain the learning results
    from these models. Recently, some work has started to explore the interpretability
    of GNNs [ying2019gnn](#bib.bib131) ; [baldassarre2019explainability](#bib.bib18)
    . In [ying2019gnn](#bib.bib131) , a GNNEXPLAINER is proposed for providing interpretable
    explanations for predictions of GNN-based models. It first identifies a subgraph
    structure and a subset of node features that are crucial in a prediction. Then
    it formulates an optimization task that maximizes the mutual information between
    a GNN’s prediction and the distribution of possible subgraph structures. [baldassarre2019explainability](#bib.bib18)
    explores the explainability of GNNs using gradient-based and decomposition-based
    methods, respectively, on a toy dataset and a chemistry task. Although these works
    have provided some insights into the interpretability of GNNs, they are mainly
    for node classification or link prediction tasks on a graph. To the best of our
    knowledge, the explainability of GNN-based graph similarity models remains unexplored.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图模型，如GNNs，通过沿图的边递归传递神经信息，将节点特征信息与图结构相结合，这一过程复杂，使得解释这些模型的学习结果变得具有挑战性。最近，一些研究开始探索GNNs的可解释性
    [ying2019gnn](#bib.bib131) ; [baldassarre2019explainability](#bib.bib18) 。在 [ying2019gnn](#bib.bib131)
    中，提出了GNNEXPLAINER，以提供对GNN模型预测的可解释性解释。它首先识别出在预测中至关重要的子图结构和节点特征子集。然后，它制定了一个优化任务，最大化GNN预测与可能子图结构分布之间的互信息。[baldassarre2019explainability](#bib.bib18)
    分别使用基于梯度和基于分解的方法探索了GNN的可解释性，在一个玩具数据集和一个化学任务上进行实验。尽管这些研究对GNN的可解释性提供了一些见解，但主要针对图上的节点分类或链接预测任务。据我们所知，基于GNN的图相似性模型的可解释性仍未得到探索。
- en: 6.3 Few-shot Learning
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 少样本学习
- en: The task of few-shot learning is to learn classifiers for new classes with only
    a few training examples per class. A big branch of work in this area is based
    on metric learning [wang2019few](#bib.bib123) . However, most of the existing
    work proposes few-shot learning problems on images, such as image recognition
    [koch2015siamese](#bib.bib65) and image retrieval[triantafillou2017few](#bib.bib113)
    . Little work has been done on metric learning for few-shot learning on graphs,
    which is an important problem for areas in which data are represented as graphs
    and data gathering is difficult, for example, brain connectivity network analysis
    in neuroscience. Since graph data usually has complex structure, how to learn
    a metric so that it can facilitate generalizing from a few graph examples is a
    big challenge. Some recent work  [guo2018neural](#bib.bib53) has begun to explore
    the few-shot 3D action recognition problem with graph-based similarity learning
    strategies, where a neural graph matching network is proposed to jointly learn
    a graph generator and a graph matching metric function to optimize the few-shot
    learning objective of 3D action recognition. However, since the objective is defined
    specifically based on the 3D action recognition task, the model can not be directly
    used for other domains. The remaining problem is to design general deep graph
    similarity learning models for the few-shot learning task for a multitude of applications.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习的任务是为新类别学习分类器，每个类别仅有少量训练样本。这一领域的大部分工作基于度量学习 [wang2019few](#bib.bib123)
    。然而，大多数现有工作提出的少样本学习问题集中在图像上，如图像识别 [koch2015siamese](#bib.bib65) 和图像检索 [triantafillou2017few](#bib.bib113)
    。对于图上的少样本学习，尤其是在数据以图形表示且数据收集困难的领域，如神经科学中的大脑连接网络分析，度量学习的研究较少。由于图数据通常具有复杂结构，如何学习度量以促进从少量图样本中进行泛化是一个重大挑战。一些近期的工作
    [guo2018neural](#bib.bib53) 开始探索使用基于图的相似性学习策略的少样本3D动作识别问题，其中提出了一种神经图匹配网络，以联合学习图生成器和图匹配度量函数，以优化3D动作识别的少样本学习目标。然而，由于目标是基于3D动作识别任务特别定义的，该模型不能直接用于其他领域。剩下的问题是为多种应用设计通用的深度图相似性学习模型以应对少样本学习任务。
- en: 7 Conclusion
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'Recently, there has been an increasing interest in deep neural network models
    for learning graph similarity. In this survey paper, we provided a comprehensive
    review of the existing work on deep graph similarity learning, and categorized
    the literature into three main categories: (1) graph embedding based graph similarity
    learning models, (2) GNN-based models, and (3) Deep graph kernels. We discussed
    and summarized the various properties and applications of the existing literature.
    Finally, we pointed out the key challenges and future research directions for
    the deep graph similarity learning problem.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，对深度神经网络模型在图形相似性学习中的应用兴趣日益增加。在这篇综述论文中，我们提供了对现有深度图形相似性学习工作的全面回顾，并将文献分为三类： (1)
    基于图嵌入的图形相似性学习模型，(2) 基于 GNN 的模型，以及 (3) 深度图核。我们讨论并总结了现有文献的各种属性和应用。最后，我们指出了深度图形相似性学习问题的关键挑战和未来的研究方向。
- en: 8 Acknowledgement
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 致谢
- en: Philip S. Yu is supported by NSF under grants III-1526499, III-1763325, III-1909323,
    and SaTC-1930941.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Philip S. Yu 由 NSF 资助，资助编号为 III-1526499、III-1763325、III-1909323 和 SaTC-1930941。
- en: References
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] functionsimsearch. https://github.com/google/functionsimsearch, 2018. Accessed:
    2018-05-14.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] functionsimsearch。https://github.com/google/functionsimsearch，2018。访问日期：2018-05-14。'
- en: '[2] Nesreen K Ahmed and Nick Duffield. Network shrinkage estimation. arXiv
    preprint arXiv:1908.01087, 2019.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Nesreen K Ahmed 和 Nick Duffield。网络收缩估计。arXiv 预印本 arXiv:1908.01087，2019。'
- en: '[3] Nesreen K Ahmed, Nick Duffield, Jennifer Neville, and Ramana Kompella.
    Graph sample and hold: A framework for big-graph analytics. In Proceedings of
    the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,
    pages 1446–1455\. ACM, 2014.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Nesreen K Ahmed、Nick Duffield、Jennifer Neville 和 Ramana Kompella。图采样与保持：大图分析的框架。在第20届
    ACM SIGKDD 国际知识发现与数据挖掘会议论文集，页面 1446–1455。ACM，2014。'
- en: '[4] Nesreen K Ahmed, Nick Duffield, and Ryan A Rossi. Temporal network sampling.
    arXiv preprint arXiv:1910.08657, 2019.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Nesreen K Ahmed、Nick Duffield 和 Ryan A Rossi。时间网络采样。arXiv 预印本 arXiv:1910.08657，2019。'
- en: '[5] Nesreen K Ahmed, Nick Duffield, Theodore L Willke, and Ryan A Rossi. On
    sampling from massive graph streams. Proceedings of the VLDB Endowment, 10(11):1430–1441,
    2017.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Nesreen K Ahmed、Nick Duffield、Theodore L Willke 和 Ryan A Rossi。来自大规模图流的采样。VLDB
    基金会会议论文集，10(11)：1430–1441，2017。'
- en: '[6] Nesreen K Ahmed, Jennifer Neville, and Ramana Kompella. Network sampling:
    From static to streaming graphs. ACM Transactions on Knowledge Discovery from
    Data (TKDD), 8(2):7, 2014.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Nesreen K Ahmed、Jennifer Neville 和 Ramana Kompella。网络采样：从静态到流式图。ACM 数据知识发现交易（TKDD），8(2)：7，2014。'
- en: '[7] Nesreen K Ahmed, Jennifer Neville, Ryan A Rossi, and Nick Duffield. Efficient
    graphlet counting for large networks. In 2015 IEEE International Conference on
    Data Mining, pages 1–10\. IEEE, 2015.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Nesreen K Ahmed、Jennifer Neville、Ryan A Rossi 和 Nick Duffield。大规模网络的高效图形计数。2015
    IEEE 数据挖掘国际会议论文集，页面 1–10。IEEE，2015。'
- en: '[8] Nesreen K Ahmed, Jennifer Neville, Ryan A Rossi, Nick G Duffield, and Theodore L
    Willke. Graphlet decomposition: Framework, algorithms, and applications. Knowledge
    and Information Systems, 50(3):689–722, 2017.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Nesreen K Ahmed、Jennifer Neville、Ryan A Rossi、Nick G Duffield 和 Theodore
    L Willke。图形分解：框架、算法和应用。知识与信息系统，50(3)：689–722，2017。'
- en: '[9] Nesreen K Ahmed, Ryan Rossi, John Lee, Theodore Willke, Rong Zhou, Xiangnan
    Kong, and Hoda Eldardiry. Role-based graph embeddings. IEEE Transactions on Knowledge
    and Data Engineering, 2020.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Nesreen K Ahmed、Ryan Rossi、John Lee、Theodore Willke、Rong Zhou、Xiangnan
    Kong 和 Hoda Eldardiry。基于角色的图嵌入。IEEE 知识与数据工程学报，2020。'
- en: '[10] Nesreen K Ahmed, Ryan Rossi, John Boaz Lee, Theodore L Willke, Rong Zhou,
    Xiangnan Kong, and Hoda Eldardiry. Learning role-based graph embeddings. arXiv
    preprint arXiv:1802.02896, 2018.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Nesreen K Ahmed、Ryan Rossi、John Boaz Lee、Theodore L Willke、Rong Zhou、Xiangnan
    Kong 和 Hoda Eldardiry。基于角色的图嵌入学习。arXiv 预印本 arXiv:1802.02896，2018。'
- en: '[11] Rami Al-Rfou, Bryan Perozzi, and Dustin Zelle. Ddgk: Learning graph representations
    for deep divergence graph kernels. In The World Wide Web Conference, pages 37–48\.
    ACM, 2019.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Rami Al-Rfou、Bryan Perozzi 和 Dustin Zelle。DDGK：用于深度发散图核的图表示学习。在万维网会议，页面
    37–48。ACM，2019。'
- en: '[12] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
    Ruosong Wang. On exact computation with an infinitely wide neural net. arXiv preprint
    arXiv:1904.11955, 2019.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Sanjeev Arora、Simon S Du、Wei Hu、Zhiyuan Li、Ruslan Salakhutdinov 和 Ruosong
    Wang。关于使用无限宽神经网络的精确计算。arXiv 预印本 arXiv:1904.11955，2019。'
- en: '[13] Asma Atamna, Nataliya Sokolovska, and Jean-Claude Crivello. Spi-gcn: A
    simple permutation-invariant graph convolutional network. 2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Asma Atamna、Nataliya Sokolovska 和 Jean-Claude Crivello。Spi-gcn：一种简单的排列不变图卷积网络。2019。'
- en: '[14] AmanPreet Badhwar, Angela Tam, Christian Dansereau, Pierre Orban, Felix
    Hoffstaedter, and Pierre Bellec. Resting-state network dysfunction in alzheimer’s
    disease: a systematic review and meta-analysis. Alzheimer’s & Dementia: Diagnosis,
    Assessment & Disease Monitoring, 8:73–85, 2017.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] AmanPreet Badhwar, Angela Tam, Christian Dansereau, Pierre Orban, Felix
    Hoffstaedter, 和 Pierre Bellec. 阿尔茨海默病中的静息态网络功能障碍：系统评价和荟萃分析。《阿尔茨海默病与痴呆症：诊断、评估与疾病监测》，8:73–85，2017年。'
- en: '[15] Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang.
    Simgnn: A neural network approach to fast graph similarity computation. In Proceedings
    of the Twelfth ACM International Conference on Web Search and Data Mining, pages
    384–392\. ACM, 2019.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, 和 Wei Wang.
    Simgnn：一种快速图相似性计算的神经网络方法。见第十二届 ACM 国际网络搜索与数据挖掘会议论文集，第384–392页。ACM，2019年。'
- en: '[16] Yunsheng Bai, Hao Ding, Yizhou Sun, and Wei Wang. Convolutional set matching
    for graph similarity. arXiv preprint arXiv:1810.10866, 2018.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Yunsheng Bai, Hao Ding, Yizhou Sun, 和 Wei Wang. 图相似性卷积集匹配。arXiv 预印本 arXiv:1810.10866，2018年。'
- en: '[17] Yunsheng Bai, Derek Xu, Ken Gu, Xueqing Wu, Agustin Marinovic, Christopher
    Ro, Yizhou Sun, and Wei Wang. Neural maximum common subgraph detection with guided
    subgraph extraction. https://openreview.net/pdf?id=BJgcwh4FwS, 2019.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Yunsheng Bai, Derek Xu, Ken Gu, Xueqing Wu, Agustin Marinovic, Christopher
    Ro, Yizhou Sun, 和 Wei Wang. 带引导子图提取的神经最大公共子图检测。https://openreview.net/pdf?id=BJgcwh4FwS，2019年。'
- en: '[18] Federico Baldassarre and Hossein Azizpour. Explainability techniques for
    graph convolutional networks. arXiv preprint arXiv:1905.13686, 2019.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Federico Baldassarre 和 Hossein Azizpour. 图卷积网络的可解释性技术。arXiv 预印本 arXiv:1905.13686，2019年。'
- en: '[19] Stefano Berretti, Alberto Del Bimbo, and Enrico Vicario. Efficient matching
    and indexing of graph models in content-based retrieval. IEEE Transactions on
    Pattern Analysis and Machine Intelligence, 23(10):1089–1105, 2001.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Stefano Berretti, Alberto Del Bimbo, 和 Enrico Vicario. 基于内容的检索中图模型的高效匹配和索引。IEEE《模式分析与机器智能》期刊，23(10):1089–1105，2001年。'
- en: '[20] UK Biobank. About uk biobank. Available at h ttps://www. ukbiobank. ac.
    uk/a bout-biobank-uk, 2014.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 英国生物样本库。关于英国生物样本库。可在 https://www.ukbiobank.ac.uk/about-biobank-uk 查阅，2014年。'
- en: '[21] Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs.
    In Fifth IEEE international conference on data mining (ICDM’05), pages 8–pp. IEEE,
    2005.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Karsten M Borgwardt 和 Hans-Peter Kriegel. 图上的最短路径核。见第五届 IEEE 国际数据挖掘会议（ICDM’05）论文集，第8–pp页。IEEE，2005年。'
- en: '[22] Karsten M Borgwardt, Hans-Peter Kriegel, SVN Vishwanathan, and Nicol N
    Schraudolph. Graph kernels for disease outcome prediction from protein-protein
    interaction networks. In Biocomputing 2007, pages 4–15\. World Scientific, 2007.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Karsten M Borgwardt, Hans-Peter Kriegel, SVN Vishwanathan, 和 Nicol N Schraudolph.
    基于蛋白质互作网络的疾病结果预测图核方法。见《生物计算 2007》，第4–15页。World Scientific，2007年。'
- en: '[23] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan,
    Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels.
    Bioinformatics, 21(suppl_1):i47–i56, 2005.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan,
    Alex J Smola, 和 Hans-Peter Kriegel. 通过图核进行蛋白质功能预测。《生物信息学》，21(suppl_1):i47–i56，2005年。'
- en: '[24] Nathan Brown. Chemoinformatics—an introduction for computer scientists.
    ACM Computing Surveys (CSUR), 41(2):8, 2009.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Nathan Brown. 化学信息学——计算机科学家的入门。ACM《计算机调查》（CSUR），41(2):8，2009年。'
- en: '[25] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks
    and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Joan Bruna, Wojciech Zaremba, Arthur Szlam, 和 Yann LeCun. 图上的谱网络和局部连接网络。arXiv
    预印本 arXiv:1312.6203，2013年。'
- en: '[26] Horst Bunke and Gudrun Allermann. Inexact graph matching for structural
    pattern recognition. Pattern Recognition Letters, 1(4):245–253, 1983.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Horst Bunke 和 Gudrun Allermann. 结构模式识别中的不精确图匹配。模式识别快报，1(4):245–253，1983年。'
- en: '[27] Horst Bunke and Kim Shearer. A graph distance metric based on the maximal
    common subgraph. Pattern recognition letters, 19(3-4):255–259, 1998.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Horst Bunke 和 Kim Shearer. 基于最大公共子图的图距离度量。模式识别快报，19(3-4):255–259，1998年。'
- en: '[28] Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. A comprehensive
    survey of graph embedding: Problems, techniques, and applications. IEEE Transactions
    on Knowledge and Data Engineering, 30(9):1616–1637, 2018.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Hongyun Cai, Vincent W Zheng, 和 Kevin Chen-Chuan Chang. 图嵌入的全面调查：问题、技术与应用。IEEE《知识与数据工程》期刊，30(9):1616–1637，2018年。'
- en: '[29] Ushasi Chaudhuri, Biplab Banerjee, and Avik Bhattacharya. Siamese graph
    convolutional network for content based remote sensing image retrieval. Computer
    Vision and Image Understanding, 184:22–30, 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Ushasi Chaudhuri, Biplab Banerjee 和 Avik Bhattacharya。用于基于内容的遥感图像检索的孪生图卷积网络。计算机视觉与图像理解，184:22–30，2019年。'
- en: '[30] Thomas M Cover, Peter Hart, et al. Nearest neighbor pattern classification.
    IEEE transactions on information theory, 13(1):21–27, 1967.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Thomas M Cover, Peter Hart 等。最近邻模式分类。IEEE 信息理论汇刊，13(1):21–27，1967年。'
- en: '[31] Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. A survey on network embedding.
    IEEE Transactions on Knowledge and Data Engineering, 31(5):833–852, 2018.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Peng Cui, Xiao Wang, Jian Pei 和 Wenwu Zhu。网络嵌入的调查。IEEE 知识与数据工程汇刊，31(5):833–852，2018年。'
- en: '[32] Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable
    models for structured data. In International conference on machine learning, pages
    2702–2711, 2016.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Hanjun Dai, Bo Dai 和 Le Song。结构数据的潜变量模型的区分嵌入。在国际机器学习会议上，页码 2702–2711，2016年。'
- en: '[33] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman,
    and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic
    nitro compounds. correlation with molecular orbital energies and hydrophobicity.
    Journal of medicinal chemistry, 34(2):786–797, 1991.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman
    和 Corwin Hansch。致突变芳香和杂芳香硝基化合物的结构-活性关系。与分子轨道能量和疏水性的相关性。医学化学杂志，34(2):786–797，1991年。'
- en: '[34] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional
    neural networks on graphs with fast localized spectral filtering. In NeurIPS,
    pages 3844–3852, 2016.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Michaël Defferrard, Xavier Bresson 和 Pierre Vandergheynst。具有快速局部谱滤波的图上的卷积神经网络。在
    NeurIPS 会议上，页码 3844–3852，2016年。'
- en: '[35] Adriana Di Martino, Chao-Gan Yan, Qingyang Li, Erin Denio, Francisco X
    Castellanos, Kaat Alaerts, Jeffrey S Anderson, Michal Assaf, Susan Y Bookheimer,
    Mirella Dapretto, et al. The autism brain imaging data exchange: towards a large-scale
    evaluation of the intrinsic brain architecture in autism. Molecular psychiatry,
    19(6):659–667, 2014.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Adriana Di Martino, Chao-Gan Yan, Qingyang Li, Erin Denio, Francisco X
    Castellanos, Kaat Alaerts, Jeffrey S Anderson, Michal Assaf, Susan Y Bookheimer,
    Mirella Dapretto等。自闭症脑成像数据交换：朝向大规模评估自闭症内在脑结构的方向。分子精神病学，19(6):659–667，2014年。'
- en: '[36] Remco Dijkman, Marlon Dumas, and Luciano García-Bañuelos. Graph matching
    algorithms for business process model similarity search. In International conference
    on business process management, pages 48–63\. Springer, 2009.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Remco Dijkman, Marlon Dumas 和 Luciano García-Bañuelos。用于业务流程模型相似性搜索的图匹配算法。在业务流程管理国际会议上，页码
    48–63。Springer，2009年。'
- en: '[37] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from
    non-enzymes without alignments. Journal of molecular biology, 330(4):771–783,
    2003.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Paul D Dobson 和 Andrew J Doig。区分酶结构与非酶结构而无需对齐。分子生物学杂志，330(4):771–783，2003年。'
- en: '[38] Brendan L Douglas. The weisfeiler-lehman method and graph isomorphism
    testing. arXiv preprint arXiv:1101.5211, 2011.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Brendan L Douglas。Weisfeiler-Lehman 方法和图同构测试。arXiv 预印本 arXiv:1101.5211，2011年。'
- en: '[39] Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong
    Wang, and Keyulu Xu. Graph neural tangent kernel: Fusing graph neural networks
    with graph kernels. In Advances in Neural Information Processing Systems, pages
    5724–5734, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong
    Wang 和 Keyulu Xu。图神经切线核：将图神经网络与图核融合。在神经信息处理系统进展会议上，页码 5724–5734，2019年。'
- en: '[40] Stefan Fankhauser, Kaspar Riesen, and Horst Bunke. Speeding up graph edit
    distance computation through fast bipartite matching. In International Workshop
    on Graph-Based Representations in Pattern Recognition, pages 102–111\. Springer,
    2011.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Stefan Fankhauser, Kaspar Riesen 和 Horst Bunke。通过快速二分匹配加速图编辑距离计算。在图形表示模式识别国际研讨会上，页码
    102–111。Springer，2011年。'
- en: '[41] Matthias Fey, Jan Lenssen, Christopher Morris, Jonathan Masci, and Nils
    Kriege. Deep graph matching consensus. ICLR, 2020.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Matthias Fey, Jan Lenssen, Christopher Morris, Jonathan Masci 和 Nils Kriege。深度图匹配共识。ICLR，2020年。'
- en: '[42] Holger Fröhlich, Jörg K Wegner, Florian Sieker, and Andreas Zell. Kernel
    functions for attributed molecular graphs–a new similarity-based approach to adme
    prediction in classification and regression. QSAR & Combinatorial Science, 25(4):317–326,
    2006.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Holger Fröhlich, Jörg K Wegner, Florian Sieker 和 Andreas Zell。用于属性分子图的核函数–一种基于相似性的分类和回归
    ADME 预测的新方法。QSAR & 组合科学，25(4):317–326，2006年。'
- en: '[43] Claudio Gallicchio and Alessio Micheli. Graph echo state networks. In
    The 2010 International Joint Conference on Neural Networks (IJCNN), pages 1–8\.
    IEEE, 2010.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Claudio Gallicchio 和 Alessio Micheli. 图回声状态网络. 在2010年国际神经网络联合会议（IJCNN）论文集中，第1–8页。IEEE，2010年。'
- en: '[44] Hongyang Gao and Shuiwang Ji. Graph representation learning via hard and
    channel-wise attention networks. In Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining, pages 741–749\. ACM, 2019.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Hongyang Gao 和 Shuiwang Ji. 通过硬注意力和通道注意力网络的图表示学习. 在第25届ACM SIGKDD国际知识发现与数据挖掘会议论文集中，第741–749页。ACM，2019年。'
- en: '[45] Hongyang Gao and Shuiwang Ji. Graph u-nets. ICML, 2019.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Hongyang Gao 和 Shuiwang Ji. 图u-net. ICML，2019年。'
- en: '[46] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph
    convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining, pages 1416–1424\. ACM, 2018.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Hongyang Gao, Zhengyang Wang, 和 Shuiwang Ji. 大规模可学习图卷积网络. 在第24届ACM SIGKDD国际知识发现与数据挖掘会议论文集中，第1416–1424页。ACM，2018年。'
- en: '[47] M. R. Garey and David S. Johnson. Computers and intractability: A guide
    to the theory of np-completeness. 1978.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. R. Garey 和 David S. Johnson. 计算机与难处理性：NP完全性理论指南。1978年。'
- en: '[48] Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for
    learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference
    on Neural Networks, 2005., volume 2, pages 729–734\. IEEE, 2005.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Marco Gori, Gabriele Monfardini, 和 Franco Scarselli. 图领域中的新学习模型. 在2005年IEEE国际神经网络联合会议论文集中，卷2，第729–734页。IEEE，2005年。'
- en: '[49] Palash Goyal and Emilio Ferrara. Graph embedding techniques, applications,
    and performance: A survey. Knowledge-Based Systems, 151:78–94, 2018.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Palash Goyal 和 Emilio Ferrara. 图嵌入技术、应用及性能：综述. 知识基础系统，151：78–94，2018年。'
- en: '[50] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf,
    and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research,
    13(Mar):723–773, 2012.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf,
    和 Alexander Smola. 核两样本检验. 机器学习研究杂志，13（3月）：723–773，2012年。'
- en: '[51] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for
    networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge
    discovery and data mining, pages 855–864\. ACM, 2016.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Aditya Grover 和 Jure Leskovec. node2vec：用于网络的可扩展特征学习. 在第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集中，第855–864页。ACM，2016年。'
- en: '[52] Matthieu Guillaumin, Jakob Verbeek, and Cordelia Schmid. Is that you?
    metric learning approaches for face identification. In 2009 IEEE 12th international
    conference on computer vision, pages 498–505\. IEEE, 2009.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Matthieu Guillaumin, Jakob Verbeek, 和 Cordelia Schmid. 那是你吗？用于面部识别的度量学习方法.
    在2009年IEEE第12届国际计算机视觉会议论文集中，第498–505页。IEEE，2009年。'
- en: '[53] Michelle Guo, Edward Chou, De-An Huang, Shuran Song, Serena Yeung, and
    Li Fei-Fei. Neural graph matching networks for fewshot 3d action recognition.
    In Proceedings of the European Conference on Computer Vision (ECCV), pages 653–669,
    2018.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Michelle Guo, Edward Chou, De-An Huang, Shuran Song, Serena Yeung, 和 Li
    Fei-Fei. 用于少样本3D动作识别的神经图匹配网络. 在欧洲计算机视觉会议（ECCV）论文集中，第653–669页，2018年。'
- en: '[54] Christoph Helma, Ross D. King, Stefan Kramer, and Ashwin Srinivasan. The
    predictive toxicology challenge 2000–2001. Bioinformatics, 17(1):107–108, 2001.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Christoph Helma, Ross D. King, Stefan Kramer, 和 Ashwin Srinivasan. 预测毒理学挑战2000–2001.
    生物信息学，17（1）：107–108，2001年。'
- en: '[55] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality
    of data with neural networks. science, 313(5786):504–507, 2006.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Geoffrey E Hinton 和 Ruslan R Salakhutdinov. 用神经网络减少数据的维度. 科学，313（5786）：504–507，2006年。'
- en: '[56] Tamás Horváth, Thomas Gärtner, and Stefan Wrobel. Cyclic pattern kernels
    for predictive graph mining. In Proceedings of the tenth ACM SIGKDD international
    conference on Knowledge discovery and data mining, pages 158–167\. ACM, 2004.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Tamás Horváth, Thomas Gärtner, 和 Stefan Wrobel. 用于预测图挖掘的周期模式核. 在第十届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集中，第158–167页。ACM，2004年。'
- en: '[57] Xin Hu, Tzi-cker Chiueh, and Kang G Shin. Large-scale malware indexing
    using function-call graphs. In Proceedings of the 16th ACM conference on Computer
    and communications security, pages 611–620\. ACM, 2009.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Xin Hu, Tzi-cker Chiueh, 和 Kang G Shin. 使用函数调用图的大规模恶意软件索引. 在第16届ACM计算机与通信安全会议论文集中，第611–620页。ACM，2009年。'
- en: '[58] Xiao Huang, Peng Cui, Yuxiao Dong, Jundong Li, Huan Liu, Jian Pei, Le Song,
    Jie Tang, Fei Wang, Hongxia Yang, et al. Learning from networks: Algorithms, theory,
    and applications. In Proceedings of the 25th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining, pages 3221–3222\. ACM, 2019.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 肖黄、彭崔、于潇·董、君栋·李、欢刘、简佩、乐·宋、杰·唐、飞王、洪霞·杨等。来自网络的学习：算法、理论与应用。载于第25届ACM SIGKDD国际知识发现与数据挖掘会议论文集，页面3221–3222。ACM，2019年。'
- en: '[59] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel:
    Convergence and generalization in neural networks. In Advances in neural information
    processing systems, pages 8571–8580, 2018.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 亚瑟·雅科、弗朗克·加布里埃尔和克莱门特·洪勒。神经切线核：神经网络中的收敛性和泛化能力。载于神经信息处理系统进展，页面8571–8580，2018年。'
- en: '[60] Bo Jiang, Pengfei Sun, Jin Tang, and Bin Luo. Glmnet: Graph learning-matching
    networks for feature matching. arXiv preprint arXiv:1911.07681, 2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 博·江、彭飞·孙、金·唐和宾·罗。Glmnet：用于特征匹配的图学习-匹配网络。arXiv预印本 arXiv:1911.07681，2019年。'
- en: '[61] Nan Jiang, Wenyu Liu, and Ying Wu. Order determination and sparsity-regularized
    metric learning adaptive visual tracking. In 2012 IEEE Conference on Computer
    Vision and Pattern Recognition, pages 1956–1963\. IEEE, 2012.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 南江、温瑜·刘和应武。顺序确定与稀疏正则化度量学习的自适应视觉跟踪。载于2012年IEEE计算机视觉与模式识别会议，页面1956–1963。IEEE，2012年。'
- en: '[62] Fredrik D Johansson and Devdatt Dubhashi. Learning with similarity functions
    on graphs using matchings of geometric embeddings. In Proceedings of the 21th
    ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages
    467–476\. ACM, 2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 弗雷德里克·D·约翰逊和德夫达特·杜巴希。使用几何嵌入的匹配在图上学习相似性函数。载于第21届ACM SIGKDD国际知识发现与数据挖掘会议论文集，页面467–476。ACM，2015年。'
- en: '[63] Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang,
    and Eric P Xing. Rethinking knowledge graph propagation for zero-shot learning.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 11487–11496, 2019.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 迈克尔·坎普费梅耶、尹博·陈、肖丹·梁、郝王、于佳·张和埃里克·P·邢。重新思考知识图谱传播在零样本学习中的作用。载于IEEE计算机视觉与模式识别会议论文集，页面11487–11496，2019年。'
- en: '[64] Thomas N Kipf and Max Welling. Semi-supervised classification with graph
    convolutional networks. arXiv preprint arXiv:1609.02907, 2016.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 托马斯·N·基普夫和马克斯·威灵。使用图卷积网络的半监督分类。arXiv预印本 arXiv:1609.02907，2016年。'
- en: '[65] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural
    networks for one-shot image recognition. In ICML deep learning workshop, volume 2,
    2015.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 格雷戈里·科赫、理查德·泽梅尔和鲁斯兰·萨拉赫特丁诺夫。用于一次性图像识别的孪生神经网络。载于ICML深度学习研讨会，第2卷，2015年。'
- en: '[66] Nils M Kriege, Pierre-Louis Giscard, and Richard Wilson. On valid optimal
    assignment kernels and applications to graph classification. In Advances in Neural
    Information Processing Systems, pages 1623–1631, 2016.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] 尼尔斯·M·克里格、皮埃尔-路易斯·吉斯卡德和理查德·威尔逊。关于有效的最优分配核及其在图分类中的应用。载于神经信息处理系统进展，页面1623–1631，2016年。'
- en: '[67] Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew
    Lee, Ben Glocker, and Daniel Rueckert. Metric learning with spectral graph convolutions
    on brain connectivity networks. NeuroImage, 169:431–442, 2018.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] 索非亚·伊拉·克特纳、莎拉·帕里索、恩佐·费兰特、马丁·拉赫尔、马修·李、本·格洛克和丹尼尔·鲁克特。使用谱图卷积的度量学习在大脑连接网络中的应用。NeuroImage，169：431–442，2018年。'
- en: '[68] Quoc Le and Tomas Mikolov. Distributed representations of sentences and
    documents. In International conference on machine learning, pages 1188–1196, 2014.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] 郭阔和托马斯·米科洛夫。句子和文档的分布式表示。载于国际机器学习会议，页面1188–1196，2014年。'
- en: '[69] John Boaz Lee, Xiangnan Kong, Constance M Moore, and Nesreen K Ahmed.
    Deep parametric model for discovering group-cohesive functional brain regions.
    In Proceedings of the 2020 SIAM International Conference on Data Mining, pages
    631–639\. SIAM, 2020.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] 约翰·博阿兹·李、项楠·孔、康斯坦斯·M·穆尔和内斯林·K·艾哈迈德。发现群体凝聚性功能脑区的深度参数模型。载于2020年SIAM国际数据挖掘会议论文集，页面631–639。SIAM，2020年。'
- en: '[70] John Boaz Lee, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee
    Koh. Attention models in graphs: A survey. ACM Transactions on Knowledge Discovery
    from Data (TKDD), 13(6):62, 2019.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] 约翰·博阿兹·李、瑞安·A·罗西、金成哲、内斯林·K·艾哈迈德和恩叶·高。图中的注意力模型：综述。ACM数据知识发现事务（TKDD），13(6)：62，2019年。'
- en: '[71] Jung-Eun Lee, Rong Jin, and Anil K Jain. Rank-based distance metric learning:
    An application to image retrieval. In 2008 IEEE Conference on Computer Vision
    and Pattern Recognition, pages 1–8\. IEEE, 2008.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] 李正恩、荣金和安尼尔·K·贾因。基于排名的距离度量学习：在图像检索中的应用。载于2008年IEEE计算机视觉与模式识别会议，页面1–8。IEEE，2008年。'
- en: '[72] Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, and Huan Liu.
    Attributed network embedding for learning in a dynamic environment. In Proceedings
    of the 2017 ACM on Conference on Information and Knowledge Management, pages 387–396\.
    ACM, 2017.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] 李俊东、Harsh Dani、徐霞、唐际良、Yi Chang 和 刘欢。动态环境下的属性网络嵌入。在2017年ACM信息与知识管理会议论文集中，页码387–396。ACM，2017年。'
- en: '[73] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli.
    Graph matching networks for learning the similarity of graph structured objects.
    ICML, 2019.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] 李宇佳、顾陈杰、Thomas Dullien、Oriol Vinyals 和 Pushmeet Kohli。用于学习图结构对象相似性的图匹配网络。ICML，2019年。'
- en: '[74] Daryl Lim, Gert Lanckriet, and Brian McFee. Robust structural metric learning.
    In International conference on machine learning, pages 615–623, 2013.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Daryl Lim、Gert Lanckriet 和 Brian McFee。鲁棒结构度量学习。在国际机器学习会议上，页码615–623，2013年。'
- en: '[75] Xiang Ling, Lingfei Wu, Saizhuo Wang, Tengfei Ma, Fangli Xu, Chunming
    Wu, and Shouling Ji. Hierarchical graph matching networks for deep graph similarity
    learning. https://openreview.net/pdf?id=rkeqn1rtDH, 2019.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] 林翔、吴玲飞、王赛卓、马腾飞、徐芳丽、吴春明 和 纪寿龄。用于深度图相似性学习的层次图匹配网络。https://openreview.net/pdf?id=rkeqn1rtDH，2019年。'
- en: '[76] Jiahao Liu, Guixiang Ma, Fei Jiang, Chun-Ta Lu, Philip S Yu, and Ann B
    Ragin. Community-preserving graph convolutions for structural and functional joint
    embedding of brain networks. arXiv preprint arXiv:1911.03583, 2019.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] 刘家豪、马贵翔、姜飞、陆春塔、Philip S Yu 和 Ann B Ragin。用于大脑网络结构和功能联合嵌入的社区保留图卷积。arXiv
    预印本 arXiv:1911.03583，2019年。'
- en: '[77] Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple
    unsupervised representation for graphs, with applications to molecules. In Advances
    in Neural Information Processing Systems, pages 8464–8476, 2019.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] 刘盛超、Mehmet F Demirel 和 梁颖宇。N-gram 图：图的简单无监督表示，应用于分子。在《神经信息处理系统进展》中，页码8464–8476，2019年。'
- en: '[78] Guixiang Ma, Nesreen K Ahmed, Theodore L Willke, Dipanjan Sengupta, Michael W
    Cole, Nicholas B Turk-Browne, and Philip S Yu. Deep graph similarity learning
    for brain data analysis. In Proceedings of the 28th ACM International Conference
    on Information and Knowledge Management, pages 2743–2751\. ACM, 2019.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] 马贵翔、Nesreen K Ahmed、Theodore L Willke、Dipanjan Sengupta、Michael W Cole、Nicholas
    B Turk-Browne 和 Philip S Yu。用于脑数据分析的深度图相似性学习。在第28届ACM国际信息与知识管理会议论文集中，页码2743–2751。ACM，2019年。'
- en: '[79] Guixiang Ma, Lifang He, Bokai Cao, Jiawei Zhang, S Yu Philip, and Ann B
    Ragin. Multi-graph clustering based on interior-node topology with applications
    to brain networks. In Joint European Conference on Machine Learning and Knowledge
    Discovery in Databases, pages 476–492\. Springer, 2016.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] 马贵翔、何丽芳、曹博凯、张家伟、S Yu Philip 和 Ann B Ragin。基于内部节点拓扑的多图聚类，应用于脑网络。在欧洲机器学习和知识发现数据库联合会议上，页码476–492。Springer，2016年。'
- en: '[80] Guixiang Ma, Lifang He, Chun-Ta Lu, Weixiang Shao, Philip S Yu, Alex D
    Leow, and Ann B Ragin. Multi-view clustering with graph embedding for connectome
    analysis. In Proceedings of the 2017 ACM on Conference on Information and Knowledge
    Management, pages 127–136\. ACM, 2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] 马贵翔、何丽芳、陆春塔、邵伟翔、Philip S Yu、Alex D Leow 和 Ann B Ragin。用于联结组分析的图嵌入多视图聚类。在2017年ACM信息与知识管理会议论文集中，页码127–136。ACM，2017年。'
- en: '[81] Guixiang Ma, Chun-Ta Lu, Lifang He, S Yu Philip, and Ann B Ragin. Multi-view
    graph embedding with hub detection for brain network analysis. In 2017 IEEE International
    Conference on Data Mining (ICDM), pages 967–972\. IEEE, 2017.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] 马贵翔、陆春塔、何丽芳、S Yu Philip 和 Ann B Ragin。用于脑网络分析的具有中心检测的多视图图嵌入。在2017年IEEE数据挖掘国际会议（ICDM）上，页码967–972。IEEE，2017年。'
- en: '[82] Pierre Mahé and Jean-Philippe Vert. Graph kernels based on tree patterns
    for molecules. Machine learning, 75(1):3–35, 2009.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Pierre Mahé 和 Jean-Philippe Vert。基于树模式的分子图核。机器学习，75(1):3–35，2009年。'
- en: '[83] Franco Manessi, Alessandro Rozza, and Mario Manzo. Dynamic graph convolutional
    networks. Pattern Recognition, 97:107000, 2020.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Franco Manessi、Alessandro Rozza 和 Mario Manzo。动态图卷积网络。模式识别，97:107000，2020年。'
- en: '[84] Sascha Reinhard Nicolai Bissantz Christof Paar Marc Fyrbiak, Sebastian Wallat.
    Graph similarity and its applications to hardware security. Cryptology ePrint
    Archive, 2019.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Sascha Reinhard、Nicolai Bissantz、Christof Paar、Marc Fyrbiak 和 Sebastian
    Wallat。图相似性及其在硬件安全中的应用。密码学ePrint档案，2019年。'
- en: '[85] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka.
    Metric learning for large scale image classification: Generalizing to new classes
    at near-zero cost. In European Conference on Computer Vision, pages 488–501. Springer,
    2012.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Thomas Mensink, Jakob Verbeek, Florent Perronnin 和 Gabriela Csurka。大规模图像分类的度量学习：以接近零成本推广到新类别。发表于欧洲计算机视觉会议，
    第 488–501 页。Springer，2012 年。'
- en: '[86] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.
    Distributed representations of words and phrases and their compositionality. In
    Advances in neural information processing systems, pages 3111–3119, 2013.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado 和 Jeff Dean。词语和短语的分布式表示及其组合性。发表于神经信息处理系统进展，第
    3111–3119 页，2013 年。'
- en: '[87] Gary L Miller. Graph isomorphism, general remarks. Journal of Computer
    and System Sciences, 18(2):128–142, 1979.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Gary L Miller。图同构，一般性备注。计算机与系统科学期刊，18(2)：128–142，1979 年。'
- en: '[88] Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui
    Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations
    of graphs. arXiv preprint arXiv:1707.05005, 2017.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui
    Chen, Yang Liu 和 Shantanu Jaiswal。graph2vec：学习图的分布式表示。arXiv 预印本 arXiv:1707.05005，2017
    年。'
- en: '[89] Michel Neuhaus, Kaspar Riesen, and Horst Bunke. Fast suboptimal algorithms
    for the computation of graph edit distance. In Joint IAPR International Workshops
    on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic
    Pattern Recognition (SSPR), pages 163–172\. Springer, 2006.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Michel Neuhaus, Kaspar Riesen 和 Horst Bunke。图编辑距离计算的快速次优算法。发表于 IAPR 联合国际研讨会：统计模式识别技术（SPR）与结构和语法模式识别（SSPR），第
    163–172 页。Springer，2006 年。'
- en: '[90] Giang H Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh,
    and Sungchul Kim. Dynamic network embeddings: From random walks to temporal random
    walks. In 2018 IEEE International Conference on Big Data (Big Data), pages 1085–1092\.
    IEEE, 2018.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Giang H Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh
    和 Sungchul Kim。动态网络嵌入：从随机游走到时间随机游走。发表于 2018 IEEE 国际大数据会议（Big Data），第 1085–1092
    页。IEEE，2018 年。'
- en: '[91] Giang Hoang Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee
    Koh, and Sungchul Kim. Continuous-time dynamic network embeddings. In Companion
    Proceedings of The Web Conference 2018, pages 969–976\. International World Wide
    Web Conferences Steering Committee, 2018.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Giang Hoang Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee
    Koh 和 Sungchul Kim。连续时间动态网络嵌入。发表于 2018 年网页会议伴随论文，第 969–976 页。国际万维网会议指导委员会，2018
    年。'
- en: '[92] Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis.
    Matching node embeddings for graph similarity. In Thirty-First AAAI Conference
    on Artificial Intelligence, 2017.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Giannis Nikolentzos, Polykarpos Meladianos 和 Michalis Vazirgiannis。图相似性的节点嵌入匹配。发表于第
    31 届 AAAI 人工智能会议，2017 年。'
- en: '[93] Giannis Nikolentzos, Giannis Siglidis, and Michalis Vazirgiannis. Graph
    kernels: A survey. arXiv preprint arXiv:1904.12218, 2019.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Giannis Nikolentzos, Giannis Siglidis 和 Michalis Vazirgiannis。图核：综述。arXiv
    预印本 arXiv:1904.12218，2019 年。'
- en: '[94] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning
    of social representations. In Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining, pages 701–710\. ACM, 2014.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Bryan Perozzi, Rami Al-Rfou 和 Steven Skiena。Deepwalk：社会表示的在线学习。发表于第 20
    届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集，第 701–710 页。ACM，2014 年。'
- en: '[95] Kaspar Riesen and Horst Bunke. Iam graph database repository for graph
    based pattern recognition and machine learning. In Joint IAPR International Workshops
    on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic
    Pattern Recognition (SSPR), pages 287–297\. Springer, 2008.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Kaspar Riesen 和 Horst Bunke。用于图模式识别和机器学习的图数据库存储库。发表于 IAPR 联合国际研讨会：统计模式识别技术（SPR）与结构和语法模式识别（SSPR），第
    287–297 页。Springer，2008 年。'
- en: '[96] Kaspar Riesen and Horst Bunke. Approximate graph edit distance computation
    by means of bipartite graph matching. Image and Vision computing, 27(7):950–959,
    2009.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Kaspar Riesen 和 Horst Bunke。通过二分图匹配近似计算图编辑距离。图像与视觉计算，27(7)：950–959，2009
    年。'
- en: '[97] Ryan Rossi and Nesreen Ahmed. The network data repository with interactive
    graph analytics and visualization. In Twenty-Ninth AAAI Conference on Artificial
    Intelligence, 2015.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Ryan Rossi 和 Nesreen Ahmed。具有互动图分析和可视化的网络数据存储库。发表于第 29 届 AAAI 人工智能会议，2015
    年。'
- en: '[98] Ryan A Rossi and Nesreen K Ahmed. Role discovery in networks. IEEE Transactions
    on Knowledge and Data Engineering, 27(4):1112–1131, 2014.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Ryan A Rossi 和 Nesreen K Ahmed。网络中的角色发现。IEEE知识与数据工程汇刊，27(4):1112–1131，2014年。'
- en: '[99] Ryan A Rossi, Nesreen K Ahmed, and Eunyee Koh. Higher-order network representation
    learning. In Companion Proceedings of the The Web Conference 2018, pages 3–4\.
    International World Wide Web Conferences Steering Committee, 2018.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Ryan A Rossi、Nesreen K Ahmed 和 Eunyee Koh。高阶网络表示学习。在《Web会议2018附录会议论文集》中，第3–4页。国际万维网会议指导委员会，2018年。'
- en: '[100] Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh, Sungchul Kim, Anup Rao, and
    Yasin Abbasi-Yadkori. A structural graph representation learning framework. In
    Proceedings of the 13th International Conference on Web Search and Data Mining,
    pages 483–491, 2020.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Ryan A Rossi、Nesreen K Ahmed、Eunyee Koh、Sungchul Kim、Anup Rao 和 Yasin
    Abbasi-Yadkori。结构图表示学习框架。在第13届国际网络搜索与数据挖掘会议论文集中，第483–491页，2020年。'
- en: '[101] Ryan A Rossi, Di Jin, Sungchul Kim, Nesreen K Ahmed, Danai Koutra, and
    John Boaz Lee. On proximity and structural role-based embeddings in networks:
    Misconceptions, techniques, and applications. ACM Transactions on Knowledge Discover
    from Data, 2020.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Ryan A Rossi、Di Jin、Sungchul Kim、Nesreen K Ahmed、Danai Koutra 和 John
    Boaz Lee。在网络中基于接近度和结构角色的嵌入：误解、技术和应用。ACM数据知识发现汇刊，2020年。'
- en: '[102] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s
    distance as a metric for image retrieval. International journal of computer vision,
    40(2):99–121, 2000.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Yossi Rubner、Carlo Tomasi 和 Leonidas J Guibas。地球搬运工距离作为图像检索的度量。国际计算机视觉期刊，40(2):99–121，2000年。'
- en: '[103] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and
    Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural
    Networks, 20(1):61–80, 2008.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Franco Scarselli、Marco Gori、Ah Chung Tsoi、Markus Hagenbuchner 和 Gabriele
    Monfardini。图神经网络模型。IEEE神经网络汇刊，20(1):61–80，2008年。'
- en: '[104] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified
    embedding for face recognition and clustering. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 815–823, 2015.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Florian Schroff、Dmitry Kalenichenko 和 James Philbin。Facenet：用于面部识别和聚类的统一嵌入。在《IEEE计算机视觉与模式识别会议论文集》中，第815–823页，2015年。'
- en: '[105] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks.
    IEEE Transactions on Signal Processing, 45(11):2673–2681, 1997.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Mike Schuster 和 Kuldip K Paliwal。双向递归神经网络。IEEE信号处理汇刊，45(11):2673–2681，1997年。'
- en: '[106] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and
    Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending
    high-dimensional data analysis to networks and other irregular domains. IEEE Signal
    Processing Magazine, 30(3):83–98, 2013.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] David I Shuman、Sunil K Narang、Pascal Frossard、Antonio Ortega 和 Pierre
    Vandergheynst。图上信号处理的新兴领域：将高维数据分析扩展到网络和其他不规则领域。IEEE信号处理杂志，30(3):83–98，2013年。'
- en: '[107] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning
    with neural tensor networks for knowledge base completion. In Advances in neural
    information processing systems, pages 926–934, 2013.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Richard Socher、Danqi Chen、Christopher D Manning 和 Andrew Ng。通过神经张量网络进行知识库补全推理。在《神经信息处理系统进展》会议论文集中，第926–934页，2013年。'
- en: '[108] S Joshua Swamidass, Jonathan Chen, Jocelyne Bruand, Peter Phung, Liva
    Ralaivola, and Pierre Baldi. Kernels for small molecules and the prediction of
    mutagenicity, toxicity and anti-cancer activity. Bioinformatics, 21(suppl_1):i359–i368,
    2005.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] S Joshua Swamidass、Jonathan Chen、Jocelyne Bruand、Peter Phung、Liva Ralaivola
    和 Pierre Baldi。用于小分子的核函数及其对突变性、毒性和抗癌活性的预测。生物信息学，21(suppl_1):i359–i368，2005年。'
- en: '[109] Sabrina K Syan, Mara Smith, Benicio N Frey, Raheem Remtulla, Flavio Kapczinski,
    Geoffrey BC Hall, and Luciano Minuzzi. Resting-state functional connectivity in
    individuals with bipolar disorder during clinical remission: a systematic review.
    Journal of psychiatry & neuroscience: JPN, 43(5):298, 2018.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Sabrina K Syan、Mara Smith、Benicio N Frey、Raheem Remtulla、Flavio Kapczinski、Geoffrey
    BC Hall 和 Luciano Minuzzi。临床缓解期双相障碍患者的静息态功能连接：系统综述。《精神病学与神经科学杂志》，43(5):298，2018年。'
- en: '[110] Yu Tian, Long Zhao, Xi Peng, and Dimitris Metaxas. Rethinking kernel
    methods for node representation learning on graphs. In Advances in Neural Information
    Processing Systems, pages 11681–11692, 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Yu Tian、Long Zhao、Xi Peng 和 Dimitris Metaxas。重新思考图上节点表示学习的核方法。在《神经信息处理系统进展》会议论文集中，第11681–11692页，2019年。'
- en: '[111] Antoine J-P Tixier, Giannis Nikolentzos, Polykarpos Meladianos, and Michalis
    Vazirgiannis. Graph classification with 2d convolutional neural networks. In International
    Conference on Artificial Neural Networks, pages 578–593\. Springer, 2019.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Antoine J-P Tixier, Giannis Nikolentzos, Polykarpos Meladianos 和 Michalis
    Vazirgiannis。利用二维卷积神经网络进行图分类。收录于国际人工神经网络会议论文集，第578–593页。Springer，2019年。'
- en: '[112] Hanghang Tong, Spiros Papadimitriou, Jimeng Sun, Philip S Yu, and Christos
    Faloutsos. Colibri: fast mining of large static and dynamic graphs. In Proceedings
    of the 14th ACM SIGKDD international conference on Knowledge discovery and data
    mining, pages 686–694\. ACM, 2008.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Hanghang Tong, Spiros Papadimitriou, Jimeng Sun, Philip S Yu 和 Christos
    Faloutsos。Colibri：大规模静态和动态图的快速挖掘。收录于第14届ACM SIGKDD国际会议论文集，第686–694页。ACM，2008年。'
- en: '[113] Eleni Triantafillou, Richard Zemel, and Raquel Urtasun. Few-shot learning
    through an information retrieval lens. In Advances in Neural Information Processing
    Systems, pages 2255–2265, 2017.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Eleni Triantafillou, Richard Zemel 和 Raquel Urtasun。通过信息检索视角的少样本学习。收录于《神经信息处理系统进展》，第2255–2265页，2017年。'
- en: '[114] Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel Müller.
    Verse: Versatile graph embeddings from similarity measures. In Proceedings of
    the 2018 World Wide Web Conference, pages 539–548\. International World Wide Web
    Conferences Steering Committee, 2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Anton Tsitsulin, Davide Mottin, Panagiotis Karras 和 Emmanuel Müller.
    Verse: 多用途图嵌入从相似性度量中提取的技术。收录于2018年万维网会议论文集，第539–548页。国际万维网会议指导委员会，2018年。'
- en: '[115] David C Van Essen, Kamil Ugurbil, E Auerbach, D Barch, TEJ Behrens, R Bucholz,
    Acer Chang, Liyong Chen, Maurizio Corbetta, Sandra W Curtiss, et al. The human
    connectome project: a data acquisition perspective. Neuroimage, 62(4):2222–2231,
    2012.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] David C Van Essen, Kamil Ugurbil, E Auerbach, D Barch, TEJ Behrens, R
    Bucholz, Acer Chang, Liyong Chen, Maurizio Corbetta, Sandra W Curtiss 等人。人类连通组计划：数据采集视角。《神经影像》，62(4)：2222–2231，2012年。'
- en: '[116] S Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M
    Borgwardt. Graph kernels. Journal of Machine Learning Research, 11(Apr):1201–1242,
    2010.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] S Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor 和 Karsten M
    Borgwardt。图核。《机器学习研究期刊》，11(Apr)：1201–1242，2010年。'
- en: '[117] Milena Vujošević-Janičić, Mladen Nikolić, Dušan Tošić, and Viktor Kuncak.
    Software verification and graph similarity for automated evaluation of students’
    assignments. Information and Software Technology, 55(6):1004–1016, 2013.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Milena Vujošević-Janičić, Mladen Nikolić, Dušan Tošić 和 Viktor Kuncak。软件验证与图相似性用于自动评估学生作业。《信息与软件技术》，55(6)：1004–1016，2013年。'
- en: '[118] Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor
    spaces for chemical compound retrieval and classification. Knowledge and Information
    Systems, 14(3):347–375, 2008.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Nikil Wale, Ian A Watson 和 George Karypis。化学化合物检索和分类的描述符空间比较。《知识与信息系统》，14(3)：347–375，2008年。'
- en: '[119] Walter D Wallis, Peter Shoubridge, M Kraetz, and D Ray. Graph distances
    using graph union. Pattern Recognition Letters, 22(6-7):701–704, 2001.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Walter D Wallis, Peter Shoubridge, M Kraetz 和 D Ray。使用图并集的图距离。《模式识别通讯》，22(6-7)：701–704，2001年。'
- en: '[120] Lichen Wang, Bo Zong, Qianqian Ma, Wei Cheng, Jingchao Ni, Wenchao Yu,
    Yanchi Liu, Dongjin Song, Haifeng Chen, and Yun Fu. Inductive and unsupervised
    representation learning on graph structured objects. ICLR, 2020.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Lichen Wang, Bo Zong, Qianqian Ma, Wei Cheng, Jingchao Ni, Wenchao Yu,
    Yanchi Liu, Dongjin Song, Haifeng Chen 和 Yun Fu。图结构对象上的归纳和无监督表示学习。ICLR，2020年。'
- en: '[121] Runzhong Wang, Junchi Yan, and Xiaokang Yang. Learning combinatorial
    embedding networks for deep graph matching. arXiv preprint arXiv:1904.00597, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Runzhong Wang, Junchi Yan 和 Xiaokang Yang。学习组合嵌入网络用于深度图匹配。arXiv 预印本 arXiv:1904.00597，2019年。'
- en: '[122] Shen Wang, Zhengzhang Chen, Xiao Yu, Ding Li, Jingchao Ni, Lu-An Tang,
    Jiaping Gui, Zhichun Li, Haifeng Chen, and Philip S. Yu. Heterogeneous graph matching
    networks for unknown malware detection. In Proceedings of the Twenty-Eighth International
    Joint Conference on Artificial Intelligence, IJCAI-19, pages 3762–3770. International
    Joint Conferences on Artificial Intelligence Organization, 7 2019.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Shen Wang, Zhengzhang Chen, Xiao Yu, Ding Li, Jingchao Ni, Lu-An Tang,
    Jiaping Gui, Zhichun Li, Haifeng Chen 和 Philip S. Yu。异质图匹配网络用于未知恶意软件检测。收录于第28届国际人工智能联合会议，IJCAI-19，第3762–3770页。国际人工智能联合会议组织，2019年7月。'
- en: '[123] Yaqing Wang and Quanming Yao. Few-shot learning: A survey. arXiv preprint
    arXiv:1904.05046, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Yaqing Wang 和 Quanming Yao. 少样本学习：综述。arXiv 预印本 arXiv:1904.05046，2019年。'
- en: '[124] Baoxin Wu, Chunfeng Yuan, and Weiming Hu. Human action recognition based
    on context-dependent graph kernels. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2609–2616, 2014.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Baoxin Wu, Chunfeng Yuan 和 Weiming Hu. 基于上下文依赖图核的人类动作识别。IEEE计算机视觉与模式识别会议论文集，2609–2616页，2014年。'
- en: '[125] Bo Wu, Yang Liu, Bo Lang, and Lei Huang. Dgcnn: Disordered graph convolutional
    neural network based on the gaussian mixture model. Neurocomputing, 321:346–356,
    2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Bo Wu, Yang Liu, Bo Lang 和 Lei Huang. Dgcnn：基于高斯混合模型的无序图卷积神经网络。神经计算，321:346–356，2018年。'
- en: '[126] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
    S Yu Philip. A comprehensive survey on graph neural networks. IEEE Transactions
    on Neural Networks and Learning Systems, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang 和 S
    Yu Philip. 图神经网络的综合调查。IEEE 神经网络与学习系统学报，2020年。'
- en: '[127] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
    Philip S Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596,
    2019.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang 和 Philip
    S Yu. 图神经网络的综合调查。arXiv 预印本 arXiv:1901.00596，2019年。'
- en: '[128] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song. Neural
    network-based graph embedding for cross-platform binary code similarity detection.
    In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
    Security, pages 363–376\. ACM, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song 和 Dawn Song. 基于神经网络的图嵌入用于跨平台二进制代码相似性检测。第2017届ACM
    SIGSAC计算机与通信安全会议论文集，363–376页。ACM，2017年。'
- en: '[129] Xifeng Yan, Philip S Yu, and Jiawei Han. Substructure similarity search
    in graph databases. In Proceedings of the 2005 ACM SIGMOD international conference
    on Management of data, pages 766–777\. ACM, 2005.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Xifeng Yan, Philip S Yu 和 Jiawei Han. 图数据库中的子结构相似性搜索。2005年ACM SIGMOD国际数据管理会议论文集，766–777页。ACM，2005年。'
- en: '[130] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings
    of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining, pages 1365–1374\. ACM, 2015.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Pinar Yanardag 和 SVN Vishwanathan. 深度图核。第21届ACM SIGKDD国际知识发现与数据挖掘大会论文集，1365–1374页。ACM，2015年。'
- en: '[131] Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
    Gnn explainer: A tool for post-hoc explanation of graph neural networks. arXiv
    preprint arXiv:1903.03894, 2019.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik 和 Jure Leskovec.
    Gnn explainer：图神经网络后验解释工具。arXiv 预印本 arXiv:1903.03894，2019年。'
- en: '[132] Tomoki Yoshida, Ichiro Takeuchi, and Masayuki Karasuyama. Learning interpretable
    metric between graphs: Convex formulation and computation with graph mining. In
    Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining, pages 1026–1036\. ACM, 2019.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Tomoki Yoshida, Ichiro Takeuchi 和 Masayuki Karasuyama. 学习图之间的可解释度度量：凸优化形式与图挖掘计算。第25届ACM
    SIGKDD国际知识发现与数据挖掘大会论文集，1026–1036页。ACM，2019年。'
- en: '[133] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional
    networks: A deep learning framework for traffic forecasting. arXiv preprint arXiv:1709.04875,
    2017.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Bing Yu, Haoteng Yin 和 Zhanxing Zhu. 时空图卷积网络：一种用于交通预测的深度学习框架。arXiv 预印本
    arXiv:1709.04875，2017年。'
- en: '[134] Andrei Zanfir and Cristian Sminchisescu. Deep learning of graph matching.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 2684–2693, 2018.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Andrei Zanfir 和 Cristian Sminchisescu. 图匹配的深度学习。IEEE计算机视觉与模式识别会议论文集，2684–2693页，2018年。'
- en: '[135] Zhiping Zeng, Anthony KH Tung, Jianyong Wang, Jianhua Feng, and Lizhu
    Zhou. Comparing stars: On approximating graph edit distance. Proceedings of the
    VLDB Endowment, 2(1):25–36, 2009.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Zhiping Zeng, Anthony KH Tung, Jianyong Wang, Jianhua Feng 和 Lizhu Zhou.
    比较星：关于图编辑距离的近似。VLDB 会议录，2(1):25–36，2009年。'
- en: '[136] Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. Network representation
    learning: A survey. IEEE transactions on Big Data, 2018.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Daokun Zhang, Jie Yin, Xingquan Zhu 和 Chengqi Zhang. 网络表示学习：综述。IEEE 大数据学报，2018年。'
- en: '[137] Si Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejewski. Graph convolutional
    networks: Algorithms, applications and open challenges. In International Conference
    on Computational Social Networks, pages 79–91\. Springer, 2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Si Zhang, Hanghang Tong, Jiejun Xu 和 Ross Maciejewski. 图卷积网络：算法、应用与开放挑战。国际计算社会网络会议论文集，79–91页。Springer，2018年。'
- en: '[138] Weiguo Zheng, Lei Zou, Xiang Lian, Dong Wang, and Dongyan Zhao. Graph
    similarity search with edit distance constraint in large graph databases. In Proceedings
    of the 22nd ACM international conference on Information & Knowledge Management,
    pages 1595–1600\. ACM, 2013.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] 郑伟国、邹磊、连翔、王东和赵东燕。具有编辑距离约束的大型图数据库中的图相似性搜索。载于第22届ACM国际信息与知识管理会议论文集，1595–1600页。ACM，2013年。'
- en: '[139] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng
    Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods
    and applications. arXiv preprint arXiv:1812.08434, 2018.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] 周杰、崔甘渠、张正炎、杨成、刘智远、王立峰、李长城和孙茂松。图神经网络：方法与应用综述。arXiv预印本 arXiv:1812.08434，2018年。'
