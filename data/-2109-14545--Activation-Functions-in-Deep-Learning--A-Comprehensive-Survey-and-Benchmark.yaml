- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:51:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:51:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2109.14545] Activation Functions in Deep Learning: A Comprehensive Survey
    and Benchmark'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2109.14545] 深度学习中的激活函数：综合调查与基准测试'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2109.14545](https://ar5iv.labs.arxiv.org/html/2109.14545)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2109.14545](https://ar5iv.labs.arxiv.org/html/2109.14545)
- en: 'Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的激活函数：综合调查与基准测试
- en: Shiv Ram Dubey¹, Satish Kumar Singh¹, Bidyut Baran Chaudhuri² ¹Computer Vision
    and Biometrics Laboratory, Indian Institute of Information Technology, Allahabad,
    India.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Shiv Ram Dubey¹, Satish Kumar Singh¹, Bidyut Baran Chaudhuri² ¹Computer Vision
    and Biometrics Laboratory, Indian Institute of Information Technology, Allahabad,
    India。
- en: ²Techno India University, Kolkata, India and Indian Statistical Institute, Kolkata,
    India.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²Techno India University, Kolkata, India 和 Indian Statistical Institute, Kolkata,
    India。
- en: srdubey@iiita.ac.in, sk.singh@iiita.ac.in, bidyutbaranchaudhuri@gmail.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: srdubey@iiita.ac.in, sk.singh@iiita.ac.in, bidyutbaranchaudhuri@gmail.com
- en: This paper is accepted in Neurocomputing. Copyright will be transferred to Elsevier.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文已被《Neurocomputing》接受。版权将转移至Elsevier。
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Neural networks have shown tremendous growth in recent years to solve numerous
    problems. Various types of neural networks have been introduced to deal with different
    types of problems. However, the main goal of any neural network is to transform
    the non-linearly separable input data into more linearly separable abstract features
    using a hierarchy of layers. These layers are combinations of linear and nonlinear
    functions. The most popular and common non-linearity layers are activation functions
    (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper,
    a comprehensive overview and survey is presented for AFs in neural networks for
    deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based,
    ReLU based, ELU based, and Learning based are covered. Several characteristics
    of AFs such as output range, monotonicity, and smoothness are also pointed out.
    A performance comparison is also performed among 18 state-of-the-art AFs with
    different networks on different types of data. The insights of AFs are presented
    to benefit the researchers for doing further research and practitioners to select
    among different choices. The code used for experimental comparison is released
    at: [https://github.com/shivram1987/ActivationFunctions](https://github.com/shivram1987/ActivationFunctions).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在近年来显示出了巨大的增长，用于解决众多问题。各种类型的神经网络已经被引入，以应对不同类型的问题。然而，任何神经网络的主要目标是使用一系列层将非线性可分的输入数据转换为更线性可分的抽象特征。这些层是线性和非线性函数的组合。最流行和常见的非线性层是激活函数（AFs），如Logistic
    Sigmoid、Tanh、ReLU、ELU、Swish和Mish。本文对神经网络中深度学习的AFs进行了全面概述和调查。涵盖了不同类别的AFs，如基于Logistic
    Sigmoid和Tanh的、基于ReLU的、基于ELU的和基于学习的。还指出了AFs的几个特征，如输出范围、单调性和光滑性。还对18种最先进的AFs在不同网络和不同类型的数据上的性能进行了比较。AFs的见解旨在为研究人员提供进一步研究的参考，并为从业人员在不同选择中进行选择提供帮助。用于实验比较的代码发布在：[https://github.com/shivram1987/ActivationFunctions](https://github.com/shivram1987/ActivationFunctions)。
- en: '^†^†journal: Neurocomputing'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†期刊：Neurocomputing
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In recent years, deep learning has shown a tremondous growth to solve the challenging
    problems such as object detection [[1](#bib.bib1)], semantic segmentation [[2](#bib.bib2)],
    person re-identification [[3](#bib.bib3)], image retrieval [[4](#bib.bib4)], anomaly
    detection [[5](#bib.bib5)], skin disease diagnosis [[6](#bib.bib6)], and many
    more. Various types of neural networks have been defined in deep learning to learn
    abstract features from data, such as Multilayer Perceptron (MLP) [[7](#bib.bib7)],
    Convolutional Neural Networks (CNN) [[8](#bib.bib8)], Recurrent Neural Networks
    (RNN) [[9](#bib.bib9)], and Generative Adversarial Networks (GAN) [[10](#bib.bib10)].
    The important aspects of neural networks include weight initialization [[11](#bib.bib11)],
    loss functions [[12](#bib.bib12)], different layers [[13](#bib.bib13)], overfitting
    [[14](#bib.bib14)], and optimization [[15](#bib.bib15)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习在解决诸如物体检测[[1](#bib.bib1)]、语义分割[[2](#bib.bib2)]、人物重新识别[[3](#bib.bib3)]、图像检索[[4](#bib.bib4)]、异常检测[[5](#bib.bib5)]、皮肤疾病诊断[[6](#bib.bib6)]等具有挑战性的问题方面表现出了巨大的增长。深度学习中定义了各种类型的神经网络，以从数据中学习抽象特征，例如多层感知器（MLP）[[7](#bib.bib7)]、卷积神经网络（CNN）[[8](#bib.bib8)]、递归神经网络（RNN）[[9](#bib.bib9)]和生成对抗网络（GAN）[[10](#bib.bib10)]。神经网络的重要方面包括权重初始化[[11](#bib.bib11)]、损失函数[[12](#bib.bib12)]、不同层次[[13](#bib.bib13)]、过拟合[[14](#bib.bib14)]和优化[[15](#bib.bib15)]。
- en: 'The activation functions (AFs) play a very crucial role in neural networks
    [[16](#bib.bib16)] by learning the abstract features through non-linear transformations.
    Some common properties of the AFs are as follows: a) it should add the non-linear
    curvature in the optimization landscape to improve the training convergence of
    the network; b) it should not increase the computational complexity of the model
    extensively; c) it should not hamper the gradient flow during training; d) it
    should retain the distribution of data to facilitate the better training of the
    network. Several AFs have been explored in recent years for deep learning to achieve
    the above mentioned properties. This survey is dedicated to the developments in
    the area of AFs in neural networks. The insights of the different AFs are presented
    along with the reasoning to benefit the deep learning community. The major contributions
    of this survey are outlined as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数（AFs）在神经网络中扮演着非常关键的角色[[16](#bib.bib16)]，通过非线性变换学习抽象特征。激活函数的一些常见属性如下：a) 应该在优化景观中添加非线性曲率，以提高网络的训练收敛性；b)
    不应大幅增加模型的计算复杂性；c) 不应阻碍训练过程中的梯度流；d) 应该保留数据的分布，以促进网络的更好训练。近年来，为了实现上述属性，已经探索了几种激活函数。本调查专注于神经网络中激活函数领域的发展。不同激活函数的见解及其理由被呈现出来，以造福深度学习社区。本调查的主要贡献如下：
- en: '1.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: This survey provides a detailed classification for a wide range of AFs. It also
    includes the AFs very comprehensively, including Logistic Sigmoid/Tanh, Rectified
    Unit, Exponential Unit, and Adaptive AFs.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查提供了对广泛激活函数的详细分类。它还非常全面地涵盖了激活函数，包括逻辑斯蒂函数/双曲正切函数、整流单元、指数单元和自适应激活函数。
- en: '2.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: This survey enriches the reader with the state-of-the-art AFs with analysis
    from various perspectives. It specifically covers the progress in AFs for deep
    learning.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查从各个角度丰富了读者对最先进激活函数的分析。它特别涵盖了深度学习中激活函数的发展。
- en: '3.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'This survey also summarizes the AFs with brief highlights and important discussions
    to depict its suitability for different types of data (Refer to Table [6](#S7.T6
    "Table 6 ‣ 7.5 Kernel Activation Functions ‣ 7 Miscellaneous Activation Functions
    ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")).'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '本调查还总结了激活函数的简要亮点和重要讨论，以描绘其对不同数据类型的适用性（参见表格 [6](#S7.T6 "Table 6 ‣ 7.5 Kernel
    Activation Functions ‣ 7 Miscellaneous Activation Functions ‣ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark")）。'
- en: '4.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'This survey is compared with the existing survey and performance analysis to
    show its importance (Refer to Table [7](#S9.T7 "Table 7 ‣ 9.1 Comparison with
    Existing Survey/Performance Analysis ‣ 9 Performance Comparison and Analysis ‣
    Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")).'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '本调查与现有调查和性能分析进行了比较，以显示其重要性（参见表格 [7](#S9.T7 "Table 7 ‣ 9.1 Comparison with Existing
    Survey/Performance Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark")）。'
- en: '5.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'This paper also presents the performance comparisons on 4 benchmark datasets
    of different modalities using 18 state-of-the-art AFs with different types of
    networks (Refer to Tables [8](#S9.T8 "Table 8 ‣ 9.1 Comparison with Existing Survey/Performance
    Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"), [9](#S9.T9 "Table 9 ‣ 9.1 Comparison
    with Existing Survey/Performance Analysis ‣ 9 Performance Comparison and Analysis
    ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")
    and [11](#S9.T11 "Table 11 ‣ 9.2 Experimental Performance Analysis ‣ 9 Performance
    Comparison and Analysis ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")).'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '本文还展示了在4个不同模态的基准数据集上使用18种最先进的 AFs 和不同类型网络的性能比较（请参见表[8](#S9.T8 "Table 8 ‣ 9.1
    Comparison with Existing Survey/Performance Analysis ‣ 9 Performance Comparison
    and Analysis ‣ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark")、[9](#S9.T9 "Table 9 ‣ 9.1 Comparison with Existing Survey/Performance
    Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") 和 [11](#S9.T11 "Table 11 ‣ 9.2
    Experimental Performance Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark")）。'
- en: 'The evolution of AFs is illustrated in Section [2](#S2 "2 Evolution of Activation
    Functions ‣ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark"). The progress in Logistic Sigmoid and Tanh, rectified, exponential,
    adaptive and miscellaneous AFs are summarized in Section [3](#S3 "3 Logistic Sigmoid
    and Tanh Based AFs ‣ Activation Functions in Deep Learning: A Comprehensive Survey
    and Benchmark"), [4](#S4 "4 Rectified Activation Functions ‣ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark"), [5](#S5 "5 Exponential
    Activation Functions ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark"), [6](#S6 "6 Learning/Adaptive Activation Functions ‣ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark"), and [7](#S7
    "7 Miscellaneous Activation Functions ‣ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark"), respectively. Some aspects of AFs are
    discussed in Section [8](#S8 "8 Aspects of Activation Functions ‣ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark"). A comprehensive performance
    analysis is conducted in Section [9](#S9 "9 Performance Comparison and Analysis
    ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark").
    A summary with conclusions and recommendations is provided in Section [10](#S10
    "10 Conclusion and Recommendations ‣ Activation Functions in Deep Learning: A
    Comprehensive Survey and Benchmark").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'AFs 的演变在第[2](#S2 "2 Evolution of Activation Functions ‣ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark")节中进行了说明。Logistic Sigmoid
    和 Tanh、整流、指数、自适应及其他各种 AFs 的进展分别总结在第[3](#S3 "3 Logistic Sigmoid and Tanh Based
    AFs ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")、[4](#S4
    "4 Rectified Activation Functions ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")、[5](#S5 "5 Exponential Activation Functions ‣ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark")、[6](#S6 "6
    Learning/Adaptive Activation Functions ‣ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark")和[7](#S7 "7 Miscellaneous Activation Functions
    ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")节中讨论。AFs
    的一些方面在第[8](#S8 "8 Aspects of Activation Functions ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark")节中进行了讨论。第[9](#S9 "9 Performance
    Comparison and Analysis ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")节进行了全面的性能分析。第[10](#S10 "10 Conclusion and Recommendations
    ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")节提供了总结、结论和建议。'
- en: 2 Evolution of Activation Functions
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 激活函数的演变
- en: 'A linear function can be thought of as a simple AF which outputs $c\times x$
    for input $x$ with $c$ as a constant. The linear AF is illustrated in Fig. [1](#S2.F1
    "Figure 1 ‣ 2 Evolution of Activation Functions ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") for $c=1$, i.e., identity function.
    Note that the linear AF does not add non-linearity into the network. However,
    the non-linearity needs to be introduced in the neural networks. Otherwise, a
    neural network produces the output as a linear function of inputs inspite of having
    several layers. Moreover, in practice data is generally not linearly separable;
    hence, the non-linear layers help to project the data in non-linear fashion in
    feature space which can be used with different objective functions. This section
    provides an overview of the evolution of AFs for deep learning. A classification
    is presented in Fig. [2](#S2.F2 "Figure 2 ‣ 2 Evolution of Activation Functions
    ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")
    in terms of the different properties and characteristic types.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '线性函数可以被认为是一个简单的 AF，对于输入 $x$ 输出 $c\times x$，其中 $c$ 是一个常数。线性 AF 在图 [1](#S2.F1
    "Figure 1 ‣ 2 Evolution of Activation Functions ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") 中为 $c=1$（即单位函数）进行了说明。请注意，线性 AF
    不会为网络引入非线性。然而，神经网络需要引入非线性。否则，即使有多层，神经网络也会将输出视为输入的线性函数。此外，在实际应用中，数据通常不是线性可分的；因此，非线性层有助于在特征空间中以非线性方式投影数据，这可以与不同的目标函数一起使用。本节概述了深度学习中
    AF 的演变。图 [2](#S2.F2 "Figure 2 ‣ 2 Evolution of Activation Functions ‣ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark") 展示了不同属性和特征类型的分类。'
- en: '![Refer to caption](img/feda0172686c2a943b0799c0d5215638.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/feda0172686c2a943b0799c0d5215638.png)'
- en: 'Figure 1: An illustration of Linear, Logistic Sigmoid and Tanh AFs.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：线性、Logistic Sigmoid 和 Tanh AFs 的示意图。
- en: 'Logistic Sigmoid/Tanh Unit Based Activation Functions: In order to introduce
    the non-linearity into the neural networks, the Logistic Sigmoid and Tanh AFs
    have been used in the early days. The firing of bilogical neurons was the motivation
    of using the Logistic Sigmoid and Tanh AFs with artificial neurons. The Logistic
    Sigmoid AF is a very popular and traditional non-linear function. It is given
    as,'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Logistic Sigmoid/Tanh 单位基础的激活函数：为了将非线性引入神经网络，早期使用了 Logistic Sigmoid 和 Tanh AF。生物神经元的激发是使用
    Logistic Sigmoid 和 Tanh AF 与人工神经元的动机。Logistic Sigmoid AF 是一个非常流行和传统的非线性函数。其形式为，
- en: '|  | $\text{Logistic Sigmoid}(x)=\frac{1}{1+e^{-x}}.$ |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Logistic Sigmoid}(x)=\frac{1}{1+e^{-x}}.$ |  | (1) |'
- en: 'This AF squashes the output between [$0$, $1$] as shown in Fig. [1](#S2.F1
    "Figure 1 ‣ 2 Evolution of Activation Functions ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"). The output of the Logistic Sigmoid
    function is saturated for higher and lower inputs, which leads to vanishing gradient
    problem. The vanishing gradient problem depicts to a scenario where the gradient
    of objective function w.r.t. a parameter becomes very close to zero and leads
    to almost no update in the parameters during the training of the network using
    stochastic gradient descent technique. Hence, the training is almost killed under
    vanishing gradient scenario. Moreover, the output not following a zero-centric
    nature leads to poor convergence. The Tanh function has also been used as the
    AF in neural networks. It is similar to the Logistic Sigmoid function while exhibiting
    the zero centric property as depicted in Fig. [1](#S2.F1 "Figure 1 ‣ 2 Evolution
    of Activation Functions ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark"). The Tanh function is written as,'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '这个 AF 将输出压缩到[$0$, $1$]之间，如图 [1](#S2.F1 "Figure 1 ‣ 2 Evolution of Activation
    Functions ‣ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark")所示。Logistic Sigmoid 函数的输出在较高和较低输入时会饱和，这导致了梯度消失问题。梯度消失问题描述了一个场景，其中目标函数相对于某个参数的梯度变得非常接近零，导致在使用随机梯度下降技术训练网络时，参数几乎没有更新。因此，在梯度消失场景下，训练几乎会被“杀死”。此外，输出不遵循零中心特性会导致收敛性差。Tanh
    函数也被用作神经网络中的 AF。它类似于 Logistic Sigmoid 函数，同时展现了零中心特性，如图 [1](#S2.F1 "Figure 1 ‣
    2 Evolution of Activation Functions ‣ Activation Functions in Deep Learning: A
    Comprehensive Survey and Benchmark")所示。Tanh 函数表示为，'
- en: '|  | $\text{Tanh}(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}.$ |  | (2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Tanh}(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}.$ |  | (2) |'
- en: 'The Tanh function also squashes the inputs, but in $[-1,1]$. The drawbacks
    of Logistic Sigmoid function such as vanishing gradient and computational complexity
    also exist with Tanh function. The Logistic Sigmoid and Tanh AFs majorly suffer
    from vanishing gradient. Several improvements have been proposed based on the
    Logistic Sigmoid and Tanh AFs which are described in Section [3](#S3 "3 Logistic
    Sigmoid and Tanh Based AFs ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark") in detail.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '双曲正切函数也将输入压缩，但范围在$[-1,1]$。双曲正切函数也存在与Logistic Sigmoid函数类似的缺点，如梯度消失和计算复杂性。Logistic
    Sigmoid和双曲正切AF主要遭遇梯度消失的问题。基于Logistic Sigmoid和双曲正切AF的几项改进已在[3](#S3 "3 Logistic
    Sigmoid and Tanh Based AFs ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")节中详细描述。'
- en: '![Refer to caption](img/73af0dc8a5973ff1c8fc86ce1160d453.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/73af0dc8a5973ff1c8fc86ce1160d453.png)'
- en: 'Figure 2: Classification of activation functions.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：激活函数的分类。
- en: 'Rectified Linear Unit Based Activation Functions: The saturated output and
    increased complexity are the key limitations of above-mentioned Logistic Sigmoid
    and Tanh based AFs. The Rectified Linear Unit (ReLU) [[17](#bib.bib17)] has become
    the state-of-the-art AF due to its simplicity and improved performance. The ReLU
    was also used in the AlexNet model [[8](#bib.bib8)]. Various variants of ReLU
    have been investigated by tackling its drawbacks, such as non-utilization of negative
    values, limited non-linearity and unbounded output, as detailed in Section [4](#S4
    "4 Rectified Activation Functions ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '经过修正的线性单元（ReLU）基础的激活函数：饱和输出和增加的复杂性是上述Logistic Sigmoid和双曲正切基础的激活函数的主要限制。由于其简单性和性能提升，经过修正的线性单元（ReLU）[[17](#bib.bib17)]
    已成为最先进的激活函数。ReLU也被用于AlexNet模型 [[8](#bib.bib8)]。已通过解决其缺点（如负值未利用、有限的非线性和无界输出）对ReLU的各种变体进行了研究，详细内容见[4](#S4
    "4 Rectified Activation Functions ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")节。'
- en: 'Table 1: Advantage and disadvantage of primary AFs.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：主要激活函数的优缺点。
- en: '| AFs | Diminishing | Limited | Optimization | Lack of | Computational |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | 渐减 | 有限 | 优化 | 缺乏 | 计算 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| gradients | non-linearity | difficulty | adaptibility | inefficiency |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 梯度 | 非线性 | 难度 | 适应性 | 低效 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Sigmoid | Yes | No | Yes | Yes | Yes |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | 是 | 否 | 是 | 是 | 是 |'
- en: '| Tanh | Yes | No | Partial | Yes | Yes |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 双曲正切（Tanh） | 是 | 否 | 部分 | 是 | 是 |'
- en: '| ReLU | Partial | Yes | Partial | Yes | No |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ReLU | 部分 | 是 | 部分 | 是 | 否 |'
- en: '| ELU | No | Partial | No | Yes | Partial |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ELU | 否 | 部分 | 否 | 是 | 部分 |'
- en: '| APL | No | Partial | No | No | No |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| APL | 否 | 部分 | 否 | 否 | 否 |'
- en: '| Swish | No | Partial | No | No | Partial |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Swish | 否 | 部分 | 否 | 否 | 部分 |'
- en: 'Table 2: Summary of Logistic Sigmoid and Tanh based activation functions.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：Logistic Sigmoid和双曲正切基础的激活函数总结。
- en: '| Name of AF | Parametric | Monotonic | Smooth | Bounded |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数名称 | 参数化 | 单调 | 光滑 | 有界 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Logistic Sigmoid | No | Yes | Yes | Yes |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Logistic Sigmoid | 否 | 是 | 是 | 是 |'
- en: '| Tanh | No | Yes | Yes | Yes |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 双曲正切（Tanh） | 否 | 是 | 是 | 是 |'
- en: '| Scaled Tanh (sTanh), 1998 [[18](#bib.bib18)] | Yes | Yes | Yes | Yes |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 缩放的双曲正切（sTanh），1998 [[18](#bib.bib18)] | 是 | 是 | 是 | 是 |'
- en: '| Rectified Hyperbolic Secant (ReSech), 2016 [[19](#bib.bib19)] | No | No |
    Yes | Yes |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 经过修正的双曲正割（ReSech），2016 [[19](#bib.bib19)] | 否 | 否 | 是 | 是 |'
- en: '| Scaled Sigmoid (sSigmoid), 2016 [[20](#bib.bib20)] | No | Yes | Yes | Yes
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 缩放Sigmoid（sSigmoid），2016 [[20](#bib.bib20)] | 否 | 是 | 是 | 是 |'
- en: '| Penalized Tanh (pTanh), 2016 [[20](#bib.bib20)] | No | Yes | No | Yes |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 罚项双曲正切（pTanh），2016 [[20](#bib.bib20)] | 否 | 是 | 否 | 是 |'
- en: '| Hexpo, 2017 [[21](#bib.bib21)] | No | Yes | Yes | Yes |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Hexpo，2017 [[21](#bib.bib21)] | 否 | 是 | 是 | 是 |'
- en: '| Improved Sigmoid (ISigmoid), 2018 [[22](#bib.bib22)] | No | Yes | Yes | No
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 改进的Sigmoid（ISigmoid），2018 [[22](#bib.bib22)] | 否 | 是 | 是 | 否 |'
- en: '| Sigmoid-Weighted Linear Units (SiLU), 2018 [[23](#bib.bib23)] | No | No |
    Yes | For negative inputs |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid加权线性单元（SiLU），2018 [[23](#bib.bib23)] | 否 | 否 | 是 | 对于负输入 |'
- en: '| Linearly Scaled Hyperbolic Tangent (LiSHT), 2019 [[24](#bib.bib24)] | No
    | No | Yes | No |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 线性缩放双曲正切（LiSHT），2019 [[24](#bib.bib24)] | 否 | 否 | 是 | 否 |'
- en: '| Elliott, 2019 [[25](#bib.bib25)] | No | Yes | Yes | Yes |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Elliott，2019 [[25](#bib.bib25)] | 否 | 是 | 是 | 是 |'
- en: '| Soft-Root-Sign (SRS), 2020 [[26](#bib.bib26)] | Yes | No | Yes | Yes |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Soft-Root-Sign (SRS)，2020 [[26](#bib.bib26)] | 是 | 否 | 是 | 是 |'
- en: 'Exponential Unit Based Activation Functions: The major problem faced by the
    Logistic Sigmoid and Tanh based AFs is with its saturated output for large positive
    and negative input. Similarly, the major problem with ReLU based AFs is with the
    under-utilization of negative values leading to vanishing gradient. In order to
    cope up with these limitations the exponential function based AFs have been used
    in the literature. The Exponential Linear Unit (ELU) [[27](#bib.bib27)] based
    AF utilizes the negative values with the help of the exponential function. Several
    AFs have been introduced in the literature as the ELU variants which are presented
    in Section [5](#S5 "5 Exponential Activation Functions ‣ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark") in detail.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '指数单位基激活函数：Logistic Sigmoid 和 Tanh 基于 AF 面临的主要问题是对于大正值和负值输入的饱和输出。类似地，ReLU 基于
    AF 的主要问题是负值的利用不足，导致梯度消失。为了应对这些限制，文献中使用了基于指数函数的 AF。指数线性单元（ELU）[[27](#bib.bib27)]
    基于 AF 利用指数函数来处理负值。文献中介绍了几种 ELU 变体，如第[5](#S5 "5 Exponential Activation Functions
    ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")节中详细介绍。'
- en: 'Learning/Adaptive Activation Functions: Most of the Sigmoid, Tanh, ReLU, and
    ELU based AFs are designed manually which might not be able to exploit the data
    complexity. The learning based adaptive AFs are the recent trends. This class
    of AFs contains learnable parameters, e.g. Adaptive Piecewise Linear (APL) [[28](#bib.bib28)]
    and Swish [[29](#bib.bib29)] AFs contain two and one learnable parameters, respectively.
    Recently, several learning based AFs have been proposed as illustrated in Section
    [6](#S6 "6 Learning/Adaptive Activation Functions ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '学习/自适应激活函数：大多数基于 Sigmoid、Tanh、ReLU 和 ELU 的 AF 是手动设计的，可能无法充分利用数据的复杂性。基于学习的自适应
    AF 是近期的趋势。这类 AF 包含可学习的参数，例如自适应分段线性（APL）[[28](#bib.bib28)] 和 Swish [[29](#bib.bib29)]
    AF 分别包含两个和一个可学习的参数。最近，已经提出了几种基于学习的 AF，如第[6](#S6 "6 Learning/Adaptive Activation
    Functions ‣ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark")节所示。'
- en: 'Miscellaneous Activation Functions: In recent years, many other AFs have also
    been investigated as presented in Section [7](#S7 "7 Miscellaneous Activation
    Functions ‣ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark"). These activations include Softplus units, probabilistic functions,
    polynomial functions, and kernel functions.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '杂项激活函数：近年来，许多其他 AF 也被研究，如第[7](#S7 "7 Miscellaneous Activation Functions ‣ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark")节所述。这些激活函数包括
    Softplus 单元、概率函数、多项式函数和核函数。'
- en: 'Table [1](#S2.T1 "Table 1 ‣ 2 Evolution of Activation Functions ‣ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark") highlights
    the advantage and disadvantage of the primary AFs in terms of the diminishing
    gradients, limited non-linearity, optimization difficulty, computational inefficiency
    and lack of adaptibility. It can be noticed that the Tanh function is computationally
    inefficient because it involves the computation of exponential multiple times
    [[30](#bib.bib30)]. However, in implementation it can be computed using single
    exponential with the help of Sigmoid function. These limitations in the existing
    AFs have been the driving factors for the development of recent AFs as surveyed
    in the further sections of this paper.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S2.T1 "Table 1 ‣ 2 Evolution of Activation Functions ‣ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark")突出了主要 AF 在梯度消失、非线性限制、优化难度、计算低效和适应性不足方面的优缺点。可以注意到，Tanh
    函数在计算上效率低，因为它涉及多次计算指数[[30](#bib.bib30)]。然而，在实现中可以通过 Sigmoid 函数的帮助来使用单次指数计算。这些现有
    AF 的限制是驱动近期 AF 发展的因素，如本文进一步章节所述。'
- en: 3 Logistic Sigmoid and Tanh Based AFs
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 Logistic Sigmoid 和 Tanh 基于 AF
- en: 'The traditional AFs such as Logistic Sigmoid and Tanh were used very extensively
    in the early days of neural networks. However, these AFs had shown the hurdle
    to train the deep networks due to their saturated output. Several attempts have
    also been made to improve these AFs for different networks. Table [2](#S2.T2 "Table
    2 ‣ 2 Evolution of Activation Functions ‣ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") presents the comparison of Logistic Sigmoid
    and Tanh based AFs in terms of their properties including parametric, monotonic,
    smooth and bounded.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '传统的AFs，如Logistic Sigmoid和Tanh，在神经网络的早期被广泛使用。然而，这些AFs由于饱和输出而使得深度网络的训练受阻。为了改进这些AFs，也进行了多次尝试。表[2](#S2.T2
    "Table 2 ‣ 2 Evolution of Activation Functions ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark")对Logistic Sigmoid和Tanh基础AFs的属性进行了比较，包括参数化、单调、平滑和有界。'
- en: In order to tackle the limited output range and zero gradient problems of Tanh,
    a scaled Hyperbolic Tangent (sTanh) is used in [[18](#bib.bib18)] which is defined
    as,
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决Tanh的输出范围有限和零梯度的问题，使用了scaled Hyperbolic Tangent (sTanh)[[18](#bib.bib18)]，其定义如下，
- en: '|  | $sTanh(x)=A\times Tanh(B\times x)$ |  | (3) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $sTanh(x)=A\times Tanh(B\times x)$ |  | (3) |'
- en: with the output range in $[-A,A]$. A Parametric Sigmoid Function (PSF) is proposed
    as a continuous, differentiable, and bounded function as,
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-A,A]$。提出了Parametric Sigmoid Function (PSF)作为一个连续、可微分、有界函数，如下，
- en: '|  | $PSF(x)=\frac{1}{(1+e^{-x})^{m}}$ |  | (4) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $PSF(x)=\frac{1}{(1+e^{-x})^{m}}$ |  | (4) |'
- en: where $m$ is a hyperparameter [[31](#bib.bib31)]. The gradient flow is improved
    for the higher value of $m$. The sum of shifted log-sigmoid is also explored as
    an AF [[32](#bib.bib32)] which retains the symmetry in the generated features.
    The Rectified Hyperbolic Secant (ReSech) AF is differentiable, symmetric, and
    bounded [[19](#bib.bib19)] which is given as,
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$m$是一个超参数[[31](#bib.bib31)]。当$m$取较高值时，梯度流会得到改善。作为AF的shifted log-sigmoid的和也被探索，保持了生成特征的对称性[[32](#bib.bib32)]。Rectified
    Hyperbolic Secant (ReSech) AF是可微分的、对称的，并且有界[[19](#bib.bib19)]，其定义如下，
- en: '|  | $ReSech(x)=x\times Sech(x)$ |  | (5) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $ReSech(x)=x\times Sech(x)$ |  | (5) |'
- en: with the output range in $[-1,1]$. However, it exhibits the vanishing gradient
    problem due to saturating behavior for both large positive and large negative
    inputs. The training of deep networks become difficult due to the uniform slope
    of the Logistic Sigmoid and Tanh AFs near the origin [[20](#bib.bib20)]. To minimize
    this limitation, the Scaled Sigmoid (sSigmoid) is defined as,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-1,1]$。然而，由于Logistic Sigmoid和Tanh在大正负输入时表现出饱和行为，导致了梯度消失问题。由于这一特性，训练深度网络变得困难[[20](#bib.bib20)]。为了减轻这一限制，定义了Scaled
    Sigmoid (sSigmoid)，如下，
- en: '|  | $sSigmoid(x)=(4\times Sigmoid(x)-2)$ |  | (6) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $sSigmoid(x)=(4\times Sigmoid(x)-2)$ |  | (6) |'
- en: with the output range in $[-2,2]$ and the Penalized Tanh (pTanh) is defined
    as,
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-2,2]$，Penalized Tanh (pTanh)定义如下，
- en: '|  | $pTanh(x)=\begin{cases}Tanh(x),&amp;x\geq 0\\ a\times Tanh(x),&amp;x<0\end{cases}$
    |  | (7) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $pTanh(x)=\begin{cases}Tanh(x),&x\geq 0\\ a\times Tanh(x),&x<0\end{cases}$
    |  | (7) |'
- en: with the output range in $[-a,1]$ where $a\in(0,1)$. However, sSigmoid and pTanh
    AFs also suffer from the vanishing gradient problem. It is noticed that the pTanh
    AF performs better for Natural Language Processing (NLP) tasks [[33](#bib.bib33)].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-a,1]$，其中$a\in(0,1)$。然而，Sigmoid和pTanh AFs也存在梯度消失问题。注意到pTanh AF在自然语言处理（NLP）任务中表现更好[[33](#bib.bib33)]。
- en: 'Table 3: Summary of Rectified Linear Unit based activation functions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：基于修正线性单元的激活函数总结。
- en: '| Name | Parametric | Monotonic | Smooth | Bounded |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | Parametric | 单调 | 平滑 | 有界 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Rectified Linear Unit (ReLU), 2010 [[17](#bib.bib17)] | No | Yes | No | For
    negative inputs |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Rectified Linear Unit (ReLU), 2010 [[17](#bib.bib17)] | 否 | 是 | 否 | 对于负输入
    |'
- en: '| Leaky ReLU (LReLU), 2013 [[34](#bib.bib34)] | No | Yes | No | No |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Leaky ReLU (LReLU), 2013 [[34](#bib.bib34)] | 否 | 是 | 否 | 否 |'
- en: '| Parametric ReLU (PReLU), 2015 [[35](#bib.bib35)] | Yes | Yes | No | No |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Parametric ReLU (PReLU), 2015 [[35](#bib.bib35)] | 是 | 是 | 否 | 否 |'
- en: '| Randomized ReLU (RReLU), 2015 [[35](#bib.bib35)] | No | Yes | No | No |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Randomized ReLU (RReLU), 2015 [[35](#bib.bib35)] | 否 | 是 | 否 | 否 |'
- en: '| Concatenated ReLU (CReLU), 2016 [[36](#bib.bib36)] | No | Yes | No | For
    negative inputs |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Concatenated ReLU (CReLU), 2016 [[36](#bib.bib36)] | 否 | 是 | 否 | 对于负输入 |'
- en: '| Bounded ReLU (BReLU), 2016 [[37](#bib.bib37)] | No | Yes | No | Yes |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Bounded ReLU (BReLU), 2016 [[37](#bib.bib37)] | 否 | 是 | 否 | 是 |'
- en: '| Parametric Tanh Linear Unit (PTELU), 2017 [[38](#bib.bib38)] | Yes | Yes
    | Yes | For negative inputs |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 参数化Tanh线性单元（PTELU），2017 [[38](#bib.bib38)] | 是 | 是 | 是 | 针对负输入 |'
- en: '| Flexible ReLU (FReLU), 2018 [[39](#bib.bib39)] | Yes | Yes | No | For negative
    inputs |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 灵活ReLU（FReLU），2018 [[39](#bib.bib39)] | 是 | 是 | 否 | 针对负输入 |'
- en: '| Elastic ReLU (EReLU), 2018 [[40](#bib.bib40)] | No | Yes | No | For negative
    inputs |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 弹性ReLU（EReLU），2018 [[40](#bib.bib40)] | 否 | 是 | 否 | 针对负输入 |'
- en: '| Randomly Translational ReLU (RTReLU), 2018 [[41](#bib.bib41)] | No | Yes
    | No | For negative inputs |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 随机平移ReLU（RTReLU），2018 [[41](#bib.bib41)] | 否 | 是 | 否 | 针对负输入 |'
- en: '| Dual ReLU (DualReLU), 2018 [[42](#bib.bib42)] | No | Yes | No | No |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 双重ReLU（DualReLU），2018 [[42](#bib.bib42)] | 否 | 是 | 否 | 否 |'
- en: '| Paired ReLU (PairedReLU), 2018 [[43](#bib.bib43)] | Yes | Yes | No | No |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 配对ReLU（PairedReLU），2018 [[43](#bib.bib43)] | 是 | 是 | 否 | 否 |'
- en: '| Average Biased ReLU (ABReLU), 2018 [[44](#bib.bib44)] | No | Yes | No | For
    negative inputs |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 平均偏置ReLU（ABReLU），2018 [[44](#bib.bib44)] | 否 | 是 | 否 | 针对负输入 |'
- en: '| Natural-Logarithm (NLReLU), 2019 [[45](#bib.bib45)] | No | Yes | No | For
    negative inputs |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 自然对数（NLReLU），2019 [[45](#bib.bib45)] | 否 | 是 | 否 | 针对负输入 |'
- en: '| Multi-bin Trainable Linear Units (MTLU), 2019 [[46](#bib.bib46)] | Yes |
    No | No | No |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 多分箱可训练线性单元（MTLU），2019 [[46](#bib.bib46)] | 是 | 否 | 否 | 否 |'
- en: '| Lipschitz ReLU (L-ReLU), 2020 [[47](#bib.bib47)] | Yes | Depends upon $\phi$
    and $\eta$ | Depends upon $\phi$ and $\eta$ | Depends upon $\phi$ and $\eta$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Lipschitz ReLU（L-ReLU），2020 [[47](#bib.bib47)] | 是 | 取决于 $\phi$ 和 $\eta$
    | 取决于 $\phi$ 和 $\eta$ | 取决于 $\phi$ 和 $\eta$ |'
- en: A noisy AF is defined to overcome the vanishing gradient problem [[48](#bib.bib48)].
    Due to the added noise the gradients may flow easily even in the saturating regime.
    The vanishing gradient problem is minimized by the Hexpo function [[21](#bib.bib21)]
    which is similar to Tanh with a scaled gradient. It is given as,
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 含噪声的激活函数被定义为克服梯度消失问题 [[48](#bib.bib48)]。由于加入了噪声，即使在饱和区域，梯度也能顺利流动。梯度消失问题通过Hexpo函数
    [[21](#bib.bib21)] 得到最小化，该函数类似于Tanh，但具有缩放的梯度。其定义为，
- en: '|  | $Hexpo(x)=\begin{cases}-a\times(e^{-x/b}-1),&amp;x\geq 0\\ c\times(e^{x/d}-1),&amp;x<0\end{cases}$
    |  | (8) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $Hexpo(x)=\begin{cases}-a\times(e^{-x/b}-1),&amp;x\geq 0\\ c\times(e^{x/d}-1),&amp;x<0\end{cases}$
    |  | (8) |'
- en: in the output range of $[-c,a]$. The output of the sigmoid function is multiplied
    with its input in sigmoid-weighted linear unit (SiLU) AF [[23](#bib.bib23)] as
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围 $[-c,a]$ 内。Sigmoid函数的输出与其输入在sigmoid加权线性单元（SiLU）激活函数 [[23](#bib.bib23)]
    中相乘，如下所示
- en: '|  | $SiLU(x)=x\times Sigmoid(x)$ |  | (9) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $SiLU(x)=x\times Sigmoid(x)$ |  | (9) |'
- en: in the output range of $(-0.5,\infty)$. At the same time an improved logistic
    Sigmoid (ISigmoid) AF [[22](#bib.bib22)] is proposed to solve the vanishing gradient
    problem of Sigmoid with the help of a piecewise combination of sigmoidal and linear
    functions. It is defined as,
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围 $(-0.5,\infty)$ 内。同时，提出了一种改进的逻辑Sigmoid（ISigmoid）激活函数 [[22](#bib.bib22)]，旨在通过将Sigmoid与分段的sigmoidal和线性函数结合来解决Sigmoid的梯度消失问题。其定义为，
- en: '|  | <math   alttext="ISigmoid(x)=\begin{cases}\alpha\times(x-a)+Sigmoid(a),&amp;x\geq
    a\\ Sigmoid(x),&amp;-a<x<a\\'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="ISigmoid(x)=\begin{cases}\alpha\times(x-a)+Sigmoid(a),&amp;x\geq
    a\\ Sigmoid(x),&amp;-a<x<a\\'
- en: \alpha\times(x+a)+Sigmoid(a),&amp;x\leq-a\end{cases}" display="block"><semantics
    ><mrow  ><mrow ><mi  >I</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >S</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi  >g</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >x</mi><mo stretchy="false" >)</mo></mrow></mrow><mo  >=</mo><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow
    ><mrow ><mrow  ><mi >α</mi><mo lspace="0.222em" rspace="0.222em"  >×</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >x</mi><mo >−</mo><mi >a</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >S</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >a</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mi >x</mi><mo  >≥</mo><mi >a</mi></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow  ><mi >S</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><mi >x</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow ><mo  >−</mo><mi >a</mi></mrow><mo
    ><</mo><mi >x</mi><mo  ><</mo><mi >a</mi></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mrow  ><mrow ><mi >α</mi><mo lspace="0.222em" rspace="0.222em" >×</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >x</mi><mo >+</mo><mi >a</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >S</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >a</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mi >x</mi><mo  >≤</mo><mrow ><mo >−</mo><mi  >a</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >𝐼</ci><ci  >𝑆</ci><ci >𝑖</ci><ci
    >𝑔</ci><ci  >𝑚</ci><ci >𝑜</ci><ci >𝑖</ci><ci  >𝑑</ci><ci >𝑥</ci></apply><apply
    ><csymbol cd="latexml"  >cases</csymbol><apply ><apply ><ci >𝛼</ci><apply ><ci
    >𝑥</ci><ci >𝑎</ci></apply></apply><apply ><ci >𝑆</ci><ci >𝑖</ci><ci >𝑔</ci><ci
    >𝑚</ci><ci >𝑜</ci><ci >𝑖</ci><ci >𝑑</ci><ci >𝑎</ci></apply></apply><apply ><ci  >𝑥</ci><ci
    >𝑎</ci></apply><apply ><ci >𝑆</ci><ci >𝑖</ci><ci >𝑔</ci><ci >𝑚</ci><ci >𝑜</ci><ci
    >𝑖</ci><ci >𝑑</ci><ci >𝑥</ci></apply><apply ><apply  ><apply ><ci >𝑎</ci></apply><ci
    >𝑥</ci></apply><apply ><ci  >𝑎</ci></apply></apply><apply ><apply ><ci >𝛼</ci><apply
    ><ci >𝑥</ci><ci >𝑎</ci></apply></apply><apply ><ci >𝑆</ci><ci >𝑖</ci><ci >𝑔</ci><ci
    >𝑚</ci><ci >𝑜</ci><ci >𝑖</ci><ci >𝑑</ci><ci >𝑎</ci></apply></apply><apply ><ci  >𝑥</ci><apply
    ><ci >𝑎</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >ISigmoid(x)=\begin{cases}\alpha\times(x-a)+Sigmoid(a),&x\geq a\\ Sigmoid(x),&-a<x<a\\
    \alpha\times(x+a)+Sigmoid(a),&x\leq-a\end{cases}</annotation></semantics></math>
    |  | (10) |
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: \(\alpha\times(x+a)+\text{Sigmoid}(a),\text{当}\ x\leq-a\end{cases}\)" display="block"><semantics
    ><mrow  ><mrow ><mi  >I</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >S</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi  >g</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >x</mi><mo stretchy="false" >)</mo></mrow></mrow><mo  >=</mo><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow
    ><mrow ><mrow  ><mi >α</mi><mo lspace="0.222em" rspace="0.222em"  >×</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >x</mi><mo >−</mo><mi >a</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >S</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >a</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mi >x</mi><mo  >≥</mo><mi >a</mi></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow  ><mi >S</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><mi >x</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow ><mo  >−</mo><mi >a</mi></mrow><mo
    ><</mo><mi >x</mi><mo  ><</mo><mi >a</mi></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mrow  ><mrow ><mi >α</mi><mo lspace="0.222em" rspace="0.222em" >×</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >x</mi><mo >+</mo><mi >a</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >S</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >a</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mi >x</mi><mo  >≤</mo><mrow ><mo >−</mo><mi  >a</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >𝐼</ci><ci  >𝑆</ci><ci >𝑖</ci><ci
    >𝑔</ci><ci  >𝑚</ci><ci >𝑜</ci><ci >𝑖</ci><ci  >𝑑</ci><ci >𝑥</ci></apply><apply
    ><csymbol cd="latexml"  >cases</csymbol><apply ><apply ><ci >𝛼</ci><apply ><ci
    >𝑥</ci><ci >𝑎</ci></apply></apply><apply ><ci >𝑆</ci><ci >𝑖</ci><ci >𝑔</ci><ci
    >𝑚</ci><ci >𝑜</ci><ci >𝑖</ci><ci >𝑑</ci><ci >𝑎</ci></apply></apply><apply ><ci  >𝑥</ci><ci
    >𝑎</ci></apply><apply ><ci >𝑆</ci><ci >𝑖</ci><ci >𝑔</ci><ci >𝑚</ci><ci >𝑜</ci><ci
    >𝑖</ci><ci >𝑑</ci><ci >𝑥</ci></apply><apply ><apply  ><apply ><ci >𝑎</ci></apply><ci
    >𝑥</ci></apply><apply ><ci  >𝑎</ci></apply></apply><apply ><apply ><ci >𝛼</ci><apply
    ><ci >𝑥</ci><ci >𝑎</ci></apply></apply><apply ><ci >𝑆</ci><ci >𝑖</ci><ci >𝑔</ci><ci
    >𝑚</ci><ci >𝑜</ci><ci >𝑖</ci><ci >𝑑</ci><ci >𝑎</ci></apply></apply><apply ><ci  >𝑥</ci><apply
    ><ci >𝑎</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >ISigmoid(x)=\begin{cases}\alpha\times(x-a)+\text{Sigmoid}(a),&x\geq a\\ \text{Sigmoid}(x),&-a<x<a\\
    \alpha\times(x+a)+\text{Sigmoid}(a),&x\leq-a\end{cases}</annotation></semantics></math>
    |  | (10) |
- en: in the output range of $(-\infty,\infty)$. The Linearly scaled hyperbolic tangent
    (LiSHT) AF scales the Tanh in a linear fashion to overcome the vanishing gradient
    issue [[24](#bib.bib24)]. The LiSHT can be defined as,
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围为 $(-\infty,\infty)$。线性缩放双曲正切 (LiSHT) AF 以线性方式缩放 Tanh 以克服梯度消失问题 [[24](#bib.bib24)]。LiSHT
    可以定义为，
- en: '|  | $LiSHT(x)=x\times Tanh(x)$ |  | (11) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $LiSHT(x)=x\times Tanh(x)$ |  | (11) |'
- en: in the output range of $[0,\infty)$. The LiSHT function is symmetric, but is
    has the shortcoming of including unbounded and non-negative outputs only. The
    Elliott AF [[25](#bib.bib25)] is similar to Sigmoid function in terms of the characteristics
    diagram and defined as,
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围为 $[0,\infty)$。LiSHT 函数是对称的，但它的缺点是仅包含无界和非负输出。Elliott AF [[25](#bib.bib25)]
    在特性图上与 Sigmoid 函数类似，定义为，
- en: '|  | $Elliott(x)=\frac{0.5\times x}{1+&#124;x&#124;}+0.5$ |  | (12) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $Elliott(x)=\frac{0.5\times x}{1+&#124;x&#124;}+0.5$ |  | (12) |'
- en: in the output range of $[0,1]$. The Soft-Root-Sign (SRS) AF [[26](#bib.bib26)]
    is defined as,
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围为 $[0,1]$。Soft-Root-Sign (SRS) AF [[26](#bib.bib26)] 定义为，
- en: '|  | $SRS(x)=\frac{x}{\frac{x}{\alpha}+e^{-x/\beta}}$ |  | (13) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $SRS(x)=\frac{x}{\frac{x}{\alpha}+e^{-x/\beta}}$ |  | (13) |'
- en: in the output range of $[\frac{\alpha\times\beta}{\beta-\alpha\times e},\alpha]$
    where $\alpha$ and $\beta$ are the learnable parameters. The use of additional
    parameters increases the complexity of the SRS function. Most of the variants
    of Sigmoid/Tanh AFs have tried to overcome the vanishing gradient issue. However,
    this issue is still present in most of these AFs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围为 $[\frac{\alpha\times\beta}{\beta-\alpha\times e},\alpha]$，其中 $\alpha$
    和 $\beta$ 是可学习参数。使用额外的参数增加了 SRS 函数的复杂性。大多数 Sigmoid/Tanh AF 的变体尝试克服梯度消失问题。然而，这个问题在大多数这些
    AF 中仍然存在。
- en: 4 Rectified Activation Functions
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 纠正激活函数
- en: 'A summary of rectified AFs is illustrated in Table [3](#S3.T3 "Table 3 ‣ 3
    Logistic Sigmoid and Tanh Based AFs ‣ Activation Functions in Deep Learning: A
    Comprehensive Survey and Benchmark"). Rectified Linear Unit (ReLU) is a simple
    function which is the identity function for positive input and zero for negative
    input and given as,'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '纠正激活函数的总结如表 [3](#S3.T3 "Table 3 ‣ 3 Logistic Sigmoid and Tanh Based AFs ‣ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark") 所示。纠正线性单元 (ReLU)
    是一个简单的函数，对于正输入是恒等函数，对于负输入是零，定义为，'
- en: '|  | $ReLU(x)=max(0,x)=\begin{cases}x,&amp;\text{if }x\geq 0\\ 0,&amp;\text{otherwise}\end{cases}.$
    |  | (14) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $ReLU(x)=max(0,x)=\begin{cases}x,&amp;\text{if }x\geq 0\\ 0,&amp;\text{otherwise}\end{cases}.$
    |  | (14) |'
- en: Hence, the range of ReLU is $[0,\infty)$. The gradient for positive and negative
    inputs is one and zero, respectively. The ReLU function solves the problem of
    computational complexity of the Logistic Sigmoid and Tanh functions. The downside
    of ReLU is with the vanishing gradient problem for the negative inputs. In spite
    of having the vanishing gradient problem, the ReLU AF has been used very extensively
    with the deep learning models. The advancements in ReLU based AFs are discussed
    in the rest of this section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，ReLU 的范围为 $[0,\infty)$。对于正输入和负输入的梯度分别为一和零。ReLU 函数解决了 Logistic Sigmoid 和 Tanh
    函数的计算复杂性问题。ReLU 的缺点在于负输入的梯度消失问题。尽管存在梯度消失问题，ReLU AF 在深度学习模型中得到了广泛使用。本节的其余部分讨论了基于
    ReLU 的 AF 的进展。
- en: 4.1 On the Non-utilization of Negative Values of ReLU
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 关于 ReLU 的负值未利用问题
- en: Vanishing gradient is the main problem with ReLU AF which is caused due to the
    non-utilization of negative values. A Leaky Rectified Linear Unit (LReLU) is the
    extension of ReLU by utilizing the negative values [[34](#bib.bib34)]. The LReLU
    is defined as,
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失是 ReLU AF 的主要问题，由于未利用负值而造成。Leaky Rectified Linear Unit (LReLU) 是 ReLU 的扩展，通过利用负值
    [[34](#bib.bib34)]。LReLU 定义为，
- en: '|  | $LReLU(x)=\begin{cases}x,&amp;x\geq 0\\ 0.01\times x,&amp;x<0\end{cases}$
    |  | (15) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $LReLU(x)=\begin{cases}x,&amp;x\geq 0\\ 0.01\times x,&amp;x<0\end{cases}$
    |  | (15) |'
- en: in the output range of $(-\infty,\infty)$. The LReLU has been used in many applications
    with promising performance. One major problem associated with LReLU is the finding
    of the right slope in linear function for negative inputs. Different slopes might
    be suited for different problems and different networks. Thus, it is extended
    to Parametric ReLU (PReLU) by considering the slope for negative input as a trainable
    parameter [[35](#bib.bib35)]. The PReLU is given as,
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围为 $(-\infty,\infty)$。LReLU 在许多应用中表现出良好的性能。LReLU 关联的一个主要问题是为负输入找到合适的斜率。不同的斜率可能适合不同的问题和网络。因此，它通过将负输入的斜率作为可训练参数扩展为
    Parametric ReLU (PReLU) [[35](#bib.bib35)]。PReLU 定义为，
- en: '|  | $PReLU(x)=\begin{cases}x,&amp;x\geq 0\\ p\times x,&amp;x<0\end{cases}$
    |  | (16) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $PReLU(x)=\begin{cases}x,&amp;x\geq 0\\ p\times x,&amp;x<0\end{cases}$
    |  | (16) |'
- en: in the output range of $(-\infty,\infty)$ where $p$ is the trainable parameter.
    However, it can lead to overfitting easily which is the downside of PReLU. The
    Maxout layer, which computes the maximum of several linear units, is also used
    as AF [[49](#bib.bib49)]. Both ReLU and Leaky ReLU can be seen as the special
    cases of Maxout. The randomized ReLU (RReLU) considers the slope of LReLU randomly
    during training sampled from an uniform distribution $U(l,u)$ [[50](#bib.bib50)].
    The RReLU is defined as,
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围 $(-\infty,\infty)$ 内，其中 $p$ 是可训练参数。然而，它容易导致过拟合，这是 PReLU 的缺点。Maxout 层计算多个线性单元的最大值，也作为
    AF 使用 [[49](#bib.bib49)]。ReLU 和 Leaky ReLU 可以看作是 Maxout 的特例。随机 ReLU (RReLU) 在训练期间从均匀分布
    $U(l,u)$ 随机考虑 LReLU 的斜率 [[50](#bib.bib50)]。RReLU 定义为，
- en: '|  | $RReLU(x)=\begin{cases}x,&amp;x\geq 0\\ R\times x,&amp;x<0\end{cases}$
    |  | (17) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $RReLU(x)=\begin{cases}x,&amp;x\geq 0\\ R\times x,&amp;x<0\end{cases}$
    |  | (17) |'
- en: in the output range of $(-\infty,\infty)$ where $R\sim{~{}}U(l,u)$, $l<u$ and
    $l,u\in[0,1)$. It uses a deterministic value $x/\left(\frac{l+u}{2}\right)$ during
    test time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围 $(-\infty,\infty)$ 内，其中 $R\sim{~{}}U(l,u)$，$l<u$ 且 $l,u\in[0,1)$。在测试时使用确定性值
    $x/\left(\frac{l+u}{2}\right)$。
- en: The ReLU is not able to utilize the potential useful information from the negative
    values. In most of the networks, the feature map given as the input to AF is dense
    near zero. Thus, a small jitter in the rectification point can lead to difficulty
    in training. Concatenated ReLU (CReLU) [[36](#bib.bib36)] concatenates the ReLU’s
    output over original input and negated input. The CReLU can be given as,
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 无法利用负值中的潜在有用信息。在大多数网络中，作为 AF 输入的特征图在零附近是密集的。因此，修正点的微小抖动可能导致训练困难。连接 ReLU（CReLU）[[36](#bib.bib36)]
    将 ReLU 的输出连接到原始输入和负输入。CReLU 可以表示为，
- en: '|  | $CReLU(x)=[ReLU(x),ReLU(-x)]$ |  | (18) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $CReLU(x)=[ReLU(x),ReLU(-x)]$ |  | (18) |'
- en: in the output range of $[0,\infty)$. The CReLU is derived from the fact that
    the lower layer kernels in CNN models form pairs with opposite phases. The shifting
    of the feature map with multiple biases is also performed before the ReLU layer
    [[51](#bib.bib51)]. However, it increases the model complexity as more ReLUs are
    required. A Parametric Tan Hyperbolic Linear Unit (P-TELU) is also used as an
    AF [[38](#bib.bib38)]. The P-TELU is defined as,
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围 $[0,\infty)$ 内。CReLU 源于 CNN 模型中下层卷积核形成相位相反的对。特征图在 ReLU 层之前还会进行多个偏置的移动
    [[51](#bib.bib51)]。然而，这增加了模型的复杂性，因为需要更多的 ReLU。也使用了参数化双曲正切线性单元（P-TELU）作为 AF [[38](#bib.bib38)]。P-TELU
    定义为，
- en: '|  | $PTELU(x)=\begin{cases}x,&amp;x\geq 0\\ \alpha\times\text{Tanh}(\beta\times
    x),&amp;x<0\end{cases}$ |  | (19) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $PTELU(x)=\begin{cases}x,&amp;x\geq 0\\ \alpha\times\text{Tanh}(\beta\times
    x),&amp;x<0\end{cases}$ |  | (19) |'
- en: in the output range of $[-\alpha,\infty)$ where $\{\alpha,\beta\}\geq 0$ are
    the learnable parameters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围 $[-\alpha,\infty)$ 内，其中 $\{\alpha,\beta\}\geq 0$ 是可学习的参数。
- en: 'Table 4: Summary of Exponential Linear Unit based activation functions.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：基于指数线性单元的激活函数总结。
- en: '| Name | Parametric | Monotonic | Smooth | Bounded |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 参数化 | 单调 | 光滑 | 有界 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Exponential Linear Unit (ELU), 2016 [[27](#bib.bib27)] | Yes | Yes | Yes
    | For negative inputs |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Exponential Linear Unit (ELU), 2016 [[27](#bib.bib27)] | 是 | 是 | 是 | 针对负输入
    |'
- en: '| Scaled ELU (SELU), 2017 [[52](#bib.bib52)] | Yes | Yes | Yes | For negative
    inputs |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Scaled ELU (SELU), 2017 [[52](#bib.bib52)] | 是 | 是 | 是 | 针对负输入 |'
- en: '| Continuously Differentiable ELU (CELU), 2017 [[53](#bib.bib53)] | Yes | Yes
    | No | For negative inputs |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Continuously Differentiable ELU (CELU), 2017 [[53](#bib.bib53)] | 是 | 是 |
    否 | 针对负输入 |'
- en: '| Parametric ELU (PELU), 2017 [[54](#bib.bib54)] | Yes | Yes | No | For negative
    inputs |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Parametric ELU (PELU), 2017 [[54](#bib.bib54)] | 是 | 是 | 否 | 针对负输入 |'
- en: '| Multiple PELU (MPELU), 2018 [[55](#bib.bib55)] | Yes | Yes | No | For negative
    inputs |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Multiple PELU (MPELU), 2018 [[55](#bib.bib55)] | 是 | 是 | 否 | 针对负输入 |'
- en: '| Fast ELU (FELU), 2019 [[56](#bib.bib56)] | Yes | Yes | No | For negative
    inputs |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Fast ELU (FELU), 2019 [[56](#bib.bib56)] | 是 | 是 | 否 | 针对负输入 |'
- en: '| Parametric Rectified Exponential Unit (PREU), 2019 [[57](#bib.bib57)] | Yes
    | No | Yes | For negative inputs |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Parametric Rectified Exponential Unit (PREU), 2019 [[57](#bib.bib57)] | 是
    | 否 | 是 | 针对负输入 |'
- en: '| Elastic ELU (EELU), 2020 [[58](#bib.bib58)] | Yes | Yes | No | For negative
    inputs |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Elastic ELU (EELU), 2020 [[58](#bib.bib58)] | 是 | 是 | 否 | 针对负输入 |'
- en: '| Parametric Deformable ELU (PDELU), 2020 [[59](#bib.bib59)] | Yes | Yes |
    Yes | For negative inputs |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Parametric Deformable ELU (PDELU), 2020 [[59](#bib.bib59)] | 是 | 是 | 是 |
    针对负输入 |'
- en: The Flexible ReLU (FReLU) [[39](#bib.bib39)] captures the negative values with
    a rectified point which is considered as trainable in the Shifted ReLU [[39](#bib.bib39)].
    The FReLU is given as,
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Flexible ReLU (FReLU) [[39](#bib.bib39)] 通过在Shifted ReLU [[39](#bib.bib39)]中将整流点视为可训练的来捕捉负值。FReLU定义为，
- en: '|  | $FReLU(x)=ReLU(x)+b$ |  | (20) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $FReLU(x)=ReLU(x)+b$ |  | (20) |'
- en: in the output range of $[b,\infty)$. A similar arrangement is also followed
    by Random Translation ReLU (RTReLU) [[41](#bib.bib41)] by utilizing an offset,
    sampled from a Gaussian distribution, given as,
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围为$[b,\infty)$中。类似的安排也被Random Translation ReLU (RTReLU) [[41](#bib.bib41)]
    通过利用从高斯分布中采样的偏移量所遵循，定义为，
- en: '|  | $RTReLU(x)=\begin{cases}x+a,&amp;x+a>0\\ 0,&amp;x+a\leq 0\end{cases}$
    |  | (21) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $RTReLU(x)=\begin{cases}x+a,&amp;x+a>0\\ 0,&amp;x+a\leq 0\end{cases}$
    |  | (21) |'
- en: in the output range of $[0,\infty)$ where $a$ is a random number. At test time,
    the offset is set to zero. A data dependent Average Biased ReLU (AB-ReLU) [[44](#bib.bib44)]
    is also investigated to tackle the negative values by a horizontal shifting based
    on the average of features. The ABReLU can be written as,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围为$[0,\infty)$中，其中$a$是一个随机数。在测试时，偏移量设置为零。还研究了一种依赖于数据的平均偏置ReLU (AB-ReLU)
    [[44](#bib.bib44)]，通过基于特征平均值的水平位移来处理负值。ABReLU可以写成，
- en: '|  | $ABReLU(x)=\begin{cases}x-\beta,&amp;x-\beta\geq 0\\ 0,&amp;x-\beta<0\end{cases}$
    |  | (22) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $ABReLU(x)=\begin{cases}x-\beta,&amp;x-\beta\geq 0\\ 0,&amp;x-\beta<0\end{cases}$
    |  | (22) |'
- en: having the output range in $[0,\infty)$ where $\beta$ is computed as the average
    of input activation map to the activation function. The batch dependent threshold
    for the ReLU is used by the Dynamic ReLU (D-ReLU) [[60](#bib.bib60)]. The Dual
    ReLU (DualReLU) [[42](#bib.bib42)] is a two dimensional AF for recurrent neural
    networks. The DualReLU is given as,
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[0,\infty)$，其中$\beta$计算为输入激活图到激活函数的平均值。Dynamic ReLU (D-ReLU) [[60](#bib.bib60)]使用ReLU的批次相关阈值。Dual
    ReLU (DualReLU) [[42](#bib.bib42)] 是用于递归神经网络的二维激活函数。DualReLU定义为，
- en: '|  | $DualReLU(a,b)=\max(0,a)-\max(0,b)$ |  | (23) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $DualReLU(a,b)=\max(0,a)-\max(0,b)$ |  | (23) |'
- en: in the output range of $(-\infty,\infty)$ where $a$ and $b$ are the inputs in
    different dimensions. Similar to the CReLU, the PairedReLU AF is used for image
    super-resolution [[43](#bib.bib43)]. The PairedReLU is given as,
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围为$(-\infty,\infty)$中，其中$a$和$b$是不同维度的输入。类似于CReLU，PairedReLU激活函数用于图像超分辨率
    [[43](#bib.bib43)]。PairedReLU定义为，
- en: '|  | $PairedReLU(x)=[\max(s\times x-\theta,0),max(s_{p}\times x-\theta_{p},0)]$
    |  | (24) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $PairedReLU(x)=[\max(s\times x-\theta,0),max(s_{p}\times x-\theta_{p},0)]$
    |  | (24) |'
- en: in the output range of $(-\infty,\infty)$. However, the computational complexity
    of PairedReLU is increased as compared to CReLU. In another attempt, V-shaped
    ReLU (vReLU) AF [[61](#bib.bib61)] is defined as,
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围为$(-\infty,\infty)$中。然而，与CReLU相比，PairedReLU的计算复杂度增加了。在另一种尝试中，V形ReLU (vReLU)
    激活函数 [[61](#bib.bib61)] 被定义为，
- en: '|  | $vReLU(x)=\begin{cases}x,&amp;x\geq 0\\ -x,&amp;x<0\end{cases}$ |  | (25)
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $vReLU(x)=\begin{cases}x,&amp;x\geq 0\\ -x,&amp;x<0\end{cases}$ |  | (25)
    |'
- en: having the output range in $[0,\infty]$. The vReLU activation function suffers
    from the non-symmetric output. The SignReLU AF utilizes the negative values using
    the Softsign function [[62](#bib.bib62)]. The positive part of SignReLU is the
    same as the ReLU.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[0,\infty]$。vReLU激活函数存在非对称输出的问题。SignReLU激活函数利用负值使用Softsign函数 [[62](#bib.bib62)]。SignReLU的正部分与ReLU相同。
- en: A Displaced ReLU (DisReLU) [[63](#bib.bib63)] is designed as a generalization
    of Shifted ReLU [[39](#bib.bib39)]. The DisReLU displaces the rectification point
    to consider the negative values, given as,
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Displaced ReLU (DisReLU) [[63](#bib.bib63)] 设计为Shifted ReLU [[39](#bib.bib39)]
    的推广。DisReLU通过将整流点移位来考虑负值，定义为，
- en: '|  | $DisReLU(x)=\begin{cases}x,&amp;x\geq-\delta\\ -\delta,&amp;x<-\delta\end{cases}$
    |  | (26) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $DisReLU(x)=\begin{cases}x,&amp;x\geq-\delta\\ -\delta,&amp;x<-\delta\end{cases}$
    |  | (26) |'
- en: having the output range in $[-\delta,\infty]$. A Bendable Linear Unit (BLU)
    AF is investigated as,
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-\delta,\infty]$。研究了可弯曲线性单元 (BLU) 激活函数，如下所示，
- en: '|  | $BLU(x)=\beta\times(\sqrt{x^{2}+1}-1)+x$ |  | (27) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $BLU(x)=\beta\times(\sqrt{x^{2}+1}-1)+x$ |  | (27) |'
- en: where $-1\leq\beta\leq 1$ is a learnable parameter to adapt the shape between
    the identity function and a rectifier function [[64](#bib.bib64)]. A Lipschitz
    ReLU (L-ReLU) AF uses the piecewise linear functions to model the degree of presence
    and the degree of absence of features [[47](#bib.bib47)]. The L-ReLU is defined
    as,
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$-1\leq\beta\leq 1$ 是一个可学习的参数，用于适应身份函数和整流函数之间的形状 [[64](#bib.bib64)]。Lipschitz
    ReLU (L-ReLU) 激活函数使用分段线性函数来建模特征的存在程度和缺失程度 [[47](#bib.bib47)]。L-ReLU定义为，
- en: '|  | $L\text{-}ReLU(x)=\begin{cases}\max(\phi(x),0),&amp;x\geq 0\\ \min(\eta(x),0),&amp;x<0\end{cases}$
    |  | (28) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $L\text{-}ReLU(x)=\begin{cases}\max(\phi(x),0),&amp;x\geq 0\\ \min(\eta(x),0),&amp;x<0\end{cases}$
    |  | (28) |'
- en: where $\phi$ and $\eta$ are non-linear functions. Moreover, the range of L-ReLU
    also depends upon the values of $\phi$ and $\eta$ functions.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi$ 和 $\eta$ 是非线性函数。此外，L-ReLU 的范围也取决于 $\phi$ 和 $\eta$ 函数的值。
- en: 4.2 On the Limited Non-linearity of ReLU
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 ReLU 的有限非线性
- en: S-shaped ReLU (SReLU) increases the non-linearity in ReLU by combining three
    linear functions with four learnable parameters [[65](#bib.bib65)]. On a similar
    line, Multi-bin Trainable Linear Unit (MTLU) [[46](#bib.bib46)] considers multiple
    bins to increase the non-linear capacity. The MTLU can be written as,
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: S形 ReLU (SReLU) 通过将三个线性函数与四个可学习的参数结合起来，提高了 ReLU 的非线性 [[65](#bib.bib65)]。类似地，多-bin
    可训练线性单元 (MTLU) [[46](#bib.bib46)] 通过考虑多个区间来增加非线性容量。MTLU 可以写成，
- en: '|  | <math   alttext="MTLU(x)=\begin{cases}a_{0}\times x+b_{0},&amp;x\leq c_{0}\\
    a_{k}\times x+b_{k},&amp;c_{k-1}<x\leq c_{k}\\'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="MTLU(x)=\begin{cases}a_{0}\times x+b_{0},&amp;x\leq c_{0}\\
    a_{k}\times x+b_{k},&amp;c_{k-1}<x\leq c_{k}\\'
- en: '...&amp;\\'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '...&amp;\\'
- en: a_{K}\times x+b_{K},&amp;c_{K-1}<x\end{cases}" display="block"><semantics ><mrow
    ><mrow  ><mi >M</mi><mo lspace="0em" rspace="0em" >​</mo><mi >T</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi  >L</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >U</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >x</mi><mo
    stretchy="false" >)</mo></mrow></mrow><mo  >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mrow
    ><mrow  ><msub ><mi >a</mi><mn >0</mn></msub><mo lspace="0.222em" rspace="0.222em"  >×</mo><mi
    >x</mi></mrow><mo >+</mo><msub ><mi >b</mi><mn >0</mn></msub></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mi  >x</mi><mo >≤</mo><msub ><mi  >c</mi><mn >0</mn></msub></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow  ><mrow ><msub ><mi >a</mi><mi >k</mi></msub><mo
    lspace="0.222em" rspace="0.222em"  >×</mo><mi >x</mi></mrow><mo >+</mo><msub ><mi
    >b</mi><mi >k</mi></msub></mrow><mo >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow
    ><msub  ><mi >c</mi><mrow ><mi >k</mi><mo >−</mo><mn >1</mn></mrow></msub><mo
    ><</mo><mi  >x</mi><mo >≤</mo><msub ><mi  >c</mi><mi >k</mi></msub></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mi mathvariant="normal" >…</mi></mtd></mtr><mtr ><mtd
    columnalign="left"  ><mrow ><mrow  ><mrow ><msub ><mi >a</mi><mi >K</mi></msub><mo
    lspace="0.222em" rspace="0.222em"  >×</mo><mi >x</mi></mrow><mo >+</mo><msub ><mi
    >b</mi><mi >K</mi></msub></mrow><mo >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow
    ><msub  ><mi >c</mi><mrow ><mi >K</mi><mo >−</mo><mn >1</mn></mrow></msub><mo
    ><</mo><mi  >x</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><ci >𝑀</ci><ci  >𝑇</ci><ci >𝐿</ci><ci >𝑈</ci><ci  >𝑥</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><apply ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑎</ci><cn type="integer"  >0</cn></apply><ci >𝑥</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑏</ci><cn type="integer"  >0</cn></apply></apply><apply
    ><ci >𝑥</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑐</ci><cn
    type="integer" >0</cn></apply></apply><apply ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑎</ci><ci >𝑘</ci></apply><ci >𝑥</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑏</ci><ci >𝑘</ci></apply></apply><apply ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑐</ci><apply ><ci >𝑘</ci><cn type="integer" >1</cn></apply></apply><ci >𝑥</ci></apply><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑐</ci><ci >𝑘</ci></apply></apply></apply><ci
    >…</ci><ci ><mtext >otherwise</mtext></ci><apply ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑎</ci><ci >𝐾</ci></apply><ci >𝑥</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑏</ci><ci >𝐾</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑐</ci><apply ><ci  >𝐾</ci><cn type="integer"  >1</cn></apply></apply><ci >𝑥</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >MTLU(x)=\begin{cases}a_{0}\times x+b_{0},&x\leq
    c_{0}\\ a_{k}\times x+b_{k},&c_{k-1}<x\leq c_{k}\\ ...&\\ a_{K}\times x+b_{K},&c_{K-1}<x\end{cases}</annotation></semantics></math>
    |  | (29) |
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \(M_{T L U}(x) = \begin{cases}a_{0}\times x + b_{0},& x \leq c_{0} \\ a_{k}\times
    x + b_{k},& c_{k-1} < x \leq c_{k} \\ \vdots & \\ a_{K}\times x + b_{K},& c_{K-1}
    < x \end{cases}\)
- en: having the output range in $(-\infty,\infty)$. The number of bins and the range
    of bins are the hyperparameters, whereas the linear function of a bin is trainable
    (i.e., $a_{0},...,a_{K}$ $b_{0},...,b_{K}$ are the learnable parameters). The
    non-differentiable nature at multiple points is the drawback of the MTLU. An Elastic
    ReLU (EReLU) considers a slope randomly drawn from a uniform distribution during
    the training for the positive inputs to control the amount of non-linearity [[40](#bib.bib40)].
    The EReLU is defined as,
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$(-\infty,\infty)$。箱子的数量和范围是超参数，而箱子的线性函数是可训练的（即，$a_{0},...,a_{K}$和$b_{0},...,b_{K}$是可学习的参数）。MTLU的缺点是多个点的非可微性质。Elastic
    ReLU (EReLU)在训练过程中对正输入考虑从均匀分布中随机抽取的斜率，以控制非线性的量[[40](#bib.bib40)]。EReLU定义为，
- en: '|  | $EReLU(x)=max(R\times x,0)$ |  | (30) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $EReLU(x)=max(R\times x,0)$ |  | (30) |'
- en: in the output range of $[0,\infty)$ where $R$ is a random number. At the test
    time, the EReLU becomes the identity function for positive inputs. The Linearized
    Sigmoidal Activation (LiSHA) function considers three linear functions to increase
    the non-linearity characteristics [[66](#bib.bib66)]. It is also extended to adaptive
    linear sigmoidal AF by learning the slope of upper and lower linear functions.
    The ReLU is combined with Tanh as Rectified Linear Tanh (ReLTanh) [[67](#bib.bib67)]
    to increase the non-linearity of ReLU and to overcome the vanishing gradient problem
    of Tanh. However, the ReLTanh is unbounded in both the positive and negative directions.
    Natural-Logarithm ReLU (NLReLU) modifies the ReLU’s output for positive inputs
    using the logarithm function to increase the degree of nonlinearity [[45](#bib.bib45)].
    The NLReLU is defined as,
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出范围$[0,\infty)$中，其中$R$是随机数。在测试时，EReLU对正输入变成恒等函数。Linearized Sigmoidal Activation
    (LiSHA)函数考虑了三个线性函数以增加非线性特性[[66](#bib.bib66)]。它还扩展为自适应线性Sigmoidal AF，通过学习上下线性函数的斜率。ReLU与Tanh结合形成Rectified
    Linear Tanh (ReLTanh) [[67](#bib.bib67)]，以增加ReLU的非线性并克服Tanh的梯度消失问题。然而，ReLTanh在正负两个方向上都是无界的。Natural-Logarithm
    ReLU (NLReLU)通过使用对数函数修改ReLU对正输入的输出，以增加非线性程度[[45](#bib.bib45)]。NLReLU定义为，
- en: '|  | $NLReLU(x)=\ln(\beta\times\max(0,x)+1.0)$ |  | (31) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $NLReLU(x)=\ln(\beta\times\max(0,x)+1.0)$ |  | (31) |'
- en: having the output range in $[0,\infty)$ where $\beta$ is a constant. The NLReLU
    does not affect the negative regime, thus suffers from vanishing gradient. The
    concept of Leaky ReLU (LReLU) is further improved to Dynamic ReLU [[68](#bib.bib68)]
    by considering a mean square error (MSE) based additional hyperparameter. Thus,
    it can control the slope of the Dynamic ReLU in every epoch based on the convergence.
    A Piecewise Linear Unit (PLU) [[69](#bib.bib69)] is defined as,
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[0,\infty)$，其中$\beta$是常数。NLReLU不影响负值范围，因此会受到梯度消失的影响。Leaky ReLU (LReLU)的概念进一步改进为Dynamic
    ReLU [[68](#bib.bib68)]，通过考虑基于均方误差（MSE）的附加超参数。这样，它可以根据收敛情况在每个训练周期控制Dynamic ReLU的斜率。Piecewise
    Linear Unit (PLU) [[69](#bib.bib69)]定义为，
- en: '|  | $PLU(x)=max(\alpha\times(x+c)-c,min(\alpha\times(x-c)+c,x))$ |  | (32)
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $PLU(x)=max(\alpha\times(x+c)-c,min(\alpha\times(x-c)+c,x))$ |  | (32)
    |'
- en: having the output range in $[-\infty,+\infty]$, where $\alpha$ and $c$ are the
    constants. Basically, the PLU activation function consists of three linear functions
    in pieces, but continuous. Hence, it avoids the saturation and leads to a good
    amount of gradient flow through the activation function during backpropagation
    in order to resolve the vanishing gradient problems of ReLU and Tanh. However,
    the PLU activation is unbounded in both positive and negative directions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-\infty,+\infty]$，其中$\alpha$和$c$是常数。基本上，PLU激活函数由三个线性函数组成，但保持连续。因此，它避免了饱和，并在反向传播过程中通过激活函数提供了良好的梯度流，以解决ReLU和Tanh的梯度消失问题。然而，PLU激活在正负两个方向上都是无界的。
- en: 4.3 On the Unbounded Output of ReLU
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 关于ReLU的无界输出
- en: The unbounded outputs of ReLU and many of its variants may lead to training
    instability. Moreover, the bounded AF is needed for the dedicated hardware based
    embedded system applications. ReLU is extended to Bounded ReLU (BReLU) [[37](#bib.bib37)]
    defined as,
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU及其许多变体的无界输出可能导致训练不稳定。此外，专用硬件嵌入式系统应用需要有界的激活函数。ReLU被扩展为Bounded ReLU (BReLU)
    [[37](#bib.bib37)]，定义为，
- en: '|  | $BReLU(x)=\min(\max(0,x),A)$ |  | (33) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $BReLU(x)=\min(\max(0,x),A)$ |  | (33) |'
- en: having the output range in $[0,A])$. The training stability is improved in BReLU
    due to two rectifications (i.e., at $0$ and $A$). ReLU is a common choice in practice
    in deep learning. ReLU based AFs are generally efficient. The major drawbacks
    of ReLU, such as gradient diminishing for negative inputs, limited non-linearity
    and unboundedness, are improved in the different AFs. However, the ReLU variants
    are not able to resolve all the issues of ReLU.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为 $[0,A]$。由于两个校正（即在 $0$ 和 $A$），BReLU 的训练稳定性得到改善。ReLU 是深度学习中常用的选择。基于 ReLU
    的激活函数通常效率高。ReLU 的主要缺点，如负输入的梯度消失、有限的非线性和无界性，在不同的激活函数中有所改善。然而，ReLU 的变体无法解决 ReLU
    的所有问题。
- en: 5 Exponential Activation Functions
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 指数激活函数
- en: 'The exponential AFs tackle the gradient diminishing problem of ReLU. Table
    [4](#S4.T4 "Table 4 ‣ 4.1 On the Non-utilization of Negative Values of ReLU ‣
    4 Rectified Activation Functions ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark") lists the properties of the exponential AFs. The Exponential
    Linear Unit (ELU) [[27](#bib.bib27)] is given as,'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '指数激活函数解决了 ReLU 的梯度消失问题。表 [4](#S4.T4 "Table 4 ‣ 4.1 On the Non-utilization of
    Negative Values of ReLU ‣ 4 Rectified Activation Functions ‣ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark") 列出了指数激活函数的属性。指数线性单元（ELU）
    [[27](#bib.bib27)] 定义为，'
- en: '|  | $ELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x}-1),&amp;x\leq 0\end{cases}$
    |  | (34) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | $ELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x}-1),&amp;x\leq 0\end{cases}$
    |  | (34) |'
- en: having the output range in $[-1,\infty)$ where $\alpha$ is a learnable parameter.
    The ELU function exhibits all the benefits of the ReLU function. The ELU is differentiable,
    saturates for large negative inputs and reduces the bias shift. The negative saturation
    regime of ELU adds some robustness to noise as compared to the Leaky ReLU and
    Parametric ReLU. The ELU is extended to Scaled ELU (SELU) [[52](#bib.bib52)] by
    using a scaling hyperparameter to make the slope larger than one for positive
    inputs. The SELU can be defined as,
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为 $[-1,\infty)$，其中 $\alpha$ 是一个可学习的参数。ELU 函数展现了 ReLU 函数的所有优点。ELU 是可微分的，对大负输入饱和，并且减少了偏移。与
    Leaky ReLU 和参数化 ReLU 相比，ELU 的负饱和状态对噪声具有一定的鲁棒性。通过使用缩放超参数使得正输入的斜率大于一，ELU 被扩展为缩放
    ELU（SELU） [[52](#bib.bib52)]。SELU 可以定义为，
- en: '|  | $SELU(x)=\lambda\times\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x}-1),&amp;x\leq
    0\end{cases}$ |  | (35) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $SELU(x)=\lambda\times\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x}-1),&amp;x\leq
    0\end{cases}$ |  | (35) |'
- en: having the output range in $[-\lambda,\infty)$ where $\alpha$ is a hyperparameter.
    Basically, the SELU induces self-normalization to automatically converge towards
    zero mean and unit variance. The Parametric ELU (PELU) [[54](#bib.bib54)] changes
    the saturation point and exponential decay and also regulates the slope of the
    linear function for the positive inputs for differentiability. The PELU AF can
    be written as,
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为 $[-\lambda,\infty)$，其中 $\alpha$ 是一个超参数。基本上，SELU 使得自我归一化自动收敛到零均值和单位方差。参数化
    ELU（PELU） [[54](#bib.bib54)] 改变了饱和点和指数衰减，并且调节了正输入的线性函数的斜率以便于可微分性。PELU 激活函数可以写成，
- en: '|  | $PELU(x)=\lambda\times\begin{cases}\frac{a}{b}\times x,&amp;x\geq 0\\
    a\times(e^{x/b}-1),&amp;x<0\end{cases}$ |  | (36) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $PELU(x)=\lambda\times\begin{cases}\frac{a}{b}\times x,&amp;x\geq 0\\
    a\times(e^{x/b}-1),&amp;x<0\end{cases}$ |  | (36) |'
- en: having $[-a,\infty)$ output range, where $a$ and $b$ are the trainable parameters.
    The parametric ELU is also explored in Continuously differentiable ELU (CELU)
    [[53](#bib.bib53)] for the negative inputs. The CELU is given as,
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为 $[-a,\infty)$，其中 $a$ 和 $b$ 是可训练的参数。参数化 ELU 还在连续可微分 ELU（CELU） [[53](#bib.bib53)]
    中进行了探索，针对负输入。CELU 定义为，
- en: '|  | $CELU(x)=\begin{cases}x,&amp;x\geq 0\\ \alpha\times(e^{x/\alpha}-1),&amp;x<0\end{cases}$
    |  | (37) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $CELU(x)=\begin{cases}x,&amp;x\geq 0\\ \alpha\times(e^{x/\alpha}-1),&amp;x<0\end{cases}$
    |  | (37) |'
- en: having the output range in $[-\alpha,\infty)$ where $\alpha$ is a learnable
    parameter. The PELU is also extended to multiple PELU (MPELU) [[55](#bib.bib55)]
    by using two learnable parameters to represent MPELU as either rectified, exponential
    or combined. The MPELU can be expressed as,
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为 $[-\alpha,\infty)$，其中 $\alpha$ 是一个可学习的参数。PELU 还被扩展为多参数 PELU（MPELU） [[55](#bib.bib55)]，通过使用两个可学习的参数来表示
    MPELU 为修正、指数或组合形式。MPELU 可以表示为，
- en: '|  | $MPELU(x)=\begin{cases}x,&amp;x>0\\ \alpha_{c}\times(e^{\beta_{c}\times
    x}-1),&amp;x\leq 0\end{cases}$ |  | (38) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $MPELU(x)=\begin{cases}x,&amp;x>0\\ \alpha_{c}\times(e^{\beta_{c}\times
    x}-1),&amp;x\leq 0\end{cases}$ |  | (38) |'
- en: having the output range in $[-\alpha_{c},\infty)$, where $\alpha_{c}$ and $\beta_{c}$
    are the trainable parameters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-\alpha_{c},\infty)$，其中$\alpha_{c}$和$\beta_{c}$是可训练的参数。
- en: A soft exponential AF interpolates between the exponential, linear and logarithmic
    functions using the trainable parameter [[70](#bib.bib70)]. A Shifted ELU (ShELU)
    AF is also explored as a locally optimal function [[71](#bib.bib71)]. A Parametric
    Rectified Exponential Unit (PREU) [[57](#bib.bib57)] is designed as,
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一种软指数激活函数通过可训练参数在指数、线性和对数函数之间进行插值[[70](#bib.bib70)]。还探讨了一种Shifted ELU (ShELU)
    激活函数作为局部最优函数[[71](#bib.bib71)]。一种参数化的修正指数单元（PREU）[[57](#bib.bib57)]定义为，
- en: '|  | $PREU(x)=\begin{cases}\alpha\times x,&amp;x>0\\ \alpha\times x\times e^{\beta\times
    x},&amp;x\leq 0\end{cases}$ |  | (39) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | $PREU(x)=\begin{cases}\alpha\times x,&amp;x>0\\ \alpha\times x\times e^{\beta\times
    x},&amp;x\leq 0\end{cases}$ |  | (39) |'
- en: having the output range in $[-1,\infty)$, where $\alpha$ and $\beta$ are the
    trainable parameters. The PREU utilizes the negative information near to zero
    effectively. The efficiency of ELU is improved in Fast ELU (FELU) AF [[56](#bib.bib56)]
    with the help of the simple displacement bits and integer algebra operations.
    The FELU is defined as,
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-1,\infty)$，其中$\alpha$和$\beta$是可训练的参数。PREU有效地利用了接近零的负信息。ELU的效率在Fast ELU
    (FELU)激活函数[[56](#bib.bib56)]中得到了提高，借助简单的位移位和整数代数运算。FELU定义为，
- en: '|  | $FELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x/\ln(2)}-1),&amp;x\leq
    0\end{cases}$ |  | (40) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $FELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x/\ln(2)}-1),&amp;x\leq
    0\end{cases}$ |  | (40) |'
- en: having the output range in $[-\alpha,\infty)$ with $\alpha$ as a learnable parameter.
    Recently, the properties of ELU and RELU have been utilized to design an Elastic
    ELU (EELU) AF [[58](#bib.bib58)]. The EELU is defined as,
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-\alpha,\infty)$，其中$\alpha$是一个可学习的参数。最近，ELU和RELU的特性被用于设计弹性ELU（EELU）激活函数[[58](#bib.bib58)]。EELU定义为，
- en: '|  | $EELU(x)=\begin{cases}k\times x,&amp;x>0\\ \alpha\times(e^{\beta\times
    x}-1),&amp;x\leq 0\end{cases}$ |  | (41) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $EELU(x)=\begin{cases}k\times x,&amp;x>0\\ \alpha\times(e^{\beta\times
    x}-1),&amp;x\leq 0\end{cases}$ |  | (41) |'
- en: having the output range in $[-\alpha,\infty)$ where $\alpha$ and $\beta$ are
    the trainable parameters. The EELU preserves a small non-zero gradient for the
    negative input and exhibits an elastic slope for the positive input. A Parametric
    Deformable ELU (PDELU) AF tries to shift the mean value of output closer to zero
    using the flexible map shape [[59](#bib.bib59)]. The PDELU is defined as,
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-\alpha,\infty)$，其中$\alpha$和$\beta$是可训练的参数。EELU保持了一个小的非零梯度用于负输入，并对正输入展示了弹性斜率。一种参数化可变形ELU
    (PDELU) 激活函数尝试通过灵活的映射形状将输出的均值向零靠近[[59](#bib.bib59)]。PDELU定义为，
- en: '|  | $PDELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times([1+(1-t)\times x]^{\frac{1}{1-t}}-1),&amp;x\leq
    0\end{cases}$ |  | (42) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $PDELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times([1+(1-t)\times x]^{\frac{1}{1-t}}-1),&amp;x\leq
    0\end{cases}$ |  | (42) |'
- en: having the output range in $[-1,\infty)$ where $\alpha$ is a learnable parameter.
    A ReLU-Memristor-like AF (RMAF) [[72](#bib.bib72)] uses two hyperparameters to
    have ReLU like shape for positive input and to give more importance to the negative
    values near to zero. An Exponential Linear Sigmoid SquasHing (ELiSH) is defined
    in [[73](#bib.bib73)] as,
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[-1,\infty)$，其中$\alpha$是一个可学习的参数。一种类似ReLU-Memristor的激活函数（RMAF）[[72](#bib.bib72)]使用两个超参数，以使其对正输入具有类似ReLU的形状，并对接近零的负值给予更多重视。在[[73](#bib.bib73)]中定义了指数线性Sigmoid
    SquasHing（ELiSH），其公式为，
- en: '|  | $ELiSH(x)=\begin{cases}x/(1+e^{-x}),&amp;x\geq 0\\ (e^{x}-1)/(1+e^{-x}),&amp;x<0\end{cases}$
    |  | (43) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $ELiSH(x)=\begin{cases}x/(1+e^{-x}),&amp;x\geq 0\\ (e^{x}-1)/(1+e^{-x}),&amp;x<0\end{cases}$
    |  | (43) |'
- en: Moreover, it is also extended to HardELiSH which is a multiplication of HardSigmoid
    and Linear in the positive part and HardSigmoid and ELU in the negative part.
    Here, HardSigmoid is defined as,
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还扩展为HardELiSH，这是一种在正部分乘以HardSigmoid和Linear，在负部分乘以HardSigmoid和ELU的函数。这里，HardSigmoid定义为，
- en: '|  | $HardELish(x)=max(0,min(1,(x+1)/2)).$ |  | (44) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | $HardELish(x)=max(0,min(1,(x+1)/2)).$ |  | (44) |'
- en: The ELU based AFs exploit the negative inputs without compromising with the
    non-linearity. Some ELU variants also modify the function for positive inputs
    to make it bounded.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 基于ELU的激活函数在不妥协非线性的情况下充分利用负输入。一些ELU变体还修改了对正输入的函数以使其有界。
- en: 'Table 5: Summary of adaptive and learning based activation functions.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：自适应和基于学习的激活函数总结。
- en: '| Name | Parametric | Monotonic | Smooth | Bounded |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 参数化 | 单调 | 平滑 | 有界 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Adaptive Piecewise Linear Unit (APL), 2015 [[28](#bib.bib28)] | Yes | No
    | No | No |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 自适应分段线性单元（APL），2015 [[28](#bib.bib28)] | 是 | 否 | 否 | 否 |'
- en: '| Spline AF (SAF), 2016 [[74](#bib.bib74)] | Yes | Yes | Yes | No |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 样条激活函数（SAF），2016 [[74](#bib.bib74)] | 是 | 是 | 是 | 否 |'
- en: '| Bi-Modal Derivative Adaptive Activation (BDAA), 2017 [[75](#bib.bib75)] |
    Yes | Yes | Yes | Yes |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 双模态导数自适应激活函数（BDAA），2017 [[75](#bib.bib75)] | 是 | 是 | 是 | 是 |'
- en: '| Adaptive AF (AAF), 2018 [[76](#bib.bib76)] | Yes | Yes | No | No |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 自适应激活函数（AAF），2018 [[76](#bib.bib76)] | 是 | 是 | 否 | 否 |'
- en: '| Swish, 2018 [[29](#bib.bib29)] | Yes | No | Yes | No |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Swish，2018 [[29](#bib.bib29)] | 是 | 否 | 是 | 否 |'
- en: '| ESwish, 2018 [[77](#bib.bib77)] | Yes | No | Yes | No |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| ESwish，2018 [[77](#bib.bib77)] | 是 | 否 | 是 | 否 |'
- en: '| Trainable AF (TAF), 2018 [[78](#bib.bib78)] | Yes | No | Yes | No |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 可训练激活函数（TAF），2018 [[78](#bib.bib78)] | 是 | 否 | 是 | 否 |'
- en: '| Self-Learnable AF (SLAF), 2019 [[79](#bib.bib79)] | Yes | No | Yes | No |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 自学习激活函数（SLAF），2019 [[79](#bib.bib79)] | 是 | 否 | 是 | 否 |'
- en: '| Mexican ReLU (MeLU), 2019 [[80](#bib.bib80)] | Yes | No | No | No |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 墨西哥 ReLU（MeLU），2019 [[80](#bib.bib80)] | 是 | 否 | 否 | 否 |'
- en: 6 Learning/Adaptive Activation Functions
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 学习/自适应激活函数
- en: 'Most of the aforementioned AFs are not adaptive and might not be able to adjust
    based on the dataset complexity. This problem is tackled using learning/adaptive
    AFs as summarized in Table [5](#S5.T5 "Table 5 ‣ 5 Exponential Activation Functions
    ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark").
    Some of the earlier mentioned AFs are also adaptive, such as PReLU [[57](#bib.bib57)],
    SReLU [[65](#bib.bib65)], PTELU [[38](#bib.bib38)], MTLU [[46](#bib.bib46)], PELU
    [[54](#bib.bib54)], MPELU [[55](#bib.bib55)], PREU [[57](#bib.bib57)], EELU [[58](#bib.bib58)],
    PDELU [[59](#bib.bib59)], SRS [[26](#bib.bib26)], etc.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '上述大多数激活函数并不是自适应的，可能无法根据数据集的复杂性进行调整。这个问题通过学习/自适应激活函数得到解决，如表 [5](#S5.T5 "Table
    5 ‣ 5 Exponential Activation Functions ‣ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") 所总结的。一些早期提到的激活函数也是自适应的，例如 PReLU [[57](#bib.bib57)]、SReLU
    [[65](#bib.bib65)]、PTELU [[38](#bib.bib38)]、MTLU [[46](#bib.bib46)]、PELU [[54](#bib.bib54)]、MPELU
    [[55](#bib.bib55)]、PREU [[57](#bib.bib57)]、EELU [[58](#bib.bib58)]、PDELU [[59](#bib.bib59)]、SRS
    [[26](#bib.bib26)] 等。'
- en: The Adaptive Piecewise Linear (APL) is defined as a sum of hinge-shape functions
    [[28](#bib.bib28)]. It is given as,
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应分段线性（APL）定义为一组铰链形状函数的和 [[28](#bib.bib28)]。其公式为，
- en: '|  | $APL(x)=\text{max}(0,x)+\sum^{S}_{s=1}a_{s}\times\text{max}(0,b_{s}-x),$
    |  | (45) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $APL(x)=\text{max}(0,x)+\sum^{S}_{s=1}a_{s}\times\text{max}(0,b_{s}-x),$
    |  | (45) |'
- en: where $a$ and $b$ are the trainable parameters and $S$ is a hyperparameter representing
    the number of hinges. The output range of APL is $[0,\infty)$. Due to the trainable
    parameters, different neurons can learn different AFs.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a$ 和 $b$ 是可训练的参数，$S$ 是表示铰链数量的超参数。APL 的输出范围是 $[0,\infty)$。由于有可训练参数，不同的神经元可以学习不同的激活函数。
- en: Ramachandran et al. [[29](#bib.bib29)] have performed an automatic search, which
    resulted in a Swish AF. It is defined as,
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Ramachandran 等人 [[29](#bib.bib29)] 进行了自动搜索，结果得到了 Swish 激活函数。其定义为，
- en: '|  | $Swish(x)=x\times Sigmoid(\beta\times x)$ |  | (46) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $Swish(x)=x\times Sigmoid(\beta\times x)$ |  | (46) |'
- en: where $\beta$ is a learnable parameter. The output range of Swish is $(-\infty,\infty)$.
    Based on the learnt value of $\beta$ the shape of the Swish AF is adjusted between
    the linear and ReLU functions. The smaller and higher values of $\beta$ lead towards
    the linear and ReLU functions, respectively. Thus, it can control the amount of
    non-linearity based on the dataset and network complexity. Swish is also extended
    to E-Swish by multiplying the Swish with a learnable parameter to control the
    slope in the positive direction [[77](#bib.bib77)]. The E-Swish is defined as,
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\beta$ 是一个可学习的参数。Swish 的输出范围是 $(-\infty,\infty)$。基于 $\beta$ 的学习值，Swish 激活函数的形状在线性函数和
    ReLU 函数之间调整。较小和较高的 $\beta$ 值分别趋向于线性函数和 ReLU 函数。因此，它可以根据数据集和网络复杂性控制非线性的程度。Swish
    也通过将 Swish 与一个可学习的参数相乘来扩展为 E-Swish，以控制正方向上的斜率 [[77](#bib.bib77)]。E-Swish 定义为，
- en: '|  | $ESwish(x)=\beta\times x\times Sigmoid(x)$ |  | (47) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $ESwish(x)=\beta\times x\times Sigmoid(x)$ |  | (47) |'
- en: having the output the range in $(-\infty,\infty)$ and $\beta$ is trainable parameter.
    A flatten-T Swish considers zero function for negative inputs similar to the ReLU
    [[81](#bib.bib81)]. The Adaptive Richard’s Curve weighted Activation (ARiA) is
    also motivated from Swish and replaces the sigmoidal function with Richard’s Curve
    [[82](#bib.bib82)]. The ARiA AF uses five hyper-parameters to control the shape
    of the non-linearity.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$(-\infty,\infty)$，$\beta$是可训练参数。Flatten-T Swish 对负输入考虑零函数，类似于 ReLU [[81](#bib.bib81)]。Adaptive
    Richard’s Curve 加权激活函数（ARiA）也受到 Swish 的启发，将 sigmoidal 函数替换为 Richard’s Curve [[82](#bib.bib82)]。ARiA
    AF 使用五个超参数来控制非线性形状。
- en: The basic AFs are combined with learnable weights in adaptive AFs [[76](#bib.bib76)].
    The Adaptive AF (AAF) designed over PReLU [[35](#bib.bib35)] and PELU [[54](#bib.bib54)]
    is given as,
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 基本 AFs 与可学习权重结合形成自适应 AFs [[76](#bib.bib76)]。设计在 PReLU [[35](#bib.bib35)] 和 PELU
    [[54](#bib.bib54)] 上的 Adaptive AF (AAF) 为，
- en: '|  | $AAF(x)=\sigma(w\times x)\times PRELU(x)+(1-\sigma(w\times x))\times PELU(x)$
    |  | (48) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $AAF(x)=\sigma(w\times x)\times PRELU(x)+(1-\sigma(w\times x))\times PELU(x)$
    |  | (48) |'
- en: having the output range in $[0,1]$, where $\sigma$ is the sigmoidal function
    and $w$ is a learnable parameter. In practice, AAF is costly as multiple AFs are
    involved. In [[83](#bib.bib83)], the AF for each neuron is selected from a library
    of AFs. In [[84](#bib.bib84)], different combinations of the identity function,
    ReLU, and Tanh are learnt automatically. In another attempt, an Adaptive Blending
    Unit (ABU) is defined to allow the networks to learn its preferred AFs [[85](#bib.bib85)].
    The ABU combines a set of AFs with trainable weights. A Lookup Table Unit (LuTU)
    function [[86](#bib.bib86)] uses a single period cosine mask based smoothing and
    linear interpolation using a set of anchor points. Activation ensembles are used
    at each layer in [[87](#bib.bib87)] with the contribution of each AF controlled
    by the trainable weights. Similarly, the Self-Learnable AF (SLAF) computes the
    sum of the different functions in an ensemble with the learnt coefficients [[79](#bib.bib79)].
    The SLAF can be expressed as,
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$[0,1]$，其中$\sigma$是sigmoidal函数，$w$是可学习参数。在实践中，AAF 成本较高，因为涉及多个 AFs。在 [[83](#bib.bib83)]
    中，每个神经元的 AF 从一个 AFs 库中选择。在 [[84](#bib.bib84)] 中，不同的身份函数、ReLU 和 Tanh 组合被自动学习。在另一个尝试中，定义了一个
    Adaptive Blending Unit (ABU) 以允许网络学习其首选的 AFs [[85](#bib.bib85)]。ABU 将一组 AFs 与可训练权重结合。Lookup
    Table Unit (LuTU) 函数 [[86](#bib.bib86)] 使用单周期余弦掩模平滑和线性插值，基于一组锚点。在 [[87](#bib.bib87)]
    中，每层使用激活集成，每个 AF 的贡献由可训练权重控制。同样，Self-Learnable AF (SLAF) 计算集成中不同函数的加权和，其系数由学习得出
    [[79](#bib.bib79)]。SLAF 可以表示为，
- en: '|  | $SLAF(x)=\sum_{i=0}^{N-1}a_{i}\times x^{i}$ |  | (49) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $SLAF(x)=\sum_{i=0}^{N-1}a_{i}\times x^{i}$ |  | (49) |'
- en: in the output range of $(-\infty,\infty)$, where $a_{i}$ is the trainable parameter.
    A Mexican ReLU (MeLU) AF is proposed in [[80](#bib.bib80)] by using a “Mexican
    hat type” function and given as,
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$(-\infty,\infty)$，其中$a_{i}$是可训练参数。[[80](#bib.bib80)]中提出了一种墨西哥 ReLU (MeLU)
    AF，使用了“墨西哥帽型”函数，表达式为，
- en: '|  | $MeLU(x)=PReLU(x)+\sum_{j=1}^{k}{c_{j}\times\max(\lambda_{j}-&#124;x-a_{j}&#124;,0)}$
    |  | (50) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $MeLU(x)=PReLU(x)+\sum_{j=1}^{k}{c_{j}\times\max(\lambda_{j}-\|x-a_{j}\|,0)}$
    |  | (50) |'
- en: in the output range of $(-\infty,\infty)$, where $c_{j}$ is the trainable parameter
    and $\lambda_{j}$ & $a_{j}$ are the real numbers.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为$(-\infty,\infty)$，其中$c_{j}$是可训练参数，$\lambda_{j}$和$a_{j}$是实数。
- en: A cubic spline interpolation is also used to learn the AF from data [[74](#bib.bib74)]
    which is given as,
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 立方样条插值也用于从数据中学习 AF [[74](#bib.bib74)]，其表达式为，
- en: '|  | $SAF(x)=\Phi(s;\textbf{q})$ |  | (51) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $SAF(x)=\Phi(s;\textbf{q})$ |  | (51) |'
- en: having the output range in $(-\infty,\infty)$ where $\Phi(.)$ is parameterized
    by a vector q cubic in nature. Fourier series basis expansion is used for nonparametrically
    learning AFs (NPF) [[88](#bib.bib88)]. Hyperactivations utilize a hypernetwork
    on top of an activation network, which are used to explore the AFs search space
    [[89](#bib.bib89)]. A shallow neural network is used in the activation network
    to produce the output for each input, whereas a neural network is used in the
    hypernetwork to produce weights for another network. A bi-modal derivative adaptive
    activation (BDAA) function uses twin maxima derivative sigmoidal function [[75](#bib.bib75)]
    by controlling the maxima’s position with an adaptive parameter. The BDAA is given
    as,
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 输出范围为 $(-\infty,\infty)$，其中 $\Phi(.)$ 由一个三次向量参数化。傅里叶级数展开用于非参数学习 AF (NPF) [[88](#bib.bib88)]。超激活利用了一个在激活网络之上的超网络，用于探索
    AF 搜索空间 [[89](#bib.bib89)]。在激活网络中使用浅层神经网络为每个输入生成输出，而在超网络中使用神经网络为另一个网络生成权重。双模导数自适应激活
    (BDAA) 函数使用双极极值导数 sigmoid 函数 [[75](#bib.bib75)]，通过自适应参数控制极值位置。BDAA 定义为，
- en: '|  | $BDAA(x)=\frac{1}{2}\times\left(\frac{1}{1+e^{-x}}-\frac{1}{1+e^{-x-a}}\right)$
    |  | (52) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $BDAA(x)=\frac{1}{2}\times\left(\frac{1}{1+e^{-x}}-\frac{1}{1+e^{-x-a}}\right)$
    |  | (52) |'
- en: in the output range of $[0,1]$ where $a$ is a learnable parameter. The authors
    have exploited the Bi-modal derivatives on four AFs. Linear regression is used
    in [[78](#bib.bib78)] to train AF for each neuron which results in different AFs
    for the different neurons. The TAF is defined as,
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $[0,1]$ 的输出范围内，其中 $a$ 是一个可学习的参数。作者在四种 AF 上利用了双模导数。 [[78](#bib.bib78)] 使用线性回归训练每个神经元的
    AF，这会导致不同神经元使用不同的 AF。TAF 定义为，
- en: '|  | $TAF(x)=\sqrt{(x-a)^{2}+b^{2}}$ |  | (53) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | $TAF(x)=\sqrt{(x-a)^{2}+b^{2}}$ |  | (53) |'
- en: in the output range of $[b,\infty)$, where $a$ and $b$ are the trainable parameters.
    Recently, a trainable parameter was used in different non-adaptive AFs such as
    Sigmoid, Tanh, and ReLU to make it adaptive [[90](#bib.bib90)].
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $[b,\infty)$ 的输出范围内，其中 $a$ 和 $b$ 是可训练的参数。最近，某些非自适应 AF 如 Sigmoid、Tanh 和 ReLU
    中使用了可训练参数，以使其适应 [[90](#bib.bib90)]。
- en: 'The adaptive and trainable AFs are the recent trend to adjust the non-linearity
    based on the data and network complexity. However, the minimal burden is increased
    in terms of the increased number of parameters. Though the complexity of tunable
    AFs is relatively increased w.r.t. non-tunable AFs, it is negligible w.r.t. all
    parameters of the entire network in practice. The same is also observed experimentally
    as reported in Table [10](#S9.T10 "Table 10 ‣ 9.2 Experimental Performance Analysis
    ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") in terms of the training time.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '自适应和可训练的 AF 是根据数据和网络复杂性调整非线性的最新趋势。然而，参数数量增加带来的最小负担有所增加。尽管可调 AF 的复杂性相对高于不可调
    AF，但在实际应用中与整个网络的所有参数相比，可以忽略不计。实验上也有类似观察，见表 [10](#S9.T10 "Table 10 ‣ 9.2 Experimental
    Performance Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark") 的训练时间。'
- en: 7 Miscellaneous Activation Functions
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 其他激活函数
- en: This section covers other attempts in AFs such as Softplus, Probabilistic, Polynomial,
    Subnetwork and Kernel.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了其他 AF 尝试，如 Softplus、概率、整数、子网络和核函数。
- en: 7.1 Softplus Activation Functions
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 Softplus 激活函数
- en: The softplus function [[91](#bib.bib91)] was proposed in 2001 as $\log(e^{x}+1)$
    and mostly used in statistical applications. After the breakthrough of deep learning
    the softmax function is used as the AF [[92](#bib.bib92)]. Softmax function produces
    the categorical probability distribution equivalent output. Softplus unit based
    AF is also used in deep neural networks [[93](#bib.bib93)]. The smooth nature
    of the Softplus facilitates the differentiability. The noisy softplus AF [[94](#bib.bib94)]
    is suitable for the spiking neural networks (SNNs). A Softplus Linear Unit (SLU)
    is also proposed by considering softplus with rectified unit [[95](#bib.bib95)].
    The SLU AF is defined as,
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Softplus 函数 [[91](#bib.bib91)] 于 2001 年提出，其形式为 $\log(e^{x}+1)$，主要用于统计应用。随着深度学习的突破，softmax
    函数作为 AF 被使用 [[92](#bib.bib92)]。Softmax 函数生成等效的分类概率分布输出。基于 Softplus 单元的 AF 也在深度神经网络中使用
    [[93](#bib.bib93)]。Softplus 的平滑特性有助于可微性。嘈杂的 softplus AF [[94](#bib.bib94)] 适用于尖峰神经网络
    (SNNs)。Softplus 线性单元 (SLU) 也通过考虑带有整流单元的 softplus 提出 [[95](#bib.bib95)]。SLU AF
    定义为，
- en: '|  | $SLU(x)=\begin{cases}\alpha\times x,&amp;x\geq 0\\ \beta\times\log(e^{x}+1)-\gamma,&amp;x<0\end{cases}$
    |  | (54) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $SLU(x)=\begin{cases}\alpha\times x,&amp;x\geq 0\\ \beta\times\log(e^{x}+1)-\gamma,&amp;x<0\end{cases}$
    |  | (54) |'
- en: where $\alpha$, $\beta$ and $\gamma$ are the trainable parameters with $\alpha$
    controlling the slope in the positive direction, $\beta$ controlling the saturation
    points in the negative direction and $\gamma$ controlling the offset in the negative
    direction w.r.t. the horizontal axis. The Rectified Softplus (ReSP) AF introduces
    the rectification for positive input in Softplus activation [[96](#bib.bib96)].
    In order to make the softplus function to follow the zero mean, a shifting and
    scaling of the outputs is performed in [[97](#bib.bib97)]. A Rand Softplus (RSP)
    AF models the stochasticity-adaptability of biological neurons as,
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$、$\beta$ 和 $\gamma$ 是可训练的参数，其中 $\alpha$ 控制正方向的斜率，$\beta$ 控制负方向的饱和点，$\gamma$
    控制相对于水平轴的负方向偏移。Rectified Softplus（ReSP）激活函数引入了对 Softplus 激活的正输入的修正[[96](#bib.bib96)]。为了使
    softplus 函数符合零均值，在 [[97](#bib.bib97)] 中对输出进行了平移和缩放。Rand Softplus（RSP）激活函数将生物神经元的随机适应性建模为，
- en: '|  | $RSP(x)=(1-\rho)\times\max(0,x)+\rho\times\log(1+e^{x})$ |  | (55) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | $RSP(x)=(1-\rho)\times\max(0,x)+\rho\times\log(1+e^{x})$ |  | (55) |'
- en: where $\rho$ is a stochastic hyperparameter [[98](#bib.bib98)]. It improves
    the capability of the network towards the noise. The softplus function is also
    used with Tanh function in Mish activation function [[99](#bib.bib99)], which
    is given as,
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\rho$ 是一个随机超参数[[98](#bib.bib98)]。它提高了网络对噪声的能力。softplus 函数还与 Tanh 函数一起用于
    Mish 激活函数[[99](#bib.bib99)]，其定义为，
- en: '|  | $Mish(x)=x\times Tanh(Softplus(x)).$ |  | (56) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $Mish(x)=x\times Tanh(Softplus(x)).$ |  | (56) |'
- en: The Mish is a non-monotonic and smooth AF. It has recently been used by the
    YOLOv4 model for object detection [[100](#bib.bib100)]. However, the increased
    complexity in Mish due to the multiple functions can be a limitation for the deep
    networks.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Mish 是一种非单调且平滑的激活函数。最近，YOLOv4 模型在目标检测中使用了它[[100](#bib.bib100)]。然而，由于多重函数的增加，Mish
    的复杂性可能对深度网络构成限制。
- en: 7.2 Probabilistic Activation Functions
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 概率激活函数
- en: So far, stochastic AFs have not been much explored due to expensive sampling
    processes. Few AFs exist in this category such as Randomized ReLU (RReLU) [[50](#bib.bib50)],
    Elastic ReLU (EReLU) [[40](#bib.bib40)], Randomly Translational ReLU (RTReLU)
    [[41](#bib.bib41)] and Gaussian Error Linear Unit (GELU) [[101](#bib.bib101)].
    GELU [[101](#bib.bib101)] considers nonlinearity as the stochastic regularization
    driven transformation and defined as,
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，由于采样过程昂贵，随机激活函数尚未得到充分探索。这类激活函数很少，如随机化 ReLU（RReLU）[[50](#bib.bib50)]、弹性 ReLU（EReLU）[[40](#bib.bib40)]、随机平移
    ReLU（RTReLU）[[41](#bib.bib41)] 和高斯误差线性单元（GELU）[[101](#bib.bib101)]。GELU [[101](#bib.bib101)]
    将非线性视为由随机正则化驱动的变换，并定义为，
- en: '|  | $GELU(x)=x\times P(X\leq x).$ |  | (57) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | $GELU(x)=x\times P(X\leq x).$ |  | (57) |'
- en: where $P$ is the probability. The complexity of GELU increases due to use of
    probabilistic nature. The GELU is also extended to the Symmetrical Gaussian Error
    Linear Unit (SGELU) [[102](#bib.bib102)] to enhance its ability of bidirectional
    convergence. Doubly truncated Gaussian distributions [[103](#bib.bib103)] is a
    family of nonlinearities which can generate different AFs such as Sigmoid, Tanh
    and ReLU by setting the appropriate truncation points. Probabilistic AF (ProbAct)
    introduces the adaptable and trainable variance in the ReLU’s output [[104](#bib.bib104)].
    It leads to the generalization of the models. However, all other drawbacks of
    ReLU exist with ProbAct also.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P$ 是概率。由于使用了概率性质，GELU 的复杂性增加。GELU 还被扩展为对称高斯误差线性单元（SGELU）[[102](#bib.bib102)]，以增强其双向收敛能力。双重截断高斯分布[[103](#bib.bib103)]
    是一种非线性函数族，可以通过设置适当的截断点生成不同的激活函数，如 Sigmoid、Tanh 和 ReLU。概率激活函数（ProbAct）引入了 ReLU
    输出中的可适应和可训练的方差[[104](#bib.bib104)]，这导致了模型的泛化。然而，ProbAct 也存在 ReLU 的所有其他缺陷。
- en: 7.3 Polynomial Activation Functions
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 多项式激活函数
- en: Smooth Adaptive AF (SAAF) is defined as the piecewise polynomial function [[105](#bib.bib105)].
    Two power functions symmetric to the linear part of ReLU are combined in [[106](#bib.bib106)]
    to improve the performance of ReLU. A piecewise polynomial approximation based
    AF is also learnt from the data [[107](#bib.bib107)]. This activation leads to
    the light-weight models suitable for the FPGAs and microcontrollers. The AF is
    also treated as the cumulative distribution function [[108](#bib.bib108)]. The
    ReLU is also extended to a Rectified Power Unit (RePU) for positive inputs as,
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑自适应 AF (SAAF) 定义为分段多项式函数 [[105](#bib.bib105)]。将对称于 ReLU 线性部分的两个幂函数组合在 [[106](#bib.bib106)]
    中，以提高 ReLU 的性能。还从数据中学习了一种基于分段多项式近似的 AF [[107](#bib.bib107)]。这种激活函数导致适用于 FPGA 和微控制器的轻量级模型。AF
    还被视为累积分布函数 [[108](#bib.bib108)]。ReLU 也扩展为正输入的修正幂单元 (RePU) 如下，
- en: '|  | $RePU(x)=\begin{cases}x^{s},&amp;x\geq 0\\ 0,&amp;x<0\end{cases}$ |  |
    (58) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | $RePU(x)=\begin{cases}x^{s},&amp;x\geq 0\\ 0,&amp;x<0\end{cases}$ |  |
    (58) |'
- en: where $s$ is a hyperparameter [[109](#bib.bib109)]. The RePU is suitable for
    smoother gradients near zero. However, vanishing gradient, unbounded and asymmetric
    nature are the downsides of RePU. The rational function of polynomials is better
    suited as compared to the polynomial functions in order to approximate the ReLU
    [[110](#bib.bib110)]. Recently, a Padé approximation is used to develop a non-smooth
    Padé Activation Unit (PAU) [[111](#bib.bib111)] as,
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s$ 是一个超参数 [[109](#bib.bib109)]。RePU 适合于接近零的平滑梯度。然而，RePU 的缺点包括梯度消失、无限制和非对称性。相比之下，多项式的有理函数更适合用来近似
    ReLU [[110](#bib.bib110)]。最近，使用 Padé 近似来开发非光滑的 Padé 激活单元 (PAU) [[111](#bib.bib111)]
    如下，
- en: '|  | $PAU(x)=\frac{P(x)}{Q(x)}$ |  | (59) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $PAU(x)=\frac{P(x)}{Q(x)}$ |  | (59) |'
- en: where $P(x)$ and $Q(x)$ are two polynomials of order $m$ and $n$, respectively.
    The PAUs can approximate the commonly used hand-designed AFs. Moreover, it can
    also learn the new AFs with compact representations. Recently, a Rational AF (RAF)
    [[112](#bib.bib112)] was proposed to tackle the problem of non-smooth nature of
    the PAU function.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P(x)$ 和 $Q(x)$ 是两个分别为 $m$ 和 $n$ 次的多项式。PAUs 可以近似常用的手动设计的 AFs。此外，它还可以学习具有紧凑表示的新
    AFs。最近，提出了一种有理 AF (RAF) [[112](#bib.bib112)] 来解决 PAU 函数的非光滑性问题。
- en: 7.4 Activations as a Subnetwork
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 作为子网络的激活
- en: A Variable AF (VAF) is used as a subnetwork of ReLUs [[113](#bib.bib113)]. It
    uses the ensemble of ReLUs in a subnetwork using learnable parameters. In a very
    similar approach, the maximum of multiple linear functions is used in the Dynamic
    ReLU (DY-ReLU) [[114](#bib.bib114)]. In Wide Hidden Expansion (WHE) [[115](#bib.bib115)],
    each WHE intermediate channel is followed by one AF before connecting to the output
    channel to increase the non-linearity of the network. An AF Unit (AFU) [[116](#bib.bib116)]
    uses a small neural network to model the activation. All neurons in the original
    network share the weights in AFU. The advantage of the AFU is that different AFs
    can be learnt by different layers.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 AF (VAF) 用作 ReLUs 的子网络 [[113](#bib.bib113)]。它使用具有可学习参数的 ReLUs 的集成。在非常类似的方法中，多个线性函数的最大值被用于动态
    ReLU (DY-ReLU) [[114](#bib.bib114)]。在宽隐藏扩展 (WHE) [[115](#bib.bib115)] 中，每个 WHE
    中间通道之后跟随一个 AF，然后连接到输出通道，以增加网络的非线性。AF 单元 (AFU) [[116](#bib.bib116)] 使用一个小型神经网络来建模激活。原始网络中的所有神经元在
    AFU 中共享权重。AFU 的优势在于不同的 AFs 可以在不同的层中学习。
- en: 7.5 Kernel Activation Functions
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 核激活函数
- en: A Kernel-based non-parametric AF (KAF) [[117](#bib.bib117)] uses an inexpensive
    kernel expansion to make the activation flexible. The KAF is further extended
    to multikernel AFs (multi-KAF) [[118](#bib.bib118)]. Several AFs are also introduced
    for complex valued neural networks [[119](#bib.bib119)], [[120](#bib.bib120)],
    [[121](#bib.bib121)].
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 基于核的非参数 AF (KAF) [[117](#bib.bib117)] 使用一种廉价的核扩展来使激活更加灵活。KAF 进一步扩展为多核 AFs (multi-KAF)
    [[118](#bib.bib118)]。还为复数值神经网络引入了几种 AFs [[119](#bib.bib119)], [[120](#bib.bib120)],
    [[121](#bib.bib121)]。
- en: 'Table 6: Summary of the existing state-of-the-art activation functions.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 现有最先进的激活函数总结。'
- en: '| Activation | Models | Datasets | Insights and Remarks |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | 模型 | 数据集 | 见解与备注 |'
- en: '| On Image Datasets |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 图像数据集 |'
- en: '| Wide Hidden Expansion (WHE) - 2020 [[115](#bib.bib115)] | ResNet, SENet,
    and MobileNet | CIFAR100 and ImageNet classification, Pascal VOC 2007 and COCO
    detection | Upto 2% higher Top-1 accuracy than baseline models of recognition
    and detection |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 宽隐藏扩展（WHE） - 2020 [[115](#bib.bib115)] | ResNet、SENet和MobileNet | CIFAR100和ImageNet分类、Pascal
    VOC 2007和COCO检测 | 比基准模型的识别和检测Top-1准确率高出最多2%。 |'
- en: '| Soft-Root-Sign (SRS) - 2020 [[26](#bib.bib26)] | VGG and MobileNet | CIFAR10
    and CIFAR100 classification | The SRS is better with MobileNet over both datasets
    and with VGG over CIFAR100\. The LReLU is better with VGG over CIFAR10. |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Soft-Root-Sign（SRS） - 2020 [[26](#bib.bib26)] | VGG和MobileNet | CIFAR10和CIFAR100分类
    | SRS在MobileNet上的表现优于这两个数据集，而在VGG上对CIFAR100更好。LReLU在VGG上对CIFAR10更好。 |'
- en: '| Relu-Memristor-Like AF (RMAF) - 2020 [[72](#bib.bib72)] | ResNet, AlexNet,
    SqueezeNet, and DenseNet | CIFAR10, CIFAR100, MNIST and ImageNet classification
    | The RMAF performs better than the ReLU, ELU, SELU, PReLU, Tanh and Swish. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| Relu-忆阻器样激活函数（RMAF） - 2020 [[72](#bib.bib72)] | ResNet、AlexNet、SqueezeNet和DenseNet
    | CIFAR10、CIFAR100、MNIST和ImageNet分类 | RMAF的表现优于ReLU、ELU、SELU、PReLU、Tanh和Swish。
    |'
- en: '| Parametric Deformable ELU (PDELU) - 2020 [[59](#bib.bib59)] | NIN and ResNet
    | CIFAR10 and CIFAR100 classification | The PDELU performs better than the ReLU,
    ELU and FReLU. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 参数化可变形ELU（PDELU） - 2020 [[59](#bib.bib59)] | NIN和ResNet | CIFAR10和CIFAR100分类
    | PDELU的表现优于ReLU、ELU和FReLU。 |'
- en: '| Pade Activation Unit (PAU) - 2020 [[111](#bib.bib111)] | VGG8, MobileNetV2,
    ResNet and DenseNet | MNIST, Fashion-MNIST, CIFAR10 and ImageNet classification
    | The PAU encode AFs as rational functions and performs better than many existing
    AFs. |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Pade激活单元（PAU） - 2020 [[111](#bib.bib111)] | VGG8、MobileNetV2、ResNet和DenseNet
    | MNIST、Fashion-MNIST、CIFAR10和ImageNet分类 | PAU将激活函数编码为有理函数，表现优于许多现有的激活函数。 |'
- en: '| Elastic Exponential Linear Unit (EELU) - 2020 [[58](#bib.bib58)] | A simple
    CNN model and VGG16 | CIFAR10, CIFAR100, ImageNet, and Tiny ImageNet classification
    | The EELU shows better results than the ReLU, ELU, EPReLU and Swish. |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 弹性指数线性单元（EELU） - 2020 [[58](#bib.bib58)] | 一个简单的CNN模型和VGG16 | CIFAR10、CIFAR100、ImageNet和Tiny
    ImageNet分类 | EELU显示出比ReLU、ELU、EPReLU和Swish更好的结果。 |'
- en: '| Dynamic ReLU (DY-ReLU) - 2020 [[114](#bib.bib114)] | MobileNetV2 | ImageNet
    classification and COCO detection | The DY-ReLU is suitable for light-weight networks.
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 动态ReLU（DY-ReLU） - 2020 [[114](#bib.bib114)] | MobileNetV2 | ImageNet分类和COCO检测
    | DY-ReLU适用于轻量级网络。 |'
- en: '| Variable AF (VAF) - 2019 [[113](#bib.bib113)] | Shallow CNN models | MNIST,
    Fashion MNIST and CIFAR10 classification | The VAF shows promising performance.
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 可变激活函数（VAF） - 2019 [[113](#bib.bib113)] | 浅层CNN模型 | MNIST、Fashion MNIST和CIFAR10分类
    | VAF显示出有前景的表现。 |'
- en: '| Multi-bin Trainable Linear Unit (MTLU) - 2019 [[46](#bib.bib46)] | FDnet
    and FSRnet | Image denoising and Super-resolution | The MTLU is significantly
    faster having comparable results with the state-of-the-arts. |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 多通道可训练线性单元（MTLU） - 2019 [[46](#bib.bib46)] | FDnet和FSRnet | 图像去噪和超分辨率 | MTLU显著更快，且结果与最先进的技术相当。
    |'
- en: '| Swish - 2018 [[29](#bib.bib29)] | MobileNet, ResNet, WRN and DenseNet | CIFAR10,
    CIFAR100 and ImageNet classification | The learnable parameter in Swish leads
    to improved performance than Softplus. |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Swish - 2018 [[29](#bib.bib29)] | MobileNet、ResNet、WRN和DenseNet | CIFAR10、CIFAR100和ImageNet分类
    | Swish中的可学习参数使得性能优于Softplus。 |'
- en: '| On Time Series Datasets |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 关于时间序列数据集 |'
- en: '| Variable AF (VAF) - 2019 [[113](#bib.bib113)] | Multi-Layered Neural Network
    | Regression tasks (Kinematics, Energy Cooling, Yatch, etc.) | Better performance
    over Kinematics, Energy Cooling and Yatch datasets. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 可变激活函数（VAF） - 2019 [[113](#bib.bib113)] | 多层神经网络 | 回归任务（运动学、能源冷却、游艇等） | 在运动学、能源冷却和游艇数据集上表现更佳。
    |'
- en: '| Self-Learnable AFs (SLAF) - 2019 [[79](#bib.bib79)] | Multi-Layered Neural
    Network | Boston Housing and Learning Sparse Polynomial regression | The newer
    parameter space makes the optimization easier. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 自学习激活函数（SLAF） - 2019 [[79](#bib.bib79)] | 多层神经网络 | 波士顿房价和稀疏多项式回归学习 | 更新的参数空间使得优化更容易。
    |'
- en: '| On Text Datasets |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 关于文本数据集 |'
- en: '| Soft-Root-Sign (SRS) - 2020 [[26](#bib.bib26)] | A 6 layer transformer network
    | IWSLT 2016 German-English translation | The SRS is better over tst2011 and tst2012
    test sets, whereas the SELU and LReLU are better over tst2013 and tst2014 test
    sets, respectively. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Soft-Root-Sign（SRS） - 2020 [[26](#bib.bib26)] | 一个6层的transformer网络 | IWSLT
    2016德英翻译 | SRS在tst2011和tst2012测试集上表现更好，而SELU和LReLU分别在tst2013和tst2014测试集上表现更好。
    |'
- en: '| Swish - 2018 [[29](#bib.bib29)] | A 12 layer transformer network | WMT 2014
    English-German dataset | The performance of Swish is comparable to state-of-the-arts.
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Swish - 2018 [[29](#bib.bib29)] | 12 层变换器网络 | WMT 2014 英德数据集 | Swish 的性能可与最先进的技术媲美。
    |'
- en: '| PenalizedTanh - 2018 [[33](#bib.bib33)] | MLP, CNN and RNN | Sentence classification,
    Document classification and Sentence tagging | The PenalizedTanh exhibits the
    stability across the different tasks in contrast to the Swish function. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| PenalizedTanh - 2018 [[33](#bib.bib33)] | MLP、CNN 和 RNN | 句子分类、文档分类和句子标注
    | 与 Swish 函数相比，PenalizedTanh 在不同任务中表现出稳定性。 |'
- en: '| On Signal Datasets |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 信号数据集 |'
- en: '| Rectified Linear Tanh (ReLTanh) - 2019 [[67](#bib.bib67)] | Stacked autoencoder
    (SAE) based DNN | Vibration signals for rotating machinery fault diagnosis | The
    ReLTanh leads to larger gradients for faster learning and reduces the vanishing
    gradient. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 线性整流 Tanh (ReLTanh) - 2019 [[67](#bib.bib67)] | 基于堆叠自编码器 (SAE) 的深度神经网络 |
    旋转机械故障诊断的振动信号 | ReLTanh 使梯度更大，学习更快，并减少了梯度消失。 |'
- en: '| On Game Datasets |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 游戏数据集 |'
- en: '| Sigmoid-weighted Linear Unit (SiLU) - 2018 [[23](#bib.bib23)] | Deep reinforcement
    learning algorithm | SZ-Tetris, $10\times 10$ Tetris, and Atari 2600 games | The
    SiLU AF outperforms the ReLU function for reinforcement learning. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid 加权线性单元 (SiLU) - 2018 [[23](#bib.bib23)] | 深度强化学习算法 | SZ-Tetris、$10\times
    10$ Tetris 和 Atari 2600 游戏 | SiLU 激活函数在强化学习中优于 ReLU 函数。 |'
- en: 8 Aspects of Activation Functions
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数的 8 个方面
- en: This section summarizes the effect of weight initialization, understanding of
    AFs and suitability with different types of data. The learning of the network
    speeds up drastically by using the orthogonal weight initialization based on the
    dynamical isometry [[122](#bib.bib122)]. A set of conditions in parameter initialization
    also boosts the performance of networks with sigmoidal activations [[123](#bib.bib123)].
    The symmetric probability distribution based weights and biases initialization
    leads the network to suffer with the dying ReLU problem. However, the asymmetric
    initialization resolves the dying ReLU problem [[124](#bib.bib124)]. The over-parameterization
    during initialization also benefits in the training [[125](#bib.bib125)]. The
    data-dependent weight initialization using a subset of data minimizes the issues
    of the ReLU [[126](#bib.bib126)], whereas an initial parameter sharing based initialization
    guarantees the dynamical isometry for the ReLU [[127](#bib.bib127)].
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了权重初始化的效果、激活函数的理解以及与不同数据类型的适用性。使用基于动态同构的正交权重初始化可以显著加快网络的学习速度[[122](#bib.bib122)]。一组参数初始化条件也提升了具有
    sigmoid 激活函数的网络的性能[[123](#bib.bib123)]。基于对称概率分布的权重和偏置初始化导致网络出现 ReLU 死亡问题。然而，非对称初始化解决了
    ReLU 死亡问题[[124](#bib.bib124)]。初始化期间的过度参数化也有利于训练[[125](#bib.bib125)]。使用数据子集的依赖数据权重初始化最小化了
    ReLU 的问题[[126](#bib.bib126)]，而基于初始参数共享的初始化保证了 ReLU 的动态同构[[127](#bib.bib127)]。
- en: Several researchers have tried to understand the working and impact of AFs through
    different strategies. The lower and upper bounds are established for network complexity
    to realize that the ReLU in deep networks approximates the smooth functions more
    efficiently as compared to shallow networks [[128](#bib.bib128)]. A ReLU network
    with only one hidden layer is trained to reach the global optimum in polynomial
    time even with exponentially growing input dimension [[129](#bib.bib129)]. The
    ReLU type AF based neural networks produce the overconfident predictions far away
    from the training data [[130](#bib.bib130)]. However, this can be resolved by
    employing adversarial confidence enhanced training. A Gaussian margin driven time
    and accuracy tradeoff analysis is also done on the ReLU’s learning [[131](#bib.bib131)].
    The singular values for ReLU layers are analyzed to understand the interaction
    of ReLU with the linear components [[132](#bib.bib132)]. The approximation of
    Gaussian posterior distribution over the ReLU network weight’s fixes the overconfidence
    problem [[133](#bib.bib133)].
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究者尝试通过不同的策略来理解AFs的工作原理和影响。已建立网络复杂度的下界和上界，以实现深度网络中的ReLU比浅层网络更有效地近似平滑函数[[128](#bib.bib128)]。即使在输入维度指数增长的情况下，一个仅有一层隐藏层的ReLU网络也能在多项式时间内达到全局最优[[129](#bib.bib129)]。基于ReLU的神经网络产生了远离训练数据的过度自信预测[[130](#bib.bib130)]。然而，通过采用对抗性信心增强训练可以解决这个问题。对ReLU学习进行的高斯边际驱动的时间与精度权衡分析也已完成[[131](#bib.bib131)]。对ReLU层的奇异值进行分析，以了解ReLU与线性组件的交互[[132](#bib.bib132)]。对ReLU网络权重的高斯后验分布进行的近似解决了过度自信的问题[[133](#bib.bib133)]。
- en: 'Despite most of the AFs are tested over image data, there are few research
    papers dealing with the AFs over other types of data. Table [6](#S7.T6 "Table
    6 ‣ 7.5 Kernel Activation Functions ‣ 7 Miscellaneous Activation Functions ‣ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark") summarizes
    the insights and remarks of state-of-the-art AFs for various networks and datasets.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管大多数AFs是在图像数据上进行测试的，但涉及其他数据类型的AFs的研究论文却很少。表[6](#S7.T6 "Table 6 ‣ 7.5 Kernel
    Activation Functions ‣ 7 Miscellaneous Activation Functions ‣ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark")总结了最新AFs在各种网络和数据集上的见解和备注。'
- en: 9 Performance Comparison and Analysis
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 性能比较与分析
- en: This survey is compared with the existing survey/performance analysis and the
    experimental performance analysis of selected AFs is performed over Image, Text
    and Speech data.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查与现有调查/性能分析进行比较，并对所选的AFs在图像、文本和语音数据上的实验性能分析进行评估。
- en: 9.1 Comparison with Existing Survey/Performance Analysis
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 与现有调查/性能分析的比较
- en: A performance analysis of AFs was conducted using multilayer perceptron network
    in [[134](#bib.bib134)]. Among compared AFs, the Tanh has shown better performance.
    A comparative performance analysis of different AFs suggests an Elliott function
    as better suited for classification using LSTM networks [[25](#bib.bib25)]. The
    ELU outperforms the ReLU, LReLU, and SELU AFs over MNIST classification task using
    Deep Neural Networks [[135](#bib.bib135)]. As per [[136](#bib.bib136)], the ELU
    is reported in [[135](#bib.bib135)] to outperform the ReLU, LReLU, PReLU and PELU
    over sufficiently large datasets for speech recognition. However, for smaller
    datasets, the ReLU is preferred. A similar trend is also reported in [[137](#bib.bib137)]
    with a note that the ELU and SELU AFs exhibit faster convergence as compared to
    the ReLU and LReLU AFs. In [[138](#bib.bib138)], 21 AFs are listed without experimental
    results comparison. In contrast to [[138](#bib.bib138)], this paper presents a
    comprehensive survey of AFs. The ReLU based deep networks perform superior or
    mildly worse than the spline methods [[139](#bib.bib139)]. A review of adaptive
    functions is conducted in [[140](#bib.bib140)] by considering 9 functions, including
    Sigmoid, Tanh, PReLU, and adaptTanh. In [[141](#bib.bib141)], the comparison between
    ReLU and LReLU is performed using CNN on MNIST dataset. An empirical study is
    also done for the variations of ReLU activation by generalizing it with the help
    of parameters [[142](#bib.bib142)]. The comparison of AFs is also performed for
    generalized learning vector quantization [[143](#bib.bib143)]. The ReLU activation
    has performed better for object, face, and text datasets [[144](#bib.bib144)].
    However, the SELU and Maxout have performed better for medical and sound datasets,
    respectively [[144](#bib.bib144)]. The piecewise AF is better suited for facial
    expression recognition in [[145](#bib.bib145)]. A survey of adaptive AFs is conducted
    in [[146](#bib.bib146)] without experimental comparison. The evaluation of seven
    AFs is conducted in [[147](#bib.bib147)] using a simple network over CIFAR10 dataset,
    whereas in our survey we cover different AFs and also perform the experimental
    comparison.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[134](#bib.bib134)] 中使用多层感知机网络对激活函数进行了性能分析。在比较的激活函数中，Tanh 显示出更好的性能。对不同激活函数的比较性能分析表明，Elliott
    函数更适合用于 LSTM 网络的分类 [[25](#bib.bib25)]。在 MNIST 分类任务中，ELU 的表现优于 ReLU、LReLU 和 SELU
    激活函数 [[135](#bib.bib135)]。根据 [[136](#bib.bib136)]，在 [[135](#bib.bib135)] 中报告了
    ELU 在足够大的数据集上优于 ReLU、LReLU、PReLU 和 PELU，用于语音识别。然而，对于较小的数据集，ReLU 更受欢迎。 [[137](#bib.bib137)]
    中也报告了类似的趋势，并指出 ELU 和 SELU 激活函数的收敛速度比 ReLU 和 LReLU 激活函数更快。在 [[138](#bib.bib138)]
    中列出了 21 种激活函数，但没有进行实验结果比较。与 [[138](#bib.bib138)] 相比，本文提供了对激活函数的全面调查。基于 ReLU 的深度网络在性能上优于或仅稍逊于
    spline 方法 [[139](#bib.bib139)]。在 [[140](#bib.bib140)] 中，通过考虑 9 种函数，包括 Sigmoid、Tanh、PReLU
    和 adaptTanh，对自适应函数进行了综述。在 [[141](#bib.bib141)] 中，使用 CNN 在 MNIST 数据集上对 ReLU 和 LReLU
    进行了比较。还通过参数化的一般化方法对 ReLU 激活函数的变体进行了实证研究 [[142](#bib.bib142)]。对自适应学习向量量化的激活函数也进行了比较
    [[143](#bib.bib143)]。在 [[144](#bib.bib144)] 中，ReLU 激活函数在对象、面部和文本数据集上表现更好。然而，SELU
    和 Maxout 分别在医学和声音数据集上表现更好 [[144](#bib.bib144)]。在 [[145](#bib.bib145)] 中，分段激活函数更适合面部表情识别。在
    [[146](#bib.bib146)] 中，对自适应激活函数进行了调查，但未进行实验比较。在 [[147](#bib.bib147)] 中，对七种激活函数进行了简单网络下的评估，而在我们的调查中，我们涵盖了不同的激活函数，并进行了实验比较。
- en: 'Table 7: Comparison of this survey with the existing surveys and performance
    evaluations.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 本次调查与现有调查和性能评估的比较。'
- en: '| Method | Models | Activations | Datasets | Remarks |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 模型 | 激活函数 | 数据集 | 备注 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Karlik and Olgac [[134](#bib.bib134)] | Multilayer Perceptron (MLP) | 5 AFs,
    including Bi-polar sigmoid, Uni-polar sigmoid, Tanh, etc. | Classification | The
    Tanh performs better compared to other traditional AFs. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Karlik 和 Olgac [[134](#bib.bib134)] | 多层感知机 (MLP) | 5 种激活函数，包括双极 sigmoid、单极
    sigmoid、Tanh 等 | 分类 | 与其他传统激活函数相比，Tanh 表现更好。 |'
- en: '| Vydana and Vuppala (2017) [[136](#bib.bib136)] | Hidden Markov Model-Deep
    Neural Network (HMM-DNN) | 5 AFs, including ReLU, LReLU, PReLU, ELU, and PELU
    | TIMIT and WSJ speech recognition | The ELU is better over sufficiently larger
    size datasets. However, the ReLU is preferred for smaller datasets. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Vydana 和 Vuppala (2017) [[136](#bib.bib136)] | 隐马尔可夫模型-深度神经网络 (HMM-DNN) |
    5 种激活函数，包括 ReLU、LReLU、PReLU、ELU 和 PELU | TIMIT 和 WSJ 语音识别 | 对于足够大的数据集，ELU 表现更好。然而，对于较小的数据集，ReLU
    更受青睐。 |'
- en: '| Alcantara (2017) [[135](#bib.bib135)] | A neural network with 2 hidden layers
    having 100 neurons/layer | 4 AFs, including ReLU, LReLU, ELU, and SELU | MNIST
    classification | The ELU AF outperforms others. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Alcantara (2017) [[135](#bib.bib135)] | 一个具有 2 层隐藏层、每层 100 个神经元的神经网络 | 4
    种激活函数，包括 ReLU、LReLU、ELU 和 SELU | MNIST 分类 | ELU 激活函数优于其他激活函数。 |'
- en: '| Pedamonti (2018) [[137](#bib.bib137)] | A neural network with 2 hidden layers
    having 100 neurons/layer | 5 AFs, including Sigmoid, ReLU, LReLU, ELU, and SELU
    | MNIST classification | The ELU and SELU AFs exhibit the faster convergence as
    compared to the ReLU and LReLU AFs. |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Pedamonti (2018) [[137](#bib.bib137)] | 一个具有 2 层隐藏层、每层 100 个神经元的神经网络 | 5
    种激活函数，包括 Sigmoid、ReLU、LReLU、ELU 和 SELU | MNIST 分类 | ELU 和 SELU 激活函数的收敛速度比 ReLU
    和 LReLU 更快。 |'
- en: '| Lau and Lim (2018) [[140](#bib.bib140)] | Deep Neural Network (DNN) | ReLU
    and Adaptive ReLU | MNIST classification | The adaptive AFs improve the generalization
    of the network. |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Lau 和 Lim (2018) [[140](#bib.bib140)] | 深度神经网络 (DNN) | ReLU 和自适应 ReLU | MNIST
    分类 | 自适应激活函数提高了网络的泛化能力。 |'
- en: '| Farzad et al. (2019) [[25](#bib.bib25)] | Long Short Term Memory (LSTM) |
    23 AFs, including Elliott, Gaussian, Logarithmic, Loglog, etc. | IMDB, Movie Review,
    MNIST classification | Elliott function is better suited to the LSTM network.
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Farzad 等 (2019) [[25](#bib.bib25)] | 长短期记忆网络 (LSTM) | 23 种激活函数，包括 Elliott、Gaussian、Logarithmic、Loglog
    等 | IMDB、电影评论、MNIST 分类 | Elliott 函数更适合 LSTM 网络。 |'
- en: '| Dubey and Jain (2019) [[141](#bib.bib141)] | Simple Convolutional Neural
    Network (CNN) | 2 AFs, including ReLU and Leaky ReLU | MNIST classification |
    The ReLU performed better than Leaky ReLU (LReLU). |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Dubey 和 Jain (2019) [[141](#bib.bib141)] | 简单卷积神经网络 (CNN) | 2 种激活函数，包括 ReLU
    和 Leaky ReLU | MNIST 分类 | ReLU 的表现优于 Leaky ReLU (LReLU)。 |'
- en: '| Banerjee et al. (2019) [[142](#bib.bib142)] | Convolutional Neural Network
    (CNN) | Generalized ReLU | MNIST classification | Network learns the parameters
    for different ReLU variations. |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Banerjee 等 (2019) [[142](#bib.bib142)] | 卷积神经网络 (CNN) | 广义 ReLU | MNIST 分类
    | 网络学习不同 ReLU 变体的参数。 |'
- en: '| Villmann et al. (2019) [[143](#bib.bib143)] | Generalized learning vector
    quantization (GLVQ) | 12 AFs, including Sigmoid, Swish, ReLU, Softplus, etc. |
    Tecator, Indian Pine and Wisconsin-Breast-Cancer classification | The Sigmoid,
    Swish and Softplus AFs are better suited with GLVQ. |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Villmann 等 (2019) [[143](#bib.bib143)] | 广义学习向量量化 (GLVQ) | 12 种激活函数，包括 Sigmoid、Swish、ReLU、Softplus
    等 | Tecator、Indian Pine 和 Wisconsin-Breast-Cancer 分类 | Sigmoid、Swish 和 Softplus
    激活函数更适合 GLVQ。 |'
- en: '| Castaneda et al. (2019) [[144](#bib.bib144)] | 6 different models for different
    applications | 3 AFs, including ReLU, SELU and Maxout | Object, Face, Text, Medical
    and Sound datasets | The ReLU is better for object, face and text datasets, whereas
    SELU and Maxout are better for medical and sound datasets, respectively. |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Castaneda 等 (2019) [[144](#bib.bib144)] | 6 种不同模型用于不同应用 | 3 种激活函数，包括 ReLU、SELU
    和 Maxout | 对象、面部、文本、医疗和声音数据集 | ReLU 更适用于对象、面部和文本数据集，而 SELU 和 Maxout 分别更适用于医疗和声音数据集。
    |'
- en: '| Wang et al. (2020) [[145](#bib.bib145)] | Inception-v3 model | 6 AFs, including
    Sigmoid, Tanh, ReLu, etc. | JAFFE and FER2013 facial expression recognition |
    The combination of log, softdesign and ReLU AFs provides improved performance.
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 (2020) [[145](#bib.bib145)] | Inception-v3 模型 | 6 种激活函数，包括 Sigmoid、Tanh、ReLU
    等 | JAFFE 和 FER2013 面部表情识别 | log、softdesign 和 ReLU 激活函数的组合提供了更好的性能。 |'
- en: '| Szandala (2020) [[147](#bib.bib147)] | A simple network | 7 AFs, including
    Sigmoid, Tanh, ReLU, LReLU, Swish, etc. | CIFAR10 classification | The LReLU performs
    better. The ReLU is efficient. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Szandala (2020) [[147](#bib.bib147)] | 一个简单的网络 | 7 种激活函数，包括 Sigmoid、Tanh、ReLU、LReLU、Swish
    等 | CIFAR10 分类 | LReLU 的表现更佳。ReLU 效率较高。 |'
- en: '| Our survey and performance analysis | MobileNet, VGG, GoogLeNet, ResNet,
    SENet, DenseNet, etc. | Exhaustive list of AFs, including performance analysis
    over $18$ state-of-the-art activations | CIFAR10 classification, Language translation,
    Speech recognition | A classification to categorize and analyze the AFs and a
    performance comparison of the state-of-the-art activations. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 我们的调查和性能分析 | MobileNet、VGG、GoogLeNet、ResNet、SENet、DenseNet 等 | 激活函数的详尽列表，包括
    $18$ 种最先进的激活函数性能分析 | CIFAR10 分类、语言翻译、语音识别 | 对激活函数进行分类、分析，并比较最先进激活函数的性能。 |'
- en: 'A summary of the comparison with existing surveys and performance analysis
    of AF is shown in Table [7](#S9.T7 "Table 7 ‣ 9.1 Comparison with Existing Survey/Performance
    Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"). Following are the observations:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [7](#S9.T7 "表 7 ‣ 9.1 与现有调查/性能分析的比较 ‣ 9 性能比较与分析 ‣ 深度学习中的激活函数：全面调查与基准") 显示了与现有调查和
    AF 性能分析的比较总结。以下是观察结果：
- en: '1.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: This survey presents a detailed classification to cover the wide range of AFs
    as compared to the existing surveys and performance analysis.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查提供了详细的分类，以覆盖广泛的 AFs，与现有调查和性能分析相比更为全面。
- en: '2.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: This survey covers exhaustive state-of-the-art AFs to date, whereas the existing
    survey/performance analysis covers either a limited number of AFs or only basic
    AFs.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查涵盖了截至目前的最新 AFs，而现有的调查/性能分析要么涵盖有限数量的 AFs，要么仅涵盖基础的 AFs。
- en: '3.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: The performance analysis conducted in this paper considers a wide range of neural
    networks over different types of data for eighteen AFs, whereas the existing analysis
    is limited to a single type of data and network.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文中进行的性能分析考虑了不同数据类型下的广泛神经网络，涵盖了十八种 AFs，而现有分析仅限于单一数据和网络类型。
- en: '4.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: This survey highlights the trends to help the researchers to further explore
    the better AFs and practitioners to choose based on the data and network types.
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查强调了趋势，以帮助研究人员进一步探索更好的 AFs（激活函数）以及实践者根据数据和网络类型做出选择。
- en: 'Table 8: Experimental results comparison over CIFAR10 dataset.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 在 CIFAR10 数据集上的实验结果比较。'
- en: '| Accuracy | CNN Models |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | CNN 模型 |'
- en: '| --- | --- |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Activations | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Sigmoid | 88.60 $\pm$ 0.17 | 87.69 $\pm$ 0.49 | 87.33 $\pm$ 2.48 | 80.13
    $\pm$ 3.33 | 90.29 $\pm$ 0.29 | 89.92 $\pm$ 1.96 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | 88.60 $\pm$ 0.17 | 87.69 $\pm$ 0.49 | 87.33 $\pm$ 2.48 | 80.13
    $\pm$ 3.33 | 90.29 $\pm$ 0.29 | 89.92 $\pm$ 1.96 |'
- en: '| Tanh | 87.21 $\pm$ 0.24 | 90.49 $\pm$ 0.11 | 90.16 $\pm$ 1.86 | 89.09 $\pm$
    1.47 | 90.44 $\pm$ 0.09 | 91.80 $\pm$ 0.69 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Tanh | 87.21 $\pm$ 0.24 | 90.49 $\pm$ 0.11 | 90.16 $\pm$ 1.86 | 89.09 $\pm$
    1.47 | 90.44 $\pm$ 0.09 | 91.80 $\pm$ 0.69 |'
- en: '| Elliott [[25](#bib.bib25)] | 88.48 $\pm$ 0.18 | 87.94 $\pm$ 0.49 | 89.84
    $\pm$ 3.43 | 81.60 $\pm$ 3.91 | 90.25 $\pm$ 0.25 | 91.53 $\pm$ 1.04 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Elliott [[25](#bib.bib25)] | 88.48 $\pm$ 0.18 | 87.94 $\pm$ 0.49 | 89.84
    $\pm$ 3.43 | 81.60 $\pm$ 3.91 | 90.25 $\pm$ 0.25 | 91.53 $\pm$ 1.04 |'
- en: '| ReLU [[8](#bib.bib8)] | 90.10 $\pm$ 0.22 | 92.84  $\pm$ 0.19 | 93.43  $\pm$
    0.48 | 93.74 $\pm$ 0.34 | 93.70 $\pm$ 0.16 | 93.96  $\pm$ 0.51 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| ReLU [[8](#bib.bib8)] | 90.10 $\pm$ 0.22 | 92.84  $\pm$ 0.19 | 93.43  $\pm$
    0.48 | 93.74 $\pm$ 0.34 | 93.70 $\pm$ 0.16 | 93.96  $\pm$ 0.51 |'
- en: '| LReLU [[17](#bib.bib17)] | 90.10 $\pm$ 0.19 | 91.09 $\pm$ 0.09 | 89.28 $\pm$
    0.82 | 93.83  $\pm$ 0.42 | 93.66 $\pm$ 0.19 | 93.85 $\pm$ 0.48 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| LReLU [[17](#bib.bib17)] | 90.10 $\pm$ 0.19 | 91.09 $\pm$ 0.09 | 89.28 $\pm$
    0.82 | 93.83  $\pm$ 0.42 | 93.66 $\pm$ 0.19 | 93.85 $\pm$ 0.48 |'
- en: '| PReLU [[35](#bib.bib35)] | 90.43 $\pm$ 0.18 | 92.19 $\pm$ 0.08 | 92.85 $\pm$
    0.55 | 92.99 $\pm$ 0.62 | 92.76 $\pm$ 0.26 | 92.82 $\pm$ 0.63 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| PReLU [[35](#bib.bib35)] | 90.43 $\pm$ 0.18 | 92.19 $\pm$ 0.08 | 92.85 $\pm$
    0.55 | 92.99 $\pm$ 0.62 | 92.76 $\pm$ 0.26 | 92.82 $\pm$ 0.63 |'
- en: '| ELU [[27](#bib.bib27)] | 90.92 $\pm$ 0.25 | 88.55 $\pm$ 1.17 | 92.47 $\pm$
    0.76 | 93.53 $\pm$ 0.66 | 93.39 $\pm$ 0.20 | 92.89 $\pm$ 0.62 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| ELU [[27](#bib.bib27)] | 90.92 $\pm$ 0.25 | 88.55 $\pm$ 1.17 | 92.47 $\pm$
    0.76 | 93.53 $\pm$ 0.66 | 93.39 $\pm$ 0.20 | 92.89 $\pm$ 0.62 |'
- en: '| SELU [[52](#bib.bib52)] | 90.11 $\pm$ 0.32 | 92.25 $\pm$ 0.28 | 91.87 $\pm$
    0.84 | 93.53 $\pm$ 0.52 | 89.96 $\pm$ 0.31 | 92.71 $\pm$ 0.73 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| SELU [[52](#bib.bib52)] | 90.11 $\pm$ 0.32 | 92.25 $\pm$ 0.28 | 91.87 $\pm$
    0.84 | 93.53 $\pm$ 0.52 | 89.96 $\pm$ 0.31 | 92.71 $\pm$ 0.73 |'
- en: '| GELU [[101](#bib.bib101)] | 90.71 $\pm$ 0.20 | 92.42 $\pm$ 0.09 | 93.16 $\pm$
    0.61 | 93.81 $\pm$ 0.46 | 93.72  $\pm$ 0.18 | 93.90  $\pm$ 0.41 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| GELU [[101](#bib.bib101)] | 90.71 $\pm$ 0.20 | 92.42 $\pm$ 0.09 | 93.16 $\pm$
    0.61 | 93.81 $\pm$ 0.46 | 93.72  $\pm$ 0.18 | 93.90  $\pm$ 0.41 |'
- en: '| CELU [[53](#bib.bib53)] | 91.04  $\pm$ 0.17 | 88.11 $\pm$ 0.14 | 92.60 $\pm$
    0.60 | 94.09  $\pm$ 0.17 | 91.63 $\pm$ 0.22 | 93.46 $\pm$ 0.35 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| CELU [[53](#bib.bib53)] | 91.04  $\pm$ 0.17 | 88.11 $\pm$ 0.14 | 92.60 $\pm$
    0.60 | 94.09  $\pm$ 0.17 | 91.63 $\pm$ 0.22 | 93.46 $\pm$ 0.35 |'
- en: '| Softplus [[93](#bib.bib93)] | 91.05  $\pm$ 0.22 | 92.69 $\pm$ 0.20 | 92.66
    $\pm$ 0.66 | 93.34 $\pm$ 0.65 | 93.25 $\pm$ 0.11 | 93.07 $\pm$ 0.70 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Softplus [[93](#bib.bib93)] | 91.05  $\pm$ 0.22 | 92.69 $\pm$ 0.20 | 92.66
    $\pm$ 0.66 | 93.34 $\pm$ 0.65 | 93.25 $\pm$ 0.11 | 93.07 $\pm$ 0.70 |'
- en: '| Swish [[29](#bib.bib29)] | 90.66 $\pm$ 0.34 | 92.32 $\pm$ 0.20 | 92.68 $\pm$
    0.53 | 93.02 $\pm$ 0.85 | 93.24 $\pm$ 0.19 | 93.16 $\pm$ 0.51 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| Swish [[29](#bib.bib29)] | 90.66 $\pm$ 0.34 | 92.32 $\pm$ 0.20 | 92.68 $\pm$
    0.53 | 93.02 $\pm$ 0.85 | 93.24 $\pm$ 0.19 | 93.16 $\pm$ 0.51 |'
- en: '| ABReLU [[44](#bib.bib44)] | 88.97 $\pm$ 0.47 | 92.36 $\pm$ 0.15 | 93.34 $\pm$
    0.23 | 93.29 $\pm$ 0.52 | 93.35 $\pm$ 0.14 | 93.26 $\pm$ 0.55 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| ABReLU [[44](#bib.bib44)] | 88.97 $\pm$ 0.47 | 92.36 $\pm$ 0.15 | 93.34 $\pm$
    0.23 | 93.29 $\pm$ 0.52 | 93.35 $\pm$ 0.14 | 93.26 $\pm$ 0.55 |'
- en: '| LiSHT [[24](#bib.bib24)] | 86.53 $\pm$ 0.49 | 89.83 $\pm$ 0.28 | 90.27 $\pm$
    0.80 | 90.89 $\pm$ 0.66 | 90.25 $\pm$ 0.84 | 87.91 $\pm$ 0.93 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| LiSHT [[24](#bib.bib24)] | 86.53 $\pm$ 0.49 | 89.83 $\pm$ 0.28 | 90.27 $\pm$
    0.80 | 90.89 $\pm$ 0.66 | 90.25 $\pm$ 0.84 | 87.91 $\pm$ 0.93 |'
- en: '| SRS [[26](#bib.bib26)] | 89.43 $\pm$ 0.81 | 92.06 $\pm$ 0.26 | 91.36 $\pm$
    1.19 | 92.28 $\pm$ 0.48 | 78.05 $\pm$ 1.37 | 90.64 $\pm$ 1.93 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| SRS [[26](#bib.bib26)] | 89.43 $\pm$ 0.81 | 92.06 $\pm$ 0.26 | 91.36 $\pm$
    1.19 | 92.28 $\pm$ 0.48 | 78.05 $\pm$ 1.37 | 90.64 $\pm$ 1.93 |'
- en: '| Mish [[99](#bib.bib99)] | 90.82 $\pm$ 0.15 | 92.85  $\pm$ 0.25 | 93.29 $\pm$
    0.61 | 93.69 $\pm$ 0.63 | 93.66 $\pm$ 0.12 | 93.62 $\pm$ 0.62 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Mish [[99](#bib.bib99)] | 90.82 $\pm$ 0.15 | 92.85 $\pm$ 0.25 | 93.29 $\pm$
    0.61 | 93.69 $\pm$ 0.63 | 93.66 $\pm$ 0.12 | 93.62 $\pm$ 0.62 |'
- en: '| PAU [[111](#bib.bib111)] | 90.67 $\pm$ 0.17 | 92.00 $\pm$ 0.26 | 92.80 $\pm$
    0.65 | 93.67 $\pm$ 0.52 | 93.08 $\pm$ 0.20 | 93.05 $\pm$ 0.53 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| PAU [[111](#bib.bib111)] | 90.67 $\pm$ 0.17 | 92.00 $\pm$ 0.26 | 92.80 $\pm$
    0.65 | 93.67 $\pm$ 0.52 | 93.08 $\pm$ 0.20 | 93.05 $\pm$ 0.53 |'
- en: '| PDELU [[59](#bib.bib59)] | 90.18 $\pm$ 0.19 | 92.80 $\pm$ 0.13 | 93.49  $\pm$
    0.30 | 93.42 $\pm$ 0.71 | 93.71  $\pm$ 0.07 | 93.96  $\pm$ 0.59 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| PDELU [[59](#bib.bib59)] | 90.18 $\pm$ 0.19 | 92.80 $\pm$ 0.13 | 93.49 $\pm$
    0.30 | 93.42 $\pm$ 0.71 | 93.71 $\pm$ 0.07 | 93.96 $\pm$ 0.59 |'
- en: 'Table 9: Experimental results comparison over CIFAR100 dataset.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 9: CIFAR100 数据集上的实验结果比较。'
- en: '| Accuracy | CNN Models |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | CNN 模型 |'
- en: '| --- | --- |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Activations | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121
    |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Sigmoid | 61.88 $\pm$ 0.18 | 37.75 $\pm$ 0.59 | 70.31 $\pm$ 0.54 | 46.78
    $\pm$ 5.42 | 66.17 $\pm$ 1.16 | 68.31 $\pm$ 2.41 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | 61.88 $\pm$ 0.18 | 37.75 $\pm$ 0.59 | 70.31 $\pm$ 0.54 | 46.78
    $\pm$ 5.42 | 66.17 $\pm$ 1.16 | 68.31 $\pm$ 2.41 |'
- en: '| Tanh | 53.10 $\pm$ 0.51 | 58.43 $\pm$ 0.38 | 67.66 $\pm$ 2.32 | 64.32 $\pm$
    1.69 | 60.13 $\pm$ 1.86 | 69.53 $\pm$ 1.68 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Tanh | 53.10 $\pm$ 0.51 | 58.43 $\pm$ 0.38 | 67.66 $\pm$ 2.32 | 64.32 $\pm$
    1.69 | 60.13 $\pm$ 1.86 | 69.53 $\pm$ 1.68 |'
- en: '| Elliott [[25](#bib.bib25)] | 60.70 $\pm$ 0.34 | 33.20 $\pm$ 0.97 | 64.85
    $\pm$ 6.28 | 49.88 $\pm$ 4.03 | 66.30 $\pm$ 0.28 | 69.58 $\pm$ 2.40 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Elliott [[25](#bib.bib25)] | 60.70 $\pm$ 0.34 | 33.20 $\pm$ 0.97 | 64.85
    $\pm$ 6.28 | 49.88 $\pm$ 4.03 | 66.30 $\pm$ 0.28 | 69.58 $\pm$ 2.40 |'
- en: '| ReLU [[8](#bib.bib8)] | 61.33 $\pm$ 0.34 | 67.47 $\pm$ 0.44 | 74.05 $\pm$
    1.69 | 71.96 $\pm$ 0.94 | 70.45 $\pm$ 0.73 | 72.99 $\pm$ 1.35 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| ReLU [[8](#bib.bib8)] | 61.33 $\pm$ 0.34 | 67.47 $\pm$ 0.44 | 74.05 $\pm$
    1.69 | 71.96 $\pm$ 0.94 | 70.45 $\pm$ 0.73 | 72.99 $\pm$ 1.35 |'
- en: '| LReLU [[17](#bib.bib17)] | 61.13 $\pm$ 0.41 | 65.72 $\pm$ 0.14 | 63.79 $\pm$
    2.38 | 72.77  $\pm$ 0.49 | 70.58 $\pm$ 0.45 | 73.33 $\pm$ 1.25 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| LReLU [[17](#bib.bib17)] | 61.13 $\pm$ 0.41 | 65.72 $\pm$ 0.14 | 63.79 $\pm$
    2.38 | 72.77 $\pm$ 0.49 | 70.58 $\pm$ 0.45 | 73.33 $\pm$ 1.25 |'
- en: '| PReLU [[35](#bib.bib35)] | 59.86 $\pm$ 0.35 | 65.26 $\pm$ 0.40 | 69.57 $\pm$
    1.50 | 71.08 $\pm$ 1.70 | 69.77 $\pm$ 0.48 | 68.23 $\pm$ 1.55 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| PReLU [[35](#bib.bib35)] | 59.86 $\pm$ 0.35 | 65.26 $\pm$ 0.40 | 69.57 $\pm$
    1.50 | 71.08 $\pm$ 1.70 | 69.77 $\pm$ 0.48 | 68.23 $\pm$ 1.55 |'
- en: '| ELU [[27](#bib.bib27)] | 61.97  $\pm$ 0.24 | 51.35 $\pm$ 3.01 | 72.57 $\pm$
    1.76 | 71.41 $\pm$ 1.63 | 71.27  $\pm$ 0.58 | 72.06 $\pm$ 1.93 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| ELU [[27](#bib.bib27)] | 61.97 $\pm$ 0.24 | 51.35 $\pm$ 3.01 | 72.57 $\pm$
    1.76 | 71.41 $\pm$ 1.63 | 71.27 $\pm$ 0.58 | 72.06 $\pm$ 1.93 |'
- en: '| SELU [[52](#bib.bib52)] | 59.62 $\pm$ 0.39 | 64.55 $\pm$ 0.43 | 71.47 $\pm$
    1.39 | 69.94 $\pm$ 1.92 | 55.01 $\pm$ 0.98 | 70.15 $\pm$ 1.04 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| SELU [[52](#bib.bib52)] | 59.62 $\pm$ 0.39 | 64.55 $\pm$ 0.43 | 71.47 $\pm$
    1.39 | 69.94 $\pm$ 1.92 | 55.01 $\pm$ 0.98 | 70.15 $\pm$ 1.04 |'
- en: '| GELU [[101](#bib.bib101)] | 61.20 $\pm$ 0.61 | 67.25 $\pm$ 0.38 | 74.27  $\pm$
    0.70 | 71.58 $\pm$ 0.87 | 71.14 $\pm$ 0.29 | 73.31 $\pm$ 1.70 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| GELU [[101](#bib.bib101)] | 61.20 $\pm$ 0.61 | 67.25 $\pm$ 0.38 | 74.27 $\pm$
    0.70 | 71.58 $\pm$ 0.87 | 71.14 $\pm$ 0.29 | 73.31 $\pm$ 1.70 |'
- en: '| CELU [[53](#bib.bib53)] | 61.90 $\pm$ 0.21 | 55.78 $\pm$ 0.69 | 72.87 $\pm$
    1.52 | 70.95 $\pm$ 1.40 | 63.43 $\pm$ 0.81 | 72.68 $\pm$ 1.16 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| CELU [[53](#bib.bib53)] | 61.90 $\pm$ 0.21 | 55.78 $\pm$ 0.69 | 72.87 $\pm$
    1.52 | 70.95 $\pm$ 1.40 | 63.43 $\pm$ 0.81 | 72.68 $\pm$ 1.16 |'
- en: '| Softplus [[93](#bib.bib93)] | 62.59  $\pm$ 0.21 | 67.70 $\pm$ 0.19 | 73.08
    $\pm$ 1.66 | 71.99 $\pm$ 2.03 | 71.16  $\pm$ 0.46 | 72.54 $\pm$ 1.73 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| Softplus [[93](#bib.bib93)] | 62.59 $\pm$ 0.21 | 67.70 $\pm$ 0.19 | 73.08
    $\pm$ 1.66 | 71.99 $\pm$ 2.03 | 71.16 $\pm$ 0.46 | 72.54 $\pm$ 1.73 |'
- en: '| Swish [[29](#bib.bib29)] | 59.40 $\pm$ 0.41 | 66.05 $\pm$ 0.82 | 71.56 $\pm$
    1.66 | 71.12 $\pm$ 2.08 | 68.42 $\pm$ 1.62 | 71.34 $\pm$ 1.10 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Swish [[29](#bib.bib29)] | 59.40 $\pm$ 0.41 | 66.05 $\pm$ 0.82 | 71.56 $\pm$
    1.66 | 71.12 $\pm$ 2.08 | 68.42 $\pm$ 1.62 | 71.34 $\pm$ 1.10 |'
- en: '| ABReLU [[44](#bib.bib44)] | 56.21 $\pm$ 0.53 | 66.95 $\pm$ 0.09 | 71.83 $\pm$
    2.26 | 71.96 $\pm$ 1.43 | 70.47 $\pm$ 0.91 | 73.79  $\pm$ 1.45 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| ABReLU [[44](#bib.bib44)] | 56.21 $\pm$ 0.53 | 66.95 $\pm$ 0.09 | 71.83 $\pm$
    2.26 | 71.96 $\pm$ 1.43 | 70.47 $\pm$ 0.91 | 73.79 $\pm$ 1.45 |'
- en: '| LiSHT [[24](#bib.bib24)] | 54.09 $\pm$ 1.54 | 58.87 $\pm$ 0.81 | 66.66 $\pm$
    2.50 | 65.28 $\pm$ 1.33 | 66.01 $\pm$ 1.04 | 65.61 $\pm$ 1.10 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| LiSHT [[24](#bib.bib24)] | 54.09 $\pm$ 1.54 | 58.87 $\pm$ 0.81 | 66.66 $\pm$
    2.50 | 65.28 $\pm$ 1.33 | 66.01 $\pm$ 1.04 | 65.61 $\pm$ 1.10 |'
- en: '| SRS [[26](#bib.bib26)] | 54.93 $\pm$ 0.80 | 58.22 $\pm$ 1.09 | 70.39 $\pm$
    1.09 | 67.11 $\pm$ 1.46 | 36.95 $\pm$ 0.93 | 64.52 $\pm$ 1.39 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| SRS [[26](#bib.bib26)] | 54.93 $\pm$ 0.80 | 58.22 $\pm$ 1.09 | 70.39 $\pm$
    1.09 | 67.11 $\pm$ 1.46 | 36.95 $\pm$ 0.93 | 64.52 $\pm$ 1.39 |'
- en: '| Mish [[99](#bib.bib99)] | 61.81 $\pm$ 0.54 | 68.13  $\pm$ 0.40 | 73.76 $\pm$
    1.48 | 71.89 $\pm$ 1.12 | 70.80 $\pm$ 0.68 | 73.49 $\pm$ 1.39 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| Mish [[99](#bib.bib99)] | 61.81 $\pm$ 0.54 | 68.13 $\pm$ 0.40 | 73.76 $\pm$
    1.48 | 71.89 $\pm$ 1.12 | 70.80 $\pm$ 0.68 | 73.49 $\pm$ 1.39 |'
- en: '| PAU [[111](#bib.bib111)] | 59.81 $\pm$ 0.61 | 64.14 $\pm$ 0.62 | 70.48 $\pm$
    1.53 | 68.59 $\pm$ 2.15 | 68.29 $\pm$ 0.77 | 67.83 $\pm$ 0.35 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| PAU [[111](#bib.bib111)] | 59.81 $\pm$ 0.61 | 64.14 $\pm$ 0.62 | 70.48 $\pm$
    1.53 | 68.59 $\pm$ 2.15 | 68.29 $\pm$ 0.77 | 67.83 $\pm$ 0.35 |'
- en: '| PDELU [[59](#bib.bib59)] | 61.35 $\pm$ 0.56 | 67.92  $\pm$ 0.32 | 74.48  $\pm$
    1.23 | 72.11  $\pm$ 1.60 | 70.81 $\pm$ 0.47 | 73.71  $\pm$ 1.64 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| PDELU [[59](#bib.bib59)] | 61.35 $\pm$ 0.56 | 67.92 $\pm$ 0.32 | 74.48 $\pm$
    1.23 | 72.11 $\pm$ 1.60 | 70.81 $\pm$ 0.47 | 73.71 $\pm$ 1.64 |'
- en: '![Refer to caption](img/f5331dcc88b4026195f29da0a2c88d56.png)![Refer to caption](img/2b7dba249145771874a090e8cb1c7c45.png)![Refer
    to caption](img/a37314d201c61434ffee27ead63a536b.png)![Refer to caption](img/f54b2d2f5973c53657d09b59643f1c60.png)![Refer
    to caption](img/dbffd53297d0d9f34e8ee2f76407f523.png)![Refer to caption](img/c830f4eb908caee10e73dad286ef189d.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f5331dcc88b4026195f29da0a2c88d56.png)![参见说明](img/2b7dba249145771874a090e8cb1c7c45.png)![参见说明](img/a37314d201c61434ffee27ead63a536b.png)![参见说明](img/f54b2d2f5973c53657d09b59643f1c60.png)![参见说明](img/dbffd53297d0d9f34e8ee2f76407f523.png)![参见说明](img/c830f4eb908caee10e73dad286ef189d.png)'
- en: 'Figure 3: Convergence plots over CIFAR100 dataset.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: CIFAR100 数据集上的收敛图。'
- en: 9.2 Experimental Performance Analysis
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 实验性能分析
- en: In order to compare the AFs, three experiments are conducted in this paper,
    including image classification, language translation and speech recognition. Eighteen
    state-of-the-art AFs are considered for analysis, including Logistic Sigmoid,
    Tanh, Elliott [[25](#bib.bib25)], ReLU [[8](#bib.bib8)], LReLU [[34](#bib.bib34)]
    PReLU [[35](#bib.bib35)], ELU [[27](#bib.bib27)], SELU [[52](#bib.bib52)], GELU
    [[101](#bib.bib101)], CELU [[53](#bib.bib53)], Softplus [[93](#bib.bib93)], Swish
    [[29](#bib.bib29)], ABReLU [[44](#bib.bib44)], LiSHT [[24](#bib.bib24)], Soft-Root-Sign
    (SRS) [[26](#bib.bib26)], Mish [[99](#bib.bib99)], PAU [[111](#bib.bib111)] and
    PDELU [[59](#bib.bib59)]. Note that Swish, ABReLU, LiSHT, SRS, Mish, PAU and PDELU
    are the most recent functions. Google Colab based computational resource is used
    in most of the experiments. Few experiments are also performed over a desktop
    system consisting of 8 GB GPU. The PyTorch framework is used in all the experiments.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较 AFs，本文进行了三项实验，包括图像分类、语言翻译和语音识别。考虑了十八种最先进的 AFs 进行分析，包括 Logistic Sigmoid、Tanh、Elliott
    [[25](#bib.bib25)]、ReLU [[8](#bib.bib8)]、LReLU [[34](#bib.bib34)]、PReLU [[35](#bib.bib35)]、ELU
    [[27](#bib.bib27)]、SELU [[52](#bib.bib52)]、GELU [[101](#bib.bib101)]、CELU [[53](#bib.bib53)]、Softplus
    [[93](#bib.bib93)]、Swish [[29](#bib.bib29)]、ABReLU [[44](#bib.bib44)]、LiSHT [[24](#bib.bib24)]、Soft-Root-Sign
    (SRS) [[26](#bib.bib26)]、Mish [[99](#bib.bib99)]、PAU [[111](#bib.bib111)] 和 PDELU
    [[59](#bib.bib59)]。注意，Swish、ABReLU、LiSHT、SRS、Mish、PAU 和 PDELU 是最新的函数。大多数实验使用了基于
    Google Colab 的计算资源。也有少数实验在配备 8 GB GPU 的桌面系统上进行。所有实验均使用了 PyTorch 框架。
- en: The CIFAR10 and CIFAR100 datasets¹¹1[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
    [[148](#bib.bib148)] are used for the image classification experiment in this
    paper. The CIFAR10 dataset contains $50,000$ training images and $10,000$ test
    images from $10$ object categories. The CIFAR100 dataset contains $50,000$ training
    images and $10,000$ test images from $100$ object categories. We also utilize
    the language translation and speech recognition datasets for the experiments.
    For the experiments over CIFAR-10 and CIFAR-100 datasets, training is performed
    for 100 Epochs. The batch size is 128 for CIFAR-10 and 64 for CIFAR-100\. The
    learning rate is 0.001 for first 80 Epochs and 0.0001 for last 20 Epochs. Random
    crop and random horizontal flip are the data augmentation used during training.
    Data normalization is performed both during train and test times. Adam optimizer
    is used for the training with cross entropy loss. All existing activation functions
    except softmax are replaced with the corresponding activation function in different
    networks.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR10 和 CIFAR100 数据集¹¹1[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
    [[148](#bib.bib148)] 被用于本文中的图像分类实验。CIFAR10 数据集包含 $50,000$ 张训练图像和 $10,000$ 张测试图像，涵盖
    $10$ 个物体类别。CIFAR100 数据集包含 $50,000$ 张训练图像和 $10,000$ 张测试图像，涵盖 $100$ 个物体类别。我们还利用了语言翻译和语音识别数据集进行实验。在
    CIFAR-10 和 CIFAR-100 数据集上的实验中，训练进行 $100$ 个 Epoch。CIFAR-10 的批次大小为 $128$，CIFAR-100
    的批次大小为 $64$。前 $80$ 个 Epoch 的学习率为 $0.001$，最后 $20$ 个 Epoch 的学习率为 $0.0001$。在训练过程中使用了随机裁剪和随机水平翻转作为数据增强方法。数据标准化在训练和测试时均有进行。训练使用
    Adam 优化器和交叉熵损失函数。所有现有激活函数（除了 softmax）均被替换为不同网络中的相应激活函数。
- en: 'The test accuracy is reported in Tables [8](#S9.T8 "Table 8 ‣ 9.1 Comparison
    with Existing Survey/Performance Analysis ‣ 9 Performance Comparison and Analysis
    ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")
    and [9](#S9.T9 "Table 9 ‣ 9.1 Comparison with Existing Survey/Performance Analysis
    ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") on CIFAR10 and CIFAR100 datasets, respectively.
    In these Tables, the mean and standard deviation of image classification accuracy
    over 5 trials are reported for each AF. Moreover, the better results are highlighted.
    Different types of CNN models are used in this experiment, such as plain models
    (i.e., MobileNet [[149](#bib.bib149)] and VGG16 [[150](#bib.bib150)]), inception
    model (i.e., GoogLeNet [[151](#bib.bib151)]) and skip/residual connection based
    models (i.e., ResNet50 [[152](#bib.bib152)], SENet18 [[153](#bib.bib153)], and
    DenseNet121 [[154](#bib.bib154)]). The MobileNet, GoogLeNet and SENet18 are light
    models, whereas the VGG16, ResNet50 and DenseNet121 are heavy models in terms
    of the number of trainable parameters. Overall, it is observed that the Softplus,
    ELU and CELU are better suited with MobileNet. The ReLU, Mish and PDELU exhibit
    good performance with VGG16, GoogleNet and DenseNet. The ReLU, LReLU, ELU, GELU,
    CELU, ABReLU, and PDELU activation functions are better for the networks having
    residual connections, such as ResNet50, SENet18 and DenseNet121. In order to demonstrate
    the convergence of different AFs, the training loss vs epochs is plotted in Fig.
    [3](#S9.F3 "Figure 3 ‣ 9.1 Comparison with Existing Survey/Performance Analysis
    ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") on CIFAR100 dataset using different models.
    The PAU has emerged as a promising AF with fastest convergence in most of the
    cases. The PReLU, GELU and PDELU AFs are also consistent with good convergence.
    Note that the training diverges with SRS for the SENet18 model. Sigmoid and Elliott
    AFs showed the poorest convergence. The time taken for the training is also computed
    for different AFs using different CNN models on CIFAR100 dataset and reported
    in Table [10](#S9.T10 "Table 10 ‣ 9.2 Experimental Performance Analysis ‣ 9 Performance
    Comparison and Analysis ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark"). These results are computed using a desktop computer system
    having 32 GB RAM and 8 GB Nvidia GPU Card for 100 epochs of training. The time
    is represented in hh:mm:ss format. It is clear that PDELU AF is very inefficient.
    Moreover, SRS and Elliott also take more time for training. The activations such
    as ReLU, ELU, CELU, and Softplus depict a good tradeoff between the accuracy and
    training time.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '测试准确率在表格[8](#S9.T8 "Table 8 ‣ 9.1 Comparison with Existing Survey/Performance
    Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark")和[9](#S9.T9 "Table 9 ‣ 9.1 Comparison
    with Existing Survey/Performance Analysis ‣ 9 Performance Comparison and Analysis
    ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")中分别报告了CIFAR10和CIFAR100数据集上的结果。在这些表格中，报告了每种激活函数（AF）在5次实验中的图像分类准确率的均值和标准差。此外，更好的结果会被突出显示。在此次实验中使用了不同类型的CNN模型，例如普通模型（即，MobileNet
    [[149](#bib.bib149)] 和 VGG16 [[150](#bib.bib150)]）、Inception模型（即，GoogLeNet [[151](#bib.bib151)]）和基于跳跃/残差连接的模型（即，ResNet50
    [[152](#bib.bib152)]、SENet18 [[153](#bib.bib153)] 和 DenseNet121 [[154](#bib.bib154)]）。MobileNet、GoogLeNet
    和 SENet18 是轻量级模型，而 VGG16、ResNet50 和 DenseNet121 是在可训练参数数量上较重的模型。总体上观察到，Softplus、ELU
    和 CELU 更适合与 MobileNet 配合使用。ReLU、Mish 和 PDELU 在 VGG16、GoogLeNet 和 DenseNet 上表现良好。ReLU、LReLU、ELU、GELU、CELU、ABReLU
    和 PDELU 激活函数更适合具有残差连接的网络，如 ResNet50、SENet18 和 DenseNet121。为了展示不同 AF 的收敛情况，图[3](#S9.F3
    "Figure 3 ‣ 9.1 Comparison with Existing Survey/Performance Analysis ‣ 9 Performance
    Comparison and Analysis ‣ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")中绘制了不同模型在 CIFAR100 数据集上的训练损失与迭代次数的关系。PAU 在大多数情况下表现出最快的收敛速度，成为一种有前景的激活函数。PReLU、GELU
    和 PDELU 激活函数也具有一致的良好收敛性。需要注意的是，SENet18 模型在使用 SRS 时训练会出现发散。Sigmoid 和 Elliott 激活函数的收敛效果最差。使用不同
    CNN 模型在 CIFAR100 数据集上计算了不同 AF 的训练时间，并在表[10](#S9.T10 "Table 10 ‣ 9.2 Experimental
    Performance Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark")中报告了这些结果。这些结果是使用配备 32
    GB RAM 和 8 GB Nvidia GPU 卡的台式计算机系统进行 100 轮训练计算得出的。时间以 hh:mm:ss 格式表示。显然，PDELU 激活函数效率非常低。此外，SRS
    和 Elliott 激活函数的训练时间也较长。像 ReLU、ELU、CELU 和 Softplus 这样的激活函数在准确性和训练时间之间表现出良好的权衡。'
- en: 'Table 10: Training time (hh:mm:ss) comparison over CIFAR100 dataset.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：CIFAR100 数据集上的训练时间 (hh:mm:ss) 比较。
- en: '| Training Time | CNN Models |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 训练时间 | CNN 模型 |'
- en: '| --- | --- |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Activations | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Sigmoid | 00:33:15 | 00:49:16 | 04:55:54 | 03:36:03 | 01:13:14 | 04:12:24
    |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | 00:33:15 | 00:49:16 | 04:55:54 | 03:36:03 | 01:13:14 | 04:12:24
    |'
- en: '| Tanh | 00:33:18 | 00:49:55 | 04:58:02 | 03:33:03 | 01:13:18 | 04:09:24 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Tanh | 00:33:18 | 00:49:55 | 04:58:02 | 03:33:03 | 01:13:18 | 04:09:24 |'
- en: '| Elliott [[25](#bib.bib25)] | 00:49:52 | 00:59:13 | 06:53:55 | 05:38:49 |
    01:41:38 | 07:46:55 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| Elliott [[25](#bib.bib25)] | 00:49:52 | 00:59:13 | 06:53:55 | 05:38:49 |
    01:41:38 | 07:46:55 |'
- en: '| ReLU [[8](#bib.bib8)] | 00:31:22 | 00:47:19 | 04:55:10 | 03:32:30 | 01:15:33
    | 04:15:06 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| ReLU [[8](#bib.bib8)] | 00:31:22 | 00:47:19 | 04:55:10 | 03:32:30 | 01:15:33
    | 04:15:06 |'
- en: '| LReLU [[34](#bib.bib34)] | 00:31:48 | 00:49:03 | 05:01:30 | 03:33:00 | 01:18:38
    | 04:14:09 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| LReLU [[34](#bib.bib34)] | 00:31:48 | 00:49:03 | 05:01:30 | 03:33:00 | 01:18:38
    | 04:14:09 |'
- en: '| PReLU [[35](#bib.bib35)] | 00:44:24 | 00:49:01 | 05:42:18 | 03:55:57 | 01:27:05
    | 04:55:47 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| PReLU [[35](#bib.bib35)] | 00:44:24 | 00:49:01 | 05:42:18 | 03:55:57 | 01:27:05
    | 04:55:47 |'
- en: '| ELU [[27](#bib.bib27)] | 00:31:05 | 00:47:38 | 04:57:37 | 03:36:47 | 01:13:25
    | 04:08:39 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| ELU [[27](#bib.bib27)] | 00:31:05 | 00:47:38 | 04:57:37 | 03:36:47 | 01:13:25
    | 04:08:39 |'
- en: '| SELU [[52](#bib.bib52)] | 00:29:40 | 00:47:31 | 04:54:57 | 03:33:47 | 01:13:27
    | 04:09:17 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| SELU [[52](#bib.bib52)] | 00:29:40 | 00:47:31 | 04:54:57 | 03:33:47 | 01:13:27
    | 04:09:17 |'
- en: '| GELU [[101](#bib.bib101)] | 00:29:43 | 00:47:22 | 04:55:53 | 03:32:32 | 01:13:32
    | 04:11:26 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| GELU [[101](#bib.bib101)] | 00:29:43 | 00:47:22 | 04:55:53 | 03:32:32 | 01:13:32
    | 04:11:26 |'
- en: '| CELU [[53](#bib.bib53)] | 00:29:36 | 00:46:47 | 05:00:44 | 03:31:40 | 01:14:08
    | 04:18:11 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| CELU [[53](#bib.bib53)] | 00:29:36 | 00:46:47 | 05:00:44 | 03:31:40 | 01:14:08
    | 04:18:11 |'
- en: '| Softplus [[93](#bib.bib93)] | 00:29:44 | 00:47:06 | 04:58:55 | 03:32:03 |
    01:14:02 | 04:12:08 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| Softplus [[93](#bib.bib93)] | 00:29:44 | 00:47:06 | 04:58:55 | 03:32:03 |
    01:14:02 | 04:12:08 |'
- en: '| Swish [[29](#bib.bib29)] | 00:43:13 | 00:55:37 | 06:18:38 | 04:58:38 | 01:32:15
    | 06:41:14 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| Swish [[29](#bib.bib29)] | 00:43:13 | 00:55:37 | 06:18:38 | 04:58:38 | 01:32:15
    | 06:41:14 |'
- en: '| ABReLU [[44](#bib.bib44)] | 00:38:51 | 00:53:49 | 05:43:59 | 04:27:02 | 01:25:30
    | 05:42:53 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| ABReLU [[44](#bib.bib44)] | 00:38:51 | 00:53:49 | 05:43:59 | 04:27:02 | 01:25:30
    | 05:42:53 |'
- en: '| LiSHT [[24](#bib.bib24)] | 00:37:01 | 00:54:10 | 05:40:00 | 04:25:57 | 01:23:59
    | 05:38:15 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| LiSHT [[24](#bib.bib24)] | 00:37:01 | 00:54:10 | 05:40:00 | 04:25:57 | 01:23:59
    | 05:38:15 |'
- en: '| SRS [[26](#bib.bib26)] | 01:06:38 | 01:11:36 | 08:43:09 | 07:35:35 | 02:05:33
    | 11:10:27 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| SRS [[26](#bib.bib26)] | 01:06:38 | 01:11:36 | 08:43:09 | 07:35:35 | 02:05:33
    | 11:10:27 |'
- en: '| Mish [[99](#bib.bib99)] | 00:40:19 | 00:54:23 | 05:59:48 | 04:46:45 | 01:28:53
    | 06:10:27 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Mish [[99](#bib.bib99)] | 00:40:19 | 00:54:23 | 05:59:48 | 04:46:45 | 01:28:53
    | 06:10:27 |'
- en: '| PAU [[111](#bib.bib111)] | 00:41:59 | 00:54:10 | 05:54:22 | 04:12:31 | 01:25:37
    | 05:39:57 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| PAU [[111](#bib.bib111)] | 00:41:59 | 00:54:10 | 05:54:22 | 04:12:31 | 01:25:37
    | 05:39:57 |'
- en: '| PDELU [[59](#bib.bib59)] | 05:23:38 | 04:01:55 | 34:22:00 | 36:48:48 | 08:32:40
    | 50:23:00 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| PDELU [[59](#bib.bib59)] | 05:23:38 | 04:01:55 | 34:22:00 | 36:48:48 | 08:32:40
    | 50:23:00 |'
- en: 'Table 11: Experimental results for German to English language translation and
    speech recognition tasks.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：德语到英语的语言翻译和语音识别任务的实验结果。
- en: '|  | Language Translation |  | Speech Recognition |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | 语言翻译 |  | 语音识别 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Activations | Bleu Score |  | Average CER | Average WER |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | Bleu Score |  | 平均 CER | 平均 WER |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Sigmoid | 14.59 $\pm$ 0.47 |  | 0.53 $\pm$ 0.18 | 1.19 $\pm$ 0.39 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | 14.59 $\pm$ 0.47 |  | 0.53 $\pm$ 0.18 | 1.19 $\pm$ 0.39 |'
- en: '| Tanh | 20.93  $\pm$ 0.91 |  | 0.26 $\pm$ 0 | 0.68 $\pm$ 0 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| Tanh | 20.93  $\pm$ 0.91 |  | 0.26 $\pm$ 0 | 0.68 $\pm$ 0 |'
- en: '| Elliott [[25](#bib.bib25)] | 14.49 $\pm$ 0.96 |  | 0.40 $\pm$ 0.01 | 0.93
    $\pm$ 0.01 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| Elliott [[25](#bib.bib25)] | 14.49 $\pm$ 0.96 |  | 0.40 $\pm$ 0.01 | 0.93
    $\pm$ 0.01 |'
- en: '| ReLU [[8](#bib.bib8)] | 18.88 $\pm$ 0.86 |  | 0.24  $\pm$ 0.01 | 0.66  $\pm$
    0.01 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| ReLU [[8](#bib.bib8)] | 18.88 $\pm$ 0.86 |  | 0.24  $\pm$ 0.01 | 0.66  $\pm$
    0.01 |'
- en: '| LReLU [[34](#bib.bib34)] | 18.89 $\pm$ 0.82 |  | 0.24  $\pm$ 0 | 0.66  $\pm$
    0.01 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| LReLU [[34](#bib.bib34)] | 18.89 $\pm$ 0.82 |  | 0.24  $\pm$ 0 | 0.66  $\pm$
    0.01 |'
- en: '| PReLU [[35](#bib.bib35)] | 20.04 $\pm$ 0.98 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| PReLU [[35](#bib.bib35)] | 20.04 $\pm$ 0.98 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
- en: '| ELU [[27](#bib.bib27)] | 19.40 $\pm$ 1.33 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| ELU [[27](#bib.bib27)] | 19.40 $\pm$ 1.33 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0 |'
- en: '| SELU [[52](#bib.bib52)] | 20.85  $\pm$ 0.64 |  | 0.26 $\pm$ 0 | 0.69 $\pm$
    0.01 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| SELU [[52](#bib.bib52)] | 20.85  $\pm$ 0.64 |  | 0.26 $\pm$ 0 | 0.69 $\pm$
    0.01 |'
- en: '| GELU [[101](#bib.bib101)] | 18.75 $\pm$ 1.83 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| GELU [[101](#bib.bib101)] | 18.75 $\pm$ 1.83 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
- en: '| CELU [[53](#bib.bib53)] | 18.71 $\pm$ 0.55 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| CELU [[53](#bib.bib53)] | 18.71 $\pm$ 0.55 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0 |'
- en: '| Softplus [[93](#bib.bib93)] | 16.78 $\pm$ 0.84 |  | 0.30 $\pm$ 0.01 | 0.76
    $\pm$ 0.02 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| Softplus [[93](#bib.bib93)] | 16.78 $\pm$ 0.84 |  | 0.30 $\pm$ 0.01 | 0.76
    $\pm$ 0.02 |'
- en: '| Swish [[29](#bib.bib29)] | 19.51 $\pm$ 0.97 |  | 0.24  $\pm$ 0.01 | 0.65  $\pm$
    0.01 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| Swish [[29](#bib.bib29)] | 19.51 $\pm$ 0.97 |  | 0.24  $\pm$ 0.01 | 0.65  $\pm$
    0.01 |'
- en: '| ABReLU [[44](#bib.bib44)] | 17.55 $\pm$ 0.63 |  | 0.25  $\pm$ 0 | 0.68 $\pm$
    0 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| ABReLU [[44](#bib.bib44)] | 17.55 $\pm$ 0.63 |  | 0.25  $\pm$ 0 | 0.68 $\pm$
    0 |'
- en: '| LiSHT [[24](#bib.bib24)] | 20.39 $\pm$ 0.93 |  | 0.29 $\pm$ 0.01 | 0.74 $\pm$
    0.01 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| LiSHT [[24](#bib.bib24)] | 20.39 $\pm$ 0.93 |  | 0.29 $\pm$ 0.01 | 0.74 $\pm$
    0.01 |'
- en: '| SRS [[26](#bib.bib26)] | 20.66 $\pm$ 0.78 |  | 0.28 $\pm$ 0 | 0.72 $\pm$
    0 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| SRS [[26](#bib.bib26)] | 20.66 $\pm$ 0.78 |  | 0.28 $\pm$ 0 | 0.72 $\pm$
    0 |'
- en: '| Mish [[99](#bib.bib99)] | 19.56 $\pm$ 1.15 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| Mish [[99](#bib.bib99)] | 19.56 $\pm$ 1.15 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
- en: '| PAU [[111](#bib.bib111)] | 20.11 $\pm$ 1.24 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0.01 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| PAU [[111](#bib.bib111)] | 20.11 $\pm$ 1.24 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0.01 |'
- en: '| PDELU [[59](#bib.bib59)] | 19.07 $\pm$ 0.95 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0.01 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| PDELU [[59](#bib.bib59)] | 19.07 $\pm$ 0.95 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0.01 |'
- en: 'The results for language translation and speech recognition for different AFs
    are illustrated in Table [11](#S9.T11 "Table 11 ‣ 9.2 Experimental Performance
    Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"). The German to English translation
    is used to test the performance of the AFs over text data. Benchmark Seq2Seq model
    consisting of a Long Short Term Memory (LSTM) based autoencoder network is used
    for the experiment. The model and dataset are downloaded from Kaggle²²2https://www.kaggle.com/parthplc/pytorch-seq2seq-machine-translation/notebook.
    The AF is applied to the feature embedding before the dropout layer. For the language
    translation experiments, the number of Epochs is set to 50 with 0.001 learning
    rate and 256 batch size. The embedding size of encoder and decoder is 300. The
    dropout factor is 0.5 for both encoder and decoder. Adam optimizer is used for
    the training with cross entropy loss. The Bleu score [[155](#bib.bib155)] with
    $4$-gram is reported in Table [11](#S9.T11 "Table 11 ‣ 9.2 Experimental Performance
    Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") in $2^{nd}$ column for different
    AFs. The mean and standard deviation of Bleu score over 5 trials are reported
    for each AF. It is noticed that the Tanh and SELU AFs are better suitable for
    language translation. The PReLU, LiSHT, SRS and PAU AFs also perform better for
    language translation.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '语言翻译和语音识别的结果在表格[11](#S9.T11 "Table 11 ‣ 9.2 Experimental Performance Analysis
    ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark")中展示。德语到英语的翻译用于测试激活函数在文本数据上的表现。实验中使用了基于长短期记忆（LSTM）的自动编码器网络作为基准Seq2Seq模型。模型和数据集从Kaggle下载²²2https://www.kaggle.com/parthplc/pytorch-seq2seq-machine-translation/notebook。在dropout层之前，将激活函数应用于特征嵌入。对于语言翻译实验，训练轮数设置为50，学习率为0.001，批量大小为256。编码器和解码器的嵌入大小为300。编码器和解码器的dropout因子均为0.5。训练使用Adam优化器和交叉熵损失。表格[11](#S9.T11
    "Table 11 ‣ 9.2 Experimental Performance Analysis ‣ 9 Performance Comparison and
    Analysis ‣ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")中的第$2^{nd}$列报告了不同激活函数的$4$-gram
    Bleu分数。每个激活函数的Bleu分数的均值和标准差在5次实验中报告。观察到Tanh和SELU激活函数更适合语言翻译。PReLU、LiSHT、SRS和PAU激活函数在语言翻译中表现也较好。'
- en: 'The speech recognition experiment is also performed to show the performance
    of the different AFs for time-series signal data. The end-to-end speech recognition
    based Deep Speech 2 framework available from assemblyai³³3https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch
    is used. The model consists of $2$ layers of residual convolution layers to learn
    the relevant audio features, and $2$ layers of bidirectional gated recurrent units
    (GRUs) to use the learned residual convolutional audio features. The $100$ hours
    of transcribed audio English data from LibriSpeech dataset is used for the experiment.
    For the speech recognition experiments, torchaudio 0.4.0 and torch 1.4.0 are used.
    The model consists of 2 CNN layers and 2 RNN layers. The dimension of a RNN layer
    is 512\. Number of classes is 29 in the dataset. Dropout factor is 0.5\. The learning
    rate is 0.0005, batch size is 10 and the number of Epochs is 10. The mean and
    standard deviation over 5 trials of character error rate (CER) and word error
    rate (WER) are reported in Table [11](#S9.T11 "Table 11 ‣ 9.2 Experimental Performance
    Analysis ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") for speech recognition. The recent
    AFs such as PReLU, GELU, Swish, Mish and PAU AFs are found as the most suitable
    for speech recognition in this experiment.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '还进行了语音识别实验，以展示不同激活函数对时间序列信号数据的性能。使用了来自 assemblyai³³3https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch
    的端到端语音识别 Deep Speech 2 框架。该模型由 $2$ 层残差卷积层组成，用于学习相关音频特征，并且有 $2$ 层双向门控循环单元（GRUs）来使用学到的残差卷积音频特征。实验使用了来自
    LibriSpeech 数据集的 $100$ 小时转录英语音频数据。语音识别实验中使用了 torchaudio 0.4.0 和 torch 1.4.0。模型由
    2 层 CNN 和 2 层 RNN 组成。RNN 层的维度为 512。数据集中类别数量为 29。Dropout 因子为 0.5。学习率为 0.0005，批量大小为
    10，Epoch 数为 10。表 [11](#S9.T11 "Table 11 ‣ 9.2 Experimental Performance Analysis
    ‣ 9 Performance Comparison and Analysis ‣ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") 中报告了 5 次试验的字符错误率（CER）和词错误率（WER）的均值和标准差。最近的激活函数如
    PReLU、GELU、Swish、Mish 和 PAU 激活函数在本实验中被发现最适合语音识别。'
- en: 10 Conclusion and Recommendations
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 结论和建议
- en: An extensive and up to date survey of activation functions is conducted in this
    paper. Different types of AFs are considered, including Logistic Sigmoid and Tanh
    based, ReLU based, ELU based, and Learning based. However, the main focus is given
    to the recent developments in AFs in view of the deep learning applications of
    neural networks. The overview of AFs presented in this paper focuses on the aspects
    including the detailed coverage of AFs, classification and performance comparison
    over image, text and speech data.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 本文进行了广泛且最新的激活函数调查。考虑了不同类型的激活函数，包括基于 Logistic Sigmoid 和 Tanh 的、基于 ReLU 的、基于 ELU
    的和基于学习的。然而，主要关注的是针对深度学习神经网络应用的激活函数的最新发展。本文对激活函数的概述重点关注激活函数的详细覆盖、分类及在图像、文本和语音数据上的性能比较。
- en: 'Following are the concluding remarks of the survey and performance analysis
    conducted through this paper:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通过本文进行的调查和性能分析的总结：
- en: '1.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: Most of the improvements in Logistic Sigmoid and Tanh targets to tackle the
    non zero-mean and zero-gradient problems. However, these improvements carry forward
    the drawback of increased complexity.
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Logistic Sigmoid 和 Tanh 的大多数改进旨在解决非零均值和零梯度问题。然而，这些改进带来了复杂性增加的缺陷。
- en: '2.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: The ReLU variants try to tackle the three major problems of ReLU, namely under-utilization
    of negative values, limited nonlinearity and unbounded output. These activations
    perform well for some applications, e.g. LReLU and ABReLU works better with residual
    networks. However, most of these activations fail to perform better than ReLU,
    e.g. LReLU, PReLU and ABReLU do not improve for MobileNet, VGG and GoogleNet models.
    Note that, the ReLU, Leaky ReLU and PReLU AFs are the most common choice among
    researchers due to its simplicity. Moreover, many networks consider the ReLU as
    a default choice for the AF.
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ReLU 的变体试图解决 ReLU 的三个主要问题，即负值的未充分利用、有限的非线性和无界输出。这些激活函数在一些应用中表现良好，例如 LReLU 和
    ABReLU 在残差网络中效果更佳。然而，大多数激活函数未能比 ReLU 表现得更好，例如 LReLU、PReLU 和 ABReLU 对 MobileNet、VGG
    和 GoogleNet 模型的改进不明显。需要注意的是，ReLU、Leaky ReLU 和 PReLU 激活函数因其简单性而成为研究者的最常用选择。此外，许多网络将
    ReLU 作为激活函数的默认选择。
- en: '3.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: The exponential based AFs also focus over the better utilization of the negative
    values and to avoid the saturation for important features. However, most of the
    exponential activations suffer due to the non-smooth functions.
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于指数的激活函数也关注更好地利用负值并避免对重要特征的饱和。然而，大多数指数激活函数由于函数的不光滑性而受到影响。
- en: '4.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: The learning based adaptive AFs try to find the best parameters to represent
    the non-linearity needed for the given dataset. This category of AF has gained
    more popularity in recent years. However, the major problem associated with such
    AF is to find the better base function and number of trainable parameters. Some
    AFs diverge during the training if not initialized properly.
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于学习的自适应激活函数（AF）试图找到最佳参数以表示给定数据集所需的非线性。这类激活函数近年来变得越来越受欢迎。然而，这类激活函数面临的主要问题是如何找到更好的基函数和可训练参数的数量。如果初始化不当，一些激活函数在训练过程中可能会发散。
- en: '5.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: In contrast to existing surveys, this survey covers an exhaustive list different
    types of AFs. Moreover, a performance analysis on different types of data using
    several AFs provides new insights for future research.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与现有的调查相比，本调查涵盖了各种类型的激活函数的详尽列表。此外，对不同数据类型使用多种激活函数的性能分析为未来的研究提供了新的见解。
- en: 'Following are the recommendations curated from this survey and performance
    analysis:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从本调查和性能分析中整理出的建议：
- en: '1.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: In order to speed up the training, both negative & positive values should be
    used to ensure the near zero mean.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了加快训练速度，应同时使用负值和正值以确保接近零的均值。
- en: '2.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: The most important aspect in deep learning is to find the network having matching
    complexity as the dataset complexity. If the complexity of the model is high then
    it may lead to overfitting and if the complexity of the model is low then it may
    lead to under convergence. Thus, the AF should bridge this gap based on the model
    and dataset complexity during training automatically.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在深度学习中，最重要的方面是找到与数据集复杂性匹配的网络。如果模型的复杂性过高，可能会导致过拟合；如果模型的复杂性过低，可能会导致欠拟合。因此，激活函数应在训练过程中根据模型和数据集的复杂性自动弥合这一差距。
- en: '3.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: The Logistic Sigmoid and Tanh AFs should be avoided for Convolutional Neural
    Networks as it leads to poor convergence. However, this type of AF is commonly
    used as gates in recurrent neural networks.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Logistic Sigmoid和Tanh激活函数应避免用于卷积神经网络，因为它们会导致较差的收敛性。然而，这类激活函数在递归神经网络中常被用作门控单元。
- en: '4.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: Despite the ReLU being a popular choice, recently proposed AFs such as Swish,
    Mish, and PAU are also worth trying for different problems.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管ReLU是一个流行的选择，但最近提出的激活函数如Swish、Mish和PAU在不同问题上也值得尝试。
- en: '5.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: The ReLU, Mish and PDELU activation functions have shown a good performance
    with VGG16 and GoogleNet. The ReLU, LReLU, ELU, GELU, CELU, and PDELU functions
    are better for the networks having residual connections for image classification.
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ReLU、Mish和PDELU激活函数在VGG16和GoogleNet上表现良好。ReLU、LReLU、ELU、GELU、CELU和PDELU函数更适合具有残差连接的图像分类网络。
- en: '6.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '6.'
- en: In general, the parametric AFs show better convergence as it can adapt the data
    faster by learning the parameter from the data. Specially, PAU, PReLU and PDELU
    have shown better convergence.
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一般来说，参数化激活函数表现出更好的收敛性，因为它可以通过从数据中学习参数来更快地适应数据。特别是，PAU、PReLU和PDELU表现出了更好的收敛性。
- en: '7.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '7.'
- en: Some AFs lead to increased training time complexity. PDELU and SRS are such
    examples. However, AFs such as ReLU, SELU, GELU, and Softplus depict a promising
    tradeoff between the accuracy and training time.
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些激活函数会导致训练时间复杂度的增加。PDELU和SRS就是这样的例子。然而，ReLU、SELU、GELU和Softplus等激活函数在准确性和训练时间之间表现出一种有希望的折衷。
- en: '8.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '8.'
- en: The exponential AFs generally lead to the increased non-linearity due to utilization
    of the negative values.
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指数型激活函数通常会由于负值的使用而导致非线性的增加。
- en: '9.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '9.'
- en: The Tanh and SELU AFs are found better for language translation along with PReLU,
    LiSHT, SRS and PAU.
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Tanh和SELU激活函数在语言翻译中表现较好，同时PReLU、LiSHT、SRS和PAU也效果不错。
- en: '10.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '10.'
- en: It is suggested to use the PReLU, GELU, Swish, Mish and PAU AFs for speech recognition.
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 建议在语音识别中使用PReLU、GELU、Swish、Mish和PAU激活函数。
- en: References
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] F. Shao, L. Chen, J. Shao, W. Ji, S. Xiao, L. Ye, Y. Zhuang, J. Xiao, Deep
    learning for weakly-supervised object detection and localization: A survey, Neurocomputing
    (2022).'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] F. Shao, L. Chen, J. Shao, W. Ji, S. Xiao, L. Ye, Y. Zhuang, J. Xiao, 深度学习在弱监督目标检测和定位中的应用：综述,
    Neurocomputing (2022).'
- en: '[2] Y. Mo, Y. Wu, X. Yang, F. Liu, Y. Liao, Review the state-of-the-art technologies
    of semantic segmentation based on deep learning, Neurocomputing (2022).'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Y. Mo, Y. Wu, X. Yang, F. Liu, Y. Liao, 基于深度学习的语义分割技术的最新进展综述，《神经计算》 (2022)。'
- en: '[3] Y. Guo, F. Feng, X. Hao, X. Chen, Jac-net: Joint learning with adaptive
    exploration and concise attention for unsupervised domain adaptive person re-identification,
    Neurocomputing (2022).'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Guo, F. Feng, X. Hao, X. Chen, Jac-net：具有自适应探索和简洁注意力的联合学习用于无监督领域自适应人重新识别，《神经计算》
    (2022)。'
- en: '[4] S. R. Dubey, A decade survey of content based image retrieval using deep
    learning, IEEE Transactions on Circuits and Systems for Video Technology (2021).'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. R. Dubey, 基于深度学习的内容检索十年综述，《IEEE 视频技术电路与系统期刊》 (2021)。'
- en: '[5] X. Xia, X. Pan, N. Li, X. He, L. Ma, X. Zhang, N. Ding, Gan-based anomaly
    detection: A review, Neurocomputing (2022).'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X. Xia, X. Pan, N. Li, X. He, L. Ma, X. Zhang, N. Ding, 基于 GAN 的异常检测：综述，《神经计算》
    (2022)。'
- en: '[6] H. Li, Y. Pan, J. Zhao, L. Zhang, Skin disease diagnosis with deep learning:
    a review, Neurocomputing 464 (2021) 364–393.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] H. Li, Y. Pan, J. Zhao, L. Zhang, 基于深度学习的皮肤疾病诊断：综述，《神经计算》464 (2021) 364–393。'
- en: '[7] C. H. Dagli, Artificial neural networks for intelligent manufacturing,
    Springer Science & Business Media, 2012.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. H. Dagli, 智能制造的人工神经网络，Springer Science & Business Media，2012。'
- en: '[8] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with
    deep convolutional neural networks, in: Advances in Neural Information Processing
    Systems, 2012, pp. 1097–1105.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Krizhevsky, I. Sutskever, G. E. Hinton, 使用深度卷积神经网络进行 Imagenet 分类，收录于：神经信息处理系统进展，2012，pp.
    1097–1105。'
- en: '[9] A. Graves, A.-r. Mohamed, G. Hinton, Speech recognition with deep recurrent
    neural networks, in: IEEE International Conference on Acoustics, Speech and Signal
    Processing, 2013, pp. 6645–6649.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Graves, A.-r. Mohamed, G. Hinton, 使用深度递归神经网络进行语音识别，收录于：IEEE 国际声学、语音与信号处理会议，2013，pp.
    6645–6649。'
- en: '[10] K. K. Babu, S. R. Dubey, Pcsgan: Perceptual cyclic-synthesized generative
    adversarial networks for thermal and nir to visible image transformation, Neurocomputing
    (2020).'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] K. K. Babu, S. R. Dubey, PCSGAN：用于热成像和近红外到可见光图像转换的感知循环合成生成对抗网络，《神经计算》
    (2020)。'
- en: '[11] J. Liu, Y. Liu, Q. Zhang, A weight initialization method based on neural
    network with asymmetric activation function, Neurocomputing (2022).'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Liu, Y. Liu, Q. Zhang, 基于非对称激活函数的神经网络权重初始化方法，《神经计算》 (2022)。'
- en: '[12] Y. Srivastava, V. Murali, S. R. Dubey, A performance evaluation of loss
    functions for deep face recognition, in: National Conference on Computer Vision,
    Pattern Recognition, Image Processing, and Graphics, Springer, 2019, pp. 322–332.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Y. Srivastava, V. Murali, S. R. Dubey, 对深度人脸识别损失函数的性能评估，收录于：计算机视觉、模式识别、图像处理与图形学全国会议，Springer，2019，pp.
    322–332。'
- en: '[13] S. S. Basha, S. R. Dubey, V. Pulabaigari, S. Mukherjee, Impact of fully
    connected layers on performance of convolutional neural networks for image classification,
    Neurocomputing 378 (2020) 112–119.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. S. Basha, S. R. Dubey, V. Pulabaigari, S. Mukherjee, 全连接层对卷积神经网络图像分类性能的影响，《神经计算》378
    (2020) 112–119。'
- en: '[14] Q. Xu, M. Zhang, Z. Gu, G. Pan, Overfitting remedy by sparsifying regularization
    on fully-connected layers of cnns, Neurocomputing 328 (2019) 69–74.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Q. Xu, M. Zhang, Z. Gu, G. Pan, 通过稀疏正则化缓解过拟合在 CNNs 的全连接层中的应用，《神经计算》328
    (2019) 69–74。'
- en: '[15] S. R. Dubey, S. Chakraborty, S. K. Roy, S. Mukherjee, S. K. Singh, B. B.
    Chaudhuri, diffgrad: An optimization method for convolutional neural networks,
    IEEE transactions on neural networks and learning systems 31 (11) (2019) 4500–4511.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. R. Dubey, S. Chakraborty, S. K. Roy, S. Mukherjee, S. K. Singh, B.
    B. Chaudhuri, Diffgrad：一种优化卷积神经网络的方法，《IEEE 神经网络与学习系统期刊》31 (11) (2019) 4500–4511。'
- en: '[16] W. Duch, N. Jankowski, Survey of neural transfer functions, Neural Computing
    Surveys 2 (1) (1999) 163–212.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] W. Duch, N. Jankowski, 神经网络传递函数的调查，《神经计算调查》2 (1) (1999) 163–212。'
- en: '[17] V. Nair, G. E. Hinton, Rectified linear units improve restricted boltzmann
    machines, in: International Conference on Machine Learning, 2010, pp. 807–814.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] V. Nair, G. E. Hinton, 纠正线性单元改善限制玻尔兹曼机，收录于：国际机器学习会议，2010，pp. 807–814。'
- en: '[18] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied
    to document recognition, Proceedings of the IEEE 86 (11) (1998) 2278–2324.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, 基于梯度的学习应用于文档识别，《IEEE 期刊》86
    (11) (1998) 2278–2324。'
- en: '[19] A. N. S. Njikam, H. Zhao, A novel activation function for multilayer feed-forward
    neural networks, Applied Intelligence 45 (1) (2016) 75–82.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. N. S. Njikam, H. Zhao, 一种用于多层前馈神经网络的新型激活函数，《应用智能》45 (1) (2016) 75–82。'
- en: '[20] B. Xu, R. Huang, M. Li, Revise saturated activation functions, International
    Conference on Learning Representations Workshop (2016).'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] B. Xu, R. Huang, M. Li, 修订饱和激活函数，国际学习表征会议研讨会 (2016)。'
- en: '[21] S. Kong, M. Takatsuka, Hexpo: A vanishing-proof activation function, in:
    International Joint Conference on Neural Networks, 2017, pp. 2562–2567.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Kong, M. Takatsuka, Hexpo：一种防止消失的激活函数，见：国际联合神经网络会议，2017年，第 2562–2567
    页。'
- en: '[22] Y. Qin, X. Wang, J. Zou, The optimized deep belief networks with improved
    logistic sigmoid units and their application in fault diagnosis for planetary
    gearboxes of wind turbines, IEEE Transactions on Industrial Electronics 66 (5)
    (2018) 3814–3824.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. Qin, X. Wang, J. Zou, 优化的深度信念网络与改进的逻辑 sigmoid 单元及其在风力涡轮机行星齿轮箱故障诊断中的应用，IEEE
    工业电子学报 66 (5) (2018) 3814–3824。'
- en: '[23] S. Elfwing, E. Uchibe, K. Doya, Sigmoid-weighted linear units for neural
    network function approximation in reinforcement learning, Neural Networks 107
    (2018) 3–11.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Elfwing, E. Uchibe, K. Doya, 用于强化学习中的神经网络函数逼近的 sigmoid 加权线性单元，Neural
    Networks 107 (2018) 3–11。'
- en: '[24] S. K. Roy, S. Manna, S. R. Dubey, B. B. Chaudhuri, Lisht: Non-parametric
    linearly scaled hyperbolic tangent activation function for neural networks, arXiv
    preprint arXiv:1901.05894 (2019).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. K. Roy, S. Manna, S. R. Dubey, B. B. Chaudhuri, Lisht: 用于神经网络的非参数线性缩放双曲正切激活函数，arXiv
    预印本 arXiv:1901.05894 (2019)。'
- en: '[25] A. Farzad, H. Mashayekhi, H. Hassanpour, A comparative performance analysis
    of different activation functions in lstm networks for classification, Neural
    Computing and Applications 31 (7) (2019) 2507–2521.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Farzad, H. Mashayekhi, H. Hassanpour, 在 LSTM 网络中对不同激活函数进行的分类性能比较分析，Neural
    Computing and Applications 31 (7) (2019) 2507–2521。'
- en: '[26] Y. Zhou, D. Li, S. Huo, S.-Y. Kung, Soft-root-sign activation function,
    arXiv preprint arXiv:2003.00547 (2020).'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. Zhou, D. Li, S. Huo, S.-Y. Kung, Soft-root-sign 激活函数，arXiv 预印本 arXiv:2003.00547
    (2020)。'
- en: '[27] D.-A. Clevert, T. Unterthiner, S. Hochreiter, Fast and accurate deep network
    learning by exponential linear units (elus), in: International Conference on Learning
    Representations, 2016.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] D.-A. Clevert, T. Unterthiner, S. Hochreiter, 通过指数线性单元（ELUs）实现快速而准确的深度网络学习，见：国际学习表征会议，2016年。'
- en: '[28] F. Agostinelli, M. Hoffman, P. Sadowski, P. Baldi, Learning activation
    functions to improve deep neural networks, International Conference on Learning
    Representations Workshops (2015).'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] F. Agostinelli, M. Hoffman, P. Sadowski, P. Baldi, 学习激活函数以改善深度神经网络，国际学习表征会议研讨会
    (2015)。'
- en: '[29] P. Ramachandran, B. Zoph, Q. V. Le, Searching for activation functions,
    International Conference on Learning Representations Workshops (2018).'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] P. Ramachandran, B. Zoph, Q. V. Le, 寻找激活函数，国际学习表征会议研讨会 (2018)。'
- en: '[30] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, nature 521 (7553) (2015)
    436–444.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. LeCun, Y. Bengio, G. Hinton, 深度学习，Nature 521 (7553) (2015) 436–444。'
- en: '[31] P. Chandra, Y. Singh, An activation function adapting training algorithm
    for sigmoidal feedforward networks, Neurocomputing 61 (2004) 429–437.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] P. Chandra, Y. Singh, 一种适应性训练算法的激活函数用于 sigmoid 前馈网络，Neurocomputing 61
    (2004) 429–437。'
- en: '[32] S. S. Sodhi, P. Chandra, Bi-modal derivative activation function for sigmoidal
    feedforward networks, Neurocomputing 143 (2014) 182–196.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. S. Sodhi, P. Chandra, 用于 sigmoid 前馈网络的双模态导数激活函数，Neurocomputing 143
    (2014) 182–196。'
- en: '[33] S. Eger, P. Youssef, I. Gurevych, Is it time to swish? comparing deep
    learning activation functions across nlp tasks, arXiv preprint arXiv:1901.02671
    (2019).'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Eger, P. Youssef, I. Gurevych, 是否该进行激活函数的切换？比较不同 NLP 任务中的深度学习激活函数，arXiv
    预印本 arXiv:1901.02671 (2019)。'
- en: '[34] A. L. Maas, A. Y. Hannun, A. Y. Ng, Rectifier nonlinearities improve neural
    network acoustic models, in: International Conference on Machine Learning, Vol. 30,
    2013, p. 3.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. L. Maas, A. Y. Hannun, A. Y. Ng, 整流非线性函数改进神经网络声学模型，见：国际机器学习会议，第 30
    卷，2013年，第 3 页。'
- en: '[35] K. He, X. Zhang, S. Ren, J. Sun, Delving deep into rectifiers: Surpassing
    human-level performance on imagenet classification, in: IEEE international conference
    on computer vision, 2015, pp. 1026–1034.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] K. He, X. Zhang, S. Ren, J. Sun, 深入研究整流器：在 ImageNet 分类上超越人类水平，见：IEEE 国际计算机视觉会议，2015年，第
    1026–1034 页。'
- en: '[36] W. Shang, K. Sohn, D. Almeida, H. Lee, Understanding and improving convolutional
    neural networks via concatenated rectified linear units, in: International Conference
    on Machine Learning, 2016, pp. 2217–2225.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] W. Shang, K. Sohn, D. Almeida, H. Lee, 通过串联的整流线性单元理解和改进卷积神经网络，见：国际机器学习会议，2016年，第
    2217–2225 页。'
- en: '[37] S. S. Liew, M. Khalil-Hani, R. Bakhteri, Bounded activation functions
    for enhanced training stability of deep neural networks on visual pattern recognition
    problems, Neurocomputing 216 (2016) 718–734.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. S. Liew, M. Khalil-Hani, R. Bakhteri, 用于视觉模式识别问题的深度神经网络增强训练稳定性的有界激活函数,
    《神经计算》216 (2016) 718–734。'
- en: '[38] R. Duggal, A. Gupta, P-telu: Parametric tan hyperbolic linear unit activation
    for deep neural networks, in: IEEE International Conference on Computer Vision
    Workshops, 2017, pp. 974–978.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] R. Duggal, A. Gupta, P-telu: 深度神经网络的参数化双曲正切线性单元激活函数, 见: IEEE国际计算机视觉研讨会,
    2017, 第974–978页。'
- en: '[39] S. Qiu, X. Xu, B. Cai, Frelu: Flexible rectified linear units for improving
    convolutional neural networks, in: International Conference on Pattern Recognition,
    2018, pp. 1223–1228.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Qiu, X. Xu, B. Cai, Frelu: 灵活整流线性单元用于改进卷积神经网络, 见: 国际模式识别会议, 2018, 第1223–1228页。'
- en: '[40] X. Jiang, Y. Pang, X. Li, J. Pan, Y. Xie, Deep neural networks with elastic
    rectified linear units for object recognition, Neurocomputing 275 (2018) 1132–1139.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] X. Jiang, Y. Pang, X. Li, J. Pan, Y. Xie, 使用弹性整流线性单元的深度神经网络用于物体识别, 《神经计算》275
    (2018) 1132–1139。'
- en: '[41] J. Cao, Y. Pang, X. Li, J. Liang, Randomly translational activation inspired
    by the input distributions of relu, Neurocomputing 275 (2018) 859–868.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Cao, Y. Pang, X. Li, J. Liang, 受relu输入分布启发的随机平移激活函数, 《神经计算》275 (2018)
    859–868。'
- en: '[42] F. Godin, J. Degrave, J. Dambre, W. De Neve, Dual rectified linear units
    (drelus): A replacement for tanh activation functions in quasi-recurrent neural
    networks, Pattern Recognition Letters 116 (2018) 8–14.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] F. Godin, J. Degrave, J. Dambre, W. De Neve, 双整流线性单元（drelus）：替代quasi-recurrent神经网络中的tanh激活函数,
    《模式识别通讯》116 (2018) 8–14。'
- en: '[43] Z. Tang, L. Luo, H. Peng, S. Li, A joint residual network with paired
    relus activation for image super-resolution, Neurocomputing 273 (2018) 37–46.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Z. Tang, L. Luo, H. Peng, S. Li, 一种结合paired relus激活的联合残差网络用于图像超分辨率, 《神经计算》273
    (2018) 37–46。'
- en: '[44] S. R. Dubey, S. Chakraborty, Average biased relu based cnn descriptor
    for improved face retrieval, arXiv preprint arXiv:1804.02051 (2018).'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. R. Dubey, S. Chakraborty, 基于平均偏置relu的cnn描述符用于改进人脸检索, arXiv预印本 arXiv:1804.02051
    (2018)。'
- en: '[45] Y. Liu, J. Zhang, C. Gao, J. Qu, L. Ji, Natural-logarithm-rectified activation
    function in convolutional neural networks, in: International Conference on Computer
    and Communications, 2019, pp. 2000–2008.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Y. Liu, J. Zhang, C. Gao, J. Qu, L. Ji, 卷积神经网络中的自然对数整流激活函数, 见: 国际计算机与通信会议,
    2019, 第2000–2008页。'
- en: '[46] S. Gu, W. Li, L. V. Gool, R. Timofte, Fast image restoration with multi-bin
    trainable linear units, in: IEEE International Conference on Computer Vision,
    2019, pp. 4190–4199.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] S. Gu, W. Li, L. V. Gool, R. Timofte, 基于多箱可训练线性单元的快速图像恢复, 见: IEEE国际计算机视觉会议,
    2019, 第4190–4199页。'
- en: '[47] M. Basirat, P. Roth, L* relu: Piece-wise linear activation functions for
    deep fine-grained visual categorization, in: IEEE Winter Conference on Applications
    of Computer Vision, 2020, pp. 1218–1227.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. Basirat, P. Roth, L* relu: 用于深度精细分类的分段线性激活函数, 见: IEEE冬季计算机视觉应用会议, 2020,
    第1218–1227页。'
- en: '[48] C. Gulcehre, M. Moczulski, M. Denil, Y. Bengio, Noisy activation functions,
    in: International Conference on Machine Learning, 2016, pp. 3059–3068.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] C. Gulcehre, M. Moczulski, M. Denil, Y. Bengio, 噪声激活函数, 见: 国际机器学习会议, 2016,
    第3059–3068页。'
- en: '[49] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, Y. Bengio,
    Maxout networks, arXiv preprint arXiv:1302.4389 (2013).'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, Y. Bengio,
    Maxout网络, arXiv预印本 arXiv:1302.4389 (2013)。'
- en: '[50] B. Xu, N. Wang, T. Chen, M. Li, Empirical evaluation of rectified activations
    in convolutional network, arXiv preprint arXiv:1505.00853 (2015).'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] B. Xu, N. Wang, T. Chen, M. Li, 卷积网络中整流激活函数的经验评估, arXiv预印本 arXiv:1505.00853
    (2015)。'
- en: '[51] H. Li, W. Ouyang, X. Wang, Multi-bias non-linear activation in deep neural
    networks, in: International Conference on Machine Learning, 2016, pp. 221–229.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] H. Li, W. Ouyang, X. Wang, 深度神经网络中的多偏置非线性激活, 见: 国际机器学习会议, 2016, 第221–229页。'
- en: '[52] G. Klambauer, T. Unterthiner, A. Mayr, S. Hochreiter, Self-normalizing
    neural networks, in: Advances in Neural Information Processing Systems, 2017,
    pp. 971–980.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] G. Klambauer, T. Unterthiner, A. Mayr, S. Hochreiter, 自归一化神经网络, 见: 《神经信息处理系统进展》,
    2017, 第971–980页。'
- en: '[53] J. T. Barron, Continuously differentiable exponential linear units, arXiv
    (2017) arXiv–1704.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. T. Barron, 连续可微分的指数线性单元, arXiv (2017) arXiv–1704。'
- en: '[54] L. Trottier, P. Gigu, B. Chaib-draa, et al., Parametric exponential linear
    unit for deep convolutional neural networks, in: IEEE International Conference
    on Machine Learning and Applications, 2017, pp. 207–214.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] L. Trottier, P. Gigu, B. Chaib-draa, 等，深度卷积神经网络的参数化指数线性单元，载于：IEEE 机器学习与应用国际会议，2017年，页码207–214。'
- en: '[55] Y. Li, C. Fan, Y. Li, Q. Wu, Y. Ming, Improving deep neural network with
    multiple parametric exponential linear units, Neurocomputing 301 (2018) 11–24.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Li, C. Fan, Y. Li, Q. Wu, Y. Ming, 使用多个参数化指数线性单元改进深度神经网络，《神经计算》301
    (2018) 11–24。'
- en: '[56] Z. Qiumei, T. Dan, W. Fenghua, Improved convolutional neural network based
    on fast exponentially linear unit activation function, IEEE Access 7 (2019) 151359–151367.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Z. Qiumei, T. Dan, W. Fenghua, 基于快速指数线性单元激活函数的改进卷积神经网络，《IEEE 访问》7 (2019)
    151359–151367。'
- en: '[57] Y. Ying, J. Su, P. Shan, L. Miao, X. Wang, S. Peng, Rectified exponential
    units for convolutional neural networks, IEEE Access 7 (2019) 101633–101640.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Ying, J. Su, P. Shan, L. Miao, X. Wang, S. Peng, 修正指数单元用于卷积神经网络，《IEEE
    访问》7 (2019) 101633–101640。'
- en: '[58] D. Kim, J. Kim, J. Kim, Elastic exponential linear units for convolutional
    neural networks, Neurocomputing 406 (2020) 253–266.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] D. Kim, J. Kim, J. Kim, 卷积神经网络的弹性指数线性单元，《神经计算》406 (2020) 253–266。'
- en: '[59] Q. Cheng, H. Li, Q. Wu, L. Ma, N. N. King, Parametric deformable exponential
    linear units for deep neural networks, Neural Networks 125 (2020) 281–289.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Q. Cheng, H. Li, Q. Wu, L. Ma, N. N. King, 深度神经网络的参数化可变形指数线性单元，《神经网络》125
    (2020) 281–289。'
- en: '[60] J. Si, S. L. Harris, E. Yfantis, A dynamic relu on neural network, in:
    IEEE Dallas Circuits and Systems Conference, 2018, pp. 1–6.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. Si, S. L. Harris, E. Yfantis, 神经网络中的动态 ReLU，载于：IEEE 达拉斯电路与系统会议，2018年，页码1–6。'
- en: '[61] H. Hu, Vrelu activation functions for artificial neural networks, in:
    International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery,
    2018, pp. 856–860.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] H. Hu, Vrelu 激活函数用于人工神经网络，载于：国际自然计算、模糊系统与知识发现会议，2018年，页码856–860。'
- en: '[62] G. Lin, W. Shen, Research on convolutional neural network based on improved
    relu piecewise activation function, Procedia Computer Science 131 (2018) 977–984.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] G. Lin, W. Shen, 基于改进的 ReLU 分段激活函数的卷积神经网络研究，《计算机科学学报》131 (2018) 977–984。'
- en: '[63] D. Macêdo, C. Zanchettin, A. L. Oliveira, T. Ludermir, Enhancing batch
    normalized convolutional networks using displaced rectifier linear units: A systematic
    comparative study, Expert Systems with Applications 124 (2019) 271–281.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] D. Macêdo, C. Zanchettin, A. L. Oliveira, T. Ludermir, 使用位移修正线性单元提升批量归一化卷积网络：系统性比较研究，《专家系统与应用》124
    (2019) 271–281。'
- en: '[64] L. B. Godfrey, An evaluation of parametric activation functions for deep
    learning, in: IEEE International Conference on Systems, Man and Cybernetics, 2019,
    pp. 3006–3011.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] L. B. Godfrey, 对深度学习的参数化激活函数的评估，载于：IEEE 系统、人与控制国际会议，2019年，页码3006–3011。'
- en: '[65] X. Jin, C. Xu, J. Feng, Y. Wei, J. Xiong, S. Yan, Deep learning with s-shaped
    rectified linear activation units, in: AAAI Conference on Artificial Intelligence,
    2016.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] X. Jin, C. Xu, J. Feng, Y. Wei, J. Xiong, S. Yan, 使用 S 形修正线性激活单元的深度学习，载于：AAAI
    人工智能会议，2016年。'
- en: '[66] V. S. Bawa, V. Kumar, Linearized sigmoidal activation: A novel activation
    function with tractable non-linear characteristics to boost representation capability,
    Expert Systems with Applications 120 (2019) 346–356.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] V. S. Bawa, V. Kumar, 线性化 S 型激活：一种具有可处理非线性特性的创新激活函数，提升表示能力，《专家系统与应用》120
    (2019) 346–356。'
- en: '[67] X. Wang, Y. Qin, Y. Wang, S. Xiang, H. Chen, Reltanh: An activation function
    with vanishing gradient resistance for sae-based dnns and its application to rotating
    machinery fault diagnosis, Neurocomputing 363 (2019) 88–98.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] X. Wang, Y. Qin, Y. Wang, S. Xiang, H. Chen, Reltanh：一种对抗梯度消失的激活函数，用于基于
    SAE 的 DNN 及其在旋转机械故障诊断中的应用，《神经计算》363 (2019) 88–98。'
- en: '[68] X. Hu, P. Niu, J. Wang, X. Zhang, A dynamic rectified linear activation
    units, IEEE Access 7 (2019) 180409–180416.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] X. Hu, P. Niu, J. Wang, X. Zhang, 动态修正线性激活单元，《IEEE 访问》7 (2019) 180409–180416。'
- en: '[69] A. Nicolae, Plu: The piecewise linear unit activation function, arXiv
    preprint arXiv:1809.09534 (2018).'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Nicolae, Plu：分段线性单元激活函数，arXiv 预印本 arXiv:1809.09534 (2018)。'
- en: '[70] L. B. Godfrey, M. S. Gashler, A continuum among logarithmic, linear, and
    exponential functions, and its potential to improve generalization in neural networks,
    in: International Joint Conference on Knowledge Discovery, Knowledge Engineering
    and Knowledge Management, Vol. 1, 2015, pp. 481–486.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] L. B. Godfrey, M. S. Gashler, 对数、线性和指数函数之间的连续性及其提高神经网络泛化能力的潜力，见：国际联合知识发现、知识工程与知识管理会议，第
    1 卷，2015，页 481–486。'
- en: '[71] B. Grelsson, M. Felsberg, Improved learning in convolutional neural networks
    with shifted exponential linear units (shelus), in: International Conference on
    Pattern Recognition, 2018, pp. 517–522.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] B. Grelsson, M. Felsberg, 使用移位指数线性单元（shelus）改进卷积神经网络中的学习，见：国际模式识别会议，2018，页
    517–522。'
- en: '[72] Y. Yu, K. Adu, N. Tashi, P. Anokye, X. Wang, M. A. Ayidzoe, Rmaf: Relu-memristor-like
    activation function for deep learning, IEEE Access 8 (2020) 72727–72741.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Yu, K. Adu, N. Tashi, P. Anokye, X. Wang, M. A. Ayidzoe, Rmaf：类似 relu
    的记忆电阻激活函数，用于深度学习，IEEE Access 8 (2020) 72727–72741。'
- en: '[73] M. Basirat, P. M. Roth, The quest for the golden activation function,
    arXiv preprint arXiv:1808.00783 (2018).'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. Basirat, P. M. Roth, 寻找黄金激活函数，arXiv 预印本 arXiv:1808.00783 (2018)。'
- en: '[74] S. Scardapane, M. Scarpiniti, D. Comminiello, A. Uncini, Learning activation
    functions from data using cubic spline interpolation, in: Italian Workshop on
    Neural Nets, 2017, pp. 73–83.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] S. Scardapane, M. Scarpiniti, D. Comminiello, A. Uncini, 使用三次样条插值从数据中学习激活函数，见：意大利神经网络研讨会，2017，页
    73–83。'
- en: '[75] A. Mishra, P. Chandra, U. Ghose, S. S. Sodhi, Bi-modal derivative adaptive
    activation function sigmoidal feedforward artificial neural networks, Applied
    Soft Computing 61 (2017) 983–994.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] A. Mishra, P. Chandra, U. Ghose, S. S. Sodhi, 双模态导数自适应激活函数 sigmoid 前馈人工神经网络，Applied
    Soft Computing 61 (2017) 983–994。'
- en: '[76] S. Qian, H. Liu, C. Liu, S. Wu, H. San Wong, Adaptive activation functions
    in convolutional neural networks, Neurocomputing 272 (2018) 204–212.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S. Qian, H. Liu, C. Liu, S. Wu, H. San Wong, 卷积神经网络中的自适应激活函数，Neurocomputing
    272 (2018) 204–212。'
- en: '[77] E. Alcaide, E-swish: Adjusting activations to different network depths,
    arXiv preprint arXiv:1801.07145 (2018).'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] E. Alcaide, E-swish：调整不同网络深度的激活函数，arXiv 预印本 arXiv:1801.07145 (2018)。'
- en: '[78] Ö. F. Ertuğrul, A novel type of activation function in artificial neural
    networks: Trained activation function, Neural Networks 99 (2018) 148–157.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Ö. F. Ertuğrul, 人工神经网络中的一种新型激活函数：训练激活函数，Neural Networks 99 (2018) 148–157。'
- en: '[79] M. Goyal, R. Goyal, B. Lall, Learning activation functions: A new paradigm
    of understanding neural networks, arXiv preprint arXiv:1906.09529 (2019).'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] M. Goyal, R. Goyal, B. Lall, 学习激活函数：理解神经网络的新范式，arXiv 预印本 arXiv:1906.09529
    (2019)。'
- en: '[80] G. Maguolo, L. Nanni, S. Ghidoni, Ensemble of convolutional neural networks
    trained with different activation functions, arXiv preprint arXiv:1905.02473 (2019).'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] G. Maguolo, L. Nanni, S. Ghidoni, 训练有不同激活函数的卷积神经网络的集成，arXiv 预印本 arXiv:1905.02473
    (2019)。'
- en: '[81] H. H. Chieng, N. Wahid, P. Ong, S. R. K. Perla, Flatten-t swish: a thresholded
    relu-swish-like activation function for deep learning, arXiv preprint arXiv:1812.06247
    (2018).'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] H. H. Chieng, N. Wahid, P. Ong, S. R. K. Perla, Flatten-t swish：一种用于深度学习的阈值化
    relu-swish 类激活函数，arXiv 预印本 arXiv:1812.06247 (2018)。'
- en: '[82] N. Patwardhan, M. Ingalhalikar, R. Walambe, Aria: Utilizing richard’s
    curve for controlling the non-monotonicity of the activation function in deep
    neural nets, arXiv preprint arXiv:1805.08878 (2018).'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] N. Patwardhan, M. Ingalhalikar, R. Walambe, Aria：利用理查德曲线控制深度神经网络中激活函数的非单调性，arXiv
    预印本 arXiv:1805.08878 (2018)。'
- en: '[83] M. Dushkoff, R. Ptucha, Adaptive activation functions for deep networks,
    Electronic Imaging 2016 (19) (2016) 1–5.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] M. Dushkoff, R. Ptucha, 深度网络的自适应激活函数，Electronic Imaging 2016 (19) (2016)
    1–5。'
- en: '[84] F. Manessi, A. Rozza, Learning combinations of activation functions, in:
    IEEE International Conference on Pattern Recognition, 2018, pp. 61–66.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] F. Manessi, A. Rozza, 学习激活函数的组合，见：IEEE 国际模式识别会议，2018，页 61–66。'
- en: '[85] L. R. Sütfeld, F. Brieger, H. Finger, S. Füllhase, G. Pipa, Adaptive blending
    units: Trainable activation functions for deep neural networks, arXiv preprint
    arXiv:1806.10064 (2018).'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] L. R. Sütfeld, F. Brieger, H. Finger, S. Füllhase, G. Pipa, 自适应混合单元：用于深度神经网络的可训练激活函数，arXiv
    预印本 arXiv:1806.10064 (2018)。'
- en: '[86] M. Wang, B. Liu, H. Foroosh, Look-up table unit activation function for
    deep convolutional neural networks, in: IEEE Winter Conference on Applications
    of Computer Vision, 2018, pp. 1225–1233.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] M. Wang, B. Liu, H. Foroosh, 用于深度卷积神经网络的查找表单元激活函数，见：IEEE 计算机视觉应用冬季会议，2018，页
    1225–1233。'
- en: '[87] D. Klabjan, M. Harmon, Activation ensembles for deep neural networks,
    in: IEEE International Conference on Big Data, 2019, pp. 206–214.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] D. Klabjan, M. Harmon, 深度神经网络的激活集成，发表于：IEEE国际大数据会议，2019年，第206–214页。'
- en: '[88] C. Eisenach, Z. Wang, H. Liu, Nonparametrically learning activation functions
    in deep neural nets, in: International Conference on Learning Representations
    Workshops, 2017.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C. Eisenach, Z. Wang, H. Liu, 在深度神经网络中非参数学习激活函数，发表于：学习表示国际会议研讨会，2017年。'
- en: '[89] C. J. Vercellino, W. Y. Wang, Hyperactivations for activation function
    exploration, in: Conference on Neural Information Processing Systems Workshop
    on Meta-learning, 2017.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] C. J. Vercellino, W. Y. Wang, 激活函数探索的超激活，发表于：神经信息处理系统会议的元学习研讨会，2017年。'
- en: '[90] A. D. Jagtap, K. Kawaguchi, G. E. Karniadakis, Adaptive activation functions
    accelerate convergence in deep and physics-informed neural networks, Journal of
    Computational Physics 404 (2020) 109136.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. D. Jagtap, K. Kawaguchi, G. E. Karniadakis, 自适应激活函数加速深度和物理信息神经网络的收敛，《计算物理学杂志》404
    (2020) 109136。'
- en: '[91] C. Dugas, Y. Bengio, F. Bélisle, C. Nadeau, R. Garcia, Incorporating second-order
    functional knowledge for better option pricing, in: Advances in Neural Information
    Processing Systems, 2001, pp. 472–478.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] C. Dugas, Y. Bengio, F. Bélisle, C. Nadeau, R. Garcia, 结合二阶功能知识以更好地进行期权定价，发表于：神经信息处理系统进展，2001年，第472–478页。'
- en: '[92] X. Glorot, A. Bordes, Y. Bengio, Deep sparse rectifier neural networks,
    in: International Conference on Artificial Intelligence and Statistics, 2011,
    pp. 315–323.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] X. Glorot, A. Bordes, Y. Bengio, 深度稀疏整流神经网络，发表于：人工智能与统计国际会议，2011年，第315–323页。'
- en: '[93] H. Zheng, Z. Yang, W. Liu, J. Liang, Y. Li, Improving deep neural networks
    using softplus units, in: International Joint Conference on Neural Networks, 2015,
    pp. 1–4.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] H. Zheng, Z. Yang, W. Liu, J. Liang, Y. Li, 利用softplus单元改进深度神经网络，发表于：国际联合神经网络会议，2015年，第1–4页。'
- en: '[94] Q. Liu, S. Furber, Noisy softplus: a biology inspired activation function,
    in: International Conference on Neural Information Processing, 2016, pp. 405–412.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Q. Liu, S. Furber, 噪声softplus：一种受生物启发的激活函数，发表于：神经信息处理国际会议，2016年，第405–412页。'
- en: '[95] H. Zhao, F. Liu, L. Li, C. Luo, A novel softplus linear unit for deep
    convolutional neural networks, Applied Intelligence 48 (7) (2018) 1707–1720.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] H. Zhao, F. Liu, L. Li, C. Luo, 用于深度卷积神经网络的新型softplus线性单元，应用智能 48 (7)
    (2018) 1707–1720。'
- en: '[96] C. Xu, J. Huang, S.-p. Wang, A.-q. Hu, A novel parameterized activation
    function in visual geometry group, in: International Conference on Data Science
    and Business Analytics, 2018, pp. 386–389.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. Xu, J. Huang, S.-p. Wang, A.-q. Hu, 视觉几何组中的一种新型参数化激活函数，发表于：数据科学与商业分析国际会议，2018年，第386–389页。'
- en: '[97] K. Sun, J. Yu, L. Zhang, Z. Dong, A convolutional neural network model
    based on improved softplus activation function, in: International Conference on
    Applications and Techniques in Cyber Security and Intelligence, 2019, pp. 1326–1335.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K. Sun, J. Yu, L. Zhang, Z. Dong, 基于改进的softplus激活函数的卷积神经网络模型，发表于：网络安全与智能应用技术国际会议，2019年，第1326–1335页。'
- en: '[98] Y. Chen, Y. Mai, J. Xiao, L. Zhang, Improving the antinoise ability of
    dnns via a bio-inspired noise adaptive activation function rand softplus, Neural
    Computation 31 (6) (2019) 1215–1233.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Chen, Y. Mai, J. Xiao, L. Zhang, 通过生物启发的噪声自适应激活函数rand softplus提高DNN的抗噪声能力，《神经计算》31
    (6) (2019) 1215–1233。'
- en: '[99] D. Misra, Mish: A self regularized non-monotonic neural activation function,
    arXiv preprint arXiv:1908.08681 (2019).'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] D. Misra, Mish: 一种自我正则化的非单调神经激活函数，arXiv 预印本 arXiv:1908.08681 (2019)。'
- en: '[100] A. Bochkovskiy, C.-Y. Wang, H.-Y. M. Liao, Yolov4: Optimal speed and
    accuracy of object detection, arXiv preprint arXiv:2004.10934 (2020).'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] A. Bochkovskiy, C.-Y. Wang, H.-Y. M. Liao, Yolov4: 目标检测的最佳速度和准确性，arXiv
    预印本 arXiv:2004.10934 (2020)。'
- en: '[101] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv preprint
    arXiv:1606.08415 (2016).'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] D. Hendrycks, K. Gimpel, 高斯误差线性单元（gelus），arXiv 预印本 arXiv:1606.08415 (2016)。'
- en: '[102] C. Yu, Z. Su, Symmetrical gaussian error linear units (sgelus), arXiv
    preprint arXiv:1911.03925 (2019).'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] C. Yu, Z. Su, 对称高斯误差线性单元（sgelus），arXiv 预印本 arXiv:1911.03925 (2019)。'
- en: '[103] Q. Su, L. Carin, et al., A probabilistic framework for nonlinearities
    in stochastic neural networks, in: Advances in Neural Information Processing Systems,
    2017, pp. 4486–4495.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Q. Su, L. Carin 等，随机神经网络中非线性的概率框架，发表于：神经信息处理系统进展，2017年，第4486–4495页。'
- en: '[104] J. Lee, K. Shridhar, H. Hayashi, B. K. Iwana, S. Kang, S. Uchida, Probact:
    A probabilistic activation function for deep neural networks, arXiv preprint arXiv:1905.10761
    (2019).'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. Lee, K. Shridhar, H. Hayashi, B. K. Iwana, S. Kang, S. Uchida, Probact：深度神经网络的概率激活函数，arXiv
    预印本 arXiv:1905.10761 (2019)。'
- en: '[105] L. Hou, D. Samaras, T. M. Kurc, Y. Gao, J. H. Saltz, Convnets with smooth
    adaptive activation functions for regression, Proceedings of Machine Learning
    Research 54 (2017) 430.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] L. Hou, D. Samaras, T. M. Kurc, Y. Gao, J. H. Saltz, 具有平滑自适应激活函数的卷积网络用于回归，机器学习研究会议论文
    54 (2017) 430。'
- en: '[106] Y. Berradi, Symmetric power activation functions for deep neural networks,
    in: International Conference on Learning and Optimization Algorithms: Theory and
    Applications, 2018, pp. 1–6.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Y. Berradi, 对称幂激活函数用于深度神经网络，在：国际学习与优化算法会议：理论与应用，2018，pp. 1–6。'
- en: '[107] E. López-Rubio, F. Ortega-Zamorano, E. Domínguez, J. Muñoz-Pérez, Piecewise
    polynomial activation functions for feedforward neural networks, Neural Processing
    Letters 50 (1) (2019) 121–147.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] E. López-Rubio, F. Ortega-Zamorano, E. Domínguez, J. Muñoz-Pérez, 用于前馈神经网络的分段多项式激活函数，神经处理信件
    50 (1) (2019) 121–147。'
- en: '[108] F. Farhadi, V. P. Nia, A. Lodi, Activation adaptation in neural networks,
    arXiv preprint arXiv:1901.09849 (2019).'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] F. Farhadi, V. P. Nia, A. Lodi, 神经网络中的激活适应，arXiv 预印本 arXiv:1901.09849
    (2019)。'
- en: '[109] B. Li, S. Tang, H. Yu, Powernet: Efficient representations of polynomials
    and smooth functions by deep neural networks with rectified power units, arXiv
    preprint arXiv:1909.05136 (2019).'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] B. Li, S. Tang, H. Yu, Powernet：通过具有整流幂单元的深度神经网络高效表示多项式和光滑函数，arXiv 预印本
    arXiv:1909.05136 (2019)。'
- en: '[110] M. Telgarsky, Neural networks and rational functions, in: International
    Conference on Machine Learning, 2017, pp. 3387–3393.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] M. Telgarsky, 神经网络与有理函数，在：国际机器学习会议，2017，pp. 3387–3393。'
- en: '[111] A. Molina, P. Schramowski, K. Kersting, Pad$\acute{e}$ activation units:
    End-to-end learning of flexible activation functions in deep networks, International
    Conference on Learning Representations (2020).'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] A. Molina, P. Schramowski, K. Kersting, Pad$\acute{e}$ 激活单元：深度网络中灵活激活函数的端到端学习，国际学习表示会议
    (2020)。'
- en: '[112] A. T. Nicolas Boullé, Yuji Nakatsukasa, Rational neural networks, arXiv
    preprint arXiv:2004.01902 (2020).'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] A. T. Nicolas Boullé, Yuji Nakatsukasa, 有理神经网络，arXiv 预印本 arXiv:2004.01902
    (2020)。'
- en: '[113] A. Apicella, F. Isgrò, R. Prevete, A simple and efficient architecture
    for trainable activation functions, Neurocomputing 370 (2019) 1–15.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Apicella, F. Isgrò, R. Prevete, 一种简单高效的可训练激活函数架构，神经计算 370 (2019) 1–15。'
- en: '[114] Y. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, Z. Liu, Dynamic relu, arXiv
    preprint arXiv:2003.10027 (2020).'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Y. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, Z. Liu, 动态 relu，arXiv 预印本
    arXiv:2003.10027 (2020)。'
- en: '[115] M. Wang, B. Liu, H. Foroosh, Wide hidden expansion layer for deep convolutional
    neural networks, in: IEEE Winter Conference on Applications of Computer Vision,
    2020, pp. 934–942.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] M. Wang, B. Liu, H. Foroosh, 深度卷积神经网络的宽隐藏扩展层，在：IEEE 冬季计算机视觉应用会议，2020，pp.
    934–942。'
- en: '[116] A. Asif, et al., Learning neural activations, arXiv preprint arXiv:1912.12187
    (2019).'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] A. Asif 等，学习神经激活，arXiv 预印本 arXiv:1912.12187 (2019)。'
- en: '[117] S. Scardapane, S. Van Vaerenbergh, S. Totaro, A. Uncini, Kafnets: Kernel-based
    non-parametric activation functions for neural networks, Neural Networks 110 (2019)
    19–32.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. Scardapane, S. Van Vaerenbergh, S. Totaro, A. Uncini, Kafnets：基于核的非参数激活函数用于神经网络，神经网络
    110 (2019) 19–32。'
- en: '[118] S. Scardapane, E. Nieddu, D. Firmani, P. Merialdo, Multikernel activation
    functions: formulation and a case study, in: INNS Big Data and Deep Learning conference,
    2019, pp. 320–329.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] S. Scardapane, E. Nieddu, D. Firmani, P. Merialdo, 多核激活函数：公式化及案例研究，在：INNS
    大数据与深度学习会议，2019，pp. 320–329。'
- en: '[119] S. Scardapane, S. Van Vaerenbergh, A. Hussain, A. Uncini, Complex-valued
    neural networks with nonparametric activation functions, IEEE Transactions on
    Emerging Topics in Computational Intelligence (2018).'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] S. Scardapane, S. Van Vaerenbergh, A. Hussain, A. Uncini, 具有非参数激活函数的复杂值神经网络，IEEE
    计算智能新兴主题交易 (2018)。'
- en: '[120] S. Scardapane, S. Van Vaerenbergh, D. Comminiello, A. Uncini, Widely
    linear kernels for complex-valued kernel activation functions, in: IEEE International
    Conference on Acoustics, Speech and Signal Processing, 2019, pp. 8528–8532.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] S. Scardapane, S. Van Vaerenbergh, D. Comminiello, A. Uncini, 复杂值核激活函数的广义线性核，在：IEEE
    国际声学、语音和信号处理会议，2019，pp. 8528–8532。'
- en: '[121] M. Kobayashi, Singularities of three-layered complex-valued neural networks
    with split activation function, IEEE Transactions on Neural Networks and Learning
    Systems 29 (5) (2017) 1900–1907.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Kobayashi, 三层复值神经网络的奇异性与分裂激活函数，IEEE 神经网络与学习系统学报 29 (5) (2017) 1900–1907。'
- en: '[122] J. Pennington, S. Schoenholz, S. Ganguli, Resurrecting the sigmoid in
    deep learning through dynamical isometry: theory and practice, in: Advances in
    Neural Information Processing Systems, 2017, pp. 4785–4795.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] J. Pennington, S. Schoenholz, S. Ganguli, 通过动态等距性复兴深度学习中的 sigmoid：理论与实践，见：神经信息处理系统进展，2017，第4785–4795页。'
- en: '[123] E. Sansone, F. G. De Natale, Training feedforward neural networks with
    standard logistic activations is feasible, arXiv preprint arXiv:1710.01013 (2017).'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] E. Sansone, F. G. De Natale, 用标准 logistic 激活函数训练前馈神经网络是可行的，arXiv 预印本
    arXiv:1710.01013 (2017)。'
- en: '[124] L. Lu, Y. Shin, Y. Su, G. E. Karniadakis, Dying relu and initialization:
    Theory and numerical examples, arXiv preprint arXiv:1903.06733 (2019).'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] L. Lu, Y. Shin, Y. Su, G. E. Karniadakis, 退化的 relu 和初始化：理论与数值例子，arXiv
    预印本 arXiv:1903.06733 (2019)。'
- en: '[125] D. Arpit, Y. Bengio, The benefits of over-parameterization at initialization
    in deep relu networks, arXiv preprint arXiv:1901.03611 (2019).'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] D. Arpit, Y. Bengio, 深度 relu 网络在初始化时的超参数化优势，arXiv 预印本 arXiv:1901.03611
    (2019)。'
- en: '[126] D. Aguirre, O. Fuentes, Improving weight initialization of relu and output
    layers, in: International Conference on Artificial Neural Networks, 2019, pp.
    170–184.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] D. Aguirre, O. Fuentes, 改进 relu 和输出层的权重初始化，见：国际人工神经网络会议，2019，第170–184页。'
- en: '[127] R. Burkholz, A. Dubatovka, Initialization of relus for dynamical isometry,
    in: Advances in Neural Information Processing Systems, 2019, pp. 2382–2392.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] R. Burkholz, A. Dubatovka, 为动态等距性初始化 relu，见：神经信息处理系统进展，2019，第2382–2392页。'
- en: '[128] D. Yarotsky, Error bounds for approximations with deep relu networks,
    Neural Networks 94 (2017) 103–114.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] D. Yarotsky, 深度 relu 网络的近似误差界限，神经网络 94 (2017) 103–114。'
- en: '[129] R. Arora, A. Basu, P. Mianjy, A. Mukherjee, Understanding deep neural
    networks with rectified linear units, arXiv preprint arXiv:1611.01491 (2016).'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] R. Arora, A. Basu, P. Mianjy, A. Mukherjee, 理解带有修正线性单元的深度神经网络，arXiv 预印本
    arXiv:1611.01491 (2016)。'
- en: '[130] M. Hein, M. Andriushchenko, J. Bitterwolf, Why relu networks yield high-confidence
    predictions far away from the training data and how to mitigate the problem, in:
    IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 41–50.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] M. Hein, M. Andriushchenko, J. Bitterwolf, 为什么 relu 网络在训练数据之外给出高置信度预测以及如何缓解这个问题，见：IEEE
    计算机视觉与模式识别会议，2019，第41–50页。'
- en: '[131] S. Goel, S. Karmalkar, A. Klivans, Time/accuracy tradeoffs for learning
    a relu with respect to gaussian marginals, in: Advances in Neural Information
    Processing Systems, 2019, pp. 8582–8591.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] S. Goel, S. Karmalkar, A. Klivans, 关于学习带有高斯边际的 relu 的时间/精度权衡，见：神经信息处理系统进展，2019，第8582–8591页。'
- en: '[132] S. Dittmer, J. Emily, P. Maass, Singular values for relu layers, IEEE
    Transactions on Neural Networks and Learning Systems (2019).'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] S. Dittmer, J. Emily, P. Maass, relu 层的奇异值，IEEE 神经网络与学习系统学报 (2019)。'
- en: '[133] A. Kristiadi, M. Hein, P. Hennig, Being bayesian, even just a bit, fixes
    overconfidence in relu networks, arXiv preprint arXiv:2002.10118 (2020).'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] A. Kristiadi, M. Hein, P. Hennig, 即使只是略微贝叶斯化，也能修正 relu 网络的过度自信，arXiv
    预印本 arXiv:2002.10118 (2020)。'
- en: '[134] B. Karlik, A. V. Olgac, Performance analysis of various activation functions
    in generalized mlp architectures of neural networks, International Journal of
    Artificial Intelligence and Expert Systems 1 (4) (2011) 111–122.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] B. Karlik, A. V. Olgac, 各种激活函数在神经网络通用 MLP 架构中的性能分析，国际人工智能与专家系统杂志 1 (4)
    (2011) 111–122。'
- en: '[135] G. Alcantara, Empirical analysis of non-linear activation functions for
    deep neural networks in classification tasks, arXiv preprint arXiv:1710.11272
    (2017).'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] G. Alcantara, 对于分类任务的深度神经网络中非线性激活函数的实证分析，arXiv 预印本 arXiv:1710.11272 (2017)。'
- en: '[136] H. K. Vydana, A. K. Vuppala, Investigative study of various activation
    functions for speech recognition, in: National Conference on Communications, 2017,
    pp. 1–5.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] H. K. Vydana, A. K. Vuppala, 各种激活函数在语音识别中的研究，见：国家通信会议，2017，第1–5页。'
- en: '[137] D. Pedamonti, Comparison of non-linear activation functions for deep
    neural networks on mnist classification task, arXiv preprint arXiv:1804.02763
    (2018).'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] D. Pedamonti, 对比深度神经网络在 MNIST 分类任务中的非线性激活函数，arXiv 预印本 arXiv:1804.02763
    (2018)。'
- en: '[138] C. Nwankpa, W. Ijomah, A. Gachagan, S. Marshall, Activation functions:
    Comparison of trends in practice and research for deep learning, arXiv preprint
    arXiv:1811.03378 (2018).'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] C. Nwankpa, W. Ijomah, A. Gachagan, S. Marshall, 激活函数：深度学习实践与研究趋势的比较，arXiv
    预印本 arXiv:1811.03378 (2018)。'
- en: '[139] K. Eckle, J. Schmidt-Hieber, A comparison of deep networks with relu
    activation function and linear spline-type methods, Neural Networks 110 (2019)
    232–242.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] K. Eckle, J. Schmidt-Hieber, 深度网络与relu激活函数及线性样条方法的比较，Neural Networks
    110 (2019) 232–242。'
- en: '[140] M. M. Lau, K. H. Lim, Review of adaptive activation function in deep
    neural network, in: IEEE-EMBS Conference on Biomedical Engineering and Sciences,
    2018, pp. 686–690.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] M. M. Lau, K. H. Lim, 深度神经网络中自适应激活函数的回顾，见：IEEE-EMBS生物医学工程与科学会议，2018年，页686–690。'
- en: '[141] A. K. Dubey, V. Jain, Comparative study of convolution neural network’s
    relu and leaky-relu activation functions, in: Applications of Computing, Automation
    and Wireless Systems in Electrical Engineering, Springer, 2019, pp. 873–880.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] A. K. Dubey, V. Jain, 卷积神经网络中relu与leaky-relu激活函数的比较研究，见：电气工程计算、自动化与无线系统应用，Springer，2019年，页873–880。'
- en: '[142] C. Banerjee, T. Mukherjee, E. Pasiliao Jr, An empirical study on generalizations
    of the relu activation function, in: ACM Southeast Conference, 2019, pp. 164–167.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] C. Banerjee, T. Mukherjee, E. Pasiliao Jr, relu激活函数的广义化实证研究，见：ACM东南会议，2019年，页164–167。'
- en: '[143] T. Villmann, J. Ravichandran, A. Villmann, D. Nebel, M. Kaden, Activation
    functions for generalized learning vector quantization-a performance comparison,
    arXiv preprint arXiv:1901.05995 (2019).'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] T. Villmann, J. Ravichandran, A. Villmann, D. Nebel, M. Kaden, 广义学习向量量化的激活函数——性能比较，arXiv
    预印本 arXiv:1901.05995 (2019)。'
- en: '[144] G. Castaneda, P. Morris, T. M. Khoshgoftaar, Evaluation of maxout activations
    in deep learning across several big data domains, Journal of Big Data 6 (1) (2019)
    72.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] G. Castaneda, P. Morris, T. M. Khoshgoftaar, 多个大数据领域深度学习中maxout激活的评估，Journal
    of Big Data 6 (1) (2019) 72。'
- en: '[145] Y. Wang, Y. Li, Y. Song, X. Rong, The influence of the activation function
    in a convolution neural network model of facial expression recognition, Applied
    Sciences 10 (5) (2020) 1897.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Y. Wang, Y. Li, Y. Song, X. Rong, 激活函数在面部表情识别卷积神经网络模型中的影响，Applied Sciences
    10 (5) (2020) 1897。'
- en: '[146] A. Apicella, F. Donnarumma, F. Isgrò, R. Prevete, A survey on modern
    trainable activation functions, arXiv preprint arXiv:2005.00817 (2020).'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] A. Apicella, F. Donnarumma, F. Isgrò, R. Prevete, 现代可训练激活函数的调查，arXiv
    预印本 arXiv:2005.00817 (2020)。'
- en: '[147] T. Szandała, Review and comparison of commonly used activation functions
    for deep neural networks, in: Bio-inspired Neurocomputing, 2020, pp. 203–224.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] T. Szandała, 常用激活函数的回顾与比较，见：生物启发神经计算，2020年，页203–224。'
- en: '[148] A. Krizhevsky, Learning multiple layers of features from tiny images,
    Tech Report, Univ. of Toronto (2009).'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] A. Krizhevsky, 从小图像中学习多个特征层，技术报告，多伦多大学（2009年）。'
- en: '[149] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    H. Adam, Mobilenets: Efficient convolutional neural networks for mobile vision
    applications, arXiv preprint arXiv:1704.04861 (2017).'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto, H. Adam, Mobilenets：移动视觉应用的高效卷积神经网络，arXiv 预印本 arXiv:1704.04861 (2017)。'
- en: '[150] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale
    image recognition, arXiv preprint arXiv:1409.1556 (2014).'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] K. Simonyan, A. Zisserman, 用于大规模图像识别的非常深卷积网络，arXiv 预印本 arXiv:1409.1556
    (2014)。'
- en: '[151] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: IEEE Conference
    on Computer Vision and Pattern Recognition, 2015, pp. 1–9.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, A. Rabinovich, 更深层次的卷积，见：IEEE计算机视觉与模式识别大会，2015年，页1–9。'
- en: '[152] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] K. He, X. Zhang, S. Ren, J. Sun, 图像识别的深度残差学习，见：IEEE计算机视觉与模式识别大会，2016年，页770–778。'
- en: '[153] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation networks, in: IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 7132–7141.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] J. Hu, L. Shen, G. Sun, 压缩与激励网络，见：IEEE计算机视觉与模式识别大会，2018年，页7132–7141。'
- en: '[154] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely connected
    convolutional networks, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2017, pp. 4700–4708.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, 《密集连接卷积网络》，载于《IEEE计算机视觉与模式识别会议论文集》，2017年，页码4700–4708。'
- en: '[155] K. Papineni, S. Roukos, T. Ward, W.-J. Zhu, Bleu: a method for automatic
    evaluation of machine translation, in: Proceedings of the 40th annual meeting
    of the Association for Computational Linguistics, 2002, pp. 311–318.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] K. Papineni, S. Roukos, T. Ward, W.-J. Zhu, 《Bleu: 一种自动评估机器翻译的方法》，载于《计算语言学协会第40届年会论文集》，2002年，页码311–318。'
