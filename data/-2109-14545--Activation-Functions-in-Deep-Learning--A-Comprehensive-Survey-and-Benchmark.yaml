- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 19:51:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 19:51:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2109.14545] Activation Functions in Deep Learning: A Comprehensive Survey
    and Benchmark'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2109.14545] æ·±åº¦å­¦ä¹ ä¸­çš„æ¿€æ´»å‡½æ•°ï¼šç»¼åˆè°ƒæŸ¥ä¸åŸºå‡†æµ‹è¯•'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2109.14545](https://ar5iv.labs.arxiv.org/html/2109.14545)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2109.14545](https://ar5iv.labs.arxiv.org/html/2109.14545)
- en: 'Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ ä¸­çš„æ¿€æ´»å‡½æ•°ï¼šç»¼åˆè°ƒæŸ¥ä¸åŸºå‡†æµ‹è¯•
- en: Shiv Ram DubeyÂ¹, Satish Kumar SinghÂ¹, Bidyut Baran ChaudhuriÂ² Â¹Computer Vision
    and Biometrics Laboratory, Indian Institute of Information Technology, Allahabad,
    India.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Shiv Ram DubeyÂ¹, Satish Kumar SinghÂ¹, Bidyut Baran ChaudhuriÂ² Â¹Computer Vision
    and Biometrics Laboratory, Indian Institute of Information Technology, Allahabad,
    Indiaã€‚
- en: Â²Techno India University, Kolkata, India and Indian Statistical Institute, Kolkata,
    India.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Â²Techno India University, Kolkata, India å’Œ Indian Statistical Institute, Kolkata,
    Indiaã€‚
- en: srdubey@iiita.ac.in, sk.singh@iiita.ac.in, bidyutbaranchaudhuri@gmail.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: srdubey@iiita.ac.in, sk.singh@iiita.ac.in, bidyutbaranchaudhuri@gmail.com
- en: This paper is accepted in Neurocomputing. Copyright will be transferred to Elsevier.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å·²è¢«ã€ŠNeurocomputingã€‹æ¥å—ã€‚ç‰ˆæƒå°†è½¬ç§»è‡³Elsevierã€‚
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'Neural networks have shown tremendous growth in recent years to solve numerous
    problems. Various types of neural networks have been introduced to deal with different
    types of problems. However, the main goal of any neural network is to transform
    the non-linearly separable input data into more linearly separable abstract features
    using a hierarchy of layers. These layers are combinations of linear and nonlinear
    functions. The most popular and common non-linearity layers are activation functions
    (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper,
    a comprehensive overview and survey is presented for AFs in neural networks for
    deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based,
    ReLU based, ELU based, and Learning based are covered. Several characteristics
    of AFs such as output range, monotonicity, and smoothness are also pointed out.
    A performance comparison is also performed among 18 state-of-the-art AFs with
    different networks on different types of data. The insights of AFs are presented
    to benefit the researchers for doing further research and practitioners to select
    among different choices. The code used for experimental comparison is released
    at: [https://github.com/shivram1987/ActivationFunctions](https://github.com/shivram1987/ActivationFunctions).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œåœ¨è¿‘å¹´æ¥æ˜¾ç¤ºå‡ºäº†å·¨å¤§çš„å¢é•¿ï¼Œç”¨äºè§£å†³ä¼—å¤šé—®é¢˜ã€‚å„ç§ç±»å‹çš„ç¥ç»ç½‘ç»œå·²ç»è¢«å¼•å…¥ï¼Œä»¥åº”å¯¹ä¸åŒç±»å‹çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œä»»ä½•ç¥ç»ç½‘ç»œçš„ä¸»è¦ç›®æ ‡æ˜¯ä½¿ç”¨ä¸€ç³»åˆ—å±‚å°†éçº¿æ€§å¯åˆ†çš„è¾“å…¥æ•°æ®è½¬æ¢ä¸ºæ›´çº¿æ€§å¯åˆ†çš„æŠ½è±¡ç‰¹å¾ã€‚è¿™äº›å±‚æ˜¯çº¿æ€§å’Œéçº¿æ€§å‡½æ•°çš„ç»„åˆã€‚æœ€æµè¡Œå’Œå¸¸è§çš„éçº¿æ€§å±‚æ˜¯æ¿€æ´»å‡½æ•°ï¼ˆAFsï¼‰ï¼Œå¦‚Logistic
    Sigmoidã€Tanhã€ReLUã€ELUã€Swishå’ŒMishã€‚æœ¬æ–‡å¯¹ç¥ç»ç½‘ç»œä¸­æ·±åº¦å­¦ä¹ çš„AFsè¿›è¡Œäº†å…¨é¢æ¦‚è¿°å’Œè°ƒæŸ¥ã€‚æ¶µç›–äº†ä¸åŒç±»åˆ«çš„AFsï¼Œå¦‚åŸºäºLogistic
    Sigmoidå’ŒTanhçš„ã€åŸºäºReLUçš„ã€åŸºäºELUçš„å’ŒåŸºäºå­¦ä¹ çš„ã€‚è¿˜æŒ‡å‡ºäº†AFsçš„å‡ ä¸ªç‰¹å¾ï¼Œå¦‚è¾“å‡ºèŒƒå›´ã€å•è°ƒæ€§å’Œå…‰æ»‘æ€§ã€‚è¿˜å¯¹18ç§æœ€å…ˆè¿›çš„AFsåœ¨ä¸åŒç½‘ç»œå’Œä¸åŒç±»å‹çš„æ•°æ®ä¸Šçš„æ€§èƒ½è¿›è¡Œäº†æ¯”è¾ƒã€‚AFsçš„è§è§£æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›è¿›ä¸€æ­¥ç ”ç©¶çš„å‚è€ƒï¼Œå¹¶ä¸ºä»ä¸šäººå‘˜åœ¨ä¸åŒé€‰æ‹©ä¸­è¿›è¡Œé€‰æ‹©æä¾›å¸®åŠ©ã€‚ç”¨äºå®éªŒæ¯”è¾ƒçš„ä»£ç å‘å¸ƒåœ¨ï¼š[https://github.com/shivram1987/ActivationFunctions](https://github.com/shivram1987/ActivationFunctions)ã€‚
- en: '^â€ ^â€ journal: Neurocomputing'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ^â€ ^â€ æœŸåˆŠï¼šNeurocomputing
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: In recent years, deep learning has shown a tremondous growth to solve the challenging
    problems such as object detection [[1](#bib.bib1)], semantic segmentation [[2](#bib.bib2)],
    person re-identification [[3](#bib.bib3)], image retrieval [[4](#bib.bib4)], anomaly
    detection [[5](#bib.bib5)], skin disease diagnosis [[6](#bib.bib6)], and many
    more. Various types of neural networks have been defined in deep learning to learn
    abstract features from data, such as Multilayer Perceptron (MLP) [[7](#bib.bib7)],
    Convolutional Neural Networks (CNN) [[8](#bib.bib8)], Recurrent Neural Networks
    (RNN) [[9](#bib.bib9)], and Generative Adversarial Networks (GAN) [[10](#bib.bib10)].
    The important aspects of neural networks include weight initialization [[11](#bib.bib11)],
    loss functions [[12](#bib.bib12)], different layers [[13](#bib.bib13)], overfitting
    [[14](#bib.bib14)], and optimization [[15](#bib.bib15)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ åœ¨è§£å†³è¯¸å¦‚ç‰©ä½“æ£€æµ‹[[1](#bib.bib1)]ã€è¯­ä¹‰åˆ†å‰²[[2](#bib.bib2)]ã€äººç‰©é‡æ–°è¯†åˆ«[[3](#bib.bib3)]ã€å›¾åƒæ£€ç´¢[[4](#bib.bib4)]ã€å¼‚å¸¸æ£€æµ‹[[5](#bib.bib5)]ã€çš®è‚¤ç–¾ç—…è¯Šæ–­[[6](#bib.bib6)]ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„å¢é•¿ã€‚æ·±åº¦å­¦ä¹ ä¸­å®šä¹‰äº†å„ç§ç±»å‹çš„ç¥ç»ç½‘ç»œï¼Œä»¥ä»æ•°æ®ä¸­å­¦ä¹ æŠ½è±¡ç‰¹å¾ï¼Œä¾‹å¦‚å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰[[7](#bib.bib7)]ã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰[[8](#bib.bib8)]ã€é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰[[9](#bib.bib9)]å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰[[10](#bib.bib10)]ã€‚ç¥ç»ç½‘ç»œçš„é‡è¦æ–¹é¢åŒ…æ‹¬æƒé‡åˆå§‹åŒ–[[11](#bib.bib11)]ã€æŸå¤±å‡½æ•°[[12](#bib.bib12)]ã€ä¸åŒå±‚æ¬¡[[13](#bib.bib13)]ã€è¿‡æ‹Ÿåˆ[[14](#bib.bib14)]å’Œä¼˜åŒ–[[15](#bib.bib15)]ã€‚
- en: 'The activation functions (AFs) play a very crucial role in neural networks
    [[16](#bib.bib16)] by learning the abstract features through non-linear transformations.
    Some common properties of the AFs are as follows: a) it should add the non-linear
    curvature in the optimization landscape to improve the training convergence of
    the network; b) it should not increase the computational complexity of the model
    extensively; c) it should not hamper the gradient flow during training; d) it
    should retain the distribution of data to facilitate the better training of the
    network. Several AFs have been explored in recent years for deep learning to achieve
    the above mentioned properties. This survey is dedicated to the developments in
    the area of AFs in neural networks. The insights of the different AFs are presented
    along with the reasoning to benefit the deep learning community. The major contributions
    of this survey are outlined as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°ï¼ˆAFsï¼‰åœ¨ç¥ç»ç½‘ç»œä¸­æ‰®æ¼”ç€éå¸¸å…³é”®çš„è§’è‰²[[16](#bib.bib16)]ï¼Œé€šè¿‡éçº¿æ€§å˜æ¢å­¦ä¹ æŠ½è±¡ç‰¹å¾ã€‚æ¿€æ´»å‡½æ•°çš„ä¸€äº›å¸¸è§å±æ€§å¦‚ä¸‹ï¼ša) åº”è¯¥åœ¨ä¼˜åŒ–æ™¯è§‚ä¸­æ·»åŠ éçº¿æ€§æ›²ç‡ï¼Œä»¥æé«˜ç½‘ç»œçš„è®­ç»ƒæ”¶æ•›æ€§ï¼›b)
    ä¸åº”å¤§å¹…å¢åŠ æ¨¡å‹çš„è®¡ç®—å¤æ‚æ€§ï¼›c) ä¸åº”é˜»ç¢è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æµï¼›d) åº”è¯¥ä¿ç•™æ•°æ®çš„åˆ†å¸ƒï¼Œä»¥ä¿ƒè¿›ç½‘ç»œçš„æ›´å¥½è®­ç»ƒã€‚è¿‘å¹´æ¥ï¼Œä¸ºäº†å®ç°ä¸Šè¿°å±æ€§ï¼Œå·²ç»æ¢ç´¢äº†å‡ ç§æ¿€æ´»å‡½æ•°ã€‚æœ¬è°ƒæŸ¥ä¸“æ³¨äºç¥ç»ç½‘ç»œä¸­æ¿€æ´»å‡½æ•°é¢†åŸŸçš„å‘å±•ã€‚ä¸åŒæ¿€æ´»å‡½æ•°çš„è§è§£åŠå…¶ç†ç”±è¢«å‘ˆç°å‡ºæ¥ï¼Œä»¥é€ ç¦æ·±åº¦å­¦ä¹ ç¤¾åŒºã€‚æœ¬è°ƒæŸ¥çš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼š
- en: '1.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: This survey provides a detailed classification for a wide range of AFs. It also
    includes the AFs very comprehensively, including Logistic Sigmoid/Tanh, Rectified
    Unit, Exponential Unit, and Adaptive AFs.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœ¬è°ƒæŸ¥æä¾›äº†å¯¹å¹¿æ³›æ¿€æ´»å‡½æ•°çš„è¯¦ç»†åˆ†ç±»ã€‚å®ƒè¿˜éå¸¸å…¨é¢åœ°æ¶µç›–äº†æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬é€»è¾‘æ–¯è’‚å‡½æ•°/åŒæ›²æ­£åˆ‡å‡½æ•°ã€æ•´æµå•å…ƒã€æŒ‡æ•°å•å…ƒå’Œè‡ªé€‚åº”æ¿€æ´»å‡½æ•°ã€‚
- en: '2.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: This survey enriches the reader with the state-of-the-art AFs with analysis
    from various perspectives. It specifically covers the progress in AFs for deep
    learning.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœ¬è°ƒæŸ¥ä»å„ä¸ªè§’åº¦ä¸°å¯Œäº†è¯»è€…å¯¹æœ€å…ˆè¿›æ¿€æ´»å‡½æ•°çš„åˆ†æã€‚å®ƒç‰¹åˆ«æ¶µç›–äº†æ·±åº¦å­¦ä¹ ä¸­æ¿€æ´»å‡½æ•°çš„å‘å±•ã€‚
- en: '3.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'This survey also summarizes the AFs with brief highlights and important discussions
    to depict its suitability for different types of data (Refer to Table [6](#S7.T6
    "Table 6 â€£ 7.5 Kernel Activation Functions â€£ 7 Miscellaneous Activation Functions
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")).'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'æœ¬è°ƒæŸ¥è¿˜æ€»ç»“äº†æ¿€æ´»å‡½æ•°çš„ç®€è¦äº®ç‚¹å’Œé‡è¦è®¨è®ºï¼Œä»¥æç»˜å…¶å¯¹ä¸åŒæ•°æ®ç±»å‹çš„é€‚ç”¨æ€§ï¼ˆå‚è§è¡¨æ ¼ [6](#S7.T6 "Table 6 â€£ 7.5 Kernel
    Activation Functions â€£ 7 Miscellaneous Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark")ï¼‰ã€‚'
- en: '4.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'This survey is compared with the existing survey and performance analysis to
    show its importance (Refer to Table [7](#S9.T7 "Table 7 â€£ 9.1 Comparison with
    Existing Survey/Performance Analysis â€£ 9 Performance Comparison and Analysis â€£
    Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")).'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'æœ¬è°ƒæŸ¥ä¸ç°æœ‰è°ƒæŸ¥å’Œæ€§èƒ½åˆ†æè¿›è¡Œäº†æ¯”è¾ƒï¼Œä»¥æ˜¾ç¤ºå…¶é‡è¦æ€§ï¼ˆå‚è§è¡¨æ ¼ [7](#S9.T7 "Table 7 â€£ 9.1 Comparison with Existing
    Survey/Performance Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark")ï¼‰ã€‚'
- en: '5.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'This paper also presents the performance comparisons on 4 benchmark datasets
    of different modalities using 18 state-of-the-art AFs with different types of
    networks (Refer to Tables [8](#S9.T8 "Table 8 â€£ 9.1 Comparison with Existing Survey/Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"), [9](#S9.T9 "Table 9 â€£ 9.1 Comparison
    with Existing Survey/Performance Analysis â€£ 9 Performance Comparison and Analysis
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")
    and [11](#S9.T11 "Table 11 â€£ 9.2 Experimental Performance Analysis â€£ 9 Performance
    Comparison and Analysis â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")).'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'æœ¬æ–‡è¿˜å±•ç¤ºäº†åœ¨4ä¸ªä¸åŒæ¨¡æ€çš„åŸºå‡†æ•°æ®é›†ä¸Šä½¿ç”¨18ç§æœ€å…ˆè¿›çš„ AFs å’Œä¸åŒç±»å‹ç½‘ç»œçš„æ€§èƒ½æ¯”è¾ƒï¼ˆè¯·å‚è§è¡¨[8](#S9.T8 "Table 8 â€£ 9.1
    Comparison with Existing Survey/Performance Analysis â€£ 9 Performance Comparison
    and Analysis â€£ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark")ã€[9](#S9.T9 "Table 9 â€£ 9.1 Comparison with Existing Survey/Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") å’Œ [11](#S9.T11 "Table 11 â€£ 9.2
    Experimental Performance Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark")ï¼‰ã€‚'
- en: 'The evolution of AFs is illustrated in Section [2](#S2 "2 Evolution of Activation
    Functions â€£ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark"). The progress in Logistic Sigmoid and Tanh, rectified, exponential,
    adaptive and miscellaneous AFs are summarized in Section [3](#S3 "3 Logistic Sigmoid
    and Tanh Based AFs â€£ Activation Functions in Deep Learning: A Comprehensive Survey
    and Benchmark"), [4](#S4 "4 Rectified Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark"), [5](#S5 "5 Exponential
    Activation Functions â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark"), [6](#S6 "6 Learning/Adaptive Activation Functions â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark"), and [7](#S7
    "7 Miscellaneous Activation Functions â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark"), respectively. Some aspects of AFs are
    discussed in Section [8](#S8 "8 Aspects of Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark"). A comprehensive performance
    analysis is conducted in Section [9](#S9 "9 Performance Comparison and Analysis
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark").
    A summary with conclusions and recommendations is provided in Section [10](#S10
    "10 Conclusion and Recommendations â€£ Activation Functions in Deep Learning: A
    Comprehensive Survey and Benchmark").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'AFs çš„æ¼”å˜åœ¨ç¬¬[2](#S2 "2 Evolution of Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark")èŠ‚ä¸­è¿›è¡Œäº†è¯´æ˜ã€‚Logistic Sigmoid
    å’Œ Tanhã€æ•´æµã€æŒ‡æ•°ã€è‡ªé€‚åº”åŠå…¶ä»–å„ç§ AFs çš„è¿›å±•åˆ†åˆ«æ€»ç»“åœ¨ç¬¬[3](#S3 "3 Logistic Sigmoid and Tanh Based
    AFs â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")ã€[4](#S4
    "4 Rectified Activation Functions â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")ã€[5](#S5 "5 Exponential Activation Functions â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark")ã€[6](#S6 "6
    Learning/Adaptive Activation Functions â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark")å’Œ[7](#S7 "7 Miscellaneous Activation Functions
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")èŠ‚ä¸­è®¨è®ºã€‚AFs
    çš„ä¸€äº›æ–¹é¢åœ¨ç¬¬[8](#S8 "8 Aspects of Activation Functions â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark")èŠ‚ä¸­è¿›è¡Œäº†è®¨è®ºã€‚ç¬¬[9](#S9 "9 Performance
    Comparison and Analysis â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")èŠ‚è¿›è¡Œäº†å…¨é¢çš„æ€§èƒ½åˆ†æã€‚ç¬¬[10](#S10 "10 Conclusion and Recommendations
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")èŠ‚æä¾›äº†æ€»ç»“ã€ç»“è®ºå’Œå»ºè®®ã€‚'
- en: 2 Evolution of Activation Functions
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 æ¿€æ´»å‡½æ•°çš„æ¼”å˜
- en: 'A linear function can be thought of as a simple AF which outputs $c\times x$
    for input $x$ with $c$ as a constant. The linear AF is illustrated in Fig. [1](#S2.F1
    "Figure 1 â€£ 2 Evolution of Activation Functions â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") for $c=1$, i.e., identity function.
    Note that the linear AF does not add non-linearity into the network. However,
    the non-linearity needs to be introduced in the neural networks. Otherwise, a
    neural network produces the output as a linear function of inputs inspite of having
    several layers. Moreover, in practice data is generally not linearly separable;
    hence, the non-linear layers help to project the data in non-linear fashion in
    feature space which can be used with different objective functions. This section
    provides an overview of the evolution of AFs for deep learning. A classification
    is presented in Fig. [2](#S2.F2 "Figure 2 â€£ 2 Evolution of Activation Functions
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")
    in terms of the different properties and characteristic types.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 'çº¿æ€§å‡½æ•°å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªç®€å•çš„ AFï¼Œå¯¹äºè¾“å…¥ $x$ è¾“å‡º $c\times x$ï¼Œå…¶ä¸­ $c$ æ˜¯ä¸€ä¸ªå¸¸æ•°ã€‚çº¿æ€§ AF åœ¨å›¾ [1](#S2.F1
    "Figure 1 â€£ 2 Evolution of Activation Functions â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") ä¸­ä¸º $c=1$ï¼ˆå³å•ä½å‡½æ•°ï¼‰è¿›è¡Œäº†è¯´æ˜ã€‚è¯·æ³¨æ„ï¼Œçº¿æ€§ AF
    ä¸ä¼šä¸ºç½‘ç»œå¼•å…¥éçº¿æ€§ã€‚ç„¶è€Œï¼Œç¥ç»ç½‘ç»œéœ€è¦å¼•å…¥éçº¿æ€§ã€‚å¦åˆ™ï¼Œå³ä½¿æœ‰å¤šå±‚ï¼Œç¥ç»ç½‘ç»œä¹Ÿä¼šå°†è¾“å‡ºè§†ä¸ºè¾“å…¥çš„çº¿æ€§å‡½æ•°ã€‚æ­¤å¤–ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ•°æ®é€šå¸¸ä¸æ˜¯çº¿æ€§å¯åˆ†çš„ï¼›å› æ­¤ï¼Œéçº¿æ€§å±‚æœ‰åŠ©äºåœ¨ç‰¹å¾ç©ºé—´ä¸­ä»¥éçº¿æ€§æ–¹å¼æŠ•å½±æ•°æ®ï¼Œè¿™å¯ä»¥ä¸ä¸åŒçš„ç›®æ ‡å‡½æ•°ä¸€èµ·ä½¿ç”¨ã€‚æœ¬èŠ‚æ¦‚è¿°äº†æ·±åº¦å­¦ä¹ ä¸­
    AF çš„æ¼”å˜ã€‚å›¾ [2](#S2.F2 "Figure 2 â€£ 2 Evolution of Activation Functions â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark") å±•ç¤ºäº†ä¸åŒå±æ€§å’Œç‰¹å¾ç±»å‹çš„åˆ†ç±»ã€‚'
- en: '![Refer to caption](img/feda0172686c2a943b0799c0d5215638.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/feda0172686c2a943b0799c0d5215638.png)'
- en: 'Figure 1: An illustration of Linear, Logistic Sigmoid and Tanh AFs.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šçº¿æ€§ã€Logistic Sigmoid å’Œ Tanh AFs çš„ç¤ºæ„å›¾ã€‚
- en: 'Logistic Sigmoid/Tanh Unit Based Activation Functions: In order to introduce
    the non-linearity into the neural networks, the Logistic Sigmoid and Tanh AFs
    have been used in the early days. The firing of bilogical neurons was the motivation
    of using the Logistic Sigmoid and Tanh AFs with artificial neurons. The Logistic
    Sigmoid AF is a very popular and traditional non-linear function. It is given
    as,'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Logistic Sigmoid/Tanh å•ä½åŸºç¡€çš„æ¿€æ´»å‡½æ•°ï¼šä¸ºäº†å°†éçº¿æ€§å¼•å…¥ç¥ç»ç½‘ç»œï¼Œæ—©æœŸä½¿ç”¨äº† Logistic Sigmoid å’Œ Tanh AFã€‚ç”Ÿç‰©ç¥ç»å…ƒçš„æ¿€å‘æ˜¯ä½¿ç”¨
    Logistic Sigmoid å’Œ Tanh AF ä¸äººå·¥ç¥ç»å…ƒçš„åŠ¨æœºã€‚Logistic Sigmoid AF æ˜¯ä¸€ä¸ªéå¸¸æµè¡Œå’Œä¼ ç»Ÿçš„éçº¿æ€§å‡½æ•°ã€‚å…¶å½¢å¼ä¸ºï¼Œ
- en: '|  | $\text{Logistic Sigmoid}(x)=\frac{1}{1+e^{-x}}.$ |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Logistic Sigmoid}(x)=\frac{1}{1+e^{-x}}.$ |  | (1) |'
- en: 'This AF squashes the output between [$0$, $1$] as shown in Fig. [1](#S2.F1
    "Figure 1 â€£ 2 Evolution of Activation Functions â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"). The output of the Logistic Sigmoid
    function is saturated for higher and lower inputs, which leads to vanishing gradient
    problem. The vanishing gradient problem depicts to a scenario where the gradient
    of objective function w.r.t. a parameter becomes very close to zero and leads
    to almost no update in the parameters during the training of the network using
    stochastic gradient descent technique. Hence, the training is almost killed under
    vanishing gradient scenario. Moreover, the output not following a zero-centric
    nature leads to poor convergence. The Tanh function has also been used as the
    AF in neural networks. It is similar to the Logistic Sigmoid function while exhibiting
    the zero centric property as depicted in Fig. [1](#S2.F1 "Figure 1 â€£ 2 Evolution
    of Activation Functions â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark"). The Tanh function is written as,'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿™ä¸ª AF å°†è¾“å‡ºå‹ç¼©åˆ°[$0$, $1$]ä¹‹é—´ï¼Œå¦‚å›¾ [1](#S2.F1 "Figure 1 â€£ 2 Evolution of Activation
    Functions â€£ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark")æ‰€ç¤ºã€‚Logistic Sigmoid å‡½æ•°çš„è¾“å‡ºåœ¨è¾ƒé«˜å’Œè¾ƒä½è¾“å…¥æ—¶ä¼šé¥±å’Œï¼Œè¿™å¯¼è‡´äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æè¿°äº†ä¸€ä¸ªåœºæ™¯ï¼Œå…¶ä¸­ç›®æ ‡å‡½æ•°ç›¸å¯¹äºæŸä¸ªå‚æ•°çš„æ¢¯åº¦å˜å¾—éå¸¸æ¥è¿‘é›¶ï¼Œå¯¼è‡´åœ¨ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æŠ€æœ¯è®­ç»ƒç½‘ç»œæ—¶ï¼Œå‚æ•°å‡ ä¹æ²¡æœ‰æ›´æ–°ã€‚å› æ­¤ï¼Œåœ¨æ¢¯åº¦æ¶ˆå¤±åœºæ™¯ä¸‹ï¼Œè®­ç»ƒå‡ ä¹ä¼šè¢«â€œæ€æ­»â€ã€‚æ­¤å¤–ï¼Œè¾“å‡ºä¸éµå¾ªé›¶ä¸­å¿ƒç‰¹æ€§ä¼šå¯¼è‡´æ”¶æ•›æ€§å·®ã€‚Tanh
    å‡½æ•°ä¹Ÿè¢«ç”¨ä½œç¥ç»ç½‘ç»œä¸­çš„ AFã€‚å®ƒç±»ä¼¼äº Logistic Sigmoid å‡½æ•°ï¼ŒåŒæ—¶å±•ç°äº†é›¶ä¸­å¿ƒç‰¹æ€§ï¼Œå¦‚å›¾ [1](#S2.F1 "Figure 1 â€£
    2 Evolution of Activation Functions â€£ Activation Functions in Deep Learning: A
    Comprehensive Survey and Benchmark")æ‰€ç¤ºã€‚Tanh å‡½æ•°è¡¨ç¤ºä¸ºï¼Œ'
- en: '|  | $\text{Tanh}(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}.$ |  | (2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Tanh}(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}.$ |  | (2) |'
- en: 'The Tanh function also squashes the inputs, but in $[-1,1]$. The drawbacks
    of Logistic Sigmoid function such as vanishing gradient and computational complexity
    also exist with Tanh function. The Logistic Sigmoid and Tanh AFs majorly suffer
    from vanishing gradient. Several improvements have been proposed based on the
    Logistic Sigmoid and Tanh AFs which are described in Section [3](#S3 "3 Logistic
    Sigmoid and Tanh Based AFs â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark") in detail.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'åŒæ›²æ­£åˆ‡å‡½æ•°ä¹Ÿå°†è¾“å…¥å‹ç¼©ï¼Œä½†èŒƒå›´åœ¨$[-1,1]$ã€‚åŒæ›²æ­£åˆ‡å‡½æ•°ä¹Ÿå­˜åœ¨ä¸Logistic Sigmoidå‡½æ•°ç±»ä¼¼çš„ç¼ºç‚¹ï¼Œå¦‚æ¢¯åº¦æ¶ˆå¤±å’Œè®¡ç®—å¤æ‚æ€§ã€‚Logistic
    Sigmoidå’ŒåŒæ›²æ­£åˆ‡AFä¸»è¦é­é‡æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚åŸºäºLogistic Sigmoidå’ŒåŒæ›²æ­£åˆ‡AFçš„å‡ é¡¹æ”¹è¿›å·²åœ¨[3](#S3 "3 Logistic
    Sigmoid and Tanh Based AFs â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")èŠ‚ä¸­è¯¦ç»†æè¿°ã€‚'
- en: '![Refer to caption](img/73af0dc8a5973ff1c8fc86ce1160d453.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/73af0dc8a5973ff1c8fc86ce1160d453.png)'
- en: 'Figure 2: Classification of activation functions.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šæ¿€æ´»å‡½æ•°çš„åˆ†ç±»ã€‚
- en: 'Rectified Linear Unit Based Activation Functions: The saturated output and
    increased complexity are the key limitations of above-mentioned Logistic Sigmoid
    and Tanh based AFs. The Rectified Linear Unit (ReLU) [[17](#bib.bib17)] has become
    the state-of-the-art AF due to its simplicity and improved performance. The ReLU
    was also used in the AlexNet model [[8](#bib.bib8)]. Various variants of ReLU
    have been investigated by tackling its drawbacks, such as non-utilization of negative
    values, limited non-linearity and unbounded output, as detailed in Section [4](#S4
    "4 Rectified Activation Functions â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç»è¿‡ä¿®æ­£çš„çº¿æ€§å•å…ƒï¼ˆReLUï¼‰åŸºç¡€çš„æ¿€æ´»å‡½æ•°ï¼šé¥±å’Œè¾“å‡ºå’Œå¢åŠ çš„å¤æ‚æ€§æ˜¯ä¸Šè¿°Logistic Sigmoidå’ŒåŒæ›²æ­£åˆ‡åŸºç¡€çš„æ¿€æ´»å‡½æ•°çš„ä¸»è¦é™åˆ¶ã€‚ç”±äºå…¶ç®€å•æ€§å’Œæ€§èƒ½æå‡ï¼Œç»è¿‡ä¿®æ­£çš„çº¿æ€§å•å…ƒï¼ˆReLUï¼‰[[17](#bib.bib17)]
    å·²æˆä¸ºæœ€å…ˆè¿›çš„æ¿€æ´»å‡½æ•°ã€‚ReLUä¹Ÿè¢«ç”¨äºAlexNetæ¨¡å‹ [[8](#bib.bib8)]ã€‚å·²é€šè¿‡è§£å†³å…¶ç¼ºç‚¹ï¼ˆå¦‚è´Ÿå€¼æœªåˆ©ç”¨ã€æœ‰é™çš„éçº¿æ€§å’Œæ— ç•Œè¾“å‡ºï¼‰å¯¹ReLUçš„å„ç§å˜ä½“è¿›è¡Œäº†ç ”ç©¶ï¼Œè¯¦ç»†å†…å®¹è§[4](#S4
    "4 Rectified Activation Functions â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")èŠ‚ã€‚'
- en: 'Table 1: Advantage and disadvantage of primary AFs.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨1ï¼šä¸»è¦æ¿€æ´»å‡½æ•°çš„ä¼˜ç¼ºç‚¹ã€‚
- en: '| AFs | Diminishing | Limited | Optimization | Lack of | Computational |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| æ¿€æ´»å‡½æ•° | æ¸å‡ | æœ‰é™ | ä¼˜åŒ– | ç¼ºä¹ | è®¡ç®— |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| gradients | non-linearity | difficulty | adaptibility | inefficiency |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| æ¢¯åº¦ | éçº¿æ€§ | éš¾åº¦ | é€‚åº”æ€§ | ä½æ•ˆ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Sigmoid | Yes | No | Yes | Yes | Yes |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | æ˜¯ | å¦ | æ˜¯ | æ˜¯ | æ˜¯ |'
- en: '| Tanh | Yes | No | Partial | Yes | Yes |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| åŒæ›²æ­£åˆ‡ï¼ˆTanhï¼‰ | æ˜¯ | å¦ | éƒ¨åˆ† | æ˜¯ | æ˜¯ |'
- en: '| ReLU | Partial | Yes | Partial | Yes | No |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ReLU | éƒ¨åˆ† | æ˜¯ | éƒ¨åˆ† | æ˜¯ | å¦ |'
- en: '| ELU | No | Partial | No | Yes | Partial |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ELU | å¦ | éƒ¨åˆ† | å¦ | æ˜¯ | éƒ¨åˆ† |'
- en: '| APL | No | Partial | No | No | No |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| APL | å¦ | éƒ¨åˆ† | å¦ | å¦ | å¦ |'
- en: '| Swish | No | Partial | No | No | Partial |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Swish | å¦ | éƒ¨åˆ† | å¦ | å¦ | éƒ¨åˆ† |'
- en: 'Table 2: Summary of Logistic Sigmoid and Tanh based activation functions.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨2ï¼šLogistic Sigmoidå’ŒåŒæ›²æ­£åˆ‡åŸºç¡€çš„æ¿€æ´»å‡½æ•°æ€»ç»“ã€‚
- en: '| Name of AF | Parametric | Monotonic | Smooth | Bounded |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| æ¿€æ´»å‡½æ•°åç§° | å‚æ•°åŒ– | å•è°ƒ | å…‰æ»‘ | æœ‰ç•Œ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Logistic Sigmoid | No | Yes | Yes | Yes |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Logistic Sigmoid | å¦ | æ˜¯ | æ˜¯ | æ˜¯ |'
- en: '| Tanh | No | Yes | Yes | Yes |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| åŒæ›²æ­£åˆ‡ï¼ˆTanhï¼‰ | å¦ | æ˜¯ | æ˜¯ | æ˜¯ |'
- en: '| Scaled Tanh (sTanh), 1998 [[18](#bib.bib18)] | Yes | Yes | Yes | Yes |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| ç¼©æ”¾çš„åŒæ›²æ­£åˆ‡ï¼ˆsTanhï¼‰ï¼Œ1998 [[18](#bib.bib18)] | æ˜¯ | æ˜¯ | æ˜¯ | æ˜¯ |'
- en: '| Rectified Hyperbolic Secant (ReSech), 2016 [[19](#bib.bib19)] | No | No |
    Yes | Yes |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| ç»è¿‡ä¿®æ­£çš„åŒæ›²æ­£å‰²ï¼ˆReSechï¼‰ï¼Œ2016 [[19](#bib.bib19)] | å¦ | å¦ | æ˜¯ | æ˜¯ |'
- en: '| Scaled Sigmoid (sSigmoid), 2016 [[20](#bib.bib20)] | No | Yes | Yes | Yes
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ç¼©æ”¾Sigmoidï¼ˆsSigmoidï¼‰ï¼Œ2016 [[20](#bib.bib20)] | å¦ | æ˜¯ | æ˜¯ | æ˜¯ |'
- en: '| Penalized Tanh (pTanh), 2016 [[20](#bib.bib20)] | No | Yes | No | Yes |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ç½šé¡¹åŒæ›²æ­£åˆ‡ï¼ˆpTanhï¼‰ï¼Œ2016 [[20](#bib.bib20)] | å¦ | æ˜¯ | å¦ | æ˜¯ |'
- en: '| Hexpo, 2017 [[21](#bib.bib21)] | No | Yes | Yes | Yes |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Hexpoï¼Œ2017 [[21](#bib.bib21)] | å¦ | æ˜¯ | æ˜¯ | æ˜¯ |'
- en: '| Improved Sigmoid (ISigmoid), 2018 [[22](#bib.bib22)] | No | Yes | Yes | No
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| æ”¹è¿›çš„Sigmoidï¼ˆISigmoidï¼‰ï¼Œ2018 [[22](#bib.bib22)] | å¦ | æ˜¯ | æ˜¯ | å¦ |'
- en: '| Sigmoid-Weighted Linear Units (SiLU), 2018 [[23](#bib.bib23)] | No | No |
    Yes | For negative inputs |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| SigmoidåŠ æƒçº¿æ€§å•å…ƒï¼ˆSiLUï¼‰ï¼Œ2018 [[23](#bib.bib23)] | å¦ | å¦ | æ˜¯ | å¯¹äºè´Ÿè¾“å…¥ |'
- en: '| Linearly Scaled Hyperbolic Tangent (LiSHT), 2019 [[24](#bib.bib24)] | No
    | No | Yes | No |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| çº¿æ€§ç¼©æ”¾åŒæ›²æ­£åˆ‡ï¼ˆLiSHTï¼‰ï¼Œ2019 [[24](#bib.bib24)] | å¦ | å¦ | æ˜¯ | å¦ |'
- en: '| Elliott, 2019 [[25](#bib.bib25)] | No | Yes | Yes | Yes |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Elliottï¼Œ2019 [[25](#bib.bib25)] | å¦ | æ˜¯ | æ˜¯ | æ˜¯ |'
- en: '| Soft-Root-Sign (SRS), 2020 [[26](#bib.bib26)] | Yes | No | Yes | Yes |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Soft-Root-Sign (SRS)ï¼Œ2020 [[26](#bib.bib26)] | æ˜¯ | å¦ | æ˜¯ | æ˜¯ |'
- en: 'Exponential Unit Based Activation Functions: The major problem faced by the
    Logistic Sigmoid and Tanh based AFs is with its saturated output for large positive
    and negative input. Similarly, the major problem with ReLU based AFs is with the
    under-utilization of negative values leading to vanishing gradient. In order to
    cope up with these limitations the exponential function based AFs have been used
    in the literature. The Exponential Linear Unit (ELU) [[27](#bib.bib27)] based
    AF utilizes the negative values with the help of the exponential function. Several
    AFs have been introduced in the literature as the ELU variants which are presented
    in Section [5](#S5 "5 Exponential Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark") in detail.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'æŒ‡æ•°å•ä½åŸºæ¿€æ´»å‡½æ•°ï¼šLogistic Sigmoid å’Œ Tanh åŸºäº AF é¢ä¸´çš„ä¸»è¦é—®é¢˜æ˜¯å¯¹äºå¤§æ­£å€¼å’Œè´Ÿå€¼è¾“å…¥çš„é¥±å’Œè¾“å‡ºã€‚ç±»ä¼¼åœ°ï¼ŒReLU åŸºäº
    AF çš„ä¸»è¦é—®é¢˜æ˜¯è´Ÿå€¼çš„åˆ©ç”¨ä¸è¶³ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚ä¸ºäº†åº”å¯¹è¿™äº›é™åˆ¶ï¼Œæ–‡çŒ®ä¸­ä½¿ç”¨äº†åŸºäºæŒ‡æ•°å‡½æ•°çš„ AFã€‚æŒ‡æ•°çº¿æ€§å•å…ƒï¼ˆELUï¼‰[[27](#bib.bib27)]
    åŸºäº AF åˆ©ç”¨æŒ‡æ•°å‡½æ•°æ¥å¤„ç†è´Ÿå€¼ã€‚æ–‡çŒ®ä¸­ä»‹ç»äº†å‡ ç§ ELU å˜ä½“ï¼Œå¦‚ç¬¬[5](#S5 "5 Exponential Activation Functions
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")èŠ‚ä¸­è¯¦ç»†ä»‹ç»ã€‚'
- en: 'Learning/Adaptive Activation Functions: Most of the Sigmoid, Tanh, ReLU, and
    ELU based AFs are designed manually which might not be able to exploit the data
    complexity. The learning based adaptive AFs are the recent trends. This class
    of AFs contains learnable parameters, e.g. Adaptive Piecewise Linear (APL) [[28](#bib.bib28)]
    and Swish [[29](#bib.bib29)] AFs contain two and one learnable parameters, respectively.
    Recently, several learning based AFs have been proposed as illustrated in Section
    [6](#S6 "6 Learning/Adaptive Activation Functions â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 'å­¦ä¹ /è‡ªé€‚åº”æ¿€æ´»å‡½æ•°ï¼šå¤§å¤šæ•°åŸºäº Sigmoidã€Tanhã€ReLU å’Œ ELU çš„ AF æ˜¯æ‰‹åŠ¨è®¾è®¡çš„ï¼Œå¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨æ•°æ®çš„å¤æ‚æ€§ã€‚åŸºäºå­¦ä¹ çš„è‡ªé€‚åº”
    AF æ˜¯è¿‘æœŸçš„è¶‹åŠ¿ã€‚è¿™ç±» AF åŒ…å«å¯å­¦ä¹ çš„å‚æ•°ï¼Œä¾‹å¦‚è‡ªé€‚åº”åˆ†æ®µçº¿æ€§ï¼ˆAPLï¼‰[[28](#bib.bib28)] å’Œ Swish [[29](#bib.bib29)]
    AF åˆ†åˆ«åŒ…å«ä¸¤ä¸ªå’Œä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚æœ€è¿‘ï¼Œå·²ç»æå‡ºäº†å‡ ç§åŸºäºå­¦ä¹ çš„ AFï¼Œå¦‚ç¬¬[6](#S6 "6 Learning/Adaptive Activation
    Functions â€£ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark")èŠ‚æ‰€ç¤ºã€‚'
- en: 'Miscellaneous Activation Functions: In recent years, many other AFs have also
    been investigated as presented in Section [7](#S7 "7 Miscellaneous Activation
    Functions â€£ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark"). These activations include Softplus units, probabilistic functions,
    polynomial functions, and kernel functions.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ‚é¡¹æ¿€æ´»å‡½æ•°ï¼šè¿‘å¹´æ¥ï¼Œè®¸å¤šå…¶ä»– AF ä¹Ÿè¢«ç ”ç©¶ï¼Œå¦‚ç¬¬[7](#S7 "7 Miscellaneous Activation Functions â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark")èŠ‚æ‰€è¿°ã€‚è¿™äº›æ¿€æ´»å‡½æ•°åŒ…æ‹¬
    Softplus å•å…ƒã€æ¦‚ç‡å‡½æ•°ã€å¤šé¡¹å¼å‡½æ•°å’Œæ ¸å‡½æ•°ã€‚'
- en: 'Table [1](#S2.T1 "Table 1 â€£ 2 Evolution of Activation Functions â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark") highlights
    the advantage and disadvantage of the primary AFs in terms of the diminishing
    gradients, limited non-linearity, optimization difficulty, computational inefficiency
    and lack of adaptibility. It can be noticed that the Tanh function is computationally
    inefficient because it involves the computation of exponential multiple times
    [[30](#bib.bib30)]. However, in implementation it can be computed using single
    exponential with the help of Sigmoid function. These limitations in the existing
    AFs have been the driving factors for the development of recent AFs as surveyed
    in the further sections of this paper.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨[1](#S2.T1 "Table 1 â€£ 2 Evolution of Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark")çªå‡ºäº†ä¸»è¦ AF åœ¨æ¢¯åº¦æ¶ˆå¤±ã€éçº¿æ€§é™åˆ¶ã€ä¼˜åŒ–éš¾åº¦ã€è®¡ç®—ä½æ•ˆå’Œé€‚åº”æ€§ä¸è¶³æ–¹é¢çš„ä¼˜ç¼ºç‚¹ã€‚å¯ä»¥æ³¨æ„åˆ°ï¼ŒTanh
    å‡½æ•°åœ¨è®¡ç®—ä¸Šæ•ˆç‡ä½ï¼Œå› ä¸ºå®ƒæ¶‰åŠå¤šæ¬¡è®¡ç®—æŒ‡æ•°[[30](#bib.bib30)]ã€‚ç„¶è€Œï¼Œåœ¨å®ç°ä¸­å¯ä»¥é€šè¿‡ Sigmoid å‡½æ•°çš„å¸®åŠ©æ¥ä½¿ç”¨å•æ¬¡æŒ‡æ•°è®¡ç®—ã€‚è¿™äº›ç°æœ‰
    AF çš„é™åˆ¶æ˜¯é©±åŠ¨è¿‘æœŸ AF å‘å±•çš„å› ç´ ï¼Œå¦‚æœ¬æ–‡è¿›ä¸€æ­¥ç« èŠ‚æ‰€è¿°ã€‚'
- en: 3 Logistic Sigmoid and Tanh Based AFs
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 Logistic Sigmoid å’Œ Tanh åŸºäº AF
- en: 'The traditional AFs such as Logistic Sigmoid and Tanh were used very extensively
    in the early days of neural networks. However, these AFs had shown the hurdle
    to train the deep networks due to their saturated output. Several attempts have
    also been made to improve these AFs for different networks. Table [2](#S2.T2 "Table
    2 â€£ 2 Evolution of Activation Functions â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") presents the comparison of Logistic Sigmoid
    and Tanh based AFs in terms of their properties including parametric, monotonic,
    smooth and bounded.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¼ ç»Ÿçš„AFsï¼Œå¦‚Logistic Sigmoidå’ŒTanhï¼Œåœ¨ç¥ç»ç½‘ç»œçš„æ—©æœŸè¢«å¹¿æ³›ä½¿ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›AFsç”±äºé¥±å’Œè¾“å‡ºè€Œä½¿å¾—æ·±åº¦ç½‘ç»œçš„è®­ç»ƒå—é˜»ã€‚ä¸ºäº†æ”¹è¿›è¿™äº›AFsï¼Œä¹Ÿè¿›è¡Œäº†å¤šæ¬¡å°è¯•ã€‚è¡¨[2](#S2.T2
    "Table 2 â€£ 2 Evolution of Activation Functions â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark")å¯¹Logistic Sigmoidå’ŒTanhåŸºç¡€AFsçš„å±æ€§è¿›è¡Œäº†æ¯”è¾ƒï¼ŒåŒ…æ‹¬å‚æ•°åŒ–ã€å•è°ƒã€å¹³æ»‘å’Œæœ‰ç•Œã€‚'
- en: In order to tackle the limited output range and zero gradient problems of Tanh,
    a scaled Hyperbolic Tangent (sTanh) is used in [[18](#bib.bib18)] which is defined
    as,
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³Tanhçš„è¾“å‡ºèŒƒå›´æœ‰é™å’Œé›¶æ¢¯åº¦çš„é—®é¢˜ï¼Œä½¿ç”¨äº†scaled Hyperbolic Tangent (sTanh)[[18](#bib.bib18)]ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼Œ
- en: '|  | $sTanh(x)=A\times Tanh(B\times x)$ |  | (3) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $sTanh(x)=A\times Tanh(B\times x)$ |  | (3) |'
- en: with the output range in $[-A,A]$. A Parametric Sigmoid Function (PSF) is proposed
    as a continuous, differentiable, and bounded function as,
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-A,A]$ã€‚æå‡ºäº†Parametric Sigmoid Function (PSF)ä½œä¸ºä¸€ä¸ªè¿ç»­ã€å¯å¾®åˆ†ã€æœ‰ç•Œå‡½æ•°ï¼Œå¦‚ä¸‹ï¼Œ
- en: '|  | $PSF(x)=\frac{1}{(1+e^{-x})^{m}}$ |  | (4) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $PSF(x)=\frac{1}{(1+e^{-x})^{m}}$ |  | (4) |'
- en: where $m$ is a hyperparameter [[31](#bib.bib31)]. The gradient flow is improved
    for the higher value of $m$. The sum of shifted log-sigmoid is also explored as
    an AF [[32](#bib.bib32)] which retains the symmetry in the generated features.
    The Rectified Hyperbolic Secant (ReSech) AF is differentiable, symmetric, and
    bounded [[19](#bib.bib19)] which is given as,
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$m$æ˜¯ä¸€ä¸ªè¶…å‚æ•°[[31](#bib.bib31)]ã€‚å½“$m$å–è¾ƒé«˜å€¼æ—¶ï¼Œæ¢¯åº¦æµä¼šå¾—åˆ°æ”¹å–„ã€‚ä½œä¸ºAFçš„shifted log-sigmoidçš„å’Œä¹Ÿè¢«æ¢ç´¢ï¼Œä¿æŒäº†ç”Ÿæˆç‰¹å¾çš„å¯¹ç§°æ€§[[32](#bib.bib32)]ã€‚Rectified
    Hyperbolic Secant (ReSech) AFæ˜¯å¯å¾®åˆ†çš„ã€å¯¹ç§°çš„ï¼Œå¹¶ä¸”æœ‰ç•Œ[[19](#bib.bib19)]ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼Œ
- en: '|  | $ReSech(x)=x\times Sech(x)$ |  | (5) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $ReSech(x)=x\times Sech(x)$ |  | (5) |'
- en: with the output range in $[-1,1]$. However, it exhibits the vanishing gradient
    problem due to saturating behavior for both large positive and large negative
    inputs. The training of deep networks become difficult due to the uniform slope
    of the Logistic Sigmoid and Tanh AFs near the origin [[20](#bib.bib20)]. To minimize
    this limitation, the Scaled Sigmoid (sSigmoid) is defined as,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-1,1]$ã€‚ç„¶è€Œï¼Œç”±äºLogistic Sigmoidå’ŒTanhåœ¨å¤§æ­£è´Ÿè¾“å…¥æ—¶è¡¨ç°å‡ºé¥±å’Œè¡Œä¸ºï¼Œå¯¼è‡´äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ç”±äºè¿™ä¸€ç‰¹æ€§ï¼Œè®­ç»ƒæ·±åº¦ç½‘ç»œå˜å¾—å›°éš¾[[20](#bib.bib20)]ã€‚ä¸ºäº†å‡è½»è¿™ä¸€é™åˆ¶ï¼Œå®šä¹‰äº†Scaled
    Sigmoid (sSigmoid)ï¼Œå¦‚ä¸‹ï¼Œ
- en: '|  | $sSigmoid(x)=(4\times Sigmoid(x)-2)$ |  | (6) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $sSigmoid(x)=(4\times Sigmoid(x)-2)$ |  | (6) |'
- en: with the output range in $[-2,2]$ and the Penalized Tanh (pTanh) is defined
    as,
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-2,2]$ï¼ŒPenalized Tanh (pTanh)å®šä¹‰å¦‚ä¸‹ï¼Œ
- en: '|  | $pTanh(x)=\begin{cases}Tanh(x),&amp;x\geq 0\\ a\times Tanh(x),&amp;x<0\end{cases}$
    |  | (7) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $pTanh(x)=\begin{cases}Tanh(x),&x\geq 0\\ a\times Tanh(x),&x<0\end{cases}$
    |  | (7) |'
- en: with the output range in $[-a,1]$ where $a\in(0,1)$. However, sSigmoid and pTanh
    AFs also suffer from the vanishing gradient problem. It is noticed that the pTanh
    AF performs better for Natural Language Processing (NLP) tasks [[33](#bib.bib33)].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-a,1]$ï¼Œå…¶ä¸­$a\in(0,1)$ã€‚ç„¶è€Œï¼ŒSigmoidå’ŒpTanh AFsä¹Ÿå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚æ³¨æ„åˆ°pTanh AFåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½[[33](#bib.bib33)]ã€‚
- en: 'Table 3: Summary of Rectified Linear Unit based activation functions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨3ï¼šåŸºäºä¿®æ­£çº¿æ€§å•å…ƒçš„æ¿€æ´»å‡½æ•°æ€»ç»“ã€‚
- en: '| Name | Parametric | Monotonic | Smooth | Bounded |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| åç§° | Parametric | å•è°ƒ | å¹³æ»‘ | æœ‰ç•Œ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Rectified Linear Unit (ReLU), 2010 [[17](#bib.bib17)] | No | Yes | No | For
    negative inputs |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Rectified Linear Unit (ReLU), 2010 [[17](#bib.bib17)] | å¦ | æ˜¯ | å¦ | å¯¹äºè´Ÿè¾“å…¥
    |'
- en: '| Leaky ReLU (LReLU), 2013 [[34](#bib.bib34)] | No | Yes | No | No |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Leaky ReLU (LReLU), 2013 [[34](#bib.bib34)] | å¦ | æ˜¯ | å¦ | å¦ |'
- en: '| Parametric ReLU (PReLU), 2015 [[35](#bib.bib35)] | Yes | Yes | No | No |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Parametric ReLU (PReLU), 2015 [[35](#bib.bib35)] | æ˜¯ | æ˜¯ | å¦ | å¦ |'
- en: '| Randomized ReLU (RReLU), 2015 [[35](#bib.bib35)] | No | Yes | No | No |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Randomized ReLU (RReLU), 2015 [[35](#bib.bib35)] | å¦ | æ˜¯ | å¦ | å¦ |'
- en: '| Concatenated ReLU (CReLU), 2016 [[36](#bib.bib36)] | No | Yes | No | For
    negative inputs |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Concatenated ReLU (CReLU), 2016 [[36](#bib.bib36)] | å¦ | æ˜¯ | å¦ | å¯¹äºè´Ÿè¾“å…¥ |'
- en: '| Bounded ReLU (BReLU), 2016 [[37](#bib.bib37)] | No | Yes | No | Yes |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Bounded ReLU (BReLU), 2016 [[37](#bib.bib37)] | å¦ | æ˜¯ | å¦ | æ˜¯ |'
- en: '| Parametric Tanh Linear Unit (PTELU), 2017 [[38](#bib.bib38)] | Yes | Yes
    | Yes | For negative inputs |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| å‚æ•°åŒ–Tanhçº¿æ€§å•å…ƒï¼ˆPTELUï¼‰ï¼Œ2017 [[38](#bib.bib38)] | æ˜¯ | æ˜¯ | æ˜¯ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Flexible ReLU (FReLU), 2018 [[39](#bib.bib39)] | Yes | Yes | No | For negative
    inputs |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| çµæ´»ReLUï¼ˆFReLUï¼‰ï¼Œ2018 [[39](#bib.bib39)] | æ˜¯ | æ˜¯ | å¦ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Elastic ReLU (EReLU), 2018 [[40](#bib.bib40)] | No | Yes | No | For negative
    inputs |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| å¼¹æ€§ReLUï¼ˆEReLUï¼‰ï¼Œ2018 [[40](#bib.bib40)] | å¦ | æ˜¯ | å¦ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Randomly Translational ReLU (RTReLU), 2018 [[41](#bib.bib41)] | No | Yes
    | No | For negative inputs |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| éšæœºå¹³ç§»ReLUï¼ˆRTReLUï¼‰ï¼Œ2018 [[41](#bib.bib41)] | å¦ | æ˜¯ | å¦ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Dual ReLU (DualReLU), 2018 [[42](#bib.bib42)] | No | Yes | No | No |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| åŒé‡ReLUï¼ˆDualReLUï¼‰ï¼Œ2018 [[42](#bib.bib42)] | å¦ | æ˜¯ | å¦ | å¦ |'
- en: '| Paired ReLU (PairedReLU), 2018 [[43](#bib.bib43)] | Yes | Yes | No | No |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| é…å¯¹ReLUï¼ˆPairedReLUï¼‰ï¼Œ2018 [[43](#bib.bib43)] | æ˜¯ | æ˜¯ | å¦ | å¦ |'
- en: '| Average Biased ReLU (ABReLU), 2018 [[44](#bib.bib44)] | No | Yes | No | For
    negative inputs |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| å¹³å‡åç½®ReLUï¼ˆABReLUï¼‰ï¼Œ2018 [[44](#bib.bib44)] | å¦ | æ˜¯ | å¦ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Natural-Logarithm (NLReLU), 2019 [[45](#bib.bib45)] | No | Yes | No | For
    negative inputs |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| è‡ªç„¶å¯¹æ•°ï¼ˆNLReLUï¼‰ï¼Œ2019 [[45](#bib.bib45)] | å¦ | æ˜¯ | å¦ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Multi-bin Trainable Linear Units (MTLU), 2019 [[46](#bib.bib46)] | Yes |
    No | No | No |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| å¤šåˆ†ç®±å¯è®­ç»ƒçº¿æ€§å•å…ƒï¼ˆMTLUï¼‰ï¼Œ2019 [[46](#bib.bib46)] | æ˜¯ | å¦ | å¦ | å¦ |'
- en: '| Lipschitz ReLU (L-ReLU), 2020 [[47](#bib.bib47)] | Yes | Depends upon $\phi$
    and $\eta$ | Depends upon $\phi$ and $\eta$ | Depends upon $\phi$ and $\eta$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Lipschitz ReLUï¼ˆL-ReLUï¼‰ï¼Œ2020 [[47](#bib.bib47)] | æ˜¯ | å–å†³äº $\phi$ å’Œ $\eta$
    | å–å†³äº $\phi$ å’Œ $\eta$ | å–å†³äº $\phi$ å’Œ $\eta$ |'
- en: A noisy AF is defined to overcome the vanishing gradient problem [[48](#bib.bib48)].
    Due to the added noise the gradients may flow easily even in the saturating regime.
    The vanishing gradient problem is minimized by the Hexpo function [[21](#bib.bib21)]
    which is similar to Tanh with a scaled gradient. It is given as,
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å«å™ªå£°çš„æ¿€æ´»å‡½æ•°è¢«å®šä¹‰ä¸ºå…‹æœæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ [[48](#bib.bib48)]ã€‚ç”±äºåŠ å…¥äº†å™ªå£°ï¼Œå³ä½¿åœ¨é¥±å’ŒåŒºåŸŸï¼Œæ¢¯åº¦ä¹Ÿèƒ½é¡ºåˆ©æµåŠ¨ã€‚æ¢¯åº¦æ¶ˆå¤±é—®é¢˜é€šè¿‡Hexpoå‡½æ•°
    [[21](#bib.bib21)] å¾—åˆ°æœ€å°åŒ–ï¼Œè¯¥å‡½æ•°ç±»ä¼¼äºTanhï¼Œä½†å…·æœ‰ç¼©æ”¾çš„æ¢¯åº¦ã€‚å…¶å®šä¹‰ä¸ºï¼Œ
- en: '|  | $Hexpo(x)=\begin{cases}-a\times(e^{-x/b}-1),&amp;x\geq 0\\ c\times(e^{x/d}-1),&amp;x<0\end{cases}$
    |  | (8) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $Hexpo(x)=\begin{cases}-a\times(e^{-x/b}-1),&amp;x\geq 0\\ c\times(e^{x/d}-1),&amp;x<0\end{cases}$
    |  | (8) |'
- en: in the output range of $[-c,a]$. The output of the sigmoid function is multiplied
    with its input in sigmoid-weighted linear unit (SiLU) AF [[23](#bib.bib23)] as
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ $[-c,a]$ å†…ã€‚Sigmoidå‡½æ•°çš„è¾“å‡ºä¸å…¶è¾“å…¥åœ¨sigmoidåŠ æƒçº¿æ€§å•å…ƒï¼ˆSiLUï¼‰æ¿€æ´»å‡½æ•° [[23](#bib.bib23)]
    ä¸­ç›¸ä¹˜ï¼Œå¦‚ä¸‹æ‰€ç¤º
- en: '|  | $SiLU(x)=x\times Sigmoid(x)$ |  | (9) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $SiLU(x)=x\times Sigmoid(x)$ |  | (9) |'
- en: in the output range of $(-0.5,\infty)$. At the same time an improved logistic
    Sigmoid (ISigmoid) AF [[22](#bib.bib22)] is proposed to solve the vanishing gradient
    problem of Sigmoid with the help of a piecewise combination of sigmoidal and linear
    functions. It is defined as,
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ $(-0.5,\infty)$ å†…ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„é€»è¾‘Sigmoidï¼ˆISigmoidï¼‰æ¿€æ´»å‡½æ•° [[22](#bib.bib22)]ï¼Œæ—¨åœ¨é€šè¿‡å°†Sigmoidä¸åˆ†æ®µçš„sigmoidalå’Œçº¿æ€§å‡½æ•°ç»“åˆæ¥è§£å†³Sigmoidçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚å…¶å®šä¹‰ä¸ºï¼Œ
- en: '|  | <math   alttext="ISigmoid(x)=\begin{cases}\alpha\times(x-a)+Sigmoid(a),&amp;x\geq
    a\\ Sigmoid(x),&amp;-a<x<a\\'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="ISigmoid(x)=\begin{cases}\alpha\times(x-a)+Sigmoid(a),&amp;x\geq
    a\\ Sigmoid(x),&amp;-a<x<a\\'
- en: \alpha\times(x+a)+Sigmoid(a),&amp;x\leq-a\end{cases}" display="block"><semantics
    ><mrow  ><mrow ><mi  >I</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >S</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >i</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi  >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >m</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi
    >o</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi  >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >d</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >x</mi><mo stretchy="false" >)</mo></mrow></mrow><mo  >=</mo><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow
    ><mrow ><mrow  ><mi >Î±</mi><mo lspace="0.222em" rspace="0.222em"  >Ã—</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >x</mi><mo >âˆ’</mo><mi >a</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >S</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >a</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mi >x</mi><mo  >â‰¥</mo><mi >a</mi></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow  ><mi >S</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >m</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >d</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mo stretchy="false" >(</mo><mi >x</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow ><mo  >âˆ’</mo><mi >a</mi></mrow><mo
    ><</mo><mi >x</mi><mo  ><</mo><mi >a</mi></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mrow  ><mrow ><mi >Î±</mi><mo lspace="0.222em" rspace="0.222em" >Ã—</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >x</mi><mo >+</mo><mi >a</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >S</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >a</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mi >x</mi><mo  >â‰¤</mo><mrow ><mo >âˆ’</mo><mi  >a</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >ğ¼</ci><ci  >ğ‘†</ci><ci >ğ‘–</ci><ci
    >ğ‘”</ci><ci  >ğ‘š</ci><ci >ğ‘œ</ci><ci >ğ‘–</ci><ci  >ğ‘‘</ci><ci >ğ‘¥</ci></apply><apply
    ><csymbol cd="latexml"  >cases</csymbol><apply ><apply ><ci >ğ›¼</ci><apply ><ci
    >ğ‘¥</ci><ci >ğ‘</ci></apply></apply><apply ><ci >ğ‘†</ci><ci >ğ‘–</ci><ci >ğ‘”</ci><ci
    >ğ‘š</ci><ci >ğ‘œ</ci><ci >ğ‘–</ci><ci >ğ‘‘</ci><ci >ğ‘</ci></apply></apply><apply ><ci  >ğ‘¥</ci><ci
    >ğ‘</ci></apply><apply ><ci >ğ‘†</ci><ci >ğ‘–</ci><ci >ğ‘”</ci><ci >ğ‘š</ci><ci >ğ‘œ</ci><ci
    >ğ‘–</ci><ci >ğ‘‘</ci><ci >ğ‘¥</ci></apply><apply ><apply  ><apply ><ci >ğ‘</ci></apply><ci
    >ğ‘¥</ci></apply><apply ><ci  >ğ‘</ci></apply></apply><apply ><apply ><ci >ğ›¼</ci><apply
    ><ci >ğ‘¥</ci><ci >ğ‘</ci></apply></apply><apply ><ci >ğ‘†</ci><ci >ğ‘–</ci><ci >ğ‘”</ci><ci
    >ğ‘š</ci><ci >ğ‘œ</ci><ci >ğ‘–</ci><ci >ğ‘‘</ci><ci >ğ‘</ci></apply></apply><apply ><ci  >ğ‘¥</ci><apply
    ><ci >ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >ISigmoid(x)=\begin{cases}\alpha\times(x-a)+Sigmoid(a),&x\geq a\\ Sigmoid(x),&-a<x<a\\
    \alpha\times(x+a)+Sigmoid(a),&x\leq-a\end{cases}</annotation></semantics></math>
    |  | (10) |
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: \(\alpha\times(x+a)+\text{Sigmoid}(a),\text{å½“}\ x\leq-a\end{cases}\)" display="block"><semantics
    ><mrow  ><mrow ><mi  >I</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >S</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >i</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi  >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >m</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi
    >o</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi  >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >d</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >x</mi><mo stretchy="false" >)</mo></mrow></mrow><mo  >=</mo><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow
    ><mrow ><mrow  ><mi >Î±</mi><mo lspace="0.222em" rspace="0.222em"  >Ã—</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >x</mi><mo >âˆ’</mo><mi >a</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >S</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >a</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mi >x</mi><mo  >â‰¥</mo><mi >a</mi></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow  ><mi >S</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >m</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >d</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mo stretchy="false" >(</mo><mi >x</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow ><mo  >âˆ’</mo><mi >a</mi></mrow><mo
    ><</mo><mi >x</mi><mo  ><</mo><mi >a</mi></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mrow  ><mrow ><mi >Î±</mi><mo lspace="0.222em" rspace="0.222em" >Ã—</mo><mrow
    ><mo stretchy="false"  >(</mo><mrow ><mi >x</mi><mo >+</mo><mi >a</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><mi >S</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >o</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >a</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mi >x</mi><mo  >â‰¤</mo><mrow ><mo >âˆ’</mo><mi  >a</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >ğ¼</ci><ci  >ğ‘†</ci><ci >ğ‘–</ci><ci
    >ğ‘”</ci><ci  >ğ‘š</ci><ci >ğ‘œ</ci><ci >ğ‘–</ci><ci  >ğ‘‘</ci><ci >ğ‘¥</ci></apply><apply
    ><csymbol cd="latexml"  >cases</csymbol><apply ><apply ><ci >ğ›¼</ci><apply ><ci
    >ğ‘¥</ci><ci >ğ‘</ci></apply></apply><apply ><ci >ğ‘†</ci><ci >ğ‘–</ci><ci >ğ‘”</ci><ci
    >ğ‘š</ci><ci >ğ‘œ</ci><ci >ğ‘–</ci><ci >ğ‘‘</ci><ci >ğ‘</ci></apply></apply><apply ><ci  >ğ‘¥</ci><ci
    >ğ‘</ci></apply><apply ><ci >ğ‘†</ci><ci >ğ‘–</ci><ci >ğ‘”</ci><ci >ğ‘š</ci><ci >ğ‘œ</ci><ci
    >ğ‘–</ci><ci >ğ‘‘</ci><ci >ğ‘¥</ci></apply><apply ><apply  ><apply ><ci >ğ‘</ci></apply><ci
    >ğ‘¥</ci></apply><apply ><ci  >ğ‘</ci></apply></apply><apply ><apply ><ci >ğ›¼</ci><apply
    ><ci >ğ‘¥</ci><ci >ğ‘</ci></apply></apply><apply ><ci >ğ‘†</ci><ci >ğ‘–</ci><ci >ğ‘”</ci><ci
    >ğ‘š</ci><ci >ğ‘œ</ci><ci >ğ‘–</ci><ci >ğ‘‘</ci><ci >ğ‘</ci></apply></apply><apply ><ci  >ğ‘¥</ci><apply
    ><ci >ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >ISigmoid(x)=\begin{cases}\alpha\times(x-a)+\text{Sigmoid}(a),&x\geq a\\ \text{Sigmoid}(x),&-a<x<a\\
    \alpha\times(x+a)+\text{Sigmoid}(a),&x\leq-a\end{cases}</annotation></semantics></math>
    |  | (10) |
- en: in the output range of $(-\infty,\infty)$. The Linearly scaled hyperbolic tangent
    (LiSHT) AF scales the Tanh in a linear fashion to overcome the vanishing gradient
    issue [[24](#bib.bib24)]. The LiSHT can be defined as,
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ä¸º $(-\infty,\infty)$ã€‚çº¿æ€§ç¼©æ”¾åŒæ›²æ­£åˆ‡ (LiSHT) AF ä»¥çº¿æ€§æ–¹å¼ç¼©æ”¾ Tanh ä»¥å…‹æœæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ [[24](#bib.bib24)]ã€‚LiSHT
    å¯ä»¥å®šä¹‰ä¸ºï¼Œ
- en: '|  | $LiSHT(x)=x\times Tanh(x)$ |  | (11) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $LiSHT(x)=x\times Tanh(x)$ |  | (11) |'
- en: in the output range of $[0,\infty)$. The LiSHT function is symmetric, but is
    has the shortcoming of including unbounded and non-negative outputs only. The
    Elliott AF [[25](#bib.bib25)] is similar to Sigmoid function in terms of the characteristics
    diagram and defined as,
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ä¸º $[0,\infty)$ã€‚LiSHT å‡½æ•°æ˜¯å¯¹ç§°çš„ï¼Œä½†å®ƒçš„ç¼ºç‚¹æ˜¯ä»…åŒ…å«æ— ç•Œå’Œéè´Ÿè¾“å‡ºã€‚Elliott AF [[25](#bib.bib25)]
    åœ¨ç‰¹æ€§å›¾ä¸Šä¸ Sigmoid å‡½æ•°ç±»ä¼¼ï¼Œå®šä¹‰ä¸ºï¼Œ
- en: '|  | $Elliott(x)=\frac{0.5\times x}{1+&#124;x&#124;}+0.5$ |  | (12) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $Elliott(x)=\frac{0.5\times x}{1+&#124;x&#124;}+0.5$ |  | (12) |'
- en: in the output range of $[0,1]$. The Soft-Root-Sign (SRS) AF [[26](#bib.bib26)]
    is defined as,
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ä¸º $[0,1]$ã€‚Soft-Root-Sign (SRS) AF [[26](#bib.bib26)] å®šä¹‰ä¸ºï¼Œ
- en: '|  | $SRS(x)=\frac{x}{\frac{x}{\alpha}+e^{-x/\beta}}$ |  | (13) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $SRS(x)=\frac{x}{\frac{x}{\alpha}+e^{-x/\beta}}$ |  | (13) |'
- en: in the output range of $[\frac{\alpha\times\beta}{\beta-\alpha\times e},\alpha]$
    where $\alpha$ and $\beta$ are the learnable parameters. The use of additional
    parameters increases the complexity of the SRS function. Most of the variants
    of Sigmoid/Tanh AFs have tried to overcome the vanishing gradient issue. However,
    this issue is still present in most of these AFs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ä¸º $[\frac{\alpha\times\beta}{\beta-\alpha\times e},\alpha]$ï¼Œå…¶ä¸­ $\alpha$
    å’Œ $\beta$ æ˜¯å¯å­¦ä¹ å‚æ•°ã€‚ä½¿ç”¨é¢å¤–çš„å‚æ•°å¢åŠ äº† SRS å‡½æ•°çš„å¤æ‚æ€§ã€‚å¤§å¤šæ•° Sigmoid/Tanh AF çš„å˜ä½“å°è¯•å…‹æœæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé—®é¢˜åœ¨å¤§å¤šæ•°è¿™äº›
    AF ä¸­ä»ç„¶å­˜åœ¨ã€‚
- en: 4 Rectified Activation Functions
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 çº æ­£æ¿€æ´»å‡½æ•°
- en: 'A summary of rectified AFs is illustrated in Table [3](#S3.T3 "Table 3 â€£ 3
    Logistic Sigmoid and Tanh Based AFs â€£ Activation Functions in Deep Learning: A
    Comprehensive Survey and Benchmark"). Rectified Linear Unit (ReLU) is a simple
    function which is the identity function for positive input and zero for negative
    input and given as,'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'çº æ­£æ¿€æ´»å‡½æ•°çš„æ€»ç»“å¦‚è¡¨ [3](#S3.T3 "Table 3 â€£ 3 Logistic Sigmoid and Tanh Based AFs â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark") æ‰€ç¤ºã€‚çº æ­£çº¿æ€§å•å…ƒ (ReLU)
    æ˜¯ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œå¯¹äºæ­£è¾“å…¥æ˜¯æ’ç­‰å‡½æ•°ï¼Œå¯¹äºè´Ÿè¾“å…¥æ˜¯é›¶ï¼Œå®šä¹‰ä¸ºï¼Œ'
- en: '|  | $ReLU(x)=max(0,x)=\begin{cases}x,&amp;\text{if }x\geq 0\\ 0,&amp;\text{otherwise}\end{cases}.$
    |  | (14) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $ReLU(x)=max(0,x)=\begin{cases}x,&amp;\text{if }x\geq 0\\ 0,&amp;\text{otherwise}\end{cases}.$
    |  | (14) |'
- en: Hence, the range of ReLU is $[0,\infty)$. The gradient for positive and negative
    inputs is one and zero, respectively. The ReLU function solves the problem of
    computational complexity of the Logistic Sigmoid and Tanh functions. The downside
    of ReLU is with the vanishing gradient problem for the negative inputs. In spite
    of having the vanishing gradient problem, the ReLU AF has been used very extensively
    with the deep learning models. The advancements in ReLU based AFs are discussed
    in the rest of this section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒReLU çš„èŒƒå›´ä¸º $[0,\infty)$ã€‚å¯¹äºæ­£è¾“å…¥å’Œè´Ÿè¾“å…¥çš„æ¢¯åº¦åˆ†åˆ«ä¸ºä¸€å’Œé›¶ã€‚ReLU å‡½æ•°è§£å†³äº† Logistic Sigmoid å’Œ Tanh
    å‡½æ•°çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜ã€‚ReLU çš„ç¼ºç‚¹åœ¨äºè´Ÿè¾“å…¥çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚å°½ç®¡å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒReLU AF åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­å¾—åˆ°äº†å¹¿æ³›ä½¿ç”¨ã€‚æœ¬èŠ‚çš„å…¶ä½™éƒ¨åˆ†è®¨è®ºäº†åŸºäº
    ReLU çš„ AF çš„è¿›å±•ã€‚
- en: 4.1 On the Non-utilization of Negative Values of ReLU
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 å…³äº ReLU çš„è´Ÿå€¼æœªåˆ©ç”¨é—®é¢˜
- en: Vanishing gradient is the main problem with ReLU AF which is caused due to the
    non-utilization of negative values. A Leaky Rectified Linear Unit (LReLU) is the
    extension of ReLU by utilizing the negative values [[34](#bib.bib34)]. The LReLU
    is defined as,
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æ¶ˆå¤±æ˜¯ ReLU AF çš„ä¸»è¦é—®é¢˜ï¼Œç”±äºæœªåˆ©ç”¨è´Ÿå€¼è€Œé€ æˆã€‚Leaky Rectified Linear Unit (LReLU) æ˜¯ ReLU çš„æ‰©å±•ï¼Œé€šè¿‡åˆ©ç”¨è´Ÿå€¼
    [[34](#bib.bib34)]ã€‚LReLU å®šä¹‰ä¸ºï¼Œ
- en: '|  | $LReLU(x)=\begin{cases}x,&amp;x\geq 0\\ 0.01\times x,&amp;x<0\end{cases}$
    |  | (15) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $LReLU(x)=\begin{cases}x,&amp;x\geq 0\\ 0.01\times x,&amp;x<0\end{cases}$
    |  | (15) |'
- en: in the output range of $(-\infty,\infty)$. The LReLU has been used in many applications
    with promising performance. One major problem associated with LReLU is the finding
    of the right slope in linear function for negative inputs. Different slopes might
    be suited for different problems and different networks. Thus, it is extended
    to Parametric ReLU (PReLU) by considering the slope for negative input as a trainable
    parameter [[35](#bib.bib35)]. The PReLU is given as,
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ä¸º $(-\infty,\infty)$ã€‚LReLU åœ¨è®¸å¤šåº”ç”¨ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚LReLU å…³è”çš„ä¸€ä¸ªä¸»è¦é—®é¢˜æ˜¯ä¸ºè´Ÿè¾“å…¥æ‰¾åˆ°åˆé€‚çš„æ–œç‡ã€‚ä¸åŒçš„æ–œç‡å¯èƒ½é€‚åˆä¸åŒçš„é—®é¢˜å’Œç½‘ç»œã€‚å› æ­¤ï¼Œå®ƒé€šè¿‡å°†è´Ÿè¾“å…¥çš„æ–œç‡ä½œä¸ºå¯è®­ç»ƒå‚æ•°æ‰©å±•ä¸º
    Parametric ReLU (PReLU) [[35](#bib.bib35)]ã€‚PReLU å®šä¹‰ä¸ºï¼Œ
- en: '|  | $PReLU(x)=\begin{cases}x,&amp;x\geq 0\\ p\times x,&amp;x<0\end{cases}$
    |  | (16) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $PReLU(x)=\begin{cases}x,&amp;x\geq 0\\ p\times x,&amp;x<0\end{cases}$
    |  | (16) |'
- en: in the output range of $(-\infty,\infty)$ where $p$ is the trainable parameter.
    However, it can lead to overfitting easily which is the downside of PReLU. The
    Maxout layer, which computes the maximum of several linear units, is also used
    as AF [[49](#bib.bib49)]. Both ReLU and Leaky ReLU can be seen as the special
    cases of Maxout. The randomized ReLU (RReLU) considers the slope of LReLU randomly
    during training sampled from an uniform distribution $U(l,u)$ [[50](#bib.bib50)].
    The RReLU is defined as,
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ $(-\infty,\infty)$ å†…ï¼Œå…¶ä¸­ $p$ æ˜¯å¯è®­ç»ƒå‚æ•°ã€‚ç„¶è€Œï¼Œå®ƒå®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œè¿™æ˜¯ PReLU çš„ç¼ºç‚¹ã€‚Maxout å±‚è®¡ç®—å¤šä¸ªçº¿æ€§å•å…ƒçš„æœ€å¤§å€¼ï¼Œä¹Ÿä½œä¸º
    AF ä½¿ç”¨ [[49](#bib.bib49)]ã€‚ReLU å’Œ Leaky ReLU å¯ä»¥çœ‹ä½œæ˜¯ Maxout çš„ç‰¹ä¾‹ã€‚éšæœº ReLU (RReLU) åœ¨è®­ç»ƒæœŸé—´ä»å‡åŒ€åˆ†å¸ƒ
    $U(l,u)$ éšæœºè€ƒè™‘ LReLU çš„æ–œç‡ [[50](#bib.bib50)]ã€‚RReLU å®šä¹‰ä¸ºï¼Œ
- en: '|  | $RReLU(x)=\begin{cases}x,&amp;x\geq 0\\ R\times x,&amp;x<0\end{cases}$
    |  | (17) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $RReLU(x)=\begin{cases}x,&amp;x\geq 0\\ R\times x,&amp;x<0\end{cases}$
    |  | (17) |'
- en: in the output range of $(-\infty,\infty)$ where $R\sim{~{}}U(l,u)$, $l<u$ and
    $l,u\in[0,1)$. It uses a deterministic value $x/\left(\frac{l+u}{2}\right)$ during
    test time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ $(-\infty,\infty)$ å†…ï¼Œå…¶ä¸­ $R\sim{~{}}U(l,u)$ï¼Œ$l<u$ ä¸” $l,u\in[0,1)$ã€‚åœ¨æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§å€¼
    $x/\left(\frac{l+u}{2}\right)$ã€‚
- en: The ReLU is not able to utilize the potential useful information from the negative
    values. In most of the networks, the feature map given as the input to AF is dense
    near zero. Thus, a small jitter in the rectification point can lead to difficulty
    in training. Concatenated ReLU (CReLU) [[36](#bib.bib36)] concatenates the ReLUâ€™s
    output over original input and negated input. The CReLU can be given as,
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU æ— æ³•åˆ©ç”¨è´Ÿå€¼ä¸­çš„æ½œåœ¨æœ‰ç”¨ä¿¡æ¯ã€‚åœ¨å¤§å¤šæ•°ç½‘ç»œä¸­ï¼Œä½œä¸º AF è¾“å…¥çš„ç‰¹å¾å›¾åœ¨é›¶é™„è¿‘æ˜¯å¯†é›†çš„ã€‚å› æ­¤ï¼Œä¿®æ­£ç‚¹çš„å¾®å°æŠ–åŠ¨å¯èƒ½å¯¼è‡´è®­ç»ƒå›°éš¾ã€‚è¿æ¥ ReLUï¼ˆCReLUï¼‰[[36](#bib.bib36)]
    å°† ReLU çš„è¾“å‡ºè¿æ¥åˆ°åŸå§‹è¾“å…¥å’Œè´Ÿè¾“å…¥ã€‚CReLU å¯ä»¥è¡¨ç¤ºä¸ºï¼Œ
- en: '|  | $CReLU(x)=[ReLU(x),ReLU(-x)]$ |  | (18) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $CReLU(x)=[ReLU(x),ReLU(-x)]$ |  | (18) |'
- en: in the output range of $[0,\infty)$. The CReLU is derived from the fact that
    the lower layer kernels in CNN models form pairs with opposite phases. The shifting
    of the feature map with multiple biases is also performed before the ReLU layer
    [[51](#bib.bib51)]. However, it increases the model complexity as more ReLUs are
    required. A Parametric Tan Hyperbolic Linear Unit (P-TELU) is also used as an
    AF [[38](#bib.bib38)]. The P-TELU is defined as,
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ $[0,\infty)$ å†…ã€‚CReLU æºäº CNN æ¨¡å‹ä¸­ä¸‹å±‚å·ç§¯æ ¸å½¢æˆç›¸ä½ç›¸åçš„å¯¹ã€‚ç‰¹å¾å›¾åœ¨ ReLU å±‚ä¹‹å‰è¿˜ä¼šè¿›è¡Œå¤šä¸ªåç½®çš„ç§»åŠ¨
    [[51](#bib.bib51)]ã€‚ç„¶è€Œï¼Œè¿™å¢åŠ äº†æ¨¡å‹çš„å¤æ‚æ€§ï¼Œå› ä¸ºéœ€è¦æ›´å¤šçš„ ReLUã€‚ä¹Ÿä½¿ç”¨äº†å‚æ•°åŒ–åŒæ›²æ­£åˆ‡çº¿æ€§å•å…ƒï¼ˆP-TELUï¼‰ä½œä¸º AF [[38](#bib.bib38)]ã€‚P-TELU
    å®šä¹‰ä¸ºï¼Œ
- en: '|  | $PTELU(x)=\begin{cases}x,&amp;x\geq 0\\ \alpha\times\text{Tanh}(\beta\times
    x),&amp;x<0\end{cases}$ |  | (19) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $PTELU(x)=\begin{cases}x,&amp;x\geq 0\\ \alpha\times\text{Tanh}(\beta\times
    x),&amp;x<0\end{cases}$ |  | (19) |'
- en: in the output range of $[-\alpha,\infty)$ where $\{\alpha,\beta\}\geq 0$ are
    the learnable parameters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ $[-\alpha,\infty)$ å†…ï¼Œå…¶ä¸­ $\{\alpha,\beta\}\geq 0$ æ˜¯å¯å­¦ä¹ çš„å‚æ•°ã€‚
- en: 'Table 4: Summary of Exponential Linear Unit based activation functions.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 4ï¼šåŸºäºæŒ‡æ•°çº¿æ€§å•å…ƒçš„æ¿€æ´»å‡½æ•°æ€»ç»“ã€‚
- en: '| Name | Parametric | Monotonic | Smooth | Bounded |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| åç§° | å‚æ•°åŒ– | å•è°ƒ | å…‰æ»‘ | æœ‰ç•Œ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Exponential Linear Unit (ELU), 2016 [[27](#bib.bib27)] | Yes | Yes | Yes
    | For negative inputs |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Exponential Linear Unit (ELU), 2016 [[27](#bib.bib27)] | æ˜¯ | æ˜¯ | æ˜¯ | é’ˆå¯¹è´Ÿè¾“å…¥
    |'
- en: '| Scaled ELU (SELU), 2017 [[52](#bib.bib52)] | Yes | Yes | Yes | For negative
    inputs |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Scaled ELU (SELU), 2017 [[52](#bib.bib52)] | æ˜¯ | æ˜¯ | æ˜¯ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Continuously Differentiable ELU (CELU), 2017 [[53](#bib.bib53)] | Yes | Yes
    | No | For negative inputs |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Continuously Differentiable ELU (CELU), 2017 [[53](#bib.bib53)] | æ˜¯ | æ˜¯ |
    å¦ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Parametric ELU (PELU), 2017 [[54](#bib.bib54)] | Yes | Yes | No | For negative
    inputs |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Parametric ELU (PELU), 2017 [[54](#bib.bib54)] | æ˜¯ | æ˜¯ | å¦ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Multiple PELU (MPELU), 2018 [[55](#bib.bib55)] | Yes | Yes | No | For negative
    inputs |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Multiple PELU (MPELU), 2018 [[55](#bib.bib55)] | æ˜¯ | æ˜¯ | å¦ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Fast ELU (FELU), 2019 [[56](#bib.bib56)] | Yes | Yes | No | For negative
    inputs |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Fast ELU (FELU), 2019 [[56](#bib.bib56)] | æ˜¯ | æ˜¯ | å¦ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Parametric Rectified Exponential Unit (PREU), 2019 [[57](#bib.bib57)] | Yes
    | No | Yes | For negative inputs |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Parametric Rectified Exponential Unit (PREU), 2019 [[57](#bib.bib57)] | æ˜¯
    | å¦ | æ˜¯ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Elastic ELU (EELU), 2020 [[58](#bib.bib58)] | Yes | Yes | No | For negative
    inputs |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Elastic ELU (EELU), 2020 [[58](#bib.bib58)] | æ˜¯ | æ˜¯ | å¦ | é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: '| Parametric Deformable ELU (PDELU), 2020 [[59](#bib.bib59)] | Yes | Yes |
    Yes | For negative inputs |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Parametric Deformable ELU (PDELU), 2020 [[59](#bib.bib59)] | æ˜¯ | æ˜¯ | æ˜¯ |
    é’ˆå¯¹è´Ÿè¾“å…¥ |'
- en: The Flexible ReLU (FReLU) [[39](#bib.bib39)] captures the negative values with
    a rectified point which is considered as trainable in the Shifted ReLU [[39](#bib.bib39)].
    The FReLU is given as,
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Flexible ReLU (FReLU) [[39](#bib.bib39)] é€šè¿‡åœ¨Shifted ReLU [[39](#bib.bib39)]ä¸­å°†æ•´æµç‚¹è§†ä¸ºå¯è®­ç»ƒçš„æ¥æ•æ‰è´Ÿå€¼ã€‚FReLUå®šä¹‰ä¸ºï¼Œ
- en: '|  | $FReLU(x)=ReLU(x)+b$ |  | (20) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $FReLU(x)=ReLU(x)+b$ |  | (20) |'
- en: in the output range of $[b,\infty)$. A similar arrangement is also followed
    by Random Translation ReLU (RTReLU) [[41](#bib.bib41)] by utilizing an offset,
    sampled from a Gaussian distribution, given as,
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ä¸º$[b,\infty)$ä¸­ã€‚ç±»ä¼¼çš„å®‰æ’ä¹Ÿè¢«Random Translation ReLU (RTReLU) [[41](#bib.bib41)]
    é€šè¿‡åˆ©ç”¨ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„åç§»é‡æ‰€éµå¾ªï¼Œå®šä¹‰ä¸ºï¼Œ
- en: '|  | $RTReLU(x)=\begin{cases}x+a,&amp;x+a>0\\ 0,&amp;x+a\leq 0\end{cases}$
    |  | (21) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $RTReLU(x)=\begin{cases}x+a,&amp;x+a>0\\ 0,&amp;x+a\leq 0\end{cases}$
    |  | (21) |'
- en: in the output range of $[0,\infty)$ where $a$ is a random number. At test time,
    the offset is set to zero. A data dependent Average Biased ReLU (AB-ReLU) [[44](#bib.bib44)]
    is also investigated to tackle the negative values by a horizontal shifting based
    on the average of features. The ABReLU can be written as,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ä¸º$[0,\infty)$ä¸­ï¼Œå…¶ä¸­$a$æ˜¯ä¸€ä¸ªéšæœºæ•°ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œåç§»é‡è®¾ç½®ä¸ºé›¶ã€‚è¿˜ç ”ç©¶äº†ä¸€ç§ä¾èµ–äºæ•°æ®çš„å¹³å‡åç½®ReLU (AB-ReLU)
    [[44](#bib.bib44)]ï¼Œé€šè¿‡åŸºäºç‰¹å¾å¹³å‡å€¼çš„æ°´å¹³ä½ç§»æ¥å¤„ç†è´Ÿå€¼ã€‚ABReLUå¯ä»¥å†™æˆï¼Œ
- en: '|  | $ABReLU(x)=\begin{cases}x-\beta,&amp;x-\beta\geq 0\\ 0,&amp;x-\beta<0\end{cases}$
    |  | (22) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $ABReLU(x)=\begin{cases}x-\beta,&amp;x-\beta\geq 0\\ 0,&amp;x-\beta<0\end{cases}$
    |  | (22) |'
- en: having the output range in $[0,\infty)$ where $\beta$ is computed as the average
    of input activation map to the activation function. The batch dependent threshold
    for the ReLU is used by the Dynamic ReLU (D-ReLU) [[60](#bib.bib60)]. The Dual
    ReLU (DualReLU) [[42](#bib.bib42)] is a two dimensional AF for recurrent neural
    networks. The DualReLU is given as,
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[0,\infty)$ï¼Œå…¶ä¸­$\beta$è®¡ç®—ä¸ºè¾“å…¥æ¿€æ´»å›¾åˆ°æ¿€æ´»å‡½æ•°çš„å¹³å‡å€¼ã€‚Dynamic ReLU (D-ReLU) [[60](#bib.bib60)]ä½¿ç”¨ReLUçš„æ‰¹æ¬¡ç›¸å…³é˜ˆå€¼ã€‚Dual
    ReLU (DualReLU) [[42](#bib.bib42)] æ˜¯ç”¨äºé€’å½’ç¥ç»ç½‘ç»œçš„äºŒç»´æ¿€æ´»å‡½æ•°ã€‚DualReLUå®šä¹‰ä¸ºï¼Œ
- en: '|  | $DualReLU(a,b)=\max(0,a)-\max(0,b)$ |  | (23) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $DualReLU(a,b)=\max(0,a)-\max(0,b)$ |  | (23) |'
- en: in the output range of $(-\infty,\infty)$ where $a$ and $b$ are the inputs in
    different dimensions. Similar to the CReLU, the PairedReLU AF is used for image
    super-resolution [[43](#bib.bib43)]. The PairedReLU is given as,
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ä¸º$(-\infty,\infty)$ä¸­ï¼Œå…¶ä¸­$a$å’Œ$b$æ˜¯ä¸åŒç»´åº¦çš„è¾“å…¥ã€‚ç±»ä¼¼äºCReLUï¼ŒPairedReLUæ¿€æ´»å‡½æ•°ç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡
    [[43](#bib.bib43)]ã€‚PairedReLUå®šä¹‰ä¸ºï¼Œ
- en: '|  | $PairedReLU(x)=[\max(s\times x-\theta,0),max(s_{p}\times x-\theta_{p},0)]$
    |  | (24) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $PairedReLU(x)=[\max(s\times x-\theta,0),max(s_{p}\times x-\theta_{p},0)]$
    |  | (24) |'
- en: in the output range of $(-\infty,\infty)$. However, the computational complexity
    of PairedReLU is increased as compared to CReLU. In another attempt, V-shaped
    ReLU (vReLU) AF [[61](#bib.bib61)] is defined as,
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´ä¸º$(-\infty,\infty)$ä¸­ã€‚ç„¶è€Œï¼Œä¸CReLUç›¸æ¯”ï¼ŒPairedReLUçš„è®¡ç®—å¤æ‚åº¦å¢åŠ äº†ã€‚åœ¨å¦ä¸€ç§å°è¯•ä¸­ï¼ŒVå½¢ReLU (vReLU)
    æ¿€æ´»å‡½æ•° [[61](#bib.bib61)] è¢«å®šä¹‰ä¸ºï¼Œ
- en: '|  | $vReLU(x)=\begin{cases}x,&amp;x\geq 0\\ -x,&amp;x<0\end{cases}$ |  | (25)
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $vReLU(x)=\begin{cases}x,&amp;x\geq 0\\ -x,&amp;x<0\end{cases}$ |  | (25)
    |'
- en: having the output range in $[0,\infty]$. The vReLU activation function suffers
    from the non-symmetric output. The SignReLU AF utilizes the negative values using
    the Softsign function [[62](#bib.bib62)]. The positive part of SignReLU is the
    same as the ReLU.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[0,\infty]$ã€‚vReLUæ¿€æ´»å‡½æ•°å­˜åœ¨éå¯¹ç§°è¾“å‡ºçš„é—®é¢˜ã€‚SignReLUæ¿€æ´»å‡½æ•°åˆ©ç”¨è´Ÿå€¼ä½¿ç”¨Softsignå‡½æ•° [[62](#bib.bib62)]ã€‚SignReLUçš„æ­£éƒ¨åˆ†ä¸ReLUç›¸åŒã€‚
- en: A Displaced ReLU (DisReLU) [[63](#bib.bib63)] is designed as a generalization
    of Shifted ReLU [[39](#bib.bib39)]. The DisReLU displaces the rectification point
    to consider the negative values, given as,
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Displaced ReLU (DisReLU) [[63](#bib.bib63)] è®¾è®¡ä¸ºShifted ReLU [[39](#bib.bib39)]
    çš„æ¨å¹¿ã€‚DisReLUé€šè¿‡å°†æ•´æµç‚¹ç§»ä½æ¥è€ƒè™‘è´Ÿå€¼ï¼Œå®šä¹‰ä¸ºï¼Œ
- en: '|  | $DisReLU(x)=\begin{cases}x,&amp;x\geq-\delta\\ -\delta,&amp;x<-\delta\end{cases}$
    |  | (26) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $DisReLU(x)=\begin{cases}x,&amp;x\geq-\delta\\ -\delta,&amp;x<-\delta\end{cases}$
    |  | (26) |'
- en: having the output range in $[-\delta,\infty]$. A Bendable Linear Unit (BLU)
    AF is investigated as,
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-\delta,\infty]$ã€‚ç ”ç©¶äº†å¯å¼¯æ›²çº¿æ€§å•å…ƒ (BLU) æ¿€æ´»å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ
- en: '|  | $BLU(x)=\beta\times(\sqrt{x^{2}+1}-1)+x$ |  | (27) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $BLU(x)=\beta\times(\sqrt{x^{2}+1}-1)+x$ |  | (27) |'
- en: where $-1\leq\beta\leq 1$ is a learnable parameter to adapt the shape between
    the identity function and a rectifier function [[64](#bib.bib64)]. A Lipschitz
    ReLU (L-ReLU) AF uses the piecewise linear functions to model the degree of presence
    and the degree of absence of features [[47](#bib.bib47)]. The L-ReLU is defined
    as,
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$-1\leq\beta\leq 1$ æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ï¼Œç”¨äºé€‚åº”èº«ä»½å‡½æ•°å’Œæ•´æµå‡½æ•°ä¹‹é—´çš„å½¢çŠ¶ [[64](#bib.bib64)]ã€‚Lipschitz
    ReLU (L-ReLU) æ¿€æ´»å‡½æ•°ä½¿ç”¨åˆ†æ®µçº¿æ€§å‡½æ•°æ¥å»ºæ¨¡ç‰¹å¾çš„å­˜åœ¨ç¨‹åº¦å’Œç¼ºå¤±ç¨‹åº¦ [[47](#bib.bib47)]ã€‚L-ReLUå®šä¹‰ä¸ºï¼Œ
- en: '|  | $L\text{-}ReLU(x)=\begin{cases}\max(\phi(x),0),&amp;x\geq 0\\ \min(\eta(x),0),&amp;x<0\end{cases}$
    |  | (28) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $L\text{-}ReLU(x)=\begin{cases}\max(\phi(x),0),&amp;x\geq 0\\ \min(\eta(x),0),&amp;x<0\end{cases}$
    |  | (28) |'
- en: where $\phi$ and $\eta$ are non-linear functions. Moreover, the range of L-ReLU
    also depends upon the values of $\phi$ and $\eta$ functions.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\phi$ å’Œ $\eta$ æ˜¯éçº¿æ€§å‡½æ•°ã€‚æ­¤å¤–ï¼ŒL-ReLU çš„èŒƒå›´ä¹Ÿå–å†³äº $\phi$ å’Œ $\eta$ å‡½æ•°çš„å€¼ã€‚
- en: 4.2 On the Limited Non-linearity of ReLU
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 ReLU çš„æœ‰é™éçº¿æ€§
- en: S-shaped ReLU (SReLU) increases the non-linearity in ReLU by combining three
    linear functions with four learnable parameters [[65](#bib.bib65)]. On a similar
    line, Multi-bin Trainable Linear Unit (MTLU) [[46](#bib.bib46)] considers multiple
    bins to increase the non-linear capacity. The MTLU can be written as,
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Så½¢ ReLU (SReLU) é€šè¿‡å°†ä¸‰ä¸ªçº¿æ€§å‡½æ•°ä¸å››ä¸ªå¯å­¦ä¹ çš„å‚æ•°ç»“åˆèµ·æ¥ï¼Œæé«˜äº† ReLU çš„éçº¿æ€§ [[65](#bib.bib65)]ã€‚ç±»ä¼¼åœ°ï¼Œå¤š-bin
    å¯è®­ç»ƒçº¿æ€§å•å…ƒ (MTLU) [[46](#bib.bib46)] é€šè¿‡è€ƒè™‘å¤šä¸ªåŒºé—´æ¥å¢åŠ éçº¿æ€§å®¹é‡ã€‚MTLU å¯ä»¥å†™æˆï¼Œ
- en: '|  | <math   alttext="MTLU(x)=\begin{cases}a_{0}\times x+b_{0},&amp;x\leq c_{0}\\
    a_{k}\times x+b_{k},&amp;c_{k-1}<x\leq c_{k}\\'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="MTLU(x)=\begin{cases}a_{0}\times x+b_{0},&amp;x\leq c_{0}\\
    a_{k}\times x+b_{k},&amp;c_{k-1}<x\leq c_{k}\\'
- en: '...&amp;\\'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '...&amp;\\'
- en: a_{K}\times x+b_{K},&amp;c_{K-1}<x\end{cases}" display="block"><semantics ><mrow
    ><mrow  ><mi >M</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi >T</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mi  >L</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >U</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi >x</mi><mo
    stretchy="false" >)</mo></mrow></mrow><mo  >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow ><mrow
    ><mrow  ><msub ><mi >a</mi><mn >0</mn></msub><mo lspace="0.222em" rspace="0.222em"  >Ã—</mo><mi
    >x</mi></mrow><mo >+</mo><msub ><mi >b</mi><mn >0</mn></msub></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mi  >x</mi><mo >â‰¤</mo><msub ><mi  >c</mi><mn >0</mn></msub></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mrow  ><mrow ><msub ><mi >a</mi><mi >k</mi></msub><mo
    lspace="0.222em" rspace="0.222em"  >Ã—</mo><mi >x</mi></mrow><mo >+</mo><msub ><mi
    >b</mi><mi >k</mi></msub></mrow><mo >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow
    ><msub  ><mi >c</mi><mrow ><mi >k</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub><mo
    ><</mo><mi  >x</mi><mo >â‰¤</mo><msub ><mi  >c</mi><mi >k</mi></msub></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mi mathvariant="normal" >â€¦</mi></mtd></mtr><mtr ><mtd
    columnalign="left"  ><mrow ><mrow  ><mrow ><msub ><mi >a</mi><mi >K</mi></msub><mo
    lspace="0.222em" rspace="0.222em"  >Ã—</mo><mi >x</mi></mrow><mo >+</mo><msub ><mi
    >b</mi><mi >K</mi></msub></mrow><mo >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow
    ><msub  ><mi >c</mi><mrow ><mi >K</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub><mo
    ><</mo><mi  >x</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><ci >ğ‘€</ci><ci  >ğ‘‡</ci><ci >ğ¿</ci><ci >ğ‘ˆ</ci><ci  >ğ‘¥</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><apply ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><cn type="integer"  >0</cn></apply><ci >ğ‘¥</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><cn type="integer"  >0</cn></apply></apply><apply
    ><ci >ğ‘¥</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><cn
    type="integer" >0</cn></apply></apply><apply ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘˜</ci></apply><ci >ğ‘¥</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘˜</ci></apply></apply><apply ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><apply ><ci >ğ‘˜</ci><cn type="integer" >1</cn></apply></apply><ci >ğ‘¥</ci></apply><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply></apply></apply><ci
    >â€¦</ci><ci ><mtext >otherwise</mtext></ci><apply ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ¾</ci></apply><ci >ğ‘¥</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ¾</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><apply ><ci  >ğ¾</ci><cn type="integer"  >1</cn></apply></apply><ci >ğ‘¥</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >MTLU(x)=\begin{cases}a_{0}\times x+b_{0},&x\leq
    c_{0}\\ a_{k}\times x+b_{k},&c_{k-1}<x\leq c_{k}\\ ...&\\ a_{K}\times x+b_{K},&c_{K-1}<x\end{cases}</annotation></semantics></math>
    |  | (29) |
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \(M_{T L U}(x) = \begin{cases}a_{0}\times x + b_{0},& x \leq c_{0} \\ a_{k}\times
    x + b_{k},& c_{k-1} < x \leq c_{k} \\ \vdots & \\ a_{K}\times x + b_{K},& c_{K-1}
    < x \end{cases}\)
- en: having the output range in $(-\infty,\infty)$. The number of bins and the range
    of bins are the hyperparameters, whereas the linear function of a bin is trainable
    (i.e., $a_{0},...,a_{K}$ $b_{0},...,b_{K}$ are the learnable parameters). The
    non-differentiable nature at multiple points is the drawback of the MTLU. An Elastic
    ReLU (EReLU) considers a slope randomly drawn from a uniform distribution during
    the training for the positive inputs to control the amount of non-linearity [[40](#bib.bib40)].
    The EReLU is defined as,
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$(-\infty,\infty)$ã€‚ç®±å­çš„æ•°é‡å’ŒèŒƒå›´æ˜¯è¶…å‚æ•°ï¼Œè€Œç®±å­çš„çº¿æ€§å‡½æ•°æ˜¯å¯è®­ç»ƒçš„ï¼ˆå³ï¼Œ$a_{0},...,a_{K}$å’Œ$b_{0},...,b_{K}$æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼‰ã€‚MTLUçš„ç¼ºç‚¹æ˜¯å¤šä¸ªç‚¹çš„éå¯å¾®æ€§è´¨ã€‚Elastic
    ReLU (EReLU)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ­£è¾“å…¥è€ƒè™‘ä»å‡åŒ€åˆ†å¸ƒä¸­éšæœºæŠ½å–çš„æ–œç‡ï¼Œä»¥æ§åˆ¶éçº¿æ€§çš„é‡[[40](#bib.bib40)]ã€‚EReLUå®šä¹‰ä¸ºï¼Œ
- en: '|  | $EReLU(x)=max(R\times x,0)$ |  | (30) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $EReLU(x)=max(R\times x,0)$ |  | (30) |'
- en: in the output range of $[0,\infty)$ where $R$ is a random number. At the test
    time, the EReLU becomes the identity function for positive inputs. The Linearized
    Sigmoidal Activation (LiSHA) function considers three linear functions to increase
    the non-linearity characteristics [[66](#bib.bib66)]. It is also extended to adaptive
    linear sigmoidal AF by learning the slope of upper and lower linear functions.
    The ReLU is combined with Tanh as Rectified Linear Tanh (ReLTanh) [[67](#bib.bib67)]
    to increase the non-linearity of ReLU and to overcome the vanishing gradient problem
    of Tanh. However, the ReLTanh is unbounded in both the positive and negative directions.
    Natural-Logarithm ReLU (NLReLU) modifies the ReLUâ€™s output for positive inputs
    using the logarithm function to increase the degree of nonlinearity [[45](#bib.bib45)].
    The NLReLU is defined as,
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å‡ºèŒƒå›´$[0,\infty)$ä¸­ï¼Œå…¶ä¸­$R$æ˜¯éšæœºæ•°ã€‚åœ¨æµ‹è¯•æ—¶ï¼ŒEReLUå¯¹æ­£è¾“å…¥å˜æˆæ’ç­‰å‡½æ•°ã€‚Linearized Sigmoidal Activation
    (LiSHA)å‡½æ•°è€ƒè™‘äº†ä¸‰ä¸ªçº¿æ€§å‡½æ•°ä»¥å¢åŠ éçº¿æ€§ç‰¹æ€§[[66](#bib.bib66)]ã€‚å®ƒè¿˜æ‰©å±•ä¸ºè‡ªé€‚åº”çº¿æ€§Sigmoidal AFï¼Œé€šè¿‡å­¦ä¹ ä¸Šä¸‹çº¿æ€§å‡½æ•°çš„æ–œç‡ã€‚ReLUä¸Tanhç»“åˆå½¢æˆRectified
    Linear Tanh (ReLTanh) [[67](#bib.bib67)]ï¼Œä»¥å¢åŠ ReLUçš„éçº¿æ€§å¹¶å…‹æœTanhçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ç„¶è€Œï¼ŒReLTanhåœ¨æ­£è´Ÿä¸¤ä¸ªæ–¹å‘ä¸Šéƒ½æ˜¯æ— ç•Œçš„ã€‚Natural-Logarithm
    ReLU (NLReLU)é€šè¿‡ä½¿ç”¨å¯¹æ•°å‡½æ•°ä¿®æ”¹ReLUå¯¹æ­£è¾“å…¥çš„è¾“å‡ºï¼Œä»¥å¢åŠ éçº¿æ€§ç¨‹åº¦[[45](#bib.bib45)]ã€‚NLReLUå®šä¹‰ä¸ºï¼Œ
- en: '|  | $NLReLU(x)=\ln(\beta\times\max(0,x)+1.0)$ |  | (31) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $NLReLU(x)=\ln(\beta\times\max(0,x)+1.0)$ |  | (31) |'
- en: having the output range in $[0,\infty)$ where $\beta$ is a constant. The NLReLU
    does not affect the negative regime, thus suffers from vanishing gradient. The
    concept of Leaky ReLU (LReLU) is further improved to Dynamic ReLU [[68](#bib.bib68)]
    by considering a mean square error (MSE) based additional hyperparameter. Thus,
    it can control the slope of the Dynamic ReLU in every epoch based on the convergence.
    A Piecewise Linear Unit (PLU) [[69](#bib.bib69)] is defined as,
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[0,\infty)$ï¼Œå…¶ä¸­$\beta$æ˜¯å¸¸æ•°ã€‚NLReLUä¸å½±å“è´Ÿå€¼èŒƒå›´ï¼Œå› æ­¤ä¼šå—åˆ°æ¢¯åº¦æ¶ˆå¤±çš„å½±å“ã€‚Leaky ReLU (LReLU)çš„æ¦‚å¿µè¿›ä¸€æ­¥æ”¹è¿›ä¸ºDynamic
    ReLU [[68](#bib.bib68)]ï¼Œé€šè¿‡è€ƒè™‘åŸºäºå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„é™„åŠ è¶…å‚æ•°ã€‚è¿™æ ·ï¼Œå®ƒå¯ä»¥æ ¹æ®æ”¶æ•›æƒ…å†µåœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸæ§åˆ¶Dynamic ReLUçš„æ–œç‡ã€‚Piecewise
    Linear Unit (PLU) [[69](#bib.bib69)]å®šä¹‰ä¸ºï¼Œ
- en: '|  | $PLU(x)=max(\alpha\times(x+c)-c,min(\alpha\times(x-c)+c,x))$ |  | (32)
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $PLU(x)=max(\alpha\times(x+c)-c,min(\alpha\times(x-c)+c,x))$ |  | (32)
    |'
- en: having the output range in $[-\infty,+\infty]$, where $\alpha$ and $c$ are the
    constants. Basically, the PLU activation function consists of three linear functions
    in pieces, but continuous. Hence, it avoids the saturation and leads to a good
    amount of gradient flow through the activation function during backpropagation
    in order to resolve the vanishing gradient problems of ReLU and Tanh. However,
    the PLU activation is unbounded in both positive and negative directions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-\infty,+\infty]$ï¼Œå…¶ä¸­$\alpha$å’Œ$c$æ˜¯å¸¸æ•°ã€‚åŸºæœ¬ä¸Šï¼ŒPLUæ¿€æ´»å‡½æ•°ç”±ä¸‰ä¸ªçº¿æ€§å‡½æ•°ç»„æˆï¼Œä½†ä¿æŒè¿ç»­ã€‚å› æ­¤ï¼Œå®ƒé¿å…äº†é¥±å’Œï¼Œå¹¶åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€šè¿‡æ¿€æ´»å‡½æ•°æä¾›äº†è‰¯å¥½çš„æ¢¯åº¦æµï¼Œä»¥è§£å†³ReLUå’ŒTanhçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ç„¶è€Œï¼ŒPLUæ¿€æ´»åœ¨æ­£è´Ÿä¸¤ä¸ªæ–¹å‘ä¸Šéƒ½æ˜¯æ— ç•Œçš„ã€‚
- en: 4.3 On the Unbounded Output of ReLU
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 å…³äºReLUçš„æ— ç•Œè¾“å‡º
- en: The unbounded outputs of ReLU and many of its variants may lead to training
    instability. Moreover, the bounded AF is needed for the dedicated hardware based
    embedded system applications. ReLU is extended to Bounded ReLU (BReLU) [[37](#bib.bib37)]
    defined as,
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ReLUåŠå…¶è®¸å¤šå˜ä½“çš„æ— ç•Œè¾“å‡ºå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚æ­¤å¤–ï¼Œä¸“ç”¨ç¡¬ä»¶åµŒå…¥å¼ç³»ç»Ÿåº”ç”¨éœ€è¦æœ‰ç•Œçš„æ¿€æ´»å‡½æ•°ã€‚ReLUè¢«æ‰©å±•ä¸ºBounded ReLU (BReLU)
    [[37](#bib.bib37)]ï¼Œå®šä¹‰ä¸ºï¼Œ
- en: '|  | $BReLU(x)=\min(\max(0,x),A)$ |  | (33) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $BReLU(x)=\min(\max(0,x),A)$ |  | (33) |'
- en: having the output range in $[0,A])$. The training stability is improved in BReLU
    due to two rectifications (i.e., at $0$ and $A$). ReLU is a common choice in practice
    in deep learning. ReLU based AFs are generally efficient. The major drawbacks
    of ReLU, such as gradient diminishing for negative inputs, limited non-linearity
    and unboundedness, are improved in the different AFs. However, the ReLU variants
    are not able to resolve all the issues of ReLU.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º $[0,A]$ã€‚ç”±äºä¸¤ä¸ªæ ¡æ­£ï¼ˆå³åœ¨ $0$ å’Œ $A$ï¼‰ï¼ŒBReLU çš„è®­ç»ƒç¨³å®šæ€§å¾—åˆ°æ”¹å–„ã€‚ReLU æ˜¯æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„é€‰æ‹©ã€‚åŸºäº ReLU
    çš„æ¿€æ´»å‡½æ•°é€šå¸¸æ•ˆç‡é«˜ã€‚ReLU çš„ä¸»è¦ç¼ºç‚¹ï¼Œå¦‚è´Ÿè¾“å…¥çš„æ¢¯åº¦æ¶ˆå¤±ã€æœ‰é™çš„éçº¿æ€§å’Œæ— ç•Œæ€§ï¼Œåœ¨ä¸åŒçš„æ¿€æ´»å‡½æ•°ä¸­æœ‰æ‰€æ”¹å–„ã€‚ç„¶è€Œï¼ŒReLU çš„å˜ä½“æ— æ³•è§£å†³ ReLU
    çš„æ‰€æœ‰é—®é¢˜ã€‚
- en: 5 Exponential Activation Functions
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 æŒ‡æ•°æ¿€æ´»å‡½æ•°
- en: 'The exponential AFs tackle the gradient diminishing problem of ReLU. Table
    [4](#S4.T4 "Table 4 â€£ 4.1 On the Non-utilization of Negative Values of ReLU â€£
    4 Rectified Activation Functions â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark") lists the properties of the exponential AFs. The Exponential
    Linear Unit (ELU) [[27](#bib.bib27)] is given as,'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'æŒ‡æ•°æ¿€æ´»å‡½æ•°è§£å†³äº† ReLU çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚è¡¨ [4](#S4.T4 "Table 4 â€£ 4.1 On the Non-utilization of
    Negative Values of ReLU â€£ 4 Rectified Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark") åˆ—å‡ºäº†æŒ‡æ•°æ¿€æ´»å‡½æ•°çš„å±æ€§ã€‚æŒ‡æ•°çº¿æ€§å•å…ƒï¼ˆELUï¼‰
    [[27](#bib.bib27)] å®šä¹‰ä¸ºï¼Œ'
- en: '|  | $ELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x}-1),&amp;x\leq 0\end{cases}$
    |  | (34) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | $ELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x}-1),&amp;x\leq 0\end{cases}$
    |  | (34) |'
- en: having the output range in $[-1,\infty)$ where $\alpha$ is a learnable parameter.
    The ELU function exhibits all the benefits of the ReLU function. The ELU is differentiable,
    saturates for large negative inputs and reduces the bias shift. The negative saturation
    regime of ELU adds some robustness to noise as compared to the Leaky ReLU and
    Parametric ReLU. The ELU is extended to Scaled ELU (SELU) [[52](#bib.bib52)] by
    using a scaling hyperparameter to make the slope larger than one for positive
    inputs. The SELU can be defined as,
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º $[-1,\infty)$ï¼Œå…¶ä¸­ $\alpha$ æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚ELU å‡½æ•°å±•ç°äº† ReLU å‡½æ•°çš„æ‰€æœ‰ä¼˜ç‚¹ã€‚ELU æ˜¯å¯å¾®åˆ†çš„ï¼Œå¯¹å¤§è´Ÿè¾“å…¥é¥±å’Œï¼Œå¹¶ä¸”å‡å°‘äº†åç§»ã€‚ä¸
    Leaky ReLU å’Œå‚æ•°åŒ– ReLU ç›¸æ¯”ï¼ŒELU çš„è´Ÿé¥±å’ŒçŠ¶æ€å¯¹å™ªå£°å…·æœ‰ä¸€å®šçš„é²æ£’æ€§ã€‚é€šè¿‡ä½¿ç”¨ç¼©æ”¾è¶…å‚æ•°ä½¿å¾—æ­£è¾“å…¥çš„æ–œç‡å¤§äºä¸€ï¼ŒELU è¢«æ‰©å±•ä¸ºç¼©æ”¾
    ELUï¼ˆSELUï¼‰ [[52](#bib.bib52)]ã€‚SELU å¯ä»¥å®šä¹‰ä¸ºï¼Œ
- en: '|  | $SELU(x)=\lambda\times\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x}-1),&amp;x\leq
    0\end{cases}$ |  | (35) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $SELU(x)=\lambda\times\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x}-1),&amp;x\leq
    0\end{cases}$ |  | (35) |'
- en: having the output range in $[-\lambda,\infty)$ where $\alpha$ is a hyperparameter.
    Basically, the SELU induces self-normalization to automatically converge towards
    zero mean and unit variance. The Parametric ELU (PELU) [[54](#bib.bib54)] changes
    the saturation point and exponential decay and also regulates the slope of the
    linear function for the positive inputs for differentiability. The PELU AF can
    be written as,
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º $[-\lambda,\infty)$ï¼Œå…¶ä¸­ $\alpha$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚åŸºæœ¬ä¸Šï¼ŒSELU ä½¿å¾—è‡ªæˆ‘å½’ä¸€åŒ–è‡ªåŠ¨æ”¶æ•›åˆ°é›¶å‡å€¼å’Œå•ä½æ–¹å·®ã€‚å‚æ•°åŒ–
    ELUï¼ˆPELUï¼‰ [[54](#bib.bib54)] æ”¹å˜äº†é¥±å’Œç‚¹å’ŒæŒ‡æ•°è¡°å‡ï¼Œå¹¶ä¸”è°ƒèŠ‚äº†æ­£è¾“å…¥çš„çº¿æ€§å‡½æ•°çš„æ–œç‡ä»¥ä¾¿äºå¯å¾®åˆ†æ€§ã€‚PELU æ¿€æ´»å‡½æ•°å¯ä»¥å†™æˆï¼Œ
- en: '|  | $PELU(x)=\lambda\times\begin{cases}\frac{a}{b}\times x,&amp;x\geq 0\\
    a\times(e^{x/b}-1),&amp;x<0\end{cases}$ |  | (36) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $PELU(x)=\lambda\times\begin{cases}\frac{a}{b}\times x,&amp;x\geq 0\\
    a\times(e^{x/b}-1),&amp;x<0\end{cases}$ |  | (36) |'
- en: having $[-a,\infty)$ output range, where $a$ and $b$ are the trainable parameters.
    The parametric ELU is also explored in Continuously differentiable ELU (CELU)
    [[53](#bib.bib53)] for the negative inputs. The CELU is given as,
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º $[-a,\infty)$ï¼Œå…¶ä¸­ $a$ å’Œ $b$ æ˜¯å¯è®­ç»ƒçš„å‚æ•°ã€‚å‚æ•°åŒ– ELU è¿˜åœ¨è¿ç»­å¯å¾®åˆ† ELUï¼ˆCELUï¼‰ [[53](#bib.bib53)]
    ä¸­è¿›è¡Œäº†æ¢ç´¢ï¼Œé’ˆå¯¹è´Ÿè¾“å…¥ã€‚CELU å®šä¹‰ä¸ºï¼Œ
- en: '|  | $CELU(x)=\begin{cases}x,&amp;x\geq 0\\ \alpha\times(e^{x/\alpha}-1),&amp;x<0\end{cases}$
    |  | (37) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $CELU(x)=\begin{cases}x,&amp;x\geq 0\\ \alpha\times(e^{x/\alpha}-1),&amp;x<0\end{cases}$
    |  | (37) |'
- en: having the output range in $[-\alpha,\infty)$ where $\alpha$ is a learnable
    parameter. The PELU is also extended to multiple PELU (MPELU) [[55](#bib.bib55)]
    by using two learnable parameters to represent MPELU as either rectified, exponential
    or combined. The MPELU can be expressed as,
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º $[-\alpha,\infty)$ï¼Œå…¶ä¸­ $\alpha$ æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚PELU è¿˜è¢«æ‰©å±•ä¸ºå¤šå‚æ•° PELUï¼ˆMPELUï¼‰ [[55](#bib.bib55)]ï¼Œé€šè¿‡ä½¿ç”¨ä¸¤ä¸ªå¯å­¦ä¹ çš„å‚æ•°æ¥è¡¨ç¤º
    MPELU ä¸ºä¿®æ­£ã€æŒ‡æ•°æˆ–ç»„åˆå½¢å¼ã€‚MPELU å¯ä»¥è¡¨ç¤ºä¸ºï¼Œ
- en: '|  | $MPELU(x)=\begin{cases}x,&amp;x>0\\ \alpha_{c}\times(e^{\beta_{c}\times
    x}-1),&amp;x\leq 0\end{cases}$ |  | (38) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $MPELU(x)=\begin{cases}x,&amp;x>0\\ \alpha_{c}\times(e^{\beta_{c}\times
    x}-1),&amp;x\leq 0\end{cases}$ |  | (38) |'
- en: having the output range in $[-\alpha_{c},\infty)$, where $\alpha_{c}$ and $\beta_{c}$
    are the trainable parameters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-\alpha_{c},\infty)$ï¼Œå…¶ä¸­$\alpha_{c}$å’Œ$\beta_{c}$æ˜¯å¯è®­ç»ƒçš„å‚æ•°ã€‚
- en: A soft exponential AF interpolates between the exponential, linear and logarithmic
    functions using the trainable parameter [[70](#bib.bib70)]. A Shifted ELU (ShELU)
    AF is also explored as a locally optimal function [[71](#bib.bib71)]. A Parametric
    Rectified Exponential Unit (PREU) [[57](#bib.bib57)] is designed as,
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§è½¯æŒ‡æ•°æ¿€æ´»å‡½æ•°é€šè¿‡å¯è®­ç»ƒå‚æ•°åœ¨æŒ‡æ•°ã€çº¿æ€§å’Œå¯¹æ•°å‡½æ•°ä¹‹é—´è¿›è¡Œæ’å€¼[[70](#bib.bib70)]ã€‚è¿˜æ¢è®¨äº†ä¸€ç§Shifted ELU (ShELU)
    æ¿€æ´»å‡½æ•°ä½œä¸ºå±€éƒ¨æœ€ä¼˜å‡½æ•°[[71](#bib.bib71)]ã€‚ä¸€ç§å‚æ•°åŒ–çš„ä¿®æ­£æŒ‡æ•°å•å…ƒï¼ˆPREUï¼‰[[57](#bib.bib57)]å®šä¹‰ä¸ºï¼Œ
- en: '|  | $PREU(x)=\begin{cases}\alpha\times x,&amp;x>0\\ \alpha\times x\times e^{\beta\times
    x},&amp;x\leq 0\end{cases}$ |  | (39) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | $PREU(x)=\begin{cases}\alpha\times x,&amp;x>0\\ \alpha\times x\times e^{\beta\times
    x},&amp;x\leq 0\end{cases}$ |  | (39) |'
- en: having the output range in $[-1,\infty)$, where $\alpha$ and $\beta$ are the
    trainable parameters. The PREU utilizes the negative information near to zero
    effectively. The efficiency of ELU is improved in Fast ELU (FELU) AF [[56](#bib.bib56)]
    with the help of the simple displacement bits and integer algebra operations.
    The FELU is defined as,
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-1,\infty)$ï¼Œå…¶ä¸­$\alpha$å’Œ$\beta$æ˜¯å¯è®­ç»ƒçš„å‚æ•°ã€‚PREUæœ‰æ•ˆåœ°åˆ©ç”¨äº†æ¥è¿‘é›¶çš„è´Ÿä¿¡æ¯ã€‚ELUçš„æ•ˆç‡åœ¨Fast ELU
    (FELU)æ¿€æ´»å‡½æ•°[[56](#bib.bib56)]ä¸­å¾—åˆ°äº†æé«˜ï¼Œå€ŸåŠ©ç®€å•çš„ä½ç§»ä½å’Œæ•´æ•°ä»£æ•°è¿ç®—ã€‚FELUå®šä¹‰ä¸ºï¼Œ
- en: '|  | $FELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x/\ln(2)}-1),&amp;x\leq
    0\end{cases}$ |  | (40) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $FELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x/\ln(2)}-1),&amp;x\leq
    0\end{cases}$ |  | (40) |'
- en: having the output range in $[-\alpha,\infty)$ with $\alpha$ as a learnable parameter.
    Recently, the properties of ELU and RELU have been utilized to design an Elastic
    ELU (EELU) AF [[58](#bib.bib58)]. The EELU is defined as,
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-\alpha,\infty)$ï¼Œå…¶ä¸­$\alpha$æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚æœ€è¿‘ï¼ŒELUå’ŒRELUçš„ç‰¹æ€§è¢«ç”¨äºè®¾è®¡å¼¹æ€§ELUï¼ˆEELUï¼‰æ¿€æ´»å‡½æ•°[[58](#bib.bib58)]ã€‚EELUå®šä¹‰ä¸ºï¼Œ
- en: '|  | $EELU(x)=\begin{cases}k\times x,&amp;x>0\\ \alpha\times(e^{\beta\times
    x}-1),&amp;x\leq 0\end{cases}$ |  | (41) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $EELU(x)=\begin{cases}k\times x,&amp;x>0\\ \alpha\times(e^{\beta\times
    x}-1),&amp;x\leq 0\end{cases}$ |  | (41) |'
- en: having the output range in $[-\alpha,\infty)$ where $\alpha$ and $\beta$ are
    the trainable parameters. The EELU preserves a small non-zero gradient for the
    negative input and exhibits an elastic slope for the positive input. A Parametric
    Deformable ELU (PDELU) AF tries to shift the mean value of output closer to zero
    using the flexible map shape [[59](#bib.bib59)]. The PDELU is defined as,
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-\alpha,\infty)$ï¼Œå…¶ä¸­$\alpha$å’Œ$\beta$æ˜¯å¯è®­ç»ƒçš„å‚æ•°ã€‚EELUä¿æŒäº†ä¸€ä¸ªå°çš„éé›¶æ¢¯åº¦ç”¨äºè´Ÿè¾“å…¥ï¼Œå¹¶å¯¹æ­£è¾“å…¥å±•ç¤ºäº†å¼¹æ€§æ–œç‡ã€‚ä¸€ç§å‚æ•°åŒ–å¯å˜å½¢ELU
    (PDELU) æ¿€æ´»å‡½æ•°å°è¯•é€šè¿‡çµæ´»çš„æ˜ å°„å½¢çŠ¶å°†è¾“å‡ºçš„å‡å€¼å‘é›¶é è¿‘[[59](#bib.bib59)]ã€‚PDELUå®šä¹‰ä¸ºï¼Œ
- en: '|  | $PDELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times([1+(1-t)\times x]^{\frac{1}{1-t}}-1),&amp;x\leq
    0\end{cases}$ |  | (42) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $PDELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times([1+(1-t)\times x]^{\frac{1}{1-t}}-1),&amp;x\leq
    0\end{cases}$ |  | (42) |'
- en: having the output range in $[-1,\infty)$ where $\alpha$ is a learnable parameter.
    A ReLU-Memristor-like AF (RMAF) [[72](#bib.bib72)] uses two hyperparameters to
    have ReLU like shape for positive input and to give more importance to the negative
    values near to zero. An Exponential Linear Sigmoid SquasHing (ELiSH) is defined
    in [[73](#bib.bib73)] as,
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[-1,\infty)$ï¼Œå…¶ä¸­$\alpha$æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚ä¸€ç§ç±»ä¼¼ReLU-Memristorçš„æ¿€æ´»å‡½æ•°ï¼ˆRMAFï¼‰[[72](#bib.bib72)]ä½¿ç”¨ä¸¤ä¸ªè¶…å‚æ•°ï¼Œä»¥ä½¿å…¶å¯¹æ­£è¾“å…¥å…·æœ‰ç±»ä¼¼ReLUçš„å½¢çŠ¶ï¼Œå¹¶å¯¹æ¥è¿‘é›¶çš„è´Ÿå€¼ç»™äºˆæ›´å¤šé‡è§†ã€‚åœ¨[[73](#bib.bib73)]ä¸­å®šä¹‰äº†æŒ‡æ•°çº¿æ€§Sigmoid
    SquasHingï¼ˆELiSHï¼‰ï¼Œå…¶å…¬å¼ä¸ºï¼Œ
- en: '|  | $ELiSH(x)=\begin{cases}x/(1+e^{-x}),&amp;x\geq 0\\ (e^{x}-1)/(1+e^{-x}),&amp;x<0\end{cases}$
    |  | (43) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $ELiSH(x)=\begin{cases}x/(1+e^{-x}),&amp;x\geq 0\\ (e^{x}-1)/(1+e^{-x}),&amp;x<0\end{cases}$
    |  | (43) |'
- en: Moreover, it is also extended to HardELiSH which is a multiplication of HardSigmoid
    and Linear in the positive part and HardSigmoid and ELU in the negative part.
    Here, HardSigmoid is defined as,
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå®ƒè¿˜æ‰©å±•ä¸ºHardELiSHï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ­£éƒ¨åˆ†ä¹˜ä»¥HardSigmoidå’ŒLinearï¼Œåœ¨è´Ÿéƒ¨åˆ†ä¹˜ä»¥HardSigmoidå’ŒELUçš„å‡½æ•°ã€‚è¿™é‡Œï¼ŒHardSigmoidå®šä¹‰ä¸ºï¼Œ
- en: '|  | $HardELish(x)=max(0,min(1,(x+1)/2)).$ |  | (44) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | $HardELish(x)=max(0,min(1,(x+1)/2)).$ |  | (44) |'
- en: The ELU based AFs exploit the negative inputs without compromising with the
    non-linearity. Some ELU variants also modify the function for positive inputs
    to make it bounded.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºELUçš„æ¿€æ´»å‡½æ•°åœ¨ä¸å¦¥åéçº¿æ€§çš„æƒ…å†µä¸‹å……åˆ†åˆ©ç”¨è´Ÿè¾“å…¥ã€‚ä¸€äº›ELUå˜ä½“è¿˜ä¿®æ”¹äº†å¯¹æ­£è¾“å…¥çš„å‡½æ•°ä»¥ä½¿å…¶æœ‰ç•Œã€‚
- en: 'Table 5: Summary of adaptive and learning based activation functions.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨5ï¼šè‡ªé€‚åº”å’ŒåŸºäºå­¦ä¹ çš„æ¿€æ´»å‡½æ•°æ€»ç»“ã€‚
- en: '| Name | Parametric | Monotonic | Smooth | Bounded |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| åç§° | å‚æ•°åŒ– | å•è°ƒ | å¹³æ»‘ | æœ‰ç•Œ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Adaptive Piecewise Linear Unit (APL), 2015 [[28](#bib.bib28)] | Yes | No
    | No | No |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| è‡ªé€‚åº”åˆ†æ®µçº¿æ€§å•å…ƒï¼ˆAPLï¼‰ï¼Œ2015 [[28](#bib.bib28)] | æ˜¯ | å¦ | å¦ | å¦ |'
- en: '| Spline AF (SAF), 2016 [[74](#bib.bib74)] | Yes | Yes | Yes | No |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| æ ·æ¡æ¿€æ´»å‡½æ•°ï¼ˆSAFï¼‰ï¼Œ2016 [[74](#bib.bib74)] | æ˜¯ | æ˜¯ | æ˜¯ | å¦ |'
- en: '| Bi-Modal Derivative Adaptive Activation (BDAA), 2017 [[75](#bib.bib75)] |
    Yes | Yes | Yes | Yes |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| åŒæ¨¡æ€å¯¼æ•°è‡ªé€‚åº”æ¿€æ´»å‡½æ•°ï¼ˆBDAAï¼‰ï¼Œ2017 [[75](#bib.bib75)] | æ˜¯ | æ˜¯ | æ˜¯ | æ˜¯ |'
- en: '| Adaptive AF (AAF), 2018 [[76](#bib.bib76)] | Yes | Yes | No | No |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| è‡ªé€‚åº”æ¿€æ´»å‡½æ•°ï¼ˆAAFï¼‰ï¼Œ2018 [[76](#bib.bib76)] | æ˜¯ | æ˜¯ | å¦ | å¦ |'
- en: '| Swish, 2018 [[29](#bib.bib29)] | Yes | No | Yes | No |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Swishï¼Œ2018 [[29](#bib.bib29)] | æ˜¯ | å¦ | æ˜¯ | å¦ |'
- en: '| ESwish, 2018 [[77](#bib.bib77)] | Yes | No | Yes | No |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| ESwishï¼Œ2018 [[77](#bib.bib77)] | æ˜¯ | å¦ | æ˜¯ | å¦ |'
- en: '| Trainable AF (TAF), 2018 [[78](#bib.bib78)] | Yes | No | Yes | No |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| å¯è®­ç»ƒæ¿€æ´»å‡½æ•°ï¼ˆTAFï¼‰ï¼Œ2018 [[78](#bib.bib78)] | æ˜¯ | å¦ | æ˜¯ | å¦ |'
- en: '| Self-Learnable AF (SLAF), 2019 [[79](#bib.bib79)] | Yes | No | Yes | No |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| è‡ªå­¦ä¹ æ¿€æ´»å‡½æ•°ï¼ˆSLAFï¼‰ï¼Œ2019 [[79](#bib.bib79)] | æ˜¯ | å¦ | æ˜¯ | å¦ |'
- en: '| Mexican ReLU (MeLU), 2019 [[80](#bib.bib80)] | Yes | No | No | No |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| å¢¨è¥¿å“¥ ReLUï¼ˆMeLUï¼‰ï¼Œ2019 [[80](#bib.bib80)] | æ˜¯ | å¦ | å¦ | å¦ |'
- en: 6 Learning/Adaptive Activation Functions
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 å­¦ä¹ /è‡ªé€‚åº”æ¿€æ´»å‡½æ•°
- en: 'Most of the aforementioned AFs are not adaptive and might not be able to adjust
    based on the dataset complexity. This problem is tackled using learning/adaptive
    AFs as summarized in Table [5](#S5.T5 "Table 5 â€£ 5 Exponential Activation Functions
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark").
    Some of the earlier mentioned AFs are also adaptive, such as PReLU [[57](#bib.bib57)],
    SReLU [[65](#bib.bib65)], PTELU [[38](#bib.bib38)], MTLU [[46](#bib.bib46)], PELU
    [[54](#bib.bib54)], MPELU [[55](#bib.bib55)], PREU [[57](#bib.bib57)], EELU [[58](#bib.bib58)],
    PDELU [[59](#bib.bib59)], SRS [[26](#bib.bib26)], etc.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸Šè¿°å¤§å¤šæ•°æ¿€æ´»å‡½æ•°å¹¶ä¸æ˜¯è‡ªé€‚åº”çš„ï¼Œå¯èƒ½æ— æ³•æ ¹æ®æ•°æ®é›†çš„å¤æ‚æ€§è¿›è¡Œè°ƒæ•´ã€‚è¿™ä¸ªé—®é¢˜é€šè¿‡å­¦ä¹ /è‡ªé€‚åº”æ¿€æ´»å‡½æ•°å¾—åˆ°è§£å†³ï¼Œå¦‚è¡¨ [5](#S5.T5 "Table
    5 â€£ 5 Exponential Activation Functions â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") æ‰€æ€»ç»“çš„ã€‚ä¸€äº›æ—©æœŸæåˆ°çš„æ¿€æ´»å‡½æ•°ä¹Ÿæ˜¯è‡ªé€‚åº”çš„ï¼Œä¾‹å¦‚ PReLU [[57](#bib.bib57)]ã€SReLU
    [[65](#bib.bib65)]ã€PTELU [[38](#bib.bib38)]ã€MTLU [[46](#bib.bib46)]ã€PELU [[54](#bib.bib54)]ã€MPELU
    [[55](#bib.bib55)]ã€PREU [[57](#bib.bib57)]ã€EELU [[58](#bib.bib58)]ã€PDELU [[59](#bib.bib59)]ã€SRS
    [[26](#bib.bib26)] ç­‰ã€‚'
- en: The Adaptive Piecewise Linear (APL) is defined as a sum of hinge-shape functions
    [[28](#bib.bib28)]. It is given as,
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”åˆ†æ®µçº¿æ€§ï¼ˆAPLï¼‰å®šä¹‰ä¸ºä¸€ç»„é“°é“¾å½¢çŠ¶å‡½æ•°çš„å’Œ [[28](#bib.bib28)]ã€‚å…¶å…¬å¼ä¸ºï¼Œ
- en: '|  | $APL(x)=\text{max}(0,x)+\sum^{S}_{s=1}a_{s}\times\text{max}(0,b_{s}-x),$
    |  | (45) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $APL(x)=\text{max}(0,x)+\sum^{S}_{s=1}a_{s}\times\text{max}(0,b_{s}-x),$
    |  | (45) |'
- en: where $a$ and $b$ are the trainable parameters and $S$ is a hyperparameter representing
    the number of hinges. The output range of APL is $[0,\infty)$. Due to the trainable
    parameters, different neurons can learn different AFs.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $a$ å’Œ $b$ æ˜¯å¯è®­ç»ƒçš„å‚æ•°ï¼Œ$S$ æ˜¯è¡¨ç¤ºé“°é“¾æ•°é‡çš„è¶…å‚æ•°ã€‚APL çš„è¾“å‡ºèŒƒå›´æ˜¯ $[0,\infty)$ã€‚ç”±äºæœ‰å¯è®­ç»ƒå‚æ•°ï¼Œä¸åŒçš„ç¥ç»å…ƒå¯ä»¥å­¦ä¹ ä¸åŒçš„æ¿€æ´»å‡½æ•°ã€‚
- en: Ramachandran et al. [[29](#bib.bib29)] have performed an automatic search, which
    resulted in a Swish AF. It is defined as,
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Ramachandran ç­‰äºº [[29](#bib.bib29)] è¿›è¡Œäº†è‡ªåŠ¨æœç´¢ï¼Œç»“æœå¾—åˆ°äº† Swish æ¿€æ´»å‡½æ•°ã€‚å…¶å®šä¹‰ä¸ºï¼Œ
- en: '|  | $Swish(x)=x\times Sigmoid(\beta\times x)$ |  | (46) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $Swish(x)=x\times Sigmoid(\beta\times x)$ |  | (46) |'
- en: where $\beta$ is a learnable parameter. The output range of Swish is $(-\infty,\infty)$.
    Based on the learnt value of $\beta$ the shape of the Swish AF is adjusted between
    the linear and ReLU functions. The smaller and higher values of $\beta$ lead towards
    the linear and ReLU functions, respectively. Thus, it can control the amount of
    non-linearity based on the dataset and network complexity. Swish is also extended
    to E-Swish by multiplying the Swish with a learnable parameter to control the
    slope in the positive direction [[77](#bib.bib77)]. The E-Swish is defined as,
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\beta$ æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚Swish çš„è¾“å‡ºèŒƒå›´æ˜¯ $(-\infty,\infty)$ã€‚åŸºäº $\beta$ çš„å­¦ä¹ å€¼ï¼ŒSwish æ¿€æ´»å‡½æ•°çš„å½¢çŠ¶åœ¨çº¿æ€§å‡½æ•°å’Œ
    ReLU å‡½æ•°ä¹‹é—´è°ƒæ•´ã€‚è¾ƒå°å’Œè¾ƒé«˜çš„ $\beta$ å€¼åˆ†åˆ«è¶‹å‘äºçº¿æ€§å‡½æ•°å’Œ ReLU å‡½æ•°ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥æ ¹æ®æ•°æ®é›†å’Œç½‘ç»œå¤æ‚æ€§æ§åˆ¶éçº¿æ€§çš„ç¨‹åº¦ã€‚Swish
    ä¹Ÿé€šè¿‡å°† Swish ä¸ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ç›¸ä¹˜æ¥æ‰©å±•ä¸º E-Swishï¼Œä»¥æ§åˆ¶æ­£æ–¹å‘ä¸Šçš„æ–œç‡ [[77](#bib.bib77)]ã€‚E-Swish å®šä¹‰ä¸ºï¼Œ
- en: '|  | $ESwish(x)=\beta\times x\times Sigmoid(x)$ |  | (47) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $ESwish(x)=\beta\times x\times Sigmoid(x)$ |  | (47) |'
- en: having the output the range in $(-\infty,\infty)$ and $\beta$ is trainable parameter.
    A flatten-T Swish considers zero function for negative inputs similar to the ReLU
    [[81](#bib.bib81)]. The Adaptive Richardâ€™s Curve weighted Activation (ARiA) is
    also motivated from Swish and replaces the sigmoidal function with Richardâ€™s Curve
    [[82](#bib.bib82)]. The ARiA AF uses five hyper-parameters to control the shape
    of the non-linearity.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$(-\infty,\infty)$ï¼Œ$\beta$æ˜¯å¯è®­ç»ƒå‚æ•°ã€‚Flatten-T Swish å¯¹è´Ÿè¾“å…¥è€ƒè™‘é›¶å‡½æ•°ï¼Œç±»ä¼¼äº ReLU [[81](#bib.bib81)]ã€‚Adaptive
    Richardâ€™s Curve åŠ æƒæ¿€æ´»å‡½æ•°ï¼ˆARiAï¼‰ä¹Ÿå—åˆ° Swish çš„å¯å‘ï¼Œå°† sigmoidal å‡½æ•°æ›¿æ¢ä¸º Richardâ€™s Curve [[82](#bib.bib82)]ã€‚ARiA
    AF ä½¿ç”¨äº”ä¸ªè¶…å‚æ•°æ¥æ§åˆ¶éçº¿æ€§å½¢çŠ¶ã€‚
- en: The basic AFs are combined with learnable weights in adaptive AFs [[76](#bib.bib76)].
    The Adaptive AF (AAF) designed over PReLU [[35](#bib.bib35)] and PELU [[54](#bib.bib54)]
    is given as,
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ AFs ä¸å¯å­¦ä¹ æƒé‡ç»“åˆå½¢æˆè‡ªé€‚åº” AFs [[76](#bib.bib76)]ã€‚è®¾è®¡åœ¨ PReLU [[35](#bib.bib35)] å’Œ PELU
    [[54](#bib.bib54)] ä¸Šçš„ Adaptive AF (AAF) ä¸ºï¼Œ
- en: '|  | $AAF(x)=\sigma(w\times x)\times PRELU(x)+(1-\sigma(w\times x))\times PELU(x)$
    |  | (48) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $AAF(x)=\sigma(w\times x)\times PRELU(x)+(1-\sigma(w\times x))\times PELU(x)$
    |  | (48) |'
- en: having the output range in $[0,1]$, where $\sigma$ is the sigmoidal function
    and $w$ is a learnable parameter. In practice, AAF is costly as multiple AFs are
    involved. In [[83](#bib.bib83)], the AF for each neuron is selected from a library
    of AFs. In [[84](#bib.bib84)], different combinations of the identity function,
    ReLU, and Tanh are learnt automatically. In another attempt, an Adaptive Blending
    Unit (ABU) is defined to allow the networks to learn its preferred AFs [[85](#bib.bib85)].
    The ABU combines a set of AFs with trainable weights. A Lookup Table Unit (LuTU)
    function [[86](#bib.bib86)] uses a single period cosine mask based smoothing and
    linear interpolation using a set of anchor points. Activation ensembles are used
    at each layer in [[87](#bib.bib87)] with the contribution of each AF controlled
    by the trainable weights. Similarly, the Self-Learnable AF (SLAF) computes the
    sum of the different functions in an ensemble with the learnt coefficients [[79](#bib.bib79)].
    The SLAF can be expressed as,
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$[0,1]$ï¼Œå…¶ä¸­$\sigma$æ˜¯sigmoidalå‡½æ•°ï¼Œ$w$æ˜¯å¯å­¦ä¹ å‚æ•°ã€‚åœ¨å®è·µä¸­ï¼ŒAAF æˆæœ¬è¾ƒé«˜ï¼Œå› ä¸ºæ¶‰åŠå¤šä¸ª AFsã€‚åœ¨ [[83](#bib.bib83)]
    ä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒçš„ AF ä»ä¸€ä¸ª AFs åº“ä¸­é€‰æ‹©ã€‚åœ¨ [[84](#bib.bib84)] ä¸­ï¼Œä¸åŒçš„èº«ä»½å‡½æ•°ã€ReLU å’Œ Tanh ç»„åˆè¢«è‡ªåŠ¨å­¦ä¹ ã€‚åœ¨å¦ä¸€ä¸ªå°è¯•ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ª
    Adaptive Blending Unit (ABU) ä»¥å…è®¸ç½‘ç»œå­¦ä¹ å…¶é¦–é€‰çš„ AFs [[85](#bib.bib85)]ã€‚ABU å°†ä¸€ç»„ AFs ä¸å¯è®­ç»ƒæƒé‡ç»“åˆã€‚Lookup
    Table Unit (LuTU) å‡½æ•° [[86](#bib.bib86)] ä½¿ç”¨å•å‘¨æœŸä½™å¼¦æ©æ¨¡å¹³æ»‘å’Œçº¿æ€§æ’å€¼ï¼ŒåŸºäºä¸€ç»„é”šç‚¹ã€‚åœ¨ [[87](#bib.bib87)]
    ä¸­ï¼Œæ¯å±‚ä½¿ç”¨æ¿€æ´»é›†æˆï¼Œæ¯ä¸ª AF çš„è´¡çŒ®ç”±å¯è®­ç»ƒæƒé‡æ§åˆ¶ã€‚åŒæ ·ï¼ŒSelf-Learnable AF (SLAF) è®¡ç®—é›†æˆä¸­ä¸åŒå‡½æ•°çš„åŠ æƒå’Œï¼Œå…¶ç³»æ•°ç”±å­¦ä¹ å¾—å‡º
    [[79](#bib.bib79)]ã€‚SLAF å¯ä»¥è¡¨ç¤ºä¸ºï¼Œ
- en: '|  | $SLAF(x)=\sum_{i=0}^{N-1}a_{i}\times x^{i}$ |  | (49) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $SLAF(x)=\sum_{i=0}^{N-1}a_{i}\times x^{i}$ |  | (49) |'
- en: in the output range of $(-\infty,\infty)$, where $a_{i}$ is the trainable parameter.
    A Mexican ReLU (MeLU) AF is proposed in [[80](#bib.bib80)] by using a â€œMexican
    hat typeâ€ function and given as,
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$(-\infty,\infty)$ï¼Œå…¶ä¸­$a_{i}$æ˜¯å¯è®­ç»ƒå‚æ•°ã€‚[[80](#bib.bib80)]ä¸­æå‡ºäº†ä¸€ç§å¢¨è¥¿å“¥ ReLU (MeLU)
    AFï¼Œä½¿ç”¨äº†â€œå¢¨è¥¿å“¥å¸½å‹â€å‡½æ•°ï¼Œè¡¨è¾¾å¼ä¸ºï¼Œ
- en: '|  | $MeLU(x)=PReLU(x)+\sum_{j=1}^{k}{c_{j}\times\max(\lambda_{j}-&#124;x-a_{j}&#124;,0)}$
    |  | (50) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $MeLU(x)=PReLU(x)+\sum_{j=1}^{k}{c_{j}\times\max(\lambda_{j}-\|x-a_{j}\|,0)}$
    |  | (50) |'
- en: in the output range of $(-\infty,\infty)$, where $c_{j}$ is the trainable parameter
    and $\lambda_{j}$ & $a_{j}$ are the real numbers.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º$(-\infty,\infty)$ï¼Œå…¶ä¸­$c_{j}$æ˜¯å¯è®­ç»ƒå‚æ•°ï¼Œ$\lambda_{j}$å’Œ$a_{j}$æ˜¯å®æ•°ã€‚
- en: A cubic spline interpolation is also used to learn the AF from data [[74](#bib.bib74)]
    which is given as,
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ç«‹æ–¹æ ·æ¡æ’å€¼ä¹Ÿç”¨äºä»æ•°æ®ä¸­å­¦ä¹  AF [[74](#bib.bib74)]ï¼Œå…¶è¡¨è¾¾å¼ä¸ºï¼Œ
- en: '|  | $SAF(x)=\Phi(s;\textbf{q})$ |  | (51) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $SAF(x)=\Phi(s;\textbf{q})$ |  | (51) |'
- en: having the output range in $(-\infty,\infty)$ where $\Phi(.)$ is parameterized
    by a vector q cubic in nature. Fourier series basis expansion is used for nonparametrically
    learning AFs (NPF) [[88](#bib.bib88)]. Hyperactivations utilize a hypernetwork
    on top of an activation network, which are used to explore the AFs search space
    [[89](#bib.bib89)]. A shallow neural network is used in the activation network
    to produce the output for each input, whereas a neural network is used in the
    hypernetwork to produce weights for another network. A bi-modal derivative adaptive
    activation (BDAA) function uses twin maxima derivative sigmoidal function [[75](#bib.bib75)]
    by controlling the maximaâ€™s position with an adaptive parameter. The BDAA is given
    as,
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŒƒå›´ä¸º $(-\infty,\infty)$ï¼Œå…¶ä¸­ $\Phi(.)$ ç”±ä¸€ä¸ªä¸‰æ¬¡å‘é‡å‚æ•°åŒ–ã€‚å‚…é‡Œå¶çº§æ•°å±•å¼€ç”¨äºéå‚æ•°å­¦ä¹  AF (NPF) [[88](#bib.bib88)]ã€‚è¶…æ¿€æ´»åˆ©ç”¨äº†ä¸€ä¸ªåœ¨æ¿€æ´»ç½‘ç»œä¹‹ä¸Šçš„è¶…ç½‘ç»œï¼Œç”¨äºæ¢ç´¢
    AF æœç´¢ç©ºé—´ [[89](#bib.bib89)]ã€‚åœ¨æ¿€æ´»ç½‘ç»œä¸­ä½¿ç”¨æµ…å±‚ç¥ç»ç½‘ç»œä¸ºæ¯ä¸ªè¾“å…¥ç”Ÿæˆè¾“å‡ºï¼Œè€Œåœ¨è¶…ç½‘ç»œä¸­ä½¿ç”¨ç¥ç»ç½‘ç»œä¸ºå¦ä¸€ä¸ªç½‘ç»œç”Ÿæˆæƒé‡ã€‚åŒæ¨¡å¯¼æ•°è‡ªé€‚åº”æ¿€æ´»
    (BDAA) å‡½æ•°ä½¿ç”¨åŒææå€¼å¯¼æ•° sigmoid å‡½æ•° [[75](#bib.bib75)]ï¼Œé€šè¿‡è‡ªé€‚åº”å‚æ•°æ§åˆ¶æå€¼ä½ç½®ã€‚BDAA å®šä¹‰ä¸ºï¼Œ
- en: '|  | $BDAA(x)=\frac{1}{2}\times\left(\frac{1}{1+e^{-x}}-\frac{1}{1+e^{-x-a}}\right)$
    |  | (52) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $BDAA(x)=\frac{1}{2}\times\left(\frac{1}{1+e^{-x}}-\frac{1}{1+e^{-x-a}}\right)$
    |  | (52) |'
- en: in the output range of $[0,1]$ where $a$ is a learnable parameter. The authors
    have exploited the Bi-modal derivatives on four AFs. Linear regression is used
    in [[78](#bib.bib78)] to train AF for each neuron which results in different AFs
    for the different neurons. The TAF is defined as,
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ $[0,1]$ çš„è¾“å‡ºèŒƒå›´å†…ï¼Œå…¶ä¸­ $a$ æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ã€‚ä½œè€…åœ¨å››ç§ AF ä¸Šåˆ©ç”¨äº†åŒæ¨¡å¯¼æ•°ã€‚ [[78](#bib.bib78)] ä½¿ç”¨çº¿æ€§å›å½’è®­ç»ƒæ¯ä¸ªç¥ç»å…ƒçš„
    AFï¼Œè¿™ä¼šå¯¼è‡´ä¸åŒç¥ç»å…ƒä½¿ç”¨ä¸åŒçš„ AFã€‚TAF å®šä¹‰ä¸ºï¼Œ
- en: '|  | $TAF(x)=\sqrt{(x-a)^{2}+b^{2}}$ |  | (53) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | $TAF(x)=\sqrt{(x-a)^{2}+b^{2}}$ |  | (53) |'
- en: in the output range of $[b,\infty)$, where $a$ and $b$ are the trainable parameters.
    Recently, a trainable parameter was used in different non-adaptive AFs such as
    Sigmoid, Tanh, and ReLU to make it adaptive [[90](#bib.bib90)].
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ $[b,\infty)$ çš„è¾“å‡ºèŒƒå›´å†…ï¼Œå…¶ä¸­ $a$ å’Œ $b$ æ˜¯å¯è®­ç»ƒçš„å‚æ•°ã€‚æœ€è¿‘ï¼ŒæŸäº›éè‡ªé€‚åº” AF å¦‚ Sigmoidã€Tanh å’Œ ReLU
    ä¸­ä½¿ç”¨äº†å¯è®­ç»ƒå‚æ•°ï¼Œä»¥ä½¿å…¶é€‚åº” [[90](#bib.bib90)]ã€‚
- en: 'The adaptive and trainable AFs are the recent trend to adjust the non-linearity
    based on the data and network complexity. However, the minimal burden is increased
    in terms of the increased number of parameters. Though the complexity of tunable
    AFs is relatively increased w.r.t. non-tunable AFs, it is negligible w.r.t. all
    parameters of the entire network in practice. The same is also observed experimentally
    as reported in Table [10](#S9.T10 "Table 10 â€£ 9.2 Experimental Performance Analysis
    â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") in terms of the training time.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 'è‡ªé€‚åº”å’Œå¯è®­ç»ƒçš„ AF æ˜¯æ ¹æ®æ•°æ®å’Œç½‘ç»œå¤æ‚æ€§è°ƒæ•´éçº¿æ€§çš„æœ€æ–°è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œå‚æ•°æ•°é‡å¢åŠ å¸¦æ¥çš„æœ€å°è´Ÿæ‹…æœ‰æ‰€å¢åŠ ã€‚å°½ç®¡å¯è°ƒ AF çš„å¤æ‚æ€§ç›¸å¯¹é«˜äºä¸å¯è°ƒ
    AFï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä¸æ•´ä¸ªç½‘ç»œçš„æ‰€æœ‰å‚æ•°ç›¸æ¯”ï¼Œå¯ä»¥å¿½ç•¥ä¸è®¡ã€‚å®éªŒä¸Šä¹Ÿæœ‰ç±»ä¼¼è§‚å¯Ÿï¼Œè§è¡¨ [10](#S9.T10 "Table 10 â€£ 9.2 Experimental
    Performance Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark") çš„è®­ç»ƒæ—¶é—´ã€‚'
- en: 7 Miscellaneous Activation Functions
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 å…¶ä»–æ¿€æ´»å‡½æ•°
- en: This section covers other attempts in AFs such as Softplus, Probabilistic, Polynomial,
    Subnetwork and Kernel.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æ¶µç›–äº†å…¶ä»– AF å°è¯•ï¼Œå¦‚ Softplusã€æ¦‚ç‡ã€æ•´æ•°ã€å­ç½‘ç»œå’Œæ ¸å‡½æ•°ã€‚
- en: 7.1 Softplus Activation Functions
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 Softplus æ¿€æ´»å‡½æ•°
- en: The softplus function [[91](#bib.bib91)] was proposed in 2001 as $\log(e^{x}+1)$
    and mostly used in statistical applications. After the breakthrough of deep learning
    the softmax function is used as the AF [[92](#bib.bib92)]. Softmax function produces
    the categorical probability distribution equivalent output. Softplus unit based
    AF is also used in deep neural networks [[93](#bib.bib93)]. The smooth nature
    of the Softplus facilitates the differentiability. The noisy softplus AF [[94](#bib.bib94)]
    is suitable for the spiking neural networks (SNNs). A Softplus Linear Unit (SLU)
    is also proposed by considering softplus with rectified unit [[95](#bib.bib95)].
    The SLU AF is defined as,
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Softplus å‡½æ•° [[91](#bib.bib91)] äº 2001 å¹´æå‡ºï¼Œå…¶å½¢å¼ä¸º $\log(e^{x}+1)$ï¼Œä¸»è¦ç”¨äºç»Ÿè®¡åº”ç”¨ã€‚éšç€æ·±åº¦å­¦ä¹ çš„çªç ´ï¼Œsoftmax
    å‡½æ•°ä½œä¸º AF è¢«ä½¿ç”¨ [[92](#bib.bib92)]ã€‚Softmax å‡½æ•°ç”Ÿæˆç­‰æ•ˆçš„åˆ†ç±»æ¦‚ç‡åˆ†å¸ƒè¾“å‡ºã€‚åŸºäº Softplus å•å…ƒçš„ AF ä¹Ÿåœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨
    [[93](#bib.bib93)]ã€‚Softplus çš„å¹³æ»‘ç‰¹æ€§æœ‰åŠ©äºå¯å¾®æ€§ã€‚å˜ˆæ‚çš„ softplus AF [[94](#bib.bib94)] é€‚ç”¨äºå°–å³°ç¥ç»ç½‘ç»œ
    (SNNs)ã€‚Softplus çº¿æ€§å•å…ƒ (SLU) ä¹Ÿé€šè¿‡è€ƒè™‘å¸¦æœ‰æ•´æµå•å…ƒçš„ softplus æå‡º [[95](#bib.bib95)]ã€‚SLU AF
    å®šä¹‰ä¸ºï¼Œ
- en: '|  | $SLU(x)=\begin{cases}\alpha\times x,&amp;x\geq 0\\ \beta\times\log(e^{x}+1)-\gamma,&amp;x<0\end{cases}$
    |  | (54) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $SLU(x)=\begin{cases}\alpha\times x,&amp;x\geq 0\\ \beta\times\log(e^{x}+1)-\gamma,&amp;x<0\end{cases}$
    |  | (54) |'
- en: where $\alpha$, $\beta$ and $\gamma$ are the trainable parameters with $\alpha$
    controlling the slope in the positive direction, $\beta$ controlling the saturation
    points in the negative direction and $\gamma$ controlling the offset in the negative
    direction w.r.t. the horizontal axis. The Rectified Softplus (ReSP) AF introduces
    the rectification for positive input in Softplus activation [[96](#bib.bib96)].
    In order to make the softplus function to follow the zero mean, a shifting and
    scaling of the outputs is performed in [[97](#bib.bib97)]. A Rand Softplus (RSP)
    AF models the stochasticity-adaptability of biological neurons as,
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\alpha$ã€$\beta$ å’Œ $\gamma$ æ˜¯å¯è®­ç»ƒçš„å‚æ•°ï¼Œå…¶ä¸­ $\alpha$ æ§åˆ¶æ­£æ–¹å‘çš„æ–œç‡ï¼Œ$\beta$ æ§åˆ¶è´Ÿæ–¹å‘çš„é¥±å’Œç‚¹ï¼Œ$\gamma$
    æ§åˆ¶ç›¸å¯¹äºæ°´å¹³è½´çš„è´Ÿæ–¹å‘åç§»ã€‚Rectified Softplusï¼ˆReSPï¼‰æ¿€æ´»å‡½æ•°å¼•å…¥äº†å¯¹ Softplus æ¿€æ´»çš„æ­£è¾“å…¥çš„ä¿®æ­£[[96](#bib.bib96)]ã€‚ä¸ºäº†ä½¿
    softplus å‡½æ•°ç¬¦åˆé›¶å‡å€¼ï¼Œåœ¨ [[97](#bib.bib97)] ä¸­å¯¹è¾“å‡ºè¿›è¡Œäº†å¹³ç§»å’Œç¼©æ”¾ã€‚Rand Softplusï¼ˆRSPï¼‰æ¿€æ´»å‡½æ•°å°†ç”Ÿç‰©ç¥ç»å…ƒçš„éšæœºé€‚åº”æ€§å»ºæ¨¡ä¸ºï¼Œ
- en: '|  | $RSP(x)=(1-\rho)\times\max(0,x)+\rho\times\log(1+e^{x})$ |  | (55) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | $RSP(x)=(1-\rho)\times\max(0,x)+\rho\times\log(1+e^{x})$ |  | (55) |'
- en: where $\rho$ is a stochastic hyperparameter [[98](#bib.bib98)]. It improves
    the capability of the network towards the noise. The softplus function is also
    used with Tanh function in Mish activation function [[99](#bib.bib99)], which
    is given as,
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\rho$ æ˜¯ä¸€ä¸ªéšæœºè¶…å‚æ•°[[98](#bib.bib98)]ã€‚å®ƒæé«˜äº†ç½‘ç»œå¯¹å™ªå£°çš„èƒ½åŠ›ã€‚softplus å‡½æ•°è¿˜ä¸ Tanh å‡½æ•°ä¸€èµ·ç”¨äº
    Mish æ¿€æ´»å‡½æ•°[[99](#bib.bib99)]ï¼Œå…¶å®šä¹‰ä¸ºï¼Œ
- en: '|  | $Mish(x)=x\times Tanh(Softplus(x)).$ |  | (56) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $Mish(x)=x\times Tanh(Softplus(x)).$ |  | (56) |'
- en: The Mish is a non-monotonic and smooth AF. It has recently been used by the
    YOLOv4 model for object detection [[100](#bib.bib100)]. However, the increased
    complexity in Mish due to the multiple functions can be a limitation for the deep
    networks.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Mish æ˜¯ä¸€ç§éå•è°ƒä¸”å¹³æ»‘çš„æ¿€æ´»å‡½æ•°ã€‚æœ€è¿‘ï¼ŒYOLOv4 æ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹ä¸­ä½¿ç”¨äº†å®ƒ[[100](#bib.bib100)]ã€‚ç„¶è€Œï¼Œç”±äºå¤šé‡å‡½æ•°çš„å¢åŠ ï¼ŒMish
    çš„å¤æ‚æ€§å¯èƒ½å¯¹æ·±åº¦ç½‘ç»œæ„æˆé™åˆ¶ã€‚
- en: 7.2 Probabilistic Activation Functions
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 æ¦‚ç‡æ¿€æ´»å‡½æ•°
- en: So far, stochastic AFs have not been much explored due to expensive sampling
    processes. Few AFs exist in this category such as Randomized ReLU (RReLU) [[50](#bib.bib50)],
    Elastic ReLU (EReLU) [[40](#bib.bib40)], Randomly Translational ReLU (RTReLU)
    [[41](#bib.bib41)] and Gaussian Error Linear Unit (GELU) [[101](#bib.bib101)].
    GELU [[101](#bib.bib101)] considers nonlinearity as the stochastic regularization
    driven transformation and defined as,
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œç”±äºé‡‡æ ·è¿‡ç¨‹æ˜‚è´µï¼Œéšæœºæ¿€æ´»å‡½æ•°å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™ç±»æ¿€æ´»å‡½æ•°å¾ˆå°‘ï¼Œå¦‚éšæœºåŒ– ReLUï¼ˆRReLUï¼‰[[50](#bib.bib50)]ã€å¼¹æ€§ ReLUï¼ˆEReLUï¼‰[[40](#bib.bib40)]ã€éšæœºå¹³ç§»
    ReLUï¼ˆRTReLUï¼‰[[41](#bib.bib41)] å’Œé«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒï¼ˆGELUï¼‰[[101](#bib.bib101)]ã€‚GELU [[101](#bib.bib101)]
    å°†éçº¿æ€§è§†ä¸ºç”±éšæœºæ­£åˆ™åŒ–é©±åŠ¨çš„å˜æ¢ï¼Œå¹¶å®šä¹‰ä¸ºï¼Œ
- en: '|  | $GELU(x)=x\times P(X\leq x).$ |  | (57) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | $GELU(x)=x\times P(X\leq x).$ |  | (57) |'
- en: where $P$ is the probability. The complexity of GELU increases due to use of
    probabilistic nature. The GELU is also extended to the Symmetrical Gaussian Error
    Linear Unit (SGELU) [[102](#bib.bib102)] to enhance its ability of bidirectional
    convergence. Doubly truncated Gaussian distributions [[103](#bib.bib103)] is a
    family of nonlinearities which can generate different AFs such as Sigmoid, Tanh
    and ReLU by setting the appropriate truncation points. Probabilistic AF (ProbAct)
    introduces the adaptable and trainable variance in the ReLUâ€™s output [[104](#bib.bib104)].
    It leads to the generalization of the models. However, all other drawbacks of
    ReLU exist with ProbAct also.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $P$ æ˜¯æ¦‚ç‡ã€‚ç”±äºä½¿ç”¨äº†æ¦‚ç‡æ€§è´¨ï¼ŒGELU çš„å¤æ‚æ€§å¢åŠ ã€‚GELU è¿˜è¢«æ‰©å±•ä¸ºå¯¹ç§°é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒï¼ˆSGELUï¼‰[[102](#bib.bib102)]ï¼Œä»¥å¢å¼ºå…¶åŒå‘æ”¶æ•›èƒ½åŠ›ã€‚åŒé‡æˆªæ–­é«˜æ–¯åˆ†å¸ƒ[[103](#bib.bib103)]
    æ˜¯ä¸€ç§éçº¿æ€§å‡½æ•°æ—ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®é€‚å½“çš„æˆªæ–­ç‚¹ç”Ÿæˆä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œå¦‚ Sigmoidã€Tanh å’Œ ReLUã€‚æ¦‚ç‡æ¿€æ´»å‡½æ•°ï¼ˆProbActï¼‰å¼•å…¥äº† ReLU
    è¾“å‡ºä¸­çš„å¯é€‚åº”å’Œå¯è®­ç»ƒçš„æ–¹å·®[[104](#bib.bib104)]ï¼Œè¿™å¯¼è‡´äº†æ¨¡å‹çš„æ³›åŒ–ã€‚ç„¶è€Œï¼ŒProbAct ä¹Ÿå­˜åœ¨ ReLU çš„æ‰€æœ‰å…¶ä»–ç¼ºé™·ã€‚
- en: 7.3 Polynomial Activation Functions
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 å¤šé¡¹å¼æ¿€æ´»å‡½æ•°
- en: Smooth Adaptive AF (SAAF) is defined as the piecewise polynomial function [[105](#bib.bib105)].
    Two power functions symmetric to the linear part of ReLU are combined in [[106](#bib.bib106)]
    to improve the performance of ReLU. A piecewise polynomial approximation based
    AF is also learnt from the data [[107](#bib.bib107)]. This activation leads to
    the light-weight models suitable for the FPGAs and microcontrollers. The AF is
    also treated as the cumulative distribution function [[108](#bib.bib108)]. The
    ReLU is also extended to a Rectified Power Unit (RePU) for positive inputs as,
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³æ»‘è‡ªé€‚åº” AF (SAAF) å®šä¹‰ä¸ºåˆ†æ®µå¤šé¡¹å¼å‡½æ•° [[105](#bib.bib105)]ã€‚å°†å¯¹ç§°äº ReLU çº¿æ€§éƒ¨åˆ†çš„ä¸¤ä¸ªå¹‚å‡½æ•°ç»„åˆåœ¨ [[106](#bib.bib106)]
    ä¸­ï¼Œä»¥æé«˜ ReLU çš„æ€§èƒ½ã€‚è¿˜ä»æ•°æ®ä¸­å­¦ä¹ äº†ä¸€ç§åŸºäºåˆ†æ®µå¤šé¡¹å¼è¿‘ä¼¼çš„ AF [[107](#bib.bib107)]ã€‚è¿™ç§æ¿€æ´»å‡½æ•°å¯¼è‡´é€‚ç”¨äº FPGA å’Œå¾®æ§åˆ¶å™¨çš„è½»é‡çº§æ¨¡å‹ã€‚AF
    è¿˜è¢«è§†ä¸ºç´¯ç§¯åˆ†å¸ƒå‡½æ•° [[108](#bib.bib108)]ã€‚ReLU ä¹Ÿæ‰©å±•ä¸ºæ­£è¾“å…¥çš„ä¿®æ­£å¹‚å•å…ƒ (RePU) å¦‚ä¸‹ï¼Œ
- en: '|  | $RePU(x)=\begin{cases}x^{s},&amp;x\geq 0\\ 0,&amp;x<0\end{cases}$ |  |
    (58) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | $RePU(x)=\begin{cases}x^{s},&amp;x\geq 0\\ 0,&amp;x<0\end{cases}$ |  |
    (58) |'
- en: where $s$ is a hyperparameter [[109](#bib.bib109)]. The RePU is suitable for
    smoother gradients near zero. However, vanishing gradient, unbounded and asymmetric
    nature are the downsides of RePU. The rational function of polynomials is better
    suited as compared to the polynomial functions in order to approximate the ReLU
    [[110](#bib.bib110)]. Recently, a PadÃ© approximation is used to develop a non-smooth
    PadÃ© Activation Unit (PAU) [[111](#bib.bib111)] as,
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $s$ æ˜¯ä¸€ä¸ªè¶…å‚æ•° [[109](#bib.bib109)]ã€‚RePU é€‚åˆäºæ¥è¿‘é›¶çš„å¹³æ»‘æ¢¯åº¦ã€‚ç„¶è€Œï¼ŒRePU çš„ç¼ºç‚¹åŒ…æ‹¬æ¢¯åº¦æ¶ˆå¤±ã€æ— é™åˆ¶å’Œéå¯¹ç§°æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤šé¡¹å¼çš„æœ‰ç†å‡½æ•°æ›´é€‚åˆç”¨æ¥è¿‘ä¼¼
    ReLU [[110](#bib.bib110)]ã€‚æœ€è¿‘ï¼Œä½¿ç”¨ PadÃ© è¿‘ä¼¼æ¥å¼€å‘éå…‰æ»‘çš„ PadÃ© æ¿€æ´»å•å…ƒ (PAU) [[111](#bib.bib111)]
    å¦‚ä¸‹ï¼Œ
- en: '|  | $PAU(x)=\frac{P(x)}{Q(x)}$ |  | (59) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $PAU(x)=\frac{P(x)}{Q(x)}$ |  | (59) |'
- en: where $P(x)$ and $Q(x)$ are two polynomials of order $m$ and $n$, respectively.
    The PAUs can approximate the commonly used hand-designed AFs. Moreover, it can
    also learn the new AFs with compact representations. Recently, a Rational AF (RAF)
    [[112](#bib.bib112)] was proposed to tackle the problem of non-smooth nature of
    the PAU function.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $P(x)$ å’Œ $Q(x)$ æ˜¯ä¸¤ä¸ªåˆ†åˆ«ä¸º $m$ å’Œ $n$ æ¬¡çš„å¤šé¡¹å¼ã€‚PAUs å¯ä»¥è¿‘ä¼¼å¸¸ç”¨çš„æ‰‹åŠ¨è®¾è®¡çš„ AFsã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥å­¦ä¹ å…·æœ‰ç´§å‡‘è¡¨ç¤ºçš„æ–°
    AFsã€‚æœ€è¿‘ï¼Œæå‡ºäº†ä¸€ç§æœ‰ç† AF (RAF) [[112](#bib.bib112)] æ¥è§£å†³ PAU å‡½æ•°çš„éå…‰æ»‘æ€§é—®é¢˜ã€‚
- en: 7.4 Activations as a Subnetwork
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 ä½œä¸ºå­ç½‘ç»œçš„æ¿€æ´»
- en: A Variable AF (VAF) is used as a subnetwork of ReLUs [[113](#bib.bib113)]. It
    uses the ensemble of ReLUs in a subnetwork using learnable parameters. In a very
    similar approach, the maximum of multiple linear functions is used in the Dynamic
    ReLU (DY-ReLU) [[114](#bib.bib114)]. In Wide Hidden Expansion (WHE) [[115](#bib.bib115)],
    each WHE intermediate channel is followed by one AF before connecting to the output
    channel to increase the non-linearity of the network. An AF Unit (AFU) [[116](#bib.bib116)]
    uses a small neural network to model the activation. All neurons in the original
    network share the weights in AFU. The advantage of the AFU is that different AFs
    can be learnt by different layers.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: å˜é‡ AF (VAF) ç”¨ä½œ ReLUs çš„å­ç½‘ç»œ [[113](#bib.bib113)]ã€‚å®ƒä½¿ç”¨å…·æœ‰å¯å­¦ä¹ å‚æ•°çš„ ReLUs çš„é›†æˆã€‚åœ¨éå¸¸ç±»ä¼¼çš„æ–¹æ³•ä¸­ï¼Œå¤šä¸ªçº¿æ€§å‡½æ•°çš„æœ€å¤§å€¼è¢«ç”¨äºåŠ¨æ€
    ReLU (DY-ReLU) [[114](#bib.bib114)]ã€‚åœ¨å®½éšè—æ‰©å±• (WHE) [[115](#bib.bib115)] ä¸­ï¼Œæ¯ä¸ª WHE
    ä¸­é—´é€šé“ä¹‹åè·Ÿéšä¸€ä¸ª AFï¼Œç„¶åè¿æ¥åˆ°è¾“å‡ºé€šé“ï¼Œä»¥å¢åŠ ç½‘ç»œçš„éçº¿æ€§ã€‚AF å•å…ƒ (AFU) [[116](#bib.bib116)] ä½¿ç”¨ä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œæ¥å»ºæ¨¡æ¿€æ´»ã€‚åŸå§‹ç½‘ç»œä¸­çš„æ‰€æœ‰ç¥ç»å…ƒåœ¨
    AFU ä¸­å…±äº«æƒé‡ã€‚AFU çš„ä¼˜åŠ¿åœ¨äºä¸åŒçš„ AFs å¯ä»¥åœ¨ä¸åŒçš„å±‚ä¸­å­¦ä¹ ã€‚
- en: 7.5 Kernel Activation Functions
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 æ ¸æ¿€æ´»å‡½æ•°
- en: A Kernel-based non-parametric AF (KAF) [[117](#bib.bib117)] uses an inexpensive
    kernel expansion to make the activation flexible. The KAF is further extended
    to multikernel AFs (multi-KAF) [[118](#bib.bib118)]. Several AFs are also introduced
    for complex valued neural networks [[119](#bib.bib119)], [[120](#bib.bib120)],
    [[121](#bib.bib121)].
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ ¸çš„éå‚æ•° AF (KAF) [[117](#bib.bib117)] ä½¿ç”¨ä¸€ç§å»‰ä»·çš„æ ¸æ‰©å±•æ¥ä½¿æ¿€æ´»æ›´åŠ çµæ´»ã€‚KAF è¿›ä¸€æ­¥æ‰©å±•ä¸ºå¤šæ ¸ AFs (multi-KAF)
    [[118](#bib.bib118)]ã€‚è¿˜ä¸ºå¤æ•°å€¼ç¥ç»ç½‘ç»œå¼•å…¥äº†å‡ ç§ AFs [[119](#bib.bib119)], [[120](#bib.bib120)],
    [[121](#bib.bib121)]ã€‚
- en: 'Table 6: Summary of the existing state-of-the-art activation functions.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 6: ç°æœ‰æœ€å…ˆè¿›çš„æ¿€æ´»å‡½æ•°æ€»ç»“ã€‚'
- en: '| Activation | Models | Datasets | Insights and Remarks |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| æ¿€æ´»å‡½æ•° | æ¨¡å‹ | æ•°æ®é›† | è§è§£ä¸å¤‡æ³¨ |'
- en: '| On Image Datasets |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| å›¾åƒæ•°æ®é›† |'
- en: '| Wide Hidden Expansion (WHE) - 2020 [[115](#bib.bib115)] | ResNet, SENet,
    and MobileNet | CIFAR100 and ImageNet classification, Pascal VOC 2007 and COCO
    detection | Upto 2% higher Top-1 accuracy than baseline models of recognition
    and detection |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| å®½éšè—æ‰©å±•ï¼ˆWHEï¼‰ - 2020 [[115](#bib.bib115)] | ResNetã€SENetå’ŒMobileNet | CIFAR100å’ŒImageNetåˆ†ç±»ã€Pascal
    VOC 2007å’ŒCOCOæ£€æµ‹ | æ¯”åŸºå‡†æ¨¡å‹çš„è¯†åˆ«å’Œæ£€æµ‹Top-1å‡†ç¡®ç‡é«˜å‡ºæœ€å¤š2%ã€‚ |'
- en: '| Soft-Root-Sign (SRS) - 2020 [[26](#bib.bib26)] | VGG and MobileNet | CIFAR10
    and CIFAR100 classification | The SRS is better with MobileNet over both datasets
    and with VGG over CIFAR100\. The LReLU is better with VGG over CIFAR10. |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Soft-Root-Signï¼ˆSRSï¼‰ - 2020 [[26](#bib.bib26)] | VGGå’ŒMobileNet | CIFAR10å’ŒCIFAR100åˆ†ç±»
    | SRSåœ¨MobileNetä¸Šçš„è¡¨ç°ä¼˜äºè¿™ä¸¤ä¸ªæ•°æ®é›†ï¼Œè€Œåœ¨VGGä¸Šå¯¹CIFAR100æ›´å¥½ã€‚LReLUåœ¨VGGä¸Šå¯¹CIFAR10æ›´å¥½ã€‚ |'
- en: '| Relu-Memristor-Like AF (RMAF) - 2020 [[72](#bib.bib72)] | ResNet, AlexNet,
    SqueezeNet, and DenseNet | CIFAR10, CIFAR100, MNIST and ImageNet classification
    | The RMAF performs better than the ReLU, ELU, SELU, PReLU, Tanh and Swish. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| Relu-å¿†é˜»å™¨æ ·æ¿€æ´»å‡½æ•°ï¼ˆRMAFï¼‰ - 2020 [[72](#bib.bib72)] | ResNetã€AlexNetã€SqueezeNetå’ŒDenseNet
    | CIFAR10ã€CIFAR100ã€MNISTå’ŒImageNetåˆ†ç±» | RMAFçš„è¡¨ç°ä¼˜äºReLUã€ELUã€SELUã€PReLUã€Tanhå’ŒSwishã€‚
    |'
- en: '| Parametric Deformable ELU (PDELU) - 2020 [[59](#bib.bib59)] | NIN and ResNet
    | CIFAR10 and CIFAR100 classification | The PDELU performs better than the ReLU,
    ELU and FReLU. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| å‚æ•°åŒ–å¯å˜å½¢ELUï¼ˆPDELUï¼‰ - 2020 [[59](#bib.bib59)] | NINå’ŒResNet | CIFAR10å’ŒCIFAR100åˆ†ç±»
    | PDELUçš„è¡¨ç°ä¼˜äºReLUã€ELUå’ŒFReLUã€‚ |'
- en: '| Pade Activation Unit (PAU) - 2020 [[111](#bib.bib111)] | VGG8, MobileNetV2,
    ResNet and DenseNet | MNIST, Fashion-MNIST, CIFAR10 and ImageNet classification
    | The PAU encode AFs as rational functions and performs better than many existing
    AFs. |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Padeæ¿€æ´»å•å…ƒï¼ˆPAUï¼‰ - 2020 [[111](#bib.bib111)] | VGG8ã€MobileNetV2ã€ResNetå’ŒDenseNet
    | MNISTã€Fashion-MNISTã€CIFAR10å’ŒImageNetåˆ†ç±» | PAUå°†æ¿€æ´»å‡½æ•°ç¼–ç ä¸ºæœ‰ç†å‡½æ•°ï¼Œè¡¨ç°ä¼˜äºè®¸å¤šç°æœ‰çš„æ¿€æ´»å‡½æ•°ã€‚ |'
- en: '| Elastic Exponential Linear Unit (EELU) - 2020 [[58](#bib.bib58)] | A simple
    CNN model and VGG16 | CIFAR10, CIFAR100, ImageNet, and Tiny ImageNet classification
    | The EELU shows better results than the ReLU, ELU, EPReLU and Swish. |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| å¼¹æ€§æŒ‡æ•°çº¿æ€§å•å…ƒï¼ˆEELUï¼‰ - 2020 [[58](#bib.bib58)] | ä¸€ä¸ªç®€å•çš„CNNæ¨¡å‹å’ŒVGG16 | CIFAR10ã€CIFAR100ã€ImageNetå’ŒTiny
    ImageNetåˆ†ç±» | EELUæ˜¾ç¤ºå‡ºæ¯”ReLUã€ELUã€EPReLUå’ŒSwishæ›´å¥½çš„ç»“æœã€‚ |'
- en: '| Dynamic ReLU (DY-ReLU) - 2020 [[114](#bib.bib114)] | MobileNetV2 | ImageNet
    classification and COCO detection | The DY-ReLU is suitable for light-weight networks.
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| åŠ¨æ€ReLUï¼ˆDY-ReLUï¼‰ - 2020 [[114](#bib.bib114)] | MobileNetV2 | ImageNetåˆ†ç±»å’ŒCOCOæ£€æµ‹
    | DY-ReLUé€‚ç”¨äºè½»é‡çº§ç½‘ç»œã€‚ |'
- en: '| Variable AF (VAF) - 2019 [[113](#bib.bib113)] | Shallow CNN models | MNIST,
    Fashion MNIST and CIFAR10 classification | The VAF shows promising performance.
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| å¯å˜æ¿€æ´»å‡½æ•°ï¼ˆVAFï¼‰ - 2019 [[113](#bib.bib113)] | æµ…å±‚CNNæ¨¡å‹ | MNISTã€Fashion MNISTå’ŒCIFAR10åˆ†ç±»
    | VAFæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„è¡¨ç°ã€‚ |'
- en: '| Multi-bin Trainable Linear Unit (MTLU) - 2019 [[46](#bib.bib46)] | FDnet
    and FSRnet | Image denoising and Super-resolution | The MTLU is significantly
    faster having comparable results with the state-of-the-arts. |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| å¤šé€šé“å¯è®­ç»ƒçº¿æ€§å•å…ƒï¼ˆMTLUï¼‰ - 2019 [[46](#bib.bib46)] | FDnetå’ŒFSRnet | å›¾åƒå»å™ªå’Œè¶…åˆ†è¾¨ç‡ | MTLUæ˜¾è‘—æ›´å¿«ï¼Œä¸”ç»“æœä¸æœ€å…ˆè¿›çš„æŠ€æœ¯ç›¸å½“ã€‚
    |'
- en: '| Swish - 2018 [[29](#bib.bib29)] | MobileNet, ResNet, WRN and DenseNet | CIFAR10,
    CIFAR100 and ImageNet classification | The learnable parameter in Swish leads
    to improved performance than Softplus. |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Swish - 2018 [[29](#bib.bib29)] | MobileNetã€ResNetã€WRNå’ŒDenseNet | CIFAR10ã€CIFAR100å’ŒImageNetåˆ†ç±»
    | Swishä¸­çš„å¯å­¦ä¹ å‚æ•°ä½¿å¾—æ€§èƒ½ä¼˜äºSoftplusã€‚ |'
- en: '| On Time Series Datasets |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| å…³äºæ—¶é—´åºåˆ—æ•°æ®é›† |'
- en: '| Variable AF (VAF) - 2019 [[113](#bib.bib113)] | Multi-Layered Neural Network
    | Regression tasks (Kinematics, Energy Cooling, Yatch, etc.) | Better performance
    over Kinematics, Energy Cooling and Yatch datasets. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| å¯å˜æ¿€æ´»å‡½æ•°ï¼ˆVAFï¼‰ - 2019 [[113](#bib.bib113)] | å¤šå±‚ç¥ç»ç½‘ç»œ | å›å½’ä»»åŠ¡ï¼ˆè¿åŠ¨å­¦ã€èƒ½æºå†·å´ã€æ¸¸è‰‡ç­‰ï¼‰ | åœ¨è¿åŠ¨å­¦ã€èƒ½æºå†·å´å’Œæ¸¸è‰‡æ•°æ®é›†ä¸Šè¡¨ç°æ›´ä½³ã€‚
    |'
- en: '| Self-Learnable AFs (SLAF) - 2019 [[79](#bib.bib79)] | Multi-Layered Neural
    Network | Boston Housing and Learning Sparse Polynomial regression | The newer
    parameter space makes the optimization easier. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| è‡ªå­¦ä¹ æ¿€æ´»å‡½æ•°ï¼ˆSLAFï¼‰ - 2019 [[79](#bib.bib79)] | å¤šå±‚ç¥ç»ç½‘ç»œ | æ³¢å£«é¡¿æˆ¿ä»·å’Œç¨€ç–å¤šé¡¹å¼å›å½’å­¦ä¹  | æ›´æ–°çš„å‚æ•°ç©ºé—´ä½¿å¾—ä¼˜åŒ–æ›´å®¹æ˜“ã€‚
    |'
- en: '| On Text Datasets |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| å…³äºæ–‡æœ¬æ•°æ®é›† |'
- en: '| Soft-Root-Sign (SRS) - 2020 [[26](#bib.bib26)] | A 6 layer transformer network
    | IWSLT 2016 German-English translation | The SRS is better over tst2011 and tst2012
    test sets, whereas the SELU and LReLU are better over tst2013 and tst2014 test
    sets, respectively. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Soft-Root-Signï¼ˆSRSï¼‰ - 2020 [[26](#bib.bib26)] | ä¸€ä¸ª6å±‚çš„transformerç½‘ç»œ | IWSLT
    2016å¾·è‹±ç¿»è¯‘ | SRSåœ¨tst2011å’Œtst2012æµ‹è¯•é›†ä¸Šè¡¨ç°æ›´å¥½ï¼Œè€ŒSELUå’ŒLReLUåˆ†åˆ«åœ¨tst2013å’Œtst2014æµ‹è¯•é›†ä¸Šè¡¨ç°æ›´å¥½ã€‚
    |'
- en: '| Swish - 2018 [[29](#bib.bib29)] | A 12 layer transformer network | WMT 2014
    English-German dataset | The performance of Swish is comparable to state-of-the-arts.
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Swish - 2018 [[29](#bib.bib29)] | 12 å±‚å˜æ¢å™¨ç½‘ç»œ | WMT 2014 è‹±å¾·æ•°æ®é›† | Swish çš„æ€§èƒ½å¯ä¸æœ€å…ˆè¿›çš„æŠ€æœ¯åª²ç¾ã€‚
    |'
- en: '| PenalizedTanh - 2018 [[33](#bib.bib33)] | MLP, CNN and RNN | Sentence classification,
    Document classification and Sentence tagging | The PenalizedTanh exhibits the
    stability across the different tasks in contrast to the Swish function. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| PenalizedTanh - 2018 [[33](#bib.bib33)] | MLPã€CNN å’Œ RNN | å¥å­åˆ†ç±»ã€æ–‡æ¡£åˆ†ç±»å’Œå¥å­æ ‡æ³¨
    | ä¸ Swish å‡½æ•°ç›¸æ¯”ï¼ŒPenalizedTanh åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°å‡ºç¨³å®šæ€§ã€‚ |'
- en: '| On Signal Datasets |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| ä¿¡å·æ•°æ®é›† |'
- en: '| Rectified Linear Tanh (ReLTanh) - 2019 [[67](#bib.bib67)] | Stacked autoencoder
    (SAE) based DNN | Vibration signals for rotating machinery fault diagnosis | The
    ReLTanh leads to larger gradients for faster learning and reduces the vanishing
    gradient. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| çº¿æ€§æ•´æµ Tanh (ReLTanh) - 2019 [[67](#bib.bib67)] | åŸºäºå †å è‡ªç¼–ç å™¨ (SAE) çš„æ·±åº¦ç¥ç»ç½‘ç»œ |
    æ—‹è½¬æœºæ¢°æ•…éšœè¯Šæ–­çš„æŒ¯åŠ¨ä¿¡å· | ReLTanh ä½¿æ¢¯åº¦æ›´å¤§ï¼Œå­¦ä¹ æ›´å¿«ï¼Œå¹¶å‡å°‘äº†æ¢¯åº¦æ¶ˆå¤±ã€‚ |'
- en: '| On Game Datasets |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| æ¸¸æˆæ•°æ®é›† |'
- en: '| Sigmoid-weighted Linear Unit (SiLU) - 2018 [[23](#bib.bib23)] | Deep reinforcement
    learning algorithm | SZ-Tetris, $10\times 10$ Tetris, and Atari 2600 games | The
    SiLU AF outperforms the ReLU function for reinforcement learning. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid åŠ æƒçº¿æ€§å•å…ƒ (SiLU) - 2018 [[23](#bib.bib23)] | æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³• | SZ-Tetrisã€$10\times
    10$ Tetris å’Œ Atari 2600 æ¸¸æˆ | SiLU æ¿€æ´»å‡½æ•°åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä¼˜äº ReLU å‡½æ•°ã€‚ |'
- en: 8 Aspects of Activation Functions
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°çš„ 8 ä¸ªæ–¹é¢
- en: This section summarizes the effect of weight initialization, understanding of
    AFs and suitability with different types of data. The learning of the network
    speeds up drastically by using the orthogonal weight initialization based on the
    dynamical isometry [[122](#bib.bib122)]. A set of conditions in parameter initialization
    also boosts the performance of networks with sigmoidal activations [[123](#bib.bib123)].
    The symmetric probability distribution based weights and biases initialization
    leads the network to suffer with the dying ReLU problem. However, the asymmetric
    initialization resolves the dying ReLU problem [[124](#bib.bib124)]. The over-parameterization
    during initialization also benefits in the training [[125](#bib.bib125)]. The
    data-dependent weight initialization using a subset of data minimizes the issues
    of the ReLU [[126](#bib.bib126)], whereas an initial parameter sharing based initialization
    guarantees the dynamical isometry for the ReLU [[127](#bib.bib127)].
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æ€»ç»“äº†æƒé‡åˆå§‹åŒ–çš„æ•ˆæœã€æ¿€æ´»å‡½æ•°çš„ç†è§£ä»¥åŠä¸ä¸åŒæ•°æ®ç±»å‹çš„é€‚ç”¨æ€§ã€‚ä½¿ç”¨åŸºäºåŠ¨æ€åŒæ„çš„æ­£äº¤æƒé‡åˆå§‹åŒ–å¯ä»¥æ˜¾è‘—åŠ å¿«ç½‘ç»œçš„å­¦ä¹ é€Ÿåº¦[[122](#bib.bib122)]ã€‚ä¸€ç»„å‚æ•°åˆå§‹åŒ–æ¡ä»¶ä¹Ÿæå‡äº†å…·æœ‰
    sigmoid æ¿€æ´»å‡½æ•°çš„ç½‘ç»œçš„æ€§èƒ½[[123](#bib.bib123)]ã€‚åŸºäºå¯¹ç§°æ¦‚ç‡åˆ†å¸ƒçš„æƒé‡å’Œåç½®åˆå§‹åŒ–å¯¼è‡´ç½‘ç»œå‡ºç° ReLU æ­»äº¡é—®é¢˜ã€‚ç„¶è€Œï¼Œéå¯¹ç§°åˆå§‹åŒ–è§£å†³äº†
    ReLU æ­»äº¡é—®é¢˜[[124](#bib.bib124)]ã€‚åˆå§‹åŒ–æœŸé—´çš„è¿‡åº¦å‚æ•°åŒ–ä¹Ÿæœ‰åˆ©äºè®­ç»ƒ[[125](#bib.bib125)]ã€‚ä½¿ç”¨æ•°æ®å­é›†çš„ä¾èµ–æ•°æ®æƒé‡åˆå§‹åŒ–æœ€å°åŒ–äº†
    ReLU çš„é—®é¢˜[[126](#bib.bib126)]ï¼Œè€ŒåŸºäºåˆå§‹å‚æ•°å…±äº«çš„åˆå§‹åŒ–ä¿è¯äº† ReLU çš„åŠ¨æ€åŒæ„[[127](#bib.bib127)]ã€‚
- en: Several researchers have tried to understand the working and impact of AFs through
    different strategies. The lower and upper bounds are established for network complexity
    to realize that the ReLU in deep networks approximates the smooth functions more
    efficiently as compared to shallow networks [[128](#bib.bib128)]. A ReLU network
    with only one hidden layer is trained to reach the global optimum in polynomial
    time even with exponentially growing input dimension [[129](#bib.bib129)]. The
    ReLU type AF based neural networks produce the overconfident predictions far away
    from the training data [[130](#bib.bib130)]. However, this can be resolved by
    employing adversarial confidence enhanced training. A Gaussian margin driven time
    and accuracy tradeoff analysis is also done on the ReLUâ€™s learning [[131](#bib.bib131)].
    The singular values for ReLU layers are analyzed to understand the interaction
    of ReLU with the linear components [[132](#bib.bib132)]. The approximation of
    Gaussian posterior distribution over the ReLU network weightâ€™s fixes the overconfidence
    problem [[133](#bib.bib133)].
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šç ”ç©¶è€…å°è¯•é€šè¿‡ä¸åŒçš„ç­–ç•¥æ¥ç†è§£AFsçš„å·¥ä½œåŸç†å’Œå½±å“ã€‚å·²å»ºç«‹ç½‘ç»œå¤æ‚åº¦çš„ä¸‹ç•Œå’Œä¸Šç•Œï¼Œä»¥å®ç°æ·±åº¦ç½‘ç»œä¸­çš„ReLUæ¯”æµ…å±‚ç½‘ç»œæ›´æœ‰æ•ˆåœ°è¿‘ä¼¼å¹³æ»‘å‡½æ•°[[128](#bib.bib128)]ã€‚å³ä½¿åœ¨è¾“å…¥ç»´åº¦æŒ‡æ•°å¢é•¿çš„æƒ…å†µä¸‹ï¼Œä¸€ä¸ªä»…æœ‰ä¸€å±‚éšè—å±‚çš„ReLUç½‘ç»œä¹Ÿèƒ½åœ¨å¤šé¡¹å¼æ—¶é—´å†…è¾¾åˆ°å…¨å±€æœ€ä¼˜[[129](#bib.bib129)]ã€‚åŸºäºReLUçš„ç¥ç»ç½‘ç»œäº§ç”Ÿäº†è¿œç¦»è®­ç»ƒæ•°æ®çš„è¿‡åº¦è‡ªä¿¡é¢„æµ‹[[130](#bib.bib130)]ã€‚ç„¶è€Œï¼Œé€šè¿‡é‡‡ç”¨å¯¹æŠ—æ€§ä¿¡å¿ƒå¢å¼ºè®­ç»ƒå¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¯¹ReLUå­¦ä¹ è¿›è¡Œçš„é«˜æ–¯è¾¹é™…é©±åŠ¨çš„æ—¶é—´ä¸ç²¾åº¦æƒè¡¡åˆ†æä¹Ÿå·²å®Œæˆ[[131](#bib.bib131)]ã€‚å¯¹ReLUå±‚çš„å¥‡å¼‚å€¼è¿›è¡Œåˆ†æï¼Œä»¥äº†è§£ReLUä¸çº¿æ€§ç»„ä»¶çš„äº¤äº’[[132](#bib.bib132)]ã€‚å¯¹ReLUç½‘ç»œæƒé‡çš„é«˜æ–¯åéªŒåˆ†å¸ƒè¿›è¡Œçš„è¿‘ä¼¼è§£å†³äº†è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜[[133](#bib.bib133)]ã€‚
- en: 'Despite most of the AFs are tested over image data, there are few research
    papers dealing with the AFs over other types of data. Table [6](#S7.T6 "Table
    6 â€£ 7.5 Kernel Activation Functions â€£ 7 Miscellaneous Activation Functions â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark") summarizes
    the insights and remarks of state-of-the-art AFs for various networks and datasets.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 'å°½ç®¡å¤§å¤šæ•°AFsæ˜¯åœ¨å›¾åƒæ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•çš„ï¼Œä½†æ¶‰åŠå…¶ä»–æ•°æ®ç±»å‹çš„AFsçš„ç ”ç©¶è®ºæ–‡å´å¾ˆå°‘ã€‚è¡¨[6](#S7.T6 "Table 6 â€£ 7.5 Kernel
    Activation Functions â€£ 7 Miscellaneous Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark")æ€»ç»“äº†æœ€æ–°AFsåœ¨å„ç§ç½‘ç»œå’Œæ•°æ®é›†ä¸Šçš„è§è§£å’Œå¤‡æ³¨ã€‚'
- en: 9 Performance Comparison and Analysis
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 æ€§èƒ½æ¯”è¾ƒä¸åˆ†æ
- en: This survey is compared with the existing survey/performance analysis and the
    experimental performance analysis of selected AFs is performed over Image, Text
    and Speech data.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è°ƒæŸ¥ä¸ç°æœ‰è°ƒæŸ¥/æ€§èƒ½åˆ†æè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å¯¹æ‰€é€‰çš„AFsåœ¨å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³æ•°æ®ä¸Šçš„å®éªŒæ€§èƒ½åˆ†æè¿›è¡Œè¯„ä¼°ã€‚
- en: 9.1 Comparison with Existing Survey/Performance Analysis
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 ä¸ç°æœ‰è°ƒæŸ¥/æ€§èƒ½åˆ†æçš„æ¯”è¾ƒ
- en: A performance analysis of AFs was conducted using multilayer perceptron network
    in [[134](#bib.bib134)]. Among compared AFs, the Tanh has shown better performance.
    A comparative performance analysis of different AFs suggests an Elliott function
    as better suited for classification using LSTM networks [[25](#bib.bib25)]. The
    ELU outperforms the ReLU, LReLU, and SELU AFs over MNIST classification task using
    Deep Neural Networks [[135](#bib.bib135)]. As per [[136](#bib.bib136)], the ELU
    is reported in [[135](#bib.bib135)] to outperform the ReLU, LReLU, PReLU and PELU
    over sufficiently large datasets for speech recognition. However, for smaller
    datasets, the ReLU is preferred. A similar trend is also reported in [[137](#bib.bib137)]
    with a note that the ELU and SELU AFs exhibit faster convergence as compared to
    the ReLU and LReLU AFs. In [[138](#bib.bib138)], 21 AFs are listed without experimental
    results comparison. In contrast to [[138](#bib.bib138)], this paper presents a
    comprehensive survey of AFs. The ReLU based deep networks perform superior or
    mildly worse than the spline methods [[139](#bib.bib139)]. A review of adaptive
    functions is conducted in [[140](#bib.bib140)] by considering 9 functions, including
    Sigmoid, Tanh, PReLU, and adaptTanh. In [[141](#bib.bib141)], the comparison between
    ReLU and LReLU is performed using CNN on MNIST dataset. An empirical study is
    also done for the variations of ReLU activation by generalizing it with the help
    of parameters [[142](#bib.bib142)]. The comparison of AFs is also performed for
    generalized learning vector quantization [[143](#bib.bib143)]. The ReLU activation
    has performed better for object, face, and text datasets [[144](#bib.bib144)].
    However, the SELU and Maxout have performed better for medical and sound datasets,
    respectively [[144](#bib.bib144)]. The piecewise AF is better suited for facial
    expression recognition in [[145](#bib.bib145)]. A survey of adaptive AFs is conducted
    in [[146](#bib.bib146)] without experimental comparison. The evaluation of seven
    AFs is conducted in [[147](#bib.bib147)] using a simple network over CIFAR10 dataset,
    whereas in our survey we cover different AFs and also perform the experimental
    comparison.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ [[134](#bib.bib134)] ä¸­ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºç½‘ç»œå¯¹æ¿€æ´»å‡½æ•°è¿›è¡Œäº†æ€§èƒ½åˆ†æã€‚åœ¨æ¯”è¾ƒçš„æ¿€æ´»å‡½æ•°ä¸­ï¼ŒTanh æ˜¾ç¤ºå‡ºæ›´å¥½çš„æ€§èƒ½ã€‚å¯¹ä¸åŒæ¿€æ´»å‡½æ•°çš„æ¯”è¾ƒæ€§èƒ½åˆ†æè¡¨æ˜ï¼ŒElliott
    å‡½æ•°æ›´é€‚åˆç”¨äº LSTM ç½‘ç»œçš„åˆ†ç±» [[25](#bib.bib25)]ã€‚åœ¨ MNIST åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒELU çš„è¡¨ç°ä¼˜äº ReLUã€LReLU å’Œ SELU
    æ¿€æ´»å‡½æ•° [[135](#bib.bib135)]ã€‚æ ¹æ® [[136](#bib.bib136)]ï¼Œåœ¨ [[135](#bib.bib135)] ä¸­æŠ¥å‘Šäº†
    ELU åœ¨è¶³å¤Ÿå¤§çš„æ•°æ®é›†ä¸Šä¼˜äº ReLUã€LReLUã€PReLU å’Œ PELUï¼Œç”¨äºè¯­éŸ³è¯†åˆ«ã€‚ç„¶è€Œï¼Œå¯¹äºè¾ƒå°çš„æ•°æ®é›†ï¼ŒReLU æ›´å—æ¬¢è¿ã€‚ [[137](#bib.bib137)]
    ä¸­ä¹ŸæŠ¥å‘Šäº†ç±»ä¼¼çš„è¶‹åŠ¿ï¼Œå¹¶æŒ‡å‡º ELU å’Œ SELU æ¿€æ´»å‡½æ•°çš„æ”¶æ•›é€Ÿåº¦æ¯” ReLU å’Œ LReLU æ¿€æ´»å‡½æ•°æ›´å¿«ã€‚åœ¨ [[138](#bib.bib138)]
    ä¸­åˆ—å‡ºäº† 21 ç§æ¿€æ´»å‡½æ•°ï¼Œä½†æ²¡æœ‰è¿›è¡Œå®éªŒç»“æœæ¯”è¾ƒã€‚ä¸ [[138](#bib.bib138)] ç›¸æ¯”ï¼Œæœ¬æ–‡æä¾›äº†å¯¹æ¿€æ´»å‡½æ•°çš„å…¨é¢è°ƒæŸ¥ã€‚åŸºäº ReLU çš„æ·±åº¦ç½‘ç»œåœ¨æ€§èƒ½ä¸Šä¼˜äºæˆ–ä»…ç¨é€Šäº
    spline æ–¹æ³• [[139](#bib.bib139)]ã€‚åœ¨ [[140](#bib.bib140)] ä¸­ï¼Œé€šè¿‡è€ƒè™‘ 9 ç§å‡½æ•°ï¼ŒåŒ…æ‹¬ Sigmoidã€Tanhã€PReLU
    å’Œ adaptTanhï¼Œå¯¹è‡ªé€‚åº”å‡½æ•°è¿›è¡Œäº†ç»¼è¿°ã€‚åœ¨ [[141](#bib.bib141)] ä¸­ï¼Œä½¿ç”¨ CNN åœ¨ MNIST æ•°æ®é›†ä¸Šå¯¹ ReLU å’Œ LReLU
    è¿›è¡Œäº†æ¯”è¾ƒã€‚è¿˜é€šè¿‡å‚æ•°åŒ–çš„ä¸€èˆ¬åŒ–æ–¹æ³•å¯¹ ReLU æ¿€æ´»å‡½æ•°çš„å˜ä½“è¿›è¡Œäº†å®è¯ç ”ç©¶ [[142](#bib.bib142)]ã€‚å¯¹è‡ªé€‚åº”å­¦ä¹ å‘é‡é‡åŒ–çš„æ¿€æ´»å‡½æ•°ä¹Ÿè¿›è¡Œäº†æ¯”è¾ƒ
    [[143](#bib.bib143)]ã€‚åœ¨ [[144](#bib.bib144)] ä¸­ï¼ŒReLU æ¿€æ´»å‡½æ•°åœ¨å¯¹è±¡ã€é¢éƒ¨å’Œæ–‡æœ¬æ•°æ®é›†ä¸Šè¡¨ç°æ›´å¥½ã€‚ç„¶è€Œï¼ŒSELU
    å’Œ Maxout åˆ†åˆ«åœ¨åŒ»å­¦å’Œå£°éŸ³æ•°æ®é›†ä¸Šè¡¨ç°æ›´å¥½ [[144](#bib.bib144)]ã€‚åœ¨ [[145](#bib.bib145)] ä¸­ï¼Œåˆ†æ®µæ¿€æ´»å‡½æ•°æ›´é€‚åˆé¢éƒ¨è¡¨æƒ…è¯†åˆ«ã€‚åœ¨
    [[146](#bib.bib146)] ä¸­ï¼Œå¯¹è‡ªé€‚åº”æ¿€æ´»å‡½æ•°è¿›è¡Œäº†è°ƒæŸ¥ï¼Œä½†æœªè¿›è¡Œå®éªŒæ¯”è¾ƒã€‚åœ¨ [[147](#bib.bib147)] ä¸­ï¼Œå¯¹ä¸ƒç§æ¿€æ´»å‡½æ•°è¿›è¡Œäº†ç®€å•ç½‘ç»œä¸‹çš„è¯„ä¼°ï¼Œè€Œåœ¨æˆ‘ä»¬çš„è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬æ¶µç›–äº†ä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œå¹¶è¿›è¡Œäº†å®éªŒæ¯”è¾ƒã€‚
- en: 'Table 7: Comparison of this survey with the existing surveys and performance
    evaluations.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 7: æœ¬æ¬¡è°ƒæŸ¥ä¸ç°æœ‰è°ƒæŸ¥å’Œæ€§èƒ½è¯„ä¼°çš„æ¯”è¾ƒã€‚'
- en: '| Method | Models | Activations | Datasets | Remarks |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | æ¨¡å‹ | æ¿€æ´»å‡½æ•° | æ•°æ®é›† | å¤‡æ³¨ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Karlik and Olgac [[134](#bib.bib134)] | Multilayer Perceptron (MLP) | 5 AFs,
    including Bi-polar sigmoid, Uni-polar sigmoid, Tanh, etc. | Classification | The
    Tanh performs better compared to other traditional AFs. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Karlik å’Œ Olgac [[134](#bib.bib134)] | å¤šå±‚æ„ŸçŸ¥æœº (MLP) | 5 ç§æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬åŒæ sigmoidã€å•æ
    sigmoidã€Tanh ç­‰ | åˆ†ç±» | ä¸å…¶ä»–ä¼ ç»Ÿæ¿€æ´»å‡½æ•°ç›¸æ¯”ï¼ŒTanh è¡¨ç°æ›´å¥½ã€‚ |'
- en: '| Vydana and Vuppala (2017) [[136](#bib.bib136)] | Hidden Markov Model-Deep
    Neural Network (HMM-DNN) | 5 AFs, including ReLU, LReLU, PReLU, ELU, and PELU
    | TIMIT and WSJ speech recognition | The ELU is better over sufficiently larger
    size datasets. However, the ReLU is preferred for smaller datasets. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Vydana å’Œ Vuppala (2017) [[136](#bib.bib136)] | éšé©¬å°”å¯å¤«æ¨¡å‹-æ·±åº¦ç¥ç»ç½‘ç»œ (HMM-DNN) |
    5 ç§æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬ ReLUã€LReLUã€PReLUã€ELU å’Œ PELU | TIMIT å’Œ WSJ è¯­éŸ³è¯†åˆ« | å¯¹äºè¶³å¤Ÿå¤§çš„æ•°æ®é›†ï¼ŒELU è¡¨ç°æ›´å¥½ã€‚ç„¶è€Œï¼Œå¯¹äºè¾ƒå°çš„æ•°æ®é›†ï¼ŒReLU
    æ›´å—é’çã€‚ |'
- en: '| Alcantara (2017) [[135](#bib.bib135)] | A neural network with 2 hidden layers
    having 100 neurons/layer | 4 AFs, including ReLU, LReLU, ELU, and SELU | MNIST
    classification | The ELU AF outperforms others. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Alcantara (2017) [[135](#bib.bib135)] | ä¸€ä¸ªå…·æœ‰ 2 å±‚éšè—å±‚ã€æ¯å±‚ 100 ä¸ªç¥ç»å…ƒçš„ç¥ç»ç½‘ç»œ | 4
    ç§æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬ ReLUã€LReLUã€ELU å’Œ SELU | MNIST åˆ†ç±» | ELU æ¿€æ´»å‡½æ•°ä¼˜äºå…¶ä»–æ¿€æ´»å‡½æ•°ã€‚ |'
- en: '| Pedamonti (2018) [[137](#bib.bib137)] | A neural network with 2 hidden layers
    having 100 neurons/layer | 5 AFs, including Sigmoid, ReLU, LReLU, ELU, and SELU
    | MNIST classification | The ELU and SELU AFs exhibit the faster convergence as
    compared to the ReLU and LReLU AFs. |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Pedamonti (2018) [[137](#bib.bib137)] | ä¸€ä¸ªå…·æœ‰ 2 å±‚éšè—å±‚ã€æ¯å±‚ 100 ä¸ªç¥ç»å…ƒçš„ç¥ç»ç½‘ç»œ | 5
    ç§æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬ Sigmoidã€ReLUã€LReLUã€ELU å’Œ SELU | MNIST åˆ†ç±» | ELU å’Œ SELU æ¿€æ´»å‡½æ•°çš„æ”¶æ•›é€Ÿåº¦æ¯” ReLU
    å’Œ LReLU æ›´å¿«ã€‚ |'
- en: '| Lau and Lim (2018) [[140](#bib.bib140)] | Deep Neural Network (DNN) | ReLU
    and Adaptive ReLU | MNIST classification | The adaptive AFs improve the generalization
    of the network. |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Lau å’Œ Lim (2018) [[140](#bib.bib140)] | æ·±åº¦ç¥ç»ç½‘ç»œ (DNN) | ReLU å’Œè‡ªé€‚åº” ReLU | MNIST
    åˆ†ç±» | è‡ªé€‚åº”æ¿€æ´»å‡½æ•°æé«˜äº†ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ã€‚ |'
- en: '| Farzad et al. (2019) [[25](#bib.bib25)] | Long Short Term Memory (LSTM) |
    23 AFs, including Elliott, Gaussian, Logarithmic, Loglog, etc. | IMDB, Movie Review,
    MNIST classification | Elliott function is better suited to the LSTM network.
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Farzad ç­‰ (2019) [[25](#bib.bib25)] | é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (LSTM) | 23 ç§æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬ Elliottã€Gaussianã€Logarithmicã€Loglog
    ç­‰ | IMDBã€ç”µå½±è¯„è®ºã€MNIST åˆ†ç±» | Elliott å‡½æ•°æ›´é€‚åˆ LSTM ç½‘ç»œã€‚ |'
- en: '| Dubey and Jain (2019) [[141](#bib.bib141)] | Simple Convolutional Neural
    Network (CNN) | 2 AFs, including ReLU and Leaky ReLU | MNIST classification |
    The ReLU performed better than Leaky ReLU (LReLU). |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Dubey å’Œ Jain (2019) [[141](#bib.bib141)] | ç®€å•å·ç§¯ç¥ç»ç½‘ç»œ (CNN) | 2 ç§æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬ ReLU
    å’Œ Leaky ReLU | MNIST åˆ†ç±» | ReLU çš„è¡¨ç°ä¼˜äº Leaky ReLU (LReLU)ã€‚ |'
- en: '| Banerjee et al. (2019) [[142](#bib.bib142)] | Convolutional Neural Network
    (CNN) | Generalized ReLU | MNIST classification | Network learns the parameters
    for different ReLU variations. |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Banerjee ç­‰ (2019) [[142](#bib.bib142)] | å·ç§¯ç¥ç»ç½‘ç»œ (CNN) | å¹¿ä¹‰ ReLU | MNIST åˆ†ç±»
    | ç½‘ç»œå­¦ä¹ ä¸åŒ ReLU å˜ä½“çš„å‚æ•°ã€‚ |'
- en: '| Villmann et al. (2019) [[143](#bib.bib143)] | Generalized learning vector
    quantization (GLVQ) | 12 AFs, including Sigmoid, Swish, ReLU, Softplus, etc. |
    Tecator, Indian Pine and Wisconsin-Breast-Cancer classification | The Sigmoid,
    Swish and Softplus AFs are better suited with GLVQ. |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Villmann ç­‰ (2019) [[143](#bib.bib143)] | å¹¿ä¹‰å­¦ä¹ å‘é‡é‡åŒ– (GLVQ) | 12 ç§æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬ Sigmoidã€Swishã€ReLUã€Softplus
    ç­‰ | Tecatorã€Indian Pine å’Œ Wisconsin-Breast-Cancer åˆ†ç±» | Sigmoidã€Swish å’Œ Softplus
    æ¿€æ´»å‡½æ•°æ›´é€‚åˆ GLVQã€‚ |'
- en: '| Castaneda et al. (2019) [[144](#bib.bib144)] | 6 different models for different
    applications | 3 AFs, including ReLU, SELU and Maxout | Object, Face, Text, Medical
    and Sound datasets | The ReLU is better for object, face and text datasets, whereas
    SELU and Maxout are better for medical and sound datasets, respectively. |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Castaneda ç­‰ (2019) [[144](#bib.bib144)] | 6 ç§ä¸åŒæ¨¡å‹ç”¨äºä¸åŒåº”ç”¨ | 3 ç§æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬ ReLUã€SELU
    å’Œ Maxout | å¯¹è±¡ã€é¢éƒ¨ã€æ–‡æœ¬ã€åŒ»ç–—å’Œå£°éŸ³æ•°æ®é›† | ReLU æ›´é€‚ç”¨äºå¯¹è±¡ã€é¢éƒ¨å’Œæ–‡æœ¬æ•°æ®é›†ï¼Œè€Œ SELU å’Œ Maxout åˆ†åˆ«æ›´é€‚ç”¨äºåŒ»ç–—å’Œå£°éŸ³æ•°æ®é›†ã€‚
    |'
- en: '| Wang et al. (2020) [[145](#bib.bib145)] | Inception-v3 model | 6 AFs, including
    Sigmoid, Tanh, ReLu, etc. | JAFFE and FER2013 facial expression recognition |
    The combination of log, softdesign and ReLU AFs provides improved performance.
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Wang ç­‰ (2020) [[145](#bib.bib145)] | Inception-v3 æ¨¡å‹ | 6 ç§æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬ Sigmoidã€Tanhã€ReLU
    ç­‰ | JAFFE å’Œ FER2013 é¢éƒ¨è¡¨æƒ…è¯†åˆ« | logã€softdesign å’Œ ReLU æ¿€æ´»å‡½æ•°çš„ç»„åˆæä¾›äº†æ›´å¥½çš„æ€§èƒ½ã€‚ |'
- en: '| Szandala (2020) [[147](#bib.bib147)] | A simple network | 7 AFs, including
    Sigmoid, Tanh, ReLU, LReLU, Swish, etc. | CIFAR10 classification | The LReLU performs
    better. The ReLU is efficient. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Szandala (2020) [[147](#bib.bib147)] | ä¸€ä¸ªç®€å•çš„ç½‘ç»œ | 7 ç§æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬ Sigmoidã€Tanhã€ReLUã€LReLUã€Swish
    ç­‰ | CIFAR10 åˆ†ç±» | LReLU çš„è¡¨ç°æ›´ä½³ã€‚ReLU æ•ˆç‡è¾ƒé«˜ã€‚ |'
- en: '| Our survey and performance analysis | MobileNet, VGG, GoogLeNet, ResNet,
    SENet, DenseNet, etc. | Exhaustive list of AFs, including performance analysis
    over $18$ state-of-the-art activations | CIFAR10 classification, Language translation,
    Speech recognition | A classification to categorize and analyze the AFs and a
    performance comparison of the state-of-the-art activations. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| æˆ‘ä»¬çš„è°ƒæŸ¥å’Œæ€§èƒ½åˆ†æ | MobileNetã€VGGã€GoogLeNetã€ResNetã€SENetã€DenseNet ç­‰ | æ¿€æ´»å‡½æ•°çš„è¯¦å°½åˆ—è¡¨ï¼ŒåŒ…æ‹¬
    $18$ ç§æœ€å…ˆè¿›çš„æ¿€æ´»å‡½æ•°æ€§èƒ½åˆ†æ | CIFAR10 åˆ†ç±»ã€è¯­è¨€ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ« | å¯¹æ¿€æ´»å‡½æ•°è¿›è¡Œåˆ†ç±»ã€åˆ†æï¼Œå¹¶æ¯”è¾ƒæœ€å…ˆè¿›æ¿€æ´»å‡½æ•°çš„æ€§èƒ½ã€‚ |'
- en: 'A summary of the comparison with existing surveys and performance analysis
    of AF is shown in Table [7](#S9.T7 "Table 7 â€£ 9.1 Comparison with Existing Survey/Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"). Following are the observations:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ [7](#S9.T7 "è¡¨ 7 â€£ 9.1 ä¸ç°æœ‰è°ƒæŸ¥/æ€§èƒ½åˆ†æçš„æ¯”è¾ƒ â€£ 9 æ€§èƒ½æ¯”è¾ƒä¸åˆ†æ â€£ æ·±åº¦å­¦ä¹ ä¸­çš„æ¿€æ´»å‡½æ•°ï¼šå…¨é¢è°ƒæŸ¥ä¸åŸºå‡†") æ˜¾ç¤ºäº†ä¸ç°æœ‰è°ƒæŸ¥å’Œ
    AF æ€§èƒ½åˆ†æçš„æ¯”è¾ƒæ€»ç»“ã€‚ä»¥ä¸‹æ˜¯è§‚å¯Ÿç»“æœï¼š
- en: '1.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: This survey presents a detailed classification to cover the wide range of AFs
    as compared to the existing surveys and performance analysis.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœ¬è°ƒæŸ¥æä¾›äº†è¯¦ç»†çš„åˆ†ç±»ï¼Œä»¥è¦†ç›–å¹¿æ³›çš„ AFsï¼Œä¸ç°æœ‰è°ƒæŸ¥å’Œæ€§èƒ½åˆ†æç›¸æ¯”æ›´ä¸ºå…¨é¢ã€‚
- en: '2.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: This survey covers exhaustive state-of-the-art AFs to date, whereas the existing
    survey/performance analysis covers either a limited number of AFs or only basic
    AFs.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœ¬è°ƒæŸ¥æ¶µç›–äº†æˆªè‡³ç›®å‰çš„æœ€æ–° AFsï¼Œè€Œç°æœ‰çš„è°ƒæŸ¥/æ€§èƒ½åˆ†æè¦ä¹ˆæ¶µç›–æœ‰é™æ•°é‡çš„ AFsï¼Œè¦ä¹ˆä»…æ¶µç›–åŸºç¡€çš„ AFsã€‚
- en: '3.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: The performance analysis conducted in this paper considers a wide range of neural
    networks over different types of data for eighteen AFs, whereas the existing analysis
    is limited to a single type of data and network.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä¸­è¿›è¡Œçš„æ€§èƒ½åˆ†æè€ƒè™‘äº†ä¸åŒæ•°æ®ç±»å‹ä¸‹çš„å¹¿æ³›ç¥ç»ç½‘ç»œï¼Œæ¶µç›–äº†åå…«ç§ AFsï¼Œè€Œç°æœ‰åˆ†æä»…é™äºå•ä¸€æ•°æ®å’Œç½‘ç»œç±»å‹ã€‚
- en: '4.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: This survey highlights the trends to help the researchers to further explore
    the better AFs and practitioners to choose based on the data and network types.
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æœ¬è°ƒæŸ¥å¼ºè°ƒäº†è¶‹åŠ¿ï¼Œä»¥å¸®åŠ©ç ”ç©¶äººå‘˜è¿›ä¸€æ­¥æ¢ç´¢æ›´å¥½çš„ AFsï¼ˆæ¿€æ´»å‡½æ•°ï¼‰ä»¥åŠå®è·µè€…æ ¹æ®æ•°æ®å’Œç½‘ç»œç±»å‹åšå‡ºé€‰æ‹©ã€‚
- en: 'Table 8: Experimental results comparison over CIFAR10 dataset.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ 8: åœ¨ CIFAR10 æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ¯”è¾ƒã€‚'
- en: '| Accuracy | CNN Models |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| å‡†ç¡®ç‡ | CNN æ¨¡å‹ |'
- en: '| --- | --- |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Activations | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| æ¿€æ´»å‡½æ•° | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Sigmoid | 88.60 $\pm$ 0.17 | 87.69 $\pm$ 0.49 | 87.33 $\pm$ 2.48 | 80.13
    $\pm$ 3.33 | 90.29 $\pm$ 0.29 | 89.92 $\pm$ 1.96 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | 88.60 $\pm$ 0.17 | 87.69 $\pm$ 0.49 | 87.33 $\pm$ 2.48 | 80.13
    $\pm$ 3.33 | 90.29 $\pm$ 0.29 | 89.92 $\pm$ 1.96 |'
- en: '| Tanh | 87.21 $\pm$ 0.24 | 90.49 $\pm$ 0.11 | 90.16 $\pm$ 1.86 | 89.09 $\pm$
    1.47 | 90.44 $\pm$ 0.09 | 91.80 $\pm$ 0.69 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Tanh | 87.21 $\pm$ 0.24 | 90.49 $\pm$ 0.11 | 90.16 $\pm$ 1.86 | 89.09 $\pm$
    1.47 | 90.44 $\pm$ 0.09 | 91.80 $\pm$ 0.69 |'
- en: '| Elliott [[25](#bib.bib25)] | 88.48 $\pm$ 0.18 | 87.94 $\pm$ 0.49 | 89.84
    $\pm$ 3.43 | 81.60 $\pm$ 3.91 | 90.25 $\pm$ 0.25 | 91.53 $\pm$ 1.04 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Elliott [[25](#bib.bib25)] | 88.48 $\pm$ 0.18 | 87.94 $\pm$ 0.49 | 89.84
    $\pm$ 3.43 | 81.60 $\pm$ 3.91 | 90.25 $\pm$ 0.25 | 91.53 $\pm$ 1.04 |'
- en: '| ReLU [[8](#bib.bib8)] | 90.10 $\pm$ 0.22 | 92.84  $\pm$ 0.19 | 93.43  $\pm$
    0.48 | 93.74 $\pm$ 0.34 | 93.70 $\pm$ 0.16 | 93.96  $\pm$ 0.51 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| ReLU [[8](#bib.bib8)] | 90.10 $\pm$ 0.22 | 92.84  $\pm$ 0.19 | 93.43  $\pm$
    0.48 | 93.74 $\pm$ 0.34 | 93.70 $\pm$ 0.16 | 93.96  $\pm$ 0.51 |'
- en: '| LReLU [[17](#bib.bib17)] | 90.10 $\pm$ 0.19 | 91.09 $\pm$ 0.09 | 89.28 $\pm$
    0.82 | 93.83  $\pm$ 0.42 | 93.66 $\pm$ 0.19 | 93.85 $\pm$ 0.48 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| LReLU [[17](#bib.bib17)] | 90.10 $\pm$ 0.19 | 91.09 $\pm$ 0.09 | 89.28 $\pm$
    0.82 | 93.83  $\pm$ 0.42 | 93.66 $\pm$ 0.19 | 93.85 $\pm$ 0.48 |'
- en: '| PReLU [[35](#bib.bib35)] | 90.43 $\pm$ 0.18 | 92.19 $\pm$ 0.08 | 92.85 $\pm$
    0.55 | 92.99 $\pm$ 0.62 | 92.76 $\pm$ 0.26 | 92.82 $\pm$ 0.63 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| PReLU [[35](#bib.bib35)] | 90.43 $\pm$ 0.18 | 92.19 $\pm$ 0.08 | 92.85 $\pm$
    0.55 | 92.99 $\pm$ 0.62 | 92.76 $\pm$ 0.26 | 92.82 $\pm$ 0.63 |'
- en: '| ELU [[27](#bib.bib27)] | 90.92 $\pm$ 0.25 | 88.55 $\pm$ 1.17 | 92.47 $\pm$
    0.76 | 93.53 $\pm$ 0.66 | 93.39 $\pm$ 0.20 | 92.89 $\pm$ 0.62 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| ELU [[27](#bib.bib27)] | 90.92 $\pm$ 0.25 | 88.55 $\pm$ 1.17 | 92.47 $\pm$
    0.76 | 93.53 $\pm$ 0.66 | 93.39 $\pm$ 0.20 | 92.89 $\pm$ 0.62 |'
- en: '| SELU [[52](#bib.bib52)] | 90.11 $\pm$ 0.32 | 92.25 $\pm$ 0.28 | 91.87 $\pm$
    0.84 | 93.53 $\pm$ 0.52 | 89.96 $\pm$ 0.31 | 92.71 $\pm$ 0.73 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| SELU [[52](#bib.bib52)] | 90.11 $\pm$ 0.32 | 92.25 $\pm$ 0.28 | 91.87 $\pm$
    0.84 | 93.53 $\pm$ 0.52 | 89.96 $\pm$ 0.31 | 92.71 $\pm$ 0.73 |'
- en: '| GELU [[101](#bib.bib101)] | 90.71 $\pm$ 0.20 | 92.42 $\pm$ 0.09 | 93.16 $\pm$
    0.61 | 93.81 $\pm$ 0.46 | 93.72  $\pm$ 0.18 | 93.90  $\pm$ 0.41 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| GELU [[101](#bib.bib101)] | 90.71 $\pm$ 0.20 | 92.42 $\pm$ 0.09 | 93.16 $\pm$
    0.61 | 93.81 $\pm$ 0.46 | 93.72  $\pm$ 0.18 | 93.90  $\pm$ 0.41 |'
- en: '| CELU [[53](#bib.bib53)] | 91.04  $\pm$ 0.17 | 88.11 $\pm$ 0.14 | 92.60 $\pm$
    0.60 | 94.09  $\pm$ 0.17 | 91.63 $\pm$ 0.22 | 93.46 $\pm$ 0.35 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| CELU [[53](#bib.bib53)] | 91.04  $\pm$ 0.17 | 88.11 $\pm$ 0.14 | 92.60 $\pm$
    0.60 | 94.09  $\pm$ 0.17 | 91.63 $\pm$ 0.22 | 93.46 $\pm$ 0.35 |'
- en: '| Softplus [[93](#bib.bib93)] | 91.05  $\pm$ 0.22 | 92.69 $\pm$ 0.20 | 92.66
    $\pm$ 0.66 | 93.34 $\pm$ 0.65 | 93.25 $\pm$ 0.11 | 93.07 $\pm$ 0.70 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Softplus [[93](#bib.bib93)] | 91.05  $\pm$ 0.22 | 92.69 $\pm$ 0.20 | 92.66
    $\pm$ 0.66 | 93.34 $\pm$ 0.65 | 93.25 $\pm$ 0.11 | 93.07 $\pm$ 0.70 |'
- en: '| Swish [[29](#bib.bib29)] | 90.66 $\pm$ 0.34 | 92.32 $\pm$ 0.20 | 92.68 $\pm$
    0.53 | 93.02 $\pm$ 0.85 | 93.24 $\pm$ 0.19 | 93.16 $\pm$ 0.51 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| Swish [[29](#bib.bib29)] | 90.66 $\pm$ 0.34 | 92.32 $\pm$ 0.20 | 92.68 $\pm$
    0.53 | 93.02 $\pm$ 0.85 | 93.24 $\pm$ 0.19 | 93.16 $\pm$ 0.51 |'
- en: '| ABReLU [[44](#bib.bib44)] | 88.97 $\pm$ 0.47 | 92.36 $\pm$ 0.15 | 93.34 $\pm$
    0.23 | 93.29 $\pm$ 0.52 | 93.35 $\pm$ 0.14 | 93.26 $\pm$ 0.55 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| ABReLU [[44](#bib.bib44)] | 88.97 $\pm$ 0.47 | 92.36 $\pm$ 0.15 | 93.34 $\pm$
    0.23 | 93.29 $\pm$ 0.52 | 93.35 $\pm$ 0.14 | 93.26 $\pm$ 0.55 |'
- en: '| LiSHT [[24](#bib.bib24)] | 86.53 $\pm$ 0.49 | 89.83 $\pm$ 0.28 | 90.27 $\pm$
    0.80 | 90.89 $\pm$ 0.66 | 90.25 $\pm$ 0.84 | 87.91 $\pm$ 0.93 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| LiSHT [[24](#bib.bib24)] | 86.53 $\pm$ 0.49 | 89.83 $\pm$ 0.28 | 90.27 $\pm$
    0.80 | 90.89 $\pm$ 0.66 | 90.25 $\pm$ 0.84 | 87.91 $\pm$ 0.93 |'
- en: '| SRS [[26](#bib.bib26)] | 89.43 $\pm$ 0.81 | 92.06 $\pm$ 0.26 | 91.36 $\pm$
    1.19 | 92.28 $\pm$ 0.48 | 78.05 $\pm$ 1.37 | 90.64 $\pm$ 1.93 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| SRS [[26](#bib.bib26)] | 89.43 $\pm$ 0.81 | 92.06 $\pm$ 0.26 | 91.36 $\pm$
    1.19 | 92.28 $\pm$ 0.48 | 78.05 $\pm$ 1.37 | 90.64 $\pm$ 1.93 |'
- en: '| Mish [[99](#bib.bib99)] | 90.82 $\pm$ 0.15 | 92.85  $\pm$ 0.25 | 93.29 $\pm$
    0.61 | 93.69 $\pm$ 0.63 | 93.66 $\pm$ 0.12 | 93.62 $\pm$ 0.62 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Mish [[99](#bib.bib99)] | 90.82 $\pm$ 0.15 | 92.85 $\pm$ 0.25 | 93.29 $\pm$
    0.61 | 93.69 $\pm$ 0.63 | 93.66 $\pm$ 0.12 | 93.62 $\pm$ 0.62 |'
- en: '| PAU [[111](#bib.bib111)] | 90.67 $\pm$ 0.17 | 92.00 $\pm$ 0.26 | 92.80 $\pm$
    0.65 | 93.67 $\pm$ 0.52 | 93.08 $\pm$ 0.20 | 93.05 $\pm$ 0.53 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| PAU [[111](#bib.bib111)] | 90.67 $\pm$ 0.17 | 92.00 $\pm$ 0.26 | 92.80 $\pm$
    0.65 | 93.67 $\pm$ 0.52 | 93.08 $\pm$ 0.20 | 93.05 $\pm$ 0.53 |'
- en: '| PDELU [[59](#bib.bib59)] | 90.18 $\pm$ 0.19 | 92.80 $\pm$ 0.13 | 93.49  $\pm$
    0.30 | 93.42 $\pm$ 0.71 | 93.71  $\pm$ 0.07 | 93.96  $\pm$ 0.59 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| PDELU [[59](#bib.bib59)] | 90.18 $\pm$ 0.19 | 92.80 $\pm$ 0.13 | 93.49 $\pm$
    0.30 | 93.42 $\pm$ 0.71 | 93.71 $\pm$ 0.07 | 93.96 $\pm$ 0.59 |'
- en: 'Table 9: Experimental results comparison over CIFAR100 dataset.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 9: CIFAR100 æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ¯”è¾ƒã€‚'
- en: '| Accuracy | CNN Models |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| å‡†ç¡®ç‡ | CNN æ¨¡å‹ |'
- en: '| --- | --- |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Activations | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121
    |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| æ¿€æ´»å‡½æ•° | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Sigmoid | 61.88 $\pm$ 0.18 | 37.75 $\pm$ 0.59 | 70.31 $\pm$ 0.54 | 46.78
    $\pm$ 5.42 | 66.17 $\pm$ 1.16 | 68.31 $\pm$ 2.41 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | 61.88 $\pm$ 0.18 | 37.75 $\pm$ 0.59 | 70.31 $\pm$ 0.54 | 46.78
    $\pm$ 5.42 | 66.17 $\pm$ 1.16 | 68.31 $\pm$ 2.41 |'
- en: '| Tanh | 53.10 $\pm$ 0.51 | 58.43 $\pm$ 0.38 | 67.66 $\pm$ 2.32 | 64.32 $\pm$
    1.69 | 60.13 $\pm$ 1.86 | 69.53 $\pm$ 1.68 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Tanh | 53.10 $\pm$ 0.51 | 58.43 $\pm$ 0.38 | 67.66 $\pm$ 2.32 | 64.32 $\pm$
    1.69 | 60.13 $\pm$ 1.86 | 69.53 $\pm$ 1.68 |'
- en: '| Elliott [[25](#bib.bib25)] | 60.70 $\pm$ 0.34 | 33.20 $\pm$ 0.97 | 64.85
    $\pm$ 6.28 | 49.88 $\pm$ 4.03 | 66.30 $\pm$ 0.28 | 69.58 $\pm$ 2.40 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Elliott [[25](#bib.bib25)] | 60.70 $\pm$ 0.34 | 33.20 $\pm$ 0.97 | 64.85
    $\pm$ 6.28 | 49.88 $\pm$ 4.03 | 66.30 $\pm$ 0.28 | 69.58 $\pm$ 2.40 |'
- en: '| ReLU [[8](#bib.bib8)] | 61.33 $\pm$ 0.34 | 67.47 $\pm$ 0.44 | 74.05 $\pm$
    1.69 | 71.96 $\pm$ 0.94 | 70.45 $\pm$ 0.73 | 72.99 $\pm$ 1.35 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| ReLU [[8](#bib.bib8)] | 61.33 $\pm$ 0.34 | 67.47 $\pm$ 0.44 | 74.05 $\pm$
    1.69 | 71.96 $\pm$ 0.94 | 70.45 $\pm$ 0.73 | 72.99 $\pm$ 1.35 |'
- en: '| LReLU [[17](#bib.bib17)] | 61.13 $\pm$ 0.41 | 65.72 $\pm$ 0.14 | 63.79 $\pm$
    2.38 | 72.77  $\pm$ 0.49 | 70.58 $\pm$ 0.45 | 73.33 $\pm$ 1.25 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| LReLU [[17](#bib.bib17)] | 61.13 $\pm$ 0.41 | 65.72 $\pm$ 0.14 | 63.79 $\pm$
    2.38 | 72.77 $\pm$ 0.49 | 70.58 $\pm$ 0.45 | 73.33 $\pm$ 1.25 |'
- en: '| PReLU [[35](#bib.bib35)] | 59.86 $\pm$ 0.35 | 65.26 $\pm$ 0.40 | 69.57 $\pm$
    1.50 | 71.08 $\pm$ 1.70 | 69.77 $\pm$ 0.48 | 68.23 $\pm$ 1.55 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| PReLU [[35](#bib.bib35)] | 59.86 $\pm$ 0.35 | 65.26 $\pm$ 0.40 | 69.57 $\pm$
    1.50 | 71.08 $\pm$ 1.70 | 69.77 $\pm$ 0.48 | 68.23 $\pm$ 1.55 |'
- en: '| ELU [[27](#bib.bib27)] | 61.97  $\pm$ 0.24 | 51.35 $\pm$ 3.01 | 72.57 $\pm$
    1.76 | 71.41 $\pm$ 1.63 | 71.27  $\pm$ 0.58 | 72.06 $\pm$ 1.93 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| ELU [[27](#bib.bib27)] | 61.97 $\pm$ 0.24 | 51.35 $\pm$ 3.01 | 72.57 $\pm$
    1.76 | 71.41 $\pm$ 1.63 | 71.27 $\pm$ 0.58 | 72.06 $\pm$ 1.93 |'
- en: '| SELU [[52](#bib.bib52)] | 59.62 $\pm$ 0.39 | 64.55 $\pm$ 0.43 | 71.47 $\pm$
    1.39 | 69.94 $\pm$ 1.92 | 55.01 $\pm$ 0.98 | 70.15 $\pm$ 1.04 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| SELU [[52](#bib.bib52)] | 59.62 $\pm$ 0.39 | 64.55 $\pm$ 0.43 | 71.47 $\pm$
    1.39 | 69.94 $\pm$ 1.92 | 55.01 $\pm$ 0.98 | 70.15 $\pm$ 1.04 |'
- en: '| GELU [[101](#bib.bib101)] | 61.20 $\pm$ 0.61 | 67.25 $\pm$ 0.38 | 74.27  $\pm$
    0.70 | 71.58 $\pm$ 0.87 | 71.14 $\pm$ 0.29 | 73.31 $\pm$ 1.70 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| GELU [[101](#bib.bib101)] | 61.20 $\pm$ 0.61 | 67.25 $\pm$ 0.38 | 74.27 $\pm$
    0.70 | 71.58 $\pm$ 0.87 | 71.14 $\pm$ 0.29 | 73.31 $\pm$ 1.70 |'
- en: '| CELU [[53](#bib.bib53)] | 61.90 $\pm$ 0.21 | 55.78 $\pm$ 0.69 | 72.87 $\pm$
    1.52 | 70.95 $\pm$ 1.40 | 63.43 $\pm$ 0.81 | 72.68 $\pm$ 1.16 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| CELU [[53](#bib.bib53)] | 61.90 $\pm$ 0.21 | 55.78 $\pm$ 0.69 | 72.87 $\pm$
    1.52 | 70.95 $\pm$ 1.40 | 63.43 $\pm$ 0.81 | 72.68 $\pm$ 1.16 |'
- en: '| Softplus [[93](#bib.bib93)] | 62.59  $\pm$ 0.21 | 67.70 $\pm$ 0.19 | 73.08
    $\pm$ 1.66 | 71.99 $\pm$ 2.03 | 71.16  $\pm$ 0.46 | 72.54 $\pm$ 1.73 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| Softplus [[93](#bib.bib93)] | 62.59 $\pm$ 0.21 | 67.70 $\pm$ 0.19 | 73.08
    $\pm$ 1.66 | 71.99 $\pm$ 2.03 | 71.16 $\pm$ 0.46 | 72.54 $\pm$ 1.73 |'
- en: '| Swish [[29](#bib.bib29)] | 59.40 $\pm$ 0.41 | 66.05 $\pm$ 0.82 | 71.56 $\pm$
    1.66 | 71.12 $\pm$ 2.08 | 68.42 $\pm$ 1.62 | 71.34 $\pm$ 1.10 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Swish [[29](#bib.bib29)] | 59.40 $\pm$ 0.41 | 66.05 $\pm$ 0.82 | 71.56 $\pm$
    1.66 | 71.12 $\pm$ 2.08 | 68.42 $\pm$ 1.62 | 71.34 $\pm$ 1.10 |'
- en: '| ABReLU [[44](#bib.bib44)] | 56.21 $\pm$ 0.53 | 66.95 $\pm$ 0.09 | 71.83 $\pm$
    2.26 | 71.96 $\pm$ 1.43 | 70.47 $\pm$ 0.91 | 73.79  $\pm$ 1.45 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| ABReLU [[44](#bib.bib44)] | 56.21 $\pm$ 0.53 | 66.95 $\pm$ 0.09 | 71.83 $\pm$
    2.26 | 71.96 $\pm$ 1.43 | 70.47 $\pm$ 0.91 | 73.79 $\pm$ 1.45 |'
- en: '| LiSHT [[24](#bib.bib24)] | 54.09 $\pm$ 1.54 | 58.87 $\pm$ 0.81 | 66.66 $\pm$
    2.50 | 65.28 $\pm$ 1.33 | 66.01 $\pm$ 1.04 | 65.61 $\pm$ 1.10 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| LiSHT [[24](#bib.bib24)] | 54.09 $\pm$ 1.54 | 58.87 $\pm$ 0.81 | 66.66 $\pm$
    2.50 | 65.28 $\pm$ 1.33 | 66.01 $\pm$ 1.04 | 65.61 $\pm$ 1.10 |'
- en: '| SRS [[26](#bib.bib26)] | 54.93 $\pm$ 0.80 | 58.22 $\pm$ 1.09 | 70.39 $\pm$
    1.09 | 67.11 $\pm$ 1.46 | 36.95 $\pm$ 0.93 | 64.52 $\pm$ 1.39 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| SRS [[26](#bib.bib26)] | 54.93 $\pm$ 0.80 | 58.22 $\pm$ 1.09 | 70.39 $\pm$
    1.09 | 67.11 $\pm$ 1.46 | 36.95 $\pm$ 0.93 | 64.52 $\pm$ 1.39 |'
- en: '| Mish [[99](#bib.bib99)] | 61.81 $\pm$ 0.54 | 68.13  $\pm$ 0.40 | 73.76 $\pm$
    1.48 | 71.89 $\pm$ 1.12 | 70.80 $\pm$ 0.68 | 73.49 $\pm$ 1.39 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| Mish [[99](#bib.bib99)] | 61.81 $\pm$ 0.54 | 68.13 $\pm$ 0.40 | 73.76 $\pm$
    1.48 | 71.89 $\pm$ 1.12 | 70.80 $\pm$ 0.68 | 73.49 $\pm$ 1.39 |'
- en: '| PAU [[111](#bib.bib111)] | 59.81 $\pm$ 0.61 | 64.14 $\pm$ 0.62 | 70.48 $\pm$
    1.53 | 68.59 $\pm$ 2.15 | 68.29 $\pm$ 0.77 | 67.83 $\pm$ 0.35 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| PAU [[111](#bib.bib111)] | 59.81 $\pm$ 0.61 | 64.14 $\pm$ 0.62 | 70.48 $\pm$
    1.53 | 68.59 $\pm$ 2.15 | 68.29 $\pm$ 0.77 | 67.83 $\pm$ 0.35 |'
- en: '| PDELU [[59](#bib.bib59)] | 61.35 $\pm$ 0.56 | 67.92  $\pm$ 0.32 | 74.48  $\pm$
    1.23 | 72.11  $\pm$ 1.60 | 70.81 $\pm$ 0.47 | 73.71  $\pm$ 1.64 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| PDELU [[59](#bib.bib59)] | 61.35 $\pm$ 0.56 | 67.92 $\pm$ 0.32 | 74.48 $\pm$
    1.23 | 72.11 $\pm$ 1.60 | 70.81 $\pm$ 0.47 | 73.71 $\pm$ 1.64 |'
- en: '![Refer to caption](img/f5331dcc88b4026195f29da0a2c88d56.png)![Refer to caption](img/2b7dba249145771874a090e8cb1c7c45.png)![Refer
    to caption](img/a37314d201c61434ffee27ead63a536b.png)![Refer to caption](img/f54b2d2f5973c53657d09b59643f1c60.png)![Refer
    to caption](img/dbffd53297d0d9f34e8ee2f76407f523.png)![Refer to caption](img/c830f4eb908caee10e73dad286ef189d.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/f5331dcc88b4026195f29da0a2c88d56.png)![å‚è§è¯´æ˜](img/2b7dba249145771874a090e8cb1c7c45.png)![å‚è§è¯´æ˜](img/a37314d201c61434ffee27ead63a536b.png)![å‚è§è¯´æ˜](img/f54b2d2f5973c53657d09b59643f1c60.png)![å‚è§è¯´æ˜](img/dbffd53297d0d9f34e8ee2f76407f523.png)![å‚è§è¯´æ˜](img/c830f4eb908caee10e73dad286ef189d.png)'
- en: 'Figure 3: Convergence plots over CIFAR100 dataset.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 3: CIFAR100 æ•°æ®é›†ä¸Šçš„æ”¶æ•›å›¾ã€‚'
- en: 9.2 Experimental Performance Analysis
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 å®éªŒæ€§èƒ½åˆ†æ
- en: In order to compare the AFs, three experiments are conducted in this paper,
    including image classification, language translation and speech recognition. Eighteen
    state-of-the-art AFs are considered for analysis, including Logistic Sigmoid,
    Tanh, Elliott [[25](#bib.bib25)], ReLU [[8](#bib.bib8)], LReLU [[34](#bib.bib34)]
    PReLU [[35](#bib.bib35)], ELU [[27](#bib.bib27)], SELU [[52](#bib.bib52)], GELU
    [[101](#bib.bib101)], CELU [[53](#bib.bib53)], Softplus [[93](#bib.bib93)], Swish
    [[29](#bib.bib29)], ABReLU [[44](#bib.bib44)], LiSHT [[24](#bib.bib24)], Soft-Root-Sign
    (SRS) [[26](#bib.bib26)], Mish [[99](#bib.bib99)], PAU [[111](#bib.bib111)] and
    PDELU [[59](#bib.bib59)]. Note that Swish, ABReLU, LiSHT, SRS, Mish, PAU and PDELU
    are the most recent functions. Google Colab based computational resource is used
    in most of the experiments. Few experiments are also performed over a desktop
    system consisting of 8 GB GPU. The PyTorch framework is used in all the experiments.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¯”è¾ƒ AFsï¼Œæœ¬æ–‡è¿›è¡Œäº†ä¸‰é¡¹å®éªŒï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€è¯­è¨€ç¿»è¯‘å’Œè¯­éŸ³è¯†åˆ«ã€‚è€ƒè™‘äº†åå…«ç§æœ€å…ˆè¿›çš„ AFs è¿›è¡Œåˆ†æï¼ŒåŒ…æ‹¬ Logistic Sigmoidã€Tanhã€Elliott
    [[25](#bib.bib25)]ã€ReLU [[8](#bib.bib8)]ã€LReLU [[34](#bib.bib34)]ã€PReLU [[35](#bib.bib35)]ã€ELU
    [[27](#bib.bib27)]ã€SELU [[52](#bib.bib52)]ã€GELU [[101](#bib.bib101)]ã€CELU [[53](#bib.bib53)]ã€Softplus
    [[93](#bib.bib93)]ã€Swish [[29](#bib.bib29)]ã€ABReLU [[44](#bib.bib44)]ã€LiSHT [[24](#bib.bib24)]ã€Soft-Root-Sign
    (SRS) [[26](#bib.bib26)]ã€Mish [[99](#bib.bib99)]ã€PAU [[111](#bib.bib111)] å’Œ PDELU
    [[59](#bib.bib59)]ã€‚æ³¨æ„ï¼ŒSwishã€ABReLUã€LiSHTã€SRSã€Mishã€PAU å’Œ PDELU æ˜¯æœ€æ–°çš„å‡½æ•°ã€‚å¤§å¤šæ•°å®éªŒä½¿ç”¨äº†åŸºäº
    Google Colab çš„è®¡ç®—èµ„æºã€‚ä¹Ÿæœ‰å°‘æ•°å®éªŒåœ¨é…å¤‡ 8 GB GPU çš„æ¡Œé¢ç³»ç»Ÿä¸Šè¿›è¡Œã€‚æ‰€æœ‰å®éªŒå‡ä½¿ç”¨äº† PyTorch æ¡†æ¶ã€‚
- en: The CIFAR10 and CIFAR100 datasetsÂ¹Â¹1[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
    [[148](#bib.bib148)] are used for the image classification experiment in this
    paper. The CIFAR10 dataset contains $50,000$ training images and $10,000$ test
    images from $10$ object categories. The CIFAR100 dataset contains $50,000$ training
    images and $10,000$ test images from $100$ object categories. We also utilize
    the language translation and speech recognition datasets for the experiments.
    For the experiments over CIFAR-10 and CIFAR-100 datasets, training is performed
    for 100 Epochs. The batch size is 128 for CIFAR-10 and 64 for CIFAR-100\. The
    learning rate is 0.001 for first 80 Epochs and 0.0001 for last 20 Epochs. Random
    crop and random horizontal flip are the data augmentation used during training.
    Data normalization is performed both during train and test times. Adam optimizer
    is used for the training with cross entropy loss. All existing activation functions
    except softmax are replaced with the corresponding activation function in different
    networks.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR10 å’Œ CIFAR100 æ•°æ®é›†Â¹Â¹1[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
    [[148](#bib.bib148)] è¢«ç”¨äºæœ¬æ–‡ä¸­çš„å›¾åƒåˆ†ç±»å®éªŒã€‚CIFAR10 æ•°æ®é›†åŒ…å« $50,000$ å¼ è®­ç»ƒå›¾åƒå’Œ $10,000$ å¼ æµ‹è¯•å›¾åƒï¼Œæ¶µç›–
    $10$ ä¸ªç‰©ä½“ç±»åˆ«ã€‚CIFAR100 æ•°æ®é›†åŒ…å« $50,000$ å¼ è®­ç»ƒå›¾åƒå’Œ $10,000$ å¼ æµ‹è¯•å›¾åƒï¼Œæ¶µç›– $100$ ä¸ªç‰©ä½“ç±»åˆ«ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨äº†è¯­è¨€ç¿»è¯‘å’Œè¯­éŸ³è¯†åˆ«æ•°æ®é›†è¿›è¡Œå®éªŒã€‚åœ¨
    CIFAR-10 å’Œ CIFAR-100 æ•°æ®é›†ä¸Šçš„å®éªŒä¸­ï¼Œè®­ç»ƒè¿›è¡Œ $100$ ä¸ª Epochã€‚CIFAR-10 çš„æ‰¹æ¬¡å¤§å°ä¸º $128$ï¼ŒCIFAR-100
    çš„æ‰¹æ¬¡å¤§å°ä¸º $64$ã€‚å‰ $80$ ä¸ª Epoch çš„å­¦ä¹ ç‡ä¸º $0.001$ï¼Œæœ€å $20$ ä¸ª Epoch çš„å­¦ä¹ ç‡ä¸º $0.0001$ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†éšæœºè£å‰ªå’Œéšæœºæ°´å¹³ç¿»è½¬ä½œä¸ºæ•°æ®å¢å¼ºæ–¹æ³•ã€‚æ•°æ®æ ‡å‡†åŒ–åœ¨è®­ç»ƒå’Œæµ‹è¯•æ—¶å‡æœ‰è¿›è¡Œã€‚è®­ç»ƒä½¿ç”¨
    Adam ä¼˜åŒ–å™¨å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚æ‰€æœ‰ç°æœ‰æ¿€æ´»å‡½æ•°ï¼ˆé™¤äº† softmaxï¼‰å‡è¢«æ›¿æ¢ä¸ºä¸åŒç½‘ç»œä¸­çš„ç›¸åº”æ¿€æ´»å‡½æ•°ã€‚
- en: 'The test accuracy is reported in Tables [8](#S9.T8 "Table 8 â€£ 9.1 Comparison
    with Existing Survey/Performance Analysis â€£ 9 Performance Comparison and Analysis
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")
    and [9](#S9.T9 "Table 9 â€£ 9.1 Comparison with Existing Survey/Performance Analysis
    â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") on CIFAR10 and CIFAR100 datasets, respectively.
    In these Tables, the mean and standard deviation of image classification accuracy
    over 5 trials are reported for each AF. Moreover, the better results are highlighted.
    Different types of CNN models are used in this experiment, such as plain models
    (i.e., MobileNet [[149](#bib.bib149)] and VGG16 [[150](#bib.bib150)]), inception
    model (i.e., GoogLeNet [[151](#bib.bib151)]) and skip/residual connection based
    models (i.e., ResNet50 [[152](#bib.bib152)], SENet18 [[153](#bib.bib153)], and
    DenseNet121 [[154](#bib.bib154)]). The MobileNet, GoogLeNet and SENet18 are light
    models, whereas the VGG16, ResNet50 and DenseNet121 are heavy models in terms
    of the number of trainable parameters. Overall, it is observed that the Softplus,
    ELU and CELU are better suited with MobileNet. The ReLU, Mish and PDELU exhibit
    good performance with VGG16, GoogleNet and DenseNet. The ReLU, LReLU, ELU, GELU,
    CELU, ABReLU, and PDELU activation functions are better for the networks having
    residual connections, such as ResNet50, SENet18 and DenseNet121. In order to demonstrate
    the convergence of different AFs, the training loss vs epochs is plotted in Fig.
    [3](#S9.F3 "Figure 3 â€£ 9.1 Comparison with Existing Survey/Performance Analysis
    â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") on CIFAR100 dataset using different models.
    The PAU has emerged as a promising AF with fastest convergence in most of the
    cases. The PReLU, GELU and PDELU AFs are also consistent with good convergence.
    Note that the training diverges with SRS for the SENet18 model. Sigmoid and Elliott
    AFs showed the poorest convergence. The time taken for the training is also computed
    for different AFs using different CNN models on CIFAR100 dataset and reported
    in Table [10](#S9.T10 "Table 10 â€£ 9.2 Experimental Performance Analysis â€£ 9 Performance
    Comparison and Analysis â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark"). These results are computed using a desktop computer system
    having 32 GB RAM and 8 GB Nvidia GPU Card for 100 epochs of training. The time
    is represented in hh:mm:ss format. It is clear that PDELU AF is very inefficient.
    Moreover, SRS and Elliott also take more time for training. The activations such
    as ReLU, ELU, CELU, and Softplus depict a good tradeoff between the accuracy and
    training time.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 'æµ‹è¯•å‡†ç¡®ç‡åœ¨è¡¨æ ¼[8](#S9.T8 "Table 8 â€£ 9.1 Comparison with Existing Survey/Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark")å’Œ[9](#S9.T9 "Table 9 â€£ 9.1 Comparison
    with Existing Survey/Performance Analysis â€£ 9 Performance Comparison and Analysis
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")ä¸­åˆ†åˆ«æŠ¥å‘Šäº†CIFAR10å’ŒCIFAR100æ•°æ®é›†ä¸Šçš„ç»“æœã€‚åœ¨è¿™äº›è¡¨æ ¼ä¸­ï¼ŒæŠ¥å‘Šäº†æ¯ç§æ¿€æ´»å‡½æ•°ï¼ˆAFï¼‰åœ¨5æ¬¡å®éªŒä¸­çš„å›¾åƒåˆ†ç±»å‡†ç¡®ç‡çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚æ­¤å¤–ï¼Œæ›´å¥½çš„ç»“æœä¼šè¢«çªå‡ºæ˜¾ç¤ºã€‚åœ¨æ­¤æ¬¡å®éªŒä¸­ä½¿ç”¨äº†ä¸åŒç±»å‹çš„CNNæ¨¡å‹ï¼Œä¾‹å¦‚æ™®é€šæ¨¡å‹ï¼ˆå³ï¼ŒMobileNet
    [[149](#bib.bib149)] å’Œ VGG16 [[150](#bib.bib150)]ï¼‰ã€Inceptionæ¨¡å‹ï¼ˆå³ï¼ŒGoogLeNet [[151](#bib.bib151)]ï¼‰å’ŒåŸºäºè·³è·ƒ/æ®‹å·®è¿æ¥çš„æ¨¡å‹ï¼ˆå³ï¼ŒResNet50
    [[152](#bib.bib152)]ã€SENet18 [[153](#bib.bib153)] å’Œ DenseNet121 [[154](#bib.bib154)]ï¼‰ã€‚MobileNetã€GoogLeNet
    å’Œ SENet18 æ˜¯è½»é‡çº§æ¨¡å‹ï¼Œè€Œ VGG16ã€ResNet50 å’Œ DenseNet121 æ˜¯åœ¨å¯è®­ç»ƒå‚æ•°æ•°é‡ä¸Šè¾ƒé‡çš„æ¨¡å‹ã€‚æ€»ä½“ä¸Šè§‚å¯Ÿåˆ°ï¼ŒSoftplusã€ELU
    å’Œ CELU æ›´é€‚åˆä¸ MobileNet é…åˆä½¿ç”¨ã€‚ReLUã€Mish å’Œ PDELU åœ¨ VGG16ã€GoogLeNet å’Œ DenseNet ä¸Šè¡¨ç°è‰¯å¥½ã€‚ReLUã€LReLUã€ELUã€GELUã€CELUã€ABReLU
    å’Œ PDELU æ¿€æ´»å‡½æ•°æ›´é€‚åˆå…·æœ‰æ®‹å·®è¿æ¥çš„ç½‘ç»œï¼Œå¦‚ ResNet50ã€SENet18 å’Œ DenseNet121ã€‚ä¸ºäº†å±•ç¤ºä¸åŒ AF çš„æ”¶æ•›æƒ…å†µï¼Œå›¾[3](#S9.F3
    "Figure 3 â€£ 9.1 Comparison with Existing Survey/Performance Analysis â€£ 9 Performance
    Comparison and Analysis â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")ä¸­ç»˜åˆ¶äº†ä¸åŒæ¨¡å‹åœ¨ CIFAR100 æ•°æ®é›†ä¸Šçš„è®­ç»ƒæŸå¤±ä¸è¿­ä»£æ¬¡æ•°çš„å…³ç³»ã€‚PAU åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¡¨ç°å‡ºæœ€å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œæˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ¿€æ´»å‡½æ•°ã€‚PReLUã€GELU
    å’Œ PDELU æ¿€æ´»å‡½æ•°ä¹Ÿå…·æœ‰ä¸€è‡´çš„è‰¯å¥½æ”¶æ•›æ€§ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒSENet18 æ¨¡å‹åœ¨ä½¿ç”¨ SRS æ—¶è®­ç»ƒä¼šå‡ºç°å‘æ•£ã€‚Sigmoid å’Œ Elliott æ¿€æ´»å‡½æ•°çš„æ”¶æ•›æ•ˆæœæœ€å·®ã€‚ä½¿ç”¨ä¸åŒ
    CNN æ¨¡å‹åœ¨ CIFAR100 æ•°æ®é›†ä¸Šè®¡ç®—äº†ä¸åŒ AF çš„è®­ç»ƒæ—¶é—´ï¼Œå¹¶åœ¨è¡¨[10](#S9.T10 "Table 10 â€£ 9.2 Experimental
    Performance Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark")ä¸­æŠ¥å‘Šäº†è¿™äº›ç»“æœã€‚è¿™äº›ç»“æœæ˜¯ä½¿ç”¨é…å¤‡ 32
    GB RAM å’Œ 8 GB Nvidia GPU å¡çš„å°å¼è®¡ç®—æœºç³»ç»Ÿè¿›è¡Œ 100 è½®è®­ç»ƒè®¡ç®—å¾—å‡ºçš„ã€‚æ—¶é—´ä»¥ hh:mm:ss æ ¼å¼è¡¨ç¤ºã€‚æ˜¾ç„¶ï¼ŒPDELU æ¿€æ´»å‡½æ•°æ•ˆç‡éå¸¸ä½ã€‚æ­¤å¤–ï¼ŒSRS
    å’Œ Elliott æ¿€æ´»å‡½æ•°çš„è®­ç»ƒæ—¶é—´ä¹Ÿè¾ƒé•¿ã€‚åƒ ReLUã€ELUã€CELU å’Œ Softplus è¿™æ ·çš„æ¿€æ´»å‡½æ•°åœ¨å‡†ç¡®æ€§å’Œè®­ç»ƒæ—¶é—´ä¹‹é—´è¡¨ç°å‡ºè‰¯å¥½çš„æƒè¡¡ã€‚'
- en: 'Table 10: Training time (hh:mm:ss) comparison over CIFAR100 dataset.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 10ï¼šCIFAR100 æ•°æ®é›†ä¸Šçš„è®­ç»ƒæ—¶é—´ (hh:mm:ss) æ¯”è¾ƒã€‚
- en: '| Training Time | CNN Models |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| è®­ç»ƒæ—¶é—´ | CNN æ¨¡å‹ |'
- en: '| --- | --- |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Activations | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| æ¿€æ´»å‡½æ•° | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Sigmoid | 00:33:15 | 00:49:16 | 04:55:54 | 03:36:03 | 01:13:14 | 04:12:24
    |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | 00:33:15 | 00:49:16 | 04:55:54 | 03:36:03 | 01:13:14 | 04:12:24
    |'
- en: '| Tanh | 00:33:18 | 00:49:55 | 04:58:02 | 03:33:03 | 01:13:18 | 04:09:24 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Tanh | 00:33:18 | 00:49:55 | 04:58:02 | 03:33:03 | 01:13:18 | 04:09:24 |'
- en: '| Elliott [[25](#bib.bib25)] | 00:49:52 | 00:59:13 | 06:53:55 | 05:38:49 |
    01:41:38 | 07:46:55 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| Elliott [[25](#bib.bib25)] | 00:49:52 | 00:59:13 | 06:53:55 | 05:38:49 |
    01:41:38 | 07:46:55 |'
- en: '| ReLU [[8](#bib.bib8)] | 00:31:22 | 00:47:19 | 04:55:10 | 03:32:30 | 01:15:33
    | 04:15:06 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| ReLU [[8](#bib.bib8)] | 00:31:22 | 00:47:19 | 04:55:10 | 03:32:30 | 01:15:33
    | 04:15:06 |'
- en: '| LReLU [[34](#bib.bib34)] | 00:31:48 | 00:49:03 | 05:01:30 | 03:33:00 | 01:18:38
    | 04:14:09 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| LReLU [[34](#bib.bib34)] | 00:31:48 | 00:49:03 | 05:01:30 | 03:33:00 | 01:18:38
    | 04:14:09 |'
- en: '| PReLU [[35](#bib.bib35)] | 00:44:24 | 00:49:01 | 05:42:18 | 03:55:57 | 01:27:05
    | 04:55:47 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| PReLU [[35](#bib.bib35)] | 00:44:24 | 00:49:01 | 05:42:18 | 03:55:57 | 01:27:05
    | 04:55:47 |'
- en: '| ELU [[27](#bib.bib27)] | 00:31:05 | 00:47:38 | 04:57:37 | 03:36:47 | 01:13:25
    | 04:08:39 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| ELU [[27](#bib.bib27)] | 00:31:05 | 00:47:38 | 04:57:37 | 03:36:47 | 01:13:25
    | 04:08:39 |'
- en: '| SELU [[52](#bib.bib52)] | 00:29:40 | 00:47:31 | 04:54:57 | 03:33:47 | 01:13:27
    | 04:09:17 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| SELU [[52](#bib.bib52)] | 00:29:40 | 00:47:31 | 04:54:57 | 03:33:47 | 01:13:27
    | 04:09:17 |'
- en: '| GELU [[101](#bib.bib101)] | 00:29:43 | 00:47:22 | 04:55:53 | 03:32:32 | 01:13:32
    | 04:11:26 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| GELU [[101](#bib.bib101)] | 00:29:43 | 00:47:22 | 04:55:53 | 03:32:32 | 01:13:32
    | 04:11:26 |'
- en: '| CELU [[53](#bib.bib53)] | 00:29:36 | 00:46:47 | 05:00:44 | 03:31:40 | 01:14:08
    | 04:18:11 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| CELU [[53](#bib.bib53)] | 00:29:36 | 00:46:47 | 05:00:44 | 03:31:40 | 01:14:08
    | 04:18:11 |'
- en: '| Softplus [[93](#bib.bib93)] | 00:29:44 | 00:47:06 | 04:58:55 | 03:32:03 |
    01:14:02 | 04:12:08 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| Softplus [[93](#bib.bib93)] | 00:29:44 | 00:47:06 | 04:58:55 | 03:32:03 |
    01:14:02 | 04:12:08 |'
- en: '| Swish [[29](#bib.bib29)] | 00:43:13 | 00:55:37 | 06:18:38 | 04:58:38 | 01:32:15
    | 06:41:14 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| Swish [[29](#bib.bib29)] | 00:43:13 | 00:55:37 | 06:18:38 | 04:58:38 | 01:32:15
    | 06:41:14 |'
- en: '| ABReLU [[44](#bib.bib44)] | 00:38:51 | 00:53:49 | 05:43:59 | 04:27:02 | 01:25:30
    | 05:42:53 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| ABReLU [[44](#bib.bib44)] | 00:38:51 | 00:53:49 | 05:43:59 | 04:27:02 | 01:25:30
    | 05:42:53 |'
- en: '| LiSHT [[24](#bib.bib24)] | 00:37:01 | 00:54:10 | 05:40:00 | 04:25:57 | 01:23:59
    | 05:38:15 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| LiSHT [[24](#bib.bib24)] | 00:37:01 | 00:54:10 | 05:40:00 | 04:25:57 | 01:23:59
    | 05:38:15 |'
- en: '| SRS [[26](#bib.bib26)] | 01:06:38 | 01:11:36 | 08:43:09 | 07:35:35 | 02:05:33
    | 11:10:27 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| SRS [[26](#bib.bib26)] | 01:06:38 | 01:11:36 | 08:43:09 | 07:35:35 | 02:05:33
    | 11:10:27 |'
- en: '| Mish [[99](#bib.bib99)] | 00:40:19 | 00:54:23 | 05:59:48 | 04:46:45 | 01:28:53
    | 06:10:27 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Mish [[99](#bib.bib99)] | 00:40:19 | 00:54:23 | 05:59:48 | 04:46:45 | 01:28:53
    | 06:10:27 |'
- en: '| PAU [[111](#bib.bib111)] | 00:41:59 | 00:54:10 | 05:54:22 | 04:12:31 | 01:25:37
    | 05:39:57 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| PAU [[111](#bib.bib111)] | 00:41:59 | 00:54:10 | 05:54:22 | 04:12:31 | 01:25:37
    | 05:39:57 |'
- en: '| PDELU [[59](#bib.bib59)] | 05:23:38 | 04:01:55 | 34:22:00 | 36:48:48 | 08:32:40
    | 50:23:00 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| PDELU [[59](#bib.bib59)] | 05:23:38 | 04:01:55 | 34:22:00 | 36:48:48 | 08:32:40
    | 50:23:00 |'
- en: 'Table 11: Experimental results for German to English language translation and
    speech recognition tasks.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨11ï¼šå¾·è¯­åˆ°è‹±è¯­çš„è¯­è¨€ç¿»è¯‘å’Œè¯­éŸ³è¯†åˆ«ä»»åŠ¡çš„å®éªŒç»“æœã€‚
- en: '|  | Language Translation |  | Speech Recognition |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | è¯­è¨€ç¿»è¯‘ |  | è¯­éŸ³è¯†åˆ« |'
- en: '| --- | --- | --- | --- |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Activations | Bleu Score |  | Average CER | Average WER |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| æ¿€æ´»å‡½æ•° | Bleu Score |  | å¹³å‡ CER | å¹³å‡ WER |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Sigmoid | 14.59 $\pm$ 0.47 |  | 0.53 $\pm$ 0.18 | 1.19 $\pm$ 0.39 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | 14.59 $\pm$ 0.47 |  | 0.53 $\pm$ 0.18 | 1.19 $\pm$ 0.39 |'
- en: '| Tanh | 20.93  $\pm$ 0.91 |  | 0.26 $\pm$ 0 | 0.68 $\pm$ 0 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| Tanh | 20.93  $\pm$ 0.91 |  | 0.26 $\pm$ 0 | 0.68 $\pm$ 0 |'
- en: '| Elliott [[25](#bib.bib25)] | 14.49 $\pm$ 0.96 |  | 0.40 $\pm$ 0.01 | 0.93
    $\pm$ 0.01 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| Elliott [[25](#bib.bib25)] | 14.49 $\pm$ 0.96 |  | 0.40 $\pm$ 0.01 | 0.93
    $\pm$ 0.01 |'
- en: '| ReLU [[8](#bib.bib8)] | 18.88 $\pm$ 0.86 |  | 0.24  $\pm$ 0.01 | 0.66  $\pm$
    0.01 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| ReLU [[8](#bib.bib8)] | 18.88 $\pm$ 0.86 |  | 0.24  $\pm$ 0.01 | 0.66  $\pm$
    0.01 |'
- en: '| LReLU [[34](#bib.bib34)] | 18.89 $\pm$ 0.82 |  | 0.24  $\pm$ 0 | 0.66  $\pm$
    0.01 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| LReLU [[34](#bib.bib34)] | 18.89 $\pm$ 0.82 |  | 0.24  $\pm$ 0 | 0.66  $\pm$
    0.01 |'
- en: '| PReLU [[35](#bib.bib35)] | 20.04 $\pm$ 0.98 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| PReLU [[35](#bib.bib35)] | 20.04 $\pm$ 0.98 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
- en: '| ELU [[27](#bib.bib27)] | 19.40 $\pm$ 1.33 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| ELU [[27](#bib.bib27)] | 19.40 $\pm$ 1.33 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0 |'
- en: '| SELU [[52](#bib.bib52)] | 20.85  $\pm$ 0.64 |  | 0.26 $\pm$ 0 | 0.69 $\pm$
    0.01 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| SELU [[52](#bib.bib52)] | 20.85  $\pm$ 0.64 |  | 0.26 $\pm$ 0 | 0.69 $\pm$
    0.01 |'
- en: '| GELU [[101](#bib.bib101)] | 18.75 $\pm$ 1.83 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| GELU [[101](#bib.bib101)] | 18.75 $\pm$ 1.83 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
- en: '| CELU [[53](#bib.bib53)] | 18.71 $\pm$ 0.55 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| CELU [[53](#bib.bib53)] | 18.71 $\pm$ 0.55 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0 |'
- en: '| Softplus [[93](#bib.bib93)] | 16.78 $\pm$ 0.84 |  | 0.30 $\pm$ 0.01 | 0.76
    $\pm$ 0.02 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| Softplus [[93](#bib.bib93)] | 16.78 $\pm$ 0.84 |  | 0.30 $\pm$ 0.01 | 0.76
    $\pm$ 0.02 |'
- en: '| Swish [[29](#bib.bib29)] | 19.51 $\pm$ 0.97 |  | 0.24  $\pm$ 0.01 | 0.65  $\pm$
    0.01 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| Swish [[29](#bib.bib29)] | 19.51 $\pm$ 0.97 |  | 0.24  $\pm$ 0.01 | 0.65  $\pm$
    0.01 |'
- en: '| ABReLU [[44](#bib.bib44)] | 17.55 $\pm$ 0.63 |  | 0.25  $\pm$ 0 | 0.68 $\pm$
    0 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| ABReLU [[44](#bib.bib44)] | 17.55 $\pm$ 0.63 |  | 0.25  $\pm$ 0 | 0.68 $\pm$
    0 |'
- en: '| LiSHT [[24](#bib.bib24)] | 20.39 $\pm$ 0.93 |  | 0.29 $\pm$ 0.01 | 0.74 $\pm$
    0.01 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| LiSHT [[24](#bib.bib24)] | 20.39 $\pm$ 0.93 |  | 0.29 $\pm$ 0.01 | 0.74 $\pm$
    0.01 |'
- en: '| SRS [[26](#bib.bib26)] | 20.66 $\pm$ 0.78 |  | 0.28 $\pm$ 0 | 0.72 $\pm$
    0 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| SRS [[26](#bib.bib26)] | 20.66 $\pm$ 0.78 |  | 0.28 $\pm$ 0 | 0.72 $\pm$
    0 |'
- en: '| Mish [[99](#bib.bib99)] | 19.56 $\pm$ 1.15 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| Mish [[99](#bib.bib99)] | 19.56 $\pm$ 1.15 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
- en: '| PAU [[111](#bib.bib111)] | 20.11 $\pm$ 1.24 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0.01 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| PAU [[111](#bib.bib111)] | 20.11 $\pm$ 1.24 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0.01 |'
- en: '| PDELU [[59](#bib.bib59)] | 19.07 $\pm$ 0.95 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0.01 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| PDELU [[59](#bib.bib59)] | 19.07 $\pm$ 0.95 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0.01 |'
- en: 'The results for language translation and speech recognition for different AFs
    are illustrated in Table [11](#S9.T11 "Table 11 â€£ 9.2 Experimental Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"). The German to English translation
    is used to test the performance of the AFs over text data. Benchmark Seq2Seq model
    consisting of a Long Short Term Memory (LSTM) based autoencoder network is used
    for the experiment. The model and dataset are downloaded from KaggleÂ²Â²2https://www.kaggle.com/parthplc/pytorch-seq2seq-machine-translation/notebook.
    The AF is applied to the feature embedding before the dropout layer. For the language
    translation experiments, the number of Epochs is set to 50 with 0.001 learning
    rate and 256 batch size. The embedding size of encoder and decoder is 300. The
    dropout factor is 0.5 for both encoder and decoder. Adam optimizer is used for
    the training with cross entropy loss. The Bleu score [[155](#bib.bib155)] with
    $4$-gram is reported in Table [11](#S9.T11 "Table 11 â€£ 9.2 Experimental Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") in $2^{nd}$ column for different
    AFs. The mean and standard deviation of Bleu score over 5 trials are reported
    for each AF. It is noticed that the Tanh and SELU AFs are better suitable for
    language translation. The PReLU, LiSHT, SRS and PAU AFs also perform better for
    language translation.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¯­è¨€ç¿»è¯‘å’Œè¯­éŸ³è¯†åˆ«çš„ç»“æœåœ¨è¡¨æ ¼[11](#S9.T11 "Table 11 â€£ 9.2 Experimental Performance Analysis
    â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark")ä¸­å±•ç¤ºã€‚å¾·è¯­åˆ°è‹±è¯­çš„ç¿»è¯‘ç”¨äºæµ‹è¯•æ¿€æ´»å‡½æ•°åœ¨æ–‡æœ¬æ•°æ®ä¸Šçš„è¡¨ç°ã€‚å®éªŒä¸­ä½¿ç”¨äº†åŸºäºé•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰çš„è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œä½œä¸ºåŸºå‡†Seq2Seqæ¨¡å‹ã€‚æ¨¡å‹å’Œæ•°æ®é›†ä»Kaggleä¸‹è½½Â²Â²2https://www.kaggle.com/parthplc/pytorch-seq2seq-machine-translation/notebookã€‚åœ¨dropoutå±‚ä¹‹å‰ï¼Œå°†æ¿€æ´»å‡½æ•°åº”ç”¨äºç‰¹å¾åµŒå…¥ã€‚å¯¹äºè¯­è¨€ç¿»è¯‘å®éªŒï¼Œè®­ç»ƒè½®æ•°è®¾ç½®ä¸º50ï¼Œå­¦ä¹ ç‡ä¸º0.001ï¼Œæ‰¹é‡å¤§å°ä¸º256ã€‚ç¼–ç å™¨å’Œè§£ç å™¨çš„åµŒå…¥å¤§å°ä¸º300ã€‚ç¼–ç å™¨å’Œè§£ç å™¨çš„dropoutå› å­å‡ä¸º0.5ã€‚è®­ç»ƒä½¿ç”¨Adamä¼˜åŒ–å™¨å’Œäº¤å‰ç†µæŸå¤±ã€‚è¡¨æ ¼[11](#S9.T11
    "Table 11 â€£ 9.2 Experimental Performance Analysis â€£ 9 Performance Comparison and
    Analysis â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")ä¸­çš„ç¬¬$2^{nd}$åˆ—æŠ¥å‘Šäº†ä¸åŒæ¿€æ´»å‡½æ•°çš„$4$-gram
    Bleuåˆ†æ•°ã€‚æ¯ä¸ªæ¿€æ´»å‡½æ•°çš„Bleuåˆ†æ•°çš„å‡å€¼å’Œæ ‡å‡†å·®åœ¨5æ¬¡å®éªŒä¸­æŠ¥å‘Šã€‚è§‚å¯Ÿåˆ°Tanhå’ŒSELUæ¿€æ´»å‡½æ•°æ›´é€‚åˆè¯­è¨€ç¿»è¯‘ã€‚PReLUã€LiSHTã€SRSå’ŒPAUæ¿€æ´»å‡½æ•°åœ¨è¯­è¨€ç¿»è¯‘ä¸­è¡¨ç°ä¹Ÿè¾ƒå¥½ã€‚'
- en: 'The speech recognition experiment is also performed to show the performance
    of the different AFs for time-series signal data. The end-to-end speech recognition
    based Deep Speech 2 framework available from assemblyaiÂ³Â³3https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch
    is used. The model consists of $2$ layers of residual convolution layers to learn
    the relevant audio features, and $2$ layers of bidirectional gated recurrent units
    (GRUs) to use the learned residual convolutional audio features. The $100$ hours
    of transcribed audio English data from LibriSpeech dataset is used for the experiment.
    For the speech recognition experiments, torchaudio 0.4.0 and torch 1.4.0 are used.
    The model consists of 2 CNN layers and 2 RNN layers. The dimension of a RNN layer
    is 512\. Number of classes is 29 in the dataset. Dropout factor is 0.5\. The learning
    rate is 0.0005, batch size is 10 and the number of Epochs is 10. The mean and
    standard deviation over 5 trials of character error rate (CER) and word error
    rate (WER) are reported in Table [11](#S9.T11 "Table 11 â€£ 9.2 Experimental Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") for speech recognition. The recent
    AFs such as PReLU, GELU, Swish, Mish and PAU AFs are found as the most suitable
    for speech recognition in this experiment.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿˜è¿›è¡Œäº†è¯­éŸ³è¯†åˆ«å®éªŒï¼Œä»¥å±•ç¤ºä¸åŒæ¿€æ´»å‡½æ•°å¯¹æ—¶é—´åºåˆ—ä¿¡å·æ•°æ®çš„æ€§èƒ½ã€‚ä½¿ç”¨äº†æ¥è‡ª assemblyaiÂ³Â³3https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch
    çš„ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ« Deep Speech 2 æ¡†æ¶ã€‚è¯¥æ¨¡å‹ç”± $2$ å±‚æ®‹å·®å·ç§¯å±‚ç»„æˆï¼Œç”¨äºå­¦ä¹ ç›¸å…³éŸ³é¢‘ç‰¹å¾ï¼Œå¹¶ä¸”æœ‰ $2$ å±‚åŒå‘é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUsï¼‰æ¥ä½¿ç”¨å­¦åˆ°çš„æ®‹å·®å·ç§¯éŸ³é¢‘ç‰¹å¾ã€‚å®éªŒä½¿ç”¨äº†æ¥è‡ª
    LibriSpeech æ•°æ®é›†çš„ $100$ å°æ—¶è½¬å½•è‹±è¯­éŸ³é¢‘æ•°æ®ã€‚è¯­éŸ³è¯†åˆ«å®éªŒä¸­ä½¿ç”¨äº† torchaudio 0.4.0 å’Œ torch 1.4.0ã€‚æ¨¡å‹ç”±
    2 å±‚ CNN å’Œ 2 å±‚ RNN ç»„æˆã€‚RNN å±‚çš„ç»´åº¦ä¸º 512ã€‚æ•°æ®é›†ä¸­ç±»åˆ«æ•°é‡ä¸º 29ã€‚Dropout å› å­ä¸º 0.5ã€‚å­¦ä¹ ç‡ä¸º 0.0005ï¼Œæ‰¹é‡å¤§å°ä¸º
    10ï¼ŒEpoch æ•°ä¸º 10ã€‚è¡¨ [11](#S9.T11 "Table 11 â€£ 9.2 Experimental Performance Analysis
    â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") ä¸­æŠ¥å‘Šäº† 5 æ¬¡è¯•éªŒçš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰å’Œè¯é”™è¯¯ç‡ï¼ˆWERï¼‰çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚æœ€è¿‘çš„æ¿€æ´»å‡½æ•°å¦‚
    PReLUã€GELUã€Swishã€Mish å’Œ PAU æ¿€æ´»å‡½æ•°åœ¨æœ¬å®éªŒä¸­è¢«å‘ç°æœ€é€‚åˆè¯­éŸ³è¯†åˆ«ã€‚'
- en: 10 Conclusion and Recommendations
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 ç»“è®ºå’Œå»ºè®®
- en: An extensive and up to date survey of activation functions is conducted in this
    paper. Different types of AFs are considered, including Logistic Sigmoid and Tanh
    based, ReLU based, ELU based, and Learning based. However, the main focus is given
    to the recent developments in AFs in view of the deep learning applications of
    neural networks. The overview of AFs presented in this paper focuses on the aspects
    including the detailed coverage of AFs, classification and performance comparison
    over image, text and speech data.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡è¿›è¡Œäº†å¹¿æ³›ä¸”æœ€æ–°çš„æ¿€æ´»å‡½æ•°è°ƒæŸ¥ã€‚è€ƒè™‘äº†ä¸åŒç±»å‹çš„æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬åŸºäº Logistic Sigmoid å’Œ Tanh çš„ã€åŸºäº ReLU çš„ã€åŸºäº ELU
    çš„å’ŒåŸºäºå­¦ä¹ çš„ã€‚ç„¶è€Œï¼Œä¸»è¦å…³æ³¨çš„æ˜¯é’ˆå¯¹æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œåº”ç”¨çš„æ¿€æ´»å‡½æ•°çš„æœ€æ–°å‘å±•ã€‚æœ¬æ–‡å¯¹æ¿€æ´»å‡½æ•°çš„æ¦‚è¿°é‡ç‚¹å…³æ³¨æ¿€æ´»å‡½æ•°çš„è¯¦ç»†è¦†ç›–ã€åˆ†ç±»åŠåœ¨å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³æ•°æ®ä¸Šçš„æ€§èƒ½æ¯”è¾ƒã€‚
- en: 'Following are the concluding remarks of the survey and performance analysis
    conducted through this paper:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯é€šè¿‡æœ¬æ–‡è¿›è¡Œçš„è°ƒæŸ¥å’Œæ€§èƒ½åˆ†æçš„æ€»ç»“ï¼š
- en: '1.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: Most of the improvements in Logistic Sigmoid and Tanh targets to tackle the
    non zero-mean and zero-gradient problems. However, these improvements carry forward
    the drawback of increased complexity.
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Logistic Sigmoid å’Œ Tanh çš„å¤§å¤šæ•°æ”¹è¿›æ—¨åœ¨è§£å†³éé›¶å‡å€¼å’Œé›¶æ¢¯åº¦é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ”¹è¿›å¸¦æ¥äº†å¤æ‚æ€§å¢åŠ çš„ç¼ºé™·ã€‚
- en: '2.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: The ReLU variants try to tackle the three major problems of ReLU, namely under-utilization
    of negative values, limited nonlinearity and unbounded output. These activations
    perform well for some applications, e.g. LReLU and ABReLU works better with residual
    networks. However, most of these activations fail to perform better than ReLU,
    e.g. LReLU, PReLU and ABReLU do not improve for MobileNet, VGG and GoogleNet models.
    Note that, the ReLU, Leaky ReLU and PReLU AFs are the most common choice among
    researchers due to its simplicity. Moreover, many networks consider the ReLU as
    a default choice for the AF.
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ReLU çš„å˜ä½“è¯•å›¾è§£å†³ ReLU çš„ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼Œå³è´Ÿå€¼çš„æœªå……åˆ†åˆ©ç”¨ã€æœ‰é™çš„éçº¿æ€§å’Œæ— ç•Œè¾“å‡ºã€‚è¿™äº›æ¿€æ´»å‡½æ•°åœ¨ä¸€äº›åº”ç”¨ä¸­è¡¨ç°è‰¯å¥½ï¼Œä¾‹å¦‚ LReLU å’Œ
    ABReLU åœ¨æ®‹å·®ç½‘ç»œä¸­æ•ˆæœæ›´ä½³ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¿€æ´»å‡½æ•°æœªèƒ½æ¯” ReLU è¡¨ç°å¾—æ›´å¥½ï¼Œä¾‹å¦‚ LReLUã€PReLU å’Œ ABReLU å¯¹ MobileNetã€VGG
    å’Œ GoogleNet æ¨¡å‹çš„æ”¹è¿›ä¸æ˜æ˜¾ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒReLUã€Leaky ReLU å’Œ PReLU æ¿€æ´»å‡½æ•°å› å…¶ç®€å•æ€§è€Œæˆä¸ºç ”ç©¶è€…çš„æœ€å¸¸ç”¨é€‰æ‹©ã€‚æ­¤å¤–ï¼Œè®¸å¤šç½‘ç»œå°†
    ReLU ä½œä¸ºæ¿€æ´»å‡½æ•°çš„é»˜è®¤é€‰æ‹©ã€‚
- en: '3.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: The exponential based AFs also focus over the better utilization of the negative
    values and to avoid the saturation for important features. However, most of the
    exponential activations suffer due to the non-smooth functions.
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸºäºæŒ‡æ•°çš„æ¿€æ´»å‡½æ•°ä¹Ÿå…³æ³¨æ›´å¥½åœ°åˆ©ç”¨è´Ÿå€¼å¹¶é¿å…å¯¹é‡è¦ç‰¹å¾çš„é¥±å’Œã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æŒ‡æ•°æ¿€æ´»å‡½æ•°ç”±äºå‡½æ•°çš„ä¸å…‰æ»‘æ€§è€Œå—åˆ°å½±å“ã€‚
- en: '4.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: The learning based adaptive AFs try to find the best parameters to represent
    the non-linearity needed for the given dataset. This category of AF has gained
    more popularity in recent years. However, the major problem associated with such
    AF is to find the better base function and number of trainable parameters. Some
    AFs diverge during the training if not initialized properly.
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸºäºå­¦ä¹ çš„è‡ªé€‚åº”æ¿€æ´»å‡½æ•°ï¼ˆAFï¼‰è¯•å›¾æ‰¾åˆ°æœ€ä½³å‚æ•°ä»¥è¡¨ç¤ºç»™å®šæ•°æ®é›†æ‰€éœ€çš„éçº¿æ€§ã€‚è¿™ç±»æ¿€æ´»å‡½æ•°è¿‘å¹´æ¥å˜å¾—è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œè¿™ç±»æ¿€æ´»å‡½æ•°é¢ä¸´çš„ä¸»è¦é—®é¢˜æ˜¯å¦‚ä½•æ‰¾åˆ°æ›´å¥½çš„åŸºå‡½æ•°å’Œå¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚å¦‚æœåˆå§‹åŒ–ä¸å½“ï¼Œä¸€äº›æ¿€æ´»å‡½æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šå‘æ•£ã€‚
- en: '5.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: In contrast to existing surveys, this survey covers an exhaustive list different
    types of AFs. Moreover, a performance analysis on different types of data using
    several AFs provides new insights for future research.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸ç°æœ‰çš„è°ƒæŸ¥ç›¸æ¯”ï¼Œæœ¬è°ƒæŸ¥æ¶µç›–äº†å„ç§ç±»å‹çš„æ¿€æ´»å‡½æ•°çš„è¯¦å°½åˆ—è¡¨ã€‚æ­¤å¤–ï¼Œå¯¹ä¸åŒæ•°æ®ç±»å‹ä½¿ç”¨å¤šç§æ¿€æ´»å‡½æ•°çš„æ€§èƒ½åˆ†æä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„è§è§£ã€‚
- en: 'Following are the recommendations curated from this survey and performance
    analysis:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä»æœ¬è°ƒæŸ¥å’Œæ€§èƒ½åˆ†æä¸­æ•´ç†å‡ºçš„å»ºè®®ï¼š
- en: '1.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: In order to speed up the training, both negative & positive values should be
    used to ensure the near zero mean.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸ºäº†åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œåº”åŒæ—¶ä½¿ç”¨è´Ÿå€¼å’Œæ­£å€¼ä»¥ç¡®ä¿æ¥è¿‘é›¶çš„å‡å€¼ã€‚
- en: '2.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: The most important aspect in deep learning is to find the network having matching
    complexity as the dataset complexity. If the complexity of the model is high then
    it may lead to overfitting and if the complexity of the model is low then it may
    lead to under convergence. Thus, the AF should bridge this gap based on the model
    and dataset complexity during training automatically.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæœ€é‡è¦çš„æ–¹é¢æ˜¯æ‰¾åˆ°ä¸æ•°æ®é›†å¤æ‚æ€§åŒ¹é…çš„ç½‘ç»œã€‚å¦‚æœæ¨¡å‹çš„å¤æ‚æ€§è¿‡é«˜ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼›å¦‚æœæ¨¡å‹çš„å¤æ‚æ€§è¿‡ä½ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¬ æ‹Ÿåˆã€‚å› æ­¤ï¼Œæ¿€æ´»å‡½æ•°åº”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†çš„å¤æ‚æ€§è‡ªåŠ¨å¼¥åˆè¿™ä¸€å·®è·ã€‚
- en: '3.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: The Logistic Sigmoid and Tanh AFs should be avoided for Convolutional Neural
    Networks as it leads to poor convergence. However, this type of AF is commonly
    used as gates in recurrent neural networks.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Logistic Sigmoidå’ŒTanhæ¿€æ´»å‡½æ•°åº”é¿å…ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œï¼Œå› ä¸ºå®ƒä»¬ä¼šå¯¼è‡´è¾ƒå·®çš„æ”¶æ•›æ€§ã€‚ç„¶è€Œï¼Œè¿™ç±»æ¿€æ´»å‡½æ•°åœ¨é€’å½’ç¥ç»ç½‘ç»œä¸­å¸¸è¢«ç”¨ä½œé—¨æ§å•å…ƒã€‚
- en: '4.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: Despite the ReLU being a popular choice, recently proposed AFs such as Swish,
    Mish, and PAU are also worth trying for different problems.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å°½ç®¡ReLUæ˜¯ä¸€ä¸ªæµè¡Œçš„é€‰æ‹©ï¼Œä½†æœ€è¿‘æå‡ºçš„æ¿€æ´»å‡½æ•°å¦‚Swishã€Mishå’ŒPAUåœ¨ä¸åŒé—®é¢˜ä¸Šä¹Ÿå€¼å¾—å°è¯•ã€‚
- en: '5.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: The ReLU, Mish and PDELU activation functions have shown a good performance
    with VGG16 and GoogleNet. The ReLU, LReLU, ELU, GELU, CELU, and PDELU functions
    are better for the networks having residual connections for image classification.
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ReLUã€Mishå’ŒPDELUæ¿€æ´»å‡½æ•°åœ¨VGG16å’ŒGoogleNetä¸Šè¡¨ç°è‰¯å¥½ã€‚ReLUã€LReLUã€ELUã€GELUã€CELUå’ŒPDELUå‡½æ•°æ›´é€‚åˆå…·æœ‰æ®‹å·®è¿æ¥çš„å›¾åƒåˆ†ç±»ç½‘ç»œã€‚
- en: '6.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '6.'
- en: In general, the parametric AFs show better convergence as it can adapt the data
    faster by learning the parameter from the data. Specially, PAU, PReLU and PDELU
    have shown better convergence.
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œå‚æ•°åŒ–æ¿€æ´»å‡½æ•°è¡¨ç°å‡ºæ›´å¥½çš„æ”¶æ•›æ€§ï¼Œå› ä¸ºå®ƒå¯ä»¥é€šè¿‡ä»æ•°æ®ä¸­å­¦ä¹ å‚æ•°æ¥æ›´å¿«åœ°é€‚åº”æ•°æ®ã€‚ç‰¹åˆ«æ˜¯ï¼ŒPAUã€PReLUå’ŒPDELUè¡¨ç°å‡ºäº†æ›´å¥½çš„æ”¶æ•›æ€§ã€‚
- en: '7.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '7.'
- en: Some AFs lead to increased training time complexity. PDELU and SRS are such
    examples. However, AFs such as ReLU, SELU, GELU, and Softplus depict a promising
    tradeoff between the accuracy and training time.
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸€äº›æ¿€æ´»å‡½æ•°ä¼šå¯¼è‡´è®­ç»ƒæ—¶é—´å¤æ‚åº¦çš„å¢åŠ ã€‚PDELUå’ŒSRSå°±æ˜¯è¿™æ ·çš„ä¾‹å­ã€‚ç„¶è€Œï¼ŒReLUã€SELUã€GELUå’ŒSoftplusç­‰æ¿€æ´»å‡½æ•°åœ¨å‡†ç¡®æ€§å’Œè®­ç»ƒæ—¶é—´ä¹‹é—´è¡¨ç°å‡ºä¸€ç§æœ‰å¸Œæœ›çš„æŠ˜è¡·ã€‚
- en: '8.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '8.'
- en: The exponential AFs generally lead to the increased non-linearity due to utilization
    of the negative values.
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æŒ‡æ•°å‹æ¿€æ´»å‡½æ•°é€šå¸¸ä¼šç”±äºè´Ÿå€¼çš„ä½¿ç”¨è€Œå¯¼è‡´éçº¿æ€§çš„å¢åŠ ã€‚
- en: '9.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '9.'
- en: The Tanh and SELU AFs are found better for language translation along with PReLU,
    LiSHT, SRS and PAU.
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Tanhå’ŒSELUæ¿€æ´»å‡½æ•°åœ¨è¯­è¨€ç¿»è¯‘ä¸­è¡¨ç°è¾ƒå¥½ï¼ŒåŒæ—¶PReLUã€LiSHTã€SRSå’ŒPAUä¹Ÿæ•ˆæœä¸é”™ã€‚
- en: '10.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '10.'
- en: It is suggested to use the PReLU, GELU, Swish, Mish and PAU AFs for speech recognition.
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å»ºè®®åœ¨è¯­éŸ³è¯†åˆ«ä¸­ä½¿ç”¨PReLUã€GELUã€Swishã€Mishå’ŒPAUæ¿€æ´»å‡½æ•°ã€‚
- en: References
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] F.Â Shao, L.Â Chen, J.Â Shao, W.Â Ji, S.Â Xiao, L.Â Ye, Y.Â Zhuang, J.Â Xiao, Deep
    learning for weakly-supervised object detection and localization: A survey, Neurocomputing
    (2022).'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] F. Shao, L. Chen, J. Shao, W. Ji, S. Xiao, L. Ye, Y. Zhuang, J. Xiao, æ·±åº¦å­¦ä¹ åœ¨å¼±ç›‘ç£ç›®æ ‡æ£€æµ‹å’Œå®šä½ä¸­çš„åº”ç”¨ï¼šç»¼è¿°,
    Neurocomputing (2022).'
- en: '[2] Y.Â Mo, Y.Â Wu, X.Â Yang, F.Â Liu, Y.Â Liao, Review the state-of-the-art technologies
    of semantic segmentation based on deep learning, Neurocomputing (2022).'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Y. Mo, Y. Wu, X. Yang, F. Liu, Y. Liao, åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­ä¹‰åˆ†å‰²æŠ€æœ¯çš„æœ€æ–°è¿›å±•ç»¼è¿°ï¼Œã€Šç¥ç»è®¡ç®—ã€‹ (2022)ã€‚'
- en: '[3] Y.Â Guo, F.Â Feng, X.Â Hao, X.Â Chen, Jac-net: Joint learning with adaptive
    exploration and concise attention for unsupervised domain adaptive person re-identification,
    Neurocomputing (2022).'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Guo, F. Feng, X. Hao, X. Chen, Jac-netï¼šå…·æœ‰è‡ªé€‚åº”æ¢ç´¢å’Œç®€æ´æ³¨æ„åŠ›çš„è”åˆå­¦ä¹ ç”¨äºæ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”äººé‡æ–°è¯†åˆ«ï¼Œã€Šç¥ç»è®¡ç®—ã€‹
    (2022)ã€‚'
- en: '[4] S.Â R. Dubey, A decade survey of content based image retrieval using deep
    learning, IEEE Transactions on Circuits and Systems for Video Technology (2021).'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. R. Dubey, åŸºäºæ·±åº¦å­¦ä¹ çš„å†…å®¹æ£€ç´¢åå¹´ç»¼è¿°ï¼Œã€ŠIEEE è§†é¢‘æŠ€æœ¯ç”µè·¯ä¸ç³»ç»ŸæœŸåˆŠã€‹ (2021)ã€‚'
- en: '[5] X.Â Xia, X.Â Pan, N.Â Li, X.Â He, L.Â Ma, X.Â Zhang, N.Â Ding, Gan-based anomaly
    detection: A review, Neurocomputing (2022).'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X. Xia, X. Pan, N. Li, X. He, L. Ma, X. Zhang, N. Ding, åŸºäº GAN çš„å¼‚å¸¸æ£€æµ‹ï¼šç»¼è¿°ï¼Œã€Šç¥ç»è®¡ç®—ã€‹
    (2022)ã€‚'
- en: '[6] H.Â Li, Y.Â Pan, J.Â Zhao, L.Â Zhang, Skin disease diagnosis with deep learning:
    a review, Neurocomputing 464 (2021) 364â€“393.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] H. Li, Y. Pan, J. Zhao, L. Zhang, åŸºäºæ·±åº¦å­¦ä¹ çš„çš®è‚¤ç–¾ç—…è¯Šæ–­ï¼šç»¼è¿°ï¼Œã€Šç¥ç»è®¡ç®—ã€‹464 (2021) 364â€“393ã€‚'
- en: '[7] C.Â H. Dagli, Artificial neural networks for intelligent manufacturing,
    Springer Science & Business Media, 2012.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. H. Dagli, æ™ºèƒ½åˆ¶é€ çš„äººå·¥ç¥ç»ç½‘ç»œï¼ŒSpringer Science & Business Mediaï¼Œ2012ã€‚'
- en: '[8] A.Â Krizhevsky, I.Â Sutskever, G.Â E. Hinton, Imagenet classification with
    deep convolutional neural networks, in: Advances in Neural Information Processing
    Systems, 2012, pp. 1097â€“1105.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Krizhevsky, I. Sutskever, G. E. Hinton, ä½¿ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œ Imagenet åˆ†ç±»ï¼Œæ”¶å½•äºï¼šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ2012ï¼Œpp.
    1097â€“1105ã€‚'
- en: '[9] A.Â Graves, A.-r. Mohamed, G.Â Hinton, Speech recognition with deep recurrent
    neural networks, in: IEEE International Conference on Acoustics, Speech and Signal
    Processing, 2013, pp. 6645â€“6649.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Graves, A.-r. Mohamed, G. Hinton, ä½¿ç”¨æ·±åº¦é€’å½’ç¥ç»ç½‘ç»œè¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œæ”¶å½•äºï¼šIEEE å›½é™…å£°å­¦ã€è¯­éŸ³ä¸ä¿¡å·å¤„ç†ä¼šè®®ï¼Œ2013ï¼Œpp.
    6645â€“6649ã€‚'
- en: '[10] K.Â K. Babu, S.Â R. Dubey, Pcsgan: Perceptual cyclic-synthesized generative
    adversarial networks for thermal and nir to visible image transformation, Neurocomputing
    (2020).'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] K. K. Babu, S. R. Dubey, PCSGANï¼šç”¨äºçƒ­æˆåƒå’Œè¿‘çº¢å¤–åˆ°å¯è§å…‰å›¾åƒè½¬æ¢çš„æ„ŸçŸ¥å¾ªç¯åˆæˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œã€Šç¥ç»è®¡ç®—ã€‹
    (2020)ã€‚'
- en: '[11] J.Â Liu, Y.Â Liu, Q.Â Zhang, A weight initialization method based on neural
    network with asymmetric activation function, Neurocomputing (2022).'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Liu, Y. Liu, Q. Zhang, åŸºäºéå¯¹ç§°æ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œæƒé‡åˆå§‹åŒ–æ–¹æ³•ï¼Œã€Šç¥ç»è®¡ç®—ã€‹ (2022)ã€‚'
- en: '[12] Y.Â Srivastava, V.Â Murali, S.Â R. Dubey, A performance evaluation of loss
    functions for deep face recognition, in: National Conference on Computer Vision,
    Pattern Recognition, Image Processing, and Graphics, Springer, 2019, pp. 322â€“332.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Y. Srivastava, V. Murali, S. R. Dubey, å¯¹æ·±åº¦äººè„¸è¯†åˆ«æŸå¤±å‡½æ•°çš„æ€§èƒ½è¯„ä¼°ï¼Œæ”¶å½•äºï¼šè®¡ç®—æœºè§†è§‰ã€æ¨¡å¼è¯†åˆ«ã€å›¾åƒå¤„ç†ä¸å›¾å½¢å­¦å…¨å›½ä¼šè®®ï¼ŒSpringerï¼Œ2019ï¼Œpp.
    322â€“332ã€‚'
- en: '[13] S.Â S. Basha, S.Â R. Dubey, V.Â Pulabaigari, S.Â Mukherjee, Impact of fully
    connected layers on performance of convolutional neural networks for image classification,
    Neurocomputing 378 (2020) 112â€“119.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. S. Basha, S. R. Dubey, V. Pulabaigari, S. Mukherjee, å…¨è¿æ¥å±‚å¯¹å·ç§¯ç¥ç»ç½‘ç»œå›¾åƒåˆ†ç±»æ€§èƒ½çš„å½±å“ï¼Œã€Šç¥ç»è®¡ç®—ã€‹378
    (2020) 112â€“119ã€‚'
- en: '[14] Q.Â Xu, M.Â Zhang, Z.Â Gu, G.Â Pan, Overfitting remedy by sparsifying regularization
    on fully-connected layers of cnns, Neurocomputing 328 (2019) 69â€“74.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Q. Xu, M. Zhang, Z. Gu, G. Pan, é€šè¿‡ç¨€ç–æ­£åˆ™åŒ–ç¼“è§£è¿‡æ‹Ÿåˆåœ¨ CNNs çš„å…¨è¿æ¥å±‚ä¸­çš„åº”ç”¨ï¼Œã€Šç¥ç»è®¡ç®—ã€‹328
    (2019) 69â€“74ã€‚'
- en: '[15] S.Â R. Dubey, S.Â Chakraborty, S.Â K. Roy, S.Â Mukherjee, S.Â K. Singh, B.Â B.
    Chaudhuri, diffgrad: An optimization method for convolutional neural networks,
    IEEE transactions on neural networks and learning systems 31Â (11) (2019) 4500â€“4511.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. R. Dubey, S. Chakraborty, S. K. Roy, S. Mukherjee, S. K. Singh, B.
    B. Chaudhuri, Diffgradï¼šä¸€ç§ä¼˜åŒ–å·ç§¯ç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œã€ŠIEEE ç¥ç»ç½‘ç»œä¸å­¦ä¹ ç³»ç»ŸæœŸåˆŠã€‹31 (11) (2019) 4500â€“4511ã€‚'
- en: '[16] W.Â Duch, N.Â Jankowski, Survey of neural transfer functions, Neural Computing
    Surveys 2Â (1) (1999) 163â€“212.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] W. Duch, N. Jankowski, ç¥ç»ç½‘ç»œä¼ é€’å‡½æ•°çš„è°ƒæŸ¥ï¼Œã€Šç¥ç»è®¡ç®—è°ƒæŸ¥ã€‹2 (1) (1999) 163â€“212ã€‚'
- en: '[17] V.Â Nair, G.Â E. Hinton, Rectified linear units improve restricted boltzmann
    machines, in: International Conference on Machine Learning, 2010, pp. 807â€“814.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] V. Nair, G. E. Hinton, çº æ­£çº¿æ€§å•å…ƒæ”¹å–„é™åˆ¶ç»å°”å…¹æ›¼æœºï¼Œæ”¶å½•äºï¼šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼Œ2010ï¼Œpp. 807â€“814ã€‚'
- en: '[18] Y.Â LeCun, L.Â Bottou, Y.Â Bengio, P.Â Haffner, Gradient-based learning applied
    to document recognition, Proceedings of the IEEE 86Â (11) (1998) 2278â€“2324.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, åŸºäºæ¢¯åº¦çš„å­¦ä¹ åº”ç”¨äºæ–‡æ¡£è¯†åˆ«ï¼Œã€ŠIEEE æœŸåˆŠã€‹86
    (11) (1998) 2278â€“2324ã€‚'
- en: '[19] A.Â N.Â S. Njikam, H.Â Zhao, A novel activation function for multilayer feed-forward
    neural networks, Applied Intelligence 45Â (1) (2016) 75â€“82.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. N. S. Njikam, H. Zhao, ä¸€ç§ç”¨äºå¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œçš„æ–°å‹æ¿€æ´»å‡½æ•°ï¼Œã€Šåº”ç”¨æ™ºèƒ½ã€‹45 (1) (2016) 75â€“82ã€‚'
- en: '[20] B.Â Xu, R.Â Huang, M.Â Li, Revise saturated activation functions, International
    Conference on Learning Representations Workshop (2016).'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] B. Xu, R. Huang, M. Li, ä¿®è®¢é¥±å’Œæ¿€æ´»å‡½æ•°ï¼Œå›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ç ”è®¨ä¼š (2016)ã€‚'
- en: '[21] S.Â Kong, M.Â Takatsuka, Hexpo: A vanishing-proof activation function, in:
    International Joint Conference on Neural Networks, 2017, pp. 2562â€“2567.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Kong, M. Takatsuka, Hexpoï¼šä¸€ç§é˜²æ­¢æ¶ˆå¤±çš„æ¿€æ´»å‡½æ•°ï¼Œè§ï¼šå›½é™…è”åˆç¥ç»ç½‘ç»œä¼šè®®ï¼Œ2017å¹´ï¼Œç¬¬ 2562â€“2567
    é¡µã€‚'
- en: '[22] Y.Â Qin, X.Â Wang, J.Â Zou, The optimized deep belief networks with improved
    logistic sigmoid units and their application in fault diagnosis for planetary
    gearboxes of wind turbines, IEEE Transactions on Industrial Electronics 66Â (5)
    (2018) 3814â€“3824.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. Qin, X. Wang, J. Zou, ä¼˜åŒ–çš„æ·±åº¦ä¿¡å¿µç½‘ç»œä¸æ”¹è¿›çš„é€»è¾‘ sigmoid å•å…ƒåŠå…¶åœ¨é£åŠ›æ¶¡è½®æœºè¡Œæ˜Ÿé½¿è½®ç®±æ•…éšœè¯Šæ–­ä¸­çš„åº”ç”¨ï¼ŒIEEE
    å·¥ä¸šç”µå­å­¦æŠ¥ 66 (5) (2018) 3814â€“3824ã€‚'
- en: '[23] S.Â Elfwing, E.Â Uchibe, K.Â Doya, Sigmoid-weighted linear units for neural
    network function approximation in reinforcement learning, Neural Networks 107
    (2018) 3â€“11.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Elfwing, E. Uchibe, K. Doya, ç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„ç¥ç»ç½‘ç»œå‡½æ•°é€¼è¿‘çš„ sigmoid åŠ æƒçº¿æ€§å•å…ƒï¼ŒNeural
    Networks 107 (2018) 3â€“11ã€‚'
- en: '[24] S.Â K. Roy, S.Â Manna, S.Â R. Dubey, B.Â B. Chaudhuri, Lisht: Non-parametric
    linearly scaled hyperbolic tangent activation function for neural networks, arXiv
    preprint arXiv:1901.05894 (2019).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. K. Roy, S. Manna, S. R. Dubey, B. B. Chaudhuri, Lisht: ç”¨äºç¥ç»ç½‘ç»œçš„éå‚æ•°çº¿æ€§ç¼©æ”¾åŒæ›²æ­£åˆ‡æ¿€æ´»å‡½æ•°ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:1901.05894 (2019)ã€‚'
- en: '[25] A.Â Farzad, H.Â Mashayekhi, H.Â Hassanpour, A comparative performance analysis
    of different activation functions in lstm networks for classification, Neural
    Computing and Applications 31Â (7) (2019) 2507â€“2521.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Farzad, H. Mashayekhi, H. Hassanpour, åœ¨ LSTM ç½‘ç»œä¸­å¯¹ä¸åŒæ¿€æ´»å‡½æ•°è¿›è¡Œçš„åˆ†ç±»æ€§èƒ½æ¯”è¾ƒåˆ†æï¼ŒNeural
    Computing and Applications 31 (7) (2019) 2507â€“2521ã€‚'
- en: '[26] Y.Â Zhou, D.Â Li, S.Â Huo, S.-Y. Kung, Soft-root-sign activation function,
    arXiv preprint arXiv:2003.00547 (2020).'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. Zhou, D. Li, S. Huo, S.-Y. Kung, Soft-root-sign æ¿€æ´»å‡½æ•°ï¼ŒarXiv é¢„å°æœ¬ arXiv:2003.00547
    (2020)ã€‚'
- en: '[27] D.-A. Clevert, T.Â Unterthiner, S.Â Hochreiter, Fast and accurate deep network
    learning by exponential linear units (elus), in: International Conference on Learning
    Representations, 2016.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] D.-A. Clevert, T. Unterthiner, S. Hochreiter, é€šè¿‡æŒ‡æ•°çº¿æ€§å•å…ƒï¼ˆELUsï¼‰å®ç°å¿«é€Ÿè€Œå‡†ç¡®çš„æ·±åº¦ç½‘ç»œå­¦ä¹ ï¼Œè§ï¼šå›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ï¼Œ2016å¹´ã€‚'
- en: '[28] F.Â Agostinelli, M.Â Hoffman, P.Â Sadowski, P.Â Baldi, Learning activation
    functions to improve deep neural networks, International Conference on Learning
    Representations Workshops (2015).'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] F. Agostinelli, M. Hoffman, P. Sadowski, P. Baldi, å­¦ä¹ æ¿€æ´»å‡½æ•°ä»¥æ”¹å–„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ç ”è®¨ä¼š
    (2015)ã€‚'
- en: '[29] P.Â Ramachandran, B.Â Zoph, Q.Â V. Le, Searching for activation functions,
    International Conference on Learning Representations Workshops (2018).'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] P. Ramachandran, B. Zoph, Q. V. Le, å¯»æ‰¾æ¿€æ´»å‡½æ•°ï¼Œå›½é™…å­¦ä¹ è¡¨å¾ä¼šè®®ç ”è®¨ä¼š (2018)ã€‚'
- en: '[30] Y.Â LeCun, Y.Â Bengio, G.Â Hinton, Deep learning, nature 521Â (7553) (2015)
    436â€“444.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. LeCun, Y. Bengio, G. Hinton, æ·±åº¦å­¦ä¹ ï¼ŒNature 521 (7553) (2015) 436â€“444ã€‚'
- en: '[31] P.Â Chandra, Y.Â Singh, An activation function adapting training algorithm
    for sigmoidal feedforward networks, Neurocomputing 61 (2004) 429â€“437.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] P. Chandra, Y. Singh, ä¸€ç§é€‚åº”æ€§è®­ç»ƒç®—æ³•çš„æ¿€æ´»å‡½æ•°ç”¨äº sigmoid å‰é¦ˆç½‘ç»œï¼ŒNeurocomputing 61
    (2004) 429â€“437ã€‚'
- en: '[32] S.Â S. Sodhi, P.Â Chandra, Bi-modal derivative activation function for sigmoidal
    feedforward networks, Neurocomputing 143 (2014) 182â€“196.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. S. Sodhi, P. Chandra, ç”¨äº sigmoid å‰é¦ˆç½‘ç»œçš„åŒæ¨¡æ€å¯¼æ•°æ¿€æ´»å‡½æ•°ï¼ŒNeurocomputing 143
    (2014) 182â€“196ã€‚'
- en: '[33] S.Â Eger, P.Â Youssef, I.Â Gurevych, Is it time to swish? comparing deep
    learning activation functions across nlp tasks, arXiv preprint arXiv:1901.02671
    (2019).'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Eger, P. Youssef, I. Gurevych, æ˜¯å¦è¯¥è¿›è¡Œæ¿€æ´»å‡½æ•°çš„åˆ‡æ¢ï¼Ÿæ¯”è¾ƒä¸åŒ NLP ä»»åŠ¡ä¸­çš„æ·±åº¦å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:1901.02671 (2019)ã€‚'
- en: '[34] A.Â L. Maas, A.Â Y. Hannun, A.Â Y. Ng, Rectifier nonlinearities improve neural
    network acoustic models, in: International Conference on Machine Learning, Vol.Â 30,
    2013, p.Â 3.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. L. Maas, A. Y. Hannun, A. Y. Ng, æ•´æµéçº¿æ€§å‡½æ•°æ”¹è¿›ç¥ç»ç½‘ç»œå£°å­¦æ¨¡å‹ï¼Œè§ï¼šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼Œç¬¬ 30
    å·ï¼Œ2013å¹´ï¼Œç¬¬ 3 é¡µã€‚'
- en: '[35] K.Â He, X.Â Zhang, S.Â Ren, J.Â Sun, Delving deep into rectifiers: Surpassing
    human-level performance on imagenet classification, in: IEEE international conference
    on computer vision, 2015, pp. 1026â€“1034.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] K. He, X. Zhang, S. Ren, J. Sun, æ·±å…¥ç ”ç©¶æ•´æµå™¨ï¼šåœ¨ ImageNet åˆ†ç±»ä¸Šè¶…è¶Šäººç±»æ°´å¹³ï¼Œè§ï¼šIEEE å›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®ï¼Œ2015å¹´ï¼Œç¬¬
    1026â€“1034 é¡µã€‚'
- en: '[36] W.Â Shang, K.Â Sohn, D.Â Almeida, H.Â Lee, Understanding and improving convolutional
    neural networks via concatenated rectified linear units, in: International Conference
    on Machine Learning, 2016, pp. 2217â€“2225.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] W. Shang, K. Sohn, D. Almeida, H. Lee, é€šè¿‡ä¸²è”çš„æ•´æµçº¿æ€§å•å…ƒç†è§£å’Œæ”¹è¿›å·ç§¯ç¥ç»ç½‘ç»œï¼Œè§ï¼šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼Œ2016å¹´ï¼Œç¬¬
    2217â€“2225 é¡µã€‚'
- en: '[37] S.Â S. Liew, M.Â Khalil-Hani, R.Â Bakhteri, Bounded activation functions
    for enhanced training stability of deep neural networks on visual pattern recognition
    problems, Neurocomputing 216 (2016) 718â€“734.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. S. Liew, M. Khalil-Hani, R. Bakhteri, ç”¨äºè§†è§‰æ¨¡å¼è¯†åˆ«é—®é¢˜çš„æ·±åº¦ç¥ç»ç½‘ç»œå¢å¼ºè®­ç»ƒç¨³å®šæ€§çš„æœ‰ç•Œæ¿€æ´»å‡½æ•°,
    ã€Šç¥ç»è®¡ç®—ã€‹216 (2016) 718â€“734ã€‚'
- en: '[38] R.Â Duggal, A.Â Gupta, P-telu: Parametric tan hyperbolic linear unit activation
    for deep neural networks, in: IEEE International Conference on Computer Vision
    Workshops, 2017, pp. 974â€“978.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] R. Duggal, A. Gupta, P-telu: æ·±åº¦ç¥ç»ç½‘ç»œçš„å‚æ•°åŒ–åŒæ›²æ­£åˆ‡çº¿æ€§å•å…ƒæ¿€æ´»å‡½æ•°, è§: IEEEå›½é™…è®¡ç®—æœºè§†è§‰ç ”è®¨ä¼š,
    2017, ç¬¬974â€“978é¡µã€‚'
- en: '[39] S.Â Qiu, X.Â Xu, B.Â Cai, Frelu: Flexible rectified linear units for improving
    convolutional neural networks, in: International Conference on Pattern Recognition,
    2018, pp. 1223â€“1228.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Qiu, X. Xu, B. Cai, Frelu: çµæ´»æ•´æµçº¿æ€§å•å…ƒç”¨äºæ”¹è¿›å·ç§¯ç¥ç»ç½‘ç»œ, è§: å›½é™…æ¨¡å¼è¯†åˆ«ä¼šè®®, 2018, ç¬¬1223â€“1228é¡µã€‚'
- en: '[40] X.Â Jiang, Y.Â Pang, X.Â Li, J.Â Pan, Y.Â Xie, Deep neural networks with elastic
    rectified linear units for object recognition, Neurocomputing 275 (2018) 1132â€“1139.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] X. Jiang, Y. Pang, X. Li, J. Pan, Y. Xie, ä½¿ç”¨å¼¹æ€§æ•´æµçº¿æ€§å•å…ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œç”¨äºç‰©ä½“è¯†åˆ«, ã€Šç¥ç»è®¡ç®—ã€‹275
    (2018) 1132â€“1139ã€‚'
- en: '[41] J.Â Cao, Y.Â Pang, X.Â Li, J.Â Liang, Randomly translational activation inspired
    by the input distributions of relu, Neurocomputing 275 (2018) 859â€“868.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Cao, Y. Pang, X. Li, J. Liang, å—reluè¾“å…¥åˆ†å¸ƒå¯å‘çš„éšæœºå¹³ç§»æ¿€æ´»å‡½æ•°, ã€Šç¥ç»è®¡ç®—ã€‹275 (2018)
    859â€“868ã€‚'
- en: '[42] F.Â Godin, J.Â Degrave, J.Â Dambre, W.Â DeÂ Neve, Dual rectified linear units
    (drelus): A replacement for tanh activation functions in quasi-recurrent neural
    networks, Pattern Recognition Letters 116 (2018) 8â€“14.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] F. Godin, J. Degrave, J. Dambre, W. De Neve, åŒæ•´æµçº¿æ€§å•å…ƒï¼ˆdrelusï¼‰ï¼šæ›¿ä»£quasi-recurrentç¥ç»ç½‘ç»œä¸­çš„tanhæ¿€æ´»å‡½æ•°,
    ã€Šæ¨¡å¼è¯†åˆ«é€šè®¯ã€‹116 (2018) 8â€“14ã€‚'
- en: '[43] Z.Â Tang, L.Â Luo, H.Â Peng, S.Â Li, A joint residual network with paired
    relus activation for image super-resolution, Neurocomputing 273 (2018) 37â€“46.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Z. Tang, L. Luo, H. Peng, S. Li, ä¸€ç§ç»“åˆpaired relusæ¿€æ´»çš„è”åˆæ®‹å·®ç½‘ç»œç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡, ã€Šç¥ç»è®¡ç®—ã€‹273
    (2018) 37â€“46ã€‚'
- en: '[44] S.Â R. Dubey, S.Â Chakraborty, Average biased relu based cnn descriptor
    for improved face retrieval, arXiv preprint arXiv:1804.02051 (2018).'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. R. Dubey, S. Chakraborty, åŸºäºå¹³å‡åç½®reluçš„cnnæè¿°ç¬¦ç”¨äºæ”¹è¿›äººè„¸æ£€ç´¢, arXivé¢„å°æœ¬ arXiv:1804.02051
    (2018)ã€‚'
- en: '[45] Y.Â Liu, J.Â Zhang, C.Â Gao, J.Â Qu, L.Â Ji, Natural-logarithm-rectified activation
    function in convolutional neural networks, in: International Conference on Computer
    and Communications, 2019, pp. 2000â€“2008.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Y. Liu, J. Zhang, C. Gao, J. Qu, L. Ji, å·ç§¯ç¥ç»ç½‘ç»œä¸­çš„è‡ªç„¶å¯¹æ•°æ•´æµæ¿€æ´»å‡½æ•°, è§: å›½é™…è®¡ç®—æœºä¸é€šä¿¡ä¼šè®®,
    2019, ç¬¬2000â€“2008é¡µã€‚'
- en: '[46] S.Â Gu, W.Â Li, L.Â V. Gool, R.Â Timofte, Fast image restoration with multi-bin
    trainable linear units, in: IEEE International Conference on Computer Vision,
    2019, pp. 4190â€“4199.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] S. Gu, W. Li, L. V. Gool, R. Timofte, åŸºäºå¤šç®±å¯è®­ç»ƒçº¿æ€§å•å…ƒçš„å¿«é€Ÿå›¾åƒæ¢å¤, è§: IEEEå›½é™…è®¡ç®—æœºè§†è§‰ä¼šè®®,
    2019, ç¬¬4190â€“4199é¡µã€‚'
- en: '[47] M.Â Basirat, P.Â Roth, L* relu: Piece-wise linear activation functions for
    deep fine-grained visual categorization, in: IEEE Winter Conference on Applications
    of Computer Vision, 2020, pp. 1218â€“1227.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. Basirat, P. Roth, L* relu: ç”¨äºæ·±åº¦ç²¾ç»†åˆ†ç±»çš„åˆ†æ®µçº¿æ€§æ¿€æ´»å‡½æ•°, è§: IEEEå†¬å­£è®¡ç®—æœºè§†è§‰åº”ç”¨ä¼šè®®, 2020,
    ç¬¬1218â€“1227é¡µã€‚'
- en: '[48] C.Â Gulcehre, M.Â Moczulski, M.Â Denil, Y.Â Bengio, Noisy activation functions,
    in: International Conference on Machine Learning, 2016, pp. 3059â€“3068.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] C. Gulcehre, M. Moczulski, M. Denil, Y. Bengio, å™ªå£°æ¿€æ´»å‡½æ•°, è§: å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®, 2016,
    ç¬¬3059â€“3068é¡µã€‚'
- en: '[49] I.Â J. Goodfellow, D.Â Warde-Farley, M.Â Mirza, A.Â Courville, Y.Â Bengio,
    Maxout networks, arXiv preprint arXiv:1302.4389 (2013).'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, Y. Bengio,
    Maxoutç½‘ç»œ, arXivé¢„å°æœ¬ arXiv:1302.4389 (2013)ã€‚'
- en: '[50] B.Â Xu, N.Â Wang, T.Â Chen, M.Â Li, Empirical evaluation of rectified activations
    in convolutional network, arXiv preprint arXiv:1505.00853 (2015).'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] B. Xu, N. Wang, T. Chen, M. Li, å·ç§¯ç½‘ç»œä¸­æ•´æµæ¿€æ´»å‡½æ•°çš„ç»éªŒè¯„ä¼°, arXivé¢„å°æœ¬ arXiv:1505.00853
    (2015)ã€‚'
- en: '[51] H.Â Li, W.Â Ouyang, X.Â Wang, Multi-bias non-linear activation in deep neural
    networks, in: International Conference on Machine Learning, 2016, pp. 221â€“229.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] H. Li, W. Ouyang, X. Wang, æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„å¤šåç½®éçº¿æ€§æ¿€æ´», è§: å›½é™…æœºå™¨å­¦ä¹ ä¼šè®®, 2016, ç¬¬221â€“229é¡µã€‚'
- en: '[52] G.Â Klambauer, T.Â Unterthiner, A.Â Mayr, S.Â Hochreiter, Self-normalizing
    neural networks, in: Advances in Neural Information Processing Systems, 2017,
    pp. 971â€“980.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] G. Klambauer, T. Unterthiner, A. Mayr, S. Hochreiter, è‡ªå½’ä¸€åŒ–ç¥ç»ç½‘ç»œ, è§: ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹,
    2017, ç¬¬971â€“980é¡µã€‚'
- en: '[53] J.Â T. Barron, Continuously differentiable exponential linear units, arXiv
    (2017) arXivâ€“1704.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. T. Barron, è¿ç»­å¯å¾®åˆ†çš„æŒ‡æ•°çº¿æ€§å•å…ƒ, arXiv (2017) arXivâ€“1704ã€‚'
- en: '[54] L.Â Trottier, P.Â Gigu, B.Â Chaib-draa, etÂ al., Parametric exponential linear
    unit for deep convolutional neural networks, in: IEEE International Conference
    on Machine Learning and Applications, 2017, pp. 207â€“214.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] L. Trottier, P. Gigu, B. Chaib-draa, ç­‰ï¼Œæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„å‚æ•°åŒ–æŒ‡æ•°çº¿æ€§å•å…ƒï¼Œè½½äºï¼šIEEE æœºå™¨å­¦ä¹ ä¸åº”ç”¨å›½é™…ä¼šè®®ï¼Œ2017å¹´ï¼Œé¡µç 207â€“214ã€‚'
- en: '[55] Y.Â Li, C.Â Fan, Y.Â Li, Q.Â Wu, Y.Â Ming, Improving deep neural network with
    multiple parametric exponential linear units, Neurocomputing 301 (2018) 11â€“24.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Li, C. Fan, Y. Li, Q. Wu, Y. Ming, ä½¿ç”¨å¤šä¸ªå‚æ•°åŒ–æŒ‡æ•°çº¿æ€§å•å…ƒæ”¹è¿›æ·±åº¦ç¥ç»ç½‘ç»œï¼Œã€Šç¥ç»è®¡ç®—ã€‹301
    (2018) 11â€“24ã€‚'
- en: '[56] Z.Â Qiumei, T.Â Dan, W.Â Fenghua, Improved convolutional neural network based
    on fast exponentially linear unit activation function, IEEE Access 7 (2019) 151359â€“151367.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Z. Qiumei, T. Dan, W. Fenghua, åŸºäºå¿«é€ŸæŒ‡æ•°çº¿æ€§å•å…ƒæ¿€æ´»å‡½æ•°çš„æ”¹è¿›å·ç§¯ç¥ç»ç½‘ç»œï¼Œã€ŠIEEE è®¿é—®ã€‹7 (2019)
    151359â€“151367ã€‚'
- en: '[57] Y.Â Ying, J.Â Su, P.Â Shan, L.Â Miao, X.Â Wang, S.Â Peng, Rectified exponential
    units for convolutional neural networks, IEEE Access 7 (2019) 101633â€“101640.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Ying, J. Su, P. Shan, L. Miao, X. Wang, S. Peng, ä¿®æ­£æŒ‡æ•°å•å…ƒç”¨äºå·ç§¯ç¥ç»ç½‘ç»œï¼Œã€ŠIEEE
    è®¿é—®ã€‹7 (2019) 101633â€“101640ã€‚'
- en: '[58] D.Â Kim, J.Â Kim, J.Â Kim, Elastic exponential linear units for convolutional
    neural networks, Neurocomputing 406 (2020) 253â€“266.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] D. Kim, J. Kim, J. Kim, å·ç§¯ç¥ç»ç½‘ç»œçš„å¼¹æ€§æŒ‡æ•°çº¿æ€§å•å…ƒï¼Œã€Šç¥ç»è®¡ç®—ã€‹406 (2020) 253â€“266ã€‚'
- en: '[59] Q.Â Cheng, H.Â Li, Q.Â Wu, L.Â Ma, N.Â N. King, Parametric deformable exponential
    linear units for deep neural networks, Neural Networks 125 (2020) 281â€“289.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Q. Cheng, H. Li, Q. Wu, L. Ma, N. N. King, æ·±åº¦ç¥ç»ç½‘ç»œçš„å‚æ•°åŒ–å¯å˜å½¢æŒ‡æ•°çº¿æ€§å•å…ƒï¼Œã€Šç¥ç»ç½‘ç»œã€‹125
    (2020) 281â€“289ã€‚'
- en: '[60] J.Â Si, S.Â L. Harris, E.Â Yfantis, A dynamic relu on neural network, in:
    IEEE Dallas Circuits and Systems Conference, 2018, pp. 1â€“6.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. Si, S. L. Harris, E. Yfantis, ç¥ç»ç½‘ç»œä¸­çš„åŠ¨æ€ ReLUï¼Œè½½äºï¼šIEEE è¾¾æ‹‰æ–¯ç”µè·¯ä¸ç³»ç»Ÿä¼šè®®ï¼Œ2018å¹´ï¼Œé¡µç 1â€“6ã€‚'
- en: '[61] H.Â Hu, Vrelu activation functions for artificial neural networks, in:
    International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery,
    2018, pp. 856â€“860.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] H. Hu, Vrelu æ¿€æ´»å‡½æ•°ç”¨äºäººå·¥ç¥ç»ç½‘ç»œï¼Œè½½äºï¼šå›½é™…è‡ªç„¶è®¡ç®—ã€æ¨¡ç³Šç³»ç»Ÿä¸çŸ¥è¯†å‘ç°ä¼šè®®ï¼Œ2018å¹´ï¼Œé¡µç 856â€“860ã€‚'
- en: '[62] G.Â Lin, W.Â Shen, Research on convolutional neural network based on improved
    relu piecewise activation function, Procedia Computer Science 131 (2018) 977â€“984.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] G. Lin, W. Shen, åŸºäºæ”¹è¿›çš„ ReLU åˆ†æ®µæ¿€æ´»å‡½æ•°çš„å·ç§¯ç¥ç»ç½‘ç»œç ”ç©¶ï¼Œã€Šè®¡ç®—æœºç§‘å­¦å­¦æŠ¥ã€‹131 (2018) 977â€“984ã€‚'
- en: '[63] D.Â MacÃªdo, C.Â Zanchettin, A.Â L. Oliveira, T.Â Ludermir, Enhancing batch
    normalized convolutional networks using displaced rectifier linear units: A systematic
    comparative study, Expert Systems with Applications 124 (2019) 271â€“281.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] D. MacÃªdo, C. Zanchettin, A. L. Oliveira, T. Ludermir, ä½¿ç”¨ä½ç§»ä¿®æ­£çº¿æ€§å•å…ƒæå‡æ‰¹é‡å½’ä¸€åŒ–å·ç§¯ç½‘ç»œï¼šç³»ç»Ÿæ€§æ¯”è¾ƒç ”ç©¶ï¼Œã€Šä¸“å®¶ç³»ç»Ÿä¸åº”ç”¨ã€‹124
    (2019) 271â€“281ã€‚'
- en: '[64] L.Â B. Godfrey, An evaluation of parametric activation functions for deep
    learning, in: IEEE International Conference on Systems, Man and Cybernetics, 2019,
    pp. 3006â€“3011.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] L. B. Godfrey, å¯¹æ·±åº¦å­¦ä¹ çš„å‚æ•°åŒ–æ¿€æ´»å‡½æ•°çš„è¯„ä¼°ï¼Œè½½äºï¼šIEEE ç³»ç»Ÿã€äººä¸æ§åˆ¶å›½é™…ä¼šè®®ï¼Œ2019å¹´ï¼Œé¡µç 3006â€“3011ã€‚'
- en: '[65] X.Â Jin, C.Â Xu, J.Â Feng, Y.Â Wei, J.Â Xiong, S.Â Yan, Deep learning with s-shaped
    rectified linear activation units, in: AAAI Conference on Artificial Intelligence,
    2016.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] X. Jin, C. Xu, J. Feng, Y. Wei, J. Xiong, S. Yan, ä½¿ç”¨ S å½¢ä¿®æ­£çº¿æ€§æ¿€æ´»å•å…ƒçš„æ·±åº¦å­¦ä¹ ï¼Œè½½äºï¼šAAAI
    äººå·¥æ™ºèƒ½ä¼šè®®ï¼Œ2016å¹´ã€‚'
- en: '[66] V.Â S. Bawa, V.Â Kumar, Linearized sigmoidal activation: A novel activation
    function with tractable non-linear characteristics to boost representation capability,
    Expert Systems with Applications 120 (2019) 346â€“356.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] V. S. Bawa, V. Kumar, çº¿æ€§åŒ– S å‹æ¿€æ´»ï¼šä¸€ç§å…·æœ‰å¯å¤„ç†éçº¿æ€§ç‰¹æ€§çš„åˆ›æ–°æ¿€æ´»å‡½æ•°ï¼Œæå‡è¡¨ç¤ºèƒ½åŠ›ï¼Œã€Šä¸“å®¶ç³»ç»Ÿä¸åº”ç”¨ã€‹120
    (2019) 346â€“356ã€‚'
- en: '[67] X.Â Wang, Y.Â Qin, Y.Â Wang, S.Â Xiang, H.Â Chen, Reltanh: An activation function
    with vanishing gradient resistance for sae-based dnns and its application to rotating
    machinery fault diagnosis, Neurocomputing 363 (2019) 88â€“98.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] X. Wang, Y. Qin, Y. Wang, S. Xiang, H. Chen, Reltanhï¼šä¸€ç§å¯¹æŠ—æ¢¯åº¦æ¶ˆå¤±çš„æ¿€æ´»å‡½æ•°ï¼Œç”¨äºåŸºäº
    SAE çš„ DNN åŠå…¶åœ¨æ—‹è½¬æœºæ¢°æ•…éšœè¯Šæ–­ä¸­çš„åº”ç”¨ï¼Œã€Šç¥ç»è®¡ç®—ã€‹363 (2019) 88â€“98ã€‚'
- en: '[68] X.Â Hu, P.Â Niu, J.Â Wang, X.Â Zhang, A dynamic rectified linear activation
    units, IEEE Access 7 (2019) 180409â€“180416.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] X. Hu, P. Niu, J. Wang, X. Zhang, åŠ¨æ€ä¿®æ­£çº¿æ€§æ¿€æ´»å•å…ƒï¼Œã€ŠIEEE è®¿é—®ã€‹7 (2019) 180409â€“180416ã€‚'
- en: '[69] A.Â Nicolae, Plu: The piecewise linear unit activation function, arXiv
    preprint arXiv:1809.09534 (2018).'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Nicolae, Pluï¼šåˆ†æ®µçº¿æ€§å•å…ƒæ¿€æ´»å‡½æ•°ï¼ŒarXiv é¢„å°æœ¬ arXiv:1809.09534 (2018)ã€‚'
- en: '[70] L.Â B. Godfrey, M.Â S. Gashler, A continuum among logarithmic, linear, and
    exponential functions, and its potential to improve generalization in neural networks,
    in: International Joint Conference on Knowledge Discovery, Knowledge Engineering
    and Knowledge Management, Vol.Â 1, 2015, pp. 481â€“486.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] L. B. Godfrey, M. S. Gashler, å¯¹æ•°ã€çº¿æ€§å’ŒæŒ‡æ•°å‡½æ•°ä¹‹é—´çš„è¿ç»­æ€§åŠå…¶æé«˜ç¥ç»ç½‘ç»œæ³›åŒ–èƒ½åŠ›çš„æ½œåŠ›ï¼Œè§ï¼šå›½é™…è”åˆçŸ¥è¯†å‘ç°ã€çŸ¥è¯†å·¥ç¨‹ä¸çŸ¥è¯†ç®¡ç†ä¼šè®®ï¼Œç¬¬
    1 å·ï¼Œ2015ï¼Œé¡µ 481â€“486ã€‚'
- en: '[71] B.Â Grelsson, M.Â Felsberg, Improved learning in convolutional neural networks
    with shifted exponential linear units (shelus), in: International Conference on
    Pattern Recognition, 2018, pp. 517â€“522.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] B. Grelsson, M. Felsberg, ä½¿ç”¨ç§»ä½æŒ‡æ•°çº¿æ€§å•å…ƒï¼ˆshelusï¼‰æ”¹è¿›å·ç§¯ç¥ç»ç½‘ç»œä¸­çš„å­¦ä¹ ï¼Œè§ï¼šå›½é™…æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼Œ2018ï¼Œé¡µ
    517â€“522ã€‚'
- en: '[72] Y.Â Yu, K.Â Adu, N.Â Tashi, P.Â Anokye, X.Â Wang, M.Â A. Ayidzoe, Rmaf: Relu-memristor-like
    activation function for deep learning, IEEE Access 8 (2020) 72727â€“72741.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Yu, K. Adu, N. Tashi, P. Anokye, X. Wang, M. A. Ayidzoe, Rmafï¼šç±»ä¼¼ relu
    çš„è®°å¿†ç”µé˜»æ¿€æ´»å‡½æ•°ï¼Œç”¨äºæ·±åº¦å­¦ä¹ ï¼ŒIEEE Access 8 (2020) 72727â€“72741ã€‚'
- en: '[73] M.Â Basirat, P.Â M. Roth, The quest for the golden activation function,
    arXiv preprint arXiv:1808.00783 (2018).'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. Basirat, P. M. Roth, å¯»æ‰¾é»„é‡‘æ¿€æ´»å‡½æ•°ï¼ŒarXiv é¢„å°æœ¬ arXiv:1808.00783 (2018)ã€‚'
- en: '[74] S.Â Scardapane, M.Â Scarpiniti, D.Â Comminiello, A.Â Uncini, Learning activation
    functions from data using cubic spline interpolation, in: Italian Workshop on
    Neural Nets, 2017, pp. 73â€“83.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] S. Scardapane, M. Scarpiniti, D. Comminiello, A. Uncini, ä½¿ç”¨ä¸‰æ¬¡æ ·æ¡æ’å€¼ä»æ•°æ®ä¸­å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼Œè§ï¼šæ„å¤§åˆ©ç¥ç»ç½‘ç»œç ”è®¨ä¼šï¼Œ2017ï¼Œé¡µ
    73â€“83ã€‚'
- en: '[75] A.Â Mishra, P.Â Chandra, U.Â Ghose, S.Â S. Sodhi, Bi-modal derivative adaptive
    activation function sigmoidal feedforward artificial neural networks, Applied
    Soft Computing 61 (2017) 983â€“994.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] A. Mishra, P. Chandra, U. Ghose, S. S. Sodhi, åŒæ¨¡æ€å¯¼æ•°è‡ªé€‚åº”æ¿€æ´»å‡½æ•° sigmoid å‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œï¼ŒApplied
    Soft Computing 61 (2017) 983â€“994ã€‚'
- en: '[76] S.Â Qian, H.Â Liu, C.Â Liu, S.Â Wu, H.Â SanÂ Wong, Adaptive activation functions
    in convolutional neural networks, Neurocomputing 272 (2018) 204â€“212.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S. Qian, H. Liu, C. Liu, S. Wu, H. San Wong, å·ç§¯ç¥ç»ç½‘ç»œä¸­çš„è‡ªé€‚åº”æ¿€æ´»å‡½æ•°ï¼ŒNeurocomputing
    272 (2018) 204â€“212ã€‚'
- en: '[77] E.Â Alcaide, E-swish: Adjusting activations to different network depths,
    arXiv preprint arXiv:1801.07145 (2018).'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] E. Alcaide, E-swishï¼šè°ƒæ•´ä¸åŒç½‘ç»œæ·±åº¦çš„æ¿€æ´»å‡½æ•°ï¼ŒarXiv é¢„å°æœ¬ arXiv:1801.07145 (2018)ã€‚'
- en: '[78] Ã–.Â F. ErtuÄŸrul, A novel type of activation function in artificial neural
    networks: Trained activation function, Neural Networks 99 (2018) 148â€“157.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Ã–. F. ErtuÄŸrul, äººå·¥ç¥ç»ç½‘ç»œä¸­çš„ä¸€ç§æ–°å‹æ¿€æ´»å‡½æ•°ï¼šè®­ç»ƒæ¿€æ´»å‡½æ•°ï¼ŒNeural Networks 99 (2018) 148â€“157ã€‚'
- en: '[79] M.Â Goyal, R.Â Goyal, B.Â Lall, Learning activation functions: A new paradigm
    of understanding neural networks, arXiv preprint arXiv:1906.09529 (2019).'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] M. Goyal, R. Goyal, B. Lall, å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼šç†è§£ç¥ç»ç½‘ç»œçš„æ–°èŒƒå¼ï¼ŒarXiv é¢„å°æœ¬ arXiv:1906.09529
    (2019)ã€‚'
- en: '[80] G.Â Maguolo, L.Â Nanni, S.Â Ghidoni, Ensemble of convolutional neural networks
    trained with different activation functions, arXiv preprint arXiv:1905.02473 (2019).'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] G. Maguolo, L. Nanni, S. Ghidoni, è®­ç»ƒæœ‰ä¸åŒæ¿€æ´»å‡½æ•°çš„å·ç§¯ç¥ç»ç½‘ç»œçš„é›†æˆï¼ŒarXiv é¢„å°æœ¬ arXiv:1905.02473
    (2019)ã€‚'
- en: '[81] H.Â H. Chieng, N.Â Wahid, P.Â Ong, S.Â R.Â K. Perla, Flatten-t swish: a thresholded
    relu-swish-like activation function for deep learning, arXiv preprint arXiv:1812.06247
    (2018).'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] H. H. Chieng, N. Wahid, P. Ong, S. R. K. Perla, Flatten-t swishï¼šä¸€ç§ç”¨äºæ·±åº¦å­¦ä¹ çš„é˜ˆå€¼åŒ–
    relu-swish ç±»æ¿€æ´»å‡½æ•°ï¼ŒarXiv é¢„å°æœ¬ arXiv:1812.06247 (2018)ã€‚'
- en: '[82] N.Â Patwardhan, M.Â Ingalhalikar, R.Â Walambe, Aria: Utilizing richardâ€™s
    curve for controlling the non-monotonicity of the activation function in deep
    neural nets, arXiv preprint arXiv:1805.08878 (2018).'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] N. Patwardhan, M. Ingalhalikar, R. Walambe, Ariaï¼šåˆ©ç”¨ç†æŸ¥å¾·æ›²çº¿æ§åˆ¶æ·±åº¦ç¥ç»ç½‘ç»œä¸­æ¿€æ´»å‡½æ•°çš„éå•è°ƒæ€§ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:1805.08878 (2018)ã€‚'
- en: '[83] M.Â Dushkoff, R.Â Ptucha, Adaptive activation functions for deep networks,
    Electronic Imaging 2016Â (19) (2016) 1â€“5.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] M. Dushkoff, R. Ptucha, æ·±åº¦ç½‘ç»œçš„è‡ªé€‚åº”æ¿€æ´»å‡½æ•°ï¼ŒElectronic Imaging 2016 (19) (2016)
    1â€“5ã€‚'
- en: '[84] F.Â Manessi, A.Â Rozza, Learning combinations of activation functions, in:
    IEEE International Conference on Pattern Recognition, 2018, pp. 61â€“66.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] F. Manessi, A. Rozza, å­¦ä¹ æ¿€æ´»å‡½æ•°çš„ç»„åˆï¼Œè§ï¼šIEEE å›½é™…æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼Œ2018ï¼Œé¡µ 61â€“66ã€‚'
- en: '[85] L.Â R. SÃ¼tfeld, F.Â Brieger, H.Â Finger, S.Â FÃ¼llhase, G.Â Pipa, Adaptive blending
    units: Trainable activation functions for deep neural networks, arXiv preprint
    arXiv:1806.10064 (2018).'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] L. R. SÃ¼tfeld, F. Brieger, H. Finger, S. FÃ¼llhase, G. Pipa, è‡ªé€‚åº”æ··åˆå•å…ƒï¼šç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¯è®­ç»ƒæ¿€æ´»å‡½æ•°ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:1806.10064 (2018)ã€‚'
- en: '[86] M.Â Wang, B.Â Liu, H.Â Foroosh, Look-up table unit activation function for
    deep convolutional neural networks, in: IEEE Winter Conference on Applications
    of Computer Vision, 2018, pp. 1225â€“1233.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] M. Wang, B. Liu, H. Foroosh, ç”¨äºæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„æŸ¥æ‰¾è¡¨å•å…ƒæ¿€æ´»å‡½æ•°ï¼Œè§ï¼šIEEE è®¡ç®—æœºè§†è§‰åº”ç”¨å†¬å­£ä¼šè®®ï¼Œ2018ï¼Œé¡µ
    1225â€“1233ã€‚'
- en: '[87] D.Â Klabjan, M.Â Harmon, Activation ensembles for deep neural networks,
    in: IEEE International Conference on Big Data, 2019, pp. 206â€“214.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] D. Klabjan, M. Harmon, æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¿€æ´»é›†æˆï¼Œå‘è¡¨äºï¼šIEEEå›½é™…å¤§æ•°æ®ä¼šè®®ï¼Œ2019å¹´ï¼Œç¬¬206â€“214é¡µã€‚'
- en: '[88] C.Â Eisenach, Z.Â Wang, H.Â Liu, Nonparametrically learning activation functions
    in deep neural nets, in: International Conference on Learning Representations
    Workshops, 2017.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C. Eisenach, Z. Wang, H. Liu, åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­éå‚æ•°å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼Œå‘è¡¨äºï¼šå­¦ä¹ è¡¨ç¤ºå›½é™…ä¼šè®®ç ”è®¨ä¼šï¼Œ2017å¹´ã€‚'
- en: '[89] C.Â J. Vercellino, W.Â Y. Wang, Hyperactivations for activation function
    exploration, in: Conference on Neural Information Processing Systems Workshop
    on Meta-learning, 2017.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] C. J. Vercellino, W. Y. Wang, æ¿€æ´»å‡½æ•°æ¢ç´¢çš„è¶…æ¿€æ´»ï¼Œå‘è¡¨äºï¼šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿä¼šè®®çš„å…ƒå­¦ä¹ ç ”è®¨ä¼šï¼Œ2017å¹´ã€‚'
- en: '[90] A.Â D. Jagtap, K.Â Kawaguchi, G.Â E. Karniadakis, Adaptive activation functions
    accelerate convergence in deep and physics-informed neural networks, Journal of
    Computational Physics 404 (2020) 109136.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. D. Jagtap, K. Kawaguchi, G. E. Karniadakis, è‡ªé€‚åº”æ¿€æ´»å‡½æ•°åŠ é€Ÿæ·±åº¦å’Œç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„æ”¶æ•›ï¼Œã€Šè®¡ç®—ç‰©ç†å­¦æ‚å¿—ã€‹404
    (2020) 109136ã€‚'
- en: '[91] C.Â Dugas, Y.Â Bengio, F.Â BÃ©lisle, C.Â Nadeau, R.Â Garcia, Incorporating second-order
    functional knowledge for better option pricing, in: Advances in Neural Information
    Processing Systems, 2001, pp. 472â€“478.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] C. Dugas, Y. Bengio, F. BÃ©lisle, C. Nadeau, R. Garcia, ç»“åˆäºŒé˜¶åŠŸèƒ½çŸ¥è¯†ä»¥æ›´å¥½åœ°è¿›è¡ŒæœŸæƒå®šä»·ï¼Œå‘è¡¨äºï¼šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ2001å¹´ï¼Œç¬¬472â€“478é¡µã€‚'
- en: '[92] X.Â Glorot, A.Â Bordes, Y.Â Bengio, Deep sparse rectifier neural networks,
    in: International Conference on Artificial Intelligence and Statistics, 2011,
    pp. 315â€“323.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] X. Glorot, A. Bordes, Y. Bengio, æ·±åº¦ç¨€ç–æ•´æµç¥ç»ç½‘ç»œï¼Œå‘è¡¨äºï¼šäººå·¥æ™ºèƒ½ä¸ç»Ÿè®¡å›½é™…ä¼šè®®ï¼Œ2011å¹´ï¼Œç¬¬315â€“323é¡µã€‚'
- en: '[93] H.Â Zheng, Z.Â Yang, W.Â Liu, J.Â Liang, Y.Â Li, Improving deep neural networks
    using softplus units, in: International Joint Conference on Neural Networks, 2015,
    pp. 1â€“4.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] H. Zheng, Z. Yang, W. Liu, J. Liang, Y. Li, åˆ©ç”¨softpluså•å…ƒæ”¹è¿›æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå‘è¡¨äºï¼šå›½é™…è”åˆç¥ç»ç½‘ç»œä¼šè®®ï¼Œ2015å¹´ï¼Œç¬¬1â€“4é¡µã€‚'
- en: '[94] Q.Â Liu, S.Â Furber, Noisy softplus: a biology inspired activation function,
    in: International Conference on Neural Information Processing, 2016, pp. 405â€“412.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Q. Liu, S. Furber, å™ªå£°softplusï¼šä¸€ç§å—ç”Ÿç‰©å¯å‘çš„æ¿€æ´»å‡½æ•°ï¼Œå‘è¡¨äºï¼šç¥ç»ä¿¡æ¯å¤„ç†å›½é™…ä¼šè®®ï¼Œ2016å¹´ï¼Œç¬¬405â€“412é¡µã€‚'
- en: '[95] H.Â Zhao, F.Â Liu, L.Â Li, C.Â Luo, A novel softplus linear unit for deep
    convolutional neural networks, Applied Intelligence 48Â (7) (2018) 1707â€“1720.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] H. Zhao, F. Liu, L. Li, C. Luo, ç”¨äºæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„æ–°å‹softplusçº¿æ€§å•å…ƒï¼Œåº”ç”¨æ™ºèƒ½ 48 (7)
    (2018) 1707â€“1720ã€‚'
- en: '[96] C.Â Xu, J.Â Huang, S.-p. Wang, A.-q. Hu, A novel parameterized activation
    function in visual geometry group, in: International Conference on Data Science
    and Business Analytics, 2018, pp. 386â€“389.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. Xu, J. Huang, S.-p. Wang, A.-q. Hu, è§†è§‰å‡ ä½•ç»„ä¸­çš„ä¸€ç§æ–°å‹å‚æ•°åŒ–æ¿€æ´»å‡½æ•°ï¼Œå‘è¡¨äºï¼šæ•°æ®ç§‘å­¦ä¸å•†ä¸šåˆ†æå›½é™…ä¼šè®®ï¼Œ2018å¹´ï¼Œç¬¬386â€“389é¡µã€‚'
- en: '[97] K.Â Sun, J.Â Yu, L.Â Zhang, Z.Â Dong, A convolutional neural network model
    based on improved softplus activation function, in: International Conference on
    Applications and Techniques in Cyber Security and Intelligence, 2019, pp. 1326â€“1335.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K. Sun, J. Yu, L. Zhang, Z. Dong, åŸºäºæ”¹è¿›çš„softplusæ¿€æ´»å‡½æ•°çš„å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå‘è¡¨äºï¼šç½‘ç»œå®‰å…¨ä¸æ™ºèƒ½åº”ç”¨æŠ€æœ¯å›½é™…ä¼šè®®ï¼Œ2019å¹´ï¼Œç¬¬1326â€“1335é¡µã€‚'
- en: '[98] Y.Â Chen, Y.Â Mai, J.Â Xiao, L.Â Zhang, Improving the antinoise ability of
    dnns via a bio-inspired noise adaptive activation function rand softplus, Neural
    Computation 31Â (6) (2019) 1215â€“1233.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Chen, Y. Mai, J. Xiao, L. Zhang, é€šè¿‡ç”Ÿç‰©å¯å‘çš„å™ªå£°è‡ªé€‚åº”æ¿€æ´»å‡½æ•°rand softplusæé«˜DNNçš„æŠ—å™ªå£°èƒ½åŠ›ï¼Œã€Šç¥ç»è®¡ç®—ã€‹31
    (6) (2019) 1215â€“1233ã€‚'
- en: '[99] D.Â Misra, Mish: A self regularized non-monotonic neural activation function,
    arXiv preprint arXiv:1908.08681 (2019).'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] D. Misra, Mish: ä¸€ç§è‡ªæˆ‘æ­£åˆ™åŒ–çš„éå•è°ƒç¥ç»æ¿€æ´»å‡½æ•°ï¼ŒarXiv é¢„å°æœ¬ arXiv:1908.08681 (2019)ã€‚'
- en: '[100] A.Â Bochkovskiy, C.-Y. Wang, H.-Y.Â M. Liao, Yolov4: Optimal speed and
    accuracy of object detection, arXiv preprint arXiv:2004.10934 (2020).'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] A. Bochkovskiy, C.-Y. Wang, H.-Y. M. Liao, Yolov4: ç›®æ ‡æ£€æµ‹çš„æœ€ä½³é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:2004.10934 (2020)ã€‚'
- en: '[101] D.Â Hendrycks, K.Â Gimpel, Gaussian error linear units (gelus), arXiv preprint
    arXiv:1606.08415 (2016).'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] D. Hendrycks, K. Gimpel, é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒï¼ˆgelusï¼‰ï¼ŒarXiv é¢„å°æœ¬ arXiv:1606.08415 (2016)ã€‚'
- en: '[102] C.Â Yu, Z.Â Su, Symmetrical gaussian error linear units (sgelus), arXiv
    preprint arXiv:1911.03925 (2019).'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] C. Yu, Z. Su, å¯¹ç§°é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒï¼ˆsgelusï¼‰ï¼ŒarXiv é¢„å°æœ¬ arXiv:1911.03925 (2019)ã€‚'
- en: '[103] Q.Â Su, L.Â Carin, etÂ al., A probabilistic framework for nonlinearities
    in stochastic neural networks, in: Advances in Neural Information Processing Systems,
    2017, pp. 4486â€“4495.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Q. Su, L. Carin ç­‰ï¼Œéšæœºç¥ç»ç½‘ç»œä¸­éçº¿æ€§çš„æ¦‚ç‡æ¡†æ¶ï¼Œå‘è¡¨äºï¼šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ2017å¹´ï¼Œç¬¬4486â€“4495é¡µã€‚'
- en: '[104] J.Â Lee, K.Â Shridhar, H.Â Hayashi, B.Â K. Iwana, S.Â Kang, S.Â Uchida, Probact:
    A probabilistic activation function for deep neural networks, arXiv preprint arXiv:1905.10761
    (2019).'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. Lee, K. Shridhar, H. Hayashi, B. K. Iwana, S. Kang, S. Uchida, Probactï¼šæ·±åº¦ç¥ç»ç½‘ç»œçš„æ¦‚ç‡æ¿€æ´»å‡½æ•°ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:1905.10761 (2019)ã€‚'
- en: '[105] L.Â Hou, D.Â Samaras, T.Â M. Kurc, Y.Â Gao, J.Â H. Saltz, Convnets with smooth
    adaptive activation functions for regression, Proceedings of Machine Learning
    Research 54 (2017) 430.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] L. Hou, D. Samaras, T. M. Kurc, Y. Gao, J. H. Saltz, å…·æœ‰å¹³æ»‘è‡ªé€‚åº”æ¿€æ´»å‡½æ•°çš„å·ç§¯ç½‘ç»œç”¨äºå›å½’ï¼Œæœºå™¨å­¦ä¹ ç ”ç©¶ä¼šè®®è®ºæ–‡
    54 (2017) 430ã€‚'
- en: '[106] Y.Â Berradi, Symmetric power activation functions for deep neural networks,
    in: International Conference on Learning and Optimization Algorithms: Theory and
    Applications, 2018, pp. 1â€“6.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Y. Berradi, å¯¹ç§°å¹‚æ¿€æ´»å‡½æ•°ç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œï¼Œåœ¨ï¼šå›½é™…å­¦ä¹ ä¸ä¼˜åŒ–ç®—æ³•ä¼šè®®ï¼šç†è®ºä¸åº”ç”¨ï¼Œ2018ï¼Œpp. 1â€“6ã€‚'
- en: '[107] E.Â LÃ³pez-Rubio, F.Â Ortega-Zamorano, E.Â DomÃ­nguez, J.Â MuÃ±oz-PÃ©rez, Piecewise
    polynomial activation functions for feedforward neural networks, Neural Processing
    Letters 50Â (1) (2019) 121â€“147.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] E. LÃ³pez-Rubio, F. Ortega-Zamorano, E. DomÃ­nguez, J. MuÃ±oz-PÃ©rez, ç”¨äºå‰é¦ˆç¥ç»ç½‘ç»œçš„åˆ†æ®µå¤šé¡¹å¼æ¿€æ´»å‡½æ•°ï¼Œç¥ç»å¤„ç†ä¿¡ä»¶
    50 (1) (2019) 121â€“147ã€‚'
- en: '[108] F.Â Farhadi, V.Â P. Nia, A.Â Lodi, Activation adaptation in neural networks,
    arXiv preprint arXiv:1901.09849 (2019).'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] F. Farhadi, V. P. Nia, A. Lodi, ç¥ç»ç½‘ç»œä¸­çš„æ¿€æ´»é€‚åº”ï¼ŒarXiv é¢„å°æœ¬ arXiv:1901.09849
    (2019)ã€‚'
- en: '[109] B.Â Li, S.Â Tang, H.Â Yu, Powernet: Efficient representations of polynomials
    and smooth functions by deep neural networks with rectified power units, arXiv
    preprint arXiv:1909.05136 (2019).'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] B. Li, S. Tang, H. Yu, Powernetï¼šé€šè¿‡å…·æœ‰æ•´æµå¹‚å•å…ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œé«˜æ•ˆè¡¨ç¤ºå¤šé¡¹å¼å’Œå…‰æ»‘å‡½æ•°ï¼ŒarXiv é¢„å°æœ¬
    arXiv:1909.05136 (2019)ã€‚'
- en: '[110] M.Â Telgarsky, Neural networks and rational functions, in: International
    Conference on Machine Learning, 2017, pp. 3387â€“3393.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] M. Telgarsky, ç¥ç»ç½‘ç»œä¸æœ‰ç†å‡½æ•°ï¼Œåœ¨ï¼šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼Œ2017ï¼Œpp. 3387â€“3393ã€‚'
- en: '[111] A.Â Molina, P.Â Schramowski, K.Â Kersting, Pad$\acute{e}$ activation units:
    End-to-end learning of flexible activation functions in deep networks, International
    Conference on Learning Representations (2020).'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] A. Molina, P. Schramowski, K. Kersting, Pad$\acute{e}$ æ¿€æ´»å•å…ƒï¼šæ·±åº¦ç½‘ç»œä¸­çµæ´»æ¿€æ´»å‡½æ•°çš„ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œå›½é™…å­¦ä¹ è¡¨ç¤ºä¼šè®®
    (2020)ã€‚'
- en: '[112] A.Â T. NicolasÂ BoullÃ©, YujiÂ Nakatsukasa, Rational neural networks, arXiv
    preprint arXiv:2004.01902 (2020).'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] A. T. Nicolas BoullÃ©, Yuji Nakatsukasa, æœ‰ç†ç¥ç»ç½‘ç»œï¼ŒarXiv é¢„å°æœ¬ arXiv:2004.01902
    (2020)ã€‚'
- en: '[113] A.Â Apicella, F.Â IsgrÃ², R.Â Prevete, A simple and efficient architecture
    for trainable activation functions, Neurocomputing 370 (2019) 1â€“15.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Apicella, F. IsgrÃ², R. Prevete, ä¸€ç§ç®€å•é«˜æ•ˆçš„å¯è®­ç»ƒæ¿€æ´»å‡½æ•°æ¶æ„ï¼Œç¥ç»è®¡ç®— 370 (2019) 1â€“15ã€‚'
- en: '[114] Y.Â Chen, X.Â Dai, M.Â Liu, D.Â Chen, L.Â Yuan, Z.Â Liu, Dynamic relu, arXiv
    preprint arXiv:2003.10027 (2020).'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Y. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, Z. Liu, åŠ¨æ€ reluï¼ŒarXiv é¢„å°æœ¬
    arXiv:2003.10027 (2020)ã€‚'
- en: '[115] M.Â Wang, B.Â Liu, H.Â Foroosh, Wide hidden expansion layer for deep convolutional
    neural networks, in: IEEE Winter Conference on Applications of Computer Vision,
    2020, pp. 934â€“942.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] M. Wang, B. Liu, H. Foroosh, æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„å®½éšè—æ‰©å±•å±‚ï¼Œåœ¨ï¼šIEEE å†¬å­£è®¡ç®—æœºè§†è§‰åº”ç”¨ä¼šè®®ï¼Œ2020ï¼Œpp.
    934â€“942ã€‚'
- en: '[116] A.Â Asif, etÂ al., Learning neural activations, arXiv preprint arXiv:1912.12187
    (2019).'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] A. Asif ç­‰ï¼Œå­¦ä¹ ç¥ç»æ¿€æ´»ï¼ŒarXiv é¢„å°æœ¬ arXiv:1912.12187 (2019)ã€‚'
- en: '[117] S.Â Scardapane, S.Â VanÂ Vaerenbergh, S.Â Totaro, A.Â Uncini, Kafnets: Kernel-based
    non-parametric activation functions for neural networks, Neural Networks 110 (2019)
    19â€“32.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. Scardapane, S. Van Vaerenbergh, S. Totaro, A. Uncini, Kafnetsï¼šåŸºäºæ ¸çš„éå‚æ•°æ¿€æ´»å‡½æ•°ç”¨äºç¥ç»ç½‘ç»œï¼Œç¥ç»ç½‘ç»œ
    110 (2019) 19â€“32ã€‚'
- en: '[118] S.Â Scardapane, E.Â Nieddu, D.Â Firmani, P.Â Merialdo, Multikernel activation
    functions: formulation and a case study, in: INNS Big Data and Deep Learning conference,
    2019, pp. 320â€“329.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] S. Scardapane, E. Nieddu, D. Firmani, P. Merialdo, å¤šæ ¸æ¿€æ´»å‡½æ•°ï¼šå…¬å¼åŒ–åŠæ¡ˆä¾‹ç ”ç©¶ï¼Œåœ¨ï¼šINNS
    å¤§æ•°æ®ä¸æ·±åº¦å­¦ä¹ ä¼šè®®ï¼Œ2019ï¼Œpp. 320â€“329ã€‚'
- en: '[119] S.Â Scardapane, S.Â VanÂ Vaerenbergh, A.Â Hussain, A.Â Uncini, Complex-valued
    neural networks with nonparametric activation functions, IEEE Transactions on
    Emerging Topics in Computational Intelligence (2018).'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] S. Scardapane, S. Van Vaerenbergh, A. Hussain, A. Uncini, å…·æœ‰éå‚æ•°æ¿€æ´»å‡½æ•°çš„å¤æ‚å€¼ç¥ç»ç½‘ç»œï¼ŒIEEE
    è®¡ç®—æ™ºèƒ½æ–°å…´ä¸»é¢˜äº¤æ˜“ (2018)ã€‚'
- en: '[120] S.Â Scardapane, S.Â VanÂ Vaerenbergh, D.Â Comminiello, A.Â Uncini, Widely
    linear kernels for complex-valued kernel activation functions, in: IEEE International
    Conference on Acoustics, Speech and Signal Processing, 2019, pp. 8528â€“8532.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] S. Scardapane, S. Van Vaerenbergh, D. Comminiello, A. Uncini, å¤æ‚å€¼æ ¸æ¿€æ´»å‡½æ•°çš„å¹¿ä¹‰çº¿æ€§æ ¸ï¼Œåœ¨ï¼šIEEE
    å›½é™…å£°å­¦ã€è¯­éŸ³å’Œä¿¡å·å¤„ç†ä¼šè®®ï¼Œ2019ï¼Œpp. 8528â€“8532ã€‚'
- en: '[121] M.Â Kobayashi, Singularities of three-layered complex-valued neural networks
    with split activation function, IEEE Transactions on Neural Networks and Learning
    Systems 29Â (5) (2017) 1900â€“1907.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Kobayashi, ä¸‰å±‚å¤å€¼ç¥ç»ç½‘ç»œçš„å¥‡å¼‚æ€§ä¸åˆ†è£‚æ¿€æ´»å‡½æ•°ï¼ŒIEEE ç¥ç»ç½‘ç»œä¸å­¦ä¹ ç³»ç»Ÿå­¦æŠ¥ 29 (5) (2017) 1900â€“1907ã€‚'
- en: '[122] J.Â Pennington, S.Â Schoenholz, S.Â Ganguli, Resurrecting the sigmoid in
    deep learning through dynamical isometry: theory and practice, in: Advances in
    Neural Information Processing Systems, 2017, pp. 4785â€“4795.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] J. Pennington, S. Schoenholz, S. Ganguli, é€šè¿‡åŠ¨æ€ç­‰è·æ€§å¤å…´æ·±åº¦å­¦ä¹ ä¸­çš„ sigmoidï¼šç†è®ºä¸å®è·µï¼Œè§ï¼šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ2017ï¼Œç¬¬4785â€“4795é¡µã€‚'
- en: '[123] E.Â Sansone, F.Â G. DeÂ Natale, Training feedforward neural networks with
    standard logistic activations is feasible, arXiv preprint arXiv:1710.01013 (2017).'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] E. Sansone, F. G. De Natale, ç”¨æ ‡å‡† logistic æ¿€æ´»å‡½æ•°è®­ç»ƒå‰é¦ˆç¥ç»ç½‘ç»œæ˜¯å¯è¡Œçš„ï¼ŒarXiv é¢„å°æœ¬
    arXiv:1710.01013 (2017)ã€‚'
- en: '[124] L.Â Lu, Y.Â Shin, Y.Â Su, G.Â E. Karniadakis, Dying relu and initialization:
    Theory and numerical examples, arXiv preprint arXiv:1903.06733 (2019).'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] L. Lu, Y. Shin, Y. Su, G. E. Karniadakis, é€€åŒ–çš„ relu å’Œåˆå§‹åŒ–ï¼šç†è®ºä¸æ•°å€¼ä¾‹å­ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:1903.06733 (2019)ã€‚'
- en: '[125] D.Â Arpit, Y.Â Bengio, The benefits of over-parameterization at initialization
    in deep relu networks, arXiv preprint arXiv:1901.03611 (2019).'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] D. Arpit, Y. Bengio, æ·±åº¦ relu ç½‘ç»œåœ¨åˆå§‹åŒ–æ—¶çš„è¶…å‚æ•°åŒ–ä¼˜åŠ¿ï¼ŒarXiv é¢„å°æœ¬ arXiv:1901.03611
    (2019)ã€‚'
- en: '[126] D.Â Aguirre, O.Â Fuentes, Improving weight initialization of relu and output
    layers, in: International Conference on Artificial Neural Networks, 2019, pp.
    170â€“184.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] D. Aguirre, O. Fuentes, æ”¹è¿› relu å’Œè¾“å‡ºå±‚çš„æƒé‡åˆå§‹åŒ–ï¼Œè§ï¼šå›½é™…äººå·¥ç¥ç»ç½‘ç»œä¼šè®®ï¼Œ2019ï¼Œç¬¬170â€“184é¡µã€‚'
- en: '[127] R.Â Burkholz, A.Â Dubatovka, Initialization of relus for dynamical isometry,
    in: Advances in Neural Information Processing Systems, 2019, pp. 2382â€“2392.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] R. Burkholz, A. Dubatovka, ä¸ºåŠ¨æ€ç­‰è·æ€§åˆå§‹åŒ– reluï¼Œè§ï¼šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ2019ï¼Œç¬¬2382â€“2392é¡µã€‚'
- en: '[128] D.Â Yarotsky, Error bounds for approximations with deep relu networks,
    Neural Networks 94 (2017) 103â€“114.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] D. Yarotsky, æ·±åº¦ relu ç½‘ç»œçš„è¿‘ä¼¼è¯¯å·®ç•Œé™ï¼Œç¥ç»ç½‘ç»œ 94 (2017) 103â€“114ã€‚'
- en: '[129] R.Â Arora, A.Â Basu, P.Â Mianjy, A.Â Mukherjee, Understanding deep neural
    networks with rectified linear units, arXiv preprint arXiv:1611.01491 (2016).'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] R. Arora, A. Basu, P. Mianjy, A. Mukherjee, ç†è§£å¸¦æœ‰ä¿®æ­£çº¿æ€§å•å…ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œï¼ŒarXiv é¢„å°æœ¬
    arXiv:1611.01491 (2016)ã€‚'
- en: '[130] M.Â Hein, M.Â Andriushchenko, J.Â Bitterwolf, Why relu networks yield high-confidence
    predictions far away from the training data and how to mitigate the problem, in:
    IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 41â€“50.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] M. Hein, M. Andriushchenko, J. Bitterwolf, ä¸ºä»€ä¹ˆ relu ç½‘ç»œåœ¨è®­ç»ƒæ•°æ®ä¹‹å¤–ç»™å‡ºé«˜ç½®ä¿¡åº¦é¢„æµ‹ä»¥åŠå¦‚ä½•ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œè§ï¼šIEEE
    è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼Œ2019ï¼Œç¬¬41â€“50é¡µã€‚'
- en: '[131] S.Â Goel, S.Â Karmalkar, A.Â Klivans, Time/accuracy tradeoffs for learning
    a relu with respect to gaussian marginals, in: Advances in Neural Information
    Processing Systems, 2019, pp. 8582â€“8591.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] S. Goel, S. Karmalkar, A. Klivans, å…³äºå­¦ä¹ å¸¦æœ‰é«˜æ–¯è¾¹é™…çš„ relu çš„æ—¶é—´/ç²¾åº¦æƒè¡¡ï¼Œè§ï¼šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ï¼Œ2019ï¼Œç¬¬8582â€“8591é¡µã€‚'
- en: '[132] S.Â Dittmer, J.Â Emily, P.Â Maass, Singular values for relu layers, IEEE
    Transactions on Neural Networks and Learning Systems (2019).'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] S. Dittmer, J. Emily, P. Maass, relu å±‚çš„å¥‡å¼‚å€¼ï¼ŒIEEE ç¥ç»ç½‘ç»œä¸å­¦ä¹ ç³»ç»Ÿå­¦æŠ¥ (2019)ã€‚'
- en: '[133] A.Â Kristiadi, M.Â Hein, P.Â Hennig, Being bayesian, even just a bit, fixes
    overconfidence in relu networks, arXiv preprint arXiv:2002.10118 (2020).'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] A. Kristiadi, M. Hein, P. Hennig, å³ä½¿åªæ˜¯ç•¥å¾®è´å¶æ–¯åŒ–ï¼Œä¹Ÿèƒ½ä¿®æ­£ relu ç½‘ç»œçš„è¿‡åº¦è‡ªä¿¡ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:2002.10118 (2020)ã€‚'
- en: '[134] B.Â Karlik, A.Â V. Olgac, Performance analysis of various activation functions
    in generalized mlp architectures of neural networks, International Journal of
    Artificial Intelligence and Expert Systems 1Â (4) (2011) 111â€“122.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] B. Karlik, A. V. Olgac, å„ç§æ¿€æ´»å‡½æ•°åœ¨ç¥ç»ç½‘ç»œé€šç”¨ MLP æ¶æ„ä¸­çš„æ€§èƒ½åˆ†æï¼Œå›½é™…äººå·¥æ™ºèƒ½ä¸ä¸“å®¶ç³»ç»Ÿæ‚å¿— 1 (4)
    (2011) 111â€“122ã€‚'
- en: '[135] G.Â Alcantara, Empirical analysis of non-linear activation functions for
    deep neural networks in classification tasks, arXiv preprint arXiv:1710.11272
    (2017).'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] G. Alcantara, å¯¹äºåˆ†ç±»ä»»åŠ¡çš„æ·±åº¦ç¥ç»ç½‘ç»œä¸­éçº¿æ€§æ¿€æ´»å‡½æ•°çš„å®è¯åˆ†æï¼ŒarXiv é¢„å°æœ¬ arXiv:1710.11272 (2017)ã€‚'
- en: '[136] H.Â K. Vydana, A.Â K. Vuppala, Investigative study of various activation
    functions for speech recognition, in: National Conference on Communications, 2017,
    pp. 1â€“5.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] H. K. Vydana, A. K. Vuppala, å„ç§æ¿€æ´»å‡½æ•°åœ¨è¯­éŸ³è¯†åˆ«ä¸­çš„ç ”ç©¶ï¼Œè§ï¼šå›½å®¶é€šä¿¡ä¼šè®®ï¼Œ2017ï¼Œç¬¬1â€“5é¡µã€‚'
- en: '[137] D.Â Pedamonti, Comparison of non-linear activation functions for deep
    neural networks on mnist classification task, arXiv preprint arXiv:1804.02763
    (2018).'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] D. Pedamonti, å¯¹æ¯”æ·±åº¦ç¥ç»ç½‘ç»œåœ¨ MNIST åˆ†ç±»ä»»åŠ¡ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ŒarXiv é¢„å°æœ¬ arXiv:1804.02763
    (2018)ã€‚'
- en: '[138] C.Â Nwankpa, W.Â Ijomah, A.Â Gachagan, S.Â Marshall, Activation functions:
    Comparison of trends in practice and research for deep learning, arXiv preprint
    arXiv:1811.03378 (2018).'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] C. Nwankpa, W. Ijomah, A. Gachagan, S. Marshall, æ¿€æ´»å‡½æ•°ï¼šæ·±åº¦å­¦ä¹ å®è·µä¸ç ”ç©¶è¶‹åŠ¿çš„æ¯”è¾ƒï¼ŒarXiv
    é¢„å°æœ¬ arXiv:1811.03378 (2018)ã€‚'
- en: '[139] K.Â Eckle, J.Â Schmidt-Hieber, A comparison of deep networks with relu
    activation function and linear spline-type methods, Neural Networks 110 (2019)
    232â€“242.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] K. Eckle, J. Schmidt-Hieber, æ·±åº¦ç½‘ç»œä¸reluæ¿€æ´»å‡½æ•°åŠçº¿æ€§æ ·æ¡æ–¹æ³•çš„æ¯”è¾ƒï¼ŒNeural Networks
    110 (2019) 232â€“242ã€‚'
- en: '[140] M.Â M. Lau, K.Â H. Lim, Review of adaptive activation function in deep
    neural network, in: IEEE-EMBS Conference on Biomedical Engineering and Sciences,
    2018, pp. 686â€“690.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] M. M. Lau, K. H. Lim, æ·±åº¦ç¥ç»ç½‘ç»œä¸­è‡ªé€‚åº”æ¿€æ´»å‡½æ•°çš„å›é¡¾ï¼Œè§ï¼šIEEE-EMBSç”Ÿç‰©åŒ»å­¦å·¥ç¨‹ä¸ç§‘å­¦ä¼šè®®ï¼Œ2018å¹´ï¼Œé¡µ686â€“690ã€‚'
- en: '[141] A.Â K. Dubey, V.Â Jain, Comparative study of convolution neural networkâ€™s
    relu and leaky-relu activation functions, in: Applications of Computing, Automation
    and Wireless Systems in Electrical Engineering, Springer, 2019, pp. 873â€“880.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] A. K. Dubey, V. Jain, å·ç§¯ç¥ç»ç½‘ç»œä¸­reluä¸leaky-reluæ¿€æ´»å‡½æ•°çš„æ¯”è¾ƒç ”ç©¶ï¼Œè§ï¼šç”µæ°”å·¥ç¨‹è®¡ç®—ã€è‡ªåŠ¨åŒ–ä¸æ— çº¿ç³»ç»Ÿåº”ç”¨ï¼ŒSpringerï¼Œ2019å¹´ï¼Œé¡µ873â€“880ã€‚'
- en: '[142] C.Â Banerjee, T.Â Mukherjee, E.Â PasiliaoÂ Jr, An empirical study on generalizations
    of the relu activation function, in: ACM Southeast Conference, 2019, pp. 164â€“167.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] C. Banerjee, T. Mukherjee, E. Pasiliao Jr, reluæ¿€æ´»å‡½æ•°çš„å¹¿ä¹‰åŒ–å®è¯ç ”ç©¶ï¼Œè§ï¼šACMä¸œå—ä¼šè®®ï¼Œ2019å¹´ï¼Œé¡µ164â€“167ã€‚'
- en: '[143] T.Â Villmann, J.Â Ravichandran, A.Â Villmann, D.Â Nebel, M.Â Kaden, Activation
    functions for generalized learning vector quantization-a performance comparison,
    arXiv preprint arXiv:1901.05995 (2019).'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] T. Villmann, J. Ravichandran, A. Villmann, D. Nebel, M. Kaden, å¹¿ä¹‰å­¦ä¹ å‘é‡é‡åŒ–çš„æ¿€æ´»å‡½æ•°â€”â€”æ€§èƒ½æ¯”è¾ƒï¼ŒarXiv
    é¢„å°æœ¬ arXiv:1901.05995 (2019)ã€‚'
- en: '[144] G.Â Castaneda, P.Â Morris, T.Â M. Khoshgoftaar, Evaluation of maxout activations
    in deep learning across several big data domains, Journal of Big Data 6Â (1) (2019)
    72.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] G. Castaneda, P. Morris, T. M. Khoshgoftaar, å¤šä¸ªå¤§æ•°æ®é¢†åŸŸæ·±åº¦å­¦ä¹ ä¸­maxoutæ¿€æ´»çš„è¯„ä¼°ï¼ŒJournal
    of Big Data 6 (1) (2019) 72ã€‚'
- en: '[145] Y.Â Wang, Y.Â Li, Y.Â Song, X.Â Rong, The influence of the activation function
    in a convolution neural network model of facial expression recognition, Applied
    Sciences 10Â (5) (2020) 1897.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Y. Wang, Y. Li, Y. Song, X. Rong, æ¿€æ´»å‡½æ•°åœ¨é¢éƒ¨è¡¨æƒ…è¯†åˆ«å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­çš„å½±å“ï¼ŒApplied Sciences
    10 (5) (2020) 1897ã€‚'
- en: '[146] A.Â Apicella, F.Â Donnarumma, F.Â IsgrÃ², R.Â Prevete, A survey on modern
    trainable activation functions, arXiv preprint arXiv:2005.00817 (2020).'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] A. Apicella, F. Donnarumma, F. IsgrÃ², R. Prevete, ç°ä»£å¯è®­ç»ƒæ¿€æ´»å‡½æ•°çš„è°ƒæŸ¥ï¼ŒarXiv
    é¢„å°æœ¬ arXiv:2005.00817 (2020)ã€‚'
- en: '[147] T.Â SzandaÅ‚a, Review and comparison of commonly used activation functions
    for deep neural networks, in: Bio-inspired Neurocomputing, 2020, pp. 203â€“224.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] T. SzandaÅ‚a, å¸¸ç”¨æ¿€æ´»å‡½æ•°çš„å›é¡¾ä¸æ¯”è¾ƒï¼Œè§ï¼šç”Ÿç‰©å¯å‘ç¥ç»è®¡ç®—ï¼Œ2020å¹´ï¼Œé¡µ203â€“224ã€‚'
- en: '[148] A.Â Krizhevsky, Learning multiple layers of features from tiny images,
    Tech Report, Univ. of Toronto (2009).'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] A. Krizhevsky, ä»å°å›¾åƒä¸­å­¦ä¹ å¤šä¸ªç‰¹å¾å±‚ï¼ŒæŠ€æœ¯æŠ¥å‘Šï¼Œå¤šä¼¦å¤šå¤§å­¦ï¼ˆ2009å¹´ï¼‰ã€‚'
- en: '[149] A.Â G. Howard, M.Â Zhu, B.Â Chen, D.Â Kalenichenko, W.Â Wang, T.Â Weyand, M.Â Andreetto,
    H.Â Adam, Mobilenets: Efficient convolutional neural networks for mobile vision
    applications, arXiv preprint arXiv:1704.04861 (2017).'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto, H. Adam, Mobilenetsï¼šç§»åŠ¨è§†è§‰åº”ç”¨çš„é«˜æ•ˆå·ç§¯ç¥ç»ç½‘ç»œï¼ŒarXiv é¢„å°æœ¬ arXiv:1704.04861 (2017)ã€‚'
- en: '[150] K.Â Simonyan, A.Â Zisserman, Very deep convolutional networks for large-scale
    image recognition, arXiv preprint arXiv:1409.1556 (2014).'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] K. Simonyan, A. Zisserman, ç”¨äºå¤§è§„æ¨¡å›¾åƒè¯†åˆ«çš„éå¸¸æ·±å·ç§¯ç½‘ç»œï¼ŒarXiv é¢„å°æœ¬ arXiv:1409.1556
    (2014)ã€‚'
- en: '[151] C.Â Szegedy, W.Â Liu, Y.Â Jia, P.Â Sermanet, S.Â Reed, D.Â Anguelov, D.Â Erhan,
    V.Â Vanhoucke, A.Â Rabinovich, Going deeper with convolutions, in: IEEE Conference
    on Computer Vision and Pattern Recognition, 2015, pp. 1â€“9.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, A. Rabinovich, æ›´æ·±å±‚æ¬¡çš„å·ç§¯ï¼Œè§ï¼šIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«å¤§ä¼šï¼Œ2015å¹´ï¼Œé¡µ1â€“9ã€‚'
- en: '[152] K.Â He, X.Â Zhang, S.Â Ren, J.Â Sun, Deep residual learning for image recognition,
    in: IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770â€“778.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] K. He, X. Zhang, S. Ren, J. Sun, å›¾åƒè¯†åˆ«çš„æ·±åº¦æ®‹å·®å­¦ä¹ ï¼Œè§ï¼šIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«å¤§ä¼šï¼Œ2016å¹´ï¼Œé¡µ770â€“778ã€‚'
- en: '[153] J.Â Hu, L.Â Shen, G.Â Sun, Squeeze-and-excitation networks, in: IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 7132â€“7141.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] J. Hu, L. Shen, G. Sun, å‹ç¼©ä¸æ¿€åŠ±ç½‘ç»œï¼Œè§ï¼šIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«å¤§ä¼šï¼Œ2018å¹´ï¼Œé¡µ7132â€“7141ã€‚'
- en: '[154] G.Â Huang, Z.Â Liu, L.Â Van DerÂ Maaten, K.Â Q. Weinberger, Densely connected
    convolutional networks, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2017, pp. 4700â€“4708.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, ã€Šå¯†é›†è¿æ¥å·ç§¯ç½‘ç»œã€‹ï¼Œè½½äºã€ŠIEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†ã€‹ï¼Œ2017å¹´ï¼Œé¡µç 4700â€“4708ã€‚'
- en: '[155] K.Â Papineni, S.Â Roukos, T.Â Ward, W.-J. Zhu, Bleu: a method for automatic
    evaluation of machine translation, in: Proceedings of the 40th annual meeting
    of the Association for Computational Linguistics, 2002, pp. 311â€“318.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] K. Papineni, S. Roukos, T. Ward, W.-J. Zhu, ã€ŠBleu: ä¸€ç§è‡ªåŠ¨è¯„ä¼°æœºå™¨ç¿»è¯‘çš„æ–¹æ³•ã€‹ï¼Œè½½äºã€Šè®¡ç®—è¯­è¨€å­¦åä¼šç¬¬40å±Šå¹´ä¼šè®ºæ–‡é›†ã€‹ï¼Œ2002å¹´ï¼Œé¡µç 311â€“318ã€‚'
