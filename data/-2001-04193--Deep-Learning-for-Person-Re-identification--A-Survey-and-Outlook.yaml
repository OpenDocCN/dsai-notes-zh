- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 20:03:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:03:03'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2001.04193] Deep Learning for Person Re-identification: A Survey and Outlook'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2001.04193] 深度学习在行人重识别中的应用：调查与展望'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2001.04193](https://ar5iv.labs.arxiv.org/html/2001.04193)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2001.04193](https://ar5iv.labs.arxiv.org/html/2001.04193)
- en: 'Deep Learning for Person Re-identification:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在行人重识别中的应用：
- en: A Survey and Outlook
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 调查与展望
- en: Mang Ye, Jianbing Shen, , Gaojie Lin, Tao Xiang
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang
- en: 'Ling Shao and Steven C. H. Hoi M. Ye is with the School of Computer Science,
    Wuhan University, China and Inception Institute of Artificial Intelligence, UAE.
    J. Shen and L. Shao are with the Inception Institute of Artificial Intelligence,
    UAE. E-mail:{mangye16, shenjianbingcg}@gmail.com G. Lin is with the School of
    Computer Science, Beijing Institute of Technology, China. T. Xiang is with the
    Centre for Vision Speech and Signal Processing, University of Surrey, UK. Email:
    t.xiang@surrey.ac.uk S. C. H. Hoi is with the Singapore Management University,
    and Salesforce Research Asia, Singapore. Email: stevenhoi@gmail.com'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Ling Shao 和 Steven C. H. Hoi M. Ye 任职于中国武汉大学计算机科学学院和阿联酋 Inception 人工智能研究所。J.
    Shen 和 L. Shao 任职于阿联酋 Inception 人工智能研究所。电子邮件：{mangye16, shenjianbingcg}@gmail.com。G.
    Lin 任职于中国北京理工大学计算机科学学院。T. Xiang 任职于英国萨里大学视觉、语音与信号处理中心。电子邮件：t.xiang@surrey.ac.uk。S.
    C. H. Hoi 任职于新加坡管理大学和 Salesforce Research Asia。电子邮件：stevenhoi@gmail.com
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Person re-identification (Re-ID) aims at retrieving a person of interest across
    multiple non-overlapping cameras. With the advancement of deep neural networks
    and increasing demand of intelligent video surveillance, it has gained significantly
    increased interest in the computer vision community. By dissecting the involved
    components in developing a person Re-ID system, we categorize it into the closed-world
    and open-world settings. The widely studied closed-world setting is usually applied
    under various research-oriented assumptions, and has achieved inspiring success
    using deep learning techniques on a number of datasets. We first conduct a comprehensive
    overview with in-depth analysis for closed-world person Re-ID from three different
    perspectives, including deep feature representation learning, deep metric learning
    and ranking optimization. With the performance saturation under closed-world setting,
    the research focus for person Re-ID has recently shifted to the open-world setting,
    facing more challenging issues. This setting is closer to practical applications
    under specific scenarios. We summarize the open-world Re-ID in terms of five different
    aspects. By analyzing the advantages of existing methods, we design a powerful
    AGW baseline, achieving state-of-the-art or at least comparable performance on
    twelve datasets for FOUR different Re-ID tasks. Meanwhile, we introduce a new
    evaluation metric (mINP) for person Re-ID, indicating the cost for finding all
    the correct matches, which provides an additional criteria to evaluate the Re-ID
    system for real applications. Finally, some important yet under-investigated open
    issues are discussed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 行人重识别（Re-ID）的目标是从多个不重叠的摄像头中检索感兴趣的人。随着深度神经网络的发展和智能视频监控需求的增加，它在计算机视觉领域获得了显著的关注。通过分析开发行人
    Re-ID 系统中涉及的组件，我们将其分为封闭世界和开放世界设置。广泛研究的封闭世界设置通常在各种研究导向的假设下应用，并且在多个数据集上使用深度学习技术取得了令人鼓舞的成功。我们首先从深度特征表示学习、深度度量学习和排序优化三个不同的角度，对封闭世界行人
    Re-ID 进行全面的综述和深入分析。随着封闭世界设置下性能的饱和，行人 Re-ID 的研究重点最近转向了开放世界设置，面临更具挑战性的问题。该设置更接近于特定场景下的实际应用。我们从五个不同的方面总结了开放世界
    Re-ID。通过分析现有方法的优点，我们设计了一个强大的 AGW 基线，在四种不同的 Re-ID 任务上在十二个数据集上实现了最先进或至少可比的性能。同时，我们引入了一种新的评估指标（mINP），用于行人
    Re-ID，表示找到所有正确匹配的成本，为实际应用中的 Re-ID 系统评估提供了额外的标准。最后，我们讨论了一些重要但尚未深入研究的开放问题。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '索引词:'
- en: Person Re-Identification, Pedestrian Retrieval, Literature Survey, Evaluation
    Metric, Deep Learning
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 行人重识别、行人检索、文献综述、评估指标、深度学习
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Person re-identification (Re-ID) has been widely studied as a specific person
    retrieval problem across non-overlapping cameras [[1](#bib.bib1), [2](#bib.bib2)].
    Given a query person-of-interest, the goal of Re-ID is to determine whether this
    person has appeared in another place at a distinct time captured by a different
    camera, or even the same camera at a different time instant [[3](#bib.bib3)].
    The query person can be represented by an image [[4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6)], a video sequence [[7](#bib.bib7), [8](#bib.bib8)], and even a
    text description [[9](#bib.bib9), [10](#bib.bib10)]. Due to the urgent demand
    of public safety and increasing number of surveillance cameras, person Re-ID is
    imperative in intelligent surveillance systems with significant research impact
    and practical importance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 行人再识别（Re-ID）作为跨越非重叠摄像头的特定人物检索问题已被广泛研究 [[1](#bib.bib1), [2](#bib.bib2)]。给定一个查询的关注人物，Re-ID
    的目标是确定这个人是否在另一个地方的不同时间被另一台摄像头捕捉到，甚至是同一台摄像头在不同时间点 [[3](#bib.bib3)]。查询人物可以通过图像 [[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)]、视频序列 [[7](#bib.bib7), [8](#bib.bib8)]，甚至文本描述 [[9](#bib.bib9),
    [10](#bib.bib10)] 来表示。由于公共安全的紧迫需求和监控摄像头数量的增加，行人 Re-ID 在智能监控系统中具有重要的研究影响和实际意义。
- en: Re-ID is a challenging task due to the presence of different viewpoints [[11](#bib.bib11),
    [12](#bib.bib12)], varying low-image resolutions [[13](#bib.bib13), [14](#bib.bib14)],
    illumination changes [[15](#bib.bib15)], unconstrained poses [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18)], occlusions [[19](#bib.bib19), [20](#bib.bib20)],
    heterogeneous modalities [[21](#bib.bib21), [10](#bib.bib10)], complex camera
    environments, background clutter [[22](#bib.bib22)], unreliable bounding box generations,
    etc. These result in varying variations and uncertainty. In addition, for practical
    model deployment, the dynamic updated camera network [[23](#bib.bib23), [24](#bib.bib24)],
    large scale gallery with efficient retrieval [[25](#bib.bib25)], group uncertainty
    [[26](#bib.bib26)], significant domain shift [[27](#bib.bib27)], unseen testing
    scenarios [[28](#bib.bib28)], incremental model updating [[29](#bib.bib29)] and
    changing cloths [[30](#bib.bib30)] also greatly increase the difficulties. These
    challenges lead that Re-ID is still unsolved problem. Early research efforts mainly
    focus on the hand-crafted feature construction with body structures [[31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)] or distance
    metric learning [[36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41)]. With the advancement of deep learning, person
    Re-ID has achieved inspiring performance on the widely used benchmarks [[5](#bib.bib5),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]. However, there is still
    a large gap between the research-oriented scenarios and practical applications
    [[45](#bib.bib45)]. This motivates us to conduct a comprehensive survey, develop
    a powerful baseline for different Re-ID tasks and discuss several future directions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Re-ID 是一项具有挑战性的任务，因为存在不同的视角 [[11](#bib.bib11), [12](#bib.bib12)]、不同的低图像分辨率 [[13](#bib.bib13),
    [14](#bib.bib14)]、光照变化 [[15](#bib.bib15)]、无约束的姿势 [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18)]、遮挡 [[19](#bib.bib19), [20](#bib.bib20)]、异构模态 [[21](#bib.bib21),
    [10](#bib.bib10)]、复杂的相机环境、背景杂乱 [[22](#bib.bib22)]、不可靠的边界框生成等。这些因素导致了不同的变化和不确定性。此外，对于实际模型部署，动态更新的相机网络
    [[23](#bib.bib23), [24](#bib.bib24)]、大规模画廊与高效检索 [[25](#bib.bib25)]、群体不确定性 [[26](#bib.bib26)]、显著的领域偏移
    [[27](#bib.bib27)]、未见过的测试场景 [[28](#bib.bib28)]、增量模型更新 [[29](#bib.bib29)] 和变化的服装
    [[30](#bib.bib30)] 也大大增加了难度。这些挑战使得 Re-ID 仍然是一个未解决的问题。早期的研究工作主要集中在使用体结构的手工特征构造
    [[31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)]
    或距离度量学习 [[36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41)] 上。随着深度学习的进步，行人 Re-ID 在广泛使用的基准测试中取得了令人鼓舞的表现
    [[5](#bib.bib5), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]。然而，研究导向的场景与实际应用之间仍存在较大差距
    [[45](#bib.bib45)]。这激励我们进行全面的调查，开发适用于不同 Re-ID 任务的强大基准，并讨论未来的几个方向。
- en: 'Though some surveys have also summarized the deep learning techniques [[2](#bib.bib2),
    [46](#bib.bib46), [47](#bib.bib47)], our survey makes three major differences:
    1) We provide an in-depth and comprehensive analysis of existing deep learning
    methods by discussing their advantages and limitations, analyzing the state-of-the-arts.
    This provides insights for future algorithm design and new topic exploration.
    2) We design a new powerful baseline (AGW: Attention Generalized mean pooling
    with Weighted triplet loss) and a new evaluation metric (mINP: mean Inverse Negative
    Penalty) for future developments. AGW achieves state-of-the-art performance on
    twelve datasets for four different Re-ID tasks. mINP provides a supplement metric
    to existing CMC/mAP, indicating the cost to find all the correct matches. 3) We
    make an attempt to discuss several important research directions with under-investigated
    open issues to narrow the gap between the closed-world and open-world applications,
    taking a step towards real-world Re-ID system design.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些调查也总结了深度学习技术[[2](#bib.bib2)、[46](#bib.bib46)、[47](#bib.bib47)]，我们的调查有三大不同点：1）我们通过讨论现有深度学习方法的优缺点，分析最前沿技术，对其进行了深入全面的分析。这为未来算法设计和新话题探索提供了见解。2）我们设计了一个新的强大基准（AGW：注意力广义均值池化与加权三元组损失）和一个新的评估指标（mINP：均值逆负惩罚）用于未来的发展。AGW在四个不同的重识别任务的十二个数据集上达到了最先进的性能。mINP提供了现有CMC/mAP的补充指标，指示了找到所有正确匹配的成本。3）我们尝试讨论几个重要的研究方向及未充分研究的开放问题，以缩小封闭世界和开放世界应用之间的差距，向现实世界的重识别系统设计迈进。
- en: '![Refer to caption](img/daa4e3c78927727058e69508c9d6f859.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/daa4e3c78927727058e69508c9d6f859.png)'
- en: 'Figure 1: The flow of designing a practical person Re-ID system, including
    five main steps: 1) Raw Data Collection, (2) Bounding Box Generation, 3) Training
    Data Annotation, 4) Model Training and 5) Pedestrian Retrieval.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：设计实用行人重识别系统的流程，包括五个主要步骤：1）原始数据收集，（2）边界框生成，3）训练数据标注，4）模型训练和5）行人检索。
- en: 'Unless otherwise specified, person Re-ID in this survey refers to the pedestrian
    retrieval problem across multiple surveillance cameras, from a computer vision
    perspective. Generally, building a person Re-ID system for a specific scenario
    requires five main steps (as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook")):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，本调查中的行人重识别指的是从计算机视觉的角度考虑的多摄像头下的行人检索问题。通常，为特定场景构建行人重识别系统需要五个主要步骤（如图[1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ 行人重识别的深度学习：调查与展望")所示）：
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Step 1: Raw Data Collection: Obtaining raw video data from surveillance cameras
    is the primary requirement of practical video investigation. These cameras are
    usually located in different places under varying environments [[48](#bib.bib48)].
    Most likely, this raw data contains a large amount of complex and noisy background
    clutter.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤1：原始数据收集：从监控摄像头获取原始视频数据是实际视频调查的主要要求。这些摄像头通常位于不同的地点，环境各异[[48](#bib.bib48)]。这些原始数据很可能包含大量复杂且嘈杂的背景杂乱。
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Step 2: Bounding Box Generation: Extracting the bounding boxes which contain
    the person images from the raw video data. Generally, it is impossible to manually
    crop all the person images in large-scale applications. The bounding boxes are
    usually obtained by the person detection [[49](#bib.bib49), [50](#bib.bib50)]
    or tracking algorithms [[51](#bib.bib51), [52](#bib.bib52)].'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤2：边界框生成：从原始视频数据中提取包含行人图像的边界框。通常，在大规模应用中，人工裁剪所有行人图像是不可能的。边界框通常通过行人检测[[49](#bib.bib49)、[50](#bib.bib50)]或跟踪算法[[51](#bib.bib51)、[52](#bib.bib52)]获得。
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Step 3: Training Data Annotation: Annotating the cross-camera labels. Training
    data annotation is usually indispensable for discriminative Re-ID model learning
    due to the large cross-camera variations. In the existence of large domain shift
    [[53](#bib.bib53)], we often need to annotate the training data in every new scenario.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤3：训练数据标注：标注跨摄像头标签。由于跨摄像头的变异，训练数据标注通常是判别性重识别模型学习的不可或缺部分。在存在大量领域迁移[[53](#bib.bib53)]的情况下，我们通常需要在每个新场景中标注训练数据。
- en: '4.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Step 4: Model Training: Training a discriminative and robust Re-ID model with
    the previous annotated person images/videos. This step is the core for developing
    a Re-ID system and it is also the most widely studied paradigm in the literature.
    Extensive models have been developed to handle the various challenges, concentrating
    on feature representation learning [[54](#bib.bib54), [55](#bib.bib55)], distance
    metric learning [[56](#bib.bib56), [57](#bib.bib57)] or their combinations.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4步：模型训练：使用之前标注的人物图像/视频训练一个具有区分能力和鲁棒性的 Re-ID 模型。这一步是开发 Re-ID 系统的核心，也是文献中研究最广泛的范式。为应对各种挑战，已经开发了大量模型，集中在特征表示学习
    [[54](#bib.bib54), [55](#bib.bib55)]、距离度量学习 [[56](#bib.bib56), [57](#bib.bib57)]
    或其组合上。
- en: '5.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Step 5: Pedestrian Retrieval: The testing phase conducts the pedestrian retrieval.
    Given a person-of-interest (query) and a gallery set, we extract the feature representations
    using the Re-ID model learned in previous stage. A retrieved ranking list is obtained
    by sorting the calculated query-to-gallery similarity. Some methods have also
    investigated the ranking optimization to improve the retrieval performance [[58](#bib.bib58),
    [59](#bib.bib59)].'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第5步：行人检索：测试阶段进行行人检索。给定一个感兴趣的人（查询）和一个图库集合，我们使用前一阶段学习的 Re-ID 模型提取特征表示。通过对计算出的查询与图库相似度进行排序，获得检索排名列表。一些方法还研究了排名优化以提高检索性能
    [[58](#bib.bib58), [59](#bib.bib59)]。
- en: 'TABLE I: Closed-world *vs*. Open-world Person Re-ID.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：封闭世界 *vs*. 开放世界人物 Re-ID。
- en: '| Closed-world (Section [2](#S2 "2 Closed-world Person Re-Identification ‣
    Deep Learning for Person Re-identification: A Survey and Outlook")) | Open-world (Section
    [3](#S3 "3 Open-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 封闭世界 (第 [2](#S2 "2 封闭世界人物再识别 ‣ 深度学习的人物再识别：调查与展望") 节) | 开放世界 (第 [3](#S3 "3
    开放世界人物再识别 ‣ 深度学习的人物再识别：调查与展望") 节) |'
- en: '| --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $\checkmark$ Single-modality Data | Heterogeneous Data (§ [3.1](#S3.SS1 "3.1
    Heterogeneous Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep Learning for
    Person Re-identification: A Survey and Outlook")) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ 单模态数据 | 异质数据 (§ [3.1](#S3.SS1 "3.1 异质 Re-ID ‣ 3 开放世界人物再识别 ‣
    深度学习的人物再识别：调查与展望")) |'
- en: '| $\checkmark$ Bounding Boxes Generation | Raw Images/Videos (§ [3.2](#S3.SS2
    "3.2 End-to-End Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ 边界框生成 | 原始图像/视频 (§ [3.2](#S3.SS2 "3.2 端到端 Re-ID ‣ 3 开放世界人物再识别
    ‣ 深度学习的人物再识别：调查与展望")) |'
- en: '| $\checkmark$ Sufficient Annotated Data | Unavailable/Limited Labels (§ [3.3](#S3.SS3
    "3.3 Semi-supervised and Unsupervised Re-ID ‣ 3 Open-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook")) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ 足够的标注数据 | 不可用/有限标签 (§ [3.3](#S3.SS3 "3.3 半监督和无监督 Re-ID ‣ 3 开放世界人物再识别
    ‣ 深度学习的人物再识别：调查与展望")) |'
- en: '| $\checkmark$ Correct Annotation | Noisy Annotation (§ [3.4](#S3.SS4 "3.4
    Noise-Robust Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep Learning for
    Person Re-identification: A Survey and Outlook")) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ 正确标注 | 噪声标注 (§ [3.4](#S3.SS4 "3.4 噪声鲁棒 Re-ID ‣ 3 开放世界人物再识别 ‣
    深度学习的人物再识别：调查与展望")) |'
- en: '| $\checkmark$ Query Exists in Gallery | Open-set (§ [3.5](#S3.SS5 "3.5 Open-set
    Re-ID and Beyond ‣ 3 Open-world Person Re-Identification ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook")) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ 查询存在于图库中 | 开放集 (§ [3.5](#S3.SS5 "3.5 开放集 Re-ID 及其他 ‣ 3 开放世界人物再识别
    ‣ 深度学习的人物再识别：调查与展望")) |'
- en: 'According to the five steps mentioned above, we categorize existing Re-ID methods
    into two main trends: closed-world and open-world settings, as summarized in Table
    [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook"). A step-by-step comparison is in the following five aspects:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述五个步骤，我们将现有的 Re-ID 方法分为两大趋势：封闭世界和开放世界设置，如表 [I](#S1.T1 "TABLE I ‣ 1 引言 ‣ 深度学习的人物再识别：调查与展望")
    所总结的。以下是这五个方面的逐步比较：
- en: '1.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Single-modality *vs*. Heterogeneous Data: For the raw data collection in Step
    [1](#S1.I1.i1 "item 1 ‣ 1 Introduction ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook"), all the persons are represented by images/videos captured
    by single-modality visible cameras in the closed-world setting [[5](#bib.bib5),
    [8](#bib.bib8), [42](#bib.bib42), [43](#bib.bib43), [31](#bib.bib31), [44](#bib.bib44)].
    However, in practical open-world applications, we might also need to process heterogeneous
    data, which are infrared images [[21](#bib.bib21), [60](#bib.bib60)], sketches
    [[61](#bib.bib61)], depth images [[62](#bib.bib62)], or even text descriptions
    [[63](#bib.bib63)]. This motivates the heterogeneous Re-ID in § [3.1](#S3.SS1
    "3.1 Heterogeneous Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook").'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '单模态 *vs*. 异构数据：在步骤[1](#S1.I1.i1 "item 1 ‣ 1 Introduction ‣ Deep Learning for
    Person Re-identification: A Survey and Outlook")中的原始数据收集中，所有人员都由在封闭世界设置下通过单模态可见光摄像头捕获的图像/视频表示[[5](#bib.bib5),
    [8](#bib.bib8), [42](#bib.bib42), [43](#bib.bib43), [31](#bib.bib31), [44](#bib.bib44)]。然而，在实际的开放世界应用中，我们可能还需要处理异构数据，例如红外图像[[21](#bib.bib21),
    [60](#bib.bib60)]、草图[[61](#bib.bib61)]、深度图像[[62](#bib.bib62)]，甚至是文本描述[[63](#bib.bib63)]。这促使了在§ [3.1](#S3.SS1
    "3.1 Heterogeneous Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")中对异构重识别的研究。'
- en: '2.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Bounding Box Generation *vs*. Raw Images/Videos : For the bounding box generation
    in Step [2](#S1.I1.i2 "item 2 ‣ 1 Introduction ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook"), the closed-world person Re-ID usually performs the training
    and testing based on the generated bounding boxes, where the bounding boxes mainly
    contain the person appearance information. In contrast, some practical open-world
    applications require end-to-end person search from the raw images or videos [[55](#bib.bib55),
    [64](#bib.bib64)]. This leads to another open-world topic, *i.e*., end-to-end
    person search in § [3.2](#S3.SS2 "3.2 End-to-End Re-ID ‣ 3 Open-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook").'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '边界框生成 *vs*. 原始图像/视频：在步骤[2](#S1.I1.i2 "item 2 ‣ 1 Introduction ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")中的边界框生成中，封闭世界的人员重识别通常基于生成的边界框进行训练和测试，这些边界框主要包含人员的外观信息。相比之下，一些实际的开放世界应用需要从原始图像或视频中进行端到端的人员搜索[[55](#bib.bib55),
    [64](#bib.bib64)]。这引出了另一个开放世界主题，*即*，在§ [3.2](#S3.SS2 "3.2 End-to-End Re-ID ‣ 3
    Open-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")中的端到端人员搜索。'
- en: '3.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Sufficient Annotated Data *vs*. Unavailable/Limited Labels: For the training
    data annotation in Step [3](#S1.I1.i3 "item 3 ‣ 1 Introduction ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook"), the closed-world person
    Re-ID usually assumes that we have enough annotated training data for supervised
    Re-ID model training. However, label annotation for each camera pair in every
    new environment is time consuming and labor intensive, incurring high costs. In
    open-world scenarios, we might not have enough annotated data (*i.e*., limited
    labels) [[65](#bib.bib65)] or even without any label information [[66](#bib.bib66)].
    This inspires the discussion of the unsupervised and semi-supervised Re-ID in
    § [3.3](#S3.SS3 "3.3 Semi-supervised and Unsupervised Re-ID ‣ 3 Open-world Person
    Re-Identification ‣ Deep Learning for Person Re-identification: A Survey and Outlook").'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '足够的标注数据 *vs*. 不可用/有限的标签：在步骤[3](#S1.I1.i3 "item 3 ‣ 1 Introduction ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")中的训练数据标注中，封闭世界的人员重识别通常假设我们有足够的标注训练数据用于监督的重识别模型训练。然而，每个新环境中每对摄像头的标签标注既耗时又费力，成本高昂。在开放世界场景中，我们可能没有足够的标注数据（*即*，有限标签）[[65](#bib.bib65)]，甚至可能没有任何标签信息[[66](#bib.bib66)]。这引发了在§ [3.3](#S3.SS3
    "3.3 Semi-supervised and Unsupervised Re-ID ‣ 3 Open-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook")中对无监督和半监督重识别的讨论。'
- en: '4.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Correct Annotation *vs*. Noisy Annotation: For Step [4](#S1.I1.i4 "item 4 ‣
    1 Introduction ‣ Deep Learning for Person Re-identification: A Survey and Outlook"),
    existing closed-world person Re-ID systems usually assume that all the annotations
    are correct, with clean labels. However, annotation noise is usually unavoidable
    due to annotation error (*i.e*., label noise) or imperfect detection/tracking
    results (*i.e*., sample noise, partial Re-ID [[67](#bib.bib67)]). This leads to
    the analysis of noise-robust person Re-ID under different noise types in § [3.4](#S3.SS4
    "3.4 Noise-Robust Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook").'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '正确标注 *vs*. 噪声标注：对于步骤 [4](#S1.I1.i4 "item 4 ‣ 1 Introduction ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")，现有的封闭世界行人重识别系统通常假设所有的标注都是正确的，且标签干净。然而，由于标注错误（*即*，标签噪声）或检测/跟踪结果的不完美（*即*，样本噪声，部分
    Re-ID [[67](#bib.bib67)]），标注噪声通常是不可避免的。这导致了在 § [3.4](#S3.SS4 "3.4 Noise-Robust
    Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook") 中对不同噪声类型下的噪声鲁棒性行人重识别的分析。'
- en: '5.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Query Exists in Gallery *vs*. Open-set: In the pedestrian retrieval stage (Step
    [5](#S1.I1.i5 "item 5 ‣ 1 Introduction ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")), most existing closed-world person Re-ID works assume
    that the query must occur in the gallery set by calculating the CMC [[68](#bib.bib68)]
    and mAP [[5](#bib.bib5)]. However, in many scenarios, the query person may not
    appear in the gallery set [[69](#bib.bib69), [70](#bib.bib70)], or we need to
    perform the verification rather than retrieval [[26](#bib.bib26)]. This brings
    us to the open-set person Re-ID in § [3.5](#S3.SS5 "3.5 Open-set Re-ID and Beyond
    ‣ 3 Open-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook").'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '查询存在于图库 *vs*. 开集：在行人检索阶段（步骤 [5](#S1.I1.i5 "item 5 ‣ 1 Introduction ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")），大多数现有的封闭世界行人重识别工作假设查询必须出现在图库中，通过计算
    CMC [[68](#bib.bib68)] 和 mAP [[5](#bib.bib5)]。然而，在许多场景中，查询的人可能不出现在图库中 [[69](#bib.bib69),
    [70](#bib.bib70)]，或者我们需要执行验证而不是检索 [[26](#bib.bib26)]。这将我们引向在 § [3.5](#S3.SS5 "3.5
    Open-set Re-ID and Beyond ‣ 3 Open-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook") 中的开集行人重识别。'
- en: 'This survey first introduces the widely studied person Re-ID under closed-world
    settings in § [2](#S2 "2 Closed-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook"). A detailed review on the
    datasets and the state-of-the-arts are conducted in § [2.4](#S2.SS4 "2.4 Datasets
    and Evaluation ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook"). We then introduce the open-world person
    Re-ID in § [3](#S3 "3 Open-world Person Re-Identification ‣ Deep Learning for
    Person Re-identification: A Survey and Outlook"). An outlook for future Re-ID
    is presented in § [4](#S4 "4 An Outlook: Re-ID in Next Era ‣ Deep Learning for
    Person Re-identification: A Survey and Outlook"), including a new evaluation metric
    (§ [4.1](#S4.SS1 "4.1 mINP: A New Evaluation Metric for Re-ID ‣ 4 An Outlook:
    Re-ID in Next Era ‣ Deep Learning for Person Re-identification: A Survey and Outlook")),
    a new powerful AGW baseline (§ [4.2](#S4.SS2 "4.2 A New Baseline for Single-/Cross-Modality
    Re-ID ‣ 4 An Outlook: Re-ID in Next Era ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")). We discuss several under-investigated open issues for
    future study (§ [4.3](#S4.SS3 "4.3 Under-Investigated Open Issues ‣ 4 An Outlook:
    Re-ID in Next Era ‣ Deep Learning for Person Re-identification: A Survey and Outlook")).
    Conclusions will be drawn in § [5](#S5 "5 Concluding Remarks ‣ Deep Learning for
    Person Re-identification: A Survey and Outlook"). A structure overview is shown
    in the supplementary.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查首先介绍了在封闭世界设置下广泛研究的人物 Re-ID，详见 § [2](#S2 "2 封闭世界人物再识别 ‣ 人物再识别的深度学习：调查与展望")。有关数据集和最新技术的详细评述见
    § [2.4](#S2.SS4 "2.4 数据集与评估 ‣ 2 封闭世界人物再识别 ‣ 人物再识别的深度学习：调查与展望")。随后，我们在 § [3](#S3
    "3 开放世界人物再识别 ‣ 人物再识别的深度学习：调查与展望") 中介绍了开放世界人物 Re-ID。关于未来 Re-ID 的展望见 § [4](#S4 "4
    展望：下一时代的 Re-ID ‣ 人物再识别的深度学习：调查与展望")，其中包括新的评估指标 (§ [4.1](#S4.SS1 "4.1 mINP：一种新的
    Re-ID 评估指标 ‣ 4 展望：下一时代的 Re-ID ‣ 人物再识别的深度学习：调查与展望") ) 和新的强大 AGW 基线 (§ [4.2](#S4.SS2
    "4.2 单模态/跨模态 Re-ID 的新基线 ‣ 4 展望：下一时代的 Re-ID ‣ 人物再识别的深度学习：调查与展望") )。我们讨论了若干尚未深入研究的开放问题，供未来研究参考
    (§ [4.3](#S4.SS3 "4.3 尚未深入研究的开放问题 ‣ 4 展望：下一时代的 Re-ID ‣ 人物再识别的深度学习：调查与展望") )。结论将在
    § [5](#S5 "5 结论性评论 ‣ 人物再识别的深度学习：调查与展望") 中得出。结构概述见附录。
- en: '![Refer to caption](img/b55f6db871140ccfa530788c5048bc5b.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b55f6db871140ccfa530788c5048bc5b.png)'
- en: 'Figure 2: Four different feature learning strategies. a) Global Feature, learning
    a global representation for each person image in § [2.1.1](#S2.SS1.SSS1 "2.1.1
    Global Feature Representation Learning ‣ 2.1 Feature Representation Learning ‣
    2 Closed-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook"); b) Local Feature, learning part-aggregated local features
    in § [2.1.2](#S2.SS1.SSS2 "2.1.2 Local Feature Representation Learning ‣ 2.1 Feature
    Representation Learning ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook"); c) Auxiliary Feature, learning
    the feature representation using auxiliary information, *e.g*., attributes [[71](#bib.bib71),
    [72](#bib.bib72)] in § [2.1.3](#S2.SS1.SSS3 "2.1.3 Auxiliary Feature Representation
    Learning ‣ 2.1 Feature Representation Learning ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook") and d) Video
    Feature , learning the video representation using multiple image frames and temporal
    information [[73](#bib.bib73), [74](#bib.bib74)] in § [2.1.4](#S2.SS1.SSS4 "2.1.4
    Video Feature Representation Learning ‣ 2.1 Feature Representation Learning ‣
    2 Closed-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 四种不同的特征学习策略。a) 全局特征，在§ [2.1.1](#S2.SS1.SSS1 "2.1.1 Global Feature Representation
    Learning ‣ 2.1 Feature Representation Learning ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook")中为每个人的图像学习全局表示；b)
    局部特征，在§ [2.1.2](#S2.SS1.SSS2 "2.1.2 Local Feature Representation Learning ‣ 2.1
    Feature Representation Learning ‣ 2 Closed-world Person Re-Identification ‣ Deep
    Learning for Person Re-identification: A Survey and Outlook")中学习部分聚合的局部特征；c) 辅助特征，使用辅助信息学习特征表示，如在§ [2.1.3](#S2.SS1.SSS3
    "2.1.3 Auxiliary Feature Representation Learning ‣ 2.1 Feature Representation
    Learning ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook")中提到的属性[[71](#bib.bib71), [72](#bib.bib72)]；d)
    视频特征，使用多个图像帧和时间信息学习视频表示，如在§ [2.1.4](#S2.SS1.SSS4 "2.1.4 Video Feature Representation
    Learning ‣ 2.1 Feature Representation Learning ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook")中提到的[[73](#bib.bib73),
    [74](#bib.bib74)]。'
- en: 2 Closed-world Person Re-Identification
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 关闭世界中的人物重识别
- en: 'This section provides an overview for closed-world person Re-ID. As discussed
    in § [1](#S1 "1 Introduction ‣ Deep Learning for Person Re-identification: A Survey
    and Outlook"), this setting usually has the following assumptions: 1) person appearances
    are captured by single-modality visible cameras, either by image or video; 2)
    The persons are represented by bounding boxes, where most of the bounding box
    area belongs the same identity; 3) The training has enough annotated training
    data for supervised discriminative Re-ID model learning; 4) The annotations are
    generally correct; 5) The query person must appear in the gallery set. Typically,
    a standard closed-world Re-ID system contains three main components: Feature Representation
    Learning (§ [2.1](#S2.SS1 "2.1 Feature Representation Learning ‣ 2 Closed-world
    Person Re-Identification ‣ Deep Learning for Person Re-identification: A Survey
    and Outlook")), which focuses on developing the feature construction strategies;
    Deep Metric Learning (§ [2.2](#S2.SS2 "2.2 Deep Metric Learning ‣ 2 Closed-world
    Person Re-Identification ‣ Deep Learning for Person Re-identification: A Survey
    and Outlook")), which aims at designing the training objectives with different
    loss functions or sampling strategies; and Ranking Optimization (§ [2.3](#S2.SS3
    "2.3 Ranking Optimization ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")), which concentrates on optimizing
    the retrieved ranking list. An overview of the datasets and state-of-the-arts
    with in-depth analysis is provided in § [2.4.2](#S2.SS4.SSS2 "2.4.2 In-depth Analysis
    on State-of-The-Arts ‣ 2.4 Datasets and Evaluation ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '本节概述了封闭世界下的人体再识别。正如在§ [1](#S1 "1 Introduction ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")中讨论的，这种设置通常有以下假设：1) 人体的外观由单模态可见摄像头捕获，可以是图像或视频；2) 人体通过边界框表示，其中大部分边界框区域属于同一身份；3)
    训练有足够的标注数据用于监督区分性再识别模型的学习；4) 注释通常是正确的；5) 查询的人物必须出现在图库集中。通常，一个标准的封闭世界再识别系统包含三个主要组件：特征表示学习
    (§ [2.1](#S2.SS1 "2.1 Feature Representation Learning ‣ 2 Closed-world Person
    Re-Identification ‣ Deep Learning for Person Re-identification: A Survey and Outlook"))，侧重于开发特征构建策略；深度度量学习
    (§ [2.2](#S2.SS2 "2.2 Deep Metric Learning ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook"))，旨在设计具有不同损失函数或采样策略的训练目标；以及排名优化
    (§ [2.3](#S2.SS3 "2.3 Ranking Optimization ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook"))，专注于优化检索排名列表。有关数据集和最前沿技术的概述及深入分析见§ [2.4.2](#S2.SS4.SSS2
    "2.4.2 In-depth Analysis on State-of-The-Arts ‣ 2.4 Datasets and Evaluation ‣
    2 Closed-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")。'
- en: 2.1 Feature Representation Learning
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 特征表示学习
- en: 'We firstly discuss the feature learning strategies in closed-world person Re-ID.
    There are four main categories (as shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook")): a) Global
    Feature (§ [2.1.1](#S2.SS1.SSS1 "2.1.1 Global Feature Representation Learning
    ‣ 2.1 Feature Representation Learning ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook")), it extracts
    a global feature representation vector for each person image without additional
    annotation cues [[55](#bib.bib55)]; b) Local Feature (§ [2.1.2](#S2.SS1.SSS2 "2.1.2
    Local Feature Representation Learning ‣ 2.1 Feature Representation Learning ‣
    2 Closed-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")), it aggregates part-level local features to formulate
    a combined representation for each person image [[75](#bib.bib75), [76](#bib.bib76),
    [77](#bib.bib77)]; c) Auxiliary Feature (§ [2.1.3](#S2.SS1.SSS3 "2.1.3 Auxiliary
    Feature Representation Learning ‣ 2.1 Feature Representation Learning ‣ 2 Closed-world
    Person Re-Identification ‣ Deep Learning for Person Re-identification: A Survey
    and Outlook")), it improves the feature representation learning using auxiliary
    information, *e.g*., attributes [[71](#bib.bib71), [72](#bib.bib72), [78](#bib.bib78)],
    GAN generated images [[42](#bib.bib42)], etc. d) Video Feature (§ [2.1.4](#S2.SS1.SSS4
    "2.1.4 Video Feature Representation Learning ‣ 2.1 Feature Representation Learning
    ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")), it learns video representation for video-based Re-ID
    [[7](#bib.bib7)] using multiple image frames and temporal information [[73](#bib.bib73),
    [74](#bib.bib74)]. We also review several specific architecture designs for person
    Re-ID in § [2.1.5](#S2.SS1.SSS5 "2.1.5 Architecture Design ‣ 2.1 Feature Representation
    Learning ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先讨论封闭世界下行人再识别中的特征学习策略。主要有四种类别（如图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣
    Deep Learning for Person Re-identification: A Survey and Outlook")所示）：a) 全局特征
    (§ [2.1.1](#S2.SS1.SSS1 "2.1.1 Global Feature Representation Learning ‣ 2.1 Feature
    Representation Learning ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook"))，它为每个人图像提取一个全局特征表示向量，无需额外的注释线索
    [[55](#bib.bib55)]；b) 局部特征 (§ [2.1.2](#S2.SS1.SSS2 "2.1.2 Local Feature Representation
    Learning ‣ 2.1 Feature Representation Learning ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook"))，它将部分级局部特征聚合起来，形成每个人图像的组合表示
    [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77)]；c) 辅助特征 (§ [2.1.3](#S2.SS1.SSS3
    "2.1.3 Auxiliary Feature Representation Learning ‣ 2.1 Feature Representation
    Learning ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook"))，它通过使用辅助信息来提高特征表示学习，例如，属性 [[71](#bib.bib71),
    [72](#bib.bib72), [78](#bib.bib78)]，GAN 生成的图像 [[42](#bib.bib42)]，等等；d) 视频特征 (§ [2.1.4](#S2.SS1.SSS4
    "2.1.4 Video Feature Representation Learning ‣ 2.1 Feature Representation Learning
    ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook"))，它利用多个图像帧和时间信息来学习视频表示，用于基于视频的再识别 [[7](#bib.bib7)] [[73](#bib.bib73),
    [74](#bib.bib74)]。我们还在 § [2.1.5](#S2.SS1.SSS5 "2.1.5 Architecture Design ‣ 2.1
    Feature Representation Learning ‣ 2 Closed-world Person Re-Identification ‣ Deep
    Learning for Person Re-identification: A Survey and Outlook") 回顾了几种针对行人再识别的具体架构设计。'
- en: 2.1.1 Global Feature Representation Learning
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 全局特征表示学习
- en: 'Global feature representation learning extracts a global feature vector for
    each person image, as shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep
    Learning for Person Re-identification: A Survey and Outlook")(a). Since deep neural
    networks are originally applied in image classification [[79](#bib.bib79), [80](#bib.bib80)],
    global feature learning is the primary choice when integrating advanced deep learning
    techniques into the person Re-ID field in early years.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '全局特征表示学习为每个人图像提取全局特征向量，如图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")(a)所示。由于深度神经网络最初应用于图像分类 [[79](#bib.bib79),
    [80](#bib.bib80)]，全局特征学习是早期将先进深度学习技术融入行人再识别领域的主要选择。'
- en: To capture the fine-grained cues in global feature learning, A joint learning
    framework consisting of a single-image representation (SIR) and cross-image representation
    (CIR) is developed in [[81](#bib.bib81)], trained with triplet loss using specific
    sub-networks. The widely-used ID-discriminative Embedding (IDE) model [[55](#bib.bib55)]
    constructs the training process as a multi-class classification problem by treating
    each identity as a distinct class. It is now widely used in Re-ID community [[42](#bib.bib42),
    [82](#bib.bib82), [58](#bib.bib58), [77](#bib.bib77), [83](#bib.bib83)]. Qian
    et al. [[84](#bib.bib84)] develop a multi-scale deep representation learning model
    to capture discriminative cues at different scales.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉全局特征学习中的细粒度线索，[[81](#bib.bib81)] 中开发了一个联合学习框架，由单图像表示（SIR）和跨图像表示（CIR）组成，使用特定子网络通过三元组损失进行训练。广泛使用的ID区分嵌入（IDE）模型
    [[55](#bib.bib55)] 将训练过程构建为多类分类问题，将每个身份视为一个不同的类别。它现在被Re-ID社区广泛使用 [[42](#bib.bib42),
    [82](#bib.bib82), [58](#bib.bib58), [77](#bib.bib77), [83](#bib.bib83)]。钱等人 [[84](#bib.bib84)]
    开发了一个多尺度深度表示学习模型，以捕捉不同尺度上的区分线索。
- en: 'Attention Information. Attention schemes have been widely studied in literature
    to enhance representation learning [[85](#bib.bib85)]. 1) Group 1: Attention within
    the person image. Typical strategies include the pixel level attention [[86](#bib.bib86)]
    and the channel-wise feature response re-weighting [[86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88), [89](#bib.bib89)], or background suppressing [[22](#bib.bib22)].
    The spatial information is integrated in [[90](#bib.bib90)]. 2) Group 2: attention
    across multiple person images. A context-aware attentive feature learning method
    is proposed in [[91](#bib.bib91)], incorporating both an intra-sequence and inter-sequence
    attention for pair-wise feature alignment and refinement. The attention consistency
    property is added in [[92](#bib.bib92), [93](#bib.bib93)]. Group similarity [[94](#bib.bib94),
    [95](#bib.bib95)] is another popular approach to leverage the cross-image attention,
    which involves multiple images for local and global similarity modeling. The first
    group mainly enhances the robustness against misalignment/imperfect detection,
    and the second improves the feature learning by mining the relations across multiple
    images.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力信息。注意力机制在文献中已被广泛研究，以增强表示学习 [[85](#bib.bib85)]。1) 第一组：人像内部的注意力。典型策略包括像素级注意力
    [[86](#bib.bib86)] 和通道特征响应重加权 [[86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89)]，或背景抑制 [[22](#bib.bib22)]。空间信息在 [[90](#bib.bib90)] 中整合。2) 第二组：跨多个人物图像的注意力。一个上下文感知的注意力特征学习方法在
    [[91](#bib.bib91)] 中被提出，结合了序列内和序列间的注意力，用于成对特征对齐和细化。在 [[92](#bib.bib92), [93](#bib.bib93)]
    中增加了注意力一致性属性。群体相似性 [[94](#bib.bib94), [95](#bib.bib95)] 是另一种流行的方法，利用跨图像注意力，涉及多个图像进行局部和全局相似性建模。第一组主要增强了对齐错误/检测不完善的鲁棒性，第二组通过挖掘跨多个图像的关系来改进特征学习。
- en: 2.1.2 Local Feature Representation Learning
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 本地特征表示学习
- en: It learns part/region aggregated features, making it robust against misalignment
    [[96](#bib.bib96), [77](#bib.bib77)]. The body parts are either automatically
    generated by human parsing/pose estimation (Group 1) or roughly horizontal division
    (Group 2).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 它学习部分/区域聚合特征，使其对对齐错误 [[96](#bib.bib96), [77](#bib.bib77)] 更具鲁棒性。身体部位要么由人体解析/姿态估计自动生成（第一组），要么通过粗略的水平划分（第二组）。
- en: With automatic body part detection, the popular solution is to combine the full
    body representation and local part features [[97](#bib.bib97), [98](#bib.bib98)].
    Specifically, the multi-channel aggregation [[99](#bib.bib99)], multi-scale context-aware
    convolutions [[100](#bib.bib100)], multi-stage feature decomposition [[17](#bib.bib17)]
    and bilinear-pooling [[97](#bib.bib97)] are designed to improve the local feature
    learning. Rather than feature level fusion, the part-level similarity combination
    is also studied in [[98](#bib.bib98)]. Another popular solution is to enhance
    the robustness against background clutter, using the pose-driven matching [[101](#bib.bib101)],
    pose-guided part attention module [[102](#bib.bib102)], semantically part alignment
    [[103](#bib.bib103), [104](#bib.bib104)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自动身体部位检测，流行的解决方案是结合全身表示和局部部分特征[[97](#bib.bib97), [98](#bib.bib98)]。具体而言，多通道聚合[[99](#bib.bib99)]、多尺度上下文感知卷积[[100](#bib.bib100)]、多阶段特征分解[[17](#bib.bib17)]和双线性池化[[97](#bib.bib97)]被设计来改善局部特征学习。除了特征级别融合，部分级相似性组合在[[98](#bib.bib98)]中也有所研究。另一种流行的解决方案是通过姿势驱动的匹配[[101](#bib.bib101)]、姿势引导的部分注意力模块[[102](#bib.bib102)]和语义部分对齐[[103](#bib.bib103),
    [104](#bib.bib104)]来增强对背景杂乱的鲁棒性。
- en: For horizontal-divided region features, multiple part-level classifiers are
    learned in Part-based Convolutional Baseline (PCB) [[77](#bib.bib77)], which now
    serves as a strong part feature learning baseline in the current state-of-the-art
    [[105](#bib.bib105), [106](#bib.bib106), [28](#bib.bib28)]. To capture the relations
    across multiple body parts, the Siamese Long Short-Term Memory (LSTM) architecture
    [[96](#bib.bib96)], second-order non-local attention [[107](#bib.bib107)], Interaction-and-Aggregation
    (IA) [[108](#bib.bib108)] are designed to reinforce the feature learning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于横向划分的区域特征，Part-based Convolutional Baseline (PCB) [[77](#bib.bib77)]中学习了多个部分级分类器，它现在作为当前最先进技术中的一个强大的部分特征学习基线[[105](#bib.bib105),
    [106](#bib.bib106), [28](#bib.bib28)]。为了捕捉多个身体部位之间的关系，设计了Siamese长短期记忆网络 (LSTM)
    [[96](#bib.bib96)]、二阶非局部注意力[[107](#bib.bib107)]和互动与聚合 (IA) [[108](#bib.bib108)]，以强化特征学习。
- en: The first group uses human parsing techniques to obtain semantically meaningful
    body parts, which provides well-align part features. However, they require an
    additional pose detector and are prone to noisy pose detections [[77](#bib.bib77)].
    The second group uses a uniform partition to obtain the horizontal stripe parts,
    which is more flexible, but it is sensitive to heavy occlusions and large background
    clutter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组使用人体解析技术来获取具有语义意义的身体部位，从而提供良对齐的部分特征。然而，这些方法需要额外的姿势检测器，并且容易受到噪声姿势检测的影响[[77](#bib.bib77)]。第二组使用均匀划分来获取横向条纹部分，这种方法更加灵活，但对重度遮挡和大量背景杂乱非常敏感。
- en: 2.1.3 Auxiliary Feature Representation Learning
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 辅助特征表示学习
- en: Auxiliary feature representation learning usually requires additional annotated
    information (*e.g*., semantic attributes [[71](#bib.bib71)]) or generated/augmented
    training samples to reinforce the feature representation [[42](#bib.bib42), [19](#bib.bib19)].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助特征表示学习通常需要额外的标注信息（*例如*，语义属性[[71](#bib.bib71)]）或生成/增强的训练样本来增强特征表示[[42](#bib.bib42),
    [19](#bib.bib19)]。
- en: Semantic Attributes. A joint identity and attribute learning baseline is introduced
    in [[72](#bib.bib72)]. Su et al. [[71](#bib.bib71)] propose a deep attribute learning
    framework by incorporating the predicted semantic attribute information, enhancing
    the generalizability and robustness of the feature representation in a semi-supervised
    learning manner. Both the semantic attributes and the attention scheme are incorporated
    to improve part feature learning [[109](#bib.bib109)]. Semantic attributes are
    also adopted in [[110](#bib.bib110)] for video Re-ID feature representation learning.
    They are also leveraged as the auxiliary supervision information in unsupervised
    learning [[111](#bib.bib111)].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 语义属性。在[[72](#bib.bib72)]中引入了一种联合身份和属性学习基线。Su等人[[71](#bib.bib71)]提出了一种深度属性学习框架，通过结合预测的语义属性信息，增强了特征表示在半监督学习方式下的泛化能力和鲁棒性。语义属性和注意力机制都被纳入以改善部分特征学习[[109](#bib.bib109)]。语义属性也在[[110](#bib.bib110)]中被应用于视频
    Re-ID 特征表示学习。它们还被用作无监督学习中的辅助监督信息[[111](#bib.bib111)]。
- en: Viewpoint Information. The viewpoint information is also leveraged to enhance
    the feature representation learning [[112](#bib.bib112), [113](#bib.bib113)].
    Multi-Level Factorisation Net (MLFN) [[112](#bib.bib112)] also tries to learn
    the identity-discriminative and view-invariant feature representations at multiple
    semantic levels. Liu et al. [[113](#bib.bib113)] extract a combination of view-generic
    and view-specific learning. An angular regularization is incorporated in [[114](#bib.bib114)]
    in the viewpoint-aware feature learning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 视角信息。视角信息也被利用来增强特征表示学习 [[112](#bib.bib112), [113](#bib.bib113)]。多层级分解网络（MLFN）
    [[112](#bib.bib112)] 还尝试在多个语义层级上学习身份区分和视角不变的特征表示。Liu 等人[[113](#bib.bib113)] 提取视角通用和视角特定的学习组合。在[[114](#bib.bib114)]中，视角感知特征学习中加入了角度正则化。
- en: Domain Information. A Domain Guided Dropout (DGD) algorithm [[54](#bib.bib54)]
    is designed to adaptively mine the domain-sharable and domain-specific neurons
    for multi-domain deep feature representation learning. Treating each camera as
    a distinct domain, Lin et al. [[115](#bib.bib115)] propose a multi-camera consistent
    matching constraint to obtain a globally optimal representation in a deep learning
    framework. Similarly, the camera view information or the detected camera location
    is also applied in [[18](#bib.bib18)] to improve the feature representation with
    camera-specific information modeling.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 领域信息。设计了一种领域引导的 Dropout (DGD) 算法 [[54](#bib.bib54)]，用于自适应挖掘领域共享和领域特定的神经元，以进行多领域深度特征表示学习。Lin
    等人[[115](#bib.bib115)] 将每个摄像头视作一个独特的领域，提出了一种多摄像头一致性匹配约束，以在深度学习框架中获得全球最优表示。类似地，摄像头视角信息或检测到的摄像头位置也应用于[[18](#bib.bib18)]，以通过摄像头特定的信息建模来改善特征表示。
- en: GAN Generation. This section discusses the use of GAN generated images as the
    auxiliary information. Zheng et al. [[42](#bib.bib42)] start the first attempt
    to apply the GAN technique for person Re-ID. It improves the supervised feature
    representation learning with the generated person images. Pose constraints are
    incorporated in [[116](#bib.bib116)] to improve the quality of the generated person
    images, generating the person images with new pose variants. A pose-normalized
    image generation approach is designed in [[117](#bib.bib117)], which enhances
    the robustness against pose variations. Camera style information [[118](#bib.bib118)]
    is also integrated in the image generation process to address the cross camera
    variations. A joint discriminative and generative learning model [[119](#bib.bib119)]
    separately learns the appearance and structure codes to improve the image generation
    quality. Using the GAN generated images is also a widely used approach in unsupervised
    domain adaptation Re-ID [[120](#bib.bib120), [121](#bib.bib121)], approximating
    the target distribution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 生成。本节讨论了将 GAN 生成的图像作为辅助信息的使用。Zheng 等人[[42](#bib.bib42)] 开始首次尝试将 GAN 技术应用于行人重识别。它通过生成的人物图像改善了监督特征表示学习。在[[116](#bib.bib116)]中，加入了姿势约束，以提高生成的人物图像的质量，生成具有新姿态变体的人物图像。在[[117](#bib.bib117)]中设计了一种姿态归一化图像生成方法，增强了对姿态变化的鲁棒性。摄像头风格信息[[118](#bib.bib118)]也被集成到图像生成过程中，以应对跨摄像头的变化。联合判别和生成学习模型[[119](#bib.bib119)]分别学习外观和结构编码，以提高图像生成质量。使用
    GAN 生成的图像也是无监督领域适应 Re-ID 的一种广泛使用的方法[[120](#bib.bib120), [121](#bib.bib121)]，用于逼近目标分布。
- en: Data Augmentation. For Re-ID, custom operations are random resize, cropping
    and horizontal flip [[122](#bib.bib122)]. Besides, adversarially occluded samples
    [[19](#bib.bib19)] are generated to augment the variation of training data. A
    similar random erasing strategy is proposed in [[123](#bib.bib123)], adding random
    noise to the input images. A batch DropBlock [[124](#bib.bib124)] randomly drops
    a region block in the feature map to reinforce the attentive feature learning.
    Bak et al. [[125](#bib.bib125)] generate the virtual humans rendered under different
    illumination conditions. These methods enrich the supervision with the augmented
    samples, improving the generalizability on the testing set.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强。在行人重识别（Re-ID）中，自定义操作包括随机调整大小、裁剪和水平翻转[[122](#bib.bib122)]。此外，还生成对抗遮挡样本[[19](#bib.bib19)]以增加训练数据的变化。类似的随机擦除策略在[[123](#bib.bib123)]中提出，通过向输入图像添加随机噪声来实现。批量
    DropBlock [[124](#bib.bib124)] 随机丢弃特征图中的区域块，以增强关注特征的学习。Bak 等人[[125](#bib.bib125)]生成在不同光照条件下渲染的虚拟人。这些方法通过增强样本丰富了监督，提高了测试集上的泛化能力。
- en: 2.1.4 Video Feature Representation Learning
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 视频特征表示学习
- en: Video-based Re-ID is another popular topic [[126](#bib.bib126)], where each
    person is represented by a video sequence with multiple frames. Due to the rich
    appearance and temporal information, it has gained increasing interest in the
    Re-ID community. This also brings in additional challenges in video feature representation
    learning with multiple images.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视频的重识别是另一个热门话题[[126](#bib.bib126)]，其中每个人通过多个帧的视频序列来表示。由于其丰富的外观和时间信息，这在重识别社区中引起了越来越多的关注。这也带来了在处理多个图像的视频特征表示学习中的额外挑战。
- en: The primary challenge is to accurately capture the temporal information. A recurrent
    neural network architecture is designed for video-based person Re-ID [[127](#bib.bib127)],
    which jointly optimizes the final recurrent layer for temporal information propagation
    and the temporal pooling layer. A weighted scheme for spatial and temporal streams
    is developed in [[128](#bib.bib128)]. Yan et al. [[129](#bib.bib129)] present
    a progressive/sequential fusion framework to aggregate the frame-level human region
    representations. Semantic attributes are also adopted in [[110](#bib.bib110)]
    for video Re-ID with feature disentangling and frame re-weighting. Jointly aggregating
    the frame-level feature and spatio-temporal appearance information is crucial
    for video representation learning [[130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132)].
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 主要挑战是准确捕捉时间信息。设计了一种递归神经网络架构用于基于视频的人物重识别[[127](#bib.bib127)]，该架构联合优化最终的递归层以进行时间信息传播以及时间池化层。开发了一种空间和时间流的加权方案[[128](#bib.bib128)]。Yan
    等人[[129](#bib.bib129)]提出了一种渐进/序列融合框架来聚合帧级别的人体区域表示。语义属性也在[[110](#bib.bib110)]中被采用，用于视频重识别，结合特征解耦和帧重加权。联合聚合帧级别特征和时空外观信息对视频表示学习至关重要[[130](#bib.bib130),
    [131](#bib.bib131), [132](#bib.bib132)]。
- en: Another major challenge is the unavoidable outlier tracking frames within the
    videos. Informative frames are selected in a joint Spatial and Temporal Attention
    Pooling Network (ASTPN) [[131](#bib.bib131)], and the contextual information is
    integrated in [[130](#bib.bib130)]. A co-segmentation inspired attention model
    [[132](#bib.bib132)] detects salient features across multiple video frames with
    mutual consensus estimation. A diversity regularization [[133](#bib.bib133)] is
    employed to mine multiple discriminative body parts in each video sequence. An
    affine hull is adopted to handle the outlier frames within the video sequence
    [[83](#bib.bib83)]. An interesting work [[20](#bib.bib20)] utilizes the multiple
    video frames to auto-complete occluded regions. These works demonstrate that handling
    the noisy frames can greatly improve the video representation learning.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个主要挑战是视频中不可避免的异常跟踪帧。信息帧在联合空间和时间注意池化网络（ASTPN）[[131](#bib.bib131)]中被选择，并且上下文信息在[[130](#bib.bib130)]中被整合。一个受共同分割启发的注意模型[[132](#bib.bib132)]检测多个视频帧中的显著特征，并进行互相一致的估计。采用多样性正则化[[133](#bib.bib133)]来挖掘每个视频序列中的多个判别性身体部位。使用仿射包络处理视频序列中的异常帧[[83](#bib.bib83)]。一个有趣的工作[[20](#bib.bib20)]利用多个视频帧来自动补全被遮挡的区域。这些工作表明，处理噪声帧可以显著提高视频表示学习的效果。
- en: It is also challenging to handle the varying lengths of video sequences, Chen
    et al. [[134](#bib.bib134)] divide the long video sequences into multiple short
    snippets, aggregating the top-ranked snippets to learn a compact embedding. A
    clip-level learning strategy [[135](#bib.bib135)] exploits both spatial and temporal
    dimensional attention cues to produce a robust clip-level representation. Both
    the short- and long-term relations [[136](#bib.bib136)] are integrated in a self-attention
    scheme.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 处理视频序列的不同长度也是一个挑战，Chen 等人[[134](#bib.bib134)]将长视频序列分成多个短片段，将排名靠前的片段聚合以学习紧凑的嵌入。一个片段级学习策略[[135](#bib.bib135)]利用空间和时间维度注意力线索来生成强健的片段级表示。短期和长期关系[[136](#bib.bib136)]都在自注意力方案中进行了整合。
- en: 2.1.5 Architecture Design
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5 架构设计
- en: Framing person Re-ID as a specific pedestrian retrieval problem, most existing
    works adopt the network architectures [[79](#bib.bib79), [80](#bib.bib80)] designed
    for image classification as the backbone. Some works have tried to modify the
    backbone architecture to achieve better Re-ID features. For the widely used ResNet50
    backbone [[80](#bib.bib80)], the important modifications include changing the
    last convolutional stripe/size to 1 [[77](#bib.bib77)], employing adaptive average
    pooling in the last pooling layer [[77](#bib.bib77)], and adding bottleneck layer
    with batch normalization after the pooling layer [[82](#bib.bib82)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将行人再识别（Re-ID）框架化为一个特定的行人检索问题，现有的大多数工作采用了为图像分类设计的网络架构[[79](#bib.bib79), [80](#bib.bib80)]作为骨干网。一些工作尝试修改骨干架构以实现更好的Re-ID特征。对于广泛使用的ResNet50骨干网[[80](#bib.bib80)]，重要的修改包括将最后一个卷积条带/大小改为1[[77](#bib.bib77)]，在最后的池化层中采用自适应平均池化[[77](#bib.bib77)]，以及在池化层后添加具有批量归一化的瓶颈层[[82](#bib.bib82)]。
- en: Accuracy is the major concern for specific Re-ID network architecture design
    to improve the accuracy, Li et al. [[43](#bib.bib43)] start the first attempt
    by designing a filter pairing neural network (FPNN), which jointly handles misalignment
    and occlusions with part discriminative information mining. Wang et al. [[89](#bib.bib89)]
    propose a BraidNet with a specially designed WConv layer and Channel Scaling layer.
    The WConv layer extracts the difference information of two images to enhance the
    robustness against misalignments and Channel Scaling layer optimizes the scaling
    factor of each input channel. A Multi-Level Factorisation Net (MLFN) [[112](#bib.bib112)]
    contains multiple stacked blocks to model various latent factors at a specific
    level, and the factors are dynamically selected to formulate the final representation.
    An efficient fully convolutional Siamese network [[137](#bib.bib137)] with convolution
    similarity module is developed to optimize multi-level similarity measurement.
    The similarity is efficiently captured and optimized by using the depth-wise convolution.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 精度是特定Re-ID网络架构设计的主要关注点。为了提高精度，Li等人[[43](#bib.bib43)]首次尝试设计了一个滤波器配对神经网络（FPNN），该网络通过部分区分信息挖掘共同处理错位和遮挡。Wang等人[[89](#bib.bib89)]提出了一种具有特别设计的WConv层和通道缩放层的BraidNet。WConv层提取两张图像的差异信息，以增强对错位的鲁棒性，而通道缩放层优化每个输入通道的缩放因子。多级分解网络（MLFN）[[112](#bib.bib112)]包含多个堆叠的块，以在特定级别上建模各种潜在因子，并动态选择这些因子以形成最终表示。开发了一种高效的全卷积Siamese网络[[137](#bib.bib137)]，配有卷积相似度模块，以优化多级相似度测量。通过使用深度卷积高效捕获和优化相似度。
- en: Efficiency is another important factor for Re-ID architecture design. An efficient
    small scale network, namely Omni-Scale Network (OSNet) [[138](#bib.bib138)], is
    designed by incorporating the point-wise and depth-wise convolutions. To achieve
    multi-scale feature learning, a residual block composed of multiple convolutional
    streams is introduced.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 效率是Re-ID架构设计的另一个重要因素。通过结合点卷积和深度卷积，设计了一种高效的小规模网络，即Omni-Scale网络（OSNet）[[138](#bib.bib138)]。为了实现多尺度特征学习，引入了由多个卷积流组成的残差块。
- en: With the increasing interest in auto-machine learning, an Auto-ReID [[139](#bib.bib139)]
    model is proposed. Auto-ReID provides an efficient and effective automated neural
    architecture design based on a set of basic architecture components, using a part-aware
    module to capture the discriminative local Re-ID features. This provides a potential
    research direction in exploring powerful domain-specific architectures.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对自动机器学习的兴趣增加，提出了一种Auto-ReID[[139](#bib.bib139)]模型。Auto-ReID基于一组基本架构组件提供了一种高效且有效的自动化神经架构设计，使用部分感知模块捕获区分性的局部Re-ID特征。这为探索强大的领域特定架构提供了潜在的研究方向。
- en: '![Refer to caption](img/e99e3b67c8d1378c9e08911e2200fe46.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e99e3b67c8d1378c9e08911e2200fe46.png)'
- en: 'Figure 3: Three kinds of widely used loss functions in the literature. (a)
    Identity Loss [[82](#bib.bib82), [42](#bib.bib42), [118](#bib.bib118), [140](#bib.bib140)]
    ; (b) Verification Loss [[94](#bib.bib94), [141](#bib.bib141)] and (c) Triplet
    Loss [[57](#bib.bib57), [22](#bib.bib22), [14](#bib.bib14)]. Many works employ
    their combinations [[137](#bib.bib137), [87](#bib.bib87), [142](#bib.bib142),
    [141](#bib.bib141)].'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：文献中三种广泛使用的损失函数。(a) 身份损失[[82](#bib.bib82), [42](#bib.bib42), [118](#bib.bib118),
    [140](#bib.bib140)]; (b) 验证损失[[94](#bib.bib94), [141](#bib.bib141)] 和 (c) 三元组损失[[57](#bib.bib57),
    [22](#bib.bib22), [14](#bib.bib14)]。许多工作使用它们的组合[[137](#bib.bib137), [87](#bib.bib87),
    [142](#bib.bib142), [141](#bib.bib141)]。
- en: 2.2 Deep Metric Learning
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 深度度量学习
- en: 'Metric learning has been extensively studied before the deep learning era by
    learning a Mahalanobis distance function [[37](#bib.bib37), [36](#bib.bib36)]
    or projection matrix [[40](#bib.bib40)]. The role of metric learning has been
    replaced by the loss function designs to guide the feature representation learning.
    We will first review the widely used loss functions in § [2.2.1](#S2.SS2.SSS1
    "2.2.1 Loss Function Design ‣ 2.2 Deep Metric Learning ‣ 2 Closed-world Person
    Re-Identification ‣ Deep Learning for Person Re-identification: A Survey and Outlook")
    and then summarize the training strategies with specific sampling designs § [2.2.2](#S2.SS2.SSS2
    "2.2.2 Training strategy ‣ 2.2 Deep Metric Learning ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '在深度学习时代之前，度量学习已经通过学习马哈拉诺比斯距离函数 [[37](#bib.bib37), [36](#bib.bib36)] 或投影矩阵 [[40](#bib.bib40)]
    被广泛研究。度量学习的角色已被损失函数设计所取代，以指导特征表示学习。我们将首先回顾 § [2.2.1](#S2.SS2.SSS1 "2.2.1 Loss
    Function Design ‣ 2.2 Deep Metric Learning ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook") 中广泛使用的损失函数，然后总结具有特定采样设计的训练策略
    § [2.2.2](#S2.SS2.SSS2 "2.2.2 Training strategy ‣ 2.2 Deep Metric Learning ‣ 2
    Closed-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")。'
- en: 2.2.1 Loss Function Design
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 损失函数设计
- en: 'This survey only focuses on the loss functions designed for deep learning [[56](#bib.bib56)].
    An overview of the distance metric learning designed for hand-crafted systems
    can be found in [[2](#bib.bib2), [143](#bib.bib143)]. There are three widely studied
    loss functions with their variants in the literature for person Re-ID, including
    the identity loss, verification loss and triplet loss. An illustration of three
    loss functions is shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.5 Architecture Design
    ‣ 2.1 Feature Representation Learning ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook").'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查仅关注于为深度学习设计的损失函数 [[56](#bib.bib56)]。关于为手工设计系统设计的距离度量学习的概述可以在 [[2](#bib.bib2),
    [143](#bib.bib143)] 中找到。文献中有三种广泛研究的损失函数及其变体用于行人重识别，包括身份损失、验证损失和三元组损失。三种损失函数的示意图见图
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.5 Architecture Design ‣ 2.1 Feature Representation
    Learning ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook")。'
- en: Identity Loss. It treats the training process of person Re-ID as an image classification
    problem [[55](#bib.bib55)], *i.e*., each identity is a distinct class. In the
    testing phase, the output of the pooling layer or embedding layer is adopted as
    the feature extractor. Given an input image $x_{i}$ with label $y_{i}$, the predicted
    probability of $x_{i}$ being recognized as class $y_{i}$ is encoded with a softmax
    function, represented by $p(y_{i}|x_{i})$. The identity loss is then computed
    by the cross-entropy
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 身份损失。它将行人重识别的训练过程视为图像分类问题 [[55](#bib.bib55)]，*即*，每个身份是一个不同的类别。在测试阶段，池化层或嵌入层的输出被作为特征提取器。给定标签为
    $y_{i}$ 的输入图像 $x_{i}$，$x_{i}$ 被识别为类别 $y_{i}$ 的预测概率通过 softmax 函数进行编码，表示为 $p(y_{i}|x_{i})$。然后通过交叉熵计算身份损失
- en: '|  | $\mathcal{L}_{id}=-\frac{1}{n}\sum\nolimits_{i=1}^{n}{\log(p(y_{i}&#124;x_{i}))},$
    |  | (1) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{id}=-\frac{1}{n}\sum\nolimits_{i=1}^{n}{\log(p(y_{i}&#124;x_{i}))},$
    |  | (1) |'
- en: where $n$ represents the number of training samples within each batch. The identity
    loss has been widely used in existing methods [[82](#bib.bib82), [42](#bib.bib42),
    [120](#bib.bib120), [19](#bib.bib19), [144](#bib.bib144), [118](#bib.bib118),
    [140](#bib.bib140), [106](#bib.bib106), [92](#bib.bib92), [95](#bib.bib95)]. Generally,
    it is easy to train and automatically mine the hard samples during the training
    process, as demonstrated in [[145](#bib.bib145)]. Several works have also investigated
    the softmax variants [[146](#bib.bib146)], such as the sphere loss in [[147](#bib.bib147)]
    and AM softmax in [[95](#bib.bib95)]. Another simple yet effective strategy, *i.e*.,
    label smoothing [[42](#bib.bib42), [122](#bib.bib122)], is generally integrated
    into the standard softmax cross-entropy loss. Its basic idea is to avoid the model
    fitting to over-confident annotated labels, improving the generalizability [[148](#bib.bib148)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n$ 表示每个批次中的训练样本数量。身份损失在现有方法中已被广泛使用 [[82](#bib.bib82), [42](#bib.bib42),
    [120](#bib.bib120), [19](#bib.bib19), [144](#bib.bib144), [118](#bib.bib118),
    [140](#bib.bib140), [106](#bib.bib106), [92](#bib.bib92), [95](#bib.bib95)]。通常，它易于训练，并在训练过程中自动挖掘困难样本，正如
    [[145](#bib.bib145)] 中所示。一些工作还研究了 softmax 的变体 [[146](#bib.bib146)]，如 [[147](#bib.bib147)]
    中的球面损失和 [[95](#bib.bib95)] 中的 AM softmax。另一种简单而有效的策略，即标签平滑 [[42](#bib.bib42),
    [122](#bib.bib122)]，通常被集成到标准的 softmax 交叉熵损失中。其基本思想是避免模型过于拟合过于自信的标注标签，从而提高泛化能力
    [[148](#bib.bib148)]。
- en: Verification Loss. It optimizes the pairwise relationship, either with a contrastive
    loss [[96](#bib.bib96), [120](#bib.bib120)] or binary verification loss [[141](#bib.bib141),
    [43](#bib.bib43)]. The contrastive loss improves the relative pairwise distance
    comparison, formulated by
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 验证损失。它优化了成对关系，使用对比损失 [[96](#bib.bib96), [120](#bib.bib120)] 或二元验证损失 [[141](#bib.bib141),
    [43](#bib.bib43)]。对比损失改进了相对的成对距离比较，公式为
- en: '|  | $\mathcal{L}_{con}=(1-\delta_{ij})\{\max(0,\rho-d_{ij})\}^{2}+\delta_{ij}d_{ij}^{2},$
    |  | (2) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{con}=(1-\delta_{ij})\{\max(0,\rho-d_{ij})\}^{2}+\delta_{ij}d_{ij}^{2},$
    |  | (2) |'
- en: where $d_{ij}$ represents the Euclidean distance between the embedding features
    of two input samples $x_{i}$ and $x_{j}$. $\delta_{ij}$ is a binary label indicator
    ($\delta_{ij}=1$ when $x_{i}$ and $x_{j}$ belong to the same identity, and $\delta_{ij}=0$,
    otherwise). $\rho$ is a margin parameter. There are several variants, *e.g*.,
    the pairwise comparison with ranking SVM in [[81](#bib.bib81)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{ij}$ 表示两个输入样本 $x_{i}$ 和 $x_{j}$ 的嵌入特征之间的欧几里得距离。$\delta_{ij}$ 是一个二元标签指示符（当
    $x_{i}$ 和 $x_{j}$ 属于同一身份时 $\delta_{ij}=1$，否则 $\delta_{ij}=0$）。$\rho$ 是一个边际参数。有几种变体，*例如*，[[81](#bib.bib81)]
    中的排名支持向量机的成对比较。
- en: Binary verification [[141](#bib.bib141), [43](#bib.bib43)] discriminates the
    positive and negative of a input image pair. Generally, a differential feature
    $f_{ij}$ is obtained by $f_{ij}=(f_{j}-f_{j})^{2}$ [[141](#bib.bib141)], where
    $f_{i}$ and $f_{j}$ are the embedding features of two samples $x_{i}$ and $x_{j}$.
    The verification network classifies the differential feature into positive or
    negative. We use $p(\delta_{ij}|f_{ij})$ to represent the probability of an input
    pair ($x_{i}$ and $x_{j}$) being recognized as $\delta_{ij}$ (0 or 1). The verification
    loss with cross-entropy is
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 二元验证 [[141](#bib.bib141), [43](#bib.bib43)] 区分输入图像对的正样本和负样本。通常，通过 $f_{ij}=(f_{j}-f_{j})^{2}$
    [[141](#bib.bib141)] 获得差异特征 $f_{ij}$，其中 $f_{i}$ 和 $f_{j}$ 是两个样本 $x_{i}$ 和 $x_{j}$
    的嵌入特征。验证网络将差异特征分类为正样本或负样本。我们使用 $p(\delta_{ij}|f_{ij})$ 来表示输入对 ($x_{i}$ 和 $x_{j}$)
    被识别为 $\delta_{ij}$ (0 或 1) 的概率。带有交叉熵的验证损失为
- en: '|  | $\mathcal{L}_{veri}(i,j)=-{\delta_{ij}\log(p(\delta_{ij}&#124;f_{ij}))}-(1-\delta_{ij})\log(1-p(\delta_{ij}&#124;f_{ij})).$
    |  | (3) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{veri}(i,j)=-{\delta_{ij}\log(p(\delta_{ij}&#124;f_{ij}))}-(1-\delta_{ij})\log(1-p(\delta_{ij}&#124;f_{ij})).$
    |  | (3) |'
- en: The verification is often combined with the identity loss to improve the performance
    [[94](#bib.bib94), [141](#bib.bib141), [120](#bib.bib120), [96](#bib.bib96)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 验证通常与身份损失结合使用，以提高性能 [[94](#bib.bib94), [141](#bib.bib141), [120](#bib.bib120),
    [96](#bib.bib96)]。
- en: Triplet loss. It treats the Re-ID model training process as a retrieval ranking
    problem. The basic idea is that the distance between the positive pair should
    be smaller than the negative pair by a pre-defined margin [[57](#bib.bib57)].
    Typically, a triplet contains one anchor sample $x_{i}$, one positive sample $x_{j}$
    with the same identity, and one negative sample $x_{k}$ from a different identity.
    The triplet loss with a margin parameter is represented by
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Triplet loss。它将Re-ID模型训练过程视为检索排序问题。基本思想是正样本对之间的距离应小于负样本对的距离，并且有一个预定义的边界[[57](#bib.bib57)]。典型的三元组包含一个锚点样本$x_{i}$、一个具有相同身份的正样本$x_{j}$，以及一个来自不同身份的负样本$x_{k}$。具有边界参数的三元组损失表示为
- en: '|  | $\mathcal{L}_{tri}(i,j,k)=\max(\rho+d_{ij}-d_{ik},0),$ |  | (4) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{tri}(i,j,k)=\max(\rho+d_{ij}-d_{ik},0),$ |  | (4) |'
- en: where $d(\cdot)$ measures the Euclidean distance between two samples. The large
    proportion of easy triplets will dominate the training process if we directly
    optimize above loss function, resulting in limited discriminability. To alleviate
    this issue, various informative triplet mining methods have been designed [[57](#bib.bib57),
    [22](#bib.bib22), [14](#bib.bib14), [97](#bib.bib97)]. The basic idea is to select
    the informative triplets [[57](#bib.bib57), [149](#bib.bib149)]. Specifically,
    a moderate positive mining with a weight constraint is introduced in [[149](#bib.bib149)],
    which directly optimizes the feature difference. Hermans et al. [[57](#bib.bib57)]
    demonstrate that the online hardest positive and negative mining within each training
    batch is beneficial for discriminative Re-ID model learning. Some methods also
    studied the point to set similarity strategy for informative triplet mining [[150](#bib.bib150),
    [151](#bib.bib151)]. This enhances robustness against the outlier samples with
    a soft hard-mining scheme.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$d(\cdot)$测量两个样本之间的欧几里得距离。如果我们直接优化上述损失函数，大比例的简单三元组将主导训练过程，从而导致区分性有限。为缓解此问题，设计了各种信息丰富的三元组挖掘方法[[57](#bib.bib57),
    [22](#bib.bib22), [14](#bib.bib14), [97](#bib.bib97)]。基本思想是选择信息丰富的三元组[[57](#bib.bib57),
    [149](#bib.bib149)]。具体而言，[[149](#bib.bib149)]引入了一种中等程度的正样本挖掘方法，具有权重约束，直接优化特征差异。Hermans等人[[57](#bib.bib57)]证明了在每个训练批次中进行在线最难的正负样本挖掘对区分性Re-ID模型学习是有益的。一些方法还研究了信息丰富的三元组挖掘的点对集相似性策略[[150](#bib.bib150),
    [151](#bib.bib151)]。这增强了针对异常样本的鲁棒性，采用了软硬挖掘方案。
- en: To further enrich the triplet supervision, a quadruplet deep network is developed
    in [[152](#bib.bib152)], where each quadruplet contains one anchor sample, one
    positive sample and two mined negative samples. The quadruplets are formulated
    with a margin-based online hard negative mining. Optimizing the quadruplet relationship
    results in smaller intra-class variation and larger inter-class variation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步丰富三元组监督，[[152](#bib.bib152)]中开发了一个四元组深度网络，其中每个四元组包含一个锚点样本、一个正样本和两个挖掘出的负样本。四元组通过基于边界的在线硬负样本挖掘进行构造。优化四元组关系会导致较小的类内变化和较大的类间变化。
- en: The combination of triplet loss and identity loss is one of the most popular
    solutions for deep Re-ID model learning [[137](#bib.bib137), [116](#bib.bib116),
    [87](#bib.bib87), [142](#bib.bib142), [103](#bib.bib103), [28](#bib.bib28), [153](#bib.bib153),
    [104](#bib.bib104), [154](#bib.bib154), [93](#bib.bib93), [90](#bib.bib90)]. These
    two components are mutually beneficial for discriminative feature representation
    learning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组损失和身份损失的组合是深度Re-ID模型学习中最受欢迎的解决方案之一[[137](#bib.bib137), [116](#bib.bib116),
    [87](#bib.bib87), [142](#bib.bib142), [103](#bib.bib103), [28](#bib.bib28), [153](#bib.bib153),
    [104](#bib.bib104), [154](#bib.bib154), [93](#bib.bib93), [90](#bib.bib90)]。这两个组件对于区分性特征表示学习互为补充。
- en: OIM loss. In addition to the above three kinds of loss functions, an Online
    Instance Matching (OIM) loss [[64](#bib.bib64)] is designed with a memory bank
    scheme. A memory bank $\{v_{k},k=1,2,\cdots,c\}$ contains the stored instance
    features, where $c$ denotes the class number. The OIM loss is then formulated
    by
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: OIM损失。除了上述三种损失函数外，还设计了一种具有内存库方案的在线实例匹配（OIM）损失[[64](#bib.bib64)]。内存库$\{v_{k},k=1,2,\cdots,c\}$包含存储的实例特征，其中$c$表示类别数量。然后OIM损失被表示为
- en: '|  | $\mathcal{L}_{oim}=-\frac{1}{n}\sum\nolimits_{i=1}^{n}{\log\frac{\exp(v_{i}^{T}f_{i}/\tau)}{\sum\nolimits_{k=1}^{c}\exp(v_{k}^{T}f_{i}/\tau)}},$
    |  | (5) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{oim}=-\frac{1}{n}\sum\nolimits_{i=1}^{n}{\log\frac{\exp(v_{i}^{T}f_{i}/\tau)}{\sum\nolimits_{k=1}^{c}\exp(v_{k}^{T}f_{i}/\tau)}},$
    |  | (5) |'
- en: where $v_{i}$ represents the corresponding stored memory feature for class $y_{i}$,
    and $\tau$ is a temperature parameter that controls the similarity space [[145](#bib.bib145)].
    $v_{i}^{T}f_{i}$ measures the online instance matching score. The comparison with
    a memorized feature set of unlabelled identities is further included to calculate
    the denominator [[64](#bib.bib64)], handling the large instance number of non-targeted
    identities. This memory scheme is also adopted in unsupervised domain adaptive
    Re-ID [[106](#bib.bib106)].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$v_{i}$表示类$y_{i}$的相应存储记忆特征，而$\tau$是控制相似性空间的温度参数[[145](#bib.bib145)]。$v_{i}^{T}f_{i}$衡量在线实例匹配分数。与未标记身份的记忆特征集的比较进一步包括在内，以计算分母[[64](#bib.bib64)]，处理非目标身份的大量实例。这种记忆方案也被应用于无监督领域自适应Re-ID[[106](#bib.bib106)]。
- en: 2.2.2 Training strategy
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 训练策略
- en: The batch sampling strategy plays an important role in discriminative Re-ID
    model learning. It is challenging since the number of annotated training images
    for each identity varies significantly [[5](#bib.bib5)]. Meanwhile, the severely
    imbalanced positive and negative sample pairs increases additional difficulty
    for the training strategy design [[40](#bib.bib40)].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 批次采样策略在判别性Re-ID模型学习中发挥了重要作用。由于每个身份的标注训练图像数量差异显著，这一任务具有挑战性[[5](#bib.bib5)]。同时，严重不平衡的正负样本对增加了训练策略设计的额外难度[[40](#bib.bib40)]。
- en: The most commonly used training strategy for handling the imbalanced issue is
    identity sampling [[57](#bib.bib57), [122](#bib.bib122)]. For each training batch,
    a certain number of identities are randomly selected, and then several images
    are sampled from each selected identity. This batch sampling strategy guarantees
    the informative positive and negative mining.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不平衡问题的最常用训练策略是身份采样[[57](#bib.bib57), [122](#bib.bib122)]。对于每个训练批次，随机选择一定数量的身份，然后从每个选择的身份中采样几张图像。这种批次采样策略保证了信息丰富的正负样本挖掘。
- en: To handle the imbalance issue between the positive and negative, adaptive sampling
    is the popular approach to adjust the contribution of positive and negative samples,
    such as Sample Rate Learning (SRL) [[89](#bib.bib89)], curriculum sampling [[87](#bib.bib87)].
    Another approach is sample re-weighting, using the sample distribution [[87](#bib.bib87)]
    or similarity difference [[52](#bib.bib52)] to adjust the sample weight. An efficient
    reference constraint is designed in [[155](#bib.bib155)] to transform the pairwise/triplet
    similarity to a sample-to-reference similarity, addressing the imbalance issue
    and enhancing the discriminability, which is also robust to outliers.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理正负样本之间的不平衡，适应性采样是一种流行的方法，用于调整正负样本的贡献，例如样本率学习（SRL）[[89](#bib.bib89)]、课程采样[[87](#bib.bib87)]。另一种方法是样本重新加权，利用样本分布[[87](#bib.bib87)]或相似性差异[[52](#bib.bib52)]来调整样本权重。在[[155](#bib.bib155)]中设计了一种高效的参考约束，将成对/三元组相似性转换为样本对参考相似性，解决了不平衡问题并增强了判别性，同时对异常值也具有鲁棒性。
- en: To adaptively combine multiple loss functions, a multi-loss dynamic training
    strategy [[156](#bib.bib156)] adaptively reweights the identity loss and triplet
    loss, extracting appropriate component shared between them. This multi-loss training
    strategy leads to consistent performance gain.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应性地组合多个损失函数，多损失动态训练策略[[156](#bib.bib156)]自适应地重新加权身份损失和三元组损失，提取它们之间的适当共享组件。这种多损失训练策略带来了稳定的性能提升。
- en: '![Refer to caption](img/dd4836bd077a2a1dbb48281940877b33.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd4836bd077a2a1dbb48281940877b33.png)'
- en: 'Figure 4: An illustration of re-ranking in person Re-ID. Given a query example,
    an initial rank list is retrieved, where the hard matches are ranked in the bottom.
    Using the top-ranked easy positive match (1) as query to search in the gallery,
    we can get the hard match (2) and (3) with similarity propagation in the gallery
    set.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：在行人Re-ID中的重新排名示意图。给定一个查询示例，检索到初始排名列表，其中难匹配的排在底部。使用排名靠前的简单正样本(1)作为查询在库中搜索，我们可以通过库集中的相似性传播得到难匹配(2)和(3)。
- en: 2.3 Ranking Optimization
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 排名优化
- en: Ranking optimization plays a crucial role in improving the retrieval performance
    in the testing stage. Given an initial ranking list, it optimizes the ranking
    order, either by automatic gallery-to-gallery similarity mining [[157](#bib.bib157),
    [58](#bib.bib58)] or human interaction [[158](#bib.bib158), [159](#bib.bib159)].
    Rank/Metric fusion [[160](#bib.bib160), [161](#bib.bib161)] is another popular
    approach for improving the ranking performance with multiple ranking list inputs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 排名优化在提高测试阶段的检索性能中发挥了关键作用。给定初始排名列表，它通过自动的画廊到画廊相似性挖掘 [[157](#bib.bib157), [58](#bib.bib58)]
    或人类互动 [[158](#bib.bib158), [159](#bib.bib159)] 来优化排名顺序。排名/度量融合 [[160](#bib.bib160),
    [161](#bib.bib161)] 是另一种流行的方法，通过多个排名列表输入来提高排名性能。
- en: 2.3.1 Re-ranking
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 重新排名
- en: 'The basic idea of re-ranking is to utilize the gallery-to-gallery similarity
    to optimize the initial ranking list, as shown in Fig. [4](#S2.F4 "Figure 4 ‣
    2.2.2 Training strategy ‣ 2.2 Deep Metric Learning ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook"). The top-ranked
    similarity pulling and bottom-ranked dissimilarity pushing is proposed in [[157](#bib.bib157)].
    The widely-used $k$-reciprocal reranking [[58](#bib.bib58)] mines the contextual
    information. Similar idea for contextual information modeling is applied in [[25](#bib.bib25)].
    Bai et al. [[162](#bib.bib162)] utilize the geometric structure of the underlying
    manifold. An expanded cross neighborhood re-ranking method [[18](#bib.bib18)]
    is introduced by integrating the cross neighborhood distance. A local blurring
    re-ranking [[95](#bib.bib95)] employs the clustering structure to improve neighborhood
    similarity measurement.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '重新排名的基本思想是利用画廊到画廊的相似性来优化初始排名列表，如图 [4](#S2.F4 "Figure 4 ‣ 2.2.2 Training strategy
    ‣ 2.2 Deep Metric Learning ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook") 所示。提出了顶部排名的相似性拉取和底部排名的不相似性推动
    [[157](#bib.bib157)]。广泛使用的 $k$-reciprocal 重新排名 [[58](#bib.bib58)] 挖掘了上下文信息。 [[25](#bib.bib25)]
    中应用了类似的上下文信息建模思想。 Bai 等人 [[162](#bib.bib162)] 利用潜在流形的几何结构。通过整合交叉邻域距离，[[18](#bib.bib18)]
    引入了一种扩展的交叉邻域重新排名方法。局部模糊重新排名 [[95](#bib.bib95)] 采用聚类结构来改进邻域相似性测量。'
- en: Query Adaptive. Considering the query difference, some methods have designed
    the query adaptive retrieval strategy to replace the uniform searching engine
    to improve the performance [[163](#bib.bib163), [164](#bib.bib164)]. Andy et al.
    [[163](#bib.bib163)] propose a query adaptive re-ranking method using locality
    preserving projections. An efficient online local metric adaptation method is
    presented in [[164](#bib.bib164)], which learns a strictly local metric with mined
    negative samples for each probe.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 查询自适应。考虑到查询差异，一些方法设计了查询自适应检索策略，以替代统一的搜索引擎来提高性能 [[163](#bib.bib163), [164](#bib.bib164)]。Andy
    等人 [[163](#bib.bib163)] 提出了一种使用局部保持投影的查询自适应重新排名方法。 [[164](#bib.bib164)] 中提出了一种高效的在线局部度量适应方法，该方法为每个探针学习了一个严格的局部度量，并挖掘了负样本。
- en: Human Interaction. It involves using human feedback to optimize the ranking
    list [[158](#bib.bib158)]. This provides reliable supervision during the re-ranking
    process. A hybrid human-computer incremental learning model is presented in [[159](#bib.bib159)],
    which cumulatively learns from human feedback, improving the Re-ID ranking performance
    on-the-fly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 人类互动。它涉及使用人类反馈来优化排名列表 [[158](#bib.bib158)]。这为重新排名过程提供了可靠的监督。一个混合的人工计算机增量学习模型在
    [[159](#bib.bib159)] 中被提出，该模型从人类反馈中逐步学习，实时提高 Re-ID 排名性能。
- en: 2.3.2 Rank Fusion
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 排名融合
- en: Rank fusion exploits multiple ranking lists obtained with different methods
    to improve the retrieval performance [[59](#bib.bib59)]. Zheng et al. [[165](#bib.bib165)]
    propose a query adaptive late fusion method on top of a “L” shaped observation
    to fuse methods. A rank aggregation method by employing the similarity and dissimilarity
    is developed in [[59](#bib.bib59)]. The rank fusion process in person Re-ID is
    formulated as a consensus-based decision problem with graph theory [[166](#bib.bib166)],
    mapping the similarity scores obtained by multiple algorithms into a graph with
    path searching. An Unified Ensemble Diffusion (UED) [[161](#bib.bib161)] is recently
    designed for metric fusion. UED maintains the advantages of three existing fusion
    algorithms, optimized by a new objective function and derivation. The metric ensemble
    learning is also studied in [[160](#bib.bib160)].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 排名融合利用多种方法获得的排名列表来提高检索性能 [[59](#bib.bib59)]。郑等人 [[165](#bib.bib165)] 提出了在"L"形观察基础上进行查询自适应后期融合的方法。通过利用相似性和不相似性，[[59](#bib.bib59)]
    开发了一种排名聚合方法。人员重识别中的排名融合过程被表述为一个基于共识的决策问题，使用图论 [[166](#bib.bib166)] 将由多种算法获得的相似性评分映射到一个图中并进行路径搜索。最近设计了一个统一的集成扩散
    (UED) [[161](#bib.bib161)] 用于度量融合。UED 保持了三种现有融合算法的优势，并通过新的目标函数和推导进行了优化。度量集成学习也在
    [[160](#bib.bib160)] 中进行了研究。
- en: 'TABLE II: Statistics of some commonly used datasets for closed-world person
    Re-ID. “both” means that it contains both hand-cropped and detected bounding boxes.
    “C&M” means both CMC and mAP are evaluated.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：一些常用闭集人脸重识别数据集的统计信息。 “both”表示包含手工裁剪和检测到的边界框。 “C&M”表示同时评估了CMC和mAP。
- en: '|  | Image datasets |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | 图像数据集 |'
- en: '| Dataset | Time | #ID | #image | #cam. | Label | Res. | Eval. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 时间 | #ID | #图像 | #摄像头 | 标签 | 分辨率 | 评估 |'
- en: '| VIPeR | 2007 | 632 | 1,264 | 2 | hand | fixed | CMC |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| VIPeR | 2007 | 632 | 1,264 | 2 | hand | fixed | CMC |'
- en: '| iLIDS | 2009 | 119 | 476 | 2 | hand | vary | CMC |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| iLIDS | 2009 | 119 | 476 | 2 | hand | vary | CMC |'
- en: '| GRID | 2009 | 250 | 1,275 | 8 | hand | vary | CMC |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| GRID | 2009 | 250 | 1,275 | 8 | hand | vary | CMC |'
- en: '| PRID2011 | 2011 | 200 | 1,134 | 2 | hand | fixed | CMC |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| PRID2011 | 2011 | 200 | 1,134 | 2 | hand | fixed | CMC |'
- en: '| CUHK01 | 2012 | 971 | 3,884 | 2 | hand | fixed | CMC |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| CUHK01 | 2012 | 971 | 3,884 | 2 | hand | fixed | CMC |'
- en: '| CUHK02 | 2013 | 1,816 | 7,264 | 10 | hand | fixed | CMC |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| CUHK02 | 2013 | 1,816 | 7,264 | 10 | hand | fixed | CMC |'
- en: '| CUHK03 | 2014 | 1,467 | 13,164 | 2 | both | vary | CMC |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| CUHK03 | 2014 | 1,467 | 13,164 | 2 | both | vary | CMC |'
- en: '| Market-1501 | 2015 | 1,501 | 32,668 | 6 | both | fixed | C&M |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Market-1501 | 2015 | 1,501 | 32,668 | 6 | both | fixed | C&M |'
- en: '| DukeMTMC | 2017 | 1,404 | 36,411 | 8 | both | fixed | C&M |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| DukeMTMC | 2017 | 1,404 | 36,411 | 8 | both | fixed | C&M |'
- en: '| Airport | 2017 | 9,651 | 39,902 | 6 | auto | fixed | C&M |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Airport | 2017 | 9,651 | 39,902 | 6 | auto | fixed | C&M |'
- en: '| MSMT17 | 2018 | 4,101 | 126,441 | 15 | auto | vary | C&M |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| MSMT17 | 2018 | 4,101 | 126,441 | 15 | auto | vary | C&M |'
- en: '|  | Video datasets |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | 视频数据集 |'
- en: '| Dataset | time | #ID | #track(#bbox) | #cam. | label | Res. | Eval |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 时间 | #ID | #轨迹（#边框） | #摄像头 | 标签 | 分辨率 | 评估 |'
- en: '| PRID-2011 | 2011 | 200 | 400 (40k) | 2 | hand | fixed | CMC |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| PRID-2011 | 2011 | 200 | 400 (40k) | 2 | hand | fixed | CMC |'
- en: '| iLIDS-VID | 2014 | 300 | 600 (44k) | 2 | hand | vary | CMC |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| iLIDS-VID | 2014 | 300 | 600 (44k) | 2 | hand | vary | CMC |'
- en: '| MARS | 2016 | 1261 | 20,715 (1M) | 6 | auto | fixed | C&M |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| MARS | 2016 | 1261 | 20,715 (1M) | 6 | auto | fixed | C&M |'
- en: '| Duke-Video | 2018 | 1,812 | 4,832 (-) | 8 | auto | fixed | C&M |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Duke-Video | 2018 | 1,812 | 4,832 (-) | 8 | auto | fixed | C&M |'
- en: '| Duke-Tracklet | 2018 | 1,788 | 12,647 (-) | 8 | auto | C&M |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Duke-Tracklet | 2018 | 1,788 | 12,647 (-) | 8 | auto | C&M |  |'
- en: '| LPW | 2018 | 2,731 | 7,694(590K) | 4 | auto | fixed | C&M |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| LPW | 2018 | 2,731 | 7,694(590K) | 4 | auto | fixed | C&M |'
- en: '| LS-VID | 2019 | 3,772 | 14,943 (3M) | 15 | auto | fixed | C&M |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| LS-VID | 2019 | 3,772 | 14,943 (3M) | 15 | auto | fixed | C&M |'
- en: '![Refer to caption](img/973b82d3e0c9e8ecdbc94818fa66986c.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/973b82d3e0c9e8ecdbc94818fa66986c.png)'
- en: (a) SOTA on Market-1501 [[5](#bib.bib5)]
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Market-1501上的SOTA [[5](#bib.bib5)]
- en: '![Refer to caption](img/952cca198b6cb85c5e41735da282c6b9.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/952cca198b6cb85c5e41735da282c6b9.png)'
- en: (b) SOTA on DukeMTMC [[42](#bib.bib42)]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (b) DukeMTMC上的SOTA [[42](#bib.bib42)]
- en: '![Refer to caption](img/af781fd5784ad61de06b9a38dbc41b72.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/af781fd5784ad61de06b9a38dbc41b72.png)'
- en: (c) SOTA on CUHK03 [[43](#bib.bib43)]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (c) CUHK03上的SOTA [[43](#bib.bib43)]
- en: '![Refer to caption](img/9d57826842e829251f255d85f2d07bd8.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9d57826842e829251f255d85f2d07bd8.png)'
- en: (d) SOTA on MSMT17 [[44](#bib.bib44)]
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: (d) MSMT17上的SOTA [[44](#bib.bib44)]
- en: 'Figure 5: State-of-the-arts (SOTA) on four image-based person Re-ID datasets.
    Both the Rank-1 accuracy (%) and mAP value (%) are reported. For CUHK03 [[43](#bib.bib43)],
    the detected data under the setting [[58](#bib.bib58)] is reported. For Market-1501,
    the single query setting is used. The best result is highlighted with a red star.
    All the listed results do not use re-ranking or additional annotated information.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：四个基于图像的人脸再识别数据集的最先进技术（SOTA）。报告了Rank-1准确率（%）和mAP值（%）。对于CUHK03 [[43](#bib.bib43)]，报告了设置
    [[58](#bib.bib58)] 下的检测数据。对于Market-1501，使用了单查询设置。最佳结果以红星突出显示。所有列出的结果均未使用重新排序或额外标注信息。
- en: 2.4 Datasets and Evaluation
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 数据集和评估
- en: 2.4.1 Datasets and Evaluation Metrics
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 数据集和评估指标
- en: 'Datasets. We first review the widely used datasets for the closed-world setting,
    including 11 image datasets (VIPeR [[31](#bib.bib31)], iLIDS [[167](#bib.bib167)],
    GRID [[168](#bib.bib168)], PRID2011 [[126](#bib.bib126)], CUHK01-03 [[43](#bib.bib43)],
    Market-1501 [[5](#bib.bib5)], DukeMTMC [[42](#bib.bib42)], Airport [[169](#bib.bib169)]
    and MSMT17 [[44](#bib.bib44)]) and 7 video datasets (PRID-2011 [[126](#bib.bib126)],
    iLIDS-VID [[7](#bib.bib7)], MARS [[8](#bib.bib8)], Duke-Video [[144](#bib.bib144)],
    Duke-Tracklet [[170](#bib.bib170)], LPW [[171](#bib.bib171)] and LS-VID [[136](#bib.bib136)]).
    The statistics of these datasets are shown in Table [II](#S2.T2 "TABLE II ‣ 2.3.2
    Rank Fusion ‣ 2.3 Ranking Optimization ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook"). This survey
    only focuses on the general large-scale datsets for deep learning methods. A comprehensive
    summarization of the Re-ID datasets can be found in [[169](#bib.bib169)] and their
    website¹¹1[https://github.com/NEU-Gou/awesome-reid-dataset](https://github.com/NEU-Gou/awesome-reid-dataset).
    Several observations can be made in terms of the dataset collection over recent
    years:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集。我们首先回顾了用于封闭世界设置的广泛使用的数据集，包括11个图像数据集（VIPeR [[31](#bib.bib31)], iLIDS [[167](#bib.bib167)],
    GRID [[168](#bib.bib168)], PRID2011 [[126](#bib.bib126)], CUHK01-03 [[43](#bib.bib43)],
    Market-1501 [[5](#bib.bib5)], DukeMTMC [[42](#bib.bib42)], Airport [[169](#bib.bib169)]
    和 MSMT17 [[44](#bib.bib44)]) 和7个视频数据集（PRID-2011 [[126](#bib.bib126)], iLIDS-VID
    [[7](#bib.bib7)], MARS [[8](#bib.bib8)], Duke-Video [[144](#bib.bib144)], Duke-Tracklet
    [[170](#bib.bib170)], LPW [[171](#bib.bib171)] 和 LS-VID [[136](#bib.bib136)]）。这些数据集的统计信息见表
    [II](#S2.T2 "TABLE II ‣ 2.3.2 Rank Fusion ‣ 2.3 Ranking Optimization ‣ 2 Closed-world
    Person Re-Identification ‣ Deep Learning for Person Re-identification: A Survey
    and Outlook")。本调查仅关注深度学习方法的通用大规模数据集。关于Re-ID数据集的全面总结可以在 [[169](#bib.bib169)] 和他们的网站¹¹1[https://github.com/NEU-Gou/awesome-reid-dataset](https://github.com/NEU-Gou/awesome-reid-dataset)中找到。近年来的数据集收集可以做出几项观察：'
- en: '1) The dataset scale (both #image and #ID) has increased rapidly. Generally,
    the deep learning approach can benefit from more training samples. This also increases
    the annotation difficulty needed in closed-world person Re-ID. 2) The camera number
    is also greatly increased to approximate the large-scale camera network in practical
    scenarios. This also introduces additional challenges for model generalizability
    in a dynamically updated network. 3) The bounding boxes generation is usually
    performed automatically detected/tracked, rather than mannually cropped. This
    simulates the real-world scenario with tracking/detection errors.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 数据集规模（包括#image和#ID）迅速增长。通常，深度学习方法可以从更多的训练样本中受益。这也增加了在封闭世界人脸再识别中所需的标注难度。2)
    摄像头数量也大幅增加，以接近实际场景中的大规模摄像头网络。这也给动态更新的网络模型的泛化能力带来了额外挑战。3) 边界框生成通常是通过自动检测/跟踪完成的，而不是手动裁剪的。这模拟了现实世界中的跟踪/检测误差。
- en: Evaluation Metrics. To evaluate a Re-ID system, Cumulative Matching Characteristics
    (CMC) [[68](#bib.bib68)] and mean Average Precision (mAP) [[5](#bib.bib5)] are
    two widely used measurements.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。要评估一个Re-ID系统，累计匹配特征（CMC）[[68](#bib.bib68)] 和均值平均精度（mAP）[[5](#bib.bib5)]
    是两种广泛使用的测量方法。
- en: CMC-$k$ (a.k.a, Rank-$k$ matching accuracy) [[68](#bib.bib68)] represents the
    probability that a correct match appears in the top-$k$ ranked retrieved results.
    CMC is accurate when only one ground truth exists for each query, since it only
    considers the first match in evaluation process. However, the gallery set usually
    contains multiple groundtruths in a large camera network, and CMC cannot completely
    reflect the discriminability of a model across multiple cameras.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: CMC-$k$（也称为Rank-$k$匹配准确率）[[68](#bib.bib68)]表示正确匹配出现在前-$k$ 排名结果中的概率。由于CMC只在评估过程中考虑第一个匹配，因此当每个查询只有一个真实值时，CMC是准确的。然而，画廊集合通常包含多个真实值，在大型摄像头网络中，CMC不能完全反映模型在多个摄像头中的区分能力。
- en: 'Another metric, *i.e*., mean Average Precision (mAP) [[5](#bib.bib5)], measures
    the average retrieval performance with multiple grountruths. It is originally
    widely used in image retrieval. For Re-ID evaluation, it can address the issue
    of two systems performing equally well in searching the first ground truth (might
    be easy match as in Fig. [4](#S2.F4 "Figure 4 ‣ 2.2.2 Training strategy ‣ 2.2
    Deep Metric Learning ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")), but having different retrieval
    abilities for other hard matches.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '另一项指标，即平均精度均值（mAP）[[5](#bib.bib5)]，衡量了具有多个真实值的平均检索性能。它最初广泛用于图像检索。在Re-ID评估中，它可以解决两个系统在搜索第一个真实值时表现相同（可能是简单匹配，如图 [4](#S2.F4
    "Figure 4 ‣ 2.2.2 Training strategy ‣ 2.2 Deep Metric Learning ‣ 2 Closed-world
    Person Re-Identification ‣ Deep Learning for Person Re-identification: A Survey
    and Outlook")），但在其他困难匹配上具有不同检索能力的问题。'
- en: Considering the efficiency and complexity of training a Re-ID model, some recent
    works [[138](#bib.bib138), [139](#bib.bib139)] also report the FLoating-point
    Operations Per second (FLOPs) and the network parameter size as the evaluation
    metrics. These two metrics are crucial when the training/testing device has limited
    computational resources.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到训练Re-ID模型的效率和复杂性，一些最新的工作[[138](#bib.bib138), [139](#bib.bib139)]还报告了每秒浮点运算次数（FLOPs）和网络参数大小作为评估指标。这两个指标在训练/测试设备计算资源有限的情况下至关重要。
- en: 2.4.2 In-depth Analysis on State-of-The-Arts
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 对最新技术的深入分析
- en: We review the state-of-the-arts from both image-based and video-based perspectives.
    We include methods published in top CV venues over the past three years.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从基于图像和基于视频的角度回顾了最新技术。我们包括了过去三年在顶级计算机视觉会议上发表的方法。
- en: 'Image-based Re-ID. There are a large number of published papers for image-based
    Re-ID²²2[https://paperswithcode.com/task/person-re-identification](https://paperswithcode.com/task/person-re-identification).
    We mainly review the works published in 2019 as well as some representative works
    in 2018\. Specifically, we include PCB [[77](#bib.bib77)], MGN [[172](#bib.bib172)],
    PyrNet [[6](#bib.bib6)], Auto-ReID [[139](#bib.bib139)], ABD-Net [[173](#bib.bib173)],
    BagTricks [[122](#bib.bib122)], OSNet [[138](#bib.bib138)], DGNet [[119](#bib.bib119)],
    SCAL [[90](#bib.bib90)], MHN [[174](#bib.bib174)], P2Net [[104](#bib.bib104)],
    BDB [[124](#bib.bib124)], SONA [[107](#bib.bib107)], SFT [[95](#bib.bib95)], ConsAtt
    [[93](#bib.bib93)], DenseS [[103](#bib.bib103)], Pyramid [[156](#bib.bib156)],
    IANet [[108](#bib.bib108)], VAL [[114](#bib.bib114)]. We summarize the results
    on four datasets (Fig. [5](#S2.F5 "Figure 5 ‣ 2.3.2 Rank Fusion ‣ 2.3 Ranking
    Optimization ‣ 2 Closed-world Person Re-Identification ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook")). This overview motivates five major
    insights, as discussed below.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '基于图像的重识别（Re-ID）。有大量关于基于图像的重识别的已发布论文²²2[https://paperswithcode.com/task/person-re-identification](https://paperswithcode.com/task/person-re-identification)。我们主要回顾了2019年发布的工作，以及2018年的一些具有代表性的工作。具体来说，我们包括了PCB
    [[77](#bib.bib77)]、MGN [[172](#bib.bib172)]、PyrNet [[6](#bib.bib6)]、Auto-ReID
    [[139](#bib.bib139)]、ABD-Net [[173](#bib.bib173)]、BagTricks [[122](#bib.bib122)]、OSNet
    [[138](#bib.bib138)]、DGNet [[119](#bib.bib119)]、SCAL [[90](#bib.bib90)]、MHN [[174](#bib.bib174)]、P2Net
    [[104](#bib.bib104)]、BDB [[124](#bib.bib124)]、SONA [[107](#bib.bib107)]、SFT [[95](#bib.bib95)]、ConsAtt
    [[93](#bib.bib93)]、DenseS [[103](#bib.bib103)]、Pyramid [[156](#bib.bib156)]、IANet
    [[108](#bib.bib108)]、VAL [[114](#bib.bib114)]。我们总结了在四个数据集上的结果（图 [5](#S2.F5 "Figure
    5 ‣ 2.3.2 Rank Fusion ‣ 2.3 Ranking Optimization ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook")）。这个概述引发了五个主要见解，下面将讨论这些见解。'
- en: First, with the advancement of deep learning, most of the image-based Re-ID
    methods have achieved higher rank-1 accuracy than humans (93.5% [[175](#bib.bib175)])
    on the widely used Market-1501 dataset. In particular, VAL [[114](#bib.bib114)]
    obtains the best mAP of 91.6% and Rank-1 accuracy of 96.2% on Market-1501 dataset.
    The major advantage of VAL is the usage of viewpoint information. The performance
    can be further improved when using re-ranking or metric fusion. The success of
    deep learning on these closed-world datasets also motivates the shift focus to
    more challenging scenarios, *i.e*., large data size [[136](#bib.bib136)] or unsupervised
    learning [[176](#bib.bib176)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 第一，随着深度学习的发展，大多数基于图像的Re-ID方法在广泛使用的Market-1501数据集上已实现了比人类更高的Rank-1准确率（93.5% [[175](#bib.bib175)]）。特别是，VAL
    [[114](#bib.bib114)] 在Market-1501数据集上获得了最佳的mAP（91.6%）和Rank-1准确率（96.2%）。VAL的主要优势在于使用了视角信息。使用重新排序或度量融合可以进一步提高性能。深度学习在这些封闭世界数据集上的成功也激发了向更具挑战性的场景转移关注，如大数据量
    [[136](#bib.bib136)] 或无监督学习 [[176](#bib.bib176)]。
- en: Second, part-level feature learning is beneficial for discriminative Re-ID model
    learning. Global feature learning directly learns the representation on the whole
    image without the part constraints [[122](#bib.bib122)]. It is discriminative
    when the person detection/ tracking can accurately locate the human body. When
    the person images suffer from large background clutter or heavy occlusions, part-level
    feature learning usually achieves better performance by mining discriminative
    body regions [[67](#bib.bib67)]. Due to its advantage in handling misalignment/occlusions,
    we observe that most of the state-of-the-art methods developed recently adopt
    the features aggregation paradigm, combining the part-level and full human body
    features [[139](#bib.bib139), [156](#bib.bib156)].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，部分级特征学习对判别性Re-ID模型的学习是有益的。全局特征学习直接学习整个图像上的表示，而不受部分约束 [[122](#bib.bib122)]。当人脸检测/跟踪能够准确定位人体时，全局特征学习具有判别性。当人物图像遭遇大量背景杂乱或严重遮挡时，部分级特征学习通常通过挖掘判别性身体区域
    [[67](#bib.bib67)] 实现更好的性能。由于其在处理对齐/遮挡方面的优势，我们观察到大多数最近开发的最先进方法采用了特征聚合范式，结合了部分级和全身特征
    [[139](#bib.bib139)]、[[156](#bib.bib156)]。
- en: Third, attention is beneficial for discriminative Re-ID model learning. We observe
    that all the methods (ConsAtt [[93](#bib.bib93)], SCAL [[90](#bib.bib90)], SONA
    [[107](#bib.bib107)], ABD-Net [[173](#bib.bib173)]) achieving the best performance
    on each dataset adopt an attention scheme. The attention captures the relationship
    between different convolutional channels, multiple feature maps, hierarchical
    layers, different body parts/regions, and even multiple images. Meanwhile, discriminative
    [[173](#bib.bib173)], diverse [[133](#bib.bib133)], consistent [[93](#bib.bib93)]
    and high-order [[107](#bib.bib107)] properties are incorporated to enhance the
    attentive feature learning. Considering the powerful attention schemes and the
    specificity of the Re-ID problem, it is highly possible that attentive deeply
    learned systems will continue dominating the Re-ID community, with more domain
    specific properties.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，注意力机制对判别性Re-ID模型的学习是有益的。我们观察到所有在每个数据集上实现最佳性能的方法（ConsAtt [[93](#bib.bib93)]、SCAL
    [[90](#bib.bib90)]、SONA [[107](#bib.bib107)]、ABD-Net [[173](#bib.bib173)]）都采用了注意力方案。注意力机制捕捉不同卷积通道、多个特征图、层次结构、不同身体部位/区域，甚至多个图像之间的关系。同时，判别性
    [[173](#bib.bib173)]、多样性 [[133](#bib.bib133)]、一致性 [[93](#bib.bib93)] 和高阶 [[107](#bib.bib107)]
    特性被整合以增强注意力特征学习。考虑到强大的注意力机制和Re-ID问题的特性，注意力深度学习系统可能会继续主导Re-ID领域，并具备更多领域特定特性。
- en: Fourth, multi-loss training can improve the Re-ID model learning. Different
    loss functions optimize the network from a multi-view perspective. Combining multiple
    loss functions can improve the performance, evidenced by the multi-loss training
    strategy in the state-of-the-art methods, including ConsAtt [[93](#bib.bib93)],
    ABD-Net [[173](#bib.bib173)] and SONA [[107](#bib.bib107)]. In addition, a dynamic
    multi-loss training strategy is designed in [[156](#bib.bib156)] to adaptively
    integrated two loss functions. The combination of identity loss and triplet loss
    with hard mining is the primary choice. Moreover, due to the imbalanced issue,
    sample weighting strategy generally improves the performance by mining informative
    triplets [[89](#bib.bib89), [52](#bib.bib52)].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，多损失训练可以提高Re-ID模型的学习能力。不同的损失函数从多视角优化网络。结合多种损失函数可以提升性能，这在先进方法中的多损失训练策略中有所体现，包括ConsAtt
    [[93](#bib.bib93)]、ABD-Net [[173](#bib.bib173)]和SONA [[107](#bib.bib107)]。此外，[[156](#bib.bib156)]中设计了一种动态多损失训练策略，以自适应地整合两个损失函数。身份损失和三元组损失结合困难挖掘是主要选择。此外，由于不平衡问题，样本加权策略通常通过挖掘有用的三元组来改善性能
    [[89](#bib.bib89), [52](#bib.bib52)]。
- en: 'Finally, there is still much room for further improvement due to the increasing
    size of datasets, complex environment, limited training samples. For example,
    the Rank-1 accuracy (82.3%) and mAP (60.8%) on the newly released MSMT17 dataset
    [[44](#bib.bib44)] are much lower than that on Market-1501 (Rank-1: 96.2% and
    mAP 91.7%) and DukeMTMC (Rank-1: 91.6% and mAP 84.5%). On some other challenging
    datasets with limited training samples (*e.g*., GRID [[168](#bib.bib168)] and
    VIPeR [[31](#bib.bib31)]), the performance is still very low. In addition, Re-ID
    models usually suffers significantly on cross-dataset evaluation [[54](#bib.bib54),
    [28](#bib.bib28)], and the performance drops dramatically under adversarial attack
    [[177](#bib.bib177)]. We are optimistic that there would be important breakthroughs
    in person Re-ID, with increasing discriminability, robustness, and generalizability.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，由于数据集规模的不断增加、环境复杂以及训练样本有限，仍有很大的进一步改进空间。例如，新的MSMT17数据集 [[44](#bib.bib44)]上的Rank-1准确率（82.3%）和mAP（60.8%）远低于Market-1501（Rank-1:
    96.2%和mAP 91.7%）和DukeMTMC（Rank-1: 91.6%和mAP 84.5%）。在一些具有挑战性的有限训练样本的数据集上（*例如*，GRID
    [[168](#bib.bib168)]和VIPeR [[31](#bib.bib31)]），性能仍然非常低。此外，Re-ID模型通常在跨数据集评估中表现显著差，且在对抗攻击下性能急剧下降
    [[177](#bib.bib177)]。我们对Re-ID领域的未来持乐观态度，期待其在辨别能力、鲁棒性和泛化能力方面取得重要突破。'
- en: '![Refer to caption](img/ec16a18fd44874eeb2c0ac9c1ec30b7c.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ec16a18fd44874eeb2c0ac9c1ec30b7c.png)'
- en: (a) SOTA on PRID-2011 [[126](#bib.bib126)]
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PRID-2011上的SOTA [[126](#bib.bib126)]
- en: '![Refer to caption](img/6d821ecdf162bd48e43abc632dc9b582.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6d821ecdf162bd48e43abc632dc9b582.png)'
- en: (b) SOTA on iLIDS-VID [[7](#bib.bib7)]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: (b) iLIDS-VID上的SOTA [[7](#bib.bib7)]
- en: '![Refer to caption](img/d37908c27e2b84de9e996e9a5db786d2.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d37908c27e2b84de9e996e9a5db786d2.png)'
- en: (c) SOTA on MARS [[8](#bib.bib8)]
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (c) MARS上的SOTA [[8](#bib.bib8)]
- en: '![Refer to caption](img/a0d7019dc27a3a030d067cb975ab1b4e.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a0d7019dc27a3a030d067cb975ab1b4e.png)'
- en: (d) SOTA on Duke-Video [[144](#bib.bib144)]
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Duke-Video上的SOTA [[144](#bib.bib144)]
- en: 'Figure 6: State-of-the-arts (SOTA) on four widely used video-based person Re-ID
    datasets. The Rank-1 accuracies (%) over years are reported. mAP values (%) on
    MARS [[8](#bib.bib8)] and Duke-Video [[144](#bib.bib144)] are reported. For Duke-Video,
    we refer to the settings in [[144](#bib.bib144)]. The best result is highlighted
    with a red star. All the listed results do not use re-ranking or additional annotated
    information.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：四个广泛使用的视频基础Re-ID数据集上的最新技术（SOTA）。报告了不同年份的Rank-1准确率（%）。报告了MARS [[8](#bib.bib8)]和Duke-Video
    [[144](#bib.bib144)]上的mAP值（%）。对于Duke-Video，我们参考了[[144](#bib.bib144)]中的设置。最佳结果用红星标出。所有列出的结果都没有使用重新排序或额外的标注信息。
- en: 'Video-based Re-ID. Video-based Re-ID has received less interest, compared to
    image-based Re-ID. We review the deeply learned Re-ID models, including CoSeg
    [[132](#bib.bib132)], GLTR [[136](#bib.bib136)], STA [[135](#bib.bib135)], ADFD
    [[110](#bib.bib110)], STC [[20](#bib.bib20)], DRSA [[133](#bib.bib133)], Snippet
    [[134](#bib.bib134)], ETAP [[144](#bib.bib144)], DuATM [[91](#bib.bib91)], SDM
    [[178](#bib.bib178)], TwoS [[128](#bib.bib128)], ASTPN [[131](#bib.bib131)], RQEN
    [[171](#bib.bib171)], Forest [[130](#bib.bib130)], RNN [[127](#bib.bib127)] and
    IDEX [[8](#bib.bib8)]. We also summarize the results on four video Re-ID datasets,
    as shown in Fig. [6](#S2.F6 "Figure 6 ‣ 2.4.2 In-depth Analysis on State-of-The-Arts
    ‣ 2.4 Datasets and Evaluation ‣ 2 Closed-world Person Re-Identification ‣ Deep
    Learning for Person Re-identification: A Survey and Outlook"). From these results,
    the following observations can be drawn.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '视频基础Re-ID。与基于图像的Re-ID相比，视频基础Re-ID的关注度较少。我们回顾了深度学习Re-ID模型，包括CoSeg [[132](#bib.bib132)]、GLTR
    [[136](#bib.bib136)]、STA [[135](#bib.bib135)]、ADFD [[110](#bib.bib110)]、STC [[20](#bib.bib20)]、DRSA
    [[133](#bib.bib133)]、Snippet [[134](#bib.bib134)]、ETAP [[144](#bib.bib144)]、DuATM
    [[91](#bib.bib91)]、SDM [[178](#bib.bib178)]、TwoS [[128](#bib.bib128)]、ASTPN [[131](#bib.bib131)]、RQEN
    [[171](#bib.bib171)]、Forest [[130](#bib.bib130)]、RNN [[127](#bib.bib127)]和IDEX
    [[8](#bib.bib8)]。我们还总结了四个视频Re-ID数据集上的结果，如图[6](#S2.F6 "Figure 6 ‣ 2.4.2 In-depth
    Analysis on State-of-The-Arts ‣ 2.4 Datasets and Evaluation ‣ 2 Closed-world Person
    Re-Identification ‣ Deep Learning for Person Re-identification: A Survey and Outlook")所示。从这些结果中可以得出以下观察。'
- en: First, a clear trend of increasing performance can be seen over the years with
    the development of deep learning techniques. Specifically, the Rank-1 accuracy
    increases from 70% (RNN [[127](#bib.bib127)] in 2016) to 95.5% (GLTR [[136](#bib.bib136)]
    in 2019) on PRID-2011 dataset, and from 58% (RNN [[127](#bib.bib127)]) to 86.3%
    (ADFD [[110](#bib.bib110)]) on iLIDS-VID dataset. On the large-scale MARS dataset,
    the Rank-1 accuracy/mAP increase from 68.3%/49.3% (IDEX [[8](#bib.bib8)]) to 88.5%/82.3%
    (STC [[20](#bib.bib20)]). On the Duke-Video dataset [[144](#bib.bib144)], STA
    [[135](#bib.bib135)] also achieves a Rank-1 accuracy of 96.2%, and the mAP is
    94.9%.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，可以看到随着深度学习技术的发展，性能有着明显的提升趋势。具体而言，在PRID-2011数据集上，Rank-1准确率从2016年的70%（RNN [[127](#bib.bib127)]）提升到2019年的95.5%（GLTR
    [[136](#bib.bib136)]），在iLIDS-VID数据集上的提升从58%（RNN [[127](#bib.bib127)]）到86.3%（ADFD
    [[110](#bib.bib110)]）。在大规模的MARS数据集上，Rank-1准确率/mAP从68.3%/49.3%（IDEX [[8](#bib.bib8)]）提高到88.5%/82.3%（STC
    [[20](#bib.bib20)]）。在Duke-Video数据集 [[144](#bib.bib144)]上，STA [[135](#bib.bib135)]也实现了96.2%的Rank-1准确率，mAP为94.9%。
- en: Second, spatial and temporal modeling is crucial for discriminative video representation
    learning. We observe that all the methods (STA [[135](#bib.bib135)], STC [[20](#bib.bib20)],
    GLTR [[136](#bib.bib136)]) design spatial-temporal aggregation strategies to improve
    the video Re-ID performance. Similar to image-based Re-ID, the attention scheme
    across multiple frames [[135](#bib.bib135), [110](#bib.bib110)] also greatly enhances
    the discriminability. Another interesting observation in [[20](#bib.bib20)] demonstrates
    that utilizing multiple frames within the video sequence can fill in the occluded
    regions, which provides a possible solution for handling the challenging occlusion
    problem in the future.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，空间和时间建模对于区分性视频表示学习至关重要。我们观察到所有的方法（STA [[135](#bib.bib135)]、STC [[20](#bib.bib20)]、GLTR
    [[136](#bib.bib136)]）都设计了空间-时间聚合策略以提升视频Re-ID的表现。类似于基于图像的Re-ID，跨多帧的注意力机制 [[135](#bib.bib135)、[110](#bib.bib110)]
    也大大增强了区分能力。在 [[20](#bib.bib20)] 中另一个有趣的观察表明，利用视频序列中的多帧可以填补遮挡区域，这为处理未来挑战性的遮挡问题提供了可能的解决方案。
- en: Finally, the performance on these datases has reached a saturation state, usually
    about less than 1% accuracy gain on these four video datasets. However, there
    is still large room for improvements on the challenging cases. For example, on
    the newly collected video dataset, LS-VID [[136](#bib.bib136)], the Rank-1 accuracy/mAP
    of GLTR [[136](#bib.bib136)] are only 63.1%/44.43%, while GLTR [[136](#bib.bib136)]
    can achieve state-of-the-art or at least comparable performance on the other four
    daatsets. LS-VID [[136](#bib.bib136)] contains significantly more identities and
    video sequences. This provides a challenging benchmark for future breakthroughs
    in video based Re-ID.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这些数据集上的表现已经达到了饱和状态，通常在这四个视频数据集上的准确率提升不到1%。然而，对于挑战性更大的案例仍有很大的改进空间。例如，在新收集的视频数据集LS-VID
    [[136](#bib.bib136)]上，GLTR [[136](#bib.bib136)] 的Rank-1准确率/mAP仅为63.1%/44.43%，而GLTR
    [[136](#bib.bib136)] 在其他四个数据集上能够实现最先进的或至少是可比的性能。LS-VID [[136](#bib.bib136)]包含了显著更多的身份和视频序列，这为未来视频基础Re-ID的突破提供了具有挑战性的基准。
- en: 3 Open-world Person Re-Identification
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 开放世界人物再识别
- en: 'This section reviews open-world person Re-ID as discussed in § [1](#S1 "1 Introduction
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook"), including
    heterogeneous Re-ID by matching person images across heterogeneous modalities
    (§ [3.1](#S3.SS1 "3.1 Heterogeneous Re-ID ‣ 3 Open-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook")), end-to-end
    Re-ID from the raw images/videos (§ [3.2](#S3.SS2 "3.2 End-to-End Re-ID ‣ 3 Open-world
    Person Re-Identification ‣ Deep Learning for Person Re-identification: A Survey
    and Outlook")), semi-/unsupervised learning with limited/unavailable annotated
    labels (§ [3.3](#S3.SS3 "3.3 Semi-supervised and Unsupervised Re-ID ‣ 3 Open-world
    Person Re-Identification ‣ Deep Learning for Person Re-identification: A Survey
    and Outlook")), robust Re-ID model learning with noisy annotations (§ [3.4](#S3.SS4
    "3.4 Noise-Robust Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")) and open-set person Re-ID
    when the correct match does not occur in the gallery (§ [3.5](#S3.SS5 "3.5 Open-set
    Re-ID and Beyond ‣ 3 Open-world Person Re-Identification ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook")).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了开放世界人物再识别的内容，讨论了§ [1](#S1 "1 介绍 ‣ 深度学习在人物再识别中的应用：综述与展望")中提到的内容，包括通过跨异质模态匹配人物图像的异质再识别
    (§ [3.1](#S3.SS1 "3.1 异质再识别 ‣ 3 开放世界人物再识别 ‣ 深度学习在人物再识别中的应用：综述与展望"))，从原始图像/视频进行端到端再识别
    (§ [3.2](#S3.SS2 "3.2 端到端再识别 ‣ 3 开放世界人物再识别 ‣ 深度学习在人物再识别中的应用：综述与展望"))，有限/不可用标注标签下的半监督/无监督学习
    (§ [3.3](#S3.SS3 "3.3 半监督和无监督再识别 ‣ 3 开放世界人物再识别 ‣ 深度学习在人物再识别中的应用：综述与展望"))，带有噪声标注的鲁棒再识别模型学习
    (§ [3.4](#S3.SS4 "3.4 噪声鲁棒再识别 ‣ 3 开放世界人物再识别 ‣ 深度学习在人物再识别中的应用：综述与展望"))，以及当正确匹配不在图库中时的开放集人物再识别
    (§ [3.5](#S3.SS5 "3.5 开放集再识别及其他 ‣ 3 开放世界人物再识别 ‣ 深度学习在人物再识别中的应用：综述与展望"))。
- en: 3.1 Heterogeneous Re-ID
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 异质再识别
- en: 'This subsection summarizes four main kinds of heterogeneous Re-ID, including
    Re-ID between depth and RGB images (§ [3.1.1](#S3.SS1.SSS1 "3.1.1 Depth-based
    Re-ID ‣ 3.1 Heterogeneous Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep
    Learning for Person Re-identification: A Survey and Outlook")), text-to-image
    Re-ID (§ [3.1.2](#S3.SS1.SSS2 "3.1.2 Text-to-Image Re-ID ‣ 3.1 Heterogeneous Re-ID
    ‣ 3 Open-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")), visible-to-infrared Re-ID (§ [3.1.3](#S3.SS1.SSS3 "3.1.3
    Visible-Infrared Re-ID ‣ 3.1 Heterogeneous Re-ID ‣ 3 Open-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook")) and cross
    resolution Re-ID (§ [3.1.4](#S3.SS1.SSS4 "3.1.4 Cross-Resolution Re-ID ‣ 3.1 Heterogeneous
    Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节总结了四种主要的异质再识别方法，包括深度图像与RGB图像之间的再识别 (§ [3.1.1](#S3.SS1.SSS1 "3.1.1 基于深度的再识别
    ‣ 3.1 异质再识别 ‣ 3 开放世界人物再识别 ‣ 深度学习在人物再识别中的应用：综述与展望"))，文本到图像的再识别 (§ [3.1.2](#S3.SS1.SSS2
    "3.1.2 文本到图像再识别 ‣ 3.1 异质再识别 ‣ 3 开放世界人物再识别 ‣ 深度学习在人物再识别中的应用：综述与展望"))，可见光到红外的再识别
    (§ [3.1.3](#S3.SS1.SSS3 "3.1.3 可见光-红外再识别 ‣ 3.1 异质再识别 ‣ 3 开放世界人物再识别 ‣ 深度学习在人物再识别中的应用：综述与展望"))和跨分辨率再识别
    (§ [3.1.4](#S3.SS1.SSS4 "3.1.4 跨分辨率再识别 ‣ 3.1 异质再识别 ‣ 3 开放世界人物再识别 ‣ 深度学习在人物再识别中的应用：综述与展望"))。
- en: 3.1.1 Depth-based Re-ID
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 基于深度的再识别
- en: Depth images capture the body shape and skeleton information. This provides
    the possibility for Re-ID under illumination/clothes changing environments, which
    is also important for personalized human interaction applications.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图像捕捉了身体形状和骨架信息。这为在光照/服装变化环境下进行再识别提供了可能，这对个性化人机交互应用也很重要。
- en: A recurrent attention-based model is proposed in [[179](#bib.bib179)] to address
    the depth-based person identification. In a reinforcement learning framework,
    they combine the convolutional and recurrent neural networks to identify small,
    discriminative local regions of the human body.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[[179](#bib.bib179)]中提出了一种基于递归注意力的模型来解决基于深度的人物识别问题。在强化学习框架下，他们结合了卷积神经网络和递归神经网络，以识别人体的小而具区分性的局部区域。'
- en: Karianakis et al. [[180](#bib.bib180)] leverage the large RGB datasets to design
    a split-rate RGB-to-Depth transfer method, which bridges the gap between the depth
    images and the RGB images. Their model further incorporates a temporal attention
    to enhance video representation for depth Re-ID.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Karianakis等人[[180](#bib.bib180)]利用大规模RGB数据集设计了一种分裂率RGB到深度转换方法，弥合了深度图像和RGB图像之间的差距。他们的模型进一步结合了时间注意力，以增强视频表示用于深度Re-ID。
- en: Some methods [[62](#bib.bib62), [181](#bib.bib181)] have also studied the combination
    of RGB and depth information to improve the Re-ID performance, addressing the
    clothes-changing challenge.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法[[62](#bib.bib62), [181](#bib.bib181)]也研究了RGB和深度信息的结合，以提高Re-ID性能，解决换衣挑战。
- en: 3.1.2 Text-to-Image Re-ID
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 文本到图像的重新识别（Re-ID）
- en: Text-to-image Re-ID addresses the matching between a text description and RGB
    images [[63](#bib.bib63)]. It is imperative when the visual image of query person
    cannot be obtained, and only a text description can be alternatively provided.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像的Re-ID解决了文本描述与RGB图像之间的匹配[[63](#bib.bib63)]。当无法获得查询对象的视觉图像时，且只能提供文本描述时，这一点尤为重要。
- en: A gated neural attention model [[63](#bib.bib63)] with recurrent neural network
    learns the shared features between the text description and the person images.
    This enables the end-to-end training for text to image pedestrian retrieval. Cheng
    et al. [[182](#bib.bib182)] propose a global discriminative image-language association
    learning method, capturing the identity discriminative information and local reconstructive
    image-language association under a reconstruction process. A cross projection
    learning method [[183](#bib.bib183)] also learns a shared space with image-to-text
    matching. A deep adversarial graph attention convolution network is designed in
    [[184](#bib.bib184)] with graph relation mining. However, the large semantic gap
    between the text descriptions and the visual images is still challenging. Meanwhile,
    how to combine the texts and hand-painting sketch image is also worth studying
    in the future.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一种带有递归神经网络的门控神经注意力模型[[63](#bib.bib63)]学习文本描述和人物图像之间的共享特征。这使得从文本到图像的行人检索可以进行端到端的训练。Cheng等人[[182](#bib.bib182)]提出了一种全球判别图像-语言关联学习方法，在重建过程中捕获身份判别信息和局部重建图像-语言关联。交叉投影学习方法[[183](#bib.bib183)]也学习了与图像到文本匹配共享的空间。[[184](#bib.bib184)]中设计了一种深度对抗图卷积网络，并进行图关系挖掘。然而，文本描述和视觉图像之间的语义差距仍然具有挑战性。同时，如何结合文本和手绘草图图像也值得未来研究。
- en: 3.1.3 Visible-Infrared Re-ID
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 可见-红外Re-ID
- en: Visible-Infrared Re-ID handles the cross-modality matching between the daytime
    visible and night-time infrared images. It is important in low-lighting conditions,
    where the images can only be captured by infrared cameras [[21](#bib.bib21), [60](#bib.bib60),
    [185](#bib.bib185)].
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 可见-红外Re-ID处理白天可见图像和夜晚红外图像之间的跨模态匹配。在低光照条件下，图像只能通过红外相机捕获[[21](#bib.bib21), [60](#bib.bib60),
    [185](#bib.bib185)]。
- en: Wu et al. [[21](#bib.bib21)] start the first attempt to address this issue,
    by proposing a deep zero-padding framework [[21](#bib.bib21)] to adaptively learn
    the modality sharable features. A two stream network is introduced in [[186](#bib.bib186),
    [142](#bib.bib142)] to model the modality-sharable and -specific information,
    addressing the intra- and cross-modality variations simultaneously. Besides the
    cross-modality shared embedding learning [[187](#bib.bib187)], the classifier-level
    discrepancy is also investigated in [[188](#bib.bib188)]. Recent methods [[189](#bib.bib189),
    [190](#bib.bib190)] adopt the GAN technique to generate cross-modality person
    images to reduce the cross-modality discrepancy at both image and feature level.
    Hierarchical cross-Modality disentanglement factors are modeled in [[191](#bib.bib191)].
    A dual-attentive aggregation learning method is presented in [[192](#bib.bib192)]
    to capture multi-level relations.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 吴等人[[21](#bib.bib21)]首次尝试解决这个问题，提出了一种深度零填充框架[[21](#bib.bib21)]以自适应地学习模态共享特征。在[[186](#bib.bib186),
    [142](#bib.bib142)]中引入了一个双流网络，以同时处理模态共享和特定信息，解决了模态内部和跨模态变化的问题。除了跨模态共享嵌入学习[[187](#bib.bib187)]外，分类器级差异也在[[188](#bib.bib188)]中进行了研究。最近的方法[[189](#bib.bib189),
    [190](#bib.bib190)]采用GAN技术生成跨模态人物图像，以减少图像和特征层面的跨模态差异。[[191](#bib.bib191)]中建模了分层跨模态解耦因子。[[192](#bib.bib192)]中提出了一种双重注意力聚合学习方法，以捕获多级关系。
- en: 3.1.4 Cross-Resolution Re-ID
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 跨分辨率Re-ID
- en: Cross-Resolution Re-ID conducts the matching between low-resolution and high-resolution
    images, addressing the large resolution variations [[13](#bib.bib13), [14](#bib.bib14)].
    A cascaded SR-GAN [[193](#bib.bib193)] generates the high-resolution person images
    in a cascaded manner, incorporating the identity information. Li et al. [[194](#bib.bib194)]
    adopt the adversarial learning technique to obtain resolution-invariant image
    representations.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 跨分辨率重识别在低分辨率和高分辨率图像之间进行匹配，解决了大分辨率变化问题[[13](#bib.bib13), [14](#bib.bib14)]。一个级联的SR-GAN[[193](#bib.bib193)]以级联方式生成高分辨率人员图像，结合了身份信息。Li等人[[194](#bib.bib194)]采用对抗学习技术来获得分辨率不变的图像表示。
- en: 3.2 End-to-End Re-ID
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 端到端重识别
- en: End-to-end Re-ID alleviates the reliance on additional step for bounding boxes
    generation. It involves the person Re-ID from raw images or videos, and multi-camera
    tracking.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端重识别减轻了对额外边界框生成步骤的依赖。它涉及从原始图像或视频中进行人员重识别以及多摄像头跟踪。
- en: Re-ID in Raw Images/Videos This task requires that the model jointly performs
    the person detection and re-identification in a single framework [[64](#bib.bib64),
    [55](#bib.bib55)]. It is challenging due to the different focuses of two major
    components.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像/视频中的重识别 该任务要求模型在一个框架中同时执行人员检测和重识别[[64](#bib.bib64), [55](#bib.bib55)]。由于两个主要组件的关注点不同，这一任务具有挑战性。
- en: Zheng et al. [[55](#bib.bib55)] present a two-stage framework, and systematically
    evaluate the benefits and limitations of person detection for the later stage
    person Re-ID. Xiao et al. [[64](#bib.bib64)] design an end-to-end person search
    system using a single convolutional neural network for joint person detection
    and re-identification. A Neural Person Search Machine (NPSM) [[195](#bib.bib195)]
    is developed to recursively refine the searching area and locate the target person
    by fully exploiting the contextual information between the query and the detected
    candidate region. Similarly, a contextual instance expansion module [[196](#bib.bib196)]
    is learned in a graph learning framework to improve the end-to-end person search.
    A query-guided end-to-end person search system [[197](#bib.bib197)] is developed
    using the Siamese squeeze-and-excitation network to capture the global context
    information with query-guided region proposal generation. A localization refinement
    scheme with discriminative Re-ID feature learning is introduced in [[198](#bib.bib198)]
    to generate more reliable bounding boxes. An Identity DiscriminativE Attention
    reinforcement Learning (IDEAL) method [[199](#bib.bib199)] selects informative
    regions for auto-generated bounding boxes, improving the Re-ID performance.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Zheng等人[[55](#bib.bib55)]提出了一个两阶段框架，并系统评估了人员检测对后续人员重识别的益处和局限性。Xiao等人[[64](#bib.bib64)]设计了一个端到端人员搜索系统，使用单一卷积神经网络进行联合人员检测和重识别。开发了一种神经人员搜索机器（NPSM）[[195](#bib.bib195)]，通过充分利用查询和检测候选区域之间的上下文信息，递归地优化搜索区域并定位目标人员。类似地，在图学习框架中学习了一个上下文实例扩展模块[[196](#bib.bib196)]，以改进端到端人员搜索。使用Siamese
    squeeze-and-excitation网络开发了一个查询引导的端到端人员搜索系统[[197](#bib.bib197)]，以捕捉全局上下文信息并生成查询引导的区域提案。[[198](#bib.bib198)]中引入了一种具有区分性重识别特征学习的定位优化方案，以生成更可靠的边界框。Identity
    DiscriminativE Attention强化学习（IDEAL）方法[[199](#bib.bib199)]选择信息丰富的区域用于自动生成的边界框，从而提升重识别性能。
- en: Yamaguchi et al. [[200](#bib.bib200)] investigate a more challenging problem,
    *i.e*., searching for the person from raw videos with text description. A multi-stage
    method with spatio-temporal person detection and multi-modal retrieval is proposed.
    Further exploration along this direction is expected.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Yamaguchi等人[[200](#bib.bib200)]研究了一个更具挑战性的问题，*即*从原始视频中搜索带有文本描述的人员。提出了一种具有时空人员检测和多模态检索的多阶段方法。预计在这个方向上的进一步探索。
- en: Multi-camera Tracking End-to-end person Re-ID is also closely related to multi-person,
    multi-camera tracking [[52](#bib.bib52)]. A graph-based formulation to link person
    hypotheses is proposed for multi-person tracking [[201](#bib.bib201)], where the
    holistic features of the full human body and body pose layout are combined as
    the representation for each person. Ristani et al. [[52](#bib.bib52)] learn the
    correlation between the multi-target multi-camera tracking and person Re-ID by
    hard-identity mining and adaptive weighted triplet learning. Recently, a locality
    aware appearance metric (LAAM) [[202](#bib.bib202)] with both intra- and inter-camera
    relation modeling is proposed.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 多摄像头跟踪端到端的人物Re-ID也与多人物、多摄像头跟踪密切相关[[52](#bib.bib52)]。提出了一种基于图的公式来链接人物假设，用于多人物跟踪[[201](#bib.bib201)]，其中结合了全身的整体特征和身体姿态布局作为每个人的表示。Ristani等人[[52](#bib.bib52)]通过硬身份挖掘和自适应加权三元组学习来学习多目标多摄像头跟踪与人物Re-ID之间的相关性。最近，提出了一种具有摄像头内和跨摄像头关系建模的局部感知外观度量（LAAM）[[202](#bib.bib202)]。
- en: 3.3 Semi-supervised and Unsupervised Re-ID
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 半监督和无监督Re-ID
- en: 3.3.1 Unsupervised Re-ID
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 无监督Re-ID
- en: Early unsupervised Re-ID mainly learns invariant components, *i.e*., dictionary
    [[203](#bib.bib203)], metric [[204](#bib.bib204)] or saliency [[66](#bib.bib66)],
    which leads to limited discriminability or scalability.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的无监督Re-ID主要学习不变的组件，即字典[[203](#bib.bib203)]、度量[[204](#bib.bib204)]或显著性[[66](#bib.bib66)]，这导致了有限的辨别能力或可扩展性。
- en: For deeply unsupervised methods, cross-camera label estimation is one the popular
    approaches [[176](#bib.bib176), [205](#bib.bib205)]. Dynamic graph matching (DGM)
    [[206](#bib.bib206)] formulates the label estimation as a bipartite graph matching
    problem. To further improve the performance, global camera network constraints
    [[207](#bib.bib207)] are exploited for consistent matching. Liu et al. progressively
    mine the labels with step-wise metric promotion [[204](#bib.bib204)]. A robust
    anchor embedding method [[83](#bib.bib83)] iteratively assigns labels to the unlabelled
    tracklets to enlarge the anchor video sequences set. With the estimated labels,
    deep learning can be applied to learn Re-ID models.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度无监督方法，跨摄像头标签估计是一种流行的方法[[176](#bib.bib176), [205](#bib.bib205)]。动态图匹配（DGM）[[206](#bib.bib206)]将标签估计公式化为一个二分图匹配问题。为了进一步提高性能，全球摄像头网络约束[[207](#bib.bib207)]被利用以实现一致匹配。刘等人通过逐步度量提升[[204](#bib.bib204)]逐步挖掘标签。一种稳健的锚点嵌入方法[[83](#bib.bib83)]迭代地将标签分配给无标签的tracklets，以扩大锚点视频序列集。通过估计的标签，可以应用深度学习来学习Re-ID模型。
- en: For end-to-end unsupervised Re-ID, an iterative clustering and Re-ID model learning
    is presented in [[205](#bib.bib205)]. Similarly, the relations among samples are
    utilized in a hierarchical clustering framework [[208](#bib.bib208)]. Soft multi-label
    learning [[209](#bib.bib209)] mines the soft label information from a reference
    set for unsupervised learning. A Tracklet Association Unsupervised Deep Learning
    (TAUDL) framework [[170](#bib.bib170)] jointly conducts the within-camera tracklet
    association and model the cross-camera tracklet correlation. Similarly, an unsupervised
    camera-aware similarity consistency mining method [[210](#bib.bib210)] is also
    presented in a coarse-to-fine consistency learning scheme. The intra-camera mining
    and inter-camera association is applied in a graph association framework [[211](#bib.bib211)].
    The semantic attributes are also adopted in Transferable Joint Attribute-Identity
    Deep Learning (TJ-AIDL) framework [[111](#bib.bib111)]. However, it is still challenging
    for model updating with newly arriving unlabelled data.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于端到端的无监督Re-ID，[[205](#bib.bib205)]提出了一种迭代聚类和Re-ID模型学习方法。同样，样本之间的关系在层次聚类框架中得到了利用[[208](#bib.bib208)]。软多标签学习[[209](#bib.bib209)]从参考集中挖掘软标签信息以进行无监督学习。一种Tracklet
    Association Unsupervised Deep Learning (TAUDL)框架[[170](#bib.bib170)]共同进行摄像头内的tracklet关联和建模跨摄像头的tracklet相关性。同样，在粗到细的一致性学习方案中也提出了一种无监督摄像头感知相似性一致性挖掘方法[[210](#bib.bib210)]。在图关联框架中应用了摄像头内部挖掘和跨摄像头关联[[211](#bib.bib211)]。在Transferable
    Joint Attribute-Identity Deep Learning (TJ-AIDL)框架[[111](#bib.bib111)]中也采用了语义属性。然而，对于新到达的无标签数据，模型更新仍然具有挑战性。
- en: Besides, several methods have also tried to learn a part-level representation
    based on the observation that it is easier to mine the label information in local
    parts than that of a whole image. A PatchNet [[153](#bib.bib153)] is designed
    to learn discriminative patch features by mining patch level similarity. A Self-similarity
    Grouping (SSG) approach [[212](#bib.bib212)] iteratively conducts grouping (exploits
    both the global body and local parts similarity for pseudo labeling) and Re-ID
    model training in a self-paced manner.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些方法还尝试基于局部部分学习表示，因为在局部部分挖掘标签信息比整个图像更容易。一个PatchNet[[153](#bib.bib153)]旨在通过挖掘补丁级相似性来学习区分补丁特征。Self-similarity
    Grouping（SSG）方法[[212](#bib.bib212)]以自适应的方式迭代进行分组（利用全局身体和局部部分的相似性进行伪标记）和Re-ID模型训练。
- en: Semi-/Weakly supervised Re-ID. With limited label information, a one-shot metric
    learning method is proposed in [[213](#bib.bib213)], which incorporates a deep
    texture representation and a color metric. A stepwise one-shot learning method
    (EUG) is proposed in [[144](#bib.bib144)] for video-based Re-ID, gradually selecting
    a few candidates from unlabeled tracklets to enrich the labeled tracklet set.
    A multiple instance attention learning framework [[214](#bib.bib214)] uses the
    video-level labels for representation learning, alleviating the reliance on full
    annotation.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督/弱监督Re-ID。在有限的标签信息下，[[213](#bib.bib213)]中提出了一种一-shot度量学习方法，该方法结合了深度纹理表示和颜色度量。在[[144](#bib.bib144)]中提出了一种逐步一-shot学习方法（EUG）用于基于视频的Re-ID，逐渐从未标记的跟踪片段中选择少量候选者，以丰富已标记的跟踪片段集。一个多实例注意力学习框架[[214](#bib.bib214)]使用视频级标签进行表示学习，从而减轻了对全面注释的依赖。
- en: 3.3.2 Unsupervised Domain Adaptation
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 无监督领域适应
- en: Unsupervised domain adaptation (UDA) transfers the knowledge on a labeled source
    dataset to the unlabeled target dataset [[53](#bib.bib53)]. Due to the large domain
    shift and powerful supervision in source dataset, it is another popular approach
    for unsupervised Re-ID without target dataset labels.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督领域适应（UDA）将知识从标记的源数据集转移到未标记的目标数据集[[53](#bib.bib53)]。由于领域转移大且源数据集具有强大的监督，这是一种无目标数据集标签的无监督Re-ID的流行方法。
- en: Target Image Generation. Using GAN generation to transfer the source domain
    images to target-domain style is a popular approach for UDA Re-ID. With the generated
    images, this enables supervised Re-ID model learning in the unlabeled target domain.
    Wei et al. [[44](#bib.bib44)] propose a Person Transfer Generative Adversarial
    Network (PTGAN), transferring the knowledge from one labeled source dataset to
    the unlabeled target dataset. Preserved self-similarity and domain-dissimilarity
    [[120](#bib.bib120)] is trained with a similarity preserving generative adversarial
    network (SPGAN). A Hetero-Homogeneous Learning (HHL) method [[215](#bib.bib215)]
    simultaneously considers the camera invariance with homogeneous learning and domain
    connectedness with heterogeneous learning. An adaptive transfer network [[216](#bib.bib216)]
    decomposes the adaptation process into certain imaging factors, including illumination,
    resolution, camera view, etc. This strategy improves the cross-dataset performance.
    Huang et al. [[217](#bib.bib217)] try to suppress the background shift to minimize
    the domain shift problem. Chen et al. [[218](#bib.bib218)] design an instance-guided
    context rendering scheme to transfer the person identities from source domain
    into diverse contexts in the target domain. Besides, a pose disentanglement scheme
    is added to improve the image generation [[121](#bib.bib121)]. A mutual mean-teacher
    learning scheme is also developed in [[219](#bib.bib219)]. However, the scalability
    and stability of the image generation for practical large-scale changing environment
    are still challenging.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 目标图像生成。使用 GAN 生成将源领域图像转换为目标领域风格是 UDA Re-ID 的一种流行方法。通过生成的图像，这使得在未标记的目标领域中进行监督式
    Re-ID 模型学习成为可能。Wei 等人 [[44](#bib.bib44)] 提出了一个人脸转移生成对抗网络（PTGAN），将知识从一个标记的源数据集转移到未标记的目标数据集。通过相似性保持生成对抗网络（SPGAN）训练保存的自相似性和领域差异
    [[120](#bib.bib120)]。一种异质-同质学习（HHL）方法 [[215](#bib.bib215)] 同时考虑了同质学习中的相机不变性和异质学习中的领域连通性。一个自适应转移网络
    [[216](#bib.bib216)] 将适应过程分解为某些成像因素，包括光照、分辨率、相机视角等。这一策略提高了跨数据集性能。Huang 等人 [[217](#bib.bib217)]
    尝试抑制背景位移，以最小化领域位移问题。Chen 等人 [[218](#bib.bib218)] 设计了一种实例引导的上下文渲染方案，将人物身份从源领域转移到目标领域的多样上下文中。此外，添加了姿态解耦方案以改善图像生成
    [[121](#bib.bib121)]。同时，在 [[219](#bib.bib219)] 中还开发了一种互助平均教师学习方案。然而，图像生成在实际大规模变化环境中的可扩展性和稳定性仍然是一个挑战。
- en: Bak et al. [[125](#bib.bib125)] generate a synthetic dataset with different
    illumination conditions to model realistic indoor and outdoor lighting. The synthesized
    dataset increases generalizability of the learned model and can be easily adapted
    to a new dataset without additional supervision [[220](#bib.bib220)].
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Bak 等人 [[125](#bib.bib125)] 生成了一个具有不同光照条件的合成数据集，以模拟现实的室内和室外光照。这一合成数据集提高了学习模型的泛化能力，并且可以在没有额外监督的情况下，轻松适应新数据集
    [[220](#bib.bib220)]。
- en: Target Domain Supervision Mining. Some methods directly mine the supervision
    on the unlabeled target dataset with a well trained model from source dataset.
    An exemplar memory learning scheme [[106](#bib.bib106)] considers three invariant
    cues as the supervision, including exemplar-invariance, camera invariance and
    neighborhood-invariance. The Domain-Invariant Mapping Network (DIMN) [[28](#bib.bib28)]
    formulates a meta-learning pipeline for the domain transfer task, and a subset
    of source domain is sampled at each training episode to update the memory bank,
    enhancing the scalability and discriminability. The camera view information is
    also applied in [[221](#bib.bib221)] as the supervision signal to reduce the domain
    gap. A self-training method with progressive augmentation [[222](#bib.bib222)]
    jointly captures the local structure and global data distribution on the target
    dataset. Recently, a self-paced contrastive learning framework with hybrid memory
    [[223](#bib.bib223)] is developed with great success, which dynamically generates
    multi-level supervision signals.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 目标领域监督挖掘。一些方法直接在未标注的目标数据集上挖掘监督，使用源数据集训练的模型。一个示例记忆学习方案 [[106](#bib.bib106)] 将示例不变性、相机不变性和邻域不变性作为监督信号。领域不变映射网络
    (DIMN) [[28](#bib.bib28)] 为领域迁移任务制定了一个元学习流程，并在每个训练回合中从源领域中抽取一个子集以更新记忆库，从而提高了可扩展性和区分度。相机视角信息也在
    [[221](#bib.bib221)] 中作为监督信号来减少领域差距。一个具有渐进增强的自训练方法 [[222](#bib.bib222)] 共同捕捉目标数据集上的局部结构和全局数据分布。最近，一个具有混合记忆的自适应对比学习框架
    [[223](#bib.bib223)] 成功地开发了，它动态生成多层次的监督信号。
- en: The spatio-temporal information is also utilized as the supervision in TFusion
    [[224](#bib.bib224)]. TFusion transfers the spatio-temporal patterns learned in
    the source domain to the target domain with a Bayesian fusion model. Similarly,
    Query-Adaptive Convolution (QAConv) [[225](#bib.bib225)] is developed to improve
    cross-dataset accuracy.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 时空信息在 TFusion [[224](#bib.bib224)] 中也作为监督信号进行利用。TFusion 通过贝叶斯融合模型将源领域中学到的时空模式转移到目标领域。类似地，Query-Adaptive
    Convolution (QAConv) [[225](#bib.bib225)] 的开发旨在提高跨数据集的准确性。
- en: 'TABLE III: Statistics of SOTA unsupervised person Re-ID on two image-based
    datasets. “Source” represents if it utilizes the source annotated data in training
    the target Re-ID model. “Gen.” indicates if it contains an image generation process.
    Rank-1 accuracy (%) and mAP (%) are reported.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：两个基于图像的数据集上的 SOTA 无监督行人重识别统计。 “来源”表示是否在训练目标 Re-ID 模型时利用了源标注数据。 “生成”表示是否包含图像生成过程。报告了
    Rank-1 准确率（%）和 mAP（%）。
- en: '|  |  |  | Market-1501 | DukeMTMC |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | Market-1501 | DukeMTMC |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Methods | Source | Gen. | R1 | mAP | R1 | mAP |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 来源 | 生成 | R1 | mAP | R1 | mAP |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| CAMEL [[226](#bib.bib226)]  ICCV17 | Model | No | 54.5 | 26.3 | - | - |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| CAMEL [[226](#bib.bib226)]  ICCV17 | 模型 | 否 | 54.5 | 26.3 | - | - |'
- en: '| PUL [[205](#bib.bib205)]  TOMM18 | Model | No | 45.5 | 20.5 | 30.0 | 16.4
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| PUL [[205](#bib.bib205)]  TOMM18 | 模型 | 否 | 45.5 | 20.5 | 30.0 | 16.4 |'
- en: '| PTGAN [[120](#bib.bib120)]  CVPR18 | Data | Yes | 58.1 | 26.9 | 46.9 | 26.4
    |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| PTGAN [[120](#bib.bib120)]  CVPR18 | 数据 | 是 | 58.1 | 26.9 | 46.9 | 26.4 |'
- en: '| TJ-AIDL^† [[111](#bib.bib111)]  CVPR18 | Data | No | 58.2 | 26.5 | 44.3 |
    23.0 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| TJ-AIDL^† [[111](#bib.bib111)]  CVPR18 | 数据 | 否 | 58.2 | 26.5 | 44.3 | 23.0
    |'
- en: '| HHL [[215](#bib.bib215)]  ECCV18 | Data | Yes | 62.2 | 31.4 | 46.9 | 27.2
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| HHL [[215](#bib.bib215)]  ECCV18 | 数据 | 是 | 62.2 | 31.4 | 46.9 | 27.2 |'
- en: '| MAR^‡ [[209](#bib.bib209)]  CVPR19 | Data | No | 67.7 | 40.0 | 67.1 | 48.0
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| MAR^‡ [[209](#bib.bib209)]  CVPR19 | 数据 | 否 | 67.7 | 40.0 | 67.1 | 48.0 |'
- en: '| ENC [[106](#bib.bib106)]  CVPR19 | Data | No | 75.1 | 43.0 | 63.3 | 40.4
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| ENC [[106](#bib.bib106)]  CVPR19 | 数据 | 否 | 75.1 | 43.0 | 63.3 | 40.4 |'
- en: '| ATNet [[216](#bib.bib216)]  CVPR19 | Data | Yes | 55.7 | 25.6 | 45.1 | 24.9
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| ATNet [[216](#bib.bib216)]  CVPR19 | 数据 | 是 | 55.7 | 25.6 | 45.1 | 24.9 |'
- en: '| PAUL^‡ [[153](#bib.bib153)]  CVPR19 | Model | No | 68.5 | 40.1 | 72.0 | 53.2
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| PAUL^‡ [[153](#bib.bib153)]  CVPR19 | 模型 | 否 | 68.5 | 40.1 | 72.0 | 53.2
    |'
- en: '| SBGAN [[217](#bib.bib217)]  ICCV19 | Data | Yes | 58.5 | 27.3 | 53.5 | 30.8
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| SBGAN [[217](#bib.bib217)]  ICCV19 | 数据 | 是 | 58.5 | 27.3 | 53.5 | 30.8 |'
- en: '| UCDA [[221](#bib.bib221)]  ICCV19 | Data | No | 64.3 | 34.5 | 55.4 | 36.7
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| UCDA [[221](#bib.bib221)]  ICCV19 | 数据 | 否 | 64.3 | 34.5 | 55.4 | 36.7 |'
- en: '| CASC^‡ [[210](#bib.bib210)]  ICCV19 | Model | No | 65.4 | 35.5 | 59.3 | 37.8
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| CASC^‡ [[210](#bib.bib210)]  ICCV19 | 模型 | 否 | 65.4 | 35.5 | 59.3 | 37.8
    |'
- en: '| PDA [[121](#bib.bib121)]  ICCV19 | Data | Yes | 75.2 | 47.6 | 63.2 | 45.1
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| PDA [[121](#bib.bib121)]  ICCV19 | 数据 | 是 | 75.2 | 47.6 | 63.2 | 45.1 |'
- en: '| CR-GAN [[218](#bib.bib218)]  ICCV19 | Data | Yes | 77.7 | 54.0 | 68.9 | 48.6
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| CR-GAN [[218](#bib.bib218)]  ICCV19 | 数据 | 是 | 77.7 | 54.0 | 68.9 | 48.6
    |'
- en: '| PAST [[222](#bib.bib222)]  ICCV19 | Model | No | 78.4 | 54.6 | 72.4 | 54.3
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| PAST [[222](#bib.bib222)]  ICCV19 | 模型 | 否 | 78.4 | 54.6 | 72.4 | 54.3 |'
- en: '| SSG [[212](#bib.bib212)]  ICCV19 | Model | No | 80.0 | 58.3 | 73.0 | 53.4
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| SSG [[212](#bib.bib212)]  ICCV19 | 模型 | 否 | 80.0 | 58.3 | 73.0 | 53.4 |'
- en: '| HCT [[208](#bib.bib208)]  CVPR20 | Model | No | 80.0 | 56.4 | 69.6 | 50.7
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| HCT [[208](#bib.bib208)]  CVPR20 | 模型 | 否 | 80.0 | 56.4 | 69.6 | 50.7 |'
- en: '| SNR [[227](#bib.bib227)]  CVPR20 | Data | No | 82.8 | 61.7 | 76.3 | 58.1
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| SNR [[227](#bib.bib227)]  CVPR20 | 数据 | 否 | 82.8 | 61.7 | 76.3 | 58.1 |'
- en: '| MMT [[219](#bib.bib219)]  ICLR20 | Data | No | 87.7 | 71.2 | 78.0 | 65.1
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| MMT [[219](#bib.bib219)]  ICLR20 | 数据 | 否 | 87.7 | 71.2 | 78.0 | 65.1 |'
- en: '| MEB-Net [[228](#bib.bib228)]  ECCV20 | Data | No | 89.9 | 76.0 | 79.6 | 66.1
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| MEB-Net [[228](#bib.bib228)]  ECCV20 | 数据 | 否 | 89.9 | 76.0 | 79.6 | 66.1
    |'
- en: '| SpCL [[223](#bib.bib223)]  NeurIPS20 | Data | No | 90.3 | 76.7 | 82.9 | 68.8
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| SpCL [[223](#bib.bib223)]  NeurIPS20 | 数据 | 否 | 90.3 | 76.7 | 82.9 | 68.8
    |'
- en: $\bullet$ ^† TJ-AIDL [[111](#bib.bib111)] requires additional attribute annotation.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ^† TJ-AIDL [[111](#bib.bib111)] 需要额外的属性标注。
- en: $\bullet$ ^§ DAS [[125](#bib.bib125)] generates synthesized virtual humans under
    vairous lightings.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ^§ DAS [[125](#bib.bib125)] 在各种光照条件下生成合成虚拟人。
- en: $\bullet$ ^‡ PAUL [[153](#bib.bib153)], MAR [[209](#bib.bib209)] and CASC [[210](#bib.bib210)]
    use MSMT17 as source dataset.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ^‡ PAUL [[153](#bib.bib153)]、MAR [[209](#bib.bib209)] 和 CASC [[210](#bib.bib210)]
    使用 MSMT17 作为源数据集。
- en: 3.3.3 State-of-The-Arts for Unsupervised Re-ID
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 无监督 Re-ID 的最新进展
- en: 'Unsupervised Re-ID has achieved increasing attention in recent years, evidenced
    by the increasing number of publications in top venues. We review the SOTA for
    unsupervised deeply learned methods on two widely-used image-based Re-ID datasets.
    The results are summarized in Table [III](#S3.T3 "TABLE III ‣ 3.3.2 Unsupervised
    Domain Adaptation ‣ 3.3 Semi-supervised and Unsupervised Re-ID ‣ 3 Open-world
    Person Re-Identification ‣ Deep Learning for Person Re-identification: A Survey
    and Outlook"). From these results, the following insights can be drawn.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '无监督 Re-ID 近年来受到了越来越多的关注，这可以通过顶级刊物中发表的文献数量增加得到证实。我们回顾了在两个广泛使用的基于图像的 Re-ID 数据集上无监督深度学习方法的最新进展。结果总结在表
    [III](#S3.T3 "TABLE III ‣ 3.3.2 Unsupervised Domain Adaptation ‣ 3.3 Semi-supervised
    and Unsupervised Re-ID ‣ 3 Open-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook") 中。从这些结果中，可以得出以下见解。'
- en: First, the unsupervised Re-ID performance has increased significantly over the
    years. The Rank-1 accuracy/mAP increases from 54.5%/26.3% (CAMEL [[226](#bib.bib226)])
    to 90.3%/76.7% (SpCL [[223](#bib.bib223)]) on the Market-1501 dataset within three
    years. The performance for DukeMTMC dataset increases from 30.0%/16.4% to 82.9%/68.8%.
    The gap between the supervised upper bound and the unsupervised learning is narrowed
    significantly. This demonstrates the success of unsupervised Re-ID with deep learning.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，无监督 Re-ID 的性能在过去几年中显著提升。在 Market-1501 数据集上，Rank-1 准确率/平均精度从 54.5%/26.3%（CAMEL
    [[226](#bib.bib226)]）提高到 90.3%/76.7%（SpCL [[223](#bib.bib223)]），时间跨度为三年。DukeMTMC
    数据集的性能从 30.0%/16.4% 提升到 82.9%/68.8%。监督学习的上限和无监督学习之间的差距显著缩小。这展示了深度学习在无监督 Re-ID
    中的成功。
- en: 'Second, current unsupervised Re-ID is still under-developed and it can be further
    improved in the following aspects: 1) The powerful attention scheme in supervised
    Re-ID methods has rarely been applied in unsupervised Re-ID. 2) Target domain
    image generation has been proved effective in some methods, but they are not applied
    in two best methods (PAST [[222](#bib.bib222)], SSG [[212](#bib.bib212)]). 3)
    Using the annotated source data in the training process of the target domain is
    beneficial for cross-dataset learning, but it is also not included in above two
    methods. These observations provide the potential basis for further improvements.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，目前的无监督 Re-ID 仍在发展中，可以在以下方面进一步改进：1) 监督 Re-ID 方法中强大的注意力机制在无监督 Re-ID 中很少应用。2)
    目标域图像生成在一些方法中已被证明有效，但在两个最佳方法（PAST [[222](#bib.bib222)]、SSG [[212](#bib.bib212)]）中未被应用。3)
    在目标域的训练过程中使用标注的源数据对跨数据集学习有益，但上述两个方法中也没有包含这些。以上观察提供了进一步改进的潜在基础。
- en: Third, there is still a large gap between the unsupervised and supervised Re-ID.
    For example, the rank-1 accuracy of supervised ConsAtt [[93](#bib.bib93)] has
    achieved 96.1% on the Market-1501 dataset, while the highest accuracy of unsupervised
    SpCL [[223](#bib.bib223)] is about 90.3%. Recently, He et al. [[229](#bib.bib229)]
    have demonstrated that unsupervised learning with large-scale unlabeled training
    data has the ability to outperform the supervised learning on various tasks [[230](#bib.bib230)].
    We expect that several breakthroughs in future unsupervised Re-ID.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，无监督和有监督的重新识别之间仍存在较大差距。例如，有监督的ConsAtt [[93](#bib.bib93)]在Market-1501数据集上的rank-1准确率已达到96.1%，而无监督的SpCL
    [[223](#bib.bib223)]的最高准确率约为90.3%。最近，He等人[[229](#bib.bib229)]展示了无监督学习利用大规模无标记训练数据在各种任务上超越有监督学习的能力[[230](#bib.bib230)]。我们期待未来在无监督重新识别领域的若干突破。
- en: 3.4 Noise-Robust Re-ID
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 噪声鲁棒的重新识别
- en: 'Re-ID usually suffers from unavoidable noise due to data collection and annotation
    difficulty. We review noise-robust Re-ID from three aspects: Partial Re-ID with
    heavy occlusion, Re-ID with sample noise caused by detection or tracking errors,
    and Re-ID with label noise caused by annotation error.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 重新识别通常因数据收集和注释困难而遭受不可避免的噪声。我们从三个方面回顾了噪声鲁棒的重新识别：带重度遮挡的部分重新识别、由检测或跟踪错误引起的样本噪声重新识别，以及由注释错误引起的标签噪声重新识别。
- en: Partial Re-ID. This addresses the Re-ID problem with heavy occlusions, *i.e*.,
    only part of the human body is visible [[231](#bib.bib231)]. A fully convolutional
    network [[232](#bib.bib232)] is adopted to generate fix-sized spatial feature
    maps for the incomplete person images. Deep Spatial feature Reconstruction (DSR)
    is further incorporated to avoid explicit alignment by exploiting the reconstructing
    error. Sun et al. [[67](#bib.bib67)] design a Visibility-aware Part Model (VPM)
    to extract sharable region-level features, thus suppressing the spatial misalignment
    in the incomplete images. A foreground-aware pyramid reconstruction scheme [[233](#bib.bib233)]
    also tries to learn from the unoccluded regions. The Pose-Guided Feature Alignment
    (PGFA) [[234](#bib.bib234)] exploits the pose landmarks to mine discriminative
    part information from occlusion noise. However, it is still challenging due to
    the severe partial misalignment, unpredictable visible regions and distracting
    unshared body regions. Meanwhile, how to adaptively adjust the matching model
    for different queries still needs further investigation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 部分重新识别。这解决了重度遮挡情况下的重新识别问题，即只有部分人体可见[[231](#bib.bib231)]。采用全卷积网络[[232](#bib.bib232)]生成固定大小的空间特征图，以处理不完整的人物图像。进一步结合深度空间特征重建（DSR）以避免通过利用重建误差进行显式对齐。Sun等人[[67](#bib.bib67)]设计了一个关注可见性的部件模型（VPM），以提取可共享的区域级特征，从而抑制不完整图像中的空间不对齐。前景感知金字塔重建方案[[233](#bib.bib233)]也尝试从未遮挡区域中学习。姿态引导特征对齐（PGFA）[[234](#bib.bib234)]利用姿态标记从遮挡噪声中挖掘辨别性部件信息。然而，由于严重的部分不对齐、不可预测的可见区域和分散的不共享身体区域，这仍然具有挑战性。同时，如何自适应调整匹配模型以适应不同查询仍需进一步研究。
- en: Re-ID with Sample Noise. This refers to the problem of the person images or
    the video sequence containing outlying regions/frames, either caused by poor detection/inaccurate
    tracking results. To handle the outlying regions or background clutter within
    the person image, pose estimation cues [[17](#bib.bib17), [18](#bib.bib18)] or
    attention cues [[199](#bib.bib199), [22](#bib.bib22), [66](#bib.bib66)] are exploited.
    The basic idea is to suppress the contribution of the noisy regions in the final
    holistic representation. For video sequences, set-level feature learning [[83](#bib.bib83)]
    or frame level re-weighting [[134](#bib.bib134)] are the commonly used approaches
    to reduce the impact of noisy frames. Hou et al. [[20](#bib.bib20)] also utilize
    multiple video frames to auto-complete occluded regions. It is expected that more
    domain-specific sample noise handling designs in the future.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 带样本噪声的重新识别。这指的是当人物图像或视频序列包含异常区域/帧时的问题，可能是由于检测不佳/跟踪结果不准确造成的。为了处理人物图像中的异常区域或背景杂乱，利用了姿态估计线索[[17](#bib.bib17),
    [18](#bib.bib18)]或注意力线索[[199](#bib.bib199), [22](#bib.bib22), [66](#bib.bib66)]。基本思想是抑制噪声区域在最终整体表示中的贡献。对于视频序列，常用的方法包括集级特征学习[[83](#bib.bib83)]或帧级重新加权[[134](#bib.bib134)]，以减少噪声帧的影响。Hou等人[[20](#bib.bib20)]还利用多个视频帧来自动补全遮挡区域。预计未来会有更多领域特定的样本噪声处理设计。
- en: Re-ID with Label Noise. Label noise is usually unavoidable due to annotation
    error. Zheng et al. adopt a label smoothing technique to avoid label overfiting
    issues [[42](#bib.bib42)]. A Distribution Net (DNet) that models the feature uncertainty
    is proposed in [[235](#bib.bib235)] for robust Re-ID model learning against label
    noise, reducing the impact of samples with high feature uncertainty. Different
    from the general classification problem, robust Re-ID model learning suffers from
    limited training samples for each identity [[236](#bib.bib236)]. In addition,
    the unknown new identities increase additional difficulty for the robust Re-ID
    model learning.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 带标签噪声的 Re-ID。由于注释错误，标签噪声通常是不可避免的。Zheng 等人采用了一种标签平滑技术来避免标签过拟合问题 [[42](#bib.bib42)]。在
    [[235](#bib.bib235)] 中提出了一种建模特征不确定性的分布网络（DNet），用于在面对标签噪声时进行稳健的 Re-ID 模型学习，减少特征不确定性较高的样本的影响。与一般分类问题不同，稳健的
    Re-ID 模型学习因每个身份的训练样本有限而受困 [[236](#bib.bib236)]。此外，未知的新身份为稳健的 Re-ID 模型学习增加了额外的困难。
- en: 3.5 Open-set Re-ID and Beyond
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 开放集 Re-ID 及其他
- en: Open-set Re-ID is usually formulated as a person verification problem, *i.e*.,
    discriminating whether or not two person images belong to the same identity [[69](#bib.bib69),
    [70](#bib.bib70)]. The verification usually requires a learned condition $\tau$,
    *i.e*., $sim(query,gallery)>\tau$. Early researches design hand-crafted systems
    [[69](#bib.bib69), [70](#bib.bib70), [26](#bib.bib26)]. For deep learning methods,
    an Adversarial PersonNet (APN) is proposed in [[237](#bib.bib237)], which jointly
    learns a GAN module and the Re-ID feature extractor. The basic idea of this GAN
    is to generate realistic target-like images (imposters) and enforce the feature
    extractor is robust to the generated image attack. Modeling feature uncertainty
    is also investigated in [[235](#bib.bib235)]. However, it remains quite challenging
    to achieve a high true target recognition and maintain low false target recognition
    rate [[238](#bib.bib238)].
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 开放集 Re-ID 通常被表述为一个人员验证问题，*即*，区分两个人员图像是否属于同一身份 [[69](#bib.bib69), [70](#bib.bib70)]。验证通常需要一个学习到的条件
    $\tau$，*即*， $sim(query,gallery)>\tau$。早期研究设计了手工制作的系统 [[69](#bib.bib69), [70](#bib.bib70),
    [26](#bib.bib26)]。对于深度学习方法，在 [[237](#bib.bib237)] 中提出了一种对抗性 PersonNet（APN），该网络联合学习了
    GAN 模块和 Re-ID 特征提取器。这个 GAN 的基本思想是生成逼真的目标类似图像（冒充者），并强制特征提取器对生成的图像攻击具有鲁棒性。在 [[235](#bib.bib235)]
    中也探讨了建模特征不确定性。然而，实现高真实目标识别率并保持低假目标识别率仍然非常具有挑战性 [[238](#bib.bib238)]。
- en: Group Re-ID. It aims at associating the persons in groups rather than individuals
    [[167](#bib.bib167)]. Early researches mainly focus on group representation extraction
    with sparse dictionary learning [[239](#bib.bib239)] or covariance descriptor
    aggregation [[240](#bib.bib240)]. The multi-grain information is integrated in
    [[241](#bib.bib241)] to fully capture the characteristics of a group. Recently,
    the graph convoltuional network is applied in [[242](#bib.bib242)], representing
    the group as a graph. The group similarity is also applied in the end-to-end person
    search [[196](#bib.bib196)] and the individual re-identification [[243](#bib.bib243),
    [197](#bib.bib197)] to improve the accuracy. However, group Re-ID is still challenging
    since the group variation is more complicated than the individuals.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 群体 Re-ID。它旨在关联群体中的人员而非单个个体 [[167](#bib.bib167)]。早期研究主要集中在通过稀疏字典学习 [[239](#bib.bib239)]
    或协方差描述符聚合 [[240](#bib.bib240)] 提取群体表示。在 [[241](#bib.bib241)] 中集成了多粒度信息，以全面捕捉群体的特征。最近，在
    [[242](#bib.bib242)] 中应用了图卷积网络，将群体表示为图。在端到端人员搜索 [[196](#bib.bib196)] 和个体重新识别 [[243](#bib.bib243),
    [197](#bib.bib197)] 中也应用了群体相似性以提高准确性。然而，由于群体变化比个体更复杂，群体 Re-ID 仍然具有挑战性。
- en: Dynamic Multi-Camera Network. Dynamic updated multi-camera network is another
    challenging issue [[27](#bib.bib27), [24](#bib.bib24), [23](#bib.bib23), [29](#bib.bib29)],
    which needs model adaptation for new cameras or probes. A human in-the-loop incremental
    learning method is introduced in [[24](#bib.bib24)] to update the Re-ID model,
    adapting the representation for different probe galleries. Early research also
    applies the active learning [[27](#bib.bib27)] for continuous Re-ID in multi-camera
    network. A continuous adaptation method based on sparse non-redundant representative
    selection is introduced in [[23](#bib.bib23)]. A transitive inference algorithm
    [[244](#bib.bib244)] is designed to exploit the best source camera model based
    on a geodesic flow kernel. Multiple environmental constraints (*e.g*., Camera
    Topology) in dense crowds and social relationships are integrated for an open-world
    person Re-ID system [[245](#bib.bib245)]. The model adaptation and environmental
    factors of cameras are crucial in practical dynamic multi-camera network. Moreover,
    how to apply the deep learning technique for the dynamic multi-camera network
    is still less investigated.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 动态多摄像头网络。动态更新的多摄像头网络是另一个具有挑战性的问题[[27](#bib.bib27), [24](#bib.bib24), [23](#bib.bib23),
    [29](#bib.bib29)]，需要对新摄像头或探针进行模型适配。[[24](#bib.bib24)]中引入了一种人机协同的增量学习方法，用于更新Re-ID模型，适应不同的探针画廊。早期研究还应用了主动学习[[27](#bib.bib27)]用于多摄像头网络中的连续Re-ID。[[23](#bib.bib23)]中引入了一种基于稀疏非冗余代表选择的连续适配方法。[[244](#bib.bib244)]设计了一种传递推理算法，用于基于测地流核利用最佳源摄像头模型。在密集人群和社交关系中的多个环境约束（*例如*，摄像头拓扑）被整合到一个开放世界的人员Re-ID系统中[[245](#bib.bib245)]。模型适配和摄像头的环境因素在实际动态多摄像头网络中至关重要。此外，如何将深度学习技术应用于动态多摄像头网络仍然研究较少。
- en: '![Refer to caption](img/02eb927562ed75b8d2434fb99d418f9a.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/02eb927562ed75b8d2434fb99d418f9a.png)'
- en: 'Figure 7: Difference between the widely used CMC, AP and the negative penalty
    (NP) measurements. True matching and false matching are bounded in green and red
    boxes, respectively. Assume that only three correct matches exist in the gallery,
    rank list 1 gets better AP, but gets much worse NP than rank list 2\. The main
    reason is that rank list 1 contains too many false matchings before finding the
    hardest true matching. For consistency with CMC and mAP, we compute the inverse
    negative penalty (INP), *e.g*., INP = 1- NP. Larger INP means better performance.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：广泛使用的CMC、AP与负惩罚（NP）测量之间的差异。真实匹配和虚假匹配分别用绿色和红色框标记。假设画廊中只有三个正确匹配，排名列表1的AP较好，但NP远逊于排名列表2。主要原因是排名列表1在找到最难的真实匹配之前包含了过多的虚假匹配。为了与CMC和mAP保持一致，我们计算了逆负惩罚（INP），*例如*，INP
    = 1 - NP。较大的INP意味着更好的性能。
- en: '4 An Outlook: Re-ID in Next Era'
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 展望：下一时代的Re-ID
- en: 'This section firstly presents a new evaluation metric in § [4.1](#S4.SS1 "4.1
    mINP: A New Evaluation Metric for Re-ID ‣ 4 An Outlook: Re-ID in Next Era ‣ Deep
    Learning for Person Re-identification: A Survey and Outlook"), a strong baseline
    (in § [4.2](#S4.SS2 "4.2 A New Baseline for Single-/Cross-Modality Re-ID ‣ 4 An
    Outlook: Re-ID in Next Era ‣ Deep Learning for Person Re-identification: A Survey
    and Outlook")) for person Re-ID. It provides an important guidance for future
    Re-ID research. Finally, we discuss some under-investigated open issues in § [4.3](#S4.SS3
    "4.3 Under-Investigated Open Issues ‣ 4 An Outlook: Re-ID in Next Era ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook").'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '本节首先在§ [4.1](#S4.SS1 "4.1 mINP: A New Evaluation Metric for Re-ID ‣ 4 An Outlook:
    Re-ID in Next Era ‣ Deep Learning for Person Re-identification: A Survey and Outlook")中介绍了一个新的评估指标，在§ [4.2](#S4.SS2
    "4.2 A New Baseline for Single-/Cross-Modality Re-ID ‣ 4 An Outlook: Re-ID in
    Next Era ‣ Deep Learning for Person Re-identification: A Survey and Outlook")中提供了一个强有力的基线。它为未来的Re-ID研究提供了重要指导。最后，我们在§ [4.3](#S4.SS3
    "4.3 Under-Investigated Open Issues ‣ 4 An Outlook: Re-ID in Next Era ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")中讨论了一些尚未深入研究的开放问题。'
- en: '4.1 mINP: A New Evaluation Metric for Re-ID'
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 mINP：Re-ID的新评估指标
- en: 'For a good Re-ID system, the target person should be retrieved as accurately
    as possible, *i.e*., all the correct matches should have low rank values. Considering
    that the target person should not be neglected in the top-ranked retrieved list,
    especially for multi-camera network, so as to accurately track the target. When
    the target person appears in the gallery set at multiple time stamps, the rank
    position of the hardest correct match determines the workload of the inspectors
    for further investigation. However, the currently widely used CMC and mAP metrics
    cannot evaluate this property, as shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.5 Open-set
    Re-ID and Beyond ‣ 3 Open-world Person Re-Identification ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook"). With the same CMC, rank list 1 achieves
    a better AP than rank list 2, but it requires more efforts to find all the correct
    matches. To address this issue, we design a computationally efficient metric,
    namely a negative penalty (NP), which measures the penalty to find the hardest
    correct match'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 对于良好的Re-ID系统，应尽可能准确地检索目标人物，即所有正确匹配的排名应较低。考虑到在多摄像头网络中，尤其在准确跟踪目标人物时，目标人物不应在排名最高的检索列表中被忽略。当目标人物在多个时间戳的画廊集中出现时，最难正确匹配的排名决定了进一步调查的工作量。然而，目前广泛使用的CMC和mAP指标无法评估这一属性，如图7所示。在相同的CMC下，排名列表1实现了比排名列表2更好的AP，但需要更多的努力来找到所有正确匹配。为了解决这个问题，我们设计了一个计算效率高的度量标准，即负面惩罚（NP），用于衡量找到最难正确匹配的惩罚
- en: '|  | $\mathrm{NP}_{i}=\frac{R_{i}^{hard}-&#124;G_{i}&#124;}{R_{i}^{hard}},$
    |  | (6) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{NP}_{i}=\frac{R_{i}^{hard}-&#124;G_{i}&#124;}{R_{i}^{hard}},$
    |  | (6) |'
- en: where $R_{i}^{hard}$ indicates the rank position of the hardest match, and $|G_{i}|$
    represents the total number of correct matches for query $i$. Naturally, a smaller
    NP represents better performance. For consistency with CMC and mAP, we prefer
    to use the inverse negative penalty (INP), an inverse operation of NP. Overall,
    the mean INP of all the queries is represented by
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$R_{i}^{hard}$表示最难匹配的排名位置，$|G_{i}|$表示查询$i$的正确匹配总数。自然而然地，较小的NP表示更好的性能。为了与CMC和mAP保持一致，我们更愿意使用负面惩罚的倒数（INP），这是NP的逆操作。总体而言，所有查询的平均INP由下式表示
- en: '|  | $\mathrm{mINP}=\frac{1}{n}\sum\nolimits_{i}(1-\mathrm{NP}_{i})=\frac{1}{n}\sum\nolimits_{i}\frac{&#124;G_{i}&#124;}{R_{i}^{hard}}.$
    |  | (7) |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{mINP}=\frac{1}{n}\sum\nolimits_{i}(1-\mathrm{NP}_{i})=\frac{1}{n}\sum\nolimits_{i}\frac{&#124;G_{i}&#124;}{R_{i}^{hard}}.$
    |  | (7) |'
- en: The calculation of mINP is quite efficient and can be seamlessly integrated
    in the CMC/mAP calculating process. mINP avoids the domination of easy matches
    in the mAP/CMC evaluation. One limitation is that mINP value difference for large
    gallery size would be much smaller compared to small galleries. But it still can
    reflect the relative performance of a Re-ID model, providing a supplement to the
    widely-used CMC and mAP metrics.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: mINP的计算非常高效，并且可以无缝集成在CMC/mAP的计算过程中。mINP避免了在mAP/CMC评估中易匹配的主导地位。一个局限是，对于大画廊大小，mINP值的差异会比小画廊小得多。但它仍然可以反映Re-ID模型的相对性能，为广泛使用的CMC和mAP指标提供补充。
- en: 'TABLE IV: Comparison with the state-of-the-arts on single-modality image-based
    Re-ID. Rank-1 accuracy (%), mAP (%) and mINP (%) are reported on two public datasets.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 IV：与基于单模态图像的Re-ID现有技术的比较。两个公共数据集上报告了排名1准确度（%），mAP（%）和mINP（%）。
- en: '|  | Market-1501 [[5](#bib.bib5)] | DukeMTMC [[42](#bib.bib42)] |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | Market-1501 【5】 | DukeMTMC 【42】 |'
- en: '| Method | R1 | mAP | mINP | R1 | mAP | mINP |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | R1 | mAP | mINP | R1 | mAP | mINP |'
- en: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 94.5 | 85.9 | 59.4 | 86.4 | 76.4
    | 40.7 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| BagTricks 【122】  CVPR19W | 94.5 | 85.9 | 59.4 | 86.4 | 76.4 | 40.7 |'
- en: '| ABD-Net [[173](#bib.bib173)]  ICCV19 | 95.6 | 88.3 | 66.2 | 89.0 | 78.6 |
    42.1 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| ABD-Net 【173】  ICCV19 | 95.6 | 88.3 | 66.2 | 89.0 | 78.6 | 42.1 |'
- en: '| B (ours) | 94.2 | 85.4 | 58.3 | 86.1 | 76.1 | 40.3 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| B (我们的) | 94.2 | 85.4 | 58.3 | 86.1 | 76.1 | 40.3 |'
- en: '| B + Att [[246](#bib.bib246)] | 94.9 | 86.9 | 62.2 | 87.5 | 77.6 | 41.9 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| B + Att 【246】 | 94.9 | 86.9 | 62.2 | 87.5 | 77.6 | 41.9 |'
- en: '| B + WRT | 94.6 | 86.8 | 61.9 | 87.1 | 77.0 | 41.4 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| B + WRT | 94.6 | 86.8 | 61.9 | 87.1 | 77.0 | 41.4 |'
- en: '| B + GeM [[247](#bib.bib247)] | 94.4 | 86.3 | 60.1 | 87.3 | 77.3 | 41.9 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| B + GeM 【247】 | 94.4 | 86.3 | 60.1 | 87.3 | 77.3 | 41.9 |'
- en: '| B + WRT + GeM | 94.9 | 87.1 | 62.5 | 88.2 | 78.1 | 43.4 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| B + WRT + GeM | 94.9 | 87.1 | 62.5 | 88.2 | 78.1 | 43.4 |'
- en: '| AGW (Full) | 95.1 | 87.8 | 65.0 | 89.0 | 79.6 | 45.7 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| AGW (完整) | 95.1 | 87.8 | 65.0 | 89.0 | 79.6 | 45.7 |'
- en: 4.2 A New Baseline for Single-/Cross-Modality Re-ID
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 单/交叉模态Re-ID的新基准
- en: 'According to the discussion in § [2.4.2](#S2.SS4.SSS2 "2.4.2 In-depth Analysis
    on State-of-The-Arts ‣ 2.4 Datasets and Evaluation ‣ 2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook"), we design
    a new AGW³³3Details are in [https://github.com/mangye16/ReID-Survey](https://github.com/mangye16/ReID-Survey)
    and comprehensive comparison is shown in the supplementary material. baseline
    for person Re-ID, which achieves competitive performance on both single-modality
    (image and video) and cross-modality Re-ID tasks. Specifically, our new baseline
    is designed on top of BagTricks [[122](#bib.bib122)], and AGW contains the following
    three major improved components:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 根据§ [2.4.2](#S2.SS4.SSS2 "2.4.2 对现有技术的深入分析 ‣ 2.4 数据集和评估 ‣ 2 闭世界行人重识别 ‣ 行人重识别的深度学习：调查与展望")中的讨论，我们设计了一种新的
    AGW³³3 详细信息见 [https://github.com/mangye16/ReID-Survey](https://github.com/mangye16/ReID-Survey)，全面比较见补充材料。行人重识别的基线模型，它在单模态（图像和视频）和跨模态
    Re-ID 任务中都实现了具有竞争力的性能。具体来说，我们的新基线是建立在 BagTricks [[122](#bib.bib122)] 之上的，AGW 包含以下三个主要改进组件：
- en: '(1) Non-local Attention (Att) Block. As discussed in § [2.4.2](#S2.SS4.SSS2
    "2.4.2 In-depth Analysis on State-of-The-Arts ‣ 2.4 Datasets and Evaluation ‣
    2 Closed-world Person Re-Identification ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook"), the attention scheme plays a crucial role in discriminative
    Re-ID model learning. We adopt the powerful non-local attention block [[246](#bib.bib246)]
    to obtain the weighted sum of the features at all positions, represented by'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 非局部注意力（Att）块。如§ [2.4.2](#S2.SS4.SSS2 "2.4.2 对现有技术的深入分析 ‣ 2.4 数据集和评估 ‣ 2
    闭世界行人重识别 ‣ 行人重识别的深度学习：调查与展望")中所讨论的，注意力机制在判别性 Re-ID 模型学习中发挥着至关重要的作用。我们采用了强大的非局部注意力块
    [[246](#bib.bib246)]，以获取所有位置特征的加权和，如下所示
- en: '|  | $\mathbf{z}_{i}=W_{z}*\phi(\mathbf{x}_{i})+\mathbf{x}_{i},$ |  | (8) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{z}_{i}=W_{z}*\phi(\mathbf{x}_{i})+\mathbf{x}_{i},$ |  | (8) |'
- en: where $W_{z}$ is a weight matrix to be learned, $\phi(\cdot)$ represents a non-local
    operation, and $+\mathbf{x}_{i}$ formulates a residual learning strategy. Details
    can be found in [[246](#bib.bib246)]. We adopt the default setting from [[246](#bib.bib246)]
    to insert the non-local attention block.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{z}$ 是一个待学习的权重矩阵，$\phi(\cdot)$ 代表非局部操作，而 $+\mathbf{x}_{i}$ 形成了一个残差学习策略。详细信息见
    [[246](#bib.bib246)]。我们采用了 [[246](#bib.bib246)] 中的默认设置来插入非局部注意力块。
- en: (2) Generalized-mean (GeM) Pooling. As a fine-grained instance retrieval, the
    widely-used max-pooling or average pooling cannot capture the domain-specific
    discriminative features. We adopt a learnable pooling layer, named generalized-mean
    (GeM) pooling [[247](#bib.bib247)], formulated by
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 广义均值（GeM）池化。作为一种细粒度实例检索，广泛使用的最大池化或平均池化无法捕捉领域特定的判别特征。我们采用了一种可学习的池化层，称为广义均值（GeM）池化
    [[247](#bib.bib247)]，其公式为
- en: '|  | $\mathbf{f}=[f_{1}\cdots f_{k}\cdots f_{K}]^{T},f_{k}=(\frac{1}{&#124;\mathcal{X}_{k}&#124;}\sum\nolimits_{x_{i}\in\mathcal{X}_{k}}x_{i}^{p_{k}})^{\frac{1}{p_{k}}},$
    |  | (9) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{f}=[f_{1}\cdots f_{k}\cdots f_{K}]^{T},f_{k}=(\frac{1}{\lvert\mathcal{X}_{k}\rvert}\sum\nolimits_{x_{i}\in\mathcal{X}_{k}}x_{i}^{p_{k}})^{\frac{1}{p_{k}}},$
    |  | (9) |'
- en: where $f_{k}$ represents the feature map, and $K$ is number of feature maps
    in the last layer. $\mathcal{X}_{k}$ is the set of $W\times H$ activations for
    feature map $k\in\{1,2,\cdots,K\}$. $p_{k}$ is a pooling hyper-parameter, which
    is learned in the back-propagation process [[247](#bib.bib247)]. The above operation
    approximates max pooling when $p_{k}\rightarrow\infty$ and average pooling when
    $p_{k}=1$.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f_{k}$ 代表特征图，$K$ 是最后一层的特征图数量。$\mathcal{X}_{k}$ 是特征图 $k\in\{1,2,\cdots,K\}$
    的 $W\times H$ 激活集合。$p_{k}$ 是一个池化超参数，它在反向传播过程中进行学习 [[247](#bib.bib247)]。上述操作在 $p_{k}\rightarrow\infty$
    时近似最大池化，在 $p_{k}=1$ 时近似平均池化。
- en: (3) Weighted Regularization Triplet (WRT) loss. In addition to the baseline
    identity loss with softmax cross-entropy, we integrate with another weighted regularized
    triplet loss,
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 加权正则化三元组（WRT）损失。除了基线身份损失与 softmax 交叉熵之外，我们还整合了另一种加权正则化三元组损失，
- en: '|  | $\mathcal{L}_{wrt}(i)=\log(1+\exp(\sum\nolimits_{j}w_{ij}^{p}d^{p}_{ij}-\sum\nolimits_{k}w_{ik}^{n}d^{n}_{ik})).$
    |  | (10) |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{wrt}(i)=\log(1+\exp(\sum\nolimits_{j}w_{ij}^{p}d^{p}_{ij}-\sum\nolimits_{k}w_{ik}^{n}d^{n}_{ik})).$
    |  | (10) |'
- en: '|  | $w_{ij}^{p}=\frac{\exp{(d^{p}_{ij})}}{\sum\nolimits_{d_{ij}^{p}\in\mathcal{P}_{i}}\exp(d^{p}_{ij})},w_{ik}^{n}=\frac{\exp{(-d^{n}_{ik})}}{\sum\nolimits_{d^{n}_{ik}\in\mathcal{N}_{i}}\exp(-d^{n}_{ik})},$
    |  | (11) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{ij}^{p}=\frac{\exp{(d^{p}_{ij})}}{\sum\nolimits_{d_{ij}^{p}\in\mathcal{P}_{i}}\exp(d^{p}_{ij})},w_{ik}^{n}=\frac{\exp{(-d^{n}_{ik})}}{\sum\nolimits_{d^{n}_{ik}\in\mathcal{N}_{i}}\exp(-d^{n}_{ik})},$
    |  | (11) |'
- en: where $(i,j,k)$ represents a hard triplet within each training batch. For anchor
    $i$, $\mathcal{P}_{i}$ is the corresponding positive set, and $\mathcal{N}_{i}$
    is the negative set. $d^{p}_{ij}$/$d^{n}_{ik}$ represents the pairwise distance
    of a positive/negative sample pair. The above weighted regularization inherits
    the advantage of relative distance optimization between positive and negative
    pairs, but it avoids introducing any additional margin parameters. Our weighting
    strategy is similar to [[248](#bib.bib248)], but our solution does not introduce
    additional hyper-parameters.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(i,j,k)$ 表示每个训练批次中的一个困难三元组。对于锚点 $i$，$\mathcal{P}_{i}$ 是相应的正样本集合，而 $\mathcal{N}_{i}$
    是负样本集合。$d^{p}_{ij}$/$d^{n}_{ik}$ 表示正样本/负样本对的成对距离。上述加权正则化继承了正负样本对之间相对距离优化的优势，但避免了引入额外的间隔参数。我们的加权策略类似于
    [[248](#bib.bib248)]，但我们的解决方案没有引入额外的超参数。
- en: '![Refer to caption](img/006c581a35b38db715f1f2793e5a27b0.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/006c581a35b38db715f1f2793e5a27b0.png)'
- en: 'Figure 8: The framework of the proposed AGW baseline using the widely used
    ResNet50 [[80](#bib.bib80)] as the backbone network.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：提出的 AGW 基线的框架，使用广泛应用的 ResNet50 [[80](#bib.bib80)] 作为主干网络。
- en: 'TABLE V: Comparison with the state-of-the-arts on two image Re-ID datasets,
    including CUHK03 and MSMT17\. Rank-1 accuracy (%), mAP (%) and mINP (%) are reported.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：在两个图像 Re-ID 数据集上的状态-of-the-arts 比较，包括 CUHK03 和 MSMT17。报告了 Rank-1 准确率（%）、mAP
    (%) 和 mINP (%)。
- en: '|  | CUHK03 [[43](#bib.bib43)] | MSMT17 [[44](#bib.bib44)] |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | CUHK03 [[43](#bib.bib43)] | MSMT17 [[44](#bib.bib44)] |'
- en: '| --- | --- | --- |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Method | R1 | mAP | mINP | R1 | mAP | mINP |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | R1 | mAP | mINP | R1 | mAP | mINP |'
- en: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 58.0 | 56.6 | 43.8 | 63.4 | 45.1
    | 12.4 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 58.0 | 56.6 | 43.8 | 63.4 | 45.1
    | 12.4 |'
- en: '| AGW (Full) | 63.6 | 62.0 | 50.3 | 68.3 | 49.3 | 14.7 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| AGW（完整） | 63.6 | 62.0 | 50.3 | 68.3 | 49.3 | 14.7 |'
- en: 'TABLE VI: Comparison with the state-of-the-arts on four video-based Re-ID datasets,
    including MARS [[8](#bib.bib8)], DukeVideo [[144](#bib.bib144)], PRID2011 [[126](#bib.bib126)]
    and iLIDS-VID [[7](#bib.bib7)]. Rank-1 accuracy (%), mAP (%) and mINP (%) are
    reported.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：在四个基于视频的 Re-ID 数据集上的状态-of-the-arts 比较，包括 MARS [[8](#bib.bib8)]、DukeVideo
    [[144](#bib.bib144)]、PRID2011 [[126](#bib.bib126)] 和 iLIDS-VID [[7](#bib.bib7)]。报告了
    Rank-1 准确率（%）、mAP (%) 和 mINP (%)。
- en: '|  | MARS [[8](#bib.bib8)] | DukeVideo [[144](#bib.bib144)] |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | MARS [[8](#bib.bib8)] | DukeVideo [[144](#bib.bib144)] |'
- en: '| Method | R1 | mAP | mINP | R1 | mAP | mINP |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | R1 | mAP | mINP | R1 | mAP | mINP |'
- en: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 85.8 | 81.6 | 62.0 | 92.6 | 92.4
    | 88.3 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 85.8 | 81.6 | 62.0 | 92.6 | 92.4
    | 88.3 |'
- en: '| CoSeg [[132](#bib.bib132)]  ICCV19 | 84.9 | 79.9 | 57.8 | 95.4 | 94.1 | 89.8
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| CoSeg [[132](#bib.bib132)]  ICCV19 | 84.9 | 79.9 | 57.8 | 95.4 | 94.1 | 89.8
    |'
- en: '| AGW (Ours) | 87.0 | 82.2 | 62.8 | 94.6 | 93.4 | 89.2 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| AGW（我们的方法） | 87.0 | 82.2 | 62.8 | 94.6 | 93.4 | 89.2 |'
- en: '| AGW[+] (Ours) | 87.6 | 83.0 | 63.9 | 95.4 | 94.9 | 91.9 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| AGW[+]（我们的方法） | 87.6 | 83.0 | 63.9 | 95.4 | 94.9 | 91.9 |'
- en: '|  | PRID2011 [[126](#bib.bib126)] | iLIDS-VID [[7](#bib.bib7)] |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | PRID2011 [[126](#bib.bib126)] | iLIDS-VID [[7](#bib.bib7)] |'
- en: '| Method | R1 | R5 | mINP | R1 | R5 | mINP |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | R1 | R5 | mINP | R1 | R5 | mINP |'
- en: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 84.3 | 93.3 | 88.5 | 74.0 | 93.3
    | 82.2 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 84.3 | 93.3 | 88.5 | 74.0 | 93.3
    | 82.2 |'
- en: '| AGW (Ours) | 87.8 | 96.6 | 91.7 | 78.0 | 97.0 | 85.5 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| AGW（我们的方法） | 87.8 | 96.6 | 91.7 | 78.0 | 97.0 | 85.5 |'
- en: '| AGW[+] (Ours) | 94.4 | 98.4 | 95.4 | 83.2 | 98.3 | 89.0 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| AGW[+]（我们的方法） | 94.4 | 98.4 | 95.4 | 83.2 | 98.3 | 89.0 |'
- en: 'TABLE VII: Comparison with the state-of-the-arts on two partial Re-ID datasets,
    including Partial-REID and Partial-iLIDS. Rank-1, -3 accuracy (%) and mINP (%)
    are reported.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：在两个部分 Re-ID 数据集上的状态-of-the-arts 比较，包括 Partial-REID 和 Partial-iLIDS。报告了
    Rank-1、Rank-3 准确率（%）和 mINP (%)。
- en: '| Method | Partial-REID | Partial-iLIDS |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Partial-REID | Partial-iLIDS |'
- en: '| --- | --- | --- |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| R1 | R3 | mINP | R1 | R3 | mINP |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| R1 | R3 | mINP | R1 | R3 | mINP |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| DSR [[232](#bib.bib232)]  CVPR18 | 50.7 | 70.0 | - | 58.8 | 67.2 | - |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| DSR [[232](#bib.bib232)]  CVPR18 | 50.7 | 70.0 | - | 58.8 | 67.2 | - |'
- en: '| SFR [[249](#bib.bib249)]  ArXiv18 | 56.9 | 78.5 | - | 63.9 | 74.8 | - |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| SFR [[249](#bib.bib249)]  ArXiv18 | 56.9 | 78.5 | - | 63.9 | 74.8 | - |'
- en: '| VPM [[67](#bib.bib67)]  CVPR19 | 67.7 | 81.9 | - | 67.2 | 76.5 | - |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| VPM [[67](#bib.bib67)]  CVPR19 | 67.7 | 81.9 | - | 67.2 | 76.5 | - |'
- en: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 62.0 | 74.0 | 45.4 | 58.8 | 73.9
    | 68.7 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 62.0 | 74.0 | 45.4 | 58.8 | 73.9
    | 68.7 |'
- en: '| AGW | 69.7 | 80.0 | 56.7 | 64.7 | 79.8 | 73.3 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| AGW | 69.7 | 80.0 | 56.7 | 64.7 | 79.8 | 73.3 |'
- en: 'The overall framework of AGW is shown in Fig [8](#S4.F8 "Figure 8 ‣ 4.2 A New
    Baseline for Single-/Cross-Modality Re-ID ‣ 4 An Outlook: Re-ID in Next Era ‣
    Deep Learning for Person Re-identification: A Survey and Outlook"). Other components
    are exactly the same as [[122](#bib.bib122)]. In the testing phase, the output
    of BN layer is adopted as the feature representation for Re-ID. The implementation
    details and more experimental results are in the supplementary material.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 'AGW的总体框架如图 [8](#S4.F8 "Figure 8 ‣ 4.2 A New Baseline for Single-/Cross-Modality
    Re-ID ‣ 4 An Outlook: Re-ID in Next Era ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook") 所示。其他组件与 [[122](#bib.bib122)] 完全相同。在测试阶段，BN层的输出被用作Re-ID的特征表示。实施细节和更多实验结果见补充材料。'
- en: 'Results on Single-modality Image Re-ID. We first evaluate each component on
    two image-based datasets (Market-1501 and DukeMTMC) in Table [IV](#S4.T4 "TABLE
    IV ‣ 4.1 mINP: A New Evaluation Metric for Re-ID ‣ 4 An Outlook: Re-ID in Next
    Era ‣ Deep Learning for Person Re-identification: A Survey and Outlook"). We also
    list two state-of-the-art methods, BagTricks [[122](#bib.bib122)] and ABD-Net
    [[173](#bib.bib173)]. We report the results on CUHK03 and MSMT17 datasets in Table
    [V](#S4.T5 "TABLE V ‣ 4.2 A New Baseline for Single-/Cross-Modality Re-ID ‣ 4
    An Outlook: Re-ID in Next Era ‣ Deep Learning for Person Re-identification: A
    Survey and Outlook"). We obtain the following two observations:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '单模态图像Re-ID的结果。我们首先在两个基于图像的数据集（Market-1501 和 DukeMTMC）上评估每个组件，结果见表 [IV](#S4.T4
    "TABLE IV ‣ 4.1 mINP: A New Evaluation Metric for Re-ID ‣ 4 An Outlook: Re-ID
    in Next Era ‣ Deep Learning for Person Re-identification: A Survey and Outlook")。我们还列出了两种最先进的方法，BagTricks
    [[122](#bib.bib122)] 和 ABD-Net [[173](#bib.bib173)]。在表 [V](#S4.T5 "TABLE V ‣ 4.2
    A New Baseline for Single-/Cross-Modality Re-ID ‣ 4 An Outlook: Re-ID in Next
    Era ‣ Deep Learning for Person Re-identification: A Survey and Outlook") 中报告了
    CUHK03 和 MSMT17 数据集的结果。我们得出以下两个观察结论：'
- en: 1) All the components consistently contribute the accuracy gain, and AGW performs
    much better than the original BagTricks under various metrics. AGW provides a
    strong baseline for future improvements. We have also tried to incorporate part-level
    feature learning [[77](#bib.bib77)], but extensive experiments show that it does
    not improve the performance. How to aggregate part-level feature learning with
    AGW needs further study in the future. 2) Compared to the current state-of-the-art,
    ABD-Net [[173](#bib.bib173)], AGW performs favorably in most cases. In particular,
    we achieve much higher mINP on DukeMTMC dataset, 45.7% *vs*. 42.1%. This demonstrates
    that AGW requires less effort to find all the correct matches, verifying the ability
    of mINP.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 所有组件一致地提高了准确率，AGW 在各种指标下的表现远优于原始的 BagTricks。AGW 为未来的改进提供了强大的基准。我们也尝试了部分级特征学习
    [[77](#bib.bib77)]，但广泛的实验表明它没有提高性能。如何将部分级特征学习与 AGW 结合仍需在未来进一步研究。2) 与当前最先进的 ABD-Net
    [[173](#bib.bib173)] 相比，AGW 在大多数情况下表现较好。特别是在 DukeMTMC 数据集上，我们达到了更高的 mINP，45.7%
    *vs*. 42.1%。这表明 AGW 需要更少的努力来找到所有正确的匹配，验证了 mINP 的能力。
- en: 'Results on Single-modality Video Re-ID. We also evaluate the proposed AGW on
    four widely used single modality video-based datasets ( MARS [[8](#bib.bib8)],
    DukeVideo [[144](#bib.bib144)], PRID2011 [[126](#bib.bib126)] and iLIDS-VID [[7](#bib.bib7)],
    as shown in Table [VI](#S4.T6 "TABLE VI ‣ 4.2 A New Baseline for Single-/Cross-Modality
    Re-ID ‣ 4 An Outlook: Re-ID in Next Era ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook"). We also compare two state-of-the-art methods, BagTricks
    [[122](#bib.bib122)] and Co-Seg [[132](#bib.bib132)]. For video data, we develop
    a variant (AGW[+]) to capture the temporal information with frame-level average
    pooling for sequence representation. Meanwhile, constraint random sampling strategy
    [[133](#bib.bib133)] is applied for training. Compared to Co-Seg [[132](#bib.bib132)],
    our AGW[+] obtains better Rank-1, mAP and mINP in most cases.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '单模态视频 Re-ID 的结果。我们还在四个广泛使用的单模态视频数据集（MARS [[8](#bib.bib8)]、DukeVideo [[144](#bib.bib144)]、PRID2011
    [[126](#bib.bib126)] 和 iLIDS-VID [[7](#bib.bib7)]）上评估了所提出的 AGW，结果见表 [VI](#S4.T6
    "TABLE VI ‣ 4.2 A New Baseline for Single-/Cross-Modality Re-ID ‣ 4 An Outlook:
    Re-ID in Next Era ‣ Deep Learning for Person Re-identification: A Survey and Outlook")。我们还比较了两种最先进的方法，BagTricks
    [[122](#bib.bib122)] 和 Co-Seg [[132](#bib.bib132)]。对于视频数据，我们开发了一个变体（AGW[+]），通过帧级平均池化来捕捉时间信息以进行序列表示。同时，应用了约束随机采样策略
    [[133](#bib.bib133)] 进行训练。与 Co-Seg [[132](#bib.bib132)] 相比，我们的 AGW[+] 在大多数情况下获得了更好的
    Rank-1、mAP 和 mINP。'
- en: 'TABLE VIII: Comparison with the state-of-the-arts on cross-modality visible-infrared
    Re-ID. Rank-1 accuracy (%), mAP (%) and mINP (%) are reported on two public datasets.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VIII：在跨模态可见光-红外 Re-ID 上与最先进技术的比较。报告了两个公共数据集上的 Rank-1 准确率（%）、mAP（%）和 mINP（%）。
- en: '|  | RegDB [[60](#bib.bib60)] | SYSU-MM01 [[21](#bib.bib21)] |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | RegDB [[60](#bib.bib60)] | SYSU-MM01 [[21](#bib.bib21)] |'
- en: '|  | *Visible-Thermal* | *All Search* | *Indoor Search* |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | *可见-热成像* | *全搜索* | *室内搜索* |'
- en: '| Method | R1 | mAP | R1 | mAP | R1 | mAP |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | R1 | mAP | R1 | mAP | R1 | mAP |'
- en: '| Zero-Pad [[21](#bib.bib21)]  ICCV17 | 17.75 | 18.90 | 14.8 | 15.95 | 20.58
    | 26.92 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| Zero-Pad [[21](#bib.bib21)]  ICCV17 | 17.75 | 18.90 | 14.8 | 15.95 | 20.58
    | 26.92 |'
- en: '| HCML [[186](#bib.bib186)]  AAAI18 | 24.44 | 20.08 | 14.32 | 16.16 | 24.52
    | 30.08 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| HCML [[186](#bib.bib186)]  AAAI18 | 24.44 | 20.08 | 14.32 | 16.16 | 24.52
    | 30.08 |'
- en: '| eBDTR [[142](#bib.bib142)]  TIFS19 | 34.62 | 33.46 | 27.82 | 28.42 | 32.46
    | 42.46 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| eBDTR [[142](#bib.bib142)]  TIFS19 | 34.62 | 33.46 | 27.82 | 28.42 | 32.46
    | 42.46 |'
- en: '| HSME [[187](#bib.bib187)]  AAAI19 | 50.85 | 47.00 | 20.68 | 23.12 | - | -
    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| HSME [[187](#bib.bib187)]  AAAI19 | 50.85 | 47.00 | 20.68 | 23.12 | - | -
    |'
- en: '| D²RL [[189](#bib.bib189)]  CVPR19 | 43.4 | 44.1 | 28.9 | 29.2 | - | - |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| D²RL [[189](#bib.bib189)]  CVPR19 | 43.4 | 44.1 | 28.9 | 29.2 | - | - |'
- en: '| AlignG [[190](#bib.bib190)]  ICCV19 | 57.9 | 53.6 | 42.4 | 40.7 | 45.9 |
    54.3 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| AlignG [[190](#bib.bib190)]  ICCV19 | 57.9 | 53.6 | 42.4 | 40.7 | 45.9 |
    54.3 |'
- en: '| Hi-CMD [[191](#bib.bib191)]  CVPR20 | 70.93 | 66.04 | 34.9 | 35.9 |  |  |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Hi-CMD [[191](#bib.bib191)]  CVPR20 | 70.93 | 66.04 | 34.9 | 35.9 |  |  |'
- en: '| AGW (Ours) | 70.05 | 66.37 | 47.50 | 47.65 | 54.17 | 62.97 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| AGW (我们的方法) | 70.05 | 66.37 | 47.50 | 47.65 | 54.17 | 62.97 |'
- en: '| mINP = 50.19 | mINP =35.30 | mINP = 59.23 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| mINP = 50.19 | mINP =35.30 | mINP = 59.23 |'
- en: 'Results on Partial Re-ID. We also test the performance of AGW on two partial
    Re-ID datasets, as shown in Table [VII](#S4.T7 "TABLE VII ‣ 4.2 A New Baseline
    for Single-/Cross-Modality Re-ID ‣ 4 An Outlook: Re-ID in Next Era ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook"). The experimental setting
    are from DSR [[232](#bib.bib232)]. We also achieve comparable performance with
    the state-of-the-art VPM method [[67](#bib.bib67)]. This experiment further demonstrates
    the superiority of AGW for the open-world partial Re-ID task. Meanwhile, the mINP
    also shows the applicability for this open-world Re-ID problem.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '部分 Re-ID 的结果。我们还测试了 AGW 在两个部分 Re-ID 数据集上的表现，如表 [VII](#S4.T7 "TABLE VII ‣ 4.2
    A New Baseline for Single-/Cross-Modality Re-ID ‣ 4 An Outlook: Re-ID in Next
    Era ‣ Deep Learning for Person Re-identification: A Survey and Outlook") 所示。实验设置来自
    DSR [[232](#bib.bib232)]。我们还与最先进的 VPM 方法 [[67](#bib.bib67)] 达到了可比的性能。这项实验进一步证明了
    AGW 在开放世界部分 Re-ID 任务中的优越性。同时，mINP 也显示了其在这一开放世界 Re-ID 问题中的适用性。'
- en: 'Results on Cross-modality Re-ID. We also test the performance of AGW using
    a two-stream architecture on the cross-modality visible-infrared Re-ID task. The
    comparison with the current state-of-the-arts on two datasets is shown in Table
    [VIII](#S4.T8 "TABLE VIII ‣ 4.2 A New Baseline for Single-/Cross-Modality Re-ID
    ‣ 4 An Outlook: Re-ID in Next Era ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook"). We follow the settings in AlignG [[190](#bib.bib190)]
    to perform the experiments. Results show that AGW achieves much higher accuracy
    than existing cross-modality Re-ID models, verifying the effectiveness for the
    open-world Re-ID task.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '跨模态 Re-ID 的结果。我们还测试了 AGW 在跨模态可见-红外 Re-ID 任务中的表现，使用了双流架构。与两个数据集上的当前最先进方法的比较见表
    [VIII](#S4.T8 "TABLE VIII ‣ 4.2 A New Baseline for Single-/Cross-Modality Re-ID
    ‣ 4 An Outlook: Re-ID in Next Era ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")。我们遵循 AlignG [[190](#bib.bib190)] 的设置进行实验。结果表明，AGW 的准确性远高于现有的跨模态
    Re-ID 模型，验证了其在开放世界 Re-ID 任务中的有效性。'
- en: 4.3 Under-Investigated Open Issues
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 尚未研究的开放问题
- en: 'We discuss the open-issues from five different aspects according to the five
    steps in §[1](#S1 "1 Introduction ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook"), including uncontrollable data collection, human annotation
    minimization, domain-specific/generalizable architecture design, dynamic model
    updating and efficient model deployment.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从五个不同的方面讨论了开放问题，这五个方面对应于 §[1](#S1 "1 Introduction ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook") 中的五个步骤，包括不可控的数据收集、最小化人工标注、特定领域/通用架构设计、动态模型更新和高效模型部署。'
- en: 4.3.1 Uncontrollable Data Collection
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 不可控的数据收集
- en: Most existing Re-ID works evaluate their method on a well-defined data collection
    environment. However, the data collection in real complex environment is uncontrollable.
    The data might be captured from unpredictable modality, modality combinations,
    or even cloth changing data [[30](#bib.bib30)].
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有的 Re-ID 工作在定义明确的数据收集环境中评估其方法。然而，现实复杂环境中的数据收集是不可控的。这些数据可能来自不可预测的模态、模态组合，甚至是换衣数据
    [[30](#bib.bib30)]。
- en: Multi-Heterogeneous Data. In real applications, the Re-ID data might be captured
    from multiple heterogeneous modalities, *i.e*., the resolutions of person images
    vary a lot [[193](#bib.bib193)], both the query and gallery sets may contain different
    modalities (visible, thermal [[21](#bib.bib21)], depth [[62](#bib.bib62)] or text
    description [[10](#bib.bib10)]). This results in a challenging multiple heterogeneous
    person Re-ID. A good person Re-ID system would be able to automatically handle
    the changing resolutions, different modalities, various environments and multiple
    domains. Future work with broad generalizability is expected, evaluating their
    method for different Re-ID tasks.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 多异质数据。在实际应用中，Re-ID数据可能来自多个异质模态，即，人物图像的分辨率差异很大[[193](#bib.bib193)]，查询集和库集可能包含不同的模态（可见、热成像[[21](#bib.bib21)]、深度[[62](#bib.bib62)]或文本描述[[10](#bib.bib10)]）。这导致了一个具有挑战性的多异质人物Re-ID问题。一个好的人物Re-ID系统应该能够自动处理分辨率变化、不同模态、各种环境和多个领域。未来的工作需要具备广泛的泛化能力，并评估其方法在不同Re-ID任务中的效果。
- en: Cloth-Changing Data. In practical surveillance system, it is very likely to
    contain a large number of target persons with changing clothes. A cloth-Clothing
    Change Aware Network (CCAN) [[250](#bib.bib250)] addresses this issue by separately
    extracting the face and body context representation, and similar idea is applied
    in [[251](#bib.bib251)]. Yang et al. [[30](#bib.bib30)] present a spatial polar
    transformation (SPT) to learn cross-cloth invariant representation. However, they
    still rely heavily on the face and body appearance, which might be unavailable
    and unstable in real scenarios. It would be interesting to further explore the
    possibility of other discriminative cues (e.g., gait, shape) to address the cloth-changing
    issue.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 换衣数据。在实际的监控系统中，目标人物可能会有大量换衣服的情况。一个名为换衣服识别网络（CCAN）[[250](#bib.bib250)]的方法通过分别提取面部和身体背景表示来解决这个问题，相似的思想也应用在[[251](#bib.bib251)]中。杨等人[[30](#bib.bib30)]提出了一种空间极坐标变换（SPT）来学习跨衣物不变表示。然而，他们仍然严重依赖于面部和身体的外观，这在实际场景中可能不可用或不稳定。进一步探索其他辨别线索（例如步态、形状）来解决换衣问题将是很有趣的。
- en: 4.3.2 Human Annotation Minimization
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 人工标注最小化
- en: Besides the unsupervised learning, active learning or human interaction [[24](#bib.bib24),
    [27](#bib.bib27), [154](#bib.bib154), [159](#bib.bib159)] provides another possible
    solution to alleviate the reliance on human annotation.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 除了无监督学习，主动学习或人机互动[[24](#bib.bib24), [27](#bib.bib27), [154](#bib.bib154), [159](#bib.bib159)]提供了另一种可能的解决方案，以减少对人工标注的依赖。
- en: Active Learning. Incorporating human interaction, labels are easily provided
    for newly arriving data and the model can be subsequently updated [[27](#bib.bib27),
    [24](#bib.bib24)]. A pairwise subset selection framework [[252](#bib.bib252)]
    minimizes human labeling effort by firstly constructing an edge-weighted complete
    $k$-partite graph and then solving it as a triangle free subgraph maximization
    problem. Along this line, a deep reinforcement active learning method [[154](#bib.bib154)]
    iteratively refines the learning policy and trains a Re-ID network with human-in-the-loop
    supervision. For video data, an interpretable reinforcement learning method with
    sequential decision making [[178](#bib.bib178)] is designed. The active learning
    is crucial in practical Re-ID system design, but it has received less attention
    in the research community. Additionally, the newly arriving identities is extremely
    challenging, even for human. Efficient human in-the-loop active learning is expected
    in the future.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习。通过人机互动，可以为新到达的数据轻松提供标签，然后模型可以随之更新[[27](#bib.bib27), [24](#bib.bib24)]。一种成对子集选择框架[[252](#bib.bib252)]通过首先构建一个边权重完全的$k$-部分图，然后将其作为一个三角形无子图最大化问题来最小化人工标注工作量。沿着这个方向，一种深度强化主动学习方法[[154](#bib.bib154)]迭代地优化学习策略，并在有人工监督的情况下训练一个Re-ID网络。对于视频数据，设计了一种具有序列决策的可解释强化学习方法[[178](#bib.bib178)]。主动学习在实际的Re-ID系统设计中至关重要，但在研究界受到了较少关注。此外，新到达的身份对人类来说也是极具挑战的。未来有望出现高效的人机互动主动学习。
- en: Learning for Virtual Data. This provides an alternative for minimizing the human
    annotation. A synthetic dataset is collected in [[220](#bib.bib220)] for training,
    and they achieve competitive performance on real-world datasets when trained on
    this synthesized dataset. Bak et al. [[125](#bib.bib125)] generate a new synthetic
    dataset with different illumination conditions to model realistic indoor and outdoor
    lighting. A large-scale synthetic PersonX dataset is collected in [[105](#bib.bib105)]
    to systematically study the effect of viewpoint for a person Re-ID system. Recently,
    the 3D person images are also studied in [[253](#bib.bib253)], generating the
    3D body structure from 2D images. However, how to bridge the gap between synthesized
    images and real-world datasets remains challenging.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟数据学习。这提供了一种减少人工标注的替代方案。收集了一个合成数据集用于训练，[[220](#bib.bib220)]中的研究显示，当在这个合成数据集上训练时，他们在真实世界数据集上取得了有竞争力的表现。Bak等人[[125](#bib.bib125)]生成了一个具有不同光照条件的新合成数据集，用于模拟现实的室内和室外光照。[[105](#bib.bib105)]中收集了一个大规模的合成PersonX数据集，用于系统地研究视角对人物重识别系统的影响。最近，[[253](#bib.bib253)]也研究了3D人像，从2D图像生成3D身体结构。然而，如何弥合合成图像与真实世界数据集之间的差距仍然具有挑战性。
- en: 4.3.3 Domain-Specific/Generalizable Architecture Design
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 特定领域/通用架构设计
- en: Re-ID Specific Architecture. Existing Re-ID methods usually adopt architectures
    designed for image classification as the backbone. Some methods modify the architecture
    to achieve better Re-ID features [[82](#bib.bib82), [122](#bib.bib122)]. Very
    recently, researchers have started to design domain specific architectures, *e.g*.,
    OSNet with omni-scale feature learning [[138](#bib.bib138)]. It detects the small-scale
    discriminative features at a certain scale. OSNet is extremely lightweight and
    achieves competitive performance. With the advancement of automatic neural architecture
    search (*e.g*., Auto-ReID [[139](#bib.bib139)]), more domain-specific powerful
    architectures are expected to address the task-specific Re-ID challenges. Limited
    training samples in Re-ID also increase the difficulty in architecture design.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 重识别特定架构。现有的重识别方法通常采用为图像分类设计的架构作为骨干网络。一些方法修改了架构以实现更好的重识别特征[[82](#bib.bib82),
    [122](#bib.bib122)]。最近，研究人员已经开始设计特定领域的架构，例如，具有全尺度特征学习的OSNet[[138](#bib.bib138)]。它在特定尺度上检测小尺度的区分特征。OSNet极其轻量化，并且取得了有竞争力的表现。随着自动神经架构搜索的发展（例如，Auto-ReID[[139](#bib.bib139)]），预计会出现更多领域特定的强大架构来解决任务特定的重识别挑战。重识别中的有限训练样本也增加了架构设计的难度。
- en: Domain Generalizable Re-ID. It is well recognized that there is a large domain
    gap between different datsets [[56](#bib.bib56), [225](#bib.bib225)]. Most existing
    methods adopt domain adaptation for cross-dataset training. A more practical solution
    would be learning a domain generalized model with a number of source datasets,
    such that the learned model can be generalized to new unseen datasets for discriminative
    Re-ID without additional training [[28](#bib.bib28)]. Hu et al. [[254](#bib.bib254)]
    studied the cross-dataset person Re-ID by introducing a part-level CNN framework.
    The Domain-Invariant Mapping Network (DIMN) [[28](#bib.bib28)] designs a meta-learning
    pipeline for domain generalizable Re-ID, learning a mapping between a person image
    and its identity classifier. The domain generalizability is crucial to deploy
    the learned Re-ID model under an unknown scenario.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 域通用重识别。大家都认识到不同数据集之间存在较大的领域差距[[56](#bib.bib56), [225](#bib.bib225)]。大多数现有方法采用领域适应进行跨数据集训练。更实际的解决方案是学习一个领域通用的模型，使用多个源数据集，使得学习到的模型能够推广到新的未见数据集进行区分性重识别，而无需额外训练[[28](#bib.bib28)]。Hu等人[[254](#bib.bib254)]通过引入一个部件级CNN框架来研究跨数据集人物重识别。领域不变映射网络（DIMN）[[28](#bib.bib28)]设计了一个元学习管道用于领域通用的重识别，学习人物图像与其身份分类器之间的映射。领域通用性对于在未知场景下部署学习到的重识别模型至关重要。
- en: 4.3.4 Dynamic Model Updating
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 动态模型更新
- en: Fixed model is inappropriate for practical dynamically updated surveillance
    system. To alleviate this issue, dynamic model updating is imperative, either
    to a new domain/camera or adaptation with newly collected data.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 固定模型不适用于实际的动态更新监控系统。为了解决这个问题，动态模型更新是必要的，无论是对新领域/相机还是通过新收集的数据进行适应。
- en: Model Adaptation to New Domain/Camera. Model adaptation to a new domain has
    been widely studied in the literature as a domain adaptation problem [[125](#bib.bib125),
    [216](#bib.bib216)]. In practical dynamic camera network, a new camera may be
    temporarily inserted into an existing surveillance system. Model adaptation is
    crucial for continuous identification in a multi-camera network [[23](#bib.bib23),
    [29](#bib.bib29)]. To adapt a learned model to a new camera, a transitive inference
    algorithm [[244](#bib.bib244)] is designed to exploit the best source camera model
    based on a geodesic flow kernel. However, it is still challenging when the newly
    collected data by the new camera has totally different distributions. In addition,
    the privacy and efficiency issue [[255](#bib.bib255)] also need further consideration.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 模型适应新领域/摄像头。模型适应新领域在文献中作为领域适应问题得到了广泛研究[[125](#bib.bib125), [216](#bib.bib216)]。在实际的动态摄像头网络中，可能会有新的摄像头暂时插入到现有的监控系统中。模型适应对于多摄像头网络中的持续识别至关重要[[23](#bib.bib23),
    [29](#bib.bib29)]。为了将学习到的模型适应到新的摄像头，设计了一种传递推理算法[[244](#bib.bib244)]，它利用基于测地流核的最佳源摄像头模型。然而，当新摄像头收集的数据具有完全不同的分布时，这仍然具有挑战性。此外，隐私和效率问题[[255](#bib.bib255)]也需要进一步考虑。
- en: Model Updating with Newly Arriving Data. With the newly collected data, it is
    impractical to training the previously learned model from the scratch [[24](#bib.bib24)].
    An incremental learning approach together with human interaction is designed in
    [[24](#bib.bib24)]. For deeply learned model, an addition using covariance loss
    [[256](#bib.bib256)] is integrated in the overall learning function. However,
    this problem is not well studied since the deep model training require large amount
    of training data. Besides, the unknown new identities in the newly arriving data
    is hard to be identified for the model updating.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新到达数据更新模型。使用新收集的数据，从头训练之前学习的模型是不切实际的[[24](#bib.bib24)]。在[[24](#bib.bib24)]中设计了一种增量学习方法，结合了人工交互。对于深度学习模型，整体学习函数中集成了使用协方差损失[[256](#bib.bib256)]的附加功能。然而，由于深度模型训练需要大量的训练数据，这个问题尚未得到充分研究。此外，新到达数据中的未知新身份难以识别用于模型更新。
- en: 4.3.5 Efficient Model Deployment
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5 高效模型部署
- en: It is important to design efficient and adaptive models to address scalability
    issue for practical model deployment.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 设计高效且自适应的模型以解决实际模型部署中的可扩展性问题是非常重要的。
- en: Fast Re-ID. For fast retrieval, hashing has been extensively studied to boost
    the searching speed, approximating the nearest neighbor search [[257](#bib.bib257)].
    Cross-camera Semantic Binary Transformation (CSBT) [[258](#bib.bib258)] transforms
    the original high-dimensional feature representations into compact low-dimensional
    identity-preserving binary codes. A Coarse-to-Fine (CtF) hashing code search strategy
    is developed in [[259](#bib.bib259)], complementarily using short and long codes.
    However, the domain-specific hashing still needs further study.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 快速重识别（Fast Re-ID）。为了快速检索，哈希方法已被广泛研究以提升搜索速度，从而近似最近邻搜索[[257](#bib.bib257)]。跨摄像头语义二进制变换（CSBT）[[258](#bib.bib258)]将原始高维特征表示转换为紧凑的低维身份保持二进制代码。在[[259](#bib.bib259)]中开发了一种粗到细（CtF）哈希码搜索策略，互补使用短码和长码。然而，特定领域的哈希仍需进一步研究。
- en: Lightweight Model. Another direction for addressing the scalability issue is
    to design a lightweight Re-ID model. Modifying the network architecture to achieve
    a lightweight model is investigated in [[86](#bib.bib86), [139](#bib.bib139),
    [138](#bib.bib138)]. Model distillation is another approach, *e.g*., a multi-teacher
    adaptive similarity distillation framework is proposed in [[260](#bib.bib260)],
    which learns a user-specified lightweight student model from multiple teacher
    models, without access to source domain data.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 轻量级模型。解决可扩展性问题的另一个方向是设计轻量级重识别模型。在[[86](#bib.bib86), [139](#bib.bib139), [138](#bib.bib138)]中研究了修改网络架构以实现轻量级模型的方法。模型蒸馏是另一种方法，例如，在[[260](#bib.bib260)]中提出了一种多教师自适应相似度蒸馏框架，该框架从多个教师模型中学习用户指定的轻量级学生模型，而无需访问源领域数据。
- en: Resource Aware Re-ID. Adaptively adjusting the model according to the hardware
    configurations also provides a solution to handle the scalability issue. Deep
    Anytime Re-ID (DaRe) [[14](#bib.bib14)] employs a simple distance based routing
    strategy to adaptively adjust the model, fitting to hardware devices with different
    computational resources.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 资源感知Re-ID。根据硬件配置自适应调整模型也提供了处理可扩展性问题的解决方案。Deep Anytime Re-ID（DaRe）[[14](#bib.bib14)]采用简单的基于距离的路由策略来自适应调整模型，适配具有不同计算资源的硬件设备。
- en: 5 Concluding Remarks
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'This paper presents a comprehensive survey with in-depth analysis from a both
    closed-world and open-world perspectives. We first introduce the widely studied
    person Re-ID under the closed-world setting from three aspects: feature representation
    learning, deep metric learning and ranking optimization. With powerful deep learning,
    the closed-world person Re-ID has achieved performance saturation on several datasets.
    Correspondingly, the open-world setting has recently gained increasing attention,
    with efforts to address various practical challenges. We also design a new AGW
    baseline, which achieves competitive performance on four Re-ID tasks under various
    metrics. It provides a strong baseline for future improvements. This survey also
    introduces a new evaluation metric to measure the cost of finding all the correct
    matches. We believe this survey will provide important guidance for future Re-ID
    research.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了从封闭世界和开放世界两种视角的综合调查和深入分析。我们首先从特征表示学习、深度度量学习和排序优化三个方面介绍了在封闭世界环境下广泛研究的人物Re-ID。通过强大的深度学习，封闭世界的人物Re-ID在多个数据集上已达到性能饱和。相应地，开放世界环境最近也获得了越来越多的关注，致力于解决各种实际挑战。我们还设计了一个新的AGW基准，在各种度量下在四个Re-ID任务上表现出竞争力。这为未来的改进提供了强有力的基准。该调查还引入了一种新的评估指标，用于衡量找到所有正确匹配的成本。我们相信，这项调查将为未来的Re-ID研究提供重要的指导。
- en: References
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y.-C. Chen, X. Zhu, W.-S. Zheng, and J.-H. Lai, “Person re-identification
    by camera correlation aware feature augmentation,” *IEEE TPAMI*, vol. 40, no. 2,
    pp. 392–408, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y.-C. Chen, X. Zhu, W.-S. Zheng 和 J.-H. Lai，“通过摄像头相关特征增强进行人物再识别”，*IEEE
    TPAMI*，第40卷，第2期，第392–408页，2018年。'
- en: '[2] L. Zheng, Y. Yang, and A. G. Hauptmann, “Person re-identification: Past,
    present and future,” *arXiv preprint arXiv:1610.02984*, 2016.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] L. Zheng, Y. Yang 和 A. G. Hauptmann，“人物再识别：过去、现在与未来”，*arXiv预印本arXiv:1610.02984*，2016年。'
- en: '[3] N. Gheissari, T. B. Sebastian, and R. Hartley, “Person reidentification
    using spatiotemporal appearance,” in *CVPR*, 2006, pp. 1528–1535.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] N. Gheissari, T. B. Sebastian 和 R. Hartley，“基于时空外观的人物再识别”，发表于*CVPR*，2006年，第1528–1535页。'
- en: '[4] J. Almazan, B. Gajic, N. Murray, and D. Larlus, “Re-id done right: towards
    good practices for person re-identification,” *arXiv preprint arXiv:1801.05339*,
    2018.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Almazan, B. Gajic, N. Murray 和 D. Larlus，“做对Re-ID：朝着良好的人物再识别实践迈进”，*arXiv预印本arXiv:1801.05339*，2018年。'
- en: '[5] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable person
    re-identification: A benchmark,” in *ICCV*, 2015, pp. 1116–1124.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang 和 Q. Tian，“可扩展的人物再识别：一个基准”，发表于*ICCV*，2015年，第1116–1124页。'
- en: '[6] N. Martinel, G. Luca Foresti, and C. Micheloni, “Aggregating deep pyramidal
    representations for person re-identification,” in *CVPR Workshops*, 2019, pp.
    0–0.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] N. Martinel, G. Luca Foresti 和 C. Micheloni，“聚合深度金字塔表示进行人物再识别”，发表于*CVPR
    Workshops*，2019年，第0–0页。'
- en: '[7] T. Wang, S. Gong, X. Zhu, and S. Wang, “Person re-identification by video
    ranking,” in *ECCV*, 2014, pp. 688–703.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] T. Wang, S. Gong, X. Zhu 和 S. Wang，“通过视频排序进行人物再识别”，发表于*ECCV*，2014年，第688–703页。'
- en: '[8] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and Q. Tian, “Mars:
    A video benchmark for large-scale person re-identification,” in *ECCV*, 2016.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang 和 Q. Tian，“Mars：一个大规模人物再识别的视频基准”，发表于*ECCV*，2016年。'
- en: '[9] M. Ye, C. Liang, Z. Wang, Q. Leng, J. Chen, and J. Liu, “Specific person
    retrieval via incomplete text description,” in *ACM ICMR*, 2015, pp. 547–550.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] M. Ye, C. Liang, Z. Wang, Q. Leng, J. Chen 和 J. Liu，“通过不完整文本描述进行特定人物检索”，发表于*ACM
    ICMR*，2015年，第547–550页。'
- en: '[10] S. Li, T. Xiao, H. Li, W. Yang, and X. Wang, “Identity-aware textual-visual
    matching with latent co-attention,” in *ICCV*, 2017, pp. 1890–1899.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. Li, T. Xiao, H. Li, W. Yang 和 X. Wang，“具有潜在共同注意的身份感知文本-视觉匹配”，发表于*ICCV*，2017年，第1890–1899页。'
- en: '[11] S. Karanam, Y. Li, and R. J. Radke, “Person re-identification with discriminatively
    trained viewpoint invariant dictionaries,” in *ICCV*, 2015, pp. 4516–4524.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Karanam, Y. Li 和 R. J. Radke，“通过判别性训练的视角不变字典进行人物再识别”，发表于*ICCV*，2015年，第4516–4524页。'
- en: '[12] S. Bak, S. Zaidenberg, B. Boulay, and F. Bremond, “Improving person re-identification
    by viewpoint cues,” in *AVSS*, 2014, pp. 175–180.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. Bak, S. Zaidenberg, B. Boulay, 和 F. Bremond，“通过视角提示改善人脸重新识别”，发表于*AVSS*，2014年，页码175–180。'
- en: '[13] X. Li, W.-S. Zheng, X. Wang, T. Xiang, and S. Gong, “Multi-scale learning
    for low-resolution person re-identification,” in *ICCV*, 2015, pp. 3765–3773.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] X. Li, W.-S. Zheng, X. Wang, T. Xiang, 和 S. Gong，“低分辨率人脸重新识别的多尺度学习”，发表于*ICCV*，2015年，页码3765–3773。'
- en: '[14] Y. Wang, L. Wang, Y. You, X. Zou, V. Chen, S. Li, G. Huang, B. Hariharan,
    and K. Q. Weinberger, “Resource aware person re-identification across multiple
    resolutions,” in *CVPR*, 2018, pp. 8042–8051.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Wang, L. Wang, Y. You, X. Zou, V. Chen, S. Li, G. Huang, B. Hariharan,
    和 K. Q. Weinberger，“跨多个分辨率的资源感知人脸重新识别”，发表于*CVPR*，2018年，页码8042–8051。'
- en: '[15] Y. Huang, Z.-J. Zha, X. Fu, and W. Zhang, “Illumination-invariant person
    re-identification,” in *ACM MM*, 2019, pp. 365–373.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Huang, Z.-J. Zha, X. Fu, 和 W. Zhang，“光照不变的人脸重新识别”，发表于*ACM MM*，2019年，页码365–373。'
- en: '[16] Y.-J. Cho and K.-J. Yoon, “Improving person re-identification via pose-aware
    multi-shot matching,” in *CVPR*, 2016, pp. 1354–1362.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y.-J. Cho 和 K.-J. Yoon，“通过姿态感知的多镜头匹配提高人脸重新识别”，发表于*CVPR*，2016年，页码1354–1362。'
- en: '[17] H. Zhao, M. Tian, S. Sun, and et al, “Spindle net: Person re-identification
    with human body region guided feature decomposition and fusion,” in *CVPR*, 2017,
    pp. 1077–1085.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] H. Zhao, M. Tian, S. Sun, 等，“Spindle net：利用人体区域指导特征分解和融合的人脸重新识别”，发表于*CVPR*，2017年，页码1077–1085。'
- en: '[18] M. S. Sarfraz, A. Schumann, A. Eberle, and R. Stiefelhagen, “A pose-sensitive
    embedding for person re-identification with expanded cross neighborhood re-ranking,”
    in *CVPR*, 2018, pp. 420–429.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. S. Sarfraz, A. Schumann, A. Eberle, 和 R. Stiefelhagen，“用于人脸重新识别的姿态敏感嵌入及扩展的交叉邻域重新排序”，发表于*CVPR*，2018年，页码420–429。'
- en: '[19] H. Huang, D. Li, Z. Zhang, X. Chen, and K. Huang, “Adversarially occluded
    samples for person re-identification,” in *CVPR*, 2018, pp. 5098–5107.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] H. Huang, D. Li, Z. Zhang, X. Chen, 和 K. Huang，“用于人脸重新识别的对抗遮挡样本”，发表于*CVPR*，2018年，页码5098–5107。'
- en: '[20] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, and X. Chen, “Vrstc: Occlusion-free
    video person re-identification,” in *CVPR*, 2019, pp. 7183–7192.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, 和 X. Chen，“Vrstc：无遮挡视频人脸重新识别”，发表于*CVPR*，2019年，页码7183–7192。'
- en: '[21] A. Wu, W.-s. Zheng, H.-X. Yu, S. Gong, and J. Lai, “Rgb-infrared cross-modality
    person re-identification,” in *ICCV*, 2017, pp. 5380–5389.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Wu, W.-s. Zheng, H.-X. Yu, S. Gong, 和 J. Lai，“RGB-红外跨模态人脸重新识别”，发表于*ICCV*，2017年，页码5380–5389。'
- en: '[22] C. Song, Y. Huang, W. Ouyang, and L. Wang, “Mask-guided contrastive attention
    model for person re-identification,” in *CVPR*, 2018, pp. 1179–1188.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] C. Song, Y. Huang, W. Ouyang, 和 L. Wang，“基于掩膜指导的对比注意力模型用于人脸重新识别”，发表于*CVPR*，2018年，页码1179–1188。'
- en: '[23] A. Das, R. Panda, and A. K. Roy-Chowdhury, “Continuous adaptation of multi-camera
    person identification models through sparse non-redundant representative selection,”
    *CVIU*, vol. 156, pp. 66–78, 2017.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Das, R. Panda, 和 A. K. Roy-Chowdhury，“通过稀疏非冗余代表选择进行多摄像头人脸识别模型的连续适应”，*CVIU*，第156卷，页码66–78，2017年。'
- en: '[24] N. Martinel, A. Das, C. Micheloni, and A. K. Roy-Chowdhury, “Temporal
    model adaptation for person re-identification,” in *ECCV*, 2016, pp. 858–877.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] N. Martinel, A. Das, C. Micheloni, 和 A. K. Roy-Chowdhury，“人脸重新识别的时间模型适应”，发表于*ECCV*，2016年，页码858–877。'
- en: '[25] J. Garcia, N. Martinel, A. Gardel, I. Bravo, G. L. Foresti, and C. Micheloni,
    “Discriminant context information analysis for post-ranking person re-identification,”
    *IEEE Transactions on Image Processing*, vol. 26, no. 4, pp. 1650–1665, 2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J. Garcia, N. Martinel, A. Gardel, I. Bravo, G. L. Foresti, 和 C. Micheloni，“用于后排序人脸重新识别的判别上下文信息分析”，*IEEE
    Transactions on Image Processing*，第26卷，第4期，页码1650–1665，2017年。'
- en: '[26] W.-S. Zheng, S. Gong, and T. Xiang, “Towards open-world person re-identification
    by one-shot group-based verification,” *IEEE TPAMI*, vol. 38, no. 3, pp. 591–606,
    2015.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] W.-S. Zheng, S. Gong, 和 T. Xiang，“通过单次群体验证迈向开放世界人脸重新识别”，*IEEE TPAMI*，第38卷，第3期，页码591–606，2015年。'
- en: '[27] A. Das, R. Panda, and A. Roy-Chowdhury, “Active image pair selection for
    continuous person re-identification,” in *ICIP*, 2015, pp. 4263–4267.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Das, R. Panda, 和 A. Roy-Chowdhury，“用于连续人脸重新识别的主动图像对选择”，发表于*ICIP*，2015年，页码4263–4267。'
- en: '[28] J. Song, Y. Yang, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Generalizable
    person re-identification by domain-invariant mapping network,” in *CVPR*, 2019,
    pp. 719–728.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Song, Y. Yang, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales，“通过领域不变映射网络实现通用的人脸重新识别”，发表于*CVPR*，2019年，页码719–728。'
- en: '[29] A. Das, A. Chakraborty, and A. K. Roy-Chowdhury, “Consistent re-identification
    in a camera network,” in *ECCV*, 2014, pp. 330–345.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] A. Das, A. Chakraborty, 和 A. K. Roy-Chowdhury，“在摄像头网络中的一致性重新识别”，发表于*ECCV*，2014年，页码330–345。'
- en: '[30] Q. Yang, A. Wu, and W. Zheng, “Person re-identification by contour sketch
    under moderate clothing change.” *IEEE TPAMI*, 2019.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Q. Yang, A. Wu 和 W. Zheng，“通过轮廓草图进行中等衣物变化下的人员重新识别。” *IEEE TPAMI*，2019年。'
- en: '[31] D. Gray and H. Tao, “Viewpoint invariant pedestrian recognition with an
    ensemble of localized features,” in *ECCV*, 2008, pp. 262–275.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. Gray 和 H. Tao，“通过局部特征集成实现视角不变的行人识别，”发表于 *ECCV*，2008年，第262–275页。'
- en: '[32] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and M. Cristani, “Person
    re-identification by symmetry-driven accumulation of local features,” in *CVPR*,
    2010, pp. 2360–2367.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. Farenzena, L. Bazzani, A. Perina, V. Murino 和 M. Cristani，“通过对称驱动的局部特征积累进行人员重新识别，”发表于
    *CVPR*，2010年，第2360–2367页。'
- en: '[33] Y. Yang, J. Yang, J. Yan, S. Liao, D. Yi, and S. Z. Li, “Salient color
    names for person re-identification,” in *ECCV*, 2014, pp. 536–551.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Yang, J. Yang, J. Yan, S. Liao, D. Yi 和 S. Z. Li，“用于人员重新识别的显著颜色名称，”发表于
    *ECCV*，2014年，第536–551页。'
- en: '[34] S. Liao, Y. Hu, X. Zhu, and S. Z. Li, “Person re-identification by local
    maximal occurrence representation and metric learning,” in *CVPR*, 2015, pp. 2197–2206.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Liao, Y. Hu, X. Zhu 和 S. Z. Li，“通过局部最大出现表示和度量学习进行人员重新识别，”发表于 *CVPR*，2015年，第2197–2206页。'
- en: '[35] T. Matsukawa, T. Okabe, E. Suzuki, and Y. Sato, “Hierarchical gaussian
    descriptor for person re-identification,” in *CVPR*, 2016, pp. 1363–1372.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T. Matsukawa, T. Okabe, E. Suzuki 和 Y. Sato，“用于人员重新识别的层次高斯描述符，”发表于 *CVPR*，2016年，第1363–1372页。'
- en: '[36] M. Kostinger, M. Hirzer, P. Wohlhart, and et al, “Large scale metric learning
    from equivalence constraints,” in *CVPR*, 2012, pp. 2288–2295.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] M. Kostinger, M. Hirzer, P. Wohlhart 等，“从等价约束中进行大规模度量学习，”发表于 *CVPR*，2012年，第2288–2295页。'
- en: '[37] W.-S. Zheng, S. Gong, and T. Xiang, “Person re-identification by probabilistic
    relative distance comparison,” in *CVPR*, 2011, pp. 649–656.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] W.-S. Zheng, S. Gong 和 T. Xiang，“通过概率相对距离比较进行人员重新识别，”发表于 *CVPR*，2011年，第649–656页。'
- en: '[38] F. Xiong, M. Gou, O. Camps, and M. Sznaier, “Person re-identification
    using kernel-based metric learning methods,” in *ECCV*, 2014, pp. 1–16.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] F. Xiong, M. Gou, O. Camps 和 M. Sznaier，“使用基于核的度量学习方法进行人员重新识别，”发表于 *ECCV*，2014年，第1–16页。'
- en: '[39] M. Hirzer, P. M. Roth, M. Köstinger, and H. Bischof, “Relaxed pairwise
    learned metric for person re-identification,” in *ECCV*, 2012, pp. 780–793.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. Hirzer, P. M. Roth, M. Köstinger 和 H. Bischof，“放松的成对学习度量用于人员重新识别，”发表于
    *ECCV*，2012年，第780–793页。'
- en: '[40] S. Liao and S. Z. Li, “Efficient psd constrained asymmetric metric learning
    for person re-identification,” in *ICCV*, 2015, pp. 3685–3693.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. Liao 和 S. Z. Li，“高效的 PSD 约束非对称度量学习用于人员重新识别，”发表于 *ICCV*，2015年，第3685–3693页。'
- en: '[41] H.-X. Yu, A. Wu, and W.-S. Zheng, “Unsupervised person re-identification
    by deep asymmetric metric embedding,” *IEEE TPAMI*, 2018.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H.-X. Yu, A. Wu 和 W.-S. Zheng，“通过深度非对称度量嵌入进行无监督人员重新识别，”*IEEE TPAMI*，2018年。'
- en: '[42] Z. Zheng, L. Zheng, and Y. Yang, “Unlabeled samples generated by gan improve
    the person re-identification baseline in vitro,” in *ICCV*, 2017, pp. 3754–3762.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Z. Zheng, L. Zheng 和 Y. Yang，“由 GAN 生成的未标记样本改善体内人员重新识别基线，”发表于 *ICCV*，2017年，第3754–3762页。'
- en: '[43] W. Li, R. Zhao, T. Xiao, and X. Wang, “Deepreid: Deep filter pairing neural
    network for person re-identification,” in *CVPR*, 2014, pp. 152–159.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] W. Li, R. Zhao, T. Xiao 和 X. Wang，“Deepreid：用于人员重新识别的深度滤波配对神经网络，”发表于 *CVPR*，2014年，第152–159页。'
- en: '[44] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer gan to bridge
    domain gap for person re-identification,” in *CVPR*, 2018, pp. 79–88.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] L. Wei, S. Zhang, W. Gao 和 Q. Tian，“人员迁移 GAN 桥接领域间隙以进行人员重新识别，”发表于 *CVPR*，2018年，第79–88页。'
- en: '[45] Q. Leng, M. Ye, and Q. Tian, “A survey of open-world person re-identification,”
    *IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)*, 2019.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Q. Leng, M. Ye 和 Q. Tian，“开放世界人员重新识别的调查，”*IEEE Transactions on Circuits
    and Systems for Video Technology (TCSVT)*，2019年。'
- en: '[46] D. Wu, S.-J. Zheng, X.-P. Zhang, C.-A. Yuan, F. Cheng, Y. Zhao, Y.-J.
    Lin, Z.-Q. Zhao, Y.-L. Jiang, and D.-S. Huang, “Deep learning-based methods for
    person re-identification: A comprehensive review,” *Neurocomputing*, 2019.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] D. Wu, S.-J. Zheng, X.-P. Zhang, C.-A. Yuan, F. Cheng, Y. Zhao, Y.-J.
    Lin, Z.-Q. Zhao, Y.-L. Jiang 和 D.-S. Huang，“基于深度学习的方法用于人员重新识别：综合评审，”*Neurocomputing*，2019年。'
- en: '[47] B. Lavi, M. F. Serj, and I. Ullah, “Survey on deep learning techniques
    for person re-identification task,” *arXiv preprint arXiv:1807.05284*, 2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] B. Lavi, M. F. Serj 和 I. Ullah，“针对人员重新识别任务的深度学习技术调查，”*arXiv preprint arXiv:1807.05284*，2018年。'
- en: '[48] X. Wang, “Intelligent multi-camera video surveillance: A review,” *Pattern
    recognition letters*, vol. 34, no. 1, pp. 3–19, 2013.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] X. Wang，“智能多摄像头视频监控：综述，”*Pattern recognition letters*，第34卷，第1期，第3–19页，2013年。'
- en: '[49] D. Geronimo, A. M. Lopez, A. D. Sappa, and T. Graf, “Survey of pedestrian
    detection for advanced driver assistance systems,” *IEEE TPAMI*, no. 7, pp. 1239–1258,
    2009.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] D. Geronimo, A. M. Lopez, A. D. Sappa, 和 T. Graf, “高级驾驶辅助系统的行人检测调查，” *IEEE
    TPAMI*，第7期，第1239–1258页，2009年。'
- en: '[50] P. Dollar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection:
    A benchmark,” in *CVPR*, 2009, pp. 304–311.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] P. Dollar, C. Wojek, B. Schiele, 和 P. Perona, “行人检测：基准测试，” 在 *CVPR*，2009年，第304–311页。'
- en: '[51] E. Insafutdinov, M. Andriluka, L. Pishchulin, S. Tang, E. Levinkov, B. Andres,
    and B. Schiele, “Arttrack: Articulated multi-person tracking in the wild,” in
    *CVPR*, 2017, pp. 6457–6465.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] E. Insafutdinov, M. Andriluka, L. Pishchulin, S. Tang, E. Levinkov, B.
    Andres, 和 B. Schiele, “Arttrack: 自然环境下的多人体跟踪，” 在 *CVPR*，2017年，第6457–6465页。'
- en: '[52] E. Ristani and C. Tomasi, “Features for multi-target multi-camera tracking
    and re-identification,” in *CVPR*, 2018, pp. 6036–6046.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] E. Ristani 和 C. Tomasi, “多目标多摄像头跟踪和再识别的特征，” 在 *CVPR*，2018年，第6036–6046页。'
- en: '[53] A. J. Ma, P. C. Yuen, and J. Li, “Domain transfer support vector ranking
    for person re-identification without target camera label information,” in *ICCV*,
    2013, pp. 3567–3574.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] A. J. Ma, P. C. Yuen, 和 J. Li, “无目标摄像头标签信息的人物再识别领域迁移支持向量排序，” 在 *ICCV*，2013年，第3567–3574页。'
- en: '[54] T. Xiao, H. Li, W. Ouyang, and X. Wang, “Learning deep feature representations
    with domain guided dropout for person re-identification,” in *CVPR*, 2016, pp.
    1249–1258.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] T. Xiao, H. Li, W. Ouyang, 和 X. Wang, “通过领域引导的dropout学习深度特征表示用于人物再识别，”
    在 *CVPR*，2016年，第1249–1258页。'
- en: '[55] L. Zheng, H. Zhang, S. Sun, M. Chandraker, Y. Yang, and Q. Tian, “Person
    re-identification in the wild,” in *CVPR*, 2017, pp. 1367–1376.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] L. Zheng, H. Zhang, S. Sun, M. Chandraker, Y. Yang, 和 Q. Tian, “野外人物再识别，”
    在 *CVPR*，2017年，第1367–1376页。'
- en: '[56] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Deep metric learning for person
    re-identification,” in *ICPR*, 2014, pp. 34–39.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] D. Yi, Z. Lei, S. Liao, 和 S. Z. Li, “用于人物再识别的深度度量学习，” 在 *ICPR*，2014年，第34–39页。'
- en: '[57] A. Hermans, L. Beyer, and B. Leibe, “In defense of the triplet loss for
    person re-identification,” *arXiv preprint arXiv:1703.07737*, 2017.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] A. Hermans, L. Beyer, 和 B. Leibe, “为人物再识别辩护triplet loss，” *arXiv preprint
    arXiv:1703.07737*，2017年。'
- en: '[58] Z. Zhong, L. Zheng, D. Cao, and S. Li, “Re-ranking person re-identification
    with k-reciprocal encoding,” in *CVPR*, 2017, pp. 1318–1327.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Z. Zhong, L. Zheng, D. Cao, 和 S. Li, “基于k-互逆编码的再排序人物再识别，” 在 *CVPR*，2017年，第1318–1327页。'
- en: '[59] M. Ye, C. Liang, Y. Yu, Z. Wang, Q. Leng, C. Xiao, J. Chen, and R. Hu,
    “Person reidentification via ranking aggregation of similarity pulling and dissimilarity
    pushing,” *IEEE Transactions on Multimedia (TMM)*, vol. 18, no. 12, pp. 2553–2566,
    2016.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. Ye, C. Liang, Y. Yu, Z. Wang, Q. Leng, C. Xiao, J. Chen, 和 R. Hu, “通过相似度拉拽和不相似度推挤的排名聚合进行人物再识别，”
    *IEEE Transactions on Multimedia (TMM)*，第18卷，第12期，第2553–2566页，2016年。'
- en: '[60] D. T. Nguyen, H. G. Hong, K. W. Kim, and K. R. Park, “Person recognition
    system based on a combination of body images from visible light and thermal cameras,”
    *Sensors*, vol. 17, no. 3, p. 605, 2017.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] D. T. Nguyen, H. G. Hong, K. W. Kim, 和 K. R. Park, “基于可见光和热成像摄像头的身体图像组合的人物识别系统，”
    *Sensors*，第17卷，第3期，第605页，2017年。'
- en: '[61] W.-H. Li, Z. Zhong, and W.-S. Zheng, “One-pass person re-identification
    by sketch online discriminant analysis,” *Pattern Recognition*, vol. 93, pp. 237–250,
    2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] W.-H. Li, Z. Zhong, 和 W.-S. Zheng, “通过草图在线判别分析的一次性人物再识别，” *Pattern Recognition*，第93卷，第237–250页，2019年。'
- en: '[62] A. Wu, W.-S. Zheng, and J.-H. Lai, “Robust depth-based person re-identification,”
    *IEEE Transactions on Image Processing (TIP)*, vol. 26, no. 6, pp. 2588–2603,
    2017.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. Wu, W.-S. Zheng, 和 J.-H. Lai, “基于深度的鲁棒人物再识别，” *IEEE Transactions on
    Image Processing (TIP)*, 第26卷，第6期，第2588–2603页, 2017年。'
- en: '[63] S. Li, T. Xiao, H. Li, B. Zhou, D. Yue, and X. Wang, “Person search with
    natural language description,” in *CVPR*, 2017, pp. 1345–1353.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] S. Li, T. Xiao, H. Li, B. Zhou, D. Yue, 和 X. Wang, “带有自然语言描述的人物搜索，” 在
    *CVPR*，2017年，第1345–1353页。'
- en: '[64] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang, “Joint detection and identification
    feature learning for person search,” in *CVPR*, 2017, pp. 3415–3424.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] T. Xiao, S. Li, B. Wang, L. Lin, 和 X. Wang, “用于人物搜索的联合检测和识别特征学习，” 在 *CVPR*，2017年，第3415–3424页。'
- en: '[65] X. Liu, M. Song, D. Tao, X. Zhou, C. Chen, and J. Bu, “Semi-supervised
    coupled dictionary learning for person re-identification,” in *CVPR*, 2014, pp.
    3550–3557.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] X. Liu, M. Song, D. Tao, X. Zhou, C. Chen, 和 J. Bu, “用于人物再识别的半监督耦合字典学习，”
    在 *CVPR*，2014年，第3550–3557页。'
- en: '[66] R. Zhao, W. Ouyang, and X. Wang, “Unsupervised salience learning for person
    re-identification,” in *CVPR*, 2013, pp. 3586–3593.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] R. Zhao, W. Ouyang, 和 X. Wang, “用于人物再识别的无监督显著性学习，” 在 *CVPR*，2013年，第3586–3593页。'
- en: '[67] Y. Sun, Q. Xu, Y. Li, C. Zhang, Y. Li, S. Wang, and J. Sun, “Perceive
    where to focus: Learning visibility-aware part-level features for partial person
    re-identification,” in *CVPR*, 2019, pp. 393–402.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Y. Sun, Q. Xu, Y. Li, C. Zhang, Y. Li, S. Wang, and J. Sun, “Perceive
    where to focus: Learning visibility-aware part-level features for partial person
    re-identification,”《*CVPR*》, 2019, pp. 393–402.'
- en: '[68] X. Wang, G. Doretto, T. Sebastian, J. Rittscher, and P. Tu, “Shape and
    appearance context modeling,” in *ICCV*, 2007, pp. 1–8.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] X. Wang, G. Doretto, T. Sebastian, J. Rittscher, and P. Tu, “Shape and
    appearance context modeling,”《*ICCV*》, 2007, pp. 1–8.'
- en: '[69] H. Wang, X. Zhu, T. Xiang, and S. Gong, “Towards unsupervised open-set
    person re-identification,” in *ICIP*, 2016, pp. 769–773.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] H. Wang, X. Zhu, T. Xiang, and S. Gong, “Towards unsupervised open-set
    person re-identification,”《*ICIP*》, 2016, pp. 769–773.'
- en: '[70] X. Zhu, B. Wu, D. Huang, and W.-S. Zheng, “Fast open-world person re-identification,”
    *IEEE Transactions on Image Processing (TIP)*, vol. 27, no. 5, pp. 2286 – 2300,
    2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] X. Zhu, B. Wu, D. Huang, and W.-S. Zheng, “Fast open-world person re-identification,”《*IEEE
    Transactions on Image Processing (TIP)*》, vol. 27, no. 5, pp. 2286 – 2300, 2018.'
- en: '[71] C. Su, S. Zhang, J. Xing, W. Gao, and Q. Tian, “Deep attributes driven
    multi-camera person re-identification,” in *ECCV*, 2016, pp. 475–491.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] C. Su, S. Zhang, J. Xing, W. Gao, and Q. Tian, “Deep attributes driven
    multi-camera person re-identification,”《*ECCV*》, 2016, pp. 475–491.'
- en: '[72] Y. Lin, L. Zheng, Z. Zheng, Y. Wu, and Y. Yang, “Improving person re-identification
    by attribute and identity learning,” *arXiv preprint arXiv:1703.07220*, 2017.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Lin, L. Zheng, Z. Zheng, Y. Wu, and Y. Yang, “Improving person re-identification
    by attribute and identity learning,”《*arXiv preprint arXiv:1703.07220*》, 2017.'
- en: '[73] K. Liu, B. Ma, W. Zhang, and R. Huang, “A spatio-temporal appearance representation
    for viceo-based pedestrian re-identification,” in *ICCV*, 2015, pp. 3810–3818.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] K. Liu, B. Ma, W. Zhang, and R. Huang, “A spatio-temporal appearance representation
    for viceo-based pedestrian re-identification,”《*ICCV*》, 2015, pp. 3810–3818.'
- en: '[74] J. Dai, P. Zhang, D. Wang, H. Lu, and H. Wang, “Video person re-identification
    by temporal residual learning,” *IEEE Transactions on Image Processing (TIP)*,
    vol. 28, no. 3, pp. 1366–1377, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Dai, P. Zhang, D. Wang, H. Lu, and H. Wang, “Video person re-identification
    by temporal residual learning,”《*IEEE Transactions on Image Processing (TIP)*》,
    vol. 28, no. 3, pp. 1366–1377, 2018.'
- en: '[75] L. Zhao, X. Li, Y. Zhuang, and J. Wang, “Deeply-learned part-aligned representations
    for person re-identification,” in *CVPR*, 2017, pp. 3219–3228.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] L. Zhao, X. Li, Y. Zhuang, and J. Wang, “Deeply-learned part-aligned representations
    for person re-identification,”《*CVPR*》, 2017, pp. 3219–3228.'
- en: '[76] H. Yao, S. Zhang, R. Hong, Y. Zhang, C. Xu, and Q. Tian, “Deep representation
    learning with part loss for person re-identification,” *IEEE Transactions on Image
    Processing (TIP)*, 2019.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] H. Yao, S. Zhang, R. Hong, Y. Zhang, C. Xu, and Q. Tian, “Deep representation
    learning with part loss for person re-identification,”《*IEEE Transactions on Image
    Processing (TIP)*》, 2019.'
- en: '[77] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, “Beyond part models:
    Person retrieval with refined part pooling,” in *ECCV*, 2018, pp. 480–496.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, “Beyond part models:
    Person retrieval with refined part pooling,”《*ECCV*》, 2018, pp. 480–496.'
- en: '[78] T. Matsukawa and E. Suzuki, “Person re-identification using cnn features
    learned from combination of attributes,” in *ICPR*, 2016, pp. 2428–2433.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] T. Matsukawa and E. Suzuki, “Person re-identification using cnn features
    learned from combination of attributes,”《*ICPR*》, 2016, pp. 2428–2433.'
- en: '[79] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,”《*arXiv preprint arXiv:1409.1556*》, 2014.'
- en: '[80] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,”《*CVPR*》, 2016, pp. 770–778.'
- en: '[81] F. Wang, W. Zuo, L. Lin, D. Zhang, and L. Zhang, “Joint learning of single-image
    and cross-image representations for person re-identification,” in *CVPR*, 2016,
    pp. 1288–1296.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] F. Wang, W. Zuo, L. Lin, D. Zhang, and L. Zhang, “Joint learning of single-image
    and cross-image representations for person re-identification,”《*CVPR*》, 2016,
    pp. 1288–1296.'
- en: '[82] Y. Sun, L. Zheng, W. Deng, and S. Wang, “Svdnet for pedestrian retrieval,”
    in *ICCV*, 2017, pp. 3800–3808.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Sun, L. Zheng, W. Deng, and S. Wang, “Svdnet for pedestrian retrieval,”《*ICCV*》,
    2017, pp. 3800–3808.'
- en: '[83] M. Ye, X. Lan, and P. C. Yuen, “Robust anchor embedding for unsupervised
    video person re-identification in the wild,” in *ECCV*, 2018, pp. 170–186.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] M. Ye, X. Lan, and P. C. Yuen, “Robust anchor embedding for unsupervised
    video person re-identification in the wild,”《*ECCV*》, 2018, pp. 170–186.'
- en: '[84] X. Qian, Y. Fu, Y.-G. Jiang, T. Xiang, and X. Xue, “Multi-scale deep learning
    architectures for person re-identification,” in *ICCV*, 2017, pp. 5399–5408.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] X. Qian, Y. Fu, Y.-G. Jiang, T. Xiang, and X. Xue, “Multi-scale deep learning
    architectures for person re-identification,”《*ICCV*》, 2017, pp. 5399–5408.'
- en: '[85] F. Yang, K. Yan, S. Lu, H. Jia, X. Xie, and W. Gao, “Attention driven
    person re-identification,” *Pattern Recognition*, vol. 86, pp. 143–155, 2019.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] F. Yang, K. Yan, S. Lu, H. Jia, X. Xie, and W. Gao, “Attention driven
    person re-identification,”《*Pattern Recognition*》, vol. 86, pp. 143–155, 2019.'
- en: '[86] W. Li, X. Zhu, and S. Gong, “Harmonious attention network for person re-identification,”
    in *CVPR*, 2018, pp. 2285–2294.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] W. Li, X. Zhu, 和 S. Gong, “和谐注意力网络用于行人重识别，”在 *CVPR*，2018年，页码 2285–2294。'
- en: '[87] C. Wang, Q. Zhang, C. Huang, W. Liu, and X. Wang, “Mancs: A multi-task
    attentional network with curriculum sampling for person re-identification,” in
    *ECCV*, 2018, pp. 365–381.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] C. Wang, Q. Zhang, C. Huang, W. Liu, 和 X. Wang, “Mancs：一个多任务注意力网络与课程采样用于行人重识别，”在
    *ECCV*，2018年，页码 365–381。'
- en: '[88] Y. Shen, T. Xiao, H. Li, S. Yi, and X. Wang, “End-to-end deep kronecker-product
    matching for person re-identification,” in *CVPR*, 2018, pp. 6886–6895.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Shen, T. Xiao, H. Li, S. Yi, 和 X. Wang, “端到端深度Kronecker积匹配用于行人重识别，”在
    *CVPR*，2018年，页码 6886–6895。'
- en: '[89] Y. Wang, Z. Chen, F. Wu, and G. Wang, “Person re-identification with cascaded
    pairwise convolutions,” in *CVPR*, 2018, pp. 1470–1478.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. Wang, Z. Chen, F. Wu, 和 G. Wang, “通过级联对对卷积进行行人重识别，”在 *CVPR*，2018年，页码
    1470–1478。'
- en: '[90] G. Chen, C. Lin, L. Ren, J. Lu, and J. Zhou, “Self-critical attention
    learning for person re-identification,” in *ICCV*, 2019, pp. 9637–9646.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] G. Chen, C. Lin, L. Ren, J. Lu, 和 J. Zhou, “自我批判性注意力学习用于行人重识别，”在 *ICCV*，2019年，页码
    9637–9646。'
- en: '[91] J. Si, H. Zhang, C.-G. Li, J. Kuen, X. Kong, A. C. Kot, and G. Wang, “Dual
    attention matching network for context-aware feature sequence based person re-identification,”
    in *CVPR*, 2018, pp. 5363–5372.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] J. Si, H. Zhang, C.-G. Li, J. Kuen, X. Kong, A. C. Kot, 和 G. Wang, “用于上下文感知特征序列的双重注意力匹配网络进行行人重识别，”在
    *CVPR*，2018年，页码 5363–5372。'
- en: '[92] M. Zheng, S. Karanam, Z. Wu, and R. J. Radke, “Re-identification with
    consistent attentive siamese networks,” in *CVPR*, 2019, pp. 5735–5744.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] M. Zheng, S. Karanam, Z. Wu, 和 R. J. Radke, “具有一致性注意力的Siamese网络进行重识别，”在
    *CVPR*，2019年，页码 5735–5744。'
- en: '[93] S. Zhou, F. Wang, Z. Huang, and J. Wang, “Discriminative feature learning
    with consistent attention regularization for person re-identification,” in *ICCV*,
    2019, pp. 8040–8049.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. Zhou, F. Wang, Z. Huang, 和 J. Wang, “具有一致性注意力正则化的判别特征学习用于行人重识别，”在 *ICCV*，2019年，页码
    8040–8049。'
- en: '[94] D. Chen, D. Xu, H. Li, N. Sebe, and X. Wang, “Group consistent similarity
    learning via deep crf for person re-identification,” in *CVPR*, 2018, pp. 8649–8658.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] D. Chen, D. Xu, H. Li, N. Sebe, 和 X. Wang, “通过深度CRF进行群体一致性相似性学习用于行人重识别，”在
    *CVPR*，2018年，页码 8649–8658。'
- en: '[95] C. Luo, Y. Chen, N. Wang, and Z. Zhang, “Spectral feature transformation
    for person re-identification,” in *ICCV*, 2019, pp. 4976–4985.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] C. Luo, Y. Chen, N. Wang, 和 Z. Zhang, “用于行人重识别的谱特征变换，”在 *ICCV*，2019年，页码
    4976–4985。'
- en: '[96] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang, “A siamese long short-term
    memory architecture for human re-identification,” in *ECCV*, 2016, pp. 135–153.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] R. R. Varior, B. Shuai, J. Lu, D. Xu, 和 G. Wang, “用于人类重识别的Siamese长短期记忆架构，”在
    *ECCV*，2016年，页码 135–153。'
- en: '[97] Y. Suh, J. Wang, S. Tang, T. Mei, and K. Mu Lee, “Part-aligned bilinear
    representations for person re-identification,” in *ECCV*, 2018, pp. 402–419.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Y. Suh, J. Wang, S. Tang, T. Mei, 和 K. Mu Lee, “部件对齐双线性表示用于行人重识别，”在 *ECCV*，2018年，页码
    402–419。'
- en: '[98] L. Zhao, X. Li, Y. Zhuang, and J. Wang, “Deeply-learned part-aligned representations
    for person re-identification,” in *ICCV*, 2017, pp. 3219–3228.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] L. Zhao, X. Li, Y. Zhuang, 和 J. Wang, “深度学习的部件对齐表示用于行人重识别，”在 *ICCV*，2017年，页码
    3219–3228。'
- en: '[99] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng, “Person re-identification
    by multi-channel parts-based cnn with improved triplet loss function,” in *CVPR*,
    2016, pp. 1335–1344.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] D. Cheng, Y. Gong, S. Zhou, J. Wang, 和 N. Zheng, “通过改进的三元组损失函数的多通道部件基础CNN进行行人重识别，”在
    *CVPR*，2016年，页码 1335–1344。'
- en: '[100] D. Li, X. Chen, Z. Zhang, and K. Huang, “Learning deep context-aware
    features over body and latent parts for person re-identification,” in *CVPR*,
    2017, pp. 384–393.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] D. Li, X. Chen, Z. Zhang, 和 K. Huang, “学习深度上下文感知特征以识别身体和潜在部件用于行人重识别，”在
    *CVPR*，2017年，页码 384–393。'
- en: '[101] C. Su, J. Li, S. Zhang, J. Xing, W. Gao, and Q. Tian, “Pose-driven deep
    convolutional model for person re-identification,” in *ICCV*, 2017, pp. 3960–3969.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] C. Su, J. Li, S. Zhang, J. Xing, W. Gao, 和 Q. Tian, “基于姿态驱动的深度卷积模型用于行人重识别，”在
    *ICCV*，2017年，页码 3960–3969。'
- en: '[102] J. Xu, R. Zhao, F. Zhu, H. Wang, and W. Ouyang, “Attention-aware compositional
    network for person re-identification,” in *CVPR*, 2018, pp. 2119–2128.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Xu, R. Zhao, F. Zhu, H. Wang, 和 W. Ouyang, “注意力感知的组合网络用于行人重识别，”在 *CVPR*，2018年，页码
    2119–2128。'
- en: '[103] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, “Densely semantically aligned
    person re-identification,” in *CVPR*, 2019, pp. 667–676.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Z. Zhang, C. Lan, W. Zeng, 和 Z. Chen, “密集语义对齐的行人重识别，”在 *CVPR*，2019年，页码
    667–676。'
- en: '[104] J. Guo, Y. Yuan, L. Huang, C. Zhang, J.-G. Yao, and K. Han, “Beyond human
    parts: Dual part-aligned representations for person re-identification,” in *ICCV*,
    2019, pp. 3642–3651.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. Guo, Y. Yuan, L. Huang, C. Zhang, J.-G. Yao, 和 K. Han, “超越人体部位：双部位对齐表示用于人员再识别，”发表于
    *ICCV*，2019年，页码 3642–3651。'
- en: '[105] X. Sun and L. Zheng, “Dissecting person re-identification from the viewpoint
    of viewpoint,” in *CVPR*, 2019, pp. 608–617.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] X. Sun 和 L. Zheng, “从视角的角度剖析人员再识别，”发表于 *CVPR*，2019年，页码 608–617。'
- en: '[106] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y. Yang, “Invariance matters:
    Exemplar memory for domain adaptive person re-identification,” in *CVPR*, 2019,
    pp. 598–607.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Z. Zhong, L. Zheng, Z. Luo, S. Li, 和 Y. Yang, “不变性至关重要：领域自适应人员再识别的示例记忆，”发表于
    *CVPR*，2019年，页码 598–607。'
- en: '[107] B. N. Xia, Y. Gong, Y. Zhang, and C. Poellabauer, “Second-order non-local
    attention networks for person re-identification,” in *ICCV*, 2019, pp. 3760–3769.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] B. N. Xia, Y. Gong, Y. Zhang, 和 C. Poellabauer, “二阶非局部注意力网络用于人员再识别，”发表于
    *ICCV*，2019年，页码 3760–3769。'
- en: '[108] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, and X. Chen, “Interaction-and-aggregation
    network for person re-identification,” in *CVPR*, 2019, pp. 9317–9326.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, 和 X. Chen, “互动与聚合网络用于人员再识别，”发表于
    *CVPR*，2019年，页码 9317–9326。'
- en: '[109] C.-P. Tay, S. Roy, and K.-H. Yap, “Aanet: Attribute attention network
    for person re-identifications,” in *CVPR*, 2019, pp. 7134–7143.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C.-P. Tay, S. Roy, 和 K.-H. Yap, “Aanet: 属性注意力网络用于人员再识别，”发表于 *CVPR*，2019年，页码
    7134–7143。'
- en: '[110] Y. Zhao, X. Shen, Z. Jin, H. Lu, and X.-s. Hua, “Attribute-driven feature
    disentangling and temporal aggregation for video person re-identification,” in
    *CVPR*, 2019, pp. 4913–4922.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Y. Zhao, X. Shen, Z. Jin, H. Lu, 和 X.-s. Hua, “基于属性的特征解缠结和时间聚合用于视频人员再识别，”发表于
    *CVPR*，2019年，页码 4913–4922。'
- en: '[111] W. Jingya, Z. Xiatian, G. Shaogang, and L. Wei, “Transferable joint attribute-identity
    deep learning for unsupervised person re-identification,” in *CVPR*, 2018, pp.
    2275–2284.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] W. Jingya, Z. Xiatian, G. Shaogang, 和 L. Wei, “可迁移的联合属性-身份深度学习用于无监督人员再识别，”发表于
    *CVPR*，2018年，页码 2275–2284。'
- en: '[112] X. Chang, T. M. Hospedales, and T. Xiang, “Multi-level factorisation
    net for person re-identification,” in *CVPR*, 2018, pp. 2109–2118.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] X. Chang, T. M. Hospedales, 和 T. Xiang, “多层次因子分解网络用于人员再识别，”发表于 *CVPR*，2018年，页码
    2109–2118。'
- en: '[113] F. Liu and L. Zhang, “View confusion feature learning for person re-identification,”
    in *ICCV*, 2019, pp. 6639–6648.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] F. Liu 和 L. Zhang, “视角混淆特征学习用于人员再识别，”发表于 *ICCV*，2019年，页码 6639–6648。'
- en: '[114] Z. Zhu, X. Jiang, F. Zheng, X. Guo, F. Huang, W. Zheng, and X. Sun, “Aware
    loss with angular regularization for person re-identification,” in *AAAI*, 2020.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Z. Zhu, X. Jiang, F. Zheng, X. Guo, F. Huang, W. Zheng, 和 X. Sun, “具有角度正则化的感知损失用于人员再识别，”发表于
    *AAAI*，2020年。'
- en: '[115] J. Lin, L. Ren, J. Lu, J. Feng, and J. Zhou, “Consistent-aware deep learning
    for person re-identification in a camera network,” in *CVPR*, 2017, pp. 5771–5780.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. Lin, L. Ren, J. Lu, J. Feng, 和 J. Zhou, “一致性感知深度学习用于摄像机网络中的人员再识别，”发表于
    *CVPR*，2017年，页码 5771–5780。'
- en: '[116] J. Liu, B. Ni, Y. Yan, and et al., “Pose transferrable person re-identification,”
    in *CVPR*, 2018, pp. 4099–4108.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] J. Liu, B. Ni, Y. Yan, 等, “姿态可迁移人员再识别，”发表于 *CVPR*，2018年，页码 4099–4108。'
- en: '[117] X. Qian, Y. Fu, T. Xiang, W. Wang, J. Qiu, Y. Wu, Y.-G. Jiang, and X. Xue,
    “Pose-normalized image generation for person re-identification,” in *ECCV*, 2018,
    pp. 650–667.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] X. Qian, Y. Fu, T. Xiang, W. Wang, J. Qiu, Y. Wu, Y.-G. Jiang, 和 X. Xue,
    “姿态归一化图像生成用于人员再识别，”发表于 *ECCV*，2018年，页码 650–667。'
- en: '[118] Z. Zhong, L. Zheng, Z. Zheng, and et al., “Camera style adaptation for
    person re-identification,” in *CVPR*, 2018, pp. 5157–5166.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Z. Zhong, L. Zheng, Z. Zheng, 等, “相机风格适应用于人员再识别，”发表于 *CVPR*，2018年，页码
    5157–5166。'
- en: '[119] Z. Zheng, X. Yang, Z. Yu, L. Zheng, Y. Yang, and J. Kautz, “Joint discriminative
    and generative learning for person re-identification,” in *CVPR*, 2019, pp. 2138–2147.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Z. Zheng, X. Yang, Z. Yu, L. Zheng, Y. Yang, 和 J. Kautz, “联合判别和生成学习用于人员再识别，”发表于
    *CVPR*，2019年，页码 2138–2147。'
- en: '[120] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao, “Image-image
    domain adaptation with preserved self-similarity and domain-dissimilarity for
    person re-identification,” in *CVPR*, 2018, pp. 994–1003.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, 和 J. Jiao, “具有保留自相似性和领域差异性的图像-图像领域适应用于人员再识别，”发表于
    *CVPR*，2018年，页码 994–1003。'
- en: '[121] Y.-J. Li, C.-S. Lin, Y.-B. Lin, and Y.-C. F. Wang, “Cross-dataset person
    re-identification via unsupervised pose disentanglement and adaptation,” in *ICCV*,
    2019, pp. 7919–7929.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Y.-J. Li, C.-S. Lin, Y.-B. Lin, 和 Y.-C. F. Wang, “通过无监督姿态解缠结和适应进行跨数据集人员再识别，”发表于
    *ICCV*，2019年，页码 7919–7929。'
- en: '[122] H. Luo, W. Jiang, Y. Gu, F. Liu, X. Liao, S. Lai, and J. Gu, “A strong
    baseline and batch normneuralization neck for deep person re-identification,”
    *arXiv preprint arXiv:1906.08332*, 2019.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] H. Luo, W. Jiang, Y. Gu, F. Liu, X. Liao, S. Lai, 和 J. Gu，“深度行人再识别的强基线和批量归一化颈部，”
    *arXiv 预印本 arXiv:1906.08332*，2019年。'
- en: '[123] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, “Random erasing data
    augmentation,” *arXiv preprint arXiv:1708.04896*, 2017.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Z. Zhong, L. Zheng, G. Kang, S. Li, 和 Y. Yang，“随机擦除数据增强，” *arXiv 预印本
    arXiv:1708.04896*，2017年。'
- en: '[124] Z. Dai, M. Chen, X. Gu, S. Zhu, and P. Tan, “Batch dropblock network
    for person re-identification and beyond,” in *ICCV*, 2019, pp. 3691–3701.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Z. Dai, M. Chen, X. Gu, S. Zhu, 和 P. Tan，“用于行人再识别及其他应用的批量丢块网络，” 见于 *ICCV*，2019年，第3691–3701页。'
- en: '[125] S. Bak, P. Carr, and J.-F. Lalonde, “Domain adaptation through synthesis
    for unsupervised person re-identification,” in *ECCV*, 2018, pp. 189–205.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] S. Bak, P. Carr, 和 J.-F. Lalonde，“通过合成进行无监督行人再识别的领域适应，” 见于 *ECCV*，2018年，第189–205页。'
- en: '[126] M. Hirzer, C. Beleznai, P. M. Roth, and H. Bischof, “Person re-identification
    by descriptive and discriminative classification,” in *Image Analysis*, 2011,
    pp. 91–102.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] M. Hirzer, C. Beleznai, P. M. Roth, 和 H. Bischof，“通过描述性和区分性分类进行行人再识别，”
    见于 *图像分析*，2011年，第91–102页。'
- en: '[127] N. McLaughlin, J. Martinez del Rincon, and P. Miller, “Recurrent convolutional
    network for video-based person re-identification,” in *CVPR*, 2016, pp. 1325–1334.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] N. McLaughlin, J. Martinez del Rincon, 和 P. Miller，“用于视频行人再识别的递归卷积网络，”
    见于 *CVPR*，2016年，第1325–1334页。'
- en: '[128] D. Chung, K. Tahboub, and E. J. Delp, “A two stream siamese convolutional
    neural network for person re-identification,” in *ICCV*, 2017, pp. 1983–1991.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] D. Chung, K. Tahboub, 和 E. J. Delp，“一个用于行人再识别的双流孪生卷积神经网络，” 见于 *ICCV*，2017年，第1983–1991页。'
- en: '[129] Y. Yan, B. Ni, Z. Song, C. Ma, Y. Yan, and X. Yang, “Person re-identification
    via recurrent feature aggregation,” in *ECCV*, 2016, pp. 701–716.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Y. Yan, B. Ni, Z. Song, C. Ma, Y. Yan, 和 X. Yang，“通过递归特征聚合进行行人再识别，” 见于
    *ECCV*，2016年，第701–716页。'
- en: '[130] Z. Zhou, Y. Huang, W. Wang, L. Wang, and T. Tan, “See the forest for
    the trees: Joint spatial and temporal recurrent neural networks for video-based
    person re-identification,” in *CVPR*, 2017, pp. 4747–4756.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Z. Zhou, Y. Huang, W. Wang, L. Wang, 和 T. Tan，“看森林看树木：用于基于视频的行人再识别的联合时空递归神经网络，”
    见于 *CVPR*，2017年，第4747–4756页。'
- en: '[131] S. Xu, Y. Cheng, K. Gu, Y. Yang, S. Chang, and P. Zhou, “Jointly attentive
    spatial-temporal pooling networks for video-based person re-identification,” in
    *ICCV*, 2017, pp. 4733–4742.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] S. Xu, Y. Cheng, K. Gu, Y. Yang, S. Chang, 和 P. Zhou，“联合关注的时空池化网络用于基于视频的行人再识别，”
    见于 *ICCV*，2017年，第4733–4742页。'
- en: '[132] A. Subramaniam, A. Nambiar, and A. Mittal, “Co-segmentation inspired
    attention networks for video-based person re-identification,” in *ICCV*, 2019,
    pp. 562–572.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] A. Subramaniam, A. Nambiar, 和 A. Mittal，“受共同分割启发的注意力网络用于基于视频的行人再识别，”
    见于 *ICCV*，2019年，第562–572页。'
- en: '[133] S. Li, S. Bak, P. Carr, and X. Wang, “Diversity regularized spatiotemporal
    attention for video-based person re-identification,” in *CVPR*, 2018, pp. 369–378.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] S. Li, S. Bak, P. Carr, 和 X. Wang，“多样性正则化时空注意力用于基于视频的行人再识别，” 见于 *CVPR*，2018年，第369–378页。'
- en: '[134] D. Chen, H. Li, T. Xiao, S. Yi, and X. Wang, “Video person re-identification
    with competitive snippet-similarity aggregation and co-attentive snippet embedding,”
    in *CVPR*, 2018, pp. 1169–1178.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] D. Chen, H. Li, T. Xiao, S. Yi, 和 X. Wang，“视频行人再识别通过竞争片段相似性聚合和共同关注片段嵌入，”
    见于 *CVPR*，2018年，第1169–1178页。'
- en: '[135] Y. Fu, X. Wang, Y. Wei, and T. Huang, “Sta: Spatial-temporal attention
    for large-scale video-based person re-identification,” in *AAAI*, 2019.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Y. Fu, X. Wang, Y. Wei, 和 T. Huang，“STA: 大规模视频行人再识别的时空注意力，” 见于 *AAAI*，2019年。'
- en: '[136] J. Li, J. Wang, Q. Tian, W. Gao, and S. Zhang, “Global-local temporal
    representations for video person re-identification,” in *ICCV*, 2019, pp. 3958–3967.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] J. Li, J. Wang, Q. Tian, W. Gao, 和 S. Zhang，“视频行人再识别的全局-局部时序表示，” 见于 *ICCV*，2019年，第3958–3967页。'
- en: '[137] Y. Guo and N.-M. Cheung, “Efficient and deep person re-identification
    using multi-level similarity,” in *CVPR*, 2018, pp. 2335–2344.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Y. Guo 和 N.-M. Cheung，“使用多层相似性进行高效且深度的行人再识别，” 见于 *CVPR*，2018年，第2335–2344页。'
- en: '[138] K. Zhou, Y. Yang, A. Cavallaro, and T. Xiang, “Omni-scale feature learning
    for person re-identification,” in *ICCV*, 2019, pp. 3702–3712.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] K. Zhou, Y. Yang, A. Cavallaro, 和 T. Xiang，“用于行人再识别的全尺度特征学习，” 见于 *ICCV*，2019年，第3702–3712页。'
- en: '[139] R. Quan, X. Dong, Y. Wu, L. Zhu, and Y. Yang, “Auto-reid: Searching for
    a part-aware convnet for person re-identification,” in *ICCV*, 2019, pp. 3750–3759.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] R. Quan, X. Dong, Y. Wu, L. Zhu, 和 Y. Yang，“Auto-reid: 寻找一个能够关注部件的卷积网络用于行人再识别，”
    见于 *ICCV*，2019年，第3750–3759页。'
- en: '[140] M. Tian, S. Yi, H. Li, S. Li, X. Zhang, J. Shi, J. Yan, and X. Wang,
    “Eliminating background-bias for robust person re-identification,” in *CVPR*,
    2018, pp. 5794–5803.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] M. Tian, S. Yi, H. Li, S. Li, X. Zhang, J. Shi, J. Yan, 和 X. Wang, “消除背景偏差以增强行人再识别的鲁棒性,”
    发表在 *CVPR*, 2018, 页码5794–5803。'
- en: '[141] Z. Zheng, L. Zheng, and Y. Yang, “A discriminatively learned cnn embedding
    for person re-identification,” *arXiv preprint arXiv:1611.05666*, 2016.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Z. Zheng, L. Zheng, 和 Y. Yang, “用于行人再识别的判别性学习cnn嵌入,” *arXiv预印本 arXiv:1611.05666*,
    2016。'
- en: '[142] M. Ye, X. Lan, Z. Wang, and P. C. Yuen, “Bi-directional center-constrained
    top-ranking for visible thermal person re-identification,” *IEEE Transactions
    on Information Forensics and Security (TIFS)*, vol. 15, pp. 407–419, 2020.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] M. Ye, X. Lan, Z. Wang, 和 P. C. Yuen, “双向中心约束顶级排名用于可见热成像行人再识别,” *IEEE信息取证与安全交易
    (TIFS)*, 第15卷, 页码407–419, 2020。'
- en: '[143] P. Moutafis, M. Leng, and I. A. Kakadiaris, “An overview and empirical
    comparison of distance metric learning methods,” *IEEE Transactions on Cybernetics*,
    vol. 47, no. 3, pp. 612–625, 2016.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] P. Moutafis, M. Leng, 和 I. A. Kakadiaris, “距离度量学习方法的概述与实证比较,” *IEEE网络控制学报*,
    第47卷, 第3期, 页码612–625, 2016。'
- en: '[144] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Ouyang, and Y. Yang, “Exploit the
    unknown gradually: One-shot video-based person re-identification by stepwise learning,”
    in *CVPR*, 2018, pp. 5177–5186.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Ouyang, 和 Y. Yang, “逐步开发未知: 一次性视频基础的行人再识别通过逐步学习,”
    发表在 *CVPR*, 2018, 页码5177–5186。'
- en: '[145] M. Ye, X. Zhang, P. C. Yuen, and S.-F. Chang, “Unsupervised embedding
    learning via invariant and spreading instance feature,” in *CVPR*, 2019, pp. 6210–6219.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] M. Ye, X. Zhang, P. C. Yuen, 和 S.-F. Chang, “通过不变和扩展实例特征的无监督嵌入学习,” 发表在
    *CVPR*, 2019, 页码6210–6219。'
- en: '[146] N. Wojke and A. Bewley, “Deep cosine metric learning for person re-identification,”
    in *WACV*, 2018, pp. 748–756.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] N. Wojke 和 A. Bewley, “用于行人再识别的深度余弦度量学习,” 发表在 *WACV*, 2018, 页码748–756。'
- en: '[147] X. Fan, W. Jiang, H. Luo, and M. Fei, “Spherereid: Deep hypersphere manifold
    embedding for person re-identification,” *JVCIR*, vol. 60, pp. 51–58, 2019.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] X. Fan, W. Jiang, H. Luo, 和 M. Fei, “Spherereid: 用于行人再识别的深度超球面流形嵌入,”
    *JVCIR*, 第60卷, 页码51–58, 2019。'
- en: '[148] R. Müller, S. Kornblith, and G. Hinton, “When does label smoothing help?”
    *arXiv preprint arXiv:1906.02629*, 2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] R. Müller, S. Kornblith, 和 G. Hinton, “标签平滑何时有效?” *arXiv预印本 arXiv:1906.02629*,
    2019。'
- en: '[149] H. Shi, Y. Yang, X. Zhu, S. Liao, Z. Lei, W. Zheng, and S. Z. Li, “Embedding
    deep metric for person re-identification: A study against large variations,” in
    *ECCV*, 2016, pp. 732–748.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] H. Shi, Y. Yang, X. Zhu, S. Liao, Z. Lei, W. Zheng, 和 S. Z. Li, “用于行人再识别的嵌入深度度量:
    对抗大变异的研究,” 发表在 *ECCV*, 2016, 页码732–748。'
- en: '[150] S. Zhou, J. Wang, J. Wang, Y. Gong, and N. Zheng, “Point to set similarity
    based deep feature learning for person re-identification,” in *CVPR*, 2017, pp.
    3741–3750.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. Zhou, J. Wang, J. Wang, Y. Gong, 和 N. Zheng, “基于点到集合相似度的深度特征学习用于行人再识别,”
    发表在 *CVPR*, 2017, 页码3741–3750。'
- en: '[151] R. Yu, Z. Dou, S. Bai, Z. Zhang, Y. Xu, and X. Bai, “Hard-aware point-to-set
    deep metric for person re-identification,” in *ECCV*, 2018, pp. 188–204.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] R. Yu, Z. Dou, S. Bai, Z. Zhang, Y. Xu, 和 X. Bai, “针对行人再识别的硬感知点到集合深度度量,”
    发表在 *ECCV*, 2018, 页码188–204。'
- en: '[152] W. Chen, X. Chen, J. Zhang, and K. Huang, “Beyond triplet loss: a deep
    quadruplet network for person re-identification,” in *CVPR*, 2017, pp. 403–412.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] W. Chen, X. Chen, J. Zhang, 和 K. Huang, “超越三元组损失: 一个深度四元组网络用于行人再识别,”
    发表在 *CVPR*, 2017, 页码403–412。'
- en: '[153] Q. Yang, H.-X. Yu, A. Wu, and W.-S. Zheng, “Patch-based discriminative
    feature learning for unsupervised person re-identification,” in *CVPR*, 2019,
    pp. 3633–3642.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Q. Yang, H.-X. Yu, A. Wu, 和 W.-S. Zheng, “基于补丁的判别性特征学习用于无监督行人再识别,” 发表在
    *CVPR*, 2019, 页码3633–3642。'
- en: '[154] Z. Liu, J. Wang, S. Gong, H. Lu, and D. Tao, “Deep reinforcement active
    learning for human-in-the-loop person re-identification,” in *ICCV*, 2019, pp.
    6122–6131.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Z. Liu, J. Wang, S. Gong, H. Lu, 和 D. Tao, “用于人机协作的深度强化主动学习行人再识别,” 发表在
    *ICCV*, 2019, 页码6122–6131。'
- en: '[155] J. Zhou, B. Su, and Y. Wu, “Easy identification from better constraints:
    Multi-shot person re-identification from reference constraints,” in *CVPR*, 2018,
    pp. 5373–5381.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] J. Zhou, B. Su, 和 Y. Wu, “从更好的约束中进行简单识别: 从参考约束的多镜头行人再识别,” 发表在 *CVPR*,
    2018, 页码5373–5381。'
- en: '[156] F. Zheng, C. Deng, X. Sun, X. Jiang, X. Guo, Z. Yu, F. Huang, and R. Ji,
    “Pyramidal person re-identification via multi-loss dynamic training,” in *CVPR*,
    2019, pp. 8514–8522.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] F. Zheng, C. Deng, X. Sun, X. Jiang, X. Guo, Z. Yu, F. Huang, 和 R. Ji,
    “通过多损失动态训练进行金字塔行人再识别,” 发表在 *CVPR*, 2019, 页码8514–8522。'
- en: '[157] M. Ye, C. Liang, Z. Wang, Q. Leng, and J. Chen, “Ranking optimization
    for person re-identification via similarity and dissimilarity,” in *ACM Multimedia
    (ACM MM)*, 2015, pp. 1239–1242.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] M. Ye, C. Liang, Z. Wang, Q. Leng 和 J. Chen，“通过相似性和不相似性优化人员再识别排名”，发表于
    *ACM Multimedia (ACM MM)*，2015年，页码1239–1242。'
- en: '[158] C. Liu, C. Change Loy, S. Gong, and G. Wang, “Pop: Person re-identification
    post-rank optimisation,” in *ICCV*, 2013, pp. 441–448.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] C. Liu, C. Change Loy, S. Gong 和 G. Wang，“Pop：人员再识别后排名优化”，发表于 *ICCV*，2013年，页码441–448。'
- en: '[159] H. Wang, S. Gong, X. Zhu, and T. Xiang, “Human-in-the-loop person re-identification,”
    in *ECCV*, 2016, pp. 405–422.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] H. Wang, S. Gong, X. Zhu 和 T. Xiang，“人机协作的人员再识别”，发表于 *ECCV*，2016年，页码405–422。'
- en: '[160] S. Paisitkriangkrai, C. Shen, and A. Van Den Hengel, “Learning to rank
    in person re-identification with metric ensembles,” in *CVPR*, 2015, pp. 1846–1855.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] S. Paisitkriangkrai, C. Shen 和 A. Van Den Hengel，“通过度量集合学习排名进行人员再识别”，发表于
    *CVPR*，2015年，页码1846–1855。'
- en: '[161] S. Bai, P. Tang, P. H. Torr, and L. J. Latecki, “Re-ranking via metric
    fusion for object retrieval and person re-identification,” in *CVPR*, 2019, pp.
    740–749.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] S. Bai, P. Tang, P. H. Torr 和 L. J. Latecki，“通过度量融合重新排序用于物体检索和人员再识别”，发表于
    *CVPR*，2019年，页码740–749。'
- en: '[162] S. Bai, X. Bai, and Q. Tian, “Scalable person re-identification on supervised
    smoothed manifold,” in *CVPR*, 2017, pp. 2530–2539.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] S. Bai, X. Bai 和 Q. Tian，“在监督平滑流形上进行可扩展人员再识别”，发表于 *CVPR*，2017年，页码2530–2539。'
- en: '[163] A. J. Ma and P. Li, “Query based adaptive re-ranking for person re-identification,”
    in *ACCV*, 2014, pp. 397–412.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] A. J. Ma 和 P. Li，“基于查询的自适应重新排序用于人员再识别”，发表于 *ACCV*，2014年，页码397–412。'
- en: '[164] J. Zhou, P. Yu, W. Tang, and Y. Wu, “Efficient online local metric adaptation
    via negative samples for person re-identification,” in *ICCV*, 2017, pp. 2420–2428.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] J. Zhou, P. Yu, W. Tang 和 Y. Wu，“通过负样本进行高效在线局部度量自适应，用于人员再识别”，发表于 *ICCV*，2017年，页码2420–2428。'
- en: '[165] L. Zheng, S. Wang, L. Tian, F. He, Z. Liu, and Q. Tian, “Query-adaptive
    late fusion for image search and person re-identification,” in *CVPR*, 2015, pp.
    1741–1750.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] L. Zheng, S. Wang, L. Tian, F. He, Z. Liu 和 Q. Tian，“查询自适应的图像搜索和人员再识别晚期融合”，发表于
    *CVPR*，2015年，页码1741–1750。'
- en: '[166] A. Barman and S. K. Shah, “Shape: A novel graph theoretic algorithm for
    making consensus-based decisions in person re-identification systems,” in *ICCV*,
    2017, pp. 1115–1124.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] A. Barman 和 S. K. Shah，“Shape：一种新型的图论算法，用于在人员再识别系统中做出基于共识的决策”，发表于 *ICCV*，2017年，页码1115–1124。'
- en: '[167] W.-S. Zheng, S. Gong, and T. Xiang, “Associating groups of people,” in
    *BMVC*, 2009, pp. 1–23.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] W.-S. Zheng, S. Gong 和 T. Xiang，“关联人群组”，发表于 *BMVC*，2009年，页码1–23。'
- en: '[168] C. C. Loy, C. Liu, and S. Gong, “Person re-identification by manifold
    ranking,” in *ICIP*, 2013, pp. 3567–3571.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] C. C. Loy, C. Liu 和 S. Gong，“通过流形排名进行人员再识别”，发表于 *ICIP*，2013年，页码3567–3571。'
- en: '[169] M. Gou, Z. Wu, A. Rates-Borras, O. Camps, R. J. Radke *et al.*, “A systematic
    evaluation and benchmark for person re-identification: Features, metrics, and
    datasets,” *IEEE TPAMI*, vol. 41, no. 3, pp. 523–536, 2018.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] M. Gou, Z. Wu, A. Rates-Borras, O. Camps, R. J. Radke *等*，“系统评估和人员再识别基准：特征、度量和数据集”，*IEEE
    TPAMI*，第41卷，第3期，页码523–536，2018年。'
- en: '[170] M. Li, X. Zhu, and S. Gong, “Unsupervised person re-identification by
    deep learning tracklet association,” in *ECCV*, 2018, pp. 737–753.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] M. Li, X. Zhu 和 S. Gong，“通过深度学习轨迹关联进行无监督人员再识别”，发表于 *ECCV*，2018年，页码737–753。'
- en: '[171] G. Song, B. Leng, Y. Liu, C. Hetang, and S. Cai, “Region-based quality
    estimation network for large-scale person re-identification,” in *AAAI*, 2018,
    pp. 7347–7354.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] G. Song, B. Leng, Y. Liu, C. Hetang 和 S. Cai，“基于区域的质量估计网络用于大规模人员再识别”，发表于
    *AAAI*，2018年，页码7347–7354。'
- en: '[172] G. Wang, Y. Yuan, X. Chen, J. Li, and X. Zhou, “Learning discriminative
    features with multiple granularities for person re-identification,” in *ACM MM*,
    2018, pp. 274–282.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] G. Wang, Y. Yuan, X. Chen, J. Li 和 X. Zhou，“通过多粒度学习区分特征用于人员再识别”，发表于 *ACM
    MM*，2018年，页码274–282。'
- en: '[173] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Ren, and Z. Wang,
    “Abd-net: Attentive but diverse person re-identification,” in *ICCV*, 2019, pp.
    8351–8361.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Ren 和 Z. Wang，“Abd-net：关注但多样的人员再识别”，发表于
    *ICCV*，2019年，页码8351–8361。'
- en: '[174] B. Chen, W. Deng, and J. Hu, “Mixed high-order attention network for
    person re-identification,” in *ICCV*, 2019, pp. 371–381.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] B. Chen, W. Deng 和 J. Hu，“混合高阶注意力网络用于人员再识别”，发表于 *ICCV*，2019年，页码371–381。'
- en: '[175] X. Zhang, H. Luo, X. Fan, W. Xiang, Y. Sun, Q. Xiao, W. Jiang, C. Zhang,
    and J. Sun, “Alignedreid: Surpassing human-level performance in person re-identification,”
    *arXiv preprint arXiv:1711.08184*, 2017.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] X. Zhang, H. Luo, X. Fan, W. Xiang, Y. Sun, Q. Xiao, W. Jiang, C. Zhang
    和 J. Sun，“Alignedreid：超越人类水平的人员再识别”，*arXiv preprint arXiv:1711.08184*，2017年。'
- en: '[176] M. Ye, A. J. Ma, L. Zheng, J. Li, and P. C. Yuen, “Dynamic label graph
    matching for unsupervised video re-identification,” in *ICCV*, 2017, pp. 5142–5150.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] M. Ye, A. J. Ma, L. Zheng, J. Li 和 P. C. Yuen，“用于无监督视频再识别的动态标签图匹配”，见
    *ICCV*，2017年，第5142–5150页。'
- en: '[177] Z. Wang, S. Zheng, M. Song, Q. Wang, A. Rahimpour, and H. Qi, “advpattern:
    Physical-world attacks on deep person re-identification via adversarially transformable
    patterns,” in *ICCV*, 2019, pp. 8341–8350.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Z. Wang, S. Zheng, M. Song, Q. Wang, A. Rahimpour 和 H. Qi，“advpattern：针对深度人物再识别的物理世界攻击通过对抗性可变模式”，见
    *ICCV*，2019年，第8341–8350页。'
- en: '[178] J. Zhang, N. Wang, and L. Zhang, “Multi-shot pedestrian re-identification
    via sequential decision making,” in *CVPR*, 2018, pp. 6781–6789.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] J. Zhang, N. Wang 和 L. Zhang，“通过顺序决策实现多镜头行人再识别”，见 *CVPR*，2018年，第6781–6789页。'
- en: '[179] A. Haque, A. Alahi, and L. Fei-Fei, “Recurrent attention models for depth-based
    person identification,” in *CVPR*, 2016, pp. 1229–1238.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] A. Haque, A. Alahi 和 L. Fei-Fei，“基于深度的人员识别的递归注意模型”，见 *CVPR*，2016年，第1229–1238页。'
- en: '[180] N. Karianakis, Z. Liu, Y. Chen, and S. Soatto, “Reinforced temporal attention
    and split-rate transfer for depth-based person re-identification,” in *ECCV*,
    2018, pp. 715–733.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] N. Karianakis, Z. Liu, Y. Chen 和 S. Soatto，“用于基于深度的人员再识别的增强时间注意和分裂率转移”，见
    *ECCV*，2018年，第715–733页。'
- en: '[181] I. B. Barbosa, M. Cristani, A. Del Bue, L. Bazzani, and V. Murino, “Re-identification
    with rgb-d sensors,” in *ECCV Workshop*, 2012, pp. 433–442.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] I. B. Barbosa, M. Cristani, A. Del Bue, L. Bazzani 和 V. Murino， “使用 rgb-d
    传感器进行再识别”，见 *ECCV Workshop*，2012年，第433–442页。'
- en: '[182] D. Chen, H. Li, X. Liu, Y. Shen, J. Shao, Z. Yuan, and X. Wang, “Improving
    deep visual representation for person re-identification by global and local image-language
    association,” in *ECCV*, 2018, pp. 54–70.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] D. Chen, H. Li, X. Liu, Y. Shen, J. Shao, Z. Yuan 和 X. Wang，“通过全局和局部图像-语言关联提高深度视觉表示用于人员再识别”，见
    *ECCV*，2018年，第54–70页。'
- en: '[183] Y. Zhang and H. Lu, “Deep cross-modal projection learning for image-text
    matching,” in *ECCV*, 2018, pp. 686–701.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Y. Zhang 和 H. Lu，“用于图像-文本匹配的深度跨模态投影学习”，见 *ECCV*，2018年，第686–701页。'
- en: '[184] J. Liu, Z.-J. Zha, R. Hong, M. Wang, and Y. Zhang, “Deep adversarial
    graph attention convolution network for text-based person search,” in *ACM MM*,
    2019, pp. 665–673.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] J. Liu, Z.-J. Zha, R. Hong, M. Wang 和 Y. Zhang，“基于文本的人物搜索的深度对抗图注意力卷积网络”，见
    *ACM MM*，2019年，第665–673页。'
- en: '[185] M. Ye, Z. Wang, X. Lan, and P. C. Yuen, “Visible thermal person re-identification
    via dual-constrained top-ranking,” in *IJCAI*, 2018, pp. 1092–1099.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] M. Ye, Z. Wang, X. Lan 和 P. C. Yuen，“通过双重约束顶级排名实现可见热人再识别”，见 *IJCAI*，2018年，第1092–1099页。'
- en: '[186] M. Ye, X. Lan, J. Li, and P. C. Yuen, “Hierarchical discriminative learning
    for visible thermal person re-identification,” in *AAAI*, 2018, pp. 7501–7508.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] M. Ye, X. Lan, J. Li 和 P. C. Yuen，“用于可见热人再识别的层次 discriminative 学习”，见
    *AAAI*，2018年，第7501–7508页。'
- en: '[187] Y. Hao, N. Wang, J. Li, and X. Gao, “Hsme: Hypersphere manifold embedding
    for visible thermal person re-identification,” in *AAAI*, 2019, pp. 8385–8392.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Y. Hao, N. Wang, J. Li 和 X. Gao，“Hsme：用于可见热人再识别的超球面流形嵌入”，见 *AAAI*，2019年，第8385–8392页。'
- en: '[188] M. Ye, J. Shen, and L. Shao, “Visible-infrared person re-identification
    via homogeneous augmented tri-modal learning,” *IEEE TIFS*, 2020.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] M. Ye, J. Shen 和 L. Shao，“通过同质增强三模态学习实现可见-红外人员再识别”，*IEEE TIFS*，2020年。'
- en: '[189] Z. Wang, Z. Wang, Y. Zheng, Y.-Y. Chuang, and S. Satoh, “Learning to
    reduce dual-level discrepancy for infrared-visible person re-identification,”
    in *CVPR*, 2019, pp. 618–626.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Z. Wang, Z. Wang, Y. Zheng, Y.-Y. Chuang 和 S. Satoh，“学习减少红外-可见再识别中的双层差异”，见
    *CVPR*，2019年，第618–626页。'
- en: '[190] G. Wang, T. Zhang, J. Cheng, S. Liu, Y. Yang, and Z. Hou, “Rgb-infrared
    cross-modality person re-identification via joint pixel and feature alignment,”
    in *ICCV*, 2019, pp. 3623–3632.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] G. Wang, T. Zhang, J. Cheng, S. Liu, Y. Yang 和 Z. Hou，“通过联合像素和特征对齐进行
    rgb-红外跨模态人员再识别”，见 *ICCV*，2019年，第3623–3632页。'
- en: '[191] S. Choi, S. Lee, Y. Kim, T. Kim, and C. Kim, “Hi-cmd: Hierarchical cross-modality
    disentanglement for visible-infrared person re-identification,” in *CVPR*, 2020,
    pp. 10 257–10 266.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] S. Choi, S. Lee, Y. Kim, T. Kim 和 C. Kim，“Hi-cmd：用于可见-红外人员再识别的层次跨模态解缠”，见
    *CVPR*，2020年，第10 257–10 266页。'
- en: '[192] M. Ye, J. Shen, D. J. Crandall, L. Shao, and J. Luo, “Dynamic dual-attentive
    aggregation learning for visible-infrared person re-identification,” in *ECCV*,
    2020.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] M. Ye, J. Shen, D. J. Crandall, L. Shao 和 J. Luo，“用于可见-红外人员再识别的动态双重注意聚合学习”，见
    *ECCV*，2020年。'
- en: '[193] Z. Wang, M. Ye, F. Yang, X. Bai, and S. Satoh, “Cascaded sr-gan for scale-adaptive
    low resolution person re-identification.” in *IJCAI*, 2018, pp. 3891–3897.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Z. Wang, M. Ye, F. Yang, X. Bai, 和 S. Satoh，“用于尺度自适应低分辨率人物重识别的级联 sr-gan”，发表于
    *IJCAI*，2018年，第3891–3897页。'
- en: '[194] Y.-J. Li, Y.-C. Chen, Y.-Y. Lin, X. Du, and Y.-C. F. Wang, “Recover and
    identify: A generative dual model for cross-resolution person re-identification,”
    in *ICCV*, 2019, pp. 8090–8099.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Y.-J. Li, Y.-C. Chen, Y.-Y. Lin, X. Du, 和 Y.-C. F. Wang，“恢复与识别：一种用于跨分辨率人物重识别的生成对抗模型”，发表于
    *ICCV*，2019年，第8090–8099页。'
- en: '[195] H. Liu, J. Feng, Z. Jie, K. Jayashree, B. Zhao, M. Qi, J. Jiang, and
    S. Yan, “Neural person search machines,” in *ICCV*, 2017, pp. 493–501.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] H. Liu, J. Feng, Z. Jie, K. Jayashree, B. Zhao, M. Qi, J. Jiang, 和 S.
    Yan，“神经人物搜索机器”，发表于 *ICCV*，2017年，第493–501页。'
- en: '[196] Y. Yan, Q. Zhang, B. Ni, W. Zhang, M. Xu, and X. Yang, “Learning context
    graph for person search,” in *CVPR*, 2019, pp. 2158–2167.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Y. Yan, Q. Zhang, B. Ni, W. Zhang, M. Xu, 和 X. Yang，“用于人物搜索的上下文图学习”，发表于
    *CVPR*，2019年，第2158–2167页。'
- en: '[197] B. Munjal, S. Amin, F. Tombari, and F. Galasso, “Query-guided end-to-end
    person search,” in *CVPR*, 2019, pp. 811–820.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] B. Munjal, S. Amin, F. Tombari, 和 F. Galasso，“基于查询引导的端到端人物搜索”，发表于 *CVPR*，2019年，第811–820页。'
- en: '[198] C. Han, J. Ye, Y. Zhong, X. Tan, C. Zhang, C. Gao, and N. Sang, “Re-id
    driven localization refinement for person search,” in *ICCV*, 2019, pp. 9814–9823.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] C. Han, J. Ye, Y. Zhong, X. Tan, C. Zhang, C. Gao, 和 N. Sang，“基于重识别驱动的定位优化用于人物搜索”，发表于
    *ICCV*，2019年，第9814–9823页。'
- en: '[199] X. Lan, H. Wang, S. Gong, and X. Zhu, “Deep reinforcement learning attention
    selection for person re-identification,” in *BMVC*, 2017.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] X. Lan, H. Wang, S. Gong, 和 X. Zhu，“用于人物重识别的深度强化学习注意力选择”，发表于 *BMVC*，2017年。'
- en: '[200] M. Yamaguchi, K. Saito, Y. Ushiku, and T. Harada, “Spatio-temporal person
    retrieval via natural language queries,” in *ICCV*, 2017, pp. 1453–1462.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] M. Yamaguchi, K. Saito, Y. Ushiku, 和 T. Harada，“通过自然语言查询进行时空人物检索”，发表于
    *ICCV*，2017年，第1453–1462页。'
- en: '[201] S. Tang, M. Andriluka, B. Andres, and B. Schiele, “Multiple people tracking
    by lifted multicut and person re-identification,” in *CVPR*, 2017, pp. 3539–3548.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] S. Tang, M. Andriluka, B. Andres, 和 B. Schiele，“通过提升多重切割和人物重识别进行多人跟踪”，发表于
    *CVPR*，2017年，第3539–3548页。'
- en: '[202] Y. Hou, L. Zheng, Z. Wang, and S. Wang, “Locality aware appearance metric
    for multi-target multi-camera tracking,” *arXiv preprint arXiv:1911.12037*, 2019.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Y. Hou, L. Zheng, Z. Wang, 和 S. Wang，“用于多目标多摄像头跟踪的局部性感知外观度量”，*arXiv preprint
    arXiv:1911.12037*，2019年。'
- en: '[203] E. Kodirov, T. Xiang, Z. Fu, and S. Gong, “Person re-identification by
    unsupervised l1 graph learning,” in *ECCV*, 2016, pp. 178–195.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] E. Kodirov, T. Xiang, Z. Fu, 和 S. Gong，“通过无监督 l1 图学习进行人物重识别”，发表于 *ECCV*，2016年，第178–195页。'
- en: '[204] Z. Liu, D. Wang, and H. Lu, “Stepwise metric promotion for unsupervised
    video person re-identification,” in *ICCV*, 2017, pp. 2429–2438.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Z. Liu, D. Wang, 和 H. Lu，“用于无监督视频人物重识别的逐步度量提升”，发表于 *ICCV*，2017年，第2429–2438页。'
- en: '[205] H. Fan, L. Zheng, and Y. Yang, “Unsupervised person re-identification:
    Clustering and fine-tuning,” *arXiv preprint arXiv:1705.10444*, 2017.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] H. Fan, L. Zheng, 和 Y. Yang，“无监督人物重识别：聚类与微调”，*arXiv preprint arXiv:1705.10444*，2017年。'
- en: '[206] M. Ye, J. Li, A. J. Ma, L. Zheng, and P. C. Yuen, “Dynamic graph co-matching
    for unsupervised video-based person re-identification,” *IEEE TIP*, vol. 28, no. 6,
    pp. 2976–2990, 2019.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] M. Ye, J. Li, A. J. Ma, L. Zheng, 和 P. C. Yuen，“用于无监督基于视频的人物重识别的动态图协同匹配”，*IEEE
    TIP*，第28卷，第6期，第2976–2990页，2019年。'
- en: '[207] X. Wang, R. Panda, M. Liu, Y. Wang, and et al., “Exploiting global camera
    network constraints for unsupervised video person re-identification,” *arXiv preprint
    arXiv:1908.10486*, 2019.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] X. Wang, R. Panda, M. Liu, Y. Wang, 等，“利用全球摄像头网络约束进行无监督视频人物重识别”，*arXiv
    preprint arXiv:1908.10486*，2019年。'
- en: '[208] K. Zeng, M. Ning, Y. Wang, and Y. Guo, “Hierarchical clustering with
    hard-batch triplet loss for person re-identification,” in *CVPR*, 2020, pp. 13 657–13 665.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] K. Zeng, M. Ning, Y. Wang, 和 Y. Guo，“用于人物重识别的带有硬批次三重损失的分层聚类”，发表于 *CVPR*，2020年，第13 657–13 665页。'
- en: '[209] H.-X. Yu, W.-S. Zheng, A. Wu, X. Guo, S. Gong, and J.-H. Lai, “Unsupervised
    person re-identification by soft multilabel learning,” in *CVPR*, 2019, pp. 2148–2157.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] H.-X. Yu, W.-S. Zheng, A. Wu, X. Guo, S. Gong, 和 J.-H. Lai，“通过软多标签学习进行无监督人物重识别”，发表于
    *CVPR*，2019年，第2148–2157页。'
- en: '[210] A. Wu, W.-S. Zheng, and J.-H. Lai, “Unsupervised person re-identification
    by camera-aware similarity consistency learning,” in *ICCV*, 2019, pp. 6922–6931.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] A. Wu, W.-S. Zheng, 和 J.-H. Lai，“通过摄像机感知相似性一致性学习进行无监督人物重识别”，发表于 *ICCV*，2019年，第6922–6931页。'
- en: '[211] J. Wu, Y. Yang, H. Liu, S. Liao, Z. Lei, and S. Z. Li, “Unsupervised
    graph association for person re-identification,” in *ICCV*, 2019, pp. 8321–8330.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] J. Wu, Y. Yang, H. Liu, S. Liao, Z. Lei, 和 S. Z. Li，“无监督图关联用于人员重识别”，发表于
    *ICCV*，2019年，第8321–8330页。'
- en: '[212] Y. Fu, Y. Wei, G. Wang, Y. Zhou, H. Shi, and T. S. Huang, “Self-similarity
    grouping: A simple unsupervised cross domain adaptation approach for person re-identification,”
    in *ICCV*, 2019, pp. 6112–6121.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] Y. Fu, Y. Wei, G. Wang, Y. Zhou, H. Shi, 和 T. S. Huang，“自相似分组：一种简单的无监督跨领域适应方法用于人员重识别”，发表于
    *ICCV*，2019年，第6112–6121页。'
- en: '[213] S. Bak and P. Carr, “One-shot metric learning for person re-identification,”
    in *CVPR*, 2017, pp. 2990–2999.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] S. Bak 和 P. Carr，“用于人员重识别的单次度量学习”，发表于 *CVPR*，2017年，第2990–2999页。'
- en: '[214] X. Wang, S. Paul, D. S. Raychaudhuri, and at al., “Learning person re-identification
    models from videos with weak supervision,” *arXiv preprint arXiv:2007.10631*,
    2020.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] X. Wang, S. Paul, D. S. Raychaudhuri, 等，“从视频中学习人员重识别模型的弱监督”，*arXiv 预印本
    arXiv:2007.10631*，2020年。'
- en: '[215] Z. Zhong, L. Zheng, S. Li, and Y. Yang, “Generalizing a person retrieval
    model hetero-and homogeneously,” in *ECCV*, 2018, pp. 172–188.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] Z. Zhong, L. Zheng, S. Li, 和 Y. Yang，“异质与同质泛化人员检索模型”，发表于 *ECCV*，2018年，第172–188页。'
- en: '[216] J. Liu, Z.-J. Zha, D. Chen, R. Hong, and M. Wang, “Adaptive transfer
    network for cross-domain person re-identification,” in *CVPR*, 2019, pp. 7202–7211.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] J. Liu, Z.-J. Zha, D. Chen, R. Hong, 和 M. Wang，“用于跨领域人员重识别的自适应迁移网络”，发表于
    *CVPR*，2019年，第7202–7211页。'
- en: '[217] Y. Huang, Q. Wu, J. Xu, and Y. Zhong, “Sbsgan: Suppression of inter-domain
    background shift for person re-identification,” in *ICCV*, 2019, pp. 9527–9536.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Y. Huang, Q. Wu, J. Xu, 和 Y. Zhong，“Sbsgan：抑制领域间背景漂移的人员重识别”，发表于 *ICCV*，2019年，第9527–9536页。'
- en: '[218] Y. Chen, X. Zhu, and S. Gong, “Instance-guided context rendering for
    cross-domain person re-identification,” in *ICCV*, 2019, pp. 232–242.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] Y. Chen, X. Zhu, 和 S. Gong，“实例引导的上下文渲染用于跨领域人员重识别”，发表于 *ICCV*，2019年，第232–242页。'
- en: '[219] Y. Ge, D. Chen, and H. Li, “Mutual mean-teaching: Pseudo label refinery
    for unsupervised domain adaptation on person re-identification,” in *ICLR*, 2020.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Y. Ge, D. Chen, 和 H. Li，“互助均值教学：用于人员重识别的无监督领域适应伪标签精炼”，发表于 *ICLR*，2020年。'
- en: '[220] Y. Wang, S. Liao, and L. Shao, “Surpassing real-world source training
    data: Random 3d characters for generalizable person re-identification,” in *ACM
    MM*, 2020, pp. 3422–3430.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Y. Wang, S. Liao, 和 L. Shao，“超越现实世界源训练数据：用于可泛化人员重识别的随机3D角色”，发表于 *ACM
    MM*，2020年，第3422–3430页。'
- en: '[221] L. Qi, L. Wang, J. Huo, L. Zhou, Y. Shi, and Y. Gao, “A novel unsupervised
    camera-aware domain adaptation framework for person re-identification,” in *ICCV*,
    2019, pp. 8080–8089.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] L. Qi, L. Wang, J. Huo, L. Zhou, Y. Shi, 和 Y. Gao，“一种新颖的无监督摄像头感知领域适应框架用于人员重识别”，发表于
    *ICCV*，2019年，第8080–8089页。'
- en: '[222] X. Zhang, J. Cao, C. Shen, and M. You, “Self-training with progressive
    augmentation for unsupervised cross-domain person re-identification,” in *ICCV*,
    2019, pp. 8222–8231.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] X. Zhang, J. Cao, C. Shen, 和 M. You，“通过逐步增强的自我训练进行无监督跨领域人员重识别”，发表于 *ICCV*，2019年，第8222–8231页。'
- en: '[223] Y. Ge, F. Zhu, D. Chen, R. Zhao, and H. Li, “Self-paced contrastive learning
    with hybrid memory for domain adaptive object re-id,” in *NeurIPS*, 2020.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Y. Ge, F. Zhu, D. Chen, R. Zhao, 和 H. Li，“具有混合记忆的自适应对比学习用于领域自适应对象重识别”，发表于
    *NeurIPS*，2020年。'
- en: '[224] J. Lv, W. Chen, Q. Li, and C. Yang, “Unsupervised cross-dataset person
    re-identification by transfer learning of spatial-temporal patterns,” in *CVPR*,
    2018, pp. 7948–7956.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] J. Lv, W. Chen, Q. Li, 和 C. Yang，“通过空间-时间模式迁移学习的无监督跨数据集人员重识别”，发表于 *CVPR*，2018年，第7948–7956页。'
- en: '[225] S. Liao and L. Shao, “Interpretable and generalizable person re-identification
    with query-adaptive convolution and temporal lifting,” in *ECCV*, 2020.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] S. Liao 和 L. Shao，“具有查询自适应卷积和时间提升的可解释且可泛化的人员重识别”，发表于 *ECCV*，2020年。'
- en: '[226] H.-X. Yu, A. Wu, and W.-S. Zheng, “Cross-view asymmetric metric learning
    for unsupervised person re-identification,” in *ICCV*, 2017, pp. 994–1002.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] H.-X. Yu, A. Wu, 和 W.-S. Zheng，“无监督人员重识别的跨视角非对称度量学习”，发表于 *ICCV*，2017年，第994–1002页。'
- en: '[227] X. Jin, C. Lan, W. Zeng, Z. Chen, and L. Zhang, “Style normalization
    and restitution for generalizable person re-identification,” in *CVPR*, 2020,
    pp. 3143–3152.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] X. Jin, C. Lan, W. Zeng, Z. Chen, 和 L. Zhang，“风格规范化与恢复用于可泛化的人员重识别”，发表于
    *CVPR*，2020年，第3143–3152页。'
- en: '[228] Y. Zhai, Q. Ye, S. Lu, M. Jia, R. Ji, and Y. Tian, “Multiple expert brainstorming
    for domain adaptive person re-identification,” in *ECCV*, 2020.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Y. Zhai, Q. Ye, S. Lu, M. Jia, R. Ji, 和 Y. Tian，“领域自适应人员重识别的多专家头脑风暴”，发表于
    *ECCV*，2020年。'
- en: '[229] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
    unsupervised visual representation learning,” in *CVPR*, 2020.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] K. He, H. Fan, Y. Wu, S. Xie, 和 R. Girshick，“用于无监督视觉表征学习的动量对比”，见于 *CVPR*，2020。'
- en: '[230] M. Ye, J. Shen, X. Zhang, P. C. Yuen, and S.-F. Chang, “Augmentation
    invariant and instance spreading feature for softmax embedding,” *IEEE TPAMI*,
    2020.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] M. Ye, J. Shen, X. Zhang, P. C. Yuen, 和 S.-F. Chang，“增强不变和实例扩展特征用于 softmax
    嵌入”，*IEEE TPAMI*，2020。'
- en: '[231] W.-S. Zheng, X. Li, T. Xiang, S. Liao, J. Lai, and S. Gong, “Partial
    person re-identification,” in *ICCV*, 2015, pp. 4678–4686.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] W.-S. Zheng, X. Li, T. Xiang, S. Liao, J. Lai, 和 S. Gong，“部分人脸再识别”，见于
    *ICCV*，2015，页码4678–4686。'
- en: '[232] L. He, J. Liang, H. Li, and Z. Sun, “Deep spatial feature reconstruction
    for partial person re-identification: Alignment-free approach,” in *CVPR*, 2018,
    pp. 7073–7082.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] L. He, J. Liang, H. Li, 和 Z. Sun，“用于部分人脸再识别的深度空间特征重建：无对齐方法”，见于 *CVPR*，2018，页码7073–7082。'
- en: '[233] L. He, Y. Wang, W. Liu, X. Liao, H. Zhao, Z. Sun, and J. Feng, “Foreground-aware
    pyramid reconstruction for alignment-free occluded person re-identification,”
    in *ICCV*, 2019, pp. 8450–8459.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] L. He, Y. Wang, W. Liu, X. Liao, H. Zhao, Z. Sun, 和 J. Feng，“前景感知金字塔重建用于无对齐遮挡人脸再识别”，见于
    *ICCV*，2019，页码8450–8459。'
- en: '[234] J. Miao, Y. Wu, P. Liu, Y. Ding, and Y. Yang, “Pose-guided feature alignment
    for occluded person re-identification,” in *ICCV*, 2019, pp. 542–551.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] J. Miao, Y. Wu, P. Liu, Y. Ding, 和 Y. Yang，“姿态引导特征对齐用于遮挡人脸再识别”，见于 *ICCV*，2019，页码542–551。'
- en: '[235] T. Yu, D. Li, Y. Yang, T. Hospedales, and T. Xiang, “Robust person re-identification
    by modelling feature uncertainty,” in *ICCV*, 2019, pp. 552–561.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] T. Yu, D. Li, Y. Yang, T. Hospedales, 和 T. Xiang，“通过建模特征不确定性实现鲁棒人脸再识别”，见于
    *ICCV*，2019，页码552–561。'
- en: '[236] M. Ye and P. C. Yuen, “Purifynet: A robust person re-identification model
    with noisy labels,” *IEEE TIFS*, 2020.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] M. Ye 和 P. C. Yuen，“Purifynet：具有噪声标签的鲁棒人脸再识别模型”，*IEEE TIFS*，2020。'
- en: '[237] X. Li, A. Wu, and W.-S. Zheng, “Adversarial open-world person re-identification,”
    in *ECCV*, 2018, pp. 280–296.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] X. Li, A. Wu, 和 W.-S. Zheng，“对抗性开放世界人脸再识别”，见于 *ECCV*，2018，页码280–296。'
- en: '[238] M. Golfarelli, D. Maio, and D. Malton, “On the error-reject trade-off
    in biometric verification systems,” *IEEE TPAMI*, vol. 19, no. 7, pp. 786–796,
    1997.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] M. Golfarelli, D. Maio, 和 D. Malton，“生物特征验证系统中的错误-拒绝权衡”，*IEEE TPAMI*，第19卷，第7期，页码786–796，1997。'
- en: '[239] G. Lisanti, N. Martinel, A. Del Bimbo, and G. Luca Foresti, “Group re-identification
    via unsupervised transfer of sparse features encoding,” in *ICCV*, 2017, pp. 2449–2458.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] G. Lisanti, N. Martinel, A. Del Bimbo, 和 G. Luca Foresti，“通过无监督稀疏特征编码转移进行群体再识别”，见于
    *ICCV*，2017，页码2449–2458。'
- en: '[240] Y. Cai, V. Takala, and M. Pietikainen, “Matching groups of people by
    covariance descriptor,” in *ICPR*, 2010, pp. 2744–2747.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Y. Cai, V. Takala, 和 M. Pietikainen，“通过协方差描述符匹配人群”，见于 *ICPR*，2010，页码2744–2747。'
- en: '[241] H. Xiao, W. Lin, B. Sheng, K. Lu, J. Yan, and et al., “Group re-identification:
    Leveraging and integrating multi-grain information,” in *ACM MM*, 2018, pp. 192–200.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] H. Xiao, W. Lin, B. Sheng, K. Lu, J. Yan, 等，“群体再识别：利用和整合多粒度信息”，见于 *ACM
    MM*，2018，页码192–200。'
- en: '[242] Z. Huang, Z. Wang, W. Hu, C.-W. Lin, and S. Satoh, “Dot-gnn: Domain-transferred
    graph neural network for group re-identification,” in *ACM MM*, 2019, pp. 1888–1896.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] Z. Huang, Z. Wang, W. Hu, C.-W. Lin, 和 S. Satoh，“Dot-gnn：用于群体再识别的领域转移图神经网络”，见于
    *ACM MM*，2019，页码1888–1896。'
- en: '[243] Y. Shen, H. Li, S. Yi, D. Chen, and X. Wang, “Person re-identification
    with deep similarity-guided graph neural network,” in *ECCV*, 2018, pp. 486–504.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Y. Shen, H. Li, S. Yi, D. Chen, 和 X. Wang，“利用深度相似性引导图神经网络进行人脸再识别”，见于
    *ECCV*，2018，页码486–504。'
- en: '[244] R. Panda, A. Bhuiyan, V. Murino, and A. K. Roy-Chowdhury, “Unsupervised
    adaptive re-identification in open world dynamic camera networks,” in *CVPR*,
    2017, pp. 7054–7063.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] R. Panda, A. Bhuiyan, V. Murino, 和 A. K. Roy-Chowdhury，“在开放世界动态摄像头网络中的无监督自适应再识别”，见于
    *CVPR*，2017，页码7054–7063。'
- en: '[245] S. M. Assari, H. Idrees, and M. Shah, “Human re-identification in crowd
    videos using personal, social and environmental constraints,” in *ECCV*, 2016,
    pp. 119–136.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] S. M. Assari, H. Idrees, 和 M. Shah，“利用个人、社会和环境约束进行人群视频中的人脸再识别”，见于 *ECCV*，2016，页码119–136。'
- en: '[246] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”
    in *CVPR*, 2018, pp. 7794–7803.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] X. Wang, R. Girshick, A. Gupta, 和 K. He，“非局部神经网络”，见于 *CVPR*，2018，页码7794–7803。'
- en: '[247] F. Radenović, G. Tolias, and O. Chum, “Fine-tuning cnn image retrieval
    with no human annotation,” *IEEE TPAMI*, vol. 41, no. 7, pp. 1655–1668, 2018.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] F. Radenović, G. Tolias, 和 O. Chum，“通过无人工注释对 CNN 图像检索进行微调”，*IEEE TPAMI*，第41卷，第7期，页码1655–1668，2018。'
- en: '[248] X. Wang, X. Han, W. Huang, D. Dong, and M. R. Scott, “Multi-similarity
    loss with general pair weighting for deep metric learning,” in *CVPR*, 2019, pp.
    5022–5030.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] X. Wang，X. Han，W. Huang，D. Dong 和 M. R. Scott，“深度度量学习中的多相似度损失与通用对权重，”在*CVPR*，2019，页5022–5030。'
- en: '[249] L. He, Z. Sun, Y. Zhu, and Y. Wang, “Recognizing partial biometric patterns.”
    *arXiv preprint arXiv:1810.07399*, 2018.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] L. He，Z. Sun，Y. Zhu 和 Y. Wang，“识别部分生物特征模式。”*arXiv 预印本 arXiv:1810.07399*，2018。'
- en: '[250] J. Xue, Z. Meng, K. Katipally, H. Wang, and K. van Zon, “Clothing change
    aware person identification,” in *CVPR Workshops*, 2018, pp. 2112–2120.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] J. Xue，Z. Meng，K. Katipally，H. Wang 和 K. van Zon，“考虑换衣的人员识别，”在*CVPR Workshops*，2018，页2112–2120。'
- en: '[251] F. Wan, Y. Wu, X. Qian, and Y. Fu, “When person re-identification meets
    changing clothes,” *arXiv preprint arXiv:2003.04070*, 2020.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] F. Wan，Y. Wu，X. Qian 和 Y. Fu，“当人物重识别遇上换衣服，”*arXiv 预印本 arXiv:2003.04070*，2020。'
- en: '[252] S. Roy, S. Paul, N. E. Young, and A. K. Roy-Chowdhury, “Exploiting transitivity
    for learning person re-identification models on a budget,” in *CVPR*, 2018, pp.
    7064–7072.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] S. Roy，S. Paul，N. E. Young 和 A. K. Roy-Chowdhury，“利用传递性进行预算内人物重识别模型的学习，”在*CVPR*，2018，页7064–7072。'
- en: '[253] Z. Zheng and Y. Yang, “Person re-identification in the 3d space,” *arXiv
    preprint arXiv:2006.04569*, 2020.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] Z. Zheng 和 Y. Yang，“3D空间中的人物重识别，”*arXiv 预印本 arXiv:2006.04569*，2020。'
- en: '[254] Y. Hu, D. Yi, S. Liao, Z. Lei, and S. Z. Li, “Cross dataset person re-identification,”
    in *ACCV*, 2014, pp. 650–664.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] Y. Hu，D. Yi，S. Liao，Z. Lei 和 S. Z. Li，“跨数据集人物重识别，”在*ACCV*，2014，页650–664。'
- en: '[255] G. Wu and S. Gong, “Decentralised learning from independent multi-domain
    labels for person re-identification,” *arXiv preprint arXiv:2006.04150*, 2020.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] G. Wu 和 S. Gong，“从独立的多领域标签进行去中心化学习以实现人物重识别，”*arXiv 预印本 arXiv:2006.04150*，2020。'
- en: '[256] P. Bhargava, “Incremental learning in person re-identification,” *arXiv
    preprint arXiv:1808.06281*, 2018.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] P. Bhargava，“人物重识别中的增量学习，”*arXiv 预印本 arXiv:1808.06281*，2018。'
- en: '[257] F. Zhu, X. Kong, L. Zheng, H. Fu, and Q. Tian, “Part-based deep hashing
    for large-scale person re-identification,” *IEEE Transactions on Image Processing
    (TIP)*, vol. 26, no. 10, pp. 4806–4817, 2017.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] F. Zhu，X. Kong，L. Zheng，H. Fu 和 Q. Tian，“基于部件的大规模人物重识别深度哈希，”*IEEE 图像处理交易（TIP）*，第26卷，第10期，页4806–4817，2017。'
- en: '[258] J. Chen, Y. Wang, J. Qin, L. Liu, and L. Shao, “Fast person re-identification
    via cross-camera semantic binary transformation,” in *CVPR*, 2017, pp. 3873–3882.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] J. Chen，Y. Wang，J. Qin，L. Liu 和 L. Shao，“通过跨摄像头语义二进制变换实现快速人物重识别，”在*CVPR*，2017，页3873–3882。'
- en: '[259] G. Wang, S. Gong, J. Cheng, and Z. Hou, “Faster person re-identification,”
    in *ECCV*, 2020, pp. 275–292.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] G. Wang，S. Gong，J. Cheng 和 Z. Hou，“更快的人物重识别，”在*ECCV*，2020，页275–292。'
- en: '[260] A. Wu, W.-S. Zheng, X. Guo, and J.-H. Lai, “Distilled person re-identification:
    Towards a more scalable system,” in *CVPR*, 2019, pp. 1187–1196.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] A. Wu，W.-S. Zheng，X. Guo 和 J.-H. Lai，“精炼的人员重识别：迈向更具规模的系统，”在*CVPR*，2019，页1187–1196。'
- en: '[261] M. Ye, X. Lan, and Q. Leng, “Modality-aware collaborative learning for
    visible thermal person re-identification,” in *ACM MM*, 2019, pp. 347–355.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] M. Ye，X. Lan 和 Q. Leng，“可见光-热成像人物重识别的模态感知协作学习，”在*ACM MM*，2019，页347–355。'
- en: '[262] Z. Feng, J. Lai, and X. Xie, “Learning modality-specific representations
    for visible-infrared person re-identification,” *IEEE Transactions on Image Processing
    (TIP)*, vol. 29, pp. 579–590, 2020.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] Z. Feng，J. Lai 和 X. Xie，“针对可见-红外人物重识别的模态特定表示学习，”*IEEE 图像处理交易（TIP）*，第29卷，页579–590，2020。'
- en: '[263] D. Li, X. Wei, X. Hong, and Y. Gong, “Infrared-visible cross-modal person
    re-identification with an x modality.” in *AAAI*, 2020, pp. 4610–4617.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] D. Li，X. Wei，X. Hong 和 Y. Gong，“红外-可见跨模态人物重识别与x模态。”在*AAAI*，2020，页4610–4617。'
- en: '[264] Y. Lu, Y. Wu, B. Liu, T. Zhang, B. Li, Q. Chu, and N. Yu, “Cross-modality
    person re-identification with shared-specific feature transfer,” in *CVPR*, 2020,
    pp. 13 379–13 389.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] Y. Lu，Y. Wu，B. Liu，T. Zhang，B. Li，Q. Chu 和 N. Yu，“跨模态人物重识别与共享特征转移，”在*CVPR*，2020，页13,379–13,389。'
- en: 'Supplemental Materials:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 附加材料：
- en: '![Refer to caption](img/6a7115d971446b6e8bbaac2af36bbb9c.png)'
  id: totrans-633
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6a7115d971446b6e8bbaac2af36bbb9c.png)'
- en: 'Figure R1: The framework of the proposed AGW baseline for single-modality image-based
    Re-ID.'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 图 R1：所提出的 AGW 基线框架用于单模态图像基础的重识别。
- en: This supplementary material accompanies our main manuscript with the implementation
    details and more comprehensive experiments. We first present the experiments on
    two single-modality closed-world Re-ID tasks, including image-based Re-ID on four
    datasets in Section A and video-based Re-ID on four datasets in Section B. Then
    we introduce the comprehensive comparison on two open-world Re-ID tasks, including
    visible-infrared cross-modality Re-ID on two datasets in Section C and partial
    Re-ID on two datasets in Section D. In addition, a structure overview for our
    survey is finally summarized.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 这些补充材料与我们的主要手稿一起提供了实现细节和更全面的实验。我们首先展示了两个单模态封闭世界 Re-ID 任务的实验，包括在第 A 节中基于图像的 Re-ID
    和在第 B 节中基于视频的 Re-ID，涉及四个数据集。接着，我们介绍了两个开放世界 Re-ID 任务的全面比较，包括在第 C 节中可见-红外跨模态 Re-ID
    和在第 D 节中部分 Re-ID，涉及两个数据集。此外，最后总结了我们调查的结构概述。
- en: A. Experiments on Single-modality Image-based Re-ID
  id: totrans-636
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A. 单模态图像基础的 Re-ID 实验
- en: 'Architecture Design. The overall structure⁴⁴4[https://github.com/mangye16/ReID-Survey](https://github.com/mangye16/ReID-Survey)
    of our proposed AGW baseline for single-modality Re-ID is illustrated in § [4](#S4
    "4 An Outlook: Re-ID in Next Era ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook") (Fig. [R1](#S5.F1 "Figure R1 ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook")). We adopt ResNet50 pre-trained on ImageNet
    as our backbone network and change the dimension of the fully connected layer
    to be consistent with the number of identities in the training dataset. The stride
    of the last spatial down-sampling operation in the backbone network is changed
    from 2 to 1. Consequently, the spatial size of the output feature map is changed
    from $8\times 4$ to $16\times 8$, when feeding an image of resolution $256\times
    128$ as input. In our method, we replace the Global Average Pooling in the original
    ResNet50 with the Generalized-mean (GeM) pooling. The pooling hyper parameter
    $p_{k}$ for generalized-mean pooling is initialized as 3.0. A BatchNorm layer,
    named BNNeck is plugged between the GeM pooling layer and the fully connected
    layer. The output of the GeM pooling layer is adopted for computing center loss
    and triplet loss in the training stage, while the feature after BNNeck is used
    for computing distance between pedestrian images during testing inference stage.'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 架构设计。我们提出的 AGW 基线模型的整体结构⁴⁴4[https://github.com/mangye16/ReID-Survey](https://github.com/mangye16/ReID-Survey)
    用于单模态 Re-ID，如 § [4](#S4 "4 展望：下一时代的 Re-ID ‣ 深度学习在行人重识别中的应用：调查与展望") (图 [R1](#S5.F1
    "图 R1 ‣ 深度学习在行人重识别中的应用：调查与展望")) 所示。我们采用在 ImageNet 上预训练的 ResNet50 作为骨干网络，并将全连接层的维度调整为与训练数据集中身份的数量一致。骨干网络中最后一次空间下采样操作的步幅从
    2 改为 1。因此，当输入分辨率为 $256\times 128$ 的图像时，输出特征图的空间大小从 $8\times 4$ 改为 $16\times 8$。在我们的方法中，我们用广义均值
    (GeM) 池化替换了原始 ResNet50 中的全局平均池化。广义均值池化的超参数 $p_{k}$ 初始化为 3.0。一个名为 BNNeck 的 BatchNorm
    层被插入在 GeM 池化层和全连接层之间。GeM 池化层的输出用于在训练阶段计算中心损失和三元组损失，而 BNNeck 之后的特征用于在测试推理阶段计算行人图像之间的距离。
- en: Non-local Attention. The ResNet contains 4 residual stages, i.e. $conv2\_x$,
    $conv3\_x$, $conv4\_x$ and $conv5\_x$, each containing stacks of bottleneck residual
    blocks. We inserted five non-local blocks after $conv3\_3$, $conv3\_4$, $conv4\_4$,
    $conv4\_5$ and $conv4\_6$ respectively. We adopt the Dot Product version of non-local
    block with a bottleneck of 512 channels in our experiment. For each non-local
    block, a BatchNorm layer is added right after the last linear layer that represents
    $W_{z}$. The affine parameter of this BatchNorm layer is initialized as zeros
    to ensure that the non-local block can be inserted into any pre-trained networks
    while maintaining its initial behavior.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 非局部注意力。ResNet 包含 4 个残差阶段，即 $conv2\_x$、$conv3\_x$、$conv4\_x$ 和 $conv5\_x$，每个阶段包含瓶颈残差块的堆叠。我们在
    $conv3\_3$、$conv3\_4$、$conv4\_4$、$conv4\_5$ 和 $conv4\_6$ 后分别插入了五个非局部块。我们在实验中采用了非局部块的点积版本，瓶颈通道数为
    512。对于每个非局部块，在表示 $W_{z}$ 的最后一个线性层之后添加了一个 BatchNorm 层。这个 BatchNorm 层的仿射参数初始化为零，以确保非局部块可以插入到任何预训练的网络中，同时保持其初始行为。
- en: Training Strategy. In the training stage, we randomly sample 16 identities and
    4 images for each identity to form a mini-batch of size 64. Each image is resized
    into $256\times 128$ pixels, padding 10 pixels with zero values, and then randomly
    cropped into $256\times 128$ pixels. Random horizontally flipping and random erasing
    with 0.5 probability respectively are also adopted for data augmentation. Specifically,
    random erasing augmentation [[123](#bib.bib123)] randomly selects a rectangle
    region with area ratio $r_{e}$ to the whole image, and erase its pixels with the
    mean value of the image. Besides, the aspect ratio of this region is randomly
    initialized between $r_{1}$ and $r_{2}$. In our method, we set the above hyper-parameter
    as $0.02<r_{e}<0.4$, $r1=0.3$ and $r2=3.33$. At last, we normalize the RGB channels
    of each image with mean 0.485, 0.456, 0.406 and stand deviation 0.229, 0.224,
    0.225, respectively, which are the same with settings in [[122](#bib.bib122)].
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 训练策略。在训练阶段，我们随机抽取16个身份，每个身份4张图像，形成一个大小为64的小批量。每张图像被调整为$256\times 128$像素，填充10像素的零值，然后随机裁剪为$256\times
    128$像素。数据增强还采用了随机水平翻转和随机擦除，概率分别为0.5。具体来说，随机擦除增强[[123](#bib.bib123)]随机选择一个矩形区域，其面积比$r_{e}$与整张图像的面积比，并用图像的均值擦除其像素。此外，该区域的纵横比在$r_{1}$和$r_{2}$之间随机初始化。在我们的方法中，以上超参数设置为$0.02<r_{e}<0.4$，$r1=0.3$和$r2=3.33$。最后，我们将每张图像的RGB通道标准化，均值为0.485、0.456、0.406，标准差为0.229、0.224、0.225，与[[122](#bib.bib122)]中的设置相同。
- en: 'TABLE R1: Comparison with the state-of-the-arts on four video-based Re-ID datasets,
    including MARS, DukeVideo, PRID2011 and iLIDS-VID. Rank-1, -5, -10 accuracy (%),
    mAP (%) and mINP (%) are reported.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 表R1：在包括MARS、DukeVideo、PRID2011和iLIDS-VID在内的四个基于视频的Re-ID数据集上的最先进技术比较。报告了Rank-1、-5、-10准确率（%）、mAP（%）和mINP（%）。
- en: '| Method | Venue | MARS | DukeVideo | PRID2011 | iLIDS-VID |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 会议 | MARS | DukeVideo | PRID2011 | iLIDS-VID |'
- en: '| R1 | R5 | mAP | mINP | R1 | R5 | mAP | mINP | R1 | R5 | R20 | mINP | R1 |
    R5 | R20 | mINP |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| R1 | R5 | mAP | mINP | R1 | R5 | mAP | mINP | R1 | R5 | R20 | mINP | R1 |
    R5 | R20 | mINP |'
- en: '| ETAP [[144](#bib.bib144)] | CVPR18 | 80.7 | 92.0 | 67.3 | - | 83.6 | 94.5
    | 78.3 | - | - | - | - | - | - | - | - | - |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| ETAP [[144](#bib.bib144)] | CVPR18 | 80.7 | 92.0 | 67.3 | - | 83.6 | 94.5
    | 78.3 | - | - | - | - | - | - | - | - | - |'
- en: '| DRSA [[133](#bib.bib133)] | CVPR18 | 82.3 | - | 65.8 | - | - | - | - | -
    | 93.2 | - | - | - | 80.2 | - | - | - |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| DRSA [[133](#bib.bib133)] | CVPR18 | 82.3 | - | 65.8 | - | - | - | - | -
    | 93.2 | - | - | - | 80.2 | - | - | - |'
- en: '| Snippet [[134](#bib.bib134)] | CVPR18 | 86.3 | 94.7 | 76.1 | - | - | - |
    - | - | 93.0 | 99.3 | - | - | 85.4 | 96.7 | - | - |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| Snippet [[134](#bib.bib134)] | CVPR18 | 86.3 | 94.7 | 76.1 | - | - | - |
    - | - | 93.0 | 99.3 | - | - | 85.4 | 96.7 | - | - |'
- en: '| STA [[135](#bib.bib135)] | AAAI18 | 86.3 | 95.7 | 80.8 | - | 96.2 | 99.3
    | 94.9 | - | - | - | - | - | - | - | - | - |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| STA [[135](#bib.bib135)] | AAAI18 | 86.3 | 95.7 | 80.8 | - | 96.2 | 99.3
    | 94.9 | - | - | - | - | - | - | - | - | - |'
- en: '| VRSTC [[20](#bib.bib20)] | CVPR19 | 88.5 | 96.5 | 82.3 | - | 95.0 | 99.1
    | 93.5 | - | - | - | - | - | 83.4 | 95.5 | 99.5 | - |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| VRSTC [[20](#bib.bib20)] | CVPR19 | 88.5 | 96.5 | 82.3 | - | 95.0 | 99.1
    | 93.5 | - | - | - | - | - | 83.4 | 95.5 | 99.5 | - |'
- en: '| ADFD [[110](#bib.bib110)] | CVPR19 | 87.0 | 95.4 | 78.2 | - | - | - | - |
    - | 93.9 | 99.5 | 100 | - | 86.3 | 97.4 | 99.7 | - |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| ADFD [[110](#bib.bib110)] | CVPR19 | 87.0 | 95.4 | 78.2 | - | - | - | - |
    - | 93.9 | 99.5 | 100 | - | 86.3 | 97.4 | 99.7 | - |'
- en: '| GLTR [[136](#bib.bib136)] | ICCV19 | 87.0 | 95.7 | 78.4 | - | 96.2 | 99.3
    | 93.7 | - | 95.5 | 100.0 | - | - | 86.0 | 98.0 | - | - |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| GLTR [[136](#bib.bib136)] | ICCV19 | 87.0 | 95.7 | 78.4 | - | 96.2 | 99.3
    | 93.7 | - | 95.5 | 100.0 | - | - | 86.0 | 98.0 | - | - |'
- en: '| CoSeg [[132](#bib.bib132)] | ICCV19 | 84.9 | 95.5 | 79.9 | 57.8 | 95.4 |
    99.3 | 94.1 | 89.8 | - | - | - | - | 79.6 | 95.3 | 99.3 | - |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| CoSeg [[132](#bib.bib132)] | ICCV19 | 84.9 | 95.5 | 79.9 | 57.8 | 95.4 |
    99.3 | 94.1 | 89.8 | - | - | - | - | 79.6 | 95.3 | 99.3 | - |'
- en: '| BagTricks [[122](#bib.bib122)] | CVPR19W | 85.8 | 95.2 | 81.6 | 62.0 | 92.6
    | 98.9 | 92.4 | 88.3 | 84.3 | 93.3 | 98.0 | 88.5 | 74.0 | 93.3 | 99.1 | 82.2 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| BagTricks [[122](#bib.bib122)] | CVPR19W | 85.8 | 95.2 | 81.6 | 62.0 | 92.6
    | 98.9 | 92.4 | 88.3 | 84.3 | 93.3 | 98.0 | 88.5 | 74.0 | 93.3 | 99.1 | 82.2 |'
- en: '| AGW | - | 87.0 | 95.7 | 82.2 | 62.8 | 94.6 | 99.1 | 93.4 | 89.2 | 87.8 |
    96.6 | 98.9 | 91.7 | 78.0 | 97.0 | 99.5 | 85.5 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| AGW | - | 87.0 | 95.7 | 82.2 | 62.8 | 94.6 | 99.1 | 93.4 | 89.2 | 87.8 |
    96.6 | 98.9 | 91.7 | 78.0 | 97.0 | 99.5 | 85.5 |'
- en: '| AGW[+] | - | 87.6 | 85.8 | 83.0 | 63.9 | 95.4 | 99.3 | 94.9 | 91.9 | 94.4
    | 98.4 | 100 | 95.4 | 83.2 | 98.3 | 99.7 | 89.0 |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| AGW[+] | - | 87.6 | 85.8 | 83.0 | 63.9 | 95.4 | 99.3 | 94.9 | 91.9 | 94.4
    | 98.4 | 100 | 95.4 | 83.2 | 98.3 | 99.7 | 89.0 |'
- en: Training Loss. In the training stage, three types of loss are combined for optimization,
    including identity classification loss ($\mathcal{L}_{id}$), center loss ($\mathcal{L}_{ct}$)
    and our proposed weighted regularization triplet loss ($\mathcal{L}_{wrt}$).
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失。在训练阶段，结合了三种类型的损失进行优化，包括身份分类损失 ($\mathcal{L}_{id}$)、中心损失 ($\mathcal{L}_{ct}$)
    和我们提出的加权正则化三元组损失 ($\mathcal{L}_{wrt}$)。
- en: '|  | $\mathcal{L}_{total}=\mathcal{L}_{id}+\beta_{1}\mathcal{L}_{ct}+\beta_{2}\mathcal{L}_{wrt}.$
    |  | (R1) |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{total}=\mathcal{L}_{id}+\beta_{1}\mathcal{L}_{ct}+\beta_{2}\mathcal{L}_{wrt}.$
    |  | (R1) |'
- en: 'The balanced weight of the center loss ($\beta_{1}$) is set to 0.0005 and the
    one ($\beta_{2}$) of the weighted regularized triplet loss is set to 1.0. Label
    smoothing is adopted to improve the original identity classification loss, which
    encourages the model to be less confident during training and prevent overfitting
    for classification task. Concretely, it changes the one-hot label as follow:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 中心损失的平衡权重 ($\beta_{1}$) 设为 0.0005，加权正则化三元组损失的权重 ($\beta_{2}$) 设为 1.0。采用标签平滑技术以改进原始的身份分类损失，这有助于模型在训练过程中减少自信，从而防止分类任务的过拟合。具体来说，它将
    one-hot 标签更改为如下形式：
- en: '|  | $q_{i}=\left\{\begin{array}[]{ll}{1-\frac{N-1}{N}\varepsilon}&amp;{\text{
    if }i=y}\\ {\varepsilon/N}&amp;{\text{ otherwise }},\end{array}\right.$ |  | (R2)
    |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '|  | $q_{i}=\left\{\begin{array}[]{ll}{1-\frac{N-1}{N}\varepsilon}&amp;{\text{
    如果 }i=y}\\ {\varepsilon/N}&amp;{\text{ 否则 }},\end{array}\right.$ |  | (R2) |'
- en: where $N$ is the total number of identities, $\epsilon$ is a small constant
    to reduce the confidence for the true identity label $y$ and $q_{i}$ is treated
    as a new classification target for training. In our method, we set $\epsilon$
    to be 0.1.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 是身份的总数量，$\epsilon$ 是一个小常数，用于减少对真实身份标签 $y$ 的信心，而 $q_{i}$ 被视为训练的新分类目标。在我们的方法中，我们将
    $\epsilon$ 设为 0.1。
- en: 'Optimizer Setting. Adam optimizer with a weight decay $0.0005$ is adopted to
    train our model. The initial learning rate is set as 0.00035 and is decreased
    by 0.1 at the 40th epoch and 70th epoch, respectively. The model is trained for
    120 epochs in total. Besides, a warm-up learning rate scheme is also employed
    to improve the stability of training process and bootstrap the network for better
    performance. Specifically, in the first 10 epochs, the learning rate is linearly
    increased from $3.5\times 10^{-5}$ to $3.5\times 10^{-4}$. The learning rate $lr(t)$
    at epoch $t$ can be computed as:'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器设置。我们采用带有权重衰减 $0.0005$ 的 Adam 优化器来训练我们的模型。初始学习率设为 0.00035，并在第 40 和第 70 个周期分别降低
    0.1。模型总共训练 120 个周期。此外，还采用了预热学习率方案，以提高训练过程的稳定性，并为网络的更好表现提供引导。具体来说，在前 10 个周期中，学习率从
    $3.5\times 10^{-5}$ 线性增加到 $3.5\times 10^{-4}$。第 $t$ 个周期的学习率 $lr(t)$ 可以计算为：
- en: '|  | <math   alttext="\operatorname{lr}(t)=\left\{\begin{array}[]{ll}{3.5\times
    10^{-5}\times\frac{t}{10}}&amp;{\text{ if }t\leq 10}\\ {3.5\times 10^{-4}}&amp;{\text{
    if }10<t\leq 40}\\'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\operatorname{lr}(t)=\left\{\begin{array}[]{ll}{3.5\times
    10^{-5}\times\frac{t}{10}}&amp;{\text{ 如果 }t\leq 10}\\ {3.5\times 10^{-4}}&amp;{\text{
    如果 }10<t\leq 40}\\'
- en: '{3.5\times 10^{-5}}&amp;{\text{ if }40<t\leq 70}\\'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '{3.5\times 10^{-5}}&amp;{\text{ 如果 }40<t\leq 70}\\'
- en: '{3.5\times 10^{-6}}&amp;{\text{ if }70<t\leq 120}.\end{array}\right." display="block"><semantics
    ><mrow  ><mrow ><mi  >lr</mi><mo >⁡</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >t</mi><mo stretchy="false" >)</mo></mrow></mrow><mo  >=</mo><mrow ><mo >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  columnalign="left"
    ><mrow  ><mn >3.5</mn><mo lspace="0.222em" rspace="0.222em"  >×</mo><msup ><mn
    >10</mn><mrow  ><mo >−</mo><mn >5</mn></mrow></msup><mo lspace="0.222em" rspace="0.222em"  >×</mo><mstyle
    displaystyle="false"  ><mfrac ><mi >t</mi><mn  >10</mn></mfrac></mstyle></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  > if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >t</mi></mrow><mo >≤</mo><mn >10</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow  ><mn >3.5</mn><mo lspace="0.222em" rspace="0.222em"  >×</mo><msup ><mn
    >10</mn><mrow  ><mo >−</mo><mn >4</mn></mrow></msup></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow  ><mtext > if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mn >10</mn></mrow><mo
    ><</mo><mi >t</mi><mo  >≤</mo><mn >40</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow  ><mn >3.5</mn><mo lspace="0.222em" rspace="0.222em"  >×</mo><msup ><mn
    >10</mn><mrow  ><mo >−</mo><mn >5</mn></mrow></msup></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow  ><mtext > if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mn >40</mn></mrow><mo
    ><</mo><mi >t</mi><mo  >≤</mo><mn >70</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow  ><mn >3.5</mn><mo lspace="0.222em" rspace="0.222em"  >×</mo><msup ><mn
    >10</mn><mrow  ><mo >−</mo><mn >6</mn></mrow></msup></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow  ><mrow ><mtext > if </mtext><mo lspace="0em" rspace="0em" >​</mo><mn >70</mn></mrow><mo
    ><</mo><mi >t</mi><mo  >≤</mo><mn >120</mn></mrow><mo lspace="0em"  >.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >lr</ci><ci  >𝑡</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix  ><matrixrow ><apply ><cn type="float"  >3.5</cn><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer"  >10</cn><apply
    ><cn type="integer" >5</cn></apply></apply><apply ><ci >𝑡</ci><cn type="integer"
    >10</cn></apply></apply><apply ><apply  ><ci ><mtext > if </mtext></ci><ci >𝑡</ci></apply><cn
    type="integer" >10</cn></apply></matrixrow><matrixrow ><apply  ><cn type="float"  >3.5</cn><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer"  >10</cn><apply
    ><cn type="integer" >4</cn></apply></apply></apply><apply ><apply ><apply  ><ci
    ><mtext > if </mtext></ci><cn type="integer" >10</cn></apply><ci >𝑡</ci></apply><apply
    ><cn type="integer"  >40</cn></apply></apply></matrixrow><matrixrow ><apply ><cn
    type="float"  >3.5</cn><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn
    type="integer"  >10</cn><apply ><cn type="integer" >5</cn></apply></apply></apply><apply
    ><apply ><apply  ><ci ><mtext > if </mtext></ci><cn type="integer" >40</cn></apply><ci
    >𝑡</ci></apply><apply ><cn type="integer"  >70</cn></apply></apply></matrixrow><matrixrow
    ><apply ><cn type="float"  >3.5</cn><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn
    type="integer"  >10</cn><apply ><cn type="integer" >6</cn></apply></apply></apply><apply
    ><apply ><apply  ><ci ><mtext > if </mtext></ci><cn type="integer"  >70</cn></apply><ci
    >𝑡</ci></apply><apply ><cn type="integer" >120</cn></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\operatorname{lr}(t)=\left\{\begin{array}[]{ll}{3.5\times
    10^{-5}\times\frac{t}{10}}&{\text{ if }t\leq 10}\\ {3.5\times 10^{-4}}&{\text{
    if }10<t\leq 40}\\ {3.5\times 10^{-5}}&{\text{ if }40<t\leq 70}\\ {3.5\times 10^{-6}}&{\text{
    if }70<t\leq 120}.\end{array}\right.</annotation></semantics></math> |  | (R3)
    |'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: \operatorname{lr}(t)=\left\{\begin{array}[]{ll}{3.5\times 10^{-5}\times\frac{t}{10}}&{\text{
    如果 }t\leq 10}\\ {3.5\times 10^{-4}}&{\text{ 如果 }10<t\leq 40}\\ {3.5\times 10^{-5}}&{\text{
    如果 }40<t\leq 70}\\ {3.5\times 10^{-6}}&{\text{ 如果 }70<t\leq 120}.\end{array}\right.
- en: '![Refer to caption](img/bbbf1261eaa144396f805dd3b2d25b1b.png)'
  id: totrans-663
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bbbf1261eaa144396f805dd3b2d25b1b.png)'
- en: 'Figure R2: The framework of the proposed AGW baseline for cross-modality visible-infrared
    Re-ID.'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 图 R2：提出的 AGW 基线框架用于跨模态可见-红外 Re-ID。
- en: 'TABLE R2: Comparison with the state-of-the-arts on SYSU-MM01 dataset. Rank
    at $r$ accuracy (%), mAP (%) and mINP (%) are reported. (Single-shot query setting
    [[21](#bib.bib21)] for experiments). “*” represents methods published after the
    paper submission.'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 表 R2：在 SYSU-MM01 数据集上的技术对比。报告了 $r$ 准确率（%）、mAP（%）和 mINP（%）。（实验使用单次查询设置 [[21](#bib.bib21)]。）“*”表示论文提交后发表的方法。
- en: '| Settings | All Search | Indoor Search |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | 全部搜索 | 室内搜索 |'
- en: '| --- | --- | --- |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Method | Venue | $r=1$ | $r=10$ | $r=20$ | mAP | mINP | $r=1$ | $r=10$ |
    $r=20$ | mAP | mINP |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 场地 | $r=1$ | $r=10$ | $r=20$ | mAP | mINP | $r=1$ | $r=10$ | $r=20$
    | mAP | mINP |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| One-stream [[21](#bib.bib21)] | ICCV17 | 12.04 | 49.68 | 66.74 | 13.67 |
    - | 16.94 | 63.55 | 82.10 | 22.95 | - |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| One-stream [[21](#bib.bib21)] | ICCV17 | 12.04 | 49.68 | 66.74 | 13.67 |
    - | 16.94 | 63.55 | 82.10 | 22.95 | - |'
- en: '| Two-stream [[21](#bib.bib21)] | ICCV17 | 11.65 | 47.99 | 65.50 | 12.85 |
    - | 15.60 | 61.18 | 81.02 | 21.49 | - |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| Two-stream [[21](#bib.bib21)] | ICCV17 | 11.65 | 47.99 | 65.50 | 12.85 |
    - | 15.60 | 61.18 | 81.02 | 21.49 | - |'
- en: '| Zero-Pad [[21](#bib.bib21)] | ICCV17 | 14.80 | 54.12 | 71.33 | 15.95 | -
    | 20.58 | 68.38 | 85.79 | 26.92 | - |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| Zero-Pad [[21](#bib.bib21)] | ICCV17 | 14.80 | 54.12 | 71.33 | 15.95 | -
    | 20.58 | 68.38 | 85.79 | 26.92 | - |'
- en: '| TONE [[186](#bib.bib186)] | AAAI18 | 12.52 | 50.72 | 68.60 | 14.42 | - |
    20.82 | 68.86 | 84.46 | 26.38 | - |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| TONE [[186](#bib.bib186)] | AAAI18 | 12.52 | 50.72 | 68.60 | 14.42 | - |
    20.82 | 68.86 | 84.46 | 26.38 | - |'
- en: '| HCML [[186](#bib.bib186)] | AAAI18 | 14.32 | 53.16 | 69.17 | 16.16 | - |
    24.52 | 73.25 | 86.73 | 30.08 | - |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| HCML [[186](#bib.bib186)] | AAAI18 | 14.32 | 53.16 | 69.17 | 16.16 | - |
    24.52 | 73.25 | 86.73 | 30.08 | - |'
- en: '| BDTR [[142](#bib.bib142)] | IJCAI18 | 27.32 | 66.96 | 81.07 | 27.32 | - |
    31.92 | 77.18 | 89.28 | 41.86 | - |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| BDTR [[142](#bib.bib142)] | IJCAI18 | 27.32 | 66.96 | 81.07 | 27.32 | - |
    31.92 | 77.18 | 89.28 | 41.86 | - |'
- en: '| eBDTR [[142](#bib.bib142)] | TIFS19 | 27.82 | 67.34 | 81.34 | 28.42 | - |
    32.46 | 77.42 | 89.62 | 42.46 | - |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| eBDTR [[142](#bib.bib142)] | TIFS19 | 27.82 | 67.34 | 81.34 | 28.42 | - |
    32.46 | 77.42 | 89.62 | 42.46 | - |'
- en: '| HSME [[187](#bib.bib187)] | AAAI19 | 20.68 | 32.74 | 77.95 | 23.12 | - |
    - | - | - | - | - |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| HSME [[187](#bib.bib187)] | AAAI19 | 20.68 | 32.74 | 77.95 | 23.12 | - |
    - | - | - | - | - |'
- en: '| D²RL [[189](#bib.bib189)] | CVPR19 | 28.9 | 70.6 | 82.4 | 29.2 | - | - |
    - | - | - | - |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| D²RL [[189](#bib.bib189)] | CVPR19 | 28.9 | 70.6 | 82.4 | 29.2 | - | - |
    - | - | - | - |'
- en: '| MAC [[261](#bib.bib261)] | MM19 | 33.26 | 79.04 | 90.09 | 36.22 | - | 36.43
    | 62.36 | 71.63 | 37.03 | - |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| MAC [[261](#bib.bib261)] | MM19 | 33.26 | 79.04 | 90.09 | 36.22 | - | 36.43
    | 62.36 | 71.63 | 37.03 | - |'
- en: '| MSR [[262](#bib.bib262)] | TIP19 | 37.35 | 83.40 | 93.34 | 38.11 | - | 39.64
    | 89.29 | 97.66 | 50.88 | - |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| MSR [[262](#bib.bib262)] | TIP19 | 37.35 | 83.40 | 93.34 | 38.11 | - | 39.64
    | 89.29 | 97.66 | 50.88 | - |'
- en: '| AlignGAN [[190](#bib.bib190)] | ICCV19 | 42.4 | 85.0 | 93.7 | 40.7 | - |
    45.9 | 87.6 | 94.4 | 54.3 | - |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| AlignGAN [[190](#bib.bib190)] | ICCV19 | 42.4 | 85.0 | 93.7 | 40.7 | - |
    45.9 | 87.6 | 94.4 | 54.3 | - |'
- en: '| X-Modal^∗  [[263](#bib.bib263)] | AAAI-20 | 49.9 | 89.8 | 96.0 | 50.7 | -
    | - | - | - | - | - |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| X-Modal^∗  [[263](#bib.bib263)] | AAAI-20 | 49.9 | 89.8 | 96.0 | 50.7 | -
    | - | - | - | - | - |'
- en: '| Hi-CMD^∗  [[191](#bib.bib191)] | CVPR20 | 34.9 | 77.6 | - | 35.9 | - | -
    | - | - | - | - |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| Hi-CMD^∗  [[191](#bib.bib191)] | CVPR20 | 34.9 | 77.6 | - | 35.9 | - | -
    | - | - | - | - |'
- en: '| cm-SSFT^∗  [[264](#bib.bib264)]^† | CVPR20 | 47.7 | - | - | 54.1 | - | -
    | - | - | - | - |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| cm-SSFT^∗  [[264](#bib.bib264)]^† | CVPR20 | 47.7 | - | - | 54.1 | - | -
    | - | - | - | - |'
- en: '| DDAG^∗  [[192](#bib.bib192)] | ECCV20 | 54.75 | 90.39 | 95.81 | 53.02 | 39.62
    | 61.02 | 94.06 | 98.41 | 67.98 | 62.61 |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| DDAG^∗  [[192](#bib.bib192)] | ECCV20 | 54.75 | 90.39 | 95.81 | 53.02 | 39.62
    | 61.02 | 94.06 | 98.41 | 67.98 | 62.61 |'
- en: '| HAT^∗  [[188](#bib.bib188)] | TIFS20 | 55.29 | 92.14 | 97.36 | 53.89 | -
    | 62.10 | 95.75 | 99.20 | 69.37 | - |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '| HAT^∗  [[188](#bib.bib188)] | TIFS20 | 55.29 | 92.14 | 97.36 | 53.89 | -
    | 62.10 | 95.75 | 99.20 | 69.37 | - |'
- en: '| AGW | - | 47.50 | 84.39 | 92.14 | 47.65 | 35.30 | 54.17 | 91.14 | 95.98 |
    62.97 | 59.23 |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '| AGW | - | 47.50 | 84.39 | 92.14 | 47.65 | 35.30 | 54.17 | 91.14 | 95.98 |
    62.97 | 59.23 |'
- en: 'TABLE R3: Comparison with the state-of-the-arts on RegDB dataset on both query
    settings. Rank at $r$ accuracy (%), mAP (%) and mINP (%) are reported. (Both the
    visible to thermal and thermal to visible query settings are evaluated.) “*” represents
    methods published after the paper submission.'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 表 R3：在 RegDB 数据集上对比当前技术在两种查询设置下的表现。报告了 $r$ 准确率（%）、mAP（%）和 mINP（%）。（对可见到热成像和热成像到可见的查询设置进行了评估。）“*”表示论文提交后发表的方法。
- en: '| Settings | Visible to Thermal | Thermal to Visible |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | 可见到热成像 | 热成像到可见 |'
- en: '| --- | --- | --- |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Method | Venue | $r=1$ | $r=10$ | $r=20$ | mAP | mINP | $r=1$ | $r=10$ |
    $r=20$ | mAP | mINP |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| Method | Venue | $r=1$ | $r=10$ | $r=20$ | mAP | mINP | $r=1$ | $r=10$ |
    $r=20$ | mAP | mINP |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| HCML [[186](#bib.bib186)] | AAAI18 | 24.44 | 47.53 | 56.78 | 20.08 | - |
    21.70 | 45.02 | 55.58 | 22.24 | - |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| HCML [[186](#bib.bib186)] | AAAI18 | 24.44 | 47.53 | 56.78 | 20.08 | - |
    21.70 | 45.02 | 55.58 | 22.24 | - |'
- en: '| Zero-Pad [[21](#bib.bib21)] | ICCV17 | 17.75 | 34.21 | 44.35 | 18.90 | -
    | 16.63 | 34.68 | 44.25 | 17.82 | - |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| Zero-Pad [[21](#bib.bib21)] | ICCV17 | 17.75 | 34.21 | 44.35 | 18.90 | -
    | 16.63 | 34.68 | 44.25 | 17.82 | - |'
- en: '| BDTR [[142](#bib.bib142)] | IJCAI18 | 33.56 | 58.61 | 67.43 | 32.76 | - |
    32.92 | 58.46 | 68.43 | 31.96 | - |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| BDTR [[142](#bib.bib142)] | IJCAI18 | 33.56 | 58.61 | 67.43 | 32.76 | - |
    32.92 | 58.46 | 68.43 | 31.96 | - |'
- en: '| eBDTR [[142](#bib.bib142)] | TIFS19 | 34.62 | 58.96 | 68.72 | 33.46 | - |
    34.21 | 58.74 | 68.64 | 32.49 | - |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '| eBDTR [[142](#bib.bib142)] | TIFS19 | 34.62 | 58.96 | 68.72 | 33.46 | - |
    34.21 | 58.74 | 68.64 | 32.49 | - |'
- en: '| HSME [[187](#bib.bib187)] | AAAI19 | 50.85 | 73.36 | 81.66 | 47.00 | - |
    50.15 | 72.40 | 81.07 | 46.16 | - |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '| HSME [[187](#bib.bib187)] | AAAI19 | 50.85 | 73.36 | 81.66 | 47.00 | - |
    50.15 | 72.40 | 81.07 | 46.16 | - |'
- en: '| D²RL [[189](#bib.bib189)] | CVPR19 | 43.4 | 66.1 | 76.3 | 44.1 | - | - |
    - | - | - | - |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '| D²RL [[189](#bib.bib189)] | CVPR19 | 43.4 | 66.1 | 76.3 | 44.1 | - | - |
    - | - | - | - |'
- en: '| MAC [[261](#bib.bib261)] | MM19 | 36.43 | 62.36 | 71.63 | 37.03 | - | 36.20
    | 61.68 | 70.99 | 36.63 | - |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| MAC [[261](#bib.bib261)] | MM19 | 36.43 | 62.36 | 71.63 | 37.03 | - | 36.20
    | 61.68 | 70.99 | 36.63 | - |'
- en: '| MSR [[262](#bib.bib262)] | TIP19 | 48.43 | 70.32 | 79.95 | 48.67 | - | -
    | - | - | - | - |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
  zh: '| MSR [[262](#bib.bib262)] | TIP19 | 48.43 | 70.32 | 79.95 | 48.67 | - | -
    | - | - | - | - |'
- en: '| AlignGAN [[190](#bib.bib190)] | ICCV19 | 57.9 | - | - | 53.6 | - | 56.3 |
    - | - | 53.4 | - |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '| AlignGAN [[190](#bib.bib190)] | ICCV19 | 57.9 | - | - | 53.6 | - | 56.3 |
    - | - | 53.4 | - |'
- en: '| XModal^∗  [[263](#bib.bib263)] | AAAI20 | 62.21 | 83.13 | 91.72 | 60.18 |
    - | - | - | - | - | - |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '| XModal^∗  [[263](#bib.bib263)] | AAAI20 | 62.21 | 83.13 | 91.72 | 60.18 |
    - | - | - | - | - | - |'
- en: '| Hi-CMD^∗  [[191](#bib.bib191)] | CVPR20 | 70.93 | 86.39 | - | 66.04 | - |
    - | - | - | - | - |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| Hi-CMD^∗  [[191](#bib.bib191)] | CVPR20 | 70.93 | 86.39 | - | 66.04 | - |
    - | - | - | - | - |'
- en: '| cm-SSFT^∗  [[264](#bib.bib264)] | CVPR20 | 72.3 | - | - | 72.9 | - | 71.0
    | - | - | 71.7 | - |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '| cm-SSFT^∗  [[264](#bib.bib264)] | CVPR20 | 72.3 | - | - | 72.9 | - | 71.0
    | - | - | 71.7 | - |'
- en: '| DDAG^∗  [[192](#bib.bib192)] | ECCV20 | 69.34 | 86.19 | 91.49 | 63.46 | 49.24
    | 68.06 | 85.15 | 90.31 | 61.80 | 48.62 |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| DDAG^∗  [[192](#bib.bib192)] | ECCV20 | 69.34 | 86.19 | 91.49 | 63.46 | 49.24
    | 68.06 | 85.15 | 90.31 | 61.80 | 48.62 |'
- en: '| HAT^∗  [[188](#bib.bib188)] | TIFS20 | 71.83 | 87.16 | 92.16 | 67.56 | -
    | 70.02 | 86.45 | 91.61 | 66.30 | - |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
  zh: '| HAT^∗  [[188](#bib.bib188)] | TIFS20 | 71.83 | 87.16 | 92.16 | 67.56 | -
    | 70.02 | 86.45 | 91.61 | 66.30 | - |'
- en: '| AGW | - | 70.05 | 86.21 | 91.55 | 66.37 | 50.19 | 70.49 | 87.12 | 91.84 |
    65.90 | 51.24 |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '| AGW | - | 70.05 | 86.21 | 91.55 | 66.37 | 50.19 | 70.49 | 87.12 | 91.84 |
    65.90 | 51.24 |'
- en: B. Experiments on Video-based Re-ID
  id: totrans-708
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B. 基于视频的再识别的实验
- en: Implementation Details. We extend our proposed AGW baseline to a video-based
    Re-ID model by several minor changes to the backbone structure and training strategy
    of single-modality image-based Re-ID model. The video-based AGW baseline takes
    a video sequence as input and extracts the frame-level feature vectors, which
    are then averaged to be a video-level feature vector before the BNNeck layer.
    Besides, the video-based AGW baseline is trained for 400 epochs totally to better
    fit the video person Re-ID datasets. The learning rate is decayed by 10 times
    every 100 epochs. To form an input video sequence, we adopt the constraint random
    sampling strategy [[133](#bib.bib133)] to sample 4 frames as a summary for the
    original pedestrian tracklet. The BagTricks [[122](#bib.bib122)] baseline is extended
    to a video-based Re-ID model in the same way as AGW baseline for fair comparison.
    In addition, we also develop a variant of AGW baseline, termed as AGW[+], to model
    more abundant temporal information in a pedestrian tracklet. AGW[+] baseline adopts
    the dense sampling strategy to form an input video sequence in the testing stage.
    Dense sampling strategy takes all the frames in a pedestrian tracklet to form
    input video sequence, resulting better performance but higher computational cost.
    To further improve the performance of AGW[+] baseline on video re-ID datasets,
    we also remove the warm-up learning rate strategy and add dropout operation before
    the linear classification layer.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 实现细节。我们通过对单模态图像 Re-ID 模型的骨干结构和训练策略进行几个小改动，将我们提出的 AGW 基线扩展到视频基础的 Re-ID 模型。视频基础的
    AGW 基线以视频序列作为输入，提取帧级特征向量，然后在 BNNeck 层之前将这些特征向量平均为视频级特征向量。此外，视频基础的 AGW 基线总共训练 400
    个周期，以更好地适应视频行人 Re-ID 数据集。学习率每 100 个周期衰减 10 倍。为了形成输入视频序列，我们采用约束随机采样策略 [[133](#bib.bib133)]
    从原始行人轨迹中采样 4 帧作为总结。BagTricks [[122](#bib.bib122)] 基线以与 AGW 基线相同的方式扩展到视频基础的 Re-ID
    模型，以便进行公平比较。此外，我们还开发了 AGW 基线的变体，称为 AGW[+]，以建模行人轨迹中的更多时间信息。AGW[+] 基线在测试阶段采用密集采样策略来形成输入视频序列。密集采样策略将行人轨迹中的所有帧用于形成输入视频序列，虽然性能更好，但计算成本更高。为了进一步提高
    AGW[+] 基线在视频 Re-ID 数据集上的性能，我们还去除了预热学习率策略，并在线性分类层之前添加了 dropout 操作。
- en: 'Detailed Comparison. In this section, we conduct the performance comparison
    between AGW baseline and other state-of-the-art video-based person Re-ID methods,
    including ETAP [[144](#bib.bib144)], DRSA [[133](#bib.bib133)], STA [[135](#bib.bib135)]
    Snippet [[134](#bib.bib134)], VRSTC [[20](#bib.bib20)], ADFD [[110](#bib.bib110)],
    GLTR [[136](#bib.bib136)] and CoSeg [[132](#bib.bib132)]. The comparison results
    on four video person Re-ID datasets (MARS, DukeVideo, PRID2011 and iLIDS-VID)
    are listed in Table [R1](#S5.T1 "TABLE R1 ‣ A. Experiments on Single-modality
    Image-based Re-ID ‣ Deep Learning for Person Re-identification: A Survey and Outlook").
    As we can see, by simply taking video sequence as input and adopting average pooling
    to aggregate frame-level feature, our AGW baseline achieves competitive results
    on two large-scale video Re-ID dataset, MARS and DukeVideo. Besides, AGW baseline
    also performs significantly better than BagTricks [[122](#bib.bib122)] baseline
    under multiple evaluation metrics. By further modeling more temporal information
    and adjusting training strategy, AGW[+] baseline gains huge improvement and also
    achieves competitive results on both PRID2011 and iLIDS-VID datasets. AGW[+] baseline
    outperforms most state-of-the-art methods on MARS, DukeVideo and PRID2011 datasets.
    Most of these video-based person Re-ID methods achieve state-of-the-art performance
    by designing complicate temporal attention mechanism to exploit temporal dependency
    in pedestrian video. We believe that our AGW baseline can help video Re-ID model
    achieve higher performance with properly designed mechanism to further exploit
    spatial and temporal dependency.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '详细比较。在本节中，我们对 AGW 基线与其他最先进的视频基础行人重识别方法进行性能比较，包括 ETAP [[144](#bib.bib144)]、DRSA [[133](#bib.bib133)]、STA [[135](#bib.bib135)]、Snippet [[134](#bib.bib134)]、VRSTC [[20](#bib.bib20)]、ADFD [[110](#bib.bib110)]、GLTR [[136](#bib.bib136)]
    和 CoSeg [[132](#bib.bib132)]。在四个视频行人重识别数据集（MARS、DukeVideo、PRID2011 和 iLIDS-VID）上的比较结果列在表 [R1](#S5.T1
    "TABLE R1 ‣ A. Experiments on Single-modality Image-based Re-ID ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook")中。正如我们所见，通过简单地将视频序列作为输入并采用平均池化来聚合帧级特征，我们的
    AGW 基线在两个大规模视频 Re-ID 数据集 MARS 和 DukeVideo 上取得了竞争力的结果。此外，在多个评估指标下，AGW 基线的表现也显著优于
    BagTricks [[122](#bib.bib122)] 基线。通过进一步建模更多的时间信息和调整训练策略，AGW[+] 基线取得了巨大的改进，并在 PRID2011
    和 iLIDS-VID 数据集上也达到了竞争力的结果。AGW[+] 基线在 MARS、DukeVideo 和 PRID2011 数据集上超越了大多数最先进的方法。这些基于视频的行人
    Re-ID 方法通过设计复杂的时间注意机制来利用行人视频中的时间依赖性，达到了最先进的性能。我们相信，我们的 AGW 基线可以通过适当设计的机制进一步利用空间和时间依赖性，帮助视频
    Re-ID 模型实现更高的性能。'
- en: C. Experiments on Cross-modality Re-ID
  id: totrans-711
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨模态 Re-ID 的实验
- en: 'Architecture Design. We adopt a two-stream network structure as the backbone
    for cross-modality visible-infrared Re-ID⁵⁵5[https://github.com/mangye16/Cross-Modal-Re-ID-baseline](https://github.com/mangye16/Cross-Modal-Re-ID-baseline).
    Compared to the one-stream architecture in single-modality person Re-ID (Fig. [8](#S4.F8
    "Figure 8 ‣ 4.2 A New Baseline for Single-/Cross-Modality Re-ID ‣ 4 An Outlook:
    Re-ID in Next Era ‣ Deep Learning for Person Re-identification: A Survey and Outlook")),
    the major difference is that, *i.e*., the first block is specific for two modalities
    in order to capture modality-specific information, while the remaining blocks
    are shared to learn modality sharable features. Compared to the two-stream structure
    widely used in [[142](#bib.bib142), [261](#bib.bib261)], which only has one shared
    embedding layer, our design captures more sharable components. An illustration
    for cross-modality visible-infrared Re-ID is shown in Fig. [R2](#S5.F2 "Figure
    R2 ‣ A. Experiments on Single-modality Image-based Re-ID ‣ Deep Learning for Person
    Re-identification: A Survey and Outlook").'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: '架构设计。我们采用双流网络结构作为跨模态可见光-红外 Re-ID⁵⁵5[https://github.com/mangye16/Cross-Modal-Re-ID-baseline](https://github.com/mangye16/Cross-Modal-Re-ID-baseline)的骨干网。与单模态行人
    Re-ID 中的一流架构（图 [8](#S4.F8 "Figure 8 ‣ 4.2 A New Baseline for Single-/Cross-Modality
    Re-ID ‣ 4 An Outlook: Re-ID in Next Era ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")）相比，主要区别在于，即，第一个模块是针对两种模态的，以捕捉模态特定的信息，而其余模块则是共享的，用于学习模态共享特征。与
    [[142](#bib.bib142)、[261](#bib.bib261)] 中广泛使用的双流结构相比，该结构只有一个共享嵌入层，我们的设计能够捕捉更多的共享组件。跨模态可见光-红外
    Re-ID 的示意图见图 [R2](#S5.F2 "Figure R2 ‣ A. Experiments on Single-modality Image-based
    Re-ID ‣ Deep Learning for Person Re-identification: A Survey and Outlook")。'
- en: Training Strategy. At each training step, we random sample 8 identities from
    the whole dataset. Then 4 visible and 4 infrared images are randomly selected
    for each identity. Totally, each training batch contains 32 visible and 32 infrared
    images. This guarantees the informative hard triplet mining from both modalities,
    *i.e*., we directly select the hard positive and negative from both intra- and
    inter-modalities. This approximates the idea of bi-directional center-constrained
    top-ranking loss, handling the inter- and intra-modality variations simultaneously.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 训练策略。在每次训练步骤中，我们从整个数据集中随机抽取8个身份。然后，为每个身份随机选择4张可见光图像和4张红外图像。总的来说，每个训练批次包含32张可见光图像和32张红外图像。这确保了从两种模态中提取有信息的困难三元组，即我们直接从内部和跨模态中选择困难的正样本和负样本。这接近于双向中心约束顶级排名损失的思想，同时处理模态间和模态内的变化。
- en: For fair comparison, we follow the settings in [[142](#bib.bib142)] exactly
    to conduct the image processing and data augmentation. For infrared images, we
    keep the original three channels, just like the visible RGB images. All the input
    images from both modalities are first resized to $288\times 144$, and random crop
    with zero-padding together with random horizontal flipping are adopted for data
    argumentation. The cropped image sizes are $256\times 128$ for both modality.
    The image normalization are exactly following the single-modality setting.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平比较，我们完全按照[[142](#bib.bib142)]中的设置进行图像处理和数据增强。对于红外图像，我们保留原始的三个通道，就像可见光RGB图像一样。所有来自两种模态的输入图像都首先调整为$288\times
    144$，并采用随机裁剪加零填充和随机水平翻转进行数据增强。裁剪后的图像尺寸为$256\times 128$。图像标准化完全按照单模态设置进行。
- en: Training Loss. In the training stage, we combine with the identity classification
    loss ($\mathcal{L}_{id}$) and our proposed weighted regularization triplet loss
    ($\mathcal{L}_{wrt}$). The weight of combining the identity loss and weighted
    regularized triplet loss is set to 1, the same as the single-modality setting.
    The pooling parameter $p_{k}$ is set to 3\. For stable training, we adopt the
    same identity classifier for two heterogeneous modalities, mining sharable information.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失。在训练阶段，我们结合了身份分类损失（$\mathcal{L}_{id}$）和我们提出的加权正则化三元组损失（$\mathcal{L}_{wrt}$）。身份损失和加权正则化三元组损失的权重设置为1，与单模态设置相同。池化参数$p_{k}$设置为3。为了稳定训练，我们对两个异质模态采用相同的身份分类器，以挖掘共享信息。
- en: Optimizer Setting. We set the initial learning rate as 0.1 on both datasets,
    and decay it by 0.1 and 0.01 at 20 and 50 epochs, respectively. The total number
    of training epoch is 60\. We also adopt a warm-up learning rate scheme. We adopt
    the stochastic gradient descent (SGD) optimizer for optimization, and the momentum
    parameter is set to 0.9\. We have tried the same Adam optimizer (used in single-modality
    Re-ID) on cross-modality Re-ID task, but the performance is much lower than that
    of SGD optimizer by using a large learning rate. This is crucial since ImageNet
    initialization is adopted for the infrared images.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器设置。我们将初始学习率设置为0.1，并在20和50个epoch时分别将其衰减为0.1和0.01。总的训练epoch数为60。我们还采用了预热学习率方案。我们采用随机梯度下降（SGD）优化器进行优化，动量参数设置为0.9。我们尝试了在跨模态Re-ID任务中使用的相同Adam优化器（用于单模态Re-ID），但使用较大学习率时，其性能远低于SGD优化器。这一点至关重要，因为红外图像采用了ImageNet初始化。
- en: 'Detailed Comparison This section conducts the comparison with the state-of-the-art
    cross-modality VI-ReID methods, including eBDTR [[142](#bib.bib142)], HSME [[187](#bib.bib187)],
    D²RL [[189](#bib.bib189)], MAC [[261](#bib.bib261)], MSR [[262](#bib.bib262)]
    and AlignGAN [[190](#bib.bib190)]. These methods are published in the past two
    years. AlignGAN [[190](#bib.bib190)], published in ICCV 2019, achieves the state-of-the-art
    performance by aligning the cross-modality representation at both the feature
    level and pixel level with GAN generated images. The results on two datasets are
    shown in Tables [R2](#S5.T2 "TABLE R2 ‣ A. Experiments on Single-modality Image-based
    Re-ID ‣ Deep Learning for Person Re-identification: A Survey and Outlook") and
    [R3](#S5.T3 "TABLE R3 ‣ A. Experiments on Single-modality Image-based Re-ID ‣
    Deep Learning for Person Re-identification: A Survey and Outlook"). We observe
    that the proposed AGW consistently outperforms the current state-of-the-art, without
    the time-consuming image generation process. For different query settings on RegDB
    dataset, our proposed baseline generally keeps the same performance. Our proposed
    baseline has been widely used in many recently developed methods. We believe our
    new baseline will provide a good guidance to boost the cross-modality Re-ID.'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 详细比较 本节对当前最先进的跨模态 VI-ReID 方法进行了比较，包括 eBDTR [[142](#bib.bib142)]、HSME [[187](#bib.bib187)]、D²RL
    [[189](#bib.bib189)]、MAC [[261](#bib.bib261)]、MSR [[262](#bib.bib262)] 和 AlignGAN
    [[190](#bib.bib190)]。这些方法在过去两年内发表。AlignGAN [[190](#bib.bib190)]，在 ICCV 2019 上发表，通过在特征层级和像素层级对齐跨模态表示以及使用
    GAN 生成的图像，达到了最先进的性能。两个数据集上的结果显示在表 [R2](#S5.T2 "TABLE R2 ‣ A. 单模态图像重识别实验 ‣ 人员重识别的深度学习：调查与展望")
    和 [R3](#S5.T3 "TABLE R3 ‣ A. 单模态图像重识别实验 ‣ 人员重识别的深度学习：调查与展望") 中。我们观察到，所提出的 AGW
    一贯优于当前最先进的方法，无需耗时的图像生成过程。对于 RegDB 数据集上的不同查询设置，我们提出的基线模型通常保持相同的性能。我们提出的基线模型已在许多近期开发的方法中被广泛使用。我们相信我们的新基线将提供有力的指导，以提升跨模态重识别的效果。
- en: '![Refer to caption](img/0b8c36b5699f9bc3f54f2f312750be91.png)'
  id: totrans-718
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0b8c36b5699f9bc3f54f2f312750be91.png)'
- en: 'Figure R3: An overview of this survey. It contains three main components, including
    Closed-World Setting in Section [2](#S2 "2 Closed-world Person Re-Identification
    ‣ Deep Learning for Person Re-identification: A Survey and Outlook"), Open-World
    Setting in Section [3](#S3 "3 Open-world Person Re-Identification ‣ Deep Learning
    for Person Re-identification: A Survey and Outlook") and an outlook of Re-ID in
    Next Era in Section [4](#S4 "4 An Outlook: Re-ID in Next Era ‣ Deep Learning for
    Person Re-identification: A Survey and Outlook").'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 图 R3：本次调查的概述。它包含三个主要部分，包括[2](#S2 "2 关闭世界中的人员重识别 ‣ 人员重识别的深度学习：调查与展望")节中的闭合世界设置，[3](#S3
    "3 开放世界中的人员重识别 ‣ 人员重识别的深度学习：调查与展望")节中的开放世界设置以及[4](#S4 "4 展望：下一个时代的重识别 ‣ 人员重识别的深度学习：调查与展望")节中的下一个时代的重识别展望。
- en: D. Experiments on Partial Re-ID
  id: totrans-720
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D. 部分重识别实验
- en: Implementation Details. We also evaluate the performance of our proposed AGW
    baseline on two commonly-used partial Re-ID datasets, Partial-REID and Partial-iLIDS.
    The overall backbone structure and training strategy for partial Re-ID AGW baseline
    model are the same as the one for single-modality image-based Re-ID model. Both
    Partial-REID and Partial-iLIDS datasets offer only query image set and gallery
    image set. So we train AGW baseline model on the training set of Market-1501 dataset,
    then evaluate its performance on the testing set of two partial Re-ID datasets.
    We adopt the same way to evaluate the performance of BagTricks [[122](#bib.bib122)]
    baseline on these two partial Re-ID datasets for better comparison and analysis.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节 我们还评估了所提出的 AGW 基线在两个常用的部分重识别数据集（Partial-REID 和 Partial-iLIDS）上的表现。部分重识别
    AGW 基线模型的整体骨干结构和训练策略与单模态图像重识别模型相同。Partial-REID 和 Partial-iLIDS 数据集仅提供查询图像集和图库图像集。因此，我们在
    Market-1501 数据集的训练集上训练 AGW 基线模型，然后在两个部分重识别数据集的测试集上评估其性能。我们采用相同的方法来评估 BagTricks
    [[122](#bib.bib122)] 基线在这两个部分重识别数据集上的表现，以便进行更好的比较和分析。
- en: 'Detailed Comparison. We compare the performance of AGW baseline with other
    state-of-the-art partial Re-ID methods, including DSR [[232](#bib.bib232)], SFR
    [[249](#bib.bib249)] and VPM [[67](#bib.bib67)]. All these methods are published
    in recent years. The comparison results on both Partial-REID and Partial-iLIDS
    datasets are shown in Table [R4](#S5.T4 "TABLE R4 ‣ D. Experiments on Partial
    Re-ID ‣ Deep Learning for Person Re-identification: A Survey and Outlook"). The
    VPM [[67](#bib.bib67)] achieves a very high performance by perceiving the visibility
    of regions through self-supervision and extracting region-level features. Considering
    only global features, our proposed AGW baseline still achieves competitive results
    compared to the current state-of-the-arts on both datasets. Besides, AGW baseline
    brings significant improvement comparing to BagTricks [[122](#bib.bib122)] under
    multiple evaluation metrics, demonstrating its effectiveness for partial Re-ID
    problem.'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '详细比较。我们将AGW基线与其他先进的部分Re-ID方法进行比较，包括DSR [[232](#bib.bib232)]、SFR [[249](#bib.bib249)]和VPM
    [[67](#bib.bib67)]。所有这些方法都是近年来发布的。部分REID和部分iLIDS数据集上的比较结果如表[R4](#S5.T4 "TABLE
    R4 ‣ D. Experiments on Partial Re-ID ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook")所示。VPM [[67](#bib.bib67)]通过自我监督感知区域的可见性并提取区域级特征，从而取得了非常高的性能。考虑到仅全局特征，我们提出的AGW基线在两个数据集上与当前先进技术相比仍然取得了具有竞争力的结果。此外，AGW基线在多个评估指标下相比于BagTricks
    [[122](#bib.bib122)]带来了显著的提升，证明了其在部分Re-ID问题上的有效性。'
- en: 'TABLE R4: Comparison with the state-of-the-arts on two partial Re-ID datasets,
    including Partial-REID and Partial-iLIDS. Rank-1, -3 accuracy (%) and mINP (%)
    are reported.'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 表R4：与两个部分Re-ID数据集（包括Partial-REID和Partial-iLIDS）上的先进技术进行比较。报告了Rank-1、-3准确率（%）和mINP（%）。
- en: '| Method | Partial-REID | Partial-iLIDS |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Partial-REID | Partial-iLIDS |'
- en: '| --- | --- | --- |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| R1 | R3 | mINP | R1 | R3 | mINP |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
  zh: '| R1 | R3 | mINP | R1 | R3 | mINP |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| DSR [[232](#bib.bib232)]  CVPR18 | 50.7 | 70.0 | - | 58.8 | 67.2 | - |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
  zh: '| DSR [[232](#bib.bib232)]  CVPR18 | 50.7 | 70.0 | - | 58.8 | 67.2 | - |'
- en: '| SFR [[249](#bib.bib249)]  ArXiv18 | 56.9 | 78.5 | - | 63.9 | 74.8 | - |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
  zh: '| SFR [[249](#bib.bib249)]  ArXiv18 | 56.9 | 78.5 | - | 63.9 | 74.8 | - |'
- en: '| VPM [[67](#bib.bib67)]  CVPR19 | 67.7 | 81.9 | - | 67.2 | 76.5 | - |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
  zh: '| VPM [[67](#bib.bib67)]  CVPR19 | 67.7 | 81.9 | - | 67.2 | 76.5 | - |'
- en: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 62.0 | 74.0 | 45.4 | 58.8 | 73.9
    | 68.7 |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '| BagTricks [[122](#bib.bib122)]  CVPR19W | 62.0 | 74.0 | 45.4 | 58.8 | 73.9
    | 68.7 |'
- en: '| AGW | 69.7 | 80.0 | 56.7 | 64.7 | 79.8 | 73.3 |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
  zh: '| AGW | 69.7 | 80.0 | 56.7 | 64.7 | 79.8 | 73.3 |'
- en: E. Overview of This Survey
  id: totrans-733
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E. 本调查的概述
- en: 'The overview figure of this survey is shown in Fig. [R3](#S5.F3 "Figure R3
    ‣ C. Experiments on Cross-modality Re-ID ‣ Deep Learning for Person Re-identification:
    A Survey and Outlook"). According to the five steps in developing a person Re-ID
    system, we conduct the survey from both closed-world and open-world settings.
    The closed-world setting is detailed in three different aspects: feature representation
    learning, deep metric learning and ranking optimization. We then summarize the
    datasets and SOTAs from both image- and video-based perspectives. For open-world
    person Re-ID, we summarize it into five aspects: including heterogeneous data,
    Re-ID from raw images/videos, unavailable/limited labels, noisy annotation and
    open-set Re-ID.'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查的概述图如图[R3](#S5.F3 "Figure R3 ‣ C. Experiments on Cross-modality Re-ID ‣
    Deep Learning for Person Re-identification: A Survey and Outlook")所示。根据开发人员Re-ID系统的五个步骤，我们从封闭世界和开放世界的设置进行调查。封闭世界设置在三个不同的方面进行了详细描述：特征表示学习、深度度量学习和排名优化。然后，我们从图像和视频两个角度总结数据集和先进技术。在开放世界的人物Re-ID方面，我们将其总结为五个方面：包括异质数据、从原始图像/视频中进行Re-ID、不可用/有限标签、噪声标注和开放集Re-ID。'
- en: Following the summary, we present an outlook for future person Re-ID. We design
    a new evaluation metric (mINP) to evaluate the difficulty to find all the correct
    matches. By analyzing the advantages of existing Re-ID methods, we develop a strong
    AGW baseline for future developments, which achieves competitive performance on
    four Re-ID tasks. Finally, some under-investigated open issues are discussed.
    Our survey provides a comprehensive summarization of existing state-of-the-art
    in different sub-tasks. Meanwhile, the analysis of future directions is also presented
    for further development guidance.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结之后，我们展望了未来的人脸重新识别（Re-ID）方向。我们设计了一种新的评估指标（mINP），用于评估找到所有正确匹配的难度。通过分析现有Re-ID方法的优点，我们为未来的发展制定了一个强大的AGW基线，在四个Re-ID任务上取得了竞争性的表现。最后，我们讨论了一些尚未充分研究的开放问题。我们的调查提供了对不同子任务现有最先进技术的全面总结。同时，也提出了未来方向的分析，以便进一步发展指导。
- en: Acknowledgement. The authors would like to thank the anonymous reviewers for
    providing valuable feedbacks to improve the quality of this survey. The authors
    also would like to thank the pioneer researchers in person re-identification and
    other related fields. This work is sponsored by CAAI-Huawei MindSpore Open Fund.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。作者感谢匿名审稿人提供宝贵的反馈，以提高本调查的质量。作者还要感谢在人脸重新识别及其他相关领域的开创性研究者。此项工作由CAA-Huawei MindSpore开放基金赞助。
