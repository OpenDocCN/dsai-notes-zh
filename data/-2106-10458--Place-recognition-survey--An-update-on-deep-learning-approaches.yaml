- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:53:56'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:53:56'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2106.10458] Place recognition survey: An update on deep learning approaches'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2106.10458] 地点识别调查：深度学习方法的更新'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.10458](https://ar5iv.labs.arxiv.org/html/2106.10458)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2106.10458](https://ar5iv.labs.arxiv.org/html/2106.10458)
- en: 'Place recognition survey: An update on deep learning approaches'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 地点识别调查：深度学习方法的更新
- en: Tiago Barros, Ricardo Pereira, Luís Garrote, Cristiano Premebida, Urbano J.
    Nunes The authors are with the University of Coimbra, Institute of Systems and
    Robotics, Department of Electrical and Computer Engineering, Portugal. E-mail:{tiagobarros, ricardo.pereira, garrote,
     cpremebida, urbano}@isr.uc.pt
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Tiago Barros, Ricardo Pereira, Luís Garrote, Cristiano Premebida, Urbano J.
    Nunes 作者来自葡萄牙科英布拉大学系统与机器人研究所电气与计算机工程系。电子邮件：{tiagobarros, ricardo.pereira, garrote,
    cpremebida, urbano}@isr.uc.pt
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Autonomous Vehicles (AV) are becoming more capable of navigating in complex
    environments with dynamic and changing conditions. A key component that enables
    these intelligent vehicles to overcome such conditions and become more autonomous
    is the sophistication of the perception and localization systems. As part of the
    localization system, place recognition has benefited from recent developments
    in other perception tasks such as place categorization or object recognition,
    namely with the emergence of deep learning (DL) frameworks. This paper surveys
    recent approaches and methods used in place recognition, particularly those based
    on deep learning. The contributions of this work are twofold: surveying recent
    sensors such as 3D LiDARs and RADARs, applied in place recognition; and categorizing
    the various DL-based place recognition works into supervised, unsupervised, semi-supervised,
    parallel, and hierarchical categories. First, this survey introduces key place
    recognition concepts to contextualize the reader. Then, sensor characteristics
    are addressed. This survey proceeds by elaborating on the various DL-based works,
    presenting summaries for each framework. Some lessons learned from this survey
    include: the importance of NetVLAD for supervised end-to-end learning; the advantages
    of unsupervised approaches in place recognition, namely for cross-domain applications;
    or the increasing tendency of recent works to seek, not only for higher performance
    but also for higher efficiency.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自主车辆（AV）正变得越来越能够在复杂的环境中导航，这些环境具有动态和变化的条件。使这些智能车辆克服这些条件并变得更加自主的关键组件是感知和定位系统的复杂性。作为定位系统的一部分，地点识别受益于其他感知任务的最新发展，如地点分类或对象识别，尤其是深度学习（DL）框架的出现。本文回顾了最近在地点识别中使用的方法，特别是基于深度学习的方法。这项工作的贡献有两个方面：调查最近应用于地点识别的传感器，如3D
    LiDAR和雷达；以及将各种基于DL的地点识别工作分类为监督、无监督、半监督、并行和层次类别。首先，本调查介绍了关键的地点识别概念，以便让读者理解背景。接着，讨论了传感器特性。本调查进一步详细说明了各种基于DL的工作，为每个框架提供总结。从这项调查中得到的一些经验教训包括：NetVLAD在监督端到端学习中的重要性；无监督方法在地点识别中的优势，特别是在跨域应用中的优势；以及最近的研究趋向于寻求不仅是更高的性能，还有更高的效率。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Place recognition, Deep Learning, Localization.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 地点识别，深度学习，定位。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Self-driving vehicles are increasingly able to deal with unstructured and dynamic
    environments, which is mainly due to the development of more robust long-term
    localization and perception systems. A critical aspect of long-term localization
    is to guarantee coherent mapping and bounded error over time, which is achieved
    by finding loops in revisited areas. Revisited places are detected in long-term
    localization systems by resorting to approaches such as place recognition and
    loop closure. Namely, place recognition is a perception based approach that recognizes
    previously visited places based on visual, structural, or semantic cues.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自驾车越来越能够处理无结构和动态环境，这主要得益于更强大的长期定位和感知系统的发展。长期定位的一个关键方面是保证映射的一致性和误差的有限性，这通过在重新访问的区域中找到环路来实现。通过采用地点识别和环闭等方法来检测重新访问的地方。具体来说，地点识别是一种基于感知的方法，通过视觉、结构或语义线索识别以前访问过的地方。
- en: 'Place recognition has been the focus of much research over the last decade.
    The efforts of the intelligent vehicle and machine vision communities, including
    those devoted to place recognition, resulted in great achievements, namely evolving
    towards systems that achieve promising performances in appearance changing and
    extreme viewpoint variation conditions. Despite the recent achievements, the fundamental
    challenges remain unsolved, which occur when:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 地点识别在过去十年中一直是研究的重点。智能车辆和机器视觉领域的努力，包括致力于地点识别的工作，取得了显著成就，即在外观变化和极端视角变化条件下实现了有前景的性能。尽管取得了这些成就，但仍存在一些基本挑战，当：
- en: $-$
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $-$
- en: two distinct places look similar (also known as perceptual aliasing);
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个不同的地方看起来相似（也称为感知混淆）；
- en: $-$
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $-$
- en: 'the same places exhibit significant appearance changes over time due to day-night
    variation, weather, seasonal or structural changes (as shown in Fig. [2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ Place recognition survey: An update on deep learning
    approaches"));'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '相同的地点由于昼夜变化、天气、季节性或结构变化而显示出显著的外观变化（如图[2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ Place recognition survey: An update on deep learning approaches")所示）；'
- en: $-$
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $-$
- en: same places are perceived from different viewpoints or positions.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相同的地点从不同的视角或位置被感知。
- en: Solving these challenges is essential to enable robust place recognition and
    consequently long-term localization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些挑战对于实现可靠的地点识别和因此的长期定位至关重要。
- en: '![Refer to caption](img/edeb6eddb92ccffe5cb573f85528d169.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/edeb6eddb92ccffe5cb573f85528d169.png)'
- en: 'Figure 1: Generic place recognition pipeline with the following modules: place
    modeling, belief generation and place mapping. Place modeling creates an internal
    place representation. Place mapping is concerned with maintaining a coherent representation
    of places over time. And Belief generation, finally, generates, based on the current
    place model and the map, loop candidates.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：通用地点识别流程，包括以下模块：地点建模、信念生成和地点映射。地点建模创建内部地点表示。地点映射关注于保持随时间一致的地点表示。信念生成最终基于当前的地点模型和地图生成循环候选。
- en: '![Refer to caption](img/4bdeb2663a9338218cc146dd5fcb6e0b.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4bdeb2663a9338218cc146dd5fcb6e0b.png)'
- en: 'Figure 2: Illustration of the seasonal environment changes. Images taken from
    the Oxford Robotcar [[1](#bib.bib1)] and Nordland dataset [[2](#bib.bib2)].'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：季节性环境变化的示意图。图像来自牛津机器人车[[1](#bib.bib1)]和Nordland数据集[[2](#bib.bib2)]。
- en: The primary motivation for writing this survey paper is to provide an updated
    review of the recent place recognition approaches and methods since the publication
    of previous surveys [[3](#bib.bib3), [4](#bib.bib4)]. The goal is, in particular,
    to focus on the works that are based on deep-learning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 撰写此综述论文的主要动机是提供自先前综述发布以来的最新地点识别方法的更新回顾[[3](#bib.bib3), [4](#bib.bib4)]。目标特别是关注基于深度学习的工作。
- en: Lowry et al.[[3](#bib.bib3)] presented a comprehensive overview of the existing
    visual place recognition methods up to 2016\. The work summarizes and discusses
    several fundamentals to deal with appearance changing environments and viewpoint
    variations. However, the rapid developments in deep learning (DL) and new sensor
    modalities (e.g., 3D LiDARs and RADARs) are setting unprecedented performances,
    shifting the place recognition state of the art from traditional (handcrafted-only)
    feature extraction towards data-driven methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Lowry 等人[[3](#bib.bib3)] 提供了截至2016年的现有视觉地点识别方法的全面概述。这项工作总结并讨论了处理外观变化环境和视角变化的若干基本问题。然而，深度学习（DL）和新传感器模式（例如
    3D LiDAR 和 RADAR）的快速发展正设立了前所未有的性能，将地点识别的最新技术从传统（仅手工制作）特征提取转向数据驱动方法。
- en: A key advantage of these data-driven approaches is the end-to-end training,
    which enables to learn a task directly from the sensory data without requiring
    domain knowledge for feature extraction. Instead, features are learning during
    training, using Convolutional Neural Networks (CNNs). These feature extraction
    approaches have been ultimately the driving force that has inspired recent works
    to use supervised, unsupervised, or both learning approaches combined (semi-supervised)
    to improve performance. The influence of DL frameworks in place recognition is,
    in particular, observable when considering the vast amount of place recognition
    works published in the last few years that resort to such methods.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据驱动方法的一个关键优点是端到端训练，这使得可以直接从感官数据中学习任务，而不需要领域知识来提取特征。相反，特征在训练过程中通过卷积神经网络（CNNs）进行学习。这些特征提取方法最终成为了激励近期研究使用监督、非监督或两者结合（半监督）学习方法来提高性能的驱动力。在考虑到最近几年发布的大量采用这些方法的位置识别工作时，深度学习框架的影响尤其明显。
- en: On the other hand, a disadvantage of DL methods is the requirement of a vast
    amount of training data. This requirement is in particular critical since the
    creation of suitable datasets is a demanding and expensive process. In this regard,
    place recognition has benefited considerably from the availability of autonomous
    vehicle datasets, which are becoming more and more realistic. Besides more realistic
    real-world conditions, also data from new sensor modalities are becoming available,
    for example, new camera types, 3D LiDARs, and, more recently, RADARs [[5](#bib.bib5),
    [6](#bib.bib6)]. This work does not address datasets since this topic is already
    overviewed in other works such as in [[7](#bib.bib7)] what place recognition concerns,
    and in [[8](#bib.bib8)] broader autonomous driving datasets.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，深度学习方法的一个缺点是对大量训练数据的需求。这个需求尤为关键，因为创建合适的数据集是一个繁重且昂贵的过程。在这方面，位置识别从自主驾驶车辆数据集的可用性中受益匪浅，这些数据集变得越来越真实。除了更真实的现实世界条件外，新的传感器类型的数据也变得可用，例如，新的摄像机类型、3D激光雷达以及最近的雷达[[5](#bib.bib5),
    [6](#bib.bib6)]。本工作不涉及数据集，因为这一主题已经在其他研究中概述，例如[[7](#bib.bib7)]关于位置识别的研究和[[8](#bib.bib8)]更广泛的自主驾驶数据集。
- en: 'The contribution of this work is to provide a comprehensive review of the recent
    methods and approaches, focusing in particular on:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的贡献是提供对近期方法和方法的全面回顾，特别关注于：
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'the recent introduced sensors in the context of place recognition, an outline
    of advantages and disadvantages is presented in Table [I](#S3.T1 "TABLE I ‣ III
    Sensors ‣ Place recognition survey: An update on deep learning approaches") and
    outline is illustrated in Fig. [4](#S3.F4 "Figure 4 ‣ III Sensors ‣ Place recognition
    survey: An update on deep learning approaches");'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '最近引入的用于位置识别的传感器的优缺点概述见表格[Ⅰ](#S3.T1 "TABLE I ‣ III Sensors ‣ Place recognition
    survey: An update on deep learning approaches")，概述图示见图[4](#S3.F4 "Figure 4 ‣ III
    Sensors ‣ Place recognition survey: An update on deep learning approaches")；'
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'the categorization of the various DL-based works into supervised, unsupervised,
    semi-supervised and other frameworks (as illustrated in Fig. [3](#S1.F3 "Figure
    3 ‣ I Introduction ‣ Place recognition survey: An update on deep learning approaches")),
    in order to provide to the reader a more comprehensive and meaningful understanding
    of this topic.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '将各种基于深度学习的工作分类为监督、非监督、半监督和其他框架（如图[3](#S1.F3 "Figure 3 ‣ I Introduction ‣ Place
    recognition survey: An update on deep learning approaches")所示），以便为读者提供更全面和有意义的理解。'
- en: 'The remainder of this paper is organized as follows. Section [II](#S2 "II Key
    concepts of place recognition ‣ Place recognition survey: An update on deep learning
    approaches") is dedicated to the key concepts regarding place recognition. Section
    [III](#S3 "III Sensors ‣ Place recognition survey: An update on deep learning
    approaches") addresses the supervised place recognition approaches, which include
    pre-trained and end-to-end frameworks. Section [V](#S5 "V Unsupervised Place Recognition
    ‣ Place recognition survey: An update on deep learning approaches") addresses
    the unsupervised place recognition approaches. Section [VI](#S6 "VI Semi-supervised
    Place Recognition ‣ Place recognition survey: An update on deep learning approaches")
    addresses approaches that combine both supervised and unsupervised. Section [VII](#S7
    "VII Other Frameworks ‣ Place recognition survey: An update on deep learning approaches")
    addresses alternative frameworks that resort to parallel and hierarchical architectures.
    Lastly, Section [VIII](#S8 "VIII Conclusion and discussion ‣ Place recognition
    survey: An update on deep learning approaches") concludes the paper.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分组织如下。第[II](#S2 "II Key concepts of place recognition ‣ Place recognition
    survey: An update on deep learning approaches")节专注于地点识别的关键概念。第[III](#S3 "III Sensors
    ‣ Place recognition survey: An update on deep learning approaches")节讨论了监督地点识别方法，包括预训练和端到端框架。第[V](#S5
    "V Unsupervised Place Recognition ‣ Place recognition survey: An update on deep
    learning approaches")节讨论了无监督地点识别方法。第[VI](#S6 "VI Semi-supervised Place Recognition
    ‣ Place recognition survey: An update on deep learning approaches")节讨论了结合监督和无监督的混合方法。第[VII](#S7
    "VII Other Frameworks ‣ Place recognition survey: An update on deep learning approaches")节讨论了利用并行和层次结构的替代框架。最后，第[VIII](#S8
    "VIII Conclusion and discussion ‣ Place recognition survey: An update on deep
    learning approaches")节总结了本文。'
- en: '![Refer to caption](img/2701af03c781208c1c1a82a96a614651.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2701af03c781208c1c1a82a96a614651.png)'
- en: 'Figure 3: A taxonomy of recent DL-based place recognition approaches.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：最近基于深度学习的地点识别方法的分类。
- en: II Key concepts of place recognition
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 地点识别的关键概念
- en: This section introduces the fundamentals and key concepts of place recognition.
    Most of the concepts here discussed have been already presented in [[3](#bib.bib3)][[9](#bib.bib9)][[10](#bib.bib10)][[11](#bib.bib11)],
    but they are concisely revisited in this section to contextualize the reader and
    thus facilitate the reading process.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了地点识别的基本概念和关键要素。这里讨论的大多数概念已经在[[3](#bib.bib3)][[9](#bib.bib9)][[10](#bib.bib10)][[11](#bib.bib11)]中介绍过，但在本节中简明回顾，以帮助读者理解并便于阅读。
- en: Thus, before diving into more details, some fundamental questions have to be
    addressed beforehand. What is a ‘place’ in the place recognition context? How
    are places recognized and remembered? Moreover, what are the difficulties/challenges
    when places change over time?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在深入细节之前，必须首先解决一些基本问题。在地点识别的背景下，什么是“地点”？地点是如何被识别和记忆的？此外，当地点随着时间变化时，面临哪些困难/挑战？
- en: II-A What is a place?
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 什么是地点？
- en: 'Places are segments of the physical world that can have any given scale - at
    the limit, a place may represent a single location to an entire region of discrete
    locations [[3](#bib.bib3)] (see examples in Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ Place recognition survey: An update on deep learning approaches")). The segments’
    physical bounds can be defined, resorting to different segmentation criteria:
    time step, traveled distance, or appearance. In particular, the appearance criterion
    is widely used in place recognition. In such a case, a new place is created whenever
    the appearance of the current location differs significantly from locations that
    were previously observed [[3](#bib.bib3)].'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '地点是物理世界中的片段，可以具有任意尺度——在极限情况下，一个地点可能代表一个单一的位置到整个离散位置的区域[[3](#bib.bib3)]（参见图[2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ Place recognition survey: An update on deep learning
    approaches)中的示例）。这些片段的物理边界可以通过不同的分割标准来定义：时间步长、行进距离或外观。特别是，外观标准在地点识别中被广泛使用。在这种情况下，每当当前位置的外观与之前观察到的位置有显著不同时，就会创建一个新地点[[3](#bib.bib3)]。'
- en: II-B How are places recognized and remembered?
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 地点如何被识别和记忆？
- en: 'Place recognition is the process of recognizing places within a global map,
    utilizing cues from surrounding environments. This process is typically divided
    into three modules (as illustrated in Fig.[1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Place recognition survey: An update on deep learning approaches"): place modeling,
    belief generation, and place mapping.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '地点识别是利用来自周围环境的线索，在全球地图中识别地点的过程。这个过程通常分为三个模块（如图 [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Place recognition survey: An update on deep learning approaches") 所示）：地点建模、信念生成和地点映射。'
- en: II-B1 Place Modeling
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 地点建模
- en: Place modeling is the module that maps the data from a sensor space into a descriptor
    space. Sensory data from cameras, 3D LiDARs [[12](#bib.bib12), [13](#bib.bib13)]
    or RADARs [[14](#bib.bib14)] are used to model the surrounding environment, which
    is achieved by extracting meaningful features.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 地点建模是将传感器空间中的数据映射到描述符空间的模块。来自相机、3D LiDAR [[12](#bib.bib12), [13](#bib.bib13)]
    或雷达 [[14](#bib.bib14)] 的感测数据用于建模周围环境，这通过提取有意义的特征来实现。
- en: Feature extraction approaches have evolved immensely over the last decade. Classical
    approaches rely on handcrafted descriptors such as SWIFT[[15](#bib.bib15)], SURF[[16](#bib.bib16)],
    Multiscale Superpixel Grids [[17](#bib.bib17)], HOG[[18](#bib.bib18)] or bag-of-words[[19](#bib.bib19)],
    which are mainly build based on the knowledge of domain experts (see [[3](#bib.bib3)]
    for further understanding). On the other hand, DL-based techniques, namely CNNs,
    are optimized to learn the best features for a given task [[20](#bib.bib20)].
    With the increasing dominance of DL in the various perception tasks, also place
    recognition slowly benefited from these techniques, using initially pre-trained
    models from other tasks (e.g., object recognition [[21](#bib.bib21)] or place
    categorization [[22](#bib.bib22), [23](#bib.bib23)]), and more recently using
    end-to-end learning techniques trained directly on place recognition tasks [[24](#bib.bib24),
    [25](#bib.bib25)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取方法在过去十年中取得了巨大进步。经典方法依赖于手工制作的描述符，如 SWIFT [[15](#bib.bib15)]、SURF [[16](#bib.bib16)]、多尺度超像素网格
    [[17](#bib.bib17)]、HOG [[18](#bib.bib18)] 或词袋模型 [[19](#bib.bib19)]，这些方法主要基于领域专家的知识（参见
    [[3](#bib.bib3)] 以了解更多）。另一方面，基于 DL 的技术，即 CNN，经过优化以学习特定任务的最佳特征 [[20](#bib.bib20)]。随着
    DL 在各种感知任务中的主导地位，地点识别也逐渐从这些技术中受益，最初使用从其他任务（例如对象识别 [[21](#bib.bib21)] 或地点分类 [[22](#bib.bib22),
    [23](#bib.bib23)]）中预训练的模型，最近则使用直接在地点识别任务上训练的端到端学习技术 [[24](#bib.bib24), [25](#bib.bib25)]。
- en: II-B2 Place Mapping
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 地点映射
- en: 'Place mapping refers to the process of maintaining a faithful representation
    of the physical world. To this end, place recognition approaches rely on various
    mapping frameworks and map update mechanisms. Regarding the mapping frameworks,
    three main approaches are highlighted: database [[26](#bib.bib26), [27](#bib.bib27)],
    topological [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)] or topological-metric
    [[31](#bib.bib31), [32](#bib.bib32)].'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 地点映射指的是保持对物理世界的真实表示的过程。为此，地点识别方法依赖于各种映射框架和地图更新机制。关于映射框架，主要有三种方法：数据库 [[26](#bib.bib26),
    [27](#bib.bib27)]、拓扑 [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)] 或拓扑-度量
    [[31](#bib.bib31), [32](#bib.bib32)]。
- en: Database frameworks are abstract map structures, which store arbitrary amounts
    of data without any relation between them. These frameworks are mainly used in
    pure retrieval tasks and resort, for retrieval efficiency, to k-dimensional [[33](#bib.bib33),
    [34](#bib.bib34)], Chow Liu trees [[35](#bib.bib35)] or Hierarchical Navigable
    Small World (NSW) [[36](#bib.bib36)] to accelerate nearest neighbor search.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库框架是抽象的地图结构，用于存储任意数量的数据，但这些数据之间没有任何关系。这些框架主要用于纯检索任务，并通过 k 维 [[33](#bib.bib33),
    [34](#bib.bib34)]、Chow Liu 树 [[35](#bib.bib35)] 或分层可导航小世界 (NSW) [[36](#bib.bib36)]
    来加速最近邻搜索，从而提高检索效率。
- en: Topological(-metric) maps, on the other hand, are graph-based frameworks, which
    represent the map through nodes and edges. The nodes represent places in the physical
    world, while the edges represent the relationships among the nodes (e.g., the
    similarity between two nodes). A node may represent one, or several locations,
    defining in the latter case a region in the physical world. The topological-metric
    map differs from pure topological in respect of how nodes relate i.e., while in
    pure topological maps no metric information is used in the edges; in topological-metric
    maps, nodes may relate with other nodes through relative position, orientation,
    or metric distance [[3](#bib.bib3)]. An example of such a mapping approach is
    the HTMap approach [[37](#bib.bib37)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑（-度量）地图则是基于图的框架，通过节点和边表示地图。节点代表物理世界中的位置，而边代表节点之间的关系（例如，两个节点之间的相似性）。一个节点可以表示一个或多个位置，在后一种情况下定义了物理世界中的一个区域。拓扑-度量地图与纯拓扑地图的不同在于节点的关系，即，纯拓扑地图中边不使用度量信息；而拓扑-度量地图中，节点可以通过相对位置、方向或度量距离
    [[3](#bib.bib3)] 与其他节点相关联。HTMap 方法 [[37](#bib.bib37)] 是这种映射方法的一个示例。
- en: Regarding map updating, database frameworks usually are not updated during operation
    time, while topological frameworks can be updated. Update strategies include simple
    methods, which update nodes as loops occur[[28](#bib.bib28)], or more sophisticated
    ones, where long- short-term memory-based methods are used [[38](#bib.bib38)].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 关于地图更新，数据库框架通常在操作过程中不会更新，而拓扑框架可以更新。更新策略包括简单的方法，这些方法在环路发生时更新节点 [[28](#bib.bib28)]，或更复杂的方法，使用基于长短期记忆的方法
    [[38](#bib.bib38)]。
- en: II-B3 Belief Generation
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 信念生成
- en: The belief generation module refers to the process of generating a belief distribution,
    which represents the likelihood or confidence of the input data matching a place
    in the map. This module is thus responsible to generate loop candidates based
    on the belief scores, which can be computed using methods based on frame-to-frame
    [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)], sequence of frames[[42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [14](#bib.bib14)], hierarchical, graphs[[45](#bib.bib45)]
    or probabilistics[[46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 信念生成模块指的是生成信念分布的过程，该分布表示输入数据匹配地图中某个位置的可能性或信心。因此，该模块负责基于信念分数生成环路候选，这些分数可以通过基于帧对帧
    [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]、帧序列 [[42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [14](#bib.bib14)]、层次结构、图 [[45](#bib.bib45)]
    或概率 [[46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48)] 的方法进行计算。
- en: The frame-to-frame matching approach is the most common in place recognition.
    This approach usually computes the belief distribution by matching only one frame
    at the time; and uses KD trees [[33](#bib.bib33), [34](#bib.bib34)] or Chow Liu
    trees [[35](#bib.bib35)] for nearest neighbor search, and cosine [[41](#bib.bib41)],
    Euclidean distance [[49](#bib.bib49)], Hamming distance [[50](#bib.bib50)] to
    compute the similarity score.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 帧对帧匹配方法是位置识别中最常见的方法。这种方法通常通过逐帧匹配来计算信念分布，并使用 KD 树 [[33](#bib.bib33), [34](#bib.bib34)]
    或 Chow Liu 树 [[35](#bib.bib35)] 进行最近邻搜索，使用余弦 [[41](#bib.bib41)]、欧几里得距离 [[49](#bib.bib49)]、汉明距离
    [[50](#bib.bib50)] 来计算相似性得分。
- en: On the other hand, sequence-based approaches compute the scores based on sequences
    of consecutive frames, using, for example, cost flow minimization [[51](#bib.bib51)]
    to find matches in the similarity matrix. Sequence matching is also implementable
    in a probabilistic framework using Hidden Markov Models [[47](#bib.bib47)] or
    Conditional Random Fields [[48](#bib.bib48)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于序列的方法根据连续帧的序列计算得分，例如使用成本流最小化 [[51](#bib.bib51)] 在相似性矩阵中寻找匹配。序列匹配也可以在概率框架中实现，使用隐马尔可夫模型
    [[47](#bib.bib47)] 或条件随机场 [[48](#bib.bib48)]。
- en: Hierarchical methods combine multiple matching approaches in a single place
    recognition framework. For example, the coarse-to-fine architecture [[52](#bib.bib52),
    [27](#bib.bib27)] selects top candidates in a coarse tier, and from those, selects
    the best match in a fine tier.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 层次方法将多种匹配方法结合在一个位置识别框架中。例如，粗到细架构 [[52](#bib.bib52), [27](#bib.bib27)] 在粗层中选择前几个候选者，然后从中选择最佳匹配。
- en: II-C What are the major challenges?
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 主要挑战是什么？
- en: Place recognition approaches are becoming more and more sophisticated as the
    environment and operation conditions become more similar to real-world situations.
    An example of this is the current state-of-the-art of place recognition approaches,
    which can operate over extended areas in real-world conditions with unprecedented
    performances. Despite these achievements, the major place recognition challenges
    remain unsolved, namely places with similar appearances; places that change in
    appearance over time; places that are perceived from different viewpoints; and
    scalability of the proposed approaches in large environments.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随着环境和操作条件越来越接近现实世界情况，地点识别方法也变得越来越复杂。当前的尖端地点识别方法就是一个例子，它们能够在现实世界条件下覆盖广泛区域，表现出前所未有的性能。尽管取得了这些成就，主要的地点识别挑战仍未解决，即：具有相似外观的地点；随着时间变化而改变外观的地点；从不同视角感知的地点；以及在大型环境中提出的方法的可扩展性。
- en: II-C1 Appearance Change and Perceptual Aliasing
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 外观变化与感知混淆
- en: Appearance-changing environments and perceptual aliasing have been in particular
    the focus of much research. As autonomous vehicles operate over extended periods,
    their perception systems have to deal with environments that change over time
    due to for example different weather or seasonal conditions or due to structural
    changes. While the appearance changing problem is originated when the same place
    changes over time in appearance, perceptual aliasing is caused when different
    places have a similar appearance. These conditions affect in particular place
    recognition since the loop decisions are affected directly by the appearance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 外观变化的环境和感知混淆特别受到大量研究的关注。随着自主车辆长时间运行，它们的感知系统必须处理由于不同天气、季节条件或结构变化等原因而随时间变化的环境。外观变化问题源于同一地点的外观随着时间变化，而感知混淆则是由于不同地点具有相似外观造成的。这些条件特别影响地点识别，因为环路决策直接受到外观的影响。
- en: 'A variety of works have been addressing these challenges from various perspectives.
    From the belief generation perspective, sequence-based matching approaches [[53](#bib.bib53),
    [54](#bib.bib54), [42](#bib.bib42), [55](#bib.bib55), [48](#bib.bib48)] are highlighted
    as very effective in these conditions. Sequence matching is the task of aligning
    a pair of a template and query sequences, which can be implemented through minimum
    cost flow [[42](#bib.bib42), [56](#bib.bib56)], or probabilistically using Hidden
    Markov models[[47](#bib.bib47)] or Conditional Random Fields[[48](#bib.bib48)].
    Another way is to address this problem from the place modeling perspective: extracting
    condition-invariant features [[57](#bib.bib57), [58](#bib.bib58)], extracting
    for example features from the middle layers of CNN’s [[22](#bib.bib22)]. On the
    other hand, matching quality of descriptors can be improved through descriptors
    normalization[[23](#bib.bib23), [59](#bib.bib59)] or through unsupervised techniques
    such as dimensionality reduction [[60](#bib.bib60)], change removal [[61](#bib.bib61)],
    K-STD [[62](#bib.bib62)] or delta descriptors [[43](#bib.bib43)].'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 各种研究工作从不同角度解决这些挑战。从信念生成的角度来看，基于序列的匹配方法[[53](#bib.bib53), [54](#bib.bib54), [42](#bib.bib42),
    [55](#bib.bib55), [48](#bib.bib48)]被认为在这些条件下非常有效。序列匹配是将模板和查询序列配对对齐的任务，可以通过最小成本流[[42](#bib.bib42),
    [56](#bib.bib56)]实现，或者使用隐马尔可夫模型[[47](#bib.bib47)]或条件随机场[[48](#bib.bib48)]进行概率性实现。另一种方法是从地点建模的角度来解决这个问题：提取条件不变的特征[[57](#bib.bib57),
    [58](#bib.bib58)]，例如从CNN的中间层提取特征[[22](#bib.bib22)]。另一方面，可以通过描述符归一化[[23](#bib.bib23),
    [59](#bib.bib59)]或通过无监督技术如降维[[60](#bib.bib60)]、变化去除[[61](#bib.bib61)]、K-STD[[62](#bib.bib62)]或delta描述符[[43](#bib.bib43)]来提高描述符的匹配质量。
- en: II-C2 Viewpoint Changing
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 视角变化
- en: Revisiting a place from different viewpoints - at the limit opposite direction
    (180º viewpoint variation)[[23](#bib.bib23)] - is also challenging for place recognition.
    That is, in particular, true for approaches that rely on sensors with a restricted
    field-of-view (FoV) or without geometrical sensing capabilities. When visiting
    a place, these sensors only capture a fraction of the environment, and when revisiting
    from a different angle or position, the appearance of the scene may differ or
    even additional elements may be sensed, generating a complete different place
    model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从不同视角重新访问一个地方——在极限情况下对面方向（180º视角变化）[[23](#bib.bib23)]——对位置识别也具有挑战性。特别是对于依赖于视场受限（FoV）或没有几何传感能力的传感器的方法尤为如此。当访问一个地方时，这些传感器只能捕捉到环境的一部分，而从不同角度或位置重新访问时，场景的外观可能会有所不同，甚至可能感知到额外的元素，从而生成完全不同的地方模型。
- en: To overcome these shortcomings, visual-based approaches have resorted to semantic-based
    features[[41](#bib.bib41), [63](#bib.bib63)]. For example extracting features
    from higher-order CNN layers, which have a semantic meaning, have demonstrated
    to be more robust to viewpoint variation [[22](#bib.bib22)]. Other works propose
    the use of panoramic cameras [[64](#bib.bib64)] or 3DLiDAR [[65](#bib.bib65)],
    being thus irrelevant what orientation places are perceived in future visits.
    Thus relying on sensors and methods that do not depend on orientation (also called
    viewpoint-invariant) turn place recognition more robust.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些不足，基于视觉的方法已经转向语义特征[[41](#bib.bib41), [63](#bib.bib63)]。例如，从具有语义意义的高阶CNN层中提取特征，已被证明对视角变化更具鲁棒性[[22](#bib.bib22)]。其他研究提出使用全景相机[[64](#bib.bib64)]或3DLiDAR
    [[65](#bib.bib65)]，因此未来访问中位置的朝向不再重要。因此，依赖于不依赖于朝向的传感器和方法（也称为视角不变）可以使位置识别更具鲁棒性。
- en: II-C3 Scalability
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 可扩展性
- en: 'Another critical factor of place recognition is concerned with scalability
    [[66](#bib.bib66), [67](#bib.bib67), [50](#bib.bib50), [68](#bib.bib68), [69](#bib.bib69),
    [67](#bib.bib67), [70](#bib.bib70)]. As self-driving vehicles operate in increasingly
    larger areas, more places are visited and maps become larger and larger, increasing
    thus computational demand, which affects negatively the inference efficiency.
    Thus, to boost inference efficiency, approaches include: efficient indexing [[71](#bib.bib71),
    [72](#bib.bib72)], hierarchical searching [[73](#bib.bib73), [74](#bib.bib74)],
    hashing [[50](#bib.bib50), [68](#bib.bib68), [75](#bib.bib75), [22](#bib.bib22),
    [70](#bib.bib70)], scalar quantization [[70](#bib.bib70)], Hidden Markov Models
    (HMMs) [[67](#bib.bib67), [69](#bib.bib69)] or learning regularly repeating visual
    patterns [[66](#bib.bib66)]. For example in [[70](#bib.bib70)] a hashing-based
    approach is used in a visual place recognition task with a large database to both
    maintain the storage footprint of the descriptor space small and boost retrieval.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 位置识别的另一个关键因素与可扩展性有关[[66](#bib.bib66), [67](#bib.bib67), [50](#bib.bib50), [68](#bib.bib68),
    [69](#bib.bib69), [67](#bib.bib67), [70](#bib.bib70)]。随着自动驾驶车辆在越来越大的区域内运行，访问的地点增多，地图变得越来越大，从而增加了计算需求，这对推理效率产生了负面影响。因此，为了提高推理效率，方法包括：高效索引[[71](#bib.bib71),
    [72](#bib.bib72)]、层级搜索[[73](#bib.bib73), [74](#bib.bib74)]、哈希[[50](#bib.bib50),
    [68](#bib.bib68), [75](#bib.bib75), [22](#bib.bib22), [70](#bib.bib70)]、标量量化[[70](#bib.bib70)]、隐马尔可夫模型（HMMs）[[67](#bib.bib67),
    [69](#bib.bib69)]或学习规律性重复的视觉模式[[66](#bib.bib66)]。例如，在[[70](#bib.bib70)]中，使用了一种基于哈希的方法来进行视觉位置识别任务，数据库很大，以保持描述符空间的存储占用小并提高检索速度。
- en: III Sensors
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 传感器
- en: 'An important aspect of any perception-based application is the selection of
    appropriate sensors. To this end, the selection criterion has to consider the
    specificities of both the application and the environment for the task in hand.
    In place recognition, the must used sensors are cameras [[26](#bib.bib26), [27](#bib.bib27),
    [63](#bib.bib63), [24](#bib.bib24), [41](#bib.bib41), [23](#bib.bib23), [42](#bib.bib42),
    [76](#bib.bib76), [32](#bib.bib32)], LiDARs [[77](#bib.bib77), [78](#bib.bib78),
    [65](#bib.bib65), [28](#bib.bib28), [79](#bib.bib79), [80](#bib.bib80), [46](#bib.bib46),
    [34](#bib.bib34), [13](#bib.bib13)] and RADARs [[14](#bib.bib14), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83)]. Although in a broader AV context, these sensors
    are widely adopted [[84](#bib.bib84), [85](#bib.bib85)], in place recognition,
    cameras are the most popular in the literature, followed by LiDARs, while RADARs
    are a very recent technology in this domain. For the remaining of this section,
    each sensor is detailed and an outline is presented in Table [I](#S3.T1 "TABLE
    I ‣ III Sensors ‣ Place recognition survey: An update on deep learning approaches").'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '任何基于感知的应用程序的重要方面是选择合适的传感器。为此，选择标准必须考虑应用程序和任务环境的特定性。在位置识别中，必须使用的传感器是相机[[26](#bib.bib26)、[27](#bib.bib27)、[63](#bib.bib63)、[24](#bib.bib24)、[41](#bib.bib41)、[23](#bib.bib23)、[42](#bib.bib42)、[76](#bib.bib76)、[32](#bib.bib32)]、LiDAR[[77](#bib.bib77)、[78](#bib.bib78)、[65](#bib.bib65)、[28](#bib.bib28)、[79](#bib.bib79)、[80](#bib.bib80)、[46](#bib.bib46)、[34](#bib.bib34)、[13](#bib.bib13)]和雷达[[14](#bib.bib14)、[81](#bib.bib81)、[82](#bib.bib82)、[83](#bib.bib83)]。虽然在更广泛的自动驾驶上下文中，这些传感器被广泛采用[[84](#bib.bib84)、[85](#bib.bib85)]，但在位置识别中，相机在文献中最受欢迎，其次是LiDAR，而雷达在这个领域仍然是非常新的技术。在本节的其余部分，将详细介绍每种传感器，并在表[I](#S3.T1
    "TABLE I ‣ III Sensors ‣ Place recognition survey: An update on deep learning
    approaches")中展示大纲。'
- en: '![Refer to caption](img/cbd295d6d9c61d7a2464f048fa264f73.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cbd295d6d9c61d7a2464f048fa264f73.png)'
- en: 'Figure 4: Popular sensors in place recognition.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：位置识别中的常用传感器。
- en: In the place recognition literature, cameras are by far the most used sensor.
    The vision category includes camera sensors such as monocular [[26](#bib.bib26)],
    stereo [[86](#bib.bib86)], RGBD [[87](#bib.bib87)], thermal [[88](#bib.bib88)]
    or event-triggered [[89](#bib.bib89)]. Cameras provide dense and rich visual information,
    which can be provided at a high frame rate (ranging up to 60Hz) with a relatively
    low cost. On the other hand, vision data is very sensitive when faced with visual
    appearance change and viewpoint variation, which is a tremendous disadvantage
    compared with the other modalities. Besides vision data, cameras are also able
    to return depth maps. This is achieved either with RGB-D[[90](#bib.bib90), [87](#bib.bib87)],
    stereo cameras[[86](#bib.bib86)], or trough structure from motion (SfM)[[91](#bib.bib91)]
    methods. In outdoor environments, the limited field-of-view (FoV) and noisy depth
    measurements are a clear disadvantage when compared with the depth measurements
    of recent 3D LiDARs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在位置识别的文献中，相机是使用最广泛的传感器。视觉类别包括单目[[26](#bib.bib26)]、立体[[86](#bib.bib86)]、RGBD[[87](#bib.bib87)]、热成像[[88](#bib.bib88)]或事件触发[[89](#bib.bib89)]等相机传感器。相机提供密集且丰富的视觉信息，可以以高帧率（高达60Hz）提供，并且成本相对较低。另一方面，当面对视觉外观变化和视角变化时，视觉数据非常敏感，这相比其他模态是一个巨大的劣势。除了视觉数据，相机还能够返回深度图。这可以通过RGB-D[[90](#bib.bib90)、[87](#bib.bib87)]、立体相机[[86](#bib.bib86)]，或通过结构光流（SfM）[[91](#bib.bib91)]方法实现。在户外环境中，与最新的3D
    LiDAR的深度测量相比，有限的视场（FoV）和噪声深度测量明显是一个劣势。
- en: LiDAR sensors gained more attention, in place recognition, with the emergence
    of the 3D rotating version. 3D LiDARs capture the spatial (or geometrical) structure
    of the surrounding environment in a single 360°swift, measuring the time-of-flight
    (ToF) of reflected laser beams. These sensors have a sensing capacity of up to
    120m with a frame rate of 10 to 15 Hz. Such features are particularly suitable
    for outdoor environment, since measuring depth through ToF is not influenced by
    lighting or visual appearance conditions. This is a major advantage when compared
    with cameras. On the other hand, disadvantages are related to the high cost and
    the large size, which have been promised to be surpassed by the solid-state versions.
    An additional weak point is the sensitiveness of this technology towards the reflectance
    property of objects. For example, glass, mirror, smoke, fog, and dust reduce sensing
    capabilities.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 3D 旋转版本的出现，LiDAR 传感器在地点识别中获得了更多关注。3D LiDAR 在一次 360° 快速旋转中捕捉周围环境的空间（或几何）结构，测量反射激光束的飞行时间（ToF）。这些传感器的探测能力可达
    120 米，帧率为 10 到 15 Hz。这样的特性特别适合户外环境，因为通过 ToF 测量深度不受光照或视觉外观条件的影响。这与相机相比是一个主要优势。另一方面，劣势与高成本和大尺寸有关，固态版本承诺能克服这些问题。另一个弱点是该技术对物体反射特性的敏感性。例如，玻璃、镜子、烟雾、雾气和灰尘会降低探测能力。
- en: Radar sensors measure distance through time delay or phase shift of radio signals,
    which makes them very robust to different weather or lighting conditions. The
    reasonable cost and long-range capability [[92](#bib.bib92)] are features that
    are popularizing radars in tasks such as environment understanding [[93](#bib.bib93)]
    and place recognition [[14](#bib.bib14)]. However, radars continue to face weaknesses
    in terms of low spatial resolution and interoperability [[93](#bib.bib93)], disadvantages
    when compared with LiDARs or cameras.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 雷达传感器通过无线电信号的时间延迟或相位偏移来测量距离，这使得它们对不同的天气或光照条件非常鲁棒。合理的成本和长距离能力 [[92](#bib.bib92)]
    是雷达在环境理解 [[93](#bib.bib93)] 和地点识别 [[14](#bib.bib14)] 等任务中流行的特点。然而，雷达在空间分辨率和互操作性方面仍面临缺点
    [[93](#bib.bib93)]，这些缺点与 LiDAR 或相机相比。
- en: 'TABLE I: Sensors for place recognition: pros and cons.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：地点识别传感器：优缺点。
- en: '| Sensor | Advantage | Disadvantage |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 传感器 | 优势 | 劣势 |'
- en: '| --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Camera | - Low cost - Dense color information'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '| 相机 | - 低成本 - 密集的彩色信息'
- en: '- Low energy consumption'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '- 低能耗'
- en: '- High precision/resolution'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '- 高精度/分辨率'
- en: '- High frame rate | - Short range - Sensitive to light'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '- 高帧率 | - 范围短 - 对光线敏感'
- en: '- Sensitive to calibration'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对校准敏感'
- en: '- Limited FoV'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '- 视场有限'
- en: '- Difficulty in textureless environment |'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在无纹理环境中的困难 |'
- en: '| 3D LiDAR | - Long range - 360º FoV'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '| 3D LiDAR | - 长范围 - 360º 视场'
- en: '- Robust to appearance changing conditions'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对外观变化条件具有鲁棒性'
- en: -high precision/resolution | - High cost - Sensitive to reflective and foggy
    environments
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '- 高精度/分辨率 | - 高成本 - 对反射和雾霾环境敏感'
- en: '- Bulky'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '- 笨重'
- en: '- Fragile mechanics |'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '- 机械易损 |'
- en: '| RADAR | - Low cost - Very Long range'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '| 雷达 | - 低成本 - 非常长的范围'
- en: '- Precise velocity estimation'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '- 精确的速度估计'
- en: '- Insensitive to weather conditions | - Narrow FoV - Low resolution |'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对天气条件不敏感 | - 视场狭窄 - 低分辨率 |'
- en: '![Refer to caption](img/ac65765bf739a326e02bfaee859a5a5f.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ac65765bf739a326e02bfaee859a5a5f.png)'
- en: 'Figure 5: Block diagram of pre-trained frameworks a) holistic-based b) landmark-based
    and c) region-based.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：预训练框架的框图 a) 基于整体 b) 基于特征点和 c) 基于区域。
- en: IV Supervised place recognition
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 监督式地点识别
- en: This section addresses the place recognition approaches that resort to supervised
    deep learning. Supervised machine learning techniques learn a function that maps
    an input representation (e.g., images, point clouds) into an output representation
    (e.g.categories, scores, bounding boxes, descriptor) utilizing labeled data. In
    deep learning, this function assumes the form of weights in a network with staked
    layers. The weights are learned progressively by computing the error between predictions
    and ground-truth in a first step, and in a second step, the error is backpropagated
    using gradient vectors[[20](#bib.bib20)]. This procedure (i.e., error measuring
    and weight adjusting) is repeated until the network’s predictions achieve adequate
    performance. The advantage of such a learning process, particularly when using
    convolutional networks (CNN), is the capability of automatically learning features
    from the training data, which, in classical approaches, required a considerable
    amount of engineering skill and domain expertise. On the other hand, the disadvantages
    are related to the necessity of a massive amount of labeled data for training,
    which is expensive to obtain[[94](#bib.bib94)].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了依赖监督深度学习的地点识别方法。监督机器学习技术通过利用标记数据，学习一个将输入表示（例如，图像、点云）映射到输出表示（例如，类别、分数、边界框、描述符）的函数。在深度学习中，这个函数以具有堆叠层的网络中的权重形式存在。权重通过在第一步中计算预测和真实值之间的误差，逐步学习，在第二步中，误差通过使用梯度向量进行反向传播[[20](#bib.bib20)]。这一过程（即误差测量和权重调整）会重复进行，直到网络的预测达到足够的性能。这种学习过程的优点，特别是使用卷积网络（CNN）时，是能够自动从训练数据中学习特征，而在经典方法中，这需要相当大的工程技能和领域专业知识。另一方面，缺点与训练所需的大量标记数据相关，而这些数据获取成本昂贵[[94](#bib.bib94)]。
- en: In place recognition, deep supervised learning enabled breakthroughs. Especially,
    the capability of CNNs to extract features led to more descriptive place models,
    improving place matching. Early approaches relied mostly on pre-trained (or off-the-shelf)
    CNNs that were trained on other vision tasks[[21](#bib.bib21), [22](#bib.bib22)].
    But more recently, new approaches enabled the training of DL networks directly
    on place recognition tasks in a end-to-end fashion [[24](#bib.bib24), [65](#bib.bib65),
    [95](#bib.bib95), [96](#bib.bib96)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在地点识别中，深度监督学习实现了突破。特别是，CNN 提取特征的能力导致了更具描述性的地点模型，从而改善了地点匹配。早期方法主要依赖于预训练（或现成）的
    CNN，这些 CNN 在其他视觉任务上进行了训练[[21](#bib.bib21), [22](#bib.bib22)]。但最近，新的方法使得深度学习网络能够以端到端的方式直接在地点识别任务上进行训练
    [[24](#bib.bib24), [65](#bib.bib65), [95](#bib.bib95), [96](#bib.bib96)]。
- en: IV-A Pre-trained-based Frameworks
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 基于预训练的框架
- en: 'TABLE II: Summary of recent works on supervised place recognition using pre-trained
    frameworks. All the works use camera-based data. BG = Belief Generation and PM
    = Place mapping.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：基于预训练框架的监督地点识别的近期工作总结。所有工作都使用基于相机的数据。BG = 信念生成，PM = 地点映射。
- en: 'Type Ref Model BG/PM Dataset Holistic-based [[21](#bib.bib21)] Feature Extraction:
    OxfordNet [[97](#bib.bib97)] and GoogLeNet[[98](#bib.bib98)];'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 类型 参考模型 BG/PM 数据集 基于整体的 [[21](#bib.bib21)] 特征提取：OxfordNet [[97](#bib.bib97)]
    和 GoogLeNet[[98](#bib.bib98)];
- en: 'Descriptor: VLAD [[99](#bib.bib99)] + PCA [[100](#bib.bib100)] L2 distance/Database
    Holidays [[101](#bib.bib101)]; Oxford [[102](#bib.bib102)];'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：VLAD [[99](#bib.bib99)] + PCA [[100](#bib.bib100)] L2 距离/数据库 Holidays [[101](#bib.bib101)];
    Oxford [[102](#bib.bib102)];
- en: 'Paris [[103](#bib.bib103)] [[104](#bib.bib104)] Feature Extraction: CNN-VTL
    (VGG-F[[105](#bib.bib105)])'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 巴黎 [[103](#bib.bib103)] [[104](#bib.bib104)] 特征提取：CNN-VTL (VGG-F[[105](#bib.bib105)])
- en: 'Descriptor: Conv5 layer + Random selection (LDB[[106](#bib.bib106)]) Hamming
    distance/Database Nordland [[2](#bib.bib2)]; CMU-CVG Visual Localization [[107](#bib.bib107)];
    Alderley [[51](#bib.bib51)]; [[22](#bib.bib22)] Feature Extraction: AlexNet [[108](#bib.bib108)];'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：Conv5 层 + 随机选择 (LDB[[106](#bib.bib106)]) 汉明距离/数据库 Nordland [[2](#bib.bib2)];
    CMU-CVG 视觉定位 [[107](#bib.bib107)]; Alderley [[51](#bib.bib51)]; [[22](#bib.bib22)]
    特征提取：AlexNet [[108](#bib.bib108)];
- en: 'Descriptor: Conv3 layer Hamming KNN/Database Nordland[[2](#bib.bib2)]; Gardens
    Point[[22](#bib.bib22)]; The Campus Human vs. Robot; The St. Lucia [[109](#bib.bib109)]
    Landmark-based [[23](#bib.bib23)] Landmark Detection: Left and right image regions'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：Conv3 层 汉明 KNN/数据库 Nordland[[2](#bib.bib2)]; Gardens Point[[22](#bib.bib22)];
    The Campus 人类与机器人；The St. Lucia [[109](#bib.bib109)] 基于地标的 [[23](#bib.bib23)]
    地标检测：左右图像区域
- en: 'Feature Extraction: CNN Places365 [[110](#bib.bib110)]'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取：CNN Places365 [[110](#bib.bib110)]
- en: 'Descriptor: fc6 + normalization + concatenation Sequence Match/Database Oxford
    Robotcar [[102](#bib.bib102)];'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：fc6 + 归一化 + 拼接序列匹配/数据库 Oxford Robotcar [[102](#bib.bib102)];
- en: 'University Campus; [[111](#bib.bib111)] Landmark Detection: Edge Boxes'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 大学校园；[[111](#bib.bib111)] 地标检测：Edge Boxes
- en: 'Feature Extraction: ALexNet'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取：ALexNet
- en: 'Descriptor: Conv3 layer + Gaussian Random Projection [[112](#bib.bib112)] Cosine
    KNN/Database Gardens Point [[22](#bib.bib22)]; Mapillary;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：Conv3 层 + 高斯随机投影 [[112](#bib.bib112)] 余弦 KNN/数据库 Gardens Point [[22](#bib.bib22)];
    Mapillary;
- en: 'Library Robot Indoor; Nordland [[2](#bib.bib2)]; [[113](#bib.bib113)] Landmrk
    detection: BING [[114](#bib.bib114)]'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图书馆机器人室内；Nordland [[2](#bib.bib2)]; [[113](#bib.bib113)] 地标检测：BING [[114](#bib.bib114)]
- en: 'Feature Extraction: AlexNet'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取：AlexNet
- en: 'Descriptor: pool 5 layer + Gaussian Random Projection [[115](#bib.bib115),
    [112](#bib.bib112)] + normalization L2- KNN/Database Gardens Point[[22](#bib.bib22)];
    Berlin A100, Berlin Halenseestrasse and Berlin Kudamm [[111](#bib.bib111)]; Nordland
    [[2](#bib.bib2)]; St. Lucia [[109](#bib.bib109)]; Region-based [[59](#bib.bib59)]
    Feature Extraction: Fast-Net (VGG) [[116](#bib.bib116)]'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：pool 5 层 + 高斯随机投影 [[115](#bib.bib115), [112](#bib.bib112)] + 归一化 L2- KNN/数据库
    Gardens Point[[22](#bib.bib22)]; Berlin A100、Berlin Halenseestrasse 和 Berlin Kudamm
    [[111](#bib.bib111)]; Nordland [[2](#bib.bib2)]; St. Lucia [[109](#bib.bib109)];
    基于区域的 [[59](#bib.bib59)] 特征提取：Fast-Net (VGG) [[116](#bib.bib116)]
- en: 'Descriptor: conv3 + L2-normalization + Sparse Random Projection [[117](#bib.bib117)]
    Cosine distance/Database Cityscapes [[118](#bib.bib118)];'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：conv3 + L2-归一化 + 稀疏随机投影 [[117](#bib.bib117)] 余弦距离/数据库 Cityscapes [[118](#bib.bib118)];
- en: Virtual KITTI [[119](#bib.bib119)];
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟 KITTI [[119](#bib.bib119)];
- en: 'Freiburg; [[40](#bib.bib40)] Feature Extraction: VGG16[[120](#bib.bib120)]'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Freiburg; [[40](#bib.bib40)] 特征提取：VGG16[[120](#bib.bib120)]
- en: 'Descriptor: Salient regions from different layers + Bag-of-Words [[121](#bib.bib121)]
    Cross matching/ Database Gardens Point [[22](#bib.bib22)]; Nordland [[2](#bib.bib2)];
    Berlin A100, Berlin Halenseestrasse and Berlin Kudamm [[111](#bib.bib111)]; [[122](#bib.bib122)]
    Feature Extraction: AlexNet365[[123](#bib.bib123)]'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：来自不同层的显著区域 + 词袋模型 [[121](#bib.bib121)] 交叉匹配/数据库 Gardens Point [[22](#bib.bib22)];
    Nordland [[2](#bib.bib2)]; Berlin A100、Berlin Halenseestrasse 和 Berlin Kudamm
    [[111](#bib.bib111)]; [[122](#bib.bib122)] 特征提取：AlexNet365[[123](#bib.bib123)]
- en: 'Descriptor: (Region-VLAD) salient regions + VLAD Cosine distance/Database Mapillary;
    Gardens Point [[22](#bib.bib22)]; Nordland [[2](#bib.bib2)]; Berlin A100, Berlin
    Halenseestrasse and Berlin Kudamm [[111](#bib.bib111)];'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：(区域-VLAD) 显著区域 + VLAD 余弦距离/数据库 Mapillary; Gardens Point [[22](#bib.bib22)];
    Nordland [[2](#bib.bib2)]; Berlin A100、Berlin Halenseestrasse 和 Berlin Kudamm
    [[111](#bib.bib111)];
- en: 'In this work, pre-trained place recognition frameworks refer to approaches
    that extract features from pre-trained CNN models, which are originally trained
    on other perception tasks (e.g., object recognition [[21](#bib.bib21)], place
    categorization [[22](#bib.bib22), [23](#bib.bib23)] or segmentation [[59](#bib.bib59)]).
    Works using such models fall into three categories: holistic-based, landmark-based,
    and region-based. Figure [5](#S3.F5 "Figure 5 ‣ III Sensors ‣ Place recognition
    survey: An update on deep learning approaches") illustrates such approaches applied
    to an input image.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '在本工作中，预训练的地点识别框架指的是从预训练的 CNN 模型中提取特征的方法，这些模型最初是针对其他感知任务（例如物体识别 [[21](#bib.bib21)]、地点分类
    [[22](#bib.bib22), [23](#bib.bib23)] 或分割 [[59](#bib.bib59)]）进行训练的。使用这些模型的工作分为三类：整体型、地标型和基于区域的。图
    [5](#S3.F5 "Figure 5 ‣ III Sensors ‣ Place recognition survey: An update on deep
    learning approaches") 说明了这些方法如何应用于输入图像。'
- en: IV-A1 Holistic-based
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 整体型
- en: Holistic approaches refer to works that feed the whole image to a CNN and use
    all activations from a layer as a descriptor. The hierarchical nature of CNNs
    makes that the various layers contain features with different semantic meanings.
    Thus, to assess which layers generate the best features for place recognition,
    works have conducted ablation studies, which compared the performance of the various
    layers towards appearance and viewpoint robustness and compared the performance
    of object-centric, place-centric, and hybrid networks (i.e., networks trained
    respectively for object recognition, place categorization and both). Moreover,
    as CNN layers tend to have many activations, the proposed approaches compress
    the descriptor to a more tractable sized for efficiency reasons.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 全局方法指的是将整个图像输入到CNN中，并使用某一层的所有激活作为描述符。CNN的层级特性使得不同层包含具有不同语义含义的特征。因此，为了评估哪些层生成的特征对地点识别最佳，研究进行了消融研究，比较了各层在外观和视角鲁棒性方面的表现，并比较了以对象为中心、地点为中心和混合网络（即分别训练用于对象识别、地点分类和两者的网络）的性能。此外，由于CNN层通常有很多激活，提出的方法将描述符压缩为更易处理的大小以提高效率。
- en: Ng et al. [[21](#bib.bib21)] study the performance of each layer, using pre-trained
    object-centric networks such as OxfordNet [[97](#bib.bib97)] and GoogLeNet[[98](#bib.bib98)]
    to extract feature from images. The features are encoded into VLAD descriptors
    and compressed using PCA [[100](#bib.bib100)]. Results show that performance increases
    as features are extracted from deeper layers, but drops again at the latest layers.
    Matching is achieved by computing the L2 distance of two descriptors.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Ng 等人 [[21](#bib.bib21)] 研究了每层的性能，使用预训练的以对象为中心的网络，如OxfordNet [[97](#bib.bib97)]
    和GoogLeNet [[98](#bib.bib98)] 从图像中提取特征。特征被编码成VLAD描述符，并使用PCA [[100](#bib.bib100)]
    压缩。结果显示，随着特征从更深层提取，性能有所提升，但在最深层又会下降。匹配通过计算两个描述符的L2距离来实现。
- en: A similar conclusion is reached by Sünderhauf et al. [[22](#bib.bib22)], using
    holistic image descriptors extracted from AlexNet[[124](#bib.bib124)]. Authors
    argue that the semantic information encoded in the middle layers improves place
    recognition when faced with severe appearance change, while features from higher
    layers are more robust to viewpoint change. The work further compares AlexNet
    (object-centric) with Places205 and Hybrid [[125](#bib.bib125)], both trained
    on a scene categorization task (i.e., place-centric networks) [[125](#bib.bib125)],
    concluding that, for place recognition, place-centric networks outperform object-centric
    CNNs. The networks are tested using a cosine-based KNN approach for matching,
    but for efficiency reason, the cosine similarity was approximated by the Hamming
    distance [[126](#bib.bib126)].
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Sünderhauf 等人 [[22](#bib.bib22)] 达到了类似的结论，使用从AlexNet [[124](#bib.bib124)] 中提取的全局图像描述符。作者认为，中间层编码的语义信息在面临严重外观变化时能改善地点识别，而高层的特征对视角变化更具鲁棒性。该研究进一步比较了AlexNet（以对象为中心）与Places205和Hybrid
    [[125](#bib.bib125)]，两者均在场景分类任务（即地点为中心的网络） [[125](#bib.bib125)] 上训练，得出结论：在地点识别方面，地点为中心的网络优于以对象为中心的CNN。网络通过基于余弦的KNN方法进行匹配，但出于效率考虑，余弦相似度被汉明距离
    [[126](#bib.bib126)] 近似。
- en: On the other hand, Arroyo et al. [[104](#bib.bib104)] fuse features from multiple
    convolutional layers at several levels and granularities and show that this approach
    outperforms approaches that only use features from a single layer. The CNN architecture
    is based on the VGG-F [[105](#bib.bib105)], and the output features are further
    compressed using a random selection approach for efficient matching.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Arroyo 等人 [[104](#bib.bib104)] 在多个卷积层的不同层级和粒度下融合特征，结果表明这种方法优于仅使用单层特征的方法。CNN架构基于VGG-F
    [[105](#bib.bib105)]，并使用随机选择方法对输出特征进行进一步压缩以实现高效匹配。
- en: IV-A2 Landmark-based
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 基于地标的
- en: Landmark-based approaches, contrary to the aforementioned methods, do not feed
    the entire image to the network; instead, these approaches use, in a pre-processing
    stage, object proposal techniques to identify potential landmarks in the images,
    which are feed to the CNN. Contrary to holistic-based approaches, where all image
    features are transformed to descriptors, in landmark-based approaches, only the
    features from the detected landmarks are converted to descriptors. Detection approaches
    used in these works include Edge Boxes, BING, or simple heuristics.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与前述方法相反，基于地标的方法不会将整个图像馈送到网络；而是在预处理阶段使用对象提议技术来识别图像中的潜在地标，然后将其馈送至 CNN。与整体方法不同的是，在基于地标的方法中，仅将检测到的地标的特征转换为描述符。这些工作中使用的检测方法包括边缘框、BING
    或简单启发式方法。
- en: With the aim of addressing the extreme appearance and viewpoint variations problem
    in place recognition, Sünderhauf et al. [[111](#bib.bib111)] propose such a landmark
    detection approach. Landmarks are detected using Edge Boxes [[127](#bib.bib127)]
    and are mapped into a feature space using the features from Alexnet’s[[124](#bib.bib124)]
    conv3 layer. The descriptor is also compressed for efficiency reasons, using a
    Gaussian Random Projection approach [[112](#bib.bib112)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决地点识别中的极端外观和视角变化问题，Sünderhauf 等人提出了一种地标检测方法。地标是使用 Edge Boxes 检测的 [[127](#bib.bib127)]，并且使用来自
    Alexnet 的 conv3 层的特征映射到特征空间。由于效率原因，描述符也进行了压缩，使用了高斯随机投影方法 [[112](#bib.bib112)]。
- en: A similar approach is proposed by Kong et al. [[113](#bib.bib113)]. However,
    instead of detecting landmarks using Edge Boxes [[127](#bib.bib127)] and extracting
    features from conv3 layer of Alexnet, landmarks are detected using BING [[114](#bib.bib114)]
    and features are extracted from a pooling layer.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Kong 等人提出了一种类似的方法 [[113](#bib.bib113)]。然而，与使用 Edge Boxes [[127](#bib.bib127)]
    检测地标并从 Alexnet 的 conv3 层提取特征的方法相反，这些地标是使用 BING [[114](#bib.bib114)] 检测并从池化层提取特征的。
- en: A slightly different approach is proposed by Garg et al. [[23](#bib.bib23)],
    which resorting to Places365 [[110](#bib.bib110)], also highlights the effectiveness
    of place-centric semantic information in extreme variations such as front versus
    rear view. In particular, this work crops the right and left regions of images,
    which has been demonstrated to possess useful information[[59](#bib.bib59)], for
    place description. The work highlights the importance of semantics-aware features
    from higher-order layers for viewpoint and condition invariance. Additionally,
    to improve robustness against appearance, a descriptor normalization approach
    is proposed. Descriptor normalization of the query and reference descriptors are
    computed independently since the image conditions differ (i.e., day-time vs. night-time).
    While matching is computed using SeqSLAM [[42](#bib.bib42)].
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Garg 等人提出了一个稍有不同的方法 [[23](#bib.bib23)]，该方法利用 Places365 [[110](#bib.bib110)]，也强调了在极端变化中（如前视和后视）地点中心语义信息的有效性。具体来说，这项工作剪裁了图像的右侧和左侧区域，这已被证明对地点描述具有有用信息
    [[59](#bib.bib59)]。该工作突出了来自高阶层的语义感知特征对视角和条件的不变性的重要性。此外，为了提高对外观的鲁棒性，提出了一种描述符归一化方法。查询描述符和参考描述符的归一化是独立计算的，因为图像条件不同（即白天与黑夜）。匹配则使用
    SeqSLAM 计算 [[42](#bib.bib42)]。
- en: '![Refer to caption](img/358d807b767d423e8f3f90368ff57211.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/358d807b767d423e8f3f90368ff57211.png)'
- en: 'Figure 6: Block diagram of training strategies using a) contrastive-based and
    margin-based, b) triplet and c) quadruplet loss function.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：使用 a) 对比度和基于边距的训练策略的块图，b) 三元组和 c) 四元组损失函数。
- en: IV-A3 Region-based
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 基于区域的
- en: Region-based methods, similarly to landmark-based approaches, rely on local
    features; however, instead of utilizing object proposal methods, the regions of
    interest are identified on the CNN layers, detecting salient layer activations.
    Therefore, region-based methods feed the entire image to the DL model and use,
    for the descriptors, only the salient activation in the CNN layers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 基于区域的方法与基于地标的方法类似，依赖于局部特征；但是，不同于利用对象提议方法，基于区域的方法在 CNN 层上识别感兴趣的区域，检测显著的层激活。因此，基于区域的方法将整个图像馈送到
    DL 模型，并且仅使用 CNN 层中的显著激活作为描述符。
- en: Addressing the problem of viewpoint and appearance changing in place recognition,
    Chen et al. [[40](#bib.bib40)] propose such a region-based approach that extracts
    salient regions without relying on external landmark proposal techniques. Regions
    of interest are extracted from various CNN layers of a pre-trained VGG16[[120](#bib.bib120)].
    The approach extracts explicitly local features from the early layers and semantic
    information from later layers. The extracted regions are encoded into a descriptor,
    using a bag-of-words-based approach [[121](#bib.bib121)] which is matched using
    a cross-matching approach.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 针对位置识别中的视角和外观变化问题，Chen 等人 [[40](#bib.bib40)] 提出了一种基于区域的方法，该方法提取显著区域而不依赖于外部地标提议技术。感兴趣的区域从预训练的
    VGG16 [[120](#bib.bib120)] 的各个 CNN 层中提取。该方法从早期层中显式提取局部特征，从后期层中提取语义信息。提取的区域被编码成描述符，使用基于词袋的方法
    [[121](#bib.bib121)] 进行匹配。
- en: Naseer et al. [[59](#bib.bib59)], on the other hand, learn the activation regions
    of interest resorting to segmentation. In this work, regions of interest represent
    stable image areas, which are learned using Fast-Net [[116](#bib.bib116)]. Fast-Net
    is an up-convolutional Network that provides near real-time image segmentation.
    Due to being too large for real-time matching, the features resulting from the
    learned segments are encoded into a lower dimensionality using L2-normalization
    and Sparse Random Projection [[117](#bib.bib117)]. This approach, in particular,
    learns human-made structure due to being more stable for more extended periods.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Naseer 等人 [[59](#bib.bib59)] 另一方面，利用分割来学习感兴趣的激活区域。在这项工作中，感兴趣的区域代表了稳定的图像区域，这些区域使用
    Fast-Net [[116](#bib.bib116)] 进行学习。Fast-Net 是一种上卷积网络，提供接近实时的图像分割。由于过于庞大以至于无法进行实时匹配，因此通过
    L2 正则化和稀疏随机投影 [[117](#bib.bib117)] 将从学习到的分段中得到的特征编码为较低维度。这种方法特别学习人造结构，因为其在较长时间内更为稳定。
- en: With the aim of reducing the memory and computational cost, Khaliq et al. [[122](#bib.bib122)]
    propose Region-VLAD. This approach leverages a lightweight place-centric CNN architecture
    (AlexNet365[[123](#bib.bib123)]) to extract regional features. These features
    are encoded using a VLAD method, which is specially adapted to gain computation-efficiency
    and environment invariance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低内存和计算成本，Khaliq 等人 [[122](#bib.bib122)] 提出了 Region-VLAD。这种方法利用轻量级的以地点为中心的
    CNN 体系结构（AlexNet365 [[123](#bib.bib123)]）来提取区域特征。这些特征使用 VLAD 方法进行编码，该方法特别调整以提高计算效率和环境不变性。
- en: 'TABLE III: Summary of recent works on supervised end-to-end place recognition.
    BG = Belief Generation and PM = Place mapping.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：近期监督端到端位置识别工作的总结。BG = 信念生成，PM = 位置映射。
- en: 'Sensor Ref Architecture Loss Function BG/PM Dataset Camera [[24](#bib.bib24)]
    NetVLAD:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '传感器 参考 体系结构 损失函数 BG/PM 数据集 相机 [[24](#bib.bib24)] NetVLAD:'
- en: 'VGG/AlexNet + NetVLAD layer Triplet loss KNN /Database Google Street View Time
    Machine; Pitts250k [[128](#bib.bib128)]; [[25](#bib.bib25)] 2D CNN visual and
    3D CNN structural feature extraction + Feature fusion network; Margin-based loss
    [[129](#bib.bib129)] KNN /Database Oxford RobotCar [[1](#bib.bib1)]; [[96](#bib.bib96)]
    SPE-VLAD:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 'VGG/AlexNet + NetVLAD 层 三元组损失 KNN /数据库 Google 街景时间机器； Pitts250k [[128](#bib.bib128)];
    [[25](#bib.bib25)] 2D CNN 视觉和 3D CNN 结构特征提取 + 特征融合网络； 基于边距的损失 [[129](#bib.bib129)]
    KNN /数据库 Oxford RobotCar [[1](#bib.bib1)]; [[96](#bib.bib96)] SPE-VLAD:'
- en: (VGG-16 network or ResNet18) + spatial pyramid structure + NetVLAD layer Weighted
    Triplet L2 /Database Pittsburgh [[128](#bib.bib128)];
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (VGG-16 网络或 ResNet18) + 空间金字塔结构 + NetVLAD 层 加权三元组 L2 /数据库 Pittsburgh [[128](#bib.bib128)];
- en: TokyoTimeMachine[[130](#bib.bib130)];
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: TokyoTimeMachine [[130](#bib.bib130)];
- en: 'Places365-Standard [[110](#bib.bib110)]; [[131](#bib.bib131)] Siamese-ResNet:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 'Places365-Standard [[110](#bib.bib110)]; [[131](#bib.bib131)] Siamese-ResNet:'
- en: ResNet in the siamese network L2-based loss [[132](#bib.bib132)] L2 /Database
    TUM [[133](#bib.bib133)]; [[50](#bib.bib50)] MobileNet [[134](#bib.bib134)] Triplet
    loss [[135](#bib.bib135), [136](#bib.bib136)] Hamming K-NN /Database Nordland
    [[2](#bib.bib2)];
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese 网络中的 ResNet L2 基于损失 [[132](#bib.bib132)] L2 /数据库 TUM [[133](#bib.bib133)];
    [[50](#bib.bib50)] MobileNet [[134](#bib.bib134)] 三元组损失 [[135](#bib.bib135), [136](#bib.bib136)]
    哈明 K-NN /数据库 Nordland [[2](#bib.bib2)];
- en: 'Gardens Point [[22](#bib.bib22)] [[137](#bib.bib137)] HybridNet [[138](#bib.bib138)]
    Triplet Loss Cosine /Database Oxford RobotCar [[6](#bib.bib6)]; Nordland [[2](#bib.bib2)];
    Gardens Point [[22](#bib.bib22)]; 3D LiDAR [[49](#bib.bib49)] LPD-Net:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gardens Point [[22](#bib.bib22)] [[137](#bib.bib137)] HybridNet [[138](#bib.bib138)]
    三元组损失 余弦 /数据库 Oxford RobotCar [[6](#bib.bib6)]; Nordland [[2](#bib.bib2)]; Gardens
    Point [[22](#bib.bib22)]; 3D LiDAR [[49](#bib.bib49)] LPD-Net:'
- en: 'Adaptive feature extraction + a graph-based neighborhood aggregation + NetVLAD
    layer Lazy quadruplet loss L2 /Database Oxford Robotcar [[1](#bib.bib1)]; [[65](#bib.bib65)]
    PointNetVLAD:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '自适应特征提取 + 基于图的邻域聚合 + NetVLAD 层 懒惰四元组损失 L2 /数据库 牛津 Robotcar [[1](#bib.bib1)];
    [[65](#bib.bib65)] PointNetVLAD:'
- en: 'PointNet + NetVLAD layer Lazy triplet and quadruplet loss KNN /Database Oxford
    RobotCar [[1](#bib.bib1)]; [[34](#bib.bib34)] OREOS:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 'PointNet + NetVLAD 层 懒惰三元组和四元组损失 KNN /数据库 牛津 RobotCar [[1](#bib.bib1)]; [[34](#bib.bib34)]
    OREOS:'
- en: CNN as in [[120](#bib.bib120), [139](#bib.bib139)] Triplet loss [[140](#bib.bib140)]
    KNN /Database NCLT [[141](#bib.bib141)];
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 如在 [[120](#bib.bib120), [139](#bib.bib139)] 中所示 三元组损失 [[140](#bib.bib140)]
    KNN /数据库 NCLT [[141](#bib.bib141)];
- en: 'KITTI [[142](#bib.bib142)]; [[13](#bib.bib13)] LocNet:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 'KITTI [[142](#bib.bib142)]; [[13](#bib.bib13)] LocNet:'
- en: Siamese network Contrastive loss function[[143](#bib.bib143)] L2 KNN /Database
    KITTI [[142](#bib.bib142)]
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 孪生网络对比损失函数[[143](#bib.bib143)] L2 KNN /数据库 KITTI [[142](#bib.bib142)]
- en: inhouse dataset [[144](#bib.bib144)] Siamese network Contrastive loss [[143](#bib.bib143)]
    L2 KNN /Database KITTI [[142](#bib.bib142)]; inhouse dataset; RADAR [[81](#bib.bib81)]
    VGG-16 + NetVLAD layer Triplet loss KNN /Database Oxford Radar RobotCar [[6](#bib.bib6)]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 内部数据集 [[144](#bib.bib144)] 孪生网络对比损失 [[143](#bib.bib143)] L2 KNN /数据库 KITTI [[142](#bib.bib142)];
    内部数据集; RADAR [[81](#bib.bib81)] VGG-16 + NetVLAD 层 三元组损失 KNN /数据库 牛津 Radar RobotCar
    [[6](#bib.bib6)]
- en: IV-B End-to-End Frameworks
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 端到端框架
- en: 'Conversely to pre-trained frameworks, end-to-end frameworks resort to machine
    learning approaches that learn the feature representation and obtain a descriptor
    directly from the sensor data while training on a place recognition task. A key
    aspect of end-to-end learning is concerned with the definition of the training
    objective: i.e., what are the networks optimized for, and how are they optimized.
    In place recognition, networks are mostly optimized to generate unique descriptors
    that can identify the same physical place regardless of the appearance or viewpoint.
    The achievement of such an objective is determined by selecting, for the task
    in hands, an adequate network , and adequate network training, which depends on
    the loss function.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与预训练框架相反，端到端框架利用机器学习方法，从传感器数据中直接学习特征表示并获取描述符，同时进行位置识别任务的训练。端到端学习的一个关键方面是定义训练目标：即网络优化的内容及其优化方式。在位置识别中，网络通常被优化以生成能够识别相同物理位置的唯一描述符，无论其外观或视角如何。实现这一目标依赖于为当前任务选择合适的网络和适当的网络训练，这取决于损失函数。
- en: IV-B1 Loss functions
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 损失函数
- en: 'The loss function is in particular a major concern in the training phase, since
    it represents the matematical interpretation of the training objective, and thus
    determining the successful convergence of the optimization process. In place recognition
    loss functions include triplet-based [[24](#bib.bib24), [96](#bib.bib96), [137](#bib.bib137),
    [65](#bib.bib65), [34](#bib.bib34), [81](#bib.bib81), [50](#bib.bib50)], margin-based[[25](#bib.bib25)],
    quadruplet-based [[49](#bib.bib49)], and contrastive-based [[13](#bib.bib13)].
    Figure [6](#S4.F6 "Figure 6 ‣ IV-A2 Landmark-based ‣ IV-A Pre-trained-based Frameworks
    ‣ IV Supervised place recognition ‣ Place recognition survey: An update on deep
    learning approaches") illustrates the various training strategies of the loss
    functions.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数在训练阶段尤其重要，因为它代表了训练目标的数学解释，从而决定了优化过程的成功收敛。在位置识别中，损失函数包括基于三元组的[[24](#bib.bib24),
    [96](#bib.bib96), [137](#bib.bib137), [65](#bib.bib65), [34](#bib.bib34), [81](#bib.bib81),
    [50](#bib.bib50)]，基于边际的[[25](#bib.bib25)]，基于四元组的[[49](#bib.bib49)]，以及基于对比的[[13](#bib.bib13)]。图[6](#S4.F6
    "图 6 ‣ IV-A2 基于标志点 ‣ IV-A 预训练基础框架 ‣ IV 监督位置识别 ‣ 位置识别调查：关于深度学习方法的更新") 说明了损失函数的各种训练策略。
- en: 'The contrastive loss is used in siamese networks [[13](#bib.bib13), [143](#bib.bib143)],
    which have two branches with shared parameters. This function computes the similarity
    distance between the output descriptors of the branches, forcing the netwroks
    to decrease the distance between positive pairs (input data from the same place)
    and increase the distance between negative pairs. The function can be described
    as follow:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失在孪生网络中使用[[13](#bib.bib13), [143](#bib.bib143)]，这些网络有两个分支，具有共享的参数。该函数计算两个分支输出描述符之间的相似性距离，迫使网络减少正样本对（来自同一位置的输入数据）之间的距离，并增加负样本对之间的距离。该函数可以描述如下：
- en: '|  | $\begin{gathered}L=\frac{1}{2}YD^{2}+\frac{1}{2}(1-Y)\max(0,m-D)^{2}\end{gathered}$
    |  | (1) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}L=\frac{1}{2}YD^{2}+\frac{1}{2}(1-Y)\max(0,m-D)^{2}\end{gathered}$
    |  | (1) |'
- en: where $D=||R_{a}-R_{x}||_{2}$ represents the Euclidean distance between the
    descriptor representation from the branch of the anchor image ($R_{a}$) and the
    descriptor representation from the other branch ($R_{x}$). While $m$ represents
    a margin parameter, $Y$ represents the label, where $Y=1$ refers to a positive
    pair and $Y=0$ otherwise.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D=||R_{a}-R_{x}||_{2}$ 表示锚点图像分支（$R_{a}$）和另一分支（$R_{x}$）之间的欧几里得距离。$m$ 表示边际参数，$Y$
    表示标签，其中 $Y=1$ 代表正样本对，$Y=0$ 代表负样本对。
- en: 'Similar to the former loss, the triplet loss also relies on more than one branch
    during training. However, instead of computing the distance between positive or
    negative pairs at each iteration, the triplet loss function computes the distance
    between a positive and a negative pair at the same iteration, relying, thus, on
    three branches. As in the former loss function, the objective is to train a network
    to keep positive pairs close and negative pairs apart. The Triplet loss function
    can be formulated as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的损失类似，三元组损失在训练过程中也依赖于多个分支。然而，不同于每次迭代计算正负样本对之间的距离，三元组损失函数在同一次迭代中计算正样本对和负样本对之间的距离，从而依赖于三个分支。与前面的损失函数一样，目标是训练一个网络，使正样本对保持接近，负样本对远离。三元组损失函数可以如下表示：
- en: '|  | $L=\max(0,D_{p}^{2}-D_{n}^{2}+m)$ |  | (2) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=\max(0,D_{p}^{2}-D_{n}^{2}+m)$ |  | (2) |'
- en: where $D_{p}$ refers to the distance between positive pairs (i.e., between anchor
    and positive sample) and $D_{n}$ refers to the distance between the negative pair.
    This function is widely used in place recognition, namely in frameworks that use
    input data from the camera, 3d LiDARs, and RADARs, which adapt the function to
    fit the training requirements. Loss functions that drive from the triplet loss
    include Lazy triplet [[65](#bib.bib65)], weighted triplet loss [[96](#bib.bib96)]
    and weakly supervised triplet ranking loss [[24](#bib.bib24)].
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{p}$ 指正样本对之间的距离（即锚点与正样本之间的距离），$D_{n}$ 指负样本对之间的距离。这个函数广泛应用于位置识别，即在使用来自相机、3D
    LiDAR 和 RADAR 的输入数据的框架中，这些框架将该函数调整以适应训练需求。从三元组损失推导出的损失函数包括 Lazy triplet [[65](#bib.bib65)]、加权三元组损失
    [[96](#bib.bib96)] 和弱监督三元组排序损失 [[24](#bib.bib24)]。
- en: 'The quadruplet is an extension of the triplet loss, introducing an additional
    constraint to push the negative pairs [[145](#bib.bib145)] from the positives
    pairs w.r.t different probe samples, while triplet loss only pushes the negatives
    from the positives w.r.t from the same probe. The additional constrain of the
    quadruplet loss reduces the intra-class variations and enlarges the inter-class
    variations. This function is formulated as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 四元组损失是三元组损失的扩展，引入了附加约束，以推动来自不同探测样本的负样本对 [[145](#bib.bib145)] 与正样本对的距离，而三元组损失仅推动来自相同探测样本的负样本对与正样本对的距离。四元组损失的附加约束减少了类内变异并扩大了类间变异。这个函数可以表示为：
- en: '|  | $\begin{gathered}L=\max(0,D_{p}^{2}-D_{n}^{2}+m_{1})+\max(0,D_{p}^{2}-D_{d}^{2}+m_{2})\end{gathered}$
    |  | (3) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}L=\max(0,D_{p}^{2}-D_{n}^{2}+m_{1})+\max(0,D_{p}^{2}-D_{d}^{2}+m_{2})\end{gathered}$
    |  | (3) |'
- en: where $D_{p}$ and $D_{n}$ represent the distance between the positive and negative
    pairs, respectively. The $m_{1}$ and $m_{2}$ represent margin parameters, while
    $R_{d}$ corresponds to the additional constraint, representing the distance between
    negative pairs from different probes. In [[49](#bib.bib49), [65](#bib.bib65)],
    the quadruplet loss function is used to train networks for the task of place recognition
    using 3D LiDAR data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{p}$ 和 $D_{n}$ 分别表示正样本对和负样本对之间的距离。$m_{1}$ 和 $m_{2}$ 表示边际参数，而 $R_{d}$ 代表附加约束，表示来自不同探测点的负样本对之间的距离。在
    [[49](#bib.bib49), [65](#bib.bib65)] 中，四元组损失函数用于训练网络以进行使用 3D LiDAR 数据的位置信息识别任务。
- en: The margin-based loss function is a simple extension to the contrastive loss
    [[129](#bib.bib129)]. While the contrastive function enforces the positive pairs
    to be as close as possible, the margin-based function only encourages the positive
    pairs to be within a distance of each other.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基于边际的损失函数是对对比损失 [[129](#bib.bib129)] 的简单扩展。对比函数强制正样本对尽可能接近，而基于边际的函数则只鼓励正样本对在彼此的距离范围内。
- en: '|  | $L=max(0,\alpha+Y(D-m))$ |  | (4) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=\max(0,\alpha+Y(D-m))$ |  | (4) |'
- en: where $Y\in{1,-1}$ represents the labels of $Y=1$ when the pair is positive
    and $Y=-1$ otherwise. $\alpha$ is a variable that determines the boundary between
    positive and negative pairs. The margin-based loss function was proposed in [[129](#bib.bib129)]
    to demonstrate that state-of-art performance could be achieved with a simple loss
    function, only by having an adequate sampling strategy of the input data during
    training. This function is used in [[25](#bib.bib25)] to train a multi-modal network.
    The network is jointly trained based on information extracted from images and
    structural data in the format of voxel grids, which are generated from the images.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Y\in{1,-1}$ 表示当对是正样本时$Y=1$，否则$Y=-1$。$\alpha$ 是一个决定正负样本边界的变量。边际损失函数在 [[129](#bib.bib129)]
    中提出，以证明只通过在训练过程中对输入数据进行适当的采样，就可以用一个简单的损失函数实现最先进的性能。这个函数在 [[25](#bib.bib25)] 中用于训练多模态网络。该网络基于从图像和以体素网格格式生成的结构数据中提取的信息进行联合训练。
- en: IV-B2 Camera-based Networks
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 基于相机的网络
- en: A key contribution to supervised end-to-end-based place recognition is the NetVLAD
    layer [[24](#bib.bib24)]. Inspired by the Vector of Locally Aggregated Descriptors
    (VLAD) [[146](#bib.bib146)], Arandjelović et al. [[24](#bib.bib24)] propose NetVLAD
    as a ‘pluggable’ layer into any CNN architecture to output a compact image descriptor.
    The network’s parameters are learned using a weakly supervised triplet ranking
    loss function.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督的端到端基于地点识别的一个重要贡献是 NetVLAD 层 [[24](#bib.bib24)]。受到局部聚合描述符（VLAD） [[146](#bib.bib146)]
    的启发，Arandjelović 等人 [[24](#bib.bib24)] 提出了 NetVLAD 作为一种“可插拔”层，可以插入任何 CNN 架构中以输出紧凑的图像描述符。网络的参数使用弱监督的三元组排序损失函数进行学习。
- en: Yu et al. [[96](#bib.bib96)] also exploited VLAD descriptors for images, proposing
    a spatial pyramid-enhanced VLAD (SPE-VLAD) layer. The proposed layer leverages
    the spatial pyramid structure of images to enhance place description, using for
    feature extraction a VGG-16 [[120](#bib.bib120)] and a ResNet-18 [[147](#bib.bib147)],
    and as loss function the weighted T-loss. The network’s parameters are learned
    under weakly supervised scenarios, using GPS tags and the Euclidean distance between
    the image representations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 于等人 [[96](#bib.bib96)] 也利用了图像的 VLAD 描述符，提出了一种空间金字塔增强的 VLAD (SPE-VLAD) 层。该层利用图像的空间金字塔结构来增强地点描述，使用
    VGG-16 [[120](#bib.bib120)] 和 ResNet-18 [[147](#bib.bib147)] 进行特征提取，并采用加权 T 损失作为损失函数。网络的参数在弱监督场景下学习，使用
    GPS 标签和图像表示之间的欧几里得距离。
- en: Qiu et al. [[131](#bib.bib131)] apply a siamese-based network to loop closure
    detection. Siamese networks are twin neural networks, which share the same parameters
    and are particularly useful when limited training data is available. Both sub-networks
    share the same parameters and mirror the update of the parameters. In this work,
    the sub-networks are replaced by ResNet to improve feature representation, and
    the network is trained, resorting to an L2-based loss function as in [[132](#bib.bib132)].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 丘等 [[131](#bib.bib131)] 将一种基于孪生网络的方法应用于环闭检测。孪生网络是双胞神经网络，它们共享相同的参数，在训练数据有限的情况下特别有用。两个子网络共享相同的参数并镜像参数的更新。在这项工作中，子网络被
    ResNet 替代，以提高特征表示，并且网络使用 L2 基础的损失函数进行训练，如 [[132](#bib.bib132)] 所述。
- en: 'Wu et al. [[50](#bib.bib50)] jointly addresses the place recognition problem
    from the efficiency and performance perspective, proposing to this end a deep
    supervised hashing approach with a similar hierarchy. Hashing is an encoding technique
    that maps high dimensional data into a set of binary codes, having low computational
    requirements and high storage efficiency. The proposed framework comprises three
    modules: features extraction based on MobileNet [[134](#bib.bib134)]; hash code
    learning, obtained using the last fully connected layer of MobileNet; and loss
    function, which is based on the likelihood [[135](#bib.bib135), [136](#bib.bib136)].
    This work proposes a similar hierarchy method to distinguish similar images. To
    this end, the distance of hashing codes between a pair of images must increase
    as similar images are more distinct and must remain the same between different
    images. These two conditions are essential to use deep supervised hashing in place
    recognition.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人[[50](#bib.bib50)] 共同从效率和性能的角度解决了位置识别问题，为此提出了一种具有类似层次结构的深度监督哈希方法。哈希是一种将高维数据映射到一组二进制代码的编码技术，具有低计算需求和高存储效率。所提框架包含三个模块：基于
    MobileNet [[134](#bib.bib134)] 的特征提取；通过 MobileNet 的最后一层全连接层获得的哈希码学习；以及基于似然性的损失函数[[135](#bib.bib135),
    [136](#bib.bib136)]。这项工作提出了一种类似的层次方法来区分相似图像。为此，图像对之间的哈希码距离必须随着相似图像的差异增大而增加，并且在不同图像之间保持不变。这两个条件对于在位置识别中使用深度监督哈希至关重要。
- en: Another efficiency improving technique for deep networks is network pruning.
    This technique aims to reduce the size of the network by removing unnecessary
    neurons or setting the weights to zero [[148](#bib.bib148)]. Hausler et al. [[137](#bib.bib137)]
    propose a feature filtering approach, which removes feature maps at the beginning
    of the network while using for matching late feature maps to foster efficiency
    and performance simultaneously. Feature maps to be removed are determined based
    on a Triplet Loss calibration procedure. As a feature extraction framework, the
    approach uses the HybridNet [[138](#bib.bib138)].
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种提高深度网络效率的技术是网络剪枝。这种技术旨在通过去除不必要的神经元或将权重设置为零来减少网络的大小[[148](#bib.bib148)]。Hausler
    等人[[137](#bib.bib137)] 提出了一种特征过滤方法，该方法在网络的开始阶段去除特征图，同时在后续阶段使用用于匹配的特征图，以提高效率和性能。待去除的特征图是基于
    Triplet Loss 校准程序确定的。作为特征提取框架，该方法使用了 HybridNet [[138](#bib.bib138)]。
- en: 'Contrary to former single modality works, Oertel et al. [[25](#bib.bib25)]
    propose a place description approach that uses vision and structural information,
    both originated from camera data. This approach jointly uses vision and depth
    data from a stereo camera in an end-to-end pipeline. The structural information
    is first obtained utilizing the Direct Sparse Odometry (DSO) framework [[149](#bib.bib149)]
    and then discretized into regular voxel grids to serve as inputs along with the
    corresponding image. The pipeline has two parallel branches: one for vision and
    another for the structural data, which use 2D and 3D convolutional layers, respectively,
    for feature extraction. Both branches are learned jointly through a margin-based
    loss function. The outputs of the branches are concatenated into a single vector,
    which is fed to a fusion network that outputs the descriptor.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的单一模态工作相反，Oertel 等人[[25](#bib.bib25)] 提出了一种利用来自摄像头数据的视觉和结构信息的地方描述方法。这种方法在端到端的管道中联合使用来自立体摄像机的视觉和深度数据。结构信息首先通过
    Direct Sparse Odometry (DSO) 框架 [[149](#bib.bib149)] 获得，然后离散化为规则的体素网格，以作为输入与相应的图像一起使用。管道有两个并行分支：一个用于视觉，另一个用于结构数据，分别使用
    2D 和 3D 卷积层进行特征提取。两个分支通过基于边距的损失函数联合学习。分支的输出被连接成一个向量，输入到融合网络中，最终输出描述符。
- en: IV-B3 3D LiDAR-based Network
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B3 3D LiDAR基础网络
- en: Although NetVLAD was originally used for images, it has also been used on 3D
    LiDAR data [[65](#bib.bib65), [49](#bib.bib49)]. Uy et al. [[65](#bib.bib65)]
    and Liu et al. [[49](#bib.bib49)] propose respectively PointNetVLAD and LPD-Net,
    which are NetVLAD-based global descriptor learning approaches for 3D LiDAR data.
    Both have compatible inputs and outputs, receiving as input raw point clouds and
    outputting a descriptor. The difference relies on the feature extraction and feature
    processing methods. PointNetVLAD [[65](#bib.bib65)] relies on PointNet [[150](#bib.bib150)],
    a 3D object detection and segmentation approach, for feature extraction. In contrast,
    LPD-Net relies on an adaptive local feature extraction module and a graph-based
    neighborhood aggregation module, aggregating both in the Feature Space and the
    Cartesian Space. Regarding the network training, Uy et al. [[65](#bib.bib65)]
    showed that the lazy quadruplet loss function enables higher performance than
    the lazy triplet loss function, motivating Liu et al. [[49](#bib.bib49)] to follow
    this approach.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 NetVLAD 最初用于图像，它也已被应用于 3D LiDAR 数据 [[65](#bib.bib65), [49](#bib.bib49)]。Uy
    等人 [[65](#bib.bib65)] 和 Liu 等人 [[49](#bib.bib49)] 分别提出了 PointNetVLAD 和 LPD-Net，这些都是基于
    NetVLAD 的 3D LiDAR 数据全局描述符学习方法。两者具有兼容的输入和输出，输入为原始点云，输出为描述符。其区别在于特征提取和特征处理方法。PointNetVLAD
    [[65](#bib.bib65)] 依赖于 PointNet [[150](#bib.bib150)]，这是一个用于特征提取的 3D 物体检测和分割方法。相反，LPD-Net
    依赖于自适应局部特征提取模块和基于图的邻域聚合模块，将两者分别聚合在特征空间和笛卡尔空间中。关于网络训练，Uy 等人 [[65](#bib.bib65)]
    表明，懒惰四元组损失函数比懒惰三元组损失函数具有更高的性能，这促使 Liu 等人 [[49](#bib.bib49)] 采用了这种方法。
- en: A different 3D-LiDAR-based place recognition approach is proposed in [[34](#bib.bib34)].
    Schaupp et al. propose OREOS, which is a triplet DL network-based architecture
    [[140](#bib.bib140)]. The OREOS approach receives as input 2D range images and
    outputs orientation-and place-dependent descriptors. The 2D range images are the
    result of the 3D point clouds projections onto an image representation. The network
    is trained using an L2 distance-based triplet loss function to compute the similarity
    between anchor-positive and anchor-negative. Place recognition is validated using
    a k-nearest neighbor framework for matching.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[[34](#bib.bib34)] 中提出了一种不同的基于 3D LiDAR 的地点识别方法。Schaupp 等人提出了 OREOS，这是一种基于三元组
    DL 网络的架构 [[140](#bib.bib140)]。OREOS 方法接收 2D 范围图像作为输入，并输出与方向和地点相关的描述符。这些 2D 范围图像是
    3D 点云投影到图像表示上的结果。网络使用基于 L2 距离的三元组损失函数进行训练，以计算锚点-正样本和锚点-负样本之间的相似性。地点识别通过 k 最近邻框架进行验证。'
- en: Yin et al. [[13](#bib.bib13)] uses 3D point clouds to address the global localization
    problem, proposing a place recognition and metric pose estimation approach. Place
    recognition is achieved using the siamese LocNets, which is a semi-handcrafted
    representation learning method for LiDAR point clouds. As input, LocNets receives
    a handcrafted rotational invariant representation extracted from point clouds
    in a pre-processing step and outputs a low-dimensional fingerprint. The network
    follows a Siamese architecture and uses for learning Euclidean distance-based
    contrastive loss function [[143](#bib.bib143)]. For belief generation, an L2-based
    KNN approach is used. A similar LocNets-based approach is proposed in [[144](#bib.bib144)].
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Yin 等人 [[13](#bib.bib13)] 使用 3D 点云解决全局定位问题，提出了一种地点识别和度量姿态估计方法。地点识别通过 siamese
    LocNets 实现，这是一种针对 LiDAR 点云的半手工表示学习方法。作为输入，LocNets 接收从点云中提取的手工旋转不变表示，并输出低维指纹。网络遵循
    Siamese 架构，并使用基于欧几里得距离的对比损失函数 [[143](#bib.bib143)] 进行学习。在信念生成方面，使用基于 L2 的 KNN
    方法。[[144](#bib.bib144)] 中提出了一种类似的基于 LocNets 的方法。
- en: IV-B4 RADAR-based
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B4 基于雷达
- en: Regarding RADAR-based place recognition, Saftescu et al. [[95](#bib.bib95)]
    also propose a NetVLAD-based approach to map FMCW RADAR scans to a descriptor
    space. Features are extracted using a specially tailored CNN based on cylindrical
    convolutions, anti-aliasing blurring, and azimuth-wise max-pooling to bolster
    the rotational invariance of polar radar images. Regarding training, the network
    uses a triplet loss function as proposed in [[151](#bib.bib151)].
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于雷达的地点识别，Saftescu 等人 [[95](#bib.bib95)] 还提出了一种基于 NetVLAD 的方法，将 FMCW 雷达扫描映射到描述符空间。特征通过基于圆柱卷积、抗混叠模糊和方位角最大池化的特别定制的
    CNN 提取，以增强极坐标雷达图像的旋转不变性。关于训练，网络使用了 [[151](#bib.bib151)] 中提出的三元组损失函数。
- en: V Unsupervised Place Recognition
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 无监督地点识别
- en: The aforementioned supervised learning approaches achieve excellent results
    in learning discriminative place models. However, these methods have the inconvenience
    of requiring a vast amount of labeled data to perform well, as it is common in
    supervised DL-based approaches. Contrary to supervised, unsupervised learning
    does not require labeled data, an advantage when annotated data are not available
    or scarce.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 上述监督学习方法在学习区分性场所模型方面取得了优秀的成果。然而，这些方法有一个不便之处，即需要大量标记数据才能表现良好，这在基于监督的深度学习方法中是常见的。与监督学习不同，无监督学习不需要标记数据，这在标注数据不可用或稀缺时具有优势。
- en: Place recognition works use unsupervised approaches such as Generative Adversarial
    Networks (GAN) for domain translation [[152](#bib.bib152)]. An example of such
    an approach is proposed by Latif et al. [[152](#bib.bib152)], which address the
    cross-season place recognition problem as a domain translation task. GANS are
    used to learn the relationship between two domains without requiring cross-domain
    image correspondences. The proposed architecture is presented as two coupled GANs.
    The generator integrated an encoder-decoder network, while the discriminator integrates
    an encoder network followed by two fully connected layers. The output of the discriminator
    is used as a descriptor for place recognition. Authors show that the discriminator’s
    feature space is more informative than image pixels translated to the target domain.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 场所识别工作使用了无监督的方法，例如生成对抗网络（GAN）进行领域翻译 [[152](#bib.bib152)]。Latif等人 [[152](#bib.bib152)]
    提出的一个例子将跨季节场所识别问题视为领域翻译任务。GAN用于学习两个领域之间的关系，而不需要跨领域图像对应。所提出的架构呈现为两个耦合的GAN。生成器集成了一个编码-解码网络，而判别器集成了一个编码网络，并跟随两个全连接层。判别器的输出被用作场所识别的描述符。作者展示了判别器的特征空间比翻译到目标领域的图像像素更具信息性。
- en: Yin et al. [[153](#bib.bib153)] also proposes a GAN-based approach, but for
    3D LiDAR-based. LiDAR data are first mapped into dynamic octree maps, from which
    bird-view images are extracted. These images are used in a GAN-based pipeline
    to learn stable and generalized place features. The network trained using adversarial
    and conditional entropy strategies to produce a higher generalization ability
    and capture the unique mapping between the original data space and the compressed
    latent code space.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Yin等人 [[153](#bib.bib153)] 也提出了一种基于GAN的方法，但用于3D LiDAR数据。LiDAR数据首先映射到动态八叉树图中，从中提取鸟瞰图像。这些图像用于基于GAN的流程，以学习稳定且通用的场所特征。该网络通过对抗和条件熵策略进行训练，以产生更高的泛化能力，并捕捉原始数据空间与压缩潜在编码空间之间的独特映射。
- en: Han et al.,(2020) [[88](#bib.bib88)] propose a Multispectral Domain Invariant
    framework for the translation between unpaired RGB and thermal imagery. The proposed
    approach is based on CycleGAN [[154](#bib.bib154)], which relies, for training,
    on the single scale structural similarity index (SSIM [[155](#bib.bib155)]) loss,
    triplet loss, adversarial loss, and two types of consistency losses (cyclic loss
    [[154](#bib.bib154)] and pixel-wise loss). The proposed framework is further validated
    on semantic segmentation and domain adaptation tasks.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Han等人（2020） [[88](#bib.bib88)] 提出了一个多光谱领域不变框架，用于未配对RGB和热成像之间的翻译。所提出的方法基于CycleGAN
    [[154](#bib.bib154)]，其训练依赖于单尺度结构相似性指数（SSIM [[155](#bib.bib155)]）损失、三元组损失、对抗损失和两种类型的一致性损失（循环损失
    [[154](#bib.bib154)] 和逐像素损失）。所提出的框架在语义分割和领域适应任务中进一步验证。
- en: 'Contrary to the former works, which were mainly based on GAN approaches, Merril
    and Huang [[94](#bib.bib94)] propose, for visual loop closure, an autoencoder-based
    approach to handle the feature embedding. Instead of reconstructing original images,
    this unsupervised approach is specifically tailored to map images to a HOG descriptor
    space. The autoencoder network is trained, having as input a pre-processing stage,
    where two classical geometric vision techniques are exploited: histogram of oriented
    gradients (HOG) [[156](#bib.bib156)], and the projective transformation (homography)
    [[157](#bib.bib157)]. HOG enables the compression of images while preserving salient
    features. On the other hand, the projective transformation allows the relation
    of images with differing viewpoints. The network has a minimal architecture, enabling
    fast and reliable close-loop detection in real-time with no dimensionality reduction.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前主要基于 GAN 方法的工作不同，Merril 和 Huang [[94](#bib.bib94)] 提出了一个基于自编码器的方法来处理特征嵌入，用于视觉回环闭合。该无监督方法专门用于将图像映射到
    HOG 描述符空间，而不是重建原始图像。自编码器网络经过训练，输入为一个预处理阶段，其中利用了两种经典的几何视觉技术：方向梯度直方图（HOG） [[156](#bib.bib156)]
    和投影变换（单应性） [[157](#bib.bib157)]。HOG 能够在保留显著特征的同时压缩图像。另一方面，投影变换允许将不同视点的图像关联起来。该网络具有最小架构，能够在实时中快速可靠地进行闭环检测，而无需降维。
- en: 'TABLE IV: Summary of recent works using unsupervised end-to-end learning techniques
    for place recognition.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：总结了使用无监督端到端学习技术进行地点识别的近期工作。
- en: 'Sensor Ref Architecture Loss Function Task Dataset Camera [[152](#bib.bib152)]
    Architecture: Coupled GANs + encoder-decoder network Minimization of the cyclic
    reconstruction loss [[158](#bib.bib158)] Domain translation for cross domain place
    recognition Norland [[2](#bib.bib2)]; Camera [[94](#bib.bib94)] Pre-processing:
    HOG [[156](#bib.bib156)] and homography [[157](#bib.bib157)]'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器 参考 架构 损失函数 任务 数据集 摄像头 [[152](#bib.bib152)] 架构：耦合 GANs + 编码器-解码器网络 循环重建损失最小化
    [[158](#bib.bib158)] 跨域地点识别的领域翻译 Norland [[2](#bib.bib2)]; 摄像头 [[94](#bib.bib94)]
    预处理：HOG [[156](#bib.bib156)] 和单应性 [[157](#bib.bib157)]
- en: 'Architecture: small Autoencoder L2 loss function Unsupervised feature embedding
    for visual loop closure Places [[110](#bib.bib110)]; KITTI [[142](#bib.bib142)];
    Alderley [[42](#bib.bib42)]; Norland [[2](#bib.bib2)]; Gardens Point [[22](#bib.bib22)];
    RGB + Thermal [[88](#bib.bib88)] Multispectral Domain Invariant model'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 架构：小型自编码器 L2 损失函数 无监督特征嵌入用于视觉回环闭合 Places [[110](#bib.bib110)]; KITTI [[142](#bib.bib142)];
    Alderley [[42](#bib.bib42)]; Norland [[2](#bib.bib2)]; Gardens Point [[22](#bib.bib22)];
    RGB + 热成像 [[88](#bib.bib88)] 多光谱领域不变模型
- en: 'Architecture: CycleGAN [[154](#bib.bib154)] SSIM [[155](#bib.bib155)] + triplet
    + adversarial + cyclic loss [[154](#bib.bib154)] + pixel-wise loss Unsupervised
    multispectral imagery translation task KAIST [[159](#bib.bib159)]; 3D LiDAR [[152](#bib.bib152)]
    Pre-processing: Mapping LiDAR to dynamic octree maps to bird-view images'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 架构：CycleGAN [[154](#bib.bib154)] SSIM [[155](#bib.bib155)] + 三元组 + 对抗 + 循环损失
    [[154](#bib.bib154)] + 像素级损失 无监督多光谱图像翻译任务 KAIST [[159](#bib.bib159)]; 3D LiDAR
    [[152](#bib.bib152)] 预处理：将 LiDAR 映射到动态八叉树地图和鸟瞰图像
- en: 'Architecture: GAN + encoder-decoder network Adversarial learning and conditional
    entropy Unsupervised Feature learning for a 3D LiDAR-based place recognition task
    KITTI [[142](#bib.bib142)];'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 架构：GAN + 编码器-解码器网络 对抗学习和条件熵 无监督特征学习用于基于 3D LiDAR 的地点识别任务 KITTI [[142](#bib.bib142)];
- en: NCTL [[141](#bib.bib141)];
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: NCTL [[141](#bib.bib141)];
- en: 'TABLE V: Recent works that combine supervised and unsupervised learning in
    place recognition systems.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：结合有监督和无监督学习的地点识别系统的近期工作。
- en: 'Sensor Ref Architecture Loss Function Task Dataset Camera [[160](#bib.bib160)]
    Feature extraction: AlexNet cropped(conv5)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器 参考 架构 损失函数 任务 数据集 摄像头 [[160](#bib.bib160)] 特征提取：AlexNet 剪裁（conv5）
- en: 'Supervised: VLAD + attention module'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督：VLAD + 注意力模块
- en: 'Unsupervised: domain adaptation Supervised: triplet ranking;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督：领域适应 有监督：三元组排序；
- en: 'Unsupervised: MK-MMD [[161](#bib.bib161)] Single and cross-domain VPR Mapillary
    ¹¹1https://www.mapillary.com'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督：MK-MMD [[161](#bib.bib161)] 单域和跨域 VPR Mapillary ¹¹1https://www.mapillary.com
- en: 'Beeldbank ²²2https://beeldbank.amsterdam.nl/beeldbank [[162](#bib.bib162)]
    Supervised: adversarial learning'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Beeldbank ²²2https://beeldbank.amsterdam.nl/beeldbank [[162](#bib.bib162)] 有监督：对抗学习
- en: 'Unsupervised: autoencoder Adversarial Learning: Least square [[163](#bib.bib163)];'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督：自编码器 对抗学习：最小二乘法 [[163](#bib.bib163)];
- en: 'Reconstruction: L2 distance Disentanglement of place and appearance features
    in a cross domain VPR Nordland [[2](#bib.bib2)];'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 重建：L2距离 跨领域VPR Nordland中的地点和外观特征解耦[[2](#bib.bib2)];
- en: 'Alderley [[42](#bib.bib42)]; 3D LiDAR [[77](#bib.bib77)] Supervised: latent
    space + classification network'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Alderley[[42](#bib.bib42)]; 3D LiDAR[[77](#bib.bib77)] 监督：潜在空间+分类网络
- en: 'Unsupervised: Autoencoder-like network Classification: softmax cross entropy
    [[164](#bib.bib164)]'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督：类似自编码器的网络 分类：softmax交叉熵[[164](#bib.bib164)]
- en: 'Reconstruction: binary cross entropy [[165](#bib.bib165)] Global localization,
    3D dense map reconstruction, and semantic information extraction KITTI odometry
    [[142](#bib.bib142)];'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 重建：二元交叉熵[[165](#bib.bib165)] 全球定位、3D稠密地图重建和语义信息提取 KITTI里程计[[142](#bib.bib142)];
- en: VI Semi-supervised Place Recognition
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 半监督地点识别
- en: 'In this work, Semi-supervised approaches refer to works that jointly rely on
    supervised and unsupervised methods. The combination of these two learning approaches
    is particularly used for the cross-domain problem. However, rather than translating
    one domain to another, these learning techniques are used to learn features that
    are independent of the domain appearance. A summary of recent works is presented
    in Table [V](#S5.T5 "TABLE V ‣ V Unsupervised Place Recognition ‣ Place recognition
    survey: An update on deep learning approaches").'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，半监督方法指的是那些同时依赖于监督和无监督方法的工作。这两种学习方法的结合特别用于跨领域问题。然而，与其将一个领域转化为另一个领域，不如利用这些学习技术学习与领域外观无关的特征。最近工作的总结见表[V](#S5.T5
    "TABLE V ‣ V 无监督地点识别 ‣ 地点识别调查：深度学习方法的更新")。
- en: 'To learn domain-invariant features for cross-domain visual place recognition,
    Wang et al. [[160](#bib.bib160)] propose an approach that combines weakly supervised
    learning with unsupervised learning. The proposed architecture has three primary
    modules: an attention module, an attention-aware VLAD module, and a domain adaptation
    module. The supervised branch is trained with a triplet ranking loss function,
    while the unsupervised branch resorts to a multi-kernel maximum mean discrepancy
    (MK-MMD) loss function.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习跨领域视觉地点识别的领域不变特征，王等人[[160](#bib.bib160)]提出了一种将弱监督学习与无监督学习相结合的方法。该架构主要包含三个模块：一个注意力模块、一个注意力感知VLAD模块和一个领域适配模块。监督分支使用三重排序损失函数进行训练，而无监督分支则采用多核最大均值差异（MK-MMD）损失函数。
- en: 'On the other hand, Tang et al. [[162](#bib.bib162)] propose a self-supervised
    learning approach to disentangle place-rated features from domain-related features.
    The backbone architecture of the proposed approach is a modified autoencoder for
    adversarial learning: i.e., two input encoder branches converging into one output
    decoder. The disentanglement of the two feature domains is solved through adversarial
    learning, which constrains the learning of domain specific features (i.e., features
    depending on the appearance); a task that is not guaranteed by the reconstruction
    loss of autoencoders. For adversarial learning, the proposed loss function is
    the least square adversarial loss [[163](#bib.bib163)]; while for reconstruction,
    the loss function is the L2 distance.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，唐等人[[162](#bib.bib162)]提出了一种自监督学习方法，用于将地点相关特征与领域相关特征分离。该方法的主干架构是一个修改版的自编码器，用于对抗学习：即两个输入编码器分支汇聚到一个输出解码器中。两个特征领域的解耦通过对抗学习解决，这限制了领域特定特征的学习（即，依赖于外观的特征）；这是自编码器的重建损失无法保证的任务。对于对抗学习，提出的损失函数是最小二乘对抗损失[[163](#bib.bib163)]；而对于重建，损失函数是L2距离。
- en: 'Dubé et al. [[77](#bib.bib77)] propose SegMap, an data-driven learning approach
    for the task of localization and mapping. The approach uses as the main framework
    an autoencoder-like architecture to learn object segments of 3D point clouds.
    The framework is used for two tasks: (supervised) classification and (unsupervised)
    reconstruction. The work proposes a customized learning technique to train the
    network, which comprises, for classification, the softmax cross-entropy loss function
    in conjunction with the N-ways classification problem learning technique [[164](#bib.bib164)],
    and, for reconstruction, the binary cross-entropy loss function [[165](#bib.bib165)].
    The latent space, which is jointly learned on the two tasks, is used as a descriptor
    for segment retrieval. The proposed framework can be used in global localization,
    3D dense map reconstruction, and semantic information extraction tasks.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Dubé 等人[[77](#bib.bib77)] 提出了 SegMap，这是一种数据驱动的学习方法，用于定位和映射任务。该方法以类似自编码器的架构为主要框架，学习
    3D 点云的物体分段。该框架用于两个任务：（监督式）分类和（无监督式）重建。该工作提出了一种定制的学习技术来训练网络，其中，分类部分采用 softmax 交叉熵损失函数结合
    N-way 分类问题学习技术[[164](#bib.bib164)]，重建部分则采用二进制交叉熵损失函数[[165](#bib.bib165)]。在两个任务上联合学习的潜在空间被用作分段检索的描述符。所提出的框架可用于全局定位、3D
    密集地图重建和语义信息提取任务。
- en: VII Other Frameworks
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 其他框架
- en: '![Refer to caption](img/227e19af188ef84b62ff305cec8d457a.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/227e19af188ef84b62ff305cec8d457a.png)'
- en: 'Figure 7: Block diagram of a) hierarchical b) and c) parallel place recognition
    frameworks. The example in b) fuses the descriptors, while in c) the belief scores
    are fused.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：a) 层次化框架，b) 和 c) 并行位置识别框架的框图。b) 中的示例融合了描述符，而 c) 中则融合了置信度评分。
- en: 'This section is dedicated to the frameworks that have more complex and entangled
    architectures: i.e., containing more than one place recognition approach for the
    purpose of finding the best loop candidates. Two main frameworks are highlighted:
    parallel and hierarchical. While parallel frameworks have a very defined structure,
    hierarchical frameworks may assume very complex and entangled configurations,
    but both frameworks have the end goal of representing more performant place recognition
    methods.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本节专门讨论那些具有更复杂和纠缠架构的框架：即，包含多个位置识别方法以寻找最佳的循环候选项。突出介绍了两个主要框架：并行和层次化。虽然并行框架具有非常明确的结构，层次化框架可能会假设非常复杂和纠缠的配置，但两个框架的最终目标都是表示更高效的地点识别方法。
- en: VII-A Parallel Frameworks
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 并行框架
- en: 'Parallel frameworks refer to approaches that rely on multiple information streams,
    which are fused into one branch to generate place recognition decisions. These
    parallel architectures fuse the various branches utilizing methods such as feature
    concatenation [[44](#bib.bib44)], HMM [[166](#bib.bib166)] or multiplying normalized
    data across Gaussian-distributed clusters [[167](#bib.bib167)]. Approaches such
    as proposed by Oertel et al. [[25](#bib.bib25)], where vision and structural data
    are fused in an end-to-end fashion, are considered to belong to the Section [IV-B](#S4.SS2
    "IV-B End-to-End Frameworks ‣ IV Supervised place recognition ‣ Place recognition
    survey: An update on deep learning approaches") because features are jointly learned
    in an end-to-end pipeline. An example of parallel frameworks is illustrated in
    Fig. [7](#S7.F7 "Figure 7 ‣ VII Other Frameworks ‣ Place recognition survey: An
    update on deep learning approaches"), and a summary of recent works is presented
    in Table [VI](#S7.T6 "TABLE VI ‣ VII-A Parallel Frameworks ‣ VII Other Frameworks
    ‣ Place recognition survey: An update on deep learning approaches").'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '并行框架指的是依赖于多个信息流的方法，这些信息流被融合到一个分支中以生成位置识别决策。这些并行架构通过特征连接[[44](#bib.bib44)]、HMM[[166](#bib.bib166)]或在高斯分布簇上乘以归一化数据[[167](#bib.bib167)]等方法来融合各个分支。诸如
    Oertel 等人提出的方法[[25](#bib.bib25)]，其中视觉和结构数据以端到端的方式融合，因特征在端到端的管道中联合学习而被视为属于第 [IV-B](#S4.SS2
    "IV-B End-to-End Frameworks ‣ IV Supervised place recognition ‣ Place recognition
    survey: An update on deep learning approaches")节。并行框架的示例见图 [7](#S7.F7 "Figure
    7 ‣ VII Other Frameworks ‣ Place recognition survey: An update on deep learning
    approaches")，最近工作的总结见表 [VI](#S7.T6 "TABLE VI ‣ VII-A Parallel Frameworks ‣ VII
    Other Frameworks ‣ Place recognition survey: An update on deep learning approaches")。'
- en: 'Relying on multiple information streams allows overcoming individual sensory
    data limitations, which can be, for instance, due to environment changing conditions.
    Zhang et al. [[44](#bib.bib44)] address the loop closure detection problem under
    strong perceptual aliasing and appearance variations, proposing Robust Multimodal
    Sequence-based (ROMS). ROMS concatenates LDB features [[168](#bib.bib168)], GIST
    features[[169](#bib.bib169)], CNN-based deep features[[170](#bib.bib170)] and
    ORB local features [[171](#bib.bib171)] in a single vector. A similar (parallel)
    architecture is proposed by Hausler et al. [[166](#bib.bib166)], where an approach,
    called Multi-Process Fusion, fuses four image processing methods: SAD with patch
    normalization [[42](#bib.bib42), [172](#bib.bib172)]; HOG [[173](#bib.bib173),
    [174](#bib.bib174)]; multiple spatial regions of CNN features [[138](#bib.bib138),
    [175](#bib.bib175)]; and spatial coordinates of maximum CNN activations [[41](#bib.bib41)].
    However, instead of fusing all features to generate one descriptor as proposed
    in [[44](#bib.bib44)], here, each feature stream is matched separately using cosine
    distance, and only the resulting similarity values are fused using the Hidden
    Markov model.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖于多个信息流可以克服单一感官数据的局限性，例如，由于环境变化条件的影响。张等人[[44](#bib.bib44)] 解决了在强感知别名和外观变化下的回环闭合检测问题，提出了基于鲁棒多模态序列（ROMS）。ROMS
    将 LDB 特征[[168](#bib.bib168)]、GIST 特征[[169](#bib.bib169)]、基于 CNN 的深度特征[[170](#bib.bib170)]
    和 ORB 局部特征[[171](#bib.bib171)] 连接在一个向量中。Hausler 等人[[166](#bib.bib166)] 提出了类似的（并行）架构，其中一种称为多进程融合的方法融合了四种图像处理方法：带补丁归一化的
    SAD [[42](#bib.bib42), [172](#bib.bib172)]; HOG [[173](#bib.bib173), [174](#bib.bib174)];
    CNN 特征的多个空间区域 [[138](#bib.bib138), [175](#bib.bib175)]; 和最大 CNN 激活的空间坐标 [[41](#bib.bib41)]。然而，与[[44](#bib.bib44)]中提出的将所有特征融合生成一个描述符的方法不同，这里每个特征流使用余弦距离单独匹配，并且仅将结果相似性值使用隐马尔可夫模型进行融合。
- en: 'TABLE VI: Summary of recent works on supervised place recognition using parallel
    frameworks. BG = Belief Generation and PM = Place mapping.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：使用并行框架的监督位置识别近期工作的总结。BG = 信念生成，PM = 位置映射。
- en: 'Sensor Ref Model Fusion BG/PM Dataset Camera [[44](#bib.bib44)] Feacture Extraction:
    LDB [[168](#bib.bib168)] + GIST [[169](#bib.bib169)] + CNN [[170](#bib.bib170)]
    + ORB [[171](#bib.bib171)] Concatenation of all features Sequence /Database St
    Lucia [[109](#bib.bib109)]; CMU-VL [[107](#bib.bib107)]; Nordland [[2](#bib.bib2)];
    [[166](#bib.bib166)] Feacture Extraction: SAD [[42](#bib.bib42), [172](#bib.bib172)]
    + HOG [[173](#bib.bib173), [174](#bib.bib174)] + spatial regions of HybridNet(Conv-5
    layer) [[138](#bib.bib138), [175](#bib.bib175)] + spatial coordinates of maximum
    activations HybridNet(Conv-5 layer) [[41](#bib.bib41)]'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器 参考 模型 融合 BG/PM 数据集 相机 [[44](#bib.bib44)] 特征提取：LDB [[168](#bib.bib168)] +
    GIST [[169](#bib.bib169)] + CNN [[170](#bib.bib170)] + ORB [[171](#bib.bib171)]
    所有特征的连接 序列 / 数据库 St Lucia [[109](#bib.bib109)]; CMU-VL [[107](#bib.bib107)]; Nordland
    [[2](#bib.bib2)]; [[166](#bib.bib166)] 特征提取：SAD [[42](#bib.bib42), [172](#bib.bib172)]
    + HOG [[173](#bib.bib173), [174](#bib.bib174)] + HybridNet(Conv-5 层) 的空间区域 [[138](#bib.bib138),
    [175](#bib.bib175)] + HybridNet(Conv-5 层) 的最大激活的空间坐标 [[41](#bib.bib41)]
- en: 'Descriptor: features + normalization Hidden Markov Model of the similarity
    distances of each feature stream Dynamic sequence /Database St Lucia [[109](#bib.bib109)]'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：特征 + 归一化 隐马尔可夫模型 每个特征流的相似性距离 动态序列 / 数据库 St Lucia [[109](#bib.bib109)]
- en: Nordland [[2](#bib.bib2)]
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Nordland [[2](#bib.bib2)]
- en: Oxford RobotCar [[102](#bib.bib102)]
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Oxford RobotCar [[102](#bib.bib102)]
- en: VII-B Hierarchical Frameworks
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 层次框架
- en: 'In this work, hierarchical frameworks refer to place recognition approaches
    that, similarly to parallel frameworks, rely on multiple methods; however, instead
    of having as main framework a parallel architecture, the architecture is formed
    by various stacked tiers. The hierarchical architectures find the best loop candidate
    by filter candidates progressively in each tier. An example of such a framework
    is the coarse-to-fine architecture, which has a coarse and a fine tier. The coarse
    tier is mostly dedicated to retrieving top candidates utilizing methods that rather
    are computer efficient than accurate. These top candidates are feed to the fine
    tier, which can use more computer demanding methods to find the best loop candidate.
    The coarse-to-fine architecture, while being the most common, is not the only.
    Other architectures exist, for example Fig. [7](#S7.F7 "Figure 7 ‣ VII Other Frameworks
    ‣ Place recognition survey: An update on deep learning approaches") illustrates
    a framework proposed in [[176](#bib.bib176)] and Table [VII](#S7.T7 "TABLE VII
    ‣ VII-B Hierarchical Frameworks ‣ VII Other Frameworks ‣ Place recognition survey:
    An update on deep learning approaches") presents a summary of recent works.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，层次化框架指的是位置识别方法，这些方法类似于并行框架，依赖于多种方法；然而，区别在于，其主要框架不是并行架构，而是由多个堆叠层组成的架构。层次化架构通过在每一层逐步筛选候选项来找到最佳循环候选项。一个这样的框架示例是粗到细架构，它具有粗层和细层。粗层主要用于利用计算效率高但准确性低的方法检索前几名候选项。这些前几名候选项被送到细层，细层可以使用更高计算要求的方法来找到最佳循环候选项。尽管粗到细架构是最常见的，但并不是唯一的。其他架构也存在，例如图[7](#S7.F7
    "Figure 7 ‣ VII Other Frameworks ‣ Place recognition survey: An update on deep
    learning approaches")展示了一个在[[176](#bib.bib176)]中提出的框架，而表格[VII](#S7.T7 "TABLE VII
    ‣ VII-B Hierarchical Frameworks ‣ VII Other Frameworks ‣ Place recognition survey:
    An update on deep learning approaches")则总结了最近的研究工作。'
- en: 'Hausler and Milford [[176](#bib.bib176)] show that parallel fusion strategies
    have inferior performance compared with hierarchical approaches and therefore
    propose Hierarchical Multi-Process Fusion, which has a three-tier hierarchy. In
    the first tier, top candidates are retrieved from the database based on HybridNet[[138](#bib.bib138)]
    and Gist[[177](#bib.bib177)] features. In the second tier, from the top candidates
    of the previous tier, a more narrow selection is performed based on KAZE[[178](#bib.bib178)]
    and Only Look Once (OLO)[[123](#bib.bib123)] features. Finally, the best loop
    candidate is obtained in the third tier using NetVLAD[[24](#bib.bib24)] and HOG[[18](#bib.bib18)].
    An illustration of this framework is presented in Fig. [7](#S7.F7 "Figure 7 ‣
    VII Other Frameworks ‣ Place recognition survey: An update on deep learning approaches").'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hausler和Milford[[176](#bib.bib176)]表明，平行融合策略相比于层次化方法表现较差，因此提出了层次化多过程融合，它具有三层层次结构。在第一层，从数据库中检索基于HybridNet[[138](#bib.bib138)]和Gist[[177](#bib.bib177)]特征的前几名候选项。在第二层，从上一层的前几名候选项中，根据KAZE[[178](#bib.bib178)]和Only
    Look Once (OLO)[[123](#bib.bib123)]特征进行更精确的选择。最后，在第三层使用NetVLAD[[24](#bib.bib24)]和HOG[[18](#bib.bib18)]获得最佳循环候选项。该框架的示意图见图[7](#S7.F7
    "Figure 7 ‣ VII Other Frameworks ‣ Place recognition survey: An update on deep
    learning approaches")。'
- en: Garg et al. [[41](#bib.bib41)] also follow a similar framework, proposing a
    hierarchical place recognition approach, called X-Lost. In the coarse tier, top
    candidates are found by matching the Local Semantic Tensor (LoST) descriptor,
    which comprises feature maps from the RefineNet [[179](#bib.bib179)] (a dense
    segmentation network) and semantic label scores of the road, building, and vegetation
    classes. The best match is found in the fine tier by verifying the spatial layout
    of semantically salient keypoint correspondences.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Garg等人[[41](#bib.bib41)]也遵循了类似的框架，提出了一种称为X-Lost的层次化位置识别方法。在粗层，通过匹配包含来自RefineNet[[179](#bib.bib179)]（一个密集分割网络）和道路、建筑物及植被类的语义标签得分的特征图的Local
    Semantic Tensor (LoST)描述符来找到前几名候选项。通过验证语义上显著的关键点对应关系的空间布局，在细层中找到最佳匹配。
- en: This semantic- and keypoint-based approach is further exploited in [[63](#bib.bib63),
    [27](#bib.bib27)]. In [[63](#bib.bib63)], top candidates are obtained fusing NetVLAD[[24](#bib.bib24)]
    and LoST[[41](#bib.bib41)] descriptors in a coarse stage, while in [[27](#bib.bib27)],
    depth maps are computed from camera data in an intermediate stage to remove keypoints
    that are out of range.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于语义和关键点的方法在[[63](#bib.bib63), [27](#bib.bib27)]中得到了进一步的利用。在[[63](#bib.bib63)]中，前几名候选项通过在粗略阶段融合NetVLAD[[24](#bib.bib24)]和LoST[[41](#bib.bib41)]描述符获得，而在[[27](#bib.bib27)]中，通过在中间阶段从相机数据计算深度图以去除超出范围的关键点。
- en: Contrary to the former approaches, where performance is the primary goal, An
    et al.[[180](#bib.bib180)] address the efficiency problem of place recognition,
    proposing an approach based on an HNSW graph for efficient map management. HNSW
    graph guarantees low map building and retrieval time. In the coarse stage, top
    candidates are retrieved from the HNSW graph by matching features extracted from
    the MobileNetV2 [[181](#bib.bib181)] using normalized scalar product [[182](#bib.bib182)].
    The final loop candidate is obtained by matching hash codes from SURF features
    and the top candidates retrieved in the coarse stage. On the other hand, Liu et
    al. [[52](#bib.bib52)] exploits 3D point clouds instead of camera data, proposing
    SeqLPD, which is a lightweight variant of our LPD-Net [[49](#bib.bib49)]. This
    approach resorts to super keyframe clusters for coarse search, while for fine
    search, local sequence matching is preferred.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方法不同，An 等人[[180](#bib.bib180)] 解决了地点识别的效率问题，提出了一种基于 HNSW 图的高效地图管理方法。HNSW
    图保证了低的地图构建和检索时间。在粗略阶段，通过使用归一化标量积 [[182](#bib.bib182)] 匹配从 MobileNetV2 [[181](#bib.bib181)]
    提取的特征，从 HNSW 图中检索出前候选。最终的循环候选是通过匹配 SURF 特征的哈希码和粗略阶段检索出的前候选获得的。另一方面，Liu 等人 [[52](#bib.bib52)]
    利用 3D 点云而不是相机数据，提出了 SeqLPD，这是我们 LPD-Net [[49](#bib.bib49)] 的轻量级变体。这种方法利用超级关键帧簇进行粗略搜索，而在精细搜索中，则更倾向于局部序列匹配。
- en: 'TABLE VII: Summary of recent works using hierarchical frameworks. BG = Belief
    Generation and PM = Place mapping.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：使用层次化框架的近期工作总结。BG = 信念生成，PM = 地点映射。
- en: 'Sensor Ref Coarse Stage Fine Stage PM Dataset Camera [[41](#bib.bib41)] Features:
    RefineNet [[179](#bib.bib179)]'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器 参考 粗略阶段 精细阶段 PM 数据集 摄像头 [[41](#bib.bib41)] 特征：RefineNet [[179](#bib.bib179)]
- en: 'Descriptor: LosT (semantic label scores + conv5 layer feature maps ) + normalization'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：LosT（语义标签分数 + 卷积5层特征图）+ 归一化
- en: 'BG: cosine distance Features: RefineNet [[179](#bib.bib179)]'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: BG：余弦距离 特征：RefineNet [[179](#bib.bib179)]
- en: 'Descriptor: keypoint extracted from CNN layer activations'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：从 CNN 层激活中提取的关键点
- en: 'GB: spatial layout Verification ( Semantic Label Consistency + weighted Euclidean
    distance) Coarse: Database'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: GB：空间布局验证（语义标签一致性 + 加权欧几里得距离） 粗略：数据库
- en: 'Fine: Top Candidates Oxford Robotcar [[1](#bib.bib1)];'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 精细：前候选 Oxford Robotcar [[1](#bib.bib1)];
- en: 'Synthia Dataset [[183](#bib.bib183)]; [[63](#bib.bib63)] Descriptor: concatenation
    of LoST [[41](#bib.bib41)] + NetVLAD[[24](#bib.bib24)]'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Synthia 数据集 [[183](#bib.bib183)]; [[63](#bib.bib63)] 描述符：LoST [[41](#bib.bib41)]
    + NetVLAD [[24](#bib.bib24)] 的串联
- en: 'BG: cosine distance Features: pre-trained CNN'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: BG：余弦距离 特征：预训练 CNN
- en: 'Descriptor: keypoint extracted from CNN activations'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：从 CNN 激活中提取的关键点
- en: 'BG: spatial layout consistency Coarse: Database'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: BG：空间布局一致性 粗略：数据库
- en: 'Fine: Top Candidates Oxford Robotcar [[1](#bib.bib1)]; MLFR; Parking Lot; Residence
    Indoor Outdoor [[27](#bib.bib27)] Features: RefineNet(Resnet101) [[179](#bib.bib179)]'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 精细：前候选 Oxford Robotcar [[1](#bib.bib1)]; MLFR; 停车场; 住宅 室内 室外 [[27](#bib.bib27)]
    特征：RefineNet(Resnet101) [[179](#bib.bib179)]
- en: 'Descriptor: conv 5 feature maps'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：卷积 5 特征图
- en: 'BG: cosine distance Filtering: out-of-range keypoins based on depth maps'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: BG：余弦距离 过滤：基于深度图的范围外关键点
- en: 'Descriptor: same as coarse stage (filtered)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：与粗略阶段相同（过滤后）
- en: 'BG: cosine distance Coarse: Database'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: BG：余弦距离 粗略：数据库
- en: 'Fine: Top Candidates Oxford Robotcar [[1](#bib.bib1)]'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 精细：前候选 Oxford Robotcar [[1](#bib.bib1)]
- en: 'Synthia [[183](#bib.bib183)] (for depth evaluation) [[180](#bib.bib180)] Top
    Candidates:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Synthia [[183](#bib.bib183)] （用于深度评估） [[180](#bib.bib180)] 前候选：
- en: 'Features: MobileNetV2 [[181](#bib.bib181)]'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 特征：MobileNetV2 [[181](#bib.bib181)]
- en: 'Descriptor: final average pooling layer'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：最终的平均池化层
- en: 'BG: nearest neighbors + normalized scalar product [[182](#bib.bib182)] Features:
    SURF'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: BG：最近邻 + 归一化标量积 [[182](#bib.bib182)] 特征：SURF
- en: 'Descriptor: hash codes'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：哈希码
- en: 'BG: Hamming Distance between top candidates and SURF-based descriptor + ratio
    test [[184](#bib.bib184)] + RANSAC Coarse: HNSW graphs'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: BG：前候选和基于 SURF 的描述符之间的汉明距离 + 比率测试 [[184](#bib.bib184)] + RANSAC 粗略：HNSW 图
- en: 'Fine: Top candidates KITTI [[142](#bib.bib142)]'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 精细：前候选 KITTI [[142](#bib.bib142)]
- en: Malaga 2009 Parking 6L [[185](#bib.bib185)]
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 马拉加 2009 停车 6L [[185](#bib.bib185)]
- en: 'New College [[186](#bib.bib186)] [[176](#bib.bib176)] 1st Tier:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 新学院 [[186](#bib.bib186)] [[176](#bib.bib176)] 第一层：
- en: 'Features: HybridNet(AlexNet) [[138](#bib.bib138)] and Gist [[177](#bib.bib177)]'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 特征：HybridNet(AlexNet) [[138](#bib.bib138)] 和 Gist [[177](#bib.bib177)]
- en: 'BG: Difference Scores + normalization'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: BG：差异分数 + 归一化
- en: '2nd Tier:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第二级：
- en: 'Features: KAZE [[178](#bib.bib178)] and Only Look Once [[40](#bib.bib40)]'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 特征：KAZE [[178](#bib.bib178)] 和 Only Look Once [[40](#bib.bib40)]
- en: 'BG: (KAZE) sum of the residual distances and difference scores + normalization
    3 Tier:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 'BG: (KAZE) 残差距离和差异得分的总和 + 归一化 第三层：'
- en: 'Features: NetVLAD [[24](#bib.bib24)] and HOG [[18](#bib.bib18)]'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 特征：NetVLAD [[24](#bib.bib24)] 和 HOG [[18](#bib.bib18)]
- en: 'BG: max(Average of Difference Scores) 1st tier: Database'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 'BG: max(平均差异得分) 第一层：数据库'
- en: '2nd tier: Top candidates of the 1st tier'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 第二层：第一层的顶级候选者
- en: '3th tier: Top candidates of the 2nd tier Nordland [[187](#bib.bib187)]'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 第三层：第二层的顶级候选者 Nordland [[187](#bib.bib187)]
- en: 'Berlin Kurfurstendamm [[111](#bib.bib111)] 3D LiDAR [[52](#bib.bib52)] Find
    the cluster:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 柏林 Kurfurstendamm [[111](#bib.bib111)] 3D LiDAR [[52](#bib.bib52)] 查找聚类：
- en: 'Descriptor: lightweight variant LPD-Net'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符：轻量级变体 LPD-Net
- en: 'Matching: the nearest L2 distance to the cluster center is selected as the
    super keyframe Descriptor: same as in coarse'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配：选择到聚类中心的最近L2距离作为超关键帧 描述符：与粗略匹配相同
- en: 'BG: Local sequence matching Super keyframe clusters Oxford Robotcar [[1](#bib.bib1)]'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 'BG: 本地序列匹配 超关键帧簇 牛津机器人车 [[1](#bib.bib1)]'
- en: KITTI [[142](#bib.bib142)]
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: KITTI [[142](#bib.bib142)]
- en: VIII Conclusion and discussion
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第八章 结论与讨论
- en: This paper presents a critical survey on place recognition approaches, emphasizing
    the recent developments on deep learning frameworks, namely supervised, unsupervised,
    semi-supervised, parallel, and hierarchical approaches.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对地点识别方法进行了关键性综述，强调了在深度学习框架上的近期发展，即监督、无监督、半监督、并行和层次化方法。
- en: An overview of each of these frameworks is presented. In supervised approaches,
    the pre-trained frameworks tend to resort to semantic information by detecting
    landmarks or leveraging regional activation from CNN layers. On the other hand,
    among the end-to-end frameworks, the NetVLAD layer has inspired various works,
    which integrated this layer in deep architectures to train the model directly
    on place recognition using sensory data from the camera, 3D LiDAR, or RADAR. The
    main application of unsupervised approaches, such as GANs and autoencoders, is
    to address the domain translation problem. While in semi-supervised, which in
    this work refers to works that jointly leverage supervised and unsupervised methods,
    the works address the cross-domain problem, however instead of translating a source
    domain into a target domain, these works seek to obtain a descriptor space that
    is invariant to domains. Besides these traditional machine learning frameworks,
    other frameworks have been suggested, combining multiple DL or classical ML approaches
    into a parallel or hierarchical architecture. In particular, the hierarchical
    approach has been shown to improve performances in general. Until recently, the
    primary motivation of the majority of the published articles was to increase performance.
    However, recent works additionally to high performance are also seeking efficiency.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了这些框架的概述。在监督方法中，预训练的框架倾向于通过检测地标或利用CNN层的区域激活来借助语义信息。另一方面，在端到端框架中，NetVLAD层激发了各种工作，这些工作将此层集成到深度架构中，直接在使用来自相机、3D
    LiDAR或RADAR的传感数据的地点识别上训练模型。无监督方法，如GANs和自编码器，主要应用于解决领域转换问题。而在半监督方法中，本研究指的是结合监督和无监督方法的工作，这些工作解决了跨领域问题，但不是将源领域转换为目标领域，而是寻求获得对领域不变的描述符空间。除了这些传统的机器学习框架外，还提出了其他框架，将多种深度学习或经典机器学习方法结合成并行或层次化架构。特别是，层次化方法已被证明能提高整体性能。直到最近，大多数已发布文章的主要动机是提高性能。然而，最近的工作除了追求高性能外，还在寻求效率。
- en: Acknowledgments
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work has been supported by the projects MATIS-CENTRO-01-0145-FEDER-000014
    and SafeForest CENTRO-01-0247-FEDER-045931, Portugal. It was also partially supported
    by FCT through grant UID/EEA/00048/2019.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了MATIS-CENTRO-01-0145-FEDER-000014和SafeForest CENTRO-01-0247-FEDER-045931项目的支持。它还得到了FCT通过UID/EEA/00048/2019资助的部分支持。
- en: References
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year, 1000km: The
    Oxford RobotCar Dataset,” *The International Journal of Robotics Research (IJRR)*,
    vol. 36, no. 1, pp. 3–15, 2017\. [Online]. Available: http://dx.doi.org/10.1177/0278364916679498'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] W. Maddern, G. Pascoe, C. Linegar, 和 P. Newman，“1年，1000公里：牛津机器人车数据集，”*国际机器人研究期刊
    (IJRR)*，第36卷，第1期，第3–15页，2017年。[在线]. 可用: http://dx.doi.org/10.1177/0278364916679498'
- en: '[2] D. Olid, J. M. Fácil, and J. Civera, “Single-view place recognition under
    seasonal changes,” in *PPNIV Workshop at IROS 2018*, 2018.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Olid, J. M. Fácil, 和 J. Civera，“在季节变化下的单视角地点识别，”在*PPNIV Workshop at
    IROS 2018*，2018年。'
- en: '[3] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, and
    M. J. Milford, “Visual place recognition: A survey,” *IEEE Transactions on Robotics*,
    vol. 32, no. 1, pp. 1–19, 2015.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] S. Lowry, N. Sünderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke, 和
    M. J. Milford，“视觉地点识别：综述，” *IEEE Transactions on Robotics*，第32卷，第1期，第1–19页，2015年。'
- en: '[4] E. Garcia-Fidalgo and A. Ortiz, “Vision-based topological mapping and localization
    methods: A survey,” *Robotics and Autonomous Systems*, vol. 64, pp. 1–20, 2015.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] E. Garcia-Fidalgo 和 A. Ortiz，“基于视觉的拓扑地图构建和定位方法：综述，” *Robotics and Autonomous
    Systems*，第64卷，第1–20页，2015年。'
- en: '[5] G. Kim, Y. S. Park, Y. Cho, J. Jeong, and A. Kim, “Mulran: Multimodal range
    dataset for urban place recognition,” in *2020 IEEE International Conference on
    Robotics and Automation (ICRA)*, 2020, pp. 6246–6253.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] G. Kim, Y. S. Park, Y. Cho, J. Jeong, 和 A. Kim，“Mulran：用于城市地点识别的多模态范围数据集，”
    收录于 *2020 IEEE International Conference on Robotics and Automation (ICRA)*，2020年，第6246–6253页。'
- en: '[6] D. Barnes, M. Gadd, P. Murcutt, P. Newman, and I. Posner, “The oxford radar
    robotcar dataset: A radar extension to the oxford robotcar dataset,” in *2020
    IEEE International Conference on Robotics and Automation (ICRA)*.   IEEE, 2020,
    pp. 6433–6438.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. Barnes, M. Gadd, P. Murcutt, P. Newman, 和 I. Posner，“牛津雷达机器人车数据集：牛津机器人车数据集的雷达扩展，”
    收录于 *2020 IEEE International Conference on Robotics and Automation (ICRA)*。 IEEE，2020年，第6433–6438页。'
- en: '[7] F. Warburg, S. Hauberg, M. López-Antequera, P. Gargallo, Y. Kuang, and
    J. Civera, “Mapillary street-level sequences: A dataset for lifelong place recognition,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 2626–2635.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] F. Warburg, S. Hauberg, M. López-Antequera, P. Gargallo, Y. Kuang, 和 J.
    Civera，“Mapillary街景序列：用于终身地点识别的数据集，” 收录于 *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*，2020年，第2626–2635页。'
- en: '[8] Y. Kang, H. Yin, and C. Berger, “Test your self-driving algorithm: An overview
    of publicly available driving datasets and virtual testing environments,” *IEEE
    Transactions on Intelligent Vehicles*, vol. 4, no. 2, pp. 171–185, 2019.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. Kang, H. Yin, 和 C. Berger，“测试你的自动驾驶算法：公开可用的驾驶数据集和虚拟测试环境概述，” *IEEE Transactions
    on Intelligent Vehicles*，第4卷，第2期，第171–185页，2019年。'
- en: '[9] I. Kostavelis and A. Gasteratos, “Semantic mapping for mobile robotics
    tasks: A survey,” *Robotics and Autonomous Systems*, vol. 66, pp. 86–103, 2015.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] I. Kostavelis 和 A. Gasteratos，“移动机器人任务的语义映射：综述，” *Robotics and Autonomous
    Systems*，第66卷，第86–103页，2015年。'
- en: '[10] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
    I. Reid, and J. J. Leonard, “Past, present, and future of simultaneous localization
    and mapping: Toward the robust-perception age,” *IEEE Transactions on Robotics*,
    vol. 32, no. 6, pp. 1309–1332, 2016.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
    I. Reid, 和 J. J. Leonard，“同时定位与地图构建的过去、现在和未来：迈向鲁棒感知时代，” *IEEE Transactions on
    Robotics*，第32卷，第6期，第1309–1332页，2016年。'
- en: '[11] G. Bresson, Z. Alsayed, L. Yu, and S. Glaser, “Simultaneous localization
    and mapping: A survey of current trends in autonomous driving,” *IEEE Transactions
    on Intelligent Vehicles*, vol. 2, no. 3, pp. 194–220, 2017.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] G. Bresson, Z. Alsayed, L. Yu, 和 S. Glaser，“同时定位与地图构建：当前自动驾驶趋势的调查，” *IEEE
    Transactions on Intelligent Vehicles*，第2卷，第3期，第194–220页，2017年。'
- en: '[12] G. Kim and A. Kim, “Scan context: Egocentric spatial descriptor for place
    recognition within 3d point cloud map,” in *2018 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*, 2018, pp. 4802–4809.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] G. Kim 和 A. Kim，“扫描上下文：用于3D点云地图中的地点识别的自我中心空间描述符，” 收录于 *2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*，2018年，第4802–4809页。'
- en: '[13] H. Yin, L. Tang, X. Ding, Y. Wang, and R. Xiong, “Locnet: Global localization
    in 3d point clouds for mobile vehicles,” in *2018 IEEE Intelligent Vehicles Symposium
    (IV)*, 2018, pp. 728–733.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] H. Yin, L. Tang, X. Ding, Y. Wang, 和 R. Xiong，“Locnet：用于移动车辆的3D点云中的全球定位，”
    收录于 *2018 IEEE Intelligent Vehicles Symposium (IV)*，2018年，第728–733页。'
- en: '[14] M. Gadd, D. De Martini, and P. Newman, “Look around you: Sequence-based
    radar place recognition with learned rotational invariance,” *arXiv preprint arXiv:2003.04699*,
    2020.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. Gadd, D. De Martini, 和 P. Newman，“看看你周围：基于序列的雷达地点识别与学习到的旋转不变性，” *arXiv
    preprint arXiv:2003.04699*，2020年。'
- en: '[15] D. G. Lowe, “Object recognition from local scale-invariant features,”
    in *Proceedings of the Seventh IEEE International Conference on Computer Vision*,
    vol. 2, 1999, pp. 1150–1157 vol.2.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] D. G. Lowe，“从局部尺度不变特征中进行物体识别，” 收录于 *Proceedings of the Seventh IEEE International
    Conference on Computer Vision*，第2卷，1999年，第1150–1157页。'
- en: '[16] H. Bay, T. Tuytelaars, and L. Van Gool, “Surf: Speeded up robust features,”
    in *European conference on computer vision*.   Springer, 2006, pp. 404–417.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] H. Bay, T. Tuytelaars, 和 L. Van Gool，“Surf：加速鲁棒特征，” 收录于 *European conference
    on computer vision*。 Springer，2006年，第404–417页。'
- en: '[17] P. Neubert and P. Protzel, “Beyond holistic descriptors, keypoints, and
    fixed patches: Multiscale superpixel grids for place recognition in changing environments,”
    *IEEE Robotics and Automation Letters*, vol. 1, no. 1, pp. 484–491, 2016.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] P. Neubert 和 P. Protzel, “超越整体描述符、关键点和固定补丁：用于变化环境中的地点识别的多尺度超像素网格”，*IEEE机器人与自动化信函*，第
    1 卷，第 1 期，页码 484–491，2016年。'
- en: '[18] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in *2005 IEEE computer society conference on computer vision and pattern recognition
    (CVPR’05)*, vol. 1.   IEEE, 2005, pp. 886–893.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] N. Dalal 和 B. Triggs, “用于人类检测的方向梯度直方图”，收录于*2005 IEEE计算机学会计算机视觉与模式识别会议（CVPR’05）*，第
    1 卷。IEEE，2005年，页码 886–893。'
- en: '[19] L. Fei-Fei and P. Perona, “A bayesian hierarchical model for learning
    natural scene categories,” in *2005 IEEE Computer Society Conference on Computer
    Vision and Pattern Recognition (CVPR’05)*, vol. 2, 2005, pp. 524–531 vol. 2.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] L. Fei-Fei 和 P. Perona, “用于学习自然场景类别的贝叶斯层次模型”，收录于*2005 IEEE计算机学会计算机视觉与模式识别会议（CVPR’05）*，第
    2 卷，2005年，页码 524–531，第 2 卷。'
- en: '[20] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. LeCun, Y. Bengio, 和 G. Hinton, “深度学习”，*自然*，第 521 卷，第 7553 期，页码 436–444，2015年。'
- en: '[21] J. Yue-Hei Ng, F. Yang, and L. S. Davis, “Exploiting local features from
    deep networks for image retrieval,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition workshops*, 2015, pp. 53–61.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Yue-Hei Ng, F. Yang, 和 L. S. Davis, “利用深度网络的局部特征进行图像检索”，收录于*IEEE计算机视觉与模式识别会议论文集*，2015年，页码
    53–61。'
- en: '[22] N. Sünderhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford, “On
    the performance of convnet features for place recognition,” in *2015 IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS)*.   IEEE, 2015,
    pp. 4297–4304.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] N. Sünderhauf, S. Shirazi, F. Dayoub, B. Upcroft, 和 M. Milford, “卷积网络特征在地点识别中的表现”，收录于*2015
    IEEE/RSJ国际智能机器人与系统会议（IROS）*。IEEE，2015年，页码 4297–4304。'
- en: '[23] S. Garg, N. Suenderhauf, and M. Milford, “Don’t look back: Robustifying
    place categorization for viewpoint- and condition-invariant place recognition,”
    in *2018 IEEE International Conference on Robotics and Automation (ICRA)*, 2018,
    pp. 3645–3652.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Garg, N. Suenderhauf, 和 M. Milford, “不要回头：增强地点分类以适应视角和条件无关的地点识别”，收录于*2018
    IEEE国际机器人与自动化会议（ICRA）*，2018年，页码 3645–3652。'
- en: '[24] R. Arandjelović, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, “Netvlad:
    Cnn architecture for weakly supervised place recognition,” in *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, vol. 40, no. 6, 2018, pp. 1437–1451.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] R. Arandjelović, P. Gronat, A. Torii, T. Pajdla, 和 J. Sivic, “Netvlad：用于弱监督地点识别的CNN架构”，*IEEE模式分析与机器智能汇刊*，第
    40 卷，第 6 期，2018年，页码 1437–1451。'
- en: '[25] A. Oertel, T. Cieslewski, and D. Scaramuzza, “Augmenting visual place
    recognition with structural cues,” *IEEE Robotics and Automation Letters*, vol. 5,
    no. 4, pp. 5534–5541, 2020.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Oertel, T. Cieslewski, 和 D. Scaramuzza, “通过结构线索增强视觉地点识别”，*IEEE机器人与自动化信函*，第
    5 卷，第 4 期，页码 5534–5541，2020年。'
- en: '[26] J. L. Schönberger, M. Pollefeys, A. Geiger, and T. Sattler, “Semantic
    visual localization,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 6896–6906.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. L. Schönberger, M. Pollefeys, A. Geiger, 和 T. Sattler, “语义视觉定位”，收录于*IEEE计算机视觉与模式识别会议论文集*，2018年，页码
    6896–6906。'
- en: '[27] S. Garg, M. Babu, T. Dharmasiri, S. Hausler, N. Suenderhauf, S. Kumar,
    T. Drummond, and M. Milford, “Look no deeper: Recognizing places from opposing
    viewpoints under varying scene appearance using single-view depth estimation,”
    in *2019 International Conference on Robotics and Automation (ICRA)*.   IEEE,
    2019, pp. 4916–4923.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Garg, M. Babu, T. Dharmasiri, S. Hausler, N. Suenderhauf, S. Kumar,
    T. Drummond, 和 M. Milford, “无需更深入：通过单视角深度估计在场景外观变化下从对立视角识别地点”，收录于*2019国际机器人与自动化会议（ICRA）*。IEEE，2019年，页码
    4916–4923。'
- en: '[28] R. Paul and P. Newman, “Fab-map 3d: Topological mapping with spatial and
    visual appearance,” in *2010 IEEE International Conference on Robotics and Automation*.   IEEE,
    2010, pp. 2649–2656.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. Paul 和 P. Newman, “Fab-map 3d：结合空间和视觉外观的拓扑映射”，收录于*2010 IEEE国际机器人与自动化会议*。IEEE，2010年，页码
    2649–2656。'
- en: '[29] H. Korrapati, J. Courbon, Y. Mezouar, and P. Martinet, “Image sequence
    partitioning for outdoor mapping,” in *2012 IEEE International Conference on Robotics
    and Automation*.   IEEE, 2012, pp. 1650–1655.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] H. Korrapati, J. Courbon, Y. Mezouar, 和 P. Martinet, “用于户外制图的图像序列分割”，收录于*2012
    IEEE国际机器人与自动化会议*。IEEE，2012年，页码 1650–1655。'
- en: '[30] Z. Hong, Y. Petillot, D. Lane, Y. Miao, and S. Wang, “Textplace: Visual
    place recognition and topological localization through reading scene texts,” in
    *2019 IEEE/CVF International Conference on Computer Vision (ICCV)*, 2019, pp.
    2861–2870.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Z. Hong, Y. Petillot, D. Lane, Y. Miao, 和 S. Wang, “Textplace：通过阅读场景文本进行视觉地点识别和拓扑定位，”在
    *2019 IEEE/CVF 国际计算机视觉会议（ICCV）*，2019年，第2861–2870页。'
- en: '[31] F. Dayoub, G. Cielniak, and T. Duckett, “Long-term experiments with an
    adaptive spherical view representation for navigation in changing environments,”
    *Robotics and Autonomous Systems*, vol. 59, no. 5, pp. 285–295, 2011.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] F. Dayoub, G. Cielniak 和 T. Duckett, “在变化环境中使用自适应球形视图表示进行的长期实验，” *机器人与自动系统*，第59卷，第5期，第285–295页，2011年。'
- en: '[32] W. Churchill and P. Newman, “Experience-based navigation for long-term
    localisation,” *The International Journal of Robotics Research*, vol. 32, no. 14,
    pp. 1645–1661, 2013.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] W. Churchill 和 P. Newman, “基于经验的长时间定位导航，” *国际机器人研究期刊*，第32卷，第14期，第1645–1661页，2013年。'
- en: '[33] M. J. Milford, G. F. Wyeth, and D. Prasser, “Ratslam: a hippocampal model
    for simultaneous localization and mapping,” in *IEEE International Conference
    on Robotics and Automation, 2004\. Proceedings. ICRA ’04\. 2004*, vol. 1, 2004,
    pp. 403–408 Vol.1.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] M. J. Milford, G. F. Wyeth 和 D. Prasser, “Ratslam：用于同时定位与地图构建的海马体模型，”在
    *IEEE 国际机器人与自动化会议，2004年. 会议录. ICRA ’04. 2004*，第1卷，2004年，第403–408页 第1卷。'
- en: '[34] L. Schaupp, M. Bürki, R. Dubé, R. Siegwart, and C. Cadena, “Oreos: Oriented
    recognition of 3d point clouds in outdoor scenarios,” in *2019 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2019, pp. 3255–3261.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] L. Schaupp, M. Bürki, R. Dubé, R. Siegwart 和 C. Cadena, “Oreos：在户外场景中对3D点云的定向识别，”在
    *2019 IEEE/RSJ 国际智能机器人与系统会议（IROS）*，2019年，第3255–3261页。'
- en: '[35] M. Cummins and P. Newman, “Fab-map: Probabilistic localization and mapping
    in the space of appearance,” *The International Journal of Robotics Research*,
    vol. 27, no. 6, pp. 647–665, 2008.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] M. Cummins 和 P. Newman, “Fab-map：在外观空间中的概率定位和地图构建，” *国际机器人研究期刊*，第27卷，第6期，第647–665页，2008年。'
- en: '[36] Y. A. Malkov and D. A. Yashunin, “Efficient and robust approximate nearest
    neighbor search using hierarchical navigable small world graphs,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligencee*, 2018.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Y. A. Malkov 和 D. A. Yashunin, “使用分层可导航的小世界图进行高效且稳健的近似最近邻搜索，” *IEEE 模式分析与机器智能汇刊*，2018年。'
- en: '[37] E. Garcia-Fidalgo and A. Ortiz, “Hierarchical place recognition for topological
    mapping,” *IEEE Transactions on Robotics*, vol. 33, no. 5, pp. 1061–1074, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] E. Garcia-Fidalgo 和 A. Ortiz, “用于拓扑地图构建的分层地点识别，” *IEEE 机器人学汇刊*，第33卷，第5期，第1061–1074页，2017年。'
- en: '[38] T. Morris, F. Dayoub, P. Corke, G. Wyeth, and B. Upcroft, “Multiple map
    hypotheses for planning and navigating in non-stationary environments,” in *2014
    IEEE International Conference on Robotics and Automation (ICRA)*, 2014, pp. 2765–2770.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T. Morris, F. Dayoub, P. Corke, G. Wyeth 和 B. Upcroft, “在非静态环境中进行规划和导航的多地图假设，”在
    *2014 IEEE 国际机器人与自动化会议（ICRA）*，2014年，第2765–2770页。'
- en: '[39] M. Zaffar, S. Ehsan, M. Milford, and K. McDonald-Maier, “Cohog: A light-weight,
    compute-efficient, and training-free visual place recognition technique for changing
    environments,” *IEEE Robotics and Automation Letters*, vol. 5, no. 2, pp. 1835–1842,
    2020.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. Zaffar, S. Ehsan, M. Milford 和 K. McDonald-Maier, “Cohog：一种轻量、高效且无需训练的视觉地点识别技术，用于变化环境，”
    *IEEE 机器人与自动化信函*，第5卷，第2期，第1835–1842页，2020年。'
- en: '[40] Z. Chen, F. Maffra, I. Sa, and M. Chli, “Only look once, mining distinctive
    landmarks from convnet for visual place recognition,” in *2017 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2017, pp. 9–16.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Z. Chen, F. Maffra, I. Sa 和 M. Chli, “只看一次，从卷积网络中挖掘独特地标进行视觉地点识别，”在 *2017
    IEEE/RSJ 国际智能机器人与系统会议（IROS）*，2017年，第9–16页。'
- en: '[41] S. Garg, N. Suenderhauf, and M. Milford, “Lost? appearance-invariant place
    recognition for opposite viewpoints using visual semantics,” *Proceedings of Robotics:
    Science and Systems XIV*, 2018.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Garg, N. Suenderhauf 和 M. Milford, “迷路了？使用视觉语义进行对立视角的外观不变地点识别，” *机器人学：科学与系统第十四卷会议录*，2018年。'
- en: '[42] M. J. Milford and G. F. Wyeth, “SeqSLAM: Visual route-based navigation
    for sunny summer days and stormy winter nights,” in *2012 IEEE International Conference
    on Robotics and Automation*, 2012, pp. 1643–1649.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. J. Milford 和 G. F. Wyeth, “SeqSLAM：在阳光明媚的夏日和暴风雪的冬夜进行基于视觉的路线导航，”在 *2012
    IEEE 国际机器人与自动化会议*，2012年，第1643–1649页。'
- en: '[43] S. Garg, B. Harwood, G. Anand, and M. Milford, “Delta descriptors: Change-based
    place representation for robust visual localization,” *IEEE Robotics and Automation
    Letters*, vol. 5, no. 4, pp. 5120–5127, 2020.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] S. Garg, B. Harwood, G. Anand 和 M. Milford, “Delta descriptors: 基于变化的稳健视觉定位表示，”
    *IEEE Robotics and Automation Letters*, 第 5 卷，第 4 期, 页码 5120–5127, 2020 年。'
- en: '[44] H. Zhang, F. Han, and H. Wang, “Robust multimodal sequence-based loop
    closure detection via structured sparsity.” in *Robotics: Science and systems*,
    2016.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] H. Zhang, F. Han 和 H. Wang, “通过结构稀疏性进行稳健的多模态序列基础循环闭合检测。” 见于 *Robotics:
    Science and Systems*, 2016 年。'
- en: '[45] P. Gao and H. Zhang, “Long-term place recognition through worst-case graph
    matching to integrate landmark appearances and spatial relationships,” in *2020
    IEEE International Conference on Robotics and Automation (ICRA)*, 2020, pp. 1070–1076.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] P. Gao 和 H. Zhang, “通过最坏情况图匹配进行长期位置识别，以整合地标外观和空间关系，” 见于 *2020 IEEE International
    Conference on Robotics and Automation (ICRA)*, 2020 年, 页码 1070–1076。'
- en: '[46] J. Guo, P. V. K. Borges, C. Park, and A. Gawel, “Local descriptor for
    robust place recognition using lidar intensity,” *IEEE Robotics and Automation
    Letters*, vol. 4, no. 2, pp. 1470–1477, 2019.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Guo, P. V. K. Borges, C. Park 和 A. Gawel, “基于 lidar 强度的稳健位置识别的局部描述符，”
    *IEEE Robotics and Automation Letters*, 第 4 卷，第 2 期, 页码 1470–1477, 2019 年。'
- en: '[47] P. Hansen and B. Browning, “Visual place recognition using hmm sequence
    matching,” in *2014 IEEE/RSJ International Conference on Intelligent Robots and
    Systems*, 2014, pp. 4549–4555.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] P. Hansen 和 B. Browning, “使用 hmm 序列匹配的视觉位置识别，” 见于 *2014 IEEE/RSJ International
    Conference on Intelligent Robots and Systems*, 2014 年, 页码 4549–4555。'
- en: '[48] C. Cadena, D. Galvez-López, J. D. Tardos, and J. Neira, “Robust place
    recognition with stereo sequences,” *IEEE Transactions on Robotics*, vol. 28,
    no. 4, pp. 871–885, 2012.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] C. Cadena, D. Galvez-López, J. D. Tardos 和 J. Neira, “使用立体序列的稳健位置识别，”
    *IEEE Transactions on Robotics*, 第 28 卷，第 4 期, 页码 871–885, 2012 年。'
- en: '[49] Z. Liu, S. Zhou, C. Suo, P. Yin, W. Chen, H. Wang, H. Li, and Y.-H. Liu,
    “LPD-Net: 3D point cloud learning for large-scale place recognition and environment
    analysis,” in *Proceedings of the IEEE International Conference on Computer Vision*,
    2019, pp. 2831–2840.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Z. Liu, S. Zhou, C. Suo, P. Yin, W. Chen, H. Wang, H. Li 和 Y.-H. Liu,
    “LPD-Net: 用于大规模位置识别和环境分析的 3D 点云学习，” 见于 *Proceedings of the IEEE International
    Conference on Computer Vision*, 2019 年, 页码 2831–2840。'
- en: '[50] L. Wu and Y. Wu, “Deep supervised hashing with similar hierarchy for place
    recognition,” in *2019 IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS)*, 2019, pp. 3781–3786.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] L. Wu 和 Y. Wu, “具有相似层级的深度监督哈希用于位置识别，” 见于 *2019 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2019 年, 页码 3781–3786。'
- en: '[51] S. M. Siam and H. Zhang, “Fast-seqslam: A fast appearance based place
    recognition algorithm,” in *2017 IEEE International Conference on Robotics and
    Automation (ICRA)*, 2017, pp. 5702–5708.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] S. M. Siam 和 H. Zhang, “Fast-seqslam: 一种基于外观的快速位置识别算法，” 见于 *2017 IEEE
    International Conference on Robotics and Automation (ICRA)*, 2017 年, 页码 5702–5708。'
- en: '[52] Z. Liu, C. Suo, S. Zhou, H. Wei, Y. Liu, H. Wang, and Y.-H. Liu, “Seqlpd:
    Sequence matching enhanced loop-closure detection based on large-scale point cloud
    description for self-driving vehicles,” *arXiv preprint arXiv:1904.13030*, 2019.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Z. Liu, C. Suo, S. Zhou, H. Wei, Y. Liu, H. Wang 和 Y.-H. Liu, “Seqlpd:
    基于大规模点云描述的增强序列匹配循环闭合检测，用于自动驾驶车辆，” *arXiv 预印本 arXiv:1904.13030*, 2019 年。'
- en: '[53] O. Vysotska and C. Stachniss, “Effective visual place recognition using
    multi-sequence maps,” *IEEE Robotics and Automation Letters*, vol. 4, no. 2, pp.
    1730–1736, 2019.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] O. Vysotska 和 C. Stachniss, “使用多序列地图进行有效的视觉位置识别，” *IEEE Robotics and Automation
    Letters*, 第 4 卷，第 2 期, 页码 1730–1736, 2019 年。'
- en: '[54] O. Vysotska and C. Stachniss, “Lazy data association for image sequences
    matching under substantial appearance changes,” *IEEE Robotics and Automation
    Letters*, vol. 1, no. 1, pp. 213–220, 2015.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] O. Vysotska 和 C. Stachniss, “图像序列匹配的惰性数据关联，在显著外观变化下，” *IEEE Robotics and
    Automation Letters*, 第 1 卷，第 1 期, 页码 213–220, 2015 年。'
- en: '[55] L. Bampis, A. Amanatiadis, and A. Gasteratos, “Encoding the description
    of image sequences: A two-layered pipeline for loop closure detection,” in *2016
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, 2016,
    pp. 4530–4536.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] L. Bampis, A. Amanatiadis 和 A. Gasteratos, “图像序列描述的编码：一种用于循环闭合检测的双层管道，”
    见于 *2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*,
    2016 年, 页码 4530–4536。'
- en: '[56] T. Naseer, L. Spinello, W. Burgard, and C. Stachniss, “Robust visual robot
    localization across seasons using network flows,” in *Proceedings of the Twenty-Eighth
    AAAI Conference on Artificial Intelligence*, ser. AAAI’14.   AAAI Press, 2014,
    p. 2564–2570.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] T. Naseer, L. Spinello, W. Burgard 和 C. Stachniss, “使用网络流进行跨季节的稳健视觉机器人定位，”
    见于 *Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence*,
    版 AAAI’14. AAAI Press, 2014 年, 页码 2564–2570。'
- en: '[57] P. Yin, L. Xu, X. Li, C. Yin, Y. Li, R. A. Srivatsan, L. Li, J. Ji, and
    Y. He, “A multi-domain feature learning method for visual place recognition,”
    in *2019 International Conference on Robotics and Automation (ICRA)*, 2019, pp.
    319–324.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] P. Yin, L. Xu, X. Li, C. Yin, Y. Li, R. A. Srivatsan, L. Li, J. Ji 和 Y.
    He，“一种用于视觉场所识别的多域特征学习方法”，发表于 *2019年国际机器人与自动化会议（ICRA）*，2019年，页码319–324。'
- en: '[58] M. Shakeri and H. Zhang, “Illumination invariant representation of natural
    images for visual place recognition,” in *2016 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*, 2016, pp. 466–472.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. Shakeri 和 H. Zhang，“用于视觉场所识别的自然图像的光照不变表示”，发表于 *2016年IEEE/RSJ国际智能机器人与系统会议（IROS）*，2016年，页码466–472。'
- en: '[59] T. Naseer, G. L. Oliveira, T. Brox, and W. Burgard, “Semantics-aware visual
    localization under challenging perceptual conditions,” in *2017 IEEE International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2017, pp. 2614–2620.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] T. Naseer, G. L. Oliveira, T. Brox 和 W. Burgard，“在挑战性感知条件下的语义感知视觉定位”，发表于
    *2017年IEEE国际机器人与自动化会议（ICRA）*。 IEEE，2017年，页码2614–2620。'
- en: '[60] Yang Liu and Hong Zhang, “Visual loop closure detection with a compact
    image descriptor,” in *2012 IEEE/RSJ International Conference on Intelligent Robots
    and Systems*, 2012, pp. 1051–1056.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Yang Liu 和 Hong Zhang，“具有紧凑图像描述符的视觉闭环检测”，发表于 *2012年IEEE/RSJ国际智能机器人与系统会议*，2012年，页码1051–1056。'
- en: '[61] S. Lowry and M. J. Milford, “Supervised and unsupervised linear learning
    techniques for visual place recognition in changing environments,” *IEEE Transactions
    on Robotics*, vol. 32, no. 3, pp. 600–613, 2016.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] S. Lowry 和 M. J. Milford，“用于变化环境中视觉场所识别的监督与无监督线性学习技术”，*IEEE机器人学报*，第32卷，第3期，页码600–613，2016年。'
- en: '[62] S. Schubert, P. Neubert, and P. Protzel, “Unsupervised learning methods
    for visual place recognition in discretely and continuously changing environments,”
    in *2020 IEEE International Conference on Robotics and Automation (ICRA)*, 2020,
    pp. 4372–4378.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. Schubert, P. Neubert 和 P. Protzel，“用于视觉场所识别的无监督学习方法在离散和连续变化环境中的应用”，发表于
    *2020年IEEE国际机器人与自动化会议（ICRA）*，2020年，页码4372–4378。'
- en: '[63] S. Garg, N. Suenderhauf, and M. Milford, “Semantic–geometric visual place
    recognition: a new perspective for reconciling opposing views,” *The International
    Journal of Robotics Research*, p. 0278364919839761, 2019.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] S. Garg, N. Suenderhauf 和 M. Milford，“语义-几何视觉场所识别：调和对立观点的新视角”，*国际机器人研究期刊*，页码0278364919839761，2019年。'
- en: '[64] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, J. J. Yebes, and S. Gámez,
    “Bidirectional loop closure detection on panoramas for visual navigation,” in
    *2014 IEEE Intelligent Vehicles Symposium Proceedings*.   IEEE, 2014, pp. 1378–1383.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, J. J. Yebes 和 S. Gámez，“在全景图上的双向闭环检测用于视觉导航”，发表于
    *2014年IEEE智能车辆研讨会论文集*。 IEEE，2014年，页码1378–1383。'
- en: '[65] M. Angelina Uy and G. Hee Lee, “Pointnetvlad: Deep point cloud based retrieval
    for large-scale place recognition,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2018, pp. 4470–4479.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] M. Angelina Uy 和 G. Hee Lee，“Pointnetvlad：基于深度点云的大规模场所识别检索”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2018年，页码4470–4479。'
- en: '[66] L. Yu, A. Jacobson, and M. Milford, “Rhythmic representations: Learning
    periodic patterns for scalable place recognition at a sublinear storage cost,”
    *IEEE Robotics and Automation Letters*, vol. 3, no. 2, pp. 811–818, 2018.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] L. Yu, A. Jacobson 和 M. Milford，“节奏表示：学习周期性模式以在子线性存储成本下实现可扩展场所识别”，*IEEE机器人与自动化快报*，第3卷，第2期，页码811–818，2018年。'
- en: '[67] D. Doan, Y. Latif, T. Chin, Y. Liu, T. Do, and I. Reid, “Scalable place
    recognition under appearance change for autonomous driving,” in *2019 IEEE/CVF
    International Conference on Computer Vision (ICCV)*, 2019, pp. 9318–9327.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] D. Doan, Y. Latif, T. Chin, Y. Liu, T. Do 和 I. Reid，“在外观变化下可扩展的场所识别用于自动驾驶”，发表于
    *2019年IEEE/CVF国际计算机视觉会议（ICCV）*，2019年，页码9318–9327。'
- en: '[68] T.-T. Do, T. Hoang, D.-K. Le Tan, A.-D. Doan, and N.-M. Cheung, “Compact
    hash code learning with binary deep neural network,” *IEEE Transactions on Multimedia*,
    vol. 22, no. 4, pp. 992–1004, 2019.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] T.-T. Do, T. Hoang, D.-K. Le Tan, A.-D. Doan 和 N.-M. Cheung，“基于二进制深度神经网络的紧凑哈希码学习”，*IEEE多媒体学报*，第22卷，第4期，页码992–1004，2019年。'
- en: '[69] Y. Latif, A. D. Doan, T. J. Chin, and I. Reid, “Sprint: Subgraph place
    recognition for intelligent transportation,” in *2020 IEEE International Conference
    on Robotics and Automation (ICRA)*, 2020, pp. 5408–5414.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Latif, A. D. Doan, T. J. Chin 和 I. Reid，“Sprint：用于智能交通的子图场所识别”，发表于
    *2020年IEEE国际机器人与自动化会议（ICRA）*，2020年，页码5408–5414。'
- en: '[70] S. Garg and M. Milford, “Fast, compact and highly scalable visual place
    recognition through sequence-based matching of overloaded representations,” in
    *2020 IEEE International Conference on Robotics and Automation (ICRA)*, 2020,
    pp. 3341–3348.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Garg 和 M. Milford，“通过基于序列的过载表示匹配进行快速、紧凑且高度可扩展的视觉地点识别，”发表于 *2020 IEEE
    国际机器人与自动化会议 (ICRA)*，2020年，第3341–3348页。'
- en: '[71] M. Cummins and P. Newman, “Appearance-only slam at large scale with fab-map
    2.0,” *The International Journal of Robotics Research*, vol. 30, no. 9, pp. 1100–1123,
    2011.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Cummins 和 P. Newman，“仅基于外观的全尺度SLAM与fab-map 2.0，”*国际机器人研究期刊*，第30卷，第9期，第1100–1123页，2011年。'
- en: '[72] T. Cieslewski and D. Scaramuzza, “Efficient decentralized visual place
    recognition using a distributed inverted index,” *IEEE Robotics and Automation
    Letters*, vol. 2, no. 2, pp. 640–647, 2017.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] T. Cieslewski 和 D. Scaramuzza，“利用分布式倒排索引进行高效的去中心化视觉地点识别，”*IEEE 机器人与自动化快报*，第2卷，第2期，第640–647页，2017年。'
- en: '[73] M. Mohan, D. Gálvez-López, C. Monteleoni, and G. Sibley, “Environment
    selection and hierarchical place recognition,” in *2015 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2015, pp. 5487–5494.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. Mohan, D. Gálvez-López, C. Monteleoni, 和 G. Sibley，“环境选择与层次化地点识别，”发表于
    *2015 IEEE 国际机器人与自动化会议 (ICRA)*。 IEEE，2015年，第5487–5494页。'
- en: '[74] K. MacTavish and T. D. Barfoot, “Towards hierarchical place recognition
    for long-term autonomy,” in *ICRA Workshop on Visual Place Recognition in Changing
    Environments*, 2014, pp. 1–6.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] K. MacTavish 和 T. D. Barfoot，“迈向长期自主性的层次化地点识别，”发表于 *ICRA 关于视觉地点识别在变化环境中的研讨会*，2014年，第1–6页。'
- en: '[75] L. Han and L. Fang, “Mild: Multi-index hashing for appearance based loop
    closure detection,” in *2017 IEEE International Conference on Multimedia and Expo
    (ICME)*, 2017, pp. 139–144.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] L. Han 和 L. Fang，“MILD：基于外观的回环检测的多索引哈希，”发表于 *2017 IEEE 国际多媒体与博览会 (ICME)*，2017年，第139–144页。'
- en: '[76] P. Hansen and B. Browning, “Visual place recognition using hmm sequence
    matching,” in *2014 IEEE/RSJ International Conference on Intelligent Robots and
    Systems*.   IEEE, 2014, pp. 4549–4555.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] P. Hansen 和 B. Browning，“使用HMM序列匹配进行视觉地点识别，”发表于 *2014 IEEE/RSJ 国际智能机器人与系统会议*。
    IEEE，2014年，第4549–4555页。'
- en: '[77] R. Dubé, A. Cramariuc, D. Dugas, H. Sommer, M. Dymczyk, J. Nieto, R. Siegwart,
    and C. Cadena, “Segmap: Segment-based mapping and localization using data-driven
    descriptors,” *The International Journal of Robotics Research*, vol. 39, no. 2-3,
    pp. 339–355, 2020.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] R. Dubé, A. Cramariuc, D. Dugas, H. Sommer, M. Dymczyk, J. Nieto, R. Siegwart,
    和 C. Cadena，“Segmap：基于段的映射和定位，使用数据驱动描述符，”*国际机器人研究期刊*，第39卷，第2-3期，第339–355页，2020年。'
- en: '[78] D. L. Rizzini, F. Galasso, and S. Caselli, “Geometric relation distribution
    for place recognition,” *IEEE Robotics and Automation Letters*, vol. 4, no. 2,
    pp. 523–529, 2019.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] D. L. Rizzini, F. Galasso, 和 S. Caselli，“用于地点识别的几何关系分布，”*IEEE 机器人与自动化快报*，第4卷，第2期，第523–529页，2019年。'
- en: '[79] C. Premebida, D. R. Faria, and U. Nunes, “Dynamic bayesian network for
    semantic place classification in mobile robotics,” *Autonomous Robots*, vol. 41,
    no. 5, pp. 1161–1172, 2017.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] C. Premebida, D. R. Faria, 和 U. Nunes，“移动机器人中用于语义地点分类的动态贝叶斯网络，”*自主机器人*，第41卷，第5期，第1161–1172页，2017年。'
- en: '[80] F. Cao, Y. Zhuang, H. Zhang, and W. Wang, “Robust place recognition and
    loop closing in laser-based slam for ugvs in urban environments,” *IEEE Sensors
    Journal*, vol. 18, no. 10, pp. 4242–4252, 2018.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] F. Cao, Y. Zhuang, H. Zhang, 和 W. Wang，“激光SLAM中用于城市环境UGV的鲁棒地点识别与回环闭合，”*IEEE
    传感器期刊*，第18卷，第10期，第4242–4252页，2018年。'
- en: '[81] Ş. Săftescu, M. Gadd, D. De Martini, D. Barnes, and P. Newman, “Kidnapped
    radar: Topological radar localisation using rotationally-invariant metric learning,”
    *arXiv preprint arXiv:2001.09438*, 2020.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Ş. Săftescu, M. Gadd, D. De Martini, D. Barnes, 和 P. Newman，“被绑架的雷达：使用旋转不变度量学习进行拓扑雷达定位，”*arXiv
    预印本 arXiv:2001.09438*，2020年。'
- en: '[82] T. Y. Tang, D. De Martini, D. Barnes, and P. Newman, “Rsl-net: Localising
    in satellite images from a radar on the ground,” *IEEE Robotics and Automation
    Letters*, vol. 5, no. 2, pp. 1087–1094, 2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] T. Y. Tang, D. De Martini, D. Barnes, 和 P. Newman，“RSL-Net：从地面雷达定位卫星图像，”*IEEE
    机器人与自动化快报*，第5卷，第2期，第1087–1094页，2020年。'
- en: '[83] T. Cieslewski, E. Stumm, A. Gawel, M. Bosse, S. Lynen, and R. Siegwart,
    “Point cloud descriptors for place recognition using sparse visual information,”
    in *2016 IEEE International Conference on Robotics and Automation (ICRA)*, 2016,
    pp. 4830–4836.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] T. Cieslewski, E. Stumm, A. Gawel, M. Bosse, S. Lynen, 和 R. Siegwart，“利用稀疏视觉信息进行地点识别的点云描述符，”发表于
    *2016 IEEE 国际机器人与自动化会议 (ICRA)*，2016年，第4830–4836页。'
- en: '[84] S. Campbell, N. O’Mahony, L. Krpalcova, D. Riordan, J. Walsh, A. Murphy,
    and C. Ryan, “Sensor technology in autonomous vehicles : A review,” in *2018 29th
    Irish Signals and Systems Conference (ISSC)*, 2018, pp. 1–4.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] S. Campbell, N. O’Mahony, L. Krpalcova, D. Riordan, J. Walsh, A. Murphy,
    和 C. Ryan，“自动驾驶车辆中的传感器技术：综述，”在*2018年第29届爱尔兰信号与系统会议（ISSC）*中，2018年，页码1–4。'
- en: '[85] A. Broggi, P. Grisleri, and P. Zani, “Sensors technologies for intelligent
    vehicles perception systems: A comparison between vision and 3d-lidar,” in *16th
    International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)*,
    2013, pp. 887–892.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] A. Broggi, P. Grisleri, 和 P. Zani，“智能车辆感知系统的传感器技术：视觉和3D-LIDAR的比较，”在*第16届国际IEEE智能交通系统会议（ITSC
    2013）*中，2013年，页码887–892。'
- en: '[86] C. Cadena, D. Gálvez-López, F. Ramos, J. D. Tardós, and J. Neira, “Robust
    place recognition with stereo cameras,” in *2010 IEEE/RSJ International Conference
    on Intelligent Robots and Systems*.   IEEE, 2010, pp. 5182–5189.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] C. Cadena, D. Gálvez-López, F. Ramos, J. D. Tardós, 和 J. Neira，“使用立体相机的鲁棒地点识别，”在*2010
    IEEE/RSJ 国际智能机器人与系统会议*中。IEEE，2010年，页码5182–5189。'
- en: '[87] T. Morris, F. Dayoub, P. Corke, G. Wyeth, and B. Upcroft, “Multiple map
    hypotheses for planning and navigating in non-stationary environments,” in *2014
    IEEE International Conference on Robotics and Automation (ICRA)*.   IEEE, 2014,
    pp. 2765–2770.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] T. Morris, F. Dayoub, P. Corke, G. Wyeth, 和 B. Upcroft，“在非静态环境中进行规划和导航的多个地图假设，”在*2014
    IEEE 国际机器人与自动化会议（ICRA）*中。IEEE，2014年，页码2765–2770。'
- en: '[88] D. Han, Y. Hwang, N. Kim, and Y. Choi, “Multispectral domain invariant
    image for retrieval-based place recognition,” in *2020 IEEE International Conference
    on Robotics and Automation (ICRA)*, 2020, pp. 9271–9277.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] D. Han, Y. Hwang, N. Kim, 和 Y. Choi，“基于检索的地点识别的多光谱领域不变图像，”在*2020 IEEE
    国际机器人与自动化会议（ICRA）*中，2020年，页码9271–9277。'
- en: '[89] T. Fischer and M. J. Milford, “Event-based visual place recognition with
    ensembles of temporal windows,” *IEEE Robotics and Automation Letters*, 2020.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] T. Fischer 和 M. J. Milford，“基于事件的视觉地点识别与时间窗口集成，”*IEEE 机器人与自动化信件*，2020年。'
- en: '[90] H. Yu, H.-W. Chae, and J.-B. Song, “Place recognition based on surface
    graph for a mobile robot,” in *2017 14th International Conference on Ubiquitous
    Robots and Ambient Intelligence (URAI)*.   IEEE, 2017, pp. 342–346.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] H. Yu, H.-W. Chae, 和 J.-B. Song，“基于表面图的移动机器人地点识别，”在*2017年第14届普适机器人与环境智能国际会议（URAI）*中。IEEE，2017年，页码342–346。'
- en: '[91] D. Scaramuzza, F. Fraundorfer, and M. Pollefeys, “Closing the loop in
    appearance-guided omnidirectional visual odometry by using vocabulary trees,”
    *Robotics and Autonomous Systems*, vol. 58, no. 6, pp. 820–827, 2010.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] D. Scaramuzza, F. Fraundorfer, 和 M. Pollefeys，“通过使用词汇树闭合外观引导的全向视觉里程计回路，”*机器人与自动化系统*，第58卷，第6期，页码820–827，2010年。'
- en: '[92] L. Sless, G. Cohen, B. E. Shlomo, and S. Oron, “Self supervised occupancy
    grid learning from sparse radar for autonomous driving,” *arXiv preprint arXiv:1904.00415*,
    2019.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] L. Sless, G. Cohen, B. E. Shlomo, 和 S. Oron，“自监督稀疏雷达占用网格学习用于自动驾驶，”*arXiv
    预印本 arXiv:1904.00415*，2019年。'
- en: '[93] J. Dickmann, J. Klappstein, M. Hahn, N. Appenrodt, H.-L. Bloecher, K. Werber,
    and A. Sailer, “Automotive radar the key technology for autonomous driving: From
    detection and ranging to environmental understanding,” in *2016 IEEE Radar Conference
    (RadarConf)*.   IEEE, 2016, pp. 1–6.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Dickmann, J. Klappstein, M. Hahn, N. Appenrodt, H.-L. Bloecher, K.
    Werber, 和 A. Sailer，“汽车雷达：自动驾驶的关键技术：从检测和测距到环境理解，”在*2016 IEEE 雷达会议（RadarConf）*中。IEEE，2016年，页码1–6。'
- en: '[94] N. Merrill and G. Huang, “Lightweight unsupervised deep loop closure,”
    *arXiv preprint arXiv:1805.07703*, 2018.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] N. Merrill 和 G. Huang，“轻量级无监督深度回环闭合，”*arXiv 预印本 arXiv:1805.07703*，2018年。'
- en: '[95] S. Saftescu, M. Gadd, D. D. Martini, D. Barnes, and P. Newman, “Kidnapped
    radar: Topological radar localisation using rotationally-invariant metric learning,”
    in *2020 IEEE International Conference on Robotics and Automation, ICRA 2020,
    Paris, France, May 31 - August 31, 2020*.   IEEE, 2020, pp. 4358–4364\. [Online].
    Available: https://doi.org/10.1109/ICRA40945.2020.9196682'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] S. Saftescu, M. Gadd, D. D. Martini, D. Barnes, 和 P. Newman，“被劫持雷达：使用旋转不变度量学习的拓扑雷达定位，”在*2020
    IEEE 国际机器人与自动化会议，ICRA 2020，法国巴黎，2020年5月31日-8月31日*。IEEE，2020年，页码4358–4364。 [在线].
    可用链接： https://doi.org/10.1109/ICRA40945.2020.9196682'
- en: '[96] J. Yu, C. Zhu, J. Zhang, Q. Huang, and D. Tao, “Spatial pyramid-enhanced
    netvlad with weighted triplet loss for place recognition,” *IEEE transactions
    on Neural Networks and Learning Systems*, vol. 31, no. 2, pp. 661–674, 2019.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J. Yu, C. Zhu, J. Zhang, Q. Huang, 和 D. Tao，“基于加权三元组损失的空间金字塔增强NetVLAD用于地点识别，”*IEEE
    神经网络与学习系统交易*，第31卷，第2期，页码661–674，2019年。'
- en: '[97] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *International Conference on Learning Representations*,
    2015.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络，” 收录于 *国际学习表示大会*，2015年。'
- en: '[98] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in
    *2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2015,
    pp. 1–9.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke, 和 A. Rabinovich, “深入卷积网络，” 收录于 *2015年IEEE计算机视觉与模式识别大会（CVPR）*，2015年，页码1–9。'
- en: '[99] H. Jegou, F. Perronnin, M. Douze, J. Sánchez, P. Perez, and C. Schmid,
    “Aggregating local image descriptors into compact codes,” *IEEE Transactions on
    Pattern Analysis and Machine Intelligence*, vol. 34, no. 9, pp. 1704–1716, 2011.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] H. Jegou, F. Perronnin, M. Douze, J. Sánchez, P. Perez, 和 C. Schmid, “将局部图像描述符聚合为紧凑编码，”
    *IEEE模式分析与机器智能汇刊*，第34卷，第9期，页码1704–1716，2011年。'
- en: '[100] H. Jégou and O. Chum, “Negative evidences and co-occurences in image
    retrieval: The benefit of pca and whitening,” in *European conference on computer
    vision*.   Springer, 2012, pp. 774–787.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] H. Jégou 和 O. Chum, “图像检索中的负证据和共现：PCA和白化的好处，” 收录于 *欧洲计算机视觉大会*。Springer出版社，2012年，页码774–787。'
- en: '[101] H. Jegou, M. Douze, and C. Schmid, “Hamming embedding and weak geometric
    consistency for large scale image search,” in *European conference on computer
    vision*.   Springer, 2008, pp. 304–317.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] H. Jegou, M. Douze, 和 C. Schmid, “哈明嵌入和弱几何一致性用于大规模图像检索，” 收录于 *欧洲计算机视觉大会*。Springer出版社，2008年，页码304–317。'
- en: '[102] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Object retrieval
    with large vocabularies and fast spatial matching,” in *2007 IEEE conference on
    computer vision and pattern recognition*.   IEEE, 2007, pp. 1–8.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Philbin, O. Chum, M. Isard, J. Sivic, 和 A. Zisserman, “使用大型词汇和快速空间匹配进行物体检索，”
    收录于 *2007年IEEE计算机视觉与模式识别大会*。IEEE出版社，2007年，页码1–8。'
- en: '[103] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Lost in quantization:
    Improving particular object retrieval in large scale image databases,” in *2008
    IEEE Conference on Computer Vision and Pattern Recognition*, 2008, pp. 1–8.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. Philbin, O. Chum, M. Isard, J. Sivic, 和 A. Zisserman, “在量化中迷失：提高大型图像数据库中特定物体的检索，”
    收录于 *2008年IEEE计算机视觉与模式识别大会*，2008年，页码1–8。'
- en: '[104] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, and E. Romera, “Fusion
    and binarization of cnn features for robust topological localization across seasons,”
    in *2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*,
    2016, pp. 4656–4663.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, 和 E. Romera, “CNN特征的融合与二值化用于跨季节鲁棒的拓扑定位，”
    收录于 *2016年IEEE/RSJ国际智能机器人与系统会议（IROS）*，2016年，页码4656–4663。'
- en: '[105] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the
    devil in the details: Delving deep into convolutional nets,” in *Proceedings of
    the British Machine Vision Conference*.   BMVA Press, 2014.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] K. Chatfield, K. Simonyan, A. Vedaldi, 和 A. Zisserman, “细节中的魔鬼归来：深入探讨卷积网络，”
    收录于 *英国机器视觉会议论文集*。BMVA出版社，2014年。'
- en: '[106] X. Yang and K.-T. T. Cheng, “Local difference binary for ultrafast and
    distinctive feature description,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 36, no. 1, pp. 188–194, 2013.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] X. Yang 和 K.-T. T. Cheng, “用于超快和独特特征描述的局部差异二值化，” *IEEE模式分析与机器智能汇刊*，第36卷，第1期，页码188–194，2013年。'
- en: '[107] H. Badino, D. Huber, and T. Kanade, “Real-time topometric localization,”
    in *2012 IEEE International Conference on Robotics and Automation*.   IEEE, 2012,
    pp. 1635–1642.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] H. Badino, D. Huber, 和 T. Kanade, “实时拓扑定位，” 收录于 *2012年IEEE国际机器人与自动化会议*。IEEE出版社，2012年，页码1635–1642。'
- en: '[108] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Communications of the ACM*, vol. 60,
    no. 6, pp. 84–90, 2017.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行Imagenet分类，”
    *ACM通讯*，第60卷，第6期，页码84–90，2017年。'
- en: '[109] A. J. Glover, W. P. Maddern, M. J. Milford, and G. F. Wyeth, “Fab-map
    + ratslam: Appearance-based slam for multiple times of day,” in *2010 IEEE International
    Conference on Robotics and Automation*, 2010, pp. 3507–3512.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] A. J. Glover, W. P. Maddern, M. J. Milford, 和 G. F. Wyeth, “Fab-map +
    ratslam：基于外观的SLAM用于多时间段，” 收录于 *2010年IEEE国际机器人与自动化会议*，2010年，页码3507–3512。'
- en: '[110] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, “Places:
    A 10 million image database for scene recognition,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 40, no. 6, pp. 1452–1464, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, 和 A. Torralba，“Places：一个用于场景识别的
    1000 万图像数据库，” *IEEE 模式分析与机器智能学报*，第 40 卷，第 6 期，第 1452–1464 页，2018。'
- en: '[111] N. Sünderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft,
    and M. Milford, “Place recognition with convnet landmarks: Viewpoint-robust, condition-robust,
    training-free,” *Proceedings of Robotics: Science and Systems XII*, 2015.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] N. Sünderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft,
    和 M. Milford，“使用 convnet 地标进行位置识别：视角鲁棒，条件鲁棒，无需训练，” *机器人学：科学与系统 XII*，2015。'
- en: '[112] S. Dasgupta, “Experiments with random projection,” in *Proceedings of
    the 16th Conference on Uncertainty in Artificial Intelligence*, ser. UAI ’00.   San
    Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2000, p. 143–151.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] S. Dasgupta，“随机投影实验，” 收录于 *第 16 届人工智能不确定性会议论文集*，系列 UAI ’00。美国加州旧金山：Morgan
    Kaufmann Publishers Inc.，2000，第 143–151 页。'
- en: '[113] Y. Kong, W. Liu, and Z. Chen, “Robust convnet landmark-based visual place
    recognition by optimizing landmark matching,” *IEEE Access*, vol. 7, pp. 30 754–30 767,
    2019.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Y. Kong, W. Liu, 和 Z. Chen，“通过优化地标匹配实现鲁棒的 convnet 地标基础视觉位置识别，” *IEEE
    Access*，第 7 卷，第 30,754–30,767 页，2019。'
- en: '[114] M. Cheng, Z. Zhang, W. Lin, and P. Torr, “Bing: Binarized normed gradients
    for objectness estimation at 300fps,” in *2014 IEEE Conference on Computer Vision
    and Pattern Recognition*, 2014, pp. 3286–3293.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] M. Cheng, Z. Zhang, W. Lin, 和 P. Torr，“Bing：用于对象性估计的二值化标准梯度，速度达 300fps，”
    收录于 *2014 IEEE 计算机视觉与模式识别会议*，2014，第 3286–3293 页。'
- en: '[115] E. Bingham and H. Mannila, “Random projection in dimensionality reduction:
    applications to image and text data,” in *Proceedings of the seventh ACM SIGKDD
    international conference on Knowledge discovery and data mining*, 2001, pp. 245–250.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] E. Bingham 和 H. Mannila，“维度约简中的随机投影：应用于图像和文本数据，” 收录于 *第七届 ACM SIGKDD
    国际知识发现与数据挖掘会议论文集*，2001，第 245–250 页。'
- en: '[116] G. L. Oliveira, W. Burgard, and T. Brox, “Efficient deep models for monocular
    road segmentation,” in *2016 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)*, 2016, pp. 4885–4891.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] G. L. Oliveira, W. Burgard, 和 T. Brox，“高效的深度模型用于单目道路分割，” 收录于 *2016 IEEE/RSJ
    国际智能机器人与系统会议 (IROS)*，2016，第 4885–4891 页。'
- en: '[117] D. Achlioptas, “Database-friendly random projections: Johnson-lindenstrauss
    with binary coins,” *Journal of computer and System Sciences*, vol. 66, no. 4,
    pp. 671–687, 2003.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] D. Achlioptas，“数据库友好的随机投影：使用二进制硬币的 Johnson-Lindenstrauss，” *计算机与系统科学学报*，第
    66 卷，第 4 期，第 671–687 页，2003。'
- en: '[118] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, 2016, pp. 3213–3223.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, 和 B. Schiele，“用于语义城市场景理解的 cityscapes 数据集，” 收录于 *2016 IEEE
    计算机视觉与模式识别会议 (CVPR)*，2016，第 3213–3223 页。'
- en: '[119] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, “Virtualworlds as proxy for
    multi-object tracking analysis,” in *2016 IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*, 2016, pp. 4340–4349.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] A. Gaidon, Q. Wang, Y. Cabon, 和 E. Vig，“虚拟世界作为多目标跟踪分析的代理，” 收录于 *2016
    IEEE 计算机视觉与模式识别会议 (CVPR)*，2016，第 4340–4349 页。'
- en: '[120] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] K. Simonyan 和 A. Zisserman，“用于大规模图像识别的非常深的卷积网络，” *arXiv 预印本 arXiv:1409.1556*，2014。'
- en: '[121] J. Sivic and A. Zisserman, “Video google: A text retrieval approach to
    object matching in videos,” in *null*.   IEEE, 2003, p. 1470.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] J. Sivic 和 A. Zisserman，“视频谷歌：一种基于文本的对象匹配方法，” 收录于 *null*。 IEEE，2003，第
    1470 页。'
- en: '[122] A. Khaliq, S. Ehsan, Z. Chen, M. Milford, and K. McDonald-Maier, “A holistic
    visual place recognition approach using lightweight cnns for significant viewpoint
    and appearance changes,” *IEEE Transactions on Robotics*, vol. 36, no. 2, pp.
    561–569, 2020.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] A. Khaliq, S. Ehsan, Z. Chen, M. Milford, 和 K. McDonald-Maier，“一种使用轻量级
    cnns 的整体视觉位置识别方法，适用于显著的视角和外观变化，” *IEEE 机器人学学报*，第 36 卷，第 2 期，第 561–569 页，2020。'
- en: '[123] Z. Chen, F. Maffra, I. Sa, and M. Chli, “Only look once, mining distinctive
    landmarks from convnet for visual place recognition,” in *2017 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*.   IEEE, 2017, pp. 9–16.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Z. Chen, F. Maffra, I. Sa, 和 M. Chli, “仅查看一次，从卷积网络中挖掘独特的地标以进行视觉位置识别”，见
    *2017 IEEE/RSJ 国际智能机器人与系统会议 (IROS)*。IEEE, 2017, pp. 9–16。'
- en: '[124] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in neural information processing
    systems*, 2012, pp. 1097–1105.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行ImageNet分类”，见
    *神经信息处理系统进展*，2012, pp. 1097–1105。'
- en: '[125] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, “Learning
    deep features for scene recognition using places database,” in *Advances in neural
    information processing systems*, 2014, pp. 487–495.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, 和 A. Oliva, “利用Places数据库进行场景识别的深度特征学习”，见
    *神经信息处理系统进展*，2014, pp. 487–495。'
- en: '[126] D. Ravichandran, P. Pantel, and E. Hovy, “Randomized algorithms and nlp:
    Using locality sensitive hash functions for high speed noun clustering,” in *Proceedings
    of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05)*,
    2005, pp. 622–629.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] D. Ravichandran, P. Pantel, 和 E. Hovy, “随机算法与自然语言处理：使用局部敏感哈希函数进行高速名词聚类”，见
    *第43届计算语言学协会年会 (ACL’05)*，2005, pp. 622–629。'
- en: '[127] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals from
    edges,” in *European conference on computer vision*.   Springer, 2014, pp. 391–405.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] C. L. Zitnick 和 P. Dollár, “边缘框：从边缘定位对象提议”，见 *欧洲计算机视觉会议*。Springer, 2014,
    pp. 391–405。'
- en: '[128] A. Torii, J. Sivic, M. Okutomi, and T. Pajdla, “Visual place recognition
    with repetitive structures,” in *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 37, no. 11, 2015, pp. 2346–2359.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Torii, J. Sivic, M. Okutomi, 和 T. Pajdla, “具有重复结构的视觉位置识别”，见 *IEEE
    计算机视觉与模式识别汇刊*，第37卷，第11期，2015, pp. 2346–2359。'
- en: '[129] C. Wu, R. Manmatha, A. J. Smola, and P. Krähenbühl, “Sampling matters
    in deep embedding learning,” in *2017 IEEE International Conference on Computer
    Vision (ICCV)*, 2017, pp. 2859–2867.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] C. Wu, R. Manmatha, A. J. Smola, 和 P. Krähenbühl, “在深度嵌入学习中采样的重要性”，见
    *2017 IEEE 计算机视觉国际会议 (ICCV)*，2017, pp. 2859–2867。'
- en: '[130] A. Torii, R. Arandjelović, J. Sivic, M. Okutomi, and T. Pajdla, “24/7
    place recognition by view synthesis,” in *CVPR*, 2015.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] A. Torii, R. Arandjelović, J. Sivic, M. Okutomi, 和 T. Pajdla, “通过视图合成进行24/7地点识别”，见
    *CVPR*，2015。'
- en: '[131] K. Qiu, Y. Ai, B. Tian, B. Wang, and D. Cao, “Siamese-resnet: implementing
    loop closure detection based on siamese network,” in *2018 IEEE Intelligent Vehicles
    Symposium (IV)*.   IEEE, 2018, pp. 716–721.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] K. Qiu, Y. Ai, B. Tian, B. Wang, 和 D. Cao, “Siamese-ResNet：基于Siamese网络实现回环闭合检测”，见
    *2018 IEEE 智能车辆研讨会 (IV)*。IEEE, 2018, pp. 716–721。'
- en: '[132] S. Chopra, R. Hadsell, and Y. LeCun, “Learning a similarity metric discriminatively,
    with application to face verification,” in *2005 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition (CVPR’05)*, vol. 1.   IEEE, 2005, pp.
    539–546.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] S. Chopra, R. Hadsell, 和 Y. LeCun, “通过应用于面部验证的相似度度量学习”，见 *2005 IEEE 计算机视觉与模式识别会议
    (CVPR’05)*，第1卷。IEEE, 2005, pp. 539–546。'
- en: '[133] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A benchmark
    for the evaluation of rgb-d slam systems,” in *2012 IEEE/RSJ International Conference
    on Intelligent Robots and Systems*, 2012, pp. 573–580.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. Sturm, N. Engelhard, F. Endres, W. Burgard, 和 D. Cremers, “RGB-D SLAM系统评估基准”，见
    *2012 IEEE/RSJ 国际智能机器人与系统会议*，2012, pp. 573–580。'
- en: '[134] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision
    applications,” *arXiv preprint arXiv:1704.04861*, 2017.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto, 和 H. Adam, “Mobilenets：面向移动视觉应用的高效卷积神经网络”，*arXiv 预印本 arXiv:1704.04861*，2017。'
- en: '[135] X. Wang, Y. Shi, and K. M. Kitani, “Deep supervised hashing with triplet
    labels,” in *Asian conference on computer vision*.   Springer, 2016, pp. 70–84.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] X. Wang, Y. Shi, 和 K. M. Kitani, “带有三元组标签的深度监督哈希”，见 *亚洲计算机视觉大会*。Springer,
    2016, pp. 70–84。'
- en: '[136] W.-J. Li, S. Wang, and W.-C. Kang, “Feature learning based deep supervised
    hashing with pairwise labels,” *arXiv preprint arXiv:1511.03855*, 2015.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] W.-J. Li, S. Wang, 和 W.-C. Kang, “基于深度监督哈希的特征学习与成对标签”，*arXiv 预印本 arXiv:1511.03855*，2015。'
- en: '[137] S. Hausler, A. Jacobson, and M. Milford, “Filter early, match late: Improving
    network-based visual place recognition,” in *2019 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*, 2019, pp. 3268–3275.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] S. Hausler, A. Jacobson, 和 M. Milford，“早期过滤，晚期匹配：改进基于网络的视觉地点识别，”发表在*2019
    IEEE/RSJ国际智能机器人与系统大会（IROS）*，2019年，页码3268–3275。'
- en: '[138] Z. Chen, A. Jacobson, N. Sünderhauf, B. Upcroft, L. Liu, C. Shen, I. Reid,
    and M. Milford, “Deep learning features at scale for visual place recognition,”
    in *2017 IEEE International Conference on Robotics and Automation (ICRA)*, 2017,
    pp. 3223–3230.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Z. Chen, A. Jacobson, N. Sünderhauf, B. Upcroft, L. Liu, C. Shen, I.
    Reid, 和 M. Milford，“大规模深度学习特征用于视觉地点识别，”发表在*2017 IEEE国际机器人与自动化会议（ICRA）*，2017年，页码3223–3230。'
- en: '[139] S. Appalaraju and V. Chaoji, “Image similarity using deep cnn and curriculum
    learning,” *arXiv preprint arXiv:1709.08761*, 2017.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] S. Appalaraju 和 V. Chaoji，“使用深度CNN和课程学习的图像相似性，”*arXiv预印本 arXiv:1709.08761*，2017年。'
- en: '[140] E. Hoffer and N. Ailon, “Deep metric learning using triplet network,”
    in *Similarity-Based Pattern Recognition*, A. Feragen, M. Pelillo, and M. Loog,
    Eds.   Cham: Springer International Publishing, 2015, pp. 84–92.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] E. Hoffer 和 N. Ailon，“使用三元组网络的深度度量学习，”在*基于相似性的模式识别*，A. Feragen, M. Pelillo,
    和 M. Loog 编，Cham: Springer International Publishing，2015年，页码84–92。'
- en: '[141] N. Carlevaris-Bianco, A. K. Ushani, and R. M. Eustice, “University of
    Michigan North Campus long-term vision and LiDAR dataset,” *International Journal
    of Robotics Research*, vol. 35, no. 9, pp. 1023–1035, 2015.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] N. Carlevaris-Bianco, A. K. Ushani, 和 R. M. Eustice，“密歇根大学北校区长期视觉与LiDAR数据集，”*国际机器人研究杂志*，第35卷，第9期，页码1023–1035，2015年。'
- en: '[142] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *2012 IEEE Conference on Computer Vision
    and Pattern Recognition*, 2012, pp. 3354–3361.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] A. Geiger, P. Lenz, 和 R. Urtasun，“我们准备好自动驾驶了吗？KITTI视觉基准套件，”发表在*2012 IEEE计算机视觉与模式识别会议*，2012年，页码3354–3361。'
- en: '[143] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction by learning
    an invariant mapping,” in *2006 IEEE Computer Society Conference on Computer Vision
    and Pattern Recognition (CVPR’06)*, vol. 2, 2006, pp. 1735–1742.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] R. Hadsell, S. Chopra, 和 Y. LeCun，“通过学习不变映射实现维度减少，”发表在*2006 IEEE计算机协会计算机视觉与模式识别会议（CVPR’06）*，第2卷，2006年，页码1735–1742。'
- en: '[144] H. Yin, Y. Wang, X. Ding, L. Tang, S. Huang, and R. Xiong, “3d lidar-based
    global localization using siamese neural network,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 21, no. 4, pp. 1380–1392, 2020.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] H. Yin, Y. Wang, X. Ding, L. Tang, S. Huang, 和 R. Xiong，“基于3D LiDAR的全局定位使用Siamese神经网络，”*IEEE智能交通系统汇刊*，第21卷，第4期，页码1380–1392，2020年。'
- en: '[145] W. Chen, X. Chen, J. Zhang, and K. Huang, “Beyond triplet loss: a deep
    quadruplet network for person re-identification,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2017, pp. 403–412.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] W. Chen, X. Chen, J. Zhang, 和 K. Huang，“超越三元组损失：用于行人重识别的深度四元组网络，”发表在*IEEE计算机视觉与模式识别会议论文集*，2017年，页码403–412。'
- en: '[146] H. Jégou, M. Douze, C. Schmid, and P. Pérez, “Aggregating local descriptors
    into a compact image representation,” in *2010 IEEE computer society conference
    on computer vision and pattern recognition*.   IEEE, 2010, pp. 3304–3311.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] H. Jégou, M. Douze, C. Schmid, 和 P. Pérez，“将局部描述符聚合成紧凑的图像表示，”发表在*2010
    IEEE计算机协会计算机视觉与模式识别会议论文集*。IEEE，2010年，页码3304–3311。'
- en: '[147] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，”发表在*IEEE计算机视觉与模式识别会议论文集*，2016年，页码770–778。'
- en: '[148] D. Blalock, J. J. G. Ortiz, J. Frankle, and J. Guttag, “What is the state
    of neural network pruning?” *arXiv preprint arXiv:2003.03033*, 2020.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] D. Blalock, J. J. G. Ortiz, J. Frankle, 和 J. Guttag，“神经网络剪枝的现状如何？”*arXiv预印本
    arXiv:2003.03033*，2020年。'
- en: '[149] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” *IEEE
    Transactions on Pattern Analysis and Machine Intelligence*, vol. 40, no. 3, pp.
    611–625, 2017.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] J. Engel, V. Koltun, 和 D. Cremers，“直接稀疏视觉里程计，”*IEEE模式分析与机器智能汇刊*，第40卷，第3期，页码611–625，2017年。'
- en: '[150] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 652–660.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas，“Pointnet：用于3D分类和分割的点集深度学习，”发表在*IEEE计算机视觉与模式识别会议论文集*，2017年，页码652–660。'
- en: '[151] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embedding
    for face recognition and clustering,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2015, pp. 815–823.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] F. Schroff, D. Kalenichenko, 和 J. Philbin，“Facenet：一个统一的人脸识别和聚类嵌入，” 在
    *IEEE计算机视觉与模式识别大会论文集*，2015年，页码815–823。'
- en: '[152] Y. Latif, R. Garg, M. Milford, and I. Reid, “Addressing challenging place
    recognition tasks using generative adversarial networks,” in *2018 IEEE International
    Conference on Robotics and Automation (ICRA)*, 2018, pp. 2349–2355.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Y. Latif, R. Garg, M. Milford, 和 I. Reid，“使用生成对抗网络解决具有挑战性的地点识别任务，” 在
    *2018 IEEE国际机器人与自动化大会（ICRA）*，2018年，页码2349–2355。'
- en: '[153] P. Yin, L. Xu, Z. Liu, L. Li, H. Salman, Y. He, W. Xu, H. Wang, and H. Choset,
    “Stabilize an unsupervised feature learning for lidar-based place recognition,”
    in *2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*,
    2018, pp. 1162–1167.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] P. Yin, L. Xu, Z. Liu, L. Li, H. Salman, Y. He, W. Xu, H. Wang, 和 H.
    Choset，“稳定化基于激光雷达的无监督特征学习以进行地点识别，” 在 *2018 IEEE/RSJ国际智能机器人与系统大会（IROS）*，2018年，页码1162–1167。'
- en: '[154] J. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *2017 IEEE International
    Conference on Computer Vision (ICCV)*, 2017, pp. 2242–2251.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] J. Zhu, T. Park, P. Isola, 和 A. A. Efros，“使用循环一致性对抗网络的无配对图像到图像转换，” 在
    *2017 IEEE国际计算机视觉大会（ICCV）*，2017年，页码2242–2251。'
- en: '[155] Zhou Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE Transactions
    on Image Processing*, vol. 13, no. 4, pp. 600–612, 2004.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Zhou Wang, A. C. Bovik, H. R. Sheikh, 和 E. P. Simoncelli，“图像质量评估：从误差可视性到结构相似性，”
    *IEEE图像处理汇刊*，第13卷，第4期，页码600–612，2004年。'
- en: '[156] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in *2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    (CVPR’05)*, vol. 1, 2005, pp. 886–893 vol. 1.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] N. Dalal 和 B. Triggs，“用于人类检测的方向梯度直方图，” 在 *2005 IEEE计算机学会计算机视觉与模式识别会议（CVPR’05）*，第1卷，2005年，页码886–893，第1卷。'
- en: '[157] R. Hartley and A. Zisserman, *Multiple view geometry in computer vision*.   Cambridge
    university press, 2003.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] R. Hartley 和 A. Zisserman，*计算机视觉中的多视角几何*。剑桥大学出版社，2003年。'
- en: '[158] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim, “Learning to discover
    cross-domain relations with generative adversarial networks,” *arXiv preprint
    arXiv:1703.05192*, 2017.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] T. Kim, M. Cha, H. Kim, J. K. Lee, 和 J. Kim，“利用生成对抗网络学习发现跨域关系，” *arXiv预印本arXiv:1703.05192*，2017年。'
- en: '[159] S. Hwang, J. Park, N. Kim, Y. Choi, and I. S. Kweon, “Multispectral pedestrian
    detection: Benchmark dataset and baseline,” in *2015 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2015, pp. 1037–1045.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] S. Hwang, J. Park, N. Kim, Y. Choi, 和 I. S. Kweon，“多光谱行人检测：基准数据集和基线，”
    在 *2015 IEEE计算机视觉与模式识别大会（CVPR）*，2015年，页码1037–1045。'
- en: '[160] Z. Wang, J. Li, S. Khademi, and J. van Gemert, “Attention-aware age-agnostic
    visual place recognition,” in *2019 IEEE/CVF International Conference on Computer
    Vision Workshop (ICCVW)*, 2019, pp. 1437–1446.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Z. Wang, J. Li, S. Khademi, 和 J. van Gemert，“关注年龄无关的视觉地点识别，” 在 *2019
    IEEE/CVF国际计算机视觉大会研讨会（ICCVW）*，2019年，页码1437–1446。'
- en: '[161] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Schölkopf,
    and A. J. Smola, “Integrating structured biological data by kernel maximum mean
    discrepancy,” *Bioinformatics*, vol. 22, no. 14, pp. e49–e57, 2006.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Schölkopf,
    和 A. J. Smola，“通过核最大均值差异整合结构化生物数据，” *生物信息学*，第22卷，第14期，页码e49–e57，2006年。'
- en: '[162] L. Tang, Y. Wang, Q. Luo, X. Ding, and R. Xiong, “Adversarial feature
    disentanglement for place recognition across changing appearance,” in *2020 IEEE
    International Conference on Robotics and Automation (ICRA)*, 2020, pp. 1301–1307.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] L. Tang, Y. Wang, Q. Luo, X. Ding, 和 R. Xiong，“针对外观变化的地点识别的对抗特征解缠结，”
    在 *2020 IEEE国际机器人与自动化大会（ICRA）*，2020年，页码1301–1307。'
- en: '[163] X. Mao, Q. Li, H. Xie, R. Y. K. Lau, Z. Wang, and S. P. Smolley, “Least
    squares generative adversarial networks,” in *2017 IEEE International Conference
    on Computer Vision (ICCV)*, 2017, pp. 2813–2821.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] X. Mao, Q. Li, H. Xie, R. Y. K. Lau, Z. Wang, 和 S. P. Smolley，“最小二乘生成对抗网络，”
    在 *2017 IEEE国际计算机视觉大会（ICCV）*，2017年，页码2813–2821。'
- en: '[164] O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,”
    *British Machine Vision Association*, 2015.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] O. M. Parkhi, A. Vedaldi, 和 A. Zisserman，“深度人脸识别，” *英国机器视觉协会*，2015年。'
- en: '[165] A. Brock, T. Lim, J. M. Ritchie, and N. Weston, “Generative and discriminative
    voxel modeling with convolutional neural networks,” *In: Workshop on 3D Deep Learning,
    NIPS*, 2016.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] A. Brock, T. Lim, J. M. Ritchie, 和 N. Weston，“使用卷积神经网络的生成与判别体素建模，”*在：3D深度学习研讨会，NIPS*，2016年。'
- en: '[166] S. Hausler, A. Jacobson, and M. Milford, “Multi-process fusion: Visual
    place recognition using multiple image processing methods,” *IEEE Robotics and
    Automation Letters*, vol. 4, no. 2, pp. 1924–1931, 2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] S. Hausler, A. Jacobson, 和 M. Milford，“多处理融合：使用多种图像处理方法的视觉位置识别，”*IEEE机器人与自动化汇刊*，第4卷，第2期，第1924–1931页，2019年。'
- en: '[167] A. Jacobson, Z. Chen, and M. Milford, “Leveraging variable sensor spatial
    acuity with a homogeneous, multi-scale place recognition framework,” *Biological
    cybernetics*, vol. 112, no. 3, pp. 209–225, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] A. Jacobson, Z. Chen, 和 M. Milford，“利用变量传感器空间分辨率的均匀多尺度位置识别框架，”*生物控制论*，第112卷，第3期，第209–225页，2018年。'
- en: '[168] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, and E. Romera, “Towards
    life-long visual localization using an efficient matching of binary sequences
    from images,” in *2015 IEEE International Conference on Robotics and Automation
    (ICRA)*, 2015, pp. 6328–6335.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, 和 E. Romera，“通过高效匹配图像中的二进制序列实现终身视觉定位，”发表于*2015
    IEEE国际机器人与自动化会议 (ICRA)*，2015，第6328–6335页。'
- en: '[169] Y. Latif, G. Huang, J. Leonard, and J. Neira, “An online sparsity-cognizant
    loop-closure algorithm for visual navigation,” in *Proceedings of Robotics: Science
    and Systems*, Berkeley, USA, July 2014.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Y. Latif, G. Huang, J. Leonard, 和 J. Neira，“一种用于视觉导航的在线稀疏感知回环闭合算法，”发表于*机器人学：科学与系统会议论文集*，美国伯克利，2014年7月。'
- en: '[170] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time
    object detection with region proposal networks,” in *Advances in neural information
    processing systems*, 2015, pp. 91–99.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] S. Ren, K. He, R. Girshick, 和 J. Sun，“Faster R-CNN：实现实时目标检测的区域提议网络，”发表于*神经信息处理系统进展*，2015，第91–99页。'
- en: '[171] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardós, “Orb-slam: A versatile
    and accurate monocular slam system,” *IEEE Transactions on Robotics*, vol. 31,
    no. 5, pp. 1147–1163, 2015.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] R. Mur-Artal, J. M. M. Montiel, 和 J. D. Tardós，“Orb-slam：一种多功能且精确的单目slam系统，”*IEEE机器人学汇刊*，第31卷，第5期，第1147–1163页，2015年。'
- en: '[172] E. Pepperell, P. I. Corke, and M. J. Milford, “All-environment visual
    place recognition with smart,” in *2014 IEEE international conference on robotics
    and automation (ICRA)*.   IEEE, 2014, pp. 1612–1618.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] E. Pepperell, P. I. Corke, 和 M. J. Milford，“智能全环境视觉位置识别，”发表于*2014 IEEE国际机器人与自动化会议
    (ICRA)*。 IEEE，2014，第1612–1618页。'
- en: '[173] T. Naseer, W. Burgard, and C. Stachniss, “Robust visual localization
    across seasons,” *IEEE Transactions on Robotics*, vol. 34, no. 2, pp. 289–302,
    2018.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] T. Naseer, W. Burgard, 和 C. Stachniss，“跨季节的稳健视觉定位，”*IEEE机器人学汇刊*，第34卷，第2期，第289–302页，2018年。'
- en: '[174] C. McManus, B. Upcroft, and P. Newmann, “Scene signatures: Localised
    and point-less features for localisation,” in *Proceedings of Robotics: Science
    and Systems*, Berkeley, USA, July 2014.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] C. McManus, B. Upcroft, 和 P. Newmann，“场景签名：用于定位的局部化和无点特征，”发表于*机器人学：科学与系统会议论文集*，美国伯克利，2014年7月。'
- en: '[175] Z. Chen, L. Liu, I. Sa, Z. Ge, and M. Chli, “Learning context flexible
    attention model for long-term visual place recognition,” *IEEE Robotics and Automation
    Letters*, vol. 3, no. 4, pp. 4015–4022, 2018.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Z. Chen, L. Liu, I. Sa, Z. Ge, 和 M. Chli，“学习上下文灵活的注意力模型用于长期视觉位置识别，”*IEEE机器人与自动化汇刊*，第3卷，第4期，第4015–4022页，2018年。'
- en: '[176] S. Hausler and M. Milford, “Hierarchical multi-process fusion for visual
    place recognition,” in *2020 IEEE International Conference on Robotics and Automation,
    ICRA 2020, Paris, France, May 31 - August 31, 2020*.   IEEE, 2020, pp. 3327–3333\.
    [Online]. Available: https://doi.org/10.1109/ICRA40945.2020.9197360'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] S. Hausler 和 M. Milford，“用于视觉位置识别的层次多处理融合，”发表于*2020 IEEE国际机器人与自动化会议，ICRA
    2020，法国巴黎，2020年5月31日 - 8月31日*。 IEEE，2020，第3327–3333页。[在线]. 可用链接: https://doi.org/10.1109/ICRA40945.2020.9197360'
- en: '[177] A. Oliva and A. Torralba, “Modeling the shape of the scene: A holistic
    representation of the spatial envelope,” *International journal of computer vision*,
    vol. 42, no. 3, pp. 145–175, 2001.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] A. Oliva 和 A. Torralba，“场景形状建模：空间包络的整体表示，”*国际计算机视觉期刊*，第42卷，第3期，第145–175页，2001年。'
- en: '[178] P. F. Alcantarilla, A. Bartoli, and A. J. Davison, “Kaze features,” in
    *European Conference on Computer Vision*.   Springer, 2012, pp. 214–227.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] P. F. Alcantarilla, A. Bartoli, 和 A. J. Davison，“Kaze特征，”发表于*欧洲计算机视觉会议*。
    Springer，2012，第214–227页。'
- en: '[179] G. Lin, A. Milan, C. Shen, and I. Reid, “Refinenet: Multi-path refinement
    networks for high-resolution semantic segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 1925–1934.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] G. Lin, A. Milan, C. Shen, 和 I. Reid, “Refinenet: 多路径细化网络用于高分辨率语义分割,”
    发表在 *IEEE 计算机视觉与模式识别会议论文集*，2017年，第1925–1934页。'
- en: '[180] S. An, G. Che, F. Zhou, X. Liu, X. Ma, and Y. Chen, “Fast and incremental
    loop closure detection using proximity graphs,” in *2019 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2019, pp. 378–385.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] S. An, G. Che, F. Zhou, X. Liu, X. Ma, 和 Y. Chen, “使用邻近图进行快速和增量式回环检测,”
    发表在 *2019 IEEE/RSJ 国际智能机器人与系统会议 (IROS)*，2019年，第378–385页。'
- en: '[181] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in *2018 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2018, pp. 4510–4520.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, 和 L. Chen, “Mobilenetv2:
    反向残差和线性瓶颈,” 发表在 *2018 IEEE/CVF 计算机视觉与模式识别会议*，2018年，第4510–4520页。'
- en: '[182] J. Sivic and A. Zisserman, “Efficient visual search of videos cast as
    text retrieval,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 31, no. 4, pp. 591–606, 2008.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] J. Sivic 和 A. Zisserman, “将视频的高效视觉搜索视作文本检索,” *IEEE 计算机视觉与模式分析学报*，第31卷，第4期，第591–606页，2008年。'
- en: '[183] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The
    synthia dataset: A large collection of synthetic images for semantic segmentation
    of urban scenes,” in *2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2016, pp. 3234–3243.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, 和 A. M. Lopez, “Synthia
    数据集: 一大集合用于城市场景语义分割的合成图像,” 发表在 *2016 IEEE 计算机视觉与模式识别会议 (CVPR)*，2016年，第3234–3243页。'
- en: '[184] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    *International journal of computer vision*, vol. 60, no. 2, pp. 91–110, 2004.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] D. G. Lowe, “基于尺度不变关键点的独特图像特征,” *国际计算机视觉杂志*，第60卷，第2期，第91–110页，2004年。'
- en: '[185] J.-L. Blanco, F.-A. Moreno, and J. Gonzalez, “A collection of outdoor
    robotic datasets with centimeter-accuracy ground truth,” *Autonomous Robots*,
    vol. 27, no. 4, p. 327, 2009.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] J.-L. Blanco, F.-A. Moreno, 和 J. Gonzalez, “一个具有厘米级准确度的户外机器人数据集集合,” *自主机器人*，第27卷，第4期，第327页，2009年。'
- en: '[186] M. Smith, I. Baldwin, W. Churchill, R. Paul, and P. Newman, “The new
    college vision and laser data set,” *The International Journal of Robotics Research*,
    vol. 28, no. 5, pp. 595–599, 2009.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] M. Smith, I. Baldwin, W. Churchill, R. Paul, 和 P. Newman, “新学院视觉与激光数据集,”
    *国际机器人研究杂志*，第28卷，第5期，第595–599页，2009年。'
- en: '[187] S. Niko, P. Neubert, and P. Protzel, “Are we there yet? challenging SeqSLAM
    on a 3000 km journey across all four seasons,” in *Proc. of IEEE International
    Conference on Robotics and Automation Workshops*, 2013.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] S. Niko, P. Neubert, 和 P. Protzel, “我们到了吗？挑战 SeqSLAM 在 3000 公里的四季旅行中,”
    发表在 *IEEE 国际机器人与自动化会议工作坊论文集*，2013年。'
- en: '![[Uncaptioned image]](img/fe64f3b2b8c15bc4f52971f46800b081.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/fe64f3b2b8c15bc4f52971f46800b081.png)'
- en: Tiago Barros received the Master degree in Electrical and Computer Engineering
    from the University of Coimbra, Portugal, in 2015\. He is currently working towards
    the Ph.D. degree in the Institute of Systems and Robotics, University of Coimbra,
    Portugal. His research interests include deep learning, perception and localization.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: Tiago Barros 于2015年获得了葡萄牙科英布拉大学电气与计算机工程硕士学位。他目前在葡萄牙科英布拉大学系统与机器人研究所攻读博士学位。他的研究兴趣包括深度学习、感知和定位。
- en: '![[Uncaptioned image]](img/a9156b2cec8a00c04c2629406c2f25fe.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/a9156b2cec8a00c04c2629406c2f25fe.png)'
- en: Ricardo Pereira received the Master degree in Electrical and Computer Engineering
    from the University of Coimbra, Portugal. He is currently working towards the
    Ph.D. degree in the Institute of Systems and Robotics, University of Coimbra,
    Portugal. His research interests include deep learning, perception, and mobile
    robotics.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: Ricardo Pereira 获得了葡萄牙科英布拉大学电气与计算机工程硕士学位。他目前在葡萄牙科英布拉大学系统与机器人研究所攻读博士学位。他的研究兴趣包括深度学习、感知和移动机器人。
- en: '![[Uncaptioned image]](img/f2e9873e145fb9e687187d659bc3f2ef.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/f2e9873e145fb9e687187d659bc3f2ef.png)'
- en: Cristiano Premebida is Assistant Professor in the department of electrical and
    computer engineering at the University of Coimbra, Portugal, where he is a member
    of the Institute of Systems and Robotics (ISR-UC). His main research interests
    are autonomous vehicles, autonomous robots, robotic perception, cooperative/connected
    intelligent transport systems (CITS), ADAS, machine learning, and sensor fusion.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: Cristiano Premebida 是葡萄牙科英布拉大学电气与计算机工程系的助理教授，同时是系统与机器人研究所（ISR-UC）的成员。他的主要研究兴趣包括自动驾驶车辆、自动机器人、机器人视觉、合作/连接智能运输系统（CITS）、ADAS、机器学习和传感器融合。
- en: C. Premebida has collaborated on research projects in the areas related to C-ITS,
    autonomous driving, and applied machine learning, including national and international
    projects. He is an IEEE-ITS society member, has served as AE in the flagship conferences
    ITSC and IVS, and has regularly organized international workshops on automated
    driving, AI/ML perception, and C-ITS.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: C. Premebida 曾参与与 C-ITS、自动驾驶和应用机器学习相关的研究项目，包括国家和国际项目。他是 IEEE-ITS 社区的成员，曾在旗舰会议
    ITSC 和 IVS 担任 AE，并定期组织关于自动驾驶、AI/ML 视觉和 C-ITS 的国际研讨会。
- en: '![[Uncaptioned image]](img/f8e2b37ee22aa51f09d36c1ab69d98b7.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/f8e2b37ee22aa51f09d36c1ab69d98b7.png)'
- en: Luís Garrote received the Master degree in Electrical and Computer Engineering
    from the University of Coimbra, Portugal. He is currently working towards the
    Ph.D. degree in the Institute of Systems and Robotics, University of Coimbra,
    Portugal. His research interests include deep learning, perception, and mobile
    robotics.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: Luís Garrote 获得了葡萄牙科英布拉大学电气与计算机工程硕士学位。目前，他在科英布拉大学系统与机器人研究所攻读博士学位。他的研究兴趣包括深度学习、视觉感知和移动机器人。
- en: '![[Uncaptioned image]](img/b6a49661295e092f359242eb9a6b617b.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/b6a49661295e092f359242eb9a6b617b.png)'
- en: 'Urbano J. Nunes (S’90-M’95-SM’09) received the Ph.D. in Electrical Engineering
    from the University of Coimbra, Portugal, in 1995\. He is a Full Professor with
    the Electrical and Computer Engineering Department of Coimbra University, and
    a Senior researcher of the Institute for Systems and Robotics (ISR-UC) where he
    is the coordinator of the Human-Centered Mobile Robotics lab. He has been involved
    with/responsible for several funded projects at both national and international
    levels in the areas of mobile robotics. He serves as Associate Editor the IEEE
    Transactions on Intelligent Vehicles (2015-). Prof. Nunes was with several international
    conferences: General co-chair of the 11th IEEE ICAR2003; Program Chair of IEEE
    ITSC2006; General Chair of the 13th IEEE ITSC2010; General Chair for the IEEE/RSJ
    IROS 2012; and General Chair of the IEEE ROMAN2017.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: Urbano J. Nunes (S’90-M’95-SM’09) 于 1995 年在葡萄牙科英布拉大学获得电气工程博士学位。他是科英布拉大学电气与计算机工程系的全职教授，同时也是系统与机器人研究所（ISR-UC）的高级研究员，负责领导以人为本的移动机器人实验室。他曾负责多个国家和国际层面的移动机器人领域的资助项目。他还担任
    IEEE Transactions on Intelligent Vehicles 的副编辑（2015-）。Nunes 教授曾参与多个国际会议：第 11 届
    IEEE ICAR2003 的总主席；IEEE ITSC2006 的程序主席；第 13 届 IEEE ITSC2010 的总主席；IEEE/RSJ IROS
    2012 的总主席；以及 IEEE ROMAN2017 的总主席。
