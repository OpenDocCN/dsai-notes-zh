- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:34:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:34:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2401.17592] Local Feature Matching Using Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2401.17592] 本地特征匹配使用深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.17592](https://ar5iv.labs.arxiv.org/html/2401.17592)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.17592](https://ar5iv.labs.arxiv.org/html/2401.17592)
- en: 'Local Feature Matching Using Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地特征匹配使用深度学习：综述
- en: Shibiao Xu Shunpeng Chen Rongtao Xu Changwei Wang Peng Lu Li Guo School of Artificial
    Intelligence, Beijing University of Posts and Telecommunications, China The State
    Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation,
    Chinese Academy of Sciences, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Shibiao Xu Shunpeng Chen Rongtao Xu Changwei Wang Peng Lu Li Guo 北京邮电大学人工智能学院，中国
    中国科学院自动化研究所多模态人工智能系统国家重点实验室，中国
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Local feature matching enjoys wide-ranging applications in the realm of computer
    vision, encompassing domains such as image retrieval, 3D reconstruction, and object
    recognition. However, challenges persist in improving the accuracy and robustness
    of matching due to factors like viewpoint and lighting variations. In recent years,
    the introduction of deep learning models has sparked widespread exploration into
    local feature matching techniques. The objective of this endeavor is to furnish
    a comprehensive overview of local feature matching methods. These methods are
    categorized into two key segments based on the presence of detectors. The Detector-based
    category encompasses models inclusive of Detect-then-Describe, Joint Detection
    and Description, Describe-then-Detect, as well as Graph Based techniques. In contrast,
    the Detector-free category comprises CNN Based, Transformer Based, and Patch Based
    methods. Our study extends beyond methodological analysis, incorporating evaluations
    of prevalent datasets and metrics to facilitate a quantitative comparison of state-of-the-art
    techniques. The paper also explores the practical application of local feature
    matching in diverse domains such as Structure from Motion, Remote Sensing Image
    Registration, and Medical Image Registration, underscoring its versatility and
    significance across various fields. Ultimately, we endeavor to outline the current
    challenges faced in this domain and furnish future research directions, thereby
    serving as a reference for researchers involved in local feature matching and
    its interconnected domains.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本地特征匹配在计算机视觉领域具有广泛的应用，包括图像检索、三维重建和对象识别等领域。然而，由于视角和光照变化等因素，提升匹配的准确性和鲁棒性仍然面临挑战。近年来，深度学习模型的引入激发了对本地特征匹配技术的广泛探索。该研究旨在提供本地特征匹配方法的全面概述。这些方法根据是否使用检测器分为两个主要类别。基于检测器的类别包括Detect-then-Describe、Joint
    Detection and Description、Describe-then-Detect以及基于图的方法。相反，基于检测器自由的方法包括CNN Based、Transformer
    Based和Patch Based方法。我们的研究不仅限于方法论分析，还结合了对常见数据集和指标的评估，以便对最先进技术进行定量比较。本文还探讨了本地特征匹配在运动结构、遥感图像配准和医学图像配准等不同领域的实际应用，突显了其在各个领域的多样性和重要性。最终，我们力图概述该领域当前面临的挑战，并提供未来研究方向，为从事本地特征匹配及其相关领域的研究人员提供参考。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'Local Feature Matching, Image Matching, Deep Learning, Survey.^†^†journal:
    Information Fusion^(myfootnote)^(myfootnote)footnotetext: Rongtao Xu is the corresponding
    author.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本地特征匹配，图像匹配，深度学习，综述。^†^†期刊：信息融合^(myfootnote)^(myfootnote)脚注：Rongtao Xu 为通讯作者。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In the field of image processing, the core objective of local feature matching
    tasks is to establish precise feature correspondences between different images.
    This encompasses various types of image features, such as keypoints, feature regions,
    straight lines, and curves, among others. Establishing correspondences between
    similar features in different images serves as the foundation for many computer
    vision tasks, including image fusion [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)], visual localization [[6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)], Structure from Motion (SfM) [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)], Simultaneous Localization
    and Mapping (SLAM) [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)], optical
    flow estimation [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)], image
    retrieval [[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)], and more.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像处理领域，本地特征匹配任务的核心目标是建立不同图像之间的精确特征对应关系。这包括各种类型的图像特征，如关键点、特征区域、直线和曲线等。建立不同图像中相似特征之间的对应关系是许多计算机视觉任务的基础，包括图像融合 [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)], 视觉定位 [[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)], 运动重建（SfM） [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)], 同时定位与地图构建（SLAM） [[14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)], 光流估计 [[17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19)], 图像检索 [[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]，等等。
- en: 'Owing to influences such as scale transformations, viewpoint diversity, shifts
    in illumination, pattern recurrences, and texture variations, the depiction of
    an identical physical space within distinct images may exhibit substantial divergence.
    For instance, Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Local Feature Matching
    Using Deep Learning: A Survey") provides a visual representation of the performance
    of several popular deep learning models engaged in local image matching tasks.
    Nevertheless, guaranteeing the establishment of precise correspondences between
    distinct images mandates surmounting manifold perplexities and challenges, engendered
    by the aforementioned factors. Consequently, the quest for accuracy and dependability
    in local feature matching continues to be a formidable problem beset with intricacies.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '由于尺度变换、视点多样性、光照变化、模式重复和纹理变化等因素的影响，相同物理空间在不同图像中的表现可能会有显著的差异。例如，图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Local Feature Matching Using Deep Learning: A Survey")
    提供了几个流行深度学习模型在本地图像匹配任务中的性能视觉表示。然而，确保在不同图像之间建立精确的对应关系需要克服诸多复杂性和挑战，由上述因素造成。因此，在本地特征匹配中寻求准确性和可靠性仍然是一个复杂的问题。'
- en: '![Refer to caption](img/6a0f0a3e75ec0912e72eb515fe694cf0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6a0f0a3e75ec0912e72eb515fe694cf0.png)'
- en: 'Figure 1: Matching results for outdoor images. It can be observed that for
    images with significant variations in viewpoint and lighting conditions, the matching
    task encounters considerable challenges.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：户外图像的匹配结果。可以观察到，对于视点和光照条件有显著变化的图像，匹配任务遇到相当大的挑战。
- en: 'In conventional image matching pipelines, the process can be decomposed into
    four fundamental steps: feature detection, feature description, feature matching,
    and geometric transformation estimation. Prior to the advent of deep learning [[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)], many celebrated algorithms were tailored
    to focus primarily on one or several stages within this pipeline. Various techniques
    were committed to the process of feature detection [[26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)], while others were honed on locally executing
    the task of feature description [[30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)].
    Additionally, certain algorithms have been devised to cater to both feature detection
    and description [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38)]. For the purpose of feature matching, conventional
    approaches typically relied on either minimizing or maximizing specific well-established
    metrics, such as the Sum of Squared Differences or correlation. During the stage
    of geometric transformation estimation, algorithms were generally employed on
    the basis of techniques akin to RANSAC [[39](#bib.bib39)] to estimate the underlying
    epipolar geometry or homographies. Both traditional handcrafted methods and learning-centric
    approaches that were constructed upon low-level image features like gradients
    and sequences of grayscale. Despite being theoretically resilient to certain forms
    of transformations, these techniques were inherently restricted by the inherent
    prior knowledge imposed by researchers on their tasks.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的图像匹配流程中，过程可以分解为四个基本步骤：特征检测、特征描述、特征匹配和几何变换估计。在深度学习出现之前[[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)]，许多著名的算法主要专注于该流程中的一个或多个阶段。各种技术致力于特征检测的过程[[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)]，而其他技术则专注于局部执行特征描述任务[[30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32)]。此外，还设计了一些算法以同时适应特征检测和描述[[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]。在特征匹配方面，传统方法通常依赖于最小化或最大化特定的成熟度量标准，如平方和差或相关性。在几何变换估计阶段，算法通常基于类似于RANSAC[[39](#bib.bib39)]的技术来估计基础的极几何或单应性。这些传统手工制作的方法和以学习为中心的方法是基于低级图像特征，如梯度和灰度序列。尽管在理论上对某些形式的变换具有一定的抗性，这些技术在本质上仍受限于研究人员对其任务施加的固有先验知识。
- en: 'In recent years, substantial advancements have been realized in addressing
    the challenges associated with local feature matching [[40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42)], especially those posed by scale variations, shifts in viewpoint,
    and other forms of diversities. The extant methods of image matching can be sorted
    into two major categories: Detector-based and Detector-free methods. Detector-based
    methods hinge on the detection and description of sparsely distributed keypoints
    in order to establish matches between images. The efficacy of these methods is
    largely dependent on the performance of keypoint detectors and feature descriptors,
    given their significant role in the process. Contrastingly, Detector-free methods
    sidestep the necessity for separate keypoint detection and feature description
    stages by tapping into the rich contextual information prevalent within the images.
    These methods enable end-to-end image matching, thereby offering a distinct mechanism
    to tackle the task.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，在解决局部特征匹配相关挑战[[40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42)]，特别是那些由尺度变化、视点偏移和其他形式的多样性带来的挑战方面，取得了显著的进展。现有的图像匹配方法可以分为两大类：基于检测器的方法和无检测器的方法。基于检测器的方法依赖于稀疏分布关键点的检测和描述，以在图像之间建立匹配。这些方法的有效性在很大程度上取决于关键点检测器和特征描述符的性能，因为它们在过程中扮演了重要角色。相对而言，无检测器的方法通过利用图像中丰富的上下文信息，绕过了独立的关键点检测和特征描述阶段。这些方法实现了端到端的图像匹配，从而提供了一种不同的机制来处理这一任务。
- en: Image matching plays a pivotal role in the domain of image registration, where
    it contributes significantly by enabling the precise fitting of transformation
    functions through a reliable set of feature matches. This functionality positions
    image matching as a crucial area of study within the broader context of image
    fusion [[43](#bib.bib43)]. To coherently encapsulate the evolution of the local
    feature matching domain and stimulate innovative research avenues, this paper
    presents an exhaustive review and thorough analysis of the latest progress in
    local feature matching, particularly emphasizing the use of deep learning algorithms.
    In addition, we re-examine pertinent datasets and evaluation criteria, and conduct
    detailed comparative analyses of key methodologies. Our investigation addresses
    both the gap and potential bridging between traditional manual methods and modern
    deep learning technologies. We emphasize the ongoing relevance and collaboration
    between these two approaches by analyzing the latest developments in traditional
    manual methods alongside deep learning techniques. Further, we address the emerging
    focus on multi-modal images. This includes a detailed overview of methods specifically
    tailored for multi-modal image analysis. Our survey also identifies and discusses
    the gaps and future needs in existing datasets for evaluating local feature matching
    methods, highlighting the importance of adapting to diverse and dynamic scenarios.
    In keeping with current trends, we examine the role of large foundation models
    in feature matching. These models represent a significant shift from traditional
    semantic segmentation models [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48)], offering superior generalization capabilities
    for a wide array of scenes and objects.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图像匹配在图像配准领域中发挥着关键作用，通过可靠的特征匹配集实现精确的变换函数拟合，显著贡献于该领域。这使得图像匹配成为图像融合广泛背景下的一个重要研究领域[[43](#bib.bib43)]。为了系统地概括局部特征匹配领域的发展并激发创新研究方向，本文对局部特征匹配的最新进展进行了详尽的回顾和深入分析，特别强调了深度学习算法的应用。此外，我们重新审视了相关数据集和评估标准，并对关键方法进行了详细的比较分析。我们的研究涵盖了传统手工方法与现代深度学习技术之间的差距和潜在桥接。通过分析传统手工方法和深度学习技术的最新发展，我们强调了这两种方法的持续相关性和协作关系。进一步地，我们关注了多模态图像的新兴焦点，包括针对多模态图像分析的专门方法的详细概述。我们的调查还识别并讨论了现有数据集在评估局部特征匹配方法中的不足和未来需求，突出了适应多样化和动态场景的重要性。符合当前趋势，我们审视了大型基础模型在特征匹配中的作用。这些模型代表了相较于传统语义分割模型[[44](#bib.bib44)、[45](#bib.bib45)、[46](#bib.bib46)、[47](#bib.bib47)、[48](#bib.bib48)]的重大转变，为各种场景和物体提供了优越的泛化能力。
- en: 'In summary, some of the key contributions of this survey can be summarized
    as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这项调查的一些关键贡献可以概括如下：
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'This survey extensively covers the literature on contemporary local feature
    matching problems and provides a detailed overview of various local feature matching
    algorithms proposed since 2018\. Following the prevalent image matching pipeline,
    we primarily categorize these methods into two major classes: Detector-based and
    Detector-free, and provide a comprehensive review of matching algorithms employing
    deep learning.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查广泛涵盖了当代局部特征匹配问题的文献，并详细概述了自2018年以来提出的各种局部特征匹配算法。按照普遍的图像匹配流程，我们主要将这些方法分为两大类：基于检测器的方法和无检测器的方法，并对采用深度学习的匹配算法进行了全面回顾。
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: We scrutinize the deployment of these methodologies in a myriad of real-world
    scenarios, encompassing SfM, Remote Sensing Image Registration, and Medical Image
    Registration. This investigation highlights the versatility and extensive applicability
    inherent in local feature matching techniques.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们详细审查了这些方法在众多现实世界场景中的应用，包括 SfM、遥感图像配准和医学图像配准。这项研究突出了局部特征匹配技术固有的多功能性和广泛适用性。
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: We start from relevant computer vision tasks, review the major datasets involved
    in local feature matching, and classify them according to different tasks to delve
    into specific research requirements within each domain.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从相关计算机视觉任务出发，审查了局部特征匹配中涉及的主要数据集，并根据不同任务对其进行分类，以深入探讨每个领域中的具体研究需求。
- en: '4.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: We analyze various metrics used for performance evaluation and conduct a quantitative
    comparison of key local feature matching methods.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了用于性能评估的各种指标，并对关键的局部特征匹配方法进行了定量比较。
- en: '5.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: We present a series of challenges and future research directions, offering valuable
    guidance for further advancements in this field.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一系列挑战和未来研究方向，为该领域的进一步发展提供了宝贵的指导。
- en: 'It is important to note that the initial surveys [[49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51)] primarily focused on manual methods, hence they do not provide
    sufficient reference points for research centered around deep learning. Although
    recent surveys [[52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)] have incorporated
    trainable methods, they have failed to timely summarize the plethora of literature
    that has emerged in the past five years. Furthermore, many are limited to specific
    aspects of image matching within the field, such as some articles introducing
    only the feature detection and description methods of local features, but not
    including matching [[52](#bib.bib52)], some particularly focusing on matching
    of cultural heritage images [[55](#bib.bib55)], and others solely concentrating
    on medical image registration [[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58)],
    remote sensing image registration [[59](#bib.bib59), [60](#bib.bib60)], and so
    on. In this survey, our goal is to provide the most recent and comprehensive overview
    by assessing the existing methods of image matching, particularly the state-of-the-art
    learning-based approaches. Importantly, we not only discuss the existing methods
    that serve natural image applications, but also the wide application of feature
    matching in SfM, remote sensing images, and medical images. We illustrate the
    close connection of this research with the field of information fusion through
    a detailed discussion on the matching of multimodal images. Additionally, we have
    conducted a thorough examination and analysis of the recent mainstream methods,
    discussions that are evidently missing in the existing literature. Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Local Feature Matching Using Deep Learning: A Survey")
    showcases a representative timeline of local feature matching methodologies, which
    provides insights on the evolution of these methods and their pivotal contributions
    towards spearheading advancements in the field.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，最初的调查[[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)]主要集中在手动方法上，因此它们未能为以深度学习为中心的研究提供足够的参考点。尽管近期的调查[[52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54)]已经纳入了可训练的方法，但它们未能及时总结过去五年中出现的大量文献。此外，许多调查仅限于图像匹配领域的特定方面，例如一些文章仅介绍了局部特征的特征检测和描述方法，却没有包括匹配[[52](#bib.bib52)]，一些文章特别关注文化遗产图像的匹配[[55](#bib.bib55)]，还有一些则仅集中在医学图像配准[[56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58)]、遥感图像配准[[59](#bib.bib59), [60](#bib.bib60)]等方面。在这项调查中，我们的目标是通过评估现有的图像匹配方法，特别是最先进的基于学习的方法，提供最新和最全面的概述。重要的是，我们不仅讨论了服务于自然图像应用的现有方法，还探讨了特征匹配在SfM、遥感图像和医学图像中的广泛应用。通过详细讨论多模态图像的匹配，我们阐明了这一研究与信息融合领域的密切联系。此外，我们还对最近的主流方法进行了彻底的审查和分析，这些讨论在现有文献中明显缺失。图 [2](#S1.F2
    "图 2 ‣ 1 引言 ‣ 使用深度学习的局部特征匹配：综述")展示了局部特征匹配方法的代表性时间线，提供了这些方法的演变和它们在推动该领域进步中的重要贡献的见解。
- en: '![Refer to caption](img/dd021532117c3bd2ea0d1f866ac7708f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd021532117c3bd2ea0d1f866ac7708f.png)'
- en: 'Figure 2: Representative local feature matching methods. Blue and gray represent
    Detector-based Models, where gray represents the Graph Based method. The yellow
    and green blocks represent the CNN Based and Transformer Based methods in Detector-free
    Models, respectively.In 2018, Superpoint [[61](#bib.bib61)] pioneered the computation
    of keypoints and descriptors within a single network. Subsequently, numerous works
    such as D2Net [[62](#bib.bib62)], R2D2 [[63](#bib.bib63)], and others attempted
    to integrate keypoint detection and description for matching purposes. Concurrently,
    the NCNet [[64](#bib.bib64)] method introduced four-dimensional cost volumes into
    local feature matching, initiating a trend in utilizing correlation-based or cost
    volume-based convolutional neural networks for Detector-free matching research.
    Building upon this trend, methods like Sparse-NCNet [[65](#bib.bib65)], DRC-Net [[66](#bib.bib66)],
    GLU-Net [[67](#bib.bib67)], and PDC-Net [[68](#bib.bib68)] emerged. In 2020, SuperGlue [[69](#bib.bib69)]
    framed the task as a graph matching problem involving two sets of features. Following
    this, SGMNet [[70](#bib.bib70)] and ClusterGNN [[71](#bib.bib71)] focused on improving
    the graph matching process by addressing the complexity of matching. In 2021,
    approaches such as LoFTR [[72](#bib.bib72)] and Aspanformer [[73](#bib.bib73)]
    successfully incorporated Transformer or Attention mechanisms into the Detector-free
    matching process. They achieved this by employing interleaved self and cross-attention
    modules, significantly expanding the receptive field and further advancing deep
    learning-based matching techniques.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：代表性的局部特征匹配方法。蓝色和灰色代表基于检测器的模型，其中灰色代表图形基方法。黄色和绿色块分别代表基于 CNN 和 Transformer
    的检测器无关模型。2018 年，Superpoint [[61](#bib.bib61)] 首创了在单个网络中计算关键点和描述符。随后，许多工作如 D2Net
    [[62](#bib.bib62)]、R2D2 [[63](#bib.bib63)] 等尝试整合关键点检测和描述以实现匹配。同时，NCNet [[64](#bib.bib64)]
    方法将四维成本体积引入局部特征匹配，开启了利用基于相关性或成本体积的卷积神经网络进行检测器无关匹配研究的趋势。在这一趋势的基础上，出现了 Sparse-NCNet
    [[65](#bib.bib65)]、DRC-Net [[66](#bib.bib66)]、GLU-Net [[67](#bib.bib67)] 和 PDC-Net
    [[68](#bib.bib68)] 等方法。2020 年，SuperGlue [[69](#bib.bib69)] 将任务框架设定为涉及两组特征的图匹配问题。随后，SGMNet
    [[70](#bib.bib70)] 和 ClusterGNN [[71](#bib.bib71)] 通过解决匹配复杂性来改善图匹配过程。2021 年，LoFTR
    [[72](#bib.bib72)] 和 Aspanformer [[73](#bib.bib73)] 等方法成功将 Transformer 或注意力机制纳入检测器无关匹配过程。它们通过使用交替的自注意力和交叉注意力模块显著扩展了感受野，进一步推进了基于深度学习的匹配技术。
- en: 2 Detector-based Models
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基于检测器的模型
- en: 'Detector-based methodologies have been the prevailing approach for local feature
    matching for a considerable duration. Numerous well-established handcrafted works,
    including SIFT [[33](#bib.bib33)] and ORB [[35](#bib.bib35)], have been broadly
    adopted for varied tasks within the field of 3D computer vision [[74](#bib.bib74),
    [75](#bib.bib75)]. These traditional Detector-based methodologies typically comprise
    three primary stages: feature detection, feature description, and feature matching.
    Initially, a set of sparse key points is extracted from the images. Subsequently,
    in the feature description stage, these key points are characterized using high-dimensional
    vectors, often designed to encapsulate the specific structure and information
    of the region surrounding these points. Lastly, during the feature matching stage,
    correspondences at a pixel level are established through mechanisms like nearest-neighbor
    searches or more complex matching algorithms. Notable among these are Gms (Grid-based
    Motion Statistics) by Bian et al.[[76](#bib.bib76)] and OANET (Order-Aware Network)
    by Zhang et al.[[77](#bib.bib77)]. GMS enhances feature correspondence quality
    using grid-based motion statistics, simplifying and accelerating matching, while
    OANET innovatively optimizes two-view matching by integrating spatial contexts
    for precise correspondence and geometry estimation. This is typically done by
    comparing the high-dimensional vectors of keypoints between different images and
    identifying matches based on the level of similarity – often defined by a distance
    function in the vector space.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检测器的方法论已经在本地特征匹配中占据了主导地位相当长的时间。许多成熟的手工设计方法，包括SIFT [[33](#bib.bib33)]和ORB [[35](#bib.bib35)]，已广泛应用于3D计算机视觉领域的各种任务 [[74](#bib.bib74),
    [75](#bib.bib75)]。这些传统的基于检测器的方法通常包括三个主要阶段：特征检测、特征描述和特征匹配。最初，从图像中提取一组稀疏的关键点。随后，在特征描述阶段，这些关键点使用高维向量进行表征，这些向量通常旨在封装这些点周围区域的特定结构和信息。最后，在特征匹配阶段，通过最近邻搜索或更复杂的匹配算法等机制，在像素级别建立对应关系。其中，Bian等人提出的Gms（Grid-based
    Motion Statistics）[[76](#bib.bib76)]和Zhang等人提出的OANET（Order-Aware Network）[[77](#bib.bib77)]尤为值得注意。GMS通过网格化运动统计来提升特征对应质量，简化并加速匹配，而OANET则通过整合空间上下文来创新性地优化两视图匹配，以实现精确的对应关系和几何估计。这通常通过比较不同图像之间关键点的高维向量，并根据相似度水平（通常由向量空间中的距离函数定义）来识别匹配。
- en: 'However, in the era of deep learning, the rise of data-driven methods has made
    approaches like LIFT [[78](#bib.bib78)] popular. These methods leverage CNNs to
    extract more robust and discriminative keypoint descriptors, resulting in significant
    progress in handling large viewpoint changes and local feature illumination variations.
    Currently, Detector-based methods can be categorized into four main classes: 1\.
    Detect-then-Describe methods; 2\. Joint Detection and Description methods; 3\.
    Describe-then-Detect methods; 4\. Graph Based methods. Additionally, we further
    subdivide Detect-then-Describe methods based on the type of supervised learning
    into Fully-Supervised methods, Weakly Supervised methods, and Other forms of Supervision
    methods. This classification is visually depicted in Figure [3](#S2.F3 "Figure
    3 ‣ 2 Detector-based Models ‣ Local Feature Matching Using Deep Learning: A Survey").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，在深度学习时代，数据驱动方法的兴起使得像LIFT [[78](#bib.bib78)]这样的方式变得流行。这些方法利用CNNs提取更强健且具有辨别力的关键点描述符，从而在处理大视角变化和局部特征光照变化方面取得了显著进展。目前，基于检测器的方法可以分为四大类：1\.
    先检测后描述方法；2\. 联合检测与描述方法；3\. 先描述后检测方法；4\. 基于图的方法。此外，我们根据监督学习类型进一步将先检测后描述的方法细分为完全监督方法、弱监督方法和其他形式的监督方法。这个分类在图 [3](#S2.F3
    "Figure 3 ‣ 2 Detector-based Models ‣ Local Feature Matching Using Deep Learning:
    A Survey")中以视觉方式呈现。'
- en: '![Refer to caption](img/4ea36f526ec2743d8e85515a0f182686.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4ea36f526ec2743d8e85515a0f182686.png)'
- en: 'Figure 3: Overview of the Local Feature Matching Models and taxonomy of the
    most relevant approaches.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：本地特征匹配模型的概述及最相关方法的分类。
- en: 2.1 Detect-then-Describe
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 先检测后描述
- en: 'In feature matching methodologies, the adoption of sparse-to-sparse feature
    matching is rather commonplace. These methods adhere to a ’detect-then-describe’
    paradigm, where the primary step involves the detection of keypoint locations.
    The detector subsequently extracts feature descriptors from patches centered on
    each detected keypoint. These descriptors are then relayed to the feature description
    stage. This procedure is typically trained utilizing metric learning methods,
    which aim to learn a distance function where similar points are close and dissimilar
    points are distant in the feature space. To enhance efficiency, feature detectors
    often focus on small image regions [[78](#bib.bib78)], generally emphasizing low-level
    structures such as corners [[26](#bib.bib26)] or blobs [[33](#bib.bib33)]. The
    descriptors, on the other hand, aim to capture more nuanced, higher-level information
    within larger patches encompassing the keypoints. Providing verbosity and distinctive
    details, these descriptors serve as the defining features for matching purposes.
    Figure [4](#S2.F4 "Figure 4 ‣ 2.1 Detect-then-Describe ‣ 2 Detector-based Models
    ‣ Local Feature Matching Using Deep Learning: A Survey")(a) illustrates the common
    structure of the Detect-then-Describe pipeline.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '在特征匹配方法中，稀疏到稀疏的特征匹配被广泛采用。这些方法遵循‘检测-再描述’的范式，其中主要步骤包括检测关键点的位置。然后，检测器从以每个检测到的关键点为中心的区域中提取特征描述符。这些描述符随后被传送到特征描述阶段。该过程通常使用度量学习方法进行训练，旨在学习一种距离函数，使得相似的点在特征空间中距离较近，而不相似的点距离较远。为了提高效率，特征检测器通常关注于较小的图像区域[[78](#bib.bib78)]，通常强调低级结构，如角点[[26](#bib.bib26)]或斑点[[33](#bib.bib33)]。而特征描述符则旨在捕捉更细致、更高级的信息，涵盖关键点所在的较大区域。这些描述符提供详细且独特的细节，作为匹配的定义特征。图[4](#S2.F4
    "Figure 4 ‣ 2.1 Detect-then-Describe ‣ 2 Detector-based Models ‣ Local Feature
    Matching Using Deep Learning: A Survey")(a) 说明了‘检测-再描述’流程的常见结构。'
- en: '![Refer to caption](img/0d7b1415dc57118b2869f6ffc3e73197.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d7b1415dc57118b2869f6ffc3e73197.png)'
- en: 'Figure 4: The comparison of various prominent Detector-based pipelines for
    trainable local feature matching is presented. Here, the categorization is based
    on the relationship between the detection and description steps: (a) Detect-then-Describe
    framework, (b) Joint Detection and Description framework, and (c) Describe-then-Detect
    framework.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 展示了各种显著的检测器基础管道在可训练本地特征匹配中的比较。这里的分类基于检测和描述步骤之间的关系：(a) 检测-再描述框架，(b) 联合检测与描述框架，以及
    (c) 描述-再检测框架。'
- en: 2.1.1 Fully-Supervised
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 完全监督
- en: The field of local feature matching has undergone a remarkable transformation,
    primarily driven by the advent of annotated patch datasets [[79](#bib.bib79)]
    and the integration of deep learning technologies. This transformation marks a
    departure from traditional handcrafted methods to more data-driven methodologies,
    reshaping the landscape of feature matching. This section aims to trace the historical
    development of these changes, emphasizing the sequential progression and the interconnected
    nature of various fully supervised methods. At the forefront of this evolution
    are CNNs, which have been pivotal in revolutionizing the process of descriptor
    learning. By enabling end-to-end learning directly from raw local patches, CNNs
    have facilitated the construction of a hierarchy of local features. This capability
    has allowed CNNs to capture complex patterns in data, leading to the creation
    of more specialized and distinct descriptors that significantly enhance the matching
    process. This revolutionary shift was largely influenced by innovative models
    like L2Net [[80](#bib.bib80)], which pioneered a progressive sampling strategy.
    L2Net’s approach accentuated the relative distances between descriptors while
    applying additional supervision to intermediate feature maps. This strategy significantly
    contributed to the development of robust descriptors, setting a new standard in
    descriptor learning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本地特征匹配领域经历了显著的变革，这一变革主要受到标注补丁数据集[[79](#bib.bib79)]和深度学习技术的推动。这一变革标志着从传统手工方法向数据驱动的方法的转变，重塑了特征匹配的格局。本节旨在追溯这些变化的历史发展，强调各种全监督方法的顺序进展和相互关联的性质。在这一进化的前沿是CNN，这在革命性地改变描述符学习的过程中发挥了关键作用。通过直接从原始本地补丁进行端到端学习，CNN促进了本地特征层次的构建。这种能力使得CNN能够捕捉数据中的复杂模式，从而创建出更加专业和独特的描述符，显著提升了匹配过程。这一革命性转变在很大程度上受到了像L2Net[[80](#bib.bib80)]这样创新模型的影响，L2Net开创了一种渐进采样策略。L2Net的方法强调了描述符之间的相对距离，同时对中间特征图施加了额外的监督。这一策略对鲁棒描述符的发展做出了重要贡献，为描述符学习设立了新的标准。
- en: 'The shift towards these data-driven methodologies, underpinned by CNNs, has
    not only improved the accuracy and efficiency of local feature matching but has
    also opened new avenues for research and innovation in this area. As we explore
    the chronological advancements in this field, we observe a clear trajectory of
    growth and refinement, moving from the conventional to the contemporary, each
    method building upon the successes of its predecessors while introducing novel
    concepts and techniques. OriNet [[81](#bib.bib81)] present a method using CNNs
    to assign a canonical orientation to feature points in an image, enhancing feature
    point matching. They introduce a Siamese network [[82](#bib.bib82)] training approach
    that eliminates the need for predefined orientations and propose a novel GHH activation
    function, showing significant performance improvements in feature descriptors
    across multiple datasets. Building on the architectural principles of L2Net, HardNet [[83](#bib.bib83)]
    streamlined the learning process by focusing on metric learning and eliminating
    the need for auxiliary loss terms, setting a precedent for subsequent models to
    simplify learning objectives. DOAP [[84](#bib.bib84)] shifted the focus to a learning-to-rank
    formulation, optimizing local feature descriptors for nearest-neighbor matching,
    a methodology that found success in specific matching scenarios and influenced
    later models to consider ranking-based approaches. The KSP [[85](#bib.bib85)]
    method is notable for its introduction of a subspace pooling methodology, leveraging
    CNNs to learn invariant and discriminative descriptors. DeepBit [[86](#bib.bib86)]
    offers an unsupervised deep learning framework to learn compact binary descriptors.
    It encodes crucial properties like rotation, translation, and scale invariance
    of local descriptors into binary representations. Bingan [[87](#bib.bib87)] proposes
    a method to learn compact binary image descriptors using regularized Generative
    Adversarial Networks (GANs). GLAD [[88](#bib.bib88)] addresses the Person Re-Identification
    task by considering both local and global cues from human bodies. A four-stream
    CNN framework is implemented to generate discriminative and robust descriptors.
    Geodesc [[89](#bib.bib89)] advances descriptor computation by integrating geometric
    constraints from SfM algorithms. This approach emphasizes two aspects: first,
    the construction of training data using geometric information to measure sample
    hardness, where hardness is defined by the variability between pixel blocks of
    the same 3D point and uniformity for different points. Second, a geometric similarity
    loss function is devised, promoting closeness among pixel blocks corresponding
    to the same 3D point. These innovations enable Geodesc to significantly enhance
    descriptor effectiveness in 3D reconstruction tasks. For GIFT [[90](#bib.bib90)]
    and COLD [[91](#bib.bib91)], the former underscores the importance of incorporating
    underlying structural information from group features to construct potent descriptors.
    Through the utilization of group convolutions, GIFT generates dense descriptors
    that exhibit both distinctiveness and invariance to transformation groups. In
    contrast, COLD introduces a novel approach through a multi-level feature distillation
    network architecture. This architecture leverages intermediate layers of ImageNet
    pre-trained convolutional neural networks to encapsulate hierarchical features,
    ultimately extracting highly compact and robust local descriptors.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 向这些以数据驱动的方法转变，基于 CNN 的支持，不仅提高了局部特征匹配的准确性和效率，还为这一领域的研究和创新开辟了新的途径。随着我们深入探讨这一领域的时间进展，我们可以观察到从传统到现代的清晰增长和改进轨迹，每种方法都在前人的成功基础上进行构建，同时引入新颖的概念和技术。OriNet
    [[81](#bib.bib81)] 提出了一种利用 CNN 为图像中的特征点分配标准方向的方法，从而增强了特征点匹配。他们介绍了一种消除预定义方向需求的
    Siamese 网络 [[82](#bib.bib82)] 训练方法，并提出了一种新颖的 GHH 激活函数，在多个数据集上的特征描述符中显示出显著的性能提升。基于
    L2Net 的架构原则，HardNet [[83](#bib.bib83)] 通过关注度量学习并消除辅助损失项，简化了学习过程，为后续模型简化学习目标树立了先例。DOAP
    [[84](#bib.bib84)] 将焦点转向学习排序的形式，优化了用于最近邻匹配的局部特征描述符，这一方法在特定匹配场景中取得了成功，并影响了后续模型考虑基于排名的方法。KSP
    [[85](#bib.bib85)] 方法因引入子空间池化方法而值得关注，利用 CNN 学习不变且具有区分性的描述符。DeepBit [[86](#bib.bib86)]
    提供了一种无监督深度学习框架来学习紧凑的二进制描述符。它将局部描述符的旋转、平移和尺度不变性等重要属性编码为二进制表示。Bingan [[87](#bib.bib87)]
    提出了一种使用正则化生成对抗网络（GAN）学习紧凑二进制图像描述符的方法。GLAD [[88](#bib.bib88)] 通过考虑人体的局部和全局线索来解决人物重识别任务。实现了一个四流
    CNN 框架以生成具有区分性和鲁棒性的描述符。Geodesc [[89](#bib.bib89)] 通过结合 SfM 算法中的几何约束来推进描述符计算。这种方法强调两个方面：首先，利用几何信息构建训练数据来测量样本的难度，难度由相同
    3D 点的像素块之间的变异性以及不同点的均匀性定义。其次，制定了几何相似性损失函数，促进了与同一 3D 点对应的像素块之间的接近性。这些创新使 Geodesc
    在 3D 重建任务中显著提高了描述符的有效性。对于 GIFT [[90](#bib.bib90)] 和 COLD [[91](#bib.bib91)]，前者强调了从组特征中纳入潜在结构信息以构建有效描述符的重要性。通过利用组卷积，GIFT
    生成了具有明显特性和对变换组具有不变性的密集描述符。相比之下，COLD 通过多级特征蒸馏网络架构引入了一种新颖的方法。该架构利用了 ImageNet 预训练卷积神经网络的中间层来封装层次特征，最终提取出高度紧凑且鲁棒的局部描述符。
- en: Advancing the narrative, our exploration extends to recent strides in fully-supervised
    methodologies, constituting a noteworthy augmentation of the repertoire of local
    feature matching capabilities. These pioneering approaches, building upon the
    foundational frameworks expounded earlier, synergistically elevate and finesse
    the methodologies that underpin the field. Continuing the trend of enhancing descriptor
    robustness, SOSNet [[92](#bib.bib92)] extends HardNet by introducing a second-order
    similarity regularization term for descriptor learning. This enhancement involves
    integrating second-order similarity constraints into the training process, thereby
    augmenting the performance of learning robust descriptors. The term ”second-order
    similarity” denotes a metric that evaluates the consistency of relative distances
    among descriptor pairs in a training batch. It measures the similarity between
    a descriptor pair not only directly but also by considering their relative distances
    to other descriptor pairs within the same batch. Ebel et al. [[93](#bib.bib93)]
    proposes a local feature descriptor based on a log-polar sampling scheme to achieve
    scale invariance. This unique approach allows for keypoint matching across different
    scales and exhibits less sensitivity to occlusion and background motion. Thus,
    it effectively utilizes a larger image region to improve performance. To design
    a better loss function, HyNet [[94](#bib.bib94)] introduces a mixed similarity
    measure for triplet margin loss and implements a regularization term to constrain
    descriptor norms, thus establishing a balanced and effective learning framework.
    CNDesc [[95](#bib.bib95)] also investigates L2 normalization, presenting an innovative
    dense local descriptor learning approach. It uses a special cross-normalization
    technique instead of L2 normalization, introducing a new way of normalizing the
    feature vectors. Key.Net [[96](#bib.bib96)] proposes a keypoint detector that
    combines handcrafted and learned CNN features, and uses scale space representation
    in the network to extract keypoints at different levels. To address the non-differentiability
    issue in keypoint detection methods, ALIKE [[97](#bib.bib97)] offers a differentiable
    keypoint detection (DKD) module based on the score map. In contrast to methods
    relying on non-maximum suppression (NMS), DKD can backpropagate gradients and
    produce keypoints at subpixel levels. This enables the direct optimization of
    keypoint locations. ZippyPoint [[98](#bib.bib98)] is designed based on KP2D [[99](#bib.bib99)],
    introduces an entire set of accelerated extraction and matching techniques. This
    method suggests the use of a binary descriptor normalization layer, thereby enabling
    the generation of unique, length-invariant binary descriptors.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 继续叙述，我们的探索扩展到最近在完全监督方法上的进展，这构成了本地特征匹配能力的显著增强。这些开创性的方法在早期阐述的基础框架上构建，协同提升并精细化了支撑该领域的方法论。延续增强描述符鲁棒性的趋势，SOSNet [[92](#bib.bib92)]
    通过引入二阶相似度正则化项来扩展 HardNet，以进行描述符学习。这一改进涉及将二阶相似度约束整合到训练过程中，从而提升学习鲁棒描述符的性能。术语“二阶相似度”表示一种度量，评估训练批次中描述符对相对距离的一致性。它不仅直接测量描述符对之间的相似度，还通过考虑它们在同一批次中与其他描述符对的相对距离来进行测量。Ebel
    等人 [[93](#bib.bib93)] 提出了基于对数极坐标采样方案的本地特征描述符，以实现尺度不变性。这一独特方法允许在不同尺度下进行关键点匹配，并对遮挡和背景运动的敏感性较低。因此，它有效地利用了更大的图像区域来提高性能。为了设计更好的损失函数，HyNet [[94](#bib.bib94)]
    引入了一种用于三元组边距损失的混合相似性度量，并实现了一个正则化项来约束描述符的范数，从而建立了一个平衡且有效的学习框架。CNDesc [[95](#bib.bib95)]
    还研究了 L2 归一化，提出了一种创新的稠密本地描述符学习方法。它使用一种特殊的交叉归一化技术，而不是 L2 归一化，引入了一种新的特征向量归一化方法。Key.Net [[96](#bib.bib96)]
    提出了一个结合手工设计和学习的 CNN 特征的关键点检测器，并在网络中使用尺度空间表示来提取不同层次的关键点。为了解决关键点检测方法中的不可微分问题，ALIKE [[97](#bib.bib97)]
    提供了基于评分图的可微分关键点检测 (DKD) 模块。与依赖非极大值抑制 (NMS) 的方法不同，DKD 可以反向传播梯度并生成亚像素级别的关键点。这使得关键点位置可以直接优化。ZippyPoint [[98](#bib.bib98)]
    基于 KP2D [[99](#bib.bib99)] 设计，介绍了一整套加速提取和匹配技术。该方法建议使用二进制描述符归一化层，从而实现生成独特的、长度不变的二进制描述符。
- en: Implementing contextual information into feature descriptors has been a rising
    trend in the advancement of local feature matching methods. ContextDesc [[100](#bib.bib100)]
    introduces context awareness to improve off-the-shelf local feature descriptors.
    It encodes both geometric and visual contexts by using keypoint locations, raw
    local features, and high-level regional features as inputs. A novel aspect of
    its training process is the use of an N-pair loss, which is self-adaptive and
    requires no parameter tuning. This dynamic loss function can allow for a more
    efficient learning process. MTLDesc [[101](#bib.bib101)] offers a strategy to
    address the inherent locality issue confronted in the domains of convolutional
    neural networks. This is attained by introducing an adaptive global context enhancement
    module and multiple local context enhancement modules to inject non-local contextual
    information. By adding these non-local connections, it can efficiently learn high-level
    dependencies between distant features. Building upon MTLDesc, AWDesc [[102](#bib.bib102)]
    seeks to transfer knowledge from a larger, more complex model (teacher) to a smaller
    and simpler one (student). This approach leverages the knowledge learned by the
    teacher, while enabling significantly faster computations with the student, allowing
    the model to achieve an optimal balance between accuracy and speed. The focus
    on context-awareness in these methods emphasizes the importance of considering
    more global information when describing local features. Each method leverages
    this information in a slightly different way, leading to diverse but potentially
    complementary approaches for tackling the challenge of feature matching.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将上下文信息融入特征描述符已成为局部特征匹配方法发展的一个新趋势。ContextDesc [[100](#bib.bib100)] 引入了上下文感知，以改进现有的局部特征描述符。它通过使用关键点位置、原始局部特征和高层次区域特征作为输入，编码了几何和视觉上下文。其训练过程中的一个新颖之处是使用N-pair
    loss，这种自适应损失函数无需参数调整，能实现更高效的学习过程。MTLDesc [[101](#bib.bib101)] 提供了一种策略来解决卷积神经网络领域固有的局部性问题。这通过引入自适应全局上下文增强模块和多个局部上下文增强模块来注入非局部上下文信息。通过增加这些非局部连接，它可以有效地学习远程特征之间的高层次依赖关系。在MTLDesc的基础上，AWDesc [[102](#bib.bib102)]
    旨在将知识从一个更大、更复杂的模型（教师）转移到一个更小、更简单的模型（学生）。这种方法利用教师学到的知识，同时允许学生进行显著更快的计算，使模型在准确性和速度之间实现**最佳平衡**。这些方法对上下文感知的关注突出了在描述局部特征时考虑更多全局信息的重要性。每种方法以稍微不同的方式利用这些信息，从而导致多样化但潜在互补的特征匹配挑战应对方法。
- en: In light of the limitations inherent in traditional image feature descriptors
    (like gradients, grayscale, etc.), which struggle to handle the geometric and
    radiometric disparities across different modal image types [[103](#bib.bib103)],
    there is an emerging focus on frequency-domain-based feature descriptors. These
    descriptors exhibit improved proficiency in matching cross-modal images. For instance,
    RIFT [[104](#bib.bib104)] utilizes FAST  [[105](#bib.bib105)] for extracting repeatable
    feature points on the phase congruency (PC) map, subsequently constructing robust
    descriptors using frequency domain information to tackle the challenges in multimodal
    image feature matching. Building on RIFT, SRIFT  [[106](#bib.bib106)] further
    refines this approach by establishing a nonlinear diffusion scale (NDS) space,
    thus constructing a multiscale space that not only achieves scale and rotation
    invariance but also addresses the issue of slow inference speeds associated with
    RIFT. With the evolution of deep learning technologies, depth-based methods have
    demonstrated significant prowess in feature extraction. SemLA [[107](#bib.bib107)]
    uses semantic guidance in its registration and fusion processes. The feature matching
    is limited to the semantic sensing area, so as to provide the most accurate registration
    effect for image fusion tasks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到传统图像特征描述符（如梯度、灰度等）在处理不同模态图像类型的几何和辐射差异方面存在的局限性[[103](#bib.bib103)]，目前出现了对基于频率域的特征描述符的关注。这些描述符在跨模态图像匹配方面表现出更高的熟练度。例如，RIFT[[104](#bib.bib104)]利用FAST[[105](#bib.bib105)]在相位一致性（PC）图上提取可重复的特征点，随后使用频率域信息构建稳健的描述符，以应对多模态图像特征匹配中的挑战。在RIFT的基础上，SRIFT[[106](#bib.bib106)]进一步通过建立非线性扩散尺度（NDS）空间来改进这一方法，从而构建一个多尺度空间，不仅实现了尺度和旋转不变性，还解决了与RIFT相关的慢推断速度问题。随着深度学习技术的发展，基于深度的方法在特征提取方面展现了显著的能力。SemLA[[107](#bib.bib107)]在其配准和融合过程中使用语义引导。特征匹配仅限于语义感知区域，以提供图像融合任务的最准确配准效果。
- en: 2.1.2 Weakly Supervised and Others
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 弱监督及其他
- en: Weakly supervised learning presents opportunities for models to learn robust
    features without requiring densely annotated labels, offering a solution to one
    of the largest challenges in training deep learning models. Several weakly supervised
    local feature learning methods have emerged, leveraging easily obtainable geometric
    information from camera poses. AffNet [[108](#bib.bib108)] represents a key advancement
    in weakly supervised local feature learning, focusing on the learning of affine
    shape of local features. This method challenges the traditional emphasis on geometric
    repeatability, showing that it is insufficient for reliable feature matching and
    stressing the importance of descriptor-based learning. AffNet introduces a hard
    negative-constant loss function to improve the matchability and geometric accuracy
    of affine regions. This has proven effective in enhancing the performance of affine-covariant
    detectors, especially in wide baseline matching and image retrieval. The approach
    underscores the need to consider both descriptor matchability and repeatability
    for developing more effective local feature detectors. GLAMpoints [[109](#bib.bib109)]
    presents a semi-supervised keypoint detection method, creatively drawing insights
    from reinforcement learning loss formulations. Here, rewards are used to calculate
    the significance of detecting keypoints based on the quality of the final alignment.
    This method has been noted to significantly impact the matching and registration
    quality of the final images. CAPS [[110](#bib.bib110)] introduces a weakly supervised
    learning framework that utilizes the relative camera poses between image pairs
    to learn feature descriptors. By employing epipolar geometric constraints as supervision
    signals, they designed differentiable matching layers and a coarse-to-fine architecture,
    resulting in the generation of dense descriptors. DISK [[111](#bib.bib111)] maximizes
    the potential of reinforcement learning to integrate weakly supervised learning
    into an end-to-end Detector-based pipeline using policy gradients. This integrative
    approach of weak supervision with reinforcement learning can provide more robust
    learning signals and achieve effective optimization.  [[112](#bib.bib112)] proposes
    a group alignment approach that leverages the power of group-equivariant CNNs.
    These CNNs are efficient in extracting discriminative rotation-invariant local
    descriptors. The authors use a self-supervised loss for better orientation estimation
    and efficient local descriptor extraction. Weakly and semi-supervised methods
    using camera pose supervision and other techniques provide useful strategies to
    tackle the challenges of training robust local feature methods and may pave the
    way for more efficient and scalable learning methods in this domain.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督学习为模型提供了在无需密集标注标签的情况下学习鲁棒特征的机会，提供了训练深度学习模型面临的最大挑战之一的解决方案。几种弱监督局部特征学习方法应运而生，利用从相机姿态中容易获得的几何信息。**AffNet** [[108](#bib.bib108)]
    代表了弱监督局部特征学习的一个重要进展，专注于局部特征的仿射形状学习。这种方法挑战了传统对几何可重复性的重视，表明它对于可靠的特征匹配是不够的，并强调了基于描述符学习的重要性。**AffNet**
    引入了一个硬负样本常量损失函数，以提高仿射区域的匹配性和几何准确性。这在提升仿射协变检测器的性能方面被证明是有效的，特别是在宽基线匹配和图像检索中。这种方法强调了在开发更有效的局部特征检测器时需要同时考虑描述符匹配性和可重复性。**GLAMpoints** [[109](#bib.bib109)]
    提出了一个半监督关键点检测方法，创造性地从强化学习损失公式中汲取灵感。在这里，奖励被用来根据最终对齐的质量来计算检测关键点的意义。这种方法已被指出显著影响最终图像的匹配和配准质量。**CAPS** [[110](#bib.bib110)]
    介绍了一个弱监督学习框架，利用图像对之间的相对相机姿态来学习特征描述符。通过采用极几何约束作为监督信号，他们设计了可微分匹配层和粗到细的架构，从而生成了密集的描述符。**DISK** [[111](#bib.bib111)]
    最大限度地发挥了强化学习的潜力，将弱监督学习整合到基于检测器的端到端管道中，使用策略梯度。这种将弱监督与强化学习相结合的综合方法可以提供更鲁棒的学习信号，实现有效的优化。 [[112](#bib.bib112)]
    提出了一种利用群等变卷积神经网络（CNNs）力量的组对齐方法。这些CNN在提取判别性旋转不变的局部描述符方面效率高。作者使用自监督损失来提高方向估计的准确性和局部描述符的提取效率。使用相机姿态监督和其他技术的弱监督和半监督方法提供了应对训练鲁棒局部特征方法挑战的有用策略，并可能为该领域更高效、更可扩展的学习方法铺平道路。
- en: 2.2 Joint Detection and Description
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 联合检测与描述
- en: 'Sparse local feature matching has indeed proved very effective under a variety
    of imaging conditions. Yet, under extreme variations like day-night changes [[113](#bib.bib113)],
    different seasons [[114](#bib.bib114)], or weak-textured scenes [[115](#bib.bib115)],
    the performance of these features can deteriorate significantly. The limitations
    may stem from the nature of keypoint detectors and local descriptors. Detecting
    keypoints often involves focusing on small regions of the image and might rely
    heavily on low-level information, such as pixel intensities. This procedure makes
    keypoint detectors more susceptible to variations in low-level image statistics,
    which are often affected by changes in lighting, weather, and other environmental
    factors. Moreover, when trying to individually learn or train keypoint detectors
    or feature descriptors, even after carefully optimizing the individual components,
    integrating them into a feature matching pipeline could still lead to information
    loss or inconsistencies. This is due to the fact that the optimization of individual
    components might not fully consider the dependencies and information sharing between
    the components. To tackle these issues, the approach of Joint Detection and Description
    has been proposed. In this approach, the tasks of keypoint detection and description
    are integrated and learned simultaneously within a single model. This can enable
    the model to fuse information from both tasks during optimization, better adapting
    to specific tasks and data, and allowing deeper feature mappings through CNNs.
    Such a unified approach can benefit the task by allowing the detection and description
    process to be influenced by higher-level information, such as structural or shape-related
    features of the image. Additionally, dense descriptors involve richer image context,
    which generally leads to better performance. Figure [4](#S2.F4 "Figure 4 ‣ 2.1
    Detect-then-Describe ‣ 2 Detector-based Models ‣ Local Feature Matching Using
    Deep Learning: A Survey") (b) illustrates the common structure of the Joint Detection
    and Description pipeline.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '稀疏局部特征匹配在各种成像条件下确实证明了其高效性。然而，在极端变化情况下，例如昼夜变化[[113](#bib.bib113)]、不同季节[[114](#bib.bib114)]或纹理弱的场景[[115](#bib.bib115)]，这些特征的表现可能会显著下降。限制可能源于关键点检测器和局部描述符的性质。检测关键点通常涉及关注图像的小区域，并且可能严重依赖于低级信息，例如像素强度。这个过程使得关键点检测器更容易受到低级图像统计变化的影响，这些变化通常受到光照、天气和其他环境因素的影响。此外，即使在仔细优化了各个组件之后，当尝试单独学习或训练关键点检测器或特征描述符时，将它们整合到特征匹配管道中仍可能导致信息丢失或不一致。这是因为单个组件的优化可能没有充分考虑组件之间的依赖性和信息共享。为了解决这些问题，提出了联合检测和描述的方法。在这种方法中，关键点检测和描述的任务在一个模型中同时集成和学习。这可以使模型在优化过程中融合来自两个任务的信息，更好地适应特定任务和数据，并通过CNN实现更深层次的特征映射。这种统一的方法可以通过允许检测和描述过程受到更高级信息的影响，例如图像的结构或形状相关特征，从而使任务受益。此外，稠密描述符涉及更丰富的图像上下文，这通常会导致更好的性能。图[4](#S2.F4
    "Figure 4 ‣ 2.1 Detect-then-Describe ‣ 2 Detector-based Models ‣ Local Feature
    Matching Using Deep Learning: A Survey") (b)展示了联合检测和描述管道的常见结构。'
- en: Image-based descriptor methods, which take the entire image as input and utilize
    fully convolutional neural networks [[116](#bib.bib116)] to generate dense descriptors,
    have seen substantial progress in recent years. These methods often amalgamate
    the processes of detection and description, leading to improved performance in
    both tasks. SuperPoint [[61](#bib.bib61)] employs a self-supervised approach to
    simultaneously determine key-point locations at the pixel level and their descriptors.
    Initially, the model undergoes training on synthetic shapes and images through
    the application of random homographies. A crucial aspect of the method lies in
    its self-annotation process with real images. This process involves adapting homographies
    to enhance the model’s relevance to real-world images, and the MS-COCO dataset
    is employed for additional training. Ground truth key points for these images
    are generated through various homographic transformations, and key-point extraction
    is performed using the MagicPoint model. This strategy, which involves aggregating
    multiple key-point heatmaps, ensures precise determination of key-point locations
    on real images. Inspired by Q-learning, LF-Net [[117](#bib.bib117)] predicts geometric
    relationships, such as relative depth and camera poses, between matched image
    pairs using an existing SfM model. It employs asymmetric gradient backpropagation
    to train a network for detecting image pairs without needing manual annotation.
    Building upon LF-Net, RF-Net [[118](#bib.bib118)] introduces a receptive field-based
    keypoint detector and designs a general loss function term, referred to as ’neighbor
    mask’, which facilitates training of patch selection. Reinforced SP [[119](#bib.bib119)]
    employs principles of reinforcement learning to handle the discreteness in keypoint
    selection and descriptor matching. It integrates a feature detector into a complete
    visual pipeline and trains learnable parameters in an end-to-end manner. R2D2 [[63](#bib.bib63)]
    combines grid peak detection with reliability prediction for descriptors using
    a dense version of the L2-Net architecture, aiming to produce sparse, repeatable,
    and reliable keypoints. D2Net [[62](#bib.bib62)] adopts a joint detect-and-describe
    approach for sparse feature extraction. Unlike Superpoint, it shares all parameters
    between detection and description process and uses a joint formulation that optimizes
    both tasks simultaneously. Keypoints in their method are defined as local maxima
    within and across channels of the depth feature maps. These techniques elegantly
    illustrate how the integration of detection and description tasks in a unified
    model leads to more efficient learning and superior performance for local feature
    extraction under different imaging conditions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图像的描述符方法将整个图像作为输入，并利用完全卷积神经网络[[116](#bib.bib116)]生成密集描述符，近年来取得了显著进展。这些方法通常将检测和描述的过程融合在一起，从而提高了两个任务的性能。**SuperPoint**
    [[61](#bib.bib61)] 采用自监督方法同时确定像素级别的关键点位置及其描述符。最初，模型通过应用随机同胚进行合成形状和图像的训练。该方法的一个关键方面在于其使用真实图像进行自我标注的过程。这个过程涉及调整同胚，以增强模型与现实图像的相关性，并使用
    MS-COCO 数据集进行额外训练。对于这些图像的真实关键点通过各种同胚变换生成，关键点提取则使用**MagicPoint**模型进行。该策略通过聚合多个关键点热图，确保在真实图像上精确确定关键点位置。受到
    Q 学习启发，**LF-Net** [[117](#bib.bib117)] 预测匹配图像对之间的几何关系，如相对深度和相机姿态，使用现有的 SfM 模型。它采用非对称梯度反向传播训练一个检测图像对的网络，而无需人工标注。在**LF-Net**的基础上，**RF-Net**
    [[118](#bib.bib118)] 引入了一种基于感受野的关键点检测器，并设计了一个称为“邻域掩模”的通用损失函数项，以便于补丁选择的训练。**Reinforced
    SP** [[119](#bib.bib119)] 利用强化学习原理处理关键点选择和描述符匹配中的离散性。它将特征检测器集成到一个完整的视觉管道中，并以端到端的方式训练可学习参数。**R2D2**
    [[63](#bib.bib63)] 结合了网格峰值检测与描述符的可靠性预测，使用 L2-Net 架构的密集版本，旨在生成稀疏、可重复且可靠的关键点。**D2Net**
    [[62](#bib.bib62)] 采用联合检测和描述的方法进行稀疏特征提取。与**SuperPoint**不同的是，它在检测和描述过程中共享所有参数，并使用联合公式同时优化这两个任务。他们的方法中的关键点被定义为深度特征图中的局部极大值。这些技术优雅地展示了如何在统一模型中集成检测和描述任务，从而在不同成像条件下实现更高效的学习和更优越的局部特征提取性能。
- en: A dual-headed D2Net model with a correspondence ensemble is presented by RoRD [[120](#bib.bib120)]
    to address extreme viewpoint changes combing vanilla and rotation-robust feature
    correspondences. HDD-Net [[121](#bib.bib121)] designs an interactively learnable
    detector and descriptor fusion network, handling detector and descriptor components
    independently and focusing on their interactions during the learning process.
    MLIFeat [[122](#bib.bib122)] devises two lightweight modules used for keypoint
    detection and descriptor generation with multi-level information fusion utilized
    to jointly detect keypoints and extract descriptors. LLF [[123](#bib.bib123)]
    proposes utilizing low-level features to supervise keypoint detection. It extends
    a single CNN layer from the descriptor backbone as a detector and co-learns it
    with the descriptor to maximize descriptor matching. FeatureBooster [[124](#bib.bib124)]
    introduces a descriptor enhancement stage into traditional feature matching pipelines.
    It establishes a generic lightweight descriptor enhancement framework that takes
    original descriptors and geometric attributes of keypoints as inputs. The framework
    employs self-enhancement based on MLP and cross-enhancement based on transformers [[125](#bib.bib125)]
    to enhance descriptors. ASLFeat [[126](#bib.bib126)] improves the D2Net using
    channel and spatial peaks on multi-level feature maps. It introduces a precise
    detector and invariant descriptor as well as multi-level connections and deformable
    convolution networks. The dense prediction framework employs deformable convolution
    networks (DCN) to alleviate limitations caused by keypoint extraction from low-resolution
    feature maps. SeLF [[127](#bib.bib127)] builds on the Aslfeat architecture to
    leverage semantic information from pre-trained semantic segmentation networks
    used to learn semantically aware feature mappings. It combines learned correspondences-aware
    feature descriptors with semantic features, therefore, enhancing the robustness
    of local feature matching for long-term localization. Lastly, SFD2 [[128](#bib.bib128)]
    proposes the extraction of reliable features from global regions (e.g., buildings,
    traffic lanes) with the suppression of unreliable areas (e.g., sky, cars) by implicitly
    embedding high-level semantics into the detection and description processes. This
    enables the model to extract globally reliable features end-to-end from a single
    network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: RoRD [[120](#bib.bib120)] 提出了一个双头 D2Net 模型，结合了普通和旋转鲁棒特征对应关系，以应对极端视角变化。HDD-Net
    [[121](#bib.bib121)] 设计了一个可交互学习的检测器和描述符融合网络，独立处理检测器和描述符组件，并在学习过程中关注它们的互动。MLIFeat
    [[122](#bib.bib122)] 设计了两个轻量级模块，用于关键点检测和描述符生成，利用多级信息融合共同检测关键点和提取描述符。LLF [[123](#bib.bib123)]
    提出了利用低级特征来监督关键点检测。它将描述符主干中的单个 CNN 层扩展为检测器，并与描述符共同学习，以最大化描述符匹配。FeatureBooster [[124](#bib.bib124)]
    在传统特征匹配流程中引入了描述符增强阶段。它建立了一个通用轻量级描述符增强框架，输入为原始描述符和关键点的几何属性。该框架利用基于 MLP 的自我增强和基于
    transformers [[125](#bib.bib125)] 的交叉增强来提升描述符。ASLFeat [[126](#bib.bib126)] 使用多级特征图上的通道和空间峰值改进
    D2Net。它引入了一个精确的检测器和不变描述符，以及多级连接和可变形卷积网络。密集预测框架采用可变形卷积网络（DCN）来缓解低分辨率特征图中关键点提取的限制。SeLF
    [[127](#bib.bib127)] 基于 Aslfeat 架构，利用从预训练的语义分割网络中获得的语义信息，用于学习语义感知的特征映射。它将学习到的对应关系感知特征描述符与语义特征结合，从而增强了长期定位的局部特征匹配的鲁棒性。最后，SFD2
    [[128](#bib.bib128)] 提出了从全局区域（例如建筑物、车道）中提取可靠特征，同时通过隐式嵌入高级语义来抑制不可靠区域（例如天空、汽车）。这使得模型能够从单一网络中端到端提取全局可靠特征。
- en: 2.3 Describe-then-Detect
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 描述再检测
- en: 'One common approach to local feature extraction is the Describe-then-Detect
    pipeline, entailing the description of local image regions first using feature
    descriptors followed by the detection of keypoints based on these descriptors.
    Figure [4](#S2.F4 "Figure 4 ‣ 2.1 Detect-then-Describe ‣ 2 Detector-based Models
    ‣ Local Feature Matching Using Deep Learning: A Survey") (c) serves as an illustration
    of the standard structure of the Describe-then-Detect pipeline.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '本地特征提取的一个常见方法是描述再检测管道，即首先使用特征描述符描述本地图像区域，然后根据这些描述符检测关键点。图[4](#S2.F4 "Figure
    4 ‣ 2.1 Detect-then-Describe ‣ 2 Detector-based Models ‣ Local Feature Matching
    Using Deep Learning: A Survey") (c) 作为描述标准 Describe-then-Detect 管道结构的示例。'
- en: D2D [[129](#bib.bib129)] presents a novel framework for keypoint detection called
    Describe-to-Detect (D2D), highlighting the wealth of information inherent in the
    feature description phase. This framework involves the generation of a voluminous
    collection of dense feature descriptors followed by the selection of keypoints
    from this dataset. D2D introduces relative and absolute saliency measurements
    of local depth feature maps to define keypoints. Due to the challenges arising
    from weak supervision inability to differentiate losses between detection and
    description stages, PoSFeat [[130](#bib.bib130)] presents a decoupled training
    approach in the describe-then-detect pipeline specifically designed for weakly
    supervised local feature learning. This pipeline separates the description network
    from the detection network, leveraging camera pose information for descriptor
    learning that enhances performance. Through a novel search strategy, the descriptor
    learning process more adeptly utilizes camera pose information. ReDFeat [[131](#bib.bib131)]
    uses a mutual weighting strategy to combine multimodal feature learning’s detection
    and description aspects. SCFeat [[132](#bib.bib132)] proposes a shared coupling
    bridge strategy for weakly supervised local feature learning. Through shared coupling
    bridges and cross-normalization layers, the framework ensures the individual,
    optimal training of description networks and detection networks. This segregation
    enhances the robustness and overall performance of descriptors.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: D2D [[129](#bib.bib129)] 提出了一个新的关键点检测框架，称为Describe-to-Detect（D2D），突出了特征描述阶段固有的信息丰富性。该框架涉及生成大量密集特征描述符，并从该数据集中选择关键点。D2D引入了局部深度特征图的相对和绝对显著性度量来定义关键点。由于来自弱监督的挑战无法区分检测和描述阶段的损失，PoSFeat
    [[130](#bib.bib130)] 提出了在描述-再检测管道中的解耦训练方法，专为弱监督局部特征学习设计。该管道将描述网络与检测网络分离，利用相机姿态信息进行描述符学习，从而提升性能。通过一种新颖的搜索策略，描述符学习过程更加有效地利用相机姿态信息。ReDFeat
    [[131](#bib.bib131)] 使用互加权策略来结合多模态特征学习的检测和描述方面。SCFeat [[132](#bib.bib132)] 提出了一个共享耦合桥策略，用于弱监督局部特征学习。通过共享耦合桥和交叉归一化层，该框架确保了描述网络和检测网络的个体优化训练。这种分离增强了描述符的鲁棒性和整体性能。
- en: 2.4 Graph Based
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 基于图的
- en: 'In the conventional feature matching pipelines, correspondence relationships
    are established via nearest neighbor (NN) search of feature descriptors, and outliers
    are eliminated based on matching scores or mutual NN verification. In recent times,
    attention-based graph neural networks (GNNs) [[133](#bib.bib133)] have emerged
    as effective means to obtain local feature matching. These approaches create GNNs
    with keypoints as nodes and utilize self-attention layers and cross-attention
    layers from Transformers to exchange global visual and geometric information among
    nodes. This exchange overcomes the challenges posed by localized feature descriptors
    solely. The ultimate outcome is the generation of matches based on the soft assignment
    matrix. Figure [5](#S2.F5 "Figure 5 ‣ 2.4 Graph Based ‣ 2 Detector-based Models
    ‣ Local Feature Matching Using Deep Learning: A Survey") provides a comprehensive
    depiction of the fundamental architecture of Graph-Based matching.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '在传统的特征匹配管道中，通过对特征描述符进行最近邻（NN）搜索来建立对应关系，并根据匹配分数或互相NN验证来消除离群点。近年来，基于注意力的图神经网络（GNNs）[[133](#bib.bib133)]
    已经成为获取局部特征匹配的有效手段。这些方法创建了以关键点为节点的GNN，并利用Transformers中的自注意力层和交叉注意力层在节点之间交换全局视觉和几何信息。这种交换克服了仅依赖局部特征描述符所带来的挑战。最终结果是基于软分配矩阵生成匹配。图[5](#S2.F5
    "Figure 5 ‣ 2.4 Graph Based ‣ 2 Detector-based Models ‣ Local Feature Matching
    Using Deep Learning: A Survey")提供了基于图的匹配基本架构的全面描述。'
- en: '![Refer to caption](img/db4d0c1c33235a2460c67846948cff7a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/db4d0c1c33235a2460c67846948cff7a.png)'
- en: 'Figure 5: General GNN Matching Model Architecture. Firstly, keypoint positions
    $p_{i}$ along with their visual descriptors $d_{i}$ are mapped into individual
    vectors. Subsequently, self-attention layers and cross-attention layers are thereafter
    applied alternately, L times, within a graph neural network to create enhanced
    matching descriptors. Finally, the Sinkhorn Algorithm is utilized to determine
    the optimal partial assignment.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：一般 GNN 匹配模型架构。首先，将关键点位置 $p_{i}$ 及其视觉描述符 $d_{i}$ 映射为独立向量。随后，在图神经网络中交替应用自注意力层和交叉注意力层，共
    L 次，以创建增强的匹配描述符。最后，利用 Sinkhorn 算法来确定最优的部分分配。
- en: SuperGlue [[69](#bib.bib69)] adopts attention graph neural networks and optimal
    transport methods to address partial assignment problems. It processes two sets
    of interest points and their descriptors as inputs and leverages self and cross-attention
    to exchange messages between the two sets of descriptors. The complexity of this
    method grows quadratically with the number of keypoints, which prompted further
    exploration in subsequent works. SGMNet [[70](#bib.bib70)] builds on SuperGlue
    and adds a Seeding Module that processes only a subset of matching points as seeds.
    The fully connected graph is relinquished for a sparse connection graph. A seed
    graph neural network is then designed with an attention mechanism to aggregate
    information. Keypoints usually exhibit strong correlations with just a few points,
    resulting in a sparsely connected adjacency matrix for most keypoints. Therefore,
    ClusterGNN [[71](#bib.bib71)] makes use of graph node clustering algorithms to
    partition nodes in a graph into multiple clusters. This strategy applies attention
    GNN layers with clustering to learn feature matching between two sets of keypoints
    and their related descriptors, thus training the subgraphs to reduce redundant
    information propagation. MaKeGNN [[134](#bib.bib134)] introduces bilateral context-aware
    sampling and keypoint-assisted context aggregation in a sparse attention GNN architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: SuperGlue [[69](#bib.bib69)] 采用了注意力图神经网络和最优传输方法来解决部分分配问题。它将两个感兴趣点及其描述符作为输入，利用自注意力和交叉注意力在这两个描述符集之间交换信息。这种方法的复杂度随着关键点数量的增加而呈二次增长，这促使后续工作进行了进一步探索。SGMNet
    [[70](#bib.bib70)] 基于 SuperGlue，并添加了一个种子模块，该模块仅处理一部分匹配点作为种子。完全连接图被替换为稀疏连接图。然后设计了一个种子图神经网络，采用注意力机制来聚合信息。关键点通常与少数点表现出强相关性，从而导致大多数关键点的邻接矩阵稀疏连接。因此，ClusterGNN
    [[71](#bib.bib71)] 利用图节点聚类算法将图中的节点划分为多个簇。这一策略应用了具有聚类的注意力 GNN 层，以学习两个关键点集及其相关描述符之间的特征匹配，从而训练子图以减少冗余信息传播。MaKeGNN
    [[134](#bib.bib134)] 在稀疏注意力 GNN 架构中引入了双边上下文感知采样和关键点辅助上下文聚合。
- en: Inspired by SuperGlue, GlueStick [[135](#bib.bib135)] incorporates point and
    line descriptors into a joint framework for joint matching and leveraging point-to-point
    relationships to link lines from matched images. LightGlue [[136](#bib.bib136)],
    in an effort to make SuperGlue adaptive in computational complexity, proposes
    the dynamic alteration of the network’s depth and width based on the matching
    difficulty between each image pair. It devises a lightweight confidence classifier
    to forecast and hone state assignments. DenseGAP [[137](#bib.bib137)] devises
    a graph structure utilizing anchor points as sparse, yet reliable priors for inter-image
    and intra-image contexts. It propagates this information to all image points through
    directed edges. HTMatch [[138](#bib.bib138)] and Paraformer [[139](#bib.bib139)]
    study the application of attention for interactive mixing and explore architectures
    that strike a balance between efficiency and effectiveness. ResMatch [[140](#bib.bib140)]
    presents the idea of residual attention learning for feature matching, re-articulating
    self-attention and cross-attention as learned residual functions of relative positional
    reference and descriptor similarity. It aims to bridge the divide between interpretable
    matching and filtering pipelines and attention-based feature matching networks
    that inherently possess uncertainty via empirical means.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 SuperGlue 的启发，GlueStick [[135](#bib.bib135)] 将点和线描述符融入一个联合框架中，用于联合匹配，并利用点对点关系来连接匹配图像中的线条。LightGlue
    [[136](#bib.bib136)] 为了使 SuperGlue 在计算复杂性上具有适应性，提出了基于每对图像匹配难度动态调整网络的深度和宽度。它设计了一个轻量级的信心分类器，用于预测和优化状态分配。DenseGAP
    [[137](#bib.bib137)] 设计了一个图结构，利用锚点作为稀疏但可靠的先验信息，用于图像间和图像内的上下文。它通过有向边将这些信息传播到所有图像点。HTMatch
    [[138](#bib.bib138)] 和 Paraformer [[139](#bib.bib139)] 研究了注意力机制在交互混合中的应用，并探索了在效率和有效性之间取得平衡的架构。ResMatch
    [[140](#bib.bib140)] 提出了残差注意力学习的概念，用于特征匹配，将自注意力和交叉注意力重新表述为相对位置参考和描述符相似性的学习残差函数。它旨在通过经验手段弥合可解释匹配与过滤管道和固有不确定性的基于注意力的特征匹配网络之间的差距。
- en: 3 Detector-free Models
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 无检测器模型
- en: While the feature detection stage enables a reduction in the search space for
    matching, handling extreme circumstances, such as image pairs involving substantial
    viewpoint changes and textureless regions, prove to be difficult when using detection-based
    approaches, notwithstanding perfect descriptors and matching methodologies [[141](#bib.bib141)].
    Detector-free methods, on the other hand, eliminate feature detectors and directly
    extract visual descriptors on a dense grid spread across the images to produce
    dense matches. Thus, compared to Detector-based methods, these techniques can
    capture keypoints that are repeatable across image pairs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管特征检测阶段可以减少匹配的搜索空间，但处理极端情况，如涉及大量视角变化和无纹理区域的图像对，使用基于检测的方法仍然很困难，即使有完美的描述符和匹配方法
    [[141](#bib.bib141)]。无检测器方法则通过消除特征检测器，直接在分布于图像上的稠密网格中提取视觉描述符，从而生成稠密匹配。因此，与基于检测的方法相比，这些技术可以捕捉到在图像对之间可重复的关键点。
- en: 3.1 CNN Based
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 CNN 基础
- en: 'In the early stages, detection-free matching methodologies often relied on
    CNNs that used correlation or cost volume to identify potential neighborhood consistencies [[141](#bib.bib141)].
    Figure [6](#S3.F6 "Figure 6 ‣ 3.1 CNN Based ‣ 3 Detector-free Models ‣ Local Feature
    Matching Using Deep Learning: A Survey") illustrates the fundamental architecture
    of the 4D correspondence volume.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '在早期阶段，无检测匹配方法通常依赖于使用相关性或成本体积的 CNN 来识别潜在的邻域一致性 [[141](#bib.bib141)]。图 [6](#S3.F6
    "Figure 6 ‣ 3.1 CNN Based ‣ 3 Detector-free Models ‣ Local Feature Matching Using
    Deep Learning: A Survey") 展示了 4D 对应体积的基本架构。'
- en: '![Refer to caption](img/7404076fa79f03063cd330261ca1d666.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7404076fa79f03063cd330261ca1d666.png)'
- en: 'Figure 6: Overview of the 4D correspondence volume. Dense feature maps, denoted
    as $f^{A}$ and $f^{B}$, are extracted from images $I^{A}$ and $I^{A}$ using convolutional
    neural networks. Each individual feature match, $f_{ij}^{A}$ and $f_{kl}^{B}$,
    corresponds to the matching $\left(i,j,k,l\right)$ coordinates. The 4D correlation
    tensor $c$ is ultimately formed, which contains scores for all points between
    a pair of images that could potentially be corresponding points. Subsequently,
    matching pairs are obtained by analyzing the properties of corresponding points
    in the four-dimensional space.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：4D 对应体积概述。密集特征图，表示为 $f^{A}$ 和 $f^{B}$，是通过卷积神经网络从图像 $I^{A}$ 和 $I^{A}$ 中提取的。每个单独的特征匹配
    $f_{ij}^{A}$ 和 $f_{kl}^{B}$ 对应于匹配的 $\left(i,j,k,l\right)$ 坐标。最终形成 4D 相关张量 $c$，它包含了可能对应点的图像对之间的所有点的评分。随后，通过分析四维空间中对应点的属性来获得匹配对。
- en: NCNet [[64](#bib.bib64)] analyzes the neighborhood consistency in four-dimensional
    space of all possible corresponding points between a pair of images, obtaining
    matches without requiring a global geometric model. Sparse-NCNet [[65](#bib.bib65)]
    utilizes a 4D convolutional neural network on sparse correlation tensors and utilizes
    submanifold sparse convolutions to significantly reduce memory consumption and
    execution time. DualRC-Net [[66](#bib.bib66)] introduces an innovative methodology
    for establishing dense pixel-wise correspondences between image pairs in a coarse-to-fine
    fashion. Utilizing a dual-resolution strategy with a Feature Pyramid Network (FPN)-like
    backbone, the approach generates a 4D correlation tensor from coarse-resolution
    feature maps and refines it through a learnable neighborhood consensus module,
    thereby augmenting matching reliability and localization accuracy. GLU-Net [[67](#bib.bib67)]
    introduces a global-local universal network applicable to estimating dense correspondences
    for geometric matching, semantic matching, and optical flow. It trains the network
    in a self-supervised manner. GOCor [[142](#bib.bib142)] presents a fully differentiable
    dense matching module that predicts the global optimization matching confidence
    between two depth feature maps and can be integrated into state-of-the-art networks
    to replace feature correlation layers directly. PDCNet [[68](#bib.bib68)] proposes
    a probabilistic depth network that estimates dense image-to-image correspondences
    and their associated confidence estimates. It introduces an architecture and an
    improved self-supervised training strategy to achieve robust uncertainty prediction
    that is generalizable. PDC-Net+ [[143](#bib.bib143)] introduces a probabilistic
    deep network designed to estimate dense image-to-image correspondences and their
    associated confidence estimates. They employ a constrained mixture model to parameterize
    the predictive distribution, enhancing the modeling capacity for handling outliers.
    PUMP [[144](#bib.bib144)] combines unsupervised losses with standard self-supervised
    losses to augment synthetic images. By utilizing a 4D correlation volume, it leverages
    the non-parametric pyramid structure of DeepMatching [[145](#bib.bib145)] to learn
    unsupervised descriptors. DFM [[146](#bib.bib146)] utilizes a pre-trained VGG
    architecture as a feature extractor, capturing matches without requiring additional
    training strategies, thus demonstrating the robust power of features extracted
    from the deepest layers of the VGG network.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: NCNet [[64](#bib.bib64)] 分析了一对图像之间所有可能对应点在四维空间中的邻域一致性，从而在无需全局几何模型的情况下获得匹配。Sparse-NCNet [[65](#bib.bib65)]
    利用稀疏相关张量上的 4D 卷积神经网络，并通过子流形稀疏卷积显著减少内存消耗和执行时间。DualRC-Net [[66](#bib.bib66)] 引入了一种创新的方法，用于以粗到细的方式建立图像对之间的密集像素级对应关系。通过采用类似于特征金字塔网络（FPN）的双分辨率策略，该方法从粗分辨率特征图中生成
    4D 相关张量，并通过可学习的邻域一致性模块进行细化，从而提高匹配的可靠性和定位准确性。GLU-Net [[67](#bib.bib67)] 引入了一种全局-局部通用网络，适用于估计几何匹配、语义匹配和光流的密集对应关系。它以自我监督的方式训练网络。GOCor [[142](#bib.bib142)]
    提出了一个完全可微的密集匹配模块，预测两个深度特征图之间的全局优化匹配置信度，并且可以直接集成到最先进的网络中以替代特征相关层。PDCNet [[68](#bib.bib68)]
    提出了一个概率深度网络，用于估计密集的图像到图像的对应关系及其相关置信度估计。它引入了一种架构和改进的自我监督训练策略，以实现具有普遍性的鲁棒不确定性预测。PDC-Net+ [[143](#bib.bib143)]
    引入了一种概率深度网络，旨在估计密集的图像到图像的对应关系及其相关置信度估计。它们采用了约束混合模型来参数化预测分布，从而增强了处理异常值的建模能力。PUMP [[144](#bib.bib144)]
    结合了无监督损失和标准自我监督损失来增强合成图像。通过利用 4D 相关体积，它利用 DeepMatching [[145](#bib.bib145)] 的非参数金字塔结构来学习无监督描述符。DFM [[146](#bib.bib146)]
    使用预训练的 VGG 架构作为特征提取器，捕获匹配而无需额外的训练策略，从而展示了从 VGG 网络最深层提取的特征的强大能力。
- en: 3.2 Transformer Based
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于 Transformer
- en: The CNN’s dense feature receptive field may have limitations in handling regions
    with low texture or discerning between keypoints with similar feature representations.
    In contrast, humans tend to consider both local and global information when matching
    in such regions. Given Transformers’ success in computer vision tasks such as
    image classification [[147](#bib.bib147)], object detection [[148](#bib.bib148)],
    and semantic segmentation [[149](#bib.bib149), [150](#bib.bib150), [151](#bib.bib151),
    [152](#bib.bib152), [153](#bib.bib153)], researchers have explored incorporating
    Transformers’ global receptive field and long-range dependencies into local feature
    matching. Various approaches that integrate Transformers into feature extraction
    networks for local feature matching have emerged.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的密集特征感受野在处理低纹理区域或区分具有相似特征表示的关键点时可能存在局限性。相比之下，人类在匹配这些区域时往往会考虑局部和全局信息。鉴于变换器在计算机视觉任务如图像分类
    [[147](#bib.bib147)]、目标检测 [[148](#bib.bib148)] 和语义分割 [[149](#bib.bib149), [150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153)] 中的成功，研究人员已探讨将变换器的全局感受野和长距离依赖关系融入局部特征匹配中。已经出现了将变换器集成到特征提取网络中的各种方法用于局部特征匹配。
- en: Given that the only difference between sparse matching and dense matching is
    the quantity of points to query, COTR [[154](#bib.bib154)] combines the advantages
    of both approaches. It learns two matching images jointly with self-attention,
    using some keypoints as queries and recursively refining matches in the other
    image through a corresponding neural network. This integration combines both matches
    into one parameter-optimized problem. ECO-TR [[155](#bib.bib155)] strives to develop
    an end-to-end model to accelerate COTR by intelligently connecting multiple transformer
    blocks and progressively refining predicted coordinates in a coarse-to-fine manner
    on a shared multi-scale feature extraction network. LoFTR [[72](#bib.bib72)] is
    groundbreaking because it creates a GNN with keypoints as nodes, utilizing self-attention
    layers and mutual attention layers to obtain feature descriptors for two images
    and generating dense matches in regions with low texture. To overcome the absence
    of local attention interaction in LoFTR, Aspanformer [[73](#bib.bib73)] proposes
    an uncertainty-driven scheme based on flow prediction probabilistic modeling that
    adaptively varies the local attention span to allocate different context sizes
    for different positions. Expanding upon these developments, SE2-LoFTR [[156](#bib.bib156)]
    presents a significant advancement in addressing rotational challenges in feature
    matching. This enhancement involves modifying the LoFTR model by substituting
    its conventional backbone CNN with a steerable CNN. This alteration renders the
    model equivariant to both translations and image rotations, markedly augmenting
    its resilience to rotational variances. The fusion of steerable CNN-based feature
    extraction with techniques such as rectified planar surface matching demonstrates
    the versatility and extensive applicability of this approach.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到稀疏匹配和密集匹配之间唯一的区别是查询点的数量，COTR [[154](#bib.bib154)] 结合了这两种方法的优点。它通过自注意力机制联合学习两张匹配图像，使用一些关键点作为查询，并通过相应的神经网络递归地优化另一张图像中的匹配。这个集成将两个匹配合并为一个参数优化的问题。ECO-TR
    [[155](#bib.bib155)] 力求开发一个端到端的模型，通过智能连接多个变换器块并在共享的多尺度特征提取网络上以粗到细的方式逐步优化预测坐标，从而加速COTR。LoFTR
    [[72](#bib.bib72)] 的突破性在于它创建了一个以关键点为节点的图神经网络（GNN），利用自注意力层和互注意力层获取两张图像的特征描述符，并在纹理稀少的区域生成密集匹配。为了克服LoFTR中缺乏局部注意力交互的问题，Aspanformer
    [[73](#bib.bib73)] 提出了一个基于流预测概率建模的不确定性驱动方案，该方案自适应地调整局部注意力范围，为不同位置分配不同的上下文大小。在这些发展基础上，SE2-LoFTR
    [[156](#bib.bib156)] 在解决特征匹配中的旋转挑战方面取得了重要进展。这一改进涉及通过用可转动卷积神经网络（CNN）替代LoFTR模型的传统主干CNN，从而使模型对平移和图像旋转具有等变性，显著增强了其对旋转变化的鲁棒性。基于可转动CNN的特征提取与诸如修正平面表面匹配等技术的融合展示了这一方法的多样性和广泛应用性。
- en: To address the challenges posed by the presence of numerous similar points in
    dense matching approaches and the limitations on the performance of linear transformers
    themselves, several recent works have proposed novel methodologies. Quadtree [[157](#bib.bib157)]
    introduces quadtree attention to quickly skip calculations in irrelevant regions
    at finer levels, reducing the computational complexity of visual transformations
    from quadratic to linear. OETR [[158](#bib.bib158)] introduces the Overlap Regression
    method, which uses a Transformer decoder to estimate the degree of overlap between
    bounding boxes in an image. It incorporates a symmetric center consistency loss
    to ensure spatial consistency in the overlapping regions. OETR can be inserted
    as a preprocessing module into any local feature matching pipeline. MatchFormer [[159](#bib.bib159)]
    devises a hierarchical transformer encoder and a lightweight decoder. In each
    stage of the hierarchical structure, cross-attention modules and self-attention
    modules are interleaved to provide an optimal combination path, enhancing multi-scale
    features. CAT [[160](#bib.bib160)] proposes a context-aware network based on the
    self-attention mechanism, where attention layers can be applied along the spatial
    dimension for higher efficiency or along the channel dimension for higher accuracy
    and a reduced storage burden. TopicFM [[161](#bib.bib161)] encodes high-level
    context in images, utilizing a topic modeling approach. This improves matching
    robustness by focusing on semantically similar regions in images. ASTR [[162](#bib.bib162)]
    introduces an Adaptive Spot-guided Transformer, which includes a point-guided
    aggregation module to allow most pixels to avoid the influence of irrelevant regions,
    while using computed depth information to adaptively adjust the size of the grid
    at the refinement stage. DeepMatcher [[141](#bib.bib141)] introduces the Feature
    Transformation Module to ensure a smooth transition of locally aggregated features
    extracted from CNNs to features with a global receptive field, extracted from
    Transformers. It also presents SlimFormer, which builds deep networks, employing
    a hierarchical strategy that enables the network to adaptively absorb information
    exchange within residual blocks, simulating human-like behavior. OAMatcher [[163](#bib.bib163)]
    proposes the Overlapping Areas Prediction Module to capture keypoints in co-visible
    regions and conduct feature enhancement among them, simulating how humans shift
    focus from entire images to overlapping regions. They also propose a Matching
    Label Weight Strategy to generate coefficients for evaluating the reliability
    of true matching labels, using probabilities to determine whether the matching
    labels are correct. CasMTR [[164](#bib.bib164)] proposes to enhance the transformer-based
    matching pipeline by incorporating new stages of cascade matching and NMS detection.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对密集匹配方法中存在的大量相似点带来的挑战以及线性变压器自身性能的限制，最近一些研究提出了新颖的方法。Quadtree [[157](#bib.bib157)]
    引入了四叉树注意力，能够在更细的层级快速跳过无关区域的计算，将视觉变换的计算复杂度从平方级别降低到线性级别。OETR [[158](#bib.bib158)]
    引入了重叠回归方法，利用变压器解码器估计图像中边界框的重叠程度。它结合了对称中心一致性损失，以确保重叠区域的空间一致性。OETR 可以作为预处理模块插入任何局部特征匹配管道中。MatchFormer [[159](#bib.bib159)]
    设计了一个层次化变压器编码器和一个轻量级解码器。在层次结构的每个阶段，交叉注意力模块和自注意力模块交替排列，以提供最佳的组合路径，增强多尺度特征。CAT [[160](#bib.bib160)]
    提出了一个基于自注意力机制的上下文感知网络，其中注意力层可以沿空间维度应用以提高效率，或沿通道维度应用以提高准确性并减少存储负担。TopicFM [[161](#bib.bib161)]
    通过利用主题建模方法对图像中的高层上下文进行编码。这通过关注图像中的语义相似区域，提高了匹配的鲁棒性。ASTR [[162](#bib.bib162)] 引入了一种自适应点引导变压器，包括一个点引导聚合模块，使大多数像素能够避免无关区域的影响，同时利用计算得到的深度信息在细化阶段自适应地调整网格的大小。DeepMatcher [[141](#bib.bib141)]
    引入了特征变换模块，以确保从 CNN 中提取的局部聚合特征平滑过渡到从变压器中提取的具有全局感受野的特征。它还提出了 SlimFormer，构建深度网络，采用层次化策略，使网络能够自适应地吸收残差块中的信息交换，模拟人类行为。OAMatcher [[163](#bib.bib163)]
    提出了重叠区域预测模块，以捕捉共视区域中的关键点，并在这些区域之间进行特征增强，模拟人类如何从整个图像转移焦点到重叠区域。他们还提出了一种匹配标签权重策略，通过生成系数来评估真实匹配标签的可靠性，利用概率来判断匹配标签是否正确。CasMTR [[164](#bib.bib164)]
    提议通过引入新的级联匹配和 NMS 检测阶段来增强基于变压器的匹配管道。
- en: 'PMatch [[165](#bib.bib165)] enhances geometric matching performance by pretraining
    with transformer modules using a paired masked image modeling pretext task, utilizing
    the LoFTR module. To effectively leverage geometric priors, SEM [[166](#bib.bib166)]
    introduces a structured feature extractor that models relative positional relationships
    between pixels and highly confident anchor points. It also incorporates epipolar
    attention and matching techniques to filter out irrelevant regions based on epipolar
    constraints. DKM [[167](#bib.bib167)] addresses the two-view geometric estimation
    problem by devising a dense feature matching method. DKM presents a robust global
    matcher with a kernel regressor and embedded decoder, involving warp refinement
    through large depth-wise kernels applied to stacked feature maps. Building on
    this, RoMa [[168](#bib.bib168)] represents a significant advancement in dense
    feature matching by applying a Markov chain framework to analyze and improve the
    matching process. It introduces a two-stage approach: a coarse stage for globally
    consistent matching and a refinement stage for precise localization. This method,
    which separates the initial matching from the refinement process and employs robust
    regression losses for greater accuracy, has led to notable improvements in matching
    performance, outperforming current SotA.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: PMatch [[165](#bib.bib165)] 通过使用成对掩蔽图像建模预训练任务的变换器模块来提升几何匹配性能，并利用LoFTR模块。为了有效利用几何先验，SEM [[166](#bib.bib166)]
    引入了一个结构化特征提取器，该提取器建模像素与高度可信锚点之间的相对位置关系。它还结合了极线注意力和匹配技术，以根据极线约束过滤掉不相关的区域。DKM [[167](#bib.bib167)]
    通过设计一种密集特征匹配方法解决了双视图几何估计问题。DKM 提出了一个强大的全局匹配器，配有内核回归器和嵌入解码器，通过对堆叠特征图应用大深度内核进行变形优化。在此基础上，RoMa [[168](#bib.bib168)]
    通过应用马尔可夫链框架来分析和改进匹配过程，代表了密集特征匹配的显著进展。它引入了两阶段方法：一个用于全局一致匹配的粗略阶段和一个用于精确定位的优化阶段。这种方法将初始匹配与优化过程分开，并采用稳健的回归损失以获得更高的准确性，显著改善了匹配性能，超越了当前的最先进技术。
- en: 3.3 Patch Based
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于补丁的匹配
- en: 'The Patch-Based matching approach enhances point correspondences by matching
    local image regions. It involves dividing images into patches, extracting descriptor
    vectors for each, and then matching these vectors to establish correspondences.
    This technique accommodates large displacements and is valuable in various computer
    vision applications. Figure [7](#S3.F7 "Figure 7 ‣ 3.3 Patch Based ‣ 3 Detector-free
    Models ‣ Local Feature Matching Using Deep Learning: A Survey") illustrates the
    general architecture of the Patch-Based matching approach.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '基于补丁的匹配方法通过匹配局部图像区域来增强点对应关系。它涉及将图像分割成补丁，为每个补丁提取描述符向量，然后匹配这些向量以建立对应关系。这种技术适应大位移，并在各种计算机视觉应用中具有重要价值。图 [7](#S3.F7
    "Figure 7 ‣ 3.3 Patch Based ‣ 3 Detector-free Models ‣ Local Feature Matching
    Using Deep Learning: A Survey") 展示了基于补丁的匹配方法的总体架构。'
- en: '![Refer to caption](img/1cb9f7bfd17b666cb27cb4c03a09126f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1cb9f7bfd17b666cb27cb4c03a09126f.png)'
- en: 'Figure 7: Illustration of the Patch-Based Pipeline. Following the extraction
    of image features, the Match Refinement process is performed on the matched areas
    obtained from patch-level matches, resulting in refined point matching.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：基于补丁的管道示意图。在提取图像特征后，对从补丁级匹配中获得的匹配区域进行匹配优化处理，得到精确的点匹配。
- en: Patch2Pix [[169](#bib.bib169)] proposes a weakly supervised method to learn
    correspondences consistent with extreme geometric transformations between image
    pairs. It adopts a two-stage detection-refinement strategy for correspondence
    prediction, where the first stage captures semantic information and the second
    stage handles local details. It introduces a novel refinement network that utilizes
    weak supervision from extreme geometric transformations and outputs confidence
    in match positions and outlier rejection, allowing for geometrically consistent
    correspondence prediction. AdaMatcher [[170](#bib.bib170)] addresses the geometric
    inconsistency issue caused by applying the one-to-one assignment criterion in
    patch-level matching. It adaptively assigns patch-level matches while estimating
    the scale between images to improve the performance of dense feature matching
    methods in extreme cases. PATS [[171](#bib.bib171)] proposes Patch Area Transportation
    with Subdivision (PATS) to learn scale differences in a self-supervised manner.
    It can handle multiple-to-multiple relationships unlike bipartite graph matching,
    which only handles one-to-one matches. SGAM [[172](#bib.bib172)] presents a hierarchical
    feature matching framework that first performs region matching based on semantic
    clues, narrowing down the feature matching search space to region matches with
    significant semantic distributions between images. It then refines the region
    matches through geometric consistency to obtain accurate point matches.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Patch2Pix [[169](#bib.bib169)] 提出了一个弱监督方法，用于学习在图像对之间极端几何变换下保持一致的对应关系。它采用了一个两阶段的检测-精化策略来进行对应预测，其中第一阶段捕捉语义信息，第二阶段处理局部细节。它引入了一个新颖的精化网络，利用来自极端几何变换的弱监督，并输出匹配位置的置信度和离群点排除，从而实现几何一致的对应预测。AdaMatcher [[170](#bib.bib170)]
    解决了在补丁级匹配中应用一对一分配标准所导致的几何不一致问题。它在估计图像之间的尺度的同时，自适应地分配补丁级匹配，从而提高了在极端情况下密集特征匹配方法的性能。PATS [[171](#bib.bib171)]
    提出了带有细分的补丁区域传输（PATS），以自监督的方式学习尺度差异。它可以处理多对多关系，而不是仅处理一对一匹配的二分图匹配。SGAM [[172](#bib.bib172)]
    提出了一个分层特征匹配框架，该框架首先基于语义线索进行区域匹配，将特征匹配搜索空间缩小到图像之间具有显著语义分布的区域匹配。然后通过几何一致性精化区域匹配，以获得准确的点匹配。
- en: 4 Local Feature Matching Applications
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 本地特征匹配应用
- en: 4.1 Structure from Motion
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 从运动中重建结构
- en: SfM represents a foundational computational vision process, indispensable for
    deducing camera orientations, intrinsic parameters, and volumetric point clouds
    from an array of diverse scene images. This process substantiates endeavors such
    as visual localization, multi-view stereo, and the synthesis of innovative perspectives.
    The developmental trajectory of SfM, underscored by extensive scholarly investigations,
    has engendered firmly established methodologies, sophisticated open-source frameworks
    exemplified by Bundler [[173](#bib.bib173)] and COLMAP [[12](#bib.bib12)], and
    advanced proprietary software solutions. These frameworks are meticulously tailored
    to ensure precision and scalability when handling expansive scenes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: SfM 代表了一种基础的计算视觉过程，对于从一系列不同场景图像中推断相机方向、内在参数和体积点云至关重要。这个过程支撑了视觉定位、多视角立体视觉和新视角合成等工作。SfM
    的发展轨迹，经过广泛的学术研究，产生了成熟的方法论、复杂的开源框架，如 Bundler [[173](#bib.bib173)] 和 COLMAP [[12](#bib.bib12)]，以及先进的专有软件解决方案。这些框架被精心设计以确保在处理大规模场景时的精确性和可扩展性。
- en: Conventional SfM methodologies rely upon the identification and correlation
    of sparse characteristic points dispersed across manifold perspectives. Nonetheless,
    this methodology encounters formidable challenges in regions with scant textural
    features, where the identification of keypoints becomes a vexing undertaking.
    Lindenberger et al. [[174](#bib.bib174)] ameliorate this predicament by meticulously
    refining the initial keypoints and effecting subsequent adjustments to both points
    and camera orientations during post-processing. The proposed approach strategically
    balances an initial rudimentary estimation with sparse localized features and
    subsequent fine-tuning through locally-precise dense features, thereby elevating
    precision under challenging conditions. Recent strides in SfM have transitioned
    towards holistic approaches that either directly regress poses [[175](#bib.bib175),
    [176](#bib.bib176)] or employ differential bundle adjustment [[177](#bib.bib177),
    [178](#bib.bib178)]. These approaches, circumventing explicit feature correlation,
    sidestep the challenges associated with suboptimal feature matching. He et al. [[179](#bib.bib179)]
    introduce an innovative SfM paradigm devoid of detectors, harnessing detector-free
    matchers to defer the determination of keypoints. This strategy adeptly addresses
    the prevalent multi-view inconsistency in detector-free matchers, showcasing superior
    efficacy in texture-impoverished scenes relative to conventional detector-centric
    systems.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 SfM 方法依赖于识别和关联分布在多重视角中的稀疏特征点。然而，这种方法在纹理特征稀少的区域遇到了巨大的挑战，在这些区域中，关键点的识别变得非常棘手。Lindenberger
    等人[[174](#bib.bib174)] 通过仔细细化初始关键点并在后处理过程中对点和相机方向进行后续调整，改善了这一困境。该方法战略性地平衡了初步的粗略估计与稀疏的局部特征，并通过局部精确的密集特征进行后续的精细调整，从而在挑战条件下提高了精度。最近
    SfM 的进展已经转向整体方法，这些方法要么直接回归姿态[[175](#bib.bib175), [176](#bib.bib176)]，要么采用差分束调整[[177](#bib.bib177),
    [178](#bib.bib178)]。这些方法绕过了明确的特征关联，规避了与次优特征匹配相关的挑战。He 等人[[179](#bib.bib179)] 引入了一种创新的无探测器
    SfM 模式，利用无探测器匹配器推迟关键点的确定。这种策略巧妙地解决了无探测器匹配器中普遍存在的多视图不一致问题，相较于传统的探测器中心系统，在纹理贫乏的场景中表现出更优的效果。
- en: The evolutionary trajectory of SfM methodologies is discernible in the pivot
    from traditional sparse feature identification towards sophisticated, occasionally
    end-to-end, dense matching paradigms. The assimilation of these pioneering methodologies
    into extant SfM workflows is enhancing precision and resilience, particularly
    in arduous scenes. However, the seamless integration of these methodologies into
    contemporary SfM systems remains an intricate challenge.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: SfM 方法的发展轨迹可见于从传统的稀疏特征识别到复杂的、偶尔是端到端的密集匹配范式的转变。这些开创性方法的融合正在提升现有 SfM 工作流的精度和韧性，特别是在困难场景中。然而，将这些方法无缝集成到现代
    SfM 系统中仍然是一个复杂的挑战。
- en: 4.2 Remote Sensing Image registration
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 遥感图像配准
- en: 'In the domain of remote sensing, the emergence of deep learning has heralded
    a revolutionary epoch in Multimodal Remote Sensing Image Registration (MRSIR) [[43](#bib.bib43),
    [180](#bib.bib180), [181](#bib.bib181), [182](#bib.bib182)], augmenting conventional
    area- and feature-based techniques with a Learning-Based Pipeline (LBP) [[183](#bib.bib183),
    [184](#bib.bib184)]. This LBP diverges into several pioneering approaches: amalgamating
    deep learning with traditional registration methods, bridging the multimodal chasm
    through modality transformation, and directly regressing transformation parameters
    for a comprehensive MRSIR framework [[60](#bib.bib60)]. Techniques such as (pseudo-)
    Siamese networks and Generative Adversarial Networks (GANs) have played a pivotal
    role in this evolution, facilitating the management of geometric distortions and
    nonlinear radiometric disparities [[185](#bib.bib185), [186](#bib.bib186)]. For
    instance, the utilization of conditional GANs has enabled the creation of pseudo-images [[187](#bib.bib187)],
    thereby enhancing the precision of established methods like NCC [[188](#bib.bib188)]
    and SIFT [[17](#bib.bib17)].'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在遥感领域，深度学习的出现标志着多模态遥感图像配准（MRSIR）迎来了革命性的时代[[43](#bib.bib43), [180](#bib.bib180),
    [181](#bib.bib181), [182](#bib.bib182)]，将传统的面积和特征基础技术与基于学习的管道（LBP）[[183](#bib.bib183),
    [184](#bib.bib184)]相融合。这种LBP分为几种开创性的方法：将深度学习与传统的配准方法相结合，通过模态转换来桥接多模态差距，以及直接回归转换参数以建立全面的MRSIR框架[[60](#bib.bib60)]。诸如（伪）孪生网络和生成对抗网络（GANs）等技术在这一进化过程中发挥了关键作用，有助于管理几何失真和非线性辐射差异[[185](#bib.bib185),
    [186](#bib.bib186)]。例如，条件GANs的应用使得伪图像的创建成为可能[[187](#bib.bib187)], 从而增强了NCC[[188](#bib.bib188)]和SIFT[[17](#bib.bib17)]等已建立方法的精度。
- en: In the LBP, a multitude of innovative methods and architectures have been formulated.
    MUNet [[181](#bib.bib181)], a multiscale strategy for learning transformation
    parameters, and fully convolutional networks for scale-specific feature extraction
    stand as quintessential examples of this innovation, addressing the challenges
    of nonrigid MRSIR [[189](#bib.bib189)]. Enriching the LBP further, various research
    endeavors have concentrated on integrating middle- or high-level features extracted
    by CNNs with classical descriptors, surmounting limitations of traditional methodologies.
    For example, Ye et al. [[190](#bib.bib190), [181](#bib.bib181)] devised a novel
    multispectral image registration technique employing a CNN and SIFT amalgamation,
    substantially enhancing registration efficacy. Similarly, Wang et al. [[191](#bib.bib191)]
    developed an end-to-end deep learning architecture that discerns the mapping function
    between image patch-pairs and their matching labels, employing transfer learning
    for expedited training. Ma et al. [[192](#bib.bib192)] introduced a coarse-to-fine
    registration method using CNN and local features, attaining a profound pyramid
    feature representation via VGG-16. Zhou et al. [[193](#bib.bib193)] developed
    a deep learning-based method for matching synthetic aperture radar (SAR) and optical
    images, concentrating on extracting Multiscale Convolutional Gradient Features
    (MCGFs) using a shallow pseudo-Siamese network. This approach effectively captures
    commonalities between SAR and optical images, transcending the confines of handcrafted
    features and diminishing the necessity for extensive model parameters. Cui et
    al. [[194](#bib.bib194)] introduced MAP-Net, an image-centric convolutional network
    integrating Spatial Pyramid Aggregated Pooling (SPAP) and an attention mechanism,
    proficient in addressing geometric distortions and radiometric variations in cross-modal
    images by embedding original images to extract high-level semantic information
    and employing PCA for enhanced matching accuracy.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在LBP中，已经制定了许多创新的方法和架构。MUNet[[181](#bib.bib181)]，一种用于学习变换参数的多尺度策略，以及用于尺度特定特征提取的全卷积网络，是这一创新的典型例子，解决了非刚性MRSIR[[189](#bib.bib189)]的挑战。为了进一步丰富LBP，各种研究工作集中于将CNN提取的中级或高级特征与经典描述符相结合，克服了传统方法的局限性。例如，Ye等人[[190](#bib.bib190),
    [181](#bib.bib181)]设计了一种新型的多光谱图像配准技术，采用CNN和SIFT的结合，大大提高了配准效率。同样，Wang等人[[191](#bib.bib191)]开发了一种端到端的深度学习架构，能够识别图像补丁对及其匹配标签之间的映射函数，利用迁移学习加速训练。Ma等人[[192](#bib.bib192)]提出了一种使用CNN和局部特征的粗到细配准方法，通过VGG-16获得了深刻的金字塔特征表示。Zhou等人[[193](#bib.bib193)]开发了一种基于深度学习的方法，用于配准合成孔径雷达（SAR）图像和光学图像，专注于使用浅层伪Siamese网络提取多尺度卷积梯度特征（MCGFs）。这种方法有效地捕捉了SAR和光学图像之间的共性，超越了手工设计特征的限制，减少了对大量模型参数的需求。Cui等人[[194](#bib.bib194)]介绍了MAP-Net，一种图像中心的卷积网络，结合了空间金字塔聚合池化（SPAP）和注意机制，能够通过嵌入原始图像提取高级语义信息，并利用PCA提高匹配精度，有效解决了跨模态图像中的几何畸变和辐射变异。
- en: Notwithstanding these advancements, challenges in dataset construction and method
    generalization persist, principally owing to the diverse and intricate nature
    of remote sensing images [[181](#bib.bib181)]. The development of comprehensive
    and representative training datasets, coupled with innovative methodologies meticulously
    tailored for remote sensing imagery, remains an imperative objective. Moreover,
    there is a dearth of valuable research in pixel-level fusion of radar and optical
    images, requiring more attention in future endeavors [[195](#bib.bib195)].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些进展，数据集构建和方法泛化方面的挑战仍然存在，这主要是由于遥感图像的多样性和复杂性[[181](#bib.bib181)]。开发全面且具有代表性的训练数据集，并结合专门为遥感图像精心设计的创新方法，仍然是一个迫切的目标。此外，雷达图像和光学图像在像素级融合方面的有价值研究仍然稀缺，未来的工作中需要更多关注[[195](#bib.bib195)]。
- en: 4.3 Medical Image Registration
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 医学图像配准
- en: The field of medical image registration has undergone significant evolution
    with the integration of sophisticated deep learning techniques, particularly in
    motion estimation and 2D-3D registration. These advancements not only represent
    a technological leap but also open new vistas in various medical applications [[58](#bib.bib58)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像配准领域随着复杂深度学习技术的集成，特别是在运动估计和2D-3D配准方面，经历了显著的演变。这些进展不仅代表了技术上的飞跃，还为各种医学应用开辟了新视野[[58](#bib.bib58)]。
- en: Motion estimation in medical imaging, a crucial aspect of registration, has
    been substantially refined with deep learning. Unsupervised optical flow and point
    tracking techniques, as expounded by researchers like Bian et al. [[196](#bib.bib196)],
    Ranjan et al. [[197](#bib.bib197)], and Harley et al. [[198](#bib.bib198)], address
    the complexities inherent in medical image data, such as variability in patient
    anatomy and the need for maintaining anatomical integrity through diffeomorphism
    and incompressibility. Deep learning-based methods have shown efficacy in motion
    estimation across different organs, from the heart to the lungs. For example,
    the application of variational autoencoder-based models, as demonstrated by Qin
    et al. [[199](#bib.bib199)], by traversing the biomechanically reasonable deformation
    manifold to search for the best transformation of a given heart sequence, better
    motion tracking accuracy and more reasonable estimation of myocardial motion and
    strain are obtained, enhancing the realism and clinical reliability of motion
    estimation. DeepTag [[200](#bib.bib200)] and DRIMET [[201](#bib.bib201)] showcase
    sophisticated methodologies for tracking internal tissue motion, particularly
    in the context of Tagged-MRI. These approaches exemplify the ability to estimate
    dense 3D motion fields through advanced unsupervised learning techniques in medical
    imaging. The recent advancement in one-shot learning for deformable medical image
    registration is conspicuously exemplified in contemporary research, particularly
    in the application of one-shot learning to complex 3D and 4D medical datasets,
    thereby enhancing accuracy, reducing dependency on large training datasets, and
    broadening the scope of applicability. Fechter et al. [[202](#bib.bib202)] introduced
    a one-shot learning approach for deep motion tracking in 3D and 4D datasets, addressing
    the challenge of requiring extensive training data. Their method concatenates
    images from different phases in the channel dimension, utilizing a U-Net architecture
    with a coarse-to-fine strategy. This approach allows the simultaneous calculation
    of forward and inverse transformations in 3D datasets. Zhang et al. [[203](#bib.bib203)]
    introduced GroupRegNet, a one-shot deep learning method designed for 4D image
    registration. It employs an implicit template, effectively reducing bias and accumulated
    error. The simplicity of GroupRegNet’s network design and its direct registration
    process eliminates the need for image partitioning, resulting in a significant
    enhancement in both computational efficiency and accuracy. Furthering the development
    in this domain, Ji et al. [[204](#bib.bib204)] proposed a temporal-spatial method
    for lung 4D-CT image registration. This method integrates a CNN-ConvLSTM hybrid
    architecture, adeptly modeling the temporal motion of images while incorporating
    a dual-stream approach to address periodic motion constraints. The Hybrid Paradigm-based
    Registration Network (HPRN) [[205](#bib.bib205)] introduces an unsupervised learning
    framework for 4D-CT lung image registration, effectively handling large deformations
    without ground truth data. HPRN achieves superior registration accuracy by learning
    multi-scale features, incorporating advanced loss functions, and avoiding preprocessing
    steps like cropping and scaling.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 医学成像中的运动估计，作为配准的关键方面，已通过深度学习得到显著改进。无监督光流和点跟踪技术，如Bian等人[[196](#bib.bib196)]、Ranjan等人[[197](#bib.bib197)]和Harley等人[[198](#bib.bib198)]所阐述的，解决了医学图像数据中固有的复杂性，如患者解剖的变异性以及通过微分同胚和不可压缩性保持解剖完整性的需求。基于深度学习的方法在不同器官的运动估计中显示出有效性，从心脏到肺部。例如，Qin等人[[199](#bib.bib199)]展示了基于变分自编码器模型的应用，通过遍历生物力学合理的变形流形，寻找给定心脏序列的最佳变换，从而获得更好的运动跟踪精度和更合理的心肌运动及应变估计，提高了运动估计的真实感和临床可靠性。DeepTag[[200](#bib.bib200)]和DRIMET[[201](#bib.bib201)]展示了用于跟踪内部组织运动的复杂方法，特别是在标记MRI的背景下。这些方法展示了通过先进的无监督学习技术在医学成像中估计密集的3D运动场的能力。近期在可变形医学图像配准中的单次学习进展在当代研究中得到了显著体现，特别是在将单次学习应用于复杂的3D和4D医学数据集，从而提高了精度，减少了对大规模训练数据集的依赖，并扩展了适用范围。Fechter等人[[202](#bib.bib202)]提出了一种用于3D和4D数据集深度运动跟踪的单次学习方法，解决了对大量训练数据的需求挑战。他们的方法在通道维度上连接来自不同阶段的图像，利用U-Net架构和粗到细的策略。这种方法允许在3D数据集中同时计算前向和反向变换。Zhang等人[[203](#bib.bib203)]提出了GroupRegNet，一种用于4D图像配准的单次深度学习方法。它采用隐式模板，有效减少了偏差和累积误差。GroupRegNet的网络设计简单且直接配准过程消除了图像分割的需要，显著提高了计算效率和精度。在此领域的发展中，Ji等人[[204](#bib.bib204)]提出了一种用于肺部4D-CT图像配准的时间-空间方法。该方法整合了CNN-ConvLSTM混合架构，巧妙地建模了图像的时间运动，同时结合双流方法以应对周期性运动限制。基于Hybrid
    Paradigm的配准网络（HPRN）[[205](#bib.bib205)]引入了一种无监督学习框架，用于4D-CT肺部图像配准，有效处理了没有真实数据的大变形。HPRN通过学习多尺度特征、引入先进的损失函数，并避免了裁剪和缩放等预处理步骤，实现了更高的配准精度。
- en: 2D-3D registration is a critical component, particularly in interventional procedures [[206](#bib.bib206)].
    This process is vital for accurately overlaying 2D images (like X-Ray, ultrasound,
    or endoscopic images) onto 3D pre-operative CT or MR images. The key challenge
    here lies in the accurate geometric alignment of these differing dimensionalities.
    Traditional approaches to 2D-3D registration have relied on iterative optimization
    methods with similarity metrics based on image intensity [[207](#bib.bib207)].
    However, these methods often struggle with the non-convex nature of the problem,
    potentially resulting in convergence to incorrect solutions if the initial estimate
    is not close to the actual solution. This is compounded by the inherent difficulty
    of representing 3D spatial information on 2D images, leading to registration ambiguity.
    Recent advancements, however, have seen a shift towards deep learning-based methods.
    These approaches, unlike their traditional counterparts, do not require explicit
    functional mappings, allowing for more robust solutions to the registration challenge [[208](#bib.bib208)].
    In the realm of recent developments concerning 2D-3D medical image registration,
    Jaganathan et al. [[209](#bib.bib209)] have introduced a self-supervised paradigm
    designed for the fusion of X-ray and CT images. This method leverages simulated
    X-ray projections to facilitate the training of deep neural networks, culminating
    in a noteworthy enhancement of registration accuracy and success ratios. Simultaneously,
    Huang et al. [[210](#bib.bib210)] have devised a two-stage framework tailored
    for neurological interventions. This innovative approach amalgamates CNN regression
    with centroid alignment, manifesting superior efficacy in real-time clinical applications.
    In addition to rigid 2D-3D registration, there is growing interest in non-rigid
    registration, which is crucial in applications like cephalometry, lung tumor tracking
    in radiation therapy, and Total Hip Arthroplasty (THA) [[211](#bib.bib211), [212](#bib.bib212),
    [213](#bib.bib213)]. Deep learning models, such as convolutional encoders, have
    been used to address the challenges of non-rigid registration [[214](#bib.bib214),
    [215](#bib.bib215)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 2D-3D 注册是一个关键组件，特别是在介入手术中[[206](#bib.bib206)]。这个过程对于将2D图像（如X射线、超声波或内窥镜图像）准确地叠加到3D术前CT或MR图像上至关重要。这里的关键挑战在于这些不同维度之间的准确几何对齐。传统的2D-3D注册方法依赖于基于图像强度的相似性度量的迭代优化方法[[207](#bib.bib207)]。然而，这些方法常常面临问题的非凸性质，可能会导致如果初始估计与实际解决方案相差较远时，收敛到不正确的解决方案。这一问题还受到在2D图像上表示3D空间信息的固有困难的影响，导致注册模糊。近年来的进展中，已经出现了基于深度学习的方法。这些方法不同于传统方法，不需要明确的功能映射，从而提供了更为稳健的注册解决方案[[208](#bib.bib208)]。在2D-3D医学图像注册的最新发展领域中，Jaganathan
    等人[[209](#bib.bib209)]提出了一种自监督范式，旨在融合X射线和CT图像。这种方法利用模拟的X射线投影来促进深度神经网络的训练，显著提高了注册的准确性和成功率。与此同时，Huang
    等人[[210](#bib.bib210)]设计了一个针对神经干预的双阶段框架。这一创新方法将CNN回归与质心对齐结合在一起，在实时临床应用中表现出卓越的效能。除了刚性2D-3D注册，非刚性注册也受到越来越多的关注，这对于颅面测量、放射治疗中的肺肿瘤追踪和全髋关节置换术（THA）等应用至关重要[[211](#bib.bib211),
    [212](#bib.bib212), [213](#bib.bib213)]。深度学习模型，如卷积编码器，已被用于解决非刚性注册的挑战[[214](#bib.bib214),
    [215](#bib.bib215)]。
- en: The convergence of motion estimation and 2D-3D registration techniques in medical
    image registration addresses critical challenges in parameter optimization and
    ambiguity, enhancing both the speed and accuracy of medical imaging processes.
    The ongoing evolution in this field is poised to revolutionize diagnostic and
    interventional procedures, making them more efficient, patient-centric, and outcome-focused.
    Interested readers can refer to the comprehensive survey [[58](#bib.bib58), [57](#bib.bib57)]
    for a detailed overview of deep learning-based approaches to medical image registration.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 运动估计与2D-3D注册技术在医学图像注册中的融合解决了参数优化和模糊性方面的关键挑战，提高了医学成像过程的速度和准确性。这一领域的持续发展有望彻底改变诊断和介入程序，使其更加高效、以患者为中心，并关注结果。感兴趣的读者可以参考全面的调查[[58](#bib.bib58),
    [57](#bib.bib57)]，了解基于深度学习的医学图像注册方法的详细概述。
- en: 5 Local Feature Matching Datasets
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 本地特征匹配数据集
- en: 'Local feature matching methods are often evaluated based on their effectiveness
    in downstream tasks. In this section, we will provide summaries of some of the
    most widely used datasets for evaluating local feature matching. We categorize
    these datasets into five groups: Image Matching Datasets, Relative Pose Estimation
    Datasets, Visual Localization Datasets, Optical Flow Estimation Datasets, and
    Structure from Motion Datasets. For each dataset, we will provide detailed information
    about the features it encompasses.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 局部特征匹配方法通常根据其在下游任务中的有效性进行评估。在本节中，我们将总结一些用于评估局部特征匹配的最广泛使用的数据集。我们将这些数据集分为五个类别：图像匹配数据集、相对姿态估计数据集、视觉定位数据集、光流估计数据集和运动重建数据集。对于每个数据集，我们将提供关于其特征的详细信息。
- en: 5.1 Image Matching Datasets
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 图像匹配数据集
- en: 'HPatches [[216](#bib.bib216)] benchmark stands as a prominent yardstick for
    image matching endeavors. It comprises 116 scene sequences distinguished by fluctuations
    in viewpoint and luminosity. Within each scene, there are 5 pairs of images, with
    the inaugural image serving as the point of reference, and the subsequent images
    in the sequence progressively intensifying in complexity. This dataset is bifurcated
    into two distinct domains: viewpoint, encompassing 59 sequences marked by substantial
    viewpoint alterations, and illumination, encompassing 57 sequences marked by substantial
    variances in illumination, spanning both natural and artificial luminance conditions.
    In each test sequence, one reference image is paired with the remaining five images.
    It is worth noting that according to the evaluation method of D2Net, 56 sequences
    with significant viewpoint changes and 52 sequences with significant illumination
    changes are usually used to evaluate the performance of the network. Starting
    from SuperPoint [[61](#bib.bib61)], the HPatches dataset has also been used to
    evaluate the performance of local descriptors in homography estimation tasks.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: HPatches [[216](#bib.bib216)] 基准作为图像匹配工作的重要标准。它包含116个场景序列，特点是视点和光照的波动。在每个场景中，有5对图像，第一张图像作为参考点，后续图像的复杂度逐步增加。该数据集被分为两个不同的领域：视点，涵盖59个视点变化显著的序列，以及光照，涵盖57个光照变化显著的序列，包括自然和人工光照条件。在每个测试序列中，一张参考图像与剩下的五张图像配对。值得注意的是，根据D2Net的评估方法，通常使用56个视点变化显著的序列和52个光照变化显著的序列来评估网络的性能。从SuperPoint [[61](#bib.bib61)]开始，HPatches数据集也用于评估局部描述符在单应性估计任务中的表现。
- en: Roto-360 [[112](#bib.bib112)] is an evaluation dataset consisting of 360 image
    pairs. These pairs feature rotations within a plane ranging from 0° to 350° in
    10° intervals. The dataset was generated by randomly selecting and rotating ten
    HPatches images, making it valuable for assessing descriptor performance in terms
    of rotation invariance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Roto-360 [[112](#bib.bib112)] 是一个评估数据集，由360对图像组成。这些图像对在一个平面内旋转，角度范围从0°到350°，间隔为10°。该数据集通过随机选择和旋转十张HPatches图像生成，对于评估描述符在旋转不变性方面的表现非常有价值。
- en: 5.2 Relative Pose Estimation Datasets
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 相对姿态估计数据集
- en: ScanNet [[217](#bib.bib217)] is a large-scale indoor dataset with well-defined
    training, validation, and test splits comprising approximately 230 million well-defined
    image pairs from 1613 scenes. This dataset includes ground truth and depth images
    and contains more regions with repetitive and weak textures compared to the Hpatches
    dataset, thus posing greater challenges.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ScanNet [[217](#bib.bib217)] 是一个大规模的室内数据集，具有明确定义的训练、验证和测试划分，包括来自1613个场景的大约2.3亿对明确定义的图像。该数据集包括真实值和深度图像，并且包含更多重复和纹理较弱的区域，相较于Hpatches数据集，挑战更大。
- en: YFCC100M [[218](#bib.bib218)] is a vast dataset with a diverse collection of
    internet images of various tourist landmarks. It comprises 100 million media objects,
    of which approximately 99.2 million are photos and 0.8 million are videos, with
    each media object represented by several metadata pieces, such as the Flickr identifier,
    owner name, camera information, title, tags, geo-location, and media source. Typically,
    a subset of YFCC100M is used for evaluation, consisting of four popular landmark
    image sets of 1000 image pairs each, resulting in a total of 4000 pairs for the
    test set and following the conventions used in [[77](#bib.bib77), [69](#bib.bib69),
    [70](#bib.bib70), [73](#bib.bib73)].
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: YFCC100M [[218](#bib.bib218)] 是一个庞大的数据集，包含各种旅游地标的互联网图像。它包括1亿个媒体对象，其中约9920万个是照片，80万个是视频，每个媒体对象都包含多个元数据，如Flickr标识符、所有者姓名、相机信息、标题、标签、地理位置和媒体来源。通常，会使用YFCC100M的一个子集进行评估，该子集由四个流行的地标图像集组成，每个图像集包含1000对图像，总共有4000对用于测试集，并遵循[[77](#bib.bib77),
    [69](#bib.bib69), [70](#bib.bib70), [73](#bib.bib73)]中使用的惯例。
- en: MegaDepth [[219](#bib.bib219)] is a dataset designed to address the challenging
    task of matching under extreme viewpoint changes and repetitive patterns. It comprises
    1 million image pairs from 196 different outdoor scenes, each with known poses
    and depth information, and can be used to validate pose estimation effectiveness
    in outdoor scenarios. The authors also provide depth maps generated from sparse
    reconstruction and multi-view stereo computation using COLMAP [[12](#bib.bib12)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: MegaDepth [[219](#bib.bib219)] 是一个旨在处理在极端视点变化和重复模式下匹配的挑战性任务的数据集。它包含来自196个不同户外场景的100万对图像，每个图像都有已知的姿态和深度信息，并可用于验证户外场景中的姿态估计效果。作者还提供了通过稀疏重建和多视图立体计算生成的深度图，使用了COLMAP
    [[12](#bib.bib12)]。
- en: EVD (Extreme Viewpoint Dataset) [[220](#bib.bib220)] is a meticulously curated
    dataset, specifically developed for the assessment of two-view matching algorithms
    under scenarios of extreme viewpoint alterations. It amalgamates image pairs from
    a variety of publicly accessible datasets, distinguished by their intricate geometric
    configurations. EVD’s creation was motivated by the necessity to evaluate the
    resilience of matching methodologies in contexts characterized by pronounced variations
    in viewpoints.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: EVD（极端视点数据集）[[220](#bib.bib220)] 是一个精心策划的数据集，专门用于评估在极端视点变化情况下的双视图匹配算法。它将来自各种公开数据集的图像对汇集在一起，以其复杂的几何配置而著称。EVD的创建动机在于评估匹配方法在视点显著变化的情况下的鲁棒性。
- en: WxBS (Wide multiple Baseline Stereo) [[221](#bib.bib221)] addresses a more expansive
    challenge within the realm of wide baseline stereo matching, encompassing disparities
    in multiple facets of image acquisition such as viewpoint, lighting, sensor type,
    and visual alterations. This dataset comprises 37 image pairs, featuring a blend
    of urban and natural environments, systematically categorized based on the presence
    of various complicating factors. The ground truth for WxBS is established through
    a collection of manually selected correspondences, capturing the segments of the
    scene that are visible in both images. WxBS serves as a pivotal tool for the appraisal
    of algorithms tailored for image matching under a spectrum of demanding conditions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: WxBS（宽基线立体视觉）[[221](#bib.bib221)] 处理了宽基线立体匹配领域中的更广泛挑战，涵盖了图像获取中的多个方面的差异，如视点、光照、传感器类型和视觉变化。该数据集包含37对图像，结合了城市和自然环境，根据各种复杂因素的存在进行系统分类。WxBS的真实数据通过手动选择的对应关系来建立，捕捉在两幅图像中都可见的场景片段。WxBS作为评估针对各种苛刻条件下图像匹配算法的关键工具。
- en: 5.3 Visual Localization Datasets
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 视觉定位数据集
- en: Aachen Day-Night [[222](#bib.bib222)] is a dataset consisting of 4328 daytime
    images and 98 nighttime images, which are used for localization tasks. This benchmark
    challenges matching between daytime and nighttime images, making it a challenging
    dataset to work with. Aachen Day-Night v1.1 [[9](#bib.bib9)] is an updated version
    of the Aachen Day-Night dataset with 6697 daytime images and 1015 query images
    (824 for the day and 191 for the night). The presence of large illumination and
    viewpoint changes makes it a challenging dataset to work with.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Aachen Day-Night [[222](#bib.bib222)] 是一个包含4328张白天图像和98张夜晚图像的数据集，用于定位任务。该基准挑战了白天和夜晚图像之间的匹配，使其成为一个具有挑战性的数据集。Aachen
    Day-Night v1.1 [[9](#bib.bib9)] 是Aachen Day-Night数据集的更新版本，包含6697张白天图像和1015张查询图像（白天824张，夜晚191张）。由于存在大幅的光照和视点变化，使其成为一个具有挑战性的数据集。
- en: InLoc [[115](#bib.bib115)] is an indoor dataset that includes 9972 RGBD images;
    329 RGB images from it are used as queries to test the performance of long-term
    indoor visual localization algorithms. This dataset provides a variety of challenges
    due to its large size (around 10k images covering two buildings), significant
    differences in viewpoint and/or illumination between the database and query images,
    and temporal changes in the scene. In addition to this, the InLoc dataset provides
    a large collection of depth maps from 3D scanners.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: InLoc [[115](#bib.bib115)] 是一个室内数据集，包含9972张RGBD图像；其中329张RGB图像用于测试长期室内视觉定位算法的性能。该数据集由于其大规模（约10k张图像覆盖两栋建筑物）、数据库和查询图像之间显著的视点和/或光照差异以及场景的时间变化，提供了各种挑战。此外，InLoc数据集还提供了来自3D扫描仪的大量深度图。
- en: RobotCar-Seasons (RoCaS) [[223](#bib.bib223)] is a challenging dataset that
    contains 26121 reference images and 11934 query images. The dataset presents a
    variety of environmental conditions, including rain, snow, dusk, winter, and inadequate
    lighting in suburban areas. These factors make the task of feature matching and
    visual localization difficult.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: RobotCar-Seasons (RoCaS) [[223](#bib.bib223)] 是一个具有挑战性的数据集，包含26121张参考图像和11934张查询图像。该数据集呈现了多种环境条件，包括雨天、雪天、黄昏、冬季和郊区的光照不足。这些因素使得特征匹配和视觉定位任务变得困难。
- en: LaMAR [[224](#bib.bib224)] addresses the foundational technology of localization
    and mapping in augmented reality (AR), introducing a new benchmark for realistic
    AR scenarios. The dataset is captured using AR devices in diverse environments,
    including indoor and outdoor scenes with dynamic objects and varied lighting.
    It features multi-sensor data streams (images, depth, IMU, etc.) from devices
    like HoloLens 2 and iPhones/iPads, covering over 45,000 square meters. LaMAR’s
    ground-truth pipeline aligns AR trajectories against laser scans automatically,
    robustly handling data from heterogeneous devices. This benchmark is pivotal in
    evaluating AR-specific localization and mapping methods, highlighting the importance
    of considering additional data streams like radio signals in AR devices. LaMAR
    offers a realistic and comprehensive dataset for AR, guiding future research directions
    in visual localization and mapping.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: LaMAR [[224](#bib.bib224)] 涉及增强现实（AR）中的定位和映射基础技术，为现实的AR场景引入了一个新的基准。该数据集通过AR设备在各种环境中捕获，包括室内和室外场景，涵盖动态物体和不同的光照条件。它包含来自HoloLens
    2和iPhones/iPads等设备的多传感器数据流（图像、深度、IMU等），覆盖超过45,000平方米。LaMAR的地面真实管道自动对齐AR轨迹与激光扫描，能够稳健地处理来自异构设备的数据。该基准在评估AR特定的定位和映射方法中具有重要意义，强调了在AR设备中考虑附加数据流（如无线电信号）的重要性。LaMAR为AR提供了一个现实且全面的数据集，指导了未来在视觉定位和映射中的研究方向。
- en: 5.4 Optical Flow Estimation Datasets
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 光流估计数据集
- en: KITTI [[225](#bib.bib225)] is an image matching dataset collected in urban traffic
    scenarios with both a 2012 and 2015 version. KITTI-2012 consists of 194 training
    image pairs and 195 test image pairs of resolution 1226×370, while KITTI-2015
    contains 200 training image pairs and 200 test image pairs of resolution 1242×375\.
    The dataset includes sparse ground truth disparities obtained using a laser scanner.
    The scenes in KITTI-2012 are relatively simple, whereas the KITTI-2015 dataset
    presents challenges due to its dynamic scenes and complex scenarios.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: KITTI [[225](#bib.bib225)] 是一个在城市交通场景中收集的图像匹配数据集，分为2012版和2015版。KITTI-2012包含194对训练图像和195对测试图像，分辨率为1226×370，而KITTI-2015包含200对训练图像和200对测试图像，分辨率为1242×375。该数据集包括使用激光扫描仪获得的稀疏地面真实视差。KITTI-2012中的场景相对简单，而KITTI-2015数据集由于其动态场景和复杂情境带来了挑战。
- en: 5.5 Structure from Motion Datasets
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 运动结构数据集
- en: 'ETH [[226](#bib.bib226)] is a dataset designed to evaluate descriptors for
    SfM tasks by building a 3D model from a set of available 2D images. Following
    D2Net, three medium-sized datasets are evaluated: Madrid Metropolis, Gendarmenmarkt,
    and Tower of London [[227](#bib.bib227)]. ETH dataset includes various cameras
    and conditions, providing a challenging benchmark to compare different methods’
    performance.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ETH [[226](#bib.bib226)] 是一个旨在通过从一组可用的2D图像构建3D模型来评估 SfM 任务描述符的数据集。继 D2Net 之后，评估了三个中等规模的数据集：马德里大都会、亨德曼市场和伦敦塔
    [[227](#bib.bib227)]。ETH 数据集包括各种相机和条件，为比较不同方法的性能提供了具有挑战性的基准。
- en: ETH3D [[228](#bib.bib228)] is a comprehensive benchmark for multi-view stereo
    algorithms. The dataset encompasses a wide array of scenes, both indoors and outdoors,
    captured through high-resolution DSLR cameras and synchronized low-resolution
    stereo videos. What makes this dataset distinctive is its combination of high
    spatial and temporal resolution. With scenarios ranging from natural to man-made
    environments, it introduces novel challenges for detailed 3D reconstruction, with
    a specific focus on the application of hand-held mobile devices in stereo vision
    scenarios. ETH3D provides diverse evaluation protocols catering to high-resolution
    multi-view stereo, low-resolution multi-view on video data, and two-view stereo.
    Consequently, it stands as a valuable asset for advancing research in the field
    of dense 3D reconstruction.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ETH3D [[228](#bib.bib228)] 是一个全面的多视角立体算法基准数据集。该数据集涵盖了多种场景，包括室内和室外，通过高分辨率单反相机和同步低分辨率立体视频进行捕捉。该数据集的独特之处在于其高空间和时间分辨率的结合。涵盖从自然到人为环境的场景，它为详细的3D重建引入了新挑战，特别关注手持移动设备在立体视觉场景中的应用。ETH3D
    提供了多样的评估协议，涵盖高分辨率多视角立体、低分辨率多视角视频数据以及双视角立体。因此，它是推动密集3D重建领域研究的宝贵资产。
- en: 5.6 Dataset Gaps and Future Needs
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 数据集差距与未来需求
- en: Although the above datasets provide valuable resources for evaluating local
    feature matching methods, there are significant gaps that need to be addressed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述数据集为评估局部特征匹配方法提供了宝贵的资源，但仍存在需要解决的重大差距。
- en: One major gap is the lack of datasets that simulate extreme environmental conditions.
    While the presence of datasets like RoCaS [[223](#bib.bib223)] offers some variability
    in environmental conditions, including diverse weather scenarios and lighting
    conditions, there is a need for datasets that focus specifically on challenging
    weather scenarios like heavy rain, fog, or snow. These conditions pose unique
    challenges in feature matching and are crucial for applications in climate-sensitive
    areas. Another gap is the limited representation of highly dynamic environments.
    Current datasets, including the widely used HPatches [[216](#bib.bib216)], while
    comprehensive in examining variations in viewpoint and illumination, do not adequately
    capture the complexity of crowded urban areas or fast-moving scenes. This limitation
    is significant for applications requiring real-time monitoring and surveillance
    in densely populated areas. Datasets that can mimic the dynamics of such environments
    are essential for advancing feature matching techniques in these contexts. Additionally,
    there is a noticeable lack of datasets tailored for specific application domains,
    such as underwater or aerial imagery. These domains have unique characteristics
    and challenges that are not addressed by datasets like ETH [[226](#bib.bib226)]
    or Aachen Day-Night [[222](#bib.bib222)]. Specialized datasets in these areas
    would be invaluable for research and development in fields like marine biology
    or drone-based monitoring.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要的差距是缺乏模拟极端环境条件的数据集。虽然像 RoCaS [[223](#bib.bib223)] 这样的数据集提供了环境条件的一些变异性，包括多样的天气场景和光照条件，但仍需要专注于挑战性天气场景的数据集，如暴雨、雾霾或雪。这些条件在特征匹配中带来了独特的挑战，并且对于气候敏感领域的应用至关重要。另一个差距是高度动态环境的有限表示。目前的数据集，包括广泛使用的
    HPatches [[216](#bib.bib216)]，虽然在视角和光照变化的检查上非常全面，但没有充分捕捉到拥挤城市区域或快速移动场景的复杂性。这一限制对于需要实时监控和在高密度人口区域进行监视的应用来说非常重要。能够模拟这些环境动态的数据集对于推动这些背景下的特征匹配技术至关重要。此外，针对特定应用领域的数据集明显不足，例如水下或空中图像。这些领域具有独特的特性和挑战，而像
    ETH [[226](#bib.bib226)] 或 Aachen Day-Night [[222](#bib.bib222)] 这样的数据集没有解决这些问题。在这些领域中的专门数据集对于如海洋生物学或基于无人机的监控等领域的研究和发展将具有不可估量的价值。
- en: In conclusion, while existing datasets have significantly contributed to the
    field of local feature matching, there is a clear need for more specialized datasets.
    These datasets should aim to fill the existing gaps and cater to the evolving
    needs of various application domains, thereby enabling further advancements in
    local feature matching techniques.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，虽然现有的数据集对局部特征匹配领域做出了显著贡献，但仍然明显需要更多专门化的数据集。这些数据集应旨在填补现有的空白，并满足各种应用领域不断发展的需求，从而推动局部特征匹配技术的进一步发展。
- en: 6 Performance Review
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 性能评估
- en: 6.1 Metrics For Matching Models
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 匹配模型的度量
- en: 6.1.1 Image Matching
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 图像匹配
- en: 'Repeatability [[229](#bib.bib229), [52](#bib.bib52)]. The repeatability metric
    for comparing two images is computed by taking the count of matching feature areas
    found between the images and dividing it by the lesser count of feature areas
    found within either of the images, subsequently multiplying by 100 to express
    the result as a percentage. This quantitative assessment is essential for gauging
    the consistency of feature detectors when subjected to different geometric alterations:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 重复性 [[229](#bib.bib229), [52](#bib.bib52)]。比较两幅图像的重复性度量是通过计算两幅图像之间匹配特征区域的数量，并将其除以在任一图像中发现的特征区域的较小数量，然后乘以
    100，以百分比形式表示结果。这个定量评估对于衡量特征检测器在不同几何变换下的一致性至关重要。
- en: '|  | $\text{Repeatability}=\frac{M}{\min(F_{1},F_{2})}\times 100$ |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{重复性}=\frac{M}{\min(F_{1},F_{2})}\times 100$ |  |'
- en: where, $M$ denotes the number of matching feature areas between the two images,
    $F_{1}$ represents the total number of feature areas detected in the first image,
    and $F_{2}$ is the total number of feature areas detected in the second image.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$M$ 表示两幅图像之间匹配特征区域的数量，$F_{1}$ 表示在第一幅图像中检测到的特征区域总数，而 $F_{2}$ 是在第二幅图像中检测到的特征区域总数。
- en: Matching score (M-score) [[61](#bib.bib61), [78](#bib.bib78)]. The M-Score quantifies
    the effectiveness of a feature detection and description pipeline by calculating
    the average ratio of correctly matched features to the total features detected
    in the overlapping areas of two images. Mean Matching Accuracy (MMA) [[62](#bib.bib62)]
    is used to measure how well feature matchings are performed between image pairs
    considering multiple pixel error thresholds. It represents the average percentage
    of correct matches in image pairs considering several pixel error thresholds.
    The metric considers only mutually nearest neighbor matches, and matches are considered
    correct if the reprojection error, estimated using provided homography, is below
    the given matching threshold. Features and Matches [[62](#bib.bib62)] evaluate
    the performance of feature descriptors. Features refers to the average number
    of detected features per image, and Matches indicates the average number of successful
    feature matches. Percentage of Correct Keypoints (PCK) [[110](#bib.bib110)] metric
    is commonly used to measure the performance of dense matching. It involves extracting
    key points from the first image on the image grid and finding their nearest neighbors
    in the complete second image. Predicted matchings of query points are considered
    correct if they fall within a certain pixel threshold of ground truth matching.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配分数 (M-score) [[61](#bib.bib61), [78](#bib.bib78)]。M-Score 通过计算在两幅图像重叠区域中正确匹配的特征与总检测特征的平均比率，量化特征检测和描述管道的有效性。均值匹配准确率
    (MMA) [[62](#bib.bib62)] 用于测量在多个像素误差阈值下图像对之间特征匹配的表现。它表示考虑多个像素误差阈值的情况下，图像对中正确匹配的平均百分比。该度量仅考虑互为最近邻的匹配，如果使用提供的单应性估计的重投影误差低于给定的匹配阈值，则匹配被认为是正确的。特征和匹配 [[62](#bib.bib62)]
    评估特征描述符的性能。特征指每幅图像检测到的平均特征数量，匹配则指成功匹配的特征的平均数量。正确关键点百分比 (PCK) [[110](#bib.bib110)]
    度量常用于评估密集匹配的性能。它涉及在图像网格上从第一幅图像中提取关键点，并在完整的第二幅图像中找到它们的最近邻。如果查询点的预测匹配在地面真实匹配的某个像素阈值内，则被认为是正确的。
- en: 6.1.2 Homography Estimation
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 单应性估计
- en: The angular correctness metric is typically used in order to evaluate the performance
    of feature matching algorithms. The metric involves estimating the homography
    transformation $\hat{\mathcal{H}}$ between two images and comparing the transformed
    corners with those computed using the ground-truth homography $\mathcal{H}$ [[61](#bib.bib61)].
    To ensure a fair comparison among methods that produce different numbers of matches,
    a correctness identifier is calculated based on the corner error between the images
    warped with $\hat{\mathcal{H}}$ and $\mathcal{H}$. If the average error of the
    four corners is less than a specified pixel threshold $\varepsilon$, typically
    ranging from 1 to 10 pixels, the estimated homography is considered correct. Once
    the correctness of the estimated homography is established, the angle errors between
    images are evaluated using the Area Under Curve (AUC) metric. This metric calculates
    the area under the error accumulation curve at various thresholds, quantifying
    the accuracy and stability of matching. The AUC value represents the overall matching
    performance, with higher values indicating better performance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 角度正确性度量通常用于评估特征匹配算法的性能。该度量涉及估计两个图像之间的单应性变换 $\hat{\mathcal{H}}$，并将变换后的角点与使用真实单应性
    $\mathcal{H}$ 计算的角点进行比较[[61](#bib.bib61)]。为了确保对产生不同匹配数量的方法进行公平比较，根据使用 $\hat{\mathcal{H}}$
    和 $\mathcal{H}$ 变换的图像之间的角点误差计算正确性标识符。如果四个角点的平均误差小于指定的像素阈值 $\varepsilon$，通常在 1
    到 10 像素范围内，则估计的单应性被认为是正确的。一旦确认估计的单应性正确，使用曲线下面积（AUC）度量评估图像之间的角度误差。该度量计算在不同阈值下的误差积累曲线下面积，量化匹配的准确性和稳定性。AUC
    值代表整体匹配性能，值越高，性能越好。
- en: 6.1.3 Relative Pose Estimation
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 相对姿态估计
- en: When evaluating the estimated camera pose, the typical method involves measuring
    the angular deviations in rotations and translations [[230](#bib.bib230)]. In
    this method, if the angle deviation is less than a certain threshold, rotation
    or translation is considered to be correctly estimated, and the average accuracy
    at that threshold is reported. The interval between frames is represented by $d_{frame}$,
    where larger values indicate more challenging image pairs for matching. For the
    pose error at different thresholds. The most common metrics include AUC, matching
    accuracy, matching score. Among them, the maximum values of the translation error
    and the angular error are usually noted as the pose error.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估估计的相机姿态时，典型的方法涉及测量旋转和翻译中的角度偏差[[230](#bib.bib230)]。在此方法中，如果角度偏差小于某个阈值，则旋转或翻译被认为是正确估计的，并报告该阈值下的平均准确性。帧间隔表示为
    $d_{frame}$，其中较大的值表示更具挑战性的图像对进行匹配。对于不同阈值下的姿态误差，最常见的度量包括 AUC、匹配准确性、匹配得分。其中，通常注意到的姿态误差是平移误差和角度误差的最大值。
- en: 6.1.4 Visual Localization
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4 视觉定位
- en: The evaluation process typically follows the general evaluation protocol outlined
    in visual localization benchmarks¹¹1https://www.visuallocalization.net. Custom
    features are used as input to the system, and then the image registration process
    is performed using a framework such as COLMAP [[12](#bib.bib12)]. Finally, the
    percentage of images successfully positioned within a predefined tolerance range
    is calculated. In order to report the performance of the evaluated methods, the
    cumulative AUC of pose error at different threshold values is often used.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 评估过程通常遵循在视觉定位基准测试中概述的一般评估协议¹¹1https://www.visuallocalization.net。自定义特征作为系统的输入，然后使用诸如
    COLMAP 的框架执行图像配准过程[[12](#bib.bib12)]。最后，计算在预定义容差范围内成功定位的图像百分比。为了报告被评估方法的性能，通常使用不同阈值下的姿态误差累积
    AUC。
- en: 6.1.5 Optical Flow Estimation
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.5 光流估计
- en: Evaluation metrics employed for optical flow estimation encompass the Average
    End-point Error (AEPE), flow outlier ratio (Fl), and Percentage of Correct Keypoints
    (PCK) [[231](#bib.bib231), [67](#bib.bib67)]. AEPE is characterized as the mean
    Euclidean separation between the estimated and ground truth correspondence map.
    Specifically, it quantifies the Euclidean disparity between predicted and actual
    flow fields, computed as the average over valid pixels within the target image.
    Fl assesses the average percentage of outliers across all pixels, where outliers
    are defined as flow errors exceeding either 3 pixels or $5\%$ of the ground truth
    flows. PCK elucidates the percentage of appropriately matched estimated points
    $\hat{x}_{i}$ situated within a specified threshold (in pixels) from the corresponding
    ground truth points $x_{i}$.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 用于光流估计的评估指标包括平均端点误差（AEPE）、流异常值比例（Fl）和正确关键点比例（PCK）[[231](#bib.bib231), [67](#bib.bib67)]。AEPE
    是估计值与实际对应图之间的均方根欧几里得距离。具体而言，它量化了预测和实际流场之间的欧几里得差异，该值计算为目标图像中有效像素的平均值。Fl 评估所有像素中的异常值平均百分比，其中异常值定义为流误差超出
    3 像素或 $5\%$ 的真实流量。PCK 解释了在特定阈值（以像素为单位）内，估计点 $\hat{x}_{i}$ 相对于真实点 $x_{i}$ 的匹配百分比。
- en: 6.1.6 Structure from Motion
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.6 结构光流
- en: As delineated in the evaluation framework prescribed by ETH [[226](#bib.bib226)],
    a suite of pivotal metrics is employed to rigorously evaluate the fidelity of
    the reconstruction process. These encompass the number of registered images, which
    serves as an indicator of the reconstruction’s comprehensiveness, along with the
    sparse points metric, which provides insights into the depth and intricacy of
    the scene’s depiction. Furthermore, the total observations in image metric is
    pivotal for the calibration of cameras and the triangulation process, denoting
    the confirmed image projections of sparse points. The mean feature track length,
    indicative of the average count of verified image observations per sparse point,
    plays a vital role in ensuring precise calibration and robust triangulation. Lastly,
    the mean reprojection error is a critical measure for gauging the accuracy of
    the reconstruction, encapsulating the cumulative reprojection error observed in
    bundle adjustment and influenced by the thoroughness of input data as well as
    the precision of keypoint detection.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如 ETH [[226](#bib.bib226)] 规定的评估框架所述，一套关键指标被用来严格评估重建过程的真实性。这些指标包括注册图像的数量，作为重建全面性的指标，以及稀疏点指标，提供对场景深度和复杂度的洞察。此外，图像中总观察量指标对于相机校准和三角测量过程至关重要，表示稀疏点的确认图像投影。平均特征跟踪长度，指示每个稀疏点的验证图像观察的平均数量，在确保准确校准和稳健三角测量中发挥重要作用。最后，平均重投影误差是衡量重建准确性的重要指标，综合了在束调整中观察到的累积重投影误差，受输入数据的完整性以及关键点检测精度的影响。
- en: The key metrics in ETH3D [[228](#bib.bib228)] are crucial for evaluating the
    effectiveness of various SfM methods. The AUC of pose error at different thresholds
    is used to assess the accuracy of multi-view camera pose estimation. This metric
    reflects the precision of estimated camera poses in relation to ground truth.
    Accuracy and Completeness percentages at different distance thresholds evaluate
    the 3D Triangulation task. Accuracy indicates the proportion of reconstructed
    points within a certain distance from the ground truth, and Completeness measures
    the percentage of ground truth points that are adequately represented within the
    reconstructed point cloud.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ETH3D 的关键指标[[228](#bib.bib228)] 对评估各种 SfM 方法的有效性至关重要。不同阈值下的姿态误差的 AUC 用于评估多视角相机姿态估计的准确性。该指标反映了估计相机姿态相对于真实值的精度。在不同距离阈值下的准确度和完整度百分比评估
    3D 三角测量任务。准确度表示重建点与真实值之间的距离比例，完整度测量在重建点云中适当表示的真实点百分比。
- en: 6.2 Quantitative Performance
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 定量性能
- en: 'In this section, we analyze the performance of several key methods in terms
    of the evaluation scores provided in Section [6.1](#S6.SS1 "6.1 Metrics For Matching
    Models ‣ 6 Performance Review ‣ Local Feature Matching Using Deep Learning: A
    Survey"), which includes various algorithms discussed earlier and additional methods.
    We compile their performances on popular benchmarks into tables, where the data
    is sourced either from the original authors or from the best reported results
    of other authors under the same evaluation conditions. Furthermore, some publications
    may report performances on non-standard benchmarks/databases or only involve certain
    subsets of popular benchmark test sets. We do not present the performance of these
    methods.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们分析了几个关键方法的性能，这些方法的评估分数在第[6.1](#S6.SS1 "6.1 Metrics For Matching Models
    ‣ 6 Performance Review ‣ Local Feature Matching Using Deep Learning: A Survey")节中提供，该节涵盖了之前讨论的各种算法及其他方法。我们将它们在流行基准测试上的表现汇总成表格，其中数据来源于原作者或在相同评估条件下其他作者报告的最佳结果。此外，一些出版物可能报告了在非标准基准/数据库上的表现，或者仅涉及某些流行基准测试集的子集。我们不展示这些方法的性能。'
- en: 'The following tables provide summaries of several major DL-based matching models’
    performances on different datasets. Table [1](#S6.T1 "Table 1 ‣ 6.2 Quantitative
    Performance ‣ 6 Performance Review ‣ Local Feature Matching Using Deep Learning:
    A Survey") highlights the HPatches [[216](#bib.bib216)] test set, adopting the
    evaluation protocol utilized by the LoFTR approach [[72](#bib.bib72)]. Performance
    metrics are based on the AUC of corner error distances up to 3, 5, and 10 pixels.
    Table [2](#S6.T2 "Table 2 ‣ 6.2 Quantitative Performance ‣ 6 Performance Review
    ‣ Local Feature Matching Using Deep Learning: A Survey") focuses on the ScanNet [[217](#bib.bib217)]
    test set, following the SuperGlue [[69](#bib.bib69)] testing protocol. The reported
    metric is the pose AUC error. Table [3](#S6.T3 "Table 3 ‣ 6.2 Quantitative Performance
    ‣ 6 Performance Review ‣ Local Feature Matching Using Deep Learning: A Survey")
    centers on the YFCC100M [[218](#bib.bib218)] test set, with a protocol based on
    RANSAC-Flow [[232](#bib.bib232)]. Additionally, the pose mAP (mean Average Precision)
    value is reported. A pose estimation is considered an outlier if its maximum degree
    error in translation or rotation exceeds the threshold. Table [4](#S6.T4 "Table
    4 ‣ 6.2 Quantitative Performance ‣ 6 Performance Review ‣ Local Feature Matching
    Using Deep Learning: A Survey") highlights the MegaDepth [[219](#bib.bib219)]
    test set. The pose estimation AUC error is reported, following the SuperGlue [[69](#bib.bib69)]
    evaluation methodology. Tables [5](#S6.T5 "Table 5 ‣ 6.2 Quantitative Performance
    ‣ 6 Performance Review ‣ Local Feature Matching Using Deep Learning: A Survey")
    and [6](#S6.T6 "Table 6 ‣ 6.2 Quantitative Performance ‣ 6 Performance Review
    ‣ Local Feature Matching Using Deep Learning: A Survey") emphasize the Aachen
    Day-Night v1.0 [[222](#bib.bib222)] and v1.1 [[9](#bib.bib9)] test sets, respectively,
    in the local feature evaluation track and the full visual localization track.
    Table [7](#S6.T7 "Table 7 ‣ 6.2 Quantitative Performance ‣ 6 Performance Review
    ‣ Local Feature Matching Using Deep Learning: A Survey") focuses on the InLoc [[115](#bib.bib115)]
    test set. The reported metrics include the percentage of correctly localized queries
    under specific error thresholds, following the HLoc [[233](#bib.bib233)] pipeline.
    Table [8](#S6.T8 "Table 8 ‣ 6.2 Quantitative Performance ‣ 6 Performance Review
    ‣ Local Feature Matching Using Deep Learning: A Survey") emphasizes the KITTI [[225](#bib.bib225)]
    test set. The AEPE and the flow outlier ratio Fl are reported for both the 2012
    and 2015 versions of the KITTI dataset. Table [9](#S6.T9 "Table 9 ‣ 6.2 Quantitative
    Performance ‣ 6 Performance Review ‣ Local Feature Matching Using Deep Learning:
    A Survey") focuses on the ETH3D [[228](#bib.bib228)], presenting a detailed evaluation
    of various SfM methods as reported in the DetectorFreeSfM [[179](#bib.bib179)].
    This evaluation thoroughly examines the effectiveness of these methods across
    three crucial metrics: AUC, Accuracy, and Completeness.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了几种主要基于深度学习的匹配模型在不同数据集上的表现。表格[1](#S6.T1 "表1 ‣ 6.2 定量性能 ‣ 6 性能回顾 ‣ 基于深度学习的局部特征匹配：一项调查")突出显示了HPatches [[216](#bib.bib216)]测试集，采用了LoFTR方法 [[72](#bib.bib72)]使用的评估协议。性能指标基于角点误差距离的AUC，分别为3、5和10像素。表格[2](#S6.T2
    "表2 ‣ 6.2 定量性能 ‣ 6 性能回顾 ‣ 基于深度学习的局部特征匹配：一项调查")关注ScanNet [[217](#bib.bib217)]测试集，遵循SuperGlue [[69](#bib.bib69)]测试协议。报告的指标是姿态AUC误差。表格[3](#S6.T3
    "表3 ‣ 6.2 定量性能 ‣ 6 性能回顾 ‣ 基于深度学习的局部特征匹配：一项调查")集中于YFCC100M [[218](#bib.bib218)]测试集，采用基于RANSAC-Flow [[232](#bib.bib232)]的协议。此外，还报告了姿态mAP（均值平均精度）值。如果姿态估计的最大平移或旋转误差超过阈值，则被认为是离群值。表格[4](#S6.T4
    "表4 ‣ 6.2 定量性能 ‣ 6 性能回顾 ‣ 基于深度学习的局部特征匹配：一项调查")突出显示了MegaDepth [[219](#bib.bib219)]测试集。报告了姿态估计AUC误差，遵循SuperGlue [[69](#bib.bib69)]评估方法。表格[5](#S6.T5
    "表5 ‣ 6.2 定量性能 ‣ 6 性能回顾 ‣ 基于深度学习的局部特征匹配：一项调查")和[6](#S6.T6 "表6 ‣ 6.2 定量性能 ‣ 6 性能回顾
    ‣ 基于深度学习的局部特征匹配：一项调查")分别强调了Aachen Day-Night v1.0 [[222](#bib.bib222)]和v1.1 [[9](#bib.bib9)]测试集，在局部特征评估轨道和完整视觉定位轨道中。表格[7](#S6.T7
    "表7 ‣ 6.2 定量性能 ‣ 6 性能回顾 ‣ 基于深度学习的局部特征匹配：一项调查")关注InLoc [[115](#bib.bib115)]测试集。报告的指标包括在特定误差阈值下正确定位的查询百分比，遵循HLoc [[233](#bib.bib233)]管道。表格[8](#S6.T8
    "表8 ‣ 6.2 定量性能 ‣ 6 性能回顾 ‣ 基于深度学习的局部特征匹配：一项调查")强调了KITTI [[225](#bib.bib225)]测试集。报告了AEPE和流动离群值比例Fl，适用于KITTI数据集的2012年和2015年版本。表格[9](#S6.T9
    "表9 ‣ 6.2 定量性能 ‣ 6 性能回顾 ‣ 基于深度学习的局部特征匹配：一项调查")聚焦于ETH3D [[228](#bib.bib228)]，展示了各种SfM方法的详细评估，如在DetectorFreeSfM [[179](#bib.bib179)]中所报告的。这项评估深入探讨了这些方法在三个关键指标上的效果：AUC、准确性和完整性。
- en: 'Table 1: Evaluation on HPatches  [[216](#bib.bib216)] for homography estimation.
    We compare with two groups of the methods, Detector-based and Detector-free methods.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 在 HPatches [[216](#bib.bib216)] 上进行同质性估计的评估。我们比较了两组方法，基于检测器的方法和无检测器方法。'
- en: '| Category | Methods | Pose Estimation AUC↑ | matches |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 姿态估计 AUC↑ | 匹配数 |'
- en: '| @3px | @5px | @10px |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| @3px | @5px | @10px |'
- en: '| Detector-based | D2Net [[62](#bib.bib62)]+NN | 23.2 | 35.9 | 53.6 | 0.2K
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 基于检测器 | D2Net [[62](#bib.bib62)]+NN | 23.2 | 35.9 | 53.6 | 0.2K |'
- en: '| R2D2 [[63](#bib.bib63)]+NN | 50.6 | 63.9 | 76.8 | 0.5K |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| R2D2 [[63](#bib.bib63)]+NN | 50.6 | 63.9 | 76.8 | 0.5K |'
- en: '| DISK [[111](#bib.bib111)]+NN | 52.3 | 64.9 | 78.9 | 1.1K |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| DISK [[111](#bib.bib111)]+NN | 52.3 | 64.9 | 78.9 | 1.1K |'
- en: '| SP+GFM [[234](#bib.bib234)] | 51.9 | 65.8 | 79.1 | 2.0k |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| SP+GFM [[234](#bib.bib234)] | 51.9 | 65.8 | 79.1 | 2.0k |'
- en: '| SP+SuperGlue [[69](#bib.bib69)] | 53.9 | 68.3 | 81.7 | 0.6K |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| SP+SuperGlue [[69](#bib.bib69)] | 53.9 | 68.3 | 81.7 | 0.6K |'
- en: '| Detector-free | COTR [[154](#bib.bib154)] | 41.9 | 57.7 | 74.0 | 1.0K |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 无检测器 | COTR [[154](#bib.bib154)] | 41.9 | 57.7 | 74.0 | 1.0K |'
- en: '| Sparse-NCNet [[65](#bib.bib65)] | 48.9 | 54.2 | 67.1 | 1.0K |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Sparse-NCNet [[65](#bib.bib65)] | 48.9 | 54.2 | 67.1 | 1.0K |'
- en: '| DualRC-Net [[66](#bib.bib66)] | 50.6 | 56.2 | 68.3 | 1.0K |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| DualRC-Net [[66](#bib.bib66)] | 50.6 | 56.2 | 68.3 | 1.0K |'
- en: '| Patch2Pix [[169](#bib.bib169)] | 59.3 | 70.6 | 81.2 | 0.7K |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Patch2Pix [[169](#bib.bib169)] | 59.3 | 70.6 | 81.2 | 0.7K |'
- en: '| 3DG-STFM [[235](#bib.bib235)] | 64.7 | 73.1 | 81.0 | 1.0k |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 3DG-STFM [[235](#bib.bib235)] | 64.7 | 73.1 | 81.0 | 1.0k |'
- en: '| LoFTR [[72](#bib.bib72)] | 65.9 | 75.6 | 84.6 | 1.0K |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| LoFTR [[72](#bib.bib72)] | 65.9 | 75.6 | 84.6 | 1.0K |'
- en: '| SE2-LoFTR [[156](#bib.bib156)] | 66.2 | 76.6 | 86.0 | — |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| SE2-LoFTR [[156](#bib.bib156)] | 66.2 | 76.6 | 86.0 | — |'
- en: '| QuadTree [[157](#bib.bib157)] | 66.3 | 76.2 | 84.9 | 2.7k |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| QuadTree [[157](#bib.bib157)] | 66.3 | 76.2 | 84.9 | 2.7k |'
- en: '| PDC-Net+ [[143](#bib.bib143)] | 66.7 | 76.8 | 85.8 | 1.0k |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| PDC-Net+ [[143](#bib.bib143)] | 66.7 | 76.8 | 85.8 | 1.0k |'
- en: '| TopicFM [[161](#bib.bib161)] | 67.3 | 77.0 | 85.7 | 1.0K |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| TopicFM [[161](#bib.bib161)] | 67.3 | 77.0 | 85.7 | 1.0K |'
- en: '| ASpanFormer [[73](#bib.bib73)] | 67.4 | 76.9 | 85.6 | — |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ASpanFormer [[73](#bib.bib73)] | 67.4 | 76.9 | 85.6 | — |'
- en: '| SEM [[166](#bib.bib166)] | 69.6 | 79.0 | 87.1 | 1.0K |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| SEM [[166](#bib.bib166)] | 69.6 | 79.0 | 87.1 | 1.0K |'
- en: '| CasMTR-2c [[164](#bib.bib164)] | 71.4 | 80.2 | 87.9 | 0.5k |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| CasMTR-2c [[164](#bib.bib164)] | 71.4 | 80.2 | 87.9 | 0.5k |'
- en: '| DKM [[167](#bib.bib167)] | 71.3 | 80.6 | 88.5 | 5.0K |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| DKM [[167](#bib.bib167)] | 71.3 | 80.6 | 88.5 | 5.0K |'
- en: '| ASTR [[162](#bib.bib162)] | 71.7 | 80.3 | 88.0 | 1.0K |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| ASTR [[162](#bib.bib162)] | 71.7 | 80.3 | 88.0 | 1.0K |'
- en: '|  | PMatch [[165](#bib.bib165)] | 71.9 | 80.7 | 88.5 | — |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| | PMatch [[165](#bib.bib165)] | 71.9 | 80.7 | 88.5 | — |'
- en: '|  | RoMa [[168](#bib.bib168)] | 72.2 | 81.2 | 89.1 | — |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| | RoMa [[168](#bib.bib168)] | 72.2 | 81.2 | 89.1 | — |'
- en: 'Table 2: ScanNet [[217](#bib.bib217)] Two-View Camera Pose Estimation. We compare
    with two groups of the methods, Detector-based and Detector-free methods.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: ScanNet [[217](#bib.bib217)] 两视图相机姿态估计。我们比较了两组方法，基于检测器的方法和无检测器方法。'
- en: '| Category | Methods | Pose Estimation AUC↑ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 姿态估计 AUC↑ |'
- en: '| @5° | @10° | @20° |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| @5° | @10° | @20° |'
- en: '| Detector-based | ORB+GMS [[76](#bib.bib76)] | 5.2 | 13.7 | 25.4 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 基于检测器 | ORB+GMS [[76](#bib.bib76)] | 5.2 | 13.7 | 25.4 |'
- en: '| D2Net [[62](#bib.bib62)]+NN | 5.3 | 14.5 | 28.0 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| D2Net [[62](#bib.bib62)]+NN | 5.3 | 14.5 | 28.0 |'
- en: '| ContextDesc+RT [[100](#bib.bib100)] | 6.6 | 15.0 | 25.8 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| ContextDesc+RT [[100](#bib.bib100)] | 6.6 | 15.0 | 25.8 |'
- en: '| ContextDesc+NN [[100](#bib.bib100)] | 9.4 | 21.5 | 36.4 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| ContextDesc+NN [[100](#bib.bib100)] | 9.4 | 21.5 | 36.4 |'
- en: '| SP+NN [[61](#bib.bib61)] | 9.4 | 21.5 | 36.4 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| SP+NN [[61](#bib.bib61)] | 9.4 | 21.5 | 36.4 |'
- en: '| SP+PointCN [[236](#bib.bib236)] | 11.4 | 25.5 | 41.4 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| SP+PointCN [[236](#bib.bib236)] | 11.4 | 25.5 | 41.4 |'
- en: '| SP+HTMatch [[138](#bib.bib138)] | 15.1 | 31.4 | 48.2 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| SP+HTMatch [[138](#bib.bib138)] | 15.1 | 31.4 | 48.2 |'
- en: '| SP+SGMNet [[70](#bib.bib70)] | 15.4 | 32.1 | 48.3 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| SP+SGMNet [[70](#bib.bib70)] | 15.4 | 32.1 | 48.3 |'
- en: '| ContextDesc+SGMNet [[70](#bib.bib70)] | 15.4 | 32.3 | 48.8 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| ContextDesc+SGMNet [[70](#bib.bib70)] | 15.4 | 32.3 | 48.8 |'
- en: '| SP+SuperGlue [[69](#bib.bib69)] | 16.2 | 33.8 | 51.8 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| SP+SuperGlue [[69](#bib.bib69)] | 16.2 | 33.8 | 51.8 |'
- en: '| SP+DenseGAP [[137](#bib.bib137)] | 17.0 | 36.1 | 55.7 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| SP+DenseGAP [[137](#bib.bib137)] | 17.0 | 36.1 | 55.7 |'
- en: '| Detector-free | DualRC-Net [[66](#bib.bib66)] | 7.7 | 17.9 | 30.5 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 无检测器 | DualRC-Net [[66](#bib.bib66)] | 7.7 | 17.9 | 30.5 |'
- en: '| SEM [[166](#bib.bib166)] | 18.7 | 36.6 | 52.9 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| SEM [[166](#bib.bib166)] | 18.7 | 36.6 | 52.9 |'
- en: '| PDC-Net(H) [[68](#bib.bib68)] | 18.7 | 37.0 | 54.0 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| PDC-Net(H) [[68](#bib.bib68)] | 18.7 | 37.0 | 54.0 |'
- en: '| PDC-Net+(H) [[143](#bib.bib143)] | 20.3 | 39.4 | 57.1 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| PDC-Net+(H) [[143](#bib.bib143)] | 20.3 | 39.4 | 57.1 |'
- en: '| LoFTR-DT [[72](#bib.bib72)] | 22.1 | 40.8 | 57.6 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| LoFTR-DT [[72](#bib.bib72)] | 22.1 | 40.8 | 57.6 |'
- en: '| 3DG-STFM [[235](#bib.bib235)] | 23.6 | 43.6 | 61.2 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 3DG-STFM [[235](#bib.bib235)] | 23.6 | 43.6 | 61.2 |'
- en: '| LoFTR [[72](#bib.bib72)]+QuadTree [[157](#bib.bib157)] | 23.9 | 43.2 | 60.3
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| LoFTR [[72](#bib.bib72)]+QuadTree [[157](#bib.bib157)] | 23.9 | 43.2 | 60.3
    |'
- en: '| MatchFormer [[159](#bib.bib159)] | 24.3 | 43.9 | 61.4 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| MatchFormer [[159](#bib.bib159)] | 24.3 | 43.9 | 61.4 |'
- en: '| QuadTree [[157](#bib.bib157)] | 24.9 | 44.7 | 61.8 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| QuadTree [[157](#bib.bib157)] | 24.9 | 44.7 | 61.8 |'
- en: '| ASpanFormer [[73](#bib.bib73)] | 25.6 | 46.0 | 63.3 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| ASpanFormer [[73](#bib.bib73)] | 25.6 | 46.0 | 63.3 |'
- en: '| OAMatcher [[163](#bib.bib163)] | 26.1 | 45.3 | 62.1 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| OAMatcher [[163](#bib.bib163)] | 26.1 | 45.3 | 62.1 |'
- en: '| PATS [[171](#bib.bib171)] | 26.0 | 46.9 | 64.3 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| PATS [[171](#bib.bib171)] | 26.0 | 46.9 | 64.3 |'
- en: '| CasMTR-4c [[164](#bib.bib164)] | 27.1 | 47.0 | 64.4 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| CasMTR-4c [[164](#bib.bib164)] | 27.1 | 47.0 | 64.4 |'
- en: '| DeepMatcher-L [[141](#bib.bib141)] | 27.3 | 46.3 | 62.5 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| DeepMatcher-L [[141](#bib.bib141)] | 27.3 | 46.3 | 62.5 |'
- en: '| PMatch [[165](#bib.bib165)] | 29.4 | 50.1 | 67.4 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| PMatch [[165](#bib.bib165)] | 29.4 | 50.1 | 67.4 |'
- en: '| DKM [[167](#bib.bib167)] | 29.4 | 50.7 | 68.3 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| DKM [[167](#bib.bib167)] | 29.4 | 50.7 | 68.3 |'
- en: '|  | RoMa [[168](#bib.bib168)] | 31.8 | 53.4 | 70.9 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | RoMa [[168](#bib.bib168)] | 31.8 | 53.4 | 70.9 |'
- en: 'Table 3: Evaluation on YFCC100M  [[218](#bib.bib218)] for outdoor pose estimation.
    We compare with two groups of the methods, Detector-based and Detector-free methods.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 在 YFCC100M  [[218](#bib.bib218)] 上进行的户外姿态估计评估。我们比较了两组方法，基于检测器的方法和无检测器的方法。'
- en: '| Category | Methods | Pose estimation AUC↑ | Pose estimation mAP↑ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 方法 | 姿态估计 AUC↑ | 姿态估计 mAP↑ |'
- en: '| @5° | @10° | @20° | @5° | @10° | @20° |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| @5° | @10° | @20° | @5° | @10° | @20° |'
- en: '| Detector-based | SuperPoint(SP) [[61](#bib.bib61)] | — | — | — | 30.5 | 50.8
    | 67.9 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 基于检测器 | SuperPoint(SP) [[61](#bib.bib61)] | — | — | — | 30.5 | 50.8 | 67.9
    |'
- en: '| SIFT [[17](#bib.bib17)]+RT | 24.1 | 40.7 | 58.1 | 45.1 | 55.8 | 67.2 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| SIFT [[17](#bib.bib17)]+RT | 24.1 | 40.7 | 58.1 | 45.1 | 55.8 | 67.2 |'
- en: '| SP+OANet [[77](#bib.bib77)] | 26.8 | 45.0 | 62.2 | 50.9 | 61.4 | 71.8 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| SP+OANet [[77](#bib.bib77)] | 26.8 | 45.0 | 62.2 | 50.9 | 61.4 | 71.8 |'
- en: '| SIFT+OANet [[77](#bib.bib77)] | 29.2 | 48.1 | 65.1 | 55.1 | 65.0 | 74.8 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| SIFT+OANet [[77](#bib.bib77)] | 29.2 | 48.1 | 65.1 | 55.1 | 65.0 | 74.8 |'
- en: '| CoAM [[237](#bib.bib237)] | — | — | — | 55.6 | 66.8 | — |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| CoAM [[237](#bib.bib237)] | — | — | — | 55.6 | 66.8 | — |'
- en: '| SIFT+SuperGlue [[69](#bib.bib69)] | 30.5 | 51.3 | 69.7 | 59.3 | 70.4 | 80.4
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| SIFT+SuperGlue [[69](#bib.bib69)] | 30.5 | 51.3 | 69.7 | 59.3 | 70.4 | 80.4
    |'
- en: '| Paraformer [[139](#bib.bib139)] | 31.7 | 52.3 | 70.4 | 60.1 | 70.7 | 78.1
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Paraformer [[139](#bib.bib139)] | 31.7 | 52.3 | 70.4 | 60.1 | 70.7 | 78.1
    |'
- en: '| RootSIFT [[238](#bib.bib238)]+SGMNet [[70](#bib.bib70)] | 35.5 | 55.2 | 71.9
    | — | — | — |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| RootSIFT [[238](#bib.bib238)]+SGMNet [[70](#bib.bib70)] | 35.5 | 55.2 | 71.9
    | — | — | — |'
- en: '| SP+SuperGlue [[69](#bib.bib69)] | 38.7 | 59.1 | 75.8 | 67.8 | 77.4 | 85.7
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| SP+SuperGlue [[69](#bib.bib69)] | 38.7 | 59.1 | 75.8 | 67.8 | 77.4 | 85.7
    |'
- en: '| Detector-free | DualRC-Net [[66](#bib.bib66)] | 29.5 | 50.1 | 66.8 | — |
    — | — |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 无检测器 | DualRC-Net [[66](#bib.bib66)] | 29.5 | 50.1 | 66.8 | — | — | — |'
- en: '| RANSAC-Flow [[232](#bib.bib232)] | — | — | — | 64.9 | 73.3 | 81.6 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| RANSAC-Flow [[232](#bib.bib232)] | — | — | — | 64.9 | 73.3 | 81.6 |'
- en: '| PDC-Net(MS) [[68](#bib.bib68)] | 35.7 | 55.8 | 72.3 | 63.9 | 73.0 | 73.0
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| PDC-Net(MS) [[68](#bib.bib68)] | 35.7 | 55.8 | 72.3 | 63.9 | 73.0 | 73.0
    |'
- en: '| PDC-Net+(H) [[143](#bib.bib143)] | 37.5 | 58.1 | 74.5 | 67.4 | 76.6 | 84.6
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| PDC-Net+(H) [[143](#bib.bib143)] | 37.5 | 58.1 | 74.5 | 67.4 | 76.6 | 84.6
    |'
- en: '| LoFTR [[72](#bib.bib72)] | 42.4 | 62.5 | 77.3 | — | — | — |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| LoFTR [[72](#bib.bib72)] | 42.4 | 62.5 | 77.3 | — | — | — |'
- en: '| ASpanFormer [[73](#bib.bib73)] | 44.5 | 63.8 | 78.4 | — | — | — |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| ASpanFormer [[73](#bib.bib73)] | 44.5 | 63.8 | 78.4 | — | — | — |'
- en: '| PMatch [[165](#bib.bib165)] | 45.7 | 65.2 | 79.8 | 75.9 | 83.1 | 89.3 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| PMatch [[165](#bib.bib165)] | 45.7 | 65.2 | 79.8 | 75.9 | 83.1 | 89.3 |'
- en: '| PATS [[171](#bib.bib171)] | 47.0 | 65.3 | 79.2 | — | — | — |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| PATS [[171](#bib.bib171)] | 47.0 | 65.3 | 79.2 | — | — | — |'
- en: 'Table 4: MegaDepth [[219](#bib.bib219)] Two-View Camera Pose Estimation. We
    compare with two groups of the methods, Detector-based and Detector-free methods.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: MegaDepth [[219](#bib.bib219)] 两视图相机姿态估计。我们比较了两组方法，基于检测器的方法和无检测器的方法。'
- en: '| Category | Methods | Pose Estimation AUC↑ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 方法 | 姿态估计 AUC↑ |'
- en: '| @5° | @10° | @20° |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| @5° | @10° | @20° |'
- en: '| Detector-based | SP+SGMNet [[70](#bib.bib70)] | 40.5 | 59.0 | 72.6 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 基于检测器 | SP+SGMNet [[70](#bib.bib70)] | 40.5 | 59.0 | 72.6 |'
- en: '| SP+DenseGAP [[137](#bib.bib137)] | 41.2 | 56.9 | 70.2 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| SP+DenseGAP [[137](#bib.bib137)] | 41.2 | 56.9 | 70.2 |'
- en: '| SP+SuperGlue [[69](#bib.bib69)] | 42.2 | 61.2 | 75.9 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| SP+SuperGlue [[69](#bib.bib69)] | 42.2 | 61.2 | 75.9 |'
- en: '| SP+ClusterGNN [[71](#bib.bib71)] | 44.2 | 58.5 | 70.3 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| SP+ClusterGNN [[71](#bib.bib71)] | 44.2 | 58.5 | 70.3 |'
- en: '| Detector-free | Patch2Pix [[169](#bib.bib169)] | 41.4 | 56.3 | 68.3 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 无检测器 | Patch2Pix [[169](#bib.bib169)] | 41.4 | 56.3 | 68.3 |'
- en: '| ECO-TR [[155](#bib.bib155)] | 48.3 | 65.8 | 78.5 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| ECO-TR [[155](#bib.bib155)] | 48.3 | 65.8 | 78.5 |'
- en: '| PDC-Net+ [[143](#bib.bib143)] | 51.5 | 67.2 | 78.5 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| PDC-Net+ [[143](#bib.bib143)] | 51.5 | 67.2 | 78.5 |'
- en: '| 3DG-STFM [[235](#bib.bib235)] | 52.6 | 68.5 | 80.0 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 3DG-STFM [[235](#bib.bib235)] | 52.6 | 68.5 | 80.0 |'
- en: '| SE2-LoFTR [[156](#bib.bib156)] | 52.6 | 69.2 | 81.4 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| SE2-LoFTR [[156](#bib.bib156)] | 52.6 | 69.2 | 81.4 |'
- en: '| LoFTR [[72](#bib.bib72)] | 52.8 | 69.2 | 81.2 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| LoFTR [[72](#bib.bib72)] | 52.8 | 69.2 | 81.2 |'
- en: '| MatchFormer [[159](#bib.bib159)] | 52.9 | 69.7 | 82.0 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| MatchFormer [[159](#bib.bib159)] | 52.9 | 69.7 | 82.0 |'
- en: '| TopicFM [[161](#bib.bib161)] | 54.1 | 70.1 | 81.6 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| TopicFM [[161](#bib.bib161)] | 54.1 | 70.1 | 81.6 |'
- en: '| QuadTree [[157](#bib.bib157)] | 54.6 | 70.5 | 82.2 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| QuadTree [[157](#bib.bib157)] | 54.6 | 70.5 | 82.2 |'
- en: '| ASpanFormer [[73](#bib.bib73)] | 55.3 | 71.5 | 83.1 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| ASpanFormer [[73](#bib.bib73)] | 55.3 | 71.5 | 83.1 |'
- en: '| OAMatcher [[163](#bib.bib163)] | 56.6 | 72.3 | 83.6 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| OAMatcher [[163](#bib.bib163)] | 56.6 | 72.3 | 83.6 |'
- en: '| DeepMatcher-L [[141](#bib.bib141)] | 57.0 | 73.1 | 84.2 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| DeepMatcher-L [[141](#bib.bib141)] | 57.0 | 73.1 | 84.2 |'
- en: '| SEM [[166](#bib.bib166)] | 58.0 | 72.9 | 83.7 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| SEM [[166](#bib.bib166)] | 58.0 | 72.9 | 83.7 |'
- en: '| ASTR [[162](#bib.bib162)] | 58.4 | 73.1 | 83.8 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| ASTR [[162](#bib.bib162)] | 58.4 | 73.1 | 83.8 |'
- en: '| CasMTR-2c [[164](#bib.bib164)] | 59.1 | 74.3 | 84.8 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| CasMTR-2c [[164](#bib.bib164)] | 59.1 | 74.3 | 84.8 |'
- en: '| DKM [[167](#bib.bib167)] | 60.4 | 74.9 | 85.1 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| DKM [[167](#bib.bib167)] | 60.4 | 74.9 | 85.1 |'
- en: '|  | PMatch [[165](#bib.bib165)] | 61.4 | 75.7 | 85.7 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | PMatch [[165](#bib.bib165)] | 61.4 | 75.7 | 85.7 |'
- en: '|  | RoMa [[168](#bib.bib168)] | 62.6 | 76.7 | 86.3 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | RoMa [[168](#bib.bib168)] | 62.6 | 76.7 | 86.3 |'
- en: 'Table 5: Visual localization evaluation on the Aachen Day-Night benchmark v1.0 [[222](#bib.bib222)]
    and v1.1 [[9](#bib.bib9)]. The evaluation results on the full visual localization
    track are reported. We compare with two groups of the methods, Detector-based
    and Detector-free methods.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: Aachen Day-Night 基准测试 v1.0 [[222](#bib.bib222)] 和 v1.1 [[9](#bib.bib9)]
    的视觉定位评估。报告了全视觉定位跟踪的评估结果。我们与两组方法进行了比较，即基于检测器的方法和无检测器的方法。'
- en: '| Dataset | Category | Methods | Day | Night |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类别 | 方法 | 白天 | 晚上 |'
- en: '| (0.25m,2°) | (0.5m,5°) | (1.0m,10°) | (0.25m,2°) | (0.5m,5°) | (1.0m,10°)
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| (0.25m,2°) | (0.5m,5°) | (1.0m,10°) | (0.25m,2°) | (0.5m,5°) | (1.0m,10°)
    |'
- en: '| v1.0 | Detector-based | SP+NN [[61](#bib.bib61)] | 85.4 | 93.3 | 97.2 | 75.5
    | 86.7 | 92.9 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| v1.0 | 基于检测器 | SP+NN [[61](#bib.bib61)] | 85.4 | 93.3 | 97.2 | 75.5 | 86.7
    | 92.9 |'
- en: '| SP+CAPS [[110](#bib.bib110)]+NN | 86.3 | 93.0 | 95.9 | 83.7 | 90.8 | 96.9
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| SP+CAPS [[110](#bib.bib110)]+NN | 86.3 | 93.0 | 95.9 | 83.7 | 90.8 | 96.9
    |'
- en: '| SP+SuperGlue [[69](#bib.bib69)] | 89.6 | 95.4 | 98.8 | 86.7 | 93.9 | 100.0
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| SP+SuperGlue [[69](#bib.bib69)] | 89.6 | 95.4 | 98.8 | 86.7 | 93.9 | 100.0
    |'
- en: '| SP+SGMNet [[70](#bib.bib70)] | 86.8 | 94.2 | 97.7 | 83.7 | 91.8 | 99.0 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| SP+SGMNet [[70](#bib.bib70)] | 86.8 | 94.2 | 97.7 | 83.7 | 91.8 | 99.0 |'
- en: '| SP+ClusterGNN [[71](#bib.bib71)] | 89.4 | 95.5 | 98.5 | 81.6 | 93.9 | 100.0
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| SP+ClusterGNN [[71](#bib.bib71)] | 89.4 | 95.5 | 98.5 | 81.6 | 93.9 | 100.0
    |'
- en: '| SP+LightGlue [[136](#bib.bib136)] | 89.2 | 95.4 | 98.5 | 87.8 | 93.9 | 100.0
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| SP+LightGlue [[136](#bib.bib136)] | 89.2 | 95.4 | 98.5 | 87.8 | 93.9 | 100.0
    |'
- en: '| ASLFeat [[126](#bib.bib126)]+NN | 82.3 | 89.2 | 92.7 | 67.3 | 79.6 | 85.7
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[126](#bib.bib126)]+NN | 82.3 | 89.2 | 92.7 | 67.3 | 79.6 | 85.7
    |'
- en: '| ASLFeat [[126](#bib.bib126)]+SGMNet [[70](#bib.bib70)] | 86.8 | 93.4 | 97.1
    | 86.7 | 94.9 | 98.0 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[126](#bib.bib126)]+SGMNet [[70](#bib.bib70)] | 86.8 | 93.4 | 97.1
    | 86.7 | 94.9 | 98.0 |'
- en: '| ASLFeat [[126](#bib.bib126)]+SuperGlue [[69](#bib.bib69)] | 87.9 | 95.4 |
    98.3 | 81.6 | 91.8 | 99.0 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[126](#bib.bib126)]+SuperGlue [[69](#bib.bib69)] | 87.9 | 95.4 |
    98.3 | 81.6 | 91.8 | 99.0 |'
- en: '| ASLFeat [[126](#bib.bib126)]+ClusterGNN [[71](#bib.bib71)] | 88.6 | 95.5
    | 98.4 | 85.7 | 93.9 | 99.0 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[126](#bib.bib126)]+ClusterGNN [[71](#bib.bib71)] | 88.6 | 95.5
    | 98.4 | 85.7 | 93.9 | 99.0 |'
- en: '| Detector-free | Patch2Pix [[169](#bib.bib169)] | 84.6 | 92.1 | 96.5 | 82.7
    | 92.9 | 99.0 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 无检测器 | Patch2Pix [[169](#bib.bib169)] | 84.6 | 92.1 | 96.5 | 82.7 | 92.9
    | 99.0 |'
- en: '| v1.1 | Detector-based | ISRF [[239](#bib.bib239)] | 87.1 | 94.7 | 98.3 |
    74.3 | 86.9 | 97.4 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| v1.1 | 基于检测器 | ISRF [[239](#bib.bib239)] | 87.1 | 94.7 | 98.3 | 74.3 | 86.9
    | 97.4 |'
- en: '| Rlocs [[240](#bib.bib240)] | 88.8 | 95.4 | 99.0 | 74.3 | 90.6 | 98.4 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Rlocs [[240](#bib.bib240)] | 88.8 | 95.4 | 99.0 | 74.3 | 90.6 | 98.4 |'
- en: '| KAPTURE+R2D2+APGeM [[241](#bib.bib241)] | 90.0 | 96.2 | 99.5 | 72.3 | 86.4
    | 97.9 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| KAPTURE+R2D2+APGeM [[241](#bib.bib241)] | 90.0 | 96.2 | 99.5 | 72.3 | 86.4
    | 97.9 |'
- en: '| SP+SuperGlue [[69](#bib.bib69)] | 89.8 | 96.1 | 99.4 | 77.0 | 90.6 | 100.0
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| SP+SuperGlue [[69](#bib.bib69)] | 89.8 | 96.1 | 99.4 | 77.0 | 90.6 | 100.0
    |'
- en: '| SP+SuperGlue [[69](#bib.bib69)]+Patch2Pix [[169](#bib.bib169)] | 89.3 | 95.8
    | 99.2 | 78.0 | 90.6 | 99.0 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| SP+SuperGlue [[69](#bib.bib69)]+Patch2Pix [[169](#bib.bib169)] | 89.3 | 95.8
    | 99.2 | 78.0 | 90.6 | 99.0 |'
- en: '| SP+GFM [[234](#bib.bib234)] | 90.2 | 96.4 | 99.5 | 74.0 | 91.5 | 99.5 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| SP+GFM [[234](#bib.bib234)] | 90.2 | 96.4 | 99.5 | 74.0 | 91.5 | 99.5 |'
- en: '| SP+LightGlue [[136](#bib.bib136)] | 90.2 | 96.0 | 99.4 | 77.0 | 91.1 | 100.0
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| SP+LightGlue [[136](#bib.bib136)] | 90.2 | 96.0 | 99.4 | 77.0 | 91.1 | 100.0
    |'
- en: '| Detector-free | Patch2Pix [[169](#bib.bib169)] | 86.4 | 93.0 | 97.5 | 72.3
    | 88.5 | 97.9 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 无检测器 | Patch2Pix [[169](#bib.bib169)] | 86.4 | 93.0 | 97.5 | 72.3 | 88.5
    | 97.9 |'
- en: '| LoFTR-DS [[72](#bib.bib72)] | — | — | — | 72.8 | 88.5 | 99.0 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| LoFTR-DS [[72](#bib.bib72)] | — | — | — | 72.8 | 88.5 | 99.0 |'
- en: '| LoFTR-OT [[72](#bib.bib72)] | 88.7 | 95.6 | 99.0 | 78.5 | 90.6 | 99.0 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| LoFTR-OT [[72](#bib.bib72)] | 88.7 | 95.6 | 99.0 | 78.5 | 90.6 | 99.0 |'
- en: '| ASpanFormer [[73](#bib.bib73)] | 89.4 | 95.6 | 99.0 | 77.5 | 91.6 | 99.5
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| ASpanFormer [[73](#bib.bib73)] | 89.4 | 95.6 | 99.0 | 77.5 | 91.6 | 99.5
    |'
- en: '| AdaMatcher-LoFTR [[170](#bib.bib170)] | 89.2 | 96.0 | 99.3 | 79.1 | 90.6
    | 99.5 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| AdaMatcher-LoFTR [[170](#bib.bib170)] | 89.2 | 96.0 | 99.3 | 79.1 | 90.6
    | 99.5 |'
- en: '| AdaMatcher-Quad [[170](#bib.bib170)] | 89.2 | 95.9 | 99.2 | 79.1 | 92.1 |
    99.5 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| AdaMatcher-Quad [[170](#bib.bib170)] | 89.2 | 95.9 | 99.2 | 79.1 | 92.1 |
    99.5 |'
- en: '| ASTR [[162](#bib.bib162)] | 89.9 | 95.6 | 99.2 | 76.4 | 92.1 | 99.5 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| ASTR [[162](#bib.bib162)] | 89.9 | 95.6 | 99.2 | 76.4 | 92.1 | 99.5 |'
- en: '| TopicFM [[161](#bib.bib161)] | 90.2 | 95.9 | 98.9 | 77.5 | 91.1 | 99.5 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| TopicFM [[161](#bib.bib161)] | 90.2 | 95.9 | 98.9 | 77.5 | 91.1 | 99.5 |'
- en: '| CasMTR [[164](#bib.bib164)] | 90.4 | 96.2 | 99.3 | 78.5 | 91.6 | 99.5 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| CasMTR [[164](#bib.bib164)] | 90.4 | 96.2 | 99.3 | 78.5 | 91.6 | 99.5 |'
- en: 'Table 6: Visual localization evaluation on the Aachen Day-Night benchmark v1.0 [[222](#bib.bib222)]
    and v1.1 [[9](#bib.bib9)]. The evaluation results on the local feature evaluation
    track are reported. We compare with two groups of the methods, Detector-based
    and Detector-free methods.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在 Aachen Day-Night 基准 v1.0 [[222](#bib.bib222)] 和 v1.1 [[9](#bib.bib9)]
    上的视觉定位评估。报告了本地特征评估轨道的评估结果。我们与两组方法进行比较，即基于检测器的方法和无检测器的方法。
- en: '| Category | Method | Aachen Day-Night v1.0 | Aachen Day-Night v1.1 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | Aachen Day-Night v1.0 | Aachen Day-Night v1.1 |'
- en: '| (0.5m,2°) | (1m,5°) | (5m, 10°) | (0.5m,2°) | (1m,5°) | (5m, 10°) |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| (0.5m,2°) | (1m,5°) | (5m, 10°) | (0.5m,2°) | (1m,5°) | (5m, 10°) |'
- en: '| Detector-based | SP [[61](#bib.bib61)] | 74.5 | 78.6 | 89.8 | — | — | — |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 基于检测器 | SP [[61](#bib.bib61)] | 74.5 | 78.6 | 89.8 | — | — | — |'
- en: '| D2Net [[62](#bib.bib62)] | 74.5 | 86.7 | 100.0 | — | — | — |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| D2Net [[62](#bib.bib62)] | 74.5 | 86.7 | 100.0 | — | — | — |'
- en: '| R2D2 [[63](#bib.bib63)](K=20k) | 76.5 | 90.8 | 100.0 | 71.2 | 86.9 | 97.9
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| R2D2 [[63](#bib.bib63)](K=20k) | 76.5 | 90.8 | 100.0 | 71.2 | 86.9 | 97.9
    |'
- en: '| ASLFeat [[126](#bib.bib126)] | 81.6 | 87.8 | 100.0 | 75.4 | 75.4 | 97.6 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[126](#bib.bib126)] | 81.6 | 87.8 | 100.0 | 75.4 | 75.4 | 97.6 |'
- en: '| ISRF [[239](#bib.bib239)] | — | — | — | 69.1 | 87.4 | 98.4 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| ISRF [[239](#bib.bib239)] | — | — | — | 69.1 | 87.4 | 98.4 |'
- en: '| PoSFeat [[130](#bib.bib130)] | 81.6 | 90.8 | 100.0 | 73.8 | 87.4 | 98.4 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| PoSFeat [[130](#bib.bib130)] | 81.6 | 90.8 | 100.0 | 73.8 | 87.4 | 98.4 |'
- en: '| SIFT+CAPS [[110](#bib.bib110)] | 77.6 | 86.7 | 99.0 | — | — | — |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| SIFT+CAPS [[110](#bib.bib110)] | 77.6 | 86.7 | 99.0 | — | — | — |'
- en: '| SP+S2DNet [[242](#bib.bib242)] | 74.5 | 84.7 | 100.0 | — | — | — |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| SP+S2DNet [[242](#bib.bib242)] | 74.5 | 84.7 | 100.0 | — | — | — |'
- en: '| SP+CAPS [[110](#bib.bib110)] | 82.7 | 87.8 | 100.0 | — | — | — |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| SP+CAPS [[110](#bib.bib110)] | 82.7 | 87.8 | 100.0 | — | — | — |'
- en: '| SP+OANet [[77](#bib.bib77)] | 77.6 | 86.7 | 98.0 | — | — | — |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| SP+OANet [[77](#bib.bib77)] | 77.6 | 86.7 | 98.0 | — | — | — |'
- en: '| SP+SGMNet [[70](#bib.bib70)] | 77.6 | 88.8 | 99.0 | 72.3 | 85.3 | 97.9 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| SP+SGMNet [[70](#bib.bib70)] | 77.6 | 88.8 | 99.0 | 72.3 | 85.3 | 97.9 |'
- en: '| SP+SuperGlue [[69](#bib.bib69)] | 79.6 | 90.8 | 100.0 | 73.3 | 88.0 | 98.4
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| SP+SuperGlue [[69](#bib.bib69)] | 79.6 | 90.8 | 100.0 | 73.3 | 88.0 | 98.4
    |'
- en: '| DSD-MatchingNet [[243](#bib.bib243)] | 80.1 | 90.3 | 100.0 | 73.0 | 88.0
    | 99.3 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| DSD-MatchingNet [[243](#bib.bib243)] | 80.1 | 90.3 | 100.0 | 73.0 | 88.0
    | 99.3 |'
- en: '| Detector-free | Patch2Pix [[169](#bib.bib169)] | 79.6 | 87.8 | 100.0 | 72.3
    | 88.5 | 97.9 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 无检测器 | Patch2Pix [[169](#bib.bib169)] | 79.6 | 87.8 | 100.0 | 72.3 | 88.5
    | 97.9 |'
- en: '| Sparse-NCNet [[65](#bib.bib65)] | 76.5 | 84.7 | 98.0 | — | — | — |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Sparse-NCNet [[65](#bib.bib65)] | 76.5 | 84.7 | 98.0 | — | — | — |'
- en: '| DualRC-Net [[66](#bib.bib66)] | 79.6 | 88.8 | 100.0 | 71.2 | 86.9 | 97.9
    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| DualRC-Net [[66](#bib.bib66)] | 79.6 | 88.8 | 100.0 | 71.2 | 86.9 | 97.9
    |'
- en: '| PDC-Net [[68](#bib.bib68)] | 76.5 | 85.7 | 100.0 | — | — | — |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| PDC-Net [[68](#bib.bib68)] | 76.5 | 85.7 | 100.0 | — | — | — |'
- en: '| PDC-Net+ [[143](#bib.bib143)] | 80.6 | 89.8 | 100.0 | — | — | — |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| PDC-Net+ [[143](#bib.bib143)] | 80.6 | 89.8 | 100.0 | — | — | — |'
- en: '| LoFTR [[72](#bib.bib72)] | — | — | — | 72.8 | 88.5 | 99.0 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| LoFTR [[72](#bib.bib72)] | — | — | — | 72.8 | 88.5 | 99.0 |'
- en: 'Table 7: Visual Localization on the InLoc benchmark  [[115](#bib.bib115)].
    We compare with two groups of the methods, Detector-based and Detector-free methods.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在 InLoc 基准上进行视觉定位评估 [[115](#bib.bib115)]。我们与两组方法进行比较，即基于检测器的方法和无检测器的方法。
- en: '| Category | Methods | DUC1 | DUC2 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | DUC1 | DUC2 |'
- en: '| (0.25m, 10°) | (0.5m, 10°) | (1m, 10°) | (0.25m, 10°) | (0.5m, 10°) | (1m,
    10°) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| (0.25m, 10°) | (0.5m, 10°) | (1m, 10°) | (0.25m, 10°) | (0.5m, 10°) | (1m,
    10°) |'
- en: '| Detector-based | SIFT+CAPS [[110](#bib.bib110)]+NN | 38.4 | 56.6 | 70.7 |
    35.1 | 48.9 | 58.8 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 基于检测器 | SIFT+CAPS [[110](#bib.bib110)]+NN | 38.4 | 56.6 | 70.7 | 35.1 | 48.9
    | 58.8 |'
- en: '| ISRF [[239](#bib.bib239)] | 39.4 | 58.1 | 70.2 | 41.2 | 61.1 | 69.5 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| ISRF [[239](#bib.bib239)] | 39.4 | 58.1 | 70.2 | 41.2 | 61.1 | 69.5 |'
- en: '| D2Net [[62](#bib.bib62)]+NN | 38.4 | 56.1 | 71.2 | 37.4 | 55.0 | 64.9 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| D2Net [[62](#bib.bib62)]+NN | 38.4 | 56.1 | 71.2 | 37.4 | 55.0 | 64.9 |'
- en: '| R2D2 [[63](#bib.bib63)]+NN | 36.4 | 57.6 | 74.2 | 45.0 | 60.3 | 67.9 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| R2D2 [[63](#bib.bib63)]+NN | 36.4 | 57.6 | 74.2 | 45.0 | 60.3 | 67.9 |'
- en: '| KAPTURE [[241](#bib.bib241)]+R2D2 [[63](#bib.bib63)] | 41.4 | 60.1 | 73.7
    | 47.3 | 67.2 | 73.3 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| KAPTURE [[241](#bib.bib241)]+R2D2 [[63](#bib.bib63)] | 41.4 | 60.1 | 73.7
    | 47.3 | 67.2 | 73.3 |'
- en: '| SeLF [[127](#bib.bib127)]+NN | 41.4 | 61.6 | 73.2 | 44.3 | 61.1 | 68.7 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| SeLF [[127](#bib.bib127)]+NN | 41.4 | 61.6 | 73.2 | 44.3 | 61.1 | 68.7 |'
- en: '| AWDesc [[102](#bib.bib102)]+NN | 41.9 | 61.6 | 72.2 | 45.0 | 61.1 | 70.2
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| AWDesc [[102](#bib.bib102)]+NN | 41.9 | 61.6 | 72.2 | 45.0 | 61.1 | 70.2
    |'
- en: '| ASLFeat [[126](#bib.bib126)]+NN | 39.9 | 59.1 | 71.7 | 43.5 | 58.8 | 64.9
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[126](#bib.bib126)]+NN | 39.9 | 59.1 | 71.7 | 43.5 | 58.8 | 64.9
    |'
- en: '| ASLFeat [[126](#bib.bib126)]+SGMNet [[70](#bib.bib70)] | 43.9 | 62.1 | 68.2
    | 45.0 | 63.4 | 73.3 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[126](#bib.bib126)]+SGMNet [[70](#bib.bib70)] | 43.9 | 62.1 | 68.2
    | 45.0 | 63.4 | 73.3 |'
- en: '| ASLFeat [[126](#bib.bib126)]+SuperGlue [[69](#bib.bib69)] | 51.5 | 66.7 |
    75.8 | 53.4 | 76.3 | 84.0 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[126](#bib.bib126)]+SuperGlue [[69](#bib.bib69)] | 51.5 | 66.7 |
    75.8 | 53.4 | 76.3 | 84.0 |'
- en: '| ASLFeat [[126](#bib.bib126)]+ClusterGNN [[71](#bib.bib71)] | 52.5 | 68.7
    | 76.8 | 55.0 | 76.0 | 82.4 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| ASLFeat [[126](#bib.bib126)]+ClusterGNN [[71](#bib.bib71)] | 52.5 | 68.7
    | 76.8 | 55.0 | 76.0 | 82.4 |'
- en: '| SP+NN [[61](#bib.bib61)] | 40.4 | 58.1 | 69.7 | 42.0 | 58.8 | 69.5 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| SP+NN [[61](#bib.bib61)] | 40.4 | 58.1 | 69.7 | 42.0 | 58.8 | 69.5 |'
- en: '| SP+ClusterGNN [[71](#bib.bib71)] | 47.5 | 69.7 | 79.8 | 53.4 | 77.1 | 84.7
    |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| SP+ClusterGNN [[71](#bib.bib71)] | 47.5 | 69.7 | 79.8 | 53.4 | 77.1 | 84.7
    |'
- en: '| SP+SuperGlue [[69](#bib.bib69)] | 49.0 | 68.7 | 80.8 | 53.4 | 77.1 | 82.4
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| SP+SuperGlue [[69](#bib.bib69)] | 49.0 | 68.7 | 80.8 | 53.4 | 77.1 | 82.4
    |'
- en: '| SP+CAPS [[110](#bib.bib110)]+NN | 40.9 | 60.6 | 72.7 | 43.5 | 58.8 | 68.7
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| SP+CAPS [[110](#bib.bib110)]+NN | 40.9 | 60.6 | 72.7 | 43.5 | 58.8 | 68.7
    |'
- en: '|  | SP+LightGlue [[136](#bib.bib136)] | 49.0 | 68.2 | 79.3 | 55.0 | 74.8 |
    79.4 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | SP+LightGlue [[136](#bib.bib136)] | 49.0 | 68.2 | 79.3 | 55.0 | 74.8 |
    79.4 |'
- en: '| Detector-free | Sparse-NCNet [[65](#bib.bib65)] | 41.9 | 62.1 | 72.7 | 35.1
    | 48.1 | 55.0 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Detector-free | Sparse-NCNet [[65](#bib.bib65)] | 41.9 | 62.1 | 72.7 | 35.1
    | 48.1 | 55.0 |'
- en: '| MTLDesc [[101](#bib.bib101)] | 41.9 | 61.6 | 72.2 | 45.0 | 61.1 | 70.2 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| MTLDesc [[101](#bib.bib101)] | 41.9 | 61.6 | 72.2 | 45.0 | 61.1 | 70.2 |'
- en: '| COTR [[154](#bib.bib154)] | 41.9 | 61.1 | 73.2 | 42.7 | 67.9 | 75.6 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| COTR [[154](#bib.bib154)] | 41.9 | 61.1 | 73.2 | 42.7 | 67.9 | 75.6 |'
- en: '| Patch2Pix [[169](#bib.bib169)] | 44.4 | 66.7 | 78.3 | 49.6 | 64.9 | 72.5
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Patch2Pix [[169](#bib.bib169)] | 44.4 | 66.7 | 78.3 | 49.6 | 64.9 | 72.5
    |'
- en: '| LoFTR-OT [[72](#bib.bib72)] | 47.5 | 72.2 | 84.8 | 54.2 | 74.8 | 85.5 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| LoFTR-OT [[72](#bib.bib72)] | 47.5 | 72.2 | 84.8 | 54.2 | 74.8 | 85.5 |'
- en: '| MatchFormer [[159](#bib.bib159)] | 46.5 | 73.2 | 85.9 | 55.7 | 71.8 | 81.7
    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| MatchFormer [[159](#bib.bib159)] | 46.5 | 73.2 | 85.9 | 55.7 | 71.8 | 81.7
    |'
- en: '| ASpanFormer [[73](#bib.bib73)] | 51.5 | 73.7 | 86.4 | 55.0 | 74.0 | 81.7
    |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| ASpanFormer [[73](#bib.bib73)] | 51.5 | 73.7 | 86.4 | 55.0 | 74.0 | 81.7
    |'
- en: '| TopicFM [[161](#bib.bib161)] | 52.0 | 74.7 | 87.4 | 53.4 | 74.8 | 83.2 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| TopicFM [[161](#bib.bib161)] | 52.0 | 74.7 | 87.4 | 53.4 | 74.8 | 83.2 |'
- en: '| GlueStick [[135](#bib.bib135)] | 49.0 | 70.2 | 84.3 | 55.0 | 83.2 | 87.0
    |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| GlueStick [[135](#bib.bib135)] | 49.0 | 70.2 | 84.3 | 55.0 | 83.2 | 87.0
    |'
- en: '| SEM [[166](#bib.bib166)] | 52.0 | 74.2 | 87.4 | 50.4 | 76.3 | 83.2 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| SEM [[166](#bib.bib166)] | 52.0 | 74.2 | 87.4 | 50.4 | 76.3 | 83.2 |'
- en: '| ASTR [[162](#bib.bib162)] | 53.0 | 73.7 | 87.4 | 52.7 | 76.3 | 84.0 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| ASTR [[162](#bib.bib162)] | 53.0 | 73.7 | 87.4 | 52.7 | 76.3 | 84.0 |'
- en: '| CasMTR [[164](#bib.bib164)] | 53.5 | 76.8 | 85.4 | 51.9 | 70.2 | 83.2 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| CasMTR [[164](#bib.bib164)] | 53.5 | 76.8 | 85.4 | 51.9 | 70.2 | 83.2 |'
- en: '| ASTR [[162](#bib.bib162)] | 53.0 | 73.7 | 87.4 | 52.7 | 76.3 | 84.0 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| ASTR [[162](#bib.bib162)] | 53.0 | 73.7 | 87.4 | 52.7 | 76.3 | 84.0 |'
- en: '| PATS [[171](#bib.bib171)] | 55.6 | 71.2 | 81.0 | 58.8 | 80.9 | 85.5 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| PATS [[171](#bib.bib171)] | 55.6 | 71.2 | 81.0 | 58.8 | 80.9 | 85.5 |'
- en: 'Table 8: Optical flow results on the training splits of KITTI  [[225](#bib.bib225)].
    Following  [[154](#bib.bib154), [155](#bib.bib155)], (”+intp”) represents interpolating
    the output of the model to obtain the corresponding relationship per pixel. †
    means evaluated it with DenseMatching tools provided by the authors of GLU-Net.
    This part contains generic matching networks.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 8: 训练分割上的光流结果  [[225](#bib.bib225)]。参考  [[154](#bib.bib154), [155](#bib.bib155)]，("
    +intp")表示对模型输出进行插值以获得每像素的对应关系。† 表示使用了 GLU-Net 作者提供的 DenseMatching 工具进行评估。这部分包含通用匹配网络。'
- en: '| Methods | KITTI-2012 | KITTI-2015 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | KITTI-2012 | KITTI-2015 |'
- en: '| --- | --- | --- |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| APAE↓ | Fl-all[%]↓ | APAE↓ | Fl-all[%]↓ |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| APAE↓ | Fl-all[%]↓ | APAE↓ | Fl-all[%]↓ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| DGC-Net [[231](#bib.bib231)] | 8.50 | 32.28 | 14.97 | 50.98 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| DGC-Net [[231](#bib.bib231)] | 8.50 | 32.28 | 14.97 | 50.98 |'
- en: '| DGC-Net† [[231](#bib.bib231)] | 7.96 | 34.35 | 14.33 | 50.35 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| DGC-Net† [[231](#bib.bib231)] | 7.96 | 34.35 | 14.33 | 50.35 |'
- en: '| GLU-Net [[67](#bib.bib67)] | 3.14 | 19.76 | 7.49 | 33.83 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| GLU-Net [[67](#bib.bib67)] | 3.14 | 19.76 | 7.49 | 33.83 |'
- en: '| GLU-Net+GOCor [[142](#bib.bib142)] | 2.68 | 15.43 | 6.68 | 27.57 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| GLU-Net+GOCor [[142](#bib.bib142)] | 2.68 | 15.43 | 6.68 | 27.57 |'
- en: '| RANSAC-Flow [[232](#bib.bib232)] | — | — | 12.48 | — |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| RANSAC-Flow [[232](#bib.bib232)] | — | — | 12.48 | — |'
- en: '| COTR [[154](#bib.bib154)] | 1.28 | 7.36 | 2.62 | 9.92 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| COTR [[154](#bib.bib154)] | 1.28 | 7.36 | 2.62 | 9.92 |'
- en: '| COTR+Intp. [[154](#bib.bib154)] | 2.26 | 10.50 | 6.12 | 16.90 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| COTR+Intp. [[154](#bib.bib154)] | 2.26 | 10.50 | 6.12 | 16.90 |'
- en: '| PDC-Net(D) [[68](#bib.bib68)] | 2.08 | 7.98 | 5.22 | 15.13 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| PDC-Net(D) [[68](#bib.bib68)] | 2.08 | 7.98 | 5.22 | 15.13 |'
- en: '| PDC-Net+(D) [[143](#bib.bib143)] | 1.76 | 6.60 | 4.53 | 12.62 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| PDC-Net+(D) [[143](#bib.bib143)] | 1.76 | 6.60 | 4.53 | 12.62 |'
- en: '| COTR† [[154](#bib.bib154)] | 1.15 | 6.98 | 2.06 | 9.14 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| COTR† [[154](#bib.bib154)] | 1.15 | 6.98 | 2.06 | 9.14 |'
- en: '| COTR†+Intp. [[154](#bib.bib154)] | 1.47 | 8.79 | 3.65 | 13.65 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| COTR†+Intp. [[154](#bib.bib154)] | 1.47 | 8.79 | 3.65 | 13.65 |'
- en: '| ECO-TR [[155](#bib.bib155)] | 0.96 | 3.77 | 1.40 | 6.39 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| ECO-TR [[155](#bib.bib155)] | 0.96 | 3.77 | 1.40 | 6.39 |'
- en: '| ECO-TR+Intp. [[155](#bib.bib155)] | 1.46 | 6.64 | 3.16 | 12.10 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| ECO-TR+Intp. [[155](#bib.bib155)] | 1.46 | 6.64 | 3.16 | 12.10 |'
- en: '| PATS+Intp. [[171](#bib.bib171)] | 1.17 | 4.04 | 3.39 | 9.68 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| PATS+Intp. [[171](#bib.bib171)] | 1.17 | 4.04 | 3.39 | 9.68 |'
- en: 'Table 9: Evaluation of SfM Methods on the ETH3D [[228](#bib.bib228)] for Multi-View
    Camera Pose Estimation and 3D Triangulation. The table segregates methods into
    Detector-based and Detector-free categories. The results are derived from the
    DetectorFreeSfM [[179](#bib.bib179)].'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：对 ETH3D [[228](#bib.bib228)] 上的 SfM 方法进行的评估，用于多视角相机姿态估计和 3D 三角测量。该表将方法分为基于检测器和无检测器类别。结果来源于
    DetectorFreeSfM [[179](#bib.bib179)]。
- en: '| Category | Methods | Multi-View Camera Pose Estimation | 3D Triangulation
    |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 多视角相机姿态估计 | 3D 三角测量 |'
- en: '| AUC | Accuracy (%) | Completeness (%) |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| AUC | 准确率 (%) | 完整率 (%) |'
- en: '| @1° | @3° | @5° | 1cm | 2cm | 5cm | 1cm | 2cm | 5cm |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| @1° | @3° | @5° | 1cm | 2cm | 5cm | 1cm | 2cm | 5cm |'
- en: '| Detector-based | SIFT [[17](#bib.bib17)]+NN+PixSfM [[174](#bib.bib174)] |
    26.94 | 39.01 | 42.19 | 76.18 | 85.60 | 93.16 | 0.17 | 0.71 | 3.29 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 基于检测器 | SIFT [[17](#bib.bib17)]+NN+PixSfM [[174](#bib.bib174)] | 26.94 |
    39.01 | 42.19 | 76.18 | 85.60 | 93.16 | 0.17 | 0.71 | 3.29 |'
- en: '| D2Net [[62](#bib.bib62)]+NN+PixSfM [[174](#bib.bib174)] | 34.50 | 49.77 |
    53.58 | 74.75 | 83.81 | 91.98 | 0.83 | 2.69 | 8.95 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| D2Net [[62](#bib.bib62)]+NN+PixSfM [[174](#bib.bib174)] | 34.50 | 49.77 |
    53.58 | 74.75 | 83.81 | 91.98 | 0.83 | 2.69 | 8.95 |'
- en: '| R2D2 [[63](#bib.bib63)]+NN+PixSfM [[174](#bib.bib174)] | 43.58 | 62.09 |
    66.89 | 74.12 | 84.49 | 91.98 | 0.43 | 1.58 | 6.71 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| R2D2 [[63](#bib.bib63)]+NN+PixSfM [[174](#bib.bib174)] | 43.58 | 62.09 |
    66.89 | 74.12 | 84.49 | 91.98 | 0.43 | 1.58 | 6.71 |'
- en: '| SP+SG [[69](#bib.bib69)]+PixSfM [[174](#bib.bib174)] | 50.82 | 68.52 | 72.86
    | 79.01 | 87.04 | 93.80 | 0.75 | 2.77 | 11.28 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| SP+SG [[69](#bib.bib69)]+PixSfM [[174](#bib.bib174)] | 50.82 | 68.52 | 72.86
    | 79.01 | 87.04 | 93.80 | 0.75 | 2.77 | 11.28 |'
- en: '| Detector-free | LoFTR [[72](#bib.bib72)]+DetectorFreeSfM [[179](#bib.bib179)]
    | 59.12 | 75.59 | 79.53 | 80.38 | 89.01 | 95.83 | 3.73 | 11.07 | 29.54 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 无检测器 | LoFTR [[72](#bib.bib72)]+DetectorFreeSfM [[179](#bib.bib179)] | 59.12
    | 75.59 | 79.53 | 80.38 | 89.01 | 95.83 | 3.73 | 11.07 | 29.54 |'
- en: '| ASpanFormer [[73](#bib.bib73)]+DetectorFreeSfM [[179](#bib.bib179)] | 57.23
    | 73.71 | 77.70 | 77.63 | 87.40 | 95.02 | 3.97 | 12.18 | 32.42 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| ASpanFormer [[73](#bib.bib73)]+DetectorFreeSfM [[179](#bib.bib179)] | 57.23
    | 73.71 | 77.70 | 77.63 | 87.40 | 95.02 | 3.97 | 12.18 | 32.42 |'
- en: '| MatchFormer [[159](#bib.bib159)]+DetectorFreeSfM [[179](#bib.bib179)] | 56.70
    | 73.00 | 76.84 | 79.86 | 88.51 | 95.48 | 3.76 | 11.06 | 29.05 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| MatchFormer [[159](#bib.bib159)]+DetectorFreeSfM [[179](#bib.bib179)] | 56.70
    | 73.00 | 76.84 | 79.86 | 88.51 | 95.48 | 3.76 | 11.06 | 29.05 |'
- en: 7 Challenges and Opportunities
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 挑战与机遇
- en: Deep learning has brought significant advantages to image-based local feature
    matching. However, there are still several challenges that remain to be addressed.
    In the following sections, we will explore potential research directions that
    we believe will provide valuable momentum for further advancements in image-based
    local feature matching algorithms.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习为基于图像的局部特征匹配带来了显著的优势。然而，仍然存在几个需要解决的挑战。在接下来的部分中，我们将探讨可能的研究方向，这些方向有望为基于图像的局部特征匹配算法的进一步发展提供宝贵的动力。
- en: 7.1 Efficient Attention and Transformer
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 高效注意力与变换器
- en: For local feature matching, integrating transformers into GNN models can be
    considered, framing the task as a graph matching problem involving two sets of
    features. To enhance the construction of better matchers, the self-attention and
    cross-attention layers within transformers are commonly employed to exchange global
    visual and geometric information between nodes. In addition to sparse descriptors
    generated by matching detectors, direct application of self-attention and cross-attention
    to feature maps extracted by CNNs, and generating matches in a coarse-to-fine
    manner, has also been explored [[69](#bib.bib69), [72](#bib.bib72)]. However,
    the computational cost of basic matrix multiplication in transformers remains
    high when dealing with a large number of keypoints. In recent years, efforts have
    been made to enhance the efficiency of transformers and attempts to compute the
    two types of attention in parallel have been made, continuously reducing complexity
    while maintaining performance [[70](#bib.bib70), [71](#bib.bib71), [140](#bib.bib140),
    [139](#bib.bib139)]. Future research will further optimize the structures of attention
    mechanisms and transformers, aiming to maintain high matching performance while
    reducing computational complexity. This will make local feature matching methods
    more efficient and applicable in real-time and resource-constrained environments.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 对于局部特征匹配，可以考虑将变换器集成到GNN模型中，将任务框架视为涉及两个特征集的图匹配问题。为了增强更好的匹配器的构建，通常采用变换器中的自注意力层和交叉注意力层，以在节点之间交换全局视觉和几何信息。除了由匹配检测器生成的稀疏描述符外，还探索了将自注意力和交叉注意力直接应用于CNN提取的特征图，并以粗到细的方式生成匹配[[69](#bib.bib69),
    [72](#bib.bib72)]。然而，在处理大量关键点时，变换器中基本矩阵乘法的计算成本仍然很高。近年来，已经努力提高变换器的效率，并尝试并行计算两种注意力类型，持续降低复杂性同时保持性能[[70](#bib.bib70),
    [71](#bib.bib71), [140](#bib.bib140), [139](#bib.bib139)]。未来的研究将进一步优化注意力机制和变换器的结构，旨在在降低计算复杂性的同时保持高匹配性能。这将使局部特征匹配方法在实时和资源受限环境中更高效、更具应用性。
- en: 7.2 Adaptation strategy
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 自适应策略
- en: In recent years, significant progress has been made in the research on adaptability
    in local feature matching [[136](#bib.bib136), [162](#bib.bib162), [170](#bib.bib170),
    [171](#bib.bib171), [73](#bib.bib73)]. For latency-sensitive applications, adaptive
    mechanisms can be incorporated into the matching process. This allows the modulation
    of network depth and width based on factors such as visual overlap and appearance
    variation, enabling fine-grained control over the difficulty of the matching task.
    Furthermore, researchers have proposed various innovative approaches to address
    issues such as scale variations. One key challenge is how to adaptively adjust
    the size of the cropping grid based on image scale variations to avoid matching
    failures. Geometric inconsistencies in patch-level feature matching can also be
    alleviated through adaptive allocation strategies, combined with adaptive patch
    subdivision strategies that enhance correspondence quality progressively from
    coarse to fine during the matching process. On the other hand, attention spans
    can be adaptively adjusted based on the difficulty of matching, achieving variable-sized
    adaptive attention regions at different positions. This allows the network to
    better adapt to features at different locations while capturing contextual information,
    thereby improving matching performance.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，在局部特征匹配的适应性研究中取得了显著进展[[136](#bib.bib136), [162](#bib.bib162), [170](#bib.bib170),
    [171](#bib.bib171), [73](#bib.bib73)]。对于对延迟敏感的应用，可以将自适应机制纳入匹配过程。这允许根据视觉重叠和外观变化等因素调节网络的深度和宽度，从而对匹配任务的难度进行细粒度控制。此外，研究人员提出了各种创新方法来解决如尺度变化等问题。一个关键挑战是如何根据图像尺度变化自适应调整裁剪网格的大小，以避免匹配失败。通过自适应分配策略以及结合自适应补丁细分策略，可以缓解补丁级特征匹配中的几何不一致性，这些策略在匹配过程中从粗到细逐步提高对应质量。另一方面，可以根据匹配难度自适应调整注意力跨度，在不同位置实现可变大小的自适应注意区域。这使得网络能够更好地适应不同位置的特征，同时捕捉上下文信息，从而提高匹配性能。
- en: In summary, the research on adaptability in local feature matching offers vast
    prospects and opportunities for future developments, while enhancing efficiency
    in terms of memory and computation. With the emergence of more demands and challenges
    across various fields, it is anticipated that adaptive mechanisms will play an
    increasingly important role in local feature matching. Future research could further
    explore finer-grained adaptive strategies to achieve more efficient and accurate
    matching results.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本地特征匹配中的适应性研究为未来的发展提供了广阔的前景和机会，同时提高了内存和计算效率。随着各领域对需求和挑战的不断增加，预计自适应机制将在本地特征匹配中发挥越来越重要的作用。未来的研究可以进一步探索更细粒度的自适应策略，以实现更高效和更准确的匹配结果。
- en: 7.3 Weakly Supervised Learning
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 弱监督学习
- en: The field of local feature matching has not only achieved significant progress
    under fully supervised settings but has also shown potential in the realm of weakly
    supervised learning. Traditional fully supervised methods rely on dense ground-truth
    correspondence labels. In recent years, researchers have turned their attention
    to self-supervised and weakly supervised learning to reduce the dependency on
    precise annotations. Self-supervised learning methods like SuperPoint [[61](#bib.bib61)]
    train on pairs of images generated through virtual homography transformations,
    yielding promising results. However, these simple geometric transformations might
    not work effectively under complex scenarios. Weakly supervised learning has become
    a research focus in the domain of local feature learning [[110](#bib.bib110),
    [111](#bib.bib111), [169](#bib.bib169), [130](#bib.bib130), [132](#bib.bib132)].
    These methods often combine weakly supervised learning with the describe-detect
    pipeline, but direct use of weakly supervised loss leads to noticeable performance
    drops. Some approaches rely solely on solutions involving relative camera poses,
    learning descriptors via epipolar loss. The limitations of weakly supervised methods
    lie in their difficulty to differentiate errors introduced by descriptors and
    keypoints, as well as accurately distinguishing different descriptors. To overcome
    these challenges, carefully designed decoupled training pipelines have emerged,
    where the description network and detection network are trained separately until
    high-quality descriptors are obtained. Chen et al. [[244](#bib.bib244)] propose
    innovative methods using convolutional neural networks for feature shape estimation,
    orientation assignment, and descriptor learning. Their approach establishes a
    standard shape and orientation for each feature, enabling a transition from supervised
    to self-supervised learning by eliminating the need for known feature matching
    relationships. They also introduce a ’weak match finder’ in descriptor learning,
    enhancing feature appearance variability and improving descriptor invariance.
    These advancements signify a significant progress in weakly supervised learning
    for feature matching, especially in cases involving substantial viewpoint and
    viewing direction changes.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 本地特征匹配领域不仅在完全监督设置下取得了显著进展，而且在弱监督学习领域也展现了潜力。传统的完全监督方法依赖于密集的真实标签。近年来，研究者们转向自监督和弱监督学习，以减少对精确标注的依赖。像
    SuperPoint 这样的自监督学习方法[[61](#bib.bib61)] 通过虚拟单应性变换生成的图像对进行训练，取得了良好的结果。然而，这些简单的几何变换在复杂场景下可能效果不佳。弱监督学习已成为本地特征学习领域的研究重点[[110](#bib.bib110),
    [111](#bib.bib111), [169](#bib.bib169), [130](#bib.bib130), [132](#bib.bib132)]。这些方法通常将弱监督学习与描述-检测管道结合使用，但直接使用弱监督损失会导致明显的性能下降。一些方法仅依赖于相对相机姿态的解决方案，通过极线损失学习描述符。弱监督方法的局限性在于难以区分由描述符和关键点引入的错误，以及准确区分不同的描述符。为克服这些挑战，出现了精心设计的解耦训练管道，其中描述网络和检测网络分别训练，直到获得高质量的描述符。Chen
    等人[[244](#bib.bib244)] 提出了使用卷积神经网络进行特征形状估计、方向分配和描述符学习的创新方法。他们的方法为每个特征建立了标准形状和方向，通过消除对已知特征匹配关系的需求，实现了从监督学习到自监督学习的过渡。他们还在描述符学习中引入了“弱匹配查找器”，增强了特征外观的变异性，提高了描述符的不变性。这些进展标志着弱监督学习在特征匹配中的显著进步，特别是在涉及大量视角和视向变化的情况下。
- en: These weakly supervised methods open up new prospects and opportunities for
    local feature learning, allowing models to be trained on larger and more diverse
    datasets, thereby obtaining more generalized descriptors. However, these methods
    still face challenges, such as effectively leveraging weak supervision signals
    and addressing the uncertainty of descriptors and keypoints. In the future, developments
    in weakly supervised learning in the domain of local feature matching might focus
    on finer loss function designs, better utilization of weak supervision signals,
    and broader application domains. Exploring mechanisms for weakly supervised learning
    in conjunction with traditional fully supervised methods holds promise for enhancing
    the performance and generalization capabilities of local feature matching in complex
    scenes.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这些弱监督方法为局部特征学习开辟了新的前景和机会，允许模型在更大和更多样化的数据集上进行训练，从而获得更具泛化性的描述符。然而，这些方法仍面临挑战，如有效利用弱监督信号和解决描述符及关键点的不确定性。未来，弱监督学习在局部特征匹配领域的发展可能会关注更精细的损失函数设计、更好地利用弱监督信号以及更广泛的应用领域。探索弱监督学习机制与传统完全监督方法的结合，有望提升复杂场景中局部特征匹配的性能和泛化能力。
- en: 7.4 Foundation Segmentation Models
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 基础分割模型
- en: Typically, semantic segmentation models, trained on datasets such as Cityscapes [[245](#bib.bib245)]
    and MIT ADE20k [[246](#bib.bib246)], offer fundamental semantic information and
    play a crucial role in enhancing the detection and description processes of specific
    environments  [[127](#bib.bib127), [128](#bib.bib128)].
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，基于Cityscapes [[245](#bib.bib245)]和MIT ADE20k [[246](#bib.bib246)]等数据集训练的语义分割模型提供了基本的语义信息，并在增强特定环境的检测和描述过程中发挥了关键作用[[127](#bib.bib127)、[128](#bib.bib128)]。
- en: However, the advent of large foundation models such as SAM [[247](#bib.bib247)],
    DINO [[248](#bib.bib248)], and DINOv2 [[249](#bib.bib249)] marks a new era in
    artificial intelligence. While traditional segmentation models excel in their
    specific domains, these foundation models introduce a broader, more versatile
    approach. Their extensive pre-training on massive, diverse datasets equips them
    with a remarkable zero-shot generalization capability, enabling them to adapt
    to a wide range of scenarios. For instance, SAMFeat [[42](#bib.bib42)] demonstrates
    how SAM, a model adept at segmenting ”anything” in ”any scene,” can guide local
    feature learning with its rich, category-agnostic semantic knowledge. By distilling
    fine-grained semantic relations and focusing on edge detection, SAMFeat showcases
    a significant enhancement in local feature description and accuracy. Similarly,
    SelaVPR [[250](#bib.bib250)] demonstrates how to effectively adjust the DINOv2
    model using lightweight adapters to address the challenges in Visual Place Recognition
    (VPR) by proficiently matching local features without the need for extensive spatial
    verification, thus streamlining the retrieval process.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大型基础模型的出现，如SAM [[247](#bib.bib247)]、DINO [[248](#bib.bib248)]和DINOv2 [[249](#bib.bib249)]，标志着人工智能进入了一个新纪元。虽然传统的分割模型在特定领域表现出色，但这些基础模型引入了更广泛、更灵活的方法。它们在大规模、多样化的数据集上进行了广泛的预训练，赋予了它们卓越的零样本泛化能力，使它们能够适应各种场景。例如，SAMFeat [[42](#bib.bib42)]展示了如何利用SAM这一擅长在“任何场景”中分割“任何事物”的模型，通过其丰富的类别无关语义知识来指导局部特征学习。通过提炼细粒度语义关系并关注边缘检测，SAMFeat展示了局部特征描述和准确性的显著提升。类似地，SelaVPR [[250](#bib.bib250)]展示了如何使用轻量级适配器有效调整DINOv2模型，以应对视觉地点识别（VPR）中的挑战，通过高效匹配局部特征而无需大量空间验证，从而简化检索过程。
- en: Looking towards an open-world scenario, the versatility and robust generalization
    offered by these large foundation models present exciting prospects. Their ability
    to understand and interpret a vast array of scenes and objects far exceeds the
    scope of traditional segmentation networks, paving the way for advancements in
    feature matching across diverse and dynamic environments. In summary, while the
    contributions of traditional semantic segmentation networks remain invaluable,
    the integration of large foundation models offers a complementary and expansive
    approach, essential for pushing the boundaries of what is achievable in feature
    matching, particularly in open-world applications.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 展望开放世界场景，这些大型基础模型所提供的多功能性和强大的泛化能力带来了令人兴奋的前景。它们理解和解释各种场景和物体的能力远远超出传统分割网络的范围，为在多样化和动态环境中的特征匹配的进展铺平了道路。总之，尽管传统语义分割网络的贡献仍然不可或缺，但大型基础模型的融合提供了一种互补且广阔的方法，这对于推动特征匹配的极限，特别是在开放世界应用中的可实现性至关重要。
- en: 7.5 Mismatch Removal
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 不匹配移除
- en: Image matching, involving the establishment of reliable connections between
    two images portraying a shared object or scene, poses intricate challenges due
    to the combinatorial nature of the process and the presence of outliers. Direct
    matching methodologies, such as point set registration and graph matching, often
    grapple with formidable computational demands and erratic performance. Consequently,
    a bifurcated approach, commencing with preliminary match construction utilizing
    feature descriptors like SIFT, ORB, and SURF [[33](#bib.bib33), [35](#bib.bib35),
    [34](#bib.bib34)], succeeded by the application of local and global geometrical
    constraints, has become a prevalent strategy. Nevertheless, these methodologies
    encounter constraints, notably when confronted with multi-modal images or substantial
    variations in viewpoint and lighting [[251](#bib.bib251)].
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图像匹配涉及在两个描绘共享物体或场景的图像之间建立可靠的连接，由于过程的组合性质和存在的离群点，面临复杂的挑战。直接匹配方法，如点集配准和图匹配，通常面临巨大的计算需求和不稳定的表现。因此，一种分阶段的方法，首先利用特征描述符如SIFT、ORB和SURF进行初步匹配构建[[33](#bib.bib33),
    [35](#bib.bib35), [34](#bib.bib34)]，然后应用局部和全局几何约束，已成为一种普遍策略。然而，这些方法在面对多模态图像或视角和光照的大幅变化时遇到了一些限制[[251](#bib.bib251)]。
- en: The evolution of methodologies for outlier rejection has been crucial in overcoming
    the challenges of mismatch elimination, as highlighted by Ma et al. [[252](#bib.bib252)].
    Traditional methods, epitomized by RANSAC [[39](#bib.bib39)] and its variants
    such as USAC [[253](#bib.bib253)] and MAGSAC++ [[254](#bib.bib254)], have significantly
    improved efficiency and accuracy in outlier rejection. Nonetheless, these approaches
    are limited by computational time constraints and their suitability for non-rigid
    contexts. Techniques specific to non-rigid scenarios, like ICF [[255](#bib.bib255)],
    have shown efficacy in addressing geometric distortions. The Advent of Learning-Driven
    Strategies in Mismatch Elimination The integration of deep learning into mismatch
    elimination has opened new pathways for enhancing feature matching. Yi et al. [[236](#bib.bib236)]
    introduced Context Normalization (CNe), a groundbreaking concept that has transformed
    wide-baseline stereo correspondences by effectively distinguishing inliers from
    outliers. Expanding upon this, Sun et al. [[256](#bib.bib256)] developed Attentive
    Context Networks (ACNe), which improved the management of permutation-equivariant
    data through Attentive Context Normalization, yielding significant advancements
    in camera pose estimation and point cloud classification. Zhang et al. [[77](#bib.bib77)]
    proposed the OANet, a novel methodology that precisely determines two-view correspondences
    and bolsters geometric estimations using a hierarchical clustering approach. Zhao
    et al. [[257](#bib.bib257)] introduced NM-Net, a layered network focusing on the
    selection of feature correspondences with compatibility-specific mining, demonstrating
    outstanding performance in various settings. Shape-Former [[258](#bib.bib258)]
    innovatively addresses multimodal and multiview image matching challenges, focusing
    on robust mismatch removal through a hybrid neural network. Leveraging CNNs and
    Transformers, Shape-Former introduces ShapeConv for sparse matches learning, excelling
    in outlier estimation and consensus representation while showcasing superior performance.
    Recent developments like LSVANet [[259](#bib.bib259)], LGSC [[251](#bib.bib251)],
    and HCA-Net [[182](#bib.bib182)], have shown promise in more effectively discerning
    inliers from outliers. These approaches leverage deep learning modules for geometric
    estimation and feature correspondence categorization, signifying an advancement
    over conventional methods.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值剔除方法的演变在克服匹配消除的挑战中至关重要，正如Ma等人所强调的[[252](#bib.bib252)]。传统方法，如RANSAC[[39](#bib.bib39)]及其变体如USAC[[253](#bib.bib253)]和MAGSAC++[[254](#bib.bib254)]，在异常值剔除中显著提高了效率和准确性。然而，这些方法受到计算时间限制和对非刚性场景适用性的限制。特定于非刚性场景的技术，如ICF[[255](#bib.bib255)]，在解决几何畸变方面表现出效力。学习驱动策略在匹配消除中的崛起将深度学习整合到匹配消除中，为提升特征匹配开辟了新途径。Yi等人[[236](#bib.bib236)]提出了Context
    Normalization (CNe)这一开创性的概念，通过有效区分内点和外点，改变了宽基线立体对应关系。基于此，Sun等人[[256](#bib.bib256)]开发了Attentive
    Context Networks (ACNe)，通过Attentive Context Normalization改进了置换等变数据的管理，在相机姿态估计和点云分类中取得了显著进展。Zhang等人[[77](#bib.bib77)]提出了OANet，这是一种新颖的方法，利用层次聚类方法精确确定两视图对应关系，并增强几何估计。Zhao等人[[257](#bib.bib257)]介绍了NM-Net，这是一种关注特征对应选择的分层网络，通过兼容性特定挖掘展示了在各种环境中的出色表现。Shape-Former[[258](#bib.bib258)]创新性地解决了多模态和多视角图像匹配挑战，重点通过混合神经网络进行稳健的匹配剔除。Shape-Former利用CNNs和Transformers，引入ShapeConv用于稀疏匹配学习，在异常值估计和一致性表示方面表现优异。近期发展如LSVANet[[259](#bib.bib259)]、LGSC[[251](#bib.bib251)]和HCA-Net[[182](#bib.bib182)]，在更有效地区分内点和外点方面表现出前景。这些方法利用深度学习模块进行几何估计和特征对应分类，标志着相较于传统方法的进步。
- en: Primarily, the development of more generalized and robust learning-based methodologies
    adept at handling a diverse array of scenarios, encompassing non-rigid transformations
    and multi-modal images, is imperative. Secondly, there exists a necessity for
    methodologies that amalgamate the virtues of both traditional geometric approaches
    and contemporary learning-based techniques. Such hybrid approaches hold the potential
    to deliver superior performance by capitalizing on the strengths of both paradigms.
    Lastly, the exploration of innovative learning architectures and loss functions
    tailored for mismatch elimination can unveil novel prospects in feature matching,
    elevating the overall resilience and precision of computer vision systems. In
    conclusion, the elimination of mismatches persists as a pivotal yet formidable
    facet of local feature matching. The ongoing evolution of both traditional and
    learning-based methodologies unfolds promising trajectories to address extant
    limitations and unlock newfound potentials in computer vision applications.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，开发更通用且稳健的基于学习的方法，能够处理各种场景，包括非刚性变换和多模态图像，是非常必要的。其次，需要结合传统几何方法和现代基于学习的技术的优点的方法。这种混合方法有潜力通过利用两种范式的优势来提供更优越的性能。最后，探索为消除不匹配量身定制的创新学习架构和损失函数，可以揭示特征匹配中的新前景，提高计算机视觉系统的整体抗干扰性和精度。总之，消除不匹配仍然是局部特征匹配的一个关键而艰巨的方面。传统和基于学习的方法的持续发展展现了有希望的轨迹，以解决现有的局限性并解锁计算机视觉应用中的新潜力。
- en: 7.6 Deep Learning and Handcrafted Analogies
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6 深度学习与手工制作类比
- en: The field of image matching is witnessing a unique blend of deep learning and
    traditional handcrafted techniques. This convergence is evident in the adoption
    of foundational elements from classic methods, such as the ”SIFT” pipeline, in
    recent semi-dense, detector-free approaches. Notable examples of this trend include
    the Hybrid Pipeline (HP) by Bellavia et al.[[260](#bib.bib260)], HarrisZ+[[261](#bib.bib261)],
    and Slime [[262](#bib.bib262)], all demonstrating competitive capabilities alongside
    state-of-the-art deep methods.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图像匹配领域正经历深度学习与传统手工技术的独特融合。这种融合在最近的半密集、无检测器的方法中，明显采用了经典方法的基础元素，如"SIFT"管道。这个趋势的显著例子包括Bellavia等人的混合管道（HP）[[260](#bib.bib260)]、HarrisZ+[[261](#bib.bib261)]和Slime[[262](#bib.bib262)]，它们在竞争中表现出了与最先进的深度方法相当的能力。
- en: The HP method integrates handcrafted and learning-based approaches, maintaining
    crucial rotational invariance for photogrammetric surveys. It features the novel
    Keypoint Filtering by Coverage (KFC) module, enhancing the accuracy of the overall
    pipeline. HarrisZ+ represents an evolution of the classic Harris corner detector,
    optimized to synergize with modern image matching components. It yields more discriminative
    and accurately placed keypoints, aligning closely with results from contemporary
    deep learning models. Slime, employs a novel strategy of modeling scenes with
    local overlapping planes, merging local affine approximation principles with global
    matching constraints. This hybrid approach echoes traditional image matching processes,
    challenging the performance of deep learning methods. These advancements signify
    that despite the significant strides made by deep learning methods like LoFTR
    and SuperGlue, the foundational principles of handcrafted techniques remain vital.
    The integration of classical concepts with modern computational power, as seen
    in HP, HarrisZ+, and Slime, leads to robust image matching solutions. These methods
    offer potential avenues for future research that blends diverse methodologies,
    bridging the gap between traditional and modern approaches in image matching.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: HP方法融合了手工制作和基于学习的方法，保持了摄影测量调查中关键的旋转不变性。它具有新颖的覆盖范围关键点滤波（KFC）模块，提升了整个流程的准确性。HarrisZ+代表了经典Harris角点检测器的进化，经过优化以与现代图像匹配组件协同工作。它生成了更具辨别力且放置准确的关键点，与现代深度学习模型的结果紧密对齐。Slime采用了一种新策略，通过局部重叠平面建模场景，将局部仿射近似原理与全局匹配约束相结合。这种混合方法反映了传统图像匹配过程，对深度学习方法的表现提出了挑战。这些进展表明，尽管深度学习方法如LoFTR和SuperGlue取得了显著进展，手工技术的基本原理仍然至关重要。经典概念与现代计算能力的结合，如HP、HarrisZ+和Slime所示，导致了强大的图像匹配解决方案。这些方法为未来的研究提供了潜在途径，结合了多样的方法，弥合了传统和现代图像匹配方法之间的差距。
- en: 7.7 Utilizing geometric information
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7 利用几何信息
- en: When facing challenges such as texturelessness, occlusion, and repetitive patterns,
    traditional local feature matching methods may perform poorly. In recent years,
    researchers have started to focus on better utilizing geometric information to
    enhance the effectiveness of local feature matching in the presence of these challenges.
    Several studies [[166](#bib.bib166), [167](#bib.bib167), [143](#bib.bib143), [165](#bib.bib165)]
    have indicated that leveraging geometric information holds significant potential
    for local feature matching. By capturing geometric relationships between pixels
    more accurately and combining geometric priors with image appearance information,
    these methods can enhance the robustness and accuracy of matching in complex scenes.
    However, this direction presents numerous opportunities and challenges for future
    development. Firstly, how to model geometric information more profoundly to better
    address scenarios involving large displacements, occlusions, and textureless regions
    remains a critical question. Secondly, improving the performance of confidence
    estimation to yield more reliable matching results is also a direction worthy
    of investigation.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 面对诸如纹理缺失、遮挡和重复模式等挑战时，传统的局部特征匹配方法可能表现不佳。近年来，研究人员开始关注如何更好地利用几何信息，以提高在这些挑战下局部特征匹配的效果。几项研究[[166](#bib.bib166),
    [167](#bib.bib167), [143](#bib.bib143), [165](#bib.bib165)] 表明，利用几何信息对局部特征匹配具有重要潜力。通过更准确地捕捉像素之间的几何关系，并将几何先验与图像外观信息结合，这些方法能够增强在复杂场景中的匹配鲁棒性和准确性。然而，这一方向为未来的发展带来了诸多机遇和挑战。首先，如何更深刻地建模几何信息，以更好地应对大位移、遮挡和纹理缺失等场景，仍然是一个关键问题。其次，提高置信度估计的性能，以获得更可靠的匹配结果，也是值得探讨的方向。
- en: The introduction of geometric priors expands feature matching beyond mere appearance
    similarity to consider an object’s behavior from different viewpoints. This trend
    suggests that dense matching methods hold promise for tackling challenges posed
    by large displacements and appearance variations. It also implies that the future
    development in the field of geometric matching may increasingly focus on dense
    feature matching, leveraging geometric information and prior knowledge to enhance
    matching performance.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 几何先验的引入将特征匹配从单纯的外观相似性扩展到考虑对象从不同视角的行为。这一趋势表明，密集匹配方法在应对大位移和外观变化带来的挑战方面具有潜力。它还暗示未来几何匹配领域的发展可能会越来越关注密集特征匹配，利用几何信息和先验知识来提高匹配性能。
- en: 8 Conclusions
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We have investigated various algorithms related to local feature matching based
    on deep learning models over the past five years. These algorithms have demonstrated
    impressive performance in various local feature matching tasks and benchmark tests.
    They can be broadly categorized into Detector-based Models and Detector-free Models.
    The application of feature detectors reduces the scope of matching and relies
    on the processes of keypoint detection and feature description. Detector-free
    methods, on the other hand, directly capture richer context from the raw images
    to generate dense matches. Subsequently, we discuss the strengths and weaknesses
    of existing local feature matching algorithms, introduce popular datasets and
    evaluation standards, and summarize the quantitative performance analysis of these
    models on some common benchmarks such as HPatches, ScanNet, YFCC100M, MegaDepth,
    and Aachen Day-Night datasets. Lastly, we explore the open challenges and potential
    research avenues that the field of local feature matching may encounter in the
    forthcoming years. Our aim is not only to enhance researchers’ understanding of
    local feature matching but also to inspire and guide future research endeavors
    in this domain.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去五年中，我们调查了基于深度学习模型的各种局部特征匹配算法。这些算法在各种局部特征匹配任务和基准测试中表现出色。它们可以大致分为基于检测器的模型和无检测器的模型。使用特征检测器的应用减少了匹配的范围，并依赖于关键点检测和特征描述过程。而无检测器的方法则直接从原始图像中捕捉更丰富的上下文，以生成密集匹配。随后，我们讨论了现有局部特征匹配算法的优缺点，介绍了流行的数据集和评估标准，并总结了这些模型在HPatches、ScanNet、YFCC100M、MegaDepth和Aachen
    Day-Night数据集等常见基准上的定量性能分析。最后，我们探讨了局部特征匹配领域在未来几年可能遇到的开放挑战和潜在研究方向。我们的目标不仅是提升研究人员对局部特征匹配的理解，还旨在激发和指导该领域未来的研究工作。
- en: References
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] L. Tang, J. Yuan, J. Ma, Image fusion in the loop of high-level vision
    tasks: A semantic-aware real-time infrared and visible image fusion network, Information
    Fusion 82 (2022) 28–42.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] L. Tang, J. Yuan, J. Ma, 高级视觉任务中的图像融合：一种语义感知的实时红外与可见图像融合网络，信息融合 82 (2022)
    28–42。'
- en: '[2] S.-Y. Cao, B. Yu, L. Luo, R. Zhang, S.-J. Chen, C. Li, H.-L. Shen, Pcnet:
    A structure similarity enhancement method for multispectral and multimodal image
    registration, Information Fusion 94 (2023) 200–214.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S.-Y. Cao, B. Yu, L. Luo, R. Zhang, S.-J. Chen, C. Li, H.-L. Shen, Pcnet：一种用于多光谱和多模态图像配准的结构相似性增强方法，信息融合
    94 (2023) 200–214。'
- en: '[3] M. Hu, B. Sun, X. Kang, S. Li, Multiscale structural feature transform
    for multi-modal image matching, Information Fusion 95 (2023) 341–354.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Hu, B. Sun, X. Kang, S. Li, 多尺度结构特征变换用于多模态图像匹配，信息融合 95 (2023) 341–354。'
- en: '[4] K. Sun, J. Yu, W. Tao, X. Li, C. Tang, Y. Qian, A unified feature-spatial
    cycle consistency fusion framework for robust image matching, Information Fusion
    97 (2023) 101810.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] K. Sun, J. Yu, W. Tao, X. Li, C. Tang, Y. Qian, 一种统一的特征-空间循环一致性融合框架用于鲁棒图像匹配，信息融合
    97 (2023) 101810。'
- en: '[5] Z. Hou, Y. Liu, L. Zhang, Pos-gift: A geometric and intensity-invariant
    feature transformation for multimodal images, Information Fusion 102 (2024) 102027.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Z. Hou, Y. Liu, L. Zhang, Pos-gift：一种用于多模态图像的几何和强度不变特征变换，信息融合 102 (2024)
    102027。'
- en: '[6] T. Sattler, B. Leibe, L. Kobbelt, Improving image-based localization by
    active correspondence search, in: Computer Vision–ECCV 2012: 12th European Conference
    on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part I 12,
    Springer, 2012, pp. 752–765.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. Sattler, B. Leibe, L. Kobbelt, 通过主动对应搜索改进基于图像的定位，收录于：计算机视觉–ECCV 2012：第十二届欧洲计算机视觉大会，意大利佛罗伦萨，2012年10月7-13日，论文集，第I部分
    12，Springer，2012年，页752–765。'
- en: '[7] T. Sattler, A. Torii, J. Sivic, M. Pollefeys, H. Taira, M. Okutomi, T. Pajdla,
    Are large-scale 3d models really necessary for accurate visual localization?,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2017, pp. 1637–1646.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] T. Sattler, A. Torii, J. Sivic, M. Pollefeys, H. Taira, M. Okutomi, T.
    Pajdla, 大规模3D模型是否真的对准确的视觉定位至关重要？收录于：IEEE计算机视觉与模式识别会议论文集，2017年，页1637–1646。'
- en: '[8] S. Cai, Y. Guo, S. Khan, J. Hu, G. Wen, Ground-to-aerial image geo-localization
    with a hard exemplar reweighting triplet loss, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2019, pp. 8391–8400.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. Cai, Y. Guo, S. Khan, J. Hu, G. Wen, 基于硬样本重加权三元组损失的地面到空中图像地理定位，收录于：IEEE/CVF国际计算机视觉大会论文集，2019年，页8391–8400。'
- en: '[9] Z. Zhang, T. Sattler, D. Scaramuzza, Reference pose generation for long-term
    visual localization via learned features and view synthesis, International Journal
    of Computer Vision 129 (2021) 821–844.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Z. Zhang, T. Sattler, D. Scaramuzza, 通过学习特征和视图合成生成长时间视觉定位的参考姿态，国际计算机视觉期刊
    129 (2021) 821–844。'
- en: '[10] S. Agarwal, Y. Furukawa, N. Snavely, I. Simon, B. Curless, S. M. Seitz,
    R. Szeliski, Building rome in a day, Communications of the ACM 54 (10) (2011)
    105–112.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. Agarwal, Y. Furukawa, N. Snavely, I. Simon, B. Curless, S. M. Seitz,
    R. Szeliski, 一天建成罗马，ACM通讯 54 (10) (2011) 105–112。'
- en: '[11] J. Heinly, J. L. Schonberger, E. Dunn, J.-M. Frahm, Reconstructing the
    world* in six days*(as captured by the yahoo 100 million image dataset), in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3287–3295.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Heinly, J. L. Schonberger, E. Dunn, J.-M. Frahm, 在六天内重建世界*（由雅虎1亿图像数据集捕获）*，收录于：IEEE计算机视觉与模式识别会议论文集，2015年，页3287–3295。'
- en: '[12] J. L. Schonberger, J.-M. Frahm, Structure-from-motion revisited, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, 2016, pp. 4104–4113.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. L. Schonberger, J.-M. Frahm, 运动重建的再探讨，收录于：IEEE计算机视觉与模式识别会议论文集，2016年，页4104–4113。'
- en: '[13] J. Wang, Y. Zhong, Y. Dai, S. Birchfield, K. Zhang, N. Smolyanskiy, H. Li,
    Deep two-view structure-from-motion revisited, in: Proceedings of the IEEE/CVF
    conference on Computer Vision and Pattern Recognition, 2021, pp. 8953–8962.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Wang, Y. Zhong, Y. Dai, S. Birchfield, K. Zhang, N. Smolyanskiy, H.
    Li, 深度双视图运动重建的再探讨，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，2021年，页8953–8962。'
- en: '[14] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
    I. Reid, J. J. Leonard, Past, present, and future of simultaneous localization
    and mapping: Toward the robust-perception age, IEEE Transactions on robotics 32 (6)
    (2016) 1309–1332.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
    I. Reid, J. J. Leonard, 同步定位与建图的过去、现在和未来：迈向强鲁棒感知时代，IEEE机器人学汇刊 32 (6) (2016) 1309–1332。'
- en: '[15] R. Mur-Artal, J. D. Tardós, Orb-slam2: An open-source slam system for
    monocular, stereo, and rgb-d cameras, IEEE transactions on robotics 33 (5) (2017)
    1255–1262.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] R. Mur-Artal, J. D. Tardós, Orb-slam2：一个开源的单目、立体和RGB-D相机SLAM系统，《IEEE机器人学汇刊》33（5）（2017年）第1255–1262页。'
- en: '[16] Y. Zhao, S. Xu, S. Bu, H. Jiang, P. Han, Gslam: A general slam framework
    and benchmark, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, 2019, pp. 1110–1120.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y. Zhao, S. Xu, S. Bu, H. Jiang, P. Han, Gslam：一个通用的SLAM框架和基准，发表于：IEEE/CVF国际计算机视觉大会，2019年，第1110–1120页。'
- en: '[17] C. Liu, J. Yuen, A. Torralba, Sift flow: Dense correspondence across scenes
    and its applications, IEEE transactions on pattern analysis and machine intelligence
    33 (5) (2010) 978–994.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] C. Liu, J. Yuen, A. Torralba, Sift流：跨场景的密集对应及其应用，《IEEE模式分析与机器智能汇刊》33（5）（2010年）第978–994页。'
- en: '[18] P. Weinzaepfel, J. Revaud, Z. Harchaoui, C. Schmid, Deepflow: Large displacement
    optical flow with deep matching, in: Proceedings of the IEEE international conference
    on computer vision, 2013, pp. 1385–1392.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] P. Weinzaepfel, J. Revaud, Z. Harchaoui, C. Schmid, Deepflow：具有深度匹配的大位移光流，发表于：IEEE国际计算机视觉大会，2013年，第1385–1392页。'
- en: '[19] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
    P. Van Der Smagt, D. Cremers, T. Brox, Flownet: Learning optical flow with convolutional
    networks, in: Proceedings of the IEEE international conference on computer vision,
    2015, pp. 2758–2766.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,
    P. Van Der Smagt, D. Cremers, T. Brox, Flownet：利用卷积网络学习光流，发表于：IEEE国际计算机视觉大会，2015年，第2758–2766页。'
- en: '[20] F. Radenović, G. Tolias, O. Chum, Fine-tuning cnn image retrieval with
    no human annotation, IEEE transactions on pattern analysis and machine intelligence
    41 (7) (2018) 1655–1668.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] F. Radenović, G. Tolias, O. Chum, 无需人工标注的CNN图像检索微调，《IEEE模式分析与机器智能汇刊》41（7）（2018年）第1655–1668页。'
- en: '[21] B. Cao, A. Araujo, J. Sim, Unifying deep local and global features for
    image search, in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
    UK, August 23–28, 2020, Proceedings, Part XX 16, Springer, 2020, pp. 726–743.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] B. Cao, A. Araujo, J. Sim, 统一深度局部与全局特征用于图像检索，发表于：计算机视觉–ECCV 2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，会议论文集，部分XX
    16，Springer，2020年，第726–743页。'
- en: '[22] P. Chhabra, N. K. Garg, M. Kumar, Content-based image retrieval system
    using orb and sift features, Neural Computing and Applications 32 (2020) 2725–2733.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] P. Chhabra, N. K. Garg, M. Kumar, 基于内容的图像检索系统使用ORB和SIFT特征，《神经计算与应用》32（2020年）第2725–2733页。'
- en: '[23] D. Zhang, H. Li, W. Cong, R. Xu, J. Dong, X. Chen, Task relation distillation
    and prototypical pseudo label for incremental named entity recognition, in: Proceedings
    of the 32nd ACM International Conference on Information and Knowledge Management,
    2023, pp. 3319–3329.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] D. Zhang, H. Li, W. Cong, R. Xu, J. Dong, X. Chen, 任务关系蒸馏与增量命名实体识别的原型伪标签，发表于：第32届ACM国际信息与知识管理大会，2023年，第3319–3329页。'
- en: '[24] K. Wang, X. Fu, Y. Huang, C. Cao, G. Shi, Z.-J. Zha, Generalized uav object
    detection via frequency domain disentanglement, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2023, pp. 1064–1073.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] K. Wang, X. Fu, Y. Huang, C. Cao, G. Shi, Z.-J. Zha, 通过频域解耦的广义无人机目标检测，发表于：IEEE/CVF计算机视觉与模式识别大会，2023年，第1064–1073页。'
- en: '[25] C. Cao, X. Fu, H. Liu, Y. Huang, K. Wang, J. Luo, Z.-J. Zha, Event-guided
    person re-identification via sparse-dense complementary learning, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
    17990–17999.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] C. Cao, X. Fu, H. Liu, Y. Huang, K. Wang, J. Luo, Z.-J. Zha, 事件引导的稀疏-密集互补学习的人物重识别，发表于：IEEE/CVF计算机视觉与模式识别大会，2023年，第17990–17999页。'
- en: '[26] C. Harris, M. Stephens, et al., A combined corner and edge detector, in:
    Alvey vision conference, Vol. 15, Citeseer, 1988, pp. 10–5244.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] C. Harris, M. Stephens, 等人，结合角点和边缘检测器，发表于：Alvey视觉会议，第15卷，Citeseer，1988年，第10–5244页。'
- en: '[27] S. M. Smith, J. M. Brady, Susan—a new approach to low level image processing,
    International journal of computer vision 23 (1) (1997) 45–78.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. M. Smith, J. M. Brady, Susan—一种低级图像处理的新方法，《国际计算机视觉期刊》23（1）（1997年）第45–78页。'
- en: '[28] E. Rosten, T. Drummond, Fusing points and lines for high performance tracking,
    in: Tenth IEEE International Conference on Computer Vision (ICCV’05) Volume 1,
    Vol. 2, Ieee, 2005, pp. 1508–1515.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] E. Rosten, T. Drummond, 通过融合点与线实现高性能跟踪，发表于：第十届IEEE国际计算机视觉大会（ICCV’05）第1卷，第2卷，IEEE，2005年，第1508–1515页。'
- en: '[29] J. Matas, O. Chum, M. Urban, T. Pajdla, Robust wide-baseline stereo from
    maximally stable extremal regions, Image and vision computing 22 (10) (2004) 761–767.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Matas, O. Chum, M. Urban, T. Pajdla, 从最大稳定极值区域获取鲁棒宽基线立体视觉, Image and
    vision computing 22 (10) (2004) 761–767.'
- en: '[30] N. Dalal, B. Triggs, Histograms of oriented gradients for human detection,
    in: 2005 IEEE computer society conference on computer vision and pattern recognition
    (CVPR’05), Vol. 1, Ieee, 2005, pp. 886–893.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] N. Dalal, B. Triggs, 面向人体检测的方向梯度直方图, 在：2005年IEEE计算机学会计算机视觉与模式识别会议 (CVPR’05)，第1卷，Ieee，2005，第886–893页。'
- en: '[31] K. Mikolajczyk, C. Schmid, A performance evaluation of local descriptors,
    IEEE transactions on pattern analysis and machine intelligence 27 (10) (2005)
    1615–1630.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] K. Mikolajczyk, C. Schmid, 局部描述符性能评估, IEEE transactions on pattern analysis
    and machine intelligence 27 (10) (2005) 1615–1630.'
- en: '[32] M. Calonder, V. Lepetit, C. Strecha, P. Fua, Brief: Binary robust independent
    elementary features, in: Computer Vision–ECCV 2010: 11th European Conference on
    Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings,
    Part IV 11, Springer, 2010, pp. 778–792.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. Calonder, V. Lepetit, C. Strecha, P. Fua, Brief: 二进制稳健独立基本特征, 在：计算机视觉–ECCV
    2010：第11届欧洲计算机视觉会议，希腊克里特岛，2010年9月5-11日，会议录，第IV部分11，Springer，2010，第778–792页。'
- en: '[33] D. G. Lowe, Distinctive image features from scale-invariant keypoints,
    International journal of computer vision 60 (2004) 91–110.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] D. G. Lowe, 从尺度不变关键点提取独特图像特征, International journal of computer vision
    60 (2004) 91–110.'
- en: '[34] H. Bay, T. Tuytelaars, L. Van Gool, Surf: Speeded up robust features,
    Lecture notes in computer science 3951 (2006) 404–417.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] H. Bay, T. Tuytelaars, L. Van Gool, Surf: 加速稳健特征, Lecture notes in computer
    science 3951 (2006) 404–417.'
- en: '[35] E. Rublee, V. Rabaud, K. Konolige, G. Bradski, Orb: An efficient alternative
    to sift or surf, in: 2011 International conference on computer vision, Ieee, 2011,
    pp. 2564–2571.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] E. Rublee, V. Rabaud, K. Konolige, G. Bradski, Orb: SIFT 或 SURF 的高效替代方案,
    在：2011年国际计算机视觉会议，Ieee，2011，第2564–2571页。'
- en: '[36] S. Leutenegger, M. Chli, R. Y. Siegwart, Brisk: Binary robust invariant
    scalable keypoints, in: 2011 International conference on computer vision, Ieee,
    2011, pp. 2548–2555.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Leutenegger, M. Chli, R. Y. Siegwart, Brisk: 二进制稳健不变可缩放关键点, 在：2011年国际计算机视觉会议，Ieee，2011，第2548–2555页。'
- en: '[37] P. F. Alcantarilla, A. Bartoli, A. J. Davison, Kaze features, in: Computer
    Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy,
    October 7-13, 2012, Proceedings, Part VI 12, Springer, 2012, pp. 214–227.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] P. F. Alcantarilla, A. Bartoli, A. J. Davison, Kaze 特征, 在：计算机视觉–ECCV 2012：第12届欧洲计算机视觉会议，意大利佛罗伦萨，2012年10月7-13日，会议录，第VI部分12，Springer，2012，第214–227页。'
- en: '[38] P. F. Alcantarilla, T. Solutions, Fast explicit diffusion for accelerated
    features in nonlinear scale spaces, IEEE Trans. Patt. Anal. Mach. Intell 34 (7)
    (2011) 1281–1298.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] P. F. Alcantarilla, T. Solutions, 快速显式扩散用于加速非线性尺度空间中的特征, IEEE Trans. Patt.
    Anal. Mach. Intell 34 (7) (2011) 1281–1298.'
- en: '[39] M. A. Fischler, R. C. Bolles, Random sample consensus: a paradigm for
    model fitting with applications to image analysis and automated cartography, Communications
    of the ACM 24 (6) (1981) 381–395.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. A. Fischler, R. C. Bolles, 随机抽样一致性：一种模型拟合范式及其在图像分析和自动制图中的应用, Communications
    of the ACM 24 (6) (1981) 381–395.'
- en: '[40] R. Xu, C. Wang, B. Fan, Y. Zhang, S. Xu, W. Meng, X. Zhang, Domaindesc:
    Learning local descriptors with domain adaptation, in: ICASSP 2022-2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2022, pp.
    2505–2509.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] R. Xu, C. Wang, B. Fan, Y. Zhang, S. Xu, W. Meng, X. Zhang, Domaindesc:
    通过领域自适应学习局部描述符, 在：ICASSP 2022-2022 IEEE国际声学、语音和信号处理会议 (ICASSP)，IEEE，2022，第2505–2509页。'
- en: '[41] R. Xu, C. Wang, S. Xu, W. Meng, Y. Zhang, B. Fan, X. Zhang, Domainfeat:
    Learning local features with domain adaptation, IEEE Transactions on Circuits
    and Systems for Video Technology (2023).'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] R. Xu, C. Wang, S. Xu, W. Meng, Y. Zhang, B. Fan, X. Zhang, Domainfeat:
    通过领域自适应学习局部特征, IEEE Transactions on Circuits and Systems for Video Technology
    (2023).'
- en: '[42] J. Wu, R. Xu, Z. Wood-Doughty, C. Wang, Segment anything model is a good
    teacher for local feature learning, arXiv preprint arXiv:2309.16992 (2023).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. Wu, R. Xu, Z. Wood-Doughty, C. Wang, Segment anything model is a good
    teacher for local feature learning, arXiv 预印本 arXiv:2309.16992 (2023).'
- en: '[43] X. Jiang, J. Ma, G. Xiao, Z. Shao, X. Guo, A review of multimodal image
    matching: Methods and applications, Information Fusion 73 (2021) 22–71.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] X. Jiang, J. Ma, G. Xiao, Z. Shao, X. Guo, 多模态图像匹配综述：方法与应用, Information
    Fusion 73 (2021) 22–71.'
- en: '[44] R. Xu, Y. Li, C. Wang, S. Xu, W. Meng, X. Zhang, Instance segmentation
    of biological images using graph convolutional network, Engineering Applications
    of Artificial Intelligence 110 (2022) 104739.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Xu, Y. Li, C. Wang, S. Xu, W. Meng, X. Zhang, 使用图卷积网络进行生物图像实例分割，《工程应用人工智能》110（2022）104739。'
- en: '[45] R. Xu, C. Wang, S. Xu, W. Meng, X. Zhang, Dc-net: Dual context network
    for 2d medical image segmentation, in: Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part I 24, Springer, 2021, pp. 503–513.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] R. Xu, C. Wang, S. Xu, W. Meng, X. Zhang, Dc-net：用于二维医学图像分割的双重上下文网络，发表于《医学图像计算与计算机辅助干预–MICCAI
    2021：第24届国际会议》，法国斯特拉斯堡，2021年9月27日至10月1日，论文集第I卷24，Springer，2021，页503–513。'
- en: '[46] R. Xu, C. Wang, S. Xu, W. Meng, X. Zhang, Wave-like class activation map
    with representation fusion for weakly-supervised semantic segmentation, IEEE Transactions
    on Multimedia (2023).'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Xu, C. Wang, S. Xu, W. Meng, X. Zhang, 具有表示融合的波状类别激活图用于弱监督语义分割，《IEEE多媒体学报》（2023）。'
- en: '[47] W. Xu, R. Xu, C. Wang, S. Xu, L. Guo, M. Zhang, X. Zhang, Spectral prompt
    tuning: Unveiling unseen classes for zero-shot semantic segmentation, arXiv preprint
    arXiv:2312.12754 (2023).'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] W. Xu, R. Xu, C. Wang, S. Xu, L. Guo, M. Zhang, X. Zhang, 频谱提示调整：揭示零样本语义分割中的未见类别，arXiv预印本
    arXiv:2312.12754（2023）。'
- en: '[48] C. Wang, R. Xu, S. Xu, W. Meng, X. Zhang, Treating pseudo-labels generation
    as image matting for weakly supervised semantic segmentation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 755–765.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] C. Wang, R. Xu, S. Xu, W. Meng, X. Zhang, 将伪标签生成视为图像抠图用于弱监督语义分割，发表于《IEEE/CVF国际计算机视觉会议论文集》，2023，页755–765。'
- en: '[49] M. Awrangjeb, G. Lu, C. S. Fraser, Performance comparisons of contour-based
    corner detectors, IEEE Transactions on Image Processing 21 (9) (2012) 4167–4179.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. Awrangjeb, G. Lu, C. S. Fraser, 基于轮廓的角点检测器性能比较，《IEEE图像处理学报》21（9）（2012）4167–4179。'
- en: '[50] Y. Li, S. Wang, Q. Tian, X. Ding, A survey of recent advances in visual
    feature detection, Neurocomputing 149 (2015) 736–751.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Y. Li, S. Wang, Q. Tian, X. Ding, 视觉特征检测的最新进展综述，《神经计算》149（2015）736–751。'
- en: '[51] S. Krig, S. Krig, Interest point detector and feature descriptor survey,
    Computer Vision Metrics: Textbook Edition (2016) 187–246.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] S. Krig, S. Krig, 关键点检测器和特征描述符综述，《计算机视觉度量：教材版》（2016）187–246。'
- en: '[52] K. Joshi, M. I. Patel, Recent advances in local feature detector and descriptor:
    a literature survey, International Journal of Multimedia Information Retrieval
    9 (4) (2020) 231–247.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] K. Joshi, M. I. Patel, 局部特征检测器和描述符的最新进展：文献综述，《多媒体信息检索国际期刊》9（4）（2020）231–247。'
- en: '[53] J. Ma, X. Jiang, A. Fan, J. Jiang, J. Yan, Image matching from handcrafted
    to deep features: A survey, International Journal of Computer Vision 129 (2021)
    23–79.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Ma, X. Jiang, A. Fan, J. Jiang, J. Yan, 从手工特征到深度特征的图像匹配：综述，《计算机视觉国际期刊》129（2021）23–79。'
- en: '[54] J. Jing, T. Gao, W. Zhang, Y. Gao, C. Sun, Image feature information extraction
    for interest point detection: A comprehensive review, IEEE Transactions on Pattern
    Analysis and Machine Intelligence (2022).'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] J. Jing, T. Gao, W. Zhang, Y. Gao, C. Sun, 关键点检测的图像特征信息提取：综合评述，《IEEE模式分析与机器智能学报》（2022）。'
- en: '[55] F. Bellavia, C. Colombo, L. Morelli, F. Remondino, Challenges in image
    matching for cultural heritage: an overview and perspective, in: International
    Conference on Image Analysis and Processing, Springer, 2022, pp. 210–222.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] F. Bellavia, C. Colombo, L. Morelli, F. Remondino, 文化遗产图像匹配中的挑战：概述与展望，发表于《国际图像分析与处理会议》，Springer，2022，页210–222。'
- en: '[56] G. Haskins, U. Kruger, P. Yan, Deep learning in medical image registration:
    a survey, Machine Vision and Applications 31 (2020) 1–18.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] G. Haskins, U. Kruger, P. Yan, 医学图像配准中的深度学习：综述，《机器视觉与应用》31（2020）1–18。'
- en: '[57] S. Bharati, M. Mondal, P. Podder, V. Prasath, Deep learning for medical
    image registration: A comprehensive review, arXiv preprint arXiv:2204.11341 (2022).'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S. Bharati, M. Mondal, P. Podder, V. Prasath, 医学图像配准的深度学习：综合评述，arXiv预印本
    arXiv:2204.11341（2022）。'
- en: '[58] J. Chen, Y. Liu, S. Wei, Z. Bian, S. Subramanian, A. Carass, J. L. Prince,
    Y. Du, A survey on deep learning in medical image registration: New technologies,
    uncertainty, evaluation metrics, and beyond, arXiv preprint arXiv:2307.15615 (2023).'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] J. Chen, Y. Liu, S. Wei, Z. Bian, S. Subramanian, A. Carass, J. L. Prince,
    Y. Du, 医学图像配准中的深度学习综述：新技术、不确定性、评估指标及其扩展，arXiv预印本 arXiv:2307.15615（2023）。'
- en: '[59] S. Paul, U. C. Pati, A comprehensive review on remote sensing image registration,
    International Journal of Remote Sensing 42 (14) (2021) 5396–5432.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] S. Paul, U. C. Pati, 关于遥感图像配准的全面综述，《国际遥感杂志》42 (14) (2021) 5396–5432。'
- en: '[60] B. Zhu, L. Zhou, S. Pu, J. Fan, Y. Ye, Advances and challenges in multimodal
    remote sensing image registration, IEEE Journal on Miniaturization for Air and
    Space Systems (2023).'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] B. Zhu, L. Zhou, S. Pu, J. Fan, Y. Ye, 多模态遥感图像配准的进展与挑战，《IEEE小型化航空与空间系统杂志》（2023）。'
- en: '[61] D. DeTone, T. Malisiewicz, A. Rabinovich, Superpoint: Self-supervised
    interest point detection and description, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition workshops, 2018, pp. 224–236.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] D. DeTone, T. Malisiewicz, A. Rabinovich, Superpoint: 自监督兴趣点检测与描述，见：IEEE计算机视觉与模式识别研讨会论文集，2018年，页码224–236。'
- en: '[62] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, T. Sattler,
    D2-net: A trainable cnn for joint description and detection of local features,
    in: Proceedings of the ieee/cvf conference on computer vision and pattern recognition,
    2019, pp. 8092–8101.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, T. Sattler,
    D2-net: 用于局部特征的联合描述和检测的可训练cnn，见：IEEE/CVF计算机视觉与模式识别会议论文集，2019年，页码8092–8101。'
- en: '[63] J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon,
    M. Humenberger, R2d2: repeatable and reliable detector and descriptor, arXiv preprint
    arXiv:1906.06195 (2019).'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon,
    M. Humenberger, R2d2: 可重复且可靠的检测器和描述符，arXiv预印本 arXiv:1906.06195 (2019)。'
- en: '[64] I. Rocco, M. Cimpoi, R. Arandjelović, A. Torii, T. Pajdla, J. Sivic, Neighbourhood
    consensus networks, Advances in neural information processing systems 31 (2018).'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] I. Rocco, M. Cimpoi, R. Arandjelović, A. Torii, T. Pajdla, J. Sivic, 邻域共识网络，《神经信息处理系统进展》31
    (2018)。'
- en: '[65] I. Rocco, R. Arandjelović, J. Sivic, Efficient neighbourhood consensus
    networks via submanifold sparse convolutions, in: Computer Vision–ECCV 2020: 16th
    European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16,
    Springer, 2020, pp. 605–621.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] I. Rocco, R. Arandjelović, J. Sivic, 通过子流形稀疏卷积实现高效的邻域共识网络，见：《计算机视觉–ECCV
    2020: 第16届欧洲会议》，英国格拉斯哥，2020年8月23–28日，论文集，第IX部分16，Springer，2020年，页码605–621。'
- en: '[66] X. Li, K. Han, S. Li, V. Prisacariu, Dual-resolution correspondence networks,
    Advances in Neural Information Processing Systems 33 (2020) 17346–17357.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] X. Li, K. Han, S. Li, V. Prisacariu, 双分辨率对应网络，《神经信息处理系统进展》33 (2020) 17346–17357。'
- en: '[67] P. Truong, M. Danelljan, R. Timofte, Glu-net: Global-local universal network
    for dense flow and correspondences, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2020, pp. 6258–6268.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] P. Truong, M. Danelljan, R. Timofte, Glu-net: 用于密集流动和对应关系的全局-局部通用网络，见：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，页码6258–6268。'
- en: '[68] P. Truong, M. Danelljan, L. Van Gool, R. Timofte, Learning accurate dense
    correspondences and when to trust them, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2021, pp. 5714–5724.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] P. Truong, M. Danelljan, L. Van Gool, R. Timofte, 学习准确的密集对应关系及其信任程度，见：IEEE/CVF计算机视觉与模式识别会议论文集，2021年，页码5714–5724。'
- en: '[69] P.-E. Sarlin, D. DeTone, T. Malisiewicz, A. Rabinovich, Superglue: Learning
    feature matching with graph neural networks, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2020, pp. 4938–4947.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] P.-E. Sarlin, D. DeTone, T. Malisiewicz, A. Rabinovich, Superglue: 使用图神经网络学习特征匹配，见：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，页码4938–4947。'
- en: '[70] H. Chen, Z. Luo, J. Zhang, L. Zhou, X. Bai, Z. Hu, C.-L. Tai, L. Quan,
    Learning to match features with seeded graph matching network, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6301–6310.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] H. Chen, Z. Luo, J. Zhang, L. Zhou, X. Bai, Z. Hu, C.-L. Tai, L. Quan,
    学习通过种子图匹配网络匹配特征，见：IEEE/CVF计算机视觉国际会议论文集，2021年，页码6301–6310。'
- en: '[71] Y. Shi, J.-X. Cai, Y. Shavit, T.-J. Mu, W. Feng, K. Zhang, Clustergnn:
    Cluster-based coarse-to-fine graph neural network for efficient feature matching,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2022, pp. 12517–12526.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Y. Shi, J.-X. Cai, Y. Shavit, T.-J. Mu, W. Feng, K. Zhang, Clustergnn:
    基于簇的粗到细图神经网络，用于高效特征匹配，见：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，页码12517–12526。'
- en: '[72] J. Sun, Z. Shen, Y. Wang, H. Bao, X. Zhou, Loftr: Detector-free local
    feature matching with transformers, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2021, pp. 8922–8931.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] J. Sun, Z. Shen, Y. Wang, H. Bao, X. Zhou, Loftr：无检测器的局部特征匹配与变换器，见：2021年IEEE/CVF计算机视觉与模式识别会议论文集，第8922–8931页。'
- en: '[73] H. Chen, Z. Luo, L. Zhou, Y. Tian, M. Zhen, T. Fang, D. McKinnon, Y. Tsin,
    L. Quan, Aspanformer: Detector-free image matching with adaptive span transformer,
    in: Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October
    23–27, 2022, Proceedings, Part XXXII, Springer, 2022, pp. 20–36.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] H. Chen, Z. Luo, L. Zhou, Y. Tian, M. Zhen, T. Fang, D. McKinnon, Y. Tsin,
    L. Quan, Aspanformer：无检测器的自适应跨度变换图像匹配，见：计算机视觉–ECCV 2022：第17届欧洲会议，特拉维夫，以色列，2022年10月23–27日，论文集，第三十二部分，施普林格，2022年，第20–36页。'
- en: '[74] J. Zhang, L. Dai, F. Meng, Q. Fan, X. Chen, K. Xu, H. Wang, 3d-aware object
    goal navigation via simultaneous exploration and identification, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
    6672–6682.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Zhang, L. Dai, F. Meng, Q. Fan, X. Chen, K. Xu, H. Wang, 通过同时探索和识别进行3D感知对象目标导航，见：2023年IEEE/CVF计算机视觉与模式识别会议论文集，第6672–6682页。'
- en: '[75] J. Zhang, Y. Tang, H. Wang, K. Xu, Asro-dio: Active subspace random optimization
    based depth inertial odometry, IEEE Transactions on Robotics 39 (2) (2022) 1496–1508.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] J. Zhang, Y. Tang, H. Wang, K. Xu, Asro-dio：基于活动子空间随机优化的深度惯性测距，IEEE机器人学报
    39 (2) (2022) 1496–1508。'
- en: '[76] J. Bian, W.-Y. Lin, Y. Matsushita, S.-K. Yeung, T.-D. Nguyen, M.-M. Cheng,
    Gms: Grid-based motion statistics for fast, ultra-robust feature correspondence,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2017, pp. 4181–4190.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] J. Bian, W.-Y. Lin, Y. Matsushita, S.-K. Yeung, T.-D. Nguyen, M.-M. Cheng,
    Gms：基于网格的运动统计，用于快速、超鲁棒的特征对应，见：2017年IEEE计算机视觉与模式识别会议论文集，第4181–4190页。'
- en: '[77] J. Zhang, D. Sun, Z. Luo, A. Yao, L. Zhou, T. Shen, Y. Chen, L. Quan,
    H. Liao, Learning two-view correspondences and geometry using order-aware network,
    in: Proceedings of the IEEE/CVF international conference on computer vision, 2019,
    pp. 5845–5854.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] J. Zhang, D. Sun, Z. Luo, A. Yao, L. Zhou, T. Shen, Y. Chen, L. Quan,
    H. Liao, 使用有序网络学习双视图对应关系和几何，见：2019年IEEE/CVF国际计算机视觉会议论文集，第5845–5854页。'
- en: '[78] K. M. Yi, E. Trulls, V. Lepetit, P. Fua, Lift: Learned invariant feature
    transform, in: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam,
    The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, Springer, 2016,
    pp. 467–483.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] K. M. Yi, E. Trulls, V. Lepetit, P. Fua, Lift：学习的恒定特征变换，见：计算机视觉–ECCV 2016：第14届欧洲会议，阿姆斯特丹，荷兰，2016年10月11-14日，论文集，第六部分14，施普林格，2016年，第467–483页。'
- en: '[79] M. Brown, G. Hua, S. Winder, Discriminative learning of local image descriptors,
    IEEE transactions on pattern analysis and machine intelligence 33 (1) (2010) 43–57.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] M. Brown, G. Hua, S. Winder, 局部图像描述符的判别学习，IEEE模式分析与机器智能汇刊 33 (1) (2010)
    43–57。'
- en: '[80] Y. Tian, B. Fan, F. Wu, L2-net: Deep learning of discriminative patch
    descriptor in euclidean space, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2017, pp. 661–669.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Tian, B. Fan, F. Wu, L2-net：欧几里得空间中判别性补丁描述符的深度学习，见：2017年IEEE计算机视觉与模式识别会议论文集，第661–669页。'
- en: '[81] K. M. Yi, Y. Verdie, P. Fua, V. Lepetit, Learning to assign orientations
    to feature points, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2016, pp. 107–116.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] K. M. Yi, Y. Verdie, P. Fua, V. Lepetit, 学习为特征点分配方向，见：2016年IEEE计算机视觉与模式识别会议论文集，第107–116页。'
- en: '[82] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, R. Shah, Signature verification
    using a” siamese” time delay neural network, Advances in neural information processing
    systems 6 (1993).'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, R. Shah, 使用“孪生”时间延迟神经网络进行签名验证，神经信息处理系统进展
    6 (1993)。'
- en: '[83] A. Mishchuk, D. Mishkin, F. Radenovic, J. Matas, Working hard to know
    your neighbor’s margins: Local descriptor learning loss, Advances in neural information
    processing systems 30 (2017).'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] A. Mishchuk, D. Mishkin, F. Radenovic, J. Matas, 努力了解邻居的边际：局部描述符学习损失，神经信息处理系统进展
    30 (2017)。'
- en: '[84] K. He, Y. Lu, S. Sclaroff, Local descriptors optimized for average precision,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2018, pp. 596–605.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] K. He, Y. Lu, S. Sclaroff, 为平均精度优化的局部描述符，见：2018年IEEE计算机视觉与模式识别会议论文集，第596–605页。'
- en: '[85] X. Wei, Y. Zhang, Y. Gong, N. Zheng, Kernelized subspace pooling for deep
    local descriptors, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2018, pp. 1867–1875.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] X. Wei, Y. Zhang, Y. Gong, N. Zheng, 核化子空间池化用于深度局部描述符，载于：IEEE计算机视觉与模式识别会议论文集，2018，第1867–1875页。'
- en: '[86] K. Lin, J. Lu, C.-S. Chen, J. Zhou, M.-T. Sun, Unsupervised deep learning
    of compact binary descriptors, IEEE transactions on pattern analysis and machine
    intelligence 41 (6) (2018) 1501–1514.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] K. Lin, J. Lu, C.-S. Chen, J. Zhou, M.-T. Sun, 无监督深度学习紧凑的二进制描述符，《IEEE模式分析与机器智能汇刊》41（6）（2018）1501–1514。'
- en: '[87] M. Zieba, P. Semberecki, T. El-Gaaly, T. Trzcinski, Bingan: Learning compact
    binary descriptors with a regularized gan, Advances in neural information processing
    systems 31 (2018).'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] M. Zieba, P. Semberecki, T. El-Gaaly, T. Trzcinski, Bingan: 通过正则化GAN学习紧凑的二进制描述符，《神经信息处理系统进展》31（2018）。'
- en: '[88] L. Wei, S. Zhang, H. Yao, W. Gao, Q. Tian, Glad: Global–local-alignment
    descriptor for scalable person re-identification, IEEE Transactions on Multimedia
    21 (4) (2018) 986–999.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] L. Wei, S. Zhang, H. Yao, W. Gao, Q. Tian, Glad: 用于可扩展人脸重识别的全局–局部–对齐描述符，《IEEE多媒体汇刊》21（4）（2018）986–999。'
- en: '[89] Z. Luo, T. Shen, L. Zhou, S. Zhu, R. Zhang, Y. Yao, T. Fang, L. Quan,
    Geodesc: Learning local descriptors by integrating geometry constraints, in: Proceedings
    of the European conference on computer vision (ECCV), 2018, pp. 168–183.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Z. Luo, T. Shen, L. Zhou, S. Zhu, R. Zhang, Y. Yao, T. Fang, L. Quan,
    Geodesc: 通过整合几何约束学习局部描述符，载于：欧洲计算机视觉会议（ECCV）论文集，2018，第168–183页。'
- en: '[90] Y. Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, X. Zhou, Gift: Learning transformation-invariant
    dense visual descriptors via group cnns, Advances in Neural Information Processing
    Systems 32 (2019).'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Y. Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, X. Zhou, Gift: 通过组CNN学习变换不变的密集视觉描述符，《神经信息处理系统进展》32（2019）。'
- en: '[91] J. Lee, Y. Jeong, S. Kim, J. Min, M. Cho, Learning to distill convolutional
    features into compact local descriptors, in: Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, 2021, pp. 898–908.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] J. Lee, Y. Jeong, S. Kim, J. Min, M. Cho, 学习将卷积特征蒸馏为紧凑的局部描述符，载于：IEEE/CVF冬季计算机视觉应用会议论文集，2021，第898–908页。'
- en: '[92] Y. Tian, X. Yu, B. Fan, F. Wu, H. Heijnen, V. Balntas, Sosnet: Second
    order similarity regularization for local descriptor learning, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp.
    11016–11025.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Y. Tian, X. Yu, B. Fan, F. Wu, H. Heijnen, V. Balntas, Sosnet: 用于局部描述符学习的二阶相似性正则化，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2019，第11016–11025页。'
- en: '[93] P. Ebel, A. Mishchuk, K. M. Yi, P. Fua, E. Trulls, Beyond cartesian representations
    for local descriptors, in: Proceedings of the IEEE/CVF international conference
    on computer vision, 2019, pp. 253–262.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] P. Ebel, A. Mishchuk, K. M. Yi, P. Fua, E. Trulls, 超越笛卡尔表示的局部描述符，载于：IEEE/CVF国际计算机视觉大会论文集，2019，第253–262页。'
- en: '[94] Y. Tian, A. Barroso Laguna, T. Ng, V. Balntas, K. Mikolajczyk, Hynet:
    Learning local descriptor with hybrid similarity measure and triplet loss, Advances
    in neural information processing systems 33 (2020) 7401–7412.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. Tian, A. Barroso Laguna, T. Ng, V. Balntas, K. Mikolajczyk, Hynet:
    通过混合相似度度量和三元组损失学习局部描述符，《神经信息处理系统进展》33（2020）7401–7412。'
- en: '[95] C. Wang, R. Xu, S. Xu, W. Meng, X. Zhang, Cndesc: Cross normalization
    for local descriptors learning, IEEE Transactions on Multimedia (2022).'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] C. Wang, R. Xu, S. Xu, W. Meng, X. Zhang, Cndesc: 用于局部描述符学习的交叉归一化，《IEEE多媒体汇刊》（2022）。'
- en: '[96] A. Barroso-Laguna, E. Riba, D. Ponsa, K. Mikolajczyk, Key. net: Keypoint
    detection by handcrafted and learned cnn filters, in: Proceedings of the IEEE/CVF
    international conference on computer vision, 2019, pp. 5836–5844.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] A. Barroso-Laguna, E. Riba, D. Ponsa, K. Mikolajczyk, Key. net: 通过手工和学习的CNN滤波器进行关键点检测，载于：IEEE/CVF国际计算机视觉大会论文集，2019，第5836–5844页。'
- en: '[97] X. Zhao, X. Wu, J. Miao, W. Chen, P. C. Chen, Z. Li, Alike: Accurate and
    lightweight keypoint detection and descriptor extraction, IEEE Transactions on
    Multimedia (2022).'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] X. Zhao, X. Wu, J. Miao, W. Chen, P. C. Chen, Z. Li, Alike: 准确且轻量级的关键点检测与描述符提取，《IEEE多媒体汇刊》（2022）。'
- en: '[98] M. Kanakis, S. Maurer, M. Spallanzani, A. Chhatkuli, L. Van Gool, Zippypoint:
    Fast interest point detection, description, and matching through mixed precision
    discretization, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2023, pp. 6113–6122.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Kanakis, S. Maurer, M. Spallanzani, A. Chhatkuli, L. Van Gool, Zippypoint:
    通过混合精度离散化实现快速兴趣点检测、描述和匹配，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2023，第6113–6122页。'
- en: '[99] J. Tang, H. Kim, V. Guizilini, S. Pillai, A. Rares, Neural outlier rejection
    for self-supervised keypoint learning, in: 8th International Conference on Learning
    Representations, ICLR 2020, International Conference on Learning Representations,
    ICLR, 2020.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] J. Tang, H. Kim, V. Guizilini, S. Pillai, A. Rares, 神经异常点排除用于自监督关键点学习,
    见: 第八届国际学习表征会议, ICLR 2020, 国际学习表征会议, ICLR, 2020.'
- en: '[100] Z. Luo, T. Shen, L. Zhou, J. Zhang, Y. Yao, S. Li, T. Fang, L. Quan,
    Contextdesc: Local descriptor augmentation with cross-modality context, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp.
    2527–2536.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Z. Luo, T. Shen, L. Zhou, J. Zhang, Y. Yao, S. Li, T. Fang, L. Quan,
    Contextdesc: 跨模态上下文的局部描述符增强, 见: IEEE/CVF 计算机视觉与模式识别会议论文集, 2019, 页2527–2536.'
- en: '[101] C. Wang, R. Xu, Y. Zhang, S. Xu, W. Meng, B. Fan, X. Zhang, Mtldesc:
    Looking wider to describe better, in: Proceedings of the AAAI Conference on Artificial
    Intelligence, Vol. 36, 2022, pp. 2388–2396.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] C. Wang, R. Xu, Y. Zhang, S. Xu, W. Meng, B. Fan, X. Zhang, Mtldesc:
    拓宽视野以更好地描述, 见: AAAI 人工智能大会论文集, 第36卷, 2022, 页2388–2396.'
- en: '[102] C. Wang, R. Xu, K. Lv, S. Xu, W. Meng, Y. Zhang, B. Fan, X. Zhang, Attention
    weighted local descriptors, IEEE Transactions on Pattern Analysis and Machine
    Intelligence (2023).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] C. Wang, R. Xu, K. Lv, S. Xu, W. Meng, Y. Zhang, B. Fan, X. Zhang, 注意力加权局部描述符,
    IEEE 模式分析与机器智能汇刊 (2023).'
- en: '[103] J. Chen, S. Chen, Y. Liu, X. Chen, X. Fan, Y. Rao, C. Zhou, Y. Yang,
    Igs-net: Seeking good correspondences via interactive generative structure learning,
    IEEE Transactions on Geoscience and Remote Sensing 60 (2021) 1–13.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. Chen, S. Chen, Y. Liu, X. Chen, X. Fan, Y. Rao, C. Zhou, Y. Yang,
    Igs-net: 通过交互生成结构学习寻求良好的对应关系, IEEE 地球科学与遥感汇刊 60 (2021) 1–13.'
- en: '[104] J. Li, Q. Hu, M. Ai, Rift: Multi-modal image matching based on radiation-variation
    insensitive feature transform, IEEE Transactions on Image Processing 29 (2019)
    3296–3310.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. Li, Q. Hu, M. Ai, Rift: 基于辐射变异不敏感特征变换的多模态图像匹配, IEEE 图像处理汇刊 29 (2019)
    3296–3310.'
- en: '[105] E. Rosten, T. Drummond, Machine learning for high-speed corner detection,
    in: Computer Vision–ECCV 2006: 9th European Conference on Computer Vision, Graz,
    Austria, May 7-13, 2006\. Proceedings, Part I 9, Springer, 2006, pp. 430–443.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] E. Rosten, T. Drummond, 高速角点检测的机器学习, 见: 计算机视觉–ECCV 2006: 第九届欧洲计算机视觉会议,
    奥地利格拉茨, 2006年5月7-13日\. 会议录, 第一部分 9, 施普林格, 2006, 页430–443.'
- en: '[106] S. Cui, M. Xu, A. Ma, Y. Zhong, Modality-free feature detector and descriptor
    for multimodal remote sensing image registration, Remote Sensing 12 (18) (2020)
    2937.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] S. Cui, M. Xu, A. Ma, Y. Zhong, 无模态特征检测器和描述符用于多模态遥感图像配准, 遥感 12 (18) (2020)
    2937.'
- en: '[107] H. Xie, Y. Zhang, J. Qiu, X. Zhai, X. Liu, Y. Yang, S. Zhao, Y. Luo,
    J. Zhong, Semantics lead all: Towards unified image registration and fusion from
    a semantic perspective, Information Fusion 98 (2023) 101835.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] H. Xie, Y. Zhang, J. Qiu, X. Zhai, X. Liu, Y. Yang, S. Zhao, Y. Luo,
    J. Zhong, 语义引领一切: 从语义角度统一图像配准与融合, 信息融合 98 (2023) 101835.'
- en: '[108] D. Mishkin, F. Radenovic, J. Matas, Repeatability is not enough: Learning
    affine regions via discriminability, in: Proceedings of the European conference
    on computer vision (ECCV), 2018, pp. 284–300.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] D. Mishkin, F. Radenovic, J. Matas, 仅有重复性是不够的: 通过可区分性学习仿射区域, 见: 欧洲计算机视觉会议
    (ECCV) 论文集, 2018, 页284–300.'
- en: '[109] P. Truong, S. Apostolopoulos, A. Mosinska, S. Stucky, C. Ciller, S. D.
    Zanet, Glampoints: Greedily learned accurate match points, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, 2019, pp. 10732–10741.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] P. Truong, S. Apostolopoulos, A. Mosinska, S. Stucky, C. Ciller, S. D.
    Zanet, Glampoints: 贪婪学习的精确匹配点, 见: IEEE/CVF 国际计算机视觉会议论文集, 2019, 页10732–10741.'
- en: '[110] Q. Wang, X. Zhou, B. Hariharan, N. Snavely, Learning feature descriptors
    using camera pose supervision, in: Computer Vision–ECCV 2020: 16th European Conference,
    Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, Springer, 2020, pp. 757–774.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Q. Wang, X. Zhou, B. Hariharan, N. Snavely, 使用相机姿态监督学习特征描述符, 见: 计算机视觉–ECCV
    2020: 第十六届欧洲会议, 英国格拉斯哥, 2020年8月23–28日, 会议录, 第一部分 16, 施普林格, 2020, 页757–774.'
- en: '[111] M. Tyszkiewicz, P. Fua, E. Trulls, Disk: Learning local features with
    policy gradient, Advances in Neural Information Processing Systems 33 (2020) 14254–14265.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] M. Tyszkiewicz, P. Fua, E. Trulls, Disk: 使用策略梯度学习局部特征, 神经信息处理系统进展 33
    (2020) 14254–14265.'
- en: '[112] J. Lee, B. Kim, S. Kim, M. Cho, Learning rotation-equivariant features
    for visual correspondence, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2023, pp. 21887–21897.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Lee, B. Kim, S. Kim, M. Cho, 学习旋转等变特征以实现视觉对应, 载于: IEEE/CVF计算机视觉与模式识别会议论文集,
    2023, 第21887–21897页。'
- en: '[113] H. Zhou, T. Sattler, D. W. Jacobs, Evaluating local features for day-night
    matching, in: Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands,
    October 8-10 and 15-16, 2016, Proceedings, Part III 14, Springer, 2016, pp. 724–736.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] H. Zhou, T. Sattler, D. W. Jacobs, 评估用于昼夜匹配的局部特征, 载于: 计算机视觉–ECCV 2016研讨会:
    荷兰阿姆斯特丹, 2016年10月8-10日和15-16日, 会议论文集, 第三部分 14, Springer, 2016, 第724–736页。'
- en: '[114] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
    D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, et al., Benchmarking 6dof outdoor
    visual localization in changing conditions, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, 2018, pp. 8601–8610.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,
    D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, 等., 在变化条件下的6dof户外视觉定位基准测试, 载于:
    IEEE计算机视觉与模式识别会议论文集, 2018, 第8601–8610页。'
- en: '[115] H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic,
    T. Pajdla, A. Torii, Inloc: Indoor visual localization with dense matching and
    view synthesis, in: Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, 2018, pp. 7199–7209.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic,
    T. Pajdla, A. Torii, Inloc: 室内视觉定位通过密集匹配和视图合成, 载于: IEEE计算机视觉与模式识别会议论文集, 2018,
    第7199–7209页。'
- en: '[116] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic
    segmentation, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, 2015, pp. 3431–3440.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] J. Long, E. Shelhamer, T. Darrell, 用于语义分割的全卷积网络, 载于: IEEE计算机视觉与模式识别会议论文集,
    2015, 第3431–3440页。'
- en: '[117] Y. Ono, E. Trulls, P. Fua, K. M. Yi, Lf-net: Learning local features
    from images, Advances in neural information processing systems 31 (2018).'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Y. Ono, E. Trulls, P. Fua, K. M. Yi, Lf-net: 从图像中学习局部特征, 神经信息处理系统进展 31
    (2018)。'
- en: '[118] X. Shen, C. Wang, X. Li, Z. Yu, J. Li, C. Wen, M. Cheng, Z. He, Rf-net:
    An end-to-end image matching network based on receptive field, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp.
    8132–8140.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] X. Shen, C. Wang, X. Li, Z. Yu, J. Li, C. Wen, M. Cheng, Z. He, Rf-net:
    基于感受野的端到端图像匹配网络, 载于: IEEE/CVF计算机视觉与模式识别会议论文集, 2019, 第8132–8140页。'
- en: '[119] A. Bhowmik, S. Gumhold, C. Rother, E. Brachmann, Reinforced feature points:
    Optimizing feature detection and description for a high-level task, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp.
    4948–4957.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] A. Bhowmik, S. Gumhold, C. Rother, E. Brachmann, 强化特征点: 为高层次任务优化特征检测与描述,
    载于: IEEE/CVF计算机视觉与模式识别会议论文集, 2020, 第4948–4957页。'
- en: '[120] U. S. Parihar, A. Gujarathi, K. Mehta, S. Tourani, S. Garg, M. Milford,
    K. M. Krishna, Rord: Rotation-robust descriptors and orthographic views for local
    feature matching, in: 2021 IEEE/RSJ International Conference on Intelligent Robots
    and Systems (IROS), IEEE, 2021, pp. 1593–1600.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] U. S. Parihar, A. Gujarathi, K. Mehta, S. Tourani, S. Garg, M. Milford,
    K. M. Krishna, Rord: 旋转鲁棒描述符和正射视图用于局部特征匹配, 载于: 2021 IEEE/RSJ国际智能机器人与系统大会 (IROS),
    IEEE, 2021, 第1593–1600页。'
- en: '[121] A. Barroso-Laguna, Y. Verdie, B. Busam, K. Mikolajczyk, Hdd-net: Hybrid
    detector descriptor with mutual interactive learning, in: Proceedings of the Asian
    Conference on Computer Vision, 2020.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] A. Barroso-Laguna, Y. Verdie, B. Busam, K. Mikolajczyk, Hdd-net: 具有互学习的混合检测器描述符,
    载于: 亚洲计算机视觉会议论文集, 2020。'
- en: '[122] Y. Zhang, J. Wang, S. Xu, X. Liu, X. Zhang, Mlifeat: Multi-level information
    fusion based deep local features, in: Proceedings of the Asian Conference on Computer
    Vision, 2020.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Y. Zhang, J. Wang, S. Xu, X. Liu, X. Zhang, Mlifeat: 基于多层次信息融合的深度局部特征,
    载于: 亚洲计算机视觉会议论文集, 2020。'
- en: '[123] S. Suwanwimolkul, S. Komorita, K. Tasaka, Learning of low-level feature
    keypoints for accurate and robust detection, in: Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, 2021, pp. 2262–2271.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] S. Suwanwimolkul, S. Komorita, K. Tasaka, 低级特征关键点的学习以实现准确且鲁棒的检测, 载于:
    IEEE/CVF计算机视觉应用冬季会议论文集, 2021, 第2262–2271页。'
- en: '[124] X. Wang, Z. Liu, Y. Hu, W. Xi, W. Yu, D. Zou, Featurebooster: Boosting
    feature descriptors with a lightweight neural network, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 7630–7639.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] X. Wang, Z. Liu, Y. Hu, W. Xi, W. Yu, D. Zou, Featurebooster：通过轻量级神经网络提升特征描述符，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2023年，第7630–7639页。'
- en: '[125] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information
    processing systems 30 (2017).'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, I. Polosukhin, 注意力机制就是你所需要的，神经信息处理系统进展 30 (2017)。'
- en: '[126] Z. Luo, L. Zhou, X. Bai, H. Chen, J. Zhang, Y. Yao, S. Li, T. Fang, L. Quan,
    Aslfeat: Learning local features of accurate shape and localization, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp.
    6589–6598.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Z. Luo, L. Zhou, X. Bai, H. Chen, J. Zhang, Y. Yao, S. Li, T. Fang, L.
    Quan, Aslfeat：学习准确形状和定位的局部特征，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2020年，第6589–6598页。'
- en: '[127] B. Fan, J. Zhou, W. Feng, H. Pu, Y. Yang, Q. Kong, F. Wu, H. Liu, Learning
    semantic-aware local features for long term visual localization, IEEE Transactions
    on Image Processing 31 (2022) 4842–4855.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] B. Fan, J. Zhou, W. Feng, H. Pu, Y. Yang, Q. Kong, F. Wu, H. Liu, 学习语义感知的局部特征以实现长期视觉定位，IEEE
    图像处理学报 31 (2022) 4842–4855。'
- en: '[128] F. Xue, I. Budvytis, R. Cipolla, Sfd2: Semantic-guided feature detection
    and description, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2023, pp. 5206–5216.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] F. Xue, I. Budvytis, R. Cipolla, Sfd2：语义引导的特征检测和描述，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2023年，第5206–5216页。'
- en: '[129] Y. Tian, V. Balntas, T. Ng, A. Barroso-Laguna, Y. Demiris, K. Mikolajczyk,
    D2d: Keypoint extraction with describe to detect approach, in: Proceedings of
    the Asian conference on computer vision, 2020.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Y. Tian, V. Balntas, T. Ng, A. Barroso-Laguna, Y. Demiris, K. Mikolajczyk,
    D2d：一种通过描述来检测的方法进行关键点提取，发表于：亚洲计算机视觉大会论文集，2020年。'
- en: '[130] K. Li, L. Wang, L. Liu, Q. Ran, K. Xu, Y. Guo, Decoupling makes weakly
    supervised local feature better, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, 2022, pp. 15838–15848.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] K. Li, L. Wang, L. Liu, Q. Ran, K. Xu, Y. Guo, 解耦使得弱监督局部特征更好，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2022年，第15838–15848页。'
- en: '[131] Y. Deng, J. Ma, Redfeat: Recoupling detection and description for multimodal
    feature learning, IEEE Transactions on Image Processing 32 (2022) 591–602.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Y. Deng, J. Ma, Redfeat：用于多模态特征学习的重新耦合检测和描述，IEEE 图像处理学报 32 (2022) 591–602。'
- en: '[132] J. Sun, J. Zhu, L. Ji, Shared coupling-bridge for weakly supervised local
    feature learning, arXiv preprint arXiv:2212.07047 (2022).'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] J. Sun, J. Zhu, L. Ji, 用于弱监督局部特征学习的共享耦合桥，arXiv 预印本 arXiv:2212.07047 (2022)。'
- en: '[133] D. Zhang, F. Chen, X. Chen, Dualgats: Dual graph attention networks for
    emotion recognition in conversations, in: Proceedings of the 61st Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), 2023,
    pp. 7395–7408.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] D. Zhang, F. Chen, X. Chen, Dualgats：用于对话中的情感识别的双图注意网络，发表于：第61届计算语言学协会年会（卷1：长篇论文），2023年，第7395–7408页。'
- en: '[134] Z. Li, J. Ma, Learning feature matching via matchable keypoint-assisted
    graph neural network, arXiv preprint arXiv:2307.01447 (2023).'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Z. Li, J. Ma, 通过可匹配关键点辅助图神经网络学习特征匹配，arXiv 预印本 arXiv:2307.01447 (2023)。'
- en: '[135] R. Pautrat, I. Suárez, Y. Yu, M. Pollefeys, V. Larsson, Gluestick: Robust
    image matching by sticking points and lines together, arXiv preprint arXiv:2304.02008
    (2023).'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] R. Pautrat, I. Suárez, Y. Yu, M. Pollefeys, V. Larsson, Gluestick：通过将点和线结合在一起来实现鲁棒的图像匹配，arXiv
    预印本 arXiv:2304.02008 (2023)。'
- en: '[136] P. Lindenberger, P.-E. Sarlin, M. Pollefeys, Lightglue: Local feature
    matching at light speed, arXiv preprint arXiv:2306.13643 (2023).'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] P. Lindenberger, P.-E. Sarlin, M. Pollefeys, Lightglue：光速局部特征匹配，arXiv
    预印本 arXiv:2306.13643 (2023)。'
- en: '[137] Z. Kuang, J. Li, M. He, T. Wang, Y. Zhao, Densegap: graph-structured
    dense correspondence learning with anchor points, in: 2022 26th International
    Conference on Pattern Recognition (ICPR), IEEE, 2022, pp. 542–549.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Z. Kuang, J. Li, M. He, T. Wang, Y. Zhao, Densegap：具有锚点的图结构密集对应学习，发表于：2022年第26届国际模式识别大会（ICPR），IEEE，2022年，第542–549页。'
- en: '[138] Y. Cai, L. Li, D. Wang, X. Li, X. Liu, Htmatch: An efficient hybrid transformer
    based graph neural network for local feature matching, Signal Processing 204 (2023)
    108859.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Y. Cai, L. Li, D. Wang, X. Li, X. Liu, Htmatch：一种高效的混合变换器基础的图神经网络用于局部特征匹配，信号处理
    204 (2023) 108859。'
- en: '[139] X. Lu, Y. Yan, B. Kang, S. Du, Paraformer: Parallel attention transformer
    for efficient feature matching, arXiv preprint arXiv:2303.00941 (2023).'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] X. Lu, Y. Yan, B. Kang, S. Du, Paraformer: 用于高效特征匹配的并行注意力变换器，arXiv 预印本
    arXiv:2303.00941 (2023)。'
- en: '[140] Y. Deng, J. Ma, Resmatch: Residual attention learning for local feature
    matching, arXiv preprint arXiv:2307.05180 (2023).'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Y. Deng, J. Ma, Resmatch: 局部特征匹配的残差注意力学习，arXiv 预印本 arXiv:2307.05180 (2023)。'
- en: '[141] T. Xie, K. Dai, K. Wang, R. Li, L. Zhao, Deepmatcher: a deep transformer-based
    network for robust and accurate local feature matching, arXiv preprint arXiv:2301.02993
    (2023).'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] T. Xie, K. Dai, K. Wang, R. Li, L. Zhao, Deepmatcher: 一种基于深度变换器的网络，用于鲁棒和准确的局部特征匹配，arXiv
    预印本 arXiv:2301.02993 (2023)。'
- en: '[142] P. Truong, M. Danelljan, L. V. Gool, R. Timofte, Gocor: Bringing globally
    optimized correspondence volumes into your neural network, Advances in Neural
    Information Processing Systems 33 (2020) 14278–14290.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] P. Truong, M. Danelljan, L. V. Gool, R. Timofte, Gocor: 将全局优化的对应体积引入神经网络，神经信息处理系统进展
    33 (2020) 14278–14290。'
- en: '[143] P. Truong, M. Danelljan, R. Timofte, L. Van Gool, Pdc-net+: Enhanced
    probabilistic dense correspondence network, IEEE Transactions on Pattern Analysis
    and Machine Intelligence (2023).'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] P. Truong, M. Danelljan, R. Timofte, L. Van Gool, Pdc-net+: 增强的概率密集对应网络，IEEE
    模式分析与机器智能学报 (2023)。'
- en: '[144] J. Revaud, V. Leroy, P. Weinzaepfel, B. Chidlovskii, Pump: Pyramidal
    and uniqueness matching priors for unsupervised learning of local descriptors,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2022, pp. 3926–3936.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. Revaud, V. Leroy, P. Weinzaepfel, B. Chidlovskii, Pump: 用于无监督学习局部描述子的金字塔和唯一性匹配先验，见：IEEE/CVF
    计算机视觉与模式识别大会论文集，2022，pp. 3926–3936。'
- en: '[145] J. Revaud, P. Weinzaepfel, Z. Harchaoui, C. Schmid, Deepmatching: Hierarchical
    deformable dense matching, International Journal of Computer Vision 120 (2016)
    300–323.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] J. Revaud, P. Weinzaepfel, Z. Harchaoui, C. Schmid, Deepmatching: 分层变形密集匹配，计算机视觉国际期刊
    120 (2016) 300–323。'
- en: '[146] U. Efe, K. G. Ince, A. Alatan, Dfm: A performance baseline for deep feature
    matching, in: Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition, 2021, pp. 4284–4293.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] U. Efe, K. G. Ince, A. Alatan, Dfm: 深度特征匹配的性能基线，见：IEEE/CVF 计算机视觉与模式识别大会论文集，2021，pp.
    4284–4293。'
- en: '[147] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16
    words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929
    (2020).'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.
    Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly 等, 一张图像价值 16x16 个词：用于大规模图像识别的变换器，arXiv
    预印本 arXiv:2010.11929 (2020)。'
- en: '[148] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, S. Zagoruyko,
    End-to-end object detection with transformers, in: European conference on computer
    vision, Springer, 2020, pp. 213–229.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, S. Zagoruyko,
    端到端目标检测与变换器，见：欧洲计算机视觉大会，Springer，2020，pp. 213–229。'
- en: '[149] R. Xu, C. Wang, J. Zhang, S. Xu, W. Meng, X. Zhang, Rssformer: Foreground
    saliency enhancement for remote sensing land-cover segmentation, IEEE Transactions
    on Image Processing 32 (2023) 1052–1064.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] R. Xu, C. Wang, J. Zhang, S. Xu, W. Meng, X. Zhang, Rssformer: 用于遥感土地覆盖分割的前景显著性增强，IEEE
    图像处理学报 32 (2023) 1052–1064。'
- en: '[150] R. Xu, C. Wang, J. Sun, S. Xu, W. Meng, X. Zhang, Self correspondence
    distillation for end-to-end weakly-supervised semantic segmentation, in: Proceedings
    of the AAAI Conference on Artificial Intelligence, 2023.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] R. Xu, C. Wang, J. Sun, S. Xu, W. Meng, X. Zhang, 自我对应蒸馏用于端到端弱监督语义分割，见：AAAI
    人工智能大会论文集，2023。'
- en: '[151] R. Xu, C. Wang, S. Xu, W. Meng, X. Zhang, Dual-stream representation
    fusion learning for accurate medical image segmentation, Engineering Applications
    of Artificial Intelligence 123 (2023) 106402.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] R. Xu, C. Wang, S. Xu, W. Meng, X. Zhang, 双流表示融合学习用于精确医疗图像分割，人工智能工程应用
    123 (2023) 106402。'
- en: '[152] W. Cong, Y. Cong, J. Dong, G. Sun, H. Ding, Gradient-semantic compensation
    for incremental semantic segmentation, IEEE Transactions on Multimedia (2023)
    1–14[doi:10.1109/TMM.2023.3336243](https://doi.org/10.1109/TMM.2023.3336243).'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] W. Cong, Y. Cong, J. Dong, G. Sun, H. Ding, 梯度语义补偿用于增量语义分割，IEEE 多媒体学报
    (2023) 1–14 [doi:10.1109/TMM.2023.3336243](https://doi.org/10.1109/TMM.2023.3336243)。'
- en: '[153] W. Cong, Y. Cong, G. Sun, Y. Liu, J. Dong, Self-paced weight consolidation
    for continual learning, IEEE Transactions on Circuits and Systems for Video Technology
    (2023) 1–1[doi:10.1109/TCSVT.2023.3304567](https://doi.org/10.1109/TCSVT.2023.3304567).'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] W. Cong, Y. Cong, G. Sun, Y. Liu, J. Dong, 自适应加权整合用于持续学习，IEEE视频技术电路与系统学报
    (2023) 1–1 [doi:10.1109/TCSVT.2023.3304567](https://doi.org/10.1109/TCSVT.2023.3304567)。'
- en: '[154] W. Jiang, E. Trulls, J. Hosang, A. Tagliasacchi, K. M. Yi, Cotr: Correspondence
    transformer for matching across images, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2021, pp. 6207–6217.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] W. Jiang, E. Trulls, J. Hosang, A. Tagliasacchi, K. M. Yi, Cotr: 跨图像匹配的对应变换器，见：IEEE/CVF国际计算机视觉会议论文集，2021，页码6207–6217。'
- en: '[155] D. Tan, J.-J. Liu, X. Chen, C. Chen, R. Zhang, Y. Shen, S. Ding, R. Ji,
    Eco-tr: Efficient correspondences finding via coarse-to-fine refinement, in: European
    Conference on Computer Vision, Springer, 2022, pp. 317–334.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] D. Tan, J.-J. Liu, X. Chen, C. Chen, R. Zhang, Y. Shen, S. Ding, R. Ji,
    Eco-tr: 通过粗到细的精炼实现高效的对应查找，见：欧洲计算机视觉会议，Springer，2022，页码317–334。'
- en: '[156] G. Bökman, F. Kahl, A case for using rotation invariant features in state
    of the art feature matchers, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2022, pp. 5110–5119.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] G. Bökman, F. Kahl, 使用旋转不变特征的理由，见：IEEE/CVF计算机视觉与模式识别会议论文集，2022，页码5110–5119。'
- en: '[157] S. Tang, J. Zhang, S. Zhu, P. Tan, Quadtree attention for vision transformers,
    arXiv preprint arXiv:2201.02767 (2022).'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] S. Tang, J. Zhang, S. Zhu, P. Tan, 四叉树注意力用于视觉变换器，arXiv预印本 arXiv:2201.02767
    (2022)。'
- en: '[158] Y. Chen, D. Huang, S. Xu, J. Liu, Y. Liu, Guide local feature matching
    by overlap estimation, in: Proceedings of the AAAI Conference on Artificial Intelligence,
    Vol. 36, 2022, pp. 365–373.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Y. Chen, D. Huang, S. Xu, J. Liu, Y. Liu, 通过重叠估计引导局部特征匹配，见：AAAI人工智能会议论文集，第36卷，2022，页码365–373。'
- en: '[159] Q. Wang, J. Zhang, K. Yang, K. Peng, R. Stiefelhagen, Matchformer: Interleaving
    attention in transformers for feature matching, in: Proceedings of the Asian Conference
    on Computer Vision, 2022, pp. 2746–2762.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Q. Wang, J. Zhang, K. Yang, K. Peng, R. Stiefelhagen, Matchformer: 变换器中的交错注意力用于特征匹配，见：亚洲计算机视觉会议论文集，2022，页码2746–2762。'
- en: '[160] J. Ma, Y. Wang, A. Fan, G. Xiao, R. Chen, Correspondence attention transformer:
    A context-sensitive network for two-view correspondence learning, IEEE Transactions
    on Multimedia (2022).'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] J. Ma, Y. Wang, A. Fan, G. Xiao, R. Chen, 对应注意力变换器：用于双视图对应学习的上下文敏感网络，IEEE多媒体学报
    (2022)。'
- en: '[161] K. T. Giang, S. Song, S. Jo, Topicfm: robust and interpretable feature
    matching with topic-assisted, arXiv preprint arXiv:2207.00328 (2022).'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] K. T. Giang, S. Song, S. Jo, Topicfm: 具有主题辅助的鲁棒且可解释的特征匹配，arXiv预印本 arXiv:2207.00328
    (2022)。'
- en: '[162] J. Yu, J. Chang, J. He, T. Zhang, J. Yu, F. Wu, Adaptive spot-guided
    transformer for consistent local feature matching, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2023, pp. 21898–21908.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] J. Yu, J. Chang, J. He, T. Zhang, J. Yu, F. Wu, 自适应点引导变换器用于一致的局部特征匹配，见：IEEE/CVF计算机视觉与模式识别会议论文集，2023，页码21898–21908。'
- en: '[163] K. Dai, T. Xie, K. Wang, Z. Jiang, R. Li, L. Zhao, Oamatcher: An overlapping
    areas-based network for accurate local feature matching, arXiv preprint arXiv:2302.05846
    (2023).'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] K. Dai, T. Xie, K. Wang, Z. Jiang, R. Li, L. Zhao, Oamatcher: 基于重叠区域的网络用于准确的局部特征匹配，arXiv预印本
    arXiv:2302.05846 (2023)。'
- en: '[164] C. Cao, Y. Fu, Improving transformer-based image matching by cascaded
    capturing spatially informative keypoints, arXiv preprint arXiv:2303.02885 (2023).'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] C. Cao, Y. Fu, 通过级联捕捉空间信息关键点提高基于变换器的图像匹配，arXiv预印本 arXiv:2303.02885 (2023)。'
- en: '[165] S. Zhu, X. Liu, Pmatch: Paired masked image modeling for dense geometric
    matching, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2023, pp. 21909–21918.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] S. Zhu, X. Liu, Pmatch: 用于稠密几何匹配的配对掩膜图像建模，见：IEEE/CVF计算机视觉与模式识别会议论文集，2023，页码21909–21918。'
- en: '[166] J. Chang, J. Yu, T. Zhang, Structured epipolar matcher for local feature
    matching, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2023, pp. 6176–6185.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] J. Chang, J. Yu, T. Zhang, 结构化极线匹配器用于局部特征匹配，见：IEEE/CVF计算机视觉与模式识别会议论文集，2023，页码6176–6185。'
- en: '[167] J. Edstedt, I. Athanasiadis, M. Wadenbäck, M. Felsberg, Dkm: Dense kernelized
    feature matching for geometry estimation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2023, pp. 17765–17775.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. Edstedt, I. Athanasiadis, M. Wadenbäck, M. Felsberg, Dkm: 用于几何估计的稠密核化特征匹配，见：IEEE/CVF计算机视觉与模式识别会议论文集，2023，页码17765–17775。'
- en: '[168] J. Edstedt, Q. Sun, G. Bökman, M. Wadenbäck, M. Felsberg, Roma: Revisiting
    robust losses for dense feature matching, arXiv preprint arXiv:2305.15404 (2023).'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] J. Edstedt, Q. Sun, G. Bökman, M. Wadenbäck, M. Felsberg, ROMA: 重新审视稠密特征匹配的鲁棒损失，arXiv
    预印本 arXiv:2305.15404 (2023)。'
- en: '[169] Q. Zhou, T. Sattler, L. Leal-Taixe, Patch2pix: Epipolar-guided pixel-level
    correspondences, in: Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition, 2021, pp. 4669–4678.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Q. Zhou, T. Sattler, L. Leal-Taixe, Patch2pix: 极线引导的像素级对应关系，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，2021，第 4669–4678 页。'
- en: '[170] D. Huang, Y. Chen, Y. Liu, J. Liu, S. Xu, W. Wu, Y. Ding, F. Tang, C. Wang,
    Adaptive assignment for geometry aware local feature matching, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
    5425–5434.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] D. Huang, Y. Chen, Y. Liu, J. Liu, S. Xu, W. Wu, Y. Ding, F. Tang, C. Wang,
    几何感知局部特征匹配的自适应分配，见：IEEE/CVF 计算机视觉与模式识别会议论文集，2023，第 5425–5434 页。'
- en: '[171] J. Ni, Y. Li, Z. Huang, H. Li, H. Bao, Z. Cui, G. Zhang, Pats: Patch
    area transportation with subdivision for local feature matching, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
    17776–17786.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] J. Ni, Y. Li, Z. Huang, H. Li, H. Bao, Z. Cui, G. Zhang, PATS: 用于局部特征匹配的补丁区域传输与细分，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，2023，第 17776–17786 页。'
- en: '[172] Y. Zhang, X. Zhao, D. Qian, Searching from area to point: A hierarchical
    framework for semantic-geometric combined feature matching, arXiv preprint arXiv:2305.00194
    (2023).'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Y. Zhang, X. Zhao, D. Qian, 从区域到点的搜索：一种语义-几何结合特征匹配的分层框架，arXiv 预印本 arXiv:2305.00194
    (2023)。'
- en: '[173] N. Snavely, S. M. Seitz, R. Szeliski, Photo tourism: exploring photo
    collections in 3d, in: ACM siggraph 2006 papers, 2006, pp. 835–846.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] N. Snavely, S. M. Seitz, R. Szeliski, 照片旅游：在三维中探索照片集合，见：ACM SIGGRAPH
    2006 论文集，2006，第 835–846 页。'
- en: '[174] P. Lindenberger, P.-E. Sarlin, V. Larsson, M. Pollefeys, Pixel-perfect
    structure-from-motion with featuremetric refinement, in: Proceedings of the IEEE/CVF
    international conference on computer vision, 2021, pp. 5987–5997.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] P. Lindenberger, P.-E. Sarlin, V. Larsson, M. Pollefeys, 像素级完美结构重建与特征度量细化，见：IEEE/CVF
    国际计算机视觉会议论文集，2021，第 5987–5997 页。'
- en: '[175] C. M. Parameshwara, G. Hari, C. Fermüller, N. J. Sanket, Y. Aloimonos,
    Diffposenet: direct differentiable camera pose estimation, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.
    6845–6854.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] C. M. Parameshwara, G. Hari, C. Fermüller, N. J. Sanket, Y. Aloimonos,
    Diffposenet: 直接可微分的相机姿态估计，见：IEEE/CVF 计算机视觉与模式识别会议论文集，2022，第 6845–6854 页。'
- en: '[176] J. Y. Zhang, D. Ramanan, S. Tulsiani, Relpose: Predicting probabilistic
    relative rotation for single objects in the wild, in: European Conference on Computer
    Vision, Springer, 2022, pp. 592–611.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] J. Y. Zhang, D. Ramanan, S. Tulsiani, Relpose: 预测野外单个物体的概率性相对旋转，见：欧洲计算机视觉会议，Springer，2022，第
    592–611 页。'
- en: '[177] C. Tang, P. Tan, Ba-net: Dense bundle adjustment network, arXiv preprint
    arXiv:1806.04807 (2018).'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] C. Tang, P. Tan, BA-Net: 稠密束调整网络，arXiv 预印本 arXiv:1806.04807 (2018)。'
- en: '[178] X. Gu, W. Yuan, Z. Dai, C. Tang, S. Zhu, P. Tan, Dro: Deep recurrent
    optimizer for structure-from-motion, arXiv preprint arXiv:2103.13201 (2021).'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] X. Gu, W. Yuan, Z. Dai, C. Tang, S. Zhu, P. Tan, DRO: 深度递归优化器用于结构重建，arXiv
    预印本 arXiv:2103.13201 (2021)。'
- en: '[179] X. He, J. Sun, Y. Wang, S. Peng, Q. Huang, H. Bao, X. Zhou, Detector-free
    structure from motion, arXiv preprint arXiv:2306.15669 (2023).'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] X. He, J. Sun, Y. Wang, S. Peng, Q. Huang, H. Bao, X. Zhou, 无需检测器的结构重建，arXiv
    预印本 arXiv:2306.15669 (2023)。'
- en: '[180] L. H. Hughes, D. Marcos, S. Lobry, D. Tuia, M. Schmitt, A deep learning
    framework for matching of sar and optical imagery, ISPRS Journal of Photogrammetry
    and Remote Sensing 169 (2020) 166–179.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] L. H. Hughes, D. Marcos, S. Lobry, D. Tuia, M. Schmitt, 一种用于 SAR 和光学影像匹配的深度学习框架，《ISPRS
    摄影测量与遥感杂志》169 (2020) 166–179。'
- en: '[181] Y. Ye, T. Tang, B. Zhu, C. Yang, B. Li, S. Hao, A multiscale framework
    with unsupervised learning for remote sensing image registration, IEEE Transactions
    on Geoscience and Remote Sensing 60 (2022) 1–15.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Y. Ye, T. Tang, B. Zhu, C. Yang, B. Li, S. Hao, 一种多尺度无监督学习的遥感图像配准框架，《IEEE
    地球科学与遥感学报》60 (2022) 1–15。'
- en: '[182] S. Chen, J. Chen, Y. Rao, X. Chen, X. Fan, H. Bai, L. Xing, C. Zhou,
    Y. Yang, A hierarchical consensus attention network for feature matching of remote
    sensing images, IEEE Transactions on Geoscience and Remote Sensing 60 (2022) 1–11.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] S. Chen, J. Chen, Y. Rao, X. Chen, X. Fan, H. Bai, L. Xing, C. Zhou,
    Y. Yang, 一种用于遥感图像特征匹配的分层共识注意力网络，《IEEE 地球科学与遥感学报》60 (2022) 1–11。'
- en: '[183] Y. Liu, B. N. Zhao, S. Zhao, L. Zhang, Progressive motion coherence for
    remote sensing image matching, IEEE Transactions on Geoscience and Remote Sensing
    60 (2022) 1–13.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Y. Liu, B. N. Zhao, S. Zhao, L. Zhang, 遥感图像匹配的渐进运动一致性，IEEE地球科学与遥感学报 60
    (2022) 1–13。'
- en: '[184] Y. Ye, B. Zhu, T. Tang, C. Yang, Q. Xu, G. Zhang, A robust multimodal
    remote sensing image registration method and system using steerable filters with
    first-and second-order gradients, ISPRS Journal of Photogrammetry and Remote Sensing
    188 (2022) 331–350.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Y. Ye, B. Zhu, T. Tang, C. Yang, Q. Xu, G. Zhang, 一种基于可控滤波器的强健多模态遥感图像配准方法与系统，ISPRS摄影测量与遥感杂志
    188 (2022) 331–350。'
- en: '[185] L. H. Hughes, M. Schmitt, L. Mou, Y. Wang, X. X. Zhu, Identifying corresponding
    patches in sar and optical images with a pseudo-siamese cnn, IEEE Geoscience and
    Remote Sensing Letters 15 (5) (2018) 784–788.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] L. H. Hughes, M. Schmitt, L. Mou, Y. Wang, X. X. Zhu, 使用伪Siamese卷积神经网络识别SAR图像和光学图像中的对应块，IEEE地球科学与遥感学报快报
    15 (5) (2018) 784–788。'
- en: '[186] D. Quan, S. Wang, X. Liang, R. Wang, S. Fang, B. Hou, L. Jiao, Deep generative
    matching network for optical and sar image registration, in: IGARSS 2018-2018
    IEEE International Geoscience and Remote Sensing Symposium, IEEE, 2018, pp. 6215–6218.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] D. Quan, S. Wang, X. Liang, R. Wang, S. Fang, B. Hou, L. Jiao, 用于光学图像和SAR图像配准的深度生成匹配网络，见：IGARSS
    2018-2018 IEEE国际地球科学与遥感研讨会，IEEE，2018年，第6215–6218页。'
- en: '[187] N. Merkle, S. Auer, R. Mueller, P. Reinartz, Exploring the potential
    of conditional adversarial networks for optical and sar image matching, IEEE Journal
    of Selected Topics in Applied Earth Observations and Remote Sensing 11 (6) (2018)
    1811–1820.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] N. Merkle, S. Auer, R. Mueller, P. Reinartz, 探索条件对抗网络在光学图像和合成孔径雷达图像匹配中的潜力，IEEE应用地球观测与遥感精选主题杂志
    11 (6) (2018) 1811–1820。'
- en: '[188] W. Shi, F. Su, R. Wang, J. Fan, A visual circle based image registration
    algorithm for optical and sar imagery, in: 2012 IEEE International Geoscience
    and Remote Sensing Symposium, IEEE, 2012, pp. 2109–2112.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] W. Shi, F. Su, R. Wang, J. Fan, 一种基于视觉圆的光学图像和SAR图像配准算法，见：2012 IEEE国际地球科学与遥感研讨会，IEEE，2012年，第2109–2112页。'
- en: '[189] A. Zampieri, G. Charpiat, N. Girard, Y. Tarabalka, Multimodal image alignment
    through a multiscale chain of neural networks with application to remote sensing,
    in: Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp.
    657–673.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] A. Zampieri, G. Charpiat, N. Girard, Y. Tarabalka, 通过多尺度神经网络链进行多模态图像对齐及其在遥感中的应用，见：欧洲计算机视觉会议（ECCV）论文集，2018年，第657–673页。'
- en: '[190] F. Ye, Y. Su, H. Xiao, X. Zhao, W. Min, Remote sensing image registration
    using convolutional neural network features, IEEE Geoscience and Remote Sensing
    Letters 15 (2) (2018) 232–236.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] F. Ye, Y. Su, H. Xiao, X. Zhao, W. Min, 使用卷积神经网络特征的遥感图像配准，IEEE地球科学与遥感学报快报
    15 (2) (2018) 232–236。'
- en: '[191] T. Wang, G. Zhang, L. Yu, R. Zhao, M. Deng, K. Xu, Multi-mode gf-3 satellite
    image geometric accuracy verification using the rpc model, Sensors 17 (9) (2017)
    2005.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] T. Wang, G. Zhang, L. Yu, R. Zhao, M. Deng, K. Xu, 多模式GF-3卫星图像几何精度验证使用RPC模型，传感器
    17 (9) (2017) 2005。'
- en: '[192] W. Ma, J. Zhang, Y. Wu, L. Jiao, H. Zhu, W. Zhao, A novel two-step registration
    method for remote sensing images based on deep and local features, IEEE Transactions
    on Geoscience and Remote Sensing 57 (7) (2019) 4834–4843.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] W. Ma, J. Zhang, Y. Wu, L. Jiao, H. Zhu, W. Zhao, 一种基于深度和局部特征的遥感图像新型两步配准方法，IEEE地球科学与遥感学报
    57 (7) (2019) 4834–4843。'
- en: '[193] L. Zhou, Y. Ye, T. Tang, K. Nan, Y. Qin, Robust matching for sar and
    optical images using multiscale convolutional gradient features, IEEE Geoscience
    and Remote Sensing Letters 19 (2021) 1–5.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] L. Zhou, Y. Ye, T. Tang, K. Nan, Y. Qin, 使用多尺度卷积梯度特征的SAR和光学图像强健匹配，IEEE地球科学与遥感学报快报
    19 (2021) 1–5。'
- en: '[194] S. Cui, A. Ma, L. Zhang, M. Xu, Y. Zhong, Map-net: Sar and optical image
    matching via image-based convolutional network with attention mechanism and spatial
    pyramid aggregated pooling, IEEE Transactions on Geoscience and Remote Sensing
    60 (2021) 1–13.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] S. Cui, A. Ma, L. Zhang, M. Xu, Y. Zhong, Map-net：通过图像基础卷积网络、注意力机制和空间金字塔聚合池进行SAR和光学图像匹配，IEEE地球科学与遥感学报
    60 (2021) 1–13。'
- en: '[195] Z. Wang, Y. Ma, Y. Zhang, Review of pixel-level remote sensing image
    fusion based on deep learning, Information Fusion 90 (2023) 36–58.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Z. Wang, Y. Ma, Y. Zhang, 基于深度学习的像素级遥感图像融合综述，信息融合 90 (2023) 36–58。'
- en: '[196] Z. Bian, A. Jabri, A. A. Efros, A. Owens, Learning pixel trajectories
    with multiscale contrastive random walks, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 6508–6519.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Z. Bian, A. Jabri, A. A. Efros, A. Owens, 使用多尺度对比随机游走学习像素轨迹，见：*Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022，第6508–6519页。'
- en: '[197] A. Ranjan, V. Jampani, L. Balles, K. Kim, D. Sun, J. Wulff, M. J. Black,
    Competitive collaboration: Joint unsupervised learning of depth, camera motion,
    optical flow and motion segmentation, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2019, pp. 12240–12249.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] A. Ranjan, V. Jampani, L. Balles, K. Kim, D. Sun, J. Wulff, M. J. Black,
    竞争协作：深度、相机运动、光流和运动分割的联合无监督学习，见：*Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019，第12240–12249页。'
- en: '[198] A. W. Harley, Z. Fang, K. Fragkiadaki, Particle video revisited: Tracking
    through occlusions using point trajectories, in: European Conference on Computer
    Vision, Springer, 2022, pp. 59–75.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] A. W. Harley, Z. Fang, K. Fragkiadaki, 粒子视频再探：通过点轨迹在遮挡中追踪，见：*European
    Conference on Computer Vision*, Springer, 2022，第59–75页。'
- en: '[199] C. Qin, S. Wang, C. Chen, W. Bai, D. Rueckert, Generative myocardial
    motion tracking via latent space exploration with biomechanics-informed prior,
    Medical Image Analysis 83 (2023) 102682.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] C. Qin, S. Wang, C. Chen, W. Bai, D. Rueckert, 基于生物力学先验的潜在空间探索生成性心肌运动追踪，*Medical
    Image Analysis* 83 (2023) 102682。'
- en: '[200] M. Ye, M. Kanski, D. Yang, Q. Chang, Z. Yan, Q. Huang, L. Axel, D. Metaxas,
    Deeptag: An unsupervised deep learning method for motion tracking on cardiac tagging
    magnetic resonance images, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, 2021, pp. 7261–7271.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] M. Ye, M. Kanski, D. Yang, Q. Chang, Z. Yan, Q. Huang, L. Axel, D. Metaxas,
    Deeptag：一种用于心脏标记磁共振图像的无监督深度学习运动追踪方法，见：*Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021，第7261–7271页。'
- en: '[201] Z. Bian, F. Xing, J. Yu, M. Shao, Y. Liu, A. Carass, J. Woo, J. L. Prince,
    Drimet: Deep registration-based 3d incompressible motion estimation in tagged-mri
    with application to the tongue, in: Medical Imaging with Deep Learning, 2023.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Z. Bian, F. Xing, J. Yu, M. Shao, Y. Liu, A. Carass, J. Woo, J. L. Prince,
    Drimet：基于深度配准的标记 MRI 中 3D 不可压缩运动估计及其在舌头上的应用，见：*Medical Imaging with Deep Learning*,
    2023。'
- en: '[202] T. Fechter, D. Baltas, One-shot learning for deformable medical image
    registration and periodic motion tracking, IEEE transactions on medical imaging
    39 (7) (2020) 2506–2517.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] T. Fechter, D. Baltas, 一次性学习用于可变形医学图像配准和周期性运动追踪，*IEEE Transactions on
    Medical Imaging* 39 (7) (2020) 2506–2517。'
- en: '[203] Y. Zhang, X. Wu, H. M. Gach, H. Li, D. Yang, Groupregnet: a groupwise
    one-shot deep learning-based 4d image registration method, Physics in Medicine
    & Biology 66 (4) (2021) 045030.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Y. Zhang, X. Wu, H. M. Gach, H. Li, D. Yang, Groupregnet：一种基于深度学习的组内一次性
    4D 图像配准方法，*Physics in Medicine & Biology* 66 (4) (2021) 045030。'
- en: '[204] Y. Ji, Z. Zhu, Y. Wei, A one-shot lung 4d-ct image registration method
    with temporal-spatial features, in: 2022 IEEE Biomedical Circuits and Systems
    Conference (BioCAS), IEEE, 2022, pp. 203–207.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Y. Ji, Z. Zhu, Y. Wei, 一次性肺部 4D-CT 图像配准方法及其时空特征，见：2022 IEEE 生物医学电路与系统会议
    (BioCAS)，IEEE，2022，第203–207页。'
- en: '[205] M. Z. Iqbal, I. Razzak, A. Qayyum, T. T. Nguyen, M. Tanveer, A. Sowmya,
    Hybrid unsupervised paradigm based deformable image fusion for 4d ct lung image
    modality, Information Fusion 102 (2024) 102061.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] M. Z. Iqbal, I. Razzak, A. Qayyum, T. T. Nguyen, M. Tanveer, A. Sowmya,
    基于混合无监督范式的可变形图像融合用于 4D CT 肺图像模态，*Information Fusion* 102 (2024) 102061。'
- en: '[206] M. Pfandler, P. Stefan, C. Mehren, M. Lazarovici, M. Weigl, Technical
    and nontechnical skills in surgery: a simulated operating room environment study,
    Spine 44 (23) (2019) E1396–E1400.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] M. Pfandler, P. Stefan, C. Mehren, M. Lazarovici, M. Weigl, 外科手术中的技术和非技术技能：一个模拟手术环境研究，*Spine*
    44 (23) (2019) E1396–E1400。'
- en: '[207] F. Maes, A. Collignon, D. Vandermeulen, G. Marchal, P. Suetens, Multimodality
    image registration by maximization of mutual information, IEEE transactions on
    Medical Imaging 16 (2) (1997) 187–198.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] F. Maes, A. Collignon, D. Vandermeulen, G. Marchal, P. Suetens, 通过最大化互信息进行多模态图像配准，*IEEE
    Transactions on Medical Imaging* 16 (2) (1997) 187–198。'
- en: '[208] M. Unberath, C. Gao, Y. Hu, M. Judish, R. H. Taylor, M. Armand, R. Grupp,
    The impact of machine learning on 2d/3d registration for image-guided interventions:
    A systematic review and perspective, Frontiers in Robotics and AI 8 (2021) 716007.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] M. Unberath, C. Gao, Y. Hu, M. Judish, R. H. Taylor, M. Armand, R. Grupp,
    机器学习对图像引导干预的 2D/3D 配准的影响：系统评价与展望，*Frontiers in Robotics and AI* 8 (2021) 716007。'
- en: '[209] S. Jaganathan, M. Kukla, J. Wang, K. Shetty, A. Maier, Self-supervised
    2d/3d registration for x-ray to ct image fusion, in: Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision, 2023, pp. 2788–2798.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] S. Jaganathan, M. Kukla, J. Wang, K. Shetty, A. Maier, 自监督的2D/3D配准用于X射线与CT图像融合，见：IEEE/CVF冬季计算机视觉应用会议论文集，2023年，第2788–2798页。'
- en: '[210] D.-X. Huang, X.-H. Zhou, X.-L. Xie, S.-Q. Liu, Z.-Q. Feng, J.-L. Hao,
    Z.-G. Hou, N. Ma, L. Yan, A novel two-stage framework for 2d/3d registration in
    neurological interventions, in: 2022 IEEE International Conference on Robotics
    and Biomimetics (ROBIO), IEEE, 2022, pp. 266–271.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] D.-X. Huang, X.-H. Zhou, X.-L. Xie, S.-Q. Liu, Z.-Q. Feng, J.-L. Hao,
    Z.-G. Hou, N. Ma, L. Yan, 一种用于神经学干预的2D/3D配准的新型双阶段框架，见：2022 IEEE国际机器人与仿生学会议（ROBIO），IEEE，2022年，第266–271页。'
- en: '[211] Y. Pei, Y. Zhang, H. Qin, G. Ma, Y. Guo, T. Xu, H. Zha, Non-rigid craniofacial
    2d-3d registration using cnn-based regression, in: Deep Learning in Medical Image
    Analysis and Multimodal Learning for Clinical Decision Support: Third International
    Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction
    with MICCAI 2017, Québec City, QC, Canada, September 14, Proceedings 3, Springer,
    2017, pp. 117–125.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Y. Pei, Y. Zhang, H. Qin, G. Ma, Y. Guo, T. Xu, H. Zha, 基于CNN回归的非刚性颅面2D-3D配准，见：医学影像分析中的深度学习与临床决策支持的多模态学习：第三届国际研讨会DLMIA
    2017和第七届国际研讨会ML-CDS 2017，与MICCAI 2017一起举办，加拿大魁北克市，2017年9月14日，会议论文集3，Springer，2017年，第117–125页。'
- en: '[212] M. D. Foote, B. E. Zimmerman, A. Sawant, S. C. Joshi, Real-time 2d-3d
    deformable registration with deep learning and application to lung radiotherapy
    targeting, in: Information Processing in Medical Imaging: 26th International Conference,
    IPMI 2019, Hong Kong, China, June 2–7, 2019, Proceedings 26, Springer, 2019, pp.
    265–276.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] M. D. Foote, B. E. Zimmerman, A. Sawant, S. C. Joshi, 基于深度学习的实时2D-3D可变形配准及其在肺部放射治疗靶点中的应用，见：医学影像信息处理：第26届国际会议IPMI
    2019，中国香港，2019年6月2–7日，会议论文集26，Springer，2019年，第265–276页。'
- en: '[213] W. Yu, M. Tannast, G. Zheng, Non-rigid free-form 2d–3d registration using
    a b-spline-based statistical deformation model, Pattern recognition 63 (2017)
    689–699.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] W. Yu, M. Tannast, G. Zheng, 使用基于B样条的统计变形模型进行非刚性自由形状2D-3D配准，《模式识别》63
    (2017) 689–699。'
- en: '[214] P. Li, Y. Pei, Y. Guo, G. Ma, T. Xu, H. Zha, Non-rigid 2d-3d registration
    using convolutional autoencoders, in: 2020 IEEE 17th International Symposium on
    Biomedical Imaging (ISBI), IEEE, 2020, pp. 700–704.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] P. Li, Y. Pei, Y. Guo, G. Ma, T. Xu, H. Zha, 使用卷积自编码器的非刚性2D-3D配准，见：2020
    IEEE第17届生物医学影像国际研讨会（ISBI），IEEE，2020年，第700–704页。'
- en: '[215] G. Dong, J. Dai, N. Li, C. Zhang, W. He, L. Liu, Y. Chan, Y. Li, Y. Xie,
    X. Liang, 2d/3d non-rigid image registration via two orthogonal x-ray projection
    images for lung tumor tracking, Bioengineering 10 (2) (2023) 144.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] G. Dong, J. Dai, N. Li, C. Zhang, W. He, L. Liu, Y. Chan, Y. Li, Y. Xie,
    X. Liang, 通过两个正交X射线投影图像进行2D/3D非刚性图像配准用于肺部肿瘤追踪，《生物工程》10 (2) (2023) 144。'
- en: '[216] V. Balntas, K. Lenc, A. Vedaldi, K. Mikolajczyk, Hpatches: A benchmark
    and evaluation of handcrafted and learned local descriptors, in: Proceedings of
    the IEEE conference on computer vision and pattern recognition, 2017, pp. 5173–5182.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] V. Balntas, K. Lenc, A. Vedaldi, K. Mikolajczyk, Hpatches: 手工制作和学习局部描述符的基准和评估，见：IEEE计算机视觉与模式识别会议论文集，2017年，第5173–5182页。'
- en: '[217] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, M. Nießner,
    Scannet: Richly-annotated 3d reconstructions of indoor scenes, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, 2017, pp. 5828–5839.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, M. Nießner,
    Scannet: 富有注释的室内场景3D重建，见：IEEE计算机视觉与模式识别会议论文集，2017年，第5828–5839页。'
- en: '[218] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland,
    D. Borth, L.-J. Li, Yfcc100m: The new data in multimedia research, Communications
    of the ACM 59 (2) (2016) 64–73.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland,
    D. Borth, L.-J. Li, Yfcc100m: 新的数据在多媒体研究中的应用，《ACM通信》59 (2) (2016) 64–73。'
- en: '[219] Z. Li, N. Snavely, Megadepth: Learning single-view depth prediction from
    internet photos, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2018, pp. 2041–2050.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Z. Li, N. Snavely, Megadepth: 从互联网照片中学习单视图深度预测，见：IEEE计算机视觉与模式识别会议论文集，2018年，第2041–2050页。'
- en: '[220] D. Mishkin, J. Matas, M. Perdoch, Mods: Fast and robust method for two-view
    matching, Computer vision and image understanding 141 (2015) 81–93.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] D. Mishkin, J. Matas, M. Perdoch, Mods: 一种快速且稳健的双视图匹配方法，《计算机视觉与图像理解》141
    (2015) 81–93。'
- en: '[221] D. Mishkin, J. Matas, M. Perdoch, K. Lenc, Wxbs: Wide baseline stereo
    generalizations, arXiv preprint arXiv:1504.06603 (2015).'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] D. Mishkin, J. Matas, M. Perdoch, K. Lenc, Wxbs: 广基线立体图像泛化，arXiv预印本 arXiv:1504.06603
    (2015)。'
- en: '[222] T. Sattler, T. Weyand, B. Leibe, L. Kobbelt, Image retrieval for image-based
    localization revisited., in: BMVC, Vol. 1, 2012, p. 4.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] T. Sattler, T. Weyand, B. Leibe, L. Kobbelt, 图像检索用于基于图像的定位的重新审视，载于：BMVC，第1卷，2012年，页4。'
- en: '[223] W. Maddern, G. Pascoe, C. Linegar, P. Newman, 1 year, 1000 km: The oxford
    robotcar dataset, The International Journal of Robotics Research 36 (1) (2017)
    3–15.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] W. Maddern, G. Pascoe, C. Linegar, P. Newman, 1年，1000公里：牛津机器人车数据集，《国际机器人研究杂志》36
    (1) (2017) 3–15。'
- en: '[224] P.-E. Sarlin, M. Dusmanu, J. L. Schönberger, P. Speciale, L. Gruber,
    V. Larsson, O. Miksik, M. Pollefeys, Lamar: Benchmarking localization and mapping
    for augmented reality, in: European Conference on Computer Vision, Springer, 2022,
    pp. 686–704.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] P.-E. Sarlin, M. Dusmanu, J. L. Schönberger, P. Speciale, L. Gruber,
    V. Larsson, O. Miksik, M. Pollefeys, Lamar: 扩增现实的定位和映射基准测试，载于：欧洲计算机视觉会议，Springer，2022年，页686–704。'
- en: '[225] A. Geiger, P. Lenz, C. Stiller, R. Urtasun, Vision meets robotics: The
    kitti dataset, The International Journal of Robotics Research 32 (11) (2013) 1231–1237.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] A. Geiger, P. Lenz, C. Stiller, R. Urtasun, 视觉遇上机器人：kitti 数据集，《国际机器人研究杂志》32
    (11) (2013) 1231–1237。'
- en: '[226] J. L. Schonberger, H. Hardmeier, T. Sattler, M. Pollefeys, Comparative
    evaluation of hand-crafted and learned local features, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, 2017, pp. 1482–1491.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] J. L. Schonberger, H. Hardmeier, T. Sattler, M. Pollefeys, 手工制作与学习到的局部特征的比较评估，载于：IEEE计算机视觉与模式识别会议论文集，2017年，页1482–1491。'
- en: '[227] K. Wilson, N. Snavely, Robust global translations with 1dsfm, in: Computer
    Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,
    2014, Proceedings, Part III 13, Springer, 2014, pp. 61–75.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] K. Wilson, N. Snavely, 使用1dsfm的鲁棒全局平移，载于：计算机视觉–ECCV 2014：第13届欧洲会议，瑞士苏黎世，2014年9月6-12日，论文集，第III部分13，Springer，2014年，页61–75。'
- en: '[228] T. Schops, J. L. Schonberger, S. Galliani, T. Sattler, K. Schindler,
    M. Pollefeys, A. Geiger, A multi-view stereo benchmark with high-resolution images
    and multi-camera videos, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2017, pp. 3260–3269.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] T. Schops, J. L. Schonberger, S. Galliani, T. Sattler, K. Schindler,
    M. Pollefeys, A. Geiger, 高分辨率图像和多摄像头视频的多视图立体基准测试，载于：IEEE计算机视觉与模式识别会议论文集，2017年，页3260–3269。'
- en: '[229] C. Schmid, R. Mohr, C. Bauckhage, Evaluation of interest point detectors,
    International Journal of computer vision 37 (2) (2000) 151–172.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] C. Schmid, R. Mohr, C. Bauckhage, 兴趣点检测器的评估，《国际计算机视觉杂志》37 (2) (2000)
    151–172。'
- en: '[230] C. B. Choy, J. Gwak, S. Savarese, M. Chandraker, Universal correspondence
    network, Advances in neural information processing systems 29 (2016).'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] C. B. Choy, J. Gwak, S. Savarese, M. Chandraker, 通用对应网络，《神经信息处理系统进展》29
    (2016)。'
- en: '[231] I. Melekhov, A. Tiulpin, T. Sattler, M. Pollefeys, E. Rahtu, J. Kannala,
    Dgc-net: Dense geometric correspondence network, in: 2019 IEEE Winter Conference
    on Applications of Computer Vision (WACV), IEEE, 2019, pp. 1034–1042.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] I. Melekhov, A. Tiulpin, T. Sattler, M. Pollefeys, E. Rahtu, J. Kannala,
    Dgc-net: 密集几何对应网络，载于：2019 IEEE冬季计算机视觉应用会议 (WACV)，IEEE，2019年，页1034–1042。'
- en: '[232] X. Shen, F. Darmon, A. A. Efros, M. Aubry, Ransac-flow: generic two-stage
    image alignment, in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
    UK, August 23–28, 2020, Proceedings, Part IV 16, Springer, 2020, pp. 618–637.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] X. Shen, F. Darmon, A. A. Efros, M. Aubry, Ransac-flow: 通用两阶段图像对齐，载于：计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第IV部分16，Springer，2020年，页618–637。'
- en: '[233] P.-E. Sarlin, C. Cadena, R. Siegwart, M. Dymczyk, From coarse to fine:
    Robust hierarchical localization at large scale, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2019, pp. 12716–12725.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] P.-E. Sarlin, C. Cadena, R. Siegwart, M. Dymczyk, 从粗到细：大规模鲁棒层次定位，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2019年，页12716–12725。'
- en: '[234] X. Nan, L. Ding, Learning geometric feature embedding with transformers
    for image matching, Sensors 22 (24) (2022) 9882.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] X. Nan, L. Ding, 使用变换器进行图像匹配的几何特征嵌入，传感器 22 (24) (2022) 9882。'
- en: '[235] R. Mao, C. Bai, Y. An, F. Zhu, C. Lu, 3dg-stfm: 3d geometric guided student-teacher
    feature matching, in: European Conference on Computer Vision, Springer, 2022,
    pp. 125–142.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] R. Mao, C. Bai, Y. An, F. Zhu, C. Lu, 3dg-stfm: 3d几何引导的学生-教师特征匹配，载于：欧洲计算机视觉会议，Springer，2022年，页125–142。'
- en: '[236] K. M. Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, P. Fua, Learning
    to find good correspondences, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 2666–2674.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] K. M. Yi, E. Trulls, Y. Ono, V. Lepetit, M. Salzmann, P. Fua, 学习寻找良好对应关系，发表于：IEEE
    计算机视觉与模式识别会议论文集，2018年，页 2666–2674。'
- en: '[237] O. Wiles, S. Ehrhardt, A. Zisserman, Co-attention for conditioned image
    matching, in: Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition, 2021, pp. 15920–15929.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] O. Wiles, S. Ehrhardt, A. Zisserman, 条件图像匹配的协同注意，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2021年，页
    15920–15929。'
- en: '[238] R. Arandjelović, A. Zisserman, Three things everyone should know to improve
    object retrieval, in: 2012 IEEE conference on computer vision and pattern recognition,
    IEEE, 2012, pp. 2911–2918.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] R. Arandjelović, A. Zisserman, 提升物体检索的三件事，发表于：2012 IEEE 计算机视觉与模式识别会议，IEEE，2012年，页
    2911–2918。'
- en: '[239] I. Melekhov, G. J. Brostow, J. Kannala, D. Turmukhambetov, Image stylization
    for robust features, arXiv preprint arXiv:2008.06959 (2020).'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] I. Melekhov, G. J. Brostow, J. Kannala, D. Turmukhambetov, 图像风格化以增强鲁棒特征，arXiv
    预印本 arXiv:2008.06959 (2020)。'
- en: '[240] Y. Zhou, H. Fan, S. Gao, Y. Yang, X. Zhang, J. Li, Y. Guo, Retrieval
    and localization with observation constraints, in: 2021 IEEE International Conference
    on Robotics and Automation (ICRA), IEEE, 2021, pp. 5237–5244.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Y. Zhou, H. Fan, S. Gao, Y. Yang, X. Zhang, J. Li, Y. Guo, 带有观察约束的检索与定位，发表于：2021
    IEEE 国际机器人与自动化会议 (ICRA)，IEEE，2021年，页 5237–5244。'
- en: '[241] M. Humenberger, Y. Cabon, N. Guerin, J. Morat, V. Leroy, J. Revaud, P. Rerole,
    N. Pion, C. de Souza, G. Csurka, Robust image retrieval-based visual localization
    using kapture, arXiv preprint arXiv:2007.13867 (2020).'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] M. Humenberger, Y. Cabon, N. Guerin, J. Morat, V. Leroy, J. Revaud, P.
    Rerole, N. Pion, C. de Souza, G. Csurka, 基于图像检索的鲁棒视觉定位使用 kapture，arXiv 预印本 arXiv:2007.13867
    (2020)。'
- en: '[242] H. Germain, G. Bourmaud, V. Lepetit, S2dnet: Learning image features
    for accurate sparse-to-dense matching, in: Computer Vision–ECCV 2020: 16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16, Springer,
    2020, pp. 626–643.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] H. Germain, G. Bourmaud, V. Lepetit, S2dnet: 学习图像特征以实现准确的稀疏到密集匹配，发表于：计算机视觉–ECCV
    2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，会议论文集，第三部分 16，Springer，2020年，页 626–643。'
- en: '[243] Y. Zhao, H. Zhang, P. Lu, P. Li, E. Wu, B. Sheng, Dsd-matchingnet: Deformable
    sparse-to-dense feature matching for learning accurate correspondences, Virtual
    Reality & Intelligent Hardware 4 (5) (2022) 432–443.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Y. Zhao, H. Zhang, P. Lu, P. Li, E. Wu, B. Sheng, Dsd-matchingnet: 变形稀疏到密集特征匹配以学习准确的对应关系，虚拟现实与智能硬件
    4 (5) (2022) 432–443。'
- en: '[244] L. Chen, C. Heipke, Deep learning feature representation for image matching
    under large viewpoint and viewing direction change, ISPRS Journal of Photogrammetry
    and Remote Sensing 190 (2022) 94–112.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] L. Chen, C. Heipke, 深度学习特征表示用于大视角和观测方向变化下的图像匹配，ISPRS 遥感与摄影测量杂志 190 (2022)
    94–112。'
- en: '[245] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, B. Schiele, The cityscapes dataset for semantic urban scene
    understanding, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, 2016, pp. 3213–3223.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, B. Schiele, cityscapes 数据集用于语义城市场景理解，发表于：IEEE 计算机视觉与模式识别会议论文集，2016年，页
    3213–3223。'
- en: '[246] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, A. Torralba, Scene
    parsing through ade20k dataset, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2017, pp. 633–641.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, A. Torralba, 通过 ade20k
    数据集进行场景解析，发表于：IEEE 计算机视觉与模式识别会议论文集，2017年，页 633–641。'
- en: '[247] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo, et al., Segment anything, arXiv preprint arXiv:2304.02643
    (2023).'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T.
    Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, 等人，Segment anything，arXiv 预印本 arXiv:2304.02643
    (2023)。'
- en: '[248] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, A. Joulin,
    Emerging properties in self-supervised vision transformers, in: Proceedings of
    the IEEE/CVF international conference on computer vision, 2021, pp. 9650–9660.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, A.
    Joulin, 自监督视觉变换器中的新兴属性，发表于：IEEE/CVF 国际计算机视觉会议论文集，2021年，页 9650–9660。'
- en: '[249] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov,
    P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al., Dinov2: Learning robust
    visual features without supervision, arXiv preprint arXiv:2304.07193 (2023).'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov,
    P. Fernandez, D. Haziza, F. Massa, A. El-Nouby 等人, Dinov2: 无监督学习鲁棒视觉特征，arXiv 预印本
    arXiv:2304.07193 (2023)。'
- en: '[250] Anonymous, [Towards seamless adaptation of pre-trained models for visual
    place recognition](https://openreview.net/forum?id=TVg6hlfsKa), in: Submitted
    to The Twelfth International Conference on Learning Representations, 2023, under
    review.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] 匿名, [Towards seamless adaptation of pre-trained models for visual place
    recognition](https://openreview.net/forum?id=TVg6hlfsKa)，提交至第十二届国际学习表征会议，2023，审稿中。'
- en: URL [https://openreview.net/forum?id=TVg6hlfsKa](https://openreview.net/forum?id=TVg6hlfsKa)
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://openreview.net/forum?id=TVg6hlfsKa](https://openreview.net/forum?id=TVg6hlfsKa)
- en: '[251] X. Jiang, Y. Xia, X.-P. Zhang, J. Ma, Robust image matching via local
    graph structure consensus, Pattern Recognition 126 (2022) 108588.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] X. Jiang, Y. Xia, X.-P. Zhang, J. Ma, 通过局部图结构一致性进行鲁棒图像匹配，《模式识别》126 (2022)
    108588。'
- en: '[252] J. Ma, J. Zhao, J. Jiang, H. Zhou, X. Guo, Locality preserving matching,
    International Journal of Computer Vision 127 (2019) 512–531.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] J. Ma, J. Zhao, J. Jiang, H. Zhou, X. Guo, 局部保持匹配，《计算机视觉国际期刊》127 (2019)
    512–531。'
- en: '[253] R. Raguram, O. Chum, M. Pollefeys, J. Matas, J.-M. Frahm, Usac: A universal
    framework for random sample consensus, IEEE transactions on pattern analysis and
    machine intelligence 35 (8) (2012) 2022–2038.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] R. Raguram, O. Chum, M. Pollefeys, J. Matas, J.-M. Frahm, Usac: 一种用于随机样本一致性的通用框架，《IEEE
    模式分析与机器智能汇刊》35 (8) (2012) 2022–2038。'
- en: '[254] D. Barath, J. Noskova, M. Ivashechkin, J. Matas, Magsac++, a fast, reliable
    and accurate robust estimator, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, 2020, pp. 1304–1312.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] D. Barath, J. Noskova, M. Ivashechkin, J. Matas, Magsac++，一种快速、可靠且准确的鲁棒估计器，发表于
    IEEE/CVF 计算机视觉与模式识别会议论文集，2020，页 1304–1312。'
- en: '[255] X. Li, Z. Hu, Rejecting mismatches by correspondence function, International
    Journal of Computer Vision 89 (2010) 1–17.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] X. Li, Z. Hu, 通过对应函数拒绝不匹配，《计算机视觉国际期刊》89 (2010) 1–17。'
- en: '[256] W. Sun, W. Jiang, E. Trulls, A. Tagliasacchi, K. M. Yi, Acne: Attentive
    context normalization for robust permutation-equivariant learning, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp.
    11286–11295.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] W. Sun, W. Jiang, E. Trulls, A. Tagliasacchi, K. M. Yi, Acne: 专注上下文归一化以实现鲁棒的排列等变学习，发表于
    IEEE/CVF 计算机视觉与模式识别会议论文集，2020，页 11286–11295。'
- en: '[257] C. Zhao, Z. Cao, C. Li, X. Li, J. Yang, Nm-net: Mining reliable neighbors
    for robust feature correspondences, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2019, pp. 215–224.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] C. Zhao, Z. Cao, C. Li, X. Li, J. Yang, Nm-net: 挖掘可靠邻域以获得鲁棒的特征匹配，发表于
    IEEE/CVF 计算机视觉与模式识别会议论文集，2019，页 215–224。'
- en: '[258] J. Chen, X. Chen, S. Chen, Y. Liu, Y. Rao, Y. Yang, H. Wang, D. Wu, Shape-former:
    Bridging cnn and transformer via shapeconv for multimodal image matching, Information
    Fusion 91 (2023) 445–457.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] J. 陈, X. 陈, S. 陈, Y. 刘, Y. 饶, Y. 杨, H. 王, D. 吴, Shape-former: 通过 shapeconv
    将 cnn 和 transformer 连接起来，用于多模态图像匹配，《信息融合》91 (2023) 445–457。'
- en: '[259] J. Chen, S. Chen, X. Chen, Y. Yang, L. Xing, X. Fan, Y. Rao, Lsv-anet:
    Deep learning on local structure visualization for feature matching, IEEE Transactions
    on Geoscience and Remote Sensing 60 (2021) 1–18.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] J. 陈, S. 陈, X. 陈, Y. 杨, L. 邢, X. 范, Y. 饶, Lsv-anet: 基于局部结构可视化的深度学习用于特征匹配，《IEEE
    地球科学与遥感汇刊》60 (2021) 1–18。'
- en: '[260] F. Bellavia, L. Morelli, F. Menna, F. Remondino, Image orientation with
    a hybrid pipeline robust to rotations and wide-baselines, The International Archives
    of the Photogrammetry, Remote Sensing and Spatial Information Sciences 46 (2022)
    73–80.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] F. Bellavia, L. Morelli, F. Menna, F. Remondino, 使用对旋转和宽基线鲁棒的混合管线进行图像定向，《国际摄影测量、遥感与空间信息科学档案》46
    (2022) 73–80。'
- en: '[261] F. Bellavia, D. Mishkin, Harrisz+: Harris corner selection for next-gen
    image matching pipelines, Pattern Recognition Letters 158 (2022) 141–147.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] F. Bellavia, D. Mishkin, Harrisz+: 针对下一代图像匹配管线的 Harris 角点选择，《模式识别通讯》158
    (2022) 141–147。'
- en: '[262] F. Bellavia, Image matching by bare homography, IEEE Transactions on
    Image Processing (2024).'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] F. Bellavia, 通过裸同质性进行图像匹配，《IEEE 图像处理汇刊》（2024）。'
