- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:51:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:51:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.02511] A Survey on Recent Deep Learning-driven Singing Voice Synthesis
    Systems'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.02511] 最近深度学习驱动的歌声合成系统调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.02511](https://ar5iv.labs.arxiv.org/html/2110.02511)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.02511](https://ar5iv.labs.arxiv.org/html/2110.02511)
- en: A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最近深度学习驱动的歌声合成系统调查
- en: Yin-Ping Cho, Fu-Rong Yang, Yung-Chuan Chang, Ching-Ting Cheng, Xiao-Han Wang,
    Yi-Wen Liu Department of Electrical Engineering
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yin-Ping Cho, Fu-Rong Yang, Yung-Chuan Chang, Ching-Ting Cheng, Xiao-Han Wang,
    Yi-Wen Liu 电气工程系
- en: National Tsing Hua University Hsinchu, Taiwan
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 国立清华大学，新竹，台湾
- en: yinping.cho@outlook.com; ywliu@ee.nthu.edu.tw
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: yinping.cho@outlook.com; ywliu@ee.nthu.edu.tw
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Singing voice synthesis (SVS) is a task that aims to generate audio signals
    according to musical scores and lyrics. With its multifaceted nature concerning
    music and language, producing singing voices indistinguishable from that of human
    singers has always remained an unfulfilled pursuit. Nonetheless, the advancements
    of deep learning techniques have brought about a substantial leap in the quality
    and naturalness of synthesized singing voice. This paper aims to review some of
    the state-of-the-art deep learning-driven SVS systems. We intend to summarize
    their deployed model architectures and identify the strengths and limitations
    for each of the introduced systems. Thereby, we picture the recent advancement
    trajectory of this field and conclude the challenges left to be resolved both
    in commercial applications and academic research.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 歌声合成（SVS）是一项旨在根据乐谱和歌词生成音频信号的任务。由于其涉及音乐和语言的多面性，产生与人类歌手无异的歌声始终是一个未能实现的追求。然而，深度学习技术的进步带来了合成歌声质量和自然性的显著提升。本文旨在综述一些最先进的深度学习驱动的
    SVS 系统。我们打算总结它们的模型架构，并识别每个系统的优点和局限性。通过此，我们描绘了该领域最近的发展轨迹，并总结了商业应用和学术研究中尚待解决的挑战。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: sining voice synthesis, deep learning, review paper
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 歌声合成，深度学习，综述论文
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一、引言
- en: A singing voice synthesis (SVS) system is able to generate singing voice from
    a given musical score. For the future of music composition, one can imagine that
    a song can be listened to immediately after the song has been composed without
    recording. Recently, several approaches have been proposed to build a natural
    singing voice synthesis system in Japanese, English, Korean, Spanish, and so on
    [[14](#bib.bib14), [18](#bib.bib18), [1](#bib.bib1)]. Different from speech synthesis,
    the synthesized singing voice needs to follow the musical scores; performance
    of pitch and rhythm synthesis would directly influence the perceived quality.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个歌声合成（SVS）系统能够从给定的乐谱生成歌声。展望未来的音乐创作，可以想象在歌曲创作完成后，歌曲可以立即被听到而无需录音。近年来，已经提出了几种方法来构建自然的歌声合成系统，支持日语、英语、韩语、西班牙语等语言
    [[14](#bib.bib14), [18](#bib.bib18), [1](#bib.bib1)]。与语音合成不同，合成的歌声需要遵循乐谱；音高和节奏合成的表现会直接影响感知质量。
- en: Before neural networks were widely used, unit concatenation [[28](#bib.bib28)]
    and hidden Markov Model (HMM) [[16](#bib.bib16)] approaches were adopted for SVS.
    The unit concatenation synthesizer generates the singing voice by selecting the
    voice elements in the database and concatenation is performed consecutively. Commercially
    available tools such as Vocaloid [[15](#bib.bib15)] and Synthesizer V¹¹1https://synthesizerv.com/en/
    have successfully gathered loyal groups of users. In contrast, HMM-based SVS [[16](#bib.bib16)]
    can model the spectral envelopes, excitation, and the singing voice duration separately.
    Then, speech parameter generation algorithms [[29](#bib.bib29)] are used to produce
    singing voice parameter trajectories. As HMM predates the advances in deep learning,
    the naturalness of HMM-based SVS is outperformed by what could now be achieved
    by neural networks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络被广泛使用之前，采用了单元连接 [[28](#bib.bib28)] 和隐马尔可夫模型（HMM） [[16](#bib.bib16)] 方法进行
    SVS。单元连接合成器通过选择数据库中的语音元素来生成歌声，并进行连续连接。诸如 Vocaloid [[15](#bib.bib15)] 和 Synthesizer
    V¹¹1https://synthesizerv.com/en/ 之类的商业工具成功地聚集了忠实用户群。相比之下，基于 HMM 的 SVS [[16](#bib.bib16)]
    可以分别建模谱包络、激励和歌声持续时间。然后，使用语音参数生成算法 [[29](#bib.bib29)] 生成歌声参数轨迹。由于 HMM 早于深度学习的进展，HMM
    基础的 SVS 的自然性不如现在神经网络所能实现的效果。
- en: Over the past few years, several types of neural networks have been employed
    for SVS, such as generic deep neural networks (DNN) [[17](#bib.bib17)], convolutional
    neural networks [[20](#bib.bib20)], a recurrent neural network with long-short
    term memory (LSTM) [[18](#bib.bib18)], and generative adversarial networks (GAN)
    [[19](#bib.bib19)]. Besides, taking advantage of the similarity to text-to-speech
    (TTS), some autoregressive sequence-to-sequence(Seq2Seq) models have been proposed
    [[21](#bib.bib21), [22](#bib.bib22)]. In recent time, state-of-the-art deep learning
    architectures are adopted to tackle with the SVS task, such as the Transformer-based
    [[23](#bib.bib23)] XiaoicSing [[2](#bib.bib2)], HifiSinger [[6](#bib.bib6)] and
    diffusion denoising probabilistic model [[9](#bib.bib9)] like DiffSinger [[8](#bib.bib8)].
    However, these models typically need a large corpus for training; meanwhile, systems
    designed for lower data consumption, such as LiteSing [[10](#bib.bib10)] and Sinsy
    [[14](#bib.bib14)], are now a heated research direction.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，几种类型的神经网络被用于SVS，如通用深度神经网络（DNN）[[17](#bib.bib17)]、卷积神经网络[[20](#bib.bib20)]、长短期记忆（LSTM）的递归神经网络[[18](#bib.bib18)]以及生成对抗网络（GAN）[[19](#bib.bib19)]。此外，利用与文本到语音（TTS）的相似性，一些自回归序列到序列（Seq2Seq）模型也被提出[[21](#bib.bib21),
    [22](#bib.bib22)]。近期，最先进的深度学习架构被用来解决SVS任务，如基于Transformer的[[23](#bib.bib23)] XiaoicSing
    [[2](#bib.bib2)]、HifiSinger [[6](#bib.bib6)] 和扩散去噪概率模型[[9](#bib.bib9)]，如DiffSinger
    [[8](#bib.bib8)]。然而，这些模型通常需要大量的语料进行训练；同时，为了降低数据消耗，诸如LiteSing [[10](#bib.bib10)]
    和 Sinsy [[14](#bib.bib14)] 的系统现在成为了热门研究方向。
- en: To mitigate the one-to-many difficulty of directly predicting the raw waveform
    from a musical score, recent deep learning-driven SVS systems mostly employ an
    acoustic model-vocoder architecture as in Fig [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems"). In
    this setup, the vocoder maps frame-level parameters to the waveform, and the acoustic
    model only has to predict a parameter sequence with a length of the framed target
    waveform. This way, the mapping from the score to waveform is broken down into
    two simpler sub-tasks of lower dimension discrepancy between the mapping. Depending
    on the chosen vocoder, the frame-level synthesis parameters can be Mel-spectrograms
    [[6](#bib.bib6), [8](#bib.bib8)] or parameters designed with more insights into
    the human voice, such as those with explicit and separate F0 values [[2](#bib.bib2),
    [14](#bib.bib14), [10](#bib.bib10)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻从音乐谱直接预测原始波形的一对多困难，近期深度学习驱动的SVS系统大多采用了如图[1](#S1.F1 "图 1 ‣ I 介绍 ‣ 对近期深度学习驱动的歌声合成系统的调查")所示的声学模型-声码器架构。在这种设置中，声码器将帧级参数映射到波形，而声学模型只需要预测一个长度为目标波形的参数序列。这样，谱到波形的映射被拆解为两个更简单的低维度差异子任务。根据所选择的声码器，帧级合成参数可以是Mel谱图[[6](#bib.bib6),
    [8](#bib.bib8)]，也可以是对人声有更多洞察的参数，如具有明确和独立F0值的参数[[2](#bib.bib2), [14](#bib.bib14),
    [10](#bib.bib10)]。
- en: 'The rest of this paper is organized as follows: Section II overviews three
    landmark deep learning SVS systems with high fidelity and naturalness. Two SVS
    systems with particular designs for low data resource training are described in
    section III. Challenges and future directions of this topic are reported in Section
    IV, and the conclusions are summarized in V.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：第二部分概述了三种具有高保真度和自然性的深度学习SVS系统。第三部分描述了两种特别设计用于低数据资源训练的SVS系统。第四部分报告了这一主题的挑战和未来方向，第五部分总结了结论。
- en: '![Refer to caption](img/2b06940171482c7637bff4f36a89123f.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2b06940171482c7637bff4f36a89123f.png)'
- en: 'Figure 1: General architecture of recent deep learning-driven SVS systems.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：近期深度学习驱动的SVS系统的通用架构。
- en: II Naturalness and Audio Quality Milestones
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 自然性和音质里程碑
- en: 'II-A XiaoiceSing: Transformer + WORLD'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A XiaoiceSing：Transformer + WORLD
- en: XiaoiceSing [[2](#bib.bib2)] was among the first of the deep learning-driven
    SVS systems that saw commercial deployment. It aimed to generate a singing voice
    with accurate pitch and rhythm that sounded natural and human-like. This system
    adopts the architecture of the FastSpeech [[3](#bib.bib3)] augmented with singing-specific
    modifications for the acoustic model and uses WORLD as the vocoder [[4](#bib.bib4)].
    The singing-specific modifications mainly concern the addition of note duration
    and note pitch information. To ensure the correctness of rhythm, the authors proposed
    adding a syllable-level duration loss instead of relying solely on the phoneme-level
    duration as the original FastSpeech. As for a more robust pitch, the note pitch
    residually connects to the F0 output; therefore, the acoustic model only has to
    predict the relative F0 variations in the human singing voice and add it onto
    the given note pitch contour. The experiments showed that XiaoiceSing outperformed
    the baseline system [[5](#bib.bib5)] in both subjective and objective evaluations.
    In particular, the A/B tests showed a dominant preference favoring XiaoiceSing
    in terms of F0 and rhythm naturalness, proving the effectiveness of the proposed
    modifications. However, the authors remarked that the mean opinions scores (MOS)
    of XiaoiceSing were held back by the waveform quality bound of the WORLD vocoder,
    although the acoustic model performed well in objective metrics. Also, this system
    consumed a large total of 74 hours of single professional singer’s training data,
    which would be inaccessible in a purely academic context.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: XiaoiceSing [[2](#bib.bib2)] 是最早商业化部署的深度学习驱动的SVS系统之一。其目标是生成具有准确音高和节奏的自然且类似人声的歌声。该系统采用了FastSpeech
    [[3](#bib.bib3)] 的架构，并对声学模型进行了歌唱特定的修改，同时使用了WORLD作为声码器 [[4](#bib.bib4)]。歌唱特定的修改主要涉及添加音符时长和音符音高信息。为了确保节奏的正确性，作者建议添加音节级别的时长损失，而不是仅依赖原始FastSpeech中的音素级别时长。至于更稳健的音高，音符音高会残差地连接到F0输出，因此声学模型只需预测人声歌唱中的相对F0变化，并将其加到给定的音符音高轮廓上。实验表明，XiaoiceSing在主观和客观评估中均优于基线系统
    [[5](#bib.bib5)]。特别是，A/B测试显示，在F0和节奏自然性方面，XiaoiceSing具有明显的偏好，证明了所提修改的有效性。然而，作者指出，XiaoiceSing的平均意见得分（MOS）受限于WORLD声码器的波形质量限制，尽管声学模型在客观指标中表现良好。此外，该系统消耗了总计74小时的单一专业歌手的训练数据，这在纯学术环境中无法获得。
- en: 'II-B HiFiSinger: Transformer + Neural Vocoder'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'II-B HiFiSinger: Transformer + Neural Vocoder'
- en: Building on the foundation of XiaoiceSing, HiFiSinger [[6](#bib.bib6)] aims
    to defy its waveform quality limitations. While HiFiSinger adopted the same FastSpeech-based
    acoustic model of XiaoiceSing, it swapped out WORLD and employed a Parallel WaveGAN
    (PW-GAN) [[7](#bib.bib7)] neural vocoder to generate waveforms at a high-fidelity
    48kHz sample rate. Therefore, the acoustic model is modified to predict a Mel-spectrogram
    as the main synthesis parameter and F0 contour with voiced/unvoiced (U/UV) flags
    as auxiliaries for the neural vocoder. To prompt better Mel-spectrogram fidelity,
    the authors proposed a sub-frequency GAN (SF-GAN) scheme where three discriminator
    networks operate on three overlapping sub-bands. This way, the GAN network should
    avoid the time-frequency resolution tradeoff. On the vocoder side, the main addition
    is the multi-length GAN (ML-GAN) that deploys multiple discriminator networks
    working on different waveform input lengths. This should make the discriminator
    handle both the long-time structures and short-time details better than the original
    single network. Also, predicted F0 and U/UV flags are integrated as auxiliary
    features for the modified PW-GAN alongside the Mel-spectrogram. The comparative
    mean opinion score ablation test confirmed that these modifications all positively
    contributed to the final system. In terms of perceived naturalness and audio quality,
    this system proved to be a significant improvement over XiaoiceSing in the MOS
    test. In the 48kHz generation configuration, HiFiGAN’s MOS (3.76) approached that
    of the ground truth high-fidelity recordings (4.03).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 XiaoiceSing 的基础上，HiFiSinger [[6](#bib.bib6)] 旨在突破其波形质量的限制。尽管 HiFiSinger 采用了与
    XiaoiceSing 相同的基于 FastSpeech 的声学模型，但它替换了 WORLD，并使用了 Parallel WaveGAN (PW-GAN)
    [[7](#bib.bib7)] 神经声码器，以高保真度的 48kHz 采样率生成波形。因此，声学模型被修改为预测 Mel 频谱图作为主要合成参数，并将 F0
    轮廓及有声/无声 (U/UV) 标志作为神经声码器的辅助参数。为了提高 Mel 频谱图的保真度，作者提出了一种子频率 GAN (SF-GAN) 方案，其中三个鉴别网络在三个重叠的子带上进行操作。这种方式使得
    GAN 网络可以避免时间-频率分辨率的权衡。在声码器方面，主要的新增项是多长度 GAN (ML-GAN)，它部署了多个鉴别网络处理不同波形输入长度。这应该使鉴别器比原来的单一网络更好地处理长期结构和短期细节。此外，预测的
    F0 和 U/UV 标志被整合为修改后的 PW-GAN 的辅助特征，与 Mel 频谱图一起使用。比较平均意见评分的消融测试确认了这些修改对最终系统的积极贡献。在感知自然性和音频质量方面，该系统在
    MOS 测试中证明了相较于 XiaoiceSing 的显著改进。在 48kHz 生成配置中，HiFiGAN 的 MOS (3.76) 接近于真实高保真录音
    (4.03)。
- en: 'II-C DiffSinger: Denoising Diffusion Probabilistic Model + Neural Vocoder'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'II-C DiffSinger: 去噪扩散概率模型 + 神经声码器'
- en: 'To further enhance the acoustic model’s prediction accuracy and robustness
    of Mel-spectrograms, DiffSinger [[8](#bib.bib8)] utilizes a generative model paradigm
    novel at the time of this writing: the denoising diffusion probabilistic model
    [[9](#bib.bib9)]. Instead of directly optimizing the acoustic model to generate
    Mel-spectrograms, DiffSinger formulates the generation task into a parameterized
    Markov chain conditioned on the musical score. The diffusion process of this Markov
    chain gradually scales the Mel-spectrogram and applies noise until it becomes
    Gaussian noise. Conversely, the denoising process iteratively subtracts a portion
    of noise from the noisy input and rescales it until it becomes a Mel-spectrogram.
    To improve the speed and robustness of the denoising process, the authors proposed
    a shallow diffusion mechanism for denoising inference. This mechanism utilizes
    a simple auxiliary decoder seen in Transformer-based acoustic models [[2](#bib.bib2),
    [6](#bib.bib6)] to generate a rough Mel-spectrogram from the musical score. The
    denoising process can use this rough approximation as a starting point much closer
    to the Mel-spectrogram end of the Markov chain. Therefore, the inference process
    starts denoising from a noisy input with resemblance to the target Mel-spectrogram
    and requires much fewer steps to complete. The resulting acoustic model showed
    substantial quality improvements in MOS over its state-of-the-art counterparts
    in both text-to-speech (TTS) and SVS tasks. Also, with the shallow diffusion mechanism,
    it achieved a real-time factor (RTF) of 0.191 on a single RTX V100, meaning that
    it has real-time applicability.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高Mel-spectrograms的声学模型预测准确性和鲁棒性，DiffSinger [[8](#bib.bib8)] 采用了一种在撰写时新颖的生成模型范式：去噪扩散概率模型
    [[9](#bib.bib9)]。DiffSinger并不是直接优化声学模型以生成Mel-spectrograms，而是将生成任务表述为一个以音乐谱为条件的参数化马尔科夫链。这个马尔科夫链的扩散过程逐渐缩放Mel-spectrogram并施加噪声，直到它变成高斯噪声。相反，去噪过程则是从嘈杂的输入中迭代地减去一部分噪声并重新缩放，直到它变成Mel-spectrogram。为了提高去噪过程的速度和鲁棒性，作者提出了一种用于去噪推断的浅层扩散机制。该机制利用在基于Transformer的声学模型中看到的简单辅助解码器
    [[2](#bib.bib2), [6](#bib.bib6)] 从音乐谱生成一个粗略的Mel-spectrogram。去噪过程可以将这个粗略的近似值作为接近马尔科夫链终端Mel-spectrogram的起点。因此，推断过程从一个与目标Mel-spectrogram相似的嘈杂输入开始去噪，并且需要完成的步骤大大减少。结果显示，该声学模型在MOS上相比于其最先进的对手，在文本到语音（TTS）和声乐合成（SVS）任务中均显示出显著的质量改进。此外，凭借浅层扩散机制，它在单个RTX
    V100上的实时因子（RTF）为0.191，这意味着它具有实时适用性。
- en: III Towards Data Efficiency
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 数据效率的提升
- en: 'III-A Sinsy: DNN + Neural Vocoder'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'III-A Sinsy: DNN + 神经声码器'
- en: 'Sinsy [[14](#bib.bib14)] is designed to synthesize singing voices at appropriate
    timing from a musical score. It contains four modules: 1) a “time-lag model” which
    controls vocal start timing of each note for adapting human singing habits; 2)
    a “duration model” which estimates phoneme durations for pre-expanding features
    into the frame level; 3) a DNN-based “acoustic model” which plays the role of
    a mapping function from the score feature sequences to the acoustic feature sequences;
    4) a PeriodNet “neural vocoder” which generates time-domain waveform samples conditioned
    on acoustic features. Besides, Sinsy applied singing-specific techniques including
    accurately modeling pitch by predicting the residual connection between the note
    pitch and the output F0 in log scale, and the vibrato modeling which expresses
    fluctuations with the difference between the original F0 sequence and the smoothed
    one. Moreover, Sinsy proposed two automatic pitch correction strategies, the prior
    distribution of pitch and the pseudo-note pitch, to prevent singing voices from
    becoming out of tune. Sinsy adopted 1-hour Japanese children’s songs performed
    by a female singer for training. The mean opinion scores in subjective evaluation
    tests concluded that the proposed system could synthesize a singing voice with
    better start timing of vocal, more natural vibrato, and more accurate singing
    pitch.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Sinsy [[14](#bib.bib14)] 旨在从乐谱中合成适时的歌唱声音。它包含四个模块：1）一个“时间延迟模型”，控制每个音符的声音起始时间，以适应人类的唱歌习惯；2）一个“持续时间模型”，估计音素的持续时间，以将预扩展特征映射到帧级；3）一个基于DNN的“声学模型”，作为从乐谱特征序列到声学特征序列的映射函数；4）一个PeriodNet
    “神经声码器”，根据声学特征生成时域波形样本。此外，Sinsy应用了歌唱特定的技术，包括通过预测音符音高与输出F0之间的残差连接来准确建模音高，以及通过表达原始F0序列与平滑序列之间的差异来建模颤音。此外，Sinsy提出了两种自动音高修正策略，音高的先验分布和伪音符音高，以防止歌唱声音走音。Sinsy采用了由女性歌手演唱的1小时日本儿童歌曲进行训练。主观评价测试中的平均意见分数表明，所提系统能够合成具有更好声音起始时间、更自然颤音和更准确音高的歌唱声音。
- en: 'III-B LiteSing: WaveNet + WORLD'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'III-B LiteSing: WaveNet + WORLD'
- en: 'LiteSing [[10](#bib.bib10)] was designed to be a fast, high-quality SVS system
    with an efficient architecture that requires little training data. Instead of
    pursuing marginal audio quality gain with neural vocoders, LiteSing goes back
    to using WORLD as the vocoder. This is a conscious choice meant to utilize WORLD’s
    characteristic of separating instantaneous spectral envelopes from F0, making
    the prediction task simpler for the acoustic model. Furthermore, LiteSing employed
    a condition predictor that separately predicts dynamic acoustic energy, V/UV flags,
    and dynamic pitch curve, which means the decoder only has to predict the spectral
    envelope and aperiodic components with a much lower variance. That is, LiteSing
    disentangles the compound and complicated prediction task into well-defined and
    simpler sub-tasks for the acoustic model. Therefore, the authors could employ
    a relatively small and fast non-autoregressive WaveNet [[11](#bib.bib11)] as the
    model’s backbone and still expect robust and high-quality synthesis. In addition,
    the authors added a Wasserstein GAN (WGAN) [[12](#bib.bib12)] for the predicted
    acoustic features to combat the over-smoothing problem typical in WORLD parameter
    prediction tasks. The experiments limited the dataset to 48 minutes of audio to
    test LiteSing’s data efficiency. In the MOS evaluation, LiteSing achieved a score
    (3.60) comparable to that of the WORLD-resynthesized human singer’s audio (3.86).
    More importantly, although the comparing FastSpeech2 baseline yielded a marginally
    higher score (3.63), LiteSing used only one-fifteenth of the number of parameters
    (3.8M vs. 57.0M), proving its superior data efficiency; also, LiteSing was significantly
    preferred over FastSpeech2 in the A/B test for expressiveness. These results demonstrated
    the quality and efficiency of the proposed acoustic model. However, the tradeoff
    of using WORLD was obvious: the original audio received a MOS of 4.20, substantially
    higher than the WORLD-resynthesized version (3.86), and the audio fully synthesized
    by LiteSing (3.60).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LiteSing [[10](#bib.bib10)] 被设计成一个快速、高质量的 SVS 系统，具有高效的架构，只需要少量的训练数据。LiteSing
    不再追求使用神经声码器（neural vocoders）来获得微小的音频质量提升，而是回到使用 WORLD 作为声码器。这是一个有意识的选择，旨在利用 WORLD
    将瞬时谱包络与 F0 分离的特性，使得预测任务对声学模型来说更为简单。此外，LiteSing 采用了一个条件预测器，分别预测动态声学能量、V/UV 标志和动态音高曲线，这意味着解码器只需预测谱包络和无周期成分，方差大大降低。也就是说，LiteSing
    将复杂的预测任务解构为定义明确且简单的子任务。因此，作者可以使用一个相对较小且快速的非自回归 WaveNet [[11](#bib.bib11)] 作为模型的骨干，同时仍然期望稳健且高质量的合成。此外，作者还添加了
    Wasserstein GAN (WGAN) [[12](#bib.bib12)] 来处理预测声学特征中的过平滑问题，这是 WORLD 参数预测任务中典型的问题。实验将数据集限制为
    48 分钟音频，以测试 LiteSing 的数据效率。在 MOS 评估中，LiteSing 获得了一个与 WORLD 重新合成的人工歌手音频（3.86）相当的分数（3.60）。更重要的是，尽管比较的
    FastSpeech2 基线得分稍高（3.63），LiteSing 仅使用了 FastSpeech2 参数数量的十五分之一（3.8M 对 57.0M），证明了其卓越的数据效率；此外，LiteSing
    在 A/B 测试中在表现力方面显著优于 FastSpeech2。这些结果展示了所提声学模型的质量和效率。然而，使用 WORLD 的权衡是显而易见的：原始音频获得了
    4.20 的 MOS，显著高于 WORLD 重新合成版本（3.86），以及 LiteSing 完全合成的音频（3.60）。
- en: IV Challenges
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 挑战
- en: IV-A Data Efficiency
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 数据效率
- en: 'In Table [I](#S4.T1 "TABLE I ‣ IV-A Data Efficiency ‣ IV Challenges ‣ A Survey
    on Recent Deep Learning-driven Singing Voice Synthesis Systems"), we can observe
    a tradeoff between the amount of data and the synthesis quality. Systems that
    achieved nearly human-level quality [[2](#bib.bib2), [6](#bib.bib6), [8](#bib.bib8)]
    were those designed to be more data-driven and consumed multiple hours of a singing
    voice to train. On the other hand, although some systems may function with around
    or less than one hour of data [[14](#bib.bib14), [10](#bib.bib10)], their measures
    of simplification usually come with degradations that sum to a compromised overall
    synthesis quality. Since composing a singing voice dataset requires high-fidelity
    recording equipment, a good singer, and substantial post-editing and score annotating
    efforts, data for SVS systems are costly to obtain. Therefore, the data efficiency
    of an SVS system may dictate its real-world applicability. To enhance data efficiency,
    the strategy usually combines two techniques: simplifying the neural network modules
    [[14](#bib.bib14), [10](#bib.bib10)] and decomposing the SVS task into lower-dimension
    sub-tasks by expert knowledge in singing voices [[14](#bib.bib14)].'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [I](#S4.T1 "TABLE I ‣ IV-A Data Efficiency ‣ IV Challenges ‣ A Survey on
    Recent Deep Learning-driven Singing Voice Synthesis Systems")中，我们可以观察到数据量与合成质量之间的权衡。那些达到近乎人类水平质量的系统[[2](#bib.bib2),
    [6](#bib.bib6), [8](#bib.bib8)]是设计为数据驱动的，并消耗了多个小时的歌声进行训练。另一方面，虽然一些系统可能在大约或少于一个小时的数据下运行[[14](#bib.bib14),
    [10](#bib.bib10)]，但它们的简化措施通常伴随着总的合成质量的下降。由于编制歌声数据集需要高保真录音设备、良好的歌手以及大量的后期编辑和乐谱标注工作，因此SVS系统的数据获取成本很高。因此，SVS系统的数据效率可能决定其在现实世界中的适用性。为了提高数据效率，策略通常结合了两种技术：简化神经网络模块[[14](#bib.bib14),
    [10](#bib.bib10)]和通过在歌声领域的专家知识将SVS任务分解为低维子任务[[14](#bib.bib14)]。
- en: 'TABLE I: Summary of data usage'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：数据使用总结
- en: '| System | Amount of Singing voice data consumed |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 消耗的歌声数据量 |'
- en: '| --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| XiaoiceSing | 74 hours |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| XiaoiceSing | 74小时 |'
- en: '| HiFiSinger | 11 hours |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| HiFiSinger | 11小时 |'
- en: '| DiffSinger | 6 hours |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| DiffSinger | 6小时 |'
- en: '| Sinsy | 1 hour |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Sinsy | 1小时 |'
- en: '| LiteSing | 48 minutes |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| LiteSing | 48分钟 |'
- en: IV-B The lack of unified open datasets
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 缺乏统一的开放数据集
- en: Producing an SVS dataset requires high cost of recording and annotating. Furthermore,
    due to copyright concerns, there are very few such datasets and those that are
    publicly available are usually confined to singing old folk songs without copyright
    [[24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)]. Consequently, it is difficult
    for new researchers without resources to enter the field, and it is tricky for
    the community to objectively compare and evaluate the quality of different systems,
    as each of them is usually trained on a unique, proprietary dataset. Also, SVS
    systems that facilitate these open datasets may suffer from domain discrepancies,
    where the training dataset contains old folk songs while the songs of synthesis
    interest are of modern styles. As a comparison, text-to-speech (TTS) is a generation
    task with similar characteristics and complexity; however, an ample amount of
    free, open datasets is available for TTS [[30](#bib.bib30), [31](#bib.bib31)],
    which allows the field to progress rapidly and see a wide array of successful
    architectures and readily available commercial products. Although this issue is
    not a technical one, it is a significant limiting factor for the present and the
    prospect of SVS research.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 生成SVS数据集需要高成本的录音和标注。此外，由于版权问题，这类数据集非常稀少，公开可用的通常限于无版权的旧民歌[[24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26)]。因此，没有资源的新研究者很难进入这一领域，而社区也很难客观比较和评估不同系统的质量，因为每个系统通常在独特的专有数据集上进行训练。此外，促进这些开放数据集的SVS系统可能会遭遇领域差异的问题，即训练数据集包含旧民歌，而合成兴趣的歌曲则是现代风格。相比之下，文本到语音（TTS）是一个具有类似特征和复杂性的生成任务；然而，TTS有大量免费的开放数据集[[30](#bib.bib30),
    [31](#bib.bib31)]，这使得该领域得以快速发展，并看到各种成功的架构和现成的商业产品。虽然这个问题不是技术性的，但它是当前和未来SVS研究的一个重大限制因素。
- en: IV-C The lack of interpretability and transparency
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 缺乏可解释性和透明性
- en: Deep learning has pushed the envelope of SVS systems in terms of synthesized
    audio quality. Nevertheless, deep learning systems’ complexity makes it almost
    impossible to analytically understand the learned mapping from the input musical
    score to the end output waveform. This characteristic prevents researchers from
    extracting knowledge about the mechanisms that comprise the process of singing.
    Consequently, it substantially weakens the motivation of doing SVS research as
    a reverse-engineering means looking to gain insights about human vocalization
    and perception of music. Although this issue does not prevent application-driven
    research, it terminates these works’ potential to extend into fields such as medicine
    or psychoacoustics. Considering the high cost of constructing SVS systems, improving
    the interpretability and transparency of deep learning-driven SVS systems is worthy
    of research effort.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习推动了SVS系统在合成音质方面的极限。然而，深度学习系统的复杂性使得从输入乐谱到最终输出波形的学习映射几乎无法进行分析理解。这一特性阻碍了研究人员提取关于唱歌过程机制的知识。因此，它大大削弱了作为逆向工程手段的SVS研究动机，无法深入了解人类发声和音乐感知。尽管这个问题不会阻碍应用驱动的研究，但它终结了这些工作的潜力扩展到如医学或心理声学等领域。考虑到构建SVS系统的高成本，提高深度学习驱动的SVS系统的可解释性和透明性是值得研究的。
- en: IV-D Absence of emotion and singing technique control and variations
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 情感缺失和唱歌技巧控制及变化
- en: 'The high variation and diversity of emotion and vocalization techniques are
    quintessential to singing voices. However, although state-of-the-art deep learning-driven
    SVS systems can synthesize singing voices with reasonable naturalness and sound
    quality, they offer no means to condition these systems to synthesize with specific
    emotions or singing techniques. This is an advanced issue we surmise will become
    a research hot spot for two reasons: 1) In a commercial context, being able to
    sing according to the user’s desired style for the songs is basic functionality
    of professional singers. Without similar controllability or customizability, an
    SVS system lacks completeness to enter the real-world market. 2) This is an issue
    entangled with data efficiency. It is proven in a similar high-variance speech
    synthesis research [[27](#bib.bib27)] that conditioning the system on implicit
    features such as emotion and prosodic styles are instrumental to enhancing the
    system’s robustness and naturalness given the same training data. In cases that
    these implicit features are statistically dominant to the synthesis target, the
    absence of these conditions may lead to failure of convergence. Therefore, we
    are confident that this issue is one that should be put in the spotlight of future
    SVS research.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 情感和发声技巧的高变化性和多样性是唱歌声音的本质特征。然而，尽管最先进的深度学习驱动的SVS系统能够以合理的自然度和音质合成歌声，但它们并未提供调节系统以合成特定情感或唱歌技巧的方法。这是一个高级问题，我们推测将成为研究的热点，原因有二：1）在商业背景下，能够按照用户所需的风格演唱歌曲是专业歌手的基本功能。没有类似的可控性或自定义功能，SVS系统缺乏进入现实市场的完整性。2）这是一个与数据效率相关的问题。在类似的高变异语音合成研究[[27](#bib.bib27)]中，系统在隐含特征如情感和韵律风格上的条件化被证明对提高系统的鲁棒性和自然性具有重要作用。在这些隐含特征在合成目标中占主导地位的情况下，缺乏这些条件可能会导致收敛失败。因此，我们有信心这个问题应该成为未来SVS研究的重点。
- en: V Conclusions
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: In this work, we reviewed some of the deep learning-driven SVS systems that
    are representative of this research topic. We have shown that these methods have
    demonstrated synthesis quality and naturalness comparable to real human singers.
    Despite these achievements, challenges yet to be resolved for this topic were
    identified — namely, the need for better data efficiency of the systems, the lack
    of open and unified benchmark datasets like those available in TTS research, deep
    neural networks’ inherent absence of interpretability, and the lack of control
    and explicit variations of emotion and singing techniques quintessential for singing
    voice synthesis. These issues hurdle the commercial deployment of SVS systems
    and are of high academic interest. Therefore, we can expect them to be the center
    of advancements in the coming times.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们回顾了一些具有代表性的深度学习驱动的SVS系统，这些系统在该研究主题中具有代表性。我们已经展示了这些方法在合成质量和自然性上与真实人类歌手相当。尽管取得了这些成就，但仍然发现了尚待解决的挑战——即系统数据效率的提升需求、缺乏类似TTS研究中可用的开放和统一的基准数据集、深度神经网络固有的不可解释性以及缺乏对情感和歌唱技巧的控制和明确变异，这些都是歌声合成的核心问题。这些问题阻碍了SVS系统的商业部署，并且具有很高的学术兴趣。因此，我们可以预期它们将在未来的发展中成为重点。
- en: References
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Blaauw and J. Bonada, “A neural parametric singing synthesizer modeling
    timbre and expression from natural songs,” in *Applied Sciences*, 2017, p. 1313.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Blaauw and J. Bonada, “一个神经参数化歌声合成器，从自然歌曲中建模音色和表情，” 在 *应用科学*，2017年，p.
    1313。'
- en: '[2] P. Lu, J. Wu, J. Luan, X. Tan, and L. Zhou, “XiaoiceSing: A high-quality
    and integrated singing voice synthesis system,” *arXiv* preprint arXiv:2006.06261,
    2020.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] P. Lu, J. Wu, J. Luan, X. Tan, and L. Zhou, “XiaoiceSing: 一个高质量且集成的歌声合成系统，”
    *arXiv* 预印本 arXiv:2006.06261, 2020。'
- en: '[3] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu, “FastSpeech:
    Fast, robust and controllable text to speech,” in *Conference on Neural Information
    Processing Systems (NeurIPS)*, 2019, pp. 3165–3174.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu, “FastSpeech:
    快速、稳健且可控的文本到语音合成，” 在 *神经信息处理系统会议（NeurIPS）*，2019年，pp. 3165–3174。'
- en: '[4] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: A vocoder-based high-quality
    speech synthesis system for real-time applications,” in *IEICE transactions on
    information and systems*, 2016, pp. 1877-1884.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: 一个基于声码器的高质量语音合成系统，用于实时应用，”
    在 *IEICE 信息与系统交易*，2016年，pp. 1877-1884。'
- en: '[5] K. Nakamura, S. Takaki, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda,
    “Fast and high-quality singing voice synthesis system based on convolutional neural
    networks,” *arXiv* preprint arXiv:1910.11690, 2019.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] K. Nakamura, S. Takaki, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda,
    “基于卷积神经网络的快速高质量歌声合成系统，” *arXiv* 预印本 arXiv:1910.11690, 2019。'
- en: '[6] J. Chen, X. Tan, J. Luan, T. Qin, and T. Liu, “HiFiSinger: Towards high-fidelity
    neural singing voice synthesis,” *arXiv* preprint arXiv:2009.01776, 2020.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. Chen, X. Tan, J. Luan, T. Qin, and T. Liu, “HiFiSinger: 迈向高保真神经歌声合成，”
    *arXiv* 预印本 arXiv:2009.01776, 2020。'
- en: '[7] R. Yamamoto, E. Song and J. Kim, “Parallel WaveGAN: A fast waveform generation
    model based on generative adversarial networks with multi-resolution spectrogram,”
    in *International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2020, pp. 6199-6203.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] R. Yamamoto, E. Song and J. Kim, “Parallel WaveGAN: 基于具有多分辨率谱图的生成对抗网络的快速波形生成模型，”
    在 *国际声学、语音与信号处理会议（ICASSP）*，2020年，pp. 6199-6203。'
- en: '[8] J. Liu, C. Li, Y. Ren, F. Chen, P. Liu, and Z. Zhao, “Diffsinger: Diffusion
    acoustic model for singing voice synthesis,” *arXiv* preprint arXiv:2105.02446,
    2021.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Liu, C. Li, Y. Ren, F. Chen, P. Liu, and Z. Zhao, “Diffsinger: 用于歌声合成的扩散声学模型，”
    *arXiv* 预印本 arXiv:2105.02446, 2021。'
- en: '[9] Ho, Jonathan, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic
    models,” in *Conference on Neural Information Processing Systems (NeurIPS)*, 2020.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Ho, Jonathan, A. Jain, and P. Abbeel, “去噪扩散概率模型，” 在 *神经信息处理系统会议（NeurIPS）*，2020年。'
- en: '[10] X. Zhuang, T. Jiang, S. -Y. Chou, B. Wu, P. Hu, and S. Lui, “Litesing:
    Towards fast, lightweight and expressive singing voice synthesis,” in *ICASSP*,
    2021, pp. 7078-7082.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] X. Zhuang, T. Jiang, S. -Y. Chou, B. Wu, P. Hu, and S. Lui, “Litesing:
    迈向快速、轻量和富有表现力的歌声合成，” 在 *ICASSP*，2021年，pp. 7078-7082。'
- en: '[11] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves,
    N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “WaveNet: A generative model for
    raw audio,” *arXiv* preprint arXiv:1609.03499, 2016.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves,
    N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “WaveNet: 原始音频的生成模型，” *arXiv*
    预印本 arXiv:1609.03499, 2016。'
- en: '[12] A. Martin, S. Chintala, and L. Bottou, “Wasserstein generative adversarial
    networks,” *International conference on machine learning (PMLR)*, 2017.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Martin, S. Chintala, 和 L. Bottou，“Wasserstein生成对抗网络，” *国际机器学习会议（PMLR）*，2017年。'
- en: '[13] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu, “Fastspeech
    2: Fast and high-quality end-to-end text to speech,” *arXiv* preprint arXiv:2006.04558,
    2020.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, 和 T. Liu，“Fastspeech
    2: 快速且高质量的端到端文本转语音，” *arXiv* 预印本 arXiv:2006.04558, 2020。'
- en: '[14] Y. Hono, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda, “Sinsy: A deep
    neural network-based singing voice synthesis system,” in *IEEE/ACM Transactions
    on Audio, Speech, and Language Processing*, 2021, pp. 2803-2815.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Hono, K. Hashimoto, K. Oura, Y. Nankaku, 和 K. Tokuda，“Sinsy：基于深度神经网络的唱歌声音合成系统，”
    发表在 *IEEE/ACM音频、语音和语言处理汇刊*，2021年，pp. 2803-2815。'
- en: '[15] H. Kenmochi and H. Ohshita, “Vocaloid-commercial singing synthesizer based
    on sample concatenation,” in *Eighth Annual Conference of the International Speech
    Communication Association*, 2007.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] H. Kenmochi 和 H. Ohshita，“基于样本拼接的Vocaloid商业唱歌合成器，” 发表在 *第八届国际语音通信协会年会*，2007年。'
- en: '[16] K. Saino, H. Zen, Y. Nankaku, A. Lee, and K. Tokuda, “An HMM-based singing
    voice synthesis system,” in *International Conference on Spoken Language Processing*,
    2006.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] K. Saino, H. Zen, Y. Nankaku, A. Lee, 和 K. Tokuda，“基于HMM的唱歌声音合成系统，” 发表在
    *国际口语处理会议*，2006年。'
- en: '[17] M. Nishimura, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda, “Singing
    voice synthesis based on deep neural networks,” in *Conference of the International
    Speech Communication Association (INTERSPEECH)*, 2016, pp. 2478–2482.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] M. Nishimura, K. Hashimoto, K. Oura, Y. Nankaku, 和 K. Tokuda，“基于深度神经网络的唱歌声音合成，”
    发表在 *国际语音通信协会会议（INTERSPEECH）*，2016年，pp. 2478–2482。'
- en: '[18] J. Kim, H. Choi, J. Park, M. Hahn, S. J. Kim, and J. J. Kim, “Korean singing
    voice synthesis based on an LSTM recurrent neural network,” in *INTERSPEECH*,
    2018, pp. 1551–1555.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Kim, H. Choi, J. Park, M. Hahn, S. J. Kim, 和 J. J. Kim，“基于LSTM递归神经网络的韩语唱歌声音合成，”
    发表在 *INTERSPEECH*，2018年，pp. 1551–1555。'
- en: '[19] Y. Hono, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda, “Singing voice
    synthesis based on generative adversarial networks,” in *IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2019, pp. 6955–6959.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Y. Hono, K. Hashimoto, K. Oura, Y. Nankaku, 和 K. Tokuda，“基于生成对抗网络的唱歌声音合成，”
    发表在 *IEEE国际声学、语音和信号处理会议（ICASSP）*，2019年，pp. 6955–6959。'
- en: '[20] K. Nakamura, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda, “Singing
    voice synthesis based on convolutional neural networks,” *arXiv* preprint arXiv:1904.06868,
    2019.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Nakamura, K. Hashimoto, K. Oura, Y. Nankaku, 和 K. Tokuda，“基于卷积神经网络的唱歌声音合成，”
    *arXiv* 预印本 arXiv:1904.06868, 2019年。'
- en: '[21] Y. Gu, X. Yin, Y. Rao, Y. Wan, B. Tang, Y. Zhang, J. Chen, Y. Wang, and
    Z. Ma, “ByteSing: A chinese singing voice synthesis system using duration allocated
    encoder-decoder acoustic models and WaveRNN vocoders,” in *International Symposium
    on Chinese Spoken Language Processing (ISCSLP)*, 2021.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Gu, X. Yin, Y. Rao, Y. Wan, B. Tang, Y. Zhang, J. Chen, Y. Wang, 和
    Z. Ma，“ByteSing：一种基于时长分配编码器-解码器声学模型和WaveRNN声码器的中文唱歌声音合成系统，” 发表在 *国际中文语言处理研讨会（ISCSLP）*，2021年。'
- en: '[22] J. Lee, H. Choi, C. Jeon, J. Koo, and K. Lee, “Adversarially trained end-to-end
    Korean singing voice synthesis system,” in *INTERSPEECH*, 2019, pp. 2588–2592.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Lee, H. Choi, C. Jeon, J. Koo, 和 K. Lee，“对抗训练的端到端韩语唱歌声音合成系统，” 发表在 *INTERSPEECH*，2019年，pp.
    2588–2592。'
- en: '[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Advances in neural
    information processing systems*, 2017.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“Attention is all you need，” 发表在 *神经信息处理系统进展*，2017年。'
- en: '[24] S. Choi, W. Kim, S. Park, S. Yong, and J. Nam, “Children’s Song Dataset
    for Singing Voice Research,” in *International Society for Music Information Retrieval
    Conference (ISMIR)*, 2020.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Choi, W. Kim, S. Park, S. Yong, 和 J. Nam，“儿童歌曲数据集用于唱歌声音研究，” 发表在 *国际音乐信息检索会议（ISMIR）*，2020年。'
- en: '[25] Nagoya Institute of Technology, “NIT-SONG070-F001,” http://hts.sp.nitech.ac.jp/archives/2.3/HTSdemo\_NIT-SONG070-F001.tar.bz2'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 名古屋工业大学，“NIT-SONG070-F001，” http://hts.sp.nitech.ac.jp/archives/2.3/HTSdemo\_NIT-SONG070-F001.tar.bz2'
- en: '[26] J. Wilkins, P. Seetharaman, A. Wahl, and B. Pardo, “VocalSet: A singing
    voice dataset,” in *International Society for Music Information Retrieval Conference
    (ISMIR)*, 2018.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Wilkins, P. Seetharaman, A. Wahl, 和 B. Pardo，“VocalSet：一个唱歌声音数据集，”
    发表在 *国际音乐信息检索会议（ISMIR）*，2018年。'
- en: '[27] Y. Wang, D. Stanton, Y. Zhang, R. S. Ryan, E. Battenberg, J. Shor, Y.
    Xiao, F. Ren, Y. Jia, and R. A. Saurous, “Style Tokens: Unsupervised Style Modeling,
    Control and Transfer in End-to-End Speech Synthesis,” *arXiv* preprint arXiv:1803.09017,
    2018.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Wang, D. Stanton, Y. Zhang, R. S. Ryan, E. Battenberg, J. Shor, Y.
    Xiao, F. Ren, Y. Jia, 和 R. A. Saurous，“风格标记：端到端语音合成中的无监督风格建模、控制与迁移”，*arXiv* 预印本
    arXiv:1803.09017，2018年。'
- en: '[28] J. Bonada, X. Serra, “Synthesis of the singing voice by performance sampling
    and spectral models,” *IEEE Signal Processing Magazine*, vol. 24, pp. 69-79, 2007.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Bonada, X. Serra，“通过性能采样和谱模型合成歌唱声音”，*IEEE信号处理杂志*，第24卷，第69-79页，2007年。'
- en: '[29] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi and T. Kitamura, “Speech
    parameter generation algorithms for HMM-based speech synthesis,” in *IEEE International
    Conference on Acoustics, Speech, and Signal Processing. Proceedings*, 2000, pp.1315-1318.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi 和 T. Kitamura，“基于HMM的语音合成的语音参数生成算法”，见
    *IEEE国际声学、语音与信号处理会议论文集*，2000年，第1315-1318页。'
- en: '[30] Bakhturina, Evelina and Lavrukhin, Vitaly and Ginsburg, Boris and Zhang,
    Yang, ”Hi-Fi Multi-Speaker English TTS Dataset,” in *arXiv preprint arXiv:2104.01497*,
    2021.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Bakhturina, Evelina 和 Lavrukhin, Vitaly 和 Ginsburg, Boris 和 Zhang, Yang，“高保真多扬声器英语TTS数据集”，见
    *arXiv 预印本 arXiv:2104.01497*，2021年。'
- en: '[31] Keith Ito and Linda Johnson, ”The LJ Speech Dataset,” https://keithito.com/LJ-Speech-Dataset/,
    2017'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Keith Ito 和 Linda Johnson，“LJ语音数据集”，https://keithito.com/LJ-Speech-Dataset/，2017年。'
