- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:34:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:34:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2402.17695] Geometric Deep Learning for Computer-Aided Design: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2402.17695] 计算机辅助设计中的几何深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.17695](https://ar5iv.labs.arxiv.org/html/2402.17695)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.17695](https://ar5iv.labs.arxiv.org/html/2402.17695)
- en: 'Geometric Deep Learning for Computer-Aided Design: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机辅助设计中的几何深度学习：综述
- en: Negar Heidari and Alexandros Iosifidis Department of Electrical and Computer
    Engineering, Aarhus University, Denmark
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Negar Heidari 和 Alexandros Iosifidis，丹麦奥胡斯大学电气与计算机工程系
- en: 'Emails: {negar.heidari, ai}@ece.au.dk'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：{negar.heidari, ai}@ece.au.dk
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Geometric Deep Learning techniques have become a transformative force in the
    field of Computer-Aided Design (CAD), and have the potential to revolutionize
    how designers and engineers approach and enhance the design process. By harnessing
    the power of machine learning-based methods, CAD designers can optimize their
    workflows, save time and effort while making better informed decisions, and create
    designs that are both innovative and practical. The ability to process the CAD
    designs represented by geometric data and to analyze their encoded features enables
    the identification of similarities among diverse CAD models, the proposition of
    alternative designs and enhancements, and even the generation of novel design
    alternatives. This survey offers a comprehensive overview of learning-based methods
    in computer-aided design across various categories, including similarity analysis
    and retrieval, 2D and 3D CAD model synthesis, and CAD generation from point clouds.
    Additionally, it provides a complete list of benchmark datasets and their characteristics,
    along with open-source codes that have propelled research in this domain. The
    final discussion delves into the challenges prevalent in this field, followed
    by potential future research directions in this rapidly evolving field.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 几何深度学习技术已成为计算机辅助设计（CAD）领域的变革力量，有潜力彻底改变设计师和工程师处理和提升设计过程的方式。通过利用基于机器学习的方法，CAD设计师可以优化工作流程，节省时间和精力，同时做出更明智的决策，并创造出既创新又实用的设计。处理由几何数据表示的CAD设计并分析其编码特征的能力使得能够识别各种CAD模型之间的相似性，提出替代设计和改进方案，甚至生成新的设计备选方案。本综述全面概述了计算机辅助设计中基于学习的方法，涵盖了各种类别，包括相似性分析与检索、2D和3D
    CAD模型合成，以及从点云生成CAD。除此之外，还提供了基准数据集及其特征的完整列表，并附上了推动该领域研究的开源代码。最后的讨论深入探讨了该领域的挑战，并提出了未来在这一快速发展的领域中的潜在研究方向。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: geometric deep learning, machine learning, graph neural networks, computer aided
    design, automated CAD design, 2D-3D shape modeling
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 几何深度学习，机器学习，图神经网络，计算机辅助设计，自动化CAD设计，2D-3D形状建模
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Deep Learning has revolutionized data processing across different domains spanning
    from grid-structured data in Euclidean spaces, such as image, video, text, to
    graph-structured data with complex relationships. Geometric Deep Learning (GDL)
    encompasses a wide array of neural network architectures ranging from Convolutional
    Neural Networks (CNNs), Graph Neural Networks (GNNs), Recurrent Neural Networks
    (RNNs), to Transformer Networks, all of which encode a geometric understanding
    of the data such as symmetry and invariance as an inductive bias in their learning
    process [[1](#bib.bib1)]. Over the past decade, GDL methods, including CNNs, GNNs,
    RNNs, and Transformer Networks, have made remarkable strides in diverse tasks
    coming from different applications, including computer vision, natural language
    processing, computer graphics, and bioinformatics. However, the application of
    GDL methods on complex parametric Computer-Aided Design (CAD) data is rarely studied.
    Boundary Representation (B-Rep), which is a fundamental data format for CAD models
    encoding a high level of parametric details in CAD models, can be used to learn
    the intricate geometric features of such models. Although GDL methods have seen
    considerable success in analyzing 3D shapes in mesh [[2](#bib.bib2), [3](#bib.bib3)],
    voxel [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)], and point cloud [[7](#bib.bib7),
    [8](#bib.bib8)] data formats, extracting feature representations directly from
    B-Rep data poses challenges. Converting B-Rep data to conventional formats like
    triangle meshes is not only computationally expensive but also leads to information
    loss [[9](#bib.bib9)]. Hence, learning feature representations directly from B-Rep
    data becomes imperative, ensuring an efficient capture of the most representative
    geometric features in CAD models for subsequent analyses without the burden of
    extensive computation and memory usage.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经彻底改变了不同领域的数据处理，这些领域涵盖了从欧几里得空间中的网格结构数据（如图像、视频、文本）到具有复杂关系的图形结构数据。几何深度学习（GDL）包括了广泛的神经网络架构，从卷积神经网络（CNNs）、图神经网络（GNNs）、递归神经网络（RNNs）到变换器网络，这些网络在学习过程中将对数据的几何理解（如对称性和不变性）作为归纳偏差进行编码[[1](#bib.bib1)]。在过去的十年中，GDL方法，包括CNNs、GNNs、RNNs和变换器网络，在来自计算机视觉、自然语言处理、计算机图形学和生物信息学的各种任务中取得了显著进展。然而，GDL方法在复杂的参数化计算机辅助设计（CAD）数据上的应用仍然很少被研究。边界表示（B-Rep），作为CAD模型的一种基本数据格式，编码了CAD模型中的高度参数化细节，可以用于学习这些模型的复杂几何特征。尽管GDL方法在分析网格[[2](#bib.bib2)、[3](#bib.bib3)]、体素[[4](#bib.bib4)、[5](#bib.bib5)、[6](#bib.bib6)]和点云[[7](#bib.bib7)、[8](#bib.bib8)]数据格式中的3D形状方面取得了相当大的成功，但直接从B-Rep数据中提取特征表示仍面临挑战。将B-Rep数据转换为传统格式如三角网格不仅计算开销大，而且还会导致信息丢失[[9](#bib.bib9)]。因此，直接从B-Rep数据中学习特征表示变得至关重要，以确保高效捕捉CAD模型中最具代表性的几何特征，以便后续分析而不增加大量计算和内存使用的负担。
- en: Recently, there has been a great research interest in leveraging GDL methods
    for learning the structure of CAD models and for facilitating the design process
    in different aspects. While conventional machine learning and deep learning approaches
    have been explored for CAD classification and clustering tasks [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)], these methods predominantly focus on learning
    feature representations based on physical features like volume, bounding box,
    area, density, mass, principal axes, or directly work on other data formats for
    3D data, such as point cloud, without considering B-Rep. Accordingly, they often
    fall short in capturing the concise geometric properties embedded in CAD models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，利用GDL方法学习CAD模型结构和促进设计过程的研究兴趣显著增加。虽然传统的机器学习和深度学习方法已被探索用于CAD分类和聚类任务[[10](#bib.bib10)、[11](#bib.bib11)、[12](#bib.bib12)]，这些方法主要集中在基于体积、边界框、面积、密度、质量、主轴等物理特征学习特征表示，或直接在其他3D数据格式（如点云）上进行工作，而忽略了B-Rep。因此，它们通常难以捕捉CAD模型中嵌入的简洁几何属性。
- en: As GDL methods continue to evolve, they are expected to play a pivotal role
    in shaping the future of CAD design across industries. In recent years, more advanced
    GDL methods such as GNNs and (Graph) Transformer Networks have shown great potential
    in learning the complex geometric features embedded in CAD models, particularly
    from B-Rep data, irrespective of their physical features [[9](#bib.bib9), [13](#bib.bib13),
    [14](#bib.bib14), [10](#bib.bib10), [15](#bib.bib15), [16](#bib.bib16)]. These
    learned feature representations serve various purposes, such as reconstructing
    CAD models [[17](#bib.bib17), [15](#bib.bib15), [16](#bib.bib16)], determining
    joints between CAD solids [[18](#bib.bib18), [19](#bib.bib19)], and autocompleting
    unfinished CAD models [[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)].
    The overarching objective of these learning-based approaches is to elevate the
    level of automation in the CAD design process. By alleviating the need for experts
    to perform repetitive and time-consuming tasks, such as sketching and drafting,
    these methods have the potential to empower designers to focus on the more creative
    facets of the design process. By analyzing historical design data, extracting
    geometric features, and discerning valuable insights like similarities between
    CAD models, these methods can facilitate the reuse of CAD models in new products.
    This not only saves time and resources by preventing redundant designs but also
    aids designers in customization by generating design alternatives based on specific
    parameters and objectives. Furthermore, there is a growing demand for CAD tools
    that can reverse engineer models and generate diverse design options based on
    concise parameters derived from preliminary concept sketches. GDL-based methodologies
    have made some progress in meeting this demand [[24](#bib.bib24), [22](#bib.bib22)].
    However, the introduction of machine learning-based methods in this area is not
    without challenges. A major challenge is the scarcity of annotated CAD datasets
    in B-Rep format. Unlike other data formats such as images, videos, or text, collecting
    CAD models is intricate and time-consuming, typically requiring the expertise
    of skilled engineers. Moreover, these datasets often remain proprietary and inaccessible
    to the public. On the other hand, annotating CAD datasets, especially for mechanical
    objects, demands substantial domain knowledge. Given that training deep learning
    methods without large-scale annotated datasets is impractical, recent research
    efforts have also made valuable contributions by providing benchmark CAD datasets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 GDL 方法的不断发展，它们预计将在塑造未来的 CAD 设计中发挥关键作用。近年来，更先进的 GDL 方法如 GNNs 和 (Graph) Transformer
    Networks 展现了在学习嵌入 CAD 模型中的复杂几何特征方面的巨大潜力，特别是从 B-Rep 数据中获取这些特征，无论其物理特征如何 [[9](#bib.bib9),
    [13](#bib.bib13), [14](#bib.bib14), [10](#bib.bib10), [15](#bib.bib15), [16](#bib.bib16)]。这些学习到的特征表示用于各种目的，例如重建
    CAD 模型 [[17](#bib.bib17), [15](#bib.bib15), [16](#bib.bib16)]、确定 CAD 实体之间的连接 [[18](#bib.bib18),
    [19](#bib.bib19)]，以及自动完成未完成的 CAD 模型 [[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23)]。这些基于学习的方法的总体目标是提高 CAD 设计过程的自动化水平。通过减轻专家进行重复性和耗时任务（如草图绘制和制图）的需求，这些方法有可能使设计师专注于设计过程中的更具创造性的方面。通过分析历史设计数据、提取几何特征和辨识类似
    CAD 模型的有价值的见解，这些方法可以促进 CAD 模型在新产品中的重复使用。这不仅通过防止冗余设计节省了时间和资源，还通过根据特定参数和目标生成设计替代方案，帮助设计师进行定制。此外，对能够逆向工程模型并根据初步概念草图中得出的简洁参数生成多样化设计选项的
    CAD 工具的需求不断增长。基于 GDL 的方法在满足这一需求方面取得了一些进展 [[24](#bib.bib24), [22](#bib.bib22)]。然而，在这一领域引入机器学习方法并非没有挑战。一个主要挑战是
    B-Rep 格式中标注 CAD 数据集的稀缺。与图像、视频或文本等其他数据格式不同，收集 CAD 模型复杂且耗时，通常需要熟练工程师的专业知识。此外，这些数据集通常是专有的，公众无法访问。另一方面，标注
    CAD 数据集，尤其是机械物体，要求具有 substantial 领域知识。鉴于没有大规模标注数据集的情况下训练深度学习方法是不切实际的，最近的研究工作也通过提供基准
    CAD 数据集做出了宝贵的贡献。
- en: 'As this field is still emerging, there is plenty of room for future research
    and development, and several challenges need to be explored and addressed. Our
    survey, to the best of our knowledge, stands as the first comprehensive review
    in this rapidly evolving domain, which endeavors to provide a thorough exploration
    of recent advancements, challenges, and contributions in this area. Figure [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey") shows the structure of the survey and highlights the most representative
    methods in the field reviewed in more detail. As we embark on this survey, our
    goal is to serve as a guiding resource for researchers and practitioners eager
    to navigate the dynamic intersection of GDL and CAD. The survey is tailored to
    provide:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '由于这一领域仍在发展中，未来的研究和发展空间巨大，许多挑战需要探索和解决。根据我们所知，我们的调查是该快速发展领域的首个全面回顾，旨在提供对近期进展、挑战和贡献的深入探讨。图
    [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Geometric Deep Learning for Computer-Aided
    Design: A Survey") 展示了调查的结构，并突出了在该领域中详细审查的最具代表性的方法。在我们展开这项调查时，我们的目标是为希望导航GDL和CAD动态交集的研究人员和从业者提供指导性资源。该调查旨在提供：'
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A comprehensive review of the state-of-the-art GDL methods employed in the analysis
    of CAD data. This encompasses diverse categories, including similarity analysis,
    classification, and retrieval, as well as segmentation, along with the synthesis
    of 2D and 3D CAD models. Furthermore, the survey explores techniques for generating
    CAD models from alternative data representations, such as point clouds.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对用于CAD数据分析的最先进GDL方法的全面回顾。这涵盖了各种类别，包括相似性分析、分类和检索，以及分割，还包括2D和3D CAD模型的合成。此外，调查还探讨了从点云等替代数据表示生成CAD模型的技术。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A detailed overview of benchmark CAD datasets crucial for the advancement of
    research in this field, accompanied by open-source codes that have been instrumental
    in pushing the boundaries of achievable outcomes.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于这一领域研究进展至关重要的基准CAD数据集的详细概述，附带了在推动可实现结果的边界方面起到了关键作用的开源代码。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An in-depth discussion about the challenges that persist in this domain followed
    by potential future research directions by assessing the limitations of existing
    methodologies.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深入讨论了该领域中仍然存在的挑战，并通过评估现有方法的局限性，提出了未来的研究方向。
- en: '{forest}'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: for tree= font=, grow’=0, child anchor=west, parent anchor=south, anchor=west,
    calign=first, edge path= [draw, \forestoptionedge] (!u.south west) +(15pt,0) —-
    (.child anchor) \forestoptionedge label; , before typesetting nodes= if n=1 insert
    before=[,phantom] , fit=band, before computing xy=l=25pt, s sep=15pt, draw, inner
    sep=5pt, rounded corners, [CAD analysis with GDL [Representation Learning on CAD
    [CAD Classification and Retrieval [[9](#bib.bib9), [13](#bib.bib13)]] [CAD Segmentation
    [[9](#bib.bib9), [14](#bib.bib14), [25](#bib.bib25)]] [CAD Assembly [[19](#bib.bib19),
    [18](#bib.bib18)]] ] [CAD Construction with Generative Deep Learning [Engineering
    2D Sketch Generation [[20](#bib.bib20), [21](#bib.bib21), [26](#bib.bib26), [27](#bib.bib27),
    [23](#bib.bib23)]] [3D CAD Generation [3D CAD Generation from Sketch [[24](#bib.bib24),
    [22](#bib.bib22), [28](#bib.bib28)]] [3D CAD Command Generation [[29](#bib.bib29),
    [30](#bib.bib30), [17](#bib.bib17), [31](#bib.bib31)]] [3D CAD Generation with
    Direct B-rep synthesis [[15](#bib.bib15), [16](#bib.bib16)]] [3D CAD Generation
    from Point Cloud [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]] ] ] ]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: for tree= font=, grow’=0, child anchor=west, parent anchor=south, anchor=west,
    calign=first, edge path= [draw, \forestoptionedge] (!u.south west) +(15pt,0) —-
    (.child anchor) \forestoptionedge label; , before typesetting nodes= if n=1 insert
    before=[,phantom] , fit=band, before computing xy=l=25pt, s sep=15pt, draw, inner
    sep=5pt, rounded corners, [CAD analysis with GDL [Representation Learning on CAD
    [CAD Classification and Retrieval [[9](#bib.bib9), [13](#bib.bib13)]] [CAD Segmentation
    [[9](#bib.bib9), [14](#bib.bib14), [25](#bib.bib25)]] [CAD Assembly [[19](#bib.bib19),
    [18](#bib.bib18)]] ] [CAD Construction with Generative Deep Learning [Engineering
    2D Sketch Generation [[20](#bib.bib20), [21](#bib.bib21), [26](#bib.bib26), [27](#bib.bib27),
    [23](#bib.bib23)]] [3D CAD Generation [3D CAD Generation from Sketch [[24](#bib.bib24),
    [22](#bib.bib22), [28](#bib.bib28)]] [3D CAD Command Generation [[29](#bib.bib29),
    [30](#bib.bib30), [17](#bib.bib17), [31](#bib.bib31)]] [3D CAD Generation with
    Direct B-rep synthesis [[15](#bib.bib15), [16](#bib.bib16)]] [3D CAD Generation
    from Point Cloud [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]] ] ] ]
- en: 'Figure 1: Taxonomy and the structure of the most representative methods reviewed
    in this survey.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：调查中审查的最具代表性方法的分类及结构。
- en: II Background
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: II-A Computer Aided Design (CAD)
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 计算机辅助设计 (CAD)
- en: Computer-Aided Design (CAD) is a manufacturing technology that has revolutionized
    the way engineers, architects, designers, and other professionals create and visualize
    designs [[35](#bib.bib35)]. CAD process involves the use of specialized software
    to design, modify, analyze, and optimize 2D drawings and 3D models of physical
    objects or systems digitally before constructing them. Engineering drawing entails
    the use of graphical symbols such as points, lines, curves, planes and shapes,
    and it essentially gives detailed description about any component in a graphical
    form. There are several CAD software options which are widely used in industry
    for different purposes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机辅助设计（CAD）是一种制造技术，它彻底改变了工程师、建筑师、设计师和其他专业人员创建和可视化设计的方式 [[35](#bib.bib35)]。CAD过程涉及使用专门的软件在构建之前数字化地设计、修改、分析和优化2D图纸和3D模型。工程图涉及使用图形符号，如点、线、曲线、平面和形状，它从图形形式中详细描述了任何组件。工业中广泛使用的CAD软件选项有很多，适用于不同的目的。
- en: II-A1 CAD Tools and Python APIs
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 CAD工具和Python API
- en: To name some of the most popular software options, AutoCAD [[36](#bib.bib36)]
    and Fusion 360 [[37](#bib.bib37)] (a cloud-based software) offer 2D/3D design
    tools for a variety of industries. SolidWorks [[38](#bib.bib38)] and CATIA [[39](#bib.bib39)]
    provide parametric modeling and simulation for the mechanical, aerospace, and
    automotive industries. OnShape [[40](#bib.bib40)] (a cloud-based software) and
    Creo [[41](#bib.bib41)] offer 3D parametric design and simulation. TinkerCAD [[42](#bib.bib42)]
    is a free-of-charge and web-based CAD design tool which is mostly used for beginners
    and educational purposes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 举一些最受欢迎的软件选项，AutoCAD [[36](#bib.bib36)] 和Fusion 360 [[37](#bib.bib37)]（基于云的软件）为各种行业提供2D/3D设计工具。SolidWorks
    [[38](#bib.bib38)] 和CATIA [[39](#bib.bib39)] 提供机械、航空航天和汽车行业的参数化建模和仿真。OnShape [[40](#bib.bib40)]（基于云的软件）和Creo
    [[41](#bib.bib41)] 提供3D参数化设计和仿真。TinkerCAD [[42](#bib.bib42)] 是一个免费的基于Web的CAD设计工具，主要用于初学者和教育目的。
- en: For CAD parsing and development, several Python APIs are available, providing
    tools and libraries to work with CAD data, create 3D models, visualize CAD models,
    and to perform various related operations. To name some of the most popular Python
    APIs for CAD, PythonOCC [[43](#bib.bib43)] offers an open-source 3D modeling and
    CAD library for Python. It is based on OpenCASCADE Technology (OCCT) [[44](#bib.bib44)]
    which is a powerful geometry kernel and modeling framework used for CAD. PythonOCC
    provides Python bindings to the OpenCASCADE C++ library, allowing developers to
    use Python for creating, manipulating, visualizing and analyzing 3D geometry and
    CAD models. OCCWL [[45](#bib.bib45)] has been recently released as a simple, lightweight
    Pythonic wrapper around PythonOCC. CADQuery [[46](#bib.bib46)], a Python library
    for scripting 3D CAD models, is also built on top of the OpenCASCADE geometry
    kernel.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CAD解析和开发，有几个Python API可用，提供了处理CAD数据、创建3D模型、可视化CAD模型以及执行各种相关操作的工具和库。举几个最受欢迎的Python
    API的例子，PythonOCC [[43](#bib.bib43)] 提供了一个开源的3D建模和CAD库，基于OpenCASCADE技术（OCCT）[[44](#bib.bib44)]，这是一个用于CAD的强大几何内核和建模框架。PythonOCC为OpenCASCADE
    C++库提供了Python绑定，允许开发人员使用Python来创建、操作、可视化和分析3D几何和CAD模型。OCCWL [[45](#bib.bib45)]
    最近作为一个简单、轻量的Python化包装器围绕PythonOCC发布。CADQuery [[46](#bib.bib46)] 是一个用于脚本编写3D CAD模型的Python库，也建立在OpenCASCADE几何内核之上。
- en: Another powerful 3D geometric modeling kernel is Parasolid [[47](#bib.bib47)],
    which provides a set of functions and methods for creating, manipulating, and
    visualizing and analyzing 3D solid models. However, unlike OpenCASCADE, Parasolid
    is not open-source and software developers need to license it to integrate its
    capabilities into CAD applications.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个强大的3D几何建模内核是Parasolid [[47](#bib.bib47)]，它提供了一套用于创建、操作、可视化和分析3D实体模型的功能和方法。然而，与OpenCASCADE不同，Parasolid不是开源的，软件开发人员需要授权才能将其功能集成到CAD应用程序中。
- en: II-A2 Terminology in CAD Design
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 CAD设计中的术语
- en: 'Boundary Representation (B-Rep), serves as a fundamental modeling technique
    utilized in CAD to represent intricate 3D models along with their geometrical
    attributes. It facilitates accurate and consistent design, modification, manipulation,
    analysis and representation of 3D entities by characterizing surfaces, curves,
    points, and topological relationships between them in 3D space. In Table [I](#S2.T1
    "TABLE I ‣ II-A2 Terminology in CAD Design ‣ II-A Computer Aided Design (CAD)
    ‣ II Background ‣ Geometric Deep Learning for Computer-Aided Design: A Survey"),
    we introduce the basic terminology of CAD design which is essential for understanding
    the concepts and methodologies in this field. However, depending on the industries
    and different design software applications that are used for design, there might
    exist more specialized terms and concepts that are not covered here. For the sake
    of maintaining consistency throughout this paper, we will employ this terminology
    for all methodologies and datasets discussed in the subsequent sections.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '边界表示（B-Rep）是CAD中用于表示复杂三维模型及其几何属性的基本建模技术。它通过表面、曲线、点及其在三维空间中的拓扑关系的特征，促进了三维实体的准确和一致的设计、修改、操作、分析和表示。在表
    [I](#S2.T1 "TABLE I ‣ II-A2 Terminology in CAD Design ‣ II-A Computer Aided Design
    (CAD) ‣ II Background ‣ Geometric Deep Learning for Computer-Aided Design: A Survey")
    中，我们介绍了CAD设计的基本术语，这对理解该领域的概念和方法至关重要。然而，根据所使用的行业和不同的设计软件应用，可能会存在更多专业术语和概念，这些术语和概念在这里没有涵盖。为了保持本文的一致性，我们将在后续章节讨论的所有方法和数据集上使用这些术语。
    |'
- en: '| Term | Description |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 术语 | 描述 |'
- en: '| --- | --- |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Model | 2D or 3D representation of a real-world object or system created
    within a CAD software. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 在CAD软件中创建的现实世界对象或系统的二维或三维表示。 |'
- en: '| B-Rep | Boundary Representation, a data format for geometrically describing
    objects by representing their topological components such as surfaces, edges,
    vertices, and the relationship between them. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| B-Rep | 边界表示，一种数据格式，用于通过表示对象的拓扑组件（如表面、边缘、顶点及其之间的关系）来几何描述对象。 |'
- en: '| Sketch | 2D drawing of an object, served as the basis for creating 3D models,
    made of lines, curves, and other basic geometric shapes. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 草图 | 对象的二维绘图，作为创建三维模型的基础，由线条、曲线和其他基本几何形状构成。 |'
- en: '| Extrude | A CAD operation for expanding a 2D drawing along a specific dimension
    to create a 3D model. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 拉伸 | 一种CAD操作，通过沿特定维度扩展二维绘图来创建三维模型。 |'
- en: '| Primitive | A basic 2D/3D geometric shape which serves as backbone for more
    complex designs. For 2D sketches, primitives are points, lines, arcs, circles,
    ellipses and polygons. For 3D shapes primitives are cubes, spheres, cylinders,
    cones. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 原语 | 基本的二维/三维几何形状，作为更复杂设计的基础。对于二维草图，原语包括点、线、弧线、圆、椭圆和多边形。对于三维形状，原语包括立方体、球体、圆柱体、圆锥体。
    |'
- en: '| Constraint | Geometric relationship, such as coincident, tangent, perpendicular,
    between primitives in a 2D/3D design. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 约束 | 在二维/三维设计中，原语之间的几何关系，如共点、切线、垂直。 |'
- en: '| Point | Basic geometric entity representing a specific location in space
    with 3D coordinates X, Y, Z. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 点 | 基本几何实体，表示空间中的特定位置，具有三维坐标 X、Y、Z。 |'
- en: '| Line | A straight path between two points in a 3D space. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 线 | 在三维空间中连接两点的直线路径。 |'
- en: '| Circle | A closed curve defined by its center and radius. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 圆 | 由其中心和半径定义的封闭曲线。 |'
- en: '| Arc | A curved line, as a part of a circle or ellipse, defined by a center,
    start and end points. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 弧线 | 一个弯曲的线段，作为圆或椭圆的一部分，通过中心、起点和终点定义。 |'
- en: '| Loop | A closed sequence of curves forming the boundary of a surface. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 循环 | 形成表面边界的封闭曲线序列。 |'
- en: '| Profile | A closed loop made by a collection of curves joint together. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 轮廓 | 由一组连接的曲线组成的封闭循环。 |'
- en: '| Face | A 2D surface bounded by a profile. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 面 | 由轮廓界定的二维表面。 |'
- en: '| Edge | A piece of a curve bounded by two points, defining the boundaries
    of a face. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 边 | 由两点界定的曲线段，定义面边界。 |'
- en: '| Shell | A collection of connected faces with joint boundaries and points.
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 壳体 | 一组连接的面，具有连接的边界和点。 |'
- en: '| Body/Part | A 3D shape created by extruding a face. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 体/部件 | 通过挤出一个面创建的三维形状。 |'
- en: '| Solid | A 3D shape that is used as a building block to design an object.
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 实体 | 用作设计对象的构建块的三维形状。 |'
- en: '| Component | Building blocks of assemblies containing one or more bodies (parts).
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 包含一个或多个体（零件）的装配构建块。 |'
- en: '| Assembly | A collection of connected bodies (or parts) to create a larger
    object. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 装配体 | 由连接的部件（或零件）组成的集合，用于创建一个更大的对象。 |'
- en: '| Joint | The connection, defined by coordinate and axis, between bodies (parts)
    to make an assembly. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 关节 | 由坐标和轴定义的连接，用于将部件组装在一起。 |'
- en: '| Topology | The structure of points, curves, faces and other geometric components
    in a 3D model. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 拓扑 | 3D模型中点、曲线、面和其他几何组件的结构。 |'
- en: '| Wireframe | Visual representation of 3D B-Rep models, showing the structure
    of the object with only lines and curves. Also known as skeletal representation
    of 3D objects. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 线框 | 3D B-Rep模型的视觉表示，仅用线条和曲线展示对象的结构。也称为3D对象的骨架表示。 |'
- en: '| Rendering | The process of visualizing CAD models to simulate their real-world
    appearance. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 渲染 | 可视化CAD模型以模拟其现实世界外观的过程。 |'
- en: '| Surface Normal | A unit vector perpendicular to a surface at a specific point.
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 表面法线 | 在特定点垂直于表面的单位向量。 |'
- en: 'TABLE I: Terminology in CAD design. A summary of the widely used terms through
    different research works and CAD platforms.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：CAD设计术语。对不同研究工作和CAD平台中广泛使用的术语进行总结。
- en: '| Type | Description | Format |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 描述 | 格式 |'
- en: '| --- | --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| STEP | Original parametric B-Rep format containing explicit description of
    the topology and geometry information of the CAD models. | .txt, .step, .smt |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| STEP | 原始参数化B-Rep格式，包含CAD模型的拓扑和几何信息的明确描述。 | .txt, .step, .smt |'
- en: '| STL | Standard Tessellation Language, a format describing 3D surfaces of
    an object with triangular facets (meshes). | .stl |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| STL | 标准镶嵌语言，描述带有三角面（网格）的对象3D表面的格式。 | .stl |'
- en: '| Obj | A format describing 3D surface geometry using a triangular meshes.
    | .obj |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Obj | 描述使用三角网格的3D表面几何的格式。 | .obj |'
- en: '| Statistics | Statistical infromation of CAD models. | .yml, .txt |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 统计信息 | CAD模型的统计信息。 | .yml, .txt |'
- en: '| Features | A description of the properties of surfaces and curves with references
    to the corresponding vertices and faces of the discrete triangle mesh representation.
    | .yml |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 特性 | 对表面和曲线属性的描述，并参考离散三角网格表示的相应顶点和面。 | .yml |'
- en: '| Image | Visualization of objects produced by rendering process. | .png |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 图像 | 通过渲染过程生成的对象可视化。 | .png |'
- en: '| PCD | Point Cloud Data, a collection of points in the 3D space representing
    the structure of a 3D model. | .ASC, .PTX, .OBJ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| PCD | 点云数据，表示3D模型结构的3D空间中的点集合。 | .ASC, .PTX, .OBJ |'
- en: 'TABLE II: A summary of different file formats and their corresponding description.
    Every CAD dataset provides at least one of these data formats to be processed
    for different purposes.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：不同文件格式及其相应描述的总结。每个CAD数据集至少提供一种这些数据格式，以用于不同的目的。
- en: II-B Geometric Deep Learning (GDL)
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 几何深度学习（GDL）
- en: Geometric Deep Learning (GDL) has arisen as a special and fundamental branch
    of artificial intelligence (AI) that expands deep learning approaches from regular
    Euclidean data to complex geometric structured data [[1](#bib.bib1)]. While traditional
    deep learning methods have led to great advancements in different applications
    by processing regular Euclidean data structures, such as audio, text, images and
    videos, formed by regular 1D, 2D and 3D grids, GDL is tailored for processing
    and extracting intricate spatial and topological features from irregular structured
    data like 3D shapes, meshes, point clouds, and graphs. These irregular data structures
    processed by GDL can be either Euclidean or non-Euclidean, depending on the context
    and the geometric properties they possess. GDL has gained significant research
    attention in the CAD domain for its ability in learning complex geometric features
    and facilitating design process for engineers and designers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 几何深度学习（GDL）作为人工智能（AI）的一个特殊且基础的分支，扩展了深度学习方法，从常规的欧几里得数据到复杂的几何结构数据[[1](#bib.bib1)]。尽管传统的深度学习方法通过处理常规的欧几里得数据结构（如音频、文本、图像和视频）在不同应用领域取得了重大进展，GDL则专门用于处理和提取不规则结构数据中的复杂空间和拓扑特征，如3D形状、网格、点云和图形。这些由GDL处理的不规则数据结构可以是欧几里得的或非欧几里得的，具体取决于上下文和它们所具有的几何属性。GDL在CAD领域获得了显著的研究关注，因为它能够学习复杂的几何特征并促进工程师和设计师的设计过程。
- en: Graph neural networks (GNNs) [[48](#bib.bib48), [49](#bib.bib49)] are one of
    the most popular types of GDL approaches which excel in processing graph-structured
    data. GNNs have made remarkable strides in diverse tasks tasks coming from different
    applications, such as computer vision [[50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52)],
    bio-chemical pattern recognition [[53](#bib.bib53), [54](#bib.bib54)], and financial
    data analysis [[55](#bib.bib55)]. GNNs offer a specialized approach for modeling
    complex CAD geometric structures encoded in B-Rep, as a graph which represents
    the topology and geometry of 3D shapes through nodes and edges. GNN models can
    effectively capture the topological structure, connectivity, and attributes inherent
    in graphs (B-Reps) and allow hierarchical analysis by considering local and global
    contexts, aligning with the complex structure of B-Rep models. GNNs are also able
    to extract and propagate geometric features through the graph, capturing subtle
    aspects of shape and curvatures and leveraging data-driven insights from existing
    B-Rep models, facilitating tasks like solid and/or sketch segmentation [[14](#bib.bib14),
    [9](#bib.bib9), [56](#bib.bib56)], shape reconstruction and generation [[16](#bib.bib16),
    [20](#bib.bib20)], shape analysis and retrieval [[13](#bib.bib13), [9](#bib.bib9)].
    By representing B-Rep as a graph and leveraging GNN capabilities to capture its
    topological structure, connectivity, and geometry, designers and engineers can
    analyze, optimize, and create intricate 3D models with enhanced precision and
    efficiency.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）[[48](#bib.bib48), [49](#bib.bib49)] 是最受欢迎的GDL方法之一，擅长处理图结构数据。GNNs在各种任务中取得了显著进展，这些任务来自不同的应用领域，如计算机视觉
    [[50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52)]、生物化学模式识别 [[53](#bib.bib53),
    [54](#bib.bib54)] 和金融数据分析 [[55](#bib.bib55)]。GNNs 提供了一种专门的方式来建模复杂的CAD几何结构，这些结构在B-Rep中被编码为图形，通过节点和边表示3D形状的拓扑和几何。GNN模型可以有效地捕捉图（B-Reps）中固有的拓扑结构、连通性和属性，并通过考虑局部和全球上下文进行分层分析，这与B-Rep模型的复杂结构相一致。GNNs
    还能够通过图形提取和传播几何特征，捕捉形状和曲率的微妙方面，并利用现有B-Rep模型的数据驱动见解，促进诸如固体和/或草图分割 [[14](#bib.bib14),
    [9](#bib.bib9), [56](#bib.bib56)]、形状重建和生成 [[16](#bib.bib16), [20](#bib.bib20)]、形状分析和检索
    [[13](#bib.bib13), [9](#bib.bib9)] 等任务。通过将B-Rep表示为图，并利用GNN的能力捕捉其拓扑结构、连通性和几何，设计师和工程师可以以更高的精度和效率分析、优化和创建复杂的3D模型。
- en: 'Let us present a more detailed explanation of how B-Rep models are represented
    as graphs. In the context of B-Rep, graph nodes can represent points, lines, and
    faces with associated attributes like geometric coordinates, line lengths, curvature,
    face area and surface normals. Edges in the graph represent connections between
    nodes. For B-Rep, these connections indicate constraints, like adjacency, tangency,
    or coincidence, between primitives like points, lines, faces, etc. GNN methods
    embed each node’s features, creating node representations that capture geometric
    attributes that aggregate information from neighboring nodes. In the B-Rep context,
    this simulates the way geometric properties flow through adjacent points, lines,
    curves, and faces. Through feature aggregation, the method updates each node’s
    representation by combining its own features with information of its neighbors.
    This step captures the influence of neighboring geometry. The core operation in
    GNNs is the graph convolution, which is combined with a nonlinear function applied
    on the fused information from neighboring nodes. In a multi-layer GNN, the graph
    convolution operation captures multi-hop neighboring features allowing for hierarchical
    analysis. For B-Rep, this operation helps to capture the hierarchical relationships
    between primitives. To obtain a global representation of the B-Rep model, GNNs
    can employ global pooling, summarizing the entire graph information into a single
    feature vector. Figure [2](#S2.F2 "Figure 2 ‣ II-B Geometric Deep Learning (GDL)
    ‣ II Background ‣ Geometric Deep Learning for Computer-Aided Design: A Survey"),
    [3](#S3.F3 "Figure 3 ‣ III Datasets ‣ Geometric Deep Learning for Computer-Aided
    Design: A Survey") show an example of a 3D shape and a 2D sketch represented as
    graph, respectively.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们更详细地解释一下 B-Rep 模型如何表示为图。在 B-Rep 的背景下，图的节点可以表示点、线和面，并且附有几何坐标、线段长度、曲率、面面积和表面法线等属性。图中的边表示节点之间的连接。对于
    B-Rep，这些连接表示约束，如点、线、面等原语之间的邻接、切线或重合关系。GNN 方法嵌入每个节点的特征，创建捕捉几何属性的节点表示，这些表示聚合来自相邻节点的信息。在
    B-Rep 背景下，这模拟了几何属性如何通过相邻的点、线、曲线和面流动。通过特征聚合，该方法通过将节点自身的特征与邻居的信息结合来更新每个节点的表示。这一步骤捕捉了邻近几何的影响。GNN
    的核心操作是图卷积，它结合了对邻居节点融合信息应用的非线性函数。在多层 GNN 中，图卷积操作捕捉多跳邻近特征，从而允许进行分层分析。对于 B-Rep，这一操作有助于捕捉原语之间的分层关系。为了获得
    B-Rep 模型的全局表示，GNN 可以采用全局池化，将整个图的信息总结为一个特征向量。图 [2](#S2.F2 "Figure 2 ‣ II-B Geometric
    Deep Learning (GDL) ‣ II Background ‣ Geometric Deep Learning for Computer-Aided
    Design: A Survey")、[3](#S3.F3 "Figure 3 ‣ III Datasets ‣ Geometric Deep Learning
    for Computer-Aided Design: A Survey") 显示了一个 3D 形状和一个 2D 草图被表示为图的示例。'
- en: '![Refer to caption](img/fc3fa1103b6409e5954ffc593e73700a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fc3fa1103b6409e5954ffc593e73700a.png)'
- en: 'Figure 2: An example of a 3D solid represented as a graph, where the solid
    primitives such as curves and surfaces are represented as graph nodes, and their
    adjacency relationship between the solid primitives are represented as graph edges.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：一个 3D 实体被表示为图的示例，其中实体原语如曲线和表面被表示为图节点，它们之间的邻接关系被表示为图边。
- en: The most popular GNN architectures, i.e., Graph Convolutional Networks (GCNs)
    [[57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59)] and Graph AutoEncoders
    (GAEs) [[60](#bib.bib60)], which are used for (semi-)supervised and unsupervised
    learning on graphs, respectively, for different graph analysis tasks such as node/graph
    classification and graph reconstruction/generation. As GNN based methods continue
    to evolve, they are expected to play a pivotal role in shaping the future of CAD
    design across industries. In the following sections, we delve into details of
    the current state-of-the-art methods in this area and the challenges.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的 GNN 架构，即图卷积网络 (GCNs) [[57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59)]
    和图自编码器 (GAEs) [[60](#bib.bib60)]，分别用于图的（半）监督和无监督学习，用于不同的图分析任务，如节点/图分类和图重建/生成。随着
    GNN 基于方法的不断发展，它们预计将在塑造 CAD 设计的未来中发挥关键作用。在接下来的部分中，我们将深入探讨该领域当前的最先进方法和挑战。
- en: III Datasets
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 数据集
- en: Large data collections play a pivotal role in enhancing performance of deep
    learning models when applied in problems coming from different applications. Collecting
    such large datasets in regular formats such as image, video, audio, text, and
    their distribution through different platforms like social media has greatly accelerated
    the progress of deep learning in computer vision and natural language processing.
    GDL offers advantages for tasks like 3D shape analysis, shape reconstruction,
    and building geometric feature descriptors. Nonetheless, creating and annotating
    high-quality 3D geometric data needs significant level of domain knowledge, and
    engineering and design skills. Collecting such datasets is also challenging due
    to various factors such as concerns about proprietary rights and ownership, and
    lack of consistency and compatibility among data from available sources.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 大型数据集合在提升深度学习模型的性能方面发挥着关键作用，特别是当这些模型应用于来自不同领域的问题时。收集这些大型数据集，通常包括图像、视频、音频、文本等格式，并通过社交媒体等不同平台进行分发，极大地加速了深度学习在计算机视觉和自然语言处理领域的进展。GDL（几何深度学习）在诸如3D形状分析、形状重建和构建几何特征描述符等任务中提供了优势。然而，创建和注释高质量的3D几何数据需要相当高的领域知识，以及工程和设计技能。收集这些数据集也具有挑战性，原因包括对专有权和所有权的担忧，以及可用来源之间的数据一致性和兼容性不足。
- en: '![Refer to caption](img/879b8df9041d727a6508e1972be04a7c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/879b8df9041d727a6508e1972be04a7c.png)'
- en: 'Figure 3: An example of a simple 2D sketch represented as a graph, where the
    sketch primitives such as curves (circle, arc, line, etc.) are modeled as graph
    nodes, and the constraints between these primitives are shown as graph edges.
    SketchGraphs [[20](#bib.bib20)] dataset contains such 2D sketches modeled as graphs.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：一个简单的二维草图示例，被表示为图，其中草图原语如曲线（圆形、弧形、直线等）被建模为图节点，这些原语之间的约束则以图边的形式展示。SketchGraphs
    [[20](#bib.bib20)] 数据集包含了这样的二维草图，以图的形式进行建模。
- en: 'Table [III](#S3.T3 "TABLE III ‣ III Datasets ‣ Geometric Deep Learning for
    Computer-Aided Design: A Survey") provides a summary of the existing datasets
    along with their properties. The details about each dataset are provided in the
    dedicated sections for methodologies linked to each dataset. Table [II](#S2.T2
    "TABLE II ‣ II-A2 Terminology in CAD Design ‣ II-A Computer Aided Design (CAD)
    ‣ II Background ‣ Geometric Deep Learning for Computer-Aided Design: A Survey")
    introduces different CAD data formats along with their corresponding description.
    Existing commonly used 3D CAD datasets mostly provide mesh geometry for 3D shape
    segmentation, classification and retrieval [[61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63), [5](#bib.bib5), [64](#bib.bib64)], human body mesh registration
    [[65](#bib.bib65)], 2D/3D shape alignment [[66](#bib.bib66)] and 3D scene categorization,
    semantic segmentation and object detection [[67](#bib.bib67), [68](#bib.bib68)].
    Engineering shape datasets such as ESB [[69](#bib.bib69)], MCB [[70](#bib.bib70)],
    AAD [[71](#bib.bib71)], FeatureNet [[72](#bib.bib72)], and CADNet [[73](#bib.bib73)]
    also provide annotated data with mesh geometry for mechanical shape classification
    and segmentation. The primary limitation of these datasets is their lack of parametric
    and topological features of curves and surfaces commonly referred to as boundary
    representation (B-Rep). These B-Rep features are essential for conducting parametric
    CAD shape analysis. Recently, several geometric B-Rep datasets of different sizes
    and properties were introduced to boost GDL progress on CAD design. FabWave [[74](#bib.bib74)]
    is a collection of $5,373$ 3D shapes annotated with $52$ mechanical part classes,
    including gears and brackets, and Traceparts [[75](#bib.bib75)] is a small collection
    of $600$ CAD models produced by different companies labeled into $6$ classes (100
    CAD models in each class), including screws, nuts, and hinges, which can be used
    for 3D shape classification. MFCAD [[76](#bib.bib76)] is a synthetic 3D segmentation
    dataset containing $15,488$ shapes with annotated faces of 16 classes, which can
    be used for parametric face segmentation in CAD shapes.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [III](#S3.T3 "TABLE III ‣ III Datasets ‣ Geometric Deep Learning for Computer-Aided
    Design: A Survey") 提供了现有数据集及其属性的总结。有关每个数据集的详细信息在与每个数据集相关的方法部分中提供。表 [II](#S2.T2
    "TABLE II ‣ II-A2 Terminology in CAD Design ‣ II-A Computer Aided Design (CAD)
    ‣ II Background ‣ Geometric Deep Learning for Computer-Aided Design: A Survey")
    介绍了不同的 CAD 数据格式及其相应的描述。现有的常用 3D CAD 数据集主要提供 3D 形状分割、分类和检索所需的网格几何 [[61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [5](#bib.bib5), [64](#bib.bib64)]，人体网格配准 [[65](#bib.bib65)]，2D/3D
    形状对齐 [[66](#bib.bib66)] 和 3D 场景分类、语义分割及物体检测 [[67](#bib.bib67), [68](#bib.bib68)]。工程形状数据集如
    ESB [[69](#bib.bib69)]、MCB [[70](#bib.bib70)]、AAD [[71](#bib.bib71)]、FeatureNet
    [[72](#bib.bib72)] 和 CADNet [[73](#bib.bib73)] 也提供了带有网格几何的标注数据，用于机械形状分类和分割。这些数据集的主要限制是缺乏通常称为边界表示（B-Rep）的曲线和表面的参数化及拓扑特征。这些
    B-Rep 特征对于进行参数化 CAD 形状分析至关重要。最近，推出了几种不同规模和属性的几何 B-Rep 数据集，以推动 CAD 设计中的 GDL 进展。FabWave
    [[74](#bib.bib74)] 是一个包含 $5,373$ 个 3D 形状的集合，标注有 $52$ 种机械部件类别，包括齿轮和支架，而 Traceparts
    [[75](#bib.bib75)] 是一个小型集合，包含 $600$ 个由不同公司生产的 CAD 模型，分为 $6$ 个类别（每个类别 $100$ 个 CAD
    模型），包括螺丝、螺母和铰链，可用于 3D 形状分类。MFCAD [[76](#bib.bib76)] 是一个合成的 3D 分割数据集，包含 $15,488$
    个带有 16 类标注面的形状，可用于 CAD 形状中的参数化面分割。'
- en: Advancing across various tasks in GDL, and effective training of deep learning
    models, necessitate large parametric CAD data collections. The existing parametric
    CAD datasets with B-Rep data are limited in size and insufficient to meet these
    demands. To address this shortfall, three big datasets, ABC [[77](#bib.bib77)],
    Fusion 360 Gallery [[30](#bib.bib30)], and SketchGraphs [[20](#bib.bib20)], were
    introduced recently and provide valuable resources for research in this area.
    ABC [[77](#bib.bib77)] is the first large-scale, real-world, and hand designed
    dataset with over $1$ million high-quality 3D CAD models covering a wide range
    of object types. Each CAD model consists of a set of precisely parameterized curves
    and surfaces, offering accurate reference points for sharp geometric feature representation,
    patch segmentation, analytic differential measurements, and the process of shape
    reconstruction. The CAD models in ABC dataset are compiled and collected via an
    openly accessible interface hosted by Onshape [[40](#bib.bib40)], and an open-source
    geometry processing pipeline has been developed to process and prepare the CAD
    models in ABC dataset to be used by deep learning methods.¹¹1https://deep-geometry.github.io/abc-dataset/
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在GDL的各种任务推进以及深度学习模型的有效训练中，需要大量的参数化CAD数据集合。现有的含有B-Rep数据的参数化CAD数据集在规模上有限，无法满足这些需求。为了解决这一不足，最近引入了三个大型数据集：ABC
    [[77](#bib.bib77)]、Fusion 360 Gallery [[30](#bib.bib30)]和SketchGraphs [[20](#bib.bib20)]，它们为该领域的研究提供了宝贵的资源。ABC
    [[77](#bib.bib77)] 是第一个大规模的、真实世界的、手工设计的数据集，拥有超过 $1$ 百万高质量的3D CAD模型，涵盖了广泛的对象类型。每个CAD模型由一组精确参数化的曲线和表面组成，提供了用于锐利几何特征表示、补丁分割、解析微分测量和形状重建过程的准确参考点。ABC数据集中的CAD模型通过Onshape
    [[40](#bib.bib40)] 提供的公开访问接口进行编译和收集，并开发了一个开源几何处理管道，用于处理和准备ABC数据集中的CAD模型，以便深度学习方法使用。
- en: 'Training machine learning models to facilitate CAD construction and synthesis
    can significantly benefit designers by minimizing the time and effort required
    for the design process. An essential necessity for tasks related to CAD (re-)construction
    is to understand how a CAD model is designed and how to interpret the provided
    construction information in the dataset. The construction history information
    in ABC dataset can only be retrieved by querying the Onshape API in a proprietary
    format with limited documentation which makes it challenging to develop CAD (re-)construction
    methods on that dataset. SketchGraphs dataset has been produced to fill this gap
    by providing a collection of $15$ million human-designed 2D sketches of real-world
    CAD models from Onshape API. 2D sketches representing geometric primitives (like
    lines and arcs) and constraints between them (like coincidence and tangency) can
    be seen as the basis of 3D shape parametric construction, and each 2D sketch is
    presented as a geometric constraint graph where nodes denote 2D primitives and
    edges are geometric constraints between nodes ([3](#S3.F3 "Figure 3 ‣ III Datasets
    ‣ Geometric Deep Learning for Computer-Aided Design: A Survey")). The SketchGraphs
    dataset can be used for various applications in automating design process such
    as auto-completing sketch drawing by predicting sequences of sketch construction
    operations or interactively suggesting next steps to designer, and autoconstraint
    application where the method is predicting a set of constraints between geometric
    primitives of sketch. Other potential applications are CAD inference from images
    where the method receives a noisy 2D drawing image and infers its design steps
    to produce the corresponding parametric CAD model, and learning semantic representation
    encoded in sketches. An open-source Python pipeline for data processing and preparation
    for deep learning frameworks also comes along the dataset.²²2https://github.com/PrincetonLIPS/SketchGraphs
    Similar to SketchGraphs, freehand 2D sketch datasets such as [[78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80)] also have been introduced to tackle this challenge
    by providing the sketch construction sequence.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习模型以促进 CAD 构建和合成可以显著减少设计过程所需的时间和精力，从而使设计师受益。与 CAD（重）构建相关的任务的一个基本需求是了解 CAD
    模型的设计方式以及如何解读数据集中提供的构建信息。ABC 数据集中的构建历史信息只能通过查询具有有限文档说明的专有格式 Onshape API 来检索，这使得在该数据集上开发
    CAD（重）构建方法变得具有挑战性。SketchGraphs 数据集旨在填补这一空白，提供了来自 Onshape API 的 $15$ 百万个人设计的真实世界
    CAD 模型的 2D 草图集合。2D 草图代表几何原语（如线条和弧线）以及它们之间的约束（如重合和切线），可以看作是 3D 形状参数化构建的基础，每个 2D
    草图被呈现为几何约束图，其中节点表示 2D 原语，边是节点之间的几何约束 ([3](#S3.F3 "图 3 ‣ III 数据集 ‣ 计算机辅助设计的几何深度学习：综述"))。SketchGraphs
    数据集可以用于自动化设计过程的各种应用，如通过预测草图构建操作序列自动完成草图绘制，或与设计师交互地建议下一步操作，以及自动约束应用，其中方法是预测草图中几何原语之间的一组约束。其他潜在应用包括从图像中推断
    CAD，其中方法接收一个嘈杂的 2D 绘图图像，并推断其设计步骤以生成相应的参数化 CAD 模型，以及学习草图中编码的语义表示。数据集还附带了一个开源 Python
    管道，用于数据处理和深度学习框架的准备。²²2https://github.com/PrincetonLIPS/SketchGraphs 类似于 SketchGraphs，自由手绘
    2D 草图数据集如 [[78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)] 也已被引入，以通过提供草图构建序列来解决这一挑战。
- en: Fusion 360 Gallery [[30](#bib.bib30)] has been introduced recently by Autodesk
    as the first human designed 3D CAD dataset and environment with 3D operation sequences
    for programmatic CAD construction. The fusion 360 Gallery dataset comes along
    an open source Python environment named as Fusion 360 Gym for processing and preparing
    the CAD operations for machine learning methods. This dataset contains both 2D
    and 3D geometric CAD data which are produced by the users of the Autodesk Fusion
    360 CAD software and are collected into the Autodesk Online Gallery. Several datasets
    for different learning goals, such as CAD reconstruction [[30](#bib.bib30)], CAD
    segmentation [[14](#bib.bib14)], and CAD assembly [[18](#bib.bib18)], are created
    based on a total of real-world $20,000$ designs in Fusion 360 Gallery.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Fusion 360 Gallery [[30](#bib.bib30)] 最近由 Autodesk 推出，作为首个以人为设计的 3D CAD 数据集和环境，包含用于程序化
    CAD 构建的 3D 操作序列。Fusion 360 Gallery 数据集配备了一个名为 Fusion 360 Gym 的开源 Python 环境，用于处理和准备
    CAD 操作以用于机器学习方法。该数据集包含用户使用 Autodesk Fusion 360 CAD 软件生成的 2D 和 3D 几何 CAD 数据，并收集到
    Autodesk 在线画廊中。为实现不同学习目标，如 CAD 重建 [[30](#bib.bib30)]、CAD 分割 [[14](#bib.bib14)]
    和 CAD 装配 [[18](#bib.bib18)]，基于 Fusion 360 Gallery 中总计 $20,000$ 个真实世界设计创建了若干数据集。
- en: '| Dataset | #Models | B-Rep | Mesh | Sketch | #Categories | Application |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | #模型 | B-Rep | 网格 | 草图 | #类别 | 应用 |'
- en: '| ShapeNet [[62](#bib.bib62)] | 3M+ | $\times$ | ✓ | $\times$ | 3,135 | Classification
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ShapeNet [[62](#bib.bib62)] | 3M+ | $\times$ | ✓ | $\times$ | 3,135 | 分类
    |'
- en: '| ModelNet [[5](#bib.bib5)] | 12,311 | $\times$ | ✓ | $\times$ | 40 | Classification
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet [[5](#bib.bib5)] | 12,311 | $\times$ | ✓ | $\times$ | 40 | 分类 |'
- en: '| PartNet [[64](#bib.bib64)] | 26,671 | $\times$ | ✓ | $\times$ | 24 | Classification,
    Segmentation |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| PartNet [[64](#bib.bib64)] | 26,671 | $\times$ | ✓ | $\times$ | 24 | 分类，分割
    |'
- en: '| PrincetonSB [[61](#bib.bib61)] | 6,670 | $\times$ | ✓ | $\times$ | 92 | Classification
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| PrincetonSB [[61](#bib.bib61)] | 6,670 | $\times$ | ✓ | $\times$ | 92 | 分类
    |'
- en: '| AAD [[71](#bib.bib71)] | 180 | $\times$ | ✓ | $\times$ | 9 | Classification
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| AAD [[71](#bib.bib71)] | 180 | $\times$ | ✓ | $\times$ | 9 | 分类 |'
- en: '| ESB [[69](#bib.bib69)] | 867 | $\times$ | ✓ | $\times$ | 45 | Classification
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ESB [[69](#bib.bib69)] | 867 | $\times$ | ✓ | $\times$ | 45 | 分类 |'
- en: '| Thingi10k [[63](#bib.bib63)] | 10,000 | $\times$ | ✓ | $\times$ | 2,011 |
    Classification |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Thingi10k [[63](#bib.bib63)] | 10,000 | $\times$ | ✓ | $\times$ | 2,011 |
    分类 |'
- en: '| FeatureNet [[72](#bib.bib72)] | 23,995 | $\times$ | ✓ | $\times$ | 24 | Classification
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| FeatureNet [[72](#bib.bib72)] | 23,995 | $\times$ | ✓ | $\times$ | 24 | 分类
    |'
- en: '| MCB [[70](#bib.bib70)] | 58,696 | $\times$ | ✓ | $\times$ | 68 | Classification
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| MCB [[70](#bib.bib70)] | 58,696 | $\times$ | ✓ | $\times$ | 68 | 分类 |'
- en: '| CADNet [[73](#bib.bib73)] | 3,317 | $\times$ | ✓ | $\times$ | 43 | Classification
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| CADNet [[73](#bib.bib73)] | 3,317 | $\times$ | ✓ | $\times$ | 43 | 分类 |'
- en: '| FABWave [[74](#bib.bib74)] | 5,373 | ✓ | ✓ | $\times$ | 52 | Classification
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| FABWave [[74](#bib.bib74)] | 5,373 | ✓ | ✓ | $\times$ | 52 | 分类 |'
- en: '| Traceparts [[75](#bib.bib75)] | 600 | ✓ | ✓ | $\times$ | 6 | Classification
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Traceparts [[75](#bib.bib75)] | 600 | ✓ | ✓ | $\times$ | 6 | 分类 |'
- en: '| SolidLetters [[9](#bib.bib9)] | 96,000 | ✓ | $\times$ | $\times$ | 26 | Classification
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SolidLetters [[9](#bib.bib9)] | 96,000 | ✓ | $\times$ | $\times$ | 26 | 分类
    |'
- en: '| MFCAD [[76](#bib.bib76)] | 15,488 | ✓ | $\times$ | $\times$ | 16 | Segmentation
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| MFCAD [[76](#bib.bib76)] | 15,488 | ✓ | $\times$ | $\times$ | 16 | 分割 |'
- en: '| MFCAD++ [[81](#bib.bib81)] | 59,655 | ✓ | $\times$ | $\times$ | 25 | Segmentation
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| MFCAD++ [[81](#bib.bib81)] | 59,655 | ✓ | $\times$ | $\times$ | 25 | 分割 |'
- en: '| Fusion 360 Segmentation [[14](#bib.bib14)] | 35,680 | ✓ | ✓ | $\times$ |
    8 | Segmentation |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Fusion 360 Segmentation [[14](#bib.bib14)] | 35,680 | ✓ | ✓ | $\times$ |
    8 | 分割 |'
- en: '| CC3D-Ops | 37,000 | ✓ | ✓ | $\times$ | - | Segmentation |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| CC3D-Ops | 37,000 | ✓ | ✓ | $\times$ | - | 分割 |'
- en: '| Fusion 360 Assembly [[18](#bib.bib18)] | 154,468 | ✓ | ✓ | $\times$ | $-$
    | Joint Prediction |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Fusion 360 Assembly [[18](#bib.bib18)] | 154,468 | ✓ | ✓ | $\times$ | $-$
    | 联合预测 |'
- en: '| AutoMate [[19](#bib.bib19)] | 3M+ | ✓ | ✓ | $\times$ | $-$ | Joint Prediction
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| AutoMate [[19](#bib.bib19)] | 3M+ | ✓ | ✓ | $\times$ | $-$ | 联合预测 |'
- en: '| ABC [[77](#bib.bib77)] | 1M+ | ✓ | ✓ | $\times$ | $-$ | Reconstruction |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ABC [[77](#bib.bib77)] | 1M+ | ✓ | ✓ | $\times$ | $-$ | 重建 |'
- en: '| Fusion 360 Reconstruction [[30](#bib.bib30)] | 8,625 | ✓ | ✓ | $\times$ |
    $-$ | Reconstruction |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Fusion 360 Reconstruction [[30](#bib.bib30)] | 8,625 | ✓ | ✓ | $\times$ |
    $-$ | 重建 |'
- en: '| SketchGraphs [[20](#bib.bib20)] | 15M+ | $\times$ | $\times$ | ✓ | $-$ |
    Reconstruction |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| SketchGraphs [[20](#bib.bib20)] | 15M+ | $\times$ | $\times$ | ✓ | $-$ |
    重建 |'
- en: '| CAD as Language [[27](#bib.bib27)] | 4.7M+ | $\times$ | $\times$ | ✓ | $-$
    | Reconstruction |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| CAD as Language [[27](#bib.bib27)] | 4.7M+ | $\times$ | $\times$ | ✓ | $-$
    | 重建 |'
- en: '| Sketch2CAD [[24](#bib.bib24)] | 50,000 | $\times$ | $\times$ | ✓ | $-$ |
    Reconstruction |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Sketch2CAD [[24](#bib.bib24)] | 50,000 | $\times$ | $\times$ | ✓ | $-$ |
    重建 |'
- en: '| CAD2Sketch [[28](#bib.bib28)] | 6000 | $\times$ | $\times$ | ✓ | $-$ | Reconstruction
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| CAD2Sketch [[28](#bib.bib28)] | 6000 | $\times$ | $\times$ | ✓ | $-$ | 重建
    |'
- en: '| DeepCAD [[17](#bib.bib17)] | 178,238 | ✓ | $\times$ | $\times$ | $-$ | Reconstruction
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| DeepCAD [[17](#bib.bib17)] | 178,238 | ✓ | $\times$ | $\times$ | $-$ | 重建
    |'
- en: '| PVar [[15](#bib.bib15)] | 120,000 | ✓ | $\times$ | $\times$ | 60 | Reconstruction,
    Classification |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| PVar [[15](#bib.bib15)] | 120,000 | ✓ | $\times$ | $\times$ | 60 | 重建、分类
    |'
- en: '| CADParser [[16](#bib.bib16)] | 40,000 | ✓ | $\times$ | $\times$ | $-$ | Reconstruction
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| CADParser [[16](#bib.bib16)] | 40,000 | ✓ | $\times$ | $\times$ | $-$ | 重建
    |'
- en: '| Free2CAD [[22](#bib.bib22)] | 82,000 | ✓ | $\times$ | ✓ | $-$ | Reconstruction
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Free2CAD [[22](#bib.bib22)] | 82,000 | ✓ | $\times$ | ✓ | $-$ | 重建 |'
- en: 'TABLE III: Overview of the existing common object and mechanical CAD datasets
    with their properties. For each dataset, the number of CAD models, representation
    formats such as B-Reps, Mesh, Sketch, and different tasks they are annotated for,
    such as Segmentation, Classification and CAD Reconstruction, are reported. The
    first 4 rows of the table show the common object datasets annotated for 3D shape
    classification. The remaining rows list mechanical object datasets, created for
    CAD analysis.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：现有的常见对象和机械 CAD 数据集及其属性概述。对于每个数据集，报告了 CAD 模型数量、表示格式如 B-Reps、Mesh、Sketch
    以及它们被注释的不同任务，如分割、分类和 CAD 重建。表格的前 4 行显示了用于 3D 形状分类的常见对象数据集。其余行列出了为 CAD 分析创建的机械对象数据集。
- en: IV CAD Representation Learning
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV CAD 表示学习
- en: Studying extensive data repositories and uncovering hidden features in data
    for various tasks such as similarity analysis and shape retrieval has been a vibrant
    field of research in machine learning and artificial intelligence. The significance
    of this concept extends to CAD data as well. Machine learning-based similarity
    analysis of CAD models can effectively facilitate the design process for designers
    by categorizing designs, and retrieving similar CAD models as design alternatives.
    It has been shown that approximately 40% of new CAD designs could be constructed
    based on existing designs in the CAD repository, and at least 75% of design processes
    leverage existing knowledge for designing a new CAD model [[82](#bib.bib82)].
    Learning from CAD data and extracting features from extensive collections of geometric
    CAD shapes is important for CAD retrieval and similarity analysis, and it involves
    employing various machine learning and deep learning methods for extracting geometric
    attributes encoded in B-Rep data for assessing similarity between pairs of CAD
    models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 研究广泛的数据仓库并揭示数据中的隐藏特征，以便进行相似性分析和形状检索，已经成为机器学习和人工智能领域的一个活跃研究方向。这一概念在 CAD 数据中的重要性也不容忽视。基于机器学习的
    CAD 模型相似性分析可以通过对设计进行分类和检索类似的 CAD 模型作为设计替代方案，来有效促进设计过程。研究表明，大约 40% 的新 CAD 设计可以基于
    CAD 仓库中的现有设计构建，至少 75% 的设计过程利用现有知识来设计新的 CAD 模型 [[82](#bib.bib82)]。从 CAD 数据中学习并从大量几何
    CAD 形状中提取特征对于 CAD 检索和相似性分析至关重要，这涉及使用各种机器学习和深度学习方法，从 B-Rep 数据中提取几何属性，以评估 CAD 模型对之间的相似性。
- en: The first step in this process is to select a subset of geometrical, topological,
    functional and other properties of the CAD model, based on specific analysis goals,
    to be represented in a suitable format, such as numerical vectors, matrices, tensors,
    or graphs, for the deep learning methods to process. Nevertheless, representing
    B-Rep data is challenging due to the coexistence of continuous non-Euclidean geometric
    features and discrete topological properties, making it difficult to fit into
    regular structured formats such as tensors or fixed-length encodings. A key contribution
    of each state-of-the-art method in this topic is the introduction of a method
    for encoding or tokenizing B-Rep data into a format suitable for the adopted deep
    learning architecture, tailored to the particular application they are addressing.
    The deep learning methods receive these representations as input and learn to
    classify, cluster, segment, or reconstruct CAD models, considering the annotations
    at hand. However, the application of machine learning and deep learning methods
    to CAD models is very rare because of the scarcity of annotated CAD data available
    in the B-Rep format. In contrast to the geometric CAD models like ShapeNet [[62](#bib.bib62)],
    many parametric CAD datasets are not publicly released due to the proprietary
    nature of design data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的第一步是根据特定的分析目标选择CAD模型的几何、拓扑、功能和其他属性的子集，将其表示为适合的格式，例如数值向量、矩阵、张量或图形，以供深度学习方法处理。然而，由于连续的非欧几里得几何特征和离散的拓扑属性共存，表示B-Rep数据具有挑战性，使其难以适应常规结构化格式，如张量或固定长度编码。每种前沿方法在这一主题中的关键贡献是引入了一种将B-Rep数据编码或标记化为适合所采用深度学习架构的格式的方法，以满足其特定应用的需求。深度学习方法将这些表示作为输入，学习对CAD模型进行分类、聚类、分割或重建，考虑手头的注释。然而，由于B-Rep格式中可用的注释CAD数据稀缺，机器学习和深度学习方法在CAD模型中的应用非常少见。与几何CAD模型如ShapeNet
    [[62](#bib.bib62)]相比，许多参数化CAD数据集由于设计数据的专有性质而未公开发布。
- en: While there have been recent releases of small annotated datasets containing
    mechanical parts in B-Rep format for machine learning research [[75](#bib.bib75)],
    the majority of large-scale public databases, such as ABC, remain predominantly
    unlabeled. Additionally, not only the process of manually annotating B-Rep data
    in a specialized format needs engineering expertise, but it is also very time-consuming
    and costly, thus posing a significant limitation. Consequently, deep learning
    approaches that do not rely on external annotations, such as unsupervised and
    self-supervised learning, become particularly crucial alongside traditional supervised
    learning approaches in such scenarios. Learning feature representations based
    on intrinsic data features, without external annotations or expert knowledge,
    proves highly advantageous in overcoming the scarcity of annotated CAD data. In
    this section, we introduce existing research endeavors that employ GDL to extract
    feature representations from CAD B-Reps. These methods operate in either a supervised,
    self-supervised, or unsupervised manner, catering to various tasks such as classification,
    segmentation, similarity analysis and retrieval, and assembly prediction.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近发布了一些包含机械部件的B-Rep格式小型注释数据集用于机器学习研究[[75](#bib.bib75)]，但大多数大规模公共数据库，如ABC，仍主要是未标记的。此外，手动注释B-Rep数据的过程不仅需要工程专业知识，而且非常耗时且成本高昂，因此成为一个重要限制。因此，深度学习方法如无监督学习和自监督学习，在这种情况下与传统的监督学习方法一起变得尤为重要。基于内在数据特征学习特征表示，无需外部注释或专家知识，对于克服注释CAD数据的稀缺性极为有利。在本节中，我们介绍了采用GDL从CAD
    B-Reps中提取特征表示的现有研究工作。这些方法以监督、自监督或无监督的方式进行，适用于分类、分割、相似性分析与检索、组装预测等各种任务。
- en: IV-A CAD Classification and Retrieval
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A CAD分类与检索
- en: 'Designing methodologies for categorizing 3D components within B-Rep assemblies
    is significantly important for various applications, such as re-using similar
    CAD components in different assemblies, shape recommendation and alternative suggestion,
    and product complexity estimation, especially when 3D CAD models with substantially
    different geometries are within the same category and share similar topology.
    One of the first deep learning methods proposed to work directly on B-Rep data
    format of 3D CAD models is UV-Net [[9](#bib.bib9)]. UV-Net proposed a cohesive
    graph representation for B-Rep data by modeling topology with an adjacency graph,
    and modeling geometry in a regular grid format based on the U and V parameter
    domains of curves and surfaces. One of the main contributions of this work is
    to extract crucial geometric and topological features out of B-Rep data and make
    a grid data structure out of complex B-Rep data to feed to deep learning models
    for different tasks such as B-Rep classification and retrieval. To generate grid-structured
    feature representations from B-Rep data, this approach transforms each 3D surface
    into a regular 2D grid by performing surface sampling with fixed step sizes. In
    a similar fashion, it transforms each solid curve into a 1D grid. The resulting
    1D/2D grid mappings are referred to as UV-grids. Each sampled point in a surface’s
    2D grid conveys three distinct values across $7$ channels: a) The 3D absolute
    point position, represented as $xyz$ in the UV coordinate system, b) The 3D absolute
    surface normal, and c) The trimming mask with $1$ and $0$ denoting samples in
    the visible region and trimmed region, respectively. For the sampled points in
    a curve’s 1D grid, the encoding includes the absolute point UV coordinates and,
    optionally, the unit tangent vector.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 设计用于在 B-Rep 装配体中对 3D 组件进行分类的方法对于各种应用非常重要，例如在不同装配体中重用类似的 CAD 组件、形状推荐和替代建议、以及产品复杂性估算，特别是在具有显著不同几何形状的
    3D CAD 模型处于同一类别并共享相似拓扑结构时。首批被提议直接在 3D CAD 模型的 B-Rep 数据格式上工作的深度学习方法之一是 UV-Net [[9](#bib.bib9)]。UV-Net
    通过使用邻接图来建模拓扑结构，并基于曲线和表面的 U 和 V 参数域以规则网格格式建模几何形状，从而提出了一种连贯的 B-Rep 数据图表示方法。该工作的主要贡献之一是从
    B-Rep 数据中提取关键的几何和拓扑特征，并将复杂的 B-Rep 数据转化为网格数据结构，以供深度学习模型用于诸如 B-Rep 分类和检索等不同任务。为了从
    B-Rep 数据生成网格结构的特征表示，该方法通过以固定步长进行表面采样，将每个 3D 表面转换为规则的 2D 网格。类似地，它将每条实体曲线转换为 1D
    网格。得到的 1D/2D 网格映射称为 UV-网格。表面 2D 网格中的每个采样点在 $7$ 个通道中传递三个不同的值：a) 3D 绝对点位置，以 $xyz$
    在 UV 坐标系统中表示，b) 3D 绝对表面法线，以及 c) 修剪掩码，其中 $1$ 和 $0$ 分别表示可见区域和修剪区域的样本。对于曲线 1D 网格中的采样点，编码包括绝对点
    UV 坐标，并且可以选择性地包含单位切向量。
- en: 'As illustrated in Figure [4](#S4.F4 "Figure 4 ‣ IV-A CAD Classification and
    Retrieval ‣ IV CAD Representation Learning ‣ Geometric Deep Learning for Computer-Aided
    Design: A Survey"), UV-Net model architecture is comprised of both CNN and GCN
    layers to first extract features from 1D and 2D grids representing curves and
    surfaces, respectively, and then to capture the topological structure of the 3D
    shape encoded as a graph with hierarchical graph convolution layers. The 1D curve
    and 2D surface UV-grids are processed by 1D and 2D convolution and pooling layers
    while the weights of the convolution layers are shared among all curves and surfaces
    in a B-Rep to make them permutation-invariant. The $64$-dimensional feature vectors
    derived from surfaces and curves by the convolution layers of the CNN serve as
    the node and edge features within a face-adjacency graph $\mathcal{G}(\mathcal{V},\mathcal{E})$,
    where the set of graph nodes $\mathcal{V}$ represent the faces (surfaces) in B-Rep
    and the set of edges $\mathcal{E}$ represents the connection between faces. Subsequently,
    this graph is introduced to a multi-layer GCN, where the graph convolution layers
    propagate these features across the graph, enabling the capturing of both local
    and global structures inherent in the shape.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[4](#S4.F4 "图 4 ‣ IV-A CAD 分类与检索 ‣ IV CAD 表示学习 ‣ 针对计算机辅助设计的几何深度学习：综述")所示，UV-Net模型架构包括CNN和GCN层，首先从代表曲线和表面的1D和2D网格中提取特征，然后捕捉作为图表示的3D形状的拓扑结构，并使用层次图卷积层进行编码。1D曲线和2D表面UV网格通过1D和2D卷积及池化层进行处理，同时卷积层的权重在B-Rep中的所有曲线和表面之间共享，以使其具有置换不变性。CNN卷积层从表面和曲线中导出的$64$维特征向量作为面邻接图$\mathcal{G}(\mathcal{V},\mathcal{E})$中的节点和边特征，其中图节点集合$\mathcal{V}$表示B-Rep中的面（表面），边集合$\mathcal{E}$表示面之间的连接。随后，这个图被引入多层GCN中，图卷积层在图中传播这些特征，从而捕捉形状中固有的局部和全局结构。
- en: '![Refer to caption](img/c96f35f877935decdd80d04f82ccdf8b.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c96f35f877935decdd80d04f82ccdf8b.png)'
- en: 'Figure 4: Schematic representation of the UV-Net model architecture [[9](#bib.bib9)].
    The model takes B-Rep data as input, generating grid-structured features for surfaces
    and connecting curves. These UV-grid mappings are further processed through CNN
    and GCN architectures to learn feature embeddings for the overall graph and its
    individual nodes.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：UV-Net模型架构的示意图[[9](#bib.bib9)]。模型以B-Rep数据为输入，生成表面和连接曲线的网格结构特征。这些UV网格映射通过CNN和GCN架构进一步处理，以学习整体图及其单个节点的特征嵌入。
- en: The UV-Net encoder is used as a backbone for supervised and self-supervised
    learning on labeled and unlabeled datasets, respectively. For CAD classification,
    which is a supervised task, UV-Net encoder is followed by a 2-layer classification
    network to map the learned features to classes and the model is trained in an
    end-to-end manner on $3$ annotated datasets, SolidLetters [[9](#bib.bib9)], FabWave
    [[74](#bib.bib74)], and FeatureNet [[72](#bib.bib72)]. SolidLetters is currently
    the biggest synthetic 3D B-Rep dataset with a great variation in both geometry
    and topology annotated for a classification task. It consists of $96,000$ 3D shapes
    which represent $26$ English alphabet letters (a-z) with different fonts and dimensions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: UV-Net编码器被用作监督和自监督学习的骨干网络，分别用于标记和未标记的数据集。在CAD分类任务中，UV-Net编码器后接一个2层分类网络，将学习到的特征映射到类别，模型在$3$个注释数据集上以端到端的方式进行训练，这些数据集是SolidLetters
    [[9](#bib.bib9)]、FabWave [[74](#bib.bib74)] 和 FeatureNet [[72](#bib.bib72)]。SolidLetters是当前最大的人造3D
    B-Rep数据集，具有几何和拓扑的巨大变异，用于分类任务。它包含$96,000$个3D形状，代表$26$个英文字母（a-z），具有不同的字体和尺寸。
- en: For CAD retrieval, however, the premise is the absence of labeled data, and
    thus, the UV-Net encoder needs to be trained in a self-supervised manner. This
    leads to the utilization of deep learning models designed for self-supervised
    training, such as Graph Contrastive Learning (GCL) [[83](#bib.bib83)] or Graph
    AutoEncoder (GAE) [[60](#bib.bib60)]. For training the UV-Net encoder as a self-supervised
    model, GCL is leveraged to apply transformations on the face-adjacency graph and
    make positive pairs for each B-Rep sample. These transformations can be performed
    in various ways, such as randomly deleting nodes or edges with uniform probability
    or extracting a random node and its n-hop neighbors within a graph. Assuming that
    each B-Rep and its transformed verison are positive pairs, the UV-Net encoder
    extracts the shape embeddings of each pair as $\{h_{i},h_{j}\}$ and a 3-layer
    non-linear projection head with ReLU activations transforms these embeddings into
    latent vectors $z_{i}$ and $z_{j}$, respectively. For a batch comprising $N$ B-Rep
    samples, the latent vector for each sample, along with its corresponding positive
    pair, is computed. The entire model is then trained in an end-to-end manner with
    the objective of bringing the embedding of each sample closer to its positive
    pair. Simultaneously, the model works to treat the remaining $2(N-1)$ B-Reps as
    negative examples, aiming to push them further away from the positive pair. Through
    this process, the model learns to capture the intrinsic features and patterns
    within the data without the need for labeled examples. It essentially leverages
    the relationships within the data itself, generated through transformations or
    augmentations, to learn meaningful representations. This makes contrastive learning
    a self-supervised learning approach which is particularly useful in scenarios
    where obtaining labeled data is challenging or impractical. For retrieving similar
    CAD models, the model is trained on an unlabeled dataset like ABC. Subsequently,
    embeddings for random samples from the test set are used as queries, and their
    k-nearest neighbors are calculated within the UV-Net shape embedding space.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CAD检索，前提是缺乏标注数据，因此UV-Net编码器需要以自监督的方式进行训练。这导致使用专为自监督训练设计的深度学习模型，如图对比学习（GCL）[[83](#bib.bib83)]或图自编码器（GAE）[[60](#bib.bib60)]。在将UV-Net编码器训练为自监督模型时，GCL被用来对面邻接图进行变换，并为每个B-Rep样本创建正对。这些变换可以通过多种方式进行，例如以均匀概率随机删除节点或边，或提取图中的随机节点及其n-hop邻居。假设每个B-Rep及其变换版本是正对，UV-Net编码器提取每对的形状嵌入作为$\{h_{i},h_{j}\}$，并通过一个具有ReLU激活的3层非线性投影头将这些嵌入转换为潜在向量$z_{i}$和$z_{j}$。对于一个包含$N$个B-Rep样本的批次，计算每个样本及其对应正对的潜在向量。整个模型以端到端的方式进行训练，目标是将每个样本的嵌入向量拉近其正对。同时，模型还将剩余的$2(N-1)$个B-Rep视为负例，旨在将它们进一步推离正对。通过这一过程，模型学习捕捉数据中的内在特征和模式，而无需标注示例。它本质上利用数据自身的关系，通过变换或增强生成有意义的表示。这使得对比学习成为一种自监督学习方法，特别适用于获取标注数据具有挑战性或不切实际的场景。为了检索相似的CAD模型，模型在像ABC这样的未标记数据集上进行训练。随后，使用来自测试集的随机样本的嵌入作为查询，并计算它们在UV-Net形状嵌入空间中的k近邻。
- en: Likewise, the method proposed in [[13](#bib.bib13)] utilizes geometry as a means
    of self-supervision, extending its application to few-shot learning. Specifically,
    the method involves training an encoder-decoder structure to rasterize local CAD
    geometry, taking CAD B-Reps as input and generating surface rasterizations as
    output. B-Reps are assembled piecewise through explicitly defined surfaces with
    implicitly defined boundaries. Accordingly, the encoder of this approach adopts
    the hierarchical message passing architecture of SB-GCN proposed in [[19](#bib.bib19)]
    to effectively capture boundary features for encoding B-Rep faces. The decoder,
    in turn, reconstructs faces by simultaneously decoding the explicit surface parameterization
    and the implicit surface boundary. The embeddings learned through self-supervised
    learning in this method serve as input features for subsequent supervised learning
    tasks, including CAD classification on the FabWave dataset. Notably, with very
    limited labeled data (ranging from tens to hundreds of examples), the method outperforms
    previous supervised approaches while leveraging smaller training sets. This underscores
    the effectiveness of the differentiable CAD rasterizer in learning intricate 3D
    geometric features.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，[[13](#bib.bib13)]中提出的方法利用几何学作为自我监督的手段，将其应用扩展到少样本学习中。具体而言，该方法涉及训练一个编码器-解码器结构来光栅化局部
    CAD 几何图形，以 CAD B-Reps 作为输入，生成表面光栅化图像作为输出。B-Reps 通过显式定义的表面和隐式定义的边界逐块组装。因此，该方法的编码器采用了[[19](#bib.bib19)]中提出的
    SB-GCN 的层次消息传递架构，以有效捕捉边界特征来编码 B-Rep 面。解码器则通过同时解码显式表面参数化和隐式表面边界来重建面。通过这种方法自我监督学习得到的嵌入作为后续监督学习任务的输入特征，包括在
    FabWave 数据集上的 CAD 分类。值得注意的是，在有非常有限的标注数据（从几十到几百个例子）情况下，该方法在利用较小的训练集时超越了以往的监督方法。这突显了可微分
    CAD 光栅化器在学习复杂 3D 几何特征方面的有效性。
- en: IV-B CAD Segmentation
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B CAD 分割
- en: 'CAD segmentation entails decomposing a geometric solid model, represented in
    B-Rep format, into its individual components including faces and edges. CAD segmentation
    finds applications in retrieving CAD parametric feature history, machining feature
    recognition, Computer Aided Engineering (CAE) and Computer Aided Process Planning
    (CAPP). This is particularly intriguing as it facilitates the automation of several
    tedious manual tasks within CAD design and analysis, especially when users need
    to repeatedly select groups of faces and/or edges according to the manufacturing
    process as input for modeling or manufacturing operations [[84](#bib.bib84), [85](#bib.bib85),
    [76](#bib.bib76)]. However, progress in CAD segmentation was hindered until very
    recently due to the absence of advanced deep learning methods and large annotated
    datasets. MFCAD [[76](#bib.bib76)] is the first synthetic segmentation dataset
    consisting of $15,488$ 3D CAD models with planar faces, each annotated with $16$
    types of machining feature such as chamfer, triangular pocket, through hole, etc.
    Figure [5](#S4.F5 "Figure 5 ‣ IV-B CAD Segmentation ‣ IV CAD Representation Learning
    ‣ Geometric Deep Learning for Computer-Aided Design: A Survey") shows some examples
    from this dataset.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: CAD 分割涉及将以 B-Rep 格式表示的几何实体模型分解为其各个组成部分，包括面和边。CAD 分割在检索 CAD 参数特征历史、加工特征识别、计算机辅助工程（CAE）和计算机辅助工艺规划（CAPP）中具有应用。这尤其令人感兴趣，因为它有助于自动化
    CAD 设计和分析中的几个繁琐手动任务，特别是当用户需要根据制造过程反复选择面和/或边的组作为建模或制造操作的输入时 [[84](#bib.bib84),
    [85](#bib.bib85), [76](#bib.bib76)]。然而，由于缺乏先进的深度学习方法和大型注释数据集，CAD 分割的进展一直受到阻碍。MFCAD
    [[76](#bib.bib76)] 是第一个合成分割数据集，包含 $15,488$ 个具有平面面的 3D CAD 模型，每个模型标注有 $16$ 种加工特征，如倒角、三角形凹槽、通孔等。图
    [5](#S4.F5 "图 5 ‣ IV-B CAD 分割 ‣ IV CAD 表示学习 ‣ 面向计算机辅助设计的几何深度学习：综述") 显示了该数据集的一些示例。
- en: '![Refer to caption](img/3efe50eef566cb07e889b3973712854b.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3efe50eef566cb07e889b3973712854b.png)'
- en: 'Figure 5: Examples from the MFCAD dataset for manufacturing driven segmentation.
    Each solid face is labeled with $16$ types of machining features [[76](#bib.bib76)].'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 来自 MFCAD 数据集的制造驱动分割示例。每个实心面被标注为$16$种加工特征 [[76](#bib.bib76)]。'
- en: The CAD segmentation task can be framed as a node classification problem, with
    each 3D solid represented as a face adjacency graph. The graph nodes, corresponding
    to B-Rep faces, are then classified into distinct machining feature classes. CADNet
    [[81](#bib.bib81)] is one of the first proposed methods in this regard which represents
    the B-Rep solid as a graph encoding face geometry and topology, and utilizes a
    hierarchical graph convolutional network called Hierarchical CADNet to classify
    the graph nodes (or solid faces) into different machining features. For evaluation,
    this method not only leveraged MFCAD dataset, but it also created and released
    the extended version of the dataset, MFCAD++, which consists of $59,655$ CAD models
    with $3$ to $10$ machining features including both planar and non-planar faces.
    The UV-Net method, as introduced in the previous section, also addresses CAD segmentation
    in the same fashion, training its encoder in a supervised manner on the MFCAD
    dataset. However, translating the B-Rep into a face adjacency graph results in
    the loss of some information regarding the relative topological locations of nearby
    entities. Moreover, graph representations constructed based on UV coordinates
    lack invariance to translation and rotation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: CAD 分割任务可以被框架化为一个节点分类问题，其中每个 3D 实体表示为一个面邻接图。图中的节点对应于 B-Rep 面，然后将这些节点分类为不同的加工特征类别。CADNet
    [[81](#bib.bib81)] 是第一个提出这种方法的算法之一，它将 B-Rep 实体表示为一个图，编码面几何和拓扑结构，并利用一种称为层次 CADNet
    的层次图卷积网络将图节点（或实体面）分类为不同的加工特征。为了评估，该方法不仅利用了 MFCAD 数据集，还创建并发布了扩展版本的数据集 MFCAD++，该数据集包含
    $59,655$ 个 CAD 模型，具有 $3$ 到 $10$ 种加工特征，包括平面和非平面面。UV-Net 方法，如前面部分介绍的，也以相同的方式解决了
    CAD 分割问题，利用监督方式在 MFCAD 数据集上训练其编码器。然而，将 B-Rep 转换为面邻接图会导致一些关于相邻实体的相对拓扑位置的信息丢失。此外，基于
    UV 坐标构建的图表示缺乏对平移和旋转的不变性。
- en: 'BRepNet [[14](#bib.bib14)] is the first method specifically designed for B-Rep
    segmentation based on deep learning and, notably, it accomplishes this without
    introducing to the network any coordinate information. It operates directly on
    B-Rep faces and edges, exploiting compact information derived from their topological
    relationships for B-Rep segmentation. The motivation behind the BRepNet approach
    stems from the idea seen in the convolutional operation in CNNs used for image
    processing. In this operation, the local features are aggregated by sliding a
    small window called a filter or kernel over the grid data and performing element-wise
    multiplication between the filter and the overlapping grid cells, then pooling
    the results. This concept is extended to B-Reps, allowing the identification of
    a collection of faces, edges, and coedges at precisely defined locations relative
    to each coedge in the data structure. Coedge is a doubly linked list of directed
    edges, representing the neighboring structures of B-Rep entities. Each coedge
    also retains information about its parent face and edge, its neighbor (mating)
    coedge, and also pointers to the next and previous coedge in the loop around a
    face. Figure [6](#S4.F6 "Figure 6 ‣ IV-B CAD Segmentation ‣ IV CAD Representation
    Learning ‣ Geometric Deep Learning for Computer-Aided Design: A Survey") illustrates
    a topology example traversed by a sequence of walks from a given coedge (in red)
    to some of its neighboring entities such as mating coedge, next and previous coedges,
    faces and edges.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: BRepNet [[14](#bib.bib14)] 是第一个专门为 B-Rep 分割设计的基于深度学习的方法，值得注意的是，它在网络中并未引入任何坐标信息。它直接在
    B-Rep 面和边上操作，利用从其拓扑关系中派生的紧凑信息进行 B-Rep 分割。BRepNet 方法的动机源于 CNNs 中用于图像处理的卷积操作。在这一操作中，局部特征通过在网格数据上滑动一个称为滤波器或核的小窗口来聚合，执行滤波器与重叠网格单元之间的逐元素乘法，然后对结果进行池化。这个概念扩展到
    B-Reps，允许在数据结构中相对于每个 coedge 精确定位一组面、边和 coedge。Coedge 是一个双向链表的有向边，表示 B-Rep 实体的邻近结构。每个
    coedge 还保留关于其父面和边、邻居（配对）coedge 以及指向面周围循环中的下一个和前一个 coedge 的指针的信息。图 [6](#S4.F6 "图
    6 ‣ IV-B CAD 分割 ‣ IV CAD 表示学习 ‣ 计算机辅助设计的几何深度学习：综述") 说明了一个拓扑示例，该示例通过从给定的 coedge（红色）到其一些邻接实体（如配对的
    coedge、下一个和前一个 coedge、面和边）的一系列行走进行遍历。
- en: '![Refer to caption](img/59b11dfe45aca8c73b56220990e0f9e3.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/59b11dfe45aca8c73b56220990e0f9e3.png)'
- en: 'Figure 6: (left): The topology of a solid can be defined as a set of face,
    edge, coedge, vertex entities. and it can be traveresed by a sequence of walks.
    (right): The topology can be traveresed by a sequence of walks from the starting
    entitiy (in red). The walks can be Edge, Face, Next, Previous and Mate. As an
    example, here the starting entity is the red coedge, followed by Mate, Next, Mate,
    Edge walks [[14](#bib.bib14)].'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '图6: （左）固体的拓扑可以定义为一组面、边、共边、顶点实体，并且可以通过一系列路径进行遍历。（右）拓扑可以通过从起始实体（红色）开始的一系列路径进行遍历。路径可以是边、面、下一个、上一个和配对。例如，这里起始实体是红色共边，随后是配对、下一个、配对、边路径[[14](#bib.bib14)]。'
- en: 'Information about geometric features around a coedge, including face and edge
    type, face area, edge convexity and length, and coedge direction, is encoded as
    one-hot vectors and concatenated in a predetermined order forming feature matrices
    $X^{f}$, $X^{e}$, $X^{c}$ of face, edge and coedge, respectively. These matrices
    are then passed into a neural network where convolution operations are performed
    through matrix/vector multiplication to recognize patterns around each coedge.
    Additionally, the performance of BRepNet is evaluated on a segmentation dataset
    from the Fusion 360 Gallery which is released alongside BRepNet method, as the
    first segmentation dataset comprising of real 3D designs. As shown in Table [III](#S3.T3
    "TABLE III ‣ III Datasets ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey"), this dataset comprises $35,680$ 3D shapes each annotated with $8$
    types of modeling operations utilized to create faces in the respective model.
    Figure [7](#S4.F7 "Figure 7 ‣ IV-B CAD Segmentation ‣ IV CAD Representation Learning
    ‣ Geometric Deep Learning for Computer-Aided Design: A Survey") shows some examples
    from this dataset.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '关于共边周围几何特征的信息，包括面和边的类型、面面积、边的凸度和长度以及共边方向，被编码为独热向量并按照预定顺序连接形成面、边和共边的特征矩阵$X^{f}$、$X^{e}$、$X^{c}$。这些矩阵随后被传入神经网络，通过矩阵/向量乘法执行卷积操作，以识别每个共边周围的模式。此外，BRepNet的性能在从Fusion
    360 Gallery发布的分割数据集上进行评估，这是第一个包含真实3D设计的分割数据集。正如表[III](#S3.T3 "TABLE III ‣ III
    Datasets ‣ Geometric Deep Learning for Computer-Aided Design: A Survey")所示，该数据集包含$35,680$个3D形状，每个形状都标注了$8$种建模操作类型，用于创建模型中的面。图[7](#S4.F7
    "Figure 7 ‣ IV-B CAD Segmentation ‣ IV CAD Representation Learning ‣ Geometric
    Deep Learning for Computer-Aided Design: A Survey")展示了来自该数据集的一些示例。'
- en: '![Refer to caption](img/7dd60906b950bd844a915f3bf21e70a0.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7dd60906b950bd844a915f3bf21e70a0.png)'
- en: 'Figure 7: Examples from the Fusion 360 Gallery dataset annotated for construction-based
    segmentation. Each solid face is labeled with operation used in its construction
    [[14](#bib.bib14)].'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '图7: 来自Fusion 360 Gallery数据集的构建基础分割标注示例。每个固体面都标记了其构建中使用的操作[[14](#bib.bib14)]。'
- en: The self-supervised method proposed in [[13](#bib.bib13)], is also assessed
    on a segmentation task by initially pre-training the network on a subset of $27,450$
    parts from the Fusion 360 Gallery segmentation dataset. This pre-training is conducted
    in a self-supervised manner without utilizing face annotations. Subsequently,
    the pre-trained network undergoes fine-tuning in a supervised manner, being exposed
    to only a limited number of annotated parts to demonstrate the method’s performance
    in a few-shot setting on both Fusion 360 and MFCAD datasets. CADOps-Net [[25](#bib.bib25)]
    draws inspiration from CAD segmentation methods like UV-Net [[9](#bib.bib9)] and
    BRepNet [[14](#bib.bib14)], which segment B-Reps into distinct faces based on
    their associated CAD operations, and proposes a neural network architecture that
    takes the B-Rep data of a 3D shape as input and learns to decompose it into various
    operation steps and their corresponding types. Besides, it introduces the CC3D-Ops
    dataset, comprising $37,000$ CAD models annotated with per-face CAD operation
    types and construction steps ³³3https://cvi2.uni.lu/cc3d-ops/.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[13](#bib.bib13)]中提出的自监督方法，也在一个分割任务上进行了评估，方法是首先在Fusion 360 Gallery分割数据集中的$27,450$个零件子集上进行网络预训练。此预训练以自监督方式进行，不利用面注释。随后，预训练的网络在监督方式下进行微调，仅暴露给有限数量的注释零件，以展示该方法在Fusion
    360和MFCAD数据集上的少样本设置中的性能。CADOps-Net [[25](#bib.bib25)] 从CAD分割方法如UV-Net [[9](#bib.bib9)]
    和BRepNet [[14](#bib.bib14)]中获得灵感，这些方法将B-Reps分割成基于其关联CAD操作的不同面，并提出了一种神经网络架构，该架构将3D形状的B-Rep数据作为输入，学习将其分解为各种操作步骤及其对应类型。此外，它引入了CC3D-Ops数据集，包含$37,000$个CAD模型，标注了每个面的CAD操作类型和构建步骤³³3https://cvi2.uni.lu/cc3d-ops/。
- en: IV-C CAD Assembly
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C CAD装配
- en: The physical objects we encounter all around us are primarily intricate assemblies
    built up by CAD designers by designing and then aligning multiple smaller and
    simpler parts through CAD software. The meticulous pairing of parts in CAD is
    a laborious manual process, consuming approximately one-third of designers’ time
    [[19](#bib.bib19)]. It entails precise positioning of parts in relation to each
    other and specifying their relative movement. Consequently, optimizing this process
    is crucial for enhancing the efficiency of CAD systems. This issue has been explored
    in various studies using deep learning methods in recent years [[86](#bib.bib86),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93)] to simplify the part assembling and open avenues
    for various applications such as robotic assembly [[94](#bib.bib94)], CAD assembly
    synthesis [[95](#bib.bib95)], part motion prediction [[96](#bib.bib96)], robot
    design optimization [[97](#bib.bib97)] and similarity analysis in CAD assemblies
    [[98](#bib.bib98)]. However, all these approaches operate with non-parametric
    data structures like meshes, point clouds, and voxel grids. Consequently, they
    leverage GDL methods tailored for these data structures, such as DGCNN [[99](#bib.bib99)],
    PCPNet [[100](#bib.bib100)], PointNet [[8](#bib.bib8)], and PointNet++ [[8](#bib.bib8)],
    to learn the surface representations. These methods primarily adopt a top-down
    approach to predict the absolute pose of a set of parts for assembly in a global
    coordinate system. However, this approach lacks support for the parametric variations
    of parts for modifying the assembly or modeling degrees of freedom, and it may
    also result in failures when parts cannot achieve complete alignment. Additionally,
    these methods heavily rely on annotated datasets like PartNet [[64](#bib.bib64)],
    which exclusively provides data in mesh format.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们周围遇到的物理对象主要是由CAD设计师通过CAD软件设计并对齐多个较小且简单的部件构建而成的复杂组件。CAD中部件的细致配对是一个繁琐的手动过程，消耗了设计师约三分之一的时间[[19](#bib.bib19)]。它涉及到部件之间的精确定位及其相对运动的指定。因此，优化这一过程对提高CAD系统的效率至关重要。近年来，许多研究利用深度学习方法探讨了这个问题[[86](#bib.bib86),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93)]，以简化部件装配，并为诸如机器人装配[[94](#bib.bib94)]、CAD装配综合[[95](#bib.bib95)]、部件运动预测[[96](#bib.bib96)]、机器人设计优化[[97](#bib.bib97)]以及CAD装配中的相似性分析[[98](#bib.bib98)]等各种应用开辟了新的途径。然而，所有这些方法都使用了诸如网格、点云和体素网格等非参数化数据结构。因此，它们利用了为这些数据结构量身定制的GDL方法，如DGCNN
    [[99](#bib.bib99)]、PCPNet [[100](#bib.bib100)]、PointNet [[8](#bib.bib8)]和PointNet++
    [[8](#bib.bib8)]来学习表面表示。这些方法主要采用自上而下的方法来预测一组部件在全局坐标系统中的绝对姿态。然而，这种方法不支持部件的参数化变化以修改装配或建模自由度，并且当部件无法完全对齐时可能会导致失败。此外，这些方法严重依赖于像PartNet
    [[64](#bib.bib64)]这样的标注数据集，该数据集仅提供网格格式的数据。
- en: A bottom-up approach to assembling pairs of parts, relying on pairwise constraints
    and utilizing the joint or contact information available in B-Rep data, can address
    this issue without requiring class annotations on datasets. However, current B-Rep
    datasets like ABC [[77](#bib.bib77)] and Fusion 360 Gallery [[30](#bib.bib30)]
    lack pairing benchmarks for CAD assemblies, making them unsuitable for training
    assembly prediction models. AutoMate [[19](#bib.bib19)] and JoinABLe [[18](#bib.bib18)]
    are the only two works proposed recently, employing a bottom-up learning approach
    for pairing parts locally to form assemblies. Additionally, they have released
    B-Rep datasets along with their methods, providing pairing benchmarks for training
    models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一种依赖于成对约束并利用B-Rep数据中可用的连接或接触信息的自下而上的部件配对方法，可以解决这个问题，而无需对数据集进行类别标注。然而，当前的B-Rep数据集如ABC
    [[77](#bib.bib77)]和Fusion 360 Gallery [[30](#bib.bib30)]缺乏CAD装配的配对基准，使其不适合用于训练装配预测模型。AutoMate
    [[19](#bib.bib19)]和JoinABLe [[18](#bib.bib18)]是最近提出的仅有的两个工作，它们采用了自下而上的学习方法，以局部配对部件形成装配。此外，它们还发布了与其方法一起的B-Rep数据集，为训练模型提供了配对基准。
- en: IV-C1 AutoMate [[19](#bib.bib19)]
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 AutoMate [[19](#bib.bib19)]
- en: Automate is the first work in this area focusing on CAD assembly by operating
    on parametric B-Rep data format. Additionally, this work introduces the first
    large-scale dataset of CAD assemblies in B-Rep format with mating benchmarks,
    released to facilitate future research in this field. AutoMate dataset is made
    by collecting publicly available CAD designs from OnShape API, containing $92,529$
    unique assemblies with an average size of $12$ mates each, and $541,635$ unique
    mates. Mating in this work means aligning two parts with pairwise constraints
    defined based on B-Rep topology. These pairwise constraints are referred to as
    mates or joints, and they dictate the relative pose and degrees of freedom (DOF)
    of parts within an assembly. Two parts can be mated through various topological
    entities, such as faces, edges, and vertices. Therefore, it is essential to learn
    the feature representation of multiple topological entities at different levels
    to address the complexities of the CAD assembly problem. In contrast to previous
    CAD representation learning approaches, like BRepNet [[14](#bib.bib14)] and UV-Net
    [[9](#bib.bib9)], which construct a face adjacency graph to capture the homogeneous
    structure of the B-Rep and focus on learning feature representations for face
    entities, AutoMate [[19](#bib.bib19)] takes a different approach. It aims to learn
    representations for faces, loops, edges, and vertices by capturing the heterogeneous
    B-Rep structure. This is achieved through the introduction of the Structured B-Rep
    Graph Convolution Network (SB-GCN) architecture, a message-passing network designed
    for learning the heterogeneous graph representation of the B-Rep.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Automate 是在这一领域中首个专注于通过操作参数化 B-Rep 数据格式进行 CAD 装配的工作。此外，这项工作引入了首个大型的 CAD 装配 B-Rep
    格式数据集，包含配合基准，用于促进该领域未来的研究。AutoMate 数据集由从 OnShape API 收集的公开 CAD 设计组成，包含 $92,529$
    个独特的装配，每个装配的平均大小为 $12$ 个配合，以及 $541,635$ 个独特的配合。在这项工作中，配合意味着通过基于 B-Rep 拓扑定义的配对约束对齐两个部件。这些配对约束被称为配合或接头，它们决定了装配中部件的相对姿态和自由度
    (DOF)。两个部件可以通过各种拓扑实体进行配合，例如面、边和顶点。因此，学习不同层次上多个拓扑实体的特征表示对于解决 CAD 装配问题的复杂性至关重要。与以前的
    CAD 表示学习方法（如 BRepNet [[14](#bib.bib14)] 和 UV-Net [[9](#bib.bib9)]）通过构建面邻接图来捕捉
    B-Rep 的同质结构并专注于学习面实体的特征表示不同，AutoMate [[19](#bib.bib19)] 采取了不同的方法。它旨在通过捕捉异质 B-Rep
    结构来学习面、环、边和顶点的表示。这是通过引入结构化 B-Rep 图卷积网络 (SB-GCN) 架构实现的，该架构是一个消息传递网络，旨在学习 B-Rep
    的异质图表示。
- en: 'SB-GCN takes a heterogeneous graph as input, where faces F, edges E, vertices
    V, and loops L serve as graph nodes, and directed bipartite connection sets between
    them represent graph edges. Specifically, the relations between B-Rep vertices
    and edges are denoted by V:E and its transpose E:V. Similarly, E:L and L:E represent
    relationships between B-Rep edges and loops, while L:F and F:L denote connections
    between faces and loops. Additionally, there are undirected relations (meta-path)
    between geometrically adjacent faces, expressed as F:F. Each node is associated
    with a parametric geometry function encoded as a one-hot feature vector. The network
    utilizes structured convolutions to generate output feature vectors for all graph
    nodes. The adjacency structure of different node types is captured in an ordered
    hierarchy in different network layers. The initial three layers capture relations
    in the B-Rep hierarchy in a bottom-up order: Vertex to Edge, Edge to Loop, and
    Loop to Face. Subsequently, the following $k$ layers focus on capturing meta-relations
    between faces. The final three layers reverse the node relations: Face to Loop,
    Loop to Edge, and Edge to Vertex. The network makes predictions for mate location
    and type using two distinct output heads. The mate location prediction head assesses
    pairs of mating coordinate frames (MCFs) that are adjacent to the selected faces
    on the two mating parts. Meanwhile, the mate type prediction head anticipates
    how this pair of MCFs should be mated. This is done by classifying features into
    seven different mating categories, i.e. fastened,revolute, planar, slider, cylindrical,
    parallel, ball, pin slot. The model is trained on $180,102$ mates from the AutoMate
    dataset, and is integrated as an extension to the Onshape CAD system. This integration
    assists designers by providing mate recommendations for pairs of parts during
    the CAD design process.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: SB-GCN 以异构图作为输入，其中面 F、边 E、顶点 V 和环 L 作为图节点，它们之间的有向二分连接集表示图的边。具体来说，B-Rep 顶点和边之间的关系由
    V:E 和其转置 E:V 表示。类似地，E:L 和 L:E 表示 B-Rep 边和环之间的关系，而 L:F 和 F:L 则表示面和环之间的连接。此外，几何上相邻的面之间存在无向关系（meta-path），用
    F:F 表示。每个节点都与一个参数化几何函数相关，该函数被编码为一维热编码特征向量。网络利用结构化卷积生成所有图节点的输出特征向量。不同节点类型的邻接结构在不同网络层中按有序层级捕获。初始三层按自下而上的顺序捕获
    B-Rep 层次中的关系：顶点到边、边到环、环到面。随后，接下来的 $k$ 层集中于捕获面之间的元关系。最后三层则逆转节点关系：面到环、环到边、边到顶点。网络使用两个不同的输出头对配合位置和类型进行预测。配合位置预测头评估与选定面的两个配合部件相邻的配合坐标系（MCFs）对。而配合类型预测头则预测这一对
    MCFs 应如何配合。这通过将特征分类为七种不同的配合类别进行，即紧固、旋转、平面、滑块、圆柱、平行、球体、插销槽。该模型在来自 AutoMate 数据集的
    $180,102$ 个配合上进行训练，并作为扩展集成到 Onshape CAD 系统中。这一集成通过在 CAD 设计过程中为零件对提供配合建议来帮助设计师。
- en: IV-C2 JoinABLe [[18](#bib.bib18)]
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 JoinABLe [[18](#bib.bib18)]
- en: 'Joinable is another recent method that employs a bottom-up approach to predict
    joints (or mates) between parts based on pairwise constraints. In contrast to
    AutoMate, which relies on user contact surface selections on parts to rank and
    recommend multiple mating solutions, JoinABLe identifies joints between parts
    without being limited to predefined surfaces, doing so automatically and without
    requiring any user assistance. The Fusion 360 Gallery Assembly dataset is concurrently
    introduced and made available with the JoinABLe method for the purpose of training
    and evaluating the JoinABLe model. This dataset comprises two interconnected sets
    of data: Assembly and Joint. These datasets are collected from user-designed CAD
    models publicly accessible in the Autodesk Online Gallery. The Assembly data encompasses
    $8,251$ assemblies, totaling $154,468$ individual parts, along with their corresponding
    contact surfaces, holes, joints, and the associated graph structure. The Joint
    data includes $23,029$ distinct parts, incorporating $32,148$ joints between them.
    JoinABLe is trained using the joint information extracted from Fusion 360 Gallery
    Assembly-Joint dataset. The Assembly Dataset [[18](#bib.bib18)] is a subset of
    designs in which each CAD model is made of multiple parts. In CAD design context,
    each assembly or 3D shape is a collection of parts that joined together and a
    set of assemblies can make a CAD design or 3D object.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Joinable 是一种最近的方法，它采用自下而上的方式，通过配对约束来预测部件之间的接头（或配合）。与 AutoMate 依赖用户对部件表面进行选择以排序和推荐多个配合解决方案不同，JoinABLe
    在不受预定义表面限制的情况下自动识别部件之间的接头，并且不需要用户的任何帮助。Fusion 360 Gallery Assembly 数据集与 JoinABLe
    方法同时推出并提供，用于训练和评估 JoinABLe 模型。该数据集包含两组相互关联的数据：Assembly 和 Joint。这些数据集来自于在 Autodesk
    在线画廊中公开访问的用户设计的 CAD 模型。Assembly 数据包括 $8,251$ 个组件，总计 $154,468$ 个单独的部件，以及相应的接触面、孔、接头和相关的图结构。Joint
    数据包括 $23,029$ 个不同的部件，其中包含 $32,148$ 个接头。JoinABLe 是使用从 Fusion 360 Gallery Assembly-Joint
    数据集中提取的接头信息进行训练的。Assembly 数据集 [[18](#bib.bib18)] 是一个设计子集，其中每个 CAD 模型由多个部件组成。在
    CAD 设计的背景下，每个组件或 3D 形状都是多个部件的集合，这些部件结合在一起，一组组件可以形成一个 CAD 设计或 3D 对象。
- en: 'Two parts are aligned through their joint axes, each possessing a specific
    direction. The joint axis on each part can be defined on either a face or edge
    entity, featuring an origin point and a directional vector. For instance, on a
    circular surface entity, the origin point is the center of the circle and the
    direction vector is the normal vector. These joint axes information for pairs
    of parts are provided in Fusion 360 Gallery Assembly-Joint dataset, which is a
    subset of Fusion 360 Gallery Assembly dataset, as ground truth for training a
    joint prediction deep learning model. This dataset contains 23,029 parts with
    $32,148$ joints between them. As shown in Figure [8](#S4.F8 "Figure 8 ‣ IV-C2
    JoinABLe [18] ‣ IV-C CAD Assembly ‣ IV CAD Representation Learning ‣ Geometric
    Deep Learning for Computer-Aided Design: A Survey"), given two parts, JoinABLe
    is designed to predict the parametric joint between them, encompassing joint axis
    prediction (origin points on two parts and direction vectors) and joint pose prediction.
    To facilitate this, each part is modeled as a graph $\mathcal{G}(\mathcal{V},\mathcal{E})$
    with B-Rep faces and edges serving as graph nodes, and their adjacency relationships
    functioning as graph edges. B-Rep faces are encoded as one-hot feature vectors,
    representing surface types like plane and cylinder, along with a flag indicating
    if the surface is reversed relative to the face. Similarly, one-hot vectors for
    B-Rep edges encompass curve characteristics such as line and circle, curve length,
    and a flag denoting whether the curve is reversed concerning the edge. These one-hot
    vectors function as graph node features.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '两个部分通过它们的关节轴对齐，每个部分都有一个特定的方向。每个部分上的关节轴可以定义在一个面或边实体上，具有一个原点和一个方向向量。例如，在一个圆形表面实体上，原点是圆心，方向向量是法向量。这些关节轴的信息针对部分对提供在
    Fusion 360 Gallery Assembly-Joint 数据集中，这是 Fusion 360 Gallery Assembly 数据集的一个子集，用作训练关节预测深度学习模型的真实数据。该数据集包含
    23,029 个部件，以及它们之间 $32,148$ 个关节。如图 [8](#S4.F8 "Figure 8 ‣ IV-C2 JoinABLe [18] ‣
    IV-C CAD Assembly ‣ IV CAD Representation Learning ‣ Geometric Deep Learning for
    Computer-Aided Design: A Survey") 所示，给定两个部件，JoinABLe 被设计用来预测它们之间的参数化关节，包括关节轴预测（两个部件上的原点和方向向量）和关节姿态预测。为此，每个部件被建模为一个图
    $\mathcal{G}(\mathcal{V},\mathcal{E})$，其中 B-Rep 面和边作为图节点，它们的邻接关系作为图边。B-Rep 面被编码为
    one-hot 特征向量，表示诸如平面和圆柱等表面类型，并带有一个标志指示表面是否相对于面反转。类似地，B-Rep 边的 one-hot 向量包括曲线特征，如直线和圆，曲线长度，以及一个标志表示曲线是否相对于边反转。这些
    one-hot 向量作为图节点特征。'
- en: '![Refer to caption](img/50a7c4253e5c51735579c82822233f6f.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/50a7c4253e5c51735579c82822233f6f.png)'
- en: 'Figure 8: An example showing how joints are defined between parts (solids)
    in Fusion 360 Assembly-Joint dataset. Give a pair of parts, JoinABLe predicts
    the joint axis and pose between them according to the joint defined by the ground
    truth [[18](#bib.bib18)].'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：一个示例，展示了如何在 Fusion 360 Assembly-Joint 数据集中定义部件（固体）之间的关节。给定一对部件，JoinABLe
    根据真实数据定义的关节预测它们之间的关节轴和姿态 [[18](#bib.bib18)]。
- en: 'For two parts represented by graphs $G_{1}$ and $G_{2}$ with $N$ and $M$ nodes,
    respectively, the joint graph $G_{j}$ is constructed to illustrate connections
    between the two graphs. This connection information in $G_{j}$ can be expressed
    as a binary matrix of dimensions $N\times M$. In alignment with the ground truth
    joints provided in the Joint dataset, only one matrix element, representing the
    joint between two entities across the two parts, should be set to $1$, while the
    others remain $0$. The JoinABLe model aims to find that one positive element and
    it comprises three main components: encoder, joint axis prediction, and joint
    pose prediction. The encoder follows a Siamese-style architecture with two distinct
    MLP branches. One branch focuses on learning node features that represent B-Rep
    faces, while the other concentrates on learning node features representing B-Rep
    edges. The learned embeddings for faces and edges in each part are subsequently
    concatenated to form the node embeddings for the corresponding graph. These node
    embeddings are then input into a Graph Attention Network v2 (GATv2) [[101](#bib.bib101)]
    which captures the local neighborhood structure within each graph through message
    passing.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于由图 $G_{1}$ 和 $G_{2}$ 表示的两个部分，其中分别有 $N$ 和 $M$ 个节点，联合图 $G_{j}$ 被构建以展示两个图之间的连接。
    $G_{j}$ 中的连接信息可以表示为一个 $N\times M$ 维的二进制矩阵。与Joint数据集中提供的真实联合对齐，仅一个矩阵元素表示两个部分之间的联合应设置为
    $1$，其余元素保持 $0$。JoinABLe 模型旨在找到那个唯一的正元素，它包含三个主要组件：编码器、联合轴预测和联合姿态预测。编码器采用Siamese风格架构，具有两个独立的MLP分支。一个分支专注于学习表示B-Rep面片的节点特征，而另一个分支则专注于学习表示B-Rep边缘的节点特征。每部分中面片和边缘的学习嵌入随后被连接以形成相应图的节点嵌入。这些节点嵌入随后输入到Graph
    Attention Network v2 (GATv2) [[101](#bib.bib101)]，通过消息传递捕获每个图内的局部邻域结构。
- en: 'The joint axis prediction is formulated as a link prediction task, aiming to
    predict a connection between the graphs $G_{1}$ and $G_{2}$ by linking two nodes.
    This involves edge convolution on the joint graph $G_{j}$ which illustrates the
    connections between the two graphs and is updated with the node features learned
    by the encoder in the previous step. Assuming that $h_{v}$, $h_{u}$ represent
    the learned features for the two nodes $v$, $u$, from $G_{1}$, $G_{2}$, respectively,
    the edge convolution to learn the connection between the two nodes in the two
    graphs is performed as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 联合轴预测被制定为一个连接预测任务，旨在通过连接两个节点来预测图 $G_{1}$ 和 $G_{2}$ 之间的连接。这涉及在联合图 $G_{j}$ 上进行边卷积，联合图
    $G_{j}$ 展示了两个图之间的连接，并使用编码器在先前步骤中学习的节点特征进行更新。假设 $h_{v}$ 和 $h_{u}$ 分别表示 $G_{1}$
    和 $G_{2}$ 中两个节点 $v$ 和 $u$ 的学习特征，边卷积用于学习两个图中两个节点之间的连接，如下所示：
- en: '|  | $h_{uv}=\Phi(h_{u}\oplus h_{v}),$ |  | (1) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{uv}=\Phi(h_{u}\oplus h_{v}),$ |  | (1) |'
- en: where $\Phi(\cdot)$ is a 3-layer MLP applied to the concatenated features of
    $h_{u}$, $h_{v}$. Following the edge convolution, a softmax function is applied
    to normalize the features and predict the most probable link between the nodes
    in $G_{j}$. After predicting the joint axes to align the two parts, the pose prediction
    head employs a neurally guided search approach to iterate through the top-k joint
    axis predictions. As an supplementary work, JoinABLe has also proposed assembling
    a multi-part design using only the individual parts and the sequence of part pairs
    derived from the Assembly dataset. However, in real-world applications, this well-defined
    assembly sequence and the corresponding assembly graph might not be available
    for the network to use. Additionally, a misalignment in any of the assembly steps
    could result in an incorrect overall assembly. Therefore, it is suggested that
    for large and complex assemblies, a combination of top-down and bottom-up approaches
    might be more effective.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Phi(\cdot)$ 是一个3层MLP，应用于 $h_{u}$ 和 $h_{v}$ 的连接特征。紧接着，在边卷积之后，应用softmax函数来归一化特征并预测
    $G_{j}$ 中节点之间最可能的连接。在预测联合轴以对齐两个部分之后，姿态预测头采用神经引导的搜索方法，通过前k个联合轴预测进行迭代。作为补充工作，JoinABLe
    还提出了使用单独的部分和从Assembly数据集中获得的部件对序列来组装多部分设计。然而，在实际应用中，这种明确定义的组装序列和相应的组装图可能无法提供给网络使用。此外，任何组装步骤中的错位可能导致整体组装不正确。因此，建议对于大型复杂组装，可能更有效的方式是结合自上而下和自下而上的方法。
- en: 'In the CAD Assembly problem two primary questions often arise: how to select
    the pair of parts to joint (or mate), and how to assemble them [[102](#bib.bib102)].
    The former question can be addressed by leveraging similarity analysis and retrieval
    methods to identify suitable pairs of parts. AutoMate and JoinAble, on the other
    hand concentrate on answering the latter question by learning the process of mating
    two parts in an assembly.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CAD 装配问题中，通常会出现两个主要问题：如何选择要连接（或配合）的零件对，以及如何将它们组装起来 [[102](#bib.bib102)]。前一个问题可以通过利用相似性分析和检索方法来识别合适的零件对来解决。而
    AutoMate 和 JoinAble 则集中于通过学习装配过程中配合两个零件的过程来回答后一个问题。
- en: V CAD Construction with Generative Deep Learning
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 生成式深度学习的 CAD 构建
- en: 'CAD construction with generative deep learning involves leveraging advanced
    GDL methods to automatically generate or assist in the creation of parametric
    CAD models. These approaches can support designers in various ways to streamline
    the design process. This includes tasks like generating or auto-completing sketches,
    as well as generating CAD operations to construct a 3D model based on the designed
    sketch. In this section, we categorize these methods into five distinct groups:
    1) methods focusing on 2D sketches, aiming to automate the sketching process in
    a 2D space as an initial step before developing 3D CAD models, 2) methods targeting
    3D CAD model reconstruction given the sketch of the model, 3) methods generating
    CAD construction sequences, specifically focused on sketch and extrude operations,
    for CAD construction, 4) methods which perform direct B-Rep synthesis to generate
    3D CAD models, and 5) methods generating 3D CAD models from Point Cloud data.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 利用生成式深度学习进行 CAD 构建涉及利用先进的 GDL 方法自动生成或协助创建参数化 CAD 模型。这些方法可以以多种方式支持设计师，从而简化设计过程。这包括生成或自动完成草图，以及根据设计草图生成
    CAD 操作以构建 3D 模型。在这一部分，我们将这些方法分为五个不同的组：1) 关注 2D 草图的方法，旨在自动化 2D 空间中的草图绘制过程，作为开发
    3D CAD 模型之前的初步步骤，2) 针对给定模型草图的 3D CAD 模型重建方法，3) 生成 CAD 构建序列的方法，特别关注草图和拉伸操作，用于 CAD
    构建，4) 直接进行 B-Rep 合成以生成 3D CAD 模型的方法，以及 5) 从点云数据生成 3D CAD 模型的方法。
- en: V-A Engineering 2D Sketch Generation for CAD
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 工程 2D 草图生成用于 CAD
- en: Engineering 2D sketches form the basis 3D CAD design. The 2D sketches composed
    of a collection of geometric primitives, such as vertex, lines, arcs, and circles,
    with their corresponding parameters (e.g. radius, length, and coordinates), and
    explicit imposed constraints between primitives (e.g. perpendicularity, orthogonality,
    coincidence, parallelism, symmetry, and equality) determining their final configuration.
    These 2D sketches can then be extruded to make 3D designs. Synthesizing parametric
    2D sketches and learning their encoded relational structure can save a lot of
    time and effort from designers when designing complex engineering sketches. However,
    leveraging deep learning approaches in this regard needs large-scale datasets
    of 2D engineering sketches. Most of the existing large-scale datasets provide
    hand-drawn sketches of common objects, such as furniture, cars, etc. The QuickDraw
    dataset [[103](#bib.bib103)] is collected from the Quick, Draw! online game [[104](#bib.bib104)],
    and Sketchy dataset [[79](#bib.bib79)] is a collection of paired pixel-based natural
    images and their corresponding vector sketches. These sketch datasets are based
    on vector images of sketches not their underlying parametric relational geometry.
    For reasoning about parametric CAD sketches and inferring their design steps using
    deep learning models, a large-scale dataset of parametric CAD sketches is needed.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 工程 2D 草图构成了 3D CAD 设计的基础。这些 2D 草图由一组几何基元组成，例如顶点、直线、弧和圆，以及它们的相应参数（例如半径、长度和坐标），并且在基元之间施加显式约束（例如垂直、正交、重合、平行、对称和相等），这些约束决定了它们的最终配置。这些
    2D 草图可以被拉伸以生成 3D 设计。合成参数化的 2D 草图并学习它们编码的关系结构可以节省设计师在设计复杂工程草图时的大量时间和精力。然而，在这方面利用深度学习方法需要大规模的
    2D 工程草图数据集。现有的大规模数据集大多提供了常见物体的手绘草图，如家具、汽车等。QuickDraw 数据集 [[103](#bib.bib103)]
    是从 Quick, Draw! 在线游戏 [[104](#bib.bib104)] 收集的，而 Sketchy 数据集 [[79](#bib.bib79)]
    是自然图像与其相应矢量草图的配对集合。这些草图数据集基于草图的矢量图像，而不是其底层参数化关系几何。为了利用深度学习模型推理参数化 CAD 草图并推断其设计步骤，需要一个大规模的参数化
    CAD 草图数据集。
- en: V-A1 SketchGraphs [[20](#bib.bib20)]
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 SketchGraphs [[20](#bib.bib20)]
- en: 'The first dataset introduced in this regard is SketchGraphs, which is a collection
    of $15$ million real-world 2D CAD sketches from Onshape platform [[40](#bib.bib40)],
    each of which represented as a geometric constraint graph where the nodes are
    the geometric primitives and edges denote the designer-imposed geometric relationships
    between primitives. An open-source data processing pipeline is also released along
    with the dataset to facilitate further research on this area.⁴⁴4https://github.com/PrincetonLIPS/SketchGraphs
    SketchGraphs does not only provide the underlying parametric geometry of the sketches,
    but it also provides ground truth construction operations for both geometric primitives
    and the constraints between them. Therefore, it can be used for training deep
    learning models for different generative applications facilitating the design
    process. One of these applications which could be used as an advanced feature
    in CAD software is to automatically build the parameteric CAD model given a hand-drawn
    sketch or a noisy scan of the object. The SketchGraphs processing pipeline can
    be used to produce noisy renderings of the sketches. In this way, a large-scale
    dataset or paired geometric sketches and their noisy rendered images can be created
    to train a deep learning model for predicting the design steps of a sketch given
    its hand-drawn image. The 3D design of the model can then be obtained by extruding
    the designed 2D sketch. In [[20](#bib.bib20)], an auto-regressive model is proposed
    which employs SketchGraphs dataset for two use cases: 1) Autoconstrain, which
    is conditional completion of an sketch by generating constraints between primitives
    given the unconstrained geometry, and 2) Generative modeling, which is auto-completing
    a partially designed sketch by generating construction operations for adding the
    next primitives and constraints between them. Although most of the CAD software
    have a built-in constraint solver to be used in design process, these generative
    methods are useful when an unconstrained sketch is uploaded to the software as
    a drawing scan designer needs to find the constraints between the sketch primitives
    and/or complete the sketch. This problem is quite similar to program synthesis
    or induction in constraint programming. The SketchGraphs dataset and its generative
    methods can be a good baseline for the future works in this direction. In the
    following, the two generative use-cases are described in more detail.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面引入的第一个数据集是 SketchGraphs，这是一个来自 Onshape 平台的 $15$ 万个现实世界 2D CAD 草图的集合 [[40](#bib.bib40)]，每个草图都被表示为一个几何约束图，其中节点是几何原件，边缘表示设计师施加的几何关系。数据集还发布了一个开源的数据处理管道，以促进该领域的进一步研究。⁴⁴4https://github.com/PrincetonLIPS/SketchGraphs
    SketchGraphs 不仅提供了草图的基础参数几何，还提供了几何原件及其之间约束的真实构建操作。因此，它可以用于训练深度学习模型，以便用于不同的生成应用，从而促进设计过程。一个可以作为
    CAD 软件高级功能的应用是：给定手绘草图或物体的噪声扫描，自动构建参数化的 CAD 模型。SketchGraphs 处理管道可以用于生成草图的噪声渲染图。通过这种方式，可以创建一个大规模数据集，包含配对的几何草图及其噪声渲染图像，以训练深度学习模型来预测给定手绘图像的设计步骤。然后，通过挤出设计的
    2D 草图，可以获得模型的 3D 设计。在 [[20](#bib.bib20)] 中，提出了一种自回归模型，该模型利用 SketchGraphs 数据集进行两个用例：1）Autoconstrain，即通过生成原件之间的约束来条件性地完成草图，给定未约束的几何，2）Generative
    modeling，即通过生成构建操作来自动完成部分设计的草图，以添加下一个原件及其之间的约束。尽管大多数 CAD 软件都有内置的约束求解器可用于设计过程，但这些生成方法在软件上传了未约束的草图作为绘图扫描时非常有用，设计师需要找出草图原件之间的约束和/或完成草图。这个问题与约束编程中的程序合成或归纳相似。SketchGraphs
    数据集及其生成方法可以作为未来这方面工作的良好基准。以下是对这两种生成用例的更详细描述。
- en: 'Let us assume a sketch represented by a multi-hypergraph $\mathcal{G=(V,E)}$
    where nodes $\mathcal{V}$ denote primitives and edges $\mathcal{E}$ denote constraints
    between them. In this graph, each edge might connect one or more nodes, and multiple
    edges might share the same set of connected nodes. An edge with a single node
    is indicated as a self-loop showing a single constraint (e.g. length) for a single
    primitive (e.g. line), and a hyper-edge applies on three or more nodes (e.g. mirror
    constraint applied on two primitives while assigning one more primitive as an
    axis of symmetry). An example of a sketch graph is illustrated in Figure [3](#S3.F3
    "Figure 3 ‣ III Datasets ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey"). Each constraint is identified by its type, and each primitive is identified
    by its type and its parameters (different primitives might have different number
    of parameters). For Autoconstrain task, all the graph nodes (primitives) are given,
    and the model is trained in a supervised manner to predict the sequence of graph
    edges considering the ground truth ordering of constraints in the dataset. This
    problem can be seen as an example of graph link prediction [[105](#bib.bib105)]
    which predicts the induced relationships between the graph nodes. Starting from
    the first construction step, the model first predicts which node should be connected
    to the current node and then creates a link (edge) between these two neighboring
    nodes. Then it predicts the type of this edge (constraint). For generative modeling
    task which auto-completes a partially completed sketch (graph) by generating new
    primitives (nodes) and constraints between them (edges), the primitives are only
    represented by their type (the primitive parameters are ignored) and the constraints
    are represented by both their type and their numerical or categorical parameters.
    As an example, if the constraint between two primitives is distance, its parameter
    could be a scalar value denoting the Euclidean distance between the two primitives.
    However, while this model predicts both type and parameters for constraints, it
    only predicts the type of the primitives, not their parameters, and the initial
    coordinates of the primitives might not fit into the sketch and constraints. Therefore,
    the final configuration of the primitive coordinates in the sketch needs to be
    found by the CAD software’s built-in geometric constraint solver. This issue is
    addressed in the next proposed work for 2D sketch generation, CurveGen-TurtleGen
    [[21](#bib.bib21)], which is introduced in [V-A2](#S5.SS1.SSS2 "V-A2 CurveGen-TurtleGen
    [21] ‣ V-A Engineering 2D Sketch Generation for CAD ‣ V CAD Construction with
    Generative Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey").'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '设想一个由多重超图 $\mathcal{G=(V,E)}$ 表示的草图，其中节点 $\mathcal{V}$ 表示原始对象，边 $\mathcal{E}$
    表示它们之间的约束。在这个图中，每条边可能连接一个或多个节点，并且多个边可能共享相同的连接节点集。单节点的边被标示为自环，表示一个单一约束（例如长度）对于一个单一原始对象（例如直线），而超边则作用于三个或更多节点（例如，将一个原始对象作为对称轴时，施加在两个原始对象上的镜像约束）。图
    [3](#S3.F3 "Figure 3 ‣ III Datasets ‣ Geometric Deep Learning for Computer-Aided
    Design: A Survey") 中展示了一个草图图的示例。每个约束都通过其类型进行标识，每个原始对象则通过其类型及其参数进行标识（不同的原始对象可能有不同数量的参数）。对于自动约束任务，所有图节点（原始对象）都已给定，模型以监督方式进行训练，以预测图边的顺序，考虑到数据集中约束的实际顺序。这个问题可以视为图链接预测的一个示例
    [[105](#bib.bib105)]，它预测图节点之间的诱发关系。从第一个构建步骤开始，模型首先预测哪个节点应该连接到当前节点，然后在这两个相邻节点之间创建一个链接（边）。接着，它预测这条边（约束）的类型。对于生成建模任务，该任务通过生成新的原始对象（节点）和它们之间的约束（边）来自动完成一个部分完成的草图（图），原始对象仅通过其类型表示（原始对象的参数被忽略），约束则通过其类型及其数值或分类参数表示。例如，如果两个原始对象之间的约束是距离，则其参数可以是表示两个原始对象之间欧几里得距离的标量值。然而，虽然该模型预测了约束的类型和参数，但它仅预测原始对象的类型，而不是它们的参数，并且原始对象的初始坐标可能不符合草图和约束。因此，草图中原始对象坐标的最终配置需要由
    CAD 软件的内置几何约束求解器来确定。这个问题在下一步提出的 2D 草图生成工作中得到解决，即 CurveGen-TurtleGen [[21](#bib.bib21)]，该工作在
    [V-A2](#S5.SS1.SSS2 "V-A2 CurveGen-TurtleGen [21] ‣ V-A Engineering 2D Sketch
    Generation for CAD ‣ V CAD Construction with Generative Deep Learning ‣ Geometric
    Deep Learning for Computer-Aided Design: A Survey") 中介绍。'
- en: V-A2 CurveGen-TurtleGen [[21](#bib.bib21)]
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 CurveGen-TurtleGen [[21](#bib.bib21)]
- en: 'As discussed previously in [V-A1](#S5.SS1.SSS1 "V-A1 SketchGraphs [20] ‣ V-A
    Engineering 2D Sketch Generation for CAD ‣ V CAD Construction with Generative
    Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design: A Survey"),
    the generative models in SketchGraphs rely on the built-in sketch constraint solver
    in CAD software to set the final configuration of the produced sketch. This issue
    is addressed by two generative models, CurveGen and TurtleGen, proposed in [[21](#bib.bib21)]
    which encode constraint information implicitly in the geometric coordinates to
    be independent from the sketch constraint solver. In this work, the primitive
    types are simply limited to lines, curves, and circles, as they are the most common
    primitives used in 2D sketches, and it is considered that the constraints between
    the primitives in sketches should be defined in a way that the geometric primitives
    can form closed profile loops. In this regard, this method proposed two different
    representations for engineering sketches: Sketch Hypergraph representation which
    is used by CurveGen, and Turtle Graphics representation which is used by TurtleGen.
    In the Sketch Hypergraph representation, the sketch is represented as a hypergraph
    $\mathcal{G}=(\mathcal{V},\mathcal{E})$, where a set of vertices $\mathcal{V}=\left\{\nu_{1},\nu_{2},...,\nu_{n}\right\}$
    with their corresponding 2D coordinates $\nu_{i}=(x_{i},y_{i})$ are encoded as
    graph nodes, while $\mathcal{E}$ denotes a set of hyperedges connecting two or
    more vertices to make different primitives. The primitive type is defined by the
    cardinality of the hyperedge. For example, the primitive line is made by two connected
    vertices, arc is made by three connected vertices and circle can be seen as a
    set of four connected vertices. Figure [9](#S5.F9 "Figure 9 ‣ V-A2 CurveGen-TurtleGen
    [21] ‣ V-A Engineering 2D Sketch Generation for CAD ‣ V CAD Construction with
    Generative Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey") shows an example of a simple sketch consisting of $12$ vertices and
    $9$ hyperedges.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '如之前在[V-A1](#S5.SS1.SSS1 "V-A1 SketchGraphs [20] ‣ V-A Engineering 2D Sketch
    Generation for CAD ‣ V CAD Construction with Generative Deep Learning ‣ Geometric
    Deep Learning for Computer-Aided Design: A Survey")中讨论的那样，SketchGraphs中的生成模型依赖于CAD软件内置的草图约束求解器来设置生成草图的最终配置。这个问题由两个生成模型CurveGen和TurtleGen解决，它们在几何坐标中隐式编码约束信息，从而不依赖于草图约束求解器。这项工作中，原始类型简单地限制为线条、曲线和圆圈，因为它们是2D草图中最常见的基本图形，并且认为草图中原始图形之间的约束应该以几何原始图形能够形成封闭轮廓循环的方式进行定义。在这方面，该方法为工程草图提出了两种不同的表示：CurveGen使用的Sketch
    Hypergraph表示法和TurtleGen使用的Turtle Graphics表示法。在Sketch Hypergraph表示法中，草图表示为一个超图
    $\mathcal{G}=(\mathcal{V},\mathcal{E})$，其中一组顶点 $\mathcal{V}=\left\{\nu_{1},\nu_{2},...,\nu_{n}\right\}$
    及其对应的2D坐标 $\nu_{i}=(x_{i},y_{i})$ 被编码为图节点，而 $\mathcal{E}$ 表示一组超边，这些超边连接两个或更多的顶点以形成不同的原始图形。原始图形类型由超边的基数定义。例如，原始线条由两个连接的顶点构成，弧由三个连接的顶点构成，圆可以看作是四个连接的顶点。图[9](#S5.F9
    "Figure 9 ‣ V-A2 CurveGen-TurtleGen [21] ‣ V-A Engineering 2D Sketch Generation
    for CAD ‣ V CAD Construction with Generative Deep Learning ‣ Geometric Deep Learning
    for Computer-Aided Design: A Survey")展示了一个由$12$个顶点和$9$条超边组成的简单草图示例。'
- en: '![Refer to caption](img/c3573390f2080af9872e3f2dfd7c6206.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c3573390f2080af9872e3f2dfd7c6206.png)'
- en: 'Figure 9: The illustration showcases a basic sketch composed of $12$ vertices
    and $9$ hyperedges of types line, circle and arc entities. On the right side,
    a sequence of commands for rendering this sketch, following the grammar proposed
    by [[21](#bib.bib21)], is presented. Each line connects $2$ vertices denoted by
    $\Delta$, every arc is defined by traversing $3$ vertices, and circles pass through
    $4$ vertices. Notably, in each loop, the initiation point of each entity coincides
    with the termination point of the preceding entity in the sequence.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：插图展示了一个由$12$个顶点和$9$条类型为线条、圆圈和弧的超边组成的基本草图。在右侧，展示了一系列渲染该草图的命令，遵循[[21](#bib.bib21)]提出的语法。每条线连接$2$个顶点，用
    $\Delta$ 表示，每条弧由经过$3$个顶点定义，而圆则通过$4$个顶点。值得注意的是，在每个循环中，每个实体的起始点与序列中前一个实体的终点重合。
- en: 'This representation is used by CurveGen, which is an autoregressive Transformer
    based on PolyGen [[106](#bib.bib106)]. PolyGen is a an autoregressive generative
    method for generating 3D meshes using Transformer architectures which are able
    to capture long-range dependencies. The mesh vertices are modeled unconditionally
    by a Transformer, and the mesh faces are modeled conditioned on the mesh vertices
    by a combination of Transformers and pointer networks [[107](#bib.bib107)]. Similar
    to PolyGen, CurveGen also generates directly the sketch hypergraph representation
    by first generating the graph vertices $\mathcal{V}$ which are used to make curves,
    and then generating the graph hyperedges $\mathcal{E}$ conditioned on generated
    vertices as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示法被 CurveGen 使用，它是一个基于 PolyGen 的自回归 Transformer [[106](#bib.bib106)]。PolyGen
    是一种自回归生成方法，用于利用 Transformer 架构生成 3D 网格，能够捕捉长程依赖。网格顶点由 Transformer 无条件建模，网格面则通过
    Transformer 和指针网络的组合以网格顶点为条件建模 [[107](#bib.bib107)]。与 PolyGen 类似，CurveGen 也直接生成草图超图表示，首先生成用于绘制曲线的图顶点
    $\mathcal{V}$，然后基于生成的顶点生成图超边 $\mathcal{E}$，具体如下：
- en: '|  | $p(\mathcal{G})=p(\mathcal{E}&#124;\mathcal{V})p(\mathcal{V}).$ |  | (2)
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathcal{G})=p(\mathcal{E} \mid \mathcal{V})p(\mathcal{V}).$ |  | (2)
    |'
- en: 'In this way, the network predicts the precise coordinates of each primitive
    while the type of primitive is implicitly encoded in the hyperedges which group
    vertices together to make different types of primitives. Therefore, this method
    is independent of any constraint solver for finding the final primitives configuration.
    By having the precise coordinates of the sketch curves, the constraints between
    them can be automatically obtained as a post-processing step. However, implicit
    inference of constraints makes editing the sketch in the software more difficult.
    If the designer wants to change one of the constraints between the primitives,
    for example scale of a distance, this change will not propagate through the whole
    sketch and primitives, because the exact positioning of the primitives are somehow
    fixed. In the Turtle Graphics representation, the sketch is represented by a sequence
    of drawing commands, pen-down, pen-draw, pen-up, which can be executed to form
    an engineering sketch in the hypergraph representation. The TurtleGen network
    is an autoregressive Transfromer model generating a sequence of drawing commands
    to iteratively draw a series of closed loops forming the engineering sketch. In
    this way, each sketch is represented as a sequence of loops, and each loop is
    made by a LoopStart command which lifts the pen, displace it to the specified
    position, put it down, and starts drawing parametric curves specified by the Draw
    command. In Figure [9](#S5.F9 "Figure 9 ‣ V-A2 CurveGen-TurtleGen [21] ‣ V-A Engineering
    2D Sketch Generation for CAD ‣ V CAD Construction with Generative Deep Learning
    ‣ Geometric Deep Learning for Computer-Aided Design: A Survey"), a simple sketch
    with its corresponding command sequence is illustrated. The sketch encompasses
    $2$ loops and $9$ Draw commands, including arc, line, and circle types. The initial
    loop starts at a position $\Delta=(int,int)$ and comprises $4$ arcs. Each arc
    originates from the endpoint of the preceding arc, passing through $2$ additional
    vertices. This loop transitions into a sequence of $4$ lines. The first line begins
    where the last arc ended, traversing one more vertex, and this pattern repeats
    for the subsequent lines. The second loop is a circle that begins at another position
    $\Delta$ and passes through $3$ additional vertices. CurveGen and TurtleGen have
    been evaluated on the SketchGraphs dataset, demonstrating superior performance
    compared to the generative models proposed in SketchGraphs. We refer the reader
    to [[21](#bib.bib21)] for further details on models’ architecture, training and
    evaluation settings.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '通过这种方式，网络预测每个原始图形的精确坐标，而原始图形的类型则隐式地编码在超边中，这些超边将顶点组合在一起形成不同类型的原始图形。因此，这种方法不依赖于任何约束求解器来找到最终的原始图形配置。通过获得草图曲线的精确坐标，可以在后处理步骤中自动获得它们之间的约束。然而，隐式推断约束使得在软件中编辑草图变得更加困难。例如，如果设计师想要改变原始图形之间的某个约束，比如距离的尺度，这种变化不会在整个草图和原始图形中传播，因为原始图形的精确位置在某种程度上是固定的。在
    Turtle Graphics 表示中，草图由一系列绘图命令组成，如“笔落下”、“笔绘制”和“笔抬起”，这些命令可以执行以在超图表示中形成工程草图。TurtleGen
    网络是一个自回归的 Transformer 模型，生成一系列绘图命令，迭代地绘制一系列封闭的循环，形成工程草图。通过这种方式，每个草图被表示为一个循环序列，每个循环由一个
    LoopStart 命令组成，该命令提起笔，将其移动到指定位置，放下，然后开始绘制由 Draw 命令指定的参数曲线。在图 [9](#S5.F9 "Figure
    9 ‣ V-A2 CurveGen-TurtleGen [21] ‣ V-A Engineering 2D Sketch Generation for CAD
    ‣ V CAD Construction with Generative Deep Learning ‣ Geometric Deep Learning for
    Computer-Aided Design: A Survey") 中，展示了一个简单的草图及其对应的命令序列。该草图包含 $2$ 个循环和 $9$ 个 Draw
    命令，包括弧线、直线和圆类型。初始循环从位置 $\Delta=(int,int)$ 开始，包括 $4$ 个弧线。每个弧线从前一个弧线的终点开始，经过 $2$
    个额外的顶点。这个循环过渡成 $4$ 条直线。第一条直线从最后一个弧线结束的地方开始，再经过一个顶点，这个模式在随后的直线中重复。第二个循环是一个圆形，从另一个位置
    $\Delta$ 开始，经过 $3$ 个额外的顶点。CurveGen 和 TurtleGen 已在 SketchGraphs 数据集上进行了评估，显示出比
    SketchGraphs 中提出的生成模型更优越的性能。有关模型架构、训练和评估设置的更多细节，请参见 [[21](#bib.bib21)]。'
- en: V-A3 SketchGen [[26](#bib.bib26)]
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A3 SketchGen [[26](#bib.bib26)]
- en: 'Concurrent to CurveGen-TurtleGen, SketchGen, which is an autoregressive generative
    method, proposed based on PolyGen [[106](#bib.bib106)] and pointer networks [[107](#bib.bib107)].
    Unlike PolyGen, SketchGen aims to capture the heterogeneity of the primitives
    and constraints in sketches, where each type of primitive and constraint might
    have a different number of parameters, each of different type, and therefore have
    a representation of a different size. The selection of input sequence representation
    has a great impact on the performance of the Transformers, and transforming heterogeneous
    constraint graphs of sketches into an appropriate sequence of tokens is considerably
    challenging. One simple solution is padding all the primitives’ and constraints’
    representations to make them of the same size. However, this technique is inefficient
    and inaccurate for complex sketches. SketchGen proposed a language with a simple
    syntax to describe heterogeneous constraint graphs effectively. The proposed language
    for CAD sketches encodes the constraints and primitives parameters using a formal
    grammar. The terminal symbols for encoding the type and parameters of primitives
    are $\left\{\Lambda,\Omega,\tau,\kappa,x,y,u,v,a,b\right\}$ and for constraints
    are $\left\{\Lambda,\nu,\lambda,\mu,\Omega\right\}$. The start and end of a new
    primitive or constraint sequence are marked with $\Lambda$ and $\Omega$, respectively.
    The primitive type is denoted with $\tau$, and $\nu$ shows the the constraint
    type. $\kappa,x,y,u,v,a,b$ show the specific parameters for each primitive, such
    as coordinate and direction. $\lambda$, and $\mu$ are the specific constraint
    parameters indicating the primitive reference of the constraint and the part of
    the primitive it is targeting, respectively. This formal language, enables distinguishing
    different primitive or constraint types along with their respective parameters.
    For example, the sequence for a line primitive is $\Lambda,\tau,\kappa,x,y,u,v,a,b,\Omega$,
    which starts with $\Lambda$, followed by the primitive type $\tau$ = line, the
    construction indicator $\kappa$, the coordinates of the starting point $x$ and
    $y$, the line direction $u$ and $v$, the line range $a$, $b$, and ends with $\Omega$.
    The sequence for a parallelism constraint is $\Lambda,\nu,\lambda_{1},\mu_{1},\lambda_{2},\mu_{2},\Omega$
    which starts with $\Lambda$, followed by the constraint type $\nu$ = parallelism,
    the reference to the first primitive $\lambda_{1}$, the part of the first primitive
    $\mu_{1}$, the reference to the second primitive $\lambda_{2}$, the part of the
    second primitive $\mu_{2}$, and ends with $\Omega$. In this way, each token $q_{i}$
    (a sequence of symbols) represents either a primitive or a constraint, and each
    sketch $Q$ is represented as a sequence of tokens. Similar to PolyGen and CurveGen-TurtleGen,
    as stated in Eq. ([2](#S5.E2 "In V-A2 CurveGen-TurtleGen [21] ‣ V-A Engineering
    2D Sketch Generation for CAD ‣ V CAD Construction with Generative Deep Learning
    ‣ Geometric Deep Learning for Computer-Aided Design: A Survey")), the generative
    model of SketchGen is also decomposed into two parts by first generating the primitives
    $p(\mathcal{P})$ and then generating constraints conditioned on primitives $p(\mathcal{C}|\mathcal{P})p(\mathcal{P})$
    as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '与 CurveGen-TurtleGen 并行的，SketchGen 是一种基于 PolyGen [[106](#bib.bib106)] 和指针网络
    [[107](#bib.bib107)] 提出的自回归生成方法。与 PolyGen 不同，SketchGen 旨在捕捉草图中原始数据和约束的异质性，其中每种原始数据和约束可能具有不同数量的参数，每种参数类型不同，因此具有不同大小的表示。输入序列表示的选择对
    Transformers 的性能有很大影响，将异质的草图约束图转换为合适的标记序列是相当具有挑战性的。一种简单的解决方案是将所有原始数据和约束的表示填充为相同的大小。然而，这种技术对于复杂的草图既低效又不准确。SketchGen
    提出了一个具有简单语法的语言，以有效描述异质的约束图。针对 CAD 草图提出的语言使用形式化语法对约束和原始数据参数进行编码。用于编码原始数据类型和参数的终结符为
    $\left\{\Lambda,\Omega,\tau,\kappa,x,y,u,v,a,b\right\}$，用于约束的为 $\left\{\Lambda,\nu,\lambda,\mu,\Omega\right\}$。新的原始数据或约束序列的开始和结束分别用
    $\Lambda$ 和 $\Omega$ 标记。原始数据类型用 $\tau$ 表示，$\nu$ 显示约束类型。$\kappa,x,y,u,v,a,b$ 显示每个原始数据的具体参数，如坐标和方向。$\lambda$
    和 $\mu$ 是表示约束的具体参数，分别指示约束的原始数据参考和约束所针对的原始数据部分。这种形式化语言使得能够区分不同的原始数据或约束类型及其各自的参数。例如，线段原始数据的序列为
    $\Lambda,\tau,\kappa,x,y,u,v,a,b,\Omega$，以 $\Lambda$ 开始，接着是原始数据类型 $\tau$ = line，构造指示符
    $\kappa$，起点坐标 $x$ 和 $y$，线段方向 $u$ 和 $v$，线段范围 $a$，$b$，最后以 $\Omega$ 结束。平行约束的序列为 $\Lambda,\nu,\lambda_{1},\mu_{1},\lambda_{2},\mu_{2},\Omega$，以
    $\Lambda$ 开始，接着是约束类型 $\nu$ = parallelism，第一个原始数据的参考 $\lambda_{1}$，第一个原始数据的部分 $\mu_{1}$，第二个原始数据的参考
    $\lambda_{2}$，第二个原始数据的部分 $\mu_{2}$，最后以 $\Omega$ 结束。这样，每个标记 $q_{i}$（符号序列）表示一个原始数据或约束，每个草图
    $Q$ 表示为一个标记序列。与 PolyGen 和 CurveGen-TurtleGen 相似，如 Eq. ([2](#S5.E2 "In V-A2 CurveGen-TurtleGen
    [21] ‣ V-A Engineering 2D Sketch Generation for CAD ‣ V CAD Construction with
    Generative Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey")) 中所述，SketchGen 的生成模型也被分解为两个部分，首先生成原始数据 $p(\mathcal{P})$，然后生成基于原始数据的约束
    $p(\mathcal{C}|\mathcal{P})p(\mathcal{P})$。'
- en: '|  | $p(\mathcal{S})=p(\mathcal{C}&#124;\mathcal{P})p(\mathcal{P}).$ |  | (3)
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathcal{S})=p(\mathcal{C}&#124;\mathcal{P})p(\mathcal{P}).$ |  | (3)
    |'
- en: 'Therefore, the generative network learns the distribution of constraint sketches
    via two autoregressive Transformers, one for primitive generation, and the other
    for conditional constraint generation. The sketch is parsed into a sequence of
    tokens, by initially iterating through all the primitives and expressing them
    using the language sequences explained above. Subsequently, a similar process
    is applied to represent all the constraints within the sketch. As illustrated
    in Figure [10](#S5.F10 "Figure 10 ‣ V-A3 SketchGen [26] ‣ V-A Engineering 2D Sketch
    Generation for CAD ‣ V CAD Construction with Generative Deep Learning ‣ Geometric
    Deep Learning for Computer-Aided Design: A Survey"), the input of the primitive
    generator network is a sequence of concatenated primitive tokens seperated by
    $\Lambda$, such as $\Lambda,\tau_{1},\kappa_{1},x_{1},y_{1},u_{1},v_{1},a_{1},b_{1},\Lambda,\tau_{2},\kappa_{2},x_{2},y_{2},...,\Omega$,
    and the input of the constraint generator is a sequence of concatenated constraint
    tokens seperated by $\Lambda$, like $\Lambda,\nu_{1},\lambda_{11},\mu_{11},\lambda_{12},\mu_{12},\Lambda,\nu_{2},\lambda_{21},....\Omega$.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，生成网络通过两个自回归 Transformer 来学习约束草图的分布，一个用于原始生成，另一个用于条件约束生成。草图被解析为一系列令牌，首先遍历所有原始对象，并使用上述语言序列表达它们。随后，类似的过程应用于表示草图中的所有约束。如图
    [10](#S5.F10 "Figure 10 ‣ V-A3 SketchGen [26] ‣ V-A Engineering 2D Sketch Generation
    for CAD ‣ V CAD Construction with Generative Deep Learning ‣ Geometric Deep Learning
    for Computer-Aided Design: A Survey") 所示，原始生成器网络的输入是由 $\Lambda$ 分隔的原始令牌序列，例如 $\Lambda,\tau_{1},\kappa_{1},x_{1},y_{1},u_{1},v_{1},a_{1},b_{1},\Lambda,\tau_{2},\kappa_{2},x_{2},y_{2},...,\Omega$，而约束生成器的输入是由
    $\Lambda$ 分隔的约束令牌序列，例如 $\Lambda,\nu_{1},\lambda_{11},\mu_{11},\lambda_{12},\mu_{12},\Lambda,\nu_{2},\lambda_{21},....\Omega$。'
- en: All the primitive and constraint parameters are quantized first and then mapped
    by an embedding layer to make the input feature vectors for the network. The positional
    information of the tokens in the sequence is also captured by a positional encoding
    added to each embedding vector. The resulting sequences of tokens are then fed
    into the Transformer architectures to generate primitives and constraints. The
    constraint generator network not only receives the embedded and positionally encoded
    tokens for constraints as input, but also it receives the embedded and positionally
    encoded sequence of tokens that represents the primitives generated in the previous
    step, to generate constraints conditioned on primitives. Similar to [[20](#bib.bib20)],
    this model is evaluated on SketchGraphs dataset for the two tasks of constraint
    prediction given sketch primitives, and full sketch generation from scratch by
    generating both primitives and constraints sequentially. The final generated sketch
    needs to be regularized by a constraint solver to remove the potential errors
    caused by quantizing the sketch parameters.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 所有原始和约束参数首先被量化，然后通过嵌入层映射以形成网络的输入特征向量。序列中令牌的位置信息也通过添加到每个嵌入向量的位置信息编码来捕获。生成的令牌序列随后被输入到
    Transformer 架构中以生成原始对象和约束。约束生成网络不仅接收用于约束的嵌入和位置编码令牌作为输入，还接收表示在前一步生成的原始对象的嵌入和位置编码令牌序列，以生成基于原始对象的约束。类似于
    [[20](#bib.bib20)]，该模型在 SketchGraphs 数据集上进行评估，任务包括给定草图原始对象的约束预测以及从头开始生成完整草图，通过顺序生成原始对象和约束。最终生成的草图需要通过约束求解器进行正则化，以去除由于量化草图参数而可能导致的潜在错误。
- en: '![Refer to caption](img/193f8971149083453a68412a12162f3d.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/193f8971149083453a68412a12162f3d.png)'
- en: 'Figure 10: A simple illustration of the SketchGen generative approach, comprising
    two generative networks for primitive and constraint generation. At each generation
    step, the network produces the next token based on both the input and the previously
    generated token. The start and end of a new primitive or constraint sequence are
    denoted by $\Lambda$ and $\Omega$, respectively. The figure showcases the parameters
    of a line primitive and a parallelism constraint as an example. More details about
    the model structure can be found in the original paper [[26](#bib.bib26)].'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：SketchGen 生成方法的简单示意图，包括用于原始对象和约束生成的两个生成网络。在每次生成步骤中，网络基于输入和之前生成的令牌生成下一个令牌。新原始对象或约束序列的开始和结束分别由
    $\Lambda$ 和 $\Omega$ 表示。图中展示了一个线性原始对象和一个平行约束的参数作为示例。有关模型结构的更多细节，请参见原始论文 [[26](#bib.bib26)]。
- en: V-A4 CAD as Language [[27](#bib.bib27)]
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A4 CAD 作为语言 [[27](#bib.bib27)]
- en: Concurrent to CurveGen-TurtleGen and SketchGen, CAD as Language is another autoregressive
    Transformer method proposed for 2D sketch generation based on PolyGen [[106](#bib.bib106)].
    Unlike CurveGen-TurtleGen that predicts only the primitive parameters with implicit
    constraints and independent of constraint solver, CAD as Language and SketchGen
    methods generate both primitives and constraints but are dependent to a built-in
    constraint solver to obtain the final configuration of the sketch. However, SketchGen
    produces primivies and constrains via two separate Transformer networks while
    do not support arbitrary orderings of primitive and constraint tokens. In SketchGen,
    the sketch is parsed into a sequence of tokens, by initially iterating through
    all the primitives and then iterating through all the constraints. CAD as Language
    method, not only handles the arbitrary orderings of primitives and constraints,
    but also generates both primitives an constraint via one Transformer network.
    Unlike all the previous sketch generation methods which are evaluated on SketchGraphs
    dataset, CAD as Language evaluated its generative method on a new collection of
    over $4.7$ million sketches from the OnShape platform which avoids the problem
    of data redundancy in SketchGraphs. This collected dataset and the corresponding
    processing pipeline are publicly available on Github.⁵⁵5https://github.com/google-deepmind/deepmind-research/tree/master/cadl
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CurveGen-TurtleGen 和 SketchGen 并行，CAD 作为语言是另一种基于 PolyGen [[106](#bib.bib106)]
    的自回归 Transformer 方法，用于 2D 草图生成。与 CurveGen-TurtleGen 仅预测隐式约束的原始参数且独立于约束求解器不同，CAD
    作为语言和 SketchGen 方法生成原始体和约束，但依赖于内置的约束求解器来获得草图的最终配置。然而，SketchGen 通过两个独立的 Transformer
    网络生成原始体和约束，并且不支持原始体和约束标记的任意排序。在 SketchGen 中，草图被解析为一系列标记，首先遍历所有原始体，然后遍历所有约束。而 CAD
    作为语言的方法不仅处理原始体和约束的任意排序，而且通过一个 Transformer 网络生成原始体和约束。与所有先前在 SketchGraphs 数据集上评估的草图生成方法不同，CAD
    作为语言在来自 OnShape 平台的超过 $4.7$ 百万个草图的新数据集上评估了其生成方法，从而避免了 SketchGraphs 数据集中的数据冗余问题。该数据集及相应处理流程在
    Github 上公开可用。⁵⁵5https://github.com/google-deepmind/deepmind-research/tree/master/cadl
- en: 'CAD as Language uses a method for describing structured objects using Protocol
    Buffers (PB) [[108](#bib.bib108)], which demonstrates more efficiency and flexibility
    for representing the precise structure of complex objects than JSON format. In
    this format, each sketch is described as a PB message. Similar to other Transformer-based
    methods, the first and most important step in the processing pipeline is to parse
    sketches to a sequence of tokens. In this method, each sketch (or PB message)
    is represented as a sequence of triplets $(d_{i},c_{i},f_{i})$, where each triplet
    with index $i$ denotes a token. Each token (triplet) represents only one component
    (type or parameter) of a primitive or constraint in a sketch, where $d_{i}$ is
    a discrete value denoting the type of object it is referring to, the type of entity
    (primitive, constraint, etc), $c_{i}$ is a continuous value denoting the parameter
    value for the corresponding entity. At each time, either $d_{i}$ or $c_{i}$ is
    active and gets a value, and the other one is set to zero. $f_{i}$ is a boolean
    flag specifying the end of a repeated token (for example the end of an object
    containing an entity). [11](#S5.F11 "Figure 11 ‣ V-A4 CAD as Language [27] ‣ V-A
    Engineering 2D Sketch Generation for CAD ‣ V CAD Construction with Generative
    Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design: A Survey")
    shows an example of tokens specifying a line and a point primitive on one of its
    ends, respectively.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'CAD 作为语言使用了一种方法，通过 Protocol Buffers (PB) [[108](#bib.bib108)] 来描述结构化对象，这种方法在表示复杂对象的精确结构方面比
    JSON 格式更高效、更灵活。在这种格式中，每个草图被描述为一个 PB 消息。类似于其他基于 Transformer 的方法，处理流程中的第一步也是最重要的一步是将草图解析为一系列的标记。在这种方法中，每个草图（或
    PB 消息）被表示为一系列的三元组 $(d_{i},c_{i},f_{i})$，其中每个三元组的索引 $i$ 表示一个标记。每个标记（三元组）仅表示草图中原始对象或约束的一个组件（类型或参数），其中
    $d_{i}$ 是一个离散值，表示它所指代的对象类型、实体类型（原始体、约束等），$c_{i}$ 是一个连续值，表示对应实体的参数值。在每个时刻，$d_{i}$
    或 $c_{i}$ 之一是激活的并获得一个值，另一个被设为零。$f_{i}$ 是一个布尔标志，指定重复标记的结束（例如，包含实体的对象的结束）。[11](#S5.F11
    "Figure 11 ‣ V-A4 CAD as Language [27] ‣ V-A Engineering 2D Sketch Generation
    for CAD ‣ V CAD Construction with Generative Deep Learning ‣ Geometric Deep Learning
    for Computer-Aided Design: A Survey") 显示了分别指定线和点原始体的标记示例。'
- en: '![Refer to caption](img/1e3d5b68028fb1323204131f72d183d5.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1e3d5b68028fb1323204131f72d183d5.png)'
- en: 'Figure 11: The description of a simple sketch, consisting of a line and one
    point on one of its ends, using the language structure proposed by [[27](#bib.bib27)].
    The active element of each triplet (on left side) is specified in bold red color,
    and the corresponding field of the object for each triplet is shown on the right
    side.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：使用[[27](#bib.bib27)]提出的语言结构描述一个简单的草图，包括一条线和其一端的一个点。每个三元组的活动元素（左侧）用粗体红色标出，每个三元组对应的对象字段显示在右侧。
- en: As it is shown in this example, the first triplet (objects.kind) is always associated
    with the type of the object the token referring to. The values in the second triplet
    depends on the type of object specified in the first triplet. As in this example,
    $d_{1}=0$ shows that this sequence of tokens is about creating a primitive (like
    a line), therefore the second triplet specifies the type of the primitive (entity.kind)
    which is $0$ for line, and $1$ for point. The next triplets in the sequence specify
    the specific parameters associated with the corresponding primitive identified
    in the second triplet. For example, line primitive is defined with a start and
    ending point, while the point primitive, which is also repeated in the line primitive,
    is defined with $x$, $y$ coordinates.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子所示，第一个三元组（objects.kind）总是与token所指的对象类型相关。第二个三元组中的值取决于第一个三元组中指定的对象类型。正如这个例子中，$d_{1}=0$表明这个token序列是关于创建一个原始体（如一条线），因此第二个三元组指定了原始体的类型（entity.kind），其中$0$表示线，$1$表示点。序列中的下一个三元组指定了与第二个三元组中识别出的原始体相关的具体参数。例如，线原始体通过起点和终点定义，而点原始体（也在线原始体中重复）则通过$x$，$y$坐标定义。
- en: For interpreting these triplets (tokens), CAD as Language method also proposed
    a custom interpreter which receives as input a sequence of tokens, each representing
    a sketch component (which can be an entity type, parameter or any other design
    step), and converts it into a valid PB message. This interpreter is designed in
    a way to handle arbitrary orderings of tokens and make sure that all the token
    sequences can be converted into a valid PB message (sketch). This interpreter
    guides the Transformer network through the sketch generation process. The Transformer
    network receives as input a sequence of tokens, and at each time step it outputs
    a raw value which is passed to the interpreter to infer the corresponding triplet
    for that value. This triplet is a part of a PB message which makes the final sketch.
    When the interpreter inferred the output value, it propagates its interpretation
    back to the Transformer to guide it through generating the next value. Therefore,
    the structure this method proposed for parsing a sketch into tokens and interpreting
    them, enables it to generate every sketch component (primitives and constraints)
    via only one Transformer network, while handling different orderings of input
    tokens. A conditional variant of the proposed Transformer model is also explored
    and evaluated in this method, where it is conditioned on an input image of the
    sketch.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释这些三元组（tokens），CAD作为语言的方法还提出了一个自定义解释器，该解释器接收一系列tokens作为输入，每个token代表一个草图组件（可以是实体类型、参数或任何其他设计步骤），并将其转换为有效的PB消息。该解释器设计成能够处理任意顺序的tokens，并确保所有的token序列都能转换为有效的PB消息（草图）。这个解释器引导Transformer网络完成草图生成过程。Transformer网络接收一系列tokens作为输入，并在每个时间步骤输出一个原始值，该值传递给解释器以推断该值对应的三元组。这个三元组是PB消息的一部分，构成最终草图。当解释器推断出输出值时，它将其解释传递回Transformer，以指导它生成下一个值。因此，这种方法为解析草图为tokens并解释它们所提出的结构，使其能够通过一个Transformer网络生成每个草图组件（原始体和约束），同时处理不同顺序的输入tokens。该方法还探索并评估了提出的Transformer模型的一个条件变体，其中它以草图的输入图像为条件。
- en: V-A5 VITRUVION [[23](#bib.bib23)]
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A5 VITRUVION [[23](#bib.bib23)]
- en: This method is the latest autoregressive generative model proposed for sketch
    generation. Similar to SketchGen and CAD as Language, VITROVION also generates
    both primitives and constraints autoregressively. However, this method is more
    similar to SketchGen in a sense that primitives and constraints are generated
    independently through training two distinct Transformer networks. The major contribution
    of this method compared to the previous ones, is conditioning the model on various
    contexts such as hand-drawn sketches. This contribution is one step forward towards
    the highly-sought feature in CAD softwares to reverse engineer a mechanical part
    given a hand-drawing or a noisy scan of it. Generating parametric primitives and
    constraints of a sketch given a hand-drawing or noisy image saves a lot of time
    and effort in designing process.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是最新提出的自回归生成模型，用于草图生成。与 SketchGen 和 CAD as Language 类似，VITROVION 也自回归地生成原语和约束。然而，这种方法在某种程度上更类似于
    SketchGen，因为原语和约束是通过训练两个不同的 Transformer 网络独立生成的。与之前的方法相比，这种方法的主要贡献是将模型条件化于各种上下文，例如手绘草图。这一贡献是朝着
    CAD 软件中非常期待的功能迈出的一步，即在给定手绘图或其噪声扫描的情况下，反向工程一个机械部件。根据手绘图或噪声图像生成参数化原语和约束可以节省大量的设计时间和精力。
- en: 'Similar to the previous methods, VITROVION is a generalization of PolyGen [[106](#bib.bib106)].
    It generates propability distribution of sketches by first generating primitives
    $\mathcal{P}$, and then generating constraints $\mathcal{C}$ conditioned on primitives.
    However, in this method, the primitive generation is optionally conditioned on
    an image follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与前述方法类似，VITROVION 是 PolyGen 的一种泛化 [[106](#bib.bib106)]。它通过首先生成原语 $\mathcal{P}$，然后生成以原语为条件的约束
    $\mathcal{C}$ 来生成草图的概率分布。然而，在这种方法中，原语的生成是可选地以图像为条件的，如下所示：
- en: '|  | $p(\mathcal{P},\mathcal{C}&#124;I)=p(\mathcal{C}&#124;\mathcal{P})p(\mathcal{P}&#124;\mathcal{I}),$
    |  | (4) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathcal{P},\mathcal{C}&#124;I)=p(\mathcal{C}&#124;\mathcal{P})p(\mathcal{P}&#124;\mathcal{I}),$
    |  | (4) |'
- en: 'where $\mathcal{I}$ is a context such as hand-drawn image of a sketch. However,
    constraint modeling in this method only supports constraints with one or two reference
    primitives, and not hyperedges connecting more than two primitives as in CurveGen-TurtleGen
    method. The generative models are trained and evaluated on SketchGraphs dataset
    for autoconstraint, autocomplete, and conditional sketch synthesis tasks. As explained
    in [V-A1](#S5.SS1.SSS1 "V-A1 SketchGraphs [20] ‣ V-A Engineering 2D Sketch Generation
    for CAD ‣ V CAD Construction with Generative Deep Learning ‣ Geometric Deep Learning
    for Computer-Aided Design: A Survey"), in autoconstraint application the network
    generates constraints conditioning on a set of available primitives, but in autocomplete
    task, an incomplete sketch is completed by generating both primitives and constraints.
    In both cases, the constraint generator network is conditioned on generated primitives
    which can be imperfect. VITROVION increases the robustness of the constraint generator
    network by conditioning it on noise-injected primitives. The final primitive and
    constraint parameters are finally adjusted via a standard constraint solver. For
    image-conditional sketch synthesis, the model infers primitives conditioning on
    a raster-image of a hand-drawn sketch. In this regard, an encoder network based
    on Vision Transformer [[109](#bib.bib109)] architecture is used to obtain the
    embeddings of the image patches, and the primitive generator network then cross-attends
    to these patch embeddings for predicting sketch primitives. This idea is based
    on a similar idea in PolyGen for image-conditioned mesh generation.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathcal{I}$ 是一个上下文，例如手绘草图图像。然而，这种方法中的约束建模仅支持具有一个或两个参考原语的约束，而不支持连接多个原语的超边，如
    CurveGen-TurtleGen 方法所示。生成模型在 SketchGraphs 数据集上进行训练和评估，用于自动约束、自动补全和条件草图合成任务。如
    [V-A1](#S5.SS1.SSS1 "V-A1 SketchGraphs [20] ‣ V-A Engineering 2D Sketch Generation
    for CAD ‣ V CAD Construction with Generative Deep Learning ‣ Geometric Deep Learning
    for Computer-Aided Design: A Survey") 中解释，在自动约束应用中，网络在一组可用的原语上生成约束，而在自动补全任务中，不完整的草图通过生成原语和约束来完成。在这两种情况下，约束生成网络都以生成的原语为条件，而这些原语可能是不完美的。VITROVION
    通过将其条件化于注入噪声的原语来增加约束生成网络的鲁棒性。最终的原语和约束参数通过标准约束求解器进行调整。对于图像条件的草图合成，模型根据手绘草图的光栅图像推断原语。在这方面，使用基于
    Vision Transformer [[109](#bib.bib109)] 架构的编码器网络来获得图像补丁的嵌入，原语生成网络然后跨注意这些补丁嵌入以预测草图原语。这个想法基于
    PolyGen 在图像条件网格生成中的类似想法。'
- en: 'This method tokenizes a sketch by representing each primitive and constraint
    as a tuple of three tokens: value, ID, position. The value token has two-folds,
    one indicating the type of primitive or constraint and the other indicating the
    numerical value of the associated parameter of that primitive or constraint. ID
    token indicates the type of parameter specified by the value token, and position
    token indicates the ordered index of the primitive or constraint to which this
    ID and value tokens belong to. The ordering of primitives are according to design
    steps indicated in SketchGraphs dataset, and the ordering of constraints are according
    to the ordering of their corresponding reference primitives.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法通过将每个原始图形和约束表示为三个令牌的元组：值、ID、位置，来对草图进行标记化。值令牌有两个部分，一个表示原始图形或约束的类型，另一个表示该原始图形或约束的相关参数的数值。ID
    令牌表示值令牌指定的参数类型，而位置令牌表示该 ID 和值令牌所属的原始图形或约束的有序索引。原始图形的排序依据 SketchGraphs 数据集中所示的设计步骤，而约束的排序则依据其对应的参考原始图形的排序。
- en: V-B 3D CAD Generation from Sketch
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 3D CAD 从草图生成
- en: V-B1 Sketch2CAD [[24](#bib.bib24)]
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 Sketch2CAD [[24](#bib.bib24)]
- en: The first work proposed for sequential 3D CAD modeling by interactive sketching
    in context is Sketch2CAD. This work is a learning-based interactive modeling system
    that unifies CAD modeling and sketching by interpreting the user’s input sketches
    as a sequence of parametric 3D CAD operations. Given an existing incomplete 3D
    shape and input sketch strokes added on it by the user, Sketch2CAD first obtains
    the normal and depth maps of sketching local context which are introduced to the
    CAD operator classification and segmentation networks. Classification is done
    by a CNN network predicting the type of CAD operation needed for creating the
    corresponding input sketch on the 3D shape. It receives as input the concatenation
    of three maps, each of size $256\times 256$, i.e., the sketching map representing
    stroke pixels with binary values, and normal and depth maps representing the local
    context via rendering the shape through a specific viewpoint. It should be noted
    that only four types of CAD operations, which are the most widely used ones, are
    supported in this method, i.e., Extrude, Add/Subtract, Bevel, Sweep. According
    to the predicted operation type, the parameters of the operator are regressed
    via specific segmentation networks. For example, for the predicted Sweep operator,
    a SweepNet is trained to infer the corresponding parameters. Each of the four
    segmentation networks have U-Net structure with one encoder and two decoders,
    one producing the probability map of base face for the sketch and the other one
    producing the corresponding curve segmentation map. The segmentation is followed
    by an optimization process for fitting the operation parameters.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首个在上下文中通过交互式草图进行顺序 3D CAD 建模的工作是 Sketch2CAD。该工作是一个基于学习的交互式建模系统，通过将用户输入的草图解释为一系列参数化的
    3D CAD 操作来统一 CAD 建模和草图绘制。给定一个现有的不完整的 3D 形状以及用户在其上添加的输入草图，Sketch2CAD 首先获取草图局部上下文的法线图和深度图，这些图像被引入
    CAD 操作员分类和分割网络。分类由 CNN 网络完成，该网络预测创建对应输入草图所需的 CAD 操作类型。它的输入是三个图像的拼接，每个图像大小为 $256\times
    256$，即表示草图像素的二值值草图图、以及通过特定视角渲染形状的法线图和深度图。需要注意的是，该方法仅支持四种最常用的 CAD 操作，即挤出、加法/减法、倒角、扫描。根据预测的操作类型，通过特定的分割网络对操作员的参数进行回归。例如，对于预测的扫描操作员，训练了一个
    SweepNet 来推断相应的参数。四个分割网络都具有 U-Net 结构，包括一个编码器和两个解码器，一个生成草图的基础面的概率图，另一个生成相应的曲线分割图。分割后是一个优化过程，用于拟合操作参数。
- en: The parametric nature of the recognized operations provides strong regularization
    to approximate input strokes and allows users to refine the result by adjusting
    and regressing parameters. The modeling system also incorporates standard CAD
    modeling features such as automatic snapping and autocompletion. The output of
    the system is a series of CAD instructions ready to be processed by downstream
    CAD tools. Since no dataset of paired sketches and 3D CAD modeling operation sequences
    exists, Sketch2CAD introduced a synthetic dataset of $40,000$ shapes for training
    and $10,000$ shapes for testing, containing step-by-step CAD operation sequences
    and their corresponding sketch image renderings. The dataset, code, and visual
    examples are publicly available.⁶⁶6https://geometry.cs.ucl.ac.uk/projects/2020/sketch2cad/
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 识别操作的参数化特性为近似输入笔画提供了强大的正则化，并允许用户通过调整和回归参数来细化结果。建模系统还包括标准的 CAD 建模功能，如自动对齐和自动补全。系统的输出是一系列准备好供下游
    CAD 工具处理的 CAD 指令。由于没有配对的草图和 3D CAD 建模操作序列的数据集，Sketch2CAD 引入了一个合成数据集，其中包含 40,000
    个形状用于训练，10,000 个形状用于测试，包含逐步的 CAD 操作序列及其相应的草图图像渲染。数据集、代码和视觉示例是公开的。⁶⁶6https://geometry.cs.ucl.ac.uk/projects/2020/sketch2cad/
- en: V-B2 Free2CAD [[22](#bib.bib22)]
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 Free2CAD [[22](#bib.bib22)]
- en: One of the main challenges of Sketch2CAD is that it can only handle one CAD
    operation at a time, which means that it assumes that at each step, the user drawing
    added to the shape corresponds to only one CAD operation. It requires the user
    to draw the sketch sequentially and part-by-part so that it can be decomposed
    into the meaningful CAD operations. Free2CAD is proposed to address this restriction.
    This is the first sketch-based modeling method in which the user can input the
    complete drawing of a complex 3D shape without needing to have expert knowledge
    of how this sketch should be decomposed into the CAD operations or following a
    specific strategy for drawing the sketch and working with the system.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Sketch2CAD 的主要挑战之一是它一次只能处理一个 CAD 操作，这意味着它假设在每一步中，用户绘制的形状只对应一个 CAD 操作。它要求用户按顺序、逐部分绘制草图，以便可以将其分解为有意义的
    CAD 操作。为了解决这一限制，提出了 Free2CAD。这是第一个基于草图的建模方法，用户可以输入复杂 3D 形状的完整绘图，而无需掌握如何将此草图分解为
    CAD 操作的专家知识或遵循特定的绘图策略和系统操作。
- en: Free2CAD is a sequence-to-sequence Transformer network which receives as input
    a sequence of user drawn strokes depicting a 3D shape, processes and analyzes
    them to produce a valid sequence of CAD operation commands, which may be executed
    to create the CAD model. The main contribution of this method is the automatic
    grouping of the sketch strokes and the production of the parametric CAD operations
    for each group of strokes sequentially, conditioned on the groups that have been
    reconstructed in the previous iterations. The method is comprised of two phases,
    namely stroke grouping phase and operation reconstruction phase. In the stroke
    grouping phase, first each sketch stroke is embedded as a token via a specially
    designed Transformer encoder network, then it is processed by the Transformer
    decoder network which produces group probabilities for the input tokens. In this
    way, the sketch strokes which might make a specific part of the shape are grouped
    together. The most closely related work to the stroke grouping phase is the SketchGNN
    [[56](#bib.bib56)] method which proposes a Graph Neural Network approach for freehand
    sketch semantic segmentation. Next, in the operation reconstruction phase, the
    candidate groups are converted into geometric primitives with their corresponding
    parameters, conditioned on the existing geometric context. This step is followed
    by geometric fitting and grouping correction before passing back the updated groups
    as geometric context for the next iteration. At the end of this process, the desired
    CAD shape and the sequence of CAD commands are obtained. The method is also extended
    to handle long stroke sequences making complex shapes using a sliding window scheme
    progressively outputting CAD models.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Free2CAD 是一个序列到序列的 Transformer 网络，它接收用户绘制的描绘 3D 形状的笔画序列作为输入，处理和分析这些笔画，以生成一系列有效的
    CAD 操作命令，这些命令可以被执行以创建 CAD 模型。该方法的主要贡献在于自动分组草图笔画，并为每组笔画生成参数化的 CAD 操作，依据前面迭代中重建的组。该方法包括两个阶段，即笔画分组阶段和操作重建阶段。在笔画分组阶段，首先通过专门设计的
    Transformer 编码器网络将每个草图笔画嵌入为一个标记，然后由 Transformer 解码器网络处理这些标记，生成输入标记的组概率。通过这种方式，可能构成形状特定部分的草图笔画会被分组在一起。与笔画分组阶段最相关的工作是
    SketchGNN [[56](#bib.bib56)] 方法，该方法提出了一种图神经网络方法用于自由手绘草图的语义分割。接下来，在操作重建阶段，候选组被转换为几何原始体及其对应参数，依赖于现有几何上下文。该步骤之后是几何拟合和分组修正，然后将更新后的组作为几何上下文传递给下一次迭代。在这个过程中结束时，得到所需的
    CAD 形状和 CAD 命令序列。该方法还扩展到处理长笔画序列，通过滑动窗口方案逐步输出复杂形状的 CAD 模型。
- en: Similar to the Sketch2CAD [[24](#bib.bib24)], Free2CAD also provides a large-scale
    synthetic dataset of $82,000$ paired CAD modeling operation sequences and their
    corresponding rendered sketches which are segmented based their corresponding
    CAD commands. The code and dataset of this method are publicly available ⁷⁷7https://geometry.cs.ucl.ac.uk/projects/2022/free2cad/.
    The evaluation results of Free2CAD on both their generated dataset and on Fusion
    360 dataset illustrate its high performance in processing different user drawings
    and producing CAD commands which make desirable 3D shapes when executed by CAD
    tools.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Sketch2CAD [[24](#bib.bib24)] 类似，Free2CAD 也提供了一个大规模的合成数据集，包括 $82,000$ 对 CAD
    建模操作序列及其对应的渲染草图，这些草图根据其对应的 CAD 命令进行了分割。该方法的代码和数据集是公开可用的 ⁷⁷7https://geometry.cs.ucl.ac.uk/projects/2022/free2cad/。Free2CAD
    在其生成的数据集以及 Fusion 360 数据集上的评估结果展示了其在处理不同用户绘图和生成 CAD 命令方面的高性能，这些命令在 CAD 工具中执行时能够生成理想的
    3D 形状。
- en: V-B3 CAD2Sketch [[28](#bib.bib28)]
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B3 CAD2Sketch [[28](#bib.bib28)]
- en: 'Unlike Sketch2CAD which tries to facilitate the design process for non-expert
    users in an interactive modeling system, CAD2Sketch is designed to assist expert
    industrial designers. As the name suggests, CAD2Sketch is a method dedicated to
    synthesizing concept sketches from CAD models. Concept sketching is a preliminary
    stage in CAD modeling, wherein designers refine their mental conception of a 3D
    object from rough outlines to intricate details, often using numerous construction
    lines. An example of a simple concept sketch is depicted in [12](#S5.F12 "Figure
    12 ‣ V-B3 CAD2Sketch [28] ‣ V-B 3D CAD Generation from Sketch ‣ V CAD Construction
    with Generative Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey"), shown on the left side, alongside its refined version displayed on
    the right side.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '与旨在通过交互建模系统促进非专业用户设计过程的Sketch2CAD不同，CAD2Sketch旨在帮助专业工业设计师。正如其名所示，CAD2Sketch是一种致力于从CAD模型合成概念草图的方法。概念草图是CAD建模中的初步阶段，设计师在这一阶段将对3D对象的心理构思从粗略轮廓细化到复杂细节，通常使用大量的构造线。一个简单的概念草图的例子展示在[12](#S5.F12
    "Figure 12 ‣ V-B3 CAD2Sketch [28] ‣ V-B 3D CAD Generation from Sketch ‣ V CAD
    Construction with Generative Deep Learning ‣ Geometric Deep Learning for Computer-Aided
    Design: A Survey")中，左侧为该草图的原始版本，右侧则为其精细化版本。'
- en: '![Refer to caption](img/8c67663bc93183f415436b502a721cef.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8c67663bc93183f415436b502a721cef.png)'
- en: 'Figure 12: An illustration showcasing a conceptual sketch (left) designed for
    creating a 3D shape. Examples of these sketches can be found in the CAD2Sketch
    dataset [[28](#bib.bib28)]. After refining the concept sketch, the final freehand
    sketch of the model is achieved (right). Refined sketches like these are available
    in the Sketch2CAD dataset [[24](#bib.bib24)].'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：展示了一个用于创建3D形状的概念草图（左）。这些草图的示例可以在CAD2Sketch数据集中找到[[28](#bib.bib28)]。经过概念草图的精细化处理后，最终的手绘模型草图就完成了（右）。这样的精细化草图在Sketch2CAD数据集中也可以找到[[24](#bib.bib24)]。
- en: Notably, concept sketching mirrors the detailed steps in a designer’s mind,
    akin to the stages of CAD modeling, while sketches in the Sketch2CAD dataset typically
    present the final free-hand sketch without these detailed auxiliary construction
    lines. CAD2Sketch introduced a large-scale synthetic dataset of concept sketches
    by proposing a method for converting CAD B-Rep data into concept sketches. CAD2Sketch
    establishes a large-scale synthetic dataset of concept sketches by introducing
    an method to convert CAD B-Rep data into concept sketches. The dataset is comparable
    to the OpenSketch [[80](#bib.bib80)] dataset which has $400$ real concept sketches
    crafted by various expert designers. However, CAD2Sketch targets to bridge the
    gap between synthetic and real data, enabling the training of neural networks
    on concept sketches. The CAD2Sketch method initially generates construction lines
    for each operation in a CAD sequence. To avoid overwhelming the sketch with too
    many lines, a subset of these lines is chosen by solving a binary optimization
    problem. Subsequently, the opacity and shape of each line are adjusted to achieve
    a visual resemblance to real concept sketches. The synthetic concept sketches
    produced by CAD2Sketch closely resemble their corresponding real pairs in OpenSketch,
    so that even designers can hardly distinguish between them [[28](#bib.bib28)].
    CAD2Sketch also generates a substantial number of paired sketches and normal maps,
    utilized for training a neural network to infer normal maps from concept sketches.
    The dataset contains approximately $6,000$ paired concept sketches and normal
    maps. The trained neural network’s generalization to real shapes is evaluated
    on a test set of $108$ CAD sequences from the ABC dataset, yielding promising
    results. However, it is worth noting that this method is evaluated on a limited
    number of real sketches, given the relatively small size of the OpenSketch dataset.
    Additionally, the CAD2Sketch method, built on CAD sequences from existing large-scale
    CAD datasets, is limited to sequences composed of sketch and extrusion operations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，概念草图反映了设计师思维中的详细步骤，类似于 CAD 建模的各个阶段，而 Sketch2CAD 数据集中的草图通常只呈现最终的手绘草图，而没有这些详细的辅助构造线。CAD2Sketch
    通过提出将 CAD B-Rep 数据转换为概念草图的方法，推出了一个大规模的合成概念草图数据集。CAD2Sketch 通过引入一种将 CAD B-Rep 数据转换为概念草图的方法，建立了一个大规模的合成概念草图数据集。该数据集可与
    OpenSketch [[80](#bib.bib80)] 数据集相媲美，后者拥有 $400$ 张由各种专家设计师制作的真实概念草图。然而，CAD2Sketch
    旨在弥合合成数据和真实数据之间的差距，使神经网络能够在概念草图上进行训练。CAD2Sketch 方法最初为 CAD 序列中的每个操作生成构造线。为了避免使草图被过多的线条淹没，通过解决一个二进制优化问题来选择这些线条的子集。随后，调整每条线的透明度和形状，以实现与真实概念草图的视觉相似性。CAD2Sketch
    生成的合成概念草图与 OpenSketch 中的真实配对草图非常相似，以至于即使是设计师也很难区分 [[28](#bib.bib28)]。CAD2Sketch
    还生成了大量配对草图和法线贴图，用于训练神经网络从概念草图推断法线贴图。数据集中包含大约 $6,000$ 张配对概念草图和法线贴图。训练后的神经网络在 ABC
    数据集的 $108$ 个 CAD 序列的测试集上的泛化能力得到了令人鼓舞的结果。然而，值得注意的是，由于 OpenSketch 数据集的规模相对较小，这种方法的评估仅限于少量真实草图。此外，CAD2Sketch
    方法建立在现有大规模 CAD 数据集的 CAD 序列上，限制于由草图和挤出操作组成的序列。
- en: V-C 3D CAD Command Generation
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 3D CAD 命令生成
- en: While previous approaches have introduced synthetic datasets for 3D CAD reconstruction,
    the absence of a standardized collection of human-designed 3D CAD models with
    retained CAD command sequences poses a limitation. Similar to the valuable contribution
    of the SketchGraphs dataset in the area of 2D sketch synthesis, a curated dataset
    of human-designed 3D CAD models with preserved CAD command sequences would greatly
    benefit research and the development of practical methods for real-world applications.
    Fusion 360 Reconstruction dataset [[30](#bib.bib30)] fills this gap as the first
    human-designed dataset, featuring $8,625$ CAD models constructed using a sequence
    of Sketch and Extrude CAD operations. Accompanying this dataset is an environment
    known as the Fusion 360 Gym, capable of executing these CAD operations. Each CAD
    model is represented as a Domain-Specific Language (DSL) program, a stateful language
    serving as a simplified wrapper for the underlying Fusion 360 Python API. This
    language keeps track of the current geometry under construction, updated iteratively
    through a sequence of sketch and extrude commands. The data and correponding codes
    are publicly available ⁸⁸8https://github.com/AutodeskAILab/Fusion360GalleryDataset/blob/master/docs/reconstruction.md.
    This dataset is benchmarked through the training and evaluation of a machine learning-based
    approach featuring neurally guided search for programmatic CAD reconstruction
    from a specified geometry. The approach begins by training a policy, which is
    instantiated as a Message Passing Network (MPN) [[110](#bib.bib110), [111](#bib.bib111)]
    with an original encoding of state and action. This training is conducted through
    imitation learning, drawing insights from ground truth construction sequences.
    In the subsequent inference stage, the method incorporates a search mechanism,
    utilizing the learned neural policy to iteratively engage with the Fusion 360
    Gym environment until a precise CAD program is discerned. However, a limitation
    of this method is its assumption that the 2D geometry is given, with the method
    solely predicting the sketches to be extruded and their extent at each iteration.
    Subsequently, this dataset has been utilized by generative methods that tackle
    scenarios where the geometry is not provided, and the entire 3D model needs to
    be synthesized from scratch using Sketch and Extrude operations.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以往的方法已经引入了用于3D CAD重建的合成数据集，但缺乏一个标准化的人类设计的3D CAD模型集合，并保留CAD命令序列，这是一个限制。类似于SketchGraphs数据集在2D草图合成领域的宝贵贡献，一个经过策划的人类设计的3D
    CAD模型数据集，并保留CAD命令序列，将极大地促进研究和实际方法的开发。Fusion 360重建数据集[[30](#bib.bib30)]填补了这一空白，作为第一个人类设计的数据集，包含$8,625$个使用Sketch和Extrude
    CAD操作序列构建的CAD模型。这个数据集配备了一个称为Fusion 360 Gym的环境，能够执行这些CAD操作。每个CAD模型被表示为一种领域特定语言（DSL）程序，这是一种状态语言，用作Fusion
    360 Python API的简化封装。这种语言跟踪当前构建中的几何图形，并通过一系列草图和挤出命令进行迭代更新。数据和相应的代码可以公开访问⁸⁸8https://github.com/AutodeskAILab/Fusion360GalleryDataset/blob/master/docs/reconstruction.md。该数据集通过训练和评估一种基于机器学习的方法来进行基准测试，该方法具有神经引导的搜索，用于从指定几何体进行程序化CAD重建。这种方法首先训练一个策略，该策略被实例化为消息传递网络（MPN）[[110](#bib.bib110),
    [111](#bib.bib111)]，其具有原始的状态和动作编码。这种训练通过模仿学习进行，从真实构建序列中获得见解。在随后的推断阶段，该方法结合了搜索机制，利用学习到的神经策略与Fusion
    360 Gym环境进行迭代互动，直到识别出精确的CAD程序。然而，这种方法的一个限制是它假设2D几何图形已给出，方法仅预测要挤出的草图及其在每次迭代中的范围。随后，这个数据集被生成方法用于处理未提供几何体的情况，需要使用Sketch和Extrude操作从头合成整个3D模型。
- en: As another effort to infer CAD modeling construction sequences through neural-guided
    search, a method in which the B-Rep data of each CAD model is expressed in the
    form of a zone graph, where solid regions constitute the zones and surface patches
    (curves) form the edges is proposed in [[29](#bib.bib29)]. This representation
    is then introduced to a GCN to facilitate feature learning and, subsequently,
    a search method is employed to deduce a sequence of CAD operations that can faithfully
    recreate this zone graph. However, none of these two aforementioned methods leverage
    generative methods for CAD operation generation. In this subsection, the most
    recent generative methods in this domain are introduced.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作为通过神经引导搜索推断CAD建模构建顺序的另一个努力，[[29](#bib.bib29)]中提出了一种方法，其中每个CAD模型的B-Rep数据以区域图的形式表达，其中固体区域构成区域，表面贴片（曲线）形成边缘。然后将这种表示引入GCN，以便进行特征学习，随后采用一种搜索方法来推导出能够真实再现该区域图的CAD操作序列。然而，以上两种方法都没有利用生成方法来生成CAD操作。在本小节中，将介绍该领域最新的生成方法。
- en: V-C1 DeepCAD [[17](#bib.bib17)]
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C1 DeepCAD [[17](#bib.bib17)]
- en: 'The first generative deep learning model proposed for creating 3D CAD commands
    is DeepCAD. Given the sequential and irregular nature of CAD operations and B-Rep
    data, it is essential to choose a fixed set of the most commonly employed CAD
    operations and organize them into a unified structure for utilization by a generative
    neural network. Drawing inspiration from earlier generative techniques designed
    for 2D CAD analysis and sketch generation such as CurveGen-TurtleGen, SketchGen,
    CAD as Language, and VITROVION outlined in Section [V-A](#S5.SS1 "V-A Engineering
    2D Sketch Generation for CAD ‣ V CAD Construction with Generative Deep Learning
    ‣ Geometric Deep Learning for Computer-Aided Design: A Survey"), DeepCAD follows
    a similar approach by likening CAD operations to natural language. It introduces
    a generative Transformer network for autoencoding CAD operations. It is worth
    noting that DeepCAD diverges from previous generative methods by adopting a feed-forward
    Transformer structure instead of the autoregressive Transformer commonly used
    in such contexts. Additionally, DeepCAD has constructed and released a large-scale
    dataset featuring $178,238$ CAD models from OnShape repository generated through
    Sketch and Extrude operations, accompanied by their respective CAD construction
    sequences. This dataset surpasses the size of the Fusion 360 Gallery dataset,
    which contained approximately $8,000$ designs. The substantial increase in the
    number of designs within the DeepCAD dataset enhances its suitability for training
    generative networks.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个用于创建3D CAD命令的生成深度学习模型是DeepCAD。鉴于CAD操作和B-Rep数据的顺序性和不规则性，选择一组固定的最常用CAD操作并将其组织成一个统一的结构，以供生成神经网络使用是至关重要的。受到早期为2D
    CAD分析和草图生成设计的生成技术（如CurveGen-TurtleGen、SketchGen、CAD as Language和VITROVION，详见[V-A](#S5.SS1
    "V-A Engineering 2D Sketch Generation for CAD ‣ V CAD Construction with Generative
    Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design: A Survey)")的启发，DeepCAD采用类似的方法，将CAD操作类比于自然语言。它引入了一个生成的Transformer网络用于自编码CAD操作。值得注意的是，DeepCAD不同于以往的生成方法，它采用了前馈Transformer结构，而不是通常在此类上下文中使用的自回归Transformer。此外，DeepCAD构建并发布了一个大规模的数据集，包含$178,238$个来自OnShape库的CAD模型，这些模型是通过Sketch和Extrude操作生成的，并附有各自的CAD构建序列。该数据集的规模超过了包含大约$8,000$个设计的Fusion
    360 Gallery数据集。DeepCAD数据集中设计数量的显著增加提高了其训练生成网络的适用性。'
- en: 'In the standardized structure suggested for CAD operations in DeepCAD, the
    CAD commands are explicitly detailed, providing information on their type, parameters,
    and sequential index. The Sketch commands encompass curves of types line, arc
    and circle along with their respective parameters. Meanwhile, Extrude commands
    signify extrusion operations of types one-sided, symmetric, two-sided and boolean
    operations of types new body, join, cut, or intersect. These operations are employed
    to integrate the modified shape with the previously constructed form. A CAD model
    $M$ is represented as a sequence of curve commands which build the sketch, intertwined
    with extrude commands. The total number of CAD commands to represent each CAD
    model is fixed to $60$ and a padding approach with empty commands is used to fit
    the CAD models with shorter command sequence in this fixed-length structure. Each
    command undergoes encoding into a $16$-dimensional vector representing the whole
    set of parameters of all commands. In instances where specific parameters for
    a given command are not applicable, they are uniformly set to $1$. As illustrated
    in Figure [13](#S5.F13 "Figure 13 ‣ V-C1 DeepCAD [17] ‣ V-C 3D CAD Command Generation
    ‣ V CAD Construction with Generative Deep Learning ‣ Geometric Deep Learning for
    Computer-Aided Design: A Survey"), the autoencoder network takes a sequence of
    CAD commands as input, transforms them into a latent space through Transformer
    encoder, and subsequently decodes a latent vector to reconstruct a sequence of
    CAD commands. The generated CAD commands can be imported into CAD software for
    final user editing.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '在 DeepCAD 中建议的标准化结构下，CAD 命令被详细列出，提供了其类型、参数和顺序索引的信息。Sketch 命令包括线、弧和圆等曲线类型以及各自的参数。同时，Extrude
    命令表示一系列挤压操作，包括单面、对称、双面挤压以及布尔操作（如新体、连接、切割或交集）。这些操作用于将修改后的形状与之前构建的形式整合。一个 CAD 模型
    $M$ 表示为一系列曲线命令构建的草图，并与挤压命令交织在一起。表示每个 CAD 模型的 CAD 命令总数固定为 $60$，对于命令序列较短的 CAD 模型，采用填充方式使用空命令以适应这一固定长度结构。每个命令被编码为一个
    $16$ 维向量，表示所有命令的参数集合。如果某个命令的特定参数不适用，则统一设置为 $1$。如图 [13](#S5.F13 "Figure 13 ‣ V-C1
    DeepCAD [17] ‣ V-C 3D CAD Command Generation ‣ V CAD Construction with Generative
    Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design: A Survey")
    所示，自动编码器网络将 CAD 命令序列作为输入，通过 Transformer 编码器转换为潜在空间，然后解码潜在向量以重建一系列 CAD 命令。生成的 CAD
    命令可以导入 CAD 软件中进行最终用户编辑。'
- en: '![Refer to caption](img/562f3afe5984877bb1956bff74ed20ac.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/562f3afe5984877bb1956bff74ed20ac.png)'
- en: 'Figure 13: Schematic representation of the DeepCAD model architecture [[17](#bib.bib17)].
    The model, depicted on the left, is a transformer autoencoder network trained
    in an unsupervised manner. It takes a sequence of CAD commands as input and reconstructs
    them. The detailed structure of this autoencoder is shown on the right, where
    $\left\{C_{1},C_{2},...,C_{N}\right\}$ tokens represent the input CAD command
    sequence, and $\left\{C_{1},C_{2},...,C_{N}\right\}$ are the reconstructed commands.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：DeepCAD 模型架构的示意图 [[17](#bib.bib17)]。模型在左侧展示，为一种以无监督方式训练的 Transformer 自动编码器网络。它将
    CAD 命令序列作为输入，并重建这些命令。右侧展示了这个自动编码器的详细结构，其中 $\left\{C_{1},C_{2},...,C_{N}\right\}$
    代表输入的 CAD 命令序列，而 $\left\{C_{1},C_{2},...,C_{N}\right\}$ 为重建的命令。
- en: 'The performance of DeepCAD is evaluated on two tasks: CAD model autoencoding
    and random CAD model generation. Once the autoencoder network is trained for reconstructing
    CAD commands, the latent-GAN technique [[112](#bib.bib112)] is employed to train
    a generator and a discriminator on the learned latent space. The generator produces
    a latent vector $z$ by receiving as input a random vector sampled from a multivariate
    Gaussian distribution. This latent vector can then be introduced to the trained
    Transformer decoder to produce the CAD model commands. Their experiments also
    demonstrate that the pretrained model on the DeepCAD dataset exhibits good generalization
    capabilities when applied to the Fusion 360 dataset. Notably, these two datasets
    originate from distinct sources, namely OnShape and Autodesk Fusion 360.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: DeepCAD 的性能在两个任务上进行了评估：CAD 模型自编码和随机 CAD 模型生成。一旦自编码器网络被训练用于重建 CAD 命令，就会采用潜在-GAN
    技术 [[112](#bib.bib112)] 来训练生成器和鉴别器。生成器通过接收从多变量高斯分布中采样的随机向量来生成潜在向量 $z$。然后，可以将该潜在向量引入到训练好的
    Transformer 解码器中，以生成 CAD 模型命令。他们的实验还表明，在 DeepCAD 数据集上预训练的模型在应用于 Fusion 360 数据集时展现出了良好的泛化能力。值得注意的是，这两个数据集来自不同的来源，即
    OnShape 和 Autodesk Fusion 360。
- en: The application of DeepCAD generative model is further demonstrated in the conversion
    of 3D point cloud data into a CAD model. In this context, both the generative
    autoencoder and the PointNet++ encoder [[8](#bib.bib8)] are trained concurrently
    to encode the CAD model into the same latent vector $z$ from the CAD commands
    sequence and its corresponding point cloud data, respectively. During inference,
    the pretrained PointNet++ [[8](#bib.bib8)] encoder embeds the point cloud data
    into the latent vector $z$ which is subsequently input to the generative decoder
    of the pretrained DeepCAD autoencoder to produce the CAD commands sequence.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: DeepCAD 生成模型的应用进一步体现在将 3D 点云数据转换为 CAD 模型。在这种情况下，生成自编码器和 PointNet++ 编码器 [[8](#bib.bib8)]
    同时进行训练，以将 CAD 模型编码为来自 CAD 命令序列和相应点云数据的相同潜在向量 $z$。在推断过程中，预训练的 PointNet++ [[8](#bib.bib8)]
    编码器将点云数据嵌入到潜在向量 $z$ 中，随后将其输入到预训练的 DeepCAD 自编码器的生成解码器中，以生成 CAD 命令序列。
- en: V-C2 SkexGen [[31](#bib.bib31)]
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C2 SkexGen [[31](#bib.bib31)]
- en: Despite advancements achieved by DeepCAD which excels in generating a diverse
    range of shapes, a persistent challenge remains in the limited user control over
    the generated designs. The ability for users to exert influence over the output
    and tailor designs to meet specific requirements would be a great advantage for
    the real world applications. In response to this challenge, SkexGen proposed a
    novel autoregressive Transformer network with three separate encoders capturing
    the topological, geometric and extrusion variations in the CAD command sequences
    separately. This approach enables more effective and distinct user control over
    the topology and geometry of the model, facilitating exploration within a broader
    search space of related designs which in turn, results in the generation of more
    realistic and diverse CAD models.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DeepCAD 在生成多样化形状方面取得了进展，但用户对生成设计的控制仍然是一个持续的挑战。允许用户对输出进行影响并定制设计以满足特定要求将对实际应用大有裨益。对此挑战，SkexGen
    提出了一个新颖的自回归 Transformer 网络，该网络有三个独立的编码器，分别捕捉 CAD 命令序列中的拓扑、几何和挤出变化。这种方法使得用户能够对模型的拓扑和几何形状进行更有效和明确的控制，从而在更广泛的相关设计搜索空间内进行探索，进而生成更加逼真和多样化的
    CAD 模型。
- en: 'Inspired by CurveGen-TurtleGen [[21](#bib.bib21)] and DeepCAD [[17](#bib.bib17)]
    methods, a CAD model in SkexGen is represented by a hierarchy of primitives with
    a sketch-and-extrude construction sequence. Within this hierarchy, a 3D model
    is composed of 3D solids, where each solid is defined as an extruded sketch. A
    sketch constitutes a collection of faces, and each face represents a 2D surface
    enclosed by a loop. A loop is formed by one or multiple curves, including lines,
    arcs or circles, and curve represents the fundamental level of the hierarchy.
    Consequently, CAD models are encoded using five types of tokens for input to the
    Transformer: 1) topology tokens, denoting the curve type, 2) geometry tokens,
    specifying 2D coordinates along the curves, 3) end of primitive tokens, 4) extrusion
    tokens, indicating parameters of extrusion and Boolean operations, and 5) end
    of sequence tokens. The autoregressive Transformer network introduced by SkexGen
    is composed of two independent branches, each trained separately: 1) the Sketch
    branch is composed of two distinct encoders dedicated to learning topological
    and geometrical variations in sketches. Additionally, a single decoder is employed,
    receiving concatenated topology and geometry encoded codebooks as input and predicting
    the sketch subsequence autoregressively. 2) The “Extrude” branch comprises an
    encoder and decoder specifically designed to learn variations in extrusion and
    Boolean operations. Furthermore, an additional autoregressive decoder is positioned
    at the end, tasked with learning an effective combination of geometrical, topological,
    and extrusion codebooks, thereby generating CAD construction sequences. An additional
    autoregressive decoder on top learns an effective combination of geometrical,
    topological and extrusion codebooks as CAD construction sequences. This intricate
    tokenization and multifaceted network architecture allows for a nuanced and comprehensive
    control over both topology and geometry, enabling the generation of CAD models
    by effectively capturing various design aspects.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 CurveGen-TurtleGen [[21](#bib.bib21)] 和 DeepCAD [[17](#bib.bib17)] 方法的启发，SkexGen
    中的 CAD 模型通过具有草图和挤出构建序列的原语层次结构来表示。在这个层次结构中，3D 模型由 3D 实体组成，每个实体被定义为一个挤出的草图。一个草图由一组面组成，每个面代表一个由循环包围的
    2D 表面。循环由一个或多个曲线形成，包括直线、弧线或圆弧，曲线代表了层次结构的基本层级。因此，CAD 模型使用五种类型的标记编码以输入到 Transformer：1）拓扑标记，表示曲线类型，2）几何标记，指定沿曲线的
    2D 坐标，3）原语结束标记，4）挤出标记，指示挤出和布尔操作的参数，以及 5）序列结束标记。SkexGen 引入的自回归 Transformer 网络由两个独立的分支组成，每个分支分别训练：1）Sketch
    分支由两个不同的编码器组成，专注于学习草图中的拓扑和几何变化。此外，使用一个解码器，接收拼接的拓扑和几何编码代码本作为输入，并自回归预测草图子序列。2）“Extrude”分支包括一个编码器和解码器，专门用于学习挤出和布尔操作的变化。此外，网络末尾还配置了一个额外的自回归解码器，负责学习几何、拓扑和挤出代码本的有效组合，从而生成
    CAD 构建序列。额外的自回归解码器在顶部学习几何、拓扑和挤出代码本的有效组合作为 CAD 构建序列。这种复杂的标记化和多层次网络架构使得对拓扑和几何的控制更为细致和全面，从而通过有效捕捉各种设计方面来生成
    CAD 模型。
- en: Given capacity of SkexGen to independently encode and generate Sketch and Extrude
    command sequences, this approach is versatile and can be applied to both 2D sketch
    generation and 3D CAD generation tasks. Evaluation results highlight its proficiency
    in generating more intricate designs compared to CurveGen-TurtleGen [[21](#bib.bib21)]
    and DeepCAD [[17](#bib.bib17)] methods. Notably, SkexGen excels in supporting
    multi-step Sketch and Extrude sequences, a capability lacking in DeepCAD, which
    primarily produces single-step results.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 SkexGen 能够独立编码和生成 Sketch 和 Extrude 命令序列，这种方法非常灵活，适用于 2D 草图生成和 3D CAD 生成任务。评估结果突显了它在生成比
    CurveGen-TurtleGen [[21](#bib.bib21)] 和 DeepCAD [[17](#bib.bib17)] 方法更复杂设计方面的高效性。特别是，SkexGen
    在支持多步骤 Sketch 和 Extrude 序列方面表现出色，而 DeepCAD 主要生成单步骤结果。
- en: V-D 3D CAD Generation with Direct B-Rep Synthesis
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 3D CAD 生成与直接 B-Rep 合成
- en: V-D1 SolidGen [[15](#bib.bib15)]
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-D1 SolidGen [[15](#bib.bib15)]
- en: Creating 3D CAD designs through either the generation of CAD commands or direct
    B-Rep synthesis comes with a set of advantages and disadvantages. The generative
    methods introduced in previous works, namely Fusion 360 Reconstruction, DeepCAD,
    and SkexGen, share a common goal of producing 3D CAD models by generating sequences
    of 3D CAD operations or commands. These commands are then processed by a solid
    modeling kernel in a CAD tool to recover the final CAD design in B-Rep format.
    Generating CAD commands, as opposed to directly creating the B-Rep, offers several
    advantages. Converting CAD commands into B-Rep format is feasible, while the reverse
    process is more challenging due to the potential ambiguity where different command
    sequences may result in the same B-Rep. Additionally, CAD commands are more human-interpretable,
    enabling users to edit designs for various applications by processing the commands
    in a CAD tool. However, training models for such tasks necessitates large-scale
    CAD datasets that retain the history of CAD modeling operations. Consequently,
    datasets like DeepCAD (comprising around 190,000 models) and Fusion 360 Reconstruction
    (with approximately 8,000 models) are exclusively constructed for this purpose.
    In contrast, most large-scale datasets in the field, such as ABC (with over 1
    million models), solely provide B-Rep data without a stored sequence of CAD modeling
    operations. While generating CAD commands offers increased flexibility, interoperability,
    and user control, an alternative strategy is to directly synthesize the B-Rep.
    This approach may prove beneficial when leveraging existing large-scale datasets.
    Moreover, direct B-Rep synthesis allows for the support of more complex curves
    and surfaces in creating 3D shapes. This stands in contrast to CAD command generative
    methods, which are constrained to CAD models constructed using Sketch and Extrude
    commands with a limited list of supported curve types.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过生成 CAD 命令或直接 B-Rep 合成来创建 3D CAD 设计具有一系列优缺点。之前介绍的生成方法，例如 Fusion 360 Reconstruction、DeepCAD
    和 SkexGen，都有一个共同的目标，即通过生成一系列 3D CAD 操作或命令来制作 3D CAD 模型。这些命令随后由 CAD 工具中的实体建模内核处理，以恢复最终的
    B-Rep 格式的 CAD 设计。生成 CAD 命令，相较于直接创建 B-Rep，提供了几个优势。将 CAD 命令转换为 B-Rep 格式是可行的，而反向过程则更具挑战性，因为不同的命令序列可能会导致相同的
    B-Rep 结果。此外，CAD 命令更具人类可解释性，允许用户通过在 CAD 工具中处理这些命令来编辑设计以满足各种应用。然而，训练用于这些任务的模型需要大规模的
    CAD 数据集，这些数据集保留了 CAD 建模操作的历史。因此，像 DeepCAD（包含约 190,000 个模型）和 Fusion 360 Reconstruction（约
    8,000 个模型）这样的数据集专门为此目的构建。相比之下，该领域的大多数大规模数据集，例如 ABC（包含超过 1 百万模型），仅提供 B-Rep 数据，而没有存储
    CAD 建模操作的序列。尽管生成 CAD 命令提供了更多的灵活性、互操作性和用户控制，但另一种策略是直接合成 B-Rep。这种方法在利用现有的大规模数据集时可能会有所裨益。此外，直接
    B-Rep 合成允许支持更复杂的曲线和表面来创建 3D 形状。这与 CAD 命令生成方法形成对比，后者仅限于使用 Sketch 和 Extrude 命令构建的
    CAD 模型，并且支持的曲线类型列表有限。
- en: 'SolidGen [[15](#bib.bib15)] introduces a method for direct B-Rep synthesis,
    eliminating the necessity for a history of CAD command sequences. The approach
    leverages pointer networks [[107](#bib.bib107)] and autoregressive Transformer
    networks to learn B-Rep topology and progressively predict vertices, edges, and
    faces individually. Running in parallel with SolidGen, the work by [[113](#bib.bib113)]
    is focused on 3D face identification within B-Rep data, given a single 2D line
    drawing. Drawing inspiration from pointer networks, this approach also employs
    an autoregressive Transformer network to identify edge loops in the 2D line drawing,
    and predicts one co-edge index at a time, corresponding to the actual planar and
    cylindrical faces in the 3D design. However, SolidGen stands out as a more advantageous
    method. It goes beyond edge loop identification and synthesizes the complete B-Rep
    data for the 3D shape. Additionally, it supports the representation of all types
    of faces in the design, providing a more comprehensive and versatile solution.
    SolidGen also introduces the Indexed Boundary Representation (Indexed B-Rep) to
    represent B-Reps as numeric arrays suitable for neural network use. This indexed
    B-Rep organizes B-Rep vertices, edges, and faces in a clearly defined hierarchy
    to capture both geometric and topological relations. Structurally, it consists
    of three lists, denoted as ${\mathcal{V},\mathcal{E},\mathcal{F}}$, representing
    Vertices, Edges, and Faces, respectively. In such hierarchical structure, edges
    $\mathcal{E}$ are denoted as list of indices referring to vertices $\mathcal{V}$,
    and each face in $\mathcal{F}$ indicates an index list referring into edges $\mathcal{E}$.
    The proposed autoregressive network progressively predicts the B-Rep tokens through
    training three distinct Transformers for generating vertices, edges, and faces,
    respectively. Formally, its goal is to learn a joint distribution over B-Rep $\mathcal{B}$:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: SolidGen [[15](#bib.bib15)] 引入了一种直接 B-Rep 合成的方法，消除了对 CAD 命令序列历史的需求。这种方法利用了指针网络
    [[107](#bib.bib107)] 和自回归 Transformer 网络来学习 B-Rep 拓扑，并逐步预测顶点、边和面。与 SolidGen 并行进行的
    [[113](#bib.bib113)] 工作则侧重于在给定单一 2D 线条图的情况下，在 B-Rep 数据中进行 3D 面识别。该方法借鉴了指针网络的灵感，采用了自回归
    Transformer 网络来识别 2D 线条图中的边缘环，并逐个预测与 3D 设计中的实际平面和圆柱面对应的共同边索引。然而，SolidGen 被认为是一种更有优势的方法。它不仅限于边缘环识别，还能合成
    3D 形状的完整 B-Rep 数据。此外，它支持设计中所有类型面的表示，提供了一个更全面和多样化的解决方案。SolidGen 还引入了索引边界表示（Indexed
    B-Rep），将 B-Rep 表示为适合神经网络使用的数字数组。这个索引 B-Rep 将 B-Rep 顶点、边和面组织成一个明确的层次结构，以捕捉几何和拓扑关系。在这种层次结构中，边
    $\mathcal{E}$ 被表示为指向顶点 $\mathcal{V}$ 的索引列表，每个面 $\mathcal{F}$ 中的每一项表示指向边 $\mathcal{E}$
    的索引列表。所提出的自回归网络通过训练三个不同的 Transformer 网络来逐步预测 B-Rep 标记，分别用于生成顶点、边和面。形式上，它的目标是学习
    B-Rep $\mathcal{B}$ 的联合分布：
- en: '|  | $p(\mathcal{B})=p(\mathcal{V},\mathcal{E},\mathcal{F}),$ |  | (5) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathcal{B})=p(\mathcal{V},\mathcal{E},\mathcal{F}),$ |  | (5) |'
- en: 'factorized as:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 分解为：
- en: '|  | $p(\mathcal{B})=p(\mathcal{F}\mid\mathcal{E},\mathcal{V})p(\mathcal{E}\mid\mathcal{V})p(\mathcal{V}).$
    |  | (6) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathcal{B})=p(\mathcal{F}\mid\mathcal{E},\mathcal{V})p(\mathcal{E}\mid\mathcal{V})p(\mathcal{V}).$
    |  | (6) |'
- en: 'This structure also allows for conditioning the distribution on an external
    context $c$, such as class labels, images, and voxels:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构还允许在外部上下文$c$（例如类别标签、图像和体素）上进行分布的条件化：
- en: '|  | $p(\mathcal{B})=p(\mathcal{F}\mid\mathcal{E},\mathcal{V},c)p(\mathcal{E}\mid\mathcal{V},c)p(\mathcal{V}\mid
    c).$ |  | (7) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathcal{B})=p(\mathcal{F}\mid\mathcal{E},\mathcal{V},c)p(\mathcal{E}\mid\mathcal{V},c)p(\mathcal{V}\mid
    c).$ |  | (7) |'
- en: Following training, the generation of an Indexed B-Rep involves sampling vertices,
    edges conditioned on vertices, and faces conditioned on both edges and vertices.
    Subsequently, the obtained Indexed B-Rep can be transformed into the actual B-Rep
    through a post-processing step. For a more comprehensive understanding of these
    processes, interested readers are encouraged to refer to the original paper [[15](#bib.bib15)].
    The efficacy of this method is assessed using a refined version of the DeepCAD
    dataset. Additionally, SolidGen introduces the Parametric Variations (PVar) dataset,
    purposefully designed for evaluating the model’s performance in class-conditional
    generation tasks. This synthetic dataset comprises $120,000$ CAD models distributed
    across $60$ classes.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，生成一个索引化的B-Rep涉及对顶点、以顶点为条件的边以及以边和顶点为条件的面进行采样。随后，获得的索引化B-Rep可以通过后处理步骤转换为实际的B-Rep。为了更全面地了解这些过程，感兴趣的读者可以参考原始论文[[15](#bib.bib15)]。该方法的有效性通过改进版的DeepCAD数据集进行评估。此外，SolidGen引入了参数化变化（PVar）数据集，专门用于评估模型在类别条件生成任务中的表现。该合成数据集包含$120,000$个CAD模型，分布在$60$个类别中。
- en: V-D2 CADParser [[16](#bib.bib16)]
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-D2 CADParser [[16](#bib.bib16)]
- en: In conjunction with SolidGen, CADParser has been introduced to predict the sequence
    of CAD commands from a given B-Rep CAD model. Unlike previous approaches that
    often utilized synthetic CAD datasets or relied on datasets like DeepCAD and Fusion
    360 reconstruction, which were restricted to CAD models created with only two
    operations, namely Sketch and Extrude, CADParser has introduced a comprehensive
    dataset featuring $40,000$ CAD models. These models incorporate a broader range
    of CAD operations, including Sketch, Extrusion, Revolution, Fillet, and Chamfer.
    This dataset provides a more diverse collection of CAD models constructed with
    five distinct types of CAD operations compared to previous datasets limited to
    only two operations. Each CAD model in this dataset is accompanied by both B-Rep
    data and the corresponding construction command sequence.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 与SolidGen配套，CADParser被引入用于预测给定B-Rep CAD模型的CAD命令序列。与之前通常使用合成CAD数据集或依赖于仅使用两个操作（即草图和挤压）创建的CAD模型的数据集（如DeepCAD和Fusion
    360重建）不同，CADParser引入了一个包含$40,000$个CAD模型的综合数据集。这些模型涵盖了更广泛的CAD操作，包括草图、挤压、旋转、倒角和斜角。与之前仅限于两种操作的数据集相比，该数据集提供了一个包含五种不同类型CAD操作的更为多样化的CAD模型集合。该数据集中的每个CAD模型都附有B-Rep数据及其相应的构建命令序列。
- en: 'CADParser also introduces a deep neural network architecture, referred to as
    the deep parser, designed to predict the CAD construction sequence for each B-Rep
    model. Drawing inspiration from UV-Net and BRepNet, both discussed in Sec. [IV](#S4
    "IV CAD Representation Learning ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey") as pioneering methods in representation learning for B-Rep data, CADParser
    treats each CAD model as a graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$ where
    the nodes represent faces, edges, and coedges of the model, and $\mathcal{E}$
    signifies the connections between the graph nodes. The BRepNet architecture serves
    as the graph encoder backbone, taking node features and the constructed adjacency
    matrix as input, and extracting local and global features of the graph through
    graph convolutions and topological walks. Simultaneously, the sequence of CAD
    commands $S=(C_{1},C_{2},...,C_{n})$ that constructs the CAD model is encoded
    as feature vectors. These are combined with the graph-encoded global and local
    features and fed into a Transformer decoder to autoregressively predict the next
    command sequence. Similar to previous methods, CAD commands are tokenized by representing
    each command $C_{i}$ as a tuple of command type $t_{i}$ and command parameter
    $p_{i}$. $t_{i}$ is encoded as a $12$-dimensional one-hot vector, representing
    12 different command types one at a time, and $p_{i}$ is a $257$-dimensional vector.
    This vector represents the quantized $256$-dimensional parameter vector of the
    command and a $1$-dimensional index indicating whether this command is used for
    the corresponding CAD model or not. The length of the CAD command sequence for
    all CAD models is fixed at $32$ for simplicity, with unused commands for each
    CAD model indicated by an index set to $-1$. The Transformer decoder has two separate
    output branches for predicting the CAD command type $t_{i}$ and parameter $p_{i}$
    vectors, respectively. For more details on the model architecture and training
    process, additional information can be found in [[16](#bib.bib16)]. The contribution
    of this work represents a step forward in generating more diverse CAD models by
    incorporating various CAD commands.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'CADParser 还引入了一种深度神经网络架构，称为深度解析器，旨在预测每个 B-Rep 模型的 CAD 构建顺序。受到 UV-Net 和 BRepNet
    的启发，这两者在第 [IV](#S4 "IV CAD Representation Learning ‣ Geometric Deep Learning for
    Computer-Aided Design: A Survey") 节中被讨论作为 B-Rep 数据表示学习的开创性方法，CADParser 将每个 CAD
    模型视为一个图 $\mathcal{G}=(\mathcal{V},\mathcal{E})$，其中节点代表模型的面、边和共边，$\mathcal{E}$
    表示图节点之间的连接。BRepNet 架构作为图编码器的骨干网络，将节点特征和构建的邻接矩阵作为输入，通过图卷积和拓扑遍历提取图的局部和全局特征。同时，构建
    CAD 模型的 CAD 命令序列 $S=(C_{1},C_{2},...,C_{n})$ 被编码为特征向量。这些特征向量与图编码的全局和局部特征相结合，并输入到
    Transformer 解码器中，以自回归的方式预测下一个命令序列。与之前的方法类似，CAD 命令通过将每个命令 $C_{i}$ 表示为命令类型 $t_{i}$
    和命令参数 $p_{i}$ 的元组进行标记化。$t_{i}$ 被编码为一个 $12$ 维的独热向量，表示 $12$ 种不同的命令类型，每次一个，而 $p_{i}$
    是一个 $257$ 维的向量。该向量表示命令的量化 $256$ 维参数向量和一个 $1$ 维的索引，指示该命令是否用于对应的 CAD 模型。所有 CAD 模型的
    CAD 命令序列长度固定为 $32$，以简化处理，对于每个 CAD 模型，未使用的命令由索引 $-1$ 表示。Transformer 解码器有两个独立的输出分支，分别用于预测
    CAD 命令类型 $t_{i}$ 和参数 $p_{i}$ 向量。有关模型架构和训练过程的更多细节，请参见 [[16](#bib.bib16)]。这项工作的贡献代表了通过整合各种
    CAD 命令生成更多样化的 CAD 模型的一个进步。'
- en: V-E 3D CAD Generation from Point Cloud
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 从点云生成 3D CAD
- en: 'As highlighted in the overview of existing 3D CAD generative models, the reverse
    engineering of CAD shapes has primarily been explored through methods utilizing
    either CAD sketches, B-Rep data or sequences of CAD commands as input. These approaches
    have made promising advances so far in reconstructing and generating CAD models.
    However, an equally crucial aspect is generating CAD data from alternative raw
    geometric data modalities, such as point clouds. This consideration also aligns
    with the future work outlined by the DeepCAD method. The ability to seamlessly
    convert diverse data representations into CAD models opens new avenues for real-world
    applications, bridging the gap between different data modalities and expanding
    the utility of CAD technology. This becomes especially critical in scenarios where
    new variations of a physical object are needed or when repairing a machinery object
    without access to the corresponding CAD model. This can be particularly challenging
    in instances where the object predates the digital era of manufacturing. In these
    scenarios, the process typically begins with scanning the object using a 3D sensor,
    which generates a point cloud. Subsequently, the acquired point cloud data needs
    to be decomposed into a collection of geometric primitives, such as curves or
    surfaces, to be interpreted by CAD software. The traditional three-step procedure
    [[114](#bib.bib114)] involves converting the point cloud into a mesh, explaining
    it through parametric surfaces to create a solid (B-Rep), and inferring a CAD
    program. Recent advancements in fitting primitives to point clouds [[115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117)] have managed to bypass the initial step
    of converting point clouds to meshes. Nevertheless, a notable limitation of these
    methods is their reliance on a finite set of fixed and disjoint primitives, which
    poses a challenge for convenient shape editing in subsequent steps. Addressing
    this, the recently proposed Point2Cyl [[32](#bib.bib32)] method frames the problem
    as an Extrusion Cylinder decomposition task, leveraging a neural network to predict
    per-point extrusion instances, surface normals, and base/barrel membership. These
    geometric proxies can then be used to estimate the extrusion parameters through
    differentiable and closed-form formulations. In this method, an Extrusion Cylinder
    is considered a fundamental primitive that signifies an extruded 2D sketch, characterized
    by parameters such as the extrusion axis, center, sketch, and sketch scale, which
    can be used to represent 3D CAD models. The terms base and barrel are utilized
    to denote specific surfaces of an extrusion cylinder, representing the base/top
    plane and the side surface of the extrusion cylinder, respectively. Figure [14](#S5.F14
    "Figure 14 ‣ V-E 3D CAD Generation from Point Cloud ‣ V CAD Construction with
    Generative Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey") shows an example of Extrusion Cylinder which is obtained by extruding
    a circle as the base 2D sketch.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在现有3D CAD生成模型的概述中所强调的，CAD形状的逆向工程主要通过利用CAD草图、B-Rep数据或CAD命令序列作为输入的方法来探索。这些方法在重建和生成CAD模型方面已经取得了有希望的进展。然而，同样重要的是从替代的原始几何数据模态（如点云）生成CAD数据。这一考虑也与DeepCAD方法所概述的未来工作相一致。能够将多样的数据表示无缝转换为CAD模型为实际应用开辟了新的途径，弥合了不同数据模态之间的差距，并扩展了CAD技术的实用性。这在需要物理对象的新变体或在没有相应CAD模型的情况下修复机械对象的情境中尤为重要。在这些情况下，通常的过程是使用3D传感器扫描对象，生成点云。随后，获取的点云数据需要被分解成几何原始体的集合，如曲线或表面，以供CAD软件解释。传统的三步程序[[114](#bib.bib114)]涉及将点云转换为网格，通过参数化表面解释它以创建一个固体（B-Rep），然后推断一个CAD程序。近期在点云拟合原始体方面的进展[[115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117)]已经成功绕过了将点云转换为网格的初步步骤。然而，这些方法的一个显著限制是它们依赖于有限的固定和不相交的原始体，这对后续步骤中的形状编辑构成了挑战。为解决这一问题，最近提出的Point2Cyl
    [[32](#bib.bib32)]方法将问题框架定义为挤出圆柱体分解任务，利用神经网络预测每个点的挤出实例、表面法线以及基座/圆筒成员。这些几何代理可以通过可微分和封闭形式的公式来估计挤出参数。在此方法中，挤出圆柱体被视为一个基本原始体，表示一个挤出的2D草图，其参数包括挤出轴、中心、草图和草图比例，这些参数可以用来表示3D
    CAD模型。术语“基座”和“圆筒”用来表示挤出圆柱体的特定表面，分别表示挤出圆柱体的底面/顶面和平面表面。图[14](#S5.F14 "图 14 ‣ 从点云生成V-E
    3D CAD ‣ 使用生成深度学习的V CAD构造 ‣ 计算机辅助设计的几何深度学习：综述")展示了一个通过将圆形作为基准2D草图进行挤出而获得的挤出圆柱体示例。
- en: '![Refer to caption](img/e3959f58aef110ead02791c76f51364b.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e3959f58aef110ead02791c76f51364b.png)'
- en: 'Figure 14: An example of Extrusion Cylinder. The 2D sketch is a circle here
    (right) which is extruded to make the Extrusion Cylinder (left) [[32](#bib.bib32)].'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：一个挤出圆柱体的示例。这里的 2D 草图是一个圆（右），它被挤出以形成挤出圆柱体（左）[[32](#bib.bib32)]。
- en: Point2Cyl leverages PointNet++ [[8](#bib.bib8)] to learn point cloud feature
    embeddings, which are then passed into two distinct fully connected networks for
    point cloud segmentation into extrusion cylinder, base/barrel and surface normal
    prediction. The approach is evaluated on Fusion Gallery and DeepCAD datasets,
    outperforming baselines and showcasing its effectiveness in reconstruction and
    shape editing. The code of this method is publicly available.⁹⁹9point2cyl.github.io
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Point2Cyl 利用 PointNet++ [[8](#bib.bib8)] 学习点云特征嵌入，然后将其传递到两个不同的全连接网络中，用于点云分割成挤出圆柱体、基座/桶和表面法线预测。该方法在
    Fusion Gallery 和 DeepCAD 数据集上进行了评估，超越了基线并展示了其在重建和形状编辑方面的有效性。该方法的代码公开可用。⁹⁹9point2cyl.github.io
- en: Nevertheless, it is important to note that this method is limited in its ability
    to handle cases where the input data is not noisy or distorted. The task of reconstructing
    the sharp edges and surfaces in prismatic 3D shapes from noisy point cloud data
    becomes challenging, given that point clouds inherently offer only an approximate
    representation of the 3D shape. This challenge is particularly pronounced when
    dealing with point clouds acquired through low-cost scanners, where any distortions
    or irregularities present in the shape may be addressed by smoothing during the
    surface reconstruction process. Lambourne et al. [[33](#bib.bib33)] proposed a
    method concurrently with Point2Cyl, addressing the reconstruction challenge of
    sharp prismatic shapes when provided with an approximate rounded (smoothed) point
    cloud. This approach introduces a differentiable pipeline that reconstructs a
    target shape in terms of voxels while extracting geometric parameters. An autoencoder
    network is trained to process a signed distance function represented as a voxel
    grid, obtained through methods converting dense point clouds into signed distance
    functions [[118](#bib.bib118), [119](#bib.bib119)]. The encoder produces an embedding
    vector, and the decoders further decompose the shape into 2D profile images and
    1D envelope arrays. During inference, CAD data is generated by searching a repository
    of 2D constrained sketches, extruding them, and combine them via Boolean operations
    to construct the final CAD model. Evaluation on the ABC dataset demonstrates the
    method’s ability to better approximate target shapes compared to DeepCAD method.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，这种方法在处理输入数据没有噪声或失真的情况下能力有限。从噪声点云数据中重建棱柱形 3D 形状的锐利边缘和表面是一项具有挑战性的任务，因为点云本质上只能提供
    3D 形状的近似表示。特别是当处理通过低成本扫描仪获取的点云时，这一挑战尤为突出，其中形状中的任何失真或不规则性可能会在表面重建过程中通过平滑处理得到解决。Lambourne
    等人[[33](#bib.bib33)] 提出了一个方法，与 Point2Cyl 同时提出，解决了在提供近似圆滑（平滑）点云的情况下，重建锐利棱柱形状的挑战。这种方法引入了一个可微分的流程，通过体素重建目标形状，同时提取几何参数。一个自编码器网络被训练来处理表示为体素网格的带符号距离函数，该函数通过将密集点云转换为带符号距离函数的方法获得[[118](#bib.bib118),
    [119](#bib.bib119)]。编码器生成一个嵌入向量，解码器进一步将形状分解为 2D 轮廓图像和 1D 包络数组。在推理过程中，通过搜索 2D 受限草图库生成
    CAD 数据，挤出这些草图，并通过布尔运算将它们组合起来，构建最终的 CAD 模型。对 ABC 数据集的评估表明，该方法在目标形状的逼近程度上优于 DeepCAD
    方法。
- en: Concurrent to the two previous works, ComplexGen [[34](#bib.bib34)] has introduced
    a new approach, ComplexNet, for directly generating B-Rep data from point clouds.
    This approach reframes the reconstruction task as a holistic detection of geometric
    primitives and their interconnections, encapsulated within a chain complex structure.
    ComplexNet, as a neural network architecture, harnesses a sparse CNN for embedding
    point cloud features and a tri-path Transformer decoder to produce three distinct
    groups of geometric primitives and their mutual relationships defined as adjacency
    matrices. Subsequently, a global optimization step refines the predicted probabilistic
    structure into a definite B-Rep chain complex, considering structural validity
    constraints and geometric refinements. Extensive experiments on the ABC dataset
    demonstrate the effectiveness of this approach in generating structurally complete
    and accurate CAD B-Rep models.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的两个工作并行，ComplexGen [[34](#bib.bib34)] 引入了一种新的方法，ComplexNet，用于直接从点云生成 B-Rep
    数据。这种方法将重建任务重新定义为几何基元及其相互连接的整体检测，这些都被封装在链复杂结构中。ComplexNet 作为一种神经网络架构，利用稀疏 CNN
    来嵌入点云特征，并使用三路径 Transformer 解码器生成三组不同的几何基元及其相互关系，这些关系定义为邻接矩阵。随后，一个全局优化步骤将预测的概率结构细化为确定的
    B-Rep 链复杂结构，同时考虑结构有效性约束和几何细化。在 ABC 数据集上进行的大量实验展示了这种方法在生成结构完整且准确的 CAD B-Rep 模型方面的有效性。
- en: VI Discussion and Future Works
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 讨论与未来工作
- en: Although Geometric Deep Learning (GDL) methods have made remarkable progress
    in analyzing CAD models and automating the design process at different levels,
    several challenges remain in this field. In this section, some of these challenges
    are discussed and the potential future research directions for tackling them are
    proposed.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管几何深度学习（GDL）方法在分析 CAD 模型和自动化设计过程的不同层面上取得了显著进展，但该领域仍面临一些挑战。在这一部分，讨论了这些挑战，并提出了应对这些挑战的潜在未来研究方向。
- en: 'Limited Annotated B-Rep Data: Despite the release of several large-scale CAD
    datasets, which include B-Rep format alongside conventional 3D data formats in
    recent years, there remains a considerable demand for annotated datasets for supervised
    learning-based approaches. The datasets annotated for CAD model classification
    are still limited in size, lacking the diversity and complexity needed for a comprehensive
    analysis of CAD models. While there are large-scale annotated datasets for common
    object CAD shapes, like ShapeNet [[62](#bib.bib62)], these datasets are only available
    in mesh format. Consequently, transferring knowledge from common object datasets
    to mechanical datasets is only viable when using mesh or point cloud data, not
    B-Rep. On the other hand, annotating mechanical CAD models is challenging and
    requires domain knowledge expertise to recognize the type, functionality, or other
    properties of mechanical objects. Inconsistencies in expert annotations may arise
    due to different terminologies used for these objects in different industries.
    Similar challenges exist for datasets annotated for CAD segmentation into different
    faces. As shown in Table [III](#S3.T3 "TABLE III ‣ III Datasets ‣ Geometric Deep
    Learning for Computer-Aided Design: A Survey"), there are a few annotated datasets
    in this regard with different types of annotation, such as how different faces
    are manufactured or the CAD operation used to construct each face. These datasets
    lack shared classes in their surface types, making it challenging to transfer
    learned features from one dataset to another using transfer learning or domain
    generalization approaches. More extensive datasets, like MFCAD++ extending the
    MFCAD dataset, are needed in this domain, with different annotation types. Collecting
    large-scale CAD datasets that provide various data formats, including mesh, point
    cloud, and B-Rep, featuring a diverse range of CAD models with various complexities,
    and annotating them for different tasks, such as CAD classification and segmentation,
    will significantly contribute to future research and development in this field.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '限量注释的 B-Rep 数据：尽管近年来发布了几种大规模的 CAD 数据集，这些数据集包含 B-Rep 格式以及传统的 3D 数据格式，但对用于监督学习的方法的注释数据集仍然有相当大的需求。用于
    CAD 模型分类的注释数据集仍然很有限，缺乏进行全面分析所需的多样性和复杂性。虽然针对常见物体 CAD 形状（如 ShapeNet [[62](#bib.bib62)]）的大规模注释数据集存在，但这些数据集仅以网格格式提供。因此，将知识从常见物体数据集转移到机械数据集时，只能使用网格或点云数据，而不是
    B-Rep。另一方面，注释机械 CAD 模型具有挑战性，需要领域知识的专业性来识别机械物体的类型、功能或其他属性。由于不同工业中对这些物体使用不同的术语，专家注释中的不一致性可能会出现。类似的挑战也存在于
    CAD 分割数据集中，这些数据集对不同的面进行注释。如表 [III](#S3.T3 "TABLE III ‣ III Datasets ‣ Geometric
    Deep Learning for Computer-Aided Design: A Survey") 所示，相关的注释数据集数量较少，注释类型各异，例如不同面的制造方式或用于构建每个面的
    CAD 操作。这些数据集在其表面类型中缺乏共享类别，使得使用迁移学习或领域泛化方法将学习到的特征从一个数据集转移到另一个数据集变得困难。这个领域需要更大规模的数据集，例如扩展
    MFCAD 数据集的 MFCAD++，以及不同类型的注释。收集提供多种数据格式（包括网格、点云和 B-Rep）的大规模 CAD 数据集，涵盖各种复杂性的 CAD
    模型，并对不同任务进行注释，例如 CAD 分类和分割，将显著推动该领域未来的研究和发展。'
- en: 'Analysis on Complex CAD Assemblies: Existing works for analyzing CAD assemblies
    are focusing on how to connect simple parts (solids), predicting the joint coordinates
    and direction of the connection. A future research direction could be considering
    how to assemble multiple solids hierarchicaly to make a more complex CAD model,
    or how to connect two sub-assemblies, each containing multiple parts (solids),
    to make a bigger assembly. However, the existing datasets are not annotated for
    such applications, since the joints between different parts are not explicitly
    specified in the B-Rep data by default. The AutoMate and Fusion 360 Assembly-Joint
    datasets are annotated specifically for their corresponding methods [[19](#bib.bib19),
    [18](#bib.bib18)], respectively. Another open problem in analyzing complex CAD
    assemblies is to segment each assembly into its building block in different levels
    (either sub-assembly or part (solid)). Learning how to replace a part in an assembly,
    while considering all its connections to other parts, its complexity, functionality,
    materials, etc, with another part in the repository would be a great advantage
    in automating CAD models customization process. Annotating datasets for this goal
    is also necessary.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对复杂CAD装配体的分析：现有对CAD装配体的分析工作主要集中在如何连接简单部件（固体），预测连接的坐标和方向。未来的研究方向可以考虑如何分层装配多个固体，以构建更复杂的CAD模型，或者如何连接两个包含多个部件（固体）的子装配体，以形成更大的装配体。然而，现有的数据集并未为此类应用进行标注，因为不同部件之间的接头在B-Rep数据中默认情况下并未明确指定。AutoMate和Fusion
    360 Assembly-Joint数据集分别为其对应的方法进行了标注[[19](#bib.bib19), [18](#bib.bib18)]。分析复杂CAD装配体的另一个开放问题是将每个装配体分割成不同级别的构建块（子装配体或部件（固体））。学习如何在考虑所有连接、复杂性、功能、材料等因素的情况下，将一个部件替换为库中的另一个部件，将极大地有利于自动化CAD模型的定制过程。为此目标标注数据集也是必要的。
- en: 'Representation Learning on B-Rep Data: The initial and the most important step
    in training deep learning models on B-Rep data involves creating numerical feature
    representations in a format suitable for deep learning architectures. Many methods
    for CAD classification and segmentation, particularly those based on UV-Net, use
    UV-grid sampling. However, UV-grid features lack permutation invariance. In other
    words, when the CAD solid is represented as a face adjacency graph, the arrangement
    of nodes in the graph is crucial for the deep learning model to recognize the
    graph structure and its similarity to other graphs (or CAD solids), while two
    graphs with different node arrangements might still be similar together. Hence,
    exploring various invariances of the B-Rep graph and alternative methods of representing
    its data in a structured format, while considering the relative orientation of
    the solid faces and edges, holds promise for future research.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: B-Rep数据的表示学习：在B-Rep数据上训练深度学习模型的初步且最重要的一步是创建适合深度学习架构的数值特征表示。许多用于CAD分类和分割的方法，尤其是基于UV-Net的方法，使用UV网格采样。然而，UV网格特征缺乏置换不变性。换句话说，当CAD固体被表示为一个面邻接图时，图中节点的排列对于深度学习模型识别图结构及其与其他图（或CAD固体）的相似性至关重要，而两个节点排列不同的图仍可能在一起相似。因此，探索B-Rep图的各种不变性和将其数据以结构化格式表示的替代方法，同时考虑固体面和边的相对方向，对未来的研究充满了希望。
- en: 'Unsupervised and Self-Supervised Methods: Given the shortage of annotated data
    for supervised learning in CAD, there is significant potential to enhance self-supervised
    and/or unsupervised methods, leveraging large-scale datasets like ABC. For instance,
    UV-Net utilized Graph Contrastive Learning (GCL) for self-supervised learning,
    and [[13](#bib.bib13)] explored shape rasterization to get the self-supervision
    from data for training an autoencoder and use the decoder for other supervised
    tasks. Investigating diverse forms of self-supervision in CAD data for training
    autoencoders or graph autoencoders would be intriguing. Examining and assessing
    different graph transformations in GCL approaches to understand the structure
    of B-Rep graph locally and globally, maintaining various invariances, is a crucial
    avenue for future research. Additionally, focusing on Variational Autoencoders
    for direct synthesis on B-Rep data, generating meaningful new shapes from the
    same distribution as existing shapes, would enhance similarity analysis and CAD
    retrieval.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督和自监督方法：鉴于CAD中注释数据的短缺，有很大潜力提升自监督和/或无监督方法，利用像ABC这样的规模大数据集。例如，UV-Net利用图对比学习（GCL）进行自监督学习，[[13](#bib.bib13)]探讨了形状光栅化从数据中获得自监督以训练自编码器，并使用解码器进行其他监督任务。在CAD数据中研究各种自监督形式以训练自编码器或图自编码器将是非常有趣的。研究和评估GCL方法中不同的图形变换，以了解B-Rep图的局部和全局结构，并保持各种不变性，是未来研究的重要方向。此外，关注变分自编码器在B-Rep数据上的直接合成，从与现有形状相同的分布中生成有意义的新形状，将增强相似性分析和CAD检索。
- en: 'CAD Generation and ̵‌B-Rep Synthesis: The current generative methods for CAD
    commands often focus on a limited set of operations, like Sketch and Extrude,
    restricting the complexity and diversity of the resulting CAD models. Notably,
    there is no assurance in certain methods, such as DeepCAD [[17](#bib.bib17)],
    that all generated CAD command sequences will yield topologically valid CAD models,
    particularly in cases of complex models with long command sequences. Therefore,
    an avenue for future exploration involves expanding generative methods to encompass
    a broader range of CAD operations, such as fillet and chamfer, allowing the generation
    of command sequences for more complex CAD shapes. Additionally, advancements in
    methods like SolidGen [[15](#bib.bib15)] and CADParser [[16](#bib.bib16)], which
    directly focus on synthesizing B-Rep data without relying on CAD command sequences,
    mark a promising direction. Despite these advancements, there is still room for
    creativity and improvement in this area. Another direction for future research
    involves extending generative methods to produce CAD construction operations or
    B-Rep synthesis from other 3D data formats, like mesh, voxel and point cloud.
    Recent progress in converting point clouds to CAD models, described in Section
    [V-E](#S5.SS5 "V-E 3D CAD Generation from Point Cloud ‣ V CAD Construction with
    Generative Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey"), opens possibilities for transferring knowledge across different data
    domains. Moreover, generating 3D CAD models from noisy scans or hand-drawn sketches
    is in high demand in CAD tools and presents significant potential for improvement.
    Although some methods have been introduced in this domain, the lack of a paired
    sketch and B-Rep dataset makes training supervised learning methods currently
    infeasible. For example, the sketches of SketchGraphs and 3D models in ABC datasets
    are both collected from Onshape repository. However, the 2D and 3D models in these
    two datasets are not paired. Collecting such a paired dataset would greatly aid
    future research.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 'CAD 生成和 B-Rep 合成：当前的 CAD 命令生成方法通常关注于有限的一组操作，如 Sketch 和 Extrude，这限制了生成的 CAD
    模型的复杂性和多样性。值得注意的是，某些方法（如 DeepCAD [[17](#bib.bib17)]）不能保证所有生成的 CAD 命令序列都会产生拓扑有效的
    CAD 模型，特别是在长命令序列的复杂模型中。因此，未来的研究方向之一是扩展生成方法，以涵盖更广泛的 CAD 操作，如 fillet 和 chamfer，从而生成用于更复杂
    CAD 形状的命令序列。此外，直接关注合成 B-Rep 数据而不依赖于 CAD 命令序列的方法，如 SolidGen [[15](#bib.bib15)]
    和 CADParser [[16](#bib.bib16)]，标志着一个有前景的方向。尽管有这些进展，但在这一领域仍有创造性和改进的空间。另一个未来研究的方向是扩展生成方法，从其他
    3D 数据格式（如 mesh、voxel 和 point cloud）生成 CAD 构建操作或 B-Rep 合成。最近在将点云转换为 CAD 模型方面的进展（在第
    [V-E](#S5.SS5 "V-E 3D CAD Generation from Point Cloud ‣ V CAD Construction with
    Generative Deep Learning ‣ Geometric Deep Learning for Computer-Aided Design:
    A Survey") 节中描述）为跨不同数据领域传递知识提供了可能性。此外，从噪声扫描或手绘草图生成 3D CAD 模型在 CAD 工具中需求很高，并具有显著的改进潜力。尽管在这个领域已经引入了一些方法，但缺乏配对的草图和
    B-Rep 数据集使得目前训练监督学习方法不可行。例如，SketchGraphs 的草图和 ABC 数据集中的 3D 模型均来自 Onshape 存储库。然而，这两个数据集中的
    2D 和 3D 模型并未配对。收集这样的配对数据集将极大地促进未来的研究。'
- en: 'Reproducibility: In this domain, a significant challenge lies in reproducing
    and comparing the experimental results of different methodologies. The absence
    of large-scale annotated benchmark datasets leads each method proposed for machine
    learning-based CAD analysis to either introduce a new annotated CAD dataset tailored
    for the specific task or modify and annotate a portion of a large-scale dataset
    for evaluation. Supervised methods trained on these smaller datasets often exhibit
    high performance, leaving little room for improvement. Moreover, since each method
    is benchmarked on a dataset adapted for its specific task, comparing results becomes
    complex. For instance, when analyzing the results of different methods evaluated
    on a subset of the Fusion 360 segmentation dataset in [[13](#bib.bib13)], it is
    unclear which part of the dataset each method used for evaluation. Reproducing
    the results of different methods and conducting performance comparisons become
    especially challenging when researchers do not release their preprocessed data
    or do not provide clear preprocessing instructions. Another hurdle in replicating
    research results is the reliance of codes on CAD kernels that are not open-source.
    For instance, replicating the results of AutoMate [[19](#bib.bib19)] requires
    installing the Parasolid kernel, which is typically accessible only to industrial
    developers and companies. This limited availability makes it challenging for independent
    academic researchers to utilize and build upon such research works. While codes
    dependent on the open-source OpenCASCADE kernel are more accessible, many methods
    in this field either use constraint solvers through CAD tools during training
    or are in a way reliant on a CAD tool kernel, necessitating access and licensing
    for at least one CAD software. To address these issues, it is highly recommended
    that researchers provide comprehensive documentation for their released codes,
    detailing the data preprocessing setup and offering sufficient information on
    the experimental setting and code dependencies. This transparency can significantly
    facilitate future research efforts in the field.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 可重复性：在这一领域，一个显著的挑战在于重复和比较不同方法的实验结果。由于缺乏大规模注释的基准数据集，每个用于机器学习基础的计算机辅助设计（CAD）分析的方法都要么引入一个针对特定任务的新注释CAD数据集，要么修改并注释一个大规模数据集的一部分以进行评估。训练于这些较小数据集上的监督方法通常表现出高性能，几乎没有改进的空间。此外，由于每种方法都是在针对其特定任务调整的数据集上进行基准测试的，结果比较变得复杂。例如，在[[13](#bib.bib13)]中，当分析在Fusion
    360分割数据集的一个子集上评估的不同方法的结果时，不清楚每种方法使用了数据集的哪一部分进行评估。当研究人员没有发布其预处理数据或没有提供明确的预处理说明时，重复不同方法的结果并进行性能比较变得尤其具有挑战性。另一个重复研究结果的障碍是代码对不可开源的CAD内核的依赖。例如，重复AutoMate
    [[19](#bib.bib19)]的结果需要安装Parasolid内核，这通常只有工业开发人员和公司才能访问。这种有限的可用性使独立的学术研究人员难以利用并在此类研究工作基础上进行开发。虽然依赖于开源OpenCASCADE内核的代码更为可访问，但该领域的许多方法要么在训练过程中通过CAD工具使用约束求解器，要么在某种程度上依赖于CAD工具内核，从而需要获取和许可至少一种CAD软件。为了解决这些问题，强烈建议研究人员为其发布的代码提供全面的文档，详细说明数据预处理设置，并提供足够的信息关于实验设置和代码依赖。这种透明度可以显著促进该领域未来的研究工作。
- en: VII Conclusion
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: 'Developing and training Geometric Deep Learning (GDL) models to learn and reason
    about CAD designs holds the promise of revolutionizing design workflows, bringing
    in greater efficiency. However, extending machine learning-based methods to complex
    parametric data, like B-Rep, poses a crucial and challenging task. This survey
    offers a comprehensive review of GDL methods tailored for CAD data analysis. It
    presents detailed explanation, comparisons, and summaries within two primary categories:
    1) CAD representation learning, encompassing supervised and self-supervised methods
    designed for CAD classification, segmentation, assembly, and retrieval, and 2)
    CAD generation, involving generative methods for 2D and 3D CAD construction. Additionally,
    benchmark datasets and open-source codes are introduced. The survey concludes
    by discussing the challenges inherent in this rapidly evolving field and proposes
    potential avenues for future research.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 开发和训练几何深度学习（GDL）模型以学习和推理CAD设计有望彻底改变设计工作流程，提高效率。然而，将基于机器学习的方法扩展到复杂的参数数据，如B-Rep，是一个关键且具有挑战性的任务。本调查提供了针对CAD数据分析的GDL方法的全面综述。它详细解释、比较和总结了两个主要类别：1)
    CAD表示学习，包括用于CAD分类、分割、装配和检索的监督和自监督方法，2) CAD生成，涉及用于2D和3D CAD构建的生成方法。此外，还介绍了基准数据集和开源代码。调查的最后讨论了该快速发展的领域中的挑战，并提出了未来研究的潜在方向。
- en: Acknowledgement
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The research leading to the results of this paper received funding from the
    Thomas B. Thriges Foundation and the Industriens Foundation as part of the AI
    Supported Modular Design and Implementation project.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文结果的研究得到了Thomas B. Thriges基金会和Industriens基金会的资助，作为AI支持的模块化设计与实施项目的一部分。
- en: References
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veličković, “Geometric deep
    learning: Grids, groups, graphs, geodesics, and gauges,” *arXiv preprint arXiv:2104.13478*,
    2021.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. M. Bronstein, J. Bruna, T. Cohen, 和 P. Veličković, “几何深度学习：网格、群组、图形、测地线和测量，”
    *arXiv预印本 arXiv:2104.13478*，2021年。'
- en: '[2] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri, “3d shape segmentation
    with projective convolutional networks,” in *IEEE conference on computer vision
    and pattern recognition*, 2017.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] E. Kalogerakis, M. Averkiou, S. Maji, 和 S. Chaudhuri, “用投影卷积网络进行3D形状分割，”
    发表在 *IEEE计算机视觉与模式识别会议*，2017年。'
- en: '[3] Y. Feng, Y. Feng, H. You, X. Zhao, and Y. Gao, “Meshnet: Mesh neural network
    for 3d shape representation,” in *AAAI conference on artificial intelligence*,
    2019.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Feng, Y. Feng, H. You, X. Zhao, 和 Y. Gao, “Meshnet: 用于3D形状表示的网格神经网络，”
    发表在 *AAAI人工智能会议*，2019年。'
- en: '[4] D. Maturana and S. Scherer, “Voxnet: A 3d convolutional neural network
    for real-time object recognition,” in *IEEE International Conference on Intelligent
    Robots and Systems (IROS)*, 2015.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] D. Maturana 和 S. Scherer, “Voxnet: 一种用于实时物体识别的3D卷积神经网络，” 发表在 *IEEE国际智能机器人与系统会议（IROS）*，2015年。'
- en: '[5] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d shapenets:
    A deep representation for volumetric shapes,” in *IEEE conference on computer
    vision and pattern recognition*, 2015.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, 和 J. Xiao, “3d shapenets:
    深度体积形状表示，” 发表在 *IEEE计算机视觉与模式识别会议*，2015年。'
- en: '[6] C. Wang, M. Cheng, F. Sohel, M. Bennamoun, and J. Li, “Normalnet: A voxel-based
    cnn for 3d object classification and retrieval,” *Neurocomputing*, vol. 323, pp.
    139–147, 2019.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C. Wang, M. Cheng, F. Sohel, M. Bennamoun, 和 J. Li, “Normalnet: 一种用于3D物体分类和检索的体素基CNN，”
    *Neurocomputing*, vol. 323, pp. 139–147, 2019年。'
- en: '[7] T. Le and Y. Duan, “Pointgrid: A deep network for 3d shape understanding,”
    in *IEEE conference on computer vision and pattern recognition*, 2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] T. Le 和 Y. Duan, “Pointgrid: 用于3D形状理解的深度网络，” 发表在 *IEEE计算机视觉与模式识别会议*，2018年。'
- en: '[8] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on point
    sets for 3d classification and segmentation,” in *IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas, “Pointnet: 基于点集的深度学习用于3D分类和分割，”
    发表在 *IEEE计算机视觉与模式识别会议*，2017年。'
- en: '[9] P. K. Jayaraman, A. Sanghi, J. G. Lambourne, K. D. Willis, T. Davies, H. Shayani,
    and N. Morris, “Uv-net: Learning from boundary representations,” in *IEEE Conference
    on Computer Vision and Pattern Recognition*, 2021.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. K. Jayaraman, A. Sanghi, J. G. Lambourne, K. D. Willis, T. Davies, H. Shayani,
    和 N. Morris, “Uv-net: 从边界表示中学习，” 发表在 *IEEE计算机视觉与模式识别会议*，2021年。'
- en: '[10] C. Krahe, A. Bräunche, A. Jacob, N. Stricker, and G. Lanza, “Deep learning
    for automated product design,” *CIRP Design Conference*, 2020.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] C. Krahe, A. Bräunche, A. Jacob, N. Stricker 和 G. Lanza, “用于自动化产品设计的深度学习，”
    *CIRP 设计会议*，2020年。'
- en: '[11] C. Krahe, M. Marinov, T. Schmutz, Y. Hermann, M. Bonny, M. May, and G. Lanza,
    “Ai based geometric similarity search supporting component reuse in engineering
    design,” *CIRP Design Conference*, 2022.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] C. Krahe, M. Marinov, T. Schmutz, Y. Hermann, M. Bonny, M. May 和 G. Lanza,
    “基于 AI 的几何相似性搜索支持工程设计中的组件重用，” *CIRP 设计会议*，2022年。'
- en: '[12] D. Machalica and M. Matyjewski, “Cad models clustering with machine learning,”
    *Archive of Mechanical Engineering*, vol. 66, no. 2, 2019.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] D. Machalica 和 M. Matyjewski, “利用机器学习进行 CAD 模型聚类，” *机械工程档案*，第66卷，第2期，2019年。'
- en: '[13] B. T. Jones, M. Hu, M. Kodnongbua, V. G. Kim, and A. Schulz, “Self-supervised
    representation learning for cad,” in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] B. T. Jones, M. Hu, M. Kodnongbua, V. G. Kim 和 A. Schulz, “用于 CAD 的自监督表示学习，”
    见 *IEEE 计算机视觉与模式识别会议*，2023年。'
- en: '[14] J. G. Lambourne, K. D. Willis, P. K. Jayaraman, A. Sanghi, P. Meltzer,
    and H. Shayani, “Brepnet: A topological message passing system for solid models,”
    in *IEEE Conference on Computer Vision and Pattern Recognition*, 2021.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. G. Lambourne, K. D. Willis, P. K. Jayaraman, A. Sanghi, P. Meltzer
    和 H. Shayani, “Brepnet: 一个用于实体模型的拓扑信息传递系统，” 见 *IEEE 计算机视觉与模式识别会议*，2021年。'
- en: '[15] P. K. Jayaraman, J. G. Lambourne, N. Desai, K. D. D. Willis, A. Sanghi,
    and N. J. W. Morris, “Solidgen: An autoregressive model for direct b-rep synthesis,”
    *Transactions on Machine Learning Research*, 2023.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] P. K. Jayaraman, J. G. Lambourne, N. Desai, K. D. D. Willis, A. Sanghi
    和 N. J. W. Morris, “Solidgen: 一种用于直接 B-rep 合成的自回归模型，” *机器学习研究汇刊*，2023年。'
- en: '[16] S. Zhou, T. Tang, and B. Zhou, “Cadparser: a learning approach of sequence
    modeling for b-rep cad,” in *International Joint Conference on Artificial Intelligence*,
    2023.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Zhou, T. Tang 和 B. Zhou, “Cadparser: 一种用于 B-rep CAD 的序列建模学习方法，” 见 *国际人工智能联合会议*，2023年。'
- en: '[17] R. Wu, C. Xiao, and C. Zheng, “Deepcad: A deep generative network for
    computer-aided design models,” in *IEEE International Conference on Computer Vision*,
    2021.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] R. Wu, C. Xiao 和 C. Zheng, “Deepcad: 一种用于计算机辅助设计模型的深度生成网络，” 见 *IEEE 国际计算机视觉会议*，2021年。'
- en: '[18] K. D. Willis, P. K. Jayaraman, H. Chu, Y. Tian, Y. Li, D. Grandi, A. Sanghi,
    L. Tran, J. G. Lambourne, A. Solar-Lezama *et al.*, “Joinable: Learning bottom-up
    assembly of parametric cad joints,” in *IEEE Conference on Computer Vision and
    Pattern Recognition*, 2022.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] K. D. Willis, P. K. Jayaraman, H. Chu, Y. Tian, Y. Li, D. Grandi, A. Sanghi,
    L. Tran, J. G. Lambourne 和 A. Solar-Lezama *等*， “Joinable: 学习参数化 CAD 接头的自下而上的组装，”
    见 *IEEE 计算机视觉与模式识别会议*，2022年。'
- en: '[19] B. Jones, D. Hildreth, D. Chen, I. Baran, V. G. Kim, and A. Schulz, “Automate:
    A dataset and learning approach for automatic mating of cad assemblies,” *ACM
    Transactions on Graphics (TOG)*, vol. 40, no. 6, pp. 1–18, 2021.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] B. Jones, D. Hildreth, D. Chen, I. Baran, V. G. Kim 和 A. Schulz, “Automate:
    用于自动配对 CAD 装配的数据库和学习方法，” *ACM 图形学汇刊 (TOG)*，第40卷，第6期，页码 1–18，2021年。'
- en: '[20] A. Seff, Y. Ovadia, W. Zhou, and R. P. Adams, “Sketchgraphs: A large-scale
    dataset for modeling relational geometry in computer-aided design,” *arXiv preprint
    arXiv:2007.08506*, 2020.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Seff, Y. Ovadia, W. Zhou 和 R. P. Adams, “Sketchgraphs: 一个用于建模计算机辅助设计中的关系几何的大规模数据集，”
    *arXiv 预印本 arXiv:2007.08506*，2020年。'
- en: '[21] K. D. Willis, P. K. Jayaraman, J. G. Lambourne, H. Chu, and Y. Pu, “Engineering
    sketch generation for computer-aided design,” in *IEEE Conference on Computer
    Vision and Pattern Recognition*, 2021.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] K. D. Willis, P. K. Jayaraman, J. G. Lambourne, H. Chu 和 Y. Pu, “用于计算机辅助设计的工程草图生成，”
    见 *IEEE 计算机视觉与模式识别会议*，2021年。'
- en: '[22] C. Li, H. Pan, A. Bousseau, and N. J. Mitra, “Free2cad: Parsing freehand
    drawings into cad commands,” *ACM Transactions on Graphics (TOG)*, vol. 41, no. 4,
    pp. 1–16, 2022.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] C. Li, H. Pan, A. Bousseau 和 N. J. Mitra, “Free2cad: 将自由手绘图转换为 CAD 命令，”
    *ACM 图形学汇刊 (TOG)*，第41卷，第4期，页码 1–16，2022年。'
- en: '[23] A. Seff, W. Zhou, N. Richardson, and R. P. Adams, “Vitruvion: A generative
    model of parametric CAD sketches,” in *International Conference on Learning Representations,
    ICLR*, 2022.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Seff, W. Zhou, N. Richardson 和 R. P. Adams, “Vitruvion: 一种参数化 CAD 草图的生成模型，”
    见 *国际学习表示会议，ICLR*，2022年。'
- en: '[24] C. Li, H. Pan, A. Bousseau, and N. J. Mitra, “Sketch2cad: Sequential cad
    modeling by sketching in context,” *ACM Transactions on Graphics (TOG)*, vol. 39,
    no. 6, pp. 1–14, 2020.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] C. Li, H. Pan, A. Bousseau 和 N. J. Mitra, “Sketch2cad: 通过上下文草图进行顺序 CAD
    建模，” *ACM 图形学汇刊 (TOG)*，第39卷，第6期，页码 1–14，2020年。'
- en: '[25] E. Dupont, K. Cherenkova, A. Kacem, S. A. Ali, I. Arzhannikov, G. Gusev,
    and D. Aouada, “Cadops-net: Jointly learning cad operation types and steps from
    boundary-representations,” in *International Conference on 3D Vision (3DV)*.   IEEE,
    2022.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] E. Dupont, K. Cherenkova, A. Kacem, S. A. Ali, I. Arzhannikov, G. Gusev,
    和 D. Aouada, “Cadops-net: 从边界表示中联合学习CAD操作类型和步骤”，发表于*International Conference on
    3D Vision (3DV)*，IEEE，2022年。'
- en: '[26] W. Para, S. Bhat, P. Guerrero, T. Kelly, N. Mitra, L. J. Guibas, and P. Wonka,
    “Sketchgen: Generating constrained cad sketches,” *Advances in Neural Information
    Processing Systems*, vol. 34, pp. 5077–5088, 2021.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] W. Para, S. Bhat, P. Guerrero, T. Kelly, N. Mitra, L. J. Guibas, 和 P.
    Wonka, “Sketchgen: 生成约束CAD草图”，*Advances in Neural Information Processing Systems*，第34卷，页码5077–5088，2021年。'
- en: '[27] Y. Ganin, S. Bartunov, Y. Li, E. Keller, and S. Saliceti, “Computer-aided
    design as language,” *Advances in Neural Information Processing Systems*, vol. 34,
    pp. 5885–5897, 2021.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Ganin, S. Bartunov, Y. Li, E. Keller, 和 S. Saliceti, “计算机辅助设计作为语言”，*Advances
    in Neural Information Processing Systems*，第34卷，页码5885–5897，2021年。'
- en: '[28] F. Hähnlein, C. Li, N. J. Mitra, and A. Bousseau, “Cad2sketch: Generating
    concept sketches from cad sequences,” *ACM Transactions on Graphics (TOG)*, vol. 41,
    no. 6, pp. 1–18, 2022.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] F. Hähnlein, C. Li, N. J. Mitra, 和 A. Bousseau, “Cad2sketch: 从CAD序列生成概念草图”，*ACM
    Transactions on Graphics (TOG)*，第41卷，第6期，页码1–18，2022年。'
- en: '[29] X. Xu, W. Peng, C.-Y. Cheng, K. D. Willis, and D. Ritchie, “Inferring
    cad modeling sequences using zone graphs,” in *IEEE conference on computer vision
    and pattern recognition*, 2021.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] X. Xu, W. Peng, C.-Y. Cheng, K. D. Willis, 和 D. Ritchie, “使用区域图推断CAD建模序列”，发表于*IEEE
    Conference on Computer Vision and Pattern Recognition*，2021年。'
- en: '[30] K. D. Willis, Y. Pu, J. Luo, H. Chu, T. Du, J. G. Lambourne, A. Solar-Lezama,
    and W. Matusik, “Fusion 360 gallery: A dataset and environment for programmatic
    cad construction from human design sequences,” *ACM Transactions on Graphics (TOG)*,
    vol. 40, no. 4, pp. 1–24, 2021.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] K. D. Willis, Y. Pu, J. Luo, H. Chu, T. Du, J. G. Lambourne, A. Solar-Lezama,
    和 W. Matusik, “Fusion 360画廊: 用于从人类设计序列进行程序化CAD构建的数据集和环境”，*ACM Transactions on
    Graphics (TOG)*，第40卷，第4期，页码1–24，2021年。'
- en: '[31] X. Xu, K. D. D. Willis, J. G. Lambourne, C. Cheng, P. K. Jayaraman, and
    Y. Furukawa, “Skexgen: Autoregressive generation of CAD construction sequences
    with disentangled codebooks,” in *International Conference on Machine Learning*,
    2022.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] X. Xu, K. D. D. Willis, J. G. Lambourne, C. Cheng, P. K. Jayaraman, 和
    Y. Furukawa, “Skexgen: 使用解耦的代码本自回归生成CAD构建序列”，发表于*International Conference on Machine
    Learning*，2022年。'
- en: '[32] M. A. Uy, Y.-Y. Chang, M. Sung, P. Goel, J. G. Lambourne, T. Birdal, and
    L. J. Guibas, “Point2cyl: Reverse engineering 3d objects from point clouds to
    extrusion cylinders,” in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2022.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. A. Uy, Y.-Y. Chang, M. Sung, P. Goel, J. G. Lambourne, T. Birdal, 和
    L. J. Guibas, “Point2cyl: 从点云到挤出圆柱的3D对象逆向工程”，发表于*IEEE Conference on Computer Vision
    and Pattern Recognition*，2022年。'
- en: '[33] J. G. Lambourne, K. Willis, P. K. Jayaraman, L. Zhang, A. Sanghi, and
    K. R. Malekshan, “Reconstructing editable prismatic cad from rounded voxel models,”
    in *SIGGRAPH Asia 2022 Conference Papers*, 2022.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. G. Lambourne, K. Willis, P. K. Jayaraman, L. Zhang, A. Sanghi, 和 K.
    R. Malekshan, “从圆形体素模型重建可编辑棱柱体CAD”，发表于*SIGGRAPH Asia 2022 Conference Papers*，2022年。'
- en: '[34] H. Guo, S. Liu, H. Pan, Y. Liu, X. Tong, and B. Guo, “Complexgen: Cad
    reconstruction by b-rep chain complex generation,” *ACM Transactions on Graphics
    (TOG)*, vol. 41, no. 4, pp. 1–18, 2022.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] H. Guo, S. Liu, H. Pan, Y. Liu, X. Tong, 和 B. Guo, “Complexgen: 通过B-REP链复合体生成进行CAD重建”，*ACM
    Transactions on Graphics (TOG)*，第41卷，第4期，页码1–18，2022年。'
- en: '[35] M. Groover and E. Zimmers, *CAD/CAM: computer-aided design and manufacturing*.   Pearson
    Education, 1983.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] M. Groover 和 E. Zimmers, *CAD/CAM: 计算机辅助设计与制造*。Pearson Education，1983年。'
- en: '[36] Autodesk, “Autocad,” 1982\. [Online]. Available: [https://www.autodesk.com/products/autocad/overview?term=1-YEAR&tab=subscription](https://www.autodesk.com/products/autocad/overview?term=1-YEAR&tab=subscription)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Autodesk, “Autocad”，1982年。[在线]。可用链接：[https://www.autodesk.com/products/autocad/overview?term=1-YEAR&tab=subscription](https://www.autodesk.com/products/autocad/overview?term=1-YEAR&tab=subscription)'
- en: '[37] ——, “Fusion 360,” 2013\. [Online]. Available: [https://www.autodesk.com/products/fusion-360/overview?term=1-YEAR&tab=subscription](https://www.autodesk.com/products/fusion-360/overview?term=1-YEAR&tab=subscription)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] ——, “Fusion 360”，2013年。[在线]。可用链接：[https://www.autodesk.com/products/fusion-360/overview?term=1-YEAR&tab=subscription](https://www.autodesk.com/products/fusion-360/overview?term=1-YEAR&tab=subscription)'
- en: '[38] Dassault Systèmes, “Solidworks,” 1995\. [Online]. Available: [https://www.3ds.com/products/solidworks](https://www.3ds.com/products/solidworks)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Dassault Systèmes, “Solidworks”，1995年。[在线]。可用链接：[https://www.3ds.com/products/solidworks](https://www.3ds.com/products/solidworks)'
- en: '[39] ——, “Catia,” 1977\. [Online]. Available: [https://www.3ds.com/products-services/catia/](https://www.3ds.com/products-services/catia/)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] ——, “Catia,” 1977\. [在线]. 可用链接: [https://www.3ds.com/products-services/catia/](https://www.3ds.com/products-services/catia/)'
- en: '[40] PTC, “Onshape,” 2015\. [Online]. Available: [https://www.onshape.com/en/](https://www.onshape.com/en/)'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] PTC, “Onshape,” 2015\. [在线]. 可用链接: [https://www.onshape.com/en/](https://www.onshape.com/en/)'
- en: '[41] ——, “Creo,” 2011\. [Online]. Available: [https://www.ptc.com/en/products/creo](https://www.ptc.com/en/products/creo)'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] ——, “Creo,” 2011\. [在线]. 可用链接: [https://www.ptc.com/en/products/creo](https://www.ptc.com/en/products/creo)'
- en: '[42] Kai Backman, Mikko Mononen, “Tinkercad,” 2011\. [Online]. Available: [https://www.tinkercad.com](https://www.tinkercad.com)'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Kai Backman, Mikko Mononen, “Tinkercad,” 2011\. [在线]. 可用链接: [https://www.tinkercad.com](https://www.tinkercad.com)'
- en: '[43] Thomas Paviot, “pythonocc-core,” 2022\. [Online]. Available: [https://doi.org/10.5281/zenodo.3605364](https://doi.org/10.5281/zenodo.3605364)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Thomas Paviot, “pythonocc-core,” 2022\. [在线]. 可用链接: [https://doi.org/10.5281/zenodo.3605364](https://doi.org/10.5281/zenodo.3605364)'
- en: '[44] “Opencascade technology (occt),” 2002\. [Online]. Available: [https://github.com/AutodeskAILab/occwl](https://github.com/AutodeskAILab/occwl)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] “Opencascade技术 (occt),” 2002\. [在线]. 可用链接: [https://github.com/AutodeskAILab/occwl](https://github.com/AutodeskAILab/occwl)'
- en: '[45] Pradeep Kumar Jayaraman, Joseph Lambourne, “Occwl,” 2023\. [Online]. Available:
    [https://dev.opencascade.org](https://dev.opencascade.org)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Pradeep Kumar Jayaraman, Joseph Lambourne, “Occwl,” 2023\. [在线]. 可用链接:
    [https://dev.opencascade.org](https://dev.opencascade.org)'
- en: '[46] “Cadquery,” 2019\. [Online]. Available: [https://github.com/CadQuery/cadquery/tree/master](https://github.com/CadQuery/cadquery/tree/master)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] “Cadquery,” 2019\. [在线]. 可用链接: [https://github.com/CadQuery/cadquery/tree/master](https://github.com/CadQuery/cadquery/tree/master)'
- en: '[47] “Parasolid,” 1980\. [Online]. Available: [https://www.plm.automation.siemens.com/global/en/products/plm-components/parasolid.html](https://www.plm.automation.siemens.com/global/en/products/plm-components/parasolid.html)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] “Parasolid,” 1980\. [在线]. 可用链接: [https://www.plm.automation.siemens.com/global/en/products/plm-components/parasolid.html](https://www.plm.automation.siemens.com/global/en/products/plm-components/parasolid.html)'
- en: '[48] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE transactions on neural networks*, vol. 20,
    no. 1, pp. 61–80, 2008.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner 和 G. Monfardini, “图神经网络模型,”
    *IEEE神经网络学报*, 第20卷, 第1期, 第61–80页, 2008.'
- en: '[49] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive
    survey on graph neural networks,” *IEEE transactions on neural networks and learning
    systems*, vol. 32, no. 1, pp. 4–24, 2020.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang 和 S. Y. Philip, “关于图神经网络的综合调查,”
    *IEEE神经网络与学习系统学报*, 第32卷, 第1期, 第4–24页, 2020.'
- en: '[50] N. Heidari and A. Iosifidis, “Temporal attention-augmented graph convolutional
    network for efficient skeleton-based human action recognition,” in *International
    Conference on Pattern Recognition (ICPR)*.   IEEE, 2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] N. Heidari 和 A. Iosifidis, “用于高效骨架基础人体动作识别的时间注意力增强图卷积网络,” 发表在*国际模式识别大会
    (ICPR)*。IEEE, 2021.'
- en: '[51] ——, “Progressive spatio-temporal graph convolutional network for skeleton-based
    human action recognition,” in *IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP)*, 2021.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] ——, “用于骨架基础人体动作识别的渐进时空图卷积网络,” 发表在*IEEE国际声学、语音和信号处理大会 (ICASSP)*, 2021.'
- en: '[52] L. Hedegaard, N. Heidari, and A. Iosifidis, “Continual spatio-temporal
    graph convolutional networks,” *Pattern Recognition*, vol. 140, p. 109528, 2023.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] L. Hedegaard, N. Heidari 和 A. Iosifidis, “持续的时空图卷积网络,” *模式识别*, 第140卷,
    第109528页, 2023.'
- en: '[53] E. Mansimov, O. Mahmood, S. Kang, and K. Cho, “Molecular geometry prediction
    using a deep generative graph neural network,” *Scientific reports*, vol. 9, no. 1,
    p. 20381, 2019.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] E. Mansimov, O. Mahmood, S. Kang 和 K. Cho, “使用深度生成图神经网络进行分子几何预测,” *科学报告*,
    第9卷, 第1期, 第20381页, 2019.'
- en: '[54] Y. Wang, J. Wang, Z. Cao, and A. Barati Farimani, “Molecular contrastive
    learning of representations via graph neural networks,” *Nature Machine Intelligence*,
    vol. 4, no. 3, pp. 279–287, 2022.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Y. Wang, J. Wang, Z. Cao 和 A. Barati Farimani, “通过图神经网络的分子对比学习表示,” *自然机器智能*,
    第4卷, 第3期, 第279–287页, 2022.'
- en: '[55] K. Baltakys, M. Baltakienė, N. Heidari, A. Iosifidis, and J. Kanniainen,
    “Predicting the trading behavior of socially connected investors: Graph neural
    network approach with implications to market surveillance,” *Expert Systems with
    Applications*, vol. 228, p. 120285, 2023.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] K. Baltakys, M. Baltakienė, N. Heidari, A. Iosifidis 和 J. Kanniainen,
    “预测社会连接投资者的交易行为: 具有市场监控含义的图神经网络方法,” *专家系统应用*, 第228卷, 第120285页, 2023.'
- en: '[56] L. Yang, J. Zhuang, H. Fu, X. Wei, K. Zhou, and Y. Zheng, “Sketchgnn:
    Semantic sketch segmentation with graph neural networks,” *ACM Transactions on
    Graphics (TOG)*, vol. 40, no. 3, pp. 1–13, 2021.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] L. Yang, J. Zhuang, H. Fu, X. Wei, K. Zhou, 和 Y. Zheng，“SketchGNN：使用图神经网络进行语义草图分割”，*ACM图形学交易*（TOG），第40卷，第3期，页码1–13，2021。'
- en: '[57] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *International Conference on Learning Representations*,
    2017.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] T. N. Kipf 和 M. Welling，“图卷积网络的半监督分类”，在*国际学习表征会议*，2017。'
- en: '[58] N. Heidari and A. Iosifidis, “Progressive graph convolutional networks
    for semi-supervised node classification,” *IEEE Access*, vol. 9, pp. 81 957–81 968,
    2021.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] N. Heidari 和 A. Iosifidis，“用于半监督节点分类的渐进图卷积网络”，*IEEE Access*，第9卷，页码81,957–81,968，2021。'
- en: '[59] N. Heidari, L. Hedegaard, and A. Iosifidis, “Graph convolutional networks,”
    in *Deep Learning for Robot Perception and Cognition*.   Elsevier, 2022, pp. 71–99.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] N. Heidari, L. Hedegaard, 和 A. Iosifidis，“图卷积网络”，收录于*机器人感知与认知的深度学习*。Elsevier，2022，页码71–99。'
- en: '[60] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, “Learning deep
    generative models of graphs,” *arXiv preprint arXiv:1803.03324*, 2018.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, 和 P. Battaglia，“学习深度生成图模型”，*arXiv预印本
    arXiv:1803.03324*，2018。'
- en: '[61] P. Shilane, P. Min, M. Kazhdan, and T. Funkhouser, “The princeton shape
    benchmark,” in *Proceedings Shape Modeling Applications, 2004.*   IEEE, 2004,
    pp. 167–178.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] P. Shilane, P. Min, M. Kazhdan, 和 T. Funkhouser，“普林斯顿形状基准”，在*形状建模应用会议录，2004*。IEEE，2004，页码167–178。'
- en: '[62] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su *et al.*, “Shapenet: An information-rich 3d model repository,”
    *arXiv preprint arXiv:1512.03012*, 2015.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S.
    Savarese, M. Savva, S. Song, H. Su *等*，“Shapenet：一个信息丰富的3D模型库”，*arXiv预印本 arXiv:1512.03012*，2015。'
- en: '[63] Q. Zhou and A. Jacobson, “Thingi10k: A dataset of 10,000 3d-printing models,”
    *arXiv preprint arXiv:1605.04797*, 2016.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Q. Zhou 和 A. Jacobson，“Thingi10k：一个包含10,000个3D打印模型的数据集”，*arXiv预印本 arXiv:1605.04797*，2016。'
- en: '[64] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su,
    “Partnet: A large-scale benchmark for fine-grained and hierarchical part-level
    3d object understanding,” in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2019.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, 和 H. Su，“Partnet：一个大规模的细粒度和层次化部件级3D对象理解基准”，在*IEEE计算机视觉与模式识别会议*，2019。'
- en: '[65] F. Bogo, J. Romero, G. Pons-Moll, and M. J. Black, “Dynamic faust: Registering
    human bodies in motion,” in *IEEE conference on computer vision and pattern recognition*,
    2017, pp. 6233–6242.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] F. Bogo, J. Romero, G. Pons-Moll, 和 M. J. Black，“动态Faust：注册运动中的人体”，在*IEEE计算机视觉与模式识别会议*，2017，页码6233–6242。'
- en: '[66] M. Aubry, D. Maturana, A. A. Efros, B. C. Russell, and J. Sivic, “Seeing
    3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models,”
    in *IEEE conference on computer vision and pattern recognition*, 2014, pp. 3762–3769.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] M. Aubry, D. Maturana, A. A. Efros, B. C. Russell, 和 J. Sivic，“观察3D椅子：基于示例的2D-3D对齐，使用大型CAD模型数据集”，在*IEEE计算机视觉与模式识别会议*，2014，页码3762–3769。'
- en: '[67] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene understanding
    benchmark suite,” in *IEEE conference on computer vision and pattern recognition*,
    2015.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] S. Song, S. P. Lichtenberg, 和 J. Xiao，“Sun RGB-D：一个RGB-D场景理解基准套件”，在*IEEE计算机视觉与模式识别会议*，2015。'
- en: '[68] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *IEEE conference
    on computer vision and pattern recognition*, 2017.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, 和 M. Nießner，“ScanNet：丰富注释的室内场景3D重建”，在*IEEE计算机视觉与模式识别会议*，2017。'
- en: '[69] S. Jayanti, Y. Kalyanaraman, N. Iyer, and K. Ramani, “Developing an engineering
    shape benchmark for cad models,” *Computer-Aided Design*, vol. 38, no. 9, pp.
    939–953, 2006.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] S. Jayanti, Y. Kalyanaraman, N. Iyer, 和 K. Ramani，“为CAD模型开发工程形状基准”，*计算机辅助设计*，第38卷，第9期，页码939–953，2006。'
- en: '[70] S. Kim, H.-g. Chi, X. Hu, Q. Huang, and K. Ramani, “A large-scale annotated
    mechanical components benchmark for classification and retrieval tasks with deep
    neural networks,” in *European Conference on Computer Vision*.   Springer, 2020.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Kim, H.-g. Chi, X. Hu, Q. Huang, 和 K. Ramani，“一个大规模注释的机械部件基准，用于分类和检索任务，结合深度神经网络”，在*欧洲计算机视觉大会*。Springer，2020。'
- en: '[71] D. Bespalov, C. Y. Ip, W. C. Regli, and J. Shaffer, “Benchmarking cad
    search techniques,” in *ACM symposium on Solid and physical modeling*, 2005.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] D. Bespalov, C. Y. Ip, W. C. Regli, 和 J. Shaffer, “cad 搜索技术的基准测试，” 见 *ACM
    symposium on Solid and physical modeling*，2005 年。'
- en: '[72] Z. Zhang, P. Jaiswal, and R. Rai, “Featurenet: Machining feature recognition
    based on 3d convolution neural network,” *Computer-Aided Design*, vol. 101, pp.
    12–22, 2018.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Z. Zhang, P. Jaiswal, 和 R. Rai, “Featurenet：基于 3d 卷积神经网络的加工特征识别，” *Computer-Aided
    Design*，第 101 卷，页码 12–22，2018 年。'
- en: '[73] B. Manda, P. Bhaskare, and R. Muthuganapathy, “A convolutional neural
    network approach to the classification of engineering models,” *IEEE Access*,
    vol. 9, pp. 22 711–22 723, 2021.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] B. Manda, P. Bhaskare, 和 R. Muthuganapathy, “一种卷积神经网络方法用于工程模型分类，” *IEEE
    Access*，第 9 卷，页码 22 711–22 723，2021 年。'
- en: '[74] A. Angrish, B. Craver, and B. Starly, ““fabsearch”: A 3d cad model-based
    search engine for sourcing manufacturing services,” *Journal of Computing and
    Information Science in Engineering*, vol. 19, no. 4, p. 041006, 2019.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] A. Angrish, B. Craver, 和 B. Starly, “‘fabsearch’：基于 3d cad 模型的制造服务搜索引擎，”
    *Journal of Computing and Information Science in Engineering*，第 19 卷，第 4 期，页码
    041006，2019 年。'
- en: '[75] L. Mandelli and S. Berretti, “Cad 3d model classification by graph neural
    networks: A new approach based on step format,” *arXiv preprint arXiv:2210.16815*,
    2022.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] L. Mandelli 和 S. Berretti, “基于图神经网络的 3d cad 模型分类：一种基于 step 格式的新方法，” *arXiv
    预印本 arXiv:2210.16815*，2022 年。'
- en: '[76] W. Cao, T. Robinson, Y. Hua, F. Boussuge, A. R. Colligan, and W. Pan,
    “Graph representation of 3d cad models for machining feature recognition with
    deep learning,” in *International Design Engineering Technical Conferences and
    Computers and Information in Engineering Conference*, 2020.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] W. Cao, T. Robinson, Y. Hua, F. Boussuge, A. R. Colligan, 和 W. Pan, “用于加工特征识别的
    3d cad 模型的图表示与深度学习，” 见 *International Design Engineering Technical Conferences
    and Computers and Information in Engineering Conference*，2020 年。'
- en: '[77] S. Koch, A. Matveev, Z. Jiang, F. Williams, A. Artemov, E. Burnaev, M. Alexa,
    D. Zorin, and D. Panozzo, “Abc: A big cad model dataset for geometric deep learning,”
    in *IEEE Conference on Computer Vision and Pattern Recognition*, 2019.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S. Koch, A. Matveev, Z. Jiang, F. Williams, A. Artemov, E. Burnaev, M.
    Alexa, D. Zorin, 和 D. Panozzo, “ABC：一个用于几何深度学习的大型 cad 模型数据集，” 见 *IEEE Conference
    on Computer Vision and Pattern Recognition*，2019 年。'
- en: '[78] M. Eitz, J. Hays, and M. Alexa, “How do humans sketch objects?” *ACM Transactions
    on graphics (TOG)*, vol. 31, no. 4, pp. 1–10, 2012.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Eitz, J. Hays, 和 M. Alexa, “人类如何勾画物体？” *ACM Transactions on Graphics
    (TOG)*，第 31 卷，第 4 期，页码 1–10，2012 年。'
- en: '[79] P. Sangkloy, N. Burnell, C. Ham, and J. Hays, “The sketchy database: learning
    to retrieve badly drawn bunnies,” *ACM Transactions on Graphics (TOG)*, vol. 35,
    no. 4, pp. 1–12, 2016.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] P. Sangkloy, N. Burnell, C. Ham, 和 J. Hays, “Sketchy 数据库：学习检索画得不好的兔子，”
    *ACM Transactions on Graphics (TOG)*，第 35 卷，第 4 期，页码 1–12，2016 年。'
- en: '[80] Y. Gryaditskaya, M. Sypesteyn, J. W. Hoftijzer, S. C. Pont, F. Durand,
    and A. Bousseau, “Opensketch: a richly-annotated dataset of product design sketches.”
    *ACM Transactions on Graphics (TOG)*, vol. 38, no. 6, pp. 232–1, 2019.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Gryaditskaya, M. Sypesteyn, J. W. Hoftijzer, S. C. Pont, F. Durand,
    和 A. Bousseau, “Opensketch：一个丰富注释的产品设计草图数据集。” *ACM Transactions on Graphics (TOG)*，第
    38 卷，第 6 期，页码 232–1，2019 年。'
- en: '[81] A. R. Colligan, T. T. Robinson, D. C. Nolan, Y. Hua, and W. Cao, “Hierarchical
    cadnet: Learning from b-reps for machining feature recognition,” *Computer-Aided
    Design*, vol. 147, p. 103226, 2022.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] A. R. Colligan, T. T. Robinson, D. C. Nolan, Y. Hua, 和 W. Cao, “层次化 cadnet：从
    b-reps 学习以识别加工特征，” *Computer-Aided Design*，第 147 卷，页码 103226，2022 年。'
- en: '[82] T. G. Gunn, “The mechanization of design and manufacturing,” *Scientific
    American*, vol. 247, no. 3, pp. 114–131, 1982.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] T. G. Gunn, “设计与制造的机械化，” *Scientific American*，第 247 卷，第 3 期，页码 114–131，1982
    年。'
- en: '[83] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen, “Graph contrastive
    learning with augmentations,” *Advances in neural information processing systems*,
    vol. 33, pp. 5812–5823, 2020.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, 和 Y. Shen, “具有增强的图对比学习，” *Advances
    in Neural Information Processing Systems*，第 33 卷，页码 5812–5823，2020 年。'
- en: '[84] M. Al-Wswasi, A. Ivanov, and H. Makatsoris, “A survey on smart automated
    computer-aided process planning (acapp) techniques,” *The International Journal
    of Advanced Manufacturing Technology*, vol. 97, pp. 809–832, 2018.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] M. Al-Wswasi, A. Ivanov, 和 H. Makatsoris, “智能自动计算机辅助工艺规划 (acapp) 技术的综述，”
    *The International Journal of Advanced Manufacturing Technology*，第 97 卷，页码 809–832，2018
    年。'
- en: '[85] Y. Shi, Y. Zhang, K. Xia, and R. Harik, “A critical review of feature
    recognition techniques,” *Computer-Aided Design and Applications*, vol. 17, no. 5,
    pp. 861–899, 2020.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Y. Shi, Y. Zhang, K. Xia, 和 R. Harik, “特征识别技术的批判性回顾，” *Computer-Aided
    Design and Applications*，第 17 卷，第 5 期，页码 861–899，2020 年。'
- en: '[86] G. Zhan, Q. Fan, K. Mo, L. Shao, B. Chen, L. J. Guibas, H. Dong *et al.*,
    “Generative 3d part assembly via dynamic graph learning,” *Advances in Neural
    Information Processing Systems*, vol. 33, pp. 6315–6326, 2020.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] G. Zhan, Q. Fan, K. Mo, L. Shao, B. Chen, L. J. Guibas, H. Dong *等*, “通过动态图学习生成
    3D 部件装配，” *神经信息处理系统进展*, 第 33 卷, 页码 6315–6326, 2020。'
- en: '[87] H. Lin, M. Averkiou, E. Kalogerakis, B. Kovacs, S. Ranade, V. Kim, S. Chaudhuri,
    and K. Bala, “Learning material-aware local descriptors for 3d shapes,” in *International
    Conference on 3D Vision (3DV)*.   IEEE, 2018.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. Lin, M. Averkiou, E. Kalogerakis, B. Kovacs, S. Ranade, V. Kim, S.
    Chaudhuri, 和 K. Bala, “为 3D 形状学习材料感知局部描述符，” 发表在 *国际 3D 视觉会议 (3DV)*。IEEE, 2018。'
- en: '[88] K. Mo, P. Guerrero, L. Yi, H. Su, P. Wonka, N. Mitra, and L. J. Guibas,
    “Structurenet: hierarchical graph networks for 3d shape generation,” *ACM Transactions
    on Graphics (TOG)*, vol. 38, no. 6, pp. 242:1–242:19, 2019.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] K. Mo, P. Guerrero, L. Yi, H. Su, P. Wonka, N. Mitra, 和 L. J. Guibas,
    “Structurenet: 用于 3D 形状生成的分层图网络，” *ACM 图形学汇刊 (TOG)*, 第 38 卷，第 6 期, 页码 242:1–242:19,
    2019。'
- en: '[89] R. K. Jones, T. Barton, X. Xu, K. Wang, E. Jiang, P. Guerrero, N. J. Mitra,
    and D. Ritchie, “Shapeassembly: Learning to generate programs for 3d shape structure
    synthesis,” *ACM Transactions on Graphics (TOG)*, vol. 39, no. 6, pp. 1–20, 2020.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] R. K. Jones, T. Barton, X. Xu, K. Wang, E. Jiang, P. Guerrero, N. J. Mitra,
    和 D. Ritchie, “Shapeassembly: 学习生成 3D 形状结构合成的程序，” *ACM 图形学汇刊 (TOG)*, 第 39 卷，第
    6 期, 页码 1–20, 2020。'
- en: '[90] Z. Wu, X. Wang, D. Lin, D. Lischinski, D. Cohen-Or, and H. Huang, “Sagnet:
    Structure-aware generative network for 3d-shape modeling,” *ACM Transactions on
    Graphics (TOG)*, vol. 38, no. 4, pp. 1–14, 2019.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Z. Wu, X. Wang, D. Lin, D. Lischinski, D. Cohen-Or, 和 H. Huang, “Sagnet:
    结构感知生成网络用于 3D 形状建模，” *ACM 图形学汇刊 (TOG)*, 第 38 卷，第 4 期, 页码 1–14, 2019。'
- en: '[91] K. Yin, Z. Chen, S. Chaudhuri, M. Fisher, V. G. Kim, and H. Zhang, “Coalesce:
    Component assembly by learning to synthesize connections,” in *International Conference
    on 3D Vision (3DV)*.   IEEE, 2020.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] K. Yin, Z. Chen, S. Chaudhuri, M. Fisher, V. G. Kim, 和 H. Zhang, “Coalesce:
    通过学习合成连接进行组件装配，” 发表在 *国际 3D 视觉会议 (3DV)*。IEEE, 2020。'
- en: '[92] C. Zhu, K. Xu, S. Chaudhuri, R. Yi, and H. Zhang, “Scores: Shape composition
    with recursive substructure priors,” *ACM Transactions on Graphics (TOG)*, vol. 37,
    no. 6, pp. 1–14, 2018.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] C. Zhu, K. Xu, S. Chaudhuri, R. Yi, 和 H. Zhang, “Scores: 具有递归子结构先验的形状组合，”
    *ACM 图形学汇刊 (TOG)*, 第 37 卷，第 6 期, 页码 1–14, 2018。'
- en: '[93] A. N. Harish, R. Nagar, and S. Raman, “Rgl-net: A recurrent graph learning
    framework for progressive part assembly,” in *IEEE Winter Conference on Applications
    of Computer Vision (WACV)*, 2022.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] A. N. Harish, R. Nagar, 和 S. Raman, “Rgl-net: 用于渐进部件装配的递归图学习框架，” 发表在 *IEEE
    冬季计算机视觉应用会议 (WACV)*, 2022。'
- en: '[94] Y. Lee, E. S. Hu, and J. J. Lim, “Ikea furniture assembly environment
    for long-horizon complex manipulation tasks,” in *IEEE International Conference
    on Robotics and Automation (icra)*.   IEEE, 2021.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. Lee, E. S. Hu, 和 J. J. Lim, “宜家家具装配环境用于长期复杂操控任务，” 发表在 *IEEE 国际机器人与自动化会议
    (ICRA)*。IEEE, 2021。'
- en: '[95] M. Sung, H. Su, V. G. Kim, S. Chaudhuri, and L. Guibas, “Complementme:
    Weakly-supervised component suggestions for 3d modeling,” *ACM Transactions on
    Graphics (TOG)*, vol. 36, no. 6, pp. 1–12, 2017.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] M. Sung, H. Su, V. G. Kim, S. Chaudhuri, 和 L. Guibas, “Complementme: 用于
    3D 建模的弱监督组件建议，” *ACM 图形学汇刊 (TOG)*, 第 36 卷，第 6 期, 页码 1–12, 2017。'
- en: '[96] X. Wang, B. Zhou, Y. Shi, X. Chen, Q. Zhao, and K. Xu, “Shape2motion:
    Joint analysis of motion parts and attributes from 3d shapes,” in *IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] X. Wang, B. Zhou, Y. Shi, X. Chen, Q. Zhao, 和 K. Xu, “Shape2motion: 从
    3D 形状中联合分析运动部件和属性，” 发表在 *IEEE 计算机视觉与模式识别会议*，2019。'
- en: '[97] A. Zhao, J. Xu, M. Konaković-Luković, J. Hughes, A. Spielberg, D. Rus,
    and W. Matusik, “Robogrammar: graph grammar for terrain-optimized robot design,”
    *ACM Transactions on Graphics (TOG)*, vol. 39, no. 6, pp. 1–16, 2020.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] A. Zhao, J. Xu, M. Konaković-Luković, J. Hughes, A. Spielberg, D. Rus,
    和 W. Matusik, “Robogrammar: 用于地形优化机器人设计的图语法，” *ACM 图形学汇刊 (TOG)*, 第 39 卷，第 6 期,
    页码 1–16, 2020。'
- en: '[98] F. Boussuge, C. M. Tierney, T. T. Robinson, and C. G. Armstrong, “Application
    of tensor factorisation to analyze similarities in cad assembly models,” in *International
    Meshing Roundtable and User Forum*, vol. 1, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] F. Boussuge, C. M. Tierney, T. T. Robinson, 和 C. G. Armstrong, “张量分解在
    CAD 装配模型相似性分析中的应用，” 发表在 *国际网格化圆桌会议和用户论坛*，第 1 卷, 2019。'
- en: '[99] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon,
    “Dynamic graph cnn for learning on point clouds,” *ACM Transactions on Graphics
    (tog)*, vol. 38, no. 5, pp. 1–12, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, 和 J. M. Solomon,
    “用于点云学习的动态图 CNN，” *ACM 图形学汇刊 (TOG)*, 第 38 卷，第 5 期, 页码 1–12, 2019。'
- en: '[100] P. Guerrero, Y. Kleiman, M. Ovsjanikov, and N. J. Mitra, “Pcpnet learning
    local shape properties from raw point clouds,” in *Computer graphics forum*, vol. 37,
    no. 2.   Wiley Online Library, 2018, pp. 75–85.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] P. Guerrero, Y. Kleiman, M. Ovsjanikov, 和 N. J. Mitra, “Pcpnet 从原始点云中学习局部形状特征，”
    在 *计算机图形学论坛*，第37卷，第2期。Wiley 在线图书馆，2018年，页码75–85。'
- en: '[101] S. Brody, U. Alon, and E. Yahav, “How attentive are graph attention networks?”
    in *International Conference on Learning Representations, ICLR*, 2022.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. Brody, U. Alon, 和 E. Yahav, “图注意力网络的关注程度如何？” 在 *国际学习表征会议，ICLR*，2022年。'
- en: '[102] T. Funkhouser, M. Kazhdan, P. Shilane, P. Min, W. Kiefer, A. Tal, S. Rusinkiewicz,
    and D. Dobkin, “Modeling by example,” *ACM Transactions on Graphics (TOG)*, vol. 23,
    no. 3, pp. 652–663, 2004.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] T. Funkhouser, M. Kazhdan, P. Shilane, P. Min, W. Kiefer, A. Tal, S.
    Rusinkiewicz, 和 D. Dobkin, “通过实例建模，” *ACM 图形学学报（TOG）*，第23卷，第3期，页码652–663，2004年。'
- en: '[103] D. Ha and D. Eck, “A neural representation of sketch drawings,” in *International
    Conference on Learning Representations, ICLR*, 2018.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] D. Ha 和 D. Eck, “草图绘制的神经表征，” 在 *国际学习表征会议，ICLR*，2018年。'
- en: '[104] J. Jongejan, H. Rowley, T. Kawashima, J. Kim, and N. Fox-Gieg, “The quick,
    draw!-ai experiment,” *Mount View, CA, accessed Feb*, vol. 17, no. 2018, p. 4,
    2016.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. Jongejan, H. Rowley, T. Kawashima, J. Kim, 和 N. Fox-Gieg, “快速绘制！-ai
    实验，” *加州山景城，访问时间为2月*，第17卷，第2018期，第4页，2016年。'
- en: '[105] M. Zhang and Y. Chen, “Link prediction based on graph neural networks,”
    *Advances in neural information processing systems*, vol. 31, 2018.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] M. Zhang 和 Y. Chen, “基于图神经网络的链路预测，” *神经信息处理系统的进展*，第31卷，2018年。'
- en: '[106] C. Nash, Y. Ganin, S. A. Eslami, and P. Battaglia, “Polygen: An autoregressive
    generative model of 3d meshes,” in *International Conference on Machine Learning*.   PMLR,
    2020.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] C. Nash, Y. Ganin, S. A. Eslami, 和 P. Battaglia, “Polygen：一种自回归生成模型的三维网格，”
    在 *国际机器学习会议*。PMLR，2020年。'
- en: '[107] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” *Advances
    in neural information processing systems*, vol. 28, 2015.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] O. Vinyals, M. Fortunato, 和 N. Jaitly, “指针网络，” *神经信息处理系统的进展*，第28卷，2015年。'
- en: '[108] K. Varda, “Protocol buffers: Google’s data interchange format,” *Google
    Open Source Blog, Available at least as early as Jul*, vol. 72, p. 23, 2008.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] K. Varda, “协议缓冲区：谷歌的数据交换格式，” *谷歌开源博客，最早可追溯至7月*，第72卷，第23页，2008年。'
- en: '[109] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An image is worth 16x16 words: Transformers for image recognition at scale,”
    in *International Conference on Learning Representations, ICLR*, 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.
    Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, 和 N.
    Houlsby, “一张图片价值16x16个词：大规模图像识别的变换器，” 在 *国际学习表征会议，ICLR*，2021年。'
- en: '[110] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
    A. Aspuru-Guzik, and R. P. Adams, “Convolutional networks on graphs for learning
    molecular fingerprints,” *Advances in Neural Information Processing Systems*,
    vol. 28, 2015.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
    A. Aspuru-Guzik, 和 R. P. Adams, “用于学习分子指纹的图卷积网络，” *神经信息处理系统的进展*，第28卷，2015年。'
- en: '[111] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
    “Neural message passing for quantum chemistry,” in *International Conference on
    Machine Learning*.   PMLR, 2017.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, 和 G. E. Dahl, “量子化学的神经消息传递，”
    在 *国际机器学习会议*。PMLR，2017年。'
- en: '[112] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, “Learning representations
    and generative models for 3d point clouds,” in *International conference on machine
    learning*.   PMLR, 2018.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] P. Achlioptas, O. Diamanti, I. Mitliagkas, 和 L. Guibas, “学习三维点云的表征和生成模型，”
    在 *国际机器学习会议*。PMLR，2018年。'
- en: '[113] K. Wang, J. Zheng, and Z. Zhou, “Neural face identification in a 2d wireframe
    projection of a manifold object,” in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2022.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] K. Wang, J. Zheng, 和 Z. Zhou, “在流形对象的二维线框投影中的神经人脸识别，” 在 *IEEE计算机视觉与模式识别会议*，2022年。'
- en: '[114] P. Benkő and T. Várady, “Segmentation methods for smooth point regions
    of conventional engineering objects,” *Computer-Aided Design*, vol. 36, no. 6,
    pp. 511–523, 2004.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] P. Benkő 和 T. Várady, “传统工程对象平滑点区域的分割方法，” *计算机辅助设计*，第36卷，第6期，页码511–523，2004年。'
- en: '[115] T. Birdal, B. Busam, N. Navab, S. Ilic, and P. Sturm, “Generic primitive
    detection in point clouds using novel minimal quadric fits,” *IEEE transactions
    on pattern analysis and machine intelligence*, vol. 42, no. 6, pp. 1333–1347,
    2019.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] T. Birdal, B. Busam, N. Navab, S. Ilic, 和 P. Sturm, “在点云中使用新型最小二次拟合进行通用原始体检测，”
    *IEEE模式分析与机器智能汇刊*，第42卷，第6期，第1333–1347页，2019年。'
- en: '[116] L. Li, M. Sung, A. Dubrovina, L. Yi, and L. J. Guibas, “Supervised fitting
    of geometric primitives to 3d point clouds,” in *IEEE Conference on Computer Vision
    and Pattern Recognition*, 2019.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] L. Li, M. Sung, A. Dubrovina, L. Yi, 和 L. J. Guibas, “对三维点云进行几何原始体的监督拟合，”
    见 *IEEE计算机视觉与模式识别会议*，2019年。'
- en: '[117] C. Sommer, Y. Sun, E. Bylow, and D. Cremers, “Primitect: Fast continuous
    hough voting for primitive detection,” in *International Conference on Robotics
    and Automation (ICRA)*.   IEEE, 2020.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] C. Sommer, Y. Sun, E. Bylow, 和 D. Cremers, “Primitect: 快速连续霍夫投票用于原始体检测，”
    见 *国际机器人与自动化会议（ICRA）*。 IEEE，2020年。'
- en: '[118] J. A. Bærentzen, “Robust generation of signed distance fields from triangle
    meshes,” in *International Workshop on Volume Graphics*.   IEEE, 2005.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] J. A. Bærentzen, “从三角网格中稳健生成有符号距离场，” 见 *国际体积图形研讨会*。 IEEE，2005年。'
- en: '[119] M. Sanchez, O. Fryazinov, and A. Pasko, “Efficient evaluation of continuous
    signed distance to a polygonal mesh,” in *Spring Conference on Computer Graphics*,
    2012.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] M. Sanchez, O. Fryazinov, 和 A. Pasko, “对多边形网格的连续有符号距离的高效评估，” 见 *计算机图形学春季会议*，2012年。'
