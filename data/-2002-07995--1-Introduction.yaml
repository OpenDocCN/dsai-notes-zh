- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 20:02:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:02:22'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2002.07995] 1 Introduction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2002.07995] 1 引言'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2002.07995](https://ar5iv.labs.arxiv.org/html/2002.07995)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2002.07995](https://ar5iv.labs.arxiv.org/html/2002.07995)
- en: \headevenname
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \headevenname
- en: Xiao et al.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao et al.
- en: \MakePageStyle\MakeAbstract
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \MakePageStyle\MakeAbstract
- en: Researchers have now achieved great success on dealing with 2D images using
    deep learning. In recent years, 3D computer vision and Geometry Deep Learning
    gain more and more attention. Many advanced techniques for 3D shapes have been
    proposed for different applications. Unlike 2D images, which can be uniformly
    represented by regular grids of pixels, 3D shapes have various representations,
    such as depth and multi-view images, voxel-based representation, point-based representation,
    mesh-based representation, implicit surface representation, etc. However, the
    performance for different applications largely depends on the representation used,
    and there is no unique representation that works well for all applications. Therefore,
    in this survey, we review recent development in deep learning for 3D geometry
    from a representation perspective, summarizing the advantages and disadvantages
    of different representations in different applications. We also present existing
    datasets in these representations and further discuss future research directions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员现在在使用深度学习处理2D图像方面取得了巨大的成功。近年来，3D计算机视觉和几何深度学习获得了越来越多的关注。许多先进的3D形状技术已被提出用于不同的应用。与可以通过规则的像素网格均匀表示的2D图像不同，3D形状有各种表示方式，如深度图和多视角图像、体素表示、点云表示、网格表示、隐式表面表示等。然而，不同应用的性能在很大程度上取决于所使用的表示，并且没有一种唯一的表示适用于所有应用。因此，在本综述中，我们从表示的角度回顾了3D几何深度学习的最新发展，总结了不同表示在不同应用中的优缺点。我们还介绍了这些表示中的现有数据集，并进一步讨论了未来的研究方向。
- en: \MakeKeywords
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \MakeKeywords
- en: 3D representation, geometry learning, neural networks, computer graphics
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 3D表示、几何学习、神经网络、计算机图形学
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent improvements in methods for acquisition and rendering of 3D models haven
    resulted in consolidated repositories containing massive amounts of 3D shapes
    on the Internet. With the increased availability of 3D models, we have been seeing
    an explosion in the demands of processing, generation and visualization of 3D
    models in a variety of disciplines, such as medicine, architecture and entertainment.
    The techniques for matching, identification and manipulation of 3D shapes have
    become fundamental building blocks in modern computer vision and computer graphics
    systems. Due to the complexity and irregularity of 3D shape data, how to effectively
    represent 3D shapes remains a challenging problem. Thus, there have been extensive
    research efforts concentrating on how to deal with and generate 3D shapes based
    on different representations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在3D模型获取和渲染方法上的改进导致了包含大量3D形状的整合库在互联网的出现。随着3D模型的可用性增加，我们在医学、建筑和娱乐等多个领域看到了对3D模型处理、生成和可视化需求的爆炸性增长。3D形状的匹配、识别和操作技术已经成为现代计算机视觉和计算机图形系统中的基础构件。由于3D形状数据的复杂性和不规则性，如何有效地表示3D形状仍然是一个具有挑战性的问题。因此，已经有大量的研究工作集中在如何基于不同的表示方法处理和生成3D形状上。
- en: In early research on 3D shape representations, 3D objects were normally modeled
    with a global approach, such as constructive solid geometry and deformed superquadrics.
    Those approaches have several drawbacks when utilized for the tasks like recognition
    and retrieval. First, when representing imperfect 3D shapes, including those with
    noise and incompleteness, which are common in practice, such representations may
    impose negative influence on matching performance. Second, the high-dimensionality
    heavily burdens the computation and tends to make models overfit. Hence, more
    sophisticated methods are designed to extract representations of 3D shapes in
    a more concise, yet discriminative and informative form.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的3D形状表示研究中，3D对象通常采用全局方法进行建模，如构造固体几何和变形超二次体。这些方法在用于识别和检索等任务时有若干缺点。首先，在表示不完美的3D形状时，包括那些常见的噪声和不完整性，这些表示可能会对匹配性能产生负面影响。其次，高维性严重负担计算，并倾向于使模型过拟合。因此，设计了更复杂的方法来提取3D形状的表示，使其更简洁，但又具有区分性和信息性。
- en: Several related surveys have been published [[9](#bib.bib9), [1](#bib.bib1),
    [35](#bib.bib35)], which focus on different aspects of deep learning for 3D geometry.
    Moreover, with rapid development of 3D shape representations and related techniques
    for deep learning, it is essential to further summarize up-to-date research works.
    In this survey, we mainly review deep learning methods on 3D shape representations
    and discuss their advantages and disadvantages considering different application
    scenarios. We now give a brief summary of different 3D shape representation categories.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 已经出版了几篇相关的调查文章 [[9](#bib.bib9), [1](#bib.bib1), [35](#bib.bib35)]，它们集中于深度学习在三维几何方面的不同方面。此外，随着三维形状表示和深度学习相关技术的快速发展，进一步总结最新的研究工作变得尤为重要。在这项调查中，我们主要回顾了三维形状表示的深度学习方法，并讨论了它们在不同应用场景下的优缺点。我们现在简要总结了不同的三维形状表示类别。
- en: Depth and multi-view images can be used to represent 3D models in the 2D field.
    The regular structure of images makes them efficient to be processed. Depending
    on whether depth maps are included, 3D shapes can be presented by RGB (color)
    or RGB-D (color and depth) images viewed from different viewpoints. Because of
    the influx of available depth data due to the popularity of 2.5D sensors, such
    as Microsoft Kinect, Intel RealSense, etc., multi-view RGB-D images are widely
    used to represent real-world 3D shapes. The large asset of image-based processing
    models can be leveraged using this representation. But it is inevitable that this
    kind of representation loses some geometry features.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图和多视角图像可以用来在二维领域中表示三维模型。图像的规则结构使得它们能够高效处理。根据是否包含深度图，三维形状可以通过不同视角下的 RGB（颜色）或
    RGB-D（颜色和深度）图像来呈现。由于 2.5D 传感器（如微软 Kinect、英特尔 RealSense 等）的普及，使得大量深度数据涌现，多视角 RGB-D
    图像被广泛用于表示现实世界的三维形状。可以利用这种表示方式来发挥图像处理模型的大量资产。但不可避免地，这种表示方式会丢失一些几何特征。
- en: A voxel is a 3D extension of the concept of pixel. Similar with pixels in 2D,
    the voxel-based representation also has a regular structure in the 3D space. The
    architectures of some neural networks which have been demonstrated useful in the
    2D image field [[48](#bib.bib48), [50](#bib.bib50)] can be easily extended to
    the voxel form. Nevertheless, adding one dimension means an exponentially increased
    data size. As the resolution increases, the amount of required memory and computational
    costs increase dramatically, which restricts the representation only to low resolutions
    when representing 3D shapes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 体素是像素概念的三维扩展。与二维中的像素类似，基于体素的表示在三维空间中也具有规则结构。已经在二维图像领域中证明有效的某些神经网络架构 [[48](#bib.bib48),
    [50](#bib.bib50)] 可以很容易地扩展到体素形式。然而，增加一个维度意味着数据量呈指数增长。随着分辨率的提高，所需的内存和计算成本急剧增加，这限制了三维形状表示仅适用于低分辨率。
- en: Surface-based representation describes 3D shapes by encoding their surfaces,
    which can also be regarded as 2-manifolds. Point clouds and meshes are both discretized
    forms of 3D shape surfaces. Point clouds use a set of sampled 3D point coordinates
    to represent the surface. It can be easily generated by scanners but difficult
    to process due to their lack of order and connectivity information. Researchers
    use order invariant operators such as the max pooling operator in deep neural
    networks [[75](#bib.bib75), [77](#bib.bib77)] to mitigate the lack of order problem.
    Meshes can depict higher quality 3D shapes with less memory and computational
    cost compared with point clouds and voxels. A mesh contains a vertex set and an
    edge set. Due to its graphical nature, researchers have made attempts to build
    graph-based convolutional neural networks for coping with meshes. Some other methods
    regard meshes as the discretization of 2-manifolds. Moreover, meshes are more
    suitable for 3D shape deformation. One can deform a mesh model by transforming
    vertices while keeping the connectivity at the same time.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于表面的表示通过编码三维形状的表面来描述三维形状，这也可以被视为二维流形。点云和网格都是三维形状表面的离散化形式。点云使用一组采样的三维点坐标来表示表面。它可以通过扫描仪轻松生成，但由于缺乏顺序和连接信息，处理起来困难。研究人员使用顺序不变的运算符，例如深度神经网络中的最大池化运算符 [[75](#bib.bib75),
    [77](#bib.bib77)] 来缓解缺乏顺序的问题。与点云和体素相比，网格可以以更少的内存和计算成本描绘更高质量的三维形状。一个网格包含一个顶点集和一个边集。由于其图形特性，研究人员尝试构建基于图的卷积神经网络来处理网格。其他一些方法将网格视为二维流形的离散化。此外，网格更适合三维形状变形。可以通过变换顶点来变形一个网格模型，同时保持连接性。
- en: Implicit surface representation exploits implicit field functions, such as occupancy
    functions [[67](#bib.bib67)] and signed distance functions [[116](#bib.bib116)],
    to describe the surface of 3D shapes. The implicit functions learned by deep neural
    networks define the spatial relationship between points and surfaces. They provide
    a description with infinite resolution of 3D shapes with reasonable memory consumption,
    and are capable of representing shapes with changing topology. Nevertheless, implicit
    representations cannot reflect the geometric features of 3D shapes directly, and
    usually need to be transformed to explicit representations such as meshes. Most
    methods apply iso-surfacing, such as marching cubes [[58](#bib.bib58)], which
    is an expensive operation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式表面表示利用隐式场函数，如占用函数[[67](#bib.bib67)]和有符号距离函数[[116](#bib.bib116)]，来描述3D形状的表面。深度神经网络学习的隐式函数定义了点与表面之间的空间关系。它们在合理的内存消耗下提供了具有无限分辨率的3D形状描述，并能够表示拓扑变化的形状。然而，隐式表示无法直接反映3D形状的几何特征，通常需要转换为显式表示，如网格。大多数方法应用等值面提取，如行进立方体[[58](#bib.bib58)]，这是一项昂贵的操作。
- en: Structured representation. One way to cope with complex 3D shapes is to decompose
    them into structure and geometric details, leading to structured representations.
    Recently, increasingly more methods regard a 3D shape as a collection of parts
    and organize them linearly or hierarchically. The structure of 3D shapes is processed
    by Recurrent Neural Networks (RNNs) [[121](#bib.bib121)], Recursive Neural Networks
    (RvNNs) [[51](#bib.bib51)] or other network architectures. Each part of the shape
    can be processed by unstructured models. The structured representation focuses
    on the relations (such as symmetry, supporting, being supported, etc.) between
    different parts within a 3D shape, which provides better description capability
    than alternative representations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化表示。应对复杂的3D形状的一种方法是将其分解为结构和几何细节，从而得到结构化表示。近年来，越来越多的方法将3D形状视为部件的集合，并将其线性或层次性地组织起来。3D形状的结构由递归神经网络（RNNs）[[121](#bib.bib121)]、递归神经网络（RvNNs）[[51](#bib.bib51)]或其他网络架构处理。形状的每个部分可以由非结构化模型处理。结构化表示关注于3D形状内部不同部分之间的关系（例如对称、支撑、被支撑等），相比于其他表示方法，它提供了更好的描述能力。
- en: Deformation-based representation. Unlike rigid man-made 3D shapes such as chairs
    and tables, there are also a large number of non-rigid (e.g. articulated) 3D shapes
    such as human bodies, which also play an important role in computer animation,
    augmented reality, etc. The deformation-based representation is proposed mainly
    for describing the intrinsic deformation properties while ignoring the extrinsic
    transformation properties. Many methods use rotation-invariant local features
    for describing shape deformation to reduce the distortion and keep the geometry
    details at the same time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变形的表示。与诸如椅子和桌子等刚性人造3D形状不同，还有大量非刚性（例如关节）3D形状，如人体，这些形状在计算机动画、增强现实等领域也发挥着重要作用。基于变形的表示主要用于描述固有的变形属性，同时忽略外在的变换属性。许多方法使用旋转不变的局部特征来描述形状变形，以减少扭曲并同时保持几何细节。
- en: 'Recently, deep learning has achieved superior performance in contrast to classical
    methods in many fields, including 3D shape analysis, reconstruction, etc. A variety
    of architectures of deep networks have been designed to process or generate 3D
    shape representations, which we refer to as *geometry learning*. In the following
    sections, we focus more on most recent deep learning based methods for representing
    and processing 3D shapes in different forms. According to how the representation
    is encoded and stored, our survey is organized in the following structure: Section [2](#S2
    "2 Image-based methods") reviews image-based shape representation methods. Sections
    [3](#S3 "3 Voxel-based representations") and [4](#S4 "4 Surface-based representations")
    introduce voxel-based and surface-based representations respectively. Section
    [5](#S5 "5 Implicit representations") further introduces implicit surface representations.
    Sections [6](#S6 "6 Structure-based representations") and [7](#S7 "7 Deformation-based
    representations") review structure-based and deformation-based description methods.
    We then summarize typical datasets in Section [8](#S8 "8 Datasets") and typical
    applications for shape analysis and reconstruction in Section [9](#S9 "9 Shape
    Analysis and Reconstruction"), before concluding the paper in Section [10](#S10
    "10 Summary"). Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction") summarizes the timeline
    of representative deep learning methods based on various 3D shape representations.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习在许多领域，包括3D形状分析、重建等方面，相较于经典方法取得了卓越的性能。已经设计了多种深度网络架构来处理或生成3D形状表示，我们称之为*几何学习*。在接下来的章节中，我们将更多地关注基于深度学习的最新方法，用于以不同形式表示和处理3D形状。根据表示的编码和存储方式，我们的综述按以下结构组织：第[2](#S2
    "2 Image-based methods")节回顾了基于图像的形状表示方法。第[3](#S3 "3 Voxel-based representations")节和第[4](#S4
    "4 Surface-based representations")节分别介绍了体素基和表面基的表示方法。第[5](#S5 "5 Implicit representations")节进一步介绍了隐式表面表示方法。第[6](#S6
    "6 Structure-based representations")节和第[7](#S7 "7 Deformation-based representations")节回顾了基于结构和基于变形的描述方法。接着，在第[8](#S8
    "8 Datasets")节中总结了典型的数据集，在第[9](#S9 "9 Shape Analysis and Reconstruction")节中总结了形状分析和重建的典型应用，然后在第[10](#S10
    "10 Summary")节中总结全文。图[1](#S1.F1 "Figure 1 ‣ 1 Introduction")总结了基于各种3D形状表示的代表性深度学习方法的时间线。
- en: <svg   height="322.23" overflow="visible" version="1.1" width="729.7"><g transform="translate(0,322.23)
    matrix(1 0 0 -1 0 0) translate(4.61,0) translate(0,5)"><g stroke="#000000" fill="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 25.53 168.27)" fill="#000000"
    stroke="#000000" color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2015</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 143.64 137.77)" fill="#000000" stroke="#000000" color="#B3B3B3"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2016</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.75 168.27)" fill="#000000" stroke="#000000"
    color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2017</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 379.86 137.77)" fill="#000000" stroke="#000000" color="#B3B3B3"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 497.97 168.27)" fill="#000000" stroke="#000000"
    color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g></g><g stroke="#FF0000"
    fill="#FF0000" stroke-width="1.42264pt" color="#FF0000"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 303.97)" fill="#000000" stroke="#000000"><foreignobject width="58.81"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">depth map</foreignobject></g></g><g
    stroke="#FF8000" fill="#FF8000" stroke-width="1.42264pt" color="#FF8000"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 292.16)" fill="#000000" stroke="#000000"><foreignobject width="98.31"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">multi-view
    images</foreignobject></g></g><g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt"
    color="#00FFFF"><g transform="matrix(1.0 0.0 0.0 1.0 52.84 280.35)" fill="#000000"
    stroke="#000000"><foreignobject width="109.8" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">voxel representation</foreignobject></g></g><g
    stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 268.71)" fill="#000000" stroke="#000000"><foreignobject width="110.49"
    height="10.74" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">point representation</foreignobject></g></g><g
    stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 303.97)" fill="#000000" stroke="#000000"><foreignobject width="109.52"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">mesh representation</foreignobject></g></g><g
    stroke="#D9668C" fill="#D9668C" stroke-width="1.42264pt" color="#D9668C"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 292.16)" fill="#000000" stroke="#000000"><foreignobject width="83.47"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">implicit
    surface</foreignobject></g></g><g stroke="#BF8040" fill="#BF8040" stroke-width="1.42264pt"
    color="#BF8040"><g transform="matrix(1.0 0.0 0.0 1.0 214.26 280.35)" fill="#000000"
    stroke="#000000"><foreignobject width="137.96" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">structured representation</foreignobject></g></g><g
    stroke="#333333" fill="#333333" stroke-width="1.42264pt" color="#333333"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 268.54)" fill="#000000" stroke="#000000"><foreignobject width="147.19"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">deformation
    representation</foreignobject></g></g> <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt"
    color="#00FFFF"><g transform="matrix(1.0 0.0 0.0 1.0 0 118.58)" fill="#00FFFF"
    stroke="#00FFFF"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3D ShapeNets[[112](#bib.bib112)] <g stroke="#FF8000"
    fill="#FF8000" stroke-width="1.42264pt" color="#FF8000"><path d="M 66.93 157.48
    L 66.93 181.1" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 27.56 193.62)"
    fill="#FF8000" stroke="#FF8000"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">MVCNN[[93](#bib.bib93)]</foreignobject></g></path></g>
    <g stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><path
    d="M 66.93 200.79 L 66.93 224.41" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 27.56 236.92)" fill="#0000FF" stroke="#0000FF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GCNN[[61](#bib.bib61)]</foreignobject></g></path></g>
    <g stroke="#FF0000" fill="#FF0000" stroke-width="1.42264pt" color="#FF0000"><path
    d="M 106.3 157.48 L 106.3 118.11" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 47.24 102.83)" fill="#FF0000" stroke="#FF0000"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Eigen et
    al.[[20](#bib.bib20)]</foreignobject></g></path></g> <g stroke="#000000" fill="#000000"
    stroke-width="1.42264pt" color="#000000"><path d="M 165.35 157.48 L 165.35 196.85"
    style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 145.67 209.37)" fill="#000000"
    stroke="#000000"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">RIMD[[24](#bib.bib24)]</foreignobject></g></path></g>
    <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path
    d="M 188.98 157.48 L 188.98 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 149.61 118.58)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3D-R2N2[[16](#bib.bib16)]</foreignobject></g></path></g>
    <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path
    d="M 220.47 157.48 L 220.47 181.1" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 188.98 193.62)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3D-GAN[[110](#bib.bib110)]</foreignobject></g></path></g>
    <g stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><path
    d="M 295.28 157.48 L 295.28 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 251.97 118.58)" fill="#00FF00" stroke="#00FF00"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PointNet[[75](#bib.bib75)]
    PointOutNet[[21](#bib.bib21)]</foreignobject></g></path></g> <g stroke="#00FFFF"
    fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path d="M 295.28 102.36
    L 295.28 78.74" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 263.78
    63.46)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="118.11" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">OctNet[[80](#bib.bib80)]</foreignobject></g></path></g>
    <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt" color="#00FFFF"><path
    d="M 295.28 157.48 L 295.28 181.1" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 275.59 193.62)" fill="#00FFFF" stroke="#00FFFF"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O-CNN[[105](#bib.bib105)]</foreignobject></g></path></g>
    <g stroke="#BF8040" fill="#BF8040" stroke-width="1.42264pt" color="#BF8040"><path
    d="M 295.28 204.72 L 295.28 228.35" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 275.59 240.86)" fill="#BF8040" stroke="#BF8040"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GRASS[[51](#bib.bib51)]</foreignobject></g></path></g>
    <g stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><path
    d="M 362.2 157.48 L 362.2 196.85" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 314.96 209.37)" fill="#00FF00" stroke="#00FF00"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PointNet++[[77](#bib.bib77)]</foreignobject></g></path></g>
    <g stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><path
    d="M 433.07 157.48 L 433.07 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 393.7 118.58)" fill="#0000FF" stroke="#0000FF"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Pixel2Mesh[[104](#bib.bib104)]</foreignobject></g></path></g>
    <g stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><path
    d="M 472.44 157.48 L 472.44 181.1" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 425.2 193.62)" fill="#00FF00" stroke="#00FF00"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PointCNN[[53](#bib.bib53)]</foreignobject></g></path></g>
    <g stroke="#D9668C" fill="#D9668C" stroke-width="1.42264pt" color="#D9668C"><path
    d="M 519.68 157.48 L 519.68 133.86" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 480.31 118.58)" fill="#D9668C" stroke="#D9668C"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DeepSDF [[74](#bib.bib74)]
    IM-NET[[14](#bib.bib14)]</foreignobject></g></path></g> <g stroke="#BF8040" fill="#BF8040"
    stroke-width="1.42264pt" color="#BF8040"><path d="M 551.18 157.48 L 551.18 181.1"
    style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 511.81 193.62)" fill="#BF8040"
    stroke="#BF8040"><foreignobject width="78.74" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SDM-NET[[27](#bib.bib27)] StructureNet[[68](#bib.bib68)]</foreignobject></g></path></g>
    <g stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><path
    d="M 551.18 216.54 L 551.18 240.16" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 511.81 248.74)" fill="#0000FF" stroke="#0000FF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MeshCNN[[39](#bib.bib39)]</foreignobject></g></path></g>
    <g stroke="#D9668C" fill="#D9668C" stroke-width="1.42264pt" color="#D9668C"><path
    d="M 590.55 157.48 L 590.55 118.11" style="fill:none"><g transform="matrix(1.0
    0.0 0.0 1.0 551.18 102.83)" fill="#D9668C" stroke="#D9668C"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Without 3D
    Supervision [[56](#bib.bib56)]</foreignobject></g></path></g> <g stroke="#BF8040"
    fill="#BF8040" stroke-width="1.42264pt" color="#BF8040"><path d="M 637.8 157.48
    L 637.8 133.86" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 602.36
    118.58)" fill="#BF8040" stroke="#BF8040"><foreignobject width="118.11" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">BSP-Net[[13](#bib.bib13)]</foreignobject></g></path></g>
    <g stroke="#333333" fill="#333333" stroke-width="1.42264pt" color="#333333"><path
    d="M 637.8 118.11 L 637.8 86.61" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 622.05 71.33)" fill="#333333" stroke="#333333"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NASA[[45](#bib.bib45)]</foreignobject></g></path></g><g
    stroke="#B3B3B3" fill="#B3B3B3" stroke-width="2.27621pt" color="#B3B3B3"><path
    d="M 7.87 157.48 L 657.3 157.48" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 657.3 157.48)"><path d="M 11.99 0 C 8.44 0.67 2.66 2.66 -1.33 5 L -1.33
    -5 C 2.66 -2.66 8.44 -0.67 11.99 0" style="stroke:none"></path></g></path></g>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`<svg height="322.23" overflow="visible" version="1.1" width="729.7"><g transform="translate(0,322.23)
    matrix(1 0 0 -1 0 0) translate(4.61,0) translate(0,5)"><g stroke="#000000" fill="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 25.53 168.27)" fill="#000000"
    stroke="#000000" color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2015</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 143.64 137.77)" fill="#000000" stroke="#000000" color="#B3B3B3"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2016</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 261.75 168.27)" fill="#000000" stroke="#000000"
    color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2017</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 379.86 137.77)" fill="#000000" stroke="#000000" color="#B3B3B3"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 497.97 168.27)" fill="#000000" stroke="#000000"
    color="#B3B3B3"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g></g><g stroke="#FF0000"
    fill="#FF0000" stroke-width="1.42264pt" color="#FF0000"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 303.97)" fill="#000000" stroke="#000000"><foreignobject width="58.81"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">depth map</foreignobject></g></g><g
    stroke="#FF8000" fill="#FF8000" stroke-width="1.42264pt" color="#FF8000"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 292.16)" fill="#000000" stroke="#000000"><foreignobject width="98.31"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">multi-view
    images</foreignobject></g></g><g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt"
    color="#00FFFF"><g transform="matrix(1.0 0.0 0.0 1.0 52.84 280.35)" fill="#000000"
    stroke="#000000"><foreignobject width="109.8" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">voxel representation</foreignobject></g></g><g
    stroke="#00FF00" fill="#00FF00" stroke-width="1.42264pt" color="#00FF00"><g transform="matrix(1.0
    0.0 0.0 1.0 52.84 268.71)" fill="#000000" stroke="#000000"><foreignobject width="110.49"
    height="10.74" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">point representation</foreignobject></g></g><g
    stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 303.97)" fill="#000000" stroke="#000000"><foreignobject width="109.52"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">mesh representation</foreignobject></g></g><g
    stroke="#D9668C" fill="#D9668C" stroke-width="1.42264pt" color="#D9668C"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 292.16)" fill="#000000" stroke="#000000"><foreignobject width="83.47"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">implicit
    surface</foreignobject></g></g><g stroke="#BF8040" fill="#BF8040" stroke-width="1.42264pt"
    color="#BF8040"><g transform="matrix(1.0 0.0 0.0 1.0 214.26 280.35)" fill="#000000"
    stroke="#000000"><foreignobject width="137.96" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">structured representation</foreignobject></g></g><g
    stroke="#333333" fill="#333333" stroke-width="1.42264pt" color="#333333"><g transform="matrix(1.0
    0.0 0.0 1.0 214.26 268.54)" fill="#000000" stroke="#000000"><foreignobject width="147.19"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">deformation
    representation</foreignobject></g></g> <g stroke="#00FFFF" fill="#00FFFF" stroke-width="1.42264pt"
    color="#00FFFF"><g transform="matrix(1.0 0.0 0.0 1.0 0 118.58)" fill="#00FFFF"
    stroke="#00FFFF"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3D ShapeNets[[112](#bib.bib112)] <g stroke="#FF8000"
    fill="#FF8000" stroke-width="1.42264pt" color="#FF8000"><path d="M 66.93 157.48
    L 66.93 181.1" style="fill:none"><g transform="matrix(1.0 0.0 0.0 1.0 27.56 193.62)"
    fill="#FF8000" stroke="#FF8000"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">MVCNN[[93](#bib.bib93)]</foreignobject></g></path></g>
    <g stroke="#0000FF" fill="#0000FF" stroke-width="1.42264pt" color="#0000FF"><path
    d="M 66.93 200.79 L 66.93 224.41" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 27.56 236.92)" fill="#0000FF" stroke="#0000FF"><foreignobject width="78.74"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GCNN[[61](#bib.bib61)]</foreignobject></g></path></g>
    <g stroke="#FF0000" fill="#FF0000" stroke-width="1.42264pt" color="#FF0000"><path
    d="M 106.3 157.48 L 106.3 118.11" style="fill:none"><g transform="matrix(1.0 0.0
    0.0 1.0 47.24 102.83)" fill="#FF0000" stroke="#FF0000"><foreignobject width="118.11"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Eigen et
    al.[[20](#bib.bib20)]</foreignobject></g></path></g> <g stroke="#000000" fill="#000000"
    stroke-width="1.42264'
- en: 'Figure 1: The timeline of deep learning based methods for various 3D shape
    representations.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于深度学习的各种3D形状表示方法的时间线。
- en: 2 Image-based methods
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基于图像的方法
- en: 2D images are the projections of 3D entities. Although the geometric information
    carried by one image is incomplete, a plausible 3D shape could be inferred from
    a set of images with different perspectives. The extra channel of depth in RGB-D
    data further enhances the capacity of image-based representations on encoding
    geometric cues. Benefiting from its image-like structure, the research using deep
    neural networks on 3D shape inferences from images started earlier than alternative
    representations that can depict the surface or geometry of 3D shapes explicitly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2D图像是3D实体的投影。虽然单张图像所携带的几何信息是不完整的，但可以从一组不同视角的图像中推断出一个合理的3D形状。RGB-D数据中的深度通道进一步增强了基于图像的表示在编码几何线索方面的能力。由于其类似图像的结构，使用深度神经网络从图像中进行3D形状推断的研究早于那些可以明确描绘3D形状表面或几何的替代表示方法。
- en: Socher et al. [[89](#bib.bib89)] proposed a convolutional and recursive neural
    network for 3D object recognition, which copes with RGB and depth images by single
    convolutional layers separately and merges the features by a recursive network.
    Eigen et al. [[20](#bib.bib20)] first proposed to reconstruct the depth map from
    a single RGB image and designed a new scale invariant loss for the training stage.
    Gupta et al. [[37](#bib.bib37)] encoded the depth map into three channels including
    disparity, height and angle. Other deep learning methods based on RGB-D images
    are designed for 3D object detection [[36](#bib.bib36), [91](#bib.bib91)], outperforming
    previous methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Socher等人[[89](#bib.bib89)]提出了一种卷积和递归神经网络用于3D对象识别，该网络通过单独的卷积层处理RGB和深度图像，并通过递归网络合并特征。Eigen等人[[20](#bib.bib20)]首次提出从单张RGB图像重建深度图，并为训练阶段设计了一种新的尺度不变损失。Gupta等人[[37](#bib.bib37)]将深度图编码为包括视差、高度和角度的三个通道。其他基于RGB-D图像的深度学习方法被设计用于3D对象检测[[36](#bib.bib36),
    [91](#bib.bib91)]，表现优于之前的方法。
- en: Images from different viewpoints can provide complementary cues to infer 3D
    objects. Thanks to the development of deep learning models in 2D fields, the learning
    methods based on multi-view image representation perform better in the 3D shape
    recognition application than those based on other 3D representations. Su et al. [[93](#bib.bib93)]
    proposed MVCNN (Multi-View Convolutional Neural Network) for 3D object recognition.
    MVCNN first processes the images in different views separately by the first part
    of CNN, then aggregates the features extracted from different views by view-pooling
    layers, and finally puts the merged feature to the remaining part of CNN. Qi et
    al. [[76](#bib.bib76)] propose to add multi-resolution into MVCNN for higher classification
    accuracy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从不同视角获取的图像可以提供互补线索以推断3D对象。得益于深度学习模型在2D领域的发展，基于多视图图像表示的学习方法在3D形状识别应用中表现优于基于其他3D表示的方法。Su等人[[93](#bib.bib93)]提出了MVCNN（多视图卷积神经网络）用于3D对象识别。MVCNN首先通过CNN的第一部分分别处理不同视角的图像，然后通过视图池化层聚合从不同视角提取的特征，最后将合并后的特征输入到CNN的剩余部分。Qi等人[[76](#bib.bib76)]提出在MVCNN中加入多分辨率以提高分类准确性。
- en: 3 Voxel-based representations
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 体素基表示
- en: 3.1 Dense Voxel Representation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 密集体素表示
- en: The voxel-based representation is traditionally a dense representation, which
    describes 3D shape data by volumetric grids in 3D space. Each voxel in the grid
    records the status of occupancy (e.g., occupied or unoccupied) within a cuboid
    grid.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 体素基表示传统上是一种密集表示，它通过3D空间中的体积网格描述3D形状数据。网格中的每个体素记录了在立方体网格内的占用状态（例如，已占用或未占用）。
- en: 'One of the earliest methods that applies deep neural networks to volumetric
    representations was proposed by Wu et al. [[112](#bib.bib112)] in 2015, which
    is called 3D ShapeNets. Wu et al. assigned three different states to the voxels
    in the volumetric representation produced by 2.5D depth maps: observed, unobserved
    and free. 3D ShapeNets extended the deep belief network (DBN) [[41](#bib.bib41)]
    from pixel data to voxel data and replaced fully connected layers in DBN with
    convolutional layers. The model takes the aforementioned volumetric representation
    as input, and outputs category labels and predicted 3D shape by iterative computations.
    Concurrently, Maturana et al. proposed to process the volumetric representation
    with 3D Convolutional Neural Networks (3D CNNs) [[62](#bib.bib62)] and designed
    VoxNet [[63](#bib.bib63)] for object recognition. VoxNet defines several volumetric
    layers, including Input Layer, Convolutional Layers, Pooling Layers and Fully
    Connected Layers. Although these defined layers are simple extensions of traditional
    2D CNNs [[48](#bib.bib48)] to 3D, VoxNet is easy to implement and train and gets
    promising performance as the first attempt on volumetric convolutions. In addition,
    to ensure that VoxNet is invariant to orientation, Maturana et al. further augment
    the input data by rotating each shape into $n$ instances with different orientations
    in the training stage and adding a pooling operation after the output layer to
    group all the predictions from the $n$ instances in the test stage.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 应用深度神经网络到体积表示的最早方法之一是由吴等人于2015年提出的，这种方法称为3D ShapeNets。吴等人将体积表示中的体素分配为三种不同的状态：已观察、未观察和自由。3D
    ShapeNets将深度置信网络（DBN）从像素数据扩展到体素数据，并将DBN中的全连接层替换为卷积层。该模型将上述体积表示作为输入，通过迭代计算输出类别标签和预测的3D形状。与此同时，Maturana等人提出使用3D卷积神经网络（3D
    CNNs）处理体积表示，并设计了用于物体识别的VoxNet。VoxNet定义了几个体积层，包括输入层、卷积层、池化层和全连接层。虽然这些定义的层是将传统的2D
    CNNs扩展到3D的简单扩展，但VoxNet易于实现和训练，并在体积卷积的首次尝试中取得了有希望的性能。此外，为了确保VoxNet对方向的不变性，Maturana等人在训练阶段通过将每个形状旋转成$n$个不同方向的实例来扩充输入数据，并在输出层后添加一个池化操作，以在测试阶段将所有来自$n$个实例的预测结果进行分组。
- en: In addition to the development of deep belief networks and convolutional neural
    networks in shape analysis based on volumetric representation, two most successful
    generative models, namely auto-encoders and Generative Adversarial Networks (GANs) [[33](#bib.bib33)]
    are also extended to support this representation. Inspired by Denoising Auto-Encoders
    (DAEs) [[101](#bib.bib101), [102](#bib.bib102)], Sharma et al. proposed an autoencoder
    model VConv-DAE for coping with voxels [[83](#bib.bib83)]. It is one of the earliest
    unsupervised learning approaches in voxel-based shape analysis to our knowledge.
    Without object labels for training, VConv-DAE chooses mean square loss or cross
    entropy loss as the reconstruction loss function. Girdhar et al. [[32](#bib.bib32)]
    also proposed TL-embedding Network, which combine an auto-encoder for generating
    a voxel-based representation with a convolutional neural network for predicting
    the embeddings from the 2D images.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于体积表示的形状分析中深度置信网络和卷积神经网络的发展外，两个最成功的生成模型，即自编码器和生成对抗网络（GANs）也被扩展以支持这种表示。受去噪自编码器（DAEs）的启发，Sharma等人提出了一种自编码器模型VConv-DAE，用于处理体素。这是我们所知的最早的体素基础形状分析的无监督学习方法之一。在没有对象标签进行训练的情况下，VConv-DAE选择均方误差或交叉熵损失作为重建损失函数。Girdhar等人还提出了TL-embedding网络，该网络结合了一个自编码器用于生成基于体素的表示和一个卷积神经网络用于从2D图像中预测嵌入。
- en: 'Choy et al. [[16](#bib.bib16)] proposed 3D-R2N2 which takes single or multiple
    images as input and reconstructs objects in occupancy grids. 3D-R2N2 regards input
    images as a sequence and designs the 3D recurrent neural network based on LSTM
    (Long Short-Term Memory) [[42](#bib.bib42)] or GRU (Gated Recurrent Unit) [[15](#bib.bib15)].
    The architecture consists of three parts: an image encoder to extract features
    from 2D images, 3D-LSTM to predict hidden states as coarse representations of
    final 3D models, and a decoder to increase the resolution and generate target
    shapes.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Choy 等人[[16](#bib.bib16)] 提出了 3D-R2N2，该方法将单张或多张图像作为输入，并在占据网格中重建物体。3D-R2N2 将输入图像视为一个序列，并基于
    LSTM（长短期记忆）[[42](#bib.bib42)] 或 GRU（门控递归单元）[[15](#bib.bib15)] 设计了3D递归神经网络。该架构包括三个部分：一个图像编码器用于从2D图像中提取特征，3D-LSTM
    用于预测隐藏状态作为最终3D模型的粗略表示，以及一个解码器用于增加分辨率并生成目标形状。
- en: Wu et al. [[110](#bib.bib110)] designed a generative model called 3D-GAN that
    applies the Generative Adversarial Network (GAN) [[33](#bib.bib33)] in voxel data.
    3D GAN learns to synthesize a 3D object from a sampled latent space vector $z$
    with the probability distribution $P(z)$. Moreover, [[110](#bib.bib110)] also
    proposed 3D-VAE-GAN inspired by VAE-GAN [[49](#bib.bib49)] for the object reconstruction
    task. 3D-VAE-GAN puts the encoder before 3D-GAN for inferring the latent vector
    $z$ from input 2D images and shares the decoder with the generator of 3D-GAN.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人[[110](#bib.bib110)] 设计了一个名为 3D-GAN 的生成模型，该模型将生成对抗网络（GAN）[[33](#bib.bib33)]
    应用于体素数据。3D GAN 学会从采样的潜在空间向量 $z$ 中合成一个3D物体，且概率分布为 $P(z)$。此外，[[110](#bib.bib110)]
    还提出了受 VAE-GAN[[49](#bib.bib49)] 启发的 3D-VAE-GAN，用于物体重建任务。3D-VAE-GAN 在 3D-GAN 之前放置了编码器，用于从输入的2D图像中推断潜在向量
    $z$，并与3D-GAN的生成器共享解码器。
- en: After the early attempts in dealing with volumetric representations by deep
    learning, researchers began to optimize the architecture of volumetric networks
    for better performance and more applications. A motivation is that the naive extension
    from traditional 2D domain networks often does not perform better than image-based
    CNNs such as MVCNN [[93](#bib.bib93)]. The main challenges affecting the performance
    include overfitting, orientation, data sparsity and low resolution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期处理体积表示的深度学习尝试之后，研究人员开始优化体积网络的架构，以提高性能和拓展应用。一个动机是，传统2D领域网络的简单扩展往往不如基于图像的CNNs（如MVCNN[[93](#bib.bib93)]）表现得更好。影响性能的主要挑战包括过拟合、方向、数据稀疏性和低分辨率。
- en: Qi et al. [[76](#bib.bib76)] proposed two new network structures aiming to improve
    the performance of volumetric CNNs. One introduces an extra task namely predicting
    class labels with subvolume space to prevent overfitting, and another utilizes
    elongated kernels to compress the 3D information into the 2D field in order to
    use 2D CNNs directly. Both of them use mlpconv layers [[55](#bib.bib55)] to replace
    traditional convolutional layers. [[76](#bib.bib76)] also augments the input data
    in different orientation and elevation to encourage the network to get more local
    features in different poses so that the results are less influenced by orientation
    changes. To further mitigate the orientation impact on recognition accuracy, instead
    of using data augmentation like [[63](#bib.bib63), [76](#bib.bib76)], [[82](#bib.bib82)]
    proposed a new model called ORION which extends VoxNet [[63](#bib.bib63)] and
    uses a fully connected layer to predict the object class label and orientation
    label simultaneously.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Qi 等人[[76](#bib.bib76)] 提出了两种新的网络结构，旨在提高体积卷积神经网络（CNNs）的性能。其中一种引入了一个额外的任务，即利用子体积空间预测类别标签，以防止过拟合；另一种则利用延长的卷积核将3D信息压缩到2D领域中，以便直接使用2D
    CNNs。这两种方法都使用了 mlpconv 层[[55](#bib.bib55)] 来替代传统的卷积层。[[76](#bib.bib76)] 还通过不同的方向和高度来增强输入数据，以鼓励网络在不同的姿态下获取更多的局部特征，从而使结果不易受到方向变化的影响。为了进一步减轻方向对识别准确性的影响，[[82](#bib.bib82)]
    提出了一个名为 ORION 的新模型，该模型扩展了 VoxNet[[63](#bib.bib63)]，并使用全连接层同时预测物体类别标签和方向标签。
- en: 3.2 Sparse Voxel Representation (Octree)
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 稀疏体素表示（Octree）
- en: Voxel-based representations often lead to high computational cost because of
    the exponential increase of computations from pixels to voxels. Most of the methods
    cannot cope with or generate high-resolution models within reasonable time. For
    instance, TL-embedding Network [[32](#bib.bib32)] was designed for $20^{3}$ voxel
    grids; 3DShapeNets [[112](#bib.bib112)] and VConv-DAE [[83](#bib.bib83)] were
    designed for $24^{3}$ voxel grids with 3 voxels padding on each direction of the
    voxel grids; VoxNet [[63](#bib.bib63)], 3D-R2N2 [[16](#bib.bib16)] and ORION [[82](#bib.bib82)]
    were designed for $32^{3}$ voxel grids; 3D-GAN was designed for generating $64^{3}$
    occupancy grids as 3D shape representation. As the voxel resolution increases,
    the occupied grids become sparser in the whole 3D space, which leads to more unnecessary
    computation. To address this problem, Li et al. [[54](#bib.bib54)] designed a
    novel method called FPNN to cope with the data sparsity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基于体素的表示通常会导致高计算成本，因为从像素到体素的计算量呈指数增长。大多数方法无法应对或在合理时间内生成高分辨率模型。例如，TL-embedding
    Network [[32](#bib.bib32)] 设计用于 $20^{3}$ 体素网格；3DShapeNets [[112](#bib.bib112)]
    和 VConv-DAE [[83](#bib.bib83)] 设计用于 $24^{3}$ 体素网格，每个方向上有 3 体素填充；VoxNet [[63](#bib.bib63)]、3D-R2N2 [[16](#bib.bib16)]
    和 ORION [[82](#bib.bib82)] 设计用于 $32^{3}$ 体素网格；3D-GAN 设计用于生成 $64^{3}$ 占据网格作为 3D
    形状表示。随着体素分辨率的增加，占据网格在整个 3D 空间中变得更加稀疏，从而导致更多不必要的计算。为了解决这个问题，Li 等人 [[54](#bib.bib54)]
    设计了一种新方法，称为 FPNN，以应对数据稀疏性。
- en: Some methods instead encode the voxel grids by a sparse, adaptive data structure,
    namely octree [[64](#bib.bib64)] to reduce the dimensionality of the input data.
    Häne et al. [[38](#bib.bib38)] proposed Hierarchical Surface Prediction (HSP)
    that can generate voxel grids in the form of octree from coarse to fine. Häne
    et al. observed that only the voxels near the object surface need to be predicted
    in a high resolution, so that the proposed HSP can avoid unnecessary calculation
    to ensure affordable generation of high resolution voxel grids. As introduced
    in [[38](#bib.bib38)], each node in the octree is defined as a voxel block with
    a fixed number ($16^{3}$ in the paper) of voxels in different size, and each voxel
    block is classified into occupied, boundary and free. The decoder of the model
    takes a feature vector as input, and predicts feature blocks that correspond to
    voxel blocks hierarchically. The HSP defines that the octree has 5 layers and
    each voxel blocks contains $16^{3}$ voxels, therefore, HSP can generate up to
    $256^{3}$ voxel grids. Tatarchenko et al. [[98](#bib.bib98)] also proposed a decoder
    called OGN for generating high resolution volumetric representations. In [[98](#bib.bib98)],
    nodes in the octree are separated into three categories, including “empty”, “filled”
    and “mixed”. The octree representing a 3D model and the feature map of the octree
    are stored in the form of hashing tables which are indexed by the spatial position
    and the octree level. In order to process the feature maps represented as hash
    tables, Tatarchenko et al. designed a convolutional layer named OGN-Conv, which
    converts the convolutional operation into matrix multiplication. [[98](#bib.bib98)]
    adopts the method that generates different resolution of voxel grids in each decoder
    layer by convolutional operations in feature maps, and then decides whether to
    propagate the features to the next layer by specific labels (propagating the features
    if “boundary” and skipping the feature propagation if “mixed”).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法则通过稀疏的自适应数据结构，即八叉树 [[64](#bib.bib64)]，来编码体素网格，从而减少输入数据的维度。Häne 等人 [[38](#bib.bib38)]
    提出了层次表面预测（HSP），可以从粗到细生成八叉树形式的体素网格。Häne 等人观察到，仅需对靠近物体表面的体素进行高分辨率预测，因此提出的 HSP 可以避免不必要的计算，以确保高分辨率体素网格的生成是可承受的。如
    [[38](#bib.bib38)] 中介绍的，八叉树中的每个节点定义为一个体素块，具有固定数量（论文中为 $16^{3}$）的不同大小的体素，每个体素块被分类为占用、边界和自由。模型的解码器以特征向量作为输入，并分层预测对应于体素块的特征块。HSP
    定义八叉树有 5 层，每个体素块包含 $16^{3}$ 个体素，因此，HSP 可以生成最多 $256^{3}$ 体素网格。Tatarchenko 等人 [[98](#bib.bib98)]
    还提出了一种名为 OGN 的解码器，用于生成高分辨率体积表示。在 [[98](#bib.bib98)] 中，八叉树中的节点分为三类，包括“空”、“填充”和“混合”。表示
    3D 模型的八叉树和八叉树的特征图以哈希表的形式存储，这些哈希表由空间位置和八叉树级别索引。为了处理表示为哈希表的特征图，Tatarchenko 等人设计了一种名为
    OGN-Conv 的卷积层，将卷积操作转换为矩阵乘法。[[98](#bib.bib98)] 采用的方法是在每个解码器层中通过特征图的卷积操作生成不同分辨率的体素网格，然后根据特定标签决定是否将特征传播到下一层（如果是“边界”则传播特征，如果是“混合”则跳过特征传播）。
- en: Besides the decoder model design for synthesizing voxel grids, shape analysis
    methods are also designed using octrees. However, conventional octree structure [[64](#bib.bib64)]
    has difficulty to be used in deep networks, so many researchers try to resolve
    the problem by designing new structures of octrees and special operations such
    as convolution, pooling and unpooling on octrees. Riegler et al. [[80](#bib.bib80)]
    proposed OctNet. The octree representation mentioned in [[80](#bib.bib80)] has
    a relatively regular structure than a traditional octree, which places a shallow
    octree in regular 3D grids. The shallow octree is constrained to have up to 3
    levels and is encoded in 73 bits. Each bit determines if the corresponding cell
    needs to be split. Wang et al. [[105](#bib.bib105)] also proposed a convolutional
    neural network based on octree called O-CNN, where the model also removes pointers
    like shallow octree [[80](#bib.bib80)] and stores the octree data and structure
    by a series of vectors including shuffle key vectors, labels and input signals.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于合成体素网格的解码器模型设计外，形状分析方法也使用八叉树进行设计。然而，传统的八叉树结构[[64](#bib.bib64)]在深度网络中难以使用，因此许多研究人员尝试通过设计新的八叉树结构和特殊操作（如卷积、池化和反池化）来解决这个问题。Riegler等人[[80](#bib.bib80)]提出了OctNet。[[80](#bib.bib80)]中提到的八叉树表示相比传统八叉树具有相对规则的结构，它将一个浅层八叉树放置在规则的三维网格中。该浅层八叉树限制最多有3个层级，并以73位进行编码。每一位决定相应的单元是否需要拆分。Wang等人[[105](#bib.bib105)]还提出了一种基于八叉树的卷积神经网络，称为O-CNN，其中模型也去除了类似于浅层八叉树[[80](#bib.bib80)]的指针，并通过包括洗牌键向量、标签和输入信号的一系列向量来存储八叉树数据和结构。
- en: In addition to representing voxels, octree structure can also be utilized to
    represent 3D shape surfaces with planar patches. Wang et al. [[106](#bib.bib106)]
    proposed Adaptive O-CNN, where they defined another form of octree named patch-guided
    adaptive octree, which divides a 3D shape surface into a set of planar patches
    restricted by bounding boxes corresponding to octants. They also provided an encoder
    and a decoder for the octree defined by this paper.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了表示体素外，八叉树结构还可以用于表示带有平面补丁的三维形状表面。Wang等人[[106](#bib.bib106)]提出了自适应O-CNN，他们定义了另一种形式的八叉树，称为补丁引导自适应八叉树，该树将三维形状表面划分为一组由对应于八分体的边界框限制的平面补丁。他们还为本文定义的八叉树提供了一个编码器和解码器。
- en: 4 Surface-based representations
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于表面的表示
- en: 4.1 Point-based Representation
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于点的表示
- en: The typical point-based representation is also referred to as point clouds or
    point sets. They can be raw data generated by 3D scanning devices. Because of
    its unordered and irregular structure, this kind of representation is relatively
    difficult to cope with by traditional deep learning methods. Therefore, most researchers
    avoided to use point clouds in a direct way at the early stage of the deep learning-based
    geometry research. One of the first models to generate point clouds by deep learning
    came out in 2017 [[21](#bib.bib21)]. They designed a neural network to learn a
    point sampler based on 3D shape point distribution. The network takes a single
    image and a random vector as input, and outputs an $N\times 3$ matrix representing
    the predicted point sets ($x$, $y$, $z$ coordinates for $N$ points). In addition,
    [[21](#bib.bib21)] proposed to use Chamfer Distance (CD) and Earth Mover’s Distance
    (EMD) [[81](#bib.bib81)] as the loss function to train the networks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的基于点的表示也被称为点云或点集。它们可以是由三维扫描设备生成的原始数据。由于其无序和不规则的结构，这种表示方法相对较难被传统的深度学习方法处理。因此，大多数研究人员在深度学习基础的几何研究初期避免直接使用点云。2017年出现了第一个通过深度学习生成点云的模型[[21](#bib.bib21)]。他们设计了一种神经网络，用于学习基于三维形状点分布的点采样器。该网络以单张图像和一个随机向量作为输入，输出一个$N\times
    3$矩阵，表示预测的点集（$N$个点的$x$、$y$、$z$坐标）。此外，[[21](#bib.bib21)]提出使用Chamfer距离（CD）和地球搬运工距离（EMD）[[81](#bib.bib81)]作为损失函数来训练网络。
- en: PointNet. At almost the same time, Qi et al. [[75](#bib.bib75)] proposed PointNet
    for shape analysis, which was the first successful deep network architecture that
    directly processes point clouds without unnecessary rendering. The pipeline of
    PointNet is illustrated in Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Point-based Representation
    ‣ 4 Surface-based representations"). On account of three properties of point sets
    mentioned in [[75](#bib.bib75)], PointNet designed three components in their network,
    including using max-pooling layers as symmetry functions for dealing with the
    unordered property, concatenating global and local features together for point
    interaction, and jointly aligning the network for transformation invariance. Based
    on PointNet, Qi et al. further improved this model and proposed PointNet++ [[77](#bib.bib77)],
    in order to resolve the problem that PointNet cannot capture and deal with local
    features induced by metric well. Compared with PointNet, PointNet++ introduces
    a hierarchical structure, so that the model can capture features in different
    scales, which improves the capability of extracting 3D shape features. As PointNet
    and PointNet++ show state-of-the-art performance in shape classification and semantic
    segmentation, more and more deep learning models were proposed based on point-based
    representations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: PointNet。几乎在同一时间，Qi等人[[75](#bib.bib75)]提出了用于形状分析的PointNet，这是第一个成功的深度网络架构，直接处理点云而无需不必要的渲染。PointNet的管道如图[2](#S4.F2
    "Figure 2 ‣ 4.1 Point-based Representation ‣ 4 Surface-based representations")所示。根据[[75](#bib.bib75)]中提到的点集的三个属性，PointNet在其网络中设计了三个组件，包括使用最大池化层作为对无序属性进行处理的对称函数，将全局和局部特征连接在一起以进行点间交互，并联合对齐网络以实现变换不变性。在PointNet的基础上，Qi等人进一步改进了该模型，提出了PointNet++[[77](#bib.bib77)]，以解决PointNet无法有效捕捉和处理由度量引起的局部特征的问题。与PointNet相比，PointNet++引入了分层结构，使得模型能够捕捉不同尺度的特征，从而提高了提取3D形状特征的能力。由于PointNet和PointNet++在形状分类和语义分割中表现出最先进的性能，越来越多基于点表示的深度学习模型被提出。
- en: '![Refer to caption](img/f48dfdedb86af55dc200d7232469297d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f48dfdedb86af55dc200d7232469297d.png)'
- en: 'Figure 2: The pipeline of PointNet Ref. [[75](#bib.bib75)], ©IEEE 2017.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：PointNet的管道参考[[75](#bib.bib75)]，©IEEE 2017。
- en: Convolutional Neural Networks for Point Clouds. Some research works focus on
    applying CNNs to the irregular and unordered form of point clouds for analysis.
    Li et al. [[53](#bib.bib53)] proposed PointCNN for point clouds and designed the
    $\mathcal{X}$-transformation to weight and permute the input point features, which
    guarantees the equivariance in different point orders. Each feature matrix needs
    to be multiplied by the $\mathcal{X}$-transformation matrix before passing through
    the convolutional operator. This process is named $\mathcal{X}$-Conv operator,
    which is the key of PointCNN. Wang et al. [[108](#bib.bib108)] proposed DGCNN,
    a dynamic graph CNN architecture for point cloud classification and segmentation.
    Instead of processing point features like PointNet [[75](#bib.bib75)], DGCNN first
    connects neighboring points in spatial or semantic space to generate a graph,
    and then captures the local geometry features by applying the EdgeConv operator
    on it. Moreover, different from other graph CNNs which process the fixed input
    graph, DGCNN changes the graph to obtain new nearest neighbors in the feature
    space in different layers, which is beneficial to get larger and sparser receptive
    fields.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 点云的卷积神经网络。一些研究工作专注于将CNN应用于点云的非规则和无序形式进行分析。Li等人[[53](#bib.bib53)]提出了用于点云的PointCNN，并设计了$\mathcal{X}$-transformation来加权和排列输入点特征，这保证了不同点顺序下的等变性。每个特征矩阵在通过卷积操作符之前需要乘以$\mathcal{X}$-transformation矩阵。这个过程被称为$\mathcal{X}$-Conv操作符，是PointCNN的关键。Wang等人[[108](#bib.bib108)]提出了DGCNN，一种用于点云分类和分割的动态图CNN架构。与处理点特征的PointNet[[75](#bib.bib75)]不同，DGCNN首先在空间或语义空间中连接邻近点以生成图，然后通过在图上应用EdgeConv操作符来捕捉局部几何特征。此外，与其他处理固定输入图的图CNN不同，DGCNN在不同层中改变图以获得特征空间中新邻近点，这有助于获得更大且更稀疏的感受野。
- en: Other Point Cloud Processing Techniques using Neural Networks. Klokov et al. [[47](#bib.bib47)]
    proposed Kd-Network to process point clouds based on the form of kd-trees. Yang
    et al. [[117](#bib.bib117)] proposed FoldingNet, an end-to-end auto-encoder for
    further compressing a point-based representation with unsupervised learning. Because
    point clouds can be transformed into 2D grids by folding operations, FoldingNet
    integrates folding operations in their encoder-decoder to recover input 3D shapes.
    Mehr et al. [[65](#bib.bib65)] further proposed DiscoNet for 3D model editing
    by combining multiple autoencoders which are trained for different types of 3D
    shapes specifically. The autoencoders use pre-learned mean geometry of training
    3D shapes as their templates. Meng et al. [[66](#bib.bib66)] proposed VV-Net (Voxel
    VAE Net) for point segmentation, which represents a point cloud by a structured
    voxel representation. In VV-Net, instead of containing a boolean value to represent
    occupancy status of each voxel as a normal volumetric representation, it uses
    a latent code computed by an RBF-VAE, a variational autoencoder based on a radial
    basis function (RBF) interpolation of points to describe point distribution within
    a voxel. This representation is used to extract intrinsic symmetry of point clouds
    by a group equivariant CNN, and the output is combined with PointNet [[75](#bib.bib75)]
    for better segmentation performance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络的其他点云处理技术。Klokov等人 [[47](#bib.bib47)] 提出了Kd-Network，用于基于kd树的点云处理。Yang等人 [[117](#bib.bib117)]
    提出了FoldingNet，这是一种端到端的自编码器，用于通过无监督学习进一步压缩基于点的表示。由于点云可以通过折叠操作转换为2D网格，FoldingNet在其编码器-解码器中集成了折叠操作以恢复输入的3D形状。Mehr等人 [[65](#bib.bib65)]
    进一步提出了DiscoNet，通过结合多个针对不同类型3D形状专门训练的自编码器进行3D模型编辑。这些自编码器使用训练3D形状的预学习平均几何形状作为其模板。Meng等人 [[66](#bib.bib66)]
    提出了VV-Net（体素VAE网络）用于点分割，该方法通过结构化体素表示来表示点云。在VV-Net中，它使用由RBF-VAE计算的潜在代码来描述体素内的点分布，而不是像常规体积表示那样包含一个布尔值来表示每个体素的占用状态。该表示方法用于通过群等变CNN提取点云的内在对称性，并将输出与PointNet [[75](#bib.bib75)]
    结合，以获得更好的分割性能。
- en: Although the point-based representation can be more easily obtained by 3D scanners
    than other 3D representations, this raw form of 3D shapes is often unsuitable
    for 3D shape analysis, due to noise and data sparsity. Therefore, compared with
    other representations, it is essential for the point-based representation to incorporate
    an upsampling module to obtain fine-grained point clouds, such as PU-NET [[119](#bib.bib119)],
    MPU [[118](#bib.bib118)], PU-GAN [[52](#bib.bib52)], etc. Additionally, point
    cloud registration is also an essential preprocessing step, e.g. to fuse points
    from multiple scans, which aims to calculate rigid transformation parameters to
    align the point clouds. Wang et al. [[107](#bib.bib107)] proposed Deep Closest
    Point (DCP), which extends traditional Iteractive Closest Point (ICP) method [[4](#bib.bib4)]
    and uses a deep learning method to obtain the transformation parameters. Recently,
    Guo et al. [[35](#bib.bib35)] presented a survey focusing on deep learning models
    in point clouds, which provides more details in this field.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于点的表示方法比其他3D表示方法更容易通过3D扫描仪获得，但由于噪声和数据稀疏，这种原始的3D形状形式通常不适合3D形状分析。因此，与其他表示方法相比，基于点的表示方法必须结合上采样模块以获取细粒度的点云，例如PU-NET [[119](#bib.bib119)]、MPU [[118](#bib.bib118)]、PU-GAN [[52](#bib.bib52)]等。此外，点云配准也是一个重要的预处理步骤，例如，融合多个扫描点，旨在计算刚性变换参数以对齐点云。Wang等人 [[107](#bib.bib107)]
    提出了Deep Closest Point (DCP)，该方法扩展了传统的迭代最近点（ICP）方法 [[4](#bib.bib4)]，并使用深度学习方法获得变换参数。最近，Guo等人 [[35](#bib.bib35)]
    提出了一个重点关注点云中的深度学习模型的调查，提供了该领域的更多细节。
- en: 4.2 Mesh-based Representations
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于网格的表示方法
- en: Compared with point-based representations, mesh-based representations contain
    connectivity between neighboring points, so they are more suitable for describing
    local regions on surfaces. As a typical type of representation in non-Euclidean
    space, mesh-based representations can be processed by deep learning models both
    in spatial and spectral domains [[9](#bib.bib9)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于点的表示方法相比，基于网格的表示方法包含邻近点之间的连通性，因此更适合描述表面上的局部区域。作为非欧几里得空间中的典型表示类型，基于网格的表示方法可以在空间和谱域中通过深度学习模型进行处理 [[9](#bib.bib9)]。
- en: Parametric representations for meshes. Directly applying CNNs to irregular data
    structures like meshes is non-trivial, so there emerged a handful of approaches
    that map 3D shape surfaces to 2D domains such as 2D geometry images which can
    also be regarded as another 3D shape representation, and apply traditional 2D
    CNNs on them [[87](#bib.bib87), [60](#bib.bib60)]. Based on geometry images, Sinha
    et al. [[88](#bib.bib88)] proposed SurfNet for shape generation using a deep residual
    network. Similarly, Shi et al. [[84](#bib.bib84)] projected 3D models into cylinder
    panoramic images, which are processed by CNNs. Some other methods convert mesh
    models into spherical signals, and design a convolutional operator in the spherical
    domain for shape analysis. To address high-resolution signals on 3D meshes, in
    particular texture information, Huang et al. [[43](#bib.bib43)] proposed TextureNet
    to extract features in this situation, where a 4-rotational symmetric (4-RoSy)
    field is defined to parametrize surfaces. In the following, we will review deep
    learning models according to how meshes are directly treated as input, and introduce
    generative models working on meshes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 网格的参数化表示。将 CNNs 直接应用于像网格这样的不规则数据结构并不简单，因此出现了一些将 3D 形状表面映射到 2D 域的方法，例如可以视为另一种
    3D 形状表示的 2D 几何图像，并在其上应用传统的 2D CNNs[[87](#bib.bib87), [60](#bib.bib60)]。基于几何图像，Sinha
    等人[[88](#bib.bib88)] 提出了 SurfNet，用于通过深度残差网络生成形状。类似地，Shi 等人[[84](#bib.bib84)] 将
    3D 模型投影到圆柱全景图像中，由 CNNs 处理。一些其他方法将网格模型转换为球面信号，并在球面域中设计卷积操作符以进行形状分析。为了解决 3D 网格上的高分辨率信号，特别是纹理信息，Huang
    等人[[43](#bib.bib43)] 提出了 TextureNet 以在这种情况下提取特征，其中定义了一个 4-旋转对称 (4-RoSy) 场来参数化表面。接下来，我们将回顾根据网格直接作为输入处理的深度学习模型，并介绍在网格上工作的生成模型。
- en: Graphs. The mesh-based representation is constructed by sets of vertices and
    edges, which can be seen as a graph. Some models were proposed based on the graph
    spectral theorem. They generalize CNNs on graphs [[10](#bib.bib10), [40](#bib.bib40),
    [18](#bib.bib18), [46](#bib.bib46), [2](#bib.bib2)] by eigen-decomposition of
    Laplacian matrices, which is able to generalize convolutional operators to the
    spectral domain of graphs. Verma et al. [[100](#bib.bib100)] proposed another
    graph-based CNN named FeaStNet, which computes the receptive fields of convolution
    operator dynamically. Specifically, FeaStNet determines the assignment of the
    neighbor vertices by using features obtained in networks. Hanocka et al. [[39](#bib.bib39)]
    also designed operators of convolution, pooling and unpooling for triangle meshes,
    and proposed MeshCNN. Different from other graph-based methods, MeshCNN focuses
    on processing the features stored in edges, and proposes a convolution operator
    that is applied to the edges with a fixed number of neighbors and a pooling operator
    based on edge collapse. MeshCNN extracts 3D shape features with respect to specific
    tasks, and the network learns to preserve the important features and ignore the
    unimportant ones.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图形。基于网格的表示是通过顶点和边的集合构建的，可以视为一种图形。基于图谱定理提出了一些模型。这些模型通过对拉普拉斯矩阵的特征分解来推广图形上的CNNs[[10](#bib.bib10),
    [40](#bib.bib40), [18](#bib.bib18), [46](#bib.bib46), [2](#bib.bib2)]，能够将卷积操作符推广到图形的谱域。Verma
    等人[[100](#bib.bib100)] 提出了另一种基于图形的 CNN，名为 FeaStNet，该网络动态计算卷积操作符的感受野。具体来说，FeaStNet
    通过使用在网络中获得的特征来确定邻居顶点的分配。Hanocka 等人[[39](#bib.bib39)] 还为三角网格设计了卷积、池化和反池化操作符，并提出了
    MeshCNN。与其他基于图形的方法不同，MeshCNN 侧重于处理存储在边上的特征，并提出了一种应用于具有固定数量邻居的边的卷积操作符，以及一种基于边收缩的池化操作符。MeshCNN
    根据特定任务提取 3D 形状特征，并使网络学习保留重要特征而忽略不重要的特征。
- en: '2-Manifolds. The mesh-based representation can be viewed as the discretization
    of 2-manifolds. Several works are designed in 2-manifolds with a series of refined
    CNN operators to adapt to this non-Euclidean space. These methods define their
    own local patches and kernel functions for generalizing CNN models. Masci et al. [[61](#bib.bib61)]
    proposed Geodesic Convolutional Neural Networks (GCNNs) for manifolds, which extract
    and discretize local geodesic patches and apply convolutional filters on these
    patches in polar coordinates. The convolution operator is designed in the spatial
    domain and their Geodesic CNN is quite similar to conventional CNNs applied in
    Euclidean space. Localized Spectral CNNs [[6](#bib.bib6)] proposed by Boscaini
    et al. apply Windowed Fourier transform to non-Euclidean space. Anisotropic Convolutional
    Neural Networks (ACNNs) [[7](#bib.bib7)] further designed an anisotropic heat
    kernel to replace the isotropic patch operator in GCNN [[61](#bib.bib61)], which
    gives another solution to avoid ambiguity. Xu et al. [[115](#bib.bib115)] proposed
    Directionally Convolutional Networks (DCNs), which defined local patches based
    on faces of the mesh representation. In this work, researchers also designed a
    two-stream network for 3D shape segmentation, which takes local face normals and
    the global face distance histogram as input for training. Moti et al. [[70](#bib.bib70)]
    proposed MoNet to replace the weight functions in [[61](#bib.bib61), [7](#bib.bib7)]
    with Gaussian kernels with learnable parameters. Fey et al. [[22](#bib.bib22)]
    proposed SplineCNN which designed a convolutional operator based on B-splines.
    Pan et al. [[72](#bib.bib72)] designed a surface CNN for 3D irregular surface
    to preserve the standard CNN property of translation equivariance by using parallel
    translation frames and group convolutional operations. Qiao et al. [[78](#bib.bib78)]
    proposed Laplacian Pooling Network (LaplacianNet) for 3D mesh analysis. The LaplacianNet
    considers both spectral and spatial information of the mesh, and contains 3 parts:
    preprocessing features as the network input, Mesh Pooling Blocks to split surface
    and cluster patches for feature extraction, and the Correlation Network to aggregate
    global information.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 2-流形。
- en: Generative Models. There are also many generative models for the mesh-based
    representation. Wang et al. [[104](#bib.bib104)] proposed Pixel2Mesh for reconstructing
    3D shapes from single images, which generates the target triangular mesh by deforming
    an ellipsoid template. As shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Mesh-based
    Representations ‣ 4 Surface-based representations"), the Pixel2Mesh network is
    implemented based on Graph-based Convolutional Networks (GCNs) [[9](#bib.bib9)]
    and generates the target mesh from coarse to fine by an unpooling operation. Wen
    et al. [[109](#bib.bib109)] advanced Pixel2Mesh and proposed Pixel2Mesh++, which
    extends single image 3D shape reconstruction to 3D shape reconstruction from multi-view
    images. To achieve this, Pixel2Mesh++ introduces a Multi-view Deformation Network
    (MDN) to the original Pixel2Mesh, and the MDN incorporates the cross-view information
    into the process of mesh generation. Groueix et al. [[34](#bib.bib34)] proposed
    AtlasNet, which generates 3D surfaces by multiple patches. AtlasNet learns to
    convert 2D square patches into 2-manifolds to cover the surface of 3D shapes by
    MLP (Multi-Layer Perceptron). Ben-Hamu et al. [[3](#bib.bib3)] proposed a multi-chart
    generative model for 3D shape generation. The method uses a multi-chart structure
    as input and builds the network architecture based on standard image GAN [[33](#bib.bib33)].
    The transformation between 3D surface and multi-chart structure is based on  [[60](#bib.bib60)].
    However, the methods based on deforming a template mesh into the target shape
    cannot express complex topology of some 3D shapes. Pan et al. [[73](#bib.bib73)]
    proposed a new single-view reconstruction method, which combines a deformation
    network and a topology modification network to model meshes with complex topology.
    In the topology modification network, the faces with high distortion are removed.
    Tang et al. [[97](#bib.bib97)] proposed to generate complex topology meshes by
    a skeleton-bridged learning method, because skeleton can well preserve topology
    information. Instead of generating triangular meshes, Nash et al. [[71](#bib.bib71)]
    proposed PolyGen to generate the polygon mesh representation. Inspired by neural
    autoregressive models in other fields like natural language processing, researchers
    regard mesh generation as a sequence, and design a transformer-based network [[99](#bib.bib99)],
    including a vertex model and a face model. The vertex model generates a sequence
    of vertex positions and the face model generates variable-length vertex sequences
    conditioned on input vertices.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型。对于基于网格的表示，也有许多生成模型。Wang 等人[[104](#bib.bib104)] 提出了 Pixel2Mesh 用于从单张图像重建
    3D 形状，该模型通过变形椭球体模板来生成目标三角网格。如图 [3](#S4.F3 "图 3 ‣ 4.2 基于网格的表示 ‣ 4 基于表面的表示") 所示，Pixel2Mesh
    网络基于图卷积网络（GCNs）[[9](#bib.bib9)] 实现，通过一个反池化操作从粗糙到精细地生成目标网格。Wen 等人[[109](#bib.bib109)]
    对 Pixel2Mesh 进行了改进，提出了 Pixel2Mesh++，将单图像 3D 形状重建扩展到多视图图像的 3D 形状重建。为了实现这一点，Pixel2Mesh++
    在原始 Pixel2Mesh 中引入了多视图变形网络（MDN），MDN 将视图间的信息融入网格生成过程。Groueix 等人[[34](#bib.bib34)]
    提出了 AtlasNet，通过多个贴片生成 3D 表面。AtlasNet 通过 MLP（多层感知机）学习将 2D 方形贴片转换为 2-流形，以覆盖 3D 形状的表面。Ben-Hamu
    等人[[3](#bib.bib3)] 提出了一个多图表生成模型用于 3D 形状生成。该方法使用多图表结构作为输入，并基于标准图像 GAN[[33](#bib.bib33)]
    构建网络架构。3D 表面与多图表结构之间的转换基于 [[60](#bib.bib60)]。然而，基于将模板网格变形为目标形状的方法无法表达某些 3D 形状的复杂拓扑。Pan
    等人[[73](#bib.bib73)] 提出了一个新的单视图重建方法，该方法结合了变形网络和拓扑修改网络来建模具有复杂拓扑的网格。在拓扑修改网络中，高度扭曲的面会被移除。Tang
    等人[[97](#bib.bib97)] 提出了通过骨架桥接学习方法生成复杂拓扑网格，因为骨架能够很好地保留拓扑信息。Nash 等人[[71](#bib.bib71)]
    提出了 PolyGen 来生成多边形网格表示。受到自然语言处理等领域的神经自回归模型的启发，研究人员将网格生成视为一个序列，并设计了一个基于变换器的网络[[99](#bib.bib99)]，包括一个顶点模型和一个面模型。顶点模型生成顶点位置的序列，面模型生成基于输入顶点的可变长度顶点序列。
- en: '![Refer to caption](img/6ff955df32f94d5ed156863623aa093f.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6ff955df32f94d5ed156863623aa093f.png)'
- en: 'Figure 3: The pipeline of Pixel2Mesh Ref.[[104](#bib.bib104)] ©Springer 2018.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: Pixel2Mesh 的流程 [[104](#bib.bib104)] ©Springer 2018。'
- en: 5 Implicit representations
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 隐式表示
- en: In addition to explicit representations such as point clouds and meshes, implicit
    fields have been in greater popularity in recent studies. A major reason is that
    the implicit representation is not limited by fixed topology and resolution. There
    are an increasing number of deep models, which define their own implicit representations
    and building on them further propose various methods for shape analysis and generation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了点云和网格等显式表示外，隐式场在近期研究中越来越受欢迎。一个主要原因是隐式表示不受固定拓扑和分辨率的限制。越来越多的深度模型定义了自己的隐式表示，并在此基础上进一步提出了各种形状分析和生成的方法。
- en: The Occupancy/Indicator Function is one of the forms to represent 3D shapes
    implicitly. Occupancy Network was proposed by Mescheder et al. [[67](#bib.bib67)]
    to learn a continuous occupancy function as a new representation of 3D shapes
    by neural networks. The occupancy function reflects the 3D point status with respect
    to the 3D shape surface, where 1 means inside the surface and 0 otherwise. Researchers
    regarded this problem as a binary classification task and designed an occupancy
    network which inputs 3D point position and 3D shape observation and outputs the
    probability of occupancy. The generated implicit field is then processed by a
    Multi-resolution IsoSurface Extraction method MISE and marching cubes algorithm [[58](#bib.bib58)]
    to obtain meshes. Moreover, researchers introduce encoder networks to obtain latent
    embeddings. Similarly, Chen et al. [[14](#bib.bib14)] designed IM-NET as a decoder
    for learning generative models, which also takes an implicit function in the form
    of an indicator function.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 占用/指示函数是隐式表示三维形状的一种形式。Mescheder等人提出了占用网络（Occupancy Network）[[67](#bib.bib67)]，通过神经网络学习一个连续的占用函数作为三维形状的新表示形式。占用函数反映了相对于三维形状表面的三维点状态，其中1表示在表面内部，0表示在外部。研究人员将这个问题视为一个二分类任务，并设计了一个占用网络，该网络输入三维点位置和三维形状观测，并输出占用的概率。生成的隐式场随后通过多分辨率等值面提取方法MISE和marching
    cubes算法[[58](#bib.bib58)]处理以获得网格。此外，研究人员引入编码器网络以获得潜在的嵌入。类似地，Chen等人[[14](#bib.bib14)]设计了IM-NET作为解码器来学习生成模型，该模型也采用了指示函数形式的隐式函数。
- en: Signed Distance Functions (SDFs) are also a form of implicit representation.
    Signed distance functions map a 3D point to a real value instead of a probability,
    which indicates the spatial relation and distance to the 3D surface. Denote $SDF(x)$
    as the signed distance value of a given 3D point $x\in\mathbb{R}^{3}$. Then $SDF(x)>0$
    if point $x$ is outside the 3D shape surface, $SDF(x)<0$ if point $x$ is inside
    the surface, and $SDF(x)=0$ means point $x$ is on the surface. The absolute value
    of $SDF(x)$ refers to the distance between point $x$ and the surface. Park et
    al. [[74](#bib.bib74)] proposed DeepSDF and introduced an auto-decoder-based DeepSDF
    as a new 3D shape representation. Wang et al. [[116](#bib.bib116)] also proposed
    Deep Implicit Surface Networks (DISNs) for single-view 3D reconstruction based
    on SDFs. Thanks to the advantages of SDF, DISN was the first to reconstruct 3D
    shapes with flexible topology and thin structure in the single-view reconstruction
    task, which is difficult for other 3D representations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 符号距离函数（SDFs）也是一种隐式表示形式。符号距离函数将一个三维点映射到一个实际值，而不是概率，这个值表示三维表面的空间关系和距离。设$SDF(x)$为给定三维点$x\in\mathbb{R}^{3}$的符号距离值。则$SDF(x)>0$表示点$x$在三维形状表面之外，$SDF(x)<0$表示点$x$在表面内部，$SDF(x)=0$表示点$x$在表面上。$SDF(x)$的绝对值表示点$x$与表面之间的距离。Park等人[[74](#bib.bib74)]提出了DeepSDF，并引入了一种基于自编码器的DeepSDF作为新的三维形状表示。Wang等人[[116](#bib.bib116)]还提出了基于SDF的深度隐式表面网络（DISNs），用于单视角三维重建。由于SDF的优势，DISN首次在单视角重建任务中重建了具有灵活拓扑和细结构的三维形状，这对于其他三维表示来说是困难的。
- en: Function Sets. The occupancy functions and signed distance functions represent
    the 3D shape surface by a single function learned by a deep neural network. Genova
    et al. [[31](#bib.bib31), [30](#bib.bib30)] proposed to represent the whole 3D
    shape by combining a set of shape elements. In [[31](#bib.bib31)], researchers
    proposed Structured Implicit Functions (SIFs) where each element is represented
    by a scaled axis-aligned anisotropic 3D Gaussian, and the sum of these shape elements
    represents the whole 3D shape. The parameters of Gaussians are learned by the
    CNN. [[30](#bib.bib30)] improved the SIF and proposed Deep Structured Implicit
    Functions (DSIFs) which added deep neural networks as Deep Implicit Functions
    (DIFs) to provide local geometry details. To summarize, DSIF exploits SIF to depict
    coarse information of each shape element, and applies DIF for local shape details.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 函数集。占据函数和带符号距离函数通过深度神经网络学习的单个函数来表示3D形状表面。Genova等人[[31](#bib.bib31), [30](#bib.bib30)]提出通过组合一组形状元素来表示整个3D形状。在[[31](#bib.bib31)]中，研究人员提出了结构化隐式函数（SIFs），其中每个元素由缩放的轴对齐各向异性3D高斯表示，这些形状元素的总和表示整个3D形状。高斯的参数由CNN学习。[[30](#bib.bib30)]改进了SIF，并提出了深度结构化隐式函数（DSIFs），它添加了深度神经网络作为深度隐式函数（DIFs）以提供局部几何细节。总之，DSIF利用SIF描绘每个形状元素的粗略信息，并应用DIF来获取局部形状细节。
- en: Approach without 3D supervision. The above implicit representation models need
    to sample 3D points in the 3D shape bounding box as ground truth and train the
    model supervised with 3D information. But 3D ground truth may not be easy to access
    in some situations. Liu et al. [[56](#bib.bib56)] proposed a framework which learns
    implicit representations without explicit 3D supervision. The model uses a field
    probing algorithm to bridge the gap between the 3D shape and 2D images, and designs
    a silhouette loss to constrain 3D shape outline and geometry regularization to
    constrain the surface to be plausible.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 无需3D监督的方法。上述隐式表示模型需要在3D形状边界框内采样3D点作为真实数据，并用3D信息对模型进行监督训练。但在某些情况下，获取3D真实数据可能并不容易。刘等人[[56](#bib.bib56)]提出了一种框架，可以在没有显式3D监督的情况下学习隐式表示。该模型使用场探测算法弥合3D形状和2D图像之间的差距，并设计了轮廓损失以约束3D形状的轮廓，同时设计了几何正则化以约束表面保持合理。
- en: 6 Structure-based representations
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 基于结构的表示
- en: Recently, more and more researchers began to realize the importance of structure
    of 3D shapes and integrate structural information into deep learning models. Primitive
    representations are a typical type of structure-based representation which depict
    3D shape structure well. A primitive representation represents the 3D shape with
    primitives such as oriented 3D boxes. Instead of providing a description of geometry
    details, the primitive representation concentrates more on the overall structure
    of 3D shapes. It represents 3D shape structure as several primitives with a compact
    parameter set. More importantly, obtaining a primitive representation encourages
    to generate more detailed and plausible 3D shapes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，越来越多的研究人员开始认识到3D形状结构的重要性，并将结构信息整合到深度学习模型中。原始表示是基于结构的表示的一种典型类型，它很好地描述了3D形状的结构。原始表示通过如定向3D盒子等原始元素来表示3D形状。原始表示不提供几何细节的描述，而是更关注3D形状的整体结构。它将3D形状结构表示为具有紧凑参数集的几个原始元素。更重要的是，获得原始表示有助于生成更详细和合理的3D形状。
- en: Linearly Organized. Observing that humans often regard 3D shapes as a collection
    of parts, Zou et al. [[121](#bib.bib121)] proposed 3D-PRNN, which applies LSTM
    in a primitive generator, so that 3D-PRNN can generate primitives sequentially.
    The generated primitive representations show great efficiency in depicting simple
    and regular 3D shapes. Wu et al. [[111](#bib.bib111)] further proposed an RCNN-based
    method called PQ-NET which also regards 3D shape parts as a sequence. The difference
    is that PQ-NET encodes geometry features in the network. Gao et al. [[27](#bib.bib27)]
    proposed a deep generative model named SDM-NET (Structured Deformable Mesh-Net).
    They designed a two-level VAE, containing a PartVAE for part geometry and a SP-VAE
    (Structured Parts VAE) for both structure and geometry features. In [[27](#bib.bib27)],
    each shape part is encoded in a well designed form, which records both the structure
    information (symmetry, supporting and supported) and geometry features.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 线性组织。观察到人类常常将 3D 形状视为部件的集合，Zou 等人 [[121](#bib.bib121)] 提出了 3D-PRNN，它在原始生成器中应用
    LSTM，从而使 3D-PRNN 能够顺序生成原始体素。生成的原始体素表示在描绘简单和规则的 3D 形状时表现出极大的效率。Wu 等人 [[111](#bib.bib111)]
    进一步提出了一种基于 RCNN 的方法，称为 PQ-NET，也将 3D 形状部分视为一个序列。不同之处在于 PQ-NET 在网络中编码几何特征。Gao 等人
    [[27](#bib.bib27)] 提出了一个深度生成模型，名为 SDM-NET（结构可变形网）。他们设计了一个两级 VAE，包括用于部分几何的 PartVAE
    和用于结构与几何特征的 SP-VAE（结构部分 VAE）。在 [[27](#bib.bib27)] 中，每个形状部分都以良好设计的形式进行编码，记录了结构信息（对称、支持和被支持）和几何特征。
- en: 'Hierarchically Organized. Li et al. [[51](#bib.bib51)] proposed GRASS (Generative
    Recursive Autoencoders for Shape Structures), which is one of the first attempts
    to encode the 3D shape structure by a neural network. They describe the shape
    structure by a hierarchical binary tree, in which the child nodes are merged into
    the parent node by either adjacency or symmetry relations. Leaves in this structure
    tree represent the oriented bounding boxes (OBBs) and geometry features for each
    part, and intermediate nodes represent both the geometry feature of child nodes
    and the relations between child nodes. Inspired by recursive neural networks (RvNNs) [[90](#bib.bib90),
    [89](#bib.bib89)], GRASS also recursively merges the codes representing the OBBs
    into a root code which depicts the whole shape structure. The architecture of
    GRASS can be divided into three parts: (1) an RvNN autoencoder for encoding a
    3D shape into a fixed length code, (2) a GAN for learning the distribution of
    root codes and generating plausible structures, (3) another autoencoder for synthesizing
    geometry of each part which is inspired by [[32](#bib.bib32)]. Furthermore, to
    synthesize fine-grained geometry in voxel grids, Structure-aware recursive feature
    (SARF) is proposed, which contains both the geometry features of each part and
    global and local OBB layout.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 层次组织。Li 等人 [[51](#bib.bib51)] 提出了 GRASS（生成递归自编码器用于形状结构），这是通过神经网络对 3D 形状结构进行编码的首批尝试之一。他们通过层次二叉树描述形状结构，其中子节点通过邻接或对称关系合并到父节点。这个结构树中的叶子节点表示每个部分的定向包围盒（OBBs）和几何特征，中间节点表示子节点的几何特征以及子节点之间的关系。受到递归神经网络（RvNNs）
    [[90](#bib.bib90), [89](#bib.bib89)] 的启发，GRASS 也递归地将表示 OBB 的代码合并成一个根代码，描述整个形状结构。GRASS
    的架构可以分为三部分：（1）一个 RvNN 自编码器用于将 3D 形状编码为固定长度的代码，（2）一个 GAN 用于学习根代码的分布并生成合理的结构，（3）另一个自编码器用于合成每个部分的几何形状，这受到
    [[32](#bib.bib32)] 的启发。此外，为了在体素网格中合成细粒度几何，提出了结构感知递归特征（SARF），它包含每个部分的几何特征以及全局和局部
    OBB 布局。
- en: However, the GRASS [[51](#bib.bib51)] uses a binary tree to organize the part
    structure, which leads to ambiguity. Therefore, binary trees are not suitable
    for large scale datasets. To address the problem, Mo et al. [[68](#bib.bib68)]
    proposed StructureNet which organized the hierarchical structure in the form of
    graphs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GRASS [[51](#bib.bib51)] 使用二叉树来组织部分结构，这导致了歧义。因此，二叉树不适合大规模数据集。为了解决这个问题，Mo
    等人 [[68](#bib.bib68)] 提出了 StructureNet，它以图的形式组织层级结构。
- en: The BSP-Net (Binary Space Partitioning-Net) proposed by Chen et al. [[13](#bib.bib13)]
    is the first method to depict sharp geometry features, which constructs a 3D shape
    by convexes organized by a BSP-tree. The Binary Space Partitioning (BSP) tree
    defined in [[13](#bib.bib13)] is used to represent 3D shapes by collections of
    convexes, which includes three layers, namely hyperplane extraction, hyerplane
    grouping and shape assembly. The convexes can also be seen as a new form of primitives
    which can represent geometry details of 3D shapes rather than general structures.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人提出的BSP-Net（Binary Space Partitioning-Net）[[13](#bib.bib13)]是首个描绘锐利几何特征的方法，它通过BSP树组织的凸体构建3D形状。在[[13](#bib.bib13)]中定义的Binary
    Space Partitioning（BSP）树用于通过凸体集合表示3D形状，包含三个层次，即超平面提取、超平面分组和形状组装。这些凸体也可以视为一种新的原始形式，可以表示3D形状的几何细节，而非一般结构。
- en: Structure and Geometry. Researchers try to encode the 3D shape structure and
    geometry features separately [[51](#bib.bib51)] or jointly [[113](#bib.bib113)].
    Wang et al. [[103](#bib.bib103)] proposed Global-to-Local (G2L) generative model
    to generate man-made 3D shapes from coarse to fine. To address the problem that
    GANs cannot generate geometry details well [[110](#bib.bib110)], G2L first applies
    a GAN to generate coarse voxel grids with semantic labels that represent shape
    structure at the global level, and then puts the voxels separated by semantic
    labels into an autoencoder called Part Refiner (PR) to optimize part geometry
    details part by part at the local level. Wu et al. [[113](#bib.bib113)] proposed
    SAGNet for detailed 3D shape generation, which encodes the structure and geometry
    jointly by a GRU [[15](#bib.bib15)] architecture in order to find intra-relation
    between them. The SAGNet shows better performance in tenon-mortise joints than
    other structure-based learning methods.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 结构与几何。研究人员尝试分别[[51](#bib.bib51)]或联合[[113](#bib.bib113)]编码3D形状的结构和几何特征。王等人[[103](#bib.bib103)]提出了Global-to-Local（G2L）生成模型，从粗到细生成人工3D形状。为了解决GAN无法很好生成几何细节的问题[[110](#bib.bib110)]，G2L首先应用GAN生成带有语义标签的粗略体素网格，表示全局级别的形状结构，然后将按语义标签分隔的体素输入到一个称为Part
    Refiner（PR）的自编码器中，以局部级别优化部件几何细节。吴等人[[113](#bib.bib113)]提出了SAGNet用于详细的3D形状生成，通过GRU[[15](#bib.bib15)]架构联合编码结构和几何，以发现它们之间的内在关系。SAGNet在榫卯接头上的表现优于其他基于结构的学习方法。
- en: 7 Deformation-based representations
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 基于形变的表示
- en: Deformable 3D models play an important role in computer animation. However,
    most of the methods mentioned above mainly focus on rigid 3D models, while paying
    less attention to the deformation of non-rigid models. Compared with other representations,
    deformation-based representations parameterize the deformation information and
    have better performance when used to cope with non-rigid 3D shapes, such as articulated
    models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 可变形的3D模型在计算机动画中扮演着重要角色。然而，上述大多数方法主要关注于刚性3D模型，而对非刚性模型的形变关注较少。与其他表示方法相比，基于形变的表示通过参数化形变信息，在处理非刚性3D形状（如关节模型）时表现更好。
- en: Mesh-based Deformation Description. A mesh can be seen as a graph, which is
    convenient when manipulating the vertex positions while maintaining the connectivity
    between vertices. Therefore, a great number of methods choose meshes to represent
    deformable 3D shapes. Based on this property, some mesh-based generation methods
    generate target shapes by deforming a mesh template [[104](#bib.bib104), [109](#bib.bib109),
    [73](#bib.bib73), [27](#bib.bib27)], and these methods can also be regarded as
    deformation-based methods. The graph structure makes it easy to store deformation
    information as vertices features, which can be seen as deformation representations.
    Gao et al. [[24](#bib.bib24)] designed an efficient and rotation-invariant deformation
    representation called Rotation-Invariant Mesh Difference (RIMD), which achieves
    high performance in shape reconstruction, deformation and registration. Based
    on  [[24](#bib.bib24)], Tan et al. [[94](#bib.bib94)] proposed Mesh VAE for deformable
    shape analysis and synthesis, which takes RIMD as the feature inputs of VAE and
    uses fully connected layers for the encoder and decoder. Further, Gao et al. [[25](#bib.bib25)]
    designed an as-consistent-as-possible (ACAP) representation to constrain the rotation
    angle and rotation axes between adjacent vertices in the deformable mesh which
    the graph convolution is easily applied. Tan et al. [[95](#bib.bib95)] proposed
    the SparseAE based on the ACAP representation [[25](#bib.bib25)], which applies
    graph convolutional operators [[19](#bib.bib19)] with the ACAP [[25](#bib.bib25)]
    to analysis the mesh deformations. Gao et al. [[26](#bib.bib26)] proposed VC-GAN
    (VAE CycleGAN) for unpaired mesh deformation transfer, which is the first automatic
    work for mesh deformation transfer. This work takes the ACAP representation as
    input, and encodes the representation into latent space by a VAE, and then transfer
    deformations between source and target in the latent space domain with the cycle
    consistency and visual similarity consistency. Gao et al.  [[27](#bib.bib27)]
    firstly view the geometric details shown in Fig [5](#S7.F5 "Figure 5 ‣ 7 Deformation-based
    representations") as the deformations. Based on the previous techniques [[25](#bib.bib25),
    [26](#bib.bib26), [94](#bib.bib94), [95](#bib.bib95)], the geometric details could
    be encoded and generated. The structure in  [[27](#bib.bib27)] is also analyzed
    in the stable supportable manner [[44](#bib.bib44)]. Yuan et al.[[120](#bib.bib120)]
    apply newly designed pooling operation based on mesh simplification and graph
    convolution to VAE architecture, which also takes ACAP representation as input
    of network. Tan et al. [[96](#bib.bib96)] use ACAP representation for simulating
    thin-shell deformable materials, which apply graph-based CNN to embed high-dimensional
    features into low-dimensional features. In addition of considering a single deformable
    mesh, mesh sequences play a more important role in computer animation. And the
    deformation-based representation ACAP [[25](#bib.bib25)] is suitable for representing
    mesh sequence. Qiao et al.[[79](#bib.bib79)] also takes ACAP representation as
    input to generate mesh animation sequences by a bidirectional LSTM network.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基于网格的形变描述。网格可以被视为一个图，这在操作顶点位置的同时保持顶点之间的连接性时非常方便。因此，大量方法选择网格来表示可变形的3D形状。基于这一属性，一些基于网格的生成方法通过变形网格模板来生成目标形状[[104](#bib.bib104),
    [109](#bib.bib109), [73](#bib.bib73), [27](#bib.bib27)]，这些方法也可以被视为基于形变的方法。图结构使得将形变信息存储为顶点特征变得容易，这可以视为形变表示。高等人[[24](#bib.bib24)]设计了一种高效且旋转不变的形变表示，称为旋转不变网格差异（RIMD），它在形状重建、变形和配准中表现出色。基于[[24](#bib.bib24)]，谭等人[[94](#bib.bib94)]提出了Mesh
    VAE用于可变形形状分析和合成，它将RIMD作为VAE的特征输入，并使用全连接层作为编码器和解码器。此外，高等人[[25](#bib.bib25)]设计了一种尽可能一致的（ACAP）表示，以约束可变形网格中相邻顶点之间的旋转角度和旋转轴，使得图卷积可以轻松应用。谭等人[[95](#bib.bib95)]提出了基于ACAP表示[[25](#bib.bib25)]的SparseAE，它应用了图卷积算子[[19](#bib.bib19)]来分析网格形变。高等人[[26](#bib.bib26)]提出了VC-GAN（VAE
    CycleGAN）用于未配对的网格形变传递，这是首个自动网格形变传递的工作。该工作将ACAP表示作为输入，通过VAE对表示进行编码到潜在空间，然后在潜在空间领域中通过循环一致性和视觉相似性一致性传递源和目标之间的形变。高等人[[27](#bib.bib27)]首次将图中展示的几何细节视为形变[[5](#S7.F5
    "Figure 5 ‣ 7 Deformation-based representations")]. 基于之前的技术[[25](#bib.bib25),
    [26](#bib.bib26), [94](#bib.bib94), [95](#bib.bib95)]，几何细节可以被编码和生成。[[27](#bib.bib27)]中的结构也以稳定可支持的方式进行了分析[[44](#bib.bib44)]。袁等人[[120](#bib.bib120)]将基于网格简化和图卷积的新设计的池化操作应用于VAE架构，这也将ACAP表示作为网络输入。谭等人[[96](#bib.bib96)]使用ACAP表示来模拟薄壳可变形材料，应用基于图的CNN将高维特征嵌入到低维特征中。除了考虑单个可变形网格外，网格序列在计算机动画中扮演着更重要的角色。而基于形变的表示ACAP[[25](#bib.bib25)]适合于表示网格序列。乔等人[[79](#bib.bib79)]也将ACAP表示作为输入，通过双向LSTM网络生成网格动画序列。
- en: '![Refer to caption](img/75aa016fc81f41b577401df96820dd51.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/75aa016fc81f41b577401df96820dd51.png)'
- en: 'Figure 4: The research works on deformation-based shape representation of the
    geometrylearning group in ICT, CAS'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：ICT中国科学院几何学习组关于基于变形的形状表示的研究工作。
- en: '![Refer to caption](img/c4d751d1d30ba7f159441944dce29ab8.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c4d751d1d30ba7f159441944dce29ab8.png)'
- en: 'Figure 5: An example of representing a chair leg by deforming bounding box
    in SDM-NET. (a)a chair with one of its leg parts highlighted, (b)the highlighted
    part in (a) and the overlaid bounding box, (c)the bounding box used as the template,
    (d)deformed bounding box, (e)recovered shape. Ref.[[27](#bib.bib27)] ©ACM 2019'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：通过在SDM-NET中变形边界框表示椅子腿的示例。（a）突出显示的椅子腿部，（b）(a)中突出显示的部分和叠加的边界框，（c）用作模板的边界框，（d）变形后的边界框，（e）恢复的形状。参考文献[[27](#bib.bib27)]
    ©ACM 2019
- en: Implicit surface based approaches. With the development of implicit surface
    representations, Jeruzalski et al. [[45](#bib.bib45)] proposed a method to represent
    articulated deformable shapes by pose parameters, called Neural Articulated Shape
    Approximation (NASA). The pose parameters mentioned in [[45](#bib.bib45)] record
    the transformation of bones defined in models. They compared three different network
    architectures, including unstructured model (U), piecewise rigid model (R) and
    piecewise deformable model (D) in the training dataset and test dataset, which
    opens another direction to represent deformable 3D shapes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基于隐式表面的 approaches。随着隐式表面表示的发展，Jeruzalski等[[45](#bib.bib45)] 提出了通过姿态参数表示关节可变形形状的方法，称为神经关节形状近似（NASA）。在[[45](#bib.bib45)]中提到的姿态参数记录了模型中定义的骨骼的变换。他们比较了三种不同的网络架构，包括非结构化模型（U）、分段刚性模型（R）和分段变形模型（D），在训练数据集和测试数据集中，这为表示可变形3D形状开辟了另一种方向。
- en: 8 Datasets
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 数据集
- en: With the development of 3D scanners, 3D models become easier to obtain, so there
    are more and more 3D shape datasets that have been proposed with different 3D
    representations. The larger datasets with more details bring more challenges for
    existing techniques, which further promotes the development of deep learning on
    different 3D representations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随着3D扫描仪的发展，获取3D模型变得更加容易，因此提出了越来越多具有不同3D表示的3D形状数据集。更大的数据集带来了更多细节，这对现有技术提出了更多挑战，从而进一步推动了深度学习在不同3D表示上的发展。
- en: The datasets can be divided into several types in different representations
    and different applications. Choosing the appropriate dataset benefits the performance
    and generalization for learning based models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以根据不同的表示和应用划分为几种类型。选择合适的数据集有利于基于学习的模型的性能和泛化。
- en: RGB-D Images. RGB-D image datasets can be collected by depth sensors like Microsoft
    Kinect. Most of the RGB-D image datasets can be regarded as a sequence of video.
    The indoor scene RGB-D image dataset NYU Depth [[85](#bib.bib85), [86](#bib.bib86)]
    was first provided for the segmentation problem, and the v1 version [[85](#bib.bib85)]
    collects 64 categories while the v2 version [[86](#bib.bib86)] collects 464 categories.
    The KITTI [[29](#bib.bib29)] dataset provides outdoor scene images mainly for
    autonomous driving, which contains 5 categories including ‘Road’, ‘City’, ‘Residential’,
    ‘Campus’ and ‘Person’. The depth map of images can be calculated by the development
    kit provided by the KITTI dataset. And the KITTI dataset also contains 3D objects
    annotations for applications such as object detection. ScanNet [[17](#bib.bib17)]
    is a large annotated RGB-D video dataset, which includes 2.5M views in 1,513 scenes
    with 3D camera pose, surface reconstructions and semantic segmentations. Another
    dataset Human10 [[11](#bib.bib11)] is sampled from 10 human action sequences.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: RGB-D 图像。RGB-D 图像数据集可以通过深度传感器（如微软Kinect）收集。大多数RGB-D图像数据集可以视为视频序列。室内场景RGB-D图像数据集NYU
    Depth [[85](#bib.bib85), [86](#bib.bib86)] 首次用于分割问题，v1版本[[85](#bib.bib85)] 收集了64个类别，而v2版本[[86](#bib.bib86)]
    收集了464个类别。KITTI [[29](#bib.bib29)] 数据集提供了主要用于自动驾驶的户外场景图像，包括‘道路’、‘城市’、‘住宅’、‘校园’和‘人’五个类别。图像的深度图可以通过KITTI数据集提供的开发套件计算。KITTI数据集还包含用于物体检测等应用的3D物体注释。ScanNet
    [[17](#bib.bib17)] 是一个大型注释RGB-D视频数据集，包含1,513个场景中的2.5M视图，具有3D相机姿态、表面重建和语义分割。另一个数据集Human10
    [[11](#bib.bib11)] 是从10个人体动作序列中采样的。
- en: 'Man-made 3D Object Datasets. The ModelNet [[112](#bib.bib112)] is one of the
    famous CAD model datasets for 3D shape analysis, including 127,915 3D CAD Models
    in 662 categories. ModelNet provides two subsets named ModelNet10 and ModelNet40
    respectively. ModelNet10 includes 10 categories from the whole dataset, and the
    3D models in ModelNet10 are aligned manually; ModelNet40 includes 40 categories,
    and the 3D models are also aligned. ShapeNet [[12](#bib.bib12)] provides a larger
    scale dataset, containing more than 3 million models in more than 4K categories.
    ShapeNet also contains two smaller subsets: ShapeNetCore and ShapeNetSem. For
    various geometry applications, ShapeNet [[12](#bib.bib12)] provides rich annotations
    for 3D objects in the dataset, including category labels, part labels, symmetry
    information, etc. ObjectNet3D [[114](#bib.bib114)] is a large-scale dataset for
    3D object recognition from 2D images, which includes 201,888 3D objects in 90,127
    images and 44,147 different 3D shapes. The dataset is annotated with 3D pose parameters,
    which align 3D objects with 2D images. SUNCG [[92](#bib.bib92)] includes full
    room 3D models, which is suitable for 3D scene analysis and scene completion tasks.
    The 3D models in SUNCG are represented by dense voxel grids with object annotations.
    The whole dataset includes 49,884 valid floors with 404,058 rooms and 5,697,217
    object instances. PartNet provides a more detailed CAD model dataset with fine-grained,
    hierarchical part annotations, which brings more challenges and resources for
    3D object applications such as semantic segmentation, shape editing and shape
    generation. 3D-Future[[23](#bib.bib23)] provides a large-scale furniture dataset,
    which includes 20,000+ scenes in 5,000+ rooms with 10,000+ 3D instances. Each
    3D shape is of high quality with the best texture information for now.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 人工3D对象数据集。ModelNet [[112](#bib.bib112)] 是著名的CAD模型数据集之一，用于3D形状分析，包括662个类别的127,915个3D
    CAD模型。ModelNet提供了两个子集，分别是ModelNet10和ModelNet40。ModelNet10包含来自整个数据集的10个类别，且ModelNet10中的3D模型是手动对齐的；ModelNet40包含40个类别，3D模型也经过对齐。ShapeNet [[12](#bib.bib12)]
    提供了更大规模的数据集，包含超过300万个模型和4000多个类别。ShapeNet还包含两个较小的子集：ShapeNetCore和ShapeNetSem。对于各种几何应用，ShapeNet [[12](#bib.bib12)]
    提供了丰富的3D对象注释，包括类别标签、部件标签、对称信息等。ObjectNet3D [[114](#bib.bib114)] 是一个大规模的3D对象识别数据集，来自2D图像，包含201,888个3D对象，90,127张图像和44,147种不同的3D形状。该数据集附有3D姿态参数，将3D对象与2D图像对齐。SUNCG [[92](#bib.bib92)]
    包含完整的房间3D模型，适用于3D场景分析和场景完成任务。SUNCG中的3D模型由密集的体素网格和对象注释表示。整个数据集包括49,884个有效楼层，404,058个房间和5,697,217个对象实例。PartNet提供了一个更详细的CAD模型数据集，具有细粒度的层次化部件注释，为3D对象应用如语义分割、形状编辑和形状生成带来了更多挑战和资源。3D-Future[[23](#bib.bib23)]
    提供了一个大规模的家具数据集，包含20,000多个场景，5,000多个房间和10,000多个3D实例。每个3D形状都是高质量的，具有最佳的纹理信息。
- en: Non-Rigid Model Datasets. TOSCA[[8](#bib.bib8)] is one of the high-resolution
    3D non-rigid model datasets, which contains 80 objects in 9 categories. The models
    are in the mesh representation, and the objects within the same category have
    the same resolution. FAUST[[5](#bib.bib5)] is a dataset of 3D human body scans
    in 10 different people with a variety of poses and the ground truth correspondences
    are also provided. Because FAUST was proposed for real-world shape registration,
    the scans provided in the dataset are noisy and incomplete, but the corresponding
    ground truth is water-tight and aligned. AMASS [[59](#bib.bib59)] provides a large
    and varied human motion dataset, which gathers previous mocap datasets with a
    consistent framework and parameterization. It contains 344 subjects, 11,265 motions
    and more than 40 hours of recordings.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 非刚性模型数据集。TOSCA[[8](#bib.bib8)] 是高分辨率的3D非刚性模型数据集之一，包含9个类别的80个对象。模型以网格表示，同一类别的对象具有相同的分辨率。FAUST[[5](#bib.bib5)]
    是一个包含10个不同人物3D人体扫描的数据集，具有各种姿势，并提供了真实对应关系。由于FAUST旨在用于真实世界的形状配准，数据集中的扫描数据是有噪声和不完整的，但相应的真实数据是密封且对齐的。AMASS [[59](#bib.bib59)]
    提供了一个大规模和多样的人体运动数据集，汇集了以前的动作捕捉数据集，具有一致的框架和参数化。它包含344个受试者，11,265个动作和超过40小时的录音。
- en: '| Source | Type | Dataset | Year | Category | Size | Description |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 来源 | 类型 | 数据集 | 年份 | 类别 | 大小 | 描述 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Real-world | RGB-D Images | NYU Depth v1[[85](#bib.bib85)] | 2011 | 64 |
    - | Indoor Scene |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 现实世界 | RGB-D 图像 | NYU Depth v1[[85](#bib.bib85)] | 2011 | 64 | - | 室内场景 |'
- en: '| Real-world | RGB-D Images | NYU Depth v2[[86](#bib.bib86)] | 2012 | 464 |
    407024 | Indoor Scene |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 现实世界 | RGB-D 图像 | NYU Depth v2[[86](#bib.bib86)] | 2012 | 464 | 407024 |
    室内场景 |'
- en: '| Real-world | RGB-D Images | KITTI[[29](#bib.bib29)] | 2013 | 5 | - | Outdoor
    Scene |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 现实世界 | RGB-D 图像 | KITTI[[29](#bib.bib29)] | 2013 | 5 | - | 户外场景 |'
- en: '| Real-world | RGB-D Images | ScanNet[[17](#bib.bib17)] | 2017 | 1513 | 2.5M
    | Indoor Scene video |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 现实世界 | RGB-D 图像 | ScanNet[[17](#bib.bib17)] | 2017 | 1513 | 250万 | 室内场景视频
    |'
- en: '| Real-world | RGB-D Images | Human10[[11](#bib.bib11)] | 2018 | 10 | 9746
    | Human Action |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 现实世界 | RGB-D 图像 | Human10[[11](#bib.bib11)] | 2018 | 10 | 9746 | 人体动作 |'
- en: '| Synthetic | 3D CAD Models | ModelNet[[112](#bib.bib112)] | 2015 | 662 | 127915
    | Mesh Representation |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 3D CAD模型 | ModelNet[[112](#bib.bib112)] | 2015 | 662 | 127915 | 网格表示
    |'
- en: '| Synthetic | 3D CAD Models | ModelNet10[[112](#bib.bib112)] | 2015 | 10 |
    4899 | - |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 3D CAD模型 | ModelNet10[[112](#bib.bib112)] | 2015 | 10 | 4899 | - |'
- en: '| Synthetic | 3D CAD Models | ModelNet40[[112](#bib.bib112)] | 2015 | 40 |
    12311 | - |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 3D CAD模型 | ModelNet40[[112](#bib.bib112)] | 2015 | 40 | 12311 | - |'
- en: '| Synthetic | 3D CAD Models | ShpaeNet[[12](#bib.bib12)] | 2015 | 4K | 3millions
    | Rich Annotations |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 3D CAD模型 | ShpaeNet[[12](#bib.bib12)] | 2015 | 4K | 300万 | 丰富注释 |'
- en: '| Synthetic | 3D CAD Models | ShapeNetCore[[12](#bib.bib12)] | 2015 | 55 |
    51300 | - |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 3D CAD模型 | ShapeNetCore[[12](#bib.bib12)] | 2015 | 55 | 51300 | - |'
- en: '| Synthetic | 3D CAD Models | ShapeNetSem[[12](#bib.bib12)] | 2015 | 270 |
    12000 | - |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 3D CAD模型 | ShapeNetSem[[12](#bib.bib12)] | 2015 | 270 | 12000 | - |'
- en: '| Synthetic | Images and 3D Models | ObjectNet3D[[114](#bib.bib114)] | 2016
    | 100 | 44161 | 2D aligned with 3D |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 图像和3D模型 | ObjectNet3D[[114](#bib.bib114)] | 2016 | 100 | 44161 | 2D与3D对齐
    |'
- en: '| Synthetic | 3D CAD Models | SUNCG[[92](#bib.bib92)] | 2017 | - | 49884 |
    Full Room Scene |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 3D CAD模型 | SUNCG[[92](#bib.bib92)] | 2017 | - | 49884 | 完整房间场景 |'
- en: '| Synthetic | 3D CAD Models | PartNet[[69](#bib.bib69)] | 2019 | 24 | 26671
    | 573585 Part Instance |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 3D CAD模型 | PartNet[[69](#bib.bib69)] | 2019 | 24 | 26671 | 573585 部件实例
    |'
- en: '| Synthetic | 3D CAD Models | 3D-FUTURE[[23](#bib.bib23)] | 2020 | - | 10K
    | Texture Information |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 3D CAD模型 | 3D-FUTURE[[23](#bib.bib23)] | 2020 | - | 10K | 纹理信息 |'
- en: '| Synthetic | Non-Rigid Models | TOSCA[[8](#bib.bib8)] | 2008 | 9 | 80 | -
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 非刚性模型 | TOSCA[[8](#bib.bib8)] | 2008 | 9 | 80 | - |'
- en: '| Real-world | Non-Rigid Models | FAUST[[5](#bib.bib5)] | 2014 | 10 | 300 |
    Human Bodies |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 现实世界 | 非刚性模型 | FAUST[[5](#bib.bib5)] | 2014 | 10 | 300 | 人体 |'
- en: '| Synthetic | Non-Rigid Models | AMASS[[59](#bib.bib59)] | 2019 | 344 | 11265
    | Human Motions |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 非刚性模型 | AMASS[[59](#bib.bib59)] | 2019 | 344 | 11265 | 人体运动 |'
- en: 'Table 1: The Overview of 3D Model Datasets'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：3D模型数据集概览
- en: 9 Shape Analysis and Reconstruction
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 形状分析与重建
- en: The shape representations mentioned above are fundamental for shape analysis
    and shape reconstruction. In this section, we summarize representative works in
    these two directions respectively and compare the performance of these works.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 上述形状表示对形状分析和形状重建至关重要。在本节中，我们分别总结了这两个方向的代表性工作，并比较了这些工作的性能。
- en: 9.1 Shape Analysis
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 形状分析
- en: Shape analysis methods usually extract the latent codes from different 3D shape
    representations by different network architectures. The latent codes are then
    used for specific applications like shape classification, shape retrieval, shape
    segmentation, etc. And different representations are usually suitable for different
    applications. We now review the performance of different representations in different
    models and discuss suitable representations for specific applications.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 形状分析方法通常通过不同的网络架构从不同的3D形状表示中提取潜在编码。然后，这些潜在编码用于特定应用，如形状分类、形状检索、形状分割等。不同的表示通常适用于不同的应用。我们现在回顾不同模型中不同表示的性能，并讨论适合特定应用的表示。
- en: Shape Classification and Retrieval are the basic problems of shape analysis.
    Both of them rely on the feature vectors extracted from the analysis networks.
    For shape classification, the datasets ModelNet10 and ModelNet40 [[112](#bib.bib112)]
    are widely used and Table [2](#S9.T2 "Table 2 ‣ 9.1 Shape Analysis ‣ 9 Shape Analysis
    and Reconstruction") shows the accuracy of some different methods on ModelNet10
    and ModelNet40\. For shape retrieval, given a 3D shape as a query, the target
    is to find the most similar shape(s) in the dataset to match the query. Retrieval
    methods usually learn to find a compact code to represent the object in a latent
    space, and query the closest object as the result based on Euclidean distance,
    Mahalanobis distance or other distance metrics. Different from the classification
    task, the shape retrieval task has a number of evaluation measures, including
    precision, recall, mAP (mean average precision), etc.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 形状分类和检索是形状分析的基本问题。它们都依赖于从分析网络中提取的特征向量。对于形状分类，数据集ModelNet10和ModelNet40 [[112](#bib.bib112)]
    被广泛使用，表格 [2](#S9.T2 "Table 2 ‣ 9.1 Shape Analysis ‣ 9 Shape Analysis and Reconstruction")
    显示了在ModelNet10和ModelNet40上不同方法的准确率。对于形状检索，给定一个3D形状作为查询目标，是在数据集中找到与查询最相似的形状。检索方法通常学习找到一个紧凑的代码来表示对象在潜在空间中的位置，并基于欧几里得距离、马氏距离或其他距离度量查询最近的对象作为结果。与分类任务不同，形状检索任务有许多评估指标，包括精度、召回率、mAP（平均精度均值）等。
- en: '| Form | Model | Accuracy(%) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 形式 | 模型 | 准确率(%) |'
- en: '| --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 10 | 40 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 40 |'
- en: '| --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Voxel | 3DShapeNet [[112](#bib.bib112)] | 83.54 | 77.32 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 体素 | 3DShapeNet [[112](#bib.bib112)] | 83.54 | 77.32 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Voxel | VoxNet [[63](#bib.bib63)] | 92 | 83 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 体素 | VoxNet [[63](#bib.bib63)] | 92 | 83 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Voxel | 3D-GAN [[110](#bib.bib110)] | 91.0 | 83.3 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 体素 | 3D-GAN [[110](#bib.bib110)] | 91.0 | 83.3 |'
- en: '| Voxel | Qi et al. [[76](#bib.bib76)] | - | 86 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 体素 | Qi 等人 [[76](#bib.bib76)] | - | 86 |'
- en: '| Voxel | ORION [[82](#bib.bib82)] | 93.8 | - |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 体素 | ORION [[82](#bib.bib82)] | 93.8 | - |'
- en: '| Point | PointNet [[75](#bib.bib75)] | - | 89.2 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 点云 | PointNet [[75](#bib.bib75)] | - | 89.2 |'
- en: '| Multi-view | MVCNN [[93](#bib.bib93)] | - | 90.1 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 多视角 | MVCNN [[93](#bib.bib93)] | - | 90.1 |'
- en: '| Point | Kd-net[[47](#bib.bib47)] | 93.3 | 90.6 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 点云 | Kd-net[[47](#bib.bib47)] | 93.3 | 90.6 |'
- en: '| Multi-view | Qi et al. [[76](#bib.bib76)] | - | 91.4 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 多视角 | Qi 等人 [[76](#bib.bib76)] | - | 91.4 |'
- en: '| Point | PointNet++ [[77](#bib.bib77)] | - | 91.9 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 点云 | PointNet++ [[77](#bib.bib77)] | - | 91.9 |'
- en: '| Point | Point2Sequence [[57](#bib.bib57)] | 95.3 | 92.6 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 点云 | Point2Sequence [[57](#bib.bib57)] | 95.3 | 92.6 |'
- en: 'Table 2: Accuracy of shape classification on ModelNet10 and ModelNet40 datasets.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：ModelNet10和ModelNet40数据集上形状分类的准确率。
- en: Shape Segmentation aims to discriminate the part categories of a 3D shape. This
    task plays an important role in understanding 3D shapes. The mean Intersection-over-Union
    (mIOU) is often used as the evaluation metric of shape segmentation. Most researchers
    choose to use the point-based representation for the segmentation task [[47](#bib.bib47),
    [75](#bib.bib75), [77](#bib.bib77), [53](#bib.bib53), [66](#bib.bib66)].
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 形状分割旨在区分3D形状的部件类别。这个任务在理解3D形状中发挥了重要作用。均值交并比（mIOU）通常用作形状分割的评估指标。大多数研究者选择使用基于点的表示来进行分割任务 [[47](#bib.bib47),
    [75](#bib.bib75), [77](#bib.bib77), [53](#bib.bib53), [66](#bib.bib66)]。
- en: Shape Symmetry Detection. Symmetry is important geometry information in 3D shapes,
    and it can be further used in many other applications such as shape alignment,
    registration, completion, etc. Gao et al. [[28](#bib.bib28)] designed the first
    unsupervised deep learning method named PRS-Net (Planar Reflective Symmetry Net)
    to detect planar reflective symmetry of 3D shapes, which designs a new symmetry
    distance loss and a regularization loss. And PRS-Net was proved to be robust in
    noisy and incomplete input and more efficient than traditional methods. As symmetry
    is largely determined by the overall shape, PRS-Net is based on a 3D voxel CNN
    and gains high performance in a low resolution.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 形状对称性检测。对称性是3D形状中的重要几何信息，可以在许多其他应用中进一步使用，如形状对齐、配准、补全等。Gao 等人 [[28](#bib.bib28)]
    设计了首个无监督深度学习方法PRS-Net（平面反射对称网络）来检测3D形状的平面反射对称性，该方法设计了一种新的对称性距离损失和正则化损失。PRS-Net被证明在噪声和不完整输入下表现稳健，比传统方法更高效。由于对称性主要由整体形状决定，PRS-Net基于3D体素卷积神经网络，并在低分辨率下表现出色。
- en: '![Refer to caption](img/3f2534f99503f704bc3853e50d1b2bb9.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3f2534f99503f704bc3853e50d1b2bb9.png)'
- en: 'Figure 6: The pipeline of PRS-Net Ref. [[28](#bib.bib28)] ©IEEE 2020'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：PRS-Net的流程图 参考文献 [[28](#bib.bib28)] ©IEEE 2020
- en: 9.2 Shape Reconstruction
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 形状重建
- en: Learning based generative models have been proposed for different representations,
    which is also an important field in geometry learning. The reconstruction applications
    include single-view shape reconstruction, shape generation, shape editing, etc.
    The generation methods can be summarized on the basis of representations. For
    voxel-based representations, learning based models try to predict the occupancy
    probability of each voxel in the grid. For point-based representations, learning
    based models either sample 3D points in the space or fold the 2D grids into target
    3D objects. For mesh-based representations, most of the generation methods choose
    to deform a mesh template into the final mesh. In recent study, more and more
    methods choose to use structured representation and generate coarse-to-fine 3D
    shapes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的生成模型已经被提出用于不同的表示，这也是几何学习中的一个重要领域。重建应用包括单视角形状重建、形状生成、形状编辑等。生成方法可以基于表示进行总结。对于基于体素的表示，基于学习的模型尝试预测网格中每个体素的占用概率。对于基于点的表示，基于学习的模型要么在空间中采样3D点，要么将2D网格折叠成目标3D对象。对于基于网格的表示，大多数生成方法选择将网格模板变形为最终网格。在最近的研究中，越来越多的方法选择使用结构化表示，并生成由粗到精的3D形状。
- en: 10 Summary
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 总结
- en: In this survey, we review a series of deep learning methods based on different
    3D object representations. We first overview different 3D representation learning
    models. And the tendency of the geometry learning can be summarized to be less
    computation and memory demanding, and more detailed and structured. Then, we introduce
    3D datasets which are widely used in the research. These datasets provide rich
    resources and support evaluation for data-driven learning methods. Finally, we
    discuss 3D shape applications based on different 3D representations, including
    shape analysis and shape reconstruction. Different representations are usually
    suitable for different applications. Therefore, it is vitally important to choose
    suitable 3D representations for specific tasks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本综述中，我们回顾了一系列基于不同3D对象表示的深度学习方法。我们首先概述了不同的3D表示学习模型。几何学习的趋势可以总结为计算和内存需求较少，更加详细和结构化。然后，我们介绍了广泛用于研究的3D数据集。这些数据集提供了丰富的资源，并支持数据驱动学习方法的评估。最后，我们讨论了基于不同3D表示的3D形状应用，包括形状分析和形状重建。不同的表示通常适用于不同的应用。因此，为特定任务选择合适的3D表示至关重要。
- en: \CvmAck
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: \CvmAck
- en: This work was supported by National Natural Science Foundation of China (No.
    61828204 and No. 61872440), Beijing Municipal Natural Science Foundation (No.
    L182016), Youth Innovation Promotion Association CAS, CCF-Tencent Open Fund, Royal
    Society-Newton Advanced Fellowship (No. NAF\R2\192151) and the Royal Society (no.
    IES\R1\180126).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了中国国家自然科学基金（编号：61828204 和 61872440）、北京市自然科学基金（编号：L182016）、中国科学院青年创新促进会、CCF-腾讯开放基金、皇家学会-牛顿高级研究员奖（编号：NAF\R2\192151）及皇家学会（编号：IES\R1\180126）的资助。
- en: References
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] E. Ahmed, A. Saint, A. E. R. Shabayek, K. Cherenkova, R. Das, G. Gusev,
    D. Aouada, and B. Ottersten. Deep learning advances on different 3D data representations:
    A survey. arXiv preprint arXiv:1808.01462, 1, 2018.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] E. Ahmed, A. Saint, A. E. R. Shabayek, K. Cherenkova, R. Das, G. Gusev,
    D. Aouada 和 B. Ottersten. 不同3D数据表示的深度学习进展：综述. arXiv预印本 arXiv:1808.01462, 1, 2018年.'
- en: '[2] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances
    in Neural Information Processing Systems, pages 1993–2001, 2016.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. Atwood 和 D. Towsley. 扩散卷积神经网络. 载于《神经信息处理系统进展》, 第1993–2001页, 2016年.'
- en: '[3] H. Ben-Hamu, H. Maron, I. Kezurer, G. Avineri, and Y. Lipman. Multi-chart
    generative surface modeling. In SIGGRAPH Asia 2018 Technical Papers, page 215\.
    ACM, 2018.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] H. Ben-Hamu, H. Maron, I. Kezurer, G. Avineri 和 Y. Lipman. 多图表生成表面建模. 载于《SIGGRAPH
    Asia 2018技术论文》，第215页，ACM, 2018年.'
- en: '[4] P. J. Besl and N. D. McKay. Method for registration of 3-d shapes. In Sensor
    fusion IV: control paradigms and data structures, volume 1611, pages 586–606\.
    International Society for Optics and Photonics, 1992.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] P. J. Besl 和 N. D. McKay. 3D形状配准的方法. 载于《传感器融合IV：控制范式与数据结构》，卷1611，第586–606页，国际光学与光子学学会，1992年.'
- en: '[5] F. Bogo, J. Romero, M. Loper, and M. J. Black. FAUST: Dataset and evaluation
    for 3D mesh registration. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 3794–3801, 2014.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] F. Bogo, J. Romero, M. Loper 和 M. J. Black. FAUST：3D网格配准的数据集和评估. 载于《IEEE计算机视觉与模式识别会议论文集》，第3794–3801页,
    2014年.'
- en: '[6] D. Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castellani, and P. Vandergheynst.
    Learning class-specific descriptors for deformable shapes using localized spectral
    convolutional networks. In Computer Graphics Forum, volume 34, pages 13–23\. Wiley
    Online Library, 2015.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castellani, 和 P. Vandergheynst.
    使用局部谱卷积网络学习类别特定描述符。发表于计算机图形学论坛，第 34 卷，页面 13–23。Wiley 在线图书馆, 2015.'
- en: '[7] D. Boscaini, J. Masci, E. Rodolà, and M. Bronstein. Learning shape correspondence
    with anisotropic convolutional neural networks. In Advances in Neural Information
    Processing Systems, pages 3189–3197, 2016.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. Boscaini, J. Masci, E. Rodolà, 和 M. Bronstein. 使用各向异性卷积神经网络学习形状对应关系。发表于神经信息处理系统进展，页面
    3189–3197, 2016.'
- en: '[8] A. M. Bronstein, M. M. Bronstein, and R. Kimmel. Numerical geometry of
    non-rigid shapes. Springer Science & Business Media, 2008.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. M. Bronstein, M. M. Bronstein, 和 R. Kimmel. 非刚性形状的数值几何。Springer 科学与商业媒体,
    2008.'
- en: '[9] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric
    deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4):18–42,
    2017.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, 和 P. Vandergheynst. 几何深度学习：超越欧几里得数据。IEEE
    信号处理杂志, 34(4):18–42, 2017.'
- en: '[10] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally
    connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. Bruna, W. Zaremba, A. Szlam, 和 Y. LeCun. 谱网络和图上的局部连接网络。arXiv 预印本 arXiv:1312.6203,
    2013.'
- en: '[11] Y.-P. Cao, Z.-N. Liu, Z.-F. Kuang, L. Kobbelt, and S.-M. Hu. Learning
    to reconstruct high-quality 3D shapes with cascaded fully convolutional networks.
    In The European Conference on Computer Vision (ECCV), September 2018.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Y.-P. Cao, Z.-N. Liu, Z.-F. Kuang, L. Kobbelt, 和 S.-M. Hu. 学习使用级联全卷积网络重建高质量
    3D 形状。发表于欧洲计算机视觉会议（ECCV），2018 年 9 月。'
- en: '[12] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su, et al. ShapeNet: An information-rich 3D model repository.
    arXiv preprint arXiv:1512.03012, 2015.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S.
    Savarese, M. Savva, S. Song, H. Su 等. ShapeNet：一个信息丰富的 3D 模型库。arXiv 预印本 arXiv:1512.03012,
    2015.'
- en: '[13] Z. Chen, A. Tagliasacchi, and H. Zhang. BSP-Net: Generating compact meshes
    via binary space partitioning. arXiv preprint arXiv:1911.06971, 2019.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Z. Chen, A. Tagliasacchi, 和 H. Zhang. BSP-Net：通过二进制空间分割生成紧凑网格。arXiv 预印本
    arXiv:1911.06971, 2019.'
- en: '[14] Z. Chen and H. Zhang. Learning implicit fields for generative shape modeling.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5939–5948, 2019.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Z. Chen 和 H. Zhang. 学习隐式场进行生成形状建模。发表于 IEEE 计算机视觉与模式识别会议论文集，页面 5939–5948,
    2019.'
- en: '[15] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical
    machine translation. arXiv preprint arXiv:1406.1078, 2014.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H.
    Schwenk, 和 Y. Bengio. 使用 RNN 编码器-解码器学习短语表示以进行统计机器翻译。arXiv 预印本 arXiv:1406.1078,
    2014.'
- en: '[16] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3D-R2N2: A unified
    approach for single and multi-view 3D object reconstruction. In European Conference
    on Computer Vision (ECCV), pages 628–644\. Springer, 2016.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] C. B. Choy, D. Xu, J. Gwak, K. Chen, 和 S. Savarese. 3D-R2N2：单视图和多视图 3D
    物体重建的统一方法。发表于欧洲计算机视觉会议（ECCV），页面 628–644。Springer, 2016.'
- en: '[17] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner.
    ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5828–5839,
    2017.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, 和 M. Nießner.
    ScanNet：丰富标注的室内场景 3D 重建。发表于 IEEE 计算机视觉与模式识别会议论文集，页面 5828–5839, 2017.'
- en: '[18] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural
    networks on graphs with fast localized spectral filtering. In Advances in neural
    information processing systems, pages 3844–3852, 2016.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. Defferrard, X. Bresson, 和 P. Vandergheynst. 在图上使用快速局部谱滤波的卷积神经网络。发表于神经信息处理系统进展，页面
    3844–3852, 2016.'
- en: '[19] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
    A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning
    molecular fingerprints. In Advances in neural information processing systems,
    pages 2224–2232, 2015.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,
    A. Aspuru-Guzik, 和 R. P. Adams. 用于学习分子指纹的图卷积网络。发表于神经信息处理系统进展，页面 2224–2232, 2015.'
- en: '[20] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single
    image using a multi-scale deep network. In Advances in neural information processing
    systems, pages 2366–2374, 2014.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] D. Eigen, C. Puhrsch, 和 R. Fergus. 使用多尺度深度网络从单幅图像预测深度图。在神经信息处理系统进展，第2366-2374页，2014年。'
- en: '[21] H. Fan, H. Su, and L. J. Guibas. A point set generation network for 3D
    object reconstruction from a single image. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 605–613, 2017.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] H. Fan, H. Su, 和 L. J. Guibas. 用于从单幅图像重建3D对象的点集生成网络。收录于IEEE计算机视觉与模式识别会议论文，第605-613页，2017年。'
- en: '[22] M. Fey, J. Eric Lenssen, F. Weichert, and H. Müller. SplineCNN: Fast geometric
    deep learning with continuous b-spline kernels. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 869–877, 2018.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. Fey, J. Eric Lenssen, F. Weichert, 和 H. Müller. SplineCNN：带有连续B样条核的快速几何深度学习。收录于IEEE计算机视觉与模式识别会议论文，第869-877页，2018年。'
- en: '[23] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. Maybank, and D. Tao. 3d-future:
    3d furniture shape with texture.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. Maybank, 和 D. Tao. 3d-future：带纹理的3D家具形状。'
- en: '[24] L. Gao, Y.-K. Lai, D. Liang, S.-Y. Chen, and S. Xia. Efficient and flexible
    deformation representation for data-driven surface modeling. ACM Transactions
    on Graphics (TOG), 35(5):158, 2016.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] L. Gao, Y.-K. Lai, D. Liang, S.-Y. Chen, 和 S. Xia. 高效且灵活的数据驱动表面建模变形表示。ACM计算机图形学学报（TOG），35(5):158，2016年。'
- en: '[25] L. Gao, Y.-K. Lai, J. Yang, Z. Ling-Xiao, S. Xia, and L. Kobbelt. Sparse
    data driven mesh deformation. IEEE transactions on visualization and computer
    graphics, 2019.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] L. Gao, Y.-K. Lai, J. Yang, Z. Ling-Xiao, S. Xia, 和 L. Kobbelt. 稀疏数据驱动的网格变形。IEEE视觉与计算机图形学学报，2019年。'
- en: '[26] L. Gao, J. Yang, Y.-L. Qiao, Y.-K. Lai, P. L. Rosin, W. Xu, and S. Xia.
    Automatic unpaired shape deformation transfer. In SIGGRAPH Asia 2018 Technical
    Papers, page 237\. ACM, 2018.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] L. Gao, J. Yang, Y.-L. Qiao, Y.-K. Lai, P. L. Rosin, W. Xu, 和 S. Xia.
    自动无配对形状变形转移。收录于SIGGRAPH Asia 2018技术论文，第237页。ACM，2018年。'
- en: '[27] L. Gao, J. Yang, T. Wu, Y.-J. Yuan, H. Fu, Y.-K. Lai, and H. Zhang. SDM-NET:
    Deep generative network for structured deformable mesh. ACM Transactions on Graphics
    (TOG), 38(6):243, 2019.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] L. Gao, J. Yang, T. Wu, Y.-J. Yuan, H. Fu, Y.-K. Lai, 和 H. Zhang. SDM-NET：用于结构化可变形网格的深度生成网络。ACM计算机图形学学报（TOG），38(6):243，2019年。'
- en: '[28] L. Gao, L.-X. Zhang, H.-Y. Meng, Y.-H. Ren, Y.-K. Lai, and L. Kobbelt.
    PRS-Net: Planar reflective symmetry detection net for 3D models. IEEE Transactions
    on Visualization and Computer Graphics, 2020.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] L. Gao, L.-X. Zhang, H.-Y. Meng, Y.-H. Ren, Y.-K. Lai, 和 L. Kobbelt. PRS-Net：用于3D模型的平面反射对称检测网络。IEEE视觉与计算机图形学学报，2020年。'
- en: '[29] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics:
    The KITTI dataset. The International Journal of Robotics Research, 32(11):1231–1237,
    2013.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] A. Geiger, P. Lenz, C. Stiller, 和 R. Urtasun. 视觉与机器人技术的结合：KITTI数据集。《国际机器人研究期刊》，32(11):1231-1237，2013年。'
- en: '[30] K. Genova, F. Cole, A. Sud, A. Sarna, and T. Funkhouser. Deep structured
    implicit functions. arXiv preprint arXiv:1912.06126, 2019.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] K. Genova, F. Cole, A. Sud, A. Sarna, 和 T. Funkhouser. 深度结构化隐式函数。arXiv预印本arXiv:1912.06126，2019年。'
- en: '[31] K. Genova, F. Cole, D. Vlasic, A. Sarna, W. T. Freeman, and T. Funkhouser.
    Learning shape templates with structured implicit functions. arXiv preprint arXiv:1904.06447,
    2019.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] K. Genova, F. Cole, D. Vlasic, A. Sarna, W. T. Freeman, 和 T. Funkhouser.
    使用结构化隐式函数学习形状模板。arXiv预印本arXiv:1904.06447，2019年。'
- en: '[32] R. Girdhar, D. Fouhey, M. Rodriguez, and A. Gupta. Learning a predictable
    and generative vector representation for objects. In European Conference on Computer
    Vision (ECCV), 2016.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] R. Girdhar, D. Fouhey, M. Rodriguez, 和 A. Gupta. 学习可预测和生成的对象向量表示。收录于欧洲计算机视觉大会（ECCV），2016年。'
- en: '[33] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural
    information processing systems, pages 2672–2680, 2014.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio. 生成对抗网络。收录于神经信息处理系统进展，第2672-2680页，2014年。'
- en: '[34] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry. AtlasNet:
    A papier-mâché approach to learning 3D surface generation. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, 2018.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, 和 M. Aubry. AtlasNet：一种学习3D表面生成的纸板方法。收录于IEEE计算机视觉与模式识别会议论文，2018年。'
- en: '[35] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun. Deep learning
    for 3D point clouds: A survey. arXiv: 1912.12033, 2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, 和 M. Bennamoun. 针对3D点云的深度学习：综述。arXiv:
    1912.12033，2019年。'
- en: '[36] S. Gupta, P. Arbeláez, R. Girshick, and J. Malik. Aligning 3D models to
    RGB-D images of cluttered scenes. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 4731–4740, 2015.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Gupta, P. Arbeláez, R. Girshick, 和 J. Malik. 将3D模型对齐到RGB-D图像中的混乱场景。在IEEE计算机视觉与模式识别会议论文集中，页面
    4731–4740, 2015。'
- en: '[37] S. Gupta, R. Girshick, P. Arbeláez, and J. Malik. Learning rich features
    from RGB-D images for object detection and segmentation. In European conference
    on computer vision, pages 345–360. Springer, 2014.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. Gupta, R. Girshick, P. Arbeláez, 和 J. Malik. 从RGB-D图像中学习丰富的特征以进行物体检测和分割。在欧洲计算机视觉会议上，页面
    345–360。Springer, 2014。'
- en: '[38] C. Häne, S. Tulsiani, and J. Malik. Hierarchical surface prediction for
    3D object reconstruction. In 2017 International Conference on 3D Vision (3DV),
    pages 412–420\. IEEE, 2017.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] C. Häné, S. Tulsiani, 和 J. Malik. 用于3D对象重建的层次表面预测。在2017年国际3D视觉会议（3DV）上，页面
    412–420。IEEE, 2017。'
- en: '[39] R. Hanocka, A. Hertz, N. Fish, R. Giryes, S. Fleishman, and D. Cohen-Or.
    MeshCNN: a network with an edge. ACM Transactions on Graphics (TOG), 38(4):90,
    2019.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] R. Hanocka, A. Hertz, N. Fish, R. Giryes, S. Fleishman, 和 D. Cohen-Or.
    MeshCNN: 一个边缘网络。ACM Transactions on Graphics (TOG), 38(4):90, 2019。'
- en: '[40] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured
    data. arXiv preprint arXiv:1506.05163, 2015.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. Henaff, J. Bruna, 和 Y. LeCun. 基于图结构数据的深度卷积网络。arXiv预印本 arXiv:1506.05163,
    2015。'
- en: '[41] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for
    deep belief nets. Neural computation, 18(7):1527–1554, 2006.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] G. E. Hinton, S. Osindero, 和 Y.-W. Teh. 深度置信网络的快速学习算法。Neural Computation,
    18(7):1527–1554, 2006。'
- en: '[42] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation,
    9(8):1735–1780, 1997.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] S. Hochreiter 和 J. Schmidhuber. 长短期记忆。Neural Computation, 9(8):1735–1780,
    1997。'
- en: '[43] J. Huang, H. Zhang, L. Yi, T. Funkhouser, M. Nießner, and L. J. Guibas.
    Texturenet: Consistent local parametrizations for learning from high-resolution
    signals on meshes. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 4440–4449, 2019.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Huang, H. Zhang, L. Yi, T. Funkhouser, M. Nießner, 和 L. J. Guibas.
    Texturenet: 一致的局部参数化，用于从网格上的高分辨率信号中学习。在IEEE计算机视觉与模式识别会议论文集中，页面 4440–4449, 2019。'
- en: '[44] S.-S. Huang, H. Fu, L.-Y. Wei, and S.-M. Hu. Support substructures: support-induced
    part-level structural representation. IEEE transactions on visualization and computer
    graphics, 22(8):2024–2036, 2015.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S.-S. Huang, H. Fu, L.-Y. Wei, 和 S.-M. Hu. 支持子结构：支持诱导的部分级结构表示。IEEE Transactions
    on Visualization and Computer Graphics, 22(8):2024–2036, 2015。'
- en: '[45] T. Jeruzalski, B. Deng, M. Norouzi, J. Lewis, G. Hinton, and A. Tagliasacchi.
    NASA: Neural articulated shape approximation. arXiv preprint arXiv:1912.03207,
    2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] T. Jeruzalski, B. Deng, M. Norouzi, J. Lewis, G. Hinton, 和 A. Tagliasacchi.
    NASA: 神经关节形状近似。arXiv预印本 arXiv:1912.03207, 2019。'
- en: '[46] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional
    networks. arXiv preprint arXiv:1609.02907, 2016.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] T. N. Kipf 和 M. Welling. 使用图卷积网络的半监督分类。arXiv预印本 arXiv:1609.02907, 2016。'
- en: '[47] R. Klokov and V. Lempitsky. Escape from cells: Deep kd-networks for the
    recognition of 3D point cloud models. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 863–872, 2017.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] R. Klokov 和 V. Lempitsky. 逃离单元：用于3D点云模型识别的深度kd网络。在IEEE国际计算机视觉会议论文集中，页面
    863–872, 2017。'
- en: '[48] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems, pages 1097–1105, 2012.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton. 使用深度卷积神经网络进行ImageNet分类。在神经信息处理系统进展中，页面
    1097–1105, 2012。'
- en: '[49] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding
    beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300,
    2015.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, 和 O. Winther. 使用学习的相似性度量进行超越像素的自编码。arXiv预印本
    arXiv:1512.09300, 2015。'
- en: '[50] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications
    in vision. In Proceedings of 2010 IEEE International Symposium on Circuits and
    Systems, pages 253–256\. IEEE, 2010.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Y. LeCun, K. Kavukcuoglu, 和 C. Farabet. 卷积网络及其在视觉中的应用。在2010年IEEE国际电路与系统研讨会上，页面
    253–256。IEEE, 2010。'
- en: '[51] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, and L. Guibas. GRASS:
    Generative recursive autoencoders for shape structures. ACM Transactions on Graphics
    (TOG), 36(4):52, 2017.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, 和 L. Guibas. GRASS: 形状结构的生成递归自编码器。ACM
    Transactions on Graphics (TOG), 36(4):52, 2017。'
- en: '[52] R. Li, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng. PU-GAN: a point cloud
    upsampling adversarial network. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 7203–7212, 2019.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] R. Li, X. Li, C.-W. Fu, D. Cohen-Or, 和 P.-A. Heng. PU-GAN: 一种点云上采样对抗网络。发表于IEEE国际计算机视觉会议论文集，第7203–7212页，2019年。'
- en: '[53] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen. PointCNN: Convolution
    on x-transformed points. In Advances in neural information processing systems,
    pages 820–830, 2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, 和 B. Chen. PointCNN: 在x变换点上的卷积。发表于神经信息处理系统进展，第820–830页，2018年。'
- en: '[54] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas. FPNN: Field probing
    neural networks for 3D data. In Advances in Neural Information Processing Systems,
    pages 307–315, 2016.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Y. Li, S. Pirk, H. Su, C. R. Qi, 和 L. J. Guibas. FPNN: 用于3D数据的场探测神经网络。发表于神经信息处理系统进展，第307–315页，2016年。'
- en: '[55] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400,
    2013.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] M. Lin, Q. Chen, 和 S. Yan. 网络中的网络。arXiv预印本 arXiv:1312.4400，2013年。'
- en: '[56] S. Liu, S. Saito, W. Chen, and H. Li. Learning to infer implicit surfaces
    without 3D supervision. In Advances in Neural Information Processing Systems,
    pages 8293–8304, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] S. Liu, S. Saito, W. Chen, 和 H. Li. 在没有3D监督的情况下学习推断隐式表面。发表于神经信息处理系统进展，第8293–8304页，2019年。'
- en: '[57] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker. Point2sequence: Learning the
    shape representation of 3D point clouds with an attention-based sequence to sequence
    network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,
    pages 8778–8785, 2019.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] X. Liu, Z. Han, Y.-S. Liu, 和 M. Zwicker. Point2sequence: 使用基于注意力的序列到序列网络学习3D点云的形状表示。发表于AAAI人工智能会议论文集，第33卷，第8778–8785页，2019年。'
- en: '[58] W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3D surface
    construction algorithm. In ACM siggraph computer graphics, volume 21, pages 163–169.
    ACM, 1987.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] W. E. Lorensen 和 H. E. Cline. Marching cubes: 一种高分辨率3D表面构建算法。发表于ACM SIGGRAPH计算机图形学，第21卷，第163–169页。ACM，1987年。'
- en: '[59] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black. Amass:
    Archive of motion capture as surface shapes. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 5442–5451, 2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, 和 M. J. Black. Amass:
    作为表面形状的运动捕捉档案。发表于IEEE国际计算机视觉会议论文集，第5442–5451页，2019年。'
- en: '[60] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. G. Kim,
    and Y. Lipman. Convolutional neural networks on surfaces via seamless toric covers.
    ACM Trans. Graph., 36(4):71–1, 2017.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. G. Kim,
    和 Y. Lipman. 通过无缝环面覆盖的表面上的卷积神经网络。ACM Trans. Graph., 36(4):71–1，2017年。'
- en: '[61] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst. Geodesic convolutional
    neural networks on Riemannian manifolds. In Proceedings of the IEEE international
    conference on computer vision workshops, pages 37–45, 2015.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. Masci, D. Boscaini, M. Bronstein, 和 P. Vandergheynst. 在黎曼流形上的测地卷积神经网络。发表于IEEE国际计算机视觉会议研讨会论文集，第37–45页，2015年。'
- en: '[62] D. Maturana and S. Scherer. 3D convolutional neural networks for landing
    zone detection from LiDAR. In 2015 IEEE International Conference on Robotics and
    Automation (ICRA), pages 3471–3478\. IEEE, 2015.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] D. Maturana 和 S. Scherer. 用于从LiDAR中检测着陆区的3D卷积神经网络。发表于2015年IEEE国际机器人与自动化会议（ICRA）论文集，第3471–3478页。IEEE，2015年。'
- en: '[63] D. Maturana and S. Scherer. VoxNet: A 3D convolutional neural network
    for real-time object recognition. In 2015 IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS), pages 922–928\. IEEE, 2015.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] D. Maturana 和 S. Scherer. VoxNet: 用于实时物体识别的3D卷积神经网络。发表于2015年IEEE/RSJ国际智能机器人与系统会议（IROS）论文集，第922–928页。IEEE，2015年。'
- en: '[64] D. Meagher. Geometric modeling using octree encoding. Computer graphics
    and image processing, 19(2):129–147, 1982.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] D. Meagher. 使用八叉树编码的几何建模。计算机图形学与图像处理，19(2):129–147，1982年。'
- en: '[65] E. Mehr, A. Jourdan, N. Thome, M. Cord, and V. Guitteny. DiscoNet: Shapes
    learning on disconnected manifolds for 3D editing. In Proceedings of the IEEE
    International Conference on Computer Vision, pages 3474–3483, 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] E. Mehr, A. Jourdan, N. Thome, M. Cord, 和 V. Guitteny. DiscoNet: 用于3D编辑的断开流形上的形状学习。发表于IEEE国际计算机视觉会议论文集，第3474–3483页，2019年。'
- en: '[66] H.-Y. Meng, L. Gao, Y.-K. Lai, and D. Manocha. Vv-net: Voxel vae net with
    group convolutions for point cloud segmentation. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 8500–8508, 2019.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] H.-Y. Meng, L. Gao, Y.-K. Lai, 和 D. Manocha. Vv-net: 带有组卷积的体素VAE网络用于点云分割。发表于IEEE国际计算机视觉会议论文集，第8500–8508页，2019年。'
- en: '[67] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy
    networks: Learning 3D reconstruction in function space. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pages 4460–4470, 2019.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, 和 A. Geiger. 占用网络:
    在函数空间中学习 3D 重建。见《IEEE 计算机视觉与模式识别大会论文集》，第 4460–4470 页，2019 年。'
- en: '[68] K. Mo, P. Guerrero, L. Yi, H. Su, P. Wonka, N. J. Mitra, and L. J. Guibas.
    StructureNet: hierarchical graph networks for 3D shape generation. ACM Transactions
    on Graphics (TOG), 38(6):242, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] K. Mo, P. Guerrero, L. Yi, H. Su, P. Wonka, N. J. Mitra, 和 L. J. Guibas.
    StructureNet: 用于 3D 形状生成的分层图网络。《ACM 图形学汇刊》(TOG)，38(6):242，2019 年。'
- en: '[69] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su.
    PartNet: A large-scale benchmark for fine-grained and hierarchical part-level
    3D object understanding. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 909–918, 2019.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, 和 H. Su.
    PartNet: 一个大型基准测试用于细粒度和分层部分级别的 3D 物体理解。见《IEEE 计算机视觉与模式识别大会论文集》，第 909–918 页，2019
    年。'
- en: '[70] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein.
    Geometric deep learning on graphs and manifolds using mixture model CNNs. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5115–5124,
    2017.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, 和 M. M. Bronstein.
    使用混合模型 CNN 进行图形和流形上的几何深度学习。见《IEEE 计算机视觉与模式识别大会论文集》，第 5115–5124 页，2017 年。'
- en: '[71] C. Nash, Y. Ganin, S. Eslami, and P. W. Battaglia. PolyGen: An autoregressive
    generative model of 3D meshes. arXiv preprint arXiv:2002.10880, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] C. Nash, Y. Ganin, S. Eslami, 和 P. W. Battaglia. PolyGen: 3D 网格的自回归生成模型。arXiv
    预印本 arXiv:2002.10880，2020 年。'
- en: '[72] H. Pan, S. Liu, Y. Liu, and X. Tong. Convolutional neural networks on
    3D surfaces using parallel frames. arXiv preprint arXiv:1808.04952, 2018.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] H. Pan, S. Liu, Y. Liu, 和 X. Tong. 在 3D 表面上使用平行帧的卷积神经网络。arXiv 预印本 arXiv:1808.04952，2018
    年。'
- en: '[73] J. Pan, X. Han, W. Chen, J. Tang, and K. Jia. Deep mesh reconstruction
    from single rgb images via topology modification networks. In Proceedings of the
    IEEE International Conference on Computer Vision, pages 9964–9973, 2019.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] J. Pan, X. Han, W. Chen, J. Tang, 和 K. Jia. 通过拓扑修改网络从单一 RGB 图像中进行深度网格重建。见《IEEE
    国际计算机视觉会议论文集》，第 9964–9973 页，2019 年。'
- en: '[74] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. DeepSDF:
    Learning continuous signed distance functions for shape representation. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. J. Park, P. Florence, J. Straub, R. Newcombe, 和 S. Lovegrove. DeepSDF:
    学习连续的有符号距离函数用于形状表示。见《IEEE 计算机视觉与模式识别大会论文集》，2019 年。'
- en: '[75] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep learning on point
    sets for 3D classification and segmentation. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 652–660, 2017.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas. PointNet: 对点集进行深度学习以进行 3D 分类和分割。见《IEEE
    计算机视觉与模式识别大会论文集》，第 652–660 页，2017 年。'
- en: '[76] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J. Guibas. Volumetric
    and multi-view CNNs for object classification on 3D data. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 5648–5656, 2016.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, 和 L. J. Guibas. 用于 3D 数据对象分类的体积和多视角
    CNN。见《IEEE 计算机视觉与模式识别大会论文集》，第 5648–5656 页，2016 年。'
- en: '[77] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. PointNet++: Deep hierarchical
    feature learning on point sets in a metric space. In Advances in neural information
    processing systems, pages 5099–5108, 2017.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] C. R. Qi, L. Yi, H. Su, 和 L. J. Guibas. PointNet++: 在度量空间中的点集上进行深度分层特征学习。见《神经信息处理系统进展》，第
    5099–5108 页，2017 年。'
- en: '[78] Y.-L. Qiao, L. Gao, J. Yang, P. L. Rosin, Y.-K. Lai, and X. Chen. LaplacianNet:
    Learning on 3D meshes with laplacian encoding and pooling. arXiv preprint arXiv:1910.14063,
    2019.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y.-L. Qiao, L. Gao, J. Yang, P. L. Rosin, Y.-K. Lai, 和 X. Chen. LaplacianNet:
    使用拉普拉斯编码和池化进行 3D 网格学习。arXiv 预印本 arXiv:1910.14063，2019 年。'
- en: '[79] Y.-L. Qiao, Y.-K. Lai, H. Fu, and L. Gao. Synthesizing mesh deformation
    sequences with bidirectional lstm. IEEE Transactions on Visualization and Computer
    Graphics, 2020.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Y.-L. Qiao, Y.-K. Lai, H. Fu, 和 L. Gao. 使用双向 LSTM 合成网格变形序列。《IEEE 可视化与计算机图形学学报》，2020
    年。'
- en: '[80] G. Riegler, A. Osman Ulusoy, and A. Geiger. OctNet: Learning deep 3D representations
    at high resolutions. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 3577–3586, 2017.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] G. Riegler, A. Osman Ulusoy, 和 A. Geiger. OctNet: 在高分辨率下学习深度三维表示. 发表在
    IEEE 计算机视觉与模式识别会议论文集, 页码 3577–3586, 2017.'
- en: '[81] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover’s distance as
    a metric for image retrieval. International journal of computer vision, 40(2):99–121,
    2000.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. Rubner, C. Tomasi, 和 L. J. Guibas. 作为图像检索度量的地球搬运工距离. 国际计算机视觉期刊, 40(2):99–121,
    2000.'
- en: '[82] N. Sedaghat, M. Zolfaghari, E. Amiri, and T. Brox. Orientation-boosted
    voxel nets for 3D object recognition. In British Machine Vision Conference, 2017.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] N. Sedaghat, M. Zolfaghari, E. Amiri, 和 T. Brox. 面向三维物体识别的方向增强体素网. 发表在英国机器视觉会议,
    2017.'
- en: '[83] A. Sharma, O. Grau, and M. Fritz. Vconv-DAE: Deep volumetric shape learning
    without object labels. In European Conference on Computer Vision, pages 236–250.
    Springer, 2016.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] A. Sharma, O. Grau, 和 M. Fritz. Vconv-DAE: 无需物体标签的深度体积形状学习. 发表在欧洲计算机视觉大会,
    页码 236–250. Springer, 2016.'
- en: '[84] B. Shi, S. Bai, Z. Zhou, and X. Bai. Deeppano: Deep panoramic representation
    for 3-d shape recognition. IEEE Signal Processing Letters, 22(12):2339–2343, 2015.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] B. Shi, S. Bai, Z. Zhou, 和 X. Bai. Deeppano: 用于三维形状识别的深度全景表示. IEEE 信号处理通讯,
    22(12):2339–2343, 2015.'
- en: '[85] N. Silberman and R. Fergus. Indoor scene segmentation using a structured
    light sensor. In Proceedings of the International Conference on Computer Vision
    - Workshop on 3D Representation and Recognition, 2011.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] N. Silberman 和 R. Fergus. 使用结构光传感器进行室内场景分割. 发表在国际计算机视觉大会 - 3D 表示与识别研讨会,
    2011.'
- en: '[86] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and
    support inference from RGBD images. In European Conference on Computer Vision,
    pages 746–760. Springer, 2012.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] N. Silberman, D. Hoiem, P. Kohli, 和 R. Fergus. 从 RGBD 图像中进行室内分割和支持推断.
    发表在欧洲计算机视觉大会, 页码 746–760. Springer, 2012.'
- en: '[87] A. Sinha, J. Bai, and K. Ramani. Deep learning 3D shape surfaces using
    geometry images. In European Conference on Computer Vision, pages 223–240. Springer,
    2016.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] A. Sinha, J. Bai, 和 K. Ramani. 使用几何图像进行深度学习三维形状表面. 发表在欧洲计算机视觉大会, 页码 223–240.
    Springer, 2016.'
- en: '[88] A. Sinha, A. Unmesh, Q. Huang, and K. Ramani. SurfNet: Generating 3D shape
    surfaces using deep residual networks. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 6040–6049, 2017.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] A. Sinha, A. Unmesh, Q. Huang, 和 K. Ramani. SurfNet: 使用深度残差网络生成三维形状表面.
    发表在 IEEE 计算机视觉与模式识别会议论文集, 页码 6040–6049, 2017.'
- en: '[89] R. Socher, B. Huval, B. Bath, C. D. Manning, and A. Y. Ng. Convolutional-recursive
    deep learning for 3D object classification. In Advances in neural information
    processing systems, pages 656–664, 2012.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] R. Socher, B. Huval, B. Bath, C. D. Manning, 和 A. Y. Ng. 用于三维物体分类的卷积递归深度学习.
    发表在神经信息处理系统进展, 页码 656–664, 2012.'
- en: '[90] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng. Parsing natural scenes
    and natural language with recursive neural networks. In Proceedings of the 28th
    international conference on machine learning (ICML-11), pages 129–136, 2011.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] R. Socher, C. C. Lin, C. Manning, 和 A. Y. Ng. 使用递归神经网络解析自然场景和自然语言. 发表在第28届国际机器学习大会
    (ICML-11) 论文集, 页码 129–136, 2011.'
- en: '[91] S. Song and J. Xiao. Deep sliding shapes for amodal 3D object detection
    in RGB-D images. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 808–816, 2016.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] S. Song 和 J. Xiao. 用于 RGB-D 图像中模态无关 3D 物体检测的深度滑动形状. 发表在 IEEE 计算机视觉与模式识别会议论文集,
    页码 808–816, 2016.'
- en: '[92] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic
    scene completion from a single depth image. Proceedings of 30th IEEE Conference
    on Computer Vision and Pattern Recognition, 2017.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, 和 T. Funkhouser. 从单一深度图像中进行语义场景补全.
    发表在第30届 IEEE 计算机视觉与模式识别会议, 2017.'
- en: '[93] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multi-view convolutional
    neural networks for 3D shape recognition. In Proceedings of the IEEE international
    conference on computer vision, pages 945–953, 2015.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] H. Su, S. Maji, E. Kalogerakis, 和 E. Learned-Miller. 用于三维形状识别的多视图卷积神经网络.
    发表在 IEEE 国际计算机视觉会议论文集, 页码 945–953, 2015.'
- en: '[94] Q. Tan, L. Gao, Y.-K. Lai, and S. Xia. Variational autoencoders for deforming
    3D mesh models. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 5841–5850, 2018.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Q. Tan, L. Gao, Y.-K. Lai, 和 S. Xia. 用于变形三维网格模型的变分自编码器. 发表在 IEEE 计算机视觉与模式识别会议论文集,
    页码 5841–5850, 2018.'
- en: '[95] Q. Tan, L. Gao, Y.-K. Lai, J. Yang, and S. Xia. Mesh-based autoencoders
    for localized deformation component analysis. In Thirty-Second AAAI Conference
    on Artificial Intelligence, 2018.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Q. Tan, L. Gao, Y.-K. Lai, J. Yang 和 S. Xia. 基于网格的自动编码器用于局部变形分量分析. 载于《第三十二届
    AAAI 人工智能会议》，2018年。'
- en: '[96] Q. Tan, Z. Pan, L. Gao, and D. Manocha. Realtime simulation of thin-shell
    deformable materials using cnn-based mesh embedding. IEEE Robotics and Automation
    Letters, 5(2):2325–2332, 2020.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Q. Tan, Z. Pan, L. Gao 和 D. Manocha. 基于 CNN 的网格嵌入的薄壳可变形材料的实时模拟. 《IEEE机器人与自动化快报》，5(2):2325–2332，2020年。'
- en: '[97] J. Tang, X. Han, J. Pan, K. Jia, and X. Tong. A skeleton-bridged deep
    learning approach for generating meshes of complex topologies from single RGB
    images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 4541–4550, 2019.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. Tang, X. Han, J. Pan, K. Jia 和 X. Tong. 一种基于骨架的深度学习方法，用于从单张 RGB 图像生成复杂拓扑的网格.
    载于《IEEE 计算机视觉与模式识别会议论文集》，页码 4541–4550，2019年。'
- en: '[98] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree generating networks:
    Efficient convolutional architectures for high-resolution 3D outputs. In Proceedings
    of the IEEE International Conference on Computer Vision, pages 2088–2096, 2017.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Tatarchenko, A. Dosovitskiy 和 T. Brox. 八叉树生成网络：用于高分辨率 3D 输出的高效卷积架构.
    载于《IEEE 国际计算机视觉会议论文集》，页码 2088–2096，2017年。'
- en: '[99] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural
    information processing systems, pages 5998–6008, 2017.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser 和 I. Polosukhin. 注意力机制即你所需的一切. 载于《神经信息处理系统进展》，页码 5998–6008，2017年。'
- en: '[100] N. Verma, E. Boyer, and J. Verbeek. FeastNet: Feature-steered graph convolutions
    for 3D shape analysis. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 2598–2606, 2018.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] N. Verma, E. Boyer 和 J. Verbeek. FeastNet：用于 3D 形状分析的特征引导图卷积. 载于《IEEE计算机视觉与模式识别会议论文集》，页码
    2598–2606，2018年。'
- en: '[101] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting
    and composing robust features with denoising autoencoders. In Proceedings of the
    25th international conference on Machine learning, pages 1096–1103\. ACM, 2008.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] P. Vincent, H. Larochelle, Y. Bengio 和 P.-A. Manzagol. 利用去噪自动编码器提取和组合稳健特征.
    载于《第 25 届国际机器学习会议论文集》，页码 1096–1103。ACM，2008年。'
- en: '[102] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol.
    Stacked denoising autoencoders: Learning useful representations in a deep network
    with a local denoising criterion. Journal of machine learning research, 11(Dec):3371–3408,
    2010.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio 和 P.-A. Manzagol. 堆叠去噪自动编码器：在具有局部去噪准则的深度网络中学习有用的表示.
    《机器学习研究杂志》，11(12):3371–3408，2010年。'
- en: '[103] H. Wang, N. Schor, R. Hu, H. Huang, D. Cohen-Or, and H. Huang. Global-to-local
    generative model for 3D shapes. In SIGGRAPH Asia 2018 Technical Papers, page 214\.
    ACM, 2018.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] H. Wang, N. Schor, R. Hu, H. Huang, D. Cohen-Or 和 H. Huang. 全球到局部的 3D
    形状生成模型. 载于《SIGGRAPH 亚洲 2018 技术论文》，第 214 页。ACM，2018年。'
- en: '[104] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2Mesh:
    Generating 3D mesh models from single RGB images. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 52–67, 2018.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu 和 Y.-G. Jiang. Pixel2Mesh：从单张
    RGB 图像生成 3D 网格模型. 载于《欧洲计算机视觉会议（ECCV）论文集》，页码 52–67，2018年。'
- en: '[105] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong. O-CNN: Octree-based
    convolutional neural networks for 3D shape analysis. ACM Transactions on Graphics
    (TOG), 36(4):72, 2017.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun 和 X. Tong. O-CNN：基于八叉树的卷积神经网络用于
    3D 形状分析. 《ACM 图形学通讯（TOG）》，36(4):72，2017年。'
- en: '[106] P.-S. Wang, C.-Y. Sun, Y. Liu, and X. Tong. Adaptive O-CNN: a patch-based
    deep representation of 3D shapes. In SIGGRAPH Asia 2018 Technical Papers, page
    217\. ACM, 2018.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] P.-S. Wang, C.-Y. Sun, Y. Liu 和 X. Tong. 自适应 O-CNN：基于补丁的 3D 形状深度表示. 载于《SIGGRAPH
    亚洲 2018 技术论文》，第 217 页。ACM，2018年。'
- en: '[107] Y. Wang and J. M. Solomon. Deep closest point: Learning representations
    for point cloud registration. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 3523–3532, 2019.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Y. Wang 和 J. M. Solomon. 深度最近点：学习点云配准的表示. 载于《IEEE 国际计算机视觉会议论文集》，页码 3523–3532，2019年。'
- en: '[108] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon.
    Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG),
    38(5):1–12, 2019.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein 和 J. M. Solomon.
    动态图 CNN 用于点云学习. 《ACM 图形学通讯（TOG）》，38(5):1–12，2019年。'
- en: '[109] C. Wen, Y. Zhang, Z. Li, and Y. Fu. Pixel2Mesh++: Multi-view 3D mesh
    generation via deformation. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 1042–1051, 2019.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. Wen, Y. Zhang, Z. Li, 和 Y. Fu. Pixel2Mesh++: 通过变形进行多视角3D网格生成。见于《IEEE国际计算机视觉会议论文集》，页1042–1051，2019年。'
- en: '[110] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum. Learning a probabilistic
    latent space of object shapes via 3D generative-adversarial modeling. In Advances
    in neural information processing systems, pages 82–90, 2016.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. Wu, C. Zhang, T. Xue, B. Freeman, 和 J. Tenenbaum. 通过3D生成对抗建模学习对象形状的概率潜在空间。见于《神经信息处理系统进展》，页82–90，2016年。'
- en: '[111] R. Wu, Y. Zhuang, K. Xu, H. Zhang, and B. Chen. PQ-NET: A generative
    part seq2seq network for 3D shapes. arXiv preprint arXiv:1911.10949, 2019.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] R. Wu, Y. Zhuang, K. Xu, H. Zhang, 和 B. Chen. PQ-NET: 用于3D形状的生成部件seq2seq网络。arXiv预印本
    arXiv:1911.10949，2019年。'
- en: '[112] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3D
    ShapeNets: A deep representation for volumetric shapes. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 1912–1920, 2015.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, 和 J. Xiao. 3D ShapeNets:
    一种用于体积形状的深度表示。见于《IEEE计算机视觉与模式识别会议论文集》，页1912–1920，2015年。'
- en: '[113] Z. Wu, X. Wang, D. Lin, D. Lischinski, D. Cohen-Or, and H. Huang. SAGNet:
    structure-aware generative network for 3D-shape modeling. ACM Transactions on
    Graphics (TOG), 38(4):91, 2019.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Z. Wu, X. Wang, D. Lin, D. Lischinski, D. Cohen-Or, 和 H. Huang. SAGNet:
    结构感知生成网络用于3D形状建模。《ACM图形学汇刊》（TOG），38(4):91，2019年。'
- en: '[114] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas,
    and S. Savarese. ObjectNet3D: A large scale database for 3D object recognition.
    In European Conference on Computer Vision, pages 160–176. Springer, 2016.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas,
    和 S. Savarese. ObjectNet3D: 一个大规模的3D对象识别数据库。见于《欧洲计算机视觉会议》，页160–176。Springer，2016年。'
- en: '[115] H. Xu, M. Dong, and Z. Zhong. Directionally convolutional networks for
    3D shape segmentation. In Proceedings of the IEEE International Conference on
    Computer Vision, pages 2698–2707, 2017.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] H. Xu, M. Dong, 和 Z. Zhong. 用于3D形状分割的方向卷积网络。见于《IEEE国际计算机视觉会议论文集》，页2698–2707，2017年。'
- en: '[116] Q. Xu, W. Wang, D. Ceylan, R. Mech, and U. Neumann. DISN: Deep implicit
    surface network for high-quality single-view 3D reconstruction. In NeurIPS, 2019.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Q. Xu, W. Wang, D. Ceylan, R. Mech, 和 U. Neumann. DISN: 高质量单视图3D重建的深度隐式表面网络。见于NeurIPS，2019年。'
- en: '[117] Y. Yang, C. Feng, Y. Shen, and D. Tian. FoldingNet: Point cloud auto-encoder
    via deep grid deformation. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 206–215, 2018.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Y. Yang, C. Feng, Y. Shen, 和 D. Tian. FoldingNet: 通过深度网格变形的点云自编码器。见于《IEEE计算机视觉与模式识别会议论文集》，页206–215，2018年。'
- en: '[118] W. Yifan, S. Wu, H. Huang, D. Cohen-Or, and O. Sorkine-Hornung. Patch-based
    progressive 3D point set upsampling. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 5958–5967, 2019.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] W. Yifan, S. Wu, H. Huang, D. Cohen-Or, 和 O. Sorkine-Hornung. 基于补丁的渐进式3D点集上采样。见于《IEEE计算机视觉与模式识别会议论文集》，页5958–5967，2019年。'
- en: '[119] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng. PU-Net: Point cloud
    upsampling network. In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pages 2790–2799, 2018.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, 和 P.-A. Heng. PU-Net: 点云上采样网络。见于《IEEE计算机视觉与模式识别会议论文集》，页2790–2799，2018年。'
- en: '[120] Y.-J. Yuan, Y.-K. Lai, J. Yang, H. Fu, and L. Gao. Mesh variational autoencoders
    with edge contraction pooling. arXiv preprint arXiv:1908.02507, 2019.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Y.-J. Yuan, Y.-K. Lai, J. Yang, H. Fu, 和 L. Gao. 带边缘收缩池的网格变分自编码器。arXiv预印本
    arXiv:1908.02507，2019年。'
- en: '[121] C. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem. 3D-PRNN: Generating
    shape primitives with recurrent neural networks. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 900–909, 2017.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] C. Zou, E. Yumer, J. Yang, D. Ceylan, 和 D. Hoiem. 3D-PRNN: 使用递归神经网络生成形状原语。见于《IEEE国际计算机视觉会议论文集》，页900–909，2017年。'
- en: \Author
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: \Author
- en: YunPengXiao.pdfYun-Peng Xiao received his bachelor’s degree in computer science
    from Nankai University, He is currently a Master Student in the Institute of Computing
    Technology, Chinese Academy of Sciences. His research interests include computer
    graphics and geometric processing.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: YunPengXiao.pdf 云鹏肖获得了南开大学计算机科学学士学位。他目前是中国科学院计算技术研究所的硕士研究生。他的研究兴趣包括计算机图形学和几何处理。
- en: \Author
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: \Author
- en: YuKunLai.pdfYu-Kun Lai received his bachelor’s degree and PhD degree in computer
    science from Tsinghua University in 2003 and 2008, respectively. He is currently
    a Reader in the School of Computer Science & Informatics, Cardiff University.
    His research interests include computer graphics, geometry processing, image processing
    and computer vision. He is on the editorial boards of *Computer Graphics Forum*
    and *The Visual Computer*.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Yu-Kun Lai 于 2003 年和 2008 年分别在清华大学获得计算机科学的学士和博士学位。他目前是卡迪夫大学计算机科学与信息学学院的讲师。他的研究兴趣包括计算机图形学、几何处理、图像处理和计算机视觉。他是*Computer
    Graphics Forum*和*The Visual Computer*的编委会成员。
- en: \Author
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: \Author
- en: FangLueZhang.pdfFang-Lue Zhang is currently a Lecturer with Victoria University
    of Wellington, New Zealand. He received the Bachelor’s degree from Zhejiang University,
    Hangzhou, China, in 2009, and the Doctoral degree from Tsinghua University, Beijing,
    China, in 2015\. His research interests include image and video editing, computer
    vision, and computer graphics. He is a member of IEEE and ACM. He received Victoria
    Early-Career Research Excellence Award in 2019.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 张芳略目前是新西兰维多利亚大学的讲师。他于 2009 年在中国杭州的浙江大学获得学士学位，并于 2015 年在中国北京的清华大学获得博士学位。他的研究兴趣包括图像和视频编辑、计算机视觉以及计算机图形学。他是
    IEEE 和 ACM 的会员。他于 2019 年获得维多利亚早期职业研究卓越奖。
- en: \Author
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: \Author
- en: ChunpengLi.pdfChunpeng Li was born in 1980\. He received his PhD degree in 2008
    and now is an Associate Professor at the Institute of Computing Technology, Chinese
    Academy of Sciences. His main research interests are virtual reality, human–computer
    interaction, and computer graphics.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 李春鹏于 1980 年出生。他于 2008 年获得博士学位，目前是中国科学院计算技术研究所的副教授。他的主要研究兴趣包括虚拟现实、人机交互和计算机图形学。
- en: \Author
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: \Author
- en: GL1.pdfLin Gao received a bachelor’s degree in mathematics from Sichuan University
    and a PhD degree in computer science from Tsinghua University. He is currently
    an Associate Professor at the Institute of Computing Technology, Chinese Academy
    of Sciences. His research interests include computer graphics and geometric processing.
    He received the Newton Advanced Fellowship award from the Royal Society in 2019.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 林高在四川大学获得数学学士学位，并在清华大学获得计算机科学博士学位。他目前是中国科学院计算技术研究所的副教授。他的研究兴趣包括计算机图形学和几何处理。他于
    2019 年获得皇家学会的牛顿高级研究奖。
