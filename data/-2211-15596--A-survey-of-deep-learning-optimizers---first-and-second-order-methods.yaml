- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:42:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:42:57'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2211.15596] A survey of deep learning optimizers - first and second order
    methods'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2211.15596] 深度学习优化器的调查 - 第一和第二阶方法'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2211.15596](https://ar5iv.labs.arxiv.org/html/2211.15596)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2211.15596](https://ar5iv.labs.arxiv.org/html/2211.15596)
- en: A survey of deep learning optimizers - first and second order methods
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习优化器的调查 - 第一和第二阶方法
- en: ROHAN V KASHYAP [rohankashyap@iisc.ac.in](mailto:rohankashyap@iisc.ac.in) [1234-5678-9012](https://orcid.org/1234-5678-9012
    "ORCID identifier") Indian Institute of ScienceBangaloreKarnatakaIndia560012(2023)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ROHAN V KASHYAP [rohankashyap@iisc.ac.in](mailto:rohankashyap@iisc.ac.in) [1234-5678-9012](https://orcid.org/1234-5678-9012
    "ORCID 标识符") 印度科学研究所印度卡纳塔克邦班加罗尔560012（2023）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Deep Learning optimization involves minimizing a high-dimensional loss function
    in the weight space which is often perceived as difficult due to its inherent
    difficulties such as saddle points, local minima, ill-conditioning of the Hessian
    and limited compute resources. In this paper, we provide a comprehensive review
    of $14$ standard optimization methods successfully used in deep learning research
    and a theoretical assessment of the difficulties in numerical optimization from
    the optimization literature.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习优化涉及在权重空间中最小化高维损失函数，通常由于固有困难，如鞍点、局部最小值、Hessian 矩阵的病态和有限的计算资源而被视为困难。在本文中，我们全面回顾了成功在深度学习研究中使用的
    $14$ 种标准优化方法，并从优化文献中对数值优化的困难进行了理论评估。
- en: '^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†journal: POMACS^†^†journalvolume:
    37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth: 8'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†journal: POMACS^†^†journalvolume:
    37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth: 8'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: '![[Uncaptioned image]](img/7efe3cf416cd586add043d4cf29ec4ca.png)![[Uncaptioned
    image]](img/3ca43765c9553039b9d6af3278cdc6dd.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![未加标题的图片](img/7efe3cf416cd586add043d4cf29ec4ca.png)![未加标题的图片](img/3ca43765c9553039b9d6af3278cdc6dd.png)'
- en: Optimization encompasses a central role in machine learning ([b1,](#bib.bib1)
    ), statistical physics, pure mathematics ([b11,](#bib.bib11) ; [b16,](#bib.bib16)
    ; [b30,](#bib.bib30) ), random matrix theory and also scientific research. Deep
    learning involves optimization of non-convex loss functions over continuous, high-dimensional
    spaces. Neural network training involves solving a loss function that is differentiable
    and continuous using gradient-based optimization techniques that utilize the first
    order or second order information to update its weight parameters and thus seek
    to find the critical point. Gradient descent ([b30,](#bib.bib30) ), quasi-Newton
    ([b62,](#bib.bib62) ), BFGS ([b59,](#bib.bib59) ) and conjugate gradient ([b55,](#bib.bib55)
    ) methods are commonly used to perform such minimizations and find the optimal
    solution.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 优化在机器学习 ([b1,](#bib.bib1))、统计物理学、纯数学 ([b11,](#bib.bib11); [b16,](#bib.bib16);
    [b30,](#bib.bib30))、随机矩阵理论以及科学研究中占据着核心地位。深度学习涉及在连续、高维空间中优化非凸损失函数。神经网络训练涉及求解可微且连续的损失函数，使用基于梯度的优化技术来更新其权重参数，从而寻求找到临界点。梯度下降
    ([b30,](#bib.bib30))、拟牛顿 ([b62,](#bib.bib62))、BFGS ([b59,](#bib.bib59)) 和共轭梯度
    ([b55,](#bib.bib55)) 方法通常用于执行此类最小化操作并寻找最优解。
- en: 'Definition 1. Given $K$ weight matrices $(\theta_{k})_{k\leq K}$ the output
    $y$ of a deep neural network with a activation function $\phi$ is given as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 1. 给定 $K$ 个权重矩阵 $(\theta_{k})_{k \leq K}$，带有激活函数 $\phi$ 的深度神经网络的输出 $y$ 如下：
- en: '| (1) |  | $f(x)=\phi\left(\phi\left(\cdots\phi\left(x\cdot\theta_{1}\right)\cdots\right)\cdot\theta_{K-1}\right)\cdot\theta_{K},$
    |  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $f(x)=\phi\left(\phi\left(\cdots\phi\left(x\cdot\theta_{1}\right)\cdots\right)\cdot\theta_{K-1}\right)\cdot\theta_{K},$
    |  |'
- en: 'where $x\in R^{d}$ is the input to the model. Let $\left\{x_{i},y_{i}\right\}_{i=1}^{m}$
    denote the set of training examples. In principle, the goal of a deep learning
    algorithm is to reduce the expected generalization error $\mathbb{E}(L(f(x;\theta),y))$
    where $L$ is the designated loss-function such as mean-squared error, and $f(x;\theta)$
    is the function estimate for $y=f(x)$ with parameters $\theta$. The most common
    estimator is the maximum likelihood estimation (MLE) for $\theta$ defined as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $x \in R^{d}$ 是模型的输入。让 $\left\{x_{i},y_{i}\right\}_{i=1}^{m}$ 表示训练示例集。原则上，深度学习算法的目标是减少预期的广义误差
    $\mathbb{E}(L(f(x;\theta),y))$ 其中 $L$ 是指定的损失函数，如均方误差，$f(x;\theta)$ 是具有参数 $\theta$
    的 $y=f(x)$ 的函数估计。最常见的估计方法是 $\theta$ 的最大似然估计（MLE），定义如下：
- en: '|  | $\boldsymbol{\theta}_{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg\max}\hskip
    1.99997pt\mathbb{E}_{\mathbf{x}\sim\hat{p}_{\text{data }}}\log p_{\text{model
    }}(\boldsymbol{x};\boldsymbol{\theta})$ |  |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}_{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg\max}\hskip
    1.99997pt\mathbb{E}_{\mathbf{x}\sim\hat{p}_{\text{data }}}\log p_{\text{model
    }}(\boldsymbol{x};\boldsymbol{\theta})$ |  |'
- en: 'where $p_{\text{model }}$ is the model distribution. This accounts to minimizing
    the Kullback-Leibler (KL) divergence (a metric) between the empirical data distribution
    $p_{model}$ and the true data distribution $\hat{p}_{\text{data}}$:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{\text{model }}$ 是模型分布。这意味着最小化经验数据分布 $p_{model}$ 和真实数据分布 $\hat{p}_{\text{data}}$
    之间的 Kullback-Leibler (KL) 散度（一个度量）：
- en: '|  | $D_{\mathrm{KL}}\left(\hat{p}_{\text{data }}\&#124;p_{\text{model }}\right)=\mathbb{E}_{\mathbf{x}\sim\hat{p}_{\text{data
    }}}\left[\log\hat{p}_{\text{data }}(\boldsymbol{x})-\log p_{\text{model }}(\boldsymbol{x})\right]$
    |  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  | $D_{\mathrm{KL}}\left(\hat{p}_{\text{data }}\&#124;p_{\text{model }}\right)=\mathbb{E}_{\mathbf{x}\sim\hat{p}_{\text{data
    }}}\left[\log\hat{p}_{\text{data }}(\boldsymbol{x})-\log p_{\text{model }}(\boldsymbol{x})\right]$
    |  |'
- en: 'Since the true data distribution is largely unknown, we minimize the expected
    loss on the training set - a quantity called the empirical risk given as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于真实数据分布在很大程度上是未知的，我们最小化训练集上的期望损失——称为经验风险，如下所示：
- en: '|  | $\mathbb{E}_{\boldsymbol{x},\mathrm{y}\sim\hat{p}_{\mathrm{data}}(\boldsymbol{x},y)}[L(f(\boldsymbol{x};\boldsymbol{\theta}),y)]=\frac{1}{m}\sum_{i=1}^{m}L\left(f\left(\boldsymbol{x}^{(i)};\boldsymbol{\theta}\right),y^{(i)}\right)$
    |  |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{\boldsymbol{x},\mathrm{y}\sim\hat{p}_{\mathrm{data}}(\boldsymbol{x},y)}[L(f(\boldsymbol{x};\boldsymbol{\theta}),y)]=\frac{1}{m}\sum_{i=1}^{m}L\left(f\left(\boldsymbol{x}^{(i)};\boldsymbol{\theta}\right),y^{(i)}\right)$
    |  |'
- en: 'where $L$ is the loss-function and $f(x;\theta)$ is the prediction output when
    the input is $x$ and $\hat{p}_{data}$ is the empirical distribution. In a supervised
    setting, $y$ is the target output and $p(y|x)$ is the probability distribution
    to be estimated. MLE is the best estimator asymptotically and is consistent in
    the sense that it converges ¹¹1We refer to converges in the sense of pointwise
    and not in distribution i.e., $\hat{\theta}\rightarrow\theta^{*}$, as $m\rightarrow\infty$.
    to the parameters of the true data distribution $\hat{p}_{\text{data }}$ as $m\rightarrow\infty$.
    Although this method in theory, is highly prone to overfitting since models with
    high capacity can simply memorize the training set, empirical results surprisingly
    enough dictate the opposite and prove the model’s capability to achieve local
    generalization power within its capacity and thus generalizes to unseen examples
    in the test samples. The solution is to add a penalty term called the regularizer
    to the empirical risk defined as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L$ 是损失函数，$f(x;\theta)$ 是当输入为 $x$ 时的预测输出，而 $\hat{p}_{data}$ 是经验分布。在监督学习中，$y$
    是目标输出，$p(y|x)$ 是待估计的概率分布。极大似然估计（MLE）在渐近意义上是最佳估计量，并且在 $m\rightarrow\infty$ 时收敛于真实数据分布
    $\hat{p}_{\text{data }}$ 的参数。尽管在理论上，这种方法极易过拟合，因为高容量的模型可以简单地记住训练集，但经验结果却令人惊讶地表明相反的情况，并证明模型在其容量范围内具有局部泛化能力，从而能够对测试样本中的未见示例进行泛化。解决方案是向经验风险中添加一个称为正则化项的惩罚项，如下所示：
- en: '|  | $\tilde{L}(\boldsymbol{\theta})=\underbrace{\frac{1}{N}\sum_{i=1}^{N}L(y(\mathbf{x},\boldsymbol{\theta}),t)}_{\text{training
    loss }}+\underbrace{\mathcal{R}(\boldsymbol{\theta})}_{\text{regularizer }}$ |  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{L}(\boldsymbol{\theta})=\underbrace{\frac{1}{N}\sum_{i=1}^{N}L(y(\mathbf{x},\boldsymbol{\theta}),t)}_{\text{训练损失
    }}+\underbrace{\mathcal{R}(\boldsymbol{\theta})}_{\text{正则化项 }}$ |  |'
- en: where $\mathcal{R}(\boldsymbol{\theta})$ is the regularization term such as
    $L1$ or $L2$ regularization as illustrated in Fig. LABEL:fig:regularization. However,
    high-dimensional non-convex optimization techniques come at the cost of no theoretical
    guarantees yet obtain state-of-the-art results on numerous tasks such as image
    classification ([b64,](#bib.bib64) ), text processing ([b63,](#bib.bib63) ), and
    representation learning ([b65,](#bib.bib65) ). Also, deep nets are composed of
    layers of affine transformations followed by point-wise non-linear activations
    such as RELU, sigmoid and tanh functions. Thus, the choice of the non-linear function
    becomes more important to avoid vanishing and exploding gradients problems ([b33,](#bib.bib33)
    ).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{R}(\boldsymbol{\theta})$ 是正则化项，如图 LABEL:fig:regularization 所示的$L1$或$L2$正则化。然而，高维非凸优化技术虽然尚无理论保证，但在图像分类（[b64,](#bib.bib64)）、文本处理（[b63,](#bib.bib63)）和表示学习（[b65,](#bib.bib65)）等众多任务上取得了最先进的结果。此外，深度网络由一系列仿射变换层和逐点非线性激活函数（如RELU、sigmoid和tanh函数）组成。因此，选择非线性函数变得更加重要，以避免梯度消失和爆炸问题（[b33,](#bib.bib33)）。
- en: 1.1\. Mathematical Preliminaries and Notations
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 数学预备知识和符号
- en: Let $({x^{(i)},y^{(i)}})$ denote the training examples and the corresponding
    targets respectively. Let $f(.,.;\theta)$ denote a neural network with parameters
    $\theta$ and a loss function $L$. For $R^{n}$-valued functions $\nabla$ corresponds
    to the gradient operator. For all the optimization methods discussed in the paper,
    $g$ corresponds to the computed gradient vector across multiple training samples,
    $\epsilon$ is the learning rate and $\rho$ is the decay rate. For first order
    methods, $r$ is the accumulated square gradient and $v$ is the velocity vector
    respectively. For second order methods, we denote the Hessian matrix of the loss
    function $L$ with respect to the model parameters $\theta$ by $H$ and $\lambda_{i}$’s
    are the corresponding eigenvalues. Unless stated otherwise, the update rule is
    given as $\theta=\theta-\Delta\theta$, where $\Delta\theta=\epsilon g$. We denote
    the basic version of stochastic gradient descent algorithm in Algorithm [1](#algorithm1
    "In 1.1\. Mathematical Preliminaries and Notations ‣ 1\. Introduction ‣ A survey
    of deep learning optimizers - first and second order methods").
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 设$({x^{(i)},y^{(i)}})$表示训练样本及其对应目标。设$f(.,.;\theta)$表示具有参数$\theta$的神经网络及其损失函数$L$。对于$R^{n}$-值函数，$\nabla$对应于梯度算子。对于本文讨论的所有优化方法，$g$表示计算的跨多个训练样本的梯度向量，$\epsilon$是学习率，$\rho$是衰减率。对于一阶方法，$r$是累积平方梯度，$v$是速度向量。对于二阶方法，我们用$H$表示相对于模型参数$\theta$的损失函数$L$的Hessian矩阵，$\lambda_{i}$是相应的特征值。除非另有说明，更新规则为$\theta=\theta-\Delta\theta$，其中$\Delta\theta=\epsilon
    g$。我们在算法[1](#algorithm1 "在 1.1 数学预备知识和符号 ‣ 1 引言 ‣ 深度学习优化器综述 - 一阶和二阶方法")中给出了随机梯度下降算法的基本版本。
- en: '1 Initialize: Initial parameter $\theta$, Learning rate $\epsilon$.2 while *stopping
    criterion not met* do3       Sample $m$ training examples ${x^{(1)},...,x^{(m)}}$
    and their corresponding targets $y^{(i)}s$.4       Compute gradient estimate:
    $g\leftarrow\left[\frac{1}{m}\sum_{i=1}^{m}L\left(f\left(\boldsymbol{x}^{(i)};\boldsymbol{\theta}+\alpha\boldsymbol{v}\right),\boldsymbol{y}^{(i)}\right)\right]$5      
    Apply update: $\theta\leftarrow\theta-\epsilon g$.6 end while'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 1 初始化：初始参数$\theta$，学习率$\epsilon$。2 当*停止准则未满足*时3       取样$m$个训练样本${x^{(1)},...,x^{(m)}}$及其对应目标$y^{(i)}s$。4       计算梯度估计：$g\leftarrow\left[\frac{1}{m}\sum_{i=1}^{m}L\left(f\left(\boldsymbol{x}^{(i)};\boldsymbol{\theta}+\alpha\boldsymbol{v}\right),\boldsymbol{y}^{(i)}\right)\right]$5       应用更新：$\theta\leftarrow\theta-\epsilon
    g$。6 结束
- en: Algorithm 1 Stochastic Gradient Descent
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 随机梯度下降
- en: 1.2\. Literature Overview
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 文献综述
- en: The most important algorithm for machine learning optimization is the stochastic
    gradient descent method (SGD). This is a general extension to the gradient descent
    method to handle large training sets without being computationally expensive as
    discussed in Alg [1](#algorithm1 "In 1.1\. Mathematical Preliminaries and Notations
    ‣ 1\. Introduction ‣ A survey of deep learning optimizers - first and second order
    methods"). Since, we perform gradient updates at each step using a mini-batch
    of examples the asymptotic cost for SGD is $O(1)$ as a function of $m$, where
    m is the number of training examples. In this section, we provide a detailed discussion
    of the various problems associated with this method.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习优化中最重要的算法是随机梯度下降法（SGD）。这是对梯度下降法的一种通用扩展，用于处理大规模训练集，而不会像在 Alg [1](#algorithm1
    "在 1.1\. 数学预备知识和符号 ‣ 1\. 介绍 ‣ 深度学习优化器的综述 - 一阶和二阶方法") 中讨论的那样计算开销很大。由于我们在每一步都使用一个小批量的样本进行梯度更新，因此
    SGD 的渐近成本为 $O(1)$，作为 $m$ 的函数，其中 m 是训练样本的数量。在这一部分，我们将详细讨论与这种方法相关的各种问题。
- en: '![Refer to caption](img/c9ee0b51a55a63a0f20d3371e4d581e6.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c9ee0b51a55a63a0f20d3371e4d581e6.png)'
- en: Figure 1\. Illustration of flat and sharp minima. Source Keskar ([b46,](#bib.bib46)
    ).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 平坦和陡峭极小值的示意图。来源 Keskar ([b46,](#bib.bib46) )。
- en: 1.3\. Flat Minima
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 平坦极小值
- en: Hochreiter et al. ([b45,](#bib.bib45) ) conjectured that the generalization
    of deepnets is related to the curvature of the loss at the converged solution.
    Their algorithm the ”flat minima search” utilizes the Hessian information to find
    a large region in the weight space called the ”flat minima” where all the solutions
    lead to a small error function value. It is also noted that high error local minima
    traps do not appear when the model is overparameterized, and Keskar et al. ([b46,](#bib.bib46)
    ) demonstrated that the large batch methods always tend to converge to sharp minimizers
    (large number of positive eigenvalues) and thus generalize a bit worse than the
    small batch methods that are known to converge to flat minimizers which are characterized
    by having numerous small eigenvalues though they have comparable training accuracies
    ([b4,](#bib.bib4) ).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Hochreiter 等人 ([b45,](#bib.bib45) ) 推测深度网络的泛化能力与收敛解处的损失曲率有关。他们的算法“平坦极小值搜索”利用
    Hessian 信息在权重空间中找到一个被称为“平坦极小值”的大区域，在这个区域中所有的解都导致较小的误差函数值。还注意到，当模型被过参数化时，不会出现高误差的局部极小值陷阱，而
    Keskar 等人 ([b46,](#bib.bib46) ) 证明，大批量方法总是倾向于收敛到陡峭的极小值（大量正特征值），因此泛化效果略差于小批量方法，而小批量方法已知会收敛到平坦极小值，这些平坦极小值的特征是具有许多小特征值，尽管它们的训练准确率相当
    ([b4,](#bib.bib4) )。
- en: This is attributed to the empirical evidence as discussed in ([b13,](#bib.bib13)
    ), ([b45,](#bib.bib45) ) and ([b46,](#bib.bib46) ) that large batch methods are
    attracted to regions of sharp minima while the basins found by small batch methods
    are wider, and since they are unable to escape these basins of attractions, they
    generalize better. Although a larger batch size provides a more accurate approximation
    of the true gradient with fewer fluctuations in the update step, it is computationally
    very expensive and leads to fewer update steps, while a smaller batch size offers
    a regularizing effect due to the noise they add during the learning process. However,
    this might lead to high variance in the gradient estimate and thus requires a
    smaller learning rate to maintain stability and converge to an optimal solution.
    It turns out that most directions in the weight space have similar loss values
    since the Hessian consists of a large number of eigenvalues close to zero, even
    at random initialization points.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这归因于在 ([b13,](#bib.bib13) )、([b45,](#bib.bib45) ) 和 ([b46,](#bib.bib46) ) 中讨论的实证证据，即大批量方法被吸引到陡峭极小值区域，而小批量方法找到的盆地则更宽广，由于无法逃脱这些吸引盆地，它们的泛化效果更好。虽然较大的批量大小可以更准确地近似真实梯度，更新步骤中的波动较小，但它在计算上非常昂贵，并且更新步骤较少，而较小的批量大小由于在学习过程中引入的噪声提供了正则化效果。然而，这可能会导致梯度估计的方差较高，因此需要较小的学习率以保持稳定性并收敛到最优解。事实证明，大多数权重空间中的方向具有类似的损失值，因为
    Hessian 包含大量接近零的特征值，即使在随机初始化点上也是如此。
- en: '![Refer to caption](img/76bb7aa7409c943413efea0adde0327a.png)![Refer to caption](img/624cb4a7d1864bec84a696156483c9ba.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/76bb7aa7409c943413efea0adde0327a.png)![参见说明](img/624cb4a7d1864bec84a696156483c9ba.png)'
- en: 'Figure 2\. a. The $2-D$ visualization of the SGD trajectory. Source: Shervine
    ([b67,](#bib.bib67) ). b. Illustration of projected gradient descent where $w$
    is the current parameter estimate, $w^{\prime}$ is the update after a gradient
    step, and $PC(w^{\prime})$ projects this onto the constraint set $C$. Source:
    Ian Kevin P. Murphy ([b44,](#bib.bib44) )'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. a. $2-D$ SGD 轨迹的可视化。来源：Shervine ([b67,](#bib.bib67))。b. 投影梯度下降的示意图，其中
    $w$ 是当前的参数估计，$w^{\prime}$ 是经过梯度步骤后的更新，$PC(w^{\prime})$ 将其投影到约束集 $C$ 上。来源：Ian Kevin
    P. Murphy ([b44,](#bib.bib44))
- en: 1.4\. Linear Subspace
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4\. 线性子空间
- en: Goodfellow et al. ([b47,](#bib.bib47) ) show the existence of a linear subspace
    during neural network training with no barriers and a smooth path connecting initialization
    and the final minima. Also, they show that poor conditioning of the Hessian and
    variance in the gradient estimate are significant obstacles in the SGD training
    process. These could deviate the algorithm completely from the desired basin of
    attraction. Thus the final minima or the final region in the parameter space and
    the width of the final minima that SGD converges to depend largely on the learning
    rate, batch size, and gradient covariance, with different geometries and generalization
    properties.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Goodfellow 等人 ([b47,](#bib.bib47)) 证明了在神经网络训练过程中存在一个没有障碍的线性子空间，并且存在一条平滑的路径连接初始化和最终极小值。此外，他们还展示了
    Hessian 的较差条件和梯度估计的方差是 SGD 训练过程中的重要障碍。这些因素可能会使算法完全偏离预期的吸引盆地。因此，SGD 收敛到的最终极小值或参数空间中的最终区域及最终极小值的宽度在很大程度上取决于学习率、批量大小和梯度协方差，同时具有不同的几何特征和泛化属性。
- en: 1.5\. SGD Trajectory
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5\. SGD 轨迹
- en: Felix et al. ([b13,](#bib.bib13) ) constructed continuous paths between minima
    for recurrent neural networks and conjectured that the loss minima are not isolated
    points but essentially form a continuous manifold. Also, since these paths are
    flat, this implies the existence of a single connected manifold of low loss and
    not a distinct basin of attraction. More precisely, the part of the parameter
    space when the loss remains below a certain low threshold form a connected region
    of low error valleys as shown in Figure [3](#S1.F3 "Figure 3 ‣ 1.6\. SGD Analysis
    ‣ 1\. Introduction ‣ A survey of deep learning optimizers - first and second order
    methods"). These reflect the generalization capabilities of the network since
    this provides evidence that large neural nets have enough parameters to produce
    accurate predictions even though they undergo huge structural changes. This is
    quite a deviation from the current literature, where the minima are typically
    depicted as points at the bottom of a strictly convex valley of a certain width
    with network parameters given by the location of the minimum.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Felix 等人 ([b13,](#bib.bib13)) 构造了递归神经网络中极小值之间的连续路径，并推测这些损失极小值并不是孤立的点，而是基本上形成了一个连续的流形。此外，由于这些路径是平坦的，这意味着存在一个低损失的单一连通流形，而不是一个独立的吸引盆地。更精确地说，当损失保持在某个低阈值以下时，参数空间的部分区域形成了低错误谷的连通区域，如图
    [3](#S1.F3 "Figure 3 ‣ 1.6\. SGD Analysis ‣ 1\. Introduction ‣ A survey of deep
    learning optimizers - first and second order methods") 所示。这反映了网络的泛化能力，因为这提供了证据表明，即使在经历了巨大的结构变化后，大型神经网络也有足够的参数来产生准确的预测。这与当前文献中的观点有很大偏差，后者通常将极小值描述为在特定宽度的严格凸谷底部的点，网络参数由极小值的位置给出。
- en: 1.6\. SGD Analysis
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6\. SGD 分析
- en: 'Stanislaw et al. ([b40,](#bib.bib40) ) conjectured that the SGD process depends
    on the ratio of the learning rate to batch size, which thus determines the width
    of the endpoint and its generalization capabilities. First, as we approach the
    local minima, the loss surface can be approximated by a quadratic bowl, with the
    minimum at zero loss. Thus, the training can be approximated by an Ornstein-Unhlenbeck
    ²²2The Ornstein-Unhlenbeck $x_{t}$ process is a Gaussian process defined using
    the following differential equation: $dx_{t}=-\theta x_{t}dt+\sigma dW_{t}$. process
    as discussed in Section LABEL:Second-order_methods. Second, the covariance of
    the gradients $C=\frac{1}{N}\sum_{i=1}^{N}\left(g_{i}-g\right)^{T}\left(g_{i}-g\right)$,
    where $g_{i}$ is the gradient of the loss function $L$ with respect to the ($x_{i},y_{i}$)
    training example, and the Hessian of the loss function $H$ are approximately equal,
    and is possibly the reason SGD escapes the global minima. Since the loss functions
    of deep neural networks are typically non-convex, with complex structure and potentially
    multiple minima and saddle points, SGD generally converges to different regions
    of parameter space, with different geometries and generalization properties ([b2,](#bib.bib2)
    ; [b3,](#bib.bib3) ), depending on optimization hyper-parameters and initialization.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Stanislaw 等人 ([b40,](#bib.bib40) ) 推测，SGD 过程依赖于学习率与批量大小的比例，因此决定了终点的宽度及其泛化能力。首先，当我们接近局部极小值时，损失面可以近似为一个二次碗，最小值在零损失处。因此，训练可以近似为
    Ornstein-Unhlenbeck ²²2Ornstein-Unhlenbeck $x_{t}$ 过程是一个高斯过程，定义为以下微分方程：$dx_{t}=-\theta
    x_{t}dt+\sigma dW_{t}$。过程，如第 LABEL:Second-order_methods 节中讨论的。其次，梯度的协方差 $C=\frac{1}{N}\sum_{i=1}^{N}\left(g_{i}-g\right)^{T}\left(g_{i}-g\right)$，其中
    $g_{i}$ 是相对于 ($x_{i},y_{i}$) 训练示例的损失函数 $L$ 的梯度，而损失函数 $H$ 的 Hessian 近似相等，这可能是 SGD
    逃离全局极小值的原因。由于深度神经网络的损失函数通常是非凸的，具有复杂的结构，并可能具有多个极小值和鞍点，SGD 通常会收敛到参数空间的不同区域，具有不同的几何特征和泛化属性
    ([b2,](#bib.bib2) ; [b3,](#bib.bib3) )，这取决于优化超参数和初始化。
- en: '![Refer to caption](img/e9c6dd202b75c2b401fdeae108757fb0.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e9c6dd202b75c2b401fdeae108757fb0.png)'
- en: 'Figure 3\. Left: A slice through the one million-dimensional training loss
    function of DenseNet-40-12 on CIFAR10 and the minimum energy path as discussed
    in ([b13,](#bib.bib13) ). The plane is spanned by the two minima and the mean
    of the path nodes. Source: Felix Draxler ([b13,](#bib.bib13) )'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 左侧：DenseNet-40-12 在 CIFAR10 上的一百万维训练损失函数的切片以及最低能量路径，如([b13,](#bib.bib13)
    )中讨论的。该平面由两个极小值和路径节点的均值构成。来源：Felix Draxler ([b13,](#bib.bib13) )
- en: 1.7\. Curse of Dimensionality
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7\. 维度的诅咒
- en: Finding a good descent direction in a high-dimensional space is a difficult
    problem, but it is not nearly as difficult as navigating an error surface with
    complicated obstacles within multiple low-dimensional subspaces. It is also highly
    likely that the non-convex loss landscape is such that it contains regions of
    local minima at high energies which full-batch methods couldn’t have possibly
    avoided. But, the stochastic nature of SGD due to its inherent noise and a smaller
    batch size doesn’t suffer from this problem and is not trapped in irrelevant basins
    of attraction. With non-convex functions, such as neural nets, it is possible
    to have many local minima. Indeed, nearly any deep model is guaranteed to have
    an extremely large number of local minima. However, this is not necessarily a
    major problem. It is empirically observed that as the dimensionality $N$ increases,
    local minima with high error relative to the global minimum occur with a probability
    that is exponentially small in $N$ ([b5,](#bib.bib5) ). This is mainly because
    neural nets exhibit model identifiability problem, which results in an extremely
    large amount of local minima, but all are equivalent in their cost function value.
    They do not have very high error values relative to the global minima and are
    thus good solutions that stochastic gradient descent often finds itself at during
    the training process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维空间中找到良好的下降方向是一个困难的问题，但这远不如在多个低维子空间内具有复杂障碍的误差面上导航那样困难。非凸损失景观很可能包含高能量的局部最小值区域，而全批量方法可能无法避免这些区域。然而，由于其固有噪声和较小的批量大小，随机梯度下降的随机性质并不会受到这个问题的影响，也不会陷入无关的吸引盆地。对于非凸函数，如神经网络，可能会有许多局部最小值。实际上，几乎任何深度模型都保证有极大量的局部最小值。然而，这不一定是一个重大问题。经验上观察到，随着维度
    $N$ 的增加，相对于全局最小值的高误差局部最小值出现的概率是 $N$ 的指数级小 ([b5,](#bib.bib5))。这主要是因为神经网络表现出模型可识别性问题，导致极大量的局部最小值，但它们在成本函数值上都是等价的。它们相对于全局最小值没有非常高的误差值，因此是随机梯度下降在训练过程中经常找到的良好解决方案。
- en: 1.8\. Critical Points
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.8\. 临界点
- en: Critical points ³³3A critical point is a point whose first-order derivative
    is zero i.e., $f^{\prime}(x)=0$. with high costs are far more likely to be saddle
    points ([b5,](#bib.bib5) ). We are not exactly concerned with finding the exact
    minimum of a function but seek only to reduce its value significantly to obtain
    a good generalization error. In the gradient descent method, the main problem
    is not the direction but its step size along each eigenvector because the update
    step is directly proportional to the corresponding eigenvalue. Thus, we move towards
    the optimal point if the eigenvalue is positive else; we move away from it if
    it is negative along those directions, and this slows down the learning process
    since now we might circumnavigate a tall mountain or a flat region of high error
    or might simply take a very long time to move away from the saddle points.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 临界点³³3A临界点是其一阶导数为零的点，即 $f^{\prime}(x)=0$。具有高成本的点更可能是鞍点 ([b5,](#bib.bib5))。我们并不完全关注于找到函数的精确最小值，而只是寻求显著减少其值以获得良好的泛化误差。在梯度下降法中，主要问题不是方向，而是沿每个特征向量的步长，因为更新步长与对应的特征值成正比。因此，如果特征值为正，我们就朝向最优点移动；如果为负，则沿这些方向远离最优点，这会减缓学习过程，因为我们可能会绕过一个高误差的高山或平坦区域，或者可能需要很长时间才能远离鞍点。
- en: '![Refer to caption](img/5c7a4e001b8633c1a9d7fed821a4568a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5c7a4e001b8633c1a9d7fed821a4568a.png)'
- en: Figure 4\. For two dimensions, the lines of constant of the error surface E
    are oval in shape. The eigenvectors of H along the major and minor axes. The eigenvalues
    measure the steepness of E along each eigendirection. Source:Yann LeCun ([b1,](#bib.bib1)
    )
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 对于二维情况，误差面 E 的等值线呈椭圆形状。H 的特征向量沿主要和次要轴方向。特征值衡量 E 在每个特征方向上的陡峭程度。来源：Yann
    LeCun ([b1,](#bib.bib1))
- en: For a convex function, such a flat region corresponds to the global minima,
    but in a non-convex optimization problem, it could correspond to a point which
    a high value of the loss function. Although, Newton’s method solves this slowness
    issue by rescaling the gradients in each direction with the inverse of Hessian,
    it can often take the wrong direction and be applied iteratively as long as the
    Hessian remains positive definite. Thus for a locally quadratic function, Newton’s
    method jumps directly to the minimum, but if the eigenvalues are not all positive
    (near a saddle point), then the update can lead us to move in the wrong direction.
    Also, since second-order methods are computationally intensive, the computational
    complexity of computing the Hessian is $O\left(N^{3}\right)$, and thus, practically
    only networks with a small number of parameters can be trained via Newton’s method.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个凸函数，这样的平坦区域对应于全局最小值，但在非凸优化问题中，它可能对应于一个损失函数值较高的点。尽管 Newton 方法通过用 Hessian
    的逆矩阵重新缩放每个方向的梯度来解决这种缓慢性问题，但它往往会朝错误的方向前进，并且只要 Hessian 仍为正定矩阵，就会迭代应用。因此，对于局部二次函数，Newton
    方法直接跳到最小值，但如果特征值并非全部为正（接近鞍点），那么更新可能会导致我们朝错误的方向移动。此外，由于二阶方法计算量大，计算 Hessian 的时间复杂度为
    $O\left(N^{3}\right)$，因此实际上只有参数较少的网络可以通过 Newton 方法进行训练。
- en: Several methods such as BFGS and saddle-free newton method ([b5,](#bib.bib5)
    ; [b53,](#bib.bib53) ; [b54,](#bib.bib54) ) are proposed to overcome this problem
    in non-convex settings, specifically when the Hessian has negative eigenvalues.
    This includes adding a constant along the Hessian diagonal, which largely offsets
    the negative eigenvalues. Recently, Luke ([b48,](#bib.bib48) ) established the
    connections between gradient explosion and the initialization point by measuring
    the recurrent Jacobian. They found that with a random initialization, the neural
    network’s policy is poorly behaved and has many eigenvalues with a norm greater
    than length $1$, and thus the gradient norm grows. However, for a stable initialization
    (shrinks the initialization by multiplying by $0.01$) which was picked specifically
    to prevent the gradients from exploding, we find that many eigenvalues close to
    $1$, and thus the gradients do not explode. Sutskever et al. ([b9,](#bib.bib9)
    ) demonstrated that poorly initialized and the absence of momentum perform worse
    than well-initialized networks with momentum.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决在非凸设置中，特别是当 Hessian 矩阵具有负特征值时的问题，提出了几种方法，如 BFGS 和无鞍点 Newton 方法 ([b5,](#bib.bib5)
    ; [b53,](#bib.bib53) ; [b54,](#bib.bib54) )。这包括在 Hessian 对角线添加一个常数，这在很大程度上抵消了负特征值。最近，Luke
    ([b48,](#bib.bib48) ) 通过测量递归雅可比矩阵建立了梯度爆炸与初始化点之间的联系。他们发现，在随机初始化的情况下，神经网络的策略表现不佳，并且有许多特征值的范数大于
    $1$，因此梯度范数增长。然而，对于稳定初始化（通过乘以 $0.01$ 缩小初始化），这是特意选择以防止梯度爆炸，我们发现许多特征值接近 $1$，因此梯度没有爆炸。Sutskever
    等人 ([b9,](#bib.bib9) ) 证明了初始化不良且缺乏动量的网络表现不如良好初始化且带有动量的网络。
- en: '![Refer to caption](img/d569e215aa0fc74fb7f285f3c6738c5e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d569e215aa0fc74fb7f285f3c6738c5e.png)'
- en: 'Figure 5\. The loss surface represented as a union of $n$-dimensional manifolds
    called $n$-wedges. A model of the low-loss manifold comprising of $2$-wedges in
    a $3$-dimensional space. Source: Stanislav ([b8,](#bib.bib8) )'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 以 $n$-维流形的并集表示的损失面，被称为 $n$-楔。一个包含 $2$-楔的低损失流形模型，位于 $3$-维空间中。来源：Stanislav
    ([b8,](#bib.bib8) )
- en: 1.9\. Difficulties in Neural Network Optimization
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.9\. 神经网络优化中的困难
- en: The main challenges for neural net optimization in these high-dimensional spaces
    can be ill-conditioning of the Hessian, poor condition number, local minima, plateaus,
    and flat regions, the proliferation of saddle points, and poor correspondence
    between local and global structure. The existence of a large number of saddle
    points can significantly slow down learning since they are regions of high error
    plateaus and give the impression of the presence of a local minimum. Also, since
    these critical points are surrounded by plateaus of small curvature, second-order
    methods such as the Newton method are attracted to saddle points and thus slow
    down the learning process ([b5,](#bib.bib5) ). Also, a common observation is that
    at the end of training process, the norm of the gradients $\nabla f(x)$ is not
    necessarily small i.e., $\nabla f(x)<\epsilon$, and the Hessian $H$ has negative
    eigenvalues, indicating that the algorithm did not converge to the local minima
    ([b23,](#bib.bib23) ).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些高维空间中，神经网络优化面临的主要挑战包括 Hessian 矩阵的条件变差、条件数差、局部极小值、平台、平坦区域、鞍点的激增，以及局部结构和全局结构之间的对应关系差。大量鞍点的存在会显著减缓学习过程，因为它们是高误差平台的区域，并给人一种局部极小值存在的印象。此外，由于这些关键点被小曲率的平坦区域包围，二阶方法如牛顿法容易被鞍点吸引，从而减慢学习过程
    ([b5,](#bib.bib5) )。另外，一个常见的观察结果是，在训练过程结束时，梯度的范数 $\nabla f(x)$ 不一定很小，即 $\nabla
    f(x)<\epsilon$，并且 Hessian $H$ 具有负特征值，表明算法并未收敛到局部极小值 ([b23,](#bib.bib23) )。
- en: It is also possible that models with high capacity have a large number of local
    minima which can increase the number of interactions and cause the Hessian to
    be ill-conditioned. There are also wide, flat regions of constant value where
    the gradients and Hessian are all zero. Such degenerate locations pose a significant
    problem to optimization algorithms. We conjecture that large neural nets often
    converge to local minima, most of which are equivalent and yield similar generalization
    performance on the test set. Also, the probability of finding a high error local
    minima is exponentially small in high-dimensional structures and decreases quickly
    with network size.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 高容量的模型也可能存在大量的局部极小值，这会增加交互次数，并导致 Hessian 矩阵的条件变差。还有一些宽阔的平坦区域，其中梯度和 Hessian 都为零。这些退化点对优化算法构成了重大挑战。我们推测，大型神经网络往往会收敛到局部极小值，其中大多数是等效的，并在测试集上表现出相似的泛化性能。此外，在高维结构中找到高误差局部极小值的概率是指数级地小，并且随着网络规模的增大迅速减少。
- en: '![Refer to caption](img/8090437743c3e7e08907a760fbbb566f.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8090437743c3e7e08907a760fbbb566f.png)'
- en: 'Figure 6\. a. Batch gradient descent moves directly downhill. b. SGD takes
    steps in a noisy direction, but moves downhill on average. Source: Roger Grosse
    ([b21,](#bib.bib21) )'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. a. 批量梯度下降直接向下移动。b. SGD 在噪声方向上采取步骤，但平均而言仍然向下移动。来源：Roger Grosse ([b21,](#bib.bib21)
    )
- en: 2\. First Order Methods
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 一阶方法
- en: 2.1\. Momentum
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 动量
- en: Polyak et al. ([b49,](#bib.bib49) ) introduced the momentum method to accelerate
    the learning process of first-order gradient-based methods, especially in regions
    of high curvature. It also addresses the poor conditioning of the Hessian matrix
    and the variance in the stochastic gradient descent process. This algorithm computes
    an update step by accumulating an exponentially decaying moving average of the
    past gradients and continues to move in their direction. The momentum term increases
    for dimensions whose gradients point in the same directions and reduces updates
    for dimensions whose gradient change directions, and we gain a faster convergence.
    Although it can be applied to both full-batch and mini-batch learning methods,
    stochastic gradient descent combined with mini-batches and a momentum term has
    been the golden rule in the deep learning community to get excellent results on
    perception tasks. It dampens oscillations in directions of high curvature by combining
    gradients of opposite signs and also yields a larger learning rate in regions
    of low curvature. It builds but the speed in directions of consistent gradient
    and thus traverses a lot faster than the steepest descent due to its accumulated
    velocity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Polyak 等人 ([b49,](#bib.bib49) ) 引入了动量方法，以加速基于一阶梯度的方法的学习过程，特别是在高曲率区域。它还解决了Hessian矩阵的条件较差以及随机梯度下降过程中的方差问题。该算法通过累积过去梯度的指数衰减移动平均来计算更新步骤，并继续沿其方向移动。动量项对于梯度方向一致的维度会增加，对于梯度方向改变的维度则减少更新，从而实现更快的收敛。虽然它可以应用于全批次和小批次学习方法，但结合小批次和动量项的随机梯度下降已成为深度学习领域获得感知任务优秀结果的黄金法则。它通过结合相反符号的梯度来抑制高曲率方向的振荡，并在低曲率区域提供更大的学习率。它在一致梯度方向上积累速度，从而比最陡下降法更快地遍历。
- en: If the momentum is close to $1$, this algorithm is a lot faster, and it is equally
    essential to ensure a smaller momentum value (close to $0.5$) at the start of
    the learning process. In the beginning, since the points are randomly initialized
    in the weight space, there may be large gradients, and the velocity term ($v$)
    can lead in not being entirely beneficial. Thus, once these large gradients disappear
    and we find the desired basin of attraction, we can increase the momentum term
    smoothly to its final value, which is $0.9$ to $0.99$, to ensure that learning
    is not stuck and wastes computational time in traversing flat regions where the
    gradients are almost zero.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果动量接近$1$，这个算法会更快，同时在学习过程开始时确保较小的动量值（接近$0.5$）同样重要。一开始，由于点在权重空间中是随机初始化的，可能会出现较大的梯度，速度项($v$)可能不会完全有利。因此，一旦这些大梯度消失，并且我们找到期望的吸引盆地，我们可以将动量项平滑地增加到其最终值，即$0.9$到$0.99$，以确保学习不会卡住，并且不会在梯度几乎为零的平坦区域浪费计算时间。
- en: '|  | $\begin{array}[]{l}\boldsymbol{v}\leftarrow\alpha\boldsymbol{v}-\epsilon\nabla_{\theta}\left(\frac{1}{m}\sum_{i=1}^{m}L\left(\boldsymbol{f}\left(\boldsymbol{x}^{(i)};\boldsymbol{\theta}\right),\boldsymbol{y}^{(i)}\right)\right)\\
    \boldsymbol{\theta}\leftarrow\boldsymbol{\theta}+\boldsymbol{v}\end{array}$ |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{array}[]{l}\boldsymbol{v}\leftarrow\alpha\boldsymbol{v}-\epsilon\nabla_{\theta}\left(\frac{1}{m}\sum_{i=1}^{m}L\left(\boldsymbol{f}\left(\boldsymbol{x}^{(i)};\boldsymbol{\theta}\right),\boldsymbol{y}^{(i)}\right)\right)\\
    \boldsymbol{\theta}\leftarrow\boldsymbol{\theta}+\boldsymbol{v}\end{array}$ |  |'
- en: The momentum algorithm accelerated convergence to a local minimum, requiring
    fewer iterations than the steepest descent method, precisely by $\sqrt{R}$ times
    fewer iterations, where R is the condition number of the curvature at the local
    minimum. The velocity term plays the role of velocity as in physical analogy in
    which it accelerates the particle through the parameter space in reinforced directions,
    and thus the velocity vector is also known as imparting momentum to the particle.
    The hyperparameter determines how quickly the contributions of previous gradients
    exponentially decay. Previously in gradient descent, the step size was proportional
    to the norm of the gradient. Now, it depends on how large and aligned a sequence
    of gradients are. The step size is largest when many gradients point in exactly
    the same direction. We can think of the analogy of a ball rolling down a curved
    hill, and whenever it descends a steep part of the surface, it gathers speed and
    continues sliding in that direction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 动量算法加速了对局部最小值的收敛，比最速下降法所需的迭代次数少，具体来说，迭代次数减少了 $\sqrt{R}$ 倍，其中 R 是局部最小值的曲率条件数。速度项在物理类比中起到加速粒子在参数空间中按强化方向运动的作用，因此速度向量也被称为赋予粒子动量。超参数决定了之前梯度的贡献如何以指数形式衰减。在之前的梯度下降中，步长与梯度的范数成正比。现在，它取决于梯度序列的大小和对齐程度。当许多梯度完全指向相同方向时，步长最大。我们可以把它类比为一个球在曲折的山坡上滚动，每当它下滑到陡峭的部分，它就会加速，并继续沿着那个方向滑动。
- en: 2.2\. Nesterov Momentum
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. Nesterov 动量
- en: '![Refer to caption](img/b154551e56f6edb7719a500f62f7fffd.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b154551e56f6edb7719a500f62f7fffd.png)'
- en: 'Figure 7\. Nesterov momentum first makes a big jump in the direction of the
    previously accumulated gradient and then computes the gradient where you end up
    and thus makes a correction. Source: Geoffrey Hinton ([b29,](#bib.bib29) )'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. Nesterov 动量首先在之前累积的梯度方向上做一个大跳跃，然后在最终位置计算梯度，从而进行修正。来源：Geoffrey Hinton ([b29,](#bib.bib29))
- en: 'Sutskever ([b9,](#bib.bib9) ) introduced a variant of the classical momentum
    (CM) that aligns in line with the work of Nesterov’s accelerated gradient (NAG)
    method for optimizing convex functions. The update rule is given as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Sutskever ([b9,](#bib.bib9)) 引入了一种经典动量（CM）的变体，这种变体与 Nesterov 加速梯度（NAG）方法在优化凸函数方面的工作相一致。更新规则如下：
- en: '|  | $\boldsymbol{v}\leftarrow\alpha\boldsymbol{v}-\epsilon\nabla_{\boldsymbol{\theta}}\left[\frac{1}{m}\sum_{i=1}^{m}L\left(f\left(\boldsymbol{x}^{(i)};\boldsymbol{\theta}+\alpha\boldsymbol{v}\right),\boldsymbol{y}^{(i)}\right)\right]$
    |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{v}\leftarrow\alpha\boldsymbol{v}-\epsilon\nabla_{\boldsymbol{\theta}}\left[\frac{1}{m}\sum_{i=1}^{m}L\left(f\left(\boldsymbol{x}^{(i)};\boldsymbol{\theta}+\alpha\boldsymbol{v}\right),\boldsymbol{y}^{(i)}\right)\right]$
    |  |'
- en: '|  | $\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}+\boldsymbol{v}$ |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}+\boldsymbol{v}$ |  |'
- en: where $\alpha\in[0,1]$ is a hyperparameter that determines the contributions
    of previous gradients across multiple iterations. From the formula, it is clear
    that NAG is similar to CM⁴⁴4CM is the abbreviation for Classical Momentum as discussed
    in Section [2.1](#S2.SS1 "2.1\. Momentum ‣ 2\. First Order Methods ‣ A survey
    of deep learning optimizers - first and second order methods"). except where the
    gradient is evaluated. With Nesterov momentum, the gradient is evaluated after
    the current velocity is applied. This means that in NAG we first make a big jump
    in the direction of the previously accumulated gradient and then compute the gradient
    where you end up and thus make a correction. This gradient-based correction factor
    is vital for a stable gradient update, especially for higher values of µ. The
    gradient correction to the velocity responds much quicker in NAG, and if $\mu
    v_{t}$ is indeed a poor update and an inappropriate velocity, then NAG will point
    back towards $\theta_{t}$ more strongly than $\nabla f\left(\theta_{t}\right)$
    does, thus providing a larger and more timely correction to $v_{t}$ than CM. Therefore,
    it can avoid oscillations and is much more effective than CM along high-curvature
    vertical directions. Thus, it is more tolerant to larger values of µ than CM.
    For convex functions, Nesterov momentum achieves a convergence rate of $O\left(1/K^{2}\right)$
    where k is the number of steps.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha\in[0,1]$ 是一个超参数，决定了多次迭代中之前梯度的贡献。从公式中可以看出，NAG 类似于 CM⁴⁴4CM 是经典动量的缩写，如第
    [2.1](#S2.SS1 "2.1\. 动量 ‣ 2\. 一阶方法 ‣ 深度学习优化器调查 - 一阶和二阶方法")节所讨论。不同之处在于梯度的评估方式。使用
    Nesterov 动量时，梯度在当前速度应用后进行评估。这意味着在 NAG 中，我们首先沿着之前积累的梯度方向大步前进，然后在最终位置计算梯度，从而进行修正。这个基于梯度的修正因子对于稳定的梯度更新至关重要，尤其是在较高的
    µ 值下。梯度对速度的修正在 NAG 中响应更快，如果 $\mu v_{t}$ 确实是一个不佳的更新和不适当的速度，那么 NAG 将比 $\nabla f\left(\theta_{t}\right)$
    更强烈地指向 $\theta_{t}$，从而提供比 CM 更大、更及时的对 $v_{t}$ 的修正。因此，它可以避免振荡，并且在高曲率垂直方向上比 CM 更有效。因此，它对比
    CM 更能容忍较大的 µ 值。对于凸函数，Nesterov 动量实现了 $O\left(1/K^{2}\right)$ 的收敛率，其中 k 是步骤数。
- en: '![Refer to caption](img/288e186b96b59874cd4d8fb7bba0a894.png)![Refer to caption](img/1977ca78da4ca03aa28ac35ffe935d2c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/288e186b96b59874cd4d8fb7bba0a894.png)![参考说明](img/1977ca78da4ca03aa28ac35ffe935d2c.png)'
- en: 'Figure 8\. The effect of different learning rate $\epsilon$ and the momentum
    coefficients $\mu$ for various hyperparameters. (left) The plot represents the
    norm of the residual of the parameter value after projecting the parameters into
    a $1$-D subspace $\forall t$. (right) The divergence of SGD from the main linear
    path when trained using a maxout network on MNIST. Source: Goodfellow ([b35,](#bib.bib35)
    )'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 不同学习率 $\epsilon$ 和动量系数 $\mu$ 对各种超参数的影响。（左）图表示将参数投影到 $1$-D 子空间 $\forall
    t$ 后，参数值残差的范数。（右）使用 maxout 网络在 MNIST 上训练时 SGD 从主线性路径的偏离。来源：Goodfellow ([b35,](#bib.bib35)
    )
- en: 2.3\. Adaptive Learning Rates
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 自适应学习率
- en: The delta-bar-delta algorithm ([b50,](#bib.bib50) ) introduced the concept of
    having separate adaptive learning rates for each individual connection (weight)
    which is set empirically by observing its gradient after each iteration. The idea
    is that if the gradient stays consistent, i.e., remains the same sign, we tune
    up the learning rate; else if the gradient sign reverses, we decrease its learning
    rate. The intuition is that for deep neural nets, the appropriate learning rate
    varies widely for each of its weights. The magnitude of the gradients is often
    different for different layers, and if the weight initialization is small then
    the gradients are small for early layers than the later ones for very deep neural
    nets. We start with a local gain of $1$ for each weight and use small additive
    increases or multiplicative decreases depending on the sign of the gradient (mini-batch).
    This ensures that the big gains decay rapidly when the oscillations start. It
    is essential to ensure that we limit the gains to lie within a reasonable range
    and use bigger mini-batches to ensure that the sign of the gradient is not due
    to the sampling error of the mini-batches.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Delta-bar-delta 算法 ([b50,](#bib.bib50)) 引入了为每个单独的连接（权重）设置不同的自适应学习率的概念，该学习率通过在每次迭代后观察其梯度来经验性地确定。其思想是，如果梯度保持一致，即符号保持不变，则增加学习率；否则，如果梯度符号发生反转，则降低学习率。直观上，对于深度神经网络，每个权重的适当学习率差异很大。不同层的梯度幅度通常不同，如果权重初始化较小，则早期层的梯度通常小于较深层的梯度。我们从每个权重的局部增益为$1$开始，并根据梯度的符号（小批量）使用小的加性增加或乘法减少。这确保了在震荡开始时，大的增益迅速衰减。必须确保我们将增益限制在合理范围内，并使用更大的小批量，以确保梯度的符号不是由于小批量的采样误差。
- en: Jacobs ([b50,](#bib.bib50) ) proposed a method for combining adaptive learning
    rates with momentum. He conjectures that instead of using the sign agreement between
    the current gradient and the previous gradient, we use the current gradient and
    the velocity of the weight and thus combine the advantages of momentum and adaptive
    learning rates. Since momentum doesn’t care about axis-aligned effects, it can
    deal with diagonal ellipses and traverse in diagonal directions quickly.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Jacobs ([b50,](#bib.bib50)) 提出了将自适应学习率与动量结合的方法。他猜测，与其使用当前梯度和前一个梯度之间的符号一致性，不如使用当前梯度和权重的速度，从而结合动量和自适应学习率的优点。由于动量不关心轴对齐效应，它可以处理对角椭圆并在对角方向上快速遍历。
- en: 2.4\. Adagrad
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. Adagrad
- en: The Adagrad ([b51,](#bib.bib51) ) algorithm individually adapts the learning
    rates of all model parameters by scaling them inversely proportional to the square
    root of the sum of the historical squared values of the gradient.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad ([b51,](#bib.bib51)) 算法通过将所有模型参数的学习率缩放为与梯度历史平方值的和的平方根成反比来实现自适应。
- en: '|  | $\boldsymbol{g}\leftarrow\frac{1}{m}\nabla_{\boldsymbol{\theta}}\sum_{i}L\left(f\left(\boldsymbol{x}^{(i)};\boldsymbol{\theta}\right),\boldsymbol{y}^{(i)}\right)$
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{g}\leftarrow\frac{1}{m}\nabla_{\boldsymbol{\theta}}\sum_{i}L\left(f\left(\boldsymbol{x}^{(i)};\boldsymbol{\theta}\right),\boldsymbol{y}^{(i)}\right)$
    |  |'
- en: '|  | $\boldsymbol{r}\leftarrow\boldsymbol{r}+\boldsymbol{g}\odot\boldsymbol{g}$
    |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{r}\leftarrow\boldsymbol{r}+\boldsymbol{g}\odot\boldsymbol{g}$
    |  |'
- en: 'For weight parameter with the largest gradient has the largest decrease in
    its learning rate, while the parameter with the smallest gradient has the smallest
    decrease in its learning rate. This results in faster progress in the more gently
    sloped regions of parameter space and works well when the gradient is sparse.
    For non-convex optimization problems, accumulating the squared gradients can result
    in a premature and excessive decrease in the learning rate. The parameter update
    is given as:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于梯度最大的权重参数，其学习率下降幅度最大，而梯度最小的参数，其学习率下降幅度最小。这使得在参数空间的坡度较缓的区域进展更快，并且在梯度稀疏时表现良好。对于非凸优化问题，累积平方梯度可能会导致学习率过早和过度下降。参数更新公式为：
- en: '|  | $\Delta\boldsymbol{\theta}=-\frac{\epsilon}{\sqrt{\delta+\boldsymbol{r}}}\odot\boldsymbol{g}$
    |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta\boldsymbol{\theta}=-\frac{\epsilon}{\sqrt{\delta+\boldsymbol{r}}}\odot\boldsymbol{g}$
    |  |'
- en: 2.5\. RMSProp
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. RMSProp
- en: The RMSProp algorithm ([b29,](#bib.bib29) ) modifies the Adagrad by changing
    the accumulated squared gradient into an exponentially weighted moving average.
    In the non-convex setting, since Adagrad decreases the learning rate rapidly by
    using the history of the squared gradient, it is possible that the learning halts
    before arriving at the desired basin of attraction or locally convex bowl structure.
    RMSProp uses an exponentially decaying average of the squared gradients to discard
    the past history and can converge rapidly after finding a convex bowl it behaves
    like an instance of the AdaGrad algorithm since the AdaGrad method is designed
    to converge rapidly for a convex function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: RMSProp 算法 ([b29,](#bib.bib29)) 通过将累积的平方梯度改为指数加权移动平均来修改 Adagrad。在非凸设置中，由于 Adagrad
    使用平方梯度的历史迅速降低学习率，因此可能会在到达期望的吸引盆地或局部凸碗结构之前停止学习。RMSProp 使用平方梯度的指数衰减平均来丢弃过去的历史，并且在找到一个凸碗后可以迅速收敛，因为
    AdaGrad 方法旨在对凸函数快速收敛，所以它的行为类似于 AdaGrad 算法的一个实例。
- en: '|  | $\boldsymbol{r}\leftarrow\rho\boldsymbol{r}+(1-\rho)\boldsymbol{g}\odot\boldsymbol{g}\vspace{0.5em}$
    |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{r}\leftarrow\rho\boldsymbol{r}+(1-\rho)\boldsymbol{g}\odot\boldsymbol{g}\vspace{0.5em}$
    |  |'
- en: If the eigenvectors of the Hessian are axis-aligned, then RMSprop can correct
    the curvature. Since RMSProp lacks the bias-correction term, it can often lead
    to large step sizes and divergence with sparse gradients. If the eigenvectors
    of the Hessian are axis-aligned (dubious assumption), then RMSProp can correct
    the curvature.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Hessian 的特征向量是轴对齐的，则 RMSprop 可以修正曲率。由于 RMSProp 缺乏偏差修正项，它通常会导致较大的步长和稀疏梯度的发散。如果
    Hessian 的特征向量是轴对齐的（这一假设值得怀疑），则 RMSProp 可以修正曲率。
- en: 2.6\. Adam
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6\. Adam
- en: Adam ([b18,](#bib.bib18) ) (adaptive moments) aligns along the works of RMSProp
    ([b29,](#bib.bib29) ) and momentum with a few distinctions, such as invariance
    to the diagonal rescaling of the gradients. First, in Adam, we incorporate momentum
    by computing the biased first-order moment of the gradient as an exponentially
    decaying moving average. Second, similar to the RMSProp algorithm, we incorporate
    the second-order moment of the gradient as an exponentially decaying average.
    This combination has theoretical guarantees in convex settings. Since the moving
    averages are initialized to a vector of all zeros, the moment estimates are biased
    during zero during the initial steps especially when the biased first moment ($s$)
    and and second moment ($r$) estimates are close to $1$. ⁵⁵5$\odot$ corresponds
    to element-wise product.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Adam ([b18,](#bib.bib18))（自适应矩）在 RMSProp ([b29,](#bib.bib29)) 和动量的基础上进行了调整，具有一些区别，例如对梯度的对角线重缩放的不可变性。首先，在
    Adam 中，我们通过计算梯度的偏置一阶矩作为指数衰减的移动平均来结合动量。其次，类似于 RMSProp 算法，我们将梯度的二阶矩结合为指数衰减的平均值。这种组合在凸设置中具有理论保证。由于移动平均值初始化为全零的向量，因此在初始步骤期间，特别是当偏置一阶矩（$s$）和二阶矩（$r$）估计值接近
    $1$ 时，矩估计值会有偏差。⁵⁵5$\odot$ 对应于逐元素乘积。
- en: '|  | $\boldsymbol{s}\leftarrow\rho_{1}\boldsymbol{s}+\left(1-\rho_{1}\right)\boldsymbol{g};\hskip
    10.00002pt\boldsymbol{r}\leftarrow\rho_{2}\boldsymbol{r}+\left(1-\rho_{2}\right)\boldsymbol{g}\vspace{0.5em}$
    |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{s}\leftarrow\rho_{1}\boldsymbol{s}+\left(1-\rho_{1}\right)\boldsymbol{g};\hskip
    10.00002pt\boldsymbol{r}\leftarrow\rho_{2}\boldsymbol{r}+\left(1-\rho_{2}\right)\boldsymbol{g}\vspace{0.5em}$
    |  |'
- en: Thus, we introduce bias corrections to these estimates to account for their
    initialization for both the momentum term and the second-order moment (uncentered
    variance).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们引入偏差修正来考虑动量项和二阶矩（未中心化方差）的初始化。
- en: '|  | $\hat{\boldsymbol{s}}\leftarrow\frac{\boldsymbol{s}}{1-\rho_{1}^{t}};\hskip
    10.00002pt\hat{\boldsymbol{r}}\leftarrow\frac{\boldsymbol{r}}{1-\rho_{r}^{t}}$
    |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\boldsymbol{s}}\leftarrow\frac{\boldsymbol{s}}{1-\rho_{1}^{t}};\hskip
    10.00002pt\hat{\boldsymbol{r}}\leftarrow\frac{\boldsymbol{r}}{1-\rho_{r}^{t}}$
    |  |'
- en: Since RMSProp lacks the correction factor, it has a high-bias in the early stages
    of training, while Adam is robust to the choice of hyperparameters. Since the
    accumulated squared values of the gradients term are an approximation to the diagonal
    of the Fisher information matrix, it is more adaptive and leads to faster convergence.
    The parameter update is given as:⁶⁶6In Adam optimization, $\delta$ corresponds
    to a small constant of the order of $10^{-6}$ to ensure numerical stability.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RMSProp缺少校正因子，它在训练早期有较高的偏差，而Adam对超参数的选择具有鲁棒性。由于梯度平方值的累积项是Fisher信息矩阵对角线的近似值，它更具适应性，并且收敛速度更快。参数更新公式为：⁶⁶6在Adam优化中，$\delta$
    对应于一个约为 $10^{-6}$ 的小常数，以确保数值稳定性。
- en: '|  | $\Delta\boldsymbol{\theta}=-\epsilon\frac{\hat{\boldsymbol{s}}}{\sqrt{\hat{\boldsymbol{r}}}+\delta}$
    |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta\boldsymbol{\theta}=-\epsilon\frac{\hat{\boldsymbol{s}}}{\sqrt{\hat{\boldsymbol{r}}}+\delta}$
    |  |'
- en: 3\. Second-order methods
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 二阶方法
- en: '![Refer to caption](img/bd1b5f2ffa57fd509664abf7405f8cbb.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/bd1b5f2ffa57fd509664abf7405f8cbb.png)'
- en: Figure 9\. Illustration of the gradient descent and Newton’s method for a quadratic
    function. Newton’s method converges in a single step (direct route) by exploiting
    the curvature information contained in the Hessian matrix.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 描述了二次函数的梯度下降和牛顿方法。牛顿方法通过利用Hessian矩阵中的曲率信息在单步内收敛（直接路径）。
- en: 3.1\. Preliminaries and Motivation
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 前提条件与动机
- en: 'For a scalar-valued function, the matrix containing the second-order derivatives
    is the Hessian matrix ($H$). The Hessian $H$ is a measure of curvature or concavity
    of the function as shown in Figure [10](#S3.F10 "Figure 10 ‣ 3.1\. Preliminaries
    and Motivation ‣ 3\. Second-order methods ‣ A survey of deep learning optimizers
    - first and second order methods"). The matrix containing all the first-order
    derivatives for vector-valued functions $f:R^{n}\rightarrow R^{m}$ is known as
    the Jacobian matrix ($J$). Suppose $f:R^{n}\rightarrow R$ (input is a vector and
    output is scalar), then the Hessian matrix $H$ and the Jacobian $J$ for $f$ is
    given as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标量值函数，包含二阶导数的矩阵称为Hessian矩阵（$H$）。Hessian $H$ 衡量了函数的曲率或凹凸性，如图 [10](#S3.F10 "图
    10 ‣ 3.1\. 前提条件与动机 ‣ 3\. 二阶方法 ‣ 深度学习优化器概述 - 一阶与二阶方法") 所示。对于向量值函数 $f:R^{n}\rightarrow
    R^{m}$，包含所有一阶导数的矩阵称为Jacobian矩阵（$J$）。假设 $f:R^{n}\rightarrow R$（输入为向量，输出为标量），那么Hessian矩阵
    $H$ 和Jacobian $J$ 为：
- en: '|  | <math   alttext="\mathbf{J}=\begin{bmatrix}\nabla^{\mathrm{T}}f_{1}\\
    \vdots\\'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathbf{J}=\begin{bmatrix}\nabla^{\mathrm{T}}f_{1}\\
    \vdots\\'
- en: \nabla^{\mathrm{T}}f_{m}\end{bmatrix}=\begin{bmatrix}\frac{\partial f_{1}}{\partial
    x_{1}}&amp;\cdots&amp;\frac{\partial f_{1}}{\partial x_{n}}\\
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: \nabla^{\mathrm{T}}f_{m}\end{bmatrix}=\begin{bmatrix}\frac{\partial f_{1}}{\partial
    x_{1}}&amp;\cdots&amp;\frac{\partial f_{1}}{\partial x_{n}}\\
- en: \vdots&amp;\ddots&amp;\vdots\\
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&amp;\ddots&amp;\vdots\\
- en: \frac{\partial f_{m}}{\partial x_{1}}&amp;\cdots&amp;\frac{\partial f_{m}}{\partial
    x_{n}}\end{bmatrix}\quad\mathbf{H}=\begin{bmatrix}\frac{\partial^{2}f}{\partial
    x_{1}^{2}}&amp;\frac{\partial^{2}f}{\partial x_{1}\partial x_{2}}&amp;\cdots&amp;\frac{\partial^{2}f}{\partial
    x_{1}\partial x_{n}}\\
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: \frac{\partial f_{m}}{\partial x_{1}}&amp;\cdots&amp;\frac{\partial f_{m}}{\partial
    x_{n}}\end{bmatrix}\quad\mathbf{H}=\begin{bmatrix}\frac{\partial^{2}f}{\partial
    x_{1}^{2}}&amp;\frac{\partial^{2}f}{\partial x_{1}\partial x_{2}}&amp;\cdots&amp;\frac{\partial^{2}f}{\partial
    x_{1}\partial x_{n}}\\
- en: \frac{\partial^{2}f}{\partial x_{2}\partial x_{1}}&amp;\frac{\partial^{2}f}{\partial
    x_{2}^{2}}&amp;\cdots&amp;\frac{\partial^{2}f}{\partial x_{2}\partial x_{n}}\\
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: \frac{\partial^{2}f}{\partial x_{2}\partial x_{1}}&amp;\frac{\partial^{2}f}{\partial
    x_{2}^{2}}&amp;\cdots&amp;\frac{\partial^{2}f}{\partial x_{2}\partial x_{n}}\\
- en: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
- en: \frac{\partial^{2}f}{\partial x_{n}\partial x_{1}}&amp;\frac{\partial^{2}f}{\partial
    x_{n}\partial x_{2}}&amp;\cdots&amp;\frac{\partial^{2}f}{\partial x_{n}^{2}}\end{bmatrix}\vspace{1em}"
    display="block"><semantics ><mrow  ><mrow ><mi >𝐉</mi><mo  >=</mo><mrow ><mo >[</mo><mtable
    displaystyle="true" rowspacing="0pt" ><mtr ><mtd  ><mrow ><msup ><mo  >∇</mo><mi
    mathvariant="normal"  >T</mi></msup><msub ><mi >f</mi><mn >1</mn></msub></mrow></mtd></mtr><mtr
    ><mtd ><mi mathvariant="normal" >⋮</mi></mtd></mtr><mtr ><mtd  ><mrow ><msup ><mo  >∇</mo><mi
    mathvariant="normal"  >T</mi></msup><msub ><mi >f</mi><mi >m</mi></msub></mrow></mtd></mtr></mtable><mo
    >]</mo></mrow><mo >=</mo><mrow  ><mo >[</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><mo
    rspace="0em" >∂</mo><msub ><mi >f</mi><mn >1</mn></msub></mrow><mrow ><mo rspace="0em"
    >∂</mo><msub ><mi >x</mi><mn >1</mn></msub></mrow></mfrac></mstyle></mtd><mtd
    ><mi mathvariant="normal" >⋯</mi></mtd><mtd ><mstyle displaystyle="false" ><mfrac
    ><mrow  ><mo rspace="0em"  >∂</mo><msub ><mi >f</mi><mn >1</mn></msub></mrow><mrow
    ><mo rspace="0em" >∂</mo><msub ><mi >x</mi><mi >n</mi></msub></mrow></mfrac></mstyle></mtd></mtr><mtr
    ><mtd ><mi mathvariant="normal" >⋮</mi></mtd><mtd ><mi mathvariant="normal" >⋱</mi></mtd><mtd
    ><mi mathvariant="normal" >⋮</mi></mtd></mtr><mtr ><mtd  ><mstyle displaystyle="false"  ><mfrac
    ><mrow ><mo rspace="0em" >∂</mo><msub ><mi >f</mi><mi >m</mi></msub></mrow><mrow
    ><mo rspace="0em" >∂</mo><msub ><mi >x</mi><mn >1</mn></msub></mrow></mfrac></mstyle></mtd><mtd
    ><mi mathvariant="normal" >⋯</mi></mtd><mtd ><mstyle displaystyle="false" ><mfrac
    ><mrow  ><mo rspace="0em"  >∂</mo><msub ><mi >f</mi><mi >m</mi></msub></mrow><mrow
    ><mo rspace="0em" >∂</mo><msub ><mi >x</mi><mi >n</mi></msub></mrow></mfrac></mstyle></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow><mrow ><mi >𝐇</mi><mo  >=</mo><mrow ><mo >[</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  ><mstyle displaystyle="false"  ><mfrac
    ><mrow ><msup ><mo >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow ><mo rspace="0em"
    >∂</mo><msubsup ><mi >x</mi><mn >1</mn><mn >2</mn></msubsup></mrow></mfrac></mstyle></mtd><mtd
    ><mstyle displaystyle="false" ><mfrac ><mrow  ><msup ><mo >∂</mo><mn >2</mn></msup><mi
    >f</mi></mrow><mrow ><mo rspace="0em" >∂</mo><mrow ><msub ><mi >x</mi><mn >1</mn></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo rspace="0em"  >∂</mo><msub ><mi >x</mi><mn
    >2</mn></msub></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd ><mi mathvariant="normal"
    >⋯</mi></mtd><mtd ><mstyle displaystyle="false" ><mfrac ><mrow  ><msup ><mo >∂</mo><mn
    >2</mn></msup><mi >f</mi></mrow><mrow ><mo rspace="0em" >∂</mo><mrow ><msub ><mi
    >x</mi><mn >1</mn></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo rspace="0em"  >∂</mo><msub
    ><mi >x</mi><mi >n</mi></msub></mrow></mrow></mrow></mfrac></mstyle></mtd></mtr><mtr
    ><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><msup ><mo >∂</mo><mn >2</mn></msup><mi
    >f</mi></mrow><mrow ><mo rspace="0em" >∂</mo><mrow ><msub ><mi >x</mi><mn >2</mn></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo rspace="0em"  >∂</mo><msub ><mi >x</mi><mn
    >1</mn></msub></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd ><mstyle displaystyle="false"
    ><mfrac ><mrow  ><msup ><mo >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow ><mo
    rspace="0em" >∂</mo><msubsup ><mi >x</mi><mn >2</mn><mn >2</mn></msubsup></mrow></mfrac></mstyle></mtd><mtd
    ><mi mathvariant="normal" >⋯</mi></mtd><mtd ><mstyle displaystyle="false" ><mfrac
    ><mrow  ><msup ><mo >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow ><mo rspace="0em"
    >∂</mo><mrow ><msub ><mi >x</mi><mn >2</mn></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo rspace="0em"  >∂</mo><msub ><mi >x</mi><mi >n</mi></msub></mrow></mrow></mrow></mfrac></mstyle></mtd></mtr><mtr
    ><mtd ><mi mathvariant="normal" >⋮</mi></mtd><mtd ><mi mathvariant="normal" >⋮</mi></mtd><mtd
    ><mi mathvariant="normal" >⋱</mi></mtd><mtd ><mi mathvariant="normal" >⋮</mi></mtd></mtr><mtr
    ><mtd  ><mstyle displaystyle="false"  ><mfrac ><mrow ><msup ><mo >∂</mo><mn >2</mn></msup><mi
    >f</mi></mrow><mrow ><mo rspace="0em" >∂</mo><mrow ><msub ><mi >x</mi><mi >n</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo rspace="0em"  >∂</mo><msub ><mi >x</mi><mn
    >1</mn></msub></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd ><mstyle displaystyle="false"
    ><mfrac ><mrow  ><msup ><mo >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow ><mo
    rspace="0em" >∂</mo><mrow ><msub ><mi >x</mi><mi >n</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo rspace="0em"  >∂</mo><msub ><mi >x</mi><mn >2</mn></msub></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd
    ><mi mathvariant="normal" >⋯</mi></mtd><mtd ><mstyle displaystyle="false" ><mfrac
    ><mrow  ><msup ><mo >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow ><mo rspace="0em"
    >∂</mo><msubsup ><mi >x</mi><mi >n</mi><mn >2</mn></msubsup></mrow></mfrac></mstyle></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply
    ><csymbol cd="ambiguous"  >formulae-sequence</csymbol><apply ><apply ><ci  >𝐉</ci><apply
    ><csymbol cd="latexml" >matrix</csymbol><matrix ><matrixrow  ><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >∇</ci><ci >T</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑓</ci><cn type="integer" >1</cn></apply></apply></matrixrow><matrixrow
    ><ci >⋮</ci></matrixrow><matrixrow ><apply  ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >∇</ci><ci >T</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑓</ci><ci >𝑚</ci></apply></apply></matrixrow></matrix></apply></apply><apply
    ><apply ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><apply  ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑓</ci><cn type="integer"  >1</cn></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><cn type="integer"  >1</cn></apply></apply></apply><ci
    >⋯</ci><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑓</ci><cn type="integer" >1</cn></apply></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply></apply></apply></matrixrow><matrixrow
    ><ci >⋮</ci><ci  >⋱</ci><ci >⋮</ci></matrixrow><matrixrow ><apply ><apply  ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑓</ci><ci >𝑚</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><cn type="integer"  >1</cn></apply></apply></apply><ci
    >⋯</ci><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑓</ci><ci >𝑚</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><ci >𝑛</ci></apply></apply></apply></matrixrow></matrix></apply></apply></apply><apply
    ><ci >𝐇</ci><apply  ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow
    ><apply  ><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><cn type="integer"  >1</cn></apply><cn
    type="integer"  >2</cn></apply></apply></apply><apply ><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><cn type="integer" >2</cn></apply><ci >𝑓</ci></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><cn type="integer"
    >1</cn></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><cn type="integer" >2</cn></apply></apply></apply></apply></apply><ci >⋯</ci><apply
    ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer"
    >2</cn></apply><ci >𝑓</ci></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑥</ci><cn type="integer" >1</cn></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply></apply></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><cn type="integer"  >2</cn></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><cn type="integer"  >1</cn></apply></apply></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer"
    >2</cn></apply><ci >𝑓</ci></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><cn type="integer" >2</cn></apply><cn
    type="integer" >2</cn></apply></apply></apply><ci >⋯</ci><apply ><apply  ><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><cn type="integer"  >2</cn></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑛</ci></apply></apply></apply></apply></apply></matrixrow><matrixrow
    ><ci >⋮</ci><ci  >⋮</ci><ci >⋱</ci><ci >⋮</ci></matrixrow><matrixrow ><apply  ><apply
    ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑛</ci></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><cn type="integer"  >1</cn></apply></apply></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer"
    >2</cn></apply><ci >𝑓</ci></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑥</ci><cn type="integer" >2</cn></apply></apply></apply></apply></apply><ci
    >⋯</ci><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn
    type="integer" >2</cn></apply><ci >𝑓</ci></apply><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><ci >𝑛</ci></apply><cn type="integer" >2</cn></apply></apply></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{J}=\begin{bmatrix}\nabla^{\mathrm{T}}f_{1}\\
    \vdots\\ \nabla^{\mathrm{T}}f_{m}\end{bmatrix}=\begin{bmatrix}\frac{\partial f_{1}}{\partial
    x_{1}}&\cdots&\frac{\partial f_{1}}{\partial x_{n}}\\ \vdots&\ddots&\vdots\\ \frac{\partial
    f_{m}}{\partial x_{1}}&\cdots&\frac{\partial f_{m}}{\partial x_{n}}\end{bmatrix}\quad\mathbf{H}=\begin{bmatrix}\frac{\partial^{2}f}{\partial
    x_{1}^{2}}&\frac{\partial^{2}f}{\partial x_{1}\partial x_{2}}&\cdots&\frac{\partial^{2}f}{\partial
    x_{1}\partial x_{n}}\\ \frac{\partial^{2}f}{\partial x_{2}\partial x_{1}}&\frac{\partial^{2}f}{\partial
    x_{2}^{2}}&\cdots&\frac{\partial^{2}f}{\partial x_{2}\partial x_{n}}\\ \vdots&\vdots&\ddots&\vdots\\
    \frac{\partial^{2}f}{\partial x_{n}\partial x_{1}}&\frac{\partial^{2}f}{\partial
    x_{n}\partial x_{2}}&\cdots&\frac{\partial^{2}f}{\partial x_{n}^{2}}\end{bmatrix}\vspace{1em}</annotation></semantics></math>
    |  |
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \(\mathbf{J}=\begin{bmatrix}\nabla^{\mathrm{T}}f_{1}\\ \vdots\\ \nabla^{\mathrm{T}}f_{m}\end{bmatrix}=\begin{bmatrix}\frac{\partial
    f_{1}}{\partial x_{1}}&\cdots&\frac{\partial f_{1}}{\partial x_{n}}\\ \vdots&\ddots&\vdots\\
    \frac{\partial f_{m}}{\partial x_{1}}&\cdots&\frac{\partial f_{m}}{\partial x_{n}}\end{bmatrix}\quad\mathbf{H}=\begin{bmatrix}\frac{\partial^{2}f}{\partial
    x_{1}^{2}}&\frac{\partial^{2}f}{\partial x_{1}\partial x_{2}}&\cdots&\frac{\partial^{2}f}{\partial
    x_{1}\partial x_{n}}\\ \frac{\partial^{2}f}{\partial x_{2}\partial x_{1}}&\frac{\partial^{2}f}{\partial
    x_{2}^{2}}&\cdots&\frac{\partial^{2}f}{\partial x_{2}\partial x_{n}}\\ \vdots&\vdots&\ddots&\vdots\\
    \frac{\partial^{2}f}{\partial x_{n}\partial x_{1}}&\frac{\partial^{2}f}{\partial
    x_{n}\partial x_{2}}&\cdots&\frac{\partial^{2}f}{\partial x_{n}^{2}}\end{bmatrix}\vspace{1em}
- en: 'The Hessian matrix $H$ is real and symmetric with $H_{ij}=H_{ji}$. Likewise,
    the condition number of the Hessian is defined as the ratio of the largest eigenvalue
    to the smallest eigenvalue, i.e., is the ratio of the steepest ridge’s steepness
    to the shallowest ridge’s steepness. Since the Hessian is symmetric with real
    eigenvalues, and the eigenvectors have an orthogonal basis its eigendecomposition
    of the matrix is given as:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian 矩阵 $H$ 是实对称的，具有 $H_{ij}=H_{ji}$。同样，Hessian 的条件数定义为最大特征值与最小特征值的比率，即最陡峭的山脊的陡度与最平缓的山脊的陡度之比。由于
    Hessian 是对称的且具有实特征值，并且特征向量具有正交基，其矩阵的特征分解为：
- en: '|  | $\boldsymbol{H}=\boldsymbol{Q}\operatorname{diag}(\boldsymbol{\lambda})\boldsymbol{Q}^{-1}$
    |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{H}=\boldsymbol{Q}\operatorname{diag}(\boldsymbol{\lambda})\boldsymbol{Q}^{-1}$
    |  |'
- en: where $Q$ is an orthogonal matrix ($Q^{T}Q=I$) with one eigenvector per column
    and $\lambda$ is a diagonal matrix. In higher dimensions, the eigendecomposition
    of the Hessian can be used to test the critical point. The point is a local minimum
    when $H$ is positive definite (all $\lambda_{i}>0$). Likewise, when $H$ is negative
    definite (all $\lambda_{i}$ ¡ 0), the point is a local maximum. A saddle point
    has both positive and negative eigenvalues, where it is a local minima across
    one cross-section and a local maxima within another cross-section.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q$ 是一个正交矩阵 ($Q^{T}Q=I$)，每列有一个特征向量，$\lambda$ 是一个对角矩阵。在更高维度中，Hessian 的特征分解可用于测试临界点。当
    $H$ 是正定的（所有 $\lambda_{i}>0$）时，该点是局部最小值。同样，当 $H$ 是负定的（所有 $\lambda_{i}<0$）时，该点是局部最大值。鞍点具有正负特征值，在一个截面上是局部最小值，而在另一个截面上是局部最大值。
- en: 'Second-order optimization methods ([b30,](#bib.bib30) ; [b34,](#bib.bib34)
    ; [b52,](#bib.bib52) ) utilize the second derivatives for weight update. Newton’s
    method arises naturally from the second-order Taylor series approximation of the
    function $f$, ignoring the higher-order derivatives is given as:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶优化方法 ([b30,](#bib.bib30) ; [b34,](#bib.bib34) ; [b52,](#bib.bib52)) 利用二阶导数进行权重更新。牛顿法自然源于函数
    $f$ 的二阶泰勒级数近似，忽略了高阶导数，其形式为：
- en: '|  | $f(x)\approx f(x_{0})+(x-x_{0}^{\top})\nabla_{x}f(x_{0})+\frac{1}{2}(x-x_{0})^{\top}H(x-x_{0})$
    |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(x)\approx f(x_{0})+(x-x_{0}^{\top})\nabla_{x}f(x_{0})+\frac{1}{2}(x-x_{0})^{\top}H(x-x_{0})$
    |  |'
- en: 'where $H$ is the Hessian of $f$ with respect to $x$ evaluated at $x_{0}$. Solving
    for the critical point $x^{*}$ we get the Newton update rule:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H$ 是相对于 $x$ 的 $f$ 在 $x_{0}$ 处的 Hessian。求解临界点 $x^{*}$，我们得到牛顿更新规则：
- en: '|  | $x^{*}=x_{0}-H(f)(x_{0})^{-1}\nabla_{x}f(x_{0})$ |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $x^{*}=x_{0}-H(f)(x_{0})^{-1}\nabla_{x}f(x_{0})$ |  |'
- en: '![Refer to caption](img/deccfc2c2e9777062d436d84334436c3.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/deccfc2c2e9777062d436d84334436c3.png)'
- en: 'Figure 10\. A quadratic function with various curvature. The dashed line indicates
    the expected descent direction and the value of the cost function downhill performed
    by gradient descent. With negative curvature, the cost function decreases faster
    than the gradient predicts, while with positive curvature, the function decreases
    much more slowly than expected and thus begins to increase. With no curvature,
    the gradient correctly predicts the decrease. Source: Ian Goodfellow ([b23,](#bib.bib23)
    )'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 各种曲率的二次函数。虚线表示预期的下降方向和梯度下降所执行的成本函数的下坡值。具有负曲率时，成本函数下降比梯度预测的速度更快，而具有正曲率时，函数下降的速度远低于预期，因此开始上升。在没有曲率的情况下，梯度正确地预测了下降。来源：Ian
    Goodfellow ([b23,](#bib.bib23))
- en: Convex functions have strong theoretical guarantees because they are well-behaved
    and have nice properties. They lack saddle points, and all their local minima
    correspond to the global minima specifically because the Hessian $H$ is positive
    semi-definite. For a quadratic function, the Hessian $H$ is constant and thus
    well-conditioned. A detailed analysis for a quadratic convex function is discussed
    in Section [3.2](#S3.SS2 "3.2\. Quadratic Function Optimization ‣ 3\. Second-order
    methods ‣ A survey of deep learning optimizers - first and second order methods").
    However, for non-convex functions, the Hessian matrix $\nabla^{2}f_{k}$ may not
    always be positive-definite, eq. [3.1](#S3.Ex18 "3.1\. Preliminaries and Motivation
    ‣ 3\. Second-order methods ‣ A survey of deep learning optimizers - first and
    second order methods") may not be a descent direction. Also, Hessian being ill-conditioned
    matters more near the minima since if the Hessian changes too fast we may follow
    a zig-zag path leading to sub-optimal performance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 凸函数具有很强的理论保证，因为它们表现良好并且具有良好的性质。它们没有鞍点，所有局部最小值都对应于全局最小值，特别是因为Hessian矩阵$H$是正半定的。对于二次函数，Hessian矩阵$H$是常数，因此表现良好。关于二次凸函数的详细分析在第[3.2节](#S3.SS2
    "3.2\. Quadratic Function Optimization ‣ 3\. Second-order methods ‣ A survey of
    deep learning optimizers - first and second order methods")讨论。然而，对于非凸函数，Hessian矩阵$\nabla^{2}f_{k}$可能并不总是正定的，等式[3.1](#S3.Ex18
    "3.1\. Preliminaries and Motivation ‣ 3\. Second-order methods ‣ A survey of deep
    learning optimizers - first and second order methods")可能不是一个下降方向。此外，Hessian矩阵在接近最小值时病态更为严重，因为如果Hessian变化过快，我们可能会沿着一个锯齿形路径前进，从而导致次优性能。
- en: 'To understand the ill-conditioned problem, we consider a convex quadratic objective
    given as:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解病态问题，我们考虑一个给定的凸二次目标：
- en: '|  | $\mathcal{J}(\theta)=\frac{1}{2}\theta^{T}A\theta$ |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{J}(\theta)=\frac{1}{2}\theta^{T}A\theta$ |  |'
- en: 'where $A$ is a positive semi-definite symmetric matrix. If the Hessian of the
    objective i.e., $\nabla^{2}\mathcal{J}(\theta)=A$ is ill-conditioned then gradient
    also changing rapidly and the update using gradient descent is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$A$是一个正半定对称矩阵。如果目标的Hessian矩阵，即$\nabla^{2}\mathcal{J}(\theta)=A$是病态的，那么梯度也会迅速变化，使用梯度下降的更新如下：
- en: <math   alttext="\begin{array}[]{rlr}\theta_{k+1}&amp;\leftarrow\theta_{k}-\alpha\nabla\mathcal{J}\left(\theta_{k}\right)&amp;\\
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math   alttext="\begin{array}[]{rlr}\theta_{k+1}&amp;\leftarrow\theta_{k}-\alpha\nabla\mathcal{J}\left(\theta_{k}\right)&amp;\\
- en: '&amp;=\theta_{k}-\alpha A\theta_{k}&amp;\\'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=\theta_{k}-\alpha A\theta_{k}&amp;\\'
- en: '&amp;=(I-\alpha A)\theta_{k}&amp;\\'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=(I-\alpha A)\theta_{k}&amp;\\'
- en: \Longrightarrow\theta_{k}&amp;=(I-\alpha A)^{k}\theta_{0}&amp;\\
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \Longrightarrow\theta_{k}&amp;=(I-\alpha A)^{k}\theta_{0}&amp;\\
- en: '&amp;=\left(I-\alpha Q\Lambda Q^{T}\right)^{k}\theta_{0}&amp;\\'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=\left(I-\alpha Q\Lambda Q^{T}\right)^{k}\theta_{0}&amp;\\'
- en: '&amp;=\left[Q(I-\alpha\Lambda)Q^{T}\right]^{k}\theta_{0}&amp;\\'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=\left[Q(I-\alpha\Lambda)Q^{T}\right]^{k}\theta_{0}&amp;\\'
- en: '&amp;=Q(I-\alpha\Lambda)^{k}Q^{T}\theta_{0}&amp;\end{array}" display="inline"><semantics
    ><mtable columnspacing="5pt" rowspacing="0pt" ><mtr  ><mtd columnalign="right"  ><msub
    ><mi >θ</mi><mrow ><mi  >k</mi><mo >+</mo><mn >1</mn></mrow></msub></mtd><mtd
    columnalign="left"  ><mrow ><mo stretchy="false" >←</mo><mrow ><msub ><mi >θ</mi><mi
    >k</mi></msub><mo >−</mo><mrow ><mi >α</mi><mo lspace="0.167em" rspace="0em" >​</mo><mrow
    ><mo rspace="0.167em"  >∇</mo><mi >𝒥</mi></mrow><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo >(</mo><msub ><mi >θ</mi><mi >k</mi></msub><mo >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow ><msub ><mi >θ</mi><mi >k</mi></msub><mo
    >−</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em" >​</mo><mi >A</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi >θ</mi><mi >k</mi></msub></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow ><mrow ><mo stretchy="false"
    >(</mo><mrow ><mi >I</mi><mo >−</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >A</mi></mrow></mrow><mo stretchy="false"  >)</mo></mrow><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi >θ</mi><mi >k</mi></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><mo stretchy="false" >⟹</mo><msub ><mi >θ</mi><mi
    >k</mi></msub></mrow></mtd><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow ><msup
    ><mrow ><mo stretchy="false" >(</mo><mrow ><mi >I</mi><mo >−</mo><mrow ><mi >α</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >A</mi></mrow></mrow><mo stretchy="false"  >)</mo></mrow><mi
    >k</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >θ</mi><mn >0</mn></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow ><msup ><mrow ><mo >(</mo><mrow
    ><mi >I</mi><mo >−</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >Q</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >Λ</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msup ><mi >Q</mi><mi >T</mi></msup></mrow></mrow><mo
    >)</mo></mrow><mi >k</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi
    >θ</mi><mn >0</mn></msub></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mo >=</mo><mrow ><msup ><mrow ><mo >[</mo><mrow ><mi >Q</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow ><mi >I</mi><mo
    >−</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >Λ</mi></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow><mo lspace="0em" rspace="0em"  >​</mo><msup ><mi
    >Q</mi><mi >T</mi></msup></mrow><mo >]</mo></mrow><mi >k</mi></msup><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi >θ</mi><mn >0</mn></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow ><mi  >Q</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msup ><mrow ><mo stretchy="false"  >(</mo><mrow ><mi >I</mi><mo
    >−</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >Λ</mi></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow><mi >k</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msup
    ><mi >Q</mi><mi >T</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi
    >θ</mi><mn >0</mn></msub></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    ><matrix ><matrixrow  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><apply ><ci >𝑘</ci><cn type="integer" >1</cn></apply></apply><apply ><ci  >←</ci><csymbol
    cd="latexml"  >absent</csymbol><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><ci >𝑘</ci></apply><apply ><ci >𝛼</ci><apply ><ci >∇</ci><ci >𝒥</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑘</ci></apply></apply></apply></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><apply
    ><csymbol cd="latexml" >absent</csymbol><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝜃</ci><ci >𝑘</ci></apply><apply ><ci >𝛼</ci><ci >𝐴</ci><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜃</ci><ci >𝑘</ci></apply></apply></apply></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><apply
    ><csymbol cd="latexml" >absent</csymbol><apply ><apply ><ci >𝐼</ci><apply ><ci
    >𝛼</ci><ci >𝐴</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><ci >𝑘</ci></apply></apply></apply><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><apply ><ci  >⟹</ci><csymbol cd="latexml"  >absent</csymbol><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑘</ci></apply></apply><apply
    ><csymbol cd="latexml" >absent</csymbol><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><ci >𝐼</ci><apply ><ci >𝛼</ci><ci >𝐴</ci></apply></apply><ci >𝑘</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><cn type="integer"  >0</cn></apply></apply></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><apply
    ><csymbol cd="latexml" >absent</csymbol><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><ci >𝐼</ci><apply ><ci >𝛼</ci><ci >𝑄</ci><ci >Λ</ci><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑄</ci><ci >𝑇</ci></apply></apply></apply><ci
    >𝑘</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><cn
    type="integer"  >0</cn></apply></apply></apply><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror></matrixrow><matrixrow ><cerror ><csymbol
    cd="ambiguous" >missing-subexpression</csymbol></cerror><apply ><csymbol cd="latexml"
    >absent</csymbol><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="latexml" >delimited-[]</csymbol><apply ><ci >𝑄</ci><apply ><ci >𝐼</ci><apply
    ><ci >𝛼</ci><ci >Λ</ci></apply></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑄</ci><ci >𝑇</ci></apply></apply></apply><ci >𝑘</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><cn type="integer"  >0</cn></apply></apply></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><apply
    ><csymbol cd="latexml" >absent</csymbol><apply ><ci  >𝑄</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><ci >𝐼</ci><apply ><ci >𝛼</ci><ci >Λ</ci></apply></apply><ci >𝑘</ci></apply><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑄</ci><ci >𝑇</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><cn type="integer"  >0</cn></apply></apply></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{rlr}\theta_{k+1}&\leftarrow\theta_{k}-\alpha\nabla\mathcal{J}\left(\theta_{k}\right)&\\
    &=\theta_{k}-\alpha A\theta_{k}&\\ &=(I-\alpha A)\theta_{k}&\\ \Longrightarrow\theta_{k}&=(I-\alpha
    A)^{k}\theta_{0}&\\ &=\left(I-\alpha Q\Lambda Q^{T}\right)^{k}\theta_{0}&\\ &=\left[Q(I-\alpha\Lambda)Q^{T}\right]^{k}\theta_{0}&\\
    &=Q(I-\alpha\Lambda)^{k}Q^{T}\theta_{0}&\end{array}</annotation></semantics></math>'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=Q(I-\alpha\Lambda)^{k}Q^{T}\theta_{0}&amp;\end{array}" display="inline"><semantics
    ><mtable columnspacing="5pt" rowspacing="0pt" ><mtr  ><mtd columnalign="right"  ><msub
    ><mi >θ</mi><mrow ><mi  >k</mi><mo >+</mo><mn >1</mn></mrow></msub></mtd><mtd
    columnalign="left"  ><mrow ><mo stretchy="false" >←</mo><mrow ><msub ><mi >θ</mi><mi
    >k</mi></msub><mo >−</mo><mrow ><mi >α</mi><mo lspace="0.167em" rspace="0em" >​</mo><mrow
    ><mo rspace="0.167em"  >∇</mo><mi >𝒥</mi></mrow><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo >(</mo><msub ><mi >θ</mi><mi >k</mi></msub><mo >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow ><msub ><mi >θ</mi><mi >k</mi></msub><mo
    >−</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em" >​</mo><mi >A</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi >θ</mi><mi >k</mi></msub></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow ><mrow ><mo stretchy="false"
    >(</mo><mrow ><mi >I</mi><mo >−</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >A</mi></mrow></mrow><mo stretchy="false"  >)</mo></mrow><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi >θ</mi><mi >k</mi></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><mo stretchy="false" >⟹</mo><msub ><mi >θ</mi><mi
    >k</mi></msub></mrow></mtd><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow ><msup
    ><mrow ><mo stretchy="false" >(</mo><mrow ><mi >I</mi><mo >−</mo><mrow ><mi >α</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >A</mi></mrow></mrow><mo stretchy="false"  >)</mo></mrow><mi
    >k</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >θ</mi><mn >0</mn></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow ><msup ><mrow ><mo >(</mo><mrow
    ><mi >I</mi><mo >−</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >Q</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >Λ</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msup ><mi >Q</mi><mi >T</mi></msup></mrow></mrow><mo
    >)</mo></mrow><mi >k</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi
    >θ</mi><mn >0</mn></msub></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mo >=</mo><mrow ><msup ><mrow ><mo >[</mo><mrow ><mi >Q</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow ><mi >I</mi><mo
    >−</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >Λ</mi></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow><mo lspace="0em" rspace="0em"  >​</mo><msup ><mi
    >Q</mi><mi >T</mi></msup></mrow><mo >]</mo></mrow><mi >k</mi></msup><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi >θ</mi><mn >0</mn></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow ><mi  >Q</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msup ><mrow ><mo stretchy="false"  >(</mo><mrow ><mi >I</mi><mo
    >−</mo><mrow ><mi >α</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >Λ</mi></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow><mi >k</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msup
    ><mi >Q</mi><mi >T</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi
    >θ</mi><mn >0</mn></msub></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    ><matrix ><matrixrow  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><apply ><ci >𝑘</ci><cn type="integer" >1</cn></apply></apply><apply ><ci  >←</ci><csymbol
    cd="latexml"  >absent</csymbol><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><ci >𝑘</ci></apply><apply ><ci >𝛼</ci><apply ><ci >∇</ci><ci >𝒥</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑘</ci></apply></apply></apply></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><apply
    ><csymbol cd="latexml" >absent</csymbol><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝜃</ci><ci >𝑘</ci></apply><apply ><ci >𝛼</ci><ci >𝐴</ci><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜃</ci><ci >𝑘</ci></apply></apply></apply></apply><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror><apply
    ><csymbol cd="latexml" >absent</csymbol><apply ><apply ><ci >𝐼</ci><apply ><ci
    >𝛼</ci><ci >𝐴</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><ci >𝑘</ci></apply></apply></apply><cerror ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><apply ><ci  >⟹</ci><csymbol cd="latexml"  >absent</csymbol><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >'
- en: 'where $Q\Lambda Q^{T}$ is the eigendecomposition of $A$ as discussed in eq.
    [3.1](#S3.Ex16 "3.1\. Preliminaries and Motivation ‣ 3\. Second-order methods
    ‣ A survey of deep learning optimizers - first and second order methods"). The
    stability conditions is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q\Lambda Q^{T}$ 是 $A$ 的特征分解，如方程[3.1](#S3.Ex16 "3.1\. 初步了解与动机 ‣ 3\. 二阶方法
    ‣ 深度学习优化器调查 - 一阶与二阶方法")中讨论的。稳定性条件如下：
- en: 1\. $0<\alpha\lambda_{i}\leq 1:$ decays to $0$ at a rate that depends on $\alpha\lambda_{i}$.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. $0<\alpha\lambda_{i}\leq 1:$ 以依赖于 $\alpha\lambda_{i}$ 的速率衰减到 $0$。
- en: 2\. $1<\alpha\lambda_{i}\leq 2:$ oscillates.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. $1<\alpha\lambda_{i}\leq 2:$ 振荡。
- en: 3\. $\alpha\lambda_{i}>2:$ unstable (diverges).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. $\alpha\lambda_{i}>2:$ 不稳定（发散）。
- en: where $\alpha$ is the learning-rate of the algorithm and $\lambda_{i}$ are the
    eigenvalues. Hence, we need the learning rate to be bound by $\alpha<2/\lambda_{\max}$
    to prevent instability. This ensures that we avoid overshooting the minima and
    not going uphill in directions with strong positive curvature. This also bounds
    the rate of progress in other directions given as $\alpha\lambda_{i}<2\lambda_{i}/\lambda_{\max}$.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是算法的学习率，$\lambda_{i}$ 是特征值。因此，我们需要将学习率限制为 $\alpha<2/\lambda_{\max}$
    以防止不稳定。这确保了我们避免超越极小值，并且在具有强正曲率的方向上不会上坡。这也限制了其他方向的进展速率，给出为 $\alpha\lambda_{i}<2\lambda_{i}/\lambda_{\max}$。
- en: Given a starting point $x_{i}$, one could construct a local quadratic approximation
    to the objective function ($\mathcal{J}(q)$) that matches the first and second
    derivative values at that point. We then minimize the approximate (quadratic function)
    instead of the original objective function. The minimizer of the approximate function
    is used as the starting point in the next step, and repeat the procedure iteratively.
    This ensures that the search direction accounts for the curvature information
    along almost all directions, for all iterations as shown in Figure [12](#S3.F12
    "Figure 12 ‣ 3.3\. Newton’s Method ‣ 3\. Second-order methods ‣ A survey of deep
    learning optimizers - first and second order methods").
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 给定起始点 $x_{i}$，可以构建目标函数 ($\mathcal{J}(q)$) 的局部二次近似，该近似与该点的第一和第二导数值匹配。然后，我们最小化近似（二次函数）而不是原始目标函数。近似函数的最小值被用作下一步的起始点，并迭代重复这一过程。这确保了搜索方向在几乎所有方向上都考虑了曲率信息，如图所示[12](#S3.F12
    "图 12 ‣ 3.3\. 牛顿法 ‣ 3\. 二阶方法 ‣ 深度学习优化器调查 - 一阶与二阶方法")。
- en: In the case, where the loss surface is the shape of an elongated quadratic function
    or an ellipsoid as shown in Figure [11](#S3.F11 "Figure 11 ‣ 3.1\. Preliminaries
    and Motivation ‣ 3\. Second-order methods ‣ A survey of deep learning optimizers
    - first and second order methods"), we can transform it into a spherical shape
    using a whitening transform since the Hessian inverse $H^{-1}$ spheres out of
    the error surface locally, and we can now perform gradient descent in the new
    coordinate system.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在损失面呈现为图中所示的细长二次函数或椭球体的情况下[11](#S3.F11 "图 11 ‣ 3.1\. 初步了解与动机 ‣ 3\. 二阶方法 ‣ 深度学习优化器调查
    - 一阶与二阶方法")，我们可以使用白化变换将其转化为球形，因为Hessian逆$H^{-1}$局部地将误差面球化，我们现在可以在新的坐标系中执行梯度下降。
- en: '![Refer to caption](img/90c4e89830bdb51fe2be6d3236341048.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/90c4e89830bdb51fe2be6d3236341048.png)'
- en: 'Figure 11\. The first-order gradient descent method fails to exploit the curvature
    information contained in the Hessian matrix. The condition number greatly influences
    the SGD trajectory, and we illustrate it using a quadratic function f(x) whose
    Hessian has a condition number 5\. This means that the direction of the most curvature
    has five times more curvature than that of the least curvature. In this case,
    the most curvature is in the direction $[1,1]$, and the least curvature is in
    the direction $[1,-1]$. The red lines indicate the path followed by gradient descent.
    This results in an ellipsoidal quadratic function, and gradient descent follows
    a zig-zag path, often bouncing off the canyon walls (since the gradient is perpendicular
    to the surface), and we observe that it spends a lot of descending the canyon
    walls because they are the steepest feature. It indicates that gradient descent
    takes a larger number of steps to converge, and near the minima, it oscillates
    back and forth until it converges to the local minima. If the step size is large,
    it can overshoot and reach the opposite canyon wall on the next iteration, hence
    not the best search direction. The large positive eigenvalue of the Hessian corresponding
    to the eigenvector pointed in this direction indicates that this directional derivative
    is rapidly increasing, so an optimization algorithm based on the Hessian could
    predict the steepest direction is not actually a promising search direction in
    this context. Source: Ian Goodfellow ([b23,](#bib.bib23) ).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 一阶梯度下降法未能利用 Hessian 矩阵中包含的曲率信息。条件数极大地影响了 SGD 路径，我们用一个二次函数 f(x) 来说明，它的
    Hessian 矩阵的条件数为 5。这意味着曲率最大的方向的曲率是最小曲率方向的五倍。在这种情况下，曲率最大的方向是 $[1,1]$，而曲率最小的方向是 $[1,-1]$。红线表示梯度下降所经过的路径。这导致了一个椭圆形的二次函数，梯度下降沿着锯齿形路径前进，通常会在峡谷墙上弹跳（因为梯度是垂直于表面的），我们观察到它在下降峡谷墙时花费了大量时间，因为这些墙是最陡峭的特征。这表明梯度下降需要更多的步骤才能收敛，并且在最小值附近，它会来回振荡，直到收敛到局部最小值。如果步长较大，它可能会超越并在下一次迭代中到达对面的峡谷墙，因此不是最佳的搜索方向。Hessian
    对应于该方向的特征值较大的正特征值表明该方向的导数快速增加，因此基于 Hessian 的优化算法可以预测最陡方向实际上在这种情况下并不是一个有前途的搜索方向。来源：Ian
    Goodfellow ([b23,](#bib.bib23) )。
- en: 3.2\. Quadratic Function Optimization
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 二次函数优化
- en: For a locally quadratic function, by rescaling the gradients with the inverse
    of the Hessian, Newton’s method jumps directly to the minimum and thus converges
    in a single step. If the function is nearly quadratic, then this is a very good
    estimate of the minimizer of $f$. Since $f$ is twice differentiable, the quadratic
    model of $f$ will be very accurate when $x$ is near the local minima. When $f$
    is not quadratic but can be locally approximated as a positive definite quadratic,
    and the Newton’s method is updated iteratively using eq. [3.3](#S3.Ex21 "3.3\.
    Newton’s Method ‣ 3\. Second-order methods ‣ A survey of deep learning optimizers
    - first and second order methods") with quadratic rate of convergence. The following
    result as stated in ([b30,](#bib.bib30) ) asserts the rate of converge for the
    Newton’s method and is given below.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于局部二次函数，通过用 Hessian 矩阵的逆来重新缩放梯度，牛顿法可以直接跳转到最小值，从而在一步内收敛。如果函数接近二次函数，这将是 $f$ 的最小化值的一个很好的估计。由于
    $f$ 是二次可微的，当 $x$ 接近局部最小值时，$f$ 的二次模型将非常准确。当 $f$ 不是二次函数但可以被局部近似为正定二次函数时，牛顿法通过使用等式
    [3.3](#S3.Ex21 "3.3\. 牛顿法 ‣ 3\. 二阶方法 ‣ 深度学习优化器的调查 - 一阶和二阶方法") 进行迭代更新，具有二次收敛速率。以下结果在
    ([b30,](#bib.bib30) ) 中指出了牛顿法的收敛速率，如下所示。
- en: 'THEOREM 1\. Suppose that $f$ is twice differentiable and that the Hessian $\nabla^{2}f(x)$
    is Lipschitz continuous in a neighborhood of a solution $x^{*}$ at which the sufficient
    conditions are satisfied. Consider the iteration $x_{k+1}=x_{k}+p_{k}$, where
    $p_{k}$ is given by $-\nabla^{2}f_{k}^{-1}\nabla f_{k}$. Then:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 1\. 设 $f$ 是二次可微的，并且在满足充分条件的解 $x^{*}$ 的邻域内，Hessian $\nabla^{2}f(x)$ 是 Lipschitz
    连续的。考虑迭代 $x_{k+1}=x_{k}+p_{k}$，其中 $p_{k}$ 由 $-\nabla^{2}f_{k}^{-1}\nabla f_{k}$
    给出。那么：
- en: (i) if the starting point $x_{0}$ is sufficiently close to $x^{*}$, the sequence
    of iterates converges to $x^{*}$.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 如果起始点 $x_{0}$ 足够接近 $x^{*}$，那么迭代序列将收敛到 $x^{*}$。
- en: (ii) the rate of convergence of $\left\{x_{k}\right\}$ is quadratic.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) $\left\{x_{k}\right\}$ 的收敛速率是二次的。
- en: (iii) the sequence of gradient norms $\left\{\left\|\nabla f_{k}\right\|\right\}$
    converges quadratically to zero.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 梯度范数序列 $\left\{\left\|\nabla f_{k}\right\|\right\}$ 二次收敛于零。
- en: '1 Initialize: Initial point $x_{0}$.2 for *$k=0,1,2,\ldots$* do3       Factorize
    the matrix $B_{k}=\nabla^{2}f\left(x_{k}\right)+E_{k}$, where $E_{k}=0$ if $\nabla^{2}f\left(x_{k}\right)$
    is sufficiently positive definite4       otherwise, $E_{k}$ is chosen to ensure
    that $B_{k}$ is sufficiently positive definite.5       Solve $B_{k}p_{k}=-\nabla
    f\left(x_{k}\right)$.6       $x_{k+1}\leftarrow x_{k}+\alpha_{k}p_{k}$7 end for'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 1 初始化：初始点 $x_{0}$。2 对于 *$k=0,1,2,\ldots$* 执行 3       对矩阵 $B_{k}=\nabla^{2}f\left(x_{k}\right)+E_{k}$
    进行分解，其中如果 $\nabla^{2}f\left(x_{k}\right)$ 足够正定则 $E_{k}=0$ 4       否则，选择 $E_{k}$
    以确保 $B_{k}$ 足够正定。5       解方程 $B_{k}p_{k}=-\nabla f\left(x_{k}\right)$。6      
    $x_{k+1}\leftarrow x_{k}+\alpha_{k}p_{k}$ 7 结束循环
- en: Algorithm 2 Newton’s Method with Hessian Modification
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 牛顿方法与 Hessian 修改
- en: 3.3\. Newton’s Method
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 牛顿方法
- en: In Newton’s optimization algorithm, to find the local minimum $x^{*}$ with iteration
    sequence of $x_{0}\rightarrow x_{1}\rightarrow x_{2}\rightarrow\ldots\ldots..\rightarrow
    x_{k}$ the Hessian $\nabla f^{2}\left(x_{k}\right)$ must be positive definite
    $\forall k$ iteration steps else the search direction might not correspond to
    an actual the descent direction i.e., $\nabla f_{k}^{T}p_{k}<0$, where $p_{k}$
    is the Newton search direction. If $f$ is strongly convex then f converges quadratically
    fast to $x^{*}=\arg\min_{x}f(x)$ i.e.,
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在牛顿优化算法中，要找到局部最小值 $x^{*}$，其迭代序列为 $x_{0}\rightarrow x_{1}\rightarrow x_{2}\rightarrow\ldots\ldots..\rightarrow
    x_{k}$，Hessian $\nabla f^{2}\left(x_{k}\right)$ 必须在 $\forall k$ 迭代步骤中是正定的，否则搜索方向可能不对应实际的下降方向，即
    $\nabla f_{k}^{T}p_{k}<0$，其中 $p_{k}$ 是牛顿搜索方向。如果 $f$ 是强凸的，则 $f$ 将以二次速度收敛于 $x^{*}=\arg\min_{x}f(x)$。
- en: '|  | $\left\&#124;x_{k+1}-x_{*}\right\&#124;\leq\frac{1}{2}\left\&#124;x_{k}-x_{*}\right\&#124;^{2},\quad\forall
    k\geq 0.$ |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left\&#124;x_{k+1}-x_{*}\right\&#124;\leq\frac{1}{2}\left\&#124;x_{k}-x_{*}\right\&#124;^{2},\quad\forall
    k\geq 0.$ |  |'
- en: 'The update rule is given as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 更新规则如下：
- en: '|  | $\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}-\boldsymbol{H^{-1}g}$
    |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}-\boldsymbol{H^{-1}g}$
    |  |'
- en: where $H$ is the Hessian matrix of the loss function $L$ with respect to $\theta$.
    Since the Hessian ($H=\nabla f^{2}(x_{k})$) is symmetric, we can determine the
    Newton update using Cholesky⁷⁷7Every symmetric positive definite matrix $A$ can
    be written as $A=LDL^{T}$ where $L$ is a lower triangular matrix with unit diagonal
    elements and $D$ is a diagonal matrix with positive elements on the diagonal algorithm.
    An essential feature of Newton’s method is that it is independent of linear changes
    in coordinates. If $H$ is positive semi-definite, then there is nothing that confirms
    that the iterator has converged to a minimizer since the higher derivatives of
    $f(x)$ are unknown. The algorithm will diverge if the Hessian is not positive
    definite ($\lambda_{i}\leq 0$ indicates flat regions or directions of negative
    curvature).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H$ 是关于 $\theta$ 的损失函数 $L$ 的 Hessian 矩阵。由于 Hessian ($H=\nabla f^{2}(x_{k})$)
    是对称的，我们可以使用 Cholesky 分解来确定牛顿更新⁷⁷7每个对称正定矩阵 $A$ 可以写成 $A=LDL^{T}$，其中 $L$ 是具有单位对角线元素的下三角矩阵，$D$
    是具有正对角线元素的对角矩阵。牛顿方法的一个基本特性是它与坐标的线性变化无关。如果 $H$ 是半正定的，那么没有任何东西能确认迭代器是否已收敛到最小点，因为
    $f(x)$ 的高阶导数是未知的。如果 Hessian 不是正定的（$\lambda_{i}\leq 0$ 表示平坦区域或负曲率方向），则算法将发散。
- en: Thus, alternative methods to ensure that eq. [3.3](#S3.Ex21 "3.3\. Newton’s
    Method ‣ 3\. Second-order methods ‣ A survey of deep learning optimizers - first
    and second order methods") corresponds to a descent direction while retaining
    the second-order information in $\nabla^{2}f_{k}$ with superlinear rate of convergence
    is described in the following sections. In particular, the computation of the
    Hessian $H$ is computationally expensive and error-prone. For example, in Quasi-Newton
    methods an approximation to the inverse of the Hessian $B_{k}$ is computed at
    each step using a low-rank formula while a trust region approach, in which $\nabla^{2}f_{k}$
    is used to form a quadratic model that is minimized in a ball around the current
    iterate $x_{k}$.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了确保等式 [3.3](#S3.Ex21 "3.3\. 牛顿方法 ‣ 3\. 二阶方法 ‣ 深度学习优化器调查 - 一阶和二阶方法") 对应于一个下降方向，同时保留
    $\nabla^{2}f_{k}$ 的二阶信息，并具有超线性收敛速度的方法在以下章节中描述。特别地，Hessian $H$ 的计算是计算量大且易出错的。例如，在准牛顿方法中，每一步使用低秩公式计算
    Hessian $B_{k}$ 的逆，而信任区域方法则利用 $\nabla^{2}f_{k}$ 形成一个二次模型，并在当前迭代点 $x_{k}$ 周围的球体中进行最小化。
- en: '![Refer to caption](img/0c30a96cd2664ff51131dfa71ed5886b.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0c30a96cd2664ff51131dfa71ed5886b.png)'
- en: 'Figure 12\. Newton method constructs a local quadratic approximation of the
    function about the current point, moves towards the minimizer of this quadratic
    model, and then iteratively progress towards the global minima. Source: Nicholas
    Vieau Alger ([b17,](#bib.bib17) ).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图12\. 牛顿法在当前点构造了函数的局部二次近似，向该二次模型的最小化方向移动，然后迭代地朝着全局最小值前进。来源：Nicholas Vieau Alger
    ([b17,](#bib.bib17) )。
- en: 3.4\. Dynamics of Optimization
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 优化的动态
- en: 'As discussed in Section [3.3](#S3.SS3 "3.3\. Newton’s Method ‣ 3\. Second-order
    methods ‣ A survey of deep learning optimizers - first and second order methods"),
    the major drawback of the Newton method is that the update step can result in
    the wrong direction if the eigenvalue $\lambda_{i}$ of the Hessian $H$ is negative
    i.e., it moves along the eigenvector in a direction opposite to the gradient descent
    step and thus moves in the direction of increasing error towards $\theta^{*}$.
    To address this issue, generalized trust region ([b30,](#bib.bib30) ), saddle-free
    Newton ([b5,](#bib.bib5) ) and natural gradient descent method are used and is
    discussed briefly as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [3.3](#S3.SS3 "3.3\. 牛顿法 ‣ 3\. 二阶方法 ‣ 深度学习优化器调查 - 一阶和二阶方法") 节中讨论的那样，牛顿法的主要缺点是当Hessian
    $H$ 的特征值 $\lambda_{i}$ 为负时，更新步骤可能会朝错误方向移动，即它沿着特征向量沿梯度下降步骤的相反方向移动，从而朝着 $\theta^{*}$
    增加误差的方向移动。为了解决这个问题，使用了广义信任区域 ([b30,](#bib.bib30) )、无鞍点牛顿法 ([b5,](#bib.bib5) )
    和自然梯度下降方法，简要讨论如下：
- en: •
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In the trust region method, a damping constant $\alpha$ is added along the diagonal
    of the Hessian to offset the direction of negative curvature, which is equivalent
    to $\lambda_{i}+\alpha$. However, to ensure that this is a descent direction,
    one must ensure that $\lambda_{i}+\alpha>0$, and can result in small step size
    along the eigen-directions.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在信任区域方法中，在Hessian的对角线添加了一个阻尼常数 $\alpha$ 以抵消负曲率的方向，这相当于 $\lambda_{i}+\alpha$。然而，为了确保这是一个下降方向，必须确保
    $\lambda_{i}+\alpha>0$，这可能会导致沿特征方向的步长较小。
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Although, the truncated Newton method ignores directions of negative curvature
    it can get struck at saddle points. However, since the natural gradient descent
    relies on the Fisher information matrix $F$ to incorporate the curvature information
    of the parameter manifold where the update rule is given as:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管截断牛顿法忽略了负曲率的方向，但它可能会在鞍点处卡住。然而，由于自然梯度下降法依赖于费舍尔信息矩阵 $F$ 来纳入参数流形的曲率信息，其更新规则为：
- en: '|  | $w_{t+1}=w_{t}-\eta F\left(w_{t}\right)^{-1}g_{t}$ |  |'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $w_{t+1}=w_{t}-\eta F\left(w_{t}\right)^{-1}g_{t}$ |  |'
- en: •
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'However, ([b5,](#bib.bib5) ) argue that natural gradient descent method can
    also suffer from negative curvature and can converge to a non-stationary point
    when the Fisher matrix is rank-deficient. The saddle-free Newton method provides
    a elegant solution to this, where the update rule is given as:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，([b5,](#bib.bib5) ) 认为自然梯度下降法也可能遭遇负曲率，并且当费舍尔矩阵的秩不足时，可能会收敛到非平稳点。无鞍点牛顿法提供了一个优雅的解决方案，其更新规则为：
- en: '|  | $\Delta\theta=-\nabla f&#124;\mathbf{H}&#124;^{-1}$ |  |'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\Delta\theta=-\nabla f&#124;\mathbf{H}&#124;^{-1}$ |  |'
- en: •
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Since, it does not utilize the second-order Taylor-series approximation to leverage
    the information, as in classical methods, it can move further in direction of
    low curvature and escape saddle points as shown in Figure [14](#S3.F14 "Figure
    14 ‣ 3.6\. Conjugate Gradients ‣ 3\. Second-order methods ‣ A survey of deep learning
    optimizers - first and second order methods").
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于它不像经典方法那样利用二阶泰勒级数近似来利用信息，因此可以在低曲率的方向上移动，逃离鞍点，如图 [14](#S3.F14 "图 14 ‣ 3.6\.
    共轭梯度 ‣ 3\. 二阶方法 ‣ 深度学习优化器调查 - 一阶和二阶方法") 所示。
- en: 3.5\. Gauss-Newton and Levenberg-Marquardt algorithm
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 高斯-牛顿和列文贝格-马夸特算法
- en: 'Following [1.1](#S1.SS1 "1.1\. Mathematical Preliminaries and Notations ‣ 1\.
    Introduction ‣ A survey of deep learning optimizers - first and second order methods"),
    we define the non-linear least square consider a set of m points $(x_{i},y_{i})$
    and the curve $y=f(x,\theta)$, where $\theta\in R^{n}$ denotes the model parameters
    and $i\in[m]$, with $m\geq n$. We define the non-linear least square problem as:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [1.1](#S1.SS1 "1.1\. 数学预备知识和符号 ‣ 1\. 引言 ‣ 深度学习优化器调查 - 一阶和二阶方法")，我们定义非线性最小二乘问题，考虑一组
    $m$ 个点 $(x_{i},y_{i})$ 和曲线 $y=f(x,\theta)$，其中 $\theta\in R^{n}$ 表示模型参数，$i\in[m]$，且
    $m\geq n$。我们将非线性最小二乘问题定义为：
- en: '|  | $\min_{\theta}\&#124;y_{i}-f(x_{i},\theta)\&#124;_{2}^{2}=\min_{\theta}\sum_{i=1}^{m}\&#124;r_{i}\&#124;_{2}^{2}\vspace{0.3em}$
    |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\&#124;y_{i}-f(x_{i},\theta)\&#124;_{2}^{2}=\min_{\theta}\sum_{i=1}^{m}\&#124;r_{i}\&#124;_{2}^{2}\vspace{0.3em}$
    |  |'
- en: 'where $r_{i}=y_{i}-f(x_{i},\theta)$ are the residuals $\forall i=1,2,...,m$.
    The Gauss-Newton ([b10,](#bib.bib10) ; [b52,](#bib.bib52) ) method utilizes the
    square of the Jacobian (not the Hessian) and is used to minimize non-linear least
    square problems. It can be viewed as a modified Newton’s method with line search
    where we approximate the Hessian as $\bigtriangledown f_{k}\approx J_{k}^{T}J_{k}$
    and has $O\left(N^{3}\right)$ complexity. The update rule is given as:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r_{i}=y_{i}-f(x_{i},\theta)$ 是残差 $\forall i=1,2,...,m$。高斯-牛顿 ([b10,](#bib.bib10)
    ; [b52,](#bib.bib52) ) 方法利用雅可比矩阵的平方（而不是 Hessian），用于最小化非线性最小二乘问题。它可以被视为一种修改的牛顿法带线搜索，其中我们将
    Hessian 近似为 $\bigtriangledown f_{k}\approx J_{k}^{T}J_{k}$，其复杂度为 $O\left(N^{3}\right)$。更新规则为：
- en: '|  | $\Delta\theta=-(J_{k}^{T}J_{k})J_{k}^{T}r_{i}\vspace{0.4em}$ |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta\theta=-(J_{k}^{T}J_{k})J_{k}^{T}r_{i}\vspace{0.4em}$ |  |'
- en: where $J_{i}(x)=\left[\begin{array}[]{c}\nabla r_{1}(x)^{T},\nabla r_{2}(x)^{T},\ldots,\nabla
    r_{m}(x)^{T}\end{array}\right]^{T}$ is Jacobian of the residual. Note that, the
    Gauss-Newton method does not require the computation of the individual residual
    Hessians $\bigtriangledown^{2}r_{i}$ and saves significant computational time.
    Also, near the minimum since the residuals are significantly smaller i.e., $\|r_{i}|\
    \leq\epsilon$, this leads to rapid-convergence.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $J_{i}(x)=\left[\begin{array}[]{c}\nabla r_{1}(x)^{T},\nabla r_{2}(x)^{T},\ldots,\nabla
    r_{m}(x)^{T}\end{array}\right]^{T}$ 是残差的雅可比矩阵。请注意，高斯-牛顿方法不需要计算单独的残差 Hessians $\bigtriangledown^{2}r_{i}$，从而节省了大量计算时间。此外，由于残差在接近最小值时显著变小，即
    $\|r_{i}\|\ \leq\epsilon$，这导致了快速收敛。
- en: 'The Levenberg-Marquardt ([b53,](#bib.bib53) ; [b54,](#bib.bib54) ) algorithm
    is an approximation to Gauss-Newton method that uses a diagonal approximation
    to the Hessian and takes care of extreme directions of curvature, thus preventing
    the update from moving in the wrong direction. This addresses the issue namely,
    when the Jacobian $J(x)$ is rank-deficient (i.e., the columns are not linearly
    independent) while maintaining similar local convergence properties since we only
    replace the line search with a trust-region method. The new update rule is given
    as:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Levenberg-Marquardt ([b53,](#bib.bib53) ; [b54,](#bib.bib54) ) 算法是对高斯-牛顿方法的近似，它使用对角线近似来代替
    Hessian 矩阵，并处理曲率的极端方向，从而防止更新方向出错。这解决了当雅可比矩阵 $J(x)$ 是秩缺乏的（即列不线性独立）的问题，同时保持类似的局部收敛性质，因为我们只将线搜索替换为信任域方法。新的更新规则为：
- en: '|  | $\Delta\theta=-(J_{k}^{T}J_{k}+\lambda I)J_{k}^{T}r_{i}$ |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta\theta=-(J_{k}^{T}J_{k}+\lambda I)J_{k}^{T}r_{i}$ |  |'
- en: where $\mu$ is the regularization parameter and $I$ is the Identity matrix.
    The regularization parameter is used to address the issue when the Hessian is
    not positive definite and has directions that correspond to small negative eigenvalues.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu$ 是正则化参数，$I$ 是单位矩阵。正则化参数用于解决当 Hessian 不是正定的并且存在对应小负特征值的方向时的问题。
- en: '![Refer to caption](img/754bc662b0ebe9d2bc5604e405ad9276.png)![Refer to caption](img/d294179d76c1f857b0fdb4b1c13f1cd6.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/754bc662b0ebe9d2bc5604e405ad9276.png)![参见说明文字](img/d294179d76c1f857b0fdb4b1c13f1cd6.png)'
- en: Figure 13\. Eigenvalue spectrum in a 4 layer shared weights network. Source:Yann
    LeCun ([b1,](#bib.bib1) )
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13\. 四层共享权重网络中的特征值谱。来源：Yann LeCun ([b1,](#bib.bib1) )
- en: 3.6\. Conjugate Gradients
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. 共轭梯度
- en: The conjugate gradient ([b55,](#bib.bib55) ; [b56,](#bib.bib56) ) optimization
    is an iterative method that utilizes the conjugate directions i.e. orthogonal
    directions to perform a gradient update step and thus avoids the explicit computation
    of the inverse of the Hessian. This approach is used to overcome the weakness
    of the gradient descent method by aligning the current search direction along
    the current gradient step with a memory component given by the linear combination
    of previous directions. For a quadratic function, the gradient descent method
    fails to exploit the curvature information present in the Hessian matrix, wherein
    it descends down the canyon iteratively by following a zig-zag pattern and wastes
    most of its time by repeatedly hitting the canyon walls and makes no significant
    progress. This happens because each line search direction is orthogonal to the
    previous direction since the minimum of the objective along a given direction
    corresponds to the steepest descent direction at that point.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 共轭梯度优化（[b55,](#bib.bib55) ; [b56,](#bib.bib56)）是一种迭代方法，利用共轭方向即正交方向来执行梯度更新步骤，从而避免显式计算Hessian矩阵的逆。这种方法用于克服梯度下降法的不足，通过将当前搜索方向与当前梯度步骤对齐，并通过前几次方向的线性组合提供一个记忆组件。对于二次函数，梯度下降法未能利用Hessian矩阵中存在的曲率信息，它通过遵循之字形模式迭代地下降到峡谷中，并通过反复撞击峡谷墙壁浪费了大部分时间，没有取得显著进展。这是因为每个线搜索方向都与之前的方向正交，因为在给定方向上的目标最小值对应于该点的最陡下降方向。
- en: Thus, this descent direction does not preserve the progress in the previous
    direction and will undo the progress in that direction, and we need to reiterate
    to minimize the progress made in the previous direction. Thus, we obtain an ineffective
    zig-zag pattern of progress towards the optimum, and because of the large step
    size, it often overshoots the minima. The conjugate gradients address this problem
    by making the search directions orthogonal, i.e., the current search direction
    is conjugate to the previous line search direction, and it will not spoil the
    progress made in the previous iteration.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种下降方向不会保留之前方向上的进展，并会撤销在那个方向上的进展，我们需要重新迭代以最小化在之前方向上取得的进展。因此，我们会得到一个低效的向最优点进展的之字形模式，由于步长过大，它通常会超越最小值。共轭梯度法通过使搜索方向正交来解决这个问题，即当前搜索方向与之前的线搜索方向共轭，它不会破坏在上一次迭代中取得的进展。
- en: 'This method is only suitable for solving positive definite systems, i.e. when
    the Hessian has all positive eigenvalues. The orthogonality condition rightly
    finds the minimum along the direction $d_{i}$ since the eigenvector $e_{i+1}$
    is orthogonal to $d_{i}$. Two directions $d_{t-1}$ and $d_{t}$ are defined as
    conjugate if it satisfies:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法仅适用于解决正定系统，即Hessian矩阵具有所有正特征值时。正交性条件可以正确找到沿方向 $d_{i}$ 的最小值，因为特征向量 $e_{i+1}$
    与 $d_{i}$ 正交。如果两个方向 $d_{t-1}$ 和 $d_{t}$ 满足以下条件，则它们被定义为共轭方向：
- en: '|  | $\boldsymbol{d}_{t}^{\top}\boldsymbol{H}\boldsymbol{d}_{t-1}=0$ |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{d}_{t}^{\top}\boldsymbol{H}\boldsymbol{d}_{t-1}=0$ |  |'
- en: where $H$ is the Hessian matrix. Thus, this method is an example of Gram-Schmidt
    ortho-normalization, converging in atmost $k$ line searches. At training iteration
    $t$, the current search direction $d_{t}$ is computed as discussed in Algorithm
    [4](#algorithm4 "In 3.7\. BFGS ‣ 3\. Second-order methods ‣ A survey of deep learning
    optimizers - first and second order methods").
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H$ 是Hessian矩阵。因此，该方法是Gram-Schmidt正交归一化的一个示例，最多在 $k$ 次线搜索中收敛。在训练迭代 $t$ 时，当前搜索方向
    $d_{t}$ 按照算法 [4](#algorithm4 "In 3.7\. BFGS ‣ 3\. Second-order methods ‣ A survey
    of deep learning optimizers - first and second order methods") 中讨论的方法计算。
- en: '1 Initialize: Initial point $x_{0}$. $r_{0}=d_{0}$.2 while *$r_{k}\neq 0$* do3      
    Set $r_{0}\leftarrow Ax_{0}-b_{0}$.4       $\alpha_{i}\leftarrow\frac{r_{i}^{T}r_{i}}{d_{i}^{T}Ad_{i}}$.5      
    $x_{i+1}\leftarrow x_{i}+\alpha_{i}d_{i}$.6       $r_{i+1}\leftarrow r_{i}-\alpha_{i}Ad_{i}$.7      
    $\beta_{i+1}\leftarrow\frac{r_{i+1}^{T}r_{i+1}}{r_{i}^{T}r_{i}}$.8       $d_{i+1}\leftarrow
    r_{i+1}+\beta_{i+1}d_{i}$9 end while'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 1 初始化：初始点 $x_{0}$。 $r_{0}=d_{0}$。2 当 *$r_{k}\neq 0$* 时执行3       设 $r_{0}\leftarrow
    Ax_{0}-b_{0}$。4       $\alpha_{i}\leftarrow\frac{r_{i}^{T}r_{i}}{d_{i}^{T}Ad_{i}}$。5       $x_{i+1}\leftarrow
    x_{i}+\alpha_{i}d_{i}$。6       $r_{i+1}\leftarrow r_{i}-\alpha_{i}Ad_{i}$。7       $\beta_{i+1}\leftarrow\frac{r_{i+1}^{T}r_{i+1}}{r_{i}^{T}r_{i}}$。8       $d_{i+1}\leftarrow
    r_{i+1}+\beta_{i+1}d_{i}$9 结束循环
- en: Algorithm 3 Conjugate Gradient Method
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 共轭梯度法
- en: 'The parameter $\beta_{t}$ controls how much of the previous direction should
    contribute to the current search direction. Since, this line search method utilizes
    the eigenvectors of H to choose $\beta_{t}$ which could be computationally expensive,
    we mainly use two popular methods ([b57,](#bib.bib57) ; [b58,](#bib.bib58) ) for
    computing $\beta_{t}$ and they are as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 参数$\beta_{t}$控制了之前的方向应该在当前搜索方向中贡献多少。由于这种线搜索方法利用H的特征向量来选择$\beta_{t}$，这可能计算上较为昂贵，我们主要使用两种流行的方法([b57,](#bib.bib57);
    [b58,](#bib.bib58))来计算$\beta_{t}$，它们如下：
- en: '1\. The Fletcher-Reeves:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. Fletcher-Reeves：
- en: '|  | $\beta_{t}=\frac{\nabla_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}_{t}\right)^{\top}\nabla_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}_{t}\right)}{\nabla_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}_{t-1}\right)^{\top}\nabla_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}_{t-1}\right)}$
    |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\beta_{t}=\frac{\nabla_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}_{t}\right)^{\top}\nabla_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}_{t}\right)}{\nabla_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}_{t-1}\right)^{\top}\nabla_{\boldsymbol{\theta}}J\left(\boldsymbol{\theta}_{t-1}\right)}$
    |  |'
- en: '2\. The Polak-Ribiere:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. Polak-Ribiere：
- en: '|  | $\beta_{t}=\frac{\left(\nabla_{\theta}J\left(\theta_{t}\right)-\nabla_{\theta}J\left(\theta_{t-1}\right)\right)^{\top}\nabla_{\theta}J\left(\theta_{t}\right)}{\nabla_{\theta}J\left(\theta_{t-1}\right)^{\top}\nabla_{\theta}J\left(\theta_{t-1}\right)}$
    |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\beta_{t}=\frac{\left(\nabla_{\theta}J\left(\theta_{t}\right)-\nabla_{\theta}J\left(\theta_{t-1}\right)\right)^{\top}\nabla_{\theta}J\left(\theta_{t}\right)}{\nabla_{\theta}J\left(\theta_{t-1}\right)^{\top}\nabla_{\theta}J\left(\theta_{t-1}\right)}$
    |  |'
- en: For non-linear conjugate gradient method, we set $\beta_{t}$ to zero i.e., it
    restarts at each step by forgetting the past search directions with the current
    direction given by the unaltered gradient at that point, to ensure that it is
    locally optimal and ensures faster convergence. Also, the initial point has a
    strong influence on the number of the steps taken for convergence, and it is observed
    that the Fletcher-Reeves method converges only if the initial point is sufficiently
    close to the desired minima and thus, the Polak-Ribiere method converges much
    more quickly.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非线性共轭梯度法，我们将$\beta_{t}$设为零，即每一步都通过遗忘过去的搜索方向而从当前方向开始，当前方向由该点的未改变梯度给出，以确保其在局部最优，并确保更快的收敛。同时，初始点对收敛所需的步骤数有很大影响，并且观察到，Fletcher-Reeves方法仅在初始点足够接近所需的最小值时才收敛，因此，Polak-Ribiere方法收敛得更快。
- en: '![Refer to caption](img/d98feb082827234df552400249fe2e11.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d98feb082827234df552400249fe2e11.png)'
- en: Figure 14\. Illustration of various optimization methods for various quadratic
    functions. The yellow dot indicates the starting point. Source Pascanu ([b5,](#bib.bib5)
    ).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图14\. 不同优化方法对各种二次函数的示意图。黄色点表示起点。来源：Pascanu ([b5,](#bib.bib5)).
- en: 3.7\. BFGS
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7\. BFGS
- en: The Quasi-Newton Broyden-Fletcher-Goldfarb-Shanno (BFGS) ([b59,](#bib.bib59)
    ) method iteratively builds an approximation of the inverse of the Hessian at
    each step. The BFGS incorporates the second-order information using a low-rank
    approximation of the Hessian and is thus similar to the method of conjugate gradients
    without the computational burden. Here the explicit calculation of the Hessian
    is not required since we now use the secant equation to construct the approximation
    matrix by successively analyzing the gradient vector.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Quasi-Newton Broyden-Fletcher-Goldfarb-Shanno (BFGS) ([b59,](#bib.bib59)) 方法在每一步迭代地构建Hessian矩阵的逆的近似。BFGS使用Hessian的低秩近似来融入二阶信息，因此类似于共轭梯度法，但没有计算负担。这里不需要显式计算Hessian，因为我们现在使用割线方程通过连续分析梯度向量来构建近似矩阵。
- en: '1 Initialize: Initial point $x_{0}$, inverse Hessian approximation $B_{0}$,
    where $B_{0}$ is positive definite.2 for *$k=0,1,2,\ldots$* do3       Determine
    the search direction $p_{k}$ by solving $B_{k}\mathbf{p}_{k}=-\nabla f\left(\mathbf{x}_{k}\right)$.4      
    A line search is performed in this direction to determine the step size $\varepsilon_{k}^{*}$,
    satisfying the Wolfe5       conditions.6       Define $s_{k}=x_{k+1}-x_{k}$ and
    $y_{k}=\nabla f(x_{k+1})-\nabla f(x_{k})$.7       Apply Update $x_{k+1}\leftarrow
    x_{k}+s_{k}$.8       $B_{k+1}=\left(I-\rho_{k}s_{k}y_{k}^{T}\right)B_{k}\left(I-\rho_{k}y_{k}s_{k}^{T}\right)+\rho_{k}s_{k}s_{k}^{T}$,
    where $\rho_{k}=\frac{1}{y_{k}^{T}s_{k}}$.9 end for'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 1 初始化：初始点$x_{0}$，逆Hessian近似$B_{0}$，其中$B_{0}$是正定的。2 对于 *$k=0,1,2,\ldots$* 执行3       通过求解$B_{k}\mathbf{p}_{k}=-\nabla
    f\left(\mathbf{x}_{k}\right)$确定搜索方向$p_{k}$。4       在这个方向上进行线搜索以确定步长$\varepsilon_{k}^{*}$，满足Wolfe5       条件。6       定义$s_{k}=x_{k+1}-x_{k}$
    和 $y_{k}=\nabla f(x_{k+1})-\nabla f(x_{k})$。7       应用更新 $x_{k+1}\leftarrow x_{k}+s_{k}$。8       $B_{k+1}=\left(I-\rho_{k}s_{k}y_{k}^{T}\right)B_{k}\left(I-\rho_{k}y_{k}s_{k}^{T}\right)+\rho_{k}s_{k}s_{k}^{T}$，其中
    $\rho_{k}=\frac{1}{y_{k}^{T}s_{k}}$。9 结束循环
- en: Algorithm 4 BFGS Method
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 BFGS 方法
- en: 'We perform a series of line searches along this direction, and since it takes
    more steps to converge due to the lack of precision in the approximation of the
    true inverse Hessian i.e., $H_{k}=B_{k}^{-1}$, as illustrated in Algorithm [3](#algorithm3
    "In 3.6\. Conjugate Gradients ‣ 3\. Second-order methods ‣ A survey of deep learning
    optimizers - first and second order methods"). The computational complexity for
    the BFGS method is a $O\left(N^{2}\right)$ since matrix inversion is not required.
    The rank-two approximation in Alg [3](#algorithm3 "In 3.6\. Conjugate Gradients
    ‣ 3\. Second-order methods ‣ A survey of deep learning optimizers - first and
    second order methods") can be considered as a diagonal and a low-rank approximation
    to the Hessian $H_{k}$. Since it does not utilize only second-order information,
    it is sometimes more efficient than the Newton’s method for non-quadratic functions
    with super-linear rate of convergence and the cost per iteration is usually lower.
    Moreover, it is estimated that BFGS has self-correcting properties, i.e., when
    the previous estimate is bad and slows down the convergence, then BFGS will correct
    itself in the next few steps. We mainly compute the approximation to the Hessian
    matrix $H_{k}$ using: 1\. Finite difference method 2\. Hessian-vector products.
    3\. Diagonal Computation. 4\. Square Jacobian approximation.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们沿着这个方向进行了一系列的线搜索，由于缺乏对真实逆Hessian，即$H_{k}=B_{k}^{-1}$的精确近似，收敛需要更多的步骤，如算法 [3](#algorithm3
    "在 3.6\. 共轭梯度 ‣ 3\. 二阶方法 ‣ 深度学习优化器调查 - 一阶与二阶方法") 所示。BFGS方法的计算复杂度是$O\left(N^{2}\right)$，因为不需要矩阵求逆。算法
    [3](#algorithm3 "在 3.6\. 共轭梯度 ‣ 3\. 二阶方法 ‣ 深度学习优化器调查 - 一阶与二阶方法")中的二阶近似可以被视为对Hessian
    $H_{k}$的对角和低秩近似。由于它不只利用二阶信息，有时对于具有超线性收敛速率的非二次函数比牛顿方法更高效，并且每次迭代的成本通常更低。此外，据估计，BFGS具有自我修正的特性，即当之前的估计不佳且减慢收敛时，BFGS将在接下来的几步中自我修正。我们主要使用以下方法来计算Hessian矩阵$H_{k}$的近似：1\.
    有限差分法 2\. Hessian-向量积 3\. 对角计算 4\. 方形雅可比近似。
- en: '![Refer to caption](img/d0a5426910ebb34995b416ef6f566e0e.png)![Refer to caption](img/c827e7063fc090bb5386218d378a7024.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d0a5426910ebb34995b416ef6f566e0e.png)![参见标题](img/c827e7063fc090bb5386218d378a7024.png)'
- en: 'Figure 15\. An example of a radial transformation on a 2-dimensional space
    to illustrate the effect of a local perturbation on the relative flatness between
    the minima. We can see that only the area in blue and red, i.e. inside the ball
    $B_{2}(\hat{\theta},\delta)$, are affected. Here, $\psi(r,\hat{r},\delta,\rho)=\mathbb{1}(r\notin[0,\delta])r+\mathbb{1}(r\in[0,\hat{r}])\rho\frac{r}{\hat{r}}+\mathbb{1}(r\in]\hat{r},\delta])\left((\rho-\delta)\frac{r-\delta}{\hat{r}-\delta}+\delta\right)$
    represents the function under consideration $g^{-1}(\theta)=\frac{\psi\left(\|\theta-\hat{\theta}\|_{2},\hat{r},\delta,\rho\right)}{\|\theta-\hat{\theta}\|_{2}}(\theta-\hat{\theta})+\hat{\theta}$
    $ is the radial-basis transformation respectively. B. Source: Dinh ([b4,](#bib.bib4)
    )'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图15\. 这是一个二维空间中的径向变换示例，用于说明局部扰动对最小值之间相对平坦性的影响。我们可以看到，只有蓝色和红色区域，即球体 $B_{2}(\hat{\theta},\delta)$
    内部受到影响。这里，$\psi(r,\hat{r},\delta,\rho)=\mathbb{1}(r\notin[0,\delta])r+\mathbb{1}(r\in[0,\hat{r}])\rho\frac{r}{\hat{r}}+\mathbb{1}(r\in]\hat{r},\delta])\left((\rho-\delta)\frac{r-\delta}{\hat{r}-\delta}+\delta\right)$
    表示待考虑的函数 $g^{-1}(\theta)=\frac{\psi\left(\|\theta-\hat{\theta}\|_{2},\hat{r},\delta,\rho\right)}{\|\theta-\hat{\theta}\|_{2}}(\theta-\hat{\theta})+\hat{\theta}$
    是径向基变换。来源：Dinh ([b4,](#bib.bib4))
- en: 3.8\. L-BFGS
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8\. L-BFGS
- en: Since the memory costs of storing and manipulating the inverse of the Hessian
    is prohibitively large for deep neural nets, we utilize the L-BFGS ([b60,](#bib.bib60)
    ) method to circumvent this issue. It maintains a simple and compact approximation
    of the Hessian matrices by constructing an Hessian approximation by using only
    the $m$ most recent iteration to incorporate the curvature information, where
    typically $3<m<20$. Thus, instead of storing the fully dense $n^{*}n$ approximators,
    we modify $H_{k}$ by implicitly storing only a certain number of correction pairs
    ($m$) and thus include the curvature information from only the $m$ recent iteration
    (we store only a set of $n$ length vectors). Thus, we add and delete information
    at each stage, discard the past $m$ correction pairs, and start the process with
    the new $s_{k},y_{k}$ pairs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存储和操作Hessian的逆矩阵的内存成本对于深度神经网络来说过于庞大，我们利用L-BFGS ([b60,](#bib.bib60)) 方法来解决这一问题。它通过仅使用最近
    $m$ 次迭代来构建Hessian矩阵的简单紧凑的近似，来维持Hessian矩阵的简单紧凑近似，其中通常 $3<m<20$。因此，我们不再存储完全密集的 $n^{*}n$
    近似，而是通过隐式地存储一定数量的校正对（$m$），从而仅包括来自 $m$ 次最近迭代的曲率信息（我们仅存储一组 $n$ 长度的向量）。因此，我们在每个阶段添加和删除信息，丢弃过去的
    $m$ 个校正对，并用新的 $s_{k},y_{k}$ 对开始新过程。
- en: This ensures to an optimal rate of convergence and may also outperform Hessian-free
    methods, since we only maintain a fixed memory system that iteratively discards
    the curvature information from the past since it is not likely to influence the
    behavior of the Hessian in the current iteration, thus saving computational storage.
    The update rule is exactly similar to the BFGS method as discussed in Algorithm
    [4](#algorithm4 "In 3.7\. BFGS ‣ 3\. Second-order methods ‣ A survey of deep learning
    optimizers - first and second order methods"). The only modification is that we
    discard the vector pair $\left\{s_{k-m},y_{k-m}\right\}$ from storage and the
    $s_{k},y_{k}$ and $x_{k}$ updates follows from Algorithm [4](#algorithm4 "In 3.7\.
    BFGS ‣ 3\. Second-order methods ‣ A survey of deep learning optimizers - first
    and second order methods"). The main weakness of L-BFGS is that it converges slowly
    for functions, where the Hessian is ill-conditioned and contains a wide distribution
    of eigenvalues.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了最优的收敛速度，并且可能超越了无Hessian方法，因为我们只维持一个固定的记忆系统，迭代性地丢弃过去的曲率信息，因为这些信息不太可能影响当前迭代中的Hessian行为，从而节省计算存储。更新规则与BFGS方法完全相同，如算法
    [4](#algorithm4 "在3.7\. BFGS ‣ 3\. 二阶方法 ‣ 深度学习优化器综述 - 一阶和二阶方法")中所述。唯一的修改是，我们丢弃存储中的向量对
    $\left\{s_{k-m},y_{k-m}\right\}$，而 $s_{k},y_{k}$ 和 $x_{k}$ 的更新遵循算法 [4](#algorithm4
    "在3.7\. BFGS ‣ 3\. 二阶方法 ‣ 深度学习优化器综述 - 一阶和二阶方法")。L-BFGS 的主要弱点是，对于Hessian条件不良且包含广泛特征值分布的函数，它的收敛速度较慢。
- en: 3.9\. ADAHESSIAN
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.9\. ADAHESSIAN
- en: '![Refer to caption](img/c1eab69a6073c1cf1fc5703815fe0537.png)![Refer to caption](img/df569cec0092eee54301cc6efd92aa32.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c1eab69a6073c1cf1fc5703815fe0537.png)![参见说明](img/df569cec0092eee54301cc6efd92aa32.png)'
- en: 'Figure 16\. Illustration of the local and global curvature wherein the local
    curvature information is avoided using an exponential moving average. Adahessian
    converges in $1000$ iterations without a moving average, while it takes only seven
    iterations with the moving average enabled. Source: Zhewei Yao ([b34,](#bib.bib34)
    )'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16\. 局部和全局曲率的示意图，其中局部曲率信息通过指数移动平均避免。Adahessian 在没有移动平均的情况下需要 $1000$ 次迭代才能收敛，而启用移动平均仅需七次迭代。来源：Zhewei
    Yao ([b34,](#bib.bib34) )
- en: 'Adahessian ([b34,](#bib.bib34) ) is a second-order stochastic approximation
    method that incorporates the curvature information present in the Hessian for
    faster convergence and reduced memory overhead using:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Adahessian ([b34,](#bib.bib34) ) 是一种二阶随机近似方法，它利用 Hessian 中存在的曲率信息，以加快收敛速度并减少内存开销，使用：
- en: 1\. Hessian diagonal approximation using Hutchinson’s method.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 使用 Hutchinson 方法的 Hessian 对角近似。
- en: 2\. RMSE exponential moving average to reduce the variations in the diagonal
    approximation of the Hessian.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. RMSE 指数移动平均以减少 Hessian 对角近似的变化。
- en: 3\. Block diagonal averaging to reduce the variance of Hessian diagonal elements.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 块对角平均以减少 Hessian 对角元素的方差。
- en: 'As discussed earlier, second-order methods involve preconditioning the gradient
    with the inverse of the Hessian, which is designed to accelerate learning for
    ill-conditioned problems i.e., flat curvatures in some directions and sharp curvature
    in other directions by rescaling the gradient vector along each of its directions.
    It handles extreme curvature directions by appropriate gradient rotation and scaling
    and ensures convergence to a critical point of any sort (local minima). It is
    thus more reliable than other adaptive first-order methods. But this comes at
    a prohibitive cost since it is computationally infeasible to compute the Hessian
    at each iteration and not scalable for modern neural network architectures. Thus,
    we approximate the Hessian matrix as a diagonal operator given as:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，二阶方法涉及用 Hessian 的逆来预处理梯度，旨在加速学习以解决病态问题，即在某些方向上曲率平坦，在其他方向上曲率陡峭，通过沿每个方向重新缩放梯度向量来处理极端曲率方向，并通过适当的梯度旋转和缩放确保收敛到任何类型的临界点（局部最小值）。因此，它比其他自适应一阶方法更可靠。但这带来了难以承受的成本，因为在每次迭代中计算
    Hessian 是计算上不可行的，并且对现代神经网络架构不具有可扩展性。因此，我们将 Hessian 矩阵近似为对角算子，表示为：
- en: '|  | $\Delta w=\operatorname{diag}(\mathbf{H})^{-k}\mathbf{g}$ |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta w=\operatorname{diag}(\mathbf{H})^{-k}\mathbf{g}$ |  |'
- en: 'where $\Delta w$ is the weight update and $0\leq k\leq 1$ is the Hessian power.
    The Hessian diagonal denoted as $D=\operatorname{diag}(\mathrm{H})$ is computed
    using Hutchinson’s method. To obtain the Hessian matrix $H$ without explicit computation,
    they utilize the Hessian-free approximation method given as:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Delta w$ 是权重更新，$0\leq k\leq 1$ 是 Hessian 的幂。Hessian 对角线表示为 $D=\operatorname{diag}(\mathrm{H})$，使用
    Hutchinson 方法计算。为了在不进行显式计算的情况下获得 Hessian 矩阵 $H$，他们利用 Hessian 自由近似方法，其表示为：
- en: '|  | $\frac{\partial\mathbf{g}^{T}z}{\partial\theta}=\frac{\partial\mathbf{g}^{T}}{\partial\theta}z+\mathbf{g}^{T}\frac{\partial
    z}{\partial\theta}=\frac{\partial\mathbf{g}^{T}}{\partial\theta}z=\mathbf{H}z$
    |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial\mathbf{g}^{T}z}{\partial\theta}=\frac{\partial\mathbf{g}^{T}}{\partial\theta}z+\mathbf{g}^{T}\frac{\partial
    z}{\partial\theta}=\frac{\partial\mathbf{g}^{T}}{\partial\theta}z=\mathbf{H}z$
    |  |'
- en: where $z$ is a random vector with Rademacher distribution. The Hessian diagonal
    is given as the expectation of $z\odot Hz$ which corresponds to optimal performance
    on various tasks.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $z$ 是具有 Rademacher 分布的随机向量。Hessian 对角线作为 $z\odot Hz$ 的期望值给出，这对应于各种任务的最佳性能。
- en: '|  | $\boldsymbol{D}=\operatorname{diag}(\mathbf{H})=\mathbb{E}[z\odot(\mathbf{H}z)]$
    |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{D}=\operatorname{diag}(\mathbf{H})=\mathbb{E}[z\odot(\mathbf{H}z)]$
    |  |'
- en: This has the computational overhead equivalent to a gradient backpropagation
    step. They employ a moving-average term (spatial averaging) and Hessian momentum
    (the update rule is similar to Adam) to smooth out stochastic variations in the
    curvature information per iteration. This leads to faster convergence for strict
    and smooth convex functions.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这具有与梯度反向传播步骤相当的计算开销。他们使用移动平均项（空间平均）和 Hessian 动量（更新规则类似于 Adam）来平滑每次迭代中的曲率信息的随机变化。这导致严格和平滑凸函数的更快收敛。
- en: '|  | $D^{(s)}[ib+j]=\frac{\sum_{k=1}^{b}D[ib+k]}{b},\text{ for }1\leq j\leq
    b,0\leq i\leq\frac{d}{b}-1$ |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $D^{(s)}[ib+j]=\frac{\sum_{k=1}^{b}D[ib+k]}{b},\text{ for }1\leq j\leq
    b,0\leq i\leq\frac{d}{b}-1$ |  |'
- en: where $D^{(s)}$ is spatially averaged Hessian diagonal and $b$ is the block
    size. We implement momentum for the Hessian diagonal $H$ with spatial averaging
    as shown in Table [1](#S3.T1 "Table 1 ‣ 3.9\. ADAHESSIAN ‣ 3\. Second-order methods
    ‣ A survey of deep learning optimizers - first and second order methods").
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D^{(s)}$ 是空间平均的 Hessian 对角线，$b$ 是块大小。我们对 Hessian 对角线 $H$ 实现了动量，并进行了空间平均，如表
    [1](#S3.T1 "Table 1 ‣ 3.9\. ADAHESSIAN ‣ 3\. Second-order methods ‣ A survey of
    deep learning optimizers - first and second order methods") 所示。
- en: '| Term | Equation |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 项目 | 方程 |'
- en: '| --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $\overline{\boldsymbol{D}}_{t}$ | $\sqrt{\frac{\left(1-\beta_{2}\right)\sum_{i=1}^{t}\beta_{2}^{t-i}\boldsymbol{D}_{i}^{(s)}\boldsymbol{D}_{i}^{(s)}}{1-\beta_{2}^{t}}}$
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| $\overline{\boldsymbol{D}}_{t}$ | $\sqrt{\frac{\left(1-\beta_{2}\right)\sum_{i=1}^{t}\beta_{2}^{t-i}\boldsymbol{D}_{i}^{(s)}\boldsymbol{D}_{i}^{(s)}}{1-\beta_{2}^{t}}}$
    |'
- en: '| $m_{t}$ | $\frac{\left(1-\beta_{1}\right)\sum_{i=1}^{t}\beta_{1}^{t-i}\mathbf{g}_{i}}{1-\beta_{1}^{t}}$
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| $m_{t}$ | $\frac{\left(1-\beta_{1}\right)\sum_{i=1}^{t}\beta_{1}^{t-i}\mathbf{g}_{i}}{1-\beta_{1}^{t}}$
    |'
- en: '| $v_{t}$ | $\left(\overline{\boldsymbol{D}}_{t}\right)^{k}$ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| $v_{t}$ | $\left(\overline{\boldsymbol{D}}_{t}\right)^{k}$ |'
- en: Table 1\. Update Rule
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 更新规则
- en: The parameter update rule is given as, $\theta_{t+1}=\theta_{t}-\eta m_{t}/v_{t}$,
    where $m_{t}$ and $v_{t}$ are the first and second order moments for AdaHessian
    with $0<\beta_{1}<\beta_{2}<1$.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 参数更新规则为 $\theta_{t+1}=\theta_{t}-\eta m_{t}/v_{t}$，其中 $m_{t}$ 和 $v_{t}$ 是 AdaHessian
    的一阶和二阶矩，且 $0<\beta_{1}<\beta_{2}<1$。
- en: '| Optimizer | CIFAR-10 | CIFAR-100 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | CIFAR-10 | CIFAR-100 |'
- en: '| --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SGD | $94.33\pm 0.1$ | $79.12\pm 0.54$ |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| SGD | $94.33\pm 0.1$ | $79.12\pm 0.54$ |'
- en: '| Polyak | $95.62\pm 0.07$ | $78.11\pm 0.37$ |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Polyak | $95.62\pm 0.07$ | $78.11\pm 0.37$ |'
- en: '| Adam | $94.76\pm 0.23$ | $77.24\pm 0.12$ |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Adam | $94.76\pm 0.23$ | $77.24\pm 0.12$ |'
- en: '| LOOKAHEAD | $\textbf{95.63}\pm\textbf{0.03}$ | $\textbf{79.42}\pm\textbf{0.05}$
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| LOOKAHEAD | $\textbf{95.63}\pm\textbf{0.03}$ | $\textbf{79.42}\pm\textbf{0.05}$
    |'
- en: '| Dataset | Cifar10 | ImageNet |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Dataset | Cifar10 | ImageNet |'
- en: '|  | ResNet20 | ResNet32 | ResNet18 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNet20 | ResNet32 | ResNet18 |'
- en: '| SGD | 93.01 $\pm$ 0.05 | $\mathbf{94.23}\pm\mathbf{0.17}$ | 70.01 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| SGD | 93.01 $\pm$ 0.05 | $\mathbf{94.23}\pm\mathbf{0.17}$ | 70.01 |'
- en: '| Adam | 90.73 $\pm$ 0.23 | 91.53 $\pm$ 0.1 | 65.42 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Adam | 90.73 $\pm$ 0.23 | 91.53 $\pm$ 0.1 | 65.42 |'
- en: '| AdamW | 91.96 $\pm$ 0.15 | 92.82 $\pm$ 0.1 | 67.82 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| AdamW | 91.96 $\pm$ 0.15 | 92.82 $\pm$ 0.1 | 67.82 |'
- en: '| ADAHESSIAN | $\mathbf{93.11}\pm\mathbf{0.25}$ | 93.08 $\pm$ 0.10 | $\mathbf{70.12}$
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| ADAHESSIAN | $\mathbf{93.11}\pm\mathbf{0.25}$ | 93.08 $\pm$ 0.10 | $\mathbf{70.12}$
    |'
- en: Figure 17\. (left) We report the test accuracy (+ standard deviation) across
    $10$ trails using different optimizers on the CIFAR-$10/100$ dataset using the
    ResNet$18$ and architecture. (right) We report the model accuracy (+ standard
    deviation) across $10$ trails using different optimizers on the CIFAR-$10/100$
    and ImageNet dataset using the ResNet$20$ and ResNet$32$ architecture. We train
    all our models for $200$ epochs with a batch size of $128$ and an initial learning
    rate of $0.1/0.001/0.01$ for SGD/Adam/AdamW respectively. For Adahessian we set
    block size equal to $9$, $k=1$ and a learning rate of $0.15$; while for Lookahead
    we set $k=5$ and use a learning rate of $0.1$. The exact training details as mentioned
    in ([b34,](#bib.bib34) ; [b61,](#bib.bib61) ) was used for model training.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17\. (左) 我们报告在 CIFAR-$10/100$ 数据集上使用不同优化器的测试准确率（+ 标准差），采用 ResNet$18$ 结构。（右）我们报告在
    CIFAR-$10/100$ 和 ImageNet 数据集上使用不同优化器的模型准确率（+ 标准差），采用 ResNet$20$ 和 ResNet$32$
    结构。我们将所有模型训练 $200$ 个 epoch，批量大小为 $128$，SGD/Adam/AdamW 的初始学习率分别为 $0.1/0.001/0.01$。对于
    Adahessian，我们将块大小设为 $9$，$k=1$，学习率为 $0.15$；对于 Lookahead，我们将 $k=5$，学习率为 $0.1$。训练细节如
    ([b34,](#bib.bib34) ; [b61,](#bib.bib61) ) 所述。
- en: 3.10\. Newton-CG method
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.10\. Newton-CG 方法
- en: 'The line search Newton-CG ([b30,](#bib.bib30) ) method utilizes the Hessian-vector
    products ($\nabla^{2}f_{k}$) has attractive global convergence properties with
    super-linear convergence, and results in optimal search directions when the Hessian
    is indefinite. This Hessian-free (HF) method does not require explicit knowledge
    of the Hessian but uses the finite-difference method to access the actual Hessian
    at each point. Here, we compute the product of the Hessian ($\nabla^{2}f_{k}$)
    and the vector $d$ using a finite-differencing technique to get the approximation:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 线搜索 Newton-CG ([b30,](#bib.bib30) ) 方法利用 Hessian-向量乘积 ($\nabla^{2}f_{k}$) 具有吸引人的全局收敛性特征，具备超线性收敛，并在
    Hessian 不确定时产生最佳搜索方向。此 Hessian-free (HF) 方法不需要明确知道 Hessian，而是使用有限差分方法在每个点访问实际
    Hessian。这里，我们使用有限差分技术计算 Hessian ($\nabla^{2}f_{k}$) 和向量 $d$ 的乘积以获得近似值：
- en: '|  | $\nabla^{2}f_{k}d\approx\frac{\nabla f\left(x_{k}+hd\right)-\nabla f\left(x_{k}\right)}{h}$
    |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla^{2}f_{k}d\approx\frac{\nabla f\left(x_{k}+hd\right)-\nabla f\left(x_{k}\right)}{h}$
    |  |'
- en: for some small differencing interval h. Also, using the Gauss-Newton matrix
    instead of the Hessian results in better search directions since $G$ is always
    positive definite and thus avoids the problem of negative curvature. CG is designed
    for positive-definite systems, and the Hessian is indefinite for points far away
    from the solution; we can terminate early or use negative directions when they
    are detected to account for this. Since the HF approach has no access to the true
    Hessian, we may run across directions of extremely low curvatures or even negative
    curvatures. Thus, the search directions can only result in small function reductions
    even after many function evaluations in line search. If we are not careful, such
    a direction could result in a very large CG step, possibly taking the outside
    of the region where the Newton approximation is even valid. One solution is to
    add a damping parameter to the Hessian given as $\mathrm{B}=\mathrm{H}+\lambda\mathrm{I}$,
    where $I$ is the Identity matrix. This reconditions $H$ and controls the length
    of each CG step, mimicking a trust region method.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些小的差分间隔 h。此外，使用高斯-牛顿矩阵代替海森矩阵能得到更好的搜索方向，因为 $G$ 始终是正定的，从而避免了负曲率的问题。CG 设计用于正定系统，而对于离解的点，海森矩阵是不定的；我们可以在检测到负方向时提前终止或使用负方向来解决这个问题。由于
    HF 方法无法访问真实的海森矩阵，我们可能会遇到极低曲率甚至负曲率的方向。因此，即使在进行许多函数评估的情况下，搜索方向也只能导致函数值的较小减少。如果我们不小心，这样的方向可能会导致非常大的
    CG 步骤，可能会使得离开牛顿近似有效的区域。一个解决方案是给海森矩阵添加一个阻尼参数，设为 $\mathrm{B}=\mathrm{H}+\lambda\mathrm{I}$，其中
    $I$ 是单位矩阵。这可以重新调整 $H$，并控制每个 CG 步骤的长度，类似于信赖域方法。
- en: '|  | $\rho_{k}=\frac{f\left(x_{k}+p_{k}\right)-f\left(x_{k}\right)}{q_{k}\left(p_{k}\right)-q_{k}(0)}.$
    |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\rho_{k}=\frac{f\left(x_{k}+p_{k}\right)-f\left(x_{k}\right)}{q_{k}\left(p_{k}\right)-q_{k}(0)}.$
    |  |'
- en: '|  | $\text{ If }\rho_{k}<\frac{1}{4},\text{ then }\lambda\leftarrow\alpha\lambda.\hskip
    3.99994pt\text{ If}\hskip 5.0pt\rho_{k}>\frac{3}{4},\text{ then }\lambda\leftarrow\alpha^{-1}\lambda.$
    |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{ 如果 }\rho_{k}<\frac{1}{4},\text{ 那么 }\lambda\leftarrow\alpha\lambda.\hskip
    3.99994pt\text{ 如果}\hskip 5.0pt\rho_{k}>\frac{3}{4},\text{ 那么 }\lambda\leftarrow\alpha^{-1}\lambda.$
    |  |'
- en: The main drawback of this method is when $\nabla f(x_{k})$ is nearly singular,
    and thus requiring many function evaluations without significant reduction in
    the function values. To alleviate this difficulty, trust-region Newton-CG method
    and Newton-Lanczos method can be used respectively.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要缺点是当 $\nabla f(x_{k})$ 几乎是奇异的时候，通常需要进行许多函数评估而没有显著减少函数值。为了缓解这个困难，可以分别使用信赖域牛顿-CG方法和牛顿-兰索斯方法。
- en: 3.11\. Lookahead
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.11\. Lookahead
- en: 'Lookahead ([b61,](#bib.bib61) ) optimizer is orthogonal to the previous approaches
    and proposes a distinct way for weight updates in which it maintains two sets
    of weights namely, slow weights ($\phi$) and fast weights ($\theta)$. It maintains
    an inner loop of optimization for $k$ steps using standard methods such as the
    SGD or Adam for iterative fast weights update and follows it up with a linear
    interpolation of the slow weights along the direction given as $\theta-\phi$ in
    the weight space. Thus, after each slow weights update, we reset it as the new
    fast weights, making rapid progress in low curvature directions with reduced oscillations
    and quicker convergence. The slow weights trajectory is characterized as an exponential
    moving average i.e., Polyak averaging has strong theoretical guarantees of the
    fast weights. After $k$ inner-loop steps, the slow weights is updated as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Lookahead ([b61,](#bib.bib61)) 优化器与之前的方法正交，并提出了一种不同的权重更新方式，其中它维持两组权重，即慢速权重（$\phi$）和快速权重（$\theta$）。它在内循环中使用标准方法（如
    SGD 或 Adam）进行 $k$ 步的优化，以迭代更新快速权重，然后进行沿权重空间中的 $\theta-\phi$ 方向的慢速权重线性插值。因此，在每次更新慢速权重后，我们将其重置为新的快速权重，从而在低曲率方向上迅速进展，减少振荡并加快收敛。慢速权重轨迹被描述为指数移动平均，即
    Polyak 平均，对快速权重有很强的理论保证。在 $k$ 次内循环步骤后，慢速权重更新如下：
- en: '|  | $\displaystyle\phi_{t+1}$ | $\displaystyle=\phi_{t}+\alpha\left(\theta_{t,k}-\phi_{t}\right)$
    |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi_{t+1}$ | $\displaystyle=\phi_{t}+\alpha\left(\theta_{t,k}-\phi_{t}\right)$
    |  |'
- en: '|  |  | $\displaystyle=\alpha\left[\theta_{t,k}+(1-\alpha)\theta_{t-1,k}+\ldots+(1-\alpha)^{t-1}\theta_{0,k}\right]+(1-\alpha)^{t}\phi_{0}$
    |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\alpha\left[\theta_{t,k}+(1-\alpha)\theta_{t-1,k}+\ldots+(1-\alpha)^{t-1}\theta_{0,k}\right]+(1-\alpha)^{t}\phi_{0}$
    |  |'
- en: 'where $\alpha$ is the learning rate for slow weights $\phi$. For a choice of
    optimizer $A$ and a mini-batch of $d$ data points the update rule for fast weights
    is:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是慢速权重 $\phi$ 的学习率。对于优化器 $A$ 和一个包含 $d$ 数据点的小批量，快速权重的更新规则为：
- en: '|  | $\theta_{t,i+1}=\theta_{t,i}+A\left(L,\theta_{t,i-1},d\right)$ |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{t,i+1}=\theta_{t,i}+A\left(L,\theta_{t,i-1},d\right)$ |  |'
- en: 4\. Conclusion
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 结论
- en: We summarize first-order and second-order optimization methods from a theoretical
    viewpoint using the optimization literature. Since high error, local minima traps
    do not appear when the model is overparameterized, and local minima with high
    error relative to the global minimum occur with an exponentially small probability
    in $N$. Critical points with high cost are far more likely to be saddle points.
    Also, since second-order methods are computationally intensive, the computational
    complexity of computing the Hessian is $O(N^{3})$, and thus practically only networks
    with a small number of parameters can be trained via Newton’s method.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从理论视角总结了一阶和二阶优化方法。由于高错误率，本地极小值陷阱在模型过参数化时不会出现，且相对于全局最小值的高错误率的局部极小值在$N$中出现的概率呈指数级小。高成本的临界点更可能是鞍点。此外，由于二阶方法计算密集，计算赫西矩阵的计算复杂度为$O(N^{3})$，因此实际中只有少量参数的网络可以通过牛顿法进行训练。
- en: 5\. Acknowledgments
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 致谢
- en: We thank the anonymous reviewers for their review and suggestions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢匿名评审员的审阅和建议。
- en: References
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '(1) LeCun, Y. A., Bottou, L., Orr, G. B., and Muller, K. R. (2012). Efficient
    backprop. In Neural networks: Tricks of the trade (pp. 9-48). Springer, Berlin,
    Heidelberg.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) LeCun, Y. A., Bottou, L., Orr, G. B., 和 Muller, K. R. (2012). 高效的反向传播。在《神经网络：技巧与诀窍》
    (第9-48页). Springer, Berlin, Heidelberg.
- en: (2) Lee, J. D., Simchowitz, M., Jordan, M. I., and Recht, B. (2016, June). Gradient
    descent only converges to minimizers. In Conference on learning theory (pp. 1246-1257).
    PMLR.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) Lee, J. D., Simchowitz, M., Jordan, M. I., 和 Recht, B. (2016年6月). 梯度下降法仅收敛于最小值。在学习理论会议
    (第1246-1257页). PMLR.
- en: (3) Sagun, L., Guney, V. U., Arous, G. B., and LeCun, Y. (2014). Explorations
    on high dimensional landscapes. arXiv preprint arXiv:1412.6615.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) Sagun, L., Guney, V. U., Arous, G. B., 和 LeCun, Y. (2014). 高维景观的探索。arXiv
    预印本 arXiv:1412.6615。
- en: (4) Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. (2017, July). Sharp minima
    can generalize for deep nets. In International Conference on Machine Learning
    (pp. 1019-1028). PMLR.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) Dinh, L., Pascanu, R., Bengio, S., 和 Bengio, Y. (2017年7月). 锐化极小值可以为深度网络泛化。在国际机器学习会议
    (第1019-1028页). PMLR.
- en: (5) Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio,
    Y. (2014). Identifying and attacking the saddle point problem in high-dimensional
    non-convex optimization. Advances in neural information processing systems, 27.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., 和 Bengio,
    Y. (2014). 识别和解决高维非凸优化中的鞍点问题。《神经信息处理系统进展》，27。
- en: (6) Martens, James. ”Deep learning via hessian-free optimization.” ICML. Vol.
    27\. 2010.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) Martens, James. “通过无赫西优化的深度学习。” ICML. 第27卷\. 2010年。
- en: (7) Pascanu, Razvan, et al. ”On the saddle point problem for non-convex optimization.”
    arXiv preprint arXiv:1405.4604 (2014).
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) Pascanu, Razvan, 等. “非凸优化的鞍点问题。” arXiv 预印本 arXiv:1405.4604 (2014)。
- en: (8) Fort, Stanislav, and Stanislaw Jastrzebski. ”Large scale structure of neural
    network loss landscapes.” Advances in Neural Information Processing Systems 32
    (2019).
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) Fort, Stanislav, 和 Stanislaw Jastrzebski. “神经网络损失景观的大规模结构。” 《神经信息处理系统进展》
    32 (2019)。
- en: (9) Sutskever, Ilya, et al. ”On the importance of initialization and momentum
    in deep learning.” International conference on machine learning. PMLR, 2013.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) Sutskever, Ilya, 等. “深度学习中初始化和动量的重要性。” 国际机器学习会议。PMLR, 2013。
- en: (10) Moré, Jorge J., and Danny C. Sorensen. Newton’s method. No. ANL-82-8\.
    Argonne National Lab., IL (USA), 1982\.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) Moré, Jorge J., 和 Danny C. Sorensen. 牛顿法。编号 ANL-82-8\. Argonne 国家实验室, IL
    (USA), 1982\。
- en: (11) Fletcher, Roger. Practical methods of optimization. John Wiley and Sons,
    2013.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) Fletcher, Roger. 实用的优化方法。John Wiley and Sons, 2013。
- en: (12) Choromanska, Anna, et al. ”The loss surfaces of multilayer networks.” Artificial
    intelligence and statistics. PMLR, 2015.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) Choromanska, Anna, 等. “多层网络的损失表面。” 人工智能与统计学。PMLR, 2015。
- en: (13) Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F. (2018, July).
    Essentially no barriers in neural network energy landscape. In International conference
    on machine learning (pp. 1309-1318). PMLR.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (13) Draxler, F., Veschgini, K., Salmhofer, M., 和 Hamprecht, F. (2018年7月). 神经网络能量景观中基本没有障碍。在国际机器学习会议
    (第1309-1318页). PMLR.
- en: (14) Li, Chunyuan, et al. ”Measuring the intrinsic dimension of objective landscapes.”
    arXiv preprint arXiv:1804.08838 (2018).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (14) Li, Chunyuan 等. ”测量目标景观的内在维度。” arXiv 预印本 arXiv:1804.08838 (2018)。
- en: (15) Jastrzębski, Stanisław, et al. ”Three factors influencing minima in sgd.”
    arXiv preprint arXiv:1711.04623 (2017).
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) Jastrzębski, Stanisław 等. ”影响 SGD 最小值的三个因素。” arXiv 预印本 arXiv:1711.04623
    (2017)。
- en: '(16) Yurii, Nesterov. ”Introductory lectures on convex optimization: a basic
    course.” (2004).'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Yurii, Nesterov. ”《凸优化入门讲义：基础课程》。” (2004)。
- en: (17) Alger, Nicholas Vieau. Data-scalable Hessian preconditioning for distributed
    parameter PDE-constrained inverse problems. Diss. 2019.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (17) Alger, Nicholas Vieau. 数据可扩展 Hessian 预处理用于分布式参数 PDE 约束的逆问题。博士论文, 2019。
- en: '(18) Kingma, Diederik P., and Jimmy Ba. ”Adam: A method for stochastic optimization.”
    arXiv preprint arXiv:1412.6980 (2014).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(18) Kingma, Diederik P. 和 Jimmy Ba. ”Adam: 一种用于随机优化的方法。” arXiv 预印本 arXiv:1412.6980
    (2014)。'
- en: '(19) Shewchuk, Jonathan Richard. ”An introduction to the conjugate gradient
    method without the agonizing pain.” (1994): 1.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(19) Shewchuk, Jonathan Richard. ”一种无痛的共轭梯度法介绍。” (1994): 1。'
- en: '(20) Gill, Philip E., and Walter Murray. ”Newton-type methods for unconstrained
    and linearly constrained optimization.” Mathematical Programming 7.1 (1974): 311-350.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(20) Gill, Philip E. 和 Walter Murray. ”用于无约束和线性约束优化的牛顿型方法。” 《数学编程》 7.1 (1974):
    311-350。'
- en: '(21) Roger Grosse and Jimmy Ba. ”CSC 421/2516 Lectures 7–8: Optimization”.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) Roger Grosse 和 Jimmy Ba. ”CSC 421/2516 第 7-8 讲：优化”。
- en: (22) Boyd, Stephen, Stephen P. Boyd, and Lieven Vandenberghe. Convex optimization.
    Cambridge university press, 2004.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (22) Boyd, Stephen, Stephen P. Boyd 和 Lieven Vandenberghe. 《凸优化》。剑桥大学出版社, 2004。
- en: (23) Ian Goodfellow, Yoshua Bengio, Aaron Courville. ”Deep Learning”, (2016).
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) Ian Goodfellow, Yoshua Bengio 和 Aaron Courville. ”深度学习”， (2016)。
- en: '(24) Luenberger, David G., and Yinyu Ye. Linear and nonlinear programming.
    Vol. 2\. Reading, MA: Addison-wesley, 1984.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(24) Luenberger, David G. 和 Yinyu Ye. 《线性与非线性编程》。第 2 卷。Reading, MA: Addison-Wesley,
    1984。'
- en: '(25) Moré, Jorge J., and Danny C. Sorensen. ”On the use of directions of negative
    curvature in a modified Newton method.” Mathematical Programming 16.1 (1979):
    1-20.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(25) Moré, Jorge J. 和 Danny C. Sorensen. ”在修改的牛顿方法中使用负曲率方向。” 《数学编程》 16.1 (1979):
    1-20。'
- en: '(26) Gill, Philip E., and Walter Murray. ”Newton-type methods for unconstrained
    and linearly constrained optimization.” Mathematical Programming 7.1 (1974): 311-350.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(26) Gill, Philip E. 和 Walter Murray. ”用于无约束和线性约束优化的牛顿型方法。” 《数学编程》 7.1 (1974):
    311-350。'
- en: (27) Nesterov, Yu E. ”A method for solving the convex programming problem with
    convergence rate $Obigl(frac{1}{k^{2}}bigr)$.” Dokl. Akad. Nauk SSSR,. Vol. 269\.
    1983.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) Nesterov, Yu E. ”一种解决凸规划问题的方法，收敛速度为 $Obigl(frac{1}{k^{2}}bigr)$。” 《苏联科学院学报》，第
    269 卷，1983 年。
- en: '(28) Nesterov, Yurii. Introductory lectures on convex optimization: A basic
    course. Vol. 87\. Springer Science and Business Media, 2003.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) Nesterov, Yurii. 《凸优化入门讲义：基础课程》。第 87 卷。Springer Science and Business Media,
    2003。
- en: '(29) Hinton, Geoffrey, Nitish Srivastava, and Kevin Swersky. ”Neural networks
    for machine learning lecture 6a overview of mini-batch gradient descent.” Cited
    on 14.8 (2012): 2.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(29) Hinton, Geoffrey, Nitish Srivastava 和 Kevin Swersky. ”机器学习中的神经网络讲座 6a
    微型批量梯度下降概述。” 引用于 14.8 (2012): 2。'
- en: '(30) Nocedal, Jorge, and Stephen J. Wright, eds. Numerical optimization. New
    York, NY: Springer New York, 1999.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(30) Nocedal, Jorge 和 Stephen J. Wright 编. 《数值优化》。纽约，NY: Springer New York,
    1999。'
- en: '(31) Bengio, Yoshua, Aaron Courville, and Pascal Vincent. ”Representation learning:
    A review and new perspectives.” IEEE transactions on pattern analysis and machine
    intelligence 35.8 (2013): 1798-1828.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(31) Bengio, Yoshua, Aaron Courville 和 Pascal Vincent. ”表示学习：综述与新视角。” 《IEEE
    模式分析与机器智能交易》 35.8 (2013): 1798-1828。'
- en: (32) Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. ”On the difficulty of
    training recurrent neural networks.” International conference on machine learning.
    PMLR, 2013.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (32) Pascanu, Razvan, Tomas Mikolov 和 Yoshua Bengio. ”训练递归神经网络的难度。” 国际机器学习会议。PMLR,
    2013。
- en: '(33) Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. ”Learning long-term
    dependencies with gradient descent is difficult.” IEEE transactions on neural
    networks 5.2 (1994): 157-166.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(33) Bengio, Yoshua, Patrice Simard 和 Paolo Frasconi. ”用梯度下降学习长期依赖性是困难的。” 《IEEE
    神经网络交易》 5.2 (1994): 157-166。'
- en: '(34) Yao, Zhewei, et al. ”Adahessian: An adaptive second order optimizer for
    machine learning.” Proceedings of the AAAI Conference on Artificial Intelligence.
    Vol. 35\. No. 12\. 2021.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(34) Yao, Zhewei 等. ”Adahessian: 一种用于机器学习的自适应二阶优化器。” 《美国人工智能协会会议论文集》。第 35 卷，第
    12 期，2021 年。'
- en: (35) Goodfellow, Ian J., Oriol Vinyals, and Andrew M. Saxe. ”Qualitatively characterizing
    neural network optimization problems.” arXiv preprint arXiv:1412.6544 (2014).
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (35) Goodfellow, Ian J., Oriol Vinyals, 和 Andrew M. Saxe. ”定性描述神经网络优化问题。” arXiv
    预印本 arXiv:1412.6544 (2014).
- en: '(36) Tan, Hong Hui, and King Hann Lim. ”Review of second-order optimization
    techniques in artificial neural networks backpropagation.” IOP conference series:
    materials science and engineering. Vol. 495\. No. 1\. IOP Publishing, 2019.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (36) Tan, Hong Hui, 和 King Hann Lim. ”人工神经网络反向传播中的二阶优化技术综述。” 《IOP会议系列：材料科学与工程》。第495卷，第1期。IOP出版社，2019年。
- en: (37) Yinyu Ye. ”Second Order Optimization Algorithms I Lecture Notes 13”.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (37) Yinyu Ye. ”二阶优化算法 I 讲义 13”。
- en: '(38) Battiti, Roberto. ”First-and second-order methods for learning: between
    steepest descent and Newton’s method.” Neural computation 4.2 (1992): 141-166.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(38) Battiti, Roberto. ”学习的第一和第二阶方法：在最陡下降和牛顿法之间。” 《神经计算》 4.2 (1992): 141-166.'
- en: (39) Balestriero, Randall, Jerome Pesenti, and Yann LeCun. ”Learning in high
    dimension always amounts to extrapolation.” arXiv preprint arXiv:2110.09485 (2021).
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (39) Balestriero, Randall, Jerome Pesenti, 和 Yann LeCun. ”高维学习总是涉及外推。” arXiv
    预印本 arXiv:2110.09485 (2021).
- en: (40) Jastrzebski, Stanislaw, et al. ”The break-even point on optimization trajectories
    of deep neural networks.” arXiv preprint arXiv:2002.09572 (2020).
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (40) Jastrzebski, Stanislaw, 等. ”深度神经网络优化轨迹上的盈亏平衡点。” arXiv 预印本 arXiv:2002.09572
    (2020).
- en: '(41) Fort, Stanislav, and Adam Scherlis. ”The goldilocks zone: Towards better
    understanding of neural network loss landscapes.” Proceedings of the AAAI Conference
    on Artificial Intelligence. Vol. 33\. No. 01\. 2019.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (41) Fort, Stanislav, 和 Adam Scherlis. ”金发姑娘区域：更好地理解神经网络损失景观。” 《AAAI人工智能会议论文集》。第33卷，第01期。2019年。
- en: (42) Schmidt, Robin M., Frank Schneider, and Philipp Hennig. ”Descending through
    a crowded valley-benchmarking deep learning optimizers.” International Conference
    on Machine Learning. PMLR, 2021.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (42) Schmidt, Robin M., Frank Schneider, 和 Philipp Hennig. ”穿越拥挤山谷——深度学习优化器基准测试。”
    《国际机器学习会议》。PMLR，2021年。
- en: '(43) Xia, Lin, et al. ”Positive-definite sparse precision matrix estimation.”
    Advances in Pure Mathematics 7.1 (2017): 21-30.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(43) Xia, Lin, 等. ”正定稀疏精度矩阵估计。” 《纯数学进展》 7.1 (2017): 21-30.'
- en: '(44) Murphy, Kevin P. Probabilistic machine learning: an introduction. MIT
    press, 2022.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (44) Murphy, Kevin P. 《概率机器学习：导论》。MIT出版社，2022年。
- en: '(45) Hochreiter, Sepp, and Jürgen Schmidhuber. ”Flat minima.” Neural computation
    9.1 (1997): 1-42.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(45) Hochreiter, Sepp, 和 Jürgen Schmidhuber. ”平坦极小值。” 《神经计算》 9.1 (1997): 1-42.'
- en: '(46) Keskar, Nitish Shirish, et al. ”On large-batch training for deep learning:
    Generalization gap and sharp minima.” arXiv preprint arXiv:1609.04836 (2016).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (46) Keskar, Nitish Shirish, 等. ”大批量训练深度学习：泛化差距与尖锐极小值。” arXiv 预印本 arXiv:1609.04836
    (2016).
- en: (47) Goodfellow, Ian J., Oriol Vinyals, and Andrew M. Saxe. ”Qualitatively characterizing
    neural network optimization problems.” arXiv preprint arXiv:1412.6544 (2014).
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) Goodfellow, Ian J., Oriol Vinyals, 和 Andrew M. Saxe. ”定性描述神经网络优化问题。” arXiv
    预印本 arXiv:1412.6544 (2014).
- en: (48) Metz, Luke, et al. ”Gradients are not all you need.” arXiv preprint arXiv:2111.05803
    (2021).
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (48) Metz, Luke, 等. ”梯度不是你所需的全部。” arXiv 预印本 arXiv:2111.05803 (2021).
- en: '(49) Polyak, Boris T. ”Some methods of speeding up the convergence of iteration
    methods.” Ussr computational mathematics and mathematical physics 4.5 (1964):
    1-17.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(49) Polyak, Boris T. ”加速迭代方法收敛的一些方法。” 《苏联计算数学与数学物理》 4.5 (1964): 1-17.'
- en: '(50) Jacobs, Robert A. ”Increased rates of convergence through learning rate
    adaptation.” Neural networks 1.4 (1988): 295-307.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(50) Jacobs, Robert A. ”通过学习率自适应提高收敛速度。” 《神经网络》 1.4 (1988): 295-307.'
- en: (51) Duchi, John, Elad Hazan, and Yoram Singer. ”Adaptive subgradient methods
    for online learning and stochastic optimization.” Journal of machine learning
    research 12.7 (2011).
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (51) Duchi, John, Elad Hazan, 和 Yoram Singer. ”在线学习和随机优化的自适应子梯度方法。” 《机器学习研究杂志》
    12.7 (2011).
- en: (52) Gauss, Carl Friedrich. Theoria motus corporum coelestium in sectionibus
    conicis solem ambientium auctore Carolo Friderico Gauss. sumtibus Frid. Perthes
    et IH Besser, 1809.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (52) Gauss, Carl Friedrich. 《绕太阳运行的椭圆切割面上天体运动理论》，作者卡尔·弗里德里希·高斯。Frid. Perthes
    和 IH Besser 出版，1809年。
- en: '(53) Levenberg, Kenneth. ”A method for the solution of certain non-linear problems
    in least squares.” Quarterly of applied mathematics 2.2 (1944): 164-168.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(53) Levenberg, Kenneth. ”解决某些非线性最小二乘问题的方法。” 《应用数学季刊》 2.2 (1944): 164-168.'
- en: '(54) Marquardt, Donald W. ”An algorithm for least-squares estimation of nonlinear
    parameters.” Journal of the society for Industrial and Applied Mathematics 11.2
    (1963): 431-441.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(54) Marquardt, Donald W. ”一种非线性参数最小二乘估计的算法。” 工业与应用数学会期刊 11.2 (1963): 431-441.'
- en: '(55) Hestenes, Magnus R., and Eduard Stiefel. ”Methods of conjugate gradients
    for solving.” Journal of research of the National Bureau of Standards 49.6 (1952):
    409.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(55) Hestenes, Magnus R., 和 Eduard Stiefel. ”用于求解的共轭梯度法。” 国家标准局研究期刊 49.6 (1952):
    409.'
- en: (56) Straeter, Terry Anthony. On the extension of the davidon-broyden class
    of rank one, quasi-newton minimization methods to an infinite dimensional hilbert
    space with applications to optimal control problems. No. NASA-CR-111975\. 1971.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (56) Straeter, Terry Anthony. 关于将Davidson-Broyden等级一的准牛顿最小化方法扩展到无限维希尔伯特空间，并应用于最优控制问题.
    No. NASA-CR-111975. 1971.
- en: '(57) Fletcher, Reeves, and Colin M. Reeves. ”Function minimization by conjugate
    gradients.” The computer journal 7.2 (1964): 149-154.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(57) Fletcher, Reeves, 和 Colin M. Reeves. ”通过共轭梯度法进行函数最小化。” 计算机期刊 7.2 (1964):
    149-154.'
- en: '(58) Polak, Elijah, and Gerard Ribiere. ”Note sur la convergence de méthodes
    de directions conjuguées.” Revue française d’informatique et de recherche opérationnelle.
    Série rouge 3.16 (1969): 35-43.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(58) Polak, Elijah, 和 Gerard Ribiere. ”关于共轭方向法的收敛性说明。” 法国计算机与运筹学评论. 红色系列 3.16
    (1969): 35-43.'
- en: '(59) Broyden, Charles George. ”The convergence of a class of double-rank minimization
    algorithms 1\. general considerations.” IMA Journal of Applied Mathematics 6.1
    (1970): 76-90.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(59) Broyden, Charles George. ”一类双重秩最小化算法的收敛性 1. 一般考虑。” IMA 应用数学期刊 6.1 (1970):
    76-90.'
- en: '(60) Liu, Dong C., and Jorge Nocedal. ”On the limited memory BFGS method for
    large scale optimization.” Mathematical programming 45.1 (1989): 503-528.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(60) Liu, Dong C., 和 Jorge Nocedal. ”关于大规模优化的有限记忆BFGS方法。” 数学规划 45.1 (1989):
    503-528.'
- en: '(61) Zhang, Michael, et al. ”Lookahead optimizer: k steps forward, 1 step back.”
    Advances in neural information processing systems 32 (2019).'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (61) Zhang, Michael, 等. ”前瞻优化器：向前`k`步，向后1步。” 神经信息处理系统进展 32 (2019).
- en: '(62) Gill, Philip E., and Walter Murray. ”Quasi-Newton methods for unconstrained
    optimization.” IMA Journal of Applied Mathematics 9.1 (1972): 91-108.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(62) Gill, Philip E., 和 Walter Murray. ”无约束优化的准牛顿方法。” IMA 应用数学期刊 9.1 (1972):
    91-108.'
- en: (63) Vaswani, Ashish, et al. ”Attention is all you need.” Advances in neural
    information processing systems 30 (2017).
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (63) Vaswani, Ashish, 等. ”注意力机制是你所需的一切。” 神经信息处理系统进展 30 (2017).
- en: '(64) Nichol, Alex, et al. ”Glide: Towards photorealistic image generation and
    editing with text-guided diffusion models.” (2021).'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(64) Nichol, Alex, 等. ”Glide: 通过文本引导的扩散模型实现逼真的图像生成和编辑。” (2021).'
- en: (65) Goodfellow, Ian, et al. ”Generative adversarial nets.” Advances in neural
    information processing systems 27 (2014).
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (65) Goodfellow, Ian, 等. ”生成对抗网络。” 神经信息处理系统进展 27 (2014).
- en: '(66) Rojas, Raul, and Raúl Rojas. ”The backpropagation algorithm.” Neural networks:
    a systematic introduction (1996).'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (66) Rojas, Raul, 和 Raúl Rojas. ”反向传播算法。” 神经网络：系统介绍 (1996).
- en: '(67) Shervine Amidi. ”CS-229: Machine Learning”.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(67) Shervine Amidi. ”CS-229: 机器学习”。'
