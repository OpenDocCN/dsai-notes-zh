- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:06:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:04
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1907.03069] Deep learning for fine-grained image analysis: A survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1907.03069] 深度学习在细粒度图像分析中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1907.03069](https://ar5iv.labs.arxiv.org/html/1907.03069)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1907.03069](https://ar5iv.labs.arxiv.org/html/1907.03069)
- en: 'Deep learning for fine-grained image analysis: A survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在细粒度图像分析中的应用：综述
- en: Xiu-Shen Wei¹    Jianxin Wu²    Quan Cui^(1,3)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xiu-Shen Wei¹    Jianxin Wu²    Quan Cui^(1,3)
- en: ¹Megvii Research Nanjing, Megvii Technology, Nanjing, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹旷视研究南京，旷视科技，中国南京
- en: ²National Key Laboratory for Novel Software Technology, Nanjing University,
    Nanjing, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²国家重点新型软件技术实验室，南京大学，中国南京
- en: ³Graduate School of IPS, Waseda University, Fukuoka, Japan
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³早稻田大学IPS研究生院，日本福冈
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Computer vision (CV) is the process of using machines to understand and analyze
    imagery, which is an integral branch of artificial intelligence. Among various
    research areas of CV, fine-grained image analysis (FGIA) is a longstanding and
    fundamental problem, and has become ubiquitous in diverse real-world applications.
    The task of FGIA targets analyzing visual objects from subordinate categories,
    *e.g.*, species of birds or models of cars. The small inter-class variations and
    the large intra-class variations caused by the fine-grained nature makes it a
    challenging problem. During the booming of deep learning, recent years have witnessed
    remarkable progress of FGIA using deep learning techniques. In this paper, we
    aim to give a survey on recent advances of deep learning based FGIA techniques
    in a systematic way. Specifically, we organize the existing studies of FGIA techniques
    into three major categories: fine-grained image recognition, fine-grained image
    retrieval and fine-grained image generation. In addition, we also cover some other
    important issues of FGIA, such as publicly available benchmark datasets and its
    related domain specific applications. Finally, we conclude this survey by highlighting
    several directions and open problems which need be further explored by the community
    in the future.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉（CV）是使用机器理解和分析图像的过程，是人工智能的一个重要分支。在计算机视觉的各种研究领域中，细粒度图像分析（FGIA）是一个长期存在且基础性的问题，并且在各种实际应用中变得无处不在。FGIA任务旨在分析来自下级类别的视觉对象，例如鸟类的种类或汽车的型号。由于细粒度特性造成的类间小变化和类内大变化使其成为一个具有挑战性的问题。在深度学习蓬勃发展的背景下，近年来在使用深度学习技术的FGIA上取得了显著进展。本文旨在系统地综述基于深度学习的FGIA技术的最新进展。具体而言，我们将现有的FGIA技术研究组织成三个主要类别：细粒度图像识别、细粒度图像检索和细粒度图像生成。此外，我们还涉及一些其他重要的FGIA问题，如公开的基准数据集及其相关领域应用。最后，我们通过突出几个方向和需要进一步探索的开放问题来总结这次综述。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Computer vision (CV) is an interdisciplinary scientific field of artificial
    intelligence (AI), which deals with how computers can be made to gain high-level
    understanding from digital images or videos. The tasks of computer vision include
    methods for acquiring, processing, analyzing and understanding digital images,
    and the process of extracting numerical or symbolic information, *e.g.*, in the
    forms of decisions or predictions, from high-dimensional raw image data in the
    real world.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉（CV）是人工智能（AI）的一门跨学科科学领域，处理计算机如何从数字图像或视频中获得高级理解。计算机视觉的任务包括获取、处理、分析和理解数字图像的方法，以及从现实世界中的高维原始图像数据中提取数值或符号信息，例如，以决策或预测的形式。
- en: 'As an interesting, fundamental and challenging problem in computer vision,
    fine-grained image analysis (FGIA) has been an active area of research for several
    decades. The goal of FGIA is to retrieve, recognize and generate images belonging
    to multiple subordinate categories of a super-category (*aka* meta-category),
    *e.g.*, different species of animals/plants, different models of cars, different
    kinds of retail products, etc (cf. Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep learning for fine-grained image analysis: A survey")). In the real-world,
    FGIA enjoys a wide-range of applications in both industry and research societies,
    such as automatic biodiversity monitoring, climate change evaluation, intelligent
    retail, intelligent transportation, and many more. Particularly, a number of influential
    academic competitions about FGIA are frequently held on Kaggle.¹¹1Kaggle is an
    online community of data scientists and machine learners: [https://www.kaggle.com/](https://www.kaggle.com/).
    Several representative competitions, to name a few, are the Nature Conservancy
    Fisheries Monitoring (for fish species categorization), Humpback Whale Identification
    (for whale identity categorization) and so on. Each competition attracted more
    than 300 teams worldwide to participate, and some even exceeded 2,000 teams.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '作为计算机视觉中的一个有趣、基础且具有挑战性的问题，细粒度图像分析（FGIA）已经成为了数十年来活跃的研究领域。FGIA的目标是检索、识别和生成属于超类别（*即*元类别）多个从属类别的图像，例如，不同物种的动物/植物，不同型号的汽车，不同种类的零售产品等（参见图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Deep learning for fine-grained image analysis: A
    survey")）。在现实世界中，FGIA在工业和研究领域中享有广泛的应用，如自动生物多样性监测、气候变化评估、智能零售、智能交通等。特别是，关于FGIA的许多有影响力的学术竞赛经常在Kaggle上举行。¹¹Kaggle是一个数据科学家和机器学习者的在线社区：[https://www.kaggle.com/](https://www.kaggle.com/)。其中几个具有代表性的竞赛包括自然保护协会渔业监测（用于鱼类物种分类）、座头鲸识别（用于鲸鱼身份分类）等。每个竞赛吸引了全球超过300支团队参与，有些甚至超过了2000支团队。'
- en: '![Refer to caption](img/c125d096aec45f7e12a485e0f0bb53ff.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c125d096aec45f7e12a485e0f0bb53ff.png)'
- en: 'Figure 1: Fine-grained image analysis *vs*. generic image analysis (taking
    the recognitiont task for an example).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：细粒度图像分析*vs*一般图像分析（以识别任务为例）。
- en: On the other hand, deep learning techniques LeCun et al. ([2015](#bib.bib22))
    have emerged in recent years as powerful methods for learning feature representations
    directly from data, and have led to remarkable breakthroughs in the filed of FGIA.
    With rough statistics on each year, on average, there are around ten conference
    papers of deep learning based FGIA techniques published on each of AI’s and CV’s
    premium conferences, like IJCAI, AAAI, CVPR, ICCV, ECCV, etc. It shows that FGIA
    with deep learning is of notable research interests. Given this period of rapid
    evolution, the aim of this paper is to provide a comprehensive survey of the recent
    achievements in the FGIA filed brought by deep learning techniques.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，深度学习技术LeCun等人（[2015](#bib.bib22)）近年来作为直接从数据中学习特征表示的强大方法，已在FGIA领域取得了显著突破。根据每年的粗略统计，平均每年在人工智能和计算机视觉的顶级会议上，例如IJCAI、AAAI、CVPR、ICCV、ECCV等，都会有大约十篇基于深度学习的FGIA技术的会议论文发表。这表明，深度学习在FGIA领域具有显著的研究兴趣。在这一快速演变的时期，本文的目的是提供对深度学习技术在FGIA领域带来的最新成就的全面综述。
- en: In the literature, there was an existing survey related to fine-grained tasks,
    *i.e.*, Zhao et al. ([2017](#bib.bib49)), which simply included several fine-grained
    *recognition* approaches for comparisons. Our work differs with it in that ours
    is more comprehensive. Specifically, except for fine-grained recognition, we also
    analyze and discuss the other two central fine-grained analysis tasks, *i.e.*,
    fine-grained image *retrieval* and fine-grained image *generation*, which can
    not be overlooked as they are two integral aspects of FGIA. Additionally, on another
    important AI conference in the Pacific Rim nations, PRICAI, Wei and Wu organized
    a specific tutorial²²2[http://www.weixiushen.com/tutorial/PRICAI18/FGIA.html](http://www.weixiushen.com/tutorial/PRICAI18/FGIA.html)
    aiming at the fine-grained image analysis topic. We refer interested readers to
    the tutorial which provides some additional detailed information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，已有相关于细粒度任务的调查，如赵等人（[2017](#bib.bib49)），该调查简单地包含了几种细粒度*识别*方法进行比较。我们的工作不同在于我们更加全面。具体来说，除了细粒度识别，我们还分析和讨论了其他两个核心的细粒度分析任务，即细粒度图像*检索*和细粒度图像*生成*，这两个方面不可忽视，因为它们是FGIA的两个重要方面。此外，在另一个重要的AI会议——太平洋沿岸国家的PRICAI，魏和吴组织了一个特定的教程²²2[http://www.weixiushen.com/tutorial/PRICAI18/FGIA.html](http://www.weixiushen.com/tutorial/PRICAI18/FGIA.html)，旨在细粒度图像分析主题。我们建议感兴趣的读者参考该教程，它提供了一些额外的详细信息。
- en: '![Refer to caption](img/270d2ed0b0128d0f3a04334ffb529a9b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/270d2ed0b0128d0f3a04334ffb529a9b.png)'
- en: 'Figure 2: Main aspects of our hierarchical and structrual organization of fine-grained
    image analysis (FGIA) in this survey paper.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：本调查论文中我们对细粒度图像分析（FGIA）层级和结构组织的主要方面。
- en: 'In this paper, our survey take a unique deep learning based perspective to
    review the recent advances of FGIA in a systematic and comprehensive manner. The
    main contributions of this survey are three-fold:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的调查从独特的深度学习视角出发，以系统和全面的方式回顾了FGIA的最新进展。本调查的主要贡献有三方面：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We give a comprehensive review of FGIA techniques based on deep learning, including
    problem backgrounds, benchmark datasets, a family of FGIA methods with deep learning,
    domain-specific FGIA applications, etc.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对基于深度学习的FGIA技术进行了全面的回顾，包括问题背景、基准数据集、一系列基于深度学习的FGIA方法、特定领域的FGIA应用等。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We provide a systematic overview of recent advances of deep learning based
    FGIA techniques in a hierarchical and structural manner, cf. Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Deep learning for fine-grained image analysis: A survey").'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们以层级和结构化的方式系统地概述了基于深度学习的FGIA技术的最新进展，见图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Deep learning for fine-grained image analysis: A survey")。'
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss the challenges and open issues, and identify the new trends and future
    directions to provide a potential road map for fine-grained researchers or other
    interested readers in the broad AI community.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了挑战和未解问题，确定了新的趋势和未来方向，为细粒度研究人员或广泛AI社区的其他感兴趣读者提供了潜在的路线图。
- en: 'The rest of the survey is organized as follows. Section [2](#S2 "2 Background:
    problem and main challenges ‣ Deep learning for fine-grained image analysis: A
    survey") introduce backgrounds of this paper, *i.e.*, the FGIA problem and its
    main challenges. In Section [3](#S3 "3 Benchmark datasets ‣ Deep learning for
    fine-grained image analysis: A survey"), we review multiple commonly used fine-grained
    benchmark datasets. Section [4](#S4 "4 Fine-grained image recognition ‣ Deep learning
    for fine-grained image analysis: A survey") analyzes the three main paradigms
    of fine-grained image recognition. Section [5](#S5 "5 Fine-grained image retrieval
    ‣ Deep learning for fine-grained image analysis: A survey") presents recent progress
    of fine-grained image retrieval. Section [6](#S6 "6 Fine-grained image generation
    ‣ Deep learning for fine-grained image analysis: A survey") discusses fine-grained
    image generation from a generative perspective. Furthermore, in Section [7](#S7
    "7 Domain specific applications related to fine-grained image analysis ‣ Deep
    learning for fine-grained image analysis: A survey"), we introduce some other
    domain specific applications of real-world related to FGIA. Finally, we conclude
    this paper and discuss future directions and open issues in Section [8](#S8 "8
    Concluding remarks and future directions ‣ Deep learning for fine-grained image
    analysis: A survey").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '其余的调查组织如下。第 [2](#S2 "2 Background: problem and main challenges ‣ Deep learning
    for fine-grained image analysis: A survey") 节介绍了本文的背景，即 FGIA 问题及其主要挑战。在第 [3](#S3
    "3 Benchmark datasets ‣ Deep learning for fine-grained image analysis: A survey")
    节中，我们回顾了多个常用的细粒度基准数据集。第 [4](#S4 "4 Fine-grained image recognition ‣ Deep learning
    for fine-grained image analysis: A survey") 节分析了细粒度图像识别的三种主要范式。第 [5](#S5 "5 Fine-grained
    image retrieval ‣ Deep learning for fine-grained image analysis: A survey") 节展示了细粒度图像检索的最新进展。第
    [6](#S6 "6 Fine-grained image generation ‣ Deep learning for fine-grained image
    analysis: A survey") 节从生成视角讨论了细粒度图像生成。此外，在第 [7](#S7 "7 Domain specific applications
    related to fine-grained image analysis ‣ Deep learning for fine-grained image
    analysis: A survey") 节中，我们介绍了一些与 FGIA 相关的实际领域应用。最后，在第 [8](#S8 "8 Concluding remarks
    and future directions ‣ Deep learning for fine-grained image analysis: A survey")
    节中，我们总结了本文并讨论了未来的方向和开放性问题。'
- en: '2 Background: problem and main challenges'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景：问题与主要挑战
- en: '![Refer to caption](img/621364e720d1587511716b6d564d049f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/621364e720d1587511716b6d564d049f.png)'
- en: 'Figure 3: Key challenge of fine-grained image analysis, *i.e.*, small inter-class
    variations and large intra-class variations. We here present each of four Tern
    species in each row in the figure, respectively.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：细粒度图像分析的关键挑战，即，小的类间差异和大的类内差异。我们在图中分别展示了四种角雉的每一行。
- en: In this section, we summarize the related background of this paper, including
    the problem and its key challenges.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了本文的相关背景，包括问题及其关键挑战。
- en: Fine-grained image analysis (FGIA) focuses on dealing with the objects belonging
    to multiple *sub-categories* of the same meta-category (*e.g.*, birds, dogs and
    cars), and generally involves central tasks like fine-grained image recognition,
    fine-grained image retrieval, fine-grained image generation, etc.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度图像分析（FGIA）专注于处理属于同一元类别的多个子类别（例如，鸟类、狗和汽车）的对象，通常涉及细粒度图像识别、细粒度图像检索、细粒度图像生成等核心任务。
- en: 'What distinguishes FGIA from the generic one is: in generic image analysis,
    the target objects belong to coarse-grained meta-categories (*e.g.*, birds, oranges
    and dogs), and thus are visually quite different. However, in FGIA, since objects
    come from sub-categories of one meta-category, the fine-grained nature causes
    them visually quite similar. We take image recognition for illustration. As shown
    in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep learning for fine-grained
    image analysis: A survey"), in fine-grained recognition, the task is required
    to identify multiple similar species of dogs, *e.g.*, Husky, Samoyed and Alaska.
    For accurate recognition, it is desirable to distinguish them by capturing slight
    and subtle differences (*e.g.*, ears, noses, tails), which also meets the demand
    of other FGIA tasks (*e.g.*, retrieval and generation).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'FGIA 与通用图像分析的区别在于：在通用图像分析中，目标对象属于粗粒度的元类别（例如，鸟类、橙子和狗），因此在视觉上相差较大。然而，在 FGIA 中，由于对象来自一个元类别的子类别，细粒度特性使它们在视觉上非常相似。我们以图像识别为例。正如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep learning for fine-grained image analysis:
    A survey") 所示，在细粒度识别中，任务要求识别多种相似的狗品种，例如，哈士奇、萨摩耶和阿拉斯加。为了准确识别，理想的做法是通过捕捉微小而细微的差异（例如，耳朵、鼻子、尾巴）来区分它们，这也满足了其他
    FGIA 任务的需求（例如，检索和生成）。'
- en: '![Refer to caption](img/a10e3b113deeae364dae0b9c5e6dd60a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a10e3b113deeae364dae0b9c5e6dd60a.png)'
- en: 'Figure 4: Example fine-grained images belonging to different species of flowers/vegetable,
    different models of cars/aircrafts and different kinds of retail products. Accurate
    identification of these fine-grained objects requires the dependences on the discriminative
    but subtle object parts or image regions. (Best viewed in color and zoomed in.)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：不同花卉/蔬菜物种、不同车型/飞机模型以及不同类型零售产品的细粒度图像示例。准确识别这些细粒度物体需要依赖于区分但微妙的物体部位或图像区域。（最佳查看效果为彩色且放大。）
- en: 'Table 1: Summary of popular fine-grained image datasets. Note that “BBox” indicates
    whether this dataset provides object bounding box supervisions. “Part anno.” means
    providing the key part localizations. “HRCHY” corresponds to hierarchical labels.
    “ATR” represents the attribute labels (*e.g.*, wing color, male, female, etc).
    “Texts” indicates whether fine-grained text descriptions of images are supplied.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：流行的细粒度图像数据集汇总。注意，“BBox”表示该数据集是否提供物体边界框监督。 “Part anno.” 表示提供关键部位定位。 “HRCHY”
    代表层次标签。 “ATR” 代表属性标签（*例如*，翅膀颜色，雄性，雌性等）。 “Texts” 表示是否提供图像的细粒度文本描述。
- en: '| Dataset name | Meta-class | $\sharp$ images | $\sharp$ categories | BBox
    | Part anno. | HRCHY | ATR | Texts |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 数据集名称 | 元类别 | $\sharp$ 图像 | $\sharp$ 类别 | BBox | 部位注释 | 层次 | 属性 | 文本 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Oxford Flower  Nilsback and Zisserman ([2008](#bib.bib29)) | Flowers |     8,189
    |   102 |  |  |  |  | ✓ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 牛津花卉  Nilsback 和 Zisserman ([2008](#bib.bib29)) | 花卉 |     8,189 |   102
    |  |  |  |  | ✓ |'
- en: '| CUB200-2011  Wah et al. ([2011](#bib.bib37)) | Birds |   11,788 |   200 |
    ✓ | ✓ |  | ✓ | ✓ |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| CUB200-2011  Wah et al. ([2011](#bib.bib37)) | 鸟类 |   11,788 |   200 | ✓
    | ✓ |  | ✓ | ✓ |'
- en: '| Stanford Dog  Khosla et al. ([2011](#bib.bib19)) | Dogs |   20,580 |   120
    | ✓ |  |  |  |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 斯坦福狗  Khosla et al. ([2011](#bib.bib19)) | 狗 |   20,580 |   120 | ✓ |  |  |  |  |'
- en: '| Stanford Car  Krause et al. ([2013](#bib.bib21)) | Cars |   16,185 |   196
    | ✓ |  |  |  |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 斯坦福车  Krause et al. ([2013](#bib.bib21)) | 汽车 |   16,185 |   196 | ✓ |  |  |  |  |'
- en: '| FGVC Aircraft  Maji et al. ([2013](#bib.bib28)) | Aircrafts |   10,000 |
      100 | ✓ |  | ✓ |  |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| FGVC飞机  Maji et al. ([2013](#bib.bib28)) | 飞机 |   10,000 |   100 | ✓ |  |
    ✓ |  |  |'
- en: '| Birdsnap  Berg et al. ([2014](#bib.bib2)) | Birds |   49,829 |   500 | ✓
    | ✓ |  | ✓ |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Birdsnap  Berg et al. ([2014](#bib.bib2)) | 鸟类 |   49,829 |   500 | ✓ | ✓
    |  | ✓ |  |'
- en: '| Fru92  Hou et al. ([2017](#bib.bib17)) | Fruits |   69,614 |     92 |  |  |
    ✓ |  |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Fru92  Hou et al. ([2017](#bib.bib17)) | 水果 |   69,614 |     92 |  |  | ✓
    |  |  |'
- en: '| Veg200  Hou et al. ([2017](#bib.bib17)) | Vegetable |   91,117 |   200 |  |  |
    ✓ |  |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Veg200  Hou et al. ([2017](#bib.bib17)) | 蔬菜 |   91,117 |   200 |  |  | ✓
    |  |  |'
- en: '| iNat2017  Horn et al. ([2017](#bib.bib16)) | Plants & Animals | 859,000 |
    5,089 | ✓ |  | ✓ |  |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| iNat2017  Horn et al. ([2017](#bib.bib16)) | 植物和动物 | 859,000 | 5,089 | ✓
    |  | ✓ |  |  |'
- en: '| RPC  Wei et al. ([2019a](#bib.bib42)) | Retail products |   83,739 |   200
    | ✓ |  | ✓ |  |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| RPC  Wei et al. ([2019a](#bib.bib42)) | 零售产品 |   83,739 |   200 | ✓ |  |
    ✓ |  |  |'
- en: 'Furthermore, fine-grained nature also brings the *small inter-class variations*
    caused by highly similar sub-categories, and the *large intra-class variations*
    in poses, scales and rotations, as presented by Fig. [3](#S2.F3 "Figure 3 ‣ 2
    Background: problem and main challenges ‣ Deep learning for fine-grained image
    analysis: A survey"). It is the opposite of the generic image analysis (*i.e.*,
    the small intra-class variations and the large inter-class variations), which
    makes fine-grained image analysis a challenging problem.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，细粒度特性还带来了由高度相似的子类别造成的*小的类间变异*，以及在姿态、尺度和旋转中表现出的*大的类内变异*，如图 [3](#S2.F3 "图 3
    ‣ 2 背景：问题和主要挑战 ‣ 深度学习在细粒度图像分析中的应用：综述")所示。这与通用图像分析（*即*，小的类内变异和大的类间变异）正好相反，这使得细粒度图像分析成为一个具有挑战性的问题。
- en: 3 Benchmark datasets
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 个基准数据集
- en: 'In the past decade, the vision community has released many benchmark fine-grained
    datasets covering diverse domains such as birds Wah et al. ([2011](#bib.bib37));
    Berg et al. ([2014](#bib.bib2)), dogs Khosla et al. ([2011](#bib.bib19)), cars Krause
    et al. ([2013](#bib.bib21)), airplanes Maji et al. ([2013](#bib.bib28)), flowers Nilsback
    and Zisserman ([2008](#bib.bib29)), vegetable Hou et al. ([2017](#bib.bib17)),
    fruits Hou et al. ([2017](#bib.bib17)), retail products Wei et al. ([2019a](#bib.bib42)),
    etc (cf. Fig. [4](#S2.F4 "Figure 4 ‣ 2 Background: problem and main challenges
    ‣ Deep learning for fine-grained image analysis: A survey")). In Table [1](#S2.T1
    "Table 1 ‣ 2 Background: problem and main challenges ‣ Deep learning for fine-grained
    image analysis: A survey"), we list a number of image datasets commonly used by
    the fine-grained community, and specifically indicate their meta-category, the
    amounts of fine-grained images, the number of fine-grained categories, extra different
    kinds of available supervisions, *i.e.*, bounding boxes, part annotations, hierarchical
    labels, attribute labels and text visual descriptions, cf. Fig. [5](#S3.F5 "Figure
    5 ‣ 3 Benchmark datasets ‣ Deep learning for fine-grained image analysis: A survey").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，视觉领域发布了许多涵盖多种领域的基准细粒度数据集，如鸟类 Wah 等人 ([2011](#bib.bib37)); Berg 等人 ([2014](#bib.bib2))，狗
    Khosla 等人 ([2011](#bib.bib19))，汽车 Krause 等人 ([2013](#bib.bib21))，飞机 Maji 等人 ([2013](#bib.bib28))，花卉
    Nilsback 和 Zisserman ([2008](#bib.bib29))，蔬菜 Hou 等人 ([2017](#bib.bib17))，水果 Hou
    等人 ([2017](#bib.bib17))，零售产品 Wei 等人 ([2019a](#bib.bib42)) 等（参见图 [4](#S2.F4 "图
    4 ‣ 2 背景：问题和主要挑战 ‣ 深度学习在细粒度图像分析中的应用：综述")）。在表 [1](#S2.T1 "表 1 ‣ 2 背景：问题和主要挑战 ‣
    深度学习在细粒度图像分析中的应用：综述") 中，我们列出了细粒度领域常用的若干图像数据集，并具体指明它们的元类别、细粒度图像的数量、细粒度类别的数量、额外的不同类型的可用标注，即边界框、部分注释、层级标签、属性标签和文本视觉描述，参见图
    [5](#S3.F5 "图 5 ‣ 3 基准数据集 ‣ 深度学习在细粒度图像分析中的应用：综述")。
- en: These datasets have been one of the most important factors for the considerable
    progress in the filed, not only as a common ground for measuring and comparing
    performance of competing approaches, but also pushing this filed towards increasingly
    complex, practical and challenging problems.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集已成为领域中显著进展的重要因素，不仅作为测量和比较竞争方法性能的共同基础，还推动了这一领域向越来越复杂、实际和具有挑战性的问题发展。
- en: '![Refer to caption](img/afebbdc0aebf93d9000dcc6c0db4feef.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/afebbdc0aebf93d9000dcc6c0db4feef.png)'
- en: 'Figure 5: An example image with its supervisions associated with *CUB200-2011*.
    As shown, multiple types of supervisions include: image labels, part annotations
    (*aka* key point localizations), object bounding boxes (*i.e.*, the green one),
    attribute labels (*i.e.*, “ATR”), and text descriptions by natural languages.
    (Best viewed in color.)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：一个示例图像及其与 *CUB200-2011* 相关的标注。如图所示，多种类型的标注包括：图像标签、部分注释（*即* 关键点定位）、对象边界框（*即*
    绿色框），属性标签（*即* “ATR”）和自然语言描述。（最佳以彩色查看。）
- en: Specifically, among them, *CUB200-2011* is one of the most popular fine-grained
    datasets. Almost all the FGIA approaches choose it for comparisons with state-of-the-arts.
    Moreover, constant contributions are made upon *CUB200-2011* for further research,
    *e.g.*, collecting text descriptions of the fine-grained images for multi-modality
    analysis, cf. Reed et al. ([2016](#bib.bib32)); He and Peng ([2017a](#bib.bib14)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在这些数据集中，*CUB200-2011* 是最受欢迎的细粒度数据集之一。几乎所有的 FGIA 方法都选择它与最先进的方法进行比较。此外，*CUB200-2011*
    还不断被用于进一步研究，例如，收集细粒度图像的文本描述以进行多模态分析，参见 Reed 等人 ([2016](#bib.bib32)); He 和 Peng
    ([2017a](#bib.bib14))。
- en: Additionally, in recent years, more challenging and practical fine-grained datasets
    are proposed increasingly, *e.g.*, *iNat2017* for natural species of plants, animals Horn
    et al. ([2017](#bib.bib16)) and *RPC* for daily retail products Wei et al. ([2019a](#bib.bib42)).
    Many novel features deriving from these datasets are, to name a few, large-scale,
    hierarchical structure, domain gap and long-tail distribution, which reveals the
    practical requirements in real-world and could arouse the studies of FGIA in more
    realistic settings.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，近年来提出了越来越多具有挑战性和实际意义的细粒度数据集，*例如*，*iNat2017* 用于自然植物和动物物种 Horn 等人 ([2017](#bib.bib16))
    和 *RPC* 用于日常零售产品 Wei 等人 ([2019a](#bib.bib42))。这些数据集中衍生出的许多新特征，包括大规模、层级结构、领域差距和长尾分布，揭示了现实世界中的实际需求，并可能激发对
    FGIA 在更现实设置下的研究。
- en: 4 Fine-grained image recognition
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 细粒度图像识别
- en: Fine-grained image recognition has been the most active research area of FGIA
    in the past decade. In this section, we review the milestones of fine-grained
    recognition frameworks since deep learning entered the filed. Broadly, these fine-grained
    recognition approaches can be organized into three main paradigms, *i.e.*, fine-grained
    recognition (1) with localization-classification subnetworks; (2) with end-to-end
    feature encoding and (3) with external information. Among them, the first and
    second paradigms restrict themselves by only utilizing the supervisions associated
    with fine-grained images such as image labels, bounding boxes, part annotations,
    etc. In addition, automatic recognition systems cannot yet achieve excellent performance
    due to the fine-grained challenges. Thus, researchers gradually attempt to involve
    external but cheap information (*e.g.*, web data, text descriptions) into fine-grained
    recognition for further improving accuracy, which corresponds to the third paradigm
    of fine-grained recognition. Popularly used evaluation metric in fine-grained
    recognition is the averaged classification accuracy across all the subordinate
    categories of the datasets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，细粒度图像识别一直是FGIA中最活跃的研究领域。在这一部分，我们回顾了自深度学习进入该领域以来细粒度识别框架的里程碑。广泛地，这些细粒度识别方法可以组织为三种主要范式，*即*，细粒度识别（1）具有定位-分类子网络；（2）具有端到端特征编码；（3）具有外部信息。其中，第一个和第二个范式仅通过利用与细粒度图像相关的监督信息，如图像标签、边界框、部件注释等，来限制自己。此外，自动识别系统由于细粒度挑战尚未能够实现卓越的性能。因此，研究人员逐渐尝试将外部但廉价的信息（*例如*，网络数据、文本描述）引入细粒度识别中，以进一步提高准确性，这对应于细粒度识别的第三个范式。在细粒度识别中，常用的评估指标是数据集中所有子类别的平均分类准确率。
- en: 4.1 By localization-classification subnetworks
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 通过定位-分类子网络
- en: To mitigate the challenge of intra-class variations, researchers in the fine-grained
    community pay attentions on capturing discriminative semantic parts of fine-grained
    objects and then constructing a mid-level representation corresponding to these
    parts for the final classification. Specifically, a localization subnetwork is
    designed for locating these key parts. While later, a classification subnetwork
    follows and is employed for recognition. The framework of such two collaborative
    subnetworks forms the first paradigm, *i.e.*, fine-grained recognition with *localization-classification
    subnetworks*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解类别内变化的挑战，细粒度领域的研究人员关注于捕捉细粒度对象的辨别性语义部件，然后构建一个对应于这些部件的中层表示以进行最终分类。具体而言，设计了一个定位子网络用于定位这些关键部件。随后，分类子网络会跟随其后并用于识别。这种两个协作子网络的框架形成了第一个范式，*即*，具有*定位-分类子网络*的细粒度识别。
- en: Thanks to the localization information, *e.g.*, part-level bounding boxes or
    segmentation masks, it can obtain more discriminative mid-level (part-level) representations
    w.r.t. these fine-grained parts. Also, it further enhances the learning capability
    of the classification subnetwork, which could significantly boost the final recognition
    accuracy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 得益于定位信息，*例如*，部件级边界框或分割掩码，它可以获得更具辨别力的中层（部件级）表示。还进一步增强了分类子网络的学习能力，这可能显著提高最终的识别准确性。
- en: Earlier works belonging to this paradigm depend on additional dense part annotations
    (*aka* key points localization) to locate semantic key parts (*e.g.*, head, torso)
    of objects. Some of them learn part-based detectors Zhang et al. ([2014](#bib.bib47));
    Lin et al. ([2015a](#bib.bib25)), and some of them leverage segmentation methods
    for localizing parts Wei et al. ([2018a](#bib.bib40)). Then, these methods concatenate
    multiple part-level features as a whole image representation, and feed it into
    the following classification subnetwork for final recognition. Thus, these approaches
    are also termed as *part-based* recognition methods.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 早期属于这一范式的工作依赖于额外的密集部分注释（*即*关键点定位）来定位对象的语义关键部分（*例如*，头部、躯干）。其中一些工作通过基于部件的检测器进行学习，如张等人（[2014](#bib.bib47)）；林等人（[2015a](#bib.bib25)），而一些则利用分割方法来定位部件，如魏等人（[2018a](#bib.bib40)）。然后，这些方法将多个部件级特征连接起来作为整个图像的表示，并将其输入到随后的分类子网络中以进行最终识别。因此，这些方法也被称为*基于部件*的识别方法。
- en: However, obtaining such dense part annotations is labor-intensive, which limits
    both scalability and practicality of real-world fine-grained applications. Recently,
    it emerges a trend that more techniques under this paradigm only require image
    labels Jaderberg et al. ([2015](#bib.bib18)); Fu et al. ([2017](#bib.bib11));
    Zheng et al. ([2017](#bib.bib50)); Sun et al. ([2018](#bib.bib35)) for accurate
    part localization. The common motivation of them is to first find the corresponding
    parts and then compare their appearance. Concretely, it is desirable to capture
    semantic parts (*e.g.*, head and torso) to be shared across fine-grained categories,
    and meanwhile, it is also eager for discovering the subtle differences between
    these part representations. Advanced techniques, like attention mechanisms Yang
    et al. ([2018](#bib.bib46)) and multi-stage strategies He and Peng ([2017b](#bib.bib15))
    complicate the joint training of the integrated localization-classification subnetworks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，获取如此密集的部件注释是劳动密集型的，这限制了真实世界细粒度应用的可扩展性和实用性。最近出现了一种趋势，即在这个范式下的更多技术仅需图像标签**Jaderberg**等（[2015](#bib.bib18)）；**Fu**等（[2017](#bib.bib11)）；**Zheng**等（[2017](#bib.bib50)）；**Sun**等（[2018](#bib.bib35)），即可实现准确的部件定位。它们的共同动机是首先找到对应的部件，然后比较它们的外观。具体来说，希望捕捉到可以在细粒度类别中共享的语义部件（如头部和躯干），同时也希望发现这些部件表示之间的细微差别。先进的技术，如注意力机制**Yang**等（[2018](#bib.bib46)）和多阶段策略**He**和**Peng**（[2017b](#bib.bib15)），使得集成定位-分类子网络的联合训练更加复杂。
- en: 4.2 By end-to-end feature encoding
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 通过端到端特征编码
- en: Different from the first paradigm, the second paradigm, *i.e.*, *end-to-end
    feature encoding*, leans to directly learn a more discriminative feature representation
    by developing powerful deep models for fine-grained recognition. The most representative
    method among them is Bilinear CNNs Lin et al. ([2015b](#bib.bib26)), which represents
    an image as a pooled outer product of features derived from two deep CNNs, and
    thus encodes higher order statistics of convolutional activations to enhance the
    mid-level learning capability. Thanks to its high model capacity, Bilinear CNNs
    achieve remarkable fine-grained recognition performance. However, the extremely
    high dimensionality of bilinear features still makes it impractical for realistic
    applications, especially for the large-scale ones.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一个范式不同，第二个范式，即*端到端特征编码*，倾向于通过开发强大的深度模型直接学习更具区分性的特征表示。其中最具代表性的方法是双线性CNN**Lin**等（[2015b](#bib.bib26)），它将图像表示为从两个深度CNN中提取的特征的池化外积，从而编码卷积激活的高阶统计信息，以增强中级学习能力。由于其高模型容量，双线性CNN实现了显著的细粒度识别性能。然而，双线性特征的极高维度仍然使其在现实应用中不切实际，特别是在大规模应用中。
- en: Aiming at this problem, more recent attempts, *e.g.*, Gao et al. ([2016](#bib.bib12));
    Kong and Fowlkes ([2017](#bib.bib20)); Cui et al. ([2017](#bib.bib6)), try to
    aggregate low-dimensional embeddings by applying tensor sketching Pham and Pagh
    ([2013](#bib.bib31)); Charikar et al. ([2002](#bib.bib3)), which can approximate
    the bilinear features and maintain comparable or higher recognition accuracy.
    Other works, *e.g.*, Dubey et al. ([2018](#bib.bib8)), focus on designing a specific
    loss function tailored for fine-grained and is able to drive the whole deep model
    for learning discriminative fine-grained representations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这个问题，最近的一些尝试，如**Gao**等（[2016](#bib.bib12)）；**Kong**和**Fowlkes**（[2017](#bib.bib20)）；**Cui**等（[2017](#bib.bib6)），尝试通过应用张量草图**Pham**和**Pagh**（[2013](#bib.bib31)）；**Charikar**等（[2002](#bib.bib3)）来聚合低维嵌入，这可以近似双线性特征，并保持相当或更高的识别准确性。其他工作，如**Dubey**等（[2018](#bib.bib8)），则专注于设计特定的损失函数，针对细粒度任务进行调整，能够驱动整个深度模型学习区分性的细粒度表示。
- en: 4.3 With external information
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 使用外部信息
- en: As aforementioned, beyond the conventional recognition paradigms, another paradigm
    is to leverage external information, *e.g.*, web data, multi-modality data or
    human-computer interactions, to further assist fine-grained recognition.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，除了传统的识别范式外，另一种范式是利用外部信息，如网页数据、多模态数据或人机交互，以进一步辅助细粒度识别。
- en: 4.3.1 With web data
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 使用网页数据
- en: To identify the minor distinction among various fine-grained categories, sufficient
    well-labeled training images are in high demand. However, accurate human annotations
    for fine-grained categories are not easy to acquire, due to the difficulty of
    annotations (always requiring domain experts) and the myriads of fine-grained
    categories (*i.e.*, more than thousands of subordinate categories in a meta-category).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要识别各种细粒度类别之间的细微差异，迫切需要大量标注良好的训练图像。然而，由于标注的难度（总是需要领域专家）和细粒度类别的繁多（*即*，一个元类别中有数千个从属类别），准确的人类标注难以获得。
- en: 'Therefore, a part of fine-grained recognition methods seek to utilize the free
    but noisy web data to boost recognition performance. The majority of existing
    works in this line can be roughly grouped into two directions. One of them is
    to crawl noisy labeled web data for the test categories as training data, which
    is regarded as webly supervised learning Zhuang et al. ([2017](#bib.bib53)); Sun
    et al. ([2019](#bib.bib36)). Main efforts of these approaches concentrate on:
    (1) overcoming the dataset gap between easily acquired web images and the well-labeled
    data from standard datasets; and (2) reducing the negative effects caused by the
    noisy data. For dealing with the aforementioned problems, deep learning techniques
    of adversarial learning Goodfellow et al. ([2014](#bib.bib13)) and attention mechanisms Zhuang
    et al. ([2017](#bib.bib53)) are frequently utilized. The other direction of using
    web data is to transfer the knowledge from an auxiliary categories with well-labeled
    training data to the test categories, which usually employs zero-shot learning Niu
    et al. ([2018](#bib.bib30)) or meta learning Zhang et al. ([2018](#bib.bib48))
    to achieve that goal.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一些细粒度识别方法寻求利用免费的但噪声较多的网络数据来提升识别性能。现有的大多数相关工作大致可以分为两个方向。其中一个是爬取噪声标注的网络数据作为测试类别的训练数据，这被视为网络监督学习 Zhuang
    et al. ([2017](#bib.bib53)); Sun et al. ([2019](#bib.bib36))。这些方法的主要工作集中在：(1)
    克服易得的网络图像与标准数据集中良好标注数据之间的数据集差距；(2) 减少噪声数据造成的负面影响。为解决上述问题，通常使用对抗学习 Goodfellow et
    al. ([2014](#bib.bib13)) 和注意力机制 Zhuang et al. ([2017](#bib.bib53))。使用网络数据的另一种方向是将来自具有良好标注训练数据的辅助类别的知识转移到测试类别，通常使用零样本学习 Niu
    et al. ([2018](#bib.bib30)) 或元学习 Zhang et al. ([2018](#bib.bib48)) 实现这一目标。
- en: 4.3.2 With multi-modality data
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 使用多模态数据
- en: Multi-modal analysis has attracted a lot of attentions with the rapid growth
    of multi-media data (*e.g.*, image, text, knowledge base, etc). In fine-grained
    recognition, it takes multi-modality data to establish joint-representations/embeddings
    for incorporating multi-modality information. It is able to boost fine-grained
    recognition accuracy. In particular, frequently utilized multi-modality data includes
    text descriptions (*e.g.*, sentences and phrases of natural languages) and graph-structured
    knowledge base. Compared with strong supervisions of fine-grained images, *e.g.*,
    part annotations, text descriptions are weak supervisions. Besides, text descriptions
    can be relatively accurately returned by ordinary humans, rather than the experts
    in a specific domain. In addition, high-level knowledge graph is an existing resource
    and contains rich professional knowledge, such as *DBpedia* Lehmann et al. ([2015](#bib.bib23)).
    In practice, both text descriptions and knowledge base are effective as extra
    guidance for better fine-grained image representation learning.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随着多媒体数据（*例如*，图像、文本、知识库等）的快速增长，多模态分析受到了大量关注。在细粒度识别中，需要使用多模态数据建立联合表示/嵌入，以整合多模态信息。这能够提升细粒度识别的准确性。特别是，常用的多模态数据包括文本描述（*例如*，自然语言的句子和短语）和图结构知识库。与细粒度图像的强监督（*例如*，部分标注）相比，文本描述是弱监督。此外，文本描述可以由普通人相对准确地返回，而不是特定领域的专家。此外，高级知识图谱是现有资源，并包含丰富的专业知识，例如*DBpedia* Lehmann
    et al. ([2015](#bib.bib23))。在实际应用中，文本描述和知识库作为额外指导，对更好的细粒度图像表示学习都是有效的。
- en: '![Refer to caption](img/b9ab08e34ab782ae4272ee656b7af6e1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b9ab08e34ab782ae4272ee656b7af6e1.png)'
- en: 'Figure 6: An example knowledge graph for modeling the category-attribute correlations
    on *CUB200-2011*.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：用于建模*CUB200-2011*上类别-属性关联的示例知识图谱。
- en: 'Specifically, Reed et al. ([2016](#bib.bib32)) collects text descriptions,
    and introduces a structured joint embedding for zero-shot fine-grained image recognition
    by combining texts and images. Later, He and Peng ([2017a](#bib.bib14)) combines
    the vision and language streams in a joint training end-to-end fashion to preserve
    the intra-modality and inter-modality information for generating complementary
    fine-grained representations. For fine-grained recognition with knowledge base,
    some works, *e.g.*, Chen et al. ([2018](#bib.bib4)); Xu et al. ([2018a](#bib.bib44)),
    introduce the knowledge base information (always associating with attribute labels,
    cf. Fig. [6](#S4.F6 "Figure 6 ‣ 4.3.2 With multi-modality data ‣ 4.3 With external
    information ‣ 4 Fine-grained image recognition ‣ Deep learning for fine-grained
    image analysis: A survey")) to implicitly enriching the embedding space (also
    reasoning about the discriminative attributes for fine-grained objects).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，Reed 等人 ([2016](#bib.bib32)) 收集了文本描述，并通过结合文本和图像引入了一种结构化的联合嵌入用于零样本细粒度图像识别。随后，He
    和 Peng ([2017a](#bib.bib14)) 通过端到端的联合训练方式结合了视觉和语言流，以保留内部模态和跨模态信息，用于生成互补的细粒度表示。对于具有知识库的细粒度识别，一些研究，例如
    Chen 等人 ([2018](#bib.bib4)); Xu 等人 ([2018a](#bib.bib44))，引入了知识库信息（通常与属性标签相关联，参见图
    [6](#S4.F6 "Figure 6 ‣ 4.3.2 With multi-modality data ‣ 4.3 With external information
    ‣ 4 Fine-grained image recognition ‣ Deep learning for fine-grained image analysis:
    A survey")），以隐式地丰富嵌入空间（并推理出细粒度对象的区分属性）。'
- en: 4.3.3 With humans in the loop
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 人工参与的细粒度识别
- en: Fine-grained recognition with humans in the loop is usually an iterative system
    composed of a machine and a human user, which combines both human and machine
    efforts and intelligence. Also, it requires the system to work in a human labor-economy
    way as possible. Generally, for these kinds of recognition methods, the system
    in each round is seeking to understand how humans perform recognition, *e.g.*,
    by asking untrained humans to label the image class and pick up hard examples Cui
    et al. ([2016](#bib.bib5)), or by identifying key part localization and selecting
    discriminative features Deng et al. ([2016](#bib.bib7)) for fine-grained recognition.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 人工参与的细粒度识别通常是一个由机器和人类用户组成的迭代系统，结合了人类和机器的努力与智能。同时，它要求系统尽可能以人力劳动经济的方式运作。通常，对于这些识别方法，每一轮系统都在试图理解人类如何进行识别，*例如*，通过要求未经培训的人类标记图像类别并挑选困难样本
    Cui 等人 ([2016](#bib.bib5))，或通过识别关键部分定位和选择区分特征 Deng 等人 ([2016](#bib.bib7)) 进行细粒度识别。
- en: 5 Fine-grained image retrieval
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 细粒度图像检索
- en: Beyond image recognition, fine-grained retrieval is another crucial aspect of
    FGIA and emerges as a hot topic. Its evaluation metric is the common mean average
    precision (mAP).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 超越图像识别，细粒度检索是 FGIA 的另一个关键方面，并且成为一个热门话题。其评估指标是常用的平均精度均值 (mAP)。
- en: 'In fine-grained image retrieval, given database images of the same sub-category
    (*e.g.*, birds or cars) and a query, it should return images which are in the
    same variety as the query, without resorting to any other supervision signals,
    cf. Fig. [7](#S5.F7 "Figure 7 ‣ 5 Fine-grained image retrieval ‣ Deep learning
    for fine-grained image analysis: A survey"). Compared with generic image retrieval
    which focuses on retrieving near-duplicate images based on similarities in their
    contents (*e.g.*, textures, colors and shapes), while fine-grained retrieval focuses
    on retrieving the images of the same types (*e.g.*, the same subordinate species
    for the animals and the same model for the cars). Meanwhile, objects in fine-grained
    images have only subtle differences, and vary in poses, scales and rotations.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在细粒度图像检索中，给定同一子类别（*例如*，鸟类或汽车）的数据库图像和一个查询，系统应返回与查询相同种类的图像，而无需依赖其他监督信号，参见图 [7](#S5.F7
    "Figure 7 ‣ 5 Fine-grained image retrieval ‣ Deep learning for fine-grained image
    analysis: A survey")。与专注于基于内容相似性（*例如*，纹理、颜色和形状）检索近似重复图像的通用图像检索相比，细粒度检索专注于检索同一类型的图像（*例如*，相同的动物下属种类或相同的汽车型号）。同时，细粒度图像中的对象只有微小的差异，并且在姿态、尺度和旋转上有所变化。'
- en: '![Refer to caption](img/1c607cbd081b0a825f64ca7d76c35c9e.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1c607cbd081b0a825f64ca7d76c35c9e.png)'
- en: 'Figure 7: An illustration of fine-grained *retrieval*. Given a query image
    (*aka* probe) of “Dodge Charger Sedan 2012”, fine-grained retrieval is required
    to return images of the same car model from a car database (*aka* galaxy). In
    this figure, the top-4 returned image marked in a red rectangle presents a wrong
    result, since its model is “Dodge Caliber Wagon 2012”.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：细粒度*检索*的示意图。给定一个“Dodge Charger Sedan 2012”的查询图像（*即*探测器），细粒度检索要求从汽车数据库（*即*星系）中返回相同车型的图像。在此图中，标记为红色矩形的前四个返回图像呈现了错误的结果，因为其型号为“Dodge
    Caliber Wagon 2012”。
- en: In the literature, Wei et al. ([2017](#bib.bib39)) is the first attempt to fine-grained
    image retrieval using deep learning. It employs pre-trained CNN models to select
    the meaningful deep descriptors by localizing the main object in fine-grained
    images *unsupervisedly*, and further reveals that selecting only useful deep descriptors
    with removing background or noise could significantly benefit retrieval tasks.
    Recently, to break through the limitation of unsupervised fine-grained retrieval
    by pre-trained models, some trials Zheng et al. ([2018](#bib.bib51), [2019](#bib.bib52))
    tend to discovery novel loss functions under the *supervised* metric learning
    paradigm. Meanwhile, they still design additional specific sub-modules tailored
    for fine-grained objects, *e.g.*, the weakly-supervised localization module proposed
    in Zheng et al. ([2018](#bib.bib51)), which is under the inspiration of Wei et
    al. ([2017](#bib.bib39)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，Wei 等人（[2017](#bib.bib39)）是首次尝试使用深度学习进行细粒度图像检索的研究。该研究采用了预训练的 CNN 模型，通过在细粒度图像中无监督地定位主要对象来选择有意义的深度描述符，并进一步揭示了仅选择有用的深度描述符并去除背景或噪声可以显著提高检索任务的效果。最近，为了突破通过预训练模型进行无监督细粒度检索的限制，一些研究，如
    Zheng 等人（[2018](#bib.bib51)，[2019](#bib.bib52)），倾向于在*监督*度量学习范式下发现新的损失函数。同时，他们仍然设计了额外的特定子模块以适应细粒度对象，例如
    Zheng 等人（[2018](#bib.bib51)）提出的弱监督定位模块，该模块的灵感来源于 Wei 等人（[2017](#bib.bib39)）。
- en: 6 Fine-grained image generation
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 细粒度图像生成
- en: Apart from the supervised learning tasks, image generation is a representative
    topic of unsupervised learning. It deploys deep generative models, *e.g.*, GAN Goodfellow
    et al. ([2014](#bib.bib13)), to learn to synthesize realistic images which looks
    visually authentic. With the quality of generated images becoming higher, more
    challenging goals are expected, *i.e.*, fine-grained image generation. As the
    term suggests, fine-grained generation will synthesize images in fine-grained
    categories such as faces of a specific person or objects in a subordinate category.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 除了监督学习任务外，图像生成是无监督学习的一个代表性话题。它部署了深度生成模型，例如 GAN Goodfellow 等人（[2014](#bib.bib13)），以学习合成视觉上真实的图像。随着生成图像质量的提高，期望更具挑战性的目标，即细粒度图像生成。顾名思义，细粒度生成将合成细粒度类别中的图像，例如特定人物的面孔或从属类别中的物体。
- en: The first work in this line was CVAE-GAN proposed in Bao et al. ([2017](#bib.bib1)),
    which combines a variational auto-encoder with a generative adversarial network
    under a conditional generative process to tackle this problem. Specifically, CVAE-GAN
    models an image as a composition of label and latent attributes in a probabilistic
    model. Then, by varying the fine-grained category fed into the resulting generative
    model, it can generate images in a specific category. More recently, generating
    images from text descriptions Xu et al. ([2018b](#bib.bib45)) behaves popular
    in the light of its diverse and practical applications, *e.g.*, art generation
    and computer-aided design. By performing an attention equipped generative network,
    the model can synthesize fine-grained details of subtle regions by focusing on
    the relevant words of text descriptions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这一领域的首个工作是 Bao 等人（[2017](#bib.bib1)）提出的 CVAE-GAN，它结合了变分自编码器和条件生成对抗网络，以处理这个问题。具体来说，CVAE-GAN
    将图像建模为标签和潜在属性的组合。在生成模型中通过调整输入的细粒度类别，可以生成特定类别的图像。最近，基于文本描述生成图像的研究，如 Xu 等人（[2018b](#bib.bib45)），因其多样而实际的应用，如艺术创作和计算机辅助设计，变得越来越受欢迎。通过执行配备注意力机制的生成网络，该模型可以通过关注文本描述中的相关词汇来合成细粒度的细节。
- en: 7 Domain specific applications related to fine-grained image analysis
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 与细粒度图像分析相关的领域特定应用
- en: In the real world, deep learning based fine-grained image analysis techniques
    are also adopted to diverse domain specific applications and shows great performance,
    such as clothes/shoes retrieval Song et al. ([2017](#bib.bib33)) in recommendation
    systems, fashion image recognition Liu et al. ([2016](#bib.bib27)) in e-commerce
    platforms, product recognition Wei et al. ([2019a](#bib.bib42)) in intelligent
    retail, etc. These applications are highly related to both fine-grained retrieval
    and recognition of FGIA.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，基于深度学习的细粒度图像分析技术也被应用于多样的领域特定应用，并表现出优异的性能，例如在推荐系统中的衣物/鞋子检索Song等人（[2017](#bib.bib33)），在电子商务平台中的时尚图像识别Liu等人（[2016](#bib.bib27)），在智能零售中的产品识别Wei等人（[2019a](#bib.bib42)）等。这些应用与FGIA的细粒度检索和识别密切相关。
- en: Additionally, if we move down the spectrum of granularity, in the extreme, face
    identification can be viewed as an instance of fine-grained recognition, where
    the granularity is under the identity granularity level. Moreover, person/vehicle
    re-identification is another fine-grained related task, which aims at determining
    whether two images are taken from the same specific person/vehicle. Apparently,
    re-identification tasks are also under identity granularity.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们将粒度谱向下移动，在极端情况下，面部识别可以被视为细粒度识别的一个实例，其粒度低于身份粒度级别。此外，人物/车辆重新识别是另一个与细粒度相关的任务，旨在确定两张图像是否拍摄自同一特定人物/车辆。显然，重新识别任务也属于身份粒度。
- en: In practice, these works solve the corresponding domain specific tasks by following
    the motivations of FGIA, which includes capturing the discriminative parts of
    objects (faces, persons and vehicles) Suh et al. ([2018](#bib.bib34)), discovering
    coarse-to-fine structural information Wei et al. ([2018b](#bib.bib41)), developing
    attribute-based models Liu et al. ([2016](#bib.bib27)), and so on.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这些工作通过遵循FGIA的动机来解决相应领域的特定任务，包括捕捉对象的区分部分（面孔、人物和车辆）Suh等人（[2018](#bib.bib34)），发现从粗到细的结构信息Wei等人（[2018b](#bib.bib41)），开发基于属性的模型Liu等人（[2016](#bib.bib27)）等。
- en: 8 Concluding remarks and future directions
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论和未来方向
- en: Fine-grained image analysis (FGIA) based on deep learning have made great progress
    in recent years. In this paper, we give an extensive survey on recent advances
    in FGIA with deep learning. We mainly introduced the FGIA problem and its challenges,
    discussed the significant improvements of fine-grained image recognition/retrieval/generation,
    and also presented some domain specific applications related to FGIA. Despite
    the great success, there are still many unsolved problems. Thus, in this section,
    we will point out these problems explicitly and introduce some research trends
    for the future evolution. We hope that this survey not only provides a better
    understanding of FGIA but also facilitates future research activities and application
    developments in this field.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的细粒度图像分析（FGIA）近年来取得了重大进展。在本文中，我们对FGIA在深度学习中的最新进展进行了广泛的调查。我们主要介绍了FGIA问题及其挑战，讨论了细粒度图像识别/检索/生成的重大改进，并介绍了一些与FGIA相关的领域特定应用。尽管取得了巨大成功，但仍然存在许多未解决的问题。因此，在这一部分，我们将明确指出这些问题，并介绍一些未来演变的研究趋势。我们希望这项调查不仅能提供对FGIA的更好理解，还能促进该领域未来的研究活动和应用开发。
- en: Automatic fine-grained models
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自动细粒度模型
- en: Nowadays, automated machine learning (AutoML) Feurer et al. ([2015](#bib.bib10))
    and neural architecture search (NAS) Elsken et al. ([2018](#bib.bib9)) are attracting
    fervent attentions in the artificial intelligence community, especially in computer
    vision. AutoML targets automating the end-to-end process of applying machine learning
    to real-world tasks. While, NAS, the process of automating neural network architecture
    designing, is thus a logical next step in AutoML. Recent methods of AutoML and
    NAS could be comparable or even outperform hand-designed architectures in various
    computer vision applications. Thus, it is also promising that automatic fine-grained
    models developed by AutoML or NAS techniques could find a better and more tailor-made
    deep models, and meanwhile it can advance the studies of AutoML and NAS in turn.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，自动化机器学习（AutoML）（Feurer et al. ([2015](#bib.bib10)））和神经网络架构搜索（NAS）（Elsken
    et al. ([2018](#bib.bib9)））在人工智能社区，特别是在计算机视觉领域，正受到热烈关注。AutoML旨在自动化将机器学习应用于现实任务的端到端过程。而NAS，自动化神经网络架构设计的过程，因此是AutoML的逻辑下一步。最近的AutoML和NAS方法在各种计算机视觉应用中可与手工设计的架构相媲美，甚至超越。因此，利用AutoML或NAS技术开发的自动化细粒度模型也有可能找到更好、更量体裁衣的深度模型，同时也能促进AutoML和NAS研究的发展。
- en: Fine-grained few-shot learning
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 细粒度少样本学习
- en: Humans are capable of learning a new fine-grained concept with very little supervision,
    *e.g.*, few exemplary images for a species of bird, yet our best deep learning
    fine-grained systems need hundreds or thousands of labeled examples. Even worse,
    the supervision of fine-grained images are both time-consuming and expensive,
    since fine-grained objects should be always accurately labeled by domain experts.
    Thus, it is desirable to develop fine-grained few-shot learning (FGFS) Wei et
    al. ([2019b](#bib.bib43)). The task of FGFS requires the learning systems to build
    classifiers for novel fine-grained categories from few examples (only one or less
    than five) in an meta-learning fashion. Robust FGFS methods could extremely strengthen
    the usability and scalability of fine-grained recognition.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在非常少量的监督下能够学习新的细粒度概念，例如，对于某种鸟类只需少量的示例图像，而我们目前最先进的深度学习细粒度系统则需要数百或数千个标注样本。更糟的是，细粒度图像的监督既耗时又昂贵，因为细粒度物体需要由领域专家进行准确标注。因此，开发细粒度少样本学习（FGFS）是非常必要的（Wei
    et al. ([2019b](#bib.bib43)））。FGFS任务要求学习系统以元学习的方式从少量样本（仅一个或少于五个）中为新颖的细粒度类别建立分类器。鲁棒的FGFS方法可以极大地增强细粒度识别的可用性和扩展性。
- en: Fine-grained hashing
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 细粒度哈希
- en: As there exist growing attentions on FGIA, more large-scale and well-constructed
    fine-grained datasets have been released, *e.g.*, Berg et al. ([2014](#bib.bib2));
    Horn et al. ([2017](#bib.bib16)); Wei et al. ([2019a](#bib.bib42)). In real applications
    like fine-grained image retrieval, it is natural to raise a problem that the cost
    of finding the exact nearest neighbor is prohibitively high in the case that the
    reference database is very large. Hashing Wang et al. ([2018](#bib.bib38)); Li
    et al. ([2016](#bib.bib24)), acting as one of the most popular and effective techniques
    of approximate nearest neighbor search, has the potential to deal with large-scale
    fine-grained data. Therefore, fine-grained hashing is a promising direction worth
    further explorations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对FGIA的关注日益增加，越来越多的大规模、结构良好的细粒度数据集被发布，例如，Berg et al. ([2014](#bib.bib2))；Horn
    et al. ([2017](#bib.bib16))；Wei et al. ([2019a](#bib.bib42))。在实际应用中，如细粒度图像检索，问题的自然产生是当参考数据库非常大时，找到准确的最近邻的成本极高。哈希（Wang
    et al. ([2018](#bib.bib38))；Li et al. ([2016](#bib.bib24))）作为一种最流行和有效的近似最近邻搜索技术之一，有潜力处理大规模细粒度数据。因此，细粒度哈希是一个值得进一步探索的有前途的方向。
- en: Fine-grained analysis within more realistic settings
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在更现实的环境中的细粒度分析
- en: In the past decade, fine-grained image analysis related techniques have been
    developed and achieve good performance in its traditional settings, *e.g.*, the
    empirical protocols of Wah et al. ([2011](#bib.bib37)); Khosla et al. ([2011](#bib.bib19));
    Krause et al. ([2013](#bib.bib21)). However, these settings can not satisfy the
    daily requirements of various real-world applications nowadays, *e.g.*, recognizing
    retail products in storage racks by models trained with images collected in controlled
    environments Wei et al. ([2019a](#bib.bib42)) and recognizing/detecting natural
    species in the wild Horn et al. ([2017](#bib.bib16)). In consequence, novel fine-grained
    image analysis topics, to name a few—fine-grained analysis with domain adaptation,
    fine-grained analysis with knowledge transfer, fine-grained analysis with long-tailed
    distribution, and fine-grained analysis running on resource constrained embedded
    devices—deserve a lot of research efforts towards the more advanced and practical
    FGIA.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，相关技术的细粒度图像分析已经得到发展，并在其传统设置中取得了良好的表现，*例如*，Wah 等人 ([2011](#bib.bib37))
    的经验协议；Khosla 等人 ([2011](#bib.bib19))；Krause 等人 ([2013](#bib.bib21))。然而，这些设置目前无法满足各种现实世界应用的日常需求，*例如*，通过在受控环境中收集的图像训练的模型识别存储架上的零售产品
    Wei 等人 ([2019a](#bib.bib42)) 和识别/检测野外的自然物种 Horn 等人 ([2017](#bib.bib16))。因此，新兴的细粒度图像分析主题，如领域适应的细粒度分析、知识迁移的细粒度分析、长尾分布的细粒度分析以及在资源受限嵌入式设备上运行的细粒度分析——都值得大量研究工作，以推动更先进和实用的细粒度图像分析。
- en: References
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bao et al. [2017] J. Bao, D. Chen, F. Wen, H. Li, and G. Hua. CVAE-GAN: Fine-grained
    image generation through asymmetric training. In ICCV, pages 2745–2754, 2017.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao 等人 [2017] J. Bao, D. Chen, F. Wen, H. Li, 和 G. Hua. CVAE-GAN：通过非对称训练生成细粒度图像。发表于
    ICCV, 页码 2745–2754, 2017。
- en: 'Berg et al. [2014] T. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs,
    and P. N. Belhumeur. Birdsnap: Large-scale fine-grained visual categorization
    of birds. In CVPR, pages 2019–2026, 2014.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berg 等人 [2014] T. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs, 和
    P. N. Belhumeur. Birdsnap：大规模细粒度鸟类视觉分类。发表于 CVPR, 页码 2019–2026, 2014。
- en: Charikar et al. [2002] M. Charikar, K. Chen, and M. Farach-Colton. Finding frequent
    items in data streams. In ICALP, pages 693–703, 2002.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charikar 等人 [2002] M. Charikar, K. Chen, 和 M. Farach-Colton. 在数据流中查找频繁项。发表于
    ICALP, 页码 693–703, 2002。
- en: Chen et al. [2018] T. Chen, L. Lin, R. Chen, Y. Wu, and X. Luo. Knowledge-embedded
    representation learning for fine-grained image recognition. In IJCAI, pages 627–634,
    2018.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2018] T. Chen, L. Lin, R. Chen, Y. Wu, 和 X. Luo. 知识嵌入表示学习用于细粒度图像识别。发表于
    IJCAI, 页码 627–634, 2018。
- en: Cui et al. [2016] Y. Cui, F. Zhou, Y. Lin, and S. Belongie. Fine-grained categorization
    and dataset bootstrapping using deep metric learning with humans in the loop.
    In CVPR, pages 1153–1162, 2016.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等人 [2016] Y. Cui, F. Zhou, Y. Lin, 和 S. Belongie. 使用深度度量学习与人工干预进行细粒度分类和数据集自举。发表于
    CVPR, 页码 1153–1162, 2016。
- en: Cui et al. [2017] Y. Cui, F. Zhou, J. Wang, X. Liu, Y. Lin, and S. Belongie.
    Kernel pooling for convolutional neural network. In CVPR, pages 2921–2930, 2017.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等人 [2017] Y. Cui, F. Zhou, J. Wang, X. Liu, Y. Lin, 和 S. Belongie. 卷积神经网络的核池化。发表于
    CVPR, 页码 2921–2930, 2017。
- en: Deng et al. [2016] J. Deng, J. Krause, M. Stark, and L. Fei-Fei. Leveraging
    the wisdom of the crowd for fine-grained recognition. TPAMI, 38(4):666–676, 2016.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人 [2016] J. Deng, J. Krause, M. Stark, 和 L. Fei-Fei. 利用众智进行细粒度识别。TPAMI,
    38(4):666–676, 2016。
- en: Dubey et al. [2018] A. Dubey, O. Gupta, R. Raskar, and N. Naik. Maximum entropy
    fine-grained classification. In NeurIPS, pages 637–647, 2018.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey 等人 [2018] A. Dubey, O. Gupta, R. Raskar, 和 N. Naik. 最大熵细粒度分类。发表于 NeurIPS,
    页码 637–647, 2018。
- en: 'Elsken et al. [2018] T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture
    search: A survey. arXiv preprint arXiv:1808.05377, 2018.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsken 等人 [2018] T. Elsken, J. H. Metzen, 和 F. Hutter. 神经网络架构搜索：综述。arXiv 预印本
    arXiv:1808.05377, 2018。
- en: Feurer et al. [2015] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg,
    M. Blum, and F. Hutter. Efficient and robust automated machine learning. In NIPS,
    pages 2962–2970, 2015.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feurer 等人 [2015] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum,
    和 F. Hutter. 高效且稳健的自动化机器学习。发表于 NIPS, 页码 2962–2970, 2015。
- en: 'Fu et al. [2017] J. Fu, H. Zheng, and T. Mei. Look closer to see better: Recurrent
    attention convolutional neural network for fine-grained image recognition. In
    CVPR, pages 4438–4446, 2017.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人 [2017] J. Fu, H. Zheng, 和 T. Mei. 更仔细地观察以获得更好的效果：用于细粒度图像识别的递归注意卷积神经网络。发表于
    CVPR, 页码 4438–4446, 2017。
- en: Gao et al. [2016] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact bilinear
    pooling. In CVPR, pages 317–326, 2016.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 [2016] Y. Gao, O. Beijbom, N. Zhang, 和 T. Darrell. 紧凑双线性池化。发表于 CVPR,
    页码 317–326, 2016。
- en: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pages
    2672–2680, 2014.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio. 生成对抗网络。发表于 NIPS，第 2672–2680 页，2014 年。
- en: He and Peng [2017a] X. He and Y. Peng. Fine-grained image classification via
    combining vision and language. In CVPR, pages 5994–6002, 2017.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He and Peng [2017a] X. He and Y. Peng. 通过结合视觉和语言进行细粒度图像分类。发表于 CVPR，第 5994–6002
    页，2017 年。
- en: He and Peng [2017b] X. He and Y. Peng. Weakly supervised learning of part selection
    model with spatial constraints for fine-grained image classification. In AAAI,
    pages 4075–4081, 2017.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He and Peng [2017b] X. He and Y. Peng. 基于空间约束的弱监督部分选择模型用于细粒度图像分类。发表于 AAAI，第
    4075–4081 页，2017 年。
- en: Horn et al. [2017] G. Van Horn, O. M. Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard,
    H. Adam, P. Perona, and S. Belongie. The iNaturalist species classification and
    detection dataset. In CVPR, pages 8769–8778, 2017.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horn et al. [2017] G. Van Horn, O. M. Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard,
    H. Adam, P. Perona, and S. Belongie. iNaturalist 物种分类与检测数据集。发表于 CVPR，第 8769–8778
    页，2017 年。
- en: 'Hou et al. [2017] S. Hou, Y. Feng, and Z. Wang. VegFru: A domain-specific dataset
    for fine-grained visual categorization. In ICCV, pages 541–549, 2017.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hou et al. [2017] S. Hou, Y. Feng, and Z. Wang. VegFru: 一个用于细粒度视觉分类的领域特定数据集。发表于
    ICCV，第 541–549 页，2017 年。'
- en: Jaderberg et al. [2015] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu.
    Spatial transformer networks. In NIPS, pages 2017–2025, 2015.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg et al. [2015] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu.
    空间变换网络。发表于 NIPS，第 2017–2025 页，2015 年。
- en: Khosla et al. [2011] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei.
    Novel dataset for fine-grained image categorization. In CVPR Workshop on Fine-Grained
    Visual Categorization, pages 806–813, 2011.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosla et al. [2011] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei.
    新的细粒度图像分类数据集。发表于 CVPR 细粒度视觉分类研讨会，第 806–813 页，2011 年。
- en: Kong and Fowlkes [2017] S. Kong and C. Fowlkes. Low-rank bilinear pooling for
    fine-grained classification. In CVPR, pages 365–374, 2017.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong and Fowlkes [2017] S. Kong and C. Fowlkes. 用于细粒度分类的低秩双线性池化。发表于 CVPR，第 365–374
    页，2017 年。
- en: Krause et al. [2013] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3D object
    representations for fine-grained categorization. In ICCV Workshop on 3D Representation
    and Recognition, 2013.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krause et al. [2013] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 用于细粒度分类的
    3D 物体表示。发表于 ICCV 3D 表示与识别研讨会，2013 年。
- en: LeCun et al. [2015] Y. LeCun, Y. Bengion, and G. Hinton. Deep learning. Nature,
    521:436–444, 2015.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. [2015] Y. LeCun, Y. Bengio, and G. Hinton. 深度学习。Nature, 521:436–444,
    2015 年。
- en: Lehmann et al. [2015] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas,
    P. N. Mendes, S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer. DBpedia
    - A large-scale, multilingual knowledge base extracted from Wikipedia. Semantic
    Web Journal, pages 167–195, 2015.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lehmann et al. [2015] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas,
    P. N. Mendes, S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer. DBpedia
    - 从 Wikipedia 提取的大规模多语言知识库。语义网期刊，第 167–195 页，2015 年。
- en: Li et al. [2016] W.-J. Li, S. Wang, and W.-C. Kang. Feature learning based deep
    supervised hashing with pairwise labels. In IJCAI, pages 1711–1717, 2016.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2016] W.-J. Li, S. Wang, and W.-C. Kang. 基于对偶标签的深度监督哈希特征学习。发表于 IJCAI，第
    1711–1717 页，2016 年。
- en: 'Lin et al. [2015a] D. Lin, X. Shen, C. Lu, and J. Jia. Deep LAC: Deep localization,
    alignment and classification for fine-grained recognition. In CVPR, pages 1666–1674,
    2015.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2015a] D. Lin, X. Shen, C. Lu, and J. Jia. Deep LAC: 用于细粒度识别的深度定位、对齐和分类。发表于
    CVPR，第 1666–1674 页，2015 年。'
- en: Lin et al. [2015b] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear CNN models
    for fine-grained visual recognition. In ICCV, pages 1449–1457, 2015.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2015b] T.-Y. Lin, A. RoyChowdhury, and S. Maji. 用于细粒度视觉识别的双线性 CNN
    模型。发表于 ICCV，第 1449–1457 页，2015 年。
- en: 'Liu et al. [2016] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. DeepFashion:
    Powering robust clothes recognition and retrieval with rich annotations. In CVPR,
    pages 1096–1104, 2016.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2016] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. DeepFashion:
    通过丰富的注释提升鲁棒的服装识别与检索。发表于 CVPR，第 1096–1104 页，2016 年。'
- en: Maji et al. [2013] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.
    Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151,
    2013.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maji et al. [2013] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.
    飞机的细粒度视觉分类。arXiv 预印本 arXiv:1306.5151, 2013 年。
- en: Nilsback and Zisserman [2008] M.-E. Nilsback and A. Zisserman. Automated flower
    classification over a large number of classes. In Indian Conf. on Comput. Vision,
    Graph. and Image Process., pages 722–729, 2008.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nilsback and Zisserman [2008] M.-E. Nilsback and A. Zisserman. 在大量类别中进行自动花卉分类。发表于印度计算机视觉、图形与图像处理会议，第
    722–729 页，2008 年。
- en: 'Niu et al. [2018] L. Niu, A. Veeraraghavan, and A. Sabharwal. Webly supervised
    learning meets zero-shot learning: A hybrid approach for fine-grained classification.
    In CVPR, pages 7171–7180, 2018.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu 等人 [2018] L. Niu, A. Veeraraghavan, 和 A. Sabharwal. Webly 监督学习与零样本学习相结合：一种用于细粒度分类的混合方法。发表于
    CVPR, 页码 7171–7180, 2018。
- en: Pham and Pagh [2013] N. Pham and R. Pagh. Fast and scalable polynomial kernels
    via explicit feature maps. In KDD, pages 239–247, 2013.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pham 和 Pagh [2013] N. Pham 和 R. Pagh. 通过显式特征映射实现快速可扩展的多项式核。发表于 KDD, 页码 239–247,
    2013。
- en: Reed et al. [2016] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning deep
    representations of fine-grained visual descriptions. In CVPR, pages 49–58, 2016.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reed 等人 [2016] S. Reed, Z. Akata, H. Lee, 和 B. Schiele. 学习细粒度视觉描述的深度表示。发表于 CVPR,
    页码 49–58, 2016。
- en: Song et al. [2017] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, and T. M. Hospedales.
    Deep spatial-semantic attention for fine-grained sketch-based image retrieval.
    In ICCV, pages 5551–5560, 2017.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 [2017] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, 和 T. M. Hospedales. 用于细粒度草图图像检索的深度空间-语义注意力。发表于
    ICCV, 页码 5551–5560, 2017。
- en: Suh et al. [2018] Y. Suh, J. Wang, S. Tang, T. Mei, and K. M. Lee. Part-aligned
    bilinear representations for person re-identification. In ECCV, pages 402–419,
    2018.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suh 等人 [2018] Y. Suh, J. Wang, S. Tang, T. Mei, 和 K. M. Lee. 部分对齐的双线性表示用于行人重新识别。发表于
    ECCV, 页码 402–419, 2018。
- en: Sun et al. [2018] M. Sun, Y. Yuan, F. Zhou, and E. Ding. Multi-attention multi-class
    constraint for fine-grained image recognition. In ECCV, pages 834–850, 2018.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2018] M. Sun, Y. Yuan, F. Zhou, 和 E. Ding. 用于细粒度图像识别的多重注意力多类约束。发表于 ECCV,
    页码 834–850, 2018。
- en: Sun et al. [2019] X. Sun, L. Chen, and J. Yang. Learning from web data using
    adversarial discriminative neural networks for fine-grained classification. In
    AAAI, 2019.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2019] X. Sun, L. Chen, 和 J. Yang. 使用对抗性判别神经网络从网络数据中学习以进行细粒度分类。发表于 AAAI,
    2019。
- en: Wah et al. [2011] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
    The Caltech-UCSD birds-200-2011 dataset. Tech. Report CNS-TR-2011-001, 2011.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wah 等人 [2011] C. Wah, S. Branson, P. Welinder, P. Perona, 和 S. Belongie. Caltech-UCSD
    鸟类数据集 200-2011。技术报告 CNS-TR-2011-001, 2011。
- en: Wang et al. [2018] J. Wang, T. Zhang, J. Song, N. Sebe, and H. T. Shen. A survey
    on learning to hash. TPAMI, 40(4):769–790, 2018.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2018] J. Wang, T. Zhang, J. Song, N. Sebe, 和 H. T. Shen. 学习哈希的调查。TPAMI,
    40(4):769–790, 2018。
- en: Wei et al. [2017] X.-S. Wei, J.-H. Luo, J. Wu, and Z.-H. Zhou. Selective convolutional
    descriptor aggregation for fine-grained image retrieval. TIP, 26(6):2868–2881,
    2017.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2017] X.-S. Wei, J.-H. Luo, J. Wu, 和 Z.-H. Zhou. 用于细粒度图像检索的选择性卷积描述符聚合。TIP,
    26(6):2868–2881, 2017。
- en: 'Wei et al. [2018a] X.-S. Wei, C.-W. Xie, J. Wu, and C. Shen. Mask-CNN: Localizing
    parts and selecting descriptors for fine-grained bird species categorization.
    Pattern Recognition, 76:704–714, 2018.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2018a] X.-S. Wei, C.-W. Xie, J. Wu, 和 C. Shen. Mask-CNN：局部化部件并选择描述符用于细粒度鸟类物种分类。Pattern
    Recognition, 76:704–714, 2018。
- en: 'Wei et al. [2018b] X.-S. Wei, C.-L. Zhang, L. Liu, C. Shen, and J. Wu. Coarse-to-fine:
    A RNN-based hierarchical attention model for vehicle re-identification. In ACCV,
    2018.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2018b] X.-S. Wei, C.-L. Zhang, L. Liu, C. Shen, 和 J. Wu. 粗到细：一种基于 RNN
    的层次注意力模型用于车辆重新识别。发表于 ACCV, 2018。
- en: 'Wei et al. [2019a] X.-S. Wei, Q. Cui, L. Yang, P. Wang, and L. Liu. RPC: A
    large-scale retail product checkout dataset. arXiv preprint arXiv:1901.07249,
    2019.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2019a] X.-S. Wei, Q. Cui, L. Yang, P. Wang, 和 L. Liu. RPC：一个大规模零售产品结账数据集。arXiv
    预印本 arXiv:1901.07249, 2019。
- en: 'Wei et al. [2019b] X.-S. Wei, P. Wang, L. Liu, C. Shen, and J. Wu. Piecewise
    classifier mappings: Learning fine-grained learners for novel categories with
    few examples. TIP, in press, 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2019b] X.-S. Wei, P. Wang, L. Liu, C. Shen, 和 J. Wu. 分段分类器映射：为具有少量样本的新类别学习细粒度学习者。TIP,
    即将发表, 2019。
- en: Xu et al. [2018a] H. Xu, G. Qi, J. Li, M. Wang, K. Xu, and H. Gao. Fine-grained
    image classification by visual-semantic embedding. In IJCAI, pages 1043–1049,
    2018.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 [2018a] H. Xu, G. Qi, J. Li, M. Wang, K. Xu, 和 H. Gao. 通过视觉-语义嵌入进行细粒度图像分类。发表于
    IJCAI, 页码 1043–1049, 2018。
- en: 'Xu et al. [2018b] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and
    X. He. AttnGAN: Fine-grained text to image generation with attentional generative
    adversarial networks. In CVPR, pages 1316–1324, 2018.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 [2018b] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, 和 X. He.
    AttnGAN：使用注意力生成对抗网络进行细粒度文本到图像生成。发表于 CVPR, 页码 1316–1324, 2018。
- en: Yang et al. [2018] Z. Yang, T. Luo, D. Wang, Z. Hu, J. Gao, and L. Wang. Learning
    to navigate for fine-grained classification. In ECCV, pages 438–454, 2018.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2018] Z. Yang, T. Luo, D. Wang, Z. Hu, J. Gao, 和 L. Wang. 学习导航以进行细粒度分类。发表于
    ECCV, 页码 438–454, 2018。
- en: Zhang et al. [2014] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-based
    R-CNNs for fine-grained category detection. In ECCV, pages 834–849, 2014.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2014] N. Zhang, J. Donahue, R. Girshick, 和 T. Darrell. 基于部件的 R-CNNs 用于细粒度类别检测。在
    ECCV 上，页面 834–849，2014年。
- en: Zhang et al. [2018] Y. Zhang, H. Tang, and K. Jia. Fine-grained visual categorization
    using meta-learning optimization with sample selection of auxiliary data. In ECCV,
    pages 233–248, 2018.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2018] Y. Zhang, H. Tang, 和 K. Jia. 使用元学习优化和辅助数据样本选择的细粒度视觉分类。在 ECCV 上，页面
    233–248，2018年。
- en: Zhao et al. [2017] B. Zhao, J. Feng, X. Wu, and S. Yan. A survey on deep learning-based
    fine-grained object classification and semantic segmentation. International Journal
    of Automation and Computing, 14(2):119–135, 2017.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人 [2017] B. Zhao, J. Feng, X. Wu, 和 S. Yan. 基于深度学习的细粒度物体分类和语义分割综述。国际自动化与计算期刊，14(2)：119–135，2017年。
- en: Zheng et al. [2017] H. Zheng, J. Fu, T. Mei, and J. Luo. Learning multi-attention
    convolutional neural network for fine-grained image recognition. In ICCV, pages
    5209–5217, 2017.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人 [2017] H. Zheng, J. Fu, T. Mei, 和 J. Luo. 学习用于细粒度图像识别的多重注意力卷积神经网络。在 ICCV
    上，页面 5209–5217，2017年。
- en: Zheng et al. [2018] X. Zheng, R. Ji, X. Sun, Y. Wu, F. Huang, and Y. Yang. Centralized
    ranking loss with weakly supervised localization for fine-grained object retrieval.
    In IJCAI, pages 1226–1233, 2018.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人 [2018] X. Zheng, R. Ji, X. Sun, Y. Wu, F. Huang, 和 Y. Yang. 具有弱监督定位的集中排名损失用于细粒度物体检索。在
    IJCAI 上，页面 1226–1233，2018年。
- en: Zheng et al. [2019] X. Zheng, R. Ji, X. Sun, B. Zhang, Y. Wu, and F. Huang.
    Towards optimal fine grained retrieval via decorrelated centralized loss with
    normalize-scale layer. In AAAI, 2019.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人 [2019] X. Zheng, R. Ji, X. Sun, B. Zhang, Y. Wu, 和 F. Huang. 通过去相关集中损失与归一化缩放层实现最佳细粒度检索。在
    AAAI 上，2019年。
- en: 'Zhuang et al. [2017] B. Zhuang, L. Liu, Y. Li, C. Shen, and I. Reid. Attend
    in groups: a weakly-supervised deep learning framework for learning from web data.
    In CVPR, pages 1878–1887, 2017.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 庄等人 [2017] B. Zhuang, L. Liu, Y. Li, C. Shen, 和 I. Reid. 以组为单位进行关注：一种弱监督深度学习框架，用于从网页数据中学习。在
    CVPR 上，页面 1878–1887，2017年。
